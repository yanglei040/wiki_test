{"hands_on_practices": [{"introduction": "要掌握稀疏恢复，理解其核心算法的内在机制至关重要。迭代硬阈值（IHT）算法因其简洁和直观性，是学习压缩感知的绝佳起点。本练习将引导你通过第一性原理，推导保证IHT算法收敛的关键参数——步长$ \\eta $的约束条件。通过这个过程，你将深刻理解算法的收敛性如何与测量矩阵$ A $的谱范数等基本性质联系起来，并学会如何为迭代算法设定合理的超参数。", "problem": "考虑压缩感知中的一个线性测量模型，其形式为 $y = A x^{\\star}$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x^{\\star} \\in \\mathbb{R}^{n}$ 是一个未知的 $s$-稀疏信号，$y \\in \\mathbb{R}^{m}$ 是测量向量。设目标为最小二乘损失 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$，并考虑迭代硬阈值（Iterative Hard Thresholding, IHT）算法，其定义为 $x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$，其中 $H_{s}(\\cdot)$ 表示按幅值对 $s$ 个元素进行硬阈值处理，$\\eta > 0$ 是一个常数步长。假设 $A$ 满足适当阶数的约束等距性质（Restricted Isometry Property, RIP），其约束等距常数 $\\delta_{k}$ 定义为对于所有支撑集大小至多为 $k$ 的向量 $u$，都有 $(1 - \\delta_{k})\\|u\\|_{2}^{2} \\leq \\|A u\\|_{2}^{2} \\leq (1 + \\delta_{k})\\|u\\|_{2}^{2}$。在确保IHT误差递推在迭代过程中遇到的支撑集并集上收缩的适当RIP条件下，从第一性原理出发，推导允许的常数步长的一个显式上界，该上界仅用谱范数 $\\|A\\|_{2 \\to 2}$ 表示，并保证IHT迭代收敛到 $s$-稀疏解。将您的最终界限表示为单个闭式解析表达式。无需数值取整。", "solution": "迭代硬阈值（IHT）算法旨在为线性系统 $y = Ax$ 找到一个稀疏解 $x$。其更新规则为：\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta \\nabla f(x^{t})\\right)$$\n其中 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ 是最小二乘损失函数，$\\eta$ 是步长，$H_s(\\cdot)$ 是将向量投影到 $s$-稀疏向量集合上的硬阈值算子。\n\n损失函数的梯度为 $\\nabla f(x) = A^{\\top}(A x - y)$。将其代入IHT更新规则，得到：\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - y)\\right)$$\n我们已知 $y = A x^{\\star}$，其中 $x^{\\star}$ 是我们希望恢复的真实 $s$-稀疏信号。将此代入更新规则：\n$$x^{t+1} = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}(A x^{t} - A x^{\\star})\\right) = H_{s}\\!\\left(x^{t} - \\eta A^{\\top}A(x^{t} - x^{\\star})\\right)$$\n设 $e^{t} = x^{t} - x^{\\star}$ 为第 $t$ 次迭代时的误差。我们可以用误差和真实信号 $x^{\\star}$ 来重写硬阈值算子的参数：\n$$x^{t+1} = H_{s}\\!\\left(x^{\\star} + e^{t} - \\eta A^{\\top}A e^{t}\\right) = H_{s}\\!\\left(x^{\\star} + (I - \\eta A^{\\top}A)e^{t}\\right)$$\nIHT的收敛性证明分析了误差范数 $\\|e^{t+1}\\|_2 = \\|x^{t+1}-x^\\star\\|_2$ 的演化。这些证明的核心是在受限子空间上迭代的行为。误差向量 $e^t = x^t - x^\\star$ 的支撑集是 $x^t$ 和 $x^\\star$ 支撑集的并集，即 $\\text{supp}(x^t) \\cup \\text{supp}(x^\\star)$。由于 $x^t$（根据构造）和 $x^\\star$（根据假设）都是 $s$-稀疏的，所以 $e^t$ 的支撑集大小至多为 $2s$。\n\n问题陈述引导我们假设存在适当的RIP条件来确保收缩。这使我们能够专注于收敛的主要驱动因素，即让梯度下降步骤，也就是 $z \\mapsto (I - \\eta A^{\\top}A)z$，在相关的稀疏子空间上是收缩的。\n设 $T$ 是一个索引集合，满足 $|T| \\leq 2s$。设 $z$ 是一个支撑集在 $T$ 上的向量。梯度更新在 $z$ 上的作用对应于乘以矩阵 $(I - \\eta A_T^{\\top}A_T)$，其中 $A_T$ 是由 $A$ 中索引为 $T$ 的列组成的子矩阵。为使整个算法收敛，一个关键要求是该算子在支撑集为 $T$ 的向量空间上是严格收缩的。\n严格收缩的条件是该矩阵的谱范数（算子 $2$-范数）小于 $1$：\n$$\\|I - \\eta A_T^{\\top}A_T\\|_{2 \\to 2}  1$$\n矩阵 $A_T^{\\top}A_T$ 是对称半正定的。设其特征值为 $\\lambda_i(A_T^{\\top}A_T)$。那么 $I - \\eta A_T^{\\top}A_T$ 的特征值为 $1 - \\eta \\lambda_i(A_T^{\\top}A_T)$。谱范数是这些特征值绝对值的最大值。\n$$\\max_{i} |1 - \\eta \\lambda_i(A_T^{\\top}A_T)|  1$$\n这个不等式等价于以下两个条件对所有特征值 $\\lambda_i = \\lambda_i(A_T^{\\top}A_T)$ 都成立：\n$$-1  1 - \\eta \\lambda_i  1$$\n右侧不等式 $1 - \\eta \\lambda_i  1$ 意味着 $-\\eta \\lambda_i  0$，因为 $\\eta > 0$ 且 $\\lambda_i \\ge 0$，所以这个条件恒成立（对于非零特征值）。\n左侧不等式 $-1  1 - \\eta \\lambda_i$ 意味着 $\\eta \\lambda_i  2$。\n这个条件必须对所有特征值都成立，包括最大的特征值 $\\lambda_{\\max}(A_T^{\\top}A_T)$。因此，我们要求：\n$$\\eta \\lambda_{\\max}(A_T^{\\top}A_T)  2 \\implies \\eta  \\frac{2}{\\lambda_{\\max}(A_T^{\\top}A_T)}$$\n$A_T^{\\top}A_T$ 的最大特征值等于 $A_T$ 的谱范数的平方：$\\lambda_{\\max}(A_T^{\\top}A_T) = \\|A_T\\|_{2 \\to 2}^2$。所以条件变为：\n$$\\eta  \\frac{2}{\\|A_T\\|_{2 \\to 2}^2}$$\n这个条件必须对迭代过程中遇到的任何支撑集 $T$ 都成立，如前所述，其大小至多为 $2s$。因此，我们必须为“最坏情况”的子矩阵满足这个条件，即谱范数最大的那个子矩阵：\n$$\\eta  \\frac{2}{\\max_{|T| \\leq 2s} \\|A_T\\|_{2 \\to 2}^2}$$\n问题要求一个仅用整个矩阵 $A$ 的全局谱范数（表示为 $\\|A\\|_{2 \\to 2}$）表示的上界。任何子矩阵 $A_T$ 的谱范数都受完整矩阵 $A$ 的谱范数的限制：\n$$\\|A_T\\|_{2 \\to 2} = \\sup_{\\|z\\|_2=1, \\text{supp}(z) \\subseteq T} \\|Az\\|_2 \\leq \\sup_{\\|x\\|_2=1, x \\in \\mathbb{R}^n} \\|Ax\\|_2 = \\|A\\|_{2 \\to 2}$$\n这意味着：\n$$\\max_{|T| \\leq 2s} \\|A_T\\|_{2 \\to 2}^2 \\leq \\|A\\|_{2 \\to 2}^2$$\n因此，如果我们选择一个满足更严格条件的步长 $\\eta$：\n$$\\eta  \\frac{2}{\\|A\\|_{2 \\to 2}^2}$$\n那么在所有相关子空间上的收缩条件 $\\eta  2 / (\\max_{|T| \\leq 2s} \\|A_T\\|_{2 \\to 2}^2)$ 也将被满足。\n这为步长 $\\eta$ 在问题假设下确保收敛提供了一个充分条件。因此，允许的常数步长的显式上界就是该不等式右侧的值。", "answer": "$$\n\\boxed{\\frac{2}{\\|A\\|_{2 \\to 2}^{2}}}\n$$", "id": "3459927"}, {"introduction": "除了恢复算法本身，测量矩阵$ A $的几何性质对稀疏恢复的成功与否起着决定性作用，“非相干性”是其中一个核心要求。本练习通过一个精炼的 $ 2 \\times 2 $ 示例，让你直观地感受测量矩阵的列相关性如何影响基追踪（Basis Pursuit）的恢复条件。你将分析并比较列归一化和白化这两种预处理策略对提升恢复性能的影响，从而深入理解对偶可行性以及测量矩阵几何构型在稀疏信号处理中的关键作用。[@problem_id:3459939]", "problem": "考虑一个压缩感知中的线性测量模型，形式为 $y = A x$，其中 $A \\in \\mathbb{R}^{2 \\times 2}$ 是满列秩的，其列向量为 $a_{1}$ 和 $a_{2}$。假设真实信号 $x_{0} \\in \\mathbb{R}^{2}$ 是单稀疏的，其支撑集在第一个坐标上，因此 $y = A x_{0} = a_{1}$。定义列相关系数\n$$\n\\rho \\triangleq \\frac{a_{1}^{\\top} a_{2}}{\\|a_{1}\\|_{2} \\, \\|a_{2}\\|_{2}},\n$$\n并假设 $0  |\\rho|  1$。\n\n基追踪（BP）通过以下方式寻求与测量值一致的最稀疏信号\n$$\n\\min_{x \\in \\mathbb{R}^{2}} \\|x\\|_{1} \\quad \\text{subject to} \\quad y = A x,\n$$\n其拉格朗日对偶问题为\n$$\n\\max_{z \\in \\mathbb{R}^{2}} y^{\\top} z \\quad \\text{subject to} \\quad \\|A^{\\top} z\\|_{\\infty} \\leq 1.\n$$\n对偶可行性约束 $\\|A^{\\top} z\\|_{\\infty} \\leq 1$ 对 $A$ 的列之间的相关性很敏感。\n\n有两种标准的预处理策略可以减轻列相关性：\n- 列归一化：$A_{\\mathrm{n}} \\triangleq A D^{-1}$，其中 $D = \\mathrm{diag}(\\|a_{1}\\|_{2}, \\|a_{2}\\|_{2})$，因此 $A_{\\mathrm{n}}$ 的每一列都具有单位欧几里得范数。\n- 通过左预处理进行列白化：$A_{\\mathrm{w}} \\triangleq W A$，其中 $W \\triangleq (A A^{\\top})^{-1/2}$，这可以均衡测量几何结构并得到 $A_{\\mathrm{w}}^{\\top} A_{\\mathrm{w}} = I_{2}$。\n\n为了分析这些策略如何影响对偶可行性以及 BP 对列相关的敏感性，我们将每种预处理策略的规范对偶凭证定义为与测量数据对齐的对偶变量：对于归一化，$z_{\\mathrm{n}}$ 被选择为在 $A_{\\mathrm{n}}$ 下与 $y$ 共线，使得 $a_{1,\\mathrm{n}}^{\\top} z_{\\mathrm{n}} = 1$；对于白化，$z_{\\mathrm{w}}$ 被选择为与 $y_{\\mathrm{w}} \\triangleq W y$ 共线，使得 $a_{1,\\mathrm{w}}^{\\top} z_{\\mathrm{w}} = 1$。作为对偶可行性松弛度（以及对非支撑集上相关性的敏感性）的度量，分别定义归一化和白化下的裕度为\n$$\nm_{\\mathrm{n}} \\triangleq 1 - \\left|a_{2,\\mathrm{n}}^{\\top} z_{\\mathrm{n}}\\right|, \\qquad m_{\\mathrm{w}} \\triangleq 1 - \\left|a_{2,\\mathrm{w}}^{\\top} z_{\\mathrm{w}}\\right|.\n$$\n\n从第一性原理和基本定义出发，推导改进因子\n$$\nR(\\rho) \\triangleq \\frac{m_{\\mathrm{w}}}{m_{\\mathrm{n}}}\n$$\n的闭式表达式，作为相关系数 $\\rho$ 的函数。将你的最终答案表示为单个解析表达式。无需四舍五入。", "solution": "问题要求计算两种不同预处理策略（列归一化和列白化）的对偶可行性裕度之比 $R(\\rho) = \\frac{m_{\\mathrm{w}}}{m_{\\mathrm{n}}}$。我们将从第一性原理出发推导裕度 $m_{\\mathrm{n}}$ 和 $m_{\\mathrm{w}}$ 的表达式，然后计算它们的比值。\n\n真实信号是 $x_0 \\in \\mathbb{R}^2$，它是单稀疏的，支撑集在第一个坐标上。测量值由 $y = A x_0 = a_1$ 给出。由于 $A$ 的列是 $a_1$ 和 $a_2$，我们有 $y = A x_0 = x_{0,1} a_1 + x_{0,2} a_2$。鉴于 $x_0$ 是单稀疏的，支撑集在第一个坐标上，所以 $x_{0,2} = 0$。因此，$y = x_{0,1} a_1$。问题中说明 $y=a_1$。由于 $A$ 是满列秩的，$a_1$ 是一个非零向量，这意味着 $x_{0,1} = 1$。因此，真实信号是 $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n首先，我们分析列归一化的情况来求 $m_{\\mathrm{n}}$。\n归一化矩阵是 $A_{\\mathrm{n}} = A D^{-1}$，其中 $D = \\mathrm{diag}(\\|a_1\\|_2, \\|a_2\\|_2)$。$A_{\\mathrm{n}}$ 的列是 $a_{1,\\mathrm{n}} = a_1/\\|a_1\\|_2$ 和 $a_{2,\\mathrm{n}} = a_2/\\|a_2\\|_2$。根据定义，这些列具有单位欧几里得范数，即 $\\|a_{1,\\mathrm{n}}\\|_2 = 1$ 和 $\\|a_{2,\\mathrm{n}}\\|_2 = 1$。\n相关系数 $\\rho$ 定义为 $\\rho = \\frac{a_1^{\\top} a_2}{\\|a_1\\|_2 \\|a_2\\|_2}$。这可以用归一化列表示为 $\\rho = (a_1/\\|a_1\\|_2)^{\\top} (a_2/\\|a_2\\|_2) = a_{1,\\mathrm{n}}^{\\top} a_{2,\\mathrm{n}}$。\n\n对偶凭证 $z_{\\mathrm{n}}$ 被定义为与测量数据 $y=a_1$ 共线。所以，我们可以写成 $z_{\\mathrm{n}} = c a_1$，其中 $c \\in \\mathbb{R}$ 是某个标量。$c$ 的值由条件 $a_{1,\\mathrm{n}}^{\\top} z_{\\mathrm{n}} = 1$ 确定。\n代入 $z_{\\mathrm{n}}$ 和 $a_{1,\\mathrm{n}}$ 的表达式：\n$$\n(a_1/\\|a_1\\|_2)^{\\top} (c a_1) = 1\n$$\n$$\n\\frac{c}{\\|a_1\\|_2} (a_1^{\\top} a_1) = 1\n$$\n$$\n\\frac{c}{\\|a_1\\|_2} \\|a_1\\|_2^2 = 1\n$$\n$$\nc \\|a_1\\|_2 = 1 \\implies c = \\frac{1}{\\|a_1\\|_2}\n$$\n因此，对偶凭证是 $z_{\\mathrm{n}} = \\frac{a_1}{\\|a_1\\|_2} = a_{1,\\mathrm{n}}$。\n\n归一化下的裕度 $m_{\\mathrm{n}}$ 定义为 $m_{\\mathrm{n}} = 1 - |a_{2,\\mathrm{n}}^{\\top} z_{\\mathrm{n}}|$。代入 $z_{\\mathrm{n}} = a_{1,\\mathrm{n}}$：\n$$\nm_{\\mathrm{n}} = 1 - |a_{2,\\mathrm{n}}^{\\top} a_{1,\\mathrm{n}}|\n$$\n由于 $a_{1,\\mathrm{n}}^{\\top} a_{2,\\mathrm{n}} = \\rho$，我们有：\n$$\nm_{\\mathrm{n}} = 1 - |\\rho|\n$$\n\n接下来，我们分析列白化的情况来求 $m_{\\mathrm{w}}$。\n白化矩阵是 $A_{\\mathrm{w}} = W A$，白化矩阵为 $W = (A A^{\\top})^{-1/2}$。问题中说明这会得到 $A_{\\mathrm{w}}^{\\top} A_{\\mathrm{w}} = I_2$。这个性质意味着 $A_{\\mathrm{w}}$ 的列（表示为 $a_{1,\\mathrm{w}} = W a_1$ 和 $a_{2,\\mathrm{w}} = W a_2$）是标准正交的。也就是说，$a_{i,\\mathrm{w}}^{\\top} a_{j,\\mathrm{w}} = \\delta_{ij}$，其中 $i,j \\in \\{1,2\\}$，$\\delta_{ij}$ 是克罗内克 δ。具体来说，$\\|a_{1,\\mathrm{w}}\\|_2 = 1$，$\\|a_{2,\\mathrm{w}}\\|_2 = 1$，以及 $a_{1,\\mathrm{w}}^{\\top} a_{2,\\mathrm{w}} = 0$。\n问题中说明这对满列秩的 $A \\in \\mathbb{R}^{2 \\times 2}$ 成立。由于 $A$ 是一个满列秩的方阵，所以它是可逆的。因此我们可以写出 $W^2 = (A A^{\\top})^{-1} = (A^{\\top})^{-1} A^{-1}$。由于 $W$ 是 $W^2$ 的对称正定平方根，我们有 $W=W^\\top$。那么 $A_{\\mathrm{w}}^{\\top} A_{\\mathrm{w}} = (WA)^\\top(WA) = A^\\top W^\\top W A = A^\\top W^2 A = A^\\top (A^\\top)^{-1} A^{-1} A = (A^\\top (A^\\top)^{-1}) (A^{-1} A) = I_2 I_2 = I_2$。这证实了 $A_w$ 的列是标准正交的。\n\n对于白化情况，对偶凭证 $z_{\\mathrm{w}}$ 被选择为与白化后的测量向量 $y_{\\mathrm{w}} = W y$ 共线。由于 $y=a_1$，我们有 $y_{\\mathrm{w}} = W a_1 = a_{1,\\mathrm{w}}$。所以，$z_{\\mathrm{w}} = d y_{\\mathrm{w}} = d a_{1,\\mathrm{w}}$，其中 $d \\in \\mathbb{R}$ 是某个标量。$d$ 的值由条件 $a_{1,\\mathrm{w}}^{\\top} z_{\\mathrm{w}} = 1$ 确定。\n代入 $z_{\\mathrm{w}}$ 的表达式：\n$$\na_{1,\\mathrm{w}}^{\\top} (d a_{1,\\mathrm{w}}) = 1\n$$\n$$\nd (a_{1,\\mathrm{w}}^{\\top} a_{1,\\mathrm{w}}) = 1\n$$\n$$\nd \\|a_{1,\\mathrm{w}}\\|_2^2 = 1\n$$\n由于 $A_{\\mathrm{w}}$ 的列是单位范数的，$\\|a_{1,\\mathrm{w}}\\|_2^2 = 1$，这得到 $d = 1$。\n因此，对偶凭证是 $z_{\\mathrm{w}} = a_{1,\\mathrm{w}}$。\n\n白化下的裕度 $m_{\\mathrm{w}}$ 定义为 $m_{\\mathrm{w}} = 1 - |a_{2,\\mathrm{w}}^{\\top} z_{\\mathrm{w}}|$。代入 $z_{\\mathrm{w}} = a_{1,\\mathrm{w}}$：\n$$\nm_{\\mathrm{w}} = 1 - |a_{2,\\mathrm{w}}^{\\top} a_{1,\\mathrm{w}}|\n$$\n由于 $A_{\\mathrm{w}}$ 的列是标准正交的，我们有 $a_{2,\\mathrm{w}}^{\\top} a_{1,\\mathrm{w}} = 0$。因此：\n$$\nm_{\\mathrm{w}} = 1 - |0| = 1\n$$\n\n最后，我们计算改进因子 $R(\\rho) = m_{\\mathrm{w}} / m_{\\mathrm{n}}$。\n使用推导出的 $m_{\\mathrm{w}}$ 和 $m_{\\mathrm{n}}$ 的表达式：\n$$\nR(\\rho) = \\frac{1}{1 - |\\rho|}\n$$\n问题中说明 $0  |\\rho|  1$，这确保了 $m_{\\mathrm{n}}$ 是正数且小于 1，因此 $R(\\rho)$ 是良定义的并且大于 1。", "answer": "$$\n\\boxed{\\frac{1}{1 - |\\rho|}}\n$$", "id": "3459939"}, {"introduction": "在掌握了算法分析和矩阵性质之后，我们将更进一步，从分析给定的测量模型转向主动地设计最优的测量方案。本练习将理论与实际应用相结合，模拟了一个为偏微分方程（PDE）系统进行状态监测的真实场景，其中系统状态在小波基下是稀疏的。你将需要实现一个贪婪算法，基于D-最优性准则来选择最佳的传感器布局，从而最大化观测信息。这个综合性实践将带你走完从测量设计、信号采集到最终重建与评估的全过程，完美连接了理论与工程实践。[@problem_id:3459917]", "problem": "考虑一个均匀空间网格上与偏微分方程 (PDE) 相关的一维离散化状态。设该离散化状态由向量 $x \\in \\mathbb{R}^n$ 表示。假设一个由点态传感器定义的线性测量模型 $y = A x$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个传感器布局矩阵，其每行只有一个非零元素，用于选择 $x$ 的 $m$ 个不同条目。假设 $x$ 在一个正交 Haar 小波基 $W \\in \\mathbb{R}^{n \\times n}$ 中具有稀疏表示，$x = W^\\top \\alpha$，其中 $\\alpha \\in \\mathbb{R}^n$ 是 K-稀疏的，其支撑集为 $S \\subset \\{0,1,\\dots,n-1\\}$，且 $|S| = K$。Haar 小波系数的排序定义如下：索引 $0$ 对应最终的尺度系数，随后是细节系数，从最粗尺度到最细尺度，即 $[c_{\\text{final}}, d_{\\text{coarsest}}, \\dots, d_{\\text{finest}}]$；对于 $n = 2^L$，共有 $L$ 个细节层级，其长度分别为 $1,2,4,\\dots,2^{L-1}$。\n\n从测量模型和稀疏性模型出发，构建一个基于线性模型信息论最优设计的、有原则的贪婪传感器选择策略。具体而言，对于一个固定的支撑集 $S$，令 $M_S = A W^\\top[:, S] \\in \\mathbb{R}^{m \\times K}$ 表示将 $\\alpha_S \\in \\mathbb{R}^K$ 映射到 $y$ 的子矩阵。考虑方差为 $\\sigma^2$ 的加性高斯白噪声 (AWGN)，并回想在线性高斯模型下，关于 $\\alpha_S$ 的 Fisher 信息与 $M_S^\\top M_S$ 成正比。为确保数值稳定性，使用一个正则化信息矩阵 $G = M_S^\\top M_S + \\lambda I_K$，其中 $\\lambda > 0$，且 $I_K$ 是 $K \\times K$ 的单位矩阵。贪婪选择应逐行构建传感器索引集，以最大化 $G$ 的行列式（行列式最优性，也称为 D-最优性），这在 AWGN 条件下减小了 $\\alpha_S$ 估计器的协方差体积。\n\n您的任务是在一个程序中实现以下组件：\n- 通过对每个标准基向量应用变换来形成 $W$ 的列，从而构建与上述系数排序一致的正交 Haar 小波变换矩阵 $W \\in \\mathbb{R}^{n \\times n}$。\n- 对于给定的支撑集 $S$，实现一个贪婪的 D-最优传感器选择算法，该算法选择 $m$ 个不同的传感器位置（$A$ 的行索引），以最大化 $\\log \\det(G)$，其中 $G = M_S^\\top M_S + \\lambda I_K$ 是选择 $m$ 个传感器后的矩阵。\n- 选择传感器后，使用奇异值分解 (SVD) 计算最小奇异值 $\\sigma_{\\min}(M_S)$ 并报告它。\n- 模拟一个在 $S$ 上支撑的 K-稀疏系数向量 $\\alpha$，其非零项从标准正态分布中抽取，形成 $x = W^\\top \\alpha$，计算测量值 $y = A x$，并通过求解 $\\min_{\\hat{\\alpha}_S} \\|M_S \\hat{\\alpha}_S - y\\|_2$ 使用最小二乘法 (LS) 估计 $\\alpha_S$。报告相对重构误差 $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$。\n- 同时报告贪婪选择后 $\\log \\det(G)$ 的最终值。\n\n设计选择必须从与压缩感知和线性测量模型相关的基本原理出发进行论证。问题陈述中不得引入任何快捷公式；上述目标定义是唯一允许的明确目标。\n\n使用以下参数值的测试套件。在每种情况下，$A$ 选择由贪婪算法指定的行，$W$ 是大小为 $n$ 的 Haar 矩阵，支撑集 $S$ 被明确给出：\n- 情况 1：$n = 16$, $K = 3$, $m = 6$, $S = \\{1,5,10\\}$, $\\lambda = 10^{-6}$，随机种子 $123$。\n- 情况 2：$n = 16$, $K = 4$, $m = 4$（$m=K$ 的边界情况），$S = \\{2,4,9,12\\}$, $\\lambda = 10^{-6}$，随机种子 $7$。\n- 情况 3：$n = 32$, $K = 4$, $m = 8$, $S = \\{1,3,5,20\\}$, $\\lambda = 10^{-6}$，随机种子 $42$。\n\n对于每种情况，您的程序必须输出一个列表，包含：\n- 所选的传感器索引，作为一个升序排列的整数列表。\n- 最小奇异值 $\\sigma_{\\min}(M_S)$，作为一个浮点数。\n- 相对重构误差，作为一个浮点数。\n- 最终的 $\\log \\det(G)$ 值，作为一个浮点数。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，每个案例的结果都格式化为其自己的方括号逗号分隔列表，不含空格。例如，输出必须是 $[ [\\text{case1}], [\\text{case2}], [\\text{case3}] ]$ 的形式，具体为 $[[\\text{indices},\\sigma_{\\min},\\text{error},\\log\\det],\\dots]$，行中任何地方都不能有空格。", "solution": "问题陈述在稀疏恢复的最优实验设计领域（压缩感知的一个子领域）提出了一个定义明确的任务。它要求实现一种基于D-最优性原理的贪婪传感器选择算法，并将其应用于一个在Haar小波域中稀疏的状态的线性测量模型。该问题在科学上是合理的，内容自洽，且算法上已有明确规定。所有参数都已提供，目标也是可量化的。因此，该问题被认为是有效的，可以构建解决方案。\n\n解决方案将以一种有原则的、循序渐进的方式进行开发。\n\n**1. 正交Haar小波变换矩阵 ($W$)**\n问题规定状态向量 $x \\in \\mathbb{R}^n$ 在一个正交Haar小波基中具有稀疏表示。该关系由 $x = W^\\top \\alpha$ 给出，其中 $\\alpha \\in \\mathbb{R}^n$ 是K-稀疏系数向量，$W \\in \\mathbb{R}^{n \\times n}$ 是正交小波变换矩阵。由此可得，其逆关系为 $\\alpha = W x$，因为 $W$ 是正交的，意味着 $W^{-1} = W^\\top$。\n\n矩阵 $W$ 的构建方式是，其第 $j$ 行 $W_{j,:}$ 是第 $j$ 个Haar系数的合成基向量，其第 $i$ 列 $W_{:,i}$ 是第 $i$ 个标准基向量 $e_i$ 的Haar系数向量。问题要求通过对每个 $e_i$ 应用变换来构建 $W$ 的列。\n\n指定的系数排序是：索引 $0$ 为最终的尺度系数，随后是细节系数，从最粗尺度到最细尺度。对于 $n=2^L$，这对应于一个结构为 $[c_0, d_0, (d_{1,0}, d_{1,1}), \\dots, (d_{L-1,0}, \\dots, d_{L-1, 2^{L-1}-1})]$ 的系数向量。这是一种标准的“金字塔”排序。\n\n一维正交Haar小波变换通过迭代计算均值和差值来实现。对于长度为 $2p$ 的向量段，我们计算 $p$ 个均值和 $p$ 个差值。一对值 $(a, b)$ 的均值为 $(a+b)/\\sqrt{2}$，差值为 $(a-b)/\\sqrt{2}$。因子 $1/\\sqrt{2}$ 确保了正交性。此过程递归地应用于均值向量，直到只剩下一个尺度系数为止。\n\n**2. 通过D-最优性进行贪婪传感器选择**\n问题的核心是从 $n$ 个可能的位置中选择 $m$ 个传感器位置。每个传感器对应一个点态测量，因此在位置 $p$ 选择一个传感器等同于选择单位矩阵的第 $p$ 行来构成测量矩阵 $A$ 的一行。\n测量模型为 $y=Ax$。代入稀疏模型，我们得到 $y = A W^\\top \\alpha$。由于 $\\alpha$ 在已知支撑集 $S$ 上是K-稀疏的，我们可以写成 $y = (A W^\\top[:,S]) \\alpha_S = M_S \\alpha_S$，其中 $W^\\top[:,S]$ 是 $W^\\top$ 中由 $S$ 索引的列构成的子矩阵，$\\alpha_S \\in \\mathbb{R}^K$ 包含非零系数。矩阵 $M_S \\in \\mathbb{R}^{m \\times K}$ 将非零系数映射到测量值。\n\n传感器布局的质量通过其促进 $\\alpha_S$ 精确估计的能力来评估。对于带有加性高斯白噪声的线性模型，关于 $\\alpha_S$ 的Fisher信息矩阵与 $M_S^\\top M_S$ 成正比。D-最优性旨在最大化该信息矩阵的行列式，这在几何上等同于最小化 $\\alpha_S$ 的最小二乘估计器的置信椭球体积。\n\n我们使用一个正则化信息矩阵 $G = M_S^\\top M_S + \\lambda I_K$，其中 $\\lambda > 0$ 是一个正则化参数，确保 $G$ 始终是良态且可逆的。目标是选择 $A$ 的 $m$ 行（即传感器位置），以最大化 $\\log \\det(G)$。\n\n我们采用贪婪算法来解决这个组合优化问题。从一个空的传感器集合开始，我们迭代地添加能使 $\\log \\det(G)$ 边际增益最大的传感器。设 $\\mathcal{P}_{k-1}$ 是已选择的 $k-1$ 个传感器索引的集合。对应的信息矩阵是 $G_{k-1}$。为了选择第 $k$ 个传感器，我们评估每个候选位置 $p \\notin \\mathcal{P}_{k-1}$。添加传感器 $p$ 对应于将行向量 $v_p = (W^\\top[:,S])_{p,:}$ 附加到当前的测量子矩阵上。新的信息矩阵变为 $G_k = G_{k-1} + v_p^\\top v_p$。我们选择使 $\\log\\det(G_k)$ 最大化的传感器 $p^*$。这个过程重复 $m$ 次。使用 $\\log \\det$ 可以提高数值稳定性。\n\n**3. 重构与性能评估**\n在选择了 $m$ 个传感器位置后，我们形成最终的测量矩阵 $M_S \\in \\mathbb{R}^{m \\times K}$。\n该矩阵的质量部分由其奇异值来表征。最小奇异值 $\\sigma_{\\min}(M_S)$ 是对噪声最坏情况放大的度量，并与矩阵的条件数有关。一个小的 $\\sigma_{\\min}(M_S)$ 表明 $M_S$ 接近于秩亏，这会损害估计的稳定性。\n\n为了测试重构性能，我们首先模拟一个基准真相 (ground truth)。我们创建一个K-稀疏系数向量 $\\alpha$，其在给定支撑集 $S$ 上的非零项从标准正态分布中抽取。物理状态被合成为 $x=W^\\top\\alpha$。然后计算无噪声的测量值 $y = Ax$。\n\n通过求解线性最小二乘问题来从测量值 $y$ 中估计系数向量 $\\alpha_S$：\n$$ \\hat{\\alpha}_S = \\arg\\min_{\\beta \\in \\mathbb{R}^K} \\| M_S \\beta - y \\|_2^2 $$\n其解由 $\\hat{\\alpha}_S = (M_S^\\top M_S)^{-1} M_S^\\top y$ 给出。在数值上，最好使用稳健的方法来求解，例如基于QR分解或SVD的方法，正如 `numpy.linalg.lstsq` 中所实现的。\n\n性能由相对重构误差 $\\|\\hat{\\alpha}_S - \\alpha_S\\|_2 / \\|\\alpha_S\\|_2$ 来量化。\n\n最终算法的流程是：首先构建Haar矩阵 $W$，然后运行 $m$ 步的贪婪选择循环。在每一步中，测试每个可用的传感器位置，并选择使更新后的正则化信息矩阵 $G$ 的对数行列式最大化的位置。最后，在选择完成后，计算并报告所需的指标——所选传感器索引列表、$\\sigma_{\\min}(M_S)$、重构误差以及最终的 $\\log\\det(G)$。", "answer": "```python\nimport numpy as np\n\ndef _haar_1d_transform(v):\n    \"\"\"\n    Computes the 1D orthonormal Haar wavelet transform of a vector.\n    The coefficient ordering is: final scaling coefficient, followed by\n    detail coefficients from the coarsest to the finest scale.\n    \"\"\"\n    n = len(v)\n    if n == 1:\n        return v.astype(np.float64)\n    \n    if (n  (n - 1) != 0) or n == 0:\n        raise ValueError(\"Input vector length must be a power of 2.\")\n\n    temp_v = v.astype(np.float64)\n    coeffs = np.zeros(n, dtype=np.float64)\n    \n    current_len = n\n    while current_len > 1:\n        next_len = current_len // 2\n        averages = (temp_v[0:current_len:2] + temp_v[1:current_len:2]) / np.sqrt(2)\n        details = (temp_v[0:current_len:2] - temp_v[1:current_len:2]) / np.sqrt(2)\n        \n        # Place details in the second half of the current segment in the output array.\n        # This naturally orders them from finest to coarsest as current_len decreases.\n        # Here we place them to match the problem statement.\n        coeffs[next_len:current_len] = details\n        temp_v[:next_len] = averages\n        current_len = next_len\n\n    coeffs[0] = temp_v[0]\n    return coeffs\n\ndef construct_haar_matrix(n):\n    \"\"\"\n    Constructs the orthonormal Haar wavelet transform matrix W of size n x n.\n    The columns of W are the transforms of the standard basis vectors.\n    \"\"\"\n    W = np.zeros((n, n), dtype=np.float64)\n    for i in range(n):\n        e_i = np.zeros(n)\n        e_i[i] = 1.0\n        W[:, i] = _haar_1d_transform(e_i)\n    return W\n\ndef solve():\n    \"\"\"\n    Main function to solve the sensor selection problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 16, 'K': 3, 'm': 6, 'S': {1, 5, 10}, 'lambda': 1e-6, 'seed': 123},\n        {'n': 16, 'K': 4, 'm': 4, 'S': {2, 4, 9, 12}, 'lambda': 1e-6, 'seed': 7},\n        {'n': 32, 'K': 4, 'm': 8, 'S': {1, 3, 5, 20}, 'lambda': 1e-6, 'seed': 42},\n    ]\n\n    results_for_all_cases = []\n\n    for case in test_cases:\n        n, K, m, S_set, lambda_reg, seed = case['n'], case['K'], case['m'], case['S'], case['lambda'], case['seed']\n        S = list(S_set)\n\n        # 1. Construct Haar wavelet matrix W\n        W = construct_haar_matrix(n)\n        Psi_S = W.T[:, S]\n\n        # 2. Greedy D-optimal sensor selection\n        selected_indices = []\n        available_indices = list(range(n))\n        \n        # Initialize regularized information matrix G\n        G = lambda_reg * np.eye(K)\n        \n        current_log_det = np.linalg.slogdet(G)[1]\n\n        for _ in range(m):\n            best_p = -1\n            best_log_det = -np.inf\n            \n            for p in available_indices:\n                v_p = Psi_S[p, :]\n                \n                # Rank-1 update to G\n                G_candidate = G + np.outer(v_p, v_p)\n                \n                # Using slogdet for numerical stability\n                sign, log_det_candidate = np.linalg.slogdet(G_candidate)\n                \n                if sign > 0 and log_det_candidate > best_log_det:\n                    best_log_det = log_det_candidate\n                    best_p = p\n\n            selected_indices.append(best_p)\n            available_indices.remove(best_p)\n            \n            # Update G and log_det for the next iteration\n            v_best = Psi_S[best_p, :]\n            G += np.outer(v_best, v_best)\n            current_log_det = best_log_det\n        \n        final_log_det = current_log_det\n        selected_indices.sort()\n\n        # 3. Construct final M_S and compute minimum singular value\n        M_S = Psi_S[selected_indices, :]\n        if M_S.shape[0] > 0:\n            singular_values = np.linalg.svd(M_S, compute_uv=False)\n            sigma_min = singular_values.min()\n        else:\n            sigma_min = 0.0\n\n        # 4. Simulate and reconstruct\n        np.random.seed(seed)\n        alpha = np.zeros(n)\n        alpha_S_true = np.random.randn(K)\n        alpha[S] = alpha_S_true\n        \n        x = W.T @ alpha\n        \n        # Measurement matrix A is implicit\n        y = x[selected_indices]\n        \n        # Estimate alpha_S via Least Squares\n        alpha_S_hat, _, _, _ = np.linalg.lstsq(M_S, y, rcond=None)\n        \n        # Compute relative reconstruction error\n        norm_true = np.linalg.norm(alpha_S_true)\n        if norm_true == 0:\n            error = 0.0 if np.linalg.norm(alpha_S_hat) == 0 else np.inf\n        else:\n            error = np.linalg.norm(alpha_S_hat - alpha_S_true) / norm_true\n\n        # Format results for the current case\n        indices_str = f\"[{','.join(map(str, selected_indices))}]\"\n        case_result_str = f\"[{indices_str},{sigma_min},{error},{final_log_det}]\"\n        results_for_all_cases.append(case_result_str)\n\n    # Final print statement in the exact required format with no spaces\n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```", "id": "3459917"}]}