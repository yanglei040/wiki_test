## 引言
在大数据时代，处理[高维数据](@entry_id:138874)集已成为现代统计学、机器学习和信号处理领域的一项核心挑战。LASSO（Least Absolute Shrinkage and Selection Operator）作为一种强大的[正则化技术](@entry_id:261393)，因其能够同时进行系数收缩和[变量选择](@entry_id:177971)而备受青睐，从而在海量特征中构建出稀疏且易于解释的模型。然而，LASSO[目标函数](@entry_id:267263)中的$\ell_1$范数惩罚项在原点处不可微，这为传统的[基于梯度的优化](@entry_id:169228)算法带来了难题。

[循环坐标](@entry_id:166220)下降（Cyclic Coordinate Descent, CCD）为此提供了一个极其优雅、高效且易于实现的解决方案。它巧妙地将一个复杂的[多变量优化](@entry_id:186720)[问题分解](@entry_id:272624)为一系列简单的一维子问题，并通过迭代更新每个坐标直至收敛。这种看似简单的方法，在理论和实践上都展现出强大的威力。本文旨在为读者提供一个关于[循环坐标下降法](@entry_id:178957)求解LASSO的全面而深入的指南。

在接下来的内容中，我们将分三个章节系统地展开讨论。**第一章“原则与机制”**将深入剖析算法的数学基础，从坐标[最小化原理](@entry_id:169952)出发，推导出核心的[软阈值](@entry_id:635249)更新规则，并探讨高效实现、实践考量（如[标准化](@entry_id:637219)）以及收敛性理论。**第二章“应用与交叉学科联系”**将展示该算法在更广阔舞台上的应用，包括计算[LASSO](@entry_id:751223)[解路径](@entry_id:755046)、在信号处理和数据分析中的具体实例、面向[可扩展性](@entry_id:636611)的高级算法增强技术，以及对Group [LASSO](@entry_id:751223)、非凸模型乃至[联邦学习](@entry_id:637118)等前沿领域的扩展。最后，**第三章“动手实践”**将提供一系列精心设计的练习，帮助读者通过实际操作来巩固理论知识，加深对算法行为和特性的理解。通过这一结构化的学习路径，您将全面掌握[循环坐标下降法](@entry_id:178957)这一现代[稀疏优化](@entry_id:166698)的基石。

## 原则与机制

本章深入探讨[循环坐标](@entry_id:166220)下降（Cyclic Coordinate Descent, CCD）算法在求解[LASSO](@entry_id:751223)问题中的核心原理与运行机制。我们将从最优化理论的基本原则出发，系统地推导其更新规则，分析其[计算效率](@entry_id:270255)，并探讨影响其性能的关键因素，如特征标准化和问题本身的[条件数](@entry_id:145150)。最后，我们将建立其收敛性的理论基础，并阐明[算法终止](@entry_id:143996)的判断标准。

### 坐标[最小化原理](@entry_id:169952)

[LASSO](@entry_id:751223) (Least Absolute Shrinkage and Selection Operator) 旨在求解以下带 $\ell_1$ 范数正则化的凸[优化问题](@entry_id:266749)：

$$
\min_{\beta \in \mathbb{R}^{p}} \left\{ F(\beta) = \frac{1}{2}\|y - X\beta\|_{2}^{2} + \lambda \|\beta\|_{1} \right\}
$$

其中，$y \in \mathbb{R}^{n}$ 是响应向量，$X \in \mathbb{R}^{n \times p}$ 是[设计矩阵](@entry_id:165826)，$\beta \in \mathbb{R}^{p}$ 是待估计的系数向量，$\lambda > 0$ 是正则化参数。$\|\beta\|_{1} = \sum_{j=1}^{p} |\beta_j|$ 是系数向量的 $\ell_1$ 范数，它起到了促使解稀疏化的作用。

由于 $\ell_1$ 范数在坐标轴上不可微，这使得传统的[基于梯度的优化](@entry_id:169228)算法（如[梯度下降法](@entry_id:637322)）无法直接应用。[坐标下降法](@entry_id:175433)为此类问题提供了一个简洁而强大的解决方案。其核心思想是将一个复杂的[多变量优化](@entry_id:186720)[问题分解](@entry_id:272624)为一系列简单的[一维优化](@entry_id:635076)问题。具体而言，算法循环遍历每一个坐标（即系数 $\beta_j$），在固定所有其他坐标 $\beta_k$ ($k \neq j$) 的前提下，仅针对当前坐标 $\beta_j$ 最小化目标函数。

让我们来推导这个一维子问题的解。当固定除 $\beta_j$ 之外的所有系数时，目标函数 $F(\beta)$ 中与 $\beta_j$ 相关的部分可以被分离出来。首先，我们将[残差平方和](@entry_id:174395)（Residual Sum of Squares, RSS）项展开：

$$
\|y - X\beta\|_{2}^{2} = \left\|y - \sum_{k=1}^{p} X_{k} \beta_k\right\|_{2}^{2} = \left\| \left(y - \sum_{k \neq j} X_{k} \beta_k\right) - X_j \beta_j \right\|_{2}^{2}
$$

这里，$X_k$ 是矩阵 $X$ 的第 $k$ 列。我们定义**部分残差 (partial residual)** 为 $r_{-j} = y - \sum_{k \neq j} X_k \beta_k$，它表示剔除第 $j$ 个特征影响后剩余的响应。于是，RSS项简化为 $\|r_{-j} - X_j \beta_j\|_{2}^{2}$。同时，$\ell_1$ 范数项可以分解为 $|\beta_j|$ 和一个与 $\beta_j$ 无关的常数 $\sum_{k \neq j} |\beta_k|$。

因此，关于 $\beta_j$ 的一维最小化问题是：

$$
\hat{\beta}_j = \arg\min_{\beta_j} \left( \frac{1}{2}\|r_{-j} - X_j \beta_j\|_{2}^{2} + \lambda |\beta_j| \right)
$$

展开二次项，我们得到一个关于 $\beta_j$ 的函数：

$$
L_j(\beta_j) = \frac{1}{2}(X_j^\top X_j)\beta_j^2 - (X_j^\top r_{-j})\beta_j + \lambda|\beta_j| + \text{const.}
$$

该函数是凸的，由一个二次函数和一个[绝对值函数](@entry_id:160606)相加而成。由于[绝对值函数](@entry_id:160606)在原点处不可微，我们使用[次梯度](@entry_id:142710)（subgradient）微积分来求解。最优解 $\hat{\beta}_j$ 必须满足次梯度包含零的条件，即 $0 \in \partial L_j(\hat{\beta}_j)$。$L_j(\beta_j)$ 的[次微分](@entry_id:175641)为：

$$
\partial L_j(\beta_j) = (X_j^\top X_j)\beta_j - (X_j^\top r_{-j}) + \lambda \partial(|\beta_j|)
$$

其中，$\partial(|\beta_j|)$ 在 $\beta_j > 0$ 时为 $\{1\}$，在 $\beta_j  0$ 时为 $\{-1\}$，在 $\beta_j = 0$ 时为区间 $[-1, 1]$。设 $a_j = X_j^\top X_j = \|X_j\|_2^2$ 和 $c_j = X_j^\top r_{-j}$，[最优性条件](@entry_id:634091)可写作 $c_j - a_j \hat{\beta}_j \in \lambda \partial(|\hat{\beta}_j|)$。通过分情况讨论 $\hat{\beta}_j$ 的符号，我们可以得到解析解：

1.  若 $\hat{\beta}_j > 0$，则 $c_j - a_j \hat{\beta}_j = \lambda$，解得 $\hat{\beta}_j = \frac{c_j - \lambda}{a_j}$。此解有效的前提是 $c_j > \lambda$。
2.  若 $\hat{\beta}_j  0$，则 $c_j - a_j \hat{\beta}_j = -\lambda$，解得 $\hat{\beta}_j = \frac{c_j + \lambda}{a_j}$。此解有效的前提是 $c_j  -\lambda$。
3.  若 $\hat{\beta}_j = 0$，则 $c_j \in [-\lambda, \lambda]$，即 $|c_j| \le \lambda$。

综合这三种情况，我们得到[LASSO](@entry_id:751223)的[坐标下降](@entry_id:137565)更新规则，即著名的**[软阈值算子](@entry_id:755010) (soft-thresholding operator)** $S_{\lambda}(\cdot)$：

$$
\hat{\beta}_j = \frac{1}{a_j} S_{\lambda}(c_j) = \frac{1}{\|X_j\|_2^2} \text{sign}(X_j^\top r_{-j}) \max(|X_j^\top r_{-j}| - \lambda, 0)
$$

这个公式是[循环坐标](@entry_id:166220)下降算法的基石。它表明，对每个坐标的更新都可被看作是对普通[最小二乘估计](@entry_id:262764)（即 $c_j/a_j$）进行的一种“收缩”和“筛选”。当特征与部分残差的相关性（由 $c_j$ 度量）的[绝对值](@entry_id:147688)足够大（超过阈值 $\lambda$）时，系数被收缩；否则，系数被直接设为零，从而实现了[特征选择](@entry_id:177971)。[@problem_id:3442159]

### [循环坐标](@entry_id:166220)下降算法与高效实现

基于上述坐标[最小化原理](@entry_id:169952)，[循环坐标](@entry_id:166220)下降（CCD）算法的流程非常直观：从一个初始系数向量 $\beta^{(0)}$（通常是全[零向量](@entry_id:156189)）开始，算法重复地、按顺序地（例如，从 $j=1$ 到 $p$）对每个坐标应用[软阈值](@entry_id:635249)更新规则，直到整个系数向量收敛。

一个完整的循环（或称为一个“轮次”，epoch）包括对所有 $p$ 个坐标各进行一次更新。在更新 $\beta_j$ 时，会用到其他所有系数的最新值。例如，当更新 $\beta_j$ 时，$\beta_1, \dots, \beta_{j-1}$ 已经在[本轮](@entry_id:169326)次中被更新，而 $\beta_{j+1}, \dots, \beta_p$ 仍然是上一轮次的值。

为了具体说明这一过程，我们考虑以下实例：
$$
X = \begin{pmatrix}
1  0  1 \\
0  1  1 \\
1  1  0
\end{pmatrix}, \quad
y = \begin{pmatrix}
5 \\
-1 \\
2
\end{pmatrix}, \quad
\lambda = 1.
$$
我们从 $\beta^{(0)} = (0, 0, 0)^{\top}$ 开始，按 $j=1, 2, 3$ 的顺序进行第一轮更新。

**第1步：更新 $\beta_1$**
当前 $\beta = (0, 0, 0)^{\top}$。部分残差 $r_{-1} = y - X_2\beta_2 - X_3\beta_3 = y$。
计算相关项：$c_1 = X_1^\top r_{-1} = X_1^\top y = 1(5) + 0(-1) + 1(2) = 7$。
计算范数项：$a_1 = \|X_1\|_2^2 = 1^2+0^2+1^2=2$。
由于 $c_1=7 > \lambda=1$，应用[软阈值](@entry_id:635249)规则：
$\beta_1^{(1)} = \frac{c_1 - \lambda}{a_1} = \frac{7-1}{2} = 3$。
更新后，系数向量为 $\beta = (3, 0, 0)^{\top}$。

**第2步：更新 $\beta_2$**
当前 $\beta = (3, 0, 0)^{\top}$。部分残差 $r_{-2} = y - X_1\beta_1 - X_3\beta_3 = y - 3X_1$。
$r_{-2} = \begin{pmatrix} 5 \\ -1 \\ 2 \end{pmatrix} - 3 \begin{pmatrix} 1 \\ 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 2 \\ -1 \\ -1 \end{pmatrix}$。
计算相关项：$c_2 = X_2^\top r_{-2} = 0(2) + 1(-1) + 1(-1) = -2$。
计算范数项：$a_2 = \|X_2\|_2^2 = 0^2+1^2+1^2=2$。
由于 $c_2=-2  -\lambda=-1$，应用[软阈值](@entry_id:635249)规则：
$\beta_2^{(1)} = \frac{c_2 + \lambda}{a_2} = \frac{-2+1}{2} = -\frac{1}{2}$。
更新后，系数向量为 $\beta = (3, -1/2, 0)^{\top}$。后续将继续更新 $\beta_3$ 并开始新的循环。[@problem_id:3442159]

#### 高效的残差更新

在上述计算中，我们为每个坐标更新都重新计算了部分残差 $r_{-j} = y - \sum_{k \neq j} X_k \beta_k$。如果直接按定义计算，每次更新的成本约为 $\Theta(np)$ 次[浮点运算](@entry_id:749454)，其中 $n$ 是样本数，$p$ 是特征数。这在 $p$ 很大时是极其低效的。

一个关键的优化是维护一个**全局残差 (global residual)** $r = y - X\beta$。部分残差 $r_{-j}$ 可以通过全局残差和一个简单的向量加法得到：
$$
r_{-j} = y - \sum_{k \neq j} X_k \beta_k = (y - \sum_{k=1}^p X_k \beta_k) + X_j \beta_j^{\text{old}}
$$
其中 $\beta_j^{\text{old}}$ 是更新前的值。计算 $X_j^\top r_{-j}$ 只需要 $X_j^\top(r + X_j \beta_j^{\text{old}})$，成本为 $\Theta(n)$。

更进一步，在 $\beta_j$ 从 $\beta_j^{\text{old}}$ 更新到 $\beta_j^{\text{new}}$ 之后，全局残差自身也可以进行高效的**[增量更新](@entry_id:750602) (incremental update)**：
$$
r^{\text{new}} = y - X\beta^{\text{new}} = y - X(\beta^{\text{old}} - (\beta_j^{\text{old}} - \beta_j^{\text{new}})e_j) = r^{\text{old}} + X_j(\beta_j^{\text{old}} - \beta_j^{\text{new}})
$$
这个更新操作仅涉及一次标量-向量乘法和一次[向量加法](@entry_id:155045)，其计算成本为 $\Theta(n)$。[@problem_id:3442178]

对于**稀疏[设计矩阵](@entry_id:165826)** $X$，这种[增量更新](@entry_id:750602)的优势尤为突出。假设 $X$ 以稀疏列格式存储，第 $j$ 列 $X_j$ 的非零元素个数为 $s_j = \text{nnz}(X_j)$。那么，更新 $r$ 只需要对 $s_j$ 个非零位置进行操作，其成本为 $\Theta(s_j)$。相比之下，从头计算残差需要一次稀疏矩阵-向量乘法 $X\beta$ 和一次向量减法，总成本约为 $\Theta(S+n)$，其中 $S = \text{nnz}(X)$ 是矩阵的总非零元素个数。因此，[增量更新](@entry_id:750602)的成本与从头计算的成本之比为 $\frac{\Theta(S+n)}{\Theta(s_j)}$，这通常是一个非常大的数值，凸显了[增量更新](@entry_id:750602)在实践中的极端重要性。[@problem_id:3442199]

此外，当 $p$ 较大时，还可以通过维护相关向量 $c = X^\top r$ 并利用预先计算的[Gram矩阵](@entry_id:148915) $G=X^\top X$ 来进一步加速。每次坐标更新后，相关向量 $c$ 可以通过一次[秩一更新](@entry_id:137543)在 $\Theta(p)$ 时间内完成，避免了每次都重新计算 $X^\top r$ 的 $\Theta(np)$ 开销。[@problem_id:3442178]

### 实践考量：[标准化](@entry_id:637219)与截距项

在实际应用中，我们通常需要处理带**截距项 (intercept)** 的模型，并且特征的**尺度 (scale)** 差异也需要被妥善处理。

#### 截距项的处理

带截距项 $b$ 的[LASSO](@entry_id:751223)模型的[目标函数](@entry_id:267263)为：
$$
\min_{b \in \mathbb{R}, \beta \in \mathbb{R}^p} \frac{1}{2n}\| y - b\mathbf{1} - X\beta \|_2^2 + \lambda \|\beta\|_1
$$
注意，截距项 $b$ 通常不被正则化。在CCD框架下，截距项 $b$ 也被视为一个坐标进行更新。由于它没有 $\ell_1$ 惩罚，其更新规则就是最小化二次损失项，解为当前残差的均值：
$$
b^{\text{new}} \leftarrow \frac{1}{n}\sum_{i=1}^n (y_i - (X\beta)_i) = \bar{y} - \sum_{j=1}^p \bar{x}_j \beta_j
$$
其中 $\bar{y}$ 和 $\bar{x}_j$ 分别是响应和第 $j$ 个特征的均值。这个更新不涉及[软阈值](@entry_id:635249)操作。[@problem_id:3442223]

一个重要的简化是，如果在建模前对数据进行中心化，即让 $\bar{y}=0$ 且所有 $\bar{x}_j=0$，那么最优的截距项总是 $b=0$。因此，在中心化数据上，我们可以省略截距项，直接求解不含截距的LASSO问题。[@problem_id:3442223]

#### 特征标准化的必要性

LASSO的解对特征的尺度非常敏感。回顾坐标更新规则，一个系数 $\beta_j$ 被设为零的条件是 $|X_j^\top r_{-j}| \le \lambda$。这条规则看似对所有特征一视同仁，但实际上并非如此。我们可以将此条件改写为关于一个“标准化”相关性度量的阈值：
$$
\left| \frac{X_j^\top r_{-j}}{\|X_j\|_2^2} \right| \le \frac{\lambda}{\|X_j\|_2^2}
$$
这里的 $\frac{X_j^\top r_{-j}}{\|X_j\|_2^2}$ 可以看作是 $\beta_j$ 的普通[最小二乘估计](@entry_id:262764)。该不等式表明，使得 $\beta_j$ 被压缩至零的“有效阈值”是 $\lambda/\|X_j\|_2^2$，它反比于特征列[向量的范数](@entry_id:154882)平方。这意味着，如果一个特征 $X_j$ 的范数很大，其有效阈值就很小，它的系数就更难被惩罚为零。反之，范数小的特征则会受到更强的惩罚。这种依赖于尺度的惩罚通常是不希望出现的，因为它使得模型的选择结果取决于特征的单位（例如，用米还是厘米度量长度）。[@problem_id:3442236]

为了解决这个问题，标准的做法是在运行[LASSO](@entry_id:751223)之前对特征进行**[标准化](@entry_id:637219) (standardization)**。一个常见的策略是，将每个特征列 $X_j$ 变换为 $X'_j = X_j / s_j$，使得新的特征列具有统一的尺度。一个典型的选择是令所有变换后的列具有单位二范数，即 $\|X'_j\|_2=1$。这可以通过选择缩放因子 $s_j = \|X_j\|_2$ 来实现。[@problem_id:3442236] 经过这样的标准化后，所有坐标的二次曲率 $a'_j = \|X'_j\|_2^2$ 都等于1，因此所有系数在[坐标下降](@entry_id:137565)更新中都面对一个统一的阈值 $\lambda$。

值得强调的是，对特征进行标准化并使用统一的正则化参数 $\lambda$ 求解，其结果**不等价于**在原始未[标准化](@entry_id:637219)数据上使用相同的 $\lambda$ 求解。实际上，它等价于在原始数据上求解一个带有坐标依赖[正则化参数](@entry_id:162917) $\lambda_j = \lambda s_j$ 的加权[LASSO](@entry_id:751223)问题。理解这一点对于解释标准化后的模型系数以及保证不同软件实现之间结果的一致性至关重要。[@problem_id:3442223]

### 收敛性与[最优性条件](@entry_id:634091)

[循环坐标下降法](@entry_id:178957)的一个核心问题是：算法何时停止？以及它的[收敛速度](@entry_id:636873)如何？

#### [最优性条件](@entry_id:634091)与[终止准则](@entry_id:136282)

一个[迭代算法](@entry_id:160288)的自然[终止准则](@entry_id:136282)是在其解足够接近最优解时停止。对于LASSO问题，我们可以通过**[Karush-Kuhn-Tucker (KKT) 条件](@entry_id:176491)**来检验一个解 $\hat{\beta}$ 是否为最优解。[KKT条件](@entry_id:185881)是凸[优化问题](@entry_id:266749)最优解的充要条件。对于LASSO问题，[KKT条件](@entry_id:185881)可以具体表述为：
对于所有 $j=1, \dots, p$：
- 如果 $\hat{\beta}_j \neq 0$，则 $X_j^\top (y - X\hat{\beta}) = \lambda \cdot \text{sign}(\hat{\beta}_j)$。
- 如果 $\hat{\beta}_j = 0$，则 $|X_j^\top (y - X\hat{\beta})| \le \lambda$。

在算法的每次循环后，我们可以计算 $g_j = X_j^\top r = X_j^\top (y - X\beta)$，并检查上述[KKT条件](@entry_id:185881)是否在一定容忍度内满足。如果不满足，则说明当前解尚非最优，迭代应继续。例如，在一个具体问题中，我们可能发现对于某个非零系数 $\beta_j=1$，计算出的 $g_j=3$ 而 $\lambda=2$，这违反了等式条件；或者对于某个零系数 $\beta_k=0$，计算出的 $|g_k|=4$ 而 $\lambda=2$，这违反了不等式条件。只要存在一个坐标不满足[KKT条件](@entry_id:185881)，当前解就不是最优解。[@problem_id:3442173]

此外，还可以通过**[对偶间隙](@entry_id:173383) (duality gap)** 来判断收敛。[LASSO](@entry_id:751223)问题有一个相应的[对偶问题](@entry_id:177454)，强对偶性保证了在最优点上，[原始问题和对偶问题](@entry_id:151869)的目标函数值相等。在迭代过程中，我们可以构造一个对偶[可行解](@entry_id:634783)，并计算原始目标值与对偶目标值之差（即[对偶间隙](@entry_id:173383)）。当此间隙小于一个预设的阈值时，可以认为算法已收敛。[@problem_id:3442173]

#### [收敛速度](@entry_id:636873)分析

[循环坐标下降法](@entry_id:178957)的[收敛速度](@entry_id:636873)受到[设计矩阵](@entry_id:165826) $X$ 的性质的深刻影响。

一个理想的特殊情况是当 $X$ 的列是**标准正交 (orthonormal)** 的，即 $X^\top X = I_p$（单位矩阵）。在这种情况下，$g(\beta) = \frac{1}{2}\|y - X\beta\|_2^2$ 的海森矩阵是单位阵，其等高线是正圆形。对坐标 $\beta_j$ 的更新规则中的部分[残差相关](@entry_id:754268)项 $X_j^\top r_{-j}$ 简化为 $X_j^\top y$，不再依赖于任何其他的系数 $\beta_k$ ($k \neq j$)。这意味着对每个坐标的优化是完全解耦的。因此，从 $\beta=0$ 开始，只需对每个坐标进行一次[软阈值](@entry_id:635249)更新 $\beta_j = S_{\lambda}(X_j^\top y)$，经过一个完整的循环后，算法就能立即达到[全局最优解](@entry_id:175747)。[@problem_id:3442231]

在一般情况下，当 $X$ 的列是相关的，即 $X^\top X$ 非对角时，$g(\beta)$ 的[等高线](@entry_id:268504)是椭圆形。如果 $X^\top X$ **病态 (ill-conditioned)**，即其[条件数](@entry_id:145150)（最大[特征值](@entry_id:154894)与最小特征值之比）很大，那么这个椭圆将被极度拉伸。这导致不同坐标方向上的曲率（即 $\|X_j\|_2^2$）差异巨大。CCD算法在这种情况下会表现出“之”字形收敛行为，需要在狭长的山谷中进行大量微小的迭代步才能接近最小值，从而导致[收敛速度](@entry_id:636873)非常缓慢。因此，通过特征标准化来均衡各坐标的曲率，可以看作是一种对角[预处理](@entry_id:141204)，能有效改善收敛性能。[@problem_id:3442198]

我们可以更精确地量化收敛速度。在某些简化条件下，例如在一个二维问题中，如果迭代路径始终保持在两个坐标都为正的区域，LASSO问题就退化为一个无约束的二次规划问题。此时，CCD算法等价于[求解线性方程组](@entry_id:169069) $G\beta = c'$ 的高斯-赛德尔（Gauss-Seidel）方法，其中 $G=X^\top X$。其误差的衰减由一个[迭代矩阵](@entry_id:637346)的谱半径（最大[特征值](@entry_id:154894)的[绝对值](@entry_id:147688)）决定。例如，对于 $G = \begin{pmatrix} 4  1 \\ 1  3 \end{pmatrix}$，可以推导出[迭代矩阵](@entry_id:637346)的[谱半径](@entry_id:138984)为 $1/12$，这意味着每完成一轮循环，误差大约会缩小到原来的 $1/12$。[@problem_id:3442180]

对于更一般的情形，现代优化理论使用**受限强[凸性](@entry_id:138568) (Restricted Strong Convexity, RSC)** 等概念来分析高维问题。在满足RSC假设下，可以证明CCD算法具有[线性收敛](@entry_id:163614)性，即目标函数值的误差 $f(\beta^k) - f(\beta^\star)$ 按一个固定的比例 $\rho  1$ 衰减。这个收敛因子 $\rho$ 的一个[上界](@entry_id:274738)可以表示为：
$$
\rho = 1 - \frac{\alpha_{\mathrm{RSC}}}{\sum_{j=1}^{p} L_{j}}
$$
其中 $\alpha_{\mathrm{RSC}}$ 是受限强凸性常数，它度量了损失函数在特定方向上的曲率下界；$L_j = \|X_j\|_2^2$ 是坐标级别的[利普希茨常数](@entry_id:146583)（即曲率）。这个公式清晰地表明，更强的[凸性](@entry_id:138568)（更大的 $\alpha_{\mathrm{RSC}}$）或更小的坐标曲率总和（更好的条件）都会导致 $\rho$ 减小，从而获得更快的收敛速度。[@problem_id:3442181]