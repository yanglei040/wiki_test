{"hands_on_practices": [{"introduction": "在信任一个算法之前，我们必须深刻理解它的目标。对于LASSO问题，这个目标是找到一个满足Karush-Kuhn-Tucker（KKT）条件的点。本练习提供了一个动手实践的机会，让你为一个候选解手动验证这些条件，从而具体地理解一个解满足最优性意味着什么，以及次梯度在实践中如何运作 [@problem_id:3442175]。", "problem": "考虑最小绝对收缩和选择算子 (LASSO) 问题，其目标是最小化目标函数\n$$\nF(\\beta) \\;=\\; \\frac{1}{2}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个设计矩阵，$y \\in \\mathbb{R}^{n}$ 是一个响应向量，$\\beta \\in \\mathbb{R}^{p}$ 是参数向量，$\\lambda  0$ 是一个正则化参数。$\\ell_{1}$ 范数的次梯度按分量定义为 $s_{j} \\in \\partial |\\beta_{j}|$，其中如果 $\\beta_{j} \\neq 0$，则 $s_{j} = \\operatorname{sign}(\\beta_{j})$；如果 $\\beta_{j} = 0$，则 $s_{j} \\in [-1,1]$。凸复合优化的 Karush–Kuhn–Tucker (KKT) 条件要求光滑项的梯度与非光滑项的次梯度之和具有平稳性，并满足次梯度可行性。\n\n根据这些核心定义，为一个小的合成实例数值验证 KKT 平稳性条件。令\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0  2 \\\\\n0  1  -1 \\\\\n1  1  0\n\\end{pmatrix}, \n\\qquad\ny \\;=\\; \\begin{pmatrix}\n3 \\\\ 0 \\\\ 1\n\\end{pmatrix},\n\\qquad\n\\lambda \\;=\\; 1,\n\\qquad\n\\hat{\\beta} \\;=\\; \\begin{pmatrix}\n1 \\\\ 0 \\\\ \\tfrac{1}{2}\n\\end{pmatrix}.\n$$\n仅使用上述基本定义，完成以下任务。首先，推导针对此 LASSO 问题的 KKT 平稳性条件：用在 $\\hat{\\beta}$ 处计算的二次损失的梯度和 $\\ell_{1}$ 范数的次梯度来表示它。其次，计算残差向量 $r = X\\hat{\\beta} - y$、梯度 $g = X^{\\top} r$，以及一个次梯度向量 $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$，该向量在满足次梯度可行性约束的条件下，逐分量地最小化 KKT 平稳性残差。第三，数值验证 KKT 平稳性条件在 $\\hat{\\beta}$ 处是否成立，并根据在循环坐标下降中本应执行的一次坐标最小化操作，来解释任何违规之处。特别地，将任何违规的坐标与其从基本原理推导出的单步坐标更新联系起来。\n\n最后，令 $t = g + \\lambda s$ 表示在 $\\hat{\\beta}$ 处，对于您优化选择的 $s$ 的 KKT 平稳性残差向量。将无穷范数 $\\|t\\|_{\\infty}$ 的值作为您的最终答案。请以精确分数形式给出您的最终答案（不要四舍五入）。", "solution": "LASSO 目标函数由 $F(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda\\|\\beta\\|_{1}$ 给出。Karush–Kuhn–Tucker (KKT) 条件为点 $\\hat{\\beta}$ 成为最小化点提供了必要和充分条件。平稳性条件要求零向量是 $F(\\beta)$ 在 $\\hat{\\beta}$ 处的次微分的一个元素。目标函数是一个可微项 $L(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$ 和一个不可微凸项 $R(\\beta) = \\lambda\\|\\beta\\|_{1}$ 的和。和的次微分是可微部分的梯度与不可微部分的次微分的和。\n\n首先，我们推导一般的 KKT 平稳性条件。最小二乘损失项 $L(\\beta)$ 的梯度是 $\\nabla L(\\beta) = X^{\\top}(X\\beta - y)$。$\\ell_{1}$ 范数项 $R(\\beta)$ 的次微分是 $\\partial R(\\beta) = \\lambda \\, \\partial \\|\\beta\\|_{1}$。平稳性条件是 $0 \\in \\nabla L(\\hat{\\beta}) + \\partial R(\\hat{\\beta})$，可以写成：\n$$\n-\\nabla L(\\hat{\\beta}) \\in \\partial R(\\hat{\\beta})\n$$\n或者等价地，必须存在一个次梯度向量 $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$ 使得\n$$\n\\nabla L(\\hat{\\beta}) + \\lambda s = 0.\n$$\n按分量来看，对于每个 $j \\in \\{1, \\dots, p\\}$，我们必须有 $(\\nabla L(\\hat{\\beta}))_j + \\lambda s_j = 0$，其中如果 $\\hat{\\beta}_j \\neq 0$，则 $s_j = \\operatorname{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j = 0$，则 $s_j \\in [-1, 1]$。\n\n其次，我们为给定的数值实例计算必要的量。\n数据如下：\n$$\nX = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  0 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 1, \\quad \\hat{\\beta} = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\tfrac{1}{2} \\end{pmatrix}.\n$$\n我们首先计算预测响应 $X\\hat{\\beta}$：\n$$\nX\\hat{\\beta} = \\begin{pmatrix} 1  0  2 \\\\ 0  1  -1 \\\\ 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 + 2 \\cdot \\frac{1}{2} \\\\ 0 \\cdot 1 + 1 \\cdot 0 + (-1) \\cdot \\frac{1}{2} \\\\ 1 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix}.\n$$\n接下来，我们计算残差向量 $r = X\\hat{\\beta} - y$：\n$$\nr = \\begin{pmatrix} 2 \\\\ -\\frac{1}{2} \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix}.\n$$\n现在，我们计算损失项的梯度 $g = \\nabla L(\\hat{\\beta}) = X^{\\top}r$：\n$$\ng = X^{\\top}r = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 2  -1  0 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1(-1) + 0(-\\frac{1}{2}) + 1(0) \\\\ 0(-1) + 1(-\\frac{1}{2}) + 1(0) \\\\ 2(-1) + (-1)(-\\frac{1}{2}) + 0(0) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix}.\n$$\n第三，我们必须找到一个次梯度向量 $s \\in \\partial \\|\\hat{\\beta}\\|_{1}$，它能最小化 KKT 平稳性残差 $t = g + \\lambda s$。$\\hat{\\beta}$ 的分量是 $\\hat{\\beta}_1 = 1$，$\\hat{\\beta}_2 = 0$ 和 $\\hat{\\beta}_3 = \\frac{1}{2}$。\n对于 $\\hat{\\beta}_1 = 1 \\neq 0$，次梯度分量 $s_1$ 被唯一确定为 $s_1 = \\operatorname{sign}(1) = 1$。\n对于 $\\hat{\\beta}_3 = \\frac{1}{2} \\neq 0$，次梯度分量 $s_3$ 被唯一确定为 $s_3 = \\operatorname{sign}(\\frac{1}{2}) = 1$。\n对于 $\\hat{\\beta}_2 = 0$，次梯度分量 $s_2$ 可以是区间 $[-1, 1]$ 内的任何值。为了最小化 KKT 残差，我们选择 $s_2$ 以使 $t = g + \\lambda s$ 的第二个分量尽可能接近于零。我们想要最小化 $|t_2| = |g_2 + \\lambda s_2| = |-\\frac{1}{2} + 1 \\cdot s_2|$。当 $s_2 = \\frac{1}{2}$ 时，该值被最小化。由于 $\\frac{1}{2} \\in [-1, 1]$，这是一个有效的选择。\n因此，优化选择的次梯度向量是 $s = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix}$。\n\n使用这个 $s$，KKT 平稳性残差向量 $t$ 是：\n$$\nt = g + \\lambda s = \\begin{pmatrix} -1 \\\\ -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1+1 \\\\ -\\frac{1}{2}+\\frac{1}{2} \\\\ -\\frac{3}{2}+1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\frac{1}{2} \\end{pmatrix}.\n$$\nKKT 平稳性条件 $t=0$ 未被满足，因为 $t_3 = -\\frac{1}{2} \\neq 0$。因此，$\\hat{\\beta}$ 不是 LASSO 问题的最优解。\n\n违规发生在第三个坐标上。这意味着如果我们执行循环坐标下降，$\\beta_3$ 的值将从其当前值 $\\frac{1}{2}$ 更新。为了验证这一点，我们找到在保持 $\\beta_1=1$ 和 $\\beta_2=0$ 固定的情况下，最小化目标函数的 $\\beta_3$ 的值。将目标函数看作关于 $\\beta_3$ 的函数，它为：\n$$\nF_3(\\beta_3) = \\frac{1}{2}\\left\\| y - X_{\\cdot 1}\\hat{\\beta}_1 - X_{\\cdot 2}\\hat{\\beta}_2 - X_{\\cdot 3}\\beta_3 \\right\\|_2^2 + \\lambda|\\hat{\\beta}_1| + \\lambda|\\hat{\\beta}_2| + \\lambda|\\beta_3|\n$$\n最小化此式等价于求解一维 LASSO 问题：\n$$\n\\min_{\\beta_3} \\frac{1}{2}\\left\\| (y - X_{\\cdot 1}\\hat{\\beta}_1 - X_{\\cdot 2}\\hat{\\beta}_2) - X_{\\cdot 3}\\beta_3 \\right\\|_2^2 + \\lambda|\\beta_3|\n$$\n部分残差为 $r^{(3)} = y - X_{\\cdot 1}(1) - X_{\\cdot 2}(0) = \\begin{pmatrix} 3 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n预测变量是 $X_{\\cdot 3} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\end{pmatrix}$。其范数的平方是 $\\|X_{\\cdot 3}\\|_2^2 = 2^2 + (-1)^2 + 0^2 = 5$。\n坐标下降的更新由软阈值算子给出，其标准形式为：\n$$\n\\beta_j^{\\text{new}} = \\frac{1}{\\|X_{\\cdot j}\\|_2^2} S_{\\lambda}\\left(X_{\\cdot j}^{\\top} r^{(j)}\\right)\n$$\n我们计算 $X_{\\cdot 3}^{\\top}r^{(3)} = \\begin{pmatrix} 2  -1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = 4$。\n当 $\\lambda=1$ 时，更新为：\n$$\n\\beta_3^{\\text{new}} = \\frac{1}{5} S_1(4) = \\frac{1}{5} \\operatorname{sign}(4) \\max(|4| - 1, 0) = \\frac{1}{5} (4-1) = \\frac{3}{5}.\n$$\n由于更新后的值 $\\beta_3^{\\text{new}} = \\frac{3}{5}$ 与当前值 $\\hat{\\beta}_3 = \\frac{1}{2}$ 不同，这证实了第三个坐标不是最优的。非零的 KKT 残差 $t_3 = -\\frac{1}{2}$ 正确地指出了这种次优性。\n\n最后，我们报告 KKT 平稳性残差向量 $t$ 的无穷范数：\n$$\n\\|t\\|_{\\infty} = \\max(|t_1|, |t_2|, |t_3|) = \\max\\left(|0|, |0|, \\left|-\\frac{1}{2}\\right|\\right) = \\frac{1}{2}.\n$$", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3442175"}, {"introduction": "当LASSO问题存在多个最优解时（例如，当特征完全相关时），会发生什么？这个编程实践将展示，循环坐标下降由于其序贯更新的特性，会从最优解集中选择一个特定的解。你将看到，仅仅改变更新顺序就会导致系数在重复特征间的不同分配，这种现象被称为路径依赖和对称性破缺，尽管最终的预测结果保持不变 [@problem_id:3111866]。", "problem": "您的任务是为最小绝对值收缩和选择算子 (LASSO) 实现一个循环坐标下降算法，该算法旨在最小化以下凸目标函数\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1,\n$$\n其中，$n$ 是样本数量，$p$ 是特征数量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{y} \\in \\mathbb{R}^{n}$ 是响应向量，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是正则化参数，$\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数，$\\left\\| \\cdot \\right\\|_1$ 表示 $L_1$ 范数。从基本的凸优化原理和次梯度最优性条件出发，推导并实现一个坐标更新规则，该规则在保持其他坐标固定的同时，沿着单个坐标最小化目标函数。您的实现必须：\n- 在遍历所有坐标的每一轮中，按照指定的坐标顺序执行循环更新。\n- 在每次坐标更新后，高效地维护和更新残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。\n- 当单轮完整遍历中坐标的最大绝对变化量小于一个容差（使用 $10^{-12}$）时，或者当达到最大遍历轮数（使用 $100$）时终止。\n\n需要研究的核心现象是当 $\\boldsymbol{X}$ 的两列完全相同时解的非唯一性。在这种情况下，目标函数仅取决于重复系数的和，对重复坐标进行不同的分配可以达到相同的目标函数值。您必须通过实验证明坐标下降的路径依赖性和对称性破缺：改变坐标更新的顺序会导致重复特征的系数分配不同，尽管预测结果相同。\n\n实现一个程序，在每个测试用例中运行两次坐标下降算法，使用两种不同的坐标顺序：首先是顺序 $[0,1]$，然后是顺序 $[1,0]$。计算并报告能够捕捉两次运行之间差异的量化指标。\n\n对所有测试用例使用以下固定数据：\n- 令 $n = 10$ 并定义向量 $\\boldsymbol{x} = [1,2,3,4,5,6,7,8,9,10]^\\top$。\n- 构建 $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times 2}$，使其两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。\n- 定义三个测试用例，它们的响应向量 $\\boldsymbol{y}$ 和正则化参数 $\\lambda$ 不同：\n  1. 测试用例 A（理想情况）：$\\boldsymbol{y} = 3 \\boldsymbol{x}$ 且 $\\lambda = 5$。\n  2. 测试用例 B（符号边界情况）：$\\boldsymbol{y} = -3 \\boldsymbol{x}$ 且 $\\lambda = 5$。\n  3. 测试用例 C（边界情况）：$\\boldsymbol{y} = 3 \\boldsymbol{x}$ 且 $\\lambda = 200$。\n\n初始化和停止条件：\n- 对于所有运行，将 $\\boldsymbol{\\beta}$ 初始化为零向量。\n- 按规定使用容差 $10^{-12}$ 和最大轮数 $100$。\n\n对于每个测试用例，运行算法两次（一次坐标更新顺序为 $[0,1]$，一次为 $[1,0]$），并计算以下四个指标：\n1. 两个系数向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{\\beta}^{(01)} - \\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，以浮点数形式表示。\n2. 重复系数之和的绝对差，即 $ \\left| \\left( \\beta^{(01)}_0 + \\beta^{(01)}_1 \\right) - \\left( \\beta^{(10)}_0 + \\beta^{(10)}_1 \\right) \\right| $，以浮点数形式表示。\n3. 两个预测向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{X}\\boldsymbol{\\beta}^{(01)} - \\boldsymbol{X}\\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，以浮点数形式表示。\n4. 一个指示对称性破缺的布尔值，其定义为在至少一次运行中重复系数是否不同，即 $ \\left| \\beta^{(01)}_0 - \\beta^{(01)}_1 \\right|  10^{-9} $ 或 $ \\left| \\beta^{(10)}_0 - \\beta^{(10)}_1 \\right|  10^{-9} $ 是否成立。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为一个由逗号分隔的列表，并用方括号括起来。每个测试用例的结果本身必须是一个列表，包含按上述顺序排列的四个指标。例如，输出必须具有以下形式\n$$\n[\\,[m_{A,1},m_{A,2},m_{A,3},b_A],\\,[m_{B,1},m_{B,2},m_{B,3},b_B],\\,[m_{C,1},m_{C,2},m_{C,3},b_C]\\,],\n$$\n其中 $m_{\\cdot,\\cdot}$ 是浮点数，$b_{\\cdot}$ 是布尔值。不涉及任何物理单位或角度；所有量都是无量纲的实数。通过遵循目标函数的凸性以及基于次梯度的坐标最小化定义，确保科学真实性，不要依赖任何临时启发式方法。", "solution": "### 1. LASSO 目标函数\n\n该问题要求最小化 LASSO 目标函数，该函数由一个最小二乘数据保真项和一个 $L_1$ 范数正则化项组成。对于系数向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$，目标函数 $L(\\boldsymbol{\\beta})$ 由下式给出：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1\n$$\n在此，$\\boldsymbol{y} \\in \\mathbb{R}^n$ 是响应向量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$n$ 是样本数量，$p$ 是特征数量，而 $\\lambda \\ge 0$ 是正则化参数。$L_1$ 范数定义为 $\\left\\| \\boldsymbol{\\beta} \\right\\|_1 = \\sum_{j=1}^p |\\beta_j|$。该目标函数是凸函数，这保证了全局最小值的存在。\n\n### 2. 坐标更新规则的推导\n\n坐标下降在优化目标函数时，每次只针对一个坐标进行优化，同时保持所有其他坐标固定。为了推导第 $k$ 个系数 $\\beta_k$ 的更新规则，我们将所有其他系数 $\\beta_j$（其中 $j \\neq k$）视为常数。\n\n目标函数可以被重写以分离出涉及 $\\beta_k$ 的项：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda \\sum_{j \\neq k} |\\beta_j| + \\lambda |\\beta_k|\n$$\n我们定义偏残差 $\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j$。这相当于从模型中移除特征 $k$ 时的残差。不涉及 $\\beta_k$ 的项在对 $\\beta_k$ 进行最小化时是常数。因此，需要为 $\\beta_k$ 最小化的目标是：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left\\| \\boldsymbol{r}_k - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda |\\beta_k|\n$$\n展开二次项得到：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left( \\boldsymbol{r}_k^\\top \\boldsymbol{r}_k - 2\\beta_k \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k^2 \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k} \\right) + \\lambda |\\beta_k|\n$$\n为了找到这个凸函数的最小值，我们使用次梯度最优性条件，该条件表明在最小值点，$L_k(\\beta_k)$ 的次梯度必须包含 $0$。绝对值函数 $|\\cdot|$ 的次梯度是符号函数 `sgn`，在 $0$ 点它是一个集值函数。\n$$\n\\frac{\\partial L_k(\\beta_k)}{\\partial \\beta_k} = \\frac{1}{n} \\left( -\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) \\right) + \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n令次梯度包含 $0$：\n$$\n0 \\in \\frac{1}{n} \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k - \\beta_k \\frac{1}{n} \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n我们定义 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k$ 和 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}$。条件变为：\n$$\n\\beta_k z_k \\in \\rho_k - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n这引出了软阈值函数 $S(a, \\delta) = \\text{sgn}(a)\\max(|a|-\\delta, 0)$。$\\beta_k$ 的解是：\n$$\n\\beta_k = \\frac{S(\\rho_k, \\lambda)}{z_k}\n$$\n此更新规则沿第 $k$ 个坐标最小化目标函数。\n\n### 3. 坐标下降算法与高效更新\n\n循环坐标下降算法会重复遍历所有坐标 $k=0, 1, \\dots, p-1$，直到收敛。为了提高计算效率，我们避免在每一步重新计算偏残差 $\\boldsymbol{r}_k$。取而代之的是，我们维护完整残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。偏残差可以用完整残差和更新前 $\\beta_k$ 的当前值来表示：\n$$\n\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j = \\left(\\boldsymbol{y} - \\sum_{j=1}^p \\boldsymbol{X}_{:,j}\\beta_j \\right) + \\boldsymbol{X}_{:,k}\\beta_k = \\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k\n$$\n因此，项 $\\rho_k$ 可以计算为 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k)$，其中 $\\boldsymbol{r}$ 和 $\\beta_k$ 是坐标 $k$ 更新前的值。\n\n在将 $\\beta_k$ 从 $\\beta_k^{\\text{old}}$ 更新到 $\\beta_k^{\\text{new}}$ 后，完整残差 $\\boldsymbol{r}$ 也必须更新。预测值的变化是 $\\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。新的残差是：\n$$\n\\boldsymbol{r}^{\\text{new}} = \\boldsymbol{r}^{\\text{old}} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})\n$$\n这是一个高效的 $O(n)$ 更新。算法如下：\n\n1.  初始化 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 和 $\\boldsymbol{r} = \\boldsymbol{y}$。对所有 $k$ 预先计算 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top\\boldsymbol{X}_{:,k}$。\n2.  对于最多到最大轮数的每一轮遍历：\n    a. 将 `max_abs_change` 初始化为 $0$。\n    b. 对于指定顺序中的每个坐标 $k$：\n        i.   存储 $\\beta_k^{\\text{old}} = \\beta_k$。\n        ii.  计算 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k^{\\text{old}})$。\n        iii. 计算 $\\beta_k^{\\text{new}} = S(\\rho_k, \\lambda) / z_k$。\n        iv.  更新残差：$\\boldsymbol{r} \\leftarrow \\boldsymbol{r} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。\n        v.   更新系数：$\\beta_k \\leftarrow \\beta_k^{\\text{new}}$。\n        vi.  用 $|\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}}|$ 更新 `max_abs_change`。\n    c. 如果 `max_abs_change` 小于容差（例如，$10^{-12}$），则终止。\n3.  返回最终的系数向量 $\\boldsymbol{\\beta}$。\n\n### 4. 相同列导致的非唯一性\n\n问题设置了一个场景，其中 $\\boldsymbol{X}$ 的两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。在这种情况下，模型预测为：\n$$\n\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}_{:,0}\\beta_0 + \\boldsymbol{X}_{:,1}\\beta_1 = \\boldsymbol{x}\\beta_0 + \\boldsymbol{x}\\beta_1 = (\\beta_0 + \\beta_1)\\boldsymbol{x}\n$$\n目标函数的数据保真项仅取决于系数之和 $\\beta_{\\text{sum}} = \\beta_0 + \\beta_1$。$L_1$ 惩罚项是 $\\lambda(|\\beta_0| + |\\beta_1|)$。优化问题变为：\n$$\n\\min_{\\beta_0, \\beta_1} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - (\\beta_0 + \\beta_1)\\boldsymbol{x} \\right\\|_2^2 + \\lambda(|\\beta_0| + |\\beta_1|)\n$$\n虽然和 $\\beta_{\\text{sum}}$ 的最优值是唯一的，但可以有无穷多对 $(\\beta_0, \\beta_1)$ 产生这个和。对于任何给定的最优和 $\\beta_{\\text{sum}}$，当 $\\beta_{\\text{sum}}$ 的全部大小都分配给单个系数时（例如，$\\beta_0 = \\beta_{\\text{sum}}, \\beta_1 = 0$，反之亦然），$L_1$ 惩罚项 $\\lambda(|\\beta_0| + |\\beta_1|)$ 被最小化，前提是 $\\beta_{\\text{sum}} \\neq 0$。\n\n循环坐标下降算法根据更新顺序打破了这种对称性。当首先更新坐标 $0$ 时，它将尽可能多地吸收信号，可能为坐标 $1$ 留下一个很小的残差。如果残差信号低于由 $\\lambda$ 设定的阈值，$\\beta_1$ 的更新将为零。将顺序颠倒为 $[1, 0]$ 将导致 $\\beta_1$ 首先吸收信号，从而得到一个不同的最终系数向量 $\\boldsymbol{\\beta}$，即使和 $\\beta_0+\\beta_1$ 以及预测值 $\\boldsymbol{X}\\boldsymbol{\\beta}$ 将是相同的（在数值精度范围内）。这种路径依赖性是坐标下降在非严格凸目标函数上的一个关键特征。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the coordinate descent for LASSO to demonstrate path dependence.\n    \"\"\"\n\n    def coordinate_descent(X, y, lambda_val, order, tol, max_passes):\n        \"\"\"\n        Performs cyclic coordinate descent for LASSO.\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n        residual = y.copy()\n\n        # Precompute column normalization factors\n        z = np.array([(1/n) * X[:, k].T @ X[:, k] for k in range(p)])\n\n        def soft_threshold(a, delta):\n            return np.sign(a) * np.maximum(np.abs(a) - delta, 0)\n\n        for _ in range(max_passes):\n            max_abs_change = 0.0\n            \n            for k in order:\n                beta_old_k = beta[k]\n                \n                # Calculate rho_k using the efficient residual update formula\n                # rho_k = (1/n) * X_k^T * (y - sum_{j!=k} X_j * beta_j)\n                # which is equivalent to (1/n) * X_k^T * (residual + X_k * beta_old_k)\n                rho_k = (1/n) * X[:, k].T @ (residual + X[:, k] * beta_old_k)\n\n                # Update beta_k using soft-thresholding\n                beta[k] = soft_threshold(rho_k, lambda_val) / z[k]\n                \n                delta_beta_k = beta[k] - beta_old_k\n                \n                if delta_beta_k != 0:\n                    # Update residual efficiently\n                    residual -= X[:, k] * delta_beta_k\n\n                max_abs_change = max(max_abs_change, np.abs(delta_beta_k))\n\n            if max_abs_change  tol:\n                break\n                \n        return beta\n\n    # --- Fixed Data ---\n    n = 10\n    x_vec = np.arange(1, 11, dtype=float)\n    X = np.stack([x_vec, x_vec], axis=1)\n    \n    # --- Algorithm Parameters ---\n    tol = 1e-12\n    max_passes = 100\n    sym_tol = 1e-9\n\n    # --- Test Cases ---\n    test_cases_params = [\n        # Case A: happy path\n        {'y': 3 * x_vec, 'lambda': 5.0},\n        # Case B: sign edge case\n        {'y': -3 * x_vec, 'lambda': 5.0},\n        # Case C: boundary case (high regularization)\n        {'y': 3 * x_vec, 'lambda': 200.0},\n    ]\n\n    results = []\n    \n    for params in test_cases_params:\n        y = params['y']\n        lambda_val = params['lambda']\n\n        # Run with order [0, 1]\n        beta_01 = coordinate_descent(X, y, lambda_val, order=[0, 1], tol=tol, max_passes=max_passes)\n        \n        # Run with order [1, 0]\n        beta_10 = coordinate_descent(X, y, lambda_val, order=[1, 0], tol=tol, max_passes=max_passes)\n\n        # --- Compute Metrics ---\n        # 1. Euclidean norm of the difference between the two coefficient vectors\n        metric1 = np.linalg.norm(beta_01 - beta_10)\n\n        # 2. Absolute difference in the sum of the duplicate coefficients\n        metric2 = np.abs(np.sum(beta_01) - np.sum(beta_10))\n\n        # 3. Euclidean norm of the difference between the two prediction vectors\n        pred_01 = X @ beta_01\n        pred_10 = X @ beta_10\n        metric3 = np.linalg.norm(pred_01 - pred_10)\n\n        # 4. Boolean indicating symmetry-breaking\n        m4_cond1 = np.abs(beta_01[0] - beta_01[1]) > sym_tol\n        m4_cond2 = np.abs(beta_10[0] - beta_10[1]) > sym_tol\n        metric4 = m4_cond1 or m4_cond2\n\n        results.append([metric1, metric2, metric3, metric4])\n\n    # Format output as specified: [[m_A1,m_A2,m_A3,b_A],[m_B1,m_B2,m_B3,b_B],...]\n    inner_results_str = []\n    for metrics in results:\n        m1, m2, m3, b = metrics\n        # Use str(b) to get 'True' or 'False'\n        inner_results_str.append(f\"[{m1},{m2},{m3},{str(b)}]\")\n    \n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3111866"}, {"introduction": "坐标更新的顺序不仅能从非唯一解集中选择一个特定的解，还会影响算法的收敛速度。通过在一个简单的二维问题上进行分步计算，本练习揭示了不同的循环更新顺序如何导致达到相同精度所需的迭代次数不同。这突显了坐标下降算法中设计选择对实际性能的影响 [@problem_id:3442192]。", "problem": "考虑具有以下目标函数的最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) 问题：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\;\\; \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\n并假设我们使用循环坐标下降法，其定义为：在保持其他坐标固定的情况下，每次精确地对一个坐标进行最小化，并遵循指定的循环顺序。Karush–Kuhn–Tucker (KKT) 容差由坐标级别的最大KKT残差来衡量\n$$\nV(\\beta) \\equiv \\max_{1 \\leq j \\leq p} v_j(\\beta),\n$$\n其中，记 $r \\equiv y - X\\beta$，\n$$\nv_j(\\beta) \\equiv \n\\begin{cases}\n\\left|x_j^{\\top} r - \\lambda \\,\\mathrm{sign}(\\beta_j)\\right|,  \\text{若 } \\beta_j \\neq 0,\\\\[4pt]\n\\max\\!\\big(0, \\, |x_j^{\\top} r| - \\lambda\\big),  \\text{若 } \\beta_j = 0,\n\\end{cases}\n$$\n并且 $x_j$ 表示 $X$ 的第 $j$ 列。一次迭代定义为按照给定的循环顺序对所有坐标进行一次完整的遍历。从 $\\beta^{(0)} = (0, 0)^{\\top}$ 开始。\n\n使用以下特定数据\n$$\nX \\;=\\; \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}, \\qquad y \\;=\\; \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\qquad \\lambda \\;=\\; 1,\n$$\n因此 $n = 2$ 且 $p = 2$。考虑两种循环顺序：\n- 顺序 $\\mathcal{A}$：在每次遍历中，先更新坐标 $1$，再更新坐标 $2$。\n- 顺序 $\\mathcal{B}$：在每次遍历中，先更新坐标 $2$，再更新坐标 $1$。\n\n仅使用 LASSO 目标函数的定义和必要的最优性条件，分析每种顺序下的循环坐标下降法。在每次内部更新时，给定其他坐标的当前值，精确地最小化关于所选坐标的目标函数。将停止准则定义为 KKT 容差达到或低于固定阈值\n$$\n\\tau \\;=\\; 0.2.\n$$\n\n确定在每种顺序下，达到 $V(\\beta) \\leq \\tau$ 所需的完整迭代次数（完整遍历次数），并报告其差值\n$$\nN_{\\mathcal{B}} - N_{\\mathcal{A}}.\n$$\n将你的最终答案以单个实数值的形式给出。无需四舍五入。", "solution": "问题要求计算在两种不同更新顺序下，循环坐标下降法解决 LASSO 问题收敛所需的迭代次数之差。停止准则是基于 Karush-Kuhn-Tucker (KKT) 容差 $V(\\beta)$ 小于或等于阈值 $\\tau = 0.2$。\n\nLASSO 目标函数由下式给出：\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\n其中 $\\beta \\in \\mathbb{R}^p$。给定 $p=2$ 和数据：\n$$\nX = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}, \\quad \\lambda = 1\n$$\n$X$ 的列向量为 $x_1 = (1, 0)^{\\top}$ 和 $x_2 = (1, 1)^{\\top}$。初始估计为 $\\beta^{(0)} = (0, 0)^{\\top}$。\n\n首先，我们推导通用的坐标更新规则。为了更新坐标 $\\beta_j$，我们固定所有其他坐标 $\\beta_k$（其中 $k \\ne j$）。目标函数作为 $\\beta_j$ 的函数为：\n$$\nL(\\beta_j) = \\frac{1}{2} \\|y - \\sum_{k \\ne j} x_k \\beta_k - x_j \\beta_j\\|_2^2 + \\lambda |\\beta_j| + C\n$$\n其中 $C$ 是一个关于 $\\beta_j$ 的常数。令 $r^{(j)} = y - \\sum_{k \\ne j} x_k \\beta_k$ 为部分残差。目标函数简化为最小化 $\\frac{1}{2} \\|r^{(j)} - x_j \\beta_j\\|_2^2 + \\lambda |\\beta_j|$。展开二次项并对 $\\beta_j$ 求次梯度，即可得到最优性条件。解通过软阈值法得到：\n$$\n\\beta_j \\leftarrow \\frac{1}{x_j^{\\top}x_j} S_{\\lambda}\\left(x_j^{\\top} r^{(j)}\\right)\n$$\n其中 $S_{\\alpha}(z) = \\mathrm{sign}(z) \\max(|z| - \\alpha, 0)$。\n\n我们预先计算一些值：\n$x_1^{\\top}x_1 = 1^2 + 0^2 = 1$。\n$x_2^{\\top}x_2 = 1^2 + 1^2 = 2$。\n$x_1^{\\top}y = 1 \\cdot 2 + 0 \\cdot 0 = 2$。\n$x_2^{\\top}y = 1 \\cdot 2 + 1 \\cdot 0 = 2$。\n\nKKT 容差为 $V(\\beta) = \\max_j v_j(\\beta)$，其中 $r = y - X\\beta$ 且\n$$\nv_j(\\beta) = \n\\begin{cases}\n\\left|x_j^{\\top} r - \\lambda \\,\\mathrm{sign}(\\beta_j)\\right|,  \\text{若 } \\beta_j \\neq 0 \\\\\n\\max\\!\\big(0, \\, |x_j^{\\top} r| - \\lambda\\big),  \\text{若 } \\beta_j = 0\n\\end{cases}\n$$\n\n### 顺序 $\\mathcal{A}$ 的分析：先更新 $\\beta_1$ 再更新 $\\beta_2$\n\n我们从 $\\beta^{(0)} = (0, 0)^{\\top}$ 开始。一次迭代是一次完整的遍历。\n\n**迭代 1 (顺序 $\\mathcal{A}$):**\n1.  **更新 $\\beta_1$**: 我们固定 $\\beta_2 = 0$。部分残差为 $r^{(1)} = y - x_2 \\beta_2 = y = (2, 0)^{\\top}$。\n    $$\n    \\beta_1 \\leftarrow \\frac{1}{x_1^{\\top}x_1} S_{\\lambda}(x_1^{\\top} y) = \\frac{1}{1} S_1(2) = \\mathrm{sign}(2) \\max(|2|-1, 0) = 1\n    $$\n    状态变为 $(\\beta_1, \\beta_2) = (1, 0)^{\\top}$。\n\n2.  **更新 $\\beta_2$**: 我们固定 $\\beta_1 = 1$。部分残差为 $r^{(2)} = y - x_1 \\beta_1 = (2, 0)^{\\top} - (1, 0)^{\\top} \\cdot 1 = (1, 0)^{\\top}$。\n    $$\n    x_2^{\\top} r^{(2)} = (1, 1)^{\\top} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1\n    $$\n    $$\n    \\beta_2 \\leftarrow \\frac{1}{x_2^{\\top}x_2} S_{\\lambda}(x_2^{\\top} r^{(2)}) = \\frac{1}{2} S_1(1) = \\frac{1}{2} \\mathrm{sign}(1) \\max(|1|-1, 0) = 0\n    $$\n    状态保持为 $(1, 0)^{\\top}$。\n\n迭代 1 结束：$\\beta^{(1)} = (1, 0)^{\\top}$。\n\n**检查 $\\beta^{(1)}$ 的停止准则：**\n完整残差为 $r = y - X\\beta^{(1)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n-   对于 $j=1$：$\\beta_1 = 1 \\neq 0$。\n    $x_1^{\\top}r = (1, 0)^{\\top} (1, 0)^{\\top} = 1$。\n    $v_1(\\beta^{(1)}) = |x_1^{\\top}r - \\lambda \\mathrm{sign}(\\beta_1)| = |1 - 1 \\cdot 1| = 0$。\n-   对于 $j=2$：$\\beta_2 = 0$。\n    $x_2^{\\top}r = (1, 1)^{\\top} (1, 0)^{\\top} = 1$。\n    $v_2(\\beta^{(1)}) = \\max(0, |x_2^{\\top}r| - \\lambda) = \\max(0, |1| - 1) = 0$。\n\nKKT 容差为 $V(\\beta^{(1)}) = \\max(0, 0) = 0$。由于 $0 \\le \\tau = 0.2$，算法停止。\n顺序 $\\mathcal{A}$ 所需的完整迭代次数为 $N_{\\mathcal{A}} = 1$。\n\n### 顺序 $\\mathcal{B}$ 的分析：先更新 $\\beta_2$ 再更新 $\\beta_1$\n\n我们从 $\\beta^{(0)} = (0, 0)^{\\top}$ 开始。\n\n**迭代 1 (顺序 $\\mathcal{B}$):**\n1.  **更新 $\\beta_2$**: 我们固定 $\\beta_1 = 0$。部分残差为 $r^{(2)} = y - x_1 \\beta_1 = y = (2, 0)^{\\top}$。\n    $$\n    \\beta_2 \\leftarrow \\frac{1}{x_2^{\\top}x_2} S_{\\lambda}(x_2^{\\top} y) = \\frac{1}{2} S_1(2) = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n    $$\n    状态变为 $(0, 1/2)^{\\top}$。\n2.  **更新 $\\beta_1$**: 我们固定 $\\beta_2 = 1/2$。部分残差为 $r^{(1)} = y - x_2 \\beta_2 = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\frac{1}{2} = \\begin{pmatrix} 3/2 \\\\ -1/2 \\end{pmatrix}$。\n    $$\n    x_1^{\\top} r^{(1)} = (1, 0)^{\\top} (3/2, -1/2)^{\\top} = 3/2\n    $$\n    $$\n    \\beta_1 \\leftarrow \\frac{1}{x_1^{\\top}x_1} S_{\\lambda}(x_1^{\\top} r^{(1)}) = \\frac{1}{1} S_1(3/2) = \\frac{1}{2}\n    $$\n迭代 1 结束：$\\beta^{(1)} = (1/2, 1/2)^{\\top}$。\n检查 KKT： $r = y - X\\beta^{(1)} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1/2 \\end{pmatrix}$。\n$v_1(\\beta^{(1)}) = |x_1^{\\top}r - \\lambda \\mathrm{sign}(\\beta_1)| = |1 - 1\\cdot 1| = 0$。\n$v_2(\\beta^{(1)}) = |x_2^{\\top}r - \\lambda \\mathrm{sign}(\\beta_2)| = |(1,1)^{\\top}(1, -1/2)^{\\top} - 1| = |1/2-1| = 1/2 = 0.5$。\n$V(\\beta^{(1)}) = 0.5  0.2$。我们继续。\n\n**迭代 2 (顺序 $\\mathcal{B}$):**\n从 $\\beta = (1/2, 1/2)^{\\top}$ 开始。\n1.  **更新 $\\beta_2$**: 固定 $\\beta_1=1/2$。$r^{(2)} = y-x_1(1/2) = (3/2, 0)^\\top$。\n    $x_2^\\top r^{(2)} = 3/2$。$\\beta_2 \\leftarrow \\frac{1}{2}S_1(3/2) = \\frac{1}{4}$。状态：$(1/2, 1/4)^{\\top}$。\n2.  **更新 $\\beta_1$**: 固定 $\\beta_2=1/4$。$r^{(1)} = y-x_2(1/4) = (7/4, -1/4)^\\top$。\n    $x_1^\\top r^{(1)} = 7/4$。$\\beta_1 \\leftarrow S_1(7/4) = 3/4$。\n迭代 2 结束：$\\beta^{(2)} = (3/4, 1/4)^{\\top}$。\n检查 KKT：$r = y-X\\beta^{(2)} = \\begin{pmatrix} 1 \\\\ -1/4 \\end{pmatrix}$。\n$v_1(\\beta^{(2)}) = |1-1| = 0$。\n$v_2(\\beta^{(2)}) = |x_2^\\top r - 1| = |3/4-1| = 1/4 = 0.25$。\n$V(\\beta^{(2)}) = 0.25  0.2$。我们继续。\n\n**迭代 3 (顺序 $\\mathcal{B}$):**\n从 $\\beta = (3/4, 1/4)^{\\top}$ 开始。\n1.  **更新 $\\beta_2$**: 固定 $\\beta_1=3/4$。$r^{(2)} = y-x_1(3/4) = (5/4, 0)^\\top$。\n    $x_2^\\top r^{(2)} = 5/4$。$\\beta_2 \\leftarrow \\frac{1}{2}S_1(5/4) = \\frac{1}{8}$。状态：$(3/4, 1/8)^{\\top}$。\n2.  **更新 $\\beta_1$**: 固定 $\\beta_2=1/8$。$r^{(1)} = y-x_2(1/8) = (15/8, -1/8)^\\top$。\n    $x_1^\\top r^{(1)} = 15/8$。$\\beta_1 \\leftarrow S_1(15/8) = 7/8$。\n迭代 3 结束：$\\beta^{(3)} = (7/8, 1/8)^{\\top}$。\n检查 KKT：$r = y-X\\beta^{(3)} = \\begin{pmatrix} 1 \\\\ -1/8 \\end{pmatrix}$。\n$v_1(\\beta^{(3)}) = |1-1| = 0$。\n$v_2(\\beta^{(3)}) = |x_2^\\top r - 1| = |7/8-1| = 1/8 = 0.125$。\n$V(\\beta^{(3)}) = 0.125 \\le 0.2$。算法停止。\n顺序 $\\mathcal{B}$ 所需的完整迭代次数为 $N_{\\mathcal{B}} = 3$。\n\n### 最终计算\n\n顺序 $\\mathcal{A}$ 的迭代次数为 $N_{\\mathcal{A}} = 1$。\n顺序 $\\mathcal{B}$ 的迭代次数为 $N_{\\mathcal{B}} = 3$。\n所求的差值为：\n$$\nN_{\\mathcal{B}} - N_{\\mathcal{A}} = 3 - 1 = 2\n$$", "answer": "$$\\boxed{2}$$", "id": "3442192"}]}