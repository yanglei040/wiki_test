{"hands_on_practices": [{"introduction": "本练习将引导你从第一性原理出发，为压缩感知中至关重要的LASSO估计器推导其广义Stein无偏风险估计（GSURE）。这不仅是一个理论推导，更是一次深入理解LASSO解的性质以及如何处理隐式定义估计器的绝佳实践。你将运用不动点方程和隐式微分这一强大技巧来计算关键的散度项，从而构建出不依赖于未知真实信号的风险度量。[@problem_id:3482264]", "problem": "考虑压缩感知中的欠定线性逆问题，其中观测值 $y \\in \\mathbb{R}^{m}$ 符合模型 $y = A x_{0} + w$，这里 $A \\in \\mathbb{R}^{m \\times n}$，$m  n$，且 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。令估计量 $x_{\\lambda}(y)$ 定义为某个固定 $\\lambda  0$ 的凸规划问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - y\\|^{2} + \\lambda \\|x\\|_{1}$ 的唯一最小化子。假设 $x_{\\lambda}(y)$ 是局部唯一的，并且关于 $y$ 几乎处处可微。定义均方预测风险 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A x_{\\lambda}(y) - A x_{0}\\|^{2}\\right]$。\n\n从高斯向量的Stein恒等式作为唯一的基本出发点，并且不引用任何已有的风险估计公式，完成以下任务：\n\n1. 推导 $R_{\\mathrm{pred}}(\\lambda)$ 的一个无偏估计量，该估计量仅依赖于数据 $y$、噪声方差 $\\sigma^{2}$ 以及映射 $y \\mapsto A x_{\\lambda}(y)$ 的散度。您的推导必须从以下性质出发：如果 $y = \\mu + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$，并且 $h:\\mathbb{R}^{m} \\to \\mathbb{R}^{m}$ 是弱可微的且散度可积，那么 $\\mathbb{E}\\left[\\langle \\varepsilon, h(y) \\rangle\\right] = \\sigma^{2} \\mathbb{E}\\left[\\operatorname{div} h(y)\\right]$。\n\n2. 证明对于任意固定的步长 $\\tau  0$，$x_{\\lambda}(y)$ 满足不动点关系 $x_{\\lambda}(y) = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}\\!\\left(x_{\\lambda}(y) - \\tau A^{\\top}(A x_{\\lambda}(y) - y)\\right)$，其中 $\\operatorname{prox}_{\\gamma \\|\\cdot\\|_{1}}(u) = \\arg\\min_{x} \\frac{1}{2}\\|x - u\\|^{2} + \\gamma \\|x\\|_{1}$。\n\n3. 利用第2部分中不动点方程的隐式微分，以及软阈值算子的雅可比矩阵是一个对角选择矩阵 $D = \\operatorname{diag}(d_{1},\\ldots,d_{n})$（其中 $d_{i} \\in \\{0,1\\}$ 几乎处处成立，且 $d_{i} = 1$ 当且仅当第 $i$ 个坐标在软阈值作用下是活跃的）这一事实，推导出 $\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)$ 的一个显式表达式，用在不動点处计算的 $A$、$\\tau$ 和 $D$ 来表示。您可以假设您使用的所有逆矩阵对于几乎所有的 $y$ 都存在。\n\n4. 结合您的结果，将广义Stein无偏风险估计（GSURE）用于预测，即 $\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau)$，表示为一个依赖于 $A$、$y$、$\\lambda$、$\\tau$、$\\sigma^{2}$ 以及在 $x_{\\lambda}(y)$ 处计算的 $D$ 的单一闭式解析表达式。将您的最终答案表示为单一的解析表达式。无需数值计算，也无需四舍五入。\n\n您的最终答案必须是一个单一的闭式解析表达式。", "solution": "用户提供的问题是统计信号处理和优化理论领域中一个适定且有科学依据的练习。它要求推导 LASSO 估计量预测风险的广义Stein无偏风险估计 (GSURE)。所有必要的数据、模型和假设均已提供，不存在矛盾或歧义。该问题是有效的，下面将给出完整解答。\n\n解答分为四个部分，对应于问题陈述中指定的四个任务。\n\n**第1部分：预测风险无偏估计量的推导**\n\n均方预测风险定义为 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A x_{\\lambda}(y) - A x_{0}\\|^{2}\\right]$，其中期望是针对观测向量 $y$ 的分布计算的。观测模型为 $y = A x_{0} + w$，其中 $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。\n我们将真实平均信号表示为 $\\mu \\equiv A x_{0}$。于是观测模型变为 $y = \\mu + w$，风险为 $R_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|A x_{\\lambda}(y) - \\mu\\|^{2}\\right]$。\n\n我们可以将期望内的平方范数展开如下：\n$$\n\\|A x_{\\lambda}(y) - \\mu\\|^{2} = \\|(A x_{\\lambda}(y) - y) + (y - \\mu)\\|^{2}\n$$\n项 $y - \\mu$ 是噪声向量 $w$。令 $\\hat{\\mu}(y) \\equiv A x_{\\lambda}(y)$ 为估计信号。表达式变为：\n$$\n\\|\\hat{\\mu}(y) - \\mu\\|^{2} = \\|\\hat{\\mu}(y) - y + w\\|^{2} = \\|\\hat{\\mu}(y) - y\\|^{2} + \\|w\\|^{2} + 2 \\langle \\hat{\\mu}(y) - y, w \\rangle\n$$\n对每一项取期望：\n1.  项 $\\mathbb{E}\\left[\\|\\hat{\\mu}(y) - y\\|^{2}\\right]$ 保持不变，因为它涉及对 $y$ 的函数的期望。\n2.  项 $\\mathbb{E}\\left[\\|w\\|^{2}\\right]$ 是一个高斯向量的期望平方范数，其分量 $w_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立同分布的。\n    $$\n    \\mathbb{E}\\left[\\|w\\|^{2}\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{m} w_i^2\\right] = \\sum_{i=1}^{m} \\mathbb{E}[w_i^2] = \\sum_{i=1}^{m} \\sigma^2 = m \\sigma^2\n    $$\n3.  对于交叉项 $\\mathbb{E}\\left[2 \\langle \\hat{\\mu}(y) - y, w \\rangle\\right]$，我们应用给定的Stein恒等式。令 $\\varepsilon = w = y - \\mu$。该恒等式表明，对于弱可微函数 $h(y)$，有 $\\mathbb{E}[\\langle \\varepsilon, h(y) \\rangle] = \\sigma^2 \\mathbb{E}[\\operatorname{div} h(y)]$。\n    令 $h(y) = \\hat{\\mu}(y) - y = A x_{\\lambda}(y) - y$。$h(y)$ 关于 $y$ 的散度为：\n    $$\n    \\operatorname{div}_y h(y) = \\operatorname{div}_y (A x_{\\lambda}(y) - y) = \\operatorname{div}_y(A x_{\\lambda}(y)) - \\operatorname{div}_y(y)\n    $$\n    恒等映射 $y \\mapsto y$ 的散度是 $\\operatorname{div}_y(y) = \\sum_{i=1}^{m} \\frac{\\partial y_i}{\\partial y_i} = m$。\n    因此，$\\operatorname{div}_y h(y) = \\operatorname{div}_y(A x_{\\lambda}(y)) - m$。\n    将Stein恒等式应用于交叉项：\n    $$\n    \\mathbb{E}[\\langle \\hat{\\mu}(y) - y, w \\rangle] = \\sigma^2 \\mathbb{E}[\\operatorname{div}_y(A x_{\\lambda}(y)) - m]\n    $$\n合并所有项，风险为：\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[\\|\\hat{\\mu}(y) - y\\|^{2}\\right] + m \\sigma^2 + 2\\sigma^2 \\mathbb{E}\\left[\\operatorname{div}_y(A x_{\\lambda}(y)) - m\\right]\n$$\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[ \\|A x_{\\lambda}(y) - y\\|^{2} + m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y)) - 2m \\sigma^2 \\right]\n$$\n$$\nR_{\\mathrm{pred}}(\\lambda) = \\mathbb{E}\\left[ \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y)) \\right]\n$$\n由于括号内量的期望等于风险，因此这个量是风险的一个无偏估计量。我们将其表示为 $\\mathrm{SURE}_{\\mathrm{pred}}(y)$：\n$$\n\\mathrm{SURE}_{\\mathrm{pred}}(y) = \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^2 + 2\\sigma^2 \\operatorname{div}_y(A x_{\\lambda}(y))\n$$\n该估计量依赖于数据 $y$、噪声方差 $\\sigma^2$ 以及映射 $y \\mapsto A x_{\\lambda}(y)$ 的散度，符合要求。\n\n**第2部分：估计量的不动点关系**\n\n估计量 $x_{\\lambda}(y)$ 是以下凸优化问题的解：\n$$\nx_{\\lambda}(y) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ F(x) \\equiv \\frac{1}{2} \\|A x - y\\|^{2} + \\lambda \\|x\\|_{1} \\right\\}\n$$\n令 $f(x) = \\frac{1}{2} \\|A x - y\\|^{2}$ 和 $g(x) = \\lambda \\|x\\|_{1}$。函数 $f(x)$ 可微，其梯度为 $\\nabla f(x) = A^{\\top}(Ax-y)$。函数 $g(x)$ 是凸的但不可微。最小化子 $x^* = x_{\\lambda}(y)$ 的一阶最优性条件表明，零向量必须属于目标函数在 $x^*$ 处的次微分：\n$$\n0 \\in \\partial F(x^*) = \\nabla f(x^*) + \\partial g(x^*)\n$$\n代入梯度和次微分的表达式：\n$$\n0 \\in A^{\\top}(Ax^* - y) + \\lambda \\partial \\|x^*\\|_{1}\n$$\n这可以重写为：\n$$\n-A^{\\top}(Ax^* - y) \\in \\lambda \\partial \\|x^*\\|_{1}\n$$\n现在，考虑问题中给出的不动点关系：\n$$\nx^* = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}\\!\\left(x^* - \\tau A^{\\top}(A x^* - y)\\right)\n$$\n对于某个步长 $\\tau  0$。根据定义，$z = \\operatorname{prox}_{\\gamma h}(v)$ 是 $\\frac{1}{2}\\|z-v\\|^2 + \\gamma h(z)$ 的唯一最小化子。这个邻近问题的最优性条件是 $0 \\in (z-v) + \\gamma \\partial h(z)$，这等价于 $v-z \\in \\gamma \\partial h(z)$。\n将此定义应用于我们的不动点关系，我们设：\n- $z = x^*$\n- $v = x^* - \\tau A^{\\top}(A x^* - y)$\n- $\\gamma h(\\cdot) = \\tau \\lambda \\|\\cdot\\|_{1}$\n邻近映射的最优性条件变为：\n$$\n\\left(x^* - \\tau A^{\\top}(A x^* - y)\\right) - x^* \\in \\tau \\lambda \\partial \\|x^*\\|_{1}\n$$\n$$\n-\\tau A^{\\top}(A x^* - y) \\in \\tau \\lambda \\partial \\|x^*\\|_{1}\n$$\n由于 $\\tau  0$，我们可以除以 $\\tau$ 得到：\n$$\n-A^{\\top}(A x^* - y) \\in \\lambda \\partial \\|x^*\\|_{1}\n$$\n这恰好是原始 LASSO 问题的最优性条件。因此，对于任意 $\\tau  0$，估计量 $x_{\\lambda}(y)$ 是给定迭代的不动点。\n\n**第3部分：散度项的推导**\n\n我们需要计算 $\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)$。它由 $A x_{\\lambda}(y)$ 关于 $y$ 的雅可比矩阵的迹给出。令 $J_x \\equiv \\frac{\\partial x_{\\lambda}(y)}{\\partial y^{\\top}}$ 为估计量 $x_{\\lambda}(y)$ 关于 $y$ 的 $n \\times m$ 雅可比矩阵。那么 $A x_{\\lambda}(y)$ 的雅可比矩阵是 $A J_x$，这是一个 $m \\times m$ 矩阵。散度为：\n$$\n\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right) = \\operatorname{Tr}(A J_x)\n$$\n为了找到 $J_x$，我们对第2部分中的不动点方程进行隐式微分。令 $x = x_{\\lambda}(y)$ 并将软阈值算子表示为 $S(u) \\equiv \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(u)$。不动点方程为：\n$$\nx = S\\left(x - \\tau A^{\\top}(A x - y)\\right)\n$$\n令 $S$ 的参数为 $u(x,y) = x - \\tau A^{\\top} A x + \\tau A^{\\top} y$。我们使用链式法则对等式 $x = S(u(x,y))$ 关于 $y^{\\top}$ 进行微分：\n$$\n\\frac{\\partial x}{\\partial y^{\\top}} = \\frac{\\partial S(u)}{\\partial u^{\\top}} \\left( \\frac{\\partial u}{\\partial x^{\\top}} \\frac{\\partial x}{\\partial y^{\\top}} + \\frac{\\partial u}{\\partial y^{\\top}} \\right)\n$$\n问题指出，软阈值算子的雅可比矩阵 $\\frac{\\partial S(u)}{\\partial u^{\\top}}$ 是一个对角矩阵 $D \\in \\mathbb{R}^{n \\times n}$，其中 $d_{ii} \\in \\{0, 1\\}$。\n其他雅可比矩阵为：\n- $\\frac{\\partial x}{\\partial y^{\\top}} = J_x$\n- $\\frac{\\partial u}{\\partial x^{\\top}} = I_n - \\tau A^{\\top} A$\n- $\\frac{\\partial u}{\\partial y^{\\top}} = \\tau A^{\\top}$\n将这些代入链式法则方程：\n$$\nJ_x = D \\left( (I_n - \\tau A^{\\top} A) J_x + \\tau A^{\\top} \\right)\n$$\n$$\nJ_x = D (I_n - \\tau A^{\\top} A) J_x + D \\tau A^{\\top}\n$$\n我们现在求解 $J_x$：\n$$\nJ_x - D (I_n - \\tau A^{\\top} A) J_x = \\tau D A^{\\top}\n$$\n$$\n(I_n - D + \\tau D A^{\\top} A) J_x = \\tau D A^{\\top}\n$$\n假设左侧的矩阵是可逆的（如问题陈述所允许），我们求得 $J_x$：\n$$\nJ_x = \\tau (I_n - D + \\tau D A^{\\top} A)^{-1} D A^{\\top}\n$$\n散度是 $A J_x$ 的迹。利用迹的循环性质 $\\operatorname{Tr}(XY) = \\operatorname{Tr}(YX)$，我们有 $\\operatorname{Tr}(A J_x) = \\operatorname{Tr}(J_x A)$：\n$$\n\\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right) = \\operatorname{Tr}(J_x A) = \\operatorname{Tr}\\left( \\tau (I_n - D + \\tau D A^{\\top} A)^{-1} D A^{\\top} A \\right)\n$$\n这就得到了所需的用 $A$、$\\tau$ 和 $D$ 表示的散度的显式表达式。\n\n**第4部分：GSURE 公式**\n\n最后，我们结合第1部分和第3部分的结果，得到用于预测的广义Stein无偏风险估计 $\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau)$ 的闭式表达式。我们将散度的表达式代入SURE公式：\n$$\n\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau) = \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{div}_{y}\\!\\left(A x_{\\lambda}(y)\\right)\n$$\n$$\n\\mathrm{GSURE}_{\\mathrm{pred}}(y; \\lambda, \\tau) = \\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{Tr}\\left(\\tau \\left(I_n - D + \\tau D A^{\\top} A\\right)^{-1} D A^{\\top} A \\right)\n$$\n此表达式依赖于指定的量：矩阵 $A$、数据 $y$（它决定了 $x_{\\lambda}(y)$ 和 $D$）、正则化参数 $\\lambda$（隐含在 $x_{\\lambda}(y)$ 和 $D$ 中）、步长参数 $\\tau$、噪声方差 $\\sigma^2$ 以及选择矩阵 $D$。这就是所求的最终解析表达式。", "answer": "$$\\boxed{\\|A x_{\\lambda}(y) - y\\|^{2} - m \\sigma^{2} + 2 \\sigma^{2} \\operatorname{Tr}\\left(\\tau \\left(I_n - D + \\tau D A^{\\top} A\\right)^{-1} D A^{\\top} A \\right)}$$", "id": "3482264"}, {"introduction": "理论的价值在于应用。本练习将理论付诸实践，要求你为一个在近似消息传递（AMP）等算法中常见的非凸降噪问题实现并验证SURE。你将通过编程，对SCAD和MCP这两种经典的非凸惩罚函数，利用SURE进行网格搜索来自动选择最优的正则化参数$\\lambda$。通过将SURE选择的参数所对应的经验误差与“神谕”（oracle）最优选择进行比较，你将亲身体验SURE在模型选择中的有效性。[@problem_id:3462670]", "problem": "给定一个高斯去噪子问题，该问题作为压缩感知中近似消息传递（AMP）框架的去噪模块出现。假设观测模型是可分离的，由 $y = x_{0} + \\tau z \\in \\mathbb{R}^{n}$ 给出，其中 $x_{0} \\in \\mathbb{R}^{n}$ 是未知信号，$z \\sim \\mathcal{N}(0, I_{n})$ 具有独立同分布（i.i.d.）的标准正态分布项，而 $\\tau  0$ 是（已知的）噪声标准差。考虑形式为 $\\eta(y; \\theta) = (\\eta_{1}(y_{1}; \\theta), \\ldots, \\eta_{n}(y_{n}; \\theta))$ 的可分离去噪器 $\\eta(y; \\theta)$，该去噪器逐坐标作用，其中 $\\theta$ 是一个调节参数（或一小组参数）。在本问题中，$\\eta(\\cdot; \\theta)$是由非凸惩罚项导出的邻近去噪器：惩罚项可以是平滑裁剪绝对偏差（SCAD）或极小化凹惩罚（MCP），其调节参数为 $\\lambda  0$，SCAD 的形状参数为 $a  2$，MCP 的形状参数为 $\\gamma  1$。\n\n你的任务如下。\n\n1) 从高斯随机变量的 Stein 引理出发，在对 $\\eta$ 的最小正则性条件（逐坐标弱可微性和适当的可积性）下，推导均方误差风险 $R(\\eta) = \\frac{1}{n} \\mathbb{E}\\left[\\lVert \\eta(y;\\theta) - x_{0} \\rVert_{2}^{2}\\right]$ 的一个无偏估计量，该估计量仅依赖于 $y$、$\\tau$ 和 $\\eta$ 的散度。将得到的估计量完全用 $y$、$\\tau$ 和 $\\eta$ 的逐坐标导数表示，不假设 $x_{0}$ 已知。不要引用任何未从此基础推导出的公式。\n\n2) 对于 SCAD 和 MCP 邻近去噪器，推导出 $\\eta_{i}(y_{i}; \\lambda, a)$ 和 $\\eta_{i}(y_{i}; \\lambda, \\gamma)$ 的显式逐坐标公式，以及它们关于 $y_{i}$ 的几乎处处成立的逐坐标导数，表示为 $|y_{i}|$ 的分段函数。假设 SCAD 的参数为 $a  2$，MCP 的参数为 $\\gamma  1$ 的标准定义。清楚地说明分段区域（用 $\\lambda$、$a$ 和 $\\gamma$ 表示）。\n\n3) 实现一个程序，该程序构造一个合成信号 $x_{0}$，根据上述高斯模型抽取 $y$，并使用任务 1 中得到的无偏估计量，通过在预定义网格上最小化该估计量来选择 $\\lambda$。然后，对于所选的 $\\lambda$，计算其经验均方误差相对于在同一网格上可实现的最小经验均方误差的相对次优性。对于选定的 $\\lambda$，相对次优性定义为 $\\frac{\\mathrm{MSE}(\\lambda) - \\min_{\\lambda'} \\mathrm{MSE}(\\lambda')}{\\min_{\\lambda'} \\mathrm{MSE}(\\lambda')}$，其中 $\\mathrm{MSE}(\\lambda) = \\frac{1}{n} \\lVert \\eta(y;\\lambda) - x_{0}\\rVert_{2}^{2}$。此外，确保去噪器的散度是根据任务 2 中推导出的逐坐标导数精确计算的。\n\n4) 使用以下固定的测试套件，该套件旨在探究多种情况。对于每个测试，生成一个 $x_{0}$，其包含 $k = \\lfloor \\rho n \\rfloor$ 个非零项，这些非零项的索引是无放回均匀选取的，其值是独立同分布地从 $\\mathcal{N}(0,1)$ 中抽取的，其余项为零。然后，用 $z \\sim \\mathcal{N}(0, I_{n})$ 构建 $y = x_{0} + \\tau z$。使用固定的伪随机数生成器种子 $12345$ 以确保可复现性。对于每个测试，在一个由区间 $[0, 3\\tau]$（含端点）上 61 个均匀间隔点组成的 $\\lambda$ 值网格上进行搜索。对于 SCAD，使用形状参数 $a = 3.7$。对于 MCP，使用指定的形状参数 $\\gamma$。测试如下：\n- 测试 $1$（理想情况，SCAD）：$n = 8000$, $\\rho = 0.1$, $\\tau = 0.5$，SCAD，$a = 3.7$。\n- 测试 $2$（理想情况，MCP）：$n = 8000$, $\\rho = 0.1$, $\\tau = 0.5$，MCP，$\\gamma = 3.0$。\n- 测试 $3$（边界情况，全零信号）：$n = 8000$, $\\rho = 0.0$, $\\tau = 1.0$，MCP，$\\gamma = 3.0$。\n- 测试 $4$（边缘情况，极稀疏低噪声，SCAD）：$n = 8000$, $\\rho = 0.01$, $\\tau = 0.2$，SCAD，$a = 3.7$。\n- 测试 $5$（边缘情况，较高噪声且较密集，MCP）：$n = 8000$, $\\rho = 0.2$, $\\tau = 1.5$，MCP，$\\gamma = 2.5$。\n\n5) 对于套件中的每个测试，输出一个浮点数，其值等于上面定义的相对次优性，该值是使用通过在指定网格上最小化任务 1 中的无偏估计量所选择的 $\\lambda$ 计算得出的。将所有测试的结果汇总到单行输出中，格式为逗号分隔的 Python 风格列表。也就是说，最终输出必须是形如 $[r_{1}, r_{2}, r_{3}, r_{4}, r_{5}]$ 的一行，其中 $r_{j}$ 是测试 $j$ 的相对次优性。\n\n不涉及物理单位。所有角度（如果出现）都应以弧度为单位，但此处未使用角度。你的程序必须是自包含的，使用指定的种子，并且不需要外部输入。所有测试用例的答案都是浮点数。你的程序应生成包含结果的单行输出，格式为方括号括起来的逗号分隔列表（例如，$[result1,result2,result3]$）。", "solution": "我们考虑高斯序列模型 $y = x_{0} + \\tau z \\in \\mathbb{R}^{n}$，其中 $z \\sim \\mathcal{N}(0, I_{n})$ 具有独立同分布（i.i.d.）的项，且 $\\tau  0$ 已知。我们研究逐坐标作用的可分离去噪器 $\\eta(y; \\theta)$，即 $\\eta(y; \\theta) = (\\eta_{1}(y_{1}; \\theta), \\ldots, \\eta_{n}(y_{n}; \\theta))$，并附加温和的正则性假设：逐坐标的弱可微性和可积性足以保证期望与微分运算可以交换。均方误差风险为 $R(\\eta) = \\frac{1}{n} \\mathbb{E}\\left[\\lVert \\eta(y;\\theta) - x_{0} \\rVert_{2}^{2}\\right]$。\n\n使用 Stein 引理推导无偏风险估计量，是从高斯随机变量的一个基本恒等式开始的。对于一个具有适当可积性的弱可微函数 $g: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$，Stein 引理指出 $\\mathbb{E}[z^{\\top} g(x_{0} + \\tau z)] = \\tau \\,\\mathbb{E}[\\mathrm{div}\\, g(x_{0} + \\tau z)]$，其中 $\\mathrm{div}\\, g = \\sum_{i=1}^{n} \\frac{\\partial g_{i}}{\\partial y_{i}}$。令 $g(y) = \\eta(y;\\theta) - y$，我们计算风险：\n\n$$\n\\begin{aligned}\nR(\\eta) = \\frac{1}{n} \\mathbb{E}\\left[\\lVert \\eta(y;\\theta) - x_{0} \\rVert_{2}^{2}\\right] \\\\\n= \\frac{1}{n} \\mathbb{E}\\left[\\lVert \\eta(y;\\theta) - y + y - x_{0} \\rVert_{2}^{2}\\right] \\\\\n= \\frac{1}{n} \\mathbb{E}\\left[\\lVert \\eta(y;\\theta) - y \\rVert_{2}^{2}\\right] + \\frac{2}{n} \\mathbb{E}\\left[(\\eta(y;\\theta) - y)^{\\top} (y - x_{0})\\right] + \\frac{1}{n} \\mathbb{E}\\left[\\lVert y - x_{0} \\rVert_{2}^{2}\\right].\n\\end{aligned}\n$$\n\n注意到 $\\mathbb{E}\\left[\\lVert y - x_{0} \\rVert_{2}^{2}\\right] = n \\tau^{2}$，因为 $y - x_{0} = \\tau z$。此外，由于 $y - x_{0} = \\tau z$，我们有\n\n$$\n\\mathbb{E}\\left[(\\eta(y;\\theta) - y)^{\\top} (y - x_{0})\\right] = \\tau \\,\\mathbb{E}\\left[z^{\\top} (\\eta(y;\\theta) - y)\\right].\n$$\n\n应用 Stein 引理，设 $g(y) = \\eta(y;\\theta) - y$，可得\n\n$$\n\\mathbb{E}\\left[z^{\\top} (\\eta(y;\\theta) - y)\\right] = \\tau \\,\\mathbb{E}\\left[\\mathrm{div}\\,\\eta(y;\\theta) - n\\right],\n$$\n\n因为 $\\mathrm{div}\\,(\\eta - y) = \\mathrm{div}\\,\\eta - \\mathrm{div}\\,y = \\mathrm{div}\\,\\eta - n$。因此\n\n$$\n\\mathbb{E}\\left[(\\eta(y;\\theta) - y)^{\\top} (y - x_{0})\\right] = \\tau^{2} \\,\\mathbb{E}\\left[\\mathrm{div}\\,\\eta(y;\\theta) - n\\right].\n$$\n\n合并各项，我们得到\n\n$$\nR(\\eta) = \\frac{1}{n} \\mathbb{E}\\left[\\lVert \\eta(y;\\theta) - y \\rVert_{2}^{2}\\right] + \\frac{2 \\tau^{2}}{n} \\mathbb{E}\\left[\\mathrm{div}\\,\\eta(y;\\theta) - n\\right] + \\tau^{2}.\n$$\n\n整理后，我们得到\n\n$$\nR(\\eta) = \\mathbb{E}\\left[\\frac{1}{n} \\lVert \\eta(y;\\theta) - y \\rVert_{2}^{2} + \\frac{2 \\tau^{2}}{n} \\mathrm{div}\\,\\eta(y;\\theta) - \\tau^{2}\\right].\n$$\n\n这表明随机变量\n\n$$\n\\mathrm{SURE}(y; \\theta, \\tau) = \\frac{1}{n} \\lVert \\eta(y;\\theta) - y \\rVert_{2}^{2} + \\frac{2 \\tau^{2}}{n} \\mathrm{div}\\,\\eta(y;\\theta) - \\tau^{2}\n$$\n\n是 $R(\\eta)$ 的一个无偏估计量，即 $\\mathbb{E}[\\mathrm{SURE}(y; \\theta, \\tau)] = R(\\eta)$。此推导仅依赖于 Stein 引理和基本展开，其中 $\\eta$ 的正则性确保了散度几乎处处有定义且期望存在。\n\n我们现在推导 SCAD 和 MCP 去噪器及其导数的显式逐坐标形式。这些是最小化可分离惩罚最小二乘问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\lVert x - y \\rVert_{2}^{2} + \\sum_{i=1}^{n} \\rho_{\\lambda}(|x_{i}|)$ 所产生的邻近算子，其中 $\\rho_{\\lambda}$ 是 SCAD 或 MCP 惩罚项。\n\n对于形状参数为 $\\gamma  1$ 的 MCP，当 $t \\ge 0$ 时，惩罚项的导数为 $\\rho_{\\lambda}'(t) = (\\lambda - t/\\gamma)_{+}$。得到的邻近去噪器，通常称为 firm 阈值算子，对每个坐标 $y_{i}$ 的分段定义如下：\n\n$$\n\\eta_{\\mathrm{MCP}}(y_{i}; \\lambda, \\gamma) =\n\\begin{cases}\n0,  \\text{if } |y_{i}| \\le \\lambda, \\\\\n\\mathrm{sign}(y_{i}) \\,\\frac{\\gamma}{\\gamma - 1} \\left(|y_{i}| - \\lambda\\right),  \\text{if } \\lambda  |y_{i}| \\le \\gamma \\lambda, \\\\\ny_{i},  \\text{if } |y_{i}| > \\gamma \\lambda.\n\\end{cases}\n$$\n\n其关于 $y_{i}$ 的逐坐标导数几乎处处存在，且等于\n\n$$\n\\frac{\\partial}{\\partial y_{i}} \\eta_{\\mathrm{MCP}}(y_{i}; \\lambda, \\gamma) =\n\\begin{cases}\n0,  \\text{if } |y_{i}|  \\lambda, \\\\\n\\frac{\\gamma}{\\gamma - 1},  \\text{if } \\lambda  |y_{i}|  \\gamma \\lambda, \\\\\n1,  \\text{if } |y_{i}| > \\gamma \\lambda,\n\\end{cases}\n$$\n\n在阈值 $|y_{i}| \\in \\{\\lambda, \\gamma \\lambda\\}$ 处的值可取为次梯度中的任意值，在高斯模型下，这些测度为零的点几乎不影响散度的计算。\n\n对于形状参数为 $a  2$ 的 SCAD，当 $t \\ge 0$ 时，惩罚项的导数为：当 $0 \\le t \\le \\lambda$ 时，$\\rho_{\\lambda}'(t) = \\lambda$；当 $\\lambda  t \\le a \\lambda$ 时，$\\rho_{\\lambda}'(t) = \\frac{a \\lambda - t}{a - 1}$；当 $t  a \\lambda$ 时，$\\rho_{\\lambda}'(t) = 0$。得到的邻近去噪器对每个坐标 $y_{i}$ 的分段定义如下：\n\n$$\n\\eta_{\\mathrm{SCAD}}(y_{i}; \\lambda, a) =\n\\begin{cases}\n\\mathrm{sign}(y_{i})(|y_{i}| - \\lambda)_+,  \\text{if } |y_{i}| \\le 2 \\lambda, \\\\\n\\frac{(a - 1) y_{i} - \\mathrm{sign}(y_{i}) a \\lambda}{a - 2},  \\text{if } 2 \\lambda  |y_{i}| \\le a \\lambda, \\\\\ny_{i},  \\text{if } |y_{i}| > a \\lambda.\n\\end{cases}\n$$\n\n其关于 $y_{i}$ 的逐坐标导数几乎处处存在，且等于\n\n$$\n\\frac{\\partial}{\\partial y_{i}} \\eta_{\\mathrm{SCAD}}(y_{i}; \\lambda, a) =\n\\begin{cases}\n1,  \\text{if } \\lambda  |y_{i}|  2 \\lambda, \\\\\n\\frac{a - 1}{a - 2},  \\text{if } 2 \\lambda  |y_{i}|  a \\lambda, \\\\\n1,  \\text{if } |y_{i}| > a \\lambda, \\\\\n0,  \\text{if } |y_{i}|  \\lambda.\n\\end{cases}\n$$\n\n在阈值 $|y_{i}| \\in \\{\\lambda, 2 \\lambda, a \\lambda\\}$ 处的值可取为次梯度中的任意值，几乎必然不影响散度。注意，在 SCAD 的中间区域，斜率超过 1，这反映了惩罚项的凹性，该凹性抵消了收缩效应。\n\n给定这些显式形式及其导数，可分离去噪器的散度为 $\\mathrm{div}\\,\\eta(y;\\theta) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_{i}} \\eta_{i}(y_{i}; \\theta)$，通过对由 $|y_{i}|$ 所在区域决定的逐坐标斜率求和来计算。\n\n为了通过 Stein 无偏风险估计（SURE）选择调节参数 $\\lambda$，我们对一个网格 $\\Lambda = \\{\\lambda_{1}, \\ldots, \\lambda_{m}\\}$，评估无偏风险估计量\n\n$$\n\\mathrm{SURE}(y; \\lambda, \\tau) = \\frac{1}{n} \\lVert \\eta(y;\\lambda) - y \\rVert_{2}^{2} + \\frac{2 \\tau^{2}}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_{i}} \\eta_{i}(y_{i}; \\lambda) - \\tau^{2},\n$$\n\n并选择 $\\widehat{\\lambda}_{\\mathrm{SURE}} \\in \\arg\\min_{\\lambda \\in \\Lambda} \\mathrm{SURE}(y; \\lambda, \\tau)$。然后我们为 $\\lambda \\in \\Lambda$ 计算经验均方误差 $\\mathrm{MSE}(\\lambda) = \\frac{1}{n} \\lVert \\eta(y;\\lambda) - x_{0} \\rVert_{2}^{2}$，并报告相对次优性\n\n$$\nr = \\frac{\\mathrm{MSE}(\\widehat{\\lambda}_{\\mathrm{SURE}}) - \\min_{\\lambda \\in \\Lambda} \\mathrm{MSE}(\\lambda)}{\\min_{\\lambda \\in \\Lambda} \\mathrm{MSE}(\\lambda)}.\n$$\n\n在高斯模型和我们的正则性假设下，$\\mathbb{E}[\\mathrm{SURE}(y; \\lambda, \\tau)] = R(\\eta(\\cdot;\\lambda))$，因此最小化 SURE 倾向于选择一个接近于最小化真实风险的调节参数。该测试套件探究了典型情况和边缘情况：中等稀疏度和噪声、全零信号的边界情况、极稀疏低噪声条件，以及较高噪声的中等密度信号。我们将随机种子固定为 $12345$ 以确保可复现性。对于每个测试，网格 $\\Lambda$ 设置为从 $0$ 到 $3 \\tau$（含端点）的 61 个均匀间隔点。\n\n最终的程序以向量化方式实现 SCAD 和 MCP 去噪器及其导数，以计算 SURE 值并对 $\\lambda$ 进行网格搜索。对于每个指定的测试，它报告一个等于相对次优性 $r$ 的浮点数，并按要求打印列表 $[r_{1}, r_{2}, r_{3}, r_{4}, r_{5}]$。", "answer": "```python\nimport numpy as np\n\ndef scad_shrink(y, lam, a):\n    \"\"\"\n    SCAD proximal denoiser and its derivative (coordinate-wise), vectorized.\n    Parameters:\n        y: np.ndarray\n        lam: float\n        a: float  2\n    Returns:\n        eta: np.ndarray, same shape as y\n        deriv: np.ndarray, same shape as y (coordinate-wise derivative wrt y)\n    \"\"\"\n    abs_y = np.abs(y)\n    sign_y = np.sign(y)\n    eta = np.empty_like(y)\n    deriv = np.zeros_like(y)\n\n    if lam == 0.0:\n        # With lambda=0, SCAD reduces to identity almost everywhere\n        eta[:] = y\n        deriv[:] = 1.0\n        # At y == 0, derivative is not well-defined; set to 0 for measure-zero stability\n        deriv[y == 0.0] = 0.0\n        return eta, deriv\n\n    # Regions\n    m1 = abs_y = lam\n    m2 = (abs_y > lam)  (abs_y = 2.0 * lam)\n    m3 = (abs_y > 2.0 * lam)  (abs_y = a * lam)\n    m4 = abs_y > a * lam\n\n    # Region 1: zero\n    eta[m1] = 0.0\n    deriv[m1] = 0.0\n\n    # Region 2: soft-thresholding\n    eta[m2] = sign_y[m2] * (abs_y[m2] - lam)\n    deriv[m2] = 1.0\n\n    # Region 3: linear rule\n    eta[m3] = ((a - 1.0) * y[m3] - sign_y[m3] * a * lam) / (a - 2.0)\n    deriv[m3] = (a - 1.0) / (a - 2.0)\n\n    # Region 4: identity\n    eta[m4] = y[m4]\n    deriv[m4] = 1.0\n\n    return eta, deriv\n\n\ndef mcp_shrink(y, lam, gamma):\n    \"\"\"\n    MCP proximal denoiser and its derivative (coordinate-wise), vectorized.\n    Parameters:\n        y: np.ndarray\n        lam: float\n        gamma: float  1\n    Returns:\n        eta: np.ndarray\n        deriv: np.ndarray\n    \"\"\"\n    abs_y = np.abs(y)\n    sign_y = np.sign(y)\n    eta = np.empty_like(y)\n    deriv = np.zeros_like(y)\n\n    if lam == 0.0:\n        # With lambda=0, MCP reduces to identity almost everywhere\n        eta[:] = y\n        deriv[:] = 1.0\n        deriv[y == 0.0] = 0.0\n        return eta, deriv\n\n    # Regions\n    m1 = abs_y = lam\n    m2 = (abs_y > lam)  (abs_y = gamma * lam)\n    m3 = abs_y > gamma * lam\n\n    # Region 1: zero\n    eta[m1] = 0.0\n    deriv[m1] = 0.0\n\n    # Region 2: firm-threshold slope gamma/(gamma-1)\n    c = gamma / (gamma - 1.0)\n    eta[m2] = sign_y[m2] * c * (abs_y[m2] - lam)\n    deriv[m2] = c\n\n    # Region 3: identity\n    eta[m3] = y[m3]\n    deriv[m3] = 1.0\n\n    return eta, deriv\n\n\ndef sure(y, tau, shrinker, params):\n    \"\"\"\n    Compute SURE for a given denoiser applied to y = x0 + tau*z.\n    Parameters:\n        y: np.ndarray\n        tau: float\n        shrinker: callable(y, **params) - (eta, deriv)\n        params: dict with shrinker parameters (e.g., {'lam':..., 'a':...} or {'lam':..., 'gamma':...})\n    Returns:\n        sure_value: float\n        eta: np.ndarray\n    \"\"\"\n    eta, deriv = shrinker(y, **params)\n    n = y.size\n    resid = eta - y\n    div = np.sum(deriv)\n    sure_val = (np.sum(resid * resid) / n) + (2.0 * (tau ** 2) * div / n) - (tau ** 2)\n    return sure_val, eta\n\n\ndef empirical_mse(eta, x0):\n    n = x0.size\n    return np.sum((eta - x0) ** 2) / n\n\n\ndef select_lambda_via_sure(y, tau, x0, penalty, shape_param, lam_grid):\n    \"\"\"\n    Grid-search lambda to minimize SURE; compute relative suboptimality compared to oracle MSE over grid.\n    Parameters:\n        y: np.ndarray\n        tau: float\n        x0: np.ndarray\n        penalty: 'SCAD' or 'MCP'\n        shape_param: dict containing 'a' for SCAD or 'gamma' for MCP\n        lam_grid: np.ndarray of lambda values\n    Returns:\n        rel_subopt: float\n    \"\"\"\n    n = y.size\n    m = lam_grid.size\n    sures = np.empty(m, dtype=float)\n    mses = np.empty(m, dtype=float)\n\n    if penalty == 'SCAD':\n        a = shape_param['a']\n        shrinker = lambda yy, lam: scad_shrink(yy, lam, a)\n    elif penalty == 'MCP':\n        gamma = shape_param['gamma']\n        shrinker = lambda yy, lam: mcp_shrink(yy, lam, gamma)\n    else:\n        raise ValueError(\"Unknown penalty\")\n\n    best_sure = None\n    best_eta_for_sure = None\n    best_lam_index = None\n\n    # Evaluate over grid\n    for idx, lam in enumerate(lam_grid):\n        s_val, eta = sure(y, tau, lambda yy, lam=lam: shrinker(yy, lam), {'lam': lam})\n        sures[idx] = s_val\n        mses[idx] = empirical_mse(eta, x0)\n        # Track best SURE\n        if (best_sure is None) or (s_val  best_sure):\n            best_sure = s_val\n            best_eta_for_sure = eta\n            best_lam_index = idx\n\n    mse_sel = mses[best_lam_index]\n    mse_min = float(np.min(mses))\n    # Relative suboptimality\n    eps = 1e-12\n    rel = (mse_sel - mse_min) / max(mse_min, eps)\n    # Ensure non-negative due to numerical errors\n    rel = float(max(rel, 0.0))\n    return rel\n\n\ndef generate_signal_and_observation(rng, n, rho, tau):\n    \"\"\"\n    Generate x0 with floor(rho*n) nonzeros ~ N(0,1) at random positions, and y = x0 + tau*z.\n    \"\"\"\n    x0 = np.zeros(n, dtype=float)\n    k = int(np.floor(rho * n))\n    if k > 0:\n        idx = rng.choice(n, size=k, replace=False)\n        x0_vals = rng.standard_normal(k)\n        x0[idx] = x0_vals\n    z = rng.standard_normal(n)\n    y = x0 + tau * z\n    return x0, y\n\n\ndef solve():\n    rng = np.random.default_rng(12345)\n\n    test_cases = [\n        # Test 1: SCAD, n=8000, rho=0.1, tau=0.5, a=3.7\n        {'penalty': 'SCAD', 'n': 8000, 'rho': 0.1, 'tau': 0.5, 'a': 3.7},\n        # Test 2: MCP, n=8000, rho=0.1, tau=0.5, gamma=3.0\n        {'penalty': 'MCP', 'n': 8000, 'rho': 0.1, 'tau': 0.5, 'gamma': 3.0},\n        # Test 3: MCP, all-zero signal, n=8000, rho=0.0, tau=1.0, gamma=3.0\n        {'penalty': 'MCP', 'n': 8000, 'rho': 0.0, 'tau': 1.0, 'gamma': 3.0},\n        # Test 4: SCAD, very sparse low noise, n=8000, rho=0.01, tau=0.2, a=3.7\n        {'penalty': 'SCAD', 'n': 8000, 'rho': 0.01, 'tau': 0.2, 'a': 3.7},\n        # Test 5: MCP, higher noise and denser, n=8000, rho=0.2, tau=1.5, gamma=2.5\n        {'penalty': 'MCP', 'n': 8000, 'rho': 0.2, 'tau': 1.5, 'gamma': 2.5},\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case['n']\n        rho = case['rho']\n        tau = case['tau']\n        penalty = case['penalty']\n        if penalty == 'SCAD':\n            shape_param = {'a': case['a']}\n        else:\n            shape_param = {'gamma': case['gamma']}\n\n        x0, y = generate_signal_and_observation(rng, n, rho, tau)\n        lam_grid = np.linspace(0.0, 3.0 * tau, 61)\n\n        rel = select_lambda_via_sure(y, tau, x0, penalty, shape_param, lam_grid)\n        results.append(rel)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3462670"}, {"introduction": "在掌握了风险估计之后，一个更高级的问题是如何高效地优化超参数。本练习将引导你超越简单的网格搜索，实现一种基于梯度的先进调优策略。你将探索Stein无偏梯度风险估计器（SUGAR）这一强大工具，通过它来计算SURE关于正则化参数$\\lambda$的梯度，并实现一个追踪最优$\\lambda$演化路径的连续化方法。这个实践不仅能让你掌握一种更高效的调优算法，还能加深对非凸稀疏恢复中解路径、分岔点等核心概念的理解。[@problem_id:3482335]", "problem": "考虑稀疏去噪模型，其中信号 $x_0 \\in \\mathbb{R}^n$ 通过加性高斯噪声 $w \\sim \\mathcal{N}(0,\\sigma^2 I_n)$ 进行观测，得到 $y = x_0 + w$。设估计量 $\\mu(y; \\lambda, \\gamma)$ 定义为目标函数 $J(x; y, \\lambda, \\gamma) = \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\sum_{i=1}^n p_{\\lambda,\\gamma}(|x_i|)$ 的任意最小化子，其中 $p_{\\lambda,\\gamma}(\\cdot)$ 是最小最大凹惩罚 (Minimax Concave Penalty, MCP)，其正则化尺度 $\\lambda  0$，非凸性参数 $\\gamma  1$。MCP 定义为 $p_{\\lambda,\\gamma}(t) = \\lambda \\int_0^t \\max\\{0, 1 - \\tfrac{s}{\\gamma \\lambda}\\} \\, ds$，这是稀疏优化中一个经过充分检验的非凸惩罚项。\n\n从高斯噪声建模的基本事实和关于高斯散度的 Stein 引理出发，推导出一个仅依赖于 $y$、$\\sigma$ 和 $\\mu(y; \\lambda, \\gamma)$ 相对于 $y$ 的散度的无偏风险估计。然后，使用 Stein 无偏梯度风险估计器 (Stein's Unbiased GRAdient risk estimator, SUGAR)，通过高斯探测进行散度的蒙特卡洛迹估计，推导该风险估计关于 $\\lambda$ 的梯度的可计算表达式。将 $\\mu(y; \\lambda, \\gamma)$ 视为一个逐元素作用的可分离估计量，并包含该估计量关于数据 $y$ 和参数 $\\lambda$ 所需的任何偏导数。您不得依赖任何关于最终风险或梯度的简化公式；相反，您的推导必须基于所述的定义和性质。\n\n使用这些推导，实现一个连续路径 $\\lambda(t)$，$t \\in [0,1]$，该路径由离散步长 $t_k = \\tfrac{k}{T}$ 参数化，其中 $T$ 为一个固定的正整数。将 $\\lambda(0)$ 初始化为一个依赖于数据的较大值，并使用 SUGAR 梯度，通过对关于 $\\lambda$ 的无偏风险估计进行梯度下降来演化 $\\lambda(t)$。在每一步中，将 $\\lambda(t)$ 裁剪到一个正区间内，以确保估计量是良定义的。量化以下路径分析量：\n- 正则性统计量，定义为路径上的最大绝对步长 $\\max_k |\\lambda(t_{k+1}) - \\lambda(t_k)|$。\n- 沿路径的分岔事件数，定义为索引 $k$ 的数量，在这些索引处 $\\mu(y; \\lambda(t_k), \\gamma)$ 的支撑集大小相对于 $\\mu(y; \\lambda(t_{k-1}), \\gamma)$ 发生了变化。\n- 一个布尔指示器，指示当 $t$ 增加时，支撑集大小是否沿路径单调非递减（将 $\\lambda(t)$ 解释为稀疏恢复中的递减同伦参数）。\n\n对于每个测试用例，您的程序必须执行以下任务：\n1. 生成一个 $k$-稀疏的基准真相 $x_0 \\in \\mathbb{R}^n$，其非零项独立地从标准正态分布中抽取，并放置在均匀随机的索引处。\n2. 使用测试用例中为可复现性而提供的固定伪随机种子，生成 $y = x_0 + w$ 其中 $w \\sim \\mathcal{N}(0,\\sigma^2 I_n)$。\n3. 通过所述优化目标产生的逐元素 MCP 邻近映射，实现估计量 $\\mu(y; \\lambda, \\gamma)$。\n4. 推导并实现 Stein 无偏风险估计，用 $y$、$\\sigma$ 和 $\\mu$ 相对于 $y$ 的散度表示。\n5. 使用高斯探测和小的有限差分参数进行蒙特卡洛迹估计，推导并实现 SUGAR 梯度 $\\tfrac{\\partial}{\\partial \\lambda} \\mathrm{SURE}(y; \\lambda, \\gamma)$。\n6. 使用 SUGAR 梯度，通过梯度下降追踪连续路径 $\\lambda(t)$ 共 $T$ 步，并使用指定的步长和裁剪边界。\n7. 对每个测试用例，计算并返回一个元组，其中包含最终值 $\\lambda(1)$、分岔事件数、正则性统计量以及转换为整数的单调性指示器（$1$ 表示真，$0$ 表示假）。\n\n如果您引入任何角度，必须使用弧度，尽管此处预计不会有。不涉及任何物理单位。所有数值输出都必须是无量纲数。\n\n测试套件：\n对于以下每个测试用例，您的程序必须使用指定的参数和随机种子。\n\n- 测试用例 A (理想路径，中等非凸性)：\n  - $n = 80$，$k = 8$，$\\sigma = 0.15$，$\\gamma = 3.0$，$T = 120$，步长 $\\alpha = 0.005$，种子 $s = 1234$。\n- 测试用例 B (边缘情况，强非凸性)：\n  - $n = 80$，$k = 8$，$\\sigma = 0.15$，$\\gamma = 1.2$，$T = 120$，步长 $\\alpha = 0.001$，种子 $s = 5678$。\n- 测试用例 C (边界情况，近似于最小绝对收缩和选择算子 (LASSO) 的近凸惩罚)：\n  - $n = 80$，$k = 8$，$\\sigma = 0.05$，$\\gamma = 100.0$，$T = 120$，步长 $\\alpha = 0.010$，种子 $s = 9012$。\n\n初始化和边界：\n- 将 $\\lambda(0)$ 初始化为 $\\lambda_{\\mathrm{max}} = \\max_{i} |y_i|$。\n- 将 $\\lambda(t)$ 裁剪到区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 内，其中 $\\lambda_{\\min} = 10^{-6}$ 且 $\\lambda_{\\max}$ 如上所述。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为逗号分隔的 Python 风格列表的列表，每个内部列表按上述顺序包含四个值。例如：\"[[lambda_A,bifurcations_A,regularity_A,monotone_A],[lambda_B,bifurcations_B,regularity_B,monotone_B],[lambda_C,bifurcations_C,regularity_C,monotone_C]]\"。第一个和第三个分量应提供为浮点数，第二个和第四个分量应提供为整数。", "solution": "该问题要求在一个稀疏信号去噪的背景下，为最小最大凹惩罚 (MCP) 推导并实现一个参数调整方案。任务的核心是使用 Stein 无偏风险估计 (SURE) 及其梯度（通过使用蒙特卡洛技术的 Stein 无偏梯度风险估计器 (SUGAR) 进行估计），来为正则化参数 $\\lambda$ 定义一个连续路径。\n\n首先，我们形式化估计量 $\\mu(y; \\lambda, \\gamma)$。需要最小化的目标函数是 $J(x; y, \\lambda, \\gamma) = \\tfrac{1}{2}\\lVert x - y \\rVert_2^2 + \\sum_{i=1}^n p_{\\lambda,\\gamma}(|x_i|)$。该目标函数是可分离的，意味着我们可以通过独立地最小化每个分量 $x_i$ 来找到最优的 $x = (x_1, \\dots, x_n)$：\n$$\n\\hat{x}_i = \\arg\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(x_i - y_i)^2 + p_{\\lambda,\\gamma}(|x_i|) \\right\\}\n$$\n根据定义，这是函数 $p_{\\lambda,\\gamma}(|\\cdot|)$ 应用于 $y_i$ 的邻近算子。令 $\\mu_i(y_i; \\lambda, \\gamma)$ 表示此解。对于 $t > 0$，MCP 惩罚项 $p_{\\lambda,\\gamma}(t)$ 的导数是 $p'_{\\lambda,\\gamma}(t) = \\lambda \\max\\{0, 1 - \\tfrac{t}{\\gamma \\lambda}\\}$。一阶最优性条件是 $0 \\in x_i - y_i + \\partial p_{\\lambda,\\gamma}(|x_i|)$，其中 $\\partial$ 表示次梯度。解出 $x_i$ 可得逐元素的估计量，它是一个稳固阈值函数 (firm-thresholding function)：\n$$\n\\mu_i(y_i; \\lambda, \\gamma) = \\begin{cases} 0  \\text{if } |y_i| \\le \\lambda \\\\ \\mathrm{sgn}(y_i) \\frac{\\gamma(|y_i| - \\lambda)}{\\gamma - 1}  \\text{if } \\lambda  |y_i| \\le \\gamma \\lambda \\\\ y_i  \\text{if } |y_i| > \\gamma \\lambda \\end{cases}\n$$\n该函数是可分离的，即 $\\mu_i$ 仅依赖于 $y_i$。\n\n接下来，我们推导无偏风险估计。风险，或期望均方误差 (MSE)，是 $R(\\lambda) = \\mathbb{E}[\\lVert \\mu(y; \\lambda) - x_0 \\rVert_2^2]$。我们可以展开此式，并利用 $y = x_0 + w$ 且 $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 的事实：\n$$\n\\begin{aligned}\nR(\\lambda) = \\mathbb{E}[\\lVert (\\mu(y) - y) + (y - x_0) \\rVert_2^2] \\\\\n= \\mathbb{E}[\\lVert \\mu(y) - y \\rVert_2^2 + \\lVert y - x_0 \\rVert_2^2 + 2(y-x_0)^T(\\mu(y) - y)] \\\\\n= \\mathbb{E}[\\lVert \\mu(y) - y \\rVert_2^2] + \\mathbb{E}[\\lVert w \\rVert_2^2] + 2\\mathbb{E}[w^T(\\mu(y) - y)]\n\\end{aligned}\n$$\n第二项是 $\\mathbb{E}[\\lVert w \\rVert_2^2] = n\\sigma^2$。对于第三项，我们应用 Stein 引理，该引理指出，对于随机向量 $w \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 和一个弱可微函数 $h: \\mathbb{R}^n \\to \\mathbb{R}^n$，有 $\\mathbb{E}[w^T h(y)] = \\sigma^2 \\mathbb{E}[\\nabla_y \\cdot h(y)]$。令 $h(y) = \\mu(y) - y$。其散度为 $\\nabla_y \\cdot h(y) = \\nabla_y \\cdot \\mu(y) - \\nabla_y \\cdot y = (\\sum_{i=1}^n \\frac{\\partial \\mu_i}{\\partial y_i}) - n$。\n代回原式，我们得到：\n$$\nR(\\lambda) = \\mathbb E\\left[\\lVert\\mu(y) - y\\rVert_2^2 + n\\sigma^2 + 2\\sigma^2 (\\nabla_y \\cdot \\mu(y) - n) \\right] = \\mathbb E\\left[\\lVert\\mu(y) - y\\rVert_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla_y \\cdot \\mu(y) \\right]\n$$\n通过去掉期望，我们得到 $R(\\lambda)$ 的一个无偏估计量，即 Stein 无偏风险估计 (SURE)：\n$$\n\\mathrm{SURE}(y; \\lambda) = \\lVert\\mu(y) - y\\rVert_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla_y \\cdot \\mu(y)\n$$\n其中散度项为 $\\nabla_y \\cdot \\mu(y) = \\sum_{i=1}^n \\frac{\\partial \\mu_i(y_i)}{\\partial y_i}$。导数 $\\frac{\\partial \\mu_i}{\\partial y_i}$ 是分段常数：\n$$\n\\frac{\\partial \\mu_i(y_i)}{\\partial y_i} = \\begin{cases} 0  \\text{if } |y_i|  \\lambda \\\\ \\frac{\\gamma}{\\gamma-1}  \\text{if } \\lambda  |y_i|  \\gamma\\lambda \\\\ 1  \\text{if } |y_i| > \\gamma\\lambda \\end{cases}\n$$\n不连续点的测度为零，不影响 Stein 引理中的积分。\n\n下一步是求风险关于 $\\lambda$ 的梯度 $\\frac{\\partial R}{\\partial \\lambda}$，以执行梯度下降。对 SURE 公式进行朴素的微分是有问题的，因为它涉及到不连续函数 $\\frac{\\partial \\mu_i}{\\partial y_i}$ 的导数。SUGAR 框架为真实风险梯度提供了一个无偏估计量。风险的梯度是：\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\mathbb{E}[\\lVert \\mu(y; \\lambda) - x_0 \\rVert_2^2] = \\mathbb{E}\\left[2(\\mu - x_0)^T \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\n使用 $x_0 = y - w$，我们有：\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\mathbb{E}\\left[2(\\mu - y)^T \\frac{\\partial \\mu}{\\partial \\lambda} + 2w^T \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\n对第二项应用 Stein 引理，其中 $h(y) = \\frac{\\partial \\mu}{\\partial \\lambda}(y; \\lambda)$，可得 $\\mathbb{E}[w^T \\frac{\\partial \\mu}{\\partial \\lambda}] = \\sigma^2 \\mathbb{E}[\\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}]$。因此：\n$$\n\\frac{\\partial R}{\\partial \\lambda} = \\mathbb{E}\\left[2(\\mu - y)^T \\frac{\\partial \\mu}{\\partial \\lambda} + 2\\sigma^2 \\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}\\right]\n$$\nSUGAR 梯度估计量 $\\hat{g}(\\lambda)$ 是期望内的项：\n$$\n\\hat{g}(\\lambda) = 2(\\mu(y; \\lambda) - y)^T \\frac{\\partial \\mu}{\\partial \\lambda}(y; \\lambda) + 2\\sigma^2 \\left(\\nabla_y \\cdot \\frac{\\partial \\mu}{\\partial \\lambda}\\right)(y; \\lambda)\n$$\n为了创建一个可计算的表达式，我们对偏导数和散度进行近似。关于 $\\lambda$ 的偏导数使用中心有限差分进行近似，步长很小，为 $\\epsilon_\\lambda$：\n$$\n\\frac{\\partial \\mu(y; \\lambda)}{\\partial \\lambda} \\approx \\frac{\\mu(y; \\lambda+\\epsilon_\\lambda) - \\mu(y; \\lambda-\\epsilon_\\lambda)}{2\\epsilon_\\lambda}\n$$\n散度项 $\\nabla_y \\cdot f(y) = \\mathrm{Tr}(J_f(y))$，其中 $f(y) = \\frac{\\partial \\mu}{\\partial \\lambda}$，使用蒙特卡洛方法和高斯探测向量 $\\delta \\sim \\mathcal{N}(0, I_n)$ 以及小步长 $\\epsilon_y$ 进行估计。散度估计如下：\n$$\n\\nabla_y \\cdot f(y) \\approx \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta) - f(y))\n$$\n结合这些近似，SUGAR 梯度估计量的实现如下：\n令 $f(z, \\lambda_{val}) = \\frac{\\mu(z; \\lambda_{val}+\\epsilon_\\lambda) - \\mu(z; \\lambda_{val}-\\epsilon_\\lambda)}{2\\epsilon_\\lambda}$。$\\hat{g}(\\lambda)$ 的第一项是 $T_1 = 2(\\mu(y; \\lambda) - y)^T f(y, \\lambda)$。散度估计为 $\\nabla_y \\cdot f \\approx \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta, \\lambda) - f(y, \\lambda))$。$\\hat{g}(\\lambda)$ 的第二项是 $T_2 = 2\\sigma^2 \\frac{1}{\\epsilon_y} \\delta^T (f(y+\\epsilon_y\\delta, \\lambda) - f(y, \\lambda))$。最终可计算的梯度是 $\\hat{g}(\\lambda) = T_1 + T_2$。这要求每次梯度计算需要评估估计量 $\\mu$ 四次。\n\n总体算法流程如下：\n1.  对于给定的测试用例，生成稀疏信号 $x_0$ 和带噪观测值 $y$。\n2.  初始化正则化参数 $\\lambda_0 = \\max_i |y_i|$。该值确保初始估计 $\\mu(y, \\lambda_0)$ 为零向量，这是同伦方法中一个常见的起点。\n3.  预先生成一个探测向量 $\\delta \\sim \\mathcal{N}(0, I_n)$，用于所有 SUGAR 梯度计算。\n4.  对 $k=0, \\dots, T-1$ 进行迭代：\n    a.  在当前参数值 $\\lambda_k = \\lambda(t_k)$ 处计算 SUGAR 梯度 $\\hat{g}(\\lambda_k)$。\n    b.  通过梯度下降更新参数：$\\lambda' = \\lambda_k - \\alpha \\hat{g}(\\lambda_k)$。\n    c.  将新值裁剪到区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 内以获得 $\\lambda_{k+1}$。\n5.  在 $T$ 步之后，分析生成的路径 $\\{\\lambda_k\\}_{k=0}^T$。\n    a.  对每个 $\\lambda_k$ 计算支撑集大小 $S_k = \\lVert\\mu(y; \\lambda_k)\\rVert_0$。\n    b.  分岔数是满足 $S_k \\neq S_{k-1}$ 的 $k \\in \\{1, \\dots, T\\}$ 的计数。\n    c.  正则性统计量是 $\\max_{k \\in \\{0, \\dots, T-1\\}} |\\lambda_{k+1} - \\lambda_k|$。\n    d.  如果对于所有 $k \\in \\{1, \\dots, T\\}$ 都有 $S_k \\ge S_{k-1}$，则单调性指示器为 $1$，否则为 $0$。\n6.  一个测试用例的最终结果是元组 $(\\lambda_T, \\text{分岔数}, \\text{正则性}, \\text{单调性})$。", "answer": "```python\nimport numpy as np\n\ndef mcp_prox(y, lam, gam):\n    \"\"\"\n    Computes the element-wise proximal operator of the MCP.\n    This corresponds to the estimator mu(y; lambda, gamma).\n    \"\"\"\n    if lam  0:\n        return np.full_like(y, np.nan)\n        \n    y_abs = np.abs(y)\n    y_sign = np.sign(y)\n    \n    # Handle the gamma - 1 case, which approaches hard thresholding\n    if np.isclose(gam, 1.0):\n        res = np.where(y_abs  lam, y, 0.0)\n        return res\n        \n    # Standard MCP for gamma  1\n    term1 = np.zeros_like(y)\n    term2 = y_sign * gam * (y_abs - lam) / (gam - 1.0)\n    term3 = y\n    \n    # Piecewise application\n    res = np.where(y_abs = lam, term1, term3)\n    res = np.where((y_abs  lam)  (y_abs = gam * lam), term2, res)\n    \n    return res\n\ndef compute_sugar_gradient(y, lam, gam, sigma, delta, eps_y, eps_lam, prox_func):\n    \"\"\"\n    Computes the SUGAR gradient of the risk with respect to lambda.\n    \"\"\"\n    # Finite difference approximation of d(mu)/d(lambda)\n    def grad_lam_mu_approx(vec_y, val_lam):\n        # Clip lambda for finite difference to avoid negative values\n        lam_plus = max(0.0, val_lam + eps_lam)\n        lam_minus = max(0.0, val_lam - eps_lam)\n        if lam_plus == lam_minus: # Avoid division by zero if eps_lam is too small or lam is at boundary\n             return np.zeros_like(vec_y)\n        mu_p = prox_func(vec_y, lam_plus, gam)\n        mu_m = prox_func(vec_y, lam_minus, gam)\n        return (mu_p - mu_m) / (2.0 * eps_lam)\n\n    # Compute d(mu)/d(lambda) at the current point y\n    grad_lam_mu_at_y = grad_lam_mu_approx(y, lam)\n    \n    # Compute mu at the current point\n    mu_at_y = prox_func(y, lam, gam)\n    \n    # First term of the SUGAR gradient\n    term1 = 2.0 * np.dot(mu_at_y - y, grad_lam_mu_at_y)\n    \n    # Second term (divergence) of the SUGAR gradient\n    # Compute d(mu)/d(lambda) at the perturbed point y + eps_y * delta\n    y_pert = y + eps_y * delta\n    grad_lam_mu_at_y_pert = grad_lam_mu_approx(y_pert, lam)\n    \n    # Estimate divergence using the Gaussian probe\n    div_est = (1.0 / eps_y) * np.dot(delta, grad_lam_mu_at_y_pert - grad_lam_mu_at_y)\n    term2 = 2.0 * sigma**2 * div_est\n    \n    return term1 + term2\n\ndef run_case(n, k, sigma, gam, T, alpha, seed):\n    \"\"\"\n    Runs a single test case for the MCP parameter path analysis.\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    \n    # 1. Generate data\n    x0 = np.zeros(n)\n    nonzero_indices = rng.choice(n, k, replace=False)\n    x0[nonzero_indices] = rng.randn(k)\n    \n    noise = rng.randn(n) * sigma\n    y = x0 + noise\n    \n    # 2. Initialization and bounds\n    lambda_max = np.max(np.abs(y))\n    lambda_min = 1e-6\n    \n    lambda_path = [lambda_max]\n    current_lambda = lambda_max\n    \n    # 3. Generate probe for SUGAR gradient\n    delta = rng.randn(n)\n    eps_y = 1e-6\n    eps_lam = 1e-6\n    \n    # 4. Track continuation path\n    for _ in range(T):\n        grad = compute_sugar_gradient(y, current_lambda, gam, sigma, delta, eps_y, eps_lam, mcp_prox)\n        \n        # Gradient descent step\n        current_lambda = current_lambda - alpha * grad\n        \n        # Clipping\n        current_lambda = np.clip(current_lambda, lambda_min, lambda_max)\n        \n        lambda_path.append(current_lambda)\n        \n    lambda_path = np.array(lambda_path)\n    \n    # 5. Path Analysis\n    # a. Final lambda\n    final_lambda = lambda_path[-1]\n    \n    # b. Regularity statistic\n    regularity_stat = np.max(np.abs(np.diff(lambda_path)))\n    \n    # c. Bifurcation events and Monotonicity\n    support_sizes = []\n    for lam in lambda_path:\n        mu = mcp_prox(y, lam, gam)\n        support_sizes.append(np.count_nonzero(mu))\n        \n    support_sizes = np.array(support_sizes)\n    support_diffs = np.diff(support_sizes)\n    \n    bifurcation_events = np.count_nonzero(support_diffs)\n    \n    # Monotonicity check: support must be non-decreasing as t increases (lambda decreases)\n    is_monotone = 1 if np.all(support_diffs >= 0) else 0\n\n    return final_lambda, bifurcation_events, regularity_stat, is_monotone\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, k, sigma, gamma, T, step_size_alpha, seed)\n        (80, 8, 0.15, 3.0, 120, 0.005, 1234),  # Test Case A\n        (80, 8, 0.15, 1.2, 120, 0.001, 5678),  # Test Case B\n        (80, 8, 0.05, 100.0, 120, 0.010, 9012) # Test Case C\n    ]\n\n    results = []\n    for case_params in test_cases:\n        final_lambda, bifurcations, regularity, monotone = run_case(*case_params)\n        results.append([final_lambda, bifurcations, regularity, int(monotone)])\n\n    # Format the final output string\n    inner_strings = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3482335"}]}