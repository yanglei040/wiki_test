{"hands_on_practices": [{"introduction": "本练习为理解为何强恢复阈值比弱恢复阈值更苛刻提供了理论基础。通过使用一个简化的失败概率模型和联合界，您将推导出一个描述该差距的解析表达式，这个过程将揭示该差距如何直接来源于可能信号支撑集数量的组合爆炸，即组合数 $\\binom{n}{k}$。这是一个旨在建立关键直觉的纸笔练习 [@problem_id:3494420]。", "problem": "考虑线性模型 $y = A x$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 的元素是独立同分布 (i.i.d.) 的标准正态随机变量，且 $x \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的。恢复通过凸优化的 $\\ell_{1}$-最小化方法进行。定义弱经验阈值 $m_{\\mathrm{w}}(\\varepsilon)$ 为最小的测量数量 $m$，使得对于一个大小为 $k$ 的固定支撑集，$\\ell_{1}$-最小化未能恢复 $x$ 的概率至多为 $\\varepsilon$。定义强经验阈值 $m_{\\mathrm{s}}(\\varepsilon)$ 为最小的测量数量 $m$，使得对于所有大小为 $k$ 的支撑集，存在至少一个 $k$-稀疏向量未被 $\\ell_{1}$-最小化恢复的概率至多为 $\\varepsilon$。\n\n假设一个经过充分检验的测度集中行为：对于任何大小为 $k$ 的固定支撑集，存在一个与 $n$、$m$ 和 $k$ 无关的常数 $\\beta > 0$，使得 $\\ell_{1}$-恢复的失败概率满足界 $\\mathbb{P}(\\text{在固定支撑集上失败}) \\leq \\exp(-\\beta m)$。从弱阈值和强阈值的定义、支撑集的组合计数 $\\binom{n}{k}$ 以及基本概率不等式出发，推导差距 $m_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon)$ 仅用 $n$、$k$ 和 $\\beta$ 表示的闭式表达式。然后，对于 $n = 5000$、$k = 250$ 和 $\\beta = 1$，通过使用斯特林近似以自然对数形式在第一个非平凡阶上近似 $\\ln \\binom{n}{k}$ 来评估此差距：\n$$\n\\ln \\binom{n}{k} \\approx n H(\\rho) - \\frac{1}{2} \\ln\\!\\big(2 \\pi n \\rho (1-\\rho)\\big),\n$$\n其中 $\\rho = k/n$，$H(\\rho) = - \\rho \\ln \\rho - (1-\\rho) \\ln(1-\\rho)$ 是以奈特为单位的二元熵。将您对 $m_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon)$ 的最终数值答案四舍五入到四位有效数字。将您的答案表示为一个没有单位的纯数。", "solution": "首先验证问题，以确保其科学上合理、良定且客观。\n\n### 步骤1：提取给定信息\n- **模型：** $y = Ax$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 具有独立同分布的标准正态项，且 $x \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的。\n- **恢复：** 通过 $\\ell_{1}$-最小化的凸优化。\n- **弱阈值：** $m_{\\mathrm{w}}(\\varepsilon)$ 是最小的 $m$，使得对于一个大小为 $k$ 的固定支撑集，$\\mathbb{P}(\\text{失败}) \\leq \\varepsilon$。\n- **强阈值：** $m_{\\mathrm{s}}(\\varepsilon)$ 是最小的 $m$，使得对于所有大小为 $k$ 的支撑集，$\\mathbb{P}(\\exists \\text{ 未恢复的 } k\\text{-稀疏 } x) \\leq \\varepsilon$。\n- **失败概率界：** 对于任何固定支撑集，$\\mathbb{P}(\\text{在固定支撑集上失败}) \\leq \\exp(-\\beta m)$，其中 $\\beta > 0$ 是一个常数。\n- **任务1：** 推导 $m_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon)$ 仅用 $n$、$k$ 和 $\\beta$ 表示的闭式表达式。\n- **任务2：** 计算当 $n = 5000$，$k = 250$ 和 $\\beta = 1$ 时的差距。\n- **近似：** 使用公式 $\\ln \\binom{n}{k} \\approx n H(\\rho) - \\frac{1}{2} \\ln(2 \\pi n \\rho (1-\\rho))$，其中 $\\rho = k/n$，$H(\\rho) = - \\rho \\ln \\rho - (1-\\rho) \\ln(1-\\rho)$。\n- **四舍五入：** 最终数值答案必须四舍五入到四位有效数字。\n\n### 步骤2：使用提取的给定信息进行验证\n该问题设定在压缩感知的标准理论框架内。弱恢复阈值和强恢复阈值的概念、随机高斯矩阵的使用、$\\ell_1$-最小化以及测度集中界的应用（表现为失败概率的指数衰减）在该领域都是公认的。该问题是自洽的，提供了所有必要的定义、常数和近似公式。目标明确，分为理论推导和数值计算两部分。要求仅用 $n$、$k$ 和 $\\beta$ 来表示差距，这似乎与阈值依赖于 $\\varepsilon$ 的定义相冲突，但初步分析表明 $\\varepsilon$ 在差值中被消除了，从而使问题保持一致。不存在科学或事实上的不健全之处，没有歧义，也没有缺失的信息。\n\n### 步骤3：结论与行动\n该问题被判定为**有效**。将提供完整的解答。\n\n### 差距解析表达式的推导\n问题的核心在于利用给定的失败概率界，形式上应用弱阈值和强阈值的定义。为了进行推导，我们将测量数量 $m$ 视为一个连续变量，这是在这种情况下寻找渐近行为的标准程序。\n\n弱阈值 $m_{\\mathrm{w}}(\\varepsilon)$ 对应于恢复具有单个固定支撑集的信号。根据定义，它是使失败概率至多为 $\\varepsilon$ 的最小 $m$。利用给定的界，我们将该界设为等于 $\\varepsilon$ 来找到阈值：\n$$\n\\exp(-\\beta m_{\\mathrm{w}}(\\varepsilon)) = \\varepsilon\n$$\n对两边取自然对数，得到：\n$$\n-\\beta m_{\\mathrm{w}}(\\varepsilon) = \\ln(\\varepsilon)\n$$\n解出 $m_{\\mathrm{w}}(\\varepsilon)$：\n$$\nm_{\\mathrm{w}}(\\varepsilon) = -\\frac{1}{\\beta} \\ln(\\varepsilon) = \\frac{1}{\\beta} \\ln\\left(\\frac{1}{\\varepsilon}\\right)\n$$\n\n强阈值 $m_{\\mathrm{s}}(\\varepsilon)$ 要求对所有大小为 $k$ 的可能支撑集都能成功恢复。对于一个维度为 $n$ 的向量，大小为 $k$ 的支撑集的总数由二项式系数 $\\binom{n}{k}$ 给出。令 $E_S$ 为特定支撑集 $S$ 的失败事件。在所有可能支撑集中至少发生一次失败的事件是这些单个失败事件的并集，即 $\\bigcup_{|S|=k} E_S$。我们可以使用并集界（也称为布尔不等式）来界定这个并集事件的概率：\n$$\n\\mathbb{P}\\left(\\bigcup_{|S|=k} E_S\\right) \\leq \\sum_{|S|=k} \\mathbb{P}(E_S)\n$$\n由于矩阵 $A$ 的元素是独立同分布的，所以对于任何大小为 $k$ 的支撑集，失败的概率是相同的。设这个共同的概率为 $p_{\\text{fail}} = \\mathbb{P}(\\text{在固定支撑集上失败})$。因此，总和简化为：\n$$\n\\mathbb{P}(\\text{强失败}) \\leq \\binom{n}{k} p_{\\text{fail}}\n$$\n使用给定的指数界 $p_{\\text{fail}} \\leq \\exp(-\\beta m)$，我们有：\n$$\n\\mathbb{P}(\\text{强失败}) \\leq \\binom{n}{k} \\exp(-\\beta m)\n$$\n强阈值 $m_{\\mathrm{s}}(\\varepsilon)$ 是使这个上界等于 $\\varepsilon$ 的最小 $m$：\n$$\n\\binom{n}{k} \\exp(-\\beta m_{\\mathrm{s}}(\\varepsilon)) = \\varepsilon\n$$\n为了解出 $m_{\\mathrm{s}}(\\varepsilon)$，我们再次取自然对数：\n$$\n\\ln\\left(\\binom{n}{k}\\right) - \\beta m_{\\mathrm{s}}(\\varepsilon) = \\ln(\\varepsilon)\n$$\n整理并解出 $m_{\\mathrm{s}}(\\varepsilon)$：\n$$\n\\beta m_{\\mathrm{s}}(\\varepsilon) = \\ln\\left(\\binom{n}{k}\\right) - \\ln(\\varepsilon) = \\ln\\left(\\binom{n}{k}\\right) + \\ln\\left(\\frac{1}{\\varepsilon}\\right)\n$$\n$$\nm_{\\mathrm{s}}(\\varepsilon) = \\frac{1}{\\beta} \\left( \\ln\\left(\\binom{n}{k}\\right) + \\ln\\left(\\frac{1}{\\varepsilon}\\right) \\right)\n$$\n现在，我们可以计算强阈值和弱阈值之间的差距：\n$$\nm_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon) = \\frac{1}{\\beta} \\left( \\ln\\left(\\binom{n}{k}\\right) + \\ln\\left(\\frac{1}{\\varepsilon}\\right) \\right) - \\frac{1}{\\beta} \\ln\\left(\\frac{1}{\\varepsilon}\\right)\n$$\n$$\nm_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon) = \\frac{1}{\\beta} \\ln\\left(\\binom{n}{k}\\right)\n$$\n这个差距的表达式仅依赖于 $n$、$k$ 和 $\\beta$，符合要求。它表示为防止在所有支撑集上发生失败所需的额外测量数量，这个成本是可能支撑集数量的对数。\n\n### 差距的数值评估\n我们被要求计算当 $n = 5000$，$k = 250$ 和 $\\beta = 1$ 时的差距。差距为：\n$$\nm_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon) = \\frac{1}{1} \\ln\\left(\\binom{5000}{250}\\right) = \\ln\\left(\\binom{5000}{250}\\right)\n$$\n我们使用所提供的斯特林近似来计算 $\\ln \\binom{n}{k}$：\n$$\n\\ln \\binom{n}{k} \\approx n H(\\rho) - \\frac{1}{2} \\ln(2 \\pi n \\rho (1-\\rho))\n$$\n首先，我们计算稀疏度比率 $\\rho$：\n$$\n\\rho = \\frac{k}{n} = \\frac{250}{5000} = \\frac{1}{20} = 0.05\n$$\n因此，$1-\\rho = 1 - 0.05 = 0.95$。\n\n接下来，我们计算二元熵项 $H(\\rho)$：\n$$\nH(\\rho) = - \\rho \\ln \\rho - (1-\\rho) \\ln(1-\\rho) = -0.05 \\ln(0.05) - 0.95 \\ln(0.95)\n$$\n使用自然对数：\n$$\n\\ln(0.05) \\approx -2.995732\n$$\n$$\n\\ln(0.95) \\approx -0.051293\n$$\n$$\nH(\\rho) \\approx -0.05(-2.995732) - 0.95(-0.051293) \\approx 0.1497866 + 0.0487286 \\approx 0.1985152\n$$\n近似的第一项是 $n H(\\rho)$：\n$$\nn H(\\rho) = 5000 \\times 0.1985152 \\approx 992.576\n$$\n\n现在我们计算修正项。对数的参数是：\n$$\n2 \\pi n \\rho (1-\\rho) = 2 \\pi (5000)(0.05)(0.95) = 2 \\pi (250)(0.95) = 2 \\pi (237.5) = 475 \\pi\n$$\n该值的对数是：\n$$\n\\ln(475 \\pi) = \\ln(475) + \\ln(\\pi) \\approx 6.163315 + 1.144730 \\approx 7.308045\n$$\n完整的修正项是：\n$$\n-\\frac{1}{2} \\ln(475 \\pi) \\approx -\\frac{1}{2}(7.308045) \\approx -3.654023\n$$\n\n最后，我们合并这些项以获得差距的值：\n$$\nm_{\\mathrm{s}}(\\varepsilon) - m_{\\mathrm{w}}(\\varepsilon) = \\ln\\left(\\binom{5000}{250}\\right) \\approx 992.576 - 3.654023 = 988.921977\n$$\n问题要求将最终答案四舍五入到四位有效数字。\n$$\n988.921977 \\approx 988.9\n$$", "answer": "$$\n\\boxed{988.9}\n$$", "id": "3494420"}, {"introduction": "从抽象理论转向具体矩阵实例，本练习挑战您实现两个关键程序。您将首先验证精确恢复条件（Exact Recovery Condition, ERC），这是一个针对*固定*支撑集的强保证；然后通过模拟随机选择支撑集的恢复过程，凭经验估计弱恢复阈值。这个练习将理论条件与给定传感矩阵的实际性能评估联系起来，帮助您理解理论在实践中的应用 [@problem_id:3494376]。", "problem": "给定一个固定的线性逆模型，其传感矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，支撑集为 $S \\subset \\{0,1,\\dots,n-1\\}$。您面临的是一个经典的稀疏恢复问题，即通过求解称为基追踪（Basis Pursuit）的凸规划，从无噪声数据 $y = A x$ 中恢复 $x \\in \\mathbb{R}^n$。您的任务是实现一个计算过程，对指定的支撑集进行精确恢复条件（ERC）的数值验证，并通过对随机支撑集和信号进行抽样，经验性地估计给定矩阵实例的恢复弱阈值。所有计算均为纯数学运算，不涉及任何物理单位。\n\n从以下基本定义和事实开始：\n- 对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，如果一个 $k$-稀疏向量 $x \\in \\mathbb{R}^n$ 恰好有 $k$ 个非零坐标，则满足 $\\|x\\|_0 = k$。对于无噪声测量 $y = A x$，凸规划基追踪（BP）是以下优化问题\n$$\n\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1 \\quad \\text{subject to} \\quad A z = y.\n$$\n- 对于一个大小为 $|S|=k$ 的支撑集 $S$，其精确恢复条件（ERC）定义为\n$$\n\\max_{j \\notin S} \\left\\| A_S^\\dagger a_j \\right\\|_1  1,\n$$\n其中 $A_S$ 是由 $S$ 索引的列构成的 $A$ 的子矩阵，$A_S^\\dagger$ 是 $A_S$ 的摩尔-彭若斯伪逆，$a_j$ 表示 $A$ 的第 $j$ 列。当这个严格不等式成立时，BP 可以恢复所有支撑在 $S$ 上且系数任意的 $k$-稀疏向量。\n- 对于一个固定的矩阵实例，经验性弱阈值在操作上定义如下。对于预设集合中的每个稀疏度 $k$，抽取多个独立的随机支撑集 $S$（在所有支撑集上均匀分布），其大小为 $k$，并在每个 $S$ 上抽取随机的非零系数（从一个关于零对称的连续分布中抽取）。对每个实例，生成 $y = A x$ 并尝试通过 BP 进行恢复。设稀疏度为 $k$ 时的经验成功率是试验中被精确恢复的比例。经验性弱阈值 $k_{\\mathrm{weak}}$ 是预设集合中使得经验成功率至少为 $0.5$（表示为小数）的最大 $k$。\n\n您的程序必须实现以下功能：\n1. 通过计算以下量值，为提供的 $(A,S)$ 对数值验证 ERC\n$$\n\\theta(S;A) \\triangleq \\max_{j \\notin S} \\left\\| A_S^\\dagger a_j \\right\\|_1,\n$$\n如果在一个由小的数值容差强化的严格不等式检查中 $\\theta(S;A)  1$，则声明 ERC 得到满足。如果 $A_S$ 不是满列秩，则声明 ERC 未得到满足。\n2. 通过以下方式经验性地估计给定 $A$ 的弱阈值 $k_{\\mathrm{weak}}$：\n   - 固定一个有限的候选稀疏度集合 $\\mathcal{K}$。\n   - 对于每个 $k \\in \\mathcal{K}$，执行 $T$ 次随机试验。在每次试验中，均匀随机地选择一个大小为 $k$ 的支撑集 $S$，在 $S$ 上独立地从标准正态分布中抽取系数，构成 $x$，计算 $y = A x$，并求解 BP。如果恢复出的 $\\hat{x}$ 的支撑集与 $S$ 匹配（在一个小的数值阈值内），并且相对误差 $\\|\\hat{x} - x\\|_2 / \\|x\\|_2$ 低于一个小的数值容差，则计为一次成功。经验性弱阈值是 $\\mathcal{K}$ 中其经验成功率至少为 $0.5$（作为小数）的最大 $k$。\n3. 以末尾指定的精确格式输出所有测试用例的结果。\n\n实现细节和约束：\n- 使用 BP 的线性规划重构形式，将任何 $z \\in \\mathbb{R}^n$ 写为 $z = u - v$，其中 $u,v \\in \\mathbb{R}^n$，$u \\ge 0$，$v \\ge 0$。那么 BP 等价于\n$$\n\\min_{u,v \\in \\mathbb{R}^n} \\mathbf{1}^\\top (u+v) \\quad \\text{subject to} \\quad A(u - v) = y, \\quad u \\ge 0, \\; v \\ge 0,\n$$\n这是一个标准的线性规划（LP）问题。\n- 为保证数值稳定性，在所有计算之前将 $A$ 的列归一化为单位欧几里得范数。在检查 $\\theta(S;A)  1$ 时，使用带有小数值容差的严格不等式。在成功测试中，使用小容差来判断支撑集是否相等以及相对误差。\n\n测试套件：\n您必须为以下三个矩阵实例及相关支撑集实现您的解决方案。对于每个实例，计算给定 $S$ 的 ERC 判定结果和在指定的候选集上使用指定试验次数的经验性弱阈值 $k_{\\mathrm{weak}}$。\n- 测试用例1（随机高斯矩阵，中等维度）：\n  - 维度：$m = 20$， $n = 40$。\n  - 构造方法：使用伪随机种子 $7$ 独立同分布地从标准正态分布中抽取 $A$ 的元素，然后将各列归一化为单位范数。\n  - 用于 ERC 的支撑集：$S = \\{0,7,13\\}$。\n  - 经验性弱阈值设置：候选集 $\\mathcal{K} = \\{1,2,3,4,5,6\\}$，每个 $k$ 的试验次数：$T = 12$，使用固定的伪随机种子 $101$ 以确保试验的可复现性。\n- 测试用例2（重复列，对一个1-稀疏支撑集而言 ERC 不适定）：\n  - 维度：$m = 10$，$n = 12$。\n  - 构造方法：使用伪随机种子 $3$ 抽取一个基础矩阵 $A$，其元素为独立同分布的标准正态分布。然后将列索引 $5$ 设置为与列索引 $0$ 完全相等。最后，将所有列归一化为单位范数。\n  - 用于 ERC 的支撑集：$S = \\{0\\}$。\n  - 经验性弱阈值设置：候选集 $\\mathcal{K} = \\{1,2,3,4,5\\}$，每个 $k$ 的试验次数：$T = 12$，使用固定的伪随机种子 $202$。\n- 测试用例3（具有指定相关性的近似重复列）：\n  - 维度：$m = 10$，$n = 12$。\n  - 构造方法：使用伪随机种子 $4$ 抽取一个基础矩阵 $A$，其元素为独立同分布的标准正态分布。设 $a_0$ 表示列索引 $0$。通过将一个随机标准正态向量投影到 $\\mathrm{span}\\{a_0\\}$ 的正交补上并进行归一化，构造一个与 $a_0$ 正交的单位范数向量 $r$。将列索引 $5$ 设置为 $0.99\\,a_0 + \\sqrt{1 - 0.99^2}\\, r$，然后将所有列归一化为单位范数。\n  - 用于 ERC 的支撑集：$S = \\{0\\}$。\n  - 经验性弱阈值设置：候选集 $\\mathcal{K} = \\{1,2,3,4,5\\}$，每个 $k$ 的试验次数：$T = 12$，使用固定的伪随机种子 $303$。\n\n成功标准和数值容差：\n- ERC 严格不等式检查容差：如果 $\\theta(S;A)  1 - 10^{-9}$，则声明 ERC 得到满足，否则声明未满足。如果 $A_S$ 不是满列秩，则声明未满足。\n- 经验性弱阈值的恢复成功测试：当对绝对值大于 $10^{-4}$ 的元素进行阈值化处理时，如果 $\\hat{x}$ 的支撑集与真实支撑集匹配，并且相对误差满足 $\\|\\hat{x} - x\\|_2 / \\|x\\|_2  10^{-3}$，则声明成功。\n- 使用线性规划求解器精确地如上计算 BP。\n\n最终输出格式：\n- 对于三个测试用例中的每一个，按顺序生成一个由 ERC 判定结果和经验性弱阈值组成的对：$[\\text{erc\\_bool}, k_{\\mathrm{weak}}]$，其中 $\\text{erc\\_bool}$ 是一个布尔值，$k_{\\mathrm{weak}}$ 是一个非负整数。\n- 您的程序应生成单行输出，包含一个逗号分隔且无空格的列表，其中含有这三个对，例如：$[[\\text{True},3],[\\text{False},2],[\\text{True},4]]$。\n\n您的程序必须是自包含的，不能读取任何输入，并且只能使用指定的运行时环境。通过完全按照描述使用指定的伪随机种子来确保确定性行为。此问题不出现角度和物理单位。百分比必须表示为小数；成功率截止标准使用 $0.5$。[@problem_id:483]", "solution": "所提出的问题是在压缩感知和稀疏信号恢复的数学框架内一个明确定义的计算练习。它在科学上是合理的，自洽的，并且所有参数和过程都得到了明确的规定。因此，该问题被认为是有效的。我们将继续提供一个完整的解决方案。\n\n任务有两方面：首先，为给定的矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个特定的支撑集 $S$ 验证精确恢复条件（ERC）；其次，为同一矩阵 $A$ 经验性地估计恢复的弱阈值 $k_{\\mathrm{weak}}$。我们将有条不紊地处理每个部分。\n\n**第1部分：精确恢复条件 (ERC) 的验证**\n\n精确恢复条件为通过基追踪（BP）凸规划从无噪声测量 $y=Ax$ 中唯一恢复任何支撑集为 $S$ 的稀疏信号 $x$ 提供了一个充分（且通常是必要）的条件：\n$$\n\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1 \\quad \\text{subject to} \\quad A z = y.\n$$\n$x$ 的支撑集是其非零项的索引集合，$S = \\{i : x_i \\neq 0\\}$。设 $|S| = k$。\n\n一个支撑集 $S$ 的 ERC 满足当且仅当\n$$\n\\theta(S;A) \\triangleq \\max_{j \\notin S} \\left\\| A_S^\\dagger a_j \\right\\|_1  1,\n$$\n其中 $A_S$ 是由 $S$ 索引的列构成的 $A$ 的子矩阵，$a_j$ 是 $A$ 的第 $j$ 列，$A_S^\\dagger$ 是 $A_S$ 的摩尔-彭若斯伪逆。这个条件仅在 $A_S$ 具有满列秩，即 $\\mathrm{rank}(A_S) = k$ 时才有明确定义。如果 $A_S$ 是秩亏的，它的列是线性相关的，从 $A_Sx_S$ 中唯一确定支撑在 $S$ 上的信号系数是不可能的。在这种情况下，ERC 被认为不满足。\n\n验证 ERC 的计算过程如下：\n1.  **矩阵归一化**：按照规定，我们首先将矩阵 $A$ 的每一列 $a_j$ 归一化为单位欧几里得范数：对于所有 $j \\in \\{0, \\dots, n-1\\}$，$a_j \\leftarrow a_j / \\|a_j\\|_2$。所有后续计算都使用这个归一化后的矩阵。\n2.  **子矩阵提取**：给定支撑集 $S$，我们构造子矩阵 $A_S \\in \\mathbb{R}^{m \\times k}$。不在 $S$ 中的索引构成补集 $S^c = \\{0, \\dots, n-1\\} \\setminus S$。\n3.  **秩检查**：我们验证 $A_S$ 是否具有满列秩，即 $\\mathrm{rank}(A_S) = k$。这可以通过检查 $A_S$ 的奇异值中大于一个小编移量的数量是否等于 $k$ 来完成。如果 $\\mathrm{rank}(A_S)  k$，则声明 ERC 不满足。\n4.  **伪逆计算**：如果 $A_S$ 具有满列秩，我们计算其伪逆 $A_S^\\dagger$。对于一个满秩矩阵 $A_S$，这由 $A_S^\\dagger = (A_S^\\top A_S)^{-1} A_S^\\top$ 给出。在数值上，使用像奇异值分解（SVD）这样的稳定方法，这在标准数值库中已有实现。\n5.  **ERC 量值计算**：对于每个索引 $j \\in S^c$ 的列 $a_j$，我们计算向量 $v_j = A_S^\\dagger a_j$。然后我们计算它的 $\\ell_1$-范数，$\\|v_j\\|_1 = \\sum_i |(v_j)_i|$。\n6.  **最大值和比较**：我们找到这些范数的最大值，$\\theta(S;A) = \\max_{j \\in S^c} \\|v_j\\|_1$。\n7.  **判定**：如果这个最大值严格小于1，则 ERC 满足。为了考虑有限精度算法，我们对照一个容差进行检查：$\\theta(S;A)  1 - \\epsilon$，其中问题指定 $\\epsilon = 10^{-9}$。\n\n**第2部分：恢复的弱阈值的经验性估计**\n\n恢复的弱阈值 $k_{\\mathrm{weak}}$，对于给定的矩阵 $A$，是稀疏度水平的一个经验度量，在该水平之下恢复“通常”是成功的。对于一个固定的矩阵实例，它是一个随机选择的 $k$-稀疏信号能够以高概率被恢复的最大稀疏度 $k$。问题指定了一个操作性定义，我们将其形式化为以下算法。\n\n1.  **矩阵归一化**：和之前一样，将 $A$ 的列归一化为单位 $\\ell_2$-范数。为试验初始化一个独立且可复现的随机数流。\n2.  **遍历稀疏度水平**：我们遍历每个候选稀疏度水平 $k \\in \\mathcal{K}$。\n3.  **蒙特卡洛模拟**：对于每个 $k$，我们执行 $T$ 次独立试验：\n    a. **生成一个随机稀疏信号**：\n        i.  从所有 $\\binom{n}{k}$ 个可能的大小为 $k$ 的支撑集中均匀随机地选择一个支撑集 $S$。\n        ii. 构造一个信号 $x \\in \\mathbb{R}^n$。它在 $S$ 之外处处为零。$k$ 个非零项 $x_S$ 从标准正态分布 $\\mathcal{N}(0,1)$ 中独立抽取。\n    b. **生成测量值**：计算无噪声测量向量 $y = Ax$。\n    c. **通过基追踪尝试恢复**：我们求解 BP 优化问题以找到一个估计 $\\hat{x}$。按照规定，我们使用等价的线性规划（LP）：\n        $$\n        \\min_{u,v} \\sum_{i=0}^{n-1} (u_i+v_i) \\quad \\text{subject to} \\quad A(u-v) = y, \\quad u \\ge 0, \\; v \\ge 0.\n        $$\n        使用一个标准的 LP 求解器。解向量 $[u^\\star; v^\\star]$ 给出恢复的信号 $\\hat{x} = u^\\star - v^\\star$。\n    d. **检查成功与否**：如果以下两个标准都满足，则试验被认为是成功的：\n        i.  **支撑集恢复**：估计的支撑集 $\\hat{S} = \\{i : |\\hat{x}_i| > 10^{-4}\\}$ 必须与真实支撑集 $S$ 相同。\n        ii. **信号保真度**：相对重构误差必须很小：$\\|\\hat{x} - x\\|_2 / \\|x\\|_2  10^{-3}$。\n4.  **计算成功率**：对于每个 $k$，经验成功率是成功试验的比例：$P_k = (\\text{成功次数}) / T$。\n5.  **确定弱阈值**：在为所有 $k \\in \\mathcal{K}$ 计算成功率后，弱阈值 $k_{\\mathrm{weak}}$ 是 $k \\in \\mathcal{K}$ 中使得 $P_k \\ge 0.5$ 的最大值。如果 $\\mathcal{K}$ 中没有 $k$ 满足此标准，则 $k_{\\mathrm{weak}}$ 取为 $0$。\n\n这两个过程将应用于指定的三个测试用例。测试用例2提出了一个有两列相同的矩阵，这保证了对于任何包含其中一列而不包含另一列的支撑集，ERC 都会失败，因为 $\\|A_S^\\dagger a_j\\|_1$ 将恰好为 $1$。测试用例3涉及高度相关但非完全相关的列，提供了在 ERC 边界附近的测试。实现将忠实遵守指定的随机种子，以确保完全的可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            \"m\": 20, \"n\": 40, \"A_seed\": 7, \"erc_support\": {0, 7, 13},\n            \"k_set\": [1, 2, 3, 4, 5, 6], \"trials_per_k\": 12, \"trial_seed\": 101,\n            \"case_id\": 1\n        },\n        # Test case 2\n        {\n            \"m\": 10, \"n\": 12, \"A_seed\": 3, \"erc_support\": {0},\n            \"k_set\": [1, 2, 3, 4, 5], \"trials_per_k\": 12, \"trial_seed\": 202,\n            \"case_id\": 2\n        },\n        # Test case 3\n        {\n            \"m\": 10, \"n\": 12, \"A_seed\": 4, \"erc_support\": {0},\n            \"k_set\": [1, 2, 3, 4, 5], \"trials_per_k\": 12, \"trial_seed\": 303,\n            \"case_id\": 3\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        A = generate_matrix(params)\n        erc_verdict = compute_erc_verdict(A, params[\"erc_support\"])\n        k_weak = estimate_weak_threshold(A, params[\"k_set\"], params[\"trials_per_k\"], params[\"trial_seed\"])\n        results.append([erc_verdict, k_weak])\n    \n    # Format output to be exactly [[Bool,int],[Bool,int],...] with no spaces\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\ndef generate_matrix(params):\n    \"\"\"\n    Generates the sensing matrix A based on test case parameters.\n    \"\"\"\n    m, n, seed = params[\"m\"], params[\"n\"], params[\"A_seed\"]\n    case_id = params[\"case_id\"]\n    rng = np.random.default_rng(seed)\n    \n    if case_id == 1:\n        A = rng.standard_normal((m, n))\n    elif case_id == 2:\n        A = rng.standard_normal((m, n))\n        A[:, 5] = A[:, 0]\n    elif case_id == 3:\n        A_base = rng.standard_normal((m, n))\n        a0_unnorm = A_base[:, 0].copy()\n        \n        # Construct a random vector orthogonal to a0_unnorm\n        rand_vec = rng.standard_normal(m)\n        proj_a0_rand = (rand_vec @ a0_unnorm) / (a0_unnorm @ a0_unnorm) * a0_unnorm\n        r_unorth = rand_vec - proj_a0_rand\n        r = r_unorth / np.linalg.norm(r_unorth)\n        \n        # Construct the new highly correlated column\n        a5_new = 0.99 * a0_unnorm + np.sqrt(1 - 0.99**2) * r\n        A = A_base.copy()\n        A[:, 5] = a5_new\n\n    # Normalize all columns to unit Euclidean norm\n    norms = np.linalg.norm(A, axis=0)\n    # Avoid division by zero for zero columns, although not expected here\n    norms[norms == 0] = 1\n    A = A / norms\n\n    return A\n\ndef compute_erc_verdict(A, S_set):\n    \"\"\"\n    Numerically verifies the Exact Recovery Condition (ERC).\n    \"\"\"\n    m, n = A.shape\n    S = sorted(list(S_set))\n    k = len(S)\n\n    if k == 0:\n        # ERC is trivially true for k=0 but let's handle it\n        return True\n\n    A_S = A[:, S]\n    \n    # Check if A_S is full column rank\n    if np.linalg.matrix_rank(A_S)  k:\n        return False\n    \n    # Compute pseudoinverse of A_S\n    A_S_dagger = np.linalg.pinv(A_S)\n    \n    # Get indices not in S\n    S_c = sorted(list(set(range(n)) - S_set))\n    \n    max_norm = 0.0\n    for j in S_c:\n        a_j = A[:, j]\n        v_j = A_S_dagger @ a_j\n        norm_v_j = np.linalg.norm(v_j, 1)\n        if norm_v_j > max_norm:\n            max_norm = norm_v_j\n            \n    erc_tolerance = 1e-9\n    return max_norm  1.0 - erc_tolerance\n\ndef estimate_weak_threshold(A, k_set, T, seed):\n    \"\"\"\n    Empirically estimates the weak threshold of recovery.\n    \"\"\"\n    m, n = A.shape\n    rng = np.random.default_rng(seed)\n    \n    k_weak = 0\n    \n    for k in k_set:\n        if k == 0: continue\n        success_count = 0\n        for _ in range(T):\n            # 1. Generate random sparse signal\n            S_indices = rng.choice(n, k, replace=False)\n            S = set(S_indices)\n            x = np.zeros(n)\n            x[S_indices] = rng.standard_normal(k)\n            \n            # 2. Generate measurements\n            y = A @ x\n            \n            # 3. Solve Basis Pursuit via LP\n            c = np.ones(2 * n)\n            A_eq = np.hstack([A, -A])\n            b_eq = y\n            \n            # Scipy's linprog with 'highs' is robust\n            res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=(0, None), method='highs-ds')\n            \n            if not res.success:\n                continue\n                \n            x_hat = res.x[:n] - res.x[n:]\n\n            # 4. Check for success\n            support_tol = 1e-4\n            error_tol = 1e-3\n            \n            S_hat = set(np.where(np.abs(x_hat) > support_tol)[0])\n            \n            support_recovered = (S == S_hat)\n            \n            # Avoid division by zero if x is zero vector\n            norm_x = np.linalg.norm(x)\n            if norm_x == 0:\n                relative_error = np.linalg.norm(x_hat) # Should also be near zero\n            else:\n                relative_error = np.linalg.norm(x_hat - x) / norm_x\n            \n            error_small = (relative_error  error_tol)\n            \n            if support_recovered and error_small:\n                success_count += 1\n\n        success_rate = success_count / T\n        \n        if success_rate >= 0.5:\n            k_weak = k\n            \n    return k_weak\n\nsolve()\n```", "id": "3494376"}, {"introduction": "虽然凸优化提供了强大的理论恢复保证，但许多实用算法是贪婪或迭代的。本练习通过让您比较五种不同方法的保证恢复条件，包括基追踪（Basis Pursuit）、正交匹配追踪（OMP）和压缩采样匹配追踪（CoSaMP），来探索这些算法的性能图景。通过检查它们各自基于受限等距性质（Restricted Isometry Property, RIP）和相干性的保证，您将发现，一个保证对凸恢复有效的矩阵，可能不满足其他算法所需的更严格条件 [@problem_id:3494329]。", "problem": "考虑压缩感知中的线性逆模型，其中，一个最多有 $k$ 个非零项的信号 $x_{\\star} \\in \\mathbb{R}^{n}$（即，$x_{\\star}$ 是 $k$-稀疏的）通过一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 进行测量，该矩阵的元素是方差为 $1/m$ 的独立同分布高斯分布，从而产生无噪声数据 $y = A x_{\\star}$。两个核心概念是弱恢复阈值和强恢复阈值：弱阈值指的是高维极限下典型情况的恢复（相对于随机支撑集和符号），而强阈值指的是在同一测量算子下对所有 $k$-稀疏信号同时成立的均匀恢复保证。对于具有高斯项的矩阵，从几何和概率理论可知，凸 $\\ell_{1}$-最小化具有尖锐的渐近阈值；然而，算法性能和非凸/贪婪方法可能表现出更保守的可证明阈值。\n\n您的任务是实现一个程序，在小的、有限的维度下，比较五种算法的可证明的强阈值类型的保证：\n- 正交匹配追踪 (Orthogonal Matching Pursuit, OMP)，\n- 压缩采样匹配追踪 (Compressive Sampling Matching Pursuit, CoSaMP)，\n- 迭代硬阈值 (Iterative Hard Thresholding, IHT)，\n- 基追踪 (Basis Pursuit, BP)，\n- 近似消息传递 (Approximate Message Passing, AMP)。\n\n您必须基于基本定义和经过充分测试的充分条件进行操作：\n\n1) 限制等距性质 (Restricted Isometry Property, RIP)。对于一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个正整数 $s$，限制等距常数 $\\delta_{s}$ 是满足以下条件的最小非负数：\n$$\n(1 - \\delta_{s}) \\| x \\|_{2}^{2} \\le \\| A x \\|_{2}^{2} \\le (1 + \\delta_{s}) \\| x \\|_{2}^{2}\n$$\n对于所有 $s$-稀疏的 $x$。对于一个子集 $S \\subset \\{1,\\dots,n\\}$ 且 $| S | = s$，令 $A_{S}$ 表示由 $S$ 索引的列构成的子矩阵，上述条件成立当且仅当格拉姆矩阵 (Gram matrix) $G_{S} = A_{S}^{\\top} A_{S}$ 的每个特征值都位于 $[1-\\delta_{s}, 1+\\delta_{s}]$ 区间内。如果 $s > m$，那么 $\\delta_{s} \\ge 1$，因为 $A_{S}$ 不可能具有满列秩，因此 $G_{S}$ 的某个特征值为零。\n\n2) 互相关性 (Mutual coherence)。对于列归一化的矩阵 $A$（其列为 $a_{1},\\dots,a_{n}$），互相关性为\n$$\n\\mu(A) = \\max_{i \\ne j} | a_{i}^{\\top} a_{j} |.\n$$\n\n3) 经典的充分条件（有限维、无噪声情况；下面所有的常数都是文献中出现的标准的、保守的值，在此用作验证均匀、强类型保证的充分测试事实）：\n- 基追踪 (Basis Pursuit, BP)：如果 $\\delta_{2k}  \\sqrt{2} - 1$，则 $\\ell_{1}$-最小化能唯一恢复每个 $k$-稀疏信号 $x_{\\star}$。\n- 近似消息传递 (Approximate Message Passing, AMP) 对于高斯矩阵 $A$ 使用最优调整的软阈值去噪器时：在高维极限下，与 BP 共享相同的尖锐相变；在这里，我们采用与 BP 相同的充分条件来验证强类型恢复，即如果 $\\delta_{2k}  \\sqrt{2} - 1$，则声明成功。\n- 迭代硬阈值 (Iterative Hard Thresholding, IHT)：如果 $\\delta_{3k}  1/\\sqrt{32}$ 并且步长选择小于梯度的利普希茨常数的倒数，则 IHT 收敛到 $k$-稀疏解。\n- 压缩采样匹配追踪 (Compressive Sampling Matching Pursuit, CoSaMP)：如果 $\\delta_{4k}  0.1$，则在无噪声情况下 CoSaMP 能精确恢复每个 $k$-稀疏信号 $x_{\\star}$。\n- 正交匹配追踪 (Orthogonal Matching Pursuit, OMP)：如果 $\\mu(A)  \\frac{1}{2k-1}$，则 OMP 能精确恢复每个 $k$-稀疏信号 $x_{\\star}$。\n\n由于精确计算 $\\delta_{s}$ 需要测试所有 $\\binom{n}{s}$ 个列子集，这是一个组合问题，您的程序必须：\n- 当 $s > m$ 时，通过返回 $\\delta_{s} = 1$ 来精确计算 $\\delta_{s}$。\n- 否则，通过随机抽样固定数量的大小为 $s$ 的子集 $S$，并取所有抽样子集中 $G_{S} = A_{S}^{\\top} A_{S}$ 的特征值与 1 的最大偏差，来估计 $\\delta_{s}$ 的一个上界。这将针对抽样族产生一个数据驱动的、经验上验证的 $\\delta_{s}$ 上界。为保证可复现性，请使用固定的伪随机数生成器种子。\n\n使用以上方法，对于每个测试用例，您必须：\n- 使用指定的种子生成一个高斯矩阵 $A$，其元素服从 $\\mathcal{N}(0, 1/m)$ 分布。\n- 通过抽样子集计算 $\\delta_{2k}$、$\\delta_{3k}$、$\\delta_{4k}$ 的估计值，并从列归一化的 $A$ 计算 $\\mu(A)$。\n- 生成五个布尔值，指示每种算法的既定充分条件是否满足（解释为可证明的强类型恢复证书）：$b_{\\mathrm{BP}}$、$b_{\\mathrm{AMP}}$、$b_{\\mathrm{OMP}}$、$b_{\\mathrm{CoSaMP}}$、$b_{\\mathrm{IHT}}$。\n- 同时，生成一个额外的布尔值 $b_{\\mathrm{adv}}$，指示该实例是否落入这样一个区域：凸方法的强类型条件成立，但至少一个非凸/贪婪算法的证书失败，即\n$$\nb_{\\mathrm{adv}} = \\Big( b_{\\mathrm{BP}} \\ \\text{is True} \\Big) \\ \\wedge \\ \\Big( (b_{\\mathrm{OMP}} \\ \\text{is False}) \\ \\lor \\ (b_{\\mathrm{CoSaMP}} \\ \\text{is False}) \\ \\lor \\ (b_{\\mathrm{IHT}} \\ \\text{is False}) \\ \\lor \\ (b_{\\mathrm{AMP}} \\ \\text{is False}) \\Big).\n$$\n\n测试套件：\n为以下三种情况提供结果，每种情况都有维度 $(m,n,k)$ 和一个起始种子 $s_{0}$：\n- 情况1：$m = 12$, $n = 14$, $k = 2$, $s_{0} = 7$。\n- 情况2：$m = 10$, $n = 14$, $k = 3$, $s_{0} = 1$。仅针对此情况，为了稳健地识别优势区域，您必须在种子 $s = s_{0}, s_{0} + 1, \\dots$ 上进行搜索，直至达到内部上限，直到找到一个使 $b_{\\mathrm{adv}}$ 为 True 的矩阵；如果在此上限内未找到，则返回最后尝试的种子的结果。\n- 情况3：$m = 7$, $n = 14$, $k = 4$, $s_{0} = 3$。\n\n常数：\n- 对基追踪和近似消息传递的证书，使用 $\\sqrt{2} - 1$作为 $\\delta_{2k}$ 的阈值。\n- 对迭代硬阈值的证书，使用 $1/\\sqrt{32}$ 作为 $\\delta_{3k}$ 的阈值。\n- 对压缩采样匹配追踪的证书，使用 $0.1$ 作为 $\\delta_{4k}$ 的阈值。\n- 对正交匹配追踪的证书，使用 $\\frac{1}{2k-1}$ 作为 $\\mu(A)$ 的阈值。\n\n角度单位和物理单位在此不适用。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有三种情况的聚合布尔结果，形式为一个扁平的、逗号分隔的 Python 风格列表，长度为 18，按固定的算法顺序 $[b_{\\mathrm{BP}}, b_{\\mathrm{AMP}}, b_{\\mathrm{OMP}}, b_{\\mathrm{CoSaMP}}, b_{\\mathrm{IHT}}, b_{\\mathrm{adv}}]$ 每六个为一组。例如，\"[True,False,True,True,False,True, ... for case 2 ..., ... for case 3 ...]\"。", "solution": "用户提供的问题已经过验证，被确定为一个有效、适定且具有科学依据的任务。\n\n该问题要求比较压缩感知中使用的五种常用算法的理论强恢复保证：基追踪（$BP$）、近似消息传递（$AMP$）、正交匹配追踪（$OMP$）、压缩采样匹配追踪（$CoSaMP$）和迭代硬阈值（$IHT$）。该比较将通过检查某些精确信号恢复的充分条件是否对由维度 $(m, n, k)$ 和随机生成的测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 定义的特定问题实例满足来进行。\n\n如果一个信号 $x_{\\star} \\in \\mathbb{R}^{n}$ 最多有 $k$ 个非零项，则称其为 $k$-稀疏信号。我们观测到测量值 $y = A x_{\\star}$。恢复算法的目标是从 $y$ 和 $A$ 重构 $x_{\\star}$。一个强恢复保证确保算法对于一个*固定*的矩阵 $A$，能够成功恢复*所有* $k$-稀疏信号 $x_{\\star}$。这类保证通常表示为对矩阵 $A$ 几何性质的条件。\n\n两个核心性质是限制等距性质（RIP）和互相关性。\n\n矩阵 $A$ 的限制等距常数 $\\delta_s$ 衡量了 $A$ 在所有 $s$-稀疏向量上的作用与等距变换的接近程度。对于任何 $s$-稀疏向量 $x$，我们要求 $\\| A x \\|_{2}^{2}$ 接近于 $\\| x \\|_{2}^{2}$。条件是：\n$$\n(1 - \\delta_{s}) \\| x \\|_{2}^{2} \\le \\| A x \\|_{2}^{2} \\le (1 + \\delta_{s}) \\| x \\|_{2}^{2}\n$$\n这等价于陈述：对于任何大小为 $| S | = s$ 的子集 $S \\subseteq \\{1, \\dots, n\\}$，格拉姆矩阵 $G_S = A_S^{\\top} A_S$（其中 $A_S$ 是由 $S$ 索引的列构成的 $A$ 的子矩阵）的所有特征值必须位于区间 $[1 - \\delta_s, 1 + \\delta_s]$ 内。一个小的 $\\delta_s$ 表明矩阵 $A$ 很好地保持了所有 $s$-稀疏向量的长度。如果子矩阵 $A_S$ 的列数 $s$ 大于行数 $m$，即 $s > m$，那么 $A_S$ 是秩亏的。因此，$G_S$ 必须有一个零特征值，这意味着 $\\delta_s \\ge 1$。问题规定在这种情况下，我们应设置 $\\delta_s = 1$。\n\n互相关性 $\\mu(A)$ 对于一个列归一化的矩阵（其列为 $a_i$）定义为任意两个不同列之间内积的绝对值的最大值：\n$$\n\\mu(A) = \\max_{i \\ne j} | a_{i}^{\\top} a_{j} |\n$$\n一个小的 $\\mu(A)$ 意味着 $A$ 的列几乎是正交的，这对于像 $OMP$ 这样的贪婪算法是有利的。\n\n由于精确计算 $\\delta_s$ 是一个 NP-难问题（需要检查所有 $\\binom{n}{s}$ 个子矩阵），我们必须采用估计程序。问题指示我们通过对大小为 $s$ 的列子集进行固定次数的抽样，计算每个对应格拉姆矩阵的特征值与 1 的最大偏差，并取所有抽样子集中的最大值，来估计 $\\delta_s$ 的一个上界。为了可复现性，每次估计 $\\delta_s$ 将抽样固定数量的 1000 个随机子集。\n\n问题陈述中为五种算法提供的充分条件用于验证恢复。对于每个测试用例，其参数为 $(m, n, k)$ 和生成的矩阵 $A$：\n1.  **基追踪（$BP$）**：一种凸松弛方法。其证书为 $b_{\\mathrm{BP}} = (\\delta_{2k}  \\sqrt{2} - 1)$。数值 $\\sqrt{2} - 1 \\approx 0.4142$。\n2.  **近似消息传递（$AMP$）**：一种受统计物理启发的迭代算法。问题陈述要求使用与 $BP$ 相同的证书：$b_{\\mathrm{AMP}} = (\\delta_{2k}  \\sqrt{2} - 1)$。这意味着 $b_{\\mathrm{BP}}$ 和 $b_{\\mathrm{AMP}}$ 将始终相同。\n3.  **正交匹配追踪（$OMP$）**：一种经典的贪婪算法。其证书基于相关性：$b_{\\mathrm{OMP}} = (\\mu(A)  \\frac{1}{2k-1})$。\n4.  **压缩采样匹配追踪（$CoSaMP$）**：一种更先进的贪婪算法。其证书为 $b_{\\mathrm{CoSaMP}} = (\\delta_{4k}  0.1)$。\n5.  **迭代硬阈值（$IHT$）**：另一种主要的迭代算法。其证书为 $b_{\\mathrm{IHT}} = (\\delta_{3k}  1/\\sqrt{32})$。数值 $1/\\sqrt{32} \\approx 0.1768$。\n\n最后，我们计算一个“优势”布尔值 $b_{\\mathrm{adv}}$。如果凸方法（$BP$）的保证成立，而非凸/贪婪方法之一（$OMP$、$CoSaMP$、$IHT$）的保证不成立，则该标志为真。提供的定义是：\n$$\nb_{\\mathrm{adv}} = (b_{\\mathrm{BP}}) \\wedge ((\\neg b_{\\mathrm{OMP}}) \\lor (\\neg b_{\\mathrm{CoSaMP}}) \\lor (\\neg b_{\\mathrm{IHT}}) \\lor (\\neg b_{\\mathrm{AMP}}))\n$$\n由于 $b_{\\mathrm{BP}} \\equiv b_{\\mathrm{AMP}}$，如果 $b_{\\mathrm{BP}}$ 为真，则 $\\neg b_{\\mathrm{AMP}}$ 为假。表达式简化为：\n$$\nb_{\\mathrm{adv}} = b_{\\mathrm{BP}} \\wedge ((\\neg b_{\\mathrm{OMP}}) \\lor (\\neg b_{\\mathrm{CoSaMP}}) \\lor (\\neg b_{\\mathrm{IHT}}))\n$$\n如果 $b_{\\mathrm{BP}}$ 为假，则 $b_{\\mathrm{adv}}$ 为假。\n\n对于每个测试用例，实现将按以下方式进行：\n- 一个函数 `evaluate_case(m, n, k, seed)` 将处理单个实例的逻辑。\n- 它将使用给定的种子生成矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其元素从 $\\mathcal{N}(0, 1/m)$ 中抽取。\n- 它将调用辅助函数来估计 $\\delta_{2k}$、$\\delta_{3k}$、$\\delta_{4k}$ 并计算 $\\mu(A)$。\n- 对每种情况应用特定逻辑：\n    - **情况1**：$(m, n, k) = (12, 14, 2)$，$s_0=7$。所有需要的 $\\delta_s$ 阶数（即 $s=4, 6, 8$）都小于 $m=12$，因此全部通过抽样估计。\n    - **情况2**：$(m, n, k) = (10, 14, 3)$，$s_0=1$。我们需要 $\\delta_6, \\delta_9, \\delta_{12}$。由于 $12 > m=10$，$\\delta_{12}$ 被设为 1。这确保了 $CoSaMP$ 条件不成立。代码将从 $s_0=1$ 开始搜索种子，直到达到 $s_0+100$ 的内部上限，以找到一个 $b_{\\mathrm{adv}}$ 为真的实例。如果未找到，则使用最后一个种子的结果。\n    - **情况3**：$(m, n, k) = (7, 14, 4)$，$s_0=3$。我们需要 $\\delta_8, \\delta_{12}, \\delta_{16}$。所有阶数都大于 $m=7$，因此所有三个 $\\delta_s$ 值都设为 1。这导致 $BP$、$AMP$、$CoSaMP$ 和 $IHT$ 的证书确定性地不成立。\n\n来自三种情况的结果将被聚合到一个单一的扁平列表中作为最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef estimate_delta_s(A, s, num_samples, rng):\n    \"\"\"\n    Estimates the restricted isometry constant delta_s of matrix A.\n\n    If s > m, delta_s >= 1. We return 1.0 as per problem description.\n    Otherwise, it is estimated by sampling subsets of columns.\n    \"\"\"\n    m, n = A.shape\n    if s > n:\n        # Cannot choose s columns from n if s>n. This case may not occur\n        # with problem inputs but is good practice.\n        return 1.0\n    if s > m:\n        return 1.0\n\n    max_dev = 0.0\n    indices = np.arange(n)\n\n    for _ in range(num_samples):\n        S = rng.choice(indices, size=s, replace=False)\n        A_S = A[:, S]\n        G_S = A_S.T @ A_S\n        \n        # eigvalsh is for Hermitian (or real symmetric) matrices.\n        eigenvalues = np.linalg.eigvalsh(G_S)\n        \n        dev = np.max(np.abs(eigenvalues - 1))\n        if dev > max_dev:\n            max_dev = dev\n            \n    return max_dev\n\ndef compute_mu(A):\n    \"\"\"\n    Computes the mutual coherence of matrix A.\n    \"\"\"\n    # Normalize columns of A\n    norm_A = A / np.linalg.norm(A, axis=0)\n    \n    # Compute Gram matrix\n    G = norm_A.T @ norm_A\n    \n    # Set diagonal to zero to ignore self-correlation\n    np.fill_diagonal(G, 0)\n    \n    # Mutual coherence is the max absolute off-diagonal element\n    mu = np.max(np.abs(G))\n    return mu\n\ndef evaluate_case(m, n, k, seed, num_samples):\n    \"\"\"\n    Evaluates recovery conditions for a single (m, n, k, seed) case.\n    \"\"\"\n    # Use a specific Random Number Generator for this evaluation\n    rng = np.random.default_rng(seed)\n    \n    A = rng.normal(0, 1 / np.sqrt(m), (m, n))\n\n    # Calculate RICs\n    # Note: The same RNG is passed, so subset sampling is deterministic for a given seed.\n    s_2k = 2 * k\n    s_3k = 3 * k\n    s_4k = 4 * k\n\n    delta_2k = estimate_delta_s(A, s_2k, num_samples, rng)\n    delta_3k = estimate_delta_s(A, s_3k, num_samples, rng)\n    delta_4k = estimate_delta_s(A, s_4k, num_samples, rng)\n    \n    # Calculate mutual coherence\n    mu = compute_mu(A)\n    \n    # Check sufficient conditions\n    b_BP = delta_2k  (np.sqrt(2) - 1)\n    b_AMP = delta_2k  (np.sqrt(2) - 1) # Same as BP per problem\n    b_OMP = mu  (1 / (2 * k - 1)) if (2 * k - 1) != 0 else False\n    b_CoSaMP = delta_4k  0.1\n    b_IHT = delta_3k  (1 / np.sqrt(32))\n    \n    # Calculate advantage boolean\n    # b_adv = b_BP and (not b_OMP or not b_CoSaMP or not b_IHT or not b_AMP)\n    # Since b_BP == b_AMP, this simplifies.\n    b_adv = b_BP and (not b_OMP or not b_CoSaMP or not b_IHT)\n\n    return [b_BP, b_AMP, b_OMP, b_CoSaMP, b_IHT, b_adv]\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (m, n, k, s0)\n        (12, 14, 2, 7),\n        (10, 14, 3, 1),\n        (7, 14, 4, 3),\n    ]\n\n    # Constants for computation\n    NUM_SAMPLES_DELTA = 1000\n    SEED_SEARCH_CAP = 100\n    \n    all_results = []\n\n    # Case 1\n    m, n, k, s0 = test_cases[0]\n    results_case1 = evaluate_case(m, n, k, s0, NUM_SAMPLES_DELTA)\n    all_results.extend(results_case1)\n    \n    # Case 2\n    m, n, k, s0 = test_cases[1]\n    current_seed = s0\n    found_adv = False\n    results_case2 = []\n    \n    while current_seed  s0 + SEED_SEARCH_CAP:\n        results_case2 = evaluate_case(m, n, k, current_seed, NUM_SAMPLES_DELTA)\n        b_adv = results_case2[-1]\n        if b_adv:\n            found_adv = True\n            break\n        current_seed += 1\n    all_results.extend(results_case2)\n\n    # Case 3\n    m, n, k, s0 = test_cases[2]\n    results_case3 = evaluate_case(m, n, k, s0, NUM_SAMPLES_DELTA)\n    all_results.extend(results_case3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3494329"}]}