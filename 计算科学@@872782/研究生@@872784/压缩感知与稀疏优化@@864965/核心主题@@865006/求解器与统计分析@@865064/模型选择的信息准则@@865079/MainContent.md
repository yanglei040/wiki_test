## 引言
[模型选择](@entry_id:155601)是统计学、机器学习及所有数据驱动科学领域中的一个核心挑战。在面对日益复杂的数据时，如何构建一个既能精确拟合现有观测，又具备对未来数据良好泛化能力的模型，是科学家和工程师们面临的共同难题。仅仅追求最佳的[拟合优度](@entry_id:637026)往往会导致模型过于复杂，从而陷入“[过拟合](@entry_id:139093)”的陷阱，无法揭示数据背后真正的规律。[信息准则](@entry_id:636495)通过在模型的[拟合优度](@entry_id:637026)与复杂度之间建立一个量化的权衡，为解决这一根本性问题提供了强大而优雅的理论框架。然而，经典的准则（如AIC和BIC）在现代高维稀疏设定的挑战下暴露出其局限性，促使了新一代[信息准则](@entry_id:636495)的诞生。

本文旨在为读者提供一个关于模型选择[信息准则](@entry_id:636495)的完整图景，从经典理论到前沿发展。我们将分三部分展开：

在“**原理与机制**”一章中，我们将从第一性原理出发，深入剖析[赤池信息准则](@entry_id:139671)（AIC）、[贝叶斯信息准则](@entry_id:142416)（BIC）等经典理论的推导过程与统计内涵，并探讨它们在高维环境下失效的原因，进而引出现代准则如扩展[贝叶斯信息准则](@entry_id:142416)（EBIC）和处理模型设定偏误的竹内[信息准则](@entry_id:636495)（TIC）的必要性与机制。

接着，在“**应用与交叉学科联系**”一章中，我们将展示这些理论工具如何在各种实际问题中发挥作用，包括高维[稀疏回归](@entry_id:276495)中的[变量选择](@entry_id:177971)、生物信息学中的[网络推断](@entry_id:262164)，以及如何将[信息准则](@entry_id:636495)的思想推广到[字典学习](@entry_id:748389)、结构化稀疏等更复杂的[模型选择](@entry_id:155601)任务中。

最后，在“**动手实践**”部分，我们提供了一系列精心设计的问题，引导读者亲手推导和应用[信息准则](@entry_id:636495)，将抽象的理论转化为具体可操作的技能。

通过这一结构化的学习路径，读者将不仅掌握[信息准则](@entry_id:636495)的数学公式，更能深刻理解其背后的统计思想，从而在自己的研究和实践中游刃有余地选择、评判和构建科学模型。

## 原理与机制

本章旨在深入探讨[模型选择](@entry_id:155601)中[信息准则](@entry_id:636495)的根本原理与内在机制。在前一章介绍[模型选择](@entry_id:155601)的基本概念之后，我们将从第一性原理出发，系统地构建和理解各类[信息准则](@entry_id:636495)，从经典的[赤池信息准则](@entry_id:139671)（Akaike Information Criterion, AIC）到适用于高维[稀疏优化](@entry_id:166698)的现代准则，如扩展[贝叶斯信息准则](@entry_id:142416)（Extended Bayesian Information Criterion, EBIC）。我们将揭示这些准则如何[平衡模型](@entry_id:636099)的[拟合优度](@entry_id:637026)与复杂度，并阐明它们在不同统计设定下的理论保证与局限性。

### 从预测风险到[赤池信息准则 (AIC)](@entry_id:193149)

[模型选择](@entry_id:155601)的一个核心目标是找到一个不仅能很好地解释现有数据，而且能对未来新数据做出准确预测的模型。换言之，我们追求的是最小化**预测风险**。一个衡量预测性能的强大理论工具是**库尔贝克-莱布勒（Kullback-Leibler, KL）散度**，它量化了当我们使用一个近似模型（例如，通过数据拟合得到的模型）来代表真实的、未知的数据生成过程时所造成的信息损失。

考虑一个高斯[线性模型](@entry_id:178302)，其中数据 $y \in \mathbb{R}^{n}$ 由 $y \sim \mathcal{N}(\mu^\star, \sigma^2 I_n)$ 生成，$\mu^\star = X \beta^\star$ 是真实的[均值向量](@entry_id:266544)，$\sigma^2$ 是已知的噪声[方差](@entry_id:200758)。对于一个候选的稀疏支撑集 $S$，我们可以通过[最小二乘法](@entry_id:137100)得到一个估计的[均值向量](@entry_id:266544) $\hat{\mu}_S = P_S y$，其中 $P_S$ 是到由[设计矩阵](@entry_id:165826) $X$ 的[子集](@entry_id:261956) $X_S$ 的[列空间](@entry_id:156444)上的正交投影算子。我们用拟合模型 $\mathcal{N}(\hat{\mu}_S, \sigma^2 I_n)$ 来近似真实模型 $\mathcal{N}(\mu^\star, \sigma^2 I_n)$。

KL散度的定义为 $D_{\mathrm{KL}}(P \,\|\, Q) = \mathbb{E}_{P}[\log p(Y) - \log q(Y)]$。对于这两个具有相同协[方差](@entry_id:200758)的[高斯分布](@entry_id:154414)，其[KL散度](@entry_id:140001)可以被推导为：
$$
D_{\mathrm{KL}}\!\left(\mathcal{N}(\mu^\star, \sigma^2 I_n) \,\big\|\, \mathcal{N}(\hat{\mu}_S, \sigma^2 I_n) \right) = \frac{1}{2\sigma^2} \|\mu^\star - \hat{\mu}_S\|_2^2
$$
这个量是依赖于数据 $y$ 的[随机变量](@entry_id:195330)，因为它决定了 $\hat{\mu}_S$。为了得到一个稳定的模型选择准则，我们关心的是其[期望值](@entry_id:153208)，即**KL风险**：
$$
\text{KL Risk}(S) = \mathbb{E}_y\left[ \frac{1}{2\sigma^2} \|\mu^\star - \hat{\mu}_S\|_2^2 \right] = \frac{1}{2\sigma^2} \mathbb{E}_y\left[ \|\mu^\star - \hat{\mu}_S\|_2^2 \right]
$$
$\mathbb{E}_y[ \|\mu^\star - \hat{\mu}_S\|_2^2 ]$ 正是估计量 $\hat{\mu}_S$ 的均方误差（Mean Squared Error, MSE）。通过经典的偏置-[方差分解](@entry_id:272134)，我们可以证明：
$$
\mathbb{E}_y\left[ \|\mu^\star - \hat{\mu}_S\|_2^2 \right] = \|(I - P_S) \mu^\star\|_2^2 + \sigma^2 d_S
$$
其中 $d_S = \mathrm{rank}(X_S)$ 是模型 $S$ 的维度。第一项 $\|(I - P_S) \mu^\star\|_2^2$ 是**近似误差**（偏置的平方），表示将真实信号投影到模型[子空间](@entry_id:150286)所造成的系统性偏差。第二项 $\sigma^2 d_S$ 是**估计误差**（[方差](@entry_id:200758)），反映了由于噪声存在而导致的估计不确定性，它随着模型维度的增加而增大。因此，选择最佳模型需要在减小近似误差和控制[估计误差](@entry_id:263890)之间做出权衡 [@problem_id:3452849]。

然而，KL风险依赖于未知的真实均值 $\mu^\star$，无法直接计算。我们需要一个可观测的、数据驱动的估计量。通过分析[残差平方和](@entry_id:174395)（Residual Sum of Squares, RSS）$\mathrm{RSS}_S = \|y - \hat{\mu}_S\|_2^2$ 的期望，可以建立以下关系：
$$
\mathbb{E}_y[\mathrm{RSS}_S] = \|(I - P_S)\mu^\star\|_2^2 + \sigma^2(n - d_S)
$$
结合前面的MSE分解，我们发现 $\mathrm{RSS}_S + 2\sigma^2 d_S - n\sigma^2$ 是MSE的一个[无偏估计](@entry_id:756289)。因此，最小化KL风险等价于最小化以下准则：
$$
C_p(S) = \|y - P_S y\|_2^2 + 2\sigma^2 d_S
$$
这正是著名的**马洛斯$C_p$准则（Mallows's $C_p$）**。当噪声[方差](@entry_id:200758) $\sigma^2$ 未知时，通常用一个全局模型的估计值 $\hat{\sigma}^2$ 代替。

**[赤池信息准则](@entry_id:139671)（AIC）** 提供了另一条通向相同结果的路径。AIC的一般形式为：
$$
\mathrm{AIC} = -2 \times (\text{最大化对数似然}) + 2 \times (\text{参数数量})
$$
对于我们的高斯模型，$-2 \times (\text{最大化对数似然})$ 项与 $\frac{1}{\sigma^2}\|y - P_S y\|_2^2$ 成正比。因此，最小化AIC等价于最小化 $C_p$ 准则。AIC的核心思想是通过一个惩罚项（$2d_S$）来校正样本内[拟合优度](@entry_id:637026)的“乐观偏误”，从而得到预测风险的一个近似无偏估计 [@problem_id:3452849]。这揭示了AIC的本质：它是一个以**预测为导向**的准则，其目标是选择在新数据上表现最好的模型，而非必然是“真实”的模型。

### [模型复杂度](@entry_id:145563)的推广：[有效自由度](@entry_id:161063)与SURE

AIC和$C_p$准则中的复杂度惩罚项 $2d_S$ 直接与模型中的参数数量挂钩。这在经典的线性模型中是直观的，但对于更复杂的估计过程，如Lasso等罚估计方法，参数数量的定义变得模糊。Lasso通过[连续收缩](@entry_id:154115)系数来产生稀疏解，一个$\ell_1$范数罚项的微小改变可能不会改变非零系数的个数，但会改变所有非零系数的值。那么，这类模型的“复杂度”该如何衡量？

一个更普适的概念是**[有效自由度](@entry_id:161063)（effective degrees of freedom）**。对于一个（可能[非线性](@entry_id:637147)的）估计器 $\hat{\mu}(y)$，其[有效自由度](@entry_id:161063)可以定义为 [@problem_id:3452883]：
$$
\mathrm{df}(\hat{\mu}) = \frac{1}{\sigma^2} \sum_{i=1}^n \mathrm{Cov}(y_i, \hat{\mu}_i(y))
$$
这个定义衡量了观测值 $y_i$ 与其自身拟合值 $\hat{\mu}_i(y)$ 之间的协[方差](@entry_id:200758)总和，并用噪声[方差](@entry_id:200758)进行了标准化。它捕捉了数据对拟合值的平均影响程度。

在[高斯噪声](@entry_id:260752)的设定下，一个深刻的结论将这个看似抽象的定义与一个可计算的量联系起来。借助**[斯坦因引理](@entry_id:261636)（Stein's Lemma）**，一种[高斯积分](@entry_id:187139)的分部积分技巧，可以证明：
$$
\mathrm{df}(\hat{\mu}) = \mathbb{E}[\mathrm{div}\,\hat{\mu}(y)]
$$
其中 $\mathrm{div}\,\hat{\mu}(y) = \sum_{i=1}^n \frac{\partial \hat{\mu}_i(y)}{\partial y_i}$ 是估计器映射 $\hat{\mu}$ 的**散度（divergence）**。这个等式（有时被称为SURE自由度公式）极为重要，因为它表明模型的有效复杂度可以通过其拟合函数对输入的敏感度（即散度）的期望来衡量。值得注意的是，这个简洁的关系严重依赖于噪声的高斯特性，对于其他噪声[分布](@entry_id:182848)通常不成立 [@problem_id:3452883]。

我们可以检验这个推广的定义在一些熟悉场景下的表现：
-   对于线性平滑器 $\hat{\mu}(y) = Sy$，其中 $S$ 是一个固定的平滑矩阵，其散度 $\mathrm{div}\,\hat{\mu}(y) = \mathrm{tr}(S)$ 是一个常数。因此，$\mathrm{df}(\hat{\mu}) = \mathrm{tr}(S)$，这与经典的自由度定义完全一致。
-   对于Lasso问题中的核心操作——**[软阈值](@entry_id:635249)（soft-thresholding）**估计器 $\hat{\mu}_i(y) = \mathrm{sign}(y_i)(|y_i| - \lambda)_+$，其（弱）导数为 $\frac{d \hat{\mu}_i(y_i)}{d y_i} = \mathbf{1}_{|y_i| > \lambda}$。因此，其散度就是非零系数的个数，$\mathrm{div}\,\hat{\mu}(y) = \sum_i \mathbf{1}_{|y_i| > \lambda} = \|\hat{\mu}(y)\|_0$。这意味着[软阈值](@entry_id:635249)估计器的[有效自由度](@entry_id:161063)是其稀疏度（非零元素个数）的期望 [@problem_id:3452883]。

这个框架的实际应用是**[斯坦因无偏风险估计](@entry_id:634443)（Stein's Unbiased Risk Estimate, SURE）**。SURE为估计的MSE提供了一个[无偏估计量](@entry_id:756290)，其形式为：
$$
\mathrm{SURE}(\lambda) = \|y - \hat{\mu}(y)\|_2^2 - n\sigma^2 + 2\sigma^2 \mathrm{div}\,\hat{\mu}(y)
$$
最小化SURE可以用来选择模型中的超参数，例如Lasso中的[正则化参数](@entry_id:162917) $\lambda$。以正交[设计矩阵](@entry_id:165826) $A$ ($A^\top A = I_p$)下的Lasso为例，其[估计风险](@entry_id:139340) $\mathbb{E}\|x_\lambda - x\|_2^2$ 和预测风险 $\mathbb{E}\|A x_\lambda - \tilde{y}\|_2^2$ 之间存在简单关系。通过为[估计风险](@entry_id:139340)构建SURE，我们可以推导出预测风险的一个无偏估计，即一个广义[信息准则](@entry_id:636495)（GIC）式的规则，用于选择最优的 $\lambda$ [@problem_id:3452915]。对于[软阈值](@entry_id:635249)估计器，该准则的具体形式为：
$$
C(\lambda) = \sum_{i=1}^{p} \min(((A^{\top}y)_i)^2, \lambda^2) + (n-p)\sigma^{2} + 2\sigma^{2} \sum_{i=1}^{p} \mathbf{1}_{|(A^{\top}y)_i| > \lambda}
$$
这个例子完美地展示了如何将[有效自由度](@entry_id:161063)的理论应用于实际的[稀疏优化](@entry_id:166698)问题中，为[超参数调优](@entry_id:143653)提供了一个坚实的理论基础。

### 一致性视角：[贝叶斯信息准则 (BIC)](@entry_id:181959) 及其局限性

AIC及其变体（如SURE）在预测方面表现优异，但它们通常不具备**[模型选择一致性](@entry_id:752084)（model selection consistency）**。一致性意味着当样本量 $n \to \infty$ 时，选中真实数据[生成模型](@entry_id:177561)的概率趋向于1。AIC之所以不一致，是因为其固定的惩罚项（$2d$）不足以抑制[过拟合](@entry_id:139093)的趋势。当比较真实模型与一个仅多包含一个无关变量的[嵌套模型](@entry_id:635829)时，增加这个无关变量所带来的[对数似然](@entry_id:273783)的随机增益服从一个$\chi^2_1$[分布](@entry_id:182848)。AIC选择更大模型的概率 $P(\chi^2_1 > 2) \approx 0.157$，这是一个不随样本量 $n$ 变化的常数。因此，AIC总是有一定的概率选择过于复杂的模型 [@problem_id:3452886] [@problem_id:3452925]。

为了实现一致性，我们需要一个更强的惩罚项。**[贝叶斯信息准则](@entry_id:142416)（Bayesian Information Criterion, BIC）** 应运而生。其定义为：
$$
\mathrm{BIC} = -2 \times (\text{最大化对数似然}) + d \log n
$$
BIC的惩罚项 $d \log n$ 随样本量 $n$ 的对数增长。这个形式源于对模型边缘[似然](@entry_id:167119)的[拉普拉斯近似](@entry_id:636859)，因此具有贝叶斯背景。

在经典的固定维度 $p$ 设定下，BIC是强一致的。其一致性的原因可以从两个方面理解 [@problem_id:3452858]：
1.  **对于[欠拟合](@entry_id:634904)模型（underfitted models）**（即没有包含所有真实变量的模型）：由于模型设定偏误，其最大化[对数似然](@entry_id:273783)与真实模型的对数似然之差会以 $O(n)$ 的速度增长。这个巨大的拟合损失会远超 $\log n$ 阶的惩罚项差异，因此BIC几乎不可能选择[欠拟合](@entry_id:634904)模型。
2.  **对于过拟合模型（overfitted models）**（即包含了真实模型以及一些无关变量的模型）：增加无关变量带来的[对数似然](@entry_id:273783)增益是 $O_p(1)$ 的（服从 $\chi^2$ [分布](@entry_id:182848)）。然而，BIC的惩罚项增量为 $\log n$，它会随着 $n \to \infty$ 而发散。因此，惩罚项的增长最终会压倒似然的微小增益，使得BIC能够剔除无关变量，选择正确的模型。

然而，BIC的辉煌在**高维设定**（即维度 $p$ 随样本量 $n$ 一起增长）下黯然失色。其一致性的证明依赖于对一个或有限个备选模型的比较。当 $p$ 很大时，候选模型的数量是组合爆炸式的（例如，有 $\binom{p}{k}$ 个大小为 $k$ 的模型）。在如此巨大的模型空间中搜索，仅仅由于随机性，就[几乎必然](@entry_id:262518)能找到一些无关变量，它们与响应的[伪相关](@entry_id:755254)性足以产生显著的[似然](@entry_id:167119)增益。这个由**多重性（multiplicity）**引起的最大伪似然增益不再是 $O_p(1)$，而是以 $O_p(\log p)$ 的速度增长。BIC的惩罚增量 $\log n$ 要想压制这个效应，就必须满足 $\log p = o(\log n)$。当 $p$ 以多项式速度（$p=n^\alpha$）或指数速度（$p=e^{n^\alpha}$）增长时，这个条件就会被打破，BIC会失去一致性，并倾向于选择过大的模型 [@problem_id:3452858] [@problem_id:3452925]。

### 高维环境下的[信息准则](@entry_id:636495)：为[多重性](@entry_id:136466)进行校正 (EBIC)

[高维统计](@entry_id:173687)的核心挑战之一就是如何设计能够在指数级增长的模型空间中有效工作的模型选择准则。AIC和BIC的失败根源在于它们的惩罚项未能充分考虑到被搜索的[模型空间](@entry_id:635763)的巨大规模。

一个更深刻的理解来自于分析噪声本身能带来的最大拟合改善。考虑一个纯[噪声模型](@entry_id:752540) $y=w$，其中 $w \sim \mathcal{N}(0, \sigma^2 I_n)$。当我们用一个 $s$ 维的[子空间](@entry_id:150286)去拟合这个噪声时，[残差平方和](@entry_id:174395)的减小量为 $\|P_S w\|_2^2$。在 $p \gg n$ 的情况下，我们需要控制的是在所有 $\binom{p}{s}$ 个可能的 $s$ 维[子空间](@entry_id:150286)中，这个减小量的**最大值**。通过对 $\chi^2$ [分布](@entry_id:182848)的[极值理论](@entry_id:140083)和[联合界](@entry_id:267418)（union bound）分析可以表明，这个最大值以 $\sigma^2 s \log(p/s)$ 或更粗略地以 $\sigma^2 \log\binom{p}{s}$ 的速度增长 [@problem_id:3452844]。一个有效的惩罚项必须至少与这个量级相当，才能避免在纯噪声中发现虚[假结](@entry_id:168307)构。

这直接催生了**扩展[贝叶斯信息准则](@entry_id:142416)（Extended Bayesian Information Criterion, EBIC）** [@problem_id:3452906]。EBIC在BIC的基础上增加了一个明确针对模型[空间复杂度](@entry_id:136795)的惩罚项：
$$
\mathrm{EBIC}_\gamma(S) = -2\,\ell_n(\hat{\theta}_S) + k\,\log n + 2\,\gamma\,\log\binom{p}{k}
$$
其中 $k=|S|$ 是模型 $S$ 的大小，$\gamma \in [0,1]$ 是一个超参数。
-   当 $\gamma=0$ 时，EBIC退化为BIC。
-   当 $\gamma>0$ 时，新增的 $2\gamma\log\binom{p}{k}$ 项直接惩罚了从 $p$ 个变量中选择 $k$ 个变量的组合复杂度。

$\gamma$ 的作用是调节对[多重性](@entry_id:136466)的校正强度。当 $p$ 增长速度超过 $\log n$ 时（例如多项式或指数增长），$\log\binom{p}{k} \approx k \log p$ 项将主导惩罚。通过选择一个合适的正 $\gamma$ 值，EBIC的惩罚可以压倒 $O_p(\log p)$ 的最大伪似然增益，从而在BIC失效的高维[体制](@entry_id:273290)下恢复[模型选择](@entry_id:155601)的一致性。理论研究表明，当 $p$ 以 $\exp(n^\alpha)$ 的速度增长时，只要选择足够大的 $\gamma > 0$，EBIC就能实现一致性，而BIC ($\gamma=0$) 则会因大量伪发现而失效 [@problem_id:3452906] [@problem_id:3452925]。

当然，这种增强的伪发现控制并非没有代价。更大的惩罚（即更大的 $\gamma$）虽然能更有效地排除噪声变量（降低假阳性），但也增加了排除真实但信号较弱的变量的风险（提高假阴性）。这体现了[模型选择](@entry_id:155601)中灵敏度与特异性之间永恒的权衡 [@problem_id:3452906]。

EBIC的惩罚形式也可以从**[最小描述长度](@entry_id:261078)（Minimum Description Length, MDL）**原理中得到启发。MDL的目标是找到一个能以最短编码长度来描述数据和模型本身的组合。一个完整的编码包括三个部分：描述模型结构（即支撑集 $S$）、描述模型参数（$\beta_S$）和描述在给定模型下的数据残差。可以证明，在适当的[正则性条件](@entry_id:166962)下 [@problem_id:3452893]：
-   描述残差的最优编码长度对应于 $- \log \mathcal{L}$ 项。
-   描述 $k$ 个参数的最优编码长度对应于 $\frac{k}{2}\log n$ 项。
-   描述从 $p$ 个变量中选择 $k$ 个的支撑集 $S$ 本身，需要 $\log\binom{p}{k}$ 的编码长度。

将这些编码长度加总，得到的MDL准则与EBIC在 $\gamma=1$ 时的形式惊人地一致（相差一个因子2）。这为EBIC，特别是其附加的组合惩罚项，提供了来[自信息](@entry_id:262050)论的坚实支持。

### 高级主题：处理模型设定偏误 (TIC)

到目前为止，我们的讨论大多局限于一个“良好设定”的世界，即假设我们使用的[概率模型](@entry_id:265150)（如高斯噪声模型）是正确的。然而在实践中，真实的数据生成过程可能与我们的工作模型有偏差，例如噪声可能是异[方差](@entry_id:200758)的、非高斯的，或者均值结构并非严格线性。在这种**模型设定偏误（model misspecification）**的情况下，AIC的理论基础——即其作为KL风险的无偏估计——就不再成立。

为了解决这个问题，**竹内[信息准则](@entry_id:636495)（Takeuchi's Information Criterion, TIC）** 被提了出来。TIC可以被看作是AIC在模型设定偏误下的一个鲁棒性修正。其一般形式为：
$$
\mathrm{TIC} = -2 \times (\text{最大化对数似然}) + 2 \times \mathrm{tr}(J^{-1}K)
$$
这里的关键变化在于惩罚项。简单地计算参数数量 $d$ 已不再足够。取而代之的是一个更复杂的项 $\mathrm{tr}(J^{-1}K)$，其中：
-   $J = E[-\nabla^2 \ell(\theta)]$ 是工作模型（我们假设的模型）下的期望Hessian矩阵（信息矩阵）。
-   $K = E[\nabla \ell(\theta) (\nabla \ell(\theta))^\top]$ 是在**真实**数据生成[分布](@entry_id:182848)下的[得分函数](@entry_id:164520)[协方差矩阵](@entry_id:139155)。

如果模型设定正确，那么[信息矩阵](@entry_id:750640)等式成立（$J=K$），$\mathrm{tr}(J^{-1}K)$ 就退化为参数数量 $d$，TIC也随之退化为AIC。但当[模型设定错误](@entry_id:170325)时，$J \neq K$，$J^{-1}K$ 校正了由于模型偏误导致的乐观偏误估计偏差。

在实践中，矩阵 $J$ 和 $K$ 是未知的，需要从数据中估计。这通常通过所谓的**[三明治估计量](@entry_id:754503)（sandwich estimators）**来完成。例如，$\widehat{J}$ 可以是观测到的Hessian矩阵，而 $\widehat{K}$ 可以是[得分函数](@entry_id:164520)在每个数据点上的经验外积之和。

这个思想可以被推广到像Lasso这样的罚估计问题上 [@problem_id:3452899]。对于一个给定的Lasso解 $\widehat{\beta}$ 及其活动集 $A$（非零系数的[指标集](@entry_id:268489)），我们可以将TIC的思想应用于这个活动集对应的[子模](@entry_id:148922)型。此时，[有效自由度](@entry_id:161063)被定义为：
$$
\mathrm{df}_{\mathrm{TIC}} = \mathrm{trace}(\widehat{J}_{A}^{-1}\widehat{K}_{A})
$$
其中 $\widehat{J}_{A}$ 和 $\widehat{K}_{A}$ 是在活动集 $A$ 上计算的经验三明治矩阵。例如，在一个加权最小二乘的设定下，它们的形式为：
$$
\widehat{J}_{A} = X_{A}^{\top} W X_{A}, \quad \widehat{K}_{A} = X_{A}^{\top} W \widehat{\Sigma} W X_{A}
$$
这里 $W$ 是权重矩阵，$\widehat{\Sigma}$ 是由残差平方构成的对角矩阵，它捕捉了噪声的[异方差性](@entry_id:136378)等偏误信息。通过计算这个更鲁棒的自由度，我们可以得到一个适用于Lasso在模型设定偏误下的TIC版本 $\mathrm{TIC}_{\mathrm{Lasso}} = -2\ell(\widehat{\beta}) + 2\,\mathrm{df}_{\mathrm{TIC}}$，为在更现实、更复杂的场景中进行[模型选择](@entry_id:155601)提供了有力的工具 [@problem_id:3452899]。