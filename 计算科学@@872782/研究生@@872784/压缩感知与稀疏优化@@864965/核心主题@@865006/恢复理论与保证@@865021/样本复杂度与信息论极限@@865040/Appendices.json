{"hands_on_practices": [{"introduction": "第一个练习将带您进入高维统计的核心，推导稀疏信号恢复的基本极限。通过应用信息论的第一性原理，您将确定在高斯噪声下估计稀疏向量的极小极大风险[@problem_id:3474986]。这项实践对于理解著名的 $k \\ln(p/k)$ 标度律至关重要，并为所有实际估计器的性能提供了一个基准。", "problem": "考虑带有高斯噪声的高维线性回归模型。令 $X \\in \\mathbb{R}^{n \\times p}$ 为一个固定的设计矩阵，其列被归一化，使得对于每个列索引 $j \\in \\{1,\\dots,p\\}$，都有 $(1/n)\\|X_{j}\\|_{2}^{2} = 1$。对于一个未知的参数向量 $\\theta^{\\star} \\in \\mathbb{R}^{p}$，观测值由 $Y = X \\theta^{\\star} + \\varepsilon$ 给出，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 且 $\\sigma > 0$ 是已知的。假设 $\\theta^{\\star}$ 是 $k$-稀疏的，即 $\\|\\theta^{\\star}\\|_{0} \\leq k$，其中 $k \\in \\{1,\\dots,p\\}$。\n\n定义参数空间 $\\Theta_{k} = \\{\\theta \\in \\mathbb{R}^{p} : \\|\\theta\\|_{0} \\leq k\\}$。考虑估计量 $\\widehat{\\theta}(Y)$ 和极小化极大均方 $\\ell_{2}$ 估计风险\n$$\nR^{\\star} = \\inf_{\\widehat{\\theta}} \\sup_{\\theta^{\\star} \\in \\Theta_{k}} \\mathbb{E}\\big[\\|\\widehat{\\theta}(Y) - \\theta^{\\star}\\|_{2}^{2}\\big].\n$$\n仅使用信息论和高维统计中的基本原理，即 (i) 高斯位移模型的 Kullback–Leibler 散度，(ii) 信息论中的 Fano 不等式，(iii) 离散子集选择的标准打包论证，以及 (iv) 以下定义。\n\n对于稀疏度水平 $s \\in \\{1,\\dots,p\\}$，定义受限最小和最大特征值常数\n$$\n\\kappa_{s} = \\inf_{\\substack{u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u\\|_{0} \\leq s}} \\frac{1}{n} \\frac{\\|X u\\|_{2}^{2}}{\\|u\\|_{2}^{2}}, \\qquad \\Lambda_{s} = \\sup_{\\substack{u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u\\|_{0} \\leq s}} \\frac{1}{n} \\frac{\\|X u\\|_{2}^{2}}{\\|u\\|_{2}^{2}},\n$$\n以及兼容性常数（在最小绝对值收缩和选择算子 (LASSO) 的锥限制分析意义下）\n$$\n\\phi(s) = \\inf_{\\substack{S \\subset \\{1,\\dots,p\\},\\, |S| \\leq s \\\\ u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u_{S^{c}}\\|_{1} \\leq 3 \\|u_{S}\\|_{1}}} \\frac{\\sqrt{n}\\,\\|X u\\|_{2}}{\\|u_{S}\\|_{1}/\\sqrt{s}}.\n$$\n\n从上面列出的基本事实出发（不引入任何快捷公式），推导在 $p \\gg k$ 和 $k \\ln(p/k) \\ll n$ 的条件下，$\\Theta_{k}$ 上的极小化极大风险 $R^{\\star}$ 的主阶渐近表达式，并将此速率与知道 $\\theta^{\\star}$ 的支撑集但不知道其符号或大小的估计量的神谕风险（oracle risk）明确联系起来，以及与兼容性常数和受限特征值常数在可达上界中的作用联系起来。\n\n你的最终答案必须是关于 $n$、$p$、$k$、$\\sigma$ 和一个适当的受限设计常数的主阶极小化极大速率 $R^{\\star}$ 的单一闭式解析表达式。最终答案中不允许出现不等式。不要包含任何单位。如果你引入任何渐近近似，忽略绝对常数，并仅以闭式表达式的形式给出主导项。", "solution": "该问题要求在高维稀疏线性回归模型中，极小化极大均方 $\\ell_{2}$ 估计风险的主阶渐近表达式。推导必须从第一性原理出发。极小化极大风险定义为\n$$\nR^{\\star} = \\inf_{\\widehat{\\theta}} \\sup_{\\theta^{\\star} \\in \\Theta_{k}} \\mathbb{E}\\big[\\|\\widehat{\\theta}(Y) - \\theta^{\\star}\\|_{2}^{2}\\big],\n$$\n其中 $\\Theta_{k} = \\{\\theta \\in \\mathbb{R}^{p} : \\|\\theta\\|_{0} \\leq k\\}$。我们将通过使用信息论方法推导一个下界，然后论证该下界是可达的，从而确定 $R^\\star$ 的速率，进而确定极小化极大速率。\n\n首先，我们使用 Fano 不等式推导 $R^{\\star}$ 的一个下界。该方法需要构造参数空间 $\\Theta_k$ 的一个有限子集，记为 $\\Theta_0 = \\{\\theta_1, \\dots, \\theta_M\\}$，使得其元素在估计度量（$\\ell_2$-范数）下是良好分离的，而它们所诱导的统计模型 $P_i = \\mathcal{N}(X\\theta_i, \\sigma^2 I_n)$ 难以区分。\n\n令 $\\inf_{\\widehat{\\theta}} \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}[\\|\\widehat{\\theta} - \\theta\\|_2^2]$ 为子集 $\\Theta_0$ 上的极小化极大风险。Fano 不等式的一个标准推论指出，如果我们能构造一个集合 $\\Theta_0 = \\{\\theta_1, \\dots, \\theta_M\\} \\subset \\Theta_k$ 满足：\n1. 对于所有 $i \\neq j$，有 $\\|\\theta_i - \\theta_j\\|_2 \\ge \\delta$。\n2. 任意两个对应分布 $P_i$ 和 $P_j$ 之间的 Kullback-Leibler (KL) 散度有界，即 $D_{KL}(P_i \\| P_j) \\le \\alpha \\ln(M)$，其中 $\\alpha  1$ 是某个常数。\n\n那么，$\\Theta_k$ 上的极小化极大风险的下界为\n$$\nR^{\\star} \\ge \\inf_{\\widehat{\\theta}} \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}[\\|\\widehat{\\theta} - \\theta\\|_2^2] \\ge \\frac{\\delta^2}{2}\\left(1 - \\alpha - \\frac{\\ln 2}{\\ln M}\\right).\n$$\n\n我们现在构造这样一个集合 $\\Theta_0$。这是一个标准的打包论证。令 $\\mathcal{W}$ 为二元向量 $\\{0, 1\\}^p$ 的一个子集，其中每个 $\\omega \\in \\mathcal{W}$ 的汉明权重为 $\\|\\omega\\|_0=k$。使用编码理论中的 Varshamov-Gilbert 界，可以构造这样一个集合 $\\mathcal{W}$，其中任意两个不同元素 $\\omega_i, \\omega_j \\in \\mathcal{W}$ 之间的汉明距离至少为 $d_H(\\omega_i, \\omega_j) \\ge k$。在 $p \\gg k$ 的条件下，这个集合的大小 $M=|\\mathcal{W}|$ 保证满足 $\\ln(M) \\ge c_1 k \\ln(\\frac{p}{k})$，其中 $c_1 > 0$ 是某个常数。\n\n我们基于这个打包集 $\\mathcal{W}$ 定义我们的参数子集 $\\Theta_0$ 为 $\\Theta_0 = \\{a \\cdot \\omega : \\omega \\in \\mathcal{W}\\}$，其中 $a$ 是一个待选的标量幅值。每个 $\\theta \\in \\Theta_0$ 都是 $k$-稀疏的，因此 $\\Theta_0 \\subset \\Theta_k$。\n\n接下来，我们确定分离度 $\\delta$。对于 $\\Theta_0$ 中任意两个不同的 $\\theta_i = a \\omega_i$ 和 $\\theta_j = a \\omega_j$：\n$$\n\\|\\theta_i - \\theta_j\\|_2^2 = a^2 \\|\\omega_i - \\omega_j\\|_2^2 = a^2 d_H(\\omega_i, \\omega_j) \\ge a^2 k.\n$$\n因此我们可以设置 $\\delta^2 = a^2 k$。\n\n现在，我们来界定 KL 散度。对于两个多元高斯分布 $P_i = \\mathcal{N}(\\mu_i, \\sigma^2 I_n)$ 和 $P_j = \\mathcal{N}(\\mu_j, \\sigma^2 I_n)$，KL 散度为 $D_{KL}(P_i \\| P_j) = \\frac{1}{2\\sigma^2} \\|\\mu_i - \\mu_j\\|_2^2$。在我们的情况下，$\\mu_i = X\\theta_i$，所以：\n$$\nD_{KL}(P_i \\| P_j) = \\frac{1}{2\\sigma^2} \\|X(\\theta_i - \\theta_j)\\|_2^2 = \\frac{a^2}{2\\sigma^2} \\|X(\\omega_i - \\omega_j)\\|_2^2.\n$$\n令 $u = \\omega_i - \\omega_j$。向量 $u$ 的元素来自 $\\{-1, 0, 1\\}$。其非零元素的数量为 $\\|u\\|_0 = d_H(\\omega_i, \\omega_j)$。由于 $\\|\\omega_i\\|_0 = \\|\\omega_j\\|_0 = k$，向量 $u$ 的稀疏度在范围 $k \\le \\|u\\|_0 \\le 2k$ 内。我们可以使用受限最大特征值常数 $\\Lambda_s$ 来获得一个上界：\n$$\n\\|Xu\\|_2^2 = n \\cdot \\frac{\\|Xu\\|_2^2}{n \\|u\\|_2^2} \\cdot \\|u\\|_2^2 \\le n \\cdot \\Lambda_{\\|u\\|_0} \\cdot \\|u\\|_2^2 \\le n \\cdot \\Lambda_{2k} \\cdot \\|u\\|_2^2.\n$$\n由于 $\\|u\\|_2^2 = d_H(\\omega_i, \\omega_j) \\le 2k$，我们有：\n$$\nD_{KL}(P_i \\| P_j) \\le \\frac{a^2}{2\\sigma^2} n \\Lambda_{2k} (2k) = \\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2}.\n$$\n为了应用 Fano 不等式，我们需要这个 KL 散度是组合熵 $\\ln(M)$ 的一小部分。我们要求 $\\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2} \\le \\frac{1}{2} \\ln(M)$。使用大小界 $\\ln(M) \\ge c_1 k \\ln(p/k)$，我们需要：\n$$\n\\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2} \\le \\frac{c_1}{2} k \\ln(p/k).\n$$\n这限制了幅值 $a$。我们选择 $a$ 尽可能大同时满足这个条件，所以我们设置 $a^2$ 与上界成正比：\n$$\na^2 = c_2 \\frac{\\sigma^2 \\ln(p/k)}{n \\Lambda_{2k}},\n$$\n对于某个足够小的常数 $c_2$。将此代回 $\\delta^2 = a^2 k$ 的表达式中：\n$$\n\\delta^2 = c_2 \\frac{\\sigma^2 k \\ln(p/k)}{n \\Lambda_{2k}}.\n$$\n将其代入 Fano 不等式，对于合适的常数，项 $(1 - \\alpha - \\frac{\\ln 2}{\\ln M})$ 是一个正常数。因此，极小化极大风险的下界为：\n$$\nR^{\\star} \\gtrsim \\delta^2 \\asymp \\frac{\\sigma^2 k \\ln(p/k)}{n \\Lambda_{2k}}.\n$$\n这就确立了极小化极大速率的下界。\n\n论证的第二部分是证明这个速率是可达的。高维统计中的一个著名结果断言，在设计矩阵 $X$ 满足某些条件下，这个速率可以通过一个实际的估计量达到，例如 LASSO (最小绝对值收缩和选择算子) 或 Dantzig 选择器。对此类估计量的分析表明，它们的均方误差由一个同阶的量作为上界。例如，LASSO 估计量的性能由设计矩阵的条件保证，如兼容性常数 $\\phi(s)$ 或像 $\\kappa_s > 0$ 这样的受限特征值条件。LASSO 风险的一个典型上界形式为：\n$$\n\\sup_{\\theta^{\\star} \\in \\Theta_k} \\mathbb{E}[\\|\\widehat{\\theta}_{LASSO} - \\theta^{\\star}\\|_2^2] \\lesssim \\frac{\\sigma^2 k \\ln(p/k)}{n \\kappa_{2k}^2}.\n$$\n要使设计矩阵“表现良好”，其受限特征值必须有界于零和无穷大之外，即对于所有相关的 $s$，有 $0  c_L \\le \\kappa_s \\le \\Lambda_s \\le c_U  \\infty$。在这种情况下，下界（与 $1/\\Lambda_{2k}$ 成比例）和上界（与 $1/\\kappa_{2k}^2$ 成比例）在关于 $n, p, k$ 和 $\\sigma$ 的尺度上是匹配的。忽略绝对数值常数并假设有此种表现良好的设计，则速率匹配。因此，推导出的下界是紧的，并代表了此估计问题的基本统计极限。\n\n这个极小化极大速率可以与“神谕”(oracle) 风险进行比较。一个神谕估计量，它知道 $\\theta^\\star$ 的真实支撑集 $S$（其中 $|S|=k$），会对相应的列 $X_S$ 进行最小二乘拟合。其风险为 $\\sigma^2 \\text{Tr}((X_S^T X_S)^{-1})$，其阶数为 $\\frac{\\sigma^2 k}{n \\kappa_k}$。极小化极大速率包含一个额外的因子 $\\ln(p/k)$，这代表了因不知道支撑集而必须在 $\\binom{p}{k}$ 种可能性中进行搜索所付出的不可避免的统计代价。\n\n下界是最基本的量，因为它为任何可能的估计量提供了一个终极限制。出现在此界中的常数 $\\Lambda_{2k}$ 刻画了由固定设计矩阵 $X$ 施加的“最坏情况”下的难度。因此，极小化极大速率的主阶表达式由这个下界确定。", "answer": "$$\\boxed{\\frac{\\sigma^{2} k \\ln(p/k)}{n \\Lambda_{2k}}}$$", "id": "3474986"}, {"introduction": "在基础框架之上，本实践将探讨一个更现实的场景，即部分测量值丢失或损坏。您将运用互信息和信道容量的类比，推导当传感矩阵本身存在缺失条目时，所需测量次数的必要条件 [@problem_id:3474981]。这个练习展示了信息论方法在量化由数据不完美性导致的性能下降方面的强大威力与灵活性。", "problem": "考虑以下在感知矩阵条目中存在缺失数据的压缩感知模型。令 $n \\in \\mathbb{N}$ 和 $k \\in \\{1,\\dots,n\\}$，并令 $m \\in \\mathbb{N}$ 为线性测量的数量。未知信号 $x \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的，其支撑集 $S \\subset [n]$ 的大小为 $|S|=k$。其中 $S$ 是从 $[n]$ 的所有 $k$-子集中均匀随机抽取的，非零项 $\\{x_{j} : j \\in S\\}$ 是独立同分布的，服从 $\\mathcal{N}(0,\\tau^{2})$ 分布，其中 $\\tau^{2} > 0$ 是一个固定的常数。测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的条目 $a_{ij} \\sim \\mathcal{N}(0,1/n)$ 是独立同分布的，且独立于 $S$ 和 $x$。缺失数据由一个独立的掩码 $E \\in \\{0,1\\}^{m \\times n}$ 建模，其条目 $e_{ij} \\sim \\mathrm{Bernoulli}(1-q)$，其中 $q \\in [0,1)$ 是擦除概率。观测模型为\n$$\ny \\;=\\; (A \\circ E)\\, x \\;+\\; w,\n$$\n其中 $\\circ$ 表示逐元素（哈达玛）乘积，$w \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ 是方差为 $\\sigma^{2} > 0$ 的加性高斯白噪声。解码器接收 $(y,A,E)$，其目标是在 $n \\to \\infty$ 时以趋于零的平均错误概率恢复支撑集 $S$。\n\n从互信息、数据处理不等式以及加性高斯白噪声（AWGN）信道的经典单字母信道容量的定义出发，推导一个关于测量数量 $m$ 的可靠支撑集恢复的必要条件，该条件需明确体现由缺失矩阵条目引起的信息损失。你的推导必须从第一性原理出发，且不应假设任何专用于压缩感知的预先给定的公式。在对互信息进行上界估计时，你可以使用关于半正定矩阵行列式的已验证不等式。\n\n假设错误概率趋于零，并且控制组合复杂度的主要项是 $\\ln \\binom{n}{k}$。将你最终得到的关于 $m$ 的下界表示为一个单一的闭式解析表达式，该表达式应包含 $n$、$k$、$q$、$\\tau^{2}$ 和 $\\sigma^{2}$。使用自然对数，并且不要包含任何单位。最终答案必须是一个单一的解析表达式，而不是不等式或方程。", "solution": "目标是推导出一个关于测量数量 $m$ 的必要条件，以实现对 $k$-稀疏信号支撑集 $S$ 的可靠恢复。推导将基于信息论的第一性原理。\n\n令 $S$ 为代表未知信号 $x$ 支撑集的随机变量。令 $\\hat{S}$ 为解码器在观测到 $(y, A, E)$ 后生成的 $S$ 的估计。支撑集 $S$ 是从集合 $\\{1, \\dots, n\\}$ 的所有大小为 $k$ 的 $\\binom{n}{k}$ 个子集中均匀随机抽取的。因此，$S$ 的熵为 $H(S) = \\ln\\binom{n}{k}$。\n\n为实现可靠恢复，错误概率 $P_e = \\mathbb{P}(\\hat{S} \\neq S)$ 必须在 $n \\to \\infty$ 时趋于零。Fano不等式根据条件熵 $H(S|y,A,E)$ 给出了错误概率的下界：\n$$\nP_e \\ge \\frac{H(S|y,A,E) - \\ln(2)}{\\ln\\binom{n}{k}}\n$$\n为了使 $P_e \\to 0$，必须有 $H(S|y,A,E) = o(\\ln\\binom{n}{k})$。使用互信息的链式法则，有 $H(S|y,A,E) = H(S) - I(S;y,A,E)$。由于 $S$ 独立于随机矩阵 $A$ 和 $E$，我们有 $I(S;A,E)=0$，这意味着 $I(S;y,A,E) = I(S;y|A,E)$。注意这是对 $A$ 和 $E$ 所有实现的平均，即 $I(S;y|A,E) = \\mathbb{E}_{A,E}[I(S;y|A,E)]$。\n因此，可靠恢复的条件意味着 $I(S;y,A,E)$ 必须接近 $H(S)$。形式上，对于任何 $\\epsilon > 0$ 和足够大的 $n$：\n$$\nI(S;y,A,E) \\ge (1-\\epsilon)H(S) = (1-\\epsilon)\\ln\\binom{n}{k}\n$$\n在极限情况下，这建立了必要条件：\n$$\n\\ln\\binom{n}{k} \\lesssim I(S;y,A,E)\n$$\n问题陈述观测值为 $y = (A \\circ E)x + w$。信号 $x$ 由支撑集 $S$ 及在该支撑集上的独立同分布高斯值确定。在给定 $A$ 和 $E$ 的条件下，我们有一个马尔可夫链 $S \\to x \\to y$。根据数据处理不等式，对于 $A$ 和 $E$ 的任何给定实现，我们有 $I(S; y | A, E) \\le I(x; y | A, E)$。对 $A$ 和 $E$ 的所有实现求平均，不等式仍然成立：\n$$\nI(S;y,A,E) = \\mathbb{E}_{A,E}[I(S; y | A, E)] \\le \\mathbb{E}_{A,E}[I(x; y | A, E)] = I(x;y,A,E)\n$$\n综合这些结果，可靠支撑集恢复的一个必要条件是：\n$$\n\\ln\\binom{n}{k} \\lesssim I(x;y,A,E)\n$$\n现在我们来求 $I(x;y,A,E)$ 的一个上界。令 $\\Phi = A \\circ E$。对于一个固定的 $A$ 和 $E$ 的实现，互信息为 $I(x; y|A,E) = h(y|A,E) - h(y|x,A,E)$。\n在给定 $x, A, E$ 的条件下 $y$ 的条件熵是噪声项 $w$ 的熵：\n$$\nh(y|x,A,E) = h(\\Phi x + w | x, A, E) = h(w) = \\frac{m}{2}\\ln(2\\pi e \\sigma^2)\n$$\n在给定 $A, E$ 的条件下 $y$ 的熵的上界是一个具有相同协方差矩阵的高斯随机向量的熵。\n$$\nh(y|A,E) \\le \\frac{1}{2}\\ln\\det(2\\pi e \\text{Cov}(y|A,E))\n$$\n在给定 $A, E$ 的条件下 $y$ 的协方差为：\n$$\n\\text{Cov}(y|A,E) = \\mathbb{E}_{x,w}[yy^T | A,E] = \\mathbb{E}[(\\Phi x+w)(\\Phi x+w)^T | A,E]\n$$\n由于 $x$ 和 $w$ 是独立的、零均值的，并且也独立于 $A,E$：\n$$\n\\text{Cov}(y|A,E) = \\Phi \\mathbb{E}[xx^T] \\Phi^T + \\mathbb{E}[ww^T] = \\Phi \\text{Cov}(x) \\Phi^T + \\sigma^2 I_m\n$$\n我们计算 $x$ 的协方差。对于任何对角元素，$\\mathbb{E}[x_i^2] = \\mathbb{P}(i \\in S)\\mathbb{E}[x_i^2|i \\in S] + \\mathbb{P}(i \\notin S)\\cdot 0$。由于 $S$ 是一个均匀随机的 $k$-子集，$\\mathbb{P}(i \\in S) = k/n$。给定 $i \\in S$，$x_i \\sim \\mathcal{N}(0, \\tau^2)$，所以 $\\mathbb{E}[x_i^2|i \\in S] = \\tau^2$。因此，$\\mathbb{E}[x_i^2] = \\frac{k\\tau^2}{n}$。对于任何非对角元素 $i \\neq j$，$\\mathbb{E}[x_i x_j] = \\mathbb{E}_S[\\mathbb{E}[x_i x_j|S]] = \\mathbb{E}_S[0]=0$，因为对于任何给定的 $S$， $x_i$ 或 $x_j$ 中至少有一个为零，或者它们是独立的。\n因此，$\\text{Cov}(x) = \\frac{k\\tau^2}{n} I_n$。\n将此代入 $\\text{Cov}(y|A,E)$ 的表达式中：\n$$\n\\text{Cov}(y|A,E) = \\frac{k\\tau^2}{n} \\Phi \\Phi^T + \\sigma^2 I_m\n$$\n利用这个，我们可以为固定的 $A,E$ 找到互信息的上界：\n$$\nI(x;y|A,E) \\le \\frac{1}{2}\\ln\\det\\left(2\\pi e \\left(\\frac{k\\tau^2}{n} \\Phi\\Phi^T + \\sigma^2 I_m\\right)\\right) - \\frac{m}{2}\\ln(2\\pi e \\sigma^2)\n$$\n$$\nI(x;y|A,E) \\le \\frac{1}{2}\\ln\\det\\left(\\sigma^2 I_m \\left(\\frac{k\\tau^2}{n\\sigma^2} \\Phi\\Phi^T + I_m\\right)\\right) - \\frac{m}{2}\\ln(\\sigma^2) - \\frac{m}{2}\\ln(2\\pi e) + \\frac{m}{2}\\ln(2\\pi e)\n$$\n$$\nI(x;y|A,E) \\le \\frac{1}{2}\\left[\\ln(\\det(\\sigma^2 I_m)) + \\ln\\det\\left(I_m + \\frac{k\\tau^2}{n\\sigma^2} \\Phi\\Phi^T\\right) \\right] - \\frac{m}{2}\\ln(\\sigma^2)\n$$\n$$\nI(x;y|A,E) \\le \\frac{1}{2}\\left[m\\ln(\\sigma^2) + \\ln\\det\\left(I_m + \\frac{k\\tau^2}{n\\sigma^2} \\Phi\\Phi^T\\right) \\right] - \\frac{m}{2}\\ln(\\sigma^2)\n$$\n$$\nI(x;y|A,E) \\le \\frac{1}{2}\\ln\\det\\left(I_m + \\frac{k\\tau^2}{n\\sigma^2} \\Phi\\Phi^T\\right)\n$$\n这个上界等价于输入为高斯分布 $x \\sim \\mathcal{N}(0, \\text{Cov}(x))$ 的 AWGN 信道的容量。现在我们对 $A$ 和 $E$ 取期望：\n$$\nI(x;y,A,E) = \\mathbb{E}_{A,E}[I(x;y|A,E)] \\le \\mathbb{E}_{A,E}\\left[\\frac{1}{2}\\ln\\det\\left(I_m + \\frac{k\\tau^2}{n\\sigma^2} (A \\circ E)(A \\circ E)^T\\right)\\right]\n$$\n函数 $f(X) = \\ln\\det(X)$ 在半正定矩阵空间上是凹函数。根据詹森不等式：\n$$\nI(x;y,A,E) \\le \\frac{1}{2}\\ln\\det\\left(I_m + \\frac{k\\tau^2}{n\\sigma^2} \\mathbb{E}_{A,E}\\left[(A \\circ E)(A \\circ E)^T\\right]\\right)\n$$\n让我们计算期望项 $\\mathbb{E}[(A \\circ E)(A \\circ E)^T]$。这是一个 $m \\times m$ 矩阵。对于该矩阵的任意条目 $(i,j)$：\n$$\n\\left(\\mathbb{E}[(A \\circ E)(A \\circ E)^T]\\right)_{ij} = \\mathbb{E}\\left[\\sum_{l=1}^n (a_{il}e_{il})(a_{jl}e_{jl})\\right] = \\sum_{l=1}^n \\mathbb{E}[a_{il}a_{jl}]\\mathbb{E}[e_{il}e_{jl}]\n$$\n这里我们使用了 $A$ 和 $E$ 的独立性以及期望的线性性质。\n条目 $a_{ij}$ 是独立同分布的 $\\mathcal{N}(0, 1/n)$。因此，$\\mathbb{E}[a_{il}a_{jl}] = \\delta_{ij}\\cdot\\frac{1}{n}$。\n条目 $e_{ij}$ 是独立同分布的 $\\mathrm{Bernoulli}(1-q)$。由于 $e_{ij} \\in \\{0,1\\}$，所以 $e_{ij}^2=e_{ij}$。因此，当 $i \\neq j$ 或 $l$ 不同时，有 $\\mathbb{E}[e_{il}e_{jl}] = \\mathbb{E}[e_{il}]\\mathbb{E}[e_{jl}] = (1-q)^2$；但在我们当前的计算中，下标 $l$ 是相同的。所涉及的变量是 $e_{il}$ 和 $e_{jl}$。如果 $i\\neq j$，它们是独立的，所以 $\\mathbb{E}[e_{il}e_{jl}]=(1-q)^2$。如果 $i=j$，它们是同一个变量，所以 $\\mathbb{E}[e_{il}^2]=\\mathbb{E}[e_{il}]=1-q$。\n情况1：$i \\neq j$。则 $\\mathbb{E}[a_{il}a_{jl}]=0$，所以矩阵元素为 $0$。\n情况2：$i = j$。则 $\\mathbb{E}[a_{il}^2]=1/n$ 且 $\\mathbb{E}[e_{il}^2]=1-q$。\n$$\n\\left(\\mathbb{E}[(A \\circ E)(A \\circ E)^T]\\right)_{ii} = \\sum_{l=1}^n \\mathbb{E}[a_{il}^2]\\mathbb{E}[e_{il}^2] = \\sum_{l=1}^n \\frac{1}{n}(1-q) = n \\cdot \\frac{1-q}{n} = 1-q\n$$\n所以，$\\mathbb{E}[(A \\circ E)(A \\circ E)^T] = (1-q)I_m$。\n将此代入互信息的不等式中：\n$$\nI(x;y,A,E) \\le \\frac{1}{2}\\ln\\det\\left(I_m + \\frac{k\\tau^2(1-q)}{n\\sigma^2} I_m\\right)\n$$\n$$\nI(x;y,A,E) \\le \\frac{1}{2}\\ln\\det\\left(\\left(1 + \\frac{k\\tau^2(1-q)}{n\\sigma^2}\\right)I_m\\right)\n$$\n$$\nI(x;y,A,E) \\le \\frac{1}{2}\\ln\\left(\\left(1 + \\frac{k\\tau^2(1-q)}{n\\sigma^2}\\right)^m\\right) = \\frac{m}{2}\\ln\\left(1 + \\frac{k\\tau^2(1-q)}{n\\sigma^2}\\right)\n$$\n结合 $\\ln\\binom{n}{k} \\lesssim I(x;y,A,E)$，我们得到必要条件：\n$$\n\\ln\\binom{n}{k} \\le \\frac{m}{2}\\ln\\left(1 + \\frac{k\\tau^2(1-q)}{n\\sigma^2}\\right)\n$$\n对 $m$ 求解，得到可靠恢复所需的测量数量下界：\n$$\nm \\ge \\frac{2\\ln\\binom{n}{k}}{\\ln\\left(1 + \\frac{k\\tau^2(1-q)}{n\\sigma^2}\\right)}\n$$\n该表达式的分子捕捉了支撑集不确定性的熵复杂度，分母则捕捉了测量信道的信息速率。擦除概率 $q$ 直接将有效信噪比降低了 $(1-q)$ 倍。", "answer": "$$\\boxed{\\frac{2 \\ln\\binom{n}{k}}{\\ln\\left(1 + \\frac{k\\tau^{2}(1-q)}{n\\sigma^{2}}\\right)}}$$", "id": "3474981"}, {"introduction": "我们的最后一个实践将从简单的向量稀疏性转向结构化信号领域，特别是具有稀疏因子的低秩张量。通过运用组合计数论证，您将比较非结构化的向量化方法与利用张量克罗内克积形式的结构化方法的样本复杂度[@problem_id:3474951]。这项练习有力地阐释了现代数据科学的一个核心主题：利用信号结构可以极大地减少成功恢复所需的数据量。", "problem": "考虑一个$D$阶张量 $\\mathcal{X} \\in \\mathbb{R}^{n \\times n \\times \\cdots \\times n}$，其具有典范多项(CP)分解，即 $\\mathcal{X} = \\sum_{i=1}^{r} \\mathbf{a}^{(1)}_{i} \\otimes \\mathbf{a}^{(2)}_{i} \\otimes \\cdots \\otimes \\mathbf{a}^{(D)}_{i}$，其中对于所有模式 $d \\in \\{1,2,\\ldots,D\\}$ 和分量 $i \\in \\{1,2,\\ldots,r\\}$，每个因子向量 $\\mathbf{a}^{(d)}_{i} \\in \\mathbb{R}^{n}$ 都是$k$-稀疏的，且满足 $1 \\ll k \\ll n$ 和 $1 \\leq r \\ll (n/k)^{D}$。假设所有支撑集和幅度都处于一般位置。假设我们希望恢复所有因子向量的精确支撑集。\n\n我们比较压缩感知中的两种测量范式：\n\n1. 非结构化向量化传感：我们使用一个通用测量矩阵获取向量化张量 $\\mathrm{vec}(\\mathcal{X}) \\in \\mathbb{R}^{n^{D}}$ 的$m$个线性测量，并将 $\\mathrm{vec}(\\mathcal{X})$ 视为一个稀疏向量。在最坏情况下（为了计数），每个秩1项最多贡献$k^{D}$个非零项，总稀疏度为 $s = r k^{D}$。\n\n2. 结构化Kronecker传感：我们通过一种逐模Kronecker设计来获取测量，该设计分别压缩每个模式，并且足够通用，以能够独立辨识每个模式中每个因子向量的支撑集。该设计利用了CP结构，使得跨模式和分量的离散支撑集选择可以分解为关于模式和秩分量的乘积。\n\n为了形式化一个信息论计数下界，假设每次测量最多提供$\\beta$纳特的可靠信息，其中$\\beta > 0$是一个由量化和噪声约束决定的固定值。使用在 $1 \\ll k \\ll n$ 和 $1 \\ll s \\ll n^{D}$ 条件下经过充分检验的渐近计数近似：\n- 在$\\mathbb{R}^{n^{D}}$中一个$s$-稀疏向量的不同支撑集数量为$\\binom{n^{D}}{s}$，其对数的尺度为 $\\ln\\!\\binom{n^{D}}{s} \\approx s \\ln\\!\\left(\\frac{n^{D}}{s}\\right)$。\n- 在$\\mathbb{R}^{n}$中一个$k$-稀疏向量的不同支撑集数量为$\\binom{n}{k}$，其对数的尺度为 $\\ln\\!\\binom{n}{k} \\approx k \\ln\\!\\left(\\frac{n}{k}\\right)$。\n\n使用这些计数论证来写出每种范式所需测量数量的主阶下界，分别表示为 $m_{\\mathrm{vec}}$ 和 $m_{\\mathrm{kron}}$，然后计算主阶比率\n$$\\rho(n,D,r,k) \\equiv \\frac{m_{\\mathrm{vec}}}{m_{\\mathrm{kron}}}.$$\n将最终结果表示为关于 $n$、$D$、$r$ 和 $k$ 的单个闭式解析表达式。无需进行数值计算。使用自然对数。最终答案必须是单个解析表达式，而不能是不等式或方程。", "solution": "这个问题的核心在于一个信息论的计数论证。为了唯一地恢复稀疏因子向量的支撑集，测量提供的总信息量必须至少等于从所有可能性中指定特定支撑集配置所需的信息量。问题陈述指出，每次测量最多提供$\\beta$纳特的信息。因此，收集到的总信息量为$m\\beta$。从$\\mathcal{N}$个可能的模型中指定一个模型所需的信息量是$\\ln(\\mathcal{N})$纳特。这导出了测量数量的基本下界：\n$$m\\beta \\ge \\ln(\\mathcal{N}) \\implies m \\ge \\frac{\\ln(\\mathcal{N})}{\\beta}$$\n我们将使用该下界的主阶近似 $m = \\frac{\\ln(\\mathcal{N})}{\\beta}$ 来确定每种范式所需的测量数量。\n\n首先，我们分析**非结构化向量化传感**范式。在这种情况下，张量 $\\mathcal{X}$ 被视为一个长向量 $\\mathrm{vec}(\\mathcal{X}) \\in \\mathbb{R}^{n^D}$。问题指定该向量的总稀疏度取其最坏情况下的值 $s = rk^D$。任务是在$N = n^D$的环境维度内辨识这个$s$-稀疏向量的支撑集。可能的支撑集数量 $\\mathcal{N}_{\\mathrm{vec}}$ 是从总共 $n^D$ 个条目中选择 $s$ 个非零条目的方式数，由二项式系数 $\\binom{n^D}{s}$ 给出。\n\n辨识支撑集所需的信息量为 $I_{\\mathrm{vec}} = \\ln(\\mathcal{N}_{\\mathrm{vec}}) = \\ln\\binom{n^D}{s}$。使用提供的二项式系数对数的近似，我们有：\n$$I_{\\mathrm{vec}} \\approx s \\ln\\left(\\frac{n^D}{s}\\right)$$\n代入 $s = rk^D$，我们得到：\n$$I_{\\mathrm{vec}} \\approx rk^D \\ln\\left(\\frac{n^D}{rk^D}\\right)$$\n那么，所需的测量数量 $m_{\\mathrm{vec}}$ 为：\n$$m_{\\mathrm{vec}} = \\frac{I_{\\mathrm{vec}}}{\\beta} = \\frac{rk^D}{\\beta} \\ln\\left(\\frac{n^D}{rk^D}\\right)$$\n\n接下来，我们分析**结构化Kronecker传感**范式。这种方法利用了张量 $\\mathcal{X}$ 的CP结构。该张量由$r$个秩1分量定义，每个分量是$D$个因子向量的外积。每个因子向量 $\\mathbf{a}^{(d)}_i \\in \\mathbb{R}^n$ 是$k$-稀疏的。未知支撑集的总集合由$\\mathbb{R}^n$中$r \\times D$个独立的$k$-稀疏向量的支撑集组成。\n\n在$\\mathbb{R}^n$中为单个$k$-稀疏向量选择支撑集的方式数为$\\binom{n}{k}$。由于问题陈述支撑集选择在模式和分量之间是独立的，整个张量的可能支撑集配置总数$\\mathcal{N}_{\\mathrm{kron}}$是每个$rD$因子向量可能性的乘积：\n$$\\mathcal{N}_{\\mathrm{kron}} = \\left(\\binom{n}{k}\\right)^{rD}$$\n辨识此结构化模型所需的信息量为：\n$$I_{\\mathrm{kron}} = \\ln(\\mathcal{N}_{\\mathrm{kron}}) = \\ln\\left( \\left(\\binom{n}{k}\\right)^{rD} \\right) = rD \\ln\\binom{n}{k}$$\n使用提供的近似 $\\ln\\binom{n}{k} \\approx k \\ln\\left(\\frac{n}{k}\\right)$，我们得到：\n$$I_{\\mathrm{kron}} \\approx rD k \\ln\\left(\\frac{n}{k}\\right)$$\n那么，所需的测量数量 $m_{\\mathrm{kron}}$ 为：\n$$m_{\\mathrm{kron}} = \\frac{I_{\\mathrm{kron}}}{\\beta} = \\frac{rDk}{\\beta} \\ln\\left(\\frac{n}{k}\\right)$$\n\n最后，我们计算比率 $\\rho(n, D, r, k) = \\frac{m_{\\mathrm{vec}}}{m_{\\mathrm{kron}}}$。\n$$\\rho = \\frac{m_{\\mathrm{vec}}}{m_{\\mathrm{kron}}} = \\frac{\\frac{rk^D}{\\beta} \\ln\\left(\\frac{n^D}{rk^D}\\right)}{\\frac{rDk}{\\beta} \\ln\\left(\\frac{n}{k}\\right)}$$\n项$r$和$\\beta$相互抵消：\n$$\\rho = \\frac{k^D \\ln\\left(\\frac{n^D}{rk^D}\\right)}{Dk \\ln\\left(\\frac{n}{k}\\right)} = \\frac{k^{D-1}}{D} \\frac{\\ln\\left(\\frac{n^D}{rk^D}\\right)}{\\ln\\left(\\frac{n}{k}\\right)}$$\n我们可以使用对数的性质来简化分子中的对数：\n$$\\ln\\left(\\frac{n^D}{rk^D}\\right) = \\ln\\left(\\frac{1}{r} \\cdot \\frac{n^D}{k^D}\\right) = \\ln\\left(\\frac{1}{r}\\right) + \\ln\\left(\\left(\\frac{n}{k}\\right)^D\\right) = -\\ln(r) + D\\ln\\left(\\frac{n}{k}\\right)$$\n将此代回$\\rho$的表达式中：\n$$\\rho = \\frac{k^{D-1}}{D} \\frac{D\\ln\\left(\\frac{n}{k}\\right) - \\ln(r)}{\\ln\\left(\\frac{n}{k}\\right)}$$\n我们可以将其分为两项：\n$$\\rho = \\frac{k^{D-1}}{D} \\left(\\frac{D\\ln\\left(\\frac{n}{k}\\right)}{\\ln\\left(\\frac{n}{k}\\right)} - \\frac{\\ln(r)}{\\ln\\left(\\frac{n}{k}\\right)}\\right) = \\frac{k^{D-1}}{D} \\left(D - \\frac{\\ln(r)}{\\ln\\left(\\frac{n}{k}\\right)}\\right)$$\n将因子$\\frac{k^{D-1}}{D}$分配进去，得到最终表达式：\n$$\\rho = k^{D-1} - \\frac{k^{D-1}\\ln(r)}{D\\ln\\left(\\frac{n}{k}\\right)}$$\n这可以更紧凑地写成，通过提出$k^{D-1}$项。\n$$\\rho = k^{D-1} \\left(1 - \\frac{\\ln(r)}{D\\ln\\left(\\frac{n}{k}\\right)}\\right)$$\n这就是所求的主阶比率，表示为关于 $n$、$D$、$r$ 和 $k$ 的单个闭式解析表达式。", "answer": "$$\\boxed{k^{D-1} \\left(1 - \\frac{\\ln(r)}{D \\ln\\left(\\frac{n}{k}\\right)}\\right)}$$", "id": "3474951"}]}