{"hands_on_practices": [{"introduction": "在稀疏信号恢复中，首要挑战是在噪声存在的情况下准确识别信号的非零元素所在的位置（即支撑集）。本练习将引导你通过推导，揭示为了使真实信号分量从相干矩阵的干扰和噪声中脱颖而出所需的最小信号强度。通过这个计算 [@problem_id:3462322]，你将掌握基于相干性的恢复理论的基石，并理解信号幅值、噪声水平和测量矩阵属性之间的根本制衡关系。", "problem": "考虑一个压缩感知中的线性感知模型，其测量矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，其列 $\\{a_i\\}_{i=1}^{n}$ 为单位范数列，且具有互相干性 $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$。一个 $k$-稀疏信号 $x \\in \\mathbb{R}^{n}$ 被测量为 $y = A x + w$，其中噪声 $w \\in \\mathbb{R}^{m}$ 满足能量界 $\\|w\\|_{2} \\leq \\Delta$。假设 $x$ 的非零项具有相等的幅值 $a  0$，并定义支撑集 $S \\subset \\{1, \\dots, n\\}$ 且 $|S| = k$。相关阈值法将一个索引 $i$ 判定为活动索引，如果其绝对相关性 $|a_i^{\\top} y|$ 超过某个阈值，该阈值是仅使用 $\\mu(A)$、$\\|x\\|_{1}$ 和 $\\Delta$ 来选择的，目的是将真实支撑集与其补集分离开来。\n\n从互相干性、单位范数列相关的定义，以及三角不等式和柯西-施瓦茨不等式出发，推导出一个关于最小非零幅值 $a_{\\min}$ 的充分下界，该下界能确保在有噪声的情况下通过相关阈值法实现精确的支撑集恢复。然后，对于数值实例 $\\Delta = 10^{-3}$、$m = 500$、$n = 5000$、$\\mu(A) = 0.08$ 和 $k = 6$，计算你的界所蕴含的 $a_{\\min}$ 的精确值。将最终答案表示为一个无单位的最简分数。同时，在你的推导过程中，验证为使该界有意义所必需的基于相干性的稀疏度要求是否被给定参数所满足。\n\n你的最终答案必须是一个单一的计算结果（一个数字或一个闭式表达式）。不需要四舍五入。", "solution": "该问题要求我们求解稀疏信号非零项幅值的一个充分下界，以保证在带噪压缩感知场景下能够精确恢复支撑集。我们首先将成功恢复的条件形式化，然后利用所提供的工具推导出这个界。\n\n线性测量模型由 $y = A x + w$ 给出，其中 $y \\in \\mathbb{R}^{m}$ 是测量向量，$A \\in \\mathbb{R}^{m \\times n}$ 是测量矩阵，$x \\in \\mathbb{R}^{n}$ 是 $k$-稀疏信号，$w \\in \\mathbb{R}^{m}$ 是能量有界（$\\|w\\|_{2} \\leq \\Delta$）的噪声向量。$A$ 的列向量，记为 $\\{a_i\\}_{i=1}^{n}$，是单位范数的，即对所有 $i \\in \\{1, \\dots, n\\}$ 都有 $\\|a_i\\|_{2} = 1$。$A$ 的互相干性为 $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$。信号 $x$ 是 $k$-稀疏的，意味着它最多有 $k$ 个非零项。设 $S = \\text{supp}(x)$ 为 $x$ 的支撑集，且 $|S|=k$。给定条件是，对于任何 $j \\in S$，项的幅值为 $|x_j| = a  0$。\n\n恢复过程使用相关阈值法：如果索引 $i$ 与测量向量的相关性幅值 $|a_i^{\\top} y|$ 超过某个阈值 $\\tau$，则该索引被识别为支撑集的一部分。为了实现精确的支撑集恢复，必须存在一个阈值 $\\tau$ 能够完美地将对应于真实支撑集 $S$ 的相关性与对应于其补集 $S^c$ 的相关性分离开。这要求以下条件成立：\n$$ \\min_{i \\in S} |a_i^{\\top} y|  \\max_{j \\notin S} |a_j^{\\top} y| $$\n我们将为左边的项推导一个下界，为右边的项推导一个上界。\n\n首先，考虑一个索引 $i \\in S$。其相关性为：\n$$ a_i^{\\top} y = a_i^{\\top} (A x + w) = a_i^{\\top} \\left( \\sum_{l=1}^{n} x_l a_l \\right) + a_i^{\\top} w $$\n由于当 $l \\notin S$ 时 $x_l = 0$，我们可以将求和写在支撑集 $S$ 上：\n$$ a_i^{\\top} y = \\sum_{l \\in S} x_l (a_i^{\\top} a_l) + a_i^{\\top} w $$\n我们可以分离出 $l=i$ 的项：\n$$ a_i^{\\top} y = x_i (a_i^{\\top} a_i) + \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) + a_i^{\\top} w $$\n由于列向量 $a_i$ 是单位范数的，所以 $a_i^{\\top} a_i = \\|a_i\\|_{2}^2 = 1$。这可将表达式简化为：\n$$ a_i^{\\top} y = x_i + \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) + a_i^{\\top} w $$\n为了找到 $|a_i^{\\top} y|$ 的下界，我们使用反三角不等式 $|u+v| \\ge |u| - |v|$：\n$$ |a_i^{\\top} y| \\ge |x_i| - \\left| \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) + a_i^{\\top} w \\right| $$\n对被减去的项应用三角不等式：\n$$ |a_i^{\\top} y| \\ge |x_i| - \\left( \\left| \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) \\right| + |a_i^{\\top} w| \\right) $$\n我们对每一项进行界定。给定 $|x_i|=a$。求和项使用三角不等式和互相干性的定义来界定：\n$$ \\left| \\sum_{l \\in S, l \\neq i} x_l (a_i^{\\top} a_l) \\right| \\le \\sum_{l \\in S, l \\neq i} |x_l| |a_i^{\\top} a_l| \\le \\sum_{l \\in S, l \\neq i} a \\cdot \\mu(A) = (k-1) a \\mu(A) $$\n噪声项使用柯西-施瓦茨不等式来界定：\n$$ |a_i^{\\top} w| \\le \\|a_i\\|_{2} \\|w\\|_{2} \\le 1 \\cdot \\Delta = \\Delta $$\n代入这些界，我们得到支撑集上相关性的一个下界：\n$$ \\min_{i \\in S} |a_i^{\\top} y| \\ge a - ((k-1) a \\mu(A) + \\Delta) = a(1 - (k-1)\\mu(A)) - \\Delta $$\n接下来，考虑一个索引 $j \\notin S$。其相关性为：\n$$ a_j^{\\top} y = \\sum_{l \\in S} x_l (a_j^{\\top} a_l) + a_j^{\\top} w $$\n为了找到 $|a_j^{\\top} y|$ 的一个上界，我们使用三角不等式：\n$$ |a_j^{\\top} y| \\le \\left| \\sum_{l \\in S} x_l (a_j^{\\top} a_l) \\right| + |a_j^{\\top} w| $$\n我们界定求和项。由于 $j \\notin S$ 且 $l \\in S$，我们有 $j \\neq l$，所以 $|a_j^{\\top} a_l| \\le \\mu(A)$。\n$$ \\left| \\sum_{l \\in S} x_l (a_j^{\\top} a_l) \\right| \\le \\sum_{l \\in S} |x_l| |a_j^{\\top} a_l| \\le \\sum_{l \\in S} a \\cdot \\mu(A) = k a \\mu(A) $$\n噪声项的界定如前：$|a_j^{\\top} w| \\le \\Delta$。\n代入这些界，我们得到非支撑集上相关性的一个上界：\n$$ \\max_{j \\notin S} |a_j^{\\top} y| \\le k a \\mu(A) + \\Delta $$\n精确支撑集恢复的一个充分条件是，支撑集上最小相关性的下界大于非支撑集上最大相关性的上界：\n$$ a(1 - (k-1)\\mu(A)) - \\Delta  k a \\mu(A) + \\Delta $$\n我们对 $a$ 解这个不等式：\n$$ a - a(k-1)\\mu(A) - k a \\mu(A)  2\\Delta $$\n$$ a(1 - (k-1)\\mu(A) - k\\mu(A))  2\\Delta $$\n$$ a(1 - k\\mu(A) + \\mu(A) - k\\mu(A))  2\\Delta $$\n$$ a(1 - (2k-1)\\mu(A))  2\\Delta $$\n为了使这个不等式能为 $a$ 产生一个有意义的正下界，$a$ 的系数必须是正数。这给出了众所周知的基于相干性的稀疏度要求：\n$$ 1 - (2k-1)\\mu(A)  0 \\implies (2k-1)\\mu(A)  1 $$\n如果这个条件成立，我们可以通过除法找到 $a$ 的下界：\n$$ a  \\frac{2\\Delta}{1 - (2k-1)\\mu(A)} $$\n因此，所需的最小幅值 $a_{\\min}$ 为：\n$$ a_{\\min} = \\frac{2\\Delta}{1 - (2k-1)\\mu(A)} $$\n现在我们使用给定的数值：$\\Delta = 10^{-3}$、$k = 6$ 和 $\\mu(A) = 0.08$。其他参数 $m=500$ 和 $n=5000$ 与压缩感知的设定一致，但不进入这个特定的界计算中。\n\n首先，我们验证基于相干性的稀疏度要求：\n$$ (2k-1)\\mu(A) = (2 \\cdot 6 - 1) \\cdot 0.08 = (11) \\cdot 0.08 = 0.88 $$\n由于 $0.88  1$，该要求得到满足，我们推导出的 $a_{\\min}$ 的界是有效且有意义的。\n\n现在，我们计算 $a_{\\min}$ 的值：\n$$ a_{\\min} = \\frac{2 \\cdot 10^{-3}}{1 - 0.88} = \\frac{2 \\cdot 10^{-3}}{0.12} $$\n为了将其表示为一个最简分数：\n$$ a_{\\min} = \\frac{2 \\times \\frac{1}{1000}}{\\frac{12}{100}} = \\frac{\\frac{2}{1000}}{\\frac{12}{100}} = \\frac{2}{1000} \\cdot \\frac{100}{12} = \\frac{200}{12000} = \\frac{2}{120} = \\frac{1}{60} $$\n因此，在这些条件下保证支撑集恢复所需的最小非零幅值为 $\\frac{1}{60}$。", "answer": "$$\\boxed{\\frac{1}{60}}$$", "id": "3462322"}, {"introduction": "成功识别支撑集只是第一步，我们同样关心恢复出信号幅值的精确度。本练习 [@problem_id:3462319] 深入探讨了这一问题，假设支撑集已知，要求你量化最终估计值的误差。你将运用格申圆盘定理（Gershgorin circle theorem）这一强大的谱分析工具，为留数误差建立一个上界，从而亲身体会测量矩阵的相干性如何直接影响最终解的稳定性和可靠性。", "problem": "考虑一个线性传感模型，其测量值由 $y = A x + w$ 给出，其中 $A \\in \\mathbb{R}^{m \\times n}$ 的列是单位范数列向量，$x \\in \\mathbb{R}^{n}$ 是一个 $k$-稀疏向量，其支撑集为一个未知的索引集 $S$ 且 $|S| = k$，$w \\in \\mathbb{R}^{m}$ 是一个满足 $\\|w\\|_{2} \\leq \\varepsilon$ 的加性噪声向量。$A$ 的互相关性定义为 $\\mu(A) = \\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$，其中 $a_{i}$ 表示 $A$ 的第 $i$ 列。假设一个基于相关性的恢复过程（例如，阈值法或 Orthogonal Matching Pursuit (OMP)）正确地识别了支撑集 $S$，然后通过在 $A_{S}$ 上进行最小二乘去偏步骤来估计振幅。\n\n从互相关性的定义和 Gershgorin 圆盘定理出发，推导一个形式为\n$$\n\\|x^{\\star} - x\\|_{2} \\leq c(\\mu(A), k)\\,\\varepsilon,\n$$\n的振幅估计误差的确定性上界，其中 $x^{\\star}$ 是在真实支撑集 $S$ 上的最小二乘估计，$c(\\mu(A), k)$ 是一个由格拉姆矩阵 $A_{S}^{\\top} A_{S}$ 的谱界限得出的、用 $\\mu(A)$ 和 $k$ 明确表示的常数。然后，对于初始相关性 $\\mu(A) = 0.2$ 和稀疏度 $k = 4$，计算 $c(\\mu(A), k)$。接下来，假设传感过程中的一个扰动使互相关性增加了 $0.05$，即 $\\mu(A)$ 变为 $0.25$。在受扰动的相关性下重新计算 $c(\\mu(A), k)$，并确定界限中的乘性退化因子，该因子定义为扰动后的常数与原始常数之比。\n\n将乘性退化因子报告为单个封闭形式的表达式。不要对答案进行四舍五入。", "solution": "设 $S$ 为 $k$-稀疏信号 $x$ 的支撑集，并设 $x_S \\in \\mathbb{R}^k$ 为 $x$ 的非零项组成的向量。类似地，设 $A_S \\in \\mathbb{R}^{m \\times k}$ 为由 $S$ 索引的列组成的 $A$ 的子矩阵。限制在支撑集上的测量模型为\n$$\ny = A_S x_S + w\n$$\n$x_S$ 的最小二乘估计 $x^{\\star}_S$ 通过求解正规方程得到，其结果为：\n$$\nx^{\\star}_S = (A_S^{\\top} A_S)^{-1} A_S^{\\top} y\n$$\n为了找到估计误差，我们代入 $y$ 的表达式：\n$$\nx^{\\star}_S = (A_S^{\\top} A_S)^{-1} A_S^{\\top} (A_S x_S + w) = (A_S^{\\top} A_S)^{-1} (A_S^{\\top} A_S) x_S + (A_S^{\\top} A_S)^{-1} A_S^{\\top} w\n$$\n这可以简化为：\n$$\nx^{\\star}_S = x_S + (A_S^{\\top} A_S)^{-1} A_S^{\\top} w\n$$\n估计误差是差值 $x^{\\star}_S - x_S$。其欧几里得范数的界如下：\n$$\n\\|x^{\\star} - x\\|_{2} = \\|x^{\\star}_S - x_S\\|_{2} = \\|(A_S^{\\top} A_S)^{-1} A_S^{\\top} w\\|_{2} \\leq \\|(A_S^{\\top} A_S)^{-1}\\|_{2} \\|A_S^{\\top}\\|_{2} \\|w\\|_{2}\n$$\n令 $G = A_S^{\\top} A_S$ 为大小为 $k \\times k$ 的格拉姆矩阵。其逆的谱范数为 $\\|G^{-1}\\|_{2} = 1/\\sigma_{\\min}(G)$，其中 $\\sigma_{\\min}(G)$ 是 $G$ 的最小奇异值。由于 $G$ 是一个对称半正定矩阵，其奇异值就是其特征值，所以 $\\sigma_{\\min}(G) = \\lambda_{\\min}(G)$。类似地，$A_S^{\\top}$ 的谱范数是 $\\|A_S^{\\top}\\|_{2} = \\|A_S\\|_{2} = \\sqrt{\\lambda_{\\max}(A_S^{\\top}A_S)} = \\sqrt{\\lambda_{\\max}(G)}$。误差界变为：\n$$\n\\|x^{\\star} - x\\|_{2} \\leq \\frac{\\sqrt{\\lambda_{\\max}(G)}}{\\lambda_{\\min}(G)} \\|w\\|_{2}\n$$\n我们使用 Gershgorin 圆盘定理来界定 $G$ 的特征值。$G$ 的元素是 $G_{ij} = \\langle a_i, a_j \\rangle$ 对于 $i,j \\in S$。对角线元素是 $G_{ii} = \\|a_i\\|_2^2 = 1$，因为 $A$ 的列是单位范数的。非对角线元素的幅度由互相关性界定：$|G_{ij}| \\leq \\mu(A)$ 对于 $i \\neq j$。\n\nGershgorin 圆盘定理指出，对于 $G$ 的任意特征值 $\\lambda$，至少存在一个对角线元素 $G_{ii}$ 使得 $|\\lambda - G_{ii}| \\leq \\sum_{j \\neq i, j\\in S} |G_{ij}|$。由于对于所有 $i$，$G_{ii} = 1$，且求和中有 $k-1$ 个项，我们有：\n$$\n|\\lambda - 1| \\leq \\sum_{j \\neq i, j\\in S} \\mu(A) = (k-1)\\mu(A)\n$$\n这个不等式意味着 $G$ 的所有特征值都必须位于区间 $[1 - (k-1)\\mu(A), 1 + (k-1)\\mu(A)]$ 内。因此，我们得到了最小和最大特征值的界限：\n$$\n\\lambda_{\\min}(G) \\geq 1 - (k-1)\\mu(A)\n$$\n$$\n\\lambda_{\\max}(G) \\leq 1 + (k-1)\\mu(A)\n$$\n将这些界限代入误差不等式，并使用给定条件 $\\|w\\|_{2} \\leq \\varepsilon$：\n$$\n\\|x^{\\star} - x\\|_{2} \\leq \\frac{\\sqrt{1 + (k-1)\\mu(A)}}{1 - (k-1)\\mu(A)} \\varepsilon\n$$\n这就是所求的界，其中常数 $c(\\mu(A), k)$ 被确定为：\n$$\nc(\\mu(A), k) = \\frac{\\sqrt{1 + (k-1)\\mu(A)}}{1 - (k-1)\\mu(A)}\n$$\n现在，我们进行所需的计算。\n\n首先，对于初始情况，我们有 $\\mu(A) = \\mu_1 = 0.2$ 和 $k=4$。\n$$\nc_1 = c(0.2, 4) = \\frac{\\sqrt{1 + (4-1)(0.2)}}{1 - (4-1)(0.2)} = \\frac{\\sqrt{1 + 3(0.2)}}{1 - 3(0.2)} = \\frac{\\sqrt{1+0.6}}{1-0.6} = \\frac{\\sqrt{1.6}}{0.4}\n$$\n为简化计算，我们将数字写成分数形式：$\\sqrt{1.6} = \\sqrt{16/10} = 4/\\sqrt{10}$ 和 $0.4 = 4/10$。\n$$\nc_1 = \\frac{4/\\sqrt{10}}{4/10} = \\frac{4}{\\sqrt{10}} \\cdot \\frac{10}{4} = \\frac{10}{\\sqrt{10}} = \\sqrt{10}\n$$\n接下来，对于扰动情况，相关性增加了 $0.05$，因此 $\\mu(A) = \\mu_2 = 0.2 + 0.05 = 0.25$。稀疏度保持 $k=4$。\n$$\nc_2 = c(0.25, 4) = \\frac{\\sqrt{1 + (4-1)(0.25)}}{1 - (4-1)(0.25)} = \\frac{\\sqrt{1 + 3(0.25)}}{1 - 3(0.25)} = \\frac{\\sqrt{1+0.75}}{1-0.75} = \\frac{\\sqrt{1.75}}{0.25}\n$$\n同样，我们使用分数：$\\sqrt{1.75} = \\sqrt{7/4} = \\sqrt{7}/2$ 和 $0.25 = 1/4$。\n$$\nc_2 = \\frac{\\sqrt{7}/2}{1/4} = \\frac{\\sqrt{7}}{2} \\cdot 4 = 2\\sqrt{7}\n$$\n最后，我们计算乘性退化因子，即扰动后的常数与原始常数之比：\n$$\n\\text{退化因子} = \\frac{c_2}{c_1} = \\frac{2\\sqrt{7}}{\\sqrt{10}}\n$$\n为了将其表示为单个封闭形式的表达式，我们对分母进行有理化：\n$$\n\\frac{c_2}{c_1} = \\frac{2\\sqrt{7}\\sqrt{10}}{\\sqrt{10}\\sqrt{10}} = \\frac{2\\sqrt{70}}{10} = \\frac{\\sqrt{70}}{5}\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{70}}{5}}$$", "id": "3462319"}, {"introduction": "理论的价值最终要在实践中得到检验。这个综合性的计算实验 [@problem_id:3462348] 将前述的理论概念带入一个实际的仿真环境中。你将亲手实现一个经典的稀疏恢复算法，在一个受控的最坏情况场景和一个随机场景下测试其性能，并将观察到的经验恢复边界与理论预测进行比较。这项实践旨在连接抽象的数学保证与可观测的算法行为，让你深刻理解理论模型的预测能力及其局限性。", "problem": "考虑压缩感知中的标准线性测量模型，其中测量向量 $y \\in \\mathbb{R}^m$ 是通过 $y = A x + e$ 从稀疏信号 $x \\in \\mathbb{R}^n$ 中获得的，其中 $A \\in \\mathbb{R}^{m \\times n}$ 为测量矩阵，$e \\in \\mathbb{R}^m$ 为加性噪声。假设 $x$ 是 $k$-稀疏的，即它恰好有 $k$ 个非零项。设 $A$ 的列是单位范数的。将 $A$ 的互相关性定义为 $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$，其中 $a_i$ 表示 $A$ 的第 $i$ 列。支撑集恢复指精确识别出 $x$ 的非零元素索引集。\n\n本问题分析在固定互相关性 $\\mu(A)$ 下，支撑集恢复在平面 $(k/m, \\mathrm{SNR})$ 上的相变，并验证与使用已知在限制等距性质 (RIP) 下表现良好的矩阵进行的模拟相比，基于相关性的阈值是否能预测恢复边界。\n\n推导和算法设计需基于以下基本定义和事实：\n- 互相关性 $\\mu(A)$ 控制了 $A$ 中不同列之间的最大相关性。\n- 稀疏恢复算法，如正交匹配追踪 (OMP)，根据与残差的相关性进行贪心选择。\n- 在无噪声情况下，经典的基于相关性的保证为精确支撑集恢复提供了充分的稀疏度界限（用 $\\mu(A)$ 表示）。\n- 对于含噪测量，信噪比 (SNR) 定义为 $\\mathrm{SNR} = \\|A x\\|_2^2 / \\|e\\|_2^2$，这是一个无量纲比率。更高的 $\\mathrm{SNR}$ 通常会提高恢复性能。\n- 列归一化的高斯随机矩阵在适当的维度和稀疏度下，以高概率满足限制等距性质，从而在经验上实现鲁棒的恢复。\n\n你的程序必须实现以下内容：\n- 使用正交匹配追踪 (OMP) 进行支撑集恢复，迭代次数固定为真实稀疏度 $k$。在每次迭代中，选择与当前残差具有最大绝对相关性的 $A$ 的列，更新支撑集，并在选定的支撑集上重新计算最小二乘估计。\n- 对于每个具有固定互相关性 $\\mu(A)$ 的测试矩阵，按如下方式构造 $A$：\n  - 选择一个单位向量 $u \\in \\mathbb{R}^m$ 和一个单位向量 $w \\in \\mathbb{R}^m$，且 $\\langle u, w \\rangle = 0$。\n  - 设定两列 $a_1 = u$ 和 $a_2 = \\alpha u + \\sqrt{1 - \\alpha^2}\\, w$，其中 $\\alpha \\in (0,1)$ 为预设目标，使得 $\\langle a_1, a_2 \\rangle = \\alpha$。\n  - 在 $\\mathrm{span}\\{u, w\\}$ 的正交补中生成其余的 $n-2$ 列作为单位向量（以确保与 $a_1$ 和 $a_2$ 的内积为零），并对其进行归一化。\n  - 生成的矩阵具有由对 $(a_1, a_2)$ 主导的固定 $\\mu(A)$，且 $\\mu(A) \\approx \\alpha$。\n- 对于每个测试用例，还需构造一个比较矩阵，其具有独立同分布的高斯元素并已将列归一化（一种典型的对 RIP 友好的模型）。\n- 对于噪声 $e$，使用独立的、方差经选择的高斯项，以使期望能量满足 $\\mathrm{SNR} = \\|A x\\|_2^2 / \\|e\\|_2^2$。所有测试中的 SNR 值都是无量纲比率（非分贝），必须照此处理。\n\n将固定 $\\mathrm{SNR}$ 下的相变边界定义为（在对 $k$ 递增扫描中）精确支撑集恢复的经验成功率至少达到指定阈值的最大 $k$。以比率 $k/m$ 来衡量该边界。\n\n验证标准：\n- 从基于互相关性的经典充分条件（不要在问题陈述中包含或假设任何数值常数；在解答中推导它们）计算无噪声情况下的预测稀疏度极限 $K_{\\mathrm{coh}}$。将其视为边界的预测下界。\n- 对于每个测试用例和最高 $\\mathrm{SNR}$ 水平，比较：\n  1. 对于固定 $\\mu(A)$ 矩阵，在 $\\mathrm{SNR}=20.0$ 时观察到的边界是否至少为预测的 $K_{\\mathrm{coh}}$（下界验证）。\n  2. 对于对 RIP 友好的（高斯）矩阵，观察到的边界是否大于或等于固定 $\\mu(A)$ 矩阵的边界（RIP 优越性）。\n- 此外，验证边界作为 $\\mathrm{SNR}$ 函数的单调性：边界应随 $\\mathrm{SNR}$ 的增加而非递减。\n\n实验方案：\n- 在所有测试中，生成具有恰好 $k$ 个非零项且系数单位幅值、符号随机的稀疏信号 $x$。对于固定 $\\mu(A)$ 的矩阵，当 $k \\geq 2$ 时，在支撑集中包含两个高度相关的列 $a_1$ 和 $a_2$，以探测在给定 $\\mu(A)$ 下的最坏情况行为；对于对 RIP 友好的矩阵，无需强制任何特定列，均匀随机地选择支撑集。\n- 对于每个 $(k, \\mathrm{SNR})$ 对，运行多次独立试验并估计成功率。成功定义为恢复的支撑集与真实支撑集完全相等。\n\n测试套件和参数：\n- 使用以下三个测试用例，每个用例都有一个固定的互相关性目标：\n  - 用例 1：$m = 120$，$n = 180$，目标 $\\alpha = 0.45$，$\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$。\n  - 用例 2：$m = 120$，$n = 180$，目标 $\\alpha = 0.35$，$\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$。\n  - 用例 3：$m = 140$，$n = 160$，目标 $\\alpha = 0.30$，$\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$。\n- 对每个用例，在 $k \\in \\{1, 2, \\dots, \\lfloor 0.12\\, m \\rfloor\\}$ 范围内扫描 $k$。\n- 每个 $(k, \\mathrm{SNR})$ 对使用 $25$ 次独立试验。\n- 使用 $0.8$ 的成功率阈值（以小数表示）来定义每个 $\\mathrm{SNR}$ 下的边界。\n\n要求输出：\n- 对于每个测试用例，按顺序产生三个布尔值：\n  1. 下界验证：对于固定 $\\mu(A)$ 矩阵，在 $\\mathrm{SNR}=20.0$ 时观察到的边界是否至少为预测的 $K_{\\mathrm{coh}}$。\n  2. RIP 优越性：对于对 RIP 友好的矩阵，在 $\\mathrm{SNR}=20.0$ 时观察到的边界是否大于或等于固定 $\\mu(A)$ 矩阵的边界。\n  3. SNR 单调性：对于固定 $\\mu(A)$ 矩阵，其边界是否随着 $\\mathrm{SNR}$ 的增加（从 $5.0$ 到 $10.0$ 到 $20.0$）而非递减。\n- 你的程序应产生单行输出，其中包含所有九个布尔值（每个测试用例三个），形式为用方括号括起来的逗号分隔列表（例如，“[True,False,True,True,True,False,False,True,True]”）。\n\n不涉及物理单位。不出现角度。所有百分比必须以小数或分数形式报告；成功阈值以 $0.8$ 给出。", "solution": "本解答将分三个阶段进行。首先，我们将推导无噪声恢复下的理论相关性稀疏度极限 $K_{\\mathrm{coh}}$。其次，我们将详细说明实验方案，包括正交匹配追踪 (OMP) 算法的实现和指定测量矩阵的构造。最后，我们将概述对仿真结果执行的验证检查。\n\n### 1. 基于互相关性的理论稀疏度极限\n\n本问题要求从测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的互相关性 $\\mu(A)$ 推导出一个预测的稀疏度极限 $K_{\\mathrm{coh}}$。互相关性定义为 $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$，其中 $a_i$ 是 $A$ 的单位范数列。该极限是像 OMP 这样的算法在无噪声情况下（即从测量值 $y = Ax$）完美恢复 $k$-稀疏信号 $x \\in \\mathbb{R}^n$ 的支撑集的充分条件。\n\nOMP 是一种贪心算法。在第一步中，它识别与测量向量 $y$ 最相关的 $A$ 的列。为成功恢复，该列必须属于 $x$ 的真实支撑集，记为 $S_0$。我们来分析这第一步。测量向量为 $y = Ax = \\sum_{j \\in S_0} x_j a_j$。\n\n任意列 $a_i$ 与 $y$ 的相关性为 $\\langle a_i, y \\rangle$。\n若 $i \\notin S_0$（一个不正确的原子），则相关性为：\n$$c_{incorrect} = \\langle a_i, y \\rangle = \\left\\langle a_i, \\sum_{j \\in S_0} x_j a_j \\right\\rangle = \\sum_{j \\in S_0} x_j \\langle a_i, a_j \\rangle$$\n取绝对值并根据 $\\mu(A)$ 的定义进行界定：\n$$|c_{incorrect}| \\le \\sum_{j \\in S_0} |x_j| |\\langle a_i, a_j \\rangle| \\le \\mu(A) \\sum_{j \\in S_0} |x_j| = \\mu(A) \\|x\\|_1$$\n\n若 $i \\in S_0$（一个正确的原子），则相关性为：\n$$c_{correct} = \\langle a_i, y \\rangle = \\left\\langle a_i, x_i a_i + \\sum_{j \\in S_0, j \\neq i} x_j a_j \\right\\rangle = x_i \\|a_i\\|_2^2 + \\sum_{j \\in S_0, j \\neq i} x_j \\langle a_i, a_j \\rangle$$\n由于列是单位范数的，$\\|a_i\\|_2^2=1$。使用三角不等式，我们可以界定其幅值的下界：\n$$|c_{correct}| \\ge |x_i| - \\left| \\sum_{j \\in S_0, j \\neq i} x_j \\langle a_i, a_j \\rangle \\right| \\ge |x_i| - \\mu(A) \\sum_{j \\in S_0, j \\neq i} |x_j|$$\n\nOMP 在第一步选择一个正确原子的充分条件是，至少一个正确原子的相关性严格大于任何不正确原子的相关性。一个更简单、更严格的条件保证*任何*正确的原子都优于*任何*不正确的原子：$\\min_{i \\in S_0} |c_{correct}|  \\max_{l \\notin S_0} |c_{incorrect}|$。这导致：\n$$|x_i| - \\mu(A) \\sum_{j \\in S_0, j \\neq i} |x_j|  \\mu(A) \\sum_{j \\in S_0} |x_j|$$\n这必须对某个 $i \\in S_0$ 成立。让我们考虑问题中的信号模型，其中 $x$ 的非零项具有单位幅值，即对所有 $j \\in S_0$ 都有 $|x_j| = 1$。在这种情况下，$\\|x\\|_1 = k$ 且 $|x_i|=1$。不等式变为：\n$$1 - \\mu(A) (k-1)  \\mu(A) k$$\n$$1  \\mu(A) k + \\mu(A) (k-1)$$\n$$1  \\mu(A) (2k - 1)$$\n$$1/\\mu(A)  2k - 1$$\n$$1/\\mu(A) + 1  2k$$\n$$k  \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right)$$\n虽然 OMP 恢复的完整证明需要对所有 $k$ 步进行归纳论证，但在某些信号条件下，此条件是几种贪心恢复算法的标准充分保证。因此，我们将预测的基于相关性的稀疏度极限 $K_{\\mathrm{coh}}$ 定义为满足此严格不等式的最大整数 $k$。对于具有目标相关性 $\\alpha$ 的固定 $\\mu(A)$ 矩阵，我们使用 $\\mu(A) \\approx \\alpha$ 来计算预测值。\n\n### 2. 实验设计与算法\n\n该仿真旨在寻找支撑集恢复的经验相变边界，并根据理论预测和基准矩阵类型对其进行验证。\n\n#### 正交匹配追踪 (OMP)\nOMP 算法被实现用于从测量值 $y$ 中恢复 $k$-稀疏信号的支撑集。给定固定稀疏度 $k$：\n1.  初始化残差 $r_0 = y$，支撑集 $S = \\emptyset$，以及迭代计数器 $t=1$。\n2.  当 $t \\le k$ 时：\n    a.  **匹配步骤**：找到与当前残差 $r_{t-1}$ 最相关的 $A$ 中列的索引 $i_t$：\n        $$i_t = \\arg\\max_{j \\notin S} |\\langle a_j, r_{t-1} \\rangle|$$\n    b.  **支撑集更新**：扩充支撑集：$S = S \\cup \\{i_t\\}$。\n    c.  **投影步骤**：计算当前支撑集 $S$ 上信号 $x_S$ 的最小二乘估计。令 $A_S$ 为 $A$ 中由 $S$ 索引的列构成的子矩阵。解为：\n        $$\\hat{x}_S = \\arg\\min_{z} \\|y - A_S z\\|_2^2 = A_S^\\dagger y$$\n        其中 $A_S^\\dagger = (A_S^T A_S)^{-1} A_S^T$ 是伪逆。在实践中，这通过一个稳定的线性系统求解器来解决。\n    d.  **残差更新**：通过减去当前估计的贡献来更新残差：\n        $$r_t = y - A_S \\hat{x}_S$$\n    e.  增加 $t$。\n3.  最终恢复的支撑集是 $S$。\n\n#### 矩阵构造\n-   **固定 $\\mu(A)$ 矩阵**：为构造一个在两列之间具有受控高相关性的矩阵，我们对给定的 $m, n, \\alpha$ 按如下步骤操作：\n    1.  选择两个标准正交向量 $u, w \\in \\mathbb{R}^m$。规范的选择是 $u = e_1$ 和 $w = e_2$。\n    2.  定义前两列为 $a_1 = u$ 和 $a_2 = \\alpha u + \\sqrt{1 - \\alpha^2} w$。根据构造，$\\|a_1\\|_2 = 1$，$\\|a_2\\|_2 = 1$，且 $\\langle a_1, a_2 \\rangle = \\alpha$。\n    3.  其余 $n-2$ 列必须是 $\\mathrm{span}\\{u, w\\}$ 正交补中的单位向量。对于 $u=e_1, w=e_2$，该子空间是 $\\mathrm{span}\\{e_3, \\dots, e_m\\}$。我们在 $\\mathbb{R}^{m-2}$ 中生成 $n-2$ 个随机向量，将它们归一化，并通过在前两个坐标填充零将它们嵌入到 $\\mathbb{R}^m$ 中。这确保了它们与 $a_1$ 和 $a_2$ 正交。所得矩阵 $A$ 的互相关性将由 $\\alpha$ 主导，因此 $\\mu(A) \\approx \\alpha$。\n\n-   **对 RIP 友好的（高斯）矩阵**：该矩阵作为良好经验性能的基准。它通过生成一个 $m \\times n$ 矩阵来构造，其元素独立地从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取，然后将其 $n$ 个列中的每一个归一化为单位 $\\ell_2$-范数。\n\n#### 仿真方案\n对于每个测试用例，我们扫描稀疏度值 $k \\in \\{1, \\dots, \\lfloor 0.12m \\rfloor\\}$。对于每个 $(k, \\mathrm{SNR})$ 对，我们运行 $25$ 次试验：\n1.  **信号生成**：创建一个 $k$-稀疏信号 $x$。其支撑集 $S_0$ 根据矩阵类型选择。对于固定 $\\mu(A)$ 矩阵，为测试最坏情况，如果 $k \\ge 2$，则支撑集必须包括两个高度相关的列的索引 $\\{0, 1\\}$。对于高斯矩阵，支撑集是均匀随机选择的。$k$ 个非零系数被设置为单位幅值，符号随机。\n2.  **噪声生成**：测量噪声 $e$ 是一个包含 $m$ 个从高斯分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的独立样本的向量。方差 $\\sigma^2$ 被设置为满足指定的信噪比：\n    $$\\mathrm{SNR} = \\frac{\\|Ax\\|_2^2}{\\mathbb{E}[\\|e\\|_2^2]} = \\frac{\\|Ax\\|_2^2}{m \\sigma^2} \\implies \\sigma^2 = \\frac{\\|Ax\\|_2^2}{m \\cdot \\mathrm{SNR}}$$\n3.  **恢复与评估**：测量向量形成为 $y = Ax + e$。运行 OMP 得到一个估计的支撑集 $\\hat{S}$。如果 $\\hat{S}$ 与真实支撑集 $S_0$ 完全相同，则试验成功。\n4.  **边界定义**：经验成功率是成功试验的比例。对于给定的 $\\mathrm{SNR}$，相变边界是该比率至少为 $0.8$ 的最大 $k$ 值。\n\n### 3. 验证标准\n对于每个测试用例，我们根据计算出的边界执行三项检查：\n\n1.  **下界验证**：在最高 $\\mathrm{SNR}$ ($20.0$) 下，将固定 $\\mu(A)$ 矩阵的经验边界 $k^*_{\\mu, 20}$ 与理论预测 $K_{\\mathrm{coh}}$ 进行比较。我们验证是否 $k^*_{\\mu, 20} \\ge K_{\\mathrm{coh}}$。\n\n2.  **RIP 优越性**：在最高 $\\mathrm{SNR}$ ($20.0$) 下，将高斯矩阵的边界 $k^*_{G, 20}$ 与固定 $\\mu(A)$ 矩阵的边界 $k^*_{\\mu, 20}$ 进行比较。我们验证对 RIP 友好的矩阵是否产生更好或相等的性能，即 $k^*_{G, 20} \\ge k^*_{\\mu, 20}$。\n\n3.  **SNR 单调性**：对于固定 $\\mu(A)$ 矩阵，我们检查恢复边界是否是 SNR 的非递减函数。设 $\\mathrm{SNR} \\in \\{5.0, 10.0, 20.0\\}$ 的边界为 $k^*_{\\mu, 5}, k^*_{\\mu, 10}, k^*_{\\mu, 20}$。我们验证是否 $k^*_{\\mu, 5} \\le k^*_{\\mu, 10} \\le k^*_{\\mu, 20}$。\n\n最终输出将由对应于三个测试用例中每项检查的九个布尔值组成。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lstsq\nimport random\n\ndef omp(A, y, k):\n    \"\"\"\n    Orthogonal Matching Pursuit algorithm.\n    :param A: Measurement matrix (m x n)\n    :param y: Measurement vector (m x 1)\n    :param k: Sparsity level\n    :return: A list of indices of the recovered support.\n    \"\"\"\n    m, n = A.shape\n    support = []\n    residual = y.copy()\n\n    for _ in range(k):\n        correlations = np.abs(A.T @ residual)\n        # Mask out already selected columns\n        if support:\n            correlations[support] = -1.0\n        \n        new_idx = np.argmax(correlations)\n        if new_idx in support:\n            # This can happen if all remaining correlations are zero\n            # or if k > rank(A). Stop early.\n            break\n        support.append(new_idx)\n        \n        A_s = A[:, sorted(support)]\n        \n        # Solve least squares problem: A_s @ x_s = y\n        # lstsq is more stable than forming the pseudoinverse directly\n        x_s, _, _, _ = lstsq(A_s, y)\n        \n        residual = y - A_s @ x_s\n    \n    return sorted(support)\n\ndef construct_fixed_mu_matrix(m, n, alpha):\n    \"\"\"Constructs a matrix with two columns having coherence `alpha`.\"\"\"\n    u = np.zeros(m)\n    u[0] = 1.0\n    w = np.zeros(m)\n    w[1] = 1.0\n    \n    a1 = u\n    a2 = alpha * u + np.sqrt(1 - alpha**2) * w\n    \n    A = np.zeros((m, n))\n    A[:, 0] = a1\n    if n > 1:\n        A[:, 1] = a2\n    \n    if n > 2:\n        rem_cols = np.random.randn(m - 2, n - 2)\n        rem_cols /= np.linalg.norm(rem_cols, axis=0, keepdims=True)\n        A[2:, 2:] = rem_cols\n        \n    return A\n\ndef construct_gaussian_matrix(m, n):\n    \"\"\"Constructs a matrix with normalized i.i.d. Gaussian columns.\"\"\"\n    A = np.random.randn(m, n)\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    return A\n\ndef find_boundary(m, n, k_range, snr, matrix_type, alpha):\n    \"\"\"Finds the maximum k for which recovery success rate is >= threshold.\"\"\"\n    num_trials = 25\n    success_threshold = 0.8\n\n    if matrix_type == 'fixed_mu':\n        A = construct_fixed_mu_matrix(m, n, alpha)\n    else: # 'gaussian'\n        A = construct_gaussian_matrix(m, n)\n\n    # Scan k from high to low to find the boundary efficiently\n    for k in reversed(k_range):\n        success_count = 0\n        for _ in range(num_trials):\n            # Generate sparse signal x\n            x = np.zeros(n)\n            if matrix_type == 'fixed_mu':\n                if k == 1:\n                    # To be robust, pick randomly from the first two\n                    true_support_list = random.sample(range(2), 1)\n                else:\n                    # Force the two coherent columns into the support\n                    support_set = {0, 1}\n                    while len(support_set)  k:\n                        support_set.add(random.randint(2, n - 1))\n                    true_support_list = sorted(list(support_set))\n            else: # 'gaussian'\n                true_support_list = sorted(random.sample(range(n), k))\n            \n            x[true_support_list] = np.random.choice([-1.0, 1.0], size=k)\n\n            # Generate measurements y = Ax + e\n            Ax = A @ x\n            norm_Ax_sq = np.sum(Ax**2)\n            \n            # Add noise based on SNR\n            if snr > 0:\n                noise_var = norm_Ax_sq / (m * snr)\n                sigma = np.sqrt(noise_var)\n                e = np.random.normal(0, sigma, size=m)\n                y = Ax + e\n            else: # Noise-free case\n                y = Ax\n            \n            # Recover support with OMP\n            recovered_support = omp(A, y, k)\n            \n            # Check for success\n            if recovered_support == true_support_list:\n                success_count += 1\n        \n        if success_count / num_trials >= success_threshold:\n            return k # This is the boundary\n            \n    return 0 # No k satisfied the condition\n\ndef solve():\n    \"\"\"Main function to run the simulation and validation.\"\"\"\n    test_cases = [\n        # (m, n, alpha, SNRs)\n        (120, 180, 0.45, [5.0, 10.0, 20.0]),\n        (120, 180, 0.35, [5.0, 10.0, 20.0]),\n        (140, 160, 0.30, [5.0, 10.0, 20.0]),\n    ]\n    np.random.seed(42) # For reproducibility\n    random.seed(42)\n\n    final_results = []\n    \n    for m, n, alpha, snrs in test_cases:\n        # Theoretical prediction\n        K_coh = int(np.floor(0.5 * (1.0 / alpha + 1.0)))\n        \n        k_max = int(np.floor(0.12 * m))\n        k_range = list(range(1, k_max + 1))\n        \n        # Run experiments for fixed-mu matrix\n        boundaries_mu = []\n        for snr in snrs:\n            boundary = find_boundary(m, n, k_range, snr, 'fixed_mu', alpha)\n            boundaries_mu.append(boundary)\n            \n        # Run experiment for Gaussian matrix at highest SNR\n        highest_snr = snrs[-1]\n        boundary_G_high_snr = find_boundary(m, n, k_range, highest_snr, 'gaussian', alpha)\n\n        # 1. Lower-bound validation\n        boundary_mu_high_snr = boundaries_mu[-1]\n        val1 = boundary_mu_high_snr >= K_coh\n        \n        # 2. RIP superiority\n        val2 = boundary_G_high_snr >= boundary_mu_high_snr\n        \n        # 3. SNR monotonicity\n        val3 = (boundaries_mu[0] = boundaries_mu[1]) and (boundaries_mu[1] = boundaries_mu[2])\n        \n        final_results.extend([val1, val2, val3])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3462348"}]}