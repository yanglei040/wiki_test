## 引言
在数据科学、信号处理和机器学习等众多领域，我们面临着一个共同的挑战：如何从不完全或含噪的观测中恢复出有意义的信号或模型。一个强有力的先验假设是“稀疏性”——即我们寻找的解在某个基或字典下只有少数非零分量。这种假设不仅能帮助我们在欠定问题中找到唯一解，还能有效抑制噪声并揭示数据背后的内在结构。然而，由稀疏性假设（通常通过[L1范数正则化](@entry_id:751087)）导出的[优化问题](@entry_id:266749)往往包含非光滑项，无法用传统的梯度法直接求解。

本文旨在系统地介绍并剖析解决此类问题的基石算法之一：迭代[软阈值](@entry_id:635249)算法（Iterative Soft-Thresholding Algorithm, ISTA）。它以其简洁的结构和深刻的理论基础，成为了理解更高级[稀疏优化](@entry_id:166698)方法（如FISTA、ADMM）的入口。通过本文的学习，读者将能够掌握ISTA的核心思想，并将其应用于解决实际问题。

文章将分为三个核心章节展开：
- **第一章：原理与机制**，将从贝叶斯推断和邻近梯度法的角度出发，详细推导ISTA的迭代公式，深入分析其核心部件——[软阈值算子](@entry_id:755010)，并探讨其收敛性理论、步长选择条件及其内在局限。
- **第二章：应用与跨学科联系**，将展示ISTA如何从基础的LASSO问题扩展到更广泛的应用场景，包括结构化稀疏、非光滑[损失函数](@entry_id:634569)以及在[计算神经科学](@entry_id:274500)、地球物理学等领域的大规模实现，彰显其作为算法框架的灵活性与普适性。
- **第三章：动手实践**，将通过一系列精心设计的编程练习，引导读者处理算法实现中的关键细节，如[正则化参数选择](@entry_id:754210)、[停止准则](@entry_id:136282)设计以及复合约束处理，从而将理论知识转化为实践能力。

现在，让我们从ISTA的数学基础开始，踏上这段探索[稀疏优化](@entry_id:166698)世界的旅程。

## 原理与机制

本章深入探讨迭代[软阈值](@entry_id:635249)算法 (ISTA) 的核心原理与机制。在引言中，我们介绍了在各种科学与工程领域中求解稀疏逆问题的普遍需求。本章将从该问题的数学表述出发，系统地构建算法，分析其核心操作，并阐明其收敛特性和理论基础。

### [优化问题](@entry_id:266749)：贝叶斯视角下的[稀疏正则化](@entry_id:755137)

许多逆问题的核心在于从不完全或带噪声的观测 $b \in \mathbb{R}^{m}$ 中恢复未知状态 $x \in \mathbb{R}^{n}$。这些问题通常由一个线性模型描述：

$$
b = Ax + \varepsilon
$$

其中 $A \in \mathbb{R}^{m \times n}$ 是已知的正向算子（或称传感矩阵），$\varepsilon \in \mathbb{R}^{m}$ 代表[测量噪声](@entry_id:275238)。当问题是欠定的 ($m \lt n$) 或 $A$ 是病态的时，仅凭观测 $b$ 无法唯一确定 $x$。因此，必须引入关于 $x$ 的[先验信息](@entry_id:753750)来约束[解空间](@entry_id:200470)。

在[贝叶斯推断](@entry_id:146958)框架下，我们可以通过[最大后验概率](@entry_id:268939) (MAP) 估计来形式化地结合先验知识。根据[贝叶斯定理](@entry_id:151040)，状态 $x$ 在给定观测 $b$ 下的后验概率为 $p(x|b) \propto p(b|x) p(x)$，其中 $p(b|x)$ 是似然函数，$p(x)$ 是[先验概率](@entry_id:275634)[分布](@entry_id:182848)。MAP 估计旨在找到最大化该后验概率的 $x$，这等价于最小化其负对数：

$$
\hat{x}_{\text{MAP}} = \arg\min_x \left[ -\ln p(b|x) - \ln p(x) \right]
$$

一个常见的建模假设是噪声 $\varepsilon$ 为[独立同分布](@entry_id:169067)的[高斯噪声](@entry_id:260752)，即 $\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$。这导致[似然函数](@entry_id:141927) $p(b|x) \propto \exp(-\frac{1}{2\sigma^2} \|Ax - b\|_2^2)$。其负对数（忽略常数项）是二次数据保真项 $\frac{1}{2\sigma^2} \|Ax - b\|_2^2$。

为了促进解的 **稀疏性** (sparsity)，即解向量 $x$ 中只有少数非零元素，我们可以为其分量选择一个合适的先验分布。[高斯先验](@entry_id:749752) $p(x_i) \propto \exp(-x_i^2 / (2\beta^2))$ 会产生一个 $\ell_2$ 范数惩罚项，即 $\frac{1}{2\beta^2}\|x\|_2^2$，这会使解平滑但通常不稀疏。相比之下，**拉普拉斯先验** (Laplace prior)，$p(x_i) \propto \exp(-|x_i|/\beta)$，在其[概率密度函数](@entry_id:140610)原点处有一个尖峰，并且具有比[高斯分布](@entry_id:154414)更重的尾部。这种形状更有利于产生[稀疏解](@entry_id:187463)。[@problem_id:3392949] 采用[独立同分布](@entry_id:169067)的拉普拉斯先验，$p(x) \propto \exp(-\frac{1}{\beta}\|x\|_1)$，其负对数是一个 $\ell_1$ 范数惩罚项 $\frac{1}{\beta}\|x\|_1$。

值得注意的是，拉普拉斯先验是一个连续分布，它在 $x_i=0$ 处的概率密度不为零，但任何单点的概率质量为零。它不应与“尖峰-厚板” (spike-and-slab) 先验混淆，后者是一个[混合模型](@entry_id:266571)，明确地在零点处赋予了正的概率质量（“尖峰”）。尽管如此，由拉普拉斯先验导出的 $\ell_1$ 惩罚在实践中能有效地产生[稀疏解](@entry_id:187463)。[@problem_id:3392949]

结合高斯[似然](@entry_id:167119)和拉普拉斯先验，MAP 估计问题就变成了以下[优化问题](@entry_id:266749)：

$$
\min_x \left( \frac{1}{2\sigma^2} \|Ax - b\|_2^2 + \frac{1}{\beta} \|x\|_1 \right)
$$

通过将[目标函数](@entry_id:267263)乘以常数 $\sigma^2$，我们得到一个等价的、更常见的形式，即 **[LASSO](@entry_id:751223)** (Least Absolute Shrinkage and Selection Operator) 或 **[基追踪](@entry_id:200728)去噪** (Basis Pursuit Denoising, BPDN) 问题：

$$
\min_{x \in \mathbb{R}^{n}} F(x) = \frac{1}{2} \|A x - b\|_2^2 + \lambda \|x\|_1
$$

其中正则化参数 $\lambda = \sigma^2 / \beta$ 控制着数据保真度与解的稀疏性之间的权衡。[@problem_id:3392949]

在着手求解此问题之前，我们需要确定解的存在性和唯一性。[目标函数](@entry_id:267263) $F(x)$ 是一个[凸函数](@entry_id:143075)（一个凸二次项和一个凸 $\ell_1$ 范数之和）。
- **存在性**：当 $\lambda > 0$ 时，$\ell_1$ 范数项是 **强制的** (coercive)，即当 $\|x\| \to \infty$ 时，$\lambda\|x\|_1 \to \infty$，这保证了 $F(x) \to \infty$。一个强制的、下半连续的真凸函数总能达到其最小值。因此，当 $\lambda>0$ 时，解总是存在的。当 $\lambda=0$ 时，问题退化为线性最小二乘，其解也总是存在。[@problem_id:3392932]
- **唯一性**：[解的唯一性](@entry_id:143619)取决于 $F(x)$ 是否为 **严格凸** (strictly convex)。$F(x)$ 的[严格凸性](@entry_id:193965)由其二次项 $\frac{1}{2}\|Ax-b\|_2^2$ 决定，因为 $\ell_1$ 范数不是严格凸的。二次项是严格凸的当且仅当其海森矩阵 $A^\top A$ 是正定的，这等价于 $A$ 具有[满列秩](@entry_id:749628)，即 $\ker(A)=\{0\}$。如果此条件成立，则对于任何 $\lambda \ge 0$，解都是唯一的。如果 $A$ 的列不是[线性独立](@entry_id:153759)的（即 $\ker(A) \neq \{0\}$），则[解的唯一性](@entry_id:143619)无法保证。[@problem_id:3392932]

### 邻近梯度法：一个分裂求解框架

LASSO [目标函数](@entry_id:267263) $F(x)$ 是一个[复合函数](@entry_id:147347)，由一个光滑可微的二次项 $f(x) = \frac{1}{2}\|Ax-b\|_2^2$ 和一个非光滑但凸的 $\ell_1$ 范数项 $g(x) = \lambda\|x\|_1$ 组成。这种结构天然适合使用 **邻近梯度法** (proximal gradient method) 求解。

该方法的核心思想是 **[算子分裂](@entry_id:634210)** (operator splitting)。它通过迭代的方式，交替处理光滑项和非光滑项。在每次迭代中，我们首先沿着光滑项 $f$ 的负梯度方向迈出一步（类似于梯度下降），然后应用一个校正步骤来处理非光滑项 $g$。这个校正步骤通过 **邻近算子** (proximal operator) 来实现。

对于一个真、下半连续的[凸函数](@entry_id:143075) $h$，其邻近算子 $\mathrm{prox}_{\gamma h}$ 定义为：

$$
\mathrm{prox}_{\gamma h}(v) = \arg\min_u \left( h(u) + \frac{1}{2\gamma}\|u-v\|_2^2 \right)
$$

其中 $\gamma > 0$ 是一个参数。邻近算子可以被看作是广义的投影：它找到一个点 $u$，既能使 $h(u)$ 较小，又能保持与输入点 $v$ 的邻近性。

邻近梯度法的通用迭代格式如下：

$$
x^{k+1} = \mathrm{prox}_{t g}(x^k - t \nabla f(x^k))
$$

其中 $t > 0$ 是步长。这个过程可以被解读为：首先执行一个显式的梯度下降步骤（**前向步骤**）得到中间点 $v = x^k - t \nabla f(x^k)$，然后通过邻近算子（**后向步骤**）将该点“[拉回](@entry_id:160816)”以满足由 $g$ 施加的约束。[@problem_id:3392986]

### ISTA：推导与核心机制

迭代[软阈值](@entry_id:635249)算法 (ISTA) 正是邻近梯度法在 LASSO 问题上的直接应用。

**1. 梯度步骤（前向步骤）**

光滑项是 $f(x) = \frac{1}{2}\|Ax-b\|_2^2$。其梯度为：

$$
\nabla f(x) = A^\top(Ax-b)
$$

请注意，矩阵 $A$ 的[转置](@entry_id:142115) $A^\top$ 是必需的，以确保梯度维度与 $x$ 的维度匹配。[@problem_id:3392949]

**2. 邻近步骤（后向步骤）**

非光滑项是 $g(x) = \lambda\|x\|_1$。我们需要计算其邻近算子 $\mathrm{prox}_{t\lambda\|\cdot\|_1}$。根据定义，这需要求解：

$$
\mathrm{prox}_{t\lambda\|\cdot\|_1}(v) = \arg\min_u \left( \lambda\|u\|_1 + \frac{1}{2t}\|u-v\|_2^2 \right)
$$

等价地，我们可以最小化 $\tau\|u\|_1 + \frac{1}{2}\|u-v\|_2^2$，其中 $\tau = t\lambda$。由于 $\ell_1$ 范数和 $\ell_2$ 范数的平方都是可分的，这个[优化问题](@entry_id:266749)可以分解为 $n$ 个独立的一维问题：

$$
\min_{u_i} \left( \tau|u_i| + \frac{1}{2}(u_i-v_i)^2 \right)
$$

通过分析该一维凸问题的[次梯度最优性条件](@entry_id:634317)，可以得到其[闭式](@entry_id:271343)解，这个解就是 **[软阈值算子](@entry_id:755010)** (soft-thresholding operator)，记作 $S_\tau$：

$$
(S_\tau(v))_i = \mathrm{sign}(v_i) \max(|v_i| - \tau, 0)
$$

这个算子对向量 $v$ 的每个分量进行操作：如果一个分量 $v_i$ 的[绝对值](@entry_id:147688)小于或等于阈值 $\tau$，它将被设为零；否则，它将被朝着零的方向“收缩”一个量 $\tau$。[@problem_id:3392971]

例如，考虑向量 $u = (\,3,\\,-1,\\,\\tfrac{3}{2},\\,-4,\\,\\tfrac{1}{2}\\,)^{\top}$ 和阈值 $\tau = \tfrac{3}{2}$。[软阈值算子](@entry_id:755010)的作用如下 [@problem_id:3392980]：
- $u_1 = 3 > \tau$，因此收缩为 $3 - \frac{3}{2} = \frac{3}{2}$。
- $|u_2| = 1 \le \tau$，因此设为 $0$。
- $|u_3| = \frac{3}{2} \le \tau$，因此设为 $0$。
- $u_4 = -4$，其[绝对值](@entry_id:147688) $|-4|=4 > \tau$，因此收缩为 $\mathrm{sign}(-4) \times (|-4|-\tau) = -1 \times (4 - 3/2) = -5/2$。
- $|u_5| = \frac{1}{2} \le \tau$，因此设为 $0$。
最终结果为 $S_{3/2}(u) = (\frac{3}{2}, 0, 0, -\frac{5}{2}, 0)^{\top}$。这个例子清晰地展示了[软阈值](@entry_id:635249)如何同时实现变量选择（置零）和系数收缩。

值得将[软阈值](@entry_id:635249)与 **硬阈值** (hard-thresholding) 进行对比，后者的公式为 $H_{\tau}(x)_i = x_i$（如果 $|x_i| > \tau$）或 $0$（如果 $|x_i| \le \tau$）。硬阈值算子在 $|x_i|=\tau$ 处是不连续的。一个关键的理论结果是，任何真、下半连续[凸函数](@entry_id:143075)的邻近算子都必须是连续的（实际上是固定非扩张的）。因此，不连续的硬阈值算子不可能是任何[凸函数](@entry_id:143075)的邻近算子。这解释了为什么在基于[凸优化](@entry_id:137441)的邻近梯度框架中，[软阈值](@entry_id:635249)（源于凸的 $\ell_1$ 范数）是自然的选择，而硬阈值（源于非凸的 $\ell_0$ 伪范数）则不是。[@problem_id:3392971]

**3. 完整的 ISTA 迭代**

将梯度步骤和邻近步骤结合起来，我们便得到了 ISTA 的完整迭代公式：

$$
x^{k+1} = S_{t\lambda}(x^k - t A^\top(Ax^k-b))
$$

其中 $S_{t\lambda}$ 是以 $\tau=t\lambda$ 为阈值的[软阈值算子](@entry_id:755010)。[@problem_id:3392949]

### ISTA 的[收敛性分析](@entry_id:151547)

ISTA 的收敛性与步长 $t$ 的选择密切相关。

**1. 步长条件**

[收敛性分析](@entry_id:151547)的关键在于光滑项 $f(x)$ 的梯度 $\nabla f$ 的一个性质，即 **$L$-[利普希茨连续性](@entry_id:142246)** ($L$-Lipschitz continuity)。这意味着存在一个常数 $L>0$ 使得对于所有 $x, y$：

$$
\|\nabla f(x) - \nabla f(y)\|_2 \le L \|x - y\|_2
$$

对于我们的二次[损失函数](@entry_id:634569) $f(x) = \frac{1}{2}\|Ax-b\|_2^2$，其梯度的[利普希茨常数](@entry_id:146583) $L$ 等于矩阵 $A^\top A$ 的最大[特征值](@entry_id:154894)，也等于 $A$ 的[谱范数](@entry_id:143091)的平方，即 $L = \|A^\top A\|_2 = \|A\|_2^2$。[@problem_id:3392949]

理论分析表明，只要步长 $t$ 满足以下条件，ISTA 的迭代序列就会收敛到[目标函数](@entry_id:267263)的一个最小值点：

$$
0  t  \frac{2}{L}
$$

然而，这个条件并不能保证[目标函数](@entry_id:267263)值 $F(x^k)$ 在每次迭代中都单调下降。要获得 **单调下降** 的保证，即 $F(x^{k+1}) \le F(x^k)$，需要一个更严格的步长条件：

$$
0  t \le \frac{1}{L}
$$

当步长选择在 $(1/L, 2/L)$ 区间时，虽然算法最终仍会收敛，但目标函数值可能会在迭代过程中出现震荡。[@problem_id:3392949] 我们可以通过一个具体的例子来说明这一点。考虑一个特设的 $2 \times 2$ 问题，其中 $L=101$。若选择步长 $t=0.015$，该值满足 $1/L \approx 0.0099  t  2/L \approx 0.0198$。从一个精心选择的初始点 $x^0$ 出发，进行一次 ISTA 迭代后，可以构造出目标函数值实际上增加了的例子，即 $F(x^1) - F(x^0)  0$。[@problem_id:3392942] 这个例子具体地揭示了步长选择对算法行为的微妙影响。

**2. [算子理论](@entry_id:139990)视角**

为了更深刻地理解收敛性，我们可以将 ISTA 视为一个[不动点迭代](@entry_id:749443)过程 $x^{k+1} = T(x^k)$，其中算子 $T$ 定义为 $T(x) = \mathrm{prox}_{tg}(x - t \nabla f(x))$。可以证明，[目标函数](@entry_id:267263) $F(x)$ 的[最小值点集](@entry_id:633969)合与算子 $T$ 的[不动点](@entry_id:156394)集合是完全相同的。[@problem_id:3392977]

收敛性的保证来自于 $T$ 的一个关键性质：对于 $t \in (0, 2/L)$，$T$ 是一个 **平均非扩张算子** (averaged non-expansive operator)。一个算子 $T$ 如果可以写成 $T = (1-\alpha)I + \alpha R$ 的形式，其中 $I$ 是单位算子，$R$ 是一个非扩张算子（即 $\|R(x)-R(y)\|_2 \le \|x-y\|_2$），$\alpha \in (0,1)$，那么它就是 $\alpha$-平均的。根据 Krasnosel'skii-Mann 迭代理论，对于任何平均非扩张算子，只要其存在[不动点](@entry_id:156394)，从任意初始点开始的[不动点迭代](@entry_id:749443)都会收敛到一个[不动点](@entry_id:156394)。[@problem_id:3392977]

ISTA 算子 $T$ 是梯度步骤算子 $T_G = I - t \nabla f$ 和邻近步骤算子 $T_P = \mathrm{prox}_{tg}$ 的复合 $T=T_P \circ T_G$。
- $T_P$ 作为[凸函数](@entry_id:143075)的邻近算子，是 **固定非扩张的** (firmly non-expansive)，这是一个比非扩张更强的性质，等价于 $1/2$-平均。
- 当 $t \in (0, 2/L)$ 时，$T_G$ 是 $\frac{tL}{2}$-平均的。
- 两个[平均算子](@entry_id:746605)的复合仍然是[平均算子](@entry_id:746605)。[@problem_id:3392986]

因此，ISTA 算子 $T$ 的平均非扩[张性](@entry_id:141857)质保证了其[全局收敛性](@entry_id:635436)。

**3. 收敛速率及其影响因素**

ISTA 的一个主要局限是其收敛速率较慢，理论上为 **次线性** (sublinear) 的，即误差 $\|x^k - x^*\|$ 的界以 $\mathcal{O}(1/\sqrt{k})$ 或目标函数值的误差 $F(x^k) - F(x^*)$ 的界以 $\mathcal{O}(1/k)$ 的速率下降。

值得注意的是，传感矩阵 $A$ 的性质，如 **受限等距性质** (Restricted Isometry Property, RIP) 或低 **[互相关性](@entry_id:188177)** (mutual coherence)，对于保证 LASSO 解能够准确地恢复真实的稀疏信号至关重要。这些性质确保了不同的[稀疏信号](@entry_id:755125)在测量后能够被区分开。例如，低[互相关性](@entry_id:188177)减少了不同原子（即 $A$ 的列）之间的“串扰”，有助于算法在迭代过程中正确识别真实信号的支撑集（非零元素的位置）。然而，这些性质本身并不能改变 ISTA 的基本收敛速率。即使在 RIP 条件下，ISTA 仍然是次[线性收敛](@entry_id:163614)的，不会自动变为[线性收敛](@entry_id:163614)。[@problem_id:3392943] 要获得更快的收敛速率（例如[线性收敛](@entry_id:163614)），通常需要对[目标函数](@entry_id:267263)施加更强的假设（如强[凸性](@entry_id:138568)）或采用更先进的算法，如 FISTA（快速迭代[软阈值](@entry_id:635249)算法）。

### 诠释与扩展

**1. 统计诠释：MAP 估计与偏差**

回到贝叶斯视角，ISTA 计算的 LASSO 解是[后验概率](@entry_id:153467)[分布](@entry_id:182848)的众数，即 MAP 估计。这与另一个重要的[贝叶斯估计量](@entry_id:176140)——**[后验均值](@entry_id:173826)** (posterior mean) $\mathbb{E}[x|y]$ 不同，后者是整个后验分布的期望，并且能在[均方误差](@entry_id:175403)损失下最小化[贝叶斯风险](@entry_id:178425)。

对于非对称的[后验分布](@entry_id:145605)（如此处的拉普拉斯-高斯模型），众数和均值通常不重合。ISTA 核心的[软阈值](@entry_id:635249)操作引入了一种系统的 **收缩偏差** (shrinkage bias)：即使是真实信号中的大的非零系数，其估计值也会被向零收缩一个固定的量。

为了修正这种偏差，可以采用 **去偏** (debiasing) 策略。一种简单实用的方法是：首先使用 ISTA（或任何 LASSO 求解器）来识别[稀疏解](@entry_id:187463)的支撑集 $S$（即非零元素的位置），然后，在确定了这些“有效”变量后，在这些变量上求解一个无惩罚的[最小二乘问题](@entry_id:164198)来重新估计它们的数值。这种方法在支撑集被稳定识别且对应的子矩阵 $A_S$ 良态时非常有效。[@problem_id:3392984]

更高级的去偏技术，如“去偏 [LASSO](@entry_id:751223)”(debiased/desparsified LASSO)，通过构造一个校正项来近似抵消收缩偏差，能够在一定条件下为估计的系数提供有效的置信区间和假设检验，这对于[科学推断](@entry_id:155119)至关重要。[@problem_id:3392984]

**2. 变换域[稀疏模型](@entry_id:755136)：综合与分析**

在许多应用中，信号本身不是稀疏的，而是在某个变换域（如小波域或傅里叶域）中是稀疏的。设 $W$ 是一个[线性变换](@entry_id:149133)。对此有两种主流的建模方式 [@problem_id:3392938]：

- **综合[稀疏模型](@entry_id:755136) (Synthesis Model)**：假设信号 $x$ 可以由一个字典 $W \in \mathbb{R}^{n \times p}$ 的少数列（原子）线性 **综合** 而成，即 $x = W\alpha$，其中系数向量 $\alpha \in \mathbb{R}^p$ 是稀疏的。[优化问题](@entry_id:266749)在系数 $\alpha$ 上构建：
  $$
  \min_{\alpha} \frac{1}{2} \|AW\alpha - b\|_2^2 + \lambda \|\alpha\|_1
  $$
  这完全符合我们之前讨论的标准 LASSO 形式，只是将 $A$ 替换为等效矩阵 $AW$。ISTA 直接在 $\alpha$ 上迭代，梯度项为 $(AW)^\top(AW\alpha - b)$，邻近步骤是对 $\alpha$ 进行简单的[软阈值](@entry_id:635249)操作。

- **[分析稀疏模型](@entry_id:746433) (Analysis Model)**：假设信号 $x$ 经过某个[分析算子](@entry_id:746429) $W \in \mathbb{R}^{p \times n}$ **分析** 后得到的系数 $Wx$ 是稀疏的。[优化问题](@entry_id:266749)在信号 $x$ 本身上构建：
  $$
  \min_{x} \frac{1}{2} \|Ax - b\|_2^2 + \lambda \|Wx\|_1
  $$
  在这种情况下，ISTA 仍然在 $x$ 上迭代，光滑项的梯度是 $A^\top(Ax-b)$。然而，非光滑项变为 $g(x) = \lambda \|Wx\|_1$，其邻近算子 $\mathrm{prox}_{t\lambda\|W(\cdot)\|_1}$ 不再是简单的[软阈值](@entry_id:635249)。
  - 一般来说，这个邻近算子没有闭式解，需要在 ISTA 的每次迭代中通过一个内部的[迭代算法](@entry_id:160288)来近似求解，这会大大增加计算成本。
  - 一个重要的特例是当 $W$ 是一个 **紧框架** (tight frame)，满足 $W^\top W = cI$（注意这里 $W$ 的维度是 $p \times n$，所以 $W^\top W$ 是 $n \times n$）时，邻近算子有闭式解。特别地，当 $W$ 是一个 **正交矩阵** ($p=n$ 且 $W^\top W = I$)时，邻近算子简化为 $x^{k+1} = W^\top S_{t\lambda}(W x_{\text{intermediate}})$，即将中间变量变换到 $W$ 域，进行[软阈值](@entry_id:635249)，再变换回来。[@problem_id:3392938] [@problem_id:3392949]

这个区别凸显了模型选择对算法实现复杂度的深远影响。综合模型在算法上更直接，而分析模型在某些情况下可能提供更灵活或更自然的建模方式，但可能带来更大的计算挑战。