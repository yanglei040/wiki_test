{"hands_on_practices": [{"introduction": "正则化参数 $\\lambda$ 是稀疏优化中的“调音旋钮”，它直接控制着解的稀疏性。一个过大的 $\\lambda$ 可能会抹杀所有信号，而一个过小的 $\\lambda$ 则可能无法有效抑制噪声。本练习 [@problem_id:3392955] 将引导您亲手构建一个因 $\\lambda$ 选择不当而导致算法失效的反例，并从概率论的第一性原理出发，推导出一个在含噪情况下选择 $\\lambda$ 的经典法则，从而深刻理解 $\\lambda$ 在平衡信号保真度与噪声抑制中的核心作用。", "problem": "考虑一个线性逆问题，其中未知向量为 $x^{\\star} \\in \\mathbb{R}^{n}$，传感矩阵为 $A \\in \\mathbb{R}^{n \\times n}$，观测数据为 $b \\in \\mathbb{R}^{n}$，由 $b = A x^{\\star} + \\eta$ 生成，其中 $\\eta$ 是一个加性噪声向量。针对 $\\ell_{1}$-正则化最小二乘问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的迭代软阈值算法 (ISTA) 由以下迭代映射定义：$x^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)$，其中 $\\tau  0$ 是满足标准利普希茨条件的步长，$\\mathcal{S}_{\\theta}$ 表示分量软阈值算子 $\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}$。\n\n(a) 当正则化参数 $\\lambda$ 相对于数据项过大时，构造一个反例，以展示在朴素阈值选择下出现的停滞（在零向量处的平台期）。具体地，取 $n = 3$, $A = I_{3}$, $\\tau = 1$ 以及 $b = (0.12,\\,-0.09,\\,0)^{\\top}$。从上述定义出发，不使用任何捷径，运用第一性原理进行推理，精确地证明在这种配置下，只要 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$，ISTA迭代 $x^{k}$ 对于所有 $k \\geq 1$ 都会恒等于零。\n\n(b) 在一个含噪数据同化设定中，假设噪声 $\\eta$ 具有独立同分布 (i.i.d.) 的高斯项，其均值为零，方差为 $\\sigma^{2}$，即 $\\eta \\sim \\mathcal{N}(0,\\,\\sigma^{2} I_{n})$。基于 $n$ 个独立同分布高斯变量最大值的分布，从概率论证出发，不引用预先陈述的捷径，推导出一个关于 $\\lambda$ 作为 $\\sigma$ 和 $n$ 的函数的解析上合理的缩放关系，该关系能够防止更新被噪声主导，同时避免对于项超过典型噪声驱动平台期的信号产生平凡零解。使用得到的缩放关系计算当 $n = 1024$ 和 $\\sigma = 0.03$ 时推荐的 $\\lambda$ 值。\n\n将 $\\lambda$ 的最终数值四舍五入到四位有效数字。将你的答案表示为无单位的纯数。", "solution": "该问题提出了与迭代软阈值算法（ISTA）相关的两个不同任务。第 (a) 部分要求在特定条件下展示迭代停滞现象，而第 (b) 部分则要求在随机设定中推导正则化参数 $\\lambda$ 的一个有原则的缩放关系。\n\n### 第 (a) 部分：在零向量处的停滞\n\n我们已知用于最小化问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的 ISTA 更新规则：\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)\n$$\n问题为此部分指定了参数：$n = 3$，单位矩阵 $A = I_{3}$，步长 $\\tau = 1$，以及数据向量 $b = (0.12,\\,-0.09,\\,0)^{\\top}$。\n\n将 $A=I_{3}$ 和 $\\tau=1$ 代入更新规则，得到一个显著的简化。对于任意迭代 $x^k$：\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\cdot 1}\\!\\left(x^{k} + 1 \\cdot I_{3}^{\\top}(b - I_{3} x^{k})\\right) = \\mathcal{S}_{\\lambda}\\!\\left(x^{k} + (b - x^{k})\\right) = \\mathcal{S}_{\\lambda}(b)\n$$\n这个结果表明，对于任何 $k \\geq 0$，下一个迭代 $x^{k+1}$ 仅由软阈值算子 $\\mathcal{S}_{\\lambda}$ 对数据向量 $b$ 的作用决定。它与当前迭代 $x^k$ 无关。因此，从 $k=1$ 开始的所有迭代都是相同的：\n$$\nx^{1} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\nx^{2} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\n\\vdots\n$$\n$$\nx^{k} = \\mathcal{S}_{\\lambda}(b) \\quad \\text{对于所有 } k \\geq 1.\n$$\n问题要求我们证明，在条件 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ 下，这些迭代会恒等于零，即对于所有 $k \\geq 1$，$x^k = 0$。\n\n在我们 $A=I_3$ 的特定配置下，该条件为 $\\lambda \\geq \\|I_3^{\\top} b\\|_{\\infty} = \\|b\\|_{\\infty}$。向量 $b=(0.12,\\,-0.09,\\,0)^{\\top}$ 的 $\\ell_{\\infty}$-范数是：\n$$\n\\|b\\|_{\\infty} = \\max\\big(|0.12|, |-0.09|, |0|\\big) = 0.12\n$$\n所以条件变为 $\\lambda \\geq 0.12$。\n\n我们现在分析在此条件下迭代的表达式 $x^k = \\mathcal{S}_{\\lambda}(b)$。软阈值算子 $\\mathcal{S}_{\\theta}$ 按分量定义为：\n$$\n\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}\n$$\n要使整个向量 $\\mathcal{S}_{\\theta}(z)$ 为零向量，其充分必要条件是对于每个分量 $i$，$\\max\\{|z_{i}| - \\theta, 0\\} = 0$。这又等价于对所有 $i$ 都有 $|z_i| \\leq \\theta$。这可以用 $\\ell_{\\infty}$-范数紧凑地表示为 $\\|z\\|_{\\infty} \\leq \\theta$。\n\n将此应用于我们的问题，$x^k = \\mathcal{S}_{\\lambda}(b)$ 为零向量的条件是 $\\|b\\|_{\\infty} \\leq \\lambda$。这恰好是问题陈述中对于我们选择的 $A=I_3$ 给出的条件 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$。\n\n具体来说，当 $\\lambda \\geq 0.12$ 时：\n对于第一个分量 $b_1 = 0.12$：$|b_1|=0.12$。由于 $\\lambda \\geq 0.12$，则 $|b_1| - \\lambda \\leq 0$，所以 $\\max\\{|b_1|-\\lambda, 0\\}=0$。\n对于第二个分量 $b_2 = -0.09$：$|b_2|=0.09$。由于 $\\lambda \\geq 0.12$，则 $|b_2| - \\lambda  0$，所以 $\\max\\{|b_2|-\\lambda, 0\\}=0$。\n对于第三个分量 $b_3 = 0$：$|b_3|=0$。由于 $\\lambda \\geq 0.12$，则 $|b_3| - \\lambda  0$，所以 $\\max\\{|b_3|-\\lambda, 0\\}=0$。\n\n由于 $\\mathcal{S}_{\\lambda}(b)$ 的所有分量都为零，我们有 $\\mathcal{S}_{\\lambda}(b) = 0$。\n因此，对于任意选择的初始向量 $x^0$，第一次迭代为 $x^1 = \\mathcal{S}_{\\lambda}(b) = 0$，并且对于 $k \\geq 1$ 的所有后续迭代 $x^k$ 都保持在零向量。这证明了指定的停滞现象。\n\n### 第 (b) 部分：正则化参数的概率缩放\n\n在这一部分，我们要推导一个 $\\lambda$ 的缩放关系，以防止算法被噪声主导。模型是 $b = Ax^\\star + \\eta$，其中 $\\eta$ 是一个由独立同分布高斯噪声分量组成的向量，$\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n正则化的一个关键原则是选择足够大的 $\\lambda$ 来抑制噪声，但又不能大到错误地消除真实的信号分量。一个常见的策略是选择 $\\lambda$ 略高于在被阈值处理的项中预期会看到的噪声水平。\n\n考虑从自然的初始猜测 $x^0 = 0$ 开始的 ISTA 第一次迭代。软阈值算子的自变量是 $x^0 + \\tau A^{\\top}(b - A x^0) = \\tau A^\\top b$。如果我们考虑一个没有真实信号的场景（$x^\\star=0$），那么 $b=\\eta$，这个自变量就变成 $\\tau A^\\top \\eta$。为了防止算法拟合噪声，我们希望这一项被阈值处理为零。这要求 $\\tau A^\\top\\eta$ 的每个分量的幅值都小于或等于阈值 $\\lambda\\tau$。这可以写成：\n$$\n\\|\\tau A^\\top \\eta\\|_{\\infty} \\leq \\lambda\\tau \\implies \\|A^\\top \\eta\\|_{\\infty} \\leq \\lambda\n$$\n这为我们选择 $\\lambda$ 提供了一个标准：它应该是 $\\|A^\\top \\eta\\|_{\\infty}$ 的可能值的上界。\n\n为了进行解析推导，我们必须刻画向量 $v = A^\\top \\eta$ 的分布。这个分布依赖于矩阵 $A$。在这类通用阈值推导中，一个标准的假设是传感矩阵 $A$ 是标准正交的，即 $A^\\top A = I_n$。在此假设下，$v$ 的协方差是：\n$$\n\\mathrm{Cov}(v) = E[v v^\\top] = E[A^\\top \\eta \\eta^\\top A] = A^\\top E[\\eta \\eta^\\top] A = A^\\top (\\sigma^2 I_n) A = \\sigma^2 A^\\top A = \\sigma^2 I_n\n$$\n由于 $E[v] = A^\\top E[\\eta] = 0$，向量 $v = A^\\top \\eta$ 的分量 $v_i$ 是独立同分布的高斯随机变量，均值为 $0$，方差为 $\\sigma^2$，与原始噪声分量 $\\eta_i$ 相同。\n\n我们的任务现在简化为寻找随机变量 $M_n = \\|v\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |v_i|$ 的一个高概率上界。令 $Z_i = v_i/\\sigma$ 为独立同分布的标准正态变量，$Z_i \\sim \\mathcal{N}(0, 1)$。我们寻求对 $\\max_i |Z_i|$ 的一个估计，然后将结果乘以 $\\sigma$ 进行缩放。\n\n令 $Y = \\max_{1 \\leq i \\leq n} |Z_i|$。$|Z_i|$ 的累积分布函数（CDF）是 $F_{|Z|}(y) = P(|Z_i| \\leq y) = 2\\Phi(y)-1$（对于 $y \\geq 0$），其中 $\\Phi$ 是标准正态分布的 CDF。由于 $|Z_i|$ 是独立同分布的，它们最大值的 CDF 是：\n$$\nF_Y(y) = P(Y \\leq y) = [F_{|Z|}(y)]^n = (2\\Phi(y)-1)^n\n$$\n我们希望找到一个阈值 $y_n$，使得对于大的 $n$，$P(Y  y_n)$ 很小。我们可以使用联合界作为这个尾部概率的近似：\n$$\nP(Y > y) = P(\\exists i: |Z_i| > y) \\leq \\sum_{i=1}^n P(|Z_i|y) = n P(|Z_1|y)\n$$\n概率 $P(|Z_1|y)$ 是 $2(1-\\Phi(y))$。对于大的 $y$，我们可以使用标准高斯尾部近似：\n$$\n1-\\Phi(y) \\approx \\frac{\\phi(y)}{y} = \\frac{1}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)\n$$\n因此，$P(Yy) \\approx 2n(1-\\Phi(y)) \\approx \\frac{2n}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)$。我们寻求一个 $y$ 关于 $n$ 的缩放关系，使得当 $n \\to \\infty$ 时这个概率趋于零。让我们测试候选的缩放关系 $y = \\sqrt{2\\ln n}$：\n$$\nP(Y > \\sqrt{2\\ln n}) \\approx \\frac{2n}{\\sqrt{2\\pi}\\sqrt{2\\ln n}}\\exp\\left(-\\frac{( \\sqrt{2\\ln n} )^2}{2}\\right) = \\frac{2n}{\\sqrt{4\\pi\\ln n}}\\exp(-\\ln n) = \\frac{2n}{2\\sqrt{\\pi\\ln n}} \\left(\\frac{1}{n}\\right) = \\frac{1}{\\sqrt{\\pi\\ln n}}\n$$\n当 $n \\to \\infty$ 时，这个概率趋于 $0$。这表明对于大的 $n$，$n$ 个独立同分布的 $|Z_i|$ 变量的最大值很可能不会超过 $\\sqrt{2\\ln n}$。这证明了选择该值作为阈值是合理的。\n\n用 $\\sigma$ 进行反向缩放，我们得到 $\\lambda$ 的解析上合理的缩放关系：\n$$\n\\lambda = \\sigma \\sqrt{2 \\ln n}\n$$\n这就是著名的通用阈值。它提供了一种能适应问题维度 $n$ 和噪声水平 $\\sigma$ 的参数选择。\n\n现在，我们计算当 $n = 1024$ 和 $\\sigma = 0.03$ 时推荐的 $\\lambda$。\n首先，计算 $\\ln(1024)$：\n$$\n\\ln(1024) = \\ln(2^{10}) = 10 \\ln(2)\n$$\n使用 $\\ln(2) \\approx 0.69314718$ 的标准值：\n$$\n\\ln(1024) \\approx 6.9314718\n$$\n现在，将此代入 $\\lambda$ 的公式：\n$$\n\\lambda = 0.03 \\times \\sqrt{2 \\times 6.9314718} = 0.03 \\times \\sqrt{13.8629436} \\approx 0.03 \\times 3.7232974\n$$\n$$\n\\lambda \\approx 0.1116989\n$$\n将此值四舍五入到四位有效数字，得到：\n$$\n\\lambda \\approx 0.1117\n$$", "answer": "$$\n\\boxed{0.1117}\n$$", "id": "3392955"}, {"introduction": "在运行任何迭代算法时，一个核心的实际问题是：“我们应该在何时停止？” 简单地设置一个固定的迭代次数或检查迭代变量的微小变化往往不够可靠。本练习 [@problem_id:3392931] 将带您探索一个更为严谨和健壮的解决方案：基于原始-对偶间隙 (primal-dual gap) 的停止准则。您将需要从第一性原理推导LASSO问题的对偶形式，并利用此对偶问题构造一个可计算的次优性上界，从而为ISTA算法设计一个有理论保证的“刹车系统”。", "problem": "考虑在稀疏逆问题和数据同化中常见的标准凸复合优化问题：最小化一个光滑的数据拟合项加上一个非光滑的稀疏性促进惩罚项。具体来说，考虑最小绝对收缩和选择算子 (LASSO) 目标函数，给定一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，一个数据向量 $b \\in \\mathbb{R}^{m}$，以及一个正则化参数 $\\lambda  0$，其原始目标为\n$$\nP(x) \\equiv \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1, \\quad x \\in \\mathbb{R}^n.\n$$\n您将为迭代软阈值算法 (ISTA) 设计一个实用的停止准则，该准则基于一个可作为原始次优性上界的原始-对偶间隙，通过从给定的原始迭代点构造一个对偶可行点并评估一个可计算的次优性上界来实现。\n\n问题要求：\n\n1) 从凸共轭、Fenchel 对偶和范数的次梯度的基本定义出发，从第一性原理推导与上述原始目标 $P(x)$ 相关的对偶问题。然后，利用弱对偶性，推导出一个可计算的原始-对偶间隙表达式 $G(x,\\nu)$，该表达式对于任何原始点 $x$ 和任何对偶可行点 $\\nu$ 都是非负的。\n\n2) 从任何原始迭代点 $x$ 出发，仅使用 ISTA 迭代过程中可用的量（例如，残差 $Ax - b$ 以及与 $A$ 和 $A^\\top$ 的矩阵向量乘积）来构造一个对偶可行点 $\\nu \\in \\mathbb{R}^m$。您的构造必须保证对偶约束的可行性，而无需知道精确解。\n\n3) 利用 ISTA 对于光滑项 $\\frac{1}{2}\\|Ax-b\\|_2^2$（其梯度是 Lipschitz 连续的，具有由 $A$ 决定的适当常数）和非光滑项 $\\lambda \\|x\\|_1$ 的标准近端梯度结构，指定一个终止规则，该规则使用原始-对偶间隙来判断是否在要求的容差 $\\varepsilon  0$ 内收敛。证明终止时的所得间隙是实际原始次优性 $P(x) - P(x^\\star)$ 的一个上界，其中 $x^\\star$ 表示一个最小化点。\n\n4) 实现一个程序，该程序：\n   - 计算 $A$ 的谱范数以确定一个有效的 ISTA 固定步长。\n   - 从零向量开始运行 ISTA，在每次迭代中从当前原始迭代点构造一个对偶可行点，评估原始-对偶间隙，并在间隙至多为 $\\varepsilon$ 或达到最大迭代次数时终止。\n   - 对于每个测试用例，返回最终的原始-对偶间隙，形式为浮点数。\n\n使用以下测试套件。所有随机生成必须使用指定的种子以保证可复现性。\n\n- 测试用例 1（正常路径，过参数化，带噪声的稀疏信号）：\n  - 维度：$m = 40$， $n = 60$。\n  - 随机种子：$123$。\n  - 生成 $A$，其元素为独立同分布，均值为 0，方差为 $1/m$ 的正态分布。\n  - 生成一个真实稀疏向量 $x_{\\mathrm{true}} \\in \\mathbb{R}^n$，其在均匀随机位置上有且仅有 $k = 6$ 个非零值，非零值为独立同分布的标准正态分布。\n  - 生成噪声 $\\eta \\in \\mathbb{R}^m$，其元素为独立同分布，均值为 0，标准差为 $\\sigma = 10^{-2}$ 的正态分布。\n  - 设置 $b = A x_{\\mathrm{true}} + \\eta$。\n  - 设置 $\\lambda = 0.1 \\cdot \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-6}$，最大迭代次数 $10{,}000$。\n\n- 测试用例 2（边界情况，单位算子）：\n  - 维度：$m = 30$， $n = 30$。\n  - 矩阵 $A = I$（$30 \\times 30$ 的单位矩阵）。\n  - 随机种子：$2024$。\n  - 生成 $b$，其元素为独立同分布的标准正态分布。\n  - 设置 $\\lambda = \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-12}$，最大迭代次数 $1{,}000$。\n\n- 测试用例 3（欠定系统）：\n  - 维度：$m = 30$， $n = 80$。\n  - 随机种子：$321$。\n  - 生成 $A$，其元素为独立同分布，均值为 0，方差为 $1/m$ 的正态分布。\n  - 生成 $b$，其元素为独立同分布的标准正态分布。\n  - 设置 $\\lambda = 0.01 \\cdot \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-5}$，最大迭代次数 $20{,}000$。\n\n- 测试用例 4（病态算子）：\n  - 维度：$m = 50$， $n = 50$。\n  - 随机种子：$999$。\n  - 生成一个基础矩阵，其元素为独立同分布，均值为 0，方差为 $1/m$ 的正态分布，然后对于 $j \\in \\{1,\\dots,n\\}$，将第 $j$ 列乘以一个缩放因子 $s_j = 10^{4 \\cdot (j-1)/(n-1)}$，以引入一个大约为 $10^4$ 的条件数。\n  - 生成 $b$，其元素为独立同分布的标准正态分布。\n  - 设置 $\\lambda = 0.05 \\cdot \\|A^\\top b\\|_\\infty$。\n  - 容差 $\\varepsilon = 10^{-5}$，最大迭代次数 $20{,}000$。\n\n您的程序必须产生单行输出，其中包含一个逗号分隔的列表，内容是四个测试用例的最终原始-对偶间隙，按上述测试用例的顺序排列，并用方括号括起来（例如，$[g_1,g_2,g_3,g_4]$）。输出必须是浮点数。本问题不涉及物理单位或角度单位。", "solution": "该问题要求为解决 LASSO 优化问题设计并实现一种基于原始-对偶间隙的迭代软阈值算法 (ISTA) 停止准则。解决方案将按要求分为三部分呈现：对偶问题和间隙的推导、对偶可行点的构造、终止规则的说明，最后是实现细节。\n\n### 第一部分：Fenchel 对偶与原始-对偶间隙的推导\n\n原始 LASSO 问题由以下公式给出：\n$$\nP(x) \\equiv \\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 \\right)\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda  0$。\n\n为了推导对偶问题，我们首先通过引入一个辅助变量 $r \\in \\mathbb{R}^m$ 来重构原始目标，并设 $r = Ax$。问题变成一个有约束的优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^n, r \\in \\mathbb{R}^m} \\frac{1}{2}\\|r - b\\|_2^2 + \\lambda \\|x\\|_1 \\quad \\text{subject to} \\quad r - Ax = 0.\n$$\n我们通过为等式约束引入一个对偶变量（拉格朗日乘子）$\\nu \\in \\mathbb{R}^m$ 来构造拉格朗日函数：\n$$\nL(x, r, \\nu) = \\frac{1}{2}\\|r - b\\|_2^2 + \\lambda \\|x\\|_1 + \\nu^\\top (r - Ax)\n$$\n拉格朗日对偶函数 $D(\\nu)$ 是拉格朗日函数关于原始变量 $x$ 和 $r$ 的下确界：\n$$\nD(\\nu) = \\inf_{x, r} L(x, r, \\nu) = \\inf_{r} \\left( \\frac{1}{2}\\|r - b\\|_2^2 + \\nu^\\top r \\right) + \\inf_{x} \\left( \\lambda \\|x\\|_1 - \\nu^\\top Ax \\right)\n$$\n这两个下确界可以分开计算。\n第一项是关于 $r$ 的无约束二次最小化。当梯度为零时达到最小值：$\\nabla_r (\\frac{1}{2}(r-b)^\\top (r-b) + \\nu^\\top r) = (r - b) + \\nu = 0$，这意味着 $r = b - \\nu$。将其代回得到最小值为：$\\frac{1}{2}\\|(b-\\nu) - b\\|_2^2 + \\nu^\\top(b-\\nu) = \\frac{1}{2}\\|\\nu\\|_2^2 + \\nu^\\top b - \\|\\nu\\|_2^2 = \\nu^\\top b - \\frac{1}{2}\\|\\nu\\|_2^2$。\n另一种更简洁的推导方式是利用 $f(r) = \\frac{1}{2}\\|r-b\\|_2^2$ 的 Fenchel 共轭 $f^*(\\nu) = \\frac{1}{2}\\|\\nu\\|_2^2 + b^\\top \\nu$。那么 $\\inf_r (f(r) - (-\\nu)^\\top r) = -f^*(-\\nu) = -(\\frac{1}{2}\\|-\\nu\\|_2^2 + b^\\top(-\\nu)) = b^\\top\\nu - \\frac{1}{2}\\|\\nu\\|_2^2$。\n\n第二项涉及 $\\ell_1$-范数的 Fenchel 共轭。具体来说，$\\inf_{x} (\\lambda \\|x\\|_1 - (A^\\top\\nu)^\\top x) = -\\sup_{x} ((A^\\top\\nu)^\\top x - \\lambda\\|x\\|_1)$。该上确界是 $g(x)=\\lambda\\|x\\|_1$ 的共轭函数在 $A^\\top\\nu$ 处的定义。共轭函数 $g^*(w) = \\sup_x (w^\\top x - \\lambda\\|x\\|_1)$ 是一个指示函数：\n$$\ng^*(w) = \\begin{cases} 0  \\text{if } \\|w\\|_\\infty \\le \\lambda \\\\ +\\infty  \\text{otherwise} \\end{cases}\n$$\n因此，$-\\sup_{x} ((A^\\top\\nu)^\\top x - \\lambda\\|x\\|_1) = -g^*(A^\\top\\nu)$。如果 $\\|A^\\top\\nu\\|_\\infty \\le \\lambda$，该项为 $0$，否则为 $-\\infty$。\n\n结合这些项，对偶函数为：\n$$\nD(\\nu) = b^\\top\\nu - \\frac{1}{2}\\|\\nu\\|_2^2, \\quad \\text{provided } \\|A^\\top\\nu\\|_\\infty \\le \\lambda.\n$$\n对偶问题是在 $\\nu$ 上最大化此函数，等价于最小化其负数：\n$$\n\\min_{\\nu \\in \\mathbb{R}^m} \\left( \\frac{1}{2}\\|\\nu\\|_2^2 - b^\\top\\nu \\right) \\quad \\text{subject to} \\quad \\|A^\\top\\nu\\|_\\infty \\le \\lambda.\n$$\n根据弱对偶性，对于任何原始可行点 $x$ 和任何对偶可行点 $\\nu$（即满足约束 $\\|A^\\top\\nu\\|_\\infty \\le \\lambda$ 的 $\\nu$），我们有 $P(x) \\ge D(\\nu)$。原始-对偶间隙 $G(x, \\nu)$ 定义为它们的差，它总是非负的：\n$$\nG(x, \\nu) = P(x) - D(\\nu) = \\left(\\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\\right) - \\left(b^\\top\\nu - \\frac{1}{2}\\|\\nu\\|_2^2\\right)\n$$\n$$\nG(x, \\nu) = \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1 - b^\\top\\nu + \\frac{1}{2}\\|\\nu\\|_2^2 \\ge 0.\n$$\n该表达式提供了原始次优性的一个可计算上界，如第三部分所示。\n\n### 第二部分：对偶可行点的构造\n\n在 ISTA 执行期间，我们有一系列的原始迭代点 $\\{x_k\\}$。对于每个迭代点 $x_k$，我们需要构造一个相应的对偶可行点 $\\nu_k$，用于评估间隙。对偶可行点必须满足 $\\|A^\\top\\nu_k\\|_\\infty \\le \\lambda$。\n\nKarush-Kuhn-Tucker (KKT) 最优性条件提示了最优对偶变量的形式。在一个最优原始-对偶对 $(x^\\star, \\nu^\\star)$ 处，我们有 $A^\\top(Ax^\\star - b) + \\lambda s = 0$ 其中 $s \\in \\partial\\|x^\\star\\|_1$。这启发我们通过缩放向量 $b-Ax_k$ 来为非最优迭代点 $x_k$ 构造一个对偶可行点 $\\nu_k$。\n\n让我们测试一个形式为 $\\nu_k = c_k(b-Ax_k)$ 的候选点，其中标量 $c_k  0$。为了使该点是对偶可行的，我们必须有：\n$$\n\\|A^\\top(c_k(b-Ax_k))\\|_\\infty \\le \\lambda \\implies c_k \\|A^\\top(b-Ax_k)\\|_\\infty \\le \\lambda\n$$\n如果 $A^\\top(b-Ax_k) = 0$，任何 $c_k$ 都有效。否则，我们必须有 $c_k \\le \\frac{\\lambda}{\\|A^\\top(b-Ax_k)\\|_\\infty}$。为了使对偶目标 $D(\\nu_k)$ 尽可能大（从而使间隙尽可能小），我们希望 $\\nu_k$ “较大”，因此我们选择最大的可能缩放因子 $c_k$。\n\n然而，如果 $\\|A^\\top(b-Ax_k)\\|_\\infty  \\lambda$，这将允许 $c_k  1$。在最优解 $x^\\star$ 处，我们期望 $c_{k} \\to 1$。一个能处理所有情况的稳健选择是限制缩放因子。我们如下构造 $\\nu_k$：\n$$\n\\nu_k = \\frac{\\lambda}{\\max(\\lambda, \\|A^\\top(b-Ax_k)\\|_\\infty)} \\cdot (b-Ax_k)\n$$\n让我们验证这种构造的可行性。令 $C_k = \\max(\\lambda, \\|A^\\top(b-Ax_k)\\|_\\infty)$。那么 $\\nu_k = \\frac{\\lambda}{C_k}(b-Ax_k)$。\n$$\n\\|A^\\top\\nu_k\\|_\\infty = \\left\\| A^\\top \\left( \\frac{\\lambda}{C_k}(b-Ax_k) \\right) \\right\\|_\\infty = \\frac{\\lambda}{C_k} \\|A^\\top(b-Ax_k)\\|_\\infty.\n$$\n根据 $C_k$ 的定义，我们有 $\\|A^\\top(b-Ax_k)\\|_\\infty \\le C_k$。因此，\n$$\n\\|A^\\top\\nu_k\\|_\\infty \\le \\frac{\\lambda}{C_k} \\cdot C_k = \\lambda.\n$$\n这种构造保证了对于任何原始迭代点 $x_k$，$\\nu_k$ 都是一个对偶可行点。所有需要的量，即残差 $Ax_k-b$ 和与 $A^\\top$ 的矩阵向量乘积，在 ISTA 迭代中都很容易获得。\n\n### 第三部分：ISTA 终止规则及次优性上界证明\n\nISTA 算法是一种近端梯度法。对于 LASSO 目标 $P(x) = f(x) + h(x)$，其中 $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ 是光滑部分，而 $h(x) = \\lambda\\|x\\|_1$ 是非光滑部分，更新规则为：\n$$\nx_{k+1} = \\text{prox}_{\\alpha h}(x_k - \\alpha \\nabla f(x_k))\n$$\n这里，$\\nabla f(x) = A^\\top(Ax-b)$，而 $h(x)$ 的近端算子是软阈值算子，$\\mathcal{S}_{\\tau}(z)_i = \\text{sign}(z_i)\\max(|z_i|-\\tau, 0)$。因此 ISTA 的更新是：\n$$\nx_{k+1} = \\mathcal{S}_{\\alpha\\lambda}(x_k - \\alpha A^\\top(Ax_k-b))\n$$\n为了保证收敛，步长 $\\alpha$ 必须满足 $0  \\alpha \\le 1/L$，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。Lipschitz 常数是 $L = \\|A^\\top A\\|_2 = \\|A\\|_2^2$，其中 $\\|A\\|_2$ 是 $A$ 的谱范数。一个有效的固定步长是 $\\alpha = 1/L$。\n\n建议的终止规则如下：\n在 ISTA 的每次迭代 $k$ 中，从一个初始点 $x_0$ 开始：\n1.  计算当前的原始迭代点 $x_k$。\n2.  使用第二部分的方法构造对偶可行点 $\\nu_k$。\n3.  评估原始-对偶间隙 $G(x_k, \\nu_k) = P(x_k) - D(\\nu_k)$。\n4.  如果对于给定的容差 $\\varepsilon  0$，$G(x_k, \\nu_k) \\le \\varepsilon$，则终止算法。否则，计算 $x_{k+1}$ 并继续。\n\n**次优性上界证明：**\n令 $x^\\star$ 为原始问题 $P(x)$ 的一个最小化点，因此 $P(x^\\star) = \\min_x P(x)$。迭代点 $x_k$ 的原始次优性是非负量 $P(x_k) - P(x^\\star)$。\n根据弱对偶性，对于任何原始点 $x$ 和任何对偶可行点 $\\nu$，我们有 $P(x) \\ge D(\\nu)$。\n由于 LASSO 问题满足强对偶性，我们有 $P(x^\\star) = \\max_\\nu D(\\nu)$。\n令 $\\nu_k$ 为从原始迭代点 $x_k$ 构造的对偶可行点。因为 $\\nu_k$ 是对偶可行的，所以必然有 $D(\\nu_k) \\le \\max_\\nu D(\\nu) = P(x^\\star)$。\n因此，我们有以下不等式：\n$$\nD(\\nu_k) \\le P(x^\\star) \\le P(x_k)\n$$\n原始次优性是 $P(x_k) - P(x^\\star)$。利用不等式 $D(\\nu_k) \\le P(x^\\star)$，我们可以写出：\n$$\nP(x_k) - P(x^\\star) \\le P(x_k) - D(\\nu_k)\n$$\n右边的项正是我们可计算的原始-对偶间隙 $G(x_k, \\nu_k)$。因此，我们证明了：\n$$\nP(x_k) - P(x^\\star) \\le G(x_k, \\nu_k)\n$$\n这证明了原始-对偶间隙 $G(x_k, \\nu_k)$ 是真实原始次优性的一个上界。因此，如果算法在 $G(x_k, \\nu_k) \\le \\varepsilon$ 时终止，我们可以保证原始次优性 $P(x_k) - P(x^\\star)$ 也至多为 $\\varepsilon$。这验证了所提出的停止准则。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the ISTA algorithm with a primal-dual gap stopping criterion\n    on a suite of test cases for the LASSO problem.\n    \"\"\"\n\n    def soft_threshold(z, tau):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def run_ista(A, b, lam, tol, max_iter):\n        \"\"\"\n        Solves the LASSO problem using ISTA with a primal-dual gap stopping criterion.\n\n        Args:\n            A (np.ndarray): The measurement matrix.\n            b (np.ndarray): The data vector.\n            lam (float): The regularization parameter lambda.\n            tol (float): The tolerance for the primal-dual gap.\n            max_iter (int): The maximum number of iterations.\n\n        Returns:\n            float: The final primal-dual gap.\n        \"\"\"\n        m, n = A.shape\n        \n        # Determine a valid stepsize alpha\n        L = np.linalg.norm(A, 2)**2\n        # A lipshitz constant of 0 can occur if A is a zero matrix.\n        # In this case, x=0 is the solution, and the gradient is constant. Any positive step size will work.\n        # We set alpha=1.0 for this edge case.\n        if L == 0:\n            alpha = 1.0\n        else:\n            alpha = 1.0 / L\n\n        # Initialization\n        x = np.zeros(n)\n        \n        final_gap = np.inf\n\n        for k in range(max_iter):\n            # Primal objective P(x) = 0.5 * ||Ax - b||^2 + lambda * ||x||_1\n            residual = A @ x - b\n            primal_obj = 0.5 * np.linalg.norm(residual)**2 + lam * np.linalg.norm(x, 1)\n\n            # Construct dual feasible point nu\n            b_minus_Ax = -residual\n            At_b_minus_Ax = A.T @ b_minus_Ax\n            norm_At_b_minus_Ax_inf = np.linalg.norm(At_b_minus_Ax, np.inf)\n\n            # Epsilon to avoid division by zero in the theoretical case where max is zero.\n            # max(lambda, norm) should be non-zero since lambda > 0.\n            c = lam / np.maximum(lam, norm_At_b_minus_Ax_inf)\n            nu = c * b_minus_Ax\n\n            # Corrected Dual objective D(nu) = -0.5 * ||nu||^2 - b.T @ nu\n            dual_obj = -0.5 * np.linalg.norm(nu)**2 - b.T @ nu\n\n            # Primal-dual gap G(x, nu) = P(x) - D(nu)\n            gap = primal_obj - dual_obj\n            final_gap = gap\n\n            if gap = tol:\n                break\n\n            # ISTA update\n            grad_f = A.T @ residual\n            x = soft_threshold(x - alpha * grad_f, alpha * lam)\n        \n        return final_gap\n\n    test_cases = [\n        {'m': 40, 'n': 60, 'seed': 123, 'k': 6, 'sigma': 1e-2, 'lam_factor': 0.1, 'tol': 1e-6, 'max_iter': 10000, 'name': 'overparameterized_sparse'},\n        {'m': 30, 'n': 30, 'seed': 2024, 'lam_factor': 1.0, 'tol': 1e-12, 'max_iter': 1000, 'name': 'identity'},\n        {'m': 30, 'n': 80, 'seed': 321, 'lam_factor': 0.01, 'tol': 1e-5, 'max_iter': 20000, 'name': 'underdetermined'},\n        {'m': 50, 'n': 50, 'seed': 999, 'lam_factor': 0.05, 'tol': 1e-5, 'max_iter': 20000, 'name': 'ill_conditioned'}\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n, seed = case['m'], case['n'], case['seed']\n        rng = np.random.default_rng(seed)\n\n        if case['name'] == 'identity':\n            A = np.eye(m)\n            b = rng.standard_normal(size=m)\n        elif case['name'] == 'ill_conditioned':\n            A_base = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n            scaling_factors = 10**(4.0 * np.arange(n) / (n - 1))\n            A = A_base * scaling_factors # Broadcasting column-wise\n            b = rng.standard_normal(size=m)\n        else: # Default case for 'overparameterized_sparse' and 'underdetermined'\n            A = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n            if case['name'] == 'overparameterized_sparse':\n                k, sigma = case['k'], case['sigma']\n                x_true = np.zeros(n)\n                indices = rng.choice(n, k, replace=False)\n                x_true[indices] = rng.standard_normal(size=k)\n                noise = rng.normal(0, sigma, size=m)\n                b = A @ x_true + noise\n            else: # 'underdetermined'\n                b = rng.standard_normal(size=m)\n\n        lam = case['lam_factor'] * np.linalg.norm(A.T @ b, np.inf)\n        # Ensure lambda is strictly positive\n        if lam == 0:\n            lam = 1e-10\n\n        final_gap = run_ista(A, b, lam, case['tol'], case['max_iter'])\n        results.append(final_gap)\n\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```", "id": "3392931"}, {"introduction": "许多现实世界中的信号恢复问题不仅要求解是稀疏的，还常常附加有其他物理约束，例如非负性。本练习 [@problem_id:3392928] 探讨了如何将稀疏性和非负性这两种约束同时融入ISTA框架中。您将推导并比较两种看似不同的邻近算子 (proximal operator) 实现方式——一种是“序贯投影法”，另一种是“精确邻近算子法”——并在此过程中发现一个令人惊讶的等价性，从而加深对复合约束下邻近算子背后结构的理解。", "problem": "考虑一个出现在带有非负性和稀疏性约束的反问题和数据同化中的凸优化问题：最小化目标函数 $$F(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x),$$ 其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\lambda  0$，且 $\\iota_{\\mathbb{R}_+^n}$ 是非负象限 $\\mathbb{R}_+^n$ 的指示函数，定义为 $$\\iota_{\\mathbb{R}_+^n}(x) = \\begin{cases} 0,  \\text{if } x \\in \\mathbb{R}_+^n, \\\\ +\\infty,  \\text{otherwise.} \\end{cases}$$ 设到非负象限的投影用 $\\Pi_+(y)$ 表示，其按分量定义为 $(\\Pi_+(y))_i = \\max(y_i, 0)$；设按分量的软阈值算子用 $\\mathrm{soft}_\\alpha(y)$ 表示，其定义为 $(\\mathrm{soft}_\\alpha(y))_i = \\mathrm{sign}(y_i) \\max(|y_i| - \\alpha, 0)$。Iterative Soft-Thresholding Algorithm (ISTA) 执行形如 $$x^{k+1} \\leftarrow \\mathcal{T}\\big(x^k\\big)$$ 的迭代更新，该更新基于对光滑项 $\\tfrac{1}{2}\\|A x - b\\|_2^2$ 的梯度步（步长为 $\\tau  0$），随后是对非光滑项 $\\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x)$ 的近端步。\n\n您的任务是：\n\n- 从近端算子、到凸集上的投影和软阈值的定义出发，仅使用凸分析的基本原理和各项的可分离性，推导和 $\\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x)$ 的精确近端算子。\n- 定义并实现两种 ISTA 变体：\n  1. 一种“连续投影”变体，使用 $$x^{k+1} \\leftarrow \\Pi_+\\Big(\\mathrm{soft}_{\\tau \\lambda}\\big(x^k - \\tau A^\\top (A x^k - b)\\big)\\Big)。$$\n  2. 一种“精确近端”变体，使用您推导中计算出的和的精确近端算子，并将其应用于梯度步点 $x^k - \\tau A^\\top (A x^k - b)$。\n- 使用由 $A$ 的谱范数确定的步长 $\\tau$，具体为 $$\\tau = \\frac{c}{\\|A\\|_2^2}$$，对于给定的标量 $c \\in (0, 1]$，其中 $\\|A\\|_2$ 表示 $A$ 的算子范数（最大奇异值）。\n- 对所有测试用例，使用 $x^0 = 0$（适当维度的零向量）进行初始化，并为每个用例运行指定的迭代次数。\n\n对于下面指定的每个测试用例，使用相同的参数运行两种 ISTA 变体，并报告标量值 $$d = \\|x_{\\text{succ}} - x_{\\text{exact}}\\|_2,$$ 其中 $x_{\\text{succ}}$ 和 $x_{\\text{exact}}$ 分别是由连续投影变体和精确近端变体生成的最终迭代结果，$\\|\\cdot\\|_2$ 表示欧几里得范数。最终输出应为单行，其中包含所有测试用例的这些标量差值的逗号分隔列表，并用方括号括起来，例如 $$[d_1,d_2,d_3,d_4,d_5].$$ 此问题不涉及任何物理单位或角度。\n\n使用以下测试套件，它涵盖了一般情况、步长边界行为、强稀疏性机制、数据中的负分量以及秩亏前向模型：\n\n- 测试用例 $1$（一般情况）：\n  - $$A_1 = \\begin{bmatrix} 1.0  -0.5  0.3  0.0 \\\\ 0.2  1.2  -0.7  0.5 \\\\ -0.3  0.4  1.0  -0.2 \\\\ 0.0  -0.1  0.2  0.8 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.8 \\\\ 0.3 \\end{bmatrix}, \\quad \\lambda_1 = 0.15, \\quad c_1 = 0.90, \\quad N_1 = 200.$$\n- 测试用例 $2$（边界步长）：\n  - 使用与 $A_1$、$b_1$ 和 $\\lambda_1$ 相同的参数，但 $$c_2 = 1.00, \\quad N_2 = 200.$$\n- 测试用例 $3$（强稀疏性机制）：\n  - 使用与 $A_1$ 和 $b_1$ 相同的参数，但 $$\\lambda_3 = 5.0, \\quad c_3 = 0.90, \\quad N_3 = 50.$$\n- 测试用例 $4$（数据中的负分量和矩形 $A$）：\n  - $$A_4 = \\begin{bmatrix} 0.9  -0.2  0.0 \\\\ -0.1  0.7  0.5 \\\\ 0.3  0.0  -0.8 \\\\ 0.0  0.2  0.9 \\\\ -0.4  0.1  0.3 \\end{bmatrix}, \\quad b_4 = \\begin{bmatrix} -0.5 \\\\ 0.2 \\\\ -1.0 \\\\ 0.7 \\\\ 0.0 \\end{bmatrix}, \\quad \\lambda_4 = 0.2, \\quad c_4 = 0.95, \\quad N_4 = 150.$$\n- 测试用例 $5$（秩亏的 $A$）：\n  - $$A_5 = \\begin{bmatrix} 1.0  0.5  1.0  -0.2 \\\\ -0.3  0.1  -0.3  0.4 \\\\ 0.7  -0.2  0.7  0.0 \\\\ 0.0  0.3  0.0  0.6 \\\\ -0.5  0.4  -0.5  -0.1 \\\\ 0.2  -0.1  0.2  0.8 \\end{bmatrix}, \\quad b_5 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.9 \\\\ 0.3 \\\\ -0.4 \\\\ 0.0 \\end{bmatrix}, \\quad \\lambda_5 = 0.12, \\quad c_5 = 0.99, \\quad N_5 = 250.$$\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表（例如，$$[d_1,d_2,d_3,d_4,d_5]$$），其中每个 $d_i$ 是一个实数（浮点数），代表上述为测试用例 $i$ 定义的欧几里得范数差。", "solution": "该问题在科学上基于凸优化的原理，特别是近端梯度法，并且在数学上是适定的和自洽的。\n\n问题的核心是比较用于解决非负稀疏回归问题的 Iterative Soft-Thresholding Algorithm (ISTA) 的两种变体：\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) = \\underbrace{\\frac{1}{2}\\|A x - b\\|_2^2}_{f(x)} + \\underbrace{\\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x)}_{g(x)}\n$$\n其中 $f(x)$ 是一个光滑凸函数，$g(x)$ 是一个非光滑凸函数。ISTA 更新是一个形如以下的近端梯度步：\n$$\nx^{k+1} \\leftarrow \\mathrm{prox}_{\\tau g}\\left(x^k - \\tau \\nabla f(x^k)\\right)\n$$\n此处，$\\nabla f(x) = A^\\top(Ax-b)$ 是光滑项的梯度，$\\tau  0$ 是步长。点 $y^k = x^k - \\tau A^\\top(Ax^k-b)$ 是对 $f(x)$ 进行梯度下降步的结果。通过将 $\\tau g$ 的近端算子应用于 $y^k$ 来完成更新。\n\n第一个任务是推导 $g(x) = \\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x)$ 的精确近端算子。\n\n**1. 精确近端算子的推导**\n\n根据定义，函数 $\\phi(x)$ 的近端算子由 $\\mathrm{prox}_{\\phi}(y) = \\arg\\min_x \\left( \\phi(x) + \\frac{1}{2}\\|x-y\\|_2^2 \\right)$ 给出。我们的任务是找到 $\\mathrm{prox}_{\\tau g}(y)$，即：\n$$\nx^* = \\mathrm{prox}_{\\tau g}(y) = \\arg\\min_x \\left( \\tau g(x) + \\frac{1}{2}\\|x-y\\|_2^2 \\right)\n$$\n代入 $g(x) = \\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x)$：\n$$\nx^* = \\arg\\min_x \\left( \\tau \\left( \\lambda \\|x\\|_1 + \\iota_{\\mathbb{R}_+^n}(x) \\right) + \\frac{1}{2}\\|x-y\\|_2^2 \\right)\n$$\n由于 $\\tau  0$，项 $\\tau \\iota_{\\mathbb{R}_+^n}(x)$ 等价于 $\\iota_{\\mathbb{R}_+^n}(x)$，这将解 $x$ 约束在非负象限 $\\mathbb{R}_+^n$ 中（即，对所有 $i$ 都有 $x_i \\ge 0$）。对于任何 $x \\in \\mathbb{R}_+^n$，$\\iota_{\\mathbb{R}_+^n}(x) = 0$。因此，该最小化问题等价于：\n$$\nx^* = \\arg\\min_{x \\ge 0} \\left( \\tau \\lambda \\|x\\|_1 + \\frac{1}{2}\\|x-y\\|_2^2 \\right)\n$$\n对于 $x \\ge 0$，L1 范数简化为 $\\|x\\|_1 = \\sum_{i=1}^n x_i$。目标函数关于分量 $x_i$ 是可加分离的：\n$$\nL(x) = \\sum_{i=1}^n \\left( \\tau \\lambda x_i + \\frac{1}{2}(x_i - y_i)^2 \\right)\n$$\n因此，我们可以通过求解 $n$ 个独立的标量最小化问题来找到最小值：\n$$\nx_i^* = \\arg\\min_{z \\ge 0} \\left( \\tau \\lambda z + \\frac{1}{2}(z - y_i)^2 \\right) \\quad \\text{for } i = 1, \\dots, n\n$$\n设 $h(z) = \\frac{1}{2}(z - y_i)^2 + \\tau \\lambda z$。这是一个凸二次函数。其无约束最小值可以通过将其导数设为零来找到：\n$$\nh'(z) = (z - y_i) + \\tau \\lambda = 0 \\implies z_{\\text{unc}} = y_i - \\tau \\lambda\n$$\n约束问题（$z \\ge 0$）的解是无约束最小化子 $z_{\\text{unc}}$ 在非负实数轴 $[0, \\infty)$ 上的投影。这个投影就是 $\\max(0, z_{\\text{unc}})$。\n因此，精确近端算子的第 $i$ 个分量是：\n$$\nx_i^* = \\max(0, y_i - \\tau\\lambda)\n$$\n这就为 ISTA 更新定义了“精确近端”算子。\n\n**2. “连续投影”算子的分析**\n\n问题将“连续投影”变体定义为使用算子 $T_{\\text{succ}}(y) = \\Pi_+\\Big(\\mathrm{soft}_{\\tau \\lambda}(y)\\Big)$。让我们按分量分析这个算子：\n$$\n(T_{\\text{succ}}(y))_i = \\max\\left(0, (\\mathrm{soft}_{\\tau \\lambda}(y))_i\\right)\n$$\n其中 $(\\mathrm{soft}_{\\alpha}(y))_i = \\mathrm{sign}(y_i) \\max(|y_i|-\\alpha, 0)$。我们考虑 $y_i$ 的值的三个情况：\n- 情况 1：$y_i  \\tau\\lambda$。\n  在这种情况下，$(\\mathrm{soft}_{\\tau\\lambda}(y))_i = \\mathrm{sign}(y_i)\\max(y_i - \\tau\\lambda, 0) = 1 \\cdot (y_i - \\tau\\lambda) = y_i - \\tau\\lambda$。由于 $y_i - \\tau\\lambda  0$，我们有 $(T_{\\text{succ}}(y))_i = \\max(0, y_i - \\tau\\lambda) = y_i - \\tau\\lambda$。\n- 情况 2：$0 \\le y_i \\le \\tau\\lambda$。\n  在这种情况下，$|y_i| - \\tau\\lambda \\le 0$，所以 $(\\mathrm{soft}_{\\tau\\lambda}(y))_i = \\mathrm{sign}(y_i)\\max(0, 0) = 0$。那么，$(T_{\\text{succ}}(y))_i = \\max(0, 0) = 0$。\n- 情况 3：$y_i  0$。\n  在这种情况下，$(\\mathrm{soft}_{\\tau\\lambda}(y))_i = -1 \\cdot \\max(-y_i - \\tau\\lambda, 0)$。这个值是非正的。因此，$(T_{\\text{succ}}(y))_i = \\max(0, \\text{非正值}) = 0$。\n\n综合这些情况，“连续投影”算子由下式给出：\n$$\n(T_{\\text{succ}}(y))_i = \\begin{cases} y_i - \\tau\\lambda,  \\text{if } y_i  \\tau\\lambda \\\\ 0,  \\text{if } y_i \\le \\tau\\lambda \\end{cases}\n$$\n这恰好是函数 $\\max(0, y_i - \\tau\\lambda)$。\n\n**3. 等价性结论**\n\n第 1 节的推导表明，精确近端算子是 $(\\mathrm{prox}_{\\tau g}(y))_i = \\max(0, y_i - \\tau\\lambda)$。第 2 节的分析表明，“连续投影”算子是 $(T_{\\text{succ}}(y))_i = \\max(0, y_i - \\tau\\lambda)$。\n\n这两个算子在数学上是相同的。两种 ISTA 变体是：\n1. $x^{k+1}_{\\text{succ}} \\leftarrow T_{\\text{succ}}(x^k - \\tau A^\\top(A x^k - b))$\n2. $x^{k+1}_{\\text{exact}} \\leftarrow \\mathrm{prox}_{\\tau g}(x^k - \\tau A^\\top(A x^k - b))$\n\n鉴于算子是相同的，并且两种算法都从相同的初始向量 $x^0 = 0$ 开始，通过归纳法可得，对于所有迭代 $k \\ge 0$ 都有 $x^k_{\\text{succ}} = x^k_{\\text{exact}}$。因此，最终的迭代结果将是相同的，并且它们差的欧几里得范数 $d = \\|x_{\\text{succ}} - x_{\\text{exact}}\\|_2$ 对所有测试用例都必须为 $0$，除非存在由不同计算路径引起的微小浮点差异。实现将证实这一分析结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_ista_comparison(A, b, lambda_val, c, N):\n    \"\"\"\n    Runs the two specified ISTA variants and computes the norm of the difference.\n\n    Args:\n        A (np.ndarray): The matrix A.\n        b (np.ndarray): The vector b.\n        lambda_val (float): The L1 regularization parameter.\n        c (float): The step size scaling factor.\n        N (int): The number of iterations.\n\n    Returns:\n        float: The Euclidean norm of the difference between the final iterates.\n    \"\"\"\n    m, n = A.shape\n\n    # Calculate the step size τ based on the spectral norm of A.\n    # The largest singular value of A is ||A||_2.\n    singular_values = np.linalg.svd(A, compute_uv=False)\n    spectral_norm_sq = singular_values[0]**2\n    if spectral_norm_sq == 0:\n        # Handle the case of a zero matrix, though unlikely in tests.\n        tau = 1.0\n    else:\n        tau = c / spectral_norm_sq\n    \n    # Pre-calculate the thresholding parameter for efficiency.\n    t_lambda = tau * lambda_val\n\n    # Initialize iterates for both variants.\n    x_succ = np.zeros(n)\n    x_exact = np.zeros(n)\n\n    # The \"successive projections\" operator: Π+(soft_{τλ}(y))\n    def operator_succ(y, threshold):\n        return np.maximum(0., np.sign(y) * np.maximum(np.abs(y) - threshold, 0.))\n\n    # The \"exact proximal\" operator: max(0, y - τλ)\n    def operator_exact(y, threshold):\n        return np.maximum(0., y - threshold)\n\n    # Perform N iterations for both algorithms simultaneously.\n    for _ in range(N):\n        # --- Successive Projections Variant ---\n        # Gradient step\n        grad_succ = A.T @ (A @ x_succ - b)\n        y_succ = x_succ - tau * grad_succ\n        # Proximal step\n        x_succ = operator_succ(y_succ, t_lambda)\n        \n        # --- Exact Proximal Variant ---\n        # Gradient step\n        grad_exact = A.T @ (A @ x_exact - b)\n        y_exact = x_exact - tau * grad_exact\n        # Proximal step\n        x_exact = operator_exact(y_exact, t_lambda)\n\n    # Calculate the Euclidean norm of the difference between final iterates.\n    d = np.linalg.norm(x_succ - x_exact)\n    return d\n\ndef solve():\n    \"\"\"\n    Defines the test cases, runs the ISTA comparison for each, and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    A1 = np.array([[1.0, -0.5, 0.3, 0.0], [0.2, 1.2, -0.7, 0.5], [-0.3, 0.4, 1.0, -0.2], [0.0, -0.1, 0.2, 0.8]])\n    b1 = np.array([0.5, -1.0, 0.8, 0.3])\n    lambda1 = 0.15\n\n    A4 = np.array([[0.9, -0.2, 0.0], [-0.1, 0.7, 0.5], [0.3, 0.0, -0.8], [0.0, 0.2, 0.9], [-0.4, 0.1, 0.3]])\n    b4 = np.array([-0.5, 0.2, -1.0, 0.7, 0.0])\n\n    A5 = np.array([[1.0, 0.5, 1.0, -0.2], [-0.3, 0.1, -0.3, 0.4], [0.7, -0.2, 0.7, 0.0], [0.0, 0.3, 0.0, 0.6], [-0.5, 0.4, -0.5, -0.1], [0.2, -0.1, 0.2, 0.8]])\n    b5 = np.array([0.1, -0.2, 0.9, 0.3, -0.4, 0.0])\n    \n    test_cases = [\n        # Test Case 1 (general case)\n        {'A': A1, 'b': b1, 'lambda_val': lambda1, 'c': 0.90, 'N': 200},\n        # Test Case 2 (boundary step size)\n        {'A': A1, 'b': b1, 'lambda_val': lambda1, 'c': 1.00, 'N': 200},\n        # Test Case 3 (strong sparsity regime)\n        {'A': A1, 'b': b1, 'lambda_val': 5.0, 'c': 0.90, 'N': 50},\n        # Test Case 4 (negative components in data and rectangular A)\n        {'A': A4, 'b': b4, 'lambda_val': 0.2, 'c': 0.95, 'N': 150},\n        # Test Case 5 (rank-deficient A)\n        {'A': A5, 'b': b5, 'lambda_val': 0.12, 'c': 0.99, 'N': 250},\n    ]\n\n    results = []\n    for case in test_cases:\n        d = run_ista_comparison(case['A'], case['b'], case['lambda_val'], case['c'], case['N'])\n        results.append(d)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3392928"}]}