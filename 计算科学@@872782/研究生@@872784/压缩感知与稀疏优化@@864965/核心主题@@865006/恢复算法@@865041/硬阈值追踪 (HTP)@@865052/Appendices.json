{"hands_on_practices": [{"introduction": "硬阈值追踪（HTP）算法在其核心部分包含一个梯度下降步骤，该步骤的稳定性与收敛性在很大程度上取决于步长的选择。一个过大的步长可能导致算法发散，而一个过小的步长则会减慢收敛速度。此练习旨在通过计算目标函数梯度的 Lipschitz 常数 $L$，为选择一个安全的步长提供理论依据，这对于确保算法的稳定表现至关重要 [@problem_id:3450360]。", "problem": "考虑压缩感知中的最小二乘目标函数 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{3 \\times 2}$ 且 $y \\in \\mathbb{R}^{3}$。设感知矩阵为\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}.\n$$\n从梯度利普希茨常数的定义出发，计算最小的常数 $L$，使得梯度 $\\nabla f(x)$ 对所有 $x, z \\in \\mathbb{R}^{2}$ 满足不等式 $\\|\\nabla f(x) - \\nabla f(z)\\|_{2} \\le L \\|x - z\\|_{2}$。然后，解释此常数在硬阈值追踪（Hard Thresholding Pursuit, HTP）算法中选择安全梯度步长的作用，其中 HTP 表示硬阈值追踪。给出 $L$ 的精确值作为你的最终答案。无需四舍五入，也无需单位。", "solution": "我们从梯度利普希茨常数的定义开始。如果一个连续可微函数 $f$ 的梯度是 $L$-利普希茨连续的，那么\n$$\n\\|\\nabla f(x) - \\nabla f(z)\\|_{2} \\le L \\|x - z\\|_{2} \\quad \\text{对所有 } x, z.\n$$\n对于二次函数 $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$，其梯度为\n$$\n\\nabla f(x) = A^{\\top}(A x - y).\n$$\n因此，对于任意 $x, z \\in \\mathbb{R}^{2}$，\n$$\n\\nabla f(x) - \\nabla f(z) = A^{\\top}A(x - z).\n$$\n由此可知，最小的可取 $L$ 值是 $A^{\\top}A$ 在欧几里得范数下的算子范数，即\n$$\nL = \\|A^{\\top}A\\|_{2}.\n$$\n由于 $A^{\\top}A$ 是对称半正定的，其算子范数等于其最大特征值。此外，根据奇异值分解，有 $\\|A^{\\top}A\\|_{2} = \\|A\\|_{2}^{2}$。\n\n我们现在显式地计算 $A^{\\top}A$。给定\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix},\n$$\n我们得到\n$$\nA^{\\top}A = \n\\begin{pmatrix}\n1  0  1 \\\\\n1  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix}.\n$$\n矩阵 $\\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix}$ 的特征值可以通过求解下式获得\n$$\n\\det\\!\\left(\\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix} - \\lambda I\\right) = 0,\n$$\n即\n$$\n\\det\\!\\begin{pmatrix}2 - \\lambda  1 \\\\ 1  2 - \\lambda\\end{pmatrix}\n= (2 - \\lambda)^{2} - 1 = \\lambda^{2} - 4\\lambda + 3 = 0.\n$$\n其根为 $\\lambda = 1$ 和 $\\lambda = 3$。因此，\n$$\n\\|A^{\\top}A\\|_{2} = \\lambda_{\\max}(A^{\\top}A) = 3,\n$$\n所以\n$$\nL = \\|A^{\\top}A\\|_{2} = \\|A\\|_{2}^{2} = 3.\n$$\n\n现在我们来解释 $L$ 在硬阈值追踪（Hard Thresholding Pursuit, HTP）算法中选择安全梯度步长的作用。在 HTP 中，首先执行一个中间梯度步\n$$\nx^{t + \\frac{1}{2}} = x^{t} - \\mu \\nabla f(x^{t}) = x^{t} - \\mu A^{\\top}(A x^{t} - y),\n$$\n然后进行一个硬阈值操作，以强制施加 $k$-稀疏性，并对所选的支撑集进行受限最小二乘精化。常数 $L$ 控制了 $f$ 的光滑性，并且根据具有 $L$-利普希茨梯度的函数的下降引理，\n$$\nf\\!\\left(x - \\mu \\nabla f(x)\\right) \\le f(x) - \\left(\\mu - \\frac{L \\mu^{2}}{2}\\right)\\|\\nabla f(x)\\|_{2}^{2},\n$$\n这保证了只要 $0  \\mu \\le \\frac{2}{L}$，$f$ 就会单调递减。在 HTP 和迭代硬阈值（Iterative Hard Thresholding）算法中，一个通常采用的保守选择是\n$$\n\\mu \\le \\frac{1}{L} = \\frac{1}{\\|A\\|_{2}^{2}},\n$$\n这确保了二次代理函数在 $x$ 处是 $f$ 的一个上界，并在阈值化和支撑集精化之前产生一个安全的步长。对于给定的矩阵，安全通用选择是 $\\mu \\le \\frac{1}{3}$，其中规范选择为 $\\mu = \\frac{1}{3}$。", "answer": "$$\\boxed{3}$$", "id": "3450360"}, {"introduction": "除了理论上的收敛性保证，一个算法的实用性还取决于其计算效率。此练习将引导你分解 HTP 的单次迭代过程，并量化其计算复杂度，这对于评估算法在实际应用中的可行性至关重要。通过分析在稠密测量矩阵和具有快速变换的结构化矩阵这两种不同情景下的成本，你将能更深刻地理解算法设计如何影响大规模问题的求解效率 [@problem_id:3450349]。", "problem": "考虑线性测量模型 $y = A x^{\\star} + w$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x^{\\star} \\in \\mathbb{R}^{n}$ 是 $k$-稀疏的，且 $w \\in \\mathbb{R}^{m}$。Hard Thresholding Pursuit (HTP) 算法在第 $t$ 次迭代时通过以下步骤寻求一个 $k$-稀疏估计 $x^{t}$：计算残差 $r^{t} = y - A x^{t}$，形成梯度 $g^{t} = A^{\\top} r^{t}$，构建代理信号 $p^{t} = x^{t} + g^{t}$，选择由 $|p^{t}|$ 的 $k$ 个最大分量的索引给出的支撑集 $S^{t+1}$，并通过求解 $\\min_{z \\in \\mathbb{R}^{k}} \\| y - A_{S^{t+1}} z \\|_{2}^{2}$ 来执行约束最小二乘更新，其中 $A_{S^{t+1}} \\in \\mathbb{R}^{m \\times k}$ 收集了由 $S^{t+1}$ 索引的 $A$ 的列，并设置 $(x^{t+1})_{S^{t+1}} = z$ 和 $(x^{t+1})_{(S^{t+1})^{c}} = 0$。假设在每次迭代中 $x^{t}$ 都是 $k$-稀疏的，并且算术成本遵循标准的稠密线性代数模型：将一个 $m \\times n$ 的稠密矩阵与一个稠密向量相乘的成本为 $\\mathcal{O}(m n)$ 次操作，将 $A$ 与一个 $k$-稀疏向量相乘的成本为 $\\mathcal{O}(m k)$ 次操作，使用部分选择法选出一个 $n$-维向量中最大的 $k$ 个条目的成本为 $\\mathcal{O}(n)$ 次操作，通过数值稳定的方法（例如，$QR$ 分解）为最小二乘问题分解一个 $m \\times k$ 矩阵的成本为 $\\mathcal{O}(m k^{2})$ 次操作，之后进行三角求解还需额外的 $\\mathcal{O}(k^{2})$ 次操作。\n\n基于这些基础和 HTP 的迭代结构，推导在以下两种情况下，每次迭代的渐近计算复杂度，并以 $m$、$n$ 和 $k$ 的函数的大-$\\mathcal{O}$ 记号表示：\n1. $A$ 是稠密矩阵，没有可利用的结构。\n2. $A$ 表示一个支持快速乘法（例如，部分傅里叶或哈达玛变换）的子采样酉变换，因此对于任何 $v \\in \\mathbb{R}^{n}$，$A v$ 和 $A^{\\top} v$ 均可在 $\\mathcal{O}(n \\log_{2}(n))$ 时间内计算，而对 $A_{S}$ 的受限最小二乘则使用标准的稠密方法执行。\n\n在每种情况下，汇总一次完整 HTP 迭代中的主要成本，通过舍去在渐近意义上不影响主导复杂度的低阶项进行简化，并以大-$\\mathcal{O}$ 表达式的形式给出两种情况下最终的每次迭代复杂度。你的最终答案必须包含这两个大-$\\mathcal{O}$ 表达式，并使用 LaTeX 的 $\\mathrm{pmatrix}$ 环境排列成一个单行矩阵。无需四舍五入，也不涉及物理单位。", "solution": "本任务旨在推导在关于测量矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的两种不同假设下，Hard Thresholding Pursuit (HTP) 算法的每次迭代计算复杂度。我们将通过把单次 HTP 迭代分解为其构成步骤，并以维度 $m$、$n$ 和稀疏度 $k$ 的函数的大-$\\mathcal{O}$ 记号形式，对其渐近成本求和来进行分析。\n\nHTP 迭代通过以下主要计算步骤从估计 $x^t$ 进行到 $x^{t+1}$：\n1.  计算残差：$r^{t} = y - A x^{t}$\n2.  形成梯度等效项：$g^{t} = A^{\\top} r^{t}$\n3.  构建代理信号：$p^{t} = x^{t} + g^{t}$\n4.  确定新支撑集：$S^{t+1}$ = $p^{t}$ 的 $k$ 个最大幅值分量的索引\n5.  为更新求解约束最小二乘问题：$(x^{t+1})_{S^{t+1}} = \\arg\\min_{z \\in \\mathbb{R}^{k}} \\| y - A_{S^{t+1}} z \\|_{2}^{2}$\n\n我们针对这两种指定的情况分析每个步骤的计算成本。\n\n**情况 1：$A$ 是一个稠密矩阵**\n\n在这种情况下，$A$ 是一个没有特殊结构的一般 $m \\times n$ 稠密矩阵。我们使用所提供的标准计算成本。\n\n步骤 1：计算残差 $r^{t} = y - A x^{t}$。\n向量 $x^{t}$ 是 $k$-稀疏的。一个稠密的 $m \\times n$ 矩阵 $A$ 与一个 $k$-稀疏向量 $x^{t}$ 的乘积，可以通过对 $A$ 中与 $x^{t}$ 的非零项对应的 $k$ 列，按这些项的值进行缩放后求和来计算。此操作成本为 $\\mathcal{O}(m k)$。随后的向量减法 $y - (A x^{t})$ 成本为 $\\mathcal{O}(m)$。因此，此步骤的总成本为 $\\mathcal{O}(m k + m) = \\mathcal{O}(m k)$，因为 $k \\ge 1$。\n\n步骤 2：形成梯度 $g^{t} = A^{\\top} r^{t}$。\n矩阵 $A^{\\top}$ 是一个稠密的 $n \\times m$ 矩阵，残差 $r^{t}$ 是一个稠密的 $m \\times 1$ 向量。这是一个标准的稠密矩阵-向量乘法，成本为 $\\mathcal{O}(n m)$。\n\n步骤 3：构建代理信号 $p^{t} = x^{t} + g^{t}$。\n这涉及到将一个 $k$-稀疏向量 $x^{t}$ 与一个稠密向量 $g^{t}$ 相加，两者的大小均为 $n$。此操作需要遍历稠密向量，成本为 $\\mathcal{O}(n)$。\n\n步骤 4：确定新支撑集 $S^{t+1}$。\n这需要找到 $n$ 维向量 $p^{t}$ 中 $k$ 个绝对值最大分量的索引。使用部分选择算法（例如，introselect），这可以在线性时间内完成。成本为 $\\mathcal{O}(n)$。\n\n步骤 5：执行约束最小二乘更新。\n此步骤涉及求解最小二乘问题 $\\min_{z} \\| y - A_{S^{t+1}} z \\|_{2}^{2}$。首先，必须通过提取由 $S^{t+1}$ 索引的 $A$ 的 $k$ 列来形成 $m \\times k$ 的子矩阵 $A_{S^{t+1}}$。这耗费 $\\mathcal{O}(m k)$。然后，对于这个 $m \\times k$ 系统，使用 QR 分解求解最小二乘问题的成本被给定为 $\\mathcal{O}(m k^{2})$。此步骤的总成本为 $\\mathcal{O}(m k + m k^{2}) = \\mathcal{O}(m k^{2})$，因为 $k \\ge 1$。\n\n情况 1 的总复杂度：\n每次迭代的总复杂度是这些步骤的成本之和：\n$$ \\mathcal{O}(m k) + \\mathcal{O}(n m) + \\mathcal{O}(n) + \\mathcal{O}(n) + \\mathcal{O}(m k^{2}) = \\mathcal{O}(n m + m k^{2} + m k + n) $$\n在典型的高维设置中，即 $k \\ll m  n$，$\\mathcal{O}(mk)$ 和 $\\mathcal{O}(n)$ 项分别被 $\\mathcal{O}(mk^2)$（因为 $k \\ge 1$）和 $\\mathcal{O}(nm)$（因为 $m \\ge 1$）主导。主导项是 $\\mathcal{O}(nm)$ 和 $\\mathcal{O}(mk^2)$。由于 $n$ 和 $k^2$ 之间的渐近关系未指定，因此必须保留这两个项。最终复杂度为 $\\mathcal{O}(m n + m k^{2})$。\n\n**情况 2：$A$ 是一个具有快速乘法的子采样酉变换**\n\n在这种情况下，应用 $A$ 或 $A^{\\top}$ 的成本为 $\\mathcal{O}(n \\log_{2}(n))$。最小二乘子问题仍使用稠密方法求解。\n\n步骤 1：计算残差 $r^{t} = y - A x^{t}$。\n将算子 $A$ 应用于任何向量的成本被给定为 $\\mathcal{O}(n \\log_{2}(n))$。尽管 $x^{t}$ 是 $k$-稀疏的，但像 FFT 这样的快速变换通常不会对任意稀疏输入提供计算上的简化。因此，计算 $A x^{t}$ 的成本是 $\\mathcal{O}(n \\log_{2}(n))$。向量减法的成本为 $\\mathcal{O}(m)$，是次要的。总成本为 $\\mathcal{O}(n \\log_{2}(n))$。\n\n步骤 2：形成梯度 $g^{t} = A^{\\top} r^{t}$。\n向量 $r^{t} \\in \\mathbb{R}^{m}$ 是稠密的。应用伴随算子 $A^{\\top}$ 的成本也被给定为 $\\mathcal{O}(n \\log_{2}(n))$。\n\n步骤 3：构建代理信号 $p^{t} = x^{t} + g^{t}$。\n与情况 1 中一样，这是一个向量加法，成本为 $\\mathcal{O}(n)$。\n\n步骤 4：确定新支撑集 $S^{t+1}$。\n与情况 1 中一样，这耗费 $\\mathcal{O}(n)$。\n\n步骤 5：执行约束最小二乘更新。\n问题陈述此步骤是使用“标准的稠密方法”执行的。这需要显式地构建 $m \\times k$ 矩阵 $A_{S^{t+1}}$。对于像子采样傅里叶或哈达玛矩阵这样的结构化矩阵，任何元素 $A_{ij}$ 都可以在 $\\mathcal{O}(1)$ 时间内计算。因此，整个 $m \\times k$ 矩阵 $A_{S^{t+1}}$ 可以在 $\\mathcal{O}(m k)$ 时间内构建。随后，通过 QR 分解求解最小二乘问题的成本为 $\\mathcal{O}(m k^{2})$。此步骤的总成本为 $\\mathcal{O}(m k + m k^{2}) = \\mathcal{O}(m k^{2})$。\n\n情况 2 的总复杂度：\n每次迭代的总复杂度是各项成本之和：\n$$ \\mathcal{O}(n \\log_{2}(n)) + \\mathcal{O}(n \\log_{2}(n)) + \\mathcal{O}(n) + \\mathcal{O}(n) + \\mathcal{O}(m k^{2}) = \\mathcal{O}(n \\log_{2}(n) + m k^{2}) $$\n在这里，$\\mathcal{O}(n)$ 项被 $\\mathcal{O}(n \\log_{2}(n))$ 主导。主导项来自快速变换和最小二乘求解。\n\n两种情况下最终的每次迭代复杂度为：\n1.  稠密 $A$：$\\mathcal{O}(m n + m k^{2})$\n2.  快速变换 $A$：$\\mathcal{O}(n \\log_{2}(n) + m k^{2})$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mathcal{O}(m n + m k^{2})  \\mathcal{O}(n \\log_{2}(n) + m k^{2})\n\\end{pmatrix}\n}\n$$", "id": "3450349"}, {"introduction": "对于任何迭代算法而言，确立一个有效的停止准则是其稳健实现的关键一环。过早停止会牺牲解的精度，而迭代次数过多则会浪费宝贵的计算资源。此练习要求你从第一性原理出发，审慎评估几种为 HTP 设计的候选停止准则。通过辨别哪些准则是合理且非退化的，你将能更深入地理解 HTP 的不动点特性以及如何在实践中判断算法是否已充分收敛 [@problem_id:3450363]。", "problem": "考虑压缩感知中的线性模型，其中一个稀疏度为 $\\lVert x_{\\star} \\rVert_{0} \\le k$ 的未知信号 $x_{\\star} \\in \\mathbb{R}^{n}$ 通过 $y = A x_{\\star} + w$ 进行测量，其中 $A \\in \\mathbb{R}^{m \\times n}$，$m  n$，噪声 $w \\in \\mathbb{R}^{m}$。目标是在非凸约束集 $\\{x \\in \\mathbb{R}^{n} : \\lVert x \\rVert_{0} \\le k\\}$ 上最小化最小二乘目标函数 $f(x) = \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2}$。硬阈值追踪（Hard Thresholding Pursuit, HTP）方法通过结合梯度步、对一个势为 $k$ 的支撑集进行硬阈值操作以及在所选支撑集上进行最小二乘重构来进行迭代。在一个迭代点 $x^{t}$，其支撑集为 $S^{t} := \\operatorname{supp}(x^{t})$，定义残差 $r^{t} := y - A x^{t}$ 和梯度 $g^{t} := \\nabla f(x^{t}) = A^{\\top}(A x^{t} - y) = - A^{\\top} r^{t}$。一个标准的HTP步骤是：使用步长 $\\mu^{t} > 0$ 构建一个临时向量 $z^{t} := x^{t} - \\mu^{t} g^{t}$，选择 $z^{t}$ 的 $k$ 个最大绝对值分量的索引集作为下一个支撑集 $S^{t+1}$，然后计算最小二乘重构 $x^{t+1} \\in \\arg\\min\\{\\lVert y - A u \\rVert_{2}^{2} : \\operatorname{supp}(u) \\subseteq S^{t+1}\\}$。假设测量矩阵 $A$ 的列是单位 $\\ell_{2}$ 范数归一化的，并且满足一个合适阶数（例如，阶数至少为 $3k$）的有限等距性质（Restricted Isometry Property, RIP），其常数 $\\delta \\in (0,1)$ 足够小，以至于在无噪声情况 $w = 0$ 下，HTP 收敛到一个 $k$-稀疏稳定点，而在有噪声的情况下，则获得一个噪声感知的近似解。你可以假设在每个选定的支撑集上都进行精确的最小二乘重构。\n\n你的任务是评估一些旨在决定何时在不牺牲解质量的情况下提前终止 HTP 的停止准则。每个提议都用算法过程中可获得的量来表示。仅使用以下基本事实从第一性原理出发进行推理：\n- 最小二乘目标函数的梯度为 $g = \\nabla f(x) = A^{\\top}(A x - y)$。\n- 对于任何固定的支撑集 $S$，精确的最小二乘重构 $x_{S} \\in \\arg\\min\\{\\lVert y - A u \\rVert_{2}^{2} : \\operatorname{supp}(u) \\subseteq S\\}$ 满足正规方程 $A_{S}^{\\top}(y - A x) = 0$，这等价于说支撑集受限梯度为零，即 $(g)_{S} = 0$。\n- 如果 $j \\notin S$ 且 $e_{j}$ 是第 $j$ 个标准基向量，那么 $f$ 在 $x$ 处沿 $e_{j}$ 的方向导数为 $\\langle g, e_{j} \\rangle = g_{j}$。\n\n给你四个候选停止准则，每个准则都在第 $t$ 次迭代结束时进行评估，即在对 $S^{t+1}$ 进行最小二乘重构后，使用 $(x^{t+1}, S^{t+1}, r^{t+1}, g^{t+1})$：\n\nA. 支撑集稳定：如果 $S^{t+1} = S^{t}$ 则停止。\n\nB. 残差范数停滞：如果相对残差下降在若干次连续迭代中一直低于某个容差，则停止。即，存在整数 $L \\ge 1$ 和 $\\epsilon \\in (0,1)$，使得对于所有 $\\ell \\in \\{0,1,\\dots,L-1\\}$，我们有 $\\dfrac{\\lVert r^{t-\\ell} \\rVert_{2} - \\lVert r^{t-\\ell+1} \\rVert_{2}}{\\lVert r^{t-\\ell} \\rVert_{2}} \\le \\epsilon$。\n\nC. 在当前支撑集上的投影梯度范数：如果对于某个固定的 $\\tau \\in (0,1)$，有 $\\lVert (g^{t+1})_{S^{t+1}} \\rVert_{2} \\le \\tau \\lVert g^{t+1} \\rVert_{2}$，则停止。\n\nD. 补集投影梯度筛选：如果 $\\lVert (g^{t+1})_{(S^{t+1})^{\\mathrm{c}}} \\rVert_{\\infty} \\le \\eta \\, \\widehat{\\sigma} \\, \\lVert A \\rVert_{2,\\infty}$，则停止。其中 $(S^{t+1})^{\\mathrm{c}}$ 表示 $S^{t+1}$ 的补集，$\\lVert A \\rVert_{2,\\infty} := \\max_{j \\in [n]} \\lVert A_{:,j} \\rVert_{2}$，$\\widehat{\\sigma}$ 是每次测量的噪声标准差的估计值（例如，来自基于残差的估计器），$\\eta > 0$ 是用户选择的反映显著性水平的常数。\n\n在所述模型和假设下，选项 A、B、C、D 中哪些是硬阈值追踪算法的合理且非退化的停止准则？选择所有适用的选项，并通过从列出的基本事实和 HTP 迭代结构中推导出相关含义来证明你的推理。", "solution": "该问题陈述在稀疏优化和压缩感知领域内是一个有效的表述。我们接下来基于所提供的定义和基本事实来评估硬阈值追踪（HTP）算法的每个提议的停止准则。\n\n鉴于精确最小二乘重构的假设，HTP 算法的一个核心性质由第二个基本事实给出。在第 $t$ 次迭代结束时，新的迭代点 $x^{t+1}$ 计算为 $x^{t+1} \\in \\arg\\min\\{\\lVert y - A u \\rVert_{2}^{2} : \\operatorname{supp}(u) \\subseteq S^{t+1}\\}$。令 $g^{t+1} = \\nabla f(x^{t+1})$ 为这个新迭代点的梯度。这个最小二乘问题的正规方程是 $A_{S^{t+1}}^{\\top}(y - A x^{t+1}) = 0$。这等价于说梯度在支撑集 $S^{t+1}$ 上的限制为零：\n$$\n(g^{t+1})_{S^{t+1}} = 0\n$$\n这一事实对于分析准则 C 和 D 至关重要。\n\n**选项 A 的评估**\n\n准则是如果支撑集稳定下来，即 $S^{t+1} = S^{t}$，则停止。\nHTP 算法将支撑集 $S^{t+1}$ 定义为向量 $z^{t} := x^{t} - \\mu^{t} g^{t}$ 中 $k$ 个最大绝对值分量对应的索引集。迭代点 $x^{t}$ 是通过前一次迭代在支撑集 $S^{t}$ 上进行最小二乘重构得到的。因此，假设 $t \\ge 1$，梯度 $g^{t} = \\nabla f(x^t)$ 满足 $(g^{t})_{S^{t}} = 0$。\n\n因此，$z^{t}$ 的分量为：\n- 对于索引 $i \\in S^{t}$：$(z^{t})_{i} = (x^{t})_{i} - \\mu^{t}(g^{t})_{i} = (x^{t})_{i}$。\n- 对于索引 $j \\notin S^{t}$：$(z^{t})_{j} = (x^{t})_{j} - \\mu^{t}(g^{t})_{j} = 0 - \\mu^{t}(g^{t})_{j} = -\\mu^{t}(g^{t})_{j}$。\n\n条件 $S^{t+1} = S^{t}$ 意味着 $z^{t}$ 的 $k$ 个最大绝对值分量恰好是那些由 $S^{t}$ 索引的分量。这意味着对于任何索引 $i \\in S^{t}$ 和任何索引 $j \\notin S^{t}$，我们必须有 $|(z^{t})_{i}| \\ge |(z^{t})_{j}|$。代入 $z^t$ 各分量的表达式，我们得到：\n$$\n|(x^{t})_{i}| \\ge |-\\mu^{t}(g^{t})_{j}| = \\mu^{t}|(g^{t})_{j}|\n$$\n这必须对所有 $i \\in S^{t}$ 和 $j \\notin S^{t}$ 成立。这等价于条件：\n$$\n\\min_{i \\in S^{t}} |(x^{t})_{i}| \\ge \\mu^{t} \\max_{j \\notin S^{t}} |(g^{t})_{j}|\n$$\n这是追踪算法的一个标准不动点条件。它表示算法已达到一个状态，其中当前支撑集上的最小系数（在绝对值上，按 $1/\\mu^t$ 缩放后）大于支撑集外的最大梯度分量。这表明没有任何支撑集外的元素具有足够大的梯度来证明应将其包含到支撑集中。当这种情况发生时，算法已经收敛到一个稳定点。如果 $S^{t+1} = S^t$，那么 $x^{t+1}$ 是在 $S^t$ 上的最小二乘解。但 $x^t$ 也是在 $S^t$ 上的最小二乘解。根据最小二乘解的唯一性（由 RIP 假设保证），$x^{t+1}=x^t$。迭代序列变为常数。因此，这是一个合理且非退化的停止准则。\n\n对 A 的结论：**正确**。\n\n**选项 B 的评估**\n\n准则是如果残差范数的相对减小在 $L$ 次连续迭代中保持在容差 $\\epsilon$ 以下，则停止：$\\frac{\\lVert r^{t-\\ell} \\rVert_{2} - \\lVert r^{t-\\ell+1} \\rVert_{2}}{\\lVert r^{t-\\ell} \\rVert_{2}} \\le \\epsilon$ 对 $\\ell \\in \\{0, \\dots, L-1\\}$ 成立。\n目标函数是 $f(x) = \\frac{1}{2} \\lVert y - Ax \\rVert_{2}^{2} = \\frac{1}{2} \\lVert r \\rVert_{2}^{2}$。HTP 是一种下降方法，意味着目标函数值在每次迭代中都不会增加，即 $f(x^{t+1}) \\le f(x^t)$，这也意味着 $\\lVert r^{t+1} \\rVert_2 \\le \\lVert r^t \\rVert_2$。\n基于目标函数值（或像残差范数这样的相关量）停滞的停止准则是所有迭代优化算法的标准做法。它表明算法在最小化目标函数方面不再取得有意义的进展。这可能发生在迭代点非常接近局部最小值时，或者当它在一个平坦的目标函数景观区域中进展非常缓慢时。\n在存在噪声 $w$ 的情况下，残差范数不能期望变为零。它通常会收敛到一个与噪声水平 $\\lVert w \\rVert_2$ 成比例的值。一旦算法识别出正确的支撑集并对信号有了很好的估计，残差范数将在这个理论极限附近停滞。因此，监控这种停滞是终止算法并防止其在收益递减的情况下进行过多迭代的一种实用而有效的方法。使用 $L$ 次迭代的窗口使该准则对偶然的小步长具有鲁棒性。这个准则是合理且非退化的。\n\n对 B 的结论：**正确**。\n\n**选项 C 的评估**\n\n准则是如果对于某个固定的 $\\tau \\in (0,1)$，有 $\\lVert (g^{t+1})_{S^{t+1}} \\rVert_{2} \\le \\tau \\lVert g^{t+1} \\rVert_{2}$，则停止。\n该准则是使用在支撑集 $S^{t+1}$ 上产生 $x^{t+1}$ 的最小二乘重构步骤后可用的量来评估的。根据问题中的基本事实，对支撑集 $S$ 进行精确的最小二乘重构会得到一个迭代点 $x$，其梯度 $g = \\nabla f(x)$ 在该支撑集上的分量为零，即 $(g)_{S} = 0$。\n将此应用于 HTP 迭代，我们有 $(g^{t+1})_{S^{t+1}} = 0$。\n因此，不等式的左侧总是零：\n$$\n\\lVert (g^{t+1})_{S^{t+1}} \\rVert_{2} = \\lVert 0 \\rVert_{2} = 0\n$$\n因此，停止准则简化为：\n$$\n0 \\le \\tau \\lVert g^{t+1} \\rVert_{2}\n$$\n由于 $\\tau \\in (0,1)$ 是严格为正的，并且范数 $\\lVert g^{t+1} \\rVert_{2}$ 总是非负的，这个不等式总是成立的。唯一不严格成立的情况是当 $g^{t+1}=0$ 时，此时为 $0 \\le 0$，也成立。\n因此，这个条件在第一次迭代时就会满足（只要 $x^1$ 不是无约束全局最小值，即 $g^1=0$），导致算法立即终止。这不是一个有意义的收敛度量。该准则是退化的，因为它在 HTP 算法的每次迭代中都因其构造而满足。\n\n对 C 的结论：**不正确**。\n\n**选项 D 的评估**\n\n准则是如果 $\\lVert (g^{t+1})_{(S^{t+1})^{\\mathrm{c}}} \\rVert_{\\infty} \\le \\eta \\, \\widehat{\\sigma} \\, \\lVert A \\rVert_{2,\\infty}$，则停止。\n根据问题陈述，矩阵 $A$ 的列是单位 $\\ell_{2}$ 范数归一化的，这意味着对所有 $j$ 都有 $\\lVert A_{:,j} \\rVert_{2}=1$。因此，$\\lVert A \\rVert_{2,\\infty} = \\max_{j} \\lVert A_{:,j} \\rVert_{2} = 1$。该准则简化为：\n$$\n\\max_{j \\notin S^{t+1}} |(g^{t+1})_{j}| \\le \\eta \\, \\widehat{\\sigma}\n$$\n梯度为 $g^{t+1} = A^\\top(A x^{t+1}-y)$。代入 $y=Ax_\\star+w$，我们得到 $g^{t+1} = A^\\top(A(x^{t+1}-x_\\star) - w)$。当迭代点 $x^{t+1}$ 是真实信号 $x_\\star$ 的一个良好近似时，项 $A(x^{t+1}-x_\\star)$ 很小，梯度可以近似为 $g^{t+1} \\approx -A^\\top w$。第 $j$ 个分量是 $(g^{t+1})_j \\approx -A_{:,j}^\\top w$。如果噪声向量 $w$ 由均值为 0、标准差为 $\\sigma$ 的独立同分布分量组成，那么 $(g^{t+1})_j$ 是一个标准差为 $\\sigma \\lVert A_{:,j} \\rVert_{2} = \\sigma$ 的随机变量。\n这个停止准则将当前支撑集外的最大梯度幅值 $\\lVert (g^{t+1})_{(S^{t+1})^{\\mathrm{c}}} \\rVert_{\\infty}$ 与一个与估计噪声水平 $\\widehat{\\sigma}$ 成正比的阈值进行比较。对于 $j \\notin S^{t+1}$，梯度分量 $(g^{t+1})_j$ 代表了将第 $j$ 个原子加入支撑集的边际效益（根据关于方向导数的第三个基本事实）。当最大的此类分量与噪声水平处于同一数量级时，就无法可靠地辨别它反映的是信号的真实部分还是仅仅是噪声的产物。继续迭代以减少这些小的支撑集外梯度分量可能会导致对噪声的拟合，这是不可取的。这类准则在稀疏恢复方法的分析中是标准的（例如，与 Lasso 中正则化参数的选择有关），并提供了一种基于统计原理终止算法的方法。这是一个合理且非退化的准则。\n\n对 D 的结论：**正确**。", "answer": "$$\\boxed{ABD}$$", "id": "3450363"}]}