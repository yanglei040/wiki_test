{"hands_on_practices": [{"introduction": "要想真正掌握归一化迭代硬阈值（NIHT）算法，没有什么比亲手完成一次计算更有效了。本练习 [@problem_id:3438872] 提供了一个具体的小规模算例，旨在揭开算法的神秘面纱。通过手动计算单步的NIHT更新，您将亲身体验其核心机制——自适应步长 $\\mu_t$ 的计算过程，并理解它与简单的固定步长方法的本质区别。", "problem": "考虑压缩感知中的最小二乘目标函数 $f(x) = \\tfrac{1}{2}\\|y - A x\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。迭代硬阈值（Iterative Hard Thresholding, IHT）方法通过交替进行梯度步和硬阈值算子 $H_{k}$ 来寻求一个 $k$-稀疏估计，该算子保留（绝对值）最大的 $k$ 个元素，并将其余元素置零。归一化迭代硬阈值（Normalized Iterative Hard Thresholding, NIHT）变体在每次迭代中，通过在由硬阈值候选支撑集决定的支撑集限制梯度方向上最小化目标函数，来选择一个数据自适应的步长。从最小二乘梯度和硬阈值的基本定义，以及沿搜索方向进行线搜索最小化的原理出发，按如下方式确定一次 NIHT 迭代的自适应步长。\n\n- 设 $A \\in \\mathbb{R}^{3 \\times 5}$，$k = 2$，\n$$\nA = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}, \\quad\nx_{t} = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\n- 定义残差 $r_{t} = y - A x_{t}$ 和梯度 $g_{t} = \\nabla f(x_{t}) = -A^{\\top} r_{t}$。使用最速下降方向 $-g_{t}$ 及其支撑集限制版本，其构造如下：首先形成代理向量 $x_{t} - g_{t}$，通过 $H_{k}$ 确定其 $k$-稀疏支撑集，然后将下降方向限制在该支撑集上（将支撑集外的元素置零）。\n- 通过在上面得到的支撑集限制下降方向 $d_{t}$ 上对 $\\mu \\in \\mathbb{R}$ 最小化 $f(x_{t} + \\mu d_{t})$ 来选择步长 $\\mu_{t}$，然后执行一次 NIHT 更新 $x_{t+1} = H_{k}(x_{t} + \\mu_{t}(-g_{t}))$。\n\n计算此过程所决定的自适应步长 $\\mu_{t}$ 的值。然后执行相应的单次 NIHT 更新。最后，简要评论与使用完整（无限制）梯度方向相比，在步长估计中限制在硬阈值支撑集上如何影响 $\\mu_{t}$ 的大小。\n\n仅报告 $\\mu_{t}$ 的值作为最终答案。将报告的 $\\mu_{t}$ 四舍五入到四位有效数字。", "solution": "该问题定义明确，在稀疏优化领域有科学依据，并为获得唯一解提供了所有必要信息。因此，该问题是有效的。我们按照指定的步骤进行求解。\n\n目标函数是最小二乘损失函数：\n$$\nf(x) = \\frac{1}{2}\\|y - A x\\|_{2}^{2}\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。该函数关于 $x$ 的梯度为：\n$$\n\\nabla f(x) = A^{\\top}(A x - y) = -A^{\\top}(y - A x)\n$$\n\n问题提供以下数据：\n$$\nA = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}, \\quad\nx_{t} = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix}, \\quad k=2.\n$$\n\n首先，我们计算当前迭代点 $x_t$ 处的残差 $r_t$。\n$$\nA x_t = \\begin{pmatrix}\n1  0  0  1  0 \\\\\n0  1  1  0  0 \\\\\n0  0  1  0  1\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\cdot 0 + 0 \\cdot 0.5 + \\dots \\\\ 0 \\cdot 0 + 1 \\cdot 0.5 + \\dots \\\\ 0 \\cdot 0 + 0 \\cdot 0.5 + \\dots \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\end{pmatrix}\n$$\n$$\nr_t = y - A x_t = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1 \\end{pmatrix}\n$$\n\n接下来，我们计算梯度 $g_t = \\nabla f(x_t) = -A^{\\top} r_t$。\n$$\nA^{\\top} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  1  1 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n$$\ng_t = - \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  1  1 \\\\ 1  0  0 \\\\ 0  0  1 \\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1 \\end{pmatrix}\n= - \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 1.5+1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix} -1 \\\\ -1.5 \\\\ -2.5 \\\\ -1 \\\\ -1 \\end{pmatrix}\n$$\n最速下降方向为 $-g_t = \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n\n根据步骤，我们通过沿最速下降方向取单位步长来形成一个代理向量，$z_t = x_t - g_t = x_t + (-g_t)$。\n$$\nz_t = \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n我们应用硬阈值算子 $H_k$（其中 $k=2$）来找到支撑集 $\\Omega_t$。该算子识别 $z_t$ 中 $k=2$ 个最大绝对值元素的索引。这些元素是 $1$，$2$，$2.5$，$1$，$1$。最大的两个是 $2.5$（在索引 $2$ 处）和 $2$（在索引 $1$ 处）。因此，支撑集为 $\\Omega_t = \\{1, 2\\}$（使用0-based索引）。\n\n搜索方向 $d_t$ 是限制在支撑集 $\\Omega_t$ 上的最速下降方向 $-g_t$。\n$$\nd_t = \\begin{pmatrix} 0 \\\\ 1.5 \\\\ 2.5 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n自适应步长 $\\mu_t$ 是通过关于 $\\mu$ 最小化 $f(x_t + \\mu d_t)$ 来选择的。令 $h(\\mu) = f(x_t + \\mu d_t)$。\n$$\nh(\\mu) = \\frac{1}{2} \\|y - A(x_t + \\mu d_t)\\|_2^2 = \\frac{1}{2} \\|(y - A x_t) - \\mu A d_t\\|_2^2 = \\frac{1}{2} \\|r_t - \\mu A d_t\\|_2^2\n$$\n这是一个关于 $\\mu$ 的二次函数。为了找到最小值，我们将其导数设为零：\n$$\n\\frac{dh}{d\\mu} = \\frac{d}{d\\mu} \\left( \\frac{1}{2} (r_t - \\mu A d_t)^\\top (r_t - \\mu A d_t) \\right) = -r_t^\\top A d_t + \\mu (A d_t)^\\top (A d_t) = 0\n$$\n求解 $\\mu_t$：\n$$\n\\mu_t = \\frac{r_t^\\top A d_t}{\\|A d_t\\|_2^2}\n$$\n我们可以简化分子：因为 $g_t = -A^\\top r_t$，我们有 $-g_t = A^\\top r_t$，因此 $r_t^\\top A = (-g_t)^\\top$。所以，$r_t^\\top A d_t = (-g_t)^\\top d_t$。由于 $d_t$ 是 $-g_t$ 在支撑集 $\\Omega_t$ 上的投影，该内积变为 $d_t$ 的平方范数：$(-g_t)^\\top d_t = \\|d_t\\|_2^2$。\n所以，公式为 $\\mu_t = \\frac{\\|d_t\\|_2^2}{\\|A d_t\\|_2^2}$。\n\n我们现在计算分子和分母。\n分子：\n$$\n\\|d_t\\|_2^2 = 0^2 + (1.5)^2 + (2.5)^2 + 0^2 + 0^2 = 2.25 + 6.25 = 8.5\n$$\n分母：\n$$\nA d_t = \\begin{pmatrix} 1  0  0  1  0 \\\\ 0  1  1  0  0 \\\\ 0  0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1.5 \\\\ 2.5 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1.5+2.5 \\\\ 2.5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 4 \\\\ 2.5 \\end{pmatrix}\n$$\n$$\n\\|A d_t\\|_2^2 = 0^2 + 4^2 + (2.5)^2 = 16 + 6.25 = 22.25\n$$\n步长为：\n$$\n\\mu_t = \\frac{8.5}{22.25} = \\frac{850}{2225} = \\frac{34}{89} \\approx 0.38202247...\n$$\n四舍五入到四位有效数字，我们得到 $\\mu_t = 0.3820$。\n\n更新后的估计值为 $x_{t+1} = H_k(x_t + \\mu_t(-g_t))$。\n$$\nx_t + \\mu_t(-g_t) \\approx \\begin{pmatrix} 0 \\\\ 0.5 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + 0.3820 \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.3820 \\\\ 0.5 + 0.573 \\\\ 0.955 \\\\ 0.3820 \\\\ 0.3820 \\end{pmatrix} = \\begin{pmatrix} 0.3820 \\\\ 1.073 \\\\ 0.955 \\\\ 0.3820 \\\\ 0.3820 \\end{pmatrix}\n$$\n应用 $H_2$ 得到 $x_{t+1} \\approx \\begin{pmatrix} 0 \\\\ 1.073 \\\\ 0.955 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n最后，我们评论与使用完整（无限制）梯度方向 $-g_t$ 所获得的步长 $\\mu'_t$ 相比，$\\mu_t$ 的大小。\n完整方向的步长是 $\\mu'_t = \\frac{\\|-g_t\\|_2^2}{\\|A(-g_t)\\|_2^2}$。\n$$\n\\|-g_t\\|_2^2 = 1^2 + (1.5)^2 + (2.5)^2 + 1^2 + 1^2 = 1 + 2.25 + 6.25 + 1 + 1 = 11.5\n$$\n$$\nA(-g_t) = \\begin{pmatrix} 1  0  0  1  0 \\\\ 0  1  1  0  0 \\\\ 0  0  1  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1.5 \\\\ 2.5 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1+1 \\\\ 1.5+2.5 \\\\ 2.5+1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 3.5 \\end{pmatrix}\n$$\n$$\n\\|A(-g_t)\\|_2^2 = 2^2 + 4^2 + (3.5)^2 = 4 + 16 + 12.25 = 32.25\n$$\n$$\n\\mu'_t = \\frac{11.5}{32.25} \\approx 0.3566\n$$\n在这种情况下，NIHT 步长 $\\mu_t \\approx 0.3820$ 大于最速下降步长 $\\mu'_t \\approx 0.3566$。将搜索方向从 $-g_t$ 限制到 $d_t$ 涉及将某些分量置零，这会减小步长公式的分子（$\\|d_t\\|_2^2  \\|-g_t\\|_2^2$）和分母（$\\|A d_t\\|_2^2  \\|A(-g_t)\\|_2^2$）的大小。步长的最终值取决于这两项的相对减少量。通过将问题限制在由支撑集 $\\Omega_t$ 定义的子空间上，由瑞利商 $\\|A d\\|_2^2 / \\|d\\|_2^2$ 衡量的目标函数沿搜索方向的表观曲率可能会改变。在这个具体例子中，分母 $\\|A d_t\\|_2^2$ 的下降比例大于分子 $\\|d_t\\|_2^2$ 的下降比例，导致它们的比值增加，即 $\\mu_t > \\mu'_t$。", "answer": "$$\n\\boxed{0.3820}\n$$", "id": "3438872"}, {"introduction": "像NIHT这样的迭代算法，其收敛性并非无条件的，而是依赖于传感矩阵 $A$ 的特定性质。本练习 [@problem_id:3463046] 旨在探索当这些理论保证——特别是受限等距性质（Restricted Isometry Property, RIP）——不被满足时会发生什么。通过一个精心构造的、导致NIHT无法收敛的例子，您将计算迭代矩阵的谱半径，并直接观察到矩阵的病态条件如何导致非压缩的动力学行为。", "problem": "考虑将归一化迭代硬阈值（NIHT）方法应用于线性逆问题，其中测量值为 $y \\in \\mathbb{R}^{m}$，传感矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，且 $y = A x^{\\star}$，对于一个 $k$-稀疏向量 $x^{\\star} \\in \\mathbb{R}^{n}$。NIHT 迭代定义为\n$$\nx_{t+1} \\;=\\; H_{k}\\!\\left(x_{t} \\;+\\; \\mu_{t}\\, A^{\\top}(y - A x_{t})\\right),\n$$\n其中 $H_{k}(\\cdot)$ 是硬阈值算子，它保留最大的 $k$ 个幅值分量，并将其余分量置零，而 $\\mu_{t}$ 被选为限制在活动支撑集上的子矩阵的谱范数平方的倒数，即\n$$\n\\mu_{t} \\;=\\; \\frac{1}{\\|A_{S_{t}}\\|_{2}^{2}},\n$$\n其中 $S_{t}$ 表示在第 $t$ 次迭代中由 $H_{k}(\\cdot)$ 保留的支撑集。阶数为 $s$、常数为 $\\delta_{s} \\in [0,1)$ 的有限等距性质（RIP）要求对于所有 $s$-稀疏向量 $x$，\n$$\n(1 - \\delta_{s})\\,\\|x\\|_{2}^{2} \\;\\le\\; \\|A x\\|_{2}^{2} \\;\\le\\; (1 + \\delta_{s})\\,\\|x\\|_{2}^{2}.\n$$\n\n按如下方式构造一个违反 RIP 并通过 $A_{S_{t}}^{\\top} A_{S_{t}}$ 的病态条件导致 NIHT 不收敛的显式示例。令 $m = 1$，$n = 2$，$k = 2$，并且\n$$\nA \\;=\\; \\begin{pmatrix} 1  1 \\end{pmatrix}, \\qquad y \\;=\\; 0.\n$$\n因为 $k = 2$，硬阈值算子 $H_{2}(\\cdot)$ 是 $\\mathbb{R}^{2}$ 上的恒等算子，所以对于所有 $t$，$S_{t} = \\{1,2\\}$，并且 $\\mu_{t}$ 是一个固定常数，等于 $1/\\|A\\|_{2}^{2}$。在这种设置下，NIHT 迭代简化为一个线性映射\n$$\nx_{t+1} \\;=\\; \\Big(I - \\mu\\, A^{\\top} A\\Big)\\, x_{t},\n$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵，$\\mu = 1/\\|A\\|_{2}^{2}$。$A_{S_{t}}^{\\top} A_{S_{t}}$ 的病态条件预计会产生非收缩动力学，可能表现为循环或发散。\n\n计算迭代矩阵\n$$\nM \\;=\\; I - \\mu\\, A^{\\top} A\n$$\n的谱半径，对于上述 $A$、$y$ 和 $k$，其中 $\\mu = 1/\\|A\\|_{2}^{2}$。你的最终答案必须是一个实数。无需四舍五入。", "solution": "该问题要求计算归一化迭代硬阈值（NIHT）算法特定实例的迭代矩阵 $M = I - \\mu A^{\\top} A$ 的谱半径。该问题经检验是自洽的、数学上合理的且适定的。关于违反有限等距性质（RIP）的先验声明对于给定参数是正确的，但这一验证是背景细节，并非主要计算所必需。我们进行计算。\n\n给定参数如下：\n- 传感矩阵：$A = \\begin{pmatrix} 1  1 \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}$。\n- 稀疏度：$k=2$。\n- 向量维度：$n=2$。\n- 测量数量：$m=1$。\n- 测量值：$y=0$。\n\n由于稀疏度 $k=2$ 等于信号维度 $n=2$，硬阈值算子 $H_{k}(\\cdot)$ 成为 $\\mathbb{R}^{2}$ 上的恒等算子，即对于任何 $v \\in \\mathbb{R}^{2}$ 都有 $H_{2}(v) = v$。因此，支撑集 $S_{t}$ 始终是全集 $\\{1, 2\\}$，并且对于所有迭代 $t$，受限矩阵 $A_{S_{t}}$ 就是 $A$。\n\n步长 $\\mu_{t}$ 由下式给出\n$$\n\\mu_{t} = \\mu = \\frac{1}{\\|A_{S_{t}}\\|_{2}^{2}} = \\frac{1}{\\|A\\|_{2}^{2}}.\n$$\n谱范数 $\\|A\\|_{2}$ 是 $A$ 的最大奇异值。$A$ 的奇异值是矩阵 $A^{\\top} A$ 的特征值的平方根。\n\n首先，我们计算 $A^{\\top} A$：\n$$\nA^{\\top} A = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\n\n接下来，我们求 $A^{\\top} A$ 的特征值。特征方程为 $\\det(A^{\\top} A - \\lambda I) = 0$。\n$$\n\\det\\left(\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\\right) = \\det\\begin{pmatrix} 1 - \\lambda  1 \\\\ 1  1 - \\lambda \\end{pmatrix} = 0.\n$$\n这得到 $(1 - \\lambda)^{2} - 1 = 0$，化简为 $\\lambda^{2} - 2\\lambda + 1 - 1 = 0$，即 $\\lambda(\\lambda - 2) = 0$。$A^{\\top} A$ 的特征值为 $\\lambda_{1} = 2$ 和 $\\lambda_{2} = 0$。\n\n$A$ 的奇异值为 $\\sigma_{1} = \\sqrt{\\lambda_{1}} = \\sqrt{2}$ 和 $\\sigma_{2} = \\sqrt{\\lambda_{2}} = 0$。$A$ 的谱范数是最大奇异值，所以 $\\|A\\|_{2} = \\sqrt{2}$。\n\n现在我们可以计算常数步长 $\\mu$：\n$$\n\\mu = \\frac{1}{\\|A\\|_{2}^{2}} = \\frac{1}{(\\sqrt{2})^{2}} = \\frac{1}{2}.\n$$\n\nNIHT 迭代简化为线性映射 $x_{t+1} = M x_{t}$，迭代矩阵 $M$ 由下式给出：\n$$\nM = I - \\mu A^{\\top} A = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\n进行矩阵运算：\n$$\nM = \\begin{pmatrix} 1 - \\frac{1}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{1}{2} \\end{pmatrix}.\n$$\n\n为了求出 $M$ 的谱半径，记为 $\\rho(M)$，我们必须求出它的特征值。设 $M$ 的特征值为 $\\lambda_{M}$。特征方程为 $\\det(M - \\lambda_{M} I) = 0$：\n$$\n\\det\\begin{pmatrix} \\frac{1}{2} - \\lambda_{M}  -\\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{1}{2} - \\lambda_{M} \\end{pmatrix} = 0.\n$$\n这得到 $(\\frac{1}{2} - \\lambda_{M})^{2} - (-\\frac{1}{2})^{2} = 0$，化简为 $\\frac{1}{4} - \\lambda_{M} + \\lambda_{M}^{2} - \\frac{1}{4} = 0$，即 $\\lambda_{M}^{2} - \\lambda_{M} = 0$。因式分解，我们得到 $\\lambda_{M}(\\lambda_{M} - 1) = 0$。$M$ 的特征值为 $\\lambda_{M,1} = 1$ 和 $\\lambda_{M,2} = 0$。\n\n谱半径是特征值绝对值的最大值：\n$$\n\\rho(M) = \\max(|\\lambda_{M,1}|, |\\lambda_{M,2}|) = \\max(|1|, |0|) = 1.\n$$\n谱半径为 $1$ 表明该线性迭代不是一个收缩映射，这与问题关于不收敛的框架是一致的。", "answer": "$$\\boxed{1}$$", "id": "3463046"}, {"introduction": "理论为算法提供了收敛性保证，但实际性能往往揭示了更细微的挑战。在这个计算实验 [@problem_id:3463072] 中，您将研究稀疏恢复中的一个常见难题：由于不当的初始化和高度相关的数据，算法会陷入“错误支撑集吸引子”。通过实现并比较固定步长的IHT、NIHT以及带回溯的NIHT，您将亲眼见证自适应的归一化步长如何帮助算法逃离陷阱，以及回溯线搜索如何为稳健恢复增添至关重要的稳定性。", "problem": "要求您构建并分析一个压缩感知中的稀疏恢复实验，该实验具体展示了一个不良的初始化如何导致算法陷入错误的支撑集吸引子，以及两种设计选择——梯度步长的归一化和回溯——如何帮助逃离该吸引子。该恢复任务被建模为在稀疏性约束下最小化二次损失：\n- 给定一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$、一个目标稀疏信号 $x^\\star \\in \\mathbb{R}^n$ (满足 $\\lVert x^\\star \\rVert_0 \\leq k$)，以及无噪声测量值 $y = A x^\\star$ 或含噪声测量值 $y = A x^\\star + w$，考虑目标函数 $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$，其中 $x \\in \\mathbb{R}^n$ 受约束 $\\lVert x \\rVert_0 \\leq k$。\n\n从以下核心基础开始：\n- 二次损失的梯度为 $\\nabla f(x) = - A^\\top (y - A x)$。\n- $\\nabla f$ 的 Lipschitz 常数为 $L = \\lVert A \\rVert_2^2$，即 $A$ 的谱范数的平方。\n- 硬阈值算子 $H_k(\\cdot)$ 将 $\\mathbb{R}^n$ 中的向量投影到集合 $\\{x : \\lVert x \\rVert_0 \\leq k\\}$ 上，方法是保留绝对值最大的 $k$ 个分量，并将其余分量置零。\n\n您必须实现并比较三种算法：\n- 固定步长迭代硬阈值算法 (IHT): $x_{t+1} = H_k\\!\\big(x_t + \\mu A^\\top (y - A x_t)\\big)$，其中步长 $\\mu$ 为常数。\n- 归一化迭代硬阈值算法 (NIHT): $x_{t+1} = H_k\\!\\big(x_t + \\mu_t A^\\top (y - A x_t)\\big)$，在每次迭代中选择归一化步长 $\\mu_t$，使其成为沿梯度方向的最速下降线搜索最小值点，即唯一的 $\\mu_t$ 使得 $f(x_t + \\mu A^\\top (y - A x_t))$ 在 $\\mu \\in \\mathbb{R}$ 上最小。\n- 带回溯的 NIHT (NIHT-BT): 将 NIHT 步长与回溯线搜索相结合，该搜索以乘法方式缩减步长，直到在 $H_k$ 投影后观察到 $f$ 的足够下降。\n\n设计一个实验，通过不良初始化诱导出错误支撑集吸引子，方法如下：\n- 构建一个感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$，其中 $m = 32$，$n = 64$。矩阵元素由独立的标准正态分布抽取，并将各列归一化为单位 $\\ell_2$ 范数。然后，通过创建“支撑集混淆项”来强制产生强列相关性：通过将某些列替换为其他列的近似缩放副本，来创建高度相关的列对（遵循测试套件中指定的精确方法）。这使得多个 $k$-稀疏支撑集都能对 $y$ 产生相似的良好拟合。\n- 选择一个真实（ground-truth）$k$-稀疏信号 $x^\\star$，其中 $k=2$，其支撑集位于 $A$ 中其他地方有近似副本的一对列上。\n- 通过在“错误”的 $k$-稀疏支撑集上求解最小二乘拟合来初始化 $x_0$，即计算 $x_0$ 时，令 $\\operatorname{supp}(x_0) = S_{\\text{wrong}}$ 且 $x_0[S_{\\text{wrong}}] = \\arg\\min_{z \\in \\mathbb{R}^k} \\lVert y - A_{S_{\\text{wrong}}} z \\rVert_2$，而其他分量为零。对于步长 $\\mu$ 足够小的固定步长 IHT 算法，这种初始化会产生一个错误支撑集吸引子，因为 $S_{\\text{wrong}}$ 内部的梯度会消失，而 $S_{\\text{wrong}}$ 外部的缩放后梯度分量仍然太小，无法克服阈值操作。\n\n程序要求行为：\n- 实现上述三种方法 (IHT, NIHT, NIHT-BT)，使用 $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$ 作为目标函数，并使用 $H_k$ 作为硬阈值算子。\n- 在迭代 $t$ 中，将 NIHT 归一化实现为沿梯度方向 $g_t = A^\\top (y - A x_t)$ 的精确最速下降步长，即选择 $\\mu_t$ 使 $\\phi(\\mu) = f(x_t + \\mu g_t)$ 在 $\\mu \\in \\mathbb{R}$ 上最小化。\n- 实现带回溯的线搜索，通过乘以一个因子 $\\beta \\in (0,1)$ 进行乘法收缩，直到将 $H_k$ 应用于试探性更新后获得 $f$ 的足够下降；使用一个标准的 Armijo 型条件，其中包含一个小的常数 $c \\in (0,1)$。\n\n测试套件：\n- 使用以下测试用例。在所有情况下，必须在开始时用种子 $123$ 初始化随机数生成器以确保可复现性，并且 $A$ 的所有列都必须缩放为单位 $\\ell_2$ 范数。\n    1. 案例 1 (固定步长 IHT 的错误支撑集吸引子):\n        - 维度: $m = 32$, $n = 64$, 稀疏度 $k = 2$。\n        - 矩阵 $A$: 从独立的标准正态分布元素开始，将列归一化为单位范数。然后通过将列 $10$ 设置为列 $0$ 被一个幅度为 $0.02$ 的小正交分量扰动后的重新归一化版本，并将列 $11$ 设置为列 $1$ 被一个幅度为 $0.02$ 的小正交分量扰动后的重新归一化版本，来强制加入两对中等程度的“混淆项”对。\n        - 真实（Ground truth）: 支撑集 $S^\\star = \\{0, 1\\}$，其中 $x^\\star[0] = 1.0$，$x^\\star[1] = 0.8$，所有其他分量为零。测量值 $y = A x^\\star$。\n        - 初始化: $x_0$ 是在 $S_{\\text{wrong}} = \\{10, 11\\}$ 上的最小二乘解。\n        - 算法: 固定步长 IHT，步长 $\\mu = 0.2 / L$ (其中 $L = \\lVert A \\rVert_2^2$)，运行 $500$ 次迭代。\n        - 此案例的输出: 一个布尔值，表示恢复的支撑集是否与 $S^\\star$ 完全相等。\n    2. 案例 2 (归一化帮助逃离):\n        - 与案例 1 中相同的 $A$, $x^\\star$, $y$ 和 $x_0$。\n        - 算法: NIHT，每次迭代进行最速下降归一化，运行 $200$ 次迭代，无回溯。\n        - 输出: 精确支撑集恢复的布尔值。\n    3. 案例 3 (无回溯的更难混淆项):\n        - 与上述案例相同的 $m, n, k$ 以及 $A$ 的基本归一化。除案例 1 中的修改外，再通过将列 $20$ 设置为列 $0$ 被一个幅度为 $0.001$ 的分量扰动后的重新归一化版本，并将列 $21$ 设置为列 $1$ 被一个幅度为 $0.001$ 的分量扰动后的重新归一化版本，来强制加入一对更强的混淆项。\n        - 真实（Ground truth）: 相同的 $S^\\star$ 和 $x^\\star$。\n        - 测量值: $y = A x^\\star + w$，其中 $w$ 是高斯噪声，其分量独立且标准差为 $\\sigma = 0.002$。\n        - 初始化: $x_0$ 是在 $S_{\\text{wrong}} = \\{20, 21\\}$ 上的最小二乘解。\n        - 算法: 不带回溯的 NIHT，运行 $50$ 次迭代。\n        - 输出: 精确支撑集恢复的布尔值。\n    4. 案例 4 (在更难的场景中回溯有帮助):\n        - 与案例 3 设置相同。\n        - 算法: 带回溯的 NIHT，使用 Armijo 参数 $c = 10^{-4}$ 和收缩因子 $\\beta = 0.5$，运行 $500$ 次迭代。\n        - 输出: 精确支撑集恢复的布尔值。\n\n实现细节：\n- 硬阈值算子 $H_k$ 必须精确保留绝对值最大的 $k$ 个分量，并将其余分量置零。如果出现绝对值相同的情况，通过索引的自然顺序来确定性地打破平局。\n- 对所有 $\\ell_2$ 范数，均使用欧几里得范数。\n- 每个测试用例的终止准则是指定的固定迭代次数；您无需实现任何自适应停止准则。\n- 为评估精确支撑集恢复，请将返回的估计向量的非零分量索引集合与 $S^\\star$ 进行比较；如果两个集合相等，则报告 true，否则报告 false。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个布尔结果，形式为方括号内以逗号分隔的列表（例如，$[\\text{True},\\text{False},\\text{True},\\text{True}]$），按顺序对应案例 1 到 4。不得打印任何其他文本。", "solution": "该问题要求实现并比较三种稀疏恢复算法——迭代硬阈值算法 (IHT)、归一化 IHT (NIHT) 和带回溯的 NIHT (NIHT-BT)——以展示一种特定的失效模式及其补救措施。核心任务是找到一个 $k$-稀疏信号 $x \\in \\mathbb{R}^n$，以最小化二次损失 $f(x) = \\tfrac{1}{2} \\lVert y - A x \\rVert_2^2$，其中 $y \\in \\mathbb{R}^m$ 是来自感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的测量值。\n\n所有这三种算法都基于投影梯度下降。通用更新步骤为 $x_{t+1} = P_{\\mathcal{C}_k}(x_t - \\mu_t \\nabla f(x_t))$，其中 $\\mathcal{C}_k = \\{x : \\lVert x \\rVert_0 \\leq k\\}$ 是 $k$-稀疏向量的非凸集，投影 $P_{\\mathcal{C}_k}$ 是硬阈值算子 $H_k(\\cdot)$。目标函数的梯度为 $\\nabla f(x) = A^\\top(Ax-y)$。代入后可得更新规则：\n$$x_{t+1} = H_k\\!\\big(x_t + \\mu_t A^\\top (y - A x_t)\\big)$$\n这些算法的主要区别在于步长 $\\mu_t$ 的选择。\n\n**实验设计：错误支撑集吸引子**\n该实验旨在创造一种情景，其中贪心算法容易陷入与不正确支撑集对应的局部最小值。这是通过构建一个具有高度相关列的感知矩阵 $A$ 来实现的。\n具体来说，如果我们有两个列 $a_i$ 和 $a_j$，使得对于某个标量 $\\alpha$ 有 $a_i \\approx \\alpha a_j$，那么它们就变得可混淆。如果真实信号 $x^\\star$ 的支撑集在索引 $i$ 上，那么模型 $y = A x^\\star \\approx x^\\star_i a_i$ 可以被一个使用列 $j$ 的模型很好地近似，即对于某个 $z_j$ 有 $y \\approx z_j a_j$。\n通过在一个“错误”但高度相关的支撑集 $S_{\\text{wrong}}$ 上使用最小二乘解来初始化算法，我们从一个点 $x_0$ 开始，在该点上残差 $r_0 = y - Ax_0$ 与 $A_{S_{\\text{wrong}}}$ 中的列近似正交。因此，梯度分量 $g_0[S_{\\text{wrong}}] = A_{S_{\\text{wrong}}}^\\top r_0$ 非常小。如果步长 $\\mu$ 很小，更新后的 $x_0 + \\mu g_0$ 的最大幅值分量将位于相同的支撑集 $S_{\\text{wrong}}$ 上，导致 $H_k$ 选择相同的不正确支撑集。算法因此被困住。这就是“错误支撑集吸引子”。\n\n**算法 1：固定步长迭代硬阈值算法 (IHT)**\nIHT 使用固定的步长 $\\mu$。一个保证梯度幅值收敛到零的标准选择要求 $\\mu  2/L$，其中 $L = \\lVert A \\rVert_2^2$ 是 $\\nabla f$ 的 Lipschitz 常数。问题中指定了一个保守的步长 $\\mu = 0.2/L$。在案例 1 中，这个小步长不足以克服对错误支撑集 $S_{\\text{wrong}} = \\{10, 11\\}$ 的吸引力，预计该算法将无法恢复真实支撑集 $S^\\star = \\{0, 1\\}$。\n\n**算法 2：归一化迭代硬阈值算法 (NIHT)**\nNIHT 在每次迭代中采用一个自适应计算的步长 $\\mu_t$。它被选择为在投影步骤*之前*沿梯度方向最小化二次目标函数的最优步长。令 $g_t = A^\\top(y - Ax_t) = -\\nabla f(x_t)$ 为搜索方向。我们寻求最小化 $\\phi(\\mu) = f(x_t + \\mu g_t) = \\tfrac{1}{2} \\lVert (y - Ax_t) - \\mu A g_t \\rVert_2^2$。对 $\\mu$ 求导并令其为零，可得到最优步长：\n$$\\mu_t = \\frac{g_t^\\top A^\\top(y - Ax_t)}{\\lVert A g_t \\rVert_2^2} = \\frac{g_t^\\top g_t}{\\lVert A g_t \\rVert_2^2} = \\frac{\\lVert g_t \\rVert_2^2}{\\lVert A g_t \\rVert_2^2}$$\n这个步长通常比 IHT 中的固定步长大得多，也更激进。如案例 2 所示，这个较大的步长可以为迭代值提供足够的“推动力”，将其移离起始点 $x_t$ 足够远，使得 $x_t + \\mu_t g_t$ 中的真实支撑集元素变得足够大，从而被阈值算子 $H_k$ 选中，进而逃离吸引子。然而，如案例 3 所示，当列相关性极高且存在噪声时，这种激进的步长可能导致不稳定性或过冲，从而导致失败。\n\n**算法 3：带回溯的 NIHT (NIHT-BT)**\n为了增加鲁棒性，NIHT 可以通过回溯线搜索来增强。这确保了每一步都能使目标函数有足够的下降，从而防止困扰普通 NIHT 的过冲问题。在计算出激进的 NIHT 步长 $\\mu_t$ 后，我们测试一个候选更新。只有当更新满足足够下降条件时才被接受。问题指定了一个 Armijo 型条件，对于投影梯度方法，该条件通常表示为：\n$$f(x_{t+1}) \\leq f(x_t) + c \\cdot \\nabla f(x_t)^\\top (x_{t+1} - x_t)$$\n其中 $x_{t+1} = H_k(x_t - \\mu \\nabla f(x_t))$，$c \\in (0,1)$ 是一个控制参数（此处给定为 $c=10^{-4}$）。如果条件不满足，步长 $\\mu$ 将被乘法式地减小（乘以一个因子 $\\beta=0.5$）并重复测试。这个过程从大的 NIHT 步长开始，仅在必要时才缩小它，从而将 NIHT 的速度与保证下降方法的稳定性结合起来。在案例 4 中，这种稳定化使得算法能够在具有非常高的列相关性和噪声的困难恢复场景中成功导航，并最终找到正确的支撑集。\n\n硬阈值算子 $H_k(v)$ 的实现方式是保留 $v$ 中绝对值最大的 $k$ 个元素。为确保确定性行为，幅值相同的情况通过选择索引较小的元素来打破。\n\n这一系列实验旨在展示：\n1.  案例 1：使用保守步长的简单 IHT 算法，在从错误支撑集吸引子初始化时会失败。\n2.  案例 2：NIHT 的激进、归一化步长成功逃离了这个吸引子。\n3.  案例 3：在具有更高相关性和噪声的更具挑战性的场景中，激进的 NIHT 步长变得不稳定并导致失败。\n4.  案例 4：为 NIHT 添加回溯机制提供了解决这一挑战性问题所需的稳定性。\n\n这一系列进展具体地展示了迭代稀疏恢复算法中不同步长选择策略之间的权衡。", "answer": "```python\nimport numpy as np\n\ndef hard_threshold(v, k):\n    \"\"\"\n    Performs hard thresholding on a vector v, keeping k elements.\n    Ties in magnitude are broken by the natural ordering of indices.\n    \"\"\"\n    if k == 0:\n        return np.zeros_like(v)\n    if k >= len(v):\n        return v.copy()\n    \n    # Use lexsort for deterministic tie-breaking by index.\n    # Sort keys are (-abs(v), index), so it sorts by descending magnitude,\n    # then ascending index for ties.\n    indices_to_keep = np.lexsort((np.arange(len(v)), -np.abs(v)))[:k]\n    \n    result = np.zeros_like(v)\n    result[indices_to_keep] = v[indices_to_keep]\n    return result\n\ndef create_A_with_confusers(A_base, confusers):\n    \"\"\"\n    Modifies a base matrix A to include confuser columns.\n    \"\"\"\n    A = A_base.copy()\n    n = A.shape[1]\n    for new_idx, ref_idx, pert_mag in confusers:\n        # Use a deterministic source for the orthogonal vector to ensure reproducibility.\n        # Pick an index that is not the reference or the new index itself.\n        orth_src_idx = (ref_idx + 5) % n \n        if orth_src_idx == new_idx or orth_src_idx == ref_idx:\n            orth_src_idx = (orth_src_idx + 1) % n\n        \n        u = A[:, ref_idx]\n        v = A[:, orth_src_idx]\n        \n        # Gram-Schmidt to get a vector orthogonal to u\n        v_orth = v - (v.T @ u) * u\n        v_orth_unit = v_orth / np.linalg.norm(v_orth)\n        \n        # Create the new column by perturbing u and re-normalize to unit norm\n        new_col = u + pert_mag * v_orth_unit\n        A[:, new_idx] = new_col / np.linalg.norm(new_col)\n    return A\n\ndef compute_x0(A, y, S_wrong):\n    \"\"\"\n    Computes the initial guess x0 as the least-squares solution on the wrong support.\n    \"\"\"\n    S_wrong_list = sorted(list(S_wrong))\n    A_wrong = A[:, S_wrong_list]\n    \n    z = np.linalg.lstsq(A_wrong, y, rcond=None)[0]\n    \n    x0 = np.zeros(A.shape[1])\n    x0[S_wrong_list] = z\n    return x0\n\ndef run_iht(A, y, x0, k, S_star, iterations, mu_factor):\n    \"\"\"\n    Case 1: Fixed-step Iterative Hard Thresholding.\n    \"\"\"\n    x = x0.copy()\n    L = np.linalg.norm(A, 2)**2\n    mu = mu_factor / L\n    \n    for _ in range(iterations):\n        grad = A.T @ (A @ x - y)\n        x = hard_threshold(x - mu * grad, k)\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef run_niht(A, y, x0, k, S_star, iterations):\n    \"\"\"\n    Case 2  3: Normalized Iterative Hard Thresholding.\n    \"\"\"\n    x = x0.copy()\n    \n    for _ in range(iterations):\n        r = y - A @ x\n        g = A.T @ r # This is -grad f(x), so we will add mu * g\n        \n        if np.linalg.norm(g)  1e-12:\n            break\n\n        Ag = A @ g\n        denom = Ag.T @ Ag\n        if denom  1e-20:\n            break\n        \n        mu = (g.T @ g) / denom\n        x = hard_threshold(x + mu * g, k)\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef run_niht_bt(A, y, x0, k, S_star, iterations, c, beta):\n    \"\"\"\n    Case 4: NIHT with Backtracking.\n    \"\"\"\n    x = x0.copy()\n    \n    for _ in range(iterations):\n        r = y - A @ x\n        fx = 0.5 * (r.T @ r)\n        g = A.T @ r \n        \n        if np.linalg.norm(g)  1e-12:\n            break\n            \n        Ag = A @ g\n        denom = Ag.T @ Ag\n        if denom  1e-20:\n            break\n            \n        mu = (g.T @ g) / denom\n        grad = -g\n        \n        while mu > 1e-20:\n            x_cand = hard_threshold(x + mu * g, k)\n            \n            r_cand = y - A @ x_cand\n            fx_cand = 0.5 * (r_cand.T @ r_cand)\n            \n            # Armijo condition for projected gradient methods\n            if fx_cand = fx + c * (grad.T @ (x_cand - x)):\n                break\n            \n            mu = mu * beta\n        \n        x = x_cand\n        \n    recovered_support = set(np.where(x != 0)[0])\n    return recovered_support == S_star\n\ndef main():\n    m, n, k = 32, 64, 2\n    S_star = {0, 1}\n    results = []\n\n    # Case 1\n    np.random.seed(123)\n    A_base = np.random.randn(m, n)\n    A_base /= np.linalg.norm(A_base, axis=0)\n    confusers_1 = [(10, 0, 0.02), (11, 1, 0.02)]\n    A1 = create_A_with_confusers(A_base, confusers_1)\n    \n    x_star = np.zeros(n)\n    x_star[0] = 1.0\n    x_star[1] = 0.8\n    y1 = A1 @ x_star\n    \n    S_wrong1 = {10, 11}\n    x0_1 = compute_x0(A1, y1, S_wrong1)\n    \n    result1 = run_iht(A1, y1, x0_1, k, S_star, iterations=500, mu_factor=0.2)\n    results.append(result1)\n\n    # Case 2\n    result2 = run_niht(A1, y1, x0_1, k, S_star, iterations=200)\n    results.append(result2)\n\n    # Case 3\n    np.random.seed(123) # Reset seed for comparable matrix generation\n    A_base = np.random.randn(m, n)\n    A_base /= np.linalg.norm(A_base, axis=0)\n    confusers_3 = [(10, 0, 0.02), (11, 1, 0.02), (20, 0, 0.001), (21, 1, 0.001)]\n    A3 = create_A_with_confusers(A_base, confusers_3)\n    \n    np.random.seed(123) # Reset seed for reproducible noise\n    noise = np.random.randn(m) * 0.002\n    y3 = A3 @ x_star + noise\n    \n    S_wrong3 = {20, 21}\n    x0_3 = compute_x0(A3, y3, S_wrong3)\n    \n    result3 = run_niht(A3, y3, x0_3, k, S_star, iterations=50)\n    results.append(result3)\n\n    # Case 4\n    np.random.seed(123) # Reset seed for reproducible run\n    result4 = run_niht_bt(A3, y3, x0_3, k, S_star, iterations=500, c=1e-4, beta=0.5)\n    results.append(result4)\n    \n    # Format and print the final output as specified\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nif __name__ == '__main__':\n    main()\n```", "id": "3463072"}]}