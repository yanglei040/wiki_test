{"hands_on_practices": [{"introduction": "第一个练习将引导你具体地、一步步地走完LARS-LASSO算法的全过程。通过为一个明确定义的小规模问题手动计算解路径，你将亲身体验到活动集如何演变以及系数在每个节点处如何更新，从而巩固对算法核心机制的理解。[@problem_id:3473510]", "problem": "考虑由优化问题 $\\min_{\\beta \\in \\mathbb{R}^{p}} \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$ 定义的最小绝对收缩和选择算子 (LASSO) 以及带有 LASSO 修正的最小角回归 (LARS) 算法 (LARS-LASSO)。Karush–Kuhn–Tucker (KKT) 条件意味着，对于一个解 $\\beta(\\lambda)$，存在一个次梯度向量 $s \\in \\mathbb{R}^{p}$ 且 $s_{j} \\in [-1,1]$，使得 $X^{\\top}(y - X \\beta(\\lambda)) = \\lambda s$，其中对于活动集中的所有 $j$，有 $s_{j} = \\operatorname{sign}(\\beta_{j}(\\lambda))$，而对于非活动集中的所有 $j$，有 $|X_{j}^{\\top}(y - X \\beta(\\lambda))| \\leq \\lambda$。LARS-LASSO 算法随着 $\\lambda$ 的减小，追踪分段线性的解路径 $\\beta(\\lambda)$，当预测变量与残差的相关性达到当前最大绝对相关性时，将其引入，并沿着由活动预测变量的格拉姆矩阵及其符号决定的等角方向移动，同时可能会有系数下降以维持 LASSO KKT 条件。\n\n设 $p = 3$, $n = 3$，并假设 $X$ 的列已被标准化为单位 $\\ell_{2}$ 范数。考虑\n$$\nX = \\begin{pmatrix}\n1  & \\frac{3}{5}  & \\frac{4}{5} \\\\\n0  & \\frac{4}{5}  & 0 \\\\\n0  & 0  & \\frac{3}{5}\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 0\n\\end{pmatrix}.\n$$\n从一个足够大的 $\\lambda$ 使得 $\\beta = 0$ 开始，使用基于 KKT 条件和等角方向构造的 LARS-LASSO 规则来：\n- 显式确定 LASSO 路径上活动集发生变化的前三个结点 $\\lambda_{0} > \\lambda_{1} > \\lambda_{2}$。\n- 计算每个分段上的系数更新，从而得到系数向量 $\\beta(\\lambda_{0})$、$\\beta(\\lambda_{1})$ 和 $\\beta(\\lambda_{2})$。\n\n请按顺序 $\\left(\\lambda_{0}, \\lambda_{1}, \\lambda_{2}, \\beta_{1}(\\lambda_{0}), \\beta_{2}(\\lambda_{0}), \\beta_{3}(\\lambda_{0}), \\beta_{1}(\\lambda_{1}), \\beta_{2}(\\lambda_{1}), \\beta_{3}(\\lambda_{1}), \\beta_{1}(\\lambda_{2}), \\beta_{2}(\\lambda_{2}), \\beta_{3}(\\lambda_{2})\\right)$ 以单行矩阵的形式提供最终答案。无需四舍五入；给出精确值。", "solution": "用户希望找出 LARS-LASSO 算法的前三个结点及对应的系数向量。\n\n### 问题验证\n问题陈述在计算统计和优化领域内提供了一个明确定义的数学任务。所有必要的数据，包括设计矩阵 $X$ 和响应向量 $y$，都已给出。维度是一致的（$n=3, p=3$）。关于 LASSO、KKT 条件和 LARS-LASSO 算法的基础理论是标准的且科学上合理的。该问题是自包含的、客观的，并存在一个唯一的、可验证的解。因此，该问题是有效的。\n\n### 解析解\nLARS-LASSO 算法随着正则化参数 $\\lambda$ 从 $\\infty$ 减小到 $0$ 的过程中，为系数向量 $\\beta(\\lambda)$ 构建了一条分段线性的路径。我们从 $\\beta = 0$ 开始，并识别出活动预测变量集发生变化的结点。\n\n**步骤 0：初始化与第一个结点 ($\\lambda_0$)**\n\n开始时，对于一个足够大的 $\\lambda$，解为 $\\beta = 0$。残差为 $r_0 = y - X\\beta = y$。\n$$\ny = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n我们计算预测变量与残差的初始相关性：$c = X^\\top r_0 = X^\\top y$。\n$$\nX^\\top = \\begin{pmatrix} 1  & 0  & 0 \\\\ \\frac{3}{5}  & \\frac{4}{5}  & 0 \\\\ \\frac{4}{5}  & 0  & \\frac{3}{5} \\end{pmatrix}\n$$\n$$\nc = X^\\top y = \\begin{pmatrix} 1  & 0  & 0 \\\\ \\frac{3}{5}  & \\frac{4}{5}  & 0 \\\\ \\frac{4}{5}  & 0  & \\frac{3}{5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{3}{5} \\cdot 1 + \\frac{4}{5} \\cdot 1 \\\\ \\frac{4}{5} \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{7}{5} \\\\ \\frac{4}{5} \\end{pmatrix}\n$$\n第一个结点 $\\lambda_0$ 是最大绝对相关性。\n$$\n\\lambda_0 = \\max_j |c_j| = \\max \\left( |1|, \\left|\\frac{7}{5}\\right|, \\left|\\frac{4}{5}\\right| \\right) = \\frac{7}{5}\n$$\n在此结点，相关性最高的预测变量，即预测变量 $j=2$，进入活动集。活动集变为 $\\mathcal{A}_1 = \\{2\\}$。\n在进入点，系数向量仍然为零。因此，\n$$\n\\beta(\\lambda_0) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\n**步骤 1：第一段路径与第二个结点 ($\\lambda_1$)**\n\n对于 $\\lambda < \\lambda_0$，系数 $\\beta_2$ 变为非零。$\\beta_2$ 的符号由其相关性的符号决定：$s_2 = \\operatorname{sign}(c_2) = 1$。活动集为 $\\mathcal{A}_1 = \\{2\\}$。我们定义一个从 $\\beta(\\lambda_0)$ 开始，由 $\\gamma \\ge 0$ 参数化的路径。\n系数路径的形式为 $\\beta(\\gamma) = (0, \\gamma, 0)^\\top$。\n残差演变为 $r(\\gamma) = y - X\\beta(\\gamma) = y - \\gamma X_2$。\n相关性演变为 $c(\\gamma) = X^\\top r(\\gamma) = X^\\top y - \\gamma X^\\top X_2 = c - \\gamma X^\\top X_2$。\n我们需要 $X^\\top X_2$，即格拉姆矩阵 $X^\\top X$ 的第二列：\n$$\nX^\\top X = \\begin{pmatrix} 1  & \\frac{3}{5}  & \\frac{4}{5} \\\\ \\frac{3}{5}  & 1  & \\frac{12}{25} \\\\ \\frac{4}{5}  & \\frac{12}{25}  & 1 \\end{pmatrix}, \\quad X^\\top X_2 = \\begin{pmatrix} \\frac{3}{5} \\\\ 1 \\\\ \\frac{12}{25} \\end{pmatrix}\n$$\n相关性路径为：\n$$\nc(\\gamma) = \\begin{pmatrix} 1 \\\\ \\frac{7}{5} \\\\ \\frac{4}{5} \\end{pmatrix} - \\gamma \\begin{pmatrix} \\frac{3}{5} \\\\ 1 \\\\ \\frac{12}{25} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{3}{5}\\gamma \\\\ \\frac{7}{5} - \\gamma \\\\ \\frac{4}{5} - \\frac{12}{25}\\gamma \\end{pmatrix}\n$$\n下一个结点出现在最小的 $\\gamma > 0$ 处，此时另一个预测变量的相关性在绝对值上等于活动相关性：对于 $j \\notin \\mathcal{A}_1$，有 $|c_j(\\gamma)| = |s_2 c_2(\\gamma)|$。由于 $s_2=1$ 且对于小的 $\\gamma > 0$，有 $c_2(\\gamma) > 0$，因此条件变为 $|c_j(\\gamma)| = c_2(\\gamma)$。\n- 对于 $j=1$：$|1 - \\frac{3}{5}\\gamma| = \\frac{7}{5} - \\gamma$。这得到 $1 - \\frac{3}{5}\\gamma = \\frac{7}{5} - \\gamma \\implies \\frac{2}{5}\\gamma = \\frac{2}{5} \\implies \\gamma=1$。\n- 对于 $j=3$：$|\\frac{4}{5} - \\frac{12}{25}\\gamma| = \\frac{7}{5} - \\gamma$。这得到 $\\frac{4}{5} - \\frac{12}{25}\\gamma = \\frac{7}{5} - \\gamma \\implies \\frac{13}{25}\\gamma = \\frac{3}{5} \\implies \\gamma=\\frac{15}{13}$。\n最小的正 $\\gamma$ 是 $\\gamma_1 = 1$。这是步长。此时，预测变量 $j=1$ 进入活动集。LASSO 下降条件不满足，因为 $\\beta_2(\\gamma) = \\gamma$ 的符号与 $s_2=1$ 相同。\n第二个结点 $\\lambda_1$ 是在 $\\gamma_1=1$ 处的共同相关性值：\n$$\n\\lambda_1 = c_2(\\gamma_1) = \\frac{7}{5} - 1 = \\frac{2}{5}\n$$\n此结点的系数向量为：\n$$\n\\beta(\\lambda_1) = \\begin{pmatrix} 0 \\\\ \\gamma_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n**步骤 2：第二段路径与第三个结点 ($\\lambda_2$)**\n\n现在的活动集是 $\\mathcal{A}_2 = \\{1, 2\\}$。在这段路径的开始，相关性为 $c(\\gamma_1) = (2/5, 2/5, 8/25)^\\top$。\n符号为 $s_1 = \\operatorname{sign}(c_1)=1$ 和 $s_2 = \\operatorname{sign}(c_2)=1$。活动集的符号向量是 $s_{\\mathcal{A}_2} = (1, 1)^\\top$。\n活动系数 $(\\beta_1, \\beta_2)$ 的移动方向是 $w = (X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2}$。\n$X_{\\mathcal{A}_2} = (X_1, X_2) = \\begin{pmatrix} 1  & \\frac{3}{5} \\\\ 0  & \\frac{4}{5} \\\\ 0  & 0 \\end{pmatrix}$。\n$$\nX_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2} = \\begin{pmatrix} 1  & \\frac{3}{5} \\\\ \\frac{3}{5}  & 1 \\end{pmatrix}, \\quad (X_{\\mathcal{A}_2}^\\top X_{\\mathcal{A}_2})^{-1} = \\frac{1}{1 - (\\frac{3}{5})^2} \\begin{pmatrix} 1  & -\\frac{3}{5} \\\\ -\\frac{3}{5}  & 1 \\end{pmatrix} = \\frac{25}{16} \\begin{pmatrix} 1  & -\\frac{3}{5} \\\\ -\\frac{3}{5}  & 1 \\end{pmatrix}\n$$\n$$\nw = \\frac{25}{16} \\begin{pmatrix} 1  & -\\frac{3}{5} \\\\ -\\frac{3}{5}  & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{25}{16} \\begin{pmatrix} 1 - \\frac{3}{5} \\\\ -\\frac{3}{5} + 1 \\end{pmatrix} = \\frac{25}{16} \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{8} \\\\ \\frac{5}{8} \\end{pmatrix}\n$$\n从 $\\beta(\\lambda_1)$ 开始，由新的步长 $\\gamma \\ge 0$ 参数化的系数路径为：\n$$\n\\beta(\\gamma) = \\beta(\\lambda_1) + \\gamma \\begin{pmatrix} w_1 \\\\ w_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\gamma \\begin{pmatrix} \\frac{5}{8} \\\\ \\frac{5}{8} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{8}\\gamma \\\\ 1 + \\frac{5}{8}\\gamma \\\\ 0 \\end{pmatrix}\n$$\n相关性路径为 $c(\\gamma) = c(\\gamma_1) - \\gamma X^\\top(X_1 w_1 + X_2 w_2)$。\n在原始空间中的方向向量是 $u = X_1 w_1 + X_2 w_2 = \\frac{5}{8}(X_1+X_2) = \\frac{5}{8}( (1,0,0)^\\top + (3/5, 4/5, 0)^\\top ) = \\frac{5}{8}(8/5, 4/5, 0)^\\top = (1, 1/2, 0)^\\top$。\n$$\nc(\\gamma) = c(\\gamma_1) - \\gamma X^\\top u = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\\\ \\frac{8}{25} \\end{pmatrix} - \\gamma X^\\top \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\\\ \\frac{8}{25} \\end{pmatrix} - \\gamma \\begin{pmatrix} 1 \\\\ 1 \\\\ \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} - \\gamma \\\\ \\frac{2}{5} - \\gamma \\\\ \\frac{8}{25} - \\frac{4}{5}\\gamma \\end{pmatrix}\n$$\n下一个结点 $\\lambda_2$ 出现在最小的 $\\gamma>0$ 处，此时一个非活动预测变量进入或一个活动预测变量退出。\n- 预测变量 $j=3$ 进入：$|c_3(\\gamma)| = |c_1(\\gamma)| \\implies |\\frac{8}{25} - \\frac{4}{5}\\gamma| = |\\frac{2}{5}-\\gamma|$。对于小的 $\\gamma>0$，两边都是正的。\n  $$\n  \\frac{8}{25} - \\frac{4}{5}\\gamma = \\frac{2}{5} - \\gamma \\implies \\gamma - \\frac{4}{5}\\gamma = \\frac{2}{5} - \\frac{8}{25} \\implies \\frac{1}{5}\\gamma = \\frac{2}{25} \\implies \\gamma_2 = \\frac{2}{5}\n  $$\n- 一个活动系数退出：$\\beta_1(\\gamma) = \\frac{5}{8}\\gamma$ 和 $\\beta_2(\\gamma) = 1 + \\frac{5}{8}\\gamma$。对于 $\\gamma > 0$，这些系数的符号与 $s_1=1, s_2=1$ 相匹配。因此，没有发生 LASSO 下降事件。\n\n步长为 $\\gamma_2 = 2/5$。第三个结点 $\\lambda_2$ 是共同的相关性：\n$$\n\\lambda_2 = c_1(\\gamma_2) = \\frac{2}{5} - \\frac{2}{5} = 0\n$$\n此结点的系数向量为：\n$$\n\\beta(\\lambda_2) = \\begin{pmatrix} \\frac{5}{8}\\gamma_2 \\\\ 1 + \\frac{5}{8}\\gamma_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{8} \\cdot \\frac{2}{5} \\\\ 1 + \\frac{5}{8} \\cdot \\frac{2}{5} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ 1 + \\frac{1}{4} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{5}{4} \\\\ 0 \\end{pmatrix}\n$$\n\n**结果总结**\n前三个结点是：\n$\\lambda_0 = \\frac{7}{5}$\n$\\lambda_1 = \\frac{2}{5}$\n$\\lambda_2 = 0$\n\n在这些结点处的系数向量是：\n$\\beta(\\lambda_0) = (0, 0, 0)^\\top$\n$\\beta(\\lambda_1) = (0, 1, 0)^\\top$\n$\\beta(\\lambda_2) = (\\frac{1}{4}, \\frac{5}{4}, 0)^\\top$\n\n最终答案由这 12 个值按指定顺序组成。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{5} & \\frac{2}{5} & 0 & 0 & 0 & 0 & 0 & 1 & 0 & \\frac{1}{4} & \\frac{5}{4} & 0\n\\end{pmatrix}\n}\n$$", "id": "3473510"}, {"introduction": "在上一个练习的基础上，本次实践将重点转移到LASSO问题的对偶视角。你将追踪残差向量$r$的演化，它与决定LARS步骤的相关性向量$z$直接相关。这个练习对于理解Karush-Kuhn-Tucker (KKT)条件如何在整个路径上得以维持，以及算法如何被等相关性原则驱动至关重要。[@problem_id:3473466]", "problem": "考虑在线性回归中没有截距项的最小绝对收缩和选择算子 (LASSO) 问题，其原始目标是最小化函数 $\\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应为 $y \\in \\mathbb{R}^{n}$。设 $n=3$ 且 $p=4$，并假设 $X$ 的列是单位范数的。定义残差 $r := y - X \\beta$ 和相关向量 $z := X^{\\top} r$。Karush–Kuhn–Tucker (KKT) 条件表明，对于给定的 $\\lambda > 0$，在解处，相关性满足对所有索引 $j$ 都有 $|z_{j}| \\leq \\lambda$，并且在活动集上 $z_{j} = \\lambda s_{j}$，其中 $s_{j} \\in \\{-1, +1\\}$ 是相应系数的符号。\n\n设 $X$ 的列向量为\n- $x_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$，\n- $x_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}$，\n- $x_{3} = \\begin{pmatrix}0 \\\\ 0 \\\\ 1\\end{pmatrix}$，\n- $x_{4} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1 \\\\ 0\\end{pmatrix}$，\n\n响应 $y$ 为 $y = \\begin{pmatrix}2 \\\\ \\frac{1}{2} \\\\ \\frac{1}{10}\\end{pmatrix}$。\n\n从 $\\beta = 0$ 和 $\\lambda = \\lambda_{\\max} := \\|X^{\\top} y\\|_{\\infty}$ 开始 LASSO 的最小角回归 (LARS) 路径，并遵循标准的 LARS-LASSO 规则减小 $\\lambda$。跟踪残差 $r$ 和相关向量 $z = X^{\\top} r$ 经过第一个 LARS-LASSO 事件，也就是说，从初始活动集 $\\{1\\}$（符号为 $s_{1} = \\operatorname{sign}(x_{1}^{\\top} y)$）开始，直到第一个节点，在该节点处，某个非活动变量的绝对相关性与活动相关性相等。\n\n计算该第一个节点处的残差向量的精确值。将最终答案表示为一个包含三个精确解析形式条目的单行向量。无需四舍五入，无单位。\n\n作为推导过程中的核对（不要在最终答案中报告），通过确认所有 $j$ 都满足 $|z_{j}| \\leq \\lambda$ 并且等式对活动索引和并列索引成立，来验证 KKT 等式在节点处成立。", "solution": "问题要求计算 LASSO 问题的最小角回归 (LARS) 路径上第一个节点处的残差向量。给定设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$（其中 $n=3, p=4$）和响应向量 $y \\in \\mathbb{R}^{3}$。\n\n设计矩阵 $X$ 的列向量如下：\n$x_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $x_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$, $x_{3} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$, $x_{4} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n完整的设计矩阵是 $X = \\begin{pmatrix} 1  & 0  & 0  & \\frac{1}{\\sqrt{2}} \\\\ 0  & 1  & 0  & \\frac{1}{\\sqrt{2}} \\\\ 0  & 0  & 1  & 0 \\end{pmatrix}$。\n响应向量是 $y = \\begin{pmatrix} 2 \\\\ \\frac{1}{2} \\\\ \\frac{1}{10} \\end{pmatrix}$。\n\nLARS-LASSO 算法从系数向量 $\\beta = 0$ 开始。在这个初始状态下，残差为 $r_0 = y - X \\cdot 0 = y$。\n相关向量$z$定义为$z = X^{\\top} r$。首先，我们计算初始相关性$z_0 = X^{\\top} y$：\n$$\nz_{0,1} = x_{1}^{\\top} y = 1 \\cdot 2 + 0 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{10} = 2\n$$\n$$\nz_{0,2} = x_{2}^{\\top} y = 0 \\cdot 2 + 1 \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{10} = \\frac{1}{2}\n$$\n$$\nz_{0,3} = x_{3}^{\\top} y = 0 \\cdot 2 + 0 \\cdot \\frac{1}{2} + 1 \\cdot \\frac{1}{10} = \\frac{1}{10}\n$$\n$$\nz_{0,4} = x_{4}^{\\top} y = \\frac{1}{\\sqrt{2}} \\cdot 2 + \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{2} + 0 \\cdot \\frac{1}{10} = \\frac{1}{\\sqrt{2}}\\left(2 + \\frac{1}{2}\\right) = \\frac{5}{2\\sqrt{2}} = \\frac{5\\sqrt{2}}{4}\n$$\n初始相关向量为 $z_0 = \\begin{pmatrix} 2 & \\frac{1}{2} & \\frac{1}{10} & \\frac{5\\sqrt{2}}{4} \\end{pmatrix}^{\\top}$。\n\nLARS 算法首先识别具有最高绝对相关性的预测变量。这也决定了 LASSO 参数的起始值，$\\lambda_{\\max} = \\|X^{\\top} y\\|_{\\infty} = \\|z_0\\|_{\\infty}$。\n我们比较初始相关性的绝对值：$|z_{0,1}| = 2$，$|z_{0,2}| = 0.5$，$|z_{0,3}| = 0.1$，以及 $|z_{0,4}| = \\frac{5\\sqrt{2}}{4} \\approx 1.768$。最大值为 $|z_{0,1}| = 2$。\n因此，$\\lambda_{\\max} = 2$，第一个进入模型的预测变量是$x_1$。初始活动集为 $\\mathcal{A} = \\{1\\}$。其系数的符号由其相关性的符号决定：$s_1 = \\operatorname{sign}(z_{0,1}) = +1$。\n\nLARS 算法通过沿一个方向移动 $\\beta$ 系数来继续进行，该方向使得活动集中所有预测变量的绝对相关性保持相等。系数更新为$\\beta(\\gamma) = \\beta_0 + \\gamma d$，其中$\\beta_0=0$，$d$是 LARS 方向。这导致残差演变为$r(\\gamma) = r_0 - \\gamma X d$。为系数构建的 LARS 方向$d$使得残差沿着路径$u_{\\mathcal{A}} = X_{\\mathcal{A}} (X_{\\mathcal{A}}^{\\top} X_{\\mathcal{A}})^{-1} s_{\\mathcal{A}}$移动，其中$X_{\\mathcal{A}}$是 $X$ 中列在$\\mathcal{A}$中的子矩阵，$s_{\\mathcal{A}}$是符号向量。\n对于我们的活动集$\\mathcal{A}=\\{1\\}$，我们有$X_{\\mathcal{A}} = x_1$和$s_{\\mathcal{A}} = (s_1) = (1)$。\n矩阵$X_{\\mathcal{A}}^{\\top} X_{\\mathcal{A}} = x_1^{\\top} x_1 = \\|x_1\\|_2^2 = 1$（因为列向量是单位范数）。所以$(X_{\\mathcal{A}}^{\\top} X_{\\mathcal{A}})^{-1} = 1^{-1} = 1$。\n等角方向向量为$u_{\\mathcal{A}} = x_1 \\cdot 1 \\cdot 1 = x_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n残差向量演变为$r(\\gamma) = r_0 - \\gamma u_{\\mathcal{A}}$，其中$\\gamma \\ge 0$是步长。\n\n预测变量的相关性演变为$z_j(\\gamma) = x_j^{\\top} r(\\gamma) = x_j^{\\top} r_0 - \\gamma x_j^{\\top} u_{\\mathcal{A}}$。\n对于活动预测变量$j=1$，$z_1(\\gamma) = z_{0,1} - \\gamma x_1^{\\top} x_1 = 2 - \\gamma$。活动变量的绝对相关性为$\\lambda(\\gamma) = |2-\\gamma| = 2-\\gamma$（对于$\\gamma \\le 2$）。\n当某个非活动预测变量的绝对相关性变得与活动相关性相等时，即对于某个$k \\notin \\mathcal{A}$有$|z_k(\\gamma)| = \\lambda(\\gamma)$时，出现一个“节点”。\n我们对每个非活动预测变量$k \\in \\{2, 3, 4\\}$检查此条件。\n这需要求解$|z_{0,k} - \\gamma x_k^{\\top} u_{\\mathcal{A}}| = 2 - \\gamma$以找到最小的正$\\gamma$。\n令$a_k = x_k^{\\top} u_{\\mathcal{A}} = x_k^{\\top} x_1$。\n$a_2 = x_2^{\\top} x_1 = 0$。\n$a_3 = x_3^{\\top} x_1 = 0$。\n$a_4 = x_4^{\\top} x_1 = \\frac{1}{\\sqrt{2}}$。\n\n我们对每个非活动预测变量求解$\\gamma$：\n对于 $k=2$：$|z_{0,2} - \\gamma a_2| = 2 - \\gamma \\implies |\\frac{1}{2} - 0 \\cdot \\gamma| = 2 - \\gamma \\implies \\frac{1}{2} = 2 - \\gamma$（因为 $\\frac{1}{2}>0$）。这得到 $\\gamma = 2 - \\frac{1}{2} = \\frac{3}{2}$。另一种可能性 $-\\frac{1}{2} = 2 - \\gamma$ 得到 $\\gamma = \\frac{5}{2}$。\n\n对于 $k=3$：$|z_{0,3} - \\gamma a_3| = 2 - \\gamma \\implies |\\frac{1}{10} - 0 \\cdot \\gamma| = 2 - \\gamma \\implies \\frac{1}{10} = 2-\\gamma$。这得到 $\\gamma = 2-\\frac{1}{10}=\\frac{19}{10}$。另一种可能性 $-\\frac{1}{10} = 2-\\gamma$ 得到 $\\gamma=\\frac{21}{10}$。\n\n对于 $k=4$：$|z_{0,4} - \\gamma a_4| = 2 - \\gamma \\implies |\\frac{5\\sqrt{2}}{4} - \\gamma \\frac{1}{\\sqrt{2}}| = 2 - \\gamma$。\n这产生两个方程：\n$1)$ $\\frac{5\\sqrt{2}}{4} - \\frac{\\gamma}{\\sqrt{2}} = 2 - \\gamma \\implies \\gamma\\left(1 - \\frac{1}{\\sqrt{2}}\\right) = 2 - \\frac{5\\sqrt{2}}{4}$。\n$\\gamma = \\frac{2 - \\frac{5\\sqrt{2}}{4}}{1-\\frac{\\sqrt{2}}{2}} = \\frac{\\frac{8-5\\sqrt{2}}{4}}{\\frac{2-\\sqrt{2}}{2}} = \\frac{8-5\\sqrt{2}}{2(2-\\sqrt{2})} = \\frac{(8-5\\sqrt{2})(2+\\sqrt{2})}{2(4-2)} = \\frac{16+8\\sqrt{2}-10\\sqrt{2}-10}{4} = \\frac{6-2\\sqrt{2}}{4} = \\frac{3-\\sqrt{2}}{2}$。\n$2)$ $-(\\frac{5\\sqrt{2}}{4} - \\frac{\\gamma}{\\sqrt{2}}) = 2 - \\gamma \\implies \\frac{\\gamma}{\\sqrt{2}} - \\frac{5\\sqrt{2}}{4} = 2 - \\gamma$。\n$\\gamma\\left(1 + \\frac{1}{\\sqrt{2}}\\right) = 2 + \\frac{5\\sqrt{2}}{4}$。\n$\\gamma = \\frac{2 + \\frac{5\\sqrt{2}}{4}}{1+\\frac{\\sqrt{2}}{2}} = \\frac{\\frac{8+5\\sqrt{2}}{4}}{\\frac{2+\\sqrt{2}}{2}} = \\frac{8+5\\sqrt{2}}{2(2+\\sqrt{2})} = \\frac{(8+5\\sqrt{2})(2-\\sqrt{2})}{2(4-2)} = \\frac{16-8\\sqrt{2}+10\\sqrt{2}-10}{4} = \\frac{6+2\\sqrt{2}}{4} = \\frac{3+\\sqrt{2}}{2}$。\n\n$\\gamma$ 的可能正值为 $\\frac{3}{2}=1.5$、$\\frac{5}{2}=2.5$、$\\frac{19}{10}=1.9$、$\\frac{21}{10}=2.1$、$\\frac{3-\\sqrt{2}}{2} \\approx 0.793$ 和 $\\frac{3+\\sqrt{2}}{2} \\approx 2.207$。\n第一个节点出现在这些值的最小值处，即 $\\gamma^* = \\frac{3-\\sqrt{2}}{2}$。在此步长下，预测变量 $x_4$ 进入活动集。由于我们只对第一个事件感兴趣，因此不再继续。\n\n问题要求计算这个第一个节点处的残差向量 $r$。这由 $r(\\gamma^*)$ 给出。\n$r(\\gamma^*) = r_0 - \\gamma^* u_{\\mathcal{A}} = y - \\frac{3-\\sqrt{2}}{2} x_1$。\n$$ r(\\gamma^*) = \\begin{pmatrix} 2 \\\\ \\frac{1}{2} \\\\ \\frac{1}{10} \\end{pmatrix} - \\frac{3-\\sqrt{2}}{2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n$$ r(\\gamma^*) = \\begin{pmatrix} 2 - \\frac{3-\\sqrt{2}}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{4 - (3-\\sqrt{2})}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{1+\\sqrt{2}}{2} \\\\ \\frac{1}{2} \\\\ \\frac{1}{10} \\end{pmatrix} $$\n这就是第一个节点处的残差向量。问题要求将其表示为单个行向量。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1+\\sqrt{2}}{2} & \\frac{1}{2} & \\frac{1}{10} \\end{pmatrix} } $$", "id": "3473466"}, {"introduction": "这最后一个综合性实践融合了理论、实现与仿真。你将首先从理论上推导解的$\\ell_1$范数$\\|\\hat{\\beta}(\\lambda)\\|_1$单调性的条件，这是LASSO路径的一个关键特性。然后，你将通过编程实现纯LARS和修正后的LARS-LASSO算法，以通过经验验证这一性质，并观察“丢弃”步骤在确保LASSO路径完整性方面的关键作用。[@problem_id:3473500]", "problem": "考虑由以下凸优化问题定义的最小绝对值收敛和选择算子 (LASSO)\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，其列已标准化为单位 $\\ell_2$ 范数，$y \\in \\mathbb{R}^n$ 是响应向量，$\\lambda \\ge 0$ 是正则化参数。该问题的 Karush–Kuhn–Tucker (KKT) 条件表明，存在一个次梯度向量 $z \\in \\partial \\|\\beta\\|_1$，使得\n$$\nX^\\top(y - X\\beta) = \\lambda z,\n$$\n其中，如果 $\\beta_j \\ne 0$，则 $z_j = \\operatorname{sign}(\\beta_j)$；如果 $\\beta_j = 0$，则 $z_j \\in [-1,1]$。沿着最小角回归 (LARS) 路径，变量被添加（在 LASSO 修正变体中也可选择性地被移除），以使活动集中的绝对相关性保持相等并线性减小。令 $A$ 表示给定阶段的活动索引集，令 $s_A \\in \\{-1,1\\}^{|A|}$ 为活动系数对应的符号向量。在系数空间中，LARS 等角更新方向由下式给出\n$$\nd\\beta_A = \\frac{(X_A^\\top X_A)^{-1} s_A}{\\sqrt{s_A^\\top (X_A^\\top X_A)^{-1} s_A}},\n$$\n这是当残差沿着等角向量 $u = X_A d\\beta_A$ 更新时，能够为 $A$ 中的列保持相等绝对相关性的唯一方向。在本问题中，您将需要：\n\n1. 从第一性原理推导，在何种条件下，$\\ell_1$ 范数 $\\|\\hat{\\beta}(\\lambda)\\|_1$ 沿着由 $\\lambda$ 参数化的 LARS 路径是单调的。具体来说，从 KKT 条件和 LARS 等角方向出发，证明只要活动系数的符号保持不变（即 $\\beta_A$ 的任何分量都不穿过零，从而使 $s_A$ 保持恒定），$\\ell_1$ 范数相对于 LARS 路径参数的瞬时变化率在 $\\lambda$ 减小的方向上是严格为正的，因此 $\\|\\hat{\\beta}(\\lambda)\\|_1$ 作为 $\\lambda$ 递增的函数是非递增的。请提供导数的显式表达式，以及用 $s_A$ 和 $(X_A^\\top X_A)^{-1}$ 表示的精确单调性条件。\n\n2. 实现两种同伦算法：\n   - 纯最小角回归 (LARS) 路径，该路径仅在进入事件时添加变量，从不移除它们，允许系数在活动集中改变符号。\n   - LARS–LASSO 路径，该路径强制执行 LASSO 修正，即每当一个变量的系数在下一个进入事件之前达到零时，就将其从活动集中移除，从而防止活动集内的符号变化。\n\n   您的实现必须追踪：\n   - 路径上节点值 $\\lambda_k = \\max_j |X_j^\\top (y - X\\hat{\\beta})|$ 的序列，\n   - $\\ell_1$ 范数 $\\|\\hat{\\beta}(\\lambda_k)\\|_1$ 的序列，\n   - 纯 LARS 路径的符号变化事件数量（当 $\\operatorname{sign}(\\beta_j)$ 在连续节点之间从正变为负或反之亦然时，计为一个符号翻转），\n   - LARS–LASSO 路径的移除事件数量。\n\n3. 使用模拟的设计矩阵 $X$ 和响应 $y$，测试路径上的 $\\|\\hat{\\beta}(\\lambda)\\|_1$ 是否随着 $\\lambda$ 的增加而非递增，并将纯 LARS 路径中观察到的任何偏差与 $\\beta_A(\\lambda)$ 中的符号变化联系起来。为确保科学真实性，请使用以下具有确定性种子和标准化列（对于所有 $j$，$\\|X_{\\cdot j}\\|_2 = 1$）的测试套件：\n\n   - 测试用例 1（正交设计，一个基本的边界情况）：$n = 20$，$p = 5$。通过取高斯随机矩阵（使用固定种子）的 $\\mathbf{QR}$ 分解的 $\\mathbf{Q}$ 因子的前 $p$ 列来构造具有正交列的 $X$。生成 $y$ 作为独立的高斯条目，然后将 $X$ 的列缩放至单位范数（它们本来就应该是）。在这种情况下，几何结构是理想的，预期会具有单调性。\n   - 测试用例 2（中度相关设计，一个一般情况）：$n = 60$，$p = 12$。从标准正态分布中独立抽取 $X$ 的条目，然后将每列缩放至单位范数。抽取一个稀疏的真实 $\\beta^\\star$，其恰好有 $3$ 个非零条目（值是确定性选择的），并设置 $y = X\\beta^\\star + \\varepsilon$，其中 $\\varepsilon$ 是小的高斯噪声。\n   - 测试用例 3（高度共线性设计，一个会引发符号张力的边缘情况）：$n = 40$，$p = 6$。通过设置 $X_{\\cdot 2} \\approx X_{\\cdot 1}$ 和 $X_{\\cdot 3} \\approx X_{\\cdot 1}$ 外加小的独立高斯扰动来构造具有两个几乎共线的列的 $X$；用独立的高斯向量填充其余列。将每列缩放至单位范数。将 $y$ 设置为一个确定性的线性组合，该组合会在共线预测变量之间引发竞争，并加入少量噪声。\n\n4. 对于每个测试用例，产出四个量：\n   - 一个布尔值，指示当 $\\lambda$ 按升序遍历时，纯 LARS 路径上的 $\\|\\hat{\\beta}(\\lambda)\\|_1$ 是否为非递增，\n   - 纯 LARS 路径上观察到的符号变化整数计数，\n   - 一个布尔值，指示当 $\\lambda$ 按升序遍历时，LARS–LASSO 路径上的 $\\|\\hat{\\beta}(\\lambda)\\|_1$ 是否为非递增，\n   - LARS–LASSO 路径上观察到的移除事件整数计数。\n\n您的程序必须生成单行输出，其中包含按测试用例顺序排列的、以逗号分隔的列表的列表，每个内部列表的格式为 $[\\text{布尔值}, \\text{整数}, \\text{布尔值}, \\text{整数}]$。例如，输出必须如下所示\n$$\n[[\\text{True},0,\\text{True},0],[\\text{True},1,\\text{True},0],[\\text{False},2,\\text{True},3]]\n$$\n其中布尔值采用 Python 语言中的大写形式，整数采用十进制表示法。不涉及任何物理单位或角度单位；所有输出均为无量纲。", "solution": "用户提供了一个关于最小角回归 (LARS) 算法与最小绝对值收缩和选择算子 (LASSO) 之间关系的多部分问题。该任务涉及理论推导、两种路径算法的实现，以及在指定测试用例上的数值验证。\n\n### 问题验证\n\n- **步骤 1：提取给定条件**\n    - LASSO 问题：$\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1$。\n    - 设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其列已标准化为单位 $\\ell_2$ 范数。\n    - 响应向量 $y \\in \\mathbb{R}^n$。\n    - 正则化参数 $\\lambda \\ge 0$。\n    - KKT 条件：$X^\\top(y - X\\beta) = \\lambda z$，其中 $z \\in \\partial \\|\\beta\\|_1$。\n    - LARS 活动集 $A$ 和符号向量 $s_A \\in \\{-1,1\\}^{|A|}$。\n    - LARS 活动系数的等角更新方向：$d\\beta_A = \\frac{(X_A^\\top X_A)^{-1} s_A}{\\sqrt{s_A^\\top (X_A^\\top X_A)^{-1} s_A}}$。\n    - 测试用例：\n        1.  $n=20, p=5$，正交 $X$。\n        2.  $n=60, p=12$，中度相关 $X$，稀疏 $\\beta^\\star$。\n        3.  $n=40, p=6$，高度共线性 $X$。\n    - 输出规格：对于每个测试用例，一个列表 `[布尔值, 整数, 布尔值, 整数]`，分别代表（LARS 单调性，LARS 符号变化数，LASSO 单调性，LASSO 移除数）。\n\n- **步骤 2：使用提取的给定条件进行验证**\n    - 该问题具有**科学依据**。它基于高维统计和优化领域的既定基本概念（LARS、LASSO、KKT 条件）。所有数学公式都是标准且正确的。\n    - 该问题是**适定的**。它要求进行特定的理论推导和实现定义明确的算法。测试用例被完全指定，并带有确定性种子，确保了唯一且可验证的结果。\n    - 该问题是**客观的**。它使用精确的数学语言，避免了任何主观或模糊的术语。\n    - 该问题是**完整且一致的**。为推导和实现提供了所有必要的数据、定义和约束。\n    - 该问题不是微不足道的、比喻性的，或超出科学可验证性范围的。\n\n- **步骤 3：结论与行动**\n    - 该问题被判定为**有效**。将继续进行求解过程。\n\n### 第 1 部分：L1 范数单调性的推导\n\n我们的目标是证明 LASSO 解的$\\ell_1$范数$\\|\\hat{\\beta}(\\lambda)\\|_1$是正则化参数$\\lambda$的非递增函数。\n\nLASSO 优化问题的 Karush-Kuhn-Tucker (KKT) 条件指出，对于一个解$\\hat{\\beta}(\\lambda)$，存在一个次梯度向量$z \\in \\partial\\|\\hat{\\beta}(\\lambda)\\|_1$，使得：\n$$\nX^\\top(y - X\\hat{\\beta}(\\lambda)) = \\lambda z\n$$\n次梯度向量$z$的分量为：如果$\\hat{\\beta}_j(\\lambda) \\neq 0$，则$z_j = \\operatorname{sign}(\\hat{\\beta}_j(\\lambda))$；如果$\\hat{\\beta}_j(\\lambda) = 0$，则$z_j \\in [-1, 1]$。\n\n令$A = \\{j \\mid \\hat{\\beta}_j(\\lambda) \\neq 0\\}$为预测变量的活动集，令$s_A$为相应系数的符号向量，即对于$j \\in A$，有$s_j = \\operatorname{sign}(\\hat{\\beta}_j(\\lambda))$。对于索引$j \\in A$，KKT 条件简化为$X_j^\\top(y - X\\hat{\\beta}(\\lambda)) = \\lambda s_j$。对于索引$j \\notin A$，我们有$|X_j^\\top(y - X\\hat{\\beta}(\\lambda))| \\le \\lambda$。\n\nLARS-LASSO 算法构造了解路径$\\hat{\\beta}(\\lambda)$，该路径是分段线性的。考虑两个连续事件（节点）之间的路径段，其中活动集$A$和符号向量$s_A$是恒定的。在此段上，对于$j \\notin A$的系数为零，因此$\\hat{\\beta}(\\lambda)$仅依赖于$\\hat{\\beta}_A(\\lambda)$。KKT 条件的活动部分可以写为：\n$$\nX_A^\\top (y - X_A \\hat{\\beta}_A(\\lambda)) = \\lambda s_A\n$$\n为了找到系数相对于$\\lambda$的变化率，我们对该方程求导，同时保持$A$和$s_A$不变：\n$$\n\\frac{d}{d\\lambda} [X_A^\\top y - X_A^\\top X_A \\hat{\\beta}_A(\\lambda)] = \\frac{d}{d\\lambda} [\\lambda s_A]\n$$\n鉴于$X_A^\\top y$是常数，我们得到：\n$$\n-X_A^\\top X_A \\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda} = s_A\n$$\n假设 $X_A$ 的列是线性无关的，那么格拉姆矩阵$G_A = X_A^\\top X_A$是可逆的。我们可以求解活动系数的导数：\n$$\n\\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda} = -(X_A^\\top X_A)^{-1} s_A\n$$\n这个表达式描述了活动系数如何随 $\\lambda$ 局部变化。\n\n接下来，我们考察解的$\\ell_1$范数$\\|\\hat{\\beta}(\\lambda)\\|_1 = \\sum_{j=1}^p |\\hat{\\beta}_j(\\lambda)|$。由于对于$j \\notin A$，有$\\hat{\\beta}_j(\\lambda)=0$，并且我们处于一个符号恒定的段上，我们可以写出对于$j \\in A$，有$|\\hat{\\beta}_j(\\lambda)| = s_j \\hat{\\beta}_j(\\lambda)$。因此，$\\ell_1$范数为：\n$$\n\\|\\hat{\\beta}(\\lambda)\\|_1 = \\sum_{j \\in A} s_j \\hat{\\beta}_j(\\lambda) = s_A^\\top \\hat{\\beta}_A(\\lambda)\n$$\n将$\\ell_1$范数对$\\lambda$求导得出：\n$$\n\\frac{d}{d\\lambda} \\|\\hat{\\beta}(\\lambda)\\|_1 = s_A^\\top \\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda}\n$$\n代入我们推导出的$\\frac{d\\hat{\\beta}_A(\\lambda)}{d\\lambda}$表达式：\n$$\n\\frac{d}{d\\lambda} \\|\\hat{\\beta}(\\lambda)\\|_1 = s_A^\\top \\left( -(X_A^\\top X_A)^{-1} s_A \\right) = -s_A^\\top (X_A^\\top X_A)^{-1} s_A\n$$\n矩阵 $X_A^\\top X_A$ 是由一组线性无关向量构成的格拉姆矩阵，因此是对称正定的。其逆矩阵 $(X_A^\\top X_A)^{-1}$ 也是对称正定的。表达式 $s_A^\\top (X_A^\\top X_A)^{-1} s_A$ 是一个二次型。由于 $(X_A^\\top X_A)^{-1}$ 是正定的，且 $s_A$ 是一个非零的符号向量，这个二次型是严格为正的：\n$$\ns_A^\\top (X_A^\\top X_A)^{-1} s_A > 0\n$$\n因此，$\\ell_1$范数关于$\\lambda$的导数是严格为负的：\n$$\n\\frac{d}{d\\lambda} \\|\\hat{\\beta}(\\lambda)\\|_1 < 0\n$$\n这证明了在任何活动集 $A$ 和符号向量 $s_A$ 恒定的区间上，$\\|\\hat{\\beta}(\\lambda)\\|_1$ 是 $\\lambda$ 的严格递减函数。整个解路径 $\\hat{\\beta}(\\lambda)$ 是连续的。因此，$\\|\\hat{\\beta}(\\lambda)\\|_1$ 全局上是 $\\lambda$ 的非递增函数。\n\n此单调性的显式条件是活动系数的符号 $s_A$ 保持恒定。LARS-LASSO 算法通过在系数即将穿过零时将变量从活动集中移除来强制执行此条件。纯 LARS 算法不强制执行此条件，允许系数改变符号。当发生符号变化时，推导的前提被违反，$\\ell_1$ 范数的单调性可能被破坏。\n\n### 第 2 和第 3 部分：算法实现与模拟\n\n我们实现两种同伦算法来追踪从$\\lambda = \\lambda_{\\max}$到$\\lambda \\approx 0$的解路径。\n1.  **纯 LARS**：此算法迭代地将变量添加到活动集中。在每一步，它都沿着“等角”方向移动，直到一个新变量与残差的相关性与活动变量的相关性相匹配。它从不移除变量，允许系数穿过零点。我们对这些符号变化事件进行计数。\n2.  **LARS-LASSO**：此算法修改了纯 LARS。除了进入事件，它还考虑活动系数的“过零”事件。步长被选择为触发进入或过零事件所需的最小值。如果发生过零事件，相应的变量将从活动集中移除，其系数固定为零。此修改确保活动集中的系数符号在事件之间保持恒定，并且生成的路径对应于 LASSO 解路径。我们对这些移除事件进行计数。\n\n对于每种算法，我们在每个事件（节点）$\\lambda_k$处生成一系列系数向量$\\hat{\\beta}(\\lambda_k)$。然后我们检查相应的$\\ell_1$范数序列$\\|\\hat{\\beta}(\\lambda_k)\\|_1$是否随着$\\lambda_k$值序列的减小而非递减。这等同于测试$\\|\\hat{\\beta}(\\lambda)\\|_1$是否随着$\\lambda$的增加而非递增。结果在三个指定的测试用例上进行验证。", "answer": "```python\nimport numpy as np\n\n# A small tolerance for floating-point comparisons\nTOL = 1e-12\n\ndef _run_lars_path(X, y, is_lasso):\n    \"\"\"\n    Computes the LARS or LARS-LASSO solution path.\n\n    Args:\n        X (np.ndarray): Standardized design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n        is_lasso (bool): If True, run LARS-LASSO; otherwise, run pure LARS.\n\n    Returns:\n        tuple: A tuple containing:\n            - is_monotonic (bool): True if the L1 norm is non-increasing with lambda.\n            - event_count (int): Count of sign changes (LARS) or drops (LARS-LASSO).\n    \"\"\"\n    n, p = X.shape\n    max_steps = 2 * (n + p) # Safety break for complex paths\n\n    # Initialization\n    beta = np.zeros(p)\n    betas_path = [beta.copy()]\n    l1_norms_path = [np.sum(np.abs(beta))]\n    \n    r = y.copy()\n    correlations = X.T @ r\n    \n    lambda_max = np.max(np.abs(correlations))\n    lambdas_path = [lambda_max]\n    \n    j_start = np.argmax(np.abs(correlations))\n    active_set = {j_start}\n    \n    sign_changes_count = 0\n    drop_events_count = 0\n\n    for _ in range(max_steps):\n        if not active_set or len(active_set) == p:\n            break\n\n        active_indices = sorted(list(active_set))\n        current_correlations = X.T @ (y - X @ beta)\n        \n        signs_A = np.sign(current_correlations[active_indices])\n        \n        X_A = X[:, active_indices]\n        try:\n            # Use pseudo-inverse for stability against near-collinearity\n            G_A_inv = np.linalg.pinv(X_A.T @ X_A)\n        except np.linalg.LinAlgError:\n            break\n\n        w_A = G_A_inv @ signs_A\n        a = X.T @ X_A @ w_A\n        \n        C = np.max(np.abs(current_correlations[active_indices]))\n\n        # --- Compute step size to next entry event ---\n        inactive_set = sorted(list(set(range(p)) - active_set))\n        gamma_entry = np.inf\n        \n        if inactive_set:\n            c_inactive = current_correlations[inactive_set]\n            a_inactive = a[inactive_set]\n            \n            with np.errstate(divide='ignore', invalid='ignore'):\n                g1 = (C - c_inactive) / (1 - a_inactive)\n                g2 = (C + c_inactive) / (1 + a_inactive)\n            \n            g1[np.isnan(g1) | (g1 = TOL)] = np.inf\n            g2[np.isnan(g2) | (g2 = TOL)] = np.inf\n            \n            gamma_entry = min(np.min(g1), np.min(g2))\n\n        # --- Compute step size to next zero-crossing event ---\n        gamma_zero = np.inf\n        beta_A = beta[active_indices]\n        \n        with np.errstate(divide='ignore', invalid='ignore'):\n            gammas_z = -beta_A / w_A\n        gammas_z[np.isnan(gammas_z) | (gammas_z = TOL)] = np.inf\n        if len(gammas_z) > 0:\n            gamma_zero = np.min(gammas_z)\n\n        # --- Select step size and event type ---\n        gamma = gamma_entry\n        if is_lasso:\n            gamma = min(gamma_entry, gamma_zero)\n            \n        if np.isinf(gamma):\n            break\n            \n        # Count sign changes for pure LARS\n        if not is_lasso:\n            cross_gammas = -beta_A / w_A\n            sign_changes_count += np.sum((cross_gammas > TOL)  (cross_gammas  gamma))\n\n        # --- Update coefficients and path ---\n        beta[active_indices] += gamma * w_A\n        betas_path.append(beta.copy())\n        l1_norms_path.append(np.sum(np.abs(beta)))\n        lambdas_path.append(C - gamma)\n        \n        # --- Update Active Set ---\n        if is_lasso and abs(gamma - gamma_zero)  TOL: # Drop event\n            drop_events_count += 1\n            to_drop_mask = np.isclose(gammas_z, gamma)\n            drop_indices = np.array(active_indices)[to_drop_mask]\n            for idx in drop_indices:\n                beta[idx] = 0.0\n                if idx in active_set:\n                    active_set.remove(idx)\n        else: # Entry event\n            new_corr = current_correlations - gamma * a\n            new_lambda = C - gamma\n            entering_mask = np.isclose(np.abs(new_corr), new_lambda, atol=TOL)\n            for idx in np.where(entering_mask)[0]:\n                if idx not in active_set:\n                    active_set.add(idx)\n\n    # Monotonicity check: L1 norm should increase as lambda decreases\n    l1_norms_arr = np.array(l1_norms_path)\n    is_monotonic = np.all(np.diff(l1_norms_arr) >= -TOL)\n\n    return is_monotonic, sign_changes_count if not is_lasso else drop_events_count\n\ndef _create_test_cases():\n    \"\"\"Generates the three deterministic test cases.\"\"\"\n    cases = []\n    \n    # Test Case 1: Orthonormal design\n    n, p = 20, 5\n    rng = np.random.default_rng(seed=1)\n    A_rand = rng.standard_normal((n, n))\n    Q, _ = np.linalg.qr(A_rand)\n    X1 = Q[:, :p]\n    X1 /= np.linalg.norm(X1, axis=0) # Should be redundant, but ensures spec\n    y1 = rng.standard_normal(n)\n    cases.append((X1, y1))\n\n    # Test Case 2: Moderately correlated design\n    n, p = 60, 12\n    rng = np.random.default_rng(seed=2)\n    X2 = rng.standard_normal((n, p))\n    X2 /= np.linalg.norm(X2, axis=0)\n    beta_star = np.zeros(p)\n    beta_star[:3] = [3, -5, 2]\n    epsilon = rng.standard_normal(n) * 0.1\n    y2 = X2 @ beta_star + epsilon\n    cases.append((X2, y2))\n    \n    # Test Case 3: Highly collinear design\n    n, p = 40, 6\n    rng = np.random.default_rng(seed=3)\n    X3 = rng.standard_normal((n, p))\n    X3[:, 1] = 0.99 * X3[:, 0] + 0.01 * rng.standard_normal(n)\n    X3[:, 2] = 1.01 * X3[:, 0] + 0.01 * rng.standard_normal(n)\n    X3 /= np.linalg.norm(X3, axis=0)\n    y3 = 2 * X3[:, 0] + 0.5 * X3[:, 1] - 1.5 * X3[:, 2] + rng.standard_normal(n) * 0.1\n    cases.append((X3, y3))\n    \n    return cases\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the result.\n    \"\"\"\n    test_cases = _create_test_cases()\n    results = []\n\n    for X, y in test_cases:\n        case_results = []\n        \n        # Pure LARS\n        is_lars_mono, lars_signs = _run_lars_path(X, y, is_lasso=False)\n        \n        # LARS-LASSO\n        is_lasso_mono, lasso_drops = _run_lars_path(X, y, is_lasso=True)\n\n        case_results.extend([\n            is_lars_mono, \n            int(lars_signs),\n            is_lasso_mono, \n            int(lasso_drops)\n        ])\n        results.append(case_results)\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3473500"}]}