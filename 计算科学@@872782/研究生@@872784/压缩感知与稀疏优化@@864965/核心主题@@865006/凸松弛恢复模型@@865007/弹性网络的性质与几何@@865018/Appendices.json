{"hands_on_practices": [{"introduction": "这项练习至关重要，它要求您推导出弹性网惩罚项的近端算子 (proximal operator) 的闭式解，这是许多现代优化算法（如近端梯度下降法）的核心组成部分。通过这一推导，您将揭示 $L_1$ 和 $L_2$ 惩罚项各自独立的几何效应——即软阈值化 (soft-thresholding) 和径向收缩 (radial shrinkage)，从而加深对弹性网工作原理的直观理解。[@problem_id:3469107]", "problem": "考虑定义在 $\\mathbb{R}^{n}$ 上的弹性网络惩罚项，它由凸函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 给出，具体形式为 $f(x)=\\lambda_{1}\\|x\\|_{1}+\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$，其中 $\\lambda_{1}0$ 和 $\\lambda_{2}0$。令 $\\tau0$ 并通过标准变分刻画来定义近端算子 $\\operatorname{prox}_{\\tau f}(y)$\n$$\n\\operatorname{prox}_{\\tau f}(y)=\\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{\\frac{1}{2}\\|x-y\\|_{2}^{2}+\\tau\\lambda_{1}\\|x\\|_{1}+\\frac{\\tau\\lambda_{2}}{2}\\|x\\|_{2}^{2}\\right\\}。\n$$\n使用次微分的定义和凸函数的一阶最优性条件，推导 $\\operatorname{prox}_{\\tau f}(y)$ 的闭式表达式，该表达式以软阈值算子 $S_{\\kappa}(z)$ 的形式按坐标表示，其中对于 $z\\in\\mathbb{R}$ 和 $\\kappa\\geq 0$，软阈值算子定义为 $S_{\\kappa}(z)=\\operatorname{sign}(z)\\max\\{|z|-\\kappa,0\\}$。然后，将注意力限制在 $n=2$ 的情况，并提供映射 $\\operatorname{prox}_{\\tau f}:\\mathbb{R}^{2}\\to\\mathbb{R}^{2}$ 的几何解释，将其解释为径向收缩（朝向原点的均匀缩放）和坐标级软阈值的复合，并精确说明这些运算的顺序和参数。\n\n最后，考虑不动点迭代 $x^{k+1}=T(x^{k})$，其中 $T=\\operatorname{prox}_{\\tau f}$。分析不动点集 $\\{x\\in\\mathbb{R}^{n}:x=T(x)\\}$，并利用 $T$ 的分段线性结构和 $S_{\\kappa}$ 的性质，确定 $T$ 在 $\\mathbb{R}^{n}$ 上的紧致全局利普希茨模数（最优收缩因子），该模数表示为 $\\tau$ 和 $\\lambda_{2}$ 的闭式解析函数。\n\n以精确符号形式报告 $T$ 的紧致全局利普希茨模数作为您的最终答案。无需四舍五入，不涉及单位。", "solution": "该问题要求推导和分析与弹性网络惩罚项相关的近端算子。求解过程分四步进行：(1) 推导近端算子的闭式表达式，(2) 在 $\\mathbb{R}^{2}$ 中的几何解释，(3) 分析算子的不动点，以及 (4) 确定其紧致全局利普希茨模数。\n\n**1. 近端算子的推导**\n\n弹性网络惩罚项由函数 $f(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$ 给出，其中 $x \\in \\mathbb{R}^{n}$，$\\lambda_{1}  0$ 且 $\\lambda_{2}  0$。对于 $\\tau  0$，近端算子 $\\operatorname{prox}_{\\tau f}(y)$ 定义为以下目标函数的唯一最小化子：\n$$\nx^* = \\operatorname{prox}_{\\tau f}(y) = \\arg\\min_{x\\in\\mathbb{R}^{n}}\\left\\{ G(x, y) = \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\tau f(x) \\right\\}.\n$$\n代入 $f(x)$ 的表达式，我们得到：\n$$\nG(x, y) = \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\tau\\lambda_{1}\\|x\\|_{1} + \\frac{\\tau\\lambda_{2}}{2}\\|x\\|_{2}^{2}.\n$$\n我们可以将范数的平方表示为各分量的和：\n$$\nG(x, y) = \\frac{1}{2}\\sum_{i=1}^{n}(x_i - y_i)^2 + \\tau\\lambda_{1}\\sum_{i=1}^{n}|x_i| + \\frac{\\tau\\lambda_{2}}{2}\\sum_{i=1}^{n}x_i^2.\n$$\n目标函数关于分量 $x_i$ 是可分的，这意味着我们可以写出 $G(x, y) = \\sum_{i=1}^{n} g_i(x_i, y_i)$，其中\n$$\ng_i(x_i, y_i) = \\frac{1}{2}(x_i - y_i)^2 + \\tau\\lambda_{1}|x_i| + \\frac{\\tau\\lambda_{2}}{2}x_i^2.\n$$\n因此，我们可以通过独立地最小化每个 $g_i(x_i, y_i)$ 来找到最小化子 $x^*$：\n$$\nx_i^* = \\arg\\min_{x_i\\in\\mathbb{R}} g_i(x_i, y_i).\n$$\n为了找到最小值，我们可以通过配方法重新排列 $g_i$ 中的项：\n\\begin{align*}\ng_i(x_i, y_i) = \\left(\\frac{1}{2} + \\frac{\\tau\\lambda_{2}}{2}\\right)x_i^2 - y_i x_i + \\tau\\lambda_{1}|x_i| + \\frac{1}{2}y_i^2 \\\\\n= \\frac{1+\\tau\\lambda_{2}}{2}\\left(x_i^2 - \\frac{2y_i}{1+\\tau\\lambda_{2}}x_i\\right) + \\tau\\lambda_{1}|x_i| + \\frac{1}{2}y_i^2 \\\\\n= \\frac{1+\\tau\\lambda_{2}}{2}\\left( \\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 - \\left(\\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 \\right) + \\tau\\lambda_{1}|x_i| + \\frac{1}{2}y_i^2 \\\\\n= \\frac{1+\\tau\\lambda_{2}}{2}\\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 + \\tau\\lambda_{1}|x_i| + C(y_i),\n\\end{align*}\n其中 $C(y_i)$ 包含不依赖于 $x_i$ 的项。关于 $x_i$ 最小化 $g_i(x_i, y_i)$ 等价于最小化\n$$\n\\tilde{g}_i(x_i) = \\frac{1+\\tau\\lambda_{2}}{2}\\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 + \\tau\\lambda_{1}|x_i|.\n$$\n两边除以正常数 $(1+\\tau\\lambda_{2})$，最小化问题变为：\n$$\nx_i^* = \\arg\\min_{x_i\\in\\mathbb{R}}\\left\\{ \\frac{1}{2}\\left(x_i - \\frac{y_i}{1+\\tau\\lambda_{2}}\\right)^2 + \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}|x_i| \\right\\}.\n$$\n这是函数 $z \\mapsto \\kappa|z|$（其中 $\\kappa = \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}$）在点 $\\frac{y_i}{1+\\tau\\lambda_{2}}$ 处求值的近端算子的定义。该算子是软阈值算子 $S_{\\kappa}(z) = \\operatorname{sign}(z)\\max\\{|z|-\\kappa,0\\}$。\n因此，对于每个坐标 $i$：\n$$\nx_i^* = S_{\\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}}\\left(\\frac{y_i}{1+\\tau\\lambda_{2}}\\right).\n$$\n以向量形式，近端算子由下式给出\n$$\n\\operatorname{prox}_{\\tau f}(y) = S_{\\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}}\\left(\\frac{1}{1+\\tau\\lambda_{2}}y\\right),\n$$\n其中软阈值算子是逐分量应用的。\n\n**2. 对于 $n=2$ 的几何解释**\n\n对于 $n=2$，映射 $T(y) = \\operatorname{prox}_{\\tau f}(y)$ 可以解释为两种不同几何运算的复合。令 $\\alpha = \\frac{1}{1+\\tau\\lambda_{2}}$ 且 $\\kappa = \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}$。该映射为 $T(y) = S_{\\kappa}(\\alpha y)$。由于 $\\tau > 0$ 且 $\\lambda_2 > 0$，我们有 $0  \\alpha  1$。\n从 $y \\in \\mathbb{R}^2$ 到 $T(y) \\in \\mathbb{R}^2$ 的变换分两步进行：\n1.  **均匀径向收缩：** 首先，向量 $y$ 按因子 $\\alpha$ 进行缩放。运算 $y \\mapsto \\alpha y$ 将向量 $y$ 沿着连接原点和 $y$ 的直线向原点均匀收缩。向量的长度变为 $\\alpha \\|y\\|_{2}$。\n2.  **逐分量软阈值：** 其次，软阈值算子 $S_{\\kappa}$ 应用于收缩后向量 $\\alpha y$ 的每个分量。对于向量 $z=(z_1, z_2)$，$S_{\\kappa}(z) = (S_{\\kappa}(z_1), S_{\\kappa}(z_2))$。此操作将每个分量向原点移动距离 $\\kappa$，如果其绝对值小于或等于 $\\kappa$，则将其设为零。这是一个非均匀变换，可以改变向量的方向并引入稀疏性（零分量）。\n\n因此，映射 $\\operatorname{prox}_{\\tau f}$ 是径向收缩后跟逐分量软阈值的复合。\n\n**3. 不动点分析**\n\n我们对不动点集 $\\{x \\in \\mathbb{R}^n : x=T(x)\\}$ 感兴趣，其中 $T(x) = \\operatorname{prox}_{\\tau f}(x)$。\n根据近端算子的一阶最优性条件，一个点 $x^*$ 是 $\\operatorname{prox}_{\\tau f}$ 的不动点，当且仅当它是 $f$ 的一个最小化子。\n$x^* = \\operatorname{prox}_{\\tau f}(x^*) \\iff x^* = \\arg\\min_x \\left\\{\\frac{1}{2}\\|x-x^*\\|^2_2 + \\tau f(x) \\right\\}$。\n该最小化问题的最优性条件是 $0 \\in \\partial_x \\left(\\frac{1}{2}\\|x-x^*\\|^2_2 + \\tau f(x)\\right)\\Big|_{x=x^*}$，化简为 $0 \\in (x^*-x^*) + \\tau\\partial f(x^*)$，即 $0 \\in \\partial f(x^*)$。\n这正是 $x^*$ 是凸函数 $f$ 的最小化子的条件。\n\n现在我们求 $f(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$ 的最小化子。$f$ 的次微分为 $\\partial f(x) = \\lambda_1 \\partial\\|x\\|_1 + \\lambda_2 x$。最优性条件 $0 \\in \\partial f(x^*)$ 等价于 $-\\lambda_2 x^* \\in \\lambda_1 \\partial\\|x^*\\|_1$。\n这是一个逐坐标的条件：对于每个 $i \\in \\{1,...,n\\}$，$-\\lambda_2 x_i^* \\in \\lambda_1 \\partial|x_i^*|$。\n- 如果 $x_i^* \\ne 0$，则 $\\partial|x_i^*| = \\{\\operatorname{sign}(x_i^*)\\}$，所以必须有 $-\\lambda_2 x_i^* = \\lambda_1 \\operatorname{sign}(x_i^*)$。取两边的绝对值得到 $\\lambda_2 |x_i^*| = \\lambda_1$，所以 $|x_i^*| = \\lambda_1 / \\lambda_2$。代回原式，$-\\lambda_2 x_i^* = \\lambda_1 (x_i^*/|x_i^*|) = \\lambda_1 x_i^* / (\\lambda_1/\\lambda_2) = \\lambda_2 x_i^*$。这意味着 $2\\lambda_2 x_i^* = 0$，即 $x_i^*=0$，这与我们的假设 $x_i^* \\ne 0$ 矛盾。\n- 如果 $x_i^* = 0$，条件变为 $0 \\in \\lambda_1 [-1, 1]$。因为 $\\lambda_1 > 0$，这等价于 $0 \\in [-\\lambda_1, \\lambda_1]$，这是成立的。\n\n因此，唯一的可能性是对于所有 $i=1,\\dots,n$，$x_i^*=0$。唯一的最小化子是 $x^*=0$。因此，算子 $T = \\operatorname{prox}_{\\tau f}$ 的唯一不动点是原点 $x=0$。\n\n**4. 紧致全局利普希茨模数**\n\n算子是 $T(y) = S_{\\kappa}(\\alpha y)$，其中 $\\alpha = \\frac{1}{1+\\tau\\lambda_{2}}$ 且 $\\kappa = \\frac{\\tau\\lambda_{1}}{1+\\tau\\lambda_{2}}$。我们希望找到 $T$ 关于欧几里得范数 $\\|\\cdot\\|_2$ 的紧致全局利普希茨模数 $L$，定义为\n$$\nL = \\sup_{y,z \\in \\mathbb{R}^n, y \\ne z} \\frac{\\|T(y)-T(z)\\|_2}{\\|y-z\\|_2}.\n$$\n算子 $T$ 几乎处处可微。其雅可比矩阵 $JT(y)$ 是对角的，因为 $T_i(y) = S_\\kappa(\\alpha y_i)$ 仅依赖于 $y_i$。对角线元素为：\n$$\n(JT(y))_{ii} = \\frac{\\partial T_i}{\\partial y_i} = \\frac{d}{dy_i} S_\\kappa(\\alpha y_i) = \\alpha \\cdot S'_\\kappa(\\alpha y_i).\n$$\n标量软阈值函数 $S_\\kappa(u)$ 的导数为：\n$$\nS'_\\kappa(u) = \\begin{cases} 1  \\text{若 } |u| > \\kappa \\\\ 0  \\text{若 } |u|  \\kappa \\end{cases}\n$$\n在 $|u|=\\kappa$ 处未定义。因此，雅可比矩阵的对角线元素要么是 $\\alpha$（如果 $|\\alpha y_i| > \\kappa$），要么是 $0$（如果 $|\\alpha y_i|  \\kappa$）。\n一个分段可微映射的利普希茨模数 $L$ 是其雅可比矩阵谱范数在其定义域上的上确界。对角矩阵的谱范数是其对角元素绝对值的最大值。\n$$\n\\|JT(y)\\|_2 = \\max_{i} |(JT(y))_{ii}| = \\max_{i} |\\alpha \\cdot S'_\\kappa(\\alpha y_i)| \\le \\alpha.\n$$\n这表明 $L \\le \\alpha$。为了证明这个界是紧的，我们必须证明它可以被达到。考虑两点 $y$ 和 $z$，使得对于某个分量 $j$，有 $|\\alpha y_j| > \\kappa$ 和 $|\\alpha z_j| > \\kappa$，且所有其他分量为零。例如，令 $y = (M, 0, \\dots, 0)$ 和 $z = (M+\\delta, 0, \\dots, 0)$，其中 $\\delta>0$。选择足够大的 $M$ 使得 $\\alpha M > \\kappa$。那么 $\\alpha(M+\\delta) > \\kappa$ 也成立。\n输出为：\n$$\nT(y) = (S_\\kappa(\\alpha M), 0, \\dots, 0) = (\\alpha M - \\kappa, 0, \\dots, 0)\n$$\n$$\nT(z) = (S_\\kappa(\\alpha(M+\\delta)), 0, \\dots, 0) = (\\alpha(M+\\delta) - \\kappa, 0, \\dots, 0)\n$$\n输出之间的距离是：\n$$\n\\|T(y)-T(z)\\|_2 = \\|((\\alpha M - \\kappa) - (\\alpha M + \\alpha\\delta - \\kappa), 0, \\dots, 0)\\|_2 = \\|(-\\alpha\\delta, 0, \\dots, 0)\\|_2 = \\alpha\\delta.\n$$\n输入之间的距离是：\n$$\n\\|y-z\\|_2 = \\|(M-(M+\\delta), 0, \\dots, 0)\\|_2 = \\|(-\\delta, 0, \\dots, 0)\\|_2 = \\delta.\n$$\n该比值为 $\\frac{\\|T(y)-T(z)\\|_2}{\\|y-z\\|_2} = \\frac{\\alpha\\delta}{\\delta} = \\alpha$。\n因为我们已经证明了 $L \\le \\alpha$ 并且找到了一个比率恰好为 $\\alpha$ 的情况，所以紧致全局利普希茨模数为 $L=\\alpha$。\n代入 $\\alpha$ 的表达式：\n$$\nL = \\frac{1}{1+\\tau\\lambda_{2}}.\n$$\n该表达式是 $\\tau$ 和 $\\lambda_2$ 的函数，符合要求。", "answer": "$$\\boxed{\\frac{1}{1+\\tau\\lambda_{2}}}$$", "id": "3469107"}, {"introduction": "从优化机制转向统计特性，本练习探讨了弹性网的模型复杂度。通过在正交设定下计算自由度 (degrees of freedom)，您将精确地看到正则化参数 $\\lambda_1$ 和 $\\lambda_2$ 如何共同控制模型中的“有效参数数量”。这个概念对于后续的模型选择（如使用 AIC 或 BIC 标准）和对估计器行为的理解至关重要。[@problem_id:3469093]", "problem": "考虑一个观测向量 $y \\in \\mathbb{R}^{n}$ 和一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其列满足标准正交条件 $X^{\\top} X = n I_{p}$。对于正则化参数 $\\lambda_{1} \\geq 0$ 和 $\\lambda_{2} \\geq 0$，将弹性网络估计量 $\\hat{\\beta}(y) \\in \\mathbb{R}^{p}$ 定义为以下凸目标函数的任意最小化子：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}.\n$$\n令拟合值为 $\\hat{y}(y) = X \\hat{\\beta}(y) \\in \\mathbb{R}^{n}$。假设 $y$ 处于一个一般位置，使得对于 $z = X^{\\top} y / n$，所有坐标都满足 $|z_{j}| \\neq \\lambda_{1}$（对于所有 $j \\in \\{1,\\dots,p\\}$）。对于一个几乎处处可微的映射 $y \\mapsto \\hat{y}(y)$，使用基于散度的自由度定义：\n$$\n\\mathrm{df}(y) \\equiv \\operatorname{tr}\\!\\left( \\frac{\\partial \\hat{y}(y)}{\\partial y} \\right),\n$$\n在标准正交条件 $X^{\\top} X = n I_{p}$ 下，推导 $\\mathrm{df}(y)$ 的一个显式闭式表达式，该表达式用 $z$ 以及参数 $\\lambda_{1}, \\lambda_{2}$ 表示。您的最终答案必须是只包含 $z$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单个闭式解析表达式，并且不得包含任何未消除的导数或极限。无需四舍五入。", "solution": "本问题旨在特定条件下，为弹性网络估计量的自由度 $\\mathrm{df}(y)$ 推导一个闭式表达式。自由度定义为 $\\mathrm{df}(y) \\equiv \\operatorname{tr}\\!\\left( \\frac{\\partial \\hat{y}(y)}{\\partial y} \\right)$，其中 $\\hat{y}(y) = X \\hat{\\beta}(y)$ 是拟合值。\n\n首先，我们分析需要关于 $\\beta \\in \\mathbb{R}^{p}$ 最小化的弹性网络目标函数：\n$$\nL(\\beta) = \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}\n$$\n最小二乘项可以展开为：\n$$\n\\|y - X \\beta\\|_{2}^{2} = (y - X \\beta)^{\\top}(y - X \\beta) = y^{\\top}y - 2 y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta\n$$\n我们已知标准正交条件 $X^{\\top} X = n I_{p}$，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。将此代入目标函数可得：\n$$\nL(\\beta) = \\frac{1}{2n} (y^{\\top}y - 2 y^{\\top}X\\beta + n \\beta^{\\top}\\beta) + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}\n$$\n重新排列并分配 $\\frac{1}{2n}$ 项：\n$$\nL(\\beta) = \\frac{1}{2n} y^{\\top}y - \\frac{1}{n} y^{\\top}X\\beta + \\frac{1}{2} \\beta^{\\top}\\beta + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\beta^{\\top}\\beta\n$$\n令 $z = X^{\\top} y / n$。项 $y^{\\top}X\\beta$ 可以写成 $(X^{\\top}y)^{\\top}\\beta = (nz)^{\\top}\\beta = n z^{\\top}\\beta$。项 $\\frac{1}{2n} y^{\\top}y$ 是关于 $\\beta$ 的常数，不影响最小化过程。我们可以将目标函数写为（忽略一个加性常数）：\n$$\nL(\\beta) \\propto -z^{\\top}\\beta + \\frac{1}{2} \\|\\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}\n$$\n合并 $\\|\\beta\\|_{2}^{2}$ 项：\n$$\nL(\\beta) \\propto \\frac{1+\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2} - z^{\\top}\\beta + \\lambda_{1} \\|\\beta\\|_{1}\n$$\n这个目标函数关于 $\\beta$ 的各个分量是可分的。也就是说，$L(\\beta) \\propto \\sum_{j=1}^{p} L_j(\\beta_j)$，其中\n$$\nL_j(\\beta_j) = \\frac{1+\\lambda_{2}}{2} \\beta_{j}^{2} - z_{j}\\beta_{j} + \\lambda_{1} |\\beta_{j}|\n$$\n为了找到最小化子 $\\hat{\\beta}_j$，我们使用次梯度法。$L_j$ 关于 $\\beta_j$ 的次梯度是：\n$$\n\\partial_{\\beta_j} L_j(\\beta_j) = (1+\\lambda_{2})\\beta_{j} - z_{j} + \\lambda_{1} \\partial |\\beta_{j}|\n$$\n其中 $\\partial |\\beta_{j}|$ 是绝对值函数的次梯度：如果 $\\beta_j \\neq 0$，则为 $\\operatorname{sign}(\\beta_j)$；如果 $\\beta_j = 0$，则为区间 $[-1, 1]$。在最小值 $\\hat{\\beta}_j$ 处，次梯度必须包含 $0$。\n$$\n0 \\in (1+\\lambda_{2})\\hat{\\beta}_{j} - z_{j} + \\lambda_{1} \\operatorname{sign}(\\hat{\\beta}_j)\n$$\n这可以重写为 $z_j \\in (1+\\lambda_{2})\\hat{\\beta}_{j} + \\lambda_{1} \\operatorname{sign}(\\hat{\\beta}_j)$。\n我们分情况讨论：\n1. 如果 $\\hat{\\beta}_j > 0$，那么 $\\operatorname{sign}(\\hat{\\beta}_j) = 1$，且 $z_j = (1+\\lambda_{2})\\hat{\\beta}_{j} + \\lambda_{1}$。这得到 $\\hat{\\beta}_j = \\frac{z_j - \\lambda_{1}}{1+\\lambda_{2}}$。这仅在 $\\hat{\\beta}_j > 0$ 时成立，即要求 $z_j > \\lambda_{1}$。\n2. 如果 $\\hat{\\beta}_j  0$，那么 $\\operatorname{sign}(\\hat{\\beta}_j) = -1$，且 $z_j = (1+\\lambda_{2})\\hat{\\beta}_{j} - \\lambda_{1}$。这得到 $\\hat{\\beta}_j = \\frac{z_j + \\lambda_{1}}{1+\\lambda_{2}}$。这仅在 $\\hat{\\beta}_j  0$ 时成立，即要求 $z_j  -\\lambda_{1}$。\n3. 如果 $\\hat{\\beta}_j = 0$，那么 $z_j \\in \\lambda_{1} [-1, 1]$，这意味着 $|z_j| \\leq \\lambda_{1}$。\n\n这三种情况可以使用软阈值算子紧凑地写出，其定义为 $S_{\\alpha}(x) = \\operatorname{sign}(x) \\max(|x|-\\alpha, 0)$。解是：\n$$\n\\hat{\\beta}_{j} = \\frac{1}{1+\\lambda_{2}} S_{\\lambda_{1}}(z_{j})\n$$\n问题陈述中说明对于所有 $j$，有 $|z_j| \\neq \\lambda_1$。这意味着我们远离软阈值函数的不可微点。\n\n自由度为 $\\mathrm{df}(y) = \\operatorname{tr}\\!\\left( \\frac{\\partial \\hat{y}(y)}{\\partial y} \\right)$。由于 $\\hat{y} = X\\hat{\\beta}$，我们有 $\\frac{\\partial \\hat{y}}{\\partial y} = X \\frac{\\partial \\hat{\\beta}}{\\partial y}$。自由度为：\n$$\n\\mathrm{df}(y) = \\operatorname{tr}\\!\\left( X \\frac{\\partial \\hat{\\beta}}{\\partial y} \\right)\n$$\n我们使用链式法则来计算 $\\frac{\\partial \\hat{\\beta}}{\\partial y}$。我们来求 $\\hat{\\beta}$ 关于 $y$ 的雅可比矩阵：\n$$\n\\frac{\\partial \\hat{\\beta}}{\\partial y} = \\frac{\\partial \\hat{\\beta}}{\\partial z} \\frac{\\partial z}{\\partial y}\n$$\n项 $\\frac{\\partial z}{\\partial y}$ 从定义 $z = \\frac{1}{n}X^{\\top}y$ 得到：\n$$\n\\frac{\\partial z}{\\partial y} = \\frac{1}{n}X^{\\top}\n$$\n这是一个 $p \\times n$ 的矩阵。项 $\\frac{\\partial \\hat{\\beta}}{\\partial z}$ 是一个 $p \\times p$ 的雅可比矩阵。由于每个分量 $\\hat{\\beta}_j$ 只依赖于对应的分量 $z_j$，这个矩阵是对角矩阵：\n$$\n\\left(\\frac{\\partial \\hat{\\beta}}{\\partial z}\\right)_{jk} = \\frac{\\partial \\hat{\\beta}_j}{\\partial z_k} = \\delta_{jk} \\frac{d\\hat{\\beta}_j}{dz_j}\n$$\n其中 $\\delta_{jk}$ 是克罗内克 δ 符号。我们计算导数 $\\frac{d\\hat{\\beta}_j}{dz_j}$：\n$$\n\\frac{d\\hat{\\beta}_j}{dz_j} = \\frac{d}{dz_j} \\left( \\frac{1}{1+\\lambda_{2}} S_{\\lambda_{1}}(z_{j}) \\right) = \\frac{1}{1+\\lambda_{2}} \\frac{d}{dz_j} S_{\\lambda_{1}}(z_{j})\n$$\n软阈值函数 $S_{\\lambda_1}(z_j)$ 的导数在 $|z_j| > \\lambda_1$ 时为 $1$，在 $|z_j|  \\lambda_1$ 时为 $0$。由于问题假设 $|z_j| \\neq \\lambda_1$，导数是良定义的，并由指示函数 $I(|z_j| > \\lambda_1)$ 给出。\n$$\n\\frac{d\\hat{\\beta}_j}{dz_j} = \\frac{1}{1+\\lambda_{2}} I(|z_j| > \\lambda_1)\n$$\n设 $D$ 是一个对角矩阵，其元素为 $D_{jj} = \\frac{1}{1+\\lambda_{2}} I(|z_j| > \\lambda_1)$。那么 $\\frac{\\partial \\hat{\\beta}}{\\partial z} = D$。\n将这些雅可比矩阵代回到 $\\frac{\\partial \\hat{\\beta}}{\\partial y}$ 的表达式中：\n$$\n\\frac{\\partial \\hat{\\beta}}{\\partial y} = D \\left( \\frac{1}{n}X^{\\top} \\right) = \\frac{1}{n} D X^{\\top}\n$$\n现在我们计算自由度：\n$$\n\\mathrm{df}(y) = \\operatorname{tr}\\!\\left( X \\left( \\frac{1}{n} D X^{\\top} \\right) \\right) = \\frac{1}{n} \\operatorname{tr}\\!\\left( X D X^{\\top} \\right)\n$$\n使用迹的循环性质 $\\operatorname{tr}(ABC) = \\operatorname{tr}(CAB)$，我们有：\n$$\n\\operatorname{tr}\\!\\left( X D X^{\\top} \\right) = \\operatorname{tr}\\!\\left( X^{\\top} X D \\right)\n$$\n使用给定的条件 $X^{\\top}X = n I_p$：\n$$\n\\operatorname{tr}\\!\\left( X^{\\top} X D \\right) = \\operatorname{tr}\\!\\left( n I_p D \\right) = n \\operatorname{tr}(D)\n$$\n对角矩阵 $D$ 的迹是其对角元素之和：\n$$\n\\operatorname{tr}(D) = \\sum_{j=1}^{p} D_{jj} = \\sum_{j=1}^{p} \\frac{1}{1+\\lambda_{2}} I(|z_j| > \\lambda_1) = \\frac{1}{1+\\lambda_{2}} \\sum_{j=1}^{p} I(|z_j| > \\lambda_1)\n$$\n最后，将此代回到 $\\mathrm{df}(y)$ 的表达式中：\n$$\n\\mathrm{df}(y) = \\frac{1}{n} (n \\operatorname{tr}(D)) = \\operatorname{tr}(D)\n$$\n因此，自由度为：\n$$\n\\mathrm{df}(y) = \\frac{1}{1+\\lambda_{2}} \\sum_{j=1}^{p} I(|z_{j}| > \\lambda_{1})\n$$\n项 $\\sum_{j=1}^{p} I(|z_{j}| > \\lambda_{1})$ 表示非零系数 $\\hat{\\beta}_j$ 的数量，即活动集的大小。", "answer": "$$\n\\boxed{\\frac{1}{1+\\lambda_{2}} \\sum_{j=1}^{p} I(|z_{j}| > \\lambda_{1})}\n$$", "id": "3469093"}, {"introduction": "这个问题从一个不同的角度审视弹性网：将其视为一个约束优化问题，而非惩罚问题。您将利用 Karush-Kuhn-Tucker (KKT) 条件来推导到弹性网球上的投影算子。您会发现其解的结构与近端算子惊人地相似，这揭示了惩罚形式和约束形式之间的深刻对偶关系，并从另一个角度加深了对 $L_1$ 和 $L_2$ 约束之间几何相互作用的理解。[@problem_id:3469123]", "problem": "考虑欧几里得空间中的投影问题：给定一个向量 $y \\in \\mathbb{R}^{n}$ 和参数 $\\lambda_{1}  0$、$\\lambda_{2}  0$ 以及 $t  0$，定义弹性网球 $\\mathcal{C} = \\{x \\in \\mathbb{R}^{n} : \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\le t\\}$。$y$ 在 $\\mathcal{C}$ 上的投影是以下凸优化问题的唯一解 $x^{\\star}$：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\le t.\n$$\n任务：\n- 从拉格朗日构造和 $\\ell_{1}$-范数的次微分演算开始，利用凸优化中的 Karush–Kuhn–Tucker 条件，推导此投影的充要最优性条件。证明最优解可以通过一个由拉格朗日乘子 $\\nu \\ge 0$ 参数化的软阈值操作，然后进行一个取决于 $\\nu$ 和 $\\lambda_{2}$ 的均匀缩放，来按分量表示。\n- 为该投影映射提供一个几何解释，将其视为作用于 $y$ 的两个步骤的复合：一个诱导稀疏性的收缩和一个二次项诱导的缩放，每个步骤都由相同的乘子 $\\nu$ 控制。\n- 然后，特化到 $n = 3$ 的情况，其中 $y = (3, \\tfrac{3}{2}, \\tfrac{1}{5})^{\\top}$，$\\lambda_{1} = 1$，$\\lambda_{2} = 1$，且 $t = \\tfrac{57}{32}$。计算与解处的激活约束相关联的唯一拉格朗日乘子 $\\nu^{\\star}$。请以精确值的形式给出你的答案。如果需要近似值，说明中会指定有效数字，但此处要求精确值。", "solution": "该问题要求推导将向量 $y \\in \\mathbb{R}^{n}$ 投影到弹性网球 $\\mathcal{C}$ 上的最优性条件，给出投影的几何解释，并为特定实例计算相关的拉格朗日乘子。\n\n优化问题是\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} \\quad \\text{subject to} \\quad \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} \\le t\n$$\n其中 $\\lambda_1  0$，$\\lambda_2  0$，且 $t  0$。\n\n### 第 1 部分：最优性条件和解形式的推导\n\n目标函数 $f_0(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2}$ 是严格凸的。约束由函数 $f_1(x) = \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} - t$ 定义。$\\ell_1$-范数 $\\|x\\|_1$ 和平方 $\\ell_2$-范数 $\\|x\\|_2^2$ 都是凸函数。由于 $\\lambda_1  0$ 和 $\\lambda_2  0$，它们的正线性组合 $f_1(x)+t$ 也是凸的，这使得约束集 $\\mathcal{C} = \\{x \\mid f_1(x) \\le 0\\}$ 成为一个凸集。这是一个凸优化问题。\n\n对于 $t>0$，点 $x=0$ 是严格可行的，因为 $f_1(0) = -t  0$。因此，Slater 条件成立，Karush–Kuhn–Tucker (KKT) 条件对于最优性是充要的。\n\n拉格朗日函数是\n$$\nL(x, \\nu) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\nu \\left( \\lambda_{1}\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} - t \\right)\n$$\n其中 $\\nu \\ge 0$ 是不等式约束的拉格朗日乘子。\n\n对于最优解 $x^{\\star}$ 和乘子 $\\nu^{\\star}$ 的 KKT 条件是：\n1.  **平稳性 (Stationarity)**：$0 \\in \\nabla_x L(x^{\\star}, \\nu^{\\star})$\n2.  **原始可行性 (Primal feasibility)**：$\\lambda_{1}\\|x^{\\star}\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x^{\\star}\\|_{2}^{2} - t \\le 0$\n3.  **对偶可行性 (Dual feasibility)**：$\\nu^{\\star} \\ge 0$\n4.  **互补松弛性 (Complementary slackness)**：$\\nu^{\\star} \\left( \\lambda_{1}\\|x^{\\star}\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x^{\\star}\\|_{2}^{2} - t \\right) = 0$\n\n目标函数是可微的，但 $\\ell_1$-范数不是。我们使用次微分演算。$L(x, \\nu)$ 关于 $x$ 的次微分是：\n$$\n\\partial_x L(x, \\nu) = (x - y) + \\nu \\lambda_1 \\partial\\|x\\|_1 + \\nu \\lambda_2 x\n$$\n其中 $\\partial\\|x\\|_1$ 是 $\\ell_1$-范数的次微分。平稳性条件 $0 \\in \\partial_x L(x^{\\star}, \\nu^{\\star})$ 变为：\n$$\n0 \\in (x^{\\star} - y) + \\nu^{\\star} \\lambda_1 \\partial\\|x^{\\star}\\|_1 + \\nu^{\\star} \\lambda_2 x^{\\star}\n$$\n整理得：\n$$\ny \\in (1 + \\nu^{\\star}\\lambda_2)x^{\\star} + \\nu^{\\star}\\lambda_1 \\partial\\|x^{\\star}\\|_1\n$$\n对于 $i=1, \\dots, n$，这必须按分量成立：\n$$\ny_i \\in (1 + \\nu^{\\star}\\lambda_2)x^{\\star}_i + \\nu^{\\star}\\lambda_1 \\partial|x^{\\star}_i|\n$$\n其中当 $z \\neq 0$ 时，$\\partial|z|$ 是 $\\text{sign}(z)$，当 $z=0$ 时，是区间 $[-1, 1]$。\n\n我们分析这个分量关系：\n- 如果 $x^{\\star}_i > 0$，则 $\\partial|x^{\\star}_i| = \\{1\\}$，所以 $y_i = (1 + \\nu^{\\star}\\lambda_2)x^{\\star}_i + \\nu^{\\star}\\lambda_1$。这意味着 $x^{\\star}_i = \\frac{y_i - \\nu^{\\star}\\lambda_1}{1 + \\nu^{\\star}\\lambda_2}$。为使 $x^{\\star}_i$ 为正，我们必须有 $y_i > \\nu^{\\star}\\lambda_1$。\n- 如果 $x^{\\star}_i  0$，则 $\\partial|x^{\\star}_i| = \\{-1\\}$，所以 $y_i = (1 + \\nu^{\\star}\\lambda_2)x^{\\star}_i - \\nu^{\\star}\\lambda_1$。这意味着 $x^{\\star}_i = \\frac{y_i + \\nu^{\\star}\\lambda_1}{1 + \\nu^{\\star}\\lambda_2}$。为使 $x^{\\star}_i$ 为负，我们必须有 $y_i  -\\nu^{\\star}\\lambda_1$。\n- 如果 $x^{\\star}_i = 0$，则 $\\partial|x^{\\star}_i| = [-1, 1]$，所以 $y_i \\in (1 + \\nu^{\\star}\\lambda_2)(0) + \\nu^{\\star}\\lambda_1 [-1, 1]$。这意味着 $|y_i| \\le \\nu^{\\star}\\lambda_1$。\n\n这三种情况可以使用软阈值算子 $S_\\alpha(z) = \\text{sign}(z)\\max(|z|-\\alpha, 0)$ 紧凑地写出。每个分量的解是：\n$$\nx^{\\star}_i = \\frac{1}{1 + \\nu^{\\star}\\lambda_2} S_{\\nu^{\\star}\\lambda_1}(y_i)\n$$\n以向量形式，最优解是拉格朗日乘子 $\\nu^\\star$ 的函数：\n$$\nx^{\\star} = \\frac{1}{1 + \\nu^{\\star}\\lambda_2} S_{\\nu^{\\star}\\lambda_1}(y)\n$$\n$\\nu^\\star$ 的值由 KKT 条件确定。如果 $y \\in \\mathcal{C}$，则 $x^\\star = y$ 且 $\\nu^\\star=0$。如果 $y \\notin \\mathcal{C}$，则约束必须是激活的，所以 $\\nu^\\star>0$ 是方程 $\\lambda_{1}\\|x(\\nu)\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x(\\nu)\\|_{2}^{2} = t$ 的唯一正根。\n\n### 第 2 部分：几何解释\n\n解 $x^{\\star} = \\frac{1}{1+\\nu^{\\star}\\lambda_2} S_{\\nu^{\\star}\\lambda_1}(y)$ 揭示了一个两步过程：\n1.  **诱导稀疏性的收缩**：首先，向量 $y$ 由 $S_{\\nu^{\\star}\\lambda_1}(\\cdot)$ 操作。这是软阈值函数，它是 Lasso（最小绝对收缩和选择算子）回归的核心。它执行两个动作：将 $y$ 的每个分量向零收缩一个量 $\\nu^{\\star}\\lambda_1$，并将任何幅值小于阈值 $\\nu^{\\star}\\lambda_1$ 的分量精确地设置为零。这一步促进了稀疏解。\n2.  **二次项诱导的缩放**：其次，得到的收缩后的向量 $S_{\\nu^{\\star}\\lambda_1}(y)$ 被一个因子 $\\frac{1}{1+\\nu^{\\star}\\lambda_2}$ 均匀缩放。由于 $\\nu^{\\star} \\ge 0$ 和 $\\lambda_2  0$，这个因子总是在 $(0, 1]$ 区间内。这种缩放是一种收缩形式，让人联想到岭回归，它惩罚大的系数值。\n\n本质上，投影到弹性网球上在几何上对应于一个 Lasso 型的收缩和稀疏化，然后是一个岭回归型的均匀收缩。这两个操作都由同一个拉格朗日乘子 $\\nu^\\star$ 控制，它在到 $y$ 的距离与弹性网约束之间取得平衡。\n\n### 第 3 部分：特定案例的计算\n\n我们给定 $n=3$，$y = (3, \\frac{3}{2}, \\frac{1}{5})^{\\top}$，$\\lambda_1 = 1$，$\\lambda_2 = 1$，以及 $t = \\frac{57}{32}$。\n约束为 $\\|x\\|_1 + \\frac{1}{2}\\|x\\|_2^2 \\le \\frac{57}{32}$。\n\n首先，我们检查 $y$ 是否已经在可行集 $\\mathcal{C}$ 中。\n$\\|y\\|_1 = |3| + |\\frac{3}{2}| + |\\frac{1}{5}| = 3 + \\frac{3}{2} + \\frac{1}{5} = \\frac{30+15+2}{10} = \\frac{47}{10}$。\n$\\|y\\|_2^2 = 3^2 + (\\frac{3}{2})^2 + (\\frac{1}{5})^2 = 9 + \\frac{9}{4} + \\frac{1}{25} = \\frac{900+225+4}{100} = \\frac{1129}{100}$。\n在 $y$ 处计算的约束函数值为：\n$\\|y\\|_1 + \\frac{1}{2}\\|y\\|_2^2 = \\frac{47}{10} + \\frac{1}{2}\\left(\\frac{1129}{100}\\right) = \\frac{940}{200} + \\frac{1129}{200} = \\frac{2069}{200} = 10.345$。\n约束上限是 $t = \\frac{57}{32} = 1.78125$。\n由于 $10.345 > 1.78125$，点 $y$ 在 $\\mathcal{C}$ 之外。因此，投影 $x^\\star$ 必须位于 $\\mathcal{C}$ 的边界上，这意味着约束是激活的：$\\|x^\\star\\|_1 + \\frac{1}{2}\\|x^\\star\\|_2^2 = t$。根据互补松弛性，我们必须有 $\\nu^\\star > 0$。\n\n解为 $x^\\star = \\frac{1}{1+\\nu^\\star} S_{\\nu^\\star}(y)$。我们需要找到满足激活约束方程 $g(\\nu) = \\|\\frac{1}{1+\\nu} S_{\\nu}(y)\\|_1 + \\frac{1}{2}\\|\\frac{1}{1+\\nu} S_{\\nu}(y)\\|_2^2 = \\frac{57}{32}$ 的 $\\nu^\\star > 0$。\n$y$ 的分量是 $y_1=3$，$y_2=1.5$，$y_3=0.2$。$S_\\nu(y)$ 的形式取决于 $\\nu$ 的值。\n让我们在 $\\nu$ 的区间上分析 $g(\\nu)$：\n阈值是 $0.2$，$1.5$，$3$。\n让我们测试区间 $0.2  \\nu \\le 1.5$。在此范围内，$y_3$ 被阈值化为零，但 $y_1$ 和 $y_2$ 不是。\n$S_\\nu(y) = (3-\\nu, \\frac{3}{2}-\\nu, 0)^\\top$。\n$\\|S_\\nu(y)\\|_1 = (3-\\nu) + (\\frac{3}{2}-\\nu) = \\frac{9}{2}-2\\nu$。\n$\\|S_\\nu(y)\\|_2^2 = (3-\\nu)^2 + (\\frac{3}{2}-\\nu)^2 = (9 - 6\\nu + \\nu^2) + (\\frac{9}{4} - 3\\nu + \\nu^2) = \\frac{45}{4} - 9\\nu + 2\\nu^2$。\n\n约束方程变为：\n$$\n\\frac{\\frac{9}{2}-2\\nu}{1+\\nu} + \\frac{1}{2(1+\\nu)^2} \\left( \\frac{45}{4} - 9\\nu + 2\\nu^2 \\right) = \\frac{57}{32}\n$$\n为简化，我们将左边的项合并：\n$$\n\\frac{2(1+\\nu)(\\frac{9}{2}-2\\nu) + (\\frac{45}{4} - 9\\nu + 2\\nu^2)}{2(1+\\nu)^2} = \\frac{57}{32}\n$$\n分子是 $2(\\frac{9}{2} + \\frac{5}{2}\\nu - 2\\nu^2) + \\frac{45}{4} - 9\\nu + 2\\nu^2 = 9 + 5\\nu - 4\\nu^2 + \\frac{45}{4} - 9\\nu + 2\\nu^2 = \\frac{81}{4} - 4\\nu - 2\\nu^2$。\n所以，方程为：\n$$\n\\frac{\\frac{81}{4} - 4\\nu - 2\\nu^2}{2(1+2\\nu+\\nu^2)} = \\frac{57}{32} \\implies \\frac{\\frac{81}{4} - 4\\nu - 2\\nu^2}{1+2\\nu+\\nu^2} = \\frac{57}{16}\n$$\n交叉相乘：\n$$\n16 \\left( \\frac{81}{4} - 4\\nu - 2\\nu^2 \\right) = 57(1+2\\nu+\\nu^2)\n$$\n$$\n4 \\times 81 - 64\\nu - 32\\nu^2 = 57 + 114\\nu + 57\\nu^2\n$$\n$$\n324 - 64\\nu - 32\\nu^2 = 57 + 114\\nu + 57\\nu^2\n$$\n整理成标准二次型 $ax^2+bx+c=0$：\n$$\n(57+32)\\nu^2 + (114+64)\\nu + (57-324) = 0\n$$\n$$\n89\\nu^2 + 178\\nu - 267 = 0\n$$\n将整个方程除以 $89$：\n$$\n\\nu^2 + 2\\nu - 3 = 0\n$$\n对二次方程进行因式分解：\n$$\n(\\nu+3)(\\nu-1) = 0\n$$\n根是 $\\nu = 1$ 和 $\\nu = -3$。由于我们必须有 $\\nu^\\star \\ge 0$，唯一有效的解是 $\\nu^\\star=1$。\n我们必须验证该解位于假设的区间 $0.2  \\nu \\le 1.5$ 内。确实，$\\nu=1$ 在此区间内。\n因此，唯一的拉格朗日乘子是 $\\nu^\\star=1$。\n\n为确认，我们计算 $\\nu^\\star=1$ 时的 $x^\\star$：\n$x^\\star = \\frac{1}{1+1}S_1( (3, \\frac{3}{2}, \\frac{1}{5})^\\top ) = \\frac{1}{2} (3-1, \\frac{3}{2}-1, 0)^\\top = \\frac{1}{2} (2, \\frac{1}{2}, 0)^\\top = (1, \\frac{1}{4}, 0)^\\top$。\n检查约束：\n$\\|x^\\star\\|_1 + \\frac{1}{2}\\|x^\\star\\|_2^2 = (1+\\frac{1}{4}) + \\frac{1}{2}(1^2 + (\\frac{1}{4})^2) = \\frac{5}{4} + \\frac{1}{2}(1+\\frac{1}{16}) = \\frac{5}{4} + \\frac{1}{2}(\\frac{17}{16}) = \\frac{5}{4} + \\frac{17}{32} = \\frac{40}{32} + \\frac{17}{32} = \\frac{57}{32}$。\n约束被完美满足。", "answer": "$$\n\\boxed{1}\n$$", "id": "3469123"}]}