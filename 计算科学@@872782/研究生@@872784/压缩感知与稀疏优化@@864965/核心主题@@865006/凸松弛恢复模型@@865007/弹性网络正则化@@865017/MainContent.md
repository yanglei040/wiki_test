## 引言
在现代数据分析中，尤其是在生物信息学、金融学和工程学等领域，我们经常面对“高维”数据，即特征数量远超样本数量。在这种场景下，经典的[统计模型](@entry_id:165873)（如[普通最小二乘法](@entry_id:137121)）往往会失效，导致[过拟合](@entry_id:139093)和不稳定的解。[正则化方法](@entry_id:150559)通过在优化目标中引入惩罚项来约束[模型复杂度](@entry_id:145563)，为解决此类问题提供了强大的理论框架。

在众多[正则化技术](@entry_id:261393)中，Lasso（[L1正则化](@entry_id:751088)）因其能够产生[稀疏解](@entry_id:187463)（即许多模型系数恰好为零）而备受青睐，从而实现了自动化的变量选择。然而，当特征之间存在高度相关性时，Lasso的表现并不理想：它倾向于从一组相关特征中任意选择一个，而忽略其余的。另一方面，岭回归（[L2正则化](@entry_id:162880)）虽能有效处理特征间的相关性，却无法进行变量选择，其解是稠密的。为了弥合这一鸿沟，[弹性网络](@entry_id:143357)（Elastic Net）正则化应运而生，它巧妙地结合了Lasso和[岭回归](@entry_id:140984)的优点，旨在同时实现稀疏性和[对相关](@entry_id:203353)变量的分[组选择](@entry_id:175784)。

本文将对[弹性网络](@entry_id:143357)进行系统性的剖析。在“原理与机制”一章中，我们将深入其数学核心，揭示其凸性、与Lasso及岭回归的联系，并重点分析其关键特性——分组效应。接下来，在“应用与跨学科连接”一章，我们将探讨其在[广义线性模型](@entry_id:171019)中的扩展，讨论[超参数调优](@entry_id:143653)等关键实践问题，并展示其在不同科学领域的广泛应用。最后，“动手实践”部分将通过具体的编程练习，帮助您将理论知识转化为实践技能。通过这三个章节的学习，您将全面掌握[弹性网络](@entry_id:143357)这一强大工具，从其优雅的数学结构到解决现实世界复杂数据问题的能力。

## 原理与机制

在上一章引言的基础上，本章深入探讨[弹性网络正则化](@entry_id:748859)的核心原理与数学机制。我们将从其优化目标函数的定义出发，系统地分析其基本数学性质，阐释其与Lasso和岭回归的内在联系，并着重剖析其最重要的特性——“分组效应”（grouping effect）。此外，我们还将从更高等的视角，如增广设计、[近端算子](@entry_id:635396)和[贝叶斯推断](@entry_id:146958)，来审视[弹性网络](@entry_id:143357)，从而为其有效性提供更深刻的理论支撑。

### 数学形式与基本性质

[弹性网络](@entry_id:143357)方法旨在通过在一个标准的最小二乘[损失函数](@entry_id:634569)上增加一个惩罚项来求解[线性回归](@entry_id:142318)问题。这个惩罚项是Lasso惩罚（$\ell_1$范数）和岭回归惩罚（平方$\ell_2$范数）的加权组合。

给定[设计矩阵](@entry_id:165826) $X \in \mathbb{R}^{n \times p}$ 和响应向量 $y \in \mathbb{R}^{n}$，[弹性网络](@entry_id:143357)估计量 $\hat{\beta}$ 是以下[目标函数](@entry_id:267263)的最小化解：

$$
\hat{\beta} = \arg\min_{\beta \in \mathbb{R}^{p}} \left\{ \frac{1}{2n} \|y - X\beta\|_{2}^{2} + \lambda_{1}\|\beta\|_{1} + \frac{\lambda_{2}}{2}\|\beta\|_{2}^{2} \right\}
$$

其中，$\|\beta\|_{1} = \sum_{j=1}^{p} |\beta_j|$ 是 $\ell_1$ 范数，$\|\beta\|_{2}^{2} = \sum_{j=1}^{p} \beta_j^2$ 是平方 $\ell_2$ 范数。参数 $\lambda_1 \ge 0$ 和 $\lambda_2 \ge 0$ 是非负的调优参数，分别控制 $\ell_1$ 惩罚和 $\ell_2$ 惩罚的强度。

#### 凸性与[解的唯一性](@entry_id:143619)

理解此[目标函数](@entry_id:267263)的性质是掌握[弹性网络](@entry_id:143357)的关键第一步。该目标函数 $f(\beta)$ 可以看作三个独立函数的和：

1.  **损失项**: $L(\beta) = \frac{1}{2n} \|y - X\beta\|_{2}^{2}$，即（缩放的）[残差平方和](@entry_id:174395)。
2.  **[Lasso惩罚项](@entry_id:634466)**: $P_1(\beta) = \lambda_1 \|\beta\|_1$。
3.  **岭惩罚项**: $P_2(\beta) = \frac{\lambda_2}{2} \|\beta\|_2^{2}$。

**[凸性](@entry_id:138568)（Convexity）**是现代[优化理论](@entry_id:144639)的基石，它保证了局部最优解即为[全局最优解](@entry_id:175747)。我们可以分析上述三个组成部分的[凸性](@entry_id:138568) [@problem_id:3487886]：
- 损失项 $L(\beta)$ 是一个关于 $\beta$ 的二次函数，其[海森矩阵](@entry_id:139140)（Hessian matrix）为 $\frac{1}{n} X^{\top}X$。对于任何向量 $v$，都有 $v^{\top}(X^{\top}X)v = \|Xv\|_2^2 \ge 0$，因此该海森矩阵是半正定的。这意味着损失项是[凸函数](@entry_id:143075)。
- $\ell_1$ 范数 $\|\beta\|_1$ 是一个[凸函数](@entry_id:143075)，由于 $\lambda_1 \ge 0$，其非负数倍 $P_1(\beta)$ 也是[凸函数](@entry_id:143075)。值得注意的是，$\ell_1$ 范数在坐标轴上是不可微的，正是这种“[尖点](@entry_id:636792)”特性赋予了Lasso模型产生[稀疏解](@entry_id:187463)的能力。
- 平方 $\ell_2$ 范数 $\|\beta\|_2^2$ 是一个处处可微的二次函数，其海森矩阵为 $2I$（$I$ 是单位矩阵）。由于 $\lambda_2 \ge 0$，岭惩罚项 $P_2(\beta)$ 的海森矩阵为 $\lambda_2 I$，这也是一个[半正定矩阵](@entry_id:155134)，因此 $P_2(\beta)$ 也是凸函数。

由于[凸函数](@entry_id:143075)的和仍然是凸函数，所以对于所有 $\lambda_1 \ge 0$ 和 $\lambda_2 \ge 0$，[弹性网络](@entry_id:143357)的[目标函数](@entry_id:267263) $f(\beta)$ 都是一个**[凸函数](@entry_id:143075)**。

更进一步，当 $\lambda_2 > 0$ 时，[目标函数](@entry_id:267263)具备一个更强的性质——**强[凸性](@entry_id:138568)（strong convexity）** [@problem_id:3487886]。一个函数 $f$ 被称为强凸的，如果 $f(\beta) - \frac{\mu}{2}\|\beta\|_2^2$ 对于某个 $\mu > 0$ 是凸的。对于[弹性网络](@entry_id:143357)，其光滑部分（损失项和岭惩罚项）的[海森矩阵](@entry_id:139140)为 $H = \frac{1}{n} X^{\top}X + \lambda_2 I$。由于 $\frac{1}{n} X^{\top}X$ 是半正定的，其[最小特征值](@entry_id:177333)非负。因此，当 $\lambda_2 > 0$ 时，矩阵 $H$ 的最小特征值严格大于零（至少为 $\lambda_2$）。这意味着光滑部分是强凸的，从而整个[目标函数](@entry_id:267263)也是强凸的。

强[凸性](@entry_id:138568)的一个至关重要的推论是：**[目标函数](@entry_id:267263)存在唯一的最小化解**。这解决了Lasso（对应 $\lambda_2=0$ 的情况）在某些情况下面临的解不唯一的问题，例如当特征数量 $p$ 大于样本数量 $n$ 或当特征之间存在高度相关性（[多重共线性](@entry_id:141597)）时。[弹性网络](@entry_id:143357)的 $\ell_2$ 部分通过确保[目标函数](@entry_id:267263)的[严格凸性](@entry_id:193965)，为[解的唯一性](@entry_id:143619)和稳定性提供了保障 [@problem_id:3487940]。

### 作为Lasso和[岭回归](@entry_id:140984)的桥梁

[弹性网络](@entry_id:143357)的美妙之处在于它自然地统一了Lasso和[岭回归](@entry_id:140984) [@problem_id:3487940]。通过调节 $\lambda_1$ 和 $\lambda_2$，我们可以平滑地在这两种经典方法之间过渡：
-   当 $\lambda_2 = 0$ 且 $\lambda_1 > 0$ 时，[弹性网络](@entry_id:143357)目标函数退化为 **Lasso** 的目标函数。此时，模型仅通过 $\ell_1$ 范数进行正则化，倾向于产生**[稀疏解](@entry_id:187463)**（即许多系数恰好为零），从而实现[变量选择](@entry_id:177971)。
-   当 $\lambda_1 = 0$ 且 $\lambda_2 > 0$ 时，[目标函数](@entry_id:267263)退化为 **岭回归** 的目标函数。此时，模型仅通过平方 $\ell_2$ 范数进行正则化，倾向于将所有系数都向零进行**收缩（shrinkage）**，但通常不会将任何系数精确地设置为零。它对于处理[多重共线性](@entry_id:141597)问题非常有效，但不能进行[变量选择](@entry_id:177971)。

#### 在正交设计下的解析解

为了更清晰地理解这三种方法的行为差异，考虑一个理想化的场景：**正交设计（orthonormal design）**，即[设计矩阵](@entry_id:165826) $X$ 的列是标准正交的，满足 $X^{\top}X = I$（这里为了简化，我们假设 $n=1$ 的缩放因子）。在这种情况下，[损失函数](@entry_id:634569)可以被简化，并且[目标函数](@entry_id:267263)在各个坐标上是可分的，使得我们能独立地为每个系数 $\beta_j$ 找到解析解 [@problem_id:3487889]。

令 $z = X^{\top}y$ 为普通最小二乘（OLS）的估计系数。我们可以推导出三种方法的坐标级（coordinate-wise）解：
1.  **[岭回归](@entry_id:140984) ($\lambda_1=0$)**:
    $$
    \hat{\beta}_{j}^{\text{ridge}} = \frac{z_j}{1 + \lambda_2}
    $$
    [岭回归](@entry_id:140984)对每个系数进行了[乘性缩放](@entry_id:197417)。所有系数都被一个大于1的因子 $1+\lambda_2$ 统一缩小，但只要 $z_j \neq 0$，系数就不会变为零。

2.  **Lasso ($\lambda_2=0$)**:
    $$
    \hat{\beta}_{j}^{\text{lasso}} = \text{sgn}(z_j) \max(0, |z_j| - \lambda_1) =: S_{\lambda_1}(z_j)
    $$
    Lasso执行的是**[软阈值](@entry_id:635249)（soft-thresholding）**操作。它首先将系数向零“平移”一个量 $\lambda_1$，如果系数的[绝对值](@entry_id:147688)小于 $\lambda_1$，则直接将其设为零。这就是Lasso产生[稀疏性](@entry_id:136793)的机制。

3.  **[弹性网络](@entry_id:143357) ($\lambda_1>0, \lambda_2>0$)**:
    $$
    \hat{\beta}_{j}^{\text{enet}} = \frac{\text{sgn}(z_j) \max(0, |z_j| - \lambda_1)}{1 + \lambda_2} = \frac{S_{\lambda_1}(z_j)}{1 + \lambda_2}
    $$
    [弹性网络](@entry_id:143357)的解完美地结合了前两种方法的特点：它首先对系数进行[软阈值](@entry_id:635249)操作（Lasso的特性），然后对结果进行[乘性缩放](@entry_id:197417)（岭回归的特性）。它既能像Lasso一样进行变量选择（将某些系数设为零），又能像[岭回归](@entry_id:140984)一样对非零系数进行平滑收缩。

举个具体的例子 [@problem_id:3487889]，假设对于某个特征 $j$，我们有 $z_j = 2.7$，并设定 $\lambda_1 = 1.5, \lambda_2 = 0.5$。三种方法的估计系数分别为：
-   $\hat{\beta}_{j}^{\text{ridge}} = \frac{2.7}{1 + 0.5} = 1.8$
-   $\hat{\beta}_{j}^{\text{lasso}} = \max(0, 2.7 - 1.5) = 1.2$
-   $\hat{\beta}_{j}^{\text{enet}} = \frac{\max(0, 2.7 - 1.5)}{1 + 0.5} = \frac{1.2}{1.5} = 0.8$

在这个例子中，我们可以看到[弹性网络](@entry_id:143357)同时执行了阈值截断和缩放，其估计值的[绝对值](@entry_id:147688)是最小的。

### 分组效应：[弹性网络](@entry_id:143357)的核心优势

Lasso的一个著名局限性是，当面对一组高度相关的特征时，它倾向于从中任意选择一个特征进入模型，而将其余相关特征的系数设为零。这种行为可能不稳定，因为微小的数据扰动可能导致Lasso选择不同的特征。此外，如果理论上我们认为这一组特征都与响应变量相关，Lasso的这种行为就不尽如人意。

[弹性网络](@entry_id:143357)通过其所谓的**分组效应（grouping effect）**优雅地解决了这个问题 [@problem_id:3487915]。分组效应指的是，[弹性网络](@entry_id:143357)倾向于将高度相关的特征作为一个整体，要么都选入模型，要么都排除在模型之外。并且，对于被选入模型的相关特征，它们的系数大小会趋于一致。

#### 分组效应的量化分析

我们可以通过考察[弹性网络](@entry_id:143357)解的[最优性条件](@entry_id:634091)（[KKT条件](@entry_id:185881)）来严格地证明这一效应。对于任意两个特征 $j$ 和 $k$，如果它们的估计系数 $\hat{\beta}_j$ 和 $\hat{\beta}_k$ 均不为零且符号相同，那么其[最优性条件](@entry_id:634091)意味着：

$$
\hat{\beta}_{j} - \hat{\beta}_{k} = \frac{1}{n\lambda_{2}}(X_{\cdot j} - X_{\cdot k})^{\top}r(\hat{\beta})
$$

其中 $r(\hat{\beta}) = y - X\hat{\beta}$ 是解的[残差向量](@entry_id:165091)。通过应用柯西-施瓦茨不等式，并假设预测变量被[标准化](@entry_id:637219)（例如，$\|X_{\cdot j}\|_2^2/n = 1$），我们可以推导出这两个系数差异的上限 [@problem_id:3487915] [@problem_id:3487916]：

$$
|\hat{\beta}_{j} - \hat{\beta}_{k}| \le \frac{\|r(\hat{\beta})\|_{2}}{\lambda_{2}} \sqrt{\frac{2(1-\rho_{jk})}{n}}
$$

其中 $\rho_{jk} = \frac{1}{n}X_{\cdot j}^{\top}X_{\cdot k}$ 是特征 $j$ 和 $k$ 之间的样本相关系数。这个不等式清晰地揭示了分组效应的机制：
-   当两个特征高度正相关，即 $\rho_{jk} \to 1$ 时，不等式的右侧趋向于0。这意味着 $|\hat{\beta}_{j} - \hat{\beta}_{k}| \to 0$，即它们的系数必须非常接近。
-   $\lambda_2$ 的值也起着关键作用。$\lambda_2$ 越大，对系数差异的约束就越强，分组效应也越显著。

如果两个预测变量完全相同，$X_{\cdot j} = X_{\cdot k}$（即 $\rho_{jk}=1$），Lasso的解是不唯一的，它可以将系数以任意方式分配给这两个变量（只要它们的和固定），而[弹性网络](@entry_id:143357)（当 $\lambda_2 > 0$）的唯一解必然是 $\hat{\beta}_j = \hat{\beta}_k$ [@problem_id:3487915]。这直观地展示了 $\ell_2$ 惩罚项是如何通过惩罚系数的平方和来强制分配权重的。

相比之下，岭回归虽然也能将相关变量的系数拉近，但它无法产生稀疏解。[弹性网络](@entry_id:143357)则巧妙地结合了Lasso的[稀疏性](@entry_id:136793)和[岭回归](@entry_id:140984)的分组特性，使其在处理高维相关数据时尤为强大。

### 高等视角与理论洞察

为了更深入地理解[弹性网络](@entry_id:143357)的运作方式，我们可以借助一些更高等的数学工具和理论框架。

#### 从增广设计看不可表示条件

[变量选择](@entry_id:177971)的成功与否在理论上通常由所谓的**不可表示条件（irrepresentable condition）**来刻画。对于Lasso，该条件粗略地说，要求真实支持集之外的特征与支持集内的特征的相关性不能太强。当支持集内部的特征高度相关时（例如，$\rho \to 1$），这个条件很容易被违反，导致Lasso无法正确恢复真实的支持集。

[弹性网络](@entry_id:143357)的优越性可以通过一个巧妙的数学构造——**增广设计（augmented design）**——来精确阐明 [@problem_id:3487888]。我们可以将[弹性网络](@entry_id:143357)的[目标函数](@entry_id:267263)重写为一个等价的Lasso问题，但作用于一个增广的数据集上：

$$
\tilde{X} = \begin{pmatrix} X \\ \sqrt{n \lambda_2} I_p \end{pmatrix} \in \mathbb{R}^{(n+p)\times p}, \quad \tilde{y} = \begin{pmatrix} y \\ 0 \end{pmatrix} \in \mathbb{R}^{n+p}
$$

[弹性网络](@entry_id:143357)问题等价于求解在 $(\tilde{X}, \tilde{y})$ 上的Lasso问题。这个等价问题的“[格拉姆矩阵](@entry_id:203297)”（Gram matrix）是 $\tilde{G} = \frac{1}{n}\tilde{X}^{\top}\tilde{X} = G + \lambda_2 I_p$，其中 $G = \frac{1}{n}X^{\top}X$ 是原始的格拉姆矩阵。

当我们将不可表示条件应用于这个新的[格拉姆矩阵](@entry_id:203297) $\tilde{G}$ 时，其核心项从 $G_{SS}^{-1}$（Lasso的情况）变成了 $(G_{SS} + \lambda_2 I_s)^{-1}$ [@problem_id:3487888] [@problem_id:3452180]。这里的 $G_{SS}$ 是真实支持集 $S$ 对应的格拉姆子矩阵。当 $S$ 内的特征高度相关时，$G_{SS}$ 的[最小特征值](@entry_id:177333) $\lambda_{\min}(G_{SS})$ 可能非常接近于0，使得 $G_{SS}$ 接近奇异，其逆矩阵 $G_{SS}^{-1}$ 的范数会非常大，从而导致不可表示条件被违反。

然而，对于[弹性网络](@entry_id:143357)，[相关矩阵](@entry_id:262631)变为 $G_{SS} + \lambda_2 I_s$。它的[最小特征值](@entry_id:177333)为 $\lambda_{\min}(G_{SS}) + \lambda_2$。由于 $\lambda_2 > 0$，这个值被确保严格大于零，远离了[奇异点](@entry_id:199525)。这使得其[逆矩阵](@entry_id:140380) $(G_{SS} + \lambda_2 I_s)^{-1}$ 的范数得到了控制。具体来说，我们可以得到如下的不等式界限 [@problem_id:3487888]：

$$
\|(G_{SS} + \lambda_2 I_s)^{-1}\|_2 = \frac{1}{\lambda_{\min}(G_{SS}) + \lambda_2}
$$

这个界限随着 $\lambda_2$ 的增加而减小。这意味着 $\ell_2$ 惩罚项通过“正则化”支持集内部的协[方差](@entry_id:200758)结构，有效地放宽了不可表示条件，从而使得[弹性网络](@entry_id:143357)能够在Lasso失败的强相关场景下成功地恢[复变量](@entry_id:175312)组 [@problem_id:3452180]。

#### [近端算子](@entry_id:635396)视角

在现代[大规模优化](@entry_id:168142)中，**[近端算子](@entry_id:635396)（proximal operator）**是一个核心概念。对于一个函数 $g$，其[近端算子](@entry_id:635396) $\text{prox}_{\alpha g}(v)$ 定义为：

$$
\text{prox}_{\alpha g}(v) = \arg\min_{x} \left\{ \alpha g(x) + \frac{1}{2}\|x-v\|_2^2 \right\}
$$

它在几何上可以被理解为在点 $v$ 附近寻找一个点 $x$，既要接近 $v$，又要使 $g(x)$ 的值较小。许多高效的[优化算法](@entry_id:147840)，如[近端梯度下降](@entry_id:637959)（ISTA/FISTA），都依赖于能够快速计算正则化项的[近端算子](@entry_id:635396)。

对于[弹性网络](@entry_id:143357)的惩罚项 $g(\beta) = \lambda_1 \|\beta\|_1 + \frac{\lambda_2}{2} \|\beta\|_2^2$，其[近端算子](@entry_id:635396)有一个非常简洁和富有启发性的闭式解 [@problem_id:3487946]：

$$
\text{prox}_{\alpha g}(v) = \frac{1}{1 + \alpha\lambda_2} S_{\alpha\lambda_1}(v)
$$

这个表达式揭示了一个深刻的结构：[弹性网络](@entry_id:143357)的[近端算子](@entry_id:635396)可以分解为两个基本操作的序列。首先，对输入向量 $v$ 应用[软阈值算子](@entry_id:755010) $S_{\alpha\lambda_1}(\cdot)$（这正是Lasso惩罚的[近端算子](@entry_id:635396)）；然后，对结果进行一个因子为 $\frac{1}{1 + \alpha\lambda_2}$ 的缩放（这正是岭惩罚的[近端算子](@entry_id:635396)）。这种分解不仅为设计高效的求解算法提供了直接路径，也再次从算法构建的角度印证了[弹性网络](@entry_id:143357)是Lasso和[岭回归](@entry_id:140984)特性的完美融合。

#### [贝叶斯解释](@entry_id:265644)

[正则化方法](@entry_id:150559)通常可以从贝叶斯统计的视角得到解释，此时正则化项对应于模型参数的先验分布。[弹性网络](@entry_id:143357)也不例外 [@problem_id:3487931]。

考虑一个标准的线性模型 $y = X\beta + \varepsilon$，其中噪声 $\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$。如果我们为系数 $\beta$ 赋予一个先验分布 $p(\beta)$，那么其后验分布为 $p(\beta|y) \propto p(y|\beta)p(\beta)$。**最大后验估计（Maximum A Posteriori, MAP）**旨在寻找[后验分布](@entry_id:145605)[概率密度](@entry_id:175496)最大的参数值。这等价于最小化负对数后验：

$$
\hat{\beta}_{\text{MAP}} = \arg\min_{\beta} \{-\log p(y|\beta) - \log p(\beta)\}
$$

将高斯[似然函数](@entry_id:141927) $-\log p(y|\beta) = \frac{1}{2\sigma^2}\|y-X\beta\|_2^2 + \text{const}$ 代入，我们发现，[弹性网络](@entry_id:143357)的优化目标函数恰好对应于一个特定的先验分布：

$$
p(\beta) \propto \exp\left( -c\lambda_1 \|\beta\|_1 - \frac{c\lambda_2}{2} \|\beta\|_2^2 \right)
$$

其中 $c$ 是一个与 $\sigma^2$ 相关的常数。这个先验分布是[拉普拉斯分布](@entry_id:266437)（对应 $\ell_1$ 惩罚）和高斯分布（对应 $\ell_2^2$ 惩罚）的乘积。因此，**[弹性网络](@entry_id:143357)估计量可以被看作是在高斯-拉普拉斯混合先验下的最大后验估计**。这个先验结合了拉普拉斯先验在零点处的尖峰（促进稀疏性）和[高斯先验](@entry_id:749752)的平滑尾部（促进系数收缩和分组）。

有趣的是，该贝叶斯模型下的**[后验均值](@entry_id:173826)**估计量与[MAP估计量](@entry_id:276643)（即[弹性网络](@entry_id:143357)解）表现出不同的特性。[后验均值](@entry_id:173826)是一个连续的收缩函数，它不会将任何系数精确地设为零（除非对应的[OLS估计量](@entry_id:177304) $z_j$ 恰好为零）。因此，[后验均值](@entry_id:173826)不具有稀疏性，而[稀疏性](@entry_id:136793)是作为后验分布的**众数**（mode）——即[MAP估计](@entry_id:751667)——所独有的性质 [@problem_id:3487931]。

### 与其他结构化[稀疏模型](@entry_id:755136)的比较

[弹性网络](@entry_id:143357)的分组效应是数据驱动的，它根据特征间的相关性自动形成分组。这与其他旨在对预先定义好的结构进行建模的方法形成了对比，例如**组Lasso（Group Lasso）**和**[融合Lasso](@entry_id:636401)（Fused Lasso）** [@problem_id:3487936]。

-   **组Lasso** 的惩罚项是 $\sum_{k} \|x_{G_k}\|_2$，其中 $G_k$ 是预先指定的变量分组。它的惩罚项在每个组的原点处不可微，因此可以实现**组级别**的稀疏性，即整个组的系数要么全为零，要么全不为零。这适用于那些我们有先验知识（例如，一组基因共同发挥作用）的情况。

-   **[融合Lasso](@entry_id:636401)** 的惩罚项通常包含 $\sum_j |x_j - x_{j-1}|$ 这样的项，它惩罚相邻系数之间的差异。这鼓励解是**分段常数（piecewise-constant）**的，适用于信号或图像处理中特征具有自然排序或空间邻近性的问题。

与这些方法相比，[弹性网络](@entry_id:143357)中的平方$\ell_2$惩罚项 $\sum_j x_j^2$ 是可分的，并且在原点处是光滑可微的。它本身不强制任何特定的结构或[组稀疏性](@entry_id:750076)。它的分组效应是一种隐式、平滑的耦合，完全由数据中的相关性驱动，而不需要任何关于变量分组的[先验信息](@entry_id:753750)。这使得[弹性网络](@entry_id:143357)成为一种更通用、更具探索性的工具。