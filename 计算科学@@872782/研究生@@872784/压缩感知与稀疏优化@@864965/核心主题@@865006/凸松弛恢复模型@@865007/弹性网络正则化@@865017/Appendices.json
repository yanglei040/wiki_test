{"hands_on_practices": [{"introduction": "要深入理解弹性网络正则化的工作机制，一个有效的方法是从一个理想化的场景入手。本练习 [@problem_id:3487889] 设定了一个正交设计矩阵 ($X^{\\top}X=I$) 的特殊情况，这种设定消除了特征之间的相关性。在这一简化下，我们可以推导出弹性网络解的显式表达式，从而清晰地观察其如何融合 LASSO 的稀疏性（通过软阈值）和岭回归的系数收缩效应。", "problem": "考虑一个线性回归模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，满足标准正交设计条件 $X^{\\top}X = I$，其中 $I$ 是单位矩阵。设 $y \\in \\mathbb{R}^{n}$ 是响应向量，并定义向量 $z = X^{\\top}y \\in \\mathbb{R}^{p}$。对于一个系数向量 $\\beta \\in \\mathbb{R}^{p}$，考虑以下三种惩罚最小二乘估计量：\n1. 最小绝对收缩和选择算子 (LASSO)，定义为以下表达式的最小化器\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1},\n$$\n其中 $\\lambda_{1} > 0$ 且 $\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_{j}|$。\n2. 岭回归 (Ridge regression)，定义为以下表达式的最小化器\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\lambda_{2} > 0$ 且 $\\|\\beta\\|_{2}^{2} = \\sum_{j=1}^{p} \\beta_{j}^{2}$。\n3. 弹性网络 (The elastic net)，定义为以下表达式的最小化器\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2}\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\lambda_{1} > 0$ 且 $\\lambda_{2} > 0$。\n\n从凸优化的第一性原理和标准正交条件 $X^{\\top}X = I$ 出发，推导弹性网络估计量关于 $z_{j}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的坐标级闭式解。然后类似地，求出 LASSO 和岭回归的坐标级解。最后，对于单个坐标 $j$，$z_{j} = 2.7$，$\\lambda_{1} = 1.5$ 和 $\\lambda_{2} = 0.5$，分别计算弹性网络、LASSO 和岭回归的三个坐标级估计值。将您的最终数值答案以单行矩阵的形式呈现，顺序为：弹性网络、LASSO、岭回归。无需四舍五入。", "solution": "该问题是有效的，因为它科学地基于正则化线性模型这一成熟理论，由于凸目标函数保证了唯一解的存在而是适定的，并且客观地陈述了所有必要信息。\n\n问题的核心是找到三个不同目标函数的最小化器。这三个目标函数都有一个共同的最小二乘项 $\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$。我们可以利用给定的标准正交设计条件 $X^{\\top}X = I$ 和定义 $z = X^{\\top}y$ 来简化这一项。\n\n我们来展开平方 $\\ell_2$-范数：\n$$\n\\|y - X\\beta\\|_{2}^{2} = (y - X\\beta)^{\\top}(y - X\\beta) = y^{\\top}y - y^{\\top}X\\beta - \\beta^{\\top}X^{\\top}y + \\beta^{\\top}X^{\\top}X\\beta\n$$\n使用 $X^{\\top}X = I$ 和 $z = X^{\\top}y$（这意味着 $z^{\\top} = y^{\\top}X$），表达式变为：\n$$\ny^{\\top}y - 2\\beta^{\\top}z + \\beta^{\\top}I\\beta = y^{\\top}y - 2\\beta^{\\top}z + \\|\\beta\\|_{2}^{2}\n$$\n我们可以通过对 $\\beta$ 配方来重写这个表达式：\n$$\ny^{\\top}y - 2\\beta^{\\top}z + \\|\\beta\\|_{2}^{2} = \\|\\beta\\|_{2}^{2} - 2\\beta^{\\top}z + \\|z\\|_{2}^{2} - \\|z\\|_{2}^{2} + y^{\\top}y = \\|z - \\beta\\|_{2}^{2} + y^{\\top}y - \\|z\\|_{2}^{2}\n$$\n由于 $y^{\\top}y - \\|z\\|_{2}^{2}$ 是一个关于 $\\beta$ 的常数，最小化形式为 $\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + P(\\beta)$ 的目标函数等价于最小化 $\\frac{1}{2}\\|z - \\beta\\|_{2}^{2} + P(\\beta)$，其中 $P(\\beta)$ 是惩罚项。\n\n简化后的目标函数在 $\\beta$ 的坐标上是可分的，因为平方误差项 $\\frac{1}{2}\\|z - \\beta\\|_{2}^{2} = \\frac{1}{2}\\sum_{j=1}^{p}(z_j - \\beta_j)^2$ 和惩罚项（$\\|\\beta\\|_1 = \\sum_j |\\beta_j|$，$\\|\\beta\\|_2^2 = \\sum_j \\beta_j^2$）都是对各个坐标的求和。因此，我们可以通过最小化其对应的坐标级目标函数来独立地求解每个系数 $\\hat{\\beta}_j$。\n\n首先，我们推导弹性网络估计量的解。对于单个坐标 $\\beta_j$，需要最小化的目标函数是：\n$$\nJ_{\\text{enet}}(\\beta_j) = \\frac{1}{2}(z_j - \\beta_j)^2 + \\lambda_1 |\\beta_j| + \\frac{\\lambda_2}{2}\\beta_j^2\n$$\n这是一个凸函数。其最小值在关于 $\\beta_j$ 的次梯度包含 $0$ 的点处取得。次梯度为：\n$$\n\\partial_{\\beta_j} J_{\\text{enet}}(\\beta_j) = -(z_j - \\beta_j) + \\lambda_1 \\partial(|\\beta_j|) + \\lambda_2 \\beta_j\n$$\n其中 $\\partial(|\\beta_j|)$ 是绝对值函数的次梯度：当 $\\beta_j \\neq 0$ 时，它等于 $\\text{sgn}(\\beta_j)$；当 $\\beta_j = 0$ 时，它等于区间 $[-1, 1]$。\n\n我们分析最优值 $\\hat{\\beta}_j$ 的三种情况：\n1.  如果 $\\hat{\\beta}_j > 0$，则 $\\partial(|\\hat{\\beta}_j|) = 1$。将梯度设为 $0$：\n    $$\n    -\\left(z_j - \\hat{\\beta}_j\\right) + \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j - z_j + \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j(1 + \\lambda_2) = z_j - \\lambda_1\n    $$\n    $$\n    \\hat{\\beta}_j = \\frac{z_j - \\lambda_1}{1 + \\lambda_2}\n    $$\n    这个解仅在 $\\hat{\\beta}_j > 0$ 时有效，这意味着 $z_j - \\lambda_1 > 0$，即 $z_j > \\lambda_1$。\n\n2.  如果 $\\hat{\\beta}_j < 0$，则 $\\partial(|\\hat{\\beta}_j|) = -1$。将梯度设为 $0$：\n    $$\n    -\\left(z_j - \\hat{\\beta}_j\\right) - \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j - z_j - \\lambda_1 + \\lambda_2 \\hat{\\beta}_j = 0 \\implies \\hat{\\beta}_j(1 + \\lambda_2) = z_j + \\lambda_1\n    $$\n    $$\n    \\hat{\\beta}_j = \\frac{z_j + \\lambda_1}{1 + \\lambda_2}\n    $$\n    这个解仅在 $\\hat{\\beta}_j < 0$ 时有效，这意味着 $z_j + \\lambda_1 < 0$，即 $z_j < -\\lambda_1$。\n\n3.  如果 $\\hat{\\beta}_j = 0$，次梯度条件是 $0 \\in -(z_j - 0) + \\lambda_1[-1, 1] + 0$。\n    这可以简化为 $0 \\in -z_j + [-\\lambda_1, \\lambda_1]$，这意味着 $z_j \\in [-\\lambda_1, \\lambda_1]$，或者 $|z_j| \\le \\lambda_1$。\n\n综合这三种情况，我们得到弹性网络估计量的坐标级解：\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\begin{cases} \\frac{z_j - \\lambda_1}{1 + \\lambda_2}  &\\text{若 } z_j > \\lambda_1 \\\\ \\frac{z_j + \\lambda_1}{1 + \\lambda_2}  &\\text{若 } z_j < -\\lambda_1 \\\\ 0  &\\text{若 } |z_j| \\le \\lambda_1 \\end{cases}\n$$\n这可以使用软阈值算子 $S_{\\lambda}(x) = \\text{sgn}(x)\\max(0, |x|-\\lambda)$ 紧凑地写为：\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\frac{S_{\\lambda_1}(z_j)}{1 + \\lambda_2}\n$$\n\n接下来，我们求 LASSO 估计量的解。LASSO 是弹性网络在 $\\lambda_2 = 0$ 时的特例。将 $\\lambda_2 = 0$ 代入弹性网络的解中，得到 LASSO 的坐标级解：\n$$\n\\hat{\\beta}_{j}^{\\text{lasso}} = \\frac{S_{\\lambda_1}(z_j)}{1 + 0} = S_{\\lambda_1}(z_j) = \\text{sgn}(z_j)\\max(0, |z_j|-\\lambda_1)\n$$\n\n最后，我们求解岭回归估计量。岭回归对应于弹性网络中 $\\lambda_1 = 0$ 的情况。将 $\\lambda_1 = 0$ 代入弹性网络的解中：\n条件 $z_j > \\lambda_1$ 变为 $z_j > 0$。条件 $z_j < -\\lambda_1$ 变为 $z_j < 0$。条件 $|z_j| \\le \\lambda_1$ 变为 $z_j=0$。\n所以，如果 $z_j > 0$，$\\hat{\\beta}_j = \\frac{z_j - 0}{1 + \\lambda_2} = \\frac{z_j}{1 + \\lambda_2}$。\n如果 $z_j < 0$，$\\hat{\\beta}_j = \\frac{z_j + 0}{1 + \\lambda_2} = \\frac{z_j}{1 + \\lambda_2}$。\n如果 $z_j = 0$，$\\hat{\\beta}_j = 0$，这也可以由公式 $\\frac{z_j}{1 + \\lambda_2}$ 给出。\n因此，岭回归的坐标级解是：\n$$\n\\hat{\\beta}_{j}^{\\text{ridge}} = \\frac{z_j}{1 + \\lambda_2}\n$$\n这也可以通过直接最小化可微目标函数 $J_{\\text{ridge}}(\\beta_j) = \\frac{1}{2}(z_j - \\beta_j)^2 + \\frac{\\lambda_2}{2}\\beta_j^2$ 来推导。将其导数设为 $0$ 会得到 $\\beta_j(1 + \\lambda_2) - z_j = 0$，从而得出相同的结果。\n\n现在我们使用给定的值计算单个坐标 $j$ 的数值估计：$z_j = 2.7$，$\\lambda_1 = 1.5$ 和 $\\lambda_2 = 0.5$。\n\n对于弹性网络估计，我们首先检查阈值条件：$|z_j| = 2.7 > 1.5 = \\lambda_1$。由于 $z_j > \\lambda_1$，我们使用第一种情况：\n$$\n\\hat{\\beta}_{j}^{\\text{enet}} = \\frac{z_j - \\lambda_1}{1 + \\lambda_2} = \\frac{2.7 - 1.5}{1 + 0.5} = \\frac{1.2}{1.5} = \\frac{12}{15} = \\frac{4}{5} = 0.8\n$$\n\n对于 LASSO 估计，我们使用软阈值函数：\n$$\n\\hat{\\beta}_{j}^{\\text{lasso}} = S_{\\lambda_1}(z_j) = S_{1.5}(2.7) = \\text{sgn}(2.7)\\max(0, |2.7|-1.5) = 1 \\cdot (2.7 - 1.5) = 1.2\n$$\n\n对于岭回归估计，我们使用其特定公式：\n$$\n\\hat{\\beta}_{j}^{\\text{ridge}} = \\frac{z_j}{1 + \\lambda_2} = \\frac{2.7}{1 + 0.5} = \\frac{2.7}{1.5} = \\frac{27}{15} = \\frac{9}{5} = 1.8\n$$\n\n坐标 $\\beta_j$ 的三个估计值分别是：弹性网络为 $0.8$，LASSO 为 $1.2$，岭回归为 $1.8$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8  1.2  1.8 \\end{pmatrix}}\n$$", "id": "3487889"}, {"introduction": "弹性网络等正则化问题通常有两种等价的数学表述：惩罚形式和约束形式。本练习 [@problem_id:3487879] 将引导你运用凸优化中的 Karush-Kuhn-Tucker (KKT) 条件，深入探究这两种形式之间的深刻联系。通过推导惩罚参数 $(\\lambda_{1}, \\lambda_{2})$ 与约束预算 $t$ 之间的关系，你将从根本上理解正则化是如何通过控制模型系数的“范数预算”来防止过拟合的。", "problem": "考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。令 $\\beta \\in \\mathbb{R}^{p}$ 表示系数向量。固定一个混合参数 $\\alpha \\in (0,1)$ 和一个预算 $t > 0$，考虑带约束的弹性网络优化问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\|y - X \\beta\\|_{2}^{2} \\quad \\text{subject to} \\quad \\alpha \\|\\beta\\|_{1} + \\frac{1-\\alpha}{2} \\|\\beta\\|_{2}^{2} \\le t.\n$$\n假设 $X$ 和 $y$ 是给定的，并假设该问题满足 Slater 条件。令 $\\nu \\ge 0$ 表示在解 $\\beta^{\\star}$ 处与上述约束相关的 Karush–Kuhn–Tucker (KKT) 不等式乘子，并令 $s^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ 表示在 $\\beta^{\\star}$ 处的 $\\ell_{1}$ 范数的一个次梯度。\n\n仅从凸函数、次梯度和带约束凸优化的 Karush–Kuhn–Tucker (KKT) 条件的定义出发，推导此问题的 KKT 最优性系统。然后，从基本原理出发，论证该约束形式在最优性上如何等价于以下形式的惩罚弹性网络目标\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2},\n$$\n并确定不等式乘子 $\\nu$ 与惩罚参数 $(\\lambda_{1}, \\lambda_{2})$ 之间的明确关系。\n\n您的最终答案必须是有序对 $(\\lambda_{1}, \\lambda_{2})$，写成一个关于 $\\nu$ 和 $\\alpha$ 的单一闭式解析表达式。本题不需要数值近似，也不涉及单位。请使用标准数学符号将最终的有序对表示为解析表达式。", "solution": "我们首先将带约束的弹性网络问题形式化。定义目标函数\n$$\nf(\\beta) = \\|y - X \\beta\\|_{2}^{2},\n$$\n和约束函数\n$$\ng(\\beta) = \\alpha \\|\\beta\\|_{1} + \\frac{1-\\alpha}{2} \\|\\beta\\|_{2}^{2} - t \\le 0.\n$$\n函数 $f(\\beta)$ 是关于 $\\beta$ 的凸函数，因为它是仿射映射与凸函数（欧几里得范数的平方）的复合。函数 $g(\\beta)$ 是凸函数，因为它是凸函数 $\\|\\beta\\|_{1}$ 和 $\\|\\beta\\|_{2}^{2}$ 的非负加权和（其中 $\\alpha \\in (0,1)$），再减去一个常数 $t$。根据 Slater 条件成立的假设，存在一个 $\\tilde{\\beta}$ 使得 $g(\\tilde{\\beta}) < 0$，这保证了强对偶性以及 Karush–Kuhn–Tucker (KKT) 条件作为最优性充要条件的有效性。\n\n不等式约束凸优化问题的 KKT 条件如下：\n- 原始可行性: $g(\\beta^{\\star}) \\le 0$。\n- 对偶可行性: $\\nu \\ge 0$。\n- 互补松弛性: $\\nu \\, g(\\beta^{\\star}) = 0$。\n- 平稳性: $0 \\in \\partial f(\\beta^{\\star}) + \\nu \\, \\partial g(\\beta^{\\star})$。\n\n我们现在计算次梯度。$f(\\beta)$ 的梯度是\n$$\n\\nabla f(\\beta) = 2 X^{\\top}(X \\beta - y).\n$$\n对于约束，$\\|\\beta\\|_{1}$ 的次梯度是集合\n$$\n\\partial \\|\\beta\\|_{1} = \\{ s \\in \\mathbb{R}^{p} \\, : \\, s_{j} = \\operatorname{sign}(\\beta_{j}) \\ \\text{if} \\ \\beta_{j} \\neq 0, \\ \\text{and} \\ s_{j} \\in [-1,1] \\ \\text{if} \\ \\beta_{j} = 0 \\}.\n$$\n$\\frac{1-\\alpha}{2} \\|\\beta\\|_{2}^{2}$ 的梯度是 $(1-\\alpha) \\beta$，因为\n$$\n\\nabla \\left( \\frac{1-\\alpha}{2} \\|\\beta\\|_{2}^{2} \\right) = (1-\\alpha) \\beta.\n$$\n因此，\n$$\n\\partial g(\\beta) = \\alpha \\, \\partial \\|\\beta\\|_{1} + (1-\\alpha) \\beta.\n$$\n代入平稳性条件，我们得到\n$$\n0 \\in 2 X^{\\top}(X \\beta^{\\star} - y) + \\nu \\left[ \\alpha \\, \\partial \\|\\beta^{\\star}\\|_{1} + (1-\\alpha) \\beta^{\\star} \\right].\n$$\n等价地，存在 $s^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ 使得\n$$\n0 = 2 X^{\\top}(X \\beta^{\\star} - y) + \\nu \\alpha \\, s^{\\star} + \\nu (1-\\alpha) \\beta^{\\star}.\n$$\n根据互补松弛性，如果约束在最优点处是激活的，即 $g(\\beta^{\\star}) = 0$，那么 $\\nu \\ge 0$ 可以是正数。如果约束未激活，则 $\\nu = 0$；我们推导的映射将仍然有效，并简单地得到零惩罚。\n\n我们现在将这个平稳性条件与惩罚弹性网络目标的平稳性条件进行比较，\n$$\nF(\\beta) = \\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\frac{\\lambda_{2}}{2} \\|\\beta\\|_{2}^{2}.\n$$\n对于 $F(\\beta)$，平稳性条件是\n$$\n0 \\in 2 X^{\\top}(X \\beta^{\\star} - y) + \\lambda_{1} \\, \\partial \\|\\beta^{\\star}\\|_{1} + \\lambda_{2} \\beta^{\\star}.\n$$\n与约束问题的平稳性条件逐项比较，\n$$\n2 X^{\\top}(X \\beta^{\\star} - y) + \\nu \\alpha \\, s^{\\star} + \\nu (1-\\alpha) \\beta^{\\star} = 0\n\\quad \\text{versus} \\quad\n2 X^{\\top}(X \\beta^{\\star} - y) + \\lambda_{1} \\, s^{\\star} + \\lambda_{2} \\beta^{\\star} = 0,\n$$\n我们确定了对应关系\n$$\n\\lambda_{1} = \\nu \\alpha, \\qquad \\lambda_{2} = \\nu (1-\\alpha).\n$$\n注意，约束形式的拉格朗日函数中的常数项 $-\\nu t$ 不影响平稳性，因此不影响乘子和惩罚项之间的映射关系。此外，当约束激活且 $\\nu > 0$ 时，该映射是非平凡的；当约束未激活时，$\\nu = 0$ 得到 $\\lambda_{1} = 0$ 和 $\\lambda_{2} = 0$，这与无约束最小二乘解没有惩罚项是一致的。\n\n因此，基于 KKT 条件，由不等式乘子和混合参数所确定的惩罚参数为\n$$\n(\\lambda_{1}, \\lambda_{2}) = (\\nu \\alpha, \\nu (1-\\alpha)).\n$$\n这完成了从基本原理出发的推导。", "answer": "$$\\boxed{\\begin{pmatrix}\\nu \\alpha  \\nu (1-\\alpha)\\end{pmatrix}}$$", "id": "3487879"}, {"introduction": "在理解了弹性网络的性质之后，下一步便是如何高效地求解其优化问题。本练习 [@problem_id:3487939] 聚焦于坐标下降法——求解此类问题最常用和高效的算法之一。你将亲手推导其核心的单坐标更新法则，并分析其在处理高维稀疏数据时的计算复杂度，从而理解为何残差缓存等技巧对于算法的实际性能至关重要。", "problem": "在压缩感知（CS）和稀疏优化的背景下，考虑弹性网络正则化问题。设 $X \\in \\mathbb{R}^{n \\times p}$ 是一个特征矩阵，其列为 $x_{j} \\in \\mathbb{R}^{n}$（$j \\in \\{1,\\dots,p\\}$），并设 $y \\in \\mathbb{R}^{n}$ 是一个响应向量。弹性网络的目标函数为\n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2},\n$$\n其中 $\\beta \\in \\mathbb{R}^{p}$，$\\lambda_{1} \\ge 0$ 且 $\\lambda_{2} \\ge 0$。考虑一个循环坐标下降算法，该算法维护缓存的残差 $r := y - X\\beta$ 和为所有 $j \\in \\{1,\\dots,p\\}$ 预先计算的 Gram 矩阵对角线元素 $d_{j} := \\frac{1}{n}\\,\\|x_{j}\\|_{2}^{2}$。对于给定的坐标 $j$，定义部分残差 $r^{(j)} := r + x_{j}\\beta_{j}$，并令 $s_{j} := \\mathrm{nnz}(x_{j})$ 表示列 $x_{j}$ 中非零元的数量。\n\n从凸函数的基本最优性条件和 $\\ell_{1}$ 范数的次梯度微积分出发，推导出一个 $\\beta_{j}$ 的精确闭式坐标更新，该更新纯粹用 $x_{j}$、$r$、$d_{j}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 表示，并同时推导在 $\\beta_{j}$ 改变后保持 $r = y - X\\beta$ 的残差更新。然后，在 $p \\gg n$ 且 $X$ 为稀疏矩阵的极端情况下，分析每次迭代的计算复杂度，假设您在每次坐标更改后重复使用 $r$ 和 $d_{j}$ 并执行残差更新。使用渐进表示法，将遍历所有 $p$ 个坐标的一次完整过程的成本用 $\\{s_{j}\\}_{j=1}^{p}$ 表示。\n\n您的最终答案必须是一个双元素行矩阵，其第一个元素是 $\\beta_{j}$ 的精确坐标更新，第二个元素是遍历所有 $p$ 个坐标的一次完整过程的渐进复杂度，两者均为闭式表达式。无需进行数值计算。", "solution": "我们从弹性网络的目标函数开始\n$$\nF(\\beta) \\;=\\; \\frac{1}{2n}\\,\\|y - X\\beta\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|\\beta\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|\\beta\\|_{2}^{2}.\n$$\n固定除 $j$ 之外的所有坐标，并考虑一个替换 $\\beta_{j}$ 的关于变量 $t \\in \\mathbb{R}$ 的一维子问题。设当前迭代值为 $\\beta$，残差为 $r := y - X\\beta$，部分残差为 $r^{(j)} := r + x_{j}\\beta_{j} = y - \\sum_{k \\ne j} x_{k}\\beta_{k}$。关于 $t$ 的受限目标函数是\n$$\n\\phi_{j}(t)\n\\;=\\;\n\\frac{1}{2n}\\,\\|r^{(j)} - x_{j} t\\|_{2}^{2}\n\\;+\\;\n\\lambda_{1}\\,|t|\n\\;+\\;\n\\frac{\\lambda_{2}}{2}\\,t^{2}\n\\;+\\; \\text{const},\n$$\n其中“const”不依赖于 $t$。展开二次项并合并系数可得\n$$\n\\phi_{j}(t)\n\\;=\\;\n\\frac{1}{2}\\,(d_{j} + \\lambda_{2})\\,t^{2}\n\\;-\\;\n\\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)}\\right) t\n\\;+\\;\n\\lambda_{1}\\,|t|\n\\;+\\; \\text{const},\n$$\n其中 $d_{j} := \\frac{1}{n}\\,\\|x_{j}\\|_{2}^{2}$。这是一个严格凸的一维问题，由一个二次项和一个 $\\ell_{1}$ 惩罚项组成。对于一个最小化子 $t^{\\star}$，其次梯度最优性条件是\n$$\n0 \\;\\in\\; (d_{j} + \\lambda_{2})\\,t^{\\star} \\;-\\; \\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)}\\right) \\;+\\; \\lambda_{1}\\,\\partial|t^{\\star}|,\n$$\n其中 $\\partial|t|$ 表示绝对值在 $t$ 处的次微分。解的特点是软阈值：\n$$\nt^{\\star} \\;=\\; \\frac{1}{d_{j} + \\lambda_{2}} \\; S\\!\\left(\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)},\\, \\lambda_{1}\\right),\n$$\n其中软阈值算子 $S(a,\\tau)$ 定义为 $S(a,\\tau) := \\mathrm{sign}(a)\\,\\max\\{|a| - \\tau,\\, 0\\}$。\n\n为了用缓存的残差 $r$ 和 $d_{j}$ 来表示更新，注意到\n$$\nx_{j}^{\\top} r^{(j)} \\;=\\; x_{j}^{\\top}(r + x_{j}\\beta_{j}) \\;=\\; x_{j}^{\\top} r \\;+\\; \\|x_{j}\\|_{2}^{2}\\,\\beta_{j},\n$$\n因此\n$$\n\\frac{1}{n}\\,x_{j}^{\\top} r^{(j)} \\;=\\; \\frac{1}{n}\\,x_{j}^{\\top} r \\;+\\; d_{j}\\,\\beta_{j}.\n$$\n将此代入软阈值表达式，得到精确的坐标更新\n$$\n\\beta_{j}^{\\mathrm{new}}\n\\;=\\;\n\\frac{1}{d_{j} + \\lambda_{2}}\n\\; S\\!\\left(\\frac{1}{n}\\,x_{j}^{\\top} r \\;+\\; d_{j}\\,\\beta_{j},\\, \\lambda_{1}\\right).\n$$\n在计算出 $\\beta_{j}^{\\mathrm{new}}$ 后，通过以下方式更新缓存的残差以保持 $r = y - X\\beta$\n$$\nr \\;\\leftarrow\\; r \\;-\\; x_{j}\\,\\big(\\beta_{j}^{\\mathrm{new}} - \\beta_{j}^{\\mathrm{old}}\\big).\n$$\n\n我们现在分析在 $p \\gg n$ 且 $X$ 稀疏的情况下的计算复杂度。令 $s_{j} := \\mathrm{nnz}(x_{j})$ 为第 $j$ 列中非零元的数量。每次坐标更新的操作包括：\n- 计算 $\\frac{1}{n}\\,x_{j}^{\\top} r$：这需要 $\\mathcal{O}(s_{j})$ 时间，因为只有 $x_{j}$ 的非零项有贡献。\n- 构造 $\\frac{1}{n}\\,x_{j}^{\\top} r + d_{j}\\,\\beta_{j}$：这需要 $\\mathcal{O}(1)$ 时间，因为 $d_{j}$ 是预先计算的。\n- 应用软阈值和通过 $(d_{j} + \\lambda_{2})^{-1}$ 进行缩放：这是 $\\mathcal{O}(1)$。\n- 更新残差 $r \\leftarrow r - x_{j}\\,(\\beta_{j}^{\\mathrm{new}} - \\beta_{j}^{\\mathrm{old}})$：这需要 $\\mathcal{O}(s_{j})$ 时间，因为只有在 $x_{j}$ 非零的位置才需要更改。\n\n因此，每个坐标的总成本是 $\\mathcal{O}(s_{j})$，而遍历所有 $p$ 个坐标的一次完整过程的成本为\n$$\n\\mathcal{O}\\!\\left(\\sum_{j=1}^{p} s_{j}\\right),\n$$\n即 $\\mathcal{O}(\\mathrm{nnz}(X))$，其中 $\\mathrm{nnz}(X)$ 表示 $X$ 中非零元的总数。在 $p \\gg n$ 且列稀疏的极端情况下，$s_{j} \\le n$，而 $\\sum_{j=1}^{p} s_{j}$ 捕捉了总体稀疏性；重复使用缓存的残差和预计算的 $d_{j}$ 确保了每次遍历的复杂度与稀疏度成比例，而不是与 $np$ 成比例。", "answer": "$$\\boxed{\\begin{pmatrix}\n\\dfrac{1}{d_{j} + \\lambda_{2}}\\, S\\!\\left(\\dfrac{1}{n}\\,x_{j}^{\\top} r + d_{j}\\,\\beta_{j},\\, \\lambda_{1}\\right)  \\mathcal{O}\\!\\left(\\displaystyle\\sum_{j=1}^{p} s_{j}\\right)\n\\end{pmatrix}}$$", "id": "3487939"}]}