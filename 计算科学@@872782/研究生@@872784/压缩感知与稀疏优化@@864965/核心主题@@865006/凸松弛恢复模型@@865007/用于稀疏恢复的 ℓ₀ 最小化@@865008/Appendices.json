{"hands_on_practices": [{"introduction": "本次练习将通过一个具体的动手计算，揭示贪心算法（如正交匹配追踪，作为$ℓ_0$最小化的一种近似）与凸松弛方法（$ℓ_1$最小化）之间的根本差异。你将构建一个特定的场景，其中广泛使用的$ℓ_1$最小化无法恢复真实的稀疏信号，而贪心方法在特定条件下却能成功。这个练习旨在建立你对信号特性（如动态范围）和字典属性（如相互相干性）如何相互作用以决定恢复算法成败的直观理解。[@problem_id:3455943]", "problem": "考虑一个感知矩阵 $A \\in \\mathbb{R}^{2 \\times 3}$，其列 $a_{1}, a_{2}, a_{3}$ 是单位范数列，定义为 $a_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，$a_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$，以及 $a_{3} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$。$A$ 的互相关性定义为 $\\mu(A) \\triangleq \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$，对于该矩阵 $A$ 有 $\\mu(A) = \\frac{1}{\\sqrt{2}}$。设未知信号 $x^{\\star} \\in \\mathbb{R}^{3}$ 的支撑集为索引集 $S^{\\star} = \\{1,2\\}$，其系数为正，$x_{1}^{\\star} = \\alpha$，$x_{2}^{\\star} = \\beta$，和 $x_{3}^{\\star} = 0$，其中 $\\alpha \\geq \\beta  0$。测量是无噪声的：$y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$。定义动态范围 $R \\triangleq \\alpha / \\beta$。\n\n从第一性原理出发，使用互相关性、正交匹配追踪（OMP；一种通过相关性选择原子并执行正交投影的贪心算法）和$\\ell_{1}$基追踪（在 $A x = y$ 的约束下最小化 $x$ 的 $\\ell_{1}$ 范数）的定义来完成以下任务：\n\n1. 证明 $\\ell_{1}$基追踪解无法恢复真实支撑集 $S^{\\star}$，而是倾向于一个包含错误原子 $a_{3}$ 的不同双项表示。您必须比较 $y$ 的精确双稀疏表示的 $\\ell_{1}$ 范数。\n\n2. 根据互相关性 $\\mu(A)$ 和动态范围 $R$，推导OMP第一步和第二步中正确原子与错误原子之间的精确相关性余量。具体来说，设 $M_{1}$ 表示第一步中最大正确相关性与最大错误相关性之间的差值，设 $M_{2}$ 表示经过正交投影后第二步中的该差值。将 $M_{1}$ 和 $M_{2}$ 完全表示为 $\\mu(A)$、$R$ 和 $\\beta$ 的解析函数。\n\n3. 确定最小动态范围 $R_{\\mathrm{min}}$，使得OMP在第一步选择 $a_{1}$，然后在第二步选择 $a_{2}$，从而恢复真实支撑集 $S^{\\star}$。您的最终答案必须是这个 $R_{\\mathrm{min}}$ 的精确解析形式。无需四舍五入，不涉及单位。\n\n最终答案必须是单一的闭式解析表达式。", "solution": "问题陈述已经过验证，被认为是有效的。它是自洽的，科学上基于压缩感知的原理，并且是适定的。所有提供的定义和数据都是一致的。给定的互相关性值 $\\mu(A) = \\frac{1}{\\sqrt{2}}$ 是正确的，通过计算内积可以验证：$|a_{1}^{\\top}a_{2}| = |0| = 0$，$|a_{1}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$，以及 $|a_{2}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$。其中的最大值为 $\\frac{1}{\\sqrt{2}}$。\n\n我们现在开始处理问题的三个部分。测量向量由 $y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$ 给出。代入 $a_{1}$ 和 $a_{2}$ 的定义：\n$$y = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$$\n\n**1. $\\ell_{1}$基追踪失败的证明**\n\n$\\ell_{1}$基追踪问题旨在找到一个系数向量 $x$，在满足测量约束的条件下最小化其 $\\ell_{1}$ 范数：\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y $$\n真实稀疏信号为 $x^{\\star} = (\\alpha, \\beta, 0)^{\\top}$，其支撑集为 $S^{\\star} = \\{1, 2\\}$。真实信号的 $\\ell_{1}$ 范数为：\n$$ \\|x^{\\star}\\|_{1} = |\\alpha| + |\\beta| + |0| = \\alpha + \\beta $$\n因为题目给定 $\\alpha \\geq \\beta  0$。\n\n为了证明 $\\ell_{1}$基追踪无法恢复 $x^{\\star}$，我们必须证明存在另一个信号 $x^{\\text{alt}}$，使得 $A x^{\\text{alt}} = y$ 但 $\\|x^{\\text{alt}}\\|_{1}  \\|x^{\\star}\\|_{1}$。问题建议考虑一个包含原子 $a_{3}$ 的表示。让我们使用原子 $\\{a_{1}, a_{3}\\}$ 来寻找 $y$ 的一个2-稀疏表示。设这个替代信号为 $x^{(2)} = (c_{1}, 0, c_{3})^{\\top}$。我们必须满足约束 $A x^{(2)} = y$：\n$$ c_{1} a_{1} + c_{3} a_{3} = y $$\n$$ c_{1} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_{3} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} $$\n这产生一个线性方程组：\n$$ \\begin{cases} c_{1} + \\frac{c_{3}}{\\sqrt{2}} = \\alpha \\\\ \\frac{c_{3}}{\\sqrt{2}} = \\beta \\end{cases} $$\n由第二个方程，我们得到 $c_{3} = \\sqrt{2}\\beta$。将此代入第一个方程得到 $c_{1} + \\beta = \\alpha$，所以 $c_{1} = \\alpha - \\beta$。\n替代信号为 $x^{(2)} = (\\alpha - \\beta, 0, \\sqrt{2}\\beta)^{\\top}$。\n\n现在我们计算 $x^{(2)}$ 的 $\\ell_{1}$ 范数。因为 $\\alpha \\geq \\beta  0$，我们有 $\\alpha - \\beta \\geq 0$ 且 $\\sqrt{2}\\beta  0$。\n$$ \\|x^{(2)}\\|_{1} = |\\alpha - \\beta| + |0| + |\\sqrt{2}\\beta| = (\\alpha - \\beta) + \\sqrt{2}\\beta = \\alpha + (\\sqrt{2}-1)\\beta $$\n我们将其与真实信号的 $\\ell_{1}$ 范数进行比较：\n$$ \\|x^{\\star}\\|_{1} = \\alpha + \\beta $$\n差值为 $\\|x^{\\star}\\|_{1} - \\|x^{(2)}\\|_{1} = (\\alpha + \\beta) - (\\alpha + (\\sqrt{2}-1)\\beta) = \\beta - (\\sqrt{2}-1)\\beta = (1 - (\\sqrt{2}-1))\\beta = (2-\\sqrt{2})\\beta$。\n由于 $\\sqrt{2}  2$ 且 $\\beta > 0$，该差值是严格为正的。\n$$ \\|x^{(2)}\\|_{1}  \\|x^{\\star}\\|_{1} $$\n因为存在一个 $y$ 的替代表示，其 $\\ell_{1}$ 范数严格更小，所以 $\\ell_{1}$基追踪问题的解不是真实信号 $x^{\\star}$。$\\ell_{1}$ 解的支撑集将包含索引3，因此无法恢复真实支撑集 $S^{\\star}=\\{1,2\\}$。\n\n**2. OMP相关性余量**\n\n正交匹配追踪（OMP）算法迭代地选择与当前残差最相关的 $A$ 的列。\n\n**第一步 (OMP-1):**\n初始残差为 $r_{0} = y = \\alpha a_{1} + \\beta a_{2}$。我们计算原子与此残差的相关性：\n$$ |a_{1}^{\\top} r_{0}| = |a_{1}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{1}^{\\top}a_{1}) + \\beta (a_{1}^{\\top}a_{2})| = |\\alpha \\cdot 1 + \\beta \\cdot 0| = \\alpha $$\n$$ |a_{2}^{\\top} r_{0}| = |a_{2}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{2}^{\\top}a_{1}) + \\beta (a_{2}^{\\top}a_{2})| = |\\alpha \\cdot 0 + \\beta \\cdot 1| = \\beta $$\n$$ |a_{3}^{\\top} r_{0}| = |a_{3}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{3}^{\\top}a_{1}) + \\beta (a_{3}^{\\top}a_{2})| = \\left|\\alpha \\frac{1}{\\sqrt{2}} + \\beta \\frac{1}{\\sqrt{2}}\\right| = \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\n正确的原子是 $a_{1}$ 和 $a_{2}$。错误的原子是 $a_{3}$。\n最大的正确相关性为 $\\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|) = \\max(\\alpha, \\beta) = \\alpha$，因为 $\\alpha \\geq \\beta$。\n最大的（也是唯一的）错误相关性为 $|a_{3}^{\\top} r_{0}| = \\frac{\\alpha + \\beta}{\\sqrt{2}}$。\n相关性余量 $M_{1}$ 是差值：\n$$ M_{1} = \\alpha - \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\n使用 $\\mu(A) = \\frac{1}{\\sqrt{2}}$ 和 $R = \\frac{\\alpha}{\\beta}$，我们将 $M_{1}$ 表示为：\n$$ M_{1} = \\alpha - \\mu(A)(\\alpha + \\beta) = \\alpha(1-\\mu(A)) - \\beta\\mu(A) = \\beta R(1-\\mu(A)) - \\beta\\mu(A) = \\beta[R(1-\\mu(A)) - \\mu(A)] $$\n\n**第二步 (OMP-2):**\n为了使OMP成功，它必须首先选择一个正确的原子。如第3部分所示，这要求选择 $a_{1}$，这意味着我们假设 $M_{1}0$。选择原子 $a_{1}$ 后，OMP通过将 $y$ 正交投影到由 $a_{1}$ 张成的子空间上，并从 $y$ 中减去该投影来更新残差。新的残差 $r_{1}$ 是：\n$$ r_{1} = y - P_{a_{1}}(y) = y - (a_{1}^{\\top}y)a_{1} $$\n我们有 $a_{1}^{\\top}y = a_{1}^{\\top}(\\alpha a_{1} + \\beta a_{2}) = \\alpha$。因此：\n$$ r_{1} = (\\alpha a_{1} + \\beta a_{2}) - \\alpha a_{1} = \\beta a_{2} $$\n下一个原子是通过在剩余的原子 $\\{a_{2}, a_{3}\\}$ 中最大化与 $r_{1}$ 的相关性来选择的：\n$$ |a_{2}^{\\top} r_{1}| = |a_{2}^{\\top} (\\beta a_{2})| = \\beta|a_{2}^{\\top}a_{2}| = \\beta $$\n$$ |a_{3}^{\\top} r_{1}| = |a_{3}^{\\top} (\\beta a_{2})| = \\beta|a_{3}^{\\top}a_{2}| = \\beta \\frac{1}{\\sqrt{2}} $$\n要选择的正确原子是 $a_{2}$，错误原子是 $a_{3}$。相关性余量 $M_{2}$ 是：\n$$ M_{2} = \\beta - \\frac{\\beta}{\\sqrt{2}} = \\beta \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\n使用 $\\mu(A) = \\frac{1}{\\sqrt{2}}$，这可以简化为：\n$$ M_{2} = \\beta(1 - \\mu(A)) $$\n该余量不依赖于动态范围 $R$。\n\n**3. OMP成功的最小动态范围**\n\n如果OMP在第一步选择原子 $a_{1}$ 并在第二步选择原子 $a_{2}$，它就能成功恢复支撑集 $S^{\\star} = \\{1,2\\}$。\n\n**第一步的条件：** OMP必须从真实支撑集 $S^{\\star}=\\{1,2\\}$ 中选择一个原子。相关性为 $\\alpha$ 和 $\\beta$。来自真实支撑集的最大相关性是 $\\alpha$。这必须严格大于与错误原子 $a_{3}$ 的相关性。\n$$ \\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|)  |a_{3}^{\\top} r_{0}| $$\n$$ \\alpha  \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\n我们对动态范围 $R = \\alpha/\\beta$ 求解此不等式：\n$$ \\alpha\\sqrt{2}  \\alpha + \\beta $$\n$$ \\alpha(\\sqrt{2} - 1)  \\beta $$\n$$ \\frac{\\alpha}{\\beta}  \\frac{1}{\\sqrt{2} - 1} $$\n为了使分母有理化，我们将分子和分母同乘以 $(\\sqrt{2} + 1)$：\n$$ \\frac{1}{\\sqrt{2} - 1} = \\frac{\\sqrt{2} + 1}{(\\sqrt{2} - 1)(\\sqrt{2} + 1)} = \\frac{\\sqrt{2} + 1}{2 - 1} = \\sqrt{2} + 1 $$\n因此，第一步成功的条件是 $R  \\sqrt{2} + 1$。\n\n**第二步的条件：** 如果第一步成功（即 $R  \\sqrt{2} + 1$），则选择原子 $a_{1}$。残差为 $r_{1} = \\beta a_{2}$。然后OMP必须从剩余的原子 $\\{a_{2}, a_{3}\\}$ 中选择 $a_{2}$。这要求：\n$$ |a_{2}^{\\top} r_{1}|  |a_{3}^{\\top} r_{1}| $$\n如前所计算，这是：\n$$ \\beta  \\frac{\\beta}{\\sqrt{2}} $$\n由于 $\\beta  0$，我们可以除以 $\\beta$ 得到 $1  \\frac{1}{\\sqrt{2}}$，这是成立的，因为 $\\sqrt{2}  1$。因此，如果第一步成功，第二步也保证成功。\n\nOMP通过先选择 $a_1$ 再选择 $a_2$ 来恢复真实支撑集 $S^{\\star}$ 的总体条件完全由第一步的条件决定：$R  \\sqrt{2} + 1$。问题要求出现这种情况的最小动态范围 $R_{\\mathrm{min}}$。这对应于保证成功的 $R$ 值集合的下确界。\n$$ R_{\\mathrm{min}} = \\sqrt{2} + 1 $$\n在这个边界值上，正确原子 $a_{1}$ 的相关性与错误原子 $a_{3}$ 的相关性相等，如果没有有利的平局决胜规则，就不能严格保证成功恢复。对于任何 $R  R_{\\mathrm{min}}$，OMP的成功是有保证的。", "answer": "$$\\boxed{\\sqrt{2} + 1}$$", "id": "3455943"}, {"introduction": "在具体示例的基础上，本次练习将我们的视野拓宽到对稀疏恢复理论保证的更广泛分析。$ℓ_0$最小化的成功并非偶然，它受到传感矩阵（或称“字典”）几何特性的制约。通过一系列分析性问题，你将比较众所周知且易于计算的基于相干性的恢复条件与更根本的基于Spark的条件，并理解前者的“悲观性”所在。此外，你还将研究像矩阵预处理这样的实用技术如何通过改变字典的几何结构来改善恢复保证。[@problem_id:3455953]", "problem": "考虑无噪声线性测量模型 $y = A x$，其中 $A \\in \\mathbb{R}^{3 \\times 4}$，其列向量 $a_1, a_2, a_3, a_4 \\in \\mathbb{R}^3$ 由下式给出\n$$\na_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\na_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix},\\quad\na_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\na_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n$$\n字典 $A$ 的互 coherence $\\mu(A)$ 定义为\n$$\n\\mu(A) \\triangleq \\max_{i \\neq j} \\frac{|\\langle a_i, a_j \\rangle|}{\\|a_i\\|_2 \\, \\|a_j\\|_2},\n$$\n而 $A$ 的 spark，记为 $\\mathrm{spark}(A)$，是 $A$ 中线性相关的列的最小数量。对于通过 $\\ell_0$ 最小化实现的稀疏恢复，两个广泛使用的唯一性条件是：一个依赖于 $\\mu(A)$ 的基于 coherence 的条件，以及一个依赖于 $\\mathrm{spark}(A)$ 的基于 spark 的条件。在本题中，您将分析这个特定的高 coherence 示例，量化基于 coherence 的界相对于基于 spark 的界的悲观程度，并评估列归一化、正交化和左预处理如何影响互 coherence 和隐含的唯一性阈值。\n\n定义左预处理器\n$$\nW \\triangleq \\big(A A^\\top\\big)^{-1/2},\n$$\n和预处理后的字典 $B \\triangleq W A$。您可以假设 $A A^\\top$ 是对称正定的，并且 $W$ 是良定义的。\n\n选择所有正确的陈述：\n\nA. 互 coherence 满足 $\\mu(A) = \\frac{1}{\\sqrt{3}}$，且基于 coherence 的唯一性阈值 $k$ 严格低于基于 spark 的阈值 $k  \\frac{\\mathrm{spark}(A)}{2}$，这表明对于此 $A$，基于 coherence 的界是悲观的。\n\nB. $A$ 的 spark 为 $\\mathrm{spark}(A) = 3$。\n\nC. 对于此 $A$，将其列归一化为单位 $\\ell_2$ 范数不会改变 $\\mu(A)$，因为互 coherence 是使用归一化内积定义的；因此，基于 coherence 的关于 $k$ 的阈值保持不变。\n\nD. 使用左预处理器 $W = \\big(A A^\\top\\big)^{-1/2}$，预处理后的字典 $B = W A$ 的互 coherence 变为 $\\mu(B) = \\frac{1}{3}$，且基于 coherence 的唯一性条件变为 $k  2$，与基于 spark 的阈值相匹配（尽管由于严格不等式，仍然禁止 $k=2$）。\n\nE. 对列进行右正交化（例如，用一个正交矩阵 $Q \\in \\mathbb{R}^{4 \\times 4}$ 将 $A$ 替换为 $A Q$ 以使其列向量标准正交）会保留 $x$ 的支撑集，并在不改变原始 $x$ 的稀疏恢复保证的情况下，有效地将互 coherence 降至 $0$。", "solution": "我们逐步分析并评估每个选项。\n\n**1. 计算 $\\mathrm{spark}(A)$ 和 $\\mu(A)$**\n\n*   **Spark:** 我们需要找到 $A$ 中线性相关的列的最小数量。\n    *   任意2列都是线性无关的。\n    *   我们检查任意3列的子集。例如，子矩阵 $[a_1, a_2, a_3]$ 的行列式为 $\\det \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 0  1  0 \\end{pmatrix} = -1 \\ne 0$，因此它们是线性无关的。对所有由3列构成的子集进行类似检查，都会发现它们是线性无关的。因此, $\\mathrm{spark}(A)  3$。\n    *   由于 $A$ 是一个 $3 \\times 4$ 矩阵，其4个列向量在 $\\mathbb{R}^3$ 中必定是线性相关的。因此，$\\mathrm{spark}(A) = 4$。\n\n*   **Mutual Coherence:** 首先计算列范数：$\\|a_1\\|_2 = 1$, $\\|a_2\\|_2 = \\sqrt{3}$, $\\|a_3\\|_2 = 1$, $\\|a_4\\|_2 = 1$。\n    *   然后计算归一化内积：\n        *   $\\frac{|\\langle a_1, a_2 \\rangle|}{\\|a_1\\|_2 \\|a_2\\|_2} = \\frac{|1|}{1 \\cdot \\sqrt{3}} = \\frac{1}{\\sqrt{3}}$\n        *   $\\frac{|\\langle a_2, a_3 \\rangle|}{\\|a_2\\|_2 \\|a_3\\|_2} = \\frac{|1|}{\\sqrt{3} \\cdot 1} = \\frac{1}{\\sqrt{3}}$\n        *   $\\frac{|\\langle a_2, a_4 \\rangle|}{\\|a_2\\|_2 \\|a_4\\|_2} = \\frac{|1|}{\\sqrt{3} \\cdot 1} = \\frac{1}{\\sqrt{3}}$\n    *   所有其他不同列对的内积均为0。因此，最大值为 $\\mu(A) = \\frac{1}{\\sqrt{3}}$。\n\n**2. 评估选项**\n\n*   **选项 A:**\n    *   $\\mu(A) = \\frac{1}{\\sqrt{3}}$ 的计算是正确的。\n    *   基于 coherence 的唯一性条件是 $k  \\frac{1}{2} (1 + \\frac{1}{\\mu(A)}) = \\frac{1}{2} (1 + \\sqrt{3}) \\approx 1.366$。\n    *   基于 spark 的唯一性条件是 $k  \\frac{\\mathrm{spark}(A)}{2} = \\frac{4}{2} = 2$。\n    *   由于 $1.366  2$，基于 coherence 的条件给出的保证（仅对 $k=1$）比基于 spark 的条件（同样仅对 $k=1$，但界限更宽松）更严格。这表明基于 coherence 的界是“悲观的”。因此，**选项A是正确的**。\n\n*   **选项 B:**\n    *   我们已经计算出 $\\mathrm{spark}(A) = 4$。该选项声称 $\\mathrm{spark}(A) = 3$。因此，**选项B是错误的**。\n\n*   **选项 C:**\n    *   互 coherence $\\mu(A)$ 的定义中已经包含了列范数的归一化。因此，在计算之前先将列归一化为单位范数不会改变最终结果。因此，基于 coherence 的阈值也保持不变。因此，**选项C是正确的**。\n\n*   **选项 D:**\n    *   我们计算预处理后的字典 $B = WA$ 的 Gram 矩阵 $B^\\top B = A^\\top (AA^\\top)^{-1} A$。\n    *   $AA^\\top = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix}$，其逆矩阵为 $(AA^\\top)^{-1} = \\frac{1}{4} \\begin{pmatrix} 3  -1  -1 \\\\ -1  3  -1 \\\\ -1  -1  3 \\end{pmatrix}$。\n    *   通过计算 $G_{ij} = a_i^\\top (AA^\\top)^{-1} a_j$，我们发现对角元素 $(B^\\top B)_{ii} = \\frac{3}{4}$，而所有非对角元素的绝对值 $|(B^\\top B)_{ij}| = \\frac{1}{4}$。\n    *   因此，$\\mu(B) = \\frac{\\max_{i \\ne j} |(B^\\top B)_{ij}|}{(B^\\top B)_{ii}} = \\frac{1/4}{3/4} = \\frac{1}{3}$。\n    *   新的基于 coherence 的唯一性条件变为 $k  \\frac{1}{2} (1 + \\frac{1}{\\mu(B)}) = \\frac{1}{2} (1 + 3) = 2$。\n    *   这个新界 $k  2$ 确实与基于 spark 的界相匹配。因此，**选项D是正确的**。\n\n*   **选项 E:**\n    *   $A$ 是一个 $3 \\times 4$ 矩阵。它的4个列向量位于 $\\mathbb{R}^3$ 中，因此它们不可能是标准正交的（因为标准正交向量必须是线性无关的）。所以，这个前提就是不可能实现的。\n    *   此外，变换 $z = Q^\\top x$ 通常会破坏 $x$ 的稀疏性（即，如果 $x$ 是稀疏的，$z$ 通常是稠密的），因此稀疏恢复的整个框架不再适用。因此，**选项E是错误的**。\n\n**结论：** 正确的陈述是A、C和D。", "answer": "$$\\boxed{ACD}$$", "id": "3455953"}, {"introduction": "尽管$ℓ_0$最小化在一般情况下是NP难问题，但在某些特殊条件下，它会变得计算上易于处理。这个基于编码的练习深入探讨了其中最重要的一种情况：当传感矩阵的列是正交的。你将首先推导出在这种设定下的精确且异常简洁的全局最优性条件，然后实现一个“证书”程序，该程序可以明确地验证一个候选解是否是真正的全局最优稀疏解。这个练习有力地将抽象的理论结果与具体的计算任务联系起来，展示了稀疏恢复的组合复杂性在何种情况下会消失。[@problem_id:3455948]", "problem": "考虑对于矩阵 $A \\in \\mathbb{R}^{m \\times n}$、数据向量 $b \\in \\mathbb{R}^{m}$ 以及正则化参数 $\\lambda \\in \\mathbb{R}_{+}$ 定义的稀疏惩罚最小二乘目标函数：\n$$\nF(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0,\n$$\n其中 $\\lVert x \\rVert_0$ 表示 $x$ 中非零元素的个数。令 $S \\subseteq \\{0,1,\\dots,n-1\\}$ 为一个候选支撑集。将 $S$ 上的受限最小二乘解定义为 $x_S^\\star \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2$，其中 $A_S$ 表示由 $A$ 中索引为 $S$ 的列组成的子矩阵。将残差定义为 $r_S \\triangleq b - A_S x_S^\\star$。\n\n仅使用关于 Euclidean 投影和最小二乘法的基本事实，推导当通过单个索引 $j \\notin S$ 擴增支撑集，然后在 $S \\cup \\{j\\}$ 上重新优化时 $\\lVert A x - b \\rVert_2^2$ 的精确变化量，以及当从支撑集中剪枝单个索引 $i \\in S$，然后在 $S \\setminus \\{i\\}$ 上重新优化时的精确变化量。将这两个变化量直接用 $r_S$、$A_S$ 和候选列 $a_j \\in \\mathbb{R}^m$（$A$ 的第 $j$ 列）或被剪枝的列 $a_i$ 来表示。利用这些推导，构建认证测试，以断言在 $A$ 的列是标准正交的特殊情况下（即 $A^\\top A = I_n$），$S$ 是全局最优的。在这种标准正交情况下，证明检查所有非支撑集不等式和支撑集上剪枝不等式对于全局最优性是充分且必要的。\n\n你的程序必须实现以下逻辑：\n\n- 步骤 1 (受限回归)：给定 $A$、$b$、$\\lambda$ 和 $S$，计算 $x_S^\\star$ 和 $r_S$。\n\n- 步骤 2 (非支撑集擴增不等式)：对于每个 $j \\notin S$，计算通过将 $S$ 擴增 $j$ 并重新优化所能获得的 $\\lVert A x - b \\rVert_2^2$ 的最大可能减少量。将此减少量表示为 $\\Delta_{\\mathrm{add}}(j \\mid S)$。证书要求对于所有 $j \\notin S$，$\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$。\n\n- 步骤 3 (支撑集上剪枝不等式)：对于每个 $i \\in S$，计算从 $S$ 中剪枝 $i$ 并重新优化所导致的 $\\lVert A x - b \\rVert_2^2$ 的增加量。将此增加量表示为 $\\Delta_{\\mathrm{drop}}(i \\mid S)$。证书要求对于所有 $i \\in S$，$\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$。\n\n- 步骤 4 (全局认证的标准正交列前提条件)：当且仅当 $A^\\top A = I_n$ (在数值指定的容差范围内) 且步骤 2 和 3 成立时，证书才宣布 $S$ 为全局最优。如果 $A^\\top A \\neq I_n$，即使步骤 2 和 3 成立，程序也必须返回否定认证（即无法认证全局最优性），因为这些不等式不再是充分条件。\n\n设计你的解决方案，使其推导植根于最小二乘法的投影定理：对于任何满列秩的 $A_S$，向量 $A_S x_S^\\star$ 是 $b$ 到 $\\mathrm{span}(A_S)$ 上的正交投影，即 $A_S^\\top r_S = 0$。将擴增和剪枝解释为残差的一维回归（或移除一列后的重新拟合），并相应地计算精确的目标变化。\n\n此问题不涉及角度。没有物理单位。\n\n实现一个单一程序，评估以下测试套件并打印汇总结果。每个结果必须是一个布尔值，指示候选支撑集是否在步骤 4 描述的标准正交列测试下被认证为全局最优：\n\n- 测试用例 1 (理想情况，标准正交列):\n    - 维度: $m = 6$, $n = 4$。\n    - 矩阵 $A \\in \\mathbb{R}^{6 \\times 4}$ 的列等于 $\\mathbb{R}^6$ 中的前四个标准基向量。具体来说，$A$ 是行如下的矩阵：\n      $$\n      \\begin{bmatrix}\n      1  0  0  0 \\\\\n      0  1  0  0 \\\\\n      0  0  1  0 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - 数据 $b = [\\,2.0,\\,-1.5,\\,0.2,\\,0.0,\\,3.0,\\,-2.0\\,]^\\top$。\n    - 正则化 $\\lambda = 0.5$。\n    - 候选支撑集 $S = \\{0, 1\\}$。\n\n- 测试用例 2 (不等式中的边界等式，标准正交列):\n    - 维度: $m = 6$, $n = 4$。\n    - 矩阵 $A$ 与测试用例 1 相同。\n    - 数据 $b = [\\,1.0,\\,1.1,\\,0.9,\\,0.0,\\,-0.5,\\,0.3\\,]^\\top$。\n    - 正则化 $\\lambda = 1.0$。\n    - 候选支撑集 $S = \\{0, 1\\}$。\n\n- 测试用例 3 (非标准正交设计，无法通过这些测试认证为全局最优):\n    - 维度: $m = 6$, $n = 4$。\n    - 矩阵 $A$ 的列为\n      $$\n      a_0 = [\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_1 = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_2 = [\\,0,\\,1,\\,1,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_3 = [\\,0,\\,0,\\,1,\\,1,\\,0,\\,0\\,]^\\top.\n      $$\n      因此，\n      $$\n      A =\n      \\begin{bmatrix}\n      1  1  0  0 \\\\\n      0  1  1  0 \\\\\n      0  0  1  1 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - 数据 $b = [\\,1.5,\\,-0.5,\\,2.0,\\,-1.0,\\,0.0,\\,0.0\\,]^\\top$。\n    - 正则化 $\\lambda = 0.5$。\n    - 候选支撑集 $S = \\{0, 2\\}$。\n\n你的程序應产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3]”）。预期输出是按三个测试用例顺序排列的三个布尔值的列表。不允许有其他输出。", "solution": "问题要求推导 $\\ell_0$ 惩罚最小二乘问题的最优性条件，并实现相应的算法来认证候选支撑集 $S$ 的全局最优性，在设计矩阵 $A$ 为标准正交的特殊情况下。\n\n要最小化的目标函数是 $F(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，$\\lambda  0$，且 $\\lVert x \\rVert_0$ 是 $x \\in \\mathbb{R}^n$ 中非零项的数量。\n\n如果对应的解 $x^{(S)}$ 达到了 $F(x)$ 的最小值，则支撑集 $S \\subseteq \\{0, 1, \\dots, n-1\\}$ 是全局最优的。解 $x^{(S)}$ 定义为仅在 $S$ 中的索引上具有非零项。这些非零值由向量 $x_S^\\star \\in \\mathbb{R}^{|S|}$ 表示，通过求解受限最小二乘问题确定：\n$$\nx_S^\\star = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2.\n$$\n此处，$A_S$ 是由 $A$ 中索引为 $S$ 的列构成的矩阵。这个标准最小二乘问题的解由正规方程给出：$x_S^\\star = (A_S^\\top A_S)^{-1} A_S^\\top b$，假设 $A_S$ 具有满列秩。残差向量为 $r_S \\triangleq b - A_S x_S^\\star$。该支撑集的最小二乘误差为 $\\lVert r_S \\rVert_2^2$。总目标值为 $F(x^{(S)}) = \\lVert r_S \\rVert_2^2 + \\lambda |S|$。\n\n当且仅当对 $S$ 的任何单元素更改（即添加一个不在 $S$ 中的元素或从 $S$ 中移除一个元素）都不能使目标函数 $F(x)$ 减小时，支撑集 $S$ 是最优的。\n\n**1. 擴增支撑集（添加一个元素 $j \\notin S$）**\n\n让我们考虑添加一个索引 $j \\notin S$ 来形成新的支撑集 $S' = S \\cup \\{j\\}$。新的目标值将是 $F(x^{(S')}) = \\lVert r_{S'} \\rVert_2^2 + \\lambda (|S|+1)$。目标函数的变化量是：\n$$\n\\Delta F_{\\mathrm{add}} = F(x^{(S')}) - F(x^{(S)}) = (\\lVert r_{S'} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) + \\lambda.\n$$\n项 $\\lVert r_S \\rVert_2^2 - \\lVert r_{S'} \\rVert_2^2$ 是最小二乘误差的减少量，我们将其表示为 $\\Delta_{\\mathrm{add}}(j \\mid S)$。为使 $S$ 最优，我们必须对所有 $j \\notin S$ 都有 $\\Delta F_{\\mathrm{add}} \\ge 0$，这意味着条件：\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda \\quad \\forall j \\notin S.\n$$\n为了推导 $\\Delta_{\\mathrm{add}}(j \\mid S)$，我们利用这样一个性质：$S$ 上的最小二乘拟合将 $b$ 投影到 $A_S$ 的列空间上，记作 $\\mathrm{span}(A_S)$，而 $r_S$ 是该投影的误差，因此 $r_S$ 与 $\\mathrm{span}(A_S)$ 正交（$A_S^\\top r_S = 0$）。当我们用 $j$ 擴增支撑集时，我们试图用新的列 $a_j$ 来解释剩余的残差 $r_S$。$r_S$ 在 $\\mathrm{span}(A_{S \\cup \\{j\\}})$ 中的最佳近似是其正交投影。由于 $r_S$ 已经与 $\\mathrm{span}(A_S)$ 正交，这简化为将 $r_S$ 投影到 $a_j$ 的与 $\\mathrm{span}(A_S)$ 正交的分量上。令 $P_S = A_S (A_S^\\top A_S)^{-1} A_S^\\top$ 为到 $\\mathrm{span}(A_S)$ 上的投影矩阵。相关的新方向是 $\\tilde{a}_j = (I - P_S) a_j$。平方误差的减少量是 $r_S$ 在 $\\tilde{a}_j$ 上投影的长度的平方：\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) = \\frac{\\langle r_S, \\tilde{a}_j \\rangle^2}{\\lVert \\tilde{a}_j \\rVert_2^2} = \\frac{(r_S^\\top (I - P_S) a_j)^2}{a_j^\\top (I - P_S)^\\top (I - P_S) a_j} = \\frac{(a_j^\\top r_S)^2}{a_j^\\top (I - P_S) a_j},\n$$\n其中我们使用了 $P_S r_S = 0$ 以及 $I-P_S$ 是一个投影矩阵（幂等且对称）。\n\n**2. 剪枝支撑集（移除一个元素 $i \\in S$）**\n\n现在，考虑移除一个索引 $i \\in S$ 来形成新的支撑集 $S'' = S \\setminus \\{i\\}$。目标函数的变化是：\n$$\n\\Delta F_{\\mathrm{drop}} = F(x^{(S'')}) - F(x^{(S)}) = (\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) - \\lambda.\n$$\n项 $\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2$ 是最小二乘误差的增加量，记为 $\\Delta_{\\mathrm{drop}}(i \\mid S)$。为使 $S$ 最优，我们需要对所有 $i \\in S$ 都有 $\\Delta F_{\\mathrm{drop}} \\ge 0$，这意味着：\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda \\quad \\forall i \\in S.\n$$\n在一般情况下，$\\Delta_{\\mathrm{drop}}(i \\mid S)$ 的推导更为复杂。线性回归中的一个已知结果表明，当一个变量被移除时，残差平方和的增加量与其系数和方差-协方差矩阵有关。具体来说，如果 $C_S = (A_S^\\top A_S)^{-1}$，则\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) = \\frac{((x_S^\\star)_i)^2}{(C_S)_{ii}},\n$$\n其中 $(x_S^\\star)_i$ 是受限解中列 $a_i$ 的系数，$(C_S)_{ii}$ 是 $C_S$ 中对应于索引 $i$ 的对角线元素。\n\n**3. 特殊情况：标准正交列 ($A^\\top A = I_n$)**\n\n当 $A$ 的列是标准正交时，认证全局最优性的问题大大简化。在这种情况下，$A^\\top A = I_n$，并且对于任何子集 $S$，$A_S^\\top A_S = I_{|S|}$。\n\n- **$\\Delta_{\\mathrm{add}}(j \\mid S)$ 的简化**：投影矩阵是 $P_S = A_S A_S^\\top$。由于 $j \\notin S$，$a_j$与 $A_S$ 中的所有列正交，因此 $A_S^\\top a_j = 0$，这意味着 $P_S a_j = A_S(A_S^\\top a_j) = 0$。$\\Delta_{\\mathrm{add}}$ 公式中的分母变为 $a_j^\\top (I-P_S) a_j = a_j^\\top a_j = \\lVert a_j \\rVert_2^2 = 1$。分子是 $(a_j^\\top r_S)^2$。因此：\n  $$\n  \\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2.\n  $$\n\n- **$\\Delta_{\\mathrm{drop}}(i \\mid S)$ 的简化**：矩阵 $C_S = (A_S^\\top A_S)^{-1} = I_{|S|}$，所以其对角元素都为 1。$\\Delta_{\\mathrm{drop}}$ 的公式变为：\n  $$\n  \\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_i)^2.\n  $$\n\n**4. 标准正交性下的充分必要性**\n\n对于一个标准正交矩阵 $A$，目标函数 $F(x)$ 变得可分离。令 $c = A^\\top b$。\n$$\n\\lVert Ax - b \\rVert_2^2 = (Ax-b)^\\top(Ax-b) = x^\\top A^\\top Ax - 2x^\\top A^\\top b + b^\\top b = \\lVert x \\rVert_2^2 - 2x^\\top c + \\lVert b \\rVert_2^2.\n$$\n通过配方法，得到 $\\lVert x-c \\rVert_2^2 - \\lVert c \\rVert_2^2 + \\lVert b \\rVert_2^2$。最小化 $F(x)$ 等价于最小化 $\\lVert x-c \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$。这可以分解为 $n$ 个独立的标量问题：\n$$\n\\min_{x_k} (x_k - c_k)^2 + \\lambda I(x_k \\ne 0) \\quad \\text{for } k=0, 1, \\dots, n-1.\n$$\n对于每个 $k$，我们有两个选择：\n1. $x_k = 0$：成本为 $c_k^2$。\n2. $x_k \\ne 0$：为最小化 $(x_k - c_k)^2$，我们必须选择 $x_k = c_k$。成本为 $\\lambda$。\n\n如果 $\\lambda  c_k^2$，最优选择是 $x_k=c_k$；如果 $\\lambda > c_k^2$，最优选择是 $x_k=0$。如果 $\\lambda=c_k^2$，两种选择给出相同的目标值。一个常见的约定是倾向于更稀疏的解，所以 $x_k=0$。\n最优支撑集是 $S^* = \\{k \\mid c_k^2 > \\lambda\\} = \\{k \\mid (a_k^\\top b)^2 > \\lambda\\}$。\n\n让我们对照这个基准真值来检查我们对给定支撑集 $S$ 的证书条件：\n- **擴增条件**：$\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$。在 $A$ 标准正交的情况下，$r_S = b - A_S A_S^\\top b$。那么 $a_j^\\top r_S = a_j^\\top b - a_j^\\top A_S (A_S^\\top b) = a_j^\\top b$ 因为 $a_j^\\top A_S = 0$。所以条件是 $(a_j^\\top b)^2 \\le \\lambda$。这正是将索引 $j \\notin S$ 正确地从最优支撑集中排除的条件。\n- **剪枝条件**：$\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$。在 $A$ 标准正交的情况下，$x_S^\\star = A_S^\\top b$，所以 $(x_S^\\star)_i = a_i^\\top b$。条件是 $(a_i^\\top b)^2 \\ge \\lambda$。这正是将索引 $i \\in S$ 正确地包含在最优支撑集中的条件。\n\n因此，当且仅当 $A$ 具有标准正交列时，这些条件是支撑集 $S$ 全局最优的充分必要条件。如果 $A$ 不是标准正交的，这些简化的测试不足以保证全局最优性。\n\n**算法设计**\n\n程序将对给定的测试用例 $(A, b, \\lambda, S)$ 实现以下步骤：\n1.  **标准正交性检查**：验证 $A^\\top A$ 是否接近单位矩阵 $I_n$。如果不是，证书无效，结果为 `False`。\n2.  **设置**：如果 A 是标准正交的，使用 $A_S$ 和 $b$ 的最小二乘法计算 $x_S^\\star$，然后计算残差 $r_S = b - A_S x_S^\\star$。\n3.  **擴增测试**：对于不在 $S$ 中的每个索引 $j$，计算 $\\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2$ 并检查其是否小于或等于 $\\lambda$。如果该条件对任何 $j$ 失败，结果为 `False`。\n4.  **剪枝测试**：对于 $S$ 中的每个索引 $i$，找到相应的系数 $(x_S^\\star)_k$。计算 $\\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_k)^2$ 并检查其是否大于或等于 $\\lambda$。如果该条件对任何 $i$ 失敗，结果为 `False`。\n5.  **认证**：如果所有测试都通过，支撑集 $S$ 被认证为全局最优，结果为 `True`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by evaluating the global optimality certificate for three test cases.\n    \"\"\"\n\n    def certify_global_optimality(A, b, lambda_reg, S):\n        \"\"\"\n        Certifies if a support set S is globally optimal for the l0-penalized\n        least squares problem, under the condition that matrix A has orthonormal columns.\n        \"\"\"\n        m, n = A.shape\n\n        # Step 4: Orthonormal-column prerequisite for global certification.\n        # The certificate is only sufficient if A has orthonormal columns.\n        if not np.allclose(A.T @ A, np.eye(n)):\n            return False\n\n        S_list = sorted(list(S))\n        off_support_indices = sorted(list(set(range(n)) - set(S)))\n\n        # Step 1: Restricted regression to compute x_S_star and r_S.\n        if not S_list:  # Support S is empty\n            x_S_star = np.array([])\n            r_S = b\n        else:\n            A_S = A[:, S_list]\n            # For orthonormal A, x_S_star = A_S.T @ b, but lstsq is more general and robust.\n            x_S_star = np.linalg.lstsq(A_S, b, rcond=None)[0]\n            r_S = b - A_S @ x_S_star\n\n        # Step 2: Off-support augmentation inequalities.\n        # Check if Delta_add(j|S) = lambda for all j not in S.\n        for j in off_support_indices:\n            a_j = A[:, j]\n            # For orthonormal A, Delta_add(j|S) = (a_j^T * r_S)^2\n            delta_add = (a_j.T @ r_S)**2\n            if delta_add > lambda_reg:\n                return False\n\n        # Step 3: On-support pruning inequalities.\n        # Check if Delta_drop(i|S) >= lambda for all i in S.\n        # x_S_star contains coefficients ordered according to S_list.\n        for coeff in x_S_star:\n            # For orthonormal A, Delta_drop(i|S) = (coefficient for a_i)^2\n            delta_drop = coeff**2\n            if delta_drop  lambda_reg:\n                return False\n\n        # If all checks pass, the support S is certified globally optimal.\n        return True\n\n    # Test case 1 (Happy path, orthonormal columns)\n    test_case_1 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([2.0, -1.5, 0.2, 0.0, 3.0, -2.0]),\n        0.5,\n        {0, 1}\n    )\n\n    # Test case 2 (Boundary equalities, orthonormal columns)\n    test_case_2 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.0, 1.1, 0.9, 0.0, -0.5, 0.3]),\n        1.0,\n        {0, 1}\n    )\n\n    # Test case 3 (Non-orthonormal design)\n    test_case_3 = (\n        np.array([\n            [1, 1, 0, 0],\n            [0, 1, 1, 0],\n            [0, 0, 1, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.5, -0.5, 2.0, -1.0, 0.0, 0.0]),\n        0.5,\n        {0, 2}\n    )\n    \n    test_cases = [test_case_1, test_case_2, test_case_3]\n    \n    results = []\n    for case in test_cases:\n        A, b, lambda_reg, S = case\n        result = certify_global_optimality(A, b, lambda_reg, S)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n```", "id": "3455948"}]}