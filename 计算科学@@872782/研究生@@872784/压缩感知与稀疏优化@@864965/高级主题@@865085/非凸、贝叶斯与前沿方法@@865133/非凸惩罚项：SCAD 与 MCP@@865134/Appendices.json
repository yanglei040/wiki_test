{"hands_on_practices": [{"introduction": "要掌握SCAD和MCP等非凸惩罚项，第一步是理解它们的核心机制——阈值算子。这些算子决定了在优化过程中如何对系数进行收缩或置零。本练习 [@problem_id:3462713] 通过一个具体的数值实例，让您亲手计算SCAD和MCP的阈值函数，直观地感受它们与Lasso的软阈值算子有何不同，并比较它们在特定情况下的估计误差。", "problem": "考虑稀疏估计中惩罚最小二乘法的单坐标近端更新，其中估计量 $T(z)$ 最小化关于 $b \\in \\mathbb{R}$ 的单变量目标函数 $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$。惩罚项 $P$ 是平滑裁剪绝对偏差 (SCAD) 惩罚或极小极大凹惩罚 (MCP)，每种都由一个正则化强度 $\\lambda  0$ 和一个形状参数来参数化。\n\n在标准的坐标下降框架下进行研究，其中可微分支的最优性条件是 $b - z + \\partial P(b) = 0$。这里将 $\\partial P(b)$ 解释为 $P$ 在 $b \\ge 0$ 上关于 $b$ 的导数，并利用 $z$ 符号的对称性。\n\n在非负半轴上使用以下惩罚项导数的基本定义：\n- 对于参数为 $(\\lambda,a)$（其中 $a  2$）的 SCAD，将 $P_{\\text{SCAD}}(t)$ 在 $t \\ge 0$ 上的导数 $p'_{\\text{SCAD}}(t)$ 定义为\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{a\\lambda - t}{a - 1},  \\lambda  t \\le a\\lambda, \\\\\n0,  t > a\\lambda,\n\\end{cases}\n$$\n并通过 $b$ 中次梯度的奇对称性将其扩展到 $b  0$。\n- 对于参数为 $(\\lambda,\\gamma)$（其中 $\\gamma  1$）的MCP，将 $P_{\\text{MCP}}(t)$ 在 $t \\ge 0$ 上的导数 $p'_{\\text{MCP}}(t)$ 定义为\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\dfrac{t}{\\gamma\\lambda}\\right),  0 \\le t \\le \\gamma\\lambda, \\\\\n0,  t > \\gamma\\lambda,\n\\end{cases}\n$$\n并通过 $b$ 中次梯度的奇对称性将其扩展到 $b  0$。\n\n取数值实例 $z = 3.2$, $\\lambda = 1.0$, $a = 3.7$, 以及 $\\gamma = 2.5$。显式计算 SCAD 阈值 $T_{\\text{SCAD}}(z)$ 和 MCP 阈值 $T_{\\text{MCP}}(z)$，它们被理解为在各自惩罚项下 $\\frac{1}{2}(b - z)^{2} + P(|b|)$ 的最小化子。然后，假设真实的潜在系数为零，通过计算 $\\left(T_{\\text{SCAD}}(z) - 0\\right)^{2}$ 和 $\\left(T_{\\text{MCP}}(z) - 0\\right)^{2}$ 来比较这两个估计量在平方损失下的表现。给出差值\n$$\n\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}\n$$\n的值。\n\n你的最终答案必须是包含 $T_{\\text{SCAD}}(z)$、$T_{\\text{MCP}}(z)$ 和 $\\Delta$ 这三个量的行矩阵，按此顺序排列，并写成未经四舍五入的精确值。", "solution": "该问题要求计算平滑裁剪绝对偏差 (SCAD) 和极小极大凹惩罚 (MCP) 惩罚项的阈值函数，并随后在特定情况下评估这些函数及其在平方损失下的差异。\n\n估计量 $T(z)$ 被定义为单变量目标函数的最小化子：\n$$F(b;z) = \\frac{1}{2}(b - z)^{2} + P(|b|)$$\n最小化子 $b$ 是惩罚项 $P(|\\cdot|)$ 在 $z$ 点的近端算子。由于惩罚项 $P(|b|)$ 和二次项 $(b-z)^2$ 的对称性，解 $b = T(z)$ 将与 $z$ 有相同的符号。给定 $z = 3.2 > 0$，我们可以将最小化子的搜索范围限制在 $b \\ge 0$。对于 $b \\ge 0$，目标函数变为 $F(b;z) = \\frac{1}{2}(b - z)^{2} + P(b)$ (因为 $|b|=b$）。\n\n在 $b > 0$ 处取得最小值的必要一阶条件是 $F(b;z)$ 关于 $b$ 的导数为零。该条件给出为 $b-z+p'(b) = 0$，可以重排为 $z = b + p'(b)$，其中 $p'(b)$ 是惩罚函数 $P(t)$ 在 $t=b$ 处的导数。要使 $b=0$ 成为解，我们必须满足次梯度最优性条件 $0 \\in \\partial F(0;z)$，即 $-z \\in \\partial P(|b|)|_{b=0}$。对于 SCAD 和 MCP，原点处的次梯度都是区间 $[-\\lambda, \\lambda]$。因此，对于 $|z| \\in [0, \\lambda]$，解为 $b=0$。\n\n我们给定的数值是 $z = 3.2$，SCAD 的参数为 $\\lambda = 1.0, a = 3.7$，MCP 的参数为 $\\gamma = 2.5$。\n\n首先，我们计算 SCAD 阈值 $T_{\\text{SCAD}}(z)$。\nSCAD 惩罚项在 $t \\ge 0$ 上的导数给出如下：\n$$\np'_{\\text{SCAD}}(t) = \n\\begin{cases}\n\\lambda,  0 \\le t \\le \\lambda, \\\\\n\\dfrac{a\\lambda - t}{a - 1},  \\lambda  t \\le a\\lambda, \\\\\n0,  t > a\\lambda.\n\\end{cases}\n$$\n我们基于方程 $z = b + p'_{\\text{SCAD}}(b)$ 来分析当 $z > \\lambda$ 时的解 $b$。\n1.  如果 $b \\in (\\lambda, 2\\lambda]$，这对应于软阈值解 $b = z-\\lambda$。这是从 $p'_{\\text{SCAD}}(t)$ 对于 $z \\in (\\lambda, 2\\lambda]$ 的第一种情况推导出来的。\n2.  如果 $b \\in (\\lambda, a\\lambda]$，我们使用 $p'_{\\text{SCAD}}(b)$ 的第二种情况：\n    $z = b + \\dfrac{a\\lambda - b}{a - 1} = \\dfrac{b(a-1) + a\\lambda - b}{a - 1} = \\dfrac{b(a-2) + a\\lambda}{a-1}$。\n    解出 $b$：$z(a-1) = b(a-2) + a\\lambda$，得到 $b = \\dfrac{z(a-1) - a\\lambda}{a-2}$。当得出的 $b$ 位于 $(\\lambda, a\\lambda]$ 区间内时，该解有效，这对应于 $z \\in (2\\lambda, a\\lambda]$。\n3.  如果 $b > a\\lambda$，我们有 $p'_{\\text{SCAD}}(b) = 0$，因此 $z=b$。这在 $z > a\\lambda$ 时有效。\n\n给定值为 $\\lambda=1.0$ 和 $a=3.7$， $z$ 的临界点是 $2\\lambda = 2.0$ 和 $a\\lambda = 3.7$。给定值 $z=3.2$ 满足 $2\\lambda  z \\le a\\lambda$ (即 $2.0  3.2 \\le 3.7$)。因此，我们使用情况2中的公式：\n$$T_{\\text{SCAD}}(z) = b = \\frac{z(a-1) - a\\lambda}{a-2}$$\n代入数值：\n$$T_{\\text{SCAD}}(3.2) = \\frac{3.2(3.7-1) - 3.7(1.0)}{3.7-2} = \\frac{3.2(2.7) - 3.7}{1.7} = \\frac{8.64 - 3.7}{1.7} = \\frac{4.94}{1.7}$$\n为将其表示为精确分数：\n$$T_{\\text{SCAD}}(3.2) = \\frac{4.94}{1.7} = \\frac{494}{170} = \\frac{247}{85}$$\n\n接下来，我们计算 MCP 阈值 $T_{\\text{MCP}}(z)$。\nMCP 惩罚项在 $t \\ge 0$ 上的导数为：\n$$\np'_{\\text{MCP}}(t) = \n\\begin{cases}\n\\lambda\\left(1 - \\dfrac{t}{\\gamma\\lambda}\\right),  0 \\le t \\le \\gamma\\lambda, \\\\\n0,  t > \\gamma\\lambda.\n\\end{cases}\n$$\n我们从 $z = b + p'_{\\text{MCP}}(b)$ 出发，分析当 $z > \\lambda$ 时的解 $b$。\n1.  如果 $b \\in (0, \\gamma\\lambda]$，我们使用 $p'_{\\text{MCP}}(b)$ 的第一种情况：\n    $z = b + \\lambda\\left(1 - \\dfrac{b}{\\gamma\\lambda}\\right) = b + \\lambda - \\dfrac{b}{\\gamma} = b\\left(1 - \\dfrac{1}{\\gamma}\\right) + \\lambda$。\n    解出 $b$：$b\\left(\\dfrac{\\gamma-1}{\\gamma}\\right) = z-\\lambda$，得到 $b = \\dfrac{\\gamma}{\\gamma-1}(z-\\lambda)$。当得出的 $b$ 位于 $(0, \\gamma\\lambda]$ 区间内时，该解有效，这对应于 $z \\in (\\lambda, \\gamma\\lambda]$。\n2.  如果 $b > \\gamma\\lambda$，我们有 $p'_{\\text{MCP}}(b) = 0$，因此 $z=b$。这在 $z > \\gamma\\lambda$ 时有效。\n\n给定值为 $\\lambda=1.0$ 和 $\\gamma=2.5$， $z$ 的临界点是 $\\gamma\\lambda = 2.5$。给定值 $z=3.2$ 满足 $z > \\gamma\\lambda$ (即 $3.2 > 2.5$)。因此，我们使用情况2中的公式：\n$$T_{\\text{MCP}}(z) = z$$\n代入数值 $z=3.2$：\n$$T_{\\text{MCP}}(3.2) = 3.2 = \\frac{32}{10} = \\frac{16}{5}$$\n\n最后，我们计算平方估计量之差 $\\Delta$：\n$$\\Delta = \\left(T_{\\text{MCP}}(z)\\right)^{2} - \\left(T_{\\text{SCAD}}(z)\\right)^{2}$$\n代入计算出的值：\n$$\\Delta = \\left(\\frac{16}{5}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2}$$\n为简化计算，我们找到一个公分母。由于 $85 = 5 \\times 17$，我们可以将 $\\frac{16}{5}$ 写成 $\\frac{16 \\times 17}{5 \\times 17} = \\frac{272}{85}$。\n$$\\Delta = \\left(\\frac{272}{85}\\right)^{2} - \\left(\\frac{247}{85}\\right)^{2} = \\frac{272^{2} - 247^{2}}{85^{2}}$$\n使用平方差公式 $x^2 - y^2 = (x-y)(x+y)$：\n$$\\Delta = \\frac{(272 - 247)(272 + 247)}{85^{2}} = \\frac{(25)(519)}{85^{2}}$$\n由于 $85^2 = (5 \\times 17)^2 = 25 \\times 17^2 = 25 \\times 289$：\n$$\\Delta = \\frac{25 \\times 519}{25 \\times 289} = \\frac{519}{289}$$\n分数 $\\frac{519}{289}$ 是不可约的，因为 $289 = 17^2$ 且 $519 = 17 \\times 30 + 9$，所以 $519$ 不能被 $17$ 整除。\n\n所要求的三个量是 $T_{\\text{SCAD}}(z) = \\frac{247}{85}$，$T_{\\text{MCP}}(z) = \\frac{16}{5}$，和 $\\Delta = \\frac{519}{289}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{247}{85}  \\frac{16}{5}  \\frac{519}{289}\n\\end{pmatrix}\n}\n$$", "id": "3462713"}, {"introduction": "理解了基本的阈值算子后，下一步是将其应用于解决实际的回归问题。本练习 [@problem_id:3462665] 将引导您将这些算子整合到坐标下降算法中，并解决一个在实践中常见但常被忽略的细节：当设计矩阵的列未被归一化时，如何正确地调整更新规则。通过从头推导并实现一个“重归一化”的阈值算子，您将深化对优化算法背后几何原理的理解。", "problem": "给定一个压缩感知中的线性逆模型，其设计矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，稀疏真实向量为 $x_0 \\in \\mathbb{R}^n$，以及无噪声测量值 $y = A x_0$。在实践中，$A$ 的列通常是归一化的，但制造或预处理误差可能会引入按列的缩放因子，使得实际使用的设计矩阵是 $A D$，其中 $D = \\mathrm{diag}(s_1,\\dots,s_n)$ 且 $s_j  0$。这意味着列 $A_j$ 被缩放为 $A_j \\gets s_j A_j$，并且每列的平方范数 $c_j = \\lVert A_j \\rVert_2^2$ 会偏离其目标值。您需要考察这种列归一化误差对非凸惩罚项阈值化行为的影响，并推导出一个重归一化的阈值规则，该规则在使用 Smoothly Clipped Absolute Deviation (SCAD) 和 Minimax Concave Penalty (MCP) 惩罚项进行坐标下降更新时，能正确地考虑 $c_j$。\n\n从目标函数\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2} \\lVert y - A x \\rVert_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta),\n$$\n出发，其中 $p(\\cdot;\\lambda,\\theta)$ 是 Smoothly Clipped Absolute Deviation (SCAD) 惩罚项或 Minimax Concave Penalty (MCP) 惩罚项，两者都由正则化水平 $\\lambda  0$ 和形状参数 $\\theta$（SCAD 使用 $a  2$，MCP 使用 $\\gamma  1$）参数化。您需要从第一性原理推导第 $j$ 个坐标的逐坐标一维子问题，并获得一个显式依赖于每列平方范数 $c_j = \\lVert A_j \\rVert_2^2$ 和标量 $z_j = A_j^\\top r + c_j x_j$ 的重归一化阈值规则，其中 $r = y - A x$ 是当前残差，$x_j$ 是第 $j$ 个系数的当前迭代值。您的推导必须从最小二乘项的坐标下降视角和非凸惩罚项的次梯度平稳性条件出发，并且不能直接调用任何快捷公式。\n\nSmoothly Clipped Absolute Deviation (SCAD) 惩罚项使用参数 $a  2$。Minimax Concave Penalty (MCP) 惩罚项使用参数 $\\gamma  1$。对于每种惩罚项，推导一维子问题的分段阈值函数，该函数将 $(z_j, c_j, \\lambda, \\theta)$ 映射到更新后的系数 $t_j^\\star$。当 $c_j = 1$ 时，重归一化规则必须退化为已知的阈值化行为，并且对于 $c_j \\neq 1$ 的情况，必须能正确调整阈值和收缩。\n\n实现一个近端坐标下降求解器，该求解器：\n- 将 $x$ 初始化为零向量。\n- 维护残差 $r = y - A x$。\n- 循环遍历坐标 $j = 1,\\dots,n$，计算 $z_j = A_j^\\top r + c_j x_j$，然后通过您为所选惩罚项推导的重归一化阈值规则更新 $x_j \\gets t_j^\\star$，并相应地更新残差。\n- 当所有坐标上的最大绝对变化低于某个容差或达到最大迭代次数时停止。\n\n按如下方式模拟在随机按列缩放噪声下的恢复过程：\n- 生成 $A_{\\text{base}}$，其元素独立同分布于标准正态分布，然后将每列归一化为单位欧几里得范数。\n- 对于给定的 $\\delta \\in [0,1)$，从 $[1 - \\delta, 1 + \\delta]$ 中独立均匀地抽取 $s_j$，并设置 $A = A_{\\text{base}} \\cdot \\mathrm{diag}(s)$ (按列缩放)。\n- 均匀随机地选择一个大小为 $k$ 的支撑集，并在该支撑集上用从 $[0.5, 1.0]$ 均匀抽取的随机符号和大小填充 $x_0$。设置 $y = A x_0$。\n- 使用给定的超参数 $(\\lambda, a)$ (SCAD) 或 $(\\lambda, \\gamma)$ (MCP) 运行您的求解器，惩罚项为 SCAD 或 MCP。\n\n将精确支撑集恢复定义为：恢复出的 $x$ 中 $k$ 个最大绝对值项的索引集合与 $x_0$ 的真实支撑集相等的事件。\n\n您的程序必须评估以下测试套件，每个测试用例指定为 $(\\text{penalty}, \\delta, \\lambda, \\theta, m, n, k, \\text{seed})$：\n- 用例 1: MCP, $\\delta = 0.0$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 1$。\n- 用例 2: MCP, $\\delta = 0.1$, $\\lambda = 0.08$, $\\gamma = 3.0$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 2$。\n- 用例 3: MCP, $\\delta = 0.3$, $\\lambda = 0.08$, $\\gamma = 1.2$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 3$。\n- 用例 4: SCAD, $\\delta = 0.0$, $\\lambda = 0.08$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 4$。\n- 用例 5: SCAD, $\\delta = 0.3$, $\\lambda = 0.08$, $a = 2.1$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 5$。\n- 用例 6: SCAD, $\\delta = 0.2$, $\\lambda = 0.25$, $a = 3.7$, $m = 120$, $n = 200$, $k = 10$, $\\text{seed} = 6$。\n\n对于每个用例，您的程序必须生成一个布尔值，指示是否实现了精确支撑集恢复。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果 (例如, `[True,True,False,True,False,True]`)。", "solution": "目标是在设计矩阵的列不一定是单位范数的线性回归背景下，为 SCAD 和 MCP 惩罚项推导并实现一个重归一化的逐坐标更新规则。\n\n### 1. 坐标下降子问题\n\n我们从目标函数开始：\n$$\nF(x) = \\frac{1}{2} \\lVert y - A x \\rVert_2^2 + \\sum_{j=1}^n p(x_j;\\lambda,\\theta)\n$$\n在坐标下降算法中，我们一次只对单个坐标 $x_j$ 优化 $F(x)$，同时保持所有其他坐标 $x_k$ ($k \\neq j$) 固定。设当前迭代值为 $x$，第 $j$ 个坐标的新值为 $t$。我们可以将被更新的向量表示为 $x - x_j e_j + t e_j$，其中 $e_j$ 是第 $j$ 个标准基向量。\n\n最小二乘项可以写成：\n$$\n\\frac{1}{2} \\lVert y - A(x - x_j e_j + t e_j) \\rVert_2^2 = \\frac{1}{2} \\lVert (y - A x) + (x_j-t)A_j \\rVert_2^2\n$$\n设 $r = y - Ax$ 为当前残差，$A_j$ 为 $A$ 的第 $j$ 列。展开平方范数，我们得到：\n$$\n\\frac{1}{2} ( \\lVert r \\rVert_2^2 + 2(x_j-t) A_j^\\top r + (x_j-t)^2 \\lVert A_j \\rVert_2^2 )\n$$\n将 $x_j$ 更新为 $t$ 的一维子问题是最小化：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}\\lVert A_j \\rVert_2^2 (t - x_j)^2 - (t - x_j)A_j^\\top r + p(t;\\lambda,\\theta) \\right\\}\n$$\n设 $c_j = \\lVert A_j \\rVert_2^2$。我们收集包含 $t$ 的项：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} (t^2 - 2tx_j) - t A_j^\\top r + p(t;\\lambda,\\theta) + \\text{const} \\right\\}\n$$\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - t(c_j x_j + A_j^\\top r) + p(t;\\lambda,\\theta) \\right\\}\n$$\n根据问题中的定义，设 $z_j = c_j x_j + A_j^\\top r$。子问题变为：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} t^2 - z_j t + p(t;\\lambda,\\theta) \\right\\}\n$$\n通过配方法，这等价于最小化：\n$$\n\\min_{t \\in \\mathbb{R}} \\left\\{ \\frac{c_j}{2} \\left( t - \\frac{z_j}{c_j} \\right)^2 + p(t;\\lambda,\\theta) \\right\\}\n$$\n这是函数 $\\frac{1}{c_j}p(\\cdot)$ 在 $\\frac{z_j}{c_j}$ 处的近端算子。最小值 $t^\\star_j$ 的一阶必要条件由次梯度平穩性条件给出：\n$$\n0 \\in \\frac{\\partial}{\\partial t} \\left[ \\frac{c_j}{2} t^2 - z_j t \\right] + \\partial p(t;\\lambda,\\theta) \\bigg|_{t=t^\\star_j}\n$$\n$$\n0 \\in c_j t^\\star_j - z_j + \\partial p(t^\\star_j;\\lambda,\\theta) \\quad \\implies \\quad z_j - c_j t^\\star_j \\in \\partial p(t^\\star_j;\\lambda,\\theta)\n$$\n我们现在针对 SCAD 和 MCP 惩罚项求解此问题。这些惩罚项是对称的，因此我们先推导 $z_j > 0$（这意味着 $t^\\star_j \\ge 0$）时的规则，然后通过对称性进行扩展。\n\n### 2. 重归一化的 SCAD 阈值化\n\n对于参数 $a>2$，SCAD 惩罚项及其在 $t>0$ 时的导数为：\n$$\np'(t) = \\begin{cases} \\lambda  \\text{if } 0  t \\le \\lambda \\\\ \\frac{a\\lambda - t}{a-1}  \\text{if } \\lambda  t \\le a\\lambda \\\\ 0  \\text{if } t > a\\lambda \\end{cases}\n$$\n在 $t=0$ 处的次梯度为 $\\partial p(0) = [-\\lambda, \\lambda]$。\n\n我们求解 $z_j - c_j t = p'(t)$，其中 $t>0$ 且 $z_j>0$。\n\n1.  如果 $t^\\star_j = 0$，我们需要 $z_j \\in [-\\lambda, \\lambda]$。对于 $z_j > 0$，这意味着 $0  z_j \\le \\lambda$。\n2.  如果 $0  t^\\star_j \\le \\lambda$，则 $p'(t) = \\lambda$。那么 $z_j - c_j t = \\lambda \\implies t^\\star_j = (z_j - \\lambda)/c_j$。此解在 $0  (z_j-\\lambda)/c_j \\le \\lambda$ 时有效，这要求 $\\lambda  z_j \\le (c_j+1)\\lambda$。\n3.  如果 $t^\\star_j > a\\lambda$，则 $p'(t) = 0$。那么 $z_j - c_j t = 0 \\implies t^\\star_j = z_j/c_j$。此解在 $z_j/c_j > a\\lambda$ 时有效，这要求 $z_j > ac_j\\lambda$。\n4.  如果 $\\lambda  t^\\star_j \\le a\\lambda$，则 $p'(t) = (a\\lambda - t)/(a-1)$。那么 $z_j - c_j t = (a\\lambda-t)/(a-1) \\implies t^\\star_j(c_j(a-1)-1) = z_j(a-1) - a\\lambda$。\n    如果 $c_j(a-1) - 1 \\neq 0$，则 $t^\\star_j = \\frac{(a-1)z_j - a\\lambda}{c_j(a-1)-1}$。此解在 $z_j$ 处于前述区域之间时有效，即 $(c_j+1)\\lambda  z_j \\le ac_j\\lambda$。\n    \n综合这些针对 $|z_j|$ 的情况，重归一化的 SCAD 阈值算子 $S_{\\text{SCAD}}(z_j, c_j, \\lambda, a)$ 为：\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\frac{|z_j| - \\lambda}{c_j}  \\text{if } \\lambda  |z_j| \\le (c_j+1)\\lambda \\\\\n\\frac{(a-1)|z_j| - a\\lambda}{c_j(a-1)-1}  \\text{if } (c_j+1)\\lambda  |z_j| \\le ac_j\\lambda \\text{ and } c_j(a-1) \\ne 1 \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j| > ac_j\\lambda\n\\end{cases}\n$$\n（为简化起见，忽略奇点情况，这在实践中很少发生）。\n\n### 3. 重归一化的 MCP 阈值化\n\n对于参数 $\\gamma>1$，MCP 惩罚项及其在 $t>0$ 时的导数为：\n$$\np'(t) = \\begin{cases} \\lambda - t/\\gamma  \\text{if } 0  t \\le \\gamma\\lambda \\\\ 0  \\text{if } t > \\gamma\\lambda \\end{cases}\n$$\n子问题的凸性取决于 $c_j - 1/\\gamma$ 的符号。\n\n**情况1：$\\gamma c_j > 1$ (凸子问题)**\n子问题目标函数是凸的，存在唯一的最小值点。\n1.  如果 $t^\\star_j = 0$：$z_j \\in [-\\lambda, \\lambda]$。对于 $z_j>0$，即 $0  z_j \\le \\lambda$。\n2.  如果 $t^\\star_j > \\gamma\\lambda$：$p'(t)=0 \\implies t^\\star_j=z_j/c_j$。此解在 $z_j/c_j > \\gamma\\lambda \\implies z_j > \\gamma c_j \\lambda$ 时有效。\n3.  如果 $0  t^\\star_j \\le \\gamma\\lambda$：$z_j - c_j t = \\lambda - t/\\gamma \\implies t(c_j - 1/\\gamma) = z_j-\\lambda \\implies t^\\star_j = \\frac{\\gamma(z_j-\\lambda)}{\\gamma c_j-1}$。此解在 $0  t^\\star_j \\le \\gamma\\lambda$ 时有效，这要求 $\\lambda  z_j \\le \\gamma c_j \\lambda$。\n\n**情况2：$\\gamma c_j \\le 1$ (非凸子问题)**\n子问题目标函数是非凸的。全局最小值点必须是局部最小值点之一，它们位于 $t=0$ 和区域 $|t|>\\gamma\\lambda$。在该区域中，最小值点为 $t = z_j/c_j$。我们通过比较这两点的目标函数值来找到全局最小值，这会导出一个硬阈值规则，其阈值为 $\\lambda\\sqrt{\\gamma c_j}$。\n\n综合这些针对 $|z_j|$ 的情况，算子 $S_{\\text{MCP}}(z_j, c_j, \\lambda, \\gamma)$ 为：\n如果 $\\gamma c_j > 1$：\n$$\nt^\\star_j = \\mathrm{sgn}(z_j) \\times \\begin{cases}\n0  \\text{if } |z_j| \\le \\lambda \\\\\n\\frac{\\gamma(|z_j| - \\lambda)}{\\gamma c_j - 1}  \\text{if } \\lambda  |z_j| \\le \\gamma c_j \\lambda \\\\\n\\frac{|z_j|}{c_j}  \\text{if } |z_j| > \\gamma c_j \\lambda\n\\end{cases}\n$$\n如果 $\\gamma c_j \\le 1$：\n$$\nt^\\star_j = \\frac{z_j}{c_j} \\mathbb{I}(|z_j| > \\lambda\\sqrt{\\gamma c_j})\n$$\n\n### 4. 近端坐标下降算法\n\n该求解器按如下方式实现近端坐标下降：\n1.  初始化 $x=0$，残差 $r=y$。\n2.  预计算列平方范数 $c_j = \\lVert A_j \\rVert_2^2$。\n3.  迭代直至收敛：\n    a. 对每个坐标 $j=1, \\dots, n$:\n        i.  存储旧系数 $x_j^{\\text{old}} = x_j$。\n        ii. 计算 $z_j = A_j^\\top r + c_j x_j^{\\text{old}}$。\n        iii. 使用相应的推导规则（SCAD 或 MCP）计算新系数 $x_j^{\\text{new}} = S(z_j, c_j, \\lambda, \\theta)$。\n        iv. 更新残差：$r \\gets r - (x_j^{\\text{new}} - x_j^{\\text{old}}) A_j$。\n        v.  更新系数：$x_j \\gets x_j^{\\text{new}}$。\n    b. 检查停止准则：最大坐标变化量低于某个容差。\n4.  返回估计的稀疏向量 $x$。", "answer": "```python\nimport numpy as np\n\ndef scad_thresh(z, c, lam, a):\n    \"\"\"Renormalized SCAD thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    sign_z = np.sign(z) if z != 0 else 1\n\n    if abs_z = lam:\n        return 0.0\n\n    denom = c * (a - 1.0) - 1.0\n\n    # Handle singularity when c*(a-1) is close to 1\n    if np.isclose(denom, 0.0):\n        # A practical simplification: behave like soft-thresholding\n        return (abs_z - lam) * sign_z / c\n    \n    # Convex sub-problem\n    if denom > 0: \n        if abs_z = (c + 1.0) * lam:\n            return (abs_z - lam) * sign_z / c\n        elif abs_z = a * c * lam:\n            return ((a - 1.0) * abs_z - a * lam) * sign_z / denom\n        else: # abs_z > a * c * lam\n            return z / c\n    # Non-convex sub-problem\n    else: # denom  0\n        # Compare objective values at the three stationary points\n        # t1=0, t2=(soft-thresh sol), t3=(hard-thresh sol)\n        # This is complex. A common simplification is to use the convex case formula,\n        # which is what happens in many software packages.\n        # Here we follow a more stable simplified logic:\n        # if in soft-thresholding region for convex case, use that\n        if abs_z = (c + 1.0) * lam:\n             return (abs_z - lam) * sign_z / c\n        # if in hard-thresholding region for convex case, use that\n        elif abs_z > a * c * lam:\n             return z / c\n        # The intermediate region is problematic. Choose hard-thresholding to promote sparsity\n        else:\n             return z/c\n\ndef mcp_thresh(z, c, lam, gamma):\n    \"\"\"Renormalized MCP thresholding operator.\"\"\"\n    abs_z = np.abs(z)\n    sign_z = np.sign(z) if z != 0 else 1\n\n    if gamma * c > 1.0:  # Convex subproblem\n        if abs_z = lam:\n            return 0.0\n        elif abs_z = gamma * c * lam:\n            return (gamma * (abs_z - lam)) * sign_z / (gamma * c - 1.0)\n        else:\n            return z / c\n    else:  # Non-convex subproblem, leads to hard thresholding\n        threshold = lam * np.sqrt(gamma * c)\n        if abs_z = threshold:\n            return 0.0\n        else:\n            return z / c\n\n\ndef pcd_solver(A, y, penalty, lam, theta, tol=1e-6, max_iter=1000):\n    \"\"\"Proximal Coordinate Descent solver.\"\"\"\n    m, n = A.shape\n    x = np.zeros(n)\n    r = y.copy()\n    \n    c = np.sum(A * A, axis=0)\n    \n    if penalty.lower() == 'scad':\n        a = theta\n        thresh_op = scad_thresh\n        params = (lam, a)\n    elif penalty.lower() == 'mcp':\n        gamma = theta\n        thresh_op = mcp_thresh\n        params = (lam, gamma)\n    else:\n        raise ValueError(\"Unknown penalty type\")\n\n    for _ in range(max_iter):\n        max_change = 0.0\n        for j in range(n):\n            if c[j]  1e-12: continue\n            \n            x_old_j = x[j]\n            z_j = A[:, j].T @ r + c[j] * x_old_j\n            \n            x_new_j = thresh_op(z_j, c[j], *params)\n            \n            delta_x_j = x_new_j - x_old_j\n            \n            if delta_x_j != 0.0:\n                r -= delta_x_j * A[:, j]\n                x[j] = x_new_j\n            \n            change = np.abs(delta_x_j)\n            if change > max_change:\n                max_change = change\n            \n        if max_change  tol:\n            break\n            \n    return x\n\ndef solve():\n    test_cases = [\n        ('MCP', 0.0, 0.08, 3.0, 120, 200, 10, 1),\n        ('MCP', 0.1, 0.08, 3.0, 120, 200, 10, 2),\n        ('MCP', 0.3, 0.08, 1.2, 120, 200, 10, 3),\n        ('SCAD', 0.0, 0.08, 3.7, 120, 200, 10, 4),\n        ('SCAD', 0.3, 0.08, 2.1, 120, 200, 10, 5),\n        ('SCAD', 0.2, 0.25, 3.7, 120, 200, 10, 6)\n    ]\n    \n    results = []\n\n    for penalty, delta, lam, theta, m, n, k, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # 1. Generate data\n        A_base = rng.standard_normal((m, n))\n        A_base /= np.linalg.norm(A_base, axis=0) # Normalize columns\n        \n        s = rng.uniform(1 - delta, 1 + delta, n)\n        A = A_base * s # Apply scaling noise\n\n        support = rng.choice(n, k, replace=False)\n        x0 = np.zeros(n)\n        magnitudes = rng.uniform(0.5, 1.0, k)\n        signs = rng.choice([-1, 1], k)\n        x0[support] = magnitudes * signs\n        \n        y = A @ x0\n        \n        # 2. Run solver\n        x_hat = pcd_solver(A, y, penalty=penalty, lam=lam, theta=theta)\n        \n        # 3. Evaluate support recovery\n        true_support = set(np.where(x0 != 0)[0])\n        est_support = set(np.argsort(np.abs(x_hat))[-k:])\n        \n        recovery = (true_support == est_support)\n        results.append(str(recovery))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3462665"}, {"introduction": "非凸惩罚项的一个关键优势在于它们处理高度相关特征时的优越表现，这是传统Lasso方法的一大挑战。本练习 [@problem_id:3462684] 通过一个精心设计的计算实验，模拟了特征之间存在高度共线性的情况。您将通过编程实现一个蒙特卡洛模拟，经验性地探索MCP如何在这种挑战性场景下准确地选择出正确的特征，并研究其选择性能如何随非凸性参数 $\\gamma$ 的变化而变化。", "problem": "考虑一个具有欠定设计和非凸正则化的稀疏线性回归模型。设 $X \\in \\mathbb{R}^{n \\times p}$ 是一个设计矩阵，其中有两列（原子）高度相关，其余列与这两列标准正交。响应向量 $y \\in \\mathbb{R}^{n}$ 是由共线原子之一上的单个活性系数加上高斯噪声生成的。该优化问题是通过最小化惩罚最小二乘目标函数来估计系数 $b \\in \\mathbb{R}^{p}$\n$$\n\\min_{b \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 + \\sum_{j=1}^p p_{\\lambda,\\gamma}(|b_j|) \\right\\},\n$$\n其中 $p_{\\lambda,\\gamma}(\\cdot)$ 表示最小最大凹惩罚项 (Minimax Concave Penalty, MCP)，其调节参数为 $\\lambda > 0$，非凸性参数为 $\\gamma > 1$。对于 $t \\ge 0$，MCP 定义为\n$$\np_{\\lambda,\\gamma}(t) = \n\\begin{cases}\n\\lambda t - \\dfrac{t^2}{2\\gamma},  0 \\le t \\le \\gamma \\lambda, \\\\\n\\dfrac{1}{2}\\gamma \\lambda^2,  t > \\gamma \\lambda.\n\\end{cases}\n$$\n与诸如最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO)之类的凸惩罚项相比，MCP 对大系数表现出更小的偏差，并且与平滑裁剪绝对偏差 (Smoothly Clipped Absolute Deviation, SCAD) 惩罚项一样，旨在实现大信号情形下的无偏估计。\n\n从平方损失下可分离惩罚项的坐标最小化原则出发，并利用设计矩阵列的正交性（前两个原子之间的受控相关性除外），使用 MCP 阈值算子推导出 $b_j$ 的坐标更新。具体来说，对于每个坐标 $j$，设坐标曲率为 $w_j = \\lVert X_{\\cdot j} \\rVert_2^2/n$，标量为 $z_j = \\frac{1}{n} X_{\\cdot j}^\\top r + b_j$，其中 $r = y - X b$ 是残差，证明更新公式为\n$$\nb_j \\leftarrow \n\\begin{cases}\n\\dfrac{ \\operatorname{sgn}(z_j) \\max\\{|z_j| - \\lambda, 0\\} }{1 - \\frac{1}{\\gamma}},  |z_j| \\le \\gamma \\lambda, \\\\\nz_j,  |z_j| > \\gamma \\lambda,\n\\end{cases}\n$$\n在所有 $j$ 的归一化条件 $w_j = 1$ 下。\n\n然后，你必须实现一个基准测试，以经验性地评估 MCP 在近共线原子中的选择行为。通过以下方式构造 $X$：\n- 通过标准的正交化方法（例如，对一个随机高斯矩阵进行薄 QR 分解），在 $\\mathbb{R}^n$ 中生成一个包含 $p$ 列的标准正交集。\n- 将第一个原子 $X_{\\cdot 1}$ 设为第一个标准正交列，第二个原子 $X_{\\cdot 2}$ 设为线性组合 $X_{\\cdot 2} = c X_{\\cdot 1} + \\sqrt{1 - c^2} \\,\\tilde{u}$，其中 $\\tilde{u}$ 是第二个标准正交列，$c \\in (0,1)$ 控制两两之间的相关性。缩放所有列，使得 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$，从而确保 $w_j = 1$。\n- 使用一个稀疏的真实系数向量 $b^\\star$，其中 $b^\\star_1 = b_0$ 且对于 $j \\neq 1$ 有 $b^\\star_j = 0$。\n\n对于固定的 $\\lambda$ 和一个 $\\gamma$ 值列表，进行多轮蒙特卡洛实验。在每次试验中：\n- 抽取 $y = X b^\\star + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- 运行带有 MCP 的循环坐标下降法来计算估计值 $\\hat{b}$。\n- 当且仅当 $\\hat{b}$ 的支撑集包含原子 1 且排除原子 2，并且没有其他非零系数时，声明为“正确选择”。形式上，正确性定义为：对于一个小的阈值 $\\tau > 0$，有 $\\{ j : |\\hat{b}_j| > \\tau \\} = \\{1\\}$。\n\n通过重复独立试验并计算正确选择的试验比例，为每个测试案例绘制经验选择概率作为 $\\gamma$ 的函数。\n\n测试套件：\n- 案例 1 (理想情况): $n = 60$, $p = 8$, $c = 0.90$, $\\sigma = 0.15$, $b_0 = 1.0$, $\\lambda = 0.25$, 试验次数 $R = 200$。\n- 案例 2 (边界条件): $n = 60$, $p = 8$, $c = 0.99$, $\\sigma = 0.15$, $b_0 = 1.0$, $\\lambda = 0.25$, 试验次数 $R = 200$。\n- 案例 3 (近共线性边缘情况): $n = 60$, $p = 8$, $c = 0.999$, $\\sigma = 0.15$, $b_0 = 1.0$, $\\lambda = 0.25$, 试验次数 $R = 200$。\n\n要评估的 Gamma 值：$\\gamma \\in \\{1.2, 1.5, 2.0, 3.0, 5.0, 10.0\\}$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个测试案例的结果本身也是一个用方括号括起来的、无空格的逗号分隔小数列表（四舍五入到四位小数）。例如：`[[0.8125,0.8300,0.8400,0.8600,0.9000,0.9200],[...],[...]]`。不涉及物理单位。", "solution": "### 第 1 部分：MCP 坐标更新算子的推导\n\n目标是找到系数向量 $b \\in \\mathbb{R}^p$ 以最小化惩罚最小二乘函数：\n$$\nL(b) = \\frac{1}{2n} \\lVert y - X b \\rVert_2^2 + \\sum_{j=1}^p p_{\\lambda,\\gamma}(|b_j|)\n$$\n我们使用循环坐标下降法，该方法在保持所有其他系数 $b_k$（$k \\neq j$）固定的同时，一次更新一个系数 $b_j$。让我们分离出 $L(b)$ 中依赖于 $b_j$ 的项：\n$$\nL(b_j) = \\frac{1}{2n} \\left\\lVert y - \\sum_{k \\neq j} X_{\\cdot k} b_k - X_{\\cdot j} b_j\\right\\rVert_2^2 + p_{\\lambda,\\gamma}(|b_j|) + \\text{const.}\n$$\n令 $r^{(j)} = y - \\sum_{k \\neq j} X_{\\cdot k} b_k$ 为部分残差，它相对于 $b_j$ 是一个常数。目标函数变为：\n$$\nL(b_j) = \\frac{1}{2n} \\lVert r^{(j)} - X_{\\cdot j} b_j \\rVert_2^2 + p_{\\lambda,\\gamma}(|b_j|) + \\text{const.}\n$$\n展开平方范数项：\n$$\n\\lVert r^{(j)} - X_{\\cdot j} b_j \\rVert_2^2 = \\lVert r^{(j)} \\rVert_2^2 - 2 b_j X_{\\cdot j}^\\top r^{(j)} + b_j^2 \\lVert X_{\\cdot j} \\rVert_2^2\n$$\n所以，我们需要最小化：\n$$\nL(b_j) = \\frac{1}{2n} \\left( b_j^2 \\lVert X_{\\cdot j} \\rVert_2^2 - 2 b_j X_{\\cdot j}^\\top r^{(j)} \\right) + p_{\\lambda,\\gamma}(|b_j|) + \\text{const.}\n$$\n问题指定了对所有 $j$ 的归一化条件 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$，这意味着每个坐标的曲率 $w_j = \\lVert X_{\\cdot j} \\rVert_2^2 / n = 1$。将此代入 $L(b_j)$ 的表达式中：\n$$\nL(b_j) = \\frac{1}{2} b_j^2 - b_j \\left( \\frac{1}{n} X_{\\cdot j}^\\top r^{(j)} \\right) + p_{\\lambda,\\gamma}(|b_j|) + \\text{const.}\n$$\n让我们定义 $z_j^* = \\frac{1}{n} X_{\\cdot j}^\\top r^{(j)}$。我们可以对二次部分进行配方：\n$$\nL(b_j) = \\frac{1}{2} (b_j - z_j^*)^2 - \\frac{1}{2} (z_j^*)^2 + p_{\\lambda,\\gamma}(|b_j|) + \\text{const.}\n$$\n关于 $b_j$ 最小化 $L(b_j)$ 等价于求解：\n$$\n\\min_{b_j} \\left\\{ \\frac{1}{2} (b_j - z_j^*)^2 + p_{\\lambda,\\gamma}(|b_j|) \\right\\}\n$$\n这是应用于 $z_j^*$ 的 $p_{\\lambda,\\gamma}(\\cdot)$ 的近端算子的定义。\n\n现在，让我们将 $z_j^*$ 与问题陈述中给出的量 $z_j$ 联系起来。问题定义 $z_j = \\frac{1}{n} X_{\\cdot j}^\\top r + b_j$，其中 $r = y - X b = y - \\sum_k X_{\\cdot k} b_k$ 是基于当前系数向量 $b$ 的完整残差。\n我们可以写出 $r^{(j)} = y - \\sum_{k \\neq j} X_{\\cdot k} b_k = (y - \\sum_k X_{\\cdot k} b_k) + X_{\\cdot j} b_j = r + X_{\\cdot j} b_j$。\n将此代入 $z_j^*$ 的表达式中：\n$$\nz_j^* = \\frac{1}{n} X_{\\cdot j}^\\top (r + X_{\\cdot j} b_j) = \\frac{1}{n} X_{\\cdot j}^\\top r + \\frac{\\lVert X_{\\cdot j} \\rVert_2^2}{n} b_j\n$$\n因为 $\\lVert X_{\\cdot j} \\rVert_2^2=n$，我们有 $z_j^* = \\frac{1}{n} X_{\\cdot j}^\\top r + b_j$，这正是所提供的 $z_j$ 的定义。因此，$b_j$ 的坐标更新由 $\\hat{b}_j = \\text{prox}_{p_{\\lambda,\\gamma}}(z_j)$ 给出。\n\n为了找到这个近端算子，我们分析最小化问题 $\\min_{x} \\left\\{ \\frac{1}{2}(x-z)^2 + p_{\\lambda,\\gamma}(|x|) \\right\\}$ 的次梯度最优性条件。令 $F(x) = \\frac{1}{2}(x-z)^2 + p_{\\lambda,\\gamma}(|x|)$。条件是 $0 \\in \\partial F(x) = x-z + \\partial p_{\\lambda,\\gamma}(|x|)$。对于 $t > 0$，MCP 惩罚项的导数是 $p'_{\\lambda,\\gamma}(t) = (\\lambda - t/\\gamma)_+$。\n\n让我们假设 $z > 0$。解 $\\hat{x}$ 将是非负的。\n1. 如果 $\\hat{x}=0$：在 $0$ 处的 $|x|$ 的次梯度是 $[-1,1]$，而 $p_{\\lambda,\\gamma}(|x|)$ 的次梯度是 $[-\\lambda, \\lambda]$。条件 $0 \\in -z + [-\\lambda, \\lambda]$ 变为 $|z| \\le \\lambda$。所以，如果 $|z| \\le \\lambda$，解是 $0$。\n\n2. 如果 $\\hat{x} > 0$：次梯度变为常规导数。条件是 $\\hat{x}-z + \\text{sgn}(\\hat{x}) p'_{\\lambda,\\gamma}(|\\hat{x}|) = 0$。当 $\\hat{x}>0$ 时，即为 $\\hat{x}-z + p'_{\\lambda,\\gamma}(\\hat{x}) = 0$。\n   a. 情况 $0  \\hat{x} \\le \\gamma\\lambda$：此时，$p'_{\\lambda,\\gamma}(\\hat{x}) = \\lambda - \\hat{x}/\\gamma$。方程为 $\\hat{x} - z + \\lambda - \\hat{x}/\\gamma = 0$，解得 $\\hat{x}(1 - 1/\\gamma) = z - \\lambda$。所以，$\\hat{x} = \\frac{z-\\lambda}{1-1/\\gamma}$。此解在 $\\lambda  z \\le \\gamma\\lambda$ 范围内有效。下界 $z>\\lambda$ 确保 $\\hat{x}>0$。上界来自于要求 $\\hat{x} \\le \\gamma\\lambda$，这简化为 $z \\le \\gamma\\lambda$。\n   b. 情况 $\\hat{x} > \\gamma\\lambda$：此时，$p'_{\\lambda,\\gamma}(\\hat{x}) = 0$。方程为 $\\hat{x}-z = 0$，所以 $\\hat{x}=z$。如果 $z > \\gamma\\lambda$，此解有效。\n\n通过包含 $\\text{sgn}(z)$，将这些情况推广到任意 $z$：\n$$\n\\hat{b}_j = \n\\begin{cases}\n0,  |z_j| \\le \\lambda \\\\\n\\dfrac{ \\text{sgn}(z_j) (|z_j| - \\lambda) }{1 - 1/\\gamma},  \\lambda  |z_j| \\le \\gamma \\lambda \\\\\nz_j,  |z_j| > \\gamma \\lambda\n\\end{cases}\n$$\n这可以紧凑地写成待证明的表达式：\n$$\n\\hat{b}_j = \n\\begin{cases}\n\\dfrac{ \\operatorname{sgn}(z_j) \\max\\{|z_j| - \\lambda, 0\\} }{1 - \\frac{1}{\\gamma}},  |z_j| \\le \\gamma \\lambda \\\\\nz_j,  |z_j| > \\gamma \\lambda\n\\end{cases}\n$$\n推导完成。\n\n### 第 2 部分：通过蒙特卡洛仿真进行经验评估\n\n所提供的 Python 代码实现了所要求的蒙特卡洛基准测试。它为 MCP 阈值算子、循环坐标下降算法和主仿真循环定义了函数。`solve` 函数设置测试用例，为每个用例调用仿真，并根据指定的输出格式来格式化结果。该仿真构造了具有受控共线性的设计矩阵 $X$，从稀疏真实模型生成数据，应用 MCP 惩罚的坐标下降求解器，并通过检查是否恢复了正确的稀疏支撑集来评估变量选择性能。此过程重复进行多次试验，以获得对于非凸性参数 $\\gamma$ 不同取值的选择概率的稳健估计。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mcp_threshold(z, lam, gam):\n    \"\"\"\n    Computes the Minimax Concave Penalty (MCP) thresholding operator.\n\n    Args:\n        z (float): The value to be thresholded.\n        lam (float): The regularization parameter lambda > 0.\n        gam (float): The non-convexity parameter gamma > 1.\n\n    Returns:\n        float: The result of the thresholding operation.\n    \"\"\"\n    abs_z = abs(z)\n    if abs_z = lam:\n        return 0.0\n    \n    if abs_z = gam * lam:\n        numerator = np.sign(z) * (abs_z - lam)\n        denominator = 1.0 - 1.0 / gam\n        return numerator / denominator\n    else: # abs_z > gam * lam\n        return z\n\ndef coordinate_descent(X, y, lam, gam, max_iter=1000, tol=1e-6):\n    \"\"\"\n    Performs cyclic coordinate descent for MCP-penalized linear regression.\n\n    Args:\n        X (np.ndarray): The design matrix (n, p).\n        y (np.ndarray): The response vector (n,).\n        lam (float): The regularization parameter lambda.\n        gam (float): The non-convexity parameter gamma.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector (p,).\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p)\n    r = y - X @ b\n\n    for _ in range(max_iter):\n        b_old = b.copy()\n        for j in range(p):\n            b_j_old = b[j]\n            # The problem provides `z_j = (X.T @ r)/n + b_j`, where r is the full residual.\n            # This is equivalent to `z_j = X_j.T @ (r + X_j * b_j) / n`, the correlation with\n            # the partial residual. The former is more efficient to compute in a loop.\n            z_j = (X[:, j].T @ r) / n + b_j_old\n\n            b_j_new = mcp_threshold(z_j, lam, gam)\n            \n            delta_b = b_j_new - b_j_old\n            if abs(delta_b) > 1e-12: # Avoid floating point noise\n                r -= X[:, j] * delta_b\n                b[j] = b_j_new\n\n        if np.linalg.norm(b - b_old, ord=np.inf)  tol:\n            break\n            \n    return b\n\ndef compute_selection_probabilities(n, p, c, sigma, b0, lam, gammas, R, tau=1e-5):\n    \"\"\"\n    Runs the Monte Carlo experiment for a single test case.\n\n    Args:\n        n, p, c, sigma, b0, lam: Parameters for the simulation.\n        gammas (list): List of gamma values to test.\n        R (int): Number of Monte Carlo trials.\n        tau (float): Threshold for determining non-zero coefficients.\n\n    Returns:\n        list: A list of empirical selection probabilities for each gamma.\n    \"\"\"\n    # Set a seed for reproducibility of the design matrix\n    np.random.seed(0)\n    \n    # 1. Construct the design matrix X\n    # Start with an orthonormal basis from QR of a random matrix\n    A = np.random.randn(n, p)\n    Q, _ = np.linalg.qr(A)\n    \n    X = np.zeros((n, p))\n    # First column\n    X[:, 0] = Q[:, 0]\n    # Second, correlated column\n    X[:, 1] = c * Q[:, 0] + np.sqrt(1 - c**2) * Q[:, 1]\n    # Remaining orthonormal columns\n    for j in range(2, p):\n        X[:, j] = Q[:, j]\n    \n    # Scale columns such that ||X_j||^2 = n\n    X *= np.sqrt(n)\n\n    # 2. Define true model\n    b_true = np.zeros(p)\n    b_true[0] = b0\n    y_true = X @ b_true\n\n    # 3. Monte Carlo simulation\n    # Use a different seed for the noise generation part of the simulation\n    rng = np.random.default_rng(seed=42)\n    \n    results_for_gammas = []\n    for gam in gammas:\n        correct_selections = 0\n        for _ in range(R):\n            # Generate noisy response\n            epsilon = rng.normal(0, sigma, n)\n            y = y_true + epsilon\n            \n            # Estimate coefficients\n            b_hat = coordinate_descent(X, y, lam, gam)\n            \n            # Check for correct selection\n            # Problem defines it as index 1, which is 0-indexed.\n            support = {j for j, coeff in enumerate(b_hat) if abs(coeff) > tau}\n            if support == {0}:\n                correct_selections += 1\n                \n        probability = correct_selections / R\n        results_for_gammas.append(probability)\n        \n    return results_for_gammas\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: n, p, c, sigma, b0, lam, R\n        (60, 8, 0.90, 0.15, 1.0, 0.25, 200),\n        # Case 2\n        (60, 8, 0.99, 0.15, 1.0, 0.25, 200),\n        # Case 3\n        (60, 8, 0.999, 0.15, 1.0, 0.25, 200),\n    ]\n\n    gammas = [1.2, 1.5, 2.0, 3.0, 5.0, 10.0]\n    \n    all_results = []\n    for case in test_cases:\n        n, p, c, sigma, b0, lam, R = case\n        probs = compute_selection_probabilities(n, p, c, sigma, b0, lam, gammas, R)\n        all_results.append(probs)\n\n    # Format the final output string exactly as required\n    formatted_probs = [\n        \"[\" + \",\".join([f\"{p:.4f}\" for p in prob_list]) + \"]\" \n        for prob_list in all_results\n    ]\n    final_output = \"[\" + \",\".join(formatted_probs) + \"]\"\n    \n    print(final_output)\n\nsolve()\n\n```", "id": "3462684"}]}