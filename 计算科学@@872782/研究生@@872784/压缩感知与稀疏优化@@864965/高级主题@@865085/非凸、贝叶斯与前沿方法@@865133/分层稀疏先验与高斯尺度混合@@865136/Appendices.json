{"hands_on_practices": [{"introduction": "高斯尺度混合（GSM）的核心思想之一是能够生成多样的非高斯稀疏性先验。本练习提供了一个具体的例子，通过为高斯先验的方差指定一个逆伽马（Inverse-Gamma）混合分布，从而推导出其边缘分布——即著名的学生t分布。通过分析其对应的负对数先验惩罚项的性质，如凹凸性和阈值行为，我们将揭示为何这种先验比标准的$\\ell_1$惩罚更适合对具有大数值的系数进行建模，这对于理解现代稀疏性先验至关重要([@problem_id:3451059])。", "problem": "考虑一个实系数 $x \\in \\mathbb{R}$，它被赋予一个构造为高斯尺度混合 (GSM) 的分层稀疏先验。具体来说，在给定潜方差 $v  0$ 的条件下，令 $x \\,|\\, v \\sim \\mathcal{N}(0, v)$，并令混合密度为逆伽马分布 $v \\sim \\mathrm{InvGamma}(\\alpha, \\beta)$，其形状参数为 $\\alpha = \\nu/2$，尺度参数为 $\\beta = \\nu \\tau^{2}/2$，其中 $\\nu  0$ 是自由度参数，$\\tau  0$ 是一个尺度参数。对于 $r = |x|$，定义负对数先验惩罚项 $\\phi(r)$（至多相差一个加法常数）为 $\\phi(r) := -\\ln p(x)$，其中加法常数的选择使得 $\\phi(0) = 0$。\n\n任务：\n- 仅从高斯密度、逆伽马密度和 GSM 构造的定义出发，推导惩罚项 $\\phi(r)$ 作为 $r$、$\\nu$ 和 $\\tau$ 的函数的显式闭式表达式。\n- 使用关于 $r$ 的一阶和二阶导数，确定惩罚项在 $r \\ge 0$ 的哪些区间上是凸的或凹的，并说明 $\\phi(r)$ 在 $r \\ge 0$ 上是否严格递增。\n- 当 $r \\to \\infty$ 时，比较 $\\phi(r)$ 与 $\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(r) = r$ 的渐近增长。\n- 考虑在最大后验 (MAP) 估计器下的标量高斯去噪：对于给定的观测值 $y \\in \\mathbb{R}$ 和噪声方差 $\\sigma^{2}  0$，定义\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|) \\right\\},\n$$\n其中 $\\lambda  0$ 是一个正则化权重。确定相关的近端算子是否表现出一个精确的阈值 $T  0$，使得对于所有 $|y| \\le T$，都有 $\\hat{x}(y) = 0$。如果存在这样的阈值，请以闭式形式给出 $T$；如果不存在这样的阈值，请说明 $T = 0$ 并证明这一事实。将此与 $\\ell_{1}$ 的情况 $\\phi_{\\ell_{1}}(r) = r$ 进行对比，并确定该情况下的相应阈值。\n\n你最终报告的答案必须是你推导出的 $\\phi(r)$ 的显式表达式（其中加法常数的选择使得 $\\phi(0) = 0$）。不需要数值近似。", "solution": "我们从高斯尺度混合 (GSM) 构造开始。根据定义，$x$ 在给定 $v$ 下的条件先验是高斯分布，\n$$\np(x \\,|\\, v) \\;=\\; \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right),\n$$\n混合密度是逆伽马分布，其形状参数为 $\\alpha = \\nu/2$，尺度参数为 $\\beta = \\nu \\tau^{2}/2$：\n$$\np(v) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right), \\quad v  0.\n$$\n$x$ 的边缘先验通过对 $v$ 积分得到：\n$$\np(x) \\;=\\; \\int_{0}^{\\infty} p(x \\,|\\, v) \\, p(v) \\, \\mathrm{d}v \n\\;=\\; \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right) \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\n收集 $v$ 的幂次项和指数项，得到\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\int_{0}^{\\infty} v^{-(\\alpha + \\tfrac{3}{2})} \\exp\\!\\left(-\\frac{\\tfrac{x^{2}}{2} + \\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\n定义 $C := \\tfrac{x^{2}}{2} + \\beta$。使用对于 $p  0$ 的标准积分恒等式，\n$$\n\\int_{0}^{\\infty} v^{-p-1} \\exp\\!\\left(-\\frac{C}{v}\\right) \\, \\mathrm{d}v \\;=\\; C^{-p} \\, \\Gamma(p),\n$$\n并令 $p = \\alpha + \\tfrac{1}{2}$，我们得到\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\alpha + \\tfrac{1}{2}\\right) \\, C^{-(\\alpha + \\tfrac{1}{2})}.\n$$\n代入 $\\alpha = \\nu/2$ 和 $\\beta = \\nu \\tau^{2}/2$，我们有 $C = \\tfrac{1}{2}(x^{2} + \\nu \\tau^{2})$ 且\n$$\np(x) \\;=\\; \\frac{\\left(\\tfrac{\\nu \\tau^{2}}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right) \\left(\\tfrac{x^{2} + \\nu \\tau^{2}}{2}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\n重新整理常数项，得到具有 $\\nu$ 个自由度和尺度 $\\tau$ 的标准学生t分布密度，\n$$\np(x) \\;=\\; \\frac{\\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right) \\sqrt{\\pi \\nu} \\, \\tau} \\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\n对于 $r = |x|$，定义惩罚项 $\\phi(r)$ 为 $\\phi(r) := -\\ln p(x)$（至多相差一个加法常数，该常数被选择以使 $\\phi(0) = 0$）。使用上述表达式，\n$$\n-\\ln p(x) \\;=\\; \\text{const} + \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right).\n$$\n施加 $\\phi(0) = 0$ 的条件固定了加法常数，得到显式惩罚项\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right).\n$$\n\n我们现在分析它在 $r \\ge 0$ 上的曲率和单调性。对 $r$ 求导：\n$$\n\\phi'(r) \\;=\\; \\frac{\\nu + 1}{2} \\cdot \\frac{2 r}{\\nu \\tau^{2} + r^{2}} \\;=\\; \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}}.\n$$\n对于 $r \\ge 0$，我们有 $\\phi'(r) \\ge 0$，其中 $\\phi'(r) = 0$ 当且仅当 $r = 0$，因此 $\\phi$ 在 $r  0$ 上是严格递增的。二阶导数是\n$$\n\\phi''(r) \\;=\\; (\\nu + 1) \\cdot \\frac{(\\nu \\tau^{2} + r^{2}) - 2 r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}\n\\;=\\; (\\nu + 1) \\cdot \\frac{\\nu \\tau^{2} - r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}.\n$$\n因此，当 $0 \\le r  \\sqrt{\\nu}\\,\\tau$ 时 $\\phi''(r) > 0$，在 $r = \\sqrt{\\nu}\\,\\tau$ 时 $\\phi''(r) = 0$，当 $r > \\sqrt{\\nu}\\,\\tau$ 时 $\\phi''(r)  0$。所以，该惩罚项在原点附近是凸的，在 $r = \\sqrt{\\nu}\\,\\tau$ 处有一个拐点，并且对于足够大的 $r$ 是凹的。\n\n对于渐近增长，当 $r \\to \\infty$ 时，\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(\\frac{r^{2}}{\\nu \\tau^{2}}\\left(1 + o(1)\\right)\\right)\n\\;=\\; (\\nu + 1) \\, \\ln r \\;-\\; \\frac{\\nu + 1}{2} \\, \\ln(\\nu \\tau^{2}) \\;+\\; o(1),\n$$\n其呈对数增长。相比之下，$\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(r) = r$ 是线性增长的。因此，学生t分布惩罚项对大系数的惩罚性远小于 $\\ell_{1}$，这反映了其重尾特性。\n\n接下来我们分析在使用最大后验 (MAP) 估计器进行标量高斯去噪时的阈值化问题，其目标函数为\n$$\nJ(x) \\;=\\; \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|).\n$$\n我们检查是否存在 $T  0$，使得对于所有 $|y| \\le T$ 都有 $\\hat{x}(y) = 0$。因为 $\\phi$ 在 $r = 0$ 处可微，且\n$$\n\\phi'(0^{+}) \\;=\\; \\lim_{r \\downarrow 0} \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}} \\;=\\; 0,\n$$\n所以 $J$ 在 $x=0$ 处的单侧导数为\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\downarrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} + \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}, \n\\quad\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\uparrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} - \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}.\n$$\n要使 $x=0$ 成为一个最小值点，我们需要零属于 $J$ 在 $0$ 处的次微分。由于 $J$ 在 $0$ 处是可微的，这里的次微分简化为单元素集 $\\{-y/\\sigma^{2}\\}$，并且该集合包含 $0$ 当且仅当 $y = 0$。因此，不存在非零的阈值区域：最小值点恰好为零的唯一情况是 $y = 0$。所以，对于学生t分布惩罚项，其阈值为 $T = 0$。这与 $\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(r) = r$ 的情况形成对比，其在 $0$ 处的次梯度是区间 $[-1, 1]$，从而得出众所周知的软阈值条件 $|y|/\\sigma^{2} \\le \\lambda$ 使 $\\hat{x}(y) = 0$，即阈值 $T = \\lambda \\, \\sigma^{2}$。\n\n总结如下：\n- 由学生t分布GSM先验导出的显式惩罚项是 $\\phi(r) = \\tfrac{\\nu + 1}{2} \\ln\\!\\left(1 + \\tfrac{r^{2}}{\\nu \\tau^{2}}\\right)$，且 $\\phi(0) = 0$。\n- $\\phi$ 在 $r \\ge 0$ 上严格递增，在 $0 \\le r  \\sqrt{\\nu}\\,\\tau$ 上是凸的，在 $r = \\sqrt{\\nu}\\,\\tau$ 处有一个拐点，在 $r > \\sqrt{\\nu}\\,\\tau$ 上是凹的。\n- 当 $r \\to \\infty$ 时，$\\phi(r)$ 呈对数增长，慢于 $\\ell_{1}$ 的线性增长。\n- 使用此惩罚项的标量 MAP 去噪器没有非零阈值 ($T = 0$)，这与 $\\ell_{1}$ 的情况不同，后者的阈值为 $T = \\lambda \\sigma^{2}$。", "answer": "$$\\boxed{\\frac{\\nu + 1}{2} \\,\\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right)}$$", "id": "3451059"}, {"introduction": "在掌握了如何从分层结构构建先验之后，下一步是学习如何*使用*这些先验进行贝叶斯推断。对于复杂的分层模型，吉布斯采样是一种强大的工具，它需要我们能够推导出模型中所有参数的完整条件后验分布。本练习将引导您在一个高级的稀疏性模型（马蹄铁先验）中，为局部和全局尺度参数推导这些条件后验分布([@problem_id:3451062])。掌握这一技能是实现高级贝叶斯稀疏回归算法的基础。", "problem": "考虑以下来自压缩感知领域的正态均值模型，该模型带有层次稀疏先验。对于每个索引 $i \\in \\{1,\\dots,n\\}$，观测值 $y_{i}$ 由 $y_{i} = x_{i} + w_{i}$ 生成，其中噪声 $w_{i}$ 是独立同分布的高斯随机变量，均值为 $0$，方差为 $\\sigma^{2}$。噪声方差 $\\sigma^{2}$ 是已知的。信号系数 $x_{i}$ 使用能诱导稀疏性的高斯尺度混合先验进行建模：以局部尺度 $\\lambda_{i}$ 和全局尺度 $\\tau$ 为条件，系数满足 $x_{i} \\mid \\lambda_{i}, \\tau \\sim \\mathcal{N}(0, \\tau^{2} \\lambda_{i}^{2})$，且在 $i$ 上独立。尺度参数服从半柯西先验：局部尺度满足 $\\lambda_{i} \\sim \\mathrm{C}^{+}(0,1)$ 且相互独立，全局尺度满足 $\\tau \\sim \\mathrm{C}^{+}(0,1)$，且与 $\\{\\lambda_{i}\\}_{i=1}^{n}$ 独立。\n\n使用半柯西分布的辅助逆伽马表示来引入潜变量 $\\nu_{i}$ 和 $\\xi$，使得对于每个 $i$，条件表示如下成立：$\\lambda_{i}^{2} \\mid \\nu_{i} \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\tfrac{1}{2}, \\tfrac{1}{\\nu_{i}}\\right)$，其中 $\\nu_{i} \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\tfrac{1}{2}, 1\\right)$；以及 $\\tau^{2} \\mid \\xi \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\tfrac{1}{2}, \\tfrac{1}{\\xi}\\right)$，其中 $\\xi \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\tfrac{1}{2}, 1\\right)$。在整个问题中，对逆伽马分布采用以下参数化：对于 $\\alpha0$ 和 $\\beta0$，一个变量 $z \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}(\\alpha,\\beta)$ 的密度在 $z0$ 上正比于 $z^{-\\alpha-1} \\exp\\!\\big(-\\beta / z\\big)$。\n\n从似然和先验的定义出发，仅使用高斯分布和逆伽马分布的标准性质以及代数运算，推导在给定 $\\{x_{i}\\}_{i=1}^{n}$ 和增广变量 $\\{\\nu_{i}\\}_{i=1}^{n}$ 及 $\\xi$ 的条件下，局部尺度平方 $\\lambda_{i}^{2}$ 和全局尺度平方 $\\tau^{2}$ 的全条件后验分布。您的答案必须以封闭形式确定所得分布，包括它们的形状和尺度参数，并用 $n$、$x_{1:n}$、$\\lambda_{1:n}^{2}$（在适用时）、$\\nu_{1:n}$ 和 $\\xi$ 来表示。请将您的最终结果以包含两种条件分布的单个封闭形式数学表达式给出。不需要也不允许进行数值近似。", "solution": "该问题要求在一个给定的层次贝叶斯模型中，推导局部尺度平方 $\\lambda_{i}^{2}$ 和全局尺度平方 $\\tau^{2}$ 的全条件后验分布。推导将基于所给定的分布和条件概率的原理进行。给定参数的全条件分布正比于所有变量的联合概率分布，并视其为该单个参数的函数，同时保持所有其他变量固定。\n\n该层次模型定义如下：\n1.  信号模型：$x_{i} \\mid \\lambda_{i}, \\tau \\sim \\mathcal{N}(0, \\tau^{2} \\lambda_{i}^{2})$，对于 $i=1, \\dots, n$。\n2.  半柯西先验的辅助变量表示：\n    -   $\\lambda_{i}^{2} \\mid \\nu_{i} \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{1}{2}, \\frac{1}{\\nu_{i}}\\right)$\n    -   $\\nu_{i} \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{1}{2}, 1\\right)$\n    -   $\\tau^{2} \\mid \\xi \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{1}{2}, \\frac{1}{\\xi}\\right)$\n    -   $\\xi \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{1}{2}, 1\\right)$\n\n问题指定了变量 $z \\sim \\mathrm{Inverse}\\text{-}\\mathrm{Gamma}(\\alpha, \\beta)$ 的概率密度函数（PDF）为 $p(z) \\propto z^{-\\alpha-1} \\exp(-\\beta/z)$。\n\n我们被要求求出以信号 $\\{x_{i}\\}_{i=1}^{n}$ 和增广变量为条件的 $\\lambda_{i}^{2}$ 和 $\\tau^{2}$ 的分布。在此背景下（例如，对于吉布斯采样器），“全条件”的标准解释是，我们以所有其他参数和数据为条件。问题明确地以 $\\{x_{i}\\}$、$\\{\\nu_{i}\\}$ 和 $\\xi$ 为条件。因为给定 $\\{x_{i}\\}$ 时，数据 $\\{y_{i}\\}$ 与尺度参数条件独立，所以它们不参与计算。参数 $\\theta$ 的条件分布通过应用贝叶斯定理得到，其中后验分布正比于似然乘以先验：$p(\\theta | \\text{rest}) \\propto p(\\text{data} | \\theta, \\text{...}) \\times p(\\theta | \\text{...})$。\n\n首先，我们推导单个局部尺度平方 $\\lambda_{i}^{2}$ 的全条件分布。我们以所有其他变量为条件，由于模型的条件独立性结构，这简化为以 $x_{i}$、$\\tau^{2}$ 和 $\\nu_{i}$ 为条件。\n$\\lambda_{i}^{2}$ 的条件后验分布由下式给出：\n$$p(\\lambda_{i}^{2} \\mid x_{i}, \\tau^{2}, \\nu_{i}) \\propto p(x_{i} \\mid \\lambda_{i}^{2}, \\tau^{2}) \\times p(\\lambda_{i}^{2} \\mid \\nu_{i})$$\n各项为：\n-   $p(x_{i} \\mid \\lambda_{i}^{2}, \\tau^{2})$ 是在 $x_{i}$ 处求值的 $\\mathcal{N}(0, \\tau^{2}\\lambda_{i}^{2})$ 分布的概率密度函数。作为 $\\lambda_{i}^{2}$ 的函数，它可表示为：\n    $$p(x_{i} \\mid \\lambda_{i}^{2}, \\tau^{2}) = \\frac{1}{\\sqrt{2\\pi\\tau^{2}\\lambda_{i}^{2}}} \\exp\\left(-\\frac{x_{i}^{2}}{2\\tau^{2}\\lambda_{i}^{2}}\\right) \\propto (\\lambda_{i}^{2})^{-1/2} \\exp\\left(-\\frac{x_{i}^{2}}{2\\tau^{2}\\lambda_{i}^{2}}\\right)$$\n-   $p(\\lambda_{i}^{2} \\mid \\nu_{i})$ 是 $\\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{1}{2}, \\frac{1}{\\nu_{i}}\\right)$ 分布的概率密度函数：\n    $$p(\\lambda_{i}^{2} \\mid \\nu_{i}) \\propto (\\lambda_{i}^{2})^{-1/2 - 1} \\exp\\left(-\\frac{1}{\\nu_{i}\\lambda_{i}^{2}}\\right) = (\\lambda_{i}^{2})^{-3/2} \\exp\\left(-\\frac{1}{\\nu_{i}\\lambda_{i}^{2}}\\right)$$\n将这两个表达式相乘，得到 $\\lambda_{i}^{2}$ 的条件后验分布的核：\n$$p(\\lambda_{i}^{2} \\mid x_{i}, \\tau^{2}, \\nu_{i}) \\propto \\left[ (\\lambda_{i}^{2})^{-1/2} \\exp\\left(-\\frac{x_{i}^{2}}{2\\tau^{2}\\lambda_{i}^{2}}\\right) \\right] \\times \\left[ (\\lambda_{i}^{2})^{-3/2} \\exp\\left(-\\frac{1}{\\nu_{i}\\lambda_{i}^{2}}\\right) \\right]$$\n$$p(\\lambda_{i}^{2} \\mid x_{i}, \\tau^{2}, \\nu_{i}) \\propto (\\lambda_{i}^{2})^{-1/2 - 3/2} \\exp\\left(-\\frac{x_{i}^{2}}{2\\tau^{2}\\lambda_{i}^{2}} - \\frac{1}{\\nu_{i}\\lambda_{i}^{2}}\\right)$$\n$$p(\\lambda_{i}^{2} \\mid x_{i}, \\tau^{2}, \\nu_{i}) \\propto (\\lambda_{i}^{2})^{-2} \\exp\\left(-\\left[\\frac{x_{i}^{2}}{2\\tau^{2}} + \\frac{1}{\\nu_{i}}\\right] \\frac{1}{\\lambda_{i}^{2}}\\right)$$\n这是一个逆伽马分布 $z^{-\\alpha-1} \\exp(-\\beta/z)$ 的核。通过比较变量的指数和指数项中的项，我们确定其参数：\n-   形状参数：$-\\alpha - 1 = -2 \\implies \\alpha = 1$。\n-   尺度参数：$\\beta = \\frac{x_{i}^{2}}{2\\tau^{2}} + \\frac{1}{\\nu_{i}}$。\n因此，$\\lambda_{i}^{2}$ 的全条件分布是 $\\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(1, \\frac{x_{i}^{2}}{2\\tau^{2}} + \\frac{1}{\\nu_{i}}\\right)$。\n\n其次，我们推导全局尺度平方 $\\tau^{2}$ 的全条件分布。我们以 $\\{x_{i}\\}_{i=1}^{n}$、$\\{\\lambda_{i}^{2}\\}_{i=1}^{n}$ 和 $\\xi$ 为条件。\n$\\tau^{2}$ 的条件后验分布由下式给出：\n$$p(\\tau^{2} \\mid \\{x_{i}\\}_{i=1}^{n}, \\{\\lambda_{i}^{2}\\}_{i=1}^{n}, \\xi) \\propto \\left(\\prod_{i=1}^{n} p(x_{i} \\mid \\lambda_{i}^{2}, \\tau^{2})\\right) \\times p(\\tau^{2} \\mid \\xi)$$\n每一项 $p(x_{i} \\mid \\lambda_{i}^{2}, \\tau^{2})$ 都依赖于 $\\tau^{2}$。其乘积为：\n$$\\prod_{i=1}^{n} p(x_{i} \\mid \\lambda_{i}^{2}, \\tau^{2}) \\propto \\prod_{i=1}^{n} (\\tau^{2})^{-1/2} \\exp\\left(-\\frac{x_{i}^{2}}{2\\tau^{2}\\lambda_{i}^{2}}\\right)$$\n$$= ((\\tau^{2})^{-1/2})^{n} \\exp\\left(-\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{2\\tau^{2}\\lambda_{i}^{2}}\\right)$$\n$$= (\\tau^{2})^{-n/2} \\exp\\left(-\\frac{1}{2\\tau^{2}}\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{\\lambda_{i}^{2}}\\right)$$\n先验项 $p(\\tau^{2} \\mid \\xi)$ 是 $\\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{1}{2}, \\frac{1}{\\xi}\\right)$ 分布的概率密度函数：\n$$p(\\tau^{2} \\mid \\xi) \\propto (\\tau^{2})^{-1/2 - 1} \\exp\\left(-\\frac{1}{\\xi\\tau^{2}}\\right) = (\\tau^{2})^{-3/2} \\exp\\left(-\\frac{1}{\\xi\\tau^{2}}\\right)$$\n将这些相乘，得到 $\\tau^{2}$ 的条件后验分布的核：\n$$p(\\tau^{2} \\mid \\dots) \\propto \\left[ (\\tau^{2})^{-n/2} \\exp\\left(-\\frac{1}{2\\tau^{2}}\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{\\lambda_{i}^{2}}\\right) \\right] \\times \\left[ (\\tau^{2})^{-3/2} \\exp\\left(-\\frac{1}{\\xi\\tau^{2}}\\right) \\right]$$\n$$p(\\tau^{2} \\mid \\dots) \\propto (\\tau^{2})^{-n/2-3/2} \\exp\\left(-\\frac{1}{2\\tau^{2}}\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{\\lambda_{i}^{2}} - \\frac{1}{\\xi\\tau^{2}}\\right)$$\n$$p(\\tau^{2} \\mid \\dots) \\propto (\\tau^{2})^{-(n+3)/2} \\exp\\left(-\\left[ \\frac{1}{2}\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{\\lambda_{i}^{2}} + \\frac{1}{\\xi} \\right] \\frac{1}{\\tau^{2}}\\right)$$\n再次，我们将这个核与逆伽马形式 $z^{-\\alpha-1} \\exp(-\\beta/z)$ 进行匹配：\n-   形状参数：$-\\alpha - 1 = -\\frac{n+3}{2} \\implies \\alpha = \\frac{n+3}{2} - 1 = \\frac{n+1}{2}$。\n-   尺度参数：$\\beta = \\frac{1}{2}\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{\\lambda_{i}^{2}} + \\frac{1}{\\xi}$。\n因此，$\\tau^{2}$ 的全条件分布是 $\\mathrm{Inverse}\\text{-}\\mathrm{Gamma}\\!\\left(\\frac{n+1}{2}, \\frac{1}{2}\\sum_{i=1}^{n} \\frac{x_{i}^{2}}{\\lambda_{i}^{2}} + \\frac{1}{\\xi}\\right)$。\n\n最终结果由这两个推导出的分布组成。", "answer": "$$\n\\boxed{\n\\begin{cases}\n\\lambda_{i}^{2} \\mid x_{i}, \\tau^{2}, \\nu_{i} \\sim \\mathrm{Inverse-Gamma}\\!\\left(1, \\frac{x_{i}^{2}}{2\\tau^{2}} + \\frac{1}{\\nu_{i}}\\right)  \\text{对于 } i \\in \\{1,\\dots,n\\} \\\\\n\\\\\n\\tau^{2} \\mid \\{x_{k}\\}_{k=1}^{n}, \\{\\lambda_{k}^{2}\\}_{k=1}^{n}, \\xi \\sim \\mathrm{Inverse-Gamma}\\!\\left(\\frac{n+1}{2}, \\frac{1}{2}\\sum_{k=1}^{n} \\frac{x_{k}^{2}}{\\lambda_{k}^{2}} + \\frac{1}{\\xi}\\right)\n\\end{cases}\n}\n$$", "id": "3451062"}, {"introduction": "除了贝叶斯推断，高斯尺度混合（GSM）模型也可以通过期望最大化（EM）的视角，自然地导出高效的优化算法。本练习旨在将概率模型与确定性优化联系起来，要求您为一个包含鲁棒数据保真项和稀疏正则化项的优化问题，推导并实现一个迭代重加权最小二乘（IRLS）算法([@problem_id:3451047])。您将看到，GSM框架中的潜在变量如何优雅地转化为优化算法中的迭代权重，从而将一个复杂的非凸问题转化为一系列简单的加权最小二乘问题。", "problem": "考虑压缩感知（CS）中的鲁棒稀疏恢复任务，其中寻求通过求解一个由鲁棒数据保真项和稀疏性诱导正则化组成的目标函数，从测量值 $\\boldsymbol{y} \\in \\mathbb{R}^m$ 和传感矩阵 $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ 来估计系数向量 $\\boldsymbol{x} \\in \\mathbb{R}^n$。目标是最小化函数\n$$\n\\sum_{j=1}^m \\rho\\!\\left( (\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x})_j \\right) + \\sum_{i=1}^n \\phi\\!\\left( |x_i| \\right),\n$$\n其中残差惩罚项 $\\rho$ 和系数惩罚项 $\\phi$ 都源于高斯尺度混合（GSMs）。要求是实现一个迭代重加权最小二乘（IRLS）算法，该算法利用GSM结构来产生显式的权重更新。\n\n此推导的上下文相关基本基础必须如下：\n- 高斯尺度混合（GSM）表示法，其中以潜在精度为条件的随机变量是高斯分布的，而潜在精度具有伽马先验。具体来说，对于残差和系数，使用分层定义的模型\n$$\nr_j \\mid \\tau_j \\sim \\mathcal{N}(0, \\tau_j^{-1}), \\quad \\tau_j \\sim \\mathrm{Gamma}(a_r, b_r),\n$$\n$$\nx_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\lambda_i^{-1}), \\quad \\lambda_i \\sim \\mathrm{Gamma}(a_x, b_x),\n$$\n其中 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x}$，伽马分布使用形状-率参数化，并且 $(a_r, b_r)$ 和 $(a_x, b_x)$ 是固定的正常数超参数。\n- 从GSM中产生重加权最小二乘的期望最大化或半二次解释。\n\n你必须从上述指定的第一性原理出发，推导出与残差和系数相关的IRLS权重更新，并展示IRLS如何导致求解一系列形如下式的加权最小二乘问题\n$$\n\\min_{\\boldsymbol{x}} \\ \\frac{1}{2} \\sum_{j=1}^m w_j \\, \\big( (\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x})_j \\big)^2 + \\frac{1}{2} \\sum_{i=1}^n v_i \\, x_i^2,\n$$\n其中 $w_j$ 和 $v_i$ 是从GSM结构推导出的函数。你必须提供从GSM推导出的这些权重的显式公式，并实现一个算法，该算法在权重更新和求解相应的正规方程之间交替进行\n$$\n\\left( \\boldsymbol{A}^\\top \\boldsymbol{W} \\boldsymbol{A} + \\boldsymbol{V} \\right) \\boldsymbol{x} = \\boldsymbol{A}^\\top \\boldsymbol{W} \\boldsymbol{y},\n$$\n其中 $\\boldsymbol{W} = \\mathrm{diag}(w_1, \\ldots, w_m)$ 且 $\\boldsymbol{V} = \\mathrm{diag}(v_1, \\ldots, v_n)$。\n\n你实现的程序必须：\n- 对残差使用超参数 $a_r = 2.0$, $b_r = 1.0$，对系数使用 $a_x = 1.0$, $b_x = 10^{-2}$。\n- 将 $\\boldsymbol{x}$ 初始化为零向量，并进行迭代，直到 $\\boldsymbol{x}$ 的相对变化小于 $10^{-6}$ 或达到最大迭代次数 $200$ 次。\n- 为了评估目标值，你必须使用GSM所隐含的边际惩罚项，具体为（在不依赖于 $\\boldsymbol{x}$ 的加性常数范围内）\n$$\n\\rho(r) = (a_r + \\tfrac{1}{2}) \\, \\log\\!\\left( b_r + \\tfrac{1}{2} r^2 \\right),\n\\quad\n\\phi(|x|) = (a_x + \\tfrac{1}{2}) \\, \\log\\!\\left( b_x + \\tfrac{1}{2} x^2 \\right).\n$$\n\n你的程序必须运行如下定义的四个测试用例，每个用例使用指定的随机种子以确保可复现性：\n- 测试用例1（中等噪声和稀疏真实值下的一般恢复）：\n  - 维度：$m = 60$, $n = 40$。\n  - 随机种子：$0$。\n  - 真实稀疏度：$\\boldsymbol{x}$ 中有 $K = 5$ 个非零项。\n  - 测量矩阵 $\\boldsymbol{A}$：其元素独立地从标准正态分布中采样，并且各列被归一化为单位 $\\ell_2$ 范数。\n  - 真实系数 $\\boldsymbol{x}^\\star$：随机均匀选择 $K$ 个索引，其值从 $\\mathcal{N}(0, 1)$ 中采样，其余设置为 $0$。\n  - 噪声标准差：$\\sigma = 0.05$。\n  - 离群值：$0.1$ 比例的残差项被标准差为 $1.0$ 的额外噪声扰动。\n  - 测量向量：$\\boldsymbol{y} = \\boldsymbol{A} \\boldsymbol{x}^\\star + \\boldsymbol{\\epsilon} + \\boldsymbol{o}$，其中 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\boldsymbol{I})$，$\\boldsymbol{o}$ 是上述的离群值向量。\n  - 报告恢复的 $\\boldsymbol{x}$ 与 $\\boldsymbol{x}^\\star$ 之间的均方误差，以浮点数形式表示。\n\n- 测试用例2（零测量和欠定系统的边界情况）：\n  - 维度：$m = 50$, $n = 80$。\n  - 随机种子：$1$。\n  - 真实系数 $\\boldsymbol{x}^\\star$：全为零。\n  - 测量矩阵 $\\boldsymbol{A}$：按上述方式生成并进行列归一化。\n  - 测量向量：$\\boldsymbol{y} = \\boldsymbol{0}$。\n  - 报告恢复的 $\\boldsymbol{x}$ 的 $\\ell_2$ 范数，以浮点数形式表示。\n\n- 测试用例3（具有重复列的近奇异设计）：\n  - 维度：$m = 30$, $n = 30$。\n  - 随机种子：$2$。\n  - 测量矩阵 $\\boldsymbol{A}$：按上述方式生成；归一化后将第 $1$ 列设置为与第 $0$ 列相等，以引入共线性。\n  - 真实系数 $\\boldsymbol{x}^\\star$：在随机位置有 $5$ 个非零值，其值从 $\\mathcal{N}(0, 1)$ 中采样。\n  - 噪声标准差：$\\sigma = 0.02$。\n  - 测量向量：$\\boldsymbol{y} = \\boldsymbol{A} \\boldsymbol{x}^\\star + \\boldsymbol{\\epsilon}$，其中 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(0, \\sigma^2 \\boldsymbol{I})$。\n  - 报告收敛解处的目标值 $\\sum_j \\rho((\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x})_j) + \\sum_i \\phi(|x_i|)$，以浮点数形式表示。\n\n- 测试用例4（测量值中存在严重离群值）：\n  - 维度：$m = 80$, $n = 50$。\n  - 随机种子：$3$。\n  - 真实稀疏度：$K = 8$。\n  - 测量矩阵 $\\boldsymbol{A}$：按上述方式生成并归一化。\n  - 真实系数 $\\boldsymbol{x}^\\star$：$K$ 个非零值从 $\\mathcal{N}(0, 1)$ 中采样。\n  - 噪声标准差：$\\sigma = 0.01$。\n  - 离群值：$0.3$ 比例的残差项被标准差为 $5.0$ 的额外噪声扰动。\n  - 测量向量：$\\boldsymbol{y} = \\boldsymbol{A} \\boldsymbol{x}^\\star + \\boldsymbol{\\epsilon} + \\boldsymbol{o}$，如上定义。\n  - 报告收敛解处的残差惩罚项之和 $\\sum_j \\rho((\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x})_j)$，以浮点数形式表示。\n\n你的程序应该产生一行输出，其中包含四个测试用例的结果，格式为一个逗号分隔的列表，并用方括号括起来，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是对应于测试用例 $k$ 的浮点数结果。", "solution": "用户提供的问题被评估为 **有效**。它在科学上基于贝叶斯推断、统计信号处理和数值优化的原理。该问题是适定的，为推导和实现用于鲁棒稀疏恢复的迭代重加权最小二乘（IRLS）算法指定了所有必要的参数、模型和条件。基于高斯尺度混合（GSM）的底层分层模型是一种用于引入鲁棒非高斯先验的标准且成熟的技术。该问题是自洽的、逻辑一致的，并且没有任何主观或科学上不合理的论断。因此，我们可以进行推导和求解。\n\n目标是最小化函数：\n$$\n\\mathcal{J}(\\boldsymbol{x}) = \\sum_{j=1}^m \\rho\\!\\left( (\\boldsymbol{y} - \\boldsymbol{A} \\boldsymbol{x})_j \\right) + \\sum_{i=1}^n \\phi\\!\\left( |x_i| \\right)\n$$\n其中惩罚项 $\\rho$ 和 $\\phi$ 源自高斯尺度混合（GSM）模型。这个问题可以被置于一个概率框架中，以找到系数向量 $\\boldsymbol{x}$ 的最大后验（MAP）估计。GSM框架提供了一个分层贝叶斯模型，这有助于通过期望最大化（EM）解释来推导IRLS算法。\n\n分层模型规定如下：\n1. 残差 $r_j = (\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{x})_j$ 被建模为条件高斯分布，具有潜在精度变量 $\\tau_j$：\n$$\nr_j \\mid \\tau_j \\sim \\mathcal{N}(0, \\tau_j^{-1})\n$$\n精度 $\\tau_j$ 被赋予一个伽马先验分布：\n$$\n\\tau_j \\sim \\mathrm{Gamma}(a_r, b_r)\n$$\n其中 $a_r  0$ 和 $b_r  0$ 分别是固定的形状和率超参数。\n\n2. 系数 $x_i$ 被建模为条件高斯分布，具有潜在精度变量 $\\lambda_i$：\n$$\nx_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\lambda_i^{-1})\n$$\n精度 $\\lambda_i$ 也被赋予一个伽马先验：\n$$\n\\lambda_i \\sim \\mathrm{Gamma}(a_x, b_x)\n$$\n具有形状 $a_x  0$ 和率 $b_x  0$。\n\nIRLS算法通过将潜在精度 $\\{\\tau_j\\}_{j=1}^m$ 和 $\\{\\lambda_i\\}_{i=1}^n$ 视为缺失数据，并应用EM算法来寻找 $\\boldsymbol{x}$ 的MAP估计来推导。EM算法在期望（E）步骤和最大化（M）步骤之间交替进行。\n\n**E步骤：计算潜在变量的期望**\n在E步骤中，于第 $k$ 次迭代时，我们计算给定当前参数估计 $\\boldsymbol{x}^{(k)}$ 和观测数据 $\\boldsymbol{y}$ 的潜在变量的后验分布。由于条件独立性，我们可以分别考虑每个潜在变量。\n\n对于残差精度 $\\tau_j$，其后验由贝叶斯法则给出：\n$$\np(\\tau_j \\mid r_j^{(k)}) \\propto p(r_j^{(k)} \\mid \\tau_j) p(\\tau_j)\n$$\n其中 $r_j^{(k)} = (\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{x}^{(k)})_j$。似然项来自高斯模型，$p(r_j^{(k)} \\mid \\tau_j) \\propto \\tau_j^{1/2} \\exp(-\\frac{1}{2} \\tau_j (r_j^{(k)})^2)$。先验是伽马分布，$p(\\tau_j) \\propto \\tau_j^{a_r-1} \\exp(-b_r \\tau_j)$。将这些结合起来得到后验核：\n$$\np(\\tau_j \\mid r_j^{(k)}) \\propto \\tau_j^{a_r + 1/2 - 1} \\exp\\left(-\\left(b_r + \\frac{1}{2}(r_j^{(k)})^2\\right)\\tau_j\\right)\n$$\n这是伽马分布的核，具体为 $\\mathrm{Gamma}\\left(a_r + \\frac{1}{2}, b_r + \\frac{1}{2}(r_j^{(k)})^2\\right)$。\n在该后验分布下 $\\tau_j$ 的期望是：\n$$\nw_j^{(k+1)} \\triangleq E[\\tau_j \\mid r_j^{(k)}] = \\frac{a_r + 1/2}{b_r + \\frac{1}{2}(r_j^{(k)})^2} = \\frac{2 a_r + 1}{2 b_r + (r_j^{(k)})^2}\n$$\n\n类似地，对于系数精度 $\\lambda_i$，其给定 $x_i^{(k)}$ 的后验是：\n$$\np(\\lambda_i \\mid x_i^{(k)}) \\propto p(x_i^{(k)} \\mid \\lambda_i) p(\\lambda_i) \\propto \\lambda_i^{a_x + 1/2 - 1} \\exp\\left(-\\left(b_x + \\frac{1}{2}(x_i^{(k)})^2\\right)\\lambda_i\\right)\n$$\n这对应于一个 $\\mathrm{Gamma}\\left(a_x + \\frac{1}{2}, b_x + \\frac{1}{2}(x_i^{(k)})^2\\right)$ 分布。\n$\\lambda_i$ 的期望是：\n$$\nv_i^{(k+1)} \\triangleq E[\\lambda_i \\mid x_i^{(k)}] = \\frac{a_x + 1/2}{b_x + \\frac{1}{2}(x_i^{(k)})^2} = \\frac{2 a_x + 1}{2 b_x + (x_i^{(k)})^2}\n$$\n这些期望值 $w_j^{(k+1)}$ 和 $v_i^{(k+1)}$ 将作为重加权最小二乘问题中的权重。\n\n**M步骤：更新系数向量**\n在M步骤中，我们通过最小化完整数据负对数后验的期望来更新 $\\boldsymbol{x}$ 的估计，该期望以先前的估计 $\\boldsymbol{x}^{(k)}$ 为条件。这等价于最小化辅助函数 $Q(\\boldsymbol{x} | \\boldsymbol{x}^{(k)})$：\n$$\n\\boldsymbol{x}^{(k+1)} = \\arg \\min_{\\boldsymbol{x}} Q(\\boldsymbol{x} \\mid \\boldsymbol{x}^{(k)}) = \\arg \\min_{\\boldsymbol{x}} E_{\\tau, \\lambda \\mid \\boldsymbol{y}, \\boldsymbol{x}^{(k)}}[-\\log p(\\boldsymbol{y}, \\boldsymbol{x}, \\{\\tau_j\\}, \\{\\lambda_i\\})]\n$$\n负对数联合概率，在不考虑常数的情况下，与以下成正比：\n$$\n-\\log p(\\boldsymbol{y}, \\boldsymbol{x}, \\{\\tau_j\\}, \\{\\lambda_i\\}) \\propto \\frac{1}{2}\\sum_{j=1}^m \\tau_j (y_j - (\\boldsymbol{A}\\boldsymbol{x})_j)^2 + \\frac{1}{2}\\sum_{i=1}^n \\lambda_i x_i^2 - \\log p(\\{\\tau_j\\}) - \\log p(\\{\\lambda_i\\})\n$$\n对潜在变量的后验取期望，并只保留依赖于 $\\boldsymbol{x}$ 的项，我们得到M步骤的目标：\n$$\n\\arg \\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\sum_{j=1}^m E[\\tau_j | r_j^{(k)}] (y_j - (\\boldsymbol{A}\\boldsymbol{x})_j)^2 + \\frac{1}{2}\\sum_{i=1}^n E[\\lambda_i | x_i^{(k)}] x_i^2 \\right)\n$$\n代入在E步骤中计算的期望 $w_j^{(k+1)}$ 和 $v_i^{(k+1)}$，我们得到加权最小二乘问题：\n$$\n\\boldsymbol{x}^{(k+1)} = \\arg \\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\sum_{j=1}^m w_j^{(k+1)} (y_j - (\\boldsymbol{A}\\boldsymbol{x})_j)^2 + \\frac{1}{2}\\sum_{i=1}^n v_i^{(k+1)} x_i^2 \\right)\n$$\n这是一个关于 $\\boldsymbol{x}$ 的二次函数。为求最小值，我们将其关于 $\\boldsymbol{x}$ 的梯度设为零：\n$$\n\\nabla_{\\boldsymbol{x}} \\left( \\frac{1}{2} (\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x})^\\top \\boldsymbol{W}^{(k+1)} (\\boldsymbol{y}-\\boldsymbol{A}\\boldsymbol{x}) + \\frac{1}{2} \\boldsymbol{x}^\\top \\boldsymbol{V}^{(k+1)} \\boldsymbol{x} \\right) = \\boldsymbol{0}\n$$\n其中 $\\boldsymbol{W}^{(k+1)} = \\mathrm{diag}(w_j^{(k+1)})$ 且 $\\boldsymbol{V}^{(k+1)} = \\mathrm{diag}(v_i^{(k+1)})$。\n$$\n-\\boldsymbol{A}^\\top \\boldsymbol{W}^{(k+1)} (\\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{x}) + \\boldsymbol{V}^{(k+1)} \\boldsymbol{x} = \\boldsymbol{0}\n$$\n整理各项，得到正规方程组：\n$$\n(\\boldsymbol{A}^\\top \\boldsymbol{W}^{(k+1)} \\boldsymbol{A} + \\boldsymbol{V}^{(k+1)}) \\boldsymbol{x} = \\boldsymbol{A}^\\top \\boldsymbol{W}^{(k+1)} \\boldsymbol{y}\n$$\n解这个关于 $\\boldsymbol{x}$ 的线性系统，得到更新后的估计 $\\boldsymbol{x}^{(k+1)}$。\n\n**IRLS算法总结**\n完整的IRLS算法如下：\n1. 初始化 $\\boldsymbol{x}^{(0)}$（例如，$\\boldsymbol{x}^{(0)} = \\boldsymbol{0}$），设置超参数 $a_r, b_r, a_x, b_x$ 和收敛容差 $\\epsilon$。\n2. 对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n    a. **权重更新（E步骤）**：计算残差并根据当前估计 $\\boldsymbol{x}^{(k)}$ 更新权重。\n    $$\n    \\boldsymbol{r}^{(k)} = \\boldsymbol{y} - \\boldsymbol{A}\\boldsymbol{x}^{(k)}\n    $$\n    $$\n    w_j^{(k+1)} = \\frac{2 a_r + 1}{2 b_r + (r_j^{(k)})^2} \\quad \\text{for } j=1, \\dots, m\n    $$\n    $$\n    v_i^{(k+1)} = \\frac{2 a_x + 1}{2 b_x + (x_i^{(k)})^2} \\quad \\text{for } i=1, \\dots, n\n    $$\n    b. **系数更新（M步骤）**：构建对角矩阵 $\\boldsymbol{W}^{(k+1)}$ 和 $\\boldsymbol{V}^{(k+1)}$，并求解线性系统得到 $\\boldsymbol{x}^{(k+1)}$：\n    $$\n    \\boldsymbol{x}^{(k+1)} = (\\boldsymbol{A}^\\top \\boldsymbol{W}^{(k+1)} \\boldsymbol{A} + \\boldsymbol{V}^{(k+1)})^{-1} \\boldsymbol{A}^\\top \\boldsymbol{W}^{(k+1)} \\boldsymbol{y}\n    $$\n    c. **检查收敛**：如果 $\\frac{\\|\\boldsymbol{x}^{(k+1)} - \\boldsymbol{x}^{(k)}\\|_2}{\\|\\boldsymbol{x}^{(k)}\\|_2 + \\epsilon_{norm}}  \\epsilon_{tol}$ 或达到最大迭代次数，则停止。\n\n所提供的边际惩罚项 $\\rho(r) = (a_r + \\tfrac{1}{2}) \\log(b_r + \\tfrac{1}{2} r^2)$ 和 $\\phi(|x|) = (a_x + \\tfrac{1}{2}) \\log(b_x + \\tfrac{1}{2} x^2)$ 与此分层模型一致，因为它们表示通过对潜在精度变量积分得到的负对数边际似然，在不考虑与 $\\boldsymbol{x}$ 无关的常数的情况下。实现将遵循这个推导出的算法。", "answer": "```python\nimport numpy as np\n\ndef run_irls(A, y, ar, br, ax, bx, n, max_iter=200, tol=1e-6):\n    \"\"\"\n    Solves the robust sparse recovery problem using Iteratively Reweighted Least Squares.\n\n    Args:\n        A (np.ndarray): Sensing matrix of shape (m, n).\n        y (np.ndarray): Measurement vector of shape (m,).\n        ar (float): Shape hyperparameter for residual prior.\n        br (float): Rate hyperparameter for residual prior.\n        ax (float): Shape hyperparameter for coefficient prior.\n        bx (float): Rate hyperparameter for coefficient prior.\n        n (int): Dimension of the coefficient vector x.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance for the relative change in x.\n\n    Returns:\n        np.ndarray: The recovered sparse coefficient vector x.\n    \"\"\"\n    x = np.zeros(n)\n    At = A.T\n    \n    # Pre-calculate constants for weight updates\n    w_num = 2 * ar + 1\n    v_num = 2 * ax + 1\n    w_den_const = 2 * br\n    v_den_const = 2 * bx\n\n    for _ in range(max_iter):\n        x_old = x\n\n        # E-step: Update weights\n        r = y - A @ x\n        w = w_num / (w_den_const + r**2)\n        v = v_num / (v_den_const + x**2)\n        \n        # M-step: Solve weighted least squares\n        # System: (A.T @ W @ A + V) @ x = A.T @ W @ y\n        # W is diag(w), V is diag(v)\n        \n        # Efficiently compute A.T @ W @ A\n        # (w[:, np.newaxis] * A) computes each row of A scaled by a weight\n        AtWA = At @ (w[:, np.newaxis] * A)\n        \n        LHS = AtWA + np.diag(v)\n        RHS = At @ (w * y)\n        \n        try:\n            x = np.linalg.solve(LHS, RHS)\n        except np.linalg.LinAlgError:\n            # If solver fails, use pseudo-inverse as a fallback\n            x = np.linalg.pinv(LHS) @ RHS\n\n        # Check for convergence\n        norm_x_old = np.linalg.norm(x_old)\n        if np.linalg.norm(x - x_old) / (norm_x_old + 1e-9)  tol:\n            break\n            \n    return x\n\ndef calculate_objective(x, y, A, ar, br, ax, bx):\n    \"\"\"Calculates the objective function value.\"\"\"\n    r = y - A @ x\n    rho = (ar + 0.5) * np.log(br + 0.5 * r**2)\n    phi = (ax + 0.5) * np.log(bx + 0.5 * x**2)\n    return np.sum(rho) + np.sum(phi)\n\ndef calculate_residual_penalty(x, y, A, ar, br):\n    \"\"\"Calculates the sum of residual penalties.\"\"\"\n    r = y - A @ x\n    rho = (ar + 0.5) * np.log(br + 0.5 * r**2)\n    return np.sum(rho)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    \n    # Hyperparameters from problem statement\n    ar, br = 2.0, 1.0\n    ax, bx = 1.0, 1e-2\n\n    test_cases_params = [\n        {'case_id': 1, 'm': 60, 'n': 40, 'seed': 0, 'K': 5, 'sigma': 0.05, 'outlier_frac': 0.1, 'outlier_std': 1.0},\n        {'case_id': 2, 'm': 50, 'n': 80, 'seed': 1, 'K': 0, 'sigma': 0.0,  'outlier_frac': 0.0, 'outlier_std': 0.0},\n        {'case_id': 3, 'm': 30, 'n': 30, 'seed': 2, 'K': 5, 'sigma': 0.02, 'outlier_frac': 0.0, 'outlier_std': 0.0},\n        {'case_id': 4, 'm': 80, 'n': 50, 'seed': 3, 'K': 8, 'sigma': 0.01, 'outlier_frac': 0.3, 'outlier_std': 5.0}\n    ]\n\n    results = []\n\n    for params in test_cases_params:\n        m, n, seed = params['m'], params['n'], params['seed']\n        K, sigma = params['K'], params['sigma']\n        outlier_frac, outlier_std = params['outlier_frac'], params['outlier_std']\n        \n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        A = rng.standard_normal(size=(m, n))\n        A /= np.linalg.norm(A, axis=0)\n\n        # Special handling for case 3 (collinear columns)\n        if params['case_id'] == 3:\n            A[:, 1] = A[:, 0]\n\n        x_star = np.zeros(n)\n        if K > 0:\n            support = rng.choice(n, K, replace=False)\n            x_star[support] = rng.standard_normal(size=K)\n        \n        noise = rng.normal(0, sigma, size=m)\n\n        outliers = np.zeros(m)\n        if outlier_frac > 0:\n            num_outliers = int(m * outlier_frac)\n            outlier_indices = rng.choice(m, num_outliers, replace=False)\n            outliers[outlier_indices] = rng.normal(0, outlier_std, size=num_outliers)\n\n        # Special handling for case 2 (zero measurement vector)\n        if params['case_id'] == 2:\n            y = np.zeros(m)\n        else:\n            y = A @ x_star + noise + outliers\n\n        # Run IRLS\n        x_final = run_irls(A, y, ar, br, ax, bx, n)\n\n        # Calculate and store result\n        if params['case_id'] == 1:\n            result = np.mean((x_final - x_star)**2)\n        elif params['case_id'] == 2:\n            result = np.linalg.norm(x_final)\n        elif params['case_id'] == 3:\n            result = calculate_objective(x_final, y, A, ar, br, ax, bx)\n        elif params['case_id'] == 4:\n            result = calculate_residual_penalty(x_final, y, A, ar, br)\n        \n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3451047"}]}