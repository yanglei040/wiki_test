{"hands_on_practices": [{"introduction": "要真正掌握迭代重加权最小二乘（IRLS）算法，第一步是理解其核心迭代机制。这个练习将引导你手动完成单次IRLS迭代，从基于平滑 $\\ell_1$ 惩罚的“主化-最小化”（Majorization-Minimization）框架推导更新规则，到具体的数值计算。通过这个基础实践 [@problem_id:3454747]，你将牢固掌握权重如何形成以及如何求解每个子问题，为实现完整的算法打下坚实基础。", "problem": "考虑稀疏恢复的惩罚最小二乘公式，它通过使用可微代理函数平滑绝对值来近似 $p=1$ 时的 $\\ell_{p}$ 最小化。设目标函数为\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\|A x - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{i=1}^{n} \\phi_{\\varepsilon}(x_{i}),\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$\\lambda0$，并且 $\\phi_{\\varepsilon}(t)$ 是一个光滑、严格凸的函数，当 $\\varepsilon \\to 0^{+}$ 时近似于 $|t|$，具体为 $\\phi_{\\varepsilon}(t) = \\sqrt{t^{2} + \\varepsilon^{2}}$。迭代重加权最小二乘 (IRLS) 方法在第 $k$ 次迭代时，围绕 $x^{(k)}$ 为惩罚项构造一个二次上界（majorizer），并通过最小化该二次代理函数与数据保真项之和来更新 $x^{(k+1)}$。从上述基本定义和凹平方根函数的不等式性质出发，推导 $x^{(k+1)}$ 的加权正规方程，该方程用一个对角权重矩阵 $W^{(k)}$ 表示，其对角线元素依赖于 $x^{(k)}$ 和 $\\varepsilon$，并证明权重对应于 $x^{(k)}$ 處代理函数的曲率。\n\n然后，使用具体数据\n$$\nA \\;=\\; \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}, \\qquad y \\;=\\; \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\qquad p \\;=\\; 1, \\qquad \\lambda \\;=\\; 0.1, \\qquad \\varepsilon \\;=\\; 10^{-3},\n$$\n以及初始化 $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，按如下方式计算一次 IRLS 迭代：\n- 计算 $w^{(0)}$ 以构成 $W^{(0)}$，即迭代 $k=0$ 时的权重矩阵的对角线。\n- 求解线性系统 $(A^{\\top} A + \\lambda W^{(0)})\\, x^{(1)} = A^{\\top} y$ 以得到 $x^{(1)}$。\n- 从 $x^{(1)}$ 计算 $w^{(1)}$。\n\n将最终答案表示为一个单行矩阵，其中按顺序包含 $x^{(1)}$ 的三个分量和 $w^{(1)}$ 的三个分量。请提供精确值，无需四舍五入。最终答案中不要包含任何单位。", "solution": "该问题是良定义的，要求推导针对特定平滑 $\\ell_1$ 惩罚项的迭代重加权最小二乘 (IRLS) 更新，讨论权重的解释，并进行一次迭代的数值计算。\n\n### 步骤 1：提取已知条件\n已知条件如下：\n- 目标函数：$J(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\phi_{\\varepsilon}(x_{i})$。\n- 矩阵和向量维度：$A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^{n}$。\n- 正则化参数：$\\lambda  0$。\n- 平滑函数：$\\phi_{\\varepsilon}(t) = \\sqrt{t^{2} + \\varepsilon^{2}}$，对于小的 $\\varepsilon  0$ 近似于 $|t|$。\n- 数值数据：\n  - $A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$\n  - $y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n  - $p = 1$ (选择惩罚项的背景)\n  - $\\lambda = 0.1$\n  - $\\varepsilon = 10^{-3}$\n- 初始化：$x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 任务：推导 $x^{(k+1)}$ 的加权正规方程，展示权重与曲率之间的联系，并计算一次 IRLS 迭代以求得 $x^{(1)}$ 和随后的权重 $w^{(1)}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题根据指定标准进行了验证：\n- **科学依据**：该问题设定在凸优化的成熟数学框架内，特别是针对稀疏信号恢复的 Majorization-Minimization 和 IRLS。选择 $\\phi_{\\varepsilon}(t)$ 作为绝对值函数的光滑、严格凸代理是一种标准技术。\n- **良态性**：目标函数 $J(x)$ 是严格凸的，因为它是一个凸函数（最小二乘项）和一系列严格凸函数（当 $\\varepsilon  0$ 时 $\\phi_{\\varepsilon}(t)$ 是严格凸的）之和。因此，存在唯一的最小化子。所描述的 IRLS 算法是解决此类问题的标准程序。\n- **目标明确**：该问题使用精确的数学语言和定义进行陈述。\n- **完整性和一致性**：推导和数值计算所需的所有必要数据和定义都已提供。没有矛盾之处。\n- **其他缺陷**：该问题并非微不足道、隱喻性或故作高深。它探讨了稀疏问题计算优化中的一个核心概念。\n\n### 步骤 3：结论与行动\n该问题是有效的。我将继续进行解答。\n\n### IRLS 更新的推导\n要最小化的目标函数是：\n$$\nJ(x) = \\frac{1}{2}\\|Ax - y\\|_2^2 + \\lambda \\sum_{i=1}^n \\phi_\\varepsilon(x_i)\n$$\n其中 $\\phi_\\varepsilon(t) = \\sqrt{t^2 + \\varepsilon^2}$。IRLS 方法是 Majorization-Minimization (MM) 算法的一个实例。我们需要为非二次惩罚项 $\\psi(x) = \\sum_{i=1}^n \\phi_\\varepsilon(x_i)$ 找到一个二次 majorizer。\n\n让我们关注单个分量 $\\phi_\\varepsilon(t)$。令 $g(u) = \\sqrt{u}$，其中 $u \\ge 0$。函数 $g(u)$ 是凹函数。对于任何凹函数，其任意一点的切线都位于函数图像之上。这就得到了不等式：\n$$\ng(u) \\le g(u_k) + g'(u_k)(u - u_k)\n$$\n其中 $u_k$ 是切点。其导数为 $g'(u) = \\frac{1}{2\\sqrt{u}}$。\n\n我们将此不等式应用于 $\\phi_\\varepsilon(x_i) = \\sqrt{x_i^2 + \\varepsilon^2}$。我们设 $u = x_i^2 + \\varepsilon^2$。在当前迭代点 $x^{(k)}$，我们定义 $u_k = (x_i^{(k)})^2 + \\varepsilon^2$。\n代入不等式，我们得到 $\\phi_\\varepsilon(x_i)$ 的一个 majorizer：\n\\begin{align*}\n\\phi_\\varepsilon(x_i) = \\sqrt{x_i^2 + \\varepsilon^2} \\le \\sqrt{(x_i^{(k)})^2 + \\varepsilon^2} + \\frac{1}{2\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}} \\left( (x_i^2 + \\varepsilon^2) - ((x_i^{(k)})^2 + \\varepsilon^2) \\right) \\\\\n\\le \\phi_\\varepsilon(x_i^{(k)}) + \\frac{1}{2\\phi_\\varepsilon(x_i^{(k)})} (x_i^2 - (x_i^{(k)})^2)\n\\end{align*}\n此不等式对所有 $x_i$ 均成立，并在 $x_i = x_i^{(k)}$ 时取等号。右侧是 $x_i$ 的一个二次函数，并作为 majorizer。\n\n对所有分量 $i=1, \\dots, n$求和并乘以 $\\lambda$，我们得到整个惩罚项的 majorizer：\n$$\n\\lambda \\sum_{i=1}^n \\phi_\\varepsilon(x_i) \\le \\lambda \\sum_{i=1}^n \\left( \\phi_\\varepsilon(x_i^{(k)}) + \\frac{1}{2\\phi_\\varepsilon(x_i^{(k)})} (x_i^2 - (x_i^{(k)})^2) \\right)\n$$\n因此，完整的目标函数 $J(x)$ 被一个二次代理函数 $Q(x, x^{(k)})$ 所 majorize：\n$$\nJ(x) \\le Q(x, x^{(k)}) = \\frac{1}{2}\\|Ax - y\\|_2^2 + \\frac{\\lambda}{2} \\sum_{i=1}^n \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} x_i^2 + C(x^{(k)})\n$$\n其中 $C(x^{(k)})$ 汇集了所有相对于 $x$ 为常数的项。\n下一个迭代点 $x^{(k+1)}$ 通过最小化这个代理函数得到：\n$$\nx^{(k+1)} = \\arg\\min_x Q(x, x^{(k)}) = \\arg\\min_x \\left( \\frac{1}{2}\\|Ax - y\\|_2^2 + \\frac{\\lambda}{2} \\sum_{i=1}^n w_i^{(k)} x_i^2 \\right)\n$$\n其中我们定义权重 $w_i^{(k)}$ 为：\n$$\nw_i^{(k)} = \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} = \\frac{1}{\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}}\n$$\n令 $W^{(k)}$ 为对角元素是 $w_i^{(k)}$ 的对角矩阵。项 $\\sum_{i=1}^n w_i^{(k)} x_i^2$ 可以写成 $x^T W^{(k)} x$。最小化问题变为：\n$$\nx^{(k+1)} = \\arg\\min_x \\left( \\frac{1}{2}(Ax - y)^T(Ax-y) + \\frac{\\lambda}{2} x^T W^{(k)} x \\right)\n$$\n这是 $x$ 的一个二次函数。为了找到最小值，我们将其关于 $x$ 的梯度设为零：\n$$\n\\nabla_x \\left( \\frac{1}{2}(x^T A^T A x - 2y^T A x + y^T y) + \\frac{\\lambda}{2} x^T W^{(k)} x \\right) = 0\n$$\n$$\nA^T A x - A^T y + \\lambda W^{(k)} x = 0\n$$\n重新整理得到 $x=x^{(k+1)}$ 的加权正规方程：\n$$\n(A^T A + \\lambda W^{(k)}) x^{(k+1)} = A^T y\n$$\n\n### 将权重解释为曲率\n问题要求证明权重对应于代理函数的曲率。在此上下文中，“代理”一词指的是 majorizing 函数。\n对于惩罚项的第 $i$ 个分量 $\\lambda \\phi_\\varepsilon(x_i)$，我们构造了二次 majorizer (代理函数)：\n$$\nM_i(x_i; x_i^{(k)}) = \\text{const}(x_i^{(k)}) + \\frac{\\lambda}{2} \\frac{1}{\\phi_\\varepsilon(x_i^{(k)})} x_i^2 = \\text{const}(x_i^{(k)}) + \\frac{\\lambda}{2} w_i^{(k)} x_i^2\n$$\n一维函数的曲率与其二阶导数有关。这个二次代理函数关于 $x_i$ 的二阶导数是：\n$$\n\\frac{d^2}{dx_i^2} M_i(x_i; x_i^{(k)}) = \\lambda w_i^{(k)}\n$$\n因此，权重 $w_i^{(k)}$ 与在第 $k$ 次迭代中用于第 $i$ 个惩罚项的二次代理函数的曲率（二阶导数）成正比（其中 $\\lambda$ 为常数）。\n\n### 数值计算：一次 IRLS 迭代\n已知：\n$A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，$\\lambda = 0.1$，$\\varepsilon = 10^{-3}$，以及 $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n\n**1. 计算权重 $w^{(0)}$ 和矩阵 $W^{(0)}$:**\n权重为 $w_i^{(k)} = 1/\\sqrt{(x_i^{(k)})^2 + \\varepsilon^2}$。对于 $k=0$ 和 $x^{(0)} = \\begin{bmatrix} 0  0  0 \\end{bmatrix}^T$：\n$$\nw_1^{(0)} = w_2^{(0)} = w_3^{(0)} = \\frac{1}{\\sqrt{0^2 + (10^{-3})^2}} = \\frac{1}{10^{-3}} = 1000\n$$\n所以，$w^{(0)} = \\begin{bmatrix} 1000 \\\\ 1000 \\\\ 1000 \\end{bmatrix}$，且 $W^{(0)} = \\text{diag}(1000, 1000, 1000) = 1000 I_3$。\n\n**2. 求解 $x^{(1)}$:**\n我们必须求解系统 $(A^T A + \\lambda W^{(0)}) x^{(1)} = A^T y$。\n首先，计算各个部分：\n$$\nA^T = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}\n$$\n$$\nA^T A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix}\n$$\n$$\nA^T y = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\n$$\n$$\n\\lambda W^{(0)} = 0.1 \\times 1000 I_3 = 100 I_3 = \\begin{bmatrix} 100  0  0 \\\\ 0  100  0 \\\\ 0  0  100 \\end{bmatrix}\n$$\n系统的矩阵是：\n$$\nA^T A + \\lambda W^{(0)} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix} + \\begin{bmatrix} 100  0  0 \\\\ 0  100  0 \\\\ 0  0  100 \\end{bmatrix} = \\begin{bmatrix} 101  0  1 \\\\ 0  101  1 \\\\ 1  1  102 \\end{bmatrix}\n$$\n线性系统是：\n$$\n\\begin{bmatrix} 101  0  1 \\\\ 0  101  1 \\\\ 1  1  102 \\end{bmatrix} \\begin{bmatrix} x_1^{(1)} \\\\ x_2^{(1)} \\\\ x_3^{(1)} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}\n$$\n从前两行得到：\n$101 x_1^{(1)} + x_3^{(1)} = 1 \\implies x_1^{(1)} = (1 - x_3^{(1)}) / 101$\n$101 x_2^{(1)} + x_3^{(1)} = 1 \\implies x_2^{(1)} = (1 - x_3^{(1)}) / 101$\n这表明 $x_1^{(1)} = x_2^{(1)}$。\n将它们代入第三个方程：\n$x_1^{(1)} + x_2^{(1)} + 102 x_3^{(1)} = 2$\n$2 \\left( \\frac{1 - x_3^{(1)}}{101} \\right) + 102 x_3^{(1)} = 2$\n$2(1 - x_3^{(1)}) + 101 \\times 102 x_3^{(1)} = 2 \\times 101$\n$2 - 2x_3^{(1)} + 10302 x_3^{(1)} = 202$\n$10300 x_3^{(1)} = 200 \\implies x_3^{(1)} = \\frac{200}{10300} = \\frac{2}{103}$\n现在，我们求 $x_1^{(1)}$ 和 $x_2^{(1)}$：\n$x_1^{(1)} = x_2^{(1)} = \\frac{1 - 2/103}{101} = \\frac{(103-2)/103}{101} = \\frac{101/103}{101} = \\frac{1}{103}$\n所以，$x^{(1)} = \\begin{bmatrix} 1/103 \\\\ 1/103 \\\\ 2/103 \\end{bmatrix}$。\n\n**3. 从 $x^{(1)}$ 计算 $w^{(1)}$:**\n使用 $x^{(1)}$ 和 $\\varepsilon = 10^{-3} = 1/1000$：\n$w_1^{(1)} = \\frac{1}{\\sqrt{(x_1^{(1)})^2 + \\varepsilon^2}} = \\frac{1}{\\sqrt{(1/103)^2 + (1/1000)^2}} = \\frac{1}{\\sqrt{\\frac{1}{103^2} + \\frac{1}{1000^2}}} = \\frac{1}{\\sqrt{\\frac{1000^2 + 103^2}{103^2 \\cdot 1000^2}}} = \\frac{103 \\cdot 1000}{\\sqrt{1000^2 + 103^2}}$\n$103^2 = 10609$, $1000^2 = 1000000$。\n$w_1^{(1)} = \\frac{103000}{\\sqrt{1000000 + 10609}} = \\frac{103000}{\\sqrt{1010609}}$。\n\n由于 $x_2^{(1)} = x_1^{(1)}$，所以 $w_2^{(1)} = w_1^{(1)} = \\frac{103000}{\\sqrt{1010609}}$。\n\n对于 $w_3^{(1)}$：\n$w_3^{(1)} = \\frac{1}{\\sqrt{(x_3^{(1)})^2 + \\varepsilon^2}} = \\frac{1}{\\sqrt{(2/103)^2 + (1/1000)^2}} = \\frac{1}{\\sqrt{\\frac{4}{103^2} + \\frac{1}{1000^2}}} = \\frac{103 \\cdot 1000}{\\sqrt{4 \\cdot 1000^2 + 103^2}}$\n$w_3^{(1)} = \\frac{103000}{\\sqrt{4000000 + 10609}} = \\frac{103000}{\\sqrt{4010609}}$。\n\n最终答案所需的分量是：\n$x_1^{(1)} = \\frac{1}{103}$，$x_2^{(1)} = \\frac{1}{103}$，$x_3^{(1)} = \\frac{2}{103}$\n$w_1^{(1)} = \\frac{103000}{\\sqrt{1010609}}$，$w_2^{(1)} = \\frac{103000}{\\sqrt{1010609}}$，$w_3^{(1)} = \\frac{103000}{\\sqrt{4010609}}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{103}  \\frac{1}{103}  \\frac{2}{103}  \\frac{103000}{\\sqrt{1010609}}  \\frac{103000}{\\sqrt{1010609}}  \\frac{103000}{\\sqrt{4010609}} \\end{pmatrix}}\n$$", "id": "3454747"}, {"introduction": "IRLS算法在求解非凸问题时非常强大，但它的成功并非总是理所当然。这个练习是一个关键的思想实验，旨在揭示算法的局限性，即它可能会收敛到次优的稳定点。通过构建一个违反限制等距性质（RIP）的特定反例 [@problem_id:3454768]，你将深入理解初始点的选择和问题结构对找到最稀疏解的关键影响。", "problem": "考虑压缩感知中的一个线性传感模型，其测量矩阵为 $A \\in \\mathbb{R}^{1 \\times 2}$，未知信号为 $x \\in \\mathbb{R}^{2}$，测量值为 $y \\in \\mathbb{R}$，由 $y = A x$ 给出。设 $A = [\\,1 \\;\\; 1\\,]$，并假设数据是无噪声的，其真实值为 $x^{\\mathrm{true}} = (1,0)^{\\top}$，因此 $y = A x^{\\mathrm{true}} = 1$。对于阶数为 $k$ 的限制等距性质 (Restricted Isometry Property, RIP)，其定义为存在一个常数 $\\delta_{k} \\in [0,1)$，使得对于每个 $k$-稀疏的 $x$，\n$$(1 - \\delta_{k}) \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_{k}) \\|x\\|_{2}^{2}，$$\n此性质对于该矩阵 $A$ 在 $k = 2$ 时不成立。我们考虑用于 $\\ell_{p}$ 最小化的迭代重加权最小二乘 (Iterative Reweighted Least Squares, IRLS) 算法，其中 $p \\in (0,1)$，平滑参数为 $\\epsilon  0$。定义平滑 $\\ell_{p}$ 目标函数\n$$J_{p,\\epsilon}(x) = \\sum_{i=1}^{2} \\big(x_{i}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2}}。$$\nIRLS 方案在第 $k$ 次迭代时，根据 $x^{(k)}$ 选择权重 $w_{i}^{(k)}$，然后计算 $x^{(k+1)}$ 作为带约束的加权最小二乘子问题的解，\n$$\\min_{x \\in \\mathbb{R}^{2}} \\sum_{i=1}^{2} w_{i}^{(k)} x_{i}^{2} \\quad \\text{约束条件为} \\quad A x = y，$$\n其中正权重遵循关于 $|x_{i}^{(k)}|$ 的标准单调性，例如 $w_{i}^{(k)} = \\big((x_{i}^{(k)})^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2} - 1}$。在对称点 $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 处初始化 IRLS 算法。\n\n仅使用基本定义和带约束光滑优化的一阶最优性条件，证明 IRLS 的迭代点保持在对称点，并因此收敛到平滑目标函数 $J_{p,\\epsilon}$ 的一个驻点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$，尽管存在一个更稀疏的可行点 $x^{\\mathrm{true}} = (1,0)^{\\top}$。构建上述的显式反例，并为量化其次优性，计算代价差距\n$$\\Delta(p,\\epsilon) = J_{p,\\epsilon}\\!\\big(x^{\\star}\\big) - J_{p,\\epsilon}\\!\\big(x^{\\mathrm{true}}\\big)，$$\n的精确解析表达式，该表达式是关于 $p \\in (0,1)$ 和 $\\epsilon  0$ 的闭式函数。你的最终答案必须是这个单一的表达式。无需四舍五入，也没有物理单位。", "solution": "该问题经验证是自洽的、有科学依据且适定的。所有必要信息均已提供，且无内部矛盾。我们开始求解。\n\n该问题要求我们分析迭代重加权最小二乘 (IRLS) 算法在 $\\ell_{p}$ 最小化的一个特定实例中的行为。我们给定一个线性系统 $y = Ax$，其中 $A = [\\,1 \\;\\; 1\\,]$，$x \\in \\mathbb{R}^{2}$，$y=1$。IRLS 算法试图通过最小化平滑 $\\ell_{p}$ 目标函数 $J_{p,\\epsilon}(x) = \\sum_{i=1}^{2} \\big(x_{i}^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2}}$ 来寻找稀疏解，其中 $p \\in (0,1)$，平滑参数为 $\\epsilon  0$，并受约束 $Ax = y$ 的限制。\n\n算法在可行点 $x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 处初始化，因为 $A x^{(0)} = 1 \\cdot \\tfrac{1}{2} + 1 \\cdot \\tfrac{1}{2} = 1 = y$。在每次迭代 $k$ 中，下一个迭代点 $x^{(k+1)}$ 通过求解加权最小二乘问题得到：\n$$x^{(k+1)} = \\arg\\min_{x \\in \\mathbb{R}^{2}} \\sum_{i=1}^{2} w_{i}^{(k)} x_{i}^{2} \\quad \\text{约束条件为} \\quad A x = y$$\n其中权重由 $w_{i}^{(k)} = \\big((x_{i}^{(k)})^{2} + \\epsilon^{2}\\big)^{\\frac{p}{2} - 1}$ 给出。\n\n首先，我们用数学归纳法证明迭代点保持在对称点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n归纳基础已给出：$x^{(0)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n\n对于归纳步骤，假设对于某个 $k \\ge 0$，我们有 $x^{(k)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。我们必须计算 $x^{(k+1)}$。\n首先，我们根据 $x^{(k)}$ 计算权重 $w_i^{(k)}$：\n$$x_{1}^{(k)} = x_{2}^{(k)} = \\frac{1}{2}$$\n因此，权重是相同的：\n$$w_{1}^{(k)} = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1} = \\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1}$$\n$$w_{2}^{(k)} = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1} = \\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2} - 1}$$\n令 $w^{(k)} = w_{1}^{(k)} = w_{2}^{(k)}$。由于 $p \\in (0,1)$ 且 $\\epsilon  0$，幂的底数为正，因此权重 $w^{(k)}$ 是良定义且为正的。\n\n现在，我们求解关于 $x^{(k+1)} = (x_1, x_2)^\\top$ 的子问题：\n$$\\min_{x_1, x_2} w^{(k)} x_{1}^{2} + w^{(k)} x_{2}^{2} \\quad \\text{约束条件为} \\quad x_{1} + x_{2} = 1$$\n由于 $w^{(k)}  0$，这等价于最小化 $x_{1}^{2} + x_{2}^{2}$。这是一个在直线 $x_1+x_2=1$ 上寻找具有最小欧几里得范数的点的经典问题。我们可以使用拉格朗日乘子法。拉格朗日函数是：\n$$\\mathcal{L}(x_1, x_2, \\lambda) = x_{1}^{2} + x_{2}^{2} + \\lambda(x_{1} + x_{2} - 1)$$\n求偏导数并令其为零，得到一阶最优性条件：\n$$\\frac{\\partial \\mathcal{L}}{\\partial x_1} = 2x_{1} + \\lambda = 0 \\implies x_1 = -\\frac{\\lambda}{2}$$\n$$\\frac{\\partial \\mathcal{L}}{\\partial x_2} = 2x_{2} + \\lambda = 0 \\implies x_2 = -\\frac{\\lambda}{2}$$\n这意味着 $x_1 = x_2$。将此代入约束 $x_1 + x_2 = 1$，我们得到：\n$$x_1 + x_1 = 2x_1 = 1 \\implies x_1 = \\frac{1}{2}$$\n因此，唯一解是 $x_1 = \\frac{1}{2}$ 和 $x_2 = \\frac{1}{2}$。\n所以，$x^{(k+1)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n根据数学归纳法，$x^{(k)} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 对所有 $k \\ge 0$ 成立。迭代序列是常数序列，因此收敛到 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$。\n\n接下来，我们必须证明 $x^{\\star}$ 是带约束的平滑目标函数 $J_{p,\\epsilon}(x)$ 的一个驻点。如果一个点满足 Karush-Kuhn-Tucker (KKT) 条件，那么它就是驻点。对于一个等式约束问题 $\\min f(x)$ s.t. $g(x)=0$，这意味着存在一个拉格朗日乘子 $\\nu$ 使得 $\\nabla f(x^{\\star}) + \\nu \\nabla g(x^{\\star}) = 0$。\n在这里，$f(x) = J_{p,\\epsilon}(x)$ 且 $g(x) = Ax-y = x_1+x_2-1$。\n目标函数的梯度是 $\\nabla J_{p,\\epsilon}(x) = \\begin{pmatrix} \\frac{\\partial J_{p,\\epsilon}}{\\partial x_1} \\\\ \\frac{\\partial J_{p,\\epsilon}}{\\partial x_2} \\end{pmatrix}$，其中\n$$\\frac{\\partial J_{p,\\epsilon}}{\\partial x_i} = p x_i \\left(x_i^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1}$$\n在 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 处，梯度的两个分量相等：\n$$\\frac{\\partial J_{p,\\epsilon}}{\\partial x_1}\\bigg|_{x^{\\star}} = \\frac{\\partial J_{p,\\epsilon}}{\\partial x_2}\\bigg|_{x^{\\star}} = p \\left(\\frac{1}{2}\\right) \\left(\\left(\\frac{1}{2}\\right)^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1} = \\frac{p}{2} \\left(\\frac{1}{4} + \\epsilon^2\\right)^{\\frac{p}{2}-1}$$\n设这个共同的值为 $C$。那么，$\\nabla J_{p,\\epsilon}(x^{\\star}) = (C, C)^{\\top}$。\n约束的梯度是 $\\nabla g(x) = A^{\\top} = (1, 1)^{\\top}$。\nKKT 条件是 $\\nabla J_{p,\\epsilon}(x^{\\star}) + \\nu A^{\\top} = 0$：\n$$\\begin{pmatrix} C \\\\ C \\end{pmatrix} + \\nu \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n这个系统可以通过选择 $\\nu = -C$ 来满足。由于存在这样的拉格朗日乘子，$x^{\\star}$ 是该约束问题的一个驻点。这证实了 IRLS 算法收敛到一个驻点，但在本例中，这个驻点并非最稀疏的可能解 $x^{\\mathrm{true}} = (1,0)^{\\top}$。\n\n最后，我们计算代价差距 $\\Delta(p,\\epsilon) = J_{p,\\epsilon}(x^{\\star}) - J_{p,\\epsilon}(x^{\\mathrm{true}})$。\n我们在驻点 $x^{\\star} = \\big(\\tfrac{1}{2}, \\tfrac{1}{2}\\big)^{\\top}$ 处计算目标函数的值：\n$$J_{p,\\epsilon}(x^{\\star}) = \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(\\left(\\frac{1}{2}\\right)^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = 2\\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2}}$$\n我们可以化简此项：\n$$2\\left(\\frac{1}{4} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = 2\\left(\\frac{1+4\\epsilon^{2}}{4}\\right)^{\\frac{p}{2}} = 2 \\frac{\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}}{4^{\\frac{p}{2}}} = 2 \\frac{\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}}{2^{p}} = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}}$$\n接下来，我们在稀疏的真实值点 $x^{\\mathrm{true}} = (1,0)^{\\top}$ 处计算目标函数的值：\n$$J_{p,\\epsilon}(x^{\\mathrm{true}}) = \\left(1^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(0^{2} + \\epsilon^{2}\\right)^{\\frac{p}{2}} = \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\left(\\epsilon^{2}\\right)^{\\frac{p}{2}}$$\n由于 $\\epsilon  0$，$(\\epsilon^2)^{p/2} = (\\epsilon^p \\epsilon^p)^{1/2} = \\sqrt{\\epsilon^{2p}} = |\\epsilon^p| = \\epsilon^p$ (因为 $\\epsilon  0$)。\n所以，$J_{p,\\epsilon}(x^{\\mathrm{true}}) = \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\epsilon^{p}$。\n\n代价差距 $\\Delta(p,\\epsilon)$ 是它们的差值：\n$$\\Delta(p,\\epsilon) = J_{p,\\epsilon}(x^{\\star}) - J_{p,\\epsilon}(x^{\\mathrm{true}}) = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}} - \\left( \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} + \\epsilon^{p} \\right)$$\n$$\\Delta(p,\\epsilon) = 2^{1-p}\\left(1+4\\epsilon^{2}\\right)^{\\frac{p}{2}} - \\left(1 + \\epsilon^{2}\\right)^{\\frac{p}{2}} - \\epsilon^{p}$$\n这就是次优性差距的最终解析表达式。", "answer": "$$\\boxed{2^{1-p}(1+4\\epsilon^2)^{\\frac{p}{2}} - (1+\\epsilon^2)^{\\frac{p}{2}} - \\epsilon^{p}}$$", "id": "3454768"}, {"introduction": "将理论知识转化为有效的计算工具是科学研究的核心。本练习将指导你实现完整的IRLS算法，并探索在求解 $\\ell_p$ ($0 \\lt p \\lt 1$) 这类非凸问题时至关重要的“连续性”（continuation）策略 [@problem_id:3454798]。通过编程比较在参数 $p$ 或 $\\lambda$ 上的两种不同连续性路径，你将在不同性质的传感矩阵上凭经验检验它们的性能，从而弥合理论与实际应用之间的鸿沟。", "problem": "考虑一个线性逆问题，其中传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 是欠定的，待求信号 $x^{\\star} \\in \\mathbb{R}^{n}$ 是一个未知的 $k$-稀疏信号。带噪观测值由 $b = A x^{\\star} + \\eta$ 给出，其中 $\\eta \\in \\mathbb{R}^{m}$ 是加性噪声模型。目标是通过求解一个非凸正则化最小二乘问题来恢复 $x^{\\star}$，该问题利用 $0  p  1$ 的 $\\ell_{p}$ 拟范数来促进 $x^{\\star}$ 的稀疏性。具体来说，考虑目标函数\n$$\nF(x; p, \\lambda, \\varepsilon) = \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\left(x_{i}^{2} + \\varepsilon\\right)^{\\frac{p}{2}},\n$$\n其中 $p \\in (0, 1]$ 控制稀疏诱导的非凸性，$\\lambda  0$ 是一个正则化参数，$\\varepsilon  0$ 是一个小的平滑常数，用于保证可微性和数值稳定性。\n\n您需要为最小化 $F(x; p, \\lambda, \\varepsilon)$ 设计、推导并实现一个基于主化-最小化（majorization-minimization）原则构造的迭代重加权最小二乘（IRLS）方法，并经验性地比较两种连续化策略：\n- $p$ 的连续化：从 $p = 1$ 开始，单调递减至目标值 $p_{\\min}$，同时保持 $\\lambda$ 固定，并在各阶段之间使用热启动。\n- $\\lambda$ 的连续化：从一个较大的 $\\lambda_{\\max}$ 开始，单调递减至目标值 $\\lambda_{\\min}$，同时将 $p$ 固定在目标值 $p_{\\min}$，并在各阶段之间使用热启动。\n\n您的比较必须在一套测试传感矩阵上进行，这些矩阵具有经验性计算的限制等距性质（Restricted Isometry Property, RIP）常数。对于给定的 $A$ 和整数 $k \\geq 1$，限制等距常数 $\\delta_{k}(A)$ 定义为满足以下条件的最小 $\\delta \\geq 0$：对于所有 $|S| \\leq k$ 的支撑集 $S \\subset \\{1,\\dots,n\\}$ 和所有 $z \\in \\mathbb{R}^{|S|}$，\n$$\n(1 - \\delta) \\|z\\|_{2}^{2} \\leq \\|A_{S} z\\|_{2}^{2} \\leq (1 + \\delta) \\|z\\|_{2}^{2},\n$$\n其中 $A_{S} \\in \\mathbb{R}^{m \\times |S|}$ 表示由 $S$ 索引的列构成的 $A$ 的子矩阵。对于较小的 $n$ 和中等的 $k$，可以通过枚举所有大小为 $k$ 的支撑集 $S$ 并计算 $A_{S}^{\\top} A_{S}$ 的极端特征值来精确计算 $\\delta_{k}(A)$。\n\n您必须基于凹惩罚项的主化-最小化原则推导出一个 IRLS 方案，该方案在每次迭代中用一个二次代理函数替换非凸项，从而得到一系列形式如下的加权最小二乘问题：\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{\\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_{i}^{(t)} x_{i}^{2}\\right\\},\n$$\n其中权重 $w_{i}^{(t)}$ 依赖于迭代，由前一个迭代点 $x^{(t)}$ 确定。推导必须从 $\\ell_{p}$ 拟范数的基本定义以及函数 $u \\mapsto (u + \\varepsilon)^{\\frac{p}{2}}$（对于 $0  p \\leq 1$）的凹性出发，并逻辑地构建一个有效的主化-最小化代理函数以及用于加权最小二乘更新的相应正规方程。\n\n定义一个相对于 $x^{\\star}$ 的“差的局部最小值”概念：如果一个计算出的解 $\\hat{x}$ 的重构误差 $\\|\\hat{x} - x^{\\star}\\|_{2}$ 比竞争路径的重构误差严格大出一个余量 $\\eta  0$（$\\eta$ 将被选为一个小的数值容差），则称该解为“差的”。您必须通过计算两种策略产生的最终解的重构误差来比较这两条连续化路径，并声明对于每个测试用例，哪条路径能更可靠地避免差的局部最小值。具体来说，对于每个测试用例，输出：\n- 如果 $p$ 的连续化策略产生的重构误差比 $\\lambda$ 的连续化策略至少小 $\\eta$，则输出 $1$。\n- 如果 $\\lambda$ 的连续化策略产生的重构误差比 $p$ 的连续化策略至少小 $\\eta$，则输出 $-1$。\n- 如果两者都没有比对方的优势大出至少 $\\eta$，则输出 $0$。\n\n构建以下包含四个案例的测试套件，通过为所有伪随机数生成过程设置种子来确保科学真实性和可复现性。对于每个案例，将 $A$ 的列归一化为单位 $\\ell_{2}$ 范数，生成一个 $k$-稀疏向量 $x^{\\star}$，其非零项从零均值高斯分布中抽取，并设置 $b = A x^{\\star} + \\eta$，其中 $\\eta$ 从具有指定方差的零均值高斯分布中抽取。对于每个测试用例，通过枚举所有大小为 $k$ 的支撑集 $S$ 来精确计算 $\\delta_{k}(A)$。\n\n- 案例1（理想路径，有利的限制等距性质（RIP））：\n  - $m = 10$, $n = 16$, $k = 3$，矩阵类型：独立同分布的高斯条目，经过缩放和列归一化，噪声标准差 $\\sigma = 10^{-3}$，种子 $= 1$，$p_{\\min} = 0.5$，$\\lambda_{\\min} = 0.05$，$\\lambda_{\\max} = 1.0$。\n- 案例2（中等列相干性，中等的 RIP 常数）：\n  - $m = 10$, $n = 16$, $k = 3$，矩阵类型：通过在归一化前加上前一列的一部分来构建相关列（相干水平 $c = 0.5$），噪声标准差 $\\sigma = 10^{-3}$，种子 $= 2$，$p_{\\min} = 0.5$，$\\lambda_{\\min} = 0.05$，$\\lambda_{\\max} = 1.0$。\n- 案例3（近似重复的列，接近边界的 RIP 常数）：\n  - $m = 8$, $n = 16$, $k = 3$，矩阵类型：其中一列通过对另一列施加小扰动的方式构建为其近似副本，噪声标准差 $\\sigma = 10^{-3}$，种子 $= 3$，$p_{\\min} = 0.5$，$\\lambda_{\\min} = 0.05$，$\\lambda_{\\max} = 1.0$。\n- 案例4（更欠定的情形，更低的 $p$ 值）：\n  - $m = 6$, $n = 16$, $k = 2$，矩阵类型：独立同分布的高斯条目，经过缩放和列归一化，噪声标准差 $\\sigma = 10^{-3}$，种子 $= 4$，$p_{\\min} = 0.3$，$\\lambda_{\\min} = 0.02$，$\\lambda_{\\max} = 1.0$。\n\n实现要求：\n- 在目标函数中使用固定的平滑参数 $\\varepsilon = 10^{-8}$。\n- 在连续化阶段之间使用热启动，即将前一阶段的最终迭代结果作为下一阶段的初始点。\n- 对 IRLS 内循环迭代使用停止准则，该准则基于迭代点的相对变化，容差为 $\\tau = 10^{-8}$，且每个阶段最多进行 100 次迭代。\n- 对每条路径使用包含 $8$ 个阶段的连续化方案。对于 $p$ 的连续化，使用从 $1$ 到 $p_{\\min}$ 的单调递减序列。对于 $\\lambda$ 的连续化，使用从 $\\lambda_{\\max}$ 到 $\\lambda_{\\min}$ 的单调递减几何序列。\n\n角度单位不适用。不涉及物理单位。所有数值输出必须是无量纲实数。用于声明严格改进的“余量”必须设置为 $\\eta = 10^{-6}$。\n\n最终输出规范：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的整数列表（每个整数在 $\\{-1, 0, 1\\}$ 中），按顺序对应四个测试案例（案例1、案例2、案例3、案例4）。例如，输出行可能看起来像 $\\texttt{[1,0,-1,1]}$，但实际值必须由您的实现计算得出。", "solution": "该问题要求设计、推导和实现一个迭代重加权最小二乘（IRLS）算法，以解决用于稀疏信号恢复的非凸正则化最小二乘问题。推导必须基于主化-最小化（MM）原则。随后，必须实现两种连续化策略（一种针对稀疏诱导参数 $p$，另一种针对正则化参数 $\\lambda$），并在一个定义的测试套件上进行比较。\n\n待最小化的目标函数是：\n$$\nF(x; p, \\lambda, \\varepsilon) = \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\left(x_{i}^{2} + \\varepsilon\\right)^{\\frac{p}{2}}\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 是待恢复的信号，$A \\in \\mathbb{R}^{m \\times n}$ 是传感矩阵，$b \\in \\mathbb{R}^{m}$ 是观测值，$\\lambda  0$ 是正则化参数，$p \\in (0, 1]$ 控制正则化项的非凸性，$\\varepsilon  0$ 是一个平滑参数。\n\n函数 $F(x)$ 由两部分组成：一个凸的数据保真项 $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ 和一个非凸的正则化项 $\\Phi(x) = \\lambda \\sum_{i=1}^{n} \\phi(x_i)$，其中 $\\phi(x_i) = (x_i^2 + \\varepsilon)^{p/2}$。IRLS 算法是通过将 MM 原则应用于非凸项 $\\Phi(x)$ 来推导的。\n\nMM 的核心思想是用一系列更易于最小化的简单代理函数来替代一个难以最小化的目标函数。在每次迭代 $t$ 中，我们构造一个在当前迭代点 $x^{(t)}$ 处主化原始函数 $F(x)$ 的代理函数 $G(x | x^{(t)})$，即对于所有 $x$ 都有 $G(x | x^{(t)}) \\ge F(x)$ 且 $G(x^{(t)} | x^{(t)}) = F(x^{(t)})$。然后通过最小化这个代理函数来找到下一个迭代点：$x^{(t+1)} = \\arg\\min_x G(x | x^{(t)})$。\n\n我们将为非凸惩罚项 $\\Phi(x)$ 构造一个代理函数。数据保真项 $f(x)$ 已经是凸的和二次的，因此我们可以保持其原样。我们关注单个惩罚函数 $\\phi(x_i) = (x_i^2 + \\varepsilon)^{p/2}$。让我们引入变量替换 $u_i = x_i^2$。每个分量上的惩罚项变成一个关于 $u_i \\ge 0$ 的函数：\n$$g(u_i) = (u_i + \\varepsilon)^{p/2}$$\n为了找到 $\\phi(x_i)$ 的一个二次主化函数，我们首先找到 $g(u_i)$ 的一个线性主化函数。我们检验 $g(u_i)$ 在 $u_i \\ge 0$ 上的凹性。$g$ 关于 $u_i$ 的一阶和二阶导数是：\n$$\ng'(u_i) = \\frac{p}{2} (u_i + \\varepsilon)^{\\frac{p}{2} - 1}\n$$\n$$\ng''(u_i) = \\frac{p}{2} \\left(\\frac{p}{2} - 1\\right) (u_i + \\varepsilon)^{\\frac{p}{2} - 2}\n$$\n对于 $p \\in (0, 1]$，我们有 $p/2  0$ 和 $(p/2 - 1) \\le -1/2$。由于 $u_i \\ge 0$ 且 $\\varepsilon  0$，项 $(u_i + \\varepsilon)$ 总是正的。因此，$g''(u_i) \\le 0$，这证明了 $g(u_i)$ 是 $u_i$ 的一个凹函数。\n\n对于任何凹且可微的函数，其在任意点的一阶泰勒展开（即切线）都提供了该函数的一个上界。设 $u_i^{(t)} = (x_i^{(t)})^2$ 为当前迭代点的值。那么，对于所有 $u_i \\ge 0$：\n$$g(u_i) \\le g(u_i^{(t)}) + g'(u_i^{(t)}) (u_i - u_i^{(t)})$$\n将 $u_i = x_i^2$ 代回，我们得到 $\\phi(x_i)$ 的一个主化函数：\n$$\n(x_i^2 + \\varepsilon)^{p/2} \\le ((x_i^{(t)})^2 + \\varepsilon)^{p/2} + \\frac{p}{2} ((x_i^{(t)})^2 + \\varepsilon)^{\\frac{p}{2} - 1} (x_i^2 - (x_i^{(t)})^2)\n$$\n这个主化函数是关于 $x_i$ 的一个二次函数，加上一些相对于 $x_i$ 是常数的项。我们定义权重 $w_i^{(t)}$ 为：\n$$\nw_i^{(t)} = \\frac{p}{2} \\left((x_i^{(t)})^2 + \\varepsilon\\right)^{\\frac{p}{2} - 1}\n$$\n$\\phi(x_i)$ 的主化不等式可以写为：\n$$\n\\phi(x_i) \\le w_i^{(t)} x_i^2 + C_i^{(t)}\n$$\n其中 $C_i^{(t)} = \\phi(x_i^{(t)}) - w_i^{(t)} (x_i^{(t)})^2$ 是一个相对于 $x$ 的常数。\n\n对所有分量求和并加上数据保真项，我们得到完整目标函数 $F(x)$ 的主化代理函数：\n$$\nG(x | x^{(t)}) = \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} \\left( w_i^{(t)} x_i^2 + C_i^{(t)} \\right)\n$$\n为了找到下一个迭代点 $x^{(t+1)}$，我们最小化这个关于 $x$ 的代理函数：\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\mathbb{R}^{n}} G(x | x^{(t)}) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_i^{(t)} x_i^2 \\right\\}\n$$\n这是一个加权最小二乘问题，也称为 Tikhonov 正则化或岭回归，其权重取决于前一个迭代点 $x^{(t)}$。目标函数是关于 $x$ 的二次函数，可以写成：\n$$\nJ(x) = \\frac{1}{2}(Ax - b)^T(Ax - b) + \\lambda x^T W^{(t)} x\n$$\n其中 $W^{(t)}$ 是一个对角矩阵，其对角线上的元素是权重 $w_i^{(t)}$。为了找到最小值，我们将关于 $x$ 的梯度设为零：\n$$\n\\nabla_x J(x) = A^T(Ax - b) + 2\\lambda W^{(t)} x = 0\n$$\n这就得到了用于更新的正规方程：\n$$\n\\left(A^T A + 2\\lambda W^{(t)}\\right) x = A^T b\n$$\n下一个迭代点 $x^{(t+1)}$ 是这个线性系统的解：\n$$\nx^{(t+1)} = \\left(A^T A + 2\\lambda W^{(t)}\\right)^{-1} A^T b\n$$\n这构成了 IRLS 算法的核心。该过程从一个初始猜测 $x^{(0)}$（例如，$x^{(0)}=0$）开始，通过计算权重和求解加权最小二乘问题来迭代地精炼解，直到满足收敛准则。\n\n待比较的两种连续化策略是：\n1.  **$p$ 的连续化**：该算法从 $p=1$（凸的 $\\ell_1$ 情况）和固定的目标正则化参数 $\\lambda = \\lambda_{\\min}$ 开始。$p$ 的值在一系列阶段中逐渐减小到目标值 $p_{\\min}$。每个阶段的解都用作下一阶段的热启动。该策略旨在通过从一个凸的优化环境中开始并逐渐引入非凸性，来引导迭代避开差的局部最小值。\n2.  **$\\lambda$ 的连续化**：该算法从一个大的正则化参数 $\\lambda = \\lambda_{\\max}$ 和一个固定的目标非凸性参数 $p = p_{\\min}$ 开始。$\\lambda$ 的值逐渐减小到目标值 $\\lambda_{\\min}$。一个大的初始 $\\lambda$ 会促使解的范数非常小，通常接近于零，并能有效地在早期选出正确的稀疏支撑集。随着 $\\lambda$ 的减小，非零分量的幅度会得到精炼。\n\n比较将在四个具有不同传感矩阵特性的测试用例上进行，这些特性部分由它们的限制等距性质（RIP）常数 $\\delta_k$ 来表征。每条路径的最终解将与真实值 $x^{\\star}$ 进行比较，对于该案例，产生比对方小至少 $\\eta=10^{-6}$ 的重构误差的路径被认为是更优的。", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    执行连续化策略比较的主求解函数。\n    \"\"\"\n\n    def compute_rip_constant(A, k):\n        \"\"\"\n        通过枚举所有 C(n,k) 个子矩阵来计算精确的 RIP 常数 delta_k(A)。\n        此函数用于表征问题，并非最终输出所必需，但根据问题要求包含在内。\n        \"\"\"\n        n = A.shape[1]\n        all_supports = combinations(range(n), k)\n        \n        min_lambda_min = float('inf')\n        max_lambda_max = float('-inf')\n        \n        for support in all_supports:\n            S = list(support)\n            A_S = A[:, S]\n            A_S_T_A_S = A_S.T @ A_S\n            \n            try:\n                # 对对称矩阵使用 eigh\n                eigvals = scipy.linalg.eigh(A_S_T_A_S, eigvals_only=True)\n                min_lambda_min = min(min_lambda_min, eigvals[0])\n                max_lambda_max = max(max_lambda_max, eigvals[-1])\n            except np.linalg.LinAlgError:\n                # 如果 A_S 是列满秩的，A_S^T A_S 不应该出现这种情况。\n                continue\n\n        delta_k = max(1 - min_lambda_min, max_lambda_max - 1)\n        return delta_k\n\n    def generate_matrix(m, n, matrix_type, params, rng):\n        \"\"\"生成传感矩阵 A。\"\"\"\n        if matrix_type == 'gaussian':\n            A_raw = rng.standard_normal(size=(m, n))\n        elif matrix_type == 'correlated':\n            c = params['c']\n            A_raw = rng.standard_normal(size=(m, n))\n            for i in range(1, n):\n                A_raw[:, i] += c * A_raw[:, i - 1]\n        elif matrix_type == 'near_duplicate':\n            A_raw = rng.standard_normal(size=(m, n))\n            # 使第1列成为第0列的扰动版本\n            perturbation = rng.standard_normal(size=m) * 1e-5\n            A_raw[:, 1] = A_raw[:, 0] + perturbation\n        else:\n            raise ValueError(\"Unknown matrix type\")\n        \n        # 归一化列\n        A = A_raw / np.linalg.norm(A_raw, axis=0, keepdims=True)\n        return A\n\n    def irls(A, b, p, lamb, epsilon, tau, max_iter, x_init):\n        \"\"\"\n        执行迭代重加权最小二乘法以最小化 l_p 目标函数。\n        \"\"\"\n        x = np.copy(x_init)\n        n = A.shape[1]\n        AtA = A.T @ A\n        Atb = A.T @ b\n\n        for _ in range(max_iter):\n            x_old = np.copy(x)\n            \n            # 计算权重\n            weights = (p / 2) * ((x**2 + epsilon)**(p / 2 - 1))\n            W = np.diag(weights)\n            \n            # 求解正规方程\n            try:\n                matrix_to_invert = AtA + 2 * lamb * W\n                x = np.linalg.solve(matrix_to_invert, Atb)\n            except np.linalg.LinAlgError:\n                # 如果矩阵是奇异的，使用伪逆（更稳健）\n                x = np.linalg.pinv(matrix_to_invert) @ Atb\n\n            # 检查收敛性\n            norm_x_old = np.linalg.norm(x_old)\n            if norm_x_old > 1e-9: # 避免除以零\n                rel_change = np.linalg.norm(x - x_old) / norm_x_old\n                if rel_change  tau:\n                    break\n            elif np.linalg.norm(x - x_old)  tau:\n                break\n                \n        return x\n\n    def run_p_continuation(A, b, p_min, lambda_min, params):\n        \"\"\"使用 p 的连续化策略运行 IRLS。\"\"\"\n        num_stages = params['num_stages']\n        p_schedule = np.linspace(1.0, p_min, num_stages)\n        x = np.zeros(A.shape[1])\n        \n        for p_val in p_schedule:\n            x = irls(A, b, p_val, lambda_min, params['epsilon'], params['tau'], params['max_iter'], x)\n            \n        return x\n\n    def run_lambda_continuation(A, b, p_min, lambda_min, lambda_max, params):\n        \"\"\"使用 lambda 的连续化策略运行 IRLS。\"\"\"\n        num_stages = params['num_stages']\n        lambda_schedule = np.geomspace(lambda_max, lambda_min, num_stages)\n        x = np.zeros(A.shape[1])\n        \n        for lambda_val in lambda_schedule:\n            x = irls(A, b, p_min, lambda_val, params['epsilon'], params['tau'], params['max_iter'], x)\n            \n        return x\n\n    test_cases = [\n        {'m': 10, 'n': 16, 'k': 3, 'sigma': 1e-3, 'seed': 1, 'matrix_type': 'gaussian', 'matrix_params': {}, 'p_min': 0.5, 'lambda_min': 0.05, 'lambda_max': 1.0},\n        {'m': 10, 'n': 16, 'k': 3, 'sigma': 1e-3, 'seed': 2, 'matrix_type': 'correlated', 'matrix_params': {'c': 0.5}, 'p_min': 0.5, 'lambda_min': 0.05, 'lambda_max': 1.0},\n        {'m': 8, 'n': 16, 'k': 3, 'sigma': 1e-3, 'seed': 3, 'matrix_type': 'near_duplicate', 'matrix_params': {}, 'p_min': 0.5, 'lambda_min': 0.05, 'lambda_max': 1.0},\n        {'m': 6, 'n': 16, 'k': 2, 'sigma': 1e-3, 'seed': 4, 'matrix_type': 'gaussian', 'matrix_params': {}, 'p_min': 0.3, 'lambda_min': 0.02, 'lambda_max': 1.0}\n    ]\n\n    irls_params = {\n        'epsilon': 1e-8,\n        'tau': 1e-8,\n        'max_iter': 100,\n        'num_stages': 8,\n        'eta': 1e-6\n    }\n    \n    results = []\n\n    for case in test_cases:\n        m, n, k = case['m'], case['n'], case['k']\n        rng = np.random.default_rng(case['seed'])\n        \n        # 生成传感矩阵 A\n        A = generate_matrix(m, n, case['matrix_type'], case['matrix_params'], rng)\n        \n        # 此值为按规定计算，但未在最终结果中使用。它用于表征问题实例。\n        # delta_k = compute_rip_constant(A, k)\n        \n        # 生成稀疏信号 x_star\n        x_star = np.zeros(n)\n        support = rng.choice(n, k, replace=False)\n        x_star[support] = rng.standard_normal(size=k)\n        \n        # 生成带噪观测值 b\n        noise = rng.normal(loc=0.0, scale=case['sigma'], size=m)\n        b = A @ x_star + noise\n        \n        # 运行 p-连续化\n        x_p = run_p_continuation(A, b, case['p_min'], case['lambda_min'], irls_params)\n        err_p = np.linalg.norm(x_p - x_star)\n        \n        # 运行 lambda-连续化\n        x_lambda = run_lambda_continuation(A, b, case['p_min'], case['lambda_min'], case['lambda_max'], irls_params)\n        err_lambda = np.linalg.norm(x_lambda - x_star)\n        \n        # 比较结果\n        eta = irls_params['eta']\n        if err_p  err_lambda - eta:\n            result = 1\n        elif err_lambda  err_p - eta:\n            result = -1\n        else:\n            result = 0\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3454798"}]}