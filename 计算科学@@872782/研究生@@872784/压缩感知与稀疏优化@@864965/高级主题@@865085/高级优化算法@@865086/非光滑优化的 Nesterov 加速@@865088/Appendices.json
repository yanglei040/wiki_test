{"hands_on_practices": [{"introduction": "在实现像FISTA这样复杂的算法之前，理解其工作原理至关重要。本练习旨在揭示一个关键点：为何不能简单地将动量方法应用于非光滑问题。通过分析一个忽略了近端步骤的朴素外推方案的发散行为，您将深刻体会到近端算子在保证加速方法收敛性中的根本作用。", "problem": "考虑压缩感知和稀疏优化中的一个核心复合凸优化问题：最小化函数 $F(x) = f(x) + g(x)$，其中 $f$ 具有 $L$-利普希茨连续梯度，$g$ 是一个正常、闭、凸但通常非光滑的正则化项。一个典型的实例是一维 Lasso 目标函数，其中 $f(x) = \\tfrac{1}{2} L x^{2}$（因此 $\\nabla f(x) = L x$），以及 $g(x) = \\lambda |x|$。在此示例中，对于任何 $\\lambda > 0$，极小点都是 $x^{\\star} = 0$。快速迭代收缩阈值算法 (Fast Iterative Shrinkage-Thresholding Algorithm, FISTA) 使用一个外推步，后跟一个考虑了 $g$ 的复合近端步。相比之下，一种应用外推但省略近端步的朴素加速方法，会将目标函数视为 $g$ 不存在，并仅对 $f$ 执行加速梯度步。\n\n分析以下完全忽略 $g$ 的朴素外推方案：\n- 固定 $L = 1$，$\\lambda > 0$，一个常数外推参数 $\\beta = 2$，以及步长 $t = 0.1$（满足 $t \\leq 1/L$）。\n- 定义外推点 $y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$。\n- 通过仅针对光滑梯度的规则进行更新 $x_{k+1} = y_{k} - t \\nabla f(y_{k})$。\n- 初始化为 $x_{-1} = 0$ 和 $x_{0} = 1$。\n\n从 $L$-利普希茨梯度、凸性和近端算子的基础定义出发，推导该朴素方案所引出的线性递推关系，并计算控制状态演化 $(x_{k}, x_{k-1})$ 的相关 $2 \\times 2$ 伴随矩阵的谱半径。利用此计算证明，即使步长 $t \\leq 1/L$，朴素外推也可能发散，从而证明对于像 $\\ell_{1}$ 范数这样的非光滑正则化项，在加速方法中使用复合近端结构的必要性。您的最终答案必须是谱半径这个精确的实数。不要对答案进行四舍五入。", "solution": "该问题要求分析一种应用于复合优化问题的朴素加速梯度法。问题陈述的有效性已得到确认，因为它是科学有据、适定且客观的。我们接下来推导解决方案。\n\n需要最小化的目标函数是 $F(x) = f(x) + g(x)$，其中 $f(x)$ 具有 $L$-利普希茨连续梯度，而 $g(x)$ 是一个非光滑正则化项。所考虑的具体实例是一维 Lasso 问题，其中 $f(x) = \\frac{1}{2} L x^2$ 且 $g(x) = \\lambda |x|$。问题指定了以下常量：$L=1$，$\\lambda > 0$，一个常数外推参数 $\\beta = 2$，以及步长 $t = 0.1$。步长条件 $t \\leq \\frac{1}{L}$ 得到满足，因为 $0.1 \\leq \\frac{1}{1}$。\n\n该朴素方案忽略了非光滑项 $g(x)$，并包括以下步骤：\n1. 外推：$y_{k} = x_{k} + \\beta (x_{k} - x_{k-1})$\n2. 仅对 $f$ 进行梯度更新：$x_{k+1} = y_{k} - t \\nabla f(y_{k})$\n\n对于给定的函数 $f(x) = \\frac{1}{2} L x^2$ 和 $L=1$，我们有 $f(x) = \\frac{1}{2} x^2$。$f$ 的梯度是 $\\nabla f(x) = x$。因此，梯度在点 $y_k$ 的计算结果为 $\\nabla f(y_k) = y_k$。\n\n现在我们可以将给定的参数和表达式代入迭代方案中。\n当 $\\beta=2$ 时，外推步骤为：\n$$y_{k} = x_{k} + 2(x_{k} - x_{k-1}) = 3x_k - 2x_{k-1}$$\n\n当 $t=0.1$ 且 $\\nabla f(y_k) = y_k$ 时，更新步骤为：\n$$x_{k+1} = y_{k} - 0.1 y_{k} = (1 - 0.1) y_{k} = 0.9 y_{k}$$\n\n结合这两个方程，我们可以为序列 $\\{x_k\\}$ 建立一个线性递推关系：\n$$x_{k+1} = 0.9 (3x_k - 2x_{k-1})$$\n$$x_{k+1} = 2.7 x_k - 1.8 x_{k-1}$$\n\n这是一个具有常系数的二阶齐次线性递推关系。为分析其动力学，我们可以将其表示为状态空间形式。设第 $k$ 次迭代的状态向量为 $v_k = \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}$。状态转移由 $v_{k+1} = M v_k$ 给出，其中 $M$ 是伴随矩阵。\n下一个状态向量是 $v_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix}$。我们可以将该系统写作：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ x_k \\end{pmatrix} = \\begin{pmatrix} 2.7 & -1.8 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_k \\\\ x_{k-1} \\end{pmatrix}\n$$\n因此，控制状态演化的伴随矩阵是：\n$$M = \\begin{pmatrix} 2.7 & -1.8 \\\\ 1 & 0 \\end{pmatrix}$$\n\n系统的稳定性由 $M$ 的谱半径决定，记作 $\\rho(M)$，它是其特征值绝对值的最大值。为求特征值，我们求解特征方程 $\\det(M - \\zeta I) = 0$，其中 $I$ 是单位矩阵，$\\zeta$ 代表一个特征值。\n$$\\det \\begin{pmatrix} 2.7 - \\zeta & -1.8 \\\\ 1 & 0 - \\zeta \\end{pmatrix} = 0$$\n$$(2.7 - \\zeta)(-\\zeta) - (-1.8)(1) = 0$$\n$$-2.7\\zeta + \\zeta^2 + 1.8 = 0$$\n$$\\zeta^2 - 2.7\\zeta + 1.8 = 0$$\n\n我们使用二次求根公式 $\\zeta = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 求解这个关于 $\\zeta$ 的二次方程，其中 $a=1$，$b=-2.7$，$c=1.8$。\n$$\\zeta = \\frac{-(-2.7) \\pm \\sqrt{(-2.7)^2 - 4(1)(1.8)}}{2(1)}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{7.29 - 7.2}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm \\sqrt{0.09}}{2}$$\n$$\\zeta = \\frac{2.7 \\pm 0.3}{2}$$\n\n两个特征值是：\n$$\\zeta_1 = \\frac{2.7 + 0.3}{2} = \\frac{3.0}{2} = 1.5$$\n$$\\zeta_2 = \\frac{2.7 - 0.3}{2} = \\frac{2.4}{2} = 1.2$$\n\n谱半径 $\\rho(M)$ 是特征值模的最大值：\n$$\\rho(M) = \\max(|\\zeta_1|, |\\zeta_2|) = \\max(|1.5|, |1.2|) = 1.5$$\n\n由于谱半径 $\\rho(M) = 1.5$ 大于 $1$，该迭代方案是不稳定的。对于给定的非零初始化（$x_{-1}=0, x_0=1$），状态向量的范数 $\\|v_k\\|$ 将呈指数级增长，因此迭代序列 $\\{x_k\\}$ 将会发散。这表明，在没有对非光滑项 $g(x)$ 进行相应近端步的情况下应用外推，可能会导致发散，即使步长 $t$ 的选择足够小，能够保证标准梯度下降法在光滑部分 $f(x)$ 上收敛。这证明了像 FISTA 这样的算法中固有的复合近端结构的必要性，这些算法旨在处理非光滑正则化项并确保收敛。", "answer": "$$\\boxed{1.5}$$", "id": "3461182"}, {"introduction": "在理解了算法的理论基础之后，下一步是构建一个实用且稳健的求解器。这项综合性编程任务将指导您完成针对LASSO问题的快速迭代软阈值算法（FISTA）的实现。您将整合Nesterov动量、用于自适应步长的回溯线搜索以及基于目标函数的重启规则，从而掌握开发一个功能完备的稀疏优化求解器的核心技能。", "problem": "本题要求您为一种在压缩感知和稀疏优化中经典的复合凸目标函数，推导并实现一种带有线搜索和重启策略的加速近端梯度法。考虑最小化一个形式为 $F(x) = f(x) + g(x)$ 的复合函数，其中 $f(x)$ 是一个具有 Lipschitz 连续梯度的光滑凸函数，而 $g(x)$ 是一个其近端算子可被高效计算的真闭凸函数。在本作业中，您将专注于带有 $\\ell_{1}$ 范数正则化项的最小二乘数据保真问题，即\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda \\ge 0$。\n\n从以下基本要素出发：\n- $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 的梯度为 $\\nabla f(x) = A^{\\top} (A x - b)$，并且是 Lipschitz 连续的，其 Lipschitz 常数 $L_{f}$ 等于 $A$ 的谱范数的平方，即 $L_{f} = \\|A\\|_{2}^{2}$。\n- 缩放后的 $\\ell_{1}$ 范数 $g(x)=\\lambda \\|x\\|_{1}$ 在点 $z$ 处，步长为 $\\alpha>0$ 时的近端算子由逐分量软阈值给出，即 $\\operatorname{prox}_{\\alpha g}(z) = \\mathcal{S}_{\\alpha \\lambda}(z)$，其中 $\\left(\\mathcal{S}_{\\tau}(z)\\right)_{i} = \\operatorname{sign}(z_{i}) \\max\\{|z_{i}| - \\tau, 0\\}$。\n- Nesterov 加速可通过一个由标量序列 $\\{t_{k}\\}_{k \\ge 0}$ 定义的动量序列和一个由迭代点 $x_{k}$ 构建的外推步骤 $y_{k}$，来对近端梯度法中的迭代点进行外推。\n- 回溯线搜索可以在给定当前点 $y$、试验点 $x$ 和候选 Lipschitz 常数 $L$ 的情况下，强制 $f(x)$ 满足以下二次上界条件：\n$$\nf(x) \\le f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{L}{2}\\|x - y\\|_{2}^{2}.\n$$\n- 当目标函数值增加时，可以使用自适应重启条件来重置加速过程。\n\n您的任务是：\n- 基于以上基本要素，为带有 Nesterov 动量、针对局部 Lipschitz 常数的回溯线搜索以及基于目标函数的自适应重启规则的加速近端梯度法，推导出一套完整的更新规则。\n- 将推导出的算法实现为一个程序，用于解决所提供测试套件中的稀疏最小二乘问题。您的实现必须计算梯度、近端算子，强制执行上述回溯不等式，更新 Nesterov 动量，并在重启规则能改善收敛稳定性时应用该规则。\n\n程序输入和随机性：\n- 程序必须是自包含的，且不得读取任何用户输入。当需要随机性时，请使用固定的随机种子以确保结果是确定性的。\n\n停止规则：\n- 使用一个结合了相对迭代变化阈值和适用于复合优化的近端梯度映射条件的停止规则。设 $\\epsilon$ 为一个小的容差。迭代变化测试可基于 $\\|x_{k+1} - x_{k}\\|_{2}/\\max\\{1,\\|x_{k}\\|_{2}\\} \\le \\epsilon$。在外推点 $y$ 处，步长为 $L$ 的近端梯度映射为 $G_{L}(y) = L\\left(y - \\operatorname{prox}_{g/L}\\left(y - \\nabla f(y)/L\\right)\\right)$。使用无穷范数 $\\|G_{L}(y)\\|_{\\infty} \\le \\epsilon'$，其中 $\\epsilon'$ 的选择应与 $\\epsilon$ 的量级相当。\n\n测试套件：\n实现您的方法并在以下四种情况下运行它。所有矩阵和向量必须严格按照规定生成。对于每种情况，最终结果必须是单一的基本类型（布尔值、整数或浮点数）。请酌情使用欧几里得范数 $\\|\\cdot\\|_{2}$ 和无穷范数 $\\|\\cdot\\|_{\\infty}$。\n\n- 情况 A（压缩感知“理想情况”）：\n  - 维度：$m = 40$, $n = 120$。\n  - 稀疏度：真实解 $x_{\\star}$ 中有 $k = 8$ 个非零元。\n  - 随机生成：设置随机种子为 $12345$。生成一个元素为独立标准正态分布的矩阵 $A$，并将其列按 $1/\\sqrt{m}$ 进行缩放。通过无放回均匀抽样选择 $k$ 个索引，并将独立的标准正态分布值赋给这些位置（其余位置为零），以生成一个 $k$-稀疏的 $x_{\\star}$。生成噪声 $e$，其元素为方差 $\\sigma^{2}$（其中 $\\sigma = 10^{-3}$）的独立正态分布，并设置 $b = A x_{\\star} + e$。\n  - 正则化：$\\lambda = 0.01 \\cdot \\|A^{\\top} b\\|_{\\infty}$。\n  - 此情况的输出：相对解误差 $\\|x_{\\text{alg}} - x_{\\star}\\|_{2} / \\max\\{1, \\|x_{\\star}\\|_{2}\\}$，以浮点数形式输出。\n\n- 情况 B（边界条件 $\\lambda = 0$）：\n  - 维度：$m = 50$, $n = 20$。\n  - 随机生成：设置随机种子为 $54321$。生成一个元素为独立标准正态分布的矩阵 $A$，并将其列按 $1/\\sqrt{m}$ 进行缩放。生成一个元素为独立标准正态分布的向量 $b$。\n  - 正则化：$\\lambda = 0$。\n  - 基准：计算最小二乘解 $x_{\\text{LS}}$，作为 $\\min_{x} \\|A x - b\\|_{2}^{2}$ 的最小范数解。\n  - 此情况的输出：相对差异 $\\|x_{\\text{alg}} - x_{\\text{LS}}\\|_{2} / \\max\\{1, \\|x_{\\text{LS}}\\|_{2}\\}$，以浮点数形式输出。\n\n- 情况 C（强正则化的边缘情况）：\n  - 维度：$m = 30$, $n = 50$。\n  - 随机生成：设置随机种子为 $11111$。生成一个元素为独立标准正态分布的矩阵 $A$，并将其列按 $1/\\sqrt{m}$ 进行缩放。生成一个元素为独立标准正态分布的向量 $b$。\n  - 正则化：$\\lambda = 100 \\cdot \\|A^{\\top} b\\|_{\\infty}$。\n  - 此情况的输出：估计支撑集的整数基数，即满足 $|x_{\\text{alg}, i}| > 10^{-8}$ 的索引 $i$ 的数量。\n\n- 情况 D（秩亏设计矩阵）：\n  - 维度：$m = 30, n = 60$，按如下方式构造。设置随机种子为 $22222$。首先生成一个元素为独立标准正态分布的矩阵 $A_{0} \\in \\mathbb{R}^{m \\times 30}$，并将其列按 $1/\\sqrt{m}$ 进行缩放。然后通过将 $A_{0}$ 与其自身拼接来设置 $A = [A_{0} \\;\\; A_{0}]$。生成一个元素为独立标准正态分布的向量 $b$。\n  - 正则化：$\\lambda = 0.05 \\cdot \\|A^{\\top} b\\|_{\\infty}$。\n  - 此情况的输出：在 $x_{\\text{alg}}$ 处的 Karush–Kuhn–Tucker (KKT) 平稳性残差，定义为最小次梯度残差的无穷范数\n    $$\n    r(x) \\equiv \\left\\|\\nabla f(x) + \\lambda v \\right\\|_{\\infty},\n    $$\n    其中 $v \\in \\partial \\|x\\|_{1}$ 是任意次梯度。通过逐分量公式计算 $r(x)$\n    $$\n    \\left(\\nabla f(x)\\right)_{i} = \\left(A^{\\top}(A x - b)\\right)_{i}, \\quad \\text{以及} \\quad \n    r(x) = \\max\\left\\{ \\max_{i: x_{i} \\ne 0} \\left| \\left(\\nabla f(x)\\right)_{i} + \\lambda \\operatorname{sign}(x_{i}) \\right|, \\; \\max_{i: x_{i} = 0} \\max\\{0, |\\left(\\nabla f(x)\\right)_{i}| - \\lambda\\} \\right\\}。\n    $$\n    使用惯例，即“$x_{i} = 0$”通过 $|x_{i}| \\le 10^{-12}$ 进行数值检验。以浮点数形式返回 $r(x)$。\n\n实现约束：\n- 使用 Nesterov 加速以及一个在目标函数值增加时重置动量的重启规则，即如果 $F(x_{k+1}) > F(x_{k})$，则将动量标量设置为 $t_{k+1} = 1$，并将外推点设置为 $y_{k+1} = x_{k+1}$。\n- 使用回溯线搜索，从局部 Lipschitz 常数 $L$ 的当前估计值开始，并将其乘以一个因子 $\\eta > 1$，直到满足 $f$ 的二次上界条件。\n- 使用通过奇异值计算或等效的谱范数评估得到的 $L_{0} = \\|A\\|_{2}^{2}$ 来初始化步长。\n\n数值容差：\n- 最大迭代次数使用 $N_{\\max} = 10000$，迭代变化的容差使用 $\\epsilon = 10^{-8}$。选择近端梯度映射容差 $\\epsilon'$ 与 $\\epsilon$ 的数量级相同。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个情况的结果，按情况 A、情况 B、情况 C、情况 D 的顺序排列，以逗号分隔并用方括号括起来。例如，输出格式必须与 `[r_{A}, r_{B}, r_{C}, r_{D}]` 完全一致，其中 $r_{A}$、$r_{B}$ 和 $r_{D}$ 是浮点数，$r_{C}$ 是整数。不应打印任何额外文本。", "solution": "用户要求推导并实现一种针对稀疏最小二乘问题（通常称为 LASSO）的加速近端梯度法。该算法必须包含 Nesterov 风格的动量、用于步长的回溯线搜索，以及基于目标函数行为的自适应重启机制。\n\n### 1. 问题描述\n\n优化问题是最小化复合目标函数 $F(x)$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; F(x) \\equiv f(x) + g(x)\n$$\n其中\n- $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 是光滑、凸的数据保真项。其梯度为 $\\nabla f(x) = A^{\\top} (A x - b)$，Lipschitz 连续常数为 $L_f = \\|A\\|_{2}^{2}$。\n- $g(x) = \\lambda \\|x\\|_{1}$ 是凸的、非光滑的正则化项。其近端算子 $\\operatorname{prox}_{\\alpha g}(z)$ 是软阈值算子 $\\mathcal{S}_{\\alpha\\lambda}(z)$，其逐分量定义为 $(\\mathcal{S}_{\\tau}(z))_i = \\operatorname{sign}(z_i) \\max\\{|z_i| - \\tau, 0\\}$。\n\n### 2. 算法推导\n\n该算法是快速迭代收缩阈值算法（FISTA）的一种变体。我们将通过组合所需的组件，逐步构建更新规则。设迭代序列为 $\\{x_k\\}$。\n\n#### 2.1. 核心近端梯度步\n\n最小化 $F(x)$ 的基本迭代步骤是近端梯度更新。在迭代点 $y$ 处，步长为 $\\alpha > 0$ 时，下一个迭代点 $x^{+}$ 通过最小化 $f(x)$ 在 $y$ 点的二次近似加上非光滑项 $g(x)$ 来找到：\n$$\nx^{+} = \\arg\\min_{x} \\left( f(y) + \\langle \\nabla f(y), x - y \\rangle + \\frac{1}{2\\alpha}\\|x - y\\|_{2}^{2} + g(x) \\right)\n$$\n通过配方法，这等价于：\n$$\nx^{+} = \\arg\\min_{x} \\left( \\frac{1}{2\\alpha}\\|x - (y - \\alpha \\nabla f(y))\\|_{2}^{2} + g(x) \\right)\n$$\n这是由 $\\alpha$ 缩放的 $g$ 的近端算子的定义：\n$$\nx^{+} = \\operatorname{prox}_{\\alpha g}(y - \\alpha \\nabla f(y))\n$$\n在我们的问题中，步长为 $\\alpha = 1/L$，其中 $L$ 是一个局部 Lipschitz 估计，更新公式为：\n$$\nx^{+} = \\mathcal{S}_{\\lambda/L}(y - \\frac{1}{L}A^{\\top}(Ay-b))\n$$\n\n#### 2.2. Nesterov 加速\n\nNesterov 加速引入了一个“动量”项，它不是在当前迭代点 $x_k$ 处，而是在一个外推点 $y_k$ 处计算近端梯度步。迭代点 $\\{x_k\\}$、外推点 $\\{y_k\\}$ 和动量标量 $\\{t_k\\}$ 的更新序列如下：\n1.  **计算下一个迭代点**：$x_{k+1} = \\operatorname{prox}_{g/L_k}(y_k - \\frac{1}{L_k}\\nabla f(y_k))$\n2.  **更新动量标量**：$t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n3.  **为下一步进行外推**：$y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$\n\n初始条件为 $x_0 = 0$, $y_0 = x_0$, 以及 $t_0 = 1$。注意存在一些变体；我们选择一种与重启规则能很好配合的表述形式。\n\n#### 2.3. 回溯线搜索\n\n全局 Lipschitz 常数 $L_f = \\|A\\|_2^2$ 可能是对局部曲率的一个悲观高估，导致收敛缓慢。回溯线搜索在每次迭代中自适应地寻找一个合适的局部常数 $L_k$。从 $L$ 的一个初始猜测（例如，上一步的 $L_{k-1}$）开始，我们检查对于从 $y_k$ 计算出的候选点 $x_{k+1}$，是否满足以下关于 $f$ 的二次上界：\n$$\nf(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L_k}{2}\\|x_{k+1} - y_k\\|_{2}^{2}\n$$\n如果条件不满足，我们将 $L_k$ 乘以一个因子 $\\eta > 1$（例如 $\\eta=2$）并重新计算 $x_{k+1}$，直到条件满足为止。这确保了目标函数光滑部分的充分下降。\n\n#### 2.4. 自适应重启\n\nNesterov 加速不是一种下降方法；目标函数 $F(x_k)$ 不保证单调递减。当目标函数值增加时（即 $F(x_{k+1}) > F(x_k)$），这表明动量过于激进而越过了最小值点。自适应重启规则通过重置动量来处理这种情况。我们按照规定实现该规则：如果 $F(x_{k+1}) > F(x_k)$:\n- 为下一步重置动量标量：$t_{k+1} = 1$。\n- 为下一步重置外推点：$y_{k+1} = x_{k+1}$。\n\n### 3. 完整算法\n\n结合这些组件，我们得到以下算法。\n\n**初始化**：\n- 设置 $k=0$, $x_0 = 0 \\in \\mathbb{R}^n$, $y_0 = x_0$, $t_0=1$。\n- 设置最大迭代次数 $N_{\\max}$，容差 $\\epsilon, \\epsilon'$。\n- 设置回溯因子 $\\eta > 1$。\n- 计算初始 Lipschitz 估计 $L_0 = \\|A\\|_2^2$。\n- 设置 $F_{-1} = \\infty$。\n\n**主循环（$k = 0, 1, \\dots, N_{\\max}-1$）**：\n1.  令 $x_{\\text{prev}} = x_k$, $t_{\\text{prev}} = t_k$。\n2.  **回溯线搜索**：\n    a. 初始化试验值 $L = L_k / \\eta$。\n    b. 重复更新 $L \\leftarrow \\eta L$ 直到条件\n       $f(x_{k+1}) \\le f(y_k) + \\langle \\nabla f(y_k), x_{k+1} - y_k \\rangle + \\frac{L}{2}\\|x_{k+1} - y_k\\|_{2}^{2}$\n       被满足，其中 $x_{k+1} = \\mathcal{S}_{\\lambda/L}(y_k - \\frac{1}{L}\\nabla f(y_k))$。\n    c. 设置 $L_{k+1} = L$。\n3.  **检查停止准则**：\n    a. 相对变化：$\\delta_x = \\|x_{k+1} - x_k\\|_{2} / \\max\\{1, \\|x_k\\|_{2}\\}$。\n    b. 近端梯度平稳性：$\\|G_L(y_k)\\|_\\infty = \\|L(y_k - x_{k+1})\\|_\\infty$。\n    c. 如果 $\\delta_x \\le \\epsilon$ 且 $\\|G_L(y_k)\\|_\\infty \\le \\epsilon'$，终止并返回 $x_{k+1}$。\n4.  **为下一次迭代更新（带重启）**：\n    a. 计算目标函数值 $F_k = F(x_k)$ 和 $F_{k+1} = F(x_{k+1})$。\n    b. **如果** $F_{k+1} > F_k$:\n        i.  $t_{k+1} = 1$。\n        ii. $y_{k+1} = x_{k+1}$。\n    c. **否则**（不重启）：\n        i.  $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$。\n        ii. $y_{k+1} = x_{k+1} + \\frac{t_k-1}{t_{k+1}}(x_{k+1} - x_k)$。\n5.  将 k 增加 1，$k \\leftarrow k+1$。\n\n这就为所要求的算法定义了一套完整的更新规则。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to derive, implement, and test an accelerated proximal gradient method\n    with line search and restart for sparse least-squares problems.\n    \"\"\"\n\n    def fista_with_restart(A, b, lambda_val, max_iter, tol, tol_prox_grad):\n        \"\"\"\n        Implements the FISTA algorithm with backtracking line search and adaptive restart\n        for the LASSO problem: min 0.5*||Ax-b||^2 + lambda*||x||_1.\n\n        Args:\n            A (np.ndarray): The design matrix.\n            b (np.ndarray): The measurement vector.\n            lambda_val (float): The regularization parameter.\n            max_iter (int): Maximum number of iterations.\n            tol (float): Tolerance for relative iterate change.\n            tol_prox_grad (float): Tolerance for the proximal gradient norm.\n\n        Returns:\n            np.ndarray: The optimized solution vector x.\n        \"\"\"\n        m, n = A.shape\n        eta = 2.0\n\n        def f(x_vec, A_mat, b_vec):\n            return 0.5 * np.linalg.norm(A_mat @ x_vec - b_vec)**2\n\n        def g(x_vec, lam):\n            return lam * np.linalg.norm(x_vec, 1)\n\n        def soft_threshold(z, tau):\n            return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n        # Initialization\n        x_curr = np.zeros(n)\n        y_curr = np.zeros(n)\n        t_curr = 1.0\n        \n        # Initial Lipschitz constant estimate from the spectral norm of A.\n        L = np.linalg.norm(A, 2)**2\n\n        # Storing the objective value of the previous iterate for the restart condition.\n        F_x_prev = f(x_curr, A, b) + g(x_curr, lambda_val)\n\n        for k in range(max_iter):\n            x_prev = x_curr\n            t_prev = t_curr\n\n            # Backtracking line search\n            L_trial = L / eta  # Start with a smaller L for efficiency\n            while True:\n                grad_y = A.T @ (A @ y_curr - b)\n                z = y_curr - grad_y / L_trial\n                x_curr = soft_threshold(z, lambda_val / L_trial)\n                \n                f_x = f(x_curr, A, b)\n                f_y = f(y_curr, A, b)\n                \n                quadratic_approx = f_y + np.dot(grad_y, x_curr - y_curr) + (L_trial / 2.0) * np.linalg.norm(x_curr - y_curr)**2\n                \n                if f_x <= quadratic_approx:\n                    L = L_trial\n                    break\n                else:\n                    L_trial *= eta\n            \n            # Stopping conditions\n            rel_change = np.linalg.norm(x_curr - x_prev) / max(1.0, np.linalg.norm(x_prev))\n            prox_grad_norm = L * np.linalg.norm(y_curr - x_curr, np.inf)\n\n            if k > 0 and rel_change <= tol and prox_grad_norm <= tol_prox_grad:\n                break\n            \n            F_x_curr = f_x + g(x_curr, lambda_val)\n            \n            # Restart check and momentum update for the next iteration\n            if k > 0 and F_x_curr > F_x_prev:\n                t_curr = 1.0\n                y_next = x_curr\n            else:\n                t_curr = (1.0 + np.sqrt(1.0 + 4.0 * t_prev**2)) / 2.0\n                y_next = x_curr + ((t_prev - 1.0) / t_curr) * (x_curr - x_prev)\n\n            # Update state for next iteration\n            y_curr = y_next\n            F_x_prev = F_x_curr\n            \n        return x_curr\n\n    # General parameters\n    MAX_ITER = 10000\n    TOL = 1e-8\n    results = []\n    \n    # --- Case A: Compressed sensing \"happy path\" ---\n    m_A, n_A, k_sparse_A = 40, 120, 8\n    rng_A = np.random.default_rng(12345)\n    A_A = rng_A.standard_normal((m_A, n_A)) / np.sqrt(m_A)\n    x_star_A = np.zeros(n_A)\n    support_A = rng_A.choice(n_A, k_sparse_A, replace=False)\n    x_star_A[support_A] = rng_A.standard_normal(k_sparse_A)\n    e_A = rng_A.normal(0, 1e-3, m_A)\n    b_A = A_A @ x_star_A + e_A\n    lambda_A = 0.01 * np.linalg.norm(A_A.T @ b_A, np.inf)\n    \n    x_alg_A = fista_with_restart(A_A, b_A, lambda_A, MAX_ITER, TOL, TOL)\n    rel_err_A = np.linalg.norm(x_alg_A - x_star_A) / max(1.0, np.linalg.norm(x_star_A))\n    results.append(rel_err_A)\n\n    # --- Case B: Boundary condition lambda = 0 ---\n    m_B, n_B = 50, 20\n    rng_B = np.random.default_rng(54321)\n    A_B = rng_B.standard_normal((m_B, n_B)) / np.sqrt(m_B)\n    b_B = rng_B.standard_normal(m_B)\n    lambda_B = 0.0\n    \n    x_alg_B = fista_with_restart(A_B, b_B, lambda_B, MAX_ITER, TOL, TOL)\n    x_ls_B, _, _, _ = np.linalg.lstsq(A_B, b_B, rcond=None)\n    rel_discrepancy_B = np.linalg.norm(x_alg_B - x_ls_B) / max(1.0, np.linalg.norm(x_ls_B))\n    results.append(rel_discrepancy_B)\n\n    # --- Case C: Very strong regularization ---\n    m_C, n_C = 30, 50\n    rng_C = np.random.default_rng(11111)\n    A_C = rng_C.standard_normal((m_C, n_C)) / np.sqrt(m_C)\n    b_C = rng_C.standard_normal(m_C)\n    lambda_C = 100 * np.linalg.norm(A_C.T @ b_C, np.inf)\n    \n    x_alg_C = fista_with_restart(A_C, b_C, lambda_C, MAX_ITER, TOL, TOL)\n    cardinality_C = np.sum(np.abs(x_alg_C) > 1e-8)\n    results.append(cardinality_C)\n\n    # --- Case D: Rank-deficient design matrix ---\n    m_D, n_half_D = 30, 30\n    rng_D = np.random.default_rng(22222)\n    A0_D = rng_D.standard_normal((m_D, n_half_D)) / np.sqrt(m_D)\n    A_D = np.hstack([A0_D, A0_D])\n    b_D = rng_D.standard_normal(m_D)\n    lambda_D = 0.05 * np.linalg.norm(A_D.T @ b_D, np.inf)\n\n    x_alg_D = fista_with_restart(A_D, b_D, lambda_D, MAX_ITER, TOL, TOL)\n    grad_f_D = A_D.T @ (A_D @ x_alg_D - b_D)\n    \n    tol_kkt_zero = 1e-12\n    is_zero = np.abs(x_alg_D) <= tol_kkt_zero\n    is_nonzero = ~is_zero\n    \n    res_nonzero = np.abs(grad_f_D[is_nonzero] + lambda_D * np.sign(x_alg_D[is_nonzero]))\n    res_zero = np.maximum(0, np.abs(grad_f_D[is_zero]) - lambda_D)\n    \n    max_res_nonzero = np.max(res_nonzero) if res_nonzero.size > 0 else 0.0\n    max_res_zero = np.max(res_zero) if res_zero.size > 0 else 0.0\n    \n    kkt_residual_D = np.max([max_res_nonzero, max_res_zero])\n    results.append(kkt_residual_D)\n\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```", "id": "3461192"}, {"introduction": "一个基本的FISTA实现功能强大，但通过更精巧的技术可以进一步提升其性能。本练习将带您探索FISTA算法的高级自适应重启策略，这超越了简单的目标函数监控。您将实现并检验基于原始-对偶间隙和梯度对齐性的重启准则，并评估它们在不同条件下（与受限等距性质RIP相关）对支撑集恢复速度的影响，从而将算法调优与压缩感知的核心科学问题联系起来。", "problem": "考虑被称为最小绝对收缩和选择算子 (LASSO) 的复合凸优化问题：最小化函数 $F(x) \\triangleq f(x) + \\lambda \\lVert x \\rVert_{1}$，其中 $f(x) \\triangleq \\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2}$，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda \\in \\mathbb{R}_{+}$。$f$ 的梯度为 $\\nabla f(x) = A^{\\top} (A x - b)$，该梯度是 Lipschitz 连续的，其常数 $L \\geq \\lVert A \\rVert_{2}^{2}$。快速迭代收缩阈值算法 (FISTA) 是一种加速的近端梯度法，它使用 Nesterov 加速来对此类复合目标进行非光滑优化。当加速变得有害时，通常会采用重启机制来恢复快速收敛状态。\n\n您将为 FISTA 设计并测试一种基于 LASSO 的原始-对偶间隙的自适应重启策略，并评估由梯度内积条件触发的额外重启是否在限制等距性质 (RIP) 下与更快的支撑恢复相关。仅使用纯粹的数学和算法构造；不涉及物理单位。\n\n使用的定义：\n- 最小绝对收缩和选择算子 (LASSO)：最小化 $F(x) = \\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}$。\n- 快速迭代收缩阈值算法 (FISTA)：一种用于复合凸最小化的加速近端梯度法，使用步长为 $1/L$ 的 $\\ell_{1}$ 范数的近端算子（软阈值）。\n- 限制等距性质 (RIP)：传感矩阵 $A$ 的一种结构性质，使得对于稀疏向量 $x$，$A$ 的作用近似保持其 $\\ell_{2}$ 范数。您不需要计算 RIP 常数；相反，您将生成具有与近似 RIP 相关的常见性质（例如，高斯分布并进行列归一化）的矩阵，以及一个通过增加互相关性来近似偏离 RIP 的情况。\n- LASSO 的原始-对偶间隙：将 $f(x) = g(Ax)$ 视为 $g(z) = \\tfrac{1}{2} \\lVert z - b \\rVert_{2}^{2}$ 和 $h(x) = \\lambda \\lVert x \\rVert_{1}$。凸共轭满足 $g^{\\ast}(y) = \\tfrac{1}{2} \\lVert y \\rVert_{2}^{2} + b^{\\top} y$ 和 $h^{\\ast}(s) = \\iota_{\\lVert s \\rVert_{\\infty} \\leq \\lambda}(s)$，其中 $\\iota_{C}$ 是凸集 $C$ 的指示函数。一个对偶可行点 $y$ 必须满足 $\\lVert A^{\\top} y \\rVert_{\\infty} \\leq \\lambda$。给定一个原始迭代点 $x$，可以通过缩放残差 $r(x) \\triangleq A x - b$ 来构造一个对偶可行点 $y(x)$，即 $y(x) \\triangleq \\theta(x) \\, r(x)$，其中 $\\theta(x) \\triangleq \\min\\!\\left\\{ 1, \\, \\dfrac{\\lambda}{\\lVert A^{\\top} r(x) \\rVert_{\\infty}} \\right\\}$。原始-对偶间隙即为 $G(x) \\triangleq \\left(\\tfrac{1}{2} \\lVert A x - b \\rVert_{2}^{2} + \\lambda \\lVert x \\rVert_{1}\\right) - \\left(-\\tfrac{1}{2} \\lVert y(x) \\rVert_{2}^{2} - b^{\\top} y(x)\\right)$，该值是非负的。\n\n您的任务：\n- 实现具有两种重启触发器的 FISTA：\n  - 基于间隙的自适应重启：如果原始-对偶间隙 $G(x_{k})$ 相对于 $G(x_{k-1})$ 严格增加（允许一个可忽略的数值容差），则通过将加速参数设置为其初始值并将外推点设置为当前迭代点来重置动量。\n  - 梯度内积重启：如果 Nesterov 加速导致了由条件 $\\langle x_{k} - x_{k-1}, \\nabla f(x_{k}) - \\nabla f(x_{k-1}) \\rangle > 0$ 所指示的错位，则如上所述重置动量。\n- 实现两种配置来解决每个实例：\n  - 配置 A：仅激活基于间隙的自适应重启。\n  - 配置 B：同时激活基于间隙和梯度内积的重启。\n- 将支撑恢复定义为恢复真实稀疏向量 $x^{\\star}$ 的精确支撑，即索引集相等 $\\operatorname{supp}(x_{k}) = \\operatorname{supp}(x^{\\star})$，其中 $\\operatorname{supp}(x) \\triangleq \\{ i : |x_{i}| > \\tau \\}$，$\\tau$ 是一个小的阈值。测量直到首次实现精确支撑恢复所需的迭代次数。如果在最大迭代次数 $N_{\\max}$ 内未实现精确恢复，则报告 $N_{\\max} + 1$。\n\n起始的算法基础：\n- 从 $y_{k}$ 使用步长 $1/L$ 计算 $x_{k+1}$ 的近端梯度步：$x_{k+1} = \\operatorname{soft}(y_{k} - \\tfrac{1}{L} \\nabla f(y_{k}), \\tfrac{\\lambda}{L})$，其中 $\\operatorname{soft}(v, \\tau)$ 按元素应用软阈值 $\\operatorname{soft}(v_{i}, \\tau) \\triangleq \\operatorname{sign}(v_{i}) \\max\\{ |v_{i}| - \\tau, 0 \\}$。\n- 使用标准 FISTA 参数化的 Nesterov 外推更新：$t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_{k}^{2}}}{2}$ 和 $y_{k+1} = x_{k+1} + \\tfrac{t_{k} - 1}{t_{k+1}} (x_{k+1} - x_{k})$，除非触发重启，此时设置 $t_{k+1} = 1$ 和 $y_{k+1} = x_{k+1}$。\n- Lipschitz 常数选择：使用 $L = \\lVert A \\rVert_{2}^{2}$，其中 $\\lVert A \\rVert_{2}$ 是谱范数。\n\n测试套件：\n实现以下三个确定性测试实例。对于每个实例，生成传感矩阵 $A$、稀疏真实向量 $x^{\\star}$ 和带噪声的观测值 $b = A x^{\\star} + e$，并指定信噪比（以分贝为单位），通过 $20 \\log_{10} \\left( \\dfrac{\\lVert A x^{\\star} \\rVert_{2}}{\\lVert e \\rVert_{2}} \\right)$ 计算。在所有情况下，将 $A$ 的列归一化为单位 $\\ell_{2}$ 范数。用指定的 $\\alpha \\in (0,1)$ 定义 $\\lambda = \\alpha \\, \\lVert A^{\\top} b \\rVert_{\\infty}$。\n\n- 案例 1（近似 RIP，中等难度）：\n  - 维度：$m = 80$，$n = 200$。\n  - 稀疏度：$s = 10$。\n  - 噪声：信噪比 $40$ dB。\n  - 正则化：$\\alpha = 0.10$。\n  - 随机种子：$0$。\n  - 矩阵模型：独立高斯项，方差为 $1/m$，然后进行列归一化。\n\n- 案例 2（增加的互相关性，更难，近似偏离 RIP）：\n  - 维度：$m = 60$，$n = 200$。\n  - 稀疏度：$s = 20$。\n  - 噪声：信噪比 $30$ dB。\n  - 正则化：$\\alpha = 0.05$。\n  - 随机种子：$1$。\n  - 矩阵模型：从独立高斯项（方差为 $1/m$）开始，然后对于列 $j \\geq 1$，设置 $A_{\\cdot j} \\leftarrow \\operatorname{normalize}(A_{\\cdot j} + 0.5 \\, c)$，其中 $c$ 是相关处理前的第一列，然后重新归一化所有列。\n\n- 案例 3（较高的系统，更容易，强近似 RIP）：\n  - 维度：$m = 120$，$n = 300$。\n  - 稀疏度：$s = 20$。\n  - 噪声：信噪比 $50$ dB。\n  - 正则化：$\\alpha = 0.10$。\n  - 随机种子：$2$。\n  - 矩阵模型：独立高斯项，方差为 $1/m$，然后进行列归一化。\n\n执行细节：\n- 使用支撑阈值 $\\tau = 10^{-6}$ 和最大迭代次数 $N_{\\max} = 2000$。\n- 将 $x_{0}$ 初始化为零向量，$t_{0} = 1$。\n- 在比较连续的原始-对偶间隙以判断是否增加时，使用数值容差 $\\varepsilon = 10^{-12}$。\n- 对每个测试案例，运行配置 A 和配置 B，并记录直到精确支撑恢复的迭代次数，分别表示为 $K_{A}$ 和 $K_{B}$。\n- 对每个测试案例，如果 $K_{B} \\leq K_{A}$，则输出布尔值 $\\mathrm{True}$，否则输出 $\\mathrm{False}$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，例如，$[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$ 分别对应于案例 1、2 和 3。", "solution": "用户需要实现和评估一种应用于 LASSO 问题的快速迭代收缩阈值算法（FISTA）的自适应重启策略。解决方案将涉及生成测试案例、实现具有两种不同重启配置的 FISTA，并根据支撑恢复速度比较它们的性能。\n\n### 1. 问题表述和预备知识\n\n优化问题是 LASSO：\n$$ \\min_{x \\in \\mathbb{R}^n} F(x) \\triangleq \\underbrace{\\frac{1}{2} \\lVert Ax - b \\rVert_2^2}_{f(x)} + \\underbrace{\\lambda \\lVert x \\rVert_1}_{h(x)} $$\n其中 $f(x)$ 是一个光滑凸函数，$h(x)$ 是一个非光滑凸函数。$f(x)$ 的梯度是 $\\nabla f(x) = A^\\top(Ax-b)$，它是 Lipschitz 连续的，常数为 $L$。我们将使用 $L = \\lVert A \\rVert_2^2$，即 $A$ 的谱范数的平方。\n\nFISTA 算法通过生成一系列迭代点 $(x_k, y_k, t_k)$ 来解决此问题，起始点为 $x_0 = \\mathbf{0}$，$y_0 = x_0$ 和 $t_0 = 1$。对于迭代 $k \\geq 0$，更新规则如下：\n1.  **近端梯度步**：从外推点 $y_k$ 计算下一个迭代点 $x_{k+1}$。\n    $$ x_{k+1} = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla f(y_k)) $$\n    对于 $h(x) = \\lambda \\lVert x \\rVert_1$，近端算子是软阈值函数：\n    $$ x_{k+1} = \\operatorname{soft}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - b), \\frac{\\lambda}{L}\\right) $$\n    其中 $\\operatorname{soft}(v, \\tau)_i = \\operatorname{sign}(v_i)\\max(|v_i|-\\tau, 0)$。\n\n2.  **重启检查**：在计算 $x_{k+1}$ 后，我们检查是否需要重启。如果任一激活的条件得到满足，则触发重启。使用当前迭代点 $x_{k+1}$ 和前一个迭代点 $x_k$ 来检查这些条件。问题描述中泛指使用索引 $(k, k-1)$ 表示连续的迭代点，我们将其解释在标准的自适应 FISTA 循环的上下文中。\n    *   **基于间隙的条件**：原始-对偶间隙增加。$G(x_{k+1}) > G(x_k) + \\varepsilon$。\n    *   **基于梯度的条件**：迭代点方向的变化与梯度方向的变化一致。$\\langle x_{k+1} - x_k, \\nabla f(x_{k+1}) - \\nabla f(x_k) \\rangle > 0$。\n\n3.  **外推步**：根据重启检查的结果，我们计算下一个动量参数 $t_{k+1}$ 和下一个外推点 $y_{k+1}$。\n    *   **如果重启**：重置动量。\n        $$ t_{k+1} = 1 $$\n        $$ y_{k+1} = x_{k+1} $$\n    *   **如果不重启**：应用 Nesterov 加速。\n        $$ t_{k+1} = \\frac{1 + \\sqrt{1 + 4 t_k^2}}{2} $$\n        $$ y_{k+1} = x_{k+1} + \\frac{t_k - 1}{t_{k+1}}(x_{k+1} - x_k) $$\n\n### 2. 原始-对偶间隙计算\n\n原始-对偶间隙提供了次优性的证明。对于一个原始迭代点 $x$，我们构造一个对偶可行的变量 $y(x)$：\n$$ y(x) = \\theta(x) (Ax - b) \\quad \\text{其中} \\quad \\theta(x) = \\min\\left\\{1, \\frac{\\lambda}{\\lVert A^\\top(Ax-b) \\rVert_\\infty}\\right\\} $$\n这种构造确保了与 $y(x)$ 相关联的对偶变量 $s(x) = -A^\\top y(x)$ 满足对偶可行性条件 $\\lVert s(x) \\rVert_\\infty \\le \\lambda$。\n原始目标是 $P(x) = \\frac{1}{2}\\lVert Ax-b \\rVert_2^2 + \\lambda\\lVert x \\rVert_1$。\n在 $y(x)$ 处评估的对偶目标是 $D(y(x)) = -\\frac{1}{2}\\lVert y(x) \\rVert_2^2 - b^\\top y(x)$。\n间隙是非负量 $G(x) = P(x) - D(y(x))$。\n\n### 3. 测试数据生成\n\n对于每个测试案例，我们根据规范构造矩阵 $A$、稀疏真实向量 $x^\\star$ 和观测向量 $b$。\n1.  为了可复现性，随机数生成器被设定种子。\n2.  矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 根据指定的模型（高斯或相关高斯）生成。对于相关情况（案例 2），我们首先生成一个基础高斯矩阵 $A_{\\text{base}}$。$A_{\\text{base}}$ 的第一列成为相关向量 $c$。对于其他每一列 $j$，我们将 $0.5c$ 添加到 $A_{\\text{base}}$ 的第 $j$ 列。最后，将所得矩阵的所有列归一化为单位 $\\ell_2$ 范数。对于标准高斯情况，我们生成一个独立同分布的高斯矩阵，然后将其列归一化。\n3.  真实稀疏向量 $x^\\star \\in \\mathbb{R}^n$ 在随机选择的位置上创建 $s$ 个非零项。非零值从标准正态分布中抽取。\n4.  观测向量 $b$ 计算为 $b = Ax^\\star + e$。噪声向量 $e$ 是一个高斯向量，其范数 $\\lVert e \\rVert_2$ 被缩放以达到以分贝（dB）为单位的所需信噪比（SNR），定义为 $\\text{SNR} = 20 \\log_{10}(\\lVert Ax^\\star \\rVert_2 / \\lVert e \\rVert_2)$。因此，$\\lVert e \\rVert_2 = \\lVert Ax^\\star \\rVert_2 \\cdot 10^{-\\text{SNR}/20}$。\n5.  正则化参数 $\\lambda$ 通过 $\\lambda = \\alpha \\lVert A^\\top b \\rVert_\\infty$ 相对于数据设定。这是一个常用的启发式方法，以确保 $\\lambda$ 在一个有意义的范围内。\n\n### 4. 算法实现与评估\n\n解决方案的核心是一个实现带有可配置重启的 FISTA 算法的函数。该函数接受问题数据 $(A, b, \\lambda, L)$ 和配置参数（使用哪些重启条件）。它会一直迭代，直到当前迭代点 $x_k$ 的支撑与 $x^\\star$ 的真实支撑相匹配，或者直到达到最大迭代次数 $N_{\\max}$。迭代点的支撑定义为 $\\operatorname{supp}(x_k) = \\{ i : |x_{k,i}| > \\tau \\}$，其中 $\\tau = 10^{-6}$。\n\n主脚本将为三个测试案例中的每一个执行以下操作：\n1.  生成数据 $(A, b, x^\\star, \\lambda, L)$。\n2.  使用配置 A（仅基于间隙的重启）运行 FISTA，并记录支撑恢复的迭代次数 $K_A$。\n3.  使用配置 B（同时使用基于间隙和基于梯度的重启）运行 FISTA，并记录迭代次数 $K_B$。\n4.  比较结果并确定是否 $K_B \\leq K_A$。\n\n最终输出将是一个布尔值列表，对应于每个测试案例此比较的结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the complete test suite for FISTA with adaptive restarts.\n    \"\"\"\n    test_cases = [\n        # Case 1: Approximate RIP, moderate difficulty\n        {\n            \"m\": 80, \"n\": 200, \"s\": 10, \"snr_db\": 40, \"alpha\": 0.10, \"seed\": 0,\n            \"matrix_model\": \"gaussian\",\n        },\n        # Case 2: Increased mutual coherence, harder\n        {\n            \"m\": 60, \"n\": 200, \"s\": 20, \"snr_db\": 30, \"alpha\": 0.05, \"seed\": 1,\n            \"matrix_model\": \"correlated_gaussian\",\n        },\n        # Case 3: Taller system, easier\n        {\n            \"m\": 120, \"n\": 300, \"s\": 20, \"snr_db\": 50, \"alpha\": 0.10, \"seed\": 2,\n            \"matrix_model\": \"gaussian\"\n        },\n    ]\n\n    results = []\n    for case_params in test_cases:\n        data = generate_case_data(**case_params)\n\n        # Config A: Gap restart only\n        k_a = run_fista(data, use_gap_restart=True, use_grad_restart=False)\n\n        # Config B: Both restarts active\n        k_b = run_fista(data, use_gap_restart=True, use_grad_restart=True)\n\n        results.append(k_b <= k_a)\n\n    # The problem expects Python's boolean capitalization.\n    boolean_strings = [str(r) for r in results]\n    print(f\"[{','.join(boolean_strings)}]\")\n\n\ndef generate_case_data(m, n, s, snr_db, alpha, seed, matrix_model):\n    \"\"\"\n    Generates a deterministic test case for the LASSO problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate sensing matrix A\n    if matrix_model == \"gaussian\":\n        A = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n    elif matrix_model == \"correlated_gaussian\":\n        A_base = rng.normal(0, 1.0 / np.sqrt(m), size=(m, n))\n        c = A_base[:, 0].copy()\n        A = A_base.copy()\n        for j in range(1, n):\n            A[:, j] += 0.5 * c\n    else:\n        raise ValueError(\"Unknown matrix model\")\n    \n    A /= np.linalg.norm(A, axis=0)\n\n    # Generate sparse ground-truth vector x_star\n    x_star = np.zeros(n)\n    support_indices = rng.choice(n, s, replace=False)\n    x_star[support_indices] = rng.standard_normal(s)\n    x_star_support = set(support_indices)\n\n    # Generate observation vector b\n    Ax_star = A @ x_star\n    signal_norm = np.linalg.norm(Ax_star)\n    noise_norm = signal_norm / (10**(snr_db / 20.0))\n\n    e = rng.standard_normal(m)\n    e *= noise_norm / np.linalg.norm(e)\n    b = Ax_star + e\n\n    # Calculate regularization parameter lambda\n    lambda_ = alpha * np.linalg.norm(A.T @ b, ord=np.inf)\n\n    # Calculate Lipschitz constant L\n    L = np.linalg.norm(A, 2)**2\n\n    return {\n        \"A\": A, \"b\": b, \"lambda_\": lambda_, \"L\": L,\n        \"x_star_support\": x_star_support\n    }\n\ndef soft_threshold(v, tau):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(v) * np.maximum(np.abs(v) - tau, 0)\n\ndef calculate_primal_dual_gap(A, b, lambda_, x):\n    \"\"\"Calculates the primal-dual gap for a LASSO iterate x.\"\"\"\n    residual = A @ x - b\n    primal_obj = 0.5 * np.dot(residual, residual) + lambda_ * np.linalg.norm(x, 1)\n\n    At_residual = A.T @ residual\n    At_residual_inf_norm = np.linalg.norm(At_residual, ord=np.inf)\n    \n    theta = 1.0\n    if At_residual_inf_norm > lambda_:\n        theta = lambda_ / At_residual_inf_norm\n    \n    dual_var = theta * residual\n    dual_obj = -0.5 * np.dot(dual_var, dual_var) - np.dot(b, dual_var)\n    \n    return primal_obj - dual_obj\n\n\ndef run_fista(data, use_gap_restart, use_grad_restart):\n    \"\"\"\n    Runs FISTA with specified restart configuration.\n    \"\"\"\n    A, b, lambda_, L = data[\"A\"], data[\"b\"], data[\"lambda_\"], data[\"L\"]\n    x_star_support = data[\"x_star_support\"]\n    n = A.shape[1]\n\n    N_MAX = 2000\n    SUPPORT_THRESH = 1e-6\n    GAP_TOL = 1e-12\n\n    x_curr = np.zeros(n)\n    y_curr = x_curr.copy()\n    t_curr = 1.0\n    \n    grad_fx_curr = A.T @ (A @ x_curr - b)\n    gap_curr = calculate_primal_dual_gap(A, b, lambda_, x_curr) if use_gap_restart else 0\n\n    for k in range(N_MAX):\n        grad_fy_curr = A.T @ (A @ y_curr - b)\n        x_next = soft_threshold(y_curr - grad_fy_curr / L, lambda_ / L)\n\n        current_support = {i for i, val in enumerate(x_next) if abs(val) > SUPPORT_THRESH}\n        if current_support == x_star_support:\n            return k + 1\n\n        restart = False\n        grad_fx_next = A.T @ (A @ x_next - b)\n\n        if use_gap_restart:\n            gap_next = calculate_primal_dual_gap(A, b, lambda_, x_next)\n            if gap_next > gap_curr + GAP_TOL:\n                restart = True\n        \n        if use_grad_restart and not restart:\n            if np.dot(x_next - x_curr, grad_fx_next - grad_fx_curr) > 0:\n                restart = True\n\n        if restart:\n            t_next = 1.0\n            y_next = x_next\n        else:\n            t_next = (1.0 + np.sqrt(1.0 + 4.0 * t_curr**2)) / 2.0\n            y_next = x_next + ((t_curr - 1.0) / t_next) * (x_next - x_curr)\n        \n        x_curr, y_curr, t_curr = x_next, y_next, t_next\n        grad_fx_curr = grad_fx_next\n        if use_gap_restart:\n            gap_curr = gap_next if not restart else calculate_primal_dual_gap(A, b, lambda_, x_curr)\n\n    return N_MAX + 1\n```", "id": "3461157"}]}