{"hands_on_practices": [{"introduction": "理论是实践的基石。在编写复杂的优化算法代码之前，通过一个简单的“纸笔”练习来手动演练其核心机制是至关重要的。这个实践将引导你从第一性原理出发，为一个简单的 $\\ell_1$ 最小化问题推导其对数障碍公式的KKT（Karush-Kuhn-Tucker）条件。通过精确计算中心路径上的一个点，你将深刻理解障碍参数、原始变量和对偶变量之间是如何相互作用的 [@problem_id:3453559]。", "problem": "考虑用于稀疏恢复的等式约束凸优化问题，该问题旨在在线性约束下最小化1-范数：最小化 $\\|x\\|_{1}$，约束条件为 $A x = b$。压缩感知中的一个标准方法是引入非负分裂 $(u,v)$，其中 $x = u - v$，$u \\geq 0$，$v \\geq 0$，并将问题重写为最小化 $\\mathbf{1}^{\\top}(u+v)$，约束条件为 $A(u-v) = b$，其中 $\\mathbf{1}$ 表示全1向量。内点法引入了带有参数 $t > 0$ 的对数障碍，并考虑等式约束障碍问题：最小化 $t \\,\\mathbf{1}^{\\top}(u+v) - \\sum_{i} \\ln(u_{i}) - \\sum_{i} \\ln(v_{i})$，约束条件为 $A(u-v) = b$，定义域为 $u > 0$，$v > 0$。Karush–Kuhn–Tucker (KKT) 条件通过将拉格朗日函数的梯度设为零，并结合原始可行性条件得到。\n\n请使用精确算术（无浮点近似）。给定 $A \\in \\mathbb{R}^{1 \\times 2}$ 为 $A = [\\,1 \\ \\ 1\\,]$，$b \\in \\mathbb{R}$ 为 $b = 1$。对于障碍参数 $t = 1$，从基本原理推导障碍问题的KKT最优性条件，用它们来刻画中心路径点 $(u^{\\star}, v^{\\star}, y^{\\star})$，并精确求解唯一的拉格朗日乘子 $y^{\\star} \\in \\mathbb{R}$（与等式约束相关的对偶变量）。作为合理性检验，请符号化地验证所得的 $(u^{\\star}, v^{\\star})$ 位于内部（$u^{\\star} > 0$, $v^{\\star} > 0$）并满足 $A(u^{\\star}-v^{\\star}) = b$。\n\n请以 $y^{\\star}$ 的精确闭式解值的形式提供最终答案。不允许使用小数近似。", "solution": "该问题要求解在 $\\ell_1$ 最小化中出现的一个特定等式约束障碍问题实例的拉格朗日乘子 $y^{\\star}$ 的精确值。\n\n该问题的一般形式是最小化函数 $f(u,v) = t \\,\\mathbf{1}^{\\top}(u+v) - \\sum_{i} \\ln(u_{i}) - \\sum_{i} \\ln(v_{i})$，约束条件为 $A(u-v) = b$。目标函数的定义域施加了隐式约束 $u > 0$ 和 $v > 0$。\n\n给定的具体参数如下：\n- 矩阵 $A \\in \\mathbb{R}^{1 \\times 2}$ 为 $A = [\\,1 \\ \\ 1\\,]$。\n- 向量 $b \\in \\mathbb{R}$ 为 $b = 1$。\n- 障碍参数 $t \\in \\mathbb{R}$ 为 $t = 1$。\n- 优化变量为 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$，被分裂为 $x = u - v$，其中 $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$ 且 $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$。\n\n根据这些具体参数，当 $t=1$ 时要最小化的目标函数为：\n$$f(u,v) = (u_1 + u_2) + (v_1 + v_2) - \\ln(u_1) - \\ln(u_2) - \\ln(v_1) - \\ln(v_2)$$\n等式约束为：\n$$A(u-v) = [\\,1 \\ \\ 1\\,] \\begin{pmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\end{pmatrix} = (u_1 - v_1) + (u_2 - v_2) = 1$$\n\n为了求解这个约束优化问题，我们使用拉格朗日乘子法。拉格朗日函数 $L(u, v, y)$ 定义为：\n$$L(u_1, u_2, v_1, v_2, y) = f(u,v) + y \\left( (u_1 - v_1) + (u_2 - v_2) - 1 \\right)$$\n其中 $y \\in \\mathbb{R}$ 是与等式约束相关的拉格朗日乘子。\n\nKarush-Kuhn-Tucker (KKT) 条件指出，在最优点 $(u^{\\star}, v^{\\star}, y^{\\star})$，拉格朗日函数关于原始变量 $(u,v)$ 的梯度必须为零。我们来计算这些偏导数：\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial u_1} = 1 - \\frac{1}{u_1} + y \\\\\n\\frac{\\partial L}{\\partial u_2} = 1 - \\frac{1}{u_2} + y \\\\\n\\frac{\\partial L}{\\partial v_1} = 1 - \\frac{1}{v_1} - y \\\\\n\\frac{\\partial L}{\\partial v_2} = 1 - \\frac{1}{v_2} - y\n\\end{aligned}\n$$\n将这些导数设为零，得到KKT条件的第一部分：\n$$\n\\begin{align}\n1 - \\frac{1}{u_1} + y = 0 \\quad \\implies \\quad \\frac{1}{u_1} = 1+y \\label{eq:1} \\\\\n1 - \\frac{1}{u_2} + y = 0 \\quad \\implies \\quad \\frac{1}{u_2} = 1+y \\label{eq:2} \\\\\n1 - \\frac{1}{v_1} - y = 0 \\quad \\implies \\quad \\frac{1}{v_1} = 1-y \\label{eq:3} \\\\\n1 - \\frac{1}{v_2} - y = 0 \\quad \\implies \\quad \\frac{1}{v_2} = 1-y \\label{eq:4}\n\\end{align}\n$$\n从这些方程中，我们可以用对偶变量 $y$ 来表示原始变量 $u_1, u_2, v_1, v_2$：\n$$u_1 = u_2 = \\frac{1}{1+y}$$\n$$v_1 = v_2 = \\frac{1}{1-y}$$\n障碍问题的定义域要求 $u > 0$ 和 $v > 0$，这意味着对于 $i=1,2$，$u_i > 0$ 和 $v_i > 0$。这对 $y$ 施加了条件：\n$$1+y > 0 \\implies y > -1$$\n$$1-y > 0 \\implies y  1$$\n因此，拉格朗日乘子 $y$ 必须位于区间 $(-1, 1)$ 内。\n\n最后的KKT条件是原始可行性，即原始约束：\n$$(u_1 - v_1) + (u_2 - v_2) = 1$$\n由于 $u_1 = u_2$ 和 $v_1 = v_2$，该式简化为：\n$$2(u_1 - v_1) = 1$$\n现在我们将 $u_1$ 和 $v_1$ 关于 $y$ 的表达式代入：\n$$2 \\left( \\frac{1}{1+y} - \\frac{1}{1-y} \\right) = 1$$\n为了解出 $y$，我们合并分数：\n$$2 \\left( \\frac{(1-y) - (1+y)}{(1+y)(1-y)} \\right) = 1$$\n$$2 \\left( \\frac{-2y}{1-y^2} \\right) = 1$$\n$$\\frac{-4y}{1-y^2} = 1$$\n$$-4y = 1 - y^2$$\n整理各项得到一个关于 $y$ 的二次方程：\n$$y^2 - 4y - 1 = 0$$\n我们使用二次公式 $y = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$ 来解这个方程：\n$$y = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(-1)}}{2(1)} = \\frac{4 \\pm \\sqrt{16+4}}{2} = \\frac{4 \\pm \\sqrt{20}}{2}$$\n由于 $\\sqrt{20} = \\sqrt{4 \\times 5} = 2\\sqrt{5}$，我们有：\n$$y = \\frac{4 \\pm 2\\sqrt{5}}{2} = 2 \\pm \\sqrt{5}$$\n这给出了 $y$ 的两个可能值：$y_A = 2 + \\sqrt{5}$ 和 $y_B = 2 - \\sqrt{5}$。我们必须选择满足条件 $-1  y  1$ 的解。\n$\\sqrt{5}$ 的值约等于 $2.236$。\n- 对于 $y_A = 2 + \\sqrt{5}$：$y_A \\approx 2 + 2.236 = 4.236$。该值在区间 $(-1, 1)$ 之外，因此不是有效解。\n- 对于 $y_B = 2 - \\sqrt{5}$：$y_B \\approx 2 - 2.236 = -0.236$。该值在区间 $(-1, 1)$ 之内。\n\n因此，唯一的拉格朗日乘子是 $y^{\\star} = 2 - \\sqrt{5}$。\n\n作为合理性检验，我们验证得到的原始变量 $(u^{\\star}, v^{\\star})$ 位于可行集的内部并且满足约束。\n最优原始变量由下式给出：\n$$u^{\\star}_1 = u^{\\star}_2 = \\frac{1}{1+y^{\\star}} = \\frac{1}{1+(2-\\sqrt{5})} = \\frac{1}{3-\\sqrt{5}}$$\n分母有理化后得到：\n$$u^{\\star}_1 = u^{\\star}_2 = \\frac{1}{3-\\sqrt{5}} \\cdot \\frac{3+\\sqrt{5}}{3+\\sqrt{5}} = \\frac{3+\\sqrt{5}}{9-5} = \\frac{3+\\sqrt{5}}{4}$$\n因为 $3 > 0$ 和 $\\sqrt{5} > 0$，我们有 $u^{\\star}_1, u^{\\star}_2 > 0$。\n\n$$v^{\\star}_1 = v^{\\star}_2 = \\frac{1}{1-y^{\\star}} = \\frac{1}{1-(2-\\sqrt{5})} = \\frac{1}{\\sqrt{5}-1}$$\n分母有理化后得到：\n$$v^{\\star}_1 = v^{\\star}_2 = \\frac{1}{\\sqrt{5}-1} \\cdot \\frac{\\sqrt{5}+1}{\\sqrt{5}+1} = \\frac{\\sqrt{5}+1}{5-1} = \\frac{\\sqrt{5}+1}{4}$$\n因为 $\\sqrt{5} > 0$，我们有 $v^{\\star}_1, v^{\\star}_2 > 0$。\n点 $(u^{\\star}, v^{\\star})$ 确实位于可行域的内部。\n\n最后，我们验证等式约束 $A(u^{\\star}-v^{\\star})=b$，即 $(u^{\\star}_1 - v^{\\star}_1) + (u^{\\star}_2 - v^{\\star}_2) = 1$。\n$$u^{\\star}_1 - v^{\\star}_1 = \\frac{3+\\sqrt{5}}{4} - \\frac{\\sqrt{5}+1}{4} = \\frac{3+\\sqrt{5}-(\\sqrt{5}+1)}{4} = \\frac{2}{4} = \\frac{1}{2}$$\n因此，$(u^{\\star}_1 - v^{\\star}_1) + (u^{\\star}_2 - v^{\\star}_2) = \\frac{1}{2} + \\frac{1}{2} = 1$。\n约束得到满足。所有检验都证实了解的有效性。唯一的拉格朗日乘子是 $y^{\\star} = 2-\\sqrt{5}$。", "answer": "$$\\boxed{2 - \\sqrt{5}}$$", "id": "3453559"}, {"introduction": "基础的 $\\ell_1$ 最小化是恢复稀疏信号的强大工具，但我们可以通过引入加权方案来进一步增强其性能。这个实践将带你探索一种先进的启发式算法——重加权 $\\ell_1$ 最小化。你将实现一个原始可行的障碍法求解器，并将其嵌入一个迭代循环中，该循环通过更新权重来逐步聚焦于最可能的非零信号分量，从而获得比标准方法更稀疏、更准确的解 [@problem_id:3453577]。", "problem": "考虑在压缩感知中用于稀疏信号恢复的加权和重加权 $\\ell_1$ 最小化问题。设 $A \\in \\mathbb{R}^{m \\times n}$ 为一个测量矩阵，其中 $m  n$；设 $b \\in \\mathbb{R}^m$ 为一个测量向量，其模型为 $b = A x_\\star$，其中 $x_\\star \\in \\mathbb{R}^n$ 是一个稀疏的真实信号。一种常见的稀疏恢复凸规划是加权 $\\ell_1$ 基追踪：在满足精确可行性的约束下，最小化加权 $\\ell_1$ 范数，\n$$\n\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^n w_i |x_i| \\quad \\text{subject to} \\quad A x = b,\n$$\n其中 $w \\in \\mathbb{R}^n$ 是正权重。通过将 $x$ 分解为其正部和负部 $x = u - v$（其中 $u \\in \\mathbb{R}^n$，$v \\in \\mathbb{R}^n$，且 $u \\ge 0$，$v \\ge 0$），该问题可以重构为一个线性规划。相应的问题是\n$$\n\\min_{u, v \\in \\mathbb{R}^n} \\sum_{i=1}^n w_i (u_i + v_i) \\quad \\text{subject to} \\quad A (u - v) = b, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\n重加权 $\\ell_1$ 启发式算法通过迭代更新权重来增强稀疏性：首先将所有权重初始化为 $w_i = 1$，求解加权 $\\ell_1$ 问题得到 $x$，然后为一个小的正则化参数 $\\varepsilon > 0$ 设置新权重为 $w_i \\leftarrow 1 / (|x_i| + \\varepsilon)$，并再次求解。此过程重复固定的次数。\n\n设计并实现一个使用对数障碍处理不等式约束 $u \\ge 0$ 和 $v \\ge 0$ 的内点法。使用等式约束障碍规划，其障碍参数为 $\\mu > 0$，目标函数为\n$$\n\\phi_\\mu(u, v) = \\sum_{i=1}^n w_i (u_i + v_i) - \\mu \\sum_{i=1}^n \\left( \\log u_i + \\log v_i \\right),\n$$\n并满足等式约束 $A (u - v) = b$。该算法应：\n- 使用一个满足 $A(u^{(0)} - v^{(0)}) = b$ 和 $u^{(0)} > 0, v^{(0)} > 0$ 的严格可行点 $(u^{(0)}, v^{(0)})$ 进行初始化。\n- 对于一串递减的障碍参数 $\\mu$（从 $\\mu_0$ 降至 $\\mu_{\\min}$），应用等式约束牛顿步，沿着可行流形 $A (u - v) = b$ 最小化 $\\phi_\\mu(u, v)$，并使用回溯线搜索来保持 $u$ 和 $v$ 的正性，同时确保障碍目标函数值的下降。\n- 在每一步牛顿迭代中，通过强制执行线性化约束 $A \\Delta u - A \\Delta v = 0$ 来保持原始可行性 $A(u - v) = b$。\n- 对于给定的 $\\mu$ 收敛后，减小 $\\mu$ 并从上一个 $(u, v)$ 点进行热启动，直到 $\\mu \\le \\mu_{\\min}$。\n- 恢复解 $x = u - v$ 并继续进行重加权。\n\n从以下基本原理开始推导：\n- 凸优化和拉格朗日/Karush–Kuhn–Tucker (KKT) 条件。\n- 用于不等式约束的对数障碍法。\n- 等式约束牛顿法和 KKT 线性系统。\n\n不要使用绕过这些原理的捷径。\n\n您的程序必须实现上述内点法和重加权 $\\ell_1$ 方案。它必须解决以下三个测试用例。所有随机元素必须使用指定的种子可复现地生成。此问题不涉及任何物理单位。\n\n测试套件：\n- 测试用例 1（正常路径）：设 $n = 30$，$m = 12$，稀疏度 $k = 5$，种子 $s = 101$。生成一个元素为独立标准正态分布的矩阵 $A \\in \\mathbb{R}^{12 \\times 30}$，然后将每列归一化，使其单位 $\\ell_2$ 范数为 1。通过（使用种子）选择一个大小为 $k$ 的随机支撑集来生成一个 $k$-稀疏的 $x_\\star \\in \\mathbb{R}^{30}$，并将其非零值设置为 $[0.5, 1.5]$ 区间内的独立均匀采样值，符号随机。设 $b = A x_\\star$。运行 $R = 3$ 次重加权迭代，其中 $\\varepsilon = 10^{-3}$，初始障碍参数 $\\mu_0 = 1$，最小障碍参数 $\\mu_{\\min} = 10^{-3}$，障碍参数缩减因子为 $0.5$。此测试的输出是一个布尔值，指示是否精确恢复了支撑集：如果满足 $|x_i| \\ge 10^{-3}$ 的索引 $i$ 与 $x_\\star$ 的真实支撑集匹配，则返回 $\\text{True}$，否则返回 $\\text{False}$。\n- 测试用例 2（边界情况）：设 $n = 25$，$m = 10$，种子 $s = 202$。按上述方式（独立标准正态分布元素，列归一化）生成矩阵 $A \\in \\mathbb{R}^{10 \\times 25}$。设 $b = 0 \\in \\mathbb{R}^{10}$。使用与测试用例 1 相同的内点参数，求解一个权重均匀（$w_i = 1$）的加权 $\\ell_1$ 问题。此测试的输出是一个布尔值，指示是否成功恢复零解：如果 $\\max_i |x_i| \\le 10^{-6}$，则返回 $\\text{True}$，否则返回 $\\text{False}$。\n- 测试用例 3（边缘情况）：设 $n = 40$，$m = 16$，稀疏度 $k = 6$，种子 $s = 303$。按照测试用例 1 的方式生成 $A \\in \\mathbb{R}^{16 \\times 40}$ 和 $x_\\star \\in \\mathbb{R}^{40}$，然后设 $b = A x_\\star$。使用相同的参数运行 $R = 3$ 次重加权迭代。此测试的输出是相对 $\\ell_2$ 误差，\n$$\n\\frac{\\|x - x_\\star\\|_2}{\\|x_\\star\\|_2},\n$$\n以浮点数形式返回。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含三个测试用例的结果，格式为用方括号括起来的逗号分隔列表（例如，$[\\text{result1}, \\text{result2}, \\text{result3}]$）。前两个条目必须是布尔值，第三个条目必须是浮点值。不应打印任何其他文本。", "solution": "任务是设计并实现一种用于稀疏信号恢复的重加权 $\\ell_1$ 最小化算法，该算法使用原始可行的内点法。推导和实现必须基于凸优化的第一性原理。\n\n核心问题是加权 $\\ell_1$ 最小化，这是一个凸但非光滑的优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\sum_{i=1}^n w_i |x_i| \\quad \\text{subject to} \\quad A x = b\n$$\n其中 $w_i > 0$ 是给定的正权重。绝对值函数 $|x_i|$ 在 $x_i=0$ 处不可微，这促使我们进行问题重构。通过引入非负变量 $u, v \\in \\mathbb{R}^n$ 使得 $x_i = u_i - v_i$ 且 $|x_i| = u_i + v_i$（通过设置 $u_i = \\max(x_i, 0)$ 和 $v_i = \\max(-x_i, 0)$ 来实现），该问题被转换为一个等价的线性规划 (LP)：\n$$\n\\min_{u, v \\in \\mathbb{R}^n} \\sum_{i=1}^n w_i (u_i + v_i) \\quad \\text{subject to} \\quad A (u - v) = b, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\n该线性规划具有线性等式约束和非负性不等式约束。我们将使用原始可行的内点法来解决此问题，具体来说是一种对数障碍法。\n\n对数障碍法将不等式约束 $u \\ge 0$ 和 $v \\ge 0$ 替换为目标函数中的一个对数项，该项由一个障碍参数 $\\mu > 0$ 控制。这将约束线性规划问题转化为一系列无约束（或在此情况下为等式约束）的问题。修改后的目标函数是：\n$$\n\\phi_\\mu(u, v) = \\sum_{i=1}^n w_i (u_i + v_i) - \\mu \\sum_{i=1}^n \\left( \\log u_i + \\log v_i \\right)\n$$\n对于固定的 $\\mu$，我们求解光滑、严格凸的等式约束问题：\n$$\n\\min_{u, v} \\phi_\\mu(u, v) \\quad \\text{subject to} \\quad A (u - v) = b\n$$\n对于给定的 $\\mu$，该问题的解位于中心路径上。当 $\\mu \\to 0$ 时，中心路径上的点收敛到原始线性规划的解。整个算法从一个相对较大的 $\\mu$ 开始，迭代地求解障碍问题，减小 $\\mu$，并重复此过程，直到 $\\mu$ 足够小。\n\n每个等式约束最小化问题都使用牛顿法求解。由于我们从一个可行点 $A(u-v)=b$ 开始并始终保持其可行性，因此这是一种等式约束牛顿法。牛顿步 $(\\Delta u, \\Delta v)$ 通过求解一个线性方程组来找到，该方程组源于问题二阶泰勒近似的驻点的 Karush-Kuhn-Tucker (KKT) 条件。设变量向量为 $z = [u^T, v^T]^T$，约束矩阵为 $C = [A, -A]$。用于搜索方向 $\\Delta z = [\\Delta u^T, \\Delta v^T]^T$ 和对偶变量（拉格朗日乘子） $\\lambda \\in \\mathbb{R}^m$ 的 KKT 系统是：\n$$\n\\begin{pmatrix} \\nabla^2 \\phi_\\mu(z)  C^T \\\\ C  0 \\end{pmatrix} \\begin{pmatrix} \\Delta z \\\\ \\lambda \\end{pmatrix} = -\\begin{pmatrix} \\nabla \\phi_\\mu(z) \\\\ Cz - b \\end{pmatrix}\n$$\n由于我们保持原始可行性 $Cz = b$，右下方的残差为零。梯度 $\\nabla \\phi_\\mu$ 和海森矩阵 $\\nabla^2 \\phi_\\mu$ 分别是：\n$$\n\\nabla \\phi_\\mu(u,v) = \\begin{pmatrix} w - \\mu u.^{-1} \\\\ w - \\mu v.^{-1} \\end{pmatrix}\n\\quad , \\quad\n\\nabla^2 \\phi_\\mu(u,v) = \\mu \\begin{pmatrix} \\text{diag}(u.^{-2})  0 \\\\ 0  \\text{diag}(v.^{-2}) \\end{pmatrix}\n$$\n其中 $u.^{-1}$ 和 $u.^{-2}$ 表示逐元素求逆和平方。KKT 系统展开为：\n\\begin{align*}\n\\mu \\, \\text{diag}(u.^{-2}) \\Delta u + (w - \\mu u.^{-1}) + A^T \\lambda = 0 \\\\\n\\mu \\, \\text{diag}(v.^{-2}) \\Delta v + (w - \\mu v.^{-1}) - A^T \\lambda = 0 \\\\\nA \\Delta u - A \\Delta v = 0\n\\end{align*}\n这是一个大小为 $(2n+m) \\times (2n+m)$ 的大型线性系统。我们使用分块消元法高效地求解它。首先，我们根据前两个方程，用对偶变量 $\\lambda$ 来表示原始步长 $\\Delta u$ 和 $\\Delta v$：\n$$\n\\Delta u = -\\frac{1}{\\mu} \\text{diag}(u.^2) \\left( (w - \\mu u.^{-1}) + A^T \\lambda \\right)\n$$\n$$\n\\Delta v = -\\frac{1}{\\mu} \\text{diag}(v.^2) \\left( (w - \\mu v.^{-1}) - A^T \\lambda \\right)\n$$\n将这些代入第三个方程 $A(\\Delta u - \\Delta v) = 0$，可以得到一个关于 $\\lambda$ 的小得多的 $m \\times m$ 系统，即舒尔补系统：\n$$\n\\left( A \\left( \\text{diag}(u.^2) + \\text{diag}(v.^2) \\right) A^T \\right) \\lambda = A \\left( \\text{diag}(v.^2)(w - \\mu v.^{-1}) - \\text{diag}(u.^2)(w - \\mu u.^{-1}) \\right)\n$$\n在求解此系统得到 $\\lambda$ 后，我们回代以求得牛顿步 $\\Delta u$ 和 $\\Delta v$。\n\n内点法的算法流程如下：\n1.  找到一个初始严格可行点 $(u^{(0)}, v^{(0)})$，满足 $A(u^{(0)}-v^{(0)})=b$ 且 $u^{(0)}, v^{(0)} > 0$。这可以通过计算 $Ax=b$ 的最小 $\\ell_2$ 范数解 $x_0$ 并为一个小的正常数 $\\delta$ 设置 $u_i = \\max(x_{0,i}, 0) + \\delta$ 和 $v_i = \\max(-x_{0,i}, 0) + \\delta$ 来实现。\n2.  将障碍参数 $\\mu$ 初始化为 $\\mu_0$。\n3.  外层循环持续进行，直到 $\\mu > \\mu_{\\min}$。循环内部：\n    a. 内部的“中心化”循环应用牛顿法。它计算牛顿步 $(\\Delta u, \\Delta v)$ 并更新 $(u,v)$ 直至收敛。内循环的收敛由牛顿减量 $\\lambda_{\\text{dec}}^2/2$ 小于一个容差（例如，$10^{-8}$）来判断。\n    b. 回溯线搜索确定一个步长 $t$，该步长既能保证 $\\phi_\\mu$ 有足够的下降（Armijo 条件，典型参数 $\\alpha=0.01$），又能保持 $u$ 和 $v$ 的严格正性。在每次回溯迭代中，步长按因子 $\\beta=0.5$ 减小。\n    c. 中心化步骤收敛后，减小障碍参数：$\\mu \\leftarrow \\mu_{\\text{factor}} \\cdot \\mu$。\n4.  算法终止时，恢复解 $x=u-v$。\n\n然后将这个内点法求解器嵌入到一个重加权方案中。对于固定的迭代次数 $R$，我们首先求解加权 $\\ell_1$ 问题得到解 $x$，然后使用规则 $w_i \\leftarrow 1 / (|x_i| + \\varepsilon)$ 更新权重。这种启发式方法通过更强地惩罚非零系数来迭代地增强解的稀疏性。我们从均匀权重 $w_i=1$（对所有 $i$）开始。", "answer": "```python\nimport numpy as np\n\ndef _interior_point_l1(A, b, w, mu0, mu_min, mu_factor, tol, max_inner_iter, alpha, beta):\n    \"\"\"\n    Solves the weighted l1 minimization problem using a primal-feasible\n    interior-point method.\n\n    This function implements the barrier method to solve:\n        min sum(w_i * |x_i|) s.t. Ax = b\n    by reformulating it as a linear program and applying Newton's method\n    to a sequence of equality-constrained problems.\n    \"\"\"\n    m, n = A.shape\n    \n    # Phase I: Find an initial strictly feasible point (u, v)\n    # Solve for the minimum L2-norm solution x0 to Ax = b\n    # x0 = A.T @ (A @ A.T)^(-1) @ b\n    try:\n        # This is faster and more stable if A @ A.T is well-conditioned\n        x0 = A.T @ np.linalg.solve(A @ A.T, b)\n    except np.linalg.LinAlgError:\n        # Fallback to least squares for singular or ill-conditioned cases\n        x0 = np.linalg.lstsq(A, b, rcond=None)[0]\n\n    # Use a delta to ensure all components of u and v are strictly positive\n    delta = 1.0\n    u = np.maximum(x0, 0) + delta\n    v = np.maximum(-x0, 0) + delta\n\n    mu = mu0\n    # Outer loop: barrier parameter reduction\n    while mu  mu_min:\n        # Inner loop: centering step (Newton's method)\n        for _ in range(max_inner_iter):\n            u_inv = 1.0 / u\n            v_inv = 1.0 / v\n\n            g_u = w - mu * u_inv\n            g_v = w - mu * v_inv\n\n            # Build and solve the KKT system (Schur complement form) for the Newton step\n            D_diag = u**2 + v**2\n            # Efficiently compute M = A @ diag(D_diag) @ A.T using broadcasting\n            M = (A * D_diag) @ A.T\n            \n            # Efficiently compute rhs_vec = A @ (diag(v**2) @ g_v - diag(u**2) @ g_u)\n            rhs_vec = A @ (v**2 * g_v - u**2 * g_u)\n            \n            try:\n                # Solve for the dual variable `nu` (lambda in the write-up)\n                nu = np.linalg.solve(M, rhs_vec)\n            except np.linalg.LinalgError:\n                nu = np.linalg.lstsq(M, rhs_vec, rcond=1e-15)[0]\n\n            # Back-substitute to find primal step directions\n            delta_u = -(u**2 * (g_u + A.T @ nu)) / mu\n            delta_v = -(v**2 * (g_v - A.T @ nu)) / mu\n\n            # Newton decrement for stopping criterion\n            lambda_sq = mu * (np.sum(delta_u**2 * u_inv**2) + np.sum(delta_v**2 * v_inv**2))\n            \n            if lambda_sq / 2.0  tol:\n                break\n\n            # Backtracking line search to find step size `t`\n            t = 1.0\n            # Ensure positivity of u_new and v_new\n            if np.any(delta_u  0):\n                t = min(t, np.min(-u[delta_u  0] / delta_u[delta_u  0]))\n            if np.any(delta_v  0):\n                t = min(t, np.min(-v[delta_v  0] / delta_v[delta_v  0]))\n            t *= 0.99 # Step back from the boundary\n\n            phi_current = np.sum(w * (u + v)) - mu * np.sum(np.log(u) + np.log(v))\n            dir_deriv = g_u.T @ delta_u + g_v.T @ delta_v\n\n            # Armijo-Goldstein condition\n            while t  1e-14:\n                u_new, v_new = u + t * delta_u, v + t * delta_v\n                if np.any(u_new = 0) or np.any(v_new = 0):\n                    t *= beta\n                    continue\n                \n                phi_new = np.sum(w * (u_new + v_new)) - mu * np.sum(np.log(u_new) + np.log(v_new))\n                if phi_new  phi_current + alpha * t * dir_deriv:\n                    break\n                t *= beta\n            else:\n                break # Step size became too small, convergence stalled\n\n            u, v = u + t * delta_u, v + t * delta_v\n\n        mu *= mu_factor\n\n    return u - v\n\n\ndef _run_reweighted_l1(A, b, R, epsilon, mu0, mu_min, mu_factor, ipm_params):\n    \"\"\"Wrapper for the reweighted l1 scheme.\"\"\"\n    _, n = A.shape\n    w = np.ones(n)\n    x = None\n    for _ in range(R):\n        x = _interior_point_l1(A, b, w, mu0, mu_min, mu_factor, **ipm_params)\n        w = 1.0 / (np.abs(x) + epsilon)\n    return x\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    # Define shared parameters for the interior-point method\n    ipm_params = {'tol': 1e-8, 'max_inner_iter': 30, 'alpha': 0.01, 'beta': 0.5}\n    mu0 = 1.0\n    mu_min = 1e-3\n    mu_factor = 0.5\n\n    results = []\n\n    # --- Test Case 1 ---\n    n1, m1, k1, s1 = 30, 12, 5, 101\n    R1, epsilon1 = 3, 1e-3\n    rng1 = np.random.default_rng(s1)\n    A1 = rng1.standard_normal((m1, n1))\n    A1 /= np.linalg.norm(A1, axis=0) # Normalize columns\n    \n    true_support1 = rng1.choice(n1, k1, replace=False)\n    x_star1 = np.zeros(n1)\n    x_star1[true_support1] = rng1.uniform(0.5, 1.5, k1) * rng1.choice([-1, 1], k1)\n    b1 = A1 @ x_star1\n    \n    x1 = _run_reweighted_l1(A1, b1, R1, epsilon1, mu0, mu_min, mu_factor, ipm_params)\n    \n    recovered_support1 = np.where(np.abs(x1) = 1e-3)[0]\n    result1 = set(recovered_support1) == set(true_support1)\n    results.append(result1)\n\n    # --- Test Case 2 ---\n    n2, m2, s2 = 25, 10, 202\n    rng2 = np.random.default_rng(s2)\n    A2 = rng2.standard_normal((m2, n2))\n    A2 /= np.linalg.norm(A2, axis=0)\n    b2 = np.zeros(m2)\n    w2 = np.ones(n2)\n    \n    x2 = _interior_point_l1(A2, b2, w2, mu0, mu_min, mu_factor, **ipm_params)\n    \n    result2 = np.max(np.abs(x2)) = 1e-6\n    results.append(result2)\n\n    # --- Test Case 3 ---\n    n3, m3, k3, s3 = 40, 16, 6, 303\n    R3, epsilon3 = 3, 1e-3\n    rng3 = np.random.default_rng(s3)\n    A3 = rng3.standard_normal((m3, n3))\n    A3 /= np.linalg.norm(A3, axis=0)\n    \n    true_support3 = rng3.choice(n3, k3, replace=False)\n    x_star3 = np.zeros(n3)\n    x_star3[true_support3] = rng3.uniform(0.5, 1.5, k3) * rng3.choice([-1, 1], k3)\n    b3 = A3 @ x_star3\n    \n    x3 = _run_reweighted_l1(A3, b3, R3, epsilon3, mu0, mu_min, mu_factor, ipm_params)\n    \n    result3 = np.linalg.norm(x3 - x_star3) / np.linalg.norm(x_star3)\n    results.append(result3)\n\n    # Format and print the final output\n    print(f\"[{results[0]},{results[1]},{results[2]}]\")\n\nsolve()\n```", "id": "3453577"}]}