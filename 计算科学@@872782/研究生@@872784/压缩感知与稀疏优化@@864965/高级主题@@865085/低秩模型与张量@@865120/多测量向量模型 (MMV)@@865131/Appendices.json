{"hands_on_practices": [{"introduction": "在求解多测量向量（MMV）模型时，许多先进的优化算法，如迭代软阈值算法（ISTA），都依赖于“近端算子”（proximal operator）这一核心工具。这个练习将引导你推导并计算促进联合稀疏性的$\\ell_{2,1}$范数的近端算子。通过解决这个基本问题，你将掌握处理MMV模型中非平滑正则化项的关键数学技巧，为理解更复杂的近端梯度方法奠定坚实的基础。", "problem": "考虑用于联合稀疏恢复的多测量向量（MMV）模型，其中未知系数矩阵 $X \\in \\mathbb{R}^{n \\times L}$ 具有行稀疏结构，数据保真度通过感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和测量值 $Y \\in \\mathbb{R}^{m \\times L}$ 进行建模。一个广泛用于估计 $X$ 的凸目标函数是二次数据项和混合 $\\ell_{2,1}$ 范数之和，其定义为 $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,\\cdot}\\|_{2}$。在邻近分裂方法中，需要求解函数 $g(X) = \\tau \\|X\\|_{2,1}$（其中 $\\tau  0$）的邻近算子。从邻近算子和混合 $\\ell_{2,1}$ 范数的定义出发，通过逐行分析优化问题并建立最优性条件，为一个给定的矩阵 $V \\in \\mathbb{R}^{n \\times L}$ 推导出 $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$ 的闭式表达式。然后将您的推导应用于特定矩阵\n$$\nV = \\begin{pmatrix}\n3  4  0  0 \\\\\n1  2  2  1 \\\\\n1  1  0  0\n\\end{pmatrix}\n$$\n其中参数 $\\tau = 2$，并计算邻近算子输出 $X = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$ 的弗罗贝尼乌斯范数的平方。请以精确的闭式表达式形式提供您的最终答案。无需四舍五入。", "solution": "该问题要求推导混合 $\\ell_{2,1}$ 范数的邻近算子，并将其应用于特定的矩阵 $V$ 和参数 $\\tau$。\n\n首先，我们建立邻近算子的形式化定义。对于一个函数 $g: \\mathbb{R}^{n \\times L} \\to \\mathbb{R}$，其在点 $V \\in \\mathbb{R}^{n \\times L}$ 处的邻近算子被定义为以下优化问题的唯一解：\n$$ \\operatorname{prox}_{g}(V) = \\arg\\min_{X \\in \\mathbb{R}^{n \\times L}} \\left( g(X) + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\n其中 $\\|\\cdot\\|_F$ 表示弗罗贝尼乌斯范数。\n\n在这个问题中，函数是 $g(X) = \\tau \\|X\\|_{2,1}$，其中 $\\tau  0$ 是一个标量参数，$\\|X\\|_{2,1}$ 是混合 $\\ell_{2,1}$ 范数，定义为矩阵 $X$ 各行欧几里得范数之和：\n$$ \\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,\\cdot}\\|_2 $$\n这里，$X_{i,\\cdot}$ 表示矩阵 $X$ 的第 $i$ 行向量。\n\n因此，函数 $g(X)$ 的邻近算子的优化问题是：\n$$ X^* = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V) = \\arg\\min_{X} \\left( \\tau \\sum_{i=1}^{n} \\|X_{i,\\cdot}\\|_2 + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\n弗罗贝尼乌斯范数的平方是其矩阵参数各行的欧几里得范数平方之和：\n$$ \\|X-V\\|_F^2 = \\sum_{i=1}^{n} \\|X_{i,\\cdot} - V_{i,\\cdot}\\|_2^2 $$\n将此分解代入目标函数，我们得到：\n$$ X^* = \\arg\\min_{X} \\sum_{i=1}^{n} \\left( \\tau \\|X_{i,\\cdot}\\|_2 + \\frac{1}{2} \\|X_{i,\\cdot} - V_{i,\\cdot}\\|_2^2 \\right) $$\n目标函数是各项之和，其中第 $i$ 项仅依赖于 $X$ 的第 $i$ 行 $X_{i,\\cdot}$。这种可分离性使我们能够通过独立地对每一行进行最小化来最小化整个函数。对每一行 $i \\in \\{1, \\dots, n\\}$，我们求解：\n$$ X_{i,\\cdot}^* = \\arg\\min_{x_i \\in \\mathbb{R}^{1 \\times L}} \\left( \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2 \\right) $$\n为简化符号，我们令 $x_i = X_{i,\\cdot}$ 和 $v_i = V_{i,\\cdot}$。这个子问题是缩放后的欧几里得范数 $\\tau \\|\\cdot\\|_2$ 的邻近算子。\n\n为了解决这个子问题，我们使用次微分理论。令 $J(x_i) = \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2$。最优性的一阶充要条件是 $0 \\in \\partial J(x_i^*)$。$J$ 的次微分是 $\\partial J(x_i) = \\tau \\partial \\|x_i\\|_2 + \\nabla \\left(\\frac{1}{2} \\|x_i - v_i\\|_2^2\\right) = \\tau \\partial \\|x_i\\|_2 + (x_i - v_i)$。\n欧几里得范数的次微分是：\n$$ \\partial \\|x_i\\|_2 = \\begin{cases} \\{ u \\in \\mathbb{R}^{1 \\times L} \\mid \\|u\\|_2 \\le 1 \\}  \\text{若 } x_i = 0 \\\\ \\{ \\frac{x_i}{\\|x_i\\|_2} \\}  \\text{若 } x_i \\ne 0 \\end{cases} $$\n我们考虑解 $x_i^*$ 的两种情况：\n\n情况1：$x_i^* \\ne 0$。\n最优性条件是 $0 = \\tau \\frac{x_i^*}{\\|x_i^*\\|_2} + x_i^* - v_i$。\n重新整理这个方程得到 $v_i = x_i^* \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right)$。这表明 $v_i$ 和 $x_i^*$ 是共线的。对两边取欧几里得范数得到 $\\|v_i\\|_2 = \\|x_i^*\\|_2 \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right) = \\|x_i^*\\|_2 + \\tau$。因此，$\\|x_i^*\\|_2 = \\|v_i\\|_2 - \\tau$。要使这是一个非零解，我们必须有 $\\|v_i\\|_2 - \\tau  0$，即 $\\|v_i\\|_2  \\tau$。如果这个条件成立，我们可以从共线性中找到 $x_i^*$：$x_i^* = \\frac{\\|x_i^*\\|_2}{\\|v_i\\|_2} v_i = \\frac{\\|v_i\\|_2 - \\tau}{\\|v_i\\|_2} v_i = \\left(1 - \\frac{\\tau}{\\|v_i\\|_2}\\right) v_i$。\n\n情况2：$x_i^* = 0$。\n最优性条件变为 $0 \\in \\tau \\partial \\|0\\|_2 + (0 - v_i)$，这意味着 $v_i \\in \\tau \\partial \\|0\\|_2$。这意味着 $v_i$ 必须属于集合 $\\{ u \\mid \\|u\\|_2 \\le \\tau \\}$，所以 $\\|v_i\\|_2 \\le \\tau$。\n\n结合这两种情况，每一行 $X_{i,\\cdot}^*$ 的闭式解是一个块软阈值操作：\n$$ X_{i,\\cdot}^* = \\begin{cases} \\left(1 - \\frac{\\tau}{\\|V_{i,\\cdot}\\|_2}\\right) V_{i,\\cdot}  \\text{若 } \\|V_{i,\\cdot}\\|_2  \\tau \\\\ 0  \\text{若 } \\|V_{i,\\cdot}\\|_2 \\le \\tau \\end{cases} $$\n这可以紧凑地写成 $X_{i,\\cdot}^* = \\max\\left(0, 1 - \\frac{\\tau}{\\|V_{i,\\cdot}\\|_2}\\right)V_{i,\\cdot}$。\n\n现在我们将此公式应用于给定的矩阵 $V$ 和参数 $\\tau = 2$：\n$$ V = \\begin{pmatrix} 3  4  0  0 \\\\ 1  2  2  1 \\\\ 1  1  0  0 \\end{pmatrix} $$\n令 $X = \\operatorname{prox}_{2 \\|\\cdot\\|_{2,1}}(V)$。我们逐行计算 $X$。\n\n对于第一行，$V_{1,\\cdot} = (3, 4, 0, 0)$：\n其 $\\ell_2$-范数是 $\\|V_{1,\\cdot}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{25} = 5$。\n由于 $\\|V_{1,\\cdot}\\|_2 = 5  \\tau = 2$，我们应用收缩公式：\n$X_{1,\\cdot} = \\left(1 - \\frac{2}{5}\\right) V_{1,\\cdot} = \\frac{3}{5} (3, 4, 0, 0)$。\n\n对于第二行，$V_{2,\\cdot} = (1, 2, 2, 1)$：\n其 $\\ell_2$-范数是 $\\|V_{2,\\cdot}\\|_2 = \\sqrt{1^2 + 2^2 + 2^2 + 1^2} = \\sqrt{10}$。\n由于 $\\|V_{2,\\cdot}\\|_2 = \\sqrt{10} \\approx 3.162  \\tau = 2$，我们再次应用收缩公式：\n$X_{2,\\cdot} = \\left(1 - \\frac{2}{\\sqrt{10}}\\right) V_{2,\\cdot}$。\n\n对于第三行，$V_{3,\\cdot} = (1, 1, 0, 0)$：\n其 $\\ell_2$-范数是 $\\|V_{3,\\cdot}\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$。\n由于 $\\|V_{3,\\cdot}\\|_2 = \\sqrt{2} \\approx 1.414 \\le \\tau = 2$，该行被置为零：\n$X_{3,\\cdot} = (0, 0, 0, 0)$。\n\n问题要求计算所得矩阵 $X$ 的弗罗贝尼乌斯范数的平方，即 $\\|X\\|_F^2 = \\sum_{i=1}^{3} \\|X_{i,\\cdot}\\|_2^2$。\n我们计算 $X$ 各行范数的平方。如果 $\\|V_{i,\\cdot}\\|_2  \\tau$，则 $\\|X_{i,\\cdot}\\|_2 = \\left(1 - \\frac{\\tau}{\\|V_{i,\\cdot}\\|_2}\\right) \\|V_{i,\\cdot}\\|_2 = \\|V_{i,\\cdot}\\|_2 - \\tau$。因此，$\\|X_{i,\\cdot}\\|_2^2 = (\\|V_{i,\\cdot}\\|_2 - \\tau)^2$。如果 $\\|V_{i,\\cdot}\\|_2 \\le \\tau$，则 $\\|X_{i,\\cdot}\\|_2^2 = 0$。\n\n对于第一行：\n$\\|X_{1,\\cdot}\\|_2^2 = (\\|V_{1,\\cdot}\\|_2 - \\tau)^2 = (5 - 2)^2 = 3^2 = 9$。\n\n对于第二行：\n$\\|X_{2,\\cdot}\\|_2^2 = (\\|V_{2,\\cdot}\\|_2 - \\tau)^2 = (\\sqrt{10} - 2)^2 = (\\sqrt{10})^2 - 2(2)\\sqrt{10} + 2^2 = 10 - 4\\sqrt{10} + 4 = 14 - 4\\sqrt{10}$。\n\n对于第三行：\n由于 $\\|V_{3,\\cdot}\\|_2 \\le \\tau$，我们有 $\\|X_{3,\\cdot}\\|_2^2 = 0$。\n\n$X$ 的弗罗贝尼乌斯范数的平方是这些值的总和：\n$\\|X\\|_F^2 = \\|X_{1,\\cdot}\\|_2^2 + \\|X_{2,\\cdot}\\|_2^2 + \\|X_{3,\\cdot}\\|_2^2 = 9 + (14 - 4\\sqrt{10}) + 0 = 23 - 4\\sqrt{10}$。", "answer": "$$\\boxed{23 - 4\\sqrt{10}}$$", "id": "3460758"}, {"introduction": "掌握了$\\ell_{2,1}$范数的近端算子之后[@problem_id:3460758]，下一步自然是将其应用到完整的优化迭代中。本练习将带你亲手完成一次近端梯度下降迭代，解决一个具体的MMV问题。你将把数据保真项的梯度下降步骤与强制稀疏性的近端映射步骤结合起来，从而清晰地理解算法是如何逐步逼近稀疏解的。", "problem": "考虑多测量向量 (MMV) 模型 $Y = A X_{\\star} + E$，其中 $Y \\in \\mathbb{R}^{m \\times L}$，$A \\in \\mathbb{R}^{m \\times n}$，并且 $X_{\\star} \\in \\mathbb{R}^{n \\times L}$ 是行稀疏的。假设估计值 $X \\in \\mathbb{R}^{n \\times L}$ 是通过最小化正则化最小二乘目标函数得到的：\n$$\nf(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2} + \\lambda \\|X\\|_{2,1},\n$$\n其中 $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_{2}$ 是混合 $\\ell_{2,1}$ 范数，它能促进 $L$ 个测量向量的联合行稀疏性。从初始值 $X^{(0)} = 0_{n \\times L}$ 开始，对 $f$ 执行一次完整的近端梯度迭代，步长为常数 $t$。该迭代包括对平滑的数据拟合项进行一次梯度步，然后应用 $\\ell_{2,1}$ 正则化项的近端算子。使用以下具体数据：\n$$\nA = \\begin{pmatrix}\n2  0  1 \\\\\n0  1  -1\n\\end{pmatrix}, \\quad\nY = \\begin{pmatrix}\n1  3 \\\\\n2  -1\n\\end{pmatrix}, \\quad\nX^{(0)} = \\begin{pmatrix}\n0  0 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix}, \\quad\nt = 0.5, \\quad \\lambda = 0.8.\n$$\n通过执行一次带有 $\\ell_{2,1}$ 收缩的完整近端梯度步，数值计算更新后的迭代值 $X^{(1)}$，并提供 $X^{(1)}$ 的元素值，四舍五入到四位有效数字。将最终的元素表示为无量纲数。", "solution": "本题要求计算一次近端梯度法迭代，以最小化目标函数 $f(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2} + \\lambda \\|X\\|_{2,1}$。该方法也被称为迭代收缩阈值算法 (ISTA)。目标函数是一个平滑可微项 $g(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2}$ 与一个非平滑凸正则化项 $h(X) = \\lambda \\|X\\|_{2,1}$ 的和。\n\n近端梯度迭代的更新规则由下式给出：\n$$\nX^{(k+1)} = \\text{prox}_{t h}(X^{(k)} - t \\nabla g(X^{(k)}))\n$$\n其中 $t$ 是步长，$\\text{prox}_{t h}$ 是函数 $t h(X)$ 的近端算子。对于 $h(X) = \\lambda \\|X\\|_{2,1}$，其近端算子为 $\\text{prox}_{t\\lambda\\|\\cdot\\|_{2,1}}$。题目要求从 $X^{(0)} = 0_{n\\times L}$ 开始计算 $X^{(1)}$。\n\n迭代过程分为两步：\n1. 对平滑项 $g(X)$ 进行一次梯度下降。\n2. 应用非平滑项 $h(X)$ 的近端算子。\n\n我们来逐步进行计算。\n\n首先，我们计算平滑项 $g(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2}$ 的梯度。弗罗贝尼乌斯范数的平方是所有元素平方的和，$\\|M\\|_F^2 = \\text{Tr}(M^T M)$。$g(X)$ 关于矩阵 $X$ 的梯度为：\n$$\n\\nabla g(X) = A^T (A X - Y)\n$$\n我们在初始点 $X^{(0)} = 0_{n \\times L}$ 处计算该梯度：\n$$\n\\nabla g(X^{(0)}) = A^T (A X^{(0)} - Y) = A^T (A \\cdot 0_{n \\times L} - Y) = -A^T Y\n$$\n给定的数据为：\n$$\nA = \\begin{pmatrix} 2  0  1 \\\\ 0  1  -1 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} 1  3 \\\\ 2  -1 \\end{pmatrix}\n$$\n$A$ 的转置是：\n$$\nA^T = \\begin{pmatrix} 2  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix}\n$$\n现在，我们计算乘积 $-A^T Y$：\n$$\n-A^T Y = - \\begin{pmatrix} 2  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  3 \\\\ 2  -1 \\end{pmatrix} = - \\begin{pmatrix} 2 \\cdot 1 + 0 \\cdot 2  2 \\cdot 3 + 0 \\cdot (-1) \\\\ 0 \\cdot 1 + 1 \\cdot 2  0 \\cdot 3 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 2  1 \\cdot 3 + (-1) \\cdot (-1) \\end{pmatrix} = - \\begin{pmatrix} 2  6 \\\\ 2  -1 \\\\ -1  4 \\end{pmatrix} = \\begin{pmatrix} -2  -6 \\\\ -2  1 \\\\ 1  -4 \\end{pmatrix}\n$$\n\n接下来，我们执行梯度下降步。令 $Z$ 为此步之后的矩阵：\n$$\nZ = X^{(0)} - t \\nabla g(X^{(0)}) = 0_{n \\times L} - t (-A^T Y) = t A^T Y\n$$\n给定步长 $t = 0.5$，我们有：\n$$\nZ = 0.5 \\begin{pmatrix} 2  6 \\\\ 2  -1 \\\\ -1  4 \\end{pmatrix} = \\begin{pmatrix} 1  3 \\\\ 1  -0.5 \\\\ -0.5  2 \\end{pmatrix}\n$$\n\n第二步是应用近端算子。我们需要计算 $X^{(1)} = \\text{prox}_{t\\lambda\\|\\cdot\\|_{2,1}}(Z)$。正则化参数为 $\\lambda = 0.8$，因此近端算子的参数为 $\\mu = t\\lambda = 0.5 \\times 0.8 = 0.4$。\n$\\ell_{2,1}$ 范数的近端算子 $\\text{prox}_{\\mu\\|\\cdot\\|_{2,1}}(Z)$ 是逐行作用的。对于 $Z$ 的每一行 $Z_{i,:}$，输出矩阵的对应行由块软阈值给出：\n$$\n(X^{(1)})_{i,:} = \\left(1 - \\frac{\\mu}{\\|Z_{i,:}\\|_2}\\right)_+ Z_{i,:}\n$$\n其中 $(c)_+ = \\max(c, 0)$。\n\n我们将此公式应用于 $Z$ 的每一行：\n\n第 1 行：$Z_{1,:} = \\begin{pmatrix} 1  3 \\end{pmatrix}$\n其 $\\ell_2$ 范数为 $\\|Z_{1,:}\\|_2 = \\sqrt{1^2 + 3^2} = \\sqrt{10}$。\n缩放因子为 $s_1 = 1 - \\frac{0.4}{\\sqrt{10}}$。由于 $\\sqrt{10} \\approx 3.16$ 且 $0.4 / \\sqrt{10}  1$，该因子为正。\n$s_1 \\approx 1 - \\frac{0.4}{3.162277...} \\approx 1 - 0.126491... = 0.873509...$\n$(X^{(1)})_{1,:} = s_1 Z_{1,:} \\approx 0.873509 \\times \\begin{pmatrix} 1  3 \\end{pmatrix} = \\begin{pmatrix} 0.873509  2.620527 \\end{pmatrix}$。\n\n第 2 行：$Z_{2,:} = \\begin{pmatrix} 1  -0.5 \\end{pmatrix}$\n其 $\\ell_2$ 范数为 $\\|Z_{2,:}\\|_2 = \\sqrt{1^2 + (-0.5)^2} = \\sqrt{1 + 0.25} = \\sqrt{1.25} = \\frac{\\sqrt{5}}{2}$。\n缩放因子为 $s_2 = 1 - \\frac{0.4}{\\sqrt{1.25}} = 1 - \\frac{0.4}{\\sqrt{5}/2} = 1 - \\frac{0.8}{\\sqrt{5}}$。由于 $\\sqrt{5} \\approx 2.236$ 且 $0.8 / \\sqrt{5}  1$，该因子为正。\n$s_2 \\approx 1 - \\frac{0.8}{2.236068...} \\approx 1 - 0.357771... = 0.642229...$\n$(X^{(1)})_{2,:} = s_2 Z_{2,:} \\approx 0.642229 \\times \\begin{pmatrix} 1  -0.5 \\end{pmatrix} = \\begin{pmatrix} 0.642229  -0.321115 \\end{pmatrix}$。\n\n第 3 行：$Z_{3,:} = \\begin{pmatrix} -0.5  2 \\end{pmatrix}$\n其 $\\ell_2$ 范数为 $\\|Z_{3,:}\\|_2 = \\sqrt{(-0.5)^2 + 2^2} = \\sqrt{0.25 + 4} = \\sqrt{4.25} = \\frac{\\sqrt{17}}{2}$。\n缩放因子为 $s_3 = 1 - \\frac{0.4}{\\sqrt{4.25}} = 1 - \\frac{0.4}{\\sqrt{17}/2} = 1 - \\frac{0.8}{\\sqrt{17}}$。由于 $\\sqrt{17} \\approx 4.123$ 且 $0.8 / \\sqrt{17}  1$，该因子为正。\n$s_3 \\approx 1 - \\frac{0.8}{4.123106...} \\approx 1 - 0.194030... = 0.805970...$\n$(X^{(1)})_{3,:} = s_3 Z_{3,:} \\approx 0.805970 \\times \\begin{pmatrix} -0.5  2 \\end{pmatrix} = \\begin{pmatrix} -0.402985  1.611940 \\end{pmatrix}$。\n\n将矩阵 $X^{(1)}$ 组合起来，并将每个元素四舍五入到四位有效数字：\n$X^{(1)}_{11} \\approx 0.8735$\n$X^{(1)}_{12} \\approx 2.621$\n$X^{(1)}_{21} \\approx 0.6422$\n$X^{(1)}_{22} \\approx -0.3211$\n$X^{(1)}_{31} \\approx -0.4030$\n$X^{(1)}_{32} \\approx 1.612$\n\n因此，更新后的迭代值 $X^{(1)}$ 为：\n$$\nX^{(1)} \\approx \\begin{pmatrix}\n0.8735  2.621 \\\\\n0.6422  -0.3211 \\\\\n-0.4030  1.612\n\\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8735  2.621 \\\\ 0.6422  -0.3211 \\\\ -0.4030  1.612 \\end{pmatrix}}\n$$", "id": "3460767"}, {"introduction": "除了基于凸优化的方法，贪婪算法是求解MMV问题的另一类重要策略。本练习将通过一个编程任务，让你深入了解同步正交匹配追踪（SOMP）算法的运作机制。你需要从零开始实现SOMP，并通过具体的数值实例观察其在不同条件下（如无噪声、有噪声、以及原子选择出现歧义时）如何逐步识别正确的稀疏支撑集，这对于培养算法实现和调试能力至关重要。", "problem": "考虑压缩感知中的多重测量向量 (Multiple Measurement Vector, MMV) 模型。MMV 模型假设 $L$ 个测量向量在未知系数矩阵的各行上共享一个共同的稀疏支撑集。形式上，令 $A \\in \\mathbb{R}^{m \\times n}$ 为一个测量矩阵（也称为字典），$X \\in \\mathbb{R}^{n \\times L}$ 为一个行稀疏系数矩阵，其至多有 $k$ 个非零行（联合稀疏性），$Y \\in \\mathbb{R}^{m \\times L}$ 为观测到的满足下式的测量值\n$$\nY = A X + E,\n$$\n其中 $E \\in \\mathbb{R}^{m \\times L}$ 是加性噪声。同步正交匹配追踪 (Simultaneous Orthogonal Matching Pursuit, SOMP) 算法贪婪地选择 $A$ 的列（称为原子），这些列能够最好地解释 $L$ 个测量向量上的残差。该算法通过将残差正交投影到所选原子的张成空间上来更新残差，并在预定数量的迭代后停止。\n\n从基本原理出发：\n- 第 $t$ 次迭代时的残差定义为 $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$，其中 $S^{(t)}$ 是第 $t$ 次迭代时所选索引的集合，$A_{S^{(t)}}$ 是 $A$ 中包含由 $S^{(t)}$ 索引的列的子矩阵。\n- 最小二乘更新强制 $X_{S^{(t)}}$ 最小化残差的弗罗贝尼乌斯范数，即 $X_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F$。到 $A_{S^{(t)}}$ 的张成空间上的正交投影算子表示为（当 $A_{S^{(t)}}$ 具有满列秩时）$P_{A_{S^{(t)}}} = A_{S^{(t)}} \\left( A_{S^{(t)}}^\\top A_{S^{(t)}} \\right)^{-1} A_{S^{(t)}}^\\top$，从而得到残差 $R^{(t)} = (I - P_{A_{S^{(t)}}}) Y$。\n- 在第 $t$ 次迭代时，贪婪地选择一个索引 $j \\notin S^{(t-1)}$ 的方法是通过最大化将当前残差 $R^{(t-1)}$ 投影到原子 $A_j$ 上所捕获的能量来实现的。对于单位范数列，捕获的能量为 $\\| A_j^\\top R^{(t-1)} \\|_2^2$，因此 SOMP 的选择规则是选择\n$$\nj^{\\star} \\in \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2.\n$$\n\n你的任务是实现 SOMP 算法，并在以下小型的、完全指定的实例上运行恰好两次迭代，跟踪残差和所选索引的演变。本问题中所有索引均使用从零开始的索引方式。\n\n使用参数 $(m,n,L) = (5,10,3)$ 和联合稀疏度 $k = 2$ 实现以下测试套件：\n\n- 测试用例 1（无噪声，通用字典）：\n  - 通过从固定种子 $s_A = 0$ 的标准正態分布中独立抽取元素来构造 $A$，并将每一列归一化为单位 $\\ell_2$ 范数。\n  - 选择一个已知的支撑集 $S^\\mathrm{true} = [2, 7]$，并将 $X$ 在索引 2 和 7 处的非零行设置为 $X_{2,:} = [1.0, -0.5, 0.8]$ 和 $X_{7,:} = [0.9, 0.3, -1.2]$。$X$ 的所有其他行均为零。\n  - 设置 $E = 0$ 和 $Y = A X$。\n\n- 测试用例 2（小噪声，相同字典和支撑集）：\n  - 使用与测试用例 1 中相同的 $A$ 和 $X$。\n  - 添加高斯噪声 $E$，其元素从种子为 $s_E = 1$ 的标准正态分布中独立抽取并按 $\\sigma = 0.01$ 缩放，即 $E = \\sigma \\cdot W$，其中 $W$ 具有独立的标准正态分布元素。\n  - 设置 $Y = A X + E$。\n\n- 测试用例 3（首次选择时出现平局，正交规范原子）：\n  - 构造 $A$ 为 $A = [I_5 \\,|\\, B]$，其中 $I_5 \\in \\mathbb{R}^{5 \\times 5}$ 是单位矩阵（其列已是单位范数），$B \\in \\mathbb{R}^{5 \\times 5}$ 是通过从固定种子 $s_B = 2$ 的标准正态分布中抽取元素并将其每一列归一化为单位 $\\ell_2$ 范数而形成的。\n  - 选择一个已知的支撑集 $S^\\mathrm{true} = [1, 2]$，并将 $X$ 在索引 1 和 2 处的非零行设置为 $X_{1,:} = [0.6, 0.8, 0.0]$ 和 $X_{2,:} = [0.0, 0.6, 0.8]$。$X$ 的所有其他行均为零。\n  - 设置 $E = 0$ 和 $Y = A X$。\n  - 因为 $\\|X_{1,:}\\|_2 = \\|X_{2,:}\\|_2$，且前五个原子是正交规范的，所以第一次 SOMP 迭代会在索引 1 和 2 之间引起平局。通过选择最小的索引来解决平局。\n\n算法要求：\n- 在运行 SOMP 之前，将 $A$ 的每一列归一化为单位 $\\ell_2$ 范数。\n- 在每次迭代 $t = 1,2$ 中，计算相关矩阵 $C^{(t)} = A^\\top R^{(t-1)} \\in \\mathbb{R}^{n \\times L}$，然后对所有 $j \\notin S^{(t-1)}$ 计算选择分数 $g_j^{(t)} = \\| C^{(t)}_{j,:} \\|_2$，选择获得最大分数的索引 $j^{\\star}$（通过选择最小索引来打破平局），更新 $S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$，并通过最小二乘法更新残差 $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$，其中 $X_{S^{(t)}}$ 是最小化 $\\| Y - A_{S^{(t)}} Z \\|_F$ 的最小二乘解。\n- 记录所选索引的序列 $[j^{\\star}_1, j^{\\star}_2]$ 和残差的弗罗贝尼乌斯范数 $\\| R^{(1)} \\|_F$ 和 $\\| R^{(2)} \\|_F$。\n\n最终输出格式：\n- 对于每个测试用例，输出一个包含两个元素的列表：两次迭代中所选索引的列表，以及两个残差弗罗贝尼乌斯范数（四舍五入到六位小数）的列表。\n- 将所有测试用例的结果按顺序汇总到一个列表中，并打印一行包含此汇总列表的内容，格式为逗号分隔并用方括号括起来，不含任何附加文本。例如，输出应如下所示：\n$$\n[[[j_1,j_2],[r_1,r_2]],[[\\ldots],[\\ldots]],[[\\ldots],[\\ldots]]]\n$$\n其中 $j_1, j_2$ 是整数，$r_1, r_2$ 是四舍五入到六位小数的浮点数。\n\n你的程序必须是一个完整的、可运行的实现，能够为上述三个测试用例生成指定的单行输出。", "solution": "任务是实现同步正交匹配追踪 (SOMP) 算法，从一组线性测量 $Y = AX + E$ 中恢复行稀疏矩阵 $X$。SOMP 是一种为多重测量向量 (MMV) 问题设计的迭代贪婪算法，在 MMV 问题中，多个信号向量共享一个共同的稀疏支撑集。该算法通过从字典 $A$ 中选择能够最好地解释观测测量值 $Y$ 的列（原子），来迭代地识别 $X$ 的支撑集。\n\n实现过程将忠实地执行 SOMP 算法定义的步骤，共进行恰好两次迭代。\n\n**算法原理与实现**\n\nSOMP 算法的核心在于其迭代的两阶段过程：一个选择阶段和一个更新阶段。\n\n**1. 初始化：**\n算法以一个空的支撑集 $S^{(0)} = \\emptyset$ 开始，初始残差等于测量矩阵 $R^{(0)} = Y$。\n\n**2. 迭代过程（对于迭代 $t=1, 2, \\dots$）：**\n\n**a. 原子选择：**\n选择步骤的基本原则是从字典中识别出与当前残差 $R^{(t-1)}$ 相关性最高的原子 $A_j$。这个相关性是在所有 $L$ 个测量通道上度量的。对于每个原子 $A_j$（其中 $j$ 尚未在支撑集 $S^{(t-1)}$ 中），我们计算多通道残差在其上的投影。该投影的能量由 $\\| A_j^\\top R^{(t-1)} \\|_2^2$ 给出。SOMP 选择使该能量最大化的原子，这等同于最大化相关向量的 $\\ell_2$ 范数：\n$$\nj^{\\star} = \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2\n$$\n在实现中，这是通过首先计算相关矩阵 $C^{(t)} = A^\\top R^{(t-1)}$ 来实现的。然后，每个原子 $j$ 的选择分数是 $C^{(t)}$ 的第 $j$ 行的 $\\ell_2$ 范数。为此使用了带有 `axis=1` 的 `numpy.linalg.norm` 函数。已选择的索引被屏蔽掉，`numpy.argmax` 会找到分数最高的原子的索引。问题规定，平局通过选择最小的索引来打破，这是 `numpy.argmax` 的固有行为。然后更新支撑集：$S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$。\n\n**b. 最小二乘更新与残差计算：**\n一旦支撑集 $S^{(t)}$ 更新，就使用已选择的原子重新估计系数矩阵 $X$，以最好地拟合测量值 $Y$。这是一个经典的最小二乘问题：\n$$\nX_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F\n$$\n其中 $A_{S^{(t)}}$ 是 $A$ 中包含由 $S^{(t)}$ 索引的列的子矩阵，$X_{S^{(t)}}$ 是 $X$ 的估计中相应的非零行。这个最小化问题的解是通过将 $Y$ 投影到由 $A_{S^{(t)}}$ 的列张成的子空间上得到的。当 $A_{S^{(t)}}$ 的列线性无关时，解是唯一的，并由 $X_{S^{(t)}} = (A_{S^{(t)}}^\\top A_{S^{(t)}})^{-1} A_{S^{(t)}}^\\top Y$ 给出。\n实现中使用了 `numpy.linalg.lstsq(A_S, Y)`，它为这个最小二乘问题提供了一个数值上稳定和准确的解，有效地计算了 $A_{S^{(t)}}$ 的 Moore-Penrose 伪逆与 $Y$ 的乘积。\n\n新的残差 $R^{(t)}$ 是 $Y$ 中未被此投影解释的部分：\n$$\nR^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}\n$$\n在每次迭代中计算并记录此残差的弗罗贝尼乌斯范数 $\\|R^{(t)}\\|_F$。该范数量化了剩余误差，其减小表明算法在解释数据方面的进展。\n\n对于指定的三个测试用例中的每一个，此过程都重复进行所要求的两次迭代。每个测试用例的设置都涉及根据给定参数（包括用于可复现性的随机种子和特定的噪声条件）构造矩阵 $A$、$X$ 和 $Y$。最终输出汇总了每个用例的所选索引列表和残差弗罗贝尼乌斯范数列表。", "answer": "```python\nimport numpy as np\n\ndef run_somp(A, Y, num_iterations):\n    \"\"\"\n    Implements the Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm.\n\n    Args:\n        A (np.ndarray): The measurement matrix (m x n).\n        Y (np.ndarray): The measurement vectors (m x L).\n        num_iterations (int): The number of iterations to run.\n\n    Returns:\n        tuple: A tuple containing:\n            - list: The list of selected indices.\n            - list: The list of residual Frobenius norms at each iteration.\n    \"\"\"\n    m, n = A.shape\n    \n    # Initial residual is the measurement matrix itself\n    R = Y.copy()\n    \n    # Set of selected indices\n    S = []\n    \n    # Mask for selected indices to handle argmax efficiently\n    selected_mask = np.zeros(n, dtype=bool)\n    \n    # List to store residual norms\n    residual_norms = []\n\n    for _ in range(num_iterations):\n        # 1. Selection Step: Find the atom most correlated with the residual\n        # Compute correlations: A^T * R\n        C = A.T @ R\n        \n        # Compute scores as the l2-norm of each row of the correlation matrix\n        scores = np.linalg.norm(C, axis=1)\n        \n        # Mask out already selected columns by setting their score to a negative value\n        scores[selected_mask] = -1.0\n        \n        # Select the index with the maximum score. numpy.argmax breaks ties\n        # by choosing the smallest index, as required.\n        j_star = np.argmax(scores)\n        \n        # 2. Update Step\n        # Add the new index to the support set\n        S.append(j_star)\n        selected_mask[j_star] = True\n        \n        # Form the submatrix of A with currently selected columns\n        A_S = A[:, S]\n        \n        # Solve the least-squares problem: min ||Y - A_S * Z||_F\n        # np.linalg.lstsq is a numerically stable way to do this.\n        X_S, _, _, _ = np.linalg.lstsq(A_S, Y, rcond=None)\n        \n        # Update the residual\n        R = Y - A_S @ X_S\n        \n        # Calculate and store the Frobenius norm of the new residual\n        norm_R = np.linalg.norm(R, 'fro')\n        residual_norms.append(norm_R)\n        \n    # Round norms to six decimal places as per the requirement\n    rounded_norms = [round(norm, 6) for norm in residual_norms]\n    \n    return S, rounded_norms\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases for the SOMP algorithm,\n    then prints the formatted results.\n    \"\"\"\n    results = []\n    m, n, L = 5, 10, 3\n    num_iterations = 2\n\n    # --- Test Case 1: Noise-free, general dictionary ---\n    rng_A = np.random.default_rng(seed=0)\n    A1 = rng_A.standard_normal((m, n))\n    A1 /= np.linalg.norm(A1, axis=0)  # Normalize each column to unit l2-norm\n    \n    X1 = np.zeros((n, L))\n    X1[2, :] = [1.0, -0.5, 0.8]\n    X1[7, :] = [0.9, 0.3, -1.2]\n    \n    Y1 = A1 @ X1\n    \n    indices1, norms1 = run_somp(A1, Y1, num_iterations)\n    results.append([indices1, norms1])\n\n    # --- Test Case 2: Small noise, same dictionary and support ---\n    # Use the same A1 and X1\n    rng_E = np.random.default_rng(seed=1)\n    sigma = 0.01\n    E = sigma * rng_E.standard_normal((m, L))\n    \n    Y2 = A1 @ X1 + E\n    \n    indices2, norms2 = run_somp(A1, Y2, num_iterations)\n    results.append([indices2, norms2])\n\n    # --- Test Case 3: Tie in first selection, orthonormal atoms ---\n    rng_B = np.random.default_rng(seed=2)\n    I5 = np.identity(5)\n    B = rng_B.standard_normal((5, 5))\n    B /= np.linalg.norm(B, axis=0)\n    A3 = np.hstack((I5, B))\n    \n    X3 = np.zeros((n, L))\n    X3[1, :] = [0.6, 0.8, 0.0]\n    X3[2, :] = [0.0, 0.6, 0.8]\n    \n    Y3 = A3 @ X3\n    \n    indices3, norms3 = run_somp(A3, Y3, num_iterations)\n    results.append([indices3, norms3])\n\n    # Format the final output string to remove spaces for a compact representation\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3460799"}]}