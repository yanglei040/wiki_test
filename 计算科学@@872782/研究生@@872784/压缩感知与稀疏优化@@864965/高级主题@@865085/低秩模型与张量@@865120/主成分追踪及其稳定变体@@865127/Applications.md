## 应用与跨学科连接

在前几章中，我们已经为“[主成分追踪](@entry_id:753736)” (Principal Component Pursuit, PCP) 的核心理论奠定了坚实的基础，包括其[凸松弛](@entry_id:636024)形式、[恢复保证](@entry_id:754159)以及基本的算法框架。然而，理论的真正力量在于其解决实际问题的能力。现实世界的数据往往比理想化的模型更为复杂，充满了各种结构特性、采集限制和领域特有的先验知识。

本章的目标是展示PCP框架如何超越其基础形式，通过扩展和调整以适应多样化的应用场景。我们将探讨如何修改PCP模型以处理不同类型的[稀疏结构](@entry_id:755138)、应对各种测量[范式](@entry_id:161181)，并融入特定领域的物理或几何约束。通过这些案例，我们将看到PCP不仅仅是一个孤立的数学工具，更是一个连接了[计算机视觉](@entry_id:138301)、机器学习、信号处理和数据挖掘等多个领域的强大而灵活的分析框架。本章的目的不是重复核心原理，而是阐明这些原理在实际应用中的效用、扩展和整合。

### [主成分追踪](@entry_id:753736)的结构变体

标准PCP模型假设稀疏分量$S$是“元素级稀疏” (entrywise sparse)，即其非零元素随机散布在矩阵中。然而，在许多应用中，稀疏损坏呈现出更具结构化的模式。此外，为了追求更优的恢复性能，研究者们也探索了超越[凸松弛](@entry_id:636024)的非凸模型。

#### 建模多样的[稀疏结构](@entry_id:755138)

在实践中，损坏或感兴趣的信号可能并非随机[分布](@entry_id:182848)的单个元素，而是以组或特定变换域的形式出现。

**列稀疏与行稀疏损坏**

一个常见的结构化稀疏模式是“列稀疏” (column-sparse)。例如，在一个由多个数据样本（如人脸图像）作为列构成的数据矩阵中，如果少数几个样本被完全损坏（例如，由于采集失败或恶意的整体性篡改），那么稀疏矩阵$S$的非零项将集中在少数几列。这种损坏模式被称为“样本级损坏” (sample-wise corruption)。

为了对这种结构进行建模，标准PCP中用于促进元素级稀疏的$\ell_1$范数（$\|S\|_1 = \sum_{i,j} |S_{ij}|$）不再是最优选择。取而代之的是“离群点追踪” (Outlier Pursuit) 模型，它采用$\ell_{2,1}$混合范数：
$$
\min_{L,S} \ \|L\|_{*} + \lambda \|S\|_{2,1} \quad \text{subject to} \quad M = L + S
$$
其中，$\|S\|_{2,1} = \sum_{j=1}^{n} \|S_{:j}\|_{2}$，即矩阵$S$所有列的[欧几里得范数](@entry_id:172687)之和。$\ell_{2,1}$范数之所以能诱导出列[稀疏性](@entry_id:136793)，源于其独特的几何性质。从[优化算法](@entry_id:147840)的角度看，其[近端算子](@entry_id:635396) (proximal operator) 对矩阵的每一列独立地进行“组[软阈值](@entry_id:635249)” (group soft-thresholding) 操作：
$$
\text{prox}_{\tau \|\cdot\|_{2,1}}(X)_{:j} = \left(1 - \frac{\tau}{\|X_{:j}\|_2}\right)_{+} X_{:j}
$$
这个操作要么将整列置为零（如果其欧几里得范数$\|X_{:j}\|_2$小于阈值$\tau$），要么按比例缩放整列，但保持其方向不变。这种“要么全有，要么全无”的列级别操作，精确地匹配了识别并移除整个损坏数据样本的物理直觉 [@problem_id:3468092]。

与标准PCP相比，离群点追踪的一个显著优势在于其[恢复保证](@entry_id:754159)。标准PCP的理论通常要求稀疏项的支撑集是随机[分布](@entry_id:182848)的，以避免与低秩分量的[子空间](@entry_id:150286)发生“共谋”。然而，对于列[稀疏模型](@entry_id:755136)，只要低秩分量$L_0$的[列空间](@entry_id:156444)是“非相干的” (incoherent)——即其能量不过分集中于少数坐标轴——离群点追踪就能从被“蓄意选择” (adversarially chosen) 的损坏列中精确恢复信号。其可识别性条件通常依赖于损坏列的数量$s$，而非其具体位置，例如，要求$s \le c \cdot n_2/(\mu r)$，其中$r$是秩，$\mu$是非[相干性](@entry_id:268953)参数。这与标准PCP形成了鲜明对比 [@problem_id:3468065]。同理，通过对行使用$\ell_{1,2}$范数（$\|S\|_{1,2} = \sum_{i=1}^{m} \|S_{i,:}\|_2$），也可以对行稀疏的损坏进行建模 [@problem_id:3468057]。

**变换域[稀疏性](@entry_id:136793)**

在其他应用中，稀疏分量$S$本身在原始域（像[素域](@entry_id:634209)或时域）可能是稠密的，但在某个已知的线性变换$W$下变得稀疏。例如，自然图像中的一片平滑区域可能包含稠密的像素值，但在[小波变换](@entry_id:177196)下只有少数几个非零系数。这被称为“[分析稀疏性](@entry_id:746432)” (analysis-sparsity)。

为了处理这种情况，PCP模型可以调整为：
$$
\min_{L,S} \ \|L\|_{*} + \lambda \|W S\|_{1} \quad \text{subject to} \quad L + S = M
$$
这里的目标是寻找一个在变换域$W$中稀疏的$S$。该问题的可识别性取决于低秩分量$L$的[切空间](@entry_id:199137)$T$与分析[稀疏结构](@entry_id:755138)之间的非[相干性](@entry_id:268953)。对偶可行性分析表明，存在一个对偶证书$Y$同时满足$Y \in \partial \|L_0\|_*$和$Y = \lambda W^{\top} Z$，其中$Z$是$WS_0$在$\ell_1$范数下的[次梯度](@entry_id:142710)。这表明，可恢[复性](@entry_id:162752)不仅依赖于$L_0$的非[相干性](@entry_id:268953)，还依赖于变换$W$与$L_0$的[奇异向量](@entry_id:143538)之间的相互作用。当$W$是正交变换（如傅里葉变换或正交[小波基](@entry_id:265197)）且与$L_0$的[子空间](@entry_id:150286)无关时，可以建立类似于标准PCP的[恢复保证](@entry_id:754159)。然而，如果$W$的结构与$L_0$的结构发生对齐，例如$W^{\top}$将一个在分析域中稀疏的模式映射到一个$T$中的低秩结构，那么即使$L_0$本身是非相干的，分解也可能失败 [@problem_id:3468111] [@problem_id:3468065]。

#### 超越[凸性](@entry_id:138568)：非凸公式

尽管凸PCP模型取得了巨大成功，但[核范数](@entry_id:195543)和$\ell_1$范数作为秩和基数函数的代理，会引入系统性偏差。例如，[软阈值](@entry_id:635249)操作会对所有非零系数（或奇异值）进行相同量的缩减，导致对大值的惩罚过重。为了更精确地逼近秩和$\ell_0$“范数”，研究者们提出了非凸的替代方案，例如使用Schatten-p拟范数和$\ell_p$拟范数（$0 \lt p \lt 1$）：
$$
\min_{L,S} \ \|L\|_{S_p}^p + \lambda \|S\|_{p}^p + \frac{\mu}{2} \|D-L-S\|_F^2
$$
其中$\|L\|_{S_p}^p = \sum_i \sigma_i(L)^p$且$\|S\|_{p}^p = \sum_{i,j} |S_{ij}|^p$。由于函数$t \mapsto t^p$在$t \ge 0$上是凹的，它对大值的惩罚增长得比线性慢，从而减少了对大奇异值和大稀疏系数的收缩，降低了估计偏差 [@problem_id:3468067]。

然而，这种改进并非没有代价。[非凸优化](@entry_id:634396)问题通常存在多个局部最小值，找到全局最优解是NP-难问题。虽然某些特定条件下非凸问题的几何景观可能良好（即没有“坏”的局部最小值），但这并非普遍规律。算法上，这类问题通常通过“主化-最小化” (Majorization-Minimization) 策略来求解，例如迭代重加权核范数 (IRNN) 和迭代重加权$\ell_1$范数 (IRL1) 算法。这些算法在每次迭代中求解一个加权的凸问题，其权[重根](@entry_id:151486)据上一轮迭代的解来更新（例如，权重与$\sigma_i^{p-1}$或$|S_{ij}|^{p-1}$成正比）。这类算法能够保证[目标函数](@entry_id:267263)值单调下降，并且在温和的条件下（如Kurdyka-Łojasiewicz性质，大多数实际问题都满足）可以收敛到一个[临界点](@entry_id:144653)。尽管收敛到的是局部而非全局最优，但在实践中，非凸方法往往能提供比凸方法更精确的恢复结果，尤其是在信噪比较高的情况下 [@problem_id:3468067]。

### PCP在不同测量与数据模型中的应用

标准PCP假设我们能够完全观测到数据矩阵$M$。然而，在许多科学和工程问题中，数据要么是不完整的，要么是通过间接的线性测量获得的，甚至可能被严重量化。PCP框架的灵活性使其能够适应这些复杂的观测模型。

#### 不完整与含噪观测

在[推荐系统](@entry_id:172804)或[传感器网络](@entry_id:272524)等应用中，数据矩阵$M$通常只在一小部分条目上被观测到。这个问题结合了[主成分追踪](@entry_id:753736)和[矩阵填充](@entry_id:751752) (Matrix Completion) 的挑战，被称为“鲁棒[矩阵填充](@entry_id:751752)” (Robust Matrix Completion)。设观测集为$\Omega$，观测模型为$\mathcal{P}_{\Omega}(M) = \mathcal{P}_{\Omega}(L+S+W)$，其中$W$是稠密的小噪声。

一个稳健的[凸优化](@entry_id:137441)模型是所谓的“[稳定PCP](@entry_id:755323)” (Stable PCP) 的约束形式：
$$
\min_{L,S} \ \|L\|_{*} + \lambda \|S\|_{1} \quad \text{subject to} \quad \|\mathcal{P}_{\Omega}(M - L - S)\|_{F} \le \epsilon
$$
这里，约束项确保模型在观测条目上与数据的[拟合优度](@entry_id:637026)，其容忍度$\epsilon$与噪声水平成正比（通常$\epsilon \asymp \sigma \sqrt{|\Omega|}$）。正则化参数$\lambda$则平衡低秩与稀疏的假设，其经典选择为$\lambda = 1/\sqrt{\max(m,n)}$。为了保证能够成功恢复，除了$L$的非相干性和$S$的（随机）稀疏性之外，还需要观测样本足够多，其数量通常要求$|\Omega| \gtrsim \mu r (m+n) \log(m+n)$ [@problem_id:3468069]。

深入理解这一模型有助于我们厘清PCP与[矩阵填充](@entry_id:751752)的联系与区别。纯[矩阵填充](@entry_id:751752)旨在从$\mathcal{P}_{\Omega}(M)$恢复低秩矩阵$L$，其约束为$\mathcal{P}_{\Omega}(L) = \mathcal{P}_{\Omega}(M)$。纯PCP则从完全观测的$M$中分离$L$和$S$，其约束为$L+S=M$。鲁棒[矩阵填充](@entry_id:751752)则综合了两者：它在部分观测的条件下，同时处理低秩结构和稀疏损坏。从[对偶理论](@entry_id:143133)的角度看，这两种约束导致了不同的对偶变量结构：[矩阵填充](@entry_id:751752)的对偶证书仅需在观测集$\Omega$上非零，而PCP的对偶证书则是在整个矩阵域上同时满足算子范数和[无穷范数](@entry_id:637586)的双重约束 [@problem_id:3468064]。在鲁棒[矩阵填充](@entry_id:751752)中，$\lambda$的最优尺度需要平衡这两个对偶结构，理论分析表明，在伯努利采样模型下，$\lambda$应与$(pn)^{-1/2}$成比例，其中$p$是采样率 [@problem_id:3468084]。

#### 广义线性测量

PCP模型还可以推广到更一般的压缩感知设定中。假设我们观测到的不是矩阵本身，而是经过一个线性算子$\mathcal{A}: \mathbb{R}^{m \times n} \to \mathbb{R}^{p}$作用后的向量$y = \mathcal{A}(L+S) + w$。例如，$\mathcal{A}$可以代表傅里葉变换、[随机投影](@entry_id:274693)或成像系统的物理模型。恢复$L$和$S$的凸规划为：
$$
\min_{L,S} \ \|L\|_{*} + \lambda \|S\|_{1} \quad \text{subject to} \quad \|\mathcal{A}(L+S) - y\|_{2} \le \epsilon
$$
此时，能否成功恢复，关键取决于测量算子$\mathcal{A}$的性质。并非任何[线性算子](@entry_id:149003)都能保证恢复。成功的关键在于$\mathcal{A}$是否能近似保持“低秩+稀疏”这一结构化信号集的几何形状。这个性质被形式化为“秩-稀疏限制同构性质” (Rank-Sparse Restricted Isometry Property, RIP)。如果$\mathcal{A}$对于所有秩不超过$2r$且稀疏度不超过$2k$的矩阵$X$都满足$(1-\delta)\|X\|_F \le \|\mathcal{A}(X)\|_2 \le (1+\delta)\|X\|_F$（对于小的$\delta$），那么就可以证明，上述凸规划的解能稳定地逼近真实信号，误差与噪声水平$\epsilon$成正比。RIP本质上确保了算子$\mathcal{A}$不会将两个不同的结构化信号映射到同一点，从而保证了可识别性。没有类似RIP的保证，即使是无噪声情况，恢复也可能因为$\mathcal{A}$的零空间中存在结构化信号而失败 [@problem_id:3468112]。

#### 量化数据：1比特[主成分追踪](@entry_id:753736)

在极端情况下，我们可能只能观测到数据矩阵$M=L+S$中每个元素的符号，即$Y_{ij} = \operatorname{sign}(D_{ij})$。这是“1比特PCP”问题，在处理二元或严重量化数据时非常重要。

直接从符号恢复$L$和$S$面临一个根本性的“尺度模糊” (scale ambiguity) 问题：如果$(L,S)$能够产生符号矩阵$Y$，那么对于任何正标量$c0$，$(cL, cS)$也能产生完全相同的符号。因此，从符号数据本身无法唯一确定$L$和$S$的幅度。

为了解决这个问题，通常采用[广义线性模型 (GLM)](@entry_id:749787) 的框架。例如，可以使用逻辑回归模型，假设观测到$Y_{ij}$的概率与$L_{ij}+S_{ij}$通过一个sigmoid函数关联。这引出了一个基于[负对数似然](@entry_id:637801)的数据保真项，结合PCP的正则项，形成如下的凸[优化问题](@entry_id:266749)：
$$
\min_{L,S} \ \sum_{i,j} \log(1 + \exp(-Y_{ij}(L_{ij} + S_{ij}))) + \|L\|_{*} + \lambda \|S\|_{1}
$$
这个[目标函数](@entry_id:267263)是$L$和$S$的联合凸函数。然而，由于尺度模糊性，若不加约束，当数据可分时，似然函数可以通过无限增大$L+S$的尺度来不断优化，导致不存在有限解。因此，必须通过正则化（如上式的[核范数](@entry_id:195543)和$\ell_1$范数）或显式约束（如$\|L+S\|_{\infty} \le B$）来固定尺度。在施加了合适的尺度约束，并满足标准非[相干性](@entry_id:268953)和[稀疏性](@entry_id:136793)假设的条件下，理论分析（通常依赖于“限制强凸性” (Restricted Strong Convexity, RSC) 等现代[高维统计](@entry_id:173687)工具）表明，可以高概率地恢复$L$的低秩[子空间](@entry_id:150286)和$S$的稀疏支撑集 [@problem_id:3468109]。

### 跨学科应用与领域特定的适应性改造

PCP框架的真正魅力在于其能够被巧妙地改造，以吸收特定领域的先验知识，从而在各类科学与工程问题中大放异彩。

#### 计算机视觉：人脸与场景分析

**光照变化下的人脸识别**

在[计算机视觉](@entry_id:138301)中，一个经典问题是在复杂光照变化下识别人脸。将一系列同一主体、固定姿态但在不同光照下拍摄的人脸图像向量化并堆叠成一个数据矩阵$D$，PCP提供了一个优雅的解决方案。根据Lambertian反射模型，一个凸面物体在任意远光源照射下所成的图像集合，近似地位于一个低至9维的[线性子空间](@entry_id:151815)中。因此，数据矩阵中代表理想人脸图像的“干净”部分可以被建模为一个低秩矩阵$L$。而像投射阴影和镜面高光等现象，它们不符合Lambertian模型，并且通常只影响图像的局部区域，这些便可以被建模为稀疏损坏矩阵$S$。相机传感器引入的随机噪声则是小的稠密噪声$E$。

因此，人脸图像的去噪和阴影移除问题可以被精确地表述为一个[稳定PCP](@entry_id:755323)问题：$D=L+S+E$。通过求解
$$
\min_{L,S} \ \|L\|_* + \lambda \|S\|_1 \quad \text{subject to} \quad \|D - L - S\|_F \le \delta
$$
或其[拉格朗日形式](@entry_id:145697)，我们可以将观测图像分解为代表内在身份信息的低秩人脸$L$和代表光照伪影的稀疏分量$S$。$L$可用于后续的稳健识别，而$S$则被丢弃。值得注意的是，像自动曝光变化引起的全局亮度缩放这类变化，由于其保持了[线性子空间](@entry_id:151815)结构，也会被自然地吸收到低秩分量$L$中 [@problem_id:3468108]。

**结合几何约束的同时定位与建图 (SLAM)**

在视觉SLAM中，一个核心任务是根据连续视频帧中的特征点轨迹来重建相机轨迹和三维场景结构。将这些特征点轨迹组织成一个矩阵$M$，在理想情况下（刚性场景、无匹配错误），该矩阵应具有低秩结构$L$。然而，由于特征匹配错误或“回环检测” (loop closure) 时的错误关联，会产生稀疏的、大幅度的误差，构成[稀疏矩阵](@entry_id:138197)$S$。

标准PCP可以用于滤除这些错误。但更进一步，我们可以利用多视几何的先验知识来增强分解的准确性。例如，对于任意两帧图像，对应同一三维点的像素坐标之间必须满足“对极约束” (epipolar constraint)。这一几何约束可以被线性化，并构建一个线性算子$\mathcal{E}$，使得对于一个几何上一致的轨迹矩阵$L$，其对极残差$\mathcal{E}(L)$应该接近于零。我们可以将这个先验知识直接整合到PCP的[目标函数](@entry_id:267263)中：
$$
\min_{L,S} \ \|L\|_* + \lambda \|S\|_1 + \gamma \|\mathcal{E}(L)\|_2 \quad \text{subject to} \quad \|M - L - S\|_F \le \delta
$$
这里的$\gamma \|\mathcal{E}(L)\|_2$惩罚项鼓励恢复出的低秩分量$L$遵守场景的几何一致性。由于稀疏错误$S$（如错误的回环匹配）通常会严重违反这种几何约束，这个额外的惩罚项使得将这些不一致的结构归因于$L$的“代价”变得高昂，从而迫使优化器将它们更准确地分离到$S$中。这种领域知识的融入显著提高了模型的可识别性和鲁棒性 [@problem_id:3468058]。

#### 数据挖掘：鲁棒推荐系统

推荐系统的核心是预测用户对物品的评分，这通常通过填充一个巨大的、高度不完整的用户-物品[评分矩阵](@entry_id:172456)来完成。经典的[协同过滤](@entry_id:633903)方法假设这个[评分矩阵](@entry_id:172456)是近似低秩的，即用户的偏好由少数几个潜在因素（如电影的类型、导演风格）决定。因此，这个问题本质上是一个[矩阵填充](@entry_id:751752)问题。

然而，真实的评分数据还可能受到恶意攻击，例如“托儿”或“水军”通过刷分来操纵推荐结果（称为“托攻击” (shilling attacks)）。这些恶意评分可以被看作是幅度较大但数量稀少的稀疏损坏$S$。因此，一个更鲁棒的推荐系统模型需要从不完整的、含有稀疏异常值和随机噪声的观测数据$Y = \mathcal{P}_{\Omega}(L+S+W)$中恢复潜在的评分模式$L$。

这正是鲁棒[矩阵填充](@entry_id:751752)问题，可以使用[稳定PCP](@entry_id:755323)的变体来解决。与只考虑随机噪声的简单[矩阵填充](@entry_id:751752)模型（例如，使用Huber损失来抑制离群点）相比，PCP提供了一个更强大的框架。PCP的优势在于它明确地将数据分解为低秩“好”数据和稀疏“坏”数据两个部分。这不仅能更精确地恢复$L$（因为它利用了$L$的全局低秩结构来共享统计强度），还能显式地识别出哪些评分可能是恶意的（即$S$的非零项），这本身对于平台治理就具有重要价值。实验设计表明，通过系统地改变[采样率](@entry_id:264884)、秩和损坏密度，PCP在利用数据内在的低维结构方面表现出远超于仅使用局部[鲁棒损失函数](@entry_id:634784)的方法的样本效率和恢复精度 [@problem_id:3468077]。

#### 高级信号处理应用

**高[光谱](@entry_id:185632)图像解混与[异常检测](@entry_id:635137)**

[高光谱成像](@entry_id:750488)技术能为每个像素捕獲数百个窄波段的[光谱](@entry_id:185632)信息，在[遥感](@entry_id:149993)、医学成像等领域有广泛应用。一个核心任务是“[光谱解混](@entry_id:189588)” (spectral unmixing)，即确定每个像素是由哪些纯物质（称为“端元”，endmember）以及以何种比例（称为“丰度”，abundance）混合而成。在[线性混合模型](@entry_id:139702)下，整个高[光谱](@entry_id:185632)数据立方体（可重塑为“波段$\times$像素”的矩阵$M$）可以被建模为一个低秩矩阵$L=AE$，其中$A$是端元[光谱](@entry_id:185632)矩阵，$E$是丰度矩阵。如果端元数量$r$远小于波段数，那么$L$就是低秩的。

在此基础上，PCP框架可用于同时进行背景解混和[异常检测](@entry_id:635137)。例如，一片森林的背景反射[光谱](@entry_id:185632)构成了低秩的$L$，而一小片区域出现的伪装目标或气体泄露 plumes 则构成了稀疏的异常信号$S$。与普通图像不同，高[光谱](@entry_id:185632)数据具有明确的物理约束：[光谱](@entry_id:185632)反射率和物质丰度都是非负的。因此，$L$和$S$都应为非负矩阵。这促使我们使用带非负约束的PCP模型：
$$
\min_{L,S} \ \|L\|_* + \lambda \|S\|_1 \quad \text{subject to} \quad M=L+S, \ L \ge 0, \ S \ge 0
$$
引入非负性约束极大地增强了问题的可识别性。它严格限制了可行扰动的空间，禁止了通过正负抵消来“隐藏”信号的技巧。这使得$L$和$S$的分解变得更加明确，能有效防止稀疏的正信号“泄漏”到低秩分量中，从而提高了对稀疏异常目标的支持集恢复精度。理论上，非负性约束可以放宽对低秩分量非[相干性](@entry_id:268953)的要求，在更广泛的参数范围内实现精确恢复 [@problem_id:3468097]。

**高阶数据：张量[主成分追踪](@entry_id:753736)**

许多现代数据集本质上是高阶的，例如视频数据（高$\times$宽$\times$时间）、多模态医疗数据或社交[网络演化](@entry_id:260975)数据。将这[类数](@entry_id:156164)据强行“[矩阵化](@entry_id:751739)”会破坏其内在的高维结构。因此，将PCP从矩阵推广到张量是一个自然且重要的方向。

一个成功的推广是基于“张量奇异值分解” (t-SVD) 的框架。在该框架下，一个三阶张量$\mathcal{X} \in \mathbb{R}^{n_1 \times n_2 \times n_3}$被视为一个由$n_3$个$n_1 \times n_2$正面切片组成的结构。通过对张量的“管” (tubes，即沿第三维的向量) 进行傅里葉变换，张量间的卷积运算可以转化为傅里葉域中正面切片间的矩阵乘法。这套理论定义了张量的“管秩” (tubal rank) 以及其凸代理——“管状核范数” (Tubal Nuclear Norm, TNN)。TNN被定义为张量在傅里葉域中所有正面切片的核范数之和。

基于TNN，张量PC[P问题](@entry_id:267898)可以被表述为：
$$
\min_{\mathcal{L}, \mathcal{S}} \ \|\mathcal{L}\|_{\mathrm{TNN}} + \lambda \|\mathcal{S}\|_{1} \quad \text{subject to} \quad \mathcal{X} = \mathcal{L} + \mathcal{S}
$$
这个问题可以通过“交替方向乘子法” ([ADMM](@entry_id:163024)) 高效求解。[ADMM](@entry_id:163024)算法的核心步骤涉及两个[近端算子](@entry_id:635396)的计算。$\mathcal{S}$的更新是一个简单的元素级[软阈值](@entry_id:635249)操作。而$\mathcal{L}$的更新，即TNN的[近端算子](@entry_id:635396)，则可以通过一个“FFT-SVT-IFFT”的过程实现：首先对输入张量沿第三维进行快速傅里葉变换 (FFT)，然后在傅里葉域对每个正面切片独立地执行标准的矩阵[奇异值](@entry_id:152907)阈值 (SVT) 操作，最后通过逆FFT (IFFT) 变换回原始域。这种计算方案的复杂度主要由$n_3$次矩阵SVD和FFT的开销构成，大约是$O(n_3 \cdot n_1 n_2 \min\{n_1, n_2\} + n_1 n_2 n_3 \log n_3)$，使得张量PCP在计算上是可行的 [@problem_id:3468099]。这一扩展使得PCP能够直接处理和分析具有原生高阶结构的数据，在视频监控（背景建模与移动[物体检测](@entry_id:636829)）、动态MRI成像等领域取得了显著成功。