{"hands_on_practices": [{"introduction": "本练习旨在奠定理论基础，通过探究融合LASSO模型的核心数学原理，来揭示其惩罚项的工作机制。我们将利用次梯度最优性条件，精确地分析 $\\ell_1$ 范数如何诱导稀疏性，以及总变差（TV）惩罚项如何促进分段常数解。通过完成这项练习 [@problem_id:3447208]，您将从第一性原理的层面理解这两个惩罚项如何协同作用，共同塑造最终的估计结果。", "problem": "考虑融合最小绝对收缩和选择算子 (fused Least Absolute Shrinkage and Selection Operator, LASSO) 信号近似器，也称为带附加坐标级稀疏性惩罚的一维全变分 (Total Variation, TV) 降噪。设给定 $y \\in \\mathbb{R}^{n}$，定义优化问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{n}} \\; \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1},\n$$\n其中 $\\lambda_{1} \\ge 0$，$\\lambda_{2} \\ge 0$，且 $D \\in \\mathbb{R}^{(n-1)\\times n}$ 为一阶前向差分算子，其行满足 $(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$，对于 $i \\in \\{1,\\dots,n-1\\}$。在整个过程中，使用真闭凸函数 $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 在点 $x \\in \\mathbb{R}^{p}$ 处的次微分定义，\n$$\n\\partial g(x) := \\left\\{\\, v \\in \\mathbb{R}^{p} \\; : \\; g(z) \\ge g(x) + \\langle v, z - x\\rangle \\;\\; \\text{for all } z \\in \\mathbb{R}^{p} \\,\\right\\},\n$$\n以及无约束凸最小化的充要最优性条件，即 $0 \\in \\partial f(x^{\\star})$，其中 $f$ 是凸函数，$x^{\\star}$ 是一个极小点。\n\n1) 从上述次微分定义出发，以坐标形式写出 $\\ell_{1}$ 范数 $\\|\\beta\\|_{1}$ 在任意点 $\\beta \\in \\mathbb{R}^{n}$ 处的次微分。\n\n2) 使用凸目标函数的最优性条件和和的次微分性质，推导出一个在任何极小点 $\\beta^{\\star}$ 处必须成立的坐标级包含关系：\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2}(D^{\\top}v^{\\star})_{i} \\quad \\text{for each } i \\in \\{1,\\dots,n\\},\n$$\n对于某个 $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ 和某个 $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$。利用此关系证明，对于任意固定的 $v \\in \\mathbb{R}^{n-1}$，坐标 $\\beta_{i}^{\\star}$ 可以写成一个一维强凸问题的解，该解具有依赖于 $y_{i}$、$\\lambda_{1}$ 和 $(D^{\\top}v)_{i}$ 的唯一闭式形式，并得出 $\\beta_{i}^{\\star} = 0$ 精确成立的充要条件。\n\n3) 特化到 $n=4$ 的情况，并假设在一个最优解处，TV 次梯度分量满足 $v_{1}^{\\star} = \\frac{1}{2}$，$v_{2}^{\\star} = -\\frac{1}{3}$，以及作为 $D^{\\top}$ 边界条件的 $v_{0}^{\\star} = 0$，$v_{4}^{\\star} = 0$。请提供一个关于 $y_{2}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单一闭式解析表达式来表示 $\\beta_{2}^{\\star}$，仅使用诸如 $\\operatorname{sign}$、$\\max$ 和绝对值之类的初等函数。你的最终答案必须是一个依赖于 $y_{2}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单一解析表达式，不含不等式和分段情况。无需四舍五入。", "solution": "我们从凸分析的基本原理开始。一个真闭凸函数 $g:\\mathbb{R}^{p}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ 在点 $x$ 处的次微分定义为\n$$\n\\partial g(x) := \\{\\, v \\in \\mathbb{R}^{p} : g(z) \\ge g(x) + \\langle v, z-x\\rangle \\text{ for all } z \\in \\mathbb{R}^{p} \\,\\}.\n$$\n对于真闭凸函数之和 $f = f_{1}+f_{2}$，一个标准的、经过充分检验的次微分求和法则表明\n$$\n\\partial f(x) \\subseteq \\partial f_{1}(x) + \\partial f_{2}(x),\n$$\n等式在弱正则性条件下成立，例如其中一个加项在 $x$ 处连续。对于无约束凸最小化，点 $x^{\\star}$ 是 $f$ 的极小点当且仅当 $0 \\in \\partial f(x^{\\star})$。\n\n步骤 1：$\\ell_{1}$ 范数的次微分。考虑 $g(\\beta) = \\|\\beta\\|_{1} = \\sum_{i=1}^{n} |\\beta_{i}|$。其次微分是按坐标可分的：\n$$\n\\partial \\|\\beta\\|_{1} = \\prod_{i=1}^{n} \\partial |\\beta_{i}|,\n$$\n其中，对于标量 $t \\in \\mathbb{R}$，绝对值的次微分是\n$$\n\\partial |t| = \n\\begin{cases}\n\\{ \\operatorname{sign}(t) \\},  \\text{if } t \\neq 0,\\\\\n[-1,1],  \\text{if } t = 0.\n\\end{cases}\n$$\n因此，以坐标形式表示，\n$$\n\\partial \\|\\beta\\|_{1} = \\left\\{\\, u \\in \\mathbb{R}^{n} : u_{i} =\n\\begin{cases}\n\\operatorname{sign}(\\beta_{i}),  \\text{if } \\beta_{i} \\neq 0,\\\\\n\\xi \\text{ with } \\xi \\in [-1,1],  \\text{if } \\beta_{i} = 0,\n\\end{cases}\n\\text{ for all } i \\in \\{1,\\dots,n\\}\\,\\right\\}.\n$$\n\n步骤 2：融合最小绝对收缩和选择算子 (LASSO) 目标函数的最优性。定义目标函数\n$$\nF(\\beta) := \\frac{1}{2}\\|\\beta - y\\|_{2}^{2} + \\lambda_{1}\\|\\beta\\|_{1} + \\lambda_{2}\\|D\\beta\\|_{1}.\n$$\n第一项是光滑的，其梯度为 $\\nabla \\left(\\frac{1}{2}\\|\\beta - y\\|_{2}^{2}\\right) = \\beta - y$。第二项和第三项是凸的，但通常非光滑。根据次微分求和法则，\n$$\n\\partial F(\\beta) = (\\beta - y) + \\lambda_{1}\\,\\partial \\|\\beta\\|_{1} + \\lambda_{2}\\, D^{\\top} \\partial \\|D\\beta\\|_{1},\n$$\n其中我们使用了线性映射次微分的链式法则：$\\partial \\|D\\beta\\|_{1} = D^{\\top} \\partial \\|z\\|_{1}\\big|_{z = D\\beta}$。因此，在极小点 $\\beta^{\\star}$ 处，存在 $u^{\\star} \\in \\partial \\|\\beta^{\\star}\\|_{1}$ 和 $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$，使得充要最优性条件成立：\n$$\n0 \\in \\beta^{\\star} - y + \\lambda_{1} u^{\\star} + \\lambda_{2} D^{\\top} v^{\\star}.\n$$\n等价地，在坐标上，对于每个 $i \\in \\{1,\\dots,n\\}$，\n$$\n0 \\in \\beta_{i}^{\\star} - y_{i} + \\lambda_{1} u_{i}^{\\star} + \\lambda_{2} (D^{\\top} v^{\\star})_{i}.\n$$\n因为 $D$ 是一阶前向差分，$(D\\beta)_{i} = \\beta_{i+1} - \\beta_{i}$ 对于 $i \\in \\{1,\\dots,n-1\\}$。直接计算表明\n$$\nD^{\\top} v = \\begin{bmatrix}\n- v_{1} \\\\\nv_{1} - v_{2} \\\\\n\\vdots \\\\\nv_{n-2} - v_{n-1} \\\\\nv_{n-1}\n\\end{bmatrix},\n$$\n因此对于内部指标 $i \\in \\{2,\\dots,n-1\\}$ 有 $(D^{\\top} v)_{i} = v_{i-1} - v_{i}$，而在边界处 $(D^{\\top} v)_{1} = -v_{1}$，$(D^{\\top} v)_{n} = v_{n-1}$。\n\n固定任意 $v \\in \\mathbb{R}^{n-1}$ 并定义平移后的数据\n$$\nz_{i} := y_{i} - \\lambda_{2} (D^{\\top} v)_{i}, \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\n那么坐标级最优性包含关系变为\n$$\n0 \\in \\beta_{i}^{\\star} - z_{i} + \\lambda_{1} u_{i}^{\\star}, \\quad \\text{with } u_{i}^{\\star} \\in \\partial |\\beta_{i}^{\\star}|.\n$$\n这正是在单变量 $b$ 中的标量问题的一阶最优性条件，\n$$\n\\min_{b \\in \\mathbb{R}} \\; \\frac{1}{2}(b - z_{i})^{2} + \\lambda_{1} |b|.\n$$\n该问题有一个由软阈值操作给出的唯一闭式解，\n$$\nb^{\\star} = \\operatorname{sign}(z_{i}) \\max\\left(|z_{i}| - \\lambda_{1}, 0\\right).\n$$\n因此，对于任何固定的 $v$，\n$$\n\\beta_{i}^{\\star} = \\operatorname{sign}\\!\\left(y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right) \\max\\!\\left(\\left|y_{i} - \\lambda_{2}(D^{\\top} v)_{i}\\right| - \\lambda_{1}, 0\\right).\n$$\n从这个表示中，$\\beta_{i}^{\\star} = 0$ 当且仅当阈值条件\n$$\n\\left|y_{i} - \\lambda_{2}(D^{\\top} v^{\\star})_{i}\\right| \\le \\lambda_{1}\n$$\n在 $v^{\\star} \\in \\partial \\|D\\beta^{\\star}\\|_{1}$ 的情况下成立。这精确地展示了即使在存在 TV 耦合的情况下，也可能出现精确的零：相邻差分次梯度 $(D^{\\top} v^{\\star})_{i}$ 在 $\\ell_{1}$ 收缩之前平移了有效数据 $y_{i}$，如果平移后的大小至多为 $\\lambda_{1}$，则该坐标为零。\n\n步骤 3：特化到 $n=4$ 及所要求的表达式。对于 $n=4$，假设最优 TV 次梯度满足 $v_{1}^{\\star} = \\frac{1}{2}$，$v_{2}^{\\star} = -\\frac{1}{3}$，以及边界条件 $v_{0}^{\\star} = 0$，$v_{4}^{\\star} = 0$。对于 $i=2$（一个内部指标），我们有\n$$\n(D^{\\top} v^{\\star})_{2} = v_{1}^{\\star} - v_{2}^{\\star} = \\frac{1}{2} - \\left(-\\frac{1}{3}\\right) = \\frac{5}{6}.\n$$\n因此，\n$$\n\\beta_{2}^{\\star} = \\operatorname{sign}\\!\\left(y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right) \\max\\!\\left(\\left|y_{2} - \\lambda_{2}\\cdot \\frac{5}{6}\\right| - \\lambda_{1}, 0\\right).\n$$\n这是一个关于变量 $y_{2}$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 的单一闭式解析表达式，它通过外部的 $\\max$ 隐式地编码了精确零条件。", "answer": "$$\\boxed{\\operatorname{sign}\\!\\left(y_{2} - \\frac{5}{6}\\lambda_{2}\\right)\\,\\max\\!\\left(\\left|y_{2} - \\frac{5}{6}\\lambda_{2}\\right| - \\lambda_{1},\\, 0\\right)}$$", "id": "3447208"}, {"introduction": "在掌握了一般理论之后，让我们来探讨一个具有简洁解析解的重要特例。本练习聚焦于去噪背景下的融合LASSO问题（即设计矩阵为单位矩阵），其解可以通过一个优美的两步过程求得：首先执行总变差去噪，然后应用软阈值算子。这项练习 [@problem_id:3447151] 不仅为这一常见场景提供了一种实用的求解方法，还加深了对两个惩罚项之间内在联系的理解。", "problem": "考虑使用单位设计矩阵的一维融合最小绝对收缩和选择算子 (LASSO) 信号近似器。设观测向量为 $y \\in \\mathbb{R}^{6}$，其分量为 $y_{1}=3$，$y_{2}=3$，$y_{3}=3$，$y_{4}=0$，$y_{5}=0$，$y_{6}=2$。估计量 $\\hat{\\beta} \\in \\mathbb{R}^{6}$ 定义为严格凸目标函数的唯一极小值点：\n$$\n\\frac{1}{2}\\| \\beta - y \\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\sum_{i=1}^{5} |\\beta_{i+1} - \\beta_{i}|,\n$$\n其中 $\\lambda_{1} = \\frac{1}{2}$ 且 $\\lambda_{2} = \\frac{1}{5}$。全变分算子是一阶离散差分，融合 LASSO 惩罚项是相邻系数绝对差之和。\n\n从凸优化的基本最优性条件（特别是次梯度 Karush–Kuhn–Tucker (KKT) 条件）出发，根据第一性原理推导，在单位设计矩阵设置下，为什么可以首先求解全变分 (TV) 去噪子问题\n$$\n\\min_{u \\in \\mathbb{R}^{6}} \\;\\frac{1}{2}\\|u - y\\|_{2}^{2} + \\lambda_{2} \\sum_{i=1}^{5} |u_{i+1} - u_{i}|\n$$\n以获得一个分段常数的初步信号 $u$，然后通过对 $u$ 逐点应用软阈值算子来获得 $\\hat{\\beta}$，该算子对每个坐标定义为 $S_{\\lambda_{1}}(x) = \\mathrm{sign}(x)\\max\\{|x| - \\lambda_{1}, 0\\}$。\n\n按以下步骤进行：\n- 通过次梯度最优性验证跳跃符号的一致性，从而识别 TV 解 $u$ 中的常数段。对于每个识别出的段，将其恒定平台值表示为 $c$。\n- 利用每个段上的次梯度平衡（通过对该段的 KKT 平稳性条件求和得出），求解平台值 $c$ 作为精确有理数。\n- 将软阈值算子 $S_{\\lambda_{1}}$ 应用于 $u$ 的分量，以获得融合 LASSO 估计量 $\\hat{\\beta}$。\n\n将最终答案表示为包含 $\\hat{\\beta}$ 的六个分量的单个行向量，形式为精确有理数。无需四舍五入，也不涉及单位。", "solution": "该问题要求解一个融合 LASSO 优化问题，并基于凸优化的第一性原理来证明该解法的合理性。\n\n需要最小化的目标函数是\n$$\nL(\\beta) = \\frac{1}{2}\\| \\beta - y \\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\sum_{i=1}^{5} |\\beta_{i+1} - \\beta_{i}|\n$$\n其中 $\\beta \\in \\mathbb{R}^{6}$，$y = (3, 3, 3, 0, 0, 2)^T$，$\\lambda_{1} = \\frac{1}{2}$ 且 $\\lambda_{2} = \\frac{1}{5}$。目标函数是严格凸的，因此存在唯一的极小值点 $\\hat{\\beta}$。\n\n极小值点 $\\hat{\\beta}$ 的一阶充要最优性条件是，零向量必须是 $L(\\beta)$ 在 $\\hat{\\beta}$ 处的次微分的元素：\n$$\n0 \\in \\partial L(\\hat{\\beta})\n$$\n根据次微分的求和法则，这等价于\n$$\n0 \\in (\\hat{\\beta} - y) + \\partial \\left(\\lambda_{1} \\|\\hat{\\beta}\\|_{1}\\right) + \\partial \\left(\\lambda_{2} \\sum_{i=1}^{5} |\\hat{\\beta}_{i+1} - \\hat{\\beta}_{i}|\\right)\n$$\n我们将两个惩罚项表示为 $g(\\beta) = \\lambda_1 \\|\\beta\\|_1$ 和 $h(\\beta) = \\lambda_2 \\sum_{i=1}^{5} |\\beta_{i+1} - \\beta_{i}|$。最优性条件是\n$$\ny - \\hat{\\beta} \\in \\partial g(\\hat{\\beta}) + \\partial h(\\hat{\\beta})\n$$\n\n该问题要求证明一个两步法的合理性。首先，我们求解全变分 (TV) 去噪问题，得到一个中间向量 $u$：\n$$\nu = \\arg\\min_{u' \\in \\mathbb{R}^6} \\left\\{ \\frac{1}{2}\\|u' - y\\|_{2}^{2} + h(u') \\right\\}\n$$\n该子问题的最优性条件是 $0 \\in (u - y) + \\partial h(u)$，可以写成 $y - u \\in \\partial h(u)$。设 $s_h \\in \\partial h(u)$ 是一个次梯度，使得 $y - u = s_h$。\n\n其次，我们通过求解以下问题来获得 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} = \\arg\\min_{\\beta' \\in \\mathbb{R}^6} \\left\\{ \\frac{1}{2}\\|\\beta' - u\\|_{2}^{2} + g(\\beta') \\right\\}\n$$\n这第二个子问题的最优性条件是 $0 \\in (\\hat{\\beta} - u) + \\partial g(\\hat{\\beta})$，可以写成 $u - \\hat{\\beta} \\in \\partial g(\\hat{\\beta})$。设 $s_g \\in \\partial g(\\hat{\\beta})$ 是一个次梯度，使得 $u - \\hat{\\beta} = s_g$。这第二步对应于将软阈值算子 $S_{\\lambda_1}$ 逐分量地应用于 $u$。\n\n将 $u = \\hat{\\beta} + s_g$ 代入第一个最优性条件，得到 $y - (\\hat{\\beta} + s_g) = s_h$，整理后为 $y - \\hat{\\beta} = s_g + s_h$。这与原始融合 LASSO 问题的最优性条件相符，前提是次梯度 $s_h \\in \\partial h(u)$ 也是 $\\partial h(\\hat{\\beta})$ 中的一个有效次梯度。$h(\\beta)$ 的次梯度取决于差值 $\\beta_{i+1}-\\beta_i$ 的符号。软阈值算子 $S_{\\lambda_1}(x)$ 是 $x$ 的一个非减函数。这意味着对于任意的 $u_i, u_{i+1}$，我们有 $\\mathrm{sign}(\\hat{\\beta}_{i+1} - \\hat{\\beta}_i) = \\mathrm{sign}(S_{\\lambda_1}(u_{i+1}) - S_{\\lambda_1}(u_i)) \\in \\{0, \\mathrm{sign}(u_{i+1}-u_i)\\}$。对于任意形如 $v_i \\in \\partial|u_{i+1}-u_i|$ 的次梯度，这个 $v_i$ 也是 $\\partial|\\hat{\\beta}_{i+1}-\\hat{\\beta}_i|$ 的一个有效元素，因为如果符号保持不变，次微分是相同的单点集；如果差值变为零，次微分扩展为 $[-1,1]$，其中包含了原始符号。因此，任何来自 $\\partial h(u)$ 的次梯度也存在于 $\\partial h(\\hat{\\beta})$ 中。所以该分解是有效的。\n\n**步骤 1：求解 TV 去噪子问题**\n我们求解 $\\min_{u \\in \\mathbb{R}^{6}} \\frac{1}{2}\\|u - y\\|_{2}^{2} + \\lambda_{2} \\sum_{i=1}^{5} |u_{i+1} - u_{i}|$，其中 $\\lambda_2 = \\frac{1}{5}$。最优解 $u$ 的 KKT 条件是：\n$$\nu_i - y_i + \\lambda_2 (v_{i-1} - v_i) = 0, \\quad \\text{for } i=1, \\dots, 6\n$$\n其中对于 $i=1,\\dots,5$，$v_i \\in \\partial|u_{i+1}-u_i|$，并且按照约定 $v_0 = v_6 = 0$。\n已知解 $u$ 是分段常数的。观察 $y = (3, 3, 3, 0, 0, 2)^T$，我们假设解具有三个常数段的结构：$u_1=u_2=u_3=c_1$，$u_4=u_5=c_2$ 和 $u_6=c_3$。这意味着在索引 3 和 5 处可能存在跳跃。条件 $u_1=u_2=u_3$ 意味着 $v_1, v_2 \\in [-1, 1]$。条件 $u_4=u_5$ 意味着 $v_4 \\in [-1,1]$。跳跃意味着 $u_4 \\neq u_3$ 且 $u_6 \\neq u_5$。我们设置 $v_3 = \\mathrm{sign}(u_4-u_3)$ 和 $v_5=\\mathrm{sign}(u_6-u_5)$。基于 $y$，我们假设 $c_1 > c_2$ 和 $c_3 > c_2$，因此我们测试 $v_3=-1$ 和 $v_5=1$。\n\n我们通过对每个段上的 KKT 条件求和来找到平台值。\n对于段 1 ($i=1, 2, 3$):\n$$\n\\sum_{i=1}^3 (u_i - y_i) + \\lambda_2 \\sum_{i=1}^3 (v_{i-1}-v_i) = 0 \\implies (3c_1 - (y_1+y_2+y_3)) + \\lambda_2 (v_0-v_3) = 0\n$$\n$$\n3c_1 - 9 + \\lambda_2 (0 - v_3) = 0 \\implies 3c_1 - 9 - \\lambda_2 v_3 = 0\n$$\n当 $\\lambda_2 = \\frac{1}{5}$ 且 $v_3 = -1$ 时：$3c_1 - 9 - \\frac{1}{5}(-1)=0 \\implies 3c_1 = 9 - \\frac{1}{5} = \\frac{44}{5} \\implies c_1 = \\frac{44}{15}$。\n\n对于段 2 ($i=4, 5$):\n$$\n\\sum_{i=4}^5 (u_i - y_i) + \\lambda_2 \\sum_{i=4}^5 (v_{i-1}-v_i) = 0 \\implies (2c_2 - (y_4+y_5)) + \\lambda_2 (v_3-v_5) = 0\n$$\n$$\n2c_2 - 0 + \\lambda_2 (v_3-v_5) = 0\n$$\n当 $v_3=-1$ 且 $v_5=1$ 时：$2c_2 + \\frac{1}{5}(-1-1) = 0 \\implies 2c_2 = \\frac{2}{5} \\implies c_2 = \\frac{1}{5}$。\n\n对于段 3 ($i=6$):\n$$\n(u_6 - y_6) + \\lambda_2 (v_5-v_6) = 0 \\implies c_3 - y_6 + \\lambda_2 v_5 = 0\n$$\n$$\nc_3 - 2 + \\frac{1}{5}(1) = 0 \\implies c_3 = 2 - \\frac{1}{5} = \\frac{9}{5}.\n$$\n我们的假设 $c_1 > c_2$ ($\\frac{44}{15} > \\frac{3}{15}$) 和 $c_3 > c_2$ ($\\frac{27}{15} > \\frac{3}{15}$) 与结果一致。\n\n我们必须验证常数段的次梯度条件。我们使用单个 KKT 方程求解 $v_1, v_2, v_4$：\n$u_i - y_i = \\lambda_2 (v_i - v_{i-1})$\n对于 $i=1$: $u_1-y_1 = \\lambda_2 v_1 \\implies \\frac{44}{15}-3 = \\frac{1}{5}v_1 \\implies -\\frac{1}{15} = \\frac{1}{5}v_1 \\implies v_1 = -\\frac{1}{3}$。由于 $|v_1| \\le 1$，这是有效的。\n对于 $i=2$: $u_2-y_2 = \\lambda_2(v_2-v_1) \\implies \\frac{44}{15}-3 = \\frac{1}{5}(v_2 - (-\\frac{1}{3})) \\implies -\\frac{1}{15}=\\frac{1}{5}(v_2+\\frac{1}{3}) \\implies -\\frac{1}{3}=v_2+\\frac{1}{3} \\implies v_2 = -\\frac{2}{3}$。由于 $|v_2|\\le 1$，这是有效的。\n对于 $i=4$: $u_4-y_4 = \\lambda_2(v_4-v_3) \\implies \\frac{1}{5}-0 = \\frac{1}{5}(v_4 - (-1)) \\implies 1 = v_4+1 \\implies v_4 = 0$。由于 $|v_4|\\le 1$，这是有效的。\n所有条件均已满足。\nTV 去噪解为 $u = (\\frac{44}{15}, \\frac{44}{15}, \\frac{44}{15}, \\frac{1}{5}, \\frac{1}{5}, \\frac{9}{5})^T$。\n\n**步骤 2：应用软阈值算子**\n我们通过将软阈值算子 $S_{\\lambda_1}(x) = \\mathrm{sign}(x)\\max\\{|x| - \\lambda_{1}, 0\\}$（其中 $\\lambda_1 = \\frac{1}{2}$）应用于 $u$ 的每个分量来获得最终估计量 $\\hat{\\beta}$。\n由于 $u$ 的所有分量均为正，$\\hat{\\beta}_i = \\max(u_i - \\lambda_1, 0)$。\n\n对于 $i=1, 2, 3$：\n$$\n\\hat{\\beta}_i = u_1 - \\lambda_1 = \\frac{44}{15} - \\frac{1}{2} = \\frac{88 - 15}{30} = \\frac{73}{30}\n$$\n对于 $i=4, 5$：\n$$\nu_4 - \\lambda_1 = \\frac{1}{5} - \\frac{1}{2} = \\frac{2-5}{10} = -\\frac{3}{10}\n$$\n由于结果为负，$\\hat{\\beta}_i = \\max(u_i - \\lambda_1, 0) = 0$。\n\n对于 $i=6$：\n$$\n\\hat{\\beta}_6 = u_6 - \\lambda_1 = \\frac{9}{5} - \\frac{1}{2} = \\frac{18-5}{10} = \\frac{13}{10}\n$$\n融合 LASSO 估计量为 $\\hat{\\beta} = (\\frac{73}{30}, \\frac{73}{30}, \\frac{73}{30}, 0, 0, \\frac{13}{10})^T$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{73}{30}  \\frac{73}{30}  \\frac{73}{30}  0  0  \\frac{13}{10}\n\\end{pmatrix}\n}\n$$", "id": "3447151"}, {"introduction": "尽管解析解能提供深刻的洞见，但大多数现实世界的问题都需要强大的数值算法来求解。本练习将介绍交替方向乘子法（ADMM），这是一种用于求解如融合LASSO这类结构化凸优化问题的通用且广泛应用的算法。通过手动进行一次完整的数值迭代 [@problem_id:3447147]，您将获得操作该算法核心机制的具体经验，从而搭建起从理论目标函数到实际计算的桥梁。", "problem": "考虑融合最小绝对收缩和选择算子 (LASSO) 问题，该问题在最小二乘数据保真项的基础上，增加了元素稀疏性和一维全变分稀疏性。融合 LASSO 的目标函数为\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D \\beta\\|_{1},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是观测数据，$D \\in \\mathbb{R}^{(p-1) \\times p}$ 是编码一维全变分的一阶前向差分算子。交替方向乘子法 (ADMM) 是一种用于结构化凸优化的分解方法。引入辅助变量 $z \\in \\mathbb{R}^{p}$ 和 $s \\in \\mathbb{R}^{p-1}$ 以分离非光滑项，并满足线性约束 $z = \\beta$ 和 $s = D \\beta$。从缩放增广拉格朗日原理出发，构建原始变量 $\\beta, z, s$ 和缩放对偶变量 $u, v$ 的 ADMM 更新式，然后对下面给定的实例进行一次完整的数值 ADMM 迭代。\n\n使用 $n=p=4$，设置\n$$\nX = I_{4}, \\quad y = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}, \\quad \\lambda_{1} = 1, \\quad \\lambda_{2} = 1, \\quad \\rho = 1,\n$$\n并令前向差分矩阵为\n$$\nD = \\begin{pmatrix}\n-1  1  0  0 \\\\\n0  -1  1  0 \\\\\n0  0  -1  1\n\\end{pmatrix}.\n$$\n将辅助变量和缩放对偶变量初始化为零，\n$$\nz^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad s^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad u^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad v^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n从增广拉格朗日量和一阶最优性原理出发，推导 ADMM 更新方程，并对给定实例数值计算第一次迭代的值 $\\beta^{(1)}, z^{(1)}, s^{(1)}, u^{(1)}, v^{(1)}$。然后构造堆叠的原始约束违反向量\n$$\nr^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D \\beta^{(1)} - s^{(1)} \\end{pmatrix} \\in \\mathbb{R}^{7},\n$$\n并计算其欧几里得范数的平方 $\\|r^{(1)}\\|_{2}^{2}$。提供 $\\|r^{(1)}\\|_{2}^{2}$ 的精确值作为最终答案。无需四舍五入。", "solution": "该问题要求推导交替方向乘子法 (ADMM) 并将其应用于融合 LASSO 目标函数。我们将首先验证问题陈述，然后推导通用的 ADMM 更新式，最后使用给定的数值数据应用这些更新式进行一次迭代，以计算所需的量。\n\n### 问题验证\n**步骤 1：提取已知条件**\n- 目标函数：$\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|\\beta\\|_{1} + \\lambda_{2} \\|D \\beta\\|_{1}$\n- 维度：$n=p=4$\n- 数据：$y = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}$，$X = I_{4}$\n- 正则化参数：$\\lambda_{1} = 1$，$\\lambda_{2} = 1$\n- ADMM 惩罚参数：$\\rho = 1$\n- 差分算子：$D = \\begin{pmatrix} -1  1  0  0 \\\\ 0  -1  1  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 4}$\n- 初始条件：$z^{(0)} = \\mathbf{0} \\in \\mathbb{R}^4$，$s^{(0)} = \\mathbf{0} \\in \\mathbb{R}^3$，$u^{(0)} = \\mathbf{0} \\in \\mathbb{R}^4$，$v^{(0)} = \\mathbf{0} \\in \\mathbb{R}^3$\n- 用于 ADMM 分离的约束：$z = \\beta$，$s = D \\beta$\n- 目标量：$\\|r^{(1)}\\|_{2}^{2}$，其中 $r^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D \\beta^{(1)} - s^{(1)} \\end{pmatrix}$\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题具有科学依据，是适定、客观且完整的。它描述了一个标准的融合 LASSO 正则化回归问题，这是稀疏优化和压缩感知领域的一个基本课题。所选的求解方法 ADMM 是针对此问题结构的标准且合适的算法。所提供的数据在数学上是一致的，并且足以执行所要求的计算。该问题有效。\n\n### ADMM 公式化与推导\n\n原始问题等价于以下约束优化问题：\n$$\n\\min_{\\beta, z, s} \\ \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1} \\|z\\|_{1} + \\lambda_{2} \\|s\\|_{1} \\quad \\text{subject to} \\quad \\beta - z = 0, \\ D\\beta - s = 0.\n$$\n该问题的缩放增广拉格朗日量 $\\mathcal{L}_{\\rho}$ 为：\n$$\n\\mathcal{L}_{\\rho}(\\beta, z, s, u, v) = \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\lambda_{1}\\|z\\|_{1} + \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|\\beta - z + u\\|_{2}^{2} + \\frac{\\rho}{2}\\|D\\beta - s + v\\|_{2}^{2} - \\frac{\\rho}{2}\\|u\\|_{2}^{2} - \\frac{\\rho}{2}\\|v\\|_{2}^{2}\n$$\n其中 $u$ 和 $v$ 是缩放对偶变量。ADMM 算法交替地对原始变量 $\\beta, z, s$ 最小化 $\\mathcal{L}_{\\rho}$，然后更新对偶变量 $u, v$。\n\n第 $(k+1)$ 次迭代包括以下步骤：\n1.  **$\\beta$-最小化：**\n    $\\beta^{(k+1)} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|y - X \\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - z^{(k)} + u^{(k)}\\|_{2}^{2} + \\frac{\\rho}{2}\\|D\\beta - s^{(k)} + v^{(k)}\\|_{2}^{2} \\right)$\n    这是一个关于 $\\beta$ 的二次目标函数。一阶最优性条件（令关于 $\\beta$ 的梯度为零）产生一个线性系统：\n    $$\n    X^T(X\\beta - y) + \\rho(\\beta - z^{(k)} + u^{(k)}) + D^T\\rho(D\\beta - s^{(k)} + v^{(k)}) = 0\n    $$\n    $$\n    (X^TX + \\rho I + \\rho D^TD)\\beta = X^Ty + \\rho(z^{(k)} - u^{(k)}) + \\rho D^T(s^{(k)} - v^{(k)})\n    $$\n    更新式为 $\\beta^{(k+1)} = (X^TX + \\rho(I + D^TD))^{-1} (X^Ty + \\rho(z^{(k)} - u^{(k)}) + \\rho D^T(s^{(k)} - v^{(k)}))$。\n\n2.  **$z$-最小化：**\n    $z^{(k+1)} = \\arg\\min_{z} \\left( \\lambda_{1}\\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta^{(k+1)} - z + u^{(k)}\\|_{2}^{2} \\right) = \\arg\\min_{z} \\left( \\lambda_{1}\\|z\\|_{1} + \\frac{\\rho}{2}\\|z - (\\beta^{(k+1)} + u^{(k)})\\|_{2}^{2} \\right)$\n    这是 $\\ell_1$-范数的近端算子，即软阈值算子 $S_{\\kappa}(\\cdot)$：\n    $z^{(k+1)} = S_{\\lambda_1/\\rho}(\\beta^{(k+1)} + u^{(k)})$，其中 $(S_{\\kappa}(a))_i = \\text{sign}(a_i) \\max(|a_i| - \\kappa, 0)$。\n\n3.  **$s$-最小化：**\n    $s^{(k+1)} = \\arg\\min_{s} \\left( \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|D\\beta^{(k+1)} - s + v^{(k)}\\|_{2}^{2} \\right) = \\arg\\min_{s} \\left( \\lambda_{2}\\|s\\|_{1} + \\frac{\\rho}{2}\\|s - (D\\beta^{(k+1)} + v^{(k)})\\|_{2}^{2} \\right)$\n    类似地，这也通过软阈值求解：\n    $s^{(k+1)} = S_{\\lambda_2/\\rho}(D\\beta^{(k+1)} + v^{(k)})$。\n\n4.  **对偶变量更新：**\n    $u^{(k+1)} = u^{(k)} + \\beta^{(k+1)} - z^{(k+1)}$\n    $v^{(k+1)} = v^{(k)} + D\\beta^{(k+1)} - s^{(k+1)}$\n\n### 第一次 ADMM 迭代 (k=0)\n\n我们使用给定的初始条件 $z^{(0)}=\\mathbf{0}, s^{(0)}=\\mathbf{0}, u^{(0)}=\\mathbf{0}, v^{(0)}=\\mathbf{0}$ 和参数 $\\lambda_1=\\lambda_2=\\rho=1, X=I_4$ 进行一次迭代。\n\n**1. 计算 $\\beta^{(1)}$**\n更新方程简化为：\n$$\n\\beta^{(1)} = (I_4^T I_4 + 1(I_4 + D^TD))^{-1} (I_4^T y + 1(\\mathbf{0} - \\mathbf{0}) + 1D^T(\\mathbf{0} - \\mathbf{0})) = (2I_4 + D^TD)^{-1} y\n$$\n首先，我们计算矩阵 $D^TD$：\n$$\nD^T = \\begin{pmatrix} -1  0  0 \\\\ 1  -1  0 \\\\ 0  1  -1 \\\\ 0  0  1 \\end{pmatrix}, \\quad D^TD = \\begin{pmatrix} 1  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  1 \\end{pmatrix}\n$$\n需要求逆的矩阵是 $A = 2I_4 + D^TD$：\n$$\nA = 2\\begin{pmatrix} 1000\\\\0100\\\\0010\\\\0001 \\end{pmatrix} + \\begin{pmatrix} 1  -1  0  0 \\\\ -1  2  -1  0 \\\\ 0  -1  2  -1 \\\\ 0  0  -1  1 \\end{pmatrix} = \\begin{pmatrix} 3  -1  0  0 \\\\ -1  4  -1  0 \\\\ 0  -1  4  -1 \\\\ 0  0  -1  3 \\end{pmatrix}\n$$\n我们求解线性系统 $A\\beta^{(1)} = y$：\n$$\n\\begin{pmatrix} 3  -1  0  0 \\\\ -1  4  -1  0 \\\\ 0  -1  4  -1 \\\\ 0  0  -1  3 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\beta_4 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -3 \\\\ 9 \\\\ -5 \\end{pmatrix}\n$$\n解此系统可得：\n$\\beta^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix}$。\n\n**2. 计算 $z^{(1)}$**\n$z^{(1)} = S_{\\lambda_1/\\rho}(\\beta^{(1)} + u^{(0)}) = S_{1/1}(\\beta^{(1)} + \\mathbf{0}) = S_1(\\beta^{(1)})$。\n当 $\\beta^{(1)} = (1, 0, 2, -1)^T$ 且阈值为 $\\kappa=1$ 时：\n- $z_1^{(1)} = S_1(1) = \\text{sign}(1)\\max(|1|-1, 0) = 0$\n- $z_2^{(1)} = S_1(0) = \\text{sign}(0)\\max(|0|-1, 0) = 0$\n- $z_3^{(1)} = S_1(2) = \\text{sign}(2)\\max(|2|-1, 0) = 1$\n- $z_4^{(1)} = S_1(-1) = \\text{sign}(-1)\\max(|-1|-1, 0) = 0$\n所以，$z^{(1)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$。\n\n**3. 计算 $s^{(1)}$**\n$s^{(1)} = S_{\\lambda_2/\\rho}(D\\beta^{(1)} + v^{(0)}) = S_{1/1}(D\\beta^{(1)} + \\mathbf{0}) = S_1(D\\beta^{(1)})$。\n首先，计算参数 $D\\beta^{(1)}$：\n$$\nD\\beta^{(1)} = \\begin{pmatrix} -1  1  0  0 \\\\ 0  -1  1  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1(1) + 1(0) \\\\ 0(1) - 1(0) + 1(2) \\\\ 0(1) + 0(0) -1(2) + 1(-1) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\\\ -3 \\end{pmatrix}\n$$\n现在，应用软阈值法，其中 $\\kappa=1$：\n- $s_1^{(1)} = S_1(-1) = 0$\n- $s_2^{(1)} = S_1(2) = 1$\n- $s_3^{(1)} = S_1(-3) = -2$\n所以，$s^{(1)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix}$。\n\n作为完整迭代描述的一部分，题目也要求计算 $u^{(1)}$ 和 $v^{(1)}$。\n**4. 计算 $u^{(1)}$ 和 $v^{(1)}$**\n$u^{(1)} = u^{(0)} + \\beta^{(1)} - z^{(1)} = \\mathbf{0} + \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n$v^{(1)} = v^{(0)} + D\\beta^{(1)} - s^{(1)} = \\mathbf{0} + \\begin{pmatrix} -1 \\\\ 2 \\\\ -3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n\n### 计算最终量\n\n问题要求计算堆叠的原始约束违反向量的欧几里得范数的平方，即 $\\|r^{(1)}\\|_{2}^{2}$。向量 $r^{(1)}$ 定义为：\n$$\nr^{(1)} = \\begin{pmatrix} \\beta^{(1)} - z^{(1)} \\\\ D\\beta^{(1)} - s^{(1)} \\end{pmatrix}\n$$\n$r^{(1)}$ 的两个分量恰好是（由于初始对偶变量为零）对偶变量的更新量。\n$$\n\\beta^{(1)} - z^{(1)} = u^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n$$\nD\\beta^{(1)} - s^{(1)} = v^{(1)} = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n因此，堆叠向量为：\n$$\nr^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix}\n$$\n其欧几里得范数的平方是其各分量平方之和：\n$$\n\\|r^{(1)}\\|_{2}^{2} = 1^2 + 0^2 + 1^2 + (-1)^2 + (-1)^2 + 1^2 + (-1)^2 = 1 + 0 + 1 + 1 + 1 + 1 + 1 = 6.\n$$", "answer": "$$\n\\boxed{6}\n$$", "id": "3447147"}]}