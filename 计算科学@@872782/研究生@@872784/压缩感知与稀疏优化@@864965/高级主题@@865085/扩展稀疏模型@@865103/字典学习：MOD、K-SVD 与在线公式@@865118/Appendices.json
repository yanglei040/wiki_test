{"hands_on_practices": [{"introduction": "字典学习算法的核心通常在于稀疏编码和字典更新这两个步骤之间的交替。这个练习聚焦于稀疏编码步骤，它通常归结为求解一个 LASSO 类型的问题 [@problem_id:3444184]。通过从第一性原理推导解决该问题的基本工具——迭代软阈值算法 (ISTA) 及其加速版本 (FISTA)，你将巩固对近端梯度方法的理解。", "problem": "考虑在字典学习方法中出现的稀疏编码子问题，例如最优方向法 (Method of Optimal Directions, MOD) 和 K-奇异值分解 (K-SVD)，以及在线字典学习的公式化中。设 $D \\in \\mathbb{R}^{m \\times p}$ 是一个固定字典，$y \\in \\mathbb{R}^{m}$ 是一个给定信号，且 $\\lambda  0$。稀疏编码 $x \\in \\mathbb{R}^{p}$ 通过求解复合凸优化问题得到\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\;\\; \\frac{1}{2}\\|y - D x\\|_{2}^{2} + \\lambda \\|x\\|_{1}.\n$$\n从真、闭、凸函数的邻近算子的定义以及光滑函数梯度的 Lipschitz 连续性刻画出发，从第一性原理推导：\n1. 函数 $x \\mapsto \\lambda \\|x\\|_{1}$ 的邻近算子的闭式解。\n2. 针对上述问题，使用根据光滑项梯度的 Lipschitz 常数选择的恒定步长，推导迭代收缩阈值算法 (ISTA) 的更新法则。\n3. 针对同一问题，使用相同的步长选择，推导快速迭代收缩阈值算法 (FISTA) 的加速更新法则，包括动量参数更新和外插迭代。\n\n您的推导必须从邻近算子的基本定义和 Lipschitz 梯度条件开始。明确指出光滑项梯度的 Lipschitz 常数，并用它来确定一个有效的固定步长。以闭式形式表示邻近算子以及 ISTA 和 FISTA 的更新法则，不要省略中间的逻辑步骤。将您的最终表达式紧凑地表示为一个单行矩阵，其条目按顺序为邻近算子、ISTA 更新法则和 FISTA 更新法则。不需要进行数值计算。", "solution": "所给出的问题是一个标准的复合凸优化问题，称为 Lasso 或基追踪降噪 (Basis Pursuit Denoising)。目标函数是一个光滑、凸、可微的损失函数与一个凸、非光滑的正则化项之和。我们需要从第一性原理出发，推导正则化项的邻近算子，以及迭代收缩阈值算法 (ISTA) 及其加速版本 FISTA 的更新法则。\n\n该优化问题为：\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\;\\; F(x) \\equiv f(x) + g(x)\n$$\n其中光滑项为 $f(x) = \\frac{1}{2}\\|y - D x\\|_{2}^{2}$，非光滑项为 $g(x) = \\lambda \\|x\\|_{1}$。\n\n### 1. $g(x) = \\lambda \\|x\\|_{1}$ 的邻近算子推导\n\n一个真、闭、凸函数 $h:\\mathbb{R}^p \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 的邻近算子（参数为 $\\tau  0$）定义为：\n$$\n\\text{prox}_{\\tau h}(z) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ h(x) + \\frac{1}{2\\tau} \\|x - z\\|_2^2 \\right\\}\n$$\n对于函数 $g(x) = \\lambda \\|x\\|_{1}$，其邻近算子由下式给出：\n$$\n\\text{prox}_{\\tau\\lambda\\|\\cdot\\|_1}(z) = \\arg\\min_{x \\in \\mathbb{R}^p} \\left\\{ \\lambda \\|x\\|_{1} + \\frac{1}{2\\tau} \\|x - z\\|_2^2 \\right\\}\n$$\n目标函数关于 $x$ 的分量是可分的，因为 $\\|x\\|_1 = \\sum_{i=1}^p |x_i|$ 且 $\\|x-z\\|_2^2 = \\sum_{i=1}^p (x_i-z_i)^2$。因此，我们可以通过最小化以下表达式来独立求解每个分量 $x_i$：\n$$\n\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\lambda |x_i| + \\frac{1}{2\\tau} (x_i - z_i)^2 \\right\\}\n$$\n这是一个一维凸优化问题。一个点 $x_i^*$ 是最小化点，当且仅当 $0$ 属于目标函数在 $x_i^*$ 处的次微分。目标函数关于 $x_i$ 的次微分是：\n$$\n\\partial \\left( \\lambda |x_i| + \\frac{1}{2\\tau} (x_i - z_i)^2 \\right) = \\lambda \\cdot \\partial|x_i| + \\frac{1}{\\tau}(x_i - z_i)\n$$\n其中 $\\partial|x_i|$ 是绝对值函数的次微分：\n$$\n\\partial|x_i| = \\begin{cases} \\{1\\}  \\text{if } x_i > 0 \\\\ \\{-1\\}  \\text{if } x_i  0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\n最优性条件 $0 \\in \\lambda \\cdot \\text{sgn}(x_i^*) + \\frac{1}{\\tau}(x_i^* - z_i)$ 可以重写为 $z_i - x_i^* \\in \\tau\\lambda \\cdot \\text{sgn}(x_i^*)$。我们针对 $x_i^*$ 的三种情况来分析这个条件：\n\n-   **情况 1: $x_i^* > 0$。** 此时 $\\text{sgn}(x_i^*) = \\{1\\}$，所以 $z_i - x_i^* = \\tau\\lambda$，这意味着 $x_i^* = z_i - \\tau\\lambda$。这与 $x_i^*>0$ 一致的条件是 $z_i > \\tau\\lambda$。\n-   **情况 2: $x_i^*  0$。** 此时 $\\text{sgn}(x_i^*) = \\{-1\\}$，所以 $z_i - x_i^* = -\\tau\\lambda$，这意味着 $x_i^* = z_i + \\tau\\lambda$。这与 $x_i^*0$ 一致的条件是 $z_i  -\\tau\\lambda$。\n-   **情况 3: $x_i^* = 0$。** 此时 $\\text{sgn}(x_i^*) = [-1, 1]$，所以 $z_i - 0 \\in [-\\tau\\lambda, \\tau\\lambda]$，这意味着 $|z_i| \\le \\tau\\lambda$。\n\n综合这些情况，每个分量 $x_i^*$ 的解是：\n$$\nx_i^* = \\begin{cases} z_i - \\tau\\lambda  \\text{if } z_i > \\tau\\lambda \\\\ 0  \\text{if } |z_i| \\le \\tau\\lambda \\\\ z_i + \\tau\\lambda  \\text{if } z_i  -\\tau\\lambda \\end{cases}\n$$\n这个操作被称为软阈值算子，我们记作 $S_{\\alpha}(\\cdot)$。对于一个向量 $v$ 和标量 $\\alpha0$，它按元素定义为 $(S_{\\alpha}(v))_i = \\text{sgn}(v_i) \\max(|v_i|-\\alpha, 0)$。\n因此，$g(x) = \\lambda \\|x\\|_1$ 的邻近算子是阈值为 $\\tau\\lambda$ 的软阈值算子：\n$$\n\\text{prox}_{\\tau g}(z) = S_{\\tau\\lambda}(z)\n$$\n\n### 2. ISTA 更新法则的推导\n\nISTA 是邻近梯度法的一个实例，专为形如 $\\min_x f(x) + g(x)$ 的问题而设计。迭代更新法则由下式给出：\n$$\nx^{k+1} = \\text{prox}_{\\tau_k g}(x^k - \\tau_k \\nabla f(x^k))\n$$\n其中 $\\tau_k > 0$ 是第 $k$ 次迭代的步长。为了保证收敛，对于恒定步长方案，步长通常根据 $\\nabla f$ 的 Lipschitz 常数来选择。如果对于所有 $x_1, x_2$ 都有 $\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 \\le L \\|x_1 - x_2\\|_2$，则称梯度 $\\nabla f$ 是 $L$-Lipschitz 连续的。\n\n首先，我们计算 $f(x) = \\frac{1}{2}\\|y - Dx\\|_2^2$ 的梯度：\n$$\nf(x) = \\frac{1}{2}(y - Dx)^T(y - Dx) = \\frac{1}{2}(y^T y - 2y^T Dx + x^T D^T D x)\n$$\n对 $x$ 求梯度得到：\n$$\n\\nabla f(x) = D^T(Dx - y)\n$$\n接下来，我们求 $\\nabla f(x)$ 的 Lipschitz 常数 $L$。考虑两个点 $x_1, x_2 \\in \\mathbb{R}^p$：\n$$\n\\nabla f(x_1) - \\nabla f(x_2) = (D^T(Dx_1-y)) - (D^T(Dx_2-y)) = D^T D(x_1 - x_2)\n$$\n取欧几里得范数：\n$$\n\\|\\nabla f(x_1) - \\nabla f(x_2)\\|_2 = \\|D^T D(x_1 - x_2)\\|_2 \\le \\|D^T D\\|_2 \\|x_1 - x_2\\|_2\n$$\n其中 $\\|D^T D\\|_2$ 是矩阵 $D^T D$ 的谱范数，即其最大的奇异值，或者等价地，由于 $D^T D$ 是半正定矩阵，为其最大特征值 $\\lambda_{\\max}(D^T D)$。因此，$\\nabla f$ 的最小可能 Lipschitz 常数是 $L = \\|D^T D\\|_2 = \\sigma_{\\max}^2(D)$。\n\n对于恒定步长，如果 $\\tau \\in (0, 2/L)$，则收敛性得到保证。一个标准的选择是 $\\tau = 1/L$。使用这个步长，ISTA 更新法则变为：\n$$\nx^{k+1} = \\text{prox}_{(1/L)g}(x^k - (1/L)\\nabla f(x^k))\n$$\n代入我们关于 $g$、$\\nabla f$ 和 $L$ 的表达式：\n$$\nx^{k+1} = S_{(\\lambda/L)}(x^k - (1/L)D^T(Dx^k-y))\n$$\n其中 $L = \\|D^T D\\|_2$。\n\n### 3. FISTA 更新法则的推导\n\nFISTA 是 ISTA 的一个加速版本。它通过引入一个动量项来获得更快的收敛速度。它维护一个主迭代序列，我们记作 $\\{x_k\\}$，以及一个辅助的外插序列 $\\{y_k\\}$。其更新法则源自 Nesterov 的加速梯度法。设 $t_k$ 是一个动量参数序列。算法从一个初始猜测 $x_0$ 开始，通常初始化为 $y_1 = x_0$ 和 $t_1 = 1$。\n\n对于每次迭代 $k = 1, 2, \\ldots$：\n1.  下一个迭代点 $x_k$ 是通过在外插点 $y_k$ 上应用邻近梯度步骤计算得出的，使用相同的步长 $\\tau = 1/L$：\n    $$\n    x_k = \\text{prox}_{(1/L)g}(y_k - (1/L)\\nabla f(y_k)) = S_{\\lambda/L}(y_k - (1/L)D^T(Dy_k-y))\n    $$\n2.  动量参数更新如下：\n    $$\n    t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}\n    $$\n3.  下一个外插点 $y_{k+1}$ 由前两个迭代点 $x_{k-1}$ 和 $x_k$ 的线性组合构成：\n    $$\n    y_{k+1} = x_k + \\left(\\frac{t_k - 1}{t_{k+1}}\\right)(x_k - x_{k-1})\n    $$\n这三个方程构成了针对该问题的 FISTA 更新法则。\n\n最终表达式汇集在下面的答案中。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nS_{\\tau \\lambda}(z)  x^{k+1} = S_{\\lambda/L}\\left(x^k - \\frac{1}{L} D^T(Dx^k - y)\\right)  \\begin{aligned} x_k = S_{\\lambda/L}\\left(y_k - \\frac{1}{L} D^T(Dy_k - y)\\right) \\\\ t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2} \\\\ y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1}) \\end{aligned}\n\\end{pmatrix}\n}\n$$", "id": "3444184"}, {"introduction": "在给定稀疏编码后，算法的下一步是更新字典本身。本练习将探讨这一挑战，并引入学习一个“好”字典的两个关键概念：保证原子多样性的非相干性，以及确保问题适定性的列归一化 [@problem_id:3444142]。你将练习必要的矩阵微积分，以实现一个强制执行这些属性的投影梯度下降步骤。", "problem": "在用于压缩感知和稀疏优化的字典学习中，最优方向法 (Method of Optimal Directions, MOD) 和 K-奇异值分解 (K-Singular Value Decomposition, K-SVD) 通常会引入促进非相干性的项，以抑制高度相关的原子。考虑一个字典矩阵 $D \\in \\mathbb{R}^{m \\times k}$，其列旨在具有单位欧几里得范数，以及非相干性惩罚项\n$$\\phi(D) = \\frac{\\eta}{2}\\left\\|D^{\\top} D - I\\right\\|_{F}^{2},$$\n其中 $\\eta > 0$ 是一个正则化参数，$I \\in \\mathbb{R}^{k \\times k}$ 是单位矩阵，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数。仅从以下基本事实出发：\n- 对于任意矩阵 $A$，弗罗贝尼乌斯范数恒等式 $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$，\n- 对于可乘矩阵，迹的循环性质 $\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$，\n- 格拉姆算子 $G(D) = D^{\\top}D$ 的微分，由 $\\mathrm{d}(D^{\\top}D) = \\mathrm{d}D^{\\top}\\,D + D^{\\top}\\,\\mathrm{d}D$ 给出，\n- 以及通过一阶变分 $\\mathrm{d}\\phi(D) = \\operatorname{tr}\\!\\big((\\nabla \\phi(D))^{\\top}\\mathrm{d}D\\big)$ 定义的梯度，\n推导梯度 $\\nabla \\phi(D)$，然后构建一个步长为 $\\tau  0$ 的投影梯度更新，通过投影到集合 $\\mathcal{C} = \\{D \\in \\mathbb{R}^{m \\times k} : \\|D_{:,j}\\|_{2} = 1 \\text{ for all } j \\in \\{1,\\dots,k\\}\\}$ 上，以保持 $D$ 的每一列的单位范数约束。将投影表示为与一个对角重缩放矩阵的右乘，该对角矩阵的对角线元素是预投影迭代的列范数的倒数。\n\n你的最终答案必须是关于 $D$、$\\eta$、$\\tau$ 和 $I$ 的单个闭式解析表达式。不需要进行数值计算。", "solution": "问题陈述是适定的、有科学依据的且自洽的。它提供了推导所要求的梯度更新规则所需的所有必要定义和基本事实。其背景在稀疏优化和字典学习领域是标准的。因此，该问题是有效的，我们可以继续进行求解。\n\n目标是首先求出非相干性惩罚函数 $\\phi(D)$ 的梯度，然后用它来构建一个投影梯度下降更新步骤。\n\n设惩罚函数为\n$$ \\phi(D) = \\frac{\\eta}{2}\\left\\|D^{\\top} D - I\\right\\|_{F}^{2} $$\n其中 $D \\in \\mathbb{R}^{m \\times k}$，$I \\in \\mathbb{R}^{k \\times k}$ 是单位矩阵，且 $\\eta > 0$。\n\n首先，我们推导梯度 $\\nabla \\phi(D)$。我们从使用给定的迹恒等式 $\\|A\\|_{F}^{2} = \\operatorname{tr}(A^{\\top} A)$ 来表示弗罗贝尼乌斯范数的平方开始。\n令 $A = D^{\\top}D - I$。则 $A^{\\top} = (D^{\\top}D - I)^{\\top} = (D^{\\top}D)^{\\top} - I^{\\top} = D^{\\top}(D^{\\top})^{\\top} - I = D^{\\top}D - I$，因为单位矩阵 $I$ 是对称的。因此，$A$ 是一个对称矩阵。\n惩罚函数可以写成：\n$$ \\phi(D) = \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)^{\\top}(D^{\\top}D - I)\\right) = \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)(D^{\\top}D - I)\\right) $$\n为了求梯度，我们首先计算一阶变分，即微分 $\\mathrm{d}\\phi(D)$。利用微分和迹算子的线性性质，以及微分的乘法法则，我们有：\n$$ \\mathrm{d}\\phi(D) = \\mathrm{d}\\left( \\frac{\\eta}{2} \\operatorname{tr}\\left((D^{\\top}D - I)(D^{\\top}D - I)\\right) \\right) $$\n$$ \\mathrm{d}\\phi(D) = \\frac{\\eta}{2} \\operatorname{tr}\\left( \\mathrm{d}(D^{\\top}D - I)(D^{\\top}D - I) + (D^{\\top}D - I)\\mathrm{d}(D^{\\top}D - I) \\right) $$\n由于迹内的两项互为转置，且矩阵的迹等于其转置的迹，因此它们相等。一个更简单的方法是注意到它们是相同的乘积，所以我们可以将它们合并：\n$$ \\mathrm{d}\\phi(D) = \\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}(D^{\\top}D - I) \\right) $$\n常数矩阵（$I$）的微分为零，所以 $\\mathrm{d}(D^{\\top}D - I) = \\mathrm{d}(D^{\\top}D)$。问题给出了格拉姆矩阵 $G(D) = D^{\\top}D$ 的微分，即 $\\mathrm{d}(D^{\\top}D) = \\mathrm{d}D^{\\top}D + D^{\\top}\\mathrm{d}D$。将此代入我们关于 $\\mathrm{d}\\phi(D)$ 的表达式中：\n$$ \\mathrm{d}\\phi(D) = \\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) (\\mathrm{d}D^{\\top}D + D^{\\top}\\mathrm{d}D) \\right) $$\n$$ \\mathrm{d}\\phi(D) = \\eta \\left( \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right) + \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) \\right) $$\n我们需要将此表达式变换为 $\\mathrm{d}\\phi(D) = \\operatorname{tr}((\\nabla \\phi(D))^{\\top}\\mathrm{d}D)$ 的形式。我们来分析第一项，$\\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right)$。使用性质 $\\operatorname{tr}(X) = \\operatorname{tr}(X^{\\top})$：\n$$ \\operatorname{tr}\\left( (D^{\\top}D - I) \\mathrm{d}D^{\\top}D \\right) = \\operatorname{tr}\\left( \\left((D^{\\top}D - I) \\mathrm{d}D^{\\top}D\\right)^{\\top} \\right) = \\operatorname{tr}\\left( D^{\\top}(\\mathrm{d}D^{\\top})^{\\top}(D^{\\top}D - I)^{\\top} \\right) $$\n由于 $(D^{\\top}D - I)$ 是对称的，且 $(\\mathrm{d}D^{\\top})^{\\top} = \\mathrm{d}D$，这变成：\n$$ \\operatorname{tr}\\left( D^{\\top}\\mathrm{d}D (D^{\\top}D - I) \\right) $$\n现在，使用迹的循环性质 $\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$，其中 $A=D^{\\top}$，$B=\\mathrm{d}D$，且 $C=(D^{\\top}D - I)$：\n$$ \\operatorname{tr}\\left( D^{\\top}\\mathrm{d}D (D^{\\top}D - I) \\right) = \\operatorname{tr}\\left( \\mathrm{d}D (D^{\\top}D - I)D^{\\top} \\right) $$\n我们来考察 $\\mathrm{d}\\phi(D)$ 表达式中的第二项，即 $\\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right)$。使用循环性质，其中 $A=(D^{\\top}D - I)D^{\\top}$ 且 $B=\\mathrm{d}D$，我们有 $\\operatorname{tr}(AB) = \\operatorname{tr}(BA)$：\n$$ \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) = \\operatorname{tr}\\left( \\mathrm{d}D (D^{\\top}D - I) D^{\\top} \\right) $$\n我们观察到这两项是相同的。因此，我们可以将它们合并：\n$$ \\mathrm{d}\\phi(D) = \\eta \\left( \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) + \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) \\right) $$\n$$ \\mathrm{d}\\phi(D) = 2\\eta \\, \\operatorname{tr}\\left( (D^{\\top}D - I) D^{\\top}\\mathrm{d}D \\right) $$\n为了将其与定义 $\\mathrm{d}\\phi(D) = \\operatorname{tr}((\\nabla \\phi(D))^{\\top}\\mathrm{d}D)$ 匹配，我们将 $(\\nabla \\phi(D))^{\\top}$ 识别为在迹内部从左侧乘以 $\\mathrm{d}D$ 的矩阵。然而，实矩阵的标准内积是 $\\langle X, Y \\rangle = \\operatorname{tr}(X^{\\top}Y)$。梯度的定义基于此。我们有一个形式为 $\\operatorname{tr}(A B)$ 的表达式，而我们希望它是 $\\operatorname{tr}(G^{\\top}B)$ 的形式。我们必须识别出 $G^{\\top} = A$。\n所以，我们必须有 $(\\nabla \\phi(D))^{\\top} = 2\\eta (D^{\\top}D - I) D^{\\top}$。\n梯度 $\\nabla \\phi(D)$ 是这个表达式的转置：\n$$ \\nabla \\phi(D) = \\left( 2\\eta (D^{\\top}D - I) D^{\\top} \\right)^{\\top} = 2\\eta (D^{\\top})^{\\top} (D^{\\top}D - I)^{\\top} $$\n$$ \\nabla \\phi(D) = 2\\eta D (D^{\\top}D - I) $$\n这个梯度表达式的维度是 $(m \\times k) \\times (k \\times k) = m \\times k$，与 $D$ 的维度相符，正如预期。\n\n接下来，我们构建投影梯度更新。对于 $D$ 的标准梯度下降更新，步长为 $\\tau > 0$，其形式为：\n$$ D_{\\text{temp}} = D - \\tau \\nabla \\phi(D) $$\n代入我们推导出的梯度：\n$$ D_{\\text{temp}} = D - 2\\eta\\tau D(D^{\\top}D - I) $$\n我们可以提出因子 $D$，以更紧凑地写出这个式子：\n$$ D_{\\text{temp}} = D(I - 2\\eta\\tau(D^{\\top}D - I)) $$\n这个临时迭代 $D_{\\text{temp}}$ 通常不具有单位范数的列。我们必须将其投影到约束集 $\\mathcal{C} = \\{X \\in \\mathbb{R}^{m \\times k} : \\|X_{:,j}\\|_{2} = 1 \\text{ for all } j\\}$ 上。投影操作涉及将 $D_{\\text{temp}}$ 的每一列归一化。设 $D_{\\text{next}}$ 为更新后的矩阵。$D_{\\text{next}}$ 的第 $j$ 列是：\n$$ (D_{\\text{next}})_{:,j} = \\frac{(D_{\\text{temp}})_{:,j}}{\\|(D_{\\text{temp}})_{:,j}\\|_{2}} $$\n这个操作可以表示为与一个对角矩阵 $S \\in \\mathbb{R}^{k \\times k}$ 的右乘，其中对角线元素是 $D_{\\text{temp}}$ 列范数的倒数：\n$$ D_{\\text{next}} = D_{\\text{temp}}S, \\quad \\text{其中} \\quad S = \\operatorname{diag}\\left(\\frac{1}{\\|(D_{\\text{temp}})_{:,1}\\|_{2}}, \\dots, \\frac{1}{\\|(D_{\\text{temp}})_{:,k}\\|_{2}}\\right) $$\n$D_{\\text{temp}}$ 的第 $j$ 列的范数平方是矩阵 $D_{\\text{temp}}^{\\top}D_{\\text{temp}}$ 的第 $j$ 个对角元素。令 $N = \\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}})$。这是一个对角矩阵，其元素为 $N_{jj} = \\|(D_{\\text{temp}})_{:,j}\\|_{2}^{2}$。\n因此，对角重缩放矩阵 $S$ 可以写成 $S = N^{-1/2} = (\\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}}))^{-1/2}$。\n\n将这些部分组合起来，完整的投影梯度更新为：\n$$ D_{\\text{next}} = D_{\\text{temp}} (\\operatorname{diag}(D_{\\text{temp}}^{\\top}D_{\\text{temp}}))^{-1/2} $$\n代入 $D_{\\text{temp}} = D(I - 2\\eta\\tau(D^{\\top}D - I))$ 的表达式，我们得到更新规则的最终闭式表达式。\n令 $U = D(I - 2\\eta\\tau(D^{\\top}D - I))$。更新由以下公式给出：\n$$ D_{\\text{next}} = U \\left( \\operatorname{diag}(U^{\\top} U) \\right)^{-1/2} $$\n这是关于 $D$、$\\eta$、$\\tau$ 和 $I$ 的投影梯度更新的单个解析表达式。", "answer": "$$ \\boxed{\\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right) \\left( \\operatorname{diag}\\left( \\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right)^{\\top} \\left(D\\left(I - 2\\eta\\tau(D^{\\top}D - I)\\right)\\right) \\right) \\right)^{-1/2}} $$", "id": "3444142"}, {"introduction": "前面的练习引入了如列归一化等约束条件，而这个最后的练习将揭示为何这些约束并非可选，而是绝对必要的。通过分析一个简化的无约束场景，你将发现一种病态的失败模式，即字典原子的范数可以无界增长，而目标函数值却单调下降 [@problem_id:3444131]。这为现代字典学习中使用的约束优化方法提供了深刻而直观的理论依据。", "problem": "考虑在没有任何字典归一化或投影的情况下，针对单个训练向量的无约束稀疏驱动字典学习目标：\n$$\n\\min_{D,X}\\; \\frac{1}{2}\\|Y-DX\\|_{F}^{2}+\\lambda\\|X\\|_{1},\n$$\n其中 $Y\\in\\mathbb{R}^{n\\times 1}$ 是数据矩阵（单列），$D\\in\\mathbb{R}^{n\\times 1}$ 是字典（单个原子），$X\\in\\mathbb{R}^{1\\times 1}$ 是标量编码，$\\|\\cdot\\|_{F}$ 表示 Frobenius 范数，$\\|\\cdot\\|_{1}$ 表示逐元素的 $\\ell_{1}$ 范数，$\\lambda0$ 是一个固定的正则化参数。这种形式或其变体是 Method of Optimal Directions (MOD)、K-Singular Value Decomposition (K-SVD) 和 Online Dictionary Learning (ODL) 的基础，这些方法在实践中对字典列强制施加单位范数约束或投影，以防止病态的缩放问题。\n\n设 $y\\in\\mathbb{R}^{n}$ 是一个满足 $\\|y\\|_{2}=1$ 的固定单位范数数据向量，并解释为 $Y=y$，$D=d\\in\\mathbb{R}^{n}$，$X=x\\in\\mathbb{R}$。考虑以下不对 $d$ 进行归一化或投影的朴素交替更新方法：\n- 对于迭代 $t=0,1,2,\\dots$，给定 $d_{t}$ 和 $x_{t}$，通过在目标函数上进行单步梯度下降来更新字典，同时保持 $x_{t}$ 固定：\n$$\nd_{t+1}=d_{t}-\\eta\\,\\nabla_{d}\\left(\\frac{1}{2}\\|y-dx_{t}\\|_{2}^{2}+\\lambda|x_{t}|\\right)\\Bigg|_{d=d_{t}},\n$$\n其中步长 $\\eta0$ 是固定的。\n- 然后通过求解使用更新后的 $d_{t+1}$ 的一维稀疏编码子问题来精确更新编码：\n$$\nx_{t+1}\\in\\arg\\min_{x\\in\\mathbb{R}}\\;\\frac{1}{2}\\|y-d_{t+1}x\\|_{2}^{2}+\\lambda|x|.\n$$\n\n假设初始化为 $d_{0}=a_{0}y$ 且 $a_{0}\\lambda$，并且 $x_{0}$ 被选为对于 $d_{0}$ 的精确最小化器。仅使用第一性原理（梯度微积分和一维稀疏编码的次梯度/Karush–Kuhn–Tucker 最优性条件），推导由 $d_{t}=a_{t}y$ 定义的标量振幅 $a_{t}$ 的动力学，以及由此产生的目标值\n$$\nf_{t}=\\frac{1}{2}\\|y-d_{t}x_{t}\\|_{2}^{2}+\\lambda|x_{t}|.\n$$\n证明在这种构造的情况下，字典范数 $\\|d_{t}\\|_{2}=|a_{t}|$ 无界增长，而目标值 $f_{t}$ 单调递减。计算精确极限 $\\lim_{t\\to\\infty}f_{t}$。\n\n你的最终答案必须是一个实数。无需四舍五入。", "solution": "该问题首先被验证，并被认为是良构的、有科学依据的和客观的。它提出了一个字典学习动力学的简化但典型的案例研究，该研究没有进行字典原子归一化，这是该领域的一个标准课题。\n\n设目标函数为 $f(d, x) = \\frac{1}{2}\\|y-dx\\|_{2}^{2}+\\lambda|x|$，其中 $y \\in \\mathbb{R}^{n}$ 是一个 $\\|y\\|_{2}=1$ 的固定向量，$d \\in \\mathbb{R}^{n}$ 是一个字典原子，$x \\in \\mathbb{R}$ 是其对应的编码，$\\lambda  0$ 是一个正则化参数。优化通过对 $d$ 和 $x$ 的交替更新进行。\n\n首先，我们分析字典更新步骤。对 $d_{t+1}$ 的更新是在固定 $x_t$ 的情况下对目标函数进行梯度下降的一步。项 $\\lambda|x_t|$ 相对于 $d$ 是常数。\n重构误差项关于 $d$ 的梯度是：\n$$ \\nabla_{d}\\left(\\frac{1}{2}\\|y-dx_{t}\\|_{2}^{2}\\right) = \\nabla_{d}\\left(\\frac{1}{2}(y - dx_t)^T(y - dx_t)\\right) = \\nabla_{d}\\left(\\frac{1}{2}(y^T y - 2x_t d^T y + x_t^2 d^T d)\\right) = -x_t y + x_t^2 d = -x_t(y - dx_t) $$\n因此，字典更新规则是：\n$$ d_{t+1} = d_{t} - \\eta \\nabla_{d} f(d, x_t)|_{d=d_t} = d_{t} - \\eta(-x_{t}(y - d_{t}x_{t})) = d_{t} + \\eta x_{t}(y - d_{t}x_{t}) $$\n\n接下来，我们分析 $d_t$ 的结构。问题陈述了初始化为 $d_0 = a_0 y$，其中标量 $a_0  \\lambda$。我们通过归纳法证明，对于所有迭代 $t=0, 1, 2, \\dots$，$d_t$ 始终与 $y$ 成比例。假设 $d_t = a_t y$，其中 $a_t$ 是一个标量。将此代入 $d_{t+1}$ 的更新规则中：\n$$ d_{t+1} = a_t y + \\eta x_t(y - (a_t y) x_t) = a_t y + \\eta x_t(1 - a_t x_t)y = \\left(a_t + \\eta x_t(1 - a_t x_t)\\right)y $$\n这表明 $d_{t+1}$ 也与 $y$ 成比例。因此我们可以写成 $d_{t+1} = a_{t+1}y$，其中标量振幅遵循以下递推关系：\n$$ a_{t+1} = a_t + \\eta x_t(1 - a_t x_t) $$\n\n现在，我们推导编码 $x_{t+1}$ 的表达式。它是通过在固定 $d_{t+1}$ 的情况下精确最小化目标函数得到的：\n$$ x_{t+1} = \\arg\\min_{x \\in \\mathbb{R}}\\; h(x) \\quad \\text{其中} \\quad h(x) = \\frac{1}{2}\\|y-d_{t+1}x\\|_{2}^{2}+\\lambda|x| $$\n这是一个一维 Lasso 问题。目标函数可以展开为：\n$$ h(x) = \\frac{1}{2}(y^T y - 2x y^T d_{t+1} + x^2 \\|d_{t+1}\\|_2^2) + \\lambda|x| $$\n解由软阈值算子给出。根据次梯度微积分的一阶最优性条件是 $0 \\in \\partial_x h(x)$，其中：\n$$ \\partial_x h(x) = \\|d_{t+1}\\|_2^2 x - y^T d_{t+1} + \\lambda \\cdot \\mathrm{sgn}(x) $$\n其中 $\\mathrm{sgn}(x)$ 是 $|x|$ 的次梯度。解是：\n$$ x_{t+1} = \\frac{1}{\\|d_{t+1}\\|_2^2} \\mathrm{soft-thresh}(y^T d_{t+1}, \\lambda) = \\frac{1}{\\|d_{t+1}\\|_2^2} \\mathrm{sgn}(y^T d_{t+1}) \\max(|y^T d_{t+1}| - \\lambda, 0) $$\n由于 $d_{t+1} = a_{t+1}y$ 且 $\\|y\\|_2=1$，我们有 $\\|d_{t+1}\\|_2^2 = a_{t+1}^2 \\|y\\|_2^2 = a_{t+1}^2$ 和 $y^T d_{t+1} = y^T(a_{t+1}y) = a_{t+1} \\|y\\|_2^2 = a_{t+1}$。$x_{t+1}$ 的表达式简化为：\n$$ x_{t+1} = \\frac{1}{a_{t+1}^2} \\mathrm{soft-thresh}(a_{t+1}, \\lambda) $$\n这适用于任何迭代，所以 $x_t = \\frac{1}{a_t^2} \\mathrm{soft-thresh}(a_t, \\lambda)$。\n\n现在我们来确定 $a_t$ 的动力学。我们从 $a_0 > \\lambda > 0$ 开始。因此，$\\mathrm{soft-thresh}(a_0, \\lambda) = a_0 - \\lambda > 0$。所以，$x_0 = \\frac{a_0-\\lambda}{a_0^2} > 0$。\n我们假设 $a_t > \\lambda$。那么 $x_t = \\frac{a_t-\\lambda}{a_t^2}$。我们将此代入 $a_t$ 的递推关系中：\n$$ a_{t+1} = a_t + \\eta x_t(1 - a_t x_t) = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(1 - a_t \\frac{a_t-\\lambda}{a_t^2}\\right) $$\n$$ a_{t+1} = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(1 - \\frac{a_t-\\lambda}{a_t}\\right) = a_t + \\eta \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) \\left(\\frac{a_t - (a_t-\\lambda)}{a_t}\\right) = a_t + \\eta \\lambda \\frac{a_t-\\lambda}{a_t^3} $$\n因为 $a_t > \\lambda$，$\\eta > 0$ 且 $\\lambda > 0$，所以项 $\\eta \\lambda \\frac{a_t-\\lambda}{a_t^3}$ 是严格为正的。\n因此，$a_{t+1} > a_t$。由于 $a_0 > \\lambda$，通过归纳法可知序列 $(a_t)_{t \\ge 0}$ 是严格递增的，并且对于所有 $t \\ge 0$ 都有 $a_t > \\lambda$。这证实了我们使用 $x_t$ 简化表达式的假设。\n一个递增序列要么收敛到一个有限极限，要么发散到无穷大。如果对于某个有限的 $L$，$a_t \\to L$，那么 $L$ 必须是该递推关系的一个不动点：\n$$ L = L + \\eta \\lambda \\frac{L-\\lambda}{L^3} \\quad \\implies \\quad \\eta \\lambda \\frac{L-\\lambda}{L^3} = 0 $$\n这意味着 $L=\\lambda$。然而，由于 $(a_t)$ 是严格递增的且 $a_0 > \\lambda$，极限必须满足 $L \\ge a_0 > \\lambda$。这是一个矛盾。因此，序列 $(a_t)$ 不能收敛到有限极限，必须发散：$\\lim_{t\\to\\infty} a_t = \\infty$。\n字典范数为 $\\|d_t\\|_2 = \\|a_t y\\|_2 = |a_t| \\|y\\|_2 = |a_t|$。由于 $a_t$ 是一个从 $a_0 > 0$ 开始的递增序列，$a_t > 0$ 对所有 $t$ 成立，所以 $\\|d_t\\|_2 = a_t$。因此，$\\|d_t\\|_2$ 无界增长。\n\n接下来，我们分析目标值 $f_t = \\frac{1}{2}\\|y-d_t x_t\\|_2^2 + \\lambda |x_t|$。我们用 $a_t$ 来表示 $f_t$。\n重构残差为 $y - d_t x_t = y - (a_t y) \\left(\\frac{a_t-\\lambda}{a_t^2}\\right) = y\\left(1 - \\frac{a_t-\\lambda}{a_t}\\right) = y\\left(\\frac{\\lambda}{a_t}\\right)$。\n残差的平方范数为 $\\|y-d_t x_t\\|_2^2 = \\|\\frac{\\lambda}{a_t}y\\|_2^2 = \\frac{\\lambda^2}{a_t^2}\\|y\\|_2^2 = \\frac{\\lambda^2}{a_t^2}$。\n正则化项为 $\\lambda|x_t| = \\lambda x_t = \\lambda\\frac{a_t-\\lambda}{a_t^2}$，因为 $a_t\\lambda0$ 意味着 $x_t0$。\n所以，目标值为：\n$$ f_t = \\frac{1}{2}\\left(\\frac{\\lambda^2}{a_t^2}\\right) + \\lambda\\left(\\frac{a_t-\\lambda}{a_t^2}\\right) = \\frac{\\lambda^2 + 2\\lambda a_t - 2\\lambda^2}{2a_t^2} = \\frac{2\\lambda a_t - \\lambda^2}{2a_t^2} = \\frac{\\lambda}{a_t} - \\frac{\\lambda^2}{2a_t^2} $$\n为了证明 $f_t$ 单调递减，考虑函数 $g(a) = \\frac{\\lambda}{a} - \\frac{\\lambda^2}{2a^2}$。那么 $f_t=g(a_t)$。$g(a)$ 的导数是：\n$$ g'(a) = \\frac{d}{da}\\left(\\lambda a^{-1} - \\frac{\\lambda^2}{2} a^{-2}\\right) = -\\lambda a^{-2} + \\lambda^2 a^{-3} = \\frac{-\\lambda a + \\lambda^2}{a^3} = \\frac{\\lambda(\\lambda-a)}{a^3} $$\n由于我们已经确定对于所有 $t$ 都有 $a_t > \\lambda$，项 $\\lambda - a_t$ 是负的，而 $\\lambda$ 和 $a_t^3$ 是正的。因此，$g'(a_t)  0$。\n序列 $(a_t)$ 是严格递增的。由于 $f_t = g(a_t)$ 且当 $a > \\lambda$ 时 $g(a)$ 是一个严格递减函数，因此序列 $(f_t)$ 是严格递减的。这证明了目标值的单调递减性。\n\n最后，我们计算当 $t \\to \\infty$ 时 $f_t$ 的极限。我们之前已经证明了 $\\lim_{t\\to\\infty} a_t = \\infty$。\n$$ \\lim_{t\\to\\infty} f_t = \\lim_{t\\to\\infty} \\left(\\frac{\\lambda}{a_t} - \\frac{\\lambda^2}{2a_t^2}\\right) $$\n当 $a_t \\to \\infty$ 时，表达式中的两项都趋于 $0$：\n$$ \\lim_{a_t\\to\\infty} \\frac{\\lambda}{a_t} = 0 \\quad \\text{和} \\quad \\lim_{a_t\\to\\infty} \\frac{\\lambda^2}{2a_t^2} = 0 $$\n因此，目标函数的极限是：\n$$ \\lim_{t\\to\\infty} f_t = 0 - 0 = 0 $$\n这个分析突显了无约束字典学习的一个关键病态问题：字典原子的范数可以无限增大，而稀疏编码的量值收缩到零，这使得重构误差和目标函数可以在不提供有意义的稀疏表示的情况下趋近于零。这是在像 MOD 和 K-SVD 这样的实用算法中对字典原子施加约束（例如，单位范数）的主要动机。", "answer": "$$\\boxed{0}$$", "id": "3444131"}]}