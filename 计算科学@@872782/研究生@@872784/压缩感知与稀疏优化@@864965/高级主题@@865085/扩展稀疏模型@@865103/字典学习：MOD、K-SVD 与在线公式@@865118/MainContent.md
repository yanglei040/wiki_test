## 引言
在现代信号处理、机器学习和数据科学中，如何从复杂的高维数据中提取有意义的结构是一个核心挑战。[字典学习](@entry_id:748389)提供了一个强大的[范式](@entry_id:161181)来解决此问题，其核心思想是，大多数自然信号可以被一个[过完备字典](@entry_id:180740)中的少数几个基本元素（“原子”）的[线性组合](@entry_id:154743)来稀疏地表示。这种[稀疏表示](@entry_id:191553)不仅能揭示数据内在的简洁结构，还在压缩、去噪和[特征提取](@entry_id:164394)等任务中表现出卓越的性能。

然而，学习这个“最优”字典本身是一个具有挑战性的[非凸优化](@entry_id:634396)问题。本文旨在系统性地梳理[字典学习](@entry_id:748389)的理论、算法与应用。我们将从基础出发，逐步深入到前沿的扩展技术，为读者构建一个全面的知识体系。

本文将分为三个主要部分。在第一章“原理与机制”中，我们将深入探讨[稀疏表示](@entry_id:191553)的基本模型，构建[字典学习](@entry_id:748389)的[优化问题](@entry_id:266749)，并详细阐述两种经典的批量学习算法——最优方向法（MOD）和[K-SVD](@entry_id:182204)。我们还将讨论其背后的理论保证，如[稀疏表示](@entry_id:191553)的唯一性和字典的可辨识性，并介绍处理大规模数据的[在线学习](@entry_id:637955)[范式](@entry_id:161181)。随后的第二章“应用与跨学科联系”将展示[字典学习](@entry_id:748389)框架的灵活性和强大生命力，通过一系列实际案例，探讨如何将模型扩展以处理非高斯噪声、融入如非负性或卷积结构等先验知识，并运用更高级的正则化策略以提升性能。最后，“动手实践”部分将提供一系列精心设计的编程练习，帮助读者将理论知识转化为解决实际问题的算法实现能力，加深对关键概念的理解。

## 原理与机制

本章深入探讨[字典学习](@entry_id:748389)的核心原理和关键机制。我们将从[稀疏表示](@entry_id:191553)的基本模型出发，构建[字典学习](@entry_id:748389)的[优化问题](@entry_id:266749)，并详细阐述解决该问题的经典批量算法——最优方向法 (Method of Optimal Directions, MOD) 和 [K-SVD](@entry_id:182204) 算法。随后，我们将转向理论层面，讨论[稀疏表示](@entry_id:191553)的唯一性以及字典本身[可辨识性](@entry_id:194150)的条件。最后，我们将介绍[在线学习](@entry_id:637955)[范式](@entry_id:161181)，分析其在处理大规模数据集时的收敛保证和样本复杂度。

### [稀疏表示](@entry_id:191553)模型

信号[稀疏表示](@entry_id:191553)的核心思想是，大多数自然信号虽然在高维空间中表现复杂，但可以被一个合适的“字典”中的少数几个“原子”的[线性组合](@entry_id:154743)来精确或近似地表示。

#### 综合模型

在**综合模型** (synthesis model) 中，一个信号向量 $y \in \mathbb{R}^{m}$ 被表示为一个字典矩阵 $D \in \mathbb{R}^{m \times p}$ 和一个稀疏系数向量 $x \in \mathbb{R}^{p}$ 的乘积。该字典包含 $p$ 个原子（列向量） $\{d_j\}_{j=1}^p$，每个原子 $d_j \in \mathbb{R}^m$ 都是一个基本的信号模式。稀疏性意味着向量 $x$ 中只有少数几个非零元素。该模型可以写为：

$$
y \approx D x
$$

其中，“$\approx$” 表示在有噪声或模型失配时，我们寻求一个近似表示。向量 $x$ 的稀疏度通常用其 $\ell_0$ “范数” $\|x\|_0$ 来衡量，它表示 $x$ 中非零元素的个数。如果 $\|x\|_0 \le k$，我们称 $x$ 是一个 **$k$-稀疏**向量。

找到这个[稀疏表示](@entry_id:191553)的过程称为**[稀疏编码](@entry_id:180626)** (sparse coding)。给定信号 $y$ 和字典 $D$，[稀疏编码](@entry_id:180626)旨在求解以下[优化问题](@entry_id:266749)之一：
1.  在允许一定误差 $\epsilon$ 的前提下，寻找最稀疏的表示：$\min_{x} \|x\|_0 \quad \text{s.t.} \quad \|y - D x\|_2^2 \le \epsilon$。
2.  在稀疏度不超过 $k$ 的前提下，寻找最精确的表示：$\min_{x} \|y - D x\|_2^2 \quad \text{s.t.} \quad \|x\|_0 \le k$。

由于 $\ell_0$ 范数最小化问题是 N[P-难](@entry_id:265298)的，实际应用中常常使用其[凸松弛](@entry_id:636024)形式，即 $\ell_1$ 范数。这引出了基于 $\ell_1$ 范数的正则化问题，如 LASSO (Least Absolute Shrinkage and Selection Operator)：

$$
\min_{x} \frac{1}{2}\|y - D x\|_2^2 + \lambda \|x\|_1
$$

其中 $\lambda > 0$ 是一个正则化参数，用于权衡重构误差和解的[稀疏性](@entry_id:136793)。[@problem_id:3444190]

为了消除字典原子 $d_j$ 和其对应系数 $x_j$ 之间的尺度模糊性（因为对于任意非零标量 $\alpha$，我们总有 $(\alpha d_j)(\frac{x_j}{\alpha}) = d_j x_j$），通常会对字典的列施加约束，例如，要求每个原子具有单位 $\ell_2$ 范数，即 $\|d_j\|_2 = 1$。[@problem_id:3444190]

#### 与分析模型的对比

与综合模型相对应的是**分析模型** (analysis model)。分析模型不假设信号是由字典原子的线性组合生成的，而是假设存在一个[分析算子](@entry_id:746429) $W \in \mathbb{R}^{q \times m}$，当它作用于信号 $y$ 时，能产生一个稀疏的系数向量 $Wy$。换言之，分析模型假设信号本身具有某种结构，使得它在经过特定变换后变得稀疏。这个模型中的信号 $y$ 满足所谓的“[余稀疏性](@entry_id:747929)” (cosparsity)，因为它位于由 $W$ 的行向量定义的多个[超平面](@entry_id:268044)的交集附近。[@problem_id:3444190] 尽管分析模型在特定应用中非常重要，但本章的后续内容将主要聚焦于更为普遍的综合模型及其学习算法。

### [字典学习](@entry_id:748389)问题

[字典学习](@entry_id:748389)的目标是从一系列训练样本 $Y = [y_1, y_2, \dots, y_n] \in \mathbb{R}^{m \times n}$ 中学习一个能够最优地[稀疏表示](@entry_id:191553)这些样本的字典 $D$。

#### 优化目标与尺度模糊性

将[稀疏编码](@entry_id:180626)的思想推广到整个数据集 $Y$，我们可以构建一个联合[优化问题](@entry_id:266749)，同时求解字典 $D$ 和所有信号对应的稀疏[系数矩阵](@entry_id:151473) $X = [x_1, x_2, \dots, x_n] \in \mathbb{R}^{p \times n}$。使用 $\ell_1$ 正则化，[经验风险最小化](@entry_id:633880)目标函数可以写为：

$$
\min_{D, X} \frac{1}{2}\|Y - D X\|_F^2 + \lambda \|X\|_1
$$

其中 $\| \cdot \|_F$ 表示矩阵的 Frobenius 范数，$\|X\|_1 = \sum_{i,j} |X_{ij}|$。

然而，这个[目标函数](@entry_id:267263)存在一个固有的**尺度模糊性** (scaling ambiguity)。考虑一个变换：$D' = \alpha D$ 和 $X' = X/\alpha$，其中 $\alpha > 0$ 是一个标量。数据拟合项保持不变：

$$
\|Y - D'X'\|_F^2 = \|Y - (\alpha D)(X/\alpha)\|_F^2 = \|Y - DX\|_F^2
$$

但正则化项却被缩放了：

$$
\lambda \|X'\|_1 = \frac{\lambda}{\alpha} \|X\|_1
$$

如果没有对 $D$ 的尺度进行任何约束，我们可以通过让 $\alpha \to \infty$ 来使得正则化项趋近于零，从而任意地减小总目标函数的值。这会导致一个无意义的退化解：$D \to \infty$ 且 $X \to 0$。因此，这个[优化问题](@entry_id:266749)是**不适定** (ill-posed) 的。

为了解决这个问题，必须对字典施加约束。一个标准的做法是约束字典 $D$ 属于一个集合 $\mathcal{C}$，该集合限制了其原子的范数，例如 $\mathcal{C} = \{ D \in \mathbb{R}^{m \times p} : \|d_j\|_2 \le 1, \forall j=1,\dots,p \}$。这个约束打破了尺度模糊性，阻止了[原子范数](@entry_id:746563)的无限增长，从而使[优化问题](@entry_id:266749)变得适定。[@problem_id:3444121] 最终的[字典学习](@entry_id:748389)问题通常被表述为：

$$
\min_{D \in \mathcal{C}, X} \frac{1}{2}\|Y - D X\|_F^2 + \lambda \|X\|_1
$$

#### 概率解释：最大后验估计

[字典学习](@entry_id:748389)的 $\ell_1$ 正则化形式不仅是一个启发式的[优化问题](@entry_id:266749)，它还可以从贝叶斯推断的视角得到有力的理论支持。考虑一个生成单个信号 $y$ 的概率模型：

$$
y = D x + w
$$

其中，$w \sim \mathcal{N}(0, \sigma^2 I_m)$ 是一个[独立同分布](@entry_id:169067)的[高斯噪声](@entry_id:260752)向量。为了鼓励[稀疏性](@entry_id:136793)，我们为系数向量 $x$ 设定一个[先验分布](@entry_id:141376)。一个经典的选择是**拉普拉斯先验** (Laplace prior)，它假设 $x$ 的每个分量 $x_i$ 是独立同分布的，其概率密度函数为：

$$
p(x_i) = \frac{1}{2b} \exp\left(-\frac{|x_i|}{b}\right)
$$

其中 $b > 0$ 是[尺度参数](@entry_id:268705)。由于各分量独立，整个向量 $x$ 的先验分布为 $p(x) \propto \exp(-\frac{1}{b}\|x\|_1)$。

根据[贝叶斯定理](@entry_id:151040)，给定观测值 $y$，系数 $x$ 的后验概率为 $p(x|y) \propto p(y|x) p(x)$。**最大后验估计** (Maximum A Posteriori, MAP) 的目标是找到使后验概率最大化的 $x$：

$$
x^{\star} = \arg\max_{x} p(y|x) p(x) = \arg\min_{x} [-\ln p(y|x) - \ln p(x)]
$$

代入高斯似然 $p(y|x) \propto \exp(-\frac{1}{2\sigma^2}\|y - Dx\|_2^2)$ 和拉普拉斯先验 $p(x)$，并忽略与 $x$ 无关的常数项后，最小化负对数后验等价于求解：

$$
\min_{x} \frac{1}{2\sigma^2}\|y - Dx\|_2^2 + \frac{1}{b}\|x\|_1
$$

将该式整体乘以 $\sigma^2$，我们得到了与 LASSO 完全相同的形式：

$$
\min_{x} \frac{1}{2}\|y - Dx\|_2^2 + \frac{\sigma^2}{b}\|x\|_1
$$

由此可见，$\ell_1$ [正则化参数](@entry_id:162917) $\lambda$ 直接对应于噪声[方差](@entry_id:200758)与先验[尺度参数](@entry_id:268705)之比，即 $\lambda = \sigma^2/b$。这个结果为 $\ell_1$ 正则化提供了深刻的概率解释：它等价于在[高斯噪声](@entry_id:260752)假设下，对稀疏系数施加拉普拉斯先验的 MAP 估计。[@problem_id:3444200]

### 批量[字典学习](@entry_id:748389)算法

[字典学习](@entry_id:748389)的联合[优化问题](@entry_id:266749) $\min_{D,X} F(D, X)$ 在 $D$ 和 $X$ 上是非凸的，但当固定其中一个变量时，问题关于另一个变量是凸的。这自然地引出了**[交替最小化](@entry_id:198823)** (alternating minimization) 的策略，即迭代地执行以下两个步骤直至收敛：

1.  **[稀疏编码](@entry_id:180626)步**：固定字典 $D$，为数据集 $Y$ 中的每个信号 $y_i$ 求解一个[稀疏编码](@entry_id:180626)问题（如 LASSO）以更新系数矩阵 $X$。
2.  **字典更新步**：固定[系数矩阵](@entry_id:151473) $X$，更新字典 $D$ 以更好地拟[合数](@entry_id:263553)据。

字典更新是不同算法的核心区别所在。下面我们介绍两种经典的批量学习算法：最优方向法 (MOD) 和 [K-SVD](@entry_id:182204)。

#### 最优方向法 (MOD)

在 MOD 算法的字典更新阶段，我们固定稀疏系数矩阵 $X$，然后求解关于 $D$ 的最小二乘问题：

$$
\min_{D \in \mathbb{R}^{m \times p}} \frac{1}{2} \|Y - D X\|_{F}^{2}
$$

这是一个无约束的凸[优化问题](@entry_id:266749)。其梯度为 $\frac{\partial F}{\partial D} = (DX - Y)X^{\top}$。令梯度为零，我们得到**正规方程** (normal equations)：

$$
DXX^{\top} = YX^{\top}
$$

如果矩阵 $XX^{\top} \in \mathbb{R}^{p \times p}$ 是可逆的，我们可以直接得到 $D$ 的闭式解：

$$
D^{\star} = (YX^{\top}) (XX^{\top})^{-1}
$$

在得到 $D^{\star}$ 后，还需要对其列进行归一化，以满足之前讨论的范数约束。[@problem_id:3444154]

MOD 的优点是概念简单，更新规则有直接的解析解。然而，它的计算成本很高。主要开销在于计算 $XX^{\top}$（成本为 $O(p^2 n)$）和[求解线性方程组](@entry_id:169069)（通常通过计算 $(XX^{\top})^{-1}$，成本为 $O(p^3)$）。此外，当 $X$ 的行向量之间存在较强相关性时，$XX^{\top}$ 矩阵会变得**病态** (ill-conditioned)，其[条件数](@entry_id:145150)很大。通过正规方程求解会使[条件数](@entry_id:145150)平方，导致数值计算非常不稳定。[@problem_id:3444122]

#### [K-SVD](@entry_id:182204)

与 MOD 一次性更新整个字典不同，[K-SVD](@entry_id:182204) 采用了一种更精细的策略：一次只更新一个原子及其对应的系数。具体来说，为了更新第 $k$ 个原子 $d_k$ 及其对应的系数行向量 $x_{k,:}$，[K-SVD](@entry_id:182204) 会固定所有其他的原子 $\{d_j\}_{j \neq k}$ 和系数 $\{x_{j,:}\}_{j \neq k}$。

[目标函数](@entry_id:267263)可以重写为：

$$
\|Y - DX\|_F^2 = \|Y - \sum_{j=1}^p d_j x_{j,:}\|_F^2 = \|(Y - \sum_{j \neq k} d_j x_{j,:}) - d_k x_{k,:}\|_F^2
$$

我们定义**残差矩阵** $E_k = Y - \sum_{j \neq k} d_j x_{j,:}$，它表示剔除第 $k$ 个原子贡献后剩下的误差。现在的任务是求解：

$$
\min_{d_k, x_{k,:}} \|E_k - d_k x_{k,:}\|_F^2 \quad \text{subject to} \quad \|d_k\|_2=1
$$

注意到，对于那些从未使用过原子 $d_k$ 的信号（即 $x_{k,i}=0$ 的列），$d_k$ 和 $x_{k,i}$ 的选择对这些列的重构误差没有影响。因此，我们可以只关注那些使用了原子 $d_k$ 的信号。令 $\Omega_k = \{ i : x_{k,i} \neq 0 \}$ 为使用原子 $k$ 的信号索引集，并令 $E_k^{\Omega_k}$ 为 $E_k$ 中对应于 $\Omega_k$ 的列构成的子矩阵，大小为 $m \times |\Omega_k|$。[优化问题](@entry_id:266749)简化为：

$$
\min_{d_k, x_{k,\Omega_k}} \|E_k^{\Omega_k} - d_k x_{k,\Omega_k}\|_F^2 \quad \text{subject to} \quad \|d_k\|_2=1
$$

这里的 $d_k x_{k,\Omega_k}$ 是一个列向量和一个行向量的外积，其结果是一个秩为 1 的矩阵。因此，该问题等价于寻找矩阵 $E_k^{\Omega_k}$ 的**最佳秩-1 近似**。

根据 **Eckart-Young-Mirsky 定理**，一个矩阵在 Frobenius 范数下的最佳秩-1 近似由其**奇异值分解** (Singular Value Decomposition, SVD) 的主成分给出。设 $E_k^{\Omega_k}$ 的 SVD 为 $U \Sigma V^{\top}$，其中 $\sigma_1$ 是最大的[奇异值](@entry_id:152907)，$u_1$ 和 $v_1$ 是对应的主[左奇异向量](@entry_id:751233)和主[右奇异向量](@entry_id:754365)。那么，最佳的秩-1 近似矩阵为 $\sigma_1 u_1 v_1^{\top}$。

通过匹配 $d_k x_{k,\Omega_k}$ 与 $\sigma_1 u_1 v_1^{\top}$，并满足约束 $\|d_k\|_2=1$，我们得到 [K-SVD](@entry_id:182204) 的更新规则：
-   更新原子：$d_k \leftarrow u_1$
-   更新系数：$x_{k,\Omega_k} \leftarrow \sigma_1 v_1^{\top}$

[@problem_id:3444165] [@problem_id:3444170]

相较于 MOD，[K-SVD](@entry_id:182204) 的计算成本更低，因为它在每次迭代中处理的是更小的残差子矩阵 $E_k^{\Omega_k}$，并且只需要计算其主奇异向量，这可以通过迭代方法（如幂法）高效完成，其成本约为 $O(m |\Omega_k|)$。同时，基于 SVD 的更新在数值上比求解病态的线性系统要稳定得多。[@problem_id:3444122]

### 理论保证与可辨识性

到目前为止，我们已经探讨了如何学习字典。但一个关键问题是：我们学习到的字典在多大程度上是有意义的？这涉及到两个核心理论问题：[稀疏表示](@entry_id:191553)的唯一性和字典本身的可辨识性。

#### [稀疏表示](@entry_id:191553)的唯一性

一个好的字典应该能够为每个信号提供唯一的[稀疏表示](@entry_id:191553)。如果同一个信号可以被多种不同的稀疏原子组合来表示，那么表示本身就变得模糊不清。表示的唯一性与字典的几何结构密切相关，一个关键指标是**[互相关性](@entry_id:188177)** (mutual coherence)。

对于一个列归一化的字典 $D$，其[互相关性](@entry_id:188177) $\mu(D)$ 定义为不同原子之间[内积](@entry_id:158127)[绝对值](@entry_id:147688)的最大值：

$$
\mu(D) = \max_{i \neq j} |d_i^{\top} d_j|
$$

$\mu(D)$ 的值域为 $[0, 1]$。一个较低的 $\mu(D)$ 值意味着字典的原子之间近似正交，这是一种理想的性质。可以证明，如果一个信号 $y$ 存在一个 $k$-稀疏的表示 $y=Dx$，那么当稀疏度 $k$ 满足以下条件时，这个表示是所有可能表示中唯一最稀疏的：

$$
k  \frac{1}{2} \left( 1 + \frac{1}{\mu(D)} \right)
$$

这个重要的结果表明，字典的[互相关性](@entry_id:188177)越低（即原子越不相关），它能保证唯一表示的信号稀疏度上限就越高。[@problem_id:3444176]

#### 字典的可辨识性

[字典学习](@entry_id:748389)的终极目标之一是，在理想条件下，能否从数据中恢复出生成这些数据的“真实”字典 $D^{\star}$。这个问题被称为**[可辨识性](@entry_id:194150)** (identifiability)。在无[噪声模型](@entry_id:752540) $Y = D^{\star}X^{\star}$ 中，其中 $X^{\star}$ 的列是 $k$-稀疏的，要从观测数据 $Y$ 中唯一地（在某些允许的模糊性范围内）恢复 $D^{\star}$，需要满足一组充分且必要的条件。

首先，一些模糊性是不可避免的：我们无法区分原子的顺序，也无法区分原子及其系数的联合变号（$(d_j, x_j)$ vs $(-d_j, -x_j)$）。因此，[可辨识性](@entry_id:194150)通常是指在**[置换](@entry_id:136432)和变号** (signed-permutation) 的意义下是唯一的。

要实现这种[可辨识性](@entry_id:194150)，需要以下三个关键条件：

1.  **字典列归一化**：如前所述，这是为了消除尺度模糊性。
2.  **关于字典 $D^{\star}$ 的条件**：字典必须足够“好”，以确保[稀疏表示](@entry_id:191553)的唯一性。一个比基于[互相关性](@entry_id:188177)的条件更根本的准则是**spark**。一个矩阵的 spark 定义为其[线性相关](@entry_id:185830)的列的最小数目。要唯一地恢复 $k$-稀疏解，需要 $\mathrm{spark}(D^{\star})  2k$。这个条件保证了任何两个不同的 $k$-[稀疏解](@entry_id:187463)不可能产生相同的信号。
3.  **关于系数 $X^{\star}$ 的条件**：训练数据必须足够“丰富”，以充分“激发”字典的每个原子。具体来说，系数矩阵 $X^{\star}$ 必须是**行满秩**的，即 $\mathrm{rank}(X^{\star}) = p$。如果 $X^{\star}$ 不是行满秩的，那么它的行之间存在[线性相关](@entry_id:185830)性，这将导致在求解 $Y = DX^{\star}$ 时 $D$ 的解不唯一，从而无法辨识出 $D^{\star}$。

当这三个条件同时满足时，真实的字典 $D^{\star}$ 就可以从无噪声数据 $Y$ 中被唯一地辨识出来（在[置换](@entry_id:136432)和变号的意义下）。去掉其中任何一个条件通常都会破坏可辨识性。[@problem_id:3444125]

### [在线字典学习](@entry_id:752921)

批量学习算法（如 MOD 和 [K-SVD](@entry_id:182204)）要求在每次迭代时都能访问整个数据集，这对于大规模或流式数据是不可行的。**[在线字典学习](@entry_id:752921)** (Online Dictionary Learning, ODL) 为此提供了一个高效的替代方案。

ODL 的目标是最小化**[期望风险](@entry_id:634700)** (expected risk)，即在数据真实[分布](@entry_id:182848) $\mathcal{D}$ 上的期望损失：

$$
F(D) = \mathbb{E}_{y \sim \mathcal{D}}[\ell(D;y)]
$$

其中 $\ell(D;y) = \min_x \{\frac{1}{2}\|y - Dx\|_2^2 + \lambda_1\|x\|_1\}$ 是单个样本的损失。由于真实[分布](@entry_id:182848)未知，我们采用[随机梯度下降](@entry_id:139134) (Stochastic Gradient Descent, SGD) 的思想。算法在时间步 $t$ 接收一个新数据样本 $y_t$，并执行两步：

1.  **[稀疏编码](@entry_id:180626)**：使用当前字典 $D_t$ 为 $y_t$ 找到[稀疏编码](@entry_id:180626) $x_t = \arg\min_x \ell(D_t; y_t)$。
2.  **字典更新**：使用 $(y_t, x_t)$ 计算损失函数关于 $D_t$ 的随机梯度，并沿着负梯度方向更新字典，然后投影回约束集 $\mathcal{C}$：
    $$
    D_{t+1} = \Pi_{\mathcal{C}}(D_t - \eta_t \nabla_D \ell(D_t; y_t))
    $$
    其中 $\eta_t$ 是学习率，$\nabla_D \ell(D_t; y_t) = (D_t x_t - y_t) x_t^{\top}$。

#### 收敛性保证

ODL 算法的收敛性是一个核心的理论问题。对于这个非凸的[随机优化](@entry_id:178938)问题，可以证明在特定条件下，算法的迭代序列 $\{D_t\}$ [几乎必然](@entry_id:262518)会收敛到[期望风险](@entry_id:634700) $F(D)$ 的一个稳定点集。这些充分条件包括：

*   **[数据流](@entry_id:748201)**：数据样本 $\{y_t\}$ 是[独立同分布](@entry_id:169067) (i.i.d.) 的，且其范数有界（即来自一个有界支撑的[分布](@entry_id:182848)）。
*   **约束集**：字典的约束集 $\mathcal{C}$ 是一个非空、闭、凸且**紧致**（即有界）的集合。
*   **正则化**：[稀疏编码](@entry_id:180626)子问题需要是强凸的。这可以通过在[目标函数](@entry_id:267263)中加入一个 $\ell_2$ 正则项（即弹性网正则化）来实现：$\ell(D;y) = \min_x \{\frac{1}{2}\|y - Dx\|_2^2 + \lambda_1\|x\|_1 + \frac{\lambda_2}{2}\|x\|_2^2\}$，其中 $\lambda_2  0$。强凸性保证了[稀疏编码](@entry_id:180626) $x_t$ 的唯一性，并使得[期望风险](@entry_id:634700)函数 $F(D)$ 变得可微。
*   **学习率**：步长序列 $\{\eta_t\}$ 必须满足 Robbins-Monro 条件：$\sum_{t=1}^{\infty} \eta_t = \infty$ 和 $\sum_{t=1}^{\infty} \eta_t^2  \infty$。一个典型的例子是 $\eta_t \propto 1/t$。

在这些条件下，可以保证 ODL 算法的收敛性。[@problem_id:3444157]

#### 样本复杂度

理论分析不仅能保证收敛，还能告诉我们需要多少样本才能成功学习字典。在特定的随机[生成模型](@entry_id:177561)下（例如，稀疏系数的非零位置随机选择，其值是亚[高斯分布](@entry_id:154414)），可以推导出一致性恢复所需的**样本复杂度** (sample complexity)。结果表明，要以高概率恢复一个 $m \times p$ 的字典，其中真实系数是 $k$-稀疏的，所需的样本数量 $n$ 的尺度大致为：

$$
n = \Theta\left(\frac{p \cdot m}{k} \log p\right)
$$

这个结果的直观解释是：我们需要足够的样本以确保（1）**覆盖性**：每个原子都被足够频繁地使用（由因子 $p/k$ 主导，这是“优惠券收集问题”的变体）；（2）**集中性**：对于每个原子，我们有足够的观测数据来从高维噪声（由其他 $k-1$ 个原子引起）中准确地估计其方向（由因子 $m \log p$ 主导）。[@problem_id:3444128]