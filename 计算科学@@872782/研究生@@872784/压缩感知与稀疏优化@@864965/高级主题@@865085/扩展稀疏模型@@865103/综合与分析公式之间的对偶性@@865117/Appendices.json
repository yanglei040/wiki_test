{"hands_on_practices": [{"introduction": "要真正掌握理论，最好的方法莫过于亲手实践。我们的第一个练习旨在将抽象的综合基追踪（Synthesis Basis Pursuit, BP）概念转化为一个简单、具体的计算。通过解决一个二维的例子，您将能清晰地看到稀疏系数向量 $z$ 是如何被找到的，以及它又是如何通过字典 $D$ 重构出信号 $x$ 的。这个练习将为您后续更复杂的探索打下坚实的基础。[@problem_id:3445067]", "problem": "考虑压缩感知和稀疏优化中的合成观点和分析观点。在合成模型中，信号 $x \\in \\mathbb{R}^{n}$ 表示为 $x = D z$，其中 $D \\in \\mathbb{R}^{n \\times p}$ 是一个合成字典，$z \\in \\mathbb{R}^{p}$ 是一个期望稀疏的系数向量。给定无噪声测量值 $y \\in \\mathbb{R}^{m}$ 和传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$，合成基追踪 (BP) 问题是找到具有最小 $\\ell_{1}$-范数的 $z$，使得一致性约束 $A D z = y$ 成立。在分析模型中，一个分析算子 $\\Omega \\in \\mathbb{R}^{q \\times n}$ 作用于 $x$ 以产生期望稀疏的 $\\Omega x$；分析 BP 在测量约束下最小化 $\\Omega x$ 的 $\\ell_{1}$-范数。\n\n在具体设定 $n = 2$, $m = 2$, $p = 2$, $A = I \\in \\mathbb{R}^{2 \\times 2}$, $D = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix}$ 和 $y = (1, 1)^{\\top} \\in \\mathbb{R}^{2}$ 下进行求解。从合成 BP 定义为在线性一致性约束下对 $z$ 的 $\\ell_1$-范数进行约束最小化出发，推导合成 BP 解 $z^{\\star} \\in \\mathbb{R}^{2}$ 以及相应的信号 $x^{\\star} = D z^{\\star} \\in \\mathbb{R}^{2}$。你的推导应基于第一性原理，包括通过 Lagrangian 对偶进行的可行性和最优性验证，并应通过对偶可行性条件明确说明与分析观点的关系。将最终答案表示为数对 $(z^{\\star}, x^{\\star})$。不需要四舍五入，也不涉及物理单位。", "solution": "问题要求解一个具体的合成基追踪 (BP) 问题的解 $(z^{\\star}, x^{\\star})$，并要求推导过程基于 Lagrangian 对偶的第一性原理，且明确地联系到分析观点。\n\n合成 BP 问题被表述为在先行一致性约束下，最小化系数向量 $z$ 的 $\\ell_1$-范数：\n$$ \\min_{z \\in \\mathbb{R}^{p}} \\|z\\|_1 \\quad \\text{subject to} \\quad ADz = y $$\n\n首先，我们代入指定的矩阵和向量值：\n- $n = 2$, $m = 2$, $p = 2$\n- 传感矩阵：$A = I \\in \\mathbb{R}^{2 \\times 2}$ (单位矩阵)\n- 合成字典：$D = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- 测量向量：$y = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\in \\mathbb{R}^{2}$\n\n由于传感矩阵 $A$ 是单位矩阵 $I$，约束 $ADz = y$ 简化为 $Dz = y$。因此，原优化问题是：\n$$ \\min_{z = (z_1, z_2)^{\\top} \\in \\mathbb{R}^{2}} |z_1| + |z_2| \\quad \\text{subject to} \\quad \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} z_1 \\\\ z_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n该约束代表一个包含两个变量的线性方程组：\n1. $z_1 + z_2 = 1$\n2. $z_2 = 1$\n\n从第二个方程，我们得到 $z_2 = 1$。将此代入第一个方程得到 $z_1 + 1 = 1$，这意味着 $z_1 = 0$。\n因此，该优化问题的可行集只包含一个点，$z = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。由于这是唯一的可行点，它必然是该最小化问题的解。我们将此原问题的解记为：\n$$ z^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\n\n相应的信号 $x^{\\star}$ 通过应用合成字典 $D$ 得到：\n$$ x^{\\star} = D z^{\\star} = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n为了正式验证此解并满足题目要求，我们使用 Lagrangian 对偶框架。原问题的 Lagrangian 函数为：\n$$ \\mathcal{L}(z, \\nu) = \\|z\\|_1 + \\nu^{\\top}(y - Dz) $$\n其中 $\\nu \\in \\mathbb{R}^2$ 是与等式约束相关联的 Lagrange 乘子（或对偶变量）。\n\n对于这个凸问题，Karush-Kuhn-Tucker (KKT) 条件为最优性提供了充要条件。一对 $(z^{\\star}, \\nu^{\\star})$ 是最优的，如果它满足：\n1.  **原问题可行性**：$Dz^{\\star} = y$。\n    如上所示，$z^{\\star} = (0, 1)^{\\top}$ 满足此条件：$D z^{\\star} = (1, 1)^{\\top} = y$。\n2.  **平稳性**：Lagrangian 函数关于 $z$ 在 $z^{\\star}$ 处的次梯度必须包含零向量。这意味着 $0 \\in \\partial \\|z^\\star\\|_1 - D^{\\top}\\nu^{\\star}$，或等价地，$D^{\\top}\\nu^{\\star} \\in \\partial \\|z^\\star\\|_1$。\n    $\\ell_1$-范数 $\\|z\\|_1 = |z_1| + |z_2|$ 在 $z^{\\star} = (0, 1)^{\\top}$ 处的次梯度是向量 $s=(s_1, s_2)^{\\top}$ 的集合，其中 $s_1 \\in [-1, 1]$ 且 $s_2 = \\text{sign}(z_2^{\\star}) = 1$。\n    因此，我们必须找到一个对偶向量 $\\nu^{\\star}$，使得对于某个满足 $s_1 \\in [-1, 1]$ 和 $s_2 = 1$ 的 $s$，有 $D^{\\top}\\nu^{\\star} = s$。\n    $D$ 的转置是 $D^{\\top} = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix}$。该条件是：\n    $$ \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} \\nu_1^{\\star} \\\\ \\nu_2^{\\star} \\end{bmatrix} = \\begin{bmatrix} s_1 \\\\ 1 \\end{bmatrix} $$\n    这得到方程组 $\\nu_1^{\\star} = s_1$ 和 $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$。\n3.  **对偶可行性**：对偶变量 $\\nu^{\\star}$ 必须对于对偶问题是可行的。对偶问题是 $\\max_{\\nu} y^{\\top}\\nu$ 约束于 $\\|D^{\\top}\\nu\\|_{\\infty} \\le 1$。所以条件是 $\\|D^{\\top}\\nu^{\\star}\\|_{\\infty} \\le 1$。\n    从平稳性条件，我们有 $D^{\\top}\\nu^{\\star} = (s_1, 1)^{\\top}$。因此，对偶可行性条件是 $\\|(s_1, 1)^{\\top}\\|_{\\infty} = \\max(|s_1|, |1|) \\le 1$。由于我们要求 $s_1 \\in [-1, 1]$，所以 $|s_1| \\le 1$，因此 $\\max(|s_1|, 1) = 1$。该条件对于 $s_1 \\in [-1, 1]$ 的任何选择都满足。\n\n我们可以为 $s_1$ 选择一个具体的值，例如 $s_1=0$。这得到 $\\nu_1^{\\star} = 0$。从 $\\nu_1^{\\star} + \\nu_2^{\\star} = 1$，我们得到 $\\nu_2^{\\star} = 1$。所以，一个有效的对偶凭证是 $\\nu^{\\star} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n通过这个选择，所有 KKT 条件都得到满足，从而正式证明 $z^{\\star} = (0, 1)^{\\top}$ 是唯一的最优解。\n\n最后，我们探讨与分析观点的关系。当字典 $D$ 是方阵且可逆时，合成模型 $x=Dz$ 等价于一个分析模型，其分析算子为 $\\Omega = D^{-1}$。这两个观点之间的联系可以通过它们的对偶问题来揭示。\n\n合成问题的对偶问题是：\n$$ (\\text{D}_{\\text{synth}}): \\quad \\max_{\\nu} y^{\\top}\\nu \\quad \\text{subject to} \\quad \\|D^{\\top}\\nu\\|_{\\infty} \\le 1 $$\n\n分析问题 $\\min_x \\|\\Omega x\\|_1$ s.t. $Ax=y$ 的对偶问题是：\n$$ (\\text{D}_{\\text{anal}}): \\quad \\max_{\\lambda} y^{\\top}\\lambda \\quad \\text{subject to} \\quad A^\\top\\lambda = \\Omega^\\top\\mu \\text{ for some } \\mu \\text{ with } \\|\\mu\\|_{\\infty} \\le 1 $$\n\n在我们的特定设定中，$A=I$ 且我们选择 $\\Omega=D^{-1}$。分析问题的对偶约束变为 $\\lambda = (D^{-1})^{\\top}\\mu$。我们可以用 $\\lambda$ 来表示 $\\mu$：$\\mu = (D^{-1})^{-\\top}\\lambda = D^\\top\\lambda$。将此代入约束 $\\|\\mu\\|_{\\infty} \\le 1$ 中，得到 $\\|D^\\top\\lambda\\|_\\infty \\le 1$。\n\n因此，分析问题的对偶可行集是 $\\{\\lambda \\in \\mathbb{R}^2 : \\|D^\\top\\lambda\\|_\\infty \\le 1\\}$，这与合成问题的对偶可行集 $\\{\\nu \\in \\mathbb{R}^2 : \\|D^\\top\\nu\\|_\\infty \\le 1\\}$ 的形式完全相同。这通过对偶性清晰地表明了两种观点在可逆方阵情况下的等价性。\n\n推导出的合成 BP 解是 $z^{\\star} = (0, 1)^{\\top}$，对应的信号是 $x^{\\star} = (1, 1)^{\\top}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} , \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\end{pmatrix}}$$", "id": "3445067"}, {"introduction": "在我们熟悉了综合模型之后，现在是时候来剖析硬币的另一面——分析模型了。本练习的核心不在于求解答案本身（由于问题的巧妙设计，其解非常直观），而在于引导您掌握凸优化的强大工具箱。您将学习如何将问题转化为线性规划（Linear Program, LP），推导其对偶问题，并利用 KKT 条件来理解解的结构，特别是引入与综合模型中的“支撑集”相对应的“余支撑集”（cosupport）概念。[@problem_id:3445026]", "problem": "考虑分析基追踪（ABP）问题，其定义为在线性测量约束下最小化分析1-范数：给定一个线性算子 $A \\in \\mathbb{R}^{m \\times n}$、一个分析算子 $\\Omega \\in \\mathbb{R}^{p \\times n}$ 和数据 $y \\in \\mathbb{R}^{m}$，ABP问题为\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y.\n$$\n你将分析一个具体的实例，其中 $n=3$，\n$$\n\\Omega=\\begin{bmatrix}1  -1  0\\\\ 0  1  -1\\end{bmatrix}, \\quad A=I_{3}, \\quad y=\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}.\n$$\n从第一性原理出发，完成以下任务：\n1) 引入一个辅助变量 $t \\in \\mathbb{R}^{2}$，将问题重写为仅使用线性等式/不等式和线性目标的标准不等式形式的线性规划。从1-范数的定义出发，严格证明这两种表述的等价性。\n2) 通过构造拉格朗日函数，要求对偶函数对原始变量的下确界为有限值，并写下对偶可行性条件和对偶目标，来推导你的线性规划的拉格朗日对偶问题。不要套用任何预先记下的对偶公式；直接从拉格朗日函数推导对偶问题。\n3) 对给定的数据显式地求解原始问题和对偶问题。确定一个原始最优解 $x^{\\star}$ 和相应的最优松弛变量 $t^{\\star}$，以及一个对偶最优解，并验证互补松弛性和强对偶性。\n4) 确定协支撑集（即索引 $i$ 的集合，使得 $(\\Omega x^{\\star})_{i}=0$）。解释你的确定过程是如何从你的计算和 Karush–Kuhn–Tucker 条件中得出的。\n\n以行向量的形式提供 $x^{\\star}$ 作为你的最终报告答案。无需四舍五入。协支撑集应在你的解中确定并证明，但不需要在最终的方框答案中出现。", "solution": "本问题要求分析一个特定的分析基追踪实例。尽管约束 $A=I_3$ 唯一地确定了解 $x$，使得最小化变得平凡，但题目的核心在于演练将问题转化为线性规划、推导其对偶问题、并运用 KKT 条件进行分析的完整流程。\n\n待求解的问题是：\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\n给定数据如下：\n$$\n\\Omega=\\begin{bmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{bmatrix}, \\quad A=I_{3}=\\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix}, \\quad y=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\n约束 $A x = y$ 变为 $I_3 x = y$，这唯一地确定了可行解为 $x = y = \\begin{bmatrix} 1  0  0 \\end{bmatrix}^T$。虽然这立即给出了原始最优解，但我们将遵循所要求的四步分析。\n\n**1) 线性规划表述**\n\n目标函数是向量 $\\Omega x \\in \\mathbb{R}^2$ 的 $\\ell_1$-范数。令 $z = \\Omega x$。目标函数为 $\\|z\\|_1 = |z_1| + |z_2|$。\n我们引入一个辅助变量 $t = \\begin{bmatrix} t_1 \\\\ t_2 \\end{bmatrix} \\in \\mathbb{R}^2$。表达式 $\\min \\| \\Omega x \\|_1$ 等价于在约束 $t_1 \\ge |(\\Omega x)_1|$ 和 $t_2 \\ge |(\\Omega x)_2|$ 下最小化 $(t_1 + t_2)$。\n\n这种等价性的理由如下。一个值 $c$ 是 $|a|$ 的上界，当且仅当 $c \\ge a$ 和 $c \\ge -a$。因此，条件 $t \\ge |\\Omega x|$（逐元素）等价于一对线性不等式：$\\Omega x \\le t$ 和 $-\\Omega x \\le t$。\n\n对于原始问题的任何可行解 $x$，我们可以选择 $t_i = |(\\Omega x)_i|$，其中 $i=1,2$。这个 $t$ 的选择满足不等式 $t \\ge \\Omega x$ 和 $t \\ge -\\Omega x$。对于新表述中的这对 $(x,t)$，目标值为 $\\sum_i t_i = \\sum_i |(\\Omega x)_i| = \\|\\Omega x\\|_1$。\n反之，考虑新表述中的任何可行对 $(x,t)$。约束要求 $t_i \\ge |(\\Omega x)_i|$。由于目标是最小化 $\\sum_i t_i$，且分量 $t_i$ 在任何其他约束中没有耦合，因此当每个 $t_i$ 尽可能小，即当 $t_i = |(\\Omega x)_i|$ 时，将达到最小值。因此，在 $(x,t)$ 的可行集上最小化 $\\sum_i t_i$ 等价于在 $x$ 的可行集上最小化 $\\|\\Omega x\\|_1$。\n\n因此，问题被转化为以下线性规划（LP）：\n$$\n\\min_{x \\in \\mathbb{R}^3, t \\in \\mathbb{R}^2} \\quad t_1 + t_2\n$$\n约束条件为：\n$$\n\\begin{align*}\nA x = y \\\\\n\\Omega x - t \\le 0 \\\\\n-\\Omega x - t \\le 0\n\\end{align*}\n$$\n其中不等式是分量级别的，$0$ 是 $\\mathbb{R}^2$ 中的零向量。\n\n**2) 拉格朗日对偶的推导**\n\n为了推导对偶问题，我们为每个约束关联一个对偶变量来构造拉格朗日函数。设 $\\nu \\in \\mathbb{R}^3$ 为等式约束 $y - Ax = 0$ 的拉格朗日乘子。设 $\\lambda_1, \\lambda_2 \\in \\mathbb{R}^2$ 为不等式约束的拉格朗日乘子，且有非负性要求 $\\lambda_1 \\ge 0$ 和 $\\lambda_2 \\ge 0$。\n\n拉格朗日函数 $L(x, t, \\nu, \\lambda_1, \\lambda_2)$ 为：\n$$\nL = \\mathbf{1}^T t + \\nu^T(y - Ax) + \\lambda_1^T(\\Omega x - t) + \\lambda_2^T(-\\Omega x - t)\n$$\n其中 $\\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。我们将各项重新组合，按原始变量 $x$ 和 $t$ 分组：\n$$\nL = \\nu^T y + x^T(-A^T\\nu + \\Omega^T\\lambda_1 - \\Omega^T\\lambda_2) + t^T(\\mathbf{1} - \\lambda_1 - \\lambda_2)\n$$\n拉格朗日对偶函数 $g(\\nu, \\lambda_1, \\lambda_2)$ 是拉格朗日函数关于原始变量 $x$ 和 $t$ 的下确界：\n$$\ng(\\nu, \\lambda_1, \\lambda_2) = \\inf_{x, t} L(x, t, \\nu, \\lambda_1, \\lambda_2)\n$$\n为了使下确界为有限值（即不为 $-\\infty$），$x$ 和 $t$ 的线性项的系数必须为零。这给出了对偶可行性条件：\n1.  关于 $x$ 的系数: $-A^T\\nu + \\Omega^T(\\lambda_1 - \\lambda_2) = 0 \\implies A^T\\nu = \\Omega^T(\\lambda_1 - \\lambda_2)$。\n2.  关于 $t$ 的系数: $\\mathbf{1} - \\lambda_1 - \\lambda_2 = 0 \\implies \\lambda_1 + \\lambda_2 = \\mathbf{1}$。\n\n在这些条件下，拉格朗日函数变为 $L = \\nu^T y$。对偶问题即为最大化该值：\n$$\n\\max_{\\nu, \\lambda_1, \\lambda_2} \\quad \\nu^T y\n$$\n其约束条件为：\n$$\n\\begin{align*}\nA^T\\nu = \\Omega^T(\\lambda_1 - \\lambda_2) \\\\\n\\lambda_1 + \\lambda_2 = \\mathbf{1} \\\\\n\\lambda_1 \\ge 0, \\quad \\lambda_2 \\ge 0\n\\end{align*}\n$$\n我们可以通过引入新变量 $\\mu = \\lambda_1 - \\lambda_2$ 来简化此表述。从 $\\lambda_1 + \\lambda_2 = \\mathbf{1}$，我们可以解出 $\\lambda_1 = \\frac{1}{2}(\\mathbf{1} + \\mu)$ 和 $\\lambda_2 = \\frac{1}{2}(\\mathbf{1} - \\mu)$。非负约束 $\\lambda_1, \\lambda_2 \\ge 0$ 等价于对每个分量 $i$ 都有 $-1 \\le \\mu_i \\le 1$，即 $\\|\\mu\\|_\\infty \\le 1$。\n第一个对偶约束变为 $A^T\\nu = \\Omega^T\\mu$。因为 $A=I_3$，$A^T=I_3$，所以有 $\\nu = \\Omega^T\\mu$。将此代入目标函数，对偶问题最终可写为只关于 $\\mu$ 的形式：\n$$\n\\max_{\\mu} \\quad (\\Omega^T\\mu)^T y = \\mu^T(\\Omega y) \\quad \\text{subject to} \\quad \\|\\mu\\|_\\infty \\le 1.\n$$\n\n**3) 原始解和对偶解**\n\n**原始解：**\n约束 $Ax=y$ 在 $A=I_3$ 的情况下将原始解固定为 $x^\\star = y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n向量 $\\Omega x^\\star$ 是：\n$$\n\\Omega x^\\star = \\begin{bmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\n对于LP表述，最优松弛变量 $t^\\star$ 必须满足 $t^\\star_i = |(\\Omega x^\\star)_i|$。\n$$\nt^\\star = \\begin{bmatrix} |1| \\\\ |0| \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\n原始问题的最优值为 $t^\\star_1 + t^\\star_2 = 1+0=1$。\n\n**对偶解：**\n对偶问题是在 $\\|\\mu\\|_\\infty \\le 1$ 的约束下最大化 $\\mu^T (\\Omega y)$。我们有 $\\Omega y = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n所以我们必须求解：\n$$\n\\max_{\\mu_1, \\mu_2} \\quad \\mu_1(1) + \\mu_2(0) \\quad \\text{subject to} \\quad -1 \\le \\mu_1 \\le 1, \\quad -1 \\le \\mu_2 \\le 1.\n$$\n目标函数就是 $\\mu_1$。为了最大化它，我们选择可能的最大值 $\\mu_1^\\star = 1$。变量 $\\mu_2$ 不影响目标函数，所以其可行范围 $[-1, 1]$ 内的任何值都是最优的。一个有效的选择是 $\\mu_2^\\star = 0$。\n一个对偶最优解是 $\\mu^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n对偶最优值是 $1$。这证实了强对偶性，因为原始和对偶最优值相等。\n\n我们可以找到相应的完整对偶变量集：\n$\\nu^\\star = \\Omega^T \\mu^\\star = \\begin{bmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\end{bmatrix}$。\n$\\lambda_1^\\star = \\frac{1}{2}(\\mathbf{1}+\\mu^\\star) = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}$。\n$\\lambda_2^\\star = \\frac{1}{2}(\\mathbf{1}-\\mu^\\star) = \\frac{1}{2} \\left( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 0 \\\\ 1/2 \\end{bmatrix}$。\n\n**验证互补松弛性：**\nLP的KKT互补松弛性条件是 $\\lambda_1^T(\\Omega x - t)=0$ 和 $\\lambda_2^T(-\\Omega x - t)=0$。\n让我们在最优解 $(x^\\star, t^\\star)$ 和 $(\\lambda_1^\\star, \\lambda_2^\\star)$ 处检查这些条件。\n原始松弛量：\n$\\Omega x^\\star - t^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n$-\\Omega x^\\star - t^\\star = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} -2 \\\\ 0 \\end{bmatrix}$。\n对偶变量： $\\lambda_1^\\star = \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix}$，$\\lambda_2^\\star = \\begin{bmatrix} 0 \\\\ 1/2 \\end{bmatrix}$。\n\n条件1： $(\\lambda_1^\\star)_i (\\Omega x^\\star - t^\\star)_i = 0$。\n- 对于 $i=1$：$1 \\times 0 = 0$。\n- 对于 $i=2$：$(1/2) \\times 0 = 0$。\n条件2： $(\\lambda_2^\\star)_i (-\\Omega x^\\star - t^\\star)_i = 0$。\n- 对于 $i=1$：$0 \\times (-2) = 0$。\n- 对于 $i=2$：$(1/2) \\times 0 = 0$。\n所有条件都得到满足，验证了我们解的最优性。\n\n**4) 协支撑集和 Karush–Kuhn–Tucker (KKT) 条件**\n\n向量 $z$ 的协支撑集是指标 $i$ 的集合，其中 $z_i=0$。对于最优解 $x^\\star$，我们考察向量 $\\Omega x^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。第一个分量非零，第二个分量为零。因此，$\\Omega x^\\star$ 的协支撑集是 $\\{2\\}$。\n\n与原始非光滑问题 $\\min_x \\|\\Omega x\\|_1$ s.t. $Ax=y$ 的 KKT 条件的联系为我们提供了深刻的理解。平稳性条件是 $0 \\in \\partial_x L(x, \\nu)$，其中 $L(x,\\nu) = \\|\\Omega x\\|_1 + \\nu^T(y-Ax)$。这给出：\n$$\n0 \\in \\Omega^T \\partial(\\|\\cdot\\|_1)|_{\\Omega x} - A^T\\nu\n$$\n这意味着必须存在一个向量 $\\mu \\in \\partial(\\|\\cdot\\|_1)|_{\\Omega x}$ 使得 $A^T\\nu = \\Omega^T\\mu$。\n$\\ell_1$-范数在点 $z$ 处的次梯度是向量 $\\mu$ 的集合，其中：\n- 若 $z_i \\ne 0$，则 $\\mu_i = \\text{sign}(z_i)$（即 $i$ 在支撑集中）。\n- 若 $z_i = 0$，则 $\\mu_i \\in [-1, 1]$（即 $i$ 在协支撑集中）。\n\n在最优解 $x^\\star$ 处，我们有 $\\Omega x^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。KKT 条件要求存在一个最优对偶变量 $\\mu^\\star$ 使得：\n- $\\mu^\\star_1 = \\text{sign}(1) = 1$。\n- $\\mu^\\star_2 \\in [-1, 1]$。\n\n我们的对偶问题求解得到了一组形式为 $\\begin{bmatrix} 1 \\\\ \\mu_2 \\end{bmatrix}$ 的最优对偶变量 $\\mu$，其中 $\\mu_2 \\in [-1, 1]$。这与 KKT 条件完全一致。我们的特定选择 $\\mu^\\star = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 就是这样一个有效的凭证。\n\n协支撑集的确定源于此关系。如果找到一个最优对偶解 $\\mu^\\star$ 其中 $|\\mu_i^\\star|  1$，那么必然有 $(\\Omega x^\\star)_i=0$，所以 $i$ 必须在协支撑集中。在我们的例子中，我们找到了 $\\mu^\\star=\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$。由于 $|\\mu_2^\\star|=0  1$，我们可以得出结论，指标 $2$ 在 $\\Omega x^\\star$ 的协支撑集中。由于 $|\\mu_1^\\star|=1$，指标 $1$ 不在协支撑集中（它在支撑集中）。这与我们的直接计算相符。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0  0\n\\end{pmatrix}\n}\n$$", "id": "3445026"}, {"introduction": "作为本章的总结性实践，我们将从纸笔推演迈向计算现实。这个练习将向您证明，在综合与分析模型之间的选择并非纯粹的学术问题，它直接影响着实际效果。通过亲手实现两种模型对应的 LASSO 问题的标准优化算法（FISTA 和 ADMM），并在具有不同结构特征的信号上进行测试，您将获得关于两种方法优劣的宝贵直觉，学会如何根据具体问题选择最合适的模型。[@problem_id:3445015]", "problem": "要求您实现一个完整且可运行的程序，在一个小规模场景下对稀疏恢复的合成公式和分析公式进行数值比较。该比较必须在多个测试用例上进行，以凸显两种公式之间的结构性差异，这些差异根植于压缩感知和稀疏优化的背景。其基本基础是两种公式的凸优化框架以及相关的邻近算法。\n\n设 $A \\in \\mathbb{R}^{2 \\times 3}$ 为一个随机高斯传感矩阵，$D = I_3$ 为用于合成稀疏性的单位字典，并设 $\\Omega \\in \\mathbb{R}^{2 \\times 3}$ 为由下式定义的一阶差分算子\n$$\n\\Omega = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}.\n$$\n对于一个给定的真实信号 $x^\\star \\in \\mathbb{R}^{3}$，其测量值为 $y = A x^\\star$（无噪声）。您将根据同一测量值 $y$ 计算以下两个估计量（最小绝对收缩和选择算子，Least Absolute Shrinkage and Selection Operator (LASSO)）：\n- 使用 $D=I_3$ 的合成 LASSO：\n$$\n\\widehat{x}_{\\text{synth}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n- 使用一阶差分的分析 LASSO：\n$$\n\\widehat{x}_{\\text{anal}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|\\Omega x\\|_1.\n$$\n\n您必须使用与上述目标函数一致的正确凸优化算法来计算数值解。对于合成 LASSO，请使用一种邻近梯度法，例如快速迭代收缩阈值算法 (FISTA)。对于分析 LASSO，请使用一种分裂方案，例如交替方向乘子法 (ADMM)，并使用分裂变量 $v = \\Omega x$。\n\n传感矩阵 $A$ 必须使用固定的伪随机种子生成一次，以保证可复现性，具体如下：\n- $A$ 的元素从零均值、单位方差的高斯分布中独立抽取，并按 $1/\\sqrt{2}$ 进行缩放，即 $A = \\frac{1}{\\sqrt{2}} G$，其中 $G_{ij} \\sim \\mathcal{N}(0,1)$。\n- 为此次生成使用伪随机种子 $7$，并在所有测试用例中保持 $A$ 不变。\n\n不涉及任何物理单位或角度；不使用百分比。\n\n测试套件：\n- 使用以下四个测试用例。在每个用例中，计算 $y = A x^\\star$。为每个用例报告三个浮点数：\n    1. 两个解之间的欧几里得距离，$\\|\\widehat{x}_{\\text{synth}} - \\widehat{x}_{\\text{anal}}\\|_2$。\n    2. 合成解相对于真实值的欧几里得误差，$\\|\\widehat{x}_{\\text{synth}} - x^\\star\\|_2$。\n    3. 分析解相对于真实值的欧几里得误差，$\\|\\widehat{x}_{\\text{anal}} - x^\\star\\|_2$。\n- 各用例如下：\n    - 用例 1：$x^\\star = [1.0, 1.0, 0.0]^\\top$，$\\lambda = 0.2$。\n    - 用例 2：$x^\\star = [1.5, 0.0, 0.0]^\\top$，$\\lambda = 0.2$。\n    - 用例 3：$x^\\star = [0.5, 0.5, 0.5]^\\top$，$\\lambda = 0.5$。\n    - 用例 4：$x^\\star = [0.8, -0.2, 0.8]^\\top$，$\\lambda = 5.0$。\n\n算法要求：\n- 对于合成 LASSO，实现一个可证明收敛的邻近梯度算法，其步长需满足数据保真项梯度的利普希茨常数条件。\n- 对于分析 LASSO，实现交替方向乘子法 (ADMM)，分裂方式为 $v = \\Omega x$，对 $v$ 进行软阈值操作，并通过求解所产生的正定系统来进行 $x$ 更新的线性求解。\n- ADMM 停止条件使用绝对容差 $10^{-6}$ 和相对容差 $10^{-5}$，为保证鲁棒性，设置迭代次数上限不低于 $5000$ 次。对于邻近梯度法，使用不低于 $5000$ 次的固定迭代上限，并基于连续迭代值差异的固定停止规则，容差为 $10^{-9}$。\n\n最终输出规格：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表包含四个项目（每个测试用例一个），每个项目本身是按上述顺序排列的三元素列表。所有浮点数必须四舍五入到小数点后恰好六位。格式必须严格如下：\n\"[ [d1,e1s,e1a], [d2,e2s,e2a], [d3,e3s,e3a], [d4,e4s,e4a] ]\"\n不含任何额外文本，其中 $d_k$ 是用例 $k$ 中两个解之间的欧几里得距离，$e_{ks}$ 是用例 $k$ 的合成误差，$e_{ka}$ 是用例 $k$ 的分析误差。\n\n注意：\n- 该问题纯粹是数学和算法问题。在给定种子下，所有量都是确定性的。通过使用适合小型对称正定系统的方法，确保在 ADMM 中出现的线性求解过程中的数值稳定性。", "solution": "该问题是基于凸优化和稀疏信号恢复原理的适定问题，具备获得确定性数值解所需的所有参数和条件。任务是使用指定的迭代算法，实现并比较两种标准的稀疏恢复模型：合成 LASSO 和分析 LASSO。\n\n解决方案首先定义数学模型，然后详细说明用于求解这些模型的数值算法，并提供实现代码。\n\n### 1. 问题表述\n\n给定一个线性测量模型 $y = A x^\\star$，其中 $x^\\star \\in \\mathbb{R}^3$ 是真实信号，$A \\in \\mathbb{R}^{2 \\times 3}$ 是传感矩阵，$y \\in \\mathbb{R}^2$ 是测量向量。目标是利用两种不同的促进稀疏性的优化问题，从 $y$ 和 $A$ 中恢复出 $x^\\star$ 的一个估计。\n\n**合成 LASSO：** 该模型假设信号 $x$ 在一个合成字典 $D$ 中是稀疏的，这里给定的字典是单位矩阵 $D=I_3$。优化问题为：\n$$\n\\widehat{x}_{\\text{synth}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|x\\|_1\n$$\n在这里，$\\ell_1$范数 $\\|x\\|_1$ 直接促进信号系数的稀疏性。\n\n**分析 LASSO：** 该模型假设信号 $x$ 经过分析算子 $\\Omega$ 变换后具有稀疏表示。在此问题中，$\\Omega \\in \\mathbb{R}^{2 \\times 3}$ 是一阶差分算子。优化问题为：\n$$\n\\widehat{x}_{\\text{anal}} \\in \\arg\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|\\Omega x\\|_1\n$$\n该公式鼓励信号梯度的稀疏性，对于分段常数信号非常有效。\n\n### 2. 合成 LASSO：快速迭代收缩阈值算法 (FISTA)\n\n合成 LASSO 问题是一个形式为 $\\min_x f(x) + g(x)$ 的凸优化问题，其中：\n- $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ 是一个光滑、凸、可微的函数。其梯度为 $\\nabla f(x) = A^\\top(A x - y)$。该梯度是利普希茨连续的，常数为 $L = \\|A^\\top A\\|_2$，其中 $\\|\\cdot\\|_2$ 表示谱范数。\n- $g(x) = \\lambda \\|x\\|_1$ 是一个凸的、不可微的函数。\n\n这种结构非常适合使用邻近梯度法。FISTA 是基本迭代收缩阈值算法 (ISTA) 的加速版本。这些方法的核心是 $g(x)$ 的邻近算子，即软阈值函数：\n$$\n\\text{prox}_{\\gamma g}(z) = \\text{soft}(z, \\gamma\\lambda)_i = \\text{sgn}(z_i) \\max(|z_i| - \\gamma\\lambda, 0)\n$$\nFISTA 引入了一个动量项来加速收敛。迭代更新如下，从 $x_0 \\in \\mathbb{R}^3$, $y_1=x_0$, $t_1=1$ 开始：\n对于 $k = 1, 2, \\ldots$：\n1. 通过从动量点 $y_k$ 进行梯度步长，并应用邻近算子，来计算下一个迭代值 $x_k$：\n   $$x_k = \\text{prox}_{\\frac{1}{L}g}(y_k - \\frac{1}{L}\\nabla f(y_k)) = \\text{soft}\\left(y_k - \\frac{1}{L}A^\\top(Ay_k - y), \\frac{\\lambda}{L}\\right)$$\n2. 更新动量参数 $t_{k+1}$：\n   $$t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$$\n3. 更新动量点 $y_{k+1}$：\n   $$y_{k+1} = x_k + \\frac{t_k-1}{t_{k+1}}(x_k - x_{k-1})$$\n\n算法使用 $x_0 = x_{-1} = 0 \\in \\mathbb{R}^3$ 和 $t_1 = 1$ 进行初始化。当连续迭代值之间的欧几里得距离低于某个容差时，即 $\\|x_k - x_{k-1}\\|_2  10^{-9}$，或者达到最大迭代次数（$5000$）时，算法终止。\n\n### 3. 分析 LASSO：交替方向乘子法 (ADMM)\n\n为了应用 ADMM，分析 LASSO 问题被重构成一个约束优化问题。我们引入一个分裂变量 $v \\in \\mathbb{R}^2$，使得 $v = \\Omega x$：\n$$\n\\min_{x, v} \\ \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\|v\\|_1 \\quad \\text{满足} \\quad \\Omega x - v = 0\n$$\n该问题的增广拉格朗日函数（缩放对偶形式）为：\n$$\nL_\\rho(x, v, u) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda\\|v\\|_1 + \\frac{\\rho}{2}\\|\\Omega x - v + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\n其中 $u \\in \\mathbb{R}^2$ 是缩放对偶变量，$\\rho > 0$ 是罚参数。ADMM 通过迭代地最小化 $L_\\rho$（分别对 $x$ 和 $v$），然后更新对偶变量 $u$ 来进行。\n\n迭代更新从 $x_0, v_0, u_0$（通常为零向量）开始：\n1.  **$x$ 更新**：关于 $x$ 最小化 $L_\\rho$：\n    $$x_{k+1} = \\arg\\min_x \\left( \\frac{1}{2}\\|A x - y\\|_2^2 + \\frac{\\rho}{2}\\|\\Omega x - v_k + u_k\\|_2^2 \\right)$$\n    这是一个二次问题，其解可以通过求解梯度为零得到的线性系统来找到：\n    $$(A^\\top A + \\rho \\Omega^\\top \\Omega) x_{k+1} = A^\\top y + \\rho \\Omega^\\top(v_k - u_k)$$\n    矩阵 $P = A^\\top A + \\rho \\Omega^\\top \\Omega$ 是一个小型（$3 \\times 3$）对称正定矩阵，因此可以预先计算其逆矩阵以实现高效更新。\n\n2.  **$v$ 更新**：关于 $v$ 最小化 $L_\\rho$：\n    $$v_{k+1} = \\arg\\min_v \\left( \\lambda\\|v\\|_1 + \\frac{\\rho}{2}\\|\\Omega x_{k+1} - v + u_k\\|_2^2 \\right)$$\n    其解由软阈值算子给出：\n    $$v_{k+1} = \\text{soft}\\left(\\Omega x_{k+1} + u_k, \\frac{\\lambda}{\\rho}\\right)$$\n\n3.  **$u$ 更新**：更新对偶变量：\n    $$u_{k+1} = u_k + \\Omega x_{k+1} - v_{k+1}$$\n\n算法根据原始残差和对偶残差终止。\n- 原始残差：$r_{k+1} = \\Omega x_{k+1} - v_{k+1}$\n- 对偶残差：$s_{k+1} = \\rho \\Omega^\\top(v_{k+1} - v_k)$\n\n停止条件是 $\\|r_{k+1}\\|_2 \\leq \\epsilon^{\\text{pri}}$ 和 $\\|s_{k+1}\\|_2 \\leq \\epsilon^{\\text{dual}}$，其中容差为：\n- $\\epsilon^{\\text{pri}} = \\sqrt{2}\\epsilon^{\\text{abs}} + \\epsilon^{\\text{rel}} \\max(\\|\\Omega x_{k+1}\\|_2, \\|v_{k+1}\\|_2)$\n- $\\epsilon^{\\text{dual}} = \\sqrt{3}\\epsilon^{\\text{abs}} + \\epsilon^{\\text{rel}} \\|\\rho \\Omega^\\top u_{k+1}\\|_2$\n\n指定的容差为 $\\epsilon^{\\text{abs}} = 10^{-6}$ 和 $\\epsilon^{\\text{rel}} = 10^{-5}$。使用 $\\rho=1.0$ 的值，最大迭代次数为 $5000$。\n\n### 4. 数值实现与评估\n以下是实现上述步骤的 Python 代码。\n\n```python\nimport numpy as np\n\ndef soft_threshold(u, t):\n    \"\"\"\n    Soft-thresholding operator for vectors.\n    \"\"\"\n    return np.sign(u) * np.maximum(np.abs(u) - t, 0)\n\ndef fista_solver(A, y, lambda_val, max_iter=5000, tol=1e-9):\n    \"\"\"\n    Solves the synthesis LASSO problem using FISTA.\n    min_x 0.5 * ||Ax - y||^2 + lambda * ||x||_1\n    \"\"\"\n    n, p = A.shape\n    \n    # Lipschitz constant of the gradient of the smooth term\n    L = np.linalg.norm(A.T @ A, ord=2)\n    step_size = 1.0 / L\n\n    x_k = np.zeros((p, 1))\n    x_k_minus_1 = np.zeros((p, 1))\n    y_k = np.zeros((p, 1))\n    t_k = 1.0\n\n    for _ in range(max_iter):\n        grad_f_y = A.T @ (A @ y_k - y)\n        x_k_plus_1 = soft_threshold(y_k - step_size * grad_f_y, step_size * lambda_val)\n\n        if np.linalg.norm(x_k_plus_1 - x_k)  tol:\n            break\n\n        t_k_plus_1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n        y_k_plus_1 = x_k_plus_1 + ((t_k - 1.0) / t_k_plus_1) * (x_k_plus_1 - x_k)\n\n        x_k_minus_1 = x_k\n        x_k = x_k_plus_1\n        y_k = y_k_plus_1\n        t_k = t_k_plus_1\n        \n    return x_k\n\ndef admm_solver(A, y, Omega, lambda_val, rho=1.0, max_iter=5000, eps_abs=1e-6, eps_rel=1e-5):\n    \"\"\"\n    Solves the analysis LASSO problem using ADMM.\n    min_x 0.5 * ||Ax - y||^2 + lambda * ||Omega * x||_1\n    \"\"\"\n    n, p = A.shape\n    q, _ = Omega.shape\n\n    # Pre-compute matrices for x-update\n    AtA = A.T @ A\n    Aty = A.T @ y\n    OmegatOmega = Omega.T @ Omega\n    P_inv = np.linalg.inv(AtA + rho * OmegatOmega)\n\n    x_k = np.zeros((p, 1))\n    v_k = np.zeros((q, 1))\n    u_k = np.zeros((q, 1))\n\n    for k in range(max_iter):\n        # x-update\n        rhs_x = Aty + rho * Omega.T @ (v_k - u_k)\n        x_k_plus_1 = P_inv @ rhs_x\n\n        # v-update\n        v_k_old = v_k\n        v_k_plus_1 = soft_threshold(Omega @ x_k_plus_1 + u_k, lambda_val / rho)\n\n        # u-update\n        u_k_plus_1 = u_k + Omega @ x_k_plus_1 - v_k_plus_1\n\n        # Stopping criteria\n        # Primal residual\n        r_k_plus_1 = Omega @ x_k_plus_1 - v_k_plus_1\n        eps_pri = np.sqrt(q) * eps_abs + eps_rel * np.maximum(np.linalg.norm(Omega @ x_k_plus_1), np.linalg.norm(v_k_plus_1))\n        \n        # Dual residual\n        s_k_plus_1 = rho * Omega.T @ (v_k_plus_1 - v_k_old)\n        eps_dual = np.sqrt(p) * eps_abs + eps_rel * np.linalg.norm(rho * Omega.T @ u_k_plus_1)\n\n        if np.linalg.norm(r_k_plus_1)  eps_pri and np.linalg.norm(s_k_plus_1)  eps_dual:\n            x_k = x_k_plus_1\n            break\n            \n        x_k = x_k_plus_1\n        v_k = v_k_plus_1\n        u_k = u_k_plus_1\n    \n    return x_k\n\ndef solve_and_format():\n    # Set up the problem parameters\n    np.random.seed(7)\n    G = np.random.randn(2, 3)\n    A = G / np.sqrt(2)\n    \n    Omega = np.array([[-1., 1., 0.], [0., -1., 1.]])\n\n    test_cases = [\n        {'x_star': np.array([1.0, 1.0, 0.0]), 'lambda': 0.2},\n        {'x_star': np.array([1.5, 0.0, 0.0]), 'lambda': 0.2},\n        {'x_star': np.array([0.5, 0.5, 0.5]), 'lambda': 0.5},\n        {'x_star': np.array([0.8, -0.2, 0.8]), 'lambda': 5.0}\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        x_star = case['x_star'].reshape(3, 1)\n        lambda_val = case['lambda']\n        \n        # Generate measurements\n        y = A @ x_star\n        \n        # Solve for synthesis LASSO\n        x_synth = fista_solver(A, y, lambda_val)\n        \n        # Solve for analysis LASSO\n        x_anal = admm_solver(A, y, Omega, lambda_val)\n\n        # Calculate metrics\n        dist_sol = np.linalg.norm(x_synth - x_anal)\n        err_synth = np.linalg.norm(x_synth - x_star)\n        err_anal = np.linalg.norm(x_anal - x_star)\n        \n        all_results.append([dist_sol, err_synth, err_anal])\n\n    # Format the final output string\n    results_str_list = []\n    for res_tuple in all_results:\n        formatted_tuple = [f\"{x:.6f}\" for x in res_tuple]\n        results_str_list.append(f\"[{','.join(formatted_tuple)}]\")\n    final_output = f\"[{','.join(results_str_list)}]\"\n        \n    return final_output\n```", "answer": "[[0.297059,0.228945,0.076822],[0.000000,0.000000,0.301386],[1.139634,1.139634,0.000000],[2.146033,1.386033,0.760000]]", "id": "3445015"}]}