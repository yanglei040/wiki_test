{"hands_on_practices": [{"introduction": "许多分布式优化问题都依赖于迭代算法求解，而交替方向乘子法 (ADMM) 是其中一个强大而流行的框架。本练习将引导你推导出基本分布式共识 LASSO 问题的 ADMM 更新步骤，让你对这些算法的“底层”工作原理有深刻的理解 [@problem_id:3444486]。通过这个推导，你将掌握将一个约束优化问题分解为多个可并行处理的子问题，并通过协调步骤达成共识的核心技术。", "problem": "考虑一个分布式传感网络，其数据为 $\\{A_{\\ell} \\in \\mathbb{R}^{m_{\\ell} \\times n},\\, y_{\\ell} \\in \\mathbb{R}^{m_{\\ell}}\\}_{\\ell=1}^{L}$，其中 $L \\in \\mathbb{N}$ 个代理各自通过带有加性噪声的线性模型 $y_{\\ell} \\approx A_{\\ell} z^{\\star}$ 测量一个共同的 $n$ 维信号。目标是通过求解一致性最小绝对值收缩和选择算子 (LASSO) 问题来估计一个稀疏的一致性向量 $z \\in \\mathbb{R}^{n}$：\n$$\n\\min_{\\{x_{\\ell}\\}_{\\ell=1}^{L},\\, z \\in \\mathbb{R}^{n}} \\sum_{\\ell=1}^{L} \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{约束条件} \\quad x_{\\ell} = z \\quad \\text{对所有 } \\ell,\n$$\n其中 $\\lambda > 0$ 是 $\\ell_{1}$ 正则化参数。使用带有缩放对偶变量 $\\{u_{\\ell}\\}_{\\ell=1}^{L}$ 和惩罚参数 $\\rho > 0$ 的交替方向乘子法 (ADMM; Alternating Direction Method of Multipliers) 来强制达成一致性。\n\n从约束形式和相应的带有缩放对偶变量的增广拉格朗日量出发，仅使用问题数据 $\\{A_{\\ell}, y_{\\ell}\\}$、当前迭代值 $\\{z^{k}, u_{\\ell}^{k}\\}$ 以及参数 $\\lambda$ 和 $\\rho$，推导在第 $k+1$ 次迭代时局部原始变量 $\\{x_{\\ell}\\}$ 和全局一致性变量 $z$ 的显式闭式 ADMM 更新式。假设对于每个 $\\ell$，矩阵 $A_{\\ell}^{\\top} A_{\\ell} + \\rho I$ 是可逆的，其中 $I$ 表示 $n \\times n$ 单位矩阵。\n\n将最终答案表示为一个包含局部原始变量、全局一致性变量和缩放对偶变量的三个更新映射的单个闭式解析表达式。您的答案必须是符号化的（无须数值计算），并且必须用 $A_{\\ell}$、$y_{\\ell}$、$z^{k}$、$u_{\\ell}^{k}$、$\\lambda$、$\\rho$ 和 $L$ 来表示。最终答案中不要提供不等式或方程；只提供更新的解析表达式。", "solution": "该问题要求推导分布式一致性 LASSO 形式的交替方向乘子法 (ADMM) 更新式。\n\n优化问题由下式给出：\n$$ \\min_{\\{x_{\\ell}\\}_{\\ell=1}^{L},\\, z \\in \\mathbb{R}^{n}} \\sum_{\\ell=1}^{L} \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{约束条件} \\quad x_{\\ell} = z \\quad \\text{对所有 } \\ell \\in \\{1, \\ldots, L\\} $$\n这是一个一致性问题，其中每个代理 $\\ell$ 都有一个局部变量 $x_{\\ell}$ 和一个局部数据拟合项 $f_{\\ell}(x_{\\ell}) = \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2}$。所有代理必须就单个一致性变量 $z$ 达成一致，该变量通过 $\\ell_1$ 范数进行正则化以促进稀疏性。\n\nADMM 算法通过构造增广拉格朗日量来解决此问题。对于此问题，使用缩放对偶变量 $\\{u_{\\ell}\\}$，增广拉格朗日量 $\\mathcal{L}_{\\rho}$ 为：\n$$ \\mathcal{L}_{\\rho}(\\{x_{\\ell}\\}, z, \\{u_{\\ell}\\}) = \\sum_{\\ell=1}^{L} \\left( \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} \\right) + \\lambda \\|z\\|_{1} + \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell} - z + u_{\\ell}\\|_{2}^{2} - \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|u_{\\ell}\\|_{2}^{2} $$\n此处，$\\rho > 0$ 是惩罚参数。ADMM 算法在每次迭代 $k+1$ 时通过迭代以下三个更新步骤进行：\n1.  关于局部变量 $\\{x_{\\ell}\\}$ 最小化 $\\mathcal{L}_{\\rho}$。\n2.  关于全局一致性变量 $z$ 最小化 $\\mathcal{L}_{\\rho}$。\n3.  更新缩放对偶变量 $\\{u_{\\ell}\\}$。\n\n让我们显式地推导每个更新步骤。\n\n**1. $x_{\\ell}$ 更新（局部变量更新）**\n在第 $k+1$ 次迭代时，我们通过关于 $x_{\\ell}$ 最小化增广拉格朗日量来更新每个 $x_{\\ell}$，同时将其他变量固定在它们的最新值（$z^k$、$u_{\\ell}^k$）。由于拉格朗日量的结构，关于 $\\{x_{\\ell}\\}$ 的最小化可以解耦为 $L$ 个独立的问题，每个代理 $\\ell$ 对应一个。\n$$ x_{\\ell}^{k+1} = \\arg\\min_{x_{\\ell}} \\left( \\frac{1}{2} \\|A_{\\ell} x_{\\ell} - y_{\\ell}\\|_{2}^{2} + \\frac{\\rho}{2} \\|x_{\\ell} - z^{k} + u_{\\ell}^{k}\\|_{2}^{2} \\right) $$\n这是对一个关于 $x_{\\ell}$ 的无约束严格凸二次函数的最小化。通过将关于 $x_{\\ell}$ 的梯度置为零来找到最小值。设目标函数为 $J(x_{\\ell})$。\n$$ \\nabla_{x_{\\ell}} J(x_{\\ell}) = A_{\\ell}^{\\top}(A_{\\ell} x_{\\ell} - y_{\\ell}) + \\rho(x_{\\ell} - z^{k} + u_{\\ell}^{k}) = 0 $$\n重新整理各项以求解 $x_{\\ell}$：\n$$ A_{\\ell}^{\\top}A_{\\ell} x_{\\ell} + \\rho I x_{\\ell} = A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) $$\n$$ (A_{\\ell}^{\\top}A_{\\ell} + \\rho I) x_{\\ell} = A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) $$\n根据问题陈述，矩阵 $(A_{\\ell}^{\\top}A_{\\ell} + \\rho I)$ 是可逆的。因此，我们得到 $x_{\\ell}^{k+1}$ 的闭式更新式：\n$$ x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1} \\left( A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}) \\right) $$\n对每个代理 $\\ell = 1, \\dots, L$ 并行执行此更新。\n\n**2. $z$ 更新（全局一致性更新）**\n接下来，我们使用新计算出的值 $\\{x_{\\ell}^{k+1}\\}$，通过关于 $z$ 最小化 $\\mathcal{L}_{\\rho}$ 来更新一致性变量 $z$：\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\lambda \\|z\\|_{1} + \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell}^{k+1} - z + u_{\\ell}^{k}\\|_{2}^{2} \\right) $$\n我们可以将二次项重写为：\n$$ \\sum_{\\ell=1}^{L} \\frac{\\rho}{2} \\|x_{\\ell}^{k+1} - z + u_{\\ell}^{k}\\|_{2}^{2} = \\frac{\\rho}{2} \\sum_{\\ell=1}^{L} \\|z - (x_{\\ell}^{k+1} + u_{\\ell}^{k})\\|_{2}^{2} $$\n通过对 $z$ 进行配方，这个和可以用平均值 $\\bar{v}^{k+1} = \\frac{1}{L}\\sum_{\\ell=1}^{L}(x_{\\ell}^{k+1} + u_{\\ell}^{k})$ 来表示：\n$$ \\frac{\\rho}{2} \\sum_{\\ell=1}^{L} \\|z - (x_{\\ell}^{k+1} + u_{\\ell}^{k})\\|_{2}^{2} = \\frac{L\\rho}{2} \\|z - \\bar{v}^{k+1}\\|_{2}^{2} + \\text{与 } z \\text{ 无关的常数项} $$\n因此，$z$ 的最小化问题变为：\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\lambda \\|z\\|_{1} + \\frac{L\\rho}{2} \\|z - \\bar{v}^{k+1}\\|_{2}^{2} \\right) $$\n这是 $\\ell_1$ 范数近端算子的标准形式。具体来说，它等价于：\n$$ z^{k+1} = \\arg\\min_{z} \\left( \\frac{\\lambda}{L\\rho} \\|z\\|_{1} + \\frac{1}{2} \\left\\|z - \\frac{1}{L}\\sum_{\\ell=1}^{L}(x_{\\ell}^{k+1} + u_{\\ell}^{k})\\right\\|_{2}^{2} \\right) $$\n其解由软阈值算子给出，记为 $S_{\\gamma}(\\cdot)$。其中对于向量 $v$ 和阈值 $\\gamma > 0$，有 $[S_{\\gamma}(v)]_i = \\text{sign}(v_i) \\max(|v_i| - \\gamma, 0)$。\n$z^{k+1}$ 的更新式为：\n$$ z^{k+1} = S_{\\frac{\\lambda}{L\\rho}} \\left( \\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k}) \\right) $$\n其中求和使用索引 $j$ 以避免歧义。\n\n**3. $u_{\\ell}$ 更新（对偶变量更新）**\n最后，使用原始残差 $r_{\\ell}^{k+1} = x_{\\ell}^{k+1} - z^{k+1}$ 更新缩放对偶变量：\n$$ u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1} $$\n对每个代理 $\\ell = 1, \\dots, L$ 并行执行此更新。\n\n总之，第 $k+1$ 次迭代的三个序贯更新映射为：\n1.  $x_{\\ell}^{k+1} = (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1}(A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k}))$\n2.  $z^{k+1} = S_{\\frac{\\lambda}{L\\rho}}\\left(\\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k})\\right)$\n3.  $u_{\\ell}^{k+1} = u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1}$\n\n问题要求将这三个更新映射表示为单个表达式。", "answer": "$$ \\boxed{ \\begin{pmatrix} (A_{\\ell}^{\\top}A_{\\ell} + \\rho I)^{-1}(A_{\\ell}^{\\top}y_{\\ell} + \\rho(z^{k} - u_{\\ell}^{k})) \\\\ S_{\\frac{\\lambda}{L\\rho}}\\left(\\frac{1}{L}\\sum_{j=1}^{L}(x_{j}^{k+1} + u_{j}^{k})\\right) \\\\ u_{\\ell}^{k} + x_{\\ell}^{k+1} - z^{k+1} \\end{pmatrix} } $$", "id": "3444486"}, {"introduction": "在理解了分布式算法的内部机制后，一个关键的实践步骤是选择其超参数，例如正则化参数 $\\lambda$。本练习将理论付诸实践，要求你实现一种复杂的、完全分布式的参数自适应选择方法 [@problem_id:3444471]。你将利用斯坦无偏风险估计 (SURE) 进行局部风险评估，并通过平均共识算法来寻找全网最优的 $\\lambda$，从而展示如何将统计学原理无缝集成到分布式优化工作流中。", "problem": "您的任务是设计并实现一个自适应的分布式程序，使用斯坦因无偏风险估计（SURE）和平均一致性算法，从带噪声的分布式观测中选择软阈值参数并估计未知信号的稀疏度。\n\n给定一个由代理 $i \\in \\{1,\\dots,N\\}$ 索引的网络，每个代理通过高斯去噪模型观测到同一个未知向量 $x_{0} \\in \\mathbb{R}^{n}$\n$$\nz_{i} = x_{0} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2} I_{n}),\n$$\n其中 $\\sigma_{i} > 0$ 对代理 $i$ 是已知的。每个代理使用逐元素的软阈值算子 $\\eta(\\cdot; \\lambda)$ 和一个公共的标量阈值 $\\lambda \\ge 0$ 来形成一个局部估计器：\n$$\n\\hat{x}_{i}(\\lambda) = \\eta(z_{i}; \\lambda), \\quad \\text{with} \\quad \\eta(t; \\lambda) = \\mathrm{sign}(t)\\,\\max(|t| - \\lambda, 0).\n$$\n目标是选择能最小化网络平均均方误差的 $\\lambda$：\n$$\n\\min_{\\lambda \\ge 0} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[ \\| \\hat{x}_{i}(\\lambda) - x_{0} \\|_{2}^{2} \\right],\n$$\n仅使用本地计算和邻居交换，然后通过对局部支撑集大小估计值进行分布式一致性计算，来估计稀疏度 $s = \\|x_{0}\\|_{0}$。\n\n您必须使用的基础理论：\n- 高斯去噪模型和正态分布的性质。\n- 针对独立同分布高斯噪声的斯坦因无偏风险估计（SURE）：对于一个基于 $Z \\sim x_{0} + \\mathcal{N}(0,\\sigma^{2} I_{n})$ 的 $x_{0}$ 的估计器 $\\delta(Z)$，通过应用 Stein 引理和 $\\delta(\\cdot)$ 的散度（在其存在处），可以从可观测的量构造出 $\\mathbb{E}\\left[\\|\\delta(Z) - x_{0}\\|_{2}^{2}\\right]$ 的一个无偏估计。\n- 在连通无向图上使用双随机权重矩阵的平均一致性算法：局部状态向量的重复线性迭代会收敛到全网络的算术平均值。\n\n您的实现必须：\n1. 对于每个代理 $i$，仅使用本地数据 $(z_{i}, \\sigma_{i})$ 以及通过 Stein 引理和软阈值估计器散度得到的 SURE 定义，在一个固定的网格 $\\Lambda = \\{\\lambda_{0}, \\lambda_{1}, \\dots, \\lambda_{G-1}\\}$ 上计算本地 SURE 曲线。不要使用任何未从上述基础理论推导出的公式。\n2. 使用从给定图构建的对称、双随机 Metropolis–Hastings 权重矩阵，在网络上对 SURE 曲线（作为长度为 $G$ 的向量）执行平均一致性算法。对线性一致性更新进行指定步数的迭代，并使用得到的近似网络平均 SURE 曲线来选择\n$$\n\\lambda^{\\star} \\in \\arg\\min_{\\lambda \\in \\Lambda} \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SURE}_{i}(\\lambda).\n$$\n3. 在选定的 $\\lambda^{\\star}$ 处，每个代理计算其本地软阈值估计 $\\hat{x}_{i}(\\lambda^{\\star})$ 和本地支撑集大小估计 $s_{i}(\\lambda^{\\star}) = \\|\\hat{x}_{i}(\\lambda^{\\star})\\|_{0}$，其中零点测试的数值阈值由本问题指定。\n4. 对标量 $\\{s_{i}(\\lambda^{\\star})\\}_{i=1}^{N}$ 执行平均一致性算法，以获得稀疏度的网络一致性估计值 $\\hat{s}$，该值通过将一致性平均值四舍五入到最近的整数得到。\n5. 对于每个测试用例，返回一个三元组，包含所选阈值 $\\lambda^{\\star}$（作为浮点数）、一致性稀疏度估计 $\\hat{s}$（作为整数），以及一个布尔值，该值指示根据下面为每个测试用例定义的接受准则，该估计是否可接受。\n\n数值和算法规范：\n- 使用一个公共网格 $\\Lambda$，包含从 0 到 4（含）的 $G = 401$ 个均匀间隔点，即对于 $g \\in \\{0,\\dots,400\\}$，$\\lambda_{g} = \\frac{4g}{400}$。\n- 使用 $\\tau = 10^{-6}$ 的零点测试容差来数值定义 $\\|\\cdot\\|_{0}$，即如果一个条目的绝对值超过 $\\tau$，则计为非零。\n- 为无向图构建 Metropolis–Hastings 权重，其邻接矩阵为 $A$：对于 $i \\neq j$，如果 $(i,j)$ 是一条边，则设置 $W_{ij} = \\frac{1}{1 + \\max\\{d_{i}, d_{j}\\}}$，否则为 0，其中 $d_{i}$ 是节点 $i$ 的度；对于对角线元素，设置 $W_{ii} = 1 - \\sum_{j \\neq i} W_{ij}$。对 SURE 曲线和稀疏度计数均使用 $T = 50$ 次一致性迭代。\n- 所有计算都是无量纲的；不涉及物理单位。\n\n测试套件：\n实现下面指定的三个测试用例。在所有情况下，生成随机支撑集和噪声时，使用指定的随机种子以保证可复现性。\n\n- 测试用例 1（理想情况）：\n  - $n = 128$，$N = 6$，真实稀疏度 $s = 10$，非零幅值等于常数 $a = 3$，符号随机。\n  - 所有 $i$ 的噪声标准差 $\\sigma_{i} = 0.25$。\n  - 图：$N=6$ 个节点的环形拓扑，即节点 $i$ 和 $i \\pm 1$（模 $N$）之间有边。\n  - 用于支撑集选择、符号选择和噪声的随机种子为 $12345$。\n  - 接受准则：如果 $|\\hat{s} - s| \\le 2$，则报告成功。\n\n- 测试用例 2（边界情况：零信号）：\n  - $n = 128$，$N = 4$，真实稀疏度 $s = 0$，$x_{0} = 0$。\n  - 所有 $i$ 的噪声标准差 $\\sigma_{i} = 0.4$。\n  - 图：$N=4$ 个节点的全连接图。\n  - 用于噪声的随机种子为 $54321$。\n  - 接受准则：如果 $\\hat{s} = s$，则报告成功。\n\n- 测试用例 3（异构噪声和拓扑）：\n  - $n = 64$，$N = 5$，真实稀疏度 $s = 5$，非零幅值等于常数 $a = 2.5$，符号随机。\n  - 噪声标准差 $\\sigma = [0.3, 0.3, 0.8, 0.3, 0.3]$。\n  - 图：$N=5$ 个节点的线形拓扑，即节点 $i$ 和 $i+1$ 之间有边。\n  - 用于支撑集选择、符号选择和噪声的随机种子为 $24680$。\n  - 接受准则：如果 $|\\hat{s} - s| \\le 2$，则报告成功。\n\n输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个三元组列表，每个测试用例对应一个三元组，格式为 $[\\lambda^{\\star}, \\hat{s}, \\mathrm{success}]$。总输出必须是 JSON 格式的单行字符串，形式如下：\n$$\n\\big[ [\\lambda^{\\star}_{1}, \\hat{s}_{1}, \\mathrm{success}_{1}], [\\lambda^{\\star}_{2}, \\hat{s}_{2}, \\mathrm{success}_{2}], [\\lambda^{\\star}_{3}, \\hat{s}_{3}, \\mathrm{success}_{3}] \\big],\n$$\n不含任何额外文本。\n\n您的实现必须是完全自包含的，并且不能要求任何用户输入。所有数值必须由您的代码根据上述规定使用给定的种子计算得出。最终打印的行必须严格遵守指定的格式。", "solution": "该问题要求设计并实现一种分布式算法，用于选择软阈值参数 $\\lambda$ 并估计未知信号 $x_0 \\in \\mathbb{R}^n$ 的稀疏度。该系统由一个包含 $N$ 个代理的网络组成，其中每个代理 $i$ 拥有一个带噪声的观测值 $z_i = x_0 + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2 I_n)$。\n\n解决方案的步骤是首先验证问题陈述，然后在确认其有效性后，提供算法的详细推导及其实现。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- **信号模型**: $z_{i} = x_{0} + \\varepsilon_{i}$，对于 $i \\in \\{1, \\dots, N\\}$，其中 $x_{0} \\in \\mathbb{R}^{n}$ 是一个未知信号，$\\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2} I_{n})$ 是高斯噪声，且 $\\sigma_{i} > 0$ 是一个已知的标准差。\n- **估计器**: 逐元素软阈值 $\\hat{x}_{i}(\\lambda) = \\eta(z_{i}; \\lambda)$，其中 $\\eta(t; \\lambda) = \\mathrm{sign}(t)\\,\\max(|t| - \\lambda, 0)$。\n- **目标**: 最小化网络平均均方误差 (MSE)：$\\min_{\\lambda \\ge 0} \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[ \\| \\hat{x}_{i}(\\lambda) - x_{0} \\|_{2}^{2} \\right]$。\n- **稀疏度**: $s = \\|x_{0}\\|_{0}$。\n- **方法论**: 使用斯坦因无偏风险估计（SURE）和平均一致性算法。\n- **数值规范**:\n    - Lambda 网格 $\\Lambda$：$[0, 4]$ 区间内的 $G=401$ 个点，$\\lambda_g = 4g/400$。\n    - 零范数容差：$\\tau = 10^{-6}$。\n    - 一致性迭代次数：$T=50$。\n    - 权重矩阵：对于邻接矩阵为 $A$、度为 $d_i$ 的无向图，采用 Metropolis-Hastings 方法：如果 $(i,j)$ 是一条边，则 $W_{ij} = (1 + \\max\\{d_i, d_j\\})^{-1}$，并且 $W_{ii} = 1 - \\sum_{j \\neq i} W_{ij}$。\n- **测试用例**: 提供了三个具体的测试用例，包含所有参数（$n, N, s, a, \\sigma_i$）、图拓扑（环形、全连接、线形）、随机种子和成功准则。\n\n**步骤 2：有效性检查**\n该问题在科学和数学上是完善的，依赖于统计信号处理（高斯去噪、SURE）和分布式优化（平均一致性）的标准原理。其表述是自包含的，所有必要的参数、模型和数值规范都已明确定义。测试用例具体且可复现。该问题是形式化、客观的，不违反任何科学原理，也不包含逻辑矛盾。它代表了分布式学习和优化领域一个标准的、非平凡的问题。\n\n**步骤 3：结论**\n该问题是**有效的**。将开发完整的解决方案。\n\n### 算法解决方案\n\n问题的核心是最小化网络平均风险，它是阈值参数 $\\lambda$ 的函数：\n$$\nJ(\\lambda) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}\\left[ \\| \\hat{x}_{i}(\\lambda) - x_{0} \\|_{2}^{2} \\right]\n$$\n这个目标无法直接计算，因为它涉及期望和未知信号 $x_0$。我们通过使用斯坦因无偏风险估计（SURE）来解决这个问题，SURE 为每个代理提供了一个数据驱动的、对均方误差（MSE）的无偏估计。\n\n**1. 斯坦因无偏风险估计 (SURE)**\n对于观测值为 $z_i \\sim \\mathcal{N}(x_0, \\sigma_i^2 I_n)$ 的代理 $i$，其风险 $R_i(\\lambda) = \\mathbb{E}\\left[ \\| \\eta(z_i; \\lambda) - x_{0} \\|_{2}^{2} \\right]$ 可以在不知道 $x_0$ 的情况下进行估计。根据 Stein 引理，对于一个弱可微函数 $\\delta:\\mathbb{R}^n \\to \\mathbb{R}^n$，风险 $\\mathbb{E}[\\|\\delta(z_i) - x_0\\|_2^2]$ 的一个无偏估计由下式给出：\n$$\n\\mathrm{SURE}_i(\\lambda) = \\| \\delta(z_i) - z_i \\|_{2}^{2} - n\\sigma_{i}^{2} + 2\\sigma_{i}^{2} \\nabla_{z_i} \\cdot \\delta(z_i)\n$$\n在我们的案例中，估计器是 $\\delta(z_i) = \\hat{x}_i(\\lambda) = \\eta(z_i; \\lambda)$，它是逐元素应用的。散度项 $\\nabla_{z_i} \\cdot \\eta(z_i; \\lambda)$ 是其各分量偏导数的和：\n$$\n\\nabla_{z_i} \\cdot \\eta(z_i; \\lambda) = \\sum_{j=1}^{n} \\frac{\\partial}{\\partial z_{ij}} \\eta(z_{ij}; \\lambda)\n$$\n标量软阈值函数 $\\eta(t; \\lambda)$ 的导数是 $\\frac{d}{dt}\\eta(t; \\lambda) = 1$（对于 $|t| > \\lambda$）和 $0$（对于 $|t|  \\lambda$）。在导数未定义的点 $|t| = \\lambda$，对于连续随机变量 $t$，该集合的勒贝格测度为零，因此出于积分目的我们可以忽略它。因此，导数是指示函数 $\\mathbb{I}(|t| > \\lambda)$。\n因此，散度是 $z_i$ 中幅值超过 $\\lambda$ 的分量数量：\n$$\n\\nabla_{z_i} \\cdot \\eta(z_i; \\lambda) = \\sum_{j=1}^{n} \\mathbb{I}(|z_{ij}| > \\lambda) = \\| \\eta(z_i; \\lambda) \\|_{0}\n$$\n其中 $\\|\\cdot\\|_0$ 计算非零元素的数量。将此代入 SURE 公式，得到代理 $i$ 的局部风险估计：\n$$\n\\mathrm{SURE}_i(\\lambda) = \\| \\eta(z_i; \\lambda) - z_i \\|_{2}^{2} - n\\sigma_i^2 + 2\\sigma_i^2 \\| \\eta(z_i; \\lambda) \\|_{0}\n$$\n第一项，即估计值与观测值之间的平方误差，可以更简单地表示。对于每个分量 $j$，如果 $|z_{ij}| \\le \\lambda$，则 $\\eta(z_{ij};\\lambda) = 0$，平方差为 $z_{ij}^2$。如果 $|z_{ij}| > \\lambda$，则 $\\eta(z_{ij};\\lambda) = z_{ij} - \\lambda\\,\\mathrm{sign}(z_{ij})$，平方差为 $\\lambda^2$。这可以紧凑地写为 $\\min(|z_{ij}|, \\lambda)^2$。因此，整个项是 $\\sum_{j=1}^n \\min(|z_{ij}|, \\lambda)^2$。\n\n**2. 通过平均一致性算法进行分布式优化**\n现在，全局目标可以通过最小化局部 SURE 函数的平均值来近似：\n$$\n\\min_{\\lambda \\ge 0} \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SURE}_{i}(\\lambda)\n$$\n由于每个代理 $i$ 只能计算自己的 $\\mathrm{SURE}_i(\\lambda)$，它们必须协作以找到平均值的最小值。这通过使用平均一致性算法来实现。代理在一个连通的无向图上操作。它们通过取自身状态及其邻居状态的加权平均来迭代更新其局部状态。该过程由一个双随机权重矩阵 $W$ 控制：\n$$\nv_i^{(k+1)} = \\sum_{j=1}^N W_{ij} v_j^{(k)}\n$$\n其中 $v_i^{(k)}$ 是代理 $i$ 在第 $k$ 次迭代时的状态。对于连通的非二分图，当 $k \\to \\infty$ 时，对所有 $i$ 都有 $v_i^{(k)} \\to \\frac{1}{N} \\sum_{j=1}^N v_j^{(0)}$。问题指定了用于构造 $W$ 的 Metropolis-Hastings 规则，这保证了它是对称和双随机的，从而确保收敛到算术平均值。\n\n**3. 完整的分布式程序**\n该算法分为两个主要阶段，都涉及一致性算法。\n\n**阶段 1：最优阈值选择**\n1.  **局部 SURE 曲线计算**：每个代理 $i$ 为预定义网格 $\\Lambda = \\{\\lambda_0, \\dots, \\lambda_{G-1}\\}$ 中的每个 $\\lambda$ 计算其 SURE 值。这会产生一个局部 SURE 向量 $S_i \\in \\mathbb{R}^G$，其中 $(S_i)_g = \\mathrm{SURE}_i(\\lambda_g)$。\n2.  **SURE 曲线的一致性计算**：代理运行 $T$ 次平均一致性迭代来计算平均 SURE 曲线。设 $S^{(k)}$ 为一个 $N \\times G$ 矩阵，其中第 $i$ 行是代理 $i$ 在第 $k$ 次迭代时的 SURE 向量。更新规则是 $S^{(k+1)} = W S^{(k)}$，其中 $S^{(0)}$ 是初始计算的局部 SURE 曲线矩阵。\n3.  **最优阈值($\\lambda^\\star$)识别**：经过 $T$ 次迭代后，$S^{(T)}$ 的每一行都是平均 SURE 曲线 $\\bar{S} = \\frac{1}{N}\\sum_i S_i$ 的一个近似。然后，每个代理可以通过找到其局部结果的最小值来独立地找到最优阈值：$\\lambda^\\star = \\Lambda[\\arg\\min_{g} (S_i^{(T)})_g]$。\n\n**阶段 2：稀疏度估计**\n1.  **局部稀疏度计算**：使用商定的 $\\lambda^\\star$，每个代理 $i$ 计算其信号支撑集大小的局部估计：$s_i(\\lambda^\\star) = \\| \\eta(z_i; \\lambda^\\star) \\|_{0}$，其中零范数使用容差 $\\tau$ 进行数值评估。\n2.  **稀疏度估计的一致性计算**：代理们进行第二轮一致性计算，这次是针对标量值 $\\{s_i(\\lambda^\\star)\\}_{i=1}^N$。设代理 $i$ 的初始状态为 $c_i^{(0)} = s_i(\\lambda^\\star)$。经过 $T$ 次 $c_i^{(k+1)} = \\sum_{j} W_{ij} c_j^{(k)}$ 的迭代后，每个代理获得一个估计值 $c_i^{(T)}$，该值近似于网络平均值 $\\bar{s} = \\frac{1}{N}\\sum_j s_j(\\lambda^\\star)$。\n3.  **最终稀疏度估计 ($\\hat{s}$)**：稀疏度的最终一致性估计值通过将平均值四舍五入到最近的整数得到：$\\hat{s} = \\text{round}(c_i^{(T)})$。\n\n这个完整的程序使得代理网络能够以完全分散的方式，利用局部 SURE 计算和全网一致性算法，共同确定最优正则化参数并估计信号稀疏度。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the distributed SURE-based\n    parameter selection and sparsity estimation problem.\n    \"\"\"\n\n    def create_graph(N, topology):\n        \"\"\"Creates an adjacency matrix for a given topology.\"\"\"\n        adj = np.zeros((N, N), dtype=int)\n        if topology == 'ring':\n            for i in range(N):\n                adj[i, (i + 1) % N] = 1\n                adj[i, (i - 1 + N) % N] = 1\n        elif topology == 'line':\n            if N > 1:\n                for i in range(N - 1):\n                    adj[i, i + 1] = 1\n                    adj[i + 1, i] = 1\n        elif topology == 'fully_connected':\n            adj = np.ones((N, N), dtype=int) - np.eye(N, dtype=int)\n        return adj\n\n    def metropolis_hastings_weights(adj):\n        \"\"\"Constructs a Metropolis-Hastings doubly-stochastic weight matrix.\"\"\"\n        N = adj.shape[0]\n        W = np.zeros((N, N))\n        degrees = adj.sum(axis=1)\n        for i in range(N):\n            for j in range(i + 1, N):\n                if adj[i, j] > 0:\n                    val = 1.0 / (1.0 + max(degrees[i], degrees[j]))\n                    W[i, j] = val\n                    W[j, i] = val\n        \n        row_sums = W.sum(axis=1)\n        for i in range(N):\n            W[i, i] = 1.0 - row_sums[i]\n            \n        return W\n\n    def soft_threshold(z, lam):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - lam, 0)\n\n    def l0_norm(x, tau):\n        \"\"\"Numerical L0 norm with a tolerance.\"\"\"\n        return np.sum(np.abs(x) > tau)\n\n    def run_test_case(n, N, s, a, sigmas, topology, seed, acceptance_criterion):\n        \"\"\"Executes a single test case.\"\"\"\n        # --- 1. Setup ---\n        rng = np.random.default_rng(seed)\n        G = 401\n        T = 50\n        tau = 1e-6\n        lambda_grid = np.linspace(0, 4, G)\n\n        # --- 2. Generate Graph  Weights ---\n        adj_matrix = create_graph(N, topology)\n        W = metropolis_hastings_weights(adj_matrix)\n\n        # --- 3. Generate Signal  Data ---\n        x0 = np.zeros(n)\n        if s > 0:\n            support = rng.choice(n, s, replace=False)\n            signs = rng.choice([-1, 1], s)\n            x0[support] = a * signs\n        \n        observations = np.zeros((N, n))\n        for i in range(N):\n            noise = rng.normal(0, sigmas[i], n)\n            observations[i, :] = x0 + noise\n\n        # --- 4. Local SURE Computations ---\n        local_sure_curves = np.zeros((N, G))\n        for i in range(N):\n            z_i = observations[i, :]\n            sigma_i = sigmas[i]\n            for g, lam in enumerate(lambda_grid):\n                x_hat_i = soft_threshold(z_i, lam)\n                \n                # Using the stable form for the first term: sum(min(|z_j|, lam)^2)\n                term1 = np.sum(np.minimum(np.abs(z_i), lam)**2)\n                \n                # Divergence term: 2 * sigma^2 * ||x_hat||_0\n                l0_x_hat_i = l0_norm(x_hat_i, tau)\n                term2 = -n * sigma_i**2\n                term3 = 2 * sigma_i**2 * l0_x_hat_i\n                \n                local_sure_curves[i, g] = term1 + term2 + term3\n\n        # --- 5. Consensus on SURE curves ---\n        consensus_sures = local_sure_curves.copy()\n        for _ in range(T):\n            consensus_sures = W @ consensus_sures\n        \n        # Any agent's curve is now the approximation of the average\n        avg_sure_curve = consensus_sures[0, :]\n\n        # --- 6. Select lambda_star ---\n        best_lambda_idx = np.argmin(avg_sure_curve)\n        lambda_star = lambda_grid[best_lambda_idx]\n\n        # --- 7. Local Sparsity Estimation ---\n        local_sparsities = np.zeros(N)\n        for i in range(N):\n            z_i = observations[i, :]\n            x_hat_i_star = soft_threshold(z_i, lambda_star)\n            local_sparsities[i] = l0_norm(x_hat_i_star, tau)\n            \n        # --- 8. Consensus on Sparsity ---\n        consensus_sparsities = local_sparsities.reshape(-1, 1)\n        for _ in range(T):\n            consensus_sparsities = W @ consensus_sparsities\n            \n        # Any agent's value is now the approximation of the average\n        avg_s = consensus_sparsities[0, 0]\n        s_hat = int(np.round(avg_s))\n\n        # --- 9. Check Acceptance Criterion ---\n        success = acceptance_criterion(s_hat, s)\n\n        # --- 10. Return Result ---\n        return [lambda_star, s_hat, success]\n\n    # Define test cases\n    test_cases = [\n        # Test Case 1: Happy Path\n        {\n            \"n\": 128, \"N\": 6, \"s\": 10, \"a\": 3.0,\n            \"sigmas\": np.full(6, 0.25),\n            \"topology\": \"ring\",\n            \"seed\": 12345,\n            \"acceptance_criterion\": lambda s_hat, s: abs(s_hat - s) = 2\n        },\n        # Test Case 2: Zero Signal\n        {\n            \"n\": 128, \"N\": 4, \"s\": 0, \"a\": 0.0,\n            \"sigmas\": np.full(4, 0.4),\n            \"topology\": \"fully_connected\",\n            \"seed\": 54321,\n            \"acceptance_criterion\": lambda s_hat, s: s_hat == s\n        },\n        # Test Case 3: Heterogeneous\n        {\n            \"n\": 64, \"N\": 5, \"s\": 5, \"a\": 2.5,\n            \"sigmas\": np.array([0.3, 0.3, 0.8, 0.3, 0.3]),\n            \"topology\": \"line\",\n            \"seed\": 24680,\n            \"acceptance_criterion\": lambda s_hat, s: abs(s_hat - s) = 2\n        }\n    ]\n    \n    results = []\n    for params in test_cases:\n        result = run_test_case(**params)\n        # Format lambda to a reasonable precision for output consistency\n        result[0] = round(result[0], 5)\n        results.append(result)\n\n    # Format the final output string exactly as requested\n    print(f\"[{','.join(map(str, results))}]\".replace(\"'\", \"\").replace(\" \", \"\"))\n\nsolve()\n```", "id": "3444471"}, {"introduction": "分布式系统可能易受故障或恶意节点（即“拜占庭”对手）的攻击。这个高级实践旨在解决鲁棒性这一关键问题 [@problem_id:3444450]。你将实现并分析一种基于坐标截尾均值的鲁棒聚合策略，以保护共识过程免受损坏数据的干扰。此外，你还将推导性能下降的上界，从而深入了解如何设计和分析具有韧性的分布式算法。", "problem": "给定一个包含 $N$ 个工作节点的分布式稀疏恢复设定。一个全局未知的 $s$-稀疏向量 $x^{\\star} \\in \\mathbb{R}^{p}$ 在节点 $i \\in \\{1,\\dots,N\\}$ 上被局部测量，其形式为 $y_i = A_i x^{\\star} + w_i$，其中 $A_i \\in \\mathbb{R}^{m \\times p}$ 的条目是独立同分布的高斯分布，方差为 $1/m$，而 $w_i \\in \\mathbb{R}^{m}$ 是零均值高斯噪声，协方差为 $\\sigma^2 I_m$。其中 $q$ 个节点是具备对抗性（拜占庭）的，它们可能发送任意损坏的更新。\n\n目标是设计一种基于坐标级裁剪均值聚合的鲁棒、单次分布式稀疏支撑集恢复方法，并为在存在 $q$ 个对抗者的情况下，恢复出的支撑集的错误发现率（FDR）增量设定一个上界。\n\n定义与设定：\n\n- 令每个节点的代理为 $z_i = A_i^{\\top} y_i \\in \\mathbb{R}^{p}$。一个鲁棒的聚合器使用裁剪参数 $b=q$ 计算坐标级裁剪均值：对每个坐标 $j \\in \\{1,\\dots,p\\}$，收集 $\\{z_{i,j}\\}_{i=1}^N$，排序，丢弃 $q$ 个最小值和 $q$ 个最大值，然后对剩余的 $N - 2q$ 个值求平均，得到聚合后的坐标 $\\widehat{z}_j$。鲁棒的支撑集估计为 $\\widehat{S}_{\\mathrm{rob}} = \\mathrm{Top}_s\\left(|\\widehat{z}|\\right)$，即 $\\widehat{z}$ 的 $s$ 个最大幅值的索引。\n\n- 作为一个无攻击者的基线，令 $z^{\\mathrm{ben}} = \\frac{1}{N-q}\\sum_{i \\in \\mathcal{B}} z_i$，其中 $\\mathcal{B}$ 是基数为 $N-q$ 的良性节点集合。基线支撑集估计为 $\\widehat{S}_{\\mathrm{base}} = \\mathrm{Top}_s\\left(|z^{\\mathrm{ben}}|\\right)$。\n\n- 对于一个支撑集估计 $\\widehat{S}$，定义错误发现率为 $\\mathrm{FDR}(\\widehat{S}) = \\frac{|\\widehat{S} \\setminus S^{\\star}|}{|\\widehat{S}|}$，其中 $S^{\\star} = \\mathrm{supp}(x^{\\star})$。由对抗者引起的错误发现率增量为 $\\Delta_{\\mathrm{FDR}} = \\mathrm{FDR}(\\widehat{S}_{\\mathrm{rob}}) - \\mathrm{FDR}(\\widehat{S}_{\\mathrm{base}})$。\n\n任务：\n\n- 提出一种使用上述坐标级裁剪均值聚合（裁剪参数为 $b=q$）的鲁棒分布式稀疏支撑集恢复方法。\n\n- 从第一性原理以及鲁棒统计和稀疏恢复中广为接受的事实出发，推导出一个关于 $\\Delta_{\\mathrm{FDR}}$ 的可计算上界，该上界是 $N$、$q$ 和良性局部代理 $\\{z_i\\}_{i \\in \\mathcal{B}}$ 的经验分布的函数。您的推导必须从基本定义（例如，顺序统计量、集合基数和裁剪均值的性质）开始，并产生一个仅能通过可观测的良性代理统计量和 $q$ 来评估的界。\n\n- 实现所提出的鲁棒方法和推导出的上界。然后，使用下面的特定测试套件，为每种情况计算经验错误发现率增量 $\\Delta_{\\mathrm{FDR}}$ 是否小于或等于您推导出的上界。最终输出必须是一个布尔值列表，每个测试用例对应一个值，指示该界是否成立。\n\n用于仿真的对抗者模型：\n\n- 最多有 $q$ 个节点是对抗性的。在仿真中，假定对抗者发送的损坏代理 $\\widetilde{z}_i$ 是独立的，并且可以具有很大的幅值。您必须选择一种与以下参数一致的具体损坏方式，以确保进行科学上现实的最坏情况压力测试，但您的上界除了 $q$ 之外，不得依赖于未知的攻击者值。\n\n测试套件（每个测试用例固定所有参数）：\n\n- 情况1：$N = 9$, $q = 0$, $p = 120$, $s = 5$, $m = 50$, $\\sigma = 0.05$, 破坏幅度参数 $B_{\\mathrm{adv}} = 0$。\n\n- 情况2：$N = 9$, $q = 2$, $p = 120$, $s = 5$, $m = 50$, $\\sigma = 0.05$, $B_{\\mathrm{adv}} = 20.0$。\n\n- 情况3：$N = 7$, $q = 3$, $p = 120$, $s = 5$, $m = 50$, $\\sigma = 0.05$, $B_{\\mathrm{adv}} = 100.0$。\n\n实现细节：\n\n- 构造 $x^{\\star}$，使其在均匀随机的支撑集位置上具有恰好 $s$ 个等幅值、随机符号的非零元。\n\n- 生成每个 $A_i$，其条目服从独立同分布的 $\\mathcal{N}(0, 1/m)$，以及 $w_i$ 服从 $\\mathcal{N}(0, \\sigma^2 I_m)$。\n\n- 对于对抗性节点，生成 $\\widetilde{z}_i$，其独立条目的幅值在 $B_{\\mathrm{adv}}$ 的数量级，以便对聚合器产生有意义的压力。\n\n- 对鲁棒聚合使用裁剪参数为 $b=q$ 的坐标级裁剪均值，对基线使用良性节点的简单均值。\n\n- 如上定义错误发现率增量 $\\Delta_{\\mathrm{FDR}}$。仅使用 $N$、$q$ 和良性代理的坐标级范围，从您的推导中计算一个数据驱动的、有理论依据的上界。\n\n要求的最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[True,False,True]”），其中每个布尔值指示计算出的经验 $\\Delta_{\\mathrm{FDR}}$ 是否小于或等于您为相应测试用例推导出的上界。\n\n角度单位不适用。不涉及物理单位。所有数值结果必须为标准十进制形式。程序必须是自包含的，不需要用户输入，并且必须在内部使用固定的随机种子以确保可复现性。", "solution": "用户提供的问题是分布式稀疏信号处理和鲁棒统计领域中一个有效且适定的问题。它要求在存在拜占庭对抗者的情况下，推导使用坐标级裁剪均值聚合器时错误发现率增量的上界，并对此上界进行数值验证。\n\n问题陈述已经过验证，并被认定为：\n- **科学上合理**：该设定使用了压缩感知（$y_i = A_i x^{\\star} + w_i$）和鲁棒统计（裁剪均值估计器）中的标准模型。所有定义都与既有文献一致。\n- **适定**：任务是推导并验证一个上界，这是一个数学上精确的目标。测试用例的参数已指定，并确保了裁剪均值的定义是有效的（即，$N-2q  0$）。\n- **客观**：问题以精确的数学语言陈述，没有主观性或歧义。\n\n因此，我们可以着手提供完整的解决方案。\n\n### 基于第一性原理的错误发现率上界推导\n\n我们的目标是推导 $\\Delta_{\\mathrm{FDR}} = \\mathrm{FDR}(\\widehat{S}_{\\mathrm{rob}}) - \\mathrm{FDR}(\\widehat{S}_{\\mathrm{base}})$ 的一个可计算上界。\n\n**1. 预备知识与定义**\n令 $S^{\\star}$ 为 $s$-稀疏向量 $x^{\\star}$ 的真实支撑集，且 $|S^{\\star}|=s$。支撑集估计为 $\\widehat{S}_{\\mathrm{rob}}$ 和 $\\widehat{S}_{\\mathrm{base}}$，大小均为 $s$。\n错误发现率（FDR）为 $\\mathrm{FDR}(\\widehat{S}) = \\frac{|\\widehat{S} \\setminus S^{\\star}|}{s}$。FDR的增量为 $\\Delta_{\\mathrm{FDR}} = \\frac{1}{s} (|\\widehat{S}_{\\mathrm{rob}} \\setminus S^{\\star}| - |\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star}|)$。\n\n基线聚合为 $z^{\\mathrm{ben}} = \\frac{1}{N-q}\\sum_{i \\in \\mathcal{B}} z_i$，其中 $\\mathcal{B}$ 是 $N-q$ 个良性节点的集合。基线支撑集为 $\\widehat{S}_{\\mathrm{base}} = \\mathrm{Top}_s(|z^{\\mathrm{ben}}|)$。\n鲁棒聚合 $\\widehat{z}$ 是所有 $N$ 个代理（来自 $N-q$ 个良性节点和 $q$ 个对抗性节点）的坐标级裁剪均值，裁剪参数为 $b=q$。对于每个坐标 $j \\in \\{1, \\dots, p\\}$，令排序后的代理值为 $z_{(1),j} \\le z_{(2),j} \\le \\dots \\le z_{(N),j}$。则 $\\widehat{z}_j = \\frac{1}{N-2q}\\sum_{k=q+1}^{N-q} z_{(k),j}$。鲁棒支撑集为 $\\widehat{S}_{\\mathrm{rob}} = \\mathrm{Top}_s(|\\widehat{z}|)$。\n\n**2. 界定来自对抗者的扰动**\n我们首先为任意坐标 $j$ 的鲁棒聚合 $\\widehat{z}_j$ 和基线聚合 $z^{\\mathrm{ben}}_j$ 之间的差异建立一个界。\n\n令 $Z_j^{(\\mathcal{B})} = \\{z_{i,j}\\}_{i \\in \\mathcal{B}}$ 为坐标 $j$ 的 $N-q$ 个良性代理值集合。\n令 $m_j^{(\\mathcal{B})} = \\min(Z_j^{(\\mathcal{B})})$ 且 $M_j^{(\\mathcal{B})} = \\max(Z_j^{(\\mathcal{B})})$。\n基线聚合 $z^{\\mathrm{ben}}_j$ 是这些值的均值，因此它自然地被它们的范围所限制：$m_j^{(\\mathcal{B})} \\le z^{\\mathrm{ben}}_j \\le M_j^{(\\mathcal{B})}$。\n\n鲁棒聚合 $\\widehat{z}_j$ 是全部 $N$ 个代理的裁剪均值，其中包括 $q$ 个任意的对抗性值。在存在 $q$ 个对抗者的情况下，$q$-裁剪均值的一个基本性质是其值包含在良性数据的范围内。\n要理解这一点，考虑被平均的 $N-2q$ 个值，即 $\\{z_{(q+1),j}, \\dots, z_{(N-q),j}\\}$。其中最小的值 $z_{(q+1),j}$ 是 $N$ 个值的组合集合中的第 $(q+1)$ 个顺序统计量。这 $q+1$ 个最小值中最多可包含 $q$ 个对抗性值，因此它必须至少包含一个良性值。因此，$z_{(q+1),j} \\ge m_j^{(\\mathcal{B})}$。通过对称的论证，$z_{(N-q),j} \\le M_j^{(\\mathcal{B})}$。由于 $\\widehat{z}_j$ 是介于 $z_{(q+1),j}$ 和 $z_{(N-q),j}$ 之间的值的平均值，因此可以得出 $m_j^{(\\mathcal{B})} \\le \\widehat{z}_j \\le M_j^{(\\mathcal{B})}$。\n\n$\\widehat{z}_j$ 和 $z^{\\mathrm{ben}}_j$ 都位于区间 $[m_j^{(\\mathcal{B})}, M_j^{(\\mathcal{B})}]$ 内。因此，它们之间可能的最大差异是这个区间的长度：\n$$|\\widehat{z}_j - z^{\\mathrm{ben}}_j| \\le M_j^{(\\mathcal{B})} - m_j^{(\\mathcal{B})} =: R_j^{(\\mathcal{B})}$$\n其中 $R_j^{(\\mathcal{B})}$ 是坐标 $j$ 的良性代理值的范围。这为对抗者引起的坐标级扰动提供了一个数据驱动的界。这进一步意味着对幅值变化的界：\n$$||\\widehat{z}_j| - |z^{\\mathrm{ben}}_j|| \\le |\\widehat{z}_j - z^{\\mathrm{ben}}_j| \\le R_j^{(\\mathcal{B})}$$\n\n**3. 分析支撑集变化**\n令 $I_{in} = \\widehat{S}_{\\mathrm{rob}} \\setminus \\widehat{S}_{\\mathrm{base}}$ 为进入前 $s$ 名列表的索引集合，令 $I_{out} = \\widehat{S}_{\\mathrm{base}} \\setminus \\widehat{S}_{\\mathrm{rob}}$ 为离开的索引集合。由于两个支撑集的大小都是 $s$，我们必须有 $|I_{in}| = |I_{out}| =: K$。\n\n错误发现数量的变化是 $| \\widehat{S}_{\\mathrm{rob}} \\setminus S^{\\star} | - | \\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star} |$。这可以用 $I_{in}$ 和 $I_{out}$ 来表示：\n$$ \\Delta_{\\mathrm{FDR}} = \\frac{1}{s} \\left( |I_{in} \\setminus S^{\\star}| - |I_{out} \\cap (\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star})| \\right) $$\n为了获得一个上界，我们考虑最坏情况，即错误发现的增加最大化。这种情况发生在所有进入的索引都是错误发现（$I_{in} \\cap S^{\\star} = \\emptyset$）且没有已有的错误发现被移除（$I_{out} \\cap (\\widehat{S}_{\\mathrm{base}} \\setminus S^{\\star}) = \\emptyset$）时。在这种情况下，表达式变为：\n$$ \\Delta_{\\mathrm{FDR}} \\le \\frac{|I_{in}|}{s} = \\frac{K}{s} $$\n我们的任务简化为找到 $K$ 的上界，$K$ 是改变其在前 $s$ 个元素集合中成员资格的元素数量。\n\n**4. 界定交换次数 K**\n一个索引 $j \\in I_{in}$ 和一个索引 $k \\in I_{out}$ 意味着一次排序反转。在基线中，$k$ 优先于 $j$，所以 $|z^{\\mathrm{ben}}_k| \\ge |z^{\\mathrm{ben}}_j|$。在攻击后，$j$ 优先于 $k$，所以 $|\\widehat{z}_j|  |\\widehat{z}_k|$。\n\n我们可以使用扰动界来找到这种交换的必要条件：\n$$ |\\widehat{z}_j| \\le |z^{\\mathrm{ben}}_j| + R_j^{(\\mathcal{B})} $$\n$$ |\\widehat{z}_k| \\ge |z^{\\mathrm{ben}}_k| - R_k^{(\\mathcal{B})} $$\n为了发生交换 $|\\widehat{z}_j|  |\\widehat{z}_k|$，必须有：\n$$ |z^{\\mathrm{ben}}_j| + R_j^{(\\mathcal{B})}  |z^{\\mathrm{ben}}_k| - R_k^{(\\mathcal{B})} $$\n$$ \\implies |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} $$\n这个不等式为一对索引 $(j, k)$（其中 $j \\notin \\widehat{S}_{\\mathrm{base}}$ 且 $k \\in \\widehat{S}_{\\mathrm{base}}$）交换它们的相对排序，使得 $j$ 可能进入前 $s$ 集合而 $k$ 离开，提供了一个必要条件。\n\n**5. 一个可计算的上界**\n我们现在可以基于这个条件定义“脆弱”索引集。这些是其排序可能因有界扰动而改变的索引。\n令 $V_{out}$ 为不在 $\\widehat{S}_{\\mathrm{base}}$ 中但易于被提升进入的索引集合：\n$$ V_{out} = \\{ j \\notin \\widehat{S}_{\\mathrm{base}} \\mid \\exists k \\in \\widehat{S}_{\\mathrm{base}} \\text{ s.t. } |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} \\} $$\n令 $V_{in}$ 为在 $\\widehat{S}_{\\mathrm{base}}$ 中但易于被降级出局的索引集合：\n$$ V_{in} = \\{ k \\in \\widehat{S}_{\\mathrm{base}} \\mid \\exists j \\notin \\widehat{S}_{\\mathrm{base}} \\text{ s.t. } |z^{\\mathrm{ben}}_k| - |z^{\\mathrm{ben}}_j|  R_j^{(\\mathcal{B})} + R_k^{(\\mathcal{B})} \\} $$\n任何被提升的索引 $j \\in I_{in}$ 都必须是脆弱的，所以 $I_{in} \\subseteq V_{out}$。\n任何被降级的索引 $k \\in I_{out}$ 都必须是脆弱的，所以 $I_{out} \\subseteq V_{in}$。\n这意味着 $K = |I_{in}| \\le |V_{out}|$ 和 $K = |I_{out}| \\le |V_{in}|$.\n因此，我们可以通过 $K \\le \\min(|V_{out}|, |V_{in}|)$ 来界定 $K$。\n\n将此代入我们关于 $\\Delta_{\\mathrm{FDR}}$ 的不等式，我们得到最终的可计算上界：\n$$ \\Delta_{\\mathrm{FDR}} \\le \\frac{\\min(|V_{out}|, |V_{in}|)}{s} $$\n这个界只依赖于 $N, q$（通过大小为 $N-q$ 的集合 $\\mathcal{B}$ 隐含地）以及良性代理的经验统计量（$z^{\\mathrm{ben}}$ 和 $\\{R_j^{(\\mathcal{B})}\\}_{j=1}^p$），符合要求。\n\n所提出的算法首先通过坐标级裁剪均值聚合计算鲁棒支撑集估计 $\\widehat{S}_{\\mathrm{rob}}$。然后，使用基于良性代理统计量的推导公式计算理论上界。最后，通过比较确定经验观察到的 $\\Delta_{\\mathrm{FDR}}$ 是否遵守该理论上界。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the distributed sparse support recovery problem for a given set of test cases.\n\n    For each case, it performs the following steps:\n    1.  Generates synthetic data: a sparse signal `x_star`, and local measurements\n        at N nodes, where q nodes are adversarial.\n    2.  Computes local proxies `z_i` at each node. Adversarial nodes produce\n        corrupted proxies.\n    3.  Calculates the baseline support estimate `S_base` by averaging benign proxies.\n    4.  Calculates the robust support estimate `S_rob` using a coordinate-wise\n        trimmed-mean aggregator over all proxies.\n    5.  Computes the empirical increase in false discovery rate `delta_fdr_empirical`.\n    6.  Derives a theoretical upper bound `delta_fdr_bound` on this increase, based on\n        the statistics of the benign proxies.\n    7.  Checks if the empirical value is less than or equal to the theoretical bound.\n    8.  Outputs a list of booleans indicating the result for each test case.\n    \"\"\"\n    np.random.seed(42)  # For reproducibility\n\n    test_cases = [\n        # (N, q, p, s, m, sigma, B_adv)\n        (9, 0, 120, 5, 50, 0.05, 0.0),\n        (9, 2, 120, 5, 50, 0.05, 20.0),\n        (7, 3, 120, 5, 50, 0.05, 100.0),\n    ]\n\n    results = []\n    for N, q, p, s, m, sigma, B_adv in test_cases:\n        # Step 1: Generate Data\n        # Generate true sparse vector x_star\n        x_star = np.zeros(p)\n        support_indices = np.random.choice(p, s, replace=False)\n        S_star = set(support_indices)\n        magnitudes = np.ones(s)\n        signs = np.random.choice([-1, 1], s)\n        x_star[support_indices] = magnitudes * signs\n\n        # Generate proxies for all nodes\n        benign_proxies = []\n        all_proxies = []\n        benign_node_indices = list(range(N - q))\n        \n        for i in range(N):\n            if i in benign_node_indices:  # Benign node\n                A_i = np.random.normal(0, 1 / np.sqrt(m), (m, p))\n                w_i = np.random.normal(0, sigma, m)\n                y_i = A_i @ x_star + w_i\n                z_i = A_i.T @ y_i\n                benign_proxies.append(z_i)\n                all_proxies.append(z_i)\n            else:  # Adversarial node\n                # Adversaries inject large-magnitude noise\n                z_tilde_i = np.random.uniform(-B_adv, B_adv, p)\n                all_proxies.append(z_tilde_i)\n\n        Z_benign = np.array(benign_proxies) if len(benign_proxies) > 0 else np.array([]).reshape(0,p)\n        Z_all = np.array(all_proxies)\n\n        # Step 2: Compute Aggregates and Supports\n        # Baseline aggregation (mean over benign nodes)\n        if N > q:\n            z_ben = np.mean(Z_benign, axis=0)\n        else: # All nodes are adversarial\n            z_ben = np.zeros(p)\n        \n        magnitudes_base = np.abs(z_ben)\n        base_indices_sorted = np.argsort(magnitudes_base)\n        S_base_indices = base_indices_sorted[-s:]\n        S_base = set(S_base_indices)\n\n        # Robust aggregation (trimmed-mean over all nodes)\n        z_rob = np.zeros(p)\n        if N > 2 * q:\n            for j in range(p):\n                # Sort, trim, and average\n                sorted_vals = np.sort(Z_all[:, j])\n                trimmed_vals = sorted_vals[q:-q] if q > 0 else sorted_vals\n                z_rob[j] = np.mean(trimmed_vals)\n        \n        magnitudes_rob = np.abs(z_rob)\n        S_rob_indices = np.argsort(magnitudes_rob)[-s:]\n        S_rob = set(S_rob_indices)\n        \n        # Step 3: Compute Empirical FDR Increase\n        fdr_base = len(S_base - S_star) / s if s > 0 else 0\n        fdr_rob = len(S_rob - S_star) / s if s > 0 else 0\n        delta_fdr_empirical = fdr_rob - fdr_base\n\n        # Step 4: Compute Theoretical Bound\n        if N > q:\n            # Benign proxy ranges R_j\n            R_benign = np.ptp(Z_benign, axis=0)\n            \n            # S_base_indices and not_S_base_indices\n            not_S_base_indices = base_indices_sorted[:-s]\n\n            # Construct vulnerable sets\n            V_in = set()\n            V_out = set()\n            \n            # This is O(s * (p-s)), which is acceptable for given parameters\n            for k in S_base_indices:\n                for j in not_S_base_indices:\n                    mag_k = magnitudes_base[k]\n                    mag_j = magnitudes_base[j]\n                    R_k = R_benign[k]\n                    R_j = R_benign[j]\n\n                    if mag_k - mag_j  R_j + R_k:\n                        V_in.add(k)\n                        V_out.add(j)\n\n            K_bound = min(len(V_in), len(V_out))\n            delta_fdr_bound = K_bound / s if s > 0 else 0\n        else:\n            delta_fdr_bound = 1.0 \n\n        # Step 5: Compare and store result\n        # Add a small epsilon for floating-point comparisons\n        results.append(delta_fdr_empirical = delta_fdr_bound + 1e-9)\n\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3444450"}]}