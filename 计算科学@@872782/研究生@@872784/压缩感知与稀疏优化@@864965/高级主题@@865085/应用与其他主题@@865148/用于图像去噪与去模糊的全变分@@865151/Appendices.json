{"hands_on_practices": [{"introduction": "要掌握全变分（Total Variation, TV）在图像处理中的应用，第一步是理解其基本定义。本练习将通过一个具体的计算任务，引导你从第一性原理出发，为一个小的 $3 \\times 3$ 图像区块计算各向异性（anisotropic）和各向同性（isotropic）TV。通过亲手计算，你将直观地理解这两种TV范数的区别，尤其是在图像梯度不与坐标轴对齐的区域，这对于后续理解TV正则化的性质至关重要 [@problem_id:3491268]。", "problem": "考虑一个单位间距笛卡尔网格上的离散图像模型。设像素 $(i,j)$ 处的离散梯度由前向差分算子定义，并采用齐次诺伊曼边界条件（即，任何会访问越界像素的前向差分都设为零）。具体来说，$(i,j)$ 处的离散梯度是一个二维向量，由 $(i,j)$ 处的前向水平差分和垂直差分构成。离散图像的各向异性总变分 (TV) 是所有像素上离散梯度向量的 $\\ell_{1}$ 范数之和。各向同性总变分是所有像素上离散梯度向量的 $\\ell_{2}$ 范数之和。\n\n给定一个 $3\\times 3$ 的图像块，其像素值为\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1  2  2 \\\\\n1  3  5 \\\\\n2  3  6\n\\end{pmatrix}.\n$$\n仅使用上述基本定义和给定的边界模型，完成以下任务：\n- 明确计算 $U$ 的各向异性总变分和各向同性总变分。\n- 根据向量范数的基本原理，简要解释为何这两个值对于此图像块在数值上存在差异，并指出差异在像素级别的结构性来源。\n\n令 $D$ 表示各向异性总变分与各向同性总变分之间的精确差值，\n$$\nD \\;=\\; \\mathrm{TV}_{\\mathrm{anisotropic}}(U) \\;-\\; \\mathrm{TV}_{\\mathrm{isotropic}}(U).\n$$\n将 $D$ 表示为单个闭式解析表达式。请勿对答案进行四舍五入。", "solution": "问题陈述内部一致，其科学基础植根于数值图像分析和向量范数的原理，并提供了计算唯一解所需的所有信息。因此，该问题被认定为有效。我们开始求解。\n\n设离散图像由矩阵 $U$ 表示，其中 $U_{i,j}$ 是第 $i$ 行、第 $j$ 列的像素值。该网格是一个 $3 \\times 3$ 的笛卡尔网格，因此索引 $(i,j)$ 的范围从 $(1,1)$ 到 $(3,3)$。给定的图像为：\n$$\nU =\n\\begin{pmatrix}\n1  2  2 \\\\\n1  3  5 \\\\\n2  3  6\n\\end{pmatrix}\n$$\n像素 $(i,j)$ 处的离散梯度是一个向量 $\\nabla U_{i,j} = \\begin{pmatrix} (\\nabla_x U)_{i,j} \\\\ (\\nabla_y U)_{i,j} \\end{pmatrix}$。其分量由前向差分定义：\n$$\n(\\nabla_x U)_{i,j} = U_{i,j+1} - U_{i,j}\n$$\n$$\n(\\nabla_y U)_{i,j} = U_{i+1,j} - U_{i,j}\n$$\n齐次诺伊曼边界条件意味着任何需要越界像素的差分都被设为零。对于一个 $3 \\times 3$ 的图像，这意味着当 $i \\in \\{1,2,3\\}$ 时 $(\\nabla_x U)_{i,3} = 0$，以及当 $j \\in \\{1,2,3\\}$ 时 $(\\nabla_y U)_{3,j} = 0$。\n\n首先，我们计算这 9 个像素中每一个的离散梯度向量：\n- 在 $(1,1)$ 处：$\\nabla U_{1,1} = \\begin{pmatrix} U_{1,2} - U_{1,1} \\\\ U_{2,1} - U_{1,1} \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ 1 - 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- 在 $(1,2)$ 处：$\\nabla U_{1,2} = \\begin{pmatrix} U_{1,3} - U_{1,2} \\\\ U_{2,2} - U_{1,2} \\end{pmatrix} = \\begin{pmatrix} 2 - 2 \\\\ 3 - 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n- 在 $(1,3)$ 处：$\\nabla U_{1,3} = \\begin{pmatrix} 0 \\\\ U_{2,3} - U_{1,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 5 - 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\end{pmatrix}$\n- 在 $(2,1)$ 处：$\\nabla U_{2,1} = \\begin{pmatrix} U_{2,2} - U_{2,1} \\\\ U_{3,1} - U_{2,1} \\end{pmatrix} = \\begin{pmatrix} 3 - 1 \\\\ 2 - 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n- 在 $(2,2)$ 处：$\\nabla U_{2,2} = \\begin{pmatrix} U_{2,3} - U_{2,2} \\\\ U_{3,2} - U_{2,2} \\end{pmatrix} = \\begin{pmatrix} 5 - 3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$\n- 在 $(2,3)$ 处：$\\nabla U_{2,3} = \\begin{pmatrix} 0 \\\\ U_{3,3} - U_{2,3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 6 - 5 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$\n- 在 $(3,1)$ 处：$\\nabla U_{3,1} = \\begin{pmatrix} U_{3,2} - U_{3,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 - 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n- 在 $(3,2)$ 处：$\\nabla U_{3,2} = \\begin{pmatrix} U_{3,3} - U_{3,2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6 - 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}$\n- 在 $(3,3)$ 处：$\\nabla U_{3,3} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n\n接下来，我们计算各向异性总变分 $\\mathrm{TV}_{\\mathrm{anisotropic}}(U)$，它是这些梯度向量的 $\\ell_1$ 范数之和。对于一个向量 $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$，其 $\\ell_1$ 范数为 $\\|v\\|_1 = |v_x| + |v_y|$。\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\|\\nabla U_{i,j}\\|_1\n$$\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = (|1|+|0|) + (|0|+|1|) + (|0|+|3|) + (|2|+|1|) + (|2|+|0|) + (|0|+|1|) + (|1|+|0|) + (|3|+|0|) + (|0|+|0|)\n$$\n$$\n\\mathrm{TV}_{\\mathrm{anisotropic}}(U) = 1 + 1 + 3 + 3 + 2 + 1 + 1 + 3 + 0 = 15\n$$\n\n然后，我们计算各向同性总变分 $\\mathrm{TV}_{\\mathrm{isotropic}}(U)$，它是 $\\ell_2$ 范数之和。对于一个向量 $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$，其 $\\ell_2$ 范数为 $\\|v\\|_2 = \\sqrt{v_x^2 + v_y^2}$。\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} \\|\\nabla U_{i,j}\\|_2\n$$\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = \\sqrt{1^2+0^2} + \\sqrt{0^2+1^2} + \\sqrt{0^2+3^2} + \\sqrt{2^2+1^2} + \\sqrt{2^2+0^2} + \\sqrt{0^2+1^2} + \\sqrt{1^2+0^2} + \\sqrt{3^2+0^2} + \\sqrt{0^2+0^2}\n$$\n$$\n\\mathrm{TV}_{\\mathrm{isotropic}}(U) = 1 + 1 + 3 + \\sqrt{5} + 2 + 1 + 1 + 3 + 0 = 12 + \\sqrt{5}\n$$\n\n这两个值之所以不同，是由于 $\\ell_1$ 和 $\\ell_2$ 范数的基本性质。对于 $\\mathbb{R}^n$ 中的任何向量 $v$，不等式 $\\|v\\|_1 \\ge \\|v\\|_2$ 恒成立。在我们的例子中，对于一个二维梯度向量 $v = \\begin{pmatrix} v_x \\\\ v_y \\end{pmatrix}$，我们有 $\\|v\\|_1 = |v_x|+|v_y|$ 和 $\\|v\\|_2 = \\sqrt{v_x^2+v_y^2}$。等式 $|v_x|+|v_y| = \\sqrt{v_x^2+v_y^2}$ 成立的充要条件是至少有一个分量 $v_x$ 或 $v_y$ 为零。这对应于一个纯水平或纯垂直的梯度。\n\n$\\mathrm{TV}_{\\mathrm{anisotropic}}(U)$ 和 $\\mathrm{TV}_{\\mathrm{isotropic}}(U)$ 之间的数值差异，源于所有像素上 $\\|\\nabla U_{i,j}\\|_1 - \\|\\nabla U_{i,j}\\|_2$ 的差值之和。只有在梯度向量的两个分量都非零的像素点，这个差值才不为零。\n检查我们计算出的梯度，只有 $(2,1)$ 处的像素具有含两个非零分量的梯度向量：$\\nabla U_{2,1} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。对于所有其他像素，其梯度向量至少有一个分量为零，这意味着对于所有 $(i,j) \\neq (2,1)$，都有 $\\|\\nabla U_{i,j}\\|_1 = \\|\\nabla U_{i,j}\\|_2$。\n因此，差异的结构性来源完全位于像素 $(2,1)$ 处，该点代表一个角点或图像强度同时在水平和垂直方向上都发生变化的点。\n\n最后，我们计算精确差值 $D$：\n$$\nD = \\mathrm{TV}_{\\mathrm{anisotropic}}(U) - \\mathrm{TV}_{\\mathrm{isotropic}}(U)\n$$\n这个差值是逐项差值的总和，可以简化为仅由这一个有贡献的像素所产生的差值：\n$$\nD = (\\|\\nabla U_{2,1}\\|_1 - \\|\\nabla U_{2,1}\\|_2) = (|2|+|1|) - \\sqrt{2^2+1^2} = 3 - \\sqrt{5}\n$$\n或者，使用计算出的总值：\n$$\nD = 15 - (12 + \\sqrt{5}) = 3 - \\sqrt{5}\n$$\n结果是一致的。", "answer": "$$\n\\boxed{3 - \\sqrt{5}}\n$$", "id": "3491268"}, {"introduction": "理解了全变分的定义后，下一个关键问题是它为何能有效去噪同时保护边缘。本练习通过分析一个简化的Rudin–Osher–Fatemi (ROF) 模型来揭示其内在机理。通过求解一个仅有两个像素的一维信号的ROF问题，你将推导出其解析解，并发现去噪过程本质上是一个对信号梯度进行的软阈值操作 [@problem_id:3491235]。这个练习清晰地展示了正则化参数 $\\lambda$ 如何控制噪声滤除与细节保留之间的平衡。", "problem": "考虑用于图像去噪的 Rudin–Osher–Fatemi (ROF) 模型，该模型旨在寻找一个信号 $u$，使其在与给定带噪观测 $f$ 的保真度和惩罚其全变分的正则化之间取得平衡。在具有两个像素的离散一维形式中，ROF 能量定义为\n$$\nE(u) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{2} (u_{i} - f_{i})^{2} \\;+\\; \\lambda\\,\\mathrm{TV}(u),\n$$\n其中 $\\lambda0$ 是正则化参数，$\\mathrm{TV}(u)$ 表示全变分。对于一维的各向异性全变分，全变分是前向差分绝对值之和，因此对于两个像素，它简化为\n$$\n\\mathrm{TV}(u) \\;=\\; |u_{2} - u_{1}|.\n$$\n设观测到的双像素信号为 $f=(a,b)$，其中 $a \\in \\mathbb{R}$ 且 $b \\in \\mathbb{R}$。从以上定义出发，并利用凸优化和次梯度最优性的基本原理，推导出 $E(u)$ 的精确最小值点 $u^{\\star}=(u_{1}^{\\star},u_{2}^{\\star})$ 的闭式解析表达式，用 $a$、$b$ 和 $\\lambda$ 表示。此外，确定 $|a-b|$ 的显式阈值，在该阈值下，最小值点变为分段常数，即 $u_{1}^{\\star} = u_{2}^{\\star}$。\n\n将你的最终答案表示为一个包含三个条目 $(u_{1}^{\\star},\\,u_{2}^{\\star},\\,T)$ 的单行向量，其中 $T$ 是用 $\\lambda$ 表示的阈值。无需进行数值计算。", "solution": "用户提供了一个问题陈述以供验证和求解。\n\n### 步骤 1：提取已知条件\n- **模型**：具有 $2$ 个像素的 $1$ 维信号的 Rudin–Osher–Fatemi (ROF) 模型。\n- **能量函数**：$E(u) = \\frac{1}{2}\\sum_{i=1}^{2} (u_{i} - f_{i})^{2} + \\lambda\\,\\mathrm{TV}(u)$。\n- **信号**：$u = (u_1, u_2)$，其中 $u_1, u_2 \\in \\mathbb{R}$。\n- **观测**：$f = (a, b)$，其中 $a, b \\in \\mathbb{R}$。\n- **正则化参数**：$\\lambda  0$。\n- **全变分 (TV)**：$\\mathrm{TV}(u) = |u_{2} - u_{1}|$。\n- **目标**：\n    1.  找到 $E(u)$ 的精确最小值点 $u^{\\star}=(u_{1}^{\\star},u_{2}^{\\star})$，以 $a$、$b$ 和 $\\lambda$ 表示的闭式解析表达式。\n    2.  确定 $|a-b|$ 上的显式阈值 $T$，在该阈值下，最小值点变为分段常数，即 $u_{1}^{\\star} = u_{2}^{\\star}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题基于 Rudin–Osher–Fatemi (ROF) 模型，这是图像处理、逆问题和稀疏优化中一个基础且广泛使用的模型。全变分 (TV) 的定义是 $1$ 维各向异性情况下的标准定义。该问题是凸优化原理的一个正确且成熟的应用。它不基于任何虚假前提或伪科学。\n2.  **适定性**：能量函数 $E(u)$ 是两个凸函数之和：一个严格凸的二次项 $\\frac{1}{2}\\|u-f\\|_2^2$，和一个凸的正则化项 $\\lambda|u_2-u_1|$。因此，它们的和 $E(u)$ 是严格凸的。一个严格凸函数有唯一的最小值点。因此，存在一个唯一、稳定且有意义的解。\n3.  **目标**：问题以精确的数学语言陈述，没有歧义、主观性或个人观点。\n4.  **完整性与一致性**：所有必要的变量（$a, b, \\lambda$）和定义都已提供。问题是自洽的，没有矛盾。\n5.  **相关性**：该问题与用于图像去噪的全变分主题直接相关，这是压缩感知和稀疏优化的核心概念。\n\n### 步骤 3：结论与行动\n问题是有效的。我将进行完整、有理有据的解答。\n\n需要最小化的能量函数由下式给出：\n$$\nE(u_1, u_2) = \\frac{1}{2}((u_1 - a)^2 + (u_2 - b)^2) + \\lambda|u_2 - u_1|\n$$\n该函数是凸函数，但由于绝对值项的存在，并非处处可微。唯一的最小值点 $u^{\\star}=(u_1^{\\star}, u_2^{\\star})$ 可以通过将 $E$ 的次梯度设为零来找到。次梯度最优性条件是 $0 \\in \\partial E(u^{\\star})$。\n\n$E$ 关于向量 $u=(u_1, u_2)$ 的次梯度是可微部分的梯度与不可微部分的次梯度之和。\n保真项 $F(u_1, u_2) = \\frac{1}{2}((u_1 - a)^2 + (u_2 - b)^2)$ 的梯度是：\n$$\n\\nabla F(u_1, u_2) = \\begin{pmatrix} u_1 - a \\\\ u_2 - b \\end{pmatrix}\n$$\n全变分项 $R(u_1, u_2) = \\lambda|u_2 - u_1|$ 的次梯度是：\n$$\n\\partial R(u_1, u_2) = \\lambda \\partial |u_2 - u_1| = \\lambda \\begin{pmatrix} -\\partial_z |z| \\\\ \\partial_z |z| \\end{pmatrix}_{z=u_2-u_1} = \\lambda \\begin{pmatrix} -\\mathrm{sgn}(u_2-u_1) \\\\ \\mathrm{sgn}(u_2-u_1) \\end{pmatrix}\n$$\n其中 $\\mathrm{sgn}(z)$ 是集值符号函数：\n$$\n\\mathrm{sgn}(z) = \\begin{cases} \\{1\\}  \\text{若 } z  0 \\\\ \\{-1\\}  \\text{若 } z  0 \\\\ [-1, 1]  \\text{若 } z = 0 \\end{cases}\n$$\n最优性条件 $0 \\in \\partial E(u_1, u_2) = \\nabla F(u_1, u_2) + \\partial R(u_1, u_2)$ 意味着存在一个标量 $s \\in \\mathrm{sgn}(u_2 - u_1)$ 使得：\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} u_1 - a \\\\ u_2 - b \\end{pmatrix} + \\lambda \\begin{pmatrix} -s \\\\ s \\end{pmatrix}\n$$\n这给了我们一个由两个方程组成的方程组：\n1. $u_1 - a - \\lambda s = 0 \\implies u_1 = a + \\lambda s$\n2. $u_2 - b + \\lambda s = 0 \\implies u_2 = b - \\lambda s$\n\n$s$ 的值取决于 $u_2 - u_1$ 的符号。让我们分情况讨论。\n\n**情况 1：$u_2 - u_1  0$**\n在这种情况下，$s = \\mathrm{sgn}(u_2 - u_1) = 1$。将 $s=1$ 代入 $u_1$ 和 $u_2$ 的方程中：\n$u_1^{\\star} = a + \\lambda$\n$u_2^{\\star} = b - \\lambda$\n为使该解有效，初始假设 $u_2 - u_1  0$ 必须成立：\n$(b - \\lambda) - (a + \\lambda)  0 \\implies b - a - 2\\lambda  0 \\implies b - a  2\\lambda$。\n\n**情况 2：$u_2 - u_1  0$**\n在这种情况下，$s = \\mathrm{sgn}(u_2 - u_1) = -1$。代入 $s=-1$：\n$u_1^{\\star} = a - \\lambda$\n$u_2^{\\star} = b + \\lambda$\n初始假设 $u_2 - u_1  0$ 必须成立：\n$(b + \\lambda) - (a - \\lambda)  0 \\implies b - a + 2\\lambda  0 \\implies a - b  2\\lambda$。\n\n**情况 3：$u_2 - u_1 = 0$**\n在这种情况下，$s \\in [-1, 1]$。条件 $u_1=u_2$ 意味着解是分段常数。设 $u_1^{\\star} = u_2^{\\star} = c$。能量函数变为：\n$E(c, c) = \\frac{1}{2}((c - a)^2 + (c - b)^2)$。\n为了找到最小值，我们对 $c$ 求导并令其为零：\n$\\frac{dE}{dc} = (c-a) + (c-b) = 2c - (a+b) = 0 \\implies c = \\frac{a+b}{2}$。\n所以，如果 $u_1^{\\star} = u_2^{\\star}$，那么 $u_1^{\\star} = u_2^{\\star} = \\frac{a+b}{2}$。\n现在我们检查对于 $a$ 和 $b$ 的哪些值这个解是有效的。我们需要找到一个满足最优性方程的 $s \\in [-1, 1]$：\n$u_1 = a + \\lambda s \\implies \\frac{a+b}{2} = a + \\lambda s \\implies \\lambda s = \\frac{b-a}{2} \\implies s = \\frac{b-a}{2\\lambda}$。\n条件 $s \\in [-1, 1]$ 变为：\n$-1 \\le \\frac{b-a}{2\\lambda} \\le 1 \\implies -2\\lambda \\le b-a \\le 2\\lambda \\implies |a-b| \\le 2\\lambda$。\n\n最小值点 $u^{\\star}=(u_{1}^{\\star},u_{2}^{\\star})$ 的总结：\n- 如果 $a-b  2\\lambda$，那么 $u_1^{\\star} = a - \\lambda$ 且 $u_2^{\\star} = b + \\lambda$。\n- 如果 $b-a  2\\lambda$，那么 $u_1^{\\star} = a + \\lambda$ 且 $u_2^{\\star} = b - \\lambda$。\n- 如果 $|a-b| \\le 2\\lambda$，那么 $u_1^{\\star} = u_2^{\\star} = \\frac{a+b}{2}$。\n\n当且仅当 $|a-b| \\le 2\\lambda$ 时，最小值点是分段常数（$u_1^{\\star} = u_2^{\\star}$）。问题要求的是发生这种转变时 $|a-b|$ 的显式阈值。这个阈值是区域的边界，即 $T = 2\\lambda$。\n\n为了将解表示为单一的闭式形式，我们可以使用软阈值函数 $\\mathrm{soft}(z, \\gamma) = \\operatorname{sgn}(z) \\max(0, |z|-\\gamma)$。\n根据我们的最优性条件，我们有 $u_1^{\\star}+u_2^{\\star} = a+b$ 和 $u_1^{\\star}-u_2^{\\star} = a-b+2\\lambda s$。情况分析等价于找到 $u_1^{\\star}-u_2^{\\star}$ 的值。这是一个经典的邻近算子问题。$d' = u_1^{\\star}-u_2^{\\star}$ 的解是最小化 $\\frac{1}{2}(d' - (a-b))^2 + 2\\lambda|d'|$ 的解，即 $d' = \\mathrm{soft}(a-b, 2\\lambda)$。\n所以，我们有：\n$u_1^{\\star} + u_2^{\\star} = a+b$\n$u_1^{\\star} - u_2^{\\star} = \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0)$\n求解这个关于 $u_1^{\\star}$ 和 $u_2^{\\star}$ 的方程组：\n$u_1^{\\star} = \\frac{1}{2} \\left( (a+b) + \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0) \\right)$\n$u_2^{\\star} = \\frac{1}{2} \\left( (a+b) - \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0) \\right)$\n\n所需的三个量是 $u_1^{\\star}$、$u_2^{\\star}$ 和阈值 $T=2\\lambda$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}\\left(a+b + \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0)\\right)  \\frac{1}{2}\\left(a+b - \\operatorname{sgn}(a-b)\\max(|a-b|-2\\lambda, 0)\\right)  2\\lambda \\end{pmatrix}}\n$$", "id": "3491235"}, {"introduction": "在实际应用中，大多数TV正则化问题由于其复杂性而没有解析解，必须依赖迭代优化算法求解。本练习将带你进入解决实际问题的算法世界，通过一个结合了去模糊与去噪的一维信号恢复问题，体验交替方向乘子法（Alternating Direction Method of Multipliers, ADMM）的核心步骤。你将完整执行一次ADMM迭代，包括求解一个二次规划子问题（$u$-更新）和应用邻近算子（$w$-更新），从而深入理解如何将复杂的优化问题分解为一系列更简单的子任务进行求解 [@problem_id:3491252]。", "problem": "考虑一个带全变分（TV）正则化的一维长度为3的信号去噪和去模糊问题。其正向模型是一个作用于信号 $u \\in \\mathbb{R}^{3}$ 的模糊算子，定义为在三点窗口上的循环平均模糊。在长度为3的网格上，采用周期性边界条件，模糊矩阵为 $K = \\frac{1}{3} J$，其中 $J \\in \\mathbb{R}^{3 \\times 3}$ 是全1矩阵。观测数据为 $f \\in \\mathbb{R}^{3}$，重建过程求解以下凸优化问题\n$$\n\\min_{u \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| D u \\|_{1},\n$$\n其中 $D \\in \\mathbb{R}^{3 \\times 3}$ 是带周期性边界条件的离散前向差分算子，\n$$\nD = \\begin{pmatrix}\n-1  1  0 \\\\\n0  -1  1 \\\\\n1  0  -1\n\\end{pmatrix},\n$$\n且 $\\| D u \\|_{1}$ 是各向异性TV半范数。\n\n引入辅助变量 $w \\in \\mathbb{R}^{3}$，将 $\\| D u \\|_{1}$ 分裂为 $\\| w \\|_{1}$，并满足约束 $w = D u$。考虑使用交替方向乘子法（ADMM）的缩放形式，其中缩放对偶变量为 $y \\in \\mathbb{R}^{3}$，罚参数为 $\\rho  0$。缩放ADMM迭代过程为：交替地对增广拉格朗日量分别关于 $u$ 和 $w$ 进行最小化，然后对 $y$ 执行对偶上升。\n\n考虑以下特定实例：\n- 数据 $f = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}$，\n- 正则化参数 $\\lambda = \\frac{1}{2}$，\n- 罚参数 $\\rho = 1$，\n- 初始迭代值 $u^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$，$w^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$，$y^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n从这些初始迭代值开始，执行一次完整的ADMM迭代，以计算更新后的 $u^{1}$ 和 $w^{1}$。在 $w$-更新步骤中，应将作为软阈值映射 $S_{\\tau}(x) = \\operatorname{sign}(x) \\max\\{ |x| - \\tau, 0 \\}$ 的近端算子逐分量地应用。\n\n将您的最终答案表示为一个单一的行向量，该向量由 $u^{1}$ 和 $w^{1}$ 按以下顺序拼接而成：\n$$\n\\begin{pmatrix}\nu^{1}_{1}  u^{1}_{2}  u^{1}_{3}  w^{1}_{1}  w^{1}_{2}  w^{1}_{3}\n\\end{pmatrix}.\n$$\n无需四舍五入，也不涉及单位。您的答案必须是精确值。", "solution": "该问题要求对一个特定的一维全变分（TV）正则化去模糊问题执行一次完整的交替方向乘子法（ADMM）迭代。该优化问题由下式给出：\n$$ \\min_{u \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| D u \\|_{1} $$\n我们引入辅助变量 $w = Du$ 将问题分裂为：\n$$ \\min_{u, w} \\ \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| w \\|_{1} \\quad \\text{满足约束条件} \\quad Du - w = 0 $$\n该问题的缩放增广拉格朗日量 $L_{\\rho}(u, w, y)$ 为：\n$$ L_{\\rho}(u, w, y) = \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\lambda \\| w \\|_{1} + \\frac{\\rho}{2} \\| Du - w + y \\|_{2}^{2} $$\n其中省略了不依赖于 $u$ 或 $w$ 的项。一次ADMM迭代包含三个步骤：\n1. $u$-更新：$u^{k+1} = \\arg\\min_{u} L_{\\rho}(u, w^k, y^k)$\n2. $w$-更新：$w^{k+1} = \\arg\\min_{w} L_{\\rho}(u^{k+1}, w, y^k)$\n3. $y$-更新：$y^{k+1} = y^k + Du^{k+1} - w^{k+1}$\n\n给定初始迭代值 $u^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$，$w^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 和 $y^{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。我们将使用给定参数 $\\lambda = \\frac{1}{2}$ 和 $\\rho = 1$ 来计算 $u^1$ 和 $w^1$。\n\n**步骤1：$u$-更新**\n我们通过最小化关于 $u$ 的拉格朗日量来计算 $u^1$，同时将 $w$ 和 $y$ 保持在 $k=0$ 时的值：\n$$ u^1 = \\arg\\min_{u} \\left( \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\frac{\\rho}{2} \\| Du - w^0 + y^0 \\|_{2}^{2} \\right) $$\n代入 $w^0=0$ 和 $y^0=0$：\n$$ u^1 = \\arg\\min_{u} \\left( \\frac{1}{2} \\| K u - f \\|_{2}^{2} + \\frac{\\rho}{2} \\| Du \\|_{2}^{2} \\right) $$\n这是一个标准的最小二乘问题。目标函数是关于 $u$ 的二次函数。为求最小值，我们将关于 $u$ 的梯度设为零：\n$$ \\nabla_u \\left( \\frac{1}{2} (Ku-f)^T(Ku-f) + \\frac{\\rho}{2} (Du)^T(Du) \\right) = 0 $$\n$$ K^T(Ku - f) + \\rho D^T D u = 0 $$\n这给出了正规方程组：\n$$ (K^T K + \\rho D^T D) u^1 = K^T f $$\n我们需要计算所涉及的矩阵和向量。\n模糊矩阵为 $K = \\frac{1}{3} J = \\frac{1}{3}\\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}$。\n因此，$K^T=K$。我们计算 $K^T K = K^2 = \\left(\\frac{1}{3}J\\right)^2 = \\frac{1}{9}J^2$。对于一个 $3 \\times 3$ 的全1矩阵，$J^2=3J$。因此，$K^T K = \\frac{1}{9}(3J) = \\frac{1}{3}J = K$。\n前向差分矩阵为 $D = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\\\ 1  0  -1 \\end{pmatrix}$。其转置为 $D^T = \\begin{pmatrix} -1  0  1 \\\\ 1  -1  0 \\\\ 0  1  -1 \\end{pmatrix}$。\n乘积 $D^T D$ 是：\n$$ D^T D = \\begin{pmatrix} -1  0  1 \\\\ 1  -1  0 \\\\ 0  1  -1 \\end{pmatrix} \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\\\ 1  0  -1 \\end{pmatrix} = \\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix} $$\n当 $\\rho=1$ 时，系统矩阵 $A = K^T K + \\rho D^T D$ 为：\n$$ A = \\frac{1}{3}\\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} + \\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{3}  -\\frac{2}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{7}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  -\\frac{2}{3}  \\frac{7}{3} \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 7  -2  -2 \\\\ -2  7  -2 \\\\ -2  -2  7 \\end{pmatrix} $$\n右侧项为 $b = K^T f = K f$。给定 $f = \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix}$：\n$$ b = \\frac{1}{3}\\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 4 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 1+1+4 \\\\ 1+1+4 \\\\ 1+1+4 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 6 \\\\ 6 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\n我们求解线性系统 $A u^1 = b$：\n$$ \\frac{1}{3}\\begin{pmatrix} 7  -2  -2 \\\\ -2  7  -2 \\\\ -2  -2  7 \\end{pmatrix} u^1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} $$\n通过观察，我们尝试一个常数解 $u^1 = \\begin{pmatrix} c \\\\ c \\\\ c \\end{pmatrix}$。等式左侧变为：\n$$ \\frac{1}{3}\\begin{pmatrix} 7c - 2c - 2c \\\\ -2c + 7c - 2c \\\\ -2c - 2c + 7c \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 3c \\\\ 3c \\\\ 3c \\end{pmatrix} = \\begin{pmatrix} c \\\\ c \\\\ c \\end{pmatrix} $$\n将其与右侧的 $\\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$ 相等，我们得到 $c=2$。\n因此，解为 $u^1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$。\n\n**步骤2：$w$-更新**\n我们使用新计算出的 $u^1$，通过最小化关于 $w$ 的拉格朗日量来计算 $w^1$：\n$$ w^1 = \\arg\\min_{w} \\left( \\lambda \\| w \\|_{1} + \\frac{\\rho}{2} \\| Du^1 - w + y^0 \\|_{2}^{2} \\right) $$\n代入 $y^0=0$，该问题等价于求解 $\\ell_1$-范数的近端算子：\n$$ w^1 = \\arg\\min_{w} \\left( \\lambda \\| w \\|_{1} + \\frac{\\rho}{2} \\| w - Du^1 \\|_{2}^{2} \\right) = \\text{prox}_{\\frac{\\lambda}{\\rho}\\|\\cdot\\|_1}(Du^1) $$\n这个近端算子是逐元素应用的软阈值函数 $S_{\\tau}(x)$，其阈值为 $\\tau = \\frac{\\lambda}{\\rho}$。\n给定 $\\lambda = \\frac{1}{2}$ 和 $\\rho = 1$，阈值为 $\\tau = \\frac{1/2}{1} = \\frac{1}{2}$。\n首先，我们计算自变量 $Du^1$：\n$$ Du^1 = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\\\ 1  0  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -2+2 \\\\ -2+2 \\\\ 2-2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n现在我们将软阈值算子 $S_{1/2}$ 应用于 $Du^1$ 的每个分量：\n$$ w^1_i = S_{1/2}((Du^1)_i) = \\text{sign}((Du^1)_i) \\max\\{ |(Du^1)_i| - \\frac{1}{2}, 0 \\} $$\n对于每个分量，$(Du^1)_i=0$，所以：\n$$ w^1_i = \\text{sign}(0) \\max\\{ |0| - \\frac{1}{2}, 0 \\} = 0 \\cdot 0 = 0 $$\n因此，$w^1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n**第一次迭代的结论**\n经过一次完整的ADMM迭代，更新后的变量为：\n$u^1 = \\begin{pmatrix} 2 \\\\ 2 \\\\ 2 \\end{pmatrix}$ 和 $w^1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n问题要求将最终答案表示为拼接 $u^1$ 和 $w^1$ 分量得到的单一的行向量。\n该向量为 $\\begin{pmatrix} u^1_1  u^1_2  u^1_3  w^1_1  w^1_2  w^1_3 \\end{pmatrix}$。\n代入计算出的值，我们得到 $\\begin{pmatrix} 2  2  2  0  0  0 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  2  2  0  0  0\n\\end{pmatrix}\n}\n$$", "id": "3491252"}]}