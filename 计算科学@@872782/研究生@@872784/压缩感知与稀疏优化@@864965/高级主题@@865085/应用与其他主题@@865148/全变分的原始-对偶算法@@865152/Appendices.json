{"hands_on_practices": [{"introduction": "理论是算法的基石，而亲手计算则是内化理论的最佳方式。本节的第一个练习旨在通过一个简洁的一维全变分降噪问题，让您从最基本的原理出发，推导并求解一个具体实例[@problem_id:3466896]。通过手动计算原始解与对偶解，您将能直观地验证连接二者的关键关系，从而深刻理解Rudin-Osher-Fatemi (ROF)模型背后的凸优化对偶理论。", "problem": "考虑一个长度为 $n=5$ 的网格上的一维Rudin-Osher-Fatemi (ROF) 全变分去噪问题。设离散前向差分算子 $K:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ 定义为 $(Kx)_{i}=x_{i+1}-x_{i}$，其中 $i\\in\\{1,2,3,4\\}$，并令 $K^{\\top}$ 表示其伴随算子，对于任意 $p\\in\\mathbb{R}^{4}$，它满足 $K^{\\top}p=\\left[-p_{1},\\,p_{1}-p_{2},\\,p_{2}-p_{3},\\,p_{3}-p_{4},\\,p_{4}\\right]^{\\top}$。给定数据向量 $b\\in\\mathbb{R}^{5}$ 定义为\n$$\nb=\\begin{pmatrix}1\\\\2\\\\-1\\\\0\\\\0\\end{pmatrix},\n$$\n以及正则化参数 $\\lambda=1$，考虑以下凸优化问题\n$$\n\\min_{x\\in\\mathbb{R}^{5}}\\;\\frac{1}{2}\\|x-b\\|_{2}^{2}+\\lambda\\|Kx\\|_{1}.\n$$\n仅使用凸分析和对偶性的基本原理（例如，凸共轭和次梯度的定义），并且不调用任何现成的算法，完成以下任务：\n- 推导与上述问题相关的原始-对偶对 $(x^{\\star},p^{\\star})$ 的充分必要最优性条件，其中对偶变量 $p$ 必须满足箱式约束 $\\|p\\|_{\\infty}\\leq \\lambda$。\n- 显式地计算满足这些条件的唯一原始解 $x^{\\star}$ 和一个对偶解 $p^{\\star}$。\n- 验证原始-对偶关系 $x^{\\star}=b-K^{\\top}p^{\\star}$。\n\n将你的最终答案表示为一个单行矩阵，其前五个条目依次为 $x^{\\star}$ 的分量，后四个条目依次为 $p^{\\star}$ 的分量。不需要四舍五入，也不涉及单位。最终答案必须是如上指定的单个矩阵表达式。", "solution": "该问题是求解给定数据向量的一维Rudin-Osher-Fatemi (ROF) 全变分去噪问题。\n\n### 第一步：提取已知条件\n- 网格长度：$n=5$。\n- 原始变量：$x \\in \\mathbb{R}^{5}$。\n- 数据向量：$b=\\begin{pmatrix}1 & 2 & -1 & 0 & 0\\end{pmatrix}^{\\top} \\in \\mathbb{R}^{5}$。\n- 正则化参数：$\\lambda=1$。\n- 离散前向差分算子 $K:\\mathbb{R}^{5}\\to\\mathbb{R}^{4}$ 定义为 $(Kx)_{i}=x_{i+1}-x_{i}$，其中 $i\\in\\{1,2,3,4\\}$。\n- 伴随算子 $K^{\\top}:\\mathbb{R}^{4}\\to\\mathbb{R}^{5}$ 定义为 $K^{\\top}p=\\left(-p_{1},\\,p_{1}-p_{2},\\,p_{2}-p_{3},\\,p_{3}-p_{4},\\,p_{4}\\right)^{\\top}$，其中 $p\\in\\mathbb{R}^{4}$。\n- 优化问题：$\\min_{x\\in\\mathbb{R}^{5}}\\;\\frac{1}{2}\\|x-b\\|_{2}^{2}+\\lambda\\|Kx\\|_{1}$。\n- 对偶变量约束：$\\|p\\|_{\\infty}\\leq \\lambda$。\n\n### 第二步：使用提取的已知条件进行验证\n该问题在科学上和数学上是合理的。这是 ROF 模型的一个标准实例，该模型是信号处理和反问题中的一个成熟方法。目标函数是一个严格凸、可微项（$\\frac{1}{2}\\|x-b\\|_{2}^{2}$）与一个凸、不可微项（$\\lambda\\|Kx\\|_{1}$）的和。这种结构确保了唯一原始解 $x^{\\star}$ 的存在。该问题是适定的、自洽的，并且所有术语都得到了精确定义。所提供的算子 $K$ 及其伴随算子 $K^{\\top}$ 的定义与离散网格上的标准有限差分算子一致。该问题是有效的。\n\n### 第三步：求解\n\n该优化问题可以写成：\n$$\n\\min_{x\\in\\mathbb{R}^{5}} f(x) + g(Kx)\n$$\n其中 $f(x) = \\frac{1}{2}\\|x-b\\|_{2}^{2}$ 且 $g(z) = \\lambda\\|z\\|_{1}$。$f(x)$ 和 $g(z)$ 都是凸函数。由于 $f(x)$ 是严格凸的，原始问题有唯一的解 $x^\\star$。\n\n$x^\\star$ 成为最小化点的充分必要条件是，零向量必须属于目标函数在 $x^\\star$ 处的次微分。使用次微分的求和法则和链式法则：\n$$\n0 \\in \\partial \\left( f(x^\\star) + g(Kx^\\star) \\right) = \\nabla f(x^\\star) + K^{\\top} \\partial g(Kx^\\star)\n$$\n$f(x)$ 的梯度是 $\\nabla f(x) = x-b$。\n$g(z) = \\lambda \\|z\\|_{1}$ 在点 $z_0=Kx^\\star$ 的次微分是集合 $\\partial g(z_0) = \\{ p \\in \\mathbb{R}^4 \\mid p_i \\in \\lambda \\partial|z_{0,i}| \\}$。绝对值函数的次梯度在 $t\\ne 0$ 时为 $\\text{sign}(t)$，在 $t=0$ 时为区间 $[-1, 1]$。\n\n最优性条件变为：\n$$\n0 \\in (x^\\star - b) + K^{\\top} \\left( \\lambda \\partial \\|Kx^\\star\\|_{1} \\right)\n$$\n这意味着存在一个对偶向量 $p^\\star \\in \\mathbb{R}^4$ 使得：\n$$\np^\\star \\in \\lambda \\partial \\|Kx^\\star\\|_{1} \\quad \\text{并且} \\quad x^\\star - b + K^{\\top} p^\\star = 0\n$$\n第一个条件 $p^\\star \\in \\lambda \\partial \\|Kx^\\star\\|_{1}$ 对每个分量 $i \\in \\{1,2,3,4\\}$ 等价于：\n1. $|p^\\star_i| \\leq \\lambda$。这就是箱式约束 $\\|p^\\star\\|_{\\infty} \\leq \\lambda$。\n2. 如果 $(Kx^\\star)_i \\neq 0$，则 $p^\\star_i = \\lambda \\cdot \\text{sign}((Kx^\\star)_i)$。\n这等价于互补松弛条件 $\\langle p^\\star, Kx^\\star \\rangle = \\lambda \\|Kx^\\star\\|_1$。\n\n第二个条件是原始-对偶关系：\n$$\nx^\\star = b - K^{\\top}p^\\star\n$$\n当 $\\lambda=1$ 时，对偶 $(x^\\star, p^\\star)$ 的完整最优性条件集是：\n(A) $x^\\star = b - K^{\\top}p^\\star$\n(B) $\\|p^\\star\\|_{\\infty} \\leq 1$\n(C) 对于 $i \\in \\{1,2,3,4\\}$，如果 $(Kx^\\star)_i \\neq 0$，则 $p^\\star_i = \\text{sign}((Kx^\\star)_i)$。\n\n现在，我们必须找到一个满足这些条件的对偶 $(x^\\star, p^\\star)$。我们可以将 (A) 代入 (C) 来求解 $p^\\star$。\n$$\nKx^\\star = K(b - K^{\\top}p^\\star) = Kb - (KK^{\\top})p^\\star\n$$\n我们来计算矩阵 $Kb$ 和 $KK^{\\top}$。\n$$\nKb = \\begin{pmatrix} b_2 - b_1 \\\\ b_3 - b_2 \\\\ b_4 - b_3 \\\\ b_5 - b_4 \\end{pmatrix} = \\begin{pmatrix} 2 - 1 \\\\ -1 - 2 \\\\ 0 - (-1) \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -3 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n矩阵 $L = KK^{\\top}$ 是一个 $4 \\times 4$ 的矩阵。其元素为 $L_{ij} = \\langle K_i, K_j \\rangle$，其中 $K_i$ 是 $K$ 的行。可以直接计算得到：\n$$\nKK^{\\top} = \\begin{pmatrix}\n2  &-1  &0  &0 \\\\\n-1  &2  &-1  &0 \\\\\n0  &-1  &2  &-1 \\\\\n0  &0  &-1  &2\n\\end{pmatrix}\n$$\n令 $z^\\star = Kx^\\star$。条件变为找到满足 $\\|p^\\star\\|_\\infty \\le 1$ 且对于 $i=1,2,3,4$ 满足以下条件的 $p^\\star$：\n- 如果 $z^\\star_i = (Kb - Lp^\\star)_i > 0$，则 $p^\\star_i = 1$。\n- 如果 $z^\\star_i = (Kb - Lp^\\star)_i < 0$，则 $p^\\star_i = -1$。\n- 如果 $z^\\star_i = (Kb - Lp^\\star)_i = 0$，则 $|p^\\star_i| \\le 1$。\n\n全变分正则化鼓励分段常数解。我们假设解 $x^\\star$ 有一个跳跃，位于 $x_2^\\star$ 和 $x_3^\\star$ 之间。这一点由 $b$ 的值在 $b_2=2$ 和 $b_3=-1$ 之间的大幅下降所暗示。\n这个假设意味着：\n$$\n(Kx^\\star)_1 = x_2^\\star-x_1^\\star = 0\n$$\n$$\n(Kx^\\star)_2 = x_3^\\star-x_2^\\star \\neq 0\n$$\n$$\n(Kx^\\star)_3 = x_4^\\star-x_3^\\star = 0\n$$\n$$\n(Kx^\\star)_4 = x_5^\\star-x_4^\\star = 0\n$$\n根据条件 (C)，这种结构对对偶变量 $p^\\star$ 意味着：\n- $z^\\star_1 = 0 \\implies (Kb - Lp^\\star)_1 = 0 \\implies 1 - (2p^\\star_1 - p^\\star_2) = 0$。\n- $z^\\star_2 \\ne 0 \\implies p^\\star_2 = \\text{sign}(z^\\star_2)$，所以 $p^\\star_2 = 1$ 或 $p^\\star_2 = -1$。\n- $z^\\star_3 = 0 \\implies (Kb - Lp^\\star)_3 = 0 \\implies 1 - (-p^\\star_2 + 2p^\\star_3 - p^\\star_4) = 0$。\n- $z^\\star_4 = 0 \\implies (Kb - Lp^\\star)_4 = 0 \\implies 0 - (-p^\\star_3 + 2p^\\star_4) = 0 \\implies p^\\star_3 = 2p^\\star_4$。\n\n由 $p^\\star_3 = 2p^\\star_4$ 和约束 $|p_i^\\star| \\le 1$ 可知，必须有 $|2p^\\star_4| \\le 1$，所以 $|p^\\star_4| \\le 1/2$。\n\n我们来检验 $p^\\star_2$ 的两种情况：\n情况1：假设 $p^\\star_2 = 1$。\n- 从 $1 - (2p^\\star_1 - p^\\star_2) = 0 \\implies 1 - (2p^\\star_1 - 1) = 0 \\implies 2p^\\star_1=2 \\implies p^\\star_1 = 1$。\n- 从 $1 - (-p^\\star_2 + 2p^\\star_3 - p^\\star_4) = 0 \\implies 1 - (-1 + 2p^\\star_3 - p^\\star_4) = 0 \\implies 2 - 2p^\\star_3 + p^\\star_4 = 0$。\n- 代入 $p^\\star_3 = 2p^\\star_4$：$2 - 2(2p^\\star_4) + p^\\star_4 = 0 \\implies 2 - 3p^\\star_4 = 0 \\implies p^\\star_4 = 2/3$。\n- 这违反了条件 $|p^\\star_4| \\le 1/2$。因此，$p^\\star_2=1$ 不是一个有效的解。\n\n情况2：假设 $p^\\star_2 = -1$。\n- 从 $1 - (2p^\\star_1 - p^\\star_2) = 0 \\implies 1 - (2p^\\star_1 - (-1)) = 0 \\implies 1 - 2p^\\star_1 - 1 = 0 \\implies p^\\star_1 = 0$。\n- 从 $1 - (-p^\\star_2 + 2p^\\star_3 - p^\\star_4) = 0 \\implies 1 - (-(-1) + 2p^\\star_3 - p^\\star_4) = 0 \\implies -2p^\\star_3 + p^\\star_4 = 0$。\n- 代入 $p^\\star_3 = 2p^\\star_4$：$-2(2p^\\star_4) + p^\\star_4 = 0 \\implies -3p^\\star_4 = 0 \\implies p^\\star_4 = 0$。\n- 从 $p^\\star_3 = 2p^\\star_4$ 可得 $p^\\star_3 = 0$。\n- 得到的对偶解候选为 $p^\\star = (0, -1, 0, 0)^{\\top}$。所有分量都满足 $|p_i^\\star| \\le 1$。\n\n我们必须验证这个解的一致性。我们假设 $z^\\star_2 \\ne 0$ 且 $p^\\star_2 = \\text{sign}(z^\\star_2) = -1$，这要求 $z^\\star_2 < 0$。我们来计算 $z^\\star_2$：\n$$\nz^\\star_2 = (Kb - Lp^\\star)_2 = -3 - (-p^\\star_1 + 2p^\\star_2 - p^\\star_3) = -3 - (0 + 2(-1) - 0) = -3 - (-2) = -1.\n$$\n由于 $z^\\star_2 = -1 < 0$，条件 $p^\\star_2 = -1$ 是一致的。\n所有最优性条件都得到满足。因此，我们找到了一个有效的对偶解 $p^\\star = (0, -1, 0, 0)^{\\top}$。\n\n现在我们使用关系 $x^\\star = b - K^{\\top}p^\\star$ 计算唯一的原始解 $x^\\star$。\n$$\nK^{\\top}p^\\star = K^{\\top} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -p^\\star_1 \\\\ p^\\star_1 - p^\\star_2 \\\\ p^\\star_2 - p^\\star_3 \\\\ p^\\star_3 - p^\\star_4 \\\\ p^\\star_4 \\end{pmatrix} = \\begin{pmatrix} -0 \\\\ 0 - (-1) \\\\ -1 - 0 \\\\ 0 - 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n所以，原始解是：\n$$\nx^\\star = b - K^{\\top}p^\\star = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n所以，$x^{\\star} = (1, 1, 0, 0, 0)^{\\top}$。\n\n最后，我们按要求验证原始-对偶关系。对于 $x^{\\star}=(1, 1, 0, 0, 0)^{\\top}$ 和 $p^{\\star}=(0, -1, 0, 0)^{\\top}$，我们已经计算了 $b - K^{\\top} p^{\\star}$ 并证明了它等于 $x^{\\star}$。验证完成。\n\n$x^{\\star}$ 的分量是 $(1, 1, 0, 0, 0)$，$p^{\\star}$ 的分量是 $(0, -1, 0, 0)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 1 & 0 & 0 & 0 & 0 & -1 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3466896"}, {"introduction": "掌握了基本原理后，我们将通过编程实践来探索不同正则化模型的性能差异。全变分 (TV) 正则化虽然强大，但其倾向于产生“阶梯效应”，即在斜坡区域产生平坦的片段。本练习将引导您实现一个基于原始对偶方法的算法，用于求解二阶广义全变分 (TGV) 正则化问题，并将其与一阶 TV 的结果进行对比[@problem_id:3478968]。通过量化分析斜坡偏差，您将亲眼见证 TGV 在保持线性结构方面的理论优势，这对于处理包含斜坡或平滑过渡的信号至关重要。", "problem": "要求您实现一个原始-对偶算法，使用二阶总广义变分 (TGV) 正则化对一个小的一维信号进行去噪，并将其斜坡偏差与一阶总变分 (TV) 去噪进行比较。此任务必须在以下基础之上完成：一个数据保真项和一个稀疏性诱导正则化项之和的凸优化、一维信号上的离散导数算子的定义，以及平方误差数据保真项的邻近算子。您不能假设任何非从这些基础直接推导出的结果，并且您应从凸分析、对偶性和邻近分裂的基本原理出发，推导您的算法更新步骤。\n\n考虑一个有 $n$ 个采样点的信号，表示为向量 $u \\in \\mathbb{R}^n$。令 $f \\in \\mathbb{R}^n$ 表示观测到的含噪数据。离散前向差分算子 $D : \\mathbb{R}^n \\to \\mathbb{R}^n$ 使用齐次诺伊曼边界条件的单边差分定义。数据保真项为平方误差 $F(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$。一阶总变分 (TV) 正则化项是 $D u$ 的各向异性 $\\ell_1$ 范数。二阶总广义变分 (TGV$^2$) 引入一个辅助场 $w \\in \\mathbb{R}^n$，并以正常数作为权重，惩罚 $D u - w$ 和 $D w$ 的各向异性 $\\ell_1$ 范数。您的实现必须推导并使用基于这些定义的原始-对偶方案，来计算 TV 和 TGV$^2$ 正则化的去噪解。\n\n将斜坡偏差定义为重建信号的估计斜率与底层无噪声斜坡的真实斜率之间的差异。对于每个重建结果，通过对重建信号的所有样本进行最小二乘线性回归来估计斜率。分别为 TV 和 TGV$^2$ 重建计算绝对斜坡偏差，并报告这些绝对偏差之间的差异，定义为 $\\Delta = \\lvert b_{\\mathrm{TV}} \\rvert - \\lvert b_{\\mathrm{TGV}} \\rvert$，其中 $b_{\\mathrm{TV}}$ 和 $b_{\\mathrm{TGV}}$ 是相对于真实斜率的带符号的斜率误差。$\\Delta$ 的正值表示 TGV$^2$ 比 TV 表现出更低的绝对偏差。\n\n在一个独立的自包含程序中实现这些算法，为以下测试套件生成结果。在所有情况下，都使用固定的算法参数：样本数 $n = 32$，原始步长 $\\tau = 0.1$，对偶步长 $\\sigma = 0.1$，超松弛参数 $\\theta = 1$，迭代次数 $N_{\\mathrm{iter}} = 1000$，TV 正则化权重 $\\lambda_{\\mathrm{TV}} = 0.12$，TGV$^2$ 权重 $\\alpha_1 = 0.8$ 和 $\\alpha_2 = 1.6$。为了噪声的复现性，使用固定的随机种子，设为 $42$。所有量均为无量纲量。不涉及角度。没有物理单位。\n\n测试套件：\n- 情况 1 (常规情况)：一个线性斜坡信号 $u^\\star[i] = i/(n-1)$，其中 $i \\in \\{0, \\dots, n-1\\}$，在加性独立同分布的零均值、标准差为 $\\sigma_{\\mathrm{noise}} = 0.05$ 的高斯噪声下观测。按规定计算 $\\Delta$。\n- 情况 2 (边界条件覆盖)：一个常数信号 $u^\\star[i] = 0.5$，在加性独立同分布的零均值、标准差为 $\\sigma_{\\mathrm{noise}} = 0.05$ 的高斯噪声下观测。按规定计算 $\\Delta$。\n- 情况 3 (边缘情况)：一个无噪声的线性斜坡信号 $u^\\star[i] = i/(n-1)$，其中 $\\sigma_{\\mathrm{noise}} = 0$。按规定计算 $\\Delta$。\n\n您的程序应产生单行输出，其中包含一个逗号分隔的浮点数列表形式的结果，并用方括号括起来，顺序为 $[\\Delta_{\\text{情况 }1}, \\Delta_{\\text{情况 }2}, \\Delta_{\\text{情况 }3}]$，每个数字四舍五入到六位小数（例如，$[0.123456,0.000001,-0.045000]$）。", "solution": "该问题要求实现并比较一阶总变分 (TV) 和二阶总广义变分 (TGV) 正则化在一维信号去噪中的应用。比较的依据是斜坡偏差，它量化了每种方法保持线性斜率的能力。解决方案是使用原始-对偶算法从凸优化的基本原理推导得出。\n\n去噪问题的一般形式是找到一个信号 $u \\in \\mathbb{R}^n$，使其最小化一个复合目标函数：\n$$ u^* = \\arg\\min_{u \\in \\mathbb{R}^n} \\left( F(u) + R(u) \\right) $$\n其中 $F(u)$ 是一个数据保真项，$R(u)$ 是一个正则化项。问题指定数据保真项为与观测到的含噪信号 $f \\in \\mathbb{R}^n$ 的平方 $\\ell_2$ 范数距离，由 $F(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$ 给出。\n\n该问题使用原始-对偶混合梯度 (PDHG) 方法求解，该方法也称为 Chambolle-Pock 算法，非常适合求解以下形式的问题：\n$$ \\min_{x} \\mathcal{F}(x) + \\mathcal{G}(Kx) $$\n其中 $\\mathcal{F}$ 和 $\\mathcal{G}$ 是正常、凸、下半连续函数，$K$ 是一个连续线性算子。该问题的迭代方案定义为：\n$$\n\\begin{cases}\ny^{k+1} = \\mathrm{prox}_{\\sigma \\mathcal{G}^*}(y^k + \\sigma K \\bar{x}^k) \\\\\nx^{k+1} = \\mathrm{prox}_{\\tau \\mathcal{F}}(x^k - \\tau K^* y^{k+1}) \\\\\n\\bar{x}^{k+1} = x^{k+1} + \\theta(x^{k+1} - x^k)\n\\end{cases}\n$$\n这里，$x$ 是原始变量，$y$ 是对偶变量，$K^*$ 是 $K$ 的伴随算子，$\\mathcal{G}^*$ 是 $\\mathcal{G}$ 的凸共轭，而 $\\mathrm{prox}_{\\gamma H}(z) = \\arg\\min_v ( H(v) + \\frac{1}{2\\gamma} \\lVert v-z \\rVert_2^2 )$ 是邻近算子。算法参数包括原始步长 $\\tau > 0$，对偶步长 $\\sigma > 0$，以及超松弛参数 $\\theta \\in [0, 1]$。步长必须满足 $\\tau \\sigma \\lVert K \\rVert^2 < 1$。问题指定 $\\tau=0.1$，$\\sigma=0.1$ 和 $\\theta=1$。\n\n带齐次诺伊曼边界条件的离散前向差分算子 $D: \\mathbb{R}^n \\to \\mathbb{R}^n$ 定义为 $(Du)_i = u_{i+1} - u_i$ (对于 $i \\in \\{0, \\dots, n-2\\}$) 和 $(Du)_{n-1} = 0$。其伴随算子 $D^*: \\mathbb{R}^n \\to \\mathbb{R}^n$ 必须对所有 $u, p \\in \\mathbb{R}^n$ 满足 $\\langle Du, p \\rangle = \\langle u, D^*p \\rangle$。由此可得 $(D^*p)_0 = -p_0$，$(D^*p)_i = p_{i-1} - p_i$ (对于 $i \\in \\{1, \\dots, n-2\\}$) 和 $(D^*p)_{n-1} = p_{n-2}$。\n\n对于 TV 和 TGV 模型，数据保真项 $\\mathcal{F}$ 及其邻近算子是相同的。令 $\\mathcal{F}(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$。其邻近算子为：\n$$ \\mathrm{prox}_{\\tau \\mathcal{F}}(v) = \\arg\\min_u \\left( \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert u - v \\rVert_2^2 \\right) = \\frac{v + \\tau f}{1 + \\tau} $$\n\n**一阶总变分 (TV) 去噪**\nTV 正则化问题是：\n$$ \\min_{u \\in \\mathbb{R}^n} \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\lambda_{\\mathrm{TV}} \\lVert Du \\rVert_1 $$\n这符合 PDHG 框架，具体赋值如下：\n- 原始变量 $x = u \\in \\mathbb{R}^n$。\n- $\\mathcal{F}(u) = \\frac{1}{2}\\lVert u - f \\rVert_2^2$。\n- $\\mathcal{G}(y) = \\lambda_{\\mathrm{TV}} \\lVert y \\rVert_1$，其中 $y \\in \\mathbb{R}^n$ 是一个通用变量。\n- 线性算子 $K = D$。\n\n$\\mathcal{G}$ 的凸共轭是 $\\mathcal{G}^*(p) = I_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(p)$，即半径为 $\\lambda_{\\mathrm{TV}}$ 的 $\\ell_\\infty$ 球的指示函数。$\\mathcal{G}^*$ 的邻近算子是到这个球上的投影：\n$$ \\mathrm{prox}_{\\sigma \\mathcal{G}^*}(v) = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(v) $$\n该投影按元素计算为 $(v_i)_{\\text{proj}} = v_i / \\max(1, |v_i|/\\lambda_{\\mathrm{TV}})$。\n完整的 TV 去噪算法如下：\n1. 初始化 $u^0$，$\\bar{u}^0 = u^0$，以及对偶变量 $y^0$。\n2. 对于 $k=0, 1, \\dots, N_{\\mathrm{iter}}-1$：\n   $y^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\lambda_{\\mathrm{TV}}}(y^k + \\sigma D \\bar{u}^k)$\n   $u^{k+1} = \\frac{(u^k - \\tau D^* y^{k+1}) + \\tau f}{1 + \\tau}$\n   $\\bar{u}^{k+1} = u^{k+1} + \\theta(u^{k+1} - u^k)$\n\n**二阶总广义变分 (TGV$^2$) 去噪**\nTGV$^2$ 正则化问题是关于信号 $u$ 和一个辅助场 $w \\in \\mathbb{R}^n$ 的联合最小化问题：\n$$ \\min_{u, w} \\frac{1}{2}\\lVert u - f \\rVert_2^2 + \\alpha_1 \\lVert Du - w \\rVert_1 + \\alpha_2 \\lVert Dw \\rVert_1 $$\n为了将其纳入 PDHG 框架，我们定义一个复合原始变量 $x = (u, w) \\in \\mathbb{R}^{2n}$。\n- $\\mathcal{F}(u, w) = \\frac{1}{2}\\lVert u - f \\rVert_2^2 + 0 \\cdot \\lVert w \\rVert_2^2$。\n- $\\mathcal{G}(z_1, z_2) = \\alpha_1 \\lVert z_1 \\rVert_1 + \\alpha_2 \\lVert z_2 \\rVert_1$，对于 $(z_1, z_2) \\in \\mathbb{R}^{2n}$。\n- 线性算子 $K(u,w) = \\begin{pmatrix} Du - w \\\\ Dw \\end{pmatrix} = \\begin{pmatrix} D & -I \\\\ 0 & D \\end{pmatrix} \\begin{pmatrix} u \\\\ w \\end{pmatrix}$。\n\n伴随算子为 $K^* = \\begin{pmatrix} D^* & 0 \\\\ -I & D^* \\end{pmatrix}$。对偶变量是 $y=(p,q) \\in \\mathbb{R}^{2n}$。\n$\\mathcal{F}$ 的邻近算子对 $u$ 和 $w$ 是可分离的：\n$$ \\mathrm{prox}_{\\tau\\mathcal{F}}((v_u, v_w)) = \\left(\\frac{v_u + \\tau f}{1+\\tau}, v_w\\right) $$\n共轭函数 $\\mathcal{G}^*$ 是集合 $\\{ (p,q) \\mid \\lVert p \\rVert_\\infty \\le \\alpha_1, \\lVert q \\rVert_\\infty \\le \\alpha_2 \\}$ 的指示函数。其邻近算子是到这个集合上的投影，该投影对 $p$ 和 $q$ 是可分离的。\nTGV$^2$ 去噪算法如下：\n1. 初始化 $u^0, w^0$，$\\bar{u}^0=u^0, \\bar{w}^0=w^0$，以及对偶变量 $p^0, q^0$。\n2. 对于 $k=0, 1, \\dots, N_{\\mathrm{iter}}-1$：\n   $p^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\alpha_1}(p^k + \\sigma(D\\bar{u}^k - \\bar{w}^k))$\n   $q^{k+1} = \\mathrm{proj}_{\\lVert \\cdot \\rVert_\\infty \\le \\alpha_2}(q^k + \\sigma D\\bar{w}^k)$\n   $u_{v} = u^k - \\tau D^*p^{k+1}$\n   $w_{v} = w^k - \\tau(-p^{k+1} + D^*q^{k+1})$\n   $u^{k+1} = \\frac{u_v + \\tau f}{1+\\tau}$\n   $w^{k+1} = w_v$\n   $\\bar{u}^{k+1} = u^{k+1} + \\theta(u^{k+1} - u^k)$\n   $\\bar{w}^{k+1} = w^{k+1} + \\theta(w^{k+1} - w^k)$\n\n**斜坡偏差计算**\n对于每个重建信号 $u_{\\text{recon}}$，通过寻找最小化 $\\sum_{i=0}^{n-1} ( (m \\cdot i + c) - u_{\\text{recon}}[i] )^2$ 的系数 $m$ 来估计斜率，此过程通过最小二乘回归完成。带符号的斜率误差，或称偏差，为 $b = m - m^\\star$，其中 $m^\\star$ 是真实底层信号的斜率。对于线性斜坡 $u^\\star[i] = i/(n-1)$，$m^\\star = 1/(n-1)$。对于常数信号，$m^\\star = 0$。最终的度量指标是绝对偏差的差值：$\\Delta = |b_{\\mathrm{TV}}| - |b_{\\mathrm{TGV}}|$。$\\Delta$ 为正值表示 TGV$^2$ 的绝对斜坡偏差更小，因此能更好地保留线性结构，这是 TGV$^2$ 相较于 TV 的一个已知理论优势。实现部分将对指定的测试套件执行这些推导出的算法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the denoising experiments and compute the ramp bias difference.\n    Implements primal-dual algorithms for TV and TGV denoising from first principles.\n    \"\"\"\n    # --- Global Problem Parameters ---\n    N = 32\n    TAU = 0.1\n    SIGMA = 0.1\n    THETA = 1.0\n    N_ITER = 1000\n    LAMBDA_TV = 0.12\n    ALPHA1 = 0.8\n    ALPHA2 = 1.6\n    SEED = 42\n\n    # --- Operator Definitions ---\n    def D_op(u):\n        \"\"\"Discrete forward difference operator with homogeneous Neumann boundary.\"\"\"\n        n = len(u)\n        res = np.zeros(n, dtype=np.float64)\n        res[:-1] = u[1:] - u[:-1]\n        # res[-1] is 0 by initialization\n        return res\n\n    def D_adj_op(p):\n        \"\"\"Adjoint of the discrete forward difference operator.\"\"\"\n        n = len(p)\n        res = np.zeros(n, dtype=np.float64)\n        res[0] = -p[0]\n        if n > 2:\n            res[1:-1] = p[:-2] - p[1:-1]\n        if n > 1:\n            res[-1] = p[-2]\n        return res\n\n    # --- Denoising Algorithms ---\n    def denoise_tv(f):\n        \"\"\"\n        TV denoising using a primal-dual algorithm.\n        min_u 0.5 * ||u - f||^2 + LAMBDA_TV * ||Du||_1\n        \"\"\"\n        u = np.copy(f)\n        u_bar = np.copy(u)\n        y = np.zeros_like(f)\n\n        for _ in range(N_ITER):\n            u_old = np.copy(u)\n            \n            # Dual update using prox of the conjugate\n            y_update_arg = y + SIGMA * D_op(u_bar)\n            denom = np.maximum(1.0, np.abs(y_update_arg) / LAMBDA_TV)\n            y = y_update_arg / denom\n\n            # Primal update using prox of the data fidelity term\n            prox_arg = u_old - TAU * D_adj_op(y)\n            u = (prox_arg + TAU * f) / (1.0 + TAU)\n            \n            # Over-relaxation step\n            u_bar = u + THETA * (u - u_old)\n        \n        return u\n\n    def denoise_tgv(f):\n        \"\"\"\n        TGV denoising using a primal-dual algorithm.\n        min_{u,w} 0.5*||u-f||^2 + ALPHA1*||Du-w||_1 + ALPHA2*||Dw||_1\n        \"\"\"\n        u = np.copy(f)\n        u_bar = np.copy(u)\n        w = np.zeros_like(f)\n        w_bar = np.zeros_like(f)\n        p = np.zeros_like(f)\n        q = np.zeros_like(f)\n        \n        for _ in range(N_ITER):\n            u_old = np.copy(u)\n            w_old = np.copy(w)\n            \n            # Dual update for (p, q)\n            v_p = p + SIGMA * (D_op(u_bar) - w_bar)\n            v_q = q + SIGMA * D_op(w_bar)\n            \n            p = v_p / np.maximum(1.0, np.abs(v_p) / ALPHA1)\n            q = v_q / np.maximum(1.0, np.abs(q) / ALPHA2)\n            \n            # Primal update for (u, w)\n            v_u = u_old - TAU * D_adj_op(p)\n            v_w = w_old - TAU * (-p + D_adj_op(q))\n            \n            u = (v_u + TAU * f) / (1.0 + TAU)\n            w = v_w\n            \n            # Over-relaxation step\n            u_bar = u + THETA * (u - u_old)\n            w_bar = w + THETA * (w - w_old)\n            \n        return u\n\n    # --- Bias Calculation ---\n    def get_slope(u):\n        \"\"\"Estimate the slope of a signal using linear least squares.\"\"\"\n        n = len(u)\n        x = np.arange(n, dtype=np.float64)\n        A = np.vstack([x, np.ones(n)]).T\n        m, _ = np.linalg.lstsq(A, u, rcond=None)[0]\n        return m\n\n    # --- Test Suite Execution ---\n    rng = np.random.default_rng(SEED)\n    test_specs = [\n        {'type': 'ramp', 'noise_std': 0.05},\n        {'type': 'const', 'noise_std': 0.05},\n        {'type': 'ramp', 'noise_std': 0.0},\n    ]\n    \n    results = []\n    for spec in test_specs:\n        if spec['type'] == 'ramp':\n            u_star = np.arange(N, dtype=np.float64) / (N - 1)\n            true_slope = 1.0 / (N - 1)\n        else: # 'const'\n            u_star = np.full(N, 0.5, dtype=np.float64)\n            true_slope = 0.0\n\n        if spec['noise_std'] > 0:\n            noise = rng.normal(0, spec['noise_std'], N)\n            f = u_star + noise\n        else:\n            f = u_star\n            \n        # Perform denoising\n        u_tv = denoise_tv(f)\n        u_tgv = denoise_tgv(f)\n        \n        # Calculate slope errors (bias)\n        slope_tv = get_slope(u_tv)\n        slope_tgv = get_slope(u_tgv)\n        \n        bias_tv = slope_tv - true_slope\n        bias_tgv = slope_tgv - true_slope\n        \n        # Calculate delta of absolute biases\n        delta = np.abs(bias_tv) - np.abs(bias_tgv)\n        results.append(delta)\n\n    # --- Format and Print Output a`ccording to problem specification ---\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3478968"}, {"introduction": "在逆问题的实际应用中，正则化参数的选择以及不同问题形式之间的等价性是核心挑战。最后一个练习是一项综合性的数值研究，旨在探索约束形式和拉格朗日形式的全变分问题之间的深刻联系[@problem_id:3466854]。您将为一个约束优化问题实现一个基于 Morozov 差异原理的早停策略，并将其结果与通过“神谕”参数选择的无约束问题的解进行比较。这个练习不仅能锻炼您的编程和算法实现能力，还将加深您对参数选择策略以及KKT条件在算法设计中实际应用的理解。", "problem": "针对在Morozov差异原则下应用于一维各向异性全变分（TV）约束重构的原始-对偶混合梯度（PDHG）方法，实现一个关于提前停止的完整数值研究，并将其与无约束TV正则化最小二乘的神谕参数选择进行比较。目标是从第一性原理出发，推导、实现并测试：对于约束问题，提前停止的PDHG算法何时能匹配一个通过神谕正则化参数选择以满足相同差异的拉格朗日形式。\n\n考虑以下设置。令$A \\in \\mathbb{R}^{m \\times n}$为一个线性传感算子，令$b \\in \\mathbb{R}^{m}$为观测数据，令$\\epsilon \\in \\mathbb{R}_{+}$为一个噪声水平。对于$x \\in \\mathbb{R}^{n}$，将一维各向异性全变分（TV）半范数定义为$TV(x) = \\sum_{i=1}^{n-1} |x_{i+1} - x_{i}|$，这对应于前向差分的$\\ell_{1}$范数。Morozov差异原则要求残差范数满足$\\|A x - b\\|_{2} \\approx \\epsilon$。\n\n您必须从以下基本概念出发。\n- 凸约束TV最小化：最小化$TV(x)$，约束条件为$\\|A x - b\\|_{2} \\le \\epsilon$。\n- 拉格朗日TV正则化最小二乘：对于某个$\\lambda \\ge 0$，最小化$\\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda\\, TV(x)$。\n- 凸对偶和Karush–Kuhn–Tucker (KKT) 条件对于约束问题，以及在Slater条件下连接约束和无约束问题的拉格朗日乘子$\\lambda$的存在性。\n- PDHG方法作为一种用于解决形如$\\min_{x} g(x) + f(K x)$的最小-最大鞍点问题的一阶原始-对偶算法，其步长受$\\tau \\sigma \\|K\\|^{2} < 1$约束。\n- 近端映射和凸共轭：对于$f(z) = \\frac{1}{2}\\|z - b\\|_{2}^{2}$，$f^{*}(p) = \\frac{1}{2}\\|p\\|_{2}^{2} + \\langle p, b\\rangle$，其近端算子为$\\operatorname{prox}_{\\sigma f^{*}}(v) = \\frac{v - \\sigma b}{1 + \\sigma}$；对于$h(u) = \\|u\\|_{1}$，$h^{*}(q) = \\iota_{\\|q\\|_{\\infty} \\le 1}(q)$，其近端算子由到$\\ell_{\\infty}$单位球上的投影给出；对于$f(z) = \\iota_{\\|z - b\\|_{2} \\le \\epsilon}(z)$，$f^{*}(p) = \\langle p, b\\rangle + \\epsilon \\|p\\|_{2}$，其近端算子为$\\operatorname{prox}_{\\sigma f^{*}}(v) = \\operatorname{shrink}_{\\ell_{2}}(v - \\sigma b, \\sigma \\epsilon)$，其中$\\operatorname{shrink}_{\\ell_{2}}(w, t) = \\max\\{0, 1 - t/\\|w\\|_{2}\\}\\, w$。\n\n您的任务是：\n- 推导约束问题$\\min_{x} TV(x)$（约束条件为$\\|A x - b\\|_{2} \\le \\epsilon$）的PDHG更新。具体方法是将其写成$\\min_{x} g(x) + f(K x)$的形式，其中$g(x) = 0$，$K x = \\begin{bmatrix} D x \\\\ A x \\end{bmatrix}$（$D$是前向差分算子），以及$f(u, v) = \\|u\\|_{1} + \\iota_{\\|v - b\\|_{2} \\le \\epsilon}(v)$。使用$f^{*}$的近端映射提供显式的对偶更新，并使用$K^{\\top}$提供原始更新。\n- 指定一个由原始-对偶最优性残差和差异原则引导的可计算的提前停止规则。该规则必须检查一个差异接近条件$|\\|A x^{k} - b\\|_{2} - \\epsilon| \\le \\rho\\, \\epsilon$（对于选定的容差$\\rho \\in (0, 1)$）和一个原始-对偶平稳性残差$\\|K^{\\top} y^{k}\\|_{2} \\le \\eta$（对于选定的$\\eta \\in \\mathbb{R}_{+}$），其中$y^{k}$堆叠了对偶变量。解释为什么这些条件近似于KKT可行性和互补松弛性。\n- 推导带有$f(Ax) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$和$h(Dx) = \\lambda \\|D x\\|_{1}$的无约束拉格朗日问题的PDHG更新，使用与$K$相同的乘积空间分裂和可分离的对偶更新。然后，对$\\lambda$实现一个二分法，以找到一个神谕参数$\\lambda_{\\star}$，使得解$x_{\\lambda_{\\star}}$在规定的相对容差内满足$\\|A x_{\\lambda_{\\star}} - b\\|_{2} \\approx \\epsilon$。\n- 实现一个使用幂迭代法对$K^{\\top} K = D^{\\top} D + A^{\\top} A$估计$\\|K\\|$的估计器，并选择满足$\\tau \\sigma \\|K\\|^{2} < 1$的PDHG步长$\\tau$和$\\sigma$。\n- 对于下面描述的每个测试用例，生成一个分段常数基准真相$x_{0}$，模拟数据$b = A x_{0} + \\xi$（其中$\\xi \\sim \\mathcal{N}(0, \\sigma^{2} I)$），设置$\\epsilon = \\|\\xi\\|_{2}$，使用提前停止规则运行约束PDHG以获得$x_{\\mathrm{ES}}$，通过二分法计算神谕参数$\\lambda_{\\star}$并求解无约束PDHG以获得$x_{\\lambda_{\\star}}$，然后返回一个布尔值。如果以下两个条件都成立，则该布尔值为真：\n  - 残差匹配：$|\\|A x_{\\mathrm{ES}} - b\\|_{2} - \\epsilon| \\le \\alpha\\, \\epsilon$，\n  - 解匹配：$\\|x_{\\mathrm{ES}} - x_{\\lambda_{\\star}}\\|_{2} \\le \\beta\\, \\|x_{\\lambda_{\\star}}\\|_{2}$，\n  其中$\\alpha \\in (0, 1)$和$\\beta \\in (0, 1)$是给定的容差。\n- 您的程序必须仅使用规定的运行时环境从头开始实现所有内容，并产生指定的输出。\n\n使用以下测试套件，它指定了不同的算子条件、噪声水平和停止预算。在所有情况下，不出现角度，也不需要物理单位。所有阈值都必须被视为无量纲数。\n\n测试套件（每个元组描述$(n, m, \\text{seed}, \\sigma, \\text{ill}, \\text{max\\_iter\\_ES}, \\rho, \\eta, \\alpha, \\beta)$）：\n- 情况1：$(64, 40, 0, 0.01, \\text{False}, 3000, 0.05, 10^{-3}, 0.05, 0.10)$。\n- 情况2：$(64, 40, 1, 10^{-4}, \\text{False}, 3000, 0.05, 10^{-3}, 0.05, 0.10)$。\n- 情况3：$(64, 40, 2, 0.05, \\text{False}, 3000, 0.05, 10^{-3}, 0.05, 0.10)$。\n- 情况4：$(64, 40, 3, 0.01, \\text{True}, 50, 0.05, 10^{-3}, 0.05, 0.10)$，其中标志$\\text{ill} = \\text{True}$表示通过在定义域上后乘一个条件差的方阵使$A$变得病态。\n\n需要遵守的实现细节：\n- 将$D$构造为前向差分算子，对于$i \\in \\{1,\\dots,n-1\\}$，$(D x)_{i} = x_{i+1} - x_{i}$，其中$(D x) \\in \\mathbb{R}^{n-1}$。其伴随算子$D^{\\top}$由$(D^{\\top} q)_{1} = -q_{1}$，对于$i \\in \\{2,\\dots,n-1\\}$，$(D^{\\top} q)_{i} = q_{i-1} - q_{i}$，以及$(D^{\\top} q)_{n} = q_{n-1}$给出。\n- 对于约束问题，使用对偶更新，其中包含投影$q \\leftarrow \\operatorname{proj}_{\\|\\cdot\\|_{\\infty} \\le 1}(q)$和$p \\leftarrow \\operatorname{shrink}_{\\ell_{2}}(p - \\sigma b, \\sigma \\epsilon)$，这里$q$是梯度对偶变量，$p$是数据拟合对偶变量。\n- 对于带参数$\\lambda$的无约束问题，使用对偶更新$q \\leftarrow \\operatorname{proj}_{\\|\\cdot\\|_{\\infty} \\le \\lambda}(q)$和$p \\leftarrow \\frac{p - \\sigma b}{1 + \\sigma}$。\n- 选择$\\tau = \\sigma = \\frac{0.99}{\\|K\\|}$和$\\theta = 1$，其中$\\|K\\|$通过对$K^{\\top} K$进行幂迭代来估计。\n- 对$\\lambda$使用二分法，更新区间以确保上界处的残差至少为$\\epsilon$，并对每个$\\lambda$值使用带有热启动的固定预算PDHG求解来找到$\\lambda_{\\star}$，使得$|\\|A x_{\\lambda_{\\star}} - b\\|_{2} - \\epsilon| \\le \\alpha\\, \\epsilon$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含四个测试用例的结果，格式为方括号括起来的逗号分隔列表（例如，$[r_{1},r_{2},r_{3},r_{4}]$），其中每个$r_{i}$是上述匹配测试返回的布尔值。\n\n您的实现必须完全自包含，并且能够按原样运行，仅使用允许的库，不得需要任何用户输入、外部文件或网络访问。最终输出必须严格遵循指定格式的一行。", "solution": "目标是进行一项数值研究，比较用于约束全变分（TV）最小化问题的提前停止原始-对偶算法与相应无约束拉格朗日形式下使用神谕选择的正则化参数的解。我们将从第一性原理推导必要的算法，并实现一个测试来验证在Morozov差异原则下，它们各自的解何时能够吻合。\n\n设线性逆问题建模为$b = Ax_0 + \\xi$，其中$A \\in \\mathbb{R}^{m \\times n}$是传感算子，$x_0 \\in \\mathbb{R}^n$是未知的真实信号（假设为分段常数），$b \\in \\mathbb{R}^m$是测量数据，$\\xi \\in \\mathbb{R}^m$是加性噪声，其能量已知为$\\|\\xi\\|_2 = \\epsilon$。一维各向异性TV半范数为$TV(x) = \\|Dx\\|_1$，其中$D \\in \\mathbb{R}^{(n-1) \\times n}$是前向差分算子。\n\n我们比较两种从$b$重构$x$的标准形式：\n1.  **约束形式（基追踪降噪风格）**：$\\min_{x \\in \\mathbb{R}^n} TV(x)$，约束条件为$\\|Ax - b\\|_2 \\le \\epsilon$。\n2.  **无约束形式（拉格朗日）**：$\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda TV(x)$，其中正则化参数$\\lambda \\ge 0$。\n\n在适当的条件下（例如，Slater条件），存在一个拉格朗日乘子$\\lambda_\\star \\ge 0$，使得当$\\lambda=\\lambda_\\star$时，无约束问题的解也是约束问题的解。Morozov差异原则建议应选择$\\lambda_\\star$，使得解$x_{\\lambda_\\star}$满足$\\|Ax_{\\lambda_\\star} - b\\|_2 = \\epsilon$。本研究旨在探究一个用于约束问题的提前停止迭代算法是否能产生一个与$x_{\\lambda_\\star}$相似的解。我们对这两个问题都采用原始-对偶混合梯度（PDHG）方法。\n\n**1. 约束问题的PDHG算法**\n\n约束问题为$\\min_{x} \\|Dx\\|_1$，约束条件为$\\|Ax-b\\|_2 \\le \\epsilon$。这可以写成PDHG旨在解决的通用鞍点形式$\\min_x \\max_y \\langle Kx, y \\rangle + G(x) - F^*(y)$。\n令$G(x) = 0$。我们定义一个复合线性算子$K: \\mathbb{R}^n \\to \\mathbb{R}^{n-1} \\times \\mathbb{R}^m$为$Kx = \\begin{bmatrix} Dx \\\\ Ax \\end{bmatrix}$。目标函数可以表示为$F(Kx)$，其中$F: \\mathbb{R}^{n-1} \\times \\mathbb{R}^m \\to \\mathbb{R} \\cup \\{+\\infty\\}$由$F(u, v) = \\|u\\|_1 + \\iota_{\\|v-b\\|_2 \\le \\epsilon}(v)$给出。这里，$(u,v)$是$K$值域中的一个点，$\\iota_C$是集合$C$的指示函数。\n函数$F$是可分的：$F(u,v) = F_1(u) + F_2(v)$，其中$F_1(u) = \\|u\\|_1$，$F_2(v) = \\iota_{\\|v-b\\|_2 \\le \\epsilon}(v)$。\n对偶变量$y$相应地分裂为$y = \\begin{bmatrix} q \\\\ p \\end{bmatrix}$，其中$q \\in \\mathbb{R}^{n-1}$，$p \\in \\mathbb{R}^m$。\n凸共轭$F^*(y)$也是可分的：$F^*(q,p) = F_1^*(q) + F_2^*(p)$。\n共轭函数为：\n-   $F_1^*(q) = \\sup_u (\\langle q, u \\rangle - \\|u\\|_1) = \\iota_{\\|q\\|_\\infty \\le 1}(q)$。\n-   $F_2^*(p) = \\sup_v (\\langle p, v \\rangle - \\iota_{\\|v-b\\|_2 \\le \\epsilon}(v)) = \\sup_{\\|w\\|_2 \\le \\epsilon} \\langle p, w+b \\rangle = \\langle p, b \\rangle + \\epsilon\\|p\\|_2$。\n\n外推参数$\\theta=1$的PDHG算法迭代如下：\n$y^{k+1} = \\operatorname{prox}_{\\sigma F^*} (y^k + \\sigma K \\bar{x}^k)$\n$x^{k+1} = \\operatorname{prox}_{\\tau G} (x^k - \\tau K^\\top y^{k+1})$\n$\\bar{x}^{k+1} = 2x^{k+1} - x^k$\n\n由于$G(x)=0$，其近端算子是恒等映射，$\\operatorname{prox}_{\\tau G}(z)=z$。原始更新是在对偶变量上的一个简单梯度下降步：$x^{k+1} = x^k - \\tau K^\\top y^{k+1}$。伴随算子$K^\\top$由$K^\\top y = D^\\top q + A^\\top p$给出。\n由于$F^*$的可分性，$y$的对偶更新是可分的：\n-   $q^{k+1} = \\operatorname{prox}_{\\sigma F_1^*}(q^k + \\sigma D\\bar{x}^k) = \\operatorname{proj}_{\\|\\cdot\\|_\\infty \\le 1}(q^k + \\sigma D\\bar{x}^k)$。\n-   $p^{k+1} = \\operatorname{prox}_{\\sigma F_2^*}(p^k + \\sigma A\\bar{x}^k)$。$\\sigma F_2^*$的近端算子是$\\operatorname{prox}_{\\sigma F_2^*}(v) = \\operatorname{argmin}_p \\{\\frac{1}{2}\\|p-v\\|_2^2 + \\sigma(\\langle p, b \\rangle + \\epsilon \\|p\\|_2)\\}$。解为$p = \\operatorname{prox}_{\\sigma\\epsilon\\|\\cdot\\|_2}(v-\\sigma b) = \\operatorname{shrink}_{\\ell_2}(v-\\sigma b, \\sigma\\epsilon)$。\n代入$v=p^k + \\sigma A\\bar{x}^k$，我们得到$p^{k+1} = \\operatorname{shrink}_{\\ell_2}(p^k + \\sigma(A\\bar{x}^k - b), \\sigma\\epsilon)$。\n\n完整的更新步骤是：\n1.  $q^{k+1} = \\operatorname{proj}_{\\|\\cdot\\|_\\infty \\le 1}(q^k + \\sigma D\\bar{x}^k)$\n2.  $p^{k+1} = \\operatorname{shrink}_{\\ell_2}(p^k + \\sigma(A\\bar{x}^k-b), \\sigma\\epsilon)$\n3.  $x^{k+1} = x^k - \\tau (D^\\top q^{k+1} + A^\\top p^{k+1})$\n4.  $\\bar{x}^{k+1} = 2 x^{k+1} - x^k$\n\n**2. 提前停止规则与KKT条件**\n\n约束问题的Karush-Kuhn-Tucker (KKT) 条件如下：\n1.  **平稳性**：$0 \\in \\partial(\\|Dx^*\\|_1) + \\nu^* A^\\top(Ax^*-b)$，这里我们假设解位于边界上，即$\\|Ax^*-b\\|_2=\\epsilon$。\n2.  **原始可行性**：$\\|Ax^*-b\\|_2 \\le \\epsilon$。\n3.  **对偶可行性**：$\\nu^* \\ge 0$。\n4.  **互补松弛性**：$\\nu^* (\\|Ax^*-b\\|_2 - \\epsilon) = 0$。\n\n鞍点问题的最优性条件是$K^\\top y^* = 0$（即$D^\\top q^* + A^\\top p^* = 0$）和$Kx^* \\in \\partial F^*(y^*)$。这些条件等价于KKT条件。\n所提出的提前停止规则包含两部分：\n-   $|\\|A x^{k} - b\\|_{2} - \\epsilon| \\le \\rho\\, \\epsilon$：此条件强制迭代$x^k$接近满足差异原则$\\|Ax-b\\|_2 = \\epsilon$。这对应于确保原始可行性和激活的互补松弛性条件近似满足。\n-   $\\|K^{\\top} y^{k}\\|_{2} = \\|D^\\top q^k + A^\\top p^k\\|_2 \\le \\eta$：此条件直接衡量了满足原始-对偶最优性条件$K^\\top y^* = 0$的接近程度，这与KKT平稳性条件相关。\n因此，当两个条件都满足时，迭代$(x^k, y^k)$被认为是最佳原始-对偶解的一个良好近似。\n\n**3. 无约束问题的PDHG算法**\n\n拉格朗日形式为$\\min_x \\frac{1}{2}\\|Ax-b\\|_2^2 + \\lambda \\|Dx\\|_1$。\n我们使用相同的鞍点结构，其中$G(x)=0$且$Kx = \\begin{bmatrix} Dx \\\\ Ax \\end{bmatrix}$。现在的函数$F$是$F(u, v) = \\lambda\\|u\\|_1 + \\frac{1}{2}\\|v-b\\|_2^2$。\n$F$是可分的：$F(u,v) = F_1(u) + F_2(v)$，其中$F_1(u) = \\lambda\\|u\\|_1$，$F_2(v) = \\frac{1}{2}\\|v-b\\|_2^2$。\n共轭函数为：\n-   $F_1^*(q) = (\\lambda\\|\\cdot\\|_1)^*(q) = \\iota_{\\|q\\|_\\infty \\le \\lambda}(q)$。\n-   $F_2^*(p) = (\\frac{1}{2}\\|\\cdot-b\\|_2^2)^*(p) = \\frac{1}{2}\\|p\\|_2^2 + \\langle p, b \\rangle$。\n\n对偶变量$y=\\begin{bmatrix} q \\\\ p \\end{bmatrix}$的PDHG更新为：\n-   $q^{k+1} = \\operatorname{prox}_{\\sigma F_1^*}(q^k + \\sigma D\\bar{x}^k) = \\operatorname{proj}_{\\|\\cdot\\|_\\infty \\le \\lambda}(q^k + \\sigma D\\bar{x}^k)$。\n-   $p^{k+1} = \\operatorname{prox}_{\\sigma F_2^*}(p^k + \\sigma A\\bar{x}^k)$。$\\sigma F_2^*$的近端算子是$\\operatorname{prox}_{\\sigma F_2^*}(v) = \\operatorname{argmin}_p \\{\\frac{1}{2}\\|p-v\\|_2^2 + \\sigma(\\frac{1}{2}\\|p\\|_2^2 + \\langle p, b \\rangle) \\}$。这是一个关于$p$的二次函数，其解为$p = \\frac{v-\\sigma b}{1+\\sigma}$。代入$v=p^k + \\sigma A\\bar{x}^k$，我们得到$p^{k+1} = \\frac{p^k + \\sigma(A\\bar{x}^k-b)}{1+\\sigma}$。\n原始更新和外推更新的形式与约束情况相同。\n\n**4. 参数选择**\n\n如果步长$\\tau, \\sigma > 0$满足$\\tau\\sigma\\|K\\|_2^2 < 1$，则PDHG算法的收敛性得到保证。我们通过幂迭代法找到$K^\\top K = D^\\top D + A^\\top A$的最大特征值来估计$\\|K\\|_2$。然后我们设置$\\tau = \\sigma = 0.99 / \\|K\\|_2$。\n为了找到无约束问题的神谕参数$\\lambda_\\star$，我们使用二分法。残差范数$R(\\lambda) = \\|Ax_\\lambda - b\\|_2$是关于$\\lambda$的单调递增函数。我们搜索$\\lambda_\\star$使得$|R(\\lambda_\\star) - \\epsilon| \\le \\alpha \\epsilon$。我们建立一个搜索区间$[\\lambda_{low}, \\lambda_{high}]$，并通过为$\\lambda_{mid}$求解无约束问题并将其残差与$\\epsilon$比较来迭代地缩小该区间。\n\n以下实现执行了上述描述的针对给定测试用例的数值研究。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the numerical study comparing early-stopped PDHG for constrained TV\n    reconstruction with an oracle-parameter unconstrained TV reconstruction.\n    \"\"\"\n\n    def get_operators(n, m, ill, seed):\n        \"\"\"Constructs the linear operators A and D.\"\"\"\n        rng = np.random.default_rng(seed)\n        A_rand = rng.standard_normal((m, n))\n\n        if ill:\n            # Create an ill-conditioned matrix C\n            U, _, Vh = np.linalg.svd(rng.standard_normal((n, n)))\n            s = np.logspace(0, -5, n)  # Condition number ~1e5\n            C = U @ np.diag(s) @ Vh\n            A = A_rand @ C\n        else:\n            A = A_rand\n        \n        # Normalize A to have spectral norm approx 1 for stability\n        A /= np.linalg.norm(A, 2)\n\n        def D_op(x):\n            return x[1:] - x[:-1]\n\n        def DT_op(q):\n            res = np.zeros(n)\n            res[0] = -q[0]\n            res[1:-1] = q[:-1] - q[1:]\n            res[-1] = q[-1]\n            return res\n\n        return A, D_op, DT_op\n\n    def estimate_norm_K(A, D_op, DT_op, n, iters=20):\n        \"\"\"Estimates the spectral norm of K using power iteration on K^T K.\"\"\"\n        x = np.random.randn(n)\n        x /= np.linalg.norm(x)\n        \n        for _ in range(iters):\n            x = DT_op(D_op(x)) + A.T @ (A @ x)\n            norm_val = np.linalg.norm(x)\n            x /= norm_val\n        \n        return np.sqrt(norm_val)\n\n    def proj_linf(v, radius):\n        \"\"\"Projection onto the L-infinity ball.\"\"\"\n        return np.clip(v, -radius, radius)\n\n    def shrink_l2(v, t):\n        \"\"\"L2-norm soft-thresholding (proximal of L2 norm).\"\"\"\n        norm_v = np.linalg.norm(v)\n        if norm_v > t:\n            return v * (1.0 - t / norm_v)\n        else:\n            return np.zeros_like(v)\n\n    def pdhg_solver(\n        mode, A, D_op, DT_op, b, n, n_m1, m,\n        tau, sigma, max_iter,\n        lambda_reg=None, epsilon=None, # Mode-specific parameters\n        x_init=None, q_init=None, p_init=None,\n        early_stop_params=None\n    ):\n        \"\"\"A generic PDHG solver for both constrained and unconstrained problems.\"\"\"\n        x = np.zeros(n) if x_init is None else np.copy(x_init)\n        q = np.zeros(n_m1) if q_init is None else np.copy(q_init)\n        p = np.zeros(m) if p_init is None else np.copy(p_init)\n        x_bar = np.copy(x)\n\n        for k in range(max_iter):\n            # Dual updates\n            Dx_bar = D_op(x_bar)\n            Ax_bar = A @ x_bar\n            \n            if mode == 'constrained':\n                q_next = proj_linf(q + sigma * Dx_bar, 1.0)\n                p_next = shrink_l2(p + sigma * (Ax_bar - b), sigma * epsilon)\n            elif mode == 'unconstrained':\n                q_next = proj_linf(q + sigma * Dx_bar, lambda_reg)\n                p_next = (p + sigma * (Ax_bar - b)) / (1.0 + sigma)\n            else:\n                raise ValueError(\"Invalid PDHG mode\")\n\n            # Primal update\n            x_next = x - tau * (DT_op(q_next) + A.T @ p_next)\n            \n            # Extrapolation\n            x_bar_next = 2 * x_next - x\n            \n            # Update variables\n            x, q, p, x_bar = x_next, q_next, p_next, x_bar_next\n            \n            if early_stop_params and mode == 'constrained':\n                rho, eta = early_stop_params\n                residual_norm = np.linalg.norm(A @ x - b)\n                stationarity_res = np.linalg.norm(DT_op(q) + A.T @ p)\n                if abs(residual_norm - epsilon) <= rho * epsilon and stationarity_res <= eta:\n                    break\n\n        return x, q, p\n\n    def solve_constrained_es(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, max_iter_es, epsilon, rho, eta):\n        x_es, _, _ = pdhg_solver(\n            mode='constrained', A=A, D_op=D_op, DT_op=DT_op, b=b, n=n, n_m1=n_m1, m=m,\n            tau=tau, sigma=sigma, max_iter=max_iter_es,\n            epsilon=epsilon, early_stop_params=(rho, eta)\n        )\n        return x_es\n\n    def solve_unconstrained_bisection(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, epsilon, alpha):\n        # Bisection to find lambda_star\n        lambda_low = 0.0\n        lambda_high = 1.0\n\n        pdhg_iters_per_lambda = 2000\n        \n        # Find a valid upper bound for lambda\n        x_init, q_init, p_init = None, None, None\n        while True:\n            x_lambda, q_lambda, p_lambda = pdhg_solver(\n                'unconstrained', A, D_op, DT_op, b, n, n_m1, m,\n                tau, sigma, pdhg_iters_per_lambda,\n                lambda_reg=lambda_high, x_init=x_init, q_init=q_init, p_init=p_init\n            )\n            x_init, q_init, p_init = x_lambda, q_lambda, p_lambda # Warm start\n            residual = np.linalg.norm(A @ x_lambda - b)\n            if residual > epsilon:\n                break\n            lambda_high *= 2.0\n            if lambda_high > 1e6: # Safety break\n                #print(\"Warning: Could not find lambda_high\")\n                break\n\n        # Bisection loop\n        x_lambda_star = x_lambda\n        for _ in range(50): # Max 50 bisection iterations\n            lambda_mid = (lambda_low + lambda_high) / 2.0\n            if lambda_mid == lambda_low or lambda_mid == lambda_high:\n                break\n            \n            x_lambda_star, q_init, p_init = pdhg_solver(\n                'unconstrained', A, D_op, DT_op, b, n, n_m1, m, \n                tau, sigma, pdhg_iters_per_lambda,\n                lambda_reg=lambda_mid, x_init=x_init, q_init=q_init, p_init=p_init\n            )\n            \n            residual = np.linalg.norm(A @ x_lambda_star - b)\n\n            if abs(residual - epsilon) <= alpha * epsilon:\n                break # Found a good lambda\n            \n            if residual < epsilon:\n                lambda_low = lambda_mid\n            else:\n                lambda_high = lambda_mid\n                \n        return x_lambda_star\n\n    test_cases = [\n        # (n, m, seed, noise_sigma, ill, max_iter_ES, rho, eta, alpha, beta)\n        (64, 40, 0, 0.01, False, 3000, 0.05, 1e-3, 0.05, 0.10),\n        (64, 40, 1, 1e-4, False, 3000, 0.05, 1e-3, 0.05, 0.10),\n        (64, 40, 2, 0.05, False, 3000, 0.05, 1e-3, 0.05, 0.10),\n        (64, 40, 3, 0.01, True, 50, 0.05, 1e-3, 0.05, 0.10),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        n, m, seed, noise_sigma, ill, max_iter_es, rho, eta, alpha, beta = case\n        n_m1 = n - 1\n\n        # --- Setup problem instance ---\n        rng = np.random.default_rng(seed)\n        \n        # Ground truth piecewise-constant signal\n        x0 = np.zeros(n)\n        x0[n//4 : n//2] = 1.0\n        x0[2*n//3 : 5*n//6] = -0.5\n\n        A, D_op, DT_op = get_operators(n, m, ill, seed)\n        noise = noise_sigma * rng.standard_normal(m)\n        b = A @ x0 + noise\n        epsilon = np.linalg.norm(noise)\n        \n        # --- Parameter Setup ---\n        norm_K = estimate_norm_K(A, D_op, DT_op, n)\n        tau = sigma = 0.99 / norm_K\n\n        # --- Solve and Compare ---\n        # 1. Constrained problem with early stopping\n        x_es = solve_constrained_es(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, max_iter_es, epsilon, rho, eta)\n        \n        # 2. Unconstrained problem with oracle lambda\n        x_lambda_star = solve_unconstrained_bisection(A, D_op, DT_op, b, n, n_m1, m, tau, sigma, epsilon, alpha)\n        \n        # 3. Final comparison\n        res_es = np.linalg.norm(A @ x_es - b)\n        \n        residual_match = abs(res_es - epsilon) <= alpha * epsilon\n        \n        norm_x_lambda_star = np.linalg.norm(x_lambda_star)\n        if norm_x_lambda_star == 0:\n            # Handle case where oracle solution is zero\n            solution_match = np.linalg.norm(x_es) <= beta\n        else:\n            solution_match = np.linalg.norm(x_es - x_lambda_star) <= beta * norm_x_lambda_star\n        \n        results.append(residual_match and solution_match)\n\n    print(f\"[{','.join(map(str, [r.lower() for r in results]))}]\")\n\nsolve()\n```", "id": "3466854"}]}