{"hands_on_practices": [{"introduction": "从量化数据中恢复信号的第一步，是精确地定义量化值告诉了我们关于原始信号的哪些信息。该练习旨在将由量化观测所隐含的区间约束，转化为一个在优化框架中易于处理的凸惩罚函数。掌握这种转换是构建基于优化的恢复算法的基石，因为它将物理测量过程与数学目标函数联系了起来 [@problem_id:3472918]。", "problem": "考虑一个无噪声的压缩感知模型，其中有一个未知的 $k$-稀疏向量 $x \\in \\mathbb{R}^{n}$ 和一个已知的传感矩阵 $A \\in \\mathbb{R}^{m \\times n}$。令 $a_{i}^{\\top}$ 表示 $A$ 的第 $i$ 行，因此量化前的第 $i$ 个标量测量值为 $z_{i} = a_{i}^{\\top} x$。测量值由一个非均匀标量量化器 $Q$进行量化，该量化器由一个严格递增的阈值序列 $\\{t_{k}\\}_{k=1}^{K+1}$ (其中 $t_{1}  t_{2}  \\cdots  t_{K+1}$) 指定。对于所有的 $i \\in \\{1,\\dots,m\\}$，通过 $a_{i}^{\\top} x \\in [t_{1}, t_{K+1})$ 来确保其工作范围。该量化器根据规则 $Q(z) = k$ 当且仅当 $t_{k} \\leq z  t_{k+1}$，将一个标量 $z$ 映射到一个区间索引 $Q(z) \\in \\{1,\\dots,K\\}$。观测到的标签为 $y_{i} = Q(a_{i}^{\\top} x)$，其中 $i = 1,\\dots,m$，这些标签被收集在 $y \\in \\{1,\\dots,K\\}^{m}$ 中。\n\n从非均匀量化器的定义和区间索引的区间隶属特性出发，形式化由观测值 $y$ 所蕴含的对 $A x$ 的测量一致性约束，并推导出一个单一的闭式解析表达式，该表达式为一个凸铰链惩罚项。此惩罚项当且仅当所有区间约束都满足时（即，对每个 $i$，条件 $t_{y_{i}} \\leq a_{i}^{\\top} x  t_{y_{i}+1}$ 成立）等于零，否则在每个边界上对违规行为进行线性的、单侧的惩罚。将你的最终答案表示为 $A$、$x$、$y$ 和阈值 $\\{t_{k}\\}_{k=1}^{K+1}$ 的显式函数。\n\n你的最终答案必须是一个单一的闭式解析表达式。不需要进行数值近似或四舍五入。", "solution": "首先验证该问题具有科学依据、是适定的、客观的且形式上是完备的。它代表了在压缩感知领域中，为从量化测量中恢复信号而构建优化问题的一项标准任务。因此，我们可以继续进行推导。\n\n该问题要求构建一个强制执行测量一致性的凸惩罚函数。让我们首先形式化一致性约束本身。\n\n根据问题陈述，标量量化器 $Q$ 基于一组严格递增的阈值 $\\{t_{j}\\}_{j=1}^{K+1}$，将输入 $z$ 映射到一个整数区间索引 $k \\in \\{1, \\dots, K\\}$。量化规则由下式给出：\n$$\nQ(z) = k \\quad \\text{if and only if} \\quad t_{k} \\leq z  t_{k+1}\n$$\n对于 $i \\in \\{1, \\dots, m\\}$，未量化的测量值为 $z_{i} = a_{i}^{\\top} x$，其中 $a_{i}^{\\top}$ 是传感矩阵 $A$ 的第 $i$ 行，$x \\in \\mathbb{R}^{n}$ 是未知信号向量。观测数据是区间索引 $y_{i} = Q(z_{i}) = Q(a_{i}^{\\top} x)$。\n\n对于每个测量 $i$，观测值 $y_{i}$ 意味着真实值 $a_{i}^{\\top} x$ 必须位于由阈值定义的特定区间内。应用量化规则，观测值 $y_i$ 等价于以下关于 $x$ 的约束集：\n$$\nt_{y_{i}} \\leq a_{i}^{\\top} x  t_{y_{i}+1}\n$$\n这必须对每次测量都成立，因此我们得到了一个包含 $m$ 个此类区间约束的系统，其中 $i = 1, \\dots, m$。每个区间约束可以分解为两个独立的不等式约束：\n1. $a_{i}^{\\top} x \\geq t_{y_{i}}$\n2. $a_{i}^{\\top} x  t_{y_{i}+1}$\n\n问题要求一个凸铰链惩罚项，该惩罚项当且仅当这些约束被满足时为零。根据定义，铰链惩罚项以单侧、线性的方式惩罚对不等式的违反。对于形式为 $f(x) \\leq 0$ 的一般不等式约束，相应的铰链惩罚项是 $\\max\\{0, f(x)\\}$。对于形式为 $g(x) \\geq 0$ 的约束，它等价于 $-g(x) \\leq 0$，其惩罚项是 $\\max\\{0, -g(x)\\}$。\n\n让我们将此原则应用于我们针对第 $i$ 次测量的两个不等式。\n\n对于第一个不等式 $a_{i}^{\\top} x \\geq t_{y_{i}}$，我们可以将其重写为 $t_{y_{i}} - a_{i}^{\\top} x \\leq 0$。当 $t_{y_{i}} - a_{i}^{\\top} x > 0$ 时，发生违反。违反此下界的铰链惩罚项是：\n$$\nP_{\\text{lower}, i}(x) = \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\}\n$$\n如果 $a_{i}^{\\top} x \\geq t_{y_{i}}$，此函数为零；当 $a_{i}^{\\top} x  t_{y_{i}}$ 时，它随着违规量 $t_{y_{i}} - a_{i}^{\\top} x$ 的大小线性增长。\n\n对于第二个不等式 $a_{i}^{\\top} x  t_{y_{i}+1}$，我们首先将严格不等式松弛为 $a_{i}^{\\top} x \\leq t_{y_{i}+1}$，以便于构建凸惩罚函数。这是优化中的标准做法，因为对开集进行惩罚是有问题的。松弛后的约束为 $a_{i}^{\\top} x - t_{y_{i}+1} \\leq 0$。当 $a_{i}^{\\top} x - t_{y_{i}+1} > 0$ 时，发生违反。违反此上界的铰链惩罚项是：\n$$\nP_{\\text{upper}, i}(x) = \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\}\n$$\n如果 $a_{i}^{\\top} x \\leq t_{y_{i}+1}$，此函数为零；当 $a_{i}^{\\top} x > t_{y_{i}+1}$ 时，它随着违规量 $a_{i}^{\\top} x - t_{y_{i}+1}$ 的大小线性增长。\n\n第 $i$ 次测量的总惩罚是各个惩罚项之和，它当且仅当两个约束都满足时必须为零：\n$$\nP_{i}(x) = P_{\\text{lower}, i}(x) + P_{\\text{upper}, i}(x) = \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\}\n$$\n这个组合惩罚项 $P_{i}(x)$ 当且仅当两项都为零时才为零，这发生在 $t_{y_{i}} - a_{i}^{\\top} x \\leq 0$ 且 $a_{i}^{\\top} x - t_{y_{i}+1} \\leq 0$ 时。这正是条件 $t_{y_{i}} \\leq a_{i}^{\\top} x \\leq t_{y_{i}+1}$。\n\n为了获得所有测量的总惩罚，我们将 $i = 1, \\dots, m$ 的各个惩罚项相加。令此总惩罚表示为 $\\mathcal{L}(x)$。\n$$\n\\mathcal{L}(x) = \\sum_{i=1}^{m} P_{i}(x) = \\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\} \\right)\n$$\n这个最终表达式满足了问题的所有要求：\n- 它是 $A$、$x$、$y$ 和阈值 $\\{t_{k}\\}$ 的函数。项 $a_{i}^{\\top} x$ 涉及 $A$ 和 $x$。阈值的索引 $y_{i}$ 和 $y_{i}+1$ 来自观测向量 $y$。\n- 它是关于 $x$ 的凸函数。项 $t_{y_{i}} - a_{i}^{\\top} x$ 和 $a_{i}^{\\top} x - t_{y_{i}+1}$ 是 $x$ 的仿射函数。函数 $\\max\\{0, u\\}$ 是凸且非递减的。一个凸的非递减函数与一个仿射函数的复合是凸的。凸函数的和是凸的。\n- 当且仅当对于所有 $i \\in \\{1, \\dots, m\\}$，所有的区间约束（以其松弛形式 $t_{y_{i}} \\leq a_{i}^{\\top} x \\leq t_{y_{i}+1}$）都得到满足时，它才等于零。由于和中的每一项都是非负的，所以和为零当且仅当所有单项都为零。\n- 它在每个边界上线性地、单侧地惩罚违规，这正是铰链惩罚项的本质。\n\n该表达式是所需的测量一致性惩罚的闭式解析表示。", "answer": "$$\n\\boxed{\\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_i} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\} \\right)}\n$$", "id": "3472918"}, {"introduction": "有了数学模型后，我们需要一个高效的算法来求解信号。本练习将推导一种用于1比特压缩感知的经典算法——二进制迭代硬阈值（BIHT）算法。通过从第一性原理出发，推导其核心迭代更新步骤，你将深入理解基于投影的迭代算法（如梯度下降结合稀疏投影）的工作机制，这为你设计和分析其他恢复方法提供了蓝图 [@problem_id:3472923]。", "problem": "设 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 是一个传感矩阵，其行向量为 $\\mathbf{a}_{i}^{\\top}$，并设 $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ 是一个未知的 $s$-稀疏向量。在单比特压缩感知 (CS) 中，仅观测线性测量的符号，因此测量模型为 $\\mathbf{y} = \\operatorname{sign}(\\mathbf{A}\\mathbf{x}^{\\star}) \\in \\{-1, +1\\}^{m}$。一种恢复与 $\\mathbf{y}$ 一致的稀疏向量 $\\mathbf{x}$ 的常用方法是使用平方合页损失来惩罚符号不一致的测量：\n$$\nL(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max\\!\\left(0, - y_{i}\\,\\mathbf{a}_{i}^{\\top}\\mathbf{x}\\right)\\right)^{2},\n$$\n其中 $\\max(0, \\cdot)$ 逐元素作用。二元迭代硬阈值 (BIHT) 算法通过重复对 $L(\\mathbf{x})$ 进行最速下降步，然后应用一个硬阈值算子（该算子保留 $s$ 个绝对值最大的分量并将其余分量置零），来寻求 $L(\\mathbf{x})$ 的一个稀疏最小化子。\n\n从单比特量化测量、平方合页损失、通过次梯度对非光滑目标进行最速下降迭代，以及保留 $s$ 个绝对值最大分量的硬阈值算子 $H_{s}(\\cdot)$ 的定义出发，推导将 $\\mathbf{x}^{t}$ 映射到下一次迭代的 BIHT 更新的显式解析表达式。使用逐元素正部算子 $[\\,\\cdot\\,]_{+} := \\max(0, \\cdot)$，将你的最终更新表示为关于 $\\mathbf{A}$、$\\mathbf{y}$、$\\mathbf{x}^{t}$、一个正步长参数 $\\mu$ 和 $s$ 的单个闭式表达式。你的最终答案必须是单个不带等号的闭式解析表达式。无需进行数值计算。", "solution": "二元迭代硬阈值（BIHT）算法的更新规则包含两个主要步骤：首先是对损失函数 $L(\\mathbf{x})$ 进行一次最速下降（梯度下降）更新，然后应用硬阈值算子 $H_s(\\cdot)$ 以强制稀疏性。\n\n**步骤1：计算损失函数的梯度**\n\n给定的损失函数为：\n$$\nL(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max(0, -y_i \\mathbf{a}_i^\\top \\mathbf{x})\\right)^2 = \\sum_{i=1}^{m} ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2\n$$\n其中 $[\\cdot]_+ = \\max(0, \\cdot)$。这是一个可微函数。我们可以使用链式法则来计算其梯度。对于单个分量 $L_i(\\mathbf{x}) = ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$，其梯度为：\n$$\n\\nabla L_i(\\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot \\nabla_\\mathbf{x}(-y_i \\mathbf{a}_i^\\top \\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)\n$$\n总损失函数 $L(\\mathbf{x})$ 的梯度是所有分量梯度之和：\n$$\n\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} \\nabla L_i(\\mathbf{x}) = \\sum_{i=1}^{m} 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) = -2 \\sum_{i=1}^{m} y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\mathbf{a}_i\n$$\n我们可以将这个和式写成矩阵形式。令 $\\operatorname{diag}(\\mathbf{y})$ 为对角线上元素为 $\\mathbf{y}$ 中各分量的对角矩阵。则 $\\nabla L(\\mathbf{x})$ 可以表示为：\n$$\n\\nabla L(\\mathbf{x}) = -2 \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_{+} \\right)\n$$\n\n**步骤2：执行梯度下降和硬阈值操作**\n\n从当前迭代 $\\mathbf{x}^t$ 开始，最速下降步骤以步长 $\\mu$ 更新得到一个中间向量 $\\mathbf{z}^{t+1}$：\n$$\n\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t - \\mu \\left( -2 \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_{+} \\right) \\right)\n$$\n$$\n\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_{+} \\right)\n$$\n最后，应用硬阈值算子 $H_s(\\cdot)$ 到中间向量 $\\mathbf{z}^{t+1}$ 上，以获得下一次迭代的稀疏估计 $\\mathbf{x}^{t+1}$。该算子保留向量中 $s$ 个绝对值最大的元素，并将其余元素置零。\n\n因此，完整的BIHT更新表达式为：\n$$\n\\mathbf{x}^{t+1} = H_{s}\\left( \\mathbf{z}^{t+1} \\right) = H_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)\n$$\n这就是将 $\\mathbf{x}^{t}$ 映射到 $\\mathbf{x}^{t+1}$ 的闭式解析表达式。", "answer": "$$\n\\boxed{\nH_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)\n}\n$$", "id": "3472923"}, {"introduction": "除了算法设计，实际的系统实现还涉及关键的权衡。本练习探讨了一个核心的系统级问题：在固定的总比特预算下，如何在测量数量和量化精度之间做出最佳分配。通过使用理论误差模型来指导实际的工程决策，并编写程序来寻找最优平衡点，你将学习如何将抽象的性能界限与具体的系统设计和资源分配联系起来 [@problem_id:3472956]。", "problem": "考虑一个线性压缩感知采集模型，其测量值为 $z_{i} = a_{i}^{\\top} x$，$i = 1, \\dots, m$，其中 $x \\in \\mathbb{R}^{n}$ 是一个随机信号，$a_{i} \\in \\mathbb{R}^{n}$ 表示感知矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的第 $i$ 行。假设 $x$ 是均值为零、协方差矩阵为 $\\Sigma_{x}$ 的高斯分布，并且感知矩阵被设计成使得所有测量方差相等，即对于一个已知的标量 $\\sigma  0$ 和每一个 $i$，都有 $a_{i}^{\\top} \\Sigma_{x} a_{i} = \\sigma^{2}$。每个测量值 $z_{i}$ 由一个具有 $b$ 比特和步长 $\\Delta  0$ 的对称均匀中升型量化器进行量化。当 $|z_{i}| \\geq R(\\Delta)$ 时，量化器会饱和，其中动态范围为 $R(\\Delta) = 2^{b-1} \\Delta$，并且将所有超出动态范围的值映射到最近的可表示电平。设标准正态尾函数（也称为Q函数）定义为 $Q(u) = \\int_{u}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp\\!\\left( -\\frac{t^{2}}{2} \\right) \\, dt$。\n\n1. 仅使用高斯模型所蕴含的测量分布和量化器的定义，推导单个测量值的饱和概率，将其表示为 $\\Delta$、$b$ 和 $\\sigma$ 的函数。\n\n2. 在从量化测量值中进行稀疏恢复时，期望重构误差的两个主要贡献因素是动态范围内的量化误差和因饱和导致的信息损失。一个用于平衡这些影响的常用代理目标是\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + \\beta p_{\\mathrm{sat}}(\\Delta),\n$$\n其中 $\\alpha  0$ 和 $\\beta  0$ 是已知的权衡权重，反映了量化误差和饱和对恢复的影响，而 $p_{\\mathrm{sat}}(\\Delta)$ 是您在第1部分中推导出的饱和概率。从基本原理出发，推导最小化 $J(\\Delta)$ 的唯一步长 $\\Delta^{\\star}$，并将其以 $\\alpha$、$\\beta$、$\\sigma$ 和 $b$ 的闭式解形式表示。您的最终答案必须是单一的闭式解析表达式。\n\n将最终答案表示为符号表达式。无需四舍五入。不要包含任何物理单位。方框内的最终答案必须仅为 $\\Delta^{\\star}$ 的表达式。", "solution": "这个问题分为两部分。首先，我们必须推导单个量化测量值的饱和概率。其次，我们必须利用这个概率来找到最小化给定目标函数的最优量化器步长 $\\Delta$。\n\n**第1部分：饱和概率的推导**\n\n问题陈述提供了线性测量模型 $z_{i} = a_{i}^{\\top} x$，其中信号 $x \\in \\mathbb{R}^{n}$ 是一个零均值高斯随机向量，其协方差矩阵为 $\\Sigma_{x}$，即 $x \\sim \\mathcal{N}(0, \\Sigma_{x})$。\n\n高斯随机向量的线性变换仍然是高斯分布。因此，每个测量值 $z_{i}$ 是一个高斯随机变量。其均值为：\n$$\n\\mathbb{E}[z_{i}] = \\mathbb{E}[a_{i}^{\\top} x] = a_{i}^{\\top} \\mathbb{E}[x] = a_{i}^{\\top} 0 = 0\n$$\n$z_{i}$ 的方差由下式给出：\n$$\n\\text{Var}(z_{i}) = \\mathbb{E}[(z_{i} - \\mathbb{E}[z_{i}])^{2}] = \\mathbb{E}[z_{i}^{2}] = \\mathbb{E}[(a_{i}^{\\top} x)(x^{\\top} a_{i})] = a_{i}^{\\top} \\mathbb{E}[x x^{\\top}] a_{i} = a_{i}^{\\top} \\Sigma_{x} a_{i}\n$$\n问题陈述指出，感知矩阵 $A$ 被设计成对所有 $i=1, \\dots, m$ 都有 $a_{i}^{\\top} \\Sigma_{x} a_{i} = \\sigma^{2}$。因此，每个测量值 $z_{i}$ 具有相同的分布：一个均值为0、方差为 $\\sigma^{2}$ 的高斯分布，记为 $z_i \\sim \\mathcal{N}(0, \\sigma^{2})$。\n\n当测量值的幅值 $|z_i|$ 大于或等于动态范围限制 $R(\\Delta)$ 时，量化器发生饱和。动态范围给定为 $R(\\Delta) = 2^{b-1} \\Delta$。因此，饱和事件为 $|z_{i}| \\geq 2^{b-1} \\Delta$。\n\n饱和概率 $p_{\\mathrm{sat}}(\\Delta)$ 是该事件的概率：\n$$\np_{\\mathrm{sat}}(\\Delta) = P(|z_{i}| \\geq 2^{b-1} \\Delta)\n$$\n由于 $z_i$ 的分布关于其均值0对称，我们可以将其写为：\n$$\np_{\\mathrm{sat}}(\\Delta) = P(z_{i} \\geq 2^{b-1} \\Delta) + P(z_{i} \\leq -2^{b-1} \\Delta) = 2 P(z_{i} \\geq 2^{b-1} \\Delta)\n$$\n为了使用标准正态尾函数 $Q(u)$ 表示此概率，我们对随机变量 $z_{i}$ 进行标准化。令 $U = z_{i}/\\sigma$。则 $U$ 是一个标准正态随机变量，$U \\sim \\mathcal{N}(0, 1)$。\n现在，该概率可以写成 $U$ 的形式：\n$$\nP(z_{i} \\geq 2^{b-1} \\Delta) = P(\\sigma U \\geq 2^{b-1} \\Delta) = P\\left(U \\geq \\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n根据Q函数的定义，$Q(u) = P(U \\geq u)$，我们有：\n$$\nP(z_{i} \\geq 2^{b-1} \\Delta) = Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n因此，任何单个测量值的饱和概率为：\n$$\np_{\\mathrm{sat}}(\\Delta) = 2 Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n\n**第2部分：最优步长 $\\Delta^{\\star}$ 的推导**\n\n我们需要找到最小化目标函数 $J(\\Delta)$ 的步长 $\\Delta^{\\star}$：\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + \\beta p_{\\mathrm{sat}}(\\Delta)\n$$\n将第1部分中得到的 $p_{\\mathrm{sat}}(\\Delta)$ 的表达式代入，可得：\n$$\nJ(\\Delta) = \\alpha \\Delta^{2} + 2 \\beta Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\n$$\n为了找到最小值，我们计算 $J(\\Delta)$ 关于 $\\Delta$ 的导数并令其为零。我们必须求出 $\\frac{dJ}{d\\Delta}$。\n第一项的导数是 $2 \\alpha \\Delta$。对于第二项，我们使用链式法则和Q函数的导数。Q函数定义为 $Q(u) = \\int_{u}^{\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{t^{2}}{2}) dt$。根据微积分基本定理（莱布尼茨法则），其导数为 $\\frac{dQ(u)}{du} = -\\frac{1}{\\sqrt{2 \\pi}} \\exp(-\\frac{u^{2}}{2})$。\n\n令 $u(\\Delta) = \\frac{2^{b-1} \\Delta}{\\sigma}$。则 $\\frac{du}{d\\Delta} = \\frac{2^{b-1}}{\\sigma}$。第二项的导数是：\n$$\n\\frac{d}{d\\Delta}\\left[2 \\beta Q\\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)\\right] = 2 \\beta \\cdot \\frac{dQ(u)}{du} \\cdot \\frac{du}{d\\Delta} = 2 \\beta \\left( -\\frac{1}{\\sqrt{2 \\pi}} \\exp\\left(-\\frac{u^{2}}{2}\\right) \\right) \\left( \\frac{2^{b-1}}{\\sigma} \\right)\n$$\n$$\n= -\\frac{2 \\beta \\cdot 2^{b-1}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2} \\left(\\frac{2^{b-1} \\Delta}{\\sigma}\\right)^{2}\\right) = -\\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\n现在，我们将总导数 $\\frac{dJ}{d\\Delta}$ 置为零：\n$$\n\\frac{dJ}{d\\Delta} = 2 \\alpha \\Delta - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right) = 0\n$$\n整理各项，我们得到：\n$$\n2 \\alpha \\Delta = \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\n这是一个超越方程。为了求解 $\\Delta$，我们将其变换为与朗伯W函数相容的形式，朗伯W函数由 $z = W(z)\\exp(W(z))$ 定义。\n让我们将方程两边平方：\n$$\n4 \\alpha^{2} \\Delta^{2} = \\frac{\\beta^{2} \\cdot 2^{2b}}{\\sigma^{2} \\cdot 2\\pi} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}\\right)\n$$\n$$\n4 \\alpha^{2} \\Delta^{2} = \\frac{\\beta^{2} \\cdot 2^{2b-1}}{\\pi\\sigma^{2}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}\\right)\n$$\n让我们定义一个新变量 $Y = \\frac{2^{2b-2} \\Delta^{2}}{\\sigma^{2}}$。这意味着 $\\Delta^{2} = \\frac{\\sigma^{2}}{2^{2b-2}}Y$。将此代入平方后的方程：\n$$\n4 \\alpha^{2} \\left(\\frac{\\sigma^{2}}{2^{2b-2}}Y\\right) = \\frac{\\beta^{2} \\cdot 2^{2b-1}}{\\pi\\sigma^{2}} \\exp(-Y)\n$$\n$$\n\\frac{4 \\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{2b-1} \\cdot 2^{2b-2}} Y = \\exp(-Y)\n$$\n$$\n\\frac{4 \\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{4b-3}} Y = \\exp(-Y)\n$$\n$$\n\\frac{\\pi \\alpha^{2} \\sigma^{4}}{\\beta^{2} \\cdot 2^{4b-5}} Y = \\exp(-Y)\n$$\n乘以 $\\exp(Y)$ 并整理，得到朗伯W函数的典范形式：\n$$\nY \\exp(Y) = \\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\n$$\n应用朗伯W函数，我们求解 $Y$：\n$$\nY = W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n我们将 $Y = \\frac{2^{2b-2} (\\Delta^{\\star})^{2}}{\\sigma^{2}}$ 代回，以求得最优步长 $\\Delta^{\\star}$：\n$$\n\\frac{2^{2b-2} (\\Delta^{\\star})^{2}}{\\sigma^{2}} = W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n$$\n(\\Delta^{\\star})^{2} = \\frac{\\sigma^{2}}{2^{2b-2}} W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)\n$$\n由于步长 $\\Delta$ 必须为正，我们取正平方根：\n$$\n\\Delta^{\\star} = \\sqrt{\\frac{\\sigma^{2}}{2^{2b-2}} W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)} = \\frac{\\sigma}{2^{b-1}} \\sqrt{W\\left(\\frac{\\beta^{2} \\cdot 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)}\n$$\n为了确认这是一个唯一的最小值，我们检查 $J(\\Delta)$ 的二阶导数：\n$$\n\\frac{d^2 J}{d\\Delta^2} = \\frac{d}{d\\Delta}\\left[2 \\alpha \\Delta - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\\right]\n$$\n$$\n\\frac{d^2 J}{d\\Delta^2} = 2 \\alpha - \\frac{\\beta \\cdot 2^{b}}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right) \\left(-\\frac{2 \\cdot 2^{2b-2} \\Delta}{2\\sigma^{2}}\\right)\n$$\n$$\n\\frac{d^2 J}{d\\Delta^2} = 2 \\alpha + \\frac{\\beta \\cdot 2^{b} \\cdot 2^{2b-2} \\Delta}{\\sigma^3\\sqrt{2\\pi}} \\exp\\left(-\\frac{2^{2b-2} \\Delta^{2}}{2\\sigma^{2}}\\right)\n$$\n对于 $\\Delta  0$，并且给定 $\\alpha  0$，$\\beta  0$，$\\sigma  0$，二阶导数表达式中的所有项均为正。因此，对于所有 $\\Delta  0$，$\\frac{d^2 J}{d\\Delta^2}  0$，这意味着 $J(\\Delta)$ 在 $\\Delta  0$ 上是一个严格凸函数。因此，我们找到的临界点对应于一个唯一的全局最小值。朗伯W函数的自变量为正，因此其输出是一个唯一的正实数，从而确保 $\\Delta^{\\star}$ 是一个唯一的正实数。", "answer": "$$\\boxed{\\frac{\\sigma}{2^{b-1}} \\sqrt{W\\left(\\frac{\\beta^{2} 2^{4b-5}}{\\pi \\alpha^{2} \\sigma^{4}}\\right)}}$$", "id": "3472956"}]}