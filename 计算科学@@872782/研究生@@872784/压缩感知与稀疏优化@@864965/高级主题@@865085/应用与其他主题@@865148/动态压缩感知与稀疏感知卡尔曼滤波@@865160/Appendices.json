{"hands_on_practices": [{"introduction": "第一个练习将稀疏感知卡尔曼滤波器分解为其最基本的组成部分：单步更新。通过一个简化的场景 ([@problem_id:3445409])，你将从第一性原理推导出解，并看到 $\\ell_1$ 惩罚项如何直接引导出软阈值操作。这个实践对于建立这些滤波器如何在状态估计中引入稀疏性的直观理解至关重要。", "problem": "考虑一个在稀疏感知卡尔曼滤波器 (KF) 变体下的动态稀疏状态估计问题的单个时间步。状态转移是单位矩阵，即 $F=I$，当前时间步的先验是高斯分布 $x \\sim \\mathcal{N}(\\mu, P)$，其中 $P=\\tau^2 I$。测量模型为 $y = H x + v$，其中 $v \\sim \\mathcal{N}(0, R)$ 且 $R=\\sigma^2 I$。为了促进估计的稀疏性，我们用一个 $\\ell_1$ 惩罚项 $\\lambda \\|x\\|_1$ 来增强负对数后验，这等价于对 $x$ 施加一个拉普拉斯先验。\n\n使用以下具体实例进行计算：\n- 维度 $n=5$，测量维度 $m=3$。\n- 测量矩阵 $H$ 具有标准正交的行，并充当坐标 $\\{1,3,5\\}$ 的选择器：\n  $$H=\\begin{pmatrix}1  0  0  0  0\\\\ 0  0  1  0  0\\\\ 0  0  0  0  1\\end{pmatrix}.$$\n- 噪声方差为 $\\sigma^2=0.04$，先验方差为 $\\tau^2=0.25$，稀疏性权重为 $\\lambda=0.15$。\n- 先验均值为 $\\mu=\\begin{pmatrix}0.02 \\\\ -0.08 \\\\ 0 \\\\ 0.03 \\\\ -0.01\\end{pmatrix}$，测量值为 $y=\\begin{pmatrix}0.10\\\\ -0.09\\\\ 0.02\\end{pmatrix}$。\n\n将此时间步的稀疏感知最大后验 (MAP) 估计 $x^\\star$ 定义为下式的唯一最小化子\n$$\n\\min_{x\\in\\mathbb{R}^5}\\ \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2+\\lambda\\|x\\|_1.\n$$\n从高斯似然和高斯先验模型以及 $\\ell_1$ 惩罚项出发，从第一性原理推导最优性条件的坐标形式以及最终的软阈值解。然后，根据给定的数据计算出显式向量 $x^\\star$，并确定在 $\\mu$ 和 $x^\\star$ 之间支撑集发生变化的索引集合；即支撑集的对称差\n$$\n\\Delta=\\{i:\\ (\\mu_i=0\\ \\text{and}\\ x^\\star_i\\neq 0)\\ \\text{or}\\ (\\mu_i\\neq 0\\ \\text{and}\\ x^\\star_i=0)\\}.\n$$\n\n请以整数形式提供 $\\Delta$ 的基数作为最终答案。不要四舍五入；需要一个精确的整数。", "solution": "该问题是适定的，并且在稀疏信号恢复和贝叶斯估计领域具有坚实的科学基础。所有必要的数据都已提供，且没有内部矛盾。我们可以开始求解。\n\n目标是找到向量 $x^\\star \\in \\mathbb{R}^5$，使其最小化函数：\n$$\nJ(x) = \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2+\\lambda\\|x\\|_1\n$$\n该函数是两个严格凸的二次项和一个凸的 $\\ell_1$ 范数项的和。因此，该和是严格凸的，这保证了唯一最小化子 $x^\\star$ 的存在。\n\n最小化子 $x^\\star$ 由一阶最优性条件 $0 \\in \\partial J(x^\\star)$ 刻画，其中 $\\partial J(x)$ 是 $J(x)$ 的次梯度。函数 $J(x)$ 可以分解为 $J(x) = f(x) + g(x)$，其中 $f(x) = \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2$ 是可微的，而 $g(x) = \\lambda\\|x\\|_1$ 是不可微但凸的。那么，最优性条件由下式给出：\n$$\n0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star) \\implies -\\nabla f(x^\\star) \\in \\partial g(x^\\star)\n$$\n$f(x)$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{\\sigma^2} H^T(Hx - y) + \\frac{1}{\\tau^2}(x - \\mu)\n$$\n$g(x) = \\lambda \\sum_{i=1}^5 |x_i|$ 的次梯度是其分量次梯度的笛卡尔积，$\\partial g(x) = \\lambda \\times_{i=1}^5 \\partial |x_i|$。绝对值函数在点 $z$ 处的次梯度为：\n$$\n\\partial|z| = \\begin{cases} \\{\\text{sgn}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\n测量矩阵 $H$ 的特殊结构使得该问题可以解耦。给定\n$$\nH=\\begin{pmatrix}1  0  0  0  0\\\\ 0  0  1  0  0\\\\ 0  0  0  0  1\\end{pmatrix}\n$$\n项 $\\|y-Hx\\|_2^2$ 展开为 $(y_1-x_1)^2 + (y_2-x_3)^2 + (y_3-x_5)^2$。其他项，$\\|x-\\mu\\|_2^2 = \\sum_{i=1}^5 (x_i-\\mu_i)^2$ 和 $\\|x\\|_1 = \\sum_{i=1}^5 |x_i|$，也是按坐标可分的。因此，总目标函数 $J(x)$ 可以写成五个独立函数之和，$J(x) = \\sum_{i=1}^5 J_i(x_i)$，这些函数可以被独立地最小化。\n\n我们可以将坐标分为两组：观测索引 $I_{obs} = \\{1, 3, 5\\}$ 和未观测索引 $I_{unobs} = \\{2, 4\\}$。\n\n**情况1：未观测索引 $i \\in \\{2, 4\\}$**\n对于这些索引，坐标 $x_i$ 的目标函数是：\n$$\nJ_i(x_i) = \\frac{1}{2\\tau^2}(x_i - \\mu_i)^2 + \\lambda|x_i|\n$$\n最优性条件是 $-\\frac{dJ_i}{dx_i}\\big|_{x_i \\neq 0} \\in \\lambda \\partial|x_i^\\star|$。二次项的导数是 $\\frac{1}{\\tau^2}(x_i-\\mu_i)$。这导出了条件 $\\frac{1}{\\tau^2}(\\mu_i - x_i^\\star) \\in \\lambda \\cdot \\partial|x_i^\\star|$。\n这是求解 $\\ell_1$ 范数的近端算子的问题，其解是著名的软阈值函数：\n$$\nx_i^\\star = \\text{soft}(\\mu_i, \\lambda\\tau^2) = \\text{sgn}(\\mu_i) \\max(|\\mu_i| - \\lambda\\tau^2, 0)\n$$\n\n**情况2：观测索引 $i \\in \\{1, 3, 5\\}$**\n我们将测量索引 $j$ 与状态索引 $i$ 关联如下：$j=1 \\leftrightarrow i=1$, $j=2 \\leftrightarrow i=3$, $j=3 \\leftrightarrow i=5$。坐标 $x_i$ 的目标函数是：\n$$\nJ_i(x_i) = \\frac{1}{2\\sigma^2}(y_j - x_i)^2 + \\frac{1}{2\\tau^2}(x_i - \\mu_i)^2 + \\lambda|x_i|\n$$\n二次部分是 $\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)x_i^2 - 2\\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu_i}{\\tau^2}\\right)x_i \\right] + \\text{const}$。通过配方法，该式与 $(x_i - z_{i,0})^2$ 成正比，其中\n$$\nz_{i,0} = \\frac{y_j/\\sigma^2 + \\mu_i/\\tau^2}{1/\\sigma^2 + 1/\\tau^2} = \\frac{\\tau^2 y_j + \\sigma^2 \\mu_i}{\\sigma^2 + \\tau^2}\n$$\n并且平方项的系数为 $\\frac{1}{2\\gamma_{obs}^2}$，其中 $\\frac{1}{\\gamma_{obs}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}$，所以 $\\gamma_{obs}^2 = \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}$。\n对 $x_i$ 的最小化问题等价于最小化 $\\frac{1}{2\\gamma_{obs}^2}(x_i-z_{i,0})^2 + \\lambda|x_i|$。解同样由软阈值给出：\n$$\nx_i^\\star = \\text{soft}(z_{i,0}, \\lambda\\gamma_{obs}^2) = \\text{sgn}(z_{i,0}) \\max(|z_{i,0}| - \\lambda\\gamma_{obs}^2, 0)\n$$\n\n**数值计算**\n给定的参数值为：\n$\\sigma^2 = 0.04$，$\\tau^2 = 0.25$，$\\lambda = 0.15$。\n数据为：\n$\\mu=\\begin{pmatrix}0.02 \\\\ -0.08 \\\\ 0 \\\\ 0.03 \\\\ -0.01\\end{pmatrix}$ 和 $y=\\begin{pmatrix}0.10\\\\ -0.09\\\\ 0.02\\end{pmatrix}$。\n\n首先，我们计算两种情况下的阈值：\n未观测索引的阈值：$T_{unobs} = \\lambda\\tau^2 = 0.15 \\times 0.25 = 0.0375$。\n观测索引的阈值：$T_{obs} = \\lambda\\gamma_{obs}^2 = \\lambda \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2} = 0.15 \\times \\frac{0.04 \\times 0.25}{0.04+0.25} = 0.15 \\times \\frac{0.01}{0.29} = \\frac{0.0015}{0.29}$。\n\n现在我们计算 $x^\\star$ 的每个分量：\n-   **$i=2$ (未观测):** $x_2^\\star = \\text{soft}(\\mu_2, T_{unobs}) = \\text{soft}(-0.08, 0.0375)$。因为 $|\\mu_2| = 0.08 > 0.0375$，所以 $x_2^\\star = - (0.08 - 0.0375) = -0.0425$。\n-   **$i=4$ (未观测):** $x_4^\\star = \\text{soft}(\\mu_4, T_{unobs}) = \\text{soft}(0.03, 0.0375)$。因为 $|\\mu_4| = 0.03  0.0375$，所以 $x_4^\\star = 0$。\n\n-   **$i=1$ (观测, $j=1$):**\n    $z_{1,0} = \\frac{\\tau^2 y_1 + \\sigma^2 \\mu_1}{\\sigma^2+\\tau^2} = \\frac{0.25(0.10) + 0.04(0.02)}{0.29} = \\frac{0.025 + 0.0008}{0.29} = \\frac{0.0258}{0.29}$。\n    $|z_{1,0}| = \\frac{0.0258}{0.29}  T_{obs} = \\frac{0.0015}{0.29}$。\n    $x_1^\\star = z_{1,0} - T_{obs} = \\frac{0.0258}{0.29} - \\frac{0.0015}{0.29} = \\frac{0.0243}{0.29} = \\frac{243}{2900}$。\n\n-   **$i=3$ (观测, $j=2$):**\n    $z_{3,0} = \\frac{\\tau^2 y_2 + \\sigma^2 \\mu_3}{\\sigma^2+\\tau^2} = \\frac{0.25(-0.09) + 0.04(0)}{0.29} = \\frac{-0.0225}{0.29}$。\n    $|z_{3,0}| = \\frac{0.0225}{0.29}  T_{obs} = \\frac{0.0015}{0.29}$。\n    $x_3^\\star = z_{3,0} + T_{obs} = \\frac{-0.0225}{0.29} + \\frac{0.0015}{0.29} = \\frac{-0.021}{0.29} = -\\frac{21}{290}$。\n\n-   **$i=5$ (观测, $j=3$):**\n    $z_{5,0} = \\frac{\\tau^2 y_3 + \\sigma^2 \\mu_5}{\\sigma^2+\\tau^2} = \\frac{0.25(0.02) + 0.04(-0.01)}{0.29} = \\frac{0.005 - 0.0004}{0.29} = \\frac{0.0046}{0.29}$。\n    $|z_{5,0}| = \\frac{0.0046}{0.29}  T_{obs} = \\frac{0.0015}{0.29}$。\n    $x_5^\\star = z_{5,0} - T_{obs} = \\frac{0.0046}{0.29} - \\frac{0.0015}{0.29} = \\frac{0.0031}{0.29} = \\frac{31}{2900}$。\n\n得到的 MAP 估计为 $x^\\star = \\begin{pmatrix} 243/2900 \\\\ -0.0425 \\\\ -21/290 \\\\ 0 \\\\ 31/2900 \\end{pmatrix}$。除 $x_4^\\star$ 外，所有分量都非零。\n\n**支撑集变化分析**\n我们需要找到 $\\mu$ 和 $x^\\star$ 支撑集的对称差：\n$\\Delta=\\{i:\\ (\\mu_i=0\\ \\text{and}\\ x^\\star_i\\neq 0)\\ \\text{or}\\ (\\mu_i\\neq 0\\ \\text{and}\\ x^\\star_i=0)\\}$.\n\n我们逐个检查每个索引：\n-   $i=1$: $\\mu_1 = 0.02 \\neq 0$ 且 $x_1^\\star = \\frac{243}{2900} \\neq 0$。无变化。\n-   $i=2$: $\\mu_2 = -0.08 \\neq 0$ 且 $x_2^\\star = -0.0425 \\neq 0$。无变化。\n-   $i=3$: $\\mu_3 = 0$ 且 $x_3^\\star = -\\frac{21}{290} \\neq 0$。一个零元素变为非零。$3 \\in \\Delta$。\n-   $i=4$: $\\mu_4 = 0.03 \\neq 0$ 且 $x_4^\\star = 0$。一个非零元素变为零。$4 \\in \\Delta$。\n-   $i=5$: $\\mu_5 = -0.01 \\neq 0$ 且 $x_5^\\star = \\frac{31}{2900} \\neq 0$。无变化。\n\n支撑集发生变化的索引集合是 $\\Delta = \\{3, 4\\}$。\n这个集合的基数是 $|\\Delta| = 2$。", "answer": "$$\\boxed{2}$$", "id": "3445409"}, {"introduction": "现实世界中的系统通常是非线性的，这对标准的线性滤波器构成了挑战。本练习 ([@problem_id:3445444]) 介绍了扩展卡尔曼滤波器（EKF）框架，这是一种通过在每个时间步对系统进行线性化来处理非线性的强大技术。你将练习计算雅可比矩阵，并将其整合到稀疏感知更新中，从而在理论与实际的非线性应用之间架起一座桥梁。", "problem": "给定时间索引 $t$ 处的一个非线性量测函数，定义为 $h_t(x) = \\begin{bmatrix} \\sin(a^\\top x) \\\\ b^\\top x \\end{bmatrix}$，其中 $a \\in \\mathbb{R}^n$ 和 $b \\in \\mathbb{R}^n$ 是已知的， $x \\in \\mathbb{R}^n$ 是状态。在一个动态压缩感知设定中，考虑一个带有稀疏性感知的正则化的扩展卡尔曼滤波器（EKF）的一个时间步。从先验均值 $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$ 和先验协方差 $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$ 开始，并给定一个协方差为 $R_t \\in \\mathbb{R}^{2 \\times 2}$ 的量测 $y_t \\in \\mathbb{R}^2$，执行一个使用权重为 $\\lambda \\ge 0$ 的 $\\ell_1$ 惩罚项来促进稀疏性的单一正则化 EKF 更新。\n\n目标是计算 $h_t$ 在 $\\hat{x}_{t|t-1}$ 处的雅可比矩阵 $H_t$，并对通过在 $\\hat{x}_{t|t-1}$ 处线性化量测函数得到的最大后验（MAP）估计器执行一个近端梯度步。该近端梯度步对应于稀疏感知 EKF 的一次数值更新。\n\n您的推导和算法应基于以下公认的原则：\n\n- 扩展卡尔曼滤波器（EKF）使用一阶泰勒近似，将非线性量测函数 $h_t(x)$ 在先验估计 $\\hat{x}_{t|t-1}$ 周围线性化。\n- 在高斯先验 $x \\sim \\mathcal{N}(\\hat{x}_{t|t-1}, P_{t|t-1})$ 和协方差为 $R_t$ 的高斯量测噪声下，线性化点的负对数后验是二次项之和。\n- 一个 $\\ell_1$ 范数惩罚项 $\\lambda \\lVert x \\rVert_1$ 对应于一个拉普拉斯先验，并导致一个凸复合目标函数，该函数可以通过使用软阈值算子的近端梯度步来最小化。\n\n您的任务是：\n\n- 从第一性原理出发，推导 $h_t(x)$ 在 $x = \\hat{x}_{t|t-1}$ 处的雅可比矩阵 $H_t$。\n- 在 $x = \\hat{x}_{t|t-1}$ 周围线性化量测方程，并形成线性化残差 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$。\n- 定义线性化负对数后验的平滑部分 $g(x)$：\n  $$g(x) = \\frac{1}{2}\\left(r_t - H_t (x - \\hat{x}_{t|t-1})\\right)^\\top R_t^{-1}\\left(r_t - H_t (x - \\hat{x}_{t|t-1})\\right) + \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1}).$$\n- 从 $x^{(0)} = \\hat{x}_{t|t-1}$ 开始，使用步长 $\\alpha = 1/L$ 计算一次近端梯度更新，其中 $L$ 是对称正定矩阵 $H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$ 的最大特征值。更新公式为：\n  $$x^{(1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}\\left(x^{(0)} - \\alpha \\nabla g(x^{(0)})\\right),$$\n  其中 $\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)$ 是逐元素的软阈值算子，由 $\\left[\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)\\right]_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\tau, 0\\}$ 给出。\n- 对每个提供的测试用例，输出 $x^{(1)}$。\n\n实现以下明确的测试套件。在所有情况下，状态维度为 $n = 3$，量测维度为 $2$。所有矩阵都是对称的，所有协方差矩阵都是严格正定的。\n\n测试用例 #1（一般情况）：\n- $a = \\begin{bmatrix} 0.7 \\\\ -1.2 \\\\ 0.5 \\end{bmatrix}$， $b = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ -0.5 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 0.5  0.1  0.0 \\\\ 0.1  0.3  0.05 \\\\ 0.0  0.05  0.8 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.4 \\\\ -0.15 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$。\n- $\\lambda = 0.15$。\n\n测试用例 #2（雅可比分量接近零且无稀疏性）：\n- $a = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.25 \\end{bmatrix}$， $b = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.6 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 1.4 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 0.2  0.02  0.0 \\\\ 0.02  0.4  0.01 \\\\ 0.0  0.01  0.6 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.8 \\\\ -0.05 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.02  0.0 \\\\ 0.0  0.2 \\end{bmatrix}$。\n- $\\lambda = 0.0$。\n\n测试用例 #3（无信息先验，量测主导）：\n- $a = \\begin{bmatrix} 0.3 \\\\ -0.8 \\\\ 0.25 \\end{bmatrix}$， $b = \\begin{bmatrix} -0.6 \\\\ 0.4 \\\\ 0.1 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 10.0  0.0  0.0 \\\\ 0.0  10.0  0.0 \\\\ 0.0  0.0  10.0 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.05 \\\\ 0.0 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.1  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$。\n- $\\lambda = 0.05$。\n\n测试用例 #4（强稀疏性压力）：\n- $a = \\begin{bmatrix} -0.5 \\\\ 1.5 \\\\ -1.0 \\end{bmatrix}$， $b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 0.0 \\end{bmatrix}$。\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\end{bmatrix}$。\n- $P_{t|t-1} = \\begin{bmatrix} 0.1  0.02  0.0 \\\\ 0.02  0.1  0.01 \\\\ 0.0  0.01  0.1 \\end{bmatrix}$。\n- $y_t = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$。\n- $R_t = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$。\n- $\\lambda = 1.0$。\n\n计算要求：\n\n- 严格根据 $h_t(x)$ 导数的定义，计算在 $x = \\hat{x}_{t|t-1}$ 处的雅可比矩阵 $H_t$。\n- 使用线性化残差 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$，其中 $h_t(\\hat{x}_{t|t-1})$ 按分量计算为 $h_t(\\hat{x}_{t|t-1}) = \\begin{bmatrix} \\sin(a^\\top \\hat{x}_{t|t-1}) \\\\ b^\\top \\hat{x}_{t|t-1} \\end{bmatrix}$。\n- 构建矩阵 $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$，并设置步长 $\\alpha = 1 / L$，其中 $L$ 是 $M$ 的最大特征值。\n- 计算梯度 $\\nabla g(\\hat{x}_{t|t-1})$，并使用软阈值算子执行一次近端梯度步。\n\n最终输出格式：\n\n- 对于每个测试用例，将更新后的状态 $x_{t|t}^{(1)}$ 输出为按 $x$ 的分量顺序排列的浮点数列表。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个条目对应于上面列出的一个测试用例，并且每个条目本身都是一个表示该情况下的 $x_{t|t}^{(1)}$ 的列表（例如，$\\left[ [\\dots], [\\dots], [\\dots], [\\dots] \\right]$）。", "solution": "该问题是有效的，因为它在科学上基于已建立的状态估计和优化原理，其设置完整且一致，问题陈述良好，并以客观、正式的语言表达。因此，我们可以进行推导和求解。\n\n问题的核心是为稀疏性感知的扩展卡尔曼滤波器（EKF）执行单步更新。此更新被构建为求解最大后验（MAP）估计问题的近端梯度算法的单步。该 MAP 问题的目标函数结合了线性化的量测模型、状态的高斯先验以及促进稀疏性的 $\\ell_1$ 正则化项。\n\n时间 $t$ 的状态由 $x_t \\in \\mathbb{R}^n$ 表示。系统通过非线性量测函数 $h_t(x_t)$ 进行观测，量测 $y_t \\in \\mathbb{R}^2$ 受到零均值高斯噪声的干扰，其协方差为 $R_t \\in \\mathbb{R}^{2 \\times 2}$。量测模型为：\n$$y_t = h_t(x_t) + v_t, \\quad v_t \\sim \\mathcal{N}(0, R_t)$$\n其中 $h_t(x) = \\begin{bmatrix} \\sin(a^\\top x) \\\\ b^\\top x \\end{bmatrix}$，对于已知的向量 $a, b \\in \\mathbb{R}^n$。\n\n关于状态的先验信念由均值为 $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$、协方差为 $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$ 的高斯分布给出，即 $x_t \\sim \\mathcal{N}(\\hat{x}_{t|t-1}, P_{t|t-1})$。\n\nEKF 的第一步是将非线性量测函数 $h_t(x)$ 在先验状态估计 $\\hat{x}_{t|t-1}$ 周围进行线性化。这通过一阶泰勒展开实现：\n$$h_t(x) \\approx h_t(\\hat{x}_{t|t-1}) + H_t (x - \\hat{x}_{t|t-1})$$\n其中 $H_t$ 是 $h_t(x)$ 在 $x = \\hat{x}_{t|t-1}$ 处求值的雅可比矩阵。让我们来推导这个雅可比矩阵。函数 $h_t(x)$ 有两个分量，$h_{t,1}(x) = \\sin(a^\\top x)$ 和 $h_{t,2}(x) = b^\\top x$。雅可比矩阵是一个 $2 \\times n$ 矩阵，其行是这些分量的梯度。\n\n使用链式法则，第一个分量的梯度为：\n$$\\nabla_x h_{t,1}(x) = \\nabla_x (\\sin(a^\\top x)) = \\cos(a^\\top x) \\cdot \\nabla_x(a^\\top x) = \\cos(a^\\top x) \\cdot a$$\n第二个分量的梯度为：\n$$\\nabla_x h_{t,2}(x) = \\nabla_x (b^\\top x) = b$$\n因此，雅可比矩阵为：\n$$H_t(x) = \\begin{bmatrix} (\\nabla_x h_{t,1}(x))^\\top \\\\ (\\nabla_x h_{t,2}(x))^\\top \\end{bmatrix} = \\begin{bmatrix} \\cos(a^\\top x) a^\\top \\\\ b^\\top \\end{bmatrix}$$\n对于 EKF 更新，我们在先验估计 $x = \\hat{x}_{t|t-1}$ 处对其求值：\n$$H_t = \\begin{bmatrix} \\cos(a^\\top \\hat{x}_{t|t-1}) a^\\top \\\\ b^\\top \\end{bmatrix}$$\n\nMAP 估计旨在寻找使后验概率最大化的状态 $x$，这等同于最小化负对数后验。利用高斯先验、高斯量测噪声和为促进稀疏性而增加的 $\\ell_1$ 惩罚项，要最小化的目标函数是：\n$$F(x) = \\frac{1}{2} (y_t - h_t^{\\text{lin}}(x))^\\top R_t^{-1} (y_t - h_t^{\\text{lin}}(x)) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}) + \\lambda \\lVert x \\rVert_1$$\n其中 $h_t^{\\text{lin}}(x)$ 是线性化的量测模型。让我们将线性化残差（或新息）定义为 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$。那么 $y_t - h_t^{\\text{lin}}(x) = y_t - (h_t(\\hat{x}_{t|t-1}) + H_t(x - \\hat{x}_{t|t-1})) = r_t - H_t(x - \\hat{x}_{t|t-1})$。目标函数可以写成 $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$，其中 $g(x)$ 是平滑部分：\n$$g(x) = \\frac{1}{2}(r_t - H_t (x - \\hat{x}_{t|t-1}))^\\top R_t^{-1}(r_t - H_t (x - \\hat{x}_{t|t-1})) + \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1})$$\n该目标函数是一个复合函数，由一个平滑可微部分 $g(x)$ 和一个非平滑凸部分 $\\lambda \\lVert x \\rVert_1$ 组成。这种结构非常适合近端梯度法。该方法的迭代更新规则是：\n$$x^{(k+1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}(x^{(k)} - \\alpha \\nabla g(x^{(k)}))$$\n其中 $\\alpha  0$ 是步长，$\\operatorname{prox}$ 是近端算子。\n\n问题要求从 $x^{(0)} = \\hat{x}_{t|t-1}$ 开始执行一次更新步骤。我们首先需要 $g(x)$ 的梯度：\n$$\\nabla g(x) = \\nabla_x \\left( \\frac{1}{2}(r_t - H_t (x - \\hat{x}_{t|t-1}))^\\top R_t^{-1}(r_t - H_t (x - \\hat{x}_{t|t-1})) \\right) + \\nabla_x \\left( \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1}) \\right)$$\n$$\\nabla g(x) = -H_t^\\top R_t^{-1}(r_t - H_t(x - \\hat{x}_{t|t-1})) + P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1})$$\n在起始点 $x^{(0)} = \\hat{x}_{t|t-1}$ 处计算梯度：\n$$\\nabla g(\\hat{x}_{t|t-1}) = -H_t^\\top R_t^{-1}(r_t - H_t(\\hat{x}_{t|t-1} - \\hat{x}_{t|t-1})) + P_{t|t-1}^{-1}(\\hat{x}_{t|t-1} - \\hat{x}_{t|t-1}) = -H_t^\\top R_t^{-1} r_t$$\n近端算子的参数则为：\n$$z = x^{(0)} - \\alpha \\nabla g(x^{(0)}) = \\hat{x}_{t|t-1} - \\alpha(-H_t^\\top R_t^{-1} r_t) = \\hat{x}_{t|t-1} + \\alpha H_t^\\top R_t^{-1} r_t$$\n步长 $\\alpha$ 被选为梯度 $\\nabla g(x)$ 的利普希茨常数 $L$ 的倒数。利普希茨常数由 $g(x)$ 的海森矩阵的最大特征值给出：\n$$\\nabla^2 g(x) = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$$\n令 $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$。由于 $P_{t|t-1}$ 和 $R_t$ 是对称正定的， $M$ 也是对称正定的。利普希茨常数是 $L = \\lambda_{\\max}(M)$，步长是 $\\alpha = 1/L$。\n\n最后一部分是 $\\ell_1$ 范数的近端算子，即逐元素的软阈值算子。对于阈值 $\\tau \\ge 0$，其定义为：\n$$[\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)]_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\tau, 0\\}$$\n在我们的情况下，阈值是 $\\tau = \\alpha \\lambda$。因此，单步更新为：\n$$x^{(1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}(z)$$\n\n总结来说，对于每个测试用例，我们执行以下计算：\n1.  计算标量积 $s_a = a^\\top \\hat{x}_{t|t-1}$ 和 $s_b = b^\\top \\hat{x}_{t|t-1}$。\n2.  计算雅可比矩阵 $H_t = \\begin{bmatrix} \\cos(s_a) a^\\top \\\\ b^\\top \\end{bmatrix}$。\n3.  计算预测量测 $h_t(\\hat{x}_{t|t-1}) = [\\sin(s_a), s_b]^\\top$。\n4.  计算线性化残差 $r_t = y_t - h_t(\\hat{x}_{t|t-1})$。\n5.  计算矩阵的逆 $R_t^{-1}$ 和 $P_{t|t-1}^{-1}$。\n6.  构建矩阵 $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$。\n7.  找到 $M$ 的最大特征值 $L$，即 $L = \\lambda_{\\max}(M)$。\n8.  设置步长 $\\alpha = 1/L$。\n9.  计算中间向量 $z = \\hat{x}_{t|t-1} + \\alpha H_t^\\top R_t^{-1} r_t$。\n10. 设置阈值 $\\tau = \\alpha \\lambda$。\n11. 通过对 $z$ 的每个分量应用阈值为 $\\tau$ 的软阈值算子，来计算更新后的状态 $x^{(1)}$。\n这个过程为稀疏性感知的 EKF 的一步提供了所需的状态估计。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a single proximal-gradient update for a sparsity-aware Extended Kalman Filter\n    for four given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": np.array([0.7, -1.2, 0.5]),\n            \"b\": np.array([1.0, 0.0, -0.5]),\n            \"x_prior\": np.array([0.2, -0.3, 0.1]),\n            \"P_prior\": np.array([[0.5, 0.1, 0.0], [0.1, 0.3, 0.05], [0.0, 0.05, 0.8]]),\n            \"y\": np.array([0.4, -0.15]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.1]]),\n            \"lambda_\": 0.15\n        },\n        {\n            \"a\": np.array([1.0, 0.5, -0.25]),\n            \"b\": np.array([0.2, -0.3, 0.6]),\n            \"x_prior\": np.array([1.4, 0.2, 0.0]),\n            \"P_prior\": np.array([[0.2, 0.02, 0.0], [0.02, 0.4, 0.01], [0.0, 0.01, 0.6]]),\n            \"y\": np.array([0.8, -0.05]),\n            \"R\": np.array([[0.02, 0.0], [0.0, 0.2]]),\n            \"lambda_\": 0.0\n        },\n        {\n            \"a\": np.array([0.3, -0.8, 0.25]),\n            \"b\": np.array([-0.6, 0.4, 0.1]),\n            \"x_prior\": np.array([0.0, 0.0, 0.0]),\n            \"P_prior\": np.array([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]),\n            \"y\": np.array([0.05, 0.0]),\n            \"R\": np.array([[0.1, 0.0], [0.0, 0.1]]),\n            \"lambda_\": 0.05\n        },\n        {\n            \"a\": np.array([-0.5, 1.5, -1.0]),\n            \"b\": np.array([0.0, 1.0, 0.0]),\n            \"x_prior\": np.array([0.05, -0.02, 0.03]),\n            \"P_prior\": np.array([[0.1, 0.02, 0.0], [0.02, 0.1, 0.01], [0.0, 0.01, 0.1]]),\n            \"y\": np.array([0.0, 0.0]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"lambda_\": 1.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        b = case[\"b\"]\n        x_prior = case[\"x_prior\"]\n        P_prior = case[\"P_prior\"]\n        y = case[\"y\"]\n        R = case[\"R\"]\n        lambda_ = case[\"lambda_\"]\n\n        # 1. Compute scalar products\n        a_T_x_prior = a.T @ x_prior\n        b_T_x_prior = b.T @ x_prior\n\n        # 2. Compute Jacobian H_t\n        H_t = np.vstack([np.cos(a_T_x_prior) * a.T, b.T])\n\n        # 3. Compute predicted measurement and linearized residual r_t\n        h_x_prior = np.array([np.sin(a_T_x_prior), b_T_x_prior])\n        r_t = y - h_x_prior\n\n        # 4. Compute matrix inverses\n        R_inv = np.linalg.inv(R)\n        P_prior_inv = np.linalg.inv(P_prior)\n\n        # 5. Form the Hessian matrix M of the smooth part of the objective\n        M = H_t.T @ R_inv @ H_t + P_prior_inv\n\n        # 6. Find the largest eigenvalue L of M\n        # M is symmetric positive definite, use eigvalsh for numerical stability and efficiency.\n        eigenvalues = np.linalg.eigvalsh(M)\n        L = np.max(eigenvalues)\n\n        # 7. Set step size alpha\n        alpha = 1.0 / L\n\n        # 8. Compute the argument for the proximal operator\n        # z = x^(0) - alpha * grad(g(x^(0)))\n        grad_g_x0 = -H_t.T @ R_inv @ r_t\n        z = x_prior - alpha * grad_g_x0\n\n        # 9. Set threshold tau for soft-thresholding\n        tau = alpha * lambda_\n\n        # 10. Compute updated state x^(1) via soft-thresholding\n        x_updated = np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n        \n        results.append(x_updated.tolist())\n\n    # Format the final output string to match the required format: [[...],[...],...]\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3445444"}, {"introduction": "超越单步更新，本实践 ([@problem_id:3445469]) 将处理一次性估计整个状态序列的问题，这一任务被称为批量估计或平滑。你将为一个多步目标函数实现一次近端梯度迭代，从而揭示过程模型如何在不同时间的状态之间创建依赖关系。这个练习对于理解如何利用整个测量历史信息以实现更准确的稀疏状态恢复至关重要。", "problem": "考虑一个三步线性动力学系统，该系统常用于稀疏感知卡尔曼滤波（Kalman Filter (KF)）和动态压缩感知。系统演化和测量由状态转移矩阵 $F \\in \\mathbb{R}^{n \\times n}$、步骤 $t \\in \\{1,2,3\\}$ 的测量矩阵 $H_t \\in \\mathbb{R}^{m \\times n}$、过程协方差 $Q \\in \\mathbb{R}^{n \\times n}$ 和测量协方差 $R_t \\in \\mathbb{R}^{m \\times m}$ 定义。状态为 $x_t \\in \\mathbb{R}^n$，先验为 $x_0 \\in \\mathbb{R}^n$，测量值为 $y_t \\in \\mathbb{R}^m$。在稀疏感知估计中，需要最小化一个复合目标函数，该函数包含一个二次数据保真项和一个凸稀疏诱导惩罚项，具体来说是 $\\ell_1$ 范数。平滑项源于经过充分检验的高斯建模事实：线性高斯模型的负对数似然函数会产生由协方差逆矩阵加权的二次型。近端梯度法（Proximal Gradient Method, PGM）对平滑部分执行前向梯度步，然后使用非平滑部分的邻近算子执行后向步。\n\n在堆叠状态向量 $X = (x_1, x_2, x_3)$ 上定义目标函数：\n$$\n\\min_{x_1,x_2,x_3 \\in \\mathbb{R}^n} \\; g(X) + \\lambda \\sum_{t=1}^3 \\|x_t\\|_1,\n$$\n其中\n$$\ng(X) = \\frac{1}{2} \\sum_{t=1}^3 \\left(H_t x_t - y_t\\right)^\\top R_t^{-1} \\left(H_t x_t - y_t\\right) + \\frac{1}{2} \\sum_{t=1}^3 \\left(x_t - F x_{t-1}\\right)^\\top Q^{-1} \\left(x_t - F x_{t-1}\\right).\n$$\n所有协方差均为对称正定矩阵，$Q^{-1}$ 和 $R_t^{-1}$ 表示矩阵的逆。从给定的初始值 $X^{(0)}$ 开始，使用指定的步长 $\\gamma  0$，执行一次近端梯度迭代。前向传播计算 $X^{\\text{tmp}} = X^{(0)} - \\gamma \\nabla g\\left(X^{(0)}\\right)$。后向传播将 $\\ell_1$ 范数的邻近算子独立地应用于每个分量，即使用阈值 $\\gamma \\lambda$ 的软阈值算子。具体来说，对于每个标量分量 $z$，软阈值 $\\operatorname{soft}(z; \\tau)$ 定义为\n$$\n\\operatorname{soft}(z; \\tau) = \\operatorname{sign}(z) \\cdot \\max\\{|z| - \\tau, 0\\}.\n$$\n计算 $X^{(1)} = \\operatorname{soft}\\left(X^{\\text{tmp}}; \\gamma \\lambda\\right)$。\n\n您的任务是为以下测试套件实现这个单次近端梯度迭代。所有量的维度均为 $n = 4$ 和 $m = 3$。步长 $\\gamma$ 和正则化权重 $\\lambda$ 因测试用例而异。在所有情况下，过程协方差 $Q$ 和测量协方差 $R_t$ 都是对角矩阵。\n\n通用模型参数（用于所有情况）：\n- 状态转移矩阵 $F \\in \\mathbb{R}^{4 \\times 4}$：\n$$\nF = \\begin{bmatrix}\n0.9  0.1  0  0 \\\\\n0  0.95  0.05  0 \\\\\n0  0  0.9  0.1 \\\\\n0  0  0  0.85\n\\end{bmatrix}.\n$$\n- 测量矩阵 $H_1,H_2,H_3 \\in \\mathbb{R}^{3 \\times 4}$：\n$$\nH_1 = \\begin{bmatrix}\n1  0  -0.5  0 \\\\\n0  1  0.2  0 \\\\\n0.3  0  0  1\n\\end{bmatrix}, \\quad\nH_2 = \\begin{bmatrix}\n0.9  -0.2  0  0.1 \\\\\n0  0.6  0.3  -0.1 \\\\\n0.4  0  0.5  0.2\n\\end{bmatrix}, \\quad\nH_3 = \\begin{bmatrix}\n0.5  0.2  0  -0.3 \\\\\n0.1  0.7  -0.4  0 \\\\\n0  0.3  0.6  0.5\n\\end{bmatrix}.\n$$\n- 过程协方差 $Q \\in \\mathbb{R}^{4 \\times 4}$ 及其逆矩阵：\n$$\nQ = \\operatorname{diag}(0.4, 0.6, 0.5, 0.7), \\quad Q^{-1} = \\operatorname{diag}\\left(2.5, \\frac{5}{3}, 2, \\frac{10}{7}\\right).\n$$\n- 测量协方差 $R_t \\in \\mathbb{R}^{3 \\times 3}$ 及其逆矩阵：\n$$\nR_1 = \\operatorname{diag}(0.2, 0.3, 0.25), \\quad R_1^{-1} = \\operatorname{diag}(5, \\tfrac{10}{3}, 4),\n$$\n$$\nR_2 = \\operatorname{diag}(0.35, 0.4, 0.3), \\quad R_2^{-1} = \\operatorname{diag}\\left(\\tfrac{20}{7}, \\tfrac{5}{2}, \\tfrac{10}{3}\\right),\n$$\n$$\nR_3 = \\operatorname{diag}(0.5, 0.45, 0.4), \\quad R_3^{-1} = \\operatorname{diag}\\left(2, \\tfrac{20}{9}, \\tfrac{5}{2}\\right).\n$$\n- 先验状态 $x_0 \\in \\mathbb{R}^4$ 和测量值 $y_t \\in \\mathbb{R}^3$：\n$$\nx_0 = \\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0 \\\\ 0.3\\end{bmatrix}, \\quad\ny_1 = \\begin{bmatrix}0.9 \\\\ -0.3 \\\\ 0.5\\end{bmatrix}, \\quad\ny_2 = \\begin{bmatrix}0.1 \\\\ 0.4 \\\\ -0.2\\end{bmatrix}, \\quad\ny_3 = \\begin{bmatrix}-0.5 \\\\ 0.2 \\\\ 0.3\\end{bmatrix}.\n$$\n\n测试用例：\n- 情况 $1$（具有中等稀疏性的常规情况）：\n  - 初始状态 $X^{(0)}$： $x_1^{(0)} = \\begin{bmatrix}0.05 \\\\ -0.02 \\\\ 0.1 \\\\ 0\\end{bmatrix}$, $x_2^{(0)} = \\begin{bmatrix}0 \\\\ 0.03 \\\\ -0.04 \\\\ 0.02\\end{bmatrix}$, $x_3^{(0)} = \\begin{bmatrix}-0.01 \\\\ 0 \\\\ 0 \\\\ 0.05\\end{bmatrix}$。\n  - 步长 $\\gamma = 0.15$。\n  - 正则化权重 $\\lambda = 0.05$。\n- 情况 $2$（无稀疏性惩罚的边界情况）：\n  - 初始状态 $X^{(0)}$： $x_1^{(0)} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$, $x_2^{(0)} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$, $x_3^{(0)} = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix}$。\n  - 步长 $\\gamma = 0.2$。\n  - 正则化权重 $\\lambda = 0$。\n- 情况 $3$（具有强稀疏性压力的边缘情况）：\n  - 初始状态 $X^{(0)}$： $x_1^{(0)} = \\begin{bmatrix}0.2 \\\\ -0.3 \\\\ 0.1 \\\\ -0.1\\end{bmatrix}$, $x_2^{(0)} = \\begin{bmatrix}0.4 \\\\ 0 \\\\ -0.2 \\\\ 0.2\\end{bmatrix}$, $x_3^{(0)} = \\begin{bmatrix}-0.5 \\\\ 0.1 \\\\ 0.3 \\\\ -0.4\\end{bmatrix}$。\n  - 步长 $\\gamma = 0.15$。\n  - 正则化权重 $\\lambda = 2$。\n\n为每个测试用例实现一次近端梯度迭代，并按如下方式输出结果 $X^{(1)}$。对于每种情况，将 $X^{(1)}$ 展平成一个包含12个实数的列表，顺序为 $\\left[x_1^{(1)}; x_2^{(1)}; x_3^{(1)}\\right]$（将从 $t=1$ 到 $t=3$ 的状态连接起来）。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个案例本身也是一个方括号列表。例如，最终输出格式必须为\n$$\n\\text{[ [\\text{case1\\_vals}], [\\text{case2\\_vals}], [\\text{case3\\_vals}] ]}.\n$$", "solution": "用户请求了一个动态压缩感知和稀疏优化领域问题的解决方案。第一步是问题的形式化验证。\n\n### 步骤 1：提取已知条件\n\n问题提供了以下数据、定义和条件：\n- **系统模型**：一个三步线性动力学系统 ($t \\in \\{1,2,3\\}$)。\n- **维度**：状态向量大小 $n=4$，测量向量大小 $m=3$。\n- **状态转移**：$F \\in \\mathbb{R}^{4 \\times 4} = \\begin{bmatrix} 0.9  0.1  0  0 \\\\ 0  0.95  0.05  0 \\\\ 0  0  0.9  0.1 \\\\ 0  0  0  0.85 \\end{bmatrix}$。\n- **测量矩阵**：$H_1, H_2, H_3 \\in \\mathbb{R}^{3 \\times 4}$。\n  - $H_1 = \\begin{bmatrix} 1  0  -0.5  0 \\\\ 0  1  0.2  0 \\\\ 0.3  0  0  1 \\end{bmatrix}$。\n  - $H_2 = \\begin{bmatrix} 0.9  -0.2  0  0.1 \\\\ 0  0.6  0.3  -0.1 \\\\ 0.4  0  0.5  0.2 \\end{bmatrix}$。\n  - $H_3 = \\begin{bmatrix} 0.5  0.2  0  -0.3 \\\\ 0.1  0.7  -0.4  0 \\\\ 0  0.3  0.6  0.5 \\end{bmatrix}$。\n- **协方差矩阵及其逆矩阵**：\n  - 过程：$Q = \\operatorname{diag}(0.4, 0.6, 0.5, 0.7)$, $Q^{-1} = \\operatorname{diag}(2.5, \\frac{5}{3}, 2, \\frac{10}{7})$。\n  - $t=1$ 时的测量：$R_1 = \\operatorname{diag}(0.2, 0.3, 0.25)$, $R_1^{-1} = \\operatorname{diag}(5, \\frac{10}{3}, 4)$。\n  - $t=2$ 时的测量：$R_2 = \\operatorname{diag}(0.35, 0.4, 0.3)$, $R_2^{-1} = \\operatorname{diag}(\\frac{20}{7}, \\frac{5}{2}, \\frac{10}{3})$。\n  - $t=3$ 时的测量：$R_3 = \\operatorname{diag}(0.5, 0.45, 0.4)$, $R_3^{-1} = \\operatorname{diag}(2, \\frac{20}{9}, \\frac{5}{2})$。\n- **先验和测量**：\n  - 先验状态：$x_0 = [0.2, -0.1, 0, 0.3]^\\top$。\n  - 测量值：$y_1 = [0.9, -0.3, 0.5]^\\top$, $y_2 = [0.1, 0.4, -0.2]^\\top$, $y_3 = [-0.5, 0.2, 0.3]^\\top$。\n- **目标函数**：$\\min_{x_1,x_2,x_3 \\in \\mathbb{R}^n} \\; g(X) + \\lambda \\sum_{t=1}^3 \\|x_t\\|_1$，其中 $X = (x_1, x_2, x_3)$ 且\n$g(X) = \\frac{1}{2} \\sum_{t=1}^3 \\left(H_t x_t - y_t\\right)^\\top R_t^{-1} \\left(H_t x_t - y_t\\right) + \\frac{1}{2} \\sum_{t=1}^3 \\left(x_t - F x_{t-1}\\right)^\\top Q^{-1} \\left(x_t - F x_{t-1}\\right)$。\n- **算法**：一次近端梯度法（PGM）迭代。\n  - 前向步：$X^{\\text{tmp}} = X^{(0)} - \\gamma \\nabla g\\left(X^{(0)}\\right)$。\n  - 后向步：$X^{(1)} = \\operatorname{soft}\\left(X^{\\text{tmp}}; \\gamma \\lambda\\right)$，其中 $\\operatorname{soft}(z; \\tau) = \\operatorname{sign}(z) \\cdot \\max\\{|z| - \\tau, 0\\}$。\n- **测试用例**：\n  - **情况 1**：$\\gamma = 0.15$, $\\lambda = 0.05$。$x_1^{(0)} = [0.05, -0.02, 0.1, 0]^\\top$, $x_2^{(0)} = [0, 0.03, -0.04, 0.02]^\\top$, $x_3^{(0)} = [-0.01, 0, 0, 0.05]^\\top$。\n  - **情况 2**：$\\gamma = 0.2$, $\\lambda = 0$。$x_1^{(0)} = x_2^{(0)} = x_3^{(0)} = [0, 0, 0, 0]^\\top$。\n  - **情况 3**：$\\gamma = 0.15$, $\\lambda = 2$。$x_1^{(0)} = [0.2, -0.3, 0.1, -0.1]^\\top$, $x_2^{(0)} = [0.4, 0, -0.2, 0.2]^\\top$, $x_3^{(0)} = [-0.5, 0.1, 0.3, -0.4]^\\top$。\n- **输出格式**：单行 `[ [case1_vals], [case2_vals], [case3_vals] ]`，其中 `case_vals` 是一个逗号分隔的12个数字列表，来自展平的 $X^{(1)} = [x_1^{(1)}; x_2^{(1)}; x_3^{(1)}]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n- **科学依据**：该问题在信号处理、优化和控制理论的原理上有充分的依据。目标函数表示在线性高斯状态空间模型下，具有拉普拉斯（即 $\\ell_1$）先验的状态序列的负对数后验密度，这是在动态估计问题中鼓励稀疏性的标准公式。近端梯度法是解决此类复合优化问题的典型算法。所有数学公式都是标准且正确的。\n- **适定性**：该问题是适定的。它要求一个单一的、确定性的计算步骤。由于 $Q$ 和 $R_t$ 被指定为正定矩阵，确保了 $g(X)$ 是严格凸的，并且 $\\ell_1$ 范数是凸的，因此目标函数是严格凸的。这保证了存在唯一的最小值点。执行一次 PGM 迭代的任务是明确的。\n- **客观性**：该问题以精确、客观的数学语言陈述。所有参数、变量和常量都已明确定义。没有主观或基于意见的元素。\n- **不完整或矛盾的设置**：该问题是自洽的。为每种情况都提供了所有矩阵、向量和标量参数（$\\gamma, \\lambda$）。维度（$n=4, m=3$）在所有矩阵和向量运算中保持一致。所提供的对角协方差矩阵的逆在算术上是正确的。\n- **不切实际或不可行**：该问题是一个数值练习。这些值是抽象的，不代表物理系统，因此物理真实性的问题不适用。该设置在数学上是可行的。\n\n### 步骤 3：结论与行动\n\n问题陈述是**有效的**。它在科学上是合理的、适定的、客观的和完整的。我现在将着手提供完整的解决方案。\n\n### 解决方案\n\n任务是为给定的目标函数计算一次近端梯度法（PGM）的迭代。该迭代由两个步骤定义：对目标函数的光滑部分 $g(X)$ 进行梯度下降步，然后应用非光滑 $\\ell_1$ 项的邻近算子。\n\n对于堆叠的状态向量 $X = (x_1, x_2, x_3)$，PGM 更新规则为：\n$$\nX^{(1)} = \\operatorname{soft}\\left(X^{(0)} - \\gamma \\nabla g\\left(X^{(0)}\\right); \\gamma \\lambda\\right)\n$$\n其中 `soft` 是逐元素的软阈值算子。这可以按状态块分解：\n$$\nx_t^{(1)} = \\operatorname{soft}\\left(x_t^{(0)} - \\gamma \\nabla_{x_t} g\\left(X^{(0)}\\right); \\gamma \\lambda\\right) \\quad \\text{for } t=1,2,3.\n$$\n\n第一个关键步骤是推导平滑函数 $g(X)$ 相对于每个状态向量 $x_t$ 的梯度。函数 $g(X)$ 为：\n$$\ng(X) = \\frac{1}{2} \\sum_{t=1}^3 \\| H_t x_t - y_t \\|_{R_t^{-1}}^2 + \\frac{1}{2} \\sum_{t=1}^3 \\| x_t - F x_{t-1} \\|_{Q^{-1}}^2,\n$$\n这里我们使用了记号 $\\|v\\|_M^2 = v^\\top M v$。梯度 $\\nabla g(X)$ 是由偏梯度 $\\nabla_{x_t} g(X)$ 组成的堆叠向量。\n\n为了找到 $\\nabla_{x_t} g(X)$，我们识别出 $g(X)$ 中所有依赖于 $x_t$ 的项。对于一个中间状态 $x_t$（其中 $t \\in \\{1, 2\\}$），它出现在三个二次项中：\n1. 在时间 $t$ 的测量项：$\\frac{1}{2} (H_t x_t - y_t)^\\top R_t^{-1} (H_t x_t - y_t)$。\n2. 在时间 $t$ 的过程模型项：$\\frac{1}{2} (x_t - F x_{t-1})^\\top Q^{-1} (x_t - F x_{t-1})$。\n3. 在时间 $t+1$ 的过程模型项：$\\frac{1}{2} (x_{t+1} - F x_t)^\\top Q^{-1} (x_{t+1} - F x_t)$。\n\n使用向量微积分的标准法则，特别是 $\\nabla_z \\frac{1}{2} (Az-b)^\\top C (Az-b) = A^\\top C (Az-b)$，一个形如 $\\frac{1}{2} (d - Bz)^\\top C (d-Bz)$ 的项的梯度为 $-B^\\top C(d-Bz) = B^\\top C(Bz-d)$。我们对这三项分别关于 $x_t$ 求导：\n1. $\\nabla_{x_t} \\left(\\frac{1}{2} \\| H_t x_t - y_t \\|_{R_t^{-1}}^2\\right) = H_t^\\top R_t^{-1} (H_t x_t - y_t)$。\n2. $\\nabla_{x_t} \\left(\\frac{1}{2} \\| x_t - F x_{t-1} \\|_{Q^{-1}}^2\\right) = Q^{-1} (x_t - F x_{t-1})$。\n3. $\\nabla_{x_t} \\left(\\frac{1}{2} \\| x_{t+1} - F x_t \\|_{Q^{-1}}^2\\right) = -F^\\top Q^{-1} (x_{t+1} - F x_t) = F^\\top Q^{-1} (F x_t - x_{t+1})$。\n\n将这些贡献相加，得到中间状态 $x_1$ 和 $x_2$ 的梯度。最终状态 $x_3$ 的梯度更简单，因为它与未来的状态 $x_4$ 没有关联。\n\n在 $X^{(0)} = (x_1^{(0)}, x_2^{(0)}, x_3^{(0)})$ 处计算的显式梯度分量为：\n\n对于 $t=1$：\n$$ \\nabla_{x_1} g(X^{(0)}) = H_1^\\top R_1^{-1} (H_1 x_1^{(0)} - y_1) + Q^{-1} (x_1^{(0)} - F x_0) + F^\\top Q^{-1} (F x_1^{(0)} - x_2^{(0)}) $$\n\n对于 $t=2$：\n$$ \\nabla_{x_2} g(X^{(0)}) = H_2^\\top R_2^{-1} (H_2 x_2^{(0)} - y_2) + Q^{-1} (x_2^{(0)} - F x_1^{(0)}) + F^\\top Q^{-1} (F x_2^{(0)} - x_3^{(0)}) $$\n\n对于 $t=3$：\n$$ \\nabla_{x_3} g(X^{(0)}) = H_3^\\top R_3^{-1} (H_3 x_3^{(0)} - y_3) + Q^{-1} (x_3^{(0)} - F x_2^{(0)}) $$\n\n利用这些梯度表达式，对每个测试用例，PGM 迭代按以下方式进行：\n1.  **梯度步（前向传播）**：对于每个 $t \\in \\{1, 2, 3\\}$，使用上面的相应公式和给定情况的数值计算梯度 $\\nabla_{x_t} g(X^{(0)})$。然后，计算中间状态向量：\n    $$ x_t^{\\text{tmp}} = x_t^{(0)} - \\gamma \\nabla_{x_t} g(X^{(0)}) $$\n2.  **邻近步（后向传播）**：将逐元素的软阈值算子应用于每个中间向量 $x_t^{\\text{tmp}}$，阈值为 $\\tau = \\gamma \\lambda$：\n    $$ x_t^{(1)} = \\operatorname{soft}(x_t^{\\text{tmp}}; \\tau) $$\n    其中对于任何标量 $z$，$\\operatorname{soft}(z; \\tau) = \\operatorname{sign}(z) \\cdot \\max(|z| - \\tau, 0)$。\n\n对三个测试用例中的每一个都执行此过程。每个用例的最终结果是向量 $x_1^{(1)}、x_2^{(1)}$ 和 $x_3^{(1)}$ 的串联。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs one proximal gradient iteration for a dynamic sparse estimation problem.\n    \"\"\"\n\n    # Common model parameters\n    F = np.array([\n        [0.9, 0.1, 0, 0],\n        [0, 0.95, 0.05, 0],\n        [0, 0, 0.9, 0.1],\n        [0, 0, 0, 0.85]\n    ])\n    H1 = np.array([\n        [1, 0, -0.5, 0],\n        [0, 1, 0.2, 0],\n        [0.3, 0, 0, 1]\n    ])\n    H2 = np.array([\n        [0.9, -0.2, 0, 0.1],\n        [0, 0.6, 0.3, -0.1],\n        [0.4, 0, 0.5, 0.2]\n    ])\n    H3 = np.array([\n        [0.5, 0.2, 0, -0.3],\n        [0.1, 0.7, -0.4, 0],\n        [0, 0.3, 0.6, 0.5]\n    ])\n    Q_inv = np.diag([2.5, 5/3, 2, 10/7])\n    R1_inv = np.diag([5, 10/3, 4])\n    R2_inv = np.diag([20/7, 5/2, 10/3])\n    R3_inv = np.diag([2, 20/9, 5/2])\n    x0 = np.array([0.2, -0.1, 0, 0.3])\n    y1 = np.array([0.9, -0.3, 0.5])\n    y2 = np.array([0.1, 0.4, -0.2])\n    y3 = np.array([-0.5, 0.2, 0.3])\n\n    test_cases = [\n        {\n            \"gamma\": 0.15,\n            \"lambda_\": 0.05,\n            \"X0\": np.array([\n                [0.05, -0.02, 0.1, 0],        # x1_0\n                [0, 0.03, -0.04, 0.02],      # x2_0\n                [-0.01, 0, 0, 0.05]         # x3_0\n            ])\n        },\n        {\n            \"gamma\": 0.2,\n            \"lambda_\": 0.0,\n            \"X0\": np.array([\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0]\n            ])\n        },\n        {\n            \"gamma\": 0.15,\n            \"lambda_\": 2.0,\n            \"X0\": np.array([\n                [0.2, -0.3, 0.1, -0.1],\n                [0.4, 0, -0.2, 0.2],\n                [-0.5, 0.1, 0.3, -0.4]\n            ])\n        }\n    ]\n\n    def soft_threshold(z, tau):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    results = []\n    for case in test_cases:\n        gamma = case[\"gamma\"]\n        lambda_ = case[\"lambda_\"]\n        x1_0, x2_0, x3_0 = case[\"X0\"][0], case[\"X0\"][1], case[\"X0\"][2]\n\n        # Calculate gradients\n        grad_x1 = H1.T @ R1_inv @ (H1 @ x1_0 - y1) + \\\n                  Q_inv @ (x1_0 - F @ x0) + \\\n                  F.T @ Q_inv @ (F @ x1_0 - x2_0)\n        \n        grad_x2 = H2.T @ R2_inv @ (H2 @ x2_0 - y2) + \\\n                  Q_inv @ (x2_0 - F @ x1_0) + \\\n                  F.T @ Q_inv @ (F @ x2_0 - x3_0)\n\n        grad_x3 = H3.T @ R3_inv @ (H3 @ x3_0 - y3) + \\\n                  Q_inv @ (x3_0 - F @ x2_0)\n        \n        # Forward step (gradient descent)\n        x1_tmp = x1_0 - gamma * grad_x1\n        x2_tmp = x2_0 - gamma * grad_x2\n        x3_tmp = x3_0 - gamma * grad_x3\n\n        # Backward step (proximal mapping)\n        tau = gamma * lambda_\n        x1_1 = soft_threshold(x1_tmp, tau)\n        x2_1 = soft_threshold(x2_tmp, tau)\n        x3_1 = soft_threshold(x3_tmp, tau)\n\n        # Concatenate and store the result\n        X_1 = np.concatenate((x1_1, x2_1, x3_1))\n        results.append(X_1.tolist())\n\n    # Format the final output string\n    formatted_results = []\n    for res_list in results:\n        formatted_results.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3445469"}]}