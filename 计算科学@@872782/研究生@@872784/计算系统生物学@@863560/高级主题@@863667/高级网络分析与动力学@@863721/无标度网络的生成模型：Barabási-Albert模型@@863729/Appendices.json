{"hands_on_practices": [{"introduction": "Barabási-Albert 模型的核心在于其生成无标度网络的能力，该特性由幂律度分布定义。第一个实践提供了一个基础的理论练习：从第一性原理出发推导这个分布。通过应用主方程方法，您将揭示出表征这些成长中网络的著名的渐近关系 $P(k) \\sim k^{-3}$ [@problem_id:3316388]。", "problem": "考虑一个计算系统生物学中不断演化的无向分子相互作用网络的生成模型，其中新的分子种类（节点）按顺序进入系统，并根据 Barabási-Albert (BA) 优先连接机制与现有种类建立相互作用（边）。Barabási-Albert (BA) 模型定义如下：在每个离散时间步，引入一个恰好带有 $m \\ge 1$ 条边的新节点；每条边以与现有节点的当前度成正比的概率连接到该节点。令 $k$ 表示度，令 $P(k)$ 表示在大网络极限下，度为 $k$ 的节点的稳态分数，限定于 $k \\ge m$。\n\n从上述基本的 BA 规则（优先连接与度成正比，以及每增加一条边总度增量守恒）出发，通过编写并求解每个度类中节点预期数量的大时间主方程，推导出 $P(k)$ 在 $k \\ge m$ 时的正确归一化的离散形式，并确定唯一的预因子以确保 $\\sum_{k=m}^{\\infty} P(k) = 1$。您的最终答案必须是一个关于 $k$ 和 $m$ 的单一闭式解析表达式，对所有整数 $k \\ge m$ 均有效。不需要数值近似，最终表达式中不应包含任何单位。", "solution": "所述问题是有效的。它在科学上基于网络理论的既定原则，特别是 Barabási-Albert (BA) 模型。该问题是适定的、客观的，并包含了进行严格推导所需的所有必要信息。我们将着手使用主方程方法推导度分布 $P(k)$。\n\n令 $N_k(t)$ 为时间步 $t$ 时度为 $k$ 的节点的期望数量。网络中的总节点数为 $N(t)$。模型从一个包含 $m_0$ 个节点的初始核心开始，在每个时间步，增加一个新节点。因此，$N(t) = m_0 + t$。对于大网络（$t \\to \\infty$），我们可以近似为 $N(t) \\approx t$。\n\n每个新节点引入 $m$ 条边。在时间 $t$ 的总边数是 $E(t) = E_0 + mt$，其中 $E_0$ 是初始核心中的边数。无向网络中的度之和是边数的两倍，所以 $\\sum_{i} k_i(t) = 2E(t) = 2(E_0 + mt)$。在大时间极限下，这个和由 $t$ 的线性项主导，得到 $\\sum_i k_i(t) \\approx 2mt$。\n\nBA 模型采用优先连接，即新边连接到现有节点 $i$ 的概率 $\\Pi(i)$ 与其度 $k_i$ 成正比。这个概率由下式给出：\n$$\n\\Pi(k_i) = \\frac{k_i}{\\sum_{j} k_j(t)} \\approx \\frac{k_i}{2mt}\n$$\n\n现在，我们将在对大时间 $t$ 有效的连续近似下，为 $N_k(t)$ 的变化率（记为 $\\frac{dN_k}{dt}$）建立主方程。度为 $k$ 的节点数量因两个过程而改变：\n1.  一个度为 $k-1$ 的节点获得一条边，变为度为 $k$ 的节点。\n2.  一个度为 $k$ 的节点获得一条边，变为度为 $k+1$ 的节点。\n\n在每个时间步 $dt$，增加一个新节点，它形成 $m$ 条边。单位时间内增加的新边数为 $m$。这 $m$ 条新边连接到度为 $k$ 的节点的速率是 $m$ 乘以连接到任何此类节点的总概率。这个概率是此类节点的数量 $N_k(t)$ 乘以连接到单个此类节点的概率 $\\frac{k}{2mt}$。\n因此，度为 $k-1$ 的节点转化为度为 $k$ 的节点的速率是：\n$$\n\\text{$N_k$ 的增益项} = m \\cdot N_{k-1}(t) \\cdot \\frac{k-1}{2mt} = \\frac{(k-1) N_{k-1}(t)}{2t}\n$$\n度为 $k$ 的节点转化为度为 $k+1$ 的节点的速率是：\n$$\n\\text{$N_k$ 的损失项} = m \\cdot N_k(t) \\cdot \\frac{k}{2mt} = \\frac{k N_k(t)}{2t}\n$$\n结合这些项，得到 $k  m$ 的主方程：\n$$\n\\frac{dN_k}{dt} = \\frac{(k-1) N_{k-1}}{2t} - \\frac{k N_k}{2t}\n$$\n\n对于 $k=m$ 存在一个特殊情况。度为 $m$ 的节点只能通过引入新节点来创建，因为每个新节点的度都是 $m$。因此，在每个时间步，$N_m$ 增加 1。度为 $m$ 的节点在获得一条边时会消失。因此，$N_m$ 的主方程为：\n$$\n\\frac{dN_m}{dt} = 1 - \\frac{m N_m}{2t}\n$$\n注意，我们忽略了新节点连接到自身或其 $m$ 条边中的两条连接到同一目标的微小概率，因为这些事件在大网络极限下可以忽略不计。\n\n我们关心的是大时间极限下的稳态度分布 $P(k)$。在此极限下，度为 $k$ 的节点分数 $P(k) = \\frac{N_k(t)}{N(t)}$ 变为常数。使用近似 $N(t) \\approx t$，我们有 $N_k(t) = P(k) N(t) \\approx P(k) t$。对 $t$ 求导得到 $\\frac{dN_k}{dt} \\approx P(k)$。\n\n将这些关系代入主方程：\n对于 $k  m$：\n$$\nP(k) = \\frac{(k-1) P(k-1)t}{2t} - \\frac{k P(k)t}{2t}\n$$\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\n整理得到 $P(k)$：\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{k+2}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\n这是 $k  m$ 时 $P(k)$ 的一个递推关系。\n\n对于 $k=m$：\n$$\nP(m) = 1 - \\frac{m P(m)t}{2t}\n$$\n$$\nP(m) = 1 - \\frac{m}{2} P(m)\n$$\n整理得到 $P(m)$：\n$$\nP(m) \\left(1 + \\frac{m}{2}\\right) = 1\n$$\n$$\nP(m) \\left(\\frac{m+2}{2}\\right) = 1\n$$\n$$\nP(m) = \\frac{2}{m+2}\n$$\n现在我们通过从 $k$ 向下展开到 $m$ 来求解 $P(k)$ 的递推关系：\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1) = \\frac{k-1}{k+2} \\frac{k-2}{k+1} P(k-2) = \\dots\n$$\n$$\nP(k) = \\left( \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} \\right) P(m)\n$$\n这个乘积可以计算为：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m)(m+1)\\dots(k-1)}{(m+3)(m+4)\\dots(k+2)} = \\frac{\\frac{(k-1)!}{(m-1)!}}{\\frac{(k+2)!}{(m+2)!}} = \\frac{(k-1)!}{(m-1)!} \\frac{(m+2)!}{(k+2)!}\n$$\n$$\n= \\frac{(m+2)(m+1)m}{k(k+1)(k+2)}\n$$\n将此代回 $P(k)$ 的表达式中：\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} P(m)\n$$\n现在，使用我们为 $P(m)$ 找到的值：\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\cdot \\frac{2}{m+2} = \\frac{2m(m+1)}{k(k+1)(k+2)}\n$$\n这就是 $k \\ge m$ 时度分布的闭式表达式。\n\n最后，我们必须验证该分布是正确归一化的，即 $\\sum_{k=m}^{\\infty} P(k) = 1$。其和为：\n$$\n\\sum_{k=m}^{\\infty} P(k) = \\sum_{k=m}^{\\infty} \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}\n$$\n和中的项可以使用部分分式分解：\n$$\n\\frac{1}{k(k+1)(k+2)} = \\frac{1}{2k} - \\frac{1}{k+1} + \\frac{1}{2(k+2)} = \\frac{1}{2}\\left[\\left(\\frac{1}{k} - \\frac{1}{k+1}\\right) - \\left(\\frac{1}{k+1} - \\frac{1}{k+2}\\right)\\right]\n$$\n这是一个伸缩级数。令 $S = \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}$。\n使用恒等式 $\\sum_{i=n}^{\\infty} \\frac{1}{i(i+1)(i+2)} = \\frac{1}{2n(n+1)}$，该恒等式可通过伸缩求和法证明：\n$$\nS = \\frac{1}{2} \\sum_{k=m}^{\\infty} \\left[ \\frac{1}{k(k+1)} - \\frac{1}{(k+1)(k+2)} \\right] = \\frac{1}{2} \\left[ \\frac{1}{m(m+1)} \\right]\n$$\n将此结果代回归一化求和中：\n$$\n\\sum_{k=m}^{\\infty} P(k) = 2m(m+1) \\cdot \\left( \\frac{1}{2m(m+1)} \\right) = 1\n$$\n该分布已正确归一化。预因子不是假设的，而是直接从主方程形式体系中推导出来的，满足了问题的要求。因此，$P(k)$ 的最终表达式得到了验证。对于大的 $k$，$P(k) \\sim k^{-3}$，这是 BA 模型著名的幂律特征。", "answer": "$$\\boxed{\\frac{2m(m+1)}{k(k+1)(k+2)}}$$", "id": "3316388"}, {"introduction": "虽然稳态分布描述了网络在长时间演化后的性质，但理解其瞬态行为对于模拟有限的、现实世界中的系统至关重要。这个计算实践将挑战您通过模拟来让 BA 模型变得鲜活 [@problem_id:3316319]。您将实现该生成过程，并量化网络的度分布如何随时间收敛到理论预测的幂律渐近线，从而在抽象理论与可触摸的动态过程之间架起一座桥梁。", "problem": "要求您对 Barabási-Albert (BA) 偏好连接模型中的瞬态度分布进行形式化、分析和基于仿真的量化，并测量其向指数为 $3$ 的幂律的收敛情况。建模背景是计算系统生物学，其中无标度拓扑被用作分子相互作用网络的生成模型。您的程序必须复现一个指定的测试套件，并输出一行汇总所要求的数值结果。\n\n生成过程必须定义如下。\n\n- 从一个包含 $m_0$ 个节点的完全图作为种子网络开始。也就是说，初始节点数为 $m_0$，每个节点都与所有其他节点相连，这意味着每个种子节点的初始度为 $m_0 - 1$，初始总度为 $m_0(m_0 - 1)$。\n\n- 在每个离散时间步 $t \\in \\{1,2,\\dots\\}$，一个新节点被添加，并带有 $m$ 条边，其中 $m \\leq m_0$。这 $m$ 条边中的每一条都连接到一个不同的现有节点，这些节点是无放回地选择的，选择的概率与现有节点的当前度成正比。在一个时间步内，这 $m$ 个目标节点是根据在添加任何一条边之前固定的现有度值进行无放回选择的。在该步骤中选择了所有 $m$ 个目标后，添加这 $m$ 条边，使每个目标节点的度增加 $1$，并将新节点的度设置为 $m$。通过此构造，不允许出现自环和多重边。\n\n- 令 $N(t) = m_0 + t$ 表示 $t$ 步后的节点数。令 $k_i(t)$ 表示节点 $i$ 在时间 $t$ 的度。令 $P_t(k)$ 表示在时间 $t$ 时，度至少为 $m$ 的节点的经验概率质量函数 (PMF)，即 $P_t(k) = \\frac{1}{|\\{i : k_i(t) \\ge m\\}|}\\sum_{i=1}^{N(t)} \\mathbf{1}\\{k_i(t)=k,\\,k \\ge m\\}$。\n\n- 使用重复的独立实现来估计 $P_t(k)$ 作为平均 PMF。具体来说，对于给定的参数元组 $(m_0,m,t)$，使用上述模型模拟 $R$ 个独立的网络，每个网络都从一个大小为 $m_0$ 的完全图开始，精确地增长 $t$ 步。对于每次实现，计算度在 $k \\ge m$ 范围内的直方图，将其归一化为 PMF，然后在 $R$ 次实现中对这些 PMF 进行平均。或者，您也可以汇总所有实现中 $k \\ge m$ 的度计数，然后进行一次归一化。\n\n您的分析必须从适用于 BA 模型的第一性原理开始：\n\n- 总度 $S(t)$ 满足 $S(t) = S(0) + 2mt$，其中 $S(0) = m_0(m_0 - 1)$。\n\n- 一个度为 $k_i(t)$ 的节点在时间 $t+1$ 接收一个连接的概率是 $k_i(t)/S(t)$。\n\n- 使用连续率或主方程方法来推导渐近度分布 $P_{\\infty}(k)$ (对于 $k \\ge m$) 及其幂律指数。不要假设 $P_{\\infty}(k)$ 的形式；应从上述模型定义和给定的 $S(t)$ 守恒关系中推导出来。\n\n为了量化 $P_t(k)$ 向渐近分布的收敛情况，定义以下距离。\n\n- 令 $K_{\\max}(t)$ 表示在时间 $t$ 汇总 $R$ 次实现后，在度至少为 $m$ 的节点中观测到的最大度。在有限支撑集 $\\{m, m+1, \\dots, K_{\\max}(t)\\}$ 上定义一个参考分布 $Q_t(k)$，它与指数为 $3$ 的渐近幂律成正比，即对于 $k \\in \\{m,\\dots,K_{\\max}(t)\\}$，$Q_t(k) \\propto k^{-3}$，否则 $Q_t(k) = 0$，并进行归一化使得 $\\sum_{k=m}^{K_{\\max}(t)} Q_t(k) = 1$。定义经验分布 $P_t(k)$ (在相同支撑集上进行了限制和归一化) 和 $Q_t(k)$ 之间的全变差 (TV) 距离为\n$$\n\\mathrm{TV}(P_t, Q_t) \\;=\\; \\frac{1}{2} \\sum_{k=m}^{K_{\\max}(t)} \\bigl| P_t(k) - Q_t(k) \\bigr| \\,.\n$$\n\n您的任务有两部分：\n\n- 从模型的偏好连接规则和总度守恒出发，解析推导渐近分布 $P_{\\infty}(k)$ 的幂律指数。您的推导必须清楚地证明所使用的所有近似和极限的合理性。\n\n- 实现一个程序，对于下面指定的每个测试用例，使用上述过程模拟该模型并计算 TV 距离 $\\mathrm{TV}(P_t,Q_t)$。\n\n测试套件：\n\n- 用例 1: $(m_0, m, t, R, \\mathrm{seed}) = (3, 1, 1, 10000, 17)$。\n\n- 用例 2: $(m_0, m, t, R, \\mathrm{seed}) = (5, 2, 30, 3000, 42)$。\n\n- 用例 3: $(m_0, m, t, R, \\mathrm{seed}) = (2, 1, 10, 4000, 12345)$。\n\n- 用例 4: $(m_0, m, t, R, \\mathrm{seed}) = (8, 3, 50, 2000, 2024)$。\n\n程序要求：\n\n- 程序必须完全按照规定实现仿真，并为每个测试用例使用给定的种子初始化伪随机数生成器，以确保可复现性。\n\n- 对于每个测试用例，计算一个等于 $\\mathrm{TV}(P_t,Q_t)$ 的实数。\n\n- 输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[0.123456,0.234567,0.345678,0.456789]”）。每个数字必须精确到小数点后六位。\n\n- 不涉及物理单位。不出现角度。所有输出均为十进制形式的实数。\n\n您最终提交的必须是一个独立、可运行的程序，且不需要任何输入。单行输出必须按上述顺序汇总四个用例的 TV 距离。", "solution": "该问题要求对 Barabási-Albert (BA) 模型的渐近幂律指数进行解析推导，并进行数值模拟以量化瞬態度分布向此渐近形式的收敛情况。\n\n### 渐近度分布的解析推导\n\n我们按照规定推导 BA 模型的渐近度分布 $P_{\\infty}(k)$。我们使用连续率方程方法，该方法对给定度的节点数量随时间的变化进行建模。\n\n令 $N_k(t)$ 为时间 $t$ 时度为 $k$ 的节点数。总节点数为 $N(t) = m_0 + t$，总度为 $S(t) = m_0(m_0 - 1) + 2mt$。对于较大的时间 $t$，我们可以做近似 $N(t) \\approx t$ 和 $S(t) \\approx 2mt$。\n\n当一个节点在每个时间步被选为新增的 $m$ 条边之一的目标时，其度增加 1。度为 $k_i(t)$ 的特定节点 $i$ 被选为一条新边的目标的概率是 $\\Pi(i) = k_i(t)/S(t)$。由于每个时间步增加 $m$ 条边，并为连续模型假设这些选择是独立的（这对于大型网络是有效的近似），一个度为 $k$ 的节点获得一条边的速率是 $m \\cdot (k/S(t))$。\n\n度为 $k$ 的节点数量 $N_k$ 因两个过程而改变：\n1.  **增加**：一个度为 $k-1$ 的节点被选中，其度变为 $k$。此过程的总速率是此类节点的数量 $N_{k-1}(t)$ 乘以每个节点获得一条边的速率：$m \\frac{k-1}{S(t)} N_{k-1}(t)$。\n2.  **减少**：一个度为 $k$ 的节点被选中，其度变为 $k+1$。此过程的总速率是 $m \\frac{k}{S(t)} N_k(t)$。\n\n此外，在每个时间步，都会引入一个度为 $m$ 的新节点。这对 $N_m$ 起到源项的作用，贡献的速率为 $1$。\n\n$N_k(t)$ 变化的速率方程因此是：\n$$\n\\frac{d N_k(t)}{dt} = m \\frac{k-1}{S(t)} N_{k-1}(t) - m \\frac{k}{S(t)} N_k(t) + \\delta_{k,m}\n$$\n其中 $\\delta_{k,m}$ 是克罗内克 δ，如果 $k=m$ 则等于 $1$，否则等于 $0$。当 $k=m$ 时，关于 $k-1$ 的项为零，因为没有节点的度小于 $m$。\n\n我们寻求对于较大的 $t$ 的稳态解，此时度分布 $P(k)$ 变得与时间无关。我们假设 $N_k(t) = N(t) P(k) = (m_0+t) P(k)$。对 $t$ 求导得到 $\\frac{d N_k(t)}{dt} = P(k)$。\n\n将此式以及大 $t$ 近似 $N(t) \\approx t$ 和 $S(t) \\approx 2mt$ 代入速率方程：\n$$\nP(k) \\approx m \\frac{k-1}{2mt} (t \\cdot P(k-1)) - m \\frac{k}{2mt} (t \\cdot P(k)) + \\delta_{k,m}\n$$\n$$\nP(k) \\approx \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k) + \\delta_{k,m}\n$$\n\n对于 $k  m$，源项为零：\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\n重新整理各项：\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{2+k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n这得到递推关系：\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\n我们可以通过迭代求解：\n$$\nP(k) = P(m) \\prod_{j=m+1}^{k} \\frac{j-1}{j+2}\n$$\n乘积可以展开为：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{m}{m+3} \\cdot \\frac{m+1}{m+4} \\cdot \\frac{m+2}{m+5} \\cdots \\frac{k-1}{k+2}\n$$\n这可以用阶乘或更普遍地用 Γ 函数 $\\Gamma(z+1) = z!$ 表示：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{\\prod_{j=m+1}^{k} (j-1)}{\\prod_{j=m+1}^{k} (j+2)} = \\frac{\\Gamma(k)/\\Gamma(m)}{\\Gamma(k+3)/\\Gamma(m+3)} = \\frac{\\Gamma(m+3)}{\\Gamma(m)} \\frac{\\Gamma(k)}{\\Gamma(k+3)}\n$$\n使用性质 $\\Gamma(z+1) = z\\Gamma(z)$，我们有：\n$\\Gamma(m+3) = (m+2)(m+1)m\\Gamma(m)$ 和 $\\Gamma(k+3) = (k+2)(k+1)k\\Gamma(k)$。\n因此，乘积简化为：\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m+2)(m+1)m\\Gamma(m)}{\\Gamma(m)} \\frac{\\Gamma(k)}{(k+2)(k+1)k\\Gamma(k)} = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)}\n$$\n因此，渐近度分布的形式为：\n$$\nP(k) = P(m) \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\quad \\text{for } k \\ge m\n$$\n对于较大的 $k$，项 $k(k+1)(k+2) \\sim k^3$。因此，分布遵循幂律：\n$$\nP(k) \\propto k^{-3}\n$$\n幂律指数为 $\\gamma=3$。这个推导证实了问题描述中提供的值。比例常数取决于 $m$ 但不取决于 $k$。\n\n### 仿真和测量算法\n\n该任务要求实现一个仿真程序来生成 BA 网络，并计算经验度分布与理论幂律分布之间的全变差 (TV) 距离。\n\n1.  **仿真设置**：对于每个测试用例 $(m_0, m, t, R, \\mathrm{seed})$，我们执行 $R$ 次独立仿真。每个用例都使用给定的 `seed` 初始化伪随机数生成器，以确保可复现性。\n\n2.  **网络生成**：每次仿真运行按以下步骤进行：\n    -   **初始化**：网络在时间 $t=0$ 时以一个包含 $m_0$ 个节点的完全图开始。每个初始节点的度为 $m_0 - 1$。一个数组存储网络中所有节点的度。\n    -   **增长**：仿真运行 $t$ 个时间步。在每个步骤中：\n        a. 添加一个新节点。\n        b. 选择 $m$ 个不同的现有节点作为连接目标。选择是概率性的，选择任何节点的概率与其当前度成正比。这是通过从当前节点集合中无放回地抽取 $m$ 个节点索引来实现的，使用它们的度除以总度进行归一化作为概率分布。在步骤 $s \\in \\{0, \\dots, t-1\\}$ 开始时，总度为 $S(s) = m_0(m_0-1) + 2ms$。\n        c. $m$ 个被选中的目标节点的度各增加 $1$。\n        d. 新节点以度为 $m$ 添加到网络中。它的度被附加到度数组中。\n\n3.  **度聚合**：对于给定的测试用例，在 $R$ 次仿真之后，汇总所有实现中的最终度列表。构造一个频率图（或字典）来存储在所有 $R \\times (m_0 + t)$ 个节点中观察到的每个度值 $k$ 的总计数。\n\n4.  **PMF 计算**：\n    -   令聚合的度计数表示为 $C(k)$。\n    -   所有仿真中观察到的最大度为 $K_{\\max} = \\max\\{k \\mid C(k)0\\}$。\n    -   分析仅限于度 $k \\ge m$。我们分布的支撑集是整数集合 $\\{m, m+1, \\dots, K_{\\max}\\}$。\n    -   **经验 PMF $P_t(k)$**：度 $k \\ge m$ 的节点总数为 $N_{total} = \\sum_{j=m}^{K_{\\max}} C(j)$。则经验 PMF 为 $P_t(k) = C(k) / N_{total}$，其中 $k \\in \\{m, \\dots, K_{\\max}\\}$。\n    -   **参考 PMF $Q_t(k)$**：这是一个在相同有限支撑集上指数为 $3$ 的归一化幂律分布。对于每个 $k \\in \\{m, \\dots, K_{\\max}\\}$，我们设置一个未归一化的值 $q'(k) = k^{-3}$。归一化常数为 $C_Q = \\sum_{j=m}^{K_{\\max}} j^{-3}$。参考 PMF 为 $Q_t(k) = q'(k) / C_Q$。\n\n5.  **全变差距离**：两个分布之间的距离计算如下：\n    $$\n    \\mathrm{TV}(P_t, Q_t) = \\frac{1}{2} \\sum_{k=m}^{K_{\\max}} |P_t(k) - Q_t(k)|\n    $$\n    这个值为每个测试用例计算，并四舍五入到小数点后六位。\n\n最终输出是指定测试用例的这些 TV 距离值的列表，格式化为方括号内的逗号分隔字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(m0, m, t, rng):\n    \"\"\"\n    Runs a single realization of the Barabási-Albert model.\n\n    Args:\n        m0 (int): Number of nodes in the initial complete graph.\n        m (int): Number of edges added with each new node.\n        t (int): Number of time steps to simulate.\n        rng (np.random.Generator): The random number generator to use.\n\n    Returns:\n        np.ndarray: An array of the degrees of all nodes at the end of the simulation.\n    \"\"\"\n    # Initial network: a complete graph on m0 nodes.\n    # Each node has degree m0 - 1.\n    degrees = np.full(m0, m0 - 1, dtype=int)\n    \n    # Growth process for t steps.\n    for step in range(t):\n        current_num_nodes = m0 + step\n        \n        # Total degree can be computed analytically to be slightly faster\n        # S(t) = S(0) + 2mt. At start of step `step` (0-indexed), total nodes added is `step`.\n        total_degree = m0 * (m0 - 1) + 2 * m * step\n        \n        # Attachment probabilities are proportional to degree.\n        if total_degree == 0:\n            # Handle the case of no edges, though not possible with m0 >= 2.\n            # If m0=1, total_degree is 0. But m = m0 implies m=1. This case is not in the test suite.\n            # If so, attachment is uniform.\n            probs = np.ones(current_num_nodes) / current_num_nodes\n        else:\n            probs = degrees / total_degree\n\n        # Select m distinct nodes to attach to, without replacement.\n        node_indices = np.arange(current_num_nodes)\n        targets = rng.choice(node_indices, size=m, replace=False, p=probs)\n        \n        # Update degrees of target nodes.\n        degrees[targets] += 1\n        \n        # Add the new node with degree m.\n        degrees = np.append(degrees, m)\n        \n    return degrees\n\ndef calculate_tv_distance(m0, m, t, R, seed):\n    \"\"\"\n    Calculates the Total Variation distance for a given set of parameters.\n\n    Args:\n        m0 (int): Initial number of nodes.\n        m (int): Edges per new node.\n        t (int): Number of time steps.\n        R (int): Number of independent realizations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The computed TV distance.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    total_degree_counts = {}\n\n    for _ in range(R):\n        final_degrees = run_simulation(m0, m, t, rng)\n        for k in final_degrees:\n            total_degree_counts[k] = total_degree_counts.get(k, 0) + 1\n\n    if not total_degree_counts:\n        return 0.0\n\n    k_max_obs = max(total_degree_counts.keys())\n\n    # The problem defines the distributions on the support k >= m.\n    if m > k_max_obs:\n        return 0.0\n\n    k_support = np.arange(m, k_max_obs + 1)\n\n    # Compute empirical PMF P_t(k)\n    counts_in_support = np.array([total_degree_counts.get(k, 0) for k in k_support])\n    total_nodes_in_support = np.sum(counts_in_support)\n    \n    if total_nodes_in_support == 0:\n        return 0.0\n    \n    p_t = counts_in_support / total_nodes_in_support\n    \n    # Compute reference PMF Q_t(k)\n    q_unnormalized = k_support.astype(np.float64)**-3\n    norm_const = np.sum(q_unnormalized)\n\n    if norm_const == 0:\n      # This case is unlikely unless k_support is pathological\n      q_t = np.zeros_like(p_t)\n    else:    \n      q_t = q_unnormalized / norm_const\n    \n    # Compute Total Variation distance\n    tv_dist = 0.5 * np.sum(np.abs(p_t - q_t))\n    \n    return tv_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (3, 1, 1, 10000, 17),\n        (5, 2, 30, 3000, 42),\n        (2, 1, 10, 4000, 12345),\n        (8, 3, 50, 2000, 2024),\n    ]\n\n    results = []\n    for case in test_cases:\n        m0, m, t, R, seed = case\n        tv_distance = calculate_tv_distance(m0, m, t, R, seed)\n        results.append(f\"{tv_distance:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3316319"}, {"introduction": "从理论走向实践，需要强大的统计工具来分析经验数据，例如一个实测的蛋白质相互作用网络。本练习旨在解决幂律分布参数估计这一关键任务，而这正是网络分析中常见的失败点 [@problem_id:3316339]。您将首先探究为何在对数-对数图上进行线性回归等简单方法会产生有偏的结果，然后推导出正确且强大的最大似然估计（MLE）方法来估计幂律指数。", "problem": "在计算系统生物学中，蛋白质-蛋白质相互作用网络和转录调控网络已通过类似于巴拉巴西-阿尔伯特过程的带有偏好依附的生长机制进行建模，从而产生重尾度分布。假设一个大型网络已被抽样，并且为了减轻低度模型的错误设定，仅保留度不小于已知下限截断值 $k_{\\min}$ 的节点。设保留的度是由具有尾指数 $\\gamma$ 的离散幂律模型产生的独立观测值 $k_1, k_2, \\dots, k_n$，因此对于整数 $k \\geq k_{\\min}$，该模型赋予的概率为\n$$\nP(k \\mid \\gamma, k_{\\min}) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})},\n$$\n其中 $\\zeta(\\gamma, k_{\\min})$ 是确保在离散支持集上归一化的赫尔维茨zeta函数。分析师们通常使用普通最小二乘法 (OLS) 对直方图化的度数统计来拟合 $\\log P(k)$ 与 $\\log k$ 的斜率。\n\n从多项分布或泊松抽样模型下直方图计数的统计特性以及上述离散幂律似然的定义出发，从第一性原理上解释为什么在有限样本中，对分箱的对数-对数数据使用普通最小二乘法通常会产生对尾指数 $\\gamma$ 的有偏估计。然后，通过计算费雪信息，推导该模型下 $\\gamma$ 的离散最大似然估计量 (MLE) 及其大样本方差。您的推导过程必须从上面定义的似然函数以及期望和方差的标准性质出发，不得调用任何快捷公式。\n\n将您的最终答案表示为关于 $k_{\\min}$、$n$ 和观测度 $\\{k_i\\}_{i=1}^{n}$ 的闭式解析表达式，如果需要，可以使用特殊函数及其导数。不要提供数值近似。无需单位。如果您引入缩略词（例如，OLS），必须在首次使用时对其进行定义。", "solution": "该问题要求两个不同的部分：首先，有理有据地解释为什么在对数分箱的度数统计上使用普通最小二乘法 (OLS) 回归会提供幂律指数的有偏估计；其次，推导离散幂律分布下指数 $\\gamma$ 的最大似然估计量 (MLE) 及其大样本方差。\n\n### 第一部分：对数-对数分箱数据上OLS的偏差\n\n对于 $k \\geq k_{\\min}$，观测到度为 $k$ 的概率的建议模型由下式给出：\n$$ P(k \\mid \\gamma, k_{\\min}) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} $$\n对此方程取自然对数，得到 $\\ln(P(k))$ 和 $\\ln(k)$ 之间的线性关系：\n$$ \\ln(P(k)) = -\\gamma \\ln(k) - \\ln(\\zeta(\\gamma, k_{\\min})) $$\n该方程的形式为 $y = mx + c$，其中 $y = \\ln(P(k))$，$m = -\\gamma$，$x = \\ln(k)$，且 $c = -\\ln(\\zeta(\\gamma, k_{\\min}))$。\n\n在对数分箱数据上使用OLS方法，首先需要从数据构建一个经验概率分布。设 $n_k$ 为样本中度为 $k$ 的节点数，总样本大小为 $n = \\sum_{i=1}^n 1$。对于不同 $k$ 值的计数集合 $\\{n_k\\}$ 服从多项分布。度为 $k$ 的经验概率估计为 $\\hat{P}(k) = n_k / N_{obs}$，其中 $N_{obs}$ 是直方图中的节点总数（如果我们对原始观测值进行分箱，则为 $n$）。为简单起见，我们将计数 $n_k$ 视为独立的泊松随机变量，其均值为 $\\lambda_k = n \\cdot P(k)$，这是大样本 $n$ 和小概率 $P(k)$ 情况下的常见近似。\n\n然后，OLS过程对点 $(\\ln(k), \\ln(\\hat{P}(k)))$ 拟合一条直线，将斜率的负值作为 $\\gamma$ 的估计值。在有限样本中，这个过程通常是有偏的，至少有以下三个根本原因。\n\n1.  **来自对数变换的系统性偏差**：OLS回归是在 $\\ln(\\hat{P}(k))$ 上执行的，而不是 $\\hat{P}(k)$。虽然经验频率 $\\hat{P}(k)$ 是真实概率 $P(k)$ 的无偏估计量（即 $\\mathbb{E}[\\hat{P}(k)] = P(k)$），但对数是一个非线性的凹函数。根据琴生不等式，对于任何凹函数 $f$ 和随机变量 $X$，有 $\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])$。在此处应用此不等式，令 $f(x) = \\ln(x)$ 且 $X = \\hat{P}(k)$：\n    $$ \\mathbb{E}[\\ln(\\hat{P}(k))] \\leq \\ln(\\mathbb{E}[\\hat{P}(k)]) = \\ln(P(k)) $$\n    这个不等式表明，回归中因变量的期望值 $\\ln(\\hat{P}(k))$ 系统性地低于它本应估计的真实值 $\\ln(P(k))$。这在回归拟合中引入了系统性偏差，即使所有其他假设都满足，该偏差也不会消失。\n\n2.  **违反同方差性**：OLS的有效性和效率的一个关键假设是所有观测值的误差方差是恒定的（同方差性）。在对数-对数回归中，“误差”是 $\\ln(\\hat{P}(k))$ 与真实直线的偏差。我们来分析 $\\ln(\\hat{P}(k))$ 的方差。使用泊松模型处理计数 $n_k$，我们有 $\\mathbb{E}[n_k] = nP(k)$ 和 $\\text{Var}(n_k) = nP(k)$。经验概率为 $\\hat{P}(k) = n_k/n$，所以 $\\text{Var}(\\hat{P}(k)) = \\frac{1}{n^2}\\text{Var}(n_k) = \\frac{P(k)}{n}$。对 $\\ln(\\hat{P}(k))$ 在 $P(k)$ 附近使用一阶泰勒展开（delta方法）：\n    $$ \\ln(\\hat{P}(k)) \\approx \\ln(P(k)) + \\frac{1}{P(k)}(\\hat{P}(k) - P(k)) $$\n    这个近似的方差是：\n    $$ \\text{Var}(\\ln(\\hat{P}(k))) \\approx \\frac{1}{P(k)^2} \\text{Var}(\\hat{P}(k)) = \\frac{1}{P(k)^2} \\frac{P(k)}{n} = \\frac{1}{n P(k)} $$\n    由于 $P(k)$ 是 $k$ 的函数，这个方差显然不是恒定的。对于幂律分布，$P(k)$ 随 $k$ 增大而减小，这意味着对于较大的 $k$ 值，$\\text{Var}(\\ln(\\hat{P}(k)))$ 会急剧增加。分布尾部的数据点基于非常少的计数，因此高度不确定，其误差方差要大得多。OLS对所有点给予相等的权重，这意味着这些高方差、不可靠的尾部数据点会对估计的斜率产生不成比例的巨大影响，导致对 $\\gamma$ 的估计有偏且低效。\n\n3.  **对零计数的处理**：对于有限样本 $n$，很可能对于某些度 $k$（特别是大的 $k$），计数 $n_k$ 将为零。在这种情况下，$\\hat{P}(k)=0$，而 $\\ln(\\hat{P}(k))$ 未定义。一个常见但统计上无原则的临时解决方案是简单地从回归中省略这些点。这种选择性地移除数据会引入系统性偏差，因为它以非随机的方式丢弃信息，实际上是截断了经验分布的尾部。\n\n由于这些原因，对数-对数分箱数据上的OLS不是估计幂律指数的可靠方法。应首选像最大似然估计这样有原则的方法。\n\n### 第二部分：最大似然估计量(MLE)及其方差\n\nMLE是从样本的似然函数推导出来的。给定来自离散幂律分布 $P(k \\mid \\gamma, k_{\\min})$ 的 $n$ 个独立同分布的观测值 $k_1, k_2, \\dots, k_n$，似然函数 $L(\\gamma)$ 是每个观测值概率的乘积：\n$$ L(\\gamma) = \\prod_{i=1}^{n} P(k_i \\mid \\gamma, k_{\\min}) = \\prod_{i=1}^{n} \\frac{k_i^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} = \\frac{ \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} }{ \\left[ \\zeta(\\gamma, k_{\\min}) \\right]^n } $$\n使用对数似然函数 $\\mathcal{L}(\\gamma) = \\ln L(\\gamma)$ 更为方便：\n$$ \\mathcal{L}(\\gamma) = \\ln \\left( \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} \\right) - \\ln \\left( \\left[ \\zeta(\\gamma, k_{\\min}) \\right]^n \\right) $$\n$$ \\mathcal{L}(\\gamma) = -\\gamma \\sum_{i=1}^{n} \\ln(k_i) - n \\ln(\\zeta(\\gamma, k_{\\min})) $$\n为了找到记为 $\\hat{\\gamma}$ 的MLE，我们将 $\\mathcal{L}(\\gamma)$ 对 $\\gamma$ 求导，并令结果为零。赫尔维茨zeta函数 $\\zeta(\\gamma, k_{\\min}) = \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma}$ 关于 $\\gamma$ 的导数是：\n$$ \\zeta'(\\gamma, k_{\\min}) = \\frac{\\partial}{\\partial \\gamma} \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} = \\sum_{j=k_{\\min}}^{\\infty} \\frac{\\partial}{\\partial \\gamma} e^{-\\gamma \\ln(j)} = \\sum_{j=k_{\\min}}^{\\infty} (-\\ln(j)) e^{-\\gamma \\ln(j)} = -\\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} \\ln(j) $$\n对对数似然函数求导：\n$$ \\frac{d\\mathcal{L}}{d\\gamma} = -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{1}{\\zeta(\\gamma, k_{\\min})} \\frac{d}{d\\gamma}\\zeta(\\gamma, k_{\\min}) = -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} $$\n令此导数为零可定义MLE $\\hat{\\gamma}$：\n$$ -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} = 0 $$\n整理后得到必须为 $\\hat{\\gamma}$ 求解的方程：\n$$ \\frac{1}{n} \\sum_{i=1}^{n} \\ln(k_i) = - \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} $$\n这是一个关于 $\\hat{\\gamma}$ 的超越方程，必须数值求解。这个方程本身就是该估计量的正式定义。\n\n接下来，我们推导 $\\hat{\\gamma}$ 的大样本方差。方差由费雪信息 $I(\\gamma)$ 的倒数给出。对于 $n$ 个独立同分布的观测值，$I(\\gamma) = - \\mathbb{E} \\left[ \\frac{d^2\\mathcal{L}}{d\\gamma^2} \\right]$。我们首先计算对数似然的二阶导数：\n$$ \\frac{d^2\\mathcal{L}}{d\\gamma^2} = \\frac{d}{d\\gamma} \\left( -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} \\right) = -n \\frac{d}{d\\gamma} \\left( \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} \\right) $$\n使用商法则：\n$$ \\frac{d^2\\mathcal{L}}{d\\gamma^2} = -n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})\\zeta(\\gamma, k_{\\min}) - (\\zeta'(\\gamma, k_{\\min}))^2}{(\\zeta(\\gamma, k_{\\min}))^2} \\right] $$\n其中 $\\zeta''(\\gamma, k_{\\min})$ 是赫尔维茨zeta函数的二阶导数：\n$$ \\zeta''(\\gamma, k_{\\min}) = \\frac{\\partial}{\\partial \\gamma} \\left( -\\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} \\ln(j) \\right) = -\\sum_{j=k_{\\min}}^{\\infty} (-\\ln(j)) j^{-\\gamma} \\ln(j) = \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} (\\ln(j))^2 $$\n$\\frac{d^2\\mathcal{L}}{d\\gamma^2}$ 的表达式不依赖于数据 $\\{k_i\\}$，所以其期望就是表达式本身：$\\mathbb{E} \\left[ \\frac{d^2\\mathcal{L}}{d\\gamma^2} \\right] = \\frac{d^2\\mathcal{L}}{d\\gamma^2}$。\n因此，费雪信息为：\n$$ I(\\gamma) = - \\frac{d^2\\mathcal{L}}{d\\gamma^2} = n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})\\zeta(\\gamma, k_{\\min}) - (\\zeta'(\\gamma, k_{\\min}))^2}{(\\zeta(\\gamma, k_{\\min}))^2} \\right] = n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} - \\left(\\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})}\\right)^2 \\right] $$\n在大样本 $n$ 的极限下，MLE $\\hat{\\gamma}$ 的方差趋近于克拉默-拉奥下界，即费雪信息的倒数。\n$$ \\text{Var}(\\hat{\\gamma}) \\approx [I(\\gamma)]^{-1} = \\frac{1}{n} \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} - \\left(\\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})}\\right)^2 \\right]^{-1} $$\n在实践中，$\\gamma$ 的真实值是未知的，因此通过用MLE $\\hat{\\gamma}$ 替换 $\\gamma$ 来估计此方差。", "answer": "$$\n\\boxed{\n\\pmatrix{\n\\frac{1}{n}\\sum_{i=1}^{n} \\ln(k_i) = - \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})}\n\n\\frac{1}{n} \\left[ \\frac{\\zeta''(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} - \\left( \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} \\right)^2 \\right]^{-1}\n}\n}\n$$", "id": "3316339"}]}