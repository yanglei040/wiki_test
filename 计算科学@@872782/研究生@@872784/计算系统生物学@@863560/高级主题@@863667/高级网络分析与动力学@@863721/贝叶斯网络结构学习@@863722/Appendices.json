{"hands_on_practices": [{"introduction": "在基于分数的贝叶斯网络结构学习中，算法通过迭代地修改网络结构（例如，添加、删除或反转一条边）并评估这些修改对网络总分数的影响来搜索最佳网络。这个练习将带你深入了解这一过程的核心，通过计算在添加一条候选调控边后，贝叶斯狄利克雷等价均匀 (BDeu) 分数的变化。通过完成这个计算 [@problem_id:3289730]，你将掌握在实践中评估结构变化的具体方法，这是理解和实现启发式搜索算法（如爬山法）的关键一步。", "problem": "在计算系统生物学中，贝叶斯网络结构学习常用于从离散化的表达谱中推断基因调控相互作用。考虑一个候选的调控-靶标对，其中潜在子节点 $Y$ 代表一个具有 $r_{Y} = 3$ 个离散表达状态 $\\{ \\text{low}, \\text{baseline}, \\text{high} \\}$ 的靶基因，而潜在父节点 $X$ 代表一个具有 $r_{X} = 2$ 个离散活动状态 $\\{ 0, 1 \\}$ 的转录调控因子。给定 $N = 30$ 个匹配基因活动谱的独立样本，其中 $Y$ 和 $X$ 已按上述方式离散化。\n\n当忽略 $X$ 时（即 $Y$ 没有父节点），$Y$ 的观测计数如下：\n- $n_{Y=\\text{low}} = 14$，\n- $n_{Y=\\text{baseline}} = 9$，\n- $n_{Y=\\text{high}} = 7$。\n\n当 $X$ 被视为 $Y$ 的父节点时，$Y$ 给定 $X$ 的观测列联表如下：\n- 当 $X = 0$ 时：$(n_{Y=\\text{low} \\mid X=0}, n_{Y=\\text{baseline} \\mid X=0}, n_{Y=\\text{high} \\mid X=0}) = (10, 5, 3)$，\n- 当 $X = 1$ 时：$(n_{Y=\\text{low} \\mid X=1}, n_{Y=\\text{baseline} \\mid X=1}, n_{Y=\\text{high} \\mid X=1}) = (4, 4, 4)$。\n\n假设 $Y$ 的条件概率表有一个贝叶斯狄利克雷等价均匀 (BDeu) 先验，该先验在状态和父节点配置上是均匀的，并由一个等效样本量 $\\alpha > 0$ 参数化。使用狄利克雷-多项式共轭模型的性质以及贝叶斯网络边际似然的模块化可分解性。\n\n将为节点 $Y$ 添加父节点 $X$ 引起的 BDeu 局部得分变化定义为：在 $Y$ 处，以 $X$ 为父节点时数据的对数边际似然与 $Y$ 没有父节点时数据的对数边际似然之差：\n$$\n\\Delta S_{\\text{BDeu}}(Y; X) \\equiv \\ln p(\\text{data at } Y \\mid \\text{$X$ is a parent of $Y$}, \\alpha) \\;-\\; \\ln p(\\text{data at } Y \\mid \\text{$Y$ has no parents}, \\alpha).\n$$\n\n仅基于上面提供的计数，使用自然对数和伽马函数，计算 $\\Delta S_{\\text{BDeu}}(Y; X)$ 作为一个关于 $\\alpha$ 的闭式解析表达式。将最终答案表示为关于 $\\alpha$ 的单个解析表达式。不要进行数值近似。", "solution": "该问题要求计算当从潜在父节点 $X$ 到目标节点 $Y$ 添加一条边时，BDeu 局部得分 $\\Delta S_{\\text{BDeu}}(Y; X)$ 的变化。该量定义为：在节点 $Y$ 处，有边模型 ($M_1$) 的数据对数边际似然与无边模型 ($M_0$) 的数据对数边际似然之差：\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(\\text{data at } Y \\mid M_1, \\alpha) - \\ln p(\\text{data at } Y \\mid M_0, \\alpha) $$\n对于具有给定父节点集 $Pa(Y)$ 的单个节点 $Y$ ，其数据 $D_Y$ 的对数边际似然的通用公式源自狄利克雷-多项式共轭模型。对于一个有 $r_Y$ 个状态的节点 $Y$ 和有 $q_Y$ 种配置的父节点，其对数边际似然为：\n$$ \\ln p(D_Y \\mid Pa(Y), \\alpha) = \\sum_{k=1}^{q_Y} \\left[ \\ln \\Gamma(\\alpha_k) - \\ln \\Gamma(n_k + \\alpha_k) + \\sum_{j=1}^{r_Y} \\left( \\ln \\Gamma(n_{jk} + \\alpha_{jk}) - \\ln \\Gamma(\\alpha_{jk}) \\right) \\right] $$\n其中 $n_{jk}$ 是 $Y$ 的状态 $j$ 和父节点配置 $k$ 的观测计数，$n_k = \\sum_{j=1}^{r_Y} n_{jk}$ 是父节点配置 $k$ 的总计数，而 $\\alpha_{jk}$ 是狄利克雷超参数。对于一个等效样本量为 $\\alpha$ 的 BDeu 先验，这些超参数被统一设置为：$\\alpha_{jk} = \\alpha / (q_Y r_Y)$。对于给定的父节点配置，这些超参数的和为 $\\alpha_k = \\sum_{j=1}^{r_Y} \\alpha_{jk} = r_Y \\cdot \\frac{\\alpha}{q_Y r_Y} = \\frac{\\alpha}{q_Y}$。\n\n我们首先计算 $Y$ 没有父节点的模型 $M_0$ 的对数边际似然。\n在这种情况下，$Pa(Y) = \\emptyset$，所以只有一种父节点配置，$q_Y = 1$。节点 $Y$ 有 $r_Y = 3$ 个状态。\nBDeu 超参数为 $\\alpha_j = \\alpha / (1 \\cdot 3) = \\alpha/3$，$j \\in \\{1, 2, 3\\}$。总等效样本量为 $\\alpha_0 = \\sum_{j=1}^{3} \\alpha_j = 3 (\\alpha/3) = \\alpha$。\n观测计数为 $n_{Y=\\text{low}} = 14$，$n_{Y=\\text{baseline}} = 9$ 和 $n_{Y=\\text{high}} = 7$。我们将其表示为 $n_1=14$, $n_2=9$, $n_3=7$。总样本量为 $n = 14+9+7 = 30$。\n$M_0$ 的对数边际似然为：\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\sum_{j=1}^{3} \\left( \\ln \\Gamma(n_j + \\frac{\\alpha}{3}) - \\ln \\Gamma(\\frac{\\alpha}{3}) \\right) $$\n$$ \\ln p(D_Y \\mid M_0, \\alpha) = \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30 + \\alpha) + \\ln \\Gamma(14 + \\frac{\\alpha}{3}) + \\ln \\Gamma(9 + \\frac{\\alpha}{3}) + \\ln \\Gamma(7 + \\frac{\\alpha}{3}) - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) $$\n\n接下来，我们计算 $X$ 是 $Y$ 的父节点的模型 $M_1$ 的对数边际似然。\n父节点 $X$ 有 $r_X = 2$ 个状态，因此有 $q_Y = 2$ 种父节点配置。节点 $Y$ 仍然有 $r_Y = 3$ 个状态。\nBDeu 超参数为 $\\alpha_{jk} = \\alpha / (2 \\cdot 3) = \\alpha/6$。\n对于每种父节点配置 $k$，等效样本量为 $\\alpha_k = \\alpha/q_Y = \\alpha/2$。\n总对数边际似然是每种父节点配置项的总和。\n\n对于父节点状态 $X=0$（我们称之为配置 $k=1$）：\n计数为 $n_{11}=10$, $n_{12}=5$, $n_{13}=3$。总计数为 $n_1 = 10+5+3 = 18$。\n对得分的贡献是：\n$$ \\ln p_1 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\n对于父节点状态 $X=1$（配置 $k=2$）：\n计数为 $n_{21}=4$, $n_{22}=4$, $n_{23}=4$。总计数为 $n_2 = 4+4+4 = 12$。\n对得分的贡献是：\n$$ \\ln p_2 = \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 3 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\n$M_1$ 的总对数边际似然为 $\\ln p(D_Y \\mid M_1, \\alpha) = \\ln p_1 + \\ln p_2$：\n$$ \\ln p(D_Y \\mid M_1, \\alpha) = 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18 + \\frac{\\alpha}{2}) - \\ln \\Gamma(12 + \\frac{\\alpha}{2}) + \\ln \\Gamma(10 + \\frac{\\alpha}{6}) + \\ln \\Gamma(5 + \\frac{\\alpha}{6}) + \\ln \\Gamma(3 + \\frac{\\alpha}{6}) + 3 \\ln \\Gamma(4 + \\frac{\\alpha}{6}) - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) $$\n\n最后，我们计算差值 $\\Delta S_{\\text{BDeu}}(Y; X) = \\ln p(D_Y \\mid M_1, \\alpha) - \\ln p(D_Y \\mid M_0, \\alpha)$：\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\left[ 2 \\ln \\Gamma(\\frac{\\alpha}{2}) - \\ln \\Gamma(18+\\frac{\\alpha}{2}) - \\ln \\Gamma(12+\\frac{\\alpha}{2}) + \\dots - 6 \\ln \\Gamma(\\frac{\\alpha}{6}) \\right] - \\left[ \\ln \\Gamma(\\alpha) - \\ln \\Gamma(30+\\alpha) + \\dots - 3 \\ln \\Gamma(\\frac{\\alpha}{3}) \\right] $$\n重新整理这些项得到最终表达式：\n$$ \\Delta S_{\\text{BDeu}}(Y; X) = \\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right) $$\n该表达式表示为从 $X$到 $Y$ 添加调控链接所带来的对数证据的变化。", "answer": "$$\n\\boxed{\\ln\\Gamma(30+\\alpha) - \\ln\\Gamma(\\alpha) + 2\\ln\\Gamma\\left(\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(18+\\frac{\\alpha}{2}\\right) - \\ln\\Gamma\\left(12+\\frac{\\alpha}{2}\\right) + 3\\ln\\Gamma\\left(\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(14+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(9+\\frac{\\alpha}{3}\\right) - \\ln\\Gamma\\left(7+\\frac{\\alpha}{3}\\right) + \\ln\\Gamma\\left(10+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(5+\\frac{\\alpha}{6}\\right) + \\ln\\Gamma\\left(3+\\frac{\\alpha}{6}\\right) + 3\\ln\\Gamma\\left(4+\\frac{\\alpha}{6}\\right) - 6\\ln\\Gamma\\left(\\frac{\\alpha}{6}\\right)}\n$$", "id": "3289730"}, {"introduction": "与基于分数的学习方法不同，基于约束的方法（如 Peter-Clark 或 PC 算法）通过执行一系列条件独立性检验来推断网络骨架。这种方法的计算成本是其在基因组学等高维环境中应用的主要考量因素。这个练习 [@problem_id:3289722] 要求你推导 PC 算法在最坏情况下所需的条件独立性检验次数的上限，这有助于你从根本上理解这些算法的可扩展性限制，并为在实际研究中选择合适的算法提供理论依据。", "problem": "在计算系统生物学中，将基因调控网络推断为贝叶斯网络的背景下，考虑对一组代表基因表达水平的 $p$ 个随机变量应用 Peter and Clark (PC) 算法进行骨架学习。贝叶斯网络是一个有向无环图，其结构编码了条件独立关系，而 PC 算法中的骨架学习通过对变量对 $X$ 和 $Y$ 以及条件集 $S$ 迭代地检验形如 $X \\perp Y \\mid S$ 的条件独立性陈述来进行。\n\n假设由于样本量和生物可行性的限制，最大条件集的大小被限制为 $k$，这意味着算法不会考虑 $|S| > k$ 的集合 $S$。在最坏情况下，直到所有大小不超过 $k$ 的条件集的条件独立性检验都完成后，才会有边被移除，因此在整个骨架学习阶段，邻接集都保持其最大可能的大小。\n\n从贝叶斯网络中条件独立性的核心定义和条件集的组合枚举出发，推导出一个关于 $p$ 和 $k$ 的单一边界封闭形式表达式，作为 PC 算法在骨架学习期间最坏情况下可能执行的条件独立性检验总数的显式上界。然后，基于此上界，阐明在高维基因表达环境中，当 $p$ 增长而 $k$ 固定时，其对可扩展性的影响。最终答案必须以单个符号表达式的形式给出。不需要进行数值舍入。", "solution": "目标是找到 PC 算法执行的条件独立性（CI）检验总数的上界。该算法通过对大小递增的条件集，迭代地检验变量对之间的条件独立性。如问题所述，最坏情况场景假设底层图是完全的（或足够稠密），以至于在算法的初始阶段没有边被移除。这迫使算法执行最大可能数量的检验。\n\n检验的总数可以通过考虑所有可能的变量对和有效条件集的组合来计算。\n\n1.  **变量对的选择：**\n    任何 CI 检验 $X \\perp Y \\mid S$ 的第一步是从 $p$ 个变量的集合中选择一对不同的变量 $\\{X, Y\\}$。从 $p$ 个变量中选择 2 个变量的方法数由二项式系数 $\\binom{p}{2}$ 给出。\n    $$ \\text{变量对数量} = \\binom{p}{2} = \\frac{p(p-1)}{2} $$\n\n2.  **条件集的选择：**\n    对于每个选定的对 $\\{X, Y\\}$，算法会以剩余 $p-2$ 个变量的子集 $S$ 为条件来检验独立性。PC 算法通过增加条件集的大小（我们表示为 $s = |S|$）来进行。问题陈述中指出，条件集的最大大小被限制为 $k$。因此，对于任意给定的对 $\\{X, Y\\}$，算法将检验所有大小从 $s=0$ 到 $s=k$ 的条件集。\n\n    - 对于大小为 $s=0$ 的条件集，集合 $S$ 是空集 $\\emptyset$。只有一个这样的集合，即 $\\binom{p-2}{0} = 1$。\n    - 对于大小为 $s=1$ 的条件集，我们必须从剩余的 $p-2$ 个变量中选择 1 个变量。这样的集合数量为 $\\binom{p-2}{1}$。\n    - 对于大小为 $s$ 的条件集，我们必须从剩余的 $p-2$ 个变量中选择 $s$ 个变量。这样的集合数量为 $\\binom{p-2}{s}$。\n\n    在最坏情况下，对于给定的对 $\\{X, Y\\}$，算法必须对每个大小为 $|S| \\in \\{0, 1, \\dots, k\\}$ 的可能条件集 $S$ 执行 CI 检验，其中 $S$ 的成员是从除 $X$ 和 $Y$ 之外的 $p-2$ 个变量中选择的。对单个对需要检验的条件集总数是从大小 $s=0$ 到 $k$ 的每个大小的可能集合数量之和：\n    $$ \\text{每对的条件集数量} = \\sum_{s=0}^{k} \\binom{p-2}{s} $$\n\n3.  **条件独立性检验的总数：**\n    CI 检验总数的上界，我们记为 $N_{max}$，是变量对数量与每对检验的条件集数量的乘积。\n    $$ N_{max}(p, k) = (\\text{变量对数量}) \\times (\\text{每对的条件集数量}) $$\n    代入上面推导出的表达式，我们得到上界的最终封闭形式表达式：\n    $$ N_{max}(p, k) = \\binom{p}{2} \\sum_{s=0}^{k} \\binom{p-2}{s} $$\n\n### 对可扩展性的影响\n\n为了理解对可扩展性的影响，我们分析当基因数量 $p$ 变得很大而最大条件集大小 $k$ 固定时，该表达式的渐近行为。\n\n- 第一项 $\\binom{p}{2} = \\frac{p(p-1)}{2}$ 的阶为 $O(p^2)$。\n- 第二项是二项式系数的部分和，$\\sum_{s=0}^{k} \\binom{p-2}{s}$。对于固定的 $k$ 和大的 $p$，此和中的主导项是具有最高 $p$ 次幂的项。项 $\\binom{p-2}{s}$ 是一个关于 $p$ 的 $s$ 次多项式。因此，求和中的最高次项是 $\\binom{p-2}{k}$。\n$$ \\binom{p-2}{k} = \\frac{(p-2)(p-3)\\cdots(p-2-k+1)}{k!} = O(p^k) $$\n- 因此，整个求和的阶为 $O(p^k)$。\n\n结合这些，检验次数的总体复杂度为：\n$$ N_{max}(p, k) \\sim O(p^2) \\cdot O(p^k) = O(p^{k+2}) $$\n\n这种多项式复杂度对基因组学等高维应用具有重要影响。虽然关于 $p$ 的多项式复杂度远比指数复杂度（如果允许 $k$ 随 $p$ 增长，就会出现这种情况）易于处理，但多项式的次数是 $k+2$。在实践中，基因调控网络可能表现出长程依赖性，需要 $k$ 至少为 3 或 4 才能进行有意义的发现。这导致了 $O(p^5)$ 或 $O(p^6)$ 数量级的计算负担，当 $p$ 代表数万个基因时，这在计算上是令人望而却步的。这种可扩展性问题是 PC 算法直接应用于大规模生物数据具有挑战性，并且通常需要修改、启发式方法或并行化的主要原因。推导出的表达式为这一关键结论提供了正式依据。", "answer": "$$ \\boxed{\\binom{p}{2} \\sum_{s=0}^{k} \\binom{p-2}{s}} $$", "id": "3289722"}, {"introduction": "无论使用何种算法学习网络结构，我们都需要一种严谨的方法来评估学习结果的准确性。结构汉明距离 (Structural Hamming Distance, SHD) 是比较两个网络结构（例如，学习到的网络与真实网络）的黄金标准。这个练习 [@problem_id:3289677] 要求你首先给出 SHD 的正式定义，然后在一个具体的三节点网络示例中计算它。掌握 SHD 的计算不仅能让你量化算法的性能，还能加深你对网络等价类（以 CPDAG 表示）和不同类型结构错误（如缺边、多边和方向错误）的理解。", "problem": "在计算系统生物学中，利用贝叶斯网络结构学习进行基因调控网络推断时，通常会使用定义在等价类代表上的距离来评估学习到的结构与参考结构之间的差异。考虑代表3个基因（$X_1$、$X_2$ 和 $X_3$）表达水平的随机变量。设参考结构为完备部分有向无环图（CPDAG）$\\mathcal{G}^{\\star}$，其节点集为 $\\{X_1,X_2,X_3\\}$，包含边 $X_1 \\to X_2$ 和 $X_3 \\to X_2$，且 $X_1$ 与 $X_3$ 之间没有邻接关系。设学习到的CPDAG为 $\\widehat{\\mathcal{G}}$，其包含边 $X_2 \\to X_1$ 和 $X_3 \\to X_1$，且 $X_2$ 与 $X_3$ 之间没有邻接关系。\n\n从贝叶斯网络和马尔可夫等价类的核心定义出发，首先给出两个CPDAG之间结构汉明距离（SHD）的正式定义，该定义应基于协调邻接集和端点标记（尾部和箭头）所需的最少单边编辑操作次数。然后，使用您的定义，计算 $\\mathcal{G}^{\\star}$ 和 $\\widehat{\\mathcal{G}}$ 之间的SHD。将最终答案表示为一个整数，无需四舍五入。", "solution": "问题要求给出两个完备部分有向无环图（CPDAG）之间结构汉明距离（SHD）的正式定义，并随后针对给定的参考结构 $\\mathcal{G}^{\\star}$ 和学习到的结构 $\\widehat{\\mathcal{G}}$ 进行计算。\n\n首先，我们建立背景。贝叶斯网络是一种概率图模型，通过有向无环图（DAG）来表示一组随机变量及其条件依赖关系。多个DAG可以表示同一组条件独立性；这样的一组DAG构成一个马尔可夫等价类。CPDAG是马尔可夫等价类的规范图表示。它由一组顶点和一组边组成，其中边可以是定向的（强制的）或无向的。CPDAG中的一条有向边 $X \\to Y$ 表示该方向存在于等价类中的所有DAG中。一条无向边 $X-Y$ 表示该等价类既包含带有 $X \\to Y$ 的DAG，也包含带有 $Y \\to X$ 的DAG。\n\n结构汉明距离（SHD）是一种度量标准，用于量化两个图（通常是一个学习到的图和一个真实的或参考的图）之间的差异。对于在同一顶点集 $V$ 上定义的两个CPDAG $\\mathcal{G}_1$ 和 $\\mathcal{G}_2$，SHD是将 $\\mathcal{G}_1$ 转换为 $\\mathcal{G}_2$ 所需的基本图编辑操作的总数。这些操作分为邻接关系（图的骨架）的差异和边方向的差异。\n\n形式上，SHD定义如下：\n设 $\\mathcal{G}_1$ 和 $\\mathcal{G}_2$ 是两个CPDAG。SHD是协调它们之间差异的成本总和，分两步计算：\n1.  **邻接差异**：此部分计算存在于一个图的骨架中但不存在于另一个图的骨架中的边的数量。设 $A_1$ 和 $A_2$ 分别为 $\\mathcal{G}_1$ 和 $\\mathcal{G}_2$ 的邻接集（表示邻域关系的无向边集）。邻接差异的数量是这些集合的对称差的大小，即 $|A_1 \\Delta A_2| = |(A_1 \\setminus A_2) \\cup (A_2 \\setminus A_1)|$。此对称差中的每个元素对应一条缺失的边（如果 $\\mathcal{G}_2$ 是 $\\mathcal{G}_1$ 的估计，则为假阴性）或一条多余的边（假阳性）。每个这样的差异对SHD的贡献为1。\n\n2.  **方向差异**：此部分仅考虑同时存在于两个骨架中的边（即，对于 $A_1 \\cap A_2$ 中的邻接关系）。对于每个共同的邻接关系，如果边的标记（尾部和箭头）不匹配，则产生方向差异。对于CPDAG，共同邻接关系 $\\{u,v\\}$ 可能的不匹配情况有：\n    *   $\\mathcal{G}_1$ 中有 $u-v$（无向），而 $\\mathcal{G}_2$ 中有 $u \\to v$ 或 $v \\to u$（有向）。\n    *   $\\mathcal{G}_1$ 中有 $u \\to v$，而 $\\mathcal{G}_2$ 中有 $v \\to u$（反转）。\n    每个这样的共同边的方向不匹配对SHD的贡献为1。\n\n总SHD是这两步计数的总和。\n\n现在我们应用此定义来计算给定的参考CPDAG $\\mathcal{G}^{\\star}$ 和学习到的CPDAG $\\widehat{\\mathcal{G}}$ 之间的SHD。随机变量（基因）的集合是 $V = \\{X_1, X_2, X_3\\}$。\n\n参考结构 $\\mathcal{G}^{\\star}$ 包含边 $X_1 \\to X_2$ 和 $X_3 \\to X_2$，且 $X_1$ 与 $X_3$ 之间没有邻接关系。这种结构，$X_1 \\to X_2 \\leftarrow X_3$，是一个v型结构。在CPDAG中，构成v型结构的边总是定向的。因此，$\\mathcal{G}^{\\star}$ 是一个恰好包含这两条有向边的图。\n*   $\\mathcal{G}^{\\star}$ 的邻接集：$A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$。\n*   $\\mathcal{G}^{\\star}$ 的有向边集：$D^{\\star} = \\{X_1 \\to X_2, X_3 \\to X_2\\}$。\n\n学习到的结构 $\\widehat{\\mathcal{G}}$ 包含边 $X_2 \\to X_1$ 和 $X_3 \\to X_1$，且 $X_2$ 与 $X_3$ 之间没有邻接关系。这种结构，$X_2 \\to X_1 \\leftarrow X_3$，也是一个v型结构。同样，这些边在CPDAG中必须是定向的。\n*   $\\widehat{\\mathcal{G}}$ 的邻接集：$\\widehat{A} = \\{\\{X_2, X_1\\}, \\{X_3, X_1\\}\\}$。注意 $\\{X_2, X_1\\}$ 与 $\\{X_1, X_2\\}$ 相同。为保持一致性，我们写作：$\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$。\n*   $\\widehat{\\mathcal{G}}$ 的有向边集：$\\widehat{D} = \\{X_2 \\to X_1, X_3 \\to X_1\\}$。\n\n我们分两步进行计算。\n\n**第1步：计算邻接差异**\n我们比较邻接集 $A^{\\star}$ 和 $\\widehat{A}$。\n$A^{\\star} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\}$\n$\\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\}$\n\n*   在 $A^{\\star}$ 中但不在 $\\widehat{A}$ 中的边：$A^{\\star} \\setminus \\widehat{A} = \\{\\{X_2, X_3\\}\\}$。这是存在于参考图中但缺失于学习图中的一条边（假阴性）。它对SHD的贡献为1。\n*   在 $\\widehat{A}$ 中但不在 $A^{\\star}$ 中的边：$\\widehat{A} \\setminus A^{\\star} = \\{\\{X_1, X_3\\}\\}$。这是缺失于参考图中但存在于学习图中的一条边（假阳性）。它对SHD的贡献为1。\n\n邻接差异的总数为 $1 + 1 = 2$。\n\n**第2步：计算方向差异**\n我们检查邻接集交集中的边的方向。\n$A_{common} = A^{\\star} \\cap \\widehat{A} = \\{\\{X_1, X_2\\}, \\{X_2, X_3\\}\\} \\cap \\{\\{X_1, X_2\\}, \\{X_1, X_3\\}\\} = \\{\\{X_1, X_2\\}\\}$。\n存在一个共同的邻接关系：$\\{X_1, X_2\\}$。\n\n*   在 $\\mathcal{G}^{\\star}$ 中，对应于此邻接关系的边是定向的：$X_1 \\to X_2$。\n*   在 $\\widehat{\\mathcal{G}}$ 中，对应于此邻接关系的边也是定向的，但方向相反：$X_2 \\to X_1$。\n\n这构成了一个边的反转。一个反转是一个单一的方向错误。它对SHD的贡献为1。\n\n**第3步：计算总SHD**\n总SHD是邻接差异和方向差异的总和。\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = (\\text{邻接差异}) + (\\text{方向差异})$\n$\\text{SHD}(\\mathcal{G}^{\\star}, \\widehat{\\mathcal{G}}) = 2 + 1 = 3$。\n\n因此，参考CPDAG $\\mathcal{G}^{\\star}$ 与学习到的CPDAG $\\widehat{\\mathcal{G}}$ 之间的结构汉明距离为 $3$。", "answer": "$$\n\\boxed{3}\n$$", "id": "3289677"}]}