{"hands_on_practices": [{"introduction": "node2vec 的核心创新在于其有偏二阶随机游走机制。为了真正掌握该算法，我们必须能够计算指导游走的转移概率。这个练习提供了一个具体的场景，让学生们在一个简单的网络模体上，通过手动计算来深入理解返回参数 $p$ 和进出参数 $q$ 如何共同作用，从而揭示 node2vec 探索策略的底层数学原理 [@problem_id:3331426]。", "problem": "在一项计算系统生物学研究中，考虑一个由三个标记为 $a$、$b$ 和 $c$ 的蛋白质组成的蛋白质-蛋白质相互作用（PPI）网络模体。这三个蛋白质连接成一个无向三角形，边 $a$–$b$、$b$–$c$ 和 $c$–$a$ 上的单位边权重均为 $1$。为了使用 node2vec 学习网络嵌入，采用了一种带有返回参数 $p$ 和进出参数 $q$ 的二阶有偏随机游走。\n\n假设一次游走当前位于节点 $b$，而前一个节点是 $a$。在这个无权三角形上使用参数为 $(p,q)=(2,0.5)$ 的 node2vec 算法，计算从节点 $b$ 到其每个邻居 $a$ 和 $c$ 的下一步转移概率。请将您的答案表示为一个行向量 $\\left(P_{b\\to a}\\;\\;P_{b\\to c}\\right)$，并使用精确的分数形式。请勿进行近似或四舍五入。", "solution": "该问题是有效的。它在科学上基于计算网络科学中成熟的 `node2vec` 算法，问题陈述清晰，提供了所有必要信息，并且表述客观。我们可以开始求解。\n\n`node2vec` 算法的核心是一种有偏的二阶随机游走。在给定游走刚刚从前一个节点 $t$ 到达当前节点 $v$ 的情况下，从 $v$ 转移到下一个节点 $x$ 的概率由一个特定的加权方案定义。\n\n设游走刚刚经过边 $(t, v)$ 到达当前节点 $v$。从 $v$ 到其某个邻居 $x$ 的未归一化转移概率由下式给出：\n$$\n\\pi_{vx} = \\alpha_{pq}(t, x) \\cdot w_{vx}\n$$\n其中 $w_{vx}$ 是边 $(v, x)$ 的静态权重，而 $\\alpha_{pq}(t, x)$ 是 `node2vec` 的搜索偏置。该搜索偏置取决于返回参数 $p$ 和进出参数 $q$，其定义如下：\n$$\n\\alpha_{pq}(t, x) =\n\\begin{cases}\n    \\frac{1}{p}  & \\text{若 } d_{tx} = 0 \\\\\n    1  & \\text{若 } d_{tx} = 1 \\\\\n    \\frac{1}{q}  & \\text{若 } d_{tx} = 2\n\\end{cases}\n$$\n此处，$d_{tx}$ 表示前一个节点 $t$ 和潜在的下一个节点 $x$ 之间的最短路径距离。\n\n然后，通过将未归一化的概率除以节点 $v$ 的所有邻居上的未归一化概率之和，来计算归一化转移概率 $P(v \\to x)$：\n$$\nP(v \\to x) = \\frac{\\pi_{vx}}{\\sum_{x' \\in N(v)} \\pi_{vx'}}\n$$\n其中 $N(v)$ 是节点 $v$ 的邻居集合。\n\n在给定的问题中，网络是一个由节点 $a$、$b$ 和 $c$ 组成的无权三角形。这意味着所有存在边的权重都为 $1$，因此 $w_{ab} = w_{bc} = w_{ca} = 1$。游走当前位于节点 $v=b$，前一个节点是 $t=a$。当前节点 $b$ 的邻居是 $N(b) = \\{a, c\\}$。我们必须计算从 $b$ 到 $a$ 以及从 $b$ 到 $c$ 的转移概率。给定的参数为 $p=2$ 和 $q=0.5$。\n\n我们需要为每个邻居 $x \\in N(b)$ 计算未归一化的转移概率 $\\pi_{bx}$。\n\n**1. 从 $b$ 转移到 $a$（$x=a$）：**\n潜在的下一个节点是 $a$。我们必须确定前一个节点 $t=a$ 和下一个节点 $x=a$ 之间的最短路径距离 $d_{tx}$。一个节点到其自身的距离为 $d_{aa} = 0$。\n根据搜索偏置的公式，对于 $d_{tx}=0$，我们有 $\\alpha_{pq}(a, a) = \\frac{1}{p}$。\n边权重为 $w_{ba} = 1$。\n未归一化的转移概率为：\n$$\n\\pi_{ba} = \\alpha_{pq}(a, a) \\cdot w_{ba} = \\frac{1}{p} \\cdot 1 = \\frac{1}{2}\n$$\n\n**2. 从 $b$ 转移到 $c$（$x=c$）：**\n潜在的下一个节点是 $c$。我们必须确定前一个节点 $t=a$ 和下一个节点 $x=c$ 之间的最短路径距离 $d_{tx}$。在这个三角形模体中，节点 $a$ 和 $c$ 通过一条边直接相连。因此，最短路径距离为 $d_{ac} = 1$。\n根据搜索偏置的公式，对于 $d_{tx}=1$，我们有 $\\alpha_{pq}(a, c) = 1$。\n边权重为 $w_{bc} = 1$。\n未归一化的转移概率为：\n$$\n\\pi_{bc} = \\alpha_{pq}(a, c) \\cdot w_{bc} = 1 \\cdot 1 = 1\n$$\n注意，进出参数 $q$ 在此特定计算中未使用，因为节点 $b$ 的邻居中没有一个与前一个节点 $a$ 的距离为 $2$。\n\n现在，我们计算归一化常数，即从 $b$ 出发所有可能下一步的未归一化概率之和：\n$$\nZ = \\sum_{x' \\in N(b)} \\pi_{bx'} = \\pi_{ba} + \\pi_{bc} = \\frac{1}{2} + 1 = \\frac{3}{2}\n$$\n\n最后，我们通过将每个未归一化概率除以归一化常数 $Z$ 来计算归一化转移概率。\n\n从 $b$ 转移到 $a$ 的概率是：\n$$\nP_{b \\to a} = \\frac{\\pi_{ba}}{Z} = \\frac{1/2}{3/2} = \\frac{1}{2} \\cdot \\frac{2}{3} = \\frac{1}{3}\n$$\n\n从 $b$ 转移到 $c$ 的概率是：\n$$\nP_{b \\to c} = \\frac{\\pi_{bc}}{Z} = \\frac{1}{3/2} = 1 \\cdot \\frac{2}{3} = \\frac{2}{3}\n$$\n\n最终的转移概率为 $P_{b\\to a} = \\frac{1}{3}$ 和 $P_{b\\to c} = \\frac{2}{3}$。题目要求将答案表示为行向量 $(P_{b\\to a}\\;\\;P_{b\\to c})$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "3331426"}, {"introduction": "通过随机游走生成节点序列后，下一步便是学习节点的嵌入向量。这个过程通常采用带负采样的 Skip-Gram 模型 (SGNS)。本练习聚焦于学习过程的核心：单步随机梯度上升，它展示了如何根据一个正样本对和几个负样本来更新源节点的嵌入向量，从而直观地揭示了嵌入向量是如何被优化以捕捉网络中的共现关系 [@problem_id:3331408]。", "problem": "在一个来自计算系统生物学的蛋白质-蛋白质相互作用网络中，假设我们使用带有负采样的 node2vec skip-gram 模型来训练节点表示。设源（节点）嵌入为一个向量 $\\mathbf{z}_u \\in \\mathbb{R}^2$，上下文（节点）嵌入为向量 $\\mathbf{z}'_x \\in \\mathbb{R}^2$。对于单个随机更新步骤，考虑以下设置，该设置源于一个访问了节点 $u$ 并观察到上下文节点 $v$ 的二阶随机游走：\n- 正共现对为 $(u,v)$。\n- 两个负样本为 $n_1$ 和 $n_2$。\n\n使用以下核心定义作为出发点：\n- 逻辑 sigmoid 函数为 $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$。\n- 带有两个负样本的单样本对数似然目标为\n$$\nL(u,v,n_1,n_2) \\;=\\; \\ln\\!\\big(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)\\big) \\;+\\; \\sum_{i=1}^{2} \\ln\\!\\big(\\sigma(-\\,\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\big).\n$$\n\n假设当前的嵌入和学习率如下：\n- $\\mathbf{z}_u = (1,0)$,\n- $\\mathbf{z}'_v = (0,1)$,\n- $\\mathbf{z}'_{n_1} = (1,1)$,\n- $\\mathbf{z}'_{n_2} = (-1,0)$,\n- 学习率 $\\eta = 0.05$。\n\n在此步骤中，将 $\\mathbf{z}'_v$、$\\mathbf{z}'_{n_1}$ 和 $\\mathbf{z}'_{n_2}$ 视为常量。从给定的 $\\mathbf{z}_u$ 开始，对 $L(u,v,n_1,n_2)$ 关于 $\\mathbf{z}_u$ 执行一步随机梯度上升。计算此单步更新后的源嵌入向量 $\\mathbf{z}_u^{\\text{new}}$。将最终向量的每个分量四舍五入到六位有效数字。以单个行向量的形式提供您的最终答案。", "solution": "本题的目标是执行一步随机梯度上升来更新源节点嵌入 $\\mathbf{z}_u$。随机梯度上升的更新规则由下式给出：\n$$\n\\mathbf{z}_u^{\\text{new}} = \\mathbf{z}_u + \\eta \\nabla_{\\mathbf{z}_u} L\n$$\n其中 $\\eta$ 是学习率，$\\nabla_{\\mathbf{z}_u} L$ 是对数似然目标函数 $L$ 关于源嵌入 $\\mathbf{z}_u$ 的梯度。\n\n对数似然目标函数由下式给出：\n$$\nL(u,v,n_1,n_2) = \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) + \\sum_{i=1}^{2} \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))\n$$\n\n首先，我们必须计算梯度 $\\nabla_{\\mathbf{z}_u} L$。我们将分别计算 $L$ 中每一项的梯度。逻辑 sigmoid 函数 $\\sigma(t) = (1+\\exp(-t))^{-1}$ 的导数是 $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$。\n\n对于任意标量值函数 $f(x)$，$\\ln(f(x))$ 的导数是 $\\frac{f'(x)}{f(x)}$。因此，$\\ln(\\sigma(t))$ 关于 $t$ 的导数是：\n$$\n\\frac{d}{dt}\\ln(\\sigma(t)) = \\frac{\\sigma'(t)}{\\sigma(t)} = \\frac{\\sigma(t)(1-\\sigma(t))}{\\sigma(t)} = 1-\\sigma(t)\n$$\n\n现在，我们应用链式法则来求 $L$ 中每一项关于向量 $\\mathbf{z}_u$ 的梯度。\n\n1.  **正样本项的梯度**：\n    设第一项为 $L_{pos} = \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))$。\n    使用链式法则，其梯度为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{pos} = \\frac{d}{d(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)} \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) \\cdot \\nabla_{\\mathbf{z}_u}(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)\n    $$\n    这可以简化为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{pos} = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) \\mathbf{z}'_v\n    $$\n\n2.  **负样本项的梯度**：\n    设一个负样本项为 $L_{neg,i} = \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))$。\n    使用链式法则，其梯度为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = \\frac{d}{d(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})} \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) \\cdot \\nabla_{\\mathbf{z}_u}(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\n    $$\n    这可以简化为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = (1 - \\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) (-\\mathbf{z}'_{n_i})\n    $$\n    使用恒等式 $\\sigma(-x) = 1 - \\sigma(x)$，我们可以将项 $(1 - \\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))$ 重写为 $1 - (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) = \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})$。\n    因此，负样本项的梯度为：\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = -\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\mathbf{z}'_{n_i}\n    $$\n\n综合这些结果，$L$ 的完整梯度为：\n$$\n\\nabla_{\\mathbf{z}_u} L = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))\\mathbf{z}'_v - \\sum_{i=1}^{2} \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\mathbf{z}'_{n_i}\n$$\n$$\n\\nabla_{\\mathbf{z}_u} L = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))\\mathbf{z}'_v - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_1})\\mathbf{z}'_{n_1} - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_2})\\mathbf{z}'_{n_2}\n$$\n\n接下来，我们代入给定的数值：\n$\\mathbf{z}_u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$\\mathbf{z}'_v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，$\\mathbf{z}'_{n_1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$，$\\mathbf{z}'_{n_2} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n\n我们计算点积：\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_v = (1)(0) + (0)(1) = 0$\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_1} = (1)(1) + (0)(1) = 1$\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_2} = (1)(-1) + (0)(0) = -1$\n\n现在，我们计算这些值的 sigmoid 函数：\n- $\\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}$\n- $\\sigma(1) = \\frac{1}{1+\\exp(-1)}$\n- $\\sigma(-1) = \\frac{1}{1+\\exp(1)}$\n\n将这些值代入梯度表达式：\n$$\n\\nabla_{\\mathbf{z}_u} L = \\left(1 - \\frac{1}{2}\\right)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(-1)}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(1)}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}_u} L = \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(-1)}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(1)}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n让我们求梯度向量的分量：\nx 分量：\n$0 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{1+\\exp(-1)} - (-1) \\cdot \\frac{1}{1+\\exp(1)} = -\\frac{1}{1+\\exp(-1)} + \\frac{1}{1+\\exp(1)}$\n使用 $\\exp(-1) = 1/e$，这变成 $-\\frac{1}{1+1/e} + \\frac{1}{1+e} = -\\frac{e}{e+1} + \\frac{1}{e+1} = \\frac{1-e}{e+1}$。\ny 分量：\n$1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{1+\\exp(-1)} - 0 \\cdot \\frac{1}{1+\\exp(1)} = \\frac{1}{2} - \\frac{e}{e+1} = \\frac{e+1-2e}{2(e+1)} = \\frac{1-e}{2(e+1)}$。\n\n所以，梯度向量是：\n$$\n\\nabla_{\\mathbf{z}_u} L = \\begin{pmatrix} \\frac{1-e}{e+1} \\\\ \\frac{1-e}{2(e+1)} \\end{pmatrix}\n$$\n在数值上，$\\frac{1-e}{e+1} \\approx \\frac{1-2.7182818}{1+2.7182818} \\approx -0.46211716$。\n所以, $\\nabla_{\\mathbf{z}_u} L \\approx \\begin{pmatrix} -0.46211716 \\\\ -0.23105858 \\end{pmatrix}$。\n\n现在我们用 $\\eta = 0.05$ 执行更新步骤：\n$$\n\\mathbf{z}_u^{\\text{new}} = \\mathbf{z}_u + \\eta \\nabla_{\\mathbf{z}_u} L = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.05 \\begin{pmatrix} \\frac{1-e}{e+1} \\\\ \\frac{1-e}{2(e+1)} \\end{pmatrix}\n$$\n$$\n\\mathbf{z}_u^{\\text{new}} = \\begin{pmatrix} 1 + 0.05\\left(\\frac{1-e}{e+1}\\right) \\\\ 0.05\\left(\\frac{1-e}{2(e+1)}\\right) \\end{pmatrix}\n$$\n计算各分量的数值：\nx 分量：$1 + 0.05 \\times (-0.46211716) = 1 - 0.023105858 \\approx 0.976894142$。\ny 分量：$0.05 \\times (-0.23105858) = -0.011552929$。\n\n将每个分量四舍五入到六位有效数字：\n- 第一个分量 $0.976894142$ 变成 $0.976894$。\n- 第二个分量 $-0.011552929$ 变成 $-0.0115529$。\n\n更新后的源嵌入向量为 $\\mathbf{z}_u^{\\text{new}} = (0.976894, -0.0115529)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.976894 & -0.0115529\n\\end{pmatrix}\n}\n$$", "id": "3331408"}, {"introduction": "node2vec 的强大之处在于其随机游走策略是可调的。参数 $p$ 和 $q$ 的选择并非随意，而是一种战略性决策，旨在使探索策略与最终嵌入向量应具备的特性相匹配。本练习挑战你从计算转向概念理解，思考如何调整这些参数，以分别侧重于捕捉网络中的社群结构（同质性）或功能角色（结构等价性），这是在计算系统生物学研究中有效应用该算法的关键技能 [@problem_id:3331387]。", "problem": "考虑一个蛋白质-蛋白质相互作用（PPI）网络，该网络是无向且无权的，具有密集的模块内社群和相对较少连接这些社群的连接器蛋白质。您正在使用 node2vec 为下游分类任务学习节点嵌入，并且必须选择返回参数 $p$ 和进出参数 $q$，以优先进行社群检测（同质性导向的嵌入）或角色发现（结构等價性导向的嵌入）。假设在给定图上进行标准的 node2vec 带偏二阶随机游走，并且下游嵌入模型（例如，带负采样的 Skip-gram）能捕获这些游走产生的共现统计信息。\n\n在这种 PPI 设置中，$(p,q)$ 的哪种选择最能分别强调社群检测与角色发现？\n\nA. 社群检测: $p<1, q>1$；角色发现: $p>1, q<1$。\n\nB. 社群检测: $p>1, q<1$；角色发现: $p<1, q>1$。\n\nC. 社群检测: $p\\approx 1, q\\approx 1$；角色发现: $p\\approx 1, q\\approx 1$。\n\nD. 社群检测: $p\\gg 1, q\\gg 1$；角色发现: $p\\ll 1, q\\ll 1$。\n\nE. 社群检测: $p>1, q>1$；角色发现: $p<1, q<1$。\n\n选择唯一最佳选项，并准备好从 node2vec 转移机制的基本原理出发，论证你的选择，同时考虑密集的社群和少数连接器如何影响在带偏随机游走中遇到的节点之间的距离。", "solution": "所陈述的问题是有效的。它在网络科学和机器学习领域具有科学依据，特别是关于 node2vec 算法。术语是标准且定义明確的，问题是自洽且适定的，可以根据算法的原理得出一个唯一的逻辑解。\n\n问题的核心是理解 node2vec 的参数，即返回参数 $p$ 和进出参数 $q$，如何控制底层二阶随机游走的探索策略，并将这些策略映射到社群检测（同质性）和角色发现（结构等价性）的目标上。\n\n让我们首先形式化 node2vec 的转移机制。假设一次随机游走刚刚遍历了边 $(t, v)$ 并当前位于节点 $v$。游走的下一个节点 $x$ 从 $v$ 的邻居 $N(v)$ 中选择。从 $v$ 移動到相邻节点 $x$ 的未归一化转移概率 $\\pi_{vx}$ 由下式给出：\n$$ \\pi_{vx} = \\alpha_{pq}(t, x) \\cdot w_{vx} $$\n由于图是无权的，所有存在边的边权 $w_{vx} = 1$。因此，游走的偏差完全由搜索偏差因子 $\\alpha_{pq}(t, x)$ 决定，该因子根据前一个节点 $t$ 和候选下一个节点 $x$ 之间的最短路径距离 $d_{tx}$ 定义：\n$$ \\alpha_{pq}(t, x) = \\begin{cases} 1/p & \\text{如果 } d_{tx} = 0 \\\\ 1 & \\text{如果 } d_{tx} = 1 \\\\ 1/q & \\text{如果 } d_{tx} = 2 \\end{cases} $$\n这里，$d_{tx} = 0$ 意味着 $x = t$，即游走返回到前一个节点。$d_{tx} = 1$ 意味着 $x$ 也是 $t$ 的邻居。$d_{tx} = 2$ 意味着 $x$ 是 $v$ 的邻居但不是 $t$ 的邻居。\n\n参数 $p$ 和 $q$ 控制探索的性质：\n1.  **返回参数 $p$**：该参数控制立即回溯的可能性。\n    -   一个较低的 $p$ 值（即 $p  1$）会增加返回到前一个节点 $t$ 的概率。这使得游走被紧密地限制在起始节点附近。\n    -   一个较高的 $p$ 值（即 $p > 1$）会降低回溯的概率，鼓励游走离开并探索更远的节点。\n\n2.  **进出参数 $q$**：该参数控制向“内”移動与向“外”移動的倾向。它在广度优先搜索（BFS）和深度优先搜索（DFS）之间进行插值。\n    -   一个较低的 $q$ 值（即 $q  1$）会增加访问满足 $d_{tx} = 2$ 的节点 $x$ 的概率。这鼓励游走远离前一个节点 $t$，促进类似 DFS 的、对图结构的全局探索。\n    -   一个较高的 $q$ 值（即 $q > 1$）会降低访问满足 $d_{tx} = 2$ 的节点 $x$ 的概率。这使得游走偏向于停留在前一个节点 $t$ 附近，促进类似 BFS 的、对邻域的局部探索。\n\n现在，我们将这些探索策略与指定的目标联系起来：\n\n**1. 社群检测（同质性导向的嵌入）：**\n目标是生成嵌入，使得属于同一社群的节点在嵌入空间中彼此靠近。同质性是指相似节点（例如，同一社群中的节点）相互连接的原理。为了捕捉这一点，随机游走必须密集地采样节点的局部邻域。在一个具有密集社群的图中，这意味着游走应该偏向于停留在社群*内部*。这需要一种类似于 BFS 的局部探索策略。\n- 为了实现类似 BFS 的行为，我们必须不鼓励游走移动到远离前一个节点 $t$ 的节点。这可以通过设置 $q > 1$ 来实现，该设置惩罚向 $d_{tx}=2$ 的节点 $x$ 的转移。\n- 为了进一步将游走限制在一个非常局部的区域，使其能够详尽地采样邻域，我们可以鼓励回溯或短程探索。一个较低的 $p$ 值（即 $p  1$）使得游走更有可能重新访问节点 $t$ 及其紧邻区域。\n- 因此，为了强调社群检测，合适的设置是 **$p  1$ 和 $q > 1$**。\n\n**2. 角色发现（结构等价性导向的嵌入）：**\n目标是生成嵌入，使得具有相同结构角色的节点（例如，作为两个社群之间的桥梁节点，或社群内的中心节点）在嵌入空间中彼此靠近，无论它们在网络中的距离如何。为了捕捉这一点，随机游走必须能够识别结构相似性，这通常需要对图有更宏观的视角。游走必须能够探索多样的邻域，并在结构相似的区域之間跳转。这需要一种类似于 DFS 的全局探索策略。\n- 为了实现类似 DFS 的行为，我们必须鼓励游走向外移动并探索图的新部分。这可以通过设置 $q  1$ 来实现，该设置偏好向 $d_{tx}=2$ 的节点 $x$ 的转移。\n- 为了补充这种向外搜索，我们必须通过不鼓励回溯来防止游走困在一个区域。这可以通过设置 $p > 1$ 来实现，该设置惩罚返回到前一个节点 $t$ 的行为，从而推动游走进一步探索。\n- 因此，为了强调角色发现，合适的设置是 **$p > 1$ 和 $q  1$**。\n\n基于这个推导出的理解，我们现在可以评估给定的选项。\n\n**A. 社群检测：$p1, q1$；角色发现：$p1, q1$。**\n- 社群检测的设置（$p1, q1$）正确地将用于局部限制的低返回参数与用于 BFS 式搜索的高进出参数配对。这与我们为捕捉同质性所做的推导相符。\n- 角色发现的设置（$p1, q1$）正确地将用于防止回溯的高返回参数与用于 DFS 式搜索的低进出参数配对。这与我们为捕捉结构等价性所做的推导相符。\n- **结论：正确。**\n\n**B. 社群检测：$p1, q1$；角色发现：$p1, q1$。**\n- 这个选项与上面推导出的正确逻辑完全相反。它建议对社群进行 DFS 式搜索，对角色进行 BFS 式搜索，这在根本上是错误的。\n- **结论：错误。**\n\n**C. 社群检测：$p\\approx 1, q\\approx 1$；角色发现：$p\\approx 1, q\\approx 1$。**\n- 当 $p \\approx 1$ 且 $q \\approx 1$ 时，偏差项 $\\alpha_{pq}(t, x) \\approx 1$ 在所有情况下都成立。游走变成了标准的无偏二阶随机游走，这是 DeepWalk 算法中使用的程序。虽然 DeepWalk 能有效捕捉同质性，但它不像 node2vec 那样，能以可调的方式特别*强调*同质性而非结构等价性。关键是，这种设置并不强调角色发现。因此，对于任一强调的目标来说，它都不是*最佳*选择。\n- **结论：错误。**\n\n**D. 社群检测：$p\\gg 1, q\\gg 1$；角色发现：$p\\ll 1, q\\ll 1$。**\n- 对于社群检测，$p \\gg 1$ 强烈不鼓励回溯，这对于密集的局部采样不是最优的。虽然 $q \\gg 1$ 促进了局部搜索，但与经典的 $p1, q1$ 设置相比，这种组合是次优的。\n- 对于角色发现，$p \\ll 1$ 强烈鼓励回溯，这与全局探索的目标直接冲突。$q \\ll 1$ 正确地鼓励了类似 DFS 的移动。参数的组合是矛盾的，对于既定目标是无效的。\n- **结论：错误。**\n\n**E. 社群检测：$p1, q1$；角色发现：$p1, q1$。**\n- 对于社群检测，$p > 1$ 不鼓励回溯，这是 DFS（探索）的特性，而不是 BFS（局部采样）的特性。虽然 $q1$ 是合适的，但选择 $p1$ 对于强调同质性来说并非最优。\n- 对于角色发现，$p  1$ 鼓励回溯和局部限制，这與所需的全局探索相反。虽然 $q1$ 是合适的，但选择 $p1$ 使得该策略弄巧成拙。\n- **结论：错误。**\n\n基于对 node2vec 算法的基本原理分析，只有选项 A 提供了正确的参数设置，以分别强调社群检测和角色发现这两个不同目标。", "answer": "$$\\boxed{A}$$", "id": "3331387"}]}