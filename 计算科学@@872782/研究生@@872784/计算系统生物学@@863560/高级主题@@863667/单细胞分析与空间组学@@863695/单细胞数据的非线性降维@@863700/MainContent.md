## 引言
在单细胞生物学的时代，我们能够以前所未有的分辨率测量单个细胞的分[子图](@entry_id:273342)谱，但这也带来了巨大的挑战：每个细胞都由数万个基因的表达量所描述，形成了一个难以直观理解的[高维数据](@entry_id:138874)空间。如何从这片“数据迷雾”中提炼出有意义的生物学洞见？[非线性降维](@entry_id:636435)（Nonlinear Dimensionality Reduction, NLDR）技术为此提供了关键的解决方案，它使我们能够揭示隐藏在数据背后的细胞状态、关系和动态过程的内在结构。

然而，众多NLDR算法（如[t-SNE](@entry_id:276549), UMAP）常被视为“黑箱”，其结果的解读与应用也充满了挑战。本文旨在填补这一知识鸿沟，系统性地揭示这些强大工具背后的数学原理，展示它们如何超越简单的可视化，成为解决复杂生物学问题的基石。

在接下来的内容中，读者将踏上一段从理论到实践的旅程。在“原理与机制”章节中，我们将深入探讨[流形假设](@entry_id:275135)，并剖析[t-SNE](@entry_id:276549)、UMAP、VAE等核心算法的机制。随后，在“应用与跨学科[交叉](@entry_id:147634)”章节中，我们将探索如何利用这些几何结构进行[轨迹推断](@entry_id:176370)、数据整合，并揭示其与数学和物理学的深刻联系。最后，“动手实践”部分将提供具体的编程练习，帮助读者将理论知识转化为可操作的技能。

让我们首先从支撑这一切的基础——[非线性降维](@entry_id:636435)的原理与机制开始。

## 原理与机制

在单细胞[基因表达分析](@entry_id:138388)中，每个细胞由数万个基因的表达水平来描述，构成了一个极高维的数据空间。然而，细胞的生物学状态——例如其在分化路径上的位置、所处的[细胞周期阶段](@entry_id:170415)或对刺激的反应——通常可以由远少于基因数量的几个潜在因素决定。这一核心洞见引出了**[流形假设](@entry_id:275135)（manifold hypothesis）**，即高维的单细胞数据点实际上集中在一个嵌入于高维环境空间中的低维、[非线性](@entry_id:637147)[流形](@entry_id:153038)附近。本章将深入探讨支撑[非线性降维](@entry_id:636435)（Nonlinear Dimensionality Reduction, NLDR）的数学原理和算法机制，阐释如何从高维数据中学习并可视化这一内在的低维[流形](@entry_id:153038)。

### [流形假设](@entry_id:275135)：细胞状态的几何视图

[流形假设](@entry_id:275135)为我们思考细胞状态提供了一个强大的几何框架。我们可以构建一个生成模型来形式化地描述这一假设。设 $z_i \in \mathbb{R}^d$ 是一个描述细胞 $i$ 生物学状态的低维潜在变量向量，其中 $d$ 远小于基因总数 $p$。这些潜在变量可能代表[转录因子](@entry_id:137860)活性、信号通路状态等。高维的基因表达谱 $x_i \in \mathbb{R}^p$ 则是通过一个从[潜在空间](@entry_id:171820)到基因表达空间的映射 $f$ 生成的，并伴随有[测量噪声](@entry_id:275238) $\epsilon_i$：

$$
x_i \approx f(z_i) + \epsilon_i
$$

[流形假设](@entry_id:275135)的核心在于，由于生物过程（如[细胞分化](@entry_id:273644)）的连续性，映射 $f: \mathcal{Z} \subset \mathbb{R}^d \to \mathbb{R}^p$ 通常是一个光滑的（可微的）函数。因此，所有细胞的理想基因表达谱构成了[流形](@entry_id:153038) $\mathcal{M} = f(\mathcal{Z})$，这是一个在 $\mathbb{R}^p$ 空间中维度为 $d$ 的光滑[曲面](@entry_id:267450)。我们观测到的数据点 $x_i$ 则因噪声而散布在该[流形](@entry_id:153038)的周围。

这一观点与线性[降维](@entry_id:142982)方法（如主成分分析，PCA）的假设形成了鲜明对比。PCA等方法假设数据位于或接近一个低维的**仿射[子空间](@entry_id:150286)**（即“平坦”的[流形](@entry_id:153038)），其[生成模型](@entry_id:177561)为 $x_i \approx W z_i + \mu + \epsilon_i$。那么，我们何时应该选择[非线性模型](@entry_id:276864)而不是线性模型呢？[@problem_id:3334328]

答案取决于映射 $f$ 的**曲率**与**噪声水平**的相对大小。任何光滑的[流形](@entry_id:153038)在足够小的局部区域内都可以被其切空间（一个[线性子空间](@entry_id:151815)）近似。只有当数据采样的尺度足够大，以至于[流形](@entry_id:153038)偏离其[局部线性近似](@entry_id:263289)的程度超过了噪声水平时，[非线性](@entry_id:637147)结构才变得显著且有必要进行建模。我们可以通过 $f$ 的泰勒展开来量化这一点。一个邻域内的[非线性](@entry_id:637147)偏差大小约等于 $\frac{1}{2}\|D^2 f(z)\| \cdot r^2$，其中 $D^2 f(z)$ 是 $f$ 的Hessian矩阵，代表其曲率，$r$是邻域的半径。只有当这个偏差不小于噪声的[标准差](@entry_id:153618) $\sigma$ 时，即 $\frac{1}{2}\|D^2 f(z)\| \cdot r^2 \gtrsim \sigma$，[非线性模型](@entry_id:276864)才是必需的。反之，如果数据仅覆盖了[流形](@entry_id:153038)上一个近乎平坦的区域，或者噪声过大掩盖了曲率，那么PCA等线性方法便是一个合理且高效的近似。

在处理[单细胞RNA测序](@entry_id:142269)（scRNA-seq）数据时，一个常见的[预处理](@entry_id:141204)步骤是应用[对数变换](@entry_id:267035)，如 $y_k = \ln(1+x_k)$。这个操作不仅仅是为了稳定[方差](@entry_id:200758)，它从根本上改变了数据的几何结构。我们可以通过度量张量的“[拉回](@entry_id:160816)”（pullback）来精确理解这一点。欧氏空间中的标准度量由[单位矩阵](@entry_id:156724) $\boldsymbol{I}$ 表示。经过 $T(\boldsymbol{x}) = \boldsymbol{y}$ 的变换后，原始 $x$ 空间中的度量张量 $g(\boldsymbol{x})$ 变为一个依赖于位置的对角矩阵 [@problem_id:3334346]：

$$
g(\boldsymbol{x}) = \mathrm{diag}\left(\frac{1}{(1+x_1)^2}, \frac{1}{(1+x_2)^2}, \dots, \frac{1}{(1+x_d)^2}\right)
$$

这意味着，在原始计数空间中，一个单位长度的移动，在表达量高的区域（$x_k$ 大）所对应的变换后空间中的移动距离，要远小于在表达量低的区域（$x_k$ 小）的移动。因此，[对数变换](@entry_id:267035)实际上是对空间进行了非均匀的缩放，压缩了高表达基因的坐标轴，从而重塑了我们测量距离和几何的方式。变换后的两点 $\boldsymbol{x}$ 和 $\boldsymbol{x}'$ 之间的欧氏距离变为：

$$
D(T(\boldsymbol{x}), T(\boldsymbol{x}')) = \sqrt{\sum_{k=1}^{d} \left( \ln\left(\frac{1+x_k}{1+x'_k}\right) \right)^2}
$$

这个[距离度量](@entry_id:636073)对基因表达的相对变化（比例）比绝对变化更敏感，这通常更符合生物学直觉。

### 保留局部邻域：[t-SNE](@entry_id:276549)与UMAP

在众多NLDR算法中，[t-SNE](@entry_id:276549)和UMAP因其在可视化单细胞数据方面的卓越表现而备受青睐。它们的核心思想都是通过优化高维空间和低维[嵌入空间](@entry_id:637157)中数据点之间相似性[分布](@entry_id:182848)的匹配程度来学习嵌入。

#### [t-分布随机邻域嵌入](@entry_id:276549) ([t-SNE](@entry_id:276549))

[t-SNE](@entry_id:276549)的机制可以从第一性原理构建 [@problem_id:3334366]。它分三步进行：

1.  **构建高维相似性**：首先，[t-SNE](@entry_id:276549)将高维空间中数据点之间的欧氏距离转化为条件概率 $p_{j|i}$，表示点 $x_i$ 会选择点 $x_j$ 作为其邻居的概率。这个概率是在以 $x_i$ 为中心的高斯分布下计算的：
    $$
    p_{j \mid i} = \frac{\exp(-\lVert x_i - x_j \rVert^2 / (2 \sigma_i^2))}{\sum_{k \neq i} \exp(-\lVert x_i - x_k \rVert^2 / (2 \sigma_i^2))}
    $$
    一个关键创新是，每个点 $x_i$ 的高斯核带宽 $\sigma_i$ 是独立确定的。这是通过一个用户指定的参数——**[困惑度](@entry_id:270049)（perplexity）** $\tau$ 来实现的。[困惑度](@entry_id:270049)可以被看作是每个点“软性”的邻居数量。算法会通过[二分查找](@entry_id:266342)来寻找最优的 $\sigma_i$，使得[条件概率分布](@entry_id:163069) $P_i = \{p_{j|i}\}_{j \neq i}$ 的香农熵 $H(P_i) = - \sum_{j} p_{j|i} \log_2 p_{j|i}$ 恰好等于 $\log_2 \tau$。这种自适应的带宽设置使得[t-SNE](@entry_id:276549)能够处理密度不均的数据集：在稠密区域，$\sigma_i$ 会较小；在稀疏区域，$\sigma_i$ 会较大。

2.  **对称化与低维相似性**：为了得到一个统一的[联合概率分布](@entry_id:171550) $P$，[t-SNE](@entry_id:276549)将[条件概率](@entry_id:151013)对称化：$P_{ij} = (p_{j|i} + p_{i|j}) / (2N)$，其中 $N$ 是细胞总数。在低维[嵌入空间](@entry_id:637157)中，点对 $(y_i, y_j)$ 之间的相似性 $Q_{ij}$ 使用了一个具有重尾的**[学生t-分布](@entry_id:142096)**（自由度为1）来建模：
    $$
    Q_{ij} = \frac{(1 + \lVert y_i - y_j \rVert^2)^{-1}}{\sum_{k \neq l} (1 + \lVert y_k - y_l \rVert^2)^{-1}}
    $$
    使用[重尾分布](@entry_id:142737)是[t-SNE](@entry_id:276549)成功的关键。它解决了所谓的“拥挤问题”：高维空间中中等距离的点在低维空间中往往没有足够的位置来安放。[重尾分布](@entry_id:142737)允许这些点在[嵌入空间](@entry_id:637157)中相距较远，从而更好地分离不同的簇。

3.  **最小化KL散度**：[t-SNE](@entry_id:276549)的目标是找到一个低维嵌入 $\{y_i\}$，使得低维相似性[分布](@entry_id:182848) $Q$ 与高维相似性[分布](@entry_id:182848) $P$ 尽可能地接近。这是通过最小化它们之间的**Kullback-Leibler (KL) 散度**来实现的：
    $$
    \mathcal{L} = KL(P || Q) = \sum_{i \neq j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}
    $$
    通过梯度下降法优化这个目标函数，算法会同时吸引高维空间中相似的点对（$P_{ij}$ 大），并排斥不相似的点对。

#### 均匀流形近似与投影 (UMAP)

UMAP是另一种流行的NLDR方法，它基于[流形学习](@entry_id:156668)和[拓扑数据分析](@entry_id:154661)的数学理论。尽管其理论基础不同，但在实践中，UMAP的优化过程与[t-SNE](@entry_id:276549)有共通之处，它也最小化一个高维和低维相似性[分布](@entry_id:182848)之间的**[交叉熵](@entry_id:269529)**[目标函数](@entry_id:267263)。

我们可以通过一个简化的模型来理解UMAP的两个关键参数——`n_neighbors` 和 `min_dist`——如何调控嵌入结果 [@problem_id:3334343]。`n_neighbors` (记为 $k$) 类似于[t-SNE](@entry_id:276549)的[困惑度](@entry_id:270049)，它定义了用于构建高维相似性图的局部邻域的大小，从而决定了算法关注局部结构还是全局结构的平衡。`min_dist` (记为 $\delta$) 则控制了[嵌入空间](@entry_id:637157)中点的“紧凑”程度。它通过一个依赖于 $\delta$ 的函数族 $q(d) = (1 + a(\delta) d^{2b(\delta)})^{-1}$ 来塑造低维相似性曲线的形状。

UMAP的[目标函数](@entry_id:267263)可以写成吸引项和排斥项的和。对于高维空间中相似性为 $p_{ij}$ 的一对点，其在[嵌入空间](@entry_id:637157)中的距离为 $d_{ij}$，该点对对总损失的贡献为：
$$
\mathcal{C}_{ij} = p_{ij} \log(q(d_{ij})) + (1 - p_{ij}) \log(1 - q(d_{ij}))
$$
这个[交叉熵损失](@entry_id:141524)函数揭示了UMAP的核心机制 [@problem_id:3334383]。最小化该损失的理想嵌入距离 $d^*$，恰好是使低维相似性 $q(d^*)$ 等于高维相似性 $p_{ij}$ 的那个距离。
$$
q(d^*) = p_{ij} \quad \implies \quad d^* = \left( \frac{1 - p_{ij}}{a p_{ij}} \right)^{\frac{1}{2b}}
$$
这意味着UMAP的优化过程就是在“尝试”为每一对点找到一个嵌入距离，使得其低维相似性能够复现高维相似性。

通过分析[损失函数](@entry_id:634569)对距离变化的响应，我们可以更深入地理解参数的作用。假设我们将簇[内点](@entry_id:270386)的距离 $d_{\mathrm{in}}$ 缩小 $\epsilon$ 倍，并将簇间点的距离 $d_{\mathrm{out}}$ 扩大 $\eta$ 倍。[损失函数](@entry_id:634569)的一阶变化 $\Delta L$ 表明，当低维相似性 $q(d_{\mathrm{in}})$ 小于高维相似性 $p_{\mathrm{in}}$ 时，减小簇内距离会降低总损失，从而促使簇变得更紧凑。同理，当 $q(d_{\mathrm{out}})$ 大于 $p_{\mathrm{out}}$ 时，增加簇间距离也会降低损失，从而拉大簇间的分离度 [@problem_id:3334343]。这精确地解释了UMAP为何能产生具有清晰簇结构的可视化效果。

### 全局结构与谱方法：[扩散图](@entry_id:748414)

虽然[t-SNE](@entry_id:276549)和UMAP在保留局部邻域结构方面表现出色，但它们有时会扭曲数据的全局结构。例如，两个相距很远的簇在嵌入[图中的距离](@entry_id:276146)并不直接反映它们在原始数据空间中的真实分离程度。为了更好地捕捉[全局几何](@entry_id:197506)，我们可以采用**[谱方法](@entry_id:141737)**，其中**[扩散图](@entry_id:748414) (Diffusion Maps)** 是一个典型代表。

[扩散图](@entry_id:748414)将数据点视为一个图的节点，并通过定义节点间的“[扩散](@entry_id:141445)”过程来揭示数据的几何结构。其构建过程包括：
1.  构建一个亲和矩阵 $K$，通常使用高斯核 $K_{ij} = \exp(-\|x_i - x_j\|^2 / \varepsilon)$。
2.  对该矩阵进行归一化，得到一个马尔可夫转移矩阵 $P$。$P_{ij}$ 可以被解释为从点 $i$ 一步[随机游走](@entry_id:142620)到点 $j$ 的概率。
3.  对转移矩阵 $P$ 进行谱分解（[特征分解](@entry_id:181333)）。$P$ 的[特征向量](@entry_id:151813)和[特征值](@entry_id:154894)蕴含了数据的内在几何信息。

[扩散图](@entry_id:748414)的一个重要应用是估计数据的**内在维度（intrinsic dimension）**。这可以通过分析[扩散](@entry_id:141445)操作[算子谱](@entry_id:276315)的衰减模式来实现 [@problem_id:3334322]。理论上，离散的图拉普拉斯算子（与 $P$ 相关）的谱会收敛到数据所在连续[流形](@entry_id:153038)的[拉普拉斯-贝尔特拉米算子](@entry_id:267002)的谱。一个 $d$ 维[流形](@entry_id:153038)的前 $d$ 个非平凡[特征值](@entry_id:154894)通常较小且密集，而第 $d+1$ 个[特征值](@entry_id:154894)会有一个显著的跳跃，形成所谓的“**谱隙（spectral gap）**”。

一个稳健地估计内在维度 $d$ 的方法是：
1.  对[特征值](@entry_id:154894) $\lambda_i$ (其中 $\lambda_1=1 \ge \lambda_2 \ge \dots$) 进行[对数变换](@entry_id:267035)，得到 $\tilde{\mu}_j = -\log(\lambda_{j+1})$。这个变换将[特征值](@entry_id:154894)与连续的拉普拉斯算子[特征值](@entry_id:154894)建立了线性关系。
2.  计算归一化的谱隙 $g_j = (\tilde{\mu}_{j+1} - \tilde{\mu}_j) / \tilde{\mu}_j$。这种归一化处理比原始的差值 $\lambda_i - \lambda_{i+1}$ 更能抵抗尺度的影响。
3.  寻找最大的归一化[谱隙](@entry_id:144877) $g_d$ 所在的位置 $d$。为了确保这个谱隙反映的是真实的几何结构而非噪声，该估计应该在不同的扩散时间 $t$（即考察 $P^t$ 的谱）下保持稳定。

### [核方法](@entry_id:276706)与生成模型

除了上述方法，还有两类强大的NLDR技术在[单细胞分析](@entry_id:274805)中也扮演着重要角色：基于核的方法和[深度生成模型](@entry_id:748264)。

#### [核主成分分析](@entry_id:634172) (Kernel PCA)

[核PCA](@entry_id:635832)是PCA的一种[非线性](@entry_id:637147)扩展，其核心是“**[核技巧](@entry_id:144768)（kernel trick）**” [@problem_id:3334377]。其思想是，首先通过一个[非线性映射](@entry_id:272931) $\varphi$ 将数据从原始空间 $\mathbb{R}^p$ 映射到一个更高维（甚至无限维）的[特征空间](@entry_id:638014) $\mathcal{H}$，然后在该[特征空间](@entry_id:638014)中执行标准的PCA。如果 $\varphi$ 选择得当，原始空间中的[非线性](@entry_id:637147)结构在[特征空间](@entry_id:638014)中可能会变成线性结构。

神奇之处在于，我们无需显式地定义映射 $\varphi$ 或在 $\mathcal{H}$ 中进行计算。PCA的计算只依赖于数据点的[内积](@entry_id:158127)。[核技巧](@entry_id:144768)用一个**核函数** $k(\boldsymbol{x}_i, \boldsymbol{x}_j)$ 来替代[特征空间](@entry_id:638014)中的[内积](@entry_id:158127) $\langle \varphi(\boldsymbol{x}_i), \varphi(\boldsymbol{x}_j) \rangle_{\mathcal{H}}$。只要[核函数](@entry_id:145324)满足一定的数学性质（[正定性](@entry_id:149643)），它就隐式地定义了一个特征空间和映射。

在[特征空间](@entry_id:638014)中进行PCA等价于对中心化的核矩阵 $\tilde{\boldsymbol{K}}$ 进行[特征分解](@entry_id:181333)。设原始核矩阵为 $\boldsymbol{K}$，其元素为 $K_{ij} = k(\boldsymbol{x}_i, \boldsymbol{x}_j)$。在特征空间中对数据进行中心化，可以通过对核矩阵进行如下操作来实现：
$$
\tilde{\boldsymbol{K}} = \boldsymbol{H}\boldsymbol{K}\boldsymbol{H}
$$
其中 $\boldsymbol{H} = \boldsymbol{I} - \frac{1}{n}\mathbf{1}\mathbf{1}^{\top}$ 是中心化矩阵。求解 $\tilde{\boldsymbol{K}}$ 的[特征向量](@entry_id:151813)，就得到了数据在[非线性](@entry_id:637147)主成分方向上的投影。

#### [变分自编码器 (VAE)](@entry_id:141132)

[变分自编码器](@entry_id:177996)是一种[深度生成模型](@entry_id:748264)，它为NLDR提供了一个强大的概率框架 [@problem_id:3334361]。VAE由两个[神经网](@entry_id:276355)络组成：一个**编码器** $q_{\phi}(z|x)$ 和一个**解码器** $p_{\theta}(x|z)$。
-   编码器将高维输入数据 $x$ 映射为低维[潜在空间](@entry_id:171820)中的一个[概率分布](@entry_id:146404)（通常是[高斯分布](@entry_id:154414)），而不是一个确定的点。我们从这个[分布](@entry_id:182848)中采样得到潜在表示 $z$。
-   解码器则尝试从潜在表示 $z$ 重构出原始的高维数据 $x$。

VAE的训练目标是最大化**[证据下界](@entry_id:634110) (Evidence Lower Bound, ELBO)**。ELBO由两部分组成：
1.  **[重构损失](@entry_id:636740)**：$\mathbb{E}_{q_{\phi}(z|x)}[\ln p_{\theta}(x|z)]$。这衡量了解码器从潜在表示重构原始数据的能力。对于scRNA-seq的计数数据，使用**[负二项分布](@entry_id:262151)**作为解码器的输出模型比高斯分布更合适，因为它能自然地处理离散、过离散的计数数据。此时，[重构损失](@entry_id:636740)项就包含了[负二项分布](@entry_id:262151)的[对数似然函数](@entry_id:168593)。
2.  **[KL散度](@entry_id:140001)正则化项**：$D_{KL}(q_{\phi}(z|x) || p(z))$。这一项是编码器产生的[后验分布](@entry_id:145605) $q_{\phi}(z|x)$ 与一个预设的[先验分布](@entry_id:141376) $p(z)$（通常是[标准正态分布](@entry_id:184509)）之间的KL散度。这个正则化项迫使编码器学习一个结构良好、光滑的[潜在空间](@entry_id:171820)，防止模型“作弊”式地将每个点编码到[潜在空间](@entry_id:171820)的不同区域。

训练完成后，编码器可以将新的[高维数据](@entry_id:138874)点映射到这个有意义的低维[潜在空间](@entry_id:171820)中，用于可视化、[聚类](@entry_id:266727)或[轨迹推断](@entry_id:176370)。

### 实践考量与评估

在实际应用中，选择和使用NLDR方法需要考虑其计算可扩展性和结果的可靠性。

#### 可扩展性与[近似算法](@entry_id:139835)

随着单细胞数据集的规模增长到数十万甚至数百万个细胞，许多NLDR算法的计算成本成为主要瓶颈。例如，精确[t-SNE](@entry_id:276549)的计算复杂度为 $\mathcal{O}(N^2)$，因为其排斥力的计算需要考虑所有 $N(N-1)/2$ 对点 [@problem_id:3334324]。对于大规模数据集，这是不可接受的。

为了解决这个问题，研究者开发了多种近似算法。
-   **Barnes-Hut [t-SNE](@entry_id:276549)** 采用天体物理学中的[N体模拟](@entry_id:157492)思想，通过构建[四叉树](@entry_id:753916)（或[八叉树](@entry_id:144811)）来近似计算排斥力。它将远处的一群点作为一个整体（[质心](@entry_id:265015)）来计算其[合力](@entry_id:163825)，从而避免了逐点计算。这使得每次迭代的计算复杂度从 $\mathcal{O}(N^2)$ 降低到 $\mathcal{O}(N \log N)$。
-   **基于FFT的[t-SNE](@entry_id:276549) (FI[t-SNE](@entry_id:276549))** 则采用另一种策略，它将点的“[电荷](@entry_id:275494)”散布到一个网格上，使用[快速傅里叶变换](@entry_id:143432)（FFT）在网格上高效地计算由所有点产生的“[势能](@entry_id:748988)场”，然后再将[势能](@entry_id:748988)插值回每个点的位置以计算梯度。该方法的复杂度也近似为 $\mathcal{O}(N \log N)$，并且在现代硬件上通常比Barnes-Hut方法更快。

理解这些算法的复杂度差异对于为特定规模的数据集选择合适的工具至关重要。

#### 评估嵌入质量

NLDR的可视化结果直观且富有启发性，但也可能产生误导。簇的大小、形状以及簇间距离不一定具有直接的生物学意义。因此，对嵌入结果进行定量评估是必要的。

对于存在已知连续过程（如细胞分化或周期）的数据，我们可以使用**[伪时间](@entry_id:262363)（pseudotime）**作为参考来评估嵌入是否保留了全局结构 [@problem_id:3334364]。[伪时间](@entry_id:262363) $t_i$ 为每个细胞在轨迹上的位置进行排序。我们可以定义参考距离为[伪时间](@entry_id:262363)的差值 $d_{\mathrm{ref}}(i,j) = |t_i - t_j|$。
然后，我们可以将[嵌入空间](@entry_id:637157)中的欧氏距离 $d_{\mathrm{emb}}(i,j)$ 与参考距离进行比较，定义**畸变率** $r_{ij} = d_{\mathrm{emb}}(i,j) / d_{\mathrm{ref}}(i,j)$。
-   **平均拉伸（Mean Stretch）** 和 **平均压缩（Mean Compression）** 分别计算所有 $r_{ij}>1$ 和 $r_{ij}<1$ 的均值，量化了距离被系统性地放大或缩小的程度。
-   **拉伸/压缩分数（Fraction Stretched/Compressed）** 则给出了被拉伸或压缩的点对的比例。
-   **距离的[斯皮尔曼等级相关](@entry_id:755150)系数（Spearman Correlation）** 衡量了 $d_{\mathrm{emb}}$ 和 $d_{\mathrm{ref}}$ 之间单调关系的一致性。一个接近1的[相关系数](@entry_id:147037)表明，嵌入很好地保留了点对距离的相对顺序，即全局拓扑结构。

这些定量指标为我们提供了一套超越主观视觉评估的客观标准，用以比较不同NLDR算法或同一算法不同参数设置下的性能。