{"hands_on_practices": [{"introduction": "在任何单细胞分析流程中，数据预处理都是至关重要的一步，它深刻影响着下游分析的结果。这项练习旨在探索一种常见的预处理技术——z-score标准化——如何改变细胞间的几何关系。我们将通过计算高斯亲和矩阵的特征值来量化这种变化，因为特征谱能够捕捉数据固有的流形结构[@problem_id:3334321]。通过这个实践，你将能更深刻地理解数据缩放对结果的影响，并为后续的降维分析做出更明智的决策。", "problem": "考虑一个单细胞转录组数据集，它表示为一个包含 $n$ 个细胞和 $p$ 个基因的矩阵，其中每个细胞 $i$ 是一个点 $x_i \\in \\mathbb{R}^p$。对于任意两个细胞 $i$ 和 $j$，将高斯亲和矩阵的元素定义为 $W_{ij} = \\exp\\left(-\\|x_i - x_j\\|_2^2 / (2 \\sigma^2)\\right)$，其中 $\\| \\cdot \\|_2$ 表示欧几里得范数，$\\sigma > 0$ 是核带宽参数。对每个基因进行 Z-score 标准化意味着减去该基因在所有细胞中的均值，然后除以其标准差，从而得到标准化后的点 $z_i \\in \\mathbb{R}^p$；如果某个基因在所有细胞中的方差为零，则将其在所有细胞中的标准化值定义为 $0$。使用原始数据 $\\{x_i\\}$ 构建高斯亲和矩阵 $W^{(0)}$，并使用标准化数据 $\\{z_i\\}$ 构建矩阵 $W^{(z)}$。亲和矩阵的谱指的是其特征值的多重集。对于像 $W^{(0)}$ 或 $W^{(z)}$ 这样的对称亲和矩阵，所有特征值都是实数。对于给定的数据集和带宽，将谱变化幅度定义为\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2,\n$$\n其中 $\\lambda(\\cdot)$ 表示按降序排列的特征值向量，$\\|\\cdot\\|_2$ 是欧几里得范数。\n\n仅根据 Z-score 标准化、欧几里得距离、高斯核的定义，以及关于实对称矩阵及其特征值的基本事实，设计并实现一个程序，为下面指定的每个测试用例计算 $S$。你的程序必须使用所提供的确切数据集和带宽，对零方差基因应用零方差 Z-score 标准化规则，计算亲和矩阵，获取其特征值，按降序对其进行排序，并按规定计算 $S$。每个报告的 $S$ 值必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 测试用例 $1$（正常路径，中等带宽）：使用数据集 $X^{(A)} \\in \\mathbb{R}^{5 \\times 3}$，其行向量由 $x_1 = (2.0, 5.0, 8.0)$，$x_2 = (3.0, 6.0, 7.5)$，$x_3 = (4.0, 6.5, 7.0)$，$x_4 = (5.0, 7.0, 6.0)$，$x_5 = (7.0, 7.5, 5.0)$ 给出，带宽为 $\\sigma = 1.0$。\n- 测试用例 $2$（小带宽边界）：使用与上述相同的数据集 $X^{(A)}$，带宽为 $\\sigma = 0.1$。\n- 测试用例 $3$（大带宽边界）：使用与上述相同的数据集 $X^{(A)}$，带宽为 $\\sigma = 100.0$。\n- 测试用例 $4$（零方差基因边缘情况）：使用数据集 $X^{(B)} \\in \\mathbb{R}^{5 \\times 3}$，其行向量由 $x_1 = (0.0, 0.0, 10.0)$，$x_2 = (10.0, 0.0, 10.0)$，$x_3 = (20.0, 0.0, 10.0)$，$x_4 = (30.0, 0.0, 10.0)$，$x_5 = (40.0, 0.0, 10.0)$ 给出，带宽为 $\\sigma = 5.0$。\n\n你的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序为 $[S_1,S_2,S_3,S_4]$，其中 $S_k$ 是测试用例 $k$ 四舍五入后的谱变化幅度。输出是无量纲的实数，并且必须是四舍五入到 $6$ 位小数的浮点数。", "solution": "任务是量化 Z-score 标准化对源于单细胞数据的高斯亲和矩阵特征谱的影响。我们得到了关于数据转换、亲和矩阵构建和谱变化度量指标的精确数学定义。该问题是一个定义明确的计算练习，其基础是线性代数、统计学和计算生物学的原理。\n\n整个过程包括几个不同的、顺序的步骤：\n1. 对于给定的数据集 $X$ 和带宽 $\\sigma$，构建高斯亲和矩阵 $W^{(0)}$。\n2. 计算并排序 $W^{(0)}$ 的特征值。\n3. 对数据集 $X$ 应用 Z-score 标准化以获得新数据集 $Z$，并按规定处理零方差基因。\n4. 使用相同的带宽 $\\sigma$ 从标准化数据 $Z$ 构建高斯亲和矩阵 $W^{(z)}$。\n5. 计算并排序 $W^{(z)}$ 的特征值。\n6. 计算两个已排序特征值向量之间的欧几里得距离，以求得谱变化幅度 $S$。\n\n现在我们将详细阐述每个步骤的数学和概念基础。\n\n单细胞数据集表示为一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其中 $n$ 是细胞数量， $p$ 是基因数量。每个细胞 $i$ 对应一个行向量 $x_i \\in \\mathbb{R}^p$。\n\n**1. Z-score 标准化**\n\nZ-score 标准化是一种常见的预处理技术，用于通过将特征（基因）转换到共同的尺度上使其具有可比性。它将每个基因的表达值重新缩放，使其均值为 $0$，标准差为 $1$。\n\n对于每个基因 $j \\in \\{1, \\dots, p\\}$，我们首先计算它在所有 $n$ 个细胞中的均值 $\\mu_j$ 和总体标准差 $s_j$：\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\n$$\n$$\ns_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\n$$\n原始数据点 $x_i = (x_{i1}, \\dots, x_{ip})$ 被转换为标准化点 $z_i = (z_{i1}, \\dots, z_{ip})$。如果一个基因 $j$ 具有非零方差（即 $s_j > 0$），其对于细胞 $i$ 的标准化值为：\n$$\nz_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}\n$$\n问题指定了一个关键的边缘情况：如果一个基因 $j$ 的方差为零（$s_j = 0$），则其在所有细胞中的标准化值被定义为 $0$，即对于所有 $i \\in \\{1, \\dots, n\\}$ 都有 $z_{ij} = 0$。这个规则避免了除以零，并且有效地从标准化空间中的距离计算中移除了恒定表达基因的贡献。\n\n**2. 高斯亲和矩阵构建**\n\n细胞之间的关系通过一个亲和矩阵来捕捉，该矩阵衡量成对的相似性。高斯核是用于此目的的标准选择。具有数据向量 $u_i$ 和 $u_j$ 的两个细胞 $i$ 和 $j$ 之间的亲和力（其中 $u$ 可以是原始数据 $x$ 或标准化数据 $z$）被定义为：\n$$\nW_{ij} = \\exp\\left(-\\frac{\\|u_i - u_j\\|_2^2}{2 \\sigma^2}\\right)\n$$\n此处，$\\|u_i - u_j\\|_2^2$ 是两个细胞向量之间的欧几里得距离的平方，$\\sigma > 0$ 是核带宽参数，它控制高斯核的宽度，从而控制邻域的尺度。较小的 $\\sigma$ 会导致更局部、更稀疏的相似性概念，而较大的 $\\sigma$ 则考虑更广泛的关系。\n\n我们构建两个这样的矩阵：\n- $W^{(0)}$，使用原始数据点 $\\{x_i\\}$。\n- $W^{(z)}$，使用 Z-score 标准化后的数据点 $\\{z_i\\}$。\n\n根据构造，这些矩阵是实对称矩阵（$W_{ij} = W_{ji}$），因为欧几里得距离是对称的。对角线元素总是 $W_{ii} = \\exp(0) = 1$。\n\n**3. 谱分析**\n\n矩阵的谱是其特征值的多重集。线性代数的一个基本定理指出，任何实对称矩阵，例如我们的亲和矩阵 $W^{(0)}$ 和 $W^{(z)}$，都是可对角化的，并且只有实数特征值。这些特征值为了解由亲和矩阵所代表的数据图的结构提供了深刻的洞见。\n\n我们计算 $W^{(0)}$ 和 $W^{(z)}$ 的特征值。设这些特征值的多重集为 $\\{\\lambda_k^{(0)}\\}_{k=1}^n$ 和 $\\{\\lambda_k^{(z)}\\}_{k=1}^n$。为了进行有意义的比较，我们将它们按降序排序以形成特征值向量：\n$$\n\\lambda\\left(W^{(0)}\\right) = \\left(\\lambda_1^{(0)}, \\lambda_2^{(0)}, \\dots, \\lambda_n^{(0)}\\right) \\text{ where } \\lambda_1^{(0)} \\ge \\lambda_2^{(0)} \\ge \\dots \\ge \\lambda_n^{(0)}\n$$\n$$\n\\lambda\\left(W^{(z)}\\right) = \\left(\\lambda_1^{(z)}, \\lambda_2^{(z)}, \\dots, \\lambda_n^{(z)}\\right) \\text{ where } \\lambda_1^{(z)} \\ge \\lambda_2^{(z)} \\ge \\dots \\ge \\lambda_n^{(z)}\n$$\n\n**4. 谱变化幅度计算**\n\n最后一步是量化由 Z-score 转换引起的谱的总变化。问题将谱变化幅度 $S$ 定义为两个已排序特征值向量之差的欧几里得（$L_2$）范数：\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2 = \\sqrt{\\sum_{k=1}^n \\left(\\lambda_k^{(z)} - \\lambda_k^{(0)}\\right)^2}\n$$\n这个度量提供了一个单一的标量值，总结了 Z-score 标准化在多大程度上改变了由高斯亲和矩阵的谱所捕捉的数据的几何结构。\n\n实现将为每个测试用例精确地遵循这些步骤，并使用稳健的数值库来执行矩阵和向量运算。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_spectral_change(X: np.ndarray, sigma: float) -> float:\n    \"\"\"\n    Computes the spectral change magnitude S for a given dataset and bandwidth.\n\n    The function performs the following steps:\n    1. Computes the Gaussian affinity matrix W^(0) from the original data X.\n    2. Computes the sorted eigenvalues of W^(0).\n    3. Standardizes (z-scores) the data X to get Z, handling zero-variance genes.\n    4. Computes the Gaussian affinity matrix W^(z) from the standardized data Z.\n    5. Computes the sorted eigenvalues of W^(z).\n    6. Calculates the Euclidean norm of the difference between the two eigenvalue vectors.\n\n    Args:\n        X: A numpy array of shape (n, p) representing n cells and p genes.\n        sigma: The Gaussian kernel bandwidth parameter.\n\n    Returns:\n        The spectral change magnitude S.\n    \"\"\"\n    # 1. Compute affinity matrix W^(0) from original data X\n    # The 'sqeuclidean' metric computes the squared Euclidean distance.\n    dist_sq_0 = squareform(pdist(X, 'sqeuclidean'))\n    W0 = np.exp(-dist_sq_0 / (2 * sigma**2))\n\n    # 2. Compute sorted eigenvalues of W^(0)\n    # np.linalg.eigh is specialized for Hermitian (symmetric for real matrices)\n    # and returns eigenvalues in ascending order. We reverse them.\n    eigvals_0 = np.linalg.eigh(W0)[0][::-1]\n\n    # 3. Z-score the data X to get Z\n    # We use population standard deviation (ddof=0 is the default in np.std)\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)\n\n    # Initialize standardized matrix with zeros. This correctly handles the\n    # zero-variance case where standardized values must be 0.\n    Z = np.zeros_like(X, dtype=float)\n    \n    # Create a boolean mask for columns with non-zero standard deviation\n    non_zero_std_mask = stds > 1e-15 # Use a small tolerance for floating point safety\n    \n    # Apply standardization only to columns with non-zero standard deviation\n    if np.any(non_zero_std_mask):\n        Z[:, non_zero_std_mask] = (X[:, non_zero_std_mask] - means[non_zero_std_mask]) / stds[non_zero_std_mask]\n\n    # 4. Compute affinity matrix W^(z) from standardized data Z\n    dist_sq_z = squareform(pdist(Z, 'sqeuclidean'))\n    Wz = np.exp(-dist_sq_z / (2 * sigma**2))\n    \n    # 5. Compute sorted eigenvalues of W^(z)\n    eigvals_z = np.linalg.eigh(Wz)[0][::-1]\n\n    # 6. Calculate spectral change magnitude S\n    S = np.linalg.norm(eigvals_z - eigvals_0)\n\n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # Test case 1: Happy path, moderate bandwidth\n    X_A = np.array([\n        [2.0, 5.0, 8.0],\n        [3.0, 6.0, 7.5],\n        [4.0, 6.5, 7.0],\n        [5.0, 7.0, 6.0],\n        [7.0, 7.5, 5.0]\n    ])\n    sigma_1 = 1.0\n\n    # Test case 2: Small bandwidth boundary\n    sigma_2 = 0.1\n\n    # Test case 3: Large bandwidth boundary\n    sigma_3 = 100.0\n\n    # Test case 4: Zero-variance gene edge case\n    X_B = np.array([\n        [0.0, 0.0, 10.0],\n        [10.0, 0.0, 10.0],\n        [20.0, 0.0, 10.0],\n        [30.0, 0.0, 10.0],\n        [40.0, 0.0, 10.0]\n    ])\n    sigma_4 = 5.0\n\n    test_cases = [\n        (X_A, sigma_1),\n        (X_A, sigma_2),\n        (X_A, sigma_3),\n        (X_B, sigma_4)\n    ]\n\n    results = []\n    for X, sigma in test_cases:\n        S = calculate_spectral_change(X, sigma)\n        # Format the result to 6 decimal places as a string\n        results.append(f\"{S:.6f}\")\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3334321"}, {"introduction": "随着单细胞数据集的规模增长到数百万个细胞，计算效率已成为一个无法回避的核心问题。这项练习将通过一个具体的计算，揭示稀疏数据结构在表示k近邻（kNN）图时的关键作用，而kNN图正是UMAP和t-SNE等算法的基石。你将亲手计算并比较使用压缩稀疏行（CSR）格式与传统密集矩阵存储亲和力矩阵$P$和权重矩阵$W$时的内存消耗差异[@problem_id:3334326]。这个练习将让你对稀疏表示为何是可扩展单细胞分析的“必需品”而非“优化项”有一个直观且定量的认识。", "problem": "在应用于单细胞转录组数据的非线性降维中，诸如t分布随机邻域嵌入(t-SNE)和均匀流形逼近与投影(UMAP)等算法会构建一个$k$-近邻图来捕捉细胞间的局部关系。假设数据集包含$N$个细胞。定义一个权重矩阵$W \\in \\mathbb{R}^{N \\times N}$，其中$W_{ij}$编码了细胞$i$和细胞$j$之间的相似性；以及一个高维亲和矩阵$P \\in \\mathbb{R}^{N \\times N}$，其中$P_{ij}$表示细胞$i$和细胞$j$之间的概率性亲和度。在实践中，为了反映$k$-近邻图的局部性约束，$W$和$P$的每一行仅包含$k$个非零项，这些非零项对应于该行细胞的最近邻，所有其他项均为零，从而得到稀疏矩阵。\n\n您将比较在嵌入过程中同时存储$W$和$P$的两种存储策略：\n1. 密集存储：将$W$和$P$都存储为完整的$N \\times N$数组，元素为64位浮点值。\n2. 稀疏存储：使用压缩稀疏行(CSR)格式存储$W$和$P$，每个非零项存储为64位浮点值，其列索引存储为32位整数，行指针数组(“indptr”)存储为长度为$N+1$的32位整数数组。假设每行恰好有$k$个非零项。\n\n从稀疏表示和内存计算的基本原理出发，说明稀疏表示如何相对于密集存储减少内存占用。然后，对于$N = 100{,}000$个细胞和$k = 15$的情况，计算使用稀疏CSR表示法存储$W$和$P$相对于密集存储这两个矩阵所节省的总内存。以千兆字节(GB)为单位表示最终节省的内存，并使用$1 \\,\\mathrm{GB} = 10^{9}$字节的约定。将您的答案四舍五入到四位有效数字。", "solution": "根据既定标准对问题进行验证。\n\n### 步骤1：提取已知条件\n- 数据集大小：$N$ 个细胞。\n- 矩阵：权重矩阵 $W \\in \\mathbb{R}^{N \\times N}$ 和亲和矩阵 $P \\in \\mathbb{R}^{N \\times N}$。\n- 稀疏性：$W$ 和 $P$ 的每行恰好有 $k$ 个非零项。\n- 存储策略1 (密集存储)：$W$ 和 $P$ 都存储为完整的 $N \\times N$ 64位浮点值数组。\n- 存储策略2 (稀疏CSR)：\n    - 格式：压缩稀疏行 (CSR)。\n    - 非零值：64位浮点数。\n    - 列索引：32位整数。\n    - 行指针数组 (`indptr`)：使用32位整数，长度为 $N+1$ 的数组。\n- 数值：\n    - $N = 100,000$。\n    - $k = 15$。\n- 单位换算：$1 \\,\\mathrm{GB} = 10^{9}$ 字节。\n- 输出要求：以GB为单位的总内存节省量，四舍五入至四位有效数字。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题具有科学依据，因为它描述了在使用t-SNE和UMAP等成熟算法分析大规模单细胞数据时一个标准且关键的计算考量。使用稀疏矩阵表示来处理$k$-近邻图的概念是这些方法可扩展性的基础。该问题定义明确，提供了执行计算所需的所有必要参数（$N$、$k$、数据类型位深度）。语言客观精确。该问题不违反任何无效性标准；它是完整的、一致的、现实的且可形式化的。\n\n### 步骤3：结论与行动\n该问题有效。将提供完整解答。\n\n### 解答推导\n该问题要求比较密集存储和稀疏存储两种方式下两个 $N \\times N$ 矩阵 $W$ 和 $P$ 的内存使用情况，然后进行具体的数值计算。我们首先从基本原理出发，推导每种存储策略的内存成本。\n\n首先，我们定义基本数据类型以字节为单位的内存大小。\n- 一个64位浮点值需要 $\\frac{64}{8} = 8$ 字节。\n- 一个32位整数需要 $\\frac{32}{8} = 4$ 字节。\n\n**1. 密集存储内存计算**\n\n在密集存储格式中，矩阵的每个元素都被显式存储，无论其值是多少。\n- 一个 $N \\times N$ 矩阵包含 $N^2$ 个元素。\n- 每个元素是一个64位浮点数，占用8个字节。\n- 存储单个密集矩阵所需的总内存 $M_{\\text{dense, single}}$ 为：\n$$ M_{\\text{dense, single}} = N^2 \\times 8 \\text{ bytes} $$\n- 由于我们需要存储 $W$ 和 $P$ 两个矩阵，密集存储的总内存 $M_{\\text{dense, total}}$ 为：\n$$ M_{\\text{dense, total}} = 2 \\times M_{\\text{dense, single}} = 2 \\times 8 N^2 = 16 N^2 \\text{ bytes} $$\n\n**2. 稀疏存储 (CSR) 内存计算**\n\n压缩稀疏行 (CSR) 格式只存储非零元素及其位置。它包括三个数组：\n- `data`：存储非零元素的值。\n- `indices`：存储 `data` 中每个值对应的列索引。\n- `indptr`：一个大小为 $N+1$ 的数组，其中 `indptr[i]` 指向 `data` 和 `indices` 数组中第 `i` 行的起始位置。\n\n问题说明，$N$ 行中的每一行都恰好有 $k$ 个非零项。\n- 一个矩阵中的非零项总数为 $N \\times k$。\n- `data` 数组的内存（存储 $Nk$ 个浮点值）：\n$$ M_{\\text{data}} = (N \\times k) \\times 8 \\text{ bytes} $$\n- `indices` 数组的内存（存储 $Nk$ 个整型列索引）：\n$$ M_{\\text{indices}} = (N \\times k) \\times 4 \\text{ bytes} $$\n- `indptr` 数组的内存（存储 $N+1$ 个整型指针）：\n$$ M_{\\text{indptr}} = (N+1) \\times 4 \\text{ bytes} $$\n- CSR格式下单个矩阵的总内存 $M_{\\text{sparse, single}}$ 是这些部分的总和：\n$$ M_{\\text{sparse, single}} = M_{\\text{data}} + M_{\\text{indices}} + M_{\\text{indptr}} = 8Nk + 4Nk + 4(N+1) = 12Nk + 4N + 4 \\text{ bytes} $$\n- 对于 $W$ 和 $P$ 两个矩阵，稀疏存储的总内存 $M_{\\text{sparse, total}}$ 为：\n$$ M_{\\text{sparse, total}} = 2 \\times M_{\\text{sparse, single}} = 2(12Nk + 4N + 4) = 24Nk + 8N + 8 \\text{ bytes} $$\n\n**3. 概念比较与稀疏存储的合理性**\n\n当 $M_{\\text{sparse, single}} < M_{\\text{dense, single}}$ 时，稀疏存储会减少内存占用。这个不等式是：\n$$ 12Nk + 4N + 4 < 8N^2 $$\n对于较大的 $N$，低阶项 $4N$ 和 $4$ 可以忽略不计。该条件近似为：\n$$ 12Nk < 8N^2 \\implies 12k < 8N \\implies k < \\frac{2}{3}N $$\n在单细胞分析中，最近邻的数量 $k$ (通常为 $10$-$100$) 远小于细胞数量 $N$ (通常为 $10^4$-$10^6$)。条件 $k \\ll N$ 确保 $k < \\frac{2}{3}N$ 以多个数量级的差距成立。密集存储的内存需求随 $N$呈二次方增长（即 $O(N^2)$），而稀疏存储则随 $N$呈线性增长（即 $O(Nk)$）。这种规模上的差异使得密集存储对于大型数据集来说在计算上是不可行的，而稀疏存储则保持可行。\n\n**4. 内存节省的数值计算**\n\n总内存节省量 $\\Delta M$ 是密集存储总内存与稀疏存储总内存之差。\n$$ \\Delta M = M_{\\text{dense, total}} - M_{\\text{sparse, total}} $$\n$$ \\Delta M = 16N^2 - (24Nk + 8N + 8) \\text{ bytes} $$\n\n现在，我们代入给定值 $N = 100,000 = 10^5$ 和 $k=15$。\n\n- 计算密集存储总内存：\n$$ M_{\\text{dense, total}} = 16 \\times (10^5)^2 = 16 \\times 10^{10} \\text{ bytes} $$\n\n- 计算稀疏存储总内存：\n$$ M_{\\text{sparse, total}} = 24 \\times (10^5) \\times 15 + 8 \\times (10^5) + 8 $$\n$$ M_{\\text{sparse, total}} = 360 \\times 10^5 + 8 \\times 10^5 + 8 $$\n$$ M_{\\text{sparse, total}} = (360 + 8) \\times 10^5 + 8 = 368 \\times 10^5 + 8 = 36,800,000 + 8 = 36,800,008 \\text{ bytes} $$\n\n- 以字节为单位计算内存节省量：\n$$ \\Delta M = 160,000,000,000 - 36,800,008 = 159,963,199,992 \\text{ bytes} $$\n\n最后，我们使用 $1 \\,\\mathrm{GB} = 10^{9}$ 字节的约定，将节省的内存转换为千兆字节 (GB)。\n$$ \\Delta M_{\\text{GB}} = \\frac{159,963,199,992}{10^9} = 159.963199992 \\,\\mathrm{GB} $$\n\n题目要求将答案四舍五入到四位有效数字。数值 $159.963...$ 四舍五入到四位有效数字是 $160.0$。在这种情况下，末尾的零是有效数字。", "answer": "$$\\boxed{160.0}$$", "id": "3334326"}, {"introduction": "系统生物学的一个重要前沿领域是整合来自相同细胞的多种数据类型，例如基因表达谱和空间位置信息。这项高级练习将引导你从零开始构建一个定制化的多模态对齐算法。你将通过最小化一个组合损失函数$\\mathcal{L}$，来学习一个同时与表达距离$D^{(\\text{expr})}$和空间距离$D^{(\\text{spatial})}$对齐的低维嵌入$Y$ [@problem_id:3334340]。完成这个练习后，你将获得算法开发的实战经验，学会如何将一个生物学目标转化为一个数学优化问题，并利用梯度下降法实现其求解。", "problem": "给定在同一组单细胞上测量的两种模态：基于液滴的单细胞RNA测序 (scRNA-seq) 和空间转录组学。scRNA-seq模态为每个细胞提供高维表达谱，而空间模态提供在组织中定位每个细胞的二维坐标。目标是通过优化成对距离的组合损失，推导并实现一种基于原理的非线性降维方法，该方法将scRNA-seq数据的嵌入与空间距离对齐。\n\n从以下基本定义和经过充分检验的事实出发：\n\n- 分子生物学中心法则 (CDMB) 断言，遗传信息从DNA流向RNA，再到蛋白质。对于单细胞RNA测序，信使RNA丰度是细胞状态的一个信息丰富的代理指标。这为使用表达谱之间的成对距离作为细胞状态空间的结构描述符提供了理论基础。我们将限制在标准化特征上计算的成对欧几里得距离，这是一种广泛接受的基线描述符。\n\n- 对于 $\\mathbb{R}^m$ 中的任意点集 $\\{x_i\\}_{i=1}^N$，欧几里得距离定义为 $d(x_i,x_j) = \\|x_i - x_j\\|_2$，其中 $\\|\\cdot\\|_2$ 表示 $\\ell_2$ 范数。对于 $\\mathbb{R}^2$ 中的空间坐标 $\\{s_i\\}_{i=1}^N$，空间距离为 $d(s_i,s_j) = \\|s_i - s_j\\|_2$。\n\n- 在度量多维缩放 (metric Multidimensional Scaling, MDS) 中，通过最小化一个应力目标函数来学习 $\\mathbb{R}^p$ 中的嵌入 $\\{y_i\\}_{i=1}^N$，该目标函数惩罚嵌入距离与目标距离矩阵之间的偏差。在这里，我们扩展了这一原理，以同时对齐两个目标距离矩阵，一个来自scRNA-seq表达，另一个来自空间坐标，方法是使用惩罚项的凸组合。\n\n构建并实现以下对齐目标。令 $D^{(\\text{expr})}_{ij}$ 表示从 scRNA-seq 表达谱计算出的欧几里得距离，令 $D^{(\\text{spatial})}_{ij}$ 表示从空间坐标计算出的欧几里得距离。令嵌入为 $Y \\in \\mathbb{R}^{N \\times p}$，其行为 $y_i \\in \\mathbb{R}^p$，成对嵌入距离为 $R_{ij} = \\|y_i - y_j\\|_2$。定义组合损失\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i  j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right],\n$$\n其中 $\\alpha \\in [0,1]$ 是一个模态权重，它在纯表达驱动的对齐 ($\\alpha = 1$) 和纯空间驱动的对齐 ($\\alpha = 0$) 之间进行插值。为确保跨模态距离的可公度性，您必须通过除以其所有非零上三角项的均值来归一化每个目标距离矩阵。\n\n您的任务：\n\n- 从第一性原理出发，从欧几里得距离的定义和微分的链式法则开始，推导 $\\mathcal{L}(Y;\\alpha)$ 相对于嵌入坐标 $Y$ 的梯度。通过极限论证明确处理 $R_{ij} = 0$ 的情况，以产生数值稳定的更新。\n\n- 设计一个算法，通过对 $Y$ 进行迭代梯度下降来最小化 $\\mathcal{L}(Y;\\alpha)$，并满足以下设计约束和理由：\n  - 使用空间坐标初始化 $Y$，并对其进行缩放，使非零上三角嵌入距离的均值为 $1$，这与归一化后目标距离的均值相匹配。\n  - 在每次迭代中重新中心化 $Y$，使其在所有细胞上的均值为零，以避免漂移。\n  - 使用固定的步长以确保为提供的测试套件收敛，并在达到固定的迭代次数后，或当 $\\mathcal{L}(Y;\\alpha)$ 在过去 $20$ 次迭代中的绝对变化低于一个小的阈值时停止。\n  - 通过将任何除以 $R_{ij}$ 的操作替换为除以 $\\max(R_{ij}, \\varepsilon)$ (其中 $\\varepsilon  0$ 是一个小数) 来确保数值稳定性。\n\n- 在一个完整的、无需输入的程序中实现该算法，并为指定的测试套件生成所需的输出。\n\n测试套件规范：\n\n对于所有测试用例，设细胞数量为 $N = 6$，嵌入维度为 $p = 2$。每个测试用例的空间坐标和 scRNA-seq 表达特征固定如下。\n\n对于测试用例 1 到 3，使用相同的空间坐标和表达特征：\n\n空间坐标矩阵 $S \\in \\mathbb{R}^{6 \\times 2}$：\n$$\nS \\;=\\; \\begin{bmatrix}\n0.0  0.0 \\\\\n1.0  0.0 \\\\\n2.0  0.0 \\\\\n0.0  1.0 \\\\\n1.0  1.0 \\\\\n2.0  1.0\n\\end{bmatrix}.\n$$\n\n表达特征矩阵 $X \\in \\mathbb{R}^{6 \\times 3}$，其行定义为\n$$\nx\\_i^{(1)} \\;=\\; x\\_i^{(\\text{spatial-x})} \\;+\\; \\delta\\_i^{(1)}, \\quad\nx\\_i^{(2)} \\;=\\; x\\_i^{(\\text{spatial-y})} \\;+\\; \\delta\\_i^{(2)}, \\quad\nx\\_i^{(3)} \\;=\\; 0.5\\, x\\_i^{(\\text{spatial-x})} \\;+\\; 0.5\\, x\\_i^{(\\text{spatial-y})} \\;+\\; \\delta\\_i^{(3)},\n$$\n其中空间坐标是 $S$ 相应行的条目，确定性扰动为\n$$\n\\delta^{(1)} \\;=\\; \\begin{bmatrix} 0.00  0.05  -0.05  0.025  -0.025  0.00 \\end{bmatrix}^\\top, \\quad\n\\delta^{(2)} \\;=\\; \\begin{bmatrix} 0.01  -0.01  0.015  -0.015  0.00  0.02 \\end{bmatrix}^\\top, \\quad\n\\delta^{(3)} \\;=\\; \\begin{bmatrix} 0.02  0.00  -0.02  0.03  -0.03  0.01 \\end{bmatrix}^\\top.\n$$\n测试用例 1：使用 $\\alpha = 0.5$。\n\n测试用例 2：使用 $\\alpha = 1.0$。\n\n测试用例 3：使用 $\\alpha = 0.0$。\n\n对于测试用例 4，使用相同的空间坐标 $S$，但将表达特征乘以因子 $3.0$，即使用 $X^{(\\text{scaled})} = 3.0 \\cdot X$，并设置 $\\alpha = 0.3$。\n\n对于测试用例 5，通过将第五个点复制为最后一个点来修改空间坐标，从而使一对点之间的空间距离为零：\n$$\nS^{(\\text{dup})} \\;=\\; \\begin{bmatrix}\n0.0  0.0 \\\\\n1.0  0.0 \\\\\n2.0  0.0 \\\\\n0.0  1.0 \\\\\n1.0  1.0 \\\\\n1.0  1.0\n\\end{bmatrix},\n$$\n使用原始表达特征 $X$，并设置 $\\alpha = 0.5$。\n\n距离计算与归一化：\n\n- 对于每个测试用例，将 $D^{(\\text{expr})}$ 计算为相关表达矩阵（$X$ 或 $X^{(\\text{scaled})}$）各行的成对欧几里得距离，并将 $D^{(\\text{spatial})}$ 计算为相关空间矩阵（$S$ 或 $S^{(\\text{dup})}$）各行的成对欧几里得距离。\n\n- 通过除以非零上三角项的均值来归一化 $D^{(\\text{expr})}$ 和 $D^{(\\text{spatial})}$，使得归一化后非零距离的均值为 $1$。\n\n算法超参数：\n\n- 使用一个小的正常数 $\\varepsilon = 10^{-8}$ 以确保除以 $R_{ij}$ 时的数值稳定性。\n\n- 使用固定步长 $\\eta = 0.05$。\n\n- 最多使用 $1000$ 次迭代，如果最后 $20$ 次迭代中 $\\mathcal{L}(Y;\\alpha)$ 的绝对变化低于 $10^{-9}$，则提前停止。\n\n最终输出格式：\n\n您的程序应生成一行输出，其中包含五个测试用例的结果，结果为一个用方括号括起来的逗号分隔列表，按测试用例 1 到 5 的顺序排列。每个结果是 $\\mathcal{L}(Y;\\alpha)$ 的最终值，以浮点数表示（无量纲单位）。例如，输出必须采用以下格式\n$$\n[\\ell_1, \\ell_2, \\ell_3, \\ell_4, \\ell_5],\n$$\n其中$\\ell_k$表示测试用例k的最终损失。[@problem_id:3334340]", "solution": "将嵌入与单细胞表达数据和空间坐标对齐的问题，被表述为最小化一个组合的类应力 (stress-like) 损失函数。解决方案需要推导该损失函数的梯度，并实现一个迭代梯度下降算法来找到最优的低维嵌入。\n\n### 1. 梯度推导\n\n组合损失函数 $\\mathcal{L}(Y;\\alpha)$ 定义为所有唯一细胞对 $(i,j)$ (其中 $1 \\le i  j \\le N$) 的总和：\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i  j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right]\n$$\n此处，$Y \\in \\mathbb{R}^{N \\times p}$ 是嵌入坐标 $y_i \\in \\mathbb{R}^p$ 的矩阵，$R_{ij} = \\|y_i - y_j\\|_2$ 是嵌入中点 $i$ 和 $j$ 之间的欧几里得距离，$D^{(\\text{expr})}_{ij}$ 和 $D^{(\\text{spatial})}_{ij}$ 分别是来自表达和空间模态的目标距离，而 $\\alpha \\in [0,1]$ 是一个加权参数。为简化起见，我们定义一个加权目标距离 $W_{ij} = \\alpha D^{(\\text{expr})}_{ij} + (1 - \\alpha)D^{(\\text{spatial})}_{ij}$。单个对 $(i,j)$ 的损失项可以通过展开平方来重写：\n$$\n\\mathcal{L}_{ij} = \\alpha(R_{ij}^2 - 2R_{ij}D^{(\\text{expr})}_{ij} + (D^{(\\text{expr})}_{ij})^2) + (1-\\alpha)(R_{ij}^2 - 2R_{ij}D^{(\\text{spatial})}_{ij} + (D^{(\\text{spatial})}_{ij})^2)\n$$\n$$\n\\mathcal{L}_{ij} = R_{ij}^2 - 2R_{ij}(\\alpha D^{(\\text{expr})}_{ij} + (1-\\alpha)D^{(\\text{spatial})}_{ij}) + \\text{const} = R_{ij}^2 - 2R_{ij}W_{ij} + \\text{const}\n$$\n完整损失为 $\\mathcal{L} = \\sum_{1 \\le i  j \\le N} \\mathcal{L}_{ij}$。我们需要计算关于单个细胞 $k$ 的坐标的梯度，即向量 $\\frac{\\partial \\mathcal{L}}{\\partial y_k}$。坐标向量 $y_k$ 出现在任何满足 $i=k$ 或 $j=k$ 的项 $\\mathcal{L}_{ij}$ 中。$y_k$ 的总梯度是这些项的偏导数之和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j  k} \\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} + \\sum_{i  k} \\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k}\n$$\n我们应用链式法则，$\\frac{\\partial \\mathcal{L}_{ij}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} \\frac{\\partial R_{ij}}{\\partial y_k}$。\n\n首先，我们求 $\\mathcal{L}_{ij}$ 关于 $R_{ij}$ 的导数：\n$$\n\\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} = 2\\alpha(R_{ij} - D^{(\\text{expr})}_{ij}) + 2(1-\\alpha)(R_{ij} - D^{(\\text{spatial})}_{ij}) = 2(R_{ij} - W_{ij})\n$$\n接着，我们求 $R_{ij} = \\|y_i - y_j\\|_2$ 关于 $y_i$ 和 $y_j$ 的导数。使用欧几里得范数的标准导数：\n$$\n\\frac{\\partial R_{ij}}{\\partial y_i} = \\frac{y_i - y_j}{\\|y_i - y_j\\|_2} = \\frac{y_i - y_j}{R_{ij}} \\quad \\text{和} \\quad \\frac{\\partial R_{ij}}{\\partial y_j} = \\frac{y_j - y_i}{\\|y_i - y_j\\|_2} = -\\frac{y_i - y_j}{R_{ij}}\n$$\n对于任何 $k \\ne i, j$，$\\frac{\\partial R_{ij}}{\\partial y_k}$ 是零向量。\n\n现在我们结合这些结果。对于一个项 $\\mathcal{L}_{kj}$ 且 $jk$：\n$$\n\\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{kj}}{\\partial R_{kj}} \\frac{\\partial R_{kj}}{\\partial y_k} = 2(R_{kj} - W_{kj}) \\frac{y_k - y_j}{R_{kj}} = 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\n对于一个项 $\\mathcal{L}_{ik}$ 且 $ik$：\n$$\n\\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ik}}{\\partial R_{ik}} \\frac{\\partial R_{ik}}{\\partial y_k} = 2(R_{ik} - W_{ik}) \\frac{y_k - y_i}{R_{ik}} = 2\\left(1 - \\frac{W_{ik}}{R_{ik}}\\right)(y_k - y_i)\n$$\n因此，我们得到关于 $y_k$ 的总梯度：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j \\ne k} 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\n当 $R_{kj} = 0$ 时，梯度中的 $\\frac{W_{kj}}{R_{kj}}$ 项是未定义的。然而，如果 $W_{kj}  0$，当 $R_{kj} \\to 0$ 时，$\\mathcal{L}_{kj} \\approx (D_{kj})^2 - 2R_{kj}W_{kj}$，梯度 $\\frac{\\partial \\mathcal{L}_{kj}}{\\partial R_{kj}}$ 趋向于 $-2W_{kj}$，这会产生一个无限大的力将 $y_k$ 和 $y_j$ 推开。如果 $W_{kj} = 0$，则梯度为零。为确保数值稳定性，我们将 $R_{kj}$ 替换为 $\\max(R_{kj}, \\varepsilon)$，其中 $\\varepsilon$ 是一个小的正数。\n\n**向量化梯度**：\n梯度可以优雅地用图拉普拉斯算子的形式表示。定义一个加权邻接矩阵 $C$，其中 $C_{ij} = 2\\left(1 - \\frac{W_{ij}}{R_{ij}}\\right)$ 且 $C_{ii} = 0$。由于 $W$ 和 $R$ 是对称的，$C$ 也是对称的。总梯度可以写为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j \\ne k} C_{kj}(y_k - y_j) = \\left(\\sum_{j \\ne k} C_{kj}\\right)y_k - \\sum_{j \\ne k} C_{kj}y_j\n$$\n这恰好是图拉普拉斯矩阵 $L = D - C$ 作用于坐标向量 $y_k$ 时的第 $k$ 行，其中 $D$ 是度矩阵，其对角线元素为 $D_{kk} = \\sum_{j \\ne k} C_{kj}$。\n整个梯度矩阵 $\\nabla_Y \\mathcal{L}$ 的第 $k$ 行是 $(\\nabla_Y \\mathcal{L})_k = \\frac{\\partial \\mathcal{L}}{\\partial y_k}$。因此，梯度矩阵可以一次性计算为：\n$$\n\\nabla_Y \\mathcal{L} = L Y\n$$\n这个向量化的表达式对于高效实现至关重要。\n\n### 2. 算法设计\n\n1.  **输入**：空间坐标 $S \\in \\mathbb{R}^{N \\times 2}$，表达特征 $X \\in \\mathbb{R}^{N \\times q}$，模态权重 $\\alpha$。\n2.  **预处理**：\n    a. 计算成对空间距离矩阵 $D^{(\\text{spatial})}$。\n    b. 计算成对表达距离矩阵 $D^{(\\text{expr})}$。\n    c. 通过分别除以各自非零上三角项的均值来归一化 $D^{(\\text{spatial})}$ 和 $D^{(\\text{expr})}$。\n    d. 计算加权目标距离矩阵 $W = \\alpha D^{(\\text{expr})} + (1-\\alpha)D^{(\\text{spatial})}$。\n3.  **初始化**：\n    a. 将嵌入初始化为空间坐标：$Y \\leftarrow S$。\n    b. 计算当前嵌入 $Y$ 的非零上三角距离的均值。\n    c. 通过除以该均值来缩放 $Y$，使其与归一化的目标距离具有相同的尺度。\n4.  **迭代优化**：\n    `for iter in 1 to MAX_ITER:`\n    a. **中心化**：通过减去质心来重新中心化 $Y$：$Y \\leftarrow Y - \\bar{Y}$。\n    b. **计算当前距离**：计算 $Y$ 中的成对欧几里得距离矩阵 $R$。\n    c. **计算损失**：计算当前损失 $\\mathcal{L}(Y;\\alpha)$ 并存储。\n    d. **收敛检查**：如果迭代次数足够，且损失在最近的窗口内变化不大，则中断循环。\n    e. **计算梯度**：\n        i.  用 $\\max(R_{ij}, \\varepsilon)$ 替换 $R_{ij}$ 以避免除以零。\n        ii. 计算权重矩阵 $C_{ij} = 2(1 - W_{ij} / R_{ij})$。\n        iii. 计算图拉普拉斯矩阵 $L = \\text{diag}(C\\mathbf{1}) - C$。\n        iv. 计算梯度矩阵 $\\nabla_Y \\mathcal{L} = L Y$。\n    f. **更新**：使用固定步长 $\\eta$ 执行梯度下降步骤：$Y \\leftarrow Y - \\eta \\cdot \\nabla_Y \\mathcal{L}$。\n5.  **输出**：返回最终的损失值 $\\mathcal{L}$。\n\n该算法将为每个测试用例运行，并收集最终的损失值以供输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Hyperparameters as specified in the problem statement\nEPS = 1e-8\nSTEP_SIZE = 0.05\nMAX_ITER = 1000\nSTOP_WINDOW = 20\nSTOP_TOL = 1e-9\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the test cases and print the final result.\n    \"\"\"\n\n    def run_optimization(alpha, S_in, X_in):\n        \"\"\"\n        Performs the gradient descent optimization for a single test case.\n        \"\"\"\n        # 1. Preprocessing: compute and normalize distance matrices\n        D_spatial = squareform(pdist(S_in, 'euclidean'))\n        D_expr = squareform(pdist(X_in, 'euclidean'))\n\n        # Normalize by mean of non-zero upper-triangular entries.\n        D_spatial_ut = pdist(S_in, 'euclidean')\n        mean_spatial = D_spatial_ut[D_spatial_ut > 0].mean() if np.any(D_spatial_ut > 0) else 0.\n        D_spatial_norm = D_spatial / mean_spatial if mean_spatial > 0 else D_spatial\n\n        D_expr_ut = pdist(X_in, 'euclidean')\n        mean_expr = D_expr_ut[D_expr_ut > 0].mean() if np.any(D_expr_ut > 0) else 0.\n        D_expr_norm = D_expr / mean_expr if mean_expr > 0 else D_expr\n        \n        # Weighted target distance matrix\n        W = alpha * D_expr_norm + (1 - alpha) * D_spatial_norm\n        \n        # 2. Initialization\n        Y = S_in.copy().astype(np.float64) # Use float64 for precision\n\n        # Scale Y so that the mean of its nonzero upper-triangular distances is 1.\n        R_init_ut = pdist(Y, 'euclidean')\n        mean_R_init = R_init_ut[R_init_ut > 0].mean() if np.any(R_init_ut > 0) else 0.\n        if mean_R_init > 0:\n            Y /= mean_R_init\n            \n        # 3. Iterative Optimization\n        loss_history = []\n        \n        for i in range(MAX_ITER):\n            # Recenter at each iteration to prevent drift\n            Y -= Y.mean(axis=0)\n\n            # Compute current embedding distances\n            R = squareform(pdist(Y, 'euclidean'))\n\n            # Compute loss\n            loss_term1 = alpha * np.square(R - D_expr_norm)\n            loss_term2 = (1 - alpha) * np.square(R - D_spatial_norm)\n            loss = np.sum(np.triu(loss_term1 + loss_term2, k=1))\n            loss_history.append(loss)\n\n            # Check for early stopping\n            if i >= STOP_WINDOW:\n                if abs(loss_history[-1] - loss_history[-STOP_WINDOW])  STOP_TOL:\n                    break\n            \n            # Compute gradient using the vectorized approach\n            R_safe = np.maximum(R, EPS)\n            ratio_matrix = W / R_safe\n            np.fill_diagonal(ratio_matrix, 0)\n            \n            C_matrix = 2 * (1 - ratio_matrix)\n            np.fill_diagonal(C_matrix, 0)\n                    \n            L_lap = np.diag(C_matrix.sum(axis=1)) - C_matrix\n            \n            grad = L_lap @ Y\n            \n            # Update embedding coordinates\n            Y -= STEP_SIZE * grad\n\n        # Final loss calculation after optimization loop\n        R_final = squareform(pdist(Y, 'euclidean'))\n        loss_term1_final = alpha * np.square(R_final - D_expr_norm)\n        loss_term2_final = (1 - alpha) * np.square(R_final - D_spatial_norm)\n        final_loss = np.sum(np.triu(loss_term1_final + loss_term2_final, k=1))\n        \n        return final_loss\n\n    # Define the test cases from the problem statement.\n    S = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [2.0, 1.0]\n    ])\n\n    delta1 = np.array([0.00, 0.05, -0.05, 0.025, -0.025, 0.00])\n    delta2 = np.array([0.01, -0.01, 0.015, -0.015, 0.00, 0.02])\n    delta3 = np.array([0.02, 0.00, -0.02, 0.03, -0.03, 0.01])\n    \n    X = np.zeros((6, 3))\n    X[:, 0] = S[:, 0] + delta1\n    X[:, 1] = S[:, 1] + delta2\n    X[:, 2] = 0.5 * S[:, 0] + 0.5 * S[:, 1] + delta3\n\n    X_scaled = 3.0 * X\n    S_dup = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n    ])\n\n    test_cases = [\n        {'alpha': 0.5, 'S': S, 'X': X},\n        {'alpha': 1.0, 'S': S, 'X': X},\n        {'alpha': 0.0, 'S': S, 'X': X},\n        {'alpha': 0.3, 'S': S, 'X': X_scaled},\n        {'alpha': 0.5, 'S': S_dup, 'X': X}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_loss = run_optimization(case['alpha'], case['S'], case['X'])\n        results.append(final_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3334340"}]}