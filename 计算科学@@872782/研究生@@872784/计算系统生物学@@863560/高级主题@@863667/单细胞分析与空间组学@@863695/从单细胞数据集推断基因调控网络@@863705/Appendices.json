{"hands_on_practices": [{"introduction": "在推断复杂的基因调控网络之前，我们必须首先正确地处理原始的单细胞数据。原始的基因表达计数受到测序深度等技术因素的影响，这可能导致基因之间产生误导性的相关性。本练习旨在阐明数据标准化和变换的重要性，您将亲手实现并比较标准的对数正规化与一种更高级的方差稳定变换方法。通过量化这些方法如何改变计算出的皮尔逊相关系数，您将亲身体会到基础数据处理步骤如何直接影响网络推断的基石 [@problem_id:3314516]。", "problem": "给定单细胞核糖核酸测序的计数数据，其形式为一个非负整数矩阵（表示多个基因在多个细胞中的表达）和一个表示每个细胞测序深度的正规模因子向量。您的任务是实现对该计数数据的两种变换：带伪计数的对数归一化，以及方差稳定的正则化对数变换。然后，您需要量化正则化对指定基因对之间的皮尔逊积矩相关系数的影响。\n\n请从单细胞基因调控网络分析的以下基本原理出发：\n- 分子生物学中心法则指出，脱氧核糖核酸（DNA）转录为核糖核酸（RNA），再翻译为蛋白质；基因表达通过每个细胞中每个基因的核糖核酸转录本计数来量化。\n- 单细胞核糖核酸测序计数通常可以很好地用过度离散的计数数据模型（通常为负二项分布）来描述。测序深度是原始计数的一个混杂因素，因此除以每个细胞的规模因子是一个广泛接受的归一化步骤。\n- 自然对数可以减小乘性尺度，并在与正伪计数一起使用时部分稳定计数数据的方差，以避免出现未定义的 $\\log(0)$。\n- 正则化，例如岭式收缩，通过将偏差向中心趋势收缩来减小方差。\n- 皮尔逊积矩相关系数衡量两个变量在样本间的线性关联。\n\n设 $C \\in \\mathbb{N}_0^{G \\times N}$ 表示包含 $G$ 个基因和 $N$ 个细胞的计数矩阵，设 $s \\in \\mathbb{R}_+^{N}$ 表示正规模因子向量。对于一个正伪计数 $\\alpha \\in \\mathbb{R}_+$, 逐项定义对数归一化表达矩阵 $L \\in \\mathbb{R}^{G \\times N}$ 如下\n$$\nL_{ij} = \\log\\left(\\frac{C_{ij}}{s_j} + \\alpha\\right),\n$$\n其中 $\\log$ 表示自然对数。为了构建方差稳定的正则化对数变换，我们使用一个从归一化平均表达量派生出的基因特异性岭参数，将基因层面的对数表达偏差向基因层面的均值进行收缩。具体来说，设归一化（非对数）表达为 $X_{ij} = \\frac{C_{ij}}{s_j}$，基因层面的归一化均值为 $\\bar{X}_i = \\frac{1}{N}\\sum_{j=1}^{N} X_{ij}$。对于一个正则化尺度 $\\beta \\in \\mathbb{R}_+$ 和一个为避免除以零而设的小正常数 $\\varepsilon \\in \\mathbb{R}_+$，定义基因特异性正则化强度为\n$$\n\\lambda_i = \\frac{\\beta}{\\bar{X}_i + \\varepsilon}。\n$$\n设基因层面的对数均值为 $\\mu_i = \\frac{1}{N} \\sum_{j=1}^{N} L_{ij}$。然后，通过围绕 $\\mu_i$ 收缩偏差来逐项定义正则化对数变换 $R \\in \\mathbb{R}^{G \\times N}$：\n$$\nR_{ij} = \\mu_i + \\frac{1}{1+\\lambda_i}\\left(L_{ij} - \\mu_i\\right).\n$$\n\n对于任意基因对 $(a,b)$，其中 $a \\in \\{0,1,\\dots,G-1\\}$ 且 $b \\in \\{0,1,\\dots,G-1\\}$，定义变换 $T \\in \\{L,R\\}$ 下跨细胞的皮尔逊积矩相关系数为\n$$\n\\rho_T(a,b) = \\frac{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)\\left(T_{b j} - \\bar{T}_b\\right)}{\\sqrt{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)^2}\\sqrt{\\sum_{j=1}^{N} \\left(T_{b j} - \\bar{T}_b\\right)^2}},\n$$\n其中 $\\bar{T}_a = \\frac{1}{N}\\sum_{j=1}^{N} T_{a j}$ 且 $\\bar{T}_b = \\frac{1}{N}\\sum_{j=1}^{N} T_{b j}$。如果在某种变换下，任一基因的方差为零（即分母为零），则定义该变换下的 $\\rho_T(a,b)$ 为 $0$。\n\n通过为每个指定的基因对 $(a,b)$ 计算差值，来评估方差稳定化如何改变相关性：\n$$\n\\Delta(a,b) = \\rho_R(a,b) - \\rho_L(a,b).\n$$\n\n请实现一个程序，该程序针对一个包含计数矩阵及其附带参数的小型测试套件，为每个测试用例计算指定基因对上 $\\Delta(a,b)$ 的算术平均值。最终输出必须将所有提供的测试用例的结果聚合为单行，形式为用方括号括起来的逗号分隔列表。\n\n使用以下测试套件：\n\n- 测试用例 $1$（一般情况，具有中等计数、一些零值和异构的规模因子）：\n$$\nC^{(1)} = \\begin{bmatrix}\n12  0  5  20\\\\\n0  2  0  8\\\\\n30  25  0  10\n\\end{bmatrix}, \\quad\ns^{(1)} = [0.9, 1.1, 0.8, 1.3], \\quad\n\\alpha^{(1)} = 0.5, \\quad\n\\beta^{(1)} = 1.0, \\quad\n\\varepsilon^{(1)} = 10^{-6}, \\quad\nP^{(1)} = \\{(0,1), (0,2)\\}.\n$$\n\n- 测试用例 $2$（边界情况，具有大量零值和相等的规模因子）：\n$$\nC^{(2)} = \\begin{bmatrix}\n0  0  0  1  0\\\\\n3  0  0  0  2\n\\end{bmatrix}, \\quad\ns^{(2)} = [1.0, 1.0, 1.0, 1.0, 1.0], \\quad\n\\alpha^{(2)} = 10^{-3}, \\quad\n\\beta^{(2)} = 2.0, \\quad\n\\varepsilon^{(2)} = 10^{-6}, \\quad\nP^{(2)} = \\{(0,1)\\}.\n$$\n\n- 测试用例 $3$（边缘情况，规模因子具有极端的异构性）：\n$$\nC^{(3)} = \\begin{bmatrix}\n100  200  300\\\\\n5  10  15\\\\\n0  0  1\n\\end{bmatrix}, \\quad\ns^{(3)} = [0.5, 1.0, 5.0], \\quad\n\\alpha^{(3)} = 1.0, \\quad\n\\beta^{(3)} = 0.5, \\quad\n\\varepsilon^{(3)} = 10^{-3}, \\quad\nP^{(3)} = \\{(0,1), (1,2)\\}.\n$$\n\n您的程序必须：\n- 对每个测试用例 $k$，计算 $L$、$R$，然后对所有 $(a,b) \\in P^{(k)}$ 计算 $\\Delta(a,b)$，最后计算在 $P^{(k)}$ 上的算术平均值。\n- 生成单行输出，其中包含按顺序 $k=1, 2, 3$ 排列的测试用例的平均差值，格式为用方括号括起来的逗号分隔列表（例如，$[x_1,x_2,x_3]$）。不涉及单位；所有计算都是无量纲的实数。", "solution": "该问题要求实现并比较单细胞转录组学中两种常见的数据变换方法：标准的对数归一化和方差稳定的正则化对数变换。目标是量化正则化对基因对之间皮尔逊相关性的影响。该过程基于单细胞数据分析的既定原则，即原始计数会针对技术伪影（如测序深度）进行调整，并通过变换来稳定方差，以便于进行下游分析，如基因调控网络推断。\n\n每个测试用例的处理流程如下：\n\n首先，我们处理细胞间测序深度可变造成的混杂效应。原始计数矩阵 $C \\in \\mathbb{N}_0^{G \\times N}$（其中 $G$ 是基因数， $N$ 是细胞数）通过每个细胞的规模因子 $s \\in \\mathbb{R}_+^{N}$ 进行归一化。这会产生归一化表达矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{G \\times N}$，其元素为 $X_{ij} = \\frac{C_{ij}}{s_j}$。此步骤将每个细胞的计数置于可比较的尺度上。\n\n其次，我们应用标准的对数归一化。基因表达数据通常跨越几个数量级，并表现出均值-方差关系，即较高的计数具有较高的方差。应用自然对数来压缩此范围并减轻异方差性。为了处理零计数（其 $\\log(0)$ 未定义）的问题，我们添加一个正伪计数 $\\alpha \\in \\mathbb{R}_+$。因此，对数归一化表达矩阵 $L \\in \\mathbb{R}^{G \\times N}$ 逐项计算如下：\n$$\nL_{ij} = \\log\\left(X_{ij} + \\alpha\\right) = \\log\\left(\\frac{C_{ij}}{s_j} + \\alpha\\right)\n$$\n\n第三，我们构建方差稳定的正则化对数变换。虽然对数变换有帮助，但它不能完美地稳定方差。正则化，或称收缩，是一种通过将表达值拉向中心趋势来进一步减小方差的技术。在这里，我们实现一种基因特异性的岭式收缩。对于每个基因 $i$，这种收缩的强度（表示为 $\\lambda_i$）被设计成与其平均归一化表达量 $\\bar{X}_i = \\frac{1}{N}\\sum_{j=1}^{N} X_{ij}$ 成反比。其动机在于观察到低表达基因往往具有较高的相对方差（即噪声更大），因此能从更强的正则化中受益。对于给定的正则化尺度 $\\beta \\in \\mathbb{R}_+$ 和一个为防止除以零而设的小常数 $\\varepsilon \\in \\mathbb{R}_+$，正则化强度为：\n$$\n\\lambda_i = \\frac{\\beta}{\\bar{X}_i + \\varepsilon}\n$$\n然后，通过将对数表达偏差 $(L_{ij} - \\mu_i)$ 向基因层面的对数均值 $\\mu_i = \\frac{1}{N} \\sum_{j=1}^{N} L_{ij}$ 收缩，来获得正则化矩阵 $R \\in \\mathbb{R}^{G \\times N}$。正则化值 $R_{ij}$ 的公式是：\n$$\nR_{ij} = \\mu_i + \\frac{1}{1+\\lambda_i}\\left(L_{ij} - \\mu_i\\right)\n$$\n项 $\\frac{1}{1+\\lambda_i}$ 充当收缩因子。当一个基因的平均表达量 $\\bar{X}_i$ 较低时，$\\lambda_i$ 较大，这使得收缩因子变小，从而将值 $L_{ij}$ 强烈地拉向均值 $\\mu_i$。相反，对于高表达基因，$\\lambda_i$ 较小，其值仅受到微弱的正则化。\n\n第四，我们使用皮尔逊积矩相关系数 $\\rho$ 来量化指定基因对 $(a,b)$ 之间的线性关联。这对对数归一化数据 ($L$) 和正则化数据 ($R$) 都进行计算。对于给定的变换矩阵 $T \\in \\{L, R\\}$，基因 $a$ 和基因 $b$ 之间的相关性为：\n$$\n\\rho_T(a,b) = \\frac{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)\\left(T_{b j} - \\bar{T}_b\\right)}{\\sqrt{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)^2}\\sqrt{\\sum_{j=1}^{N} \\left(T_{b j} - \\bar{T}_b\\right)^2}}\n$$\n其中 $\\bar{T}_a$ 和 $\\bar{T}_b$ 是变换后表达值在所有细胞中的基因层面均值。根据规定，如果一对基因中任一基因的表达在所有细胞中方差为零（即其表达是恒定的），则分母为零，相关性 $\\rho_T(a,b)$ 定义为 $0$。\n\n最后，为了评估正则化的影响，我们为测试用例 $k$ 的指定集合 $P^{(k)}$ 中的每个基因对 $(a,b)$ 计算相关系数的差值 $\\Delta(a,b) = \\rho_R(a,b) - \\rho_L(a,b)$。每个测试用例的最终结果是这些差值在 $P^{(k)}$ 中所有基因对上的算术平均值。实现程序将对每个提供的测试用例执行这些步骤，并将平均差值聚合到单个输出列表中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full workflow for all test cases and prints the final result.\n    \"\"\"\n    \n    # Test case 1\n    C1 = np.array([[12, 0, 5, 20],\n                   [0, 2, 0, 8],\n                   [30, 25, 0, 10]], dtype=np.float64)\n    s1 = np.array([0.9, 1.1, 0.8, 1.3], dtype=np.float64)\n    alpha1 = 0.5\n    beta1 = 1.0\n    epsilon1 = 1e-6\n    P1 = [(0, 1), (0, 2)]\n\n    # Test case 2\n    C2 = np.array([[0, 0, 0, 1, 0],\n                   [3, 0, 0, 0, 2]], dtype=np.float64)\n    s2 = np.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=np.float64)\n    alpha2 = 1e-3\n    beta2 = 2.0\n    epsilon2 = 1e-6\n    P2 = [(0, 1)]\n\n    # Test case 3\n    C3 = np.array([[100, 200, 300],\n                   [5, 10, 15],\n                   [0, 0, 1]], dtype=np.float64)\n    s3 = np.array([0.5, 1.0, 5.0], dtype=np.float64)\n    alpha3 = 1.0\n    beta3 = 0.5\n    epsilon3 = 1e-3\n    P3 = [(0, 1), (1, 2)]\n\n    test_cases = [\n        (C1, s1, alpha1, beta1, epsilon1, P1),\n        (C2, s2, alpha2, beta2, epsilon2, P2),\n        (C3, s3, alpha3, beta3, epsilon3, P3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        C, s, alpha, beta, epsilon, P = case\n        \n        # Step 1: Normalize by size factors\n        X = C / s\n        \n        # Step 2: Log-normalize with pseudocount\n        L = np.log(X + alpha)\n        \n        # Step 3: Calculate parameters for regularization\n        # Gene-wise normalized mean\n        X_bar = np.mean(X, axis=1)\n        \n        # Gene-specific regularization strength\n        lambda_i = beta / (X_bar + epsilon)\n        \n        # Gene-wise log mean\n        mu_i = np.mean(L, axis=1)\n        \n        # Step 4: Apply regularized log transform\n        # Reshape for broadcasting:\n        # L has shape (G, N)\n        # mu_i and lambda_i have shape (G,). Reshape to (G, 1) to operate on each row.\n        mu_i_reshaped = mu_i.reshape(-1, 1)\n        lambda_i_reshaped = lambda_i.reshape(-1, 1)\n        \n        shrinkage_factor = 1.0 / (1.0 + lambda_i_reshaped)\n        deviations = L - mu_i_reshaped\n        R = mu_i_reshaped + shrinkage_factor * deviations\n        \n        # Step 5: Compute correlation differences\n        \n        def pearson_corr(v_a, v_b):\n            \"\"\"\n            Computes Pearson correlation, returning 0 if variance of either vector is 0.\n            \"\"\"\n            # Using a small tolerance for floating point comparisons\n            if np.isclose(np.var(v_a), 0.0) or np.isclose(np.var(v_b), 0.0):\n                return 0.0\n            \n            # np.corrcoef calculates the correlation matrix. We need the off-diagonal element.\n            # It handles the mean-centering and normalization internally.\n            corr_matrix = np.corrcoef(v_a, v_b)\n            return corr_matrix[0, 1]\n\n        deltas = []\n        for a, b in P:\n            # Correlation on log-normalized data\n            rho_L = pearson_corr(L[a, :], L[b, :])\n            \n            # Correlation on regularized data\n            rho_R = pearson_corr(R[a, :], R[b, :])\n            \n            # Difference\n            delta = rho_R - rho_L\n            deltas.append(delta)\n            \n        # Step 6: Calculate mean difference for the test case\n        mean_delta = np.mean(deltas)\n        results.append(mean_delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3314516"}, {"introduction": "在准备好数据之后，我们就可以开始推断网络结构本身了。基于约束的方法是一大类核心算法，它们通过系统性地检验基因间的条件独立性来推断网络的图结构。本练习要求您从第一性原理出发，实现经典的 Peter-Clark (PC) 算法。您将学习如何利用统计检验构建网络骨架，并根据 v-结构和方向传播规则来确定边的方向。这项练习能让您深入、机械地理解如何从观测数据中推断因果关系，并突出马尔可夫等价类的概念，即仅凭数据无法唯一确定某些边的方向 [@problem_id:3314528]。", "problem": "您将获得离散化的单细胞基因表达状态，这些状态以整数值数组的形式表示，其中每一列对应一个基因，每一行对应一个细胞。您的任务是，从基本原理出发，实现一个基于约束的 Peter-Clark (PC) 算法，该算法从数据中的条件独立性学习一个完全部分有向无环图 (Completed Partially Directed Acyclic Graph, CPDAG)。对于每个数据集，您必须量化学习到的 CPDAG 中已定向边的数量与仍未定向边的数量。算法的规范必须源自条件独立性、有向无环图 (Directed Acyclic Graphs, DAGs) 中的图分离以及离散多项式模型中基于似然的独立性检验的基本定义。\n\n您必须使用的基本原理：\n- 条件独立性的定义：随机变量 $X$ 和 $Y$ 在给定集合 $Z$ 的条件下是条件独立的，当且仅当对于所有具有非零概率的值，$p(x,y \\mid z) = p(x \\mid z)p(y \\mid z)$。\n- 有向无环图 (DAGs) 中的图分离 (也称为 $d$-分离)：由 DAG 结构所蕴含的条件独立关系。\n- 多项式分布计数的似然原理：在（条件）独立性的原假设下，列联表中的对数似然比统计量，在适当的正则性条件下，服从渐近卡方分布，其自由度由自由参数的数量减去约束的数量决定。\n\n您的程序必须实现：\n1) 骨架发现：从变量上的完全无向图开始，当存在一个大小为 $\\lvert S \\rvert \\leq k_{\\max}$ 的条件集 $S$，使得似然比检验在水平 $\\alpha$ 下未能拒绝 $X \\perp Y \\mid S$ 的原假设时，迭代地移除变量 $X$ 和 $Y$ 之间的无向边。为每个被移除的对 $\\{X,Y\\}$ 记录一个这样的分离集 $S$。\n2) V-结构定向：对于任何三元组 $X - Z - Y$，其中 $X$ 和 $Y$ 在骨架中不相邻，并且 $Z$ 不在为 $\\{X,Y\\}$ 记录的分离集中，则定向为 $X \\to Z \\leftarrow Y$。\n3) Meek 定向传播 (重复应用直到无变化)：如果存在 $A \\to B - C$ 且 $A$ 和 $C$ 不相邻，则将 $B - C$ 定向为 $B \\to C$。\n\n独立性检验必须使用根据经验计数构建的离散列联表的对数似然比统计量来实现。对于给定集合 $Z$ 的条件检验，聚合数据中出现的所有 $Z$ 配置的特定分层贡献。使用渐近卡方参考分布，其自由度等于每个检验对的非空水平数减一的乘积在所有分层上的总和。当得到的总自由度为 $0$ 时，将检验视为无信息的，并为算法目的接受条件独立性（即，取 $p$-值为 $1$）。使用显著性水平 $\\alpha$ 来判断独立性，即 $p$-值 $\\ge \\alpha$。\n\n对于每个数据集，假设学习到的 CPDAG 有一个有向边集和一个无向边集。每个数据集报告两个整数：定向边的数量（如果 $i \\to j$ 或 $j \\to i$ 中只有一个存在，则对 $\\{i,j\\}$ 计为一条定向边）和无向边的数量（如果 $i$ 和 $j$ 之间存在无向邻接，则计为一条）。没有邻接的对不计入。\n\n测试套件。在以下三个测试案例上实现您的算法。在所有情况下，使用显著性水平 $\\alpha = 0.001$ 和最大条件集大小 $k_{\\max} = 2$。\n\n- 案例 1 (四个基因，含一个碰撞体和一个下游目标)。变量为 $A$、$B$、$C$、$D$，每个变量取值于 $\\{0,1\\}$。确定性地构造 $C$ 为异或 $C = A \\oplus B$ 以创建碰撞体 $A \\to C \\leftarrow B$。构造 $D$ 作为 $C$ 的一个含噪声子节点：对于 $(A,B)$ 的每种配置，复制 $5$ 个样本，使得 $C$ 由 $A \\oplus B$ 固定，其中 $4$ 个样本设置 $D=C$，另 $1$ 个设置 $D=1-C$。将此块完全相同地重复 $50$ 次，得到 $1000$ 个样本。预期的结构是 $A \\to C \\leftarrow B$ 和 $C \\to D$。\n- 案例 2 (三个独立基因)。变量为 $A$、$B$、$C$，取值于 $\\{0,1\\}$。通过均匀枚举所有 $2^3=8$ 种配置来构建数据集，每种配置重复 $50$ 次，得到 $400$ 个样本。预期的结构没有边。\n- 案例 3 (三个基因组成的含噪声链)。变量为 $A$、$B$、$C$，取值于 $\\{0,1\\}$。按如下方式构建一个包含 $200$ 个样本的数据集：对于 $A \\in \\{0,1\\}$ 的每个值，生成 $100$ 个样本。在这 $100$ 个样本中，$90$ 个设置 $B=A$，$10$ 个设置 $B=1-A$。对于每个这样的 $(A,B)$ 记录，通过设置 $9$ 个 $C=B$ 和 $1$ 个 $C=1-B$ 将其扩展为 $10$ 行。预期的结构是 $A \\to B \\to C$。\n\n输入格式。没有外部输入；您的程序必须嵌入这些数据集。\n\n输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是一个包含两个整数 $[o,u]$ 的列表，分别代表一个测试案例的定向边数 $o$ 和无向边数 $u$。例如，输出必须类似于 $[[o_1,u_1],[o_2,u_2],[o_3,u_3]]$，不含空格。\n\n您的实现必须是自包含的，仅依赖于上述数学原理，并确定性地运行。不涉及物理单位或角度。所有数值答案均为整数。测试套件涵盖了一个带有下游定向传播的碰撞体案例、一个完全独立的边界案例，以及一个链式结构，其中条件独立性移除了长程边，但由于缺乏 v-结构而留下了一个马尔可夫等价类，保持未定向。", "solution": "我们从核心定义开始。对于离散随机变量，$X \\perp Y \\mid Z$ 意味着对于所有具有正概率的值，$p(x,y \\mid z) = p(x \\mid z)p(y \\mid z)$。在多项式抽样模型下，用于检验独立性与一般依赖性的对数似然比统计量，比较的是约束模型（其中联合概率分解为边际概率）与非约束模型。对于无条件检验，似然比统计量等于观测计数乘以观测计数与独立性假设下期望计数之比的对数的两倍，并在所有单元格上求和。渐近地，该统计量服从自由度为 $(r_X-1)(r_Y-1)$ 的 $\\chi^2$ 分布，其中 $r_X$ 和 $r_Y$ 是样本中出现的 $X$ 和 $Y$ 的类别数量。对于给定离散集 $Z$ 的条件检验，同样的逻辑适用于 $Z$ 的每个分层内部；总统计量是各分层统计量之和，总自由度是各分层自由度 $(r_{X|z}-1)(r_{Y|z}-1)$ 之和，使用的是每个分层中非空水平的数量。如果自由度为 $0$，则该统计量不提供拒绝独立性的信息，为了算法的目的，接受原假设（设置 $p$-值为 $1$）。\n\nPeter-Clark (PC) 算法使用条件独立性约束来发现有向无环图 (DAG) 的马尔可夫等价类，表示为一个完全部分有向无环图 (CPDAG)。步骤如下：\n- 骨架发现：在变量上初始化完全无向图。对于 $s=0,1,\\dots,k_{\\max}$，对于每个相邻对 $(X,Y)$，遍历 $X$ 的邻居（不包括 $Y$）中大小为 $s$ 的所有子集 $S$。如果在显著性水平 $\\alpha$ 下 $X \\perp Y \\mid S$，则移除边 $X-Y$ 并记录 $S$ 作为 $\\{X,Y\\}$ 的一个分离集。这操作化了全局马尔可夫属性：如果一条路径被 $S$ 阻断，则条件分布因子分解使得 $X$ 和 $Y$ 变得独立；反之，若不存在有限大小的分离集，则保留邻接关系。\n- V-结构定向：一个三元组 $X-Z-Y$，其中 $X$ 和 $Y$ 在骨架中不相邻但都与 $Z$ 相邻，这表明要么是一个碰撞体 $X \\to Z \\leftarrow Y$，要么是一个非碰撞体，其中 $Z$ 属于 $X$ 和 $Y$ 之间的一个分离集。如果 $Z$ 没有被记录在 $\\{X,Y\\}$ 的任何分离集中，那么为了满足观测到的边际分离 $X \\perp Y$ 而不以 $Z$ 为条件，那么 $Z$ 必须是一个碰撞体；否则，非碰撞体模式将意味着 $X$ 和 $Y$ 之間存在依赖关系。因此，定向为 $X \\to Z \\leftarrow Y$。\n- 定向传播 (Meek 法则)：如果存在 $A \\to B - C$ 且 $A$ 和 $C$ 不相邻，则将 $B - C$ 定向为 $B \\to C$。这避免了在没有分离集支持的情况下在 $B$ 处创建未屏蔽的碰撞体，并保持了无环性和等价类约束。重复应用此规则，直到无法推导出更多定向。\n\n独立性检验设计。我们直接从数据数组构建经验列联表。对于无条件检验，我们形成 $X$ 和 $Y$ 类别上的计数 $n_{ij}$，计算行总和与列总和，计算独立性假设下的期望计数 $E_{ij} = (n_{i\\cdot} n_{\\cdot j})/n$，并计算对数似然比统计量 $G^2 = 2\\sum_{i,j:\\, n_{ij}>0} n_{ij} \\log(n_{ij}/E_{ij})$。对于条件检验，我们遍历 $Z$ 的每个观测到的配置 $z$，形成限制在 $Z=z$ 的行上的 $X \\times Y$ 列联表，并对特定分层的 $G^2_z$ 和自由度 $(r_{X|z}-1)(r_{Y|z}-1)$ 求和。$p$-值为 $\\Pr(\\chi^2_{\\text{df}} \\ge G^2)$。\n\n算法在测试套件上的应用：\n- 案例 1：数据精确地编码了 $C = A \\oplus B$，并且 $D$ 是 $C$ 的一个含噪声子节点，概率 $p(D=C \\mid C)=0.8$。在骨架发现阶段：$A$ 和 $B$ 边际独立，因此 $A-B$ 边被移除，分离集 $S=\\varnothing$。对 $(A,C)$ 和 $(B,C)$ 保持邻接，因为存在直接依赖，不能被不包含端点的条件集所阻断。对 $(A,D)$ 和 $(B,D)$ 边际上是相关的，因为存在通过 $C$ 的路径，但在给定 $C$ 的条件下是条件独立的（根据 DAG 的局部马尔可夫属性以及 $D$ 仅依赖于 $C$ 的构造），因此边 $A-D$ 和 $B-D$ 被移除，分离集 $S=\\{C\\}$。对 $(C,D)$ 保持邻接。V-结构定向将 $A \\to C \\leftarrow B$ 定向，因为 $C$ 不在 $\\{A,B\\}$ 的分离集中。然后 Meek 法则使用 $A \\to C - D$ 且 $A$ 和 $D$ 不相邻的条件，将 $C - D$ 定向为 $C \\to D$ (使用 $B$ 也能得到类似结果)。CPDAG 有 $3$ 条定向边和 $0$ 条无向边。\n- 案例 2：在 $(A,B,C)$ 上的均匀乘积分布意味着所有对都是无条件独立的，因此骨架在 $s=0$ 时丢弃所有边。不存在定向。CPDAG 有 $0$ 条定向边和 $0$ 条无向边。\n- 案例 3：数据与因子分解 $p(A)p(B \\mid A)p(C \\mid B)$ 一致，每一步都有高保真复制但严格存在正变异性。边际上，由于因果联系或沿路径的诱导依赖，所有对都是相关的。然而，$A \\perp C \\mid B$ 从因子分解中精确成立，因此边 $A-C$ 被移除，分离集为 $\\{B\\}$。不存在未屏蔽的碰撞体，因为 $A$ 和 $C$ 被 $B$ 分离，且 $B$ 在 $\\{A,C\\}$ 的分离集中。没有碰撞体，Meek 法则不会强制进行定向；因此，$(A,B)$ 和 $(B,C)$ 在 CPDAG 中保持无向，对应于链 $A \\to B \\to C$ 和 $A \\leftarrow B \\leftarrow C$ 的马尔可夫等价类。CPDAG 有 $0$ 条定向边和 $2$ 条无向边。\n\n因此，当严格按照规定实现程序时，三个案例的输出是每个案例一个包含两个整数元素的列表。最终程序必须打印一行格式为 $[[o_1,u_1],[o_2,u_2],[o_3,u_3]]$ 的内容，汇总所有测试套件的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\nfrom itertools import combinations, product\n\ndef gtest_conditional_independence(data, x_col, y_col, z_cols):\n    \"\"\"\n    Compute conditional likelihood-ratio G^2 statistic for testing\n    X ⟂ Y | Z using empirical counts.\n\n    Parameters:\n        data: np.ndarray of shape (n_samples, n_vars), integer categories starting at 0\n        x_col, y_col: int, column indices for X and Y\n        z_cols: list of ints, column indices for conditioning set Z (possibly empty)\n\n    Returns:\n        stat: float, total G^2 statistic\n        df: int, total degrees of freedom\n        p_value: float, chi-squared survival function at stat with df\n    \"\"\"\n    x = data[:, x_col]\n    y = data[:, y_col]\n\n    # Determine the set of strata over Z\n    if len(z_cols) == 0:\n        strata = [()]\n        z_values = np.empty((data.shape[0], 0), dtype=int)\n    else:\n        z_values = data[:, z_cols]\n        # Unique rows for Z to iterate strata\n        # Convert rows to tuples\n        strata = list({tuple(row) for row in z_values})\n\n    total_stat = 0.0\n    total_df = 0\n\n    # Precompute possible levels for X and Y across the dataset\n    # Use observed levels to build tables\n    x_levels = np.unique(x)\n    y_levels = np.unique(y)\n    rx = len(x_levels)\n    ry = len(y_levels)\n    x_level_index = {val: idx for idx, val in enumerate(x_levels)}\n    y_level_index = {val: idx for idx, val in enumerate(y_levels)}\n\n    for z_val in strata:\n        if len(z_cols) == 0:\n            mask = np.ones(data.shape[0], dtype=bool)\n        else:\n            # Build mask for this stratum\n            z_arr = np.array(z_val, dtype=int)\n            mask = np.all(z_values == z_arr, axis=1)\n        # Extract stratum samples\n        x_s = x[mask]\n        y_s = y[mask]\n        n_s = x_s.shape[0]\n        if n_s == 0:\n            continue\n\n        # Build contingency table for this stratum\n        table = np.zeros((rx, ry), dtype=float)\n        # Fill counts\n        for xi, yi in zip(x_s, y_s):\n            ix = x_level_index[xi]\n            iy = y_level_index[yi]\n            table[ix, iy] += 1.0\n\n        # Compute row sums, col sums, total\n        row_sums = table.sum(axis=1)\n        col_sums = table.sum(axis=0)\n        total = table.sum()\n        if total == 0:\n            continue\n\n        # Degrees of freedom for this stratum = (r_x_pos - 1)*(r_y_pos - 1)\n        r_x_pos = int(np.sum(row_sums > 0))\n        r_y_pos = int(np.sum(col_sums > 0))\n        df_stratum = max(0, (r_x_pos - 1) * (r_y_pos - 1))\n        if df_stratum == 0:\n            # No contribution to statistic (degenerate stratum)\n            continue\n\n        # Expected counts under independence for this stratum\n        expected = np.outer(row_sums, col_sums) / total\n        # G-test contribution: 2 * sum_{i,j: n_ij > 0} n_ij * log(n_ij / E_ij)\n        # Avoid invalid log by masking\n        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n            positive_mask = table > 0\n            ratio = np.zeros_like(table)\n            ratio[positive_mask] = table[positive_mask] / expected[positive_mask]\n            # Terms with expected zero should not happen if df_stratum > 0\n            contrib = np.zeros_like(table)\n            # Only valid where expected > 0 and observed > 0\n            valid = positive_mask  (expected > 0)\n            contrib[valid] = table[valid] * np.log(ratio[valid])\n            g2 = 2.0 * np.sum(contrib)\n\n        total_stat += g2\n        total_df += df_stratum\n\n    if total_df == 0:\n        # Treat as independent (uninformative) for algorithm\n        p_value = 1.0\n    else:\n        p_value = chi2.sf(total_stat, df=total_df)\n    return total_stat, total_df, p_value\n\ndef pc_algorithm(data, alpha=0.001, max_k=2):\n    \"\"\"\n    Constraint-based PC algorithm to learn a CPDAG from discrete data.\n\n    Parameters:\n        data: np.ndarray of shape (n_samples, n_vars)\n        alpha: significance level for independence tests\n        max_k: maximum size of conditioning set\n\n    Returns:\n        undirected: np.ndarray bool matrix of undirected adjacencies (symmetric)\n        directed: np.ndarray bool matrix for directed edges (directed[i,j]=True if i->j)\n    \"\"\"\n    n_vars = data.shape[1]\n    # Initialize complete undirected graph\n    undirected = np.zeros((n_vars, n_vars), dtype=bool)\n    for i in range(n_vars):\n        for j in range(i + 1, n_vars):\n            undirected[i, j] = True\n            undirected[j, i] = True\n\n    # Separation sets: map pair (i,j) with i", "id": "3314528"}, {"introduction": "许多现代基因调控网络推断方法，特别是贝叶斯方法，会产生概率性的输出，例如每个潜在调控连接强度的后验分布。最后一步是将这些概率转化为一个具体的网络图，同时控制统计误差。在这个问题中，您将处理来自贝叶斯模型的模拟后验样本。您将学习使用 Savage-Dickey 密度比为每条边计算贝叶斯因子，然后应用贝叶斯错误发现率 (FDR) 控制程序来决定哪些边应被包含在最终的网络中。这项实践将使您掌握在贝叶斯框架下进行稳健假设检验和多重检验校正的关键技能，确保最终推断出的网络在统计上是可靠且可解释的 [@problem_id:3314570]。", "problem": "给定来自马尔可夫链蒙特卡洛（MCMC）程序的基因调控网络中边权重的后验样本。对于每个有向边，您需要量化支持“边权重非零”这一备择假设的证据，并使用贝叶斯决策规则来控制预期中的假发现比例。理论基础包括贝叶斯定理、贝叶斯因子的定义、适用于具有连续先验的嵌套模型的 Savage–Dickey 密度比，以及作为一种经过充分检验的非参数方法来估计连续密度的带有高斯核的核密度估计（KDE）。\n\n模型假设：对于每个边参数 $w$，您考虑两个模型：零模型 $\\mathcal{M}_0$（其中 $w = 0$）和备择模型 $\\mathcal{M}_1$（其中 $w \\sim \\mathcal{N}(0,\\tau^2)$，已知先验尺度参数 $\\tau  0$）。您假设 $w$ 的 MCMC 后验样本是在 $\\mathcal{M}_1$ 模型下使用此先验抽取的。先验模型概率为 $\\Pr(\\mathcal{M}_1) = p_1 \\in (0,1)$ 和 $\\Pr(\\mathcal{M}_0) = 1 - p_1$。\n\n通过 Savage–Dickey 方法计算贝叶斯因子：在具有正常（proper）且在 $w=0$ 处非零的连续先验的嵌套模型的正则性条件下，支持 $\\mathcal{M}_1$ 相对于 $\\mathcal{M}_0$ 的贝叶斯因子等于\n$$\n\\mathrm{BF}_{10} = \\frac{p(w=0 \\mid \\mathcal{M}_1)}{p(w=0 \\mid \\text{data}, \\mathcal{M}_1)},\n$$\n其中 $p(w=0 \\mid \\mathcal{M}_1)$ 是在 $w=0$ 处的先验密度，而 $p(w=0 \\mid \\text{data}, \\mathcal{M}_1)$ 是在 $\\mathcal{M}_1$ 下 $w=0$ 处的后验密度。对于先验 $w \\sim \\mathcal{N}(0,\\tau^2)$，在零点的先验密度为\n$$\np(w=0 \\mid \\mathcal{M}_1) = \\frac{1}{\\sqrt{2\\pi}\\,\\tau}.\n$$\n因为您只有在 $\\mathcal{M}_1$ 模型下的后验样本，所以您必须使用带有高斯核的 KDE 来非参数地估计 $p(w=0 \\mid \\text{data}, \\mathcal{M}_1)$。给定 $n$ 个后验样本 $\\{w_i\\}_{i=1}^n$，在 $w=0$ 处使用带宽 $h0$ 的 KDE 为\n$$\n\\widehat{p}(0) = \\frac{1}{n h} \\sum_{i=1}^n \\phi\\!\\left(\\frac{0 - w_i}{h}\\right),\n$$\n其中 $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2/2)$ 是标准正态密度。使用 Silverman 经验法则 $h = 1.06 \\,\\widehat{\\sigma}\\, n^{-1/5}$，其中 $\\widehat{\\sigma}$ 是定义的尺度，即 $\\min\\{\\text{样本标准差}, \\text{四分位距}/1.34\\}$。如果 $\\widehat{\\sigma}$ 在数值上为 $0$，则将 $h$ 设置为一个小的正值以避免除以 $0$。\n\n后验模型概率与局部假发现率：$\\mathcal{M}_1$ 相对于 $\\mathcal{M}_0$ 的后验优势比由下式给出\n$$\n\\frac{\\Pr(\\mathcal{M}_1 \\mid \\text{data})}{\\Pr(\\mathcal{M}_0 \\mid \\text{data})} = \\mathrm{BF}_{10} \\times \\frac{p_1}{1 - p_1}.\n$$\n将一条边的局部假发现率定义为\n$$\n\\ell = \\Pr(\\mathcal{M}_0 \\mid \\text{data}) = \\frac{1}{1 + \\mathrm{BF}_{10} \\times \\frac{p_1}{1 - p_1}}.\n$$\n贝叶斯假发现率 (FDR) 控制：为了在目标 FDR 水平 $q \\in (0,1)$ 下选择一组边，为每条边 $j$ 计算 $\\ell_j$，按 $\\ell_j$ 升序对边进行排序，并选择最大的 $k$，使得累积均值 $\\frac{1}{k}\\sum_{i=1}^k \\ell_{(i)} \\le q$，其中 $\\ell_{(1)} \\le \\ell_{(2)} \\le \\cdots$ 是排好序的局部假发现率。选择与前 $k$ 个有序值相对应的边。如果不存在满足不等式的 $k \\ge 1$，则返回空集。\n\n你的任务：实现一个程序，对于每个测试用例，从高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 中为每条边生成 MCMC 后验样本，以模拟在 $\\mathcal{M}_1$ 模型下的后验抽样。然后，对每条边，通过在零点使用 KDE 的 Savage–Dickey 比率计算 $\\mathrm{BF}_{10}$，将其转换为 $\\ell$，并应用贝叶斯 FDR 选择规则以输出所选边的索引。\n\n本问题中所有数学量均为无量纲；没有物理单位。如果概念上涉及任何百分比，请始终将其编码为小数（例如，使用 $0.1$ 而不是 $10\\%$）。\n\n测试套件：为保证可复现性，您必须使用指定的种子、样本量、先验和后验生成参数。对于每个测试用例，使用按指示设定种子的伪随机数生成器为每条边生成 $n$ 个样本。\n\n测试用例 1：\n- 种子 $= 17$\n- 边数 $= 5$\n- 每条边的样本数 $n = 5000$\n- 先验尺度 $\\tau = 1.0$\n- 先验包含概率 $p_1 = 0.2$\n- 目标 FDR $q = 0.1$\n- 边后验生成器 $(\\mu, \\sigma)$，按顺序：$(1.5, 0.25)$, $(-2.0, 0.25)$, $(0.0, 0.2)$, $(0.2, 0.2)$, $(-0.1, 0.2)$\n\n测试用例 2：\n- 种子 $= 23$\n- 边数 $= 6$\n- 每条边的样本数 $n = 6000$\n- 先验尺度 $\\tau = 2.0$\n- 先验包含概率 $p_1 = 0.5$\n- 目标 FDR $q = 0.2$\n- 边后验生成器 $(\\mu, \\sigma)$，按顺序：$(0.0, 0.4)$, $(0.6, 0.3)$, $(-0.6, 0.3)$, $(1.2, 0.35)$, $(0.3, 0.25)$, $(-1.0, 0.5)$\n\n测试用例 3：\n- 种子 $= 5$\n- 边数 $= 5$\n- 每条边的样本数 $n = 7000$\n- 先验尺度 $\\tau = 1.5$\n- 先验包含概率 $p_1 = 0.7$\n- 目标 FDR $q = 0.05$\n- 边后验生成器 $(\\mu, \\sigma)$，按顺序：$(0.0, 0.6)$, $(0.2, 0.6)$, $(-0.1, 0.5)$, $(0.3, 0.7)$, $(-0.2, 0.7)$\n\n最终输出格式：您的程序应生成一行，其中包含所有测试用例的结果，形式为方括号内的一个逗号分隔列表。每个测试用例的结果本身必须是一个包含所选边索引（从零开始）的列表，按严格递增顺序排列，并用方括号括起来。例如，一个有效的输出形状是 $[[i_1,i_2],[j_1],[]]$，不含任何空白字符。给定上述种子，输出必须是确定性的。", "solution": "该问题要求实现一个贝叶斯假设检验程序，以识别基因调控网络中显著的非零边权重，同时控制假发现率（FDR）。解决方案首先验证问题陈述，发现其在科学上是合理的、定义明确且客观的，然后构建一个程序来执行指定的计算。该方法的核心是为每条边计算贝叶斯因子，然后基于局部假发现率进行决策。\n\n对每个提供的测试用例执行整个算法。对于每个测试用例，我们首先使用给定的种子初始化一个伪随机数生成器以确保可复现性。然后，对于测试用例中定义的每条边，我们执行一系列计算来确定其局部假发现率 $\\ell$。\n\n**步骤 1：后验样本生成**\n对于每条边，我们通过从高斯分布 $\\mathcal{N}(\\mu, \\sigma^2)$ 中抽取 $n$ 个样本来模拟马尔可夫链蒙特卡洛（MCMC）过程的输出，其参数 $(\\mu, \\sigma)$ 是为该边指定的。这些样本代表了在备择模型 $\\mathcal{M}_1$ 下边权重 $w$ 的后验分布的抽样。\n\n**步骤 2：使用 Savage-Dickey 密度比计算贝叶斯因子**\n问题指定了零模型 $\\mathcal{M}_0: w=0$ 和备择模型 $\\mathcal{M}_1: w \\neq 0$ 之间的嵌套模型比较。贝叶斯因子 $\\mathrm{BF}_{10}$ 量化了支持 $\\mathcal{M}_1$ 相对于 $\\mathcal{M}_0$ 的证据。它使用 Savage-Dickey 密度比进行计算：\n$$\n\\mathrm{BF}_{10} = \\frac{p(w=0 \\mid \\mathcal{M}_1)}{p(w=0 \\mid \\text{data}, \\mathcal{M}_1)}\n$$\n该比率涉及两个部分：先验密度和后验密度，两者都在零假设的参数值 $w=0$ 处进行评估。\n\n**2a. 零点处的先验密度, $p(w=0 \\mid \\mathcal{M}_1)$**\n在 $\\mathcal{M}_1$ 下，边权重的先验被给定为高斯分布 $w \\sim \\mathcal{N}(0, \\tau^2)$。该分布在 $w=0$ 处的密度有一个解析表达式：\n$$\np(w=0 \\mid \\mathcal{M}_1) = \\frac{1}{\\sqrt{2\\pi}\\,\\tau}\n$$\n这个值是使用给定的先验尺度参数 $\\tau$ 计算的。\n\n**2b. 零点处的后验密度, $p(w=0 \\mid \\text{data}, \\mathcal{M}_1)$**\n由于我们只有来自后验分布的样本，我们必须非参数地估计其在 $w=0$ 处的密度。问题要求使用带有高斯核的核密度估计（KDE）。对于一组 $n$ 个后验样本 $\\{w_i\\}_{i=1}^n$，在 $w=0$ 处的 KDE 为：\n$$\n\\widehat{p}(0) = \\frac{1}{n h} \\sum_{i=1}^n \\phi\\left(\\frac{-w_i}{h}\\right)\n$$\n其中 $\\phi(u)$ 是标准正态概率密度函数，而 $h$ 是带宽。\n\n带宽 $h$ 使用 Silverman 经验法则确定：\n$$\nh = 1.06 \\,\\widehat{\\sigma}\\, n^{-1/5}\n$$\n在这里，$\\widehat{\\sigma}$ 是后验样本分布尺度的一个稳健估计，定义为样本标准差和四分位距（IQR）除以 1.34 两者中的较小者。因子 1.34 使得基于 IQR 的尺度估计与正态分布数据的标准差相当。\n\n**步骤 3：局部假发现率 (lFDR) 计算**\n贝叶斯因子被转换成一个更易于解释的后验概率。局部假发现率 $\\ell$ 是给定数据下零模型 $\\mathcal{M}_0$ 的后验概率：\n$$\n\\ell = \\Pr(\\mathcal{M}_0 \\mid \\text{data})\n$$\n利用后验优势比、先验优势比和贝叶斯因子之间的关系，我们得到以下表达式：\n$$\n\\ell = \\frac{1}{1 + \\mathrm{BF}_{10} \\times \\frac{\\Pr(\\mathcal{M}_1)}{\\Pr(\\mathcal{M}_0)}} = \\frac{1}{1 + \\mathrm{BF}_{10} \\times \\frac{p_1}{1 - p_1}}\n$$\n其中 $p_1$ 是一条边被包含在网络中的先验概率（$\\Pr(\\mathcal{M}_1)$）。一个小的 $\\ell$ 值表示有强有力的证据反对零假设（即，支持边的存在）。\n\n**步骤 4：贝叶斯假发现率 (FDR) 控制**\n在为每条边 $j=1, \\dots, m$ 计算出 $\\ell_j$ 值后，我们应用一个程序来选择一组边，同时将预期的 FDR 控制在目标水平 $q$。步骤如下：\n1. 根据它们的 $\\ell_j$ 值对边进行升序排序：$\\ell_{(1)} \\le \\ell_{(2)} \\le \\cdots \\le \\ell_{(m)}$。\n2. 找到最大的整数 $k \\ge 1$，使得 $k$ 个最小 $\\ell$ 值的平均值不超过目标 FDR $q$：\n$$\n\\frac{1}{k} \\sum_{i=1}^k \\ell_{(i)} \\le q\n$$\n3. 如果存在这样的 $k$，则选择的边是与前 $k$ 个排序的 $\\ell$ 值相对应的边：$\\{\\ell_{(1)}, \\dots, \\ell_{(k)}\\}$。如果不存在这样的 $k \\ge 1$，则不选择任何边。\n\n每个测试用例的最终输出是一个列表，其中包含所选边的从 0 开始的索引，并按升序排列。然后，将所有测试用例的结果汇总成一个指定的格式化字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves all test cases for Bayesian FDR control in a gene regulatory network.\n    \"\"\"\n    test_cases = [\n        (17, 5, 5000, 1.0, 0.2, 0.1, [(1.5, 0.25), (-2.0, 0.25), (0.0, 0.2), (0.2, 0.2), (-0.1, 0.2)]),\n        (23, 6, 6000, 2.0, 0.5, 0.2, [(0.0, 0.4), (0.6, 0.3), (-0.6, 0.3), (1.2, 0.35), (0.3, 0.25), (-1.0, 0.5)]),\n        (5, 5, 7000, 1.5, 0.7, 0.05, [(0.0, 0.6), (0.2, 0.6), (-0.1, 0.5), (0.3, 0.7), (-0.2, 0.7)]),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, num_edges, n, tau, p1, q, posterior_params = case\n        \n        rng = np.random.default_rng(seed)\n        \n        l_values = []\n        for j in range(num_edges):\n            mu, sigma = posterior_params[j]\n            samples = rng.normal(loc=mu, scale=sigma, size=n)\n            \n            # --- Step 1: Calculate bandwidth h for KDE ---\n            std_dev = np.std(samples, ddof=1)\n            q75, q25 = np.percentile(samples, [75, 25])\n            iqr = q75 - q25\n            \n            sigma_hat = min(std_dev, iqr / 1.34)\n            \n            h = 1.06 * sigma_hat * (n ** -0.2)\n            if h == 0.0:\n                h = 1e-8 # Failsafe for numerically zero bandwidth\n\n            # --- Step 2: Estimate posterior density at w=0 using KDE ---\n            # phi(u) = (1/sqrt(2pi)) * exp(-u^2/2)\n            # Due to symmetry of Gaussian kernel, phi(-u) = phi(u)\n            phi_terms = (1.0 / math.sqrt(2 * math.pi)) * np.exp(-0.5 * (samples / h) ** 2)\n            p_hat_0 = np.mean(phi_terms) / h\n            \n            # --- Step 3: Calculate Bayes Factor and local FDR ---\n            l_val = 1.0 # Default to null if BF cannot be computed\n            if p_hat_0 > 0:\n                p_prior_0 = 1.0 / (math.sqrt(2 * math.pi) * tau)\n                bf_10 = p_prior_0 / p_hat_0\n                prior_odds = p1 / (1.0 - p1)\n                l_val = 1.0 / (1.0 + bf_10 * prior_odds)\n            \n            l_values.append(l_val)\n            \n        # --- Step 4: Bayesian FDR control ---\n        ell_with_indices = sorted([(l_values[j], j) for j in range(num_edges)])\n        \n        sorted_l = np.array([item[0] for item in ell_with_indices])\n        \n        k_max = 0\n        cumulative_mean_l = np.cumsum(sorted_l) / np.arange(1, num_edges + 1)\n        \n        k_satisfying_indices = np.where(cumulative_mean_l = q)[0]\n        \n        if len(k_satisfying_indices) > 0:\n            k_max = k_satisfying_indices[-1] + 1\n            \n        selected_edges = []\n        if k_max > 0:\n            original_indices = [item[1] for item in ell_with_indices[:k_max]]\n            selected_edges = sorted(original_indices)\n            \n        results.append(selected_edges)\n\n    # --- Final Output Formatting ---\n    print(f\"[{','.join(str(r).replace(' ', '') for r in results)}]\")\n\nsolve()\n```", "id": "3314570"}]}