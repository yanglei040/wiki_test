{"hands_on_practices": [{"introduction": "在高维数据分析中，多重共线性是一个常见挑战，它会使普通最小二乘估计不稳定。为了直观地理解岭回归如何缓解这一问题，我们可以借助奇异值分解（SVD）这一强大的工具。这个练习将引导您从SVD的视角出发，揭示岭回归如何沿着数据的主要变化方向（即主成分方向）对系数进行差异化收缩，从而稳定估计结果。[@problem_id:3345370]", "problem": "在计算系统生物学的一个转录调控网络推断任务中，一个基因表达向量 $y \\in \\mathbb{R}^{n}$ 被建模为设计矩阵 $X \\in \\mathbb{R}^{n \\times 2}$ 中收集的两个转录因子 (TF) 活性谱的线性组合。由于转录因子谱存在近似共线性，因此使用岭回归来估计转录因子效应 $\\beta \\in \\mathbb{R}^{2}$，方法是最小化惩罚最小二乘目标函数\n$$\n\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}.\n$$\n设 $X$ 的奇异值分解 (SVD) 为 $X = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times 2}$ 和 $V \\in \\mathbb{R}^{2 \\times 2}$ 是标准正交矩阵，且 $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}) \\in \\mathbb{R}^{2 \\times 2}$，$\\sigma_{1} \\ge \\sigma_{2}  0$。从岭回归的定义和线性代数恒等式出发，推导岭估计量 $\\hat{\\beta}^{\\mathrm{ridge}}$ 的一个以 $U$、$\\Sigma$、$V$ 和 $y$ 表示的闭式表达式，并用它来分析收缩在转录因子共线性方向上是如何作用的。\n\n假设 $n = 100$，$\\lambda = 1$，且 SVD 分量满足\n$$\n\\Sigma = \\operatorname{diag}(10, 1), \\quad V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}, \\quad U^{\\top}y = \\begin{pmatrix}10 \\\\ 2\\end{pmatrix}.\n$$\n计算第一个转录因子系数，即 $\\hat{\\beta}^{\\mathrm{ridge}}$ 的第一个分量，并以精确形式（不进行四舍五入）给出你的答案。", "solution": "所述问题具有科学依据、是适定且客观的。它为统计学习理论中的一个标准问题（岭回归）应用于相关领域（计算系统生物学）提供了一套完整且一致的给定条件。该问题是有效的。\n\n岭回归估计量 $\\hat{\\beta}^{\\mathrm{ridge}}$ 是使目标函数最小化的向量 $\\beta$：\n$$L(\\beta) = \\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2}$$\n为了找到最小值，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零。目标函数可以写成：\n$$L(\\beta) = \\frac{1}{n}(y - X\\beta)^{\\top}(y - X\\beta) + \\lambda \\beta^{\\top}\\beta = \\frac{1}{n}(y^{\\top}y - 2y^{\\top}X\\beta + \\beta^{\\top}X^{\\top}X\\beta) + \\lambda \\beta^{\\top}\\beta$$\n关于 $\\beta$ 的梯度是：\n$$\\nabla_{\\beta} L(\\beta) = \\frac{1}{n}(-2X^{\\top}y + 2X^{\\top}X\\beta) + 2\\lambda\\beta$$\n将梯度设为零以求得最优的 $\\hat{\\beta}^{\\mathrm{ridge}}$：\n$$\\frac{1}{n}(2X^{\\top}X\\hat{\\beta}^{\\mathrm{ridge}} - 2X^{\\top}y) + 2\\lambda\\hat{\\beta}^{\\mathrm{ridge}} = 0$$\n两边乘以 $\\frac{n}{2}$：\n$$(X^{\\top}X)\\hat{\\beta}^{\\mathrm{ridge}} - X^{\\top}y + n\\lambda\\hat{\\beta}^{\\mathrm{ridge}} = 0$$\n整理各项以求解 $\\hat{\\beta}^{\\mathrm{ridge}}$：\n$$(X^{\\top}X + n\\lambda I)\\hat{\\beta}^{\\mathrm{ridge}} = X^{\\top}y$$\n这就得到了岭回归解的标准形式：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = (X^{\\top}X + n\\lambda I)^{-1} X^{\\top}y$$\n现在我们将奇异值分解 (SVD) $X = U\\Sigma V^{\\top}$ 代入此表达式。首先，我们计算分量 $X^{\\top}X$ 和 $X^{\\top}y$：\n$$X^{\\top}X = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top}$$\n由于 $U$ 的列是标准正交的，因此 $U^{\\top}U = I_{2}$，即 $2 \\times 2$ 的单位矩阵。此外，$\\Sigma$ 是对角矩阵，所以 $\\Sigma^{\\top} = \\Sigma$。\n$$X^{\\top}X = V\\Sigma I_{2} \\Sigma V^{\\top} = V\\Sigma^{2}V^{\\top}$$\n对于 $X^{\\top}y$：\n$$X^{\\top}y = (U\\Sigma V^{\\top})^{\\top}y = V\\Sigma^{\\top}U^{\\top}y = V\\Sigma U^{\\top}y$$\n将这些代回 $\\hat{\\beta}^{\\mathrm{ridge}}$ 的表达式中：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = (V\\Sigma^{2}V^{\\top} + n\\lambda I)^{-1} (V\\Sigma U^{\\top}y)$$\n我们可以将单位矩阵 $I$ 重写为 $I = VV^{\\top}$，因为 $V$ 是一个标准正交矩阵 ($V^{-1} = V^{\\top}$)。\n$$(V\\Sigma^{2}V^{\\top} + n\\lambda VV^{\\top})^{-1} = (V(\\Sigma^{2} + n\\lambda I)V^{\\top})^{-1}$$\n使用性质 $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$，我们得到：\n$$(V(\\Sigma^{2} + n\\lambda I)V^{\\top})^{-1} = (V^{\\top})^{-1}(\\Sigma^{2} + n\\lambda I)^{-1}V^{-1} = V(\\Sigma^{2} + n\\lambda I)^{-1}V^{\\top}$$\n因此，$\\hat{\\beta}^{\\mathrm{ridge}}$ 的表达式变为：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = V(\\Sigma^{2} + n\\lambda I)^{-1}V^{\\top} V\\Sigma U^{\\top}y$$\n由于 $V^{\\top}V = I_{2}$：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = V(\\Sigma^{2} + n\\lambda I)^{-1}\\Sigma U^{\\top}y$$\n这就是用 $U$、$\\Sigma$、$V$ 和 $y$ 表示的岭估计量的闭式表达式。\n\n为了分析收缩是如何作用的，让我们考虑普通最小二乘 (OLS) 估计量 $\\hat{\\beta}^{\\mathrm{OLS}}$ 的解，它对应于 $\\lambda=0$ 的情况（暂时忽略 $1/n$ 的缩放，因为 OLS 是通过最小化 $\\|y-X\\beta\\|_2^2$ 来定义的）。OLS 估计量为 $\\hat{\\beta}^{\\mathrm{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y = V\\Sigma^{-1}U^{\\top}y$。我们为参数定义一个旋转坐标系，$\\alpha = V^{\\top}\\beta$。那么 $\\hat{\\alpha}^{\\mathrm{OLS}} = V^{\\top}\\hat{\\beta}^{\\mathrm{OLS}} = \\Sigma^{-1}U^{\\top}y$。对于岭估计量，$\\hat{\\alpha}^{\\mathrm{ridge}} = V^{\\top}\\hat{\\beta}^{\\mathrm{ridge}} = (\\Sigma^{2} + n\\lambda I)^{-1}\\Sigma U^{\\top}y$。\n我们按分量来写。$\\hat{\\alpha}^{\\mathrm{OLS}}$ 的第 $j$ 个分量是 $(\\hat{\\alpha}^{\\mathrm{OLS}})_{j} = \\frac{(U^{\\top}y)_j}{\\sigma_j}$。$\\hat{\\alpha}^{\\mathrm{ridge}}$ 的第 $j$ 个分量是 $(\\hat{\\alpha}^{\\mathrm{ridge}})_{j} = \\frac{\\sigma_j}{\\sigma_j^2 + n\\lambda}(U^{\\top}y)_j$。\n在这个旋转坐标系中，我们可以用 OLS 解来表示岭回归解：\n$$(\\hat{\\alpha}^{\\mathrm{ridge}})_{j} = \\frac{\\sigma_j^2}{\\sigma_j^2 + n\\lambda} \\left(\\frac{(U^{\\top}y)_j}{\\sigma_j}\\right) = \\frac{\\sigma_j^2}{\\sigma_j^2 + n\\lambda} (\\hat{\\alpha}^{\\mathrm{OLS}})_{j}$$\n收缩因子是 $s_j = \\frac{\\sigma_j^2}{\\sigma_j^2 + n\\lambda}$。由于 $\\sigma_1 \\ge \\sigma_2  0$，并且函数 $f(x) = \\frac{x}{x + c}$ (对于 $c0$) 在 $x0$ 时是递增的，所以我们有 $s_1 \\ge s_2$。这表明，与较小奇异值 $\\sigma_2$ 对应的系数会更强烈地向零收缩。$V$ 的列所定义的方向是预测变量的主成分方向。一个小的奇异值 $\\sigma_j$ 对应于预测变量空间中方差较低的方向，即（近似）共线性的方向。因此，岭回归沿着共线性较高的方向应用更大的收缩，从而稳定估计值。\n\n现在，我们使用给定的值计算第一个转录因子系数：\n$n = 100$, $\\lambda = 1$, $\\Sigma = \\operatorname{diag}(10, 1)$, $V = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix}$, 且 $U^{\\top}y = \\begin{pmatrix}10 \\\\ 2\\end{pmatrix}$。\n\n首先，计算项 $n\\lambda$：\n$$n\\lambda = 100 \\times 1 = 100$$\n接下来，计算 $\\Sigma^2$：\n$$\\Sigma^{2} = \\operatorname{diag}(10^{2}, 1^{2}) = \\begin{pmatrix}100  0 \\\\ 0  1\\end{pmatrix}$$\n现在，计算矩阵 $(\\Sigma^{2} + n\\lambda I)$：\n$$\\Sigma^{2} + n\\lambda I = \\begin{pmatrix}100  0 \\\\ 0  1\\end{pmatrix} + 100\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix} = \\begin{pmatrix}200  0 \\\\ 0  101\\end{pmatrix}$$\n其逆矩阵是：\n$$(\\Sigma^{2} + n\\lambda I)^{-1} = \\begin{pmatrix}\\frac{1}{200}  0 \\\\ 0  \\frac{1}{101}\\end{pmatrix}$$\n接下来，我们计算向量 $\\Sigma U^{\\top}y$：\n$$\\Sigma U^{\\top}y = \\begin{pmatrix}10  0 \\\\ 0  1\\end{pmatrix} \\begin{pmatrix}10 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}10 \\times 10 \\\\ 1 \\times 2\\end{pmatrix} = \\begin{pmatrix}100 \\\\ 2\\end{pmatrix}$$\n现在，我们可以计算中间向量 $z = (\\Sigma^{2} + n\\lambda I)^{-1}\\Sigma U^{\\top}y$：\n$$z = \\begin{pmatrix}\\frac{1}{200}  0 \\\\ 0  \\frac{1}{101}\\end{pmatrix} \\begin{pmatrix}100 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}\\frac{100}{200} \\\\ \\frac{2}{101}\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{2}{101}\\end{pmatrix}$$\n最后，我们计算 $\\hat{\\beta}^{\\mathrm{ridge}} = Vz$：\n$$\\hat{\\beta}^{\\mathrm{ridge}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1  1 \\\\ 1  -1\\end{pmatrix} \\begin{pmatrix}\\frac{1}{2} \\\\ \\frac{2}{101}\\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\\frac{1}{2} + \\frac{2}{101} \\\\ \\frac{1}{2} - \\frac{2}{101}\\end{pmatrix}$$\n题目要求我们计算第一个转录因子系数，也就是该向量的第一个分量：\n$$(\\hat{\\beta}^{\\mathrm{ridge}})_1 = \\frac{1}{\\sqrt{2}}\\left(\\frac{1}{2} + \\frac{2}{101}\\right)$$\n我们为括号内的和找到一个公分母：\n$$\\frac{1}{2} + \\frac{2}{101} = \\frac{101}{202} + \\frac{4}{202} = \\frac{105}{202}$$\n所以，第一个系数是：\n$$(\\hat{\\beta}^{\\mathrm{ridge}})_1 = \\frac{1}{\\sqrt{2}}\\frac{105}{202} = \\frac{105}{202\\sqrt{2}}$$\n将分母有理化得到最终的精确表达式：\n$$(\\hat{\\beta}^{\\mathrm{ridge}})_1 = \\frac{105\\sqrt{2}}{202 \\times 2} = \\frac{105\\sqrt{2}}{404}$$\n数字 $105 = 3 \\times 5 \\times 7$ 和 $404 = 4 \\times 101$ 没有公因数，所以这个分数是最简形式。", "answer": "$$\\boxed{\\frac{105\\sqrt{2}}{404}}$$", "id": "3345370"}, {"introduction": "与岭回归不同，LASSO（最小绝对收缩和选择算子）最著称的特性是它能够将某些系数精确地压缩至零，从而实现变量选择。这一“稀疏性”并非偶然，而是其优化问题内在结构的直接结果。通过这个练习，您将深入探索LASSO的凸对偶问题，从理论上理解导致稀疏解的约束条件，并学会如何通过计算原始-对偶间隙来验证解的最优性。[@problem_id:3345330]", "problem": "考虑一个计算系统生物学背景，其中单个靶基因的表达被建模为两个白化转录因子活性特征的线性组合。令 $X \\in \\mathbb{R}^{n \\times p}$ 表示特征的设计矩阵，$y \\in \\mathbb{R}^{n}$ 表示测得的基因表达。在此类设置中，一种高维推断的标准方法是使用最小绝对收缩和选择算子 (LASSO)，它求解以下优化问题\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中 $\\lambda  0$ 控制转录因子系数向量 $ \\beta $ 的稀疏性。该公式的对偶问题可以通过引入一个显式的残差变量，并应用凸分析中的 Fenchel-Legendre 共轭和拉格朗日对偶性来获得。从凸优化的基本原理（拉格朗日构造，对原始变量取下确界以形成对偶函数，以及从凸共轭中出现的约束）出发，推导 LASSO 的对偶问题，并证明它对对偶变量 $ u \\in \\mathbb{R}^{n} $ 施加了约束 $ \\|X^{\\top} u\\|_{\\infty} \\le \\lambda $。\n\n然后，特化到一个生物学上合理的白化设计，其中特征在样本空间中是标准正交的，即 $ X = I_{2} $ （$ 2 \\times 2 $ 单位矩阵），并给定 $ y = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} $ 和 $ \\lambda = 1 $。使用 Karush-Kuhn-Tucker 条件，通过残差和次梯度关系连接原始解和对偶解，计算原始最优化子 $ \\beta^{\\star} $、对偶最优化子 $ u^{\\star} $，并使用原始-对偶间隙来证明此实例的最优性。报告原始-对偶间隙 $\\Delta := f(\\beta^{\\star}) - g(u^{\\star})$ 的数值，其中 $ f $ 是原始目标函数，$ g $ 是对偶目标函数。无需四舍五入，也不涉及物理单位。最终报告的量必须是单个实数。", "solution": "### 第一部分：LASSO 对偶问题的推导\n\n原始 LASSO 问题表述为：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} f(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right)\n$$\n为了推导对偶问题，我们首先通过引入一个显式的残差变量 $r \\in \\mathbb{R}^{n}$ 来重构原始问题。这使我们能够将二次项与包含 $X$ 的项分开。问题变成一个等价的约束优化问题：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}, r \\in \\mathbb{R}^{n}} \\frac{1}{2}\\|r\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad X\\beta + r = y\n$$\n我们为等式约束 $X\\beta + r - y = 0$ 引入一个拉格朗日乘子向量 $u \\in \\mathbb{R}^{n}$。拉格朗日函数 $L$ 是原始变量 $\\beta, r$ 和对偶变量 $u$ 的函数：\n$$\nL(\\beta, r, u) = \\frac{1}{2}\\|r\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} + u^{\\top}(X\\beta + r - y)\n$$\n拉格朗日对偶函数 $g(u)$ 定义为拉格朗日函数关于原始变量的下确界：\n$$\ng(u) = \\inf_{\\beta \\in \\mathbb{R}^{p}, r \\in \\mathbb{R}^{n}} L(\\beta, r, u)\n$$\n我们可以重排拉格朗日函数中的项以将变量分组：\n$$\ng(u) = -u^{\\top}y + \\inf_{r \\in \\mathbb{R}^{n}} \\left( \\frac{1}{2}\\|r\\|_{2}^{2} + u^{\\top}r \\right) + \\inf_{\\beta \\in \\mathbb{R}^{p}} \\left( \\lambda \\|\\beta\\|_{1} + u^{\\top}X\\beta \\right)\n$$\n这两个下确界可以分别计算。它们对应于函数 $\\frac{1}{2}\\|r\\|_{2}^{2}$ 和 $\\lambda\\|\\beta\\|_{1}$ 的凸共轭的负值。\n\n对于涉及 $r$ 的项，函数 $h_1(r) = \\frac{1}{2}r^{\\top}r + u^{\\top}r$ 是一个凸二次函数。通过将其关于 $r$ 的梯度设为零来找到其最小值：\n$$\n\\nabla_r h_1(r) = r + u = 0 \\implies r = -u\n$$\n将此代回，最小值为：\n$$\n\\inf_{r} \\left( \\frac{1}{2}\\|r\\|_{2}^{2} + u^{\\top}r \\right) = \\frac{1}{2}\\|-u\\|_{2}^{2} + u^{\\top}(-u) = \\frac{1}{2}\\|u\\|_{2}^{2} - \\|u\\|_{2}^{2} = -\\frac{1}{2}\\|u\\|_{2}^{2}\n$$\n对于涉及 $\\beta$ 的项，令 $z = X^{\\top}u$。表达式为 $\\inf_{\\beta} (\\lambda \\|\\beta\\|_{1} + z^{\\top}\\beta)$。这可以按分量进行分析：\n$$\n\\inf_{\\beta} \\left( \\lambda \\sum_{j=1}^{p} |\\beta_j| + \\sum_{j=1}^{p} z_j \\beta_j \\right) = \\sum_{j=1}^{p} \\inf_{\\beta_j} (\\lambda|\\beta_j| + z_j\\beta_j)\n$$\n对于每个分量 $j$，如果 $|z_j|  \\lambda$，则可以选择与 $z_j$ 符号相反的 $\\beta_j$ 并使其大小趋于无穷，这使得表达式 $\\lambda|\\beta_j| + z_j\\beta_j$ 趋于 $-\\infty$。如果 $|z_j| \\le \\lambda$，则 $\\lambda|\\beta_j| + z_j\\beta_j \\ge \\lambda|\\beta_j| - |z_j||\\beta_j| = (\\lambda-|z_j|)|\\beta_j| \\ge 0$。最小值为 0，在 $\\beta_j=0$ 时达到。因此，为使对所有 $\\beta$ 的下确界是有限的（且等于 0），我们要求对所有 $j$ 都有 $|(X^{\\top}u)_j| \\le \\lambda$。这正是约束 $\\|X^{\\top}u\\|_{\\infty} \\le \\lambda$。\n\n综合这些结果，对偶函数为：\n$$\ng(u) = \\begin{cases} -\\frac{1}{2}\\|u\\|_{2}^{2} - y^{\\top}u  \\text{if } \\|X^{\\top}u\\|_{\\infty} \\le \\lambda \\\\ -\\infty  \\text{otherwise} \\end{cases}\n$$\n对偶问题是最大化 $g(u)$。这等价于：\n$$\n\\max_{u \\in \\mathbb{R}^{n}} \\; \\left( -\\frac{1}{2}\\|u\\|_{2}^{2} - y^{\\top}u \\right) \\quad \\text{subject to} \\quad \\|X^{\\top}u\\|_{\\infty} \\le \\lambda\n$$\n这就完成了推导，并表明对偶问题对对偶变量 $u$ 施加了所要求的约束。\n\n### 第二部分：特定实例的求解\n\n我们得到特定实例，其中 $X = I_2$ （$2 \\times 2$ 单位矩阵），这意味着 $n=p=2$。数据为 $y = \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix}$ 和 $ \\lambda = 1$。\n\n**原始解**\n当 $X=I_2$ 时，原始目标函数为：\n$$\nf(\\beta) = \\frac{1}{2} \\|y - \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} = \\frac{1}{2}\\sum_{j=1}^{2}(y_j - \\beta_j)^2 + \\lambda \\sum_{j=1}^{2}|\\beta_j|\n$$\n该问题是可分的，意味着我们可以通过独立地最小化每个分量的目标函数来找到最优的 $\\beta_j^{\\star}$：$\\min_{\\beta_j} \\frac{1}{2}(y_j - \\beta_j)^2 + \\lambda|\\beta_j|$。这个一维问题的解是软阈值算子：\n$$\n\\beta_j^{\\star} = S_{\\lambda}(y_j) = \\text{sign}(y_j)\\max(|y_j|-\\lambda, 0)\n$$\n对 $j=1$：$y_1=3, \\lambda=1 \\implies \\beta_1^{\\star} = \\text{sign}(3)\\max(3-1, 0) = 1 \\cdot 2 = 2$。\n对 $j=2$：$y_2=-0.5, \\lambda=1 \\implies \\beta_2^{\\star} = \\text{sign}(-0.5)\\max(0.5-1, 0) = -1 \\cdot 0 = 0$。\n最优原始解是 $\\beta^{\\star} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$。\n\n**对偶解**\n根据 Karush-Kuhn-Tucker (KKT) 条件，强对偶性成立，并且最优原始变量和对偶变量是相互关联的。具体来说，拉格朗日函数关于 $r$ 的平稳性条件是 $\\nabla_r L = r + u = 0$，这意味着 $r^{\\star} = -u^{\\star}$。原始可行性约束 $X\\beta+r=y$ 意味着 $r^{\\star} = y - X\\beta^{\\star}$。结合这些，我们得到最优对偶变量：\n$$\nu^{\\star} = -r^{\\star} = -(y - X\\beta^{\\star}) = X\\beta^{\\star} - y\n$$\n代入此实例的特定值 ($X=I_2$):\n$$\nu^{\\star} = I_2 \\beta^{\\star} - y = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix}\n$$\n我们可以验证这个 $u^{\\star}$ 是对偶可行的：$\\|X^{\\top}u^{\\star}\\|_{\\infty} = \\|I_2^{\\top}u^{\\star}\\|_{\\infty} = \\|u^{\\star}\\|_{\\infty} = \\max(|-1|, |0.5|) = 1$。因为 $\\lambda=1$，约束 $\\|X^{\\top}u^{\\star}\\|_{\\infty} \\le \\lambda$ 被满足 ($1 \\le 1$)。\n\n**原始-对偶间隙计算**\n最后一步是计算原始-对偶间隙 $\\Delta = f(\\beta^{\\star}) - g(u^{\\star})$，它可作为最优性的证明。\n\n在 $\\beta^{\\star}$ 处的原始目标函数值：\n残差为 $r^{\\star} = y - X\\beta^{\\star} = y - \\beta^{\\star} = \\begin{pmatrix} 1 \\\\ -0.5 \\end{pmatrix}$。\n解的 L1 范数为 $\\|\\beta^{\\star}\\|_{1} = |2| + |0| = 2$。\n$$\nf(\\beta^{\\star}) = \\frac{1}{2}\\|r^{\\star}\\|_{2}^{2} + \\lambda\\|\\beta^{\\star}\\|_{1} = \\frac{1}{2}(1^2 + (-0.5)^2) + 1 \\cdot (2) = \\frac{1}{2}(1.25) + 2 = 0.625 + 2 = 2.625\n$$\n\n在 $u^{\\star}$ 处的对偶目标函数值：\n$$\ng(u^{\\star}) = -\\frac{1}{2}\\|u^{\\star}\\|_{2}^{2} - y^{\\top}u^{\\star}\n$$\n对偶解的 L2 范数的平方为 $\\|u^{\\star}\\|_{2}^{2} = (-1)^2 + (0.5)^2 = 1 + 0.25 = 1.25$。\n内积为 $y^{\\top}u^{\\star} = \\begin{pmatrix} 3  -0.5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0.5 \\end{pmatrix} = (3)(-1) + (-0.5)(0.5) = -3 - 0.25 = -3.25$。\n$$\ng(u^{\\star}) = -\\frac{1}{2}(1.25) - (-3.25) = -0.625 + 3.25 = 2.625\n$$\n\n最后，原始-对偶间隙为：\n$$\n\\Delta = f(\\beta^{\\star}) - g(u^{\\star}) = 2.625 - 2.625 = 0\n$$\n零间隙证实了计算出的原始解和对偶解是最优的。", "answer": "$$\n\\boxed{0}\n$$", "id": "3345330"}, {"introduction": "理论模型的强大功能最终需要在实践中得到体现，而对于正则化方法而言，关键一步便是选择合适的正则化参数 $\\lambda$。这个参数决定了模型的复杂度和泛化能力之间的平衡。本练习将理论付诸实践，要求您通过编程实现并比较两种主流的模型选择技术——K折交叉验证（CV）和广义交叉验证（GCV），来为一个具有特定谱结构的岭回归模型寻找最佳的 $\\lambda$ 值。[@problem_id:3345318]", "problem": "给定一个受基因表达分析启发的高维线性模型，其中设计矩阵具有受控的特征谱，用以模拟相关的表达程序。设 $n$ 表示样本数量，$p$ 表示特征（基因）数量，响应向量为 $y \\in \\mathbb{R}^{n}$。考虑岭回归，它通过最小化惩罚经验风险来估计系数向量 $\\hat{\\beta}_{\\lambda}$\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{2}^{2},\n$$\n其中惩罚参数 $\\lambda \\ge 0$。设 $X$ 的奇异值分解 (SVD) 为 $X = U \\operatorname{diag}(d) V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times r}$，$V \\in \\mathbb{R}^{p \\times r}$，且 $r = \\min(n,p)$。将 $X^{\\top}X/n$ 的特征谱定义为 $\\gamma_{i} = d_{i}^{2}/n$（对于 $i \\in \\{1,\\dots,r\\}$）以及 $\\gamma_{i} = 0$（对于 $i \\in \\{r+1,\\dots,p\\}$）。\n\n您将使用两种方法为岭惩罚实现模型选择：广义交叉验证 (GCV) 和 K 折交叉验证 (CV)，然后将所选的惩罚与特征谱 $\\{\\gamma_{i}\\}$ 联系起来。\n\n用作基础出发点的定义：\n- 广义交叉验证 (GCV)：对于一个预测为 $\\hat{y}_{\\lambda} = H_{\\lambda} y$ 的线性平滑器，其中 $H_{\\lambda}$ 是帽子矩阵，GCV 分数为\n$$\n\\mathrm{GCV}(\\lambda) = \\frac{n \\, \\lVert y - \\hat{y}_{\\lambda} \\rVert_{2}^{2}}{\\big(n - \\mathrm{tr}(H_{\\lambda})\\big)^{2}}.\n$$\n- 对于使用上述缩放的岭回归，帽子矩阵可以在 SVD 基中表示为 $H_{\\lambda} = U \\operatorname{diag}\\!\\left(\\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}\\right) U^{\\top}$，这意味着有效自由度 (DoF) 为 $\\mathrm{df}(\\lambda) = \\mathrm{tr}(H_{\\lambda}) = \\sum_{i=1}^{r} \\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}$。\n- K 折交叉验证 (CV)：将索引集 $\\{1,\\dots,n\\}$ 划分为 $K$ 个不相交的折。对于每个 $\\lambda$ 和每个折，在 $K-1$ 个训练折上训练岭回归，并在留出的折上计算均方预测误差；然后在所有折上取平均以获得 $\\mathrm{CV}_{K}(\\lambda)$。\n\n数据生成假设：响应由 $y = X\\beta^{\\star} + \\varepsilon$ 生成，其中噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$，以及一个稀疏的真实系数向量 $\\beta^{\\star} \\in \\mathbb{R}^{p}$，它有 $s$ 个非零项，每个项的幅值为 $b  0$。\n\n具有指定特征谱的设计矩阵的构造：对于每种情况，构造具有 SVD $X = U \\operatorname{diag}(d) V^{\\top}$ 的矩阵 $X$，使得 $\\gamma_{i} = d_{i}^{2}/n$ 与指定的谱相匹配（对于 $i \\in \\{1,\\dots,r\\}$，其中 $r=\\min(n,p)$）。使用从高斯随机矩阵的固定种子 QR 分解派生出的正交矩阵 $U$ 和 $V$ 来确保可复现性。通过 $d_{i} = \\sqrt{n \\gamma_{i}}$ 缩放非零奇异值。\n\n角度单位不适用。无需物理单位。\n\n程序要求：\n- 实现岭预测器 $\\hat{y}_{\\lambda}$，并使用 SVD 基中的线性平滑器公式计算 $\\mathrm{GCV}(\\lambda)$，以避免显式地构建帽子矩阵。\n- 实现 K 折交叉验证，以在对数间隔的 $\\lambda$ 值网格上评估 $\\mathrm{CV}_{K}(\\lambda)$。\n- 对于每个测试用例，报告：\n  1) 最小化 $\\mathrm{GCV}(\\lambda)$ 的 $\\lambda$ 值，\n  2) 最小化 $\\mathrm{CV}_{K}(\\lambda)$ 的 $\\lambda$ 值，\n  3) 在 GCV 选择的 $\\lambda$ 处的有效自由度，$\\mathrm{df}(\\lambda_{\\mathrm{GCV}})$，\n  4) 根据特征阈值启发法 $\\gamma_{i}  \\lambda_{\\mathrm{GCV}}$ 被视为有效激活的非零谱分量的比例，即 $\\frac{1}{r}\\sum_{i=1}^{r} \\mathbf{1}\\{\\gamma_{i}  \\lambda_{\\mathrm{GCV}}\\}$，\n  5) 在 $\\lambda_{\\mathrm{GCV}}$ 和 $\\lambda_{\\mathrm{CV}}$ 处评估的 K 折交叉验证均方误差之差，即 $\\mathrm{CV}_{K}(\\lambda_{\\mathrm{GCV}}) - \\mathrm{CV}_{K}(\\lambda_{\\mathrm{CV}})$。\n- 对 $\\lambda$ 使用一个包含 $L$ 个值的对数网格，范围在 $10^{-6}$ 和 $10^{2}$ 之间，其中 $L = 60$。在选择最小化 $\\lambda$ 时，如果出现平局，则选择最小化值中最小的 $\\lambda$。\n- 折的构建：对于 K 折划分，如果 $n$ 不能被 $K$ 整除，则每折分配 $\\lfloor n/K \\rfloor$ 或 $\\lceil n/K \\rceil$ 个样本，使得各折的样本数最多相差 1；将多余的样本按索引顺序分配给最前面的折。\n- 随机性控制：使用一个固定的基准种子 $2025$，并在该用例的所有伪随机元素上加上测试用例索引 $c \\in \\{1,2,3\\}$，以确保可复现性。\n\n测试套件：\n- 用例 1（高维，幂律谱）：$n=80$，$p=200$，谱 $\\gamma_{i} \\propto i^{-\\alpha}$，其中 $\\alpha=1.5$，归一化后均值为 $1$，$K=5$，$s=10$，$b=1.0$，$\\sigma=0.5$。\n- 用例 2（低维，平坦谱，近乎无噪声）：$n=120$，$p=60$，谱 $\\gamma_{i} \\equiv 1$（对于 $i \\in \\{1,\\dots,r\\}$），$K=10$，$s=8$，$b=1.0$，$\\sigma=10^{-6}$。\n- 用例 3（高维，陡峭的几何谱）：$n=60$，$p=300$，谱 $\\gamma_{i} \\propto \\rho^{i-1}$，其中 $\\rho=0.7$，归一化后均值为 $1$，$K=3$，$s=5$，$b=1.0$，$\\sigma=1.5$。\n\n您的程序应生成单行输出，其中包含三个用例的结果，格式为一个包含三个列表的逗号分隔列表，每个内部列表按上述顺序包含五个所要求的值。确切格式必须是：\n\"[[case1_value1,case1_value2,case1_value3,case1_value4,case1_value5],[case2_value1,case2_value2,case2_value3,case2_value4,case2_value5],[case3_value1,case3_value2,case3_value3,case3_value4,case3_value5]]\"\n行内任何地方都不能有空格。所有报告的数值必须是标准实数。不应打印任何其他文本。", "solution": "本练习旨在实现并比较两种用于选择岭回归惩罚参数 $\\lambda$ 的方法：广义交叉验证 (GCV) 和 K 折交叉验证 (CV)。分析在模拟数据上进行，其中设计矩阵 $X$ 被构造成具有特定的、受控的特征谱，以模拟生物数据集（如基因表达谱）中常见的相关解释变量。\n\n线性模型由 $y = X\\beta^{\\star} + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta^{\\star} \\in \\mathbb{R}^{p}$ 是真实的稀疏系数向量，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 是一个独立同分布的高斯噪声向量。\n\n岭回归通过解决以下优化问题来找到一个估计值 $\\hat{\\beta}_{\\lambda}$：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{n}\\lVert y - X\\beta \\rVert_{2}^{2} + \\lambda \\lVert \\beta \\rVert_{2}^{2}\n$$\n该问题的解由 $\\hat{\\beta}_{\\lambda} = (X^{\\top}X + n\\lambda I_p)^{-1}X^{\\top}y$ 给出。相应的预测是 $\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda} = H_{\\lambda}y$，其中 $H_{\\lambda} = X(X^{\\top}X + n\\lambda I_p)^{-1}X^{\\top}$ 是“帽子”矩阵。\n\n设计矩阵 $X$ 是通过其奇异值分解 (SVD) 构建的，$X = U \\operatorname{diag}(d) V^{\\top}$，其中 $r = \\min(n,p)$，$U \\in \\mathbb{R}^{n \\times r}$ 和 $V \\in \\mathbb{R}^{p \\times r}$ 是列正交的矩阵，$d$ 是一个包含 $r$ 个奇异值的向量。类协方差矩阵 $X^{\\top}X/n$ 的特征值被指定为 $\\{\\gamma_i\\}_{i=1}^r$，它们通过 $\\gamma_i = d_i^2/n$ 与奇异值相关联。这允许构建具有预定义相关结构的数据。\n\n每个测试用例的流程如下：\n\n首先，我们生成数据。这包括：\n1.  为可复现性设置一个唯一的随机种子。\n2.  通过对适当大小的高斯随机矩阵执行 QR 分解来生成正交矩阵 $U$ 和 $V$。\n3.  根据指定的谱衰减剖面（幂律、平坦或几何）计算特征值 $\\{\\gamma_i\\}_{i=1}^r$，并将其归一化以使均值为 1。\n4.  计算奇异值 $d_i = \\sqrt{n \\gamma_i}$。\n5.  构建设计矩阵 $X = U \\operatorname{diag}(d) V^{\\top}$。\n6.  创建具有 $s$ 个幅值为 $b$ 的非零项的真实稀疏系数向量 $\\beta^{\\star}$。\n7.  生成噪声向量 $\\varepsilon$ 并计算响应 $y = X\\beta^{\\star} + \\varepsilon$。\n\n其次，我们使用 GCV 对 $\\lambda$ 进行模型选择。GCV 分数定义为：\n$$\n\\mathrm{GCV}(\\lambda) = \\frac{n \\, \\lVert y - \\hat{y}_{\\lambda} \\rVert_{2}^{2}}{\\big(n - \\mathrm{tr}(H_{\\lambda})\\big)^{2}}\n$$\n一个关键的见解是，预测值 $\\hat{y}_{\\lambda}$ 和帽子矩阵的迹 $\\mathrm{tr}(H_{\\lambda})$ 都可以使用 $X$ 的 SVD 高效计算。如问题所述，帽子矩阵可以写成 $H_{\\lambda} = U \\operatorname{diag}\\!\\left(\\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}\\right) U^{\\top}$。有效自由度 (DoF) 则是 $\\mathrm{df}(\\lambda) = \\mathrm{tr}(H_{\\lambda}) = \\sum_{i=1}^{r} \\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda}$。预测值 $\\hat{y}_{\\lambda}$ 的计算公式为 $\\hat{y}_{\\lambda} = U \\operatorname{diag}(\\frac{\\gamma_i}{\\gamma_i+\\lambda}) (U^{\\top}y)$。这避免了构建大矩阵 $H_\\lambda$ 或 $X^\\top X$，从而使计算变得高效。我们在一个对数网格上评估 $\\mathrm{GCV}(\\lambda)$，并选择最小化该分数的 $\\lambda$ 作为 $\\lambda_{\\mathrm{GCV}}$。\n\n第三，我们使用 K 折交叉验证进行模型选择。数据集被划分为 $K$ 个不相交的折。对于网格上的每个 $\\lambda$ 和每个折 $k \\in \\{1, \\dots, K\\}$，我们在剩下的 $K-1$ 个折（训练集，$(X_{\\text{train}}, y_{\\text{train}})$）上训练一个岭回归模型，并在留出的折（验证集，$(X_{\\text{val}}, y_{\\text{val}})$）上计算预测误差。\n第 $k$ 折的岭系数 $\\hat{\\beta}_{\\lambda}^{(k)}$ 是通过在训练数据上求解正规方程得到的：\n$$\n(X_{\\text{train}}^{\\top}X_{\\text{train}} + n_{\\text{train}}\\lambda I_p)\\hat{\\beta}_{\\lambda}^{(k)} = X_{\\text{train}}^{\\top}y_{\\text{train}}\n$$\n其中 $n_{\\text{train}}$ 是训练集中的样本数。求解此线性系统以得到 $\\hat{\\beta}_{\\lambda}^{(k)}$。在 $p  n_{\\text{train}}$ 的情况下，求解一个等效的、规模更小的系统（“核技巧”）在计算上更有效率。\n计算该折的均方误差，最终的 CV 分数 $\\mathrm{CV}_K(\\lambda)$ 是这 $K$ 个折的误差平均值。我们选择最小化 $\\mathrm{CV}_K(\\lambda)$ 的值作为 $\\lambda_{\\mathrm{CV}}$。\n\n最后，对于每个测试用例，我们计算并报告五个量：\n1.  $\\lambda_{\\mathrm{GCV}}$：由 GCV 选择的最优惩罚。\n2.  $\\lambda_{\\mathrm{CV}}$：由 K 折 CV 选择的最优惩罚。\n3.  $\\mathrm{df}(\\lambda_{\\mathrm{GCV}})$：在 $\\lambda_{\\mathrm{GCV}}$ 处的有效自由度，计算为 $\\sum_{i=1}^{r} \\frac{\\gamma_{i}}{\\gamma_{i} + \\lambda_{\\mathrm{GCV}}}$。这衡量了模型的复杂度。\n4.  有效激活的谱分量比例，由 $\\frac{1}{r}\\sum_{i=1}^{r} \\mathbf{1}\\{\\gamma_{i}  \\lambda_{\\mathrm{GCV}}\\}$ 给出。这是一个直观的启发式方法，用于说明岭模型使用了多少数据的主成分，其中 $\\lambda_{\\mathrm{GCV}}$ 充当对特征值的阈值。\n5.  $\\mathrm{CV}_{K}(\\lambda_{\\mathrm{GCV}}) - \\mathrm{CV}_{K}(\\lambda_{\\mathrm{CV}})$：使用 GCV 选择的 lambda 和 CV 选择的 lambda 之间的 K 折 CV 误差差异。这衡量了 GCV 在多大程度上近似于计算更为密集的 K 折 CV。一个小的正值表明 GCV 是一个很好的替代方法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation across all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'case_idx': 1, 'n': 80, 'p': 200, 'spectrum_type': 'power', 'alpha': 1.5, 'K': 5, 's': 10, 'b': 1.0, 'sigma': 0.5},\n        {'case_idx': 2, 'n': 120, 'p': 60, 'spectrum_type': 'flat', 'K': 10, 's': 8, 'b': 1.0, 'sigma': 1e-6},\n        {'case_idx': 3, 'n': 60, 'p': 300, 'spectrum_type': 'geometric', 'rho': 0.7, 'K': 3, 's': 5, 'b': 1.0, 'sigma': 1.5},\n    ]\n\n    all_results = []\n    \n    L = 60\n    lambda_grid = np.logspace(-6, 2, L)\n\n    for case in test_cases:\n        # Unpack parameters\n        n, p = case['n'], case['p']\n        K, s, b, sigma = case['K'], case['s'], case['b'], case['sigma']\n        \n        # Set seed for reproducibility\n        seed = 2025 + case['case_idx']\n        rng = np.random.default_rng(seed)\n\n        # --- Data Generation ---\n        r = min(n, p)\n\n        # Generate orthonormal matrices U and V\n        if n >= r:\n            U_full, _ = np.linalg.qr(rng.standard_normal((n, n)))\n            U = U_full[:, :r]\n        else:\n             U, _ = np.linalg.qr(rng.standard_normal((n, r)))\n\n        if p >= r:\n            V_full, _ = np.linalg.qr(rng.standard_normal((p, p)))\n            V = V_full[:, :r]\n        else:\n            V, _ = np.linalg.qr(rng.standard_normal((p,r)))\n\n\n        # Generate eigen-spectrum gamma\n        if case['spectrum_type'] == 'power':\n            i = np.arange(1, r + 1)\n            unnorm_gamma = i**(-case['alpha'])\n            norm_const = r / np.sum(unnorm_gamma)\n            gamma = norm_const * unnorm_gamma\n        elif case['spectrum_type'] == 'flat':\n            gamma = np.ones(r)\n        elif case['spectrum_type'] == 'geometric':\n            i = np.arange(r)\n            unnorm_gamma = case['rho']**i\n            norm_const = r / np.sum(unnorm_gamma)\n            gamma = norm_const * unnorm_gamma\n        \n        d = np.sqrt(n * gamma)\n        \n        # Construct design matrix X\n        X = U @ np.diag(d) @ V.T\n\n        # Generate beta_star and response y\n        beta_star = np.zeros(p)\n        if s > 0:\n            beta_indices = rng.choice(p, s, replace=False)\n            beta_star[beta_indices] = b\n        \n        epsilon = rng.standard_normal(n) * sigma\n        y = X @ beta_star + epsilon\n\n        # --- GCV Calculation ---\n        gcv_scores = np.zeros(L)\n        y_u = U.T @ y\n        \n        for i, lam in enumerate(lambda_grid):\n            dof = np.sum(gamma / (gamma + lam))\n            w_lam = (gamma / (gamma + lam)) * y_u\n            y_hat = U @ w_lam\n            rss = np.sum((y - y_hat)**2)\n            denominator = (n - dof)**2\n            gcv_scores[i] = n * rss / denominator if denominator > 1e-12 else np.inf\n\n        lam_gcv_idx = np.argmin(gcv_scores)\n        lam_gcv = lambda_grid[lam_gcv_idx]\n\n        # --- K-fold CV Calculation ---\n        indices = np.arange(n)\n        rng.shuffle(indices)\n        fold_sizes = [n // K + 1] * (n % K) + [n // K] * (K - (n % K))\n        \n        current = 0\n        folds = []\n        for fold_size in fold_sizes:\n            start, stop = current, current + fold_size\n            folds.append(indices[start:stop])\n            current = stop\n\n        cv_scores = np.zeros(L)\n        \n        for i, lam in enumerate(lambda_grid):\n            total_se = 0.0\n            for k in range(K):\n                val_idx = folds[k]\n                train_idx = np.setdiff1d(indices, val_idx)\n                \n                X_train, y_train = X[train_idx, :], y[train_idx]\n                X_val, y_val = X[val_idx, :], y[val_idx]\n                \n                n_train = len(train_idx)\n                \n                if p > n_train:\n                    A = X_train @ X_train.T + n_train * lam * np.identity(n_train)\n                    alpha = np.linalg.solve(A, y_train)\n                    beta_hat = X_train.T @ alpha\n                else:\n                    A = X_train.T @ X_train + n_train * lam * np.identity(p)\n                    b_vec = X_train.T @ y_train\n                    beta_hat = np.linalg.solve(A, b_vec)\n                \n                y_hat_val = X_val @ beta_hat\n                total_se += np.sum((y_val - y_hat_val)**2)\n            \n            cv_scores[i] = total_se / n\n\n        lam_cv_idx = np.argmin(cv_scores)\n        lam_cv = lambda_grid[lam_cv_idx]\n\n        # --- Calculate Final Results ---\n        res1 = lam_gcv\n        res2 = lam_cv\n        res3 = np.sum(gamma / (gamma + lam_gcv))\n        res4 = np.mean(gamma > lam_gcv)\n        res5 = cv_scores[lam_gcv_idx] - cv_scores[lam_cv_idx]\n        \n        all_results.append([res1, res2, res3, res4, res5])\n\n    case_strings = [f\"[{','.join(map(str, res_list))}]\" for res_list in all_results]\n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3345318"}]}