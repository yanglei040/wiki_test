{"hands_on_practices": [{"introduction": "为了真正理解图神经网络（GNNs）如何在生物网络上学习表示，掌握信息传递的基本机制至关重要。本练习将引导您在一个小规模的基因调控网络上，逐步完成一次完整的信息传递神经网络（MPNN）迭代。通过手动计算信息、聚合以及使用门控循环单元（GRU）进行状态更新，您将对驱动GNNs的信息流建立一个具体、自下而上的理解 [@problem_id:3317106]。", "problem": "考虑一个包含 $3$ 个基因 $G_1$、$G_2$ 和 $G_3$ 的有向基因调控网络。每条有向边都标有一个调控基序：激活或抑制。节点特征代表基础转录倾向。您将执行一次消息传递神经网络 (MPNN) 的迭代 ($t=0 \\to t=1$)，其中每条边的消息函数 $\\psi$ 是线性的，每个节点的更新函数 $\\phi$ 是一个门控循环单元 (GRU)。使用消息传递神经网络 (MPNN) 按求和方式进行聚合的标准定义以及标准的门控循环单元 (GRU) 方程。隐藏状态维度为 $d_h=2$，节点特征维度为 $d_x=1$，边特征维度为 $d_e=2$（基序的独热编码：激活为 $[1,0]$，抑制为 $[0,1]$）。\n\n网络结构：\n- 有向边：$G_1 \\to G_2$ (激活)，$G_3 \\to G_2$ (激活)，$G_2 \\to G_1$ (抑制)，$G_2 \\to G_3$ (抑制)。\n- 节点特征 $x_i$：$x_1 = 0.3$，$x_2 = -0.1$，$x_3 = 0.5$。\n- 初始隐藏状态 $h_i^{(0)}$：$h_1^{(0)} = [0.2, -0.1]$，$h_2^{(0)} = [0.0, 0.3]$，$h_3^{(0)} = [-0.2, 0.1]$。\n- 对于每条边 $j \\to i$，激活的边特征 $e_{j \\to i}$ 为 $[1,0]$，抑制的为 $[0,1]$。\n\n消息函数 $\\psi$ 的输入为拼接向量 $[h_j^{(0)}; x_j; e_{j \\to i}] \\in \\mathbb{R}^{5}$，并通过一个带参数的共享线性映射输出 $m_{j \\to i} \\in \\mathbb{R}^{2}$\n$$\nW_m = \n\\begin{pmatrix}\n0.5  -0.3  0.2  0.1  -0.1 \\\\\n-0.4  0.6  -0.2  0.05  0.15\n\\end{pmatrix},\n\\quad\nb_m = \n\\begin{pmatrix}\n0.01 \\\\\n-0.02\n\\end{pmatrix}.\n$$\n\n通过求和聚合传入节点 $G_2$ 的消息：$M_2 = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(2)} m_{j \\to 2}$，其中 $\\mathcal{N}_{\\text{in}}(2) = \\{1, 3\\}$。\n\n更新函数 $\\phi$ 是一个 GRU，输入为 $M_2 \\in \\mathbb{R}^{2}$ 和先前的隐藏状态 $h_2^{(0)} \\in \\mathbb{R}^{2}$。GRU 参数（在节点间共享）如下：\n$$\nW_z = \n\\begin{pmatrix}\n0.2  -0.1 \\\\\n0.05  0.3\n\\end{pmatrix},\\quad\nU_z = \n\\begin{pmatrix}\n-0.2  0.4 \\\\\n0.1  -0.3\n\\end{pmatrix},\\quad\nb_z = \n\\begin{pmatrix}\n0.01 \\\\\n-0.02\n\\end{pmatrix},\n$$\n$$\nW_r = \n\\begin{pmatrix}\n-0.1  0.2 \\\\\n0.3  -0.05\n\\end{pmatrix},\\quad\nU_r = \n\\begin{pmatrix}\n0.25  -0.15 \\\\\n-0.2  0.35\n\\end{pmatrix},\\quad\nb_r = \n\\begin{pmatrix}\n0.0 \\\\\n0.01\n\\end{pmatrix},\n$$\n$$\nW_h = \n\\begin{pmatrix}\n0.3  0.1 \\\\\n-0.2  0.25\n\\end{pmatrix},\\quad\nU_h = \n\\begin{pmatrix}\n0.5  -0.3 \\\\\n0.2  0.4\n\\end{pmatrix},\\quad\nb_h = \n\\begin{pmatrix}\n0.0 \\\\\n0.0\n\\end{pmatrix}.\n$$\n\n执行一次完整的 MPNN 迭代，计算节点 $G_2$ 更新后的隐藏状态 $h_2^{(1)}$。以实数形式报告 $h_2^{(1)}$ 的第一个分量。将答案四舍五入到四位有效数字。无需单位。", "solution": "用户希望找到在消息传递神经网络 (MPNN) 的一次迭代后，节点 $G_2$ 更新后的隐藏状态 $h_2^{(1)}$ 的第一个分量。该过程包括三个主要步骤：消息计算、消息聚合和使用门控循环单元 (GRU) 进行状态更新。\n\n### 步骤1：问题验证\n根据指定标准对问题陈述进行验证。\n\n**提取的已知条件：**\n- **网络结构：** 一个有 $3$ 个节点 ($G_1, G_2, G_3$) 的有向图，边为 $G_1 \\to G_2$ (激活)，$G_3 \\to G_2$ (激活)，$G_2 \\to G_1$ (抑制)，$G_2 \\to G_3$ (抑制)。\n- **节点特征：** $x_1 = 0.3$，$x_2 = -0.1$，$x_3 = 0.5$。这些是标量 ($d_x=1$)。\n- **初始隐藏状态：** $h_1^{(0)} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$，$h_2^{(0)} = \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix}$，$h_3^{(0)} = \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix}$。隐藏状态维度为 $d_h=2$。\n- **边特征：** 维度为 $d_e=2$ 的独热编码。激活为 $e_{act} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，抑制为 $e_{inh} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。\n- **消息函数 ($\\psi$)：** $m_{j \\to i} = W_m [h_j^{(0)}; x_j; e_{j \\to i}] + b_m$。输入分别是 $\\mathbb{R}^{2}$、$\\mathbb{R}^{1}$ 和 $\\mathbb{R}^{2}$ 中向量的拼接，形成一个 $\\mathbb{R}^{5}$ 中的向量。输出是一个消息 $m_{j \\to i} \\in \\mathbb{R}^{2}$。\n   - $W_m = \\begin{pmatrix} 0.5  -0.3  0.2  0.1  -0.1 \\\\ -0.4  0.6  -0.2  0.05  0.15 \\end{pmatrix}$，$b_m = \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}$。\n- **聚合函数：** 对传入节点 $G_2$ 的消息求和。向 $G_2$ 发送消息的邻居集合是 $\\mathcal{N}_{\\text{in}}(2) = \\{1, 3\\}$。聚合后的消息是 $M_2 = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(2)} m_{j \\to 2}$。\n- **更新函数 ($\\phi$)：** 一个 GRU，输入为 $M_2$ 和先前的隐藏状态 $h_2^{(0)}$。\n   - $W_z = \\begin{pmatrix} 0.2  -0.1 \\\\ 0.05  0.3 \\end{pmatrix}, U_z = \\begin{pmatrix} -0.2  0.4 \\\\ 0.1  -0.3 \\end{pmatrix}, b_z = \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}$\n   - $W_r = \\begin{pmatrix} -0.1  0.2 \\\\ 0.3  -0.05 \\end{pmatrix}, U_r = \\begin{pmatrix} 0.25  -0.15 \\\\ -0.2  0.35 \\end{pmatrix}, b_r = \\begin{pmatrix} 0.0 \\\\ 0.01 \\end{pmatrix}$\n   - $W_h = \\begin{pmatrix} 0.3  0.1 \\\\ -0.2  0.25 \\end{pmatrix}, U_h = \\begin{pmatrix} 0.5  -0.3 \\\\ 0.2  0.4 \\end{pmatrix}, b_h = \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix}$\n- **任务：** 计算 $h_2^{(1)}$ 的第一个分量，并四舍五入到四位有效数字。\n\n**验证结论：**\n该问题是 **有效的**。其科学基础在于图神经网络的原理及其在计算生物学中的应用。问题是良构的，所有必要的参数、初始条件和函数都有清晰的定义。所有矩阵和向量的维度都是一致的。问题是客观的，没有歧义。这是一个基于所提供定义的直接计算任务。\n\n### 步骤2：求解计算\n\n计算分三个阶段进行：消息生成、聚合和状态更新。\n\n**阶段I：消息生成**\n我们需要计算来自 $G_2$ 的邻居（即 $G_1$ 和 $G_3$）的消息。\n\n1.  **从 $G_1$ 到 $G_2$ 的消息 ($m_{1 \\to 2}$):**\n    边 $G_1 \\to G_2$ 是激活，所以 $e_{1 \\to 2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n    消息函数的输入向量是拼接向量 $[h_1^{(0)}; x_1; e_{1 \\to 2}]$：\n    $$ v_1 = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n    消息为 $m_{1 \\to 2} = W_m v_1 + b_m$：\n    $$ m_{1 \\to 2} = \\begin{pmatrix} 0.5  -0.3  0.2  0.1  -0.1 \\\\ -0.4  0.6  -0.2  0.05  0.15 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{1 \\to 2} = \\begin{pmatrix} (0.5)(0.2) + (-0.3)(-0.1) + (0.2)(0.3) + (0.1)(1) + (-0.1)(0) \\\\ (-0.4)(0.2) + (0.6)(-0.1) + (-0.2)(0.3) + (0.05)(1) + (0.15)(0) \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{1 \\to 2} = \\begin{pmatrix} 0.1 + 0.03 + 0.06 + 0.1 \\\\ -0.08 - 0.06 - 0.06 + 0.05 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ -0.15 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.30 \\\\ -0.17 \\end{pmatrix} $$\n\n2.  **从 $G_3$ 到 $G_2$ 的消息 ($m_{3 \\to 2}$):**\n    边 $G_3 \\to G_2$ 是激活，所以 $e_{3 \\to 2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n    输入向量是 $[h_3^{(0)}; x_3; e_{3 \\to 2}]$：\n    $$ v_3 = \\begin{pmatrix} -0.2 \\\\ 0.1 \\\\ 0.5 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n    消息为 $m_{3 \\to 2} = W_m v_3 + b_m$：\n    $$ m_{3 \\to 2} = \\begin{pmatrix} 0.5  -0.3  0.2  0.1  -0.1 \\\\ -0.4  0.6  -0.2  0.05  0.15 \\end{pmatrix} \\begin{pmatrix} -0.2 \\\\ 0.1 \\\\ 0.5 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{3 \\to 2} = \\begin{pmatrix} (0.5)(-0.2) + (-0.3)(0.1) + (0.2)(0.5) + (0.1)(1) + (-0.1)(0) \\\\ (-0.4)(-0.2) + (0.6)(0.1) + (-0.2)(0.5) + (0.05)(1) + (0.15)(0) \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{3 \\to 2} = \\begin{pmatrix} -0.1 - 0.03 + 0.1 + 0.1 \\\\ 0.08 + 0.06 - 0.1 + 0.05 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.07 \\\\ 0.09 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.08 \\\\ 0.07 \\end{pmatrix} $$\n\n**阶段II：消息聚合**\n节点 $G_2$ 的聚合消息是传入消息的总和：\n$$ M_2 = m_{1 \\to 2} + m_{3 \\to 2} = \\begin{pmatrix} 0.30 \\\\ -0.17 \\end{pmatrix} + \\begin{pmatrix} 0.08 \\\\ 0.07 \\end{pmatrix} = \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} $$\n\n**阶段III：状态更新 (GRU)**\n更新函数是一个 GRU，输入为 $M_2$，先前的状态为 $h_2^{(0)}$。令 $\\sigma(x) = 1/(1+\\exp(-x))$ 为 sigmoid 函数，$\\odot$ 为逐元素乘积。\n\n1.  **更新门 ($z_2$)：** $z_2 = \\sigma(W_z M_2 + U_z h_2^{(0)} + b_z)$\n    $$ W_z M_2 = \\begin{pmatrix} 0.2  -0.1 \\\\ 0.05  0.3 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} 0.076+0.01 \\\\ 0.019-0.03 \\end{pmatrix} = \\begin{pmatrix} 0.086 \\\\ -0.011 \\end{pmatrix} $$\n    $$ U_z h_2^{(0)} = \\begin{pmatrix} -0.2  0.4 \\\\ 0.1  -0.3 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.12 \\\\ -0.09 \\end{pmatrix} $$\n    $$ z_2 = \\sigma\\left( \\begin{pmatrix} 0.086 \\\\ -0.011 \\end{pmatrix} + \\begin{pmatrix} 0.12 \\\\ -0.09 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} \\right) = \\sigma\\left( \\begin{pmatrix} 0.216 \\\\ -0.121 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.553793 \\\\ 0.469799 \\end{pmatrix} $$\n\n2.  **重置门 ($r_2$)：** $r_2 = \\sigma(W_r M_2 + U_r h_2^{(0)} + b_r)$\n    $$ W_r M_2 = \\begin{pmatrix} -0.1  0.2 \\\\ 0.3  -0.05 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} -0.038-0.02 \\\\ 0.114+0.005 \\end{pmatrix} = \\begin{pmatrix} -0.058 \\\\ 0.119 \\end{pmatrix} $$\n    $$ U_r h_2^{(0)} = \\begin{pmatrix} 0.25  -0.15 \\\\ -0.2  0.35 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} -0.045 \\\\ 0.105 \\end{pmatrix} $$\n    $$ r_2 = \\sigma\\left( \\begin{pmatrix} -0.058 \\\\ 0.119 \\end{pmatrix} + \\begin{pmatrix} -0.045 \\\\ 0.105 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.01 \\end{pmatrix} \\right) = \\sigma\\left( \\begin{pmatrix} -0.103 \\\\ 0.234 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.474278 \\\\ 0.558234 \\end{pmatrix} $$\n\n3.  **候选隐藏状态 ($\\tilde{h}_2^{(1)}$)：** $\\tilde{h}_2^{(1)} = \\tanh(W_h M_2 + U_h (r_2 \\odot h_2^{(0)}) + b_h)$\n    $$ r_2 \\odot h_2^{(0)} = \\begin{pmatrix} 0.474278 \\\\ 0.558234 \\end{pmatrix} \\odot \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.167470 \\end{pmatrix} $$\n    $$ W_h M_2 = \\begin{pmatrix} 0.3  0.1 \\\\ -0.2  0.25 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} 0.114-0.01 \\\\ -0.076-0.025 \\end{pmatrix} = \\begin{pmatrix} 0.104 \\\\ -0.101 \\end{pmatrix} $$\n    $$ U_h (r_2 \\odot h_2^{(0)}) = \\begin{pmatrix} 0.5  -0.3 \\\\ 0.2  0.4 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.167470 \\end{pmatrix} = \\begin{pmatrix} -0.050241 \\\\ 0.066988 \\end{pmatrix} $$\n    $$ \\tilde{h}_2^{(1)} = \\tanh\\left( \\begin{pmatrix} 0.104 \\\\ -0.101 \\end{pmatrix} + \\begin{pmatrix} -0.050241 \\\\ 0.066988 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix} \\right) = \\tanh\\left( \\begin{pmatrix} 0.053759 \\\\ -0.034012 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.053717 \\\\ -0.033996 \\end{pmatrix} $$\n\n4.  **新隐藏状态 ($h_2^{(1)}$)：** $h_2^{(1)} = (1 - z_2) \\odot h_2^{(0)} + z_2 \\odot \\tilde{h}_2^{(1)}$\n    $$ 1 - z_2 = \\begin{pmatrix} 1 - 0.553793 \\\\ 1 - 0.469799 \\end{pmatrix} = \\begin{pmatrix} 0.446207 \\\\ 0.530201 \\end{pmatrix} $$\n    $$ (1 - z_2) \\odot h_2^{(0)} = \\begin{pmatrix} 0.446207 \\\\ 0.530201 \\end{pmatrix} \\odot \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.159060 \\end{pmatrix} $$\n    $$ z_2 \\odot \\tilde{h}_2^{(1)} = \\begin{pmatrix} 0.553793 \\\\ 0.469799 \\end{pmatrix} \\odot \\begin{pmatrix} 0.053717 \\\\ -0.033996 \\end{pmatrix} = \\begin{pmatrix} 0.029749 \\\\ -0.015971 \\end{pmatrix} $$\n    $$ h_2^{(1)} = \\begin{pmatrix} 0.0 \\\\ 0.159060 \\end{pmatrix} + \\begin{pmatrix} 0.029749 \\\\ -0.015971 \\end{pmatrix} = \\begin{pmatrix} 0.029749 \\\\ 0.143089 \\end{pmatrix} $$\n\n更新后的隐藏状态 $h_2^{(1)}$ 的第一个分量是 $0.029749$。\n四舍五入到四位有效数字得到 $0.02975$。", "answer": "$$\n\\boxed{0.02975}\n$$", "id": "3317106"}, {"introduction": "在细胞状态分类等关键生物学任务中部署GNNs，不仅要求预测准确，还需要一个可靠的机制来评估预测何时可能出错，特别是对于分布外（OOD）的样本。本实践要求您实现两种关键的不确定性量化方法——预测熵和基于能量的分数——来识别不可靠的预测。通过这个编码练习，您将学会为模型构建安全机制，实施一种“拒绝预测”策略来暂停对低置信度样本的预测，这是负责任地部署模型的一项关键技能 [@problem_id:3317151]。", "problem": "给定一个在细胞间相互作用图上为细胞状态分类任务训练的图神经网络（GNN）的节点级输出。您的目标是使用预测熵和能量分数来检测预测不可靠的分布外（OOD）细胞状态，并实施满足目标覆盖率约束的拒绝策略，以实现安全部署。您必须编写一个完整、可运行的程序，从第一性原理计算所要求的指标。\n\n推导和实现的基本依据：使用以下经过充分检验的事实和定义。对于一个有限的标签集，GNN节点分类器会为每个节点导出关于标签的类别预测分布。类别预测分布是通过对未归一化的分数（logits）进行归一化得到的。关于类别预测的不确定性可以通过该类别分布的香农熵来量化。在基于能量的模型和统计力学中，与温度下某个构型相对应的能量是负温度乘以配分函数的对数，对于logits而言，这对应于负温度乘以logits按逆温度缩放后的指数和的对数。受试者工作特征（Receiver Operating Characteristic）分析比较正例和负例的分数分布，曲线下面积（AUROC）是一个随机选择的正例比一个随机选择的负例得分更高的概率，其中平局情况计为一半。\n\n您的程序必须为下面的每个测试用例实现以下任务：\n\n- 每个节点的输入包括一个长度为 $C$ 的未归一化logit向量和一个二元指示符，表示该节点是OOD（$1$）还是分布内（ID，$0$）。所有对数均为自然对数（以 $e$ 为底），所有温度均为正实数。\n- 对于每个节点，从logits计算经过softmax归一化的类别预测分布，然后计算预测不确定性，即该类别分布的香农熵。\n- 对于每个节点，在给定温度下计算其能量分数，该分数对应于与logits和温度相关联的吉布斯分布的配分函数的对数乘以负温度。\n- 将较高的不确定性和较高的能量视为OOD可能性的更高指标。对于每种评分方法（熵和能量），计算针对OOD标签的受试者工作特征曲线下面积（AUROC），平局计为一半。\n- 为每种评分方法实施一种拒绝策略，以达到目标覆盖率 $c \\in (0,1]$。该策略通过选择得分最低的 $k$ 个节点来保留（发布预测），其中对于 $N$ 个节点，$k = \\lfloor c N \\rfloor$，并对余下的节点拒绝预测。报告实现的覆盖率（将等于 $k/N$）以及被拒绝集合的OOD召回率，该召回率定义为被拒绝集合中的OOD节点数除以OOD节点总数。如果在截断点附近出现分数平局，则在按分数排序后，通过索引顺序确定性地打破平局。\n\n测试套件。对于每个测试用例，给定一个形状为 $N \\times C$ 的logits矩阵、一个长度为 $N$ 的OOD标签向量、一个温度 $T$ 和一个覆盖率目标 $c$。\n\n- 测试用例 1：$N=8$， $C=3$， $T=1.0$， $c=0.6$。\n  Logits矩阵\n  $$\n  \\begin{bmatrix}\n  5.0  0.5  -0.5 \\\\\n  4.0  1.0  0.0 \\\\\n  3.5  0.2  0.1 \\\\\n  2.0  2.0  0.0 \\\\\n  3.2  -0.1  0.0 \\\\\n  0.1  0.0  -0.1 \\\\\n  0.5  0.5  0.5 \\\\\n  0.0  -0.2  0.1\n  \\end{bmatrix}\n  $$\n  OOD标签\n  $$\n  [\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,].\n  $$\n\n- 测试用例 2：$N=7$， $C=4$， $T=2.0$， $c=0.5$。\n  Logits矩阵\n  $$\n  \\begin{bmatrix}\n  8.0  0.0  0.0  0.0 \\\\\n  0.0  7.5  0.0  0.0 \\\\\n  0.0  0.0  0.0  0.0 \\\\\n  0.1  -0.1  0.0  0.0 \\\\\n  2.2  2.1  2.0  1.9 \\\\\n  -0.1  0.0  0.1  0.0 \\\\\n  4.0  3.9  -10.0  -9.0\n  \\end{bmatrix}\n  $$\n  OOD标签\n  $$\n  [\\,0,\\,0,\\,1,\\,1,\\,0,\\,1,\\,0\\,].\n  $$\n\n- 测试用例 3：$N=6$， $C=2$， $T=1.5$， $c=0.67$。\n  Logits矩阵\n  $$\n  \\begin{bmatrix}\n  2.0  1.9 \\\\\n  0.0  0.0 \\\\\n  0.1  -0.1 \\\\\n  1.0  0.95 \\\\\n  0.5  0.49 \\\\\n  0.2  0.21\n  \\end{bmatrix}\n  $$\n  OOD标签\n  $$\n  [\\,0,\\,1,\\,1,\\,0,\\,0,\\,1\\,].\n  $$\n\n输出规范。对于每个测试用例，按顺序计算以下五个量：\n- 使用预测熵作为OOD分数的AUROC。\n- 使用能量作为OOD分数的AUROC。\n- 在给定 $c$ 下，基于熵的拒绝策略所达到的覆盖率。\n- 基于熵的拒绝策略下，被拒绝集合的OOD召回率。\n- 基于能量的拒绝策略下，被拒绝集合的OOD召回率。\n\n您的程序应生成单行输出，其中包含所有测试用例的串联结果，形式为一个扁平的浮点数列表，每个浮点数精确到小数点后六位，用方括号括起来并用逗号分隔，按测试用例 $1, 2, 3$ 的顺序排列。例如，输出格式必须是\n$$\n[\\;v_{1,1},\\,v_{1,2},\\,v_{1,3},\\,v_{1,4},\\,v_{1,5},\\,v_{2,1},\\,\\dots,\\,v_{3,5}\\;],\n$$\n其中 $v_{i,j}$ 表示测试用例 $i$ 的第 $j$ 个量。", "solution": "我们使用概率论、信息论和统计力学的定义来形式化该问题，然后设计一个忠于这些原则的算法。\n\n对于 $C$ 个细胞状态类别，GNN节点分类器为每个节点生成一个未归一化的分数向量（logits）$\\mathbf{z} \\in \\mathbb{R}^C$。通过对 $\\mathbf{z}$ 进行归一化，使其分量和为$1$且保持为正，可以得到关于标签的类别预测分布 $\\mathbf{p} \\in \\Delta^{C-1}$。将未归一化的对数概率映射到概率的唯一平滑归一化映射由softmax变换给出，该变换是指数映射除以其配分函数。数值稳定的计算使用一个常数偏移量，该偏移量在归一化过程中会抵消，因为概率对于$\\mathbf{z}$的加性平移具有不变性。\n\n预测不确定性可以通过类别分布的香农熵来量化。对于一个类别分布 $\\mathbf{p} = (p_1,\\dots,p_C)$，香农熵由信息论的公理定义为满足连续性、在均匀分布时取最大值以及递归性的唯一泛函（在乘法常数范围内）。使用自然对数，以自然单位表示的香农熵为\n$$\nH(\\mathbf{p}) \\;=\\; -\\sum_{k=1}^C p_k \\,\\log p_k,\n$$\n并约定 $0 \\log 0 = 0$（通过连续性）。对于一个尖锐的分布 $\\mathbf{p}$，$H(\\mathbf{p})$ 很小；对于一个均匀的分布 $\\mathbf{p}$，$H(\\mathbf{p})$ 很大。\n\n基于能量的推理将logits解释为构型的未归一化负能量。在温度 $T>0$ 时，标签上的吉布斯分布的概率与 $\\exp(z_k/T)$ 成正比。相关的配分函数是\n$$\nZ(\\mathbf{z},T) \\;=\\; \\sum_{k=1}^C \\exp\\!\\left(\\frac{z_k}{T}\\right).\n$$\n与未归一化状态相关的能量是负温度乘以对数配分函数，\n$$\nE(\\mathbf{z};T) \\;=\\; -T \\,\\log Z(\\mathbf{z},T) \\;=\\; -T \\,\\log \\!\\left( \\sum_{k=1}^C \\exp\\!\\left(\\frac{z_k}{T}\\right) \\right).\n$$\n大而置信度高的logits会使对数内的和变大，从而降低 $E(\\mathbf{z};T)$（使其更负），而平坦或低幅度的logits会使 $Z$ 较小，从而增加 $E(\\mathbf{z};T)$（使其更不负）。因此，较高的 $E(\\mathbf{z};T)$ 表示较高的不确定性和更大的OOD可能性。\n\n为了将不确定性分数与OOD标签进行比较，我们采用受试者工作特征（Receiver Operating Characteristic）框架。定义一个实值分数 $s \\in \\mathbb{R}$，其中较大的 $s$ 应表示更高的OOD可能性。给定一组分数 $\\{s_i\\}_{i=1}^N$ 和二元标签 $\\{\\ell_i\\}_{i=1}^N$（其中 $\\ell_i \\in \\{0,1\\}$），我们将 $\\ell_i=1$ 解释为OOD（正例），$\\ell_i=0$ 解释为ID（负例）。受试者工作特征曲线下面积（AUROC）是一个随机抽样的正例得分严格高于一个随机抽样的负例的概率，再加上平局概率的一半。具体来说，令 $\\mathcal{P}=\\{i:\\ell_i=1\\}$ 和 $\\mathcal{N}=\\{j:\\ell_j=0\\}$，其大小分别为 $|\\mathcal{P}|=n_1$ 和 $|\\mathcal{N}|=n_0$，则AUROC为\n$$\n\\mathrm{AUROC} \\;=\\; \\frac{1}{n_1 n_0}\\sum_{i\\in\\mathcal{P}} \\sum_{j\\in\\mathcal{N}} \\left[ \\mathbf{1}\\{ s_i > s_j \\} + \\tfrac{1}{2}\\,\\mathbf{1}\\{ s_i = s_j \\} \\right],\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。该表达式等价于通过 $n_1 n_0$ 归一化的Mann–Whitney $U$ 统计量，并能正确处理平局情况。\n\n对于拒绝策略，我们寻求一种安全的部署策略，即对不确定的节点不进行预测，同时满足覆盖率约束。设覆盖率 $c \\in (0,1]$ 表示发布预测的节点比例。给定节点 $i$ 的不确定性分数 $u_i$，其中较大的 $u_i$ 表示较高的不确定性，在硬覆盖率约束下，最小化保留预测的总体不确定性的最优选择是选择具有最小 $u_i$ 的 $k = \\lfloor cN \\rfloor$ 个节点。我们将拒绝集合定义为其补集。实现的覆盖率为 $k/N$。为了衡量OOD过滤的有效性，我们将拒绝集合内的OOD召回率定义为\n$$\n\\mathrm{Recall}_{\\mathrm{OOD}} \\;=\\; \\frac{\\left|\\{\\,i \\in \\text{abstained} : \\ell_i=1\\,\\}\\right|}{\\left|\\{\\,i : \\ell_i=1\\,\\}\\right|},\n$$\n这是被拒绝策略正确捕获的OOD节点所占的比例。当在截断点附近出现平局时，一个确定性且可复现的选择是按 $(u_i, i)$（分数然后是索引）进行排序，以确保稳定性。\n\n每个测试用例的算法步骤如下：\n- 对于每个节点 $i \\in \\{1,\\dots,N\\}$，其logits为 $\\mathbf{z}_i \\in \\mathbb{R}^C$，使用指数变换并除以总和进行归一化，计算类别预测分布 $\\mathbf{p}_i$。为了数值稳定性，在求幂之前减去 $m_i = \\max_k z_{ik}$，然后进行归一化，由于softmax对加性常数的不变性，这不会改变 $\\mathbf{p}_i$。\n- 计算预测熵 $h_i = H(\\mathbf{p}_i) = -\\sum_{k=1}^C p_{ik} \\log p_{ik}$，在数值实现中为 $p_{ik}$ 设置一个小的正下界，以避免计算 $\\log 0$。\n- 计算能量 $e_i = E(\\mathbf{z}_i;T) = -T \\log \\sum_{k=1}^C \\exp(z_{ik}/T)$。为了数值稳定性，计算 $a_{ik} = z_{ik}/T$，减去 $b_i=\\max_k a_{ik}$，评估 $\\log\\sum_k \\exp(a_{ik}-b_i)$，然后加上 $b_i$，最后乘以 $-T$。\n- 构建两个OOD分数：$s^{(H)}_i = h_i$ 和 $s^{(E)}_i = e_i$，两者都定向为值越大表示越可能是OOD。\n- 通过上述的成对定义，计算每个分数相对于OOD标签的 $\\mathrm{AUROC}$。\n- 对于覆盖率为 $c$ 的拒绝策略，设置 $k=\\lfloor cN \\rfloor$。对于基于熵的拒绝，按 $(s^{(H)}_i, i)$ 升序对节点排序，并保留前 $k$ 个节点；对其余节点拒绝预测。对于基于能量的拒绝，类似地按 $(s^{(E)}_i, i)$ 升序排序。计算实现的覆盖率 $k/N$ 和每种方法下被拒绝集合的OOD召回率。\n\n边缘情况和正确性考虑：\n- 当 $cN  1$ 时，$\\lfloor cN \\rfloor$ 确保至少可以保留 $0$ 个节点；在我们的测试套件中，$c$ 和 $N$ 的选择使得 $k \\ge 1$。实现的覆盖率恰好是 $k/N$，可能由于向下取整而略低于 $c$。\n- 使用成对比较的AUROC定义对平局情况具有鲁棒性；当所有分数在不同标签间都相等时，$\\mathrm{AUROC} = 0.5$。\n- 熵在 $\\mathbf{p}_i$ 均匀时最大化，这与将高熵视为类OOD是一致的。随着logits变得平坦，能量增加（变得更不负），这与使用更高能量作为更像OOD的分离标准相一致。\n\n我们现在在指定的测试用例上实现这个算法。对于每个用例，我们按顺序计算五个量：使用熵的AUROC，使用能量的AUROC，基于熵的拒绝策略下实现的覆盖率，基于熵的拒绝策略下被拒绝集合的OOD召回率，以及基于能量的拒绝策略下被拒绝集合的OOD召回率。最终输出将测试用例1、2和3的这5个量连接起来，打印成一个扁平列表，每个值格式化为恰好6位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray) - np.ndarray:\n    # Numerically stable softmax over last axis\n    z = logits\n    m = np.max(z, axis=1, keepdims=True)\n    exp = np.exp(z - m)\n    sum_exp = np.sum(exp, axis=1, keepdims=True)\n    return exp / sum_exp\n\ndef predictive_entropy(probs: np.ndarray) - np.ndarray:\n    # Shannon entropy with natural log; clip to avoid log(0)\n    eps = 1e-12\n    p = np.clip(probs, eps, 1.0)\n    return -np.sum(p * np.log(p), axis=1)\n\ndef energy_scores(logits: np.ndarray, T: float) - np.ndarray:\n    # E = -T * log sum_k exp(z_k / T), computed stably\n    a = logits / T\n    m = np.max(a, axis=1, keepdims=True)\n    lse = m + np.log(np.sum(np.exp(a - m), axis=1, keepdims=True))\n    E = -T * lse\n    return E.ravel()\n\ndef auc_pairwise(scores: np.ndarray, labels: np.ndarray) - float:\n    # Labels: 1 for OOD (positive), 0 for ID (negative)\n    pos = scores[labels == 1]\n    neg = scores[labels == 0]\n    n_pos = pos.size\n    n_neg = neg.size\n    if n_pos == 0 or n_neg == 0:\n        return float('nan')\n    # Pairwise comparisons\n    # Use broadcasting cautiously due to sizes; here small N, safe\n    diff = pos[:, None] - neg[None, :]\n    gt = (diff  0).sum()\n    eq = (diff == 0).sum()\n    auc = (gt + 0.5 * eq) / (n_pos * n_neg)\n    return float(auc)\n\ndef abstain_with_coverage(scores: np.ndarray, labels: np.ndarray, coverage: float):\n    # Keep k = floor(c * N) with lowest scores (lowest uncertainty), abstain on the rest.\n    N = scores.size\n    k = int(np.floor(coverage * N + 1e-12))\n    # Deterministic tie-breaking by index: stable sort on (score, index)\n    idx = np.arange(N)\n    order = np.lexsort((idx, scores))  # sort by scores asc, then idx asc\n    keep_idx = order[:k]\n    abstain_idx = order[k:]\n    achieved_coverage = k / N\n    # OOD recall in abstained set\n    if np.sum(labels == 1) == 0:\n        ood_recall = 0.0\n    else:\n        ood_in_abstained = np.sum(labels[abstain_idx] == 1)\n        total_ood = np.sum(labels == 1)\n        ood_recall = ood_in_abstained / total_ood\n    return achieved_coverage, ood_recall\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"logits\": np.array([\n                [5.0, 0.5, -0.5],\n                [4.0, 1.0, 0.0],\n                [3.5, 0.2, 0.1],\n                [2.0, 2.0, 0.0],\n                [3.2, -0.1, 0.0],\n                [0.1, 0.0, -0.1],\n                [0.5, 0.5, 0.5],\n                [0.0, -0.2, 0.1]\n            ], dtype=float),\n            \"ood\": np.array([0,0,0,0,0,1,1,1], dtype=int),\n            \"T\": 1.0,\n            \"coverage\": 0.6\n        },\n        {\n            \"logits\": np.array([\n                [8.0, 0.0, 0.0, 0.0],\n                [0.0, 7.5, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.1, -0.1, 0.0, 0.0],\n                [2.2, 2.1, 2.0, 1.9],\n                [-0.1, 0.0, 0.1, 0.0],\n                [4.0, 3.9, -10.0, -9.0]\n            ], dtype=float),\n            \"ood\": np.array([0,0,1,1,0,1,0], dtype=int),\n            \"T\": 2.0,\n            \"coverage\": 0.5\n        },\n        {\n            \"logits\": np.array([\n                [2.0, 1.9],\n                [0.0, 0.0],\n                [0.1, -0.1],\n                [1.0, 0.95],\n                [0.5, 0.49],\n                [0.2, 0.21]\n            ], dtype=float),\n            \"ood\": np.array([0,1,1,0,0,1], dtype=int),\n            \"T\": 1.5,\n            \"coverage\": 0.67\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        logits = case[\"logits\"]\n        labels = case[\"ood\"].astype(int)\n        T = float(case[\"T\"])\n        coverage = float(case[\"coverage\"])\n\n        probs = softmax_rows(logits)\n        ent = predictive_entropy(probs)\n        eng = energy_scores(logits, T)\n\n        # AUROC with entropy and energy (higher means more OOD-like)\n        auroc_ent = auc_pairwise(ent, labels)\n        auroc_eng = auc_pairwise(eng, labels)\n\n        # Abstention with coverage using entropy scores\n        cov_ent, recall_ent = abstain_with_coverage(ent, labels, coverage)\n        # Abstention with coverage using energy scores\n        # The achieved coverage will be the same for both since k is the same\n        _, recall_eng = abstain_with_coverage(eng, labels, coverage)\n\n        # Append results in the specified order: AUROC_ent, AUROC_eng, cov_ent, recall_ent, recall_eng\n        results.extend([\n            f\"{auroc_ent:.6f}\",\n            f\"{auroc_eng:.6f}\",\n            f\"{cov_ent:.6f}\",\n            f\"{recall_ent:.6f}\",\n            f\"{recall_eng:.6f}\"\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3317151"}, {"introduction": "GNNs的一个已知局限性，称为“过度挤压”（oversquashing），会阻碍其对长程依赖关系的建模能力，而这在生物信号级联反应中很常见。这个高级练习将介绍一个受图几何启发的强大解决方案：使用离散里奇曲率（discrete Ricci curvature）来诊断问题，并通过重连网络来缓解过度挤压。通过实现从曲率计算到GNN模拟的完整流程，您将探索一种提高GNN性能的前沿技术，并深入了解图结构与信息流之间的关系 [@problem_id:3317174]。", "problem": "生物信号网络可以表示为一个无向简单图 $G = (V,E)$，其中顶点 $V$ 表示信号实体（例如，蛋白质），边 $E$ 表示潜在的相互作用或信息流。在图神经网络（GNN, Graph Neural Network）的消息传递范式中，顶点 $i \\in V$ 上的标量特征 $x^{(t)}_i$ 通过聚合邻居信息逐层更新；一个广泛使用的基本规则是对邻居特征进行平均。当许多远端源必须通过图的狭窄区域汇集以到达目标时，信息可能被压缩到一个低维瓶颈中，这种现象被称为“过挤压”(oversquashing)。图上的离散里奇曲率（Discrete Ricci curvature）为此类瓶颈提供了一个有原则的几何指标：强的负曲率通常预示着局部邻域之间存在瓶颈传输。在本问题中，您将通过离散曲率诊断过挤压现象，通过一种由曲率引导的重连（rewiring）方法来缓解它，该方法在负曲率边附近添加边以改善局部连通性，并评估这种方法在配体扰动下对预测通路激活的影响。\n\n使用以下基本概念：\n\n- 信号图 $G = (V,E)$ 是有限、无向且无权的。顶点 $i$ 的邻居集合是 $N(i)$，度为 $d_i = |N(i)|$，顶点 $u$ 和 $v$ 之间的最短路径距离表示为 $D(u,v)$。\n- 对于每条边 $(i,j) \\in E$，分别在 $N(i)$ 和 $N(j)$ 上定义均匀邻居测度 $m_i$ 和 $m_j$，具体为：对于 $u \\in N(i)$，$m_i(u) = 1/d_i$；对于 $v \\in N(j)$，$m_j(v) = 1/d_j$。边 $(i,j)$ 的Ollivier-Ricci曲率（ORC, Ollivier-Ricci curvature），在闲置参数为 $0$ 时，定义为\n$$\n\\kappa_{ij} = 1 - W_1(m_i, m_j),\n$$\n其中 $W_1(m_i, m_j)$ 是在离散分布 $m_i$ 和 $m_j$ 之间，使用地面度量 $D(\\cdot,\\cdot)$ 计算的 $1$-Wasserstein距离（推土机距离, Earth Mover's Distance, EMD）。由于相邻顶点的 $D(i,j) = 1$，分母为 $1$ 故省略。\n- 标量特征的单层邻居平均消息传递更新定义为\n$$\nx^{(t+1)}_i = \\sigma\\!\\left(\\frac{x^{(t)}_i + \\sum_{j \\in N(i)} x^{(t)}_j}{d_i + 1}\\right),\n$$\n其中 $\\sigma(z) = \\max(0,z)$ 是修正线性单元（rectified linear unit）非线性激活函数。此更新在平均中包含一个自环，并迭代 $T$ 层。\n- 在源顶点 $S \\subset V$ 处施加强度为 $\\{\\lambda_s\\}_{s \\in S}$ 的配体扰动时，指定目标顶点 $v^\\star$ 处通路激活的基准真值代理（ground-truth proxy）由指数衰减的路径影响力给出\n$$\ny^\\text{true} = \\sum_{s \\in S} \\lambda_s \\, \\beta^{D(s, v^\\star)},\n$$\n其中衰减参数 $\\beta \\in (0,1)$ 是在重连前的原始图上测量的。预测的激活值为 $y^\\text{pred} = x^{(T)}_{v^\\star}$，由消息传递规则得到。\n\n您必须实现一个程序，对每个测试用例，纯粹地通过算法执行以下所有步骤：\n\n$1.$ 构建指定的信号图 $G$，并计算所有顶点对之间的最短路径距离 $D(u,v)$。\n\n$2.$ 对于每条边 $(i,j) \\in E$，通过求解定义 $W_1(m_i, m_j)$ 的离散最优输运问题来精确计算Ollivier-Ricci曲率 $\\kappa_{ij}$，该问题使用地面度量 $D(\\cdot,\\cdot)$ 并通过线性规划求解。\n\n$3.$ 曲率引导的重连：对于给定的阈值 $\\tau$，识别出曲率 $\\kappa_{ij}  \\tau$ 的边集。通过在这些负曲率边周围闭合三角形来形成一个候选新边集，具体来说，考虑那些尚未存在于 $E$ 中的边 $(u,j)$（其中 $u \\in N(i) \\setminus \\{j\\}$）和边 $(i,v)$（其中 $v \\in N(j) \\setminus \\{i\\}$）。按候选边端点之间的距离 $D(\\cdot,\\cdot)$ 降序（以优先选择更长的快捷方式），然后按端点度之和 $d_u + d_v$ 升序（以避免连接到中心节点）对候选边进行排序，并添加最多 $K$ 条新边，形成重连后的图 $\\tilde{G}$。\n\n$4.$ 在原始图 $G$ 上，使用给定的衰减参数 $\\beta$ 和源 $S$ 处的配体强度 $\\{\\lambda_s\\}$，计算目标 $v^\\star$ 处的基准真值激活 $y^\\text{true}$。\n\n$5.$ 使用 $T$ 层邻居平均消息传递更新，在图 $G$ 上计算预测激活值 $y^\\text{pred}_\\text{before}$，在图 $\\tilde{G}$ 上计算 $y^\\text{pred}_\\text{after}$。初始特征为：对于 $s \\in S$， $x^{(0)}_s = \\lambda_s$；对于 $i \\notin S$，$x^{(0)}_i = 0$。\n\n$6.$ 将改进报告为绝对误差的有符号差值，\n$$\n\\Delta = \\left|y^\\text{pred}_\\text{after} - y^\\text{true}\\right| - \\left|y^\\text{pred}_\\text{before} - y^\\text{true}\\right|.\n$$\n负的 $\\Delta$ 值表示重连后有所改进。\n\n您的程序必须处理以下测试套件，该套件旨在探测典型情况、瓶颈情况和已良好连接的情况：\n\n- 测试用例 1（理想路径，带有少量源的长链）：\n  - 图：一个由12个顶点组成的简单链，标记为 $0,1,\\dots,11$，边为 $(i,i+1)$，其中 $i \\in \\{0,\\dots,10\\}$。\n  - 源和强度：$S = \\{0,1,2\\}$，$\\{\\lambda_0,\\lambda_1,\\lambda_2\\} = \\{1.0, 0.6, 0.4\\}$。\n  - 目标：$v^\\star = 11$。\n  - 衰减：$\\beta = 0.8$。\n  - 消息传递层数：$T = 12$。\n  - 曲率阈值和重连预算：$\\tau = -0.1$，$K = 4$。\n\n- 测试用例 2（带有多个源的瓶颈级联）：\n  - 图：一个由16个顶点组成的简单链，标记为 $0,1,\\dots,15$，边为 $(i,i+1)$，其中 $i \\in \\{0,\\dots,14\\}$。\n  - 源和强度：$S = \\{0,1,2,3,4\\}$，$\\{\\lambda_0,\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4\\} = \\{1.0, 0.7, 0.5, 0.3, 0.2\\}$。\n  - 目标：$v^\\star = 15$。\n  - 衰减：$\\beta = 0.85$。\n  - 消息传递层数：$T = 16$。\n  - 曲率阈值和重连预算：$\\tau = -0.05$，$K = 6$。\n\n- 测试用例 3（已良好连接的局部结构，预期变化最小）：\n  - 图：一个由8个顶点组成的链，标记为 $0,1,\\dots,7$，边为 $(i,i+1)$，其中 $i \\in \\{0,\\dots,6\\}$，以及一条额外的边 $(3,5)$，在 $\\{3,4,5\\}$ 上形成一个三角形。\n  - 源和强度：$S = \\{0,1\\}$，$\\{\\lambda_0,\\lambda_1\\} = \\{1.0, 0.5\\}$。\n  - 目标：$v^\\star = 7$。\n  - 衰减：$\\beta = 0.9$。\n  - 消息传递层数：$T = 8$。\n  - 曲率阈值和重连预算：$\\tau = -0.2$，$K = 3$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序列出结果，例如 $[r_1,r_2,r_3]$，其中每个 $r_i$ 是测试用例 $i$ 的 $\\Delta$ 的浮点数值。不涉及物理单位、角度或百分比；所有输出都是以十进制形式表示的无单位实数。", "solution": "用户要求实现一个六步计算流程，用于分析和重连生物信号网络的图表示。问题的核心是使用Ollivier-Ricci曲率（ORC）诊断网络瓶颈，通过曲率引导的加边过程缓解这些瓶颈，并评估此重连操作对图神经网络（GNN）预测任务的影响。\n\n该解决方案通过对每个测试用例以算法方式执行六个指定步骤来进行。\n\n**步骤 1：图表示与所有顶点对之间的最短路径**\n\n信号网络是一个无向、无权的图 $G=(V, E)$。它可以使用邻接表来表示，这对于测试套件中提供的稀疏图是高效的。为了计算Wasserstein距离的地面度量和基准真值激活，我们需要所有顶点对 $(u,v) \\in V \\times V$ 之间的最短路径距离 $D(u,v)$。对于此处给定的小型无权图，Floyd-Warshall算法提供了一种直接计算所有顶点对之间最短路径的方法。该算法初始化一个距离矩阵，其中对于边 $(i,j) \\in E$，$D(i,i)=0$，$D(i,j)=1$，否则 $D(i,j)=\\infty$。然后，它通过中间顶点迭代地松弛这些距离：\n$$\nD(i,j) = \\min(D(i,j), D(i,k) + D(k,j))\n$$\n对于所有中间顶点 $k \\in V$。\n\n**步骤 2：Ollivier-Ricci曲率（ORC）计算**\n\n边 $(i,j) \\in E$ 的ORC定义为 $\\kappa_{ij} = 1 - W_1(m_i, m_j)$，其中 $W_1$ 是两个概率分布 $m_i$ 和 $m_j$ 之间的 $1$-Wasserstein距离（或推土机距离）。分布 $m_i$ 是顶点 $i$ 邻域 $N(i)$ 上的一个均匀测度，为每个邻居 $u \\in N(i)$ 分配 $1/d_i$ 的概率质量，其中 $d_i = |N(i)|$ 是 $i$ 的度。分布 $m_j$ 的定义与此类似。\n\nWasserstein距离 $W_1(m_i, m_j)$ 是一个最优输运问题的解，该问题可以被表述为一个线性规划（LP）问题。我们寻求一个输运方案 $\\pi = (\\pi_{uv})$，它能最小化总运输成本，其中将单位质量从邻居 $u \\in N(i)$ 移动到邻居 $v \\in N(j)$ 的成本是图距离 $D(u,v)$。\n\n该线性规划问题表述为：\n$$\n\\text{最小化} \\quad \\sum_{u \\in N(i)} \\sum_{v \\in N(j)} D(u,v) \\pi_{uv}\n$$\n受以下约束：\n1.  对于所有 $u \\in N(i), v \\in N(j)$，$\\pi_{uv} \\ge 0$ (非负性)。\n2.  对于所有 $u \\in N(i)$，$\\sum_{v' \\in N(j)} \\pi_{uv'} = m_i(u) = 1/d_i$ (源处质量守恒)。\n3.  对于所有 $v \\in N(j)$，$\\sum_{u' \\in N(i)} \\pi_{u'v} = m_j(v) = 1/d_j$ (目标处质量守恒)。\n\n这个LP问题可以使用标准的科学计算库（如 `scipy.optimize.linprog`）来求解。目标函数的最终最小值即为 $W_1(m_i, m_j)$。对图 $G$ 中的每条边都执行此计算。\n\n对于测试用例1和2中的简单链图，任何边的曲率 $\\kappa_{ij}$ 都恰好为 $0$。这是无限一维晶格的一个已知结论，并且对这些有限链也成立。例如，在链内部的边 $(i, i+1)$ 上，$N(i) = \\{i-1, i+1\\}$ 且 $N(i+1) = \\{i, i+2\\}$。最优输运方案将质量从 $i-1$ 移动到 $i$，从 $i+1$ 移动到 $i+2$，得出 $W_1=1$，因此 $\\kappa = 1-1=0$。\n\n**步骤 3：曲率引导的重连**\n\n过挤压与负曲率边相关，这些边充当瓶颈。重连策略旨在缓解这些瓶颈。首先，我们对于给定的阈值 $\\tau$ 识别出边集 $E_{neg} = \\{(i,j) \\in E \\mid \\kappa_{ij}  \\tau\\}$。\n\n对于每个负曲率边 $(i,j) \\in E_{neg}$，我们生成一个候选边集以添加到图中。这些候选边在边 $(i,j)$ 周围“闭合三角形”，形成快捷方式。候选边的形式为 $(u,j)$（其中 $u \\in N(i) \\setminus \\{j\\}$）和 $(i,v)$（其中 $v \\in N(j) \\setminus \\{i\\}$），前提是它们尚未存在于 $E$ 中。\n\n收集所有唯一的候选边然后进行排序。主要排序标准是端点之间的最短路径距离 $D(u,v)$，按降序排列，以优先考虑长程快捷方式。次要标准是端点原始度的总和 $d_u+d_v$，按升序排列，以倾向于连接非中心节点。从这个排序列表中选取前 $K$ 条边添加到 $G$ 中，创建重连后的图 $\\tilde{G} = (V, E \\cup E_{new})$。\n\n**步骤 4：基准真值激活计算**\n\n基准真值 $y^\\text{true}$ 作为GNN预测的基准。它在原始图 $G$ 上计算，是来自源顶点 $S \\subset V$ 的影响力的加权和：\n$$\ny^\\text{true} = \\sum_{s \\in S} \\lambda_s \\beta^{D(s, v^\\star)}\n$$\n其中 $\\{\\lambda_s\\}$ 是源处的信号强度，$v^\\star$ 是目标顶点，$\\beta \\in (0,1)$ 是衰减因子，$D(s, v^\\star)$ 是步骤1中计算的最短路径距离。\n\n**步骤 5：通过GNN消息传递预测激活**\n\n目标顶点的预测激活是通过模拟一个简单的GNN $T$ 层得到的。初始特征向量 $x^{(0)}$ 设置为：对于源顶点 $s \\in S$，$x^{(0)}_s = \\lambda_s$，否则为 $0$。然后对 $t = 0, \\dots, T-1$ 迭代更新特征：\n$$\nx^{(t+1)}_i = \\sigma\\!\\left(\\frac{x^{(t)}_i + \\sum_{j \\in N(i)} x^{(t)}_j}{d_i + 1}\\right)\n$$\n其中 $\\sigma(z) = \\max(0,z)$ 是ReLU激活函数。分母包含一个自环。此模拟执行两次：一次在原始图 $G$ 上以获得 $y^\\text{pred}_\\text{before} = x^{(T)}_{v^\\star}$，另一次在重连后的图 $\\tilde{G}$ 上以获得 $y^\\text{pred}_\\text{after} = x^{(T)}_{v^\\star}$。请注意，第二次模拟的邻域 $N(i)$ 和度 $d_i$ 必须基于 $\\tilde{G}$。\n\n**步骤 6：报告改进情况**\n\n由重连带来的改进通过绝对预测误差的变化来量化：\n$$\n\\Delta = \\left|y^\\text{pred}_\\text{after} - y^\\text{true}\\right| - \\left|y^\\text{pred}_\\text{before} - y^\\text{true}\\right|\n$$\n负的 $\\Delta$ 值表示重连过程通过减少相对于基准真值的误差，改善了GNN的预测。最终输出是每个测试用例的这些 $\\Delta$ 值的列表。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\nclass GraphSignalAnalyzer:\n    \"\"\"\n    Implements the full analysis pipeline for a single graph test case.\n    \"\"\"\n    def __init__(self, num_vertices, edges, sources, lambdas, v_star, beta, T, tau, K):\n        self.num_vertices = num_vertices\n        self.edges = frozenset(tuple(sorted(edge)) for edge in edges)\n        self.sources = sources\n        self.lambdas = lambdas\n        self.v_star = v_star\n        self.beta = beta\n        self.T = T\n        self.tau = tau\n        self.K = K\n\n        # Step 1: Build graph structure and compute all-pairs shortest paths\n        self.adj = {i: [] for i in range(num_vertices)}\n        for u, v in self.edges:\n            self.adj[u].append(v)\n            self.adj[v].append(u)\n        self.degrees = {i: len(self.adj[i]) for i in range(num_vertices)}\n        self.dists = self._compute_apsp()\n\n    def _compute_apsp(self):\n        \"\"\"Computes all-pairs shortest paths using the Floyd-Warshall algorithm.\"\"\"\n        d = np.full((self.num_vertices, self.num_vertices), np.inf)\n        np.fill_diagonal(d, 0)\n        for u, v in self.edges:\n            d[u, v] = 1\n            d[v, u] = 1\n        \n        for k in range(self.num_vertices):\n            for i in range(self.num_vertices):\n                for j in range(self.num_vertices):\n                    d[i, j] = min(d[i, j], d[i, k] + d[k, j])\n        return d\n\n    def _compute_orc(self, i, j):\n        \"\"\"Computes Ollivier-Ricci curvature for an edge (i, j) via linear programming.\"\"\"\n        Ni = self.adj[i]\n        Nj = self.adj[j]\n        di, dj = len(Ni), len(Nj)\n\n        if di == 0 or dj == 0:\n            return 1.0\n\n        # Create LP problem for 1-Wasserstein distance.\n        cost = np.array([self.dists[u, v] for u in Ni for v in Nj])\n        \n        A_eq = np.zeros((di + dj, di * dj))\n        b_eq = np.concatenate([np.full(di, 1/di), np.full(dj, 1/dj)])\n        \n        for row in range(di): # Source constraints\n            for col in range(dj):\n                A_eq[row, row * dj + col] = 1\n        \n        for row in range(dj): # Target constraints\n            for col in range(di):\n                A_eq[di + row, col * dj + row] = 1\n        \n        res = linprog(c=cost, A_eq=A_eq, b_eq=b_eq, bounds=(0, None), method='highs')\n        \n        if not res.success:\n            return np.nan # Should not happen in this problem\n\n        wasserstein_dist = res.fun\n        return 1.0 - wasserstein_dist\n\n    def _run_gnn(self, adj, T, initial_features):\n        \"\"\"Simulates the GNN message passing for T layers.\"\"\"\n        x = initial_features.copy()\n        degrees = {i: len(adj[i]) for i in range(self.num_vertices)}\n        \n        for _ in range(T):\n            x_next = np.zeros_like(x)\n            for i in range(self.num_vertices):\n                neighbor_sum = sum(x[j] for j in adj[i])\n                if degrees[i] == -1: # Unused, degrees[i] >= 0\n                    agg = 0\n                else:\n                    agg = (x[i] + neighbor_sum) / (degrees[i] + 1)\n                x_next[i] = max(0, agg)\n            x = x_next\n        \n        return x[self.v_star]\n\n    def run_analysis(self):\n        \"\"\"Executes the entire 6-step pipeline and returns the improvement score delta.\"\"\"\n        # Step 2: Compute ORC for all edges\n        curvatures = {edge: self._compute_orc(edge[0], edge[1]) for edge in self.edges}\n\n        # Step 3: Curvature-guided rewiring\n        neg_curv_edges = [edge for edge, k in curvatures.items() if k  self.tau]\n        \n        candidate_edges = set()\n        for i, j in neg_curv_edges:\n            for u in self.adj[i]:\n                if u != j and tuple(sorted((u, j))) not in self.edges:\n                    candidate_edges.add(tuple(sorted((u, j))))\n            for v in self.adj[j]:\n                if v != i and tuple(sorted((i, v))) not in self.edges:\n                    candidate_edges.add(tuple(sorted((i, v))))\n\n        sorted_candidates = sorted(\n            list(candidate_edges),\n            key=lambda e: (-self.dists[e[0], e[1]], self.degrees[e[0]] + self.degrees[e[1]], e[0], e[1])\n        )\n        \n        new_edges = sorted_candidates[:self.K]\n        \n        tilde_adj = {i: list(self.adj[i]) for i in range(self.num_vertices)}\n        for u, v in new_edges:\n            tilde_adj[u].append(v)\n            tilde_adj[v].append(u)\n\n        # Step 4: Compute ground truth activation\n        y_true = sum(\n            self.lambdas[s_idx] * (self.beta ** self.dists[s, self.v_star])\n            for s_idx, s in enumerate(self.sources)\n        )\n\n        # Step 5: Compute predicted activations\n        initial_features = np.zeros(self.num_vertices)\n        for s_idx, s in enumerate(self.sources):\n            initial_features[s] = self.lambdas[s_idx]\n            \n        y_pred_before = self._run_gnn(self.adj, self.T, initial_features)\n        y_pred_after = self._run_gnn(tilde_adj, self.T, initial_features)\n        \n        # Step 6: Report improvement\n        error_before = abs(y_pred_before - y_true)\n        error_after = abs(y_pred_after - y_true)\n        delta = error_after - error_before\n        \n        return delta\n\n\ndef solve():\n    \"\"\"\n    Initializes and runs the analysis for each test case specified in the problem.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"num_vertices\": 12, \"edges\": [(i, i + 1) for i in range(11)],\n            \"sources\": [0, 1, 2], \"lambdas\": [1.0, 0.6, 0.4],\n            \"v_star\": 11, \"beta\": 0.8, \"T\": 12, \"tau\": -0.1, \"K\": 4\n        },\n        # Test case 2\n        {\n            \"num_vertices\": 16, \"edges\": [(i, i + 1) for i in range(15)],\n            \"sources\": [0, 1, 2, 3, 4], \"lambdas\": [1.0, 0.7, 0.5, 0.3, 0.2],\n            \"v_star\": 15, \"beta\": 0.85, \"T\": 16, \"tau\": -0.05, \"K\": 6\n        },\n        # Test case 3\n        {\n            \"num_vertices\": 8, \"edges\": [(i, i + 1) for i in range(7)] + [(3, 5)],\n            \"sources\": [0, 1], \"lambdas\": [1.0, 0.5],\n            \"v_star\": 7, \"beta\": 0.9, \"T\": 8, \"tau\": -0.2, \"K\": 3\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        analyzer = GraphSignalAnalyzer(**params)\n        delta = analyzer.run_analysis()\n        results.append(delta)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3317174"}]}