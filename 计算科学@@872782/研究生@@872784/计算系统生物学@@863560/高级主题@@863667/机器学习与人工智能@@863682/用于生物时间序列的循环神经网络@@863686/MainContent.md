## 引言
生物系统，从分子到生态系统，其核心都是动态变化的。理解这些随时间演化的过程是现代生物学的中心任务之一。[生物时间序列](@entry_id:746825)数据——如基因表达谱、生理信号或临床记录——蕴含着关于系统潜在[状态和](@entry_id:193625)调控机制的丰富信息。然而，这些数据往往具有[非线性](@entry_id:637147)、非马尔可夫性、不规则采样和高噪声等复杂特性，给传统线性模型带来了巨大挑战。[循环神经网络](@entry_id:171248)（RNN）作为一类专为处理[序列数据](@entry_id:636380)而设计的深度学习模型，为解决这些难题提供了强有力的框架。

本文旨在系统性地阐述如何应用[循环神经网络](@entry_id:171248)来建模和分析[生物时间序列](@entry_id:746825)。我们不仅会介绍RNN的基础理论，更重要的是，我们将探讨如何调整和扩展这些模型，以应对生物数据特有的挑战，并从中提取有意义的生物学洞见。文章将弥合理论与实践之间的鸿沟，展示RNN如何从一个“黑箱”预测器转变为一个可用于机理探索和因果推断的透明工具。

在接下来的内容中，您将踏上一段从理论到应用的旅程。第一章，**原理与机制**，我们将深入探讨RNN的核心数学原理，将其与生物动力学系统联系起来，并解释[LSTM](@entry_id:635790)和GRU等先进架构如何克服训练挑战。第二章，**应用与[交叉](@entry_id:147634)学科联系**，将展示RNN在基因调控、临床预测和因果推断等前沿领域的实际应用，并探讨其与神经[微分方程](@entry_id:264184)等模型的融合。最后，在**动手实践**部分，您将通过具体计算问题，巩固对RNN[前向传播](@entry_id:193086)和损失函数构建的理解。通过学习本章，您将掌握使用RNN分析复杂生物动力学的理论基础和实用技能。

## 原理与机制

### 使用[状态空间模型](@entry_id:137993)为生物动力学建模

[生物系统](@entry_id:272986)本质上是动态的。从[基因调控网络](@entry_id:150976)中蛋白质浓度的变化，到生态系统中种群数量的波动，其核心特征都可以通过随[时间演化](@entry_id:153943)的[状态变量](@entry_id:138790)来描述。在[计算系统生物学](@entry_id:747636)中，一个强大而普遍的[范式](@entry_id:161181)是将这些系统视为**[状态空间模型](@entry_id:137993)**。该[范式](@entry_id:161181)假设存在一个不可直接观测的**潜状态（latent state）**向量 $h(t)$，它在任意时间 $t$ 捕捉了系统的所有相关信息（例如，分子浓度、细胞活性等）。我们能够测量到的，仅仅是这个潜状态的某种不完整且带有噪声的**观测（observation）** $x(t)$。[@problem_id:3344928]

从第一性原理出发，许多生物过程的演化可以用连续时间的[常微分方程](@entry_id:147024)（ODEs）或随机微分方程（SDEs）来描述。例如，一个调控网络的动力学可以表示为：
$$ \frac{d h(t)}{d t} = F(h(t), u(t)) + \xi(t) $$
在这里，$h(t)$ 是潜状态向量，$u(t)$ 代表外源性输入或扰动（如药物施加），$F$ 是一个[非线性](@entry_id:637147)函数，描述了由生物[化学反应动力学](@entry_id:274455)（如[质量作用定律](@entry_id:144659)）决定的确定性演化规律。随机项 $\xi(t)$ 则概括了系统内在的随机性（如分子数量有限导致的涨落）和外在的未建模扰动。与此同时，观测过程本身也非完美，它将潜状态映射到我们实际测量的数据，并引入测量噪声：$x(t) = H(h(t)) + \text{measurement noise}$。[@problem_id:3344928]

这种潜状态的设定至关重要，因为[生物时间序列](@entry_id:746825)数据往往表现出**非马尔可夫性（non-Markovian behavior）**。马尔可夫性质意味着系统的未来只依赖于其当前状态，而与过去无关。如果我们可以观测到完整的系统状态 $h(t)$，那么这个性质通常是成立的。然而，在实际应用中，我们几乎总是只能观测到系统的一个或几个侧面，即 $x(t)$。在这种**部分可观测性（partial observability）**的情况下，$x(t)$ 的历史包含了关于当前未观测状态变量（$h(t)$ 的其余部分）的关键信息。因此，要预测 $x(t)$ 的未来，仅仅知道其当前值是不够的，我们必须考虑其整个历史。这使得观测序列 $x(t)$ 本身呈现出非马尔可夫性。

此外，生物过程固有的**时滞（delays）**和**反馈（feedback）**回路也是非马尔可夫行为的另一个主要来源。例如，[基因转录](@entry_id:155521)和翻译过程由于聚合酶和[核糖体](@entry_id:147360)的延伸需要有限的时间，从而引入了固有的时滞。一个蛋白质可能在 $t$ 时刻调控其自身基因的转录，但这种调控效应只有在 $t+\tau$ 时刻才能体现出来。这种动力学可以用[时滞微分方程](@entry_id:264784) $\dot{x}(t) = F(x(t), x(t-\tau))$ 来描述。为了预测系统在 $t$ 时刻之后的变化，我们必须知道系统在整个时间区间 $[t-\tau, t]$ 内的历史。同样，系统中存在的**慢变调节因子（slowly varying modulators）**，如[染色质可及性](@entry_id:163510)的变化，会引入非常缓慢的时间尺度，导致[自相关](@entry_id:138991)性延伸到很长的时间滞后，产生**[长程依赖](@entry_id:181727)（long-range dependencies）**。这两种机制都要求模型必须具备记忆能力，能够整合过去的信息来推断当前[状态和](@entry_id:193625)预测未来。[循环神经网络](@entry_id:171248)（RNN）的循环状态正是为了学习并逼近这种对历史的依赖而设计的。[@problem_id:3344948]

### 作为离散化动力学系统的[循环神经网络](@entry_id:171248)

为了在计算机上对连续时间动力学系统进行建模，我们必须将其离散化。一个简单而直观的方法是**前向欧拉法（Forward Euler method）**。对于一个由 $\dot{h}(t) = f(h(t), u(t))$ 描述的系统，我们可以用一个小的步长 $\Delta t$ 来近似其在一个时间步内的变化：
$$ h(t+\Delta t) \approx h(t) + \Delta t \cdot f(h(t), u(t)) $$
令 $h_t$ 表示在离散时间点 $t$ 的状态，这个近似就给出了一个**循环更新法则**：$h_{t+1} = h_t + \Delta t \cdot f(h_t, u_t)$。这个等式揭示了[循环神经网络](@entry_id:171248)（RNN）与动力学系统之间的深刻联系。[@problem_id:3344937] [@problem_id:3344928]

一个**经典（vanilla）[循环神经网络](@entry_id:171248)**可以被看作是上述离散化动力学系统的一个灵活、可学习的[参数化](@entry_id:272587)形式。其核心结构由状态[更新方程](@entry_id:264802)和输出方程定义：
$$ h_t = \phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h) $$
$$ y_t = g(h_t; \theta) $$
在这里，$h_t \in \mathbb{R}^{d_h}$ 是在时间步 $t$ 的隐状态，$x_t \in \mathbb{R}^{d_x}$ 是输入。$W_{xh} \in \mathbb{R}^{d_h \times d_x}$ 和 $W_{hh} \in \mathbb{R}^{d_h \times d_h}$ 分别是输入到隐[状态和](@entry_id:193625)隐状态到自身的权重矩阵，$b_h \in \mathbb{R}^{d_h}$ 是偏置项。$\phi$ 是一个逐元素应用的[非线性](@entry_id:637147)**激活函数**（如 $\tanh$ 或 ReLU），它赋予了模型捕捉[非线性动力学](@entry_id:190195)的能力。$g$ 是一个从隐状态到最终输出 $y_t$ 的映射。至关重要的是，这些参数 $\theta = \{W_{xh}, W_{hh}, b_h, \dots\}$ 在所有时间步之间是**共享**的，这反映了生物系统底层动力学规律的[时间不变性](@entry_id:198838)。[@problem_id:3344968]

这种结构与经典的**自回归（Autoregressive, AR）模型**形成鲜明对比。一个 $p$ 阶的[AR模型](@entry_id:189434)直接用过去 $p$ 个观测值的线性组合来预测当前观测值：$x_t = \sum_{i=1}^{p} A_i x_{t-i} + \varepsilon_t$。[AR模型](@entry_id:189434)不包含独立的潜状态，其“记忆”仅限于最近的 $p$ 个观测值，并且其动力学是线性的。而RNN通过其潜状态 $h_t$，理论上可以通过[非线性](@entry_id:637147)递归压缩和编码任意长的历史信息，从而能够模拟更为复杂的依赖关系。[@problem_id:3344968]

将RNN视为离散动力学系统也为分析其**稳定性**提供了理论工具。考虑一个稳定输入 $u^*$ 下的[不动点](@entry_id:156394) $h^*$，满足 $h^* = \phi(W_{hh}h^* + W_{xh}u^* + b_h)$。系统的局部动力学由状态[更新函数](@entry_id:275392)关于 $h$ 的**[雅可比矩阵](@entry_id:264467)（Jacobian matrix）** $J$ 决定。对于通过[欧拉法](@entry_id:749108)离散化的连续系统 $\dot{h} = f(h, u)$，其离散更新映射为 $h_{t+1} = h_t + \Delta t \cdot f(h_t, u_t)$。在[不动点](@entry_id:156394) $h^*$（即 $f(h^*)=0$）附近，扰动 $\delta h_t = h_t - h^*$ 的演化近似为 $\delta h_{t+1} = (I + \Delta t J) \delta h_t$，其中 $J = \frac{\partial f}{\partial h}|_{h^*}$。为了使该离散系统局部渐近稳定，[迭代矩阵](@entry_id:637346) $M = I + \Delta t J$ 的所有[特征值](@entry_id:154894)的模必须严格小于1，即 $|\mu_i| = |1 + \Delta t \lambda_i| \lt 1$，其中 $\lambda_i$ 是 $J$ 的[特征值](@entry_id:154894)。

例如，考虑一个由以下[雅可比矩阵](@entry_id:264467)描述的二维[基因调控回路](@entry_id:749823)的线性化动力学 [@problem_id:3344937]：
$$ J = \begin{pmatrix} -1  & 2 \\ -3  & -1.5 \end{pmatrix} $$
该矩阵的[特征值](@entry_id:154894)为 $\lambda_{1,2} = -1.25 \pm i \frac{\sqrt{23.75}}{2}$。[连续时间系统](@entry_id:276553)是稳定的，因为[特征值](@entry_id:154894)的实部为负。为了找到离散化后保持稳定的最大时间步长 $\Delta t_{\max}$，我们应用稳定性条件 $|1 + \Delta t \lambda| \lt 1$。这要求 $\Delta t \lambda$ 必须位于复平面上以 $-1$ 为中心、半径为 $1$ 的圆盘内。对于[复特征值](@entry_id:156384) $\lambda = x+iy$，该条件化简为 $\Delta t < \frac{-2\mathrm{Re}(\lambda)}{|\lambda|^2}$。计算可得 $\mathrm{Re}(\lambda)=-1.25$，$|\lambda|^2 = \det(J) = 7.5$。因此，最大稳定步长为：
$$ \Delta t_{\max} = \frac{-2(-1.25)}{7.5} = \frac{2.5}{7.5} = \frac{1}{3} \approx 0.3333 \text{ 小时} $$
这个计算明确地表明，将连续生物动力学转化为离散RNN模型时，[数值稳定性](@entry_id:146550)是必须考虑的关键因素，它限制了模型可以模拟的时间步长。

### 训练RNN：时间反向传播及其挑战

RNN的参数是通过梯度下降法，最小化在整个序列上的总[损失函数](@entry_id:634569) $L = \sum_{t=1}^{T} \ell(\hat{y}_t, y_t)$ 来学习的。计算损失函数对共享参数 $\theta$ 的梯度所使用的算法被称为**时间[反向传播](@entry_id:199535)（Backpropagation Through Time, [BPTT](@entry_id:633900)）**。[BPTT](@entry_id:633900)的本质是将RNN在时间维度上“展开”成一个[深度前馈网络](@entry_id:635356)，其中每一层对应一个时间步，然后应用标准的[反向传播算法](@entry_id:198231)。[@problem_id:3345013]

为了理解[BPTT](@entry_id:633900)的核心机制及其带来的挑战，我们来推导损失 $L$ 对循环权重矩阵 $W_{hh}$ 的梯度。根据[链式法则](@entry_id:190743)，该梯度是每个时间步贡献的总和：
$$ \frac{\partial L}{\partial W_{hh}} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_t} \frac{\partial h_t}{\partial W_{hh}} $$
这里的关键在于计算 $\frac{\partial L}{\partial h_t}$。隐状态 $h_t$ 通过两条路径影响总损失：一是直接通过当前时间步的输出 $\hat{y}_t$；二是通过影响下一个隐状态 $h_{t+1}$，间接影响后续所有时间步的损失。这导致了一个反向的递归关系：
$$ \frac{\partial L}{\partial h_t} = \left(\frac{\partial \hat{y}_t}{\partial h_t}\right)^T \frac{\partial L_t}{\partial \hat{y}_t} + \left(\frac{\partial h_{t+1}}{\partial h_t}\right)^T \frac{\partial L}{\partial h_{t+1}} $$
展开这个递归，我们可以看到，从未来某个时间步 $k$ 的损失回传到当前时间步 $t$ ($k > t$) 的梯度信号，必须经过一个[雅可比矩阵](@entry_id:264467)的连乘：
$$ \frac{\partial h_k}{\partial h_t} = \frac{\partial h_k}{\partial h_{k-1}} \frac{\partial h_{k-1}}{\partial h_{k-2}} \cdots \frac{\partial h_{t+1}}{\partial h_t} = \prod_{j=t+1}^{k} \frac{\partial h_j}{\partial h_{j-1}} $$
对于经典RNN，$\frac{\partial h_j}{\partial h_{j-1}} = D_j W_{hh}$，其中 $D_j = \operatorname{diag}(\phi'(a_j))$ 是激活函数导数构成的对角矩阵。因此，梯度计算的核心部分涉及形如 $(D_k W_{hh}) \cdots (D_{t+1} W_{hh})$ 的长链条乘积。[@problem_id:3344927]

这个连乘结构是**梯度消失（vanishing gradients）**和**[梯度爆炸](@entry_id:635825)（exploding gradients）**问题的根源。如果我们粗略地看这个乘积的范数，它将大致按 $(\beta \|W_{hh}\|)^{k-t}$ 的速率缩放，其中 $\beta$ 是激活函数导数的[上界](@entry_id:274738)，$\|W_{hh}\|$ 是循环权重的范数（更准确地说是[谱半径](@entry_id:138984) $\rho(W_{hh})$）。
*   如果 $\beta \rho(W_{hh}) \lt 1$，梯度信号在[反向传播](@entry_id:199535)过程中将呈指数级衰减，导致模型无法学习到[长程依赖](@entry_id:181727)关系，因为来自遥远过去的信号几乎为零。
*   如果 $\beta \rho(W_{hh}) \gt 1$，梯度信号将呈指数级增长，导致数值不稳定和训练发散。

为了处理长达数千甚至数万个时间步的[生物序列](@entry_id:174368)，完整的[BPTT](@entry_id:633900)在计算和内存上都变得不可行，因为它要求存储整个序列的[前向传播](@entry_id:193086)激活值，内存和计算成本均为 $O(T)$。一个实用的替代方案是**截断时间[反向传播](@entry_id:199535)（Truncated [BPTT](@entry_id:633900), T[BPTT](@entry_id:633900)）**。T[BPTT](@entry_id:633900)将反向传播的路径限制在最近的 $K$ 个时间步。虽然隐状态本身会无限地向前传播，但[梯度流](@entry_id:635964)在每 $K$ 步后被“截断”或“分离”。这使得每个梯度更新的内存和计算成本降低到 $O(K)$。然而，这种效率的代价是引入了**梯度偏差**：模型无法通过梯度直接学习超过 $K$ 步的依赖关系，因为相关的梯度项被忽略了。[@problem_id:3345013]

### 用于捕捉[长程依赖](@entry_id:181727)的先进架构

经典RNN的梯度问题和[有限记忆](@entry_id:136984)促使了更先进的门控RNN架构的发展，它们是现代处理[生物时间序列](@entry_id:746825)的基石。

#### [长短期记忆网络](@entry_id:635790)（[LSTM](@entry_id:635790)）

**[长短期记忆网络](@entry_id:635790)（Long Short-Term Memory, [LSTM](@entry_id:635790)）**通过引入一个独立的**细胞状态（cell state）** $c_t$ 和三个**门（gates）**——输入门 $i_t$、[遗忘门](@entry_id:637423) $f_t$ 和[输出门](@entry_id:634048) $o_t$——来解决[梯度消失问题](@entry_id:144098)。细胞状态 $c_t$ 就像一条信息高速公路，信息可以在其上传输，只受到门的轻微线性交互影响。

[LSTM](@entry_id:635790)的[更新方程](@entry_id:264802)如下 [@problem_id:3344942]：
$$ i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + b_i) \quad \text{(输入门)} $$
$$ f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1} + b_f) \quad \text{(遗忘门)} $$
$$ o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1} + b_o) \quad \text{(输出门)} $$
$$ \tilde{c}_t = \tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c) \quad \text{(候选细胞状态)} $$
$$ c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \quad \text{(细胞状态更新)} $$
$$ h_t = o_t \odot \tanh(c_t) \quad \text{(隐状态更新)} $$
这里，$\sigma$ 是 sigmoid 函数，输出在 $(0, 1)$ 之间，$\odot$ 表示逐元素乘积。

*   **[遗忘门](@entry_id:637423)** $f_t$ 决定从前一个细胞状态 $c_{t-1}$ 中丢弃多少信息。
*   **输入门** $i_t$ 决定将多少新的候选信息 $\tilde{c}_t$ 存入细胞状态。
*   **[输出门](@entry_id:634048)** $o_t$ 决定细胞状态中的哪些信息将被输出到隐状态 $h_t$。

[LSTM](@entry_id:635790)缓解梯度消失的关键在于其细胞状态的加性更新机制。在反向传播时，从 $c_t$ 到 $c_{t-1}$ 的梯度路径非常直接：$\frac{\partial c_t}{\partial c_{t-1}} = f_t$。这意味着梯度信号在时间上传播时，主要经历的是与[遗忘门](@entry_id:637423)的值的逐元素相乘。它绕过了与循环权重矩阵 $W_{hh}$ 的反复相乘，从而避免了经典RNN中主要的梯度衰减来源。如果网络学会将[遗忘门](@entry_id:637423) $f_t$ 的值保持在接近 $1$，梯度就可以几乎无损地流过很长的时间步。这个机制被称为**恒定误差传送带（constant error carousel）**，是[LSTM](@entry_id:635790)能够学习[长程依赖](@entry_id:181727)的核心。[@problem_id:3344942]

#### [门控循环单元](@entry_id:636742)（GRU）

**[门控循环单元](@entry_id:636742)（Gated Recurrent Unit, GRU）**是[LSTM](@entry_id:635790)的一个流行变体，它结构更简单，计算效率更高。GRU将细胞[状态和](@entry_id:193625)隐状态合并为一个单一的隐状态 $h_t$，并只使用两个门：[重置门](@entry_id:636535) $r_t$ 和[更新门](@entry_id:636167) $z_t$。[@problem_id:3344943]

GRU的[更新方程](@entry_id:264802)为：
$$ z_t = \sigma(W_{xz}x_t + W_{hz}h_{t-1} + b_z) \quad \text{(更新门)} $$
$$ r_t = \sigma(W_{xr}x_t + W_{hr}h_{t-1} + b_r) \quad \text{(重置门)} $$
$$ \tilde{h}_t = \tanh(W_{xh}x_t + W_{hh}(r_t \odot h_{t-1}) + b_h) \quad \text{(候选隐状态)} $$
$$ h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t \quad \text{(隐状态更新)} $$

*   **[重置门](@entry_id:636535)** $r_t$ 决定在计算候选隐状态 $\tilde{h}_t$ 时，要忽略多少过去的信息（来自 $h_{t-1}$）。
*   **[更新门](@entry_id:636167)** $z_t$ 类似于[LSTM](@entry_id:635790)中[遗忘门](@entry_id:637423)和输入门的结合。它决定了在多大程度上保留旧的隐状态 $h_{t-1}$，以及在多大程度上用新的候选状态 $\tilde{h}_t$ 来更新。

与[LSTM](@entry_id:635790)相比，GRU的参数更少（对于相同的隐层维度，大约是[LSTM](@entry_id:635790)的3/4），因为没有[输出门](@entry_id:634048)且状态向量只有一个。这使得GRU的计算成本更低。在许多任务中，GRU和[LSTM](@entry_id:635790)的性能相当，选择哪一个往往取决于具体的应用和经验验证。[@problem_id:3344943]

### 处理真实世界生物数据的复杂性

将RNN应用于真实的[生物时间序列](@entry_id:746825)，如来自电子健康记录（EHR）或高通量测序实验的数据时，我们必须面对[超越标准模型](@entry_id:161067)假设的几个关键挑战。[@problem_id:3344932]

#### 异[方差](@entry_id:200758)噪声（Heteroscedastic Noise）

与通常假设噪声[方差](@entry_id:200758)恒定的工程传感器数据不同，生物测量的噪声[方差](@entry_id:200758)往往与信号强度本身相关。例如，基于计数的[RNA测序](@entry_id:178187)数据，其[方差](@entry_id:200758)通常随表达水平的增加而增加（如泊松分布中[方差](@entry_id:200758)等于均值）。这种**[异方差性](@entry_id:136378)**意味着，使用标准的[均方误差](@entry_id:175403)（MSE）[损失函数](@entry_id:634569)（它隐式地假设了恒定[方差](@entry_id:200758)的高斯噪声）可能不是最优的。一个更符合统计学原理的方法是让RNN的输出层参数化一个完整的[概率分布](@entry_id:146404)。例如，对于计数数据，RNN可以预测一个**泊松（Poisson）**或**负二项（Negative Binomial）**[分布](@entry_id:182848)的速[率参数](@entry_id:265473) $\lambda_t$；对于可能具有信号依赖噪声的连续测量，RNN可以同时预测[高斯分布](@entry_id:154414)的均值 $\mu_t$ 和[方差](@entry_id:200758) $\sigma_t^2$。这允许模型根据其对信号水平的估计来调整其对观测不确定性的建模。[@problem_id:3344932] [@problem_id:3344928]

#### 不规则采样与缺失值

生物数据的另一个标志性特征是**不规则采样（irregular sampling）**和**缺失值（missingness）**。临床访问不会以固定的频率发生，[多组学](@entry_id:148370)实验可能在不同时间点进行。简单地忽略时间间隔 $\Delta t$ 或用零填充缺失值，会严重误导模型，因为RNN默认假定时间步是等距的。[@problem_id:3344932]

处理这个问题的策略在复杂性和物理真实性上各不相同。
*   **简单方法**：一个直接的改进是将时间间隔 $\Delta t_i = t_i - t_{i-1}$ 作为RNN的一个额外输入特征。对于缺失值，可以使用简单的[插补](@entry_id:270805)方法，如**前向填充（last-observation-carry-forward）**，并额外提供一个二[进制](@entry_id:634389)**掩码（mask）**来告知模型哪些值是真实的，哪些是插补的。然而，这些方法没有为模型在观测之间如何演化提供明确的物理约束。[@problem_id:3344938]

*   **基于动力学的原则性方法**：一个更符合物理直觉的方法是为模型在没有观测数据的时间段内的行为建立明确的动力学模型。例如，我们可以假设潜在的生理状态在没有外部扰动时会向一个[稳态](@entry_id:182458)基线衰减。**带衰减的[门控循环单元](@entry_id:636742)（GRU-D）**模型正是基于此思想。GRU-D明确地利用时间间隔 $\Delta t$ 来对隐状态进行指数衰减：
$$ h(t_{i-1} + \Delta t_i) = h_\infty + (h(t_{i-1}) - h_\infty) \odot \exp(-\boldsymbol{\gamma} \Delta t_i) $$
其中 $h_\infty$ 是一个可学习的[稳态](@entry_id:182458)基线，$boldsymbol{\gamma}$ 是可学习的衰减率向量。这种形式的更新具有理想的**[半群性质](@entry_id:271012)（semigroup property）**：在一个时间段 $\Delta t$ 内的演化只取决于总时长，而与如何将该时段划分为更小的子时段无关。这保证了模型行为的一致性，并使其能够真实地收敛到一个[稳态](@entry_id:182458)，这与许多生理过程的行为一致。在收到新的观测值时，这个衰减后的状态再与新的输入信息结合，进行标准的GRU更新。[@problem_id:3344938]

此外，生物数据中的缺失通常不是**[完全随机缺失](@entry_id:170286)（Missing Completely At Random, MCAR）**。例如，低于检测下限的测量值之所以缺失，恰恰是因为其真实值很低。这种**[非随机缺失](@entry_id:163489)（Missing Not At Random, MNAR）**本身就包含信息。先进的模型可以通过将缺失掩码作为输入，并采用能够处理审查数据（如[检测限](@entry_id:182454)）的损失函数来利用这些信息，从而获得更准确的推断。[@problem_id:3344932]

总之，将[循环神经网络](@entry_id:171248)成功应用于[生物时间序列](@entry_id:746825)，不仅需要选择合适的[网络架构](@entry_id:268981)（如[LSTM](@entry_id:635790)或GRU）来捕捉时间依赖性，还必须仔细考虑并明确地对生物数据特有的统计属性和物理过程进行建模，包括[异方差性](@entry_id:136378)、不规则采样和信息丰富的缺失模式。