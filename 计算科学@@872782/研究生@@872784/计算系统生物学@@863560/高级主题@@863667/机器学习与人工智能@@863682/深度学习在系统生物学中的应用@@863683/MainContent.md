## 引言
随着高通量测序技术的发展，系统生物学已经进入一个由数据驱动的时代。海量的组学数据为我们揭示生命系统的复杂性提供了前所未有的机遇，但同时也带来了巨大的挑战：我们如何从这些高维、嘈杂的数据中提取出有意义的生物学洞见，并构建能够预测系统行为的精确模型？[深度学习](@entry_id:142022)作为一种强大的非[线性建模](@entry_id:171589)工具，为此提供了革命性的解决方案。然而，将其有效应用于生物学问题，并不仅仅是简单地套用现成的算法。真正的挑战与知识空白在于，如何将[深度学习](@entry_id:142022)的强大[表示能力](@entry_id:636759)与系统生物学中根深蒂固的机理知识、因果逻辑和动态特性相结合，从而实现从描述性分析到预测性乃至干预性科学的飞跃。

本文旨在系统性地介绍[深度学习](@entry_id:142022)在系统生物学中的高级应用。我们将通过三个章节，带领读者逐步深入这一[交叉](@entry_id:147634)学科的前沿领域。**第一章：原理与机制**，将奠定理论基础，详细解析用于建模细胞状态、[生物网络](@entry_id:267733)和动态过程的核心[深度学习](@entry_id:142022)[范式](@entry_id:161181)，并探讨因果推断、[不确定性量化](@entry_id:138597)等关键概念。**第二章：应用与交叉学科联系**，将通过一系列真实世界的应用案例，展示这些原理如何被创造性地整合与扩展，以解决[多模态数据](@entry_id:635386)整合、扰动效应预测以及机理知识融合等复杂问题。**第三章：动手实践**，将提供具体的计算练习，让读者亲手实现和体验文章中讨论的关键思想，从而将理论知识转化为实践能力。

## 原理与机制

[深度学习](@entry_id:142022)的原理与机制为我们理解和建模复杂的生物系统提供了强大的新[范式](@entry_id:161181)。本章将深入探讨这些核心原理，阐明基本模型架构如何与生物学领域的先验知识相结合，以解决系统生物学中的关键挑战。我们将从基础的建模[范式](@entry_id:161181)出发，逐步深入到用于表示细胞状态、网络互作和动态过程的特定深度学习模型。此外，我们还将探讨一些高级但至关重要的主题，包括因果推断、[不确定性量化](@entry_id:138597)以及在面对技术变异时保证[模型鲁棒性](@entry_id:636975)的策略。这些原理共同构成了在生物数据上进行严谨、可解释和预测性建[模的基](@entry_id:156416)石。

### 深度学习在系统生物学中的核心建模[范式](@entry_id:161181)

在系统生物学中，[深度学习](@entry_id:142022)的应用可以根据其目标和所用数据的性质，大致归类为三种主要的学习[范式](@entry_id:161181)：监督学习（Supervised Learning）、[无监督学习](@entry_id:160566)（Unsupervised Learning）和生成式学习（Generative Learning）。理解这些[范式](@entry_id:161181)之间的区别对于选择和设计合适的模型至关重要 [@problem_id:3299349]。

**监督学习** 的目标是学习一个从输入到输出的映射函数，其中输出是已知的、带标签的数据。在系统生物学中，一个典型的监督学习任务是根据单细胞基因表达谱预测特定的细胞表型或通路激活状态。例如，假设我们有一个[单细胞RNA测序](@entry_id:142269)（[scRNA-seq](@entry_id:155798)）数据集，其中一部分细胞通过实验（如[磷酸化蛋白质组学](@entry_id:203908)）被标注了已知的信号通路激活分数。我们的任务是训练一个模型，输入一个细胞的基因表达向量 $X$，输出其对应的通路激活分数 $Y$。这类任务的本质是[经验风险最小化](@entry_id:633880)，其损失函数根据标签的类型而定。对于连续的分数，通常使用**均方误差（Mean Squared Error, MSE）**；对于分类性质的通路状态，则使用**[交叉熵](@entry_id:269529)（Cross-Entropy）**损失。模型的性能通过诸如[决定系数](@entry_id:142674) $R^2$、平均[绝对误差](@entry_id:139354)（Mean Absolute Error, MAE）或[受试者工作特征曲线下面积](@entry_id:636693)（Area Under the Receiver Operating Characteristic Curve, [AUROC](@entry_id:636693)）等指标进行评估 [@problem_id:3299349]。

**[无监督学习](@entry_id:160566)** 则处理没有标签的数据，旨在发现数据中固有的结构、模式或表示。在单细胞生物学中，一个核心的无监督任务是从高维、充满噪声的基因表达数据中学习一个低维的、[去噪](@entry_id:165626)的细胞[状态表示](@entry_id:141201)。这个低维表示，通常称为**[潜变量](@entry_id:143771)（latent variable）** $Z$，能够捕捉细胞间的异质性，例如将具有相似表达模式的细胞在[潜空间](@entry_id:171820)中聚集在一起。自编码器（Autoencoder）及其变体是实现这一目标的常用工具。[模型优化](@entry_id:637432)的目标通常是最小化**重构误差（reconstruction error）**，即确保从[潜变量](@entry_id:143771) $Z$ 解码回的数据与原始输入 $X$ 尽可能一致。为了学习到有意义的结构，通常会加入正则化项，例如促使潜变量[分布](@entry_id:182848)符合特定[先验分布](@entry_id:141376)的**KL散度（Kullback–Leibler divergence）**。评估[无监督学习](@entry_id:160566)任务的性能通常依赖于下游分析，例如通过[聚类](@entry_id:266727)一致性指标（如调整兰德指数 Adjusted Rand Index, ARI）或[轮廓系数](@entry_id:754846)（Silhouette Score）来衡量学习到的表示是否能有效区分不同的细胞类型 [@problem_id:3299349]。

**生成式学习** 的目标是学习数据的内在[概率分布](@entry_id:146404)，从而能够生成新的、与真实数据相似的样本。这比[无监督学习](@entry_id:160566)更进一步，它不仅要发现结构，还要能模拟数据生成过程。在系统生物学中，一个重要的应用是[多模态数据](@entry_id:635386)的**插补（imputation）**。例如，在[空间转录组学](@entry_id:270096)实验中，我们可能测量了空间位置上的基因表达 $S$，但未能测量相应的[蛋白质表达](@entry_id:142703) $R$。一个生成式模型可以学习[联合分布](@entry_id:263960) $p(S, R)$ 或[条件分布](@entry_id:138367) $p(R \mid S)$，从而根据观测到的基因表达来推断或“生成”缺失的蛋白质数据。[变分自编码器](@entry_id:177996)（Variational Autoencoders, VAEs）和[生成对抗网络](@entry_id:634268)（Generative Adversarial Networks, GANs）是实现这一目标的主要框架。其优化目标通常涉及最大化**[证据下界](@entry_id:634110)（Evidence Lower Bound, ELBO）**或解决一个极小极大博弈。评估[生成模型](@entry_id:177561)性能的指标包括在掩蔽数据上的插补准确率（如[均方根误差](@entry_id:170440) Root Mean Squared Error, RMSE）、对数据[分布](@entry_id:182848)[拟合优度](@entry_id:637026)的评估（如留出数据的对数似然）以及新生成的数据是否保留了关键的生物学相关性（如基因-[蛋白质表达](@entry_id:142703)的相关性）[@problem_id:3299349]。

### 用于细胞状态建模的生成式模型：[变分自编码器](@entry_id:177996)

[变分自编码器](@entry_id:177996)（Variational Autoencoder, VAE）是现代[计算生物学](@entry_id:146988)中用于建模单细胞数据的基石模型之一。它不仅能实现[降维](@entry_id:142982)和去噪，还能通过其概率框架捕捉数据的内在[分布](@entry_id:182848)，为下游的分析和假设生成提供坚实的基础。

一个典型的VAE由两部分组成：一个**编码器（encoder）**和一个**解码器（decoder）**。编码器，通常是一个[神经网](@entry_id:276355)络，将高维的输入数据（如一个细胞的基因表达谱 $x$）映射到一个低维的潜空间，但它输出的不是一个单一的点，而是一个[概率分布](@entry_id:146404)的参数，通常是[高斯分布](@entry_id:154414)的均值 $m_{\phi}(x)$ 和[方差](@entry_id:200758) $s_{\phi}(x)^2$。这个[分布](@entry_id:182848) $q_{\phi}(z \mid x)$ 被称为**变分后验（variational posterior）**，它旨在近似“真实”但难以计算的后验分布 $p(z \mid x)$。潜变量 $z$ 从这个[分布](@entry_id:182848)中采样得到，可以被理解为一个细胞的压缩、抽象的生物学[状态表示](@entry_id:141201) [@problem_id:3299354]。

解码器则执行相反的过程，它接收一个潜变量 $z$ 作为输入，并尝试重构原始的高维数据 $x$。关键在于，解码器也定义了一个[概率分布](@entry_id:146404) $p_{\theta}(x \mid z)$，即给定一个细胞状态 $z$，其基因表达谱 $x$ 的概率。这个[分布](@entry_id:182848)的选择至关重要，必须与数据的性质相匹配。对于scRNA-seq的**计数（count）**数据，由于其普遍存在的**过离散（overdispersion）**现象（即[方差](@entry_id:200758)大于均值），使用**[负二项分布](@entry_id:262151)（Negative Binomial, NB）**作为解码器似然是一个符合生物学现实的 principled choice。过离散现象源于转录过程的内在随机性（如[转录爆发](@entry_id:156205)）和测序过程中的技术噪音。[负二项分布](@entry_id:262151)可以被看作一个**伽马-泊松混合模型（Gamma-Poisson mixture）**：每个基因的表达率 $\lambda_g$ 本身是一个服从伽马[分布](@entry_id:182848)的[随机变量](@entry_id:195330)，而实际观测到的计数值则是在此表达率下服从[泊松分布](@entry_id:147769)。这个两层结构自然地导出了[负二项分布](@entry_id:262151)，其[方差](@entry_id:200758)为 $\operatorname{Var}(X_{g}) = \mu_{g} + \mu_{g}^{2}/r_{g}$，其中 $\mu_g$ 是均值，$r_g$ 是逆离散度参数 [@problem_id:3299354]。

VAE的训练目标是最大化观测数据的边际[对数似然](@entry_id:273783) $\log p(x)$，但这通常是难以直接计算的。取而代之，我们最大化其**[证据下界](@entry_id:634110)（Evidence Lower Bound, ELBO）**。ELBO可以被分解为两项 [@problem_id:3299421]：

$$
\mathcal{L}(\theta, \phi; x) = \underbrace{\mathbb{E}_{q_{\phi}(z \mid x)}[\log p_{\theta}(x \mid z)]}_{\text{重构项}} - \underbrace{\operatorname{KL}(q_{\phi}(z \mid x) \,\|\, p(z))}_{\text{正则化项}}
$$

**重构项**是给定潜变量 $z$ 后，观测数据 $x$ 的期望对数似然。在负二项解码器的设定下，它衡量了解码器从 $z$ 重构出原始基因表达计数的准确程度。这一项鼓励编码器将关于 $x$ 的所有重要信息都编码到 $z$ 中。

**正则化项**是变分后验 $q_{\phi}(z \mid x)$ 与一个预设的**先验分布** $p(z)$ 之间的[KL散度](@entry_id:140001)。先验分布通常选择为[标准正态分布](@entry_id:184509) $\mathcal{N}(0, I)$。这一项的作用是正则化[潜空间](@entry_id:171820)的结构，强迫编码器产生的后验分布趋向于[先验分布](@entry_id:141376)。这使得潜空间变得平滑、连续，并以原点为中心。这种结构化的[潜空间](@entry_id:171820)非常有用，因为它确保了相似的细胞在[潜空间](@entry_id:171820)中位置相近，并且我们可以在潜空间中进行插值，这些插值点可以被解码为具有生物学意义的中间细胞状态，从而模拟分化轨迹等连续的生物学过程 [@problem_id:3299354]。对于对角协[方差](@entry_id:200758)的高斯后验和标准正态先验，KL散度项有一个易于计算的解析解：

$$
\mathrm{KL}(q_{\phi}(z \mid x)\,\|\,p(z)) = \frac{1}{2} \sum_{i=1}^{d} \left( s_{\phi,i}(x)^{2} + m_{\phi,i}(x)^{2} - 1 - \ln s_{\phi,i}(x)^{2} \right)
$$

其中 $d$ 是[潜空间](@entry_id:171820)的维度，$m_{\phi,i}$ 和 $s_{\phi,i}$ 是编码器输出的均值和[标准差](@entry_id:153618)的第 $i$ 个分量 [@problem_id:3299421]。

在训练过程中，为了通过期望项 $\mathbb{E}_{q_{\phi}(z \mid x)}[\dots]$ 进行[反向传播](@entry_id:199535)，需要使用**[重参数化技巧](@entry_id:636986)（reparameterization trick）**。我们不直接从 $q_{\phi}(z \mid x)$ 中采样，而是从一个固定的、与参数无关的[分布](@entry_id:182848)（如标准正态分布 $\varepsilon \sim \mathcal{N}(0, I)$）中采样，然后通过一个确定性变换生成 $z$：$z = m_{\phi}(x) + s_{\phi}(x) \odot \varepsilon$。这样，随机性被外化，使得梯度可以无偏地、低[方差](@entry_id:200758)地通过 $m_{\phi}(x)$ 和 $s_{\phi}(x)$ 回传给编码器的参数 $\phi$ [@problem_id:3299421]。

最后，为了处理像[测序深度](@entry_id:178191)这样的技术[协变](@entry_id:634097)量，一个常见的策略是在解码器中将其分离出来。例如，可以将基因的期望表达均值 $\mu_g$ 建模为细胞总文库大小 $l$ 与一个由潜变量 $z$ 决定的相对表达比例 $\mathbf{s}_{\theta}(z)$ 的乘积，即 $\boldsymbol{\mu}_{\theta}(z, l) = l \cdot \mathbf{s}_{\theta}(z)$。这样，潜变量 $z$ 就被鼓励去学习内在的、与[测序深度](@entry_id:178191)无关的生物学状态 [@problem_id:3299354]。

### 建模[生物网络](@entry_id:267733)：图神经网络

[生物系统](@entry_id:272986)本质上是网络化的。基因、蛋白质和其他分子通过复杂的调控、信号和代谢网络相互作用。[图神经网络](@entry_id:136853)（Graph Neural Networks, GNNs）为直接在这些网络结构上学习提供了强大的框架，使得模型能够利用生物学的先验知识来指导[表示学习](@entry_id:634436)。

GNN的核心思想是**[消息传递](@entry_id:751915)（message passing）**。每个节点（例如，一个基因）的表示向量是通过聚合其邻居节点的表示来更新的。经过多层消息传递，一个节点的表示就能捕捉到其在网络中更广泛的邻域信息。然而，标准的GNN架构（如GCN）通常为[无向图](@entry_id:270905)设计，而[生物网络](@entry_id:267733)，特别是基因调控网络，往往是**有向（directed）**和**加权（weighted）**的，其中一条从调控因子到靶基因的有向边代表了因果影响。

为了在这些有向网络上进行有意义的学习，我们需要设计能够区分“上游”和“下游”影响的[图卷积](@entry_id:190378)层。一个基因的生物学角色是双重的：它被其上游的调控因子所调控（**传入影响**），同时它也可能作为调控因子去调控其下游的靶基因（**传出影响**）。一个精心设计的GNN层应该能同时捕捉这两种信息 [@problem_id:3299360]。

考虑一个由[邻接矩阵](@entry_id:151010) $A$ 表示的有向[加权图](@entry_id:274716)，其中 $A_{ij} > 0$ 表示从节点 $i$ 到节点 $j$ 的一条边。节点 $j$ 的加权入度为 $d_{\mathrm{in}}(j) = \sum_{i} A_{ij}$，节点 $i$ 的加权[出度](@entry_id:263181)为 $d_{\mathrm{out}}(i) = \sum_{j} A_{ij}$。我们可以设计一个[图卷积](@entry_id:190378)层，它包含三个部分：一个用于聚合传入消息的项，一个用于聚合传出消息的项，以及一个用于保留自身信息的项。

$$
H^{(\ell+1)} = \sigma\left( \underbrace{D_{\mathrm{in}}^{-1} A^\top H^{(\ell)} W_{\mathrm{in}}}_{\text{传入影响}} + \underbrace{D_{\mathrm{out}}^{-1} A H^{(\ell)} W_{\mathrm{out}}}_{\text{传出影响}} + \underbrace{H^{(\ell)} W_{\mathrm{self}}}_{\text{自身信息}} \right)
$$

其中，$H^{(\ell)}$ 是第 $\ell$ 层的节[点特征](@entry_id:155984)矩阵，$D_{\mathrm{in}}$ 和 $D_{\mathrm{out}}$ 是对角线的[入度和出度](@entry_id:273421)矩阵，$W_{\mathrm{in}}$, $W_{\mathrm{out}}$, $W_{\mathrm{self}}$ 是可训练的权重矩阵，$\sigma$ 是[非线性激活函数](@entry_id:635291)。

让我们解析这个公式[@problem_id:3299360]：
- **传入影响**: 项 $A^\top H^{(\ell)}$ 使用了邻接矩阵的**[转置](@entry_id:142115)** $A^\top$。这在代数上实现了从源节点到目标节点的消息聚合。对于目标节点 $j$，它收集了所有指向它的源节点 $i$ 的特征，并按边权重 $A_{ij}$ 加权求和。然后，通过左乘 $D_{\mathrm{in}}^{-1}$，每个节点聚合到的消息被其总的加权入度所归一化，这可以防止度数大的节点接收到过大的信号。最后，通过一个独立的权重矩阵 $W_{\mathrm{in}}$ 进行线性变换。
- **传出影响**: 项 $A H^{(\ell)}$ 则聚合了从一个源节点出发指向其所有目标节点的消息。这可以被理解为让一个[调控基因](@entry_id:199295)“感知”它所调控的基因群体的状态。然后，这个聚合信息被源节点的[出度](@entry_id:263181) $D_{\mathrm{out}}^{-1}$ 归一化，并通过一个独立的权重矩阵 $W_{\mathrm{out}}$ 进行变换。
- **自身信息**: $H^{(\ell)} W_{\mathrm{self}}$ 项允许每个节点保留一部分其上一层的表示，类似于[残差连接](@entry_id:637548)，这有助于[稳定训练](@entry_id:635987)和传递信息。

通过使用两个独立的[消息传递](@entry_id:751915)通道（由 $A^\top$ 和 $A$ 定义）和两个独立的[变换矩阵](@entry_id:151616)（$W_{\mathrm{in}}$ 和 $W_{\mathrm{out}}$），该模型可以学习到与基因作为“被调控者”和“调控者”两种角色相关的不同特征。这种将网络拓扑和生物学功能直接编码到模型架构中的方法，是深度学习在系统生物学中发挥强大作用的一个典型范例。

### 建模生物动态学：神经普通[微分方程](@entry_id:264184)

[生物过程](@entry_id:164026)本质上是动态的。细胞分化、对药物的反应以及疾病的进展都是随时间演变的复杂过程。神经普通[微分方程](@entry_id:264184)（Neural Ordinary Differential Equations, Neural ODEs）为学习这种连续时间动态系统提供了一个优雅而强大的框架。

传统的动力学模型，如基于质量作用定律的[常微分方程组](@entry_id:266774)，需要对系统中每个相互作用的精确函数形式做出假设。然而，在生物学中，许多调控函数的具体形式是未知的。Neural ODEs通过用[神经网](@entry_id:276355)络来参数化这种未知的动态，从而解决了这个问题。一个Neural ODE的形式为：

$$
\frac{dx(t)}{dt} = f_{\theta}(x(t), u(t), t)
$$

其中，$x(t)$ 是系统在时间 $t$ 的状态（例如，基因表达向量），$f_{\theta}$ 是一个由参数 $\theta$ 定义的[神经网](@entry_id:276355)络，它代表了状态变化的“[速度场](@entry_id:271461)”。$u(t)$ 是一个已知的、随时间变化的外部输入或干预（例如，药物浓度）。给定一个初始状态 $x(t_0)$，我们可以通过[数值积分器](@entry_id:752799)求解这个ODE，来预测系统在任何未来时间 $t$ 的状态 $\hat{x}(t)$。模型的训练目标是调整网络参数 $\theta$，使得积分得到的轨迹 $\hat{x}(t_k)$ 与在离散时间点 $t_k$ 观测到的实验数据 $y_k$ 尽可能匹配。

纯粹的数据驱动方法可能会导致模型缺乏物理解释性且不稳定。一个更强大和可靠的策略是将已知的生物学原理直接构建到Neural ODE的结构中 [@problem_id:3299381]。以基因表达动态为例，一个核心的生物物理原理是**质量平衡（mass-balance）**：mRNA的丰度变化率是其合成速率与降解速率之差。我们可以将此原理嵌入到$f_{\theta}$中：

$$
f_{\theta}(x(t), u(t), t) = \underbrace{s_{\theta}(x(t), u(t), t)}_{\text{合成项}} - \underbrace{\Gamma x(t)}_{\text{降解项}}
$$

在这个[混合模型](@entry_id:266571)中 [@problem_id:3299381]：
- **合成项 $s_{\theta}(x(t), u(t), t)$**: 基因的转录合成是一个受内部状态（其他基因的表达）和外部信号（干预 $u(t)$）共同调控的复杂[非线性](@entry_id:637147)过程。由于其函数形式未知，我们用一个[神经网](@entry_id:276355)络 $s_{\theta}$ 来学习它。这为模型提供了极大的灵活性，使其能够从数据中发现复杂的调控逻辑。
- **降解项 $-\Gamma x(t)$**: mRNA的降解通常可以近似为一级反应，即降解速率与当前mRNA浓度成正比。因此，我们可以用一个线性的、更易于解释的项 $-\Gamma x(t)$ 来建模它，其中 $\Gamma$ 是一个对角矩阵，其对角[线元](@entry_id:196833)素 $\gamma_i > 0$ 代表了基因 $i$ 的降解速率常数。这些常数可以是已知的，也可以作为模型参数进行学习，但必须强制为正以保证物理意义。

这种结构化Neural ODE的设计具有多重优势。首先，它将模型的灵活性集中在最不确定的部分（合成调控），同时对更明确的部分（降解）使用简单、可解释的形式。其次，它明确地将外部干预 $u(t)$ 作为影响系统**动态**（即合成速率）的**因果**驱动因素，这与生物学现实相符，即药物通常通过改变[基因转录](@entry_id:155521)来起作用，而不是改变我们的测量方式。最后，这种结合了领域知识的模型通常更稳定，需要更少的数据进行训练，并且其学习到的参数（如降解率 $\gamma_i$）具有直接的生物学解释。

### 因果与干预：超越关联

在系统生物学中，一个核心目标是理解和预测对系统进行**干预（intervention）**（例如，基因敲除或药物处理）的效果。这需要我们从简单的**关联（association）**学习转向**因果（causal）**推断。一个在观测数据上训练的标准监督学习模型，学习的是[条件概率分布](@entry_id:163069) $p(y \mid x)$，即“在看到 $X$ 取值为 $x$ 的情况下，$Y$ 的[分布](@entry_id:182848)是什么”。然而，预测干预效果需要我们回答一个本质上不同的问题：“如果我们将 $X$ **设定**为 $x$，$Y$ 的[分布](@entry_id:182848)会是什么？” 这对应于干预[分布](@entry_id:182848) $p(y \mid \mathrm{do}(x))$。

这两者之间的差异源于**混杂因素（confounders）**的存在。考虑一个简单的基因调控网络，其中一个上游调控因子 $Z$ 同时影响一个中间基因 $X$ 和一个下游靶基因 $Y$，而 $X$ 也直接影响 $Y$。这个因果图可以表示为 $Z \to X$，$Z \to Y$，$X \to Y$。在这个系统中，$Z$ 是 $X$ 和 $Y$ 之间的一个[共同原因](@entry_id:266381)，即混杂因素 [@problem_id:3299375]。

在一个线性高斯结构因果模型（Structural Causal Model, SCM）中，我们可以将这些关系写成：
$$
\begin{aligned}
Z = N_Z \\
X = a Z + N_X \\
Y = b X + c Z + N_Y
\end{aligned}
$$
其中 $N_Z, N_X, N_Y$ 是独立的噪声项，$a, b, c$ 是因果效应系数。

当我们从观测数据中学习 $X$ 与 $Y$ 的关系时，我们学习的是[条件期望](@entry_id:159140) $E[Y \mid X=x]$。由于 $Z$ 的存在，$X$ 和 $Y$ 之间的相关性包含了两个部分：一条是 $X$ 对 $Y$ 的直接因果路径（系数为 $b$），另一条是通过[共同原因](@entry_id:266381) $Z$ 的“后门路径” $X \leftarrow Z \to Y$。计算表明，观测到的关联强度（即[回归系数](@entry_id:634860)）是这两条路径效应的混合。例如，在特定参数设置下（$a=1, b=0.8, c=0.6$），我们可能得到 $E[Y \mid X=x] = 1.1x$ [@problem_id:3299375]。一个纯粹的关联模型会学习到这个 $1.1$ 的斜率。

然而，当我们进行干预 $\mathrm{do}(X=x)$ 时，例如通过CRISPR技术将基因 $X$ 的表达水平固定为 $x$，我们实际上是切断了所有指向 $X$ 的因果箭头，包括 $Z \to X$ 这条。在干预后的新系统中，后门路径被阻断，$Y$ 的值仅由 $X$ 的设定值和 $Z$ 的独立波动决定。此时的[条件期望](@entry_id:159140)为 $E[Y \mid \mathrm{do}(X=x)] = bx$。在我们的例子中，这将是 $0.8x$ [@problem_id:3299375]。

这个 $1.1$ 和 $0.8$ 之间的差异是至关重要的。一个在观测数据上训练的、未能考虑因果结构的[深度学习模型](@entry_id:635298)，会错误地用 $1.1$ 的斜率去预测干[预实验](@entry_id:172791)的结果，从而导致严重的预测偏差。这凸显了仅靠关联学习的局限性，并强调了开发能够学习或编码[因果结构](@entry_id:159914)的模型的必要性，只有这样的模型才能可靠地预测新颖扰动的效果。

### 高级主题与实践考量

除了核心的模型架构，要在真实世界的生物数据上成功应用[深度学习](@entry_id:142022)，还必须掌握一系列高级概念和实践策略。这些原则确保了模型的可靠性、鲁棒性和最终的科学价值。

#### 不确定性量化

生物数据充满不确定性。一个负责任的预测模型不仅应提供预测值，还应量化其预测的可信度。不确定性可以分为两种类型 [@problem_id:3299348]：

- **[偶然不确定性](@entry_id:154011)（Aleatoric Uncertainty）**：这是数据本身固有的、不可约减的随机性。它源于[生物过程](@entry_id:164026)的内在随机性（如[转录爆发](@entry_id:156205)）和测量过程中的技术噪音。即使拥有无限的数据，这种不确定性也依然存在。
- **认知不确定性（Epistemic Uncertainty）**：这是由模型自身的局限性（例如，训练数据有限）引起的。它反映了模型对其参数的不确定性。随着数据量的增加，这种不确定性可以被减小。

在深度学习模型中，偶然不确定性可以通过设计解码器的输出来捕捉。一个**同[方差](@entry_id:200758)（homoscedastic）**模型假设所有输入的噪声水平都相同，而一个更现实的**异[方差](@entry_id:200758)（heteroscedastic）**模型则允许噪声水平依赖于输入。例如，一个用于预测细胞命运的模型可以设计成输出一个高斯分布的均值 $\mu(x)$ 和[方差](@entry_id:200758) $\sigma^2(x)$。这样，模型就可以学到：对于某些基因表达状态 $x$，其对应的细胞命运结局本身就更加多变和不可预测。对于计数数据，类似的异[方差](@entry_id:200758)可以通过让[负二项分布](@entry_id:262151)的均值 $\mu(x)$ 和[离散度](@entry_id:168823)参数 $r(x)$ 都成为输入的函数来实现，从而使[方差](@entry_id:200758) $\mu(x) + \mu(x)^2 / r(x)$ 随输入变化 [@problem_id:3299348]。认知不确定性则通常通过贝叶斯方法（如[贝叶斯神经网络](@entry_id:746725)）来估计，它学习的是模型参数的[后验分布](@entry_id:145605)，而不仅仅是[点估计](@entry_id:174544)。

#### [解耦](@entry_id:637294)与泛化

在扰动生物学中，一个核心挑战是学习**解耦（disentangled）**的表示，即将细胞的内在状态 $s$ 与其所受扰动 $p$ 的影响分离开来。实现这种解耦是预测细胞对**未曾见过**的新扰动反应的关键 [@problem_id:3299385]。

然而，从观测数据 $x=g(s,p)$ 中唯一地识别出 $s$ 和 $p$ 是一个严峻的**可识别性（identifiability）**问题。没有额外的假设，任何可逆变换 $T$ 都可以产生一组新的、同样能解释数据的[潜变量](@entry_id:143771) $(\hat{s}, \hat{p}) = T(s,p)$。理论和实践表明，实现解耦需要几个关键要素的结合：
1.  **[统计独立性](@entry_id:150300)**：通过**[随机化](@entry_id:198186)实验设计**，确保扰动的分配独立于细胞的初始状态，即 $p \perp s$。这个独立性假设为模型提供了一个强大的约束。
2.  **多样的干预数据**：需要来自多种不同干预（例如，不同药物、不同剂量）的数据。每个干预都为系统提供了一个新的“视角”，这些多样化的视角共同约束了可能的潜在结构，使得[解耦](@entry_id:637294)成为可能。
3.  **结构假设**：模型本身需要具有某种“组合”结构，例如，解码器可以被设计为 $g(s,p) = h(\phi(s), \psi(p))$，其中模型分别学习状态的表示 $\phi(s)$ 和扰动的表示 $\psi(p)$，然后再将它们组合起来。

满足这些条件的模型，如组合式扰动自编码器（Compositional Perturbation Autoencoder, CPA），能够学习到可泛化的扰动效应，并预测新组合的效果 [@problem_id:3299385]。

#### 处理技术变异：[批次效应](@entry_id:265859)

[单细胞组学](@entry_id:151015)实验通常分多个**批次（batch）**进行，不同批次可能由于试剂、仪器或操作人员的差异而引入系统性的技术变异，即**[批次效应](@entry_id:265859)（batch effects）**。这些技术变异会掩盖真实的生物学信号，是数据分析中的一个主要障碍。

从因果角度看，[批次效应](@entry_id:265859)可以被形式化为一个混杂因素。假设细胞的真实生物学状态为 $z$，批次为 $b$，观测到的表达谱为 $x$。其因果图为 $z \to x$ 和 $b \to x$，即生物学[状态和](@entry_id:193625)批次都影响观测值。一个关键的假设是，批次分配与生物学状态无关（$z \perp b$），例如，在每个批次中我们都处理了相同类型的细胞。我们的目标是学习一个对批次 $b$ **不变（invariant）**的生物学表示 $z$ [@problem_id:3299393]。

[深度生成模型](@entry_id:748264)提供了一个强大的解决方案。我们可以使用一个**条件VAE（conditional VAE）**，其编码器 $q_{\phi}(z \mid x, b)$ 和解码器 $p_{\theta}(x \mid z, b)$ 都以批次信息 $b$ 作为条件。编码器利用 $b$ 来“减去”批次效应，从而推断出纯净的生物学状态 $z$。解码器则利用 $z$ 和 $b$ 来精确重构原始的、带有[批次效应](@entry_id:265859)的数据。为了确保学习到的 $z$ 确实与 $b$ 无关，我们可以在ELBO[目标函数](@entry_id:267263)中加入一个**[不变性](@entry_id:140168)正则化项**，例如通过一个对抗性分类器来最小化 $z$ 和 $b$ 之间的互信息 $I(z,b)$。训练完成后，我们就可以通过将所有细胞的 $z$ 输入到解码器，并使用一个固定的参考批次 $b^*$，来生成“批次校正后”的数据 [@problem_id:3299393]。

#### 原则性模型评估

最后，模型的评估方法必须与其预期的应用场景相匹配，并严格避免**[数据泄漏](@entry_id:260649)（data leakage）**。对于旨在预测未来[状态和](@entry_id:193625)新扰动效果的模型，简单的随机[交叉验证](@entry_id:164650)是完全错误的 [@problem_id:3299345]。

一个原则性的数据[划分方案](@entry_id:635750)必须尊重数据中固有的依赖结构，特别是时间和扰动条件。一个严谨的策略是采用**嵌套分组划分（nested grouped split）**：
1.  **测试集划分**：首先，在**扰动**层面进行划分。完全留出一部分扰动 $P_{\text{test}}$ 作为[测试集](@entry_id:637546)。模型在训练和验证阶段完全看不到这些扰动的数据。在测试集上的性能才能反映模型对全新扰动的泛化能力。
2.  **训练/[验证集](@entry_id:636445)划分**：在余下的扰动 $P \setminus P_{\text{test}}$ 中，根据**时间**进行划分。例如，使用较早的时间点进行训练，较晚的时间点进行验证。这模拟了使用历史数据预测未来的真实场景。

此外，划分的“[原子单位](@entry_id:166762)”应该是包含所有内部依赖的组。例如，一个特定扰动、特定时间点、特定细胞类型和特定批次下的所有生物学重复都应该被视为一个不可分割的整体，一同被分到[训练集](@entry_id:636396)或验证集中。这种严谨的划分策略是确保模型评估结果可靠、不产生虚高假象的根本保障 [@problem_id:3299345]。