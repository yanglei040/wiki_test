{"hands_on_practices": [{"introduction": "邻接表是表示稀疏网络（如大多数生物网络）的基石，因为它只存储实际存在的连接，从而高效地利用内存。此练习将引导你直接在这种列表结构上，设计算法来计算节点的入度、出度以及互惠邻居重叠度等基本网络指标[@problem_id:3332690]。掌握这些操作是进行任何更高级网络分析之前，必须具备的核心技能。", "problem": "考虑一个有向基因调控网络，该网络被建模为一个图，其节点集为 $V$，有向边集为 $E$。每个节点代表一个基因，从节点 $i$ 到节点 $j$ 的有向边表示基因 $i$ 对基因 $j$ 进行转录调控。该图由一个邻接表表示，其中对于每个节点 $i$，邻接表 $A[i]$ 包含其出邻居集合 $N_{\\mathrm{out}}(i)$，即满足 $(i,j) \\in E$ 的所有节点 $j$。为确保计算的正确性，$A[i]$ 中的重复边将被忽略（视为单一边），并允许自环 $(i,i)$。\n\n定义：\n- 节点 $i$ 的出度为 $d_{\\mathrm{out}}(i) = |N_{\\mathrm{out}}(i)|$。\n- 节点 $i$ 的入邻居集合为 $N_{\\mathrm{in}}(i) = \\{j \\in V \\mid (j,i) \\in E\\}$，其入度为 $d_{\\mathrm{in}}(i) = |N_{\\mathrm{in}}(i)|$。\n- 节点 $i$ 的互惠邻居重叠大小为 $s(i) = |N_{\\mathrm{out}}(i) \\cap N_{\\mathrm{in}}(i)|$，它计算了既调控 $i$ 又被 $i$ 调控的节点数量。\n- 有向边的总数为 $m = |E|$。\n- 最大度为 $\\Delta_{\\max} = \\max_{i \\in V} \\{\\max(d_{\\mathrm{out}}(i), d_{\\mathrm{in}}(i))\\}$。\n\n任务：\n- 设计并实现一个算法，给定一个包含 $n$ 个节点 $V = \\{0,1,\\dots,n-1\\}$ 的有向图的邻接表表示 $A$，该算法需要为所有节点 $i \\in V$ 计算其入度 $d_{\\mathrm{in}}(i)$、出度 $d_{\\mathrm{out}}(i)$ 和互惠邻居重叠大小 $s(i)$（如上文定义）。\n- 该算法的核心计算必须仅使用邻接表，而非邻接矩阵。\n- 从邻接表和度的基本定义出发，分析算法关于总边数 $m$ 和最大度 $\\Delta_{\\max}$ 的时间复杂度。\n\n为确保通用适用性和可测试性，请使用以下图测试套件。每个测试用例指定节点数 $n$ 和一个包含 $n$ 个条目的邻接表 $A$。对于没有出边的节点索引 $i$，请包含一个空列表。所有节点索引均从零开始。\n\n测试套件：\n- 测试用例 1（混合互惠调控和单向调控的一般情况）：\n  - $n = 5$\n  - $A[0] = [1, 2]$, $A[1] = [2, 3]$, $A[2] = [0, 3]$, $A[3] = [2]$, $A[4] = [0, 3]$\n- 测试用例 2（没有边的边界情况）：\n  - $n = 3$\n  - $A[0] = []$, $A[1] = []$, $A[2] = []$\n- 测试用例 3（包含自环和孤立节点的边缘情况）：\n  - $n = 4$\n  - $A[0] = [0, 1]$, $A[1] = [1, 2]$, $A[2] = []$, $A[3] = [3]$\n- 测试用例 4（具有互惠边的高出度中心节点，用于测试 $\\Delta_{\\max}$ 的影响）：\n  - $n = 6$\n  - $A[0] = [1, 2, 3, 4, 5]$, $A[1] = [0]$, $A[2] = [0]$, $A[3] = [0]$, $A[4] = [0]$, $A[5] = [0]$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由方括号括起来的逗号分隔列表。每个测试用例的结果本身是一个形式为 $[D_{\\mathrm{in}}, D_{\\mathrm{out}}, S, m, \\Delta_{\\max}]$ 的列表。\n- 此处，$D_{\\mathrm{in}}$是入度列表 $[d_{\\mathrm{in}}(0), d_{\\mathrm{in}}(1), \\dots, d_{\\mathrm{in}}(n-1)]$，$D_{\\mathrm{out}}$是出度列表 $[d_{\\mathrm{out}}(0), d_{\\mathrm{out}}(1), \\dots, d_{\\mathrm{out}}(n-1)]$，$S$是互惠邻居重叠大小列表 $[s(0), s(1), \\dots, s(n-1)]$，$m$是整数边数，$\\Delta_{\\max}$是整数最大度。\n- 例如，程序应打印单行，其结构为 $[[\\dots],[\\dots],[\\dots],[\\dots]]$，其中每个内部列表是按指定格式表示的测试用例结果。\n\n注意：\n- 不涉及物理单位。\n- 角度单位不适用。\n- 所有数值结果均以标准十进制表示法表示为整数。", "solution": "用户提供的问题经过了严格的验证过程。\n\n### 步骤 1：提取已知信息\n\n- **图模型**：一个有向基因调控网络被建模为一个图，其节点集为 $V = \\{0, 1, \\dots, n-1\\}$，有向边集为 $E$。\n- **表示方法**：该图由一个邻接表 $A$ 给出，其中 $A[i]$ 包含每个节点 $i$ 的出邻居集合 $N_{\\mathrm{out}}(i)$。\n- **边处理**：$A[i]$ 中的重复边将被忽略。允许自环 $(i,i)$。\n- **定义**：\n    - 出度: $d_{\\mathrm{out}}(i) = |N_{\\mathrm{out}}(i)|$。\n    - 入邻居集合: $N_{\\mathrm{in}}(i) = \\{j \\in V \\mid (j,i) \\in E\\}$。\n    - 入度: $d_{\\mathrm{in}}(i) = |N_{\\mathrm{in}}(i)|$。\n    - 互惠邻居重叠大小: $s(i) = |N_{\\mathrm{out}}(i) \\cap N_{\\mathrm{in}}(i)|$。\n    - 有向边总数: $m = |E|$。\n    - 最大度: $\\Delta_{\\max} = \\max_{i \\in V} \\{\\max(d_{\\mathrm{out}}(i), d_{\\mathrm{in}}(i))\\}$。\n- **任务**：\n    1.  设计一个算法，根据邻接表 $A$ 计算所有 $i \\in V$ 的 $d_{\\mathrm{in}}(i)$、$d_{\\mathrm{out}}(i)$ 和 $s(i)$，以及 $m$ 和 $\\Delta_{\\max}$。\n    2.  算法的核心计算必须使用邻接表，而非邻接矩阵。\n    3.  分析算法关于 $m$ 和 $\\Delta_{\\max}$ 的时间复杂度。\n- **测试套件**：提供了四个特定的测试用例，每个用例都给定了节点数 $n$ 和邻接表 $A$。\n- **输出格式**：包含每个测试用例结果的单行逗号分隔列表。每个测试用例结果必须是 $[D_{\\mathrm{in}}, D_{\\mathrm{out}}, S, m, \\Delta_{\\max}]$ 形式的列表，其中 $D_{\\mathrm{in}}$、$D_{\\mathrm{out}}$ 和 $S$ 是各节点的度量值列表。\n\n### 步骤 2：使用提取的已知信息进行验证\n\n根据验证标准对问题进行评估。\n\n- **科学依据**：该问题具有充分的科学依据。基于图的调控网络模型是计算系统生物学的基石。所有定义（$d_{\\mathrm{in}}, d_{\\mathrm{out}}$ 等）都是图論中的标准定义。此标准已满足。\n- **适定性**：该问题是适定的。输入（邻接表 $A$，节点数 $n$）定义清晰，所需输出（$D_{\\mathrm{in}}$、$D_{\\mathrm{out}}$、$S$、$m$、$\\Delta_{\\max}$）由数学定义明确指定。对于任何给定的输入，都存在唯一的解。此标准已满足。\n- **客观性**：该问题以客观的数学语言陈述，没有歧义或主观论断。此标准已满足。\n\n该问题没有任何使其无效的缺陷。它在科学上是合理的、可形式化的、完整的，并且是算法上可解的。\n\n### 步骤 3：结论与行动\n\n该问题被判定为**有效**。将提供一个解决方案。\n\n### 算法设计与分析\n\n目标是根据一个包含 $n$ 个节点的有向图的邻接表表示 $A$，计算五个图论度量。任务的核心是设计一个高效的算法，以避免使用邻接矩阵带来的 $O(n^2)$ 空间复杂度，这是强制要求。\n\n**1. 数据结构预处理**\n\n输入是一个邻接表 $A$，其中每个 $A[i]$ 是一个出邻居列表。问题规定应忽略重复的边。强制实现唯一性并实现高效查找的最有效方法是将每个列表 $A[i]$ 转换为哈希集。我们构建一个新的数据结构 `A_sets`，它是一个集合列表，其中 `A_sets[i]` 存储节点 $i$ 的唯一出邻居。此预处理步骤会遍历原始邻接表中的所有条目。如果包括重复项在内的总条目数为 $M_{raw}$，则此步骤需要 $O(M_{raw})$ 的时间。\n\n**2. 计算出度（$d_{\\mathrm{out}}$）和总边数（$m$）**\n\n一旦我们有了集合列表 `A_sets`，每个节点 $i$ 的出度 $d_{\\mathrm{out}}(i)$ 就是集合 `A_sets[i]` 的大小。这可以在 $O(n)$ 时间内为所有节点计算出来。唯一有向边的总数 $m$ 是所有出度的总和：$m = \\sum_{i=0}^{n-1} d_{\\mathrm{out}}(i)$。在所有出度已知后，这个总和可以在 $O(n)$ 时间内计算出来。\n\n**3. 计算入度（$d_{\\mathrm{in}}$）**\n\n给定的邻接表直接提供了出邻居。要计算入度，我们必须确定对于每个节点 $j$，有多少其他节点 $i$ 存在边 $(i,j)$。我们无需构建一个完整的反向图，而可以在一次遍历所有边的过程中计算出所有入度。我们初始化一个大小为 $n$、元素全为零的入度数组 `d_in`。然后，我们遍历从 $0$ 到 $n-1$ 的每个节点 $i$，并对于其每个出邻居 $j \\in \\text{A\\_sets}[i]$，我们将节点 $j$ 的入度计数器加一，即 `d_in[j]++`。此过程恰好接触每条唯一的边一次，因此其时间复杂度为 $O(m)$。\n\n**4. 计算互惠邻居重叠大小（$s(i)$）**\n\n节点 $i$ 的互惠邻居重叠大小定义为 $s(i) = |N_{\\mathrm{out}}(i) \\cap N_{\\mathrm{in}}(i)|$。一个等价的定义是同时存在边 $(i, j)$ 和边 $(j, i)$ 的节点 $j$ 的数量。基于此可以设计一个直接且高效的算法。我们初始化一个大小为 $n$、元素全为零的重叠大小数组 `s`。然后，我们遍历每个节点 $i$，并对于其每个出邻居 $j \\in \\text{A\\_sets}[i]$，我们检查是否存在反向边 $(j, i)$。这个检查等价于测试 $i$ 是否在节点 $j$ 的出邻居集合中，即 `i in A_sets[j]`。由于 `A_sets[j]` 是一个哈希集，此检查的平均时间复杂度为 $O(1)$。如果条件为真，我们就增加 `s[i]` 的计数器。此过程同样涉及遍历所有唯一的边，因此计算所有 $s(i)$ 值的总时间复杂度为 $O(m)$。\n\n**5. 计算最大度（$\\Delta_{\\max}$）**\n\n在计算完所有入度列表 $D_{\\mathrm{in}}$ 和出度列表 $D_{\\mathrm{out}}$ 之后，通过取这两个列表中存在的最大值来找到最大度 $\\Delta_{\\max}$。这需要对两个列表进行一次遍历，耗时 $O(n)$。\n\n**6. 时间复杂度分析**\n\n算法的总体时间复杂度是其各个组成步骤复杂度的总和：\n- 将 $A$ 预处理为 `A_sets`：$O(M_{raw})$，其中 $M_{raw}$ 是 $A$ 中的原始条目数。\n- 计算所有 $d_{\\mathrm{out}}(i)$：$O(n)$。\n- 计算所有 $d_{\\mathrm{in}}(i)$：$O(m)$。\n- 计算所有 $s(i)$：平均 $O(m)$。\n- 计算 $m$ 和 $\\Delta_{\\max}$：$O(n)$。\n\n总时间复杂度为 $O(M_{raw} + n + m)$。由于唯一边的数量 $m$ 至多为 $M_{raw}$，这可以简化为 $O(M_{raw} + n)$。如果假设输入没有重复项，则 $M_{raw} = m$，复杂度为 $O(n+m)$。\n\n问题要求以 $m$ 和 $\\Delta_{\\max}$ 表示复杂度。边数 $m$ 通过不等式 $m = \\sum_{i \\in V} d_{\\mathrm{out}}(i) \\le n \\cdot \\Delta_{\\max}$ 与最大度 $\\Delta_{\\max}$ 相关。因此，$O(n+m)$ 的复杂度可以用上限 $O(n + n \\cdot \\Delta_{\\max}) = O(n \\cdot \\Delta_{\\max})$ 来表示。这个界限正确地反映了算法对节点数和由 $\\Delta_{\\max}$ 表征的图密度的依赖性。空间复杂度为 $O(n+m)$，用于存储清理后的邻接集和结果数组。这符合避免使用邻接矩阵的约束。", "answer": "```python\nimport numpy as np\n\ndef calculate_metrics(n, adj_list):\n    \"\"\"\n    Computes graph metrics for a given directed graph.\n\n    Args:\n        n (int): The number of nodes in the graph.\n        adj_list (list of lists): The adjacency list representation of the graph.\n\n    Returns:\n        list: A list containing [D_in, D_out, S, m, Delta_max].\n    \"\"\"\n    if n == 0:\n        return [[], [], [], 0, 0]\n\n    # Step 1: Pre-process adjacency list to handle duplicates and for efficient lookups.\n    # A_sets[i] will store the unique out-neighbors of node i.\n    A_sets = [set(neighbors) for neighbors in adj_list]\n\n    # Step 2: Compute out-degrees.\n    d_out = [len(s) for s in A_sets]\n\n    # Initialize arrays for in-degrees and reciprocal overlap sizes.\n    d_in = [0] * n\n    s = [0] * n\n    \n    # Step 3  4: Compute in-degrees and reciprocal overlaps.\n    # Iterate through each edge (i,j) to update d_in for j and s for i.\n    for i in range(n):\n        for j in A_sets[i]:\n            # Each j is an out-neighbor of i, so this is an edge (i, j).\n            # This contributes to the in-degree of j.\n            d_in[j] += 1\n            \n            # Check for the reciprocal edge (j, i) to calculate s(i).\n            # This check is efficient due to the use of sets.\n            if i in A_sets[j]:\n                s[i] += 1\n\n    # Step 5: Compute total number of edges m.\n    m = sum(d_out)\n\n    # Step 6: Compute maximum degree Delta_max.\n    delta_max = 0\n    if n  0:\n        max_out = max(d_out) if d_out else 0\n        max_in = max(d_in) if d_in else 0\n        delta_max = max(max_out, max_in)\n\n    return [d_in, d_out, s, m, delta_max]\n\ndef solve():\n    \"\"\"\n    Runs the validation and computation for the problem's test suite.\n    \"\"\"\n    test_cases = [\n        (5, [[1, 2], [2, 3], [0, 3], [2], [0, 3]]),\n        (3, [[], [], []]),\n        (4, [[0, 1], [1, 2], [], [3]]),\n        (6, [[1, 2, 3, 4, 5], [0], [0], [0], [0], [0]])\n    ]\n\n    results = []\n    for n, A in test_cases:\n        result = calculate_metrics(n, A)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # The default str() representation of a list is used, e.g., '[1, 2, 3]'.\n    # A comma is used to join the string representations of each test case result.\n    final_output = f\"[{','.join(map(str, results))}]\"\n    \n    # Python's str() adds spaces after commas in lists. Let's remove them for a compact representation.\n    final_output = final_output.replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n\n```", "id": "3332690"}, {"introduction": "在网络分析中，我们常常需要将原始的、可能包含重复条目的边列表数据整合为密集的邻接矩阵。这个练习将探讨这一转换过程中的一个微妙但至关重要的问题：浮点数权重求和时可能出现的数值不稳定[@problem_id:3332717]。你将通过实现一种补偿求和算法，来亲身体验如何构建一个更精确、更稳健的矩阵表示，并理解其相对于朴素求和的优势。", "problem": "考虑一个带加权边的有向网络，它代表了计算系统生物学中的调控相互作用，并以纯数学术语进行抽象。一个包含 $n$ 个节点的网络由一个包含 $m$ 个三元组 $(i,j,w)$ 的邻接表表示，其中 $i \\in \\{0,1,\\dots,n-1\\}$，$j \\in \\{0,1,\\dots,n-1\\}$，且 $w \\in \\mathbb{R}$ 是从节点 $i$ 到节点 $j$ 的有向边的权重。多个条目可能共享相同的有序对 $(i,j)$，这表示存在重复边，其权重必须聚合到邻接矩阵的单个条目中。有向加权邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的定义为：$A_{ij}$ 是与从节点 $i$ 到节点 $j$ 的所有边相关联的权重之和。\n\n从图和矩阵的基本定义，以及广泛使用的浮点运算模型（Institute of Electrical and Electronics Engineers (IEEE) 754 双精度）出发，设计并实现一个算法，将带有加权有向边的邻接表转换为其对应的稠密邻接矩阵 $A$。您的算法必须使用一种数值稳定的求和方案，将重复边 $(i,j)$ 正确地聚合到单个条目 $A_{ij}$ 中，该方案能降低对浮点舍入误差的敏感性。为此，请使用一种补偿求和方法（例如 Kahan 求和算法），该方法在求和时概念性地跟踪一个修正项，以限制舍入误差的传播。同时，为了进行比较，构建一个矩阵 $A^{\\text{naive}}$，该矩阵通过按输入给定的顺序使用标准的左结合浮点加法来聚合重复边而形成。\n\n在要求输出稠密矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的前提下，根据 $n$ 和 $m$ 分析算法的时间复杂度。您的分析必须从数组初始化和邻接表迭代的核心定义开始，并必须证明每个渐近贡献的合理性。此外，讨论在浮点运算（FPA）下重复边聚合的数值稳定性，用机器精度 $\\varepsilon_{\\text{mach}}$ 和聚合权重的量级来表达您的推理。\n\n您的程序必须实现该算法，并为每个测试用例生成两个输出：使用数值稳定聚合计算的 $A$ 的行主序扁平化条目列表，以及稳定聚合和朴素聚合之间的最大逐项绝对差 $\\max_{i,j} |A_{ij} - A^{\\text{naive}}_{ij}|$。所有节点索引均为零基索引。不涉及物理单位；角度不适用。将最大绝对差表示为十进制数。\n\n使用以下测试套件，其设计旨在检验典型场景、高动态范围的重复聚合、边界条件和稀疏性：\n\n- 测试用例 1（通用“理想路径”）：$n = 4$，邻接表边为\n  $(0,1,0.1)$、$(0,1,0.2)$、$(0,3,0.5)$、$(1,2,1.0)$、$(2,2,0.3)$、$(2,2,-0.1)$、$(3,0,2.0)$。\n- 测试用例 2（重要的边缘情况：动态范围和顺序敏感性）：$n = 3$，边为\n  $(0,2,10^9)$，后跟 $100000$ 条边 $(0,2,10^{-5})$，再后跟 $(0,2,-10^9)$，最后是 $(0,2,1.0)$。\n- 测试用例 3（边界条件：没有边）：$n = 3$，没有边。\n- 测试用例 4（在极小量级下具有对称抵消的自环）：$n = 1$，边由 $100000$ 个条目 $(0,0,10^{-12})$ 和 $100000$ 个条目 $(0,0,-10^{-12})$ 组成。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。此列表中的每个元素对应一个测试用例，并且本身必须是 $[\\text{flattened\\_A}, \\text{max\\_abs\\_diff}]$ 形式的列表。例如，对于四个测试用例，输出格式的结构为 `[[...], [...], [...], [...]]`，其中每个内部列表本身就是一个形式为 `[flattened_A, max_abs_diff]` 的列表。", "solution": "将带权重的有向邻接表转换为其对应的稠密邻接矩阵是网络分析中的一项基本任务。该问题陈述是有效的，因为它在科学上基于图论和数值分析的既定原则，问题良构，目标明确，数据充分，且表述客观。我们将提供一个完整的解决方案。\n\n问题的核心在于将重复边（连接相同节点对 $(i, j)$ 的多条边）的权重聚合到单个矩阵条目 $A_{ij}$ 中。这需要对浮点数进行求和，这个过程容易产生精度误差，尤其是在数字具有大动态范围时。该问题要求在标准的“朴素”求和与数值稳定的“补偿”求和之间进行比较，为此我们将采用 Kahan 求和算法。\n\n首先，我们定义数学对象。给定一个有 $n$ 个节点的有向图。节点从 $0$ 到 $n-1$ 索引。网络结构以邻接表的形式给出，这是一个包含 $m$ 个三元组 $(i, j, w)$ 的集合，其中 $i$ 和 $j$ 是节点索引，$w \\in \\mathbb{R}$ 是从节点 $i$ 到节点 $j$ 的有向边的权重。目标表示是稠密邻接矩阵 $A$，一个 $n \\times n$ 矩阵，其中条目 $A_{ij}$ 是从节点 $i$ 到节点 $j$ 的所有边的权重之和。如果从 $i$ 到 $j$ 不存在边，则 $A_{ij} = 0$。\n\n从包含 $m$ 条边的列表构建矩阵 $A$ 的算法流程如下：\n1. 初始化一个 $n \\times n$ 的矩阵，记为 $A$，所有条目均设为 $0$。\n2. 对于邻接表中的每个三元组 $(i, j, w)$，通过将 $w$ 加到其当前值上来更新条目 $A_{ij}$。\n\n关键部分是步骤 2 中求和的实现。标准的浮点运算并非严格满足结合律，即 $(a+b)+c$ 不总是等于 $a+(b+c)$。这是由于每次运算后的舍入造成的。\n\n**朴素求和**\n对一个条目 $A_{ij}$ 的朴素求和通过遍历边并按给定顺序累加权重来进行。如果 $S$ 是 $A_{ij}$ 的运行总和，而 $w_k$ 是从 $i$到 $j$ 的第 $k$ 条边的权重，则更新操作仅为 $S \\leftarrow S + w_k$。\n这种方法，正式称为左结合求和，对运算顺序敏感，并可能遭受显著的误差累积。当将一个小数字 $w_k$ 添加到一个大总和 $S$ 中时，浮点表示的有限精度可能导致 $w_k$ 的贡献被部分或完全丢失。这种现象，称为吸收或吞噬，发生在 $|w_k/S|$ 小于机器精度 $\\varepsilon_{\\text{mach}}$ 时（对于 IEEE 754 双精度，$\\varepsilon_{\\text{mach}} \\approx 2.22 \\times 10^{-16}$）。累积的误差会随着加法次数 $m_{ij}$ 的增加而增长，其中 $m_{ij}$ 是节点对 $(i, j)$ 的重复边数量。\n\n**补偿求和（Kahan 算法）**\n为了减轻这种精度损失，我们使用补偿求和算法，具体来说是 Kahan 求和算法。该方法跟踪一个运行修正项 $c$，它代表了前一次加法中“丢失”的低位部分。将值 $w$ 加到运行总和 $S$ 的算法如下：\n1. $y \\leftarrow w - c$：新值 $w$ 首先通过减去上一步的误差来进行修正。\n2. $t \\leftarrow S + y$：将修正后的值 $y$ 加到运行总和 $S$ 上。由于舍入，这个和可能不精确。\n3. $c \\leftarrow (t - S) - y$：计算新的误差。$(t - S)$ 近似于 $y$ 中被有效添加的部分。从此值中减去 $y$ 得到 $y$ 在加法中丢失部分的相反数。\n4. $S \\leftarrow t$：更新总和。\n\n对于贡献于给定矩阵条目 $A_{ij}$ 的每个权重 $w$，都应用此过程。我们必须为矩阵的每个条目维护一个单独的修正项 $c_{ij}$。因此，我们使用一个辅助的 $n \\times n$ 修正项矩阵 $C$，并将其所有元素初始化为零。\n\n**算法设计与复杂度分析**\n\n构建数值稳定的矩阵 $A$ 和朴素矩阵 $A^{\\text{naive}}$ 的完整算法如下：\n\n1.  初始化三个 $n \\times n$ 的零矩阵：稳定求和矩阵 $A$、朴素求和矩阵 $A^{\\text{naive}}$ 和修正矩阵 $C$。此步骤耗时 $O(n^2)$。\n2.  遍历输入邻接表中的 $m$ 条边。对于每条边 $(i, j, w)$：\n    a. **朴素更新：** 使用标准加法更新 $A^{\\text{naive}}_{ij}$：$A^{\\text{naive}}_{ij} \\leftarrow A^{\\text{naive}}_{ij} + w$。\n    b. **补偿更新：** 使用 Kahan 算法及其对应的修正项 $C_{ij}$ 更新 $A_{ij}$：\n        i.   $y \\leftarrow w - C_{ij}$\n        ii.  $t \\leftarrow A_{ij} + y$\n        iii. $C_{ij} \\leftarrow (t - A_{ij}) - y$\n        iv.  $A_{ij} \\leftarrow t$\n    循环内的每次更新涉及常数数量的算术运算和数组访问，对于稠密矩阵，这耗时 $O(1)$。\n3.  此循环运行 $m$ 次，因此其总时间复杂度为 $O(m)$。\n4.  遍历所有边后，计算最大逐项绝对差：$\\max_{i,j} |A_{ij} - A^{\\text{naive}}_{ij}|$。这需要遍历所有 $n^2$ 个条目，耗时 $O(n^2)$。\n\n算法的总时间复杂度由矩阵初始化/最终比较和边处理主导，结果为 $O(n^2 + m)$。由于要求输出一个稠密的 $n \\times n$ 矩阵，任何算法都至少需要 $\\Omega(n^2)$ 的时间，因此该复杂度是最优的。\n\n**数值稳定性分析**\n$N$ 个数的朴素求和误差在最坏情况下可能与 $N$ 成正比增长。误差界大约为 $N \\cdot \\varepsilon_{\\text{mach}} \\cdot \\sum|w_k|$，这个值可能很大。结果也高度依赖于求和的顺序。\n相比之下，Kahan 求和算法的误差非常稳定。误差界与项数 $N$ 无关，量级约为 $\\varepsilon_{\\text{mach}} \\cdot \\sum|w_k|$。这意味着即使对于非常大量的加法，误差也不会无界地累积。对于测试用例 2，其中一个大值 ($10^9$) 后面跟着许多小值 ($10^{-5}$)，朴素求和将完全丢失这些小值。然而，Kahan 求和通过传递误差项会正确地累加它们，从而得到一个显著不同且更准确的结果。对于测试用例 4，它涉及对许多交替符号的小值求和，朴素求和可能会累积舍入误差，而 Kahan 求和将产生一个更接近真实值 $0$ 的结果。", "answer": "```python\nimport numpy as np\nimport json\n\ndef convert_adj_list_to_matrices(n, edges):\n    \"\"\"\n    Converts a weighted adjacency list to dense adjacency matrices using both\n    naive and Kahan compensated summation for aggregating duplicate edge weights.\n\n    Args:\n        n (int): The number of nodes in the network.\n        edges (list of tuples): A list of (i, j, w) triples representing\n                                directed edges from node i to node j with weight w.\n\n    Returns:\n        tuple: A tuple containing:\n            - A (np.ndarray): The adjacency matrix computed with Kahan summation.\n            - A_naive (np.ndarray): The adjacency matrix computed with naive summation.\n    \"\"\"\n    # Initialize matrices. A for Kahan sum, A_naive for simple sum, C for Kahan correction terms.\n    A = np.zeros((n, n), dtype=np.float64)\n    A_naive = np.zeros((n, n), dtype=np.float64)\n    C = np.zeros((n, n), dtype=np.float64)\n\n    # Process each edge from the adjacency list\n    for i, j, w in edges:\n        # Naive summation\n        A_naive[i, j] += w\n\n        # Kahan compensated summation\n        # y = w - c: Correct the new value by the error from the previous sum\n        # t = sum + y: Add the corrected value to the running sum\n        # c = (t - sum) - y: Calculate the new error term (the \"lost\" low-order part)\n        # sum = t: Update the sum\n        y = w - C[i, j]\n        t = A[i, j] + y\n        C[i, j] = (t - A[i, j]) - y\n        A[i, j] = t\n        \n    return A, A_naive\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite. For each case, it computes\n    the stable adjacency matrix, its flattened representation, and the max absolute\n    difference between the stable and naive matrices.\n    \"\"\"\n    # Define test cases\n    # Test Case 2: large dynamic range and summation order sensitivity\n    case2_edges = [(0, 2, 1e9)]\n    case2_edges.extend([(0, 2, 1e-5)] * 100000)\n    case2_edges.extend([(0, 2, -1e9), (0, 2, 1.0)])\n\n    # Test Case 4: self-loops with cancellation at tiny magnitudes\n    case4_edges = [(0, 0, 1e-12)] * 100000\n    case4_edges.extend([(0, 0, -1e-12)] * 100000)\n\n    test_cases = [\n        {\n            \"n\": 4,\n            \"edges\": [\n                (0, 1, 0.1), (0, 1, 0.2), (0, 3, 0.5), (1, 2, 1.0),\n                (2, 2, 0.3), (2, 2, -0.1), (3, 0, 2.0)\n            ]\n        },\n        {\n            \"n\": 3,\n            \"edges\": case2_edges\n        },\n        {\n            \"n\": 3,\n            \"edges\": []\n        },\n        {\n            \"n\": 1,\n            \"edges\": case4_edges\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        edges = case[\"edges\"]\n\n        A, A_naive = convert_adj_list_to_matrices(n, edges)\n\n        # Flatten the stable matrix in row-major order\n        flattened_A = A.flatten().tolist()\n\n        # Calculate the maximum absolute entry-wise difference\n        max_abs_diff = np.max(np.abs(A - A_naive))\n\n        all_results.append([flattened_A, max_abs_diff])\n    \n    # Use json.dumps for a compact, space-free string representation\n    # that matches the formatting requirements (no spaces around commas).\n    print(json.dumps(all_results, separators=(',', ':')))\n\nsolve()\n```", "id": "3332717"}, {"introduction": "密集的邻接矩阵虽然信息全面，但可能导致巨大的计算开销。因此，通过移除权重较小的“弱”相互作用来稀疏化网络是一种常用策略。此练习将带你实践这一过程，将密集矩阵通过阈值化转换为稀疏的邻接表，更重要的是，你将学习使用矩阵范数等线性代数工具来严格量化这一简化过程所造成的信息损失[@problem_id:3332692]。", "problem": "考虑一个有向加权的生物分子相互作用网络（例如，转录因子对基因的影响），该网络由一个稠密邻接矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 表示，其中每个条目 $w_{ij}$ 编码了从节点 $i$ 到节点 $j$ 的带符号影响强度。每个有序对 $(i,j)$ 还带有一个类别型边属性 $a_{ij} \\in \\mathbb{Z}$（例如，$a_{ij} = 1$ 表示激活，$a_{ij} = -1$ 表示抑制，$a_{ij} = 0$ 表示不存在或未知），该属性在一个并行的属性矩阵 $A \\in \\mathbb{Z}^{n \\times n}$ 中提供。该网络将通过使用一个阈值 $\\tau \\in \\mathbb{R}_{\\ge 0}$ 去除影响量级较小的边来进行稀疏化。\n\n从网络邻接矩阵和矩阵范数的基本定义出发，设计并实现一个程序，该程序能够：\n1. 将 $W$ 和 $A$ 转换为一个压缩邻接表，该表仅包含稀疏化后保留的边，并保留其属性。使用规则“如果 $|w_{ij}|  \\tau$ 且 $w_{ij} \\neq 0$ 则舍弃；否则保留”，这样恰好等于 $\\tau$ 的边将被保留，而零值不会被列出。\n2. 通过将所有被阈值舍弃的条目置零来形成稀疏化矩阵 $W^{(\\tau)}$，即，如果 $|w_{ij}| \\ge \\tau$ 且 $w_{ij} \\neq 0$，则 $w^{(\\tau)}_{ij} = w_{ij}$，否则 $w^{(\\tau)}_{ij} = 0$。\n3. 推导并计算基于阈值的稀疏化在弗罗贝尼乌斯范数下引入的矩阵近似误差的上界，并验证一个通用的谱范数不等式。具体来说，令 $E = W - W^{(\\tau)}$ 表示误差矩阵。令 $m$ 表示被舍弃的非零条目的数量（即满足 $w_{ij} \\neq 0$ 且 $|w_{ij}|  \\tau$ 的 $(i,j)$ 的计数）。证明\n$$\\lVert E \\rVert_F^2 = \\sum_{(i,j):\\, 0  |w_{ij}|  \\tau} w_{ij}^2 \\le \\sum_{(i,j):\\, 0  |w_{ij}|  \\tau} \\tau^2 = m \\tau^2,$$\n并因此证明\n$$\\lVert E \\rVert_F \\le \\sqrt{m}\\,\\tau.$$\n此外，论证并计算\n$$\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F,$$\n其中 $\\lVert \\cdot \\rVert_2$ 是由欧几里得向量范数诱导的谱范数，$\\lVert \\cdot \\rVert_F$ 是弗罗贝尼乌斯范数。\n\n您的程序必须为保留的边构建四元组 $(i,j,w_{ij},a_{ij})$ 的压缩邻接表，计算 $m$、$\\lVert E \\rVert_F$、上界 $\\sqrt{m}\\,\\tau$ 和 $\\lVert E \\rVert_2$，并通过生成布尔值来验证这两个不等式。必须通过检查邻接表中每条保留边的属性 $a_{ij}$ 是否等于 $A$ 中的相应条目来验证属性的保留情况。\n\n使用以下测试套件。每个案例明确提供了 $(W,A,\\tau)$。所有数值条目都是无量纲的，且不涉及角度。\n\n测试案例1（正常路径，混合符号，中等阈值）：\n$$\nW_1 =\n\\begin{bmatrix}\n0  0.21  -0.05  0.11 \\\\\n0.03  0  0.41  -0.12 \\\\\n-0.21  0.08  0  0.07 \\\\\n0.15  -0.09  0.20  0\n\\end{bmatrix},\\quad\nA_1 =\n\\begin{bmatrix}\n0  1  -1  1 \\\\\n1  0  1  -1 \\\\\n-1  1  0  1 \\\\\n1  -1  1  0\n\\end{bmatrix},\\quad\n\\tau_1 = 0.15.\n$$\n\n测试案例2（边界情况，$\\tau = 0$ 保留所有非零边）：\n$$\nW_2 =\n\\begin{bmatrix}\n0  -0.20  0 \\\\\n0.18  0  0.05 \\\\\n0  -0.04  0\n\\end{bmatrix},\\quad\nA_2 =\n\\begin{bmatrix}\n0  -1  0 \\\\\n1  0  1 \\\\\n0  -1  0\n\\end{bmatrix},\\quad\n\\tau_2 = 0.\n$$\n\n测试案例3（边缘情况，阈值超过所有权重，舍弃所有非零边）：\n$$\nW_3 =\n\\begin{bmatrix}\n0  0.30  -0.10  0  0.20 \\\\\n-0.25  0  0.05  -0.40  0 \\\\\n0.10  -0.20  0  0.30  -0.05 \\\\\n0  0  -0.15  0  0.10 \\\\\n-0.20  0.25  0  -0.05  0\n\\end{bmatrix},\\quad\nA_3 =\n\\begin{bmatrix}\n0  1  -1  0  1 \\\\\n-1  0  1  -1  0 \\\\\n1  -1  0  1  -1 \\\\\n0  0  -1  0  1 \\\\\n-1  1  0  -1  0\n\\end{bmatrix},\\quad\n\\tau_3 = 0.5.\n$$\n\n对于每个测试案例，您的程序必须按顺序输出一个包含以下条目的列表：\n- 保留边的整数数量 $k$，\n- 被舍弃的非零条目的整数数量 $m$，\n- 浮点数值 $\\lVert E \\rVert_F$，\n- 浮点数上界 $\\sqrt{m}\\,\\tau$，\n- 浮点数值 $\\lVert E \\rVert_2$，\n- 一个布尔值，指示 $\\lVert E \\rVert_F \\le \\sqrt{m}\\,\\tau$ 是否成立，\n- 一个布尔值，指示 $\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F$ 是否成立，\n- 一个布尔值，指示属性是否在压缩邻接表中被完全保留。\n\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表被方括号括起，每个测试案例一个列表，按顺序排列，例如 $[\\text{case1},\\text{case2},\\text{case3}]$，其中每个 $\\text{case}$ 本身都是如上所述的列表。", "solution": "本问题是计算网络分析与线性代数领域一个定义明确的练习，其背景源于系统生物学的成熟实践。它要求实现一个基于阈值的网络稀疏化程序，并验证与矩阵范数相关的标准数学不等式。该问题自身完备、科学上合理，且所有提供的数据和条件均一致。因此，我们可以进行形式化的求解。\n\n问题的核心是分析对一个由邻接矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 表示的加权有向网络进行稀疏化的后果。稀疏化是通过消除其影响强度 $w_{ij}$ 低于某个量级阈值 $\\tau \\in \\mathbb{R}_{\\ge 0}$ 的边来实现的。一个关联的属性矩阵 $A \\in \\mathbb{Z}^{n \\times n}$ 也必须被处理。\n\n该过程可以分解为以下逻辑步骤：\n\n**1. 网络稀疏化与邻接表生成**\n\n网络由邻接矩阵 $W$ 和属性矩阵 $A$ 定义。稀疏化规则是，如果边的权重 $w_{ij}$ 满足 $0  |w_{ij}|  \\tau$，则丢弃该边 $(i, j)$。权重满足 $|w_{ij}| \\ge \\tau$ 的边被保留， $w_{ij} = 0$ 的条目（代表不存在边）也同样保持不变。\n\n首先，我们构建稀疏化后的邻接表。这个列表将只包含被保留的边，即那些权重满足 $|w_{ij}| \\ge \\tau$ 的边。对于每个这样的边，我们存储一个四元组 $(i, j, w_{ij}, a_{ij})$，其中 $i$ 是源节点索引，$j$ 是目标节点索引，$w_{ij}$ 是来自 $W$ 的权重，$a_{ij}$ 是来自 $A$ 的相应属性。保留边的总数用 $k$ 表示。\n\n同时，我们识别出被舍弃的非零边集合。这些是满足 $0  |w_{ij}|  \\tau$ 的边。我们对这些边进行计数，这个计数用 $m$ 表示。\n\n**2. 误差矩阵的构建**\n\n稀疏化过程将原始矩阵 $W$ 转换为稀疏化矩阵 $W^{(\\tau)}$。$W^{(\\tau)}$ 的条目定义为：\n$$\nw^{(\\tau)}_{ij} =\n\\begin{cases}\nw_{ij}  \\text{if } |w_{ij}| \\ge \\tau \\\\\n0  \\text{if } |w_{ij}|  \\tau\n\\end{cases}\n$$\n这个定义正确地处理了 $w_{ij} = 0$ 的情况，它在 $W^{(\\tau)}$ 中仍然是 $0$。这个过程引入的近似误差由误差矩阵 $E = W - W^{(\\tau)}$ 捕捉。$E$ 的条目是 $e_{ij} = w_{ij} - w^{(\\tau)}_{ij}$。根据 $W^{(\\tau)}$ 的定义，误差矩阵的条目是：\n$$\ne_{ij} =\n\\begin{cases}\n0  \\text{if } |w_{ij}| \\ge \\tau \\text{ or } w_{ij} = 0 \\\\\nw_{ij}  \\text{if } 0  |w_{ij}|  \\tau\n\\end{cases}\n$$\n因此，$E$ 的非零条目恰好是被舍弃的边的权重。\n\n**3. 弗罗贝尼乌斯范数误差上界的推导**\n\n问题要求我们证明 $\\lVert E \\rVert_F \\le \\sqrt{m}\\,\\tau$，其中 $\\lVert \\cdot \\rVert_F$ 是弗罗贝尼乌斯范数。矩阵 $E$ 的弗罗贝尼乌斯范数定义为其元素平方和的平方根：$\\lVert E \\rVert_F = \\sqrt{\\sum_{i,j} |e_{ij}|^2}$。\n\n让我们从弗罗贝尼乌斯范数的平方 $\\lVert E \\rVert_F^2$ 开始：\n$$\n\\lVert E \\rVert_F^2 = \\sum_{i=1}^n \\sum_{j=1}^n e_{ij}^2\n$$\n根据 $E$ 的定义，这个和仅在 $m$ 个被舍弃的边集合上非零，其中 $e_{ij} = w_{ij}$。\n$$\n\\lVert E \\rVert_F^2 = \\sum_{(i,j):\\, 0  |w_{ij}|  \\tau} w_{ij}^2\n$$\n这就建立了我们被要求证明的恒等式的第一部分。对于这个和中的每一项，被舍弃的边的条件是 $0  |w_{ij}|  \\tau$，这意味着 $w_{ij}^2  \\tau^2$。通过将每一项 $w_{ij}^2$ 替换为不大于它的值 $\\tau^2$，我们得到了这个和的一个上界。由于恰好有 $m$ 个这样的项：\n$$\n\\sum_{(i,j):\\, 0  |w_{ij}|  \\tau} w_{ij}^2 \\le \\sum_{k=1}^m \\tau^2 = m\\tau^2\n$$\n问题陈述使用的是非严格不等式 $\\lVert E \\rVert_F^2 \\le m\\tau^2$，这是一个有效且更通用的上界。对两边取平方根（并且因为范数是非负的）得到期望的结果：\n$$\n\\lVert E \\rVert_F \\le \\sqrt{m\\tau^2} = \\sqrt{m}\\,\\tau\n$$\n\n**4. 谱范数不等式的论证**\n\n我们还必须论证著名的矩阵范数不等式 $\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F$，其中 $\\lVert \\cdot \\rVert_2$ 是谱范数。矩阵 $E$ 的谱范数定义为其最大的奇异值 $\\sigma_{\\max}(E)$。弗罗贝尼乌斯范数也可以用矩阵的奇异值 $\\{\\sigma_k\\}$ 来表示：\n$$\n\\lVert E \\rVert_F^2 = \\sum_{k=1}^{\\text{rank}(E)} \\sigma_k(E)^2\n$$\n谱范数的平方就是最大奇异值的平方：\n$$\n\\lVert E \\rVert_2^2 = (\\sigma_{\\max}(E))^2\n$$\n由于所有奇异值都是非负的，它们的平方和必须大于或等于任何单个奇异值的平方，包括最大的那个：\n$$\n(\\sigma_{\\max}(E))^2 \\le \\sum_{k=1}^{\\text{rank}(E)} \\sigma_k(E)^2\n$$\n将范数的定义代回，得到：\n$$\n\\lVert E \\rVert_2^2 \\le \\lVert E \\rVert_F^2\n$$\n由于范数是非负的，我们可以对两边取平方根，得到最终的不等式：\n$$\n\\lVert E \\rVert_2 \\le \\lVert E \\rVert_F\n$$\n这个不等式对任何矩阵 $E$ 都成立，我们将对测试案例生成的误差矩阵进行计算验证。\n\n**5. 属性保留检查**\n\n最后的任务是验证保留的边的属性在生成的邻接表中是否被正确保留。这是一个关键的数据完整性检查。对于生成列表中的每一个四元组 $(i, j, w_{ij}, a'_{ij})$，我们将验证存储的属性 $a'_{ij}$ 是否与输入属性矩阵中的原始属性 $A_{ij}$ 相同。如果这对所有 $k$ 个保留边都成立，则检查成功。\n\n实现将对每个测试案例执行这些计算，并报告所要求的量：$k$、$m$、$\\lVert E \\rVert_F$、$\\sqrt{m}\\,\\tau$、$\\lVert E \\rVert_2$，以及两个不等式检查和属性保留验证的布尔结果。", "answer": "```python\nimport numpy as np\n\ndef solve_case(W: np.ndarray, A: np.ndarray, tau: float):\n    \"\"\"\n    Processes a single test case for network sparsification and error analysis.\n\n    Args:\n        W: The weight matrix (n x n).\n        A: The attribute matrix (n x n).\n        tau: The sparsification threshold.\n\n    Returns:\n        A list containing the eight required output values for the case.\n    \"\"\"\n    n = W.shape[0]\n    adj_list = []\n    k = 0  # Number of retained edges\n    m = 0  # Number of dropped nonzero entries\n    E = np.zeros_like(W, dtype=float)\n\n    for i in range(n):\n        for j in range(n):\n            weight = W[i, j]\n            # An edge is a non-zero weight.\n            if weight != 0:\n                abs_weight = np.abs(weight)\n                if abs_weight = tau:\n                    # Retained edge\n                    k += 1\n                    adj_list.append((i, j, weight, A[i, j]))\n                else: # 0  abs_weight  tau\n                    # Dropped nonzero edge\n                    m += 1\n                    E[i, j] = weight\n    \n    # Compute norms\n    frobenius_norm_E = np.linalg.norm(E, 'fro')\n    spectral_norm_E = np.linalg.norm(E, 2)\n    \n    # Compute upper bound for the Frobenius norm\n    bound_frobenius = np.sqrt(m) * tau if m  0 else 0.0\n\n    # Verify inequalities\n    # Using np.isclose for robust floating-point comparison\n    inequality1_verified = frobenius_norm_E = bound_frobenius or np.isclose(frobenius_norm_E, bound_frobenius)\n    inequality2_verified = spectral_norm_E = frobenius_norm_E or np.isclose(spectral_norm_E, frobenius_norm_E)\n\n    # Verify attribute preservation\n    attributes_preserved = True\n    if not adj_list: # Vacuously true if no edges are retained\n        attributes_preserved = True\n    else:\n        for src, dst, w, attr in adj_list:\n            if attr != A[src, dst]:\n                attributes_preserved = False\n                break\n    \n    return [\n        k, \n        m, \n        float(frobenius_norm_E), \n        float(bound_frobenius), \n        float(spectral_norm_E), \n        inequality1_verified, \n        inequality2_verified, \n        attributes_preserved\n    ]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (\n            np.array([\n                [0, 0.21, -0.05, 0.11],\n                [0.03, 0, 0.41, -0.12],\n                [-0.21, 0.08, 0, 0.07],\n                [0.15, -0.09, 0.20, 0]\n            ]),\n            np.array([\n                [0, 1, -1, 1],\n                [1, 0, 1, -1],\n                [-1, 1, 0, 1],\n                [1, -1, 1, 0]\n            ]),\n            0.15\n        ),\n        (\n            np.array([\n                [0, -0.20, 0],\n                [0.18, 0, 0.05],\n                [0, -0.04, 0]\n            ]),\n            np.array([\n                [0, -1, 0],\n                [1, 0, 1],\n                [0, -1, 0]\n            ]),\n            0.0\n        ),\n        (\n            np.array([\n                [0, 0.30, -0.10, 0, 0.20],\n                [-0.25, 0, 0.05, -0.40, 0],\n                [0.10, -0.20, 0, 0.30, -0.05],\n                [0, 0, -0.15, 0, 0.10],\n                [-0.20, 0.25, 0, -0.05, 0]\n            ]),\n            np.array([\n                [0, 1, -1, 0, 1],\n                [-1, 0, 1, -1, 0],\n                [1, -1, 0, 1, -1],\n                [0, 0, -1, 0, 1],\n                [-1, 1, 0, -1, 0]\n            ]),\n            0.5\n        )\n    ]\n\n    results = []\n    for W, A, tau in test_cases:\n        result = solve_case(W, A, tau)\n        results.append(result)\n\n    # Format the final output string exactly as required\n    # Python's str() adds spaces, so we replace them.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3332692"}]}