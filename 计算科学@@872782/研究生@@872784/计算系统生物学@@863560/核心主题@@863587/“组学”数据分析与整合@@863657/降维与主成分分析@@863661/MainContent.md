## 引言
在[计算系统生物学](@entry_id:747636)时代，高通量技术的飞速发展使我们能够以前所未有的分辨率测量生物系统的多个层面，从基因组、[转录组](@entry_id:274025)到蛋白质组。然而，这些“组学”数据带来了巨大的挑战：它们的维度极高，往往包含数以万计的特征，而样本数量相对有限。如何在噪音和冗余中提取有意义的生物学洞见，成为数据分析的核心任务。

[主成分分析](@entry_id:145395)（Principal Component Analysis, PCA）正是应对这一挑战的基石性方法。作为一种强大且应用广泛的无监督[降维技术](@entry_id:169164)，PCA能够将复杂的[高维数据](@entry_id:138874)集投影到少数几个揭示其内在结构的关键维度上，从而实现数据的可视化、去噪和模式发现。

然而，PCA的有效应用远不止于调用一个标准函数。要真正驾驭这一工具，研究者必须深入理解其背后的数学原理、统计假设以及其固有的局限性。仅仅将其视为“黑箱”操作，往往会导致对结果的误读，甚至得出错误的科学结论。本文旨在填补理论与实践之间的鸿沟，为读者提供一个关于PCA的全面而深入的视角。

我们将分三个章节展开探讨。在“原理与机制”中，我们将深入剖析PCA的数学核心，从最大化[方差](@entry_id:200758)的目标到处理高维数据的计算技巧，并探讨其与概率模型及[非线性](@entry_id:637147)方法的联系。接着，在“应用与[交叉](@entry_id:147634)学科联系”中，我们将展示PCA在生物数据探索、实验质量控制、动态过程推断等多样化场景中的实际威力。最后，通过一系列“动手实践”问题，您将有机会将理论知识应用于解决具体的计算挑战。

让我们从第一章开始，揭开主成分分析的数学面纱，理解其如何从根本上定义和捕捉数据中的主要变异。

## 原理与机制

主成分分析（Principal Component Analysis, PCA）是探索和可视化[高维数据](@entry_id:138874)集（如[计算系统生物学](@entry_id:747636)中产生的组学数据）的基石。继导论之后，本章将深入探讨 PCA 的核心数学原理、其变体背后的机制，以及在面对现代生物学数据的复杂性时出现的关键理论考量。我们将从 PCA 的基本目标出发，逐步构建一个全面的框架，涵盖从算法实现到[非线性](@entry_id:637147)扩展的各个方面。

### PCA 的核心目标：最大化[方差](@entry_id:200758)

从根本上说，PCA 旨在通过寻找数据中[方差](@entry_id:200758)最大的方向，来识别[高维数据](@entry_id:138874)中的主导变异模式。它构建了一个新的[正交坐标](@entry_id:166074)系，其中每个坐标轴（称为**主成分**，Principal Component, PC）都依次捕获剩余[方差](@entry_id:200758)的最大可能量。

给定一个中心化的数据矩阵 $X_c \in \mathbb{R}^{n \times p}$，其中 $n$ 个样本（行）和 $p$ 个变量或特征（列），并且每列的均值为零。第一个主成分的方向由一个单位向量 $v_1 \in \mathbb{R}^p$ 定义，该向量使得数据投影到其上的[方差](@entry_id:200758)最大化。投影后的数据点（称为**[主成分得分](@entry_id:636463)**，PC scores）由向量 $X_c v_1$ 给出。由于数据已经中心化，投影后数据的样本[方差](@entry_id:200758)为：

$$
\text{Var}(X_c v_1) = \frac{1}{n-1} (X_c v_1)^T (X_c v_1) = v_1^T \left( \frac{1}{n-1} X_c^T X_c \right) v_1 = v_1^T S v_1
$$

这里，$S = \frac{1}{n-1} X_c^T X_c$ 是样本**[协方差矩阵](@entry_id:139155)**。因此，PCA 的目标是找到一个单位向量 $v$ 来最大化瑞利商 $v^T S v$。根据线性代数的基本原理，这个问题的解是协方差矩阵 $S$ 的**[特征向量](@entry_id:151813)**（eigenvector）。具体来说，最大化[方差](@entry_id:200758)的向量 $v_1$ 是与 $S$ 的最大[特征值](@entry_id:154894) $\lambda_1$ 相关联的[特征向量](@entry_id:151813)。这个[特征向量](@entry_id:151813) $v_1$ 被称为第一主成分的**[载荷向量](@entry_id:635284)**（loading vector），它表示了原始 $p$ 个特征对该主成分的贡献权重。

类似地，第二主成分 $v_2$ 是在与 $v_1$ 正交的约束下，最大化投影[方差](@entry_id:200758)的方向，它对应于 $S$ 的第二大[特征值](@entry_id:154894) $\lambda_2$ 的[特征向量](@entry_id:151813)。这个过程一直持续下去，第 $k$ 个主成分 $v_k$ 就是 $S$ 的第 $k$ 个[特征向量](@entry_id:151813)。

[特征值](@entry_id:154894) $\lambda_k$ 本身具有重要的意义：它量化了第 $k$ 个主成分所捕获的[方差](@entry_id:200758)大小。数据集中的**总[方差](@entry_id:200758)**由协方差矩阵的迹（trace）给出，即 $\text{tr}(S) = \sum_{j=1}^p S_{jj} = \sum_{k=1}^p \lambda_k$。因此，第 $k$ 个主成分解释的**[方差比](@entry_id:162608)例**（Proportion of Variance Explained, PVE）可以计算为 $\lambda_k / \text{tr}(S)$。这个比例是评估每个主成分相对重要性的关键指标。[@problem_id:3302550]

### [关联矩阵](@entry_id:263683)的选择：[协方差与相关性](@entry_id:262778)

在应用 PCA 之前，一个关键的决策是选择用于[特征分解](@entry_id:181333)的[关联矩阵](@entry_id:263683)：是样本[协方差矩阵](@entry_id:139155) $S$ 还是样本**[相关矩阵](@entry_id:262631)** $R$？这个选择对结果有深远的影响，尤其是在处理具有不同测量单位和变异尺度的[异构数据](@entry_id:265660)集时，例如整合了转录组、[蛋白质组](@entry_id:150306)和[代谢组](@entry_id:150409)数据的[多组学](@entry_id:148370)研究。[@problem_id:3302507]

[协方差矩阵](@entry_id:139155) $S$ 的元素 $s_{jk}$ 衡量了特征 $j$ 和特征 $k$ 共同变化的程度，其单位是两个特征单位的乘积。因此，PCA 在[协方差矩阵](@entry_id:139155)上进行时，对变量的尺度高度敏感。具有较大数值[方差](@entry_id:200758)的特征（可能仅仅因为其测量单位，例如，将蛋白质丰度单位从纳克变为皮克）将在主成分的计算中占据主导地位，而不管其生物学重要性如何。

相比之下，[相关矩阵](@entry_id:262631) $R$ 是通过用每个特征的[标准差](@entry_id:153618)对协[方差](@entry_id:200758)进行归一化得到的。其元素 $r_{jk} = s_{jk} / (\sqrt{s_{jj}} \sqrt{s_{kk}})$ 是无量纲的，并且值域在 $[-1, 1]$ 之间。在[相关矩阵](@entry_id:262631)上执行 PCA，相当于在**[标准化](@entry_id:637219)**（或 z-score 变换）后的数据上执行 PCA，即每个特征都被调整为零均值和单位[方差](@entry_id:200758)。这个过程确保了所有特征在分析开始时具有同等的权重，从而防止了高[方差](@entry_id:200758)特征不成比例地影响结果。[@problem_id:3302507]

因此，对于[多组学](@entry_id:148370)数据等[异构数据](@entry_id:265660)集，标准做法是在[相关矩阵](@entry_id:262631)上执行 PCA。这等效于对[标准化](@entry_id:637219)数据 $Z = X_c D^{-1}$ 进行 PCA，其中 $D$ 是一个[对角矩阵](@entry_id:637782)，其对角元素 $D_{jj} = \sqrt{S_{jj}}$ 是各特征的标准差。[@problem_id:3302580]

从更深层次的数学角度看，对[相关矩阵](@entry_id:262631) $R$ 进行 PCA 实际上是在求解一个**[广义特征值问题](@entry_id:151614)**。如果 $v$ 是 $R$ 的[特征向量](@entry_id:151813)，即 $Rv = \lambda v$，那么在原始（未缩放）[特征空间](@entry_id:638014)中产生相同[主成分得分](@entry_id:636463)的权重向量 $w = D^{-1}v$ 满足：
$$
S w = \lambda D^2 w
$$
这个关系式明确地揭示了，在[相关矩阵](@entry_id:262631) PCA 中，对原始特征的有效权重 $w_j$ 是通过其在[标准化](@entry_id:637219)空间中的载荷 $v_j$ 除以其标准差 $\sqrt{S_{jj}}$ 得到的。这意味着，原始[方差](@entry_id:200758)越大的特征，其对定义主成分方向的贡献被隐式地**减小**了。[@problem_id:3302580]

只有当所有特征的[方差](@entry_id:200758)已经大致相等时（例如，通过某种**[方差](@entry_id:200758)稳定化变换**），基于协[方差](@entry_id:200758)的 PCA 和基于相关性的 PCA 才会产生近似等价的结果。在这种情况下，$D \approx cI$（其中 $c$ 是一个常数，$I$ 是[单位矩阵](@entry_id:156724)），协方差矩阵 $S$ 近似地与[相关矩阵](@entry_id:262631) $R$ 成正比（$R \approx \frac{1}{c^2} S$），因此它们共享相同的[特征向量](@entry_id:151813)。[@problem_id:3302580]

### 计算考量与高维性

当特征数量 $p$ 远大于样本数量 $n$（即 $p \gg n$，这在基因组学中很常见）时，显式地计算和存储 $p \times p$ 的[协方差矩阵](@entry_id:139155) $S$ 会变得非常耗时且占用大量内存。幸运的是，存在更高效的计算策略。

#### 幂迭代法

PCA 的主要目标通常是找到前几个主成分，而不是完整的特征谱。**幂迭代法**（Power Iteration）是一种经典的算法，它可以在不形成完整协方差矩阵的情况下，迭代地找到主导[特征向量](@entry_id:151813)。该算法从一个随机的[单位向量](@entry_id:165907) $v^{(0)}$ 开始，并重复应用以下更新规则：
$$
v^{(t+1)} = \frac{S v^{(t)}}{\|S v^{(t)}\|_2}
$$
这里的关键在于，矩阵-向量乘积 $S v^{(t)}$ 可以通过 "无矩阵" 的方式计算：
$$
S v^{(t)} = \left( \frac{1}{n-1} X_c^T X_c \right) v^{(t)} = \frac{1}{n-1} X_c^T (X_c v^{(t)})
$$
这个操作只需要两个矩阵-向量乘法，一个与 $X_c$ 相乘，另一个与 $X_c^T$ 相乘，完全避免了构建 $p \times p$ 的矩阵 $S$。[@problem_id:3302550] [@problem_id:3302511]

只要主导[特征值](@entry_id:154894)是唯一的（即 $\lambda_1 > \lambda_2$），并且初始向量 $v^{(0)}$ 在主导[特征向量](@entry_id:151813) $v_1$ 的方向上有非零分量，[幂迭代法](@entry_id:148021)就能保证收敛到 $v_1$（或 $-v_1$）。其收敛速率由[特征值](@entry_id:154894)谱的间隙决定，具体为 $|\lambda_2 / \lambda_1|$。[@problem_id:3302511]

#### PCA 对偶性

在 $p \gg n$ 的情况下，存在一个更为高效的技巧，称为 **PCA 对偶性**（PCA duality）。与其对 $p \times p$ 的[协方差矩阵](@entry_id:139155) $S = \frac{1}{n-1} X_c^T X_c$ 进行[特征分解](@entry_id:181333)，我们可以转而分析 $n \times n$ 的 **[格拉姆矩阵](@entry_id:203297)**（Gram matrix） $G = \frac{1}{n-1} X_c X_c^T$。

$S$ 和 $G$ 的非零[特征值](@entry_id:154894)是完全相同的。此外，它们各自的[特征向量](@entry_id:151813)之间存在简单的关系。如果 $u_i$ 是 $G$ 的一个[特征向量](@entry_id:151813)，那么对应的 $S$ 的[特征向量](@entry_id:151813) $v_i$ 可以通过以下方式恢复：
$$
v_i \propto X_c^T u_i
$$
由于 $n \ll p$，对 $n \times n$ 的矩阵 $G$ 进行[特征分解](@entry_id:181333)在计算上要便宜得多。我们可以先用[幂迭代法](@entry_id:148021)或其他方法找到 $G$ 的主导[特征向量](@entry_id:151813) $u_1$，然后通过一次矩阵-向量乘法 $X_c^T u_1$ 得到 $v_1$ 的方向，最后进行归一化。这种方法在处理高维[转录组](@entry_id:274025)或基因组数据时是标准实践。[@problem_id:3302511]

### PCA 的统计解释与几何视角

PCA 不仅仅是一个计算程序；它有深刻的统计和几何基础。

#### 预处理的统计假设

我们对数据进行的操作（如中心化）隐含了对数据生成过程的假设。
*   **列中心化**（即减去每个基因在所有样本中的均值）是 PCA 的标准步骤。这一操作的合理性基于一个[统计模型](@entry_id:165873)，即假定每个样本（行）是来自某个 $p$ 维[分布](@entry_id:182848)的[独立同分布](@entry_id:169067)（i.i.d.）的抽样。在这个模型下，无论真实的群体均值 $\mu$ 是多少，减去样本均值都是对 $\mu$ 的一致估计，并且得到的样本协方差矩阵 $S$ 是群体协方差矩阵 $\Sigma$ 的[无偏估计](@entry_id:756289)。因此，其[特征向量](@entry_id:151813)可以无偏地估计 $\Sigma$ 的[特征向量](@entry_id:151813)。[@problem_id:3302510]
*   **行中心化**（即减去每个样本中所有基因的均值）则对应于一个完全不同的模型。它适用于所谓的 Q-mode 分析，当我们更关心样本之间的关系时。这种做法的合理性在于假设存在一个潜在[因子模型](@entry_id:141879)，其中每个样本具有一个需要被移除的特异性基线效应（截距）。例如，在一个模型 $X_{ij} = \alpha_i + \text{结构} + \text{噪声}$ 中，其中 $\alpha_i$ 是样本 $i$ 的特有效应，行中心化可以有效地移除 $\alpha_i$。[@problem_id:3302510]

错误地应用中心化方法会导致结果的系统性偏差。例如，对旨在分析基因间协[方差](@entry_id:200758)的数据进行行中心化会引入错误的结构。

#### PCA 的几何视角：[子空间](@entry_id:150286)估计

从几何上讲，PCA 可以被看作是一个**[子空间](@entry_id:150286)估计**问题。如果数据模型是 $X = LS + \sigma E$，其中 $L$ 的列张成一个 $r$ 维的“[信号子空间](@entry_id:185227)” $\mathcal{U}_\star$，而 $E$ 是噪声，那么 PCA 的目标就是从带噪声的数据 $X$ 中估计出这个[子空间](@entry_id:150286) $\mathcal{U}_\star$。PCA 通过取 $X$ 的前 $r$ 个[左奇异向量](@entry_id:751233)（等价于 $S$ 的前 $r$ 个[特征向量](@entry_id:151813)）来形成一个估计的[子空间](@entry_id:150286) $\hat{\mathcal{U}}$。

为了量化估计[子空间](@entry_id:150286) $\hat{\mathcal{U}}$ 与真实[子空间](@entry_id:150286) $\mathcal{U}_\star$ 之间的差异，我们可以使用**格拉斯曼[流形](@entry_id:153038)**（Grassmannian）的几何概念。格拉斯曼[流形](@entry_id:153038) $\mathrm{Gr}(r,p)$ 是 $\mathbb{R}^p$ 中所有 $r$ 维[子空间](@entry_id:150286)的集合。

*   **主角度**（Principal Angles）：两个[子空间](@entry_id:150286)之间的关系可以通过一组 $r$ 个主角度 $\theta_1, \ldots, \theta_r$ 来完全刻画。如果 $U$ 和 $V$ 是分别张成 $\mathcal{U}_\star$ 和 $\hat{\mathcal{U}}$ 的正交基矩阵，那么这些角度可以通过计算矩阵 $U^T V$ 的奇异值 $\sigma_i$ 来得到：$\cos \theta_i = \sigma_i$。如果两个[子空间](@entry_id:150286)重合，则所有 $\cos \theta_i = 1$，所有 $\theta_i = 0$。[@problem_id:3302544]

*   **[子空间距离](@entry_id:198307)**：基于主角度，可以定义多种[距离度量](@entry_id:636073)。例如，**投影距离**（或[弦距离](@entry_id:170189)）为 $d_p(\mathcal{U}_\star, \hat{\mathcal{U}}) = \left( \sum_{i=1}^r \sin^2 \theta_i \right)^{1/2}$，它等于两个[子空间](@entry_id:150286)[投影算子](@entry_id:154142)之差的[弗罗贝尼乌斯范数](@entry_id:143384)的一半，即 $\frac{1}{\sqrt{2}}\|UU^T - VV^T\|_F$。而格拉斯曼[流形](@entry_id:153038)上的**[测地线](@entry_id:269969)距离**则由 $d_g(\mathcal{U}_\star, \hat{\mathcal{U}}) = \left( \sum_{i=1}^r \theta_i^2 \right)^{1/2}$ 给出。这些[距离度量](@entry_id:636073)对于正交基的选择是不变的，为比较[子空间](@entry_id:150286)提供了一个严谨的框架。[@problem_id:3302544]

此外，PCA 具有重要的**[不变性](@entry_id:140168)**。例如，对样本进行[正交变换](@entry_id:155650)（$X \to XQ$，其中 $Q$ 是正交矩阵）不会改变 $XX^T$ 的值，因此也不会改变其[特征向量](@entry_id:151813)。这意味着 PCA 估计出的基因[子空间](@entry_id:150286) $\hat{\mathcal{U}}$ 对样本的旋转是不变的。[@problem_id:3302544]

### 确定显著成分的数量

一个核心的实际问题是：我们应该保留多少个主成分？如何区分代表真实生物信号的 PC 和那些仅由随机噪声产生的 PC？对于[高维数据](@entry_id:138874)，简单的[启发式](@entry_id:261307)规则（如 Kaiser 准则，即保留[特征值](@entry_id:154894)大于 1 的 PC）是严重误导的。现代统计学为此提供了更严谨的工具。

#### [随机矩阵理论](@entry_id:142253)与 Marchenko-Pastur 定律

**[随机矩阵理论](@entry_id:142253)**（Random Matrix Theory, RMT）为我们理解纯噪声数据的行为提供了强大的理论基础。考虑一个纯噪声数据矩阵 $X$，其元素为均值为 0、[方差](@entry_id:200758)为 $\sigma^2$ 的[独立同分布随机变量](@entry_id:270381)。在 $p, n \to \infty$ 且 $p/n \to \gamma$ 的高维极限下，其样本协方差矩阵 $S = \frac{1}{n}X^T X$ 的[特征值分布](@entry_id:194746)并不会像[经典统计学](@entry_id:150683)那样集中在 $\sigma^2$ 附近，而是遵循 **Marchenko-Pastur 定律**。[@problem_id:3302520]

该定律指出，噪声[特征值](@entry_id:154894)会[分布](@entry_id:182848)在一个紧致的区间，称为**谱的“主体”**（bulk of the spectrum），其边界为：
$$
\lambda_{\pm} = \sigma^2(1 \pm \sqrt{\gamma})^2
$$
这个结果具有革命性的意义：它为我们提供了一个基于理论的**硬阈值**。在实践中，我们可以将数据中观测到的[特征值](@entry_id:154894)与这个理论上的噪声谱[上界](@entry_id:274738) $\lambda_+$进行比较。任何显著大于 $\lambda_+$ 的观测[特征值](@entry_id:154894)，被称为**“尖峰”[特征值](@entry_id:154894)**（spike eigenvalue），可以被认为是代表了真实的、非随机的信号结构。而落在 $[\lambda_-, \lambda_+]$ 区间内的[特征值](@entry_id:154894)则与纯噪声无法区分。[@problem_id:3302547]

例如，在一个 $p=5000$ 个基因、$n=100$ 个样本的研究中，$\gamma=50$。假设数据已标准化，$\sigma^2=1$，那么噪声谱的[上界](@entry_id:274738)是 $\lambda_+ = (1+\sqrt{50})^2 \approx 65.1$。任何小于这个值的[特征值](@entry_id:154894)都可能源于噪声。这也揭示了一个反直觉的事实：在 $p \gg n$ 的情况下，纯噪声数据的非零[特征值](@entry_id:154894)的平均值约为 $p/n$（在此例中为 50），因此观测到量级为 50 的[特征值](@entry_id:154894)完全是预料之中的噪声行为。[@problem_id:3302547]

#### 平行分析

**平行分析**（Parallel Analysis）是另一种强大的、基于数据驱动的[非参数方法](@entry_id:138925)。其思想是创建一个与真实数据“统计特性”相匹配的零假设数据集。具体做法是：保持每个基因（行）的数值集合不变，但将其在样本（列）间的顺序随机打乱（[置换](@entry_id:136432)）。这个过程会破坏基因间的相关性结构，但保留了每个基因自身的[方差](@entry_id:200758)[分布](@entry_id:182848)。

通过多次重复此[置换](@entry_id:136432)过程，我们可以为每个秩次的[特征值](@entry_id:154894)（例如，第一大[特征值](@entry_id:154894)，第二大[特征值](@entry_id:154894)等）生成一个经验[零分布](@entry_id:195412)。然后，我们将真实数据中观测到的第 $k$ 大[特征值](@entry_id:154894) $\lambda_k$ 与其对应的[零分布](@entry_id:195412)的某个高分位数（如 95%）进行比较。如果 $\lambda_k$ 超出了这个阈值，我们就认为第 $k$ 个主成分是统计显著的。[@problem_id:3302547]

### 概率与[非线性](@entry_id:637147)扩展

经典 PCA 是一个纯粹的代数和几何过程，但它也可以被置于更广泛的[统计模型](@entry_id:165873)框架中，并扩展到[非线性](@entry_id:637147)领域。

#### 概率 PCA 与[因子分析](@entry_id:165399)

**概率 PCA**（Probabilistic PCA, PPCA）将 PCA 重新表述为一个高斯潜在变量模型。它假设观测数据 $x \in \mathbb{R}^p$ 是由一个低维（$k$ 维）的潜在变量 $z \in \mathbb{R}^k$ 通过线性映射 $W$ 生成的，并加上了[高斯噪声](@entry_id:260752)：
$$
x = Wz + \mu + \epsilon
$$
其中，潜在变量 $z \sim \mathcal{N}(0, I_k)$，噪声 $\epsilon \sim \mathcal{N}(0, \sigma^2 I_p)$。这种**各向同性**（isotropic）的噪声假设——即所有基因共享同一个噪声[方差](@entry_id:200758) $\sigma^2$——是 PPCA 的标志。在这个模型下，观测数据的边缘协[方差](@entry_id:200758)为 $\Sigma = WW^T + \sigma^2 I_p$。可以证明，该模型的最大似然解所确定的 $W$ 的列空间，恰好与经典 PCA 的主[子空间](@entry_id:150286)重合。[@problem_id:3302588]

**[因子分析](@entry_id:165399)**（Factor Analysis, FA）是 PPCA 的一个推广。它采用了相同的[生成模型](@entry_id:177561)，但允许噪声是**异[方差](@entry_id:200758)**（heteroscedastic）的，即 $\epsilon \sim \mathcal{N}(0, \Psi)$，其中 $\Psi$ 是一个对角矩阵，$\Psi = \text{diag}(\psi_1, \ldots, \psi_p)$。这允许每个基因拥有自己独特的噪声[方差](@entry_id:200758) $\psi_j$，这在生物学上通常更为现实，因为不同基因的[测量精度](@entry_id:271560)可能不同。然而，这种灵活性的代价是与经典 PCA 的直接联系被打破了；FA 的解通常不对应于样本[协方差矩阵](@entry_id:139155)的[特征向量](@entry_id:151813)。[@problem_id:3302588]

一个关键的共同点是，PPCA 和 FA 都存在**旋转不确定性**（rotational indeterminacy）。由于对于任何[正交矩阵](@entry_id:169220) $R$，都有 $WW^T = (WR)(WR)^T$，因此载荷矩阵 $W$ 只能被确定到相差一个任意的正交变换。这意味着潜在因子本身是没有唯一确定的“身份”的，除非施加额外的约束。[@problem_id:3302588]

#### 核 PCA 与[非线性](@entry_id:637147)[流形](@entry_id:153038)

经典 PCA 只能捕获数据中的线性结构。然而，生物学过程，如细胞分化，通常沿着嵌入在高维基因表达空间中的低维**[非线性](@entry_id:637147)[流形](@entry_id:153038)**（nonlinear manifold）展开。为了揭示这些结构，我们需要[非线性降维](@entry_id:636435)方法。

**核 PCA**（Kernel PCA, KPCA）是一种强大的扩展，它通过“[核技巧](@entry_id:144768)”隐式地将数据映射到一个高维（甚至无限维）的[特征空间](@entry_id:638014)，然后在这个空间中执行 PCA。**高斯核** $k(\mathbf{x}, \mathbf{y}) = \exp(-\|\mathbf{x}-\mathbf{y}\|^2/(2\sigma^2))$ 是一个常用选择。核的**带宽**参数 $\sigma$ 至关重要：它必须被仔细调整以[匹配数](@entry_id:274175)据的局部尺度，通常选择在与最近邻距离相当的量级。[@problem_id:3302526]

**[扩散图](@entry_id:748414)**（Diffusion Maps）是另一种先进的[非线性](@entry_id:637147)方法，它在捕获[流形](@entry_id:153038)内在几何方面尤其出色。它将数据点构建成一个图，并模拟其上的[随机游走](@entry_id:142620)（或扩散过程）。关键的一步是**马尔可夫归一化**，它将核矩阵转换为一个随机矩阵。这个步骤使得[扩散图](@entry_id:748414)对非均匀的样本密度具有内在的**鲁棒性**。相比之下，KPCA 的主成分可能会被数据空间中的高密度区域所“吸引”和主导。

理论上，[扩散图](@entry_id:748414)的嵌入坐标在特定条件下收敛于[流形](@entry_id:153038)的**[拉普拉斯-贝尔特拉米算子](@entry_id:267002)**的[特征函数](@entry_id:186820)，这些[特征函数](@entry_id:186820)构成了[流形](@entry_id:153038)上函数的“[傅里叶基](@entry_id:201167)”。这使得[扩散图](@entry_id:748414)在恢复如发育轨迹等内在几何结构时特别可靠，而 KPCA 则不具备这种与[流形](@entry_id:153038)[拉普拉斯算子](@entry_id:146319)的直接联系。因此，在分析沿着[非均匀采样](@entry_id:752610)的轨迹（如拟时序）[排列](@entry_id:136432)的单细胞数据时，[扩散图](@entry_id:748414)通常是比 KPCA 更为稳健和可靠的选择。[@problem_id:3302526]