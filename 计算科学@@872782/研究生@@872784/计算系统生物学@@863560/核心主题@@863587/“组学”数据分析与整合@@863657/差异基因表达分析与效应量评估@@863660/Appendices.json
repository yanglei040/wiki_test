{"hands_on_practices": [{"introduction": "在比较不同样本的基因表达水平之前，处理测序深度等技术性变异至关重要。此练习将引导您亲手实现几种关键的归一化方法，包括TMM、上四分位数法和DESeq2的中位数比率法。通过这个过程，您将量化比较不同归一化策略对效应量（effect-size）估计的影响，从而深刻理解为何审慎选择归一化方法是保证差异表达分析结果稳健性的基石 ([@problem_id:3301660])。", "problem": "给定代表两组条件下多个基因在生物学重复样本中的核糖核酸测序（RNA-seq）基因表达的计数矩阵，要求研究基因水平效应量估计对标准化方法选择的敏感性。任务涉及三种广泛使用的标准化方案：M值的裁剪均值（Trimmed Mean of M-values, TMM）、上四分位数（Upper-Quartile）以及在差异表达测序（Differential Expression Sequencing, DESeq）中使用的比率中位数（median-of-ratios）尺寸因子。计算目标是：对于每个基因，在每种标准化方法下估计一个效应量；计算不同方法之间效应量的两两差异；并识别对标准化方法选择最敏感的基因。\n\n基础和假设：测序计数是通过分子抽样产生的非负整数，并带有样本特异性的乘法偏差。假设有 $G$ 个基因，索引为 $g \\in \\{0,\\dots,G-1\\}$，以及 $S$ 个样本，索引为 $i \\in \\{0,\\dots,S-1\\}$。令 $x_{g i}$ 表示样本 $i$ 中基因 $g$ 的观测计数。令 $L_i = \\sum_{g=0}^{G-1} x_{g i}$ 表示样本 $i$ 的文库大小。每个样本属于一个条件 $c_i \\in \\{\\text{A}, \\text{B}\\}$。为了进行标准化，定义样本特异性尺寸因子 $s_i$，使得标准化后的计数为 $y_{g i} = x_{g i} / s_i$。条件间的效应量将通过 $y_{g i}$ 的组水平摘要统计量的对数比率来评估。\n\n要实现的标准化方法：\n- M值的裁剪均值（TMM）：TMM通过移除极端基因并计算相对表达的对数比率的均值来估计相对缩放偏移。选择一个参考样本 $r$，该样本的文库大小 $L_r$ 最接近所有文库大小的中位数 $\\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$。对于每个非参考样本 $i \\neq r$，定义每个基因的量 $p_{g i} = x_{g i} / L_i$ 和 $p_{g r} = x_{g r} / L_r$，然后对于 $x_{g i}  0$ 且 $x_{g r}  0$ 的基因，计算 $M_{g i} = \\log_2\\!\\left(\\frac{p_{g i}}{p_{g r}}\\right)$ 和 $A_{g i} = \\frac{1}{2}\\log_2\\!\\left(p_{g i} p_{g r}\\right)$。通过裁剪掉 $M_{g i}$ 的较低和较高的 $p_M$ 分位数以及 $A_{g i}$ 的较低和较高的 $p_A$ 分位数来排除基因，然后对保留的基因计算裁剪均值 $\\overline{M}_i$。设置 $s_r = 1$，对于 $i \\neq r$，设置 $s_i = 2^{\\overline{M}_i}$。如果对于某个样本 $i$，裁剪后没有基因剩下，则设置 $s_i = 1$。\n- 上四分位数尺寸因子：对于每个样本 $i$，计算其上四分位数 $U_i$，即 $\\{x_{g i} : x_{g i}  0\\}$ 的第 $75$ 百分位数。定义 $s_i = U_i$。如果一个样本没有正数计数，则设置 $s_i = 1$。\n- DESeq比率中位数尺寸因子：对于在所有样本中都具有严格正数计数（即，对所有 $i$ 都有 $x_{g i}  0$）的每个基因 $g$，计算其几何平均数 $G_g = \\exp\\!\\left(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i}\\right)$。对于每个样本 $i$，对所有此类基因 $g$ 构建比率 $r_{g i} = x_{g i}/G_g$，并将 $s_i$ 定义为 $\\operatorname{median}\\{r_{g i}\\}_g$。如果没有基因在所有样本中都具有严格正数计数，则退而使用文库大小缩放方法 $s_i = L_i / \\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$。\n\n效应量定义：对于一种产生尺寸因子 $s^{(m)}_i$ 的标准化方法 $m$，定义标准化计数为 $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$。对于一个固定的伪计数 $c  0$，定义条件 $\\text{A}$ 内的组均值为 $\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$，条件 $\\text{B}$ 内的组均值为 $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$，其中 $I_{\\text{A}}$ 和 $I_{\\text{B}}$ 分别是条件 $\\text{A}$ 和 $\\text{B}$ 中样本的索引集。效应量为 $ES^{(m)}_g = \\log_2\\!\\left(\\frac{\\mu^{(m)}_{g,\\text{A}} + c}{\\mu^{(m)}_{g,\\text{B}} + c}\\right)$。\n\n敏感性分析：对于一对方法 $(m_1,m_2)$，定义 $\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$。定义敏感性得分 $S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$。对于一个阈值 $\\tau  0$，如果 $S_g \\ge \\tau$，则一个基因被认为是敏感的。\n\n您的程序必须实现这三种标准化方法，为每个基因和每种方法计算 $ES^{(m)}_g$，为所有方法对计算 $\\Delta ES_g$，计算 $S_g$，并为每个提供的测试用例返回被分类为敏感基因的从零开始的索引。如果任何中间量需要裁剪或集合排除，请遵循上述定义。如果任何标准化方法遇到退化情况（例如，没有有效基因或百分位数未定义），请应用所述的显式回退策略。\n\n测试套件（三个用例；每个用例指定一个形状为 $G \\times S$ 的整数计数矩阵、分组成员、伪计数 $c$、裁剪参数 $p_M$ 和 $p_A$，以及敏感性阈值 $\\tau$）。对于所有用例，使用从零开始的基因索引和从零开始的样本索引。\n\n- 用例 1（理想路径，变化的文库大小，混合的差异表达）：\n    - $G = 8$ 个基因和 $S = 6$ 个样本的计数 $x_{g i}$（行是基因 $g = 0,\\dots,7$；列是样本 $i = 0,\\dots,5$）：\n      - 基因 0：$[50,55,52,49,53,50]$\n      - 基因 1：$[100,95,105,280,300,320]$\n      - 基因 2：$[200,210,190,80,75,85]$\n      - 基因 3：$[0,5,0,30,35,40]$\n      - 基因 4：$[400,420,410,390,380,400]$\n      - 基因 5：$[10,12,11,9,8,10]$\n      - 基因 6：$[8000,8200,7800,7000,7200,7100]$\n      - 基因 7：$[2,1,0,3,4,0]$\n    - 条件索引集：$I_{\\text{A}} = \\{0,1,2\\}$，$I_{\\text{B}} = \\{3,4,5\\}$。\n    - 伪计数：$c = 0.5$。\n    - TMM裁剪分数：$p_M = 0.3$，$p_A = 0.05$。\n    - 敏感性阈值：$\\tau = 0.3$。\n\n- 用例 2（边界情况，大量零值，极端的文库大小不平衡，如果需要可能会触发DESeq回退策略）：\n    - $G = 6$ 个基因和 $S = 6$ 个样本的计数 $x_{g i}$：\n      - 基因 0：$[0,0,0,0,0,0]$\n      - 基因 1：$[5,0,2,0,5,2]$\n      - 基因 2：$[0,0,50,0,0,150]$\n      - 基因 3：$[20,2000,25,10,900,20]$\n      - 基因 4：$[30,1500,20,20,800,30]$\n      - 基因 5：$[10,1000,15,5,700,10]$\n    - 条件索引集：$I_{\\text{A}} = \\{0,1,2\\}$，$I_{\\text{B}} = \\{3,4,5\\}$。\n    - 伪计数：$c = 0.5$。\n    - TMM裁剪分数：$p_M = 0.3$，$p_A = 0.05$。\n    - 敏感性阈值：$\\tau = 0.5$。\n\n- 用例 3（边缘情况，完全平衡的计数；所有方法应一致，导致零敏感性）：\n    - $G = 5$ 个基因和 $S = 6$ 个样本的计数 $x_{g i}$：\n      - 基因 0：$[100,100,100,100,100,100]$\n      - 基因 1：$[100,100,100,100,100,100]$\n      - 基因 2：$[100,100,100,100,100,100]$\n      - 基因 3：$[100,100,100,100,100,100]$\n      - 基因 4：$[100,100,100,100,100,100]$\n    - 条件索引集：$I_{\\text{A}} = \\{0,1,2\\}$，$I_{\\text{B}} = \\{3,4,5\\}$。\n    - 伪计数：$c = 0.5$。\n    - TMM裁剪分数：$p_M = 0.3$，$p_A = 0.05$。\n    - 敏感性阈值：$\\tau = 0.01$。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素本身是一个用方括号括起来的、逗号分隔的、不含空格的从零开始的基因索引列表。例如，一个有效的输出可能看起来像 $[[0,2,5],[1],[0,3]]$；如果某个用例没有敏感基因，则为该用例输出一个空列表 $[]$。", "solution": "该问题被认为是有效的，因为它科学地基于计算系统生物学的既定原则，定义明确且带有回退策略，并且内部一致。任务是实现三种不同的RNA-seq标准化方法——M值的裁剪均值（TMM）、上四分位数（UQ）和来自DESeq的比率中位数方案——并量化基因水平效应量估计对方法选择的敏感性。\n\n设给定的计数矩阵为 $X$，其元素为 $x_{g i}$，其中基因 $g \\in \\{0, \\dots, G-1\\}$，样本 $i \\in \\{0, \\dots, S-1\\}$。样本 $i$ 的文库大小为 $L_i = \\sum_{g=0}^{G-1} x_{g i}$。每个样本 $i$ 属于两个条件之一，表示为 $\\text{A}$ 和 $\\text{B}$，具有相应的样本索引集 $I_{\\text{A}}$ 和 $I_{\\text{B}}$。分析的核心是为三种标准化方法中的每一种计算样本特异性尺寸因子 $s_i$，然后用这些因子来获得标准化计数 $y_{g i} = x_{g i} / s_i$。\n\n第一种标准化方法是M值的裁剪均值（TMM）。该方法相对于一个选定的参考样本来估计尺寸因子。参考样本（索引为 $r$）被选为文库大小 $L_r$ 与所有文库大小的中位数 $\\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$ 距离最小的样本。根据定义，$s_r = 1$。对于其他每个样本 $i$，我们计算每个基因的表达比例的对数比率 $M_{g i} = \\log_2(\\frac{p_{g i}}{p_{g r}})$ 和平均表达强度 $A_{g i} = \\frac{1}{2}\\log_2(p_{g i} p_{g r})$，其中 $p_{g i} = x_{g i}/L_i$ 且 $p_{g r} = x_{g r}/L_r$。这些计算仅限于在样本 $i$ 和参考样本 $r$ 中都具有正计数的基因。通过对 $M_{gi}$ 和 $A_{gi}$ 的值进行裁剪来确定一组稳健的基因。具体来说，如果基因的 $M_{gi}$ 值落在该样本所有 $M_{gi}$ 值的较低或较高的 $p_M$ 分数内，或者其 $A_{gi}$ 值落在所有 $A_{gi}$ 值的较低或较高的 $p_A$ 分数内，则该基因被排除。然后，尺寸因子 $s_i$ 计算为 $s_i = 2^{\\overline{M}_i}$，其中 $\\overline{M}_i$ 是保留（未裁剪）基因的 $M_{gi}$ 值的均值。如果在裁剪后没有基因剩下，我们使用回退策略 $s_i = 1$。\n\n第二种方法是上四分位数（UQ）标准化。这是一种更简单的方法，其中每个样本 $i$ 的尺寸因子 $s_i$ 被设置为其正计数的第 $75$ 百分位数，即 $s_i = U_i = \\text{percentile}(\\{x_{g i} : x_{g i}  0\\}, 75)$。该方法对于少数几个表达量极高的基因是稳健的。如果一个样本不包含正计数，其尺寸因子被设置为默认值 $s_i = 1$。\n\n第三种方法是比率中位数法，这是DESeq包的特点。该方法首先通过计算每个基因 $g$ 在所有样本中的计数的几何平均数来建立一个伪参考样本：$G_g = \\exp(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i})$。此计算仅限于在每个样本中都具有严格正计数的基因。然后，对于每个样本 $i$，尺寸因子 $s_i$ 被确定为比率 $r_{g i} = x_{g i} / G_g$ 的中位数，该中位数是在用于计算几何平均数的基因集合上计算的。一个关键的回退策略被定义：如果没有一个基因在所有样本中都有正计数，则尺寸因子通过文库大小标准化来计算，$s_i = L_i / \\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$。\n\n一旦为每种方法 $m \\in \\{\\text{TMM}, \\text{UQ}, \\text{DESeq}\\}$ 计算出尺寸因子 $s^{(m)}_i$，我们就可以确定每个基因的效应量。标准化计数为 $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$。对每个基因 $g$ 在每个条件内计算这些标准化计数的算术平均值：$\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$ 和 $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$。添加一个伪计数 $c  0$ 来处理零值并稳定低计数基因的对数比率。基因 $g$ 在方法 $m$ 下的效应量是对数以2为底的倍数变化：$ES^{(m)}_g = \\log_2((\\mu^{(m)}_{g,\\text{A}} + c) / (\\mu^{(m)}_{g,\\text{B}} + c))$。\n\n最后，为评估对标准化方法的敏感性，我们计算每个基因效应量的两两差异，$\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$。基因 $g$ 的总体敏感性定义为三种方法对之间最大绝对差异：$S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$。如果一个基因的敏感性得分 $S_g$ 达到或超过给定的阈值 $\\tau$，则该基因被分类为敏感。最终输出包含每个测试用例中这些敏感基因的索引。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases for normalization sensitivity analysis.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"counts\": np.array([\n                [50, 55, 52, 49, 53, 50],\n                [100, 95, 105, 280, 300, 320],\n                [200, 210, 190, 80, 75, 85],\n                [0, 5, 0, 30, 35, 40],\n                [400, 420, 410, 390, 380, 400],\n                [10, 12, 11, 9, 8, 10],\n                [8000, 8200, 7800, 7000, 7200, 7100],\n                [2, 1, 0, 3, 4, 0]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.3\n        },\n        # Case 2 (boundary case, many zeros)\n        {\n            \"counts\": np.array([\n                [0, 0, 0, 0, 0, 0],\n                [5, 0, 2, 0, 5, 2],\n                [0, 0, 50, 0, 0, 150],\n                [20, 2000, 25, 10, 900, 20],\n                [30, 1500, 20, 20, 800, 30],\n                [10, 1000, 15, 5, 700, 10]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.5\n        },\n        # Case 3 (edge case, balanced counts)\n        {\n            \"counts\": np.array([\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.01\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        counts = params[\"counts\"]\n        cond_a_indices = params[\"cond_a_indices\"]\n        cond_b_indices = params[\"cond_b_indices\"]\n        pseudocount = params[\"pseudocount\"]\n        p_m = params[\"p_m\"]\n        p_a = params[\"p_a\"]\n        tau = params[\"tau\"]\n\n        # Run normalizations\n        s_tmm = normalize_tmm(counts, p_m, p_a)\n        s_uq = normalize_uq(counts)\n        s_deseq = normalize_deseq(counts)\n\n        # Calculate effect sizes\n        es_tmm = calculate_es(counts, s_tmm, cond_a_indices, cond_b_indices, pseudocount)\n        es_uq = calculate_es(counts, s_uq, cond_a_indices, cond_b_indices, pseudocount)\n        es_deseq = calculate_es(counts, s_deseq, cond_a_indices, cond_b_indices, pseudocount)\n\n        # Sensitivity analysis\n        delta_es_tmm_uq = np.abs(es_tmm - es_uq)\n        delta_es_tmm_deseq = np.abs(es_tmm - es_deseq)\n        delta_es_uq_deseq = np.abs(es_uq - es_deseq)\n        \n        sensitivity_scores = np.maximum.reduce([delta_es_tmm_uq, delta_es_tmm_deseq, delta_es_uq_deseq])\n        \n        sensitive_genes = np.where(sensitivity_scores = tau)[0].tolist()\n        all_results.append(sensitive_genes)\n\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef normalize_tmm(counts, p_m, p_a):\n    num_genes, num_samples = counts.shape\n    lib_sizes = np.sum(counts, axis=0)\n\n    # Handle all zero lib sizes to avoid division by zero\n    lib_sizes[lib_sizes == 0] = 1\n\n    median_lib_size = np.median(lib_sizes)\n    dists = np.abs(lib_sizes - median_lib_size)\n    ref_idx = np.argmin(dists)\n\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        if i == ref_idx:\n            continue\n\n        x_i = counts[:, i]\n        x_r = counts[:, ref_idx]\n        \n        # Filter for genes with positive counts in both samples\n        valid_genes_mask = (x_i  0)  (x_r  0)\n        \n        if not np.any(valid_genes_mask):\n            size_factors[i] = 1.0\n            continue\n            \n        x_i_filt = x_i[valid_genes_mask]\n        x_r_filt = x_r[valid_genes_mask]\n        \n        lib_i = lib_sizes[i]\n        lib_r = lib_sizes[ref_idx]\n\n        p_i = x_i_filt / lib_i\n        p_r = x_r_filt / lib_r\n\n        m_values = np.log2(p_i / p_r)\n        a_values = 0.5 * np.log2(p_i * p_r)\n        \n        # Trim M and A values\n        m_low = np.percentile(m_values, 100 * p_m, interpolation='linear')\n        m_high = np.percentile(m_values, 100 * (1 - p_m), interpolation='linear')\n\n        a_low = np.percentile(a_values, 100 * p_a, interpolation='linear')\n        a_high = np.percentile(a_values, 100 * (1 - p_a), interpolation='linear')\n        \n        retained_mask = (m_values = m_low)  (m_values = m_high)  \\\n                        (a_values = a_low)  (a_values = a_high)\n        \n        retained_m_values = m_values[retained_mask]\n\n        if len(retained_m_values) == 0:\n            size_factors[i] = 1.0\n        else:\n            trimmed_mean_m = np.mean(retained_m_values)\n            size_factors[i] = 2**trimmed_mean_m\n            \n    return size_factors\n\ndef normalize_uq(counts):\n    num_samples = counts.shape[1]\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        sample_counts = counts[:, i]\n        positive_counts = sample_counts[sample_counts  0]\n        \n        if len(positive_counts) == 0:\n            size_factors[i] = 1.0\n        else:\n            uq = np.percentile(positive_counts, 75, interpolation='linear')\n            size_factors[i] = uq if uq  0 else 1.0\n\n    return size_factors\n\ndef normalize_deseq(counts):\n    num_genes, num_samples = counts.shape\n    \n    # Find genes with strictly positive counts across all samples\n    positive_across_all_mask = np.all(counts  0, axis=1)\n\n    if not np.any(positive_across_all_mask):\n        # Fallback: library size normalization\n        lib_sizes = np.sum(counts, axis=0)\n        # Avoid division by zero for all-zero library or median of zero\n        lib_sizes[lib_sizes == 0] = 1\n        median_lib = np.median(lib_sizes)\n        if median_lib == 0:\n            median_lib = 1\n        return lib_sizes / median_lib\n\n    # Main path\n    counts_for_geo_mean = counts[positive_across_all_mask, :]\n    \n    with np.errstate(divide='ignore'):\n        log_counts = np.log(counts_for_geo_mean)\n    \n    geo_means = np.exp(np.mean(log_counts, axis=1))\n\n    ratios = counts_for_geo_mean / geo_means[:, np.newaxis]\n    size_factors = np.median(ratios, axis=0)\n    \n    return size_factors\n\ndef calculate_es(counts, size_factors, cond_a_indices, cond_b_indices, pseudocount):\n    # Normalize counts\n    norm_counts = counts / size_factors[np.newaxis, :]\n    \n    # Group means\n    mean_a = np.mean(norm_counts[:, cond_a_indices], axis=1)\n    mean_b = np.mean(norm_counts[:, cond_b_indices], axis=1)\n    \n    # Effect size\n    ratio = (mean_a + pseudocount) / (mean_b + pseudocount)\n    effect_sizes = np.log2(ratio)\n    \n    return effect_sizes\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3301660"}, {"introduction": "RNA测序的原始计数数据具有离散和过度离散（方差大于均值）的特性，因此需要专门的统计模型。负二项（Negative Binomial, NB）广义线性模型（GLM）是差异表达分析的基石，它能有效捕捉这些特性并估计实验条件对基因表达的影响。本练习将带您从第一性原理出发，推导并实现一个用于拟合NB-GLM的迭代重加权最小二乘（IRLS）算法，让您深入理解DESeq2等主流分析工具背后的核心统计引擎 ([@problem_id:3301611])。", "problem": "在计算系统生物学中，您将处理一个基于计数的基因表达场景，用于差异基因表达分析和效应量评估。其基本假设如下：(i) 分子生物学中心法则意味着基因表达水平可以通过读取计数进行量化；(ii) 当存在超出泊松模型的过度离散时，计数数据能很好地由负二项分布建模；(iii) 广义线性模型（GLM, Generalized Linear Model）提供了一个原则性框架，通过连接函数将协变量与分布的均值关联起来；以及 (iv) 最大似然估计（MLE, Maximum Likelihood Estimation）通过在指定的概率模型下最大化观测数据的似然来提供参数估计。\n\n对于由 $g$ 索引的每个基因，以及由 $i = 1, \\dots, n$ 索引的样本，原始计数 $y_{g i}$ 被建模为负二项随机变量，其均值参数为 $\\mu_{g i}$，基因特异性的离散度参数 $\\phi_g$ 已知。方差函数为 $V(y_{g i}) = \\mu_{g i} + \\phi_g \\mu_{g i}^2$。带有对数连接的广义线性模型指定为\n$$\n\\log \\mu_{g i} = \\alpha_g + x_i^\\top \\beta_g,\n$$\n其中 $\\alpha_g$ 是一个截距项，捕捉基因 $g$ 的基线表达水平，$x_i$ 是样本 $i$ 的已知协变量向量，$\\beta_g$ 是待估计的效应量参数向量（差异表达效应）。对于每个基因，离散度 $\\phi_g$ 是已知且固定的。您的任务是在指定的模型假设下，通过最大似然法估计 $\\beta_g$。\n\n从上述基本定义出发，推导一个仅基于负二项对数似然和带有对数连接的广义线性模型结构来计算 $\\beta_g$（以及滋扰截距项 $\\alpha_g$）的最大似然估计量的算法。该算法应基于第一性原理：使用最大似然估计框架，推导必要的得分方程，并设计一个能够收敛到最大似然估计量的迭代方法。根据在最大似然估计量处得到的观测信息，计算 $\\beta_g$ 每个分量的沃尔德标准误。将 $\\beta_g$ 解释为均值尺度上的对数效应量。\n\n将推导出的算法实现为一个完整的、可运行的程序。对于每个提供的测试用例基因，该程序输出 $\\beta_g$ 的第一个协变量分量（不包括截距）的估计效应量及其估计标准误。此问题中没有物理单位。所有答案都应以浮点数形式提供。\n\n使用以下测试套件，其中包含 $n = 6$ 个样本和一个表示处理组（$x_i = 1$）与对照组（$x_i = 0$）的单一协变量 $x_i$：\n\n- $i = 1, \\dots, 6$ 的协变量 $x_i$：$\\{0, 0, 0, 1, 1, 1\\}$。\n\n- 基因 $g = 1$：计数 $y_{1 i}$ 为 $\\{20, 24, 22, 40, 45, 43\\}$，离散度 $\\phi_1 = 0.1$。\n\n- 基因 $g = 2$：计数 $y_{2 i}$ 为 $\\{0, 1, 0, 3, 2, 1\\}$，离散度 $\\phi_2 = 1.0$。\n\n- 基因 $g = 3$：计数 $y_{3 i}$ 为 $\\{100, 110, 90, 95, 105, 85\\}$，离散度 $\\phi_3 = 0.5$。\n\n- 基因 $g = 4$：计数 $y_{4 i}$ 为 $\\{5, 6, 4, 5, 6, 4\\}$，离散度 $\\phi_4 = 10^{-6}$。\n\n对于每个基因，计算单元素 $\\beta_g$ 的最大似然估计量及其沃尔德标准误。您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表。对于每个基因（按 $g = 1, 2, 3, 4$ 的顺序），包括一个双元素列表 $[\\widehat{\\beta}_g, \\mathrm{SE}(\\widehat{\\beta}_g)]$，两个值都四舍五入到六位小数。例如，总输出格式必须为\n$$\n[\\,[\\widehat{\\beta}_1,\\mathrm{SE}(\\widehat{\\beta}_1)],\\,[\\widehat{\\beta}_2,\\mathrm{SE}(\\widehat{\\beta}_2)],\\,[\\widehat{\\beta}_3,\\mathrm{SE}(\\widehat{\\beta}_3)],\\,[\\widehat{\\beta}_4,\\mathrm{SE}(\\widehat{\\beta}_4)]\\,],\n$$\n打印为单行，不含任何其他文本。", "solution": "用户提供的问题是计算系统生物学中一个良构且有科学依据的任务，具体涉及广义线性模型（GLM）中计数数据参数的估计。该问题是有效的，因为它是自洽的、客观的，并且基于已建立的差异基因表达分析的统计学原理。因此，我们可以着手提供一个完整的解决方案。\n\n目标是推导并实现一个算法，以找到负二项（NB）广义线性模型中效应量参数 $\\beta_g$ 的最大似然估计量（MLE）。对于每个基因 $g$，样本 $i=1, \\dots, n$ 的观测计数 $y_{gi}$ 被建模为：\n$$\ny_{gi} \\sim \\text{NB}(\\mu_{gi}, \\phi_g)\n$$\n均值为 $\\mu_{gi}$，离散度 $\\phi_g$ 是每个基因的已知固定参数。方差由 $V(y_{gi}) = \\mu_{gi} + \\phi_g \\mu_{gi}^2$ 给出。均值 $\\mu_{gi}$ 通过对数连接函数与样本特异性协变量 $x_i$ 相关联：\n$$\n\\log(\\mu_{gi}) = \\eta_{gi} = \\alpha_g + x_i^\\top \\beta_g\n$$\n这里，$\\eta_{gi}$ 是线性预测器，$\\alpha_g$ 是截距参数，$\\beta_g$ 是效应量参数向量。令 $\\theta_g = (\\alpha_g, \\beta_g^\\top)^\\top$ 为待估计的完整参数向量。一个基因的设计矩阵 $X$ 的行为 $(1, x_i^\\top)$，使得 $\\eta_g = X \\theta_g$。为简化起见，在接下来的推导中我们将省略基因索引 $g$。\n\n**1. 对数似然函数**\n\n一个均值为 $\\mu$、离散度为 $\\phi$（其中尺寸参数 $r = 1/\\phi$）的负二项随机变量 $Y$ 的概率质量函数可以参数化为：\n$$\nP(Y=y | \\mu, \\phi) = \\frac{\\Gamma(y + 1/\\phi)}{\\Gamma(y+1)\\Gamma(1/\\phi)} \\left( \\frac{\\mu}{1/\\phi + \\mu} \\right)^y \\left( \\frac{1/\\phi}{1/\\phi + \\mu} \\right)^{1/\\phi}\n$$\n单个观测值 $y_i$ 的对数似然为：\n$$\n\\ell(\\theta | y_i) = \\log\\Gamma(y_i + 1/\\phi) - \\log\\Gamma(y_i+1) - \\log\\Gamma(1/\\phi) + y_i \\log(\\mu_i) + \\frac{1}{\\phi} \\log(1/\\phi) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\n忽略不依赖于参数 $\\theta$（也即不依赖于 $\\mu_i$）的项，对数似然的相关部分为：\n$$\n\\ell(\\theta | y_i) \\propto y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\n对于 $n$ 个独立观测值的总对数似然是所有样本的总和：\n$$\nL(\\theta) = \\sum_{i=1}^n \\ell(\\theta | y_i) = \\sum_{i=1}^n \\left[ y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi) \\right]\n$$\n\n**2. 得分方程与费雪信息**\n\n为了找到最大似然估计量，我们必须求解得分方程 $U(\\theta) = \\nabla_\\theta L(\\theta) = 0$。我们使用链式法则：$\\frac{\\partial L}{\\partial \\theta_j} = \\sum_i \\frac{\\partial \\ell_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\theta_j}$。\n\n各分量为：\n- $\\frac{\\partial \\eta_i}{\\partial \\theta_j} = X_{ij}$，其中 $X_{ij}$ 是设计矩阵中样本 $i$ 和参数 $j$ 对应的元素。\n- 对于对数连接 $\\eta_i = \\log(\\mu_i)$，我们有 $\\mu_i = e^{\\eta_i}$，所以 $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = e^{\\eta_i} = \\mu_i$。\n- $\\frac{\\partial \\ell_i}{\\partial \\mu_i} = \\frac{y_i}{\\mu_i} - \\frac{y_i + 1/\\phi}{\\mu_i + 1/\\phi} = \\frac{y_i(\\mu_i + 1/\\phi) - \\mu_i(y_i + 1/\\phi)}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i/\\phi - \\mu_i/\\phi}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)}$。注意方差为 $V(\\mu_i) = \\mu_i(1+\\phi\\mu_i)$。等等，用户指定了 $V(\\mu_i) = \\mu_i + \\phi_g \\mu_{gi}^2$。我们使用所给的方差函数 $V(\\mu_i) = \\mu_i + \\phi\\mu_i^2$。那么对数似然的导数应为 $\\frac{y_i-\\mu_i}{V(\\mu_i)} / (1/\\mu_i)$，这是不正确的。对于典范连接的广义线性模型，导数正确地为 $\\frac{y_i-\\mu_i}{V(\\mu_i)} g'(\\mu)$。对于一个通用的广义线性模型，我们重新评估 $\\frac{\\partial \\ell_i}{\\partial \\mu_i}$。与 $V(\\mu_i) = \\mu_i+\\phi\\mu_i^2$ 对应的对数似然形式确实是我们所用的形式。因此，该导数是正确的：$\\frac{\\partial \\ell_i}{\\partial \\mu_i} = \\frac{y_i - \\mu_i}{\\mu_i(1+\\phi\\mu_i)}$。我们再次验证方差公式：$\\mu = (1-p)r/p$ 且 $\\sigma^2 = \\mu + \\mu^2/r = \\mu + \\phi \\mu^2$。参数化是一致的。\n\n组合各部分得到得分贡献：\n$$\n\\frac{\\partial \\ell_i}{\\partial \\theta_j} = \\left( \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)} \\right) (\\mu_i) (X_{ij}) = \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}\n$$\n得分向量 $U(\\theta)$ 的分量为 $U_j(\\theta) = \\sum_{i=1}^n \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}$。用矩阵表示法，即为 $U(\\theta) = X^\\top W_s (y - \\mu)$，其中 $W_s$ 是一个对角矩阵，其元素为 $(W_s)_{ii} = (1+\\phi\\mu_i)^{-1}$。这些方程在 $\\theta$ 上是非线性的，需要迭代求解。\n\n**3. 费雪评分法与迭代重加权最小二乘法（IRLS）**\n\n求解广义线性模型得分方程的标准方法是费雪评分法，这是一种基于 Newton-Raphson 算法的迭代过程。更新规则是：\n$$\n\\theta^{(t+1)} = \\theta^{(t)} + I(\\theta^{(t)})^{-1} U(\\theta^{(t)})\n$$\n其中 $I(\\theta)$ 是费雪信息矩阵，由 $I(\\theta) = -E[H(\\theta)]$ 给出，而 $H(\\theta)$ 是 Hessian 矩阵（$\\nabla_\\theta^2 L(\\theta)$）。对于一个广义线性模型，费雪信息矩阵的第 $(j,k)$ 个元素是：\n$$\nI_{jk}(\\theta) = \\sum_{i=1}^n w_i X_{ij} X_{ik} \\quad \\text{其中} \\quad w_i = \\frac{1}{V(\\mu_i)}\\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\n$$\n对于我们的带有对数连接的负二项-广义线性模型：\n- $V(\\mu_i) = \\mu_i + \\phi \\mu_i^2 = \\mu_i(1 + \\phi\\mu_i)$\n- $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\mu_i$\n因此，权重为：\n$$\nw_i = \\frac{1}{\\mu_i(1 + \\phi\\mu_i)} (\\mu_i)^2 = \\frac{\\mu_i}{1 + \\phi\\mu_i}\n$$\n费雪信息矩阵是 $I(\\theta) = X^\\top W X$，其中 $W$ 是由权重 $w_i$ 构成的对角矩阵。\n\n费雪评分法的更新可以重排为一个迭代重加权最小二乘（IRLS）问题。定义一个“工作响应”向量 $z$：\n$$\nz_i = \\eta_i + (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\mu_i} = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}\n$$\n然后通过解决加权最小二乘问题来获得 $\\theta$ 的更新：\n$$\n\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}\n$$\n其中右侧的量均在当前估计值 $\\theta^{(t)}$ 处计算。\n\nIRLS 算法流程如下：\n1.  初始化参数向量 $\\theta^{(0)}$。一个合理的起点是 $\\beta^{(0)}=0$ 和 $\\alpha^{(0)}=\\log(\\bar{y})$。\n2.  对 $t=0, 1, 2, \\dots$ 进行迭代，直到收敛：\n    a. 计算线性预测器：$\\eta^{(t)} = X \\theta^{(t)}$。\n    b. 计算均值向量：$\\mu^{(t)} = \\exp(\\eta^{(t)})$。\n    c. 计算对角权重矩阵 $W^{(t)}$，其权重为 $w_i^{(t)} = \\frac{\\mu_i^{(t)}}{1 + \\phi\\mu_i^{(t)}}$。\n    d. 计算工作响应向量 $z^{(t)}$，其分量为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$。\n    e. 通过求解线性系统更新参数：$\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}$。\n3.  当 $\\theta$ 的变化（例如 $||\\theta^{(t+1)} - \\theta^{(t)}||^2$）小于一个小的容差时，迭代停止。\n\n**4. 标准误**\n\n收敛到最大似然估计量 $\\hat{\\theta}$ 后，估计量的渐近协方差矩阵由在最大似然估计量处计算的费雪信息矩阵的逆给出：\n$$\n\\text{Cov}(\\hat{\\theta}) = I(\\hat{\\theta})^{-1} = (X^\\top \\hat{W} X)^{-1}\n$$\n其中 $\\hat{W}$ 是使用最终估计均值 $\\hat{\\mu}$ 计算的权重矩阵。该协方差矩阵的对角元素是参数估计量的估计方差。第 $j$ 个参数估计值 $\\hat{\\theta}_j$ 的沃尔德标准误是第 $j$ 个对角元素的平方根：\n$$\n\\text{SE}(\\hat{\\theta}_j) = \\sqrt{(\\text{Cov}(\\hat{\\theta}))_{jj}}\n$$\n对于此问题，$\\theta = (\\alpha, \\beta)^\\top$，所以我们感兴趣的是 $\\hat{\\beta}$ 和 $\\text{SE}(\\hat{\\beta})$，它们对应于 $\\hat{\\theta}$ 的第二个元素和协方差矩阵的 (2,2) 元素的平方根。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_nb_glm(y_counts, covariates, phi, max_iter=100, tol=1e-8):\n    \"\"\"\n    Fits a Negative Binomial Generalized Linear Model using IRLS.\n\n    Args:\n        y_counts (np.ndarray): Vector of observed counts.\n        covariates (np.ndarray): Vector of covariates for the non-intercept term.\n        phi (float): Known dispersion parameter.\n        max_iter (int): Maximum number of iterations for IRLS.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        tuple: A tuple containing:\n            - beta_hat (float): The estimated coefficient for the covariate.\n            - se_beta (float): The standard error of the estimated coefficient.\n    \"\"\"\n    # Design matrix X should include an intercept column\n    n_samples = len(y_counts)\n    X = np.ones((n_samples, 2))\n    X[:, 1] = covariates\n\n    # Initial parameter estimates\n    # A safe starting point for the intercept if all y_counts are 0\n    mean_y = np.mean(y_counts)\n    if mean_y == 0:\n        alpha = -10.0  # Heuristic for log(small_number)\n    else:\n        alpha = np.log(mean_y)\n    beta = 0.0\n    theta = np.array([alpha, beta])\n\n    for i in range(max_iter):\n        # 1. Linear predictor and mean\n        eta = X @ theta\n        mu = np.exp(eta)\n\n        # 2. Weights for Fisher Information matrix\n        # w_i = mu_i / (1 + phi * mu_i)\n        weights = mu / (1.0 + phi * mu)\n        W = np.diag(weights)\n\n        # 3. Working response\n        # z_i = eta_i + (y_i - mu_i) / mu_i\n        # Handle mu_i close to zero to avoid division issues.\n        # If mu is very small, the weight is also very small, so the contribution\n        # to the weighted least squares is minimal. We can add a small epsilon.\n        mu_safe = np.maximum(mu, 1e-10)\n        z = eta + (y_counts - mu) / mu_safe\n\n        # 4. Update parameters via weighted least squares\n        # theta_new = (X^T W X)^-1 X^T W z\n        # Solving the linear system is more stable than inverting.\n        XT_W = X.T @ W\n        XT_W_X = XT_W @ X\n        XT_W_z = XT_W @ z\n        \n        try:\n            # Check if matrix is invertible\n            if np.linalg.det(XT_W_X) == 0:\n                # This can happen with perfect separation or other data issues\n                # Return NaN to indicate failure to converge\n                return np.nan, np.nan\n            theta_new = np.linalg.solve(XT_W_X, XT_W_z)\n        except np.linalg.LinAlgError:\n            # Fails to converge, likely due to singular matrix\n            return np.nan, np.nan\n\n        # 5. Check for convergence\n        diff = np.sum((theta_new - theta) ** 2)\n        theta = theta_new\n        if diff  tol:\n            break\n\n    # After convergence, calculate standard errors\n    # Cov(theta_hat) = (X^T W X)^-1\n    final_eta = X @ theta\n    final_mu = np.exp(final_eta)\n    final_weights = final_mu / (1.0 + phi * final_mu)\n    final_W = np.diag(final_weights)\n    \n    try:\n        cov_matrix = np.linalg.inv(X.T @ final_W @ X)\n        se_beta = np.sqrt(cov_matrix[1, 1])\n    except np.linalg.LinAlgError:\n        se_beta = np.nan\n\n    beta_hat = theta[1]\n    \n    return beta_hat, se_beta\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Structure: (counts, dispersion)\n    test_cases = [\n        (np.array([20, 24, 22, 40, 45, 43]), 0.1),\n        (np.array([0, 1, 0, 3, 2, 1]), 1.0),\n        (np.array([100, 110, 90, 95, 105, 85]), 0.5),\n        (np.array([5, 6, 4, 5, 6, 4]), 1e-6),\n    ]\n\n    # Covariate vector is the same for all cases.\n    covariates = np.array([0, 0, 0, 1, 1, 1])\n\n    results = []\n    for y_counts, phi in test_cases:\n        beta_hat, se_beta = fit_nb_glm(y_counts, covariates, phi)\n        results.append([beta_hat, se_beta])\n\n    # Format the output string as required.\n    # [[beta1,SE1],[beta2,SE2],...]\n    results_str_list = []\n    for beta, se in results:\n        # Round to six decimal places for output\n        beta_str = f\"{beta:.6f}\"\n        se_str = f\"{se:.6f}\"\n        results_str_list.append(f\"[{beta_str},{se_str}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3301611"}, {"introduction": "在多实验室合作或数据整合分析中，保证结果的可复现性是一个巨大挑战，因为即使经过了标准归一化，系统性偏差依然存在。外部掺入的Spike-in标准品（其浓度和预期倍数变化已知）为我们提供了一个校准“标尺”。在本练习中，您将利用Spike-in数据来估计并校正实验室特有的测量偏差，从而校准基因的对数倍数变化（$LFC$），并定量评估该校准过程对提升跨实验室结果一致性和基因排序稳定性的作用 ([@problem_id:3301688])。", "problem": "给定您在两个独立实验室中对信使RNA（mRNA）基因和外部掺入标准品的重复计数测量值。在每个实验室中，都对一个对照条件和一个处理条件进行了测定。假设以下基础模型和定义适用于差异基因表达分析和效应量评估：\n\n- 基因的对数倍数变化（LFC）定义为 $LFC = \\log_{2}\\left(\\dfrac{\\bar{T} + \\epsilon}{\\bar{C} + \\epsilon}\\right)$，其中 $\\bar{T}$ 是处理组重复的算术平均值，$\\bar{C}$ 是对照组重复的算术平均值，而 $\\epsilon$ 是一个小的正常数伪计数，用于在出现零计数时确保数值稳定性。\n- 掺入标准品是在处理组和对照组之间以已知比例添加的合成分子。它们的真实对数倍数变化（表示为 $LFC^{true}$）是先验已知的。\n- 对于掺入标准品，一个实验室特定的乘性尺度偏差被建模为 $LFC^{obs} \\approx \\kappa \\cdot LFC^{true}$，其中 $LFC^{obs}$ 是观测到的掺入标准品对数倍数变化，而 $\\kappa$ 是一个未知的、实验室特定的缩放因子。\n- 校准规则为 $LFC^{cal} = \\dfrac{LFC^{obs}}{\\hat{\\kappa}}$，其中 $\\hat{\\kappa}$ 是从掺入标准品中推导出的 $\\kappa$ 的估计值。\n\n以此为基础，为每个测试用例实现以下步骤：\n\n1. 对每个实验室：\n   a. 使用给定的重复计数和指定的 $\\epsilon$ 计算每个掺入标准品的 $LFC^{obs}$。\n   b. 在一个通过原点的回归模型下，通过最小化 $\\sum_{i}\\left(LFC^{obs}_{i} - \\hat{\\kappa} \\cdot LFC^{true}_{i}\\right)^{2}$ 来使用最小二乘法估计 $\\hat{\\kappa}$。这得出\n   $$\\hat{\\kappa} = \\dfrac{\\sum_{i} LFC^{true}_{i} \\cdot LFC^{obs}_{i}}{\\sum_{i} \\left(LFC^{true}_{i}\\right)^{2}}.$$\n   c. 计算每个基因的 $LFC^{obs}$。\n   d. 通过 $LFC^{cal} = \\dfrac{LFC^{obs}}{\\hat{\\kappa}}$ 校准基因的对数倍数变化。\n\n2. 在每个实验室内，按 $LFC^{obs}$ 和 $LFC^{cal}$ 的绝对值大小对基因进行排序。将排序函数定义为按绝对值降序排序，若出现平局则按基因名称的字典升序排序。确定校准后实验室内基因排名是否发生任何变化。变化指示器应为一个布尔值。\n\n3. 使用带符号的基因对数倍数变化的皮尔逊相关系数来量化校准前后的跨实验室可重复性。设 $x_{j}$ 为实验室 A 中基因 $LFC^{obs}$ 的向量，而 $y_{j}$ 为实验室 B 中相同基因和相同顺序的对应向量。计算\n   $$r_{\\text{pre}} = \\dfrac{\\operatorname{Cov}(x, y)}{\\sigma_{x}\\sigma_{y}}$$\n   和\n   $$r_{\\text{post}} = \\dfrac{\\operatorname{Cov}\\left(x^{cal}, y^{cal}\\right)}{\\sigma_{x^{cal}}\\sigma_{y^{cal}}},$$\n   其中 $\\operatorname{Cov}$ 表示协方差，$\\sigma$ 表示标准差。将改善报告为 $\\Delta r = r_{\\text{post}} - r_{\\text{pre}}$。\n\n4. 您的程序应为下面提供的测试套件实现上述计算。伪计数必须为 $\\epsilon = 0.5$。无需报告物理单位，也不涉及角度。所有小数输出必须为十进制形式。\n\n测试套件（三个案例）：\n\n- 案例1（理想路径：实验室间存在显著偏差差异，无零计数）：\n  - 实验室：Lab1 和 Lab2\n  - 基因和重复：\n    - 对照组重复（实验室共享）：\n      - $G1$: $[190, 210]$, $G2$: $[175, 185]$, $G3$: $[145, 155]$, $G4$: $[215, 225]$, $G5$: $[95, 105]$, $G6$: $[295, 305]$, $G7$: $[75, 85]$, $G8$: $[390, 410]$\n    - Lab1 处理组重复：\n      - $G1$: $[850, 865]$, $G2$: $[470, 480]$, $G3$: $[90, 95]$, $G4$: $[215, 225]$, $G5$: $[30, 32]$, $G6$: $[580, 595]$, $G7$: $[11, 12]$, $G8$: $[530, 540]$\n    - Lab2 处理组重复：\n      - $G1$: $[455, 465]$, $G2$: $[310, 315]$, $G3$: $[112, 115]$, $G4$: $[215, 225]$, $G5$: $[50, 53]$, $G6$: $[440, 445]$, $G7$: $[26, 27]$, $G8$: $[470, 475]$\n  - 掺入标准品：\n    - 对照组重复（实验室共享）：$S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - Lab1 处理组重复：$S1$: $[38, 38]$, $S2$: $[98, 102]$, $S3$: $[260, 268]$\n    - Lab2 处理组重复：$S1$: $[57, 58]$, $S2$: $[98, 102]$, $S3$: $[173, 175]$\n    - 真实的掺入标准品对数倍数变化：$LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n- 案例2（边界条件：轻微偏差差异，效应适中）：\n  - 实验室：LabA 和 LabB\n  - 基因和重复：\n    - 对照组重复（实验室共享）：\n      - $G1$: $[145, 155]$, $G2$: $[190, 210]$, $G3$: $[175, 185]$, $G4$: $[95, 105]$, $G5$: $[215, 225]$\n    - LabA 处理组重复：\n      - $G1$: $[214, 218]$, $G2$: $[137, 140]$, $G3$: $[178, 182]$, $G4$: $[205, 208]$, $G5$: $[105, 108]$\n    - LabB 处理组重复：\n      - $G1$: $[207, 210]$, $G2$: $[142, 145]$, $G3$: $[177, 183]$, $G4$: $[192, 195]$, $G5$: $[113, 115]$\n  - 掺入标准品：\n    - 对照组重复（实验室共享）：$S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - LabA 处理组重复：$S1$: $[48, 49]$, $S2$: $[98, 102]$, $S3$: $[206, 209]$\n    - LabB 处理组重复：$S1$: $[51, 53]$, $S2$: $[98, 102]$, $S3$: $[192, 194]$\n    - 真实的掺入标准品对数倍数变化：$LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n- 案例3（边缘情况：一个重复中存在零计数，偏差差异较大）：\n  - 实验室：LabA 和 LabB\n  - 基因和重复：\n    - LabA 对照组重复：\n      - $G1$: $[115, 125]$, $G2$: $[58, 62]$, $G3$: $[195, 205]$, $G4$: $[78, 82]$, $G5$: $[148, 152]$\n    - LabA 处理组重复：\n      - $G1$: $[292, 298]$, $G2$: $[8, 10]$, $G3$: $[260, 265]$, $G4$: $[42, 44]$, $G5$: $[148, 152]$\n    - LabB 对照组重复：\n      - $G1$: $[115, 125]$, $G2$: $[0, 60]$, $G3$: $[195, 205]$, $G4$: $[78, 82]$, $G5$: $[148, 152]$\n    - LabB 处理组重复：\n      - $G1$: $[193, 197]$, $G2$: $[11, 12]$, $G3$: $[230, 234]$, $G4$: $[56, 58]$, $G5$: $[148, 152]$\n  - 掺入标准品：\n    - 对照组重复（实验室共享）：$S1$: $[98, 102]$, $S2$: $[98, 102]$, $S3$: $[98, 102]$\n    - LabA 处理组重复：$S1$: $[40, 41]$, $S2$: $[98, 102]$, $S3$: $[244, 248]$\n    - LabB 处理组重复：$S1$: $[61, 62]$, $S2$: $[98, 102]$, $S3$: $[161, 163]$\n    - 真实的掺入标准品对数倍数变化：$LFC^{true}(S1) = -1$, $LFC^{true}(S2) = 0$, $LFC^{true}(S3) = +1$\n\n您的程序必须输出一行，按顺序包含这三个案例的结果列表。对于每个案例，返回一个包含两个元素的列表 $[\\Delta r, \\text{changed}]$，其中 $\\Delta r$ 是一个浮点数（皮尔逊相关性的改善值，$r_{\\text{post}} - r_{\\text{pre}}$），而 $\\text{changed}$ 是一个布尔值，指示在该案例中，是否有任何一个实验室的内部基因绝对 LFC 排名因校准而改变。最终输出格式必须是形如 '[[delta_r_case1,changed_case1],[delta_r_case2,changed_case2],[delta_r_case3,changed_case3]]' 的单行字符串。", "solution": "问题陈述已经过仔细审查，并被确定为有效。它在科学上基于计算系统生物学的原理，特别是关于差异基因表达数据的归一化。该问题设定良好，提供了一套完整且一致的定义、数据和计算步骤。所有术语都有正式定义，且目标是可量化的。\n\n任务是实现一个计算工作流程，用于使用外部掺入标准品分析和校正 mRNA 计数数据中的实验室间偏差。这涉及几个步骤：计算对数倍数变化（LFCs），估计实验室特定的偏差缩放因子（$\\kappa$），校准观测到的基因LFC，以及评估此校准对基因排名和跨实验室可重复性的影响。\n\n基础常数和公式如下：\n- 用于处理零计数的伪计数为 $\\epsilon = 0.5$。\n- 观测到的对数倍数变化（$LFC^{obs}$）对每个基因或掺入标准品的定义如下：\n$$LFC = \\log_{2}\\left(\\dfrac{\\bar{T} + \\epsilon}{\\bar{C} + \\epsilon}\\right)$$\n其中 $\\bar{T}$ 和 $\\bar{C}$ 分别是处理组和对照组重复计数的算术平均值。\n\n分析的核心按规定分三个阶段进行。\n\n**1. 实验室内数据处理与校准**\n\n对每个测试用例中提供的每个实验室数据集，应用以下程序：\n\na. **掺入标准品 LFC 计算**：使用每个掺入标准品的对照组和处理组重复计数以及上述公式计算其 $LFC^{obs}$。\n\nb. **偏差因子估计（$\\hat{\\kappa}$）**：问题假设观测到的掺入标准品 LFC（$LFC^{obs}_i$）与其已知的真实 LFC（$LFC^{true}_i$）之间存在线性关系，建模为 $LFC^{obs}_i \\approx \\kappa \\cdot LFC^{true}_i$。实验室特定的缩放因子 $\\kappa$ 通过通过原点的回归最小二乘法进行估计。目标是找到最小化残差平方和 $\\sum_{i}\\left(LFC^{obs}_{i} - \\hat{\\kappa} \\cdot LFC^{true}_{i}\\right)^{2}$ 的 $\\hat{\\kappa}$。这个最小化问题的解由下式给出：\n$$\\hat{\\kappa} = \\dfrac{\\sum_{i} LFC^{true}_{i} \\cdot LFC^{obs}_{i}}{\\sum_{i} \\left(LFC^{true}_{i}\\right)^{2}}$$\n这个估计量 $\\hat{\\kappa}$ 代表了实验室测量过程在对数倍数变化尺度上的系统性乘性偏差。由于提供的 $LFC^{true}$ 值为 $[-1, 0, 1]$，其平方和为 $2$，因此分母不为零。\n\nc. **基因 LFC 计算**：使用每个内源基因各自的重复计数计算其 $LFC^{obs}$。\n\nd. **基因 LFC 校准**：对观测到的基因 LFC 进行校准，以纠正估计的偏差。校准后的 LFC（$LFC^{cal}$）通过将观测到的 LFC 除以估计的缩放因子来计算：\n$$LFC^{cal} = \\dfrac{LFC^{obs}}{\\hat{\\kappa}}$$\n此过程将观测到的效应重新缩放，使其达到在无偏差测量系统（其中 $\\kappa=1$）中应有的水平。\n\n**2. 实验室内排名稳定性分析**\n\n为了评估校准对单个实验室内基因表达变化解释的影响，比较了校准前后基因的相对顺序。基因根据其 LFC 值的绝对值大小进行排名，因为这个指标常用于确定优先进行深入研究的基因。具体的排名规则是：\n- 主排序键：按绝对 LFC 值（$|LFC|$）降序排列。\n- 次排序键（平局决胜）：按基因名称的字典升序排列。\n\n为每个实验室生成两个基因名称的排名列表：一个使用 $|LFC^{obs}|$，另一个使用 $|LFC^{cal}|$。对这两个列表进行比较。如果它们不完全相同，则表明校准过程改变了至少一个基因的相对重要性，并将该实验室的变化布尔标志设置为 true。对于每个测试用例，最终的变化指示器是来自两个实验室的标志的逻辑或。\n\n**3. 跨实验室可重复性评估**\n\n归一化的一个关键目标是提高独立实验之间结果的一致性。这通过测量校准前后基因 LFC 的跨实验室一致性来量化。皮尔逊相关系数用于此目的。\n\n设 $x$ 和 $y$ 分别为来自实验室 A 和实验室 B 的所有基因的 $LFC^{obs}$ 向量，并保持一致的基因顺序。校准前的可重复性 $r_{\\text{pre}}$ 为：\n$$r_{\\text{pre}} = \\dfrac{\\operatorname{Cov}(x, y)}{\\sigma_{x}\\sigma_{y}}$$\n其中 $\\operatorname{Cov}(x, y)$ 是 $x$ 和 $y$ 的协方差，$\\sigma_{x}$ 和 $\\sigma_{y}$ 是它们的标准差。\n\n同样，设 $x^{cal}$ 和 $y^{cal}$ 为 $LFC^{cal}$ 值的向量。校准后的可重复性 $r_{\\text{post}}$ 为：\n$$r_{\\text{post}} = \\dfrac{\\operatorname{Cov}\\left(x^{cal}, y^{cal}\\right)}{\\sigma_{x^{cal}}\\sigma_{y^{cal}}}$$\n\n可重复性的改善报告为差值 $\\Delta r = r_{\\text{post}} - r_{\\text{pre}}$。一个正的 $\\Delta r$ 值表示掺入标准品校准使得两个实验室间的基因 LFC 测量结果更加一致。\n\n所描述的方法将被系统地应用于三个测试用例中的每一个，以生成所需的输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis for all test cases.\n    \"\"\"\n    epsilon = 0.5\n\n    # Define the test cases from the problem statement.\n    gene_control_c1 = np.array([[190, 210], [175, 185], [145, 155], [215, 225], [95, 105], [295, 305], [75, 85], [390, 410]], dtype=float)\n    spike_control_c1 = np.array([[98, 102], [98, 102], [98, 102]], dtype=float)\n\n    gene_control_c2 = np.array([[145, 155], [190, 210], [175, 185], [95, 105], [215, 225]], dtype=float)\n    spike_control_c2 = np.array([[98, 102], [98, 102], [98, 102]], dtype=float)\n\n    test_cases = [\n        { # Case 1\n            \"gene_names\": [f\"G{i}\" for i in range(1, 9)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"Lab1\": {\n                    \"gene_c\": gene_control_c1,\n                    \"gene_t\": np.array([[850, 865], [470, 480], [90, 95], [215, 225], [30, 32], [580, 595], [11, 12], [530, 540]], dtype=float),\n                    \"spike_c\": spike_control_c1,\n                    \"spike_t\": np.array([[38, 38], [98, 102], [260, 268]], dtype=float)\n                },\n                \"Lab2\": {\n                    \"gene_c\": gene_control_c1,\n                    \"gene_t\": np.array([[455, 465], [310, 315], [112, 115], [215, 225], [50, 53], [440, 445], [26, 27], [470, 475]], dtype=float),\n                    \"spike_c\": spike_control_c1,\n                    \"spike_t\": np.array([[57, 58], [98, 102], [173, 175]], dtype=float)\n                }\n            }\n        },\n        { # Case 2\n            \"gene_names\": [f\"G{i}\" for i in range(1, 6)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"LabA\": {\n                    \"gene_c\": gene_control_c2,\n                    \"gene_t\": np.array([[214, 218], [137, 140], [178, 182], [205, 208], [105, 108]], dtype=float),\n                    \"spike_c\": spike_control_c2,\n                    \"spike_t\": np.array([[48, 49], [98, 102], [206, 209]], dtype=float)\n                },\n                \"LabB\": {\n                    \"gene_c\": gene_control_c2,\n                    \"gene_t\": np.array([[207, 210], [142, 145], [177, 183], [192, 195], [113, 115]], dtype=float),\n                    \"spike_c\": spike_control_c2,\n                    \"spike_t\": np.array([[51, 53], [98, 102], [192, 194]], dtype=float)\n                }\n            }\n        },\n        { # Case 3\n            \"gene_names\": [f\"G{i}\" for i in range(1, 6)],\n            \"spike_lfc_true\": np.array([-1.0, 0.0, 1.0]),\n            \"labs\": {\n                \"LabA\": {\n                    \"gene_c\": np.array([[115, 125], [58, 62], [195, 205], [78, 82], [148, 152]], dtype=float),\n                    \"gene_t\": np.array([[292, 298], [8, 10], [260, 265], [42, 44], [148, 152]], dtype=float),\n                    \"spike_c\": np.array([[98, 102], [98, 102], [98, 102]], dtype=float),\n                    \"spike_t\": np.array([[40, 41], [98, 102], [244, 248]], dtype=float)\n                },\n                \"LabB\": {\n                    \"gene_c\": np.array([[115, 125], [0, 60], [195, 205], [78, 82], [148, 152]], dtype=float),\n                    \"gene_t\": np.array([[193, 197], [11, 12], [230, 234], [56, 58], [148, 152]], dtype=float),\n                    \"spike_c\": np.array([[98, 102], [98, 102], [98, 102]], dtype=float),\n                    \"spike_t\": np.array([[61, 62], [98, 102], [161, 163]], dtype=float)\n                }\n            }\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        case_result = _process_case(case, epsilon)\n        results.append(case_result)\n\n    # The required output format is a \"stringified\" list of lists with no spaces.\n    print(str(results).replace(' ', ''))\n\ndef _calculate_lfc(treatment_reps, control_reps, epsilon):\n    \"\"\"Computes log2 fold change with a pseudocount.\"\"\"\n    mean_t = np.mean(treatment_reps, axis=1)\n    mean_c = np.mean(control_reps, axis=1)\n    # Add epsilon to numerator and denominator to prevent log(0) and division by zero.\n    return np.log2((mean_t + epsilon) / (mean_c + epsilon))\n\ndef _process_lab(gene_names, lab_data, spike_lfc_true, epsilon):\n    \"\"\"Processes data for a single lab to get LFCs and ranking change.\"\"\"\n    # Step 1a: Compute observed LFC for spike-ins\n    spike_lfc_obs = _calculate_lfc(lab_data[\"spike_t\"], lab_data[\"spike_c\"], epsilon)\n    \n    # Step 1b: Estimate kappa\n    kappa_num = np.sum(spike_lfc_true * spike_lfc_obs)\n    kappa_den = np.sum(spike_lfc_true**2)\n    kappa_hat = kappa_num / kappa_den\n    \n    # Step 1c: Compute observed LFC for genes\n    gene_lfc_obs = _calculate_lfc(lab_data[\"gene_t\"], lab_data[\"gene_c\"], epsilon)\n    \n    # Step 1d: Calibrate gene LFCs\n    gene_lfc_cal = gene_lfc_obs / kappa_hat\n    \n    # Step 2: Determine if gene ranking changes\n    # Rank by descending absolute value, tie-break by ascending gene name\n    lfc_obs_with_names = list(zip(gene_names, gene_lfc_obs))\n    lfc_cal_with_names = list(zip(gene_names, gene_lfc_cal))\n    \n    # Create the ranking key: a tuple of (-abs(LFC), gene_name)\n    rank_obs = sorted(lfc_obs_with_names, key=lambda x: (-abs(x[1]), x[0]))\n    rank_cal = sorted(lfc_cal_with_names, key=lambda x: (-abs(x[1]), x[0]))\n    \n    # Extract just the gene names to check for ranking changes\n    ranked_names_obs = [item[0] for item in rank_obs]\n    ranked_names_cal = [item[0] for item in rank_cal]\n    \n    ranking_changed = (ranked_names_obs != ranked_names_cal)\n    \n    return gene_lfc_obs, gene_lfc_cal, ranking_changed\n\ndef _process_case(case_data, epsilon):\n    \"\"\"Processes a full test case with two labs.\"\"\"\n    lab_names = list(case_data[\"labs\"].keys())\n    lab_a_name, lab_b_name = lab_names[0], lab_names[1]\n\n    gene_names = case_data[\"gene_names\"]\n    spike_lfc_true = case_data[\"spike_lfc_true\"]\n\n    data_a = case_data[\"labs\"][lab_a_name]\n    lfc_obs_a, lfc_cal_a, changed_a = _process_lab(gene_names, data_a, spike_lfc_true, epsilon)\n    \n    data_b = case_data[\"labs\"][lab_b_name]\n    lfc_obs_b, lfc_cal_b, changed_b = _process_lab(gene_names, data_b, spike_lfc_true, epsilon)\n    \n    # Step 2 (final): Aggregate ranking change flag\n    case_changed = changed_a or changed_b\n    \n    # Step 3: Quantify cross-lab reproducibility improvement\n    # Using np.corrcoef which returns a 2x2 matrix. The value at [0,1] is the Pearson r.\n    r_pre = np.corrcoef(lfc_obs_a, lfc_obs_b)[0, 1]\n    r_post = np.corrcoef(lfc_cal_a, lfc_cal_b)[0, 1]\n    \n    delta_r = r_post - r_pre\n    \n    return [delta_r, case_changed]\n\nsolve()\n\n```", "id": "3301688"}]}