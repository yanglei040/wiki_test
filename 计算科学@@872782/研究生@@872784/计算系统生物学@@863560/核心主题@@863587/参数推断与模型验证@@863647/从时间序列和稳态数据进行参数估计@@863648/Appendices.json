{"hands_on_practices": [{"introduction": "在我们开始拟合模型之前，至关重要的是要理解我们使用的目标函数真正代表了什么。这个练习探讨了在最小二乘目标上增加惩罚项与采用带有先验分布的贝叶斯视角之间的深刻联系。理解这种对偶性是正确解释你的结果和正则化参数作用的关键。[@problem_id:3336675]", "problem": "在一个由常微分方程（ODE）模型控制的生化反应网络的参数估计问题中，设状态轨迹为 $x(t; p)$，其中 $p \\in \\mathbb{R}^d$ 是一个严格为正的生化反应速率常数向量。假设在时间点 $\\{t_i\\}$ 收集了时间序列观测值，其测量模型为 $y_i = h\\big(x(t_i; p), p\\big) + \\varepsilon_i$，其中 $\\varepsilon_i$ 是独立同分布（i.i.d.）的高斯随机变量，均值为 $0$，协方差矩阵 $\\Sigma_i \\succ 0$ 已知。此外，在输入条件 $u^\\ast$ 下，获得了稳态数据，此时系统假定满足 $f\\big(x^\\ast(p), u^\\ast, p\\big) = 0$；测得的稳态输出 $s$ 满足 $s = h\\big(x^\\ast(p), p\\big) + \\eta$，其中 $\\eta$ 是高斯随机变量，均值为 $0$，协方差 $\\Omega \\succ 0$ 已知。通过拼接时间序列残差和围绕 $x^\\ast(p)$ 进行适当线性化后的稳态残差，定义堆叠残差向量 $r(p)$，并设 $C \\succ 0$ 是由 $\\{\\Sigma_i\\}$ 和 $\\Omega$ 导出的 $r(p)$ 的块对角协方差矩阵。考虑以下惩罚加权最小二乘目标函数\n$$\nJ(p) \\;=\\; \\frac{1}{2}\\, r(p)^\\top C^{-1} r(p) \\;+\\; \\frac{1}{2}\\, \\lambda \\,\\|p - \\mu\\|_2^2,\n$$\n其中 $\\mu \\in \\mathbb{R}^d$ 是一个固定的参考向量，$\\lambda > 0$ 是一个调整参数。假设模型误设可以忽略不计，因此高斯噪声模型是适当的。贝叶斯推断的最大后验（MAP）范式使用贝叶斯法则将似然与先验相结合，并选择后验密度的最大化者。\n\n在这种情况下，关于添加 $L_2$ 惩罚项 $\\lambda \\|p\\|_2^2$ 和对 $p$ 设置高斯先验在认识论上的差异，以下哪些陈述是正确的？选择所有适用项。\n\nA. 如果残差向量 $r(p)$ 是高斯分布且协方差 $C$ 已知，则最小化 $J(p)$ 与高斯先验 $p \\sim \\mathcal{N}(\\mu, \\lambda^{-1} I)$ 下的 MAP 估计量完全等价，因为负对数后验函数在相差一个加性常数的情况下等于 $J(p)$。\n\nB. 即使观测噪声是重尾的（例如，拉普拉斯分布），选项 A 中的等价性仍然存在，因为对 $p$ 的二次惩罚项对应于一个高斯先验，这与似然模型无关。\n\nC. 如果不采用贝叶斯解释，由最小化 $J(p)$ 定义的惩罚估计量本身并不定义一个后验分布；因此，后验可信区间和边缘似然是未定义的。相反，对 $p$ 设置高斯先验会产生一个后验分布，其在 MAP 估计附近的局部协方差可以由负对数后验函数的 Hessian 矩阵的逆来近似，该近似值依赖于作为先验精度的 $\\lambda$。\n\nD. 通过交叉验证选择 $\\lambda$ 以最小化样本外预测误差，通常等价于通过最大化边缘似然（经验贝叶斯）来选择先验方差，因此在有限样本中，这两种程序会产生相同的 $\\lambda$。\n\nE. 如果通过 $q = \\log p$（逐分量）进行重参数化，并惩罚 $\\|q - \\nu\\|_2^2$，则这等价于对 $q$ 施加高斯先验，从而对 $p$ 施加对数正态先验，因此正则化对速率常数的作用是乘性的而不是加性的。", "solution": "用户希望我验证并解决一个关于计算系统生物学中参数估计的问题。\n\n### 步骤 1：提取已知条件\n-   **模型：** 一个常微分方程（ODE）模型，状态轨迹为 $x(t; p)$，其中 $p \\in \\mathbb{R}^d$ 是一个严格为正的生化反应速率常数向量。\n-   **时间序列数据：** 在时间点 $\\{t_i\\}$ 的观测值 $y_i$，测量模型为 $y_i = h\\big(x(t_i; p), p\\big) + \\varepsilon_i$。\n-   **时间序列噪声：** $\\varepsilon_i$ 是独立同分布（i.i.d.）的高斯随机变量，均值为 $0$，协方差矩阵 $\\Sigma_i \\succ 0$ 已知且为正定。\n-   **稳态数据：** 在输入 $u^\\ast$ 下，系统有一个稳态 $x^\\ast(p)$ 满足 $f\\big(x^\\ast(p), u^\\ast, p\\big) = 0$。\n-   **稳态测量：** 测得的输出 $s$ 满足 $s = h\\big(x^\\ast(p), p\\big) + \\eta$。\n-   **稳态噪声：** $\\eta$ 是一个高斯随机变量，均值为 $0$，协方差矩阵 $\\Omega \\succ 0$ 已知且为正定。\n-   **残差和协方差：** 一个堆叠残差向量 $r(p)$ 由时间序列残差和线性化的稳态残差构成。$r(p)$ 的总协方差矩阵记为 $C$，是块对角的、正定的，并由 $\\{\\Sigma_i\\}$ 和 $\\Omega$ 导出。\n-   **目标函数：** 定义了一个惩罚加权最小二乘目标函数 $J(p) = \\frac{1}{2}\\, r(p)^\\top C^{-1} r(p) + \\frac{1}{2}\\, \\lambda \\,\\|p - \\mu\\|_2^2$，其中 $\\lambda > 0$ 且 $\\mu \\in \\mathbb{R}^d$。\n-   **假设：** 模型误设可以忽略不计，意味着高斯噪声模型是合适的。\n-   **背景：** 该问题是在贝叶斯推断的最大后验（MAP）范式下提出的。\n-   **问题：** 识别关于添加 $L_2$ 惩罚和对 $p$ 设置高斯先验在认识论上差异的正确陈述。\n\n### 步骤 2：使用已知条件进行验证\n该问题陈述在科学上是合理的、提法明确且客观的。它描述了动态系统参数估计中的一个标准场景，结合了时间序列和稳态数据。目标函数 $J(p)$ 是吉洪诺夫正则化（Tikhonov regularization）的经典形式，或者从贝叶斯角度看，是 MAP 估计的形式。问题探讨了这两种统计范式之间的根本关系。\n\n存在一个微小的细节：问题陈述参数 $p$ 是“严格为正的”，而惩罚项 $\\frac{1}{2} \\lambda \\|p - \\mu\\|_2^2$ 对应的高斯先验的支撑集是整个 $\\mathbb{R}^d$，而不仅仅是正象限。这是建模中一个常见的实际问题，通常通过选择一个具有较大正分量的参考向量 $\\mu$ 并确保解保持为正来解决，或者通过对模型进行重参数化（选项 E 中讨论的主题）。这并未使问题陈述无效；相反，它突出了在这些方法的实际应用中的一个关键考虑因素，使问题更加丰富。该问题是一个关于统计建模概念基础的有效且表述良好的查询。\n\n### 步骤 3：判断与行动\n问题有效。解决方案将通过分析给定的目标函数与贝叶斯推断各组成部分之间的联系，然后评估每个选项来进行。\n\n### 解题推导\n\n问题的核心在于惩罚优化目标与贝叶斯后验分布之间的联系。在贝叶斯框架中，给定数据的参数 $p$ 的后验概率由贝叶斯法则给出：\n$$\nP(p | \\text{data}) \\propto P(\\text{data} | p) \\, P(p)\n$$\n其中 $P(\\text{data} | p)$ 是似然，而 $P(p)$ 是先验。\n\n$p$ 的最大后验（MAP）估计是最大化该后验概率的值，这等价于最小化后验概率的负对数：\n$$\n\\hat{p}_{\\text{MAP}} = \\arg\\max_p P(p | \\text{data}) = \\arg\\min_p \\left[ -\\log P(\\text{data} | p) - \\log P(p) \\right]\n$$\n\n让我们分析给定目标函数 $J(p)$ 中的各项。\n\n1.  **似然项：** 问题陈述观测噪声项 $\\varepsilon_i$ 和 $\\eta$ 是高斯分布的。堆叠残差向量 $r(p)$ 代表模型预测与数据之间的差异。假设模型 $h(x(t;p),p)$ 准确描述了系统，观测到数据的似然由噪声的概率决定。对于高斯噪声，其概率密度正比于 $\\exp(-\\frac{1}{2} \\chi^2)$，其中 $\\chi^2$ 是加权残差平方和。在这种情况下，负对数似然（在相差一个加性常数的情况下）是：\n    $$\n    -\\log P(\\text{data} | p) = \\frac{1}{2} r(p)^\\top C^{-1} r(p) + \\text{constant}\n    $$\n    这与 $J(p)$ 的第一项相匹配。\n\n2.  **先验项：** $J(p)$ 的第二项是惩罚项 $\\frac{1}{2} \\lambda \\|p - \\mu\\|_2^2$。让我们考虑参数 $p$ 上的一个高斯先验，形式为 $p \\sim \\mathcal{N}(\\mu, \\Sigma_p)$。该先验的概率密度函数为：\n    $$\n    P(p) = \\frac{1}{\\sqrt{\\det(2\\pi \\Sigma_p)}} \\exp\\left(-\\frac{1}{2} (p - \\mu)^\\top \\Sigma_p^{-1} (p - \\mu)\\right)\n    $$\n    负对数先验是：\n    $$\n    -\\log P(p) = \\frac{1}{2} (p - \\mu)^\\top \\Sigma_p^{-1} (p - \\mu) + \\text{constant}\n    $$\n    如果我们选择先验协方差为各向同性的，即 $\\Sigma_p = \\sigma_p^2 I$，其中 $I$ 是单位矩阵，负对数先验变为：\n    $$\n    -\\log P(p) = \\frac{1}{2\\sigma_p^2} (p - \\mu)^\\top I (p - \\mu) = \\frac{1}{2\\sigma_p^2} \\|p - \\mu\\|_2^2 + \\text{constant}\n    $$\n    将此与惩罚项 $\\frac{1}{2} \\lambda \\|p - \\mu\\|_2^2$ 比较，我们发现如果设置 $\\lambda = 1/\\sigma_p^2$，它们是相同的。因此，$J(p)$ 中的惩罚项在数学上等价于高斯分布 $p \\sim \\mathcal{N}(\\mu, \\lambda^{-1}I)$ 的负对数先验。\n\n综合这两点，目标函数 $J(p)$ 等价于负对数后验概率（相差一个常数）：\n$$\nJ(p) = -\\log P(p | \\text{data}) + \\text{constant}\n$$\n因此，最小化 $J(p)$ 等价于在高斯似然和高斯先验的假设下寻找 MAP 估计。\n\n### 逐项分析\n\n**A. 如果残差向量 $r(p)$ 是高斯分布且协方差 $C$ 已知，则最小化 $J(p)$ 与高斯先验 $p \\sim \\mathcal{N}(\\mu, \\lambda^{-1} I)$ 下的 MAP 估计量完全等价，因为负对数后验函数在相差一个加性常数的情况下等于 $J(p)$。**\n\n如上推导，$J(p)$ 的第一项 $\\frac{1}{2} r(p)^\\top C^{-1} r(p)$ 是高斯噪声模型下观测数据的负对数似然。第二项 $\\frac{1}{2} \\lambda \\|p - \\mu\\|_2^2$ 是高斯先验 $p \\sim \\mathcal{N}(\\mu, \\lambda^{-1} I)$ 的负对数先验。负对数似然和负对数先验之和是负对数后验。因此，最小化 $J(p)$ 根据定义就是寻找 MAP 估计的过程。该陈述是这种等价性的正确总结。条件“$r(p)$ 是高斯分布”是一个略微的简化，如果模型在 $p$ 上是非线性的；更准确地说，是*噪声*是高斯分布的，这导致了一个似然函数，其负对数是 $r(p)$ 的二次型。在加权最小二乘法的标准解释中，该陈述成立。\n\n**结论：正确**\n\n**B. 即使观测噪声是重尾的（例如，拉普拉斯分布），选项 A 中的等价性仍然存在，因为对 $p$ 的二次惩罚项对应于一个高斯先验，这与似然模型无关。**\n\n如果观测噪声服从拉普拉斯分布，其概率密度为 $P(\\epsilon) \\propto \\exp(-|\\epsilon|/b)$，其中 $b$ 是一个尺度参数。那么负对数似然将与残差的 $L_1$ 范数成正比，而不是 $L_2$ 范数的平方。用于 MAP 估计的最小化目标函数将是以下形式：\n$$\nJ_{\\text{Laplace}}(p) \\propto \\| W r(p) \\|_1 + \\frac{1}{2} \\lambda \\|p - \\mu\\|_2^2\n$$\n其中 $W$ 是从噪声模型导出的权重矩阵。这个目标函数与问题中给出的 $J(p)$ 有本质的不同，后者具有二次数据拟合项 $\\frac{1}{2} r(p)^\\top C^{-1} r(p)$。虽然对 $p$ 的二次惩罚项总是对应于高斯先验是正确的，但 MAP 估计的总体目标函数依赖于*先验和似然两者*。由于似然函数改变了，最小化特定函数 $J(p)$ 与寻找 MAP 估计之间的等价性就不再成立了。\n\n**结论：错误**\n\n**C. 如果不采用贝叶斯解释，由最小化 $J(p)$ 定义的惩罚估计量本身并不定义一个后验分布；因此，后验可信区间和边缘似然是未定义的。相反，对 $p$ 设置高斯先验会产生一个后验分布，其在 MAP 估计附近的局部协方差可以由负对数后验函数的 Hessian 矩阵的逆来近似，该近似值依赖于作为先验精度的 $\\lambda$。**\n\n这个陈述准确地捕捉了关键的认识论差异。\n-   从频率学派的角度看，最小化 $J(p)$ 是一种正则化技术（吉洪诺夫正则化），用于找到一个稳定的点估计 $\\hat{p}$。这个框架不会为 $p$ 产生一个概率分布。像后验分布和可信区间这样的概念是贝叶斯方法固有的，在此背景下没有定义。人们可能会计算频率学派的置信区间，但它们的解释是不同的。\n-   从贝叶斯角度看，似然和先验的结合定义了一个完整的后验分布 $P(p|\\text{data})$。这个分布量化了我们在看到数据后对 $p$ 的全部知识。虽然 MAP 估计只是这个分布的众数，但分布本身允许计算可信区间、边缘分布和其他量。\n-   用负对数后验函数在 MAP 处的 Hessian 矩阵的逆来近似后验协方差是一种称为拉普拉斯近似的标准技术。负对数后验是 $J(p)$（加上常数），所以其 Hessian 矩阵是 $\\nabla_p^2 J(p) = \\nabla_p^2 (\\frac{1}{2} r(p)^\\top C^{-1} r(p)) + \\lambda I$。项 $\\lambda I$（先验精度矩阵）的存在表明后验协方差估计明确地依赖于 $\\lambda$。\n因此，该陈述是对概念差异的精确和正确的描述。\n\n**结论：正确**\n\n**D. 通过交叉验证选择 $\\lambda$ 以最小化样本外预测误差，通常等价于通过最大化边缘似然（经验贝叶斯）来选择先验方差，因此在有限样本中，这两种程序会产生相同的 $\\lambda$。**\n\n这个陈述做出了一个非常强的等价性声明。交叉验证（CV）和最大化边缘似然（也称为第二类最大似然或经验贝叶斯）是选择像 $\\lambda$ 这样的超参数的两种不同的方法。虽然它们都旨在减轻过拟合，并且对于某些线性模型和特定形式的 CV（如广义交叉验证），它们可能渐近等价，但它们并“不通常等价”。对于一个复杂的、非线性的 ODE 模型，这种等价性不成立。此外，“在有限样本中会产生相同的 $\\lambda$”的说法是不正确的；即使在它们相关的案例中，它们也是不同的目标函数，对于给定的有限数据集，会产生不同的 $\\lambda$ 数值。\n\n**结论：错误**\n\n**E. 如果通过 $q = \\log p$（逐分量）进行重参数化，并惩罚 $\\|q - \\nu\\|_2^2$，则这等价于对 $q$ 施加高斯先验，从而对 $p$ 施加对数正态先验，因此正则化对速率常数的作用是乘性的而不是加性的。**\n\n这个陈述描述了一种处理正参数的标准而强大的技术。\n1.  **重参数化：** 令 $q_j = \\log p_j$，所以 $p_j = e^{q_j}$。由于 $p_j$ 必须为正， $q_j$ 可以是任何实数。这将一个关于 $p$ 的带约束优化问题转化为一个关于 $q$ 的无约束问题。\n2.  **先验等价性：** 如在 A 中所建立的，目标函数中形如 $\\frac{1}{2}\\lambda\\|q - \\nu\\|_2^2$ 的惩罚项，等价于为被惩罚的参数指定一个高斯先验。在这里，这意味着 $q \\sim \\mathcal{N}(\\nu, \\lambda^{-1}I)$。\n3.  **导出的先验：** 如果一个变量的对数是正态分布的，那么该变量本身就是对数正态分布的。因此，对 $q = \\log p$ 的高斯先验意味着对 $p$ 的对数正态先验。\n4.  **乘性正则化：** 对对数差的惩罚，例如 $(\\log p_j - \\nu_j)^2 = (\\log(p_j / e^{\\nu_j}))^2$，惩罚的是 $p_j$ 与参考值 $e^{\\nu_j}$ 之*比*的对数。这意味着它将从 $100$ 到 $1000$ 的变化（一个 $10$ 倍的因子）与从 $0.1$ 到 $1.0$ 的变化（也是一个 $10$ 倍的因子）视为幅度相似。相比之下，加性惩罚 $(p_j - \\mu_j)^2$ 惩罚的是算术差异。因此，惩罚对数参数是在乘性或相对尺度上对参数进行正则化，这对于可能跨越几个数量级的速率常数通常是可取的。\n整个陈述是对这种重参数化策略的正确而深刻的描述。\n\n**结论：正确**", "answer": "$$\\boxed{ACE}$$", "id": "3336675"}, {"introduction": "并非所有参数生而平等；有些参数比其他参数更难从数据中估计。本练习介绍费雪信息矩阵（Fisher Information Matrix, FIM）作为一个强大的诊断工具。通过分析FIM的条件数，你可以量化模型参数的实际可辨识性，并预见数值计算上的困难，例如当问题是病态时，信赖域优化器所遇到的挑战。[@problem_id:3336646]", "problem": "考虑一个用于基因表达调控实验中单一测量物种的双参数动力学模型，该模型由常微分方程 $ \\frac{dx(t)}{dt} = -k_{1} x(t) + k_{2} u(t) $ 和初始条件 $ x(0) = x_{0} $ 定义，其中 $ u(t) $ 是一个已知的输入曲线，$ \\theta = (k_{1}, k_{2}) $ 是待估计的未知参数。可观测量由 $ y(t_{i}) = x(t_{i}; \\theta) + \\varepsilon_{i} $ 给出，其中对于 $ i = 1, \\dots, N $ 个时间点，$ \\varepsilon_{i} $ 是独立同分布的高斯噪声项，其均值为零，方差为 $ \\sigma^{2} $。此外，在恒定输入 $ u(t) \\equiv u_{0} $下，采集了一个稳态测量值 $ y_{\\mathrm{ss}} = x_{\\mathrm{ss}}(\\theta) + \\varepsilon_{\\mathrm{ss}} $，其中 $ x_{\\mathrm{ss}}(\\theta) = \\frac{k_{2}}{k_{1}} u_{0} $ 且 $ \\varepsilon_{\\mathrm{ss}} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{ss}}^{2}) $ 与时间序列噪声无关。联合估计问题被构建为一个加权最小二乘问题，该问题结合了时间序列残差和稳态残差，其权重由噪声方差的倒数确定。\n\n在非线性最小二乘法的标准局部渐近正态性假设下，期望的Gauss–Newton法向矩阵等于费雪信息矩阵 (FIM)，此处定义为灵敏度加权曲率 $ J(\\theta)^{\\top} W J(\\theta) $ 的期望值，其中 $ J(\\theta) $ 是残差向量关于 $ \\theta $ 的雅可比矩阵，$ W $ 是正定对角权重矩阵，其对角线元素对于时间序列残差为 $ 1/\\sigma^{2} $，对于稳态残差为 $ 1/\\sigma_{\\mathrm{ss}}^{2} $。对于上述组合设计，假设在名义参数向量 $ \\theta^{\\ast} $ 处计算出的费雪信息矩阵 (FIM) 是对称正定的，其两个特征值 $ \\lambda_{\\max} $ 和 $ \\lambda_{\\min} $ 满足 $ \\lambda_{\\max} / \\lambda_{\\min} = 10^{6} $。\n\n使用谱条件数的基本定义 $ \\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} $，以及在给定假设下Gauss–Newton法向矩阵与费雪信息矩阵 (FIM) 之间的对应关系，以精确形式计算在 $ \\theta^{\\ast} $ 处的Gauss–Newton步长的期望谱条件数。然后，从非线性最小二乘法的信赖域框架出发，解释在求解Gauss–Newton子问题时，这种条件性如何影响信赖域半径和度量的选择，并阐明在存在组合的时间序列和稳态数据的情况下，其对步长接受和收敛行为的后果。\n\n将最终数值答案表示为精确的无量纲值；无需四舍五入。", "solution": "问题需经过验证。\n\n**步骤1：提取已知条件**\n-   动力学模型: $ \\frac{dx(t)}{dt} = -k_{1} x(t) + k_{2} u(t) $\n-   初始条件: $ x(0) = x_{0} $\n-   参数: $ \\theta = (k_{1}, k_{2}) $\n-   输入: $ u(t) $ 是一个已知函数。\n-   时间序列观测值: $ y(t_{i}) = x(t_{i}; \\theta) + \\varepsilon_{i} $，对于 $ i = 1, \\dots, N $。\n-   时间序列噪声: $ \\varepsilon_{i} \\sim \\mathcal{N}(0, \\sigma^{2}) $，独立同分布。\n-   稳态观测值: $ y_{\\mathrm{ss}} = x_{\\mathrm{ss}}(\\theta) + \\varepsilon_{\\mathrm{ss}} $ 在恒定输入 $ u(t) \\equiv u_{0} $ 下。\n-   稳态模型: $ x_{\\mathrm{ss}}(\\theta) = \\frac{k_{2}}{k_{1}} u_{0} $。\n-   稳态噪声: $ \\varepsilon_{\\mathrm{ss}} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{ss}}^{2}) $。\n-   估计框架: 结合两种数据类型的加权最小二乘法。\n-   假设: 期望的Gauss–Newton法向矩阵等于费雪信息矩阵 (FIM)。\n-   FIM 定义: $ J(\\theta)^{\\top} W J(\\theta) $。\n-   权重矩阵 $W$: 对角矩阵，元素为 $ 1/\\sigma^{2} $ 和 $ 1/\\sigma_{\\mathrm{ss}}^{2} $。\n-   在名义参数向量 $ \\theta^{\\ast} $ 处的FIM: 对称正定。\n-   在 $ \\theta^{\\ast} $ 处FIM的特征值: $ \\lambda_{\\max} $ 和 $ \\lambda_{\\min} $。\n-   特征值比率: $ \\lambda_{\\max} / \\lambda_{\\min} = 10^{6} $。\n-   谱条件数的定义: $ \\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} $。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在科学和数学上是合理的。该模型是一个标准的一阶线性常微分方程，广泛应用于系统生物学。涉及加权最小二乘法、高斯噪声和费雪信息矩阵的统计公式是参数估计的一种经典方法。所有提供的信息都是自洽、一致且相关的。稳态解 $ x_{\\mathrm{ss}}(\\theta) $ 的推导是正确的，因为在ODE中设置 $ \\frac{dx(t)}{dt} = 0 $ 会得到 $ 0 = -k_{1} x_{\\mathrm{ss}} + k_{2} u_{0} $，解得 $ x_{\\mathrm{ss}} = (k_{2}/k_{1})u_{0} $。该问题将FIM的理论性质（其条件数）与数值优化（信赖域方法）的实践方面联系起来，这是计算科学的核心课题。问题是适定的，并要求进行特定的数值计算和随后的概念性解释。\n\n**步骤3：结论与行动**\n问题有效。将提供完整解答。\n\n该问题要求分两部分解答：首先，计算Gauss–Newton法向矩阵的期望谱条件数；其次，解释其对信赖域优化算法的影响。\n\n**第一部分：谱条件数的计算**\n\n问题指出，在给定假设下，期望的Gauss–Newton法向矩阵等于费雪信息矩阵 (FIM)。设该矩阵为 $ A = \\text{FIM}(\\theta^{\\ast}) $。问题还指明该矩阵是对称正定的。\n\n矩阵 $ A $ 的谱条件数定义为 $ \\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} $，其中 $ \\| \\cdot \\|_{2} $ 表示矩阵的2-范数（或谱范数）。\n\n对于对称矩阵，其2-范数等于其谱半径，即最大特征值的绝对值。由于 $ A $ 是正定的，其所有特征值均为正。设 $ A $ 的特征值为 $ \\lambda_{i} > 0 $。最大特征值为 $ \\lambda_{\\max} $。因此，\n$$\n\\|A\\|_{2} = \\max_{i} |\\lambda_{i}| = \\lambda_{\\max}\n$$\n$ A $ 的逆矩阵，记为 $ A^{-1} $，也是对称正定的。$ A^{-1} $ 的特征值是 $ A $ 的特征值的倒数，即 $ 1/\\lambda_{i} $。因此，$ A^{-1} $ 的最大特征值是 $ A $ 的最小特征值的倒数。\n$$\n\\|A^{-1}\\|_{2} = \\max_{i} \\left|\\frac{1}{\\lambda_{i}}\\right| = \\frac{1}{\\min_{i} |\\lambda_{i}|} = \\frac{1}{\\lambda_{\\min}}\n$$\n将这些代入条件数的定义中，可得：\n$$\n\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = \\lambda_{\\max} \\cdot \\frac{1}{\\lambda_{\\min}} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\n$$\n问题中给出了在名义参数向量 $ \\theta^{\\ast} $ 处FIM的最大和最小特征值之比为 $ \\lambda_{\\max} / \\lambda_{\\min} = 10^{6} $。\n\n因此，在 $ \\theta^{\\ast} $ 处Gauss–Newton法向矩阵的期望谱条件数为：\n$$\n\\kappa_{2}(\\text{FIM}(\\theta^{\\ast})) = 10^{6}\n$$\n\n**第二部分：对信赖域方法的影响**\n\n条件数为 $ 10^{6} $ 表明Gauss–Newton法向矩阵是严重病态的。在非线性最小二乘法的信赖域框架下，这对优化过程具有深远的影响。\n\n在每次迭代 $k$ 中，信赖域子问题是在由半径 $ \\Delta_{k} $ 定义的信赖区域内，找到一个步长 $ p $ 来最小化目标函数的二次模型 $ m_{k}(p) $：\n$$\n\\min_{p} m_{k}(p) = f(\\theta_{k}) + g_{k}^{\\top}p + \\frac{1}{2} p^{\\top}B_{k}p \\quad \\text{约束条件为} \\quad \\|D_{k}p\\|_{2} \\le \\Delta_{k}\n$$\n在此，$ g_{k} $ 是残差平方和的梯度，$ B_{k} $ 是Hessian矩阵的Gauss-Newton近似，在本问题中对应于FIM，即 $ B_{k} = J(\\theta_{k})^{\\top} W J(\\theta_{k}) $。矩阵 $ D_{k} $ 定义了信赖域的度量。\n\n$ \\kappa_{2}(B_{k}) = 10^{6} $ 的病态矩阵 $ B_k $ 意味着二次模型 $ m_{k}(p) $ 具有高度拉长的椭圆形等值线。目标函数近似的曲率在与 $ \\lambda_{\\max} $ 相关联的特征向量方向上极高，而在与 $ \\lambda_{\\min} $ 相关联的特征向量方向上极低（平坦）。\n\n对信赖域半径和度量的影响：\n1.  **信赖域半径 ($ \\Delta_{k} $):** 信赖域方法的基本原理是根据模型预测的目标函数下降量与试验步长实现的实际下降量之间的一致性来调整半径 $ \\Delta_{k} $。病态的 $ B_{k} $ 意味着二次模型对真实函数的近似非常差，尤其是在与小特征值对应的平坦“山谷”方向上。在模型中看似合理的一个步长 $ p $ 可能导致真实目标函数发生大的、意外的变化，甚至常常是增加。这会导致一个较差的一致性比率 $ \\rho_{k} $。标准的信赖域更新规则规定，当 $ \\rho_{k} $ 很低或为负时，拒绝该步长，并大幅缩减信赖域半径 $ \\Delta_{k} $。因此，算法被迫采取非常小的步长以确保二次模型保持有效近似，从而严重减慢收敛速度。\n\n2.  **信赖域度量 ($ D_{k} $):** 如果使用标准的欧几里得度量（$ D_{k} $ 是单位矩阵），则信赖域是一个球体。对于具有高度椭圆形等值线的目标函数，球形信赖域是一个很差的几何选择。指向二次模型最小值的Newton方向 $ p_{N} = -B_{k}^{-1} g_{k} $，可能非常大，并远在合理大小的球形信赖域之外。球体内的最优步长可能与Newton方向大相径庭，导致在目标函数的狭窄山谷中出现低效的“之字形”行为。为了解决这个问题，可以使用一个缩放矩阵 $ D_{k} $ 通过 $ \\|D_{k}p\\|_{2} \\le \\Delta_{k} $ 来定义一个椭圆形信赖域。一个精心选择的 $ D_{k} $（例如，从 $ B_k $ 的对角线导出的对角矩阵，或更复杂的预处理器）可以重塑信赖域，使其与 $ m_{k}(p) $ 的椭圆形等值线对齐。这允许沿着山谷的平坦底部采取更大、更有效的步长，从而提高算法效率。病态性使得该度量的选择对性能至关重要。\n\n对收敛和步长接受的后果：\n-   **步长接受和收敛行为：** 由于模型与真实函数之间频繁不匹配，试验步长经常被拒绝，信赖域半径不断缩小。这导致一系列非常小的被接受的步长，使得算法“停滞”或以非常慢的线性速率收敛，而不是所期望的二次或超线性速率。算法在参数空间的平坦方向上进展甚微。\n\n-   **与数据和可辨识性的联系：** FIM的高条件数表示实际上的不可辨识性。与 $ \\lambda_{\\min} $ 相关的特征向量代表了参数（$ k_{1} $ 和 $ k_{2} $）的一种组合，该组合对模型输出 $ x(t; \\theta) $ 的影响非常小。这意味着组合的时间序列和稳态数据提供的信息很少，无法约束这种特定的参数组合。例如，稳态数据约束了比率 $ k_{2}/k_{1} $，但如果时间序列数据对于时间尺度（这取决于 $ k_{1} $）的信息不足，那么 $ k_{1} $ 和 $ k_{2} $ 可以在保持其比率近似恒定的情况下，沿着似然曲面上的一个“山脊”一起变化，而不会显著改变模型与数据的一致性。优化器难以沿着这个平坦的山谷找到精确的最小值，这正是这种统计不确定性的数值体现。", "answer": "$$\n\\boxed{10^{6}}\n$$", "id": "3336646"}, {"introduction": "现在，让我们通过从头开始构建一个参数估计引擎，将理论付诸实践。这个编码练习将指导你实现Metropolis-Hastings算法，它是马尔可夫链蒙特卡洛（MCMC）方法的基石。你将处理从使用常微分方程（ODE）求解器模拟模型，到计算后验概率，再到做出接受或拒绝一个提议参数值的统计决策的全过程。[@problem_id:3336664]", "problem": "考虑一个在计算系统生物学中通过非线性常微分方程 (ODE) 建模的单物种基因调控系统。浓度 $X(t)$（单位为 $\\mathrm{mol\\,L^{-1}}$）根据以下方程演化：\n$$\n\\frac{dX}{dt} \\;=\\; \\frac{k_{\\mathrm{syn}}}{1 + \\left(\\frac{X}{K}\\right)^{n}} \\;-\\; k_{\\mathrm{deg}}\\,X,\n$$\n其中 $k_{\\mathrm{syn}}$ 是合成速率，$k_{\\mathrm{deg}}$ 是降解速率，$n$ 是希尔系数，$K$ 是解离常数（一个正常数）。假设 $k_{\\mathrm{syn}} = 2.0$（单位为 $\\mathrm{mol\\,L^{-1}\\,s^{-1}}$），$k_{\\mathrm{deg}} = 0.4$（单位为 $\\mathrm{s^{-1}}$），$n = 2$（无量纲），初始条件 $X(0) = 0$（单位为 $\\mathrm{mol\\,L^{-1}}$）。\n\n合成数据按以下方式生成。设 $K_{\\mathrm{true}} = 0.8$（单位为 $\\mathrm{mol\\,L^{-1}}$）。对 ODE 进行数值积分，以获得在时间点 $t_i \\in \\{0,1,2,3,5,8,12,16,20\\}$（单位为 $\\mathrm{s}$）和稳态探测时间 $T_{\\mathrm{ss}} = 40$（单位为 $\\mathrm{s}$）处的无噪声轨迹 $X(t_i; K_{\\mathrm{true}})$。然后，使用固定的随机种子 $123$，为时间序列点和稳态探测点抽取标准差分别为 $\\sigma = 0.05$（单位为 $\\mathrm{mol\\,L^{-1}}$）和 $\\sigma_{\\mathrm{ss}} = 0.05$（单位为 $\\mathrm{mol\\,L^{-1}}$）的高斯测量噪声。观测数据为：\n$$\ny_i \\;=\\; X(t_i; K_{\\mathrm{true}}) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n和\n$$\ny_{\\mathrm{ss}} \\;=\\; X(T_{\\mathrm{ss}}; K_{\\mathrm{true}}) + \\varepsilon_{\\mathrm{ss}},\\quad \\varepsilon_{\\mathrm{ss}} \\sim \\mathcal{N}(0,\\sigma_{\\mathrm{ss}}^2).\n$$\n\n我们考虑使用时间序列数据和稳态数据对单个参数 $K$ 进行贝叶斯参数估计。设后验概率与 $L(p)\\,\\pi(p)$ 成正比，其中 $p$ 表示完整的参数矢量，$L(p)$ 是从时间序列 $\\{y_i\\}$ 和稳态探测 $y_{\\mathrm{ss}}$ 的高斯噪声模型推导出的似然函数，$\\pi(p)$ 是先验概率。在此问题中，其他参数（$k_{\\mathrm{syn}}, k_{\\mathrm{deg}}, n$）是固定的已知值，因此 $p$ 简化为 $K$。假设 $K$ 的先验分布为对数正态分布，$K \\sim \\mathrm{LogNormal}(\\mu,\\tau^2)$，其中 $\\mu = \\ln(0.8)$ 且 $\\tau = 0.3$。\n\n您必须为参数 $K$ 实现一个 Metropolis-Hastings (MH) 更新，使用一个由 $\\ln K' = \\ln K + s\\,Z$ 定义的对数正态随机游走提议 $q(K' \\mid K)$，其中 $Z \\sim \\mathcal{N}(0,1)$ 且提议尺度 $s > 0$。MH 步骤必须根据第一性原理（贝叶斯法则和 Metropolis-Hastings 算法）计算接受概率，并考虑到非对称提议 $q(K' \\mid K)$ 的影响。\n\n算法要求：\n- 使用科学上合理的方法数值积分 ODE，以计算 $X(t_i;K)$ 和 $X(T_{\\mathrm{ss}};K)$。使用适合刚性或非刚性问题的 ODE 求解器。\n- 根据高斯噪声模型为时间序列和稳态观测构建对数似然函数。对数似然函数必须包含两部分的贡献，并且必须与指定的 $\\sigma$ 和 $\\sigma_{\\mathrm{ss}}$ 一致。\n- 为 $K$ 的对数正态先验构建对数先验。\n- 从贝叶斯法则和 Metropolis-Hastings 接受准则推导 MH 步骤的接受概率，明确考虑由非对称对数正态提议引起的 Hastings 校正。\n- 实现一个单一的 MH 更新，该更新从 $q(K' \\mid K)$ 中提议 $K'$，计算接受概率，并报告该概率。使用固定的随机种子 $2025$ 进行提议生成，以确保确定性行为。\n- 如果 ODE 积分失败或 $K' \\le 0$，则将该提议视为无效，并将接受概率设置为 $0$。\n\n测试套件：\n对于每个测试用例，给定当前 $K$ 值和提议尺度 $s$。使用上面定义的共享合成数据集（由您的程序内部生成）和共享的先验参数。为每个用例评估单次 MH 更新的接受概率。\n\n- 测试用例 1（理想路径）：当前 $K = 0.5$，提议尺度 $s = 0.2$。\n- 测试用例 2（边界探索）：当前 $K = 0.05$，提议尺度 $s = 0.6$。\n- 测试用例 3（从较远值进行的保守提议）：当前 $K = 2.0$，提议尺度 $s = 0.1$。\n- 测试用例 4（接近真实值且提议相对激进）：当前 $K = 0.8$，提议尺度 $s = 0.5$。\n\n您的程序应生成单行输出，其中包含上述测试用例的接受概率，格式为方括号内的逗号分隔列表（例如，$[a_1,a_2,a_3,a_4]$）。每个接受概率必须以 $[0,1]$ 区间内的浮点数形式报告（无单位）。不应打印任何额外文本。", "solution": "该问题要求计算 Metropolis-Hastings (MH) 单次更新的接受概率，该更新用于基因调控的非线性常微分方程 (ODE) 模型中的参数 $K$。该估计在贝叶斯框架下进行，利用了合成的时间序列和稳态数据。\n\n### 1. 问题验证\n\n问题已经过验证，并被确定为**有效**。\n- **已知条件**：\n    - **ODE 模型**：$\\frac{dX}{dt} = \\frac{k_{\\mathrm{syn}}}{1 + (X/K)^n} - k_{\\mathrm{deg}}X$，其中 $X(0) = 0$。\n    - **固定常数**：$k_{\\mathrm{syn}} = 2.0\\,\\mathrm{mol\\,L^{-1}\\,s^{-1}}$，$k_{\\mathrm{deg}} = 0.4\\,\\mathrm{s^{-1}}$，$n = 2$。\n    - **数据生成**：使用真实参数 $K_{\\mathrm{true}} = 0.8\\,\\mathrm{mol\\,L^{-1}}$。数据点位于 $t_i \\in \\{0, 1, 2, 3, 5, 8, 12, 16, 20\\}\\,\\mathrm{s}$ 和 $T_{\\mathrm{ss}} = 40\\,\\mathrm{s}$。测量噪声为高斯噪声，$\\sigma = \\sigma_{\\mathrm{ss}} = 0.05\\,\\mathrm{mol\\,L^{-1}}$。用于数据生成的随机种子固定为 $123$。\n    - **贝叶斯设置**：待估计参数为 $K > 0$。$K$ 的先验为对数正态分布，$K \\sim \\mathrm{LogNormal}(\\mu, \\tau^2)$，其中 $\\mu = \\ln(0.8)$ 且 $\\tau = 0.3$。\n    - **MH 提议**：对数正态随机游走，$\\ln K' = \\ln K + sZ$，其中 $Z \\sim \\mathcal{N}(0, 1)$。用于提议生成的随机种子固定为 $2025$。\n    - **测试用例**：指定了四个用例，包含 $K$ 的初始值和提议尺度 $s$。\n- **结论**：该问题具有科学依据，是适定、客观且完整的。它代表了计算系统生物学中的一项标准任务。\n\n### 2. 贝叶斯模型构建\n\n给定观测数据 $D = \\{y_i\\} \\cup \\{y_{\\mathrm{ss}}\\}$，参数 $K$ 的后验分布由贝叶斯定理给出：\n$$\np(K | D) \\propto L(D | K) \\pi(K)\n$$\n其中 $L(D | K)$ 是似然函数，$\\pi(K)$ 是先验。在计算上，处理对数后验更为稳定：\n$$\n\\log p(K | D) = \\log L(D | K) + \\log \\pi(K) + \\mathrm{const}.\n$$\n\n**对数似然：**\n假设测量噪声是独立同分布的高斯噪声，均值为 $0$，标准差为 $\\sigma = 0.05$。令 $\\mathbf{t}_{\\mathrm{obs}}$ 为所有观测时间的向量，包括 $T_{\\mathrm{ss}}$，令 $X(\\mathbf{t}_{\\mathrm{obs}}; K)$ 为通过使用参数 $K$ 数值积分 ODE 得到的相应模型预测值。对数似然为：\n$$\n\\log L(D | K) = -\\frac{1}{2\\sigma^2} \\sum_{j} (y_j - X(t_j; K))^2 + \\mathrm{const}.\n$$\n求和遍及所有数据点。常数项可以忽略，因为它在 MH 接受率的计算中会被抵消。\n\n**对数先验：**\n参数 $K$ 被赋予对数正态先验，$K \\sim \\mathrm{LogNormal}(\\mu, \\tau^2)$。这等价于陈述 $\\ln K$ 服从正态分布，$\\ln K \\sim \\mathcal{N}(\\mu, \\tau^2)$。$K$ 的概率密度函数为：\n$$\n\\pi(K) = \\frac{1}{K\\tau\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln K - \\mu)^2}{2\\tau^2}\\right)\n$$\n忽略常数项，对数先验为：\n$$\n\\log \\pi(K) \\propto -\\ln K - \\frac{(\\ln K - \\mu)^2}{2\\tau^2}\n$$\n\n### 3. Metropolis-Hastings 接受概率\n\nMH 算法用于从后验分布中采样。给定当前参数值 $K$，从一个分布 $q(K' | K)$ 中提议一个新值 $K'$。该提议以概率 $\\alpha$ 被接受：\n$$\n\\alpha(K' | K) = \\min\\left(1, \\frac{p(K'|D)q(K|K')}{p(K|D)q(K'|K)}\\right)\n$$\n项 $\\frac{q(K|K')}{q(K'|K)}$ 是 Hastings 校正，它解释了提议分布中的任何不对称性。\n\n**提议分布与 Hastings 校正：**\n提议是一个对数正态随机游走：$K' = K e^{sZ}$，其中 $Z \\sim \\mathcal{N}(0, 1)$。这可以写作 $\\ln K' = \\ln K + sZ$。令 $v = \\ln K$ 且 $v' = \\ln K'$。$v$ 的提议是 $v' \\sim \\mathcal{N}(v, s^2)$，这是对称的：$f(v'|v) = f(v|v')$。\n为了找到提议密度 $q(K'|K)$，我们使用变量变换公式：$q(K'|K) = f(v'|v) \\left| \\frac{dv'}{dK'} \\right| = f(v'|v) \\frac{1}{K'}$。\n因此，Hastings 校正为：\n$$\n\\frac{q(K|K')}{q(K'|K)} = \\frac{f(v|v')/K}{f(v'|v)/K'} = \\frac{1/K}{1/K'} = \\frac{K'}{K}\n$$\n\n**通过重参数化进行推导：**\n一个更优雅的方法是根据 $\\kappa = \\ln K$ 对模型进行重参数化。\n- **变换后的先验**：$\\kappa$ 的先验现在是正态分布，$\\kappa \\sim \\mathcal{N}(\\mu, \\tau^2)$，所以 $\\log \\pi(\\kappa) \\propto -\\frac{(\\kappa - \\mu)^2}{2\\tau^2}$。\n- **变换后的提议**：提议是 $\\kappa' = \\kappa + sZ$，这是一个对称随机游走。提议密度 $q(\\kappa'|\\kappa)$ 是对称的，意味着 $q(\\kappa'|\\kappa) = q(\\kappa|\\kappa')$，因此 Hastings 校正为 $1$。\n- **变换后的似然**：似然是原始参数 $K=e^\\kappa$ 的函数，所以 $\\log L(D|\\kappa) = \\log L(D|K=e^\\kappa)$。\n- **变换后的后验**：$\\kappa$ 的对数后验为 $\\log p(\\kappa|D) \\propto \\log L(D|e^\\kappa) - \\frac{(\\kappa-\\mu)^2}{2\\tau^2}$。\n\n以 $\\kappa$ 表示的接受率为：\n$$\nA = \\frac{p(\\kappa'|D)}{p(\\kappa|D)} = \\frac{L(D|e^{\\kappa'}) \\pi(\\kappa')}{L(D|e^{\\kappa}) \\pi(\\kappa)}\n$$\n取对数，我们得到对数接受率：\n$$\n\\log A = \\log p(\\kappa'|D) - \\log p(\\kappa|D)\n$$\n$$\n\\log A = \\left(\\log L(D|e^{\\kappa'}) - \\frac{(\\kappa' - \\mu)^2}{2\\tau^2}\\right) - \\left(\\log L(D|e^{\\kappa}) - \\frac{(\\kappa - \\mu)^2}{2\\tau^2}\\right)\n$$\n代入 $\\kappa = \\ln K$ 和 $\\kappa' = \\ln K'$：\n$$\n\\log A = \\left(\\log L(D|K') - \\frac{(\\ln K' - \\mu)^2}{2\\tau^2}\\right) - \\left(\\log L(D|K) - \\frac{(\\ln K - \\mu)^2}{2\\tau^2}\\right)\n$$\n这个计算正确地结合了似然、先验和隐式的 Hastings 校正。接受概率 $\\alpha$ 于是为 $\\min(1, \\exp(\\log A))$。\n\n### 4. 算法实现\n\n1.  **生成合成数据**：\n    -   将随机种子设置为 $123$。\n    -   使用固定参数和 $K_{\\mathrm{true}} = 0.8$ 定义 ODE 系统。\n    -   使用 `scipy.integrate.solve_ivp` 在时间跨度 $[0, 40]$ 上对所有指定的观测时间 $t_j$ 积分 ODE。\n    -   生成高斯噪声 $\\varepsilon_j \\sim \\mathcal{N}(0, \\sigma^2)$ 并将其加到真实轨迹点 $X(t_j; K_{\\mathrm{true}})$ 上，以获得观测数据 $y_j$。\n\n2.  **定义对数后验函数**：\n    -   创建一个函数 `log_posterior(K)`，它返回对数后验值（直到一个常数）。\n    -   该函数检查是否 $K \\le 0$。如果是，则返回 $-\\infty$，对应于后验概率为 $0$。\n    -   它使用给定的 $K$ 调用 `solve_ivp`。如果积分失败，它也返回 $-\\infty$。\n    -   如果成功，它计算模型预测与观测数据之间的误差平方和。\n    -   它计算对数似然项：$-\\frac{SSE}{2\\sigma^2}$。\n    -   它使用 $\\kappa = \\ln K$ 的公式计算对数先验项：$-\\frac{(\\ln K - \\mu)^2}{2\\tau^2}$。\n    -   该函数返回这两项的和。\n\n3.  **对每个测试用例执行 MH 更新**：\n    -   将用于提议生成的随机种子设置为 $2025$。\n    -   对于每个测试用例 `(K_current, s)`：\n        a. 计算当前状态的对数后验：`log_post_current = log_posterior(K_current)`。\n        b. 生成一个标准正态随机变量 $Z$。\n        c. 提议一个新状态：$K_{\\mathrm{prop}} = K_{\\mathrm{current}} \\cdot \\exp(s \\cdot Z)$。\n        d. 计算提议状态的对数后验：`log_post_prop = log_posterior(K_prop)`。\n        e. 函数 `log_posterior` 通过返回 $-\\infty$ 来处理无效提议（$K_{\\mathrm{prop}} \\le 0$ 或求解器失败）。如果 `log_post_prop` 是 $-\\infty$，则接受概率为 $0$。\n        f. 否则，计算对数接受率：`log_ A = log_post_prop - log_post_current`。\n        g. 接受概率为 $\\alpha = \\min(1.0, \\exp(\\log A))$。\n    -   收集所有测试用例计算出的 $\\alpha$。\n\n4.  **最终输出**：\n    -   将接受概率列表格式化为所需的字符串 `[p1,p2,p3,p4]`。", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian parameter estimation problem for a gene regulation model.\n    \"\"\"\n\n    # --- 1. Define Model Constants and Parameters ---\n    K_SYN = 2.0  # mol L^-1 s^-1\n    K_DEG = 0.4  # s^-1\n    N_HILL = 2.0  # dimensionless\n    X0 = 0.0  # mol L^-1\n\n    # Data generation parameters\n    K_TRUE = 0.8  # mol L^-1\n    TIME_POINTS_TS = np.array([0, 1, 2, 3, 5, 8, 12, 16, 20], dtype=float) # s\n    TIME_POINT_SS = 40.0 # s\n    OBS_TIMES = np.sort(np.unique(np.append(TIME_POINTS_TS, TIME_POINT_SS)))\n    T_SPAN = (0, OBS_TIMES[-1])\n    \n    SIGMA = 0.05  # mol L^-1 (same for time-series and steady-state)\n    DATA_SEED = 123\n    \n    # Bayesian inference parameters\n    PRIOR_MU = np.log(0.8)\n    PRIOR_TAU = 0.3\n    PROPOSAL_SEED = 2025\n\n    # Test cases\n    test_cases = [\n        (0.5, 0.2),  # K_current, proposal_scale_s\n        (0.05, 0.6),\n        (2.0, 0.1),\n        (0.8, 0.5),\n    ]\n\n    # --- 2. Define the ODE System ---\n    def ode_system(t, x, K, k_syn, k_deg, n):\n        if K <= 0:\n            # Physically meaningless, but as a safeguard.\n            return [np.nan]\n        \n        # Ensure x is not negative for the model evaluation\n        x_val = max(0, x[0])\n        \n        dxdt = (k_syn / (1 + (x_val / K)**n)) - k_deg * x_val\n        return [dxdt]\n\n    # --- 3. Generate Synthetic Data ---\n    # This is done once and shared across all test cases.\n    def generate_data():\n        rng = np.random.default_rng(DATA_SEED)\n        \n        # Integrate ODE to get noise-free data\n        sol = solve_ivp(\n            ode_system,\n            T_SPAN,\n            [X0],\n            method='RK45',\n            t_eval=OBS_TIMES,\n            args=(K_TRUE, K_SYN, K_DEG, N_HILL)\n        )\n        \n        if not sol.success or sol.y.shape[1] != len(OBS_TIMES):\n             raise RuntimeError(\"ODE integration failed during data generation.\")\n        \n        x_true = sol.y[0]\n        \n        # Add Gaussian noise\n        noise = rng.normal(0, SIGMA, size=len(x_true))\n        y_obs = x_true + noise\n        \n        # Data at t=0 must be exactly X(0) if observed.\n        if 0 in OBS_TIMES:\n            y_obs[np.where(OBS_TIMES == 0)] = X0\n            \n        return y_obs, OBS_TIMES\n\n    y_observed, t_observed = generate_data()\n    \n    # --- 4. Define Log-Posterior Calculation ---\n    # This computation is central to the MH step.\n    memo = {}\n    def calculate_log_posterior(K):\n        # Memoization to avoid re-computing for the same K\n        if K in memo:\n            return memo[K]\n            \n        # Prior check: K must be positive\n        if K <= 0:\n            return -np.inf\n\n        # Calculate log-prior based on ln(K) ~ N(mu, tau^2)\n        log_prior_val = norm.logpdf(np.log(K), loc=PRIOR_MU, scale=PRIOR_TAU)\n\n        # Numerically integrate the ODE for the given K\n        sol = solve_ivp(\n            ode_system,\n            T_SPAN,\n            [X0],\n            method='RK45',\n            t_eval=t_observed,\n            args=(K, K_SYN, K_DEG, N_HILL)\n        )\n\n        # Handle integration failure\n        if not sol.success or sol.y.shape[1] != len(t_observed) or np.any(np.isnan(sol.y)):\n            memo[K] = -np.inf\n            return -np.inf\n\n        # Calculate log-likelihood\n        x_predicted = sol.y[0]\n        sum_sq_err = np.sum((y_observed - x_predicted)**2)\n        log_likelihood_val = -sum_sq_err / (2 * SIGMA**2)\n\n        result = log_likelihood_val + log_prior_val\n        memo[K] = result\n        return result\n\n    # --- 5. Perform MH Update for Each Test Case ---\n    rng_proposal = np.random.default_rng(PROPOSAL_SEED)\n    results = []\n\n    for k_current, s in test_cases:\n        memo.clear() # Clear memoization for each independent test case\n        \n        # Calculate log posterior for the current state\n        log_post_current = calculate_log_posterior(k_current)\n        \n        # Propose a new state using log-normal random walk\n        z = rng_proposal.normal(0, 1)\n        k_proposal = k_current * np.exp(s * z)\n        \n        # Calculate log posterior for the proposed state\n        log_post_proposal = calculate_log_posterior(k_proposal)\n\n        # If proposal is invalid (log_post = -inf), acceptance probability is 0\n        if np.isneginf(log_post_proposal):\n             acceptance_prob = 0.0\n        else:\n            # Calculate acceptance probability in log-space to avoid underflow\n            log_acceptance_ratio = log_post_proposal - log_post_current\n            acceptance_prob = min(1.0, np.exp(log_acceptance_ratio))\n        \n        results.append(acceptance_prob)\n\n    # --- 6. Final Output ---\n    # Format the output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{p:.7f}' for p in results)}]\")\n\nsolve()\n```", "id": "3336664"}]}