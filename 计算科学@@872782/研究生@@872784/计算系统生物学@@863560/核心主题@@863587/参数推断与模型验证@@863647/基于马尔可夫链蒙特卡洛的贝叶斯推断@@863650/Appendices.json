{"hands_on_practices": [{"introduction": "马尔可夫链蒙特卡洛 (MCMC) 的强大功能是有代价的：从链中抽取的样本是相关的，而非独立的。本练习将从根本上探讨如何量化这种低效率。通过一个假设的 MCMC 轨迹，你将计算积分自相关时间 ($\\tau_{\\text{int}}$) 和有效样本量 (ESS)，从而对链的性能建立起量化理解，这是迈向稳健贝叶斯推断的第一步。", "problem": "考虑一个计算系统生物学中的单参数贝叶斯推断问题，其中一个具有希尔型启动子的自调控基因表达模型被拟合到单细胞时间序列数据。未知参数是希尔系数 $n$，该系数是无量纲的。在经过适当的预烧期后，通过马尔可夫链蒙特卡洛（MCMC；Markov chain Monte Carlo）方法获得了 $n$ 的一个后验样本。假设如下：\n\n- 长度为 $N=20000$ 的预烧期后链 $\\{X_t\\}_{t=1}^{N}$ 是严格平稳和遍历的。\n- 对于所有整数 $k\\geq 1$，链的滞后k阶自相关函数为 $\\rho_k=\\rho(X_t,X_{t+k})=0.6^{k}$，且 $\\rho_0=1$。\n- 从链中估计出的 $n$ 的经验后验标准差为 $\\hat{\\sigma}=0.8$。\n\n从平稳过程的自协方差函数的基本定义，以及用自协方差序列表示的样本均值方差出发，推导后验均值估计量 $\\bar{X}=\\frac{1}{N}\\sum_{t=1}^{N}X_t$ 的方差关于自相关函数的大样本表达式，并用它来定义积分自相关时间和有效样本量。然后，对于给定的自相关函数，评估这些量，并计算 $n$ 的后验均值的蒙特卡洛标准误差（MCSE；Monte Carlo standard error）。\n\n将积分自相关时间和蒙特卡洛标准误差以行向量的形式报告，并将两个量都四舍五入到四位有效数字。希尔系数是无量纲的，因此不需要物理单位。", "solution": "该问题是有效的，因为它在科学上基于对马尔可夫链蒙特卡洛输出的统计分析，这是计算系统生物学中的一种标准技术。该问题提法恰当，提供了所有必要的信息，并且其语言客观而精确。\n\n目标是计算参数 $n$ 的后验均值的积分自相关时间（$\\tau$）和蒙特卡洛标准误差（MCSE）。后验样本由一个平稳且遍历的马尔可夫链 $\\{X_t\\}_{t=1}^{N}$ 表示。\n\n首先，我们推导样本均值 $\\bar{X} = \\frac{1}{N}\\sum_{t=1}^{N}X_t$ 的方差的一般表达式。$\\bar{X}$ 的方差由下式给出：\n$$\n\\text{Var}(\\bar{X}) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N}X_t\\right) = \\frac{1}{N^2}\\text{Var}\\left(\\sum_{t=1}^{N}X_t\\right)\n$$\n和的方差可以展开为协方差的和：\n$$\n\\text{Var}\\left(\\sum_{t=1}^{N}X_t\\right) = \\sum_{s=1}^{N}\\sum_{t=1}^{N}\\text{Cov}(X_s, X_t)\n$$\n设后验分布的真实方差为 $\\sigma^2 = \\text{Var}(X_t)$。平稳过程的自协方差函数为 $\\gamma_k = \\text{Cov}(X_t, X_{t+k})$，它只取决于滞后 $k$。注意 $\\gamma_0 = \\sigma^2$。自相关函数为 $\\rho_k = \\frac{\\gamma_k}{\\gamma_0}$。\n通过收集具有相同滞后 $|s-t|=k$ 的项，可以将双重求和重写为：\n$$\n\\sum_{s=1}^{N}\\sum_{t=1}^{N}\\text{Cov}(X_s, X_t) = \\sum_{s=1}^{N}\\sum_{t=1}^{N}\\gamma_{|s-t|} = N\\gamma_0 + 2\\sum_{k=1}^{N-1}(N-k)\\gamma_k\n$$\n将此代回 $\\text{Var}(\\bar{X})$ 的表达式中：\n$$\n\\text{Var}(\\bar{X}) = \\frac{1}{N^2}\\left(N\\gamma_0 + 2\\sum_{k=1}^{N-1}(N-k)\\gamma_k\\right) = \\frac{\\gamma_0}{N}\\left(1 + 2\\sum_{k=1}^{N-1}\\frac{N-k}{N}\\frac{\\gamma_k}{\\gamma_0}\\right)\n$$\n用自相关函数 $\\rho_k$ 表示：\n$$\n\\text{Var}(\\bar{X}) = \\frac{\\sigma^2}{N}\\left(1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\rho_k\\right)\n$$\n对于大样本量 $N$ 和一个衰减足够快的自相关函数 $\\rho_k$，对于那些 $\\rho_k$ 不可忽略的 $k$ 值，项 $(1 - k/N) \\approx 1$。求和也可以扩展到无穷大。这给出了大样本近似：\n$$\n\\text{Var}(\\bar{X}) \\approx \\frac{\\sigma^2}{N}\\left(1 + 2\\sum_{k=1}^{\\infty}\\rho_k\\right)\n$$\n这个表达式引出了积分自相关时间 $\\tau$ 的定义。它被定义为相对于独立样本，由于相关性而导致方差膨胀的因子。\n$$\n\\tau = 1 + 2\\sum_{k=1}^{\\infty}\\rho_k\n$$\n使用这个定义，均值的方差为 $\\text{Var}(\\bar{X}) \\approx \\frac{\\sigma^2 \\tau}{N}$。这等价于 $N_{eff}$ 个独立样本的均值方差，其中 $N_{eff} = N/\\tau$ 是有效样本量。\n\n现在，我们对给定的自相关函数 $\\rho_k = 0.6^k$（对于 $k \\geq 1$）计算 $\\tau$。我们需要计算无穷级数和：\n$$\n\\sum_{k=1}^{\\infty}\\rho_k = \\sum_{k=1}^{\\infty}0.6^k\n$$\n这是一个首项 $a=0.6$、公比 $r=0.6$ 的几何级数。其和由 $\\frac{a}{1-r}$ 给出：\n$$\n\\sum_{k=1}^{\\infty}0.6^k = \\frac{0.6}{1-0.6} = \\frac{0.6}{0.4} = 1.5\n$$\n将此结果代入 $\\tau$ 的定义中：\n$$\n\\tau = 1 + 2 \\times 1.5 = 1 + 3 = 4\n$$\n积分自相关时间为 $\\tau = 4$。\n\n后验均值估计量 $\\bar{X}$ 的蒙特卡洛标准误差（MCSE）是该估计量的标准差，即 $\\text{MCSE}(\\bar{X}) = \\sqrt{\\text{Var}(\\bar{X})}$。使用我们的大样本表达式和计算出的 $\\tau$ 值：\n$$\n\\text{MCSE}(\\bar{X}) \\approx \\sqrt{\\frac{\\sigma^2 \\tau}{N}} = \\sigma \\sqrt{\\frac{\\tau}{N}}\n$$\n问题提供了从链中估计的经验后验标准差 $\\hat{\\sigma} = 0.8$，我们用它作为 $\\sigma$ 的估计值。我们还已知链长 $N = 20000$。代入这些值：\n$$\n\\text{MCSE} \\approx 0.8\\sqrt{\\frac{4}{20000}} = 0.8\\sqrt{\\frac{1}{5000}} = 0.8 \\times \\frac{1}{\\sqrt{5000}} = \\frac{0.8}{50\\sqrt{2}} = \\frac{0.016}{\\sqrt{2}} = 0.008\\sqrt{2}\n$$\n现在，我们计算数值并四舍五入到四位有效数字。\n$$\n\\text{MCSE} \\approx 0.008 \\times 1.41421356... = 0.011313708...\n$$\n四舍五入到四位有效数字，我们得到 $\\text{MCSE} \\approx 0.01131$。\n问题要求积分自相关时间 $\\tau$ 和蒙特卡洛标准误差 MCSE，四舍五入到四位有效数字。\n对于 $\\tau=4$，表示为四位有效数字是 $4.000$。\n对于 MCSE，其值为 $0.01131$。\n\n最终结果以包含这两个值的行向量形式报告。\n$$\n(\\tau, \\text{MCSE}) = (4.000, 0.01131)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4.000 & 0.01131\n\\end{pmatrix}\n}\n$$", "id": "3289352"}, {"introduction": "分层模型是计算系统生物学的核心，它使我们能够对跨组（如细胞或批次）的共享结构进行建模。基于你衡量 MCMC 性能的能力 ([@problem_id:3289352])，这个动手练习要求你在一个经典的批次效应模型上，实现并比较两种基石级的 MCMC 算法：吉布斯采样 (Gibbs sampling) 和哈密顿蒙特卡洛 (HMC)。通过分析两者的有效样本量，你将亲身体会到后验几何（特别是参数相关性）如何决定采样器的实际性能。", "problem": "您必须在一个代表单细胞测量中批次效应的分层贝叶斯模型上，实现并比较两种马尔可夫链蒙特卡洛 (MCMC) 算法——吉布斯采样 (Gibbs sampling) 和哈密顿蒙特卡洛 (Hamiltonian Monte Carlo, HMC)。该模型针对细胞索引 $i \\in \\{1,\\dots,I\\}$ 和批次索引 $j \\in \\{1,\\dots,J\\}$ 的观测测量值 $y_{ij}$，通过一个具有加性效应的高斯观测模型定义：\n$$\ny_{ij} \\sim \\mathcal{N}\\!\\big(\\theta_i + \\phi_j, \\sigma_y^2\\big),\n$$\n其中 $\\theta_i$ 是潜细胞水平效应，$\\phi_j$ 是潜批次水平效应。先验是独立的高斯分布：\n$$\n\\theta_i \\sim \\mathcal{N}\\!\\big(0, \\sigma_\\theta^2\\big), \\quad \\phi_j \\sim \\mathcal{N}\\!\\big(0, \\sigma_\\phi^2\\big),\n$$\n其超参数 $\\sigma_y^2$、$\\sigma_\\theta^2$ 和 $\\sigma_\\phi^2$ 为已知。\n\n基本依据与假设：\n- 使用贝叶斯定理，根据似然和先验，构建后验密度 $p(\\theta, \\phi \\mid y)$（忽略常数比例因子）。该模型在高斯假设下是完全共轭的，这确保了吉布斯采样的条件分布是易于处理的。\n- 使用两种方法实现马尔可夫链蒙特卡洛：\n  - 从全条件分布推导出的吉布斯采样。\n  - 哈密顿蒙特卡洛 (HMC)，也称为混合蒙特卡洛 (Hybrid Monte Carlo)，它源于连续变量上的哈密顿动力学，使用蛙跳积分器和 Metropolis 接受步骤。\n- 对于哈密顿蒙特卡洛 (HMC)，将哈密顿量定义为 $H(\\mathbf{x}, \\mathbf{p}) = U(\\mathbf{x}) + K(\\mathbf{p})$，其中 $\\mathbf{x} = [\\theta_1,\\dots,\\theta_I,\\phi_1,\\dots,\\phi_J]$，$U(\\mathbf{x}) = -\\log p(\\mathbf{x} \\mid y)$，以及 $K(\\mathbf{p}) = \\frac{1}{2}\\sum_k p_k^2$。每次迭代使用恒定的步长和固定数量的蛙跳步数。本问题不涉及角度。\n- 通过计算每个参数维度在老化期后样本的有效样本量 (ESS) 来量化组水平参数的混合程度，使用公式\n$$\n\\mathrm{ESS} = \\frac{N}{1 + 2 \\sum_{k=1}^{K} \\rho_k},\n$$\n其中 $N$ 是老化期后的样本数量，$\\rho_k$ 是滞后-$k$ 的自相关系数；在第一个非正的 $\\rho_k$ 处截断求和。\n\n算法设计要求：\n- 对于吉布斯采样，推导并使用 $\\theta_i$ 和 $\\phi_j$ 的精确高斯全条件分布。\n- 对于 HMC，推导并使用负对数后验 $U(\\mathbf{x})$ 相对于 $\\theta$ 和 $\\phi$ 各个坐标的梯度，然后运行蛙跳积分器和 Metropolis 接受步骤。\n- 为保证可复现性，使用固定的随机数生成器种子。\n- 在老化期后，分别计算细胞水平参数 $\\{\\theta_i\\}$ 和批次水平参数 $\\{\\phi_j\\}$ 的平均 ESS。对于每个测试用例，报告 HMC 下的平均 ESS 与 Gibbs 下的平均 ESS 之比，此比率需分别针对 $\\theta$ 和 $\\phi$ 计算。\n- 将最终的数值输出表示为四舍五入到三位小数的无量纲浮点数。\n\n测试套件：\n实现您的程序以运行以下三个测试用例。每个用例都指定了数据矩阵 $y \\in \\mathbb{R}^{I \\times J}$ 和超参数。\n\n- 案例 1 (均衡，中等噪声): $I=8$, $J=3$, $\\sigma_y = 0.5$, $\\sigma_\\theta = 1.0$, $\\sigma_\\phi = 1.0$,\n$$\ny = \\begin{bmatrix}\n1.2 & 0.8 & 1.0\\\\\n0.9 & 1.1 & 0.7\\\\\n1.5 & 1.4 & 1.3\\\\\n0.2 & 0.4 & 0.3\\\\\n2.0 & 1.8 & 2.1\\\\\n1.0 & 1.1 & 0.9\\\\\n1.8 & 1.5 & 1.7\\\\\n0.7 & 0.6 & 0.8\n\\end{bmatrix}.\n$$\n\n- 案例 2 (边界情况：单个批次): $I=8$, $J=1$, $\\sigma_y = 0.5$, $\\sigma_\\theta = 1.0$, $\\sigma_\\phi = 1.0$,\n$$\ny = \\begin{bmatrix}\n1.0\\\\\n0.8\\\\\n1.2\\\\\n0.5\\\\\n1.7\\\\\n1.1\\\\\n1.6\\\\\n0.9\n\\end{bmatrix}.\n$$\n\n- 案例 3 (更高噪声，弱批次收缩): $I=12$, $J=4$, $\\sigma_y = 1.0$, $\\sigma_\\theta = 0.5$, $\\sigma_\\phi = 2.0$,\n$$\ny = \\begin{bmatrix}\n1.0 & 1.3 & 0.7 & 1.1\\\\\n0.6 & 0.9 & 0.4 & 0.8\\\\\n1.5 & 1.7 & 1.3 & 1.6\\\\\n0.2 & 0.3 & 0.1 & 0.3\\\\\n2.1 & 2.4 & 1.9 & 2.2\\\\\n0.9 & 1.2 & 0.8 & 1.0\\\\\n1.7 & 1.9 & 1.4 & 1.6\\\\\n0.5 & 0.7 & 0.3 & 0.6\\\\\n1.3 & 1.6 & 1.0 & 1.2\\\\\n0.4 & 0.6 & 0.2 & 0.5\\\\\n2.0 & 2.2 & 1.6 & 2.1\\\\\n1.1 & 1.3 & 0.9 & 1.2\n\\end{bmatrix}.\n$$\n\n实现细节与常量：\n- 每个测试用例的每种方法使用总共 $1200$ 次迭代，其中老化期为 $400$ 次迭代；因此用于 ESS 计算的老化期后样本数 $N = 800$。\n- 两种方法均在所有 $i$ 的 $\\theta_i = 0$ 和所有 $j$ 的 $\\phi_j = 0$ 处进行初始化。\n- 对于 HMC，使用蛙跳步长 $\\epsilon = 0.1 / \\sqrt{I + J}$ 和每次迭代 $L = 25$ 步蛙跳。每次迭代使用独立的标准正态动量。\n- 对所有随机抽样使用固定的伪随机数生成器种子。\n\n要求的最终输出格式：\n您的程序应生成单行输出，包含六个四舍五入到三位小数的浮点数，顺序如下\n$$\n[\\text{ratio}_{\\phi}^{(1)}, \\text{ratio}_{\\theta}^{(1)}, \\text{ratio}_{\\phi}^{(2)}, \\text{ratio}_{\\theta}^{(2)}, \\text{ratio}_{\\phi}^{(3)}, \\text{ratio}_{\\theta}^{(3)}],\n$$\n其中 $\\text{ratio}_{\\phi}^{(k)}$ 是测试用例 $k$ 中 $\\{\\phi_j\\}$ 的平均 ESS 比率（HMC 平均 ESS 除以 Gibbs 平均 ESS），而 $\\text{ratio}_{\\theta}^{(k)}$ 是 $\\{\\theta_i\\}$ 的类似平均 ESS 比率。该行必须精确打印为方括号内由逗号分隔的列表，不得包含多余的空格或文本，例如：\"[0.842,0.917,1.103,0.995,0.761,1.204]\"。", "solution": "用户提供了一个计算统计学中定义明确的问题，要求针对一个特定的分层贝叶斯模型，实现并比较两种马尔可夫链蒙特卡洛 (MCMC) 算法——吉布斯采样和哈密顿蒙特卡洛 (HMC)。\n\n### **问题验证**\n\n-   **已知条件提取**：所有数据 ($y_{ij}$)、模型设定 ($\\mathcal{N}(\\theta_i + \\phi_j, \\sigma_y^2)$)、先验分布 ($\\mathcal{N}(0, \\sigma_\\theta^2)$, $\\mathcal{N}(0, \\sigma_\\phi^2)$)、超参数 ($\\sigma_y, \\sigma_\\theta, \\sigma_\\phi$)、算法参数 ($N_{iter}, N_{burn}, L, \\epsilon$) 以及比较指标 (有效样本量比率) 都已为三个测试用例明确说明。\n-   **验证结论**：问题是 **有效的**。\n    -   它在科学上基于标准的贝叶斯推断和 MCMC 理论。\n    -   它是 **良构的**，为获得唯一、可复现的数值结果提供了所有必要信息。\n    -   它是 **客观的**，所有要求都以精确的数学和算法术语指定。\n    -   问题没有矛盾、歧义或不科学的前提。$J=1$ 的特殊情况代表了一种有效的后验参数相关性场景，这是对 MCMC 方法的一个重要测试。\n\n### **数学推导**\n\n设参数为 $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_I)$ 和 $\\boldsymbol{\\phi} = (\\phi_1, \\dots, \\phi_J)$。数据为 $\\mathbf{y} = \\{y_{ij}\\}$。方差为 $\\sigma_y^2$、$\\sigma_\\theta^2$ 和 $\\sigma_\\phi^2$。\n\n**1. 对数后验分布**\n\n后验分布为 $p(\\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\mathbf{y}) \\propto p(\\mathbf{y} | \\boldsymbol{\\theta}, \\boldsymbol{\\phi}) p(\\boldsymbol{\\theta}) p(\\boldsymbol{\\phi})$。对数后验（忽略一个常数项）为：\n$$\n\\log p(\\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\mathbf{y}) = -\\frac{1}{2\\sigma_y^2} \\sum_{i=1}^I \\sum_{j=1}^J (y_{ij} - \\theta_i - \\phi_j)^2 - \\frac{1}{2\\sigma_\\theta^2} \\sum_{i=1}^I \\theta_i^2 - \\frac{1}{2\\sigma_\\phi^2} \\sum_{j=1}^J \\phi_j^2 + C\n$$\n\n**2. 吉布斯采样：全条件分布**\n\n该模型是共轭的，因此全条件分布是高斯分布。我们通过收集对数后验中的项来推导其参数。\n\n-   **对于 $\\theta_k$**：\n    条件分布 $p(\\theta_k | \\mathbf{y}, \\boldsymbol{\\theta}_{-k}, \\boldsymbol{\\phi})$ 是一个高斯分布 $\\mathcal{N}(M_{\\theta_k}, V_{\\theta_k})$。通过对包含 $\\theta_k$ 的项进行配方，我们可以找到其精度（方差的倒数）和均值。\n    -   精度：$V_{\\theta_k}^{-1} = \\frac{J}{\\sigma_y^2} + \\frac{1}{\\sigma_\\theta^2}$\n    -   方差：$V_{\\theta_k} = \\left(\\frac{J}{\\sigma_y^2} + \\frac{1}{\\sigma_\\theta^2}\\right)^{-1}$\n    -   均值：$M_{\\theta_k} = V_{\\theta_k} \\left(\\frac{1}{\\sigma_y^2} \\sum_{j=1}^J (y_{kj} - \\phi_j)\\right)$\n\n-   **对于 $\\phi_k$**：\n    同样，条件分布 $p(\\phi_k | \\mathbf{y}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi}_{-k})$ 是一个高斯分布 $\\mathcal{N}(M_{\\phi_k}, V_{\\phi_k})$。\n    -   精度：$V_{\\phi_k}^{-1} = \\frac{I}{\\sigma_y^2} + \\frac{1}{\\sigma_\\phi^2}$\n    -   方差：$V_{\\phi_k} = \\left(\\frac{I}{\\sigma_y^2} + \\frac{1}{\\sigma_\\phi^2}\\right)^{-1}$\n    -   均值：$M_{\\phi_k} = V_{\\phi_k} \\left(\\frac{1}{\\sigma_y^2} \\sum_{i=1}^I (y_{ik} - \\theta_i)\\right)$\n\n吉布斯采样器通过为每个参数从这些条件分布中进行迭代抽样来进行。\n\n**3. 哈密顿蒙特卡洛 (HMC)**\n\nHMC 模拟哈密顿动力学以提出新状态。设状态向量为 $\\mathbf{x} = (\\theta_1, \\dots, \\theta_I, \\phi_1, \\dots, \\phi_J)^T$。\n\n-   **势能 ($U(\\mathbf{x})$)**：定义为负对数后验，$U(\\mathbf{x}) = -\\log p(\\mathbf{x} | \\mathbf{y})$。\n    $$\n    U(\\mathbf{x}) = \\frac{1}{2\\sigma_y^2} \\sum_{i=1}^I \\sum_{j=1}^J (y_{ij} - \\theta_i - \\phi_j)^2 + \\frac{1}{2\\sigma_\\theta^2} \\sum_{i=1}^I \\theta_i^2 + \\frac{1}{2\\sigma_\\phi^2} \\sum_{j=1}^J \\phi_j^2\n    $$\n-   **势能的梯度 ($\\nabla U(\\mathbf{x})$)**：蛙跳积分器所需。\n    -   对于 $\\theta_k$：\n        $$ \\frac{\\partial U}{\\partial \\theta_k} = \\frac{1}{\\sigma_y^2} \\sum_{j=1}^J (\\theta_k + \\phi_j - y_{kj}) + \\frac{\\theta_k}{\\sigma_\\theta^2} $$\n    -   对于 $\\phi_k$：\n        $$ \\frac{\\partial U}{\\partial \\phi_k} = \\frac{1}{\\sigma_y^2} \\sum_{i=1}^I (\\theta_i + \\phi_k - y_{ik}) + \\frac{\\phi_k}{\\sigma_\\phi^2} $$\n\n-   **HMC 算法**：\n    1.  抽样动量 $\\mathbf{p} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$。\n    2.  使用步长为 $\\epsilon$ 的蛙跳积分器模拟 $L$ 步动力学过程：\n        -   $\\mathbf{p} \\leftarrow \\mathbf{p} - \\frac{\\epsilon}{2} \\nabla U(\\mathbf{x})$\n        -   对于 $l=1,\\dots,L$：\n            -   $\\mathbf{x} \\leftarrow \\mathbf{x} + \\epsilon \\mathbf{p}$\n            -   $\\mathbf{p} \\leftarrow \\mathbf{p} - \\epsilon \\nabla U(\\mathbf{x})$ (最后半步除外)\n        -   最终的位置/动量更新需要仔细拆分最后一个动量步骤。\n    3.  基于 Metropolis 接受概率 $\\alpha = \\min\\left(1, \\exp\\left(H(\\mathbf{x}, \\mathbf{p}) - H(\\mathbf{x}', \\mathbf{p}')\\right)\\right)$ 接受或拒绝提议的状态 $\\mathbf{x}'$，其中哈密顿量 $H(\\mathbf{x}, \\mathbf{p}) = U(\\mathbf{x}) + \\frac{1}{2}\\mathbf{p}^T\\mathbf{p}$。\n\n**4. 有效样本量 (ESS)**\n\nESS 通过测量与自相关的 MCMC 链等效的独立样本数量来量化采样器的效率。\n$$\n\\mathrm{ESS} = \\frac{N}{1 + 2 \\sum_{k=1}^{K} \\rho_k}\n$$\n这里，$N$ 是老化期后的样本数，$\\rho_k$ 是滞后 $k$ 的自相关。对滞后 $k$ 的求和在第一次出现 $\\rho_k \\le 0$ 时被截断。\n\n### **实现策略**\n\n该解决方案将使用 `numpy` 库在 Python 中实现。带有固定种子的单个随机数生成器将确保可复现性。\n\n1.  一个主函数 `solve` 将协调整个过程。在其内部，将为每个 MCMC 算法和 ESS 计算定义辅助函数。\n2.  将遍历三个测试用例。对于每个用例：\n    -   运行吉布斯采样器，迭代 $1200$ 次，前 $400$ 次作为老化期丢弃。\n    -   运行 HMC 采样器（$L=25$，$\\epsilon=0.1/\\sqrt{I+J}$），迭代次数相同。\n    -   对两种算法中每个参数（$\\theta_i$ 和 $\\phi_j$）的老化期后样本轨迹应用 `ess` 函数。\n    -   分别为 $\\theta$ 参数集和 $\\phi$ 参数集计算平均 ESS。\n    -   为两个参数集计算 HMC 平均 ESS 与 Gibbs 平均 ESS 的比率。\n3.  六个得出的比率将按照问题规范，格式化为三位小数，并以方括号内逗号分隔列表的形式单行打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to implement and compare Gibbs and HMC samplers\n    for a hierarchical Bayesian model.\n    \"\"\"\n\n    # Fix the random seed for reproducibility as required by the problem statement.\n    SEED = 42\n    RNG = np.random.default_rng(SEED)\n\n    def ess(samples):\n        \"\"\"\n        Calculates the Effective Sample Size (ESS) for a single parameter's trace.\n        The formula is ESS = N / (1 + 2 * sum(rho_k)), truncated at the first\n        non-positive autocorrelation.\n        \"\"\"\n        N = len(samples)\n        if N  2:\n            return float(N)\n\n        mean_s = np.mean(samples)\n        # Autocovariance at lag 0 (variance), using a biased estimator (division by N)\n        var_s = np.var(samples, ddof=0)\n        \n        if var_s == 0:\n            return float(N)\n\n        rho_sum = 0.0\n        # Sum autocorrelations until the first non-positive one is encountered\n        for k in range(1, N):\n            # Biased autocovariance estimator\n            c_k = np.sum((samples[:N - k] - mean_s) * (samples[k:] - mean_s)) / N\n            rho_k = c_k / var_s\n            if rho_k = 0:\n                break\n            rho_sum += rho_k\n        \n        ess_val = N / (1.0 + 2.0 * rho_sum)\n        return ess_val\n\n    def run_gibbs(y_data, s_y, s_theta, s_phi, n_iter, burn_in):\n        \"\"\"\n        Runs the Gibbs sampler for the given model.\n        \"\"\"\n        I, J = y_data.shape\n        s2_y, s2_theta, s2_phi = s_y**2, s_theta**2, s_phi**2\n\n        theta = np.zeros(I)\n        phi = np.zeros(J)\n        \n        theta_samples = np.zeros((n_iter, I))\n        phi_samples = np.zeros((n_iter, J))\n\n        for n in range(n_iter):\n            # Update theta_i for all i\n            v_theta_inv = J / s2_y + 1.0 / s2_theta\n            v_theta = 1.0 / v_theta_inv\n            std_theta = np.sqrt(v_theta)\n            for i in range(I):\n                m_theta_term = np.sum(y_data[i, :] - phi) / s2_y\n                m_theta = v_theta * m_theta_term\n                theta[i] = RNG.normal(loc=m_theta, scale=std_theta)\n            \n            # Update phi_j for all j\n            v_phi_inv = I / s2_y + 1.0 / s2_phi\n            v_phi = 1.0 / v_phi_inv\n            std_phi = np.sqrt(v_phi)\n            for j in range(J):\n                m_phi_term = np.sum(y_data[:, j] - theta) / s2_y\n                m_phi = v_phi * m_phi_term\n                phi[j] = RNG.normal(loc=m_phi, scale=std_phi)\n            \n            theta_samples[n, :] = theta.copy()\n            phi_samples[n, :] = phi.copy()\n\n        return theta_samples[burn_in:], phi_samples[burn_in:]\n\n    def run_hmc(y_data, s_y, s_theta, s_phi, n_iter, burn_in, L, epsilon):\n        \"\"\"\n        Runs Hamiltonian Monte Carlo for the given model.\n        \"\"\"\n        I, J = y_data.shape\n        s2_y, s2_theta, s2_phi = s_y**2, s_theta**2, s_phi**2\n        D = I + J\n\n        # State vector x = [theta_1, ..., theta_I, phi_1, ..., phi_J]\n        x = np.zeros(D)\n\n        def potential_energy(theta, phi):\n            residuals = y_data - theta[:, np.newaxis] - phi[np.newaxis, :]\n            log_lik_term = 0.5 * np.sum(residuals**2) / s2_y\n            log_prior_theta = 0.5 * np.sum(theta**2) / s2_theta\n            log_prior_phi = 0.5 * np.sum(phi**2) / s2_phi\n            return log_lik_term + log_prior_theta + log_prior_phi\n\n        def grad_potential(theta, phi):\n            residuals = theta[:, np.newaxis] + phi[np.newaxis, :] - y_data\n            grad_theta = np.sum(residuals, axis=1) / s2_y + theta / s2_theta\n            grad_phi = np.sum(residuals, axis=0) / s2_y + phi / s2_phi\n            return np.concatenate([grad_theta, grad_phi])\n\n        theta_samples = np.zeros((n_iter, I))\n        phi_samples = np.zeros((n_iter, J))\n        \n        for n in range(n_iter):\n            p = RNG.normal(size=D)\n            \n            x_current = x.copy()\n            \n            current_U = potential_energy(x_current[:I], x_current[I:])\n            current_K = 0.5 * np.sum(p**2)\n            \n            # Propose new state using leapfrog integrator\n            x_prop = x_current.copy()\n            p_prop = p.copy()\n            \n            # Initial half-step for momentum\n            grad = grad_potential(x_prop[:I], x_prop[I:])\n            p_prop -= 0.5 * epsilon * grad\n            \n            # L-1 full steps for position and momentum\n            for _ in range(L - 1):\n                x_prop += epsilon * p_prop\n                grad = grad_potential(x_prop[:I], x_prop[I:])\n                p_prop -= epsilon * grad\n            \n            # Final full step for position and half-step for momentum\n            x_prop += epsilon * p_prop\n            grad = grad_potential(x_prop[:I], x_prop[I:])\n            p_prop -= 0.5 * epsilon * grad\n            \n            # Calculate energy of proposed state\n            prop_U = potential_energy(x_prop[:I], x_prop[I:])\n            prop_K = 0.5 * np.sum(p_prop**2)\n            \n            # Metropolis-Hastings acceptance step\n            log_accept_prob = (current_U + current_K) - (prop_U + prop_K)\n            if np.log(RNG.uniform())  log_accept_prob:\n                x = x_prop # Accept proposal\n            \n            # Store the current state (either new or old)\n            theta_samples[n, :] = x[:I]\n            phi_samples[n, :] = x[I:]\n\n        return theta_samples[burn_in:], phi_samples[burn_in:]\n\n    # Define test cases as specified in the problem statement\n    test_cases = [\n        {\n            'I': 8, 'J': 3, 's_y': 0.5, 's_theta': 1.0, 's_phi': 1.0,\n            'y': np.array([\n                [1.2, 0.8, 1.0], [0.9, 1.1, 0.7], [1.5, 1.4, 1.3], [0.2, 0.4, 0.3],\n                [2.0, 1.8, 2.1], [1.0, 1.1, 0.9], [1.8, 1.5, 1.7], [0.7, 0.6, 0.8]\n            ])\n        },\n        {\n            'I': 8, 'J': 1, 's_y': 0.5, 's_theta': 1.0, 's_phi': 1.0,\n            'y': np.array([1.0, 0.8, 1.2, 0.5, 1.7, 1.1, 1.6, 0.9]).reshape(-1, 1)\n        },\n        {\n            'I': 12, 'J': 4, 's_y': 1.0, 's_theta': 0.5, 's_phi': 2.0,\n            'y': np.array([\n                [1.0, 1.3, 0.7, 1.1], [0.6, 0.9, 0.4, 0.8], [1.5, 1.7, 1.3, 1.6],\n                [0.2, 0.3, 0.1, 0.3], [2.1, 2.4, 1.9, 2.2], [0.9, 1.2, 0.8, 1.0],\n                [1.7, 1.9, 1.4, 1.6], [0.5, 0.7, 0.3, 0.6], [1.3, 1.6, 1.0, 1.2],\n                [0.4, 0.6, 0.2, 0.5], [2.0, 2.2, 1.6, 2.1], [1.1, 1.3, 0.9, 1.2]\n            ])\n        }\n    ]\n    \n    # Global algorithm parameters\n    N_ITER = 1200\n    BURN_IN = 400\n    L_HMC = 25\n    \n    final_results = []\n\n    for case in test_cases:\n        y_data = case['y']\n        I, J = case['I'], case['J']\n        s_y, s_theta, s_phi = case['s_y'], case['s_theta'], case['s_phi']\n        \n        # Run Gibbs Sampler\n        theta_gibbs, phi_gibbs = run_gibbs(y_data, s_y, s_theta, s_phi, N_ITER, BURN_IN)\n        \n        # Calculate ESS for Gibbs\n        ess_theta_gibbs = np.mean([ess(theta_gibbs[:, i]) for i in range(I)])\n        ess_phi_gibbs = np.mean([ess(phi_gibbs[:, j]) for j in range(J)]) if J > 0 else 0.0\n\n        # Run HMC Sampler\n        epsilon_hmc = 0.1 / np.sqrt(I + J)\n        theta_hmc, phi_hmc = run_hmc(y_data, s_y, s_theta, s_phi, N_ITER, BURN_IN, L_HMC, epsilon_hmc)\n        \n        # Calculate ESS for HMC\n        ess_theta_hmc = np.mean([ess(theta_hmc[:, i]) for i in range(I)])\n        ess_phi_hmc = np.mean([ess(phi_hmc[:, j]) for j in range(J)]) if J > 0 else 0.0\n        \n        # Calculate ESS ratios (HMC / Gibbs)\n        # Add a small value to the denominator for numerical stability.\n        ratio_denom_eps = 1e-9\n        ratio_phi = ess_phi_hmc / (ess_phi_gibbs + ratio_denom_eps) if J > 0 else 0.0\n        ratio_theta = ess_theta_hmc / (ess_theta_gibbs + ratio_denom_eps)\n\n        # Append results in the specified order: ratio_phi, then ratio_theta\n        final_results.append(f\"{ratio_phi:.3f}\")\n        final_results.append(f\"{ratio_theta:.3f}\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(final_results)}]\")\n\nsolve()\n```", "id": "3289370"}, {"introduction": "在对分层模型进行采样时，参数之间强烈的后验相关性会产生困难的“漏斗”几何结构，从而严重影响 MCMC 采样器的性能。本练习介绍了一种强大的解决方案：重参数化。通过分析中心化和非中心化参数化在不同数据情境下的性能，你将学到一项关键技术，以显著提高 MCMC 效率，特别是在你之前练习中构建的分层模型 ([@problem_id:3289370]) 中。", "problem": "一个实验室正在对 $N$ 种蛋白质进行蛋白质降解建模，蛋白质索引为 $i \\in \\{1,\\ldots,N\\}$。对于每种蛋白质 $i$，其重复实验的降解等待时间 $y_{it}$（其中 $t \\in \\{1,\\ldots,T_i\\}$）被建模为来自速率为 $k_i$ 的指数分布的条件独立样本，即 $y_{it} \\mid k_i \\sim \\text{Exp}(k_i)$。速率 $k_i$ 是一个蛋白质特异性的随机效应，共享一个层级先验：$k_i \\mid \\mu,\\tau \\sim \\mathcal{N}(\\mu,\\tau^2)$，并截断到正实数域以满足 $k_i0$。超先验为 $\\mu \\sim \\mathcal{N}(m_0,s_0^2)$ 和 $\\tau \\sim \\text{Half-Normal}(a)$，其中超参数 $m_0$、$s_0$ 和 $a$ 是固定的。\n\n为了进行马尔可夫链蒙特卡洛（MCMC）推断，考虑了两种等价的参数化方法：\n- 中心化参数化（CP）：直接从 $k_i \\mid \\mu,\\tau \\sim \\mathcal{N}^+(\\mu,\\tau^2)$ 中抽样，其中 $\\mathcal{N}^+$ 表示截断到 $(0,\\infty)$ 的正态分布。\n- 非中心化参数化（NCP）：引入独立的标准正态变量 $\\eta_i \\sim \\mathcal{N}(0,1)$，并设置 $k_i = \\max\\{0,\\ \\mu + \\tau \\eta_i\\}$；在哈密顿蒙特卡洛（HMC）或基于变换的采样器中，这通过一个确定性变换 $k_i = \\mu + \\tau \\eta_i$ 来实现，并强制施加正性约束。\n\n从贝叶斯法则和似然的定义出发，分析中心化参数化（CP）和非中心化参数化（NCP）之间的后验几何和参数依赖性如何随数据信息量和边际尺度 $\\tau$ 的变化而不同。特别地，考虑以下极端情况：\n- 对于每种蛋白质 $i$，重复实验次数 $T_i$ 很大，因此数据对 $k_i$ 的信息量非常大。\n- 对于每种蛋白质 $i$，重复实验次数 $T_i$ 很小，因此数据很弱，并且 $\\tau$ 的后验在 $0$ 附近有显著的质量。\n\n下列哪些陈述是正确的？选择所有适用项。\n\nA. 当大多数 $i$ 的 $T_i$ 很大且 $\\tau$ 的后验集中在较小值附近时，中心化参数化通常对 $(k_i,\\tau)$ 的混合速度更快，因为似然函数强力地识别了每个 $k_i$，从而减少了其对 $\\tau$ 的后验依赖性。\n\nB. 当许多 $i$ 的 $T_i$ 很小且 $\\tau$ 的后验在 $0$ 附近有不可忽略的质量时，中心化参数化会表现出一种“漏斗”几何形状，阻碍了探索，而非中心化参数化则能缓解这种病态问题并改善混合效果。\n\nC. 因为指数似然函数在 $k_i$ 上是对数凹的，所以非中心化参数化在所有数据信息量和 $\\tau$ 的 régimen（状况）下都一致优于中心化参数化。\n\nD. 在非中心化参数化中，当 $\\tau$ 相对于数据中的信息量非常大时，$k_i$ 在后验上变得几乎与 $(\\mu,\\tau)$ 无关，因此与中心化参数化相比，混合效果会系统性地退化。\n\nE. 在任一参数化下，$k_i$ 的全条件分布都是共轭的，并且允许直接进行吉布斯更新，因为指数似然与 $k_i$ 的正态先验族在截断前是匹配的。\n\n请基于指数模型的似然形式、层级先验结构以及重参数化如何改变MCMC采样的后验依赖结构来提供你的推理。假设当 $\\mu \\gg 0$ 且 $\\tau$ 不是极大时，正性截断的影响可以忽略不计，因此关于混合效果的定性结论是由参数依赖性而非边界效应驱动的。", "solution": "用户要求对问题陈述进行严格验证，然后给出详细的解决方案和选项评估。\n\n### 问题验证\n\n**第1步：提取给定信息**\n\n- **模型：** $N$ 种蛋白质的降解，索引为 $i \\in \\{1,\\ldots,N\\}$。\n- **数据：** 每种蛋白质 $i$ 的等待时间 $y_{it}$，其中 $t \\in \\{1,\\ldots,T_i\\}$。\n- **似然：** $y_{it} \\mid k_i \\sim \\text{Exp}(k_i)$，条件独立。速率参数为 $k_i$。\n- **层级先验：** 蛋白质特异性速率 $k_i$ 是来自一个共同分布的随机效应：$k_i \\mid \\mu,\\tau \\sim \\mathcal{N}(\\mu,\\tau^2)$，截断到 $(0,\\infty)$ 以确保 $k_i  0$。记为 $\\mathcal{N}^+(\\mu,\\tau^2)$。\n- **超先验：** $\\mu \\sim \\mathcal{N}(m_0,s_0^2)$ 和 $\\tau \\sim \\text{Half-Normal}(a)$。\n- **超参数：** $m_0$、$s_0$ 和 $a$ 是固定常数。\n- **中心化参数化 (CP)：** 采样器在参数 $(k_1, \\ldots, k_N, \\mu, \\tau)$ 上操作，其中 $k_i$ 的先验直接指定为 $k_i \\mid \\mu,\\tau \\sim \\mathcal{N}^+(\\mu,\\tau^2)$。\n- **非中心化参数化 (NCP)：** 引入独立的标准正态变量 $\\eta_i \\sim \\mathcal{N}(0,1)$。速率通过确定性变换定义为 $k_i = \\mu + \\tau \\eta_i$，并带有正性约束（等价于 $k_i = \\max\\{0, \\mu + \\tau\\eta_i\\}$）。采样器在参数 $(\\eta_1, \\ldots, \\eta_N, \\mu, \\tau)$ 上操作。\n- **分析情景：** 问题要求分析在两种极端情况下的后验几何和MCMC性能：\n    1.  $T_i$ 很大（信息量大的数据）。\n    2.  $T_i$ 很小（弱数据），且 $\\tau$ 的后验在 $0$ 附近有显著质量。\n- **假设：** 正性截断效应对混合效果的定性结论影响可以忽略。\n\n**第2步：使用提取的给定信息进行验证**\n\n- **科学基础：** 问题描述了一个标准的分层贝叶斯模型。使用指数分布来描述等待时间，并为速率参数设置分层结构，是许多科学领域（包括计算系统生物学）中常见且合理的方法。中心化和非中心化参数化的比较是应用贝叶斯统计和MCMC方法的核心主题。该模型在科学和数学上是合理的。\n- **良构性（Well-Posed）：** 问题定义明确。统计模型被完全指定。问题要求对两种不同MCMC参数化在不同数据情景下的性能进行定性分析。这是计算统计学中一个标准且有意义的问题，具有明确的概念性（和经验可验证的）答案。\n- **客观性：** 问题以精确的技术语言陈述。它没有主观性、模糊性和非科学的主张。\n- **缺陷清单检查：**\n    1.  **科学/事实不健全：** 无。统计框架是有效的。\n    2.  **非形式化/不相关：** 问题与分层模型中贝叶斯MCMC的核心概念直接相关。\n    3.  **不完整/矛盾：** 模型和参数化已完整描述。关于截断的简化假设旨在将分析集中在参数依赖性的主要问题上，这是一种标准的教学简化方法。\n    4.  **不切实际/不可行：** 该模型是对具有组特异性参数的系统的现实表示。\n    5.  **病态问题（Ill-Posed）：** 问题不是病态的；MCMC效率的比较是一个定义明确的任务。\n    6.  **伪深刻/琐碎：** 问题不琐碎。CP和NCP之间的选择对MCMC效率有重大的实际影响，理解其根本原因需要对后验几何有扎实的掌握。\n    7.  **超出科学可验证性：** 这些主张可以通过对后验分布的数学分析和经验性MCMC实验来验证。\n\n**第3步：结论与行动**\n\n- **结论：** 问题是有效的。\n- **行动：** 继续进行求解。\n\n### 推导与分析\n\n问题的核心在于理解中心化参数化（CP）和非中心化参数化（NCP）如何影响后验几何，并因此影响像HMC这样的MCMC采样器的效率。联合后验分布（在忽略归一化常数的情况下）由贝叶斯法则给出：\n$$ p(\\mathbf{k}, \\mu, \\tau \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\mathbf{k}) p(\\mathbf{k} \\mid \\mu, \\tau) p(\\mu) p(\\tau) $$\n似然项为 $p(\\mathbf{y} \\mid \\mathbf{k}) = \\prod_{i=1}^N p(\\mathbf{y}_i \\mid k_i)$，其中 $p(\\mathbf{y}_i \\mid k_i) = \\prod_{t=1}^{T_i} k_i e^{-k_i y_{it}} = k_i^{T_i} \\exp(-k_i S_i)$，且 $S_i = \\sum_{t=1}^{T_i} y_{it}$。\n\n**中心化参数化 (CP)**\n在CP中，MCMC采样器探索参数 $(\\mathbf{k}, \\mu, \\tau)$ 的空间。先验 $p(\\mathbf{k} \\mid \\mu, \\tau) = \\prod_i \\mathcal{N}^+(k_i; \\mu, \\tau^2)$ 在个体速率 $k_i$ 和超参数 $(\\mu, \\tau)$ 之间引入了直接且强烈的依赖关系。\n\n-   **低信息量情景（$T_i$ 小，$\\tau \\to 0$）：** 当数据较弱时，似然项是宽泛的，不能强烈约束 $k_i$ 的值。后验主要受先验影响。当 $\\tau$ 的后验趋近于 $0$ 时， $k_i$ 的先验变为 $k_i \\mid \\mu, \\tau \\to \\delta(\\mu)$，即一个在 $\\mu$ 处的点质量。这意味着所有 $k_i$ 都被迫几乎等于 $\\mu$。在 $(k_i, \\tau)$ 的联合后验空间中，这会产生一个典型的“漏斗”形状。对于较大的 $\\tau$，$k_i$ 可以在很大范围内变化，但随着 $\\tau \\to 0$，$k_i$ 的允许范围会急剧缩小。基于梯度的采样器（如HMC）难以调整其步长以有效地探索这个漏斗的宽口和窄颈，导致混合效果差。\n\n-   **高信息量情景（$T_i$ 大）：** 当数据很强时，似然项 $k_i^{T_i} e^{-k_i S_i}$ 在 $k_i$ 的最大似然估计附近非常尖锐。这种强数据信息“覆盖”了先验的影响。每个 $k_i$ 的后验被其各自的数据“钉住”，使其在很大程度上独立于其他 $k_j$ 的后验以及超参数 $(\\mu, \\tau)$ 的后验。这种后验中参数的有效解耦使得采样变得高效。采样器可以更新 $k_i$ 和 $(\\mu, \\tau)$ 而无需进行高度相关的移动。\n\n**非中心化参数化 (NCP)**\n在NCP中，我们通过引入标准正态偏离量 $\\eta_i \\sim \\mathcal{N}(0,1)$ 并设置 $k_i = \\mu + \\tau\\eta_i$ 来进行重参数化。采样器探索 $(\\boldsymbol{\\eta}, \\mu, \\tau)$ 的空间。关键变化是，被采样参数的先验分布现在是独立的：$p(\\boldsymbol{\\eta}, \\mu, \\tau) = p(\\boldsymbol{\\eta}) p(\\mu) p(\\tau)$。依赖性被转移到了似然项中，该似然项现在是所有三种类型参数的函数：$p(\\mathbf{y} \\mid \\boldsymbol{\\eta}, \\mu, \\tau) = \\prod_i (\\mu+\\tau\\eta_i)^{T_i} \\exp(-(\\mu+\\tau\\eta_i)S_i)$。\n\n-   **低信息量情景（$T_i$ 小，$\\tau \\to 0$）：** 由于似然较弱，后验由先验主导。在NCP空间中，先验是因子化的，意味着其几何形状是简单的（超矩形）。不存在漏斗，因为 $\\eta_i$ 的先验是 $\\mathcal{N}(0,1)$，它独立于 $\\tau$。采样器可以轻松地探索较小的 $\\tau$ 值，而对 $\\eta_i$ 的空间没有任何相应的约束。因此，NCP缓解了漏斗病态问题，在这种情景下非常有效。\n\n-   **高信息量情景（$T_i$ 大）：** 当数据很强时，似然会强制条件 $k_i \\approx \\hat{k}_i$，其中 $\\hat{k}_i$ 是数据支持的估计值。在NCP空间中，这转化为一个强约束 $\\mu + \\tau\\eta_i \\approx \\hat{k}_i$。这在被采样的参数 $\\eta_i, \\mu, \\tau$ 之间引入了强烈的非线性后验相关性。例如，$\\tau$ 的增加必须伴随着 $\\eta_i$ 的特定减少，以保持 $\\mu + \\tau\\eta_i$ 恒定。从这样一个弯曲、受约束的流形中采样是困难的，会降低MCMC效率。在这种情况下，CP通常更优。\n\n### 逐项分析\n\n**A. 当大多数 $i$ 的 $T_i$ 很大且 $\\tau$ 的后验集中在较小值附近时，中心化参数化通常对 $(k_i,\\tau)$ 的混合速度更快，因为似然函数强力地识别了每个 $k_i$，从而减少了其对 $\\tau$ 的后验依赖性。**\n此陈述描述了高信息量情景。如上文分析，当重复次数 $T_i$ 很大时，每个 $k_i$ 的似然函数信息量非常大且非常尖锐。这种强数据信息有效地决定了每个 $k_i$ 的后验，从而削弱了在中心化参数化中由先验施加的 $k_i$ 和超参数 $\\tau$ 之间的后验依赖性。后验中参数的这种解耦使得CP变得高效。所提供的推理完全正确。\n**结论：正确。**\n\n**B. 当许多 $i$ 的 $T_i$ 很小且 $\\tau$ 的后验在 $0$ 附近有不可忽略的质量时，中心化参数化会表现出一种“漏斗”几何形状，阻碍了探索，而非中心化参数化则能缓解这种病态问题并改善混合效果。**\n此陈述描述了NCP表现出色的经典低信息量情景。在弱数据（$T_i$ 小）的情况下，后验严重受到先验结构的影响。在CP中，耦合 $k_i \\sim \\mathcal{N}^+(\\mu, \\tau^2)$ 在 $\\tau \\to 0$ 时会产生一个后验漏斗，这对于MCMC采样器来说是出了名的困难。NCP通过采样独立参数 $\\eta_i$ 并定义 $k_i = \\mu + \\tau\\eta_i$，打破了这种先验依赖性。在 $(\\eta_i, \\tau)$ 空间中的先验几何是良性的，消除了漏斗，并允许更有效的探索。\n**结论：正确。**\n\n**C. 因为指数似然函数在 $k_i$ 上是对数凹的，所以非中心化参数化在所有数据信息量和 $\\tau$ 的 régimen（状况）下都一致优于中心化参数化。**\n对数似然为 $\\log L(k_i) = T_i \\log k_i - S_i k_i$。其关于 $k_i$ 的二阶导数为 $\\frac{d^2}{dk_i^2} \\log L(k_i) = -T_i/k_i^2$，对于 $k_i  0$ 恒为负。因此，对数似然确实是凹的。然而，这个属性虽然对采样有利，但并不能解决分层模型中参数间的依赖问题。正如在对选项A和B的分析中所确立的，没有哪种参数化是“一致优越”的。CP通常更适合高信息量情景，而NCP更适合低信息量情景。声称一致优越是错误的。\n**结论：不正确。**\n\n**D. 在非中心化参数化中，当 $\\tau$ 相对于数据中的信息量非常大时，$k_i$ 在后验上变得几乎与 $(\\mu,\\tau)$ 无关，因此与中心化参数化相比，混合效果会系统性地退化。**\n此陈述讨论的是先验方差 $\\tau^2$ 很大，使得 $k_i$ 的先验是弥散的情景。在这种情况下， $k_i$ 的后验主要由似然决定，非常类似于高数据信息量的情况（选项A）。因此，CP表现良好，而NCP由于在 $\\eta_i, \\mu, \\tau$ 之间引入了后验相关性而表现不佳。所以，NCP混合效果“与中心化参数化相比...退化”的结论是正确的。然而，提供的推理是有缺陷的。短语“$k_i$ 在后验上变得几乎与 $(\\mu,\\tau)$ 无关”描述的是最终后验分布的一个属性。在直接采样 $k_i$ 的CP中，这种弱依赖性是有利的。在NCP中，为了达到同样的目标后验，却在被采样的参数 $(\\eta_i, \\mu, \\tau)$ 之间引入了强依赖性，这恰恰是混合效果退化的*原因*。该陈述错误地将此属性呈现为NCP采样空间本身的特征，并且因果关系被错误地表述了。考虑到科学正确性的严格标准，这种有缺陷的推理使得整个陈述无效。\n**结论：不正确。**\n\n**E. 在任一参数化下，$k_i$ 的全条件分布都是共轭的，并且允许直接进行吉布斯更新，因为指数似然与 $k_i$ 的正态先验族在截断前是匹配的。**\n在CP中，$k_i$ 的全条件分布与似然和先验的乘积成正比：$p(k_i \\mid \\text{rest}) \\propto [k_i^{T_i} e^{-k_i S_i}] \\times [\\exp(-\\frac{(k_i-\\mu)^2}{2\\tau^2})]$。似然项是伽马分布的核。先验项是正态分布的核。伽马分布和正态分布不是共轭对。它们的乘积不会产生一个可以轻松采样的标准分布。因此，直接吉布斯采样是不可行的。断言“指数似然与...正态先验族...是匹配的”从根本上是错误的。\n**结论：不正确。**", "answer": "$$\\boxed{AB}$$", "id": "3289393"}]}