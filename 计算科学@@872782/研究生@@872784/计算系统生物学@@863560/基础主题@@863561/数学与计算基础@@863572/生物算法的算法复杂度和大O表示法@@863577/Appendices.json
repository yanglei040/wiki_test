{"hands_on_practices": [{"introduction": "生物信息学中的许多问题，如使用隐马尔可夫模型（HMM）进行基因注释，需要在指数级巨大的搜索空间中找到最优解。动态规划，例如维特比（Viterbi）算法，能够高效地（在多项式时间内）找到精确解。然而，对于非常大的模型，即使是多项式时间也可能过慢，而像集束搜索（beam search）这样的启发式算法则提供了一种实用的权衡，通过牺牲最优性保证来换取显著的速度提升。这项练习 [@problem_id:3288378] 让你能够定量地分析精确算法与启发式近似之间的经典权衡，你将推导两种方法的计算成本，并探索如何调整启发式算法的参数以平衡准确性与性能，这是应用计算生物学中的一项关键技能。", "problem": "考虑一个用于基因注释的隐马尔可夫模型 (HMM)，其具有 $S$ 个隐状态和一个长度为 $T$ 的基因组观测序列。HMM 由状态的初始分布、一个转移概率矩阵和多个发射概率分布定义。Viterbi 解码通过对所有状态转移进行动态规划，计算给定观测值下的最可能隐状态序列。根据大O表示法的基础定义，算法成本通过作为输入规模函数的基本算术或比较操作的数量来衡量。在一个全连接HMM中，在每个时间步 $t \\in \\{1,\\dots,T\\}$，针对每个目标状态的 Viterbi 递推会考虑所有 $S$ 个可能的前驱状态，从而导致渐进成本与 $O(T S^2)$ 成正比。集束搜索是一种启发式方法，它在每个时间步通过仅保留按部分路径得分排序的前 $b$ 个状态来对状态空间进行剪枝；转移仅从该集束评估到所有 $S$ 个目标状态，从而产生与 $O(T b S)$ 成正比的成本，当 $b$ 与 $S$ 无关且有界时，该成本简化为 $O(T S)$。\n\n为保持解码准确率，剪枝后的集束在每一步都必须包含最优路径上的前驱状态。使用一个基于顺序统计量的概率排序模型将准确率形式化：设 $R_t$ 为在时间 $t$ 导致全局最优 Viterbi 路径的前驱状态的随机排名（在 $S$ 个状态中，排名 1 为最佳）。假设排名 $R_t$ 在所有时间步 $t$ 上是独立同分布的 (i.i.d.)，并满足一个由 $p \\in (0,1)$ 参数化的几何尾部边界，即对于整数 $r \\ge 0$，$\\mathbb{P}(R_t  r) = (1-p)^r$，并在 $S$ 处截断。这个尾部边界与 HMM 解码中广泛使用的得分差距模型一致，在这些模型中，一个给定的竞争者优于最优前驱的概率随其排名的增加而指数衰减。在此模型下，最优前驱在单一步骤中位于前 $b$ 名的概率是 $\\mathbb{P}(R_t \\le b) = 1 - (1-p)^b$。假设在 $T$ 个步骤中是独立的，最优路径在所有 $T$ 个步骤的剪枝过程中存活的概率是 $\\left(1 - (1-p)^b\\right)^T$。\n\n你的任务是：\n- 从动态规划递推式和大O表示法的定义出发，根据第一性原理和定义推导 Viterbi 算法的渐进成本 $O(T S^2)$ 和集束搜索的渐进成本 $O(T b S)$。\n- 使用上述概率排序模型，推导最小整数集束宽度 $b(T,S,p,\\varepsilon)$，该宽度需保证 $\\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon$，其中 $\\varepsilon \\in (0,1)$ 是指定的准确率容忍度（以小数而非百分比表示），并受约束 $b \\le S$ 的限制。明确说明当无约束的最小整数超过 $S$ 时如何选择 $b$。\n- 实现一个程序，对于以下参数值 $(T,S,p,\\varepsilon)$ 的测试套件，计算每种情况下的 $b$，并为每种情况返回一个包含所选 $b$ 和一个布尔值的对。该布尔值指示是否同时满足以下两个条件：准确率得到保证（即 $\\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon$）以及渐进成本相对于 $O(T S^2)$ 有所降低（即 $b  S$，使得 $O(T b S) = o(T S^2)$）：\n    1. $T = 1000$, $S = 50$, $p = 0.3$, $\\varepsilon = 0.01$。\n    2. $T = 1$, $S = 1000$, $p = 0.3$, $\\varepsilon = 0.01$。\n    3. $T = 10000$, $S = 10000$, $p = 0.1$, $\\varepsilon = 0.1$。\n    4. $T = 500$, $S = 200$, $p = 0.25$, $\\varepsilon = 0.000001$。\n    5. $T = 1000$, $S = 500$, $p = 0.01$, $\\varepsilon = 0.01$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，列表中的每个元素本身是一个形式为 $[b,\\text{boolean}]$ 的双元素列表。例如，一个有效的输出格式是 $[[b_1,\\text{boolean}_1],[b_2,\\text{boolean}_2],\\dots]$。本问题不涉及物理单位。不使用角度。所有概率（如 $\\varepsilon$）必须以小数形式提供和解释。", "solution": "该问题被评估为有效，因为它在科学上基于算法分析和概率建模的既定原则，提法明确，客观，并包含获得唯一解所需的所有必要信息。\n\n### 第 1 部分：渐进成本的推导\n\n#### Viterbi 算法复杂度：$O(T S^2)$\n\nViterbi 算法用于在给定观测序列的情况下，找到最可能的隐状态序列。它是一种动态规划算法。令 $S$ 为隐状态的数量，$T$ 为观测序列的长度。令 $\\delta_t(j)$ 为长度为 $t$ 且结束于状态 $j$ 的最可能路径的概率。该算法的核心是以下递推关系：\n$$ \\delta_t(j) = \\left( \\max_{i=1,\\dots,S} \\left[ \\delta_{t-1}(i) \\cdot a_{ij} \\right] \\right) \\cdot e_j(o_t) \\quad \\text{for } j=1,\\dots,S \\text{ and } t=1,\\dots,T $$\n其中 $a_{ij}$ 是从状态 $i$ 到状态 $j$ 的转移概率，而 $e_j(o_t)$ 是从状态 $j$ 发射观测值 $o_t$ 的概率。\n\n为了分析计算成本，我们计算基本算术操作的数量。为了数值稳定性，计算通常在对数空间中执行，此时乘法变为加法。递推关系变为：\n$$ \\log \\delta_t(j) = \\left( \\max_{i=1,\\dots,S} \\left[ \\log \\delta_{t-1}(i) + \\log a_{ij} \\right] \\right) + \\log e_j(o_t) $$\n\n让我们分析单个时间步 $t$ 的成本：\n1.  为了计算单个目标状态 $j$ 的值，我们必须为每个可能的前驱状态 $i \\in \\{1, \\dots, S\\}$ 评估 $\\max$ 算子内的表达式。\n2.  对于每个 $i$，这涉及一次加法：$\\log \\delta_{t-1}(i) + \\log a_{ij}$。对于一个固定的 $j$，此操作将执行 $S$ 次。\n3.  然后，我们必须找到这 $S$ 个值的最大值。这需要 $S-1$ 次比较。\n4.  最后，再执行一次加法，以加上对数发射概率。\n5.  因此，对于每个目标状态 $j$，操作数量与 $S$（加法）加上 $S-1$（比较）成正比，即 $O(S)$。\n\n由于这个计算必须对所有 $S$ 个目标状态（$j=1, \\dots, S$）执行，因此单个时间步 $t$ 的总成本是 $S \\times O(S) = O(S^2)$。\n\n这个过程对从 $t=1$ 到 $t=T$ 的每个时间步重复进行。根据大O表示法的定义，它描述了当输入规模（$T, S$）增长时的极限行为，总渐进成本是每个时间步的成本乘以时间步的数量。\n$$ \\text{Total Cost} = T \\times O(S^2) = O(T S^2) $$\n\n#### 集束搜索算法复杂度：$O(T b S)$\n\n集束搜索是一种通过剪枝搜索空间来降低复杂度的启发式方法。在每个时间步，它只保留 $b$ 个最有利状态的“集束”，其中 $b$ 是集束宽度。\n\n设 $\\mathcal{B}_{t-1}$ 为在时间 $t-1$ 时具有最高得分 $\\delta_{t-1}(i)$ 的 $b$ 个状态的集合。递推关系被修改为只考虑来自此集束内状态的转移：\n$$ \\log \\delta_t(j) = \\left( \\max_{i \\in \\mathcal{B}_{t-1}} \\left[ \\log \\delta_{t-1}(i) + \\log a_{ij} \\right] \\right) + \\log e_j(o_t) $$\n\n让我们分析单个时间步 $t$ 的成本：\n1.  为了计算单个目标状态 $j$ 的得分，我们现在只对集束 $\\mathcal{B}_{t-1}$ 中的 $b$ 个状态进行迭代。\n2.  这涉及 $b$ 次加法和找到 $b$ 个值的最大值（需要 $b-1$ 次比较）。因此，每个目标状态 $j$ 的成本是 $O(b)$。\n3.  这个计算对所有 $S$ 个可能的目标状态执行，因此计算时间 $t$ 的所有候选得分的成本是 $S \\times O(b) = O(bS)$。\n4.  计算完 $S$ 个新得分后，我们必须识别出前 $b$ 个状态以形成下一个集束 $\\mathcal{B}_t$。这可以使用线性时间选择算法（例如 Quickselect）在 $O(S)$ 时间内高效完成，以找到第 $b$ 大的得分，然后进行筛选。\n5.  一个时间步的总成本是这两个步骤的总和：$O(bS) + O(S)$。由于 $b \\ge 1$，这可以简化为 $O(bS)$。\n\n这个过程对所有 $T$ 个时间步重复进行。总渐进成本为：\n$$ \\text{Total Cost} = T \\times O(bS) = O(T b S) $$\n当 $b  S$ 时，$O(T b S)$ 是对 $O(T S^2)$ 的渐进改进。形式上，如果 $b  S$，那么 $T b S = o(T S^2)$。\n\n### 第 2 部分：最小集束宽度 $b$ 的推导\n\n我们面临的要求是，最优路径在所有 $T$ 个步骤的剪枝中存活的概率必须至少为 $1-\\varepsilon$：\n$$ \\left(1 - (1-p)^b\\right)^T \\ge 1 - \\varepsilon $$\n其中 $p \\in (0,1)$ 是排序模型的参数，$b$ 是集束宽度，而 $\\varepsilon \\in (0,1)$ 是准确率容忍度。我们需要找到满足此不等式的最小整数 $b$，并受约束 $b \\le S$ 的限制。\n\n1.  对两边取 $T$ 次方根。由于两边都为正，不等式方向保持不变。\n    $$ 1 - (1-p)^b \\ge (1 - \\varepsilon)^{1/T} $$\n2.  整理各项以分离出含 $b$ 的项：\n    $$ (1-p)^b \\le 1 - (1 - \\varepsilon)^{1/T} $$\n3.  对两边取对数。由于 $p \\in (0,1)$，我们有 $0  1-p  1$，这意味着 $\\log(1-p)$ 是负数。因此，当我们取对数并在之后除以 $\\log(1-p)$ 时，必须反转不等号。\n    $$ b \\log(1-p) \\le \\log\\left(1 - (1 - \\varepsilon)^{1/T}\\right) $$\n4.  除以 $\\log(1-p)$ 并反转不等式：\n    $$ b \\ge \\frac{\\log\\left(1 - (1 - \\varepsilon)^{1/T}\\right)}{\\log(1-p)} $$\n    对数的底可以是任何大于 1 的值；自然对数是标准选择。\n\n由于 $b$ 必须是整数，最小的无约束整数集束宽度（我们称之为 $b_{req}$）是满足此条件的最小整数，由上取整函数给出：\n$$ b_{req} = \\left\\lceil \\frac{\\ln\\left(1 - (1 - \\varepsilon)^{1/T}\\right)}{\\ln(1-p)} \\right\\rceil $$\n\n问题施加了物理约束，即集束宽度不能超过状态总数，即 $b \\le S$。因此，选择的集束宽度 $b_{chosen}$ 必须是：\n$$ b_{chosen} = \\min(S, b_{req}) $$\n\n如果计算出的所需集束宽度 $b_{req}$ 超过了状态数 $S$，我们被迫选择 $b_{chosen} = S$。在这种情况下，所选的集束宽度 $b_{chosen}$ 小于所需的宽度 $b_{req}$，因此准确率保证 $\\left(1 - (1-p)^{b_{chosen}}\\right)^T \\ge 1 - \\varepsilon$ 将无法得到满足。此外，由于 $b_{chosen} = S$，与完整的 Viterbi 算法相比，没有渐进成本上的降低（$O(TBS) = O(TS^2)$）。\n\n### 第 3 部分：实现逻辑\n\n对于每个测试用例 $(T, S, p, \\varepsilon)$：\n1.  使用推导出的公式计算所需的集束宽度 $b_{req}$。对于小 $\\varepsilon$ 的数值稳定性，表达式 $(1-\\varepsilon)^{1/T}$ 可以计算为 $\\exp(\\frac{1}{T}\\ln(1-\\varepsilon))$。项 $1 - \\exp(\\dots)$ 和 $\\ln(1-p)$ 最好分别使用 `expm1` 和 `log1p` 函数来计算，以保持接近 0 的参数的精度。\n    $$ b_{req} = \\left\\lceil \\frac{\\ln(-\\text{expm1}(\\frac{1}{T}\\ln(1-\\varepsilon)))}{\\ln(1-p)} \\right\\rceil = \\left\\lceil \\frac{\\ln(-\\text{expm1}(\\frac{\\text{log1p}(-\\varepsilon)}{T}))}{\\text{log1p}(-p)} \\right\\rceil$$\n2.  确定所选的集束宽度 $b_{chosen} = \\min(S, b_{req})$。\n3.  评估布尔标志的两个条件：\n    a.  **准确率保证**：当且仅当所选的集束宽度足够时，即 $b_{chosen} \\ge b_{req}$，准确率才能得到保证。这等价于检查是否 $S \\ge b_{req}$。\n    b.  **渐进成本降低**：如果 $b_{chosen}  S$，则成本降低。\n4.  如果两个条件都满足，最终的布尔值为 `True`，否则为 `False`。这可以简化为单个条件 $b_{req}  S$。如果 $b_{req}  S$，我们选择 $b_{chosen} = b_{req}$，这样既满足 $b_{chosen}  S$ 又满足 $b_{chosen} \\ge b_{req}$。如果 $b_{req} \\ge S$，则不可能同时满足这两个条件。\n5.  返回数据对 $[b_{chosen}, \\text{boolean}]$。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the beam search parameter problem for a suite of test cases.\n    For each case, it computes the minimal beam width 'b' that guarantees\n    a certain accuracy and determines if this choice also reduces\n    asymptotic complexity relative to the Viterbi algorithm.\n    \"\"\"\n\n    # Test suite of parameters (T, S, p, epsilon)\n    test_cases = [\n        (1000, 50, 0.3, 0.01),\n        (1, 1000, 0.3, 0.01),\n        (10000, 10000, 0.1, 0.1),\n        (500, 200, 0.25, 0.000001),\n        (1000, 500, 0.01, 0.01),\n    ]\n\n    results = []\n    for T, S, p, epsilon in test_cases:\n        # Using numerically stable functions for calculations involving\n        # numbers close to 0 or 1.\n        # log(1-x) => log1p(-x)\n        # exp(x)-1 => expm1(x)\n        #\n        # Derivation:\n        # (1 - (1-p)^b)^T >= 1 - epsilon\n        # 1 - (1-p)^b >= (1 - epsilon)^(1/T)\n        # (1-p)^b = 1 - (1 - epsilon)^(1/T)\n        # b * log(1-p) = log(1 - (1-epsilon)^(1/T))\n        # b >= log(1 - (1-epsilon)^(1/T)) / log(1-p)  (log(1-p) is negative)\n        \n        # (1-epsilon)^(1/T) = exp(log((1-epsilon)^(1/T))) = exp( (1/T) * log(1-epsilon) )\n        # Using log1p for log(1-epsilon) and expm1 for exp(x)-1:\n        # log(1 - exp( (1/T) * log1p(-epsilon) ))\n        # let term_in_log = -expm1((1/T) * log1p(-epsilon))\n        try:\n            log_numerator_arg = -math.expm1(math.log1p(-epsilon) / T)\n            \n            # Handle potential domain error if log_numerator_arg is not positive.\n            # This can happen if T is extremely large and epsilon is close to 1,\n            # but is unlikely with the given test cases.\n            if log_numerator_arg = 0:\n                # This case implies an effectively infinite required beam width,\n                # which is unrealistic but we handle it.\n                b_required_float = float('inf')\n            else:\n                numerator = math.log(log_numerator_arg)\n                denominator = math.log1p(-p)\n                b_required_float = numerator / denominator\n\n        except (ValueError, OverflowError):\n            b_required_float = float('inf')\n\n        # The required beam width must be an integer, so we take the ceiling.\n        if math.isinf(b_required_float) or math.isnan(b_required_float):\n            b_required = S + 1 # Effectively infinite, force to be > S\n        else:\n            b_required = int(math.ceil(b_required_float))\n\n        # The chosen beam width is constrained by the number of states S.\n        b_chosen = min(S, b_required)\n\n        # The accuracy is guaranteed if the chosen b is at least the required b.\n        # This is equivalent to S being large enough (S >= b_required).\n        is_accuracy_guaranteed = (b_chosen >= b_required)\n\n        # The asymptotic cost is reduced if b_chosen  S.\n        is_cost_reduced = (b_chosen  S)\n\n        # The final boolean flag is True only if BOTH conditions are met.\n        success_flag = is_accuracy_guaranteed and is_cost_reduced\n\n        # Append the pair [chosen_b, boolean_flag] to results.\n        results.append([b_chosen, success_flag])\n    \n    # Format the output as a string representing a list of lists.\n    # str() on a Python list [val, bool] produces '[val, True]' or '[val, False]',\n    # which matches the required output format style.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3288378"}, {"introduction": "现代生物学数据的海量规模，例如来自单细胞测序的数据，常常使得将整个数据集加载到内存中进行处理变得不切实际，这催生了对流式算法（streaming algorithms）的需求。流式算法在单次遍历中顺序处理数据，同时只使用有限的内存，通常是亚线性甚至常数级别的空间。蓄水池抽样（reservoir sampling）是一种经典技术，它能够从一个未知长度的数据流中维护一个均匀随机样本。这项练习 [@problem_id:3288334] 提供了流式计算模型的实践经验，这与传统的批处理模式截然不同。你将分析一个基于蓄水池样本构建的估计器的统计特性，推导其偏差和期望误差，从而将算法的空间复杂度与基础统计理论联系起来。", "problem": "设计并分析一个流式算法，该算法使用恒定内存来估计单细胞核糖核酸测序 (scRNA-seq) 读段中目标 $k$-mer 的比例。每个读段都被在线处理，且一个读段要么包含目标 $k$-mer，要么不包含。将该数据流建模为一个独立的指示变量序列 $\\{X_t\\}_{t=1}^n$，其中如果第 $t$ 个读段包含目标 $k$-mer，则 $X_t \\in \\{0,1\\}$ 等于 $1$，否则等于 $0$。令真实但未知的比例为 $p = \\frac{1}{n}\\sum_{t=1}^n X_t$。您的算法必须使用一个大小固定为 $m$ 且不依赖于 $n$ 的均匀蓄水池样本（因此总内存按大O表示法为 $O(1)$），并在处理完前 $n$ 个读段后，生成 $p$ 的一个估计量 $\\hat{p}$。\n\n约束与目标：\n- 您必须使用一个大小为 $m$ 的蓄水池，该蓄水池通过经典蓄水池抽样进行维护。对于在时间 $t$ ($t \\ge 1$) 到达的每个项目，算法确保在处理完 $t$ 个项目后，蓄水池包含一个从迄今为止所见项目中均匀随机抽取的、大小为 $\\min\\{m,t\\}$ 的子集。\n- 估计量必须是蓄水池中指示变量的样本均值，即 $\\hat{p} = \\frac{1}{m}\\sum_{i=1}^{m} Y_i$，其中 $Y_i \\in \\{0,1\\}$ 表示在终止时间 $n$，蓄水池中的第 $i$ 个条目是否包含目标 $k$-mer（此为 $n \\ge m$ 的情况；否则，均值应基于实际蓄水池大小计算）。\n- 证明 $\\mathbb{E}[\\hat{p}] = p$，并推导期望平方误差 $\\mathbb{E}\\big[(\\hat{p}-p)^2\\big]$ 作为 $n$、$m$ 和 $p$ 的函数。最终公式应仅使用 $n$、$m$ 和 $p$ 以最简形式表示。说明您的表达式在何种关于 $n$ 和 $m$ 的条件下有效。\n- 使用大O表示法分析算法的时间和空间复杂度，并证明在 $m$ 固定的情况下，空间复杂度相对于 $n$ 为 $O(1)$。\n\n实现任务：\n- 实现一个程序，在给定一个由 $(n,m,p)$ 元组组成的测试套件的情况下，计算上述蓄水池抽样模型下的理论期望平方误差 $\\mathbb{E}\\big[(\\hat{p}-p)^2\\big]$。除非另有明确说明，否则假设 $n \\ge 2$ 且 $1 \\le m \\le n$，并将 $p$ 解读为 $[0,1]$ 范围内的小数（而不是百分比）。您的程序不应进行模拟，而应计算您推导出的封闭形式表达式。\n\n测试套件：\n- 完全使用以下参数元组 $(n,m,p)$：\n  - $(n=1000000, m=5, p=0.3)$\n  - $(n=1000, m=1, p=0.2)$\n  - $(n=50, m=5, p=0.5)$\n  - $(n=10, m=3, p=0)$\n  - $(n=10, m=3, p=1)$\n  - $(n=7, m=7, p=0.35)$\n  - $(n=2, m=1, p=0.4)$\n\n答案规格：\n- 对于每个测试用例，将期望平方误差作为浮点数输出，并精确到10位小数。\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，例如 $\\texttt{[0.1234000000,0.0000000000]}$。", "solution": "所述问题在形式上是合理的且易于处理的。它在概率论和算法分析方面有科学依据，特别是在流式算法和有限总体抽样领域。这是一个适定的问题，提供了所有必要信息和明确的目标。其中没有矛盾、事实错误或主观因素。因此，可以构建一个形式化的解决方案。\n\n问题的核心是分析从蓄水池抽样中导出的估计量的统计特性。我们将输入流建模为包含 $n$ 个指示变量 $\\{X_t\\}_{t=1}^n$ 的有限总体，其中 $X_t \\in \\{0, 1\\}$。成功的总体比例（包含目标 $k$-mer 的读段）为 $p = \\frac{1}{n}\\sum_{t=1}^n X_t$。在 $n \\ge m$ 的条件下，蓄水池抽样算法从该总体中进行一次大小为 $m$ 的无放回简单随机抽样。设该样本表示为 $\\{Y_i\\}_{i=1}^m$。如果 $n  m$，蓄水池则简单地包含流中的所有 $n$ 个项目。\n\n$p$ 的估计量是最终蓄水池中元素的样本均值。\n如果 $n \\ge m$，蓄水池大小为 $m$，估计量为 $\\hat{p} = \\frac{1}{m}\\sum_{i=1}^m Y_i$。\n如果 $n  m$，蓄水池大小为 $n$，估计量为 $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n Y_i$。在这种情况下，蓄水池包含所有流元素，因此对于所有 $i \\in \\{1, \\dots, n\\}$ 都有 $Y_i = X_i$。这立即得出 $\\hat{p} = \\frac{1}{n}\\sum_{i=1}^n X_i = p$。估计是精确的，所以误差 $\\hat{p}-p$ 为 $0$，期望平方误差为 $\\mathbb{E}[(\\hat{p}-p)^2] = 0$。\n\n分析的其余部分专注于 $n \\ge m$ 的情况，这涵盖了所有提供的测试用例（包括边界情况 $n=m$）。\n\n**1. 无偏性证明：$\\mathbb{E}[\\hat{p}] = p$**\n\n估计量为 $\\hat{p} = \\frac{1}{m}\\sum_{i=1}^m Y_i$。根据期望的线性性质，其期望值为：\n$$ \\mathbb{E}[\\hat{p}] = \\mathbb{E}\\left[\\frac{1}{m}\\sum_{i=1}^m Y_i\\right] = \\frac{1}{m}\\sum_{i=1}^m \\mathbb{E}[Y_i] $$\n集合 $\\{Y_i\\}_{i=1}^m$ 是从有限总体 $\\{X_t\\}_{t=1}^n$ 中无放回抽取的简单随机样本。由于此抽样过程的对称性，样本中的每个元素 $Y_i$ 都是一个同分布的随机变量。具体来说，任何一个 $Y_i$ 的分布都与从总体中均匀抽取单个元素的分布相同。\n因此，任何此类元素 $Y_i$ 的期望值都等于总体均值 $p$：\n$$ \\mathbb{E}[Y_i] = \\frac{1}{n}\\sum_{t=1}^n X_t = p $$\n将此代回 $\\mathbb{E}[\\hat{p}]$ 的表达式中：\n$$ \\mathbb{E}[\\hat{p}] = \\frac{1}{m}\\sum_{i=1}^m p = \\frac{1}{m}(m \\cdot p) = p $$\n因此，估计量 $\\hat{p}$ 是真实比例 $p$ 的一个无偏估计量。\n\n**2. 期望平方误差的推导：$\\mathbb{E}[(\\hat{p}-p)^2]$**\n\n由于估计量是无偏的，期望平方误差等于其方差：\n$$ \\mathbb{E}[(\\hat{p}-p)^2] = \\text{Var}(\\hat{p}) $$\n样本均值的方差为：\n$$ \\text{Var}(\\hat{p}) = \\text{Var}\\left(\\frac{1}{m}\\sum_{i=1}^m Y_i\\right) = \\frac{1}{m^2} \\text{Var}\\left(\\sum_{i=1}^m Y_i\\right) $$\n和的方差可以展开为方差和协方差项：\n$$ \\text{Var}\\left(\\sum_{i=1}^m Y_i\\right) = \\sum_{i=1}^m \\text{Var}(Y_i) + \\sum_{i\\neq j} \\text{Cov}(Y_i, Y_j) $$\n由于 $Y_i$ 是同分布的，所有方差项都相等，且对于不同的对 $(i,j)$，所有协方差项也都相等。因此：\n$$ \\text{Var}\\left(\\sum_{i=1}^m Y_i\\right) = m \\cdot \\text{Var}(Y_1) + m(m-1) \\cdot \\text{Cov}(Y_1, Y_2) $$\n我们现在必须求出 $\\text{Var}(Y_1)$ 和 $\\text{Cov}(Y_1, Y_2)$。\n\n首先，是单次抽取 $Y_1$ 的方差。由于 $Y_1 \\in \\{0, 1\\}$，我们有 $Y_1^2 = Y_1$。\n$$ \\text{Var}(Y_1) = \\mathbb{E}[Y_1^2] - (\\mathbb{E}[Y_1])^2 = \\mathbb{E}[Y_1] - p^2 = p - p^2 = p(1-p) $$\n这个量是原始集合 $\\{X_t\\}_{t=1}^n$ 的总体方差，我们可以将其表示为 $\\sigma_X^2 = p(1-p)$。\n\n接下来，是两次不同抽取 $Y_1$ 和 $Y_2$ 之间的协方差。\n$$ \\text{Cov}(Y_1, Y_2) = \\mathbb{E}[Y_1 Y_2] - \\mathbb{E}[Y_1]\\mathbb{E}[Y_2] = \\mathbb{E}[Y_1 Y_2] - p^2 $$\n项 $\\mathbb{E}[Y_1 Y_2]$ 是 $Y_1$ 和 $Y_2$ 都等于 $1$ 的概率。这对应于从总体中无放回地抽取两个 $1$。总体包含 $np$ 个 $1$ 和 $n(1-p)$ 个 $0$。\n$$ \\mathbb{E}[Y_1 Y_2] = P(Y_1=1 \\text{ and } Y_2=1) = P(Y_1=1) \\cdot P(Y_2=1 | Y_1=1) $$\n第一次抽取为 $1$ 的概率是 $P(Y_1=1) = \\frac{np}{n} = p$。在第一次抽取为 $1$ 的条件下，大小为 $n-1$ 的总体中还剩下 $np-1$ 个 $1$。所以，$P(Y_2=1 | Y_1=1) = \\frac{np-1}{n-1}$。\n因此：\n$$ \\mathbb{E}[Y_1 Y_2] = p \\cdot \\frac{np-1}{n-1} $$\n现在我们可以计算协方差：\n$$ \\text{Cov}(Y_1, Y_2) = p \\frac{np-1}{n-1} - p^2 = \\frac{p(np-1) - p^2(n-1)}{n-1} = \\frac{np^2 - p - np^2 + p^2}{n-1} = \\frac{p^2-p}{n-1} = -\\frac{p(1-p)}{n-1} $$\n将方差和协方差代回 $\\text{Var}(\\hat{p})$ 的表达式中：\n$$ \\text{Var}(\\hat{p}) = \\frac{1}{m^2} \\left[ m \\cdot p(1-p) + m(m-1) \\cdot \\left(-\\frac{p(1-p)}{n-1}\\right) \\right] $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{m} \\left[ 1 - \\frac{m-1}{n-1} \\right] $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{m} \\left[ \\frac{(n-1) - (m-1)}{n-1} \\right] $$\n$$ \\text{Var}(\\hat{p}) = \\frac{p(1-p)}{m} \\left( \\frac{n-m}{n-1} \\right) $$\n这个期望平方误差的表达式在 $n \\ge m \\ge 1$ 和 $n \\ge 2$ 时有效。条件 $n \\ge 2$ 是必要的，以确保分母 $n-1$ 不为零。问题陈述保证了 $n \\ge 2$ 和 $1 \\le m \\le n$。请注意，如果 $n=m$，则项 $(n-m)$ 变为 $0$，从而正确地得到误差为 $0$。\n\n**3. 算法复杂度分析**\n\n我们所考虑的算法是经典蓄水池抽样（例如，Vitter 的算法 R）。\n- **时间复杂度**：该算法对流中的 $n$ 个项目各处理一次。对于索引为 $t$（其中 $t > m$）的每个项目，它执行常数次操作：生成一个随机数、一次比较以及可能的一次数组替换。因此，处理每个项目耗时 $O(1)$。对于长度为 $n$ 的流，总时间复杂度为 $O(n)$。\n- **空间复杂度**：该算法的内存使用主要由蓄水池的存储决定，其大小是固定的 $m$。问题指定 $m$ 是一个不依赖于流长度 $n$ 的常数。此外，还需要一个计数器来记录已见项目的数量，它使用 $O(\\log n)$ 的空间，但在流式算法复杂度的上下文中，如果该计数器能装入一个机器字，通常被认为是 $O(1)$ 空间预算的一部分。由于 $m$ 相对于 $n$ 是一个常数，因此空间复杂度为 $O(m) = O(1)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the theoretical expected squared error for an estimator based on reservoir sampling.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1000000, 5, 0.3),\n        (1000, 1, 0.2),\n        (50, 5, 0.5),\n        (10, 3, 0),\n        (10, 3, 1),\n        (7, 7, 0.35),\n        (2, 1, 0.4),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, m, p = case\n        \n        # The derived formula for the expected squared error (which is Var(p_hat)) is:\n        # E[(p_hat - p)^2] = (p * (1 - p) / m) * ((n - m) / (n - 1))\n        # This formula is valid for n >= 2 and 1 = m = n, which all test cases satisfy.\n\n        # Handle the edge case where the denominator n-1 would be zero.\n        # The problem states n >= 2, so this is not strictly needed for the test suite\n        # but is good practice for a general function.\n        if n == 1:\n            # If n=1, then m=1, the sample is the whole population, so error is 0.\n            # If n=0, the problem is ill-defined.\n            error = 0.0\n        # If p=0 or p=1, the population is homogeneous, so error is 0.\n        # If n=m, the sample is the whole population, so error is 0.\n        # The formula correctly handles these cases, as (p*(1-p)) or (n-m) will be 0.\n        else:\n            # Ensure floating-point division\n            n_float = float(n)\n            m_float = float(m)\n            p_float = float(p)\n\n            # Calculation using the derived formula.\n            # Factor 1: Variance of a single draw scaled by sample size\n            term1 = (p_float * (1.0 - p_float)) / m_float\n            # Factor 2: Finite population correction factor\n            term2 = (n_float - m_float) / (n_float - 1.0)\n            \n            error = term1 * term2\n\n        # Format the result to exactly 10 decimal places.\n        results.append(f\"{error:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver function\nsolve()\n```", "id": "3288334"}, {"introduction": "系统生物学中的许多关键问题，例如在信号网络上进行精确的概率推断，在一般情况下是计算上难解的（NP-hard）。初步分析可能会认为，对于中等规模的网络，这些问题也无法解决。然而，参数化复杂度理论提供了一个更为精细的视角，它揭示了如果输入的某个特定结构参数（例如图的树宽 treewidth）很小，那么这些问题的复杂性是可以控制的。这项高级实践 [@problem_id:3288367] 介绍了树宽和固定参数可解性（fixed-parameter tractability）这一强大概念。通过分析和实现一个估算树宽的启发式算法，你将理解如何利用生物网络固有的结构特性来解决那些在其他情况下计算上不可行的问题，这是开发高级生物信息学工具的关键洞察。", "problem": "给定无向交互图，这些图是稀疏生物化学反应或信号通路的抽象。考虑在这些网络上进行精确概率推断，网络被建模为因子图，其中每个分子种类的状态是一个域大小为 $k$ 的有限随机变量，而相互作用在相应的变量范围上导出因子。通过变量消除进行的精确推断会逐一处理变量，将被消除变量所在的所有因子替换为其边缘化结果。每个消除步骤的计算成本主要取决于乘以包含被消除变量的所有因子，然后对该变量进行求和消元。\n\n基本原理和定义：\n- 因子图在变量 $V$ 上导出一个无向图 $G = (V,E)$，其中 $E$ 中的一条边连接两个共同出现在某个因子范围内的变量。\n- 对于 $V$ 上的一个消除顺序 $\\pi$，其导出图的形成过程是：在每次消除顶点 $v$ 时，添加填充边，使其当前邻域成为一个团，然后再移除 $v$。$\\pi$ 的导出宽度是所有被消除顶点在各自消除时刻的当前度数的最大值。图 $G$ 的树宽 $t$ 是所有消除顺序中最小的导出宽度。\n- 如果每个变量的域大小为 $k$，那么在消除一个其当前邻域构成大小为 $w$ 的团的变量时，该步骤的主要算术成本与 $k^{w+1}$ 成正比。\n\n任务概述：\n1. 从第一性原理出发，为具有 $n$ 个变量和树宽 $t$ 的因子图上的变量消除精确推断推导一个渐进运行时间界。该界的形式为 $O(f(t)\\,n)$，其中函数 $f$ 是 $t$ 的指数函数。不要假设任何专门的公式；仅使用上述定义以及每个消除步骤的范围大小受导出团大小限制这一事实来推导表达式。\n2. 实现一个程序，该程序：\n   - 在无向图 $G$ 上使用最小填充启发式算法计算树宽 $t$ 的一个上界 $\\hat{t}$。该启发式算法重复消除一个顶点，该顶点的邻居成为团所需的填充边数量最少（若数量相同，则选择当前度数较小的顶点；若仍相同，则选择顶点索引较小的顶点），添加必要的填充边并记录遇到的最大邻域大小；最终的最大邻域大小即为 $\\hat{t}$。\n   - 对于 $k=2$ 的二元变量，计算每个实例的变量消除算术运算数量的渐进上界为 $n \\cdot 2^{\\hat{t}+1}$。\n3. 使用以下模拟精选通路模块的无向图测试套件。顶点索引从 $0$ 到 $n-1$。每个测试项由一个对 $(n,\\text{edges})$ 指定，其中 $\\text{edges}$ 是一组无序对：\n   - 边界情况（孤立变量）：$(n,\\text{edges}) = \\left(1,\\ \\varnothing\\right)$。\n   - 类似通路的线性链，n=10：$(n,\\text{edges}) = \\left(10,\\ \\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9)\\}\\right)$。\n   - 星形模块，n=10，中心为 0：$(n,\\text{edges}) = \\left(10,\\ \\{(0,1),(0,2),(0,3),(0,4),(0,5),(0,6),(0,7),(0,8),(0,9)\\}\\right)$。\n   - 环形基序，n=8：$(n,\\text{edges}) = \\left(8,\\ \\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,0)\\}\\right)$。\n   - 两行网格模块（2x4），n=8：$(n,\\text{edges}) = \\left(8,\\ \\{(0,1),(1,2),(2,3),(4,5),(5,6),(6,7),(0,4),(1,5),(2,6),(3,7)\\}\\right)$。\n   - 带尾部的团核心（模块内的密集枢纽）：$(n,\\text{edges}) = \\left(7,\\ \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(3,4),(4,5),(5,6)\\}\\right)$。\n4. 假设所有测试用例的变量均为二元变量（$k=2$）。您的程序应按上面列出的顺序处理测试套件，并为每个测试用例输出一个对 $[\\hat{t}, n \\cdot 2^{\\hat{t}+1}]$。\n5. 最终输出格式要求：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身是一个包含两个元素的列表。例如，三个用例的输出应类似于 $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$，其中 $a_i$ 和 $b_i$ 均为整数。\n\n此任务不涉及物理单位。所有角度（如果出现）都应理解为弧度，但此处未使用任何角度。所有要求的数值输出均为整数。", "solution": "用户提供了一个有效的问题陈述。它在科学上基于概率图模型和计算复杂度的原理，问题阐述清晰，定义和目标明确，并且没有任何指令中列出的无效缺陷。我现在将提供一个完整的解决方案。\n\n### 第1部分：渐进运行时间界的推导\n\n任务是为一个具有 $n$ 个变量和树宽 $t$ 的因子图上的变量消除精确推断推导一个渐进运行时间界。该界必须为 $O(f(t)\\,n)$ 的形式，其中 $f$ 是 $t$ 的指数函数，并且从给定的第一性原理推导得出。\n\n1.  **总计算成本**：变量消除算法根据特定的消除顺序 $\\pi = (v_1, v_2, \\dots, v_n)$ 处理图 $G=(V, E)$ 中的 $n$ 个变量。总计算成本是消除每个变量的成本之和。\n    $$ \\text{Total Cost}(\\pi) = \\sum_{i=1}^{n} \\text{Cost}(\\text{eliminate } v_i) $$\n\n2.  **单次消除步骤的成本**：问题陈述指出，消除单个变量 $v_i$ 的成本与 $k^{w_i+1}$ 成正比，其中 $k$ 是每个变量的域大小，$w_i$ 是在 $v_i$ 消除时刻其邻域所形成的团的大小。这个大小 $w_i$ 正是顶点 $v_i$ 在第 $i$ 步之前图中的度数。设 $d_i(v_i)$ 为此度数。因此，消除 $v_i$ 的成本为 $C \\cdot k^{d_i(v_i)+1}$，其中 $C$ 是某个比例常数。\n\n3.  **用导出宽度确定成本上界**：消除顺序 $\\pi$ 的导出宽度，记为 $w^*(\\pi)$，定义为在整个过程中任何顶点在被消除时的度数的最大值。\n    $$ w^*(\\pi) = \\max_{i \\in \\{1, \\dots, n\\}} d_i(v_i) $$\n    因此，对于顺序 $\\pi$ 中的任何变量 $v_i$，其在消除时的度数受导出宽度的限制：$d_i(v_i) \\leq w^*(\\pi)$。我们可以用这个来为给定顺序 $\\pi$ 的总成本建立一个上界：\n    $$ \\text{Total Cost}(\\pi) = \\sum_{i=1}^{n} C \\cdot k^{d_i(v_i)+1} \\leq \\sum_{i=1}^{n} C \\cdot k^{w^*(\\pi)+1} $$\n    由于项 $C \\cdot k^{w^*(\\pi)+1}$ 相对于对 $n$ 个变量的求和来说是常数，我们有：\n    $$ \\text{Total Cost}(\\pi) \\leq n \\cdot C \\cdot k^{w^*(\\pi)+1} $$\n\n4.  **引入树宽**：变量消除算法的效率关键取决于找到一个好的消除顺序。一个最优顺序是使导出宽度最小化的顺序。图 $G$ 的树宽 $t$ 定义为在所有可能的消除顺序中最小的导出宽度。\n    $$ t = \\min_{\\pi} w^*(\\pi) $$\n    如果我们使用一个最优消除顺序 $\\pi_{\\text{opt}}$，那么它的导出宽度等于图的树宽，即 $w^*(\\pi_{\\text{opt}}) = t$。\n\n5.  **最终渐进界**：通过将树宽 $t$ 代入我们对最优顺序的成本不等式中，我们得到了最佳情况变量消除复杂度的上界：\n    $$ \\text{Total Cost}(\\pi_{\\text{opt}}) \\leq n \\cdot C \\cdot k^{t+1} $$\n    在渐进复杂度的表示中，我们忽略常数因子 $C$。因此，运行时间由以下公式界定：\n    $$ O(n \\cdot k^{t+1}) $$\n    该表达式符合所要求的形式 $O(f(t) \\cdot n)$，其中 $f(t) = k^{t+1}$ 是树宽 $t$ 的指数函数。至此，推导完成。\n\n### 第2部分：算法实现\n\n任务的第二部分涉及实现一个算法，使用最小填充启发式计算树宽的上界 $\\hat{t}$，然后使用此界计算总成本。\n\n**最小填充启发式逻辑**\n该算法迭代进行，每步消除一个顶点。在每一步中，我们必须从剩余的顶点集合中选择要消除的顶点。选择遵循一套严格的规则：\n1.  **主要标准（最小填充）**：选择那个使其当前邻域成为一个团所需“填充”边数最少的顶点。对于一个有 $d$ 个邻居的顶点，所需的填充边数为 $\\frac{d(d-1)}{2} - m$，其中 $m$ 是其邻居对之间已经存在的边数。\n2.  **第一决胜规则（最小度数）**：如果多个顶点具有相同的最小填充数，则从此子集中选择当前度数最小的那个。\n3.  **第二决胜规则（最小索引）**：如果仍然存在平局，选择顶点索引最小的那个。\n\n**流程：**\n1.  根据给定的顶点数 $n$ 和边集初始化图。邻接表表示法（例如，Python中的字典套集合）是合适的。\n2.  维护一个剩余顶点的集合，初始时包含从 $0$ 到 $n-1$ 的所有顶点。\n3.  初始化一个变量 `max_degree_encountered` 为 $0$。\n4.  循环 $n$ 次以消除每个顶点：\n    a.  在所有剩余顶点中，通过应用最小填充启发式及其决胜规则，找到要消除的最佳顶点。一个简洁的实现方法是遍历候选顶点，为每个顶点计算一个元组 `(fill_count, degree, index)`，并找到字典序最小的元组。\n    b.  一旦选择了要消除的顶点 $v$，记录其当前度数 $d_v$。更新 `max_degree_encountered = max(max_degree_encountered, d_v)`。\n    c.  修改图：在 $v$ 的邻居之间添加所有必要的填充边，使它们成为一个团。\n    d.  从图结构中移除 $v$：将其从剩余顶点集合中移除，并从其前邻居的邻接表中移除。\n5.  循环结束后，`max_degree_encountered` 将持有 $\\hat{t}$ 的值，即该启发式算法找到的顺序的导出宽度。这作为真实树宽 $t$ 的一个上界。\n\n**成本计算：**\n对于每个具有 $n$ 个变量、二元域大小（$k=2$）和计算出的树宽上界 $\\hat{t}$ 的测试用例，总算术运算次数计算为 $n \\cdot 2^{\\hat{t}+1}$。\n最终程序对每个测试用例实现此逻辑，并将输出格式化为 `[t_hat, cost]` 对的列表。", "answer": "```python\nimport sys\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    It computes an upper bound on treewidth using the min-fill heuristic\n    and then calculates the corresponding complexity bound for variable elimination.\n    \"\"\"\n\n    # The problem specifies that numpy and scipy are allowed, but they are not\n    # necessary for this specific implementation. Using only standard libraries.\n\n    test_cases = [\n        (1, set()),\n        (10, {(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9)}),\n        (10, {(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (0, 7), (0, 8), (0, 9)}),\n        (8, {(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 0)}),\n        (8, {(0, 1), (1, 2), (2, 3), (4, 5), (5, 6), (6, 7), (0, 4), (1, 5), (2, 6), (3, 7)}),\n        (7, {(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3), (3, 4), (4, 5), (5, 6)}),\n    ]\n\n    results = []\n    for n, edges in test_cases:\n        t_hat = compute_treewidth_upper_bound(n, edges)\n        # Using k=2 as specified for binary variables\n        # Cost is n * 2^(t_hat + 1). Use bit shift for efficiency and to handle large numbers.\n        cost = n * (1  (t_hat + 1))\n        results.append([t_hat, cost])\n\n    # Format the final output string as specified\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef compute_treewidth_upper_bound(n, edges_set):\n    \"\"\"\n    Computes an upper bound on treewidth (the induced width) for a given graph\n    using the min-fill heuristic.\n\n    Args:\n        n (int): The number of vertices in the graph.\n        edges_set (set): A set of tuples representing the undirected edges.\n\n    Returns:\n        int: The computed treewidth upper bound (t_hat).\n    \"\"\"\n    if n == 1:\n        return 0\n\n    # Adjacency list representation using a dictionary of sets\n    adj = {i: set() for i in range(n)}\n    for u, v in edges_set:\n        adj[u].add(v)\n        adj[v].add(u)\n\n    remaining_nodes = set(range(n))\n    max_width_encountered = 0\n\n    # Eliminate one node in each iteration\n    for _ in range(n):\n        # Store (fill_count, degree, index) to find the best node to eliminate.\n        # Python's tuple comparison handles the tie-breaking logic automatically.\n        best_node_info = (float('inf'), float('inf'), -1)\n\n        # Iterate through remaining nodes in sorted order to ensure deterministic tie-breaking.\n        for node in sorted(list(remaining_nodes)):\n            neighbors = adj[node]\n            degree = len(neighbors)\n            \n            fill_count = 0\n            if degree > 1:\n                neighbor_list = list(neighbors)\n                for i in range(degree):\n                    for j in range(i + 1, degree):\n                        u, v = neighbor_list[i], neighbor_list[j]\n                        if v not in adj[u]:\n                            fill_count += 1\n            \n            current_node_info = (fill_count, degree, node)\n            if current_node_info  best_node_info:\n                best_node_info = current_node_info\n\n        _, degree_at_elimination, node_to_eliminate = best_node_info\n        \n        max_width_encountered = max(max_width_encountered, degree_at_elimination)\n        \n        # Add fill-in edges (triangulate the neighbors)\n        neighbors_to_connect = list(adj[node_to_eliminate])\n        for i in range(len(neighbors_to_connect)):\n            for j in range(i + 1, len(neighbors_to_connect)):\n                u, v = neighbors_to_connect[i], neighbors_to_connect[j]\n                adj[u].add(v)\n                adj[v].add(u)\n        \n        # Remove the node from the graph\n        for neighbor in neighbors_to_connect:\n            adj[neighbor].remove(node_to_eliminate)\n        del adj[node_to_eliminate]\n        remaining_nodes.remove(node_to_eliminate)\n        \n    return max_width_encountered\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3288367"}]}