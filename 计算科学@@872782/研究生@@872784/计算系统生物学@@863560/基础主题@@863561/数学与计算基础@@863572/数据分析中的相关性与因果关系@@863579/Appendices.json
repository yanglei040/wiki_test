{"hands_on_practices": [{"introduction": "在数据分析中，一个惊人但常见的现象是辛普森悖论（Simpson's paradox），即在分组数据中观察到的关联趋势在数据汇总后会发生逆转。这个练习 [@problem_id:3298678] 将通过一个关于癌症细胞系药物反应的假设性研究，引导你亲手计算这种效应。通过区分边际风险差异和条件风险差异，你将深刻理解混杂变量是如何扭曲我们对因果关系的初步印象的。", "problem": "在一项关于癌细胞系药物反应的计算系统生物学研究中，收集了一个观测数据集，以评估一种靶向治疗（用二元变量 $X \\in \\{0,1\\}$ 表示）是否能改善反应 $Y \\in \\{0,1\\}$，其中 $Y=1$ 表示有响应。一个二元协变量 $Z \\in \\{0,1\\}$ 代表一个培养条件批次因子，它同时影响治疗分配 $X$ 和反应 $Y$。目标是在 $Z$ 可能引起混杂的情况下，分析相关性与因果关系。\n\n给你按 $X$ 和 $Z$ 分层的响应者和非响应者的模拟计数：\n- 对于 $Z=0$（有利批次）：\n  - $X=1$：响应者 $Y=1$ 为 $20$ 人中的 $18$ 人，非响应者 $Y=0$ 为 $2$ 人。\n  - $X=0$：响应者 $Y=1$ 为 $180$ 人中的 $144$ 人，非响应者 $Y=0$ 为 $36$ 人。\n- 对于 $Z=1$（不利批次）：\n  - $X=1$：响应者 $Y=1$ 为 $180$ 人中的 $72$ 人，非响应者 $Y=0$ 为 $108$ 人。\n  - $X=0$：响应者 $Y=1$ 为 $20$ 人中的 $6$ 人，非响应者 $Y=0$ 为 $14$ 人。\n\n从条件概率和风险差的定义出发，其中给定条件集 $\\mathcal{C}$ 的风险差定义为\n$$\n\\mathrm{RD}(\\mathcal{C}) \\equiv \\Pr(Y=1 \\mid X=1, \\mathcal{C}) - \\Pr(Y=1 \\mid X=0, \\mathcal{C}),\n$$\n计算：\n1. 在 $Z$ 的每个水平内的条件风险差，即 $\\mathrm{RD}(Z=0)$ 和 $\\mathrm{RD}(Z=1)$。\n2. 边际风险差 $\\mathrm{RD}(\\varnothing)$，其定义为不对 $Z$ 设条件，通过合并 $Z$ 的数据得到。\n\n为了形式化悖论方向的识别，定义方向指示符\n$$\nD \\equiv \\operatorname{sign}\\big(\\mathrm{RD}(\\varnothing)\\big) \\times \\operatorname{sign}\\!\\left(\\overline{\\mathrm{RD}}_{\\text{cond}}\\right),\n$$\n其中 $\\overline{\\mathrm{RD}}_{\\text{cond}}$ 是条件风险差的分层样本量加权平均值，\n$$\n\\overline{\\mathrm{RD}}_{\\text{cond}} \\equiv \\frac{\\sum_{z \\in \\{0,1\\}} n_{z} \\, \\mathrm{RD}(Z=z)}{\\sum_{z \\in \\{0,1\\}} n_{z}},\n$$\n且 $n_{z}$ 是在 $X=0$ 和 $X=1$ 中 $Z=z$ 的样本总数。\n\n仅报告 $D$ 的单个数值作为最终答案，使用以下约定：$D=1$ 表示边际效应和条件效应方向一致，$D=-1$ 表示反转（辛普森悖论方向），$D=0$ 表示至少有一个因子的效应符号为零。不包括任何单位。无需四舍五入；在取符号之前，在需要时将精确值计算为有理数。", "solution": "该问题已经过验证，被认为是有效的。它在科学上基于概率论和统计学，问题提出得很好，数据完整且一致，并且表述客观。没有违反基本原则、信息缺失或含糊不清之处。因此，我们可以着手解决。\n\n目标是计算一个方向指示符 $D$，该指示符比较边际风险差的符号与平均条件风险差的符号，以便在存在混杂变量 $Z$ 的情况下，刻画相关性与因果关系之间的关系。\n\n令 $N(Y=y, X=x, Z=z)$ 表示具有特定反应 $Y$、治疗 $X$ 和协变量 $Z$ 值的受试者计数。给定数据可以总结如下：\n对于 $Z=0$：\n$N(Y=1, X=1, Z=0) = 18$\n$N(Y=0, X=1, Z=0) = 2$\n$N(Y=1, X=0, Z=0) = 144$\n$N(Y=0, X=0, Z=0) = 36$\n\n对于 $Z=1$：\n$N(Y=1, X=1, Z=1) = 72$\n$N(Y=0, X=1, Z=1) = 108$\n$N(Y=1, X=0, Z=1) = 6$\n$N(Y=0, X=0, Z=1) = 14$\n\n风险差定义为 $\\mathrm{RD}(\\mathcal{C}) \\equiv \\Pr(Y=1 \\mid X=1, \\mathcal{C}) - \\Pr(Y=1 \\mid X=0, \\mathcal{C})$。我们将对 $Z$ 的每个分层以及边际情况（合并数据）计算此值。\n\n**步骤1：计算 $Z=0$ 的条件风险差。**\n首先，我们根据 $Z=0$ 分层的计数计算所需的条件概率。\n$X=1$ 且 $Z=0$ 的受试者总数为 $18+2=20$。\n$X=0$ 且 $Z=0$ 的受试者总数为 $144+36=180$。\n给定治疗和 $Z=0$ 条件下的响应条件概率为：\n$$\n\\Pr(Y=1 \\mid X=1, Z=0) = \\frac{N(Y=1, X=1, Z=0)}{N(Y=1, X=1, Z=0) + N(Y=0, X=1, Z=0)} = \\frac{18}{20} = \\frac{9}{10}\n$$\n给定无治疗和 $Z=0$ 条件下的响应条件概率为：\n$$\n\\Pr(Y=1 \\mid X=0, Z=0) = \\frac{N(Y=1, X=0, Z=0)}{N(Y=1, X=0, Z=0) + N(Y=0, X=0, Z=0)} = \\frac{144}{180} = \\frac{4}{5}\n$$\n$Z=0$ 的条件风险差为：\n$$\n\\mathrm{RD}(Z=0) = \\Pr(Y=1 \\mid X=1, Z=0) - \\Pr(Y=1 \\mid X=0, Z=0) = \\frac{9}{10} - \\frac{4}{5} = \\frac{9}{10} - \\frac{8}{10} = \\frac{1}{10}\n$$\n\n**步骤2：计算 $Z=1$ 的条件风险差。**\n接下来，我们对 $Z=1$ 分层执行相同的计算。\n$X=1$ 且 $Z=1$ 的受试者总数为 $72+108=180$。\n$X=0$ 且 $Z=1$ 的受试者总数为 $6+14=20$。\n给定治疗和 $Z=1$ 条件下的响应条件概率为：\n$$\n\\Pr(Y=1 \\mid X=1, Z=1) = \\frac{N(Y=1, X=1, Z=1)}{N(Y=1, X=1, Z=1) + N(Y=0, X=1, Z=1)} = \\frac{72}{180} = \\frac{2}{5}\n$$\n给定无治疗和 $Z=1$ 条件下的响应条件概率为：\n$$\n\\Pr(Y=1 \\mid X=0, Z=1) = \\frac{N(Y=1, X=0, Z=1)}{N(Y=1, X=0, Z=1) + N(Y=0, X=0, Z=1)} = \\frac{6}{20} = \\frac{3}{10}\n$$\n$Z=1$ 的条件风险差为：\n$$\n\\mathrm{RD}(Z=1) = \\Pr(Y=1 \\mid X=1, Z=1) - \\Pr(Y=1 \\mid X=0, Z=1) = \\frac{2}{5} - \\frac{3}{10} = \\frac{4}{10} - \\frac{3}{10} = \\frac{1}{10}\n$$\n在 $Z$ 的每个分层内，治療都具有正效应，风险差为 $\\frac{1}{10}$。\n\n**步骤3：计算边际风险差 $\\mathrm{RD}(\\varnothing)$。**\n为了求得边际风险差，我们必须首先聚合（合并）$Z$ 的两个分层的数据。\n接受治疗（$X=1$）的响应者（$Y=1$）总计数：\n$N(Y=1, X=1) = N(Y=1, X=1, Z=0) + N(Y=1, X=1, Z=1) = 18 + 72 = 90$。\n接受治疗（$X=1$）的受试者总数：\n$N(X=1) = (18+2) + (72+108) = 20 + 180 = 200$。\n未接受治疗（$X=0$）的响应者（$Y=1$）总计数：\n$N(Y=1, X=0) = N(Y=1, X=0, Z=0) + N(Y=1, X=0, Z=1) = 144 + 6 = 150$。\n未接受治疗（$X=0$）的受试者总数：\n$N(X=0) = (144+36) + (6+14) = 180 + 20 = 200$。\n现在我们计算边际概率：\n$$\n\\Pr(Y=1 \\mid X=1) = \\frac{N(Y=1, X=1)}{N(X=1)} = \\frac{90}{200} = \\frac{9}{20}\n$$\n$$\n\\Pr(Y=1 \\mid X=0) = \\frac{N(Y=1, X=0)}{N(X=0)} = \\frac{150}{200} = \\frac{15}{20} = \\frac{3}{4}\n$$\n边际风险差为：\n$$\n\\mathrm{RD}(\\varnothing) = \\Pr(Y=1 \\mid X=1) - \\Pr(Y=1 \\mid X=0) = \\frac{9}{20} - \\frac{15}{20} = -\\frac{6}{20} = -\\frac{3}{10}\n$$\n边际分析表明治疗具有负效应，这与条件效应相反。这种现象被称为辛普森悖论 (Simpson's paradox)。\n\n**步骤4：计算分层样本量加权平均条件风险差 $\\overline{\\mathrm{RD}}_{\\text{cond}}$。**\n每个分层 $Z=z$ 的样本总数 $n_z$ 为：\n$n_0 = N(X=1, Z=0) + N(X=0, Z=0) = 20 + 180 = 200$。\n$n_1 = N(X=1, Z=1) + N(X=0, Z=1) = 180 + 20 = 200$。\n样本总数为 $n_0 + n_1 = 400$。\n加权平均值为：\n$$\n\\overline{\\mathrm{RD}}_{\\text{cond}} = \\frac{\\sum_{z \\in \\{0,1\\}} n_{z} \\, \\mathrm{RD}(Z=z)}{\\sum_{z \\in \\{0,1\\}} n_{z}} = \\frac{n_{0} \\mathrm{RD}(Z=0) + n_{1} \\mathrm{RD}(Z=1)}{n_{0} + n_{1}}\n$$\n代入计算出的值：\n$$\n\\overline{\\mathrm{RD}}_{\\text{cond}} = \\frac{200 \\times \\frac{1}{10} + 200 \\times \\frac{1}{10}}{200+200} = \\frac{20 + 20}{400} = \\frac{40}{400} = \\frac{1}{10}\n$$\n\n**步骤5：计算方向指示符 $D$。**\n方向指示符定义为 $D \\equiv \\operatorname{sign}\\big(\\mathrm{RD}(\\varnothing)\\big) \\times \\operatorname{sign}\\!\\left(\\overline{\\mathrm{RD}}_{\\text{cond}}\\right)$。\n我们有 $\\mathrm{RD}(\\varnothing) = -\\frac{3}{10}$，为负数，所以 $\\operatorname{sign}\\big(\\mathrm{RD}(\\varnothing)\\big) = -1$。\n我们有 $\\overline{\\mathrm{RD}}_{\\text{cond}} = \\frac{1}{10}$，为正数，所以 $\\operatorname{sign}\\!\\left(\\overline{\\mathrm{RD}}_{\\text{cond}}\\right) = 1$。\n因此，方向指示符为：\n$$\nD = (-1) \\times (1) = -1\n$$\n$D=-1$ 的值表示边际关联的方向与条件关联的方向相反，证实了辛普森悖论的存在。", "answer": "$$\\boxed{-1}$$", "id": "3298678"}, {"introduction": "虽然调整混杂变量是因果推断的关键，但不假思索地调整所有与处理和结果相关的变量可能反而会引入偏倚。这个练习 [@problem_id:3298664] 探讨了“对撞偏倚”（collider bias）这一微妙陷阱，它源于对一个“对撞节点”或其后代进行条件化。通过分析一个基因调控网络的结构因果模型，你将定量地看到，错误的调整如何在一个原本无偏的估计中凭空制造出虚假关联，这凸显了使用因果图来指导变量选择的重要性。", "problem": "考虑一个计算系统生物学背景，其中基因调控和信号传导活动由一个有向无环图 (DAG) 和一个线性高斯结构因果模型 (SCM) 建模。令 $X$ 表示一个转录因子的对数转录本丰度，$Y$ 表示一个下游磷酸化活动读数，并令 $Z$ 表示一个染色质免疫沉淀测序 (ChIP-seq) 信号，它是启动子占据变量 $C$ 的一个带噪声的后代。潜在的上游调节因子 $A$ 和 $B$ 分别影响转录因子表达和信号传导环境，并且两者都影响启动子占据 $C$，使得 $C$ 成为路径 $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$ 上的一个对撞节点。SCM 定义如下：\n$$\nA \\sim \\mathcal{N}(0,1), \\quad B \\sim \\mathcal{N}(0,1),\n$$\n$$\n\\varepsilon_{X}, \\varepsilon_{C}, \\varepsilon_{Z}, \\varepsilon_{Y} \\sim \\mathcal{N}(0,1),\n$$\n其中所有外生变量相互独立，并且\n$$\nX = A + \\varepsilon_{X}, \\quad C = A + B + \\varepsilon_{C}, \\quad Z = C + \\varepsilon_{Z}, \\quad Y = \\theta X + \\beta_{B} B + \\varepsilon_{Y}.\n$$\n取数值 $\\theta = 1$ 和 $\\beta_{B} = 1$。你可以假设样本量任意大，因此普通最小二乘法 (OLS) 的回归系数以总体矩的形式收敛到它们的概率极限。\n\n仅使用上述模型和核心因果定义（对撞节点及其后代、d-分离和后门准则），判断对 $Z$ 进行调整是否会增加估计 $X$ 对 $Y$ 的因果效应时的偏差。计算在 $Y$ 对 $X$ 的单独回归中以及在 $Y$ 对 $X$ 和 $Z$ 的回归中，$X$ 的大样本 OLS 系数。$X$ 对 $Y$ 的真实因果效应是结构参数 $\\theta$。报告因对 $Z$ 进行调整而导致的偏差绝对增加量，定义为 $\\left|b_{\\text{adj}} - \\theta\\right| - \\left|b_{\\text{unadj}} - \\theta\\right|$，其中 $b_{\\text{unadj}}$ 是回归 $Y \\sim X$ 中 $X$ 的 OLS 系数的概率极限，而 $b_{\\text{adj}}$ 是回归 $Y \\sim X + Z$ 中 $X$ 的 OLS 系数的概率极限。将你的最终答案表示为一个精确的数字。无需四舍五入，最终报告值中不应包含任何单位。[@problem_id:48]", "solution": "该问题已经过验证，被认为是有效的。它在科学上基于结构因果模型 (SCM)，问题定义明确，所有必要的结构方程和参数均已提供。没有违反基本原则、信息缺失或含糊不清之处。因此，我们可以着手解决。\n\n$X$ 对 $Y$ 的真实因果效应由 $Y$ 的结构方程中的参数 $\\theta$ 给出，即 $\\theta=1$。我们的目标是比较该参数的两个 OLS 估计量的偏差。\n\n**1. 因果图与 d-分离分析**\nSCM 蕴含了以下因果依赖关系，构成一个有向无环图 (DAG)：\n- $A \\rightarrow X$ 和 $A \\rightarrow C$\n- $B \\rightarrow Y$ 和 $B \\rightarrow C$\n- $X \\rightarrow Y$\n- $C \\rightarrow Z$\n\n路径 $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$ 是 $X$ 和 $Y$ 之间的一条非因果路径。在这条路径上，节点 $C$ 是一个对撞节点，因为它有两个指向它的箭头 ($A \\rightarrow C$ 和 $B \\rightarrow C$ )。\n- **未调整情况 (回归 $Y \\sim X$)：** 在这种情况下，我们不以任何变量为条件。对撞节点 $C$ 阻断了路径 $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$。由于这是 $X$ 和 $Y$ 之间唯一的非因果路径，因此不存在混淆。我们因此预期未调整的 OLS 估计量 $b_{\\text{unadj}}$ 是无偏的，即 $b_{\\text{unadj}} = \\theta$。\n- **调整后情况 (回归 $Y \\sim X + Z$)：** 在这种情况下，我们以 $Z$ 为条件。变量 $Z$ 是对撞节点 $C$ 的一个后代 (因为 $C \\rightarrow Z$)。根据 d-分离的规则，以一个对撞节点或其后代为条件会打开它所阻断的路径。这会通过路径 $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$ 在 $X$ 和 $Y$ 之间产生伪关联。这种诱导出的关联将使 OLS 估计量 $b_{\\text{adj}}$ 产生偏差。这是一个经典的对撞节点分层偏差的例子。\n\n**2. 总体矩的计算**\n我们考虑大样本极限情况，其中 OLS 系数是总体方差和协方差的比值。首先，我们使用 SCM 计算这些矩。设 $\\theta = 1$ 和 $\\beta_B = 1$。所有外生变量的方差均为 $1$。\n\n- **方差：**\n    - $\\text{Var}(X) = \\text{Var}(A + \\varepsilon_{X}) = \\text{Var}(A) + \\text{Var}(\\varepsilon_{X}) = 1 + 1 = 2$.\n    - $\\text{Var}(C) = \\text{Var}(A + B + \\varepsilon_{C}) = \\text{Var}(A) + \\text{Var}(B) + \\text{Var}(\\varepsilon_{C}) = 1 + 1 + 1 = 3$.\n    - $\\text{Var}(Z) = \\text{Var}(C + \\varepsilon_{Z}) = \\text{Var}(C) + \\text{Var}(\\varepsilon_{Z}) = 3 + 1 = 4$.\n\n- **协方差：**\n    由于所有外生变量的均值为 $0$，所以 $X, Y, Z$ 的均值也为 $0$。因此，$\\text{Cov}(U,V) = \\text{E}[UV]$。\n    - $\\text{Cov}(X, Y) = \\text{E}[X Y] = \\text{E}[(A + \\varepsilon_{X})((\\theta X) + (\\beta_{B} B) + \\varepsilon_{Y})]$\n        $= \\text{E}[(A + \\varepsilon_{X})((1 \\cdot (A + \\varepsilon_{X})) + (1 \\cdot B) + \\varepsilon_{Y})]$\n        $= \\text{E}[(A + \\varepsilon_{X})^{2}] + \\text{E}[(A + \\varepsilon_{X})B] + \\text{E}[(A + \\varepsilon_{X})\\varepsilon_{Y}]$\n        由于外生变量的相互独立性，最后两项为 $0$。\n        $= \\text{E}[(A + \\varepsilon_{X})^{2}] = \\text{Var}(A + \\varepsilon_{X}) = \\text{Var}(X) = 2$.\n    - $\\text{Cov}(X, Z) = \\text{E}[XZ] = \\text{E}[(A + \\varepsilon_{X})(C + \\varepsilon_{Z})] = \\text{E}[(A + \\varepsilon_{X})(A + B + \\varepsilon_{C} + \\varepsilon_{Z})]$\n        展开并利用独立性，唯一非零项是 $\\text{E}[A^2]$。\n        $= \\text{E}[A^2] = \\text{Var}(A) = 1$.\n    - $\\text{Cov}(Y, Z) = \\text{E}[YZ] = \\text{E}[(\\theta X + \\beta_{B} B + \\varepsilon_{Y})(C + \\varepsilon_{Z})]$\n        $= \\text{E}[(A + \\varepsilon_{X} + B + \\varepsilon_{Y})(A + B + \\varepsilon_{C} + \\varepsilon_{Z})]$\n        展开并利用独立性，唯一非零的交叉项是 $\\text{E}[A^2]$ 和 $\\text{E}[B^2]$。\n        $= \\text{E}[A^2] + \\text{E}[B^2] = \\text{Var}(A) + \\text{Var}(B) = 1 + 1 = 2$.\n\n**3. OLS 系数的计算**\n\n- **未调整系数 ($b_{\\text{unadj}}$):**\n    $Y$ 对 $X$ 的简单线性回归系数的概率极限是：\n    $$\n    b_{\\text{unadj}} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = \\frac{2}{2} = 1\n    $$\n- **调整后系数 ($b_{\\text{adj}}$):**\n    $Y$ 对 $X$ 和 $Z$ 的多元线性回归中 $X$ 的系数的概率极限是：\n    $$\n    b_{\\text{adj}} = \\frac{\\text{Cov}(Y, X)\\text{Var}(Z) - \\text{Cov}(Y, Z)\\text{Cov}(X, Z)}{\\text{Var}(X)\\text{Var}(Z) - [\\text{Cov}(X, Z)]^2}\n    $$\n    代入计算出的矩：\n    $$\n    b_{\\text{adj}} = \\frac{(2)(4) - (2)(1)}{(2)(4) - (1)^2} = \\frac{8 - 2}{8 - 1} = \\frac{6}{7}\n    $$\n\n**4. 偏差增加量的计算**\n真实的因果效应是 $\\theta = 1$。\n- 未调整估计量的偏差：$|b_{\\text{unadj}} - \\theta| = |1 - 1| = 0$。\n- 调整后估计量的偏差：$|b_{\\text{adj}} - \\theta| = \\left|\\frac{6}{7} - 1\\right| = \\left|-\\frac{1}{7}\\right| = \\frac{1}{7}$。\n\n因对 $Z$ 进行调整而导致的偏差绝对增加量是：\n$$\n|b_{\\text{adj}} - \\theta| - |b_{\\text{unadj}} - \\theta| = \\frac{1}{7} - 0 = \\frac{1}{7}\n$$\n计算证实，对 $C$ 的后代 $Z$ 进行调整，会在原本没有偏差的地方引入偏差。", "answer": "$$\\boxed{\\frac{1}{7}}$$", "id": "3298664"}, {"introduction": "在了解了忽略混杂变量和不当调整的风险后，我们现在转向一个建设性的解决方案。G-计算是一种基于标准化思想的强大方法，用于从观测数据中估计因果效应。在这个编程实践中 [@problem_id:3298691]，你将亲手实现G-公式（g-formula），包括模拟存在混杂的数据、拟合结果和混杂因素的统计模型，并使用蒙特卡洛积分来估计平均处理效应，从而将理论知识转化为解决实际问题的计算技能。", "problem": "要求您在一个专为计算系统生物学设计的模拟框架内，实现 g-computation 公式的参数化版本，以便在存在混杂因素的情况下估计因果估计量。该任务的重点是通过显式地对数据生成过程进行建模，并在明确定义的假设下通过 g-formula 进行调整，从而区分相关性与因果关系。您的程序必须是一个完整的、可运行的实现，不接受任何输入，并在单行中产生所需的输出。您必须使用固定的随机种子和指定的模型类别。\n\n基本原理：\n- 潜在结果框架，其假设包括一致性、给定混杂因素下的条件可交换性以及正值性。\n- 在这些假设下，被称为平均处理效应（ATE）的因果估计量等于 g-formula，\n$$\n\\text{ATE} = \\mathbb{E}_Z\\left[\\mathbb{E}[Y \\mid X=1, Z] - \\mathbb{E}[Y \\mid X=0, Z]\\right].\n$$\n- 您将使用参数化最大似然估计（MLE）进行模型拟合，包括用于连续结果的线性回归和用于二元结果的逻辑回归。您将使用高斯分布和伯努利分布等参数族对混杂因素的分布进行建模。\n\n定义：\n- 令 $Y$ 表示结果，$X \\in \\{0,1\\}$ 表示二元干预或暴露，$Z$ 表示观察到的混杂因素。\n- 广义线性模型 (GLM)：一类用于条件期望 $\\mathbb{E}[Y \\mid X, Z]$ 的模型，包括线性回归和逻辑回归。\n- 最大似然估计 (MLE)：一种通过最大化似然函数来进行参数估计的方法。\n- 平均处理效应 (ATE)：$\\mathbb{E}[Y^{(1)} - Y^{(0)}]$，其中 $Y^{(x)}$ 表示在 $X=x$ 下的潜在结果。\n\n目标：\n- 通过为 $\\mathbb{E}[Y \\mid X,Z]$ 和分布 $P(Z)$ 指定参数化模型来实现 g-computation，并计算\n$$\n\\mathbb{E}_Z\\Big(\\mathbb{E}[Y \\mid X=1,Z] - \\mathbb{E}[Y \\mid X=0,Z]\\Big).\n$$\n- 对 $Z$ 的期望将通过蒙特卡洛积分进行近似，具体方法是使用从已拟合的 $P(Z)$ 参数化模型中抽取的样本。\n\n假设：\n- 一致性：在观察到的暴露下，观察结果等于潜在结果，即 $Y = Y^{(X)}$。\n- 条件可交换性：$(Y^{(1)}, Y^{(0)}) \\perp X \\mid Z$。\n- 正值性：对于支撑集中的所有 $Z$，$P(X=1 \\mid Z)  0$ 且 $P(X=0 \\mid Z)  0$。\n\n实现要求：\n- 对于每个测试用例，使用指定的生成模型和参数（见下文）模拟一个数据集。使用固定的随机种子以确保可复现性。\n- 为每个用例使用指定的族和特征，为 $\\mathbb{E}[Y \\mid X,Z]$ 拟合一个参数化模型。\n- 为每个用例拟合一个指定的 $P(Z)$ 参数化模型（高斯分布、伯努利分布或独立乘积）。\n- 通过从拟合的 $P(Z)$ 分布中抽取 $M$ 个样本并对条件差异求平均，以蒙特卡洛方法近似积分 $\\mathbb{E}_Z[\\cdot]$。每个用例使用指定的 $M$ 值。\n- 您的最终输出必须是单行文本，其中包含所有测试用例的估计 ATE 值列表，四舍五入到六位小数，格式为方括号内的逗号分隔列表，例如 $[0.123456,0.234567,0.345678,0.456789]$。\n- 不涉及物理单位。所有角度（如果有）都必须作为无单位实数处理。请勿使用百分比；所有量均以小数表示。\n\n测试套件：\n- 用例 $1$（连续结果，线性模型，高斯混杂因素）：\n    - 种子: $1$\n    - 样本大小: $n = 10000$\n    - 混杂因素: $Z \\sim \\mathcal{N}(0,1)$\n    - 处理分配: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z))$，其中 $\\sigma(u) = 1/(1+e^{-u})$，使用 $\\gamma_0 = 0.2$, $\\gamma_1 = 1.0$\n    - 结果: $Y = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon$，$\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，使用 $\\sigma = 1.0$, $\\beta_0 = 0.5$, $\\beta_1 = 2.0$, $\\beta_2 = 1.0$\n    - $\\mathbb{E}[Y \\mid X,Z]$ 的模型：使用特征 $[1, X, Z]$ 的线性回归\n    - $P(Z)$ 的模型：通过 MLE 估计的高斯分布 $\\mathcal{N}(\\mu_Z, \\sigma_Z^2)$\n    - 蒙特卡洛抽样次数: $M = 100000$\n- 用例 $2$（二元结果，逻辑斯蒂模型，高斯混杂因素）：\n    - 种子: $2$\n    - 样本大小: $n = 20000$\n    - 混杂因素: $Z \\sim \\mathcal{N}(0,1)$\n    - 处理分配: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z))$，使用 $\\gamma_0 = -0.1$, $\\gamma_1 = 1.2$\n    - 结果: $Y \\sim \\text{Bernoulli}(\\sigma(\\alpha_0 + \\alpha_1 X + \\alpha_2 Z))$，使用 $\\alpha_0 = -0.5$, $\\alpha_1 = 1.0$, $\\alpha_2 = 0.8$\n    - $\\mathbb{E}[Y \\mid X,Z]$ 的模型：使用特征 $[1, X, Z]$ 的逻辑斯蒂回归\n    - $P(Z)$ 的模型：通过 MLE 估计的高斯分布 $\\mathcal{N}(\\mu_Z, \\sigma_Z^2)$\n    - 蒙特卡洛抽样次数: $M = 200000$\n- 用例 $3$（连续结果，多项式线性模型，混合混杂因素）：\n    - 种子: $3$\n    - 样本大小: $n = 15000$\n    - 混杂因素: $Z_1 \\sim \\text{Bernoulli}(p)$，其中 $p = 0.3$，独立于 $Z_2 \\sim \\mathcal{N}(1.0, 0.5^2)$\n    - 处理分配: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z_1 + \\gamma_2 Z_2))$，使用 $\\gamma_0 = -0.2$, $\\gamma_1 = 1.0$, $\\gamma_2 = 1.5$\n    - 结果: $Y = \\beta_0 + \\beta_1 X + \\beta_2 Z_1 + \\beta_3 Z_2 + \\beta_4 Z_2^2 + \\varepsilon$，$\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，使用 $\\sigma = 0.5$, $\\beta_0 = 0.1$, $\\beta_1 = 1.5$, $\\beta_2 = 0.7$, $\\beta_3 = 0.5$, $\\beta_4 = -0.3$\n    - $\\mathbb{E}[Y \\mid X,Z]$ 的模型：使用特征 $[1, X, Z_1, Z_2, Z_2^2]$ 的线性回归\n    - $P(Z)$ 的模型：$Z_1$ 的伯努利分布和 $Z_2$ 的高斯分布的独立乘积，参数通过 MLE 估计\n    - 蒙特卡洛抽样次数: $M = 150000$\n- 用例 $4$（连续结果，线性模型，接近违反正值性）：\n    - 种子: $4$\n    - 样本大小: $n = 10000$\n    - 混杂因素: $Z \\sim \\mathcal{N}(0,1)$\n    - 处理分配: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z))$，使用 $\\gamma_0 = 0.0$, $\\gamma_1 = 5.0$\n    - 结果: $Y = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon$，$\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$，使用 $\\sigma = 0.2$, $\\beta_0 = 0.0$, $\\beta_1 = 0.5$, $\\beta_2 = 0.2$\n    - $\\mathbb{E}[Y \\mid X,Z]$ 的模型：使用特征 $[1, X, Z]$ 的线性回归\n    - $P(Z)$ 的模型：通过 MLE 估计的高斯分布 $\\mathcal{N}(\\mu_Z, \\sigma_Z^2)$\n    - 蒙特卡洛抽样次数: $M = 150000$\n\n算法规范：\n- 对于每个用例：\n    1. 使用给定的种子和样本大小 $n$，从指定的模型中模拟 $(Z, X, Y)$。\n    2. 使用 MLE 拟合 $\\mathbb{E}[Y \\mid X,Z]$ 的参数：\n        - 对于线性模型，使用普通最小二乘法（高斯误差下 MLE 的一个特例）。\n        - 对于逻辑斯蒂模型，使用 Newton–Raphson 方法最大化伯努利对数似然。\n    3. 使用 MLE 拟合 $P(Z)$ 的参数：\n        - 对于高斯分布，估计均值向量和协方差矩阵。\n        - 对于伯努利分布，估计成功概率。\n        - 对于独立乘积，假设独立性并组合以上各项。\n    4. 从拟合的 $P(Z)$ 中抽取 $M$ 个独立样本，并计算\n       $$\n       \\widehat{\\text{ATE}} = \\frac{1}{M} \\sum_{m=1}^M \\left(\\widehat{\\mathbb{E}}[Y \\mid X=1,Z_m] - \\widehat{\\mathbb{E}}[Y \\mid X=0,Z_m]\\right).\n       $$\n- 在所有指定的地方使用逻辑斯蒂函数 $\\sigma(u) = 1/(1+e^{-u})$。\n\n最终输出格式要求：\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。\n- 该列表必须包含四个浮点数，分别对应于用例 $1$ 到 $4$，每个数都四舍五入到六位小数，例如 $[0.123456,0.234567,0.345678,0.456789]$。\n\n您的解决方案必须用 Python $3.12$ 版本实现，并且只能使用标准库以及以下库：NumPy 版本 $1.23.5$ 和 SciPy 版本 $1.11.4$。程序必须是自包含的，不得读取任何输入或文件，也不得访问任何网络资源。", "solution": "该问题已经过验证，被认为是有效的。它在科学上基于既定的因果推断潜在结果框架和参数化g-computation，问题定义明确，所有模拟参数和模型规范均已提供。没有违反基本原则、信息缺失或含糊不清之处。因此，我们可以着手解决。\n\n解决方案按规定实现了 g-computation 算法。核心原则是使用观测数据集来建模一组混杂因素 $Z$、一项处理 $X$ 和一个结果 $Y$ 之间的关系。在一致性、条件可交换性和正值性假设下，因果 ATE 可以通过 g-formula 来识别：\n$$\n\\text{ATE} = \\mathbb{E}[Y^{(1)} - Y^{(0)}] = \\mathbb{E}_Z\\left[\\mathbb{E}[Y \\mid X=1, Z] - \\mathbb{E}[Y \\mid X=0, Z]\\right]\n$$\n此处，$Y^{(x)}$ 表示将处理 $X$ 设为值 $x$ 时的潜在结果。该公式表明，我们可以通过首先对给定处理和混杂因素的结果条件期望 $\\mathbb{E}[Y \\mid X, Z]$ 进行建模，然后将处理组（$X=1$）与对照组（$X=0$）下该期望的差异在混杂因素的边际分布 $P(Z)$ 上进行平均，从而计算出 ATE。\n\n我们的实现对每个用例遵循一个三步流程：\n1.  **数据模拟：** 根据该用例指定的结构因果模型生成一个大小为 $n$ 的数据集。这包括抽取混杂因素 $Z$，然后基于 $Z$ 抽取处理 $X$，最后基于 $X$ 和 $Z$ 抽取结果 $Y$。这个过程明确地制造了混杂，因为 $Z$ 是 $X$ 和 $Y$ 的共同原因。我们使用按规定设置种子的专用随机数生成器来确保可复现性。\n2.  **参数化模型拟合：** 我们使用模拟数据，通过最大似然估计（MLE）来估计两个模型的参数：\n    a.  结果模型，$\\widehat{\\mathbb{E}}[Y \\mid X, Z]$。对于连续结果（用例 1、3、4），这是一个线性回归模型，其 MLE 参数通过普通最小二乘法（OLS）找到。对于二元结果（用例 2），这是一个逻辑斯蒂回归模型，其参数通过使用牛顿-共轭梯度（`Newton-CG`）算法（牛顿-拉夫逊法的一种变体）对对数似然函数进行数值最大化来找到。\n    b.  混杂因素分布模型，$\\widehat{P}(Z)$。对于高斯混杂因素，均值 $\\mu_Z$ 和标准差 $\\sigma_Z$ 的 MLE 是样本均值和样本标准差。对于伯努利混杂因素 $Z_1$，成功概率 $p$ 的 MLE 是样本比例。\n3.  **蒙特卡洛积分：** g-formula 中对 $Z$ 的期望通过蒙特卡洛积分来近似。我们从其拟合分布 $\\widehat{P}(Z)$ 中抽取大量样本 $M$ 个混杂因素。对于每个模拟的混杂因素抽取 $Z_m$，我们计算预测的结果差异：\n    $$\n    \\Delta_m = \\widehat{\\mathbb{E}}[Y \\mid X=1, Z=Z_m] - \\widehat{\\mathbb{E}}[Y \\mid X=0, Z=Z_m]\n    $$\n    ATE 随后被估计为这些差异的平均值：\n    $$\n    \\widehat{\\text{ATE}} = \\frac{1}{M} \\sum_{m=1}^M \\Delta_m\n    $$\n\n对于形式为 $\\mathbb{E}[Y \\mid X, Z] = \\beta_0 + \\beta_1 X + f(Z)$ 的线性结果模型，预测的差异 $\\Delta_m$ 简化为一个常数：\n$$\n\\Delta_m = (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1 + f(Z_m)) - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0 + f(Z_m)) = \\hat{\\beta}_1\n$$\n在这些用例中（用例 1、3 和 4），估计的 ATE 因此等于处理变量的拟合系数 $\\hat{\\beta}_1$。尽管我们的实现仍然执行完整的蒙特卡洛程序，但这个结果可作为有价值的分析验证。\n\n对于用例 2 中的逻辑斯蒂结果模型 $\\mathbb{E}[Y \\mid X, Z] = \\sigma(\\alpha_0 + \\alpha_1 X + \\alpha_2 Z)$，其中 $\\sigma(\\cdot)$ 是逻辑斯蒂函数，其差异不是常数，而是依赖于 $Z$：\n$$\n\\Delta_m = \\sigma(\\hat{\\alpha}_0 + \\hat{\\alpha}_1 \\cdot 1 + \\hat{\\alpha}_2 Z_m) - \\sigma(\\hat{\\alpha}_0 + \\hat{\\alpha}_1 \\cdot 0 + \\hat{\\alpha}_2 Z_m)\n$$\n在这里，对 $Z$ 的分布进行蒙特卡洛积分是必不可少的，且不能简化。这个过程正确地在风险差异尺度上估计了 ATE，这与不可折叠的条件优势比不同。\n\n最终的 Python 程序封装了这整个逻辑。它为每个用例包含了独立的函数，遵守指定的参数、种子和模型结构。逻辑斯蒂回归求解器经过精心实现，通过向 `scipy.optimize.minimize` 提供负对数似然函数的解析梯度和海森矩阵，使用了指定的牛顿族优化方法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def g_computation_case_1():\n        \"\"\"\n        Solves Case 1: continuous outcome, linear model, Gaussian confounder.\n        \"\"\"\n        seed = 1\n        n = 10000\n        M = 100000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n        propensity = expit(0.2 + 1.0 * Z)\n        X = rng.binomial(1, propensity)\n        epsilon = rng.normal(loc=0.0, scale=1.0, size=n)\n        Y = 0.5 + 2.0 * X + 1.0 * Z + epsilon\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z\n        D_outcome = np.c_[np.ones(n), X, Z]\n        betas, _, _, _ = np.linalg.lstsq(D_outcome, Y, rcond=None)\n\n        # Fit confounder model: Z ~ N(mu, sigma^2)\n        mu_z_hat = np.mean(Z)\n        sigma_z_hat = np.std(Z)\n\n        # 3. Monte Carlo integration\n        Z_mc = rng.normal(loc=mu_z_hat, scale=sigma_z_hat, size=M)\n        \n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z_mc]\n        Y_hat_1 = D_mc_1 @ betas\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z_mc]\n        Y_hat_0 = D_mc_0 @ betas\n\n        ate = np.mean(Y_hat_1 - Y_hat_0)\n        return ate\n\n    def g_computation_case_2():\n        \"\"\"\n        Solves Case 2: binary outcome, logistic model, Gaussian confounder.\n        \"\"\"\n        seed = 2\n        n = 20000\n        M = 200000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n        propensity = expit(-0.1 + 1.2 * Z)\n        X = rng.binomial(1, propensity)\n        outcome_prob = expit(-0.5 + 1.0 * X + 0.8 * Z)\n        Y = rng.binomial(1, outcome_prob)\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z (logistic)\n        D_outcome = np.c_[np.ones(n), X, Z]\n\n        def neg_log_likelihood(alphas, D, y):\n            logits = D @ alphas\n            return np.sum(np.log(1 + np.exp(logits)) - y * logits)\n\n        def jacobian(alphas, D, y):\n            p = expit(D @ alphas)\n            return (p - y) @ D\n\n        def hessian(alphas, D, y):\n            p = expit(D @ alphas)\n            w = p * (1 - p)\n            return (D.T * w) @ D\n\n        initial_alphas = np.zeros(D_outcome.shape[1])\n        res = minimize(neg_log_likelihood, initial_alphas, args=(D_outcome, Y), \n                       method='Newton-CG', jac=jacobian, hess=hessian)\n        alphas = res.x\n        \n        # Fit confounder model: Z ~ N(mu, sigma^2)\n        mu_z_hat = np.mean(Z)\n        sigma_z_hat = np.std(Z)\n\n        # 3. Monte Carlo integration\n        Z_mc = rng.normal(loc=mu_z_hat, scale=sigma_z_hat, size=M)\n\n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z_mc]\n        Y_hat_prob_1 = expit(D_mc_1 @ alphas)\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z_mc]\n        Y_hat_prob_0 = expit(D_mc_0 @ alphas)\n\n        ate = np.mean(Y_hat_prob_1 - Y_hat_prob_0)\n        return ate\n\n    def g_computation_case_3():\n        \"\"\"\n        Solves Case 3: continuous outcome, polynomial model, mixed confounders.\n        \"\"\"\n        seed = 3\n        n = 15000\n        M = 150000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z1 = rng.binomial(1, 0.3, size=n)\n        Z2 = rng.normal(loc=1.0, scale=0.5, size=n)\n        propensity = expit(-0.2 + 1.0 * Z1 + 1.5 * Z2)\n        X = rng.binomial(1, propensity)\n        epsilon = rng.normal(loc=0.0, scale=0.5, size=n)\n        Y = 0.1 + 1.5 * X + 0.7 * Z1 + 0.5 * Z2 - 0.3 * Z2**2 + epsilon\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z1 + Z2 + Z2^2\n        Z2_sq = Z2**2\n        D_outcome = np.c_[np.ones(n), X, Z1, Z2, Z2_sq]\n        betas, _, _, _ = np.linalg.lstsq(D_outcome, Y, rcond=None)\n\n        # Fit confounder models\n        p_z1_hat = np.mean(Z1)\n        mu_z2_hat = np.mean(Z2)\n        sigma_z2_hat = np.std(Z2)\n\n        # 3. Monte Carlo integration\n        Z1_mc = rng.binomial(1, p_z1_hat, size=M)\n        Z2_mc = rng.normal(loc=mu_z2_hat, scale=sigma_z2_hat, size=M)\n        Z2_sq_mc = Z2_mc**2\n\n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z1_mc, Z2_mc, Z2_sq_mc]\n        Y_hat_1 = D_mc_1 @ betas\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z1_mc, Z2_mc, Z2_sq_mc]\n        Y_hat_0 = D_mc_0 @ betas\n\n        ate = np.mean(Y_hat_1 - Y_hat_0)\n        return ate\n\n    def g_computation_case_4():\n        \"\"\"\n        Solves Case 4: continuous outcome, linear model, near-positivity violation.\n        \"\"\"\n        seed = 4\n        n = 10000\n        M = 150000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n        propensity = expit(0.0 + 5.0 * Z)\n        X = rng.binomial(1, propensity)\n        epsilon = rng.normal(loc=0.0, scale=0.2, size=n)\n        Y = 0.0 + 0.5 * X + 0.2 * Z + epsilon\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z\n        D_outcome = np.c_[np.ones(n), X, Z]\n        betas, _, _, _ = np.linalg.lstsq(D_outcome, Y, rcond=None)\n\n        # Fit confounder model: Z ~ N(mu, sigma^2)\n        mu_z_hat = np.mean(Z)\n        sigma_z_hat = np.std(Z)\n\n        # 3. Monte Carlo integration\n        Z_mc = rng.normal(loc=mu_z_hat, scale=sigma_z_hat, size=M)\n        \n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z_mc]\n        Y_hat_1 = D_mc_1 @ betas\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z_mc]\n        Y_hat_0 = D_mc_0 @ betas\n\n        ate = np.mean(Y_hat_1 - Y_hat_0)\n        return ate\n\n    results = [\n        g_computation_case_1(),\n        g_computation_case_2(),\n        g_computation_case_3(),\n        g_computation_case_4()\n    ]\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3298691"}]}