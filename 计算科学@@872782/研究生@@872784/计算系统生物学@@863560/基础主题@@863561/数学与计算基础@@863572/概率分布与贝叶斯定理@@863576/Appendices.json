{"hands_on_practices": [{"introduction": "在贝叶斯推断中，我们最终得到的是参数的后验分布，它完整地描述了在观测到数据后我们对参数的所有认知。然而，在实际应用中，我们常常需要提供一个单一的数值作为参数的最佳估计。这个动手实践将引导你从第一性原理出发，探索如何将后验分布转化为一个具体的点估计值 [@problem_id:3340172]。你将发现，“最佳”估计的定义并非唯一，它取决于我们如何量化估计误差，即通过选择不同的损失函数（loss function）。通过这个练习，你将亲手推导出三种常用损失函数——二次损失、绝对值损失和0-1损失——分别对应着后验分布的均值、中位数和众数，从而深刻理解贝叶斯决策理论的核心思想。", "problem": "在计算系统生物学的一次单细胞信号传导实验中，每个细胞针对特定通路被分为转录活跃或不活跃，对细胞 $i$ 产生一个二元观测值 $x_{i} \\in \\{0,1\\}$。假设在给定未知激活概率 $\\theta \\in (0,1)$ 的条件下，$x_{1},\\dots,x_{n}$ 是条件独立同分布的，其抽样模型为 $x_{i} \\mid \\theta \\sim \\mathrm{Bernoulli}(\\theta)$。关于 $\\theta$ 的先验信念由一个贝塔分布建模，其密度与 $\\theta^{\\alpha_{0}-1} (1-\\theta)^{\\beta_{0}-1}$ 成正比，其中超参数 $\\alpha_{0}  0$ 和 $\\beta_{0}  0$。\n\n仅使用伯努利似然、贝塔先验、用于后验的贝叶斯定理，以及贝叶斯估计量是最小化后验期望损失 $\\mathbb{E}[L(\\theta, a) \\mid x_{1:n}]$ 的行动 $a$ 的定义，完成以下任务：\n\n1. 在下列每种损失函数下，从第一性原理推导 $\\theta$ 的贝叶斯估计量 $\\delta^{\\star}(x_{1:n})$：\n   - 平方损失 $L_{2}(\\theta,a) = (\\theta - a)^{2}$。\n   - 绝对损失 $L_{1}(\\theta,a) = |\\theta - a|$。\n   - 0-1损失 $L_{01}(\\theta,a) = \\mathbf{1}\\{|\\theta - a|  0\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。您的推导必须证明为使0-1损失对连续参数有良好定义所需的任何极限或近似论证。\n\n2. 现在，具体到均匀先验（$\\alpha_{0} = 1$ 和 $\\beta_{0} = 1$）和实验数据包含 $n = 3$ 个细胞，观察到 $s = 0$ 个活跃细胞的情况。使用贝叶斯定理求出后验参数 $(\\alpha,\\beta)$，然后计算您在第1部分中为该后验推导出的三个贝叶斯估计量的值。\n\n将您的最终答案以单行矩阵的形式给出，各项顺序为：平方损失估计量、绝对损失估计量、0-1损失估计量。不要四舍五入；请提供精确值。", "solution": "问题要求推导未知参数 $\\theta$ 的三个贝叶斯估计量，并随后针对特定数据集和先验计算它们的值。该过程包括两部分：从第一性原理进行一般性推导和具体应用。\n\n首先，我们建立贝叶斯框架。数据 $x_{1}, \\dots, x_{n}$ 是从参数为 $\\theta$ 的伯努利分布中抽取的条件独立同分布样本，即 $x_i|\\theta \\sim \\mathrm{Bernoulli}(\\theta)$。整个数据集 $x_{1:n} = (x_1, \\dots, x_n)$ 的似然函数由各个概率的乘积给出：\n$$P(x_{1:n} | \\theta) = \\prod_{i=1}^{n} P(x_i | \\theta) = \\prod_{i=1}^{n} \\theta^{x_i} (1-\\theta)^{1-x_i} = \\theta^{\\sum x_i} (1-\\theta)^{n - \\sum x_i}$$\n令 $s = \\sum_{i=1}^{n} x_i$ 为成功（活跃细胞）的总数。则似然函数为 $L(\\theta; s, n) = \\theta^s (1-\\theta)^{n-s}$。\n\n关于 $\\theta$ 的先验信念由贝塔分布 $\\theta \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)$ 建模，其概率密度函数 (PDF) 为：\n$$p(\\theta) = \\frac{\\theta^{\\alpha_0-1}(1-\\theta)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}$$\n其中 $B(\\alpha_0, \\beta_0)$ 是贝塔函数，作为归一化常数。\n\n根据贝叶斯定理，给定数据 $x_{1:n}$ 时 $\\theta$ 的后验分布与似然和先验的乘积成正比：\n$$p(\\theta|x_{1:n}) \\propto P(x_{1:n}|\\theta) p(\\theta) \\propto \\left( \\theta^s (1-\\theta)^{n-s} \\right) \\left( \\theta^{\\alpha_0-1}(1-\\theta)^{\\beta_0-1} \\right)$$\n$$p(\\theta|x_{1:n}) \\propto \\theta^{s+\\alpha_0-1} (1-\\theta)^{n-s+\\beta_0-1}$$\n这是贝塔分布的核，因此后验分布也是一个贝塔分布：\n$$\\theta | x_{1:n} \\sim \\mathrm{Beta}(\\alpha_0+s, \\beta_0+n-s)$$\n我们将后验参数记为 $\\alpha = \\alpha_0+s$ 和 $\\beta = \\beta_0+n-s$。\n\n$\\theta$ 的贝叶斯估计量 $\\delta^{\\star}(x_{1:n})$ 是一个使后验期望损失 $\\mathbb{E}[L(\\theta, a) \\mid x_{1:n}]$ 最小化的行动 $a$。\n$$\\delta^{\\star}(x_{1:n}) = \\arg\\min_{a} \\mathbb{E}[L(\\theta, a) \\mid x_{1:n}] = \\arg\\min_{a} \\int_{0}^{1} L(\\theta,a) p(\\theta|x_{1:n}) d\\theta$$\n\n**第1部分：贝叶斯估计量的推导**\n\n我们推导每种指定损失函数的估计量。\n\n1.  **平方损失: $L_{2}(\\theta, a) = (\\theta - a)^{2}$**\n    后验期望损失为 $R(a) = \\mathbb{E}[(\\theta-a)^2 | x_{1:n}] = \\int_0^1 (\\theta-a)^2 p(\\theta|x_{1:n}) d\\theta$。\n    为了找到使 $R(a)$ 最小化的 $a$ 值，我们对 $a$ 求导并令其为零。\n    $$\\frac{d R(a)}{da} = \\frac{d}{da} \\int_0^1 (\\theta^2 - 2a\\theta + a^2) p(\\theta|x_{1:n}) d\\theta$$\n    使用莱布尼茨法则在积分符号下求导：\n    $$\\frac{d R(a)}{da} = \\int_0^1 \\frac{\\partial}{\\partial a} (\\theta-a)^2 p(\\theta|x_{1:n}) d\\theta = \\int_0^1 -2(\\theta-a) p(\\theta|x_{1:n}) d\\theta$$\n    令导数为零：\n    $$-2 \\int_0^1 (\\theta-a) p(\\theta|x_{1:n}) d\\theta = 0$$\n    $$\\int_0^1 \\theta p(\\theta|x_{1:n}) d\\theta - \\int_0^1 a p(\\theta|x_{1:n}) d\\theta = 0$$\n    $$a \\int_0^1 p(\\theta|x_{1:n}) d\\theta = \\int_0^1 \\theta p(\\theta|x_{1:n}) d\\theta$$\n    由于 $p(\\theta|x_{1:n})$ 是一个概率密度函数，$\\int_0^1 p(\\theta|x_{1:n}) d\\theta = 1$。剩下：\n    $$a = \\int_0^1 \\theta p(\\theta|x_{1:n}) d\\theta = \\mathbb{E}[\\theta | x_{1:n}]$$\n    二阶导数为 $\\frac{d^2 R(a)}{da^2} = \\int_0^1 2 p(\\theta|x_{1:n}) d\\theta = 2  0$，确认这是一个最小值。\n    因此，平方损失下的贝叶斯估计量是后验均值。\n    $\\delta_{L_2}^{\\star}(x_{1:n}) = \\mathbb{E}[\\theta | x_{1:n}]$。\n\n2.  **绝对损失: $L_{1}(\\theta, a) = |\\theta - a|$**\n    后验期望损失为 $R(a) = \\mathbb{E}[|\\theta-a| | x_{1:n}] = \\int_0^1 |\\theta-a| p(\\theta|x_{1:n}) d\\theta$。\n    我们可以将积分在 $a$ 处拆分：\n    $$R(a) = \\int_0^a (a-\\theta) p(\\theta|x_{1:n}) d\\theta + \\int_a^1 (\\theta-a) p(\\theta|x_{1:n}) d\\theta$$\n    我们使用莱布尼茨法则对 $a$ 求导：\n    $$\\frac{d R(a)}{da} = \\left( (a-a)p(a|x_{1:n}) + \\int_0^a 1 \\cdot p(\\theta|x_{1:n}) d\\theta \\right) - \\left( (a-a)p(a|x_{1:n}) - \\int_a^1 1 \\cdot p(\\theta|x_{1:n}) d\\theta \\right)$$\n    $$\\frac{d R(a)}{da} = \\int_0^a p(\\theta|x_{1:n}) d\\theta - \\int_a^1 p(\\theta|x_{1:n}) d\\theta = P(\\theta \\le a | x_{1:n}) - P(\\theta  a | x_{1:n})$$\n    令导数为零可得：\n    $$P(\\theta \\le a | x_{1:n}) = P(\\theta  a | x_{1:n})$$\n    如果概率质量在 $a$ 的两侧均等分配，则该等式成立。这是后验分布中位数的定义。令 $m$ 为后验中位数。那么 $P(\\theta \\le m | x_{1:n}) = 1/2$。\n    因此，绝对损失下的贝叶斯估计量是后验中位数。\n    $\\delta_{L_1}^{\\star}(x_{1:n}) = \\text{median}(\\theta | x_{1:n})$。\n\n3.  **0-1损失: $L_{01}(\\theta, a) = \\mathbf{1}\\{|\\theta - a|  0\\}$**\n    对于连续参数 $\\theta$，所述的损失函数会导致困难，因为对于任何 $a$，都有 $P(\\theta=a|x_{1:n})=0$。期望损失将为 $\\mathbb{E}[\\mathbf{1}\\{\\theta \\neq a\\}|x_{1:n}] = P(\\theta \\neq a|x_{1:n}) = 1$ 对所有 $a$ 成立，这是没有用的。\n    按照指示，我们必须使用极限论证。考虑一个修正的损失函数 $L_{\\epsilon}(\\theta, a) = \\mathbf{1}\\{|\\theta - a|  \\epsilon\\}$，其中 $\\epsilon  0$ 是一个小数。我们希望最小化后验期望损失：\n    $$R(a) = \\mathbb{E}[L_{\\epsilon}(\\theta, a) | x_{1:n}] = \\int_0^1 \\mathbf{1}\\{|\\theta - a|  \\epsilon\\} p(\\theta|x_{1:n}) d\\theta = P(|\\theta - a|  \\epsilon | x_{1:n})$$\n    最小化 $P(|\\theta - a|  \\epsilon | x_{1:n})$ 等价于最大化其补集 $P(|\\theta - a| \\le \\epsilon | x_{1:n})$。\n    $$P(|\\theta - a| \\le \\epsilon | x_{1:n}) = P(a-\\epsilon \\le \\theta \\le a+\\epsilon | x_{1:n}) = \\int_{a-\\epsilon}^{a+\\epsilon} p(\\theta|x_{1:n}) d\\theta$$\n    对于一个小的 $\\epsilon$，并假设 $p(\\theta|x_{1:n})$ 在 $a$ 处连续，该积分可以近似为一个高为 $p(a|x_{1:n})$、宽为 $2\\epsilon$ 的矩形面积：\n    $$\\int_{a-\\epsilon}^{a+\\epsilon} p(\\theta|x_{1:n}) d\\theta \\approx p(a|x_{1:n}) \\cdot (2\\epsilon)$$\n    为了最大化这个近似概率，我们必须选择 $a$ 为使后验密度函数 $p(\\theta|x_{1:n})$ 最大化的值。根据定义，这个值是后验分布的众数。这个论证在 $\\epsilon \\to 0$ 的极限下成立。\n    因此，对于连续参数，0-1损失下的贝叶斯估计量是后验众数。\n    $\\delta_{L_{01}}^{\\star}(x_{1:n}) = \\arg\\max_{\\theta} p(\\theta|x_{1:n})$。\n\n**第2部分：具体案例应用**\n\n给定均匀先验，对应于 $\\alpha_0 = 1$ 和 $\\beta_0 = 1$。数据包含 $n=3$ 个细胞，其中 $s=0$ 个活跃细胞。\n\n使用前面推导的后验参数公式：\n- $\\alpha = s + \\alpha_0 = 0 + 1 = 1$\n- $\\beta = n - s + \\beta_0 = 3 - 0 + 1 = 4$\n\n后验分布为 $\\theta | x_{1:n} \\sim \\mathrm{Beta}(1, 4)$。\n后验概率密度函数为 $p(\\theta|x_{1:n}) = \\frac{\\theta^{1-1}(1-\\theta)^{4-1}}{B(1,4)} = \\frac{(1-\\theta)^3}{B(1,4)}$。\n贝塔函数的值为 $B(1,4) = \\frac{\\Gamma(1)\\Gamma(4)}{\\Gamma(1+4)} = \\frac{0! \\cdot 3!}{4!} = \\frac{1 \\cdot 6}{24} = \\frac{1}{4}$。\n所以，对于 $\\theta \\in (0,1)$，$p(\\theta|x_{1:n}) = 4(1-\\theta)^3$。\n\n我们现在计算这个 $\\mathrm{Beta}(1, 4)$ 后验分布的三个估计量。\n\n1.  **平方损失估计量（后验均值）：**\n    对于一个 $\\mathrm{Beta}(\\alpha, \\beta)$ 分布，均值为 $\\frac{\\alpha}{\\alpha+\\beta}$。\n    $$\\delta_{L_2}^{\\star} = \\frac{1}{1+4} = \\frac{1}{5}$$\n\n2.  **绝对损失估计量（后验中位数）：**\n    我们需要找到值 $m$ 使得 $\\int_0^m p(\\theta|x_{1:n}) d\\theta = \\frac{1}{2}$。\n    $$\\int_0^m 4(1-\\theta)^3 d\\theta = 4 \\left[ -\\frac{(1-\\theta)^4}{4} \\right]_0^m = - \\left[ (1-\\theta)^4 \\right]_0^m$$\n    $$= -((1-m)^4 - (1-0)^4) = 1 - (1-m)^4$$\n    令其等于 $\\frac{1}{2}$：\n    $$1 - (1-m)^4 = \\frac{1}{2}$$\n    $$(1-m)^4 = \\frac{1}{2}$$\n    由于 $m \\in(0,1)$，$1-m$ 必须为正。取正四次方根：\n    $$1-m = \\left(\\frac{1}{2}\\right)^{1/4}$$\n    $$\\delta_{L_1}^{\\star} = m = 1 - \\left(\\frac{1}{2}\\right)^{1/4}$$\n\n3.  **0-1损失估计量（后验众数）：**\n    我们需要找到使 $p(\\theta|x_{1:n}) = 4(1-\\theta)^3$ 在 $\\theta \\in (0,1)$ 上最大化的 $\\theta$ 值。\n    密度函数关于 $\\theta$ 的导数是 $\\frac{d}{d\\theta} 4(1-\\theta)^3 = -12(1-\\theta)^2$。\n    对于 $\\theta \\in (0,1)$，该导数始终为负，这意味着函数 $p(\\theta|x_{1:n})$ 在其支撑集上是严格递减的。因此，最大值在区间的左边界处取得，即在 $\\theta = 0$ 处。\n    （注：对于 $\\alpha, \\beta  1$，$\\mathrm{Beta}(\\alpha, \\beta)$ 分布众数的一般公式是 $\\frac{\\alpha-1}{\\alpha+\\beta-2}$。对于 $\\alpha=1, \\beta1$ 的情况，众数在 $0$ 处。）\n    $$\\delta_{L_{01}}^{\\star} = 0$$\n\n给定情况下的三个估计量是 $\\frac{1}{5}$、$1 - (\\frac{1}{2})^{1/4}$ 和 $0$。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{1}{5}  1 - \\left(\\frac{1}{2}\\right)^{\\frac{1}{4}}  0 \\end{pmatrix} } $$", "id": "3340172"}, {"introduction": "在构建贝叶斯模型时，一个关键且常常充满挑战的步骤是选择先验分布。这个选择不仅反映了我们的先验知识，也可能对模型结果，尤其是在数据量较少时，产生显著影响。本实践将带你深入探讨这一核心问题，通过一个在计算系统生物学中常见的转录计数模型，对比共轭先验（Gamma分布）与非共轭先验（Lognormal分布）的应用 [@problem_id:3340205]。你将通过编程实现，亲身体验共轭先验带来的计算便利性，以及非共轭先验提供的建模灵活性。更重要的是，你将量化比较在固定先验均值但改变先验分布的“尾部行为”时，后验估计对先验选择的敏感度，这对于在小样本情况下进行稳健的贝叶斯分析至关重要。", "problem": "在计算系统生物学中，来自同质细胞群体的信使RNA在固定时间窗口内的转录计数可以被建模为服从泊松过程的独立同分布。设每个细胞的转录速率由 $\\lambda$ 表示，单位为“转录本/分钟”，观测计数被建模为 $y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$，其中 $i \\in \\{1,\\dots,N\\}$，$N$ 是样本大小，$y_i \\in \\{0,1,2,\\dots\\}$。任务是比较 $\\lambda$ 的共轭先验与非共轭先验在小样本情况下如何影响后验，重点关注对先验尾部行为的敏感性。\n\n分析应基于贝叶斯定理，该定理指出后验密度与似然和先验的乘积成正比，即 $p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda)\\,p(\\lambda)$，其中 $\\mathbf{y} = (y_1,\\dots,y_N)$，$p(\\mathbf{y} \\mid \\lambda)$ 是泊松似然的乘积。考虑两种先验：\n- 共轭先验：$\\lambda \\sim \\mathrm{Gamma}(a,b)$，由形状参数 $a0$ 和率参数 $b0$ 参数化。\n- 非共轭先验：$\\log \\lambda \\sim \\mathcal{N}(\\mu,\\sigma^2)$，即 $\\lambda$ 服从对数正态分布，其参数为 $\\mu \\in \\mathbb{R}$ 和 $\\sigma  0$。\n\n从第一性原理出发，推导共轭先验的后验分布，以及非共轭先验下后验期望的一个可计算表达式。然后，为小样本情况定义并计算以下敏感性度量：\n- 对于对数正态先验，当对数尺度标准差在“轻尾”选择 $\\sigma_{\\mathrm{light}}$ 和“重尾”选择 $\\sigma_{\\mathrm{heavy}}$ 之间变化时，后验均值的绝对差值，同时通过 $\\mu = \\log(m_0) - \\sigma^2/2$ 相应地调整 $\\mu$ 以将先验均值固定在 $m_0$。将其表示为 $\\Delta_{\\mathrm{logN}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{light}}]\\right|$。\n- 对于Gamma先验，当形状参数在“轻尾”选择 $a_{\\mathrm{light}}$ 和“重尾”选择 $a_{\\mathrm{heavy}}$ 之间变化时，后验均值的绝对差值，同时通过设置 $b = a/m_0$ 将先验均值固定在 $m_0$。将其表示为 $\\Delta_{\\mathrm{Gamma}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{light}}]\\right|$。\n- 比例 $R = \\Delta_{\\mathrm{logN}} / \\Delta_{\\mathrm{Gamma}}$ 作为相对敏感性的无量纲度量。\n\n所有关于 $\\lambda$ 的后验均值和差异必须以“转录本/分钟”为单位表示。比例 $R$ 是无量纲的。此问题不涉及角度。\n\n实现一个程序，给定以下测试套件，为每个测试用例计算 $\\Delta_{\\mathrm{logN}}$、$\\Delta_{\\mathrm{Gamma}}$ 和 $R$ 作为浮点数。使用数值稳定的积分方法来计算根据贝叶斯定理推导出的非共轭后验期望。最终输出必须是单行，包含一个列表的列表，其中每个内部列表对应一个测试用例，形式为 $[\\Delta_{\\mathrm{logN}}, \\Delta_{\\mathrm{Gamma}}, R]$。\n\n测试套件（每个用例指定 $\\mathbf{y}$, $m_0$, $\\sigma_{\\mathrm{light}}$, $\\sigma_{\\mathrm{heavy}}$, $a_{\\mathrm{light}}$, $a_{\\mathrm{heavy}}$）：\n1. 用例 1：$N=2$，$\\mathbf{y}=[0,1]$，$m_0=1.0$，$\\sigma_{\\mathrm{light}}=0.25$，$\\sigma_{\\mathrm{heavy}}=1.0$，$a_{\\mathrm{light}}=50.0$，$a_{\\mathrm{heavy}}=1.0$。\n2. 用例 2：$N=5$，$\\mathbf{y}=[0,0,1,0,2]$，$m_0=0.5$，$\\sigma_{\\mathrm{light}}=0.2$，$\\sigma_{\\mathrm{heavy}}=1.2$，$a_{\\mathrm{light}}=40.0$，$a_{\\mathrm{heavy}}=0.5$。\n3. 用例 3：$N=1$，$\\mathbf{y}=[5]$，$m_0=4.0$，$\\sigma_{\\mathrm{light}}=0.3$，$\\sigma_{\\mathrm{heavy}}=0.9$，$a_{\\mathrm{light}}=20.0$，$a_{\\mathrm{heavy}}=1.0$。\n4. 用例 4：$N=3$，$\\mathbf{y}=[0,0,0]$，$m_0=0.2$，$\\sigma_{\\mathrm{light}}=0.3$，$\\sigma_{\\mathrm{heavy}}=1.5$，$a_{\\mathrm{light}}=50.0$，$a_{\\mathrm{heavy}}=0.3$。\n5. 用例 5：$N=2$，$\\mathbf{y}=[50,60]$，$m_0=40.0$，$\\sigma_{\\mathrm{light}}=0.5$，$\\sigma_{\\mathrm{heavy}}=1.0$，$a_{\\mathrm{light}}=30.0$，$a_{\\mathrm{heavy}}=1.0$。\n\n对于每个用例，令 $S=\\sum_{i=1}^{N} y_i$ 表示总计数。你的程序应生成单行输出，包含一个用方括号括起来、无空格、逗号分隔的列表的列表，例如 $[[x_1,y_1,z_1],[x_2,y_2,z_2],\\dots]$，其中 $x_i$、$y_i$ 和 $z_i$ 是用例 $i$ 的三个浮点数，单位如上所述。", "solution": "该问题要求对mRNA转录计数的泊松模型进行贝叶斯分析。我们必须比较转录速率 $\\lambda$ 的后验均值对先验分布选择的敏感性。具体来说，我们比较共轭Gamma先验和非共轭对数正态先验，重点关注在小样本情况下每种先验的尾部行为如何影响后验。\n\n分析分为两个主要步骤。首先，我们推导两种先验情景下 $\\lambda$ 的后验均值的解析形式或可计算形式。其次，我们以数值方式实现这些计算，以针对给定的测试套件计算指定的敏感性度量。\n\n设观测数据为 $\\mathbf{y} = (y_1, \\dots, y_N)$，其中每个 $y_i \\in \\{0, 1, 2, \\dots\\}$ 是从速率为 $\\lambda$ 的泊松分布中的一个独立抽样。\n$$y_i \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$$\n整个数据集 $\\mathbf{y}$ 的似然是各个泊松概率质量函数的乘积：\n$$p(\\mathbf{y} \\mid \\lambda) = \\prod_{i=1}^N \\frac{\\lambda^{y_i} e^{-\\lambda}}{y_i!} = \\frac{\\lambda^{\\sum y_i} e^{-N\\lambda}}{\\prod y_i!}$$\n对于贝叶斯推断，我们关心的是作为参数 $\\lambda$ 的函数的似然。我们可以忽略不依赖于 $\\lambda$ 的项。设 $S = \\sum_{i=1}^N y_i$ 为计数总和。那么似然与以下表达式成正比：\n$$p(\\mathbf{y} \\mid \\lambda) \\propto \\lambda^S e^{-N\\lambda}$$\n根据贝叶斯定理，$\\lambda$ 的后验分布为 $p(\\lambda \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\lambda) p(\\lambda)$，其中 $p(\\lambda)$ 是 $\\lambda$ 的先验分布。\n\n### 共轭先验分析：Gamma-泊松模型\n\n泊松似然的共轭先验是Gamma分布。设 $\\lambda$ 的先验为：\n$$\\lambda \\sim \\mathrm{Gamma}(a, b)$$\n其中 $a  0$ 是形状参数，$b  0$ 是率参数。其概率密度函数 (PDF) 为 $p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{a-1} e^{-b\\lambda}$，它与 $\\lambda^{a-1} e^{-b\\lambda}$ 成正比。\n\n后验分布通过将似然与先验相乘得到：\n$$p(\\lambda \\mid \\mathbf{y}) \\propto (\\lambda^S e^{-N\\lambda}) (\\lambda^{a-1} e^{-b\\lambda}) = \\lambda^{a+S-1} e^{-(b+N)\\lambda}$$\n此表达式是Gamma分布的核。因此，后验分布也是一个Gamma分布：\n$$\\lambda \\mid \\mathbf{y} \\sim \\mathrm{Gamma}(a', b')$$\n后验形状为 $a' = a+S$，后验率为 $b' = b+N$。\n\n$\\mathrm{Gamma}(a', b')$ 分布的均值是 $\\frac{a'}{b'}$。因此，$\\lambda$ 的后验均值是：\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{a+S}{b+N}$$\n问题规定先验均值固定为 $m_0$。先验 $\\mathrm{Gamma}(a, b)$ 分布的均值为 $\\mathbb{E}[\\lambda] = a/b$。因此我们有约束 $m_0 = a/b$，这意味着 $b = a/m_0$。将此代入后验均值公式，得到：\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{a+S}{(a/m_0) + N}$$\n这就是用于计算Gamma先验的后验均值的表达式。\n\n### 非共轭先验分析：对数正态-泊松模型\n\n$\\lambda$ 的非共轭先验被指定为对数正态分布。这意味着 $\\log \\lambda$ 服从正态分布：\n$$\\log \\lambda \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n$\\lambda$ 的概率密度函数为 $p(\\lambda) = \\frac{1}{\\lambda \\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right)$，其中 $\\lambda  0$。\n\n后验分布同样与似然和先验的乘积成正比：\n$$p(\\lambda \\mid \\mathbf{y}) \\propto (\\lambda^S e^{-N\\lambda}) \\left( \\frac{1}{\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) \\right)$$\n$$p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{S-1} e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right)$$\n这个后验不是一个标准的、有名称的分布。其后验均值必须根据其定义来计算：\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{\\int_0^\\infty \\lambda \\cdot p(\\lambda \\mid \\mathbf{y}) d\\lambda}{\\int_0^\\infty p(\\lambda \\mid \\mathbf{y}) d\\lambda}$$\n代入未归一化的后验密度，我们得到：\n$$\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = \\frac{\\int_0^\\infty \\lambda^S e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) d\\lambda}{\\int_0^\\infty \\lambda^{S-1} e^{-N\\lambda} \\exp\\left(-\\frac{(\\log \\lambda - \\mu)^2}{2\\sigma^2}\\right) d\\lambda}$$\n这些积分没有闭式解，必须进行数值计算。为了提高数值稳定性，我们进行变量替换。令 $x = \\log \\lambda$，这意味着 $\\lambda = e^x$ 且 $d\\lambda = e^x dx$。积分域从 $(0, \\infty)$ 变为 $(-\\infty, \\infty)$。我们将分子积分表示为 $I_{\\text{num}}$，分母积分表示为 $I_{\\text{den}}$。\n$$I_{\\text{num}} = \\int_{-\\infty}^{\\infty} (e^x)^S e^{-Ne^x} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) e^x dx = \\int_{-\\infty}^{\\infty} \\exp\\left(x(S+1) - Ne^x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\n$$I_{\\text{den}} = \\int_{-\\infty}^{\\infty} (e^x)^{S-1} e^{-Ne^x} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) e^x dx = \\int_{-\\infty}^{\\infty} \\exp\\left(xS - Ne^x - \\frac{(x-\\mu)^2}{2\\sigma^2}\\right) dx$$\n后验均值则为 $\\mathbb{E}[\\lambda \\mid \\mathbf{y}] = I_{\\text{num}} / I_{\\text{den}}$。这些积分是良态的，可以使用数值积分方法可靠地计算。\n\n问题将先验均值约束为 $m_0$。一个 $\\mathrm{Lognormal}(\\mu, \\sigma^2)$ 分布的均值是 $\\mathbb{E}[\\lambda] = e^{\\mu + \\sigma^2/2}$。将其设为 $m_0$ 并求解 $\\mu$ 可得：\n$$\\mu = \\log(m_0) - \\frac{\\sigma^2}{2}$$\n这个关于 $\\mu$ 的表达式被代入被积函数中。\n\n### 敏感性度量的计算\n\n对于每个测试用例，我们计算以下量：\n$1$. $\\Delta_{\\mathrm{logN}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, \\sigma_{\\mathrm{light}}]\\right|$:\n    - 我们使用对数正态先验计算两个后验均值。\n    - 对于“轻尾”情况，使用 $\\sigma = \\sigma_{\\mathrm{light}}$ 和 $\\mu_{\\mathrm{light}} = \\log(m_0) - \\sigma_{\\mathrm{light}}^2/2$。\n    - 对于“重尾”情况，使用 $\\sigma = \\sigma_{\\mathrm{heavy}}$ 和 $\\mu_{\\mathrm{heavy}} = \\log(m_0) - \\sigma_{\\mathrm{heavy}}^2/2$。\n    - 后验均值通过数值积分求得，$\\Delta_{\\mathrm{logN}}$ 是其绝对差。\n\n$2$. $\\Delta_{\\mathrm{Gamma}} = \\left|\\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{heavy}}] - \\mathbb{E}[\\lambda \\mid \\mathbf{y}, a_{\\mathrm{light}}]\\right|$:\n    - 我们使用Gamma先验计算两个后验均值。具有固定均值 $m_0$ 的Gamma先验的方差是 $m_0^2/a$。因此，较小的 $a$ 产生较重的尾部。\n    - 对于“轻尾”情况，使用 $a = a_{\\mathrm{light}}$ 和 $b_{\\mathrm{light}} = a_{\\mathrm{light}}/m_0$。后验均值为 $\\frac{a_{\\mathrm{light}}+S}{b_{\\mathrm{light}}+N}$。\n    - 对于“重尾”情况，使用 $a = a_{\\mathrm{heavy}}$ 和 $b_{\\mathrm{heavy}} = a_{\\mathrm{heavy}}/m_0$。后验均值为 $\\frac{a_{\\mathrm{heavy}}+S}{b_{\\mathrm{heavy}}+N}$。\n    - $\\Delta_{\\mathrm{Gamma}}$ 是这两个解析值之间的绝对差。\n\n$3$. $R = \\Delta_{\\mathrm{logN}} / \\Delta_{\\mathrm{Gamma}}$:\n    - 这是两个敏感性度量的比率，提供了一个无量纲的比较。\n\n该实现将包含一个遍历测试用例的主循环。在循环内部，辅助函数将计算两种先验族在各自的轻尾和重尾参数化下的后验均值。然后计算并存储最终的度量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef calculate_gamma_posterior_mean(S, N, a, b):\n    \"\"\"\n    Calculates the posterior mean for a Gamma-Poisson model.\n    \n    Args:\n        S (int): Sum of observed counts.\n        N (int): Number of observations.\n        a (float): Shape parameter of the Gamma prior.\n        b (float): Rate parameter of the Gamma prior.\n        \n    Returns:\n        float: The posterior mean of lambda.\n    \"\"\"\n    posterior_shape = a + S\n    posterior_rate = b + N\n    return posterior_shape / posterior_rate\n\ndef calculate_lognormal_posterior_mean(S, N, mu, sigma):\n    \"\"\"\n    Calculates the posterior mean for a Lognormal-Poisson model via numerical integration.\n    \n    Args:\n        S (int): Sum of observed counts.\n        N (int): Number of observations.\n        mu (float): Location parameter of the Lognormal prior.\n        sigma (float): Scale parameter of the Lognormal prior.\n        \n    Returns:\n        float: The posterior mean of lambda.\n    \"\"\"\n    # Numerically unstable to integrate exp(log_integrand) directly.\n    # We define the log of the integrand and integrate its exponential.\n    # The change of variables is x = log(lambda).\n    \n    # log_integrand for the numerator of the posterior mean expectation\n    def log_integrand_num(x):\n        k = S + 1\n        return k * x - N * np.exp(x) - ((x - mu)**2) / (2 * sigma**2)\n\n    # log_integrand for the denominator (normalization constant)\n    def log_integrand_den(x):\n        k = S\n        return k * x - N * np.exp(x) - ((x - mu)**2) / (2 * sigma**2)\n\n    # To avoid overflow/underflow, find the peak of the log integrand and subtract it\n    # This integration is well-behaved, so direct integration is feasible,\n    # but this is a more robust approach if needed. However, quad is robust enough\n    # for these test cases.\n    integrand_num = lambda x: np.exp(log_integrand_num(x))\n    integrand_den = lambda x: np.exp(log_integrand_den(x))\n    \n    # quad returns (integral, error)\n    integral_num, _ = quad(integrand_num, -np.inf, np.inf)\n    integral_den, _ = quad(integrand_den, -np.inf, np.inf)\n    \n    if integral_den == 0:\n        # This case is unlikely with proper priors but is a safeguard.\n        return np.nan\n        \n    return integral_num / integral_den\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # Case 1: (y, m0, sigma_light, sigma_heavy, a_light, a_heavy)\n        (np.array([0, 1]), 1.0, 0.25, 1.0, 50.0, 1.0),\n        # Case 2\n        (np.array([0, 0, 1, 0, 2]), 0.5, 0.2, 1.2, 40.0, 0.5),\n        # Case 3\n        (np.array([5]), 4.0, 0.3, 0.9, 20.0, 1.0),\n        # Case 4\n        (np.array([0, 0, 0]), 0.2, 0.3, 1.5, 50.0, 0.3),\n        # Case 5\n        (np.array([50, 60]), 40.0, 0.5, 1.0, 30.0, 1.0),\n    ]\n\n    results = []\n    for y, m0, sigma_light, sigma_heavy, a_light, a_heavy in test_cases:\n        S = np.sum(y)\n        N = len(y)\n\n        # Lognormal Prior Calculation\n        mu_light = np.log(m0) - (sigma_light**2) / 2\n        mu_heavy = np.log(m0) - (sigma_heavy**2) / 2\n        \n        mean_logN_light = calculate_lognormal_posterior_mean(S, N, mu_light, sigma_light)\n        mean_logN_heavy = calculate_lognormal_posterior_mean(S, N, mu_heavy, sigma_heavy)\n        \n        delta_logN = abs(mean_logN_heavy - mean_logN_light)\n\n        # Gamma Prior Calculation\n        b_light = a_light / m0\n        b_heavy = a_heavy / m0\n        \n        mean_gamma_light = calculate_gamma_posterior_mean(S, N, a_light, b_light)\n        mean_gamma_heavy = calculate_gamma_posterior_mean(S, N, a_heavy, b_heavy)\n        \n        delta_gamma = abs(mean_gamma_heavy - mean_gamma_light)\n\n        # Ratio Calculation\n        # Avoid division by zero, though unlikely in this problem context.\n        if delta_gamma == 0:\n            ratio = np.inf if delta_logN != 0 else 0.0\n        else:\n            ratio = delta_logN / delta_gamma\n            \n        results.append([delta_logN, delta_gamma, ratio])\n\n    # Format output to a list of lists string with no spaces.\n    # Ex: [[1.0,2.0,3.0],[4.0,5.0,6.0]]\n    formatted_results = []\n    for res in results:\n        formatted_sublist = \"[\" + \",\".join(map(str, res)) + \"]\"\n        formatted_results.append(formatted_sublist)\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3340205"}, {"introduction": "真实世界的生物系统往往具有多层次的复杂结构，单一的概率模型可能不足以捕捉其内在机制。分层贝叶斯模型（Hierarchical Bayesian Models）为处理这种复杂性提供了强大的框架。在这个高级实践中，你将综合运用之前学到的概念，构建一个用于从测序数据中推断基因组拷贝数变异（CNV）的分层模型 [@problem_id:3340191]。你不仅需要利用贝叶斯定理在模型的不同层级上进行推断，计算拷贝数状态的后验概率，还要进一步生成后验预测分布（posterior predictive distribution）。最后，你将学习并实践一种重要的模型评估技术——概率积分变换（Probability Integral Transform, PIT），来检验你的模型预测是否“校准”良好。这个练习将引导你走完从复杂模型构建、推断到关键的预测与验证的完整贝叶斯分析流程。", "problem": "考虑一个拷贝数变异 (CNV) 的计算系统生物学模型，其中跨越 $K$ 个基因组区间的带噪声测序覆盖度被概括为一个计数向量 $y = (y_1,\\dots,y_K)$，其总和为 $n = \\sum_{i=1}^K y_i$。假设以下基于标准分布定义和 Bayes 定理的概率生成结构：\n\n- 对于一个离散的拷贝数状态 $c \\in \\mathcal{C}$，其先验概率为 $p(c)$，潜在的、与区间相关的覆盖度分数 $p = (p_1,\\dots,p_K)$ 服从一个 Dirichlet 分布，其集中度参数与拷贝数状态相关联：\n  $$\n  p \\mid c \\sim \\mathrm{Dirichlet}\\big(\\alpha(c)\\big), \\quad \\alpha(c) = \\kappa \\, c \\, \\omega,\n  $$\n  其中 $\\kappa  0$ 是一个集中度缩放常数，$\\omega = (\\omega_1,\\dots,\\omega_K)$ 是一个固定的非负权重向量，满足 $\\sum_{i=1}^K \\omega_i = 1$。\n\n- 在给定 $p$ 的条件下，区间计数 $y$ 服从一个多项分布：\n  $$\n  y \\mid p \\sim \\mathrm{Multinomial}(n, p).\n  $$\n\n该模型通过对 $p$ 积分，导出了给定 $c$ 时 $y$ 的 Dirichlet-Multinomial 边际分布，并允许对给定 $y$ 和 $c$ 时 $p$ 的后验进行共轭更新。使用 Bayes 定理，给定观测计数 $y$ 时拷贝数 $c$ 的后验概率与先验 $p(c)$ 和 Dirichlet-Multinomial 边际似然 $p(y \\mid c)$ 的乘积成正比。\n\n你的任务是：\n\n1.  在上述分层模型下，从第一性原理出发，从 Bayes 定理和核心分布定义开始，推导对于 $c \\in \\mathcal{C}$ 的后验概率 $p(c \\mid y)$，不使用超出这些基础的快捷公式。\n\n2.  对于一个给定的新计数向量 $y_{\\text{new}} = (y_{\\text{new},1},\\dots,y_{\\text{new},K})$，其总和为 $n_{\\text{new}} = \\sum_{i=1}^K y_{\\text{new},i}$，在该分层混合模型下计算后验预测概率 $p(y_{\\text{new}} \\mid y)$，该模型对 $c$ 和 $p$ 的不确定性都进行了积分。\n\n3.  使用概率积分变换 (PIT) 评估预测校准性。对于离散计数，对每个区间 $k \\in \\{1,\\dots,K\\}$ 中的边际计数使用随机化 PIT，其中边际后验预测分布是通过对 Dirichlet 后验中的 $p_k$ 及其补集积分得到的 Beta-二项分布。在每个测试用例中聚合 $K$ 个 PIT 值，并计算相对于 $\\mathrm{Uniform}(0,1)$ 分布的单样本 Kolmogorov-Smirnov 统计量，以总结校准性。\n\n在一个程序中实现这些计算，对每个测试用例执行以下操作：\n\n-   计算在 $\\mathcal{C}$ 上的后验 $p(c \\mid y)$ 并报告后验众数 $\\hat{c}$。\n-   计算后验预测概率 $p(y_{\\text{new}} \\mid y)$ 并以浮点数形式报告其自然对数。\n-   计算每个区间的随机化 PIT 值，并以浮点数形式报告 Kolmogorov-Smirnov 统计量。\n\n使用以下测试套件，其中 $K = 4$ 且 $\\omega = (0.4, 0.3, 0.2, 0.1)$：\n\n-   测试用例 1 (一般情况):\n    -   $\\mathcal{C} = \\{1, 2, 3\\}$，先验 $p(c) = (1/3, 1/3, 1/3)$，$\\kappa = 8$。\n    -   观测值 $y = (38, 31, 21, 10)$，其中 $n = 100$。\n    -   新值 $y_{\\text{new}} = (33, 26, 15, 6)$，其中 $n_{\\text{new}} = 80$。\n\n-   测试用例 2 (无观测计数的边界情况):\n    -   $\\mathcal{C} = \\{1, 2, 3\\}$，先验 $p(c) = (0.2, 0.5, 0.3)$，$\\kappa = 6$。\n    -   观测值 $y = (0, 0, 0, 0)$，其中 $n = 0$。\n    -   新值 $y_{\\text{new}} = (22, 15, 9, 4)$，其中 $n_{\\text{new}} = 50$。\n\n-   测试用例 3 (高集中度和大计数):\n    -   $\\mathcal{C} = \\{1, 3\\}$，先验 $p(c) = (0.5, 0.5)$，$\\kappa = 12$。\n    -   观测值 $y = (240, 180, 120, 60)$，其中 $n = 600$。\n    -   新值 $y_{\\text{new}} = (120, 90, 60, 30)$，其中 $n_{\\text{new}} = 300$。\n\n通过 PIT 进行校准必须使用为每个区间 $k$ 计算的随机化值 $u_k = F_k(y_{\\text{new},k}^- ) + r_k \\cdot \\big(F_k(y_{\\text{new},k}) - F_k(y_{\\text{new},k}^-)\\big)$，其中 $F_k(\\cdot)$ 是 Beta-二项分布的混合边际累积分布函数，$F_k(t^-)$ 表示恰好在 $t$ 以下的累积概率，而 $r_k$ 是从 $\\mathrm{Uniform}(0,1)$ 中独立抽取的样本。为确保确定性输出，请固定随机种子并分别为每个测试用例报告 Kolmogorov-Smirnov 统计量。\n\n最终输出要求：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为\n$[\\hat{c}_1, \\log p(y_{\\text{new},1} \\mid y_1), \\mathrm{KS}_1, \\hat{c}_2, \\log p(y_{\\text{new},2} \\mid y_2), \\mathrm{KS}_2, \\hat{c}_3, \\log p(y_{\\text{new},3} \\mid y_3), \\mathrm{KS}_3]$，\n其中下标索引测试用例。所有报告的值必须是整数或浮点数。不涉及物理单位或角度，小数必须以不带百分号的形式表示。", "solution": "用户需要一个关于计算系统生物学中分层贝叶斯建模问题的解决方案。我将首先从第一性原理推导必要的数学公式，然后基于这些推导提供一个实现。\n\n### 方法与推导\n\n该问题要求推导和计算三个量：拷贝数状态的后验概率、新观测值的后验预测概率，以及一个基于概率积分变换 (PIT) 的校准度量。\n\n#### 1. 后验概率 $p(c \\mid y)$\n\n在给定观测计数 $y$ 的条件下，拷贝数状态 $c$ 的后验概率由 Bayes 定理给出：\n$$\np(c \\mid y) = \\frac{p(y \\mid c) p(c)}{p(y)} = \\frac{p(y \\mid c) p(c)}{\\sum_{c' \\in \\mathcal{C}} p(y \\mid c') p(c')}\n$$\n该计算的核心是边际似然 $p(y \\mid c)$，它是通过对潜在概率 $p$ 积分得到的：\n$$\np(y \\mid c) = \\int p(y \\mid p) p(p \\mid c) \\, d p\n$$\n该模型指定了多项似然和 Dirichlet 先验：\n-   似然: $p(y \\mid p) = \\frac{n!}{\\prod_{k=1}^K y_k!} \\prod_{k=1}^K p_k^{y_k}$，其中 $n = \\sum_k y_k$。\n-   先验: $p(p \\mid c) = \\frac{\\Gamma(\\sum_{k=1}^K \\alpha_k(c))}{\\prod_{k=1}^K \\Gamma(\\alpha_k(c))} \\prod_{k=1}^K p_k^{\\alpha_k(c) - 1}$，其中 $\\alpha(c) = \\kappa c \\omega$。\n\n结合这些，被积函数变为：\n$$\np(y \\mid p) p(p \\mid c) = \\frac{n!}{\\prod_k y_k!} \\frac{\\Gamma(\\sum_k \\alpha_k(c))}{\\prod_k \\Gamma(\\alpha_k(c))} \\prod_{k=1}^K p_k^{y_k + \\alpha_k(c) - 1}\n$$\n通过识别出项 $\\prod_{k=1}^K p_k^{y_k + \\alpha_k(c) - 1}$ 是参数为 $y + \\alpha(c)$ 的 Dirichlet 分布的核，可以求解对 $p$ 的积分。一个 Dirichlet 核的积分是其归一化常数的倒数：\n$$\n\\int \\prod_{k=1}^K p_k^{y_k + \\alpha_k(c) - 1} \\, d p = \\frac{\\prod_k \\Gamma(y_k + \\alpha_k(c))}{\\Gamma\\left(\\sum_k (y_k + \\alpha_k(c))\\right)} = \\frac{\\prod_k \\Gamma(y_k + \\alpha_k(c))}{\\Gamma(n + \\sum_k \\alpha_k(c))}\n$$\n将此代回，得到边际似然，即 Dirichlet-Multinomial 分布的概率质量函数 (PMF)：\n$$\np(y \\mid c) = \\frac{n!}{\\prod_k y_k!} \\frac{\\Gamma(\\alpha_0(c))}{\\prod_k \\Gamma(\\alpha_k(c))} \\frac{\\prod_k \\Gamma(y_k + \\alpha_k(c))}{\\Gamma(n + \\alpha_0(c))}\n$$\n其中 $\\alpha_0(c) = \\sum_k \\alpha_k(c) = \\kappa c \\sum_k \\omega_k = \\kappa c$。\n\n在实践中，计算在对数空间中执行以防止数值下溢。设 $L(c) = p(y \\mid c)p(c)$。那么对数后验为：\n$$\n\\log p(c \\mid y) = \\log L(c) - \\log\\left(\\sum_{c' \\in \\mathcal{C}} L(c')\\right)\n$$\n后验众数为 $\\hat{c} = \\arg\\max_{c \\in \\mathcal{C}} p(c \\mid y)$。\n\n#### 2. 后验预测概率 $p(y_{\\text{new}} \\mid y)$\n\n在给定原始数据 $y$ 的情况下，新计数向量 $y_{\\text{new}}$ 的后验预测概率是通过对所有不确定量（即拷贝数状态 $c$ 和潜在比例 $p$）进行边际化得到的。使用全概率定律，我们可以将其写为关于 $c$ 的后验的混合形式：\n$$\np(y_{\\text{new}} \\mid y) = \\sum_{c \\in \\mathcal{C}} p(y_{\\text{new}}, c \\mid y) = \\sum_{c \\in \\mathcal{C}} p(y_{\\text{new}} \\mid y, c) p(c \\mid y)\n$$\n项 $p(c \\mid y)$ 是上一节中推导的后验概率。项 $p(y_{\\text{new}} \\mid y, c)$ 是固定状态 $c$ 的预测概率，这需要对给定 $y$ 和 $c$ 的 $p$ 的后验分布进行积分：\n$$\np(y_{\\text{new}} \\mid y, c) = \\int p(y_{\\text{new}} \\mid p) p(p \\mid y, c) \\, dp\n$$\n由于 Dirichlet 先验和多项似然的共轭性，$p$ 的后验也是一个 Dirichlet 分布：\n$$\np(p \\mid y, c) \\propto p(y \\mid p) p(p \\mid c) \\propto \\left(\\prod_k p_k^{y_k}\\right) \\left(\\prod_k p_k^{\\alpha_k(c) - 1}\\right) = \\prod_k p_k^{y_k + \\alpha_k(c) - 1}\n$$\n因此，$p \\mid y, c \\sim \\mathrm{Dirichlet}(\\alpha(c) + y)$。\n\n$p(y_{\\text{new}} \\mid y, c)$ 的计算与边际似然的推导类似，但使用后验 $\\mathrm{Dirichlet}(\\alpha(c) + y)$ 作为新的先验。结果是另一个 Dirichlet-Multinomial PMF：\n$$\np(y_{\\text{new}} \\mid y, c) \\text{ 是 } \\mathrm{DirichletMultinomial}(n_{\\text{new}}, \\alpha(c)+y) \\text{ 的 PMF}\n$$\n明确地，令 $\\alpha'(c) = \\alpha(c)+y$ 且 $\\alpha'_0(c) = \\sum_k \\alpha'_k(c) = n+\\alpha_0(c)$：\n$$\np(y_{\\text{new}} \\mid y, c) = \\frac{n_{\\text{new}}!}{\\prod_k y_{\\text{new},k}!} \\frac{\\Gamma(\\alpha'_0(c))}{\\prod_k \\Gamma(\\alpha'_k(c))} \\frac{\\prod_k \\Gamma(y_{\\text{new},k} + \\alpha'_k(c))}{\\Gamma(n_{\\text{new}} + \\alpha'_0(c))}\n$$\n最终的后验预测概率是这些概率的加权平均，权重由后验 $p(c \\mid y)$ 给出。在对数空间中，为了数值稳定性，这需要 log-sum-exp 操作：\n$$\n\\log p(y_{\\text{new}} \\mid y) = \\text{logsumexp}_{c \\in \\mathcal{C}} \\left( \\log p(y_{\\text{new}} \\mid y, c) + \\log p(c \\mid y) \\right)\n$$\n\n#### 3. 通过随机化 PIT 进行预测校准\n\n概率积分变换 (PIT) 是一种评估概率预测校准性的方法。对于具有累积分布函数 (CDF) $F_X$ 的连续随机变量 $X$，随机变量 $U = F_X(X)$ 在 $[0,1]$ 上均匀分布。对于具有 CDF $F_Y$ 的离散变量 $Y$，此性质不直接成立。一个常见的解决方案是随机化 PIT：\n$$\nU = F_Y(Y-1) + R \\cdot (F_Y(Y) - F_Y(Y-1)) = P(Y'  Y) + R \\cdot P(Y' = Y)\n$$\n其中 $R$ 是来自 $\\mathrm{Uniform}(0,1)$ 的独立随机抽样，而 $Y'$ 是一个与 $Y$ 的预测具有相同分布的随机变量。如果预测是良好校准的，则 $U$ 的值将在 $[0,1]$ 上均匀分布。\n\n在这个问题中，我们需要每个区间 $k$ 中计数的边际后验预测分布 $p(y_{\\text{new},k} \\mid y)$。这是一个 Beta-二项分布的混合：\n$$\np(y_{\\text{new},k} \\mid y) = \\sum_{c \\in \\mathcal{C}} p(y_{\\text{new},k} \\mid y, c) p(c \\mid y)\n$$\n分量 $p(y_{\\text{new},k} \\mid y, c)$ 是通过从其后验分布中积分掉单个比例 $p_k$ 得出的。$p_k$ 的边际后验是一个 Beta 分布：\n$$\np_k \\mid y, c \\sim \\mathrm{Beta}\\left(y_k + \\alpha_k(c), \\sum_{j \\neq k} (y_j + \\alpha_j(c))\\right)\n$$\n由于 $y_{\\text{new},k}$ 的似然是 $\\mathrm{Binomial}(n_{\\text{new}}, p_k)$，对于固定的 $c$，预测分布是一个 Beta-二项分布：\n$$\ny_{\\text{new},k} \\mid y, c \\sim \\mathrm{BetaBinomial}\\left(n_{\\text{new}}, a_k, b_k\\right)\n$$\n其中 $a_k = y_k + \\alpha_k(c)$ 且 $b_k = (n + \\alpha_0(c)) - a_k$。\n\n令 $F_{k,c}(t)$ 为此 Beta-二项分布的 CDF。区间 $k$ 的混合 CDF 为 $F_k(t) = \\sum_{c \\in \\mathcal{C}} p(c \\mid y) F_{k,c}(t)$。每个区间 $k$ 的随机化 PIT 值为：\n$$\nu_k = F_k(y_{\\text{new},k}-1) + r_k \\cdot \\left( F_k(y_{\\text{new},k}) - F_k(y_{\\text{new},k}-1) \\right)\n$$\n其中 $r_k \\sim \\mathrm{Uniform}(0,1)$ 是独立的随机抽样。\n\n最后，使用单样本 Kolmogorov-Smirnov (KS) 检验，将这组 $K$ 个 PIT 值 $\\{u_1, \\dots, u_K\\}$ 与 $\\mathrm{Uniform}(0,1)$ 分布进行比较以检验其均匀性。KS 统计量衡量 PIT 值的经验累积分布函数与均匀分布的累积分布函数之间的最大偏差，为校准性提供了一个单一的总结性度量。", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln, logsumexp\nfrom scipy.stats import betabinom, kstest\n\ndef solve():\n    \"\"\"\n    Solves the computational systems biology problem for all test cases.\n    \"\"\"\n    \n    # Global parameters\n    K = 4\n    omega = np.array([0.4, 0.3, 0.2, 0.1])\n    \n    # Define test cases\n    test_cases = [\n        {\n            \"C\": np.array([1, 2, 3]),\n            \"p_c\": np.array([1/3, 1/3, 1/3]),\n            \"kappa\": 8.0,\n            \"y\": np.array([38, 31, 21, 10]),\n            \"y_new\": np.array([33, 26, 15, 6]),\n        },\n        {\n            \"C\": np.array([1, 2, 3]),\n            \"p_c\": np.array([0.2, 0.5, 0.3]),\n            \"kappa\": 6.0,\n            \"y\": np.array([0, 0, 0, 0]),\n            \"y_new\": np.array([22, 15, 9, 4]),\n        },\n        {\n            \"C\": np.array([1, 3]),\n            \"p_c\": np.array([0.5, 0.5]),\n            \"kappa\": 12.0,\n            \"y\": np.array([240, 180, 120, 60]),\n            \"y_new\": np.array([120, 90, 60, 30]),\n        },\n    ]\n\n    # Initialize a single random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n    \n    results = []\n    for case in test_cases:\n        c_hat, log_p_new, ks_stat = process_case(case, K, omega, rng)\n        results.extend([c_hat, log_p_new, ks_stat])\n\n    print(f\"[{','.join(f'{v:.6f}' if isinstance(v, float) else str(v) for v in results)}]\")\n\ndef log_dirichlet_multinomial_pmf(y, alpha):\n    \"\"\"\n    Computes the log PMF of the Dirichlet-Multinomial distribution.\n    log P(y|alpha) = log(n!) - sum(log(y_k!)) + log B(y+alpha) - log B(alpha)\n                   = lgamma(n+1) - sum(lgamma(y_k+1)) +\n                     (sum(lgamma(y+alpha)) - lgamma(n+sum(alpha))) -\n                     (sum(lgamma(alpha)) - lgamma(sum(alpha)))\n    \"\"\"\n    n = np.sum(y)\n    log_multinomial_coeff = gammaln(n + 1) - np.sum(gammaln(y + 1))\n    \n    alpha_sum = np.sum(alpha)\n    \n    log_B_ratio = (np.sum(gammaln(y + alpha)) - gammaln(n + alpha_sum)) - \\\n                  (np.sum(gammaln(alpha)) - gammaln(alpha_sum))\n\n    return log_multinomial_coeff + log_B_ratio\n\ndef process_case(case, K, omega, rng):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    C_set = case[\"C\"]\n    p_c = case[\"p_c\"]\n    kappa = case[\"kappa\"]\n    y = case[\"y\"]\n    y_new = case[\"y_new\"]\n    n = np.sum(y)\n    n_new = np.sum(y_new)\n    \n    num_c = len(C_set)\n    log_p_c = np.log(p_c)\n\n    # --- 1. Compute posterior p(c|y) ---\n    log_posterior_unnormalized = np.zeros(num_c)\n    alphas = []\n    \n    for i, c in enumerate(C_set):\n        alpha_c = kappa * c * omega\n        alphas.append(alpha_c)\n        alpha_c_sum = np.sum(alpha_c) # kappa * c\n\n        # Log marginal likelihood p(y|c) without constant multinomial coefficient term\n        log_marginal_lik_part = (gammaln(alpha_c_sum) - np.sum(gammaln(alpha_c)) +\n                                 np.sum(gammaln(y + alpha_c)) - gammaln(n + alpha_c_sum))\n        \n        log_posterior_unnormalized[i] = log_marginal_lik_part + log_p_c[i]\n\n    log_norm_const = logsumexp(log_posterior_unnormalized)\n    log_p_c_given_y = log_posterior_unnormalized - log_norm_const\n    p_c_given_y = np.exp(log_p_c_given_y)\n\n    c_hat = C_set[np.argmax(p_c_given_y)]\n\n    # --- 2. Compute posterior predictive p(y_new|y) ---\n    log_pred_terms = np.zeros(num_c)\n    for i, c in enumerate(C_set):\n        alpha_posterior = alphas[i] + y\n        log_p_ynew_given_yc = log_dirichlet_multinomial_pmf(y_new, alpha_posterior)\n        log_pred_terms[i] = log_p_ynew_given_yc + log_p_c_given_y[i]\n        \n    log_p_ynew_given_y = logsumexp(log_pred_terms)\n\n    # --- 3. Compute PIT values and KS statistic ---\n    pit_values = np.zeros(K)\n    for k in range(K):\n        y_new_k = y_new[k]\n        \n        # Calculate mixture CDF and PMF\n        F_less_k = 0.0 # P(Y_k  y_new_k)\n        F_equal_k = 0.0 # P(Y_k = y_new_k)\n        \n        for i, c in enumerate(C_set):\n            alpha_c = alphas[i]\n            alpha_c_sum = np.sum(alpha_c)\n            \n            # Beta-Binomial parameters\n            a = y[k] + alpha_c[k]\n            b = (n + alpha_c_sum) - (y[k] + alpha_c[k])\n            \n            pmf_val = betabinom.pmf(y_new_k, n_new, a, b)\n            cdf_less_val = betabinom.cdf(y_new_k - 1, n_new, a, b)\n            \n            F_less_k += p_c_given_y[i] * cdf_less_val\n            F_equal_k += p_c_given_y[i] * pmf_val\n            \n        r_k = rng.uniform()\n        pit_values[k] = F_less_k + r_k * F_equal_k\n\n    ks_statistic, _ = kstest(pit_values, 'uniform')\n\n    return c_hat, log_p_ynew_given_y, ks_statistic\n\nsolve()\n```", "id": "3340191"}]}