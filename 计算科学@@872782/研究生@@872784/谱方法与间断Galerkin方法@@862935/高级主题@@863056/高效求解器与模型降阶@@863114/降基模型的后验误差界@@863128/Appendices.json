{"hands_on_practices": [{"introduction": "对于强制性问题，严格的后验误差界依赖于矫顽常数的可计算下界。逐次约束法 (SCM) 通过求解一个由离线训练数据构建的小型线性规划问题来提供这样一个下界。本练习将通过对一个参数化各向异性扩散问题实施 SCM，提供计算该下界的实践经验，这是认证降阶基模型的一项基本技能。[@problem_id:3361086]", "problem": "考虑一个单位平方域上的参数化各向异性扩散问题，其边界条件为齐次 Dirichlet 边界条件。设双线性形式对函数 $u$ 和 $v$ 定义为\n$$\na(u,v;\\mu) = \\int_{\\Omega} \\nabla u(x)^{\\top} A(\\mu) \\nabla v(x) \\, dx,\n$$\n其中 $\\Omega = (0,1)\\times(0,1)$ 且 $A(\\mu)$ 是一个与参数向量 $\\mu$ 相关的常数、对称、正定张量。假设 $A(\\mu)$ 是对角矩阵，其对角元为 $a_x(\\mu)$ 和 $a_y(\\mu)$，因此\n$$\nA(\\mu) = \\begin{pmatrix} a_x(\\mu)  0 \\\\ 0  a_y(\\mu) \\end{pmatrix}, \\quad a_x(\\mu) > 0, \\quad a_y(\\mu) > 0.\n$$\n定义一个由具有单位张量的各向同性双线性形式所导出的参考范数，\n$$\n\\|v\\|_V^2 = a_{\\mathrm{ref}}(v,v) = \\int_{\\Omega} \\left( \\left|\\frac{\\partial v}{\\partial x}\\right|^2 + \\left|\\frac{\\partial v}{\\partial y}\\right|^2 \\right) dx,\n$$\n以及矫顽常数\n$$\n\\alpha(\\mu) = \\inf_{v \\in V \\setminus \\{0\\}} \\frac{a(v,v;\\mu)}{\\|v\\|_V^2}.\n$$\n该双线性形式的参数仿射分解为\n$$\na(u,v;\\mu) = \\Theta_1(\\mu) a_1(u,v) + \\Theta_2(\\mu) a_2(u,v),\n$$\n其中 $\\Theta_1(\\mu) = a_x(\\mu)$，$\\Theta_2(\\mu) = a_y(\\mu)$，$a_1(u,v) = \\int_{\\Omega} \\frac{\\partial u}{\\partial x} \\frac{\\partial v}{\\partial x} \\, dx$，以及 $a_2(u,v) = \\int_{\\Omega} \\frac{\\partial u}{\\partial y} \\frac{\\partial v}{\\partial y} \\, dx$。对于任意非零 $v$，定义分量比率\n$$\nr_q(v) = \\frac{a_q(v,v)}{\\|v\\|_V^2}, \\quad q \\in \\{1,2\\}.\n$$\n那么 $r_1(v) \\ge 0$，$r_2(v) \\ge 0$，且 $r_1(v) + r_2(v) = 1$。\n\n为了获得可计算的离散近似，使用一个由满足齐次 Dirichlet 边界条件的张量积正弦函数张成的谱 Galerkin 子空间，\n$$\n\\phi_{k_x,k_y}(x,y) = \\sin(k_x \\pi x)\\sin(k_y \\pi y),\n$$\n其中整数波数 $k_x \\in \\{1,\\dots,N_x\\}$ 和 $k_y \\in \\{1,\\dots,N_y\\}$。在此基底下，对于一个纯模式 $\\phi_{k_x,k_y}$，其 Rayleigh 商为\n$$\n\\mathcal{R}(k_x,k_y;\\mu) = \\frac{a(\\phi_{k_x,k_y},\\phi_{k_x,k_y};\\mu)}{\\|\\phi_{k_x,k_y}\\|_V^2} = \\frac{a_x(\\mu) k_x^2 + a_y(\\mu) k_y^2}{k_x^2 + k_y^2}.\n$$\n因此，在所选谱子空间上的离散“真实”矫顽常数为\n$$\n\\alpha_{\\mathrm{true}}(\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\mathcal{R}(k_x,k_y;\\mu).\n$$\n\n使用一组具有已知离散 $\\alpha_{\\mathrm{true}}(\\mu_i)$ 的训练参数 $\\{\\mu_i\\}$，构建一个 $\\alpha(\\mu)$ 的逐次约束法 (Successive Constraint Method, SCM) 下界。定义一个向量 $y = (y_1,y_2)$ 来近似 $(r_1(v),r_2(v))$，并构建线性规划\n$$\n\\alpha_{\\mathrm{LB}}(\\mu) = \\min_{y \\in \\mathbb{R}^2} \\;\\Theta_1(\\mu) y_1 + \\Theta_2(\\mu) y_2\n$$\n其约束条件为\n$$\ny_1 \\ge 0, \\quad y_2 \\ge 0, \\quad y_1 + y_2 = 1, \\quad \\Theta_1(\\mu_i) y_1 + \\Theta_2(\\mu_i) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_i) \\quad \\text{对于所有训练参数 } \\mu_i.\n$$\n该优化产生一个下界 $\\alpha_{\\mathrm{LB}}(\\mu)$，满足 $\\alpha_{\\mathrm{LB}}(\\mu) \\le \\alpha_{\\mathrm{true}}(\\mu)$。\n\n您的任务是使用谱 Galerkin 离散公式实现 $\\alpha_{\\mathrm{true}}(\\mu)$ 的计算，并使用上述 SCM 线性规划实现 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的计算。然后，通过计算比率来研究 SCM 界随各向异性比率增大的紧致度\n$$\n\\rho(\\mu) = \\frac{\\alpha_{\\mathrm{LB}}(\\mu)}{\\alpha_{\\mathrm{true}}(\\mu)}.\n$$\n\n使用以下有限维谱参数，$N_x = 8$ 和 $N_y = 8$，并定义训练参数集为\n$$\n\\{(a_x,a_y)\\} = \\{(1,1), (1,10), (10,1), (1,100), (100,1), (1,1000), (1000,1)\\}.\n$$\n为以下测试组评估紧致度比率\n$$\n\\text{测试 1:}\\; (a_x,a_y)=(1,1), \\quad \\text{测试 2:}\\; (a_x,a_y)=(1,10), \\quad \\text{测试 3:}\\; (a_x,a_y)=(1,100), \\quad \\text{测试 4:}\\; (a_x,a_y)=(1,1000).\n$$\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，按四个测试的顺序排列，每个条目等于相应测试的 $\\rho(\\mu)$ 的浮点值，计算到完整的机器精度，不进行四舍五入。不涉及物理单位。不使用角度。\n\n该问题要求您：\n- 从矫顽常数的基本变分定义和参数仿射分解开始。\n- 推导并实现关于波数的 $\\alpha_{\\mathrm{true}}(\\mu)$ 的离散谱 Galerkin 公式。\n- 使用指定的约束条件，构建和求解 SCM 线性规划以获得 $\\alpha_{\\mathrm{LB}}(\\mu)$。\n- 汇总所提供测试组的紧致度比率 $\\rho(\\mu)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4]$）。", "solution": "用户希望计算一个参数化各向异性扩散问题的矫顽常数的逐次约束法 (SCM) 下界的紧致度比率。\n\n### 第 1 步：问题验证\n\n已根据所有验证标准对问题陈述进行了仔细审查。\n\n- **已知条件**：所有数据，包括双线性形式、参数仿射分解、谱基函数、离散化参数（$N_x=8$, $N_y=8$）、训练参数集和测试参数集，都已明确给出。\n- **科学依据**：该问题在偏微分方程数值分析理论中有坚实的基础，特别是在降基方法和矫顽常数的后验误差估计方面。所有数学公式和概念都是该领域的标准内容。\n- **适定性**：该问题是适定的。离散矫顽常数 $\\alpha_{\\mathrm{true}}(\\mu)$ 的计算是在一个有限集上的最小化问题。SCM 下界 $\\alpha_{\\mathrm{LB}}(\\mu)$ 由一个具有非空可行集的二维线性规划确定，这保证了解的存在性。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n- **完整性**：已提供解决所述问题所需的所有必要信息。\n- **一致性**：所给条件内部一致。Rayleigh 商的推导在数学上是合理的且可验证的。\n- **特殊情况**：测试参数是训练参数的一个子集。虽然这会导致一个特定的数值结果（$\\rho=1$），但这是一个有效的规定，而非缺陷。该问题测试了对 SCM 方法属性的理解，特别是其在训练点自身的行为。\n\n该问题被认为是**有效的**。我们继续进行求解。\n\n### 第 2 步：基于原理的求解推导\n\n任务是为一组给定的测试参数 $\\mu = (a_x, a_y)$ 计算紧致度比率 $\\rho(\\mu) = \\alpha_{\\mathrm{LB}}(\\mu) / \\alpha_{\\mathrm{true}}(\\mu)$。这需要推导并实现 $\\alpha_{\\mathrm{true}}(\\mu)$ 和 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的表达式。\n\n#### 2.1. 真实离散矫顽常数 $\\alpha_{\\mathrm{true}}(\\mu)$ 的计算\n\n离散矫顽常数定义为 Rayleigh 商在谱 Galerkin 子空间中所有基函数上的最小值：\n$$\n\\alpha_{\\mathrm{true}}(\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\mathcal{R}(k_x,k_y;\\mu) = \\min_{1 \\le k_x \\le N_x,\\; 1 \\le k_y \\le N_y} \\frac{a_x(\\mu) k_x^2 + a_y(\\mu) k_y^2}{k_x^2 + k_y^2}\n$$\nRayleigh 商可以重写为 $a_x(\\mu)$ 和 $a_y(\\mu)$ 的加权平均值：\n$$\n\\mathcal{R}(k_x,k_y;\\mu) = a_x(\\mu) \\frac{k_x^2}{k_x^2+k_y^2} + a_y(\\mu) \\frac{k_y^2}{k_x^2+k_y^2}\n$$\n此表达式的最小值总是受限于 $\\min(a_x(\\mu), a_y(\\mu))$。为了找到最小值，我们必须调整权重（通过选择 $k_x$ 和 $k_y$）以偏向两个系数中较小的一个。\n\n- 如果 $a_x(\\mu)  a_y(\\mu)$，我们必须最大化 $a_x(\\mu)$ 上的权重，即 $\\frac{k_x^2}{k_x^2+k_y^2}$。这可以通过最大化 $k_x$ 和最小化 $k_y$ 来实现，即设置 $k_x = N_x$ 和 $k_y = 1$。\n- 如果 $a_y(\\mu)  a_x(\\mu)$，我们必须最大化 $a_y(\\mu)$ 上的权重，即 $\\frac{k_y^2}{k_x^2+k_y^2}$。这可以通过最大化 $k_y$ 和最小化 $k_x$ 来实现，即设置 $k_x = 1$ 和 $k_y = N_y$。\n- 如果 $a_x(\\mu) = a_y(\\mu)$，则对于所有的 $k_x, k_y$，表达式是常数，等于 $a_x(\\mu)$。\n\n给定 $N_x=8$ 和 $N_y=8$，$\\alpha_{\\mathrm{true}}(\\mu)$ 的解析公式为：\n$$\n\\alpha_{\\mathrm{true}}(a_x, a_y) = \\begin{cases}\n\\frac{a_x \\cdot 8^2 + a_y \\cdot 1^2}{8^2+1^2} = \\frac{64 a_x + a_y}{65}  \\text{if } a_x  a_y \\\\\n\\frac{a_x \\cdot 1^2 + a_y \\cdot 8^2}{1^2+8^2} = \\frac{a_x + 64 a_y}{65}  \\text{if } a_y  a_x \\\\\na_x  \\text{if } a_x = a_y\n\\end{cases}\n$$\n\n#### 2.2. SCM 下界 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的计算\n\nSCM 下界 $\\alpha_{\\mathrm{LB}}(\\mu)$ 是以下线性规划（LP）的解：\n$$\n\\alpha_{\\mathrm{LB}}(\\mu) = \\min_{y_1, y_2} \\left( a_x(\\mu) y_1 + a_y(\\mu) y_2 \\right)\n$$\n约束条件为：\n1. $y_1 \\ge 0, y_2 \\ge 0$\n2. $y_1 + y_2 = 1$\n3. 对于所有训练参数 $\\mu_i$，都有 $a_x(\\mu_i) y_1 + a_y(\\mu_i) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_i)$。\n\n我们可以简化这个 LP。使用 $y_2 = 1 - y_1$，问题变成关于 $y_1$ 的一维问题：\n$$\n\\min_{y_1} \\left( (a_x(\\mu) - a_y(\\mu)) y_1 + a_y(\\mu) \\right)\n$$\n其约束条件为 $y_1$ 的一个可行区间。约束变为：\n1. $y_1 \\ge 0$ 且 $1 - y_1 \\ge 0 \\implies 0 \\le y_1 \\le 1$。\n2. 对于每个训练参数 $\\mu_i$：$(a_x(\\mu_i) - a_y(\\mu_i)) y_1 \\ge \\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)$。\n\n这第二组约束定义了可行区间 $[y_{1,\\min}, y_{1,\\max}]$。对于一个训练参数 $\\mu_i$：\n- 如果 $a_x(\\mu_i) > a_y(\\mu_i)$：$y_1 \\ge \\frac{\\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{\\frac{a_x(\\mu_i) + 64 a_y(\\mu_i)}{65} - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{a_x(\\mu_i) - a_y(\\mu_i)}{65(a_x(\\mu_i) - a_y(\\mu_i))} = \\frac{1}{65}$。这给出了 $y_1$ 的一个下界。\n- 如果 $a_x(\\mu_i)  a_y(\\mu_i)$：$y_1 \\le \\frac{\\alpha_{\\mathrm{true}}(\\mu_i) - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{\\frac{64 a_x(\\mu_i) + a_y(\\mu_i)}{65} - a_y(\\mu_i)}{a_x(\\mu_i) - a_y(\\mu_i)} = \\frac{64(a_x(\\mu_i) - a_y(\\mu_i))}{65(a_x(\\mu_i) - a_y(\\mu_i))} = \\frac{64}{65}$。这给出了 $y_1$ 的一个上界。\n\n训练集包含两种类型的参数。结合所有约束， $y_1$ 的可行区间是 $[\\frac{1}{65}, \\frac{64}{65}]$。\n该 LP 问题是在此区间上最小化一个关于 $y_1$ 的线性函数。最小值将在其中一个端点处取得：\n- 如果斜率 $(a_x(\\mu) - a_y(\\mu)) > 0$，最小值在 $y_1 = \\frac{1}{65}$ 处。\n- 如果斜率 $(a_x(\\mu) - a_y(\\mu))  0$，最小值在 $y_1 = \\frac{64}{65}$ 处。\n- 如果斜率为 0，目标函数为常数。\n\n#### 2.3. 在测试参数处评估紧致度比率 $\\rho(\\mu)$\n\n测试参数为 $\\mu_1=(1,1)$、$\\mu_2=(1,10)$、$\\mu_3=(1,100)$、$\\mu_4=(1,1000)$。所有这些参数也都在训练集中。对于训练集中的任何参数 $\\mu_k$，SCM 约束都包含 $a_x(\\mu_k) y_1 + a_y(\\mu_k) y_2 \\ge \\alpha_{\\mathrm{true}}(\\mu_k)$。 $\\alpha_{\\mathrm{LB}}(\\mu_k)$ 的目标函数是最小化 $a_x(\\mu_k) y_1 + a_y(\\mu_k) y_2$。根据构造，最小值不能小于 $\\alpha_{\\mathrm{true}}(\\mu_k)$。因此，$\\alpha_{\\mathrm{LB}}(\\mu_k) \\ge \\alpha_{\\mathrm{true}}(\\mu_k)$。又因为它是一个下界，我们有 $\\alpha_{\\mathrm{LB}}(\\mu_k) \\le \\alpha_{\\mathrm{true}}(\\mu_k)$。这强制了一个等式：$\\alpha_{\\mathrm{LB}}(\\mu_k) = \\alpha_{\\mathrm{true}}(\\mu_k)$。\n因此，对于任何同时也是训练参数的测试参数，其紧致度比率 $\\rho(\\mu)$ 必须恰好为 1。实现过程将通过直接计算来证实这一点。\n我们现在将实现推导出的 $\\alpha_{\\mathrm{true}}(\\mu)$ 和 $\\alpha_{\\mathrm{LB}}(\\mu)$ 的解析公式，以计算测试组的比率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the SCM tightness ratio for a parametrized anisotropic diffusion problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0),\n        (1.0, 10.0),\n        (1.0, 100.0),\n        (1.0, 1000.0),\n    ]\n\n    # Define the training parameter set for the SCM.\n    training_params = [\n        (1.0, 1.0), (1.0, 10.0), (10.0, 1.0), (1.0, 100.0), \n        (100.0, 1.0), (1.0, 1000.0), (1000.0, 1.0)\n    ]\n    \n    # Define the spectral Galerkin subspace parameters.\n    Nx = 8.0\n    Ny = 8.0\n\n    def calculate_alpha_true(mu, Nx_val, Ny_val):\n        \"\"\"\n        Calculates the discrete coercivity constant alpha_true based on the\n        analytical minimum of the Rayleigh quotient.\n        \n        Args:\n            mu (tuple): Parameter vector (ax, ay).\n            Nx_val (float): Number of basis functions in x-direction.\n            Ny_val (float): Number of basis functions in y-direction.\n            \n        Returns:\n            float: The value of alpha_true(mu).\n        \"\"\"\n        ax, ay = mu\n        if ax  ay:\n            # Minimum is at (kx, ky) = (Nx, 1) to maximize weight on smaller coefficient ax.\n            return (ax * Nx_val**2 + ay) / (Nx_val**2 + 1.0)\n        elif ay  ax:\n            # Minimum is at (kx, ky) = (1, Ny) to maximize weight on smaller coefficient ay.\n            return (ax + ay * Ny_val**2) / (1.0 + Ny_val**2)\n        else:  # ax == ay\n            # Rayleigh quotient is constant and equal to ax.\n            return ax\n\n    # Determine the feasible region for y1 from the training set.\n    # The linear program is to minimize (ax-ay)*y1 + ay subject to constraints.\n    # The constraints on y1 are derived from the training set.\n    y1_min_constraints = [0.0]\n    y1_max_constraints = [1.0]\n\n    for mu_i in training_params:\n        ax_i, ay_i = mu_i\n        alpha_true_i = calculate_alpha_true(mu_i, Nx, Ny)\n        \n        # Rewrite constraint a_x*y1 + a_y*y2 >= alpha_true as (a_x - a_y)*y1 >= alpha_true - a_y\n        diff = ax_i - ay_i\n        if diff > 0:\n            # y1 >= (alpha_true_i - ay_i) / (ax_i - ay_i)\n            lower_bound = (alpha_true_i - ay_i) / diff\n            y1_min_constraints.append(lower_bound)\n        elif diff  0:\n            # y1 = (alpha_true_i - ay_i) / (ax_i - ay_i) (inequality flips)\n            upper_bound = (alpha_true_i - ay_i) / diff\n            y1_max_constraints.append(upper_bound)\n        # If diff is 0, the constraint is 0 >= 0, which is always true and adds no information.\n\n    # The feasible interval for y1 is [y1_min_feasible, y1_max_feasible].\n    y1_min_feasible = max(y1_min_constraints)\n    y1_max_feasible = min(y1_max_constraints)\n\n    results = []\n    for case in test_cases:\n        ax_test, ay_test = case\n        \n        # 1. Calculate the \"true\" discrete coercivity constant for the test case.\n        alpha_t = calculate_alpha_true(case, Nx, Ny)\n        \n        # 2. Calculate the SCM lower bound alpha_LB.\n        # This involves minimizing the linear objective function f(y1) = (ax-ay)*y1 + ay\n        # over the feasible interval for y1.\n        \n        y1_opt = 0.0\n        slope = ax_test - ay_test\n        \n        if slope > 0: # Increasing function, minimum is at the left endpoint.\n            y1_opt = y1_min_feasible\n        elif slope  0: # Decreasing function, minimum is at the right endpoint.\n            y1_opt = y1_max_feasible\n        else: # slope is 0, function is constant, any point is a minimum.\n            y1_opt = y1_min_feasible\n\n        alpha_lb = slope * y1_opt + ay_test\n        \n        # 3. Compute the tightness ratio.\n        # As reasoned in the solution, since all test cases are in the training set,\n        # alpha_lb should be equal to alpha_t, making the ratio 1.0.\n        rho = alpha_lb / alpha_t\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3361086"}, {"introduction": "除了线性强制性问题，后验误差估计对于复杂的非线性系统也至关重要，例如那些使用人工粘性进行稳定化的系统。本练习将一个基于残差的误差估计器应用于非线性扩散问题的间断 Galerkin 离散化。通过为一个降阶基模型实施此方法，您将验证估计器单调性这一关键理论性质，该性质确保了随着基的丰富，误差估计会可靠地减小。[@problem_id:3361071]", "problem": "考虑一维区域 $[0,1]$ 上的标量稳态边值问题，其带有狄利克雷（Dirichlet）边界条件，\n$$\n- \\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( \\left( \\nu_0(\\mu) + \\nu_{\\mathrm{art}}(u; \\mu) \\right) \\frac{\\mathrm{d}u}{\\mathrm{d}x} \\right) = s(x;\\mu), \\quad x \\in (0,1), \\quad u(0)=1,\\quad u(1)=0,\n$$\n其中 $\\mu$ 表示一个参数矢量，$\\nu_0(\\mu) \\gt 0$ 是一个常数物理粘性系数，而 $\\nu_{\\mathrm{art}}(u; \\mu) \\ge 0$ 是一个人为粘性系数，通过熵-粘性激波捕捉机制来选择，以稳定陡峭梯度。源项取为\n$$\ns(x;\\mu) = A(\\mu)\\,\\sin\\left(5\\pi x\\right),\n$$\n其中 $A(\\mu)\\gt 0$ 是一个源项振幅。我们使用对称内部罚函数间断伽辽金方法（SIPG），在具有 $K$ 个单元和多项式次数 $p=0$ 的均匀网格上离散化此问题。令 $h = 1/K$ 表示单元尺寸，并令 $u_h \\in V_h$ 为分片常数的离散解，每个单元有一个自由度。\n\n我们使用二次熵 $\\eta(u) = \\tfrac{1}{2} u^2$ 按单元定义熵-粘性系数。对于任何 $u_h \\in V_h$，我们对单元索引 $e=0,1,\\dots,K-1$ 定义：\n- 梯度大小的单元指标\n$$\nG_e(u_h) = \\frac{1}{2}\\left( \\frac{|u_e - u_{e-1}|}{h} + \\frac{|u_{e+1} - u_e|}{h} \\right),\n$$\n约定 $u_{-1} := u(0)=1$ 和 $u_{K} := u(1)=0$，\n- 归一化\n$$\n\\Delta\\eta(u_h) = \\max_{0\\le j \\le K-1} \\left\\{ \\eta(u_j) \\right\\} - \\min_{0\\le j \\le K-1} \\left\\{ \\eta(u_j) \\right\\} + \\varepsilon_{\\eta},\n$$\n其中 $\\varepsilon_{\\eta} \\gt 0$ 是一个小的安全参数，\n- 以及熵粘性\n$$\n\\nu_{\\mathrm{art},e}(u_h;\\mu) = C_{\\mathrm{ev}}\\, h^2 \\,\\frac{|u_e|^2\\, G_e(u_h)}{\\Delta\\eta(u_h)} \\quad \\text{被裁剪到 } [0, \\nu_{\\max}] \\text{ 区间内},\n$$\n其中 $C_{\\mathrm{ev}} \\gt 0$ 是一个可调系数，$\\nu_{\\max} \\gt 0$ 是一个粘性上限。\n\n对于分片常数粘性的扩散问题，其 SIPG 双线性形式为，对试探函数 $u_h$ 和检验函数 $v_h$，\n$$\na(u_h,v_h;\\mu) = \\sum_{\\text{faces }F}\\lambda_F(\\mu)\\,[u_h]_F\\,[v_h]_F + \\lambda_{\\mathrm{L}}(\\mu)\\,u_0 v_0 + \\lambda_{\\mathrm{R}}(\\mu)\\,u_{K-1} v_{K-1},\n$$\n其中 $w_h$ 跨越单元 $e$ 和 $e+1$ 之间的一个内面 $F$ 的跳跃为 $[w_h]_F = w_e - w_{e+1}$，罚项为 $\\lambda_F(\\mu) = C_{\\mathrm{pen}} \\, \\overline{\\nu}_F(\\mu) / h$，其中 $\\overline{\\nu}_F(\\mu) = \\tfrac{1}{2} \\left( \\nu_{e}(\\mu) + \\nu_{e+1}(\\mu) \\right)$ 且 $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_h;\\mu)$。边界罚项满足 $\\lambda_{\\mathrm{L}}(\\mu) = C_{\\mathrm{pen}} \\, \\nu_0(\\mu)/h$ 和 $\\lambda_{\\mathrm{R}}(\\mu) = C_{\\mathrm{pen}} \\, \\nu_0(\\mu)/h$。该线性泛函包含了源项和狄利克雷边界条件的弱施加，\n$$\n\\ell(v_h;\\mu) = \\sum_{e=0}^{K-1} \\left( \\int_{x_e}^{x_{e+1}} s(x;\\mu)\\,\\mathrm{d}x \\right) v_e + \\lambda_{\\mathrm{L}}(\\mu)\\, g_{\\mathrm{L}}\\, v_0 + \\lambda_{\\mathrm{R}}(\\mu)\\, g_{\\mathrm{R}}\\, v_{K-1},\n$$\n其中 $g_{\\mathrm{L}}=1$ 和 $g_{\\mathrm{R}}=0$。离散问题是：找到 $u_h \\in V_h$，使得对于所有 $v_h \\in V_h$ 都有 $a(u_h,v_h;\\mu) = \\ell(v_h;\\mu)$。$\\nu_{\\mathrm{art}}$ 对 $u_h$ 的依赖性导致了一个非线性不动点问题，我们通过不动点迭代来求解。\n\n一个维度为 $N$ 的降阶基（RB）空间 $V_N(\\mu)$ 是通过对一组高保真快照在欧几里得内积下进行标准正交化来构建的。降阶解 $u_N(\\mu) \\in V_N(\\mu)$ 是通过求解投影到 $V_N(\\mu)$ 上的非线性不动点问题得到的伽辽金解。\n\n我们考虑一个基于残差的后验误差估计子，用于由算子对称正定（SPD）部分诱导的能量范数。具体来说，定义与当前降阶解 $u_N(\\mu)$ 处的 $a(\\cdot,\\cdot;\\mu)$ 相关联的 SPD 矩阵 $S(u_N;\\mu)$，以及全阶残差向量\n$$\nr_N(\\mu) = \\ell(\\cdot;\\mu) - A(u_N(\\mu);\\mu) u_N(\\mu),\n$$\n其中 $A(u_N(\\mu);\\mu)$ 是用 $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_N;\\mu)$ 组装的全阶刚度矩阵。那么，该估计子是残差在 $S(u_N;\\mu)$-内积下的对偶范数，\n$$\n\\eta_N(\\mu) = \\left\\| r_N(\\mu) \\right\\|_{S(u_N;\\mu)^{-1}} = \\sqrt{ r_N(\\mu)^{\\top} S(u_N;\\mu)^{-1} r_N(\\mu) }.\n$$\n对于具有矫顽对称双线性形式的线性问题，此量是由 $S$ 诱导的能量范数下误差的一个上界，即，\n$$\n\\| u_h(\\mu) - u_N(\\mu) \\|_{S(u_N;\\mu)} \\le \\eta_N(\\mu),\n$$\n其中 $\\| v \\|_{S}^2 = v^{\\top} S v$。人为粘性增加了对称部分，从而增强了矫顽性，因此通过 $S$ 直接进入估计子。\n\n将认证衰减比定义为序列\n$$\nq_N(\\mu) = \\frac{\\eta_{N+1}(\\mu)}{\\eta_N(\\mu)}, \\quad N=1,2,\\dots,\n$$\n对于一个固定的参数 $\\mu$。我们将对一组选定的测试参数进行数值验证，以检验认证衰减的单调性：序列 $\\{ \\eta_N(\\mu) \\}$ 随 $N$ 非增，且满足 $0 \\le q_N(\\mu) \\le 1$。\n\n任务：\n1. 使用所述的对称内部罚函数间断伽辽金方法，在具有 $K=80$ 个单元和罚系数 $C_{\\mathrm{pen}}=10$ 的均匀网格上，使用分片常数基（$p=0$）对问题进行离散化。使用不动点迭代计算高保真解 $u_h(\\mu)$，从线性初始猜测开始，当连续迭代的欧几里得范数变化小于 $10^{-10}$ 或达到 50 次迭代时，宣布收敛。使用 $C_{\\mathrm{ev}}=0.02$, $\\nu_{\\max}=0.05$, 以及 $\\varepsilon_{\\eta}=10^{-12}$。\n2. 通过收集高保真快照并进行标准正交化，从 $M=5$ 个训练参数构建一个降阶基空间。使用以下训练集：\n   - $\\mu_1^{\\mathrm{train}}$: $\\nu_0 = 0.015$, $A = 0.8$,\n   - $\\mu_2^{\\mathrm{train}}$: $\\nu_0 = 0.008$, $A = 1.2$,\n   - $\\mu_3^{\\mathrm{train}}$: $\\nu_0 = 0.012$, $A = 1.5$,\n   - $\\mu_4^{\\mathrm{train}}$: $\\nu_0 = 0.020$, $A = 0.6$,\n   - $\\mu_5^{\\mathrm{train}}$: $\\nu_0 = 0.010$, $A = 1.1$.\n3. 对于以下每个测试参数 $\\mu$，在 RB 空间中使用不动点迭代计算 $N=1,2,3,4,5$ 的降阶解 $u_N(\\mu)$，组装全阶残差 $r_N(\\mu)$ 和 SPD 矩阵 $S(u_N;\\mu)$，并评估估计子 $\\eta_N(\\mu)$。然后计算衰减比 $q_N(\\mu)$，并验证对所有 $N=1,\\dots,4$ 均满足单调性条件 $\\eta_{N+1}(\\mu) \\le \\eta_{N}(\\mu)$ 和 $0 \\le q_N(\\mu) \\le 1$。为每个测试参数报告一个布尔值，指示是否对所有 $N$ 都满足这两个条件。\n   - 测试用例 1: $\\nu_0 = 0.010$, $A = 1.0$,\n   - 测试用例 2: $\\nu_0 = 0.007$, $A = 1.5$,\n   - 测试用例 3: $\\nu_0 = 0.020$, $A = 0.5$.\n\n此问题中不出现角度单位，除了所提供的无量纲参数设置外，不需要其他物理单位。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”），其中每个结果是对应于上面列出的三个测试用例的布尔值。", "solution": "该问题是有效的。它提出了一个在数值分析领域中定义明确的任务，具体涉及非线性偏微分方程的模型降阶。所有必需的参数、方程和程序都已指明，并且该问题在科学上基于已确立的方法，如间断伽辽金法、熵粘性稳定化以及带后验误差估计的降阶基方法。\n\n我们的方法包括以下步骤，实现为一系列计算程序：\n1.  **高保真（HF）求解器：**我们首先为全阶离散问题构建一个求解器。\n2.  **降阶基（RB）生成：**我们使用高保真求解器为一组训练参数生成解“快照”，并从这些快照中形成一个标准正交基。\n3.  **降阶模型（ROM）求解器：**我们实现一个求解器，用于求解投影到降阶基空间上的问题。\n4.  **后验误差估计与验证：**对于每个测试参数，我们计算不断增加的基尺寸下的降阶解，评估基于残差的误差估计子，并验证其单调性。\n\n### 1. 高保真离散化与求解\n\n该问题是一个一维、稳态、非线性扩散方程。我们在一个包含 $K=80$ 个单元（尺寸 $h=1/K$）的均匀网格上对其进行离散化。我们使用对称内部罚函数间断伽辽金（SIPG）方法，其基函数为分片常数（$p=0$），即 $\\{\\phi_e\\}_{e=0}^{K-1}$。离散解 $u_h$ 是 $\\mathbb{R}^K$ 中的一个向量，其中分量 $u_e$ 表示解在单元 $e$ 上的常数值。\n\n离散弱形式为：找到 $u_h \\in V_h \\cong \\mathbb{R}^K$，使得\n$$A(u_h;\\mu) u_h = \\ell(\\mu)$$\n这是一个包含 $K$ 个非线性代数方程的系统。矩阵 $A$ 和向量 $\\ell$ 来自双线性形式 $a(u_h, v_h; \\mu)$ 和线性形式 $\\ell(v_h; \\mu)$。\n\n**刚度矩阵和载荷向量的组装**\n刚度矩阵 $A(u_h;\\mu)$ 的元素为 $A_{ij} = a(\\phi_j, \\phi_i; \\mu)$。由于基函数的局部支撑特性，该矩阵是三对角的。对于 $v_h=\\phi_i$，对 $a(u_h, v_h; \\mu)$ 的非零贡献为：\n-   **对角线元素 ($i=j$)：**\n    -   $i \\in \\{1, \\dots, K-2\\}$: $A_{i,i} = \\lambda_{i-1,i}(\\mu) + \\lambda_{i,i+1}(\\mu)$\n    -   $i=0$: $A_{0,0} = \\lambda_L(\\mu) + \\lambda_{0,1}(\\mu)$\n    -   $i=K-1$: $A_{K-1,K-1} = \\lambda_R(\\mu) + \\lambda_{K-2,K-1}(\\mu)$\n-   **非对角线元素 ($j=i \\pm 1$)：**\n    -   $A_{i,i+1} = A_{i+1,i} = -\\lambda_{i,i+1}(\\mu)$\n\n罚参数定义为 $\\lambda_F(\\mu) = C_{\\mathrm{pen}} \\overline{\\nu}_F(\\mu) / h$（对于内面 $F$，位于单元 $e$ 和 $e+1$ 之间），其中 $\\overline{\\nu}_F(\\mu) = \\frac{1}{2}(\\nu_e(\\mu) + \\nu_{e+1}(\\mu))$ 且 $\\nu_e(\\mu) = \\nu_0(\\mu) + \\nu_{\\mathrm{art},e}(u_h;\\mu)$。边界罚项为 $\\lambda_L(\\mu) = C_{\\mathrm{pen}}\\nu_0(\\mu)/h$ 和 $\\lambda_R(\\mu) = C_{\\mathrm{pen}}\\nu_0(\\mu)/h$，其中 $C_{\\mathrm{pen}}=10$。\n\n载荷向量 $\\ell(\\mu)$ 的元素为 $\\ell_i = \\ell(\\phi_i; \\mu)$:\n$$ \\ell_i = \\int_{x_i}^{x_{i+1}} s(x;\\mu) dx + \\delta_{i0} \\lambda_L(\\mu) g_L + \\delta_{i,K-1} \\lambda_R(\\mu) g_R $$\n其中 $g_L=1$，$g_R=0$，$\\delta_{ij}$ 是克罗内克（Kronecker）δ。积分可解析计算：\n$$ \\int_{x_i}^{x_{i+1}} A(\\mu) \\sin(5\\pi x) dx = \\frac{A(\\mu)}{5\\pi} (\\cos(5\\pi x_i) - \\cos(5\\pi x_{i+1})) $$\n\n**非线性求解器**\n矩阵 $A$ 对解 $u_h$ 的依赖性（通过人为粘性 $\\nu_{\\mathrm{art}}$）使得该系统成为非线性系统。我们通过不动点迭代来求解。给定一个迭代步 $u_h^{(k)}$，我们通过求解线性系统来计算下一个迭代步 $u_h^{(k+1)}$：\n$$ A(u_h^{(k)}; \\mu) u_h^{(k+1)} = \\ell(\\mu) $$\n迭代从一个初始猜测 $u_h^{(0)}$ 开始，该猜测通过在线性函数 $u(x)=1-x$ 的单元中心进行采样得到，即 $u_e^{(0)}=1-(e+0.5)h$。当 $\\|u_h^{(k+1)} - u_h^{(k)}\\|_2  10^{-10}$ 或达到最大 50 次迭代时，迭代终止。\n\n人为粘性 $\\nu_{\\mathrm{art}, e}(u_h; \\mu)$ 在每次迭代中更新。这涉及到计算梯度指标 $G_e(u_h)$、熵范围 $\\Delta\\eta(u_h)$，然后是粘性值本身，该值被裁剪到 $[0, \\nu_{\\max}]$ 区间内。具体来说，对于解向量 $u_h = (u_0, \\dots, u_{K-1})$，我们构建一个扩展向量 $(u(0), u_0, \\dots, u_{K-1}, u(1)) = (1, u_0, \\dots, u_{K-1}, 0)$ 来计算 $G_e(u_h)$ 所需的跳跃。\n\n### 2. 降阶基生成\n\n我们从 $M=5$ 个快照中生成一个降阶基。每个快照是针对 $M$ 个训练参数之一的高保真解 $u_h(\\mu_j^{\\mathrm{train}})$。这 $M$ 个向量，每个大小为 $K$，被收集为快照矩阵 $S_{\\text{snap}} \\in \\mathbb{R}^{K \\times M}$ 的列。然后我们对此矩阵进行（降维的）QR 分解，$S_{\\text{snap}} = VR$。矩阵 $V \\in \\mathbb{R}^{K \\times N}$ 的列构成了快照空间的一个标准正交基（在欧几里得内积下）。生成的基 $V$ 用于所有后续的测试用例。\n\n### 3. 降阶模型求解\n\n对于给定的测试参数 $\\mu$ 和基尺寸 $N \\le M$，RB 空间为 $V_N = \\text{span}\\{v_1, \\dots, v_N\\}$，其中 $v_i$ 是 $V$ 的前 $N$ 列。降阶解以 $u_N(\\mu) = V_N \\tilde{u}_N(\\mu)$ 的形式寻求，其中 $\\tilde{u}_N \\in \\mathbb{R}^N$ 是降阶坐标的向量。\n\n将伽辽金投影应用于弱形式，得到一个关于 $\\tilde{u}_N$ 的 $N \\times N$ 非线性系统：\n$$ V_N^T A(V_N \\tilde{u}_N; \\mu) V_N \\tilde{u}_N = V_N^T \\ell(\\mu) $$\n这个较小的系统使用与高保真模型相同的不动点迭代策略进行求解。全阶量 $A(u_N; \\mu)$ 和 $\\ell(\\mu)$ 在每一步中计算（此问题不需要离线/在线分解），然后投影到降阶空间上。\n\n### 4. 后验误差估计\n\n在计算出给定 $N$ 的收敛降阶解 $u_N(\\mu) = V_N \\tilde{u}_N(\\mu)$ 后，我们评估后验误差估计子 $\\eta_N(\\mu)$。这包括：\n1.  组装全阶刚度矩阵 $S(u_N;\\mu) = A(u_N(\\mu);\\mu)$ 和载荷向量 $\\ell(\\mu)$。由于双线性形式是对称正定（SPD）的，该矩阵即为用于误差范数的矩阵。\n2.  计算全阶残差向量 $r_N(\\mu) = \\ell(\\mu) - S(u_N;\\mu) u_N(\\mu)$。\n3.  将估计子计算为残差的对偶范数：\n    $$ \\eta_N(\\mu) = \\sqrt{ r_N(\\mu)^T S(u_N;\\mu)^{-1} r_N(\\mu) } $$\n    这可以通过先求解线性系统 $S(u_N;\\mu) z = r_N(\\mu)$ 得到 $z$，然后取 $\\eta_N(\\mu) = \\sqrt{r_N(\\mu)^T z}$ 来高效计算。\n\n对于每个测试用例，我们计算 $N=1, 2, 3, 4, 5$ 时的估计子 $\\eta_N(\\mu)$。然后我们计算 $N=1, 2, 3, 4$ 时的衰减比 $q_N(\\mu) = \\eta_{N+1}(\\mu) / \\eta_N(\\mu)$。最后，我们验证条件 $\\eta_{N+1}(\\mu) \\le \\eta_N(\\mu)$ 和 $0 \\le q_N(\\mu) \\le 1$ 对所有 $N \\in \\{1, 2, 3, 4\\}$ 都成立。最终输出是针对每个测试用例的布尔值，指示这些属性是否得到满足。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    # --- Problem Constants ---\n    K = 80\n    P_DEG = 0 # Not used explicitly as p=0 logic is hard-coded\n    H = 1.0 / K\n    C_PEN = 10.0\n    HF_TOL = 1e-10\n    ROM_TOL = 1e-10\n    MAX_ITER = 50\n    C_EV = 0.02\n    NU_MAX = 0.05\n    EPS_ETA = 1e-12\n    U_L, U_R = 1.0, 0.0\n    M = 5\n\n    # --- Training and Test Cases ---\n    train_cases = [\n        (0.015, 0.8), # (nu0, A)\n        (0.008, 1.2),\n        (0.012, 1.5),\n        (0.020, 0.6),\n        (0.010, 1.1),\n    ]\n    test_cases = [\n        (0.010, 1.0),\n        (0.007, 1.5),\n        (0.020, 0.5),\n    ]\n\n    # Shared pre-computations\n    x_nodes = np.linspace(0, 1, K + 1)\n    x_e_left = x_nodes[:-1]\n    x_e_right = x_nodes[1:]\n    \n    def compute_nu_art(u, nu0):\n        if not isinstance(u, np.ndarray):\n            u = np.array(u)\n\n        u_ext = np.concatenate(([U_L], u, [U_R]))\n        \n        # Gradient indicator G_e\n        jumps_abs = np.abs(u_ext[1:] - u_ext[:-1])\n        g_e = 0.5 * (jumps_abs[:-1] + jumps_abs[1:]) / H\n        \n        # Entropy normalization Delta_eta\n        eta = 0.5 * u**2\n        delta_eta = np.max(eta) - np.min(eta) + EPS_ETA\n        \n        # Artificial viscosity\n        nu_art = C_EV * H**2 * (u**2 * g_e) / delta_eta\n        np.clip(nu_art, 0, NU_MAX, out=nu_art)\n        \n        return nu_art\n\n    def assemble_stiffness(nu_art, nu0):\n        nu_total = nu0 + nu_art\n        \n        # Penalties\n        lambda_L = C_PEN * nu0 / H\n        lambda_R = C_PEN * nu0 / H\n        \n        nu_face_avg = 0.5 * (nu_total[:-1] + nu_total[1:])\n        lambda_F = C_PEN * nu_face_avg / H\n        \n        # Matrix assembly\n        diag = np.zeros(K)\n        diag[0] = lambda_L + lambda_F[0]\n        diag[K-1] = lambda_R + lambda_F[K-2]\n        diag[1:K-1] = lambda_F[:-1] + lambda_F[1:]\n        \n        off_diag = -lambda_F\n        \n        A = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        return A\n\n    def assemble_load(A_param, nu0):\n        # Source term integral\n        integral_s = (A_param / (5 * np.pi)) * (np.cos(5 * np.pi * x_e_left) - np.cos(5 * np.pi * x_e_right))\n        \n        l = np.copy(integral_s)\n        \n        # Boundary terms\n        lambda_L = C_PEN * nu0 / H\n        lambda_R = C_PEN * nu0 / H\n        l[0] += lambda_L * U_L\n        l[K-1] += lambda_R * U_R\n        \n        return l\n\n    def solve_nonlinear_system(mu, u_initial, is_rom=False, V_N=None):\n        nu0, A_param = mu\n        u = u_initial\n        \n        for _ in range(MAX_ITER):\n            u_old = u.copy()\n            \n            if is_rom:\n                u_full = V_N @ u\n                nu_art = compute_nu_art(u_full, nu0)\n                A_full = assemble_stiffness(nu_art, nu0)\n                l_full = assemble_load(A_param, nu0)\n                \n                A_rom = V_N.T @ A_full @ V_N\n                l_rom = V_N.T @ l_full\n                \n                u = np.linalg.solve(A_rom, l_rom)\n                if np.linalg.norm(u - u_old)  ROM_TOL:\n                    break\n            else: # HF\n                nu_art = compute_nu_art(u, nu0)\n                A = assemble_stiffness(nu_art, nu0)\n                l = assemble_load(A_param, nu0)\n                u = np.linalg.solve(A, l)\n                if np.linalg.norm(u - u_old)  HF_TOL:\n                    break\n        return u\n\n    # --- Step 1  2: Build Reduced Basis ---\n    snapshots = []\n    u_h_initial = 1.0 - (x_e_left + 0.5 * H)\n    for mu_train in train_cases:\n        u_h = solve_nonlinear_system(mu_train, u_h_initial)\n        snapshots.append(u_h)\n    \n    S_matrix = np.stack(snapshots, axis=1)\n    V, _ = np.linalg.qr(S_matrix, mode='reduced')\n\n    # --- Step 3: Evaluate Test Cases ---\n    final_results = []\n    for mu_test in test_cases:\n        nu0_test, A_test = mu_test\n        estimators = []\n        \n        for N in range(1, M + 1):\n            V_N = V[:, :N]\n            u_rom_initial = np.zeros(N)\n            \n            # Solve ROM\n            u_N_coeffs = solve_nonlinear_system(mu_test, u_rom_initial, is_rom=True, V_N=V_N)\n            u_N = V_N @ u_N_coeffs\n            \n            # Compute estimator\n            nu_art_N = compute_nu_art(u_N, nu0_test)\n            S_N = assemble_stiffness(nu_art_N, nu0_test)\n            l_N = assemble_load(A_test, nu0_test)\n            r_N = l_N - S_N @ u_N\n            \n            # eta_N^2 = r_N^T S_N^{-1} r_N\n            try:\n                z = np.linalg.solve(S_N, r_N)\n                eta_N_sq = r_N.T @ z\n                eta_N = np.sqrt(max(0, eta_N_sq))\n            except np.linalg.LinAlgError:\n                eta_N = np.inf # Should not happen with this SPD system\n\n            estimators.append(eta_N)\n\n        # Verify monotonicity\n        monotonic = True\n        for i in range(M - 1): # For N = 1, 2, 3, 4\n            eta_N = estimators[i]\n            eta_Np1 = estimators[i+1]\n            \n            if eta_Np1 > eta_N:\n                monotonic = False\n                break\n            \n            if eta_N > 0:\n                q_N = eta_Np1 / eta_N\n            else: # If eta_N is zero, eta_Np1 must also be zero\n                q_N = 0.0 if eta_Np1 == 0.0 else np.inf\n\n            if not (0.0 = q_N = 1.0):\n                monotonic = False\n                break\n        \n        final_results.append(monotonic)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\".lower())\n\nsolve()\n```", "id": "3361071"}, {"introduction": "误差估计器的一个强大应用是指导降阶基本身的构建，例如，为贪婪算法提供一个智能的停止准则。本练习介绍了一个先进的贝叶斯框架，用于预测估计器的效应指数，从而可以更准确地（且不过于保守地）估计真实误差。这项实践让您能够探索一种前沿的数据驱动技术，它将传统数值分析与统计学习相结合，以创建更高效的模型降阶工作流。[@problem_id:3361052]", "problem": "考虑一个强制、仿射参数化的椭圆模型问题，该问题通过高阶谱间断伽辽金(DG)方法离散化，其高保真真实解用于构建降阶基(RB)近似。对于每个参数 $\\mu \\in [0,1]$，令能量范数下的RB近似误差表示为 $e_N(\\mu) = \\lVert u(\\mu) - u_N(\\mu) \\rVert_X$，基于残差的后验误差估计量表示为 $\\Delta_N(\\mu)$，效用指数表示为 $\\eta_N(\\mu) = \\Delta_N(\\mu) / e_N(\\mu)$。在典型的强制性设置中，我们期望 $\\eta_N(\\mu) \\ge 1$。假设对于固定的RB空间大小 $N$，估计量的衰减遵循与谱DG行为一致的指数定律：$\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$，其中 $A(\\mu)$ 是一个光滑的振幅函数，$\\rho > 0$ 则代表谱衰减率。\n\n我们旨在设计一种算法，当真实误差的预测最坏情况可信上限低于目标容差 $\\tau$ 时，终止标准的贪婪RB增广过程。为此，我们使用线性高斯贝叶斯模型对效用指数的对数进行建模。令 $\\phi(\\mu) \\in \\mathbb{R}^d$ 为特征映射，$\\theta \\in \\mathbb{R}^d$ 为一个具有高斯先验 $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$ 的未知参数向量，并假设可观测值 $y(\\mu)$ 满足 $y(\\mu) = \\log(\\eta_N(\\mu)) = \\phi(\\mu)^\\top \\theta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$，且样本间相互独立。$\\theta$ 的后验分布和 $y(\\mu)$ 的后验预测分布必须根据线性高斯模型的贝叶斯法则计算得出。利用 $y(\\mu)$ 的后验预测分布，为每个 $\\mu$ 在可信水平 $1-\\beta$ 下获得 $\\eta_N(\\mu)$ 的一个可信下界，该下界在模型下被解释为 $\\eta_N(\\mu)$ 的下 $(\\beta)$-分位数。然后，将真实误差的预测可信上限定义为 $\\widehat{B}_N(\\mu) = \\Delta_N(\\mu) / \\underline{\\eta}_{\\beta}(\\mu)$，其中 $\\underline{\\eta}_{\\beta}(\\mu)$ 是 $\\eta_N(\\mu)$ 的 $(\\beta)$-分位数下界。在RB大小为 $N$ 时，贪婪过程在有限的训练网格 $\\mathcal{P}_{\\mathrm{train}}$ 上计算 $\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$，如果 $\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu) \\le \\tau$ 则终止。\n\n请构建一个程序，在一个合成但科学上一致的数据集上实现此终止逻辑，具体如下。\n\n- 使用参数域 $\\mathcal{P} = [0,1]$，将其离散化为一个均匀网格 $\\mathcal{P}_{\\mathrm{train}} = \\{\\mu_j\\}_{j=0}^{M-1}$，其中 $M = 21$，$\\mu_j = j/(M-1)$。\n- 选择特征映射 $\\phi(\\mu) = [1, \\mu, \\mu^2]^\\top$，因此 $d = 3$。\n- 使用一个固定的未知向量 $\\theta^\\star \\in \\mathbb{R}^3$ 和零观测噪声，合成为离线“神谕”评估提供基准真值的效用模型：$y^\\star(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$，其中 $\\theta^\\star = [0.25, 0.35, 0.10]^\\top$。那么对于所有 $\\mu \\in [0,1]$，$\\eta^\\star(\\mu) = \\exp(y^\\star(\\mu))$ 严格大于 $1$。\n- 使用基于残差的估计量模型 $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$，其中 $A(\\mu) = 0.5 + \\mu$。将 $\\rho$ 解释为一个抽象的谱DG衰减率，与多项式阶数控制的指数收敛性一致。\n- 用在 $\\mu \\in \\{0, 0.5, 1\\}$ 处的 $S_0 = 3$ 个精确“神谕”观测值初始化离线数据集：在这些点上观测 $y(\\mu) = y^\\star(\\mu)$ 以作为贝叶斯模型的种子。\n- 对于贝叶斯预测器，使用高斯先验 $\\theta \\sim \\mathcal{N}(\\theta_0, \\Sigma_0)$，其中 $\\theta_0 = [0, 0, 0]^\\top$ 且 $\\Sigma_0 = s_0^2 I_3$，其中 $s_0 > 0$ 是一个指定值。在预测器中使用指定的观测噪声标准差 $\\sigma > 0$，这反映了模型不确定性，而非“神谕”噪声。\n- 在RB大小 $N$ 等于离线数据集中样本数量时，执行以下循环：\n  - 计算 $\\theta$ 的后验分布和在每个 $\\mu \\in \\mathcal{P}_{\\mathrm{train}}$ 处 $y(\\mu)$ 的后验预测分布。\n  - 使用可信度参数 $\\beta \\in (0, 0.5)$，根据 $y(\\mu)$ 的高斯预测模型，构建 $\\eta_N(\\mu)$ 的可信下界 $\\underline{\\eta}_{\\beta}(\\mu)$。然后计算 $\\widehat{B}_N(\\mu) = \\Delta_N(\\mu)/\\underline{\\eta}_{\\beta}(\\mu)$ 及其在 $\\mathcal{P}_{\\mathrm{train}}$ 上的最大值，记为 $\\widehat{B}^{\\max}_N$。\n  - 如果 $\\widehat{B}^{\\max}_N \\le \\tau$，则终止并输出当前的RB大小 $N$。\n  - 否则，选择 $\\mu^\\star = \\arg\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$，用一个新的精确“神谕”观测值 $y(\\mu^\\star) = y^\\star(\\mu^\\star)$ 扩充离线数据集，将 $N$ 增加 $N \\leftarrow N+1$，并重复。不要多次选择相同的 $\\mu$。如果达到预先指定的最大迭代次数 $N_{\\max}$，则终止并输出已达到的 $N$。\n\n请根据高斯先验、线性观测模型和高斯分布的贝叶斯法则的定义，推导出必要的贝叶斯更新和预测公式。然后实现该算法并在以下测试套件上运行，其中每个测试用例是一个元组 $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max})$：\n\n- 测试 1: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.08, 0.10, 0.80, 0.10, 0.50, 20)$。\n- 测试 2: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.15, 0.10, 0.60, 0.10, 0.50, 20)$。\n- 测试 3: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.03, 0.05, 0.80, 0.15, 0.70, 20)$。\n- 测试 4: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.02, 0.01, 0.60, 0.25, 1.00, 12)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目是对应测试用例终止时的最终RB大小 $N$。例如，包含四个整数的输出应类似于 $[n_1,n_2,n_3,n_4]$，不含多余空格或文本。此问题不涉及物理单位。所有角度（如有）必须以弧度为单位，但此处不存在角度。程序输出中的所有分数量必须表示为小数。", "solution": "基本要素是强制问题的基于残差的后验界、效用指数以及线性高斯贝叶斯模型。对于每个参数 $\\mu$，RB能量范数误差 $e_N(\\mu)$ 和基于残差的界 $\\Delta_N(\\mu)$ 满足不等式 $e_N(\\mu) \\le \\Delta_N(\\mu)$。效用指数为 $\\eta_N(\\mu) = \\Delta_N(\\mu)/e_N(\\mu)$，由于使用了对强制性常数的安全下界，该值对于强制问题通常 $\\ge 1$。目标是利用从离线样本训练出的贝叶斯预测器获得的 $\\eta_N(\\mu)$ 的可信下界，来构造一个关于 $e_N(\\mu)$ 的保守可信上限。\n\n我们采用以下反映谱间断伽辽金行为的合成但科学上一致的模型。估计量以指数形式随 $N$ 衰减，即 $\\Delta_N(\\mu) = A(\\mu) \\exp(-\\rho N)$，这捕捉了谱收敛性，其速率参数为 $\\rho > 0$，振幅为一个光滑函数 $A(\\mu) = 0.5 + \\mu$。对于效用指数，我们将 $\\log(\\eta_N(\\mu))$ 建模为 $\\phi(\\mu) \\in \\mathbb{R}^3$ 的线性函数，其中 $\\phi(\\mu) = [1, \\mu, \\mu^2]^\\top$。我们假设\n$$\ny(\\mu) = \\log(\\eta_N(\\mu)) = \\phi(\\mu)^\\top \\theta + \\varepsilon,\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2),\n$$\n并带有高斯先验 $\\theta \\sim \\mathcal{N}(\\theta_0,\\Sigma_0)$。离线“神谕”提供来自基准真值 $\\theta^\\star = [0.25, 0.35, 0.10]^\\top$ 的无噪声观测值，因此 $y^\\star(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$ 且 $\\eta^\\star(\\mu) = \\exp(y^\\star(\\mu))$。由于 $\\phi(\\mu)$ 的分量非负且 $\\theta^\\star$ 的条目非负，对于所有 $\\mu \\in [0,1]$，我们有 $y^\\star(\\mu) \\ge 0$ 并因此有 $\\eta^\\star(\\mu) \\ge 1$。\n\n我们从 $\\mu \\in \\{0,0.5,1\\}$ 处的 $S_0 = 3$ 个离线观测开始，设 $N = S_0$，并通过贪婪选择进行迭代增广。在线性高斯模型下，$\\theta$ 的贝叶斯后验分布是高斯的。令 $\\Phi \\in \\mathbb{R}^{S \\times d}$ 为设计矩阵，其行是 $S$ 个已观测参数 $\\{\\mu_i\\}_{i=1}^S$ 对应的 $\\phi(\\mu_i)^\\top$，令 $y \\in \\mathbb{R}^S$ 为观测到的 $y(\\mu_i)$ 向量。高斯先验为 $\\theta \\sim \\mathcal{N}(\\theta_0,\\Sigma_0)$，其中 $\\theta_0 \\in \\mathbb{R}^d$，$\\Sigma_0 \\in \\mathbb{R}^{d \\times d}$ 是正定矩阵。似然函数为 $y \\mid \\theta \\sim \\mathcal{N}(\\Phi \\theta, \\sigma^2 I_S)$。根据高斯分布的贝叶斯法则，后验分布是\n$$\n\\theta \\mid y \\sim \\mathcal{N}(\\theta_{\\mathrm{post}}, \\Sigma_{\\mathrm{post}}),\n$$\n其中\n$$\n\\Sigma_{\\mathrm{post}} = \\left(\\Sigma_0^{-1} + \\sigma^{-2} \\Phi^\\top \\Phi\\right)^{-1}, \\quad \\theta_{\\mathrm{post}} = \\Sigma_{\\mathrm{post}} \\left(\\Sigma_0^{-1}\\theta_0 + \\sigma^{-2} \\Phi^\\top y \\right).\n$$\n对于任何新参数 $\\mu$， $y(\\mu) = \\phi(\\mu)^\\top \\theta + \\varepsilon$ 的后验预测分布是高斯分布，其均值和方差为\n$$\nm(\\mu) = \\phi(\\mu)^\\top \\theta_{\\mathrm{post}}, \\quad s^2(\\mu) = \\phi(\\mu)^\\top \\Sigma_{\\mathrm{post}} \\phi(\\mu) + \\sigma^2.\n$$\n由于 $\\eta_N(\\mu) = \\exp(y(\\mu))$ 且 $y(\\mu) \\sim \\mathcal{N}(m(\\mu), s^2(\\mu))$，我们得到了一个针对 $\\eta_N(\\mu)$ 的对数正态预测模型。对于可信度参数 $\\beta \\in (0,0.5)$，$\\eta_N(\\mu)$ 的一个 $(1-\\beta)$ 可信下界是其 $\\beta$-分位数\n$$\n\\underline{\\eta}_{\\beta}(\\mu) = \\exp\\left(m(\\mu) + s(\\mu) z_{\\beta}\\right), \\quad z_{\\beta} = \\Phi^{-1}(\\beta),\n$$\n其中 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数。那么，真实误差对应的预测可信上限为\n$$\n\\widehat{B}_N(\\mu) = \\frac{\\Delta_N(\\mu)}{\\underline{\\eta}_{\\beta}(\\mu)} = \\frac{A(\\mu)\\exp(-\\rho N)}{\\exp\\left(m(\\mu) + s(\\mu) z_{\\beta}\\right)}.\n$$\n我们计算最坏情况下的预测界为 $\\widehat{B}^{\\max}_N = \\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$。如果 $\\widehat{B}^{\\max}_N \\le \\tau$，我们终止贪婪过程；否则，我们选择 $\\mu^\\star = \\arg\\max_{\\mu \\in \\mathcal{P}_{\\mathrm{train}}} \\widehat{B}_N(\\mu)$，向“神谕”查询 $y^\\star(\\mu^\\star)$（在此合成设置中无噪声），扩充数据集，并设置 $N \\leftarrow N+1$。$\\Delta_N(\\mu)$ 的指数衰减是由谱DG方法的多项式阶数和稳定的强制性下界驱动的收敛性所启发的；贝叶斯预测器则根据从样本中学到的效用指数形态进行自适应调整。\n\n算法上，我们为每个测试用例 $(\\tau,\\beta,\\rho,\\sigma,s_0,N_{\\max})$ 实现以下步骤：\n- 初始化 $\\mathcal{P}_{\\mathrm{train}} = \\{\\mu_j\\}_{j=0}^{M-1}$，包含 $[0,1]$ 上的 $M=21$ 个均匀点。\n- 设置 $\\theta_0 = [0,0,0]^\\top$ 和 $\\Sigma_0 = s_0^2 I_3$。\n- 使用在 $\\{0,0.5,1\\}$ 处的 $S_0 = 3$ 个样本初始化数据集，其中 $y(\\mu) = \\phi(\\mu)^\\top \\theta^\\star$，并设置 $N = S_0$。\n- 循环直到终止：计算 $(\\theta_{\\mathrm{post}},\\Sigma_{\\mathrm{post}})$，然后对每个 $\\mu \\in \\mathcal{P}_{\\mathrm{train}}$ 计算 $m(\\mu)$ 和 $s(\\mu)$，接着计算 $\\underline{\\eta}_{\\beta}(\\mu)$ 和 $\\widehat{B}_N(\\mu)$，并确定 $\\widehat{B}^{\\max}_N$ 及其最大化点 $\\mu^\\star$。如果 $\\widehat{B}^{\\max}_N \\le \\tau$ 或 $N \\ge N_{\\max}$，停止并返回 $N$。否则，将 $(\\mu^\\star, y^\\star(\\mu^\\star))$ 添加到数据集并递增 $N$。\n- 确保没有参数被重复选择。\n\n测试套件使用：\n- 测试 1: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.08, 0.10, 0.80, 0.10, 0.50, 20)$。\n- 测试 2: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.15, 0.10, 0.60, 0.10, 0.50, 20)$。\n- 测试 3: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.03, 0.05, 0.80, 0.15, 0.70, 20)$。\n- 测试 4: $(\\tau, \\beta, \\rho, \\sigma, s_0, N_{\\max}) = (0.02, 0.01, 0.60, 0.25, 1.00, 12)$。\n\n程序为每个测试计算最终的RB大小 $N$，并将它们作为单个列表 $[n_1,n_2,n_3,n_4]$ 打印，不含额外文本。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n# Synthetic ground truth for log-effectivity: y*(mu) = phi(mu)^T theta_true\ntheta_true = np.array([0.25, 0.35, 0.10])\n\ndef phi(mu: float) -> np.ndarray:\n    \"\"\"Feature map phi(mu) = [1, mu, mu^2].\"\"\"\n    return np.array([1.0, mu, mu * mu])\n\ndef log_eta_true(mu: float) -> float:\n    \"\"\"Noise-free ground-truth log-effectivity.\"\"\"\n    return float(phi(mu).dot(theta_true))\n\ndef posterior_update(theta0: np.ndarray, Sigma0: np.ndarray,\n                     mus: np.ndarray, y: np.ndarray, sigma: float):\n    \"\"\"\n    Compute Gaussian posterior parameters for linear model:\n    y = Phi theta + eps, eps ~ N(0, sigma^2 I).\n    \"\"\"\n    Phi = np.vstack([phi(mu) for mu in mus])  # S x d\n    Sigma0_inv = np.linalg.inv(Sigma0)\n    precision = Sigma0_inv + (1.0 / (sigma ** 2)) * (Phi.T @ Phi)\n    Sigma_post = np.linalg.inv(precision)\n    b = Sigma0_inv @ theta0 + (1.0 / (sigma ** 2)) * (Phi.T @ y)\n    theta_post = Sigma_post @ b\n    return theta_post, Sigma_post\n\ndef predictive_log_stats(mu: float, theta_post: np.ndarray,\n                         Sigma_post: np.ndarray, sigma: float):\n    \"\"\"Posterior predictive mean and std for log-effectivity at mu.\"\"\"\n    ph = phi(mu)\n    m = float(ph.dot(theta_post))\n    s2 = float(ph.dot(Sigma_post @ ph) + sigma ** 2)\n    s = np.sqrt(max(s2, 0.0))\n    return m, s\n\ndef eta_lower_quantile(mu: float, theta_post: np.ndarray,\n                       Sigma_post: np.ndarray, sigma: float, beta: float) -> float:\n    \"\"\"\n    Lower credible bound (beta-quantile) for eta = exp(log-eta),\n    where log-eta ~ N(m, s^2). Quantile is exp(m + s * z_beta).\n    \"\"\"\n    m, s = predictive_log_stats(mu, theta_post, Sigma_post, sigma)\n    z_beta = norm.ppf(beta)\n    log_q = m + s * z_beta\n    q = float(np.exp(log_q))\n    # Guard: effectivity should be >= 1 typically; for conservatism clamp to min 1e-6\n    return max(q, 1e-12)\n\ndef amplitude(mu: float) -> float:\n    \"\"\"Amplitude A(mu) = 0.5 + mu.\"\"\"\n    return 0.5 + mu\n\ndef delta_N(mu: float, N: int, rho: float) -> float:\n    \"\"\"Residual-based estimator model: Delta_N(mu) = A(mu) * exp(-rho * N).\"\"\"\n    return amplitude(mu) * np.exp(-rho * N)\n\ndef greedy_terminate_once(tau: float, beta: float, rho: float,\n                          sigma: float, s0: float, N_max: int) -> int:\n    \"\"\"\n    Run the Bayesian-predicted greedy termination.\n    Return final basis size N at termination (either by criterion or reaching N_max).\n    \"\"\"\n    # Training grid\n    M = 21\n    mus_grid = np.linspace(0.0, 1.0, M).tolist()\n\n    # Prior\n    theta0 = np.zeros(3)\n    Sigma0 = (s0 ** 2) * np.eye(3)\n\n    # Initial offline samples (noise-free oracle)\n    init_mus = [0.0, 0.5, 1.0]\n    train_mus = list(init_mus)\n    y_vals = np.array([log_eta_true(mu) for mu in train_mus], dtype=float)\n    N = len(train_mus)\n\n    # Ensure we do not reselect already used mus\n    used = set(train_mus)\n\n    while True:\n        # Posterior given current data\n        theta_post, Sigma_post = posterior_update(theta0, Sigma0,\n                                                  np.array(train_mus, dtype=float),\n                                                  y_vals, sigma)\n\n        # Compute predicted credible upper bound on true error over grid\n        worst_B = -np.inf\n        worst_mu = None\n        for mu in mus_grid:\n            eta_low = eta_lower_quantile(mu, theta_post, Sigma_post, sigma, beta)\n            B = delta_N(mu, N, rho) / eta_low\n            if B > worst_B:\n                worst_B = B\n                worst_mu = mu\n\n        # Termination checks\n        if worst_B = tau or N >= N_max:\n            return N\n\n        # Enrich at worst_mu if not already used; otherwise, pick next worst\n        # Build a sorted list by decreasing B to find next unused if needed\n        Bs = []\n        for mu in mus_grid:\n            eta_low = eta_lower_quantile(mu, theta_post, Sigma_post, sigma, beta)\n            B = delta_N(mu, N, rho) / eta_low\n            Bs.append((B, mu))\n        Bs.sort(reverse=True, key=lambda t: t[0])\n        selected_mu = None\n        for _, mu in Bs:\n            if mu not in used:\n                selected_mu = mu\n                break\n\n        if selected_mu is None:\n            # No new parameter available; terminate to avoid infinite loop\n            return N\n\n        # Add new sample (noise-free oracle)\n        train_mus.append(selected_mu)\n        used.add(selected_mu)\n        y_vals = np.append(y_vals, log_eta_true(selected_mu))\n        N += 1\n\ndef solve():\n    # Define the test cases: (tau, beta, rho, sigma, s0, N_max)\n    test_cases = [\n        (0.08, 0.10, 0.80, 0.10, 0.50, 20),  # Test 1\n        (0.15, 0.10, 0.60, 0.10, 0.50, 20),  # Test 2\n        (0.03, 0.05, 0.80, 0.15, 0.70, 20),  # Test 3\n        (0.02, 0.01, 0.60, 0.25, 1.00, 12),  # Test 4\n    ]\n\n    results = []\n    for tau, beta, rho, sigma, s0, N_max in test_cases:\n        N_final = greedy_terminate_once(tau, beta, rho, sigma, s0, N_max)\n        # Ensure integer output\n        results.append(int(N_final))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3361052"}]}