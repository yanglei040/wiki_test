{"hands_on_practices": [{"introduction": "在贝叶斯推断中，后验分布与变分代价函数之间的联系在线性高斯模型中表现得最为清晰。本练习将引导你从贝叶斯定理出发，为这一经典情况推导出二次代价函数及其相关的正规方程。更进一步，你将探索如何利用先验协方差的结构来设计更高效的优化算法，这揭示了贝叶斯框架如何直接为数值求解提供信息。", "problem": "考虑一个线性逆问题，其状态向量为 $u \\in \\mathbb{R}^{n}$，观测算子为 $A \\in \\mathbb{R}^{n \\times n}$，数据为 $y \\in \\mathbb{R}^{n}$。假设高斯似然带有加性观测噪声 $\\eta \\sim \\mathcal{N}(0,\\Gamma)$，其中 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）矩阵，因此 $y = A u + \\eta$。并假设高斯先验 $u \\sim \\mathcal{N}(m_0,C_0)$，其协方差矩阵 $C_0 \\in \\mathbb{R}^{n \\times n}$ 为对称正定，均值为 $m_0 \\in \\mathbb{R}^{n}$。仅使用高斯测度的贝叶斯定理和多元正态分布的标准性质作为出发点。\n\n任务：\n1. 从 $p(u \\mid y) \\propto p(y \\mid u)\\,p(u)$ 和高斯密度负对数的定义（在不依赖于 $u$ 的加性常数范围内）出发，推导负对数后验代价泛函 $J(u)$（用 $A$、$\\Gamma$、$C_0$、$y$ 和 $m_0$ 表示）。然后，通过将一阶变分设为零，推导出表征后验最大化子（等价于 $J$ 的最小化子）的正规方程。除上述声明外，不要假设 $A$ 具有任何特殊结构。\n2. 引入先验白化变量 $v = C_0^{-1/2}(u - m_0)$，其中 $C_0^{1/2}$ 是 $C_0$ 唯一的对称正定平方根。通过代数变换将正规方程转化为其先验预处理形式，其中系统算子表示为单位阵加上一个先验预处理的数据失配海森矩阵。请用 $A$、$\\Gamma$、$C_0$、$y$ 和 $m_0$ 明确指出这个先验预处理系统矩阵和右侧项。\n3. 令 $\\langle x, z \\rangle_{C_0^{-1}} := x^{\\top} C_0^{-1} z$ 定义由先验精度 $C_0^{-1}$ 导出的内积。将在度量 $\\langle \\cdot, \\cdot \\rangle_{C_0^{-1}}$ 下最小化 $J$ 的最速下降法解释为 $J$ 关于此度量的梯度下降法，并推导出沿最速下降方向的精确线搜索步长公式（用欧几里得梯度 $\\nabla J(u)$、先验 $C_0$ 和海森矩阵 $H := \\nabla^{2} J(u)$ 表示）。\n4. 现在考虑一个具体实例，$n = 2$, $A = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$, $\\Gamma = \\operatorname{diag}(1,4)$, $C_0 = \\begin{pmatrix} 1  1/5 \\\\ 1/5  2 \\end{pmatrix}$, $m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$。令 $u_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 为在 $\\langle \\cdot, \\cdot \\rangle_{C_0^{-1}}$ 度量下最速下降法的初始迭代点。使用任务3中得到的公式，计算在 $u_0$ 处的精确线搜索步长 $\\alpha_0$ 并报告其数值。答案保留 $4$ 位有效数字。\n\n你的最终答案必须是 $\\alpha_0$ 的单一舍入值。", "solution": "### 任务1：代价泛函和正规方程的推导\n\n问题陈述观测模型为 $y = A u + \\eta$，噪声 $\\eta \\sim \\mathcal{N}(0, \\Gamma)$，状态的先验为 $u \\sim \\mathcal{N}(m_0, C_0)$。\n似然分布是在给定状态 $u$ 的情况下数据 $y$ 的分布。根据观测模型，这是 $y \\mid u \\sim \\mathcal{N}(A u, \\Gamma)$。因此，似然的概率密度函数（PDF）为\n$$p(y \\mid u) \\propto \\exp\\left(-\\frac{1}{2} (y - Au)^{\\top} \\Gamma^{-1} (y - Au)\\right)$$\n这里我们省略了归一化常数，因为它不依赖于 $u$。\n先验概率密度函数给出为\n$$p(u) \\propto \\exp\\left(-\\frac{1}{2} (u - m_0)^{\\top} C_0^{-1} (u - m_0)\\right)$$\n同样省略了归一化常数。\n\n根据贝叶斯定理，后验分布的概率密度函数与似然和先验的乘积成正比：$p(u \\mid y) \\propto p(y \\mid u) p(u)$。\n代入高斯概率密度函数的表达式，我们得到：\n$$p(u \\mid y) \\propto \\exp\\left(-\\frac{1}{2} (y - Au)^{\\top} \\Gamma^{-1} (y - Au)\\right) \\exp\\left(-\\frac{1}{2} (u - m_0)^{\\top} C_0^{-1} (u - m_0)\\right)$$\n$$p(u \\mid y) \\propto \\exp\\left(-\\left[ \\frac{1}{2} (y - Au)^{\\top} \\Gamma^{-1} (y - Au) + \\frac{1}{2} (u - m_0)^{\\top} C_0^{-1} (u - m_0) \\right]\\right)$$\n负对数后验代价泛函 $J(u)$ 被定义为后验概率密度函数的负对数（在加性常数范围内）。对上述表达式取负对数，我们得到：\n$$J(u) = \\frac{1}{2} (y - Au)^{\\top} \\Gamma^{-1} (y - Au) + \\frac{1}{2} (u - m_0)^{\\top} C_0^{-1} (u - m_0)$$\n这个泛函通常用加权范数写成 $J(u) = \\frac{1}{2} \\|y - Au\\|_{\\Gamma^{-1}}^2 + \\frac{1}{2} \\|u - m_0\\|_{C_0^{-1}}^2$。\n\n为了找到后验概率的最大化子（这等价于找到代价泛函 $J(u)$ 的最小化子），我们计算 $J(u)$ 关于 $u$ 的一阶变分并将其设为零。这等价于找到梯度 $\\nabla J(u)$ 并求解 $\\nabla J(u) = 0$。\n我们对 $J(u)$ 关于 $u$ 求导。对于一个一般的二次型 $q(x) = \\frac{1}{2} x^{\\top} Q x$，如果 $Q$ 是对称的，则 $\\nabla q(x) = Qx$。对于形式为 $(b-Mx)^\\top Q (b-Mx)$ 的表达式，其梯度为 $2(-M)^\\top Q (b-Mx) = -2M^\\top Q(b-Mx)$。\n第一项（数据失配项）的梯度是：\n$$\\nabla_u \\left( \\frac{1}{2} (y - Au)^{\\top} \\Gamma^{-1} (y - Au) \\right) = -A^{\\top} \\Gamma^{-1} (y - Au) = A^{\\top} \\Gamma^{-1} (Au - y)$$\n第二项（先验项，或称正则化项）的梯度是：\n$$\\nabla_u \\left( \\frac{1}{2} (u - m_0)^{\\top} C_0^{-1} (u - m_0) \\right) = C_0^{-1} (u - m_0)$$\n总梯度是这两项的和：\n$$\\nabla J(u) = A^{\\top} \\Gamma^{-1} (Au - y) + C_0^{-1} (u - m_0)$$\n将梯度设为零，$\\nabla J(u) = 0$，得到最优性条件：\n$$A^{\\top} \\Gamma^{-1} Au - A^{\\top} \\Gamma^{-1} y + C_0^{-1} u - C_0^{-1} m_0 = 0$$\n重新排列各项，形成关于 $u$ 的线性系统，我们得到正规方程：\n$$(A^{\\top} \\Gamma^{-1} A + C_0^{-1}) u = A^{\\top} \\Gamma^{-1} y + C_0^{-1} m_0$$\n\n### 任务2：先验预处理系统的推导\n\n我们引入先验白化变量 $v = C_0^{-1/2}(u - m_0)$，这意味着 $u = m_0 + C_0^{1/2} v$。我们将这个 $u$ 的表达式代入任务1中推导出的正规方程：\n$$(A^{\\top} \\Gamma^{-1} A + C_0^{-1}) (m_0 + C_0^{1/2} v) = A^{\\top} \\Gamma^{-1} y + C_0^{-1} m_0$$\n展开左侧：\n$$A^{\\top} \\Gamma^{-1} A m_0 + A^{\\top} \\Gamma^{-1} A C_0^{1/2} v + C_0^{-1} m_0 + C_0^{-1} C_0^{1/2} v = A^{\\top} \\Gamma^{-1} y + C_0^{-1} m_0$$\n项 $C_0^{-1} m_0$ 在等式两边都出现，可以消去。此外，$C_0^{-1} C_0^{1/2} = C_0^{-1/2}$。\n$$A^{\\top} \\Gamma^{-1} A m_0 + A^{\\top} \\Gamma^{-1} A C_0^{1/2} v + C_0^{-1/2} v = A^{\\top} \\Gamma^{-1} y$$\n将包含 $v$ 的项组合在一起，其余的移到右侧：\n$$(A^{\\top} \\Gamma^{-1} A C_0^{1/2} + C_0^{-1/2}) v = A^{\\top} \\Gamma^{-1} y - A^{\\top} \\Gamma^{-1} A m_0$$\n$$(A^{\\top} \\Gamma^{-1} A C_0^{1/2} + C_0^{-1/2}) v = A^{\\top} \\Gamma^{-1} (y - A m_0)$$\n为了得到先验预处理形式，我们将整个方程从左边乘以 $C_0^{1/2}$：\n$$C_0^{1/2} (A^{\\top} \\Gamma^{-1} A C_0^{1/2} + C_0^{-1/2}) v = C_0^{1/2} A^{\\top} \\Gamma^{-1} (y - A m_0)$$\n在左侧分配 $C_0^{1/2}$：\n$$(C_0^{1/2} A^{\\top} \\Gamma^{-1} A C_0^{1/2} + C_0^{1/2} C_0^{-1/2}) v = C_0^{1/2} A^{\\top} \\Gamma^{-1} (y - A m_0)$$\n由于 $C_0^{1/2} C_0^{-1/2} = I$（单位矩阵），我们得到最终的先验预处理系统：\n$$(I + C_0^{1/2} A^{\\top} \\Gamma^{-1} A C_0^{1/2}) v = C_0^{1/2} A^{\\top} \\Gamma^{-1} (y - A m_0)$$\n先验预处理系统矩阵是 $(I + C_0^{1/2} A^{\\top} \\Gamma^{-1} A C_0^{1/2})$，右侧项是 $C_0^{1/2} A^{\\top} \\Gamma^{-1} (y - A m_0)$。矩阵 $A^{\\top} \\Gamma^{-1} A$ 是数据失配项的海森矩阵，所以 $C_0^{1/2} (A^{\\top} \\Gamma^{-1} A) C_0^{1/2}$ 是先验预处理的数据失配海森矩阵。\n\n### 任务3：精确线搜索步长的推导\n\n对于关于内积 $\\langle x, z \\rangle_{C_0^{-1}} = x^{\\top} C_0^{-1} z$ 最小化 $J(u)$ 的最速下降法，使用更新规则 $u_{k+1} = u_k - \\alpha_k p_k$。下降方向 $p_k$ 是 $J$ 关于此度量的梯度。这个“黎曼”梯度，我们记作 $\\nabla_{C_0^{-1}} J$，与标准欧几里得梯度 $\\nabla J$ 的关系为 $\\nabla_{C_0^{-1}} J(u) = (C_0^{-1})^{-1} \\nabla J(u) = C_0 \\nabla J(u)$。\n因此，下降方向为 $p_k = C_0 \\nabla J(u_k)$。令 $g_k = \\nabla J(u_k)$。更新规则为 $u_{k+1} = u_k - \\alpha_k C_0 g_k$。\n\n精确线搜索步长 $\\alpha_k$ 通过沿搜索方向最小化泛函来找到：\n$$\\alpha_k = \\arg\\min_{\\alpha > 0} J(u_k - \\alpha C_0 g_k)$$\n我们通过将关于 $\\alpha$ 的导数设为零来找到最小值：\n$$\\frac{d}{d\\alpha} J(u_k - \\alpha C_0 g_k) = 0$$\n使用链式法则，这变为：\n$$-\\nabla J(u_k - \\alpha C_0 g_k)^{\\top} (C_0 g_k) = 0$$\n对于我们的二次泛函，梯度 $\\nabla J(u)$ 是 $u$ 的线性函数：$\\nabla J(u) = H u - b$，其中 $H = A^{\\top} \\Gamma^{-1} A + C_0^{-1}$ 是常数海森矩阵，而 $b = A^{\\top} \\Gamma^{-1} y + C_0^{-1} m_0$。\n所以，$\\nabla J(u_k - \\alpha C_0 g_k) = H(u_k - \\alpha C_0 g_k) - b = (H u_k - b) - \\alpha H C_0 g_k = g_k - \\alpha H C_0 g_k$。\n将此代入 $\\alpha_k$ 的最优性条件：\n$$(g_k - \\alpha_k H C_0 g_k)^{\\top} (C_0 g_k) = 0$$\n$$g_k^{\\top} C_0 g_k - \\alpha_k (H C_0 g_k)^{\\top} C_0 g_k = 0$$\n由于 $H$ 和 $C_0$ 是对称的，$(H C_0 g_k)^{\\top} = g_k^{\\top} C_0^{\\top} H^{\\top} = g_k^{\\top} C_0 H$。\n$$g_k^{\\top} C_0 g_k - \\alpha_k g_k^{\\top} C_0 H C_0 g_k = 0$$\n求解 $\\alpha_k$：\n$$\\alpha_k = \\frac{g_k^{\\top} C_0 g_k}{g_k^{\\top} C_0 H C_0 g_k}$$\n这就是精确线搜索步长公式，其中 $g_k = \\nabla J(u_k)$ 且 $H = \\nabla^2 J(u) = A^{\\top}\\Gamma^{-1}A + C_0^{-1}$。这可以根据搜索方向 $p_k = C_0 g_k$ 写成 $\\alpha_k = \\frac{g_k^\\top p_k}{p_k^\\top H p_k}$。\n\n### 任务4：步长 $\\alpha_0$ 的数值计算\n\n我们给出的具体实例是：\n$n = 2$, $A = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix}$, $\\Gamma = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$, $C_0 = \\begin{pmatrix} 1  1/5 \\\\ 1/5  2 \\end{pmatrix}$, $m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$，以及初始迭代点 $u_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。\n\n为了计算 $\\alpha_0 = \\frac{g_0^{\\top} C_0 g_0}{g_0^{\\top} C_0 H C_0 g_0}$，我们首先需要计算梯度 $g_0 = \\nabla J(u_0)$ 和海森矩阵 $H$。\n在 $u_0 = 0$ 处的欧几里得梯度是：\n$$g_0 = \\nabla J(0) = A^{\\top} \\Gamma^{-1} (A \\cdot 0 - y) + C_0^{-1} (0 - m_0)$$\n由于 $m_0 = 0$，这简化为 $g_0 = -A^{\\top} \\Gamma^{-1} y$。由于 $A$ 是对称的，所以 $A^\\top = A$。\n$g_0 = - \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = - \\begin{pmatrix} 2  1/4 \\\\ 1  3/4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = - \\begin{pmatrix} 2 + 2/4 \\\\ 1 + 6/4 \\end{pmatrix} = - \\begin{pmatrix} 5/2 \\\\ 5/2 \\end{pmatrix}$\n\n接下来，我们计算海森矩阵 $H = A^{\\top} \\Gamma^{-1} A + C_0^{-1}$。\n$A^{\\top} \\Gamma^{-1} A = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1/4 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} = \\begin{pmatrix} 2  1/4 \\\\ 1  3/4 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} = \\begin{pmatrix} 17/4  11/4 \\\\ 11/4  13/4 \\end{pmatrix}$。\n对于 $C_0^{-1}$，我们首先找到 $\\det(C_0) = 1(2) - (1/5)^2 = 49/25$。\n$C_0^{-1} = \\frac{1}{49/25} \\begin{pmatrix} 2  -1/5 \\\\ -1/5  1 \\end{pmatrix} = \\frac{25}{49} \\begin{pmatrix} 2  -1/5 \\\\ -1/5  1 \\end{pmatrix} = \\frac{1}{49} \\begin{pmatrix} 50  -5 \\\\ -5  25 \\end{pmatrix}$。\n$H = \\begin{pmatrix} 17/4  11/4 \\\\ 11/4  13/4 \\end{pmatrix} + \\frac{1}{49} \\begin{pmatrix} 50  -5 \\\\ -5  25 \\end{pmatrix} = \\frac{1}{196} \\left[ \\begin{pmatrix} 833  539 \\\\ 539  637 \\end{pmatrix} + \\begin{pmatrix} 200  -20 \\\\ -20  100 \\end{pmatrix} \\right] = \\frac{1}{196} \\begin{pmatrix} 1033  519 \\\\ 519  737 \\end{pmatrix}$。\n\n现在我们计算 $\\alpha_0$ 的分子：$g_0^{\\top} C_0 g_0$。\n$g_0^{\\top} C_0 g_0 = (-\\frac{5}{2})^2 \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1/5 \\\\ 1/5  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{25}{4} \\begin{pmatrix} 1+1/5  1/5+2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{25}{4} \\begin{pmatrix} 6/5  11/5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{25}{4} (\\frac{6}{5}+\\frac{11}{5}) = \\frac{25}{4} \\frac{17}{5} = \\frac{85}{4}$。\n\n以及分母：$g_0^{\\top} C_0 H C_0 g_0$。令 $p_0 = C_0 g_0$。\n$p_0 = \\begin{pmatrix} 1  1/5 \\\\ 1/5  2 \\end{pmatrix} \\left(-\\frac{5}{2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) = -\\frac{5}{2} \\begin{pmatrix} 1+1/5 \\\\ 1/5+2 \\end{pmatrix} = -\\frac{5}{2} \\begin{pmatrix} 6/5 \\\\ 11/5 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -11/2 \\end{pmatrix}$。\n分母是 $p_0^{\\top} H p_0$：\n$p_0^{\\top} H p_0 = \\begin{pmatrix} -3  -11/2 \\end{pmatrix} \\frac{1}{196} \\begin{pmatrix} 1033  519 \\\\ 519  737 \\end{pmatrix} \\begin{pmatrix} -3 \\\\ -11/2 \\end{pmatrix}$\n$= \\frac{1}{196} \\begin{pmatrix} 3  11/2 \\end{pmatrix} \\begin{pmatrix} 1033  519 \\\\ 519  737 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 11/2 \\end{pmatrix}$\n$= \\frac{1}{196} [1033(3^2) + 2(519)(3)(11/2) + 737(11/2)^2]$\n$= \\frac{1}{196} [9297 + 17127 + 737(121/4)] = \\frac{1}{196} [26424 + 89177/4]$\n$= \\frac{1}{196} [\\frac{105696 + 89177}{4}] = \\frac{194873}{784}$。\n\n最后，我们计算 $\\alpha_0$：\n$$\\alpha_0 = \\frac{85/4}{194873/784} = \\frac{85}{4} \\cdot \\frac{784}{194873} = \\frac{85 \\cdot 196}{194873} = \\frac{16660}{194873}$$\n数值上，这个值是 $\\alpha_0 \\approx 0.08549100...$。\n保留 $4$ 位有效数字得到 $0.08549$。", "answer": "$$\\boxed{0.08549}$$", "id": "3411439"}, {"introduction": "当正演模型为非线性时，代价函数的结构变得更加复杂，其曲率（即Hessian矩阵）对不确定性量化至关重要。本练习要求你深入分析真实后验曲率与常用的高斯-牛顿法所依赖的近似曲率之间的差异，这种差异直接影响基于拉普拉斯近似的后验不确定性的准确性。通过一个具体的计算任务，你将量化这种差异及其对不确定性评估的实际影响。", "problem": "考虑一个贝叶斯逆问题，其参数向量为 $u \\in \\mathbb{R}^d$，正演映射为 $G:\\mathbb{R}^d \\to \\mathbb{R}^m$，观测数据为 $y \\in \\mathbb{R}^m$，加性高斯观测噪声的协方差为 $\\Gamma \\in \\mathbb{R}^{m \\times m}$，以及高斯先验 $u \\sim \\mathcal{N}(u_0, C)$，其均值为 $u_0 \\in \\mathbb{R}^d$，协方差为 $C \\in \\mathbb{R}^{d \\times d}$。负对数后验（在数据同化中常称为代价函数）为\n$$\n\\Phi(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u)) + \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0).\n$$\n最大后验（MAP）估计量 $u^\\star$ 是 $\\Phi$ 的任意一个极小化子，而后验曲率在 $u^\\star$ 处的值是其 Hessian 矩阵 $\\nabla^2 \\Phi(u^\\star)$。Gauss–Newton Hessian 矩阵仅使用 $G$ 的一阶导数，由以下近似给出\n$$\nH_{\\mathrm{GN}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1},\n$$\n其中 $J_G(u)$ 表示 $G$ 在 $u$ 处的 Jacobian 矩阵。在非线性问题中，$H_{\\mathrm{GN}}(u^\\star)$ 通常与真实的后验曲率 $\\nabla^2 \\Phi(u^\\star)$ 不同，这种差异通过改变后验协方差的高斯近似来影响不确定性量化。\n\n从贝叶斯定理、似然和先验的定义以及复合函数的梯度和 Hessian 矩阵的计算法则出发，推导一个关于真实后验曲率 $\\nabla^2 \\Phi(u^\\star)$ 与 Gauss–Newton 近似 $H_{\\mathrm{GN}}(u^\\star)$ 之间差异的可计算表达式，该表达式应使用 Jacobian 矩阵 $J_G(u^\\star)$、残差 $r(u^\\star) = y - G(u^\\star)$ 以及正演模型各分量的二阶导数矩阵集合来表示。然后，设计一个算法，该算法能够：\n- 使用一个采用精确梯度和 Hessian 矩阵的二阶方法，通过最小化 $\\Phi(u)$ 来计算 $u^\\star$，\n- 在 $u^\\star$ 处评估真实的后验曲率和 Gauss–Newton 近似，\n- 量化曲率间隙及其对不确定性量化的下游影响。\n\n仅使用以下基本依据和定义：\n- 连接后验、先验和似然的贝叶斯定理。\n- 高斯噪声的负对数似然和高斯先验的负对数先验。\n- 复合映射的梯度和 Hessian 矩阵的链式法则。\n- 矩阵范数和迹的标准线性代数恒等式。\n\n对于每个测试用例，您必须实现并报告两个标量诊断指标：\n- 曲率间隙比 $g = \\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F \\,\\big/\\, \\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F$，其中 $\\lVert \\cdot \\rVert_F$ 表示 Frobenius 范数，\n- 两个高斯后验近似 $\\mathcal{N}(u^\\star, \\nabla^2 \\Phi(u^\\star)^{-1})$ 和 $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$ 之间的对称化 Kullback–Leibler 散度 $D$，其定义为这两个高斯分布之间的两个定向 Kullback–Leibler 散度之和。\n\n您的程序必须实现以下测试套件，其中所有矩阵和向量都已明确指定。在每个测试中，参数维度为 $d=2$，观测维度为 $m=2$。每个测试都指定了正演映射 $G$、先验 $(u_0, C)$、观测配置 $(y, \\Gamma)$ 以及一个与 $u_0$ 相等的指定优化初值。\n\n- 测试 1（线性，预计无曲率间隙）：\n  - $G(u) = \\begin{bmatrix} u_1 + 0.5\\,u_2 \\\\ u_2 \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.2 \\\\ -0.3 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(0.49, 0.25\\big)$,\n  - $\\Gamma = \\begin{bmatrix} 0.09  0.03 \\\\ 0.03  0.16 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$.\n\n- 测试 2（非线性，在先验均值处后验精确平稳，MAP 处残差为零）：\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(1.0, 1.0\\big)$,\n  - $\\Gamma = \\mathrm{diag}\\big(0.01, 0.01\\big)$,\n  - $y = \\begin{bmatrix} 0.2 \\\\ \\exp(-0.1) \\end{bmatrix}$.\n\n- 测试 3（非线性，数据约束良好，预计曲率间隙较小）：\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(10.0, 10.0\\big)$,\n  - $\\Gamma = \\mathrm{diag}\\big(0.04, 0.04\\big)$,\n  - $y = \\begin{bmatrix} 0.8 \\\\ \\exp(0.3) \\end{bmatrix}$.\n\n- 测试 4（非线性，弱数据和信息丰富的先验，预计曲率间隙较大）：\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(0.25, 0.25\\big)$,\n  - $\\Gamma = \\mathrm{diag}\\big(4.0, 4.0\\big)$,\n  - $y = \\begin{bmatrix} 0.8 \\\\ \\exp(0.3) \\end{bmatrix}$.\n\n- 测试 5（具有耦合和相关噪声的非线性）：\n  - $G(u) = \\begin{bmatrix} \\exp(u_1 + 0.5\\,u_2) \\\\ u_1^2 - u_2 \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(1.0, 1.0\\big)$,\n  - $\\Gamma = \\begin{bmatrix} 0.09  0.04 \\\\ 0.04  0.16 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ 0.21 \\end{bmatrix}$.\n\n算法要求：\n- 对于每个测试，使用带有精确梯度和 Hessian 矩阵的二阶方法，通过最小化 $\\Phi(u)$ 来计算 $u^\\star$。\n- 在 $u^\\star$ 处，构建残差 $r(u^\\star) = y - G(u^\\star)$、Jacobian 矩阵 $J_G(u^\\star)$ 以及各分量的 Hessian 矩阵 $\\nabla^2 G_i(u^\\star)$（其中 $i \\in \\{1,2\\}$）。\n- 构建真实的后验曲率 $\\nabla^2 \\Phi(u^\\star)$ 和 Gauss–Newton 曲率 $H_{\\mathrm{GN}}(u^\\star)$。\n- 按规定计算曲率间隙比 $g$。\n- 计算 $\\mathcal{N}(u^\\star, \\nabla^2 \\Phi(u^\\star)^{-1})$ 和 $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$ 之间的对称化 Kullback–Leibler 散度 $D$。\n- 将每个测试的两个诊断指标汇总为一个长度为 2 的列表 $[g, D]$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目对应上面顺序的一个测试。每个条目本身必须是一个包含两个浮点数的列表，顺序为 $[g,D]$。例如，输出格式必须类似于 $[[g_1,D_1],[g_2,D_2],\\dots]$。", "solution": "### Hessian 矩阵差异的推导\n\n目标是推导差异 $\\nabla^2 \\Phi(u) - H_{\\mathrm{GN}}(u)$ 的表达式。我们首先将负对数后验 $\\Phi(u)$ 分解为其似然和先验分量：\n$$\n\\Phi(u) = \\Phi_{\\text{like}}(u) + \\Phi_{\\text{prior}}(u)\n$$\n其中负对数似然为 $\\Phi_{\\text{like}}(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u))$，负对数先验为 $\\Phi_{\\text{prior}}(u) = \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0)$。\n\nHessian 矩阵是可加的，所以 $\\nabla^2 \\Phi(u) = \\nabla^2 \\Phi_{\\text{like}}(u) + \\nabla^2 \\Phi_{\\text{prior}}(u)$。\n\n**1. 先验项的 Hessian 矩阵**\n\n先验项 $\\Phi_{\\text{prior}}(u)$ 是 $u$ 的二次函数。其梯度为 $\\nabla \\Phi_{\\text{prior}}(u) = C^{-1}(u - u_0)$，Hessian 矩阵为：\n$$\n\\nabla^2 \\Phi_{\\text{prior}}(u) = C^{-1}\n$$\n这是一个常数矩阵，与 $u$ 无关。\n\n**2. 似然项的 Hessian 矩阵**\n\n似然项 $\\Phi_{\\text{like}}(u)$ 涉及正演模型 $G(u)$ 的复合。设残差为 $r(u) = y - G(u)$。则 $\\Phi_{\\text{like}}(u) = \\frac{1}{2} r(u)^\\top \\Gamma^{-1} r(u)$。\n使用链式法则求得梯度为：\n$$\n\\nabla \\Phi_{\\text{like}}(u) = -J_G(u)^\\top \\Gamma^{-1} (y - G(u))\n$$\n其中 $J_G(u)$ 是 $G(u)$ 的 Jacobian 矩阵。为了求 Hessian 矩阵，我们再次使用链式法则和乘法法则对梯度求导，得到：\n$$\n\\nabla^2 \\Phi_{\\text{like}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\n其中 $\\nabla^2 G_i(u)$ 是正演映射第 $i$ 个分量的 Hessian 矩阵，而 $[\\cdot]_i$ 表示向量的第 $i$ 个分量。\n\n**3. 总 Hessian 矩阵与差异**\n\n负对数后验的完整 Hessian 矩阵是：\n$$\n\\nabla^2 \\Phi(u) = \\left( J_G(u)^\\top \\Gamma^{-1} J_G(u) - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u) \\right) + C^{-1}\n$$\n重新整理各项以组合 Gauss-Newton 部分：\n$$\n\\nabla^2 \\Phi(u) = \\underbrace{J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1}}_{H_{\\mathrm{GN}}(u)} - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\n因此，真实 Hessian 矩阵与 Gauss-Newton 近似之间的差异是：\n$$\n\\nabla^2 \\Phi(u) - H_{\\mathrm{GN}}(u) = - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\n在 MAP 估计 $u^\\star$ 处评估此表达式以量化曲率间隙。\n\n### 算法设计\n\n对每个测试用例，算法按以下步骤进行：\n\n1.  **优化**：从指定的初始猜测值 $u_0$ 开始，通过最小化 $\\Phi(u)$ 来找到 MAP 估计 $u^\\star$。我们使用需要精确梯度 $\\nabla\\Phi(u)$ 和 Hessian 矩阵 $\\nabla^2\\Phi(u)$ 的二阶方法。\n\n2.  **曲率评估**：在 $u^\\star$ 处，评估两个曲率矩阵：\n    - 真实的后验曲率 $\\nabla^2\\Phi(u^\\star)$。\n    - Gauss-Newton 近似 $H_{\\mathrm{GN}}(u^\\star) = J_G(u^\\star)^\\top \\Gamma^{-1} J_G(u^\\star) + C^{-1}$。\n\n3.  **诊断指标计算**：\n    - **曲率间隙比 ($g$)**：两个 Hessian 矩阵之间的相对差异用 Frobenius 范数衡量：\n      $$ g = \\frac{\\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F}{\\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F} $$\n    - **对称化 KL 散度 ($D$)**：两个具有相同均值 $u^\\star$ 和不同协方差矩阵 $\\Sigma_{\\text{true}} = (\\nabla^2 \\Phi(u^\\star))^{-1}$ 与 $\\Sigma_{\\text{GN}} = (H_{\\mathrm{GN}}(u^\\star))^{-1}$ 的高斯分布之间的对称化 KL 散度为：\n      $$ D = \\frac{1}{2} \\left( \\mathrm{tr}(\\Sigma_{\\text{GN}}^{-1} \\Sigma_{\\text{true}}) + \\mathrm{tr}(\\Sigma_{\\text{true}}^{-1} \\Sigma_{\\text{GN}}) - 2d \\right) $$\n      其中 $d$ 是参数维度。\n\n4.  **聚合**：为每个测试用例计算诊断指标对 $[g, D]$，并收集到一个最终的列表的列表中以供输出。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the results.\n    \"\"\"\n    \n    test_cases = [\n        # Test 1 (linear, no curvature gap expected)\n        {\n            \"G\": lambda u: np.array([u[0] + 0.5 * u[1], u[1]]),\n            \"J_G\": lambda u: np.array([[1.0, 0.5], [0.0, 1.0]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.zeros((2, 2))],\n            \"u0\": np.array([0.2, -0.3]),\n            \"C\": np.diag([0.49, 0.25]),\n            \"Gamma\": np.array([[0.09, 0.03], [0.03, 0.16]]),\n            \"y\": np.array([1.0, -0.5])\n        },\n        # Test 2 (nonlinear, zero residual at MAP)\n        {\n            \"G\": lambda u: np.array([u[0], np.exp(u[1])]),\n            \"J_G\": lambda u: np.array([[1.0, 0.0], [0.0, np.exp(u[1])]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.array([[0.0, 0.0], [0.0, np.exp(u[1])]])],\n            \"u0\": np.array([0.2, -0.1]),\n            \"C\": np.diag([1.0, 1.0]),\n            \"Gamma\": np.diag([0.01, 0.01]),\n            \"y\": np.array([0.2, np.exp(-0.1)])\n        },\n        # Test 3 (nonlinear, well-constrained, small gap)\n        {\n            \"G\": lambda u: np.array([u[0], np.exp(u[1])]),\n            \"J_G\": lambda u: np.array([[1.0, 0.0], [0.0, np.exp(u[1])]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.array([[0.0, 0.0], [0.0, np.exp(u[1])]])],\n            \"u0\": np.array([0.0, 0.0]),\n            \"C\": np.diag([10.0, 10.0]),\n            \"Gamma\": np.diag([0.04, 0.04]),\n            \"y\": np.array([0.8, np.exp(0.3)])\n        },\n        # Test 4 (nonlinear, weak data, larger gap)\n        {\n            \"G\": lambda u: np.array([u[0], np.exp(u[1])]),\n            \"J_G\": lambda u: np.array([[1.0, 0.0], [0.0, np.exp(u[1])]]),\n            \"hess_G_components\": lambda u: [np.zeros((2, 2)), np.array([[0.0, 0.0], [0.0, np.exp(u[1])]])],\n            \"u0\": np.array([0.0, 0.0]),\n            \"C\": np.diag([0.25, 0.25]),\n            \"Gamma\": np.diag([4.0, 4.0]),\n            \"y\": np.array([0.8, np.exp(0.3)])\n        },\n        # Test 5 (nonlinear with coupling and correlated noise)\n        {\n            \"G\": lambda u: np.array([np.exp(u[0] + 0.5 * u[1]), u[0]**2 - u[1]]),\n            \"J_G\": lambda u: np.array([\n                [np.exp(u[0] + 0.5 * u[1]), 0.5 * np.exp(u[0] + 0.5 * u[1])],\n                [2 * u[0], -1.0]\n            ]),\n            \"hess_G_components\": lambda u: [\n                np.exp(u[0] + 0.5 * u[1]) * np.array([[1.0, 0.5], [0.5, 0.25]]),\n                np.array([[2.0, 0.0], [0.0, 0.0]])\n            ],\n            \"u0\": np.array([0.0, 0.0]),\n            \"C\": np.diag([1.0, 1.0]),\n            \"Gamma\": np.array([[0.09, 0.04], [0.04, 0.16]]),\n            \"y\": np.array([1.0, 0.21])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = 2 \n        G = case[\"G\"]\n        J_G = case[\"J_G\"]\n        hess_G_components = case[\"hess_G_components\"]\n        u0 = case[\"u0\"]\n        C = case[\"C\"]\n        Gamma = case[\"Gamma\"]\n        y = case[\"y\"]\n\n        C_inv = np.linalg.inv(C)\n        Gamma_inv = np.linalg.inv(Gamma)\n\n        def phi(u):\n            res_obs = y - G(u)\n            res_prior = u - u0\n            return 0.5 * res_obs.T @ Gamma_inv @ res_obs + 0.5 * res_prior.T @ C_inv @ res_prior\n\n        def grad_phi(u):\n            JG_u = J_G(u)\n            res_obs = y - G(u)\n            res_prior = u - u0\n            return -JG_u.T @ Gamma_inv @ res_obs + C_inv @ res_prior\n\n        def hess_phi(u):\n            JG_u = J_G(u)\n            res_obs = y - G(u)\n            hess_G = hess_G_components(u)\n            \n            H_GN = JG_u.T @ Gamma_inv @ JG_u + C_inv\n            \n            w = Gamma_inv @ res_obs\n            discrepancy_term = np.zeros((d, d))\n            for i in range(len(w)):\n                discrepancy_term -= w[i] * hess_G[i]\n                \n            return H_GN + discrepancy_term\n\n        # Find the MAP estimate u_star using a second-order method\n        opt_result = minimize(phi, u0, method='trust-ncg', jac=grad_phi, hess=hess_phi, tol=1e-9)\n        u_star = opt_result.x\n        \n        # Evaluate true and Gauss-Newton Hessians at u_star\n        H_true = hess_phi(u_star)\n        J_G_star = J_G(u_star)\n        H_GN = J_G_star.T @ Gamma_inv @ J_G_star + C_inv\n\n        # Compute diagnostic 1: curvature gap ratio g\n        norm_diff = np.linalg.norm(H_true - H_GN, 'fro')\n        norm_true = np.linalg.norm(H_true, 'fro')\n        g = norm_diff / norm_true if norm_true > 0 else 0.0\n\n        # Compute diagnostic 2: symmetrized KL divergence D\n        try:\n            H_true_inv = np.linalg.inv(H_true)\n            H_GN_inv = np.linalg.inv(H_GN)\n            \n            term1 = np.trace(H_GN @ H_true_inv)\n            term2 = np.trace(H_true @ H_GN_inv)\n\n            D_val = 0.5 * (term1 + term2 - 2 * d)\n        except np.linalg.LinAlgError:\n            D_val = np.nan\n\n        results.append([g, D_val])\n\n    # Format output as specified\n    formatted_results = [f\"[{g:.8f},{D:.8f}]\" for g, D in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3411467"}, {"introduction": "虽然最大后验（MAP）估计作为代价函数的最小值在实践中非常有用，但它有一个必须被理解的理论特性：在非线性参数变换下不具备不变性。本练习通过一个简洁的例子，揭示了为何直接变换代价函数会导致与经过雅可比行列式校正的真实后验密度所导出的MAP估计不一致。这个看似简单的计算突显了将代价函数最小化等同于完整贝叶斯推断时需要注意的一个重要警示。", "problem": "考虑一个数据同化领域的一维反问题，其中未知状态 $u \\in \\mathbb{R}$ 根据单个观测值 $y \\in \\mathbb{R}$ 进行推断。$u$ 的先验分布是高斯分布，其密度为 $p_U(u) \\propto \\exp\\!\\big(-\\frac{1}{2}u^{2}\\big)$；观测模型也是高斯模型，其密度为 $p(y \\mid u) \\propto \\exp\\!\\big(-\\frac{1}{2}(y - u)^{2}\\big)$。您观测到 $y = 1$。后验分布 $p(u \\mid y)$ 由贝叶斯法则定义，在 $u$-参数化下，相关的变分目标（负对数后验，不计加性常数）为 $J_u(u) = \\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2}$。\n\n现在考虑重参数化 $v = \\phi(u)$，其中 $\\phi(u) = \\exp(u)$，这定义了一个新的参数 $v \\in (0, \\infty)$，它通过 $u = \\ln(v)$ 与 $u$ 相关联。变换后的后验密度 $p(v \\mid y)$ 是通过概率密度的变量替换公式定义的。同时，定义在 $v$-参数化下的一个错误指定的变分目标，\n$$\nJ_v^{\\mathrm{naive}}(v) = \\frac{1}{2}(v - y)^{2} + \\frac{1}{2}v^{2},\n$$\n该目标将 $v$ 视为直接具有高斯先验，并忽略了变换的雅可比行列式。这个 $J_v^{\\mathrm{naive}}(v)$ 不等于正确变换后的 $J_u(\\ln v)$ 加上由后验密度的变量替换所隐含的雅可比项。\n\n从高斯似然和先验、贝叶斯法则以及概率密度的变量替换公式等基本定义出发，完成以下计算：\n\n1. 计算在 $u$-参数化下的最大后验概率 (MAP) 位置，定义为 $J_u(u)$ 的最小化子。\n2. 通过将后验 $p(u \\mid y)$ 使用变量替换公式一致地变换到 $p(v \\mid y)$，然后在 $v \\in (0, \\infty)$ 上最大化 $p(v \\mid y)$ 来计算在 $v$-参数化下的 MAP。\n3. 计算错误指定的目标函数 $J_v^{\\mathrm{naive}}(v)$ 的最小化子。\n\n将这三个值组成的元组，按 $(u_{\\mathrm{MAP}}, v_{\\mathrm{MAP}}^{\\mathrm{transformed}}, v_{\\mathrm{MAP}}^{\\mathrm{naive}})$ 的顺序，以 LaTeX 的 $\\mathrm{pmatrix}$ 环境表示为单个行矩阵。无需四舍五入。不涉及单位。最终答案仅以所要求的行矩阵形式明确给出。", "solution": "### 1. 计算 $u$ 的 MAP 估计 ($u_{\\mathrm{MAP}}$)\n\n未知状态 $u \\in \\mathbb{R}$ 的 MAP 估计 $u_{\\mathrm{MAP}}$ 是使后验密度 $p(u \\mid y)$ 最大化的 $u$ 值。这等价于最小化相关的变分目标（或成本函数）：\n$$ J_u(u) = \\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2} $$\n为求 $J_u(u)$ 的最小化子，我们计算它关于 $u$ 的导数，并令其为零：\n$$ \\frac{dJ_u}{du} = -(y - u) + u = 2u - y $$\n令导数为零，得到 $u = \\frac{y}{2}$。二阶导数为 $2 > 0$，证实了该点是一个极小值点。代入观测值 $y = 1$：\n$$ u_{\\mathrm{MAP}} = \\frac{1}{2} $$\n\n### 2. 通过变换计算 $v$ 的 MAP 估计 ($v_{\\mathrm{MAP}}^{\\mathrm{transformed}}$)\n\n我们给定了重参数化 $v = \\exp(u)$，其逆变换为 $u = \\ln(v)$。为了求出 $v$ 的后验密度 $p(v \\mid y)$，我们使用概率密度的变量替换公式：\n$$ p(v \\mid y) = p(u(v) \\mid y) \\left| \\frac{du}{dv} \\right| $$\n其中 $p(u \\mid y) \\propto \\exp(-J_u(u))$ 且 $\\left| \\frac{du}{dv} \\right| = \\left| \\frac{1}{v} \\right| = \\frac{1}{v}$（因为 $v > 0$）。代入后可得：\n$$ p(v \\mid y) \\propto \\frac{1}{v} \\exp\\left(-J_u(\\ln(v))\\right) = \\frac{1}{v} \\exp\\left(-\\left[\\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2}\\right]\\right) $$\n$v$ 的 MAP 估计，记为 $v_{\\mathrm{MAP}}^{\\mathrm{transformed}}$，是使 $p(v \\mid y)$ 最大化的值。这等价于最小化该密度的负对数。我们将 $v$ 的正确目标函数定义为 $J_v^{\\mathrm{correct}}(v) = -\\ln(p(v \\mid y))$（不计加性常数）：\n$$ J_v^{\\mathrm{correct}}(v) = \\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2} + \\ln(v) $$\n为求最小化子，我们将 $J_v^{\\mathrm{correct}}(v)$ 对 $v$ 求导，并令导数为零：\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = (y - \\ln(v)) \\cdot \\left(-\\frac{1}{v}\\right) + \\ln(v) \\cdot \\frac{1}{v} + \\frac{1}{v} = \\frac{1}{v} \\left( -y + \\ln(v) + \\ln(v) + 1 \\right) = \\frac{1}{v} \\left( 2\\ln(v) - y + 1 \\right) $$\n由于 $v > 0$，我们令括号内的项为零：\n$$ 2\\ln(v) - y + 1 = 0 \\implies \\ln(v) = \\frac{y - 1}{2} $$\n因此，$v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = \\exp\\left(\\frac{y-1}{2}\\right)$。代入观测值 $y = 1$：\n$$ v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = \\exp\\left(\\frac{1 - 1}{2}\\right) = \\exp(0) = 1 $$\n\n### 3. 计算朴素目标函数的最小化子 ($v_{\\mathrm{MAP}}^{\\mathrm{naive}}$)\n\n问题定义了一个为 $v$ 错误指定或“朴素”的目标函数：\n$$ J_v^{\\mathrm{naive}}(v) = \\frac{1}{2}(v - y)^{2} + \\frac{1}{2}v^{2} $$\n我们通过求导来找到这个函数的最小化子，记为 $v_{\\mathrm{MAP}}^{\\mathrm{naive}}$：\n$$ \\frac{dJ_v^{\\mathrm{naive}}}{dv} = (v - y) + v = 2v - y $$\n令导数为零，得到 $v = \\frac{y}{2}$。二阶导数为 $2 > 0$，确认是一个极小值点。代入观测值 $y = 1$：\n$$ v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{1}{2} $$\n\n### 结果总结\n\n计算出的三个值为：\n1.  $u_{\\mathrm{MAP}} = \\frac{1}{2}$\n2.  $v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = 1$\n3.  $v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{1}{2}$\n\n最终答案是行矩阵 $(u_{\\mathrm{MAP}}, v_{\\mathrm{MAP}}^{\\mathrm{transformed}}, v_{\\mathrm{MAP}}^{\\mathrm{naive}})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}   1  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3411380"}]}