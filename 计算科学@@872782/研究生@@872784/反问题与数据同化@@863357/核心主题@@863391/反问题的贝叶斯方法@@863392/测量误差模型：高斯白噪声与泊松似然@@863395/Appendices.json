{"hands_on_practices": [{"introduction": "在数据同化中，我们经常将连续时间过程中的测量误差理想化为高斯白噪声。然而，实际测量总是离散的，通常是对一个时间窗口的平均。本练习将理论与实践联系起来，通过推导时间平均后离散测量的方差，揭示了连续噪声强度与离散数据变异性之间的基本关系[@problem_id:3402401]。", "problem": "考虑一个连续时间测量误差模型，其中加性误差过程被理想化为均值为 $0$ 的高斯白噪声。具体来说，令 $\\varepsilon(t)$ 表示一个均值为 $0$ 的过程，其协方差分布为 $\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)$，其中 $\\delta(\\cdot)$ 是狄拉克δ分布，$\\sigma^{2} > 0$ 是一个恒定的噪声强度参数。假设通过对固定宽度 $\\Delta > 0$ 的不相交时间区间 $I_{i} = [t_{i}, t_{i} + \\Delta)$ 上的误差进行平均来收集测量值，得到区间平均误差\n$$\n\\bar{y}_{i} = \\frac{1}{\\Delta}\\int_{I_{i}} \\varepsilon(t)\\,dt.\n$$\n仅从方差和协方差的定义，以及狄拉克δ分布的定义性质出发，推导 $\\mathrm{Var}(\\bar{y}_{i})$ 作为 $\\sigma^{2}$ 和 $\\Delta$ 的函数的精确表达式。此外，请提供一个原理性的解释，说明对 $\\Delta$ 的依赖性是如何以及为何从该过程的协方差结构中产生的。请用闭式解析表达式表示你的最终答案。无需进行数值近似或四舍五入。", "solution": "所述问题在科学上是合理的、适定的、客观的且内部一致的。它提出了一个随机过程研究中的标准理论练习，具体涉及时间平均高斯白噪声的性质。所有必要的定义和参数都已提供，以得出一个唯一且有意义的解。因此，我们可以开始推导。\n\n问题要求推导 $\\mathrm{Var}(\\bar{y}_{i})$，其中区间平均误差 $\\bar{y}_{i}$ 定义为：\n$$\n\\bar{y}_{i} = \\frac{1}{\\Delta}\\int_{I_{i}} \\varepsilon(t)\\,dt = \\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\n$$\n误差过程 $\\varepsilon(t)$ 被指定为均值为0的高斯白噪声过程，其特征由其一阶和二阶矩决定：\n$$\n\\mathbb{E}[\\varepsilon(t)] = 0\n$$\n$$\n\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)\n$$\n其中 $\\mathbb{E}[\\cdot]$ 表示期望算子，$\\sigma^2$ 是恒定的噪声强度，$\\delta(\\cdot)$ 是狄拉克δ分布。\n\n我们从随机变量 $X$ 的方差基本定义开始：\n$$\n\\mathrm{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n$$\n首先，我们必须计算 $\\bar{y}_i$ 的期望值。利用期望算子的线性性质和Fubini定理来交换期望和积分，我们发现：\n$$\n\\mathbb{E}[\\bar{y}_{i}] = \\mathbb{E}\\left[\\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\\right] = \\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\mathbb{E}[\\varepsilon(t)]\\,dt\n$$\n由于该过程均值为0，即对所有 $t$ 都有 $\\mathbb{E}[\\varepsilon(t)] = 0$，所以积分为零：\n$$\n\\mathbb{E}[\\bar{y}_{i}] = \\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} 0 \\,dt = 0\n$$\n均值为 $0$ 时，方差简化为该变量平方的均值：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\mathbb{E}[(\\bar{y}_{i} - 0)^2] = \\mathbb{E}[\\bar{y}_{i}^2]\n$$\n代入 $\\bar{y}_{i}$ 的定义：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\mathbb{E}\\left[\\left(\\frac{1}{\\Delta}\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\\right)^2\\right]\n$$\n为了清晰起见，我们使用不同的积分变量 $t$ 和 $s$，将平方积分表示为两个相同积分的乘积：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\mathbb{E}\\left[\\left(\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\,dt\\right)\\left(\\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(s)\\,ds\\right)\\right]\n$$\n这个积分的乘积可以写成在方形区域 $[t_i, t_i+\\Delta) \\times [t_i, t_i+\\Delta)$ 上的二重积分：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\mathbb{E}\\left[\\int_{t_{i}}^{t_{i} + \\Delta} \\int_{t_{i}}^{t_{i} + \\Delta} \\varepsilon(t)\\varepsilon(s)\\,ds\\,dt\\right]\n$$\n再次，我们交换期望和积分算子：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\int_{t_{i}}^{t_{i} + \\Delta} \\int_{t_{i}}^{t_{i} + \\Delta} \\mathbb{E}[\\varepsilon(t)\\varepsilon(s)]\\,ds\\,dt\n$$\n现在，我们代入给定的协方差分布 $\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)$：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{1}{\\Delta^2} \\int_{t_{i}}^{t_{i} + \\Delta} \\int_{t_{i}}^{t_{i} + \\Delta} \\sigma^{2}\\delta(t-s)\\,ds\\,dt\n$$\n我们对内部关于 $s$ 的积分进行求值。常数 $\\sigma^2$ 可以被提出来。这一步的核心是应用狄拉克δ分布的筛选性质，该性质指出，对于函数 $f(x)$，如果积分区间包含 $c$，则 $\\int f(x)\\delta(x-c)\\,dx = f(c)$。在这里，积分变量是 $s$，函数是 $1$，δ分布的“尖峰”出现在 $s=t$ 处。\n$$\n\\int_{t_{i}}^{t_{i} + \\Delta} \\delta(t-s)\\,ds\n$$\n外部关于 $t$ 的积分范围从 $t_{i}$ 到 $t_{i} + \\Delta$。对于此范围内的任何 $t$ 值，点 $s=t$ 都位于内部积分的区间 $[t_{i}, t_{i} + \\Delta)$ 内。因此，筛选性质对外部积分域中的每个 $t$ 都适用，内部积分的值为 $1$：\n$$\n\\int_{t_{i}}^{t_{i} + \\Delta} \\delta(t-s)\\,ds = 1 \\quad \\text{for } t \\in [t_{i}, t_{i} + \\Delta)\n$$\n将此结果代回方差的表达式中：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{\\sigma^{2}}{\\Delta^2} \\int_{t_{i}}^{t_{i} + \\Delta} (1)\\,dt\n$$\n剩下的积分很容易计算：\n$$\n\\int_{t_{i}}^{t_{i} + \\Delta} 1\\,dt = [t]_{t_{i}}^{t_{i} + \\Delta} = (t_{i} + \\Delta) - t_{i} = \\Delta\n$$\n最后，我们将各部分组合起来，得到方差的精确表达式：\n$$\n\\mathrm{Var}(\\bar{y}_{i}) = \\frac{\\sigma^2}{\\Delta^2} \\cdot \\Delta = \\frac{\\sigma^2}{\\Delta}\n$$\n方差对平均时间 $\\Delta$ 的依赖性是一种反比关系，即 $\\mathrm{Var}(\\bar{y}_{i}) \\propto 1/\\Delta$。这直接源于白噪声过程的协方差结构。定义性质 $\\mathbb{E}[\\varepsilon(t)\\varepsilon(s)] = \\sigma^{2}\\delta(t-s)$ 意味着噪声过程在任意两个不同的时间点上是完全不相关的，无论它们有多接近。该过程没有“记忆”。\n\n因此，在时间间隔 $\\Delta$ 上对过程进行平均，类似于计算若干独立同分布随机抽样的样本均值。对于一组 $N$ 个独立的随机变量，其均值的方差随 $1/N$ 减小。在连续时间的情况下，平均窗口的持续时间 $\\Delta$ 扮演了类似于 $N$ 的角色。随着 $\\Delta$ 的增加，我们实际上是在对更多有效独立的噪声过程波动进行平均。这些波动是随机且不相关的，因此在更长的时间内更有效地相互抵消。这导致了平均值方差的减小。\n\n$\\delta$相关是相关时间为零的过程的数学理想化。正是这种特定的结构导致了 $1/\\Delta$ 的标度律。如果噪声在某个有限时间尺度上相关（即“有色”噪声过程），则时间平均信号的方差随 $\\Delta$ 的减小会更慢，因为在更长的窗口上进行平均不会按比例引入同样多的新的、独立的信息。计算过程从形式上证明了这一点：协方差在一个 $\\Delta \\times \\Delta$ 方形区域上的二重积分仅在 $t=s$ 的无穷细的对角线上非零。因此，积分中的总“功率”与该对角线的长度成线性比例，而对角线长度与 $\\Delta$ 成正比。当用平均定义中的 $\\Delta^2$ 因子进行归一化后，这种积分协方差的线性标度关系最终产生了 $1/\\Delta$ 的依赖性。", "answer": "$$\\boxed{\\frac{\\sigma^{2}}{\\Delta}}$$", "id": "3402401"}, {"introduction": "对于光子计数或事件发生率等计数数据，泊松分布是首选的测量误差模型。但其核心假设——方差等于均值——在实际应用中常常不成立。本练习介绍了一种关键的诊断技术，用于检验数据是否存在过度离散或欠离散，并演示了如何使用拟似然框架来构建一个更稳健的模型[@problem_id:3402406]。", "problem": "考虑在一数据同化设定下产生的独立计数观测值 $\\{Y_{i}\\}_{i=1}^{n}$，其中一个正向算子将参数向量 $\\theta \\in \\mathbb{R}^{p}$ 映射到预期的探测器强度，而 $Y_{i}$ 的均值由 $\\mu_{i}(\\theta)$ 给出。假设计数采用规范对数连接，因此对于已知的协变量 $a_{i} \\in \\mathbb{R}^{p}$，有 $\\mu_{i}(\\theta) = \\exp(a_{i}^{\\top}\\theta)$。其基本依据如下：对于一个泊松测量误差模型，$Y_{i} \\sim \\text{Poisson}(\\mu_{i})$ 独立地成立，且 $\\mathbb{E}[Y_{i}] = \\mu_{i}$ 和 $\\operatorname{Var}(Y_{i}) = \\mu_{i}$。\n\n1. 从泊松对数似然及其得分函数出发，使用大样本理论构建一个基于标准化残差的离散度检验。具体来说，推导皮尔逊卡方统计量\n$$\nX^{2} = \\sum_{i=1}^{n} \\frac{(Y_{i}-\\mu_{i})^{2}}{\\mu_{i}},\n$$\n该统计量在最大似然估计 (MLE) $\\hat{\\theta}$ 处进行评估，并证明在正确设定的泊松方差下，为何 $X^{2}$ 近似服从自由度为 $n-p$ 的卡方分布，其中 $p$ 是通过最大似然估计 (MLE) 估计的参数数量。解释 $X^{2}/(n-p)$ 与 $1$ 的偏差如何为相对于泊松模型的过度离散或低度离散提供证据。\n\n2. 当离散度不为 $1$ 时，假设一个拟似然框架，其方差模型为 $\\operatorname{Var}(Y_{i}\\,|\\,\\theta) = \\phi\\,\\mu_{i}(\\theta)$，其中 $\\phi > 0$ 为一个标量离散参数。仅使用均值-方差关系以及拟似然的定义属性——即拟得分与 $(Y_{i}-\\mu_{i})$ 成正比并由 $\\operatorname{Var}(Y_{i})$ 缩放——推导：\n   - 在对数连接下，当 $\\operatorname{Var}(Y_{i}) = \\phi\\,\\mu_{i}$ 时，$\\theta$ 的拟得分。\n   - 在相差一个可加常数的情况下，以 $\\phi$ 表示的 $\\mu_{i}$ 的拟似然。\n   - 以 $\\phi$ 和泊松均值模型下的期望费雪信息表示的 $\\hat{\\theta}$ 的渐近协方差调整。\n\n3. 现在，考虑 $n = 8$ 个计数和 $p = 3$ 个拟合参数。假设拟合均值（基于正向算子和 MLE $\\hat{\\theta}$）为\n$$\n\\mu = \\big(10.5,\\; 4.8,\\; 7.9,\\; 18.6,\\; 13.2,\\; 3.1,\\; 6.5,\\; 9.7\\big),\n$$\n观测到的计数为\n$$\ny = \\big(12,\\; 5,\\; 8,\\; 20,\\; 15,\\; 3,\\; 7,\\; 9\\big).\n$$\n计算基于皮尔逊的离散度估计量\n$$\n\\hat{\\phi}_{\\text{Pearson}} = \\frac{X^{2}}{n-p}.\n$$\n将您的答案四舍五入至四位有效数字。请以一个无单位的实数形式提供您的最终答案。", "solution": "该问题被认为是有效的，因为它在科学上基于统计理论（广义线性模型、拟似然），问题设定良好，有足够的信息得到唯一解，并且陈述客观、没有歧义。\n\n解答将分三部分呈现，对应题目陈述中的三个项目。\n\n第1部分：皮尔逊卡方统计量的推导与解释\n\n我们从一组 $n$ 个独立的计数观测值 $\\{Y_{i}\\}_{i=1}^{n}$ 开始。该模型假设 $Y_{i} \\sim \\text{Poisson}(\\mu_{i})$，其中均值 $\\mu_{i}$ 通过一个对数连接函数与参数向量 $\\theta \\in \\mathbb{R}^{p}$ 相关联，即 $\\mu_{i}(\\theta) = \\exp(a_{i}^{\\top}\\theta)$。单个观测值的概率质量函数为 $P(Y_i = y_i) = \\frac{\\mu_i^{y_i} \\exp(-\\mu_i)}{y_i!}$。整个数据集 $\\mathbf{y} = (y_1, \\dots, y_n)$ 的对数似然是各个对数似然之和：\n$$\nl(\\theta; \\mathbf{y}) = \\sum_{i=1}^{n} \\left( y_i \\ln(\\mu_i(\\theta)) - \\mu_i(\\theta) - \\ln(y_i!) \\right)\n$$\n得分函数 $\\mathbf{U}(\\theta)$ 是对数似然函数关于参数 $\\theta_j$ (对于 $j=1, \\dots, p$) 的一阶偏导数向量：\n$$\nU_j(\\theta) = \\frac{\\partial l}{\\partial \\theta_j} = \\sum_{i=1}^{n} \\frac{\\partial l_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\theta_j}\n$$\n我们有 $\\frac{\\partial l_i}{\\partial \\mu_i} = \\frac{y_i}{\\mu_i} - 1 = \\frac{y_i - \\mu_i}{\\mu_i}$。对于对数连接，$\\frac{\\partial \\mu_i}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j}\\exp(a_{i}^{\\top}\\theta) = \\exp(a_{i}^{\\top}\\theta) \\cdot a_{ij} = \\mu_i a_{ij}$。\n将这些代入得分方程中得到：\n$$\nU_j(\\theta) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\mu_i}{\\mu_i} \\right) (\\mu_i a_{ij}) = \\sum_{i=1}^{n} (y_i - \\mu_i) a_{ij}\n$$\n最大似然估计 (MLE) $\\hat{\\theta}$ 通过将得分函数设为零来找到，即 $\\mathbf{U}(\\hat{\\theta}) = \\mathbf{0}$。这产生了 $p$ 个方程：$\\sum_{i=1}^{n} (Y_i - \\hat{\\mu}_i) a_{ij} = 0$ (对于 $j=1, \\dots, p$)，其中 $\\hat{\\mu}_i = \\mu_i(\\hat{\\theta})$。\n\n观测值 $i$ 的皮尔逊残差定义为原始残差除以观测值的标准差：\n$$\nr_{P,i} = \\frac{Y_i - \\mathbb{E}[Y_i]}{\\sqrt{\\operatorname{Var}(Y_i)}} = \\frac{Y_i - \\mu_i}{\\sqrt{\\mu_i}}\n$$\n皮尔逊卡方统计量 $X^2$ 是皮尔逊残差的平方和，在 MLE $\\hat{\\mu}_i$ 处进行评估：\n$$\nX^2 = \\sum_{i=1}^{n} \\frac{(Y_i - \\hat{\\mu}_i)^2}{\\hat{\\mu}_i}\n$$\n对于大样本量 $n$，如果泊松模型设定正确，每一项 $(Y_i - \\mu_i)/\\sqrt{\\mu_i}$ 都近似为一个标准正态随机变量。$n$ 个独立标准正态变量的平方和服从自由度为 $n$ 的卡方分布，即 $\\chi^2_n$。然而，均值 $\\mu_i$ 是未知的，被它们的估计值 $\\hat{\\mu}_i$ 所取代。这些估计值依赖于同一个 $p$ 维参数向量 $\\hat{\\theta}$，该向量是通过满足 $p$ 个线性约束 $\\sum_{i=1}^{n} (Y_i - \\hat{\\mu}_i) a_{ij} = 0$ 得到的。对残差的这些约束减少了系统的有效自由度。根据渐近统计学的一个一般定理（与 Wilks 定理和拟合优度检验相关），估计 $p$ 个参数会使卡方统计量的自由度减少 $p$。因此，在泊松模型正确的原假设下，$X^2$ 近似服从自由度为 $n-p$ 的卡方随机变量分布：\n$$\nX^2 \\sim \\chi^2_{n-p} \\quad (\\text{近似地})\n$$\n一个 $\\chi^2_k$ 随机变量的期望值为 $k$。因此，如果模型正确，我们期望 $X^2$ 接近 $n-p$。这导出了离散参数估计量的定义：$\\hat{\\phi} = \\frac{X^2}{n-p}$。我们期望 $\\hat{\\phi} \\approx 1$。\n- 如果 $\\hat{\\phi} \\gg 1$，这意味着 $X^2 \\gg n-p$。这表明由平方残差捕捉到的观测变异性远大于泊松模型（其中方差等于均值）所预测的变异性。这是**过度离散**的证据。\n- 如果 $\\hat{\\phi} \\ll 1$，这意味着 $X^2 \\ll n-p$。这表明观测到的变异性远小于泊松模型预测的变异性。这是**低度离散**的证据。\n- 如果 $\\hat{\\phi} \\approx 1$，数据与泊松方差假设一致。\n\n第2部分：拟似然框架\n\n我们现在假设一个更一般的方差结构 $\\operatorname{Var}(Y_i) = \\phi \\mu_i(\\theta)$，其中 $\\phi$ 是一个常数离散参数。拟似然方法不要求完整的分布假设，只依赖于均值和方差结构。\n\n- **$\\theta$ 的拟得分**：$\\theta_j$ 的拟得分函数定义为对所有观测值的求和，其中每一项与 $(Y_i-\\mu_i)$ 成正比，并由 $\\operatorname{Var}(Y_i)$ 缩放：\n$$\nU_{Q,j}(\\theta) = \\sum_{i=1}^n \\frac{Y_i - \\mu_i(\\theta)}{\\operatorname{Var}(Y_i)} \\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta_j}\n$$\n代入 $\\operatorname{Var}(Y_i) = \\phi \\mu_i(\\theta)$ 和 $\\frac{\\partial \\mu_i}{\\partial \\theta_j} = \\mu_i a_{ij}$（来自对数连接），我们得到：\n$$\nU_{Q,j}(\\theta) = \\sum_{i=1}^n \\frac{Y_i - \\mu_i(\\theta)}{\\phi \\mu_i(\\theta)} (\\mu_i(\\theta) a_{ij}) = \\frac{1}{\\phi} \\sum_{i=1}^n (Y_i - \\mu_i(\\theta)) a_{ij}\n$$\n这是 $\\theta_j$ 的拟得分。完整的拟得分向量是 $\\mathbf{U}_Q(\\theta) = \\frac{1}{\\phi} \\mathbf{A}^\\top(\\mathbf{Y}-\\boldsymbol{\\mu})$，其中 $\\mathbf{A}$ 是由 $a_i^\\top$ 构成的 $n \\times p$ 协变量矩阵。\n\n- **$\\mu_i$ 的拟似然**：拟似然 $Q(\\mu_i; y_i)$ 是一个函数，其关于 $\\mu_i$ 的导数产生该观测值的得分样项：\n$$\n\\frac{\\partial Q(\\mu_i; y_i)}{\\partial \\mu_i} = \\frac{y_i - \\mu_i}{\\operatorname{Var}(Y_i)} = \\frac{y_i - \\mu_i}{\\phi \\mu_i}\n$$\n对 $\\mu_i$ 积分得到拟似然函数（在相差一个可加常数的情况下）：\n$$\nQ(\\mu_i; y_i) = \\int \\frac{y_i - t}{\\phi t} dt = \\frac{1}{\\phi} \\int \\left(\\frac{y_i}{t} - 1\\right) dt = \\frac{1}{\\phi} (y_i \\ln(\\mu_i) - \\mu_i) + \\text{常数}\n$$\n注意，这恰好是单个观测值的泊松对数似然，按 $1/\\phi$ 进行了缩放。\n\n- **$\\hat{\\theta}$ 的渐近协方差调整**：估计值 $\\hat{\\theta}$ 是通过求解 $\\mathbf{U}_Q(\\hat{\\theta})=\\mathbf{0}$ 得到的。此类 M-估计量的渐近协方差由三明治公式 $\\operatorname{Cov}(\\hat{\\theta}) \\approx \\mathcal{J}^{-1} \\mathcal{K} (\\mathcal{J}^{-1})^\\top$ 给出，其中 $\\mathcal{J} = -\\mathbb{E}\\left[\\frac{\\partial \\mathbf{U}_Q}{\\partial \\theta^\\top}\\right]$ 且 $\\mathcal{K} = \\mathbb{E}[\\mathbf{U}_Q \\mathbf{U}_Q^\\top]$。\n首先，我们计算 $\\mathcal{J}$：\n$$\n\\frac{\\partial \\mathbf{U}_Q}{\\partial \\theta^\\top} = \\frac{1}{\\phi} \\frac{\\partial}{\\partial \\theta^\\top} \\left(\\sum_{i=1}^n(Y_i - \\mu_i) \\mathbf{a}_i\\right) = -\\frac{1}{\\phi} \\sum_{i=1}^n \\mathbf{a}_i \\frac{\\partial \\mu_i}{\\partial \\theta^\\top} = -\\frac{1}{\\phi} \\sum_{i=1}^n \\mathbf{a}_i (\\mu_i \\mathbf{a}_i^\\top) = -\\frac{1}{\\phi} \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}\n$$\n其中 $\\mathbf{D} = \\operatorname{diag}(\\mu_1, \\dots, \\mu_n)$。所以，$\\mathcal{J} = \\frac{1}{\\phi} \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}$。\n接下来，我们计算 $\\mathcal{K}$：\n$$\n\\mathcal{K} = \\mathbb{E}[\\mathbf{U}_Q \\mathbf{U}_Q^\\top] = \\mathbb{E}\\left[ \\left(\\frac{1}{\\phi} \\mathbf{A}^\\top(\\mathbf{Y}-\\boldsymbol{\\mu})\\right) \\left(\\frac{1}{\\phi} \\mathbf{A}^\\top(\\mathbf{Y}-\\boldsymbol{\\mu})\\right)^\\top \\right] = \\frac{1}{\\phi^2} \\mathbf{A}^\\top \\mathbb{E}[(\\mathbf{Y}-\\boldsymbol{\\mu})(\\mathbf{Y}-\\boldsymbol{\\mu})^\\top] \\mathbf{A}\n$$\n矩阵 $\\mathbb{E}[(\\mathbf{Y}-\\boldsymbol{\\mu})(\\mathbf{Y}-\\boldsymbol{\\mu})^\\top]$ 是 $\\mathbf{Y}$ 的协方差矩阵，即 $\\operatorname{Cov}(\\mathbf{Y}) = \\operatorname{diag}(\\operatorname{Var}(Y_i)) = \\operatorname{diag}(\\phi \\mu_i) = \\phi\\mathbf{D}$。\n$$\n\\mathcal{K} = \\frac{1}{\\phi^2} \\mathbf{A}^\\top (\\phi\\mathbf{D}) \\mathbf{A} = \\frac{1}{\\phi} \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}\n$$\n泊松信息矩阵是 $I_P(\\theta) = \\mathbf{A}^\\top \\mathbf{D} \\mathbf{A}$。因此我们有 $\\mathcal{J} = \\frac{1}{\\phi}I_P(\\theta)$ 和 $\\mathcal{K} = \\frac{1}{\\phi}I_P(\\theta)$。\n渐近协方差为：\n$$\n\\operatorname{Cov}(\\hat{\\theta}) \\approx \\left(\\frac{1}{\\phi}I_P(\\theta)\\right)^{-1} \\left(\\frac{1}{\\phi}I_P(\\theta)\\right) \\left(\\frac{1}{\\phi}I_P(\\theta)\\right)^{-1} = \\left(\\phi I_P(\\theta)^{-1}\\right) \\left(\\frac{1}{\\phi}I_P(\\theta)\\right) \\left(\\phi I_P(\\theta)^{-1}\\right) = \\phi I_P(\\theta)^{-1}\n$$\n调整即为，来自泊松模型的朴素渐近协方差 $I_P(\\theta)^{-1}$，被离散参数 $\\phi$ 缩放。\n\n第3部分：数值计算\n\n给定 $n=8$ 个计数，$p=3$ 个参数，拟合均值 $\\boldsymbol{\\mu}$，以及观测计数 $\\mathbf{y}$。我们被要求计算基于皮尔逊的离散度估计量，$\\hat{\\phi}_{\\text{Pearson}} = \\frac{X^2}{n-p}$。\n拟合均值为 $\\boldsymbol{\\mu} = (10.5, 4.8, 7.9, 18.6, 13.2, 3.1, 6.5, 9.7)$。\n观测计数为 $\\mathbf{y} = (12, 5, 8, 20, 15, 3, 7, 9)$。\n自由度为 $n-p = 8-3=5$。\n\n首先，我们计算皮尔逊卡方统计量 $X^2 = \\sum_{i=1}^n \\frac{(y_i - \\mu_i)^2}{\\mu_i}$。\n\\begin{align*}\nX^2 = \\frac{(12-10.5)^2}{10.5} + \\frac{(5-4.8)^2}{4.8} + \\frac{(8-7.9)^2}{7.9} + \\frac{(20-18.6)^2}{18.6} \\\\\n\\quad + \\frac{(15-13.2)^2}{13.2} + \\frac{(3-3.1)^2}{3.1} + \\frac{(7-6.5)^2}{6.5} + \\frac{(9-9.7)^2}{9.7} \\\\\n= \\frac{1.5^2}{10.5} + \\frac{0.2^2}{4.8} + \\frac{0.1^2}{7.9} + \\frac{1.4^2}{18.6} + \\frac{1.8^2}{13.2} + \\frac{(-0.1)^2}{3.1} + \\frac{0.5^2}{6.5} + \\frac{(-0.7)^2}{9.7} \\\\\n= \\frac{2.25}{10.5} + \\frac{0.04}{4.8} + \\frac{0.01}{7.9} + \\frac{1.96}{18.6} + \\frac{3.24}{13.2} + \\frac{0.01}{3.1} + \\frac{0.25}{6.5} + \\frac{0.49}{9.7} \\\\\n\\approx 0.21428571 + 0.00833333 + 0.00126582 + 0.10537634 + 0.24545455 + 0.00322581 + 0.03846154 + 0.05051546 \\\\\n\\approx 0.66691856\n\\end{align*}\n现在，我们计算离散度估计量：\n$$\n\\hat{\\phi}_{\\text{Pearson}} = \\frac{X^2}{n-p} = \\frac{0.66691856}{5} \\approx 0.13338371\n$$\n四舍五入到四位有效数字，我们得到 $0.1334$。", "answer": "$$\\boxed{0.1334}$$", "id": "3402406"}, {"introduction": "在建模时，我们有时会为了方便而使用一个不完全正确的模型，例如用高斯分布来近似泊松数据。这样做会带来什么后果？本练习通过推导和比较正确指定模型与误设模型下的估计量方差，量化了这种模型选择的“代价”，并介绍了在模型误设情况下计算真实不确定性的重要工具——Godambe 信息矩阵（或称“三明治”估计量）[@problem_id:3402423]。", "problem": "考虑一个单参数线性逆问题，其中，未知标量参数 $\\theta_{0} \\in (0,\\infty)$ 是通过一个已知的非负设计序列 $\\{a_{i}\\}_{i=1}^{n}$（其中对所有 $i$ 都有 $a_{i} > 0$）来观测的。真实的数据生成机制产生独立的计数 $Y_{i}$，其中 $Y_{i} \\sim \\text{Poisson}(a_{i}\\,\\theta_{0})$。然而，一个数据同化算法错误地假设了一个高斯白噪声模型，其均值为 $a_{i}\\,\\theta$，方差为已知的常数 $\\tau^{2} \\in (0,\\infty)$，即 $Y_{i} \\stackrel{\\text{model}}{\\sim} \\mathcal{N}(a_{i}\\,\\theta,\\tau^{2})$，并使用该高斯模型下的最大似然估计量来估计 $\\theta$。\n\n从得分函数、其协方差以及对数似然函数的负海森矩阵的期望这些核心定义出发，推导在此模型误设下 $\\theta$ 的 Godambe（三明治）信息，并求出高斯伪最大似然估计量的相应大样本渐近方差。然后，从正确指定的泊松模型的费雪信息出发，求出泊松最大似然估计量的大样本渐近方差。最后，通过给出误设模型的渐近方差与正确指定模型的渐近方差之比来量化方差膨胀。\n\n将您的最终答案表示为仅包含 $\\{a_{i}\\}_{i=1}^{n}$ 的单个闭式解析表达式。不需要进行数值计算，也无需四舍五入。最终答案必须是单个表达式，而不是不等式或方程。[@problem_id:101]", "solution": "用户希望在一个简单的单参数估计问题中分析模型误设的影响。\n\n### 步骤 1：提取已知条件\n-   **真实数据生成模型**：观测值 $Y_i$（其中 $i=1, \\dots, n$）是独立的随机变量，其中 $Y_i \\sim \\text{Poisson}(a_i \\theta_0)$。\n-   **真实参数**：$\\theta_0 \\in (0, \\infty)$ 是未知的标量参数。\n-   **设计序列**：$\\{a_i\\}_{i=1}^n$ 是一个已知的正常数序列，$a_i > 0$。\n-   **假设的（误设的）模型**：数据同化算法假设 $Y_i \\stackrel{\\text{model}}{\\sim} \\mathcal{N}(a_i \\theta, \\tau^2)$，即一个均值为 $a_i \\theta$、方差为已知常数 $\\tau^2 \\in (0, \\infty)$ 的高斯模型。\n-   **误设下的估计量**：从假设的高斯模型推导出一个伪最大似然估计量（pseudo-MLE）。\n-   **正确指定下的估计量**：从真实的泊松模型推导出最大似然估计量（MLE）。\n-   **目标 1**：使用 Godambe（三明治）信息形式论推导高斯伪最大似然估计量的大样本渐近方差。\n-   **目标 2**：使用费雪信息推导泊松最大似然估计量的大样本渐近方差。\n-   **目标 3**：计算误设模型方差与正确指定模型方差之比，并仅用 $\\{a_i\\}_{i=1}^n$ 表示。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题在科学上和数学上都是合理的。这是统计理论中的一个标准练习，具体涉及 M-估计量和模型误设的后果。泊松分布、高斯分布、最大似然估计、伪最大似然、费雪信息和 Godambe（三明治）信息等概念都是统计学中公认的支柱。该问题是适定的，提供了推导所需量所需的所有信息。语言精确客观。设定是自洽的，没有矛盾或不科学的前提。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个完整的、有理有据的解答。\n\n### 解答推导\n\n解答过程分为三个部分。首先，我们分析来自误设的高斯模型的伪最大似然估计量。其次，我们分析来自正确指定的泊松模型的最大似然估计量。第三，我们计算它们的渐近方差之比。“大样本渐近方差”一词将被解释为估计量的精确有限样本方差，因为对于这两个估计量，该量都是易于处理的，并且其比率等价于由中心极限定理得出的渐近方差之比。\n\n**第 1 部分：高斯伪最大似然估计量的渐近方差**\n\n假设的模型是 $Y_i \\sim \\mathcal{N}(a_i\\theta, \\tau^2)$。在此误设模型下的对数似然函数 $\\ell_n(\\theta; \\mathbf{Y})$ 为：\n$$ \\ell_n(\\theta; \\mathbf{Y}) = \\sum_{i=1}^{n} \\log\\left( \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(Y_i - a_i\\theta)^2}{2\\tau^2} \\right) \\right) $$\n$$ \\ell_n(\\theta; \\mathbf{Y}) = -\\frac{n}{2}\\ln(2\\pi\\tau^2) - \\frac{1}{2\\tau^2} \\sum_{i=1}^{n} (Y_i - a_i\\theta)^2 $$\n得分函数 $S_n(\\theta)$ 是 $\\ell_n(\\theta; \\mathbf{Y})$ 关于 $\\theta$ 的导数：\n$$ S_n(\\theta) = \\frac{\\partial \\ell_n}{\\partial \\theta} = -\\frac{1}{2\\tau^2} \\sum_{i=1}^{n} 2(Y_i - a_i\\theta)(-a_i) = \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(Y_i - a_i\\theta) $$\n伪最大似然估计量（记为 $\\hat{\\theta}_G$）可通过令 $S_n(\\hat{\\theta}_G) = 0$ 求得：\n$$ \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(Y_i - a_i\\hat{\\theta}_G) = 0 \\implies \\sum_{i=1}^{n} a_i Y_i - \\hat{\\theta}_G \\sum_{i=1}^{n} a_i^2 = 0 $$\n$$ \\hat{\\theta}_G = \\frac{\\sum_{i=1}^{n} a_i Y_i}{\\sum_{i=1}^{n} a_i^2} $$\n伪最大似然估计量的渐近方差由“三明治”公式 $J_n(\\theta_0)^{-1} I_n(\\theta_0) J_n(\\theta_0)^{-1}$ 给出，其中 $J_n(\\theta_0) = -E_{\\theta_0}\\left[\\frac{\\partial^2 \\ell_n}{\\partial \\theta^2}\\right]_{\\theta=\\theta_0}$ 且 $I_n(\\theta_0) = \\text{Var}_{\\theta_0}[S_n(\\theta_0)]$。期望和方差是在真实数据生成过程下计算的，即 $Y_i \\sim \\text{Poisson}(a_i\\theta_0)$。\n\n首先，我们求 $J_n(\\theta_0)$：\n$$ \\frac{\\partial^2 \\ell_n}{\\partial \\theta^2} = \\frac{\\partial S_n(\\theta)}{\\partial \\theta} = \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(-a_i) = -\\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i^2 $$\n由于该表达式不依赖于数据 $Y_i$ 或 $\\theta$，其期望是平凡的：\n$$ J_n(\\theta_0) = - \\left( -\\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i^2 \\right) = \\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i^2 $$\n接下来，我们求 $I_n(\\theta_0)$，即在真实参数 $\\theta_0$ 处求值的得分函数的方差：\n$$ I_n(\\theta_0) = \\text{Var}_{\\theta_0}[S_n(\\theta_0)] = \\text{Var}_{\\theta_0}\\left[\\frac{1}{\\tau^2} \\sum_{i=1}^{n} a_i(Y_i - a_i\\theta_0)\\right] $$\n由于 $Y_i$ 是独立的，和的方差等于方差的和：\n$$ I_n(\\theta_0) = \\frac{1}{(\\tau^2)^2} \\sum_{i=1}^{n} \\text{Var}_{\\theta_0}[a_i Y_i] = \\frac{1}{\\tau^4} \\sum_{i=1}^{n} a_i^2 \\text{Var}_{\\theta_0}[Y_i] $$\n在真实的泊松模型下，$\\text{Var}_{\\theta_0}[Y_i] = a_i\\theta_0$。\n$$ I_n(\\theta_0) = \\frac{1}{\\tau^4} \\sum_{i=1}^{n} a_i^2 (a_i\\theta_0) = \\frac{\\theta_0}{\\tau^4} \\sum_{i=1}^{n} a_i^3 $$\n$\\hat{\\theta}_G$ 的大样本渐近方差（记为 $V_G(\\hat{\\theta}_G)$）是：\n$$ V_G(\\hat{\\theta}_G) = J_n(\\theta_0)^{-1} I_n(\\theta_0) J_n(\\theta_0)^{-1} = \\left(\\frac{1}{\\tau^2}\\sum_{i=1}^{n} a_i^2\\right)^{-1} \\left(\\frac{\\theta_0}{\\tau^4}\\sum_{i=1}^{n} a_i^3\\right) \\left(\\frac{1}{\\tau^2}\\sum_{i=1}^{n} a_i^2\\right)^{-1} $$\n$$ V_G(\\hat{\\theta}_G) = \\frac{\\tau^2}{\\sum_{i=1}^{n} a_i^2} \\frac{\\theta_0 \\sum_{i=1}^{n} a_i^3}{\\tau^4} \\frac{\\tau^2}{\\sum_{i=1}^{n} a_i^2} = \\frac{\\theta_0 \\sum_{i=1}^{n} a_i^3}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2} $$\n对于这个线性估计量，这个渐近方差与精确的有限样本方差相同，这可以通过直接计算来验证：$\\text{Var}_{\\theta_0}(\\hat{\\theta}_G) = \\text{Var}_{\\theta_0}\\left(\\frac{\\sum_i a_i Y_i}{\\sum_i a_i^2}\\right) = \\frac{\\sum_i a_i^2 \\text{Var}_{\\theta_0}(Y_i)}{(\\sum_i a_i^2)^2} = \\frac{\\sum_i a_i^2 (a_i\\theta_0)}{(\\sum_i a_i^2)^2} = \\frac{\\theta_0 \\sum_i a_i^3}{(\\sum_i a_i^2)^2}$。\n\n**第 2 部分：泊松最大似然估计量的渐近方差**\n\n真实模型是 $Y_i \\sim \\text{Poisson}(a_i\\theta)$。对数似然函数 $L_n(\\theta; \\mathbf{Y})$ 是：\n$$ L_n(\\theta; \\mathbf{Y}) = \\sum_{i=1}^{n} \\log\\left( \\frac{(a_i\\theta)^{Y_i} e^{-a_i\\theta}}{Y_i!} \\right) = \\sum_{i=1}^{n} (Y_i\\log(a_i) + Y_i\\log(\\theta) - a_i\\theta - \\log(Y_i!)) $$\n得分函数是：\n$$ \\frac{\\partial L_n}{\\partial \\theta} = \\sum_{i=1}^{n} \\left(\\frac{Y_i}{\\theta} - a_i\\right) = \\frac{1}{\\theta}\\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} a_i $$\n最大似然估计量（记为 $\\hat{\\theta}_P$）可通过将得分函数设为零求得：\n$$ \\frac{1}{\\hat{\\theta}_P}\\sum_{i=1}^{n} Y_i - \\sum_{i=1}^{n} a_i = 0 \\implies \\hat{\\theta}_P = \\frac{\\sum_{i=1}^{n} Y_i}{\\sum_{i=1}^{n} a_i} $$\n最大似然估计量的渐近方差由费雪信息的逆 $I_P(\\theta_0)^{-1}$ 给出。费雪信息是 $I_P(\\theta_0) = -E_{\\theta_0}\\left[\\frac{\\partial^2 L_n}{\\partial \\theta^2}\\right]_{\\theta=\\theta_0}$。\n对数似然的二阶导数是：\n$$ \\frac{\\partial^2 L_n}{\\partial \\theta^2} = -\\frac{1}{\\theta^2}\\sum_{i=1}^{n} Y_i $$\n在真实模型下取期望（其中 $E_{\\theta_0}[Y_i] = a_i\\theta_0$）：\n$$ E_{\\theta_0}\\left[\\frac{\\partial^2 L_n}{\\partial \\theta^2}\\right]_{\\theta=\\theta_0} = E_{\\theta_0}\\left[-\\frac{1}{\\theta_0^2}\\sum_{i=1}^{n} Y_i\\right] = -\\frac{1}{\\theta_0^2}\\sum_{i=1}^{n} E_{\\theta_0}[Y_i] = -\\frac{1}{\\theta_0^2}\\sum_{i=1}^{n} a_i\\theta_0 = -\\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i $$\n费雪信息是：\n$$ I_P(\\theta_0) = - \\left(-\\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i\\right) = \\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i $$\n$\\hat{\\theta}_P$ 的大样本渐近方差（记为 $V_P(\\hat{\\theta}_P)$）是费雪信息的逆：\n$$ V_P(\\hat{\\theta}_P) = [I_P(\\theta_0)]^{-1} = \\left(\\frac{1}{\\theta_0}\\sum_{i=1}^{n} a_i\\right)^{-1} = \\frac{\\theta_0}{\\sum_{i=1}^{n} a_i} $$\n和之前一样，这与 $\\hat{\\theta}_P$ 的精确有限样本方差相同。\n\n**第 3 部分：方差之比（方差膨胀）**\n\n最后一步是计算误设模型方差与正确指定模型方差之比。该比率量化了因使用不正确的高斯模型而导致的效率损失。\n$$ \\text{比率} = \\frac{V_G(\\hat{\\theta}_G)}{V_P(\\hat{\\theta}_P)} = \\frac{\\frac{\\theta_0 \\sum_{i=1}^{n} a_i^3}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2}}{\\frac{\\theta_0}{\\sum_{i=1}^{n} a_i}} $$\n真实参数 $\\theta_0$ 被消掉了：\n$$ \\text{比率} = \\frac{\\sum_{i=1}^{n} a_i^3}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2} \\cdot \\left(\\sum_{i=1}^{n} a_i\\right) = \\frac{\\left(\\sum_{i=1}^{n} a_i\\right) \\left(\\sum_{i=1}^{n} a_i^3\\right)}{\\left(\\sum_{i=1}^{n} a_i^2\\right)^2} $$\n该表达式仅依赖于设计序列 $\\{a_i\\}$，符合要求。根据应用于向量 $(\\sqrt{a_1}, ..., \\sqrt{a_n})$ 和 $(\\sqrt{a_1^3}, ..., \\sqrt{a_n^3})$ 的柯西-施瓦茨不等式，我们有 $\\left(\\sum_{i=1}^n a_i^2\\right)^2 \\le \\left(\\sum_{i=1}^n a_i\\right)\\left(\\sum_{i=1}^n a_i^3\\right)$，这证实了该比率总是大于或等于 1，表示方差的膨胀。", "answer": "$$\n\\boxed{\\frac{\\left(\\sum_{i=1}^{n} a_{i}\\right) \\left(\\sum_{i=1}^{n} a_{i}^{3}\\right)}{\\left(\\sum_{i=1}^{n} a_{i}^{2}\\right)^{2}}}\n$$", "id": "3402423"}]}