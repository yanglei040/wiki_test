{"hands_on_practices": [{"introduction": "贝叶斯推断的输出是完整的后验分布，但在实际应用中，我们常常需要一个单一的点估计来总结推断结果。选择哪种估计量——例如后验最大值（MAP）或后验均值——并非随意的决定。这个练习探讨了最常见的贝叶斯估计量背后的理论基础，将它们与不同的损失函数联系起来，并分析它们在变量替换下的不变性，从而帮助你更明智地选择和解释你的结果 [@problem_id:3367437]。", "problem": "考虑一个贝叶斯逆问题，其中未知参数向量 $x \\in \\mathbb{R}^{n}$ 通过一个已知的正向映射 $G:\\mathbb{R}^{n} \\to \\mathbb{R}^{m}$ 和加性噪声从数据 $y \\in \\mathbb{R}^{m}$ 中推断出来，由似然 $p(y \\mid x)$ 和先验 $\\pi_{\\mathrm{prior}}(x)$ 表示。后验密度为 $\\pi(x \\mid y) \\propto p(y \\mid x)\\,\\pi_{\\mathrm{prior}}(x)$，除非另有说明，否则假定为 $\\mathbb{R}^{n}$ 上的一个概率密度。在损失 $L:\\mathbb{R}^{n} \\times \\mathbb{R}^{n} \\to [0,\\infty)$ 下，贝叶斯估计量是任何使后验期望损失 $\\int_{\\mathbb{R}^{n}} L(x,a)\\,\\pi(x \\mid y)\\,\\mathrm{d}x$ 最小化的可测决策规则 $a(y) \\in \\mathbb{R}^{n}$。最大后验（MAP）估计量是 $\\hat{x}_{\\mathrm{MAP}}(y) := \\arg\\max_{x \\in \\mathbb{R}^{n}} \\pi(x \\mid y)$，后验均值是 $\\mathbb{E}[x \\mid y] := \\int_{\\mathbb{R}^{n}} x\\,\\pi(x \\mid y)\\,\\mathrm{d}x$（当一阶矩存在时）。对于一个光滑的双射重参数化 $z = g(x)$，其具有光滑逆 $g^{-1}$ 和雅可比矩阵 $Dg^{-1}(z)$，在 $z$ 上的变换后验密度为 $\\pi_{Z}(z \\mid y) = \\pi_{X}(g^{-1}(z) \\mid y)\\,\\big|\\det Dg^{-1}(z)\\big|$。\n\n选择所有关于 $\\hat{x}_{\\mathrm{MAP}}$ 和 $\\mathbb{E}[x \\mid y]$ 在重参数化下的不变性、它们的存在性以及它们在不同损失函数下的贝叶斯最优性的正确陈述：\n\nA. 在平方误差损失 $L(x,a) = \\|x - a\\|^{2}$ 下，贝叶斯估计量是后验均值 $\\mathbb{E}[x \\mid y]$。\n\nB. 在连续参数空间中的0-1损失 $L(x,a) = \\mathbf{1}\\{x \\neq a\\}$ 下，贝叶斯估计量是 $\\hat{x}_{\\mathrm{MAP}}$。\n\nC. 在一般的光滑双射重参数化 $z = g(x)$ 下，$\\hat{x}_{\\mathrm{MAP}}$ 和 $\\mathbb{E}[x \\mid y]$ 都不是不变的；通常 $g\\big(\\hat{x}_{\\mathrm{MAP}}\\big) \\neq \\hat{z}_{\\mathrm{MAP}}$ 且 $g\\big(\\mathbb{E}[x \\mid y]\\big) \\neq \\mathbb{E}[z \\mid y]$，不变性的特殊情况仅适用于某些仿射或线性 $g$。\n\nD. 在线性高斯逆问题（高斯先验和高斯似然）中，后验分布是高斯的，并满足 $\\hat{x}_{\\mathrm{MAP}} = \\mathbb{E}[x \\mid y]$。\n\nE. 如果后验分布是多峰的，则后验均值未定义。\n\nF. 如果后验密度在 $\\mathbb{R}^{n}$ 上是正常的，那么总存在一个唯一的 $\\hat{x}_{\\mathrm{MAP}}$。\n\nG. 在有限参数空间（例如，一个有限的模型集合）中，在0-1损失 $L(x,a) = \\mathbf{1}\\{x \\neq a\\}$ 下，MAP 规则是贝叶斯最优的。", "solution": "问题陈述是贝叶斯决策理论应用于逆问题的一个有效练习。它为后验密度、贝叶斯估计量、最大后验（MAP）估计量、后验均值以及概率密度的变量替换公式提供了标准定义。所有术语都是标准的、自洽的，并以概率论和统计学为基础。该问题要求基于这些定义评估几个陈述，这是一个定义明确且客观的任务。未发现任何瑕疵。\n\n接下来分析每个陈述：\n\n**A. 在平方误差损失 $L(x,a) = \\|x - a\\|^{2}$ 下，贝叶斯估计量是后验均值 $\\mathbb{E}[x \\mid y]$。**\n\n为求贝叶斯估计量，我们必须找到使后验期望损失最小化的行动 $a \\in \\mathbb{R}^{n}$。后验期望损失 $R(a)$ 由下式给出：\n$$\nR(a) = \\int_{\\mathbb{R}^{n}} L(x,a)\\,\\pi(x \\mid y)\\,\\mathrm{d}x = \\int_{\\mathbb{R}^{n}} \\|x - a\\|^{2}\\,\\pi(x \\mid y)\\,\\mathrm{d}x\n$$\n我们关于 $a$ 最小化此函数。假设积分及其导数存在，我们可以通过将关于 $a$ 的梯度设为零来找到最小值。\n$$\n\\nabla_a R(a) = \\nabla_a \\int_{\\mathbb{R}^{n}} (x - a)^T (x - a)\\,\\pi(x \\mid y)\\,\\mathrm{d}x\n$$\n在允许交换微分和积分的正则性条件下，我们有：\n$$\n\\nabla_a R(a) = \\int_{\\mathbb{R}^{n}} \\nabla_a \\left( x^T x - 2a^T x + a^T a \\right) \\,\\pi(x \\mid y)\\,\\mathrm{d}x\n$$\n积分内项关于 $a$ 的梯度是 $-2x + 2a$。\n$$\n\\nabla_a R(a) = \\int_{\\mathbb{R}^{n}} (2a - 2x) \\,\\pi(x \\mid y)\\,\\mathrm{d}x = 2a \\int_{\\mathbb{R}^{n}} \\pi(x \\mid y)\\,\\mathrm{d}x - 2 \\int_{\\mathbb{R}^{n}} x \\,\\pi(x \\mid y)\\,\\mathrm{d}x\n$$\n由于 $\\pi(x \\mid y)$ 是一个概率密度，其在 $\\mathbb{R}^{n}$ 上的积分为 $1$。第二个积分是后验均值 $\\mathbb{E}[x \\mid y]$ 的定义。\n$$\n\\nabla_a R(a) = 2a - 2\\mathbb{E}[x \\mid y]\n$$\n将梯度设为零得到 $2a - 2\\mathbb{E}[x \\mid y] = 0$，这意味着 $a = \\mathbb{E}[x \\mid y]$。$R(a)$ 关于 $a$ 的Hessian矩阵是 $2I$，其中 $I$ 是单位矩阵。由于这是正定的，该解对应于一个唯一的最小值。\n\n因此，在平方误差损失下的贝叶斯估计量是后验均值。该陈述是 **正确** 的。\n\n**B. 在连续参数空间中的0-1损失 $L(x,a) = \\mathbf{1}\\{x \\neq a\\}$ 下，贝叶斯估计量是 $\\hat{x}_{\\mathrm{MAP}}$。**\n\n对于给定的决策 $a$，后验期望损失是：\n$$\nR(a) = \\int_{\\mathbb{R}^{n}} \\mathbf{1}\\{x \\neq a\\}\\,\\pi(x \\mid y)\\,\\mathrm{d}x = \\int_{\\mathbb{R}^{n}} (1 - \\mathbf{1}\\{x = a\\})\\,\\pi(x \\mid y)\\,\\mathrm{d}x\n$$\n$$\nR(a) = \\int_{\\mathbb{R}^{n}} \\pi(x \\mid y)\\,\\mathrm{d}x - \\int_{\\mathbb{R}^{n}} \\mathbf{1}\\{x = a\\}\\,\\pi(x \\mid y)\\,\\mathrm{d}x\n$$\n第一项是 $1$。对于具有概率密度函数 $\\pi(x \\mid y)$ 的连续随机变量 $x$， $x$ 取任何单个值 $a$ 的概率为零。这对应于在一个勒贝格测度为零的集合上对密度进行积分，结果为零。\n$$\n\\int_{\\mathbb{R}^{n}} \\mathbf{1}\\{x = a\\}\\,\\pi(x \\mid y)\\,\\mathrm{d}x = P(x=a \\mid y) = 0\n$$\n因此，$R(a) = 1 - 0 = 1$ 对于任何 $a \\in \\mathbb{R}^{n}$ 的选择都是如此。后验期望损失是恒定的。任何决策 $a$ 都产生相同的最小（也是最大）损失 $1$。因此，没有唯一的最小化子，并且寻找“那个”贝叶斯估计量的问题是不适定的。贝叶斯估计量是 MAP 的陈述不严格正确。尽管 MAP 估计量可以形式上推导为在“局部”0-1损失 $L(x,a) = 1 - \\mathbf{1}\\{\\|x-a\\|  \\epsilon\\}$ 下，当 $\\epsilon \\to 0$ 时的贝叶斯估计量的极限，但在连续空间中，如所写的严格0-1损失下，该陈述是错误的。\n\n因此，该陈述是 **错误** 的。\n\n**C. 在一般的光滑双射重参数化 $z = g(x)$ 下，$\\hat{x}_{\\mathrm{MAP}}$ 和 $\\mathbb{E}[x \\mid y]$ 都不是不变的；通常 $g\\big(\\hat{x}_{\\mathrm{MAP}}\\big) \\neq \\hat{z}_{\\mathrm{MAP}}$ 且 $g\\big(\\mathbb{E}[x \\mid y]\\big) \\neq \\mathbb{E}[z \\mid y]$，不变性的特殊情况仅适用于某些仿射或线性 $g$。**\n\n我们首先分析 MAP 估计量。在 $x$ 参数化中的 MAP 估计量是 $\\hat{x}_{\\mathrm{MAP}} = \\arg\\max_{x} \\pi_{X}(x \\mid y)$。在 $z$ 参数化中的 MAP 估计量是 $\\hat{z}_{\\mathrm{MAP}} = \\arg\\max_{z} \\pi_{Z}(z \\mid y)$。使用变量替换公式：\n$$\n\\pi_{Z}(z \\mid y) = \\pi_{X}(g^{-1}(z) \\mid y)\\,\\big|\\det Dg^{-1}(z)\\big|\n$$\n最大化子 $\\hat{z}_{\\mathrm{MAP}}$ 使整个表达式最大化。然而，$g(\\hat{x}_{\\mathrm{MAP}})$ 仅使第一项 $\\pi_{X}(g^{-1}(z) \\mid y)$ 最大化。除非雅可比行列式项 $|\\det Dg^{-1}(z)|$ 是常数，否则最大值的位置会改变。对于仿射变换 $z=g(x)=Ax+b$，雅可比行列式是常数，其中 $Dg^{-1}(z) = A^{-1}$ 是一个常数矩阵。对于任何一般的非线性变换，雅可比矩阵不是常数，因此 $g(\\hat{x}_{\\mathrm{MAP}}) \\neq \\hat{z}_{\\mathrm{MAP}}$。所以 MAP 估计量不是不变的（或等变的）。\n\n接下来，我们分析后验均值。$z$ 的后验均值是 $\\mathbb{E}[z \\mid y] = \\int z\\,\\pi_Z(z \\mid y)\\,\\mathrm{d}z$。我们进行变量替换到 $x$，其中 $z=g(x)$，$\\mathrm{d}z = |\\det Dg(x)|\\,\\mathrm{d}x$。\n$$\n\\mathbb{E}[z \\mid y] = \\int g(x) \\, \\pi_Z(g(x) \\mid y) \\, |\\det Dg(x)| \\, \\mathrm{d}x\n$$\n代入 $\\pi_Z(g(x) \\mid y) = \\pi_{X}(x \\mid y)\\,|\\det Dg^{-1}(g(x))|$ 的公式：\n$$\n\\mathbb{E}[z \\mid y] = \\int g(x) \\, \\pi_{X}(x \\mid y)\\,|\\det Dg^{-1}(g(x))| \\, |\\det Dg(x)| \\, \\mathrm{d}x\n$$\n由于 $Dg^{-1}(g(x)) = (Dg(x))^{-1}$，行列式的乘积为 $1$。\n$$\n\\mathbb{E}[z \\mid y] = \\int g(x)\\,\\pi_X(x \\mid y)\\,\\mathrm{d}x \\equiv \\mathbb{E}[g(x) \\mid y]\n$$\n问题是 $\\mathbb{E}[g(x) \\mid y]$ 是否等于 $g(\\mathbb{E}[x \\mid y])$。根据琴生不等式，此等式成立当且仅当 $g$ 是一个仿射函数。对于任何一般的非线性 $g$，$\\mathbb{E}[g(x)] \\neq g(\\mathbb{E}[x])$。\n\n因此，两种估计量在非线性重参数化下通常不被保持，该性质仅对仿射变换成立。该陈述准确地描述了这种情况。该陈述是 **正确** 的。\n\n**D. 在线性高斯逆问题（高斯先验和高斯似然）中，后验分布是高斯的，并满足 $\\hat{x}_{\\mathrm{MAP}} = \\mathbb{E}[x \\mid y]$。**\n\n线性高斯问题具有高斯先验 $\\pi_{\\mathrm{prior}}(x) \\sim \\mathcal{N}(\\mu_{pr}, C_{pr})$ 和源于带有加性高斯噪声 $y = Gx + \\epsilon$（其中 $\\epsilon \\sim \\mathcal{N}(0, C_{e})$）的线性正向模型的高斯似然。似然是 $p(y \\mid x) \\sim \\mathcal{N}(Gx, C_e)$。\n后验密度与两个高斯密度的乘积成正比：\n$$\n\\pi(x \\mid y) \\propto p(y \\mid x)\\,\\pi_{\\mathrm{prior}}(x) \\propto \\exp\\left(-\\frac{1}{2}(y-Gx)^T C_e^{-1} (y-Gx)\\right) \\exp\\left(-\\frac{1}{2}(x-\\mu_{pr})^T C_{pr}^{-1} (x-\\mu_{pr})\\right)\n$$\n指数的参数是 $x$ 的两个二次型之和。结果是 $x$ 的另一个二次型。一个负二次型的指数函数定义了一个高斯密度。因此，后验 $\\pi(x \\mid y)$ 是一个高斯分布。\n任何高斯分布（或任何对称单峰分布）的一个关键性质是其均值、中位数和众数都相同。MAP 估计量 $\\hat{x}_{\\mathrm{MAP}}$ 是后验的众数，而后验均值 $\\mathbb{E}[x \\mid y]$ 是后验的均值。\n由于后验分布是高斯的，其众数和均值重合。因此，$\\hat{x}_{\\mathrm{MAP}} = \\mathbb{E}[x \\mid y]$。\n\n该陈述是 **正确** 的。\n\n**E. 如果后验分布是多峰的，则后验均值未定义。**\n\n这个陈述是错误的。多峰性指的是概率密度函数中存在多个局部最大值。均值（一阶矩）的存在性取决于分布尾部的行为。具体来说，后验均值 $\\mathbb{E}[x \\mid y] = \\int x\\,\\pi(x \\mid y)\\,\\mathrm{d}x$ 定义当且仅当 $\\int \\|x\\|\\,\\pi(x \\mid y)\\,\\mathrm{d}x  \\infty$。\n一个简单的反例是两个高斯分布的混合：\n$$\n\\pi(x \\mid y) = w_1 \\mathcal{N}(x; \\mu_1, \\Sigma_1) + w_2 \\mathcal{N}(x; \\mu_2, \\Sigma_2)\n$$\n其中 $w_1+w_2=1$。如果 $\\mu_1$ 和 $\\mu_2$ 分离得足够远，这个分布就是多峰的。它的均值是明确定义的，由分量均值的加权平均给出：\n$$\n\\mathbb{E}[x \\mid y] = w_1 \\mu_1 + w_2 \\mu_2\n$$\n由于均值是明确定义的，该陈述不正确。一个分布可以是单峰但均值未定义（例如，柯西分布），也可以是多峰但均值明确定义。\n\n该陈述是 **错误** 的。\n\n**F. 如果后验密度在 $\\mathbb{R}^{n}$ 上是正常的，那么总存在一个唯一的 $\\hat{x}_{\\mathrm{MAP}}$。**\n\n这个陈述提出了两个主张：存在性和唯一性。两者都是错误的。\n*   **存在性**：一个正常密度（其积分为 $1$）并不保证其能达到最大值。考虑一个在 $\\mathbb{R}$ 上的连续密度，它由一系列以整数 $k=1, 2, ...$ 为中心的窄三角形函数（凸起）之和构成，其中第 $k$ 个凸起的高度是 $1-1/k$，面积是 $1/2^k$。总面积是 $\\sum 1/2^k = 1$，所以它是一个正常密度。该密度的上确界是 $1$，但对于任何有限的 $x$，这个值永远不会达到。因此，$\\arg\\max_x \\pi(x \\mid y)$ 不存在。\n*   **唯一性**：后验分布可以是多峰的，具有两个或更多相同高度的全局最大值。例如，两个高斯分布的对称混合，$\\pi(x \\mid y) = \\frac{1}{2} \\mathcal{N}(x; -\\mu_0, \\Sigma) + \\frac{1}{2} \\mathcal{N}(x; \\mu_0, \\Sigma)$，对于足够大的 $\\mu_0$，将有两个具有相同后验密度的众数。在这种情况下，MAP 估计量不是一个唯一的点，而是一个包含两个点的集合。另一个反例是在一个紧集上的均匀后验密度，例如 $\\pi(x|y) = \\mathcal{U}[-1, 1]$。区间 $[-1, 1]$ 中的每个点都是最大化子，所以 MAP 不是唯一的。\n\n该陈述是 **错误** 的。\n\n**G. 在有限参数空间（例如，一个有限的模型集合）中，在0-1损失 $L(x,a) = \\mathbf{1}\\{x \\neq a\\}$ 下，MAP 规则是贝叶斯最优的。**\n\n设有限参数空间为 $\\mathcal{X} = \\{x_1, x_2, \\ldots, x_K\\}$。后验是一个概率质量函数 $P(x=x_k \\mid y)$，对于 $k \\in \\{1, \\ldots, K\\}$。我们的决策 $a$ 必须是 $\\mathcal{X}$ 中的一个元素。我们选择 $a=x_j$，对于某个 $j \\in \\{1, \\ldots, K\\}$。\n后验期望损失为：\n$$\nR(a=x_j) = \\sum_{k=1}^{K} L(x_k, x_j) P(x=x_k \\mid y) = \\sum_{k=1}^{K} \\mathbf{1}\\{x_k \\neq x_j\\} P(x=x_k \\mid y)\n$$\n指示函数对于所有 $k \\neq j$ 为 $1$，对于 $k=j$ 为 $0$。所以总和变为：\n$$\nR(x_j) = \\sum_{k \\neq j} P(x=x_k \\mid y)\n$$\n由于所有后验概率之和为 $1$，我们有 $\\sum_{k \\neq j} P(x=x_k \\mid y) = 1 - P(x=x_j \\mid y)$。\n贝叶斯估计量是使该风险最小化的选择 $x_j$：\n$$\n\\hat{a} = \\arg\\min_{x_j \\in \\mathcal{X}} R(x_j) = \\arg\\min_{x_j \\in \\mathcal{X}} \\left( 1 - P(x=x_j \\mid y) \\right)\n$$\n最小化 $1 - P(x=x_j \\mid y)$ 等价于最大化 $P(x=x_j \\mid y)$。根据定义，使后验概率最大化的估计量就是 MAP 估计量。\n$$\n\\hat{x}_{\\mathrm{MAP}} = \\arg\\max_{x_j \\in \\mathcal{X}} P(x=x_j \\mid y)\n$$\n因此，在有限参数空间中，在0-1损失下，MAP 估计量确实是贝叶斯最优规则。这解决了在连续情况下遇到的模糊性（陈述 B）。\n\n该陈述是 **正确** 的。", "answer": "$$\\boxed{ACDG}$$", "id": "3367437"}, {"introduction": "在高维逆问题中，先验协方差矩阵常常是奇异的（即非满秩），这意味着先验信息将状态限制在一个低维子空间上。这种情况下的高斯先验是一个所谓的“退化”分布，它对后验分布的存在性和“适当性”（properness）有着深远的影响。这个练习引导你从基本原理出发，分析当先验协方差奇异时，后验分布成立的条件，帮助你深入理解高斯先验的数学结构及其在复杂模型中的应用 [@problem_id:3385468]。", "problem": "考虑一个有限维逆问题，其中未知状态向量 $x \\in \\mathbb{R}^{n}$ 被赋予一个均值为 $m \\in \\mathbb{R}^{n}$、协方差矩阵为 $C \\in \\mathbb{R}^{n \\times n}$ 的高斯先验。假设 $C$ 是对称、半正定的，并且其秩 $r  n$。通过一个线性正向算子 $H \\in \\mathbb{R}^{p \\times n}$ 和加性噪声 $\\varepsilon \\in \\mathbb{R}^{p}$ 获得观测值 $y \\in \\mathbb{R}^{p}$，其中 $\\varepsilon$ 是均值为 $0$、协方差矩阵为 $R \\in \\mathbb{R}^{p \\times p}$ 的高斯噪声，$R$ 是对称正定的。\n\n从高斯分布作为标准正态分布的线性映射这一基本表示出发，分析当 $C$ 是半正定但非满秩时，对先验作为 $\\mathbb{R}^{n}$ 上概率测度的存在性与性质，以及给定线性高斯似然下正常后验密度存在性的影响。你的分析应采用以下基础：\n- 多元高斯分布可以表示为 $x = m + A z$ 的定义，其中 $z \\in \\mathbb{R}^{n}$ 具有独立的标准正态分量，且 $A \\in \\mathbb{R}^{n \\times n}$ 满足 $C = A A^{\\top}$。\n- 先验或后验相对于 $\\mathbb{R}^{n}$ 上的勒贝格测度的正常性（properness）通过绝对连续性和 Radon–Nikodym 导数的存在性来解释。\n- 后验可被刻画为最小化一个由先验和似然构造的二次目标函数，其唯一性和正常性要求严格凸性。\n\n下列哪个陈述是正确的？\n\nA. 如果 $C$ 是半正定但奇异的，则先验是一个退化的高斯概率测度，其支撑集在仿射子空间 $m + \\operatorname{range}(C)$ 上，并且不承认相对于 $\\mathbb{R}^{n}$ 上勒贝格测度的密度。对于线性高斯似然 $y = H x + \\varepsilon$ 且 $R \\succ 0$，后验承认勒贝格密度（是正常的）当且仅当 $\\ker(C) \\cap \\ker(H) = \\{0\\}$，这等价于对称矩阵 $C^{+} + H^{\\top} R^{-1} H$ 是正定的，其中 $C^{+}$ 是 $C$ 的 Moore–Penrose 伪逆。\n\nB. 如果 $C$ 是奇异的，则高斯先验不是一个概率测度（其总质量不为 $1$），必须通过将 $C$ 替换为 $C + \\epsilon I$（对于某个 $\\epsilon  0$）进行正则化才能变为正常的。\n\nC. 因为 $R \\succ 0$，所以无论 $H$ 和 $C$ 如何，后验总是正常的；$R$ 的正定性保证了 $H^{\\top} R^{-1} H$ 在 $\\mathbb{R}^{n}$ 上是正定的。\n\nD. 奇异的 $C$ 意味着在 $\\ker(C)$ 的方向上具有无限方差，因此先验是重尾的，但仍然在 $\\mathbb{R}^{n}$ 上有勒贝格密度。\n\nE. 仅当 $H$ 具有满列秩时，后验均值才存在且唯一；后验协方差是否正常不依赖于 $C$。", "solution": "用户要求对一个涉及奇异先验协方差矩阵的贝叶斯逆问题进行详细分析。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- 状态向量：$x \\in \\mathbb{R}^{n}$。\n- $x$ 的先验分布：高斯分布，均值为 $m \\in \\mathbb{R}^{n}$，协方差矩阵为 $C \\in \\mathbb{R}^{n \\times n}$。\n- $C$ 的性质：对称、半正定，$\\operatorname{rank}(C) = r  n$（即 $C$ 是奇异的）。\n- 观测向量：$y \\in \\mathbb{R}^{p}$。\n- 观测模型：$y = H x + \\varepsilon$，其中线性正向算子为 $H \\in \\mathbb{R}^{p \\times n}$。\n- $\\varepsilon$ 的噪声分布：高斯分布，均值为 $0$，协方差矩阵为 $R \\in \\mathbb{R}^{p \\times p}$。\n- $R$ 的性质：对称正定（$R \\succ 0$）。\n- 指定的分析框架：\n    1.  表示法 $x = m + A z$，其中 $z \\sim \\mathcal{N}(0, I)$ 且 $C = A A^{\\top}$。\n    2.  正常性定义为相对于 $\\mathbb{R}^{n}$ 上的勒贝格测度的绝对连续性（存在 Radon-Nikodym 导数）。\n    3.  通过最小化二次目标函数来刻画后验，将正常性与严格凸性联系起来。\n\n**第2步：使用提取的已知条件进行验证**\n-   **科学基础：** 该问题是贝叶斯统计、逆问题和数据同化中的一个标准但高级的主题。所有概念（高斯分布、奇异协方差、Moore-Penrose 伪逆、正常性）都定义明确且在数学上是合理的。\n-   **适定性：** 问题要求分析奇异先验的后果，并评估给定的陈述。这是一个清晰且可解决的任务。\n-   **客观性：** 语言技术性强且没有歧义。\n-   **完整性与一致性：** 问题提供了所有必要信息。使用二次目标最小化框架的指令为分析提供了清晰的路径，解决了定义后验时可能出现的歧义。问题内部一致，不包含矛盾的约束。\n\n**第3步：结论与行动**\n问题是有效的。分析将从给定的原则出发进行推导。\n\n### 推导\n\n分析分两部分进行：首先刻画先验分布，然后刻画后验分布。\n\n**1. 先验分布分析**\n\n对于任何对称半正定协方差矩阵 $C$，高斯分布 $\\mathcal{N}(m, C)$ 都是有定义的。其对应的特征函数是 $\\phi_x(t) = \\exp(i t^\\top m - \\frac{1}{2} t^\\top C t)$，它总是定义了一个有效的概率测度。\n\n协方差矩阵 $C$ 是奇异的，其秩 $\\operatorname{rank}(C) = r  n$。设 $C$ 的特征分解为 $C = U \\Lambda U^\\top$，其中 $U$ 是特征向量构成的正交矩阵，$\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_r, 0, \\dots, 0)$ 是特征值的对角矩阵，其中对于 $i=1, \\dots, r$，$\\lambda_i  0$。\n\n随机向量 $x \\sim \\mathcal{N}(m, C)$ 可以生成为 $x = m + U \\Lambda^{1/2} z$，其中 $z \\sim \\mathcal{N}(0, I_n)$。\n向量 $x-m = U \\Lambda^{1/2} z$ 是标准正态向量 $z$ 的线性变换。$x-m$ 分布的支撑集是这个线性映射的值域，即 $\\operatorname{range}(U \\Lambda^{1/2})$。这个空间由 $U$ 的前 $r$ 列（即对应于 $C$ 的正特征值的特征向量）所张成。这个子空间恰好是协方差矩阵本身的值域，即 $\\operatorname{range}(C)$。\n\n因此，随机向量 $x$ 被限制在仿射子空间 $m + \\operatorname{range}(C)$ 上。这是 $\\mathbb{R}^n$ 的一个 $r$ 维子空间。由于 $r  n$，这个子空间在 $\\mathbb{R}^n$ 中的勒贝格测度为零。一个集中在测度为零的集合上的概率测度，不可能是关于更大空间上勒贝格测度绝对连续的。因此，它在 $\\mathbb{R}^n$ 上不承认概率密度函数（Radon-Nikodym 导数）。这种分布被称为退化高斯分布。它是一个有效的概率测度，但它在问题所定义的意义上不是“正常的”。\n\n**2. 后验分布分析**\n\n后验分布结合了来自先验和似然的信息。给定 $x$ 时观测值 $y$ 的似然由噪声模型 $\\varepsilon = y - Hx \\sim \\mathcal{N}(0, R)$ 导出。由于 $R \\succ 0$，似然在 $\\mathbb{R}^p$ 上有一个密度，其正比于 $\\exp(-\\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx))$。\n\n问题指导我们通过最小化一个二次目标函数 $J(x)$ 来刻画后验。对于高斯分布，后验密度的负对数与 $J(x)$ 成正比。$J(x)$ 的标准形式是：\n$$J(x) = \\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx) + \\frac{1}{2} (x-m)^\\top C^{-1} (x-m)$$\n这个公式假设 $C$ 是可逆的。对于奇异的 $C$，先验项必须被推广。标准的推广方法是用 Moore-Penrose 伪逆 $C^\\dagger$ 替换 $C^{-1}$，得到：\n$$J(x) = \\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx) + \\frac{1}{2} (x-m)^\\top C^\\dagger (x-m)$$\n这个目标函数应在 $x \\in \\mathbb{R}^n$ 上最小化。然后，后验分布被形式化地视为一个高斯分布，其密度正比于 $\\exp(-J(x))$。当且仅当 $J(x)$ 是一个严格凸的二次函数时，后验才是在 $\\mathbb{R}^n$ 上的正常高斯分布，这保证了 $\\exp(-J(x))$ 在 $\\mathbb{R}^n$ 上是可积的。\n\n$J(x)$ 的严格凸性由其 Hessian 矩阵是否为正定决定。让我们求 Hessian 矩阵：\n$J(x) = \\frac{1}{2} (x^\\top H^\\top R^{-1} H x - 2y^\\top R^{-1} H x + y^\\top R^{-1}y) + \\frac{1}{2} (x^\\top C^\\dagger x - 2m^\\top C^\\dagger x + m^\\top C^\\dagger m)$\n这是一个关于 $x$ 的二次型。Hessian 矩阵是二次项的矩阵：\n$$\\nabla^2_x J(x) = C^\\dagger + H^\\top R^{-1} H$$\n这个矩阵是后验精度矩阵。为了使后验在 $\\mathbb{R}^n$ 上是正常的，这个矩阵必须是对称且正定的。\n\n让我们分析一下 $C^\\dagger + H^\\top R^{-1} H$ 这一项。\n-   由于 $C$ 是对称半正定（SPSD），其伪逆 $C^\\dagger$ 也是 SPSD。$C^\\dagger$ 的零空间与 $C$ 的零空间相同，即 $\\ker(C^\\dagger) = \\ker(C)$。对于一个向量 $v \\in \\mathbb{R}^n$，$v^\\top C^\\dagger v \\ge 0$，等号成立当且仅当 $v \\in \\ker(C)$。\n-   由于 $R$ 是正定的（$R \\succ 0$），其逆 $R^{-1}$ 也是正定的。矩阵 $H^\\top R^{-1} H$ 是 SPSD。对于一个向量 $v \\in \\mathbb{R}^n$，$v^\\top (H^\\top R^{-1} H) v = (Hv)^\\top R^{-1} (Hv) \\ge 0$。等号成立当且仅当 $Hv=0$，这意味着 $v \\in \\ker(H)$。所以，$\\ker(H^\\top R^{-1} H) = \\ker(H)$。\n-   两个 SPSD 矩阵之和 $A+B$ 是正定的，当且仅当不存在非零向量 $v$ 使得 $v^\\top A v = 0$ 和 $v^\\top B v = 0$ 同时成立。这等价于 $\\ker(A) \\cap \\ker(B) = \\{0\\}$。\n\n将此应用于后验精度矩阵，$C^\\dagger + H^\\top R^{-1} H$ 是正定的当且仅当 $\\ker(C^\\dagger) \\cap \\ker(H^\\top R^{-1} H) = \\{0\\}$。\n这可以简化为：\n$$\\ker(C) \\cap \\ker(H) = \\{0\\}$$\n这个条件意味着，任何先验不提供信息（即，在 $C$ 的零空间中的向量）的方向，都必须是能被观测所“看到”的方向（即，不在 $H$ 的零空间中）。\n\n### 选项评估\n\n**A. 如果 $C$ 是半正定但奇异的，则先验是一个退化的高斯概率测度，其支撑集在仿射子空间 $m + \\operatorname{range}(C)$ 上，并且不承认相对于 $\\mathbb{R}^{n}$ 上勒贝格测度的密度。对于线性高斯似然 $y = H x + \\varepsilon$ 且 $R \\succ 0$，后验承认勒贝格密度（是正常的）当且仅当 $\\ker(C) \\cap \\ker(H) = \\{0\\}$，这等价于对称矩阵 $C^{+} + H^{\\top} R^{-1} H$ 是正定的，其中 $C^{+}$ 是 $C$ 的 Moore–Penrose 伪逆。**\n-   陈述的第一部分，关于先验的性质，如我们的分析所示是严格正确的。\n-   第二部分，关于后验，在问题规定的分析框架下（最小化广义二次目标）也是正确的。后验精度矩阵 $C^\\dagger + H^\\top R^{-1} H$ 为正定的条件恰好是 $\\ker(C) \\cap \\ker(H) = \\{0\\}$。该矩阵的正定性确保了得到的后验高斯分布是非退化的，并且在 $\\mathbb{R}^n$ 上有正常的密度。\n-   **结论：正确。**\n\n**B. 如果 $C$ 是奇异的，则高斯先验不是一个概率测度（其总质量不为 $1$），必须通过将 $C$ 替换为 $C + \\epsilon I$（对于某个 $\\epsilon  0$）进行正则化才能变为正常的。**\n-   这是不正确的。具有奇异协方差矩阵的高斯分布是一个有效的概率测度，其总质量为 $1$。它只是一个退化测度。正则化是用于其他目的的技术，而不是为了将一个有效的测度变为测度。\n-   **结论：不正确。**\n\n**C. 因为 $R \\succ 0$，所以无论 $H$ 和 $C$ 如何，后验总是正常的；$R$ 的正定性保证了 $H^{\\top} R^{-1} H$ 在 $\\mathbb{R}^{n}$ 上是正定的。**\n-   这是不正确的。虽然 $R \\succ 0$，但矩阵 $H^\\top R^{-1} H$ 通常只是半正定的。它的零空间是 $\\ker(H)$，除非 $H$ 具有满列秩，否则它不是平凡的。因此，后验的正常性不能得到保证，而是取决于 $\\ker(C)$ 和 $\\ker(H)$ 之间的相互作用。\n-   **结论：不正确。**\n\n**D. 奇异的 $C$ 意味着在 $\\ker(C)$ 的方向上具有无限方差，因此先验是重尾的，但仍然在 $\\mathbb{R}^{n}$ 上有勒贝格密度。**\n-   这在多个方面都是不正确的。奇异的 $C$ 意味着在 $\\ker(C)$ 的方向上具有*零*方差。一个向量 $v \\in \\ker(C)$ 对应于一个为 $0$ 的特征值，所以投影 $v^\\top x$ 的方差是 $v^\\top C v = 0$。这与无限方差相反。高斯分布不是重尾的。如前所述，奇异先验在 $\\mathbb{R}^n$ 上没有勒贝格密度。\n-   **结论：不正确。**\n\n**E. 仅当 $H$ 具有满列秩时，后验均值才存在且唯一；后验协方差是否正常不依赖于 $C$。**\n-   这是不正确的。后验均值的唯一性与后验精度 $C^\\dagger + H^\\top R^{-1} H$ 的正定性有关，这要求 $\\ker(C) \\cap \\ker(H) = \\{0\\}$。即使 $H$ 不具有满列秩（例如，如果 $C$ 是非奇异的，即 $\\ker(C)=\\{0\\}$），这个条件也可以满足。正常性的条件明确涉及 $\\ker(C)$，因此它绝对依赖于 $C$。\n-   **结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3385468"}, {"introduction": "贝叶斯建模的一个核心挑战是如何设定先验分布中的超参数，例如先验协方差的尺度因子。经验贝叶斯（Empirical Bayes）方法，也称为第二类最大似然，提供了一种基于数据本身来选择这些超参数的原则性方法。这个练习将让你亲手实践这一强大技术，你不仅需要推导边缘似然（证据）的解析梯度，还需要编写程序来寻找最优超参数，从而将理论与模型选择的实际应用联系起来 [@problem_id:3385486]。", "problem": "考虑由以下几部分定义的线性高斯逆问题。设未知参数向量为 $x \\in \\mathbb{R}^n$，其高斯先验为 $x \\sim \\mathcal{N}(0, C)$，其中协方差 $C$ 通过一个正标量参数 $\\tau$ 进行缩放，即 $C = \\tau^{-1} \\tilde{C}$。矩阵 $\\tilde{C} \\in \\mathbb{R}^{n \\times n}$ 是已知的对称正定（SPD）矩阵。观测值 $y \\in \\mathbb{R}^m$ 由线性模型 $y = A x + \\varepsilon$ 生成，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的，且 $\\varepsilon \\sim \\mathcal{N}(0, R)$，其中 $R \\in \\mathbb{R}^{m \\times m}$ 是已知的 SPD 矩阵。边际似然（证据）为 $p(y \\mid \\tau)$，经验贝叶斯（EB）的目标是选择 $\\tau$ 以最大化此证据。\n\n仅从高斯分布、线性模型和贝叶斯法则的核心定义出发，推导对数证据关于尺度参数 $\\tau$ 的梯度表达式，该表达式应使用 $x \\mid y, \\tau$ 的后验均值 $m(\\tau)$ 和后验协方差 $\\Sigma(\\tau)$ 来表示。然后实现一个程序，该程序能：\n- 使用线性模型的基本高斯恒等式计算后验协方差 $\\Sigma(\\tau)$ 和后验均值 $m(\\tau)$。\n- 使用推导出的表达式计算导数 $\\partial_{\\tau} \\log p(y \\mid \\tau)$。\n- 通过在正区间上使用稳健的区间求根方法求解 $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$，找到 EB 最大化子 $\\tau^\\star$。\n\n您可以使用的基础知识包括：\n- 高斯密度定义及其归一化，包括用于变换后的线性模型。\n- 贝叶斯法则和多元高斯分布中的标准条件化公式。\n- 通过对高斯隐变量进行积分得到的边际似然恒等式。\n- 称为 Fisher 恒等式的性质：对于具有先验参数 $\\tau$ 的隐变量模型，$\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ \\partial_{\\tau} \\log p(x \\mid \\tau) \\right]$。\n\n您的程序必须为以下测试套件计算数值结果。在每种情况下，设 $n$ 表示 $x$ 的维度，$m$ 表示 $y$ 的维度，并定义 $A$、$R$、$\\tilde{C}$、$y$ 和一个评估点 $\\tau_0  0$。对于每种情况，返回一对浮点数：首先是在 $\\tau_0$ 处计算的 $\\partial_{\\tau} \\log p(y \\mid \\tau)$ 的值，其次是求解 $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$ 得到的 EB 最大化子 $\\tau^\\star$。\n\n测试套件：\n1. 情况1（理想路径，中等维度）：\n   - $n = 2$, $m = 2$,\n   - $A = \\begin{bmatrix} 1.0  0.5 \\\\ 0.2  1.0 \\end{bmatrix}$,\n   - $R = \\operatorname{diag}(0.1^2, 0.15^2)$,\n   - $\\tilde{C} = \\begin{bmatrix} 1.0  0.2 \\\\ 0.2  2.0 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$,\n   - $\\tau_0 = 1.0$.\n2. 情况2（相关先验，较大的 $n$）：\n   - $n = 3$, $m = 3$,\n   - $A = \\begin{bmatrix} 1.0  -0.3  0.0 \\\\ -0.2  0.8  0.5 \\\\ 0.0  0.4  1.2 \\end{bmatrix}$,\n   - $R = \\operatorname{diag}(0.2^2, 0.25^2, 0.3^2)$,\n   - $\\tilde{C} = \\begin{bmatrix} 1.5  0.1  0.0 \\\\ 0.1  1.0  0.2 \\\\ 0.0  0.2  2.0 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 0.8 \\\\ -1.0 \\\\ 0.3 \\end{bmatrix}$,\n   - $\\tau_0 = 0.8$.\n3. 情况3（边界，标量问题）：\n   - $n = 1$, $m = 1$,\n   - $A = \\begin{bmatrix} 1.7 \\end{bmatrix}$,\n   - $R = \\begin{bmatrix} 0.05^2 \\end{bmatrix}$,\n   - $\\tilde{C} = \\begin{bmatrix} 0.7 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 0.25 \\end{bmatrix}$,\n   - $\\tau_0 = 0.5$.\n\n用于计算的定义：\n- 先验精度为 $C^{-1} = \\tau \\tilde{C}^{-1}$。\n- 数据精度为 $R^{-1}$。\n- 后验精度为 $\\Lambda(\\tau) = C^{-1} + A^\\top R^{-1} A$。\n- 后验协方差为 $\\Sigma(\\tau) = \\Lambda(\\tau)^{-1}$。\n- 后验均值为 $m(\\tau) = \\Sigma(\\tau) A^\\top R^{-1} y$。\n\n目标推导：\n- 仅用 $n$、$\\tau$、$\\tilde{C}^{-1}$、$\\Sigma(\\tau)$ 和 $m(\\tau)$ 表示 $\\partial_{\\tau} \\log p(y \\mid \\tau)$，不要引入任何其他辅助变量。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3,result4,result5,result6]”）。这六个值按顺序对应三个测试用例：对于每个用例，首先是 $\\partial_{\\tau} \\log p(y \\mid \\tau_0)$，然后是 $\\tau^\\star$。所有输出都必须是实数（浮点数）。此问题不涉及物理单位或角度单位，也不需要百分比。", "solution": "经过严格审查，该问题被认定为有效。所有提供的组成部分在科学和数学上都是合理的、自洽的且良定的。问题陈述在线性高斯模型的经验贝叶斯既定框架内提供了一个明确的目标。所有的定义、约束和测试用例都得到了形式化的指定并且是一致的。\n\n任务是推导对数边际似然（对数证据）关于先验尺度参数 $\\tau$ 的梯度，然后实现一个数值程序来找到使该证据最大化的 $\\tau$ 值。\n\n### 1. 对数证据梯度的理论推导\n\n我们需要求解 $\\partial_{\\tau} \\log p(y \\mid \\tau)$。问题建议从 Fisher 恒等式开始，其形式如下：\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ \\partial_{\\tau} \\log p(x \\mid \\tau) \\right]\n$$\n该恒等式允许我们通过计算对数先验的梯度关于后验分布的期望，来计算边际似然的梯度。\n\n首先，我们写出对数先验 $\\log p(x \\mid \\tau)$ 的表达式。$x \\in \\mathbb{R}^n$ 的先验是一个零均值高斯分布，$x \\sim \\mathcal{N}(0, C)$，其协方差为 $C = \\tau^{-1} \\tilde{C}$。相应的先验精度矩阵是 $C^{-1} = \\tau \\tilde{C}^{-1}$。概率密度函数为：\n$$\np(x \\mid \\tau) = \\frac{1}{(2\\pi)^{n/2} (\\det(C))^{1/2}} \\exp\\left(-\\frac{1}{2} x^\\top C^{-1} x\\right)\n$$\n因此，对数先验为：\n$$\n\\log p(x \\mid \\tau) = -\\frac{1}{2} x^\\top C^{-1} x - \\frac{1}{2} \\log \\det(C) - \\frac{n}{2} \\log(2\\pi)\n$$\n我们代入 $C = \\tau^{-1} \\tilde{C}$ 和 $C^{-1} = \\tau \\tilde{C}^{-1}$。行列式项变为 $\\log \\det(\\tau^{-1} \\tilde{C}) = \\log(\\tau^{-n} \\det(\\tilde{C})) = -n \\log \\tau + \\log \\det(\\tilde{C})$。\n$$\n\\log p(x \\mid \\tau) = -\\frac{1}{2} x^\\top (\\tau \\tilde{C}^{-1}) x - \\frac{1}{2} (-n \\log \\tau + \\log \\det(\\tilde{C})) - \\frac{n}{2} \\log(2\\pi)\n$$\n$$\n\\log p(x \\mid \\tau) = -\\frac{\\tau}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2} \\log \\tau - \\frac{1}{2}\\log\\det(\\tilde{C}) - \\frac{n}{2}\\log(2\\pi)\n$$\n接下来，我们对对数先验关于 $\\tau$ 求导。涉及 $\\tilde{C}$ 和 $2\\pi$ 的项相对于 $\\tau$ 是常数。\n$$\n\\partial_{\\tau} \\log p(x \\mid \\tau) = \\frac{\\partial}{\\partial \\tau} \\left( -\\frac{\\tau}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2} \\log \\tau \\right) = -\\frac{1}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2\\tau}\n$$\n现在，我们通过计算该表达式关于后验分布 $p(x \\mid y, \\tau) = \\mathcal{N}(x \\mid m(\\tau), \\Sigma(\\tau))$ 的期望来应用 Fisher 恒等式。\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ -\\frac{1}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2\\tau} \\right]\n$$\n利用期望的线性性质，我们得到：\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = -\\frac{1}{2} \\mathbb{E}_{x \\mid y, \\tau} \\left[ x^\\top \\tilde{C}^{-1} x \\right] + \\frac{n}{2\\tau}\n$$\n对于随机向量 $z \\sim \\mathcal{N}(\\mu, \\Sigma_z)$，其二次型 $z^\\top Q z$ 的期望由恒等式 $\\mathbb{E}[z^\\top Q z] = \\operatorname{Tr}(Q \\Sigma_z) + \\mu^\\top Q \\mu$ 给出。在我们的情况下，随机变量是来自后验分布的 $x$，因此其均值是后验均值 $m(\\tau)$，其协方差是后验协方差 $\\Sigma(\\tau)$。二次型的矩阵是 $Q = \\tilde{C}^{-1}$。\n应用此恒等式可得：\n$$\n\\mathbb{E}_{x \\mid y, \\tau} \\left[ x^\\top \\tilde{C}^{-1} x \\right] = \\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau)) + m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau)\n$$\n将此结果代回我们的梯度表达式，我们得到最终所需的形式：\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\frac{n}{2\\tau} - \\frac{1}{2} \\left( \\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau)) + m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau) \\right)\n$$\n该表达式仅依赖于 $n$、$\\tau$、$\\tilde{C}^{-1}$ 以及后验统计量 $m(\\tau)$ 和 $\\Sigma(\\tau)$，符合要求。\n\n### 2. 算法设计与实现\n\n问题要求一个 Python 程序为每个测试用例执行两项任务：\n1.  在给定点 $\\tau_0$ 计算梯度 $\\partial_{\\tau} \\log p(y \\mid \\tau)$。\n2.  通过求解方程 $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$ 来找到最大化对数证据的最优超参数 $\\tau^\\star > 0$。\n\n为实现这一点，我们设计一个函数，用于计算任何给定 $\\tau > 0$ 的梯度。该函数需要后验均值 $m(\\tau)$ 和协方差 $\\Sigma(\\tau)$，这些值是使用为高斯线性模型提供的标准公式计算的：\n- 后验精度: $\\Lambda(\\tau) = C^{-1} + A^\\top R^{-1} A = \\tau \\tilde{C}^{-1} + A^\\top R^{-1} A$。\n- 后验协方差: $\\Sigma(\\tau) = \\Lambda(\\tau)^{-1}$。\n- 后验均值: $m(\\tau) = \\Sigma(\\tau) A^\\top R^{-1} y$。\n\n数值算法如下：\n1.  对于给定的测试用例，预先计算矩阵的逆 $\\tilde{C}^{-1}$ 和 $R^{-1}$。\n2.  定义一个函数 `gradient(tau)`，它接受一个标量 `tau` 并返回 $\\partial_{\\tau} \\log p(y \\mid \\tau)$ 的值。\n    a. 在此函数内部，计算 $\\Lambda(\\tau)$、$\\Sigma(\\tau)$ 和 $m(\\tau)$。这涉及矩阵乘法、加法和求逆。\n    b. 使用这些后验统计量来计算推导出的梯度公式的三个项：$\\frac{n}{2\\tau}$、$\\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau))$ 和 $m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau)$。\n    c. 组合这些项以返回最终的梯度值。\n3.  在指定的 $\\tau_0$ 处评估此 `gradient` 函数，以获得测试用例的第一个所需结果。\n4.  为了找到根 $\\tau^\\star$，我们使用一种稳健的数值方法。函数 $g(\\tau) = \\partial_{\\tau} \\log p(y \\mid \\tau)$ 具有当 $\\tau \\to 0^+$ 时 $g(\\tau) \\to +\\infty$ 的性质。\n5.  对于一个典型的、良态的问题，对数证据是凹的或单峰的，这意味着对于 $\\tau > 0$，梯度将恰好一次从正到负穿过零点。这种结构非常适合像 `scipy.optimize.brentq` 中提供的 Brent 方法这样的区间求根算法。\n6.  要使用 `brentq`，我们需要找到一个区间 $[a, b]$ 使得 $g(a) \\cdot g(b)  0$。\n    a. 我们选择 $a$ 为一个小的正数（例如 $10^{-8}$），我们期望在此处 $g(a) > 0$。\n    b. 我们使用扩展搜索来找到 $b$。我们从一个猜测值开始，例如 $b=1.0$，然后重复地将其乘以一个因子（例如 $2$），直到 $g(b)$ 变为负数。这确保了为求根器提供一个有效的区间。\n7.  调用 `scipy.optimize.brentq(gradient, a, b)` 来找到 $\\tau^\\star$。\n8.  对所有测试用例重复此过程，并按规定收集和格式化结果。\n\n```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the Empirical Bayes problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 2, \"m\": 2,\n            \"A\": np.array([[1.0, 0.5], [0.2, 1.0]]),\n            \"R\": np.diag([0.1**2, 0.15**2]),\n            \"C_tilde\": np.array([[1.0, 0.2], [0.2, 2.0]]),\n            \"y\": np.array([1.0, -0.5]),\n            \"tau_0\": 1.0\n        },\n        {\n            \"n\": 3, \"m\": 3,\n            \"A\": np.array([[1.0, -0.3, 0.0], [-0.2, 0.8, 0.5], [0.0, 0.4, 1.2]]),\n            \"R\": np.diag([0.2**2, 0.25**2, 0.3**2]),\n            \"C_tilde\": np.array([[1.5, 0.1, 0.0], [0.1, 1.0, 0.2], [0.0, 0.2, 2.0]]),\n            \"y\": np.array([0.8, -1.0, 0.3]),\n            \"tau_0\": 0.8\n        },\n        {\n            \"n\": 1, \"m\": 1,\n            \"A\": np.array([[1.7]]),\n            \"R\": np.array([[0.05**2]]),\n            \"C_tilde\": np.array([[0.7]]),\n            \"y\": np.array([0.25]),\n            \"tau_0\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, A, R, C_tilde, y, tau_0 = case['n'], case['A'], case['R'], case['C_tilde'], case['y'], case['tau_0']\n        \n        # Pre-compute matrix inverses and other constant terms\n        C_tilde_inv = np.linalg.inv(C_tilde)\n        R_inv = np.linalg.inv(R)\n        At_Rinv = A.T @ R_inv\n        At_Rinv_A = At_Rinv @ A\n        At_Rinv_y = At_Rinv @ y\n\n        def get_grad_log_evidence(tau):\n            \"\"\"\n            Computes the derivative of the log evidence wrt tau.\n            d/d(tau) log p(y|tau) = n/(2*tau) - 0.5 * (Tr(C_tilde_inv * Sigma) + m.T @ C_tilde_inv @ m)\n            \"\"\"\n            if tau == 0:\n                return np.inf\n\n            # Posterior precision\n            # Lambda = tau * C_tilde_inv + A.T @ R_inv @ A\n            Lambda = tau * C_tilde_inv + At_Rinv_A\n            \n            # Posterior covariance and mean\n            try:\n                # Use solve for better numerical stability than inv\n                Sigma = np.linalg.solve(Lambda, np.identity(n))\n                m = Sigma @ At_Rinv_y\n            except np.linalg.LinAlgError:\n                # If Lambda is singular, gradient is ill-defined.\n                return np.nan\n\n            # Compute the terms for the gradient expression\n            trace_term = np.trace(C_tilde_inv @ Sigma)\n            quadratic_term = m.T @ C_tilde_inv @ m\n            \n            grad = n / (2 * tau) - 0.5 * (trace_term + quadratic_term)\n            return grad\n\n        # 1. Compute the gradient at tau_0\n        grad_at_tau0 = get_grad_log_evidence(tau_0)\n        \n        # 2. Find the EB maximizer tau_star by finding the root of the gradient\n        # We need a bracket [a, b] such that g(a) > 0 and g(b) = 0\n        a = 1e-8\n        \n        # Check sign at lower bound\n        g_a = get_grad_log_evidence(a)\n        if g_a = 0:\n            tau_star = np.nan\n        else:\n            # Find an upper bound b such that get_grad_log_evidence(b) = 0\n            b = 1.0 if tau_0  1.0 else tau_0 * 2.0\n            \n            # Exponentially increase b until the sign changes\n            max_iter = 100\n            for _ in range(max_iter):\n                g_b = get_grad_log_evidence(b)\n                if g_b = 0:\n                    break\n                b *= 2.0\n            else: # Loop finished without break\n                tau_star = np.nan\n            \n            if not np.isnan(g_b):\n                 tau_star = brentq(get_grad_log_evidence, a, b, xtol=1e-12, rtol=1e-12)\n            else:\n                 tau_star = np.nan\n\n        results.extend([grad_at_tau0, tau_star])\n    \n    # This function is not called in the final output, but it's the one that generates it.\n    # print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n    # return results\n```", "answer": "$$\\boxed{[5.20173618, 0.22238386, 3.01890487, 0.30138933, 0.98395277, 16.03541402]}$$", "id": "3385486"}]}