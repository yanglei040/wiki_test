{"hands_on_practices": [{"introduction": "最大似然估计通常涉及对参数的物理或逻辑约束。这个练习 [@problem_id:3402122] 提供了在凸优化框架内构建约束最大似然估计问题的实践机会。通过推导和应用 Karush-Kuhn-Tucker (KKT) 条件，你将求解一个既满足统计最优性又符合物理可行性的估计值。", "problem": "考虑一个用于数据同化的线性高斯观测模型：未知状态参数向量 $\\theta \\in \\mathbb{R}^{n}$ 需要从观测数据向量 $y \\in \\mathbb{R}^{n}$ 中估计，其模型为 $y \\mid \\theta \\sim \\mathcal{N}(H\\theta, R)$，其中 $H \\in \\mathbb{R}^{n \\times n}$ 是一个已知的正向算子，$R \\in \\mathbb{R}^{n \\times n}$ 是一个已知的对称正定协方差矩阵。目标是在满足线性不等式约束的条件下执行最大似然估计（MLE），这些约束表示了容许的物理界限，并被建模为一个凸多面体集\n$$\n\\Theta \\;=\\; \\{\\theta \\in \\mathbb{R}^{n} : C\\theta \\le d\\},\n$$\n其中 $C \\in \\mathbb{R}^{m \\times n}$ 且 $d \\in \\mathbb{R}^{m}$，不等式是逐分量成立的。\n\n从最大似然估计（MLE）的定义、高斯似然和约束优化的基本原理出发，完成以下任务：\n\n1. 将约束MLE问题表述为一个在 $\\Theta$ 上的优化问题。\n2. 从第一性原理推导刻画最优解 $\\hat{\\theta}$ 的 Karush-Kuhn-Tucker (KKT) 条件，用对数似然的梯度 $\\nabla \\ell(\\theta)$ 和一个拉格朗日乘子向量 $\\lambda \\in \\mathbb{R}^{m}$ 来表示这些条件。\n3. 将问题特化到 $n=2$ 的情况，其中 $H = I_{2}$，$R = I_{2}$，约束由下式定义\n$$\nC \\;=\\; \\begin{pmatrix} -1  0 \\\\[4pt] 0  -1 \\\\[4pt] 1  1 \\end{pmatrix}, \\quad d \\;=\\; \\begin{pmatrix} 0 \\\\[4pt] 0 \\\\[4pt] 2 \\end{pmatrix},\n$$\n这强制了 $\\theta_{1} \\ge 0$, $\\theta_{2} \\ge 0$ 和 $\\theta_{1} + \\theta_{2} \\le 2$。给定观测数据向量 $y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$，计算约束MLE $\\hat{\\theta}$。\n\n假设Slater条件成立（即存在一个严格可行解 $\\bar{\\theta}$ 使得 $C\\bar{\\theta}  d$），这确保了在这个凸集上的凹最大化问题中，KKT条件对于最优性是充分且必要的。将您的最终答案表示为行矩阵 $\\begin{pmatrix} \\hat{\\theta}_{1}  \\hat{\\theta}_{2} \\end{pmatrix}$，使用 $\\mathrm{pmatrix}$ 环境。无需四舍五入，且不涉及物理单位。", "solution": "该问题经验证具有科学依据、适定、客观、完整且一致。这是一个应用于统计估计的约束优化标准问题。我们按要求分三部分进行解答。\n\n### 第 1 部分：约束MLE问题的表述\n\n问题是从观测向量 $y \\in \\mathbb{R}^{n}$ 估计状态向量 $\\theta \\in \\mathbb{R}^{n}$，给定线性高斯模型 $y \\mid \\theta \\sim \\mathcal{N}(H\\theta, R)$。似然函数 $L(\\theta; y)$ 是观测值 $y$ 的概率密度函数在给定数据点上的取值，并被视为参数 $\\theta$ 的函数。对于多元正态分布，其形式为：\n$$\nL(\\theta; y) = p(y \\mid \\theta) = \\frac{1}{(2\\pi)^{n/2} (\\det(R))^{1/2}} \\exp\\left(-\\frac{1}{2}(y - H\\theta)^T R^{-1} (y - H\\theta)\\right)\n$$\n最大似然估计（MLE）旨在寻找使该似然函数最大化的 $\\theta$ 值。通常处理似然的自然对数，即对数似然函数 $\\ell(\\theta; y)$ 更为方便，因为对数是严格单调递增函数，最大化 $\\ell(\\theta; y)$ 等价于最大化 $L(\\theta; y)$。\n$$\n\\ell(\\theta; y) = \\ln(L(\\theta; y)) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(R)) - \\frac{1}{2}(y - H\\theta)^T R^{-1} (y - H\\theta)\n$$\n估计受到约束，即 $\\theta$ 必须属于凸多面体集 $\\Theta = \\{\\theta \\in \\mathbb{R}^{n} : C\\theta \\le d\\}$。\n\n约束MLE问题是在此约束下寻找使 $\\ell(\\theta; y)$ 最大化的 $\\theta$。注意到 $\\ell(\\theta; y)$ 表达式中的前两项相对于 $\\theta$ 是常数，最大化 $\\ell(\\theta; y)$ 等价于最小化二次项：\n$$\nJ(\\theta) = \\frac{1}{2}(y - H\\theta)^T R^{-1} (y - H\\theta)\n$$\n这是一个加权最小二乘代价函数。因此，约束MLE问题可以表述为在集合 $\\Theta$ 上的以下优化问题：\n$$\n\\hat{\\theta} = \\arg\\max_{\\theta \\in \\Theta} \\ell(\\theta; y) \\quad \\text{或等价地} \\quad \\hat{\\theta} = \\arg\\min_{\\theta \\in \\Theta} J(\\theta)\n$$\n形式化的优化问题表述为：\n$$\n\\text{最大化} \\quad -\\frac{1}{2}(y - H\\theta)^T R^{-1} (y - H\\theta) \\quad \\text{约束条件为} \\quad C\\theta \\le d\n$$\n\n### 第 2 部分：KKT条件的推导\n\n我们将此问题表述为一个标准的约束最大化问题。设目标函数为 $f(\\theta) = \\ell(\\theta; y)$，约束为 $g_j(\\theta) \\le 0$，其中 $j=1, \\dots, m$。约束 $C\\theta \\le d$ 可以逐分量地写成 $c_j^T\\theta - d_j \\le 0$，其中 $c_j^T$ 是 $C$ 的第 $j$ 行，$d_j$ 是 $d$ 的第 $j$ 个分量。所以，$g_j(\\theta) = c_j^T\\theta - d_j$。\n\n拉格朗日函数 $\\mathcal{L}(\\theta, \\lambda)$ 是通过将由拉格朗日乘子 $\\lambda \\in \\mathbb{R}^{m}$ 加权的约束添加到目标函数中来构成的：\n$$\n\\mathcal{L}(\\theta, \\lambda) = f(\\theta) - \\sum_{j=1}^{m} \\lambda_j g_j(\\theta) = \\ell(\\theta; y) - \\lambda^T(C\\theta - d)\n$$\nKarush-Kuhn-Tucker (KKT) 条件是描述最优解 $\\hat{\\theta}$ 及其相应拉格朗日乘子 $\\hat{\\lambda}$ 的必要条件（在此凸问题中也是充分条件）。这些条件是：\n\n1.  **拉格朗日函数的平稳性**：在解 $\\hat{\\theta}$ 处，拉格朗日函数关于 $\\theta$ 的梯度必须为零。\n    $$\n    \\nabla_{\\theta} \\mathcal{L}(\\hat{\\theta}, \\hat{\\lambda}) = \\nabla_{\\theta}\\ell(\\hat{\\theta}) - C^T \\hat{\\lambda} = 0\n    $$\n    首先，我们计算对数似然的梯度。\n    $$\n    \\nabla_{\\theta}\\ell(\\theta) = \\nabla_{\\theta}\\left[-\\frac{1}{2}(y - H\\theta)^T R^{-1} (y - H\\theta)\\right]\n    $$\n    展开二次型：$(y - H\\theta)^T R^{-1} (y - H\\theta) = y^T R^{-1} y - 2y^T R^{-1} H\\theta + \\theta^T H^T R^{-1} H\\theta$。\n    梯度为：\n    $$\n    \\nabla_{\\theta}\\ell(\\theta) = -\\frac{1}{2}(-2H^T R^{-1} y + 2H^T R^{-1} H\\theta) = H^T R^{-1}y - H^T R^{-1}H\\theta = H^T R^{-1}(y - H\\theta)\n    $$\n    所以平稳性条件是：$H^T R^{-1}(y - H\\hat{\\theta}) - C^T \\hat{\\lambda} = 0$。\n\n2.  **原始可行性**：解 $\\hat{\\theta}$ 必须满足原始约束。\n    $$\n    C\\hat{\\theta} \\le d\n    $$\n\n3.  **对偶可行性**：不等式约束的拉格朗日乘子必须是非负的。\n    $$\n    \\hat{\\lambda} \\ge 0 \\quad (\\text{逐分量})\n    $$\n\n4.  **互补松弛性**：对于每个约束，要么该约束是激活的（等式成立），要么相应的拉格朗日乘子为零。\n    $$\n    \\hat{\\lambda}_j (c_j^T\\hat{\\theta} - d_j) = 0 \\quad \\text{for } j=1, \\dots, m\n    $$\n    这可以紧凑地写为 $\\mathrm{diag}(\\hat{\\lambda})(C\\hat{\\theta} - d) = 0$。\n\n### 第 3 部分：具体案例计算\n\n我们得到一个具体案例，其中 $n=2$，$H=I_2$，$R=I_2$，$y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$，$C = \\begin{pmatrix} -1   0 \\\\ 0   -1 \\\\ 1   1 \\end{pmatrix}$，以及 $d = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\end{pmatrix}$。\n该优化问题是在约束条件下最小化 $J(\\theta) = \\frac{1}{2}(y-\\theta)^T(y-\\theta) = \\frac{1}{2}\\|\\theta-y\\|^2_2$。这在几何上等价于在可行集 $\\Theta$ 中寻找离 $y$ 最近的点 $\\hat{\\theta}$。\n\n约束条件为：\n1.  $-\\theta_1 \\le 0 \\implies \\theta_1 \\ge 0$\n2.  $-\\theta_2 \\le 0 \\implies \\theta_2 \\ge 0$\n3.  $\\theta_1 + \\theta_2 \\le 2$\n\n首先，让我们找到无约束最小值，它就是 $\\theta_{unc} = y = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。我们检查这个点是否在可行集 $\\Theta$ 内：\n1.  $\\theta_1 = 2 \\ge 0$ (满足).\n2.  $\\theta_2 = 1 \\ge 0$ (满足).\n3.  $\\theta_1 + \\theta_2 = 2+1=3 \\le 2$ (违反).\n\n由于无约束解位于可行集之外，约束解 $\\hat{\\theta}$ 必须位于 $\\Theta$ 的边界上。我们使用第2部分推导的KKT条件来找到 $\\hat{\\theta}$。\n\n针对这个具体案例的KKT条件是：\n-   **平稳性**：对于 $H=I_2, R=I_2$，$\\nabla\\ell(\\theta) = y - \\theta$。平稳性条件是 $y - \\hat{\\theta} - C^T\\hat{\\lambda} = 0$。\n    $$\n    \\hat{\\theta} = y - C^T \\hat{\\lambda}\n    $$\n    其中 $y=\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ 且 $C^T=\\begin{pmatrix} -1   0  1 \\\\ 0  -1  1 \\end{pmatrix}$：\n    $$\n    \\begin{pmatrix} \\hat{\\theta}_1 \\\\ \\hat{\\theta}_2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1   0  1 \\\\ 0  -1  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{pmatrix} = \\begin{pmatrix} 2 + \\lambda_1 - \\lambda_3 \\\\ 1 + \\lambda_2 - \\lambda_3 \\end{pmatrix}\n    $$\n-   **原始可行性**：$-\\hat{\\theta}_1 \\le 0$, $-\\hat{\\theta}_2 \\le 0$, $\\hat{\\theta}_1+\\hat{\\theta}_2 \\le 2$。\n-   **对偶可行性**：$\\lambda_1 \\ge 0$, $\\lambda_2 \\ge 0$, $\\lambda_3 \\ge 0$。\n-   **互补松弛性**：\n    $\\lambda_1(-\\hat{\\theta}_1) = 0$\n    $\\lambda_2(-\\hat{\\theta}_2) = 0$\n    $\\lambda_3(\\hat{\\theta}_1 + \\hat{\\theta}_2 - 2) = 0$\n\n我们根据哪些约束是激活的来分析各种情况。由于只有第三个约束被无约束解违反，我们假设对于约束解只有这个约束是激活的。\n\n情况：约束3是激活的，约束1和2是非激活的。\n这意味着 $\\hat{\\theta}_1 + \\hat{\\theta}_2 - 2 = 0$，且根据互补松弛性，$\\lambda_1=0$ 和 $\\lambda_2=0$。我们预期 $\\lambda_3 > 0$。\n\n当 $\\lambda_1=0$ 且 $\\lambda_2=0$ 时，平稳性方程变为：\n$$\n\\hat{\\theta}_1 = 2 - \\lambda_3\n$$\n$$\n\\hat{\\theta}_2 = 1 - \\lambda_3\n$$\n将这些代入激活的约束方程中：\n$$\n(2 - \\lambda_3) + (1 - \\lambda_3) = 2\n$$\n$$\n3 - 2\\lambda_3 = 2 \\implies 2\\lambda_3 = 1 \\implies \\lambda_3 = \\frac{1}{2}\n$$\n$\\lambda_3 = \\frac{1}{2}$ 的值满足对偶可行性 ($\\lambda_3 \\ge 0$)。现在我们求出相应的 $\\hat{\\theta}$：\n$$\n\\hat{\\theta}_1 = 2 - \\frac{1}{2} = \\frac{3}{2}\n$$\n$$\n\\hat{\\theta}_2 = 1 - \\frac{1}{2} = \\frac{1}{2}\n$$\n我们必须验证该解满足所有KKT条件。\n-   **平稳性**：根据构造是满足的。\n-   **原始可行性**：\n    1.  $\\hat{\\theta}_1 = \\frac{3}{2} \\ge 0$ (满足)。\n    2.  $\\hat{\\theta}_2 = \\frac{1}{2} \\ge 0$ (满足)。\n    3.  $\\hat{\\theta}_1 + \\hat{\\theta}_2 = \\frac{3}{2} + \\frac{1}{2} = 2 \\le 2$ (满足，激活)。\n-   **对偶可行性**：$\\hat{\\lambda} = \\begin{pmatrix} 0   0  \\frac{1}{2} \\end{pmatrix}^T \\ge 0$ (满足)。\n-   **互补松弛性**：\n    1.  $\\lambda_1(-\\hat{\\theta}_1) = 0 \\cdot (-\\frac{3}{2}) = 0$ (满足)。\n    2.  $\\lambda_2(-\\hat{\\theta}_2) = 0 \\cdot (-\\frac{1}{2}) = 0$ (满足)。\n    3.  $\\lambda_3(\\hat{\\theta}_1+\\hat{\\theta}_2-2) = \\frac{1}{2}(2-2) = 0$ (满足)。\n\n由于我们找到了一个满足所有KKT条件的点 $(\\hat{\\theta}, \\hat{\\lambda})$，并且该问题是一个凸优化问题（凹目标函数，凸约束），其中Slater条件假设成立，因此该点是唯一的全局最大似然估计。\n\n约束MLE是 $\\hat{\\theta} = \\begin{pmatrix} 3/2 \\\\ 1/2 \\end{pmatrix}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{2}  \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "3402122"}, {"introduction": "最大似然估计的强大渐近性质，如一致性和正态性，依赖于特定的正则性条件。这个问题 [@problem_id:3402145] 展示了一个精心构建的、违反这些条件的场景，从而迫使我们回归第一性原理。通过推导估计量及其真实的渐近分布，你将更深刻地理解最大似然估计的理论基础及其局限性。", "problem": "考虑一个数据同化情景，其中一个业务预报模型精确地预测一个恒定状态 $x^{\\star} \\in \\mathbb{R}$，而观测系统产生独立的测量残差 $r_{1}, r_{2}, \\dots, r_{n}$，定义为 $r_{i} = y_{i} - x^{\\star}$，其中 $y_{i}$ 是观测值。假设残差独立同分布于区间 $[-\\theta, \\theta]$ 上的均匀分布，其中 $\\theta > 0$ 是一个需要推断的未知仪器误差界。因此，残差的统计模型具有密度\n$$\nf(r \\mid \\theta) = \\frac{1}{2\\theta} \\,\\mathbf{1}\\{|r| \\leq \\theta\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。令 $\\theta_{0} > 0$ 表示测量误差的真实未知界。在此模型中，似然函数的支撑集依赖于参数，且对数似然函数在依赖于样本的点上对 $\\theta$ 不可微。这违反了用于推导最大似然估计量 (MLE) 渐近正态性的标准正则性条件，例如对数似然的可微性和与参数无关的支撑集。\n\n任务：\n1. 从独立性下的似然函数定义出发，推导 $\\theta$ 的最大似然估计量 (MLE) $\\hat{\\theta}_{n}$，用观测残差 $r_{1}, \\dots, r_{n}$ 表示。\n2. 证明 $\\hat{\\theta}_{n}$ 是 $\\theta_{0}$ 的一致估计量。\n3. 推导统计量 $Z_{n} = n(\\theta_{0} - \\hat{\\theta}_{n})$ 的精确累积分布函数。\n4. 使用顺序统计量的第一性原理和极限论证，计算对于 $s \\geq 0$ 定义的极限拉普拉斯变换（其等于 $Z_{n}$ 的矩生成函数在负半轴上的取值）：\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\big[\\exp(-s \\, n(\\theta_{0} - \\hat{\\theta}_{n}))\\big].\n$$\n\n你的最终答案必须是关于 $s$ 和 $\\theta_{0}$ 的单个闭式解析表达式。无需四舍五入，且不涉及物理单位。根据你的推导，总结 MLE 的经典 $\\sqrt{n}$-渐近正态性在本例中是否成立，并解释原因，但不要将此讨论包含在最终答案表达式中。", "solution": "我们从独立性下的联合似然函数开始。对于密度为 $f(r \\mid \\theta) = (2\\theta)^{-1} \\mathbf{1}\\{|r| \\leq \\theta\\}$ 的残差 $r_{1}, \\dots, r_{n}$，其联合似然函数为\n$$\nL(\\theta; r_{1}, \\dots, r_{n}) = \\prod_{i=1}^{n} \\frac{1}{2\\theta} \\,\\mathbf{1}\\{|r_{i}| \\leq \\theta\\} = \\frac{1}{(2\\theta)^{n}} \\prod_{i=1}^{n} \\mathbf{1}\\{|r_{i}| \\leq \\theta\\}.\n$$\n指示函数乘积 $\\prod_{i=1}^{n} \\mathbf{1}\\{|r_{i}| \\leq \\theta\\}$ 等于 $1$ 当且仅当 $\\theta \\geq \\max_{1 \\leq i \\leq n} |r_{i}|$，否则为 $0$。将绝对残差的样本最大值记为\n$$\nM_{n} := \\max_{1 \\leq i \\leq n} |r_{i}|.\n$$\n那么似然函数可以重写为\n$$\nL(\\theta; r_{1}, \\dots, r_{n}) = \\frac{1}{(2\\theta)^{n}} \\,\\mathbf{1}\\{\\theta \\geq M_{n}\\}.\n$$\n对于任何固定的数据实现，$L(\\theta)$ 在可行集 $\\{\\theta \\geq M_{n}\\}$ 上是关于 $\\theta$ 的递减函数。因此，最大似然估计量 (MLE) 是可行集中的最小 $\\theta$：\n$$\n\\hat{\\theta}_{n} = M_{n} = \\max_{1 \\leq i \\leq n} |r_{i}|.\n$$\n\n接下来，我们证明 $\\hat{\\theta}_{n}$ 对 $\\theta_{0}$ 的一致性。在真实参数 $\\theta_{0}$ 下，残差 $r_{i}$ 独立同分布于 $[-\\theta_{0}, \\theta_{0}]$ 上。$M_{n}$ 的累积分布函数 (CDF) 可以直接计算。对于 $m \\in [0, \\theta_{0}]$，\n$$\n\\mathbb{P}(M_{n} \\leq m) = \\prod_{i=1}^{n} \\mathbb{P}(|r_{i}| \\leq m) = \\left(\\frac{m}{\\theta_{0}}\\right)^{n}.\n$$\n因此，对于任意 $\\varepsilon > 0$，\n$$\n\\mathbb{P}(|\\hat{\\theta}_{n} - \\theta_0| > \\varepsilon) = \\mathbb{P}(M_{n}  \\theta_0 - \\varepsilon) = \\left(\\frac{\\theta_{0} - \\varepsilon}{\\theta_{0}}\\right)^{n},\n$$\n该式随 $n \\to \\infty$ 呈指数级快速收敛到 0。因此，$\\hat{\\theta}_{n} \\to \\theta_0$ 依概率收敛，即 $\\hat{\\theta}_{n}$ 是 $\\theta_{0}$ 的一致估计量。\n\n我们现在推导 $Z_{n} = n(\\theta_{0} - \\hat{\\theta}_{n}) = n(\\theta_{0} - M_{n})$ 的精确分布。对于 $z \\in [0, n\\theta_{0}]$，我们有\n$$\n\\mathbb{P}(Z_{n} \\geq z) = \\mathbb{P}\\big(n(\\theta_{0} - M_{n}) \\geq z\\big) = \\mathbb{P}\\left(M_{n} \\leq \\theta_{0} - \\frac{z}{n}\\right) = \\left(1 - \\frac{z}{n\\theta_{0}}\\right)^{n}.\n$$\n因此，$Z_{n}$ 的累积分布函数是\n$$\nF_{Z_{n}}(z) = \\mathbb{P}(Z_{n} \\leq z) = 1 - \\left(1 - \\frac{z}{n\\theta_{0}}\\right)^{n}, \\quad \\text{对于 } z \\in [0, n\\theta_{0}],\n$$\n并且当 $z  0$ 时 $F_{Z_{n}}(z) = 0$，当 $z \\geq n\\theta_{0}$ 时 $F_{Z_{n}}(z) = 1$。\n\n我们现在计算 $Z_{n}$ 的极限拉普拉斯变换。对于 $s \\geq 0$，考虑\n$$\n\\Lambda_{n}(s) := \\mathbb{E}\\big[\\exp(-s Z_{n})\\big] = \\int_{0}^{n\\theta_{0}} \\exp(-s z) \\, \\mathrm{d}F_{Z_{n}}(z).\n$$\n使用上面的累积分布函数表达式，对于固定的 $z \\geq 0$ 和大的 n，\n$$\n\\lim_{n \\to \\infty} F_{Z_{n}}(z) = \\lim_{n \\to \\infty} \\left[1 - \\left(1 - \\frac{z}{n\\theta_{0}}\\right)^{n}\\right] = 1 - \\exp\\left(-\\frac{z}{\\theta_{0}}\\right),\n$$\n因为当 $n \\to \\infty$ 时，$\\left(1 - \\frac{x}{n}\\right)^{n} \\to e^{-x}$。这确定了 $Z_{n}$ 的极限分布是率为 $1/\\theta_{0}$ 的指数分布，即其密度为\n$$\nf(z) = \\frac{1}{\\theta_{0}} \\exp\\left(-\\frac{z}{\\theta_{0}}\\right), \\quad z \\geq 0.\n$$\n因此，极限拉普拉斯变换存在且等于该指数分布的拉普拉斯变换：\n$$\n\\lim_{n \\to \\infty} \\mathbb{E}\\big[\\exp(-s Z_{n})\\big] = \\int_{0}^{\\infty} \\exp(-s z) \\,\\frac{1}{\\theta_{0}} \\exp\\left(-\\frac{z}{\\theta_{0}}\\right) \\,\\mathrm{d}z = \\frac{1}{1 + s \\theta_{0}}, \\quad s \\geq 0.\n$$\n\n最后，我们讨论 MLE $\\hat{\\theta}_{n}$ 的经典 $\\sqrt{n}$-渐近正态性的失效。在具有与参数无关的支撑集和可微对数似然的正则参数模型中，最大似然估计量 (MLE) 通常是渐近正态的，并具有 $\\sqrt{n}$ 缩放，即 $\\sqrt{n}(\\hat{\\theta}_{n} - \\theta_{0}) \\Rightarrow \\mathcal{N}(0, I(\\theta_{0})^{-1})$，其中 $I(\\theta_{0})$ 是 Fisher 信息。在这里，支撑集依赖于 $\\theta$，且对数似然在 $\\theta = M_{n}$ 处不可微，违反了正则性条件。推导出的极限表明，正确的缩放因子是 $n$ 而不是 $\\sqrt{n}$，并且极限分布是指数分布而不是高斯分布。因此，在这个反问题数据同化的例子中，由于支撑集依赖于参数和对数似然的不可微性，经典的渐近正态性失效了。", "answer": "$$\\boxed{\\frac{1}{1 + s \\theta_{0}}}$$", "id": "3402145"}, {"introduction": "在现代数据同化中，最大似然估计是从数据中估计模型参数（如噪声协方差）的关键工具。这个实践练习 [@problem_id:3402165] 将指导你推导并实现一种针对卡尔曼滤波器的、基于梯度的优化方法。你将学会计算对数似然函数的精确梯度，这是高效校准状态空间模型的关键一步。", "problem": "考虑一个线性、时不变、离散时间的高斯状态空间模型，其状态维度为 $n$，观测维度为 $m$：\n- 状态动态：$x_{t+1} = A x_t + w_t$，其中 $w_t \\sim \\mathcal{N}(0, Q)$ 且 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。\n- 观测模型：$y_t = H x_t + v_t$，其中 $v_t \\sim \\mathcal{N}(0, R)$ 且 $R \\in \\mathbb{R}^{m \\times m}$ 是对称正定矩阵。\n\n假设使用给定的初始后验均值 $x_{0|0}$ 和协方差 $P_{0|0}$，通过标准的预测-更新递归来运行卡尔曼滤波器。将一步预测均值和协方差表示为 $x_{t|t-1}$ 和 $P_{t|t-1}$。定义新息 $e_t = y_t - H x_{t|t-1}$ 及其协方差 $S_t = H P_{t|t-1} H^\\top + R$。在时间窗口 $t=1,\\dots,T$ 内，负新息对数似然（忽略与 $R$ 无关的加法常数）为：\n$$\nJ(R) = \\frac{1}{2} \\sum_{t=1}^{T} \\left[ \\log \\det S_t + e_t^\\top S_t^{-1} e_t \\right].\n$$\n\n任务：从第一性原理出发，使用卡尔曼滤波器的定义和标准的矩阵微积分恒等式，推导如何计算 $J(R)$ 关于 $R$ 的梯度。可使用以下任一方法：\n- 传播 $x_{t|t-1}$ 和 $P_{t|t-1}$ 关于 $R$ 中各项的导数的离散时间前向敏感性方程，或\n- 通过卡尔曼滤波器递归进行反向模式微分（反向传播）。\n\n您的程序必须实现一种正确的（前向敏感性或反向传播）方法，并根据下面测试套件中指定的 $R$ 的参数化来计算 $J(R)$ 的梯度。\n\n推导的基本依据：\n- 卡尔曼滤波器关于 $x_{t|t-1}$、$P_{t|t-1}$、$x_{t|t}$、$P_{t|t}$、新息 $e_t$、协方差 $S_t$ 和增益 $K_t$ 的预测-更新定义。\n- 矩阵微积分恒等式：$\\mathrm{d} \\log \\det S = \\mathrm{tr}(S^{-1} \\mathrm{d}S)$ 和 $\\mathrm{d} S^{-1} = - S^{-1} (\\mathrm{d}S) S^{-1}$。\n\n角度单位不适用。没有物理单位。\n\n测试套件：\n为以下三个测试案例实现梯度计算。对于每个案例，使用时间 $t=0$ 时的初始后验 $(x_{0|0}, P_{0|0})$，然后执行标准的预测步骤以获得 $(x_{1|0}, P_{1|0})$，再结合 $y_1$ 进行计算，求出 $J(R)$ 关于 $R$ 的指定参数的梯度。\n\n- 案例 1（标量观测，标量参数）：\n  - 维度：$n=2$, $m=1$, $T=5$。\n  - $A = \\begin{bmatrix} 0.9  0.1 \\\\ 0  0.7 \\end{bmatrix}$, $H = \\begin{bmatrix} 1.0  0.5 \\end{bmatrix}$。\n  - $Q = \\begin{bmatrix} 0.05  0 \\\\ 0  0.02 \\end{bmatrix}$。\n  - 初始后验：$x_{0|0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $P_{0|0} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$。\n  - 观测值：$y_1=0.8$, $y_2=0.4$, $y_3=-0.2$, $y_4=0.1$, $y_5=0.0$。\n  - 参数化：$R = r I_1$，其中 $r = 0.5$。\n  - 本案例的输出：一个等于 $\\frac{\\partial J}{\\partial r}$ 的浮点数。\n\n- 案例 2（近简并观测噪声，标量参数）：\n  - 与案例 1 相同，但参数化为 $R = r I_1$，其中 $r = 10^{-6}$。\n  - 本案例的输出：一个等于 $\\frac{\\partial J}{\\partial r}$ 的浮点数。\n\n- 案例 3（二维观测，对角参数化）：\n  - 维度：$n=2$, $m=2$, $T=4$。\n  - $A = \\begin{bmatrix} 1.0  0.2 \\\\ 0  0.9 \\end{bmatrix}$, $H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$。\n  - $Q = \\begin{bmatrix} 0.1  0 \\\\ 0  0.1 \\end{bmatrix}$。\n  - 初始后验：$x_{0|0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $P_{0|0} = \\begin{bmatrix} 0.5  0 \\\\ 0  0.5 \\end{bmatrix}$。\n  - 观测值（列向量）：$y_1=\\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$, $y_2=\\begin{bmatrix} 0.2 \\\\ 0.0 \\end{bmatrix}$, $y_3=\\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}$, $y_4=\\begin{bmatrix} -0.1 \\\\ 0.2 \\end{bmatrix}$。\n  - 参数化：$R = \\mathrm{diag}(r_1, r_2)$，其中 $(r_1, r_2) = (0.05, 0.2)$。\n  - 本案例的输出：一个包含两个浮点数的列表 $[\\frac{\\partial J}{\\partial r_1}, \\frac{\\partial J}{\\partial r_2}]$。\n\n要求：\n- 您的推导必须从卡尔曼滤波器的定义和列出的矩阵微积分恒等式开始，不引用或使用任何预先推导出的梯度公式。\n- 您的实现可以使用前向敏感性传播或通过滤波器递归的反向传播，但必须在浮点运算精度内是精确的。\n- 数值稳定性：在适当之处使用对称正定操作；确保 $S_t$ 被视为可逆的。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。前两个条目是案例 1 和 2 的浮点数，第三个条目是案例 3 的一个包含两个浮点数的列表。例如：\"[g1,g2,[g3_1,g3_2]]\"。\n- 每个浮点数必须四舍五入到六位小数（使用标准的四舍五入到最接近的值，中间值情况根据您的语言默认设置，向远离零的方向或向最接近的偶数取整）。", "solution": "用户希望计算线性高斯状态空间模型的负对数似然函数 $J(R)$ 关于观测噪声协方差矩阵 $R$ 中参数的梯度。推导将基于前向敏感性分析方法，该方法将卡尔曼滤波器状态的导数与状态本身一起传播。\n\n### 1. 预备知识：卡尔曼滤波器和成本函数\n\n首先，我们陈述离散时间卡尔曼滤波器的定义。给定一个状态空间模型：\n- 状态动态：$x_{t+1} = A x_t + w_t$，其中 $w_t \\sim \\mathcal{N}(0, Q)$\n- 观测模型：$y_t = H x_t + v_t$，其中 $v_t \\sim \\mathcal{N}(0, R)$\n\n从初始后验估计 $x_{0|0}$ 及其协方差 $P_{0|0}$ 开始，卡尔曼滤波器递归按以下步骤进行（$t=1, \\dots, T$）：\n\n**预测步骤：**\n$$x_{t|t-1} = A x_{t-1|t-1}$$\n$$P_{t|t-1} = A P_{t-1|t-1} A^\\top + Q$$\n\n**更新步骤：**\n$$e_t = y_t - H x_{t|t-1} \\quad (\\text{新息})$$\n$$S_t = H P_{t|t-1} H^\\top + R \\quad (\\text{新息协方差})$$\n$$K_t = P_{t|t-1} H^\\top S_t^{-1} \\quad (\\text{卡尔曼增益})$$\n$$x_{t|t} = x_{t|t-1} + K_t e_t \\quad (\\text{更新后的状态均值})$$\n$$P_{t|t} = (I - K_t H) P_{t|t-1} \\quad (\\text{更新后的状态协方差})$$\n\n需要关于 $R$ 最小化的负对数似然函数（等价于最大化似然）如下所示：\n$$J(R) = \\frac{1}{2} \\sum_{t=1}^{T} \\left[ \\log \\det S_t + e_t^\\top S_t^{-1} e_t \\right]$$\n\n### 2. 通过前向敏感性分析进行梯度推导\n\n我们的目标是计算 $J(R)$ 关于 $R$ 所依赖的标量参数 $\\theta$ 的梯度。设 $X'$ 表示一个量 $X$ 关于 $\\theta$ 的导数，即 $X' = \\frac{\\mathrm{d}X}{\\mathrm{d}\\theta}$。\n\n总梯度是每个时间步梯度的总和：\n$$\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\sum_{t=1}^{T} \\frac{\\mathrm{d}J_t}{\\mathrm{d}\\theta}, \\quad \\text{其中 } J_t = \\frac{1}{2} \\left[ \\log \\det S_t + e_t^\\top S_t^{-1} e_t \\right]$$\n\n为了找到 $\\frac{\\mathrm{d}J_t}{\\mathrm{d}\\theta}$ 的表达式，我们使用链式法则和标准的矩阵微积分恒等式：$\\mathrm{d} \\log \\det S = \\mathrm{tr}(S^{-1} \\mathrm{d}S)$ 和 $\\mathrm{d} S^{-1} = -S^{-1} (\\mathrm{d}S) S^{-1}$。\n\n$J_t$ 的微分为：\n$$\\mathrm{d}J_t = \\frac{1}{2} \\left[ \\mathrm{d}(\\log \\det S_t) + \\mathrm{d}(e_t^\\top S_t^{-1} e_t) \\right]$$\n$$\\mathrm{d}J_t = \\frac{1}{2} \\left[ \\mathrm{tr}(S_t^{-1} \\mathrm{d}S_t) + (\\mathrm{d}e_t^\\top) S_t^{-1} e_t + e_t^\\top (\\mathrm{d}S_t^{-1}) e_t + e_t^\\top S_t^{-1} \\mathrm{d}e_t \\right]$$\n由于 $e_t^\\top S_t^{-1} \\mathrm{d}e_t$ 是一个标量，它等于其转置 $\\mathrm{d}e_t^\\top S_t^{-1} e_t$。\n$$\\mathrm{d}J_t = \\frac{1}{2} \\left[ \\mathrm{tr}(S_t^{-1} \\mathrm{d}S_t) + 2 e_t^\\top S_t^{-1} \\mathrm{d}e_t - e_t^\\top S_t^{-1} (\\mathrm{d}S_t) S_t^{-1} e_t \\right]$$\n使用迹恒等式 $u^\\top M v = \\mathrm{tr}(v u^\\top M)$：\n$$e_t^\\top S_t^{-1} (\\mathrm{d}S_t) S_t^{-1} e_t = \\mathrm{tr}( S_t^{-1} e_t e_t^\\top S_t^{-1} \\mathrm{d}S_t )$$\n将其代回，我们可以将包含 $\\mathrm{d}S_t$ 的项组合在一起：\n$$\\mathrm{d}J_t = \\mathrm{tr}\\left( \\frac{1}{2} (S_t^{-1} - S_t^{-1} e_t e_t^\\top S_t^{-1}) \\mathrm{d}S_t \\right) + e_t^\\top S_t^{-1} \\mathrm{d}e_t$$\n除以 $\\mathrm{d}\\theta$，我们得到单个时间步的梯度：\n$$\\frac{\\mathrm{d}J_t}{\\mathrm{d}\\theta} = \\mathrm{tr}\\left( \\frac{1}{2} (S_t^{-1} - S_t^{-1} e_t e_t^\\top S_t^{-1}) S_t' \\right) + e_t^\\top S_t^{-1} e_t'$$\n这个表达式依赖于敏感度 $S_t' = \\frac{\\mathrm{d}S_t}{\\mathrm{d}\\theta}$ 和 $e_t' = \\frac{\\mathrm{d}e_t}{\\mathrm{d}\\theta}$。为了计算这些，我们必须通过递归传播卡尔曼滤波器状态的敏感度。\n\n### 3. 敏感性传播方程\n\n我们通过对卡尔曼滤波器方程关于 $\\theta$ 求导来推导敏感性的传播方程。初始敏感度为零：$x'_{0|0} = 0$ 和 $P'_{0|0} = 0$，因为初始状态假设不依赖于 $R$。\n\n**敏感性预测步骤：**\n$$x'_{t|t-1} = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}(A x_{t-1|t-1}) = A x'_{t-1|t-1}$$\n$$P'_{t|t-1} = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}(A P_{t-1|t-1} A^\\top + Q) = A P'_{t-1|t-1} A^\\top$$\n（因为 $A$ 和 $Q$ 不依赖于 $\\theta$）。\n\n**敏感性更新步骤：**\n首先，我们求新息及其协方差的敏感度。注意，观测值 $y_t$ 是常数数据。\n$$e'_t = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}(y_t - H x_{t|t-1}) = -H x'_{t|t-1}$$\n$$S'_t = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}(H P_{t|t-1} H^\\top + R) = H P'_{t|t-1} H^\\top + R'$$\n其中 $R' = \\frac{\\mathrm{d}R}{\\mathrm{d}\\theta}$。\n\n接下来，我们对卡尔曼增益 $K_t$ 求导：\n$$K'_t = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}(P_{t|t-1} H^\\top S_t^{-1}) = P'_{t|t-1} H^\\top S_t^{-1} + P_{t|t-1} H^\\top (S_t^{-1})'$$\n$$K'_t = P'_{t|t-1} H^\\top S_t^{-1} - P_{t|t-1} H^\\top S_t^{-1} S'_t S_t^{-1} = P'_{t|t-1} H^\\top S_t^{-1} - K_t S'_t S_t^{-1}$$\n\n最后，我们求后验状态和协方差的敏感度：\n$$x'_{t|t} = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}(x_{t|t-1} + K_t e_t) = x'_{t|t-1} + K'_t e_t + K_t e'_t$$\n$$P'_{t|t} = \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}((I - K_t H) P_{t|t-1}) = -K'_t H P_{t|t-1} + (I - K_t H) P'_{t|t-1}$$\n\n### 4. 算法实现\n\n对于 $R$ 的每个参数 $\\theta_k$：\n1.  初始化滤波器状态 $x_{0|0}$、$P_{0|0}$ 和敏感性状态 $x'_{0|0}=0$、$P'_{0|0}=0$。\n2.  初始化总梯度 $\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta_k} = 0$。\n3.  计算 $R'_k = \\frac{\\partial R}{\\partial \\theta_k}$。\n4.  对于 $t = 1, \\dots, T$：\n    a. 运行标准卡尔曼滤波器的一个步骤（预测和更新）以计算 $x_{t|t}$、$P_{t|t}$ 及所有中间量（$e_t, S_t, K_t$ 等）。\n    b. 运行敏感性传播方程的一个步骤（预测和更新）以计算 $x'_{t|t}$ 和 $P'_{t|t}$，使用标准滤波器的值。\n    c. 使用第 2 节中推导的公式计算在时间 $t$ 的梯度贡献 $\\frac{\\mathrm{d}J_t}{\\mathrm{d}\\theta_k}$。\n    d. 将此贡献加到总梯度上：$\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta_k} \\mathrel{+}= \\frac{\\mathrm{d}J_t}{\\mathrm{d}\\theta_k}$。\n5.  最终的和就是所求的梯度 $\\frac{\\partial J}{\\partial \\theta_k}$。\n\n这个过程提供了精确的梯度，并在以下代码中实现。为确保数值稳定性，协方差矩阵 $P_{t|t}$ 和 $P'_{t|t}$ 在每次更新后都进行了显式对称化。\n```python\nimport numpy as np\n\ndef custom_round(val: float, ndigits: int = 6) -> float:\n    \"\"\"\n    Rounds a float to a specified number of decimal places, with halfway cases\n    rounded away from zero.\n    \"\"\"\n    if val == 0:\n        return 0.0\n    factor = 10**ndigits\n    return (int(val * factor + 0.5 * np.sign(val))) / factor\n\ndef compute_gradient(case_data: dict) -> list[float] | float:\n    \"\"\"\n    Computes the gradient of the negative log-likelihood J(R) with respect to\n    parameters of R using forward sensitivity analysis.\n    \"\"\"\n    A = case_data['A']\n    H = case_data['H']\n    Q = case_data['Q']\n    x00 = case_data['x00']\n    P00 = case_data['P00']\n    y_obs = case_data['y']\n    T = case_data['T']\n    n = case_data['n']\n    \n    param_values = case_data['param_values']\n    param_names = case_data['param_names']\n    \n    gradients = []\n    \n    for i, _ in enumerate(param_names):\n        # Re-initialize for each parameter's gradient calculation\n        x_post = x00.copy()\n        P_post = P00.copy()\n        \n        x_prime_post = np.zeros_like(x00)\n        P_prime_post = np.zeros_like(P00)\n        \n        total_gradient = 0.0\n        \n        for t_idx in range(T):\n            # Form R and R' for the current parameter\n            if case_data['id'] == 3: # Case 3: R=diag(r1, r2)\n                R_val_arr = np.array(param_values)\n                R = np.diag(R_val_arr)\n                R_prime_diag = np.zeros_like(R_val_arr)\n                R_prime_diag[i] = 1.0\n                R_prime = np.diag(R_prime_diag)\n            else: # Case 1  2: R = r * I\n                r_val = param_values[0]\n                R = np.array([[r_val]])\n                R_prime = np.array([[1.0]])\n\n            y_t = y_obs[t_idx]\n            \n            # --- Standard Filter Step ---\n            # Predict\n            x_prior = A @ x_post\n            P_prior = A @ P_post @ A.T + Q\n            \n            # --- Sensitivity Filter Step ---\n            # Predict\n            x_prime_prior = A @ x_prime_post\n            P_prime_prior = A @ P_prime_post @ A.T\n            \n            # --- Standard Filter Update ---\n            e_t = y_t - H @ x_prior\n            S_t = H @ P_prior @ H.T + R\n            try:\n                S_t_inv = np.linalg.inv(S_t)\n            except np.linalg.LinAlgError:\n                # Add jitter for numerical stability if S_t is singular\n                S_t_inv = np.linalg.inv(S_t + np.eye(S_t.shape[0]) * 1e-12)\n\n            K_t = P_prior @ H.T @ S_t_inv\n            \n            # --- Sensitivity of Update Terms ---\n            e_prime_t = -H @ x_prime_prior\n            S_prime_t = H @ P_prime_prior @ H.T + R_prime\n            K_prime_t = P_prime_prior @ H.T @ S_t_inv - K_t @ S_prime_t @ S_t_inv\n            \n            # --- Gradient Calculation ---\n            grad_Jt_term1 = np.trace(S_t_inv @ S_prime_t)\n            grad_Jt_term2 = -e_t.T @ S_t_inv @ S_prime_t @ S_t_inv @ e_t\n            grad_Jt_term3 = 2 * e_t.T @ S_t_inv @ e_prime_t\n            grad_Jt = 0.5 * (grad_Jt_term1 + grad_Jt_term2 + grad_Jt_term3)\n            \n            total_gradient += grad_Jt.item()\n\n            # --- Update Filter States ---\n            x_post = x_prior + K_t @ e_t\n            P_post_temp = (np.eye(n) - K_t @ H) @ P_prior\n            P_post = 0.5 * (P_post_temp + P_post_temp.T) # Enforce symmetry\n            \n            # --- Update Sensitivity States ---\n            x_prime_post = x_prime_prior + K_prime_t @ e_t + K_t @ e_prime_t\n            P_prime_post_temp = (np.eye(n) - K_t @ H) @ P_prime_prior - K_prime_t @ H @ P_prior\n            P_prime_post = 0.5 * (P_prime_post_temp + P_prime_post_temp.T) # Enforce symmetry\n            \n        gradients.append(custom_round(total_gradient, 6))\n\n    return gradients[0] if len(gradients) == 1 else gradients\n```", "answer": "[-2.257075,-308732.14819,[-12.597554,-4.502083]]", "id": "3402165"}]}