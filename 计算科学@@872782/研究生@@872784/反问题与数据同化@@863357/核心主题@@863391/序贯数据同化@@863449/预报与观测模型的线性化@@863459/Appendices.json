{"hands_on_practices": [{"introduction": "切线性模型和伴随模型是变分资料同化的计算核心。本练习将通过一个实际（尽管简化了）的物理系统——辐射传输模型，为您提供实现这些模型的第一手实践。掌握使用泰勒检验和伴随内积检验来验证代码正确性的方法，对于开发可靠的同化系统至关重要 [@problem_id:3398784]。", "problem": "考虑一个一维、平面平行、纯吸收、非散射的离散层辐射传输正向/观测模型 $H:\\mathbb{R}^N\\to\\mathbb{R}^K$，该模型基于比尔-朗伯定律，通过以下物理上一致的构造定义。设状态向量为 $x\\in\\mathbb{R}^N$，层光学厚度由 $\\tau_i=\\exp(x_i)$ 给出，以确保其为正值。对于每个光谱通道 $k=1,\\dots,K$，定义通道-层光学厚度 $\\Delta\\tau_{k,i}=w_{k,i}\\,\\tau_i$，其中 $w_{k,i}0$ 是已知的通道-层权重系数。设通道 $k$ 的大气层顶太阳源项为 $s_k0$，并设每层 $i$ 有一个取决于通道的发射/源项 $b_{k,i}\\ge 0$，假定其在层内为常数。定义从层 $i$ 顶部到层 $i+1$ 顶部的向上透射率为 $Z_{k,i}=\\exp(-\\Delta\\tau_{k,i})$，到层 $i$ 正上方的累积向上透射率为 $T_{k,i-1}=\\prod_{j=1}^{i-1} Z_{k,j}$ (其中 $T_{k,0}=1$)，层 $i$ 的发射逃逸分数为 $L_{k,i}=1-Z_{k,i}$。那么，通道 $k$ 的大气层顶上行辐射率为\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}.\n$$\n综合所有通道，正向模型为 $H(x)=(y_1,\\dots,y_K)^\\top$。\n\n您必须实现：\n1. 上面定义的正向模型 $H(x)$。\n2. 对于任意方向 $\\delta\\in\\mathbb{R}^N$，使用第一性原理和链式法则计算方向导数（切线性作用）$D H_x(\\delta)$。方向导数的计算必须避免显式地构建完整的雅可比矩阵。\n3. 对于任意 $\\eta\\in\\mathbb{R}^K$，通过推导和编码一个数学上精确的反向模式/伴随累积来计算伴随作用 $A_x(\\eta)=\\big(D H_x\\big)^\\top \\eta$，该累积满足内积恒等式\n$$\n\\langle D H_x(\\delta),\\,\\eta\\rangle_{\\mathbb{R}^K}=\\langle \\delta,\\,A_x(\\eta)\\rangle_{\\mathbb{R}^N}.\n$$\n\n通过泰勒检验进行验证：对于一组正步长 $\\alpha$，验证泰勒余项度量\n$$\nR(\\alpha) = \\frac{\\left\\|H(x+\\alpha \\delta)-H(x)-D H_x(\\alpha \\delta)\\right\\|_2}{\\alpha}\n$$\n与 $\\alpha$ 呈线性关系，即 $R(\\alpha)=\\mathcal{O}(\\alpha)$，这对应于 $\\log R(\\alpha)$ 与 $\\log \\alpha$ 的对数-对数关系图中的斜率接近 $1$。通过对给定的 $\\alpha$ 值进行 $\\log_{10} R(\\alpha)$ 对 $\\log_{10} \\alpha$ 的最小二乘拟合，实现斜率的数值估计。\n\n伴随代码诊断：通过计算相对差异来实现伴随内积检验\n$$\n\\mathrm{err}_{\\mathrm{adj}}=\\frac{\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle - \\langle \\delta,\\,A_x(\\eta)\\rangle\\right|}{\\max\\left(10^{-16},\\,\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle\\right|+\\left|\\langle \\delta,\\,A_x(\\eta)\\rangle\\right|\\right)}.\n$$\n此外，实现一个“错误的”伴随变体，该变体在伴随累积中忽略了太阳项的贡献，并证明对于这个错误的变体，内积恒等式不成立，而正向/切线性模型的泰勒检验仍然正确。如果泰勒缩放关系失效（斜率远非 $1$），则报告伴随相对误差，以帮助诊断伴随是否实现不正确。\n\n使用以下参数集测试套件，每个测试集提供 $(N,K,w,s,b,x,\\delta,\\eta,\\text{faulty})$，其中 $N$ 是层数， $K$ 是通道数，$w\\in\\mathbb{R}^{K\\times N}$，$s\\in\\mathbb{R}^K$，$b\\in\\mathbb{R}^{K\\times N}$，$x\\in\\mathbb{R}^N$，$\\delta\\in\\mathbb{R}^N$，以及 $\\eta\\in\\mathbb{R}^K$。所有量均为无量纲。泰勒检验的步长列表为 $\\alpha\\in\\{10^{-1},\\,5\\times 10^{-2},\\,2.5\\times 10^{-2},\\,1.25\\times 10^{-2},\\,6.25\\times 10^{-3}\\}$：\n\n- 案例1（通用“理想路径”，正确的伴随模型）：\n    - $N=6$, $K=4$。\n    - $w_{k,i}=0.2+0.05\\,k+0.03\\,i$ 对于 $k\\in\\{1,2,3,4\\}$, $i\\in\\{1,\\dots,6\\}$。\n    - $s_k=1.0+0.2\\,k$。\n    - $b_{k,i}=0.1+0.05\\,i+0.03\\,k$。\n    - $x=[-0.8,\\,-0.3,\\,0.2,\\,0.7,\\,-0.5,\\,0.1]^\\top$。\n    - $\\delta=[0.1,\\,-0.2,\\,0.05,\\,-0.1,\\,0.2,\\,-0.05]^\\top$。\n    - $\\eta=[0.3,\\,-0.5,\\,0.7,\\,-0.2]^\\top$。\n    - faulty = False。\n\n- 案例2（边界情况：非常小的光学厚度，正确的伴随模型）：\n    - $N=6$, $K=4$。\n    - $w_{k,i}=0.05+0.02\\,k+0.01\\,i$。\n    - $s_k=0.2+0.1\\,k$。\n    - $b_{k,i}=0.02+0.03\\,i+0.01\\,k$。\n    - $x=[-5.5,\\,-4.8,\\,-4.2,\\,-3.9,\\,-3.5,\\,-3.2]^\\top$。\n    - $\\delta=[0.02,\\,-0.01,\\,0.03,\\,-0.02,\\,0.01,\\,-0.03]^\\top$。\n    - $\\eta=[1.0,\\,-0.8,\\,0.6,\\,-0.4]^\\top$。\n    - faulty = False。\n\n- 案例3（边缘情况：非常大的光学厚度，正确的伴随模型）：\n    - $N=6$, $K=4$。\n    - $w_{k,i}=0.7+0.1\\,k+0.2\\,i$。\n    - $s_k=1.0+0.3\\,k$。\n    - $b_{k,i}=0.4+0.1\\,i+0.05\\,k$。\n    - $x=[3.0,\\,3.5,\\,4.0,\\,4.5,\\,5.0,\\,3.8]^\\top$。\n    - $\\delta=[-0.05,\\,0.1,\\,-0.08,\\,0.06,\\,-0.04,\\,0.02]^\\top$。\n    - $\\eta=[-0.5,\\,0.4,\\,-0.3,\\,0.2]^\\top$。\n    - faulty = False。\n\n- 案例4（诊断：与案例1相同，但使用错误的伴随模型）：\n    - 使用案例1的参数并设置 faulty = True。\n\n对于每个案例，计算：\n- 使用最小二乘法对给定的 $\\alpha$ 值计算 $\\log_{10} R(\\alpha)$ 对 $\\log_{10} \\alpha$ 的估计斜率（一个浮点数）。\n- 一个布尔值，指示泰勒检验是否通过，定义为斜率在区间 $[0.9,\\,1.1]$ 内。\n- 伴随内积相对误差（一个浮点数），当 faulty = False 时从精确的伴随模型计算，当 faulty = True 时从错误的伴随模型计算。\n- 一个布尔值，指示伴随内积检验是否通过，定义为 $\\mathrm{err}_{\\mathrm{adj}}  10^{-10}$。\n\n您的程序应生成单行输出，其中包含所有案例的结果，形式为一个逗号分隔的列表，列表中的每个元素是对应案例的结果列表，每个结果列表按 `[slope, taylor_pass_boolean, adjoint_relative_error, adjoint_pass_boolean]` 的顺序排列，并用方括号括起来。例如，输出必须具有以下形式\n$$\n\\texttt{[[slope\\_1,True,err\\_1,True],[slope\\_2,True,err\\_2,True],[slope\\_3,True,err\\_3,True],[slope\\_4,True,err\\_4,False]]}\n$$\n其中布尔值和浮点数反映您计算出的值。不需要物理单位，也不出现角度；所有量均为无量纲。程序必须是自包含的，无需任何输入。", "solution": "问题陈述经评估为有效。它在科学上基于辐射传输原理，在数学上是适定的，并以客观、正式的语言表达。它为一个非平凡的计算任务提供了一个完整且一致的设置，该任务涉及实现一个正向模型、其切线性对应部分以及相应的伴随模型，并包括标准的验证程序。\n\n解决方案首先从第一性原理推导切线性模型和伴随模型的数学表达式，然后概述它们的实现。\n\n设状态向量为 $x \\in \\mathbb{R}^N$，测量向量为 $y \\in \\mathbb{R}^K$。正向模型是一个函数 $H:\\mathbb{R}^N \\to \\mathbb{R}^K$，将状态映射到测量值。状态向量的分量 $x_i$ 通过变换 $\\tau_i = \\exp(x_i)$ 与层光学厚度 $\\tau_i$ 相关。这确保了 $\\tau_i  0$。\n\n通道 $k$ 的上行辐射率 $y_k$ 由下式给出：\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}\n$$\n其中中间量定义如下：\n- 通道-层光学厚度：$\\Delta\\tau_{k,i} = w_{k,i}\\,\\tau_i = w_{k,i} \\exp(x_i)$\n- 层向上透射率：$Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$\n- 层发射逃逸分数：$L_{k,i} = 1 - Z_{k,i}$\n- 到层 $i$ 正上方的累积向上透射率：$T_{k,i} = \\prod_{j=1}^{i} Z_{k,j}$，基准情况为 $T_{k,0} = 1$。因此，主方程中的项 $T_{k,i-1}$ 在 $i=1$ 时是一个空积，正确地计算为 $1$。\n\n**1. 正向模型实现**\n正向模型 $H(x)$ 的实现方式是，为每个通道 $k=1,\\dots,K$ 和每个层 $i=1,\\dots,N$ 计算一系列中间变量：\n1.  计算层光学厚度 $\\tau_i = \\exp(x_i)$，其中 $i=1,\\dots,N$。\n2.  对于每个通道 $k$，计算通道-层光学厚度 $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$。\n3.  对于每个通道 $k$，计算层透射率 $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$。\n4.  对于每个通道 $k$，递归计算累积透射率 $T_{k,i}$：$T_{k,0}=1$ 且 $T_{k,i} = T_{k,i-1} Z_{k,i}$，其中 $i=1,\\dots,N$。\n5.  然后使用主方程计算辐射率 $y_k$，该方程可以用累积透射率表示为 $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$。\n所有中间量（$\\tau_i, \\Delta\\tau_{k,i}, Z_{k,i}, T_{k,i}$）都被存储起来，用于切线性和伴随计算。\n\n**2. 切线性模型：方向导数 $D H_x(\\delta)$**\n切线性模型计算 H 的雅可比矩阵对扰动向量 $\\delta \\in \\mathbb{R}^N$ 的作用，记为 $\\delta y = DH_x(\\delta)$，而无需显式地构建雅可比矩阵。这是通过在正向模型的运算链中传播初始扰动 $\\delta x = \\delta$ 来实现的。令 $\\delta v$ 表示变量 $v$ 的扰动。\n\n1.  层光学厚度的扰动：$\\delta\\tau_i = \\frac{d\\tau_i}{dx_i}\\delta x_i = \\exp(x_i)\\delta x_i = \\tau_i \\delta x_i$。\n2.  通道-层光学厚度的扰动：$\\delta(\\Delta\\tau_{k,i}) = w_{k,i}\\delta\\tau_i = w_{k,i}\\tau_i\\delta x_i = \\Delta\\tau_{k,i}\\delta x_i$。\n3.  层透射率的扰动：$\\delta Z_{k,i} = \\frac{dZ_{k,i}}{d\\Delta\\tau_{k,i}}\\delta(\\Delta\\tau_{k,i}) = -\\exp(-\\Delta\\tau_{k,i})\\delta(\\Delta\\tau_{k,i}) = -Z_{k,i}\\delta(\\Delta\\tau_{k,i})$。\n4.  累积透射率的扰动：由 $T_{k,i} = T_{k,i-1}Z_{k,i}$，根据乘法法则得到 $\\delta T_{k,i} = \\delta T_{k,i-1}Z_{k,i} + T_{k,i-1}\\delta Z_{k,i}$。此式从 $\\delta T_{k,0} = 0$ 开始递归计算。\n5.  最终的辐射率扰动为 $\\delta y_k = s_k \\delta T_{k,N} + \\sum_{i=1}^N b_{k,i}(\\delta T_{k,i-1} - \\delta T_{k,i})$。\n\n这种扰动的正向传播产生向量 $\\delta y = (\\delta y_1, \\dots, \\delta y_K)^\\top$。\n\n**3. 伴随模型：转置作用 $A_x(\\eta) = (D H_x)^\\top \\eta$**\n伴随模型计算雅可比矩阵的转置对向量 $\\eta \\in \\mathbb{R}^K$ 的作用。这是通过使用反向模式微分来实现的，该方法将敏感度从输出反向传播到输入。令 $\\bar{v}$ 表示最终标量目标函数 $L = \\langle y, \\eta \\rangle = \\sum_k y_k \\eta_k$ 关于中间变量 $v$ 的导数。最终结果是导数向量 $\\bar{x}_j = \\partial L / \\partial x_j$。\n\n算法过程如下：\n1.  将所有伴随变量（$\\bar{\\tau}_i, \\bar{\\Delta\\tau}_{k,i}, \\dots$）初始化为 0。伴随模型的输入是 $\\bar{y}_k = \\eta_k$，其中 $k=1,\\dots,K$。\n2.  计算按通道 $k$ 进行，对 $\\bar{x}$ 的贡献被累积。对于每个通道 $k$：\n    a. 初始化一个大小为 $N+1$ 的伴随累积透射率数组 $\\bar{T}_k$ 为零。\n    b. 对于 $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$，对 $\\bar{T}_k$ 的伴随贡献是：\n       - $\\bar{T}_{k,N} \\mathrel{+}= \\eta_k s_k$。对于错误的伴随模型，此步骤被省略。\n       - 对于 $i=1,\\dots,N$：$\\bar{T}_{k,i-1} \\mathrel{+}= \\eta_k b_{k,i}$ 且 $\\bar{T}_{k,i} \\mathrel{-}= \\eta_k b_{k,i}$。\n    c. 通过累积透射率计算反向传播敏感度。对于 $i=N,\\dots,1$：\n       从 $T_{k,i} = T_{k,i-1}Z_{k,i}$，我们有：\n       - $\\bar{Z}_{k,i} \\mathrel{+}= \\bar{T}_{k,i} T_{k,i-1}$\n       - $\\bar{T}_{k,i-1} \\mathrel{+}= \\bar{T}_{k,i} Z_{k,i}$\n    d. 将敏感度从 $\\bar{Z}_{k,i}$ 传播到 $\\bar{\\Delta\\tau}_{k,i}$，其中 $i=1,\\dots,N$：\n       从 $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$，我们有 $\\bar{\\Delta\\tau}_{k,i} \\mathrel{+}= \\bar{Z}_{k,i} (-Z_{k,i})$。\n3.  在遍历所有通道 $k$ 之后，累积对 $\\bar{\\tau}_i$ 的贡献：\n    从 $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$，我们有 $\\bar{\\tau}_i \\mathrel{+}= \\sum_k \\bar{\\Delta\\tau}_{k,i} w_{k,i}$。\n4.  最后，将敏感度从 $\\bar{\\tau}_i$ 传播到 $\\bar{x}_i$：\n    从 $\\tau_i = \\exp(x_i)$，我们有 $\\bar{x}_i = \\bar{\\tau}_i \\exp(x_i) = \\bar{\\tau}_i \\tau_i$。\n\n最终得到的向量 $\\bar{x}$ 就是所求的伴随作用 $A_x(\\eta)$。\n\n**4. 验证程序**\n- **泰勒检验：** 通过验证模型是一阶精确的来检查切线性模型的质量。泰勒余项度量 $R(\\alpha) = \\|H(x+\\alpha\\delta) - H(x) - \\alpha DH_x(\\delta)\\|_2 / \\alpha$ 应该是 $\\mathcal{O}(\\alpha)$。这意味着 $R(\\alpha)$ 对 $\\alpha$ 的对数-对数图的斜率为 $1$。我们通过线性最小二乘回归来估计这个斜率。斜率在 $[0.9, 1.1]$ 范围内表示检验通过。\n- **伴随检验：** 伴随模型的正确性通过基本内积恒等式 $\\langle DH_x(\\delta), \\eta \\rangle = \\langle \\delta, (DH_x)^\\top\\eta \\rangle$ 来验证。计算该恒等式两边的相对差异。低于 $10^{-10}$ 的值表示检验通过，确认伴随代码是切线性代码的真正转置。忽略太阳项的错误伴随模型预计将无法通过此检验。", "answer": "```python\nimport numpy as np\n\nclass RadiativeTransferModel:\n    \"\"\"\n    Implements a discrete-layer radiative transfer model and its derivatives.\n    \"\"\"\n    def __init__(self, N, K, w, s, b):\n        self.N = N\n        self.K = K\n        self.w = w\n        self.s = s\n        self.b = b\n\n        # Pre-allocate for intermediate variables\n        self.tau = np.zeros(N)\n        self.Delta_tau = np.zeros((K, N))\n        self.Z = np.zeros((K, N))\n        self.T = np.zeros((K, N + 1))\n\n    def forward(self, x):\n        \"\"\"Computes the forward model H(x) and stores intermediate values.\"\"\"\n        self.tau = np.exp(x)\n        self.Delta_tau = self.w * self.tau\n        self.Z = np.exp(-self.Delta_tau)\n        \n        self.T[:, 0] = 1.0\n        for i in range(self.N):\n            self.T[:, i + 1] = self.T[:, i] * self.Z[:, i]\n            \n        y = self.s * self.T[:, self.N]\n        for i in range(self.N):\n            # L_ki * T_{k,i-1} = (1 - Z_ki) * T_{k,i-1} = T_{k,i-1} - T_{k,i}\n            y += self.b[:, i] * (self.T[:, i] - self.T[:, i + 1])\n        \n        return y\n\n    def tangent_linear(self, x, delta_x):\n        \"\"\"Computes the directional derivative DH_x(delta_x).\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n\n        delta_tau = self.tau * delta_x\n        delta_Delta_tau = self.w * delta_tau\n        delta_Z = -self.Z * delta_Delta_tau\n        \n        delta_T = np.zeros((self.K, self.N + 1))\n        # delta_T[:, 0] is always 0\n        for i in range(self.N):\n            delta_T[:, i + 1] = delta_T[:, i] * self.Z[:, i] + self.T[:, i] * delta_Z[:, i]\n\n        delta_y = self.s * delta_T[:, self.N]\n        for i in range(self.N):\n            delta_y += self.b[:, i] * (delta_T[:, i] - delta_T[:, i + 1])\n            \n        return delta_y\n\n    def adjoint(self, x, eta, faulty=False):\n        \"\"\"Computes the adjoint action (DH_x)^T * eta.\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n        \n        bar_tau = np.zeros(self.N)\n\n        for k in range(self.K):\n            bar_T_k = np.zeros(self.N + 1)\n            eta_k = eta[k]\n\n            # Contributions from y_k = s_k*T_kN + sum_i b_ki*(T_{k,i-1}-T_{k,i})\n            if not faulty:\n                bar_T_k[self.N] += eta_k * self.s[k]\n            \n            for i in range(self.N): # 1-based index i=1,...,N\n                i_idx = i\n                bar_T_k[i_idx] += eta_k * self.b[k, i_idx]\n                bar_T_k[i_idx + 1] -= eta_k * self.b[k, i_idx]\n\n            bar_Delta_tau_k = np.zeros(self.N)\n            # Propagate backwards from T\n            for i in range(self.N, 0, -1): # 1-based index i=N,...,1\n                i_idx = i - 1\n                \n                # From T_k,i = T_{k,i-1} * Z_{k,i}\n                bar_Z_ki = bar_T_k[i] * self.T[k, i_idx]\n                bar_T_k[i_idx] += bar_T_k[i] * self.Z[k, i_idx]\n\n                # From Z_k,i = exp(-Delta_tau_{k,i})\n                bar_Delta_tau_k[i_idx] += bar_Z_ki * (-self.Z[k, i_idx])\n\n            # Accumulate contributions to bar_tau for channel k\n            # From Delta_tau_ki = w_ki * tau_i\n            bar_tau += bar_Delta_tau_k * self.w[k, :]\n\n        # Final propagation from tau to x\n        # From tau_i = exp(x_i)\n        bar_x = bar_tau * self.tau\n        \n        return bar_x\n\ndef solve():\n    alpha_steps = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3])\n\n    # --- Test Case Data Generation ---\n    test_cases_params = []\n    \n    # Case 1\n    N, K = 6, 4\n    w = np.array([[0.2 + 0.05*(k+1) + 0.03*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.2*(k+1) for k in range(K)])\n    b = np.array([[0.1 + 0.05*(i+1) + 0.03*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-0.8, -0.3, 0.2, 0.7, -0.5, 0.1])\n    delta = np.array([0.1, -0.2, 0.05, -0.1, 0.2, -0.05])\n    eta = np.array([0.3, -0.5, 0.7, -0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 2\n    N, K = 6, 4\n    w = np.array([[0.05 + 0.02*(k+1) + 0.01*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([0.2 + 0.1*(k+1) for k in range(K)])\n    b = np.array([[0.02 + 0.03*(i+1) + 0.01*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-5.5, -4.8, -4.2, -3.9, -3.5, -3.2])\n    delta = np.array([0.02, -0.01, 0.03, -0.02, 0.01, -0.03])\n    eta = np.array([1.0, -0.8, 0.6, -0.4])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 3\n    N, K = 6, 4\n    w = np.array([[0.7 + 0.1*(k+1) + 0.2*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.3*(k+1) for k in range(K)])\n    b = np.array([[0.4 + 0.1*(i+1) + 0.05*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([3.0, 3.5, 4.0, 4.5, 5.0, 3.8])\n    delta = np.array([-0.05, 0.1, -0.08, 0.06, -0.04, 0.02])\n    eta = np.array([-0.5, 0.4, -0.3, 0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 4 (same as 1, but faulty adjoint)\n    params1_copy = test_cases_params[0].copy()\n    params1_copy['faulty'] = True\n    test_cases_params.append(params1_copy)\n    \n    # --- Main Loop ---\n    results = []\n    for params in test_cases_params:\n        p = params\n        model = RadiativeTransferModel(p['N'], p['K'], p['w'], p['s'], p['b'])\n        \n        # --- Taylor Test ---\n        y0 = model.forward(p['x'])\n        dy = model.tangent_linear(p['x'], p['delta'])\n        \n        remainders = []\n        for alpha in alpha_steps:\n            y_alpha = model.forward(p['x'] + alpha * p['delta'])\n            remainder_norm = np.linalg.norm(y_alpha - y0 - alpha * dy)\n            remainders.append(remainder_norm / alpha)\n        \n        log_alphas = np.log10(alpha_steps)\n        log_remainders = np.log10(remainders)\n        slope = np.polyfit(log_alphas, log_remainders, 1)[0]\n        taylor_pass = 0.9 = slope = 1.1\n\n        # --- Adjoint Test ---\n        adj = model.adjoint(p['x'], p['eta'], faulty=p['faulty'])\n        \n        ip1 = np.dot(dy, p['eta'])\n        ip2 = np.dot(p['delta'], adj)\n        \n        adjoint_err = np.abs(ip1 - ip2) / max(1e-16, np.abs(ip1) + np.abs(ip2))\n        adjoint_pass = adjoint_err  1e-10\n        \n        results.append([slope, taylor_pass, adjoint_err, adjoint_pass])\n\n    # --- Formatting Output ---\n    case_results_str = [f'[{r[0]},{r[1]},{r[2]},{r[3]}]' for r in results]\n    final_str = f\"[{','.join(case_results_str)}]\"\n    print(final_str)\n\nsolve()\n```", "id": "3398784"}, {"introduction": "线性化是一种近似，其准确性取决于系统的动力学特性。本练习使用经典的逻辑斯蒂映射，从“如何实现”转向“何时适用”，探索混沌行为（即邻近轨迹的快速发散）如何直接限制线性模型的“有效半径” [@problem_id:3398752]。这项练习旨在建立一个关键的直觉，解释为什么线性化在高度敏感或混沌的系统中可能会失效。", "problem": "考虑由逻辑斯谛映射（logistic map）给出的离散时间非线性预测模型，该模型由函数 $f(x) = r\\,x\\,\\left(1-x\\right)$ 定义，其中参数 $r \\in (0,4]$，预测递归为 $x_{t+1} = f(x_t)$，其中时间 $t \\geq 0$ 为整数。观测模型为恒等模型，因此时间 $t$ 的观测值为 $y_t = x_t$。我们感兴趣的逆问题是，通过最小化一个最小二乘目标函数，从一个观测窗口 $\\{y_1,\\dots,y_T\\}$ 中估计初始条件 $x_0$。该方法使用一种 Gauss–Newton 方法，此方法基于对预测和观测模型在当前迭代值（即初始条件的当前估计值）附近进行的线性化。\n\n您的任务是将局部拉伸率（由局部 Lyapunov 指数所量化）与一阶线性化的有效半径联系起来，并确定一个 Gauss–Newton 步长预计会发散的阈值。您的推导和计算必须遵循以下约束。\n\n1) 在分析 $f$ 的线性化误差时，仅使用带有 Lagrange 余项形式的 Taylor 定理和链式法则作为基本工具。具体来说，考虑单步 Taylor 展开式\n$$\nf(x_t + \\delta x_t) = f(x_t) + f'(x_t)\\,\\delta x_t + \\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2,\n$$\n其中 $\\xi_t$ 位于 $x_t$ 和 $x_t + \\delta x_t$ 之间。在每个时间步 $t = 0,1,\\dots,T-1$ 上施加一个相对小性条件，即余项的量级受限于一个固定比例 $\\alpha \\in (0,1)$ 乘以线性项的量级。根据此条件和跨时间的敏感度链式法则，推导出一个形式为\n$$\n|\\delta x_0| \\le \\rho(r,\\alpha; x_0,\\dots,x_{T-1}),\n$$\n的严格充分界，其中 $\\rho$ 是一个可计算的有效半径，它保证在 $T$ 个步长上的累积一阶线性化保持在规定的相对误差之内。使用 $f$ 的导数以及这些导数沿着当前迭代值的线性化轨迹的乘积来表示 $\\rho$。然后，将敏感度的指数增长或衰减与有限时间局部 Lyapunov 指数联系起来，对于轨迹 $\\{x_k\\}_{k=0}^{t-1}$，该指数定义为\n$$\n\\lambda_t(x_0) = \\frac{1}{t} \\sum_{k=0}^{t-1} \\log \\left| f'(x_k) \\right|.\n$$\n解释 $\\rho$ 如何随 $\\exp\\!\\left(-\\sum_{k=0}^{t-1} \\log|f'(x_k)|\\right)$ 缩放，并因此说明它如何依赖于局部 Lyapunov 指数。\n\n2) 将您的推导具体应用于逻辑斯谛映射，其中 $f'(x) = r(1-2x)$ 且 $f''(x) = -2r$，并提供一个仅用 $r$、$\\alpha$、线性化轨迹状态 $\\{x_t\\}$ 以及前向敏感度乘积 $\\prod_{k=0}^{t-1} f'(x_k)$ 表示的半径 $\\rho$ 的显式可计算表达式。\n\n3) 对下面列出的每个测试用例，实现一次单一的 Gauss–Newton 迭代，以从 $T$ 个步长的恒等观测中估计初始条件 $x_0$。使用当前迭代值 $x_0^{(0)}$ 和从真实初始条件 $x_0^\\star$ 生成的无噪声观测。具体地，令残差为 $r_t = x_{t}(x_0^{(0)}) - y_t$（对于 $t=1,\\dots,T$），令雅可比项为 $J_t = \\frac{\\partial x_t}{\\partial x_0}\\big|_{x_0^{(0)}}$（对于 $t=1,\\dots,T$）。使用正规方程计算此一维最小二乘问题的高斯-牛顿（Gauss–Newton）步长 $\\delta x_0$，并更新 $x_0^{(1)} = x_0^{(0)} + \\delta x_0$。将目标函数定义为\n$$\n\\Phi(x_0) = \\tfrac{1}{2}\\sum_{t=1}^{T} \\left(x_t(x_0) - y_t\\right)^2,\n$$\n并确定两个布尔值：\n- 一个预测发散指标，如果 $|\\delta x_0|$ 超过推导出的半径 $\\rho$，则为 $\\mathrm{True}$。\n- 一个实际发散指标，如果 $\\Phi(x_0^{(1)})  \\Phi(x_0^{(0)})$ 或 $x_0^{(1)} \\notin [0,1]$，则为 $\\mathrm{True}$。\n\n4) 使用以下测试套件，其中每个用例是一个元组 $(r, T, x_0^\\star, x_0^{(0)}, \\alpha)$：\n- 用例 A: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.2,\\,5,\\,0.61,\\,0.60,\\,0.1\\,)$\n- 用例 B: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.9,\\,5,\\,0.61,\\,0.60,\\,0.1\\,)$\n- 用例 C: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.9,\\,10,\\,0.61,\\,0.609,\\,0.1\\,)$\n- 用例 D: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,2.5,\\,4,\\,0.20,\\,0.80,\\,0.1\\,)$\n- 用例 E: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.5,\\,6,\\,0.5005,\\,0.48,\\,0.1\\,)$\n\n对每个用例，计算：\n- 在任务 1–2 中推导出的、沿着关于 $x_0^{(0)}$ 的线性化轨迹评估的半径 $\\rho$。\n- Gauss–Newton 步长的绝对值 $|\\delta x_0|$。\n- 预测发散布尔值。\n- 实际发散布尔值。\n\n5) 您的程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。对于每个用例，按此顺序附加四个值：$\\rho$、 $|\\delta x_0|$、预测发散布尔值、实际发散布尔值。所有浮点输出必须四舍五入到六位小数。因此，最终输出必须是一个包含 20 个条目的扁平列表，按用例 A 到 E 的顺序排列，例如 $[\\rho_A,|\\delta_A|,\\mathrm{pred}_A,\\mathrm{act}_A,\\rho_B,|\\delta_B|,\\mathrm{pred}_B,\\mathrm{act}_B,\\dots]$。\n\n不涉及物理单位。不涉及角度。如果出现百分比，必须表示为小数，但此处不需要。确保您的实现是自包含的、确定性的，并且不需要用户输入。", "solution": "我们从预测模型、观测模型和线性化框架的核心定义开始。模型是逻辑斯谛映射 $f(x) = r\\,x\\,(1-x)$，给出预测序列 $x_{t+1} = f(x_t)$。观测模型是恒等模型，$y_t = x_t$。逆问题是从 $\\{y_1,\\dots,y_T\\}$ 中对 $x_0$ 进行最小二乘估计，在一维情况下，这会产生一个由雅可比项 $J_t = \\frac{\\partial x_t}{\\partial x_0}$ 计算得出的 Gauss–Newton 步长。\n\n原理 1：带有余项的 Taylor 定理和每步相对误差。对于时间 $t$ 的单步，带有 Lagrange 余项的 Taylor 定理给出\n$$\nf(x_t + \\delta x_t) = f(x_t) + f'(x_t)\\,\\delta x_t + \\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2,\n$$\n对于某个位于 $x_t$ 和 $x_t + \\delta x_t$ 之间的 $\\xi_t$。为保证一阶线性近似主导二阶项，我们施加一个充分条件，即对于固定的 $\\alpha \\in (0,1)$，\n$$\n\\left|\\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2\\right| \\le \\alpha\\,\\left| f'(x_t)\\,\\delta x_t \\right|, \\quad \\text{对每个 } t=0,1,\\dots,T-1.\n$$\n消去一个因子 $|\\delta x_t|$（假设 $\\delta x_t \\neq 0$；$\\delta x_t = 0$ 的情况是平凡安全的），得到\n$$\n\\frac{1}{2}\\,|f''(\\xi_t)|\\,|\\delta x_t| \\le \\alpha\\,|f'(x_t)|.\n$$\n一个不需要知道 $\\xi_t$ 的充分条件可以通过在 $x_t$ 处用一个一致界来上界 $|f''(\\xi_t)|$ 来获得，或者对于包含 $x_t$ 和 $x_t + \\delta x_t$ 的区间 $I_t$，使用 $|f''(\\xi_t)| \\le \\sup_{z \\in I_t} |f''(z)|$。对于逻辑斯谛映射，$f''(x) = -2r$ 是常数，所以 $|f''(\\xi_t)| = 2r$ 处处成立。因此，每步条件简化为\n$$\n|\\delta x_t| \\le \\frac{2\\alpha\\,|f'(x_t)|}{|f''(\\xi_t)|} = \\frac{2\\alpha\\,|r(1-2x_t)|}{2r} = \\alpha\\,|1-2x_t|.\n$$\n\n原理 2：敏感度的链式法则。跨越 $t$ 步，扰动传播到一阶满足\n$$\n\\delta x_t \\approx \\left(\\prod_{k=0}^{t-1} f'(x_k)\\right)\\,\\delta x_0 \\equiv J_t\\,\\delta x_0,\n$$\n约定 $J_0 = 1$。将此与每步充分条件相结合，对每个 $t=0,1,\\dots,T-1$ 得出界\n$$\n|J_t|\\,|\\delta x_0| \\le \\alpha\\,|1-2x_t| \\quad \\Rightarrow \\quad |\\delta x_0| \\le \\frac{\\alpha\\,|1-2x_t|}{|J_t|}.\n$$\n因此，通过取最小值可以获得整个窗口上的一个充分一致界：\n$$\n\\rho(r,\\alpha; x_0,\\dots,x_{T-1}) \\equiv \\min_{0 \\le t \\le T-1} \\frac{\\alpha\\,|1-2x_t|}{\\left|\\prod_{k=0}^{t-1} f'(x_k)\\right|}.\n$$\n这便是在围绕当前迭代值的轨迹 $\\{x_t\\}$ 进行线性化时，具有容差 $\\alpha$ 的一阶线性化的可计算有效半径。\n\n原理 3：与局部 Lyapunov 指数的关系。将沿线性化轨迹的有限时间局部 Lyapunov 指数定义为\n$$\n\\lambda_t(x_0) = \\frac{1}{t} \\sum_{k=0}^{t-1} \\log |f'(x_k)|, \\quad t \\ge 1.\n$$\n注意到 $\\left|\\prod_{k=0}^{t-1} f'(x_k)\\right| = \\exp\\!\\left(\\sum_{k=0}^{t-1} \\log |f'(x_k)|\\right) = \\exp\\!\\left(t\\,\\lambda_t(x_0)\\right)$，我们得到缩放关系\n$$\n\\rho_t \\equiv \\frac{\\alpha\\,|1-2x_t|}{|J_t|} = \\alpha\\,|1-2x_t|\\,\\exp\\!\\left(-\\sum_{k=0}^{t-1} \\log |f'(x_k)|\\right) = \\alpha\\,|1-2x_t|\\,\\exp\\!\\left(-t\\,\\lambda_t(x_0)\\right).\n$$\n因此，\n$$\n\\rho = \\min_{0 \\le t \\le T-1} \\rho_t,\n$$\n这表明在任何前缀 $(0,\\dots,t-1)$ 上的正平均局部 Lyapunov 指数会使容许半径随 $t$ 指数级缩小，而负平均拉伸则会扩大半径。这直接将局部 Lyapunov 指数与线性化的有效半径联系起来。\n\n原理 4：对逻辑斯谛映射的具体化。对于 $f(x)=r\\,x(1-x)$，我们有 $f'(x)=r(1-2x)$ 和 $f''(x)=-2r$。显式的每步半径界及其聚合为\n$$\n\\rho_t = \\frac{\\alpha\\,|1-2x_t|}{\\left|\\prod_{k=0}^{t-1} r(1-2x_k)\\right|}, \\quad \\rho = \\min_{0 \\le t \\le T-1} \\rho_t.\n$$\n这可以完全从当前的线性化轨迹 $\\{x_t\\}$ 和敏感度 $J_t$ 计算得出。\n\n原理 5：一维中的 Gauss–Newton 步长。对于恒等观测、残差 $r_t = x_t(x_0^{(0)}) - y_t$（$t=1,\\dots,T$）以及雅可比项 $J_t = \\frac{\\partial x_t}{\\partial x_0}\\big|_{x_0^{(0)}}$，Gauss–Newton 步长求解正规方程\n$$\n\\left(\\sum_{t=1}^T J_t^2\\right)\\,\\delta x_0 = - \\sum_{t=1}^T J_t\\,r_t,\n$$\n因此，只要 $\\sum_{t=1}^T J_t^2  0$，\n$$\n\\delta x_0 = - \\frac{\\sum_{t=1}^T J_t\\,r_t}{\\sum_{t=1}^T J_t^2}.\n$$\n然后我们形成更新的迭代值 $x_0^{(1)} = x_0^{(0)} + \\delta x_0$ 并计算目标函数\n$$\n\\Phi(x_0) = \\tfrac{1}{2}\\sum_{t=1}^T \\left(x_t(x_0) - y_t\\right)^2.\n$$\n如果 $|\\delta x_0|  \\rho$，我们声明预测发散；如果 $\\Phi(x_0^{(1)})  \\Phi(x_0^{(0)})$ 或 $x_0^{(1)} \\notin [0,1]$，我们声明实际发散。\n\n算法设计：\n- 从 $(r, x_0^\\star)$ 生成真实轨迹 $\\{y_t\\}_{t=1}^T$。\n- 从 $(r, x_0^{(0)})$ 生成线性化轨迹 $\\{x_t\\}_{t=0}^T$。\n- 迭代计算敏感度 $J_0=1$ 和 $J_{t+1} = J_t\\,f'(x_t)$。\n- 计算半径 $\\rho = \\min_{0 \\le t \\le T-1} \\alpha\\,|1-2x_t| / |J_t|$。\n- 从 $\\{J_t\\}_{t=1}^T$ 和残差计算 Gauss–Newton 步长 $\\delta x_0$。\n- 评估预测发散和实际发散。\n- 对每个测试用例重复。\n- 将浮点输出四舍五入到六位小数，并打印一个扁平列表 $[\\rho_A,|\\delta_A|,\\mathrm{pred}_A,\\mathrm{act}_A,\\dots]$。\n\n测试套件覆盖范围：\n- 用例 A 是一个中等拉伸区域，$r=3.2$ 且初始误差较小；通常在短窗口内 $\\lambda_t$ 不会是强正值，因此 $\\rho$ 是中等大小的，且 $|\\delta x_0| \\le \\rho$ 是合理的。\n- 用例 B 和 C 使用 $r=3.9$，这是一个强混沌区域。当 $T=5$，特别是 $T=10$ 时，$\\sum \\log|f'|$ 倾向于为正，从而缩小 $\\rho$ 并经常导致 $|\\delta x_0|  \\rho$。\n- 用例 D 使用 $r=2.5$ 且初始差异较大；尽管拉伸较弱，但较大的残差可能会产生一个超出半径或离开单位区间的步长。\n- 用例 E 使用 $r=3.5$，其中 $x_0^\\star$ 接近 0.5，此时 $|f'(x)|$ 很小；这测试了对近临界点的敏感性，在这些点上 $|1-2x_t|$ 可能很小，从而局部缩小 $\\rho$ 并可能引发发散。\n\n推导和算法计划到此完成。随附的程序精确地实现了这些步骤，并生成所需的单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logistic_step(x, r):\n    return r * x * (1.0 - x)\n\ndef logistic_traj_and_sensitivities(r, x0, T):\n    \"\"\"\n    Compute trajectory x[0..T] with x[0]=x0, and sensitivities J[0..T]\n    where J[t] = d x_t / d x0 evaluated along the trajectory of x0.\n    \"\"\"\n    x = np.empty(T + 1, dtype=float)\n    J = np.empty(T + 1, dtype=float)\n    x[0] = x0\n    J[0] = 1.0\n    for t in range(T):\n        # Derivative f'(x_t) = r (1 - 2 x_t)\n        fp = r * (1.0 - 2.0 * x[t])\n        x[t + 1] = logistic_step(x[t], r)\n        J[t + 1] = J[t] * fp\n    return x, J\n\ndef observations_from_truth(r, x0_true, T):\n    y = np.empty(T + 1, dtype=float)\n    y[0] = x0_true\n    for t in range(T):\n        y[t + 1] = logistic_step(y[t], r)\n    # Return y[1..T]\n    return y[1:]\n\ndef compute_radius_alpha(r, alpha, x_traj, J_sens):\n    \"\"\"\n    Compute rho = min_{t=0..T-1} alpha * |1-2 x_t| / |J_t|\n    where x_traj has length T+1, J_sens has length T+1, and we use t = 0..T-1.\n    \"\"\"\n    T = len(x_traj) - 1\n    eps = 1e-15\n    radii = []\n    for t in range(T):\n        numerator = alpha * abs(1.0 - 2.0 * x_traj[t])\n        denom = abs(J_sens[t])\n        # Avoid division by zero; use a very small denominator to keep a very large radius.\n        denom = max(denom, eps)\n        radii.append(numerator / denom)\n    rho = min(radii) if len(radii) > 0 else 0.0\n    return rho\n\ndef gauss_newton_step(r, x0_guess, T, y_obs):\n    \"\"\"\n    One Gauss-Newton step for initial condition estimation with identity observations.\n    Returns:\n      delta_x0, phi_before, phi_after, x0_new\n    \"\"\"\n    # Trajectory and sensitivities at guess\n    xg, Jg = logistic_traj_and_sensitivities(r, x0_guess, T)\n    # Residuals r_t for t=1..T: r_t = x_t - y_t\n    residuals = xg[1:] - y_obs\n    # Jacobian entries J_t for t=1..T are Jg[1:]\n    Jrows = Jg[1:]\n    # Normal equations in 1D\n    JTJ = float(np.dot(Jrows, Jrows))\n    JTr = float(np.dot(Jrows, residuals))\n    # Objective before\n    phi_before = 0.5 * float(np.dot(residuals, residuals))\n    if JTJ  1e-18:\n        # Ill-conditioned or zero sensitivity: take no step (or treat as divergence later)\n        delta = 0.0\n    else:\n        delta = - JTr / JTJ\n    x0_new = x0_guess + delta\n    # Objective after\n    x_new_traj, _ = logistic_traj_and_sensitivities(r, x0_new, T)\n    res_after = x_new_traj[1:] - y_obs\n    phi_after = 0.5 * float(np.dot(res_after, res_after))\n    return delta, phi_before, phi_after, x0_new, xg, Jg\n\ndef run_case(case):\n    r, T, x0_true, x0_guess, alpha = case\n    # Generate noise-free observations y_t from truth\n    y_obs = observations_from_truth(r, x0_true, T)\n    # Compute one GN step at the guess\n    delta, phi_before, phi_after, x0_new, xg, Jg = gauss_newton_step(r, x0_guess, T, y_obs)\n    # Compute rho at the guess trajectory\n    rho = compute_radius_alpha(r, alpha, xg, Jg)\n    # Predicted divergence if |delta| > rho\n    predicted_diverge = abs(delta) > rho\n    # Actual divergence if objective increases or new iterate leaves [0,1]\n    actual_diverge = (phi_after > phi_before) or (x0_new  0.0) or (x0_new > 1.0)\n    return rho, abs(delta), predicted_diverge, actual_diverge\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (r, T, x0_true, x0_guess, alpha)\n    test_cases = [\n        (3.2, 5, 0.61, 0.60, 0.1),      # Case A\n        (3.9, 5, 0.61, 0.60, 0.1),      # Case B\n        (3.9, 10, 0.61, 0.609, 0.1),    # Case C\n        (2.5, 4, 0.20, 0.80, 0.1),      # Case D\n        (3.5, 6, 0.5005, 0.48, 0.1),    # Case E\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, abs_delta, pred_div, act_div = run_case(case)\n        # Round floats to six decimal places as required\n        rho_r = round(float(rho), 6)\n        abs_delta_r = round(float(abs_delta), 6)\n        results.extend([rho_r, abs_delta_r, pred_div, act_div])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3398752"}, {"introduction": "最后，我们将线性化与优化过程本身联系起来。资料同化中常用的高斯-牛顿算法依赖于对问题海森矩阵的特定线性化，这是一种一阶近似。本练习通过推导和计算一个二阶校正项，深入探讨了这种近似所带来的误差，并展示了如何诊断这种简化何时可能导致优化失败，以及何时需要更完整的牛顿类方法 [@problem_id:3398774]。", "problem": "考虑一个非线性预测模型和一个非线性观测算子，其复合形式如下。预测模型是映射 $M:\\mathbb{R}^2 \\to \\mathbb{R}^2$，由 $M(x) = \\begin{bmatrix} M_1(x) \\\\ M_2(x) \\end{bmatrix}$ 给出，其中 $M_1(x) = x_1$，$M_2(x) = a \\tanh(x_2)$，且 $x = (x_1,x_2)^{\\top}$，$a \\in \\mathbb{R}$ 是一个控制非线性的正标量参数。观测算子是 $H:\\mathbb{R}^2 \\to \\mathbb{R}$，由 $H(z) = z_1^2 + z_2$ 给出。定义复合正向映射 $h:\\mathbb{R}^2 \\to \\mathbb{R}$ 为 $h(x) = (H \\circ M)(x)$。假设给定一个标量观测值 $y \\in \\mathbb{R}$，我们考虑非线性最小二乘目标函数 $f(x) = \\tfrac{1}{2}(h(x) - y)^2$。\n\n您的任务是：\n\n- 从二阶泰勒展开和导数的链式法则出发，仅使用基本原理推导方向 $\\delta \\in \\mathbb{R}^2$ 上的二阶校正项 $\\tfrac{1}{2} D^2(H \\circ M)_x[\\delta,\\delta]$ 的显式表达式。您的推导必须仅基于雅可比矩阵和海森矩阵的定义、复合函数的链式法则，以及用 $M$ 的雅可比矩阵、$H$ 的梯度和海森矩阵、以及 $M$ 各分量的海森矩阵来表示 $D^2(H \\circ M)_x$。\n\n- 使用推导出的表达式，实现一个程序，在给定的线性化点 $x$ 处，对下述每个指定的测试用例执行以下计算：\n  1. 计算残差 $r = h(x) - y$。\n  2. 通过链式法则，仅使用 $M$ 的雅可比矩阵和 $H$ 的梯度计算复合正向映射的梯度 $J = \\nabla h(x) \\in \\mathbb{R}^2$。\n  3. 计算高斯-牛顿搜索方向 $\\delta_{\\mathrm{GN}}$，其为 $(J^{\\top}J)\\delta = -J^{\\top} r$ 的解，并在需要时对 $J^{\\top}J$ 添加一个小的 $\\ell_2$ 惩罚项 $\\lambda I$（即吉洪诺夫正则化）以确保可逆性。使用 $\\lambda = 10^{-12}$。\n  4. 计算二阶校正项 $c = \\tfrac{1}{2} D^2(H \\circ M)_x[\\delta_{\\mathrm{GN}},\\delta_{\\mathrm{GN}}]$。\n  5. 计算沿 $\\delta_{\\mathrm{GN}}$ 方向观测值的一阶预测变化，即 $p = J \\cdot \\delta_{\\mathrm{GN}}$。\n  6. 计算非线性比率 $\\rho = |c|/\\max(\\varepsilon, |p|)$，其中 $\\varepsilon = 10^{-12}$。\n  7. 当 $\\rho \\ge \\tau$ 时，将高斯-牛顿线性化分类为不可靠，并认为完全牛顿法更可取，其中阈值为 $\\tau = 0.5$。在这种情况下，设置成败指示符 $\\kappa = 1$；否则设置 $\\kappa = 0$。\n\n- 对于所有测试用例，您的程序必须生成单行输出，包含一个扁平列表，其中交替出现非线性比率和指示符，即 $[\\rho_1,\\kappa_1,\\rho_2,\\kappa_2,\\dots]$。所有角度均以弧度为单位。本问题中没有物理单位。\n\n使用以下测试套件。在所有情况下，复合正向映射和所有导数都在给定的 $x$ 处针对指定的 $a$ 进行评估，观测值 $y$ 如所述：\n\n- 测试用例 A（近线性区域）：$a = 3.0$, $x = (0.9,\\,0.1)^{\\top}$, $y = h(x_{\\star})$，其中 $x_{\\star} = (1.0,\\,0.1)^{\\top}$。\n- 测试用例 B（高度非线性，正饱和）：$a = 5.0$, $x = (0.01,\\,3.0)^{\\top}$, $y = 2.0$。\n- 测试用例 C（高度非线性，负饱和）：$a = 5.0$, $x = (0.01,\\,-3.0)^{\\top}$, $y = 2.0$。\n- 测试用例 D（精确解，零残差）：$a = 3.0$, $x = (0.5,\\,0.1)^{\\top}$, $y = h(x)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果严格按照 $[\\rho_{\\mathrm{A}},\\kappa_{\\mathrm{A}},\\rho_{\\mathrm{B}},\\kappa_{\\mathrm{B}},\\rho_{\\mathrm{C}},\\kappa_{\\mathrm{C}},\\rho_{\\mathrm{D}},\\kappa_{\\mathrm{D}}]$ 的顺序排列。每个 $\\rho$ 必须是浮点数，每个 $\\kappa$ 必须是等于 $0$ 或 $1$ 的整数。", "solution": "用户想要分析一个反问题中使用的复合模型的非线性，特别是要确定高斯-牛顿线性化在何时是不可靠的。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n- **预测模型**：$M:\\mathbb{R}^2 \\to \\mathbb{R}^2$, $M(x) = \\begin{bmatrix} M_1(x) \\\\ M_2(x) \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ a \\tanh(x_2) \\end{bmatrix}$，对于 $x = (x_1,x_2)^{\\top}$ 且 $a  0$。\n- **观测算子**：$H:\\mathbb{R}^2 \\to \\mathbb{R}$，$H(z) = z_1^2 + z_2$。\n- **复合正向映射**：$h:\\mathbb{R}^2 \\to \\mathbb{R}$，$h(x) = (H \\circ M)(x)$。\n- **目标函数**：$f(x) = \\tfrac{1}{2}(h(x) - y)^2$，对于一个观测值 $y \\in \\mathbb{R}$。\n- **二阶校正项**：$c = \\tfrac{1}{2} D^2(H \\circ M)_x[\\delta_{\\mathrm{GN}},\\delta_{\\mathrm{GN}}]$。\n- **残差**：$r = h(x) - y$。\n- **梯度/雅可比矩阵**：$J = \\nabla h(x) \\in \\mathbb{R}^2$。问题中使用了这个符号，但从上下文来看，它意味着 $J$ 是 $h$ 的 $1 \\times 2$ 雅可比矩阵。\n- **高斯-牛顿搜索方向**：$\\delta_{\\mathrm{GN}}$ 是 $(J^{\\top}J + \\lambda I)\\delta = -J^{\\top} r$ 的解。\n- **正则化参数**：$\\lambda = 10^{-12}$。\n- **一阶预测变化**：$p = J \\cdot \\delta_{\\mathrm{GN}}$。\n- **非线性比率**：$\\rho = |c|/\\max(\\varepsilon, |p|)$。\n- **分母下限**：$\\varepsilon = 10^{-12}$。\n- **可靠性阈值**：$\\tau = 0.5$。\n- **指示变量**：如果 $\\rho \\ge \\tau$，则 $\\kappa = 1$，否则 $\\kappa = 0$。\n\n- **测试用例**：\n  - 测试用例 A：$a = 3.0$, $x = (0.9,\\,0.1)^{\\top}$，$y = h(x_{\\star})$，其中 $x_{\\star} = (1.0,\\,0.1)^{\\top}$。\n  - 测试用例 B：$a = 5.0$, $x = (0.01,\\,3.0)^{\\top}$, $y = 2.0$。\n  - 测试用例 C：$a = 5.0$, $x = (0.01,\\,-3.0)^{\\top}$, $y = 2.0$。\n  - 测试用例 D：$a = 3.0$, $x = (0.5,\\,0.1)^{\\top}$, $y = h(x)$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n- **科学依据**：该问题是多元微积分（泰勒级数、链式法则、雅可比矩阵、海森矩阵）和数值优化（高斯-牛顿法）中标准概念的应用。这些是反演问题和数据同化领域的基本工具。该问题在科学和数学上是合理的。\n- **适定性**：所有函数（$M$、$H$、$h$）都是良定义且无限可微的。计算任务是按算法指定的。使用带有小正参数 $\\lambda$ 的吉洪诺夫正则化，确保了即使 $J^\\top J$ 是奇异的，高斯-牛顿方向 $\\delta_{\\mathrm{GN}}$ 的线性系统也总是有唯一解。因此，该问题是适定的。\n- **客观性**：该问题使用精确的数学语言和定义来表述，没有主观或模糊的陈述。\n\n该问题是自洽的、一致的，并满足所有有效性标准。\n\n**步骤 3：结论与行动**\n\n该问题有效。我将继续进行推导和求解。\n\n### 二阶校正项的推导\n\n问题的核心是量化正向映射 $h(x) = (H \\circ M)(x)$ 的非线性。这通过比较其在点 $x$ 处沿特定方向 $\\delta$ 的泰勒展开的一阶项和二阶项来完成。$h(x+\\delta)$ 的泰勒展开为：\n$$ h(x+\\delta) = h(x) + Dh(x)[\\delta] + \\frac{1}{2} D^2h(x)[\\delta, \\delta] + O(\\|\\delta\\|^3) $$\n项 $p = Dh(x)[\\delta_{\\mathrm{GN}}]$ 是观测值的一阶预测变化。项 $c = \\frac{1}{2} D^2h(x)[\\delta_{\\mathrm{GN}}, \\delta_{\\mathrm{GN}}]$ 是二阶校正。问题要求对此项进行显式推导。\n\n我们使用二阶导数的链式法则（法阿·迪·布鲁诺公式）。对于复合函数 $h(x) = H(M(x))$，其二阶微分由下式给出：\n$$ D^2h_x[\\delta, \\delta] = D^2H_{M(x)}[DM_x[\\delta], DM_x[\\delta]] + DH_{M(x)}[D^2M_x[\\delta, \\delta]] $$\n在矩阵表示法中，其中 $\\mathbf{M}(x)$ 是 $M$ 的雅可比矩阵，$\\mathbf{H}_H(z)$ 是 $H$ 的海森矩阵，$\\mathbf{H}_{M_i}(x)$ 是 $M$ 的第 $i$ 个分量的海森矩阵，则 $h$ 的海森矩阵（我们记为 $\\mathbf{H}_h(x)$）为：\n$$ \\mathbf{H}_h(x) = \\mathbf{M}(x)^{\\top} \\mathbf{H}_H(M(x)) \\mathbf{M}(x) + \\sum_{i=1}^{k} \\left( \\nabla H(M(x)) \\right)_i \\mathbf{H}_{M_i}(x) $$\n其中 $k=2$ 是中间空间的维度。那么二阶项为 $c = \\frac{1}{2}\\delta_{\\mathrm{GN}}^{\\top} \\mathbf{H}_h(x) \\delta_{\\mathrm{GN}}$。\n\n我们现在为给定的函数计算所需的导数：\n1.  **函数定义**：\n    $M(x) = \\begin{bmatrix} x_1 \\\\ a \\tanh(x_2) \\end{bmatrix}$，$H(z) = z_1^2 + z_2$。\n\n2.  **$M$ 的导数**：\n    - $M$ 的雅可比矩阵：$\\mathbf{M}(x) = \\begin{bmatrix} \\frac{\\partial M_1}{\\partial x_1}  \\frac{\\partial M_1}{\\partial x_2} \\\\ \\frac{\\partial M_2}{\\partial x_1}  \\frac{\\partial M_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  a \\, \\mathrm{sech}^2(x_2) \\end{bmatrix}$。\n    - $M_1(x) = x_1$ 的海森矩阵：$\\mathbf{H}_{M_1}(x) = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$。\n    - $M_2(x) = a \\tanh(x_2)$ 的海森矩阵：\n      $\\frac{\\partial^2 M_2}{\\partial x_1^2} = 0$，$\\frac{\\partial^2 M_2}{\\partial x_1 \\partial x_2} = 0$。\n      $\\frac{\\partial^2 M_2}{\\partial x_2^2} = \\frac{d}{dx_2} (a \\, \\mathrm{sech}^2(x_2)) = a \\cdot 2 \\mathrm{sech}(x_2) \\cdot (-\\mathrm{sech}(x_2)\\tanh(x_2)) = -2a \\, \\mathrm{sech}^2(x_2) \\tanh(x_2)$。\n      因此，$\\mathbf{H}_{M_2}(x) = \\begin{bmatrix} 0  0 \\\\ 0  -2a \\, \\mathrm{sech}^2(x_2) \\tanh(x_2) \\end{bmatrix}$。\n\n3.  **$H$ 的导数**：\n    令 $z = M(x) = (x_1, a\\tanh(x_2))^{\\top}$。\n    - $H$ 的梯度：$\\nabla H(z) = \\begin{bmatrix} \\frac{\\partial H}{\\partial z_1} \\\\ \\frac{\\partial H}{\\partial z_2} \\end{bmatrix} = \\begin{bmatrix} 2z_1 \\\\ 1 \\end{bmatrix}$。在 $z=M(x)$ 处求值，得到 $\\nabla H(M(x)) = \\begin{bmatrix} 2x_1 \\\\ 1 \\end{bmatrix}$。\n    - $H$ 的海森矩阵：$\\mathbf{H}_H(z) = \\begin{bmatrix} \\frac{\\partial^2 H}{\\partial z_1^2}  \\frac{\\partial^2 H}{\\partial z_1 \\partial z_2} \\\\ \\frac{\\partial^2 H}{\\partial z_2 \\partial z_1}  \\frac{\\partial^2 H}{\\partial z_2^2} \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix}$。\n\n4.  **组装 $h$ 的海森矩阵**：\n    我们将这些分量代入 $\\mathbf{H}_h(x)$ 的公式：\n    $$ \\mathbf{H}_h(x) = \\underbrace{\\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix} \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix}}_{\\text{第 1 部分}} + \\underbrace{(2x_1)\\mathbf{H}_{M_1}(x) + (1)\\mathbf{H}_{M_2}(x)}_{\\text{第 2 部分}} $$\n    - 第 1 部分：$\\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix} \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix}$。\n    - 第 2 部分：$(2x_1)\\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix} + (1)\\begin{bmatrix} 0  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} = \\begin{bmatrix} 0  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix}$。\n    - 合并各部分：\n    $$ \\mathbf{H}_h(x) = \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix} + \\begin{bmatrix} 0  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} $$\n\n5.  **校正项 $c$ 的表达式**：\n    令 $\\delta_{\\mathrm{GN}} = (\\delta_1, \\delta_2)^{\\top}$。二阶校正项为：\n    $$ c = \\frac{1}{2} \\delta_{\\mathrm{GN}}^{\\top} \\mathbf{H}_h(x) \\delta_{\\mathrm{GN}} = \\frac{1}{2} \\begin{bmatrix} \\delta_1  \\delta_2 \\end{bmatrix} \\begin{bmatrix} 2  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} \\begin{bmatrix} \\delta_1 \\\\ \\delta_2 \\end{bmatrix} $$\n    $$ c = \\frac{1}{2} (2\\delta_1^2 - 2a\\,\\delta_2^2\\,\\mathrm{sech}^2(x_2)\\tanh(x_2)) $$\n    $$ c = \\delta_1^2 - a\\,\\delta_2^2\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) $$\n    这就是二阶校正项的显式表达式。实现将遵循问题陈述中指定的算法步骤，并使用这个推导出的公式。\n\n### 算法步骤与实现\n每个测试用例的流程如下。请注意，问题中使用的符号 $J=\\nabla h(x)$ 被解释为 $J$ 是一个 $1 \\times 2$ 的雅可比矩阵（一个行向量），这与标量观测函数的标准高斯-牛顿公式一致。\n\n1.  **计算 $h(x)$ 和残差 $r$**：\n    $h(x) = x_1^2 + a \\tanh(x_2)$。\n    $r = h(x) - y$。\n\n2.  **计算雅可比矩阵 $J$**：\n    通过直接求导或链式法则：$J = \\nabla h(x)^{\\top} = [2x_1, a \\, \\mathrm{sech}^2(x_2)]$。\n\n3.  **计算高斯-牛顿方向 $\\delta_{\\mathrm{GN}}$**：\n    求解 $2 \\times 2$ 线性系统 $(J^{\\top}J + \\lambda I)\\delta = -J^{\\top} r$ 以得到 $\\delta = \\delta_{\\mathrm{GN}}$。\n\n4.  **计算二阶校正项 $c$**：\n    使用推导出的表达式：$c = \\delta_{\\mathrm{GN},1}^2 - a\\,\\delta_{\\mathrm{GN},2}^2\\,\\mathrm{sech}^2(x_2)\\tanh(x_2)$。\n\n5.  **计算一阶预测变化 $p$**：\n    $p = J \\cdot \\delta_{\\mathrm{GN}} = J_1 \\delta_{\\mathrm{GN},1} + J_2 \\delta_{\\mathrm{GN},2}$。\n\n6.  **计算非线性比率 $\\rho$**：\n    $\\rho = |c|/\\max(\\varepsilon, |p|)$。\n\n7.  **计算指示符 $\\kappa$**：\n    如果 $\\rho \\ge 0.5$，则 $\\kappa = 1$，否则 $\\kappa = 0$。\n\n以下 Python 代码为所有指定的测试用例实现了此算法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the nonlinearity of a composite model\n    for several test cases.\n    \"\"\"\n\n    def compute_rho_kappa(a, x_val, y_val, lambda_reg=1e-12, epsilon=1e-12, tau=0.5):\n        \"\"\"\n        Performs the required computations for a single test case.\n\n        Args:\n            a (float): The scalar parameter for the forecast model.\n            x_val (np.ndarray): The linearization point, shape (2,).\n            y_val (float): The scalar observation.\n            lambda_reg (float): Tikhonov regularization parameter.\n            epsilon (float): Small constant to avoid division by zero.\n            tau (float): Threshold for classifying nonlinearity.\n\n        Returns:\n            tuple: A tuple containing the nonlinearity ratio (rho) and the\n                   indicator (kappa).\n        \"\"\"\n        x1, x2 = x_val[0], x_val[1]\n\n        # 1. Compute residual r = h(x) - y\n        h_x = x1**2 + a * np.tanh(x2)\n        r = h_x - y_val\n\n        # 2. Compute the Jacobian J of h(x)\n        # J is a 1x2 row vector (matrix)\n        sech2_x2 = (1.0 / np.cosh(x2))**2\n        J = np.array([[2 * x1, a * sech2_x2]])\n\n        # 3. Compute the Gauss-Newton search direction delta_GN\n        # Solve (J^T J + lambda*I) delta = -J^T r\n        # J is (1,2), J.T is (2,1). J.T @ J is (2,2).\n        J_T_J = J.T @ J\n        A = J_T_J + lambda_reg * np.eye(2)\n        # J.T is (2,1), r is a scalar. rhs is (2,1).\n        rhs = -J.T * r\n        # delta_gn is a column vector (2,1)\n        delta_gn = np.linalg.solve(A, rhs)\n        delta_1, delta_2 = delta_gn[0, 0], delta_gn[1, 0]\n\n        # 4. Compute the second-order correction term c\n        # c = delta_1^2 - a * delta_2^2 * sech^2(x2) * tanh(x2)\n        tanh_x2 = np.tanh(x2)\n        c = delta_1**2 - a * (delta_2**2) * sech2_x2 * tanh_x2\n\n        # 5. Compute the first-order predicted change p\n        # p = J . delta_GN\n        # J is (1,2), delta_gn (2,1), so p is (1,1) matrix\n        p = (J @ delta_gn)[0, 0]\n\n        # 6. Compute the nonlinearity ratio rho\n        rho = np.abs(c) / np.maximum(epsilon, np.abs(p))\n\n        # 7. Classify reliability and compute indicator kappa\n        kappa = 1 if rho >= tau else 0\n\n        return rho, kappa\n\n    # Define test cases\n    # For h(x_star), the parameter 'a' must match the test case 'a'\n    h_func = lambda a_val, x_star_val: x_star_val[0]**2 + a_val * np.tanh(x_star_val[1])\n\n    test_cases = [\n        # Case A: Near-linear regime\n        {\n            \"a\": 3.0,\n            \"x\": np.array([0.9, 0.1]),\n            \"y\": h_func(3.0, np.array([1.0, 0.1]))\n        },\n        # Case B: Highly nonlinear, positive saturation\n        {\n            \"a\": 5.0,\n            \"x\": np.array([0.01, 3.0]),\n            \"y\": 2.0\n        },\n        # Case C: Highly nonlinear, negative saturation\n        {\n            \"a\": 5.0,\n            \"x\": np.array([0.01, -3.0]),\n            \"y\": 2.0\n        },\n        # Case D: Exact solution, zero residual\n        {\n            \"a\": 3.0,\n            \"x\": np.array([0.5, 0.1]),\n            \"y\": h_func(3.0, np.array([0.5, 0.1]))\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, kappa = compute_rho_kappa(case[\"a\"], case[\"x\"], case[\"y\"])\n        results.extend([rho, kappa])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3398774"}]}