{"hands_on_practices": [{"introduction": "在处理观测数据时，我们经常会遇到误差不仅是随机的，而且是相互关联的情况。这种相关性使得数据失配函数的结构变得复杂。然而，只要线性和高斯性假设成立，我们就可以通过一种称为“白化”（whitening）的数学变换，将问题简化为等价的标准最小二乘问题。本练习将通过一个具体的数值示例，指导您构建白化变换，从而直观地理解这些基本假设如何帮助我们应对看似复杂的问题。[@problem_id:3365471]", "problem": "考虑一个逆问题中的线性观测模型，其满足线性和高斯性假设。设观测值由 $y = H x + \\epsilon$ 建模，其中 $y \\in \\mathbb{R}^{3}$ 是观测值，$x \\in \\mathbb{R}^{2}$ 是状态，$H \\in \\mathbb{R}^{3 \\times 2}$ 是线性观测算子，$\\epsilon \\in \\mathbb{R}^{3}$ 是观测误差，服从多元正态分布 $\\epsilon \\sim \\mathcal{N}(0, R)$，其协方差矩阵 $R \\in \\mathbb{R}^{3 \\times 3}$ 是对称正定（SPD）的。假设以下具体的、具有科学现实意义的矩阵和向量：\n$$\nR = \\begin{pmatrix}\n4   2   0 \\\\\n2   10  6 \\\\\n0   6   5\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n1   0 \\\\\n1   1 \\\\\n0   2\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n4 \\\\\n0 \\\\\n3\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix}.\n$$\n仅从多元正态概率密度函数（PDF）的定义和线性观测模型出发，执行以下操作：\n\n1. 根据线性和高斯性假设，推导负对数似然（数据失配），用残差 $d = y - H x$ 和协方差 $R$ 表示。\n\n2. 构建一个白化变换，即找到一个矩阵 $W \\in \\mathbb{R}^{3 \\times 3}$ 使得 $W R W^{\\top} = I$。这通过将 $R$ 进行 Cholesky 分解得到一个对角线元素为正的下三角矩阵 $L$（其中 $R = L L^{\\top}$），然后设置 $W = L^{-1}$ 来实现。\n\n3. 使用您构建的 $W$，定义白化变量 $\\tilde{y} = W y$ 和 $\\tilde{H} = W H$，并通过将数据失配表示为白化残差的欧几里得范数来展示白化的效果，即证明它变成了一个标准的最小二乘形式。\n\n4. 对于给定的 $y$、$H$ 和 $x$，使用精确算术对白化数据失配进行数值评估，并在可能的情况下简化为精确的有理数。\n\n您的最终答案必须是一个单一的闭式解析表达式：显式的白化矩阵 $W$。不需要四舍五入，本问题中的量没有单位。", "solution": "该问题被验证为自洽的，其科学基础在于应用于逆问题的线性代数和统计学原理，并且是适定的。所提供的协方差矩阵 $R$ 是对称的，其顺序主子式分别为 $4$、$36$ 和 $36$，均为正数，这证实了 $R$ 是正定的。因此，其 Cholesky 分解存在且唯一。我们可以按照指定的四个步骤进行求解。\n\n**1. 负对数似然的推导**\n\n问题假设了一个线性观测模型 $y = H x + \\epsilon$，其中观测误差 $\\epsilon$ 服从均值为零、协方差矩阵为 $R$ 的多元正态分布，记为 $\\epsilon \\sim \\mathcal{N}(0, R)$。给定状态 $x$，观测值 $y$ 也因此服从正态分布，$y \\sim \\mathcal{N}(Hx, R)$。\n\n一个均值为 $\\mu$、协方差矩阵为 $\\Sigma$ 的多元正态随机向量 $z \\in \\mathbb{R}^k$ 的概率密度函数（PDF）由下式给出：\n$$\np(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^{\\top} \\Sigma^{-1} (z-\\mu)\\right)\n$$\n在我们的情境中，观测向量 $y$ 位于 $\\mathbb{R}^3$ 中，所以 $k=3$。均值为 $\\mu = Hx$，协方差为 $\\Sigma=R$。因此，给定状态 $x$ 时观测到 $y$ 的似然函数为：\n$$\n\\mathcal{L}(x|y) = p(y|x) = \\frac{1}{\\sqrt{(2\\pi)^3 \\det(R)}} \\exp\\left(-\\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\\right)\n$$\n负对数似然是通过对似然函数取自然对数然后取反得到的：\n$$\n-\\ln(\\mathcal{L}(x|y)) = -\\ln\\left(\\frac{1}{\\sqrt{(2\\pi)^3 \\det(R)}}\\right) - \\ln\\left(\\exp\\left(-\\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\\right)\\right)\n$$\n$$\n-\\ln(\\mathcal{L}(x|y)) = \\ln\\left(\\sqrt{(2\\pi)^3 \\det(R)}\\right) + \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n$$\n-\\ln(\\mathcal{L}(x|y)) = \\frac{3}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln(\\det(R)) + \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n在优化和数据同化中，数据失配函数（通常表示为 $J(x)$）由负对数似然中依赖于状态 $x$ 的项组成。常数项通常被舍去。因此，数据失配与二次项成正比。我们将失配定义为：\n$$\nJ(x) = \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n将残差定义为 $d = y - Hx$，数据失配表示为：\n$$\nJ(x) = \\frac{1}{2} d^{\\top} R^{-1} d\n$$\n\n**2. 白化变换的构建**\n\n我们的任务是找到一个白化矩阵 $W$ 使得 $W R W^{\\top} = I$。这通过首先对 $R$ 进行 Cholesky 分解得到 $R = L L^{\\top}$ 来实现，其中 $L$ 是一个对角线元素为正的下三角矩阵，然后设置 $W = L^{-1}$。\n\n给定 $R = \\begin{pmatrix} 4  2  0 \\\\ 2  10  6 \\\\ 0  6  5 \\end{pmatrix}$，我们寻求 $L = \\begin{pmatrix} L_{11}  0  0 \\\\ L_{21}  L_{22}  0 \\\\ L_{31}  L_{32}  L_{33} \\end{pmatrix}$ 使得 $L L^{\\top} = R$。\n\n$L$ 的分量按顺序求解如下：\n- $L_{11}^2 = R_{11} = 4 \\implies L_{11} = 2$。\n- $L_{21}L_{11} = R_{21} = 2 \\implies L_{21}(2) = 2 \\implies L_{21} = 1$。\n- $L_{31}L_{11} = R_{31} = 0 \\implies L_{31}(2) = 0 \\implies L_{31} = 0$。\n- $L_{21}^2 + L_{22}^2 = R_{22} = 10 \\implies 1^2 + L_{22}^2 = 10 \\implies L_{22}^2 = 9 \\implies L_{22} = 3$。\n- $L_{32}L_{22} + L_{31}L_{21} = R_{32} = 6 \\implies L_{32}(3) + (0)(1) = 6 \\implies L_{32} = 2$。\n- $L_{31}^2 + L_{32}^2 + L_{33}^2 = R_{33} = 5 \\implies 0^2 + 2^2 + L_{33}^2 = 5 \\implies L_{33}^2 = 1 \\implies L_{33} = 1$。\n\n因此，Cholesky 因子为 $L = \\begin{pmatrix} 2  0  0 \\\\ 1  3  0 \\\\ 0  2  1 \\end{pmatrix}$。\n\n接下来，我们通过前向替换法求解 $L W = I$ 来找到白化矩阵 $W = L^{-1}$。设 $W = (w_{ij})$。\n$$\n\\begin{pmatrix} 2  0  0 \\\\ 1  3  0 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} w_{11}  w_{12}  w_{13} \\\\ w_{21}  w_{22}  w_{23} \\\\ w_{31}  w_{32}  w_{33} \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n求解 $W$ 的每一列：\n- 第1列：$2w_{11}=1 \\implies w_{11}=\\frac{1}{2}$。 $w_{11}+3w_{21}=0 \\implies \\frac{1}{2}+3w_{21}=0 \\implies w_{21}=-\\frac{1}{6}$。 $2w_{21}+w_{31}=0 \\implies 2(-\\frac{1}{6})+w_{31}=0 \\implies w_{31}=\\frac{1}{3}$。 此外 $w_{12}=w_{13}=0$。\n- 第2列：$3w_{22}=1 \\implies w_{22}=\\frac{1}{3}$。 $2w_{22}+w_{32}=0 \\implies 2(\\frac{1}{3})+w_{32}=0 \\implies w_{32}=-\\frac{2}{3}$。 此外 $w_{23}=0$。\n- 第3列：$w_{33}=1$。\n\n得到的白化矩阵是 $W = L^{-1} = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ -\\frac{1}{6}  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  -\\frac{2}{3}  1 \\end{pmatrix}$。\n\n**3. 白化对数据失配的影响**\n\n我们从数据失配表达式 $J(x) = \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)$ 开始。\n由 $R = L L^{\\top}$，我们有 $R^{-1} = (L L^{\\top})^{-1} = (L^{\\top})^{-1}L^{-1}$。由于转置的逆等于逆的转置，所以 $(L^{\\top})^{-1} = (L^{-1})^{\\top}$。设 $W=L^{-1}$，我们得到 $R^{-1} = W^{\\top}W$。\n将此代入失配函数：\n$$\nJ(x) = \\frac{1}{2} (y-Hx)^{\\top} (W^{\\top}W) (y-Hx)\n$$\n使用性质 $(AB)^{\\top} = B^{\\top}A^{\\top}$，我们可以重新组合这些项：\n$$\nJ(x) = \\frac{1}{2} [W(y-Hx)]^{\\top} [W(y-Hx)]\n$$\n定义白化观测值 $\\tilde{y} = Wy$，白化算子 $\\tilde{H} = WH$，以及白化残差 $\\tilde{d} = \\tilde{y} - \\tilde{H}x = W(y-Hx)$，表达式变为：\n$$\nJ(x) = \\frac{1}{2} \\tilde{d}^{\\top}\\tilde{d} = \\frac{1}{2} ||\\tilde{d}||_2^2 = \\frac{1}{2} ||\\tilde{y} - \\tilde{H}x||_2^2\n$$\n这表明，由协方差矩阵的逆加权的广义最小二乘代价函数，被转换为了一个标准的、无权重的最小二乘问题，该问题涉及白化残差的平方和。\n\n**4. 白化数据失配的数值评估**\n\n首先，我们用给定的值计算残差 $d = y - Hx$：\n$H = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  2 \\end{pmatrix}$, $x = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, $y = \\begin{pmatrix} 4 \\\\ 0 \\\\ 3 \\end{pmatrix}$。\n$$\nHx = \\begin{pmatrix} 1  0 \\\\ 1  1 \\\\ 0  2 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(-1) \\\\ (1)(2) + (1)(-1) \\\\ (0)(2) + (2)(-1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\n$$\nd = y - Hx = \\begin{pmatrix} 4 \\\\ 0 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 5 \\end{pmatrix}\n$$\n接下来，我们计算白化残差 $\\tilde{d} = W d$：\n$$\n\\tilde{d} = \\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ -\\frac{1}{6}  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  -\\frac{2}{3}  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{2})(2) + (0)(-1) + (0)(5) \\\\ (-\\frac{1}{6})(2) + (\\frac{1}{3})(-1) + (0)(5) \\\\ (\\frac{1}{3})(2) + (-\\frac{2}{3})(-1) + (1)(5) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{1}{3} - \\frac{1}{3} \\\\ \\frac{2}{3} + \\frac{2}{3} + 5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{2}{3} \\\\ \\frac{4}{3} + \\frac{15}{3} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{2}{3} \\\\ \\frac{19}{3} \\end{pmatrix}\n$$\n最后，我们评估白化数据失配 $J = \\frac{1}{2}||\\tilde{d}||_2^2$：\n$$\nJ = \\frac{1}{2} \\left( (1)^2 + \\left(-\\frac{2}{3}\\right)^2 + \\left(\\frac{19}{3}\\right)^2 \\right) = \\frac{1}{2} \\left( 1 + \\frac{4}{9} + \\frac{361}{9} \\right)\n$$\n$$\nJ = \\frac{1}{2} \\left( \\frac{9}{9} + \\frac{4}{9} + \\frac{361}{9} \\right) = \\frac{1}{2} \\left( \\frac{9+4+361}{9} \\right) = \\frac{1}{2} \\left( \\frac{374}{9} \\right) = \\frac{187}{9}\n$$\n数据失配的数值是 $\\frac{187}{9}$。问题要求以白化矩阵 $W$ 作为最终答案。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  0  0 \\\\ -\\frac{1}{6}  \\frac{1}{3}  0 \\\\ \\frac{1}{3}  -\\frac{2}{3}  1 \\end{pmatrix}}\n$$", "id": "3365471"}, {"introduction": "高斯噪声假设是许多反演方法的基石，但在现实世界中，测量数据常常包含偏离正态分布的“离群值”（outliers）。本练习旨在探讨当高斯假设被违反时，直接应用基于高斯模型的估计会带来什么后果。通过将标准的高斯最大后验（MAP）估计与基于更稳健的学生t分布（Student-$t$ distribution）的模型进行对比，您将能够量化由模型误设导致的估计偏差，并体会到稳健估计方法的价值。[@problem_id:3365464]", "problem": "考虑一个线性逆问题，其前向算子由一个已知矩阵 $H \\in \\mathbb{R}^{m \\times n}$ 表示，未知状态向量为 $x \\in \\mathbb{R}^{n}$，观测数据为 $y \\in \\mathbb{R}^{m}$。观测模型为 $y = H x + \\varepsilon$，其中 $\\varepsilon$ 代表测量噪声。您的任务是分析在两种噪声建模假设下的最大后验 (MAP) 估计器：一种是错误指定的高斯似然，另一种是正确指定的重尾学生t分布似然，并量化每种MAP估计相对于已知真值的偏差。\n\n使用贝叶斯法则以及高斯和学生t分布概率密度函数 (PDF) 的定义作为基本依据。具体而言：\n- 方差参数为 $\\sigma^{2}$ 的标量高斯PDF为 $p(r) \\propto \\exp\\left(-\\frac{r^{2}}{2 \\sigma^{2}}\\right)$。\n- 自由度为 $\\nu$、尺度为 $\\sigma$ 的学生t分布PDF为 $p(r) \\propto \\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu + 1}{2}}$。\n\n假设 $x$ 服从高斯先验，即 $x \\sim \\mathcal{N}(m, P)$，其中 $m \\in \\mathbb{R}^{n}$，$P \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。在高斯似然下，MAP 估计最小化一个凸二次目标函数。在学生t分布似然下，MAP 估计最小化一个基于对数项之和的鲁棒目标函数。请从第一性原理推导 MAP 估计器，并设计一个算法，用于在给定 $H$、$y$、$m$、$P$、$\\sigma$ 和 $\\nu$ 的情况下计算两种 MAP 估计。然后，比较它们相对于已知真值 $x^{\\star}$ 的偏差。\n\n您必须编写一个完整、可运行的程序，该程序：\n1. 构造以下固定的问题实例：\n   - 维度：$m = 8$, $n = 2$。\n   - 前向矩阵 $H$（逐行列出）：\n     - 第 1 行：$\\left[1.0,\\,0.5\\right]$\n     - 第 2 行：$\\left[0.8,\\,-0.3\\right]$\n     - 第 3 行：$\\left[-0.6,\\,1.2\\right]$\n     - 第 4 行：$\\left[0.0,\\,1.0\\right]$\n     - 第 5 行：$\\left[1.5,\\,-0.7\\right]$\n     - 第 6 行：$\\left[-1.0,\\,-0.2\\right]$\n     - 第 7 行：$\\left[0.3,\\,0.8\\right]$\n     - 第 8 行：$\\left[-0.4,\\,0.5\\right]$\n   - 真值状态 $x^{\\star} = \\left[1.0,\\,-2.0\\right]$。\n   - 先验均值 $m = \\left[0.0,\\,0.0\\right]$。\n   - 先验协方差 $P = \\mathrm{diag}\\left(10.0,\\,10.0\\right)$。\n   - 噪声尺度 $\\sigma = 1.0$（注意：不涉及物理单位）。\n2. 通过使用固定的残差向量 $r$ 创建 $y$（即 $y = H x^{\\star} + r$），并为学生t分布似然指定 $\\nu$，形成包含四个案例的测试套件：\n   - 案例1（重尾残差，中等自由度）：\n     - $\\nu = 3$\n     - $r = \\left[0.2,\\,-0.1,\\,0.15,\\,-0.05,\\,0.1,\\,-0.1,\\,12.0,\\,-15.0\\right]$\n   - 案例2（近似高斯行为）：\n     - $\\nu = 30$\n     - $r = \\left[0.05,\\,-0.02,\\,0.03,\\,-0.04,\\,0.01,\\,-0.01,\\,0.02,\\,-0.03\\right]$\n   - 案例3（极端重尾，多个大离群值）：\n     - $\\nu = 1.5$\n     - $r = \\left[20.0,\\,-25.0,\\,0.2,\\,0.0,\\,-0.1,\\,18.0,\\,-22.0,\\,0.3\\right]$\n   - 案例4（无残差的基线情况）：\n     - $\\nu = 3$\n     - $r = \\left[0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\right]$\n3. 对每个案例计算：\n   - 高斯似然 MAP 估计 $x_{\\mathrm{G}}$。\n   - 使用从第一性原理推导并通过收敛迭代方案实现的鲁棒算法计算学生t分布似然 MAP 估计 $x_{\\mathrm{T}}$。\n   - 每种假设下偏差的欧几里得范数：$\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}$ 和 $\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}$。\n   - 相对改进率 $\\rho = \\frac{\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}}{\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}}$（如果分母为 $0$，则将 $\\rho$ 设置为 $+\\infty$）。\n   - 两个估计之间的差异范数 $\\left\\|x_{\\mathrm{G}} - x_{\\mathrm{T}}\\right\\|_{2}$。\n4. 生成单行输出，其中包含汇总了所有四个案例结果的、用方括号括起来的逗号分隔列表。每个案例的结果必须是包含四个浮点数的列表，顺序如下：\n   - $\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}$,\n   - $\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}$,\n   - $\\rho$,\n   - $\\left\\|x_{\\mathrm{G}} - x_{\\mathrm{T}}\\right\\|_{2}$。\n例如，最终输出必须具有格式 $\\left[\\left[a_{1},b_{1},c_{1},d_{1}\\right],\\left[a_{2},b_{2},c_{2},d_{2}\\right],\\left[a_{3},b_{3},c_{3},d_{3}\\right],\\left[a_{4},b_{4},c_{4},d_{4}\\right]\\right]$。\n\n覆盖性设计：\n- 案例1测试中等重尾建模下少数几个大离群值的影响。\n- 案例2近似高斯机制。\n- 案例3测试具有多个大离群值的极端重尾情况。\n- 案例4是无残差的边界情况。\n\n不涉及角度和物理单位；将所有量报告为不带单位的纯浮点数。您的实现必须是自包含的，并且不得读取任何输入。最终程序必须为给定的测试套件确定性地计算指定的输出。", "solution": "所提出的问题要求针对一个线性逆问题，基于对测量噪声统计特性的不同假设，推导并比较两种最大后验 (MAP) 估计器。该任务的核心是应用贝叶斯法则来找到在给定测量值 $y$ 的情况下，能使状态向量 $x$ 的后验概率最大化的估计器。\n\n观测模型由 $y = Hx + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{m}$ 是观测值，$x \\in \\mathbb{R}^{n}$ 是未知状态向量，$H \\in \\mathbb{R}^{m \\times n}$ 是前向算子，$\\varepsilon \\in \\mathbb{R}^{m}$ 是测量噪声。\n\n根据贝叶斯法则，$x$ 在给定 $y$ 下的后验概率密度函数 (PDF) 为：\n$$p(x|y) \\propto p(y|x) p(x)$$\n其中 $p(y|x)$ 是似然，$p(x)$ 是状态向量的先验 PDF。\n\nMAP 估计 $x_{\\mathrm{MAP}}$ 是使该后验概率最大化的 $x$ 值。最大化 $p(x|y)$ 等效于最小化其负对数。我们定义目标函数 $J(x)$ 为：\n$$J(x) = -\\ln(p(y|x)) - \\ln(p(x))$$\n因此，MAP 估计为 $x_{\\mathrm{MAP}} = \\arg\\min_x J(x)$。\n\n问题指定了 $x$ 的高斯先验，使得 $x \\sim \\mathcal{N}(m, P)$。先验 PDF 为：\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x-m)^T P^{-1} (x-m)\\right)$$\n目标函数中相应的负对数先验项为：\n$$J_{\\mathrm{prior}}(x) = -\\ln(p(x)) = \\frac{1}{2}(x-m)^T P^{-1} (x-m) + \\mathrm{const.}$$\n\n似然项取决于噪声 $\\varepsilon$ 的假定分布。我们假设噪声分量 $\\varepsilon_i = y_i - (Hx)_i$ 是独立同分布的。负对数似然则是个体分量的总和：\n$$J_{\\mathrm{likelihood}}(x) = -\\ln(p(y|x)) = -\\sum_{i=1}^{m} \\ln p(\\varepsilon_i) = -\\sum_{i=1}^{m} \\ln p(y_i - (Hx)_i)$$\n\n我们现在为两种指定的噪声模型推导估计器。\n\n**1. 高斯似然 MAP 估计器 ($x_{\\mathrm{G}}$)**\n\n在高斯噪声的假设下，每个分量 $\\varepsilon_i$ 服从分布 $p(\\varepsilon_i) \\propto \\exp\\left(-\\frac{\\varepsilon_i^2}{2\\sigma^2}\\right)$。负对数似然项为：\n$$J_{\\mathrm{G-like}}(x) = \\sum_{i=1}^{m} \\frac{(y_i - (Hx)_i)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}\\|y - Hx\\|_2^2$$\n要为高斯 MAP 估计 $x_{\\mathrm{G}}$ 最小化的总目标函数是：\n$$J_{\\mathrm{G}}(x) = \\frac{1}{2\\sigma^2}\\|y - Hx\\|_2^2 + \\frac{1}{2}(x-m)^T P^{-1} (x-m)$$\n这是关于 $x$ 的二次函数。通过将其关于 $x$ 的梯度置为零来找到其最小值：\n$$\\nabla_x J_{\\mathrm{G}}(x) = \\frac{1}{\\sigma^2} H^T (Hx - y) + P^{-1}(x - m) = 0$$\n重新整理各项以求解 $x$ 可得：\n$$(H^T H + \\sigma^2 P^{-1}) x = H^T y + \\sigma^2 P^{-1} m$$\n这是一个线性方程组。唯一解即为高斯 MAP 估计：\n$$x_{\\mathrm{G}} = (H^T H + \\sigma^2 P^{-1})^{-1} (H^T y + \\sigma^2 P^{-1} m)$$\n这是一个可以直接计算的闭式解。\n\n**2. 学生t分布似然 MAP 估计器 ($x_{\\mathrm{T}}$)**\n\n在学生t分布噪声的假设下，每个分量 $\\varepsilon_i$ 的 PDF 为 $p(\\varepsilon_i) \\propto \\left(1 + \\frac{\\varepsilon_i^2}{\\nu \\sigma^2}\\right)^{-(\\nu+1)/2}$。负对数似然项变为：\n$$J_{\\mathrm{T-like}}(x) = \\sum_{i=1}^{m} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - (Hx)_i)^2}{\\nu \\sigma^2}\\right)$$\n学生t分布 MAP 估计 $x_{\\mathrm{T}}$ 的总目标函数为：\n$$J_{\\mathrm{T}}(x) = \\sum_{i=1}^{m} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - (Hx)_i)^2}{\\nu \\sigma^2}\\right) + \\frac{1}{2}(x-m)^T P^{-1} (x-m)$$\n该目标函数是凸的但非二次函数，因此需要使用迭代方法来找到其最小值。我们再次将梯度置为零：\n$$\\nabla_x J_{\\mathrm{T}}(x) = -\\sum_{i=1}^{m} \\left(\\frac{\\nu+1}{\\nu\\sigma^2 + (y_i - (Hx)_i)^2}\\right) (y_i - (Hx)_i) H_i^T + P^{-1}(x-m) = 0$$\n其中 $H_i$ 是 $H$ 的第 $i$ 行。让我们定义一组依赖于当前 $x$ 估计值的权重 $w_i(x)$：\n$$w_i(x) = \\frac{\\nu+1}{\\nu\\sigma^2 + (y_i - (Hx)_i)^2}$$\n这些权重代表了每次测量的影响。具有大残差 $|y_i - (Hx)_i|$ 的离群值将获得较小的权重，这赋予了估计器鲁棒性。\n使用这些权重，梯度条件可以重写为矩阵形式，其中 $W(x)$ 是一个对角矩阵，其对角元素为 $w_i(x)$：\n$$H^T W(x) (Hx - y) + P^{-1}(x - m) = 0$$\n$$(H^T W(x) H + P^{-1}) x = H^T W(x) y + P^{-1} m$$\n该方程的结构提示了一种迭代重加权最小二乘 (IRLS) 算法。从初始猜测 $x^{(0)}$ 开始，我们可以对 $k=0, 1, 2, \\dots$ 进行如下迭代：\n1.  计算残差：$r^{(k)} = y - Hx^{(k)}$。\n2.  计算权重：$W^{(k)} = \\mathrm{diag}\\left(\\frac{\\nu+1}{\\nu\\sigma^2 + (r_i^{(k)})^2}\\right)$。\n3.  求解线性系统以获得下一个估计 $x^{(k+1)}$：\n    $$x^{(k+1)} = (H^T W^{(k)} H + P^{-1})^{-1} (H^T W^{(k)} y + P^{-1} m)$$\n重复此迭代直至收敛，即直到变化量 $\\|x^{(k+1)} - x^{(k)}\\|_2$ 小于一个很小的容差。一个合适的初始猜测是高斯 MAP 估计值，$x^{(0)} = x_{\\mathrm{G}}$。\n\n最终的程序实现了这两种估计器，并为指定的测试案例计算了所需的比较指标，从而展示了鲁棒的学生t分布模型在存在重尾噪声（离群值）的情况下的優越性能。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares MAP estimates under Gaussian and Student-t likelihoods\n    for a linear inverse problem across four test cases.\n    \"\"\"\n    # 1. Construct the fixed problem instance\n    M, N = 8, 2\n    H = np.array([\n        [1.0, 0.5],\n        [0.8, -0.3],\n        [-0.6, 1.2],\n        [0.0, 1.0],\n        [1.5, -0.7],\n        [-1.0, -0.2],\n        [0.3, 0.8],\n        [-0.4, 0.5]\n    ])\n    x_star = np.array([1.0, -2.0])\n    m_prior = np.array([0.0, 0.0])\n    P_prior = np.diag([10.0, 10.0])\n    sigma = 1.0\n\n    # Pre-compute the inverse of the prior covariance matrix\n    P_inv = np.linalg.inv(P_prior)\n    H_T = H.T\n\n    # 2. Define the test suite\n    test_cases = [\n        # Case 1: Heavy-tailed residuals, moderate nu\n        {'nu': 3.0, 'r': np.array([0.2, -0.1, 0.15, -0.05, 0.1, -0.1, 12.0, -15.0])},\n        # Case 2: Near-Gaussian behavior\n        {'nu': 30.0, 'r': np.array([0.05, -0.02, 0.03, -0.04, 0.01, -0.01, 0.02, -0.03])},\n        # Case 3: Extremely heavy-tailed, multiple large outliers\n        {'nu': 1.5, 'r': np.array([20.0, -25.0, 0.2, 0.0, -0.1, 18.0, -22.0, 0.3])},\n        # Case 4: Baseline with no residuals\n        {'nu': 3.0, 'r': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}\n    ]\n\n    all_case_results = []\n\n    # 3. Compute results for each case\n    for case in test_cases:\n        nu = case['nu']\n        r = case['r']\n        \n        # Form the observation vector y\n        y = H @ x_star + r\n\n        # Compute the Gaussian-likelihood MAP estimate (x_G)\n        A_G = H_T @ H + (sigma**2) * P_inv\n        b_G = H_T @ y + (sigma**2) * P_inv @ m_prior\n        x_G = np.linalg.solve(A_G, b_G)\n\n        # Compute the Student-t-likelihood MAP estimate (x_T) using IRLS\n        # Initialize the iteration with the Gaussian estimate\n        x_T = np.copy(x_G)\n        num_iterations = 50  # Sufficient for convergence in this problem\n        \n        for _ in range(num_iterations):\n            residuals = y - H @ x_T\n            \n            # Calculate weights for the current estimate\n            weights_diag = (nu + 1) / (nu * sigma**2 + residuals**2)\n            W = np.diag(weights_diag)\n            \n            # Form and solve the linear system for the next estimate\n            A_T = H_T @ W @ H + P_inv\n            b_T = H_T @ W @ y + P_inv @ m_prior\n            \n            x_T_new = np.linalg.solve(A_T, b_T)\n            \n            # Check for convergence\n            if np.linalg.norm(x_T_new - x_T)  1e-12:\n                x_T = x_T_new\n                break\n            \n            x_T = x_T_new\n        \n        # Calculate the required metrics\n        bias_G_norm = np.linalg.norm(x_G - x_star)\n        bias_T_norm = np.linalg.norm(x_T - x_star)\n        \n        # The prior ensures the denominator is non-zero, but handle for robustness\n        if bias_T_norm == 0.0:\n            rho = float('inf')\n        else:\n            rho = bias_G_norm / bias_T_norm\n            \n        diff_norm = np.linalg.norm(x_G - x_T)\n        \n        single_case_result = [bias_G_norm, bias_T_norm, rho, diff_norm]\n        all_case_results.append(single_case_result)\n\n    # Format and print the final output string\n    # E.g., [[a1,b1,c1,d1],[a2,b2,c2,d2]] with no spaces\n    output_str = ','.join(str(res).replace(' ', '') for res in all_case_results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3365464"}, {"introduction": "许多强大的反演与数据同化算法，如高斯–牛顿法（Gauss–Newton method），都依赖于对非线性模型进行局部线性化。本练习将探究这种线性化方法的一个关键失效点：当模型在某处不可微，形成“扭结”（kink）时。通过将真实的后验不确定性与高斯–牛顿法给出的近似不确定性进行比较，我们将揭示线性化假设在何处以及如何失效，并导致对不确定性的不准确量化。[@problem_id:3365429]", "problem": "考虑一个带高斯先验和分段线性观测算子的一维贝叶斯逆问题。设未知状态为实标量 $x \\in \\mathbb{R}$，其先验分布为 $x \\sim \\mathcal{N}(m_0, C_0)$，其中 $m_0 \\in \\mathbb{R}$ 且 $C_0  0$。设观测值 $y \\in \\mathbb{R}$ 通过分段线性观测算子 $h(x)$ 和方差为 $r  0$ 的加性高斯噪声与 $x$ 相关，因此似然函数为 $y \\mid x \\sim \\mathcal{N}(h(x), r)$。该观测算子由斜率 $a_1$ 和 $a_2$ 以及在 $x = 0$ 处的一个拐点定义：\n$$\nh(x) = \\begin{cases}\na_1 x,  \\text{for } x  0 \\\\\na_2 x,  \\text{for } x \\ge 0\n\\end{cases}\n$$\n负对数后验（忽略一个加性常数）为\n$$\n\\Phi(x) = \\tfrac{1}{2} \\left( \\frac{(x - m_0)^2}{C_0} + \\frac{(y - h(x))^2}{r} \\right).\n$$\n最大后验（MAP）估计是 $\\Phi(x)$ 在 $\\mathbb{R}$ 上的最小化子。在每个线性区域内，无约束最小化子由下式给出\n$$\nx^*(a) = \\frac{r m_0 + C_0 a y}{r + C_0 a^2}.\n$$\n由于在 $x=0$ 处存在拐点，必须通过将 $x^*(a_1)$ 限制在 $x  0$ 且将 $x^*(a_2)$ 限制在 $x \\ge 0$，并与边界候选项 $x = 0$ 进行比较来选择 MAP。\n\n假设通过高斯-牛頓法（Gauss–Newton method）构建高斯近似，该方法在 MAP 点处对观测进行线性化。在一维情况下，高斯-牛頓后验协方差近似使用 $h(x)$ 在 MAP 点处的雅可比矩阵 $H$。如果 MAP 严格位于可微区域内，则当 $x_{\\text{MAP}}  0$ 时 $H = a_1$，当 $x_{\\text{MAP}}  0$ 时 $H = a_2$，从而得到近似后验方差\n$$\nC_{\\text{GN}} = \\left( \\frac{1}{C_0} + \\frac{H^2}{r} \\right)^{-1}.\n$$\n如果 $x_{\\text{MAP}} = 0$，雅可比矩阵未定义，此时可以通过计算 $H = a_1$ 和 $H = a_2$ 两种单侧近似来评估恶化程度。\n\n通过对后验概率密度函数 (probability density function, PDF) 在 $\\mathbb{R}$上积分来定义精确后验方差：\n$$\np(x \\mid y) \\propto \\exp\\left( -\\tfrac{1}{2} \\left( \\frac{(x - m_0)^2}{C_0} + \\frac{(y - h(x))^2}{r} \\right) \\right),\n$$\n它在 $(-\\infty, 0)$ 和 $[0, \\infty)$ 上是分段定义的。精确的后验均值和二阶矩为\n$$\n\\mu = \\int_{-\\infty}^{\\infty} x \\, p(x \\mid y) \\, dx, \\quad M_2 = \\int_{-\\infty}^{\\infty} x^2 \\, p(x \\mid y) \\, dx,\n$$\n精确的后验方差为\n$$\nC_{\\text{exact}} = \\frac{M_2}{Z} - \\left( \\frac{\\mu}{Z} \\right)^2,\n$$\n其中归一化常数（也称为配分函数）为\n$$\nZ = \\int_{-\\infty}^{\\infty} p(x \\mid y) \\, dx.\n$$\n所有积分都可以分解为在 $(-\\infty, 0)$ 和 $[0, \\infty)$ 上的求和，每个区域使用相应的线性分支。\n\n您的任务是编写一个完整的、可运行的程序，该程序针对下面指定的测试套件，为每个测试用例执行以下步骤：\n- 通过使用各区域中的无约束最小化子和边界 $x=0$，最小化 $\\Phi(x)$ 在 $\\mathbb{R}$ 上的值，从而计算 $x_{\\text{MAP}}$。\n- 通过在 $(-\\infty, 0)$ 和 $[0, \\infty)$ 上使用确定性一维数值积分法，数值计算 $Z$、$\\mu$ 和 $M_2$，从而计算精确的后验方差 $C_{\\text{exact}}$。\n- 使用 $H = a_1$ 和 $H = a_2$ 计算高斯-牛頓后验方差近似值 $C_{\\text{GN}}$。如果 $x_{\\text{MAP}} \\neq 0$，则根据 $x_{\\text{MAP}}$ 的符号选择合适的 $H$，并计算单个相对误差 $E = \\left| C_{\\text{GN}} - C_{\\text{exact}} \\right| / C_{\\text{exact}}$。如果 $x_{\\text{MAP}} = 0$，则计算两种单侧近似，并报告最大相对误差 $E = \\max\\left( \\left| C_{\\text{GN}}(a_1) - C_{\\text{exact}} \\right|, \\left| C_{\\text{GN}}(a_2) - C_{\\text{exact}} \\right| \\right) / C_{\\text{exact}}$。\n- 为每个测试用例返回标量 $E$ 作为浮点数。\n\n您必须为以下测试套件实现该程序，该套件探测不同的情况：\n- 远离拐点的一般情况：$m_0 = 0$，$C_0 = 1$，$r = 0.25$，$a_1 = 1$，$a_2 = 2$，$y = 1$。\n- 位于拐点的边界情况：$m_0 = 0$，$C_0 = 1$，$r = 0.25$，$a_1 = 1$，$a_2 = 2$，$y = 0$。\n- 远离拐点的对侧情况：$m_0 = 0$，$C_0 = 1$，$r = 0.25$，$a_1 = 1$，$a_2 = 2$，$y = -0.5$。\n- 大噪声下的先验主导情况：$m_0 = 0.5$，$C_0 = 2$，$r = 10$，$a_1 = 1$，$a_2 = 3$，$y = 0.1$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[e_1,e_2,e_3,e_4]$），其中每个 $e_i$ 是相应测试用例的相对误差 $E$，以十进制数表示。不涉及物理单位，也不存在角度。计算中不允许有随机性。", "solution": "该问题关注的是，当观测算子在拐点处不可微时，作为逆问题和数据同化核心的线性和高斯性假设如何影响后验协方差估计。推导从带有高斯先验和高斯噪声的标准贝叶斯公式开始。观测算子非光滑但分段线性，因此在每个区域内，负对数后验是二次的。\n\n其基础是带有高斯先验和高斯似然的贝叶斯后验。先验为 $x \\sim \\mathcal{N}(m_0, C_0)$，似然为 $y \\mid x \\sim \\mathcal{N}(h(x), r)$。后验密度为\n$$\np(x \\mid y) \\propto \\exp\\left( -\\tfrac{1}{2} \\left( \\frac{(x - m_0)^2}{C_0} + \\frac{(y - h(x))^2}{r} \\right) \\right).\n$$\n因为 $h(x)$ 是分段线性的，所以指数在每个区间 $(-\\infty, 0)$ 和 $[0, \\infty)$上都是一个二次函数。具体来说，对于斜率 $a$ 定义，\n$$\nQ_a(x) = \\frac{(x - m_0)^2}{C_0} + \\frac{(y - a x)^2}{r}, \\quad \\text{so that} \\quad p(x \\mid y) \\propto \\exp\\left( -\\tfrac{1}{2} Q_{a_1}(x) \\right) \\text{ for } x  0, \\quad p(x \\mid y) \\propto \\exp\\left( -\\tfrac{1}{2} Q_{a_2}(x) \\right) \\text{ for } x \\ge 0.\n$$\n归一化子 $Z$、后验均值 $\\mu$ 和二阶矩 $M_2$ 的积分也相应地被分解：\n$$\nZ = \\int_{-\\infty}^{0} \\exp\\left( -\\tfrac{1}{2} Q_{a_1}(x) \\right) dx + \\int_{0}^{\\infty} \\exp\\left( -\\tfrac{1}{2} Q_{a_2}(x) \\right) dx,\n$$\n$$\n\\mu = \\int_{-\\infty}^{0} x \\exp\\left( -\\tfrac{1}{2} Q_{a_1}(x) \\right) dx + \\int_{0}^{\\infty} x \\exp\\left( -\\tfrac{1}{2} Q_{a_2}(x) \\right) dx,\n$$\n$$\nM_2 = \\int_{-\\infty}^{0} x^2 \\exp\\left( -\\tfrac{1}{2} Q_{a_1}(x) \\right) dx + \\int_{0}^{\\infty} x^2 \\exp\\left( -\\tfrac{1}{2} Q_{a_2}(x) \\right) dx.\n$$\n然后 $C_{\\text{exact}} = M_2 / Z - (\\mu / Z)^2$。这些是在半无限区间上对指数衰减函数的一维积分。尽管通过配方法和使用高斯误差函数可以得到闭式表达式，但稳健的数值积分法足以进行确定性评估，从而确保科学真实性并避免依赖随机近似。\n\n对于最大后验（MAP）估计，在斜率为 $a$ 的区域内，负对数后验为\n$$\n\\Phi_a(x) = \\tfrac{1}{2} \\left( \\frac{(x - m_0)^2}{C_0} + \\frac{(y - a x)^2}{r} \\right),\n$$\n其导数为\n$$\n\\frac{d}{dx} \\Phi_a(x) = \\frac{x - m_0}{C_0} - \\frac{a (y - a x)}{r}.\n$$\n令导数为零可得\n$$\n\\frac{x - m_0}{C_0} = \\frac{a (y - a x)}{r} \\quad \\Rightarrow \\quad x (r + C_0 a^2) = r m_0 + C_0 a y \\quad \\Rightarrow \\quad x^*(a) = \\frac{r m_0 + C_0 a y}{r + C_0 a^2}.\n$$\n每个区域中的无约束最小化子在 $(-\\infty, 0)$ 上是 $x^*(a_1)$，在 $[0, \\infty)$ 上是 $x^*(a_2)$。如果 $x^*(a_1)  0$，则它是可行的，否则该区域的最小化子出现在边界 $x = 0$ 处。类似地，如果 $x^*(a_2) \\ge 0$，则它是可行的，否则该区域的最小化子在 $x = 0$ 處。全局 MAP 是可行区域最小化子和边界点中使 $\\Phi(x)$ 值最小的点。\n\n对于高斯-牛頓协方差近似，在一维情况下，MAP 点处的线性化模型使用局部雅可比矩阵 $H$。如果 $x_{\\text{MAP}}  0$，则 $H = a_1$；如果 $x_{\\text{MAP}} > 0$，则 $H = a_2$。近似后验方差为\n$$\nC_{\\text{GN}} = \\left( \\frac{1}{C_0} + \\frac{H^2}{r} \\right)^{-1}.\n$$\n这是模型 $y \\approx H x$（其中 $x \\sim \\mathcal{N}(m_0, C_0)$ 且 $y \\mid x \\sim \\mathcal{N}(H x, r)$）的精确线性高斯后验方差。当 $x_{\\text{MAP}} = 0$ 时，雅可比矩阵未定义；为评估不可微点附近的恶化情况，我们使用 $H = a_1$ 和 $H = a_2$ 计算两种单侧近似，并取最大相对误差来表示最坏情况下的差异：\n$$\nE = \\begin{cases}\n\\left| C_{\\text{GN}} - C_{\\text{exact}} \\right| / C_{\\text{exact}},  x_{\\text{MAP}} \\ne 0, \\\\\n\\max\\left( \\left| C_{\\text{GN}}(a_1) - C_{\\text{exact}} \\right|, \\left| C_{\\text{GN}}(a_2) - C_{\\text{exact}} \\right| \\right) / C_{\\text{exact}},  x_{\\text{MAP}} = 0.\n\\end{cases}\n$$\n每个测试用例的算法步骤：\n- 根据上述公式计算 $x^*(a_1)$ 和 $x^*(a_2)$。确定可行性（$x^*(a_1)  0$, $x^*(a_2) \\ge 0$）。使用 $h(x)$ 的适当分支，计算可行最小化子处和 $x=0$ 处的 $\\Phi(x)$，并选择最小化点 $x_{\\text{MAP}}$。\n- 在 $(-\\infty, 0)$ 上使用 $Q_{a_1}(x)$ 以及在 $[0, \\infty)$ 上使用 $Q_{a_2}(x)$ 定义 $Z$、$\\mu$ 和 $M_2$ 的被积函数。使用半无限区间上的确定性数值积分法对它们进行求值。然后计算 $C_{\\text{exact}} = M_2 / Z - (\\mu / Z)^2$。\n- 计算 $C_{\\text{GN}}$：\n  - 如果 $x_{\\text{MAP}}  0$，使用 $H = a_1$ 进行计算。\n  - 如果 $x_{\\text{MAP}} > 0$，使用 $H = a_2$ 进行计算。\n  - 如果 $x_{\\text{MAP}} = 0$，同时使用 $H = a_1$ 和 $H = a_2$ 进行计算，并取最大相对误差。\n- 以浮点数形式输出相对误差 $E$。\n\n此过程测试了线性和高斯性假设对不可微点附近协方差估计的影响。远离拐点时，分段线性模型局部表现得像一个仿射映射，预计高斯-牛頓法表现良好，产生较小的相对误差。在拐点处或附近，雅可比矩阵是不明确的；后验是区域截断高斯分布的类混合集合，单侧高斯-牛頓协方差近似可能会显著偏离，这反映为较大的相对误差。在 $r$ 较大、由先验主导的情况下，后验接近于先验，协方差差异减小，因为 $H^2 / r$ 很小，这与线性高斯后验协方差的基本公式一致。", "answer": "```python\n# Python 3.12 program using numpy 1.23.5 and scipy 1.11.4\n# Computes relative errors between Gauss–Newton covariance approximation and exact posterior covariance\n# for a 1D Bayesian inverse problem with a piecewise-linear observation operator with a kink.\n\nimport numpy as np\nfrom math import inf\nfrom scipy.integrate import quad\n\ndef phi(x, m0, C0, y, a1, a2):\n    \"\"\"Negative log-posterior (up to constant), branch-aware.\"\"\"\n    a = a1 if x  0 else a2\n    return 0.5 * ((x - m0)**2 / C0 + (y - a * x)**2 / np.float64(1.0) / r)  # r is not defined here; will not use globally.\n\ndef phi_branch(x, m0, C0, y, a):\n    \"\"\"Negative log-posterior (up to constant) for a specific slope a.\"\"\"\n    return 0.5 * ((x - m0)**2 / C0 + (y - a * x)**2 / r_global)\n\ndef unconstrained_minimizer(m0, C0, y, r, a):\n    \"\"\"Compute unconstrained minimizer in a region with slope a.\"\"\"\n    return (r * m0 + C0 * a * y) / (r + C0 * a * a)\n\ndef phi_value(x, m0, C0, y, a1, a2):\n    \"\"\"Compute phi using correct branch for x.\"\"\"\n    a = a1 if x  0 else a2\n    return 0.5 * ((x - m0)**2 / C0 + (y - a * x)**2 / r_global)\n\ndef exact_posterior_variance(m0, C0, y, r, a1, a2):\n    \"\"\"Compute exact posterior variance via numerical quadrature.\"\"\"\n    def Q(a, x):\n        return ((x - m0)**2 / C0) + ((y - a * x)**2 / r)\n    # Integrands\n    def dens_left(x):\n        return np.exp(-0.5 * Q(a1, x))\n    def dens_right(x):\n        return np.exp(-0.5 * Q(a2, x))\n\n    def x_dens_left(x):\n        return x * np.exp(-0.5 * Q(a1, x))\n    def x_dens_right(x):\n        return x * np.exp(-0.5 * Q(a2, x))\n\n    def x2_dens_left(x):\n        return x * x * np.exp(-0.5 * Q(a1, x))\n    def x2_dens_right(x):\n        return x * x * np.exp(-0.5 * Q(a2, x))\n\n    # Integrate with high precision over semi-infinite intervals\n    Z_left, _ = quad(dens_left, -np.inf, 0.0, epsabs=1e-12, epsrel=1e-12, limit=200)\n    Z_right, _ = quad(dens_right, 0.0, np.inf, epsabs=1e-12, epsrel=1e-12, limit=200)\n    Z = Z_left + Z_right\n\n    mu_left, _ = quad(x_dens_left, -np.inf, 0.0, epsabs=1e-12, epsrel=1e-12, limit=200)\n    mu_right, _ = quad(x_dens_right, 0.0, np.inf, epsabs=1e-12, epsrel=1e-12, limit=200)\n    mu = mu_left + mu_right\n\n    m2_left, _ = quad(x2_dens_left, -np.inf, 0.0, epsabs=1e-12, epsrel=1e-12, limit=200)\n    m2_right, _ = quad(x2_dens_right, 0.0, np.inf, epsabs=1e-12, epsrel=1e-12, limit=200)\n    m2 = m2_left + m2_right\n\n    mu_norm = mu / Z\n    var = m2 / Z - mu_norm * mu_norm\n    return var\n\ndef gauss_newton_variance(C0, r, H):\n    \"\"\"Gauss–Newton approximate posterior variance in 1D.\"\"\"\n    return 1.0 / (1.0 / C0 + (H * H) / r)\n\ndef compute_MAP(m0, C0, y, r, a1, a2):\n    \"\"\"Compute MAP by comparing region-wise unconstrained minimizers and boundary.\"\"\"\n    x1 = (r * m0 + C0 * a1 * y) / (r + C0 * a1 * a1)\n    x2 = (r * m0 + C0 * a2 * y) / (r + C0 * a2 * a2)\n    # Feasible region minimizers\n    candidates = []\n    # Region 1: x  0\n    if x1  0:\n        candidates.append(x1)\n    else:\n        candidates.append(0.0)  # boundary for region 1\n    # Region 2: x >= 0\n    if x2 >= 0:\n        candidates.append(x2)\n    else:\n        candidates.append(0.0)  # boundary for region 2\n    # Always consider boundary explicitly\n    candidates.append(0.0)\n\n    # Evaluate objective for each candidate with correct branch\n    def phi_eval(x):\n        a = a1 if x  0 else a2\n        return 0.5 * ((x - m0)**2 / C0 + (y - a * x)**2 / r)\n\n    phi_values = [(x, phi_eval(x)) for x in candidates]\n    # Choose minimizer; in case of ties prefer 0 to flag kink\n    min_phi = min(phi_values, key=lambda pair: (pair[1], abs(pair[0])))  # prefer smaller |x| if equal phi\n    x_map = min_phi[0]\n    return x_map\n\ndef process_case(m0, C0, r, a1, a2, y):\n    \"\"\"Compute the relative error E for a single test case.\"\"\"\n    # Compute exact variance\n    var_exact = exact_posterior_variance(m0, C0, y, r, a1, a2)\n    # MAP\n    x_map = compute_MAP(m0, C0, y, r, a1, a2)\n    # Gauss–Newton variance(s)\n    if x_map  0:\n        H = a1\n        var_gn = gauss_newton_variance(C0, r, H)\n        E = abs(var_gn - var_exact) / var_exact\n    elif x_map > 0:\n        H = a2\n        var_gn = gauss_newton_variance(C0, r, H)\n        E = abs(var_gn - var_exact) / var_exact\n    else:\n        var_gn_left = gauss_newton_variance(C0, r, a1)\n        var_gn_right = gauss_newton_variance(C0, r, a2)\n        E_left = abs(var_gn_left - var_exact) / var_exact\n        E_right = abs(var_gn_right - var_exact) / var_exact\n        E = max(E_left, E_right)\n    return E\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (m0, C0, r, a1, a2, y)\n    test_cases = [\n        (0.0, 1.0, 0.25, 1.0, 2.0, 1.0),      # General case away from kink\n        (0.0, 1.0, 0.25, 1.0, 2.0, 0.0),      # Boundary case at kink\n        (0.0, 1.0, 0.25, 1.0, 2.0, -0.5),     # Opposite-side case away from kink\n        (0.5, 2.0, 10.0, 1.0, 3.0, 0.1),      # Prior-dominated case with large noise\n    ]\n\n    results = []\n    for (m0, C0, r, a1, a2, y) in test_cases:\n        # Store r globally for branch-specific phi in safe manner (used only in helper functions)\n        global r_global\n        r_global = r  # assign for functions expecting r_global\n        E = process_case(m0, C0, r, a1, a2, y)\n        # Ensure a clean float representation\n        results.append(E)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda v: f'{v:.12g}', results))}]\")\n\n# Global variable for branch evaluation\nr_global = None\n\nsolve()\n```", "id": "3365429"}]}