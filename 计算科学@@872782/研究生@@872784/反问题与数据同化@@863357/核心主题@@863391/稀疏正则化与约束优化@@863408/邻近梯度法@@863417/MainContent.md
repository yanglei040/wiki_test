## 引言
在[科学计算](@entry_id:143987)、数据科学和机器学习的众多前沿领域，我们面临的[优化问题](@entry_id:266749)往往具有一种特殊的复合结构：[目标函数](@entry_id:267263)由一个光滑的数据拟合项和一个非光滑的正则化项组成。前者量化模型与观测的匹配度，后者则引入关于解的期望性质（如[稀疏性](@entry_id:136793)或物理约束）的先验知识。虽然标准梯度下降法无法处理非光滑项，但近端梯度法（Proximal Gradient Methods）为此类问题提供了一个优雅而强大的求解框架。它已成为解决[大规模反问题](@entry_id:751147)、实现正则化学习和执行[变分数据同化](@entry_id:756439)的核心工具。

本文旨在系统性地介绍近端梯度法的理论、应用与实践。我们首先将揭示该算法的内在机理，然后探索其在不同学科中的广泛应用，最后通过动手实践来巩固理解。通过学习本文，您将能够：

- 在第一章“原理与机制”中，深入理解近端梯度法如何通过分离光滑项和非光滑项进行迭代，掌握其[收敛条件](@entry_id:166121)和步长选择策略，并了解FISTA加速和非凸扩展等高级主题。
- 在第二章“应用与跨学科联系”中，见证该方法如何应用于信号与[图像处理](@entry_id:276975)中的[稀疏恢复](@entry_id:199430)、地球物理中的反问题求解，以及如何与[贝叶斯推断](@entry_id:146958)、[分布式优化](@entry_id:170043)和深度学习等概念产生深刻联系。
- 在第三章“动手实践”中，通过一系列精心设计的编程练习，将理论知识转化为实际的编程技能，亲手实现一个用于[求解PDE](@entry_id:138485)约束[反问题](@entry_id:143129)的近端梯度求解器。

让我们开始深入探索这一连接理论与实践的强大[优化方法](@entry_id:164468)。

## 原理与机制

本章深入探讨近端梯度方法的核心原理与机制。我们将剖析这些方法旨在解决的复合目标函数，理解算法的“分离-求解”迭代机理，建立其收敛性的理论保证，并探索步长选择等实际应用细节。最后，我们会将该方法置于更广阔的背景中，通过与替代方案的比较，并讨论其在加速和非凸领域的几个重要扩展。

### 复合目标函数：连接模型与算法的桥梁

近端梯度法旨在解决一类具有特定结构的[优化问题](@entry_id:266749)，其[目标函数](@entry_id:267263)可以分解为两部分之和：
$$
\min_{x \in \mathbb{R}^n} F(x) = f(x) + g(x)
$$
其中 $f(x)$ 是一个光滑、可微的函数，而 $g(x)$ 是一个[凸函数](@entry_id:143075)，但可能非光滑。这种“光滑+非光滑”的复合结构广泛存在于[逆问题](@entry_id:143129)、数据同化和机器学习的诸多领域，其根源常常可以追溯到贝叶斯统计框架。

#### 数据保真项 $f(x)$

$f(x)$ 通常被称为**数据保真项**或**[数据失配](@entry_id:748209)项**。它量化了一个候选解 $x$ 对观测数据的解释程度。在概率论的语境下，$f(x)$ 常常对应于给定参数 $x$ 时数据出现的[负对数似然](@entry_id:637801)。

例如，考虑一个[线性逆问题](@entry_id:751313)，其观测模型为 $b = A x_{\star} + \varepsilon$，其中 $\varepsilon$ 假定为零均值且具有正定协方差矩阵 $\Sigma$ 的高斯噪声。在这种情况下，给定状态 $x$ ，观测到数据 $b$ 的似然函数正比于 $\exp(-\frac{1}{2}(Ax-b)^T \Sigma^{-1} (Ax-b))$。因此，最大化[似然](@entry_id:167119)等价于最小化[负对数似然](@entry_id:637801)，即最小化一个加权最小二乘函数：
$$
f(x) = \frac{1}{2}\|Ax-b\|_W^2 = \frac{1}{2}(Ax-b)^T W (Ax-b)
$$
其中 $W = \Sigma^{-1}$ 是[精度矩阵](@entry_id:264481)。这个二次函数是凸的，并且其梯度连续可微，满足近端梯度法所要求的光滑性条件 [@problem_id:3415722]。

#### 正则化项 $g(x)$

$g(x)$ 是**正则化项**或**惩罚项**。在**病态 (ill-posed)** 逆问题中，它的作用至关重要。在这类问题中，仅仅最小化数据保真项 $f(x)$ 往往无法得到稳定或有意义的解。病态性通常源于正向算子 $A$ 的奇异值快速衰减，这使得解对数据中的噪声极其敏感。正则化项通过引入关于解的期望性质的**先验知识**，从而稳定逆过程 [@problem_id:3415741]。

在贝叶斯框架中，$g(x)$ 对应于 $x$ 的先验概率[分布](@entry_id:182848)的负对数，即 $p(x) \propto \exp(-g(x))$ [@problem_id:3415722]。$g(x)$ 的常见选择旨在促进解的特定结构：

*   **[稀疏性](@entry_id:136793) (Sparsity):** 为了促进解中包含大量零元素，**[L1范数](@entry_id:143036)**是一个非常流行的选择，$g(x) = \lambda \|x\|_1$。这对应于假设 $x$ 的各个分量服从独立的拉普拉斯[先验分布](@entry_id:141376) [@problem_id:3415726]。

*   **分段常数解 (Piecewise-Constant Solutions):** 在[图像处理](@entry_id:276975)中，我们常常期望解是分段常数的（例如，包含清晰边缘但区域平坦的图像）。**全变分 (Total Variation, TV)** 正则化器，$g(x) = \lambda \|Dx\|_1$（其中 $D$ 是[离散梯度](@entry_id:171970)算子），对此非常有效。它惩罚解的梯度的[L1范数](@entry_id:143036)，从而促进稀疏梯度 [@problem_id:3415741]。

*   **约束 (Constraints):** 硬约束可以通过**[示性函数](@entry_id:261577) (indicator functions)** 来编码。对于一个代表可行解（例如，非负性约束 $x_i \ge 0$）的闭[凸集](@entry_id:155617) $C$，正则化项可以是 $g(x) = i_C(x)$，该函数在 $x \in C$ 时为零，否则为 $+\infty$。

参数 $\lambda > 0$（在某些文献中也记为 $\alpha$）是**[正则化参数](@entry_id:162917)**，它控制着拟[合数](@entry_id:263553)据（最小化 $f(x)$）和满足先验知识（最小化 $g(x)$）之间的权衡。这反映了经典的**[偏差-方差权衡](@entry_id:138822) (bias-variance trade-off)**：较大的 $\lambda$ 会降低解的[方差](@entry_id:200758)（对噪声不那么敏感），但代价是增加偏差（解被进一步拉向远离纯[数据拟合](@entry_id:149007)的方向）[@problem_id:3415741]。

### 近端梯度迭代：分离求解

近端梯度方法的核心思想是“分离”处理[目标函数](@entry_id:267263)的两个组成部分。在每次迭代中，它执行：

1.  一个关于光滑部分 $f(x)$ 的**梯度步**（或称“前向”步）。
2.  一个关于非光滑部分 $g(x)$ 的**近端步**（或称“后向”步）。

该算法的迭代更新规则如下：
$$
x^{k+1} = \operatorname{prox}_{\alpha g}\left(x^k - \alpha \nabla f(x^k)\right)
$$
其中 $\alpha > 0$ 是步长。

函数 $g$ 的**[近端算子](@entry_id:635396) (proximal operator)**（参数为 $\gamma > 0$）定义为：
$$
\operatorname{prox}_{\gamma g}(v) = \arg\min_{u \in \mathbb{R}^n} \left\{ g(u) + \frac{1}{2\gamma} \|u - v\|^2 \right\}
$$
这个定义可以被理解为寻找一个点 $u$，它在最小化 $g(u)$ 和保持与输入点 $v$ 的二次邻近性之间取得平衡。近端步有效地根据 $g$ 所倡导的结构，对梯度步的结果 $v = x^k - \alpha \nabla f(x^k)$ 进行“去噪”或“修正”。

#### 示例：[L1范数](@entry_id:143036)与[软阈值算子](@entry_id:755010)

一个阐明此机制的典型例子是当 $g(x) = \lambda \|x\|_1$ 时。这种正则化是统计学中LASSO方法的核心。我们需要计算[近端算子](@entry_id:635396) $\operatorname{prox}_{\alpha (\lambda \|\cdot\|_1)}$。由于[L1范数](@entry_id:143036)是可分离的（$g(x) = \sum_i \lambda |x_i|$），最小化问题可以分解为 $n$ 个独立的标量问题：
$$
[\operatorname{prox}_{\alpha\lambda\|\cdot\|_1}(v)]_i = \arg\min_{u_i \in \mathbb{R}} \left\{ \lambda|u_i| + \frac{1}{2\alpha}(u_i - v_i)^2 \right\}
$$
这个标量问题的解是众所周知的**[软阈值算子](@entry_id:755010) (soft-thresholding operator)**，记为 $\mathcal{S}_{\tau}$，其阈值为 $\tau = \alpha\lambda$：
$$
\mathcal{S}_{\tau}(v_i) = \operatorname{sgn}(v_i) \max(|v_i| - \tau, 0)
$$
以向量形式，对于一个由高斯[似然](@entry_id:167119)和拉普拉斯先验导出的L2-L1问题，其更新规则为 [@problem_id:3415726]：
$$
x^{k+1} = \mathcal{S}_{\alpha\lambda}\left(x^k - \alpha \nabla f(x^k)\right)
$$
其中算子是逐元素作用的。这个优雅的闭式解是[L1正则化](@entry_id:751088)广受欢迎的关键原因之一。

让我们通过一个具体的数值例子来追踪一次完整的迭代过程 [@problem_id:2163980]。考虑LASSO问题，其中 $f(\mathbf{w}) = \frac{1}{2} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2$ 且 $g(\mathbf{w}) = \lambda \|\mathbf{w}\|_1$。
给定：
- $\mathbf{X} = \begin{pmatrix} 1  1 \\ 1  -1 \end{pmatrix}$，$\mathbf{y} = \begin{pmatrix} 5 \\ 1 \end{pmatrix}$，$\lambda = 1$。
- 初始迭代点 $\mathbf{w}_0 = \begin{pmatrix} 2 \\ 3 \end{pmatrix}$ 和步长 $t = 0.5$。

1.  **梯度步:** 首先，计算 $f$ 在 $\mathbf{w}_0$ 处的梯度。
    $\nabla f(\mathbf{w}) = \mathbf{X}^T(\mathbf{X}\mathbf{w} - \mathbf{y})$。
    $\nabla f(\mathbf{w}_0) = \begin{pmatrix} 1  1 \\ 1  -1 \end{pmatrix} \left( \begin{pmatrix} 1  1 \\ 1  -1 \end{pmatrix} \begin{pmatrix} 2 \\ 3 \end{pmatrix} - \begin{pmatrix} 5 \\ 1 \end{pmatrix} \right) = \begin{pmatrix} -2 \\ 2 \end{pmatrix}$。
    [梯度下降](@entry_id:145942)更新得到一个中间点 $\mathbf{u}$：
    $\mathbf{u} = \mathbf{w}_0 - t \nabla f(\mathbf{w}_0) = \begin{pmatrix} 2 \\ 3 \end{pmatrix} - 0.5 \begin{pmatrix} -2 \\ 2 \end{pmatrix} = \begin{pmatrix} 3 \\ 2 \end{pmatrix}$。

2.  **近端步:** 将[软阈值算子](@entry_id:755010)应用于 $\mathbf{u}$，阈值为 $\tau = t\lambda = 0.5 \times 1 = 0.5$。
    $\mathbf{w}_1 = \mathcal{S}_{0.5}(\mathbf{u}) = \begin{pmatrix} \operatorname{sgn}(3)\max(|3|-0.5, 0) \\ \operatorname{sgn}(2)\max(|2|-0.5, 0) \end{pmatrix} = \begin{pmatrix} 2.5 \\ 1.5 \end{pmatrix}$。

更新后的向量为 $\mathbf{w}_1 = \begin{pmatrix} 2.5 \\ 1.5 \end{pmatrix}$。

### 收敛保证与下降性质

要使近端梯度法成为一个可靠的工具，我们需要理解它在何种条件下能够收敛到一个解。对于凸问题，其理论已经非常完善。

#### 收敛的基本条件

近端梯度法的标准[全局收敛性](@entry_id:635436)结果要求满足以下一组条件 [@problem_id:3415722]：

1.  **关于 $f$ 的性质**: 函数 $f$ 必须是凸的且连续可微。此外，其梯度 $\nabla f$ 必须是**利普希茨连续 (Lipschitz continuous)** 的，即存在一个常数 $L > 0$，使得对于所有 $x, y \in \mathbb{R}^n$：
    $$
    \|\nabla f(x) - \nabla f(y)\|_2 \le L \|x - y\|_2
    $$
    常数 $L$ 被称为[利普希茨常数](@entry_id:146583)。

2.  **关于 $g$ 的性质**: 函数 $g$ 必须是**真 (proper)** 的（即其定义域非空）、**下半连续 (lower semicontinuous)** 和**凸**的。这些性质确保了其[近端算子](@entry_id:635396)对于任何步长 $\alpha > 0$ 都是良定义且单值的。

3.  **存在最小点**: 整体[目标函数](@entry_id:267263) $F(x)$ 的最小点集合必须非空。

4.  **步长**: 步长 $\alpha$ 的选择必须恰当，通常在范围 $0  \alpha  2/L$ 内。

#### [下降引理](@entry_id:636345)：量化每步的进展

关于该算法为何有效的核心洞见来自于一个“[下降引理](@entry_id:636345)”，它表明在适当条件下，[目标函数](@entry_id:267263)值在每次迭代中都会减小。对于满足 $0  \alpha \le 1/L$ 的步长，我们可以保证 $F(x^{k+1}) \le F(x^k)$。更精确地，我们可以建立一个更紧的界，来量化每一步的进展 [@problem_id:495739] [@problem_id:3415735]。

这可以通过组合两个关键不等式来推导。首先，$f$ 的 $L$-光滑性意味着对于任何 $x, y$：
$$
f(y) \le f(x) + \langle \nabla f(x), y-x \rangle + \frac{L}{2}\|y-x\|^2
$$
其次，从[近端算子](@entry_id:635396)的定义中可以推导出一个基本性质：
$$
g(x^{k+1}) \le g(x^k) - \langle \nabla f(x^k), x^{k+1}-x^k \rangle - \frac{1}{2\alpha}\|x^{k+1}-x^k\|^2
$$
将这两个不等式相加（令 $x=x^k$ 且 $y=x^{k+1}$）即可得到[下降引理](@entry_id:636345)：
$$
F(x^{k+1}) \le F(x^k) - \left(\frac{1}{2\alpha} - \frac{L}{2}\right) \|x^{k+1} - x^k\|^2
$$
这个不等式至关重要。它表明，如果选择的步长 $\alpha$ 满足 $\frac{1}{2\alpha} - \frac{L}{2} > 0$，即 $\alpha  1/L$，那么在 $x^{k+1} \neq x^k$ 的每次迭代中，目标函数值都会严格减小。这确保了算法能够稳步地朝向最小值前进。

### 步长的实际选择策略

[收敛理论](@entry_id:176137)的关键在于步长 $\alpha$ 与[利普希茨常数](@entry_id:146583) $L$ 的正确关联。在实践中，我们主要有两种策略来选择 $\alpha$。

#### 固定步长

最简单的方法是在所有迭代中使用固定的步长 $\alpha$。最常见的选择是 $\alpha = 1/L$，它能保证目标函数的单调下降。这要求计算或估计 $\nabla f$ 的[利普希茨常数](@entry_id:146583) $L$。

在许多数据同化问题中，$f(x)$ 是一个加权最小二乘项 $f(x) = \frac{1}{2}\|Hx-y\|_{R^{-1}}^2$。它的梯度是 $\nabla f(x) = H^T R^{-1} (Hx-y)$，海森矩阵是 $H^T R^{-1} H$。最紧的[利普希茨常数](@entry_id:146583) $L$ 是这个[海森矩阵](@entry_id:139140)的最大[特征值](@entry_id:154894) [@problem_id:3415744]：
$$
L = \lambda_{\max}(H^T R^{-1} H)
$$
显式地构造矩阵 $H^T R^{-1} H$ 并计算其最大[特征值](@entry_id:154894)在计算上可能是昂贵的，尤其对于大规模问题。一个更实用的方法是使用**幂法 (power method)** 来估计 $L$，该方法仅需要矩阵-向量乘积。[幂法](@entry_id:148021)通过迭代计算 $v_{k+1} = (H^T R^{-1} H) v_k$ 并使用[瑞利商](@entry_id:137794)来逼近 $\lambda_{\max}$。关键在于，乘积 $(H^T R^{-1} H) v$ 可以通过一系列操作以“无矩阵”的方式计算：(1) 一次[前向传播](@entry_id:193086) $w = Hv$，(2) 一次与协方差矩阵的求解 $u = R^{-1}w$，以及 (3) 一次伴随传播 $z = H^T u$ [@problem_id:3415744]。

例如，给定具体的矩阵 $H$ 和 $R$，我们可以精确计算 $L$。对于 $H = \begin{pmatrix} 1  0 \\ 0  1 \\ 1  1 \end{pmatrix}$ 和 $R = \mathrm{diag}(1, 4, 1)$，矩阵 $M = H^T R^{-1} H$ 为 $\begin{pmatrix} 2  1 \\ 1  5/4 \end{pmatrix}$。它的最大[特征值](@entry_id:154894)是 $L = \frac{13 + \sqrt{73}}{8}$。因此，最优的固定步长是 $\alpha^\star = 1/L = \frac{13 - \sqrt{73}}{12}$ [@problem_id:3415744]。

#### [回溯线搜索](@entry_id:166118)

当 $L$ 未知、难以估计或估计值过于保守（导致步长非常小）时，一种更具适应性的策略是**[回溯线搜索](@entry_id:166118) (backtracking line search)**。这种方法完全避免了计算 $L$。

其思想是，从一个初始的步长猜测开始（例如 $\gamma_{\mathrm{init}}=1$），然后通过一个因子 $\beta \in (0,1)$（例如 $\beta=0.5$）逐步减小它，直到满足一个特定的接受准则。这个准则确保了充分的下降，而又不过于严格。一个标准的接受准则是从 $f$ 的[下降引理](@entry_id:636345)中导出的：
$$
f(x^{k+1}) \le f(x^{k}) + \langle \nabla f(x^{k}), x^{k+1} - x^{k} \rangle + \frac{1}{2\gamma} \|x^{k+1} - x^{k}\|^{2}
$$
其中 $x^{k+1}$ 是使用试验步长 $\gamma$ 计算出的候选点。正如我们之前证明的，任何满足此不等式的步长 $\gamma$ 都保证了总目标函数的下降，即 $F(x^{k+1}) \le F(x^k)$ [@problem_id:3415735]。[线搜索算法](@entry_id:139123)迭代地减小 $\gamma$（例如 $\gamma \leftarrow \beta\gamma$）并重新计算 $x^{k+1}$，直到该条件成立。与保守的固定步长相比，这种自适应方法在实践中通常能采用更大的步长，从而实现更快的收敛。一个具体的计算例子可以展示，从 $\gamma=1$ 开始，步长可能需要被缩减数次（例如，到 $1/2, 1/4, 1/8, \dots$）才能满足条件 [@problem_id:3415735]。

### 方法比较与优势分析

近端梯度法的强大之处，在与基于平滑的替代策略进行比较时，得到了最好的体现。

#### 为何不直接平滑处理？

一个自然的问题是：为什么不将[非光滑函数](@entry_id:175189) $g(x)$ 用一个光滑函数 $g_\mu(x)$ 来近似，然后对 $f(x) + g_\mu(x)$ 应用标准的[梯度下降法](@entry_id:637322)呢？虽然这种方法可行，但与近端梯度法相比，它有几个显著的缺点 [@problem_id:3415775]：

*   **精度与稳定性权衡:** 为了很好地逼近原问题，平滑参数 $\mu$ 必须非常小。然而，当 $\mu \to 0$ 时，平滑后函数的梯度会变得非常陡峭（其[利普希茨常数](@entry_id:146583)与 $1/\mu$ 成正比）。这迫使梯度下降法必须使用极小的步长，从而导致收敛缓慢。近端梯度法通过精确处理 $g$ 来避免这个问题，其步长仅依赖于 $f$ 的光滑度。

*   **保持解的结构:** [近端算子](@entry_id:635396)可以保持关键的结构特性。例如，[L1范数](@entry_id:143036)的[软阈值算子](@entry_id:755010)能够产生具有精确零值的迭代点，从而直接促进[稀疏性](@entry_id:136793)。而[L1范数](@entry_id:143036)的一个平滑版本（如Huber函数）只会产生接近零但不完全为零的值。类似地，如果 $g$ 是一个凸集的[示性函数](@entry_id:261577)，[近端算子](@entry_id:635396)就变成一个投影算子，确保每次迭代的结果都是可行的。而平滑[惩罚方法](@entry_id:636090)可能导致不可行的迭代点。

*   **无偏解:** 近端梯度法收敛到原始非光滑目标 $F(x)$ 的一个真正最小点。而[平滑方法](@entry_id:754982)收敛到*近似*目标 $F_\mu(x)$ 的最小点，这本质上是一个有偏的解。要消除这种偏差，需要一个复杂的策略，即让 $\mu$ 趋向于零，但这通常会使整体计算复杂度劣于直接的近端方法。

### 高级主题：加速与非凸扩展

#### 加速近端梯度法 (FISTA)

基础的近端梯度法（在L1情况下常被称为ISTA，即迭代收缩-阈值算法）的收敛速率为 $\mathcal{O}(1/k)$，这可能很慢。通过应用[Nesterov的加速](@entry_id:752417)技术，可以构建一个**快速ISTA (FISTA)**，它能达到更快的理论收敛速率 $\mathcal{O}(1/k^2)$。

FISTA通过引入一个“动量”项来实现加速，它不是在当前迭代点 $x^k$ 处执行近端梯度步，而是在一个外推点 $y^k = x^k + \beta_k(x^k - x^{k-1})$ 处执行。虽然这种加速很强大，但它也付出了代价：目标函数序列 $\{F(x^k)\}$ 不再保证是单调的（非增的）[@problem_id:3415751]。动量步的“[过冲](@entry_id:147201)”可能会暂时增加目标函数值。

这种非单调性给设计稳健的[停止准则](@entry_id:136282)带来了麻烦。一个实用的解决方案是**单调FISTA (Monotone FISTA)**，它增加了一个安全检查。在计算出加速候选点 $\tilde{x}^{k+1}$ 后，它会检查是否有 $F(\tilde{x}^{k+1}) \le F(x^k)$。如果成立，则接受该步骤。如果不成立，算法将拒绝该加速步，转而执行一个标准的、非加速的ISTA步，并重置动量。这种方法以每次迭代中少量额外的计算工作为代价，换取了稳定、非增的[目标函数](@entry_id:267263)序列的保证，结合了两者的优点：在可能的情况下快速收敛，并始终保持稳健的下降特性 [@problem_id:3415751]。

#### 超越凸性：处理[非凸正则化](@entry_id:636532)项

为了在[稀疏恢复](@entry_id:199430)等任务中取得更好的性能，研究人员提出了[非凸正则化](@entry_id:636532)器，例如[平滑裁剪绝对偏差](@entry_id:635969) (SCAD) 惩罚或 $\ell_p$ 惩罚（其中 $p \in (0,1)$）。与凸的 $\ell_1$ 范数相比，这些惩罚项可以提供更稀疏且偏差更小的解。

将近端梯度框架应用于非凸的 $g$ 需要更高级的理论工具 [@problem_id:3415772]：

*   **近端映射的变化:** [近端算子](@entry_id:635396)仍然是良定义的（即最小点存在），但最小化子问题现在可能有多个解。这意味着近端映射 $\operatorname{Prox}_{\alpha g}$ 可能是**集值 (set-valued)** 的。此外，它失去了在凸情况下作为收敛性证明基础的关键性质——**紧非扩[张性](@entry_id:141857) (firmly non-expansive)**。

*   **收敛到[临界点](@entry_id:144653):** 由于目标函数是非凸的，我们不再能期望算法收敛到全局最小值。取而代之的是，算法被证明会收敛到一个**[临界点](@entry_id:144653) (critical point)**（即满足一阶必要[最优性条件](@entry_id:634091)的点，其[次微分](@entry_id:175641)包含零）。

*   **Kurdyka-Łojasiewicz (KL) 性质:** [非凸优化](@entry_id:634396)领域的一个重大突破是利用KL性质来证明*整个迭代序列*的收敛性。实践中遇到的许多函数，包括半[代数函数](@entry_id:187534)（涵盖了多项式和有理函数，如当 $p$ 为有理数时的 $\ell_p$ 惩罚），都满足KL性质。对于一个具有此性质的[目标函数](@entry_id:267263) $F$，在标准假设下（例如迭代点保持在[有界集](@entry_id:157754)合内），可以证明近端梯度序列 $\{x^k\}$ 会收敛到单个[临界点](@entry_id:144653) [@problem_id:3415772]。这一强大结果为一大类重要的非凸问题提供了强有力的收敛保证。