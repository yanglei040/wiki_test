{"hands_on_practices": [{"introduction": "迭代软阈值算法 (ISTA) 的核心在于其非线性的更新步骤，即软阈值算子。通过一个具体的数值示例 [@problem_id:3392980] 进行手动计算，您可以直观地理解该算子如何通过收缩某些系数并将其余系数置零来有效促进解的稀疏性。这项基础练习是揭开 ISTA 算法神秘面纱、掌握其内在机制的第一步。", "problem": "考虑一个典型的稀疏线性反问题，其形式为最小化二次数据失配项和逐元素的稀疏性促进惩罚项之和：寻找一个 $x \\in \\mathbb{R}^{5}$ 来最小化 $f(x) + g(x)$，其中 $f(x) = \\tfrac{1}{2}\\|A x - y\\|_{2}^{2}$ 且 $g(x) = \\lambda \\|x\\|_{1}$。在迭代软阈值算法（ISTA）中，在迭代点 $x^k$ 处对 $f$ 进行步长为 $1/L$ 的前向（梯度）步骤，然后是 $g$ 的近端步骤。假设在某次迭代中，前向步骤之后，中间向量为 $u \\in \\mathbb{R}^{5}$，且近端步骤中使用的标量阈值为 $\\tau > 0$。在 $u$ 处缩放的 $\\ell_1$-范数的近端映射记为 $S_{\\tau}(u)$。\n\n对于具体值 $u = (\\,3,\\,-1,\\,\\tfrac{3}{2},\\,-4,\\,\\tfrac{1}{2}\\,)^{\\top} \\in \\mathbb{R}^{5}$ 和 $\\tau = \\tfrac{3}{2}$，精确计算 $S_{\\tau}(u)$，并根据凸分析的第一性原理和邻近算子的性质，解释哪些分量被设为零，哪些被收缩（以及收缩方向）。将最终答案表示为具有精确有理数分量的单个行向量。不要四舍五入。", "solution": "该问题是有效的。它适定，科学上基于凸优化和反问题的原理，并为获得唯一的、可验证的解提供了所有必要信息。\n\n该问题要求计算缩放的$\\ell_1$-范数的邻近算子，通常称为软阈值算子，记为 $S_{\\tau}(u)$。该算子是用于解决稀疏优化问题的迭代软阈值算法（ISTA）的核心。\n\n根据凸分析的第一性原理，作用于向量 $v$ 的函数 $h(x)$（带有缩放参数 $\\gamma > 0$）的邻近算子被定义为以下目标函数的唯一最小化子：\n$$\n\\text{prox}_{\\gamma h}(v) = \\arg\\min_{z} \\left( \\gamma h(z) + \\frac{1}{2} \\|z - v\\|_{2}^{2} \\right)\n$$\n在本问题的背景下，函数是 $\\ell_1$-范数，算子记为 $S_{\\tau}(u)$。这意味着我们要求解 $x = S_{\\tau}(u)$，其中：\n$$\nx = \\arg\\min_{x \\in \\mathbb{R}^{5}} \\left( \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2} \\right)\n$$\n待最小化的目标函数是 $J(x) = \\tau \\|x\\|_{1} + \\frac{1}{2} \\|x - u\\|_{2}^{2}$。该目标函数的一个关键性质是其在向量 $x$ 的各个分量上是可分离的。我们可以将范数重写为各分量之和：\n$$\nJ(x) = \\tau \\sum_{i=1}^{5} |x_i| + \\frac{1}{2} \\sum_{i=1}^{5} (x_i - u_i)^2 = \\sum_{i=1}^{5} \\left( \\tau |x_i| + \\frac{1}{2} (x_i - u_i)^2 \\right)\n$$\n由于整个最小化问题是各项之和，而每一项仅依赖于一个分量 $x_i$，我们可以通过对每个分量 $i \\in \\{1, 2, 3, 4, 5\\}$ 独立地最小化每一项来最小化整个和。因此，对于每个 $i$，我们求解标量最小化问题：\n$$\nx_i = \\arg\\min_{\\xi \\in \\mathbb{R}} \\left( \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2 \\right)\n$$\n令 $J_i(\\xi) = \\tau |\\xi| + \\frac{1}{2} (\\xi - u_i)^2$。由于 $J_i(\\xi)$ 是一个凸函数，其最小值在其次梯度包含零的点处取得。$J_i$ 在 $\\xi$ 处的次微分为：\n$$\n\\partial J_i(\\xi) = \\tau \\partial|\\xi| + (\\xi - u_i)\n$$\n绝对值函数 $\\partial|\\xi|$ 的次微分为：\n$$\n\\partial|\\xi| = \\begin{cases} \\{1\\} & \\text{if } \\xi > 0 \\\\ \\{-1\\} & \\text{if } \\xi < 0 \\\\ [-1, 1] & \\text{if } \\xi = 0 \\end{cases}\n$$\n最优性条件是 $0 \\in \\partial J_i(x_i)$。我们针对最优值 $x_i$ 分情况分析此条件：\n\n情况1：$x_i > 0$。\n次梯度为 $\\partial J_i(x_i) = \\{\\tau + (x_i - u_i)\\}$。令其为零可得 $\\tau + x_i - u_i = 0$，这意味着 $x_i = u_i - \\tau$。为使此解与假设 $x_i > 0$ 一致，我们必须有 $u_i - \\tau > 0$，即 $u_i > \\tau$。\n\n情况2：$x_i < 0$。\n次梯度为 $\\partial J_i(x_i) = \\{-\\tau + (x_i - u_i)\\}$。令其为零可得 $-\\tau + x_i - u_i = 0$，这意味着 $x_i = u_i + \\tau$。为使此解与假设 $x_i < 0$ 一致，我们必须有 $u_i + \\tau < 0$，即 $u_i < -\\tau$。\n\n情况3：$x_i = 0$。\n最优性条件变为 $0 \\in \\tau [-1, 1] + (0 - u_i)$，可简化为 $u_i \\in \\tau [-1, 1]$。这等价于 $-\\tau \\le u_i \\le \\tau$，或 $|u_i| \\le \\tau$。\n\n结合这三种情况，我们得到作用于标量 $u_i$ 的软阈值算子的显式公式：\n$$\nx_i = S_{\\tau}(u_i) = \\begin{cases} u_i - \\tau & \\text{if } u_i > \\tau \\\\ 0 & \\text{if } |u_i| \\le \\tau \\\\ u_i + \\tau & \\text{if } u_i < -\\tau \\end{cases}\n$$\n此规则解释了算子的行为。如果一个分量 $u_i$ 的绝对值小于或等于阈值 $\\tau$，则将其设为零，从而促进稀疏性。如果其绝对值超过阈值，则在保持其符号的同时向零“收缩”一个量 $\\tau$。\n\n现在我们将此规则应用于给定向量 $u = (\\,3,\\,-1,\\,\\frac{3}{2},\\,-4,\\,\\frac{1}{2}\\,)^{\\top}$ 和阈值 $\\tau = \\frac{3}{2}$。\n\n对于第一个分量 $u_1 = 3$：\n由于 $u_1 = 3 > \\tau = \\frac{3}{2}$，此分量被收缩。\n$x_1 = u_1 - \\tau = 3 - \\frac{3}{2} = \\frac{6}{2} - \\frac{3}{2} = \\frac{3}{2}$。\n\n对于第二个分量 $u_2 = -1$：\n由于 $|u_2| = |-1| = 1 \\le \\tau = \\frac{3}{2}$，此分量被设为零。\n$x_2 = 0$。\n\n对于第三个分量 $u_3 = \\frac{3}{2}$：\n由于 $|u_3| = |\\frac{3}{2}| = \\frac{3}{2} \\le \\tau = \\frac{3}{2}$，此分量被设为零（它落在阈值区域的边界上）。\n$x_3 = 0$。\n\n对于第四个分量 $u_4 = -4$：\n由于 $u_4 = -4 < -\\tau = -\\frac{3}{2}$，此分量被收缩。\n$x_4 = u_4 + \\tau = -4 + \\frac{3}{2} = -\\frac{8}{2} + \\frac{3}{2} = -\\frac{5}{2}$。\n\n对于第五个分量 $u_5 = \\frac{1}{2}$：\n由于 $|u_5| = |\\frac{1}{2}| = \\frac{1}{2} \\le \\tau = \\frac{3}{2}$，此分量被设为零。\n$x_5 = 0$。\n\n综合这些结果，最终的向量是 $S_{\\tau}(u) = (\\frac{3}{2}, 0, 0, -\\frac{5}{2}, 0)^{\\top}$。按要求，它被表示为单个行向量。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2} & 0 & 0 & -\\frac{5}{2} & 0\n\\end{pmatrix}\n}\n$$", "id": "3392980"}, {"introduction": "理解了核心算子后，我们进而探讨整个算法在实际应用中的动态行为和挑战。仅知道更新规则是不够的，我们还必须学会如何明智地选择算法参数并诊断潜在的失效模式，例如停滞在无效解。本练习 [@problem_id:3392955] 旨在通过构建一个反例并推导一个基于数据统计特性的正则化参数 $\\lambda$ 选择策略，来培养您应对这些实际问题的能力。", "problem": "考虑一个线性逆问题，其中未知向量为 $x^{\\star} \\in \\mathbb{R}^{n}$，传感矩阵为 $A \\in \\mathbb{R}^{n \\times n}$，观测数据为 $b \\in \\mathbb{R}^{n}$，由 $b = A x^{\\star} + \\eta$ 生成，其中 $\\eta$ 是一个加性噪声向量。针对 $\\ell_{1}$-正则化最小二乘问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的迭代软阈值算法 (ISTA) 由迭代映射 $x^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)$ 定义，其中 $\\tau > 0$ 是一个满足标准 Lipschitz 条件的步长，$\\mathcal{S}_{\\theta}$ 表示逐分量的软阈值算子 $\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}$。\n\n(a) 当正则化参数 $\\lambda$ 相对于数据项过大时，在朴素的阈值选择下，构造一个反例来展示算法停滞（在零向量处停滞）的现象。具体来说，取 $n = 3$, $A = I_{3}$, $\\tau = 1$, 以及 $b = (0.12,\\,-0.09,\\,0)^{\\top}$。从上述定义出发，不使用任何捷径，运用第一性原理精确地说明为什么在这种配置下，只要 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$，ISTA 迭代 $x^{k}$ 对于所有 $k \\geq 1$ 都会恒等于零。\n\n(b) 在一个含噪声的数据同化设置中，假设噪声 $\\eta$ 的各项是独立同分布 (i.i.d.) 的高斯随机变量，其均值为零，方差为 $\\sigma^{2}$，即 $\\eta \\sim \\mathcal{N}(0,\\,\\sigma^{2} I_{n})$。基于 $n$ 个独立同分布高斯变量最大值的分布，通过概率论证，并且不引用任何预先给出的捷径，推导出一个关于 $\\lambda$ 作为 $\\sigma$ 和 $n$ 的函数的解析上合理的缩放关系，该关系能防止更新被噪声主导，同时对于其分量超过典型噪声驱动平台期的信号能避免平凡的零解。使用得到的缩放关系为 $n = 1024$ 和 $\\sigma = 0.03$ 计算一个推荐的 $\\lambda$ 值。\n\n将你最终的 $\\lambda$ 数值四舍五入到四位有效数字。以纯数字形式表示你的答案，不带单位。", "solution": "该问题提出了两个与迭代软阈值算法 (ISTA) 相关的不同任务。第一部分 (a) 要求在特定条件下展示迭代停滞的现象，而第二部分 (b) 要求在随机设置中为正则化参数 $\\lambda$ 推导一个有原则的缩放关系。\n\n### 第一部分 (a)：在零向量处的停滞\n\n我们已知用于解决最小化问题 $\\min_{x \\in \\mathbb{R}^{n}} \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1}$ 的 ISTA 更新规则为：\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\tau}\\!\\left(x^{k} + \\tau A^{\\top}(b - A x^{k})\\right)\n$$\n问题为此部分指定了参数为 $n = 3$，单位矩阵 $A = I_{3}$，步长 $\\tau = 1$，以及数据向量 $b = (0.12,\\,-0.09,\\,0)^{\\top}$。\n\n将 $A=I_{3}$ 和 $\\tau=1$ 代入更新规则可得到显著的简化。对于任意迭代 $x^k$：\n$$\nx^{k+1} = \\mathcal{S}_{\\lambda \\cdot 1}\\!\\left(x^{k} + 1 \\cdot I_{3}^{\\top}(b - I_{3} x^{k})\\right) = \\mathcal{S}_{\\lambda}\\!\\left(x^{k} + (b - x^{k})\\right) = \\mathcal{S}_{\\lambda}(b)\n$$\n这个结果表明，对于任何 $k \\geq 0$，下一个迭代 $x^{k+1}$ 完全由软阈值算子 $\\mathcal{S}_{\\lambda}$ 作用于数据向量 $b$ 决定。它与当前迭代 $x^k$ 无关。因此，从 $k=1$ 开始的所有迭代都是相同的：\n$$\nx^{1} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\nx^{2} = \\mathcal{S}_{\\lambda}(b)\n$$\n$$\n\\vdots\n$$\n$$\nx^{k} = \\mathcal{S}_{\\lambda}(b) \\quad \\text{对于所有 } k \\geq 1.\n$$\n问题要求我们证明，在 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$ 的条件下，这些迭代会恒等于零，即对于所有 $k \\geq 1$，$x^k = 0$。\n\n对于我们 $A=I_3$ 的特定配置，条件是 $\\lambda \\geq \\|I_3^{\\top} b\\|_{\\infty} = \\|b\\|_{\\infty}$。向量 $b=(0.12,\\,-0.09,\\,0)^{\\top}$ 的 $\\ell_{\\infty}$-范数是：\n$$\n\\|b\\|_{\\infty} = \\max\\big(|0.12|, |-0.09|, |0|\\big) = 0.12\n$$\n因此条件变为 $\\lambda \\geq 0.12$。\n\n我们现在分析在此条件下迭代的表达式 $x^k = \\mathcal{S}_{\\lambda}(b)$。软阈值算子 $\\mathcal{S}_{\\theta}$ 是逐分量定义的：\n$$\n\\big(\\mathcal{S}_{\\theta}(z)\\big)_{i} = \\mathrm{sign}(z_{i}) \\max\\{|z_{i}| - \\theta, 0\\}\n$$\n对于整个向量 $\\mathcal{S}_{\\theta}(z)$ 为零向量，其充要条件是对于每个分量 $i$，$\\max\\{|z_{i}| - \\theta, 0\\} = 0$。这又等价于对所有 $i$ 都有 $|z_i| \\leq \\theta$。这可以用 $\\ell_{\\infty}$-范数紧凑地表示为 $\\|z\\|_{\\infty} \\leq \\theta$。\n\n将此应用于我们的问题，$x^k = \\mathcal{S}_{\\lambda}(b)$ 为零向量的条件是 $\\|b\\|_{\\infty} \\leq \\lambda$。这正是在我们选择 $A=I_3$ 时，问题陈述中给出的条件 $\\lambda \\geq \\|A^{\\top} b\\|_{\\infty}$。\n\n具体来说，当 $\\lambda \\geq 0.12$ 时：\n对于第一个分量 $b_1 = 0.12$：$|b_1|=0.12$。由于 $\\lambda \\geq 0.12$，所以 $|b_1| - \\lambda \\leq 0$，因此 $\\max\\{|b_1|-\\lambda, 0\\}=0$。\n对于第二个分量 $b_2 = -0.09$：$|b_2|=0.09$。由于 $\\lambda \\geq 0.12$，所以 $|b_2| - \\lambda < 0$，因此 $\\max\\{|b_2|-\\lambda, 0\\}=0$。\n对于第三个分量 $b_3 = 0$：$|b_3|=0$。由于 $\\lambda \\geq 0.12$，所以 $|b_3| - \\lambda < 0$，因此 $\\max\\{|b_3|-\\lambda, 0\\}=0$。\n\n由于 $\\mathcal{S}_{\\lambda}(b)$ 的所有分量都为零，我们有 $\\mathcal{S}_{\\lambda}(b) = 0$。\n因此，对于任意初始向量 $x^0$ 的选择，第一次迭代结果为 $x^1 = \\mathcal{S}_{\\lambda}(b) = 0$，并且所有后续迭代 $x^k$（对于 $k \\geq 1$）都保持在零向量。这证明了指定的停滞现象。\n\n### 第二部分 (b)：正则化参数的概率缩放\n\n在这一部分，我们要推导 $\\lambda$ 的一个缩放关系，以防止算法被噪声主导。模型是 $b = Ax^\\star + \\eta$，其中 $\\eta$ 是一个各项为独立同分布高斯噪声的向量，$\\eta_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n正则化的一个关键原则是选择足够大的 $\\lambda$ 以抑制噪声，但又不能大到错误地消除了真实的信号分量。一种常见的策略是选择 $\\lambda$ 略高于在被阈值处理的项中预期会看到的噪声水平。\n\n考虑从自然的初始猜测 $x^0 = 0$ 开始的 ISTA 的第一次迭代。软阈值算子的参数是 $x^0 + \\tau A^{\\top}(b - A x^0) = \\tau A^\\top b$。如果我们考虑一个没有真实信号 ($x^\\star=0$) 的场景，那么 $b=\\eta$，这个参数就变成 $\\tau A^\\top \\eta$。为了防止算法拟合噪声，我们希望这一项被阈值处理为零。这要求 $\\tau A^\\top\\eta$ 的每个分量的幅值都小于或等于阈值 $\\lambda\\tau$。这可以写成：\n$$\n\\|\\tau A^\\top \\eta\\|_{\\infty} \\leq \\lambda\\tau \\implies \\|A^\\top \\eta\\|_{\\infty} \\leq \\lambda\n$$\n这为我们选择 $\\lambda$ 提供了一个标准：它应该是 $\\|A^\\top \\eta\\|_{\\infty}$ 可能取值的上界。\n\n为了进行解析推导，我们必须描述向量 $v = A^\\top \\eta$ 的分布。该分布取决于矩阵 $A$。对于这类通用阈值的推导，一个标准的假设是传感矩阵 $A$ 是标准正交的，即 $A^\\top A = I_n$。在此假设下，$v$ 的协方差为：\n$$\n\\mathrm{Cov}(v) = E[v v^\\top] = E[A^\\top \\eta \\eta^\\top A] = A^\\top E[\\eta \\eta^\\top] A = A^\\top (\\sigma^2 I_n) A = \\sigma^2 A^\\top A = \\sigma^2 I_n\n$$\n由于 $E[v] = A^\\top E[\\eta] = 0$，$v = A^\\top \\eta$ 的分量 $v_i$ 是独立同分布的，均值为 0，方差为 $\\sigma^2$ 的高斯随机变量，就像原始噪声分量 $\\eta_i$ 一样。\n\n我们的任务现在简化为寻找随机变量 $M_n = \\|v\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |v_i|$ 的一个高概率上界。令 $Z_i = v_i/\\sigma$ 为独立同分布的标准正态变量，$Z_i \\sim \\mathcal{N}(0, 1)$。我们寻求对 $\\max_i |Z_i|$ 的一个估计，然后将结果乘以 $\\sigma$。\n\n令 $Y = \\max_{1 \\leq i \\leq n} |Z_i|$。对于 $y \\geq 0$，$|Z_i|$ 的累积分布函数 (CDF) 是 $F_{|Z|}(y) = P(|Z_i| \\leq y) = 2\\Phi(y)-1$，其中 $\\Phi$ 是标准正态分布的 CDF。由于 $|Z_i|$ 是独立同分布的，它们最大值的 CDF 是：\n$$\nF_Y(y) = P(Y \\leq y) = [F_{|Z|}(y)]^n = (2\\Phi(y)-1)^n\n$$\n我们想找到一个阈值 $y_n$，使得对于大的 $n$，$P(Y > y_n)$ 很小。我们可以使用联合界作为这个尾概率的近似：\n$$\nP(Y > y) = P(\\exists i: |Z_i| > y) \\leq \\sum_{i=1}^n P(|Z_i|>y) = n P(|Z_1|>y)\n$$\n概率 $P(|Z_1|>y)$ 是 $2(1-\\Phi(y))$。对于大的 $y$，我们可以使用标准高斯尾部近似：\n$$\n1-\\Phi(y) \\approx \\frac{\\phi(y)}{y} = \\frac{1}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)\n$$\n因此，$P(Y>y) \\approx 2n(1-\\Phi(y)) \\approx \\frac{2n}{\\sqrt{2\\pi}y}\\exp\\left(-\\frac{y^2}{2}\\right)$。我们寻求一个 $y$ 关于 $n$ 的缩放关系，使得当 $n \\to \\infty$ 时这个概率趋于零。我们来测试候选的缩放关系 $y = \\sqrt{2\\ln n}$：\n$$\nP(Y > \\sqrt{2\\ln n}) \\approx \\frac{2n}{\\sqrt{2\\pi}\\sqrt{2\\ln n}}\\exp\\left(-\\frac{( \\sqrt{2\\ln n} )^2}{2}\\right) = \\frac{2n}{\\sqrt{4\\pi\\ln n}}\\exp(-\\ln n) = \\frac{2n}{2\\sqrt{\\pi\\ln n}} \\left(\\frac{1}{n}\\right) = \\frac{1}{\\sqrt{\\pi\\ln n}}\n$$\n当 $n \\to \\infty$ 时，这个概率趋于 0。这表明对于大的 $n$，$n$ 个独立同分布的 $|Z_i|$ 变量的最大值很可能不会超过 $\\sqrt{2\\ln n}$。这为选择该值作为阈值提供了理由。\n\n通过乘以 $\\sigma$ 进行缩放，我们得到了 $\\lambda$ 的解析上合理的缩放关系：\n$$\n\\lambda = \\sigma \\sqrt{2 \\ln n}\n$$\n这便是著名的通用阈值。它提供了一个能够适应问题维度 $n$ 和噪声水平 $\\sigma$ 的参数选择。\n\n现在，我们为 $n = 1024$ 和 $\\sigma = 0.03$ 计算推荐的 $\\lambda$ 值。\n首先，计算 $\\ln(1024)$：\n$$\n\\ln(1024) = \\ln(2^{10}) = 10 \\ln(2)\n$$\n使用 $\\ln(2)$ 的一个标准值 $\\ln(2) \\approx 0.69314718$：\n$$\n\\ln(1024) \\approx 6.9314718\n$$\n现在，将此代入 $\\lambda$ 的公式：\n$$\n\\lambda = 0.03 \\times \\sqrt{2 \\times 6.9314718} = 0.03 \\times \\sqrt{13.8629436} \\approx 0.03 \\times 3.7232974\n$$\n$$\n\\lambda \\approx 0.1116989\n$$\n将此值四舍五入到四位有效数字得到：\n$$\n\\lambda \\approx 0.1117\n$$", "answer": "$$\n\\boxed{0.1117}\n$$", "id": "3392955"}, {"introduction": "在许多现实世界的逆问题和数据同化应用中，数据规模庞大，无法在单台计算机上处理。这项实践练习 [@problem_id:3392991] 将挑战您把 ISTA 框架扩展到分布式计算环境中，要求您为并行执行而分解其核心操作，如梯度计算和 Lipschitz 常数估计。这是将高级优化算法应用于大规模问题的必备技能，体现了从理论到高性能计算实现的跨越。", "problem": "考虑一个线性逆问题，其中未知状态 $x \\in \\mathbb{R}^n$ 通过一个线性观测算子 $A \\in \\mathbb{R}^{m \\times n}$ 进行观测，得到含噪声数据 $b \\in \\mathbb{R}^m$。目标是通过最小化以下凸泛函来恢复 $x$ 的一个稀疏估计：\n$$\nJ(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $\\lambda > 0$ 是一个正则化参数，$\\lVert \\cdot \\rVert_1$ 表示 $\\ell_1$-范数。迭代软阈值算法 (Iterative Soft-Thresholding Algorithm, ISTA) 是一种邻近梯度法，专为形如 $f(x) + g(x)$ 的目标函数设计，其中 $f$ 光滑，$g$ 非光滑。在该方法中，先对 $f$ 进行步长为 $\\tau$ 的梯度步，然后应用 $g$ 的邻近算子。在此设定中，$f(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2$ 且 $g(x) = \\lambda \\lVert x \\rVert_1$。$f$ 的梯度由 $\\nabla f(x) = A^{\\top} (A x - b)$ 给出，任何容许的步长 $\\tau$ 都必须满足 $0 < \\tau < 2/L$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数，其值等于谱范数的平方 $L = \\lVert A \\rVert_2^2$。\n\n假设 $A$ 是一个并行观测算子，它没有被显式地构建。相反，它按行分块分布在 $p$ 个逻辑处理器上：\n$$\nA = \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_p \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_p \\end{bmatrix},\n$$\n其中 $A_i \\in \\mathbb{R}^{m_i \\times n}$，$b_i \\in \\mathbb{R}^{m_i}$，且 $\\sum_{i=1}^p m_i = m$。每个处理器 $i$ 可以将其局部前向算子应用于任何 $x$，得到 $y_i = A_i x$，也可以将其局部伴随算子应用于一个局部向量 $r_i \\in \\mathbb{R}^{m_i}$，得到 $g_i = A_i^{\\top} r_i$。在计算 ISTA 所需的乘积时，您不能显式地构建 $A$；相反，您只能使用这些局部前向和伴随算子的应用，并通过对维度匹配的向量求和来跨处理器进行聚合。\n\n您的任务如下：\n- 从邻近梯度法的基本原理和行分块算子的线性代数性质出发，推导如何在不显式构建 $A$ 的情况下，仅使用局部可用的算子 $A_i$ 和 $A_i^{\\top}$ 来计算梯度项 $\\nabla f(x) = A^{\\top} (A x - b)$。\n- 设计一种方法，仅使用通过分布式局部算子 $A_i$ 和 $A_i^{\\top}$ 实现的与 $A$ 和 $A^{\\top}$ 的乘积来近似 Lipschitz 常数 $L = \\lVert A \\rVert_2^2$。请使用应用于自伴随映射 $A^{\\top} A$ 的幂迭代方案。\n- 使用推导出的分布式梯度计算和估计的 Lipschitz 常数来实现 ISTA，以选择步长 $\\tau \\in (0, 2/L)$。对 $\\ell_1$-正则化使用软阈值算子。\n- 为了验证，还需实现一个使用显式矩阵 $A$ 来计算梯度的集中式参考实现，并使用相同的 $\\tau$ 和 $\\lambda$ 运行 ISTA。\n\n测试套件规范。您的程序必须实现并运行以下三个测试用例，每个用例都必须生成一个合成的稀疏真实解 $x_{\\star}$、一个随机高斯矩阵 $A$ 和含噪声数据 $b = A x_{\\star} + \\eta$（其中 $\\eta$ 是独立同分布的高斯噪声）。在每个测试中，将 $A$ 分成 $p$ 个行块，使 $m_i$ 尽可能相等。对于真实解 $x_{\\star}$，在均匀随机的位置上选择恰好 $s$ 个非零项，每个非零项都从标准正态分布中独立抽取。分布式和集中式 ISTA 使用相同的步长，选择为 $\\tau = \\alpha / L_{\\text{est}}$，其中 $\\alpha = 0.99$，$L_{\\text{est}}$ 是通过分布式幂迭代估计的 $L$。用 $x^{0} = 0$ 初始化 ISTA，并精确运行 $K$ 次迭代。\n\n- 测试用例 1 (常规情况): $(n, m, p, s, \\lambda, \\sigma, K, \\text{seed}) = (64, 256, 4, 8, 10^{-2}, 10^{-3}, 200, 42)$。\n- 测试用例 2 (单处理器边界情况): $(n, m, p, s, \\lambda, \\sigma, K, \\text{seed}) = (64, 256, 1, 8, 10^{-2}, 10^{-3}, 200, 43)$。\n- 测试用例 3 (单行分块的极端并行情况): $(n, m, p, s, \\lambda, \\sigma, K, \\text{seed}) = (64, 256, 256, 8, 10^{-2}, 10^{-3}, 200, 44)$。\n\n对于每个测试用例，计算并记录以下两个量：\n- 在一个从标准正态分布中抽取的随机测试点 $x^{\\text{test}}$ 处的相对梯度聚合误差，定义为\n$$\n\\varepsilon_{\\text{grad}} = \\frac{\\lVert g_{\\text{dist}} - g_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert g_{\\text{cent}} \\rVert_2\\}},\n$$\n其中 $g_{\\text{dist}}$ 是仅使用 $A_i$ 和 $A_i^{\\top}$ 对 $A^{\\top}(A x^{\\text{test}} - b)$ 进行的分布式计算结果，$g_{\\text{cent}}$ 是使用显式矩阵 $A$ 进行的集中式计算结果。\n- 经过 $K$ 次 ISTA 迭代后的相对解差异，定义为\n$$\n\\varepsilon_{\\text{sol}} = \\frac{\\lVert x^{K}_{\\text{dist}} - x^{K}_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert x^{K}_{\\text{cent}} \\rVert_2\\}},\n$$\n其中 $x^{K}_{\\text{dist}}$ 和 $x^{K}_{\\text{cent}}$ 分别是分布式和集中式 ISTA 的迭代结果。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，这些结果汇总为一个包含六个浮点数的列表，顺序如下：\n$$\n[\\varepsilon_{\\text{grad}}^{(1)}, \\varepsilon_{\\text{sol}}^{(1)}, \\varepsilon_{\\text{grad}}^{(2)}, \\varepsilon_{\\text{sol}}^{(2)}, \\varepsilon_{\\text{grad}}^{(3)}, \\varepsilon_{\\text{sol}}^{(3)}],\n$$\n以方括号括起来的逗号分隔列表形式打印。不允许有其他输出。所有计算纯属数学运算，不涉及物理单位。", "solution": "该问题要求设计并实现一种分布式迭代软阈值算法 (ISTA) 来解决一个稀疏恢复问题。核心任务是推导梯度项的分布式计算方法，设计一种以分布式方式估计所需 Lipschitz 常数的方法，然后实现该算法并与一个集中式对应算法进行验证。\n\n目标是通过最小化以下凸泛函来寻找线性逆问题 $b \\approx Ax$ 的一个稀疏解 $x \\in \\mathbb{R}^n$：\n$$\nJ(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\n其中 $f(x) = \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2$ 是光滑的数据保真项，$g(x) = \\lambda \\lVert x \\rVert_1$ 是非光滑的稀疏性促进正则化项。\n\n### 1. 分布式梯度的推导\n\nISTA 算法需要计算光滑项的梯度 $\\nabla f(x)$。该梯度由 $\\nabla f(x) = A^{\\top} (A x - b)$ 给出。问题指定算子 $A \\in \\mathbb{R}^{m \\times n}$ 和数据向量 $b \\in \\mathbb{R}^m$ 被划分为 $p$ 个行块：\n$$\nA = \\begin{bmatrix} A_1 \\\\ A_2 \\\\ \\vdots \\\\ A_p \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_p \\end{bmatrix},\n$$\n其中每个块 $A_i \\in \\mathbb{R}^{m_i \\times n}$ 和 $b_i \\in \\mathbb{R}^{m_i}$ 由逻辑处理器 $i$ 管理。目标是仅使用 $A_i$ 及其伴随算子 $A_i^{\\top}$ 的局部应用来计算 $\\nabla f(x)$。\n\n首先，我们用这些块来表示残差向量 $r = A x - b$：\n$$\nr = A x - b = \\begin{bmatrix} A_1 \\\\ \\vdots \\\\ A_p \\end{bmatrix} x - \\begin{bmatrix} b_1 \\\\ \\vdots \\\\ b_p \\end{bmatrix} = \\begin{bmatrix} A_1 x - b_1 \\\\ \\vdots \\\\ A_p x - b_p \\end{bmatrix} = \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_p \\end{bmatrix}.\n$$\n这表明每个局部残差分量 $r_i = A_i x - b_i$ 可以在处理器 $i$ 上通过将局部前向算子 $A_i$ 应用于全局状态向量 $x$ 来独立计算。\n\n接下来，我们将全局伴随算子 $A^{\\top}$ 应用于残差 $r$。块列矩阵的转置是块行矩阵：\n$$\nA^{\\top} = \\begin{bmatrix} A_1^{\\top} & A_2^{\\top} & \\cdots & A_p^{\\top} \\end{bmatrix}.\n$$\n梯度是这个块行矩阵与块列残差向量的乘积：\n$$\n\\nabla f(x) = A^{\\top} r = \\begin{bmatrix} A_1^{\\top} & \\cdots & A_p^{\\top} \\end{bmatrix} \\begin{bmatrix} r_1 \\\\ \\vdots \\\\ r_p \\end{bmatrix} = \\sum_{i=1}^{p} A_i^{\\top} r_i.\n$$\n代入 $r_i$ 的表达式，我们得到梯度的分布式计算公式：\n$$\n\\nabla f(x) = \\sum_{i=1}^{p} A_i^{\\top} (A_i x - b_i).\n$$\n梯度的计算过程如下：\n1.  将当前迭代值 $x$ 广播到所有 $p$ 个处理器。\n2.  每个处理器 $i$ 计算其局部残差 $r_i = A_i x - b_i$。\n3.  每个处理器 $i$ 计算其局部梯度贡献 $g_i = A_i^{\\top} r_i$，这是一个在 $\\mathbb{R}^n$ 中的向量。\n4.  通过对局部贡献求和得到全局梯度 $\\nabla f(x)$：$\\nabla f(x) = \\sum_{i=1}^{p} g_i$。这一步需要在所有处理器之间进行聚合操作（例如，归约求和操作）。\n\n### 2. Lipschitz 常数的分布式估计\n\n如果步长 $\\tau$ 满足 $0 < \\tau < 2/L$，其中 $L$ 是 $\\nabla f$ 的 Lipschitz 常数，则可以保证 ISTA 的收敛性。对于我们选择的 $f(x)$，该常数是 $A$ 的谱范数的平方，即 $L = \\lVert A \\rVert_2^2$。谱范数 $\\lVert A \\rVert_2$ 是 $A$ 的最大奇异值 $\\sigma_{\\max}(A)$。因此，$L = \\sigma_{\\max}(A)^2$。\n\n$A$ 的奇异值是自伴随矩阵 $A^{\\top}A$ 特征值的平方根。因此，$L$ 是 $A^{\\top}A$ 的最大特征值，即 $L = \\lambda_{\\max}(A^{\\top}A)$。我们可以使用幂迭代法来近似这个主特征值。\n\n幂迭代算法生成一系列向量 $v_k$，这些向量收敛到对应于主特征值的特征向量。从一个随机单位向量 $v_0 \\in \\mathbb{R}^n$ 开始，迭代过程为：\n$$\nv_{k+1} = \\frac{(A^{\\top}A) v_k}{\\lVert (A^{\\top}A) v_k \\rVert_2}.\n$$\n相应的特征值估计为 $\\lambda_k = \\lVert (A^{\\top}A) v_k \\rVert_2$。该方法的核心是矩阵-向量乘积 $w = (A^{\\top}A) v$。我们可以用分布式方式计算这个乘积，类似于梯度计算。令 $u = Av$。则 $w = A^{\\top}u$。分布式计算过程如下：\n1.  在每个处理器 $i$ 上计算 $u$ 的局部部分：$u_i = A_i v$。\n2.  在每个处理器 $i$ 上计算对 $w$ 的局部贡献：$w_i = A_i^{\\top} u_i$。\n3.  聚合以找到最终结果：$w = \\sum_{i=1}^{p} w_i$。\n\n用于估计 $L$ 的分布式幂迭代算法如下：\n1.  初始化一个随机向量 $v \\in \\mathbb{R}^n$ 并将其归一化，$\\lVert v \\rVert_2 = 1$。\n2.  迭代固定次数：\n    a. 在每个处理器 $i$ 上，计算 $u_i = A_i v$。\n    b. 在每个处理器 $i$ 上，计算 $w_i = A_i^{\\top} u_i$。\n    c. 将结果聚合成 $w = \\sum_{i=1}^{p} w_i$。\n    d. 特征值估计为 $L_{\\text{est}} = \\lVert w \\rVert_2$。\n    e. 为下一次迭代将向量归一化：$v = w / L_{\\text{est}}$。\n3.  经过足够多的迭代后，$L_{\\text{est}}$ 提供了对 $L = \\lambda_{\\max}(A^{\\top}A)$ 的良好近似。\n\n### 3. 迭代软阈值算法 (ISTA)\n\nISTA 是一种邻近梯度法。每次迭代包括对光滑部分 $f(x)$ 的标准梯度下降步，然后应用非光滑部分 $g(x)$ 的邻近算子。更新规则是：\n$$\nx^{k+1} = \\text{prox}_{\\tau g}(x^k - \\tau \\nabla f(x^k)).\n$$\n对于 $g(x) = \\lambda \\lVert x \\rVert_1$，$\\tau g(x) = (\\tau \\lambda) \\lVert x \\rVert_1$ 的邻近算子是软阈值算子，其按分量定义为：\n$$\n[\\text{prox}_{\\tau g}(z)]_j = S_{\\tau \\lambda}(z_j) = \\text{sign}(z_j) \\max(|z_j| - \\tau \\lambda, 0).\n$$\n因此，完整的 ISTA 更新是：\n$$\nx^{k+1} = S_{\\tau \\lambda}(x^k - \\tau \\nabla f(x^k)).\n$$\n在我们的分布式实现中，我们用 $x^0=0$ 初始化，估计 $L$ 以选择步长 $\\tau = \\alpha / L_{\\text{est}}$（按规定 $\\alpha=0.99$），然后迭代 $K$ 次，在每一步中使用第 1 节的分布式方法计算 $\\nabla f(x^k)$。\n\n### 4. 验证框架\n\n为了验证分布式实现的正确性，我们还实现了一个集中式版本的算法。集中式版本可以访问完整的矩阵 $A$，并直接计算梯度 $\\nabla f(x) = A^{\\top}(Ax - b)$。分布式和集中式算法都使用相同的初始向量 $x^0=0$ 和相同的步长 $\\tau$（从分布式幂迭代估计 $L_{\\text{est}}$ 得到），运行固定的迭代次数 $K$。\n\n这两种方法的数学等价性意味着，在精确算术中，结果将是相同的。在浮点运算中，可能会出现微小差异。这些差异通过两个指标来量化：\n1.  相对梯度聚合误差 $\\varepsilon_{\\text{grad}}$，它比较两种方法在随机测试点 $x^{\\text{test}}$ 处计算的梯度：\n$$\n\\varepsilon_{\\text{grad}} = \\frac{\\lVert g_{\\text{dist}} - g_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert g_{\\text{cent}} \\rVert_2\\}}.\n$$\n2.  相对解差异 $\\varepsilon_{\\text{sol}}$，它比较 $K$ 步后的最终迭代值：\n$$\n\\varepsilon_{\\text{sol}} = \\frac{\\lVert x^{K}_{\\text{dist}} - x^{K}_{\\text{cent}} \\rVert_2}{\\max\\{10^{-16}, \\lVert x^{K}_{\\text{cent}} \\rVert_2\\}}.\n$$\n预计这些误差指标将接近机器精度，从而证实分布式实现正确地复制了集中式算法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a distributed Iterative Soft-Thresholding Algorithm (ISTA).\n    \"\"\"\n\n    test_cases = [\n        # (n, m, p, s, lambda, sigma, K, seed)\n        (64, 256, 4, 8, 10**-2, 10**-3, 200, 42),\n        (64, 256, 1, 8, 10**-2, 10**-3, 200, 43),\n        (64, 256, 256, 8, 10**-2, 10**-3, 200, 44),\n    ]\n\n    results = []\n\n    def soft_threshold(z, T):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\n    for case in test_cases:\n        n, m, p, s, lam, sigma, K, seed = case\n\n        # 1. Generate synthetic data\n        np.random.seed(seed)\n        A = np.random.randn(m, n)\n        \n        x_star = np.zeros(n)\n        support = np.random.choice(n, s, replace=False)\n        x_star[support] = np.random.randn(s)\n        \n        noise = sigma * np.random.randn(m)\n        b = A @ x_star + noise\n\n        # 2. Create distributed operators (simulation)\n        row_indices = np.array_split(np.arange(m), p)\n        A_blocks = [A[indices, :] for indices in row_indices]\n        b_blocks = [b[indices] for indices in row_indices]\n\n        # 3. Estimate Lipschitz constant using distributed power iteration\n        def estimate_lipschitz_dist(A_bl, n_dim, num_iters=20):\n            v = np.random.randn(n_dim)\n            norm_v = np.linalg.norm(v)\n            if norm_v > 0:\n                v = v / norm_v\n            else: # Extremely unlikely case\n                v = np.zeros(n_dim)\n                v[0] = 1.0\n\n            L_est = 0.0\n            for _ in range(num_iters):\n                # Apply A^T * A to v in a distributed fashion\n                # u_i = A_i * v\n                u_blocks = [A_i @ v for A_i in A_bl]\n                # w = sum(A_i^T * u_i)\n                w = sum(A_bl[i].T @ u_blocks[i] for i in range(len(A_bl)))\n                \n                L_est = np.linalg.norm(w)\n                if L_est  1e-16: # Matrix is effectively null\n                    return 0.0\n                v = w / L_est\n            return L_est\n\n        L_est = estimate_lipschitz_dist(A_blocks, n)\n        tau = 0.99 / L_est if L_est > 0 else 1.0\n\n        # 4. Compute relative gradient aggregation error\n        x_test = np.random.randn(n)\n\n        # Centralized gradient\n        g_cent = A.T @ (A @ x_test - b)\n        \n        # Distributed gradient\n        g_dist = sum(A_blocks[i].T @ (A_blocks[i] @ x_test - b_blocks[i]) for i in range(p))\n\n        g_cent_norm = np.linalg.norm(g_cent)\n        eps_grad = np.linalg.norm(g_dist - g_cent) / max(1e-16, g_cent_norm)\n        results.append(eps_grad)\n\n        # 5. Run ISTA and compute relative solution discrepancy\n        \n        # Distributed ISTA\n        x_k_dist = np.zeros(n)\n        for _ in range(K):\n            grad_f_dist = sum(A_blocks[i].T @ (A_blocks[i] @ x_k_dist - b_blocks[i]) for i in range(p))\n            z_dist = x_k_dist - tau * grad_f_dist\n            x_k_dist = soft_threshold(z_dist, tau * lam)\n\n        # Centralized ISTA\n        x_k_cent = np.zeros(n)\n        for _ in range(K):\n            grad_f_cent = A.T @ (A @ x_k_cent - b)\n            z_cent = x_k_cent - tau * grad_f_cent\n            x_k_cent = soft_threshold(z_cent, tau * lam)\n            \n        x_k_cent_norm = np.linalg.norm(x_k_cent)\n        eps_sol = np.linalg.norm(x_k_dist - x_k_cent) / max(1e-16, x_k_cent_norm)\n        results.append(eps_sol)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3392991"}]}