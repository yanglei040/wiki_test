{"hands_on_practices": [{"introduction": "为了从根本上理解阶梯效应，我们的第一个练习将探讨其核心机制。通过在一个连续模型中，尝试使用一个双平台的分段常数函数来逼近一个简单的线性斜坡，我们可以解析地推导出最优解 [@problem_id:3420945]。这个过程将清晰地揭示，在总变分正则化的约束下，“阶梯”是如何作为对平滑函数的最优逼近而自然产生的。", "problem": "考虑在区间 $[0,1]$ 上的一个一维数据同化问题，其确定性观测为 $f(x) = x$。我们寻求一个具有两个平台的阶梯解，即一个形式如下的分段常数函数 $u$：\n$$\nu(x) = \\begin{cases}\nc_{1},  x \\in [0,s),\\\\\nc_{2},  x \\in [s,1],\n\\end{cases}\n$$\n其中 $s \\in (0,1)$ 为某个断点，$c_{1}, c_{2} \\in \\mathbb{R}$ 为平台值。该估计量是通过最小化在反问题中使用的标准吉洪诺夫型 $L^{2}$ 保真项与全变分 (TV) 正则化项来选择的：\n$$\n\\min_{c_{1},c_{2},s} \\ \\frac{1}{2} \\int_{0}^{1} \\big(u(x)-f(x)\\big)^{2} \\,\\mathrm{d}x \\ + \\ \\lambda \\, TV(u),\n$$\n其中 $TV$ 表示全变分，对于这样一个在 $x=s$ 处有单个跳跃的分段常数函数 $u$，我们有 $TV(u) = |c_{2} - c_{1}|$。常数 $\\lambda>0$ 是正则化参数。\n\n仅从 $L^{2}$ 范数和全变分的基本定义出发，推导共同最小化以下表达式的最优平台值 $c_{1}^{\\star}$ 和 $c_{2}^{\\star}$ 以及最优断点 $s^{\\star}$：\n$$\n\\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda |c_{2}-c_{1}|,\n$$\n在假设 $0  \\lambda  \\frac{1}{8}$ 下，该假设确保最小化子呈现出两个不同的平台。你的最终答案必须是给出 $c_{1}^{\\star}$、$c_{2}^{\\star}$ 和 $s^{\\star}$ 的单个闭式解析表达式。", "solution": "问题是找到最优参数 $c_1$、$c_2$ 和 $s$，以最小化目标泛函 $J(c_1, c_2, s)$。该泛函由一个 $L^2$ 数据保真项和一个全变分 (TV) 正则化项组成。观测是区间 $[0,1]$ 上的 $f(x) = x$。所求的解 $u(x)$ 是一个分段常数函数，具有两个平台 $c_1$ 和 $c_2$，断点在 $s$ 处。\n\n目标泛函由下式给出：\n$$\nJ(c_1, c_2, s) = \\frac{1}{2} \\int_{0}^{1} \\big(u(x)-f(x)\\big)^{2} \\,\\mathrm{d}x \\ + \\ \\lambda \\, TV(u)\n$$\n将 $u(x)$、$f(x)$ 和 $TV(u)$ 的给定形式代入，我们得到：\n$$\nJ(c_1, c_2, s) = \\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda |c_{2}-c_{1}|\n$$\n这个最小化是关于 $c_1 \\in \\mathbb{R}$、$c_2 \\in \\mathbb{R}$ 和 $s \\in (0,1)$ 进行的。问题陈述指明我们寻求一个具有两个不同平台的解，这意味着 $c_1 \\neq c_2$。数据函数 $f(x)=x$ 是严格递增的。可以合理地预期最优近似 $u(x)$ 是非递减的，这意味着我们可以假设 $c_1 \\le c_2$。条件 $0  \\lambda  1/8$ 确保了 $c_1  c_2$，所以我们可以将 $|c_2 - c_1|$ 替换为 $c_2 - c_1$。目标泛函变为：\n$$\nJ(c_1, c_2, s) = \\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda (c_{2}-c_{1})\n$$\n为了找到最优值 $(c_1^{\\star}, c_2^{\\star}, s^{\\star})$，我们使用最小化的一阶必要条件。这包括将 $J$ 关于 $c_1$、$c_2$ 和 $s$ 的偏导数设为零。\n\n首先，对于一个固定的 $s$ 值，我们找到最优的 $c_1$ 和 $c_2$。我们计算 $J$ 关于 $c_1$ 和 $c_2$ 的偏导数：\n$$\n\\frac{\\partial J}{\\partial c_1} = \\frac{\\partial}{\\partial c_1} \\left[ \\frac{1}{2} \\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x - \\lambda c_1 \\right] = \\int_{0}^{s} (c_1 - x)\\,\\mathrm{d}x - \\lambda\n$$\n$$\n\\frac{\\partial J}{\\partial c_2} = \\frac{\\partial}{\\partial c_2} \\left[ \\frac{1}{2} \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x + \\lambda c_2 \\right] = \\int_{s}^{1} (c_2 - x)\\,\\mathrm{d}x + \\lambda\n$$\n将这些导数设为零可得：\n$$\n\\int_{0}^{s} (c_1 - x)\\,\\mathrm{d}x = \\lambda \\implies \\left[ c_1 x - \\frac{x^2}{2} \\right]_0^s = \\lambda \\implies c_1 s - \\frac{s^2}{2} = \\lambda\n$$\n$$\n\\int_{s}^{1} (c_2 - x)\\,\\mathrm{d}x = -\\lambda \\implies \\left[ c_2 x - \\frac{x^2}{2} \\right]_s^1 = -\\lambda \\implies (c_2 - \\frac{1}{2}) - (c_2 s - \\frac{s^2}{2}) = -\\lambda\n$$\n从第一个方程，我们解出 $c_1$ 作为 $s$ 的函数：\n$$\nc_1(s) = \\frac{s}{2} + \\frac{\\lambda}{s}\n$$\n从第二个方程，我们解出 $c_2$ 作为 $s$ 的函数：\n$$\nc_2(1-s) - \\frac{1-s^2}{2} = -\\lambda \\implies c_2(1-s) = \\frac{(1-s)(1+s)}{2} - \\lambda \\implies c_2(s) = \\frac{1+s}{2} - \\frac{\\lambda}{1-s}\n$$\n这些表达式给出了对于任何给定断点 $s$ 的最优平台值。\n\n接下来，我们求最优断点 $s^{\\star}$。我们将 $c_1(s)$ 和 $c_2(s)$ 代入 $J$ 中，得到一个仅关于 $s$ 的函数，然后将其最小化。一个更直接的方法是求 $J(c_1, c_2, s)$ 关于 $s$ 的偏导数并将其设为零，这里使用我们刚刚求得的 $c_1$ 和 $c_2$ 的值。根据包络定理，对于最优函数 $c_1(s)$ 和 $c_2(s)$，全导数 $\\frac{dJ}{ds}$ 简化为偏导数 $\\frac{\\partial J}{\\partial s}$。\n$$\n\\frac{dJ}{ds} = \\frac{\\partial J}{\\partial c_1}\\frac{dc_1}{ds} + \\frac{\\partial J}{\\partial c_2}\\frac{dc_2}{ds} + \\frac{\\partial J}{\\partial s}\n$$\n由于 $c_1(s)$ 和 $c_2(s)$ 的定义使得 $\\frac{\\partial J}{\\partial c_1} = 0$ 和 $\\frac{\\partial J}{\\partial c_2} = 0$，这简化为 $\\frac{dJ}{ds} = \\frac{\\partial J}{\\partial s}$。\n\n使用莱布尼茨积分法则，$J$ 关于 $s$ 的偏导数是：\n$$\n\\frac{\\partial J}{\\partial s} = \\frac{1}{2}(c_1-s)^2 - \\frac{1}{2}(c_2-s)^2\n$$\n将其设为零以求最优的 $s$：\n$$\n\\frac{1}{2}(c_1-s)^2 - \\frac{1}{2}(c_2-s)^2 = 0 \\implies (c_1-s)^2 = (c_2-s)^2\n$$\n这意味着 $c_1-s = \\pm(c_2-s)$。\n情况1：$c_1-s = c_2-s \\implies c_1=c_2$。这意味着没有跳跃，是一个单平台解。这种情况在 $\\lambda \\ge 1/8$ 时发生，但问题指定 $\\lambda  1/8$。所以我们舍弃这种情况。\n情况2：$c_1-s = -(c_2-s) \\implies c_1-s = s-c_2 \\implies c_1+c_2 = 2s$。\n这个条件意味着最优断点 $s$ 是平台值 $c_1$ 和 $c_2$ 的算术平均值。\n\n我们现在有了一个包含三个未知数 $c_1, c_2, s$ 的三方程系统：\n1. $c_1 = \\frac{s}{2} + \\frac{\\lambda}{s}$\n2. $c_2 = \\frac{1+s}{2} - \\frac{\\lambda}{1-s}$\n3. $c_1+c_2 = 2s$\n\n将 (1) 和 (2) 代入 (3) 中：\n$$\n\\left(\\frac{s}{2} + \\frac{\\lambda}{s}\\right) + \\left(\\frac{1+s}{2} - \\frac{\\lambda}{1-s}\\right) = 2s\n$$\n$$\ns + \\frac{1}{2} + \\lambda\\left(\\frac{1}{s} - \\frac{1}{1-s}\\right) = 2s\n$$\n$$\n\\frac{1}{2} - s + \\lambda\\left(\\frac{1-s-s}{s(1-s)}\\right) = 0\n$$\n$$\n\\frac{1-2s}{2} + \\lambda\\frac{1-2s}{s(1-s)} = 0\n$$\n$$\n(1-2s)\\left(\\frac{1}{2} + \\frac{\\lambda}{s(1-s)}\\right) = 0\n$$\n这个方程有两个可能的解。\n可能性A：$1-2s=0 \\implies s = 1/2$。这个解位于有效区间 $(0,1)$ 内。\n可能性B：$\\frac{1}{2} + \\frac{\\lambda}{s(1-s)} = 0 \\implies s(1-s) = -2\\lambda$。这是一个二次方程 $s^2-s-2\\lambda=0$。其解为 $s = \\frac{1 \\pm \\sqrt{1+8\\lambda}}{2}$。对于 $\\lambda0$，有 $\\sqrt{1+8\\lambda}1$。因此，根 $\\frac{1+\\sqrt{1+8\\lambda}}{2}  1$，而根 $\\frac{1-\\sqrt{1+8\\lambda}}{2}  0$。两个根都不在区间 $(0,1)$ 内。\n\n因此，唯一有效的最优断点是 $s^{\\star} = 1/2$。\n\n最后，我们通过将 $s^{\\star}=1/2$ 代入其表达式来求得最优平台值 $c_1^{\\star}$ 和 $c_2^{\\star}$：\n$$\nc_1^{\\star} = c_1(1/2) = \\frac{1/2}{2} + \\frac{\\lambda}{1/2} = \\frac{1}{4} + 2\\lambda\n$$\n$$\nc_2^{\\star} = c_2(1/2) = \\frac{1+1/2}{2} - \\frac{\\lambda}{1-1/2} = \\frac{3/2}{2} - \\frac{\\lambda}{1/2} = \\frac{3}{4} - 2\\lambda\n$$\n我们检验我们最初的 $c_1  c_2$ 的假设：\n$$\nc_2^{\\star} - c_1^{\\star} = \\left(\\frac{3}{4} - 2\\lambda\\right) - \\left(\\frac{1}{4} + 2\\lambda\\right) = \\frac{1}{2} - 4\\lambda\n$$\n条件 $c_2^{\\star} > c_1^{\\star}$ 要求 $\\frac{1}{2} - 4\\lambda > 0 \\implies \\lambda  \\frac{1}{8}$。这与问题的假设一致，从而证明了我们将 $|c_2-c_1|$ 替换为 $c_2-c_1$ 是合理的。\n\n最优参数为：\n$c_1^{\\star} = \\frac{1}{4} + 2\\lambda$\n$c_2^{\\star} = \\frac{3}{4} - 2\\lambda$\n$s^{\\star} = \\frac{1}{2}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{4} + 2\\lambda  \\frac{3}{4} - 2\\lambda  \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "3420945"}, {"introduction": "总变分正则化之所以被广泛应用，一个关键原因在于其保持图像或信号中锐利边缘的能力。然而，这种优势往往伴随着在平滑区域引入人为阶梯的代价。本练习将模拟一个更接近现实的场景，数据中既包含一个需要保留的强边缘，也包含平滑变化的区域 [@problem_id:3420951]。你的任务是分析并量化TV正则化解如何成功保持重要边缘，同时在信号的其他部分产生不必要的阶梯伪影，从而深刻理解其内在的权衡。", "problem": "考虑在反问题和数据同化的状态估计背景下的一维全变分 (TV) 正则化。设未知信号为向量 $u \\in \\mathbb{R}^{6}$，通过最小化 Rudin–Osher–Fatemi (ROF) 泛函从观测值 $f \\in \\mathbb{R}^{6}$ 中重构得到\n$$\nJ(u) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{6}\\big(u_{i}-f_{i}\\big)^{2} \\;+\\; \\lambda\\sum_{i=1}^{5}\\big|u_{i+1}-u_{i}\\big|,\n$$\n其中 $\\lambda0$ 是正则化参数，一维离散全变分 (TV) 为 $\\sum_{i=1}^{5}\\big|u_{i+1}-u_{i}\\big|$。该框架被广泛用于在抑制振荡的同时保持锐利特征。\n\n设观测向量编码了一个强边缘，其两侧是平缓的斜坡：\n$$\nf \\;=\\; \\big(0,\\;\\varepsilon,\\;2\\varepsilon,\\;M,\\;M+\\varepsilon,\\;M+2\\varepsilon\\big),\n$$\n其中参数 $M0$ 且 $\\varepsilon0$。假设正则化参数满足\n$$\n3\\varepsilon  \\lambda  \\frac{3}{2}M,\n$$\n并采用上述指定的离散 ROF 模型。\n\n从凸优化的基本原理和离散全变分的次微分表征出发，推导极小值点的结构，该结构在索引 $3$ 和 $4$ 之间的界面处保持强边缘，同时在该界面之外产生虚假的平台区。利用这些原理，以闭式形式计算以下两个量：\n\n- 边缘保持因子，定义为\n$$\nE \\;=\\; \\frac{\\text{重构信号在边缘处的跳变}}{\\text{观测值在边缘处的跳变}} \\;=\\; \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}.\n$$\n\n- 远离边缘的阶梯效应的定量度量，定义为左侧平台区上的均方平坦化，\n$$\nP \\;=\\; \\frac{1}{3}\\sum_{i=1}^{3}\\big(u_{i}-f_{i}\\big)^{2}.\n$$\n\n将最终答案表示为关于 $M$、$\\varepsilon$ 和 $\\lambda$ 的单个闭式表达式。不需要数值近似，也不涉及单位。将 $E$ 和 $P$ 一起表示为单个行矩阵。", "solution": "本问题要求推导一维 Rudin–Osher–Fatemi (ROF) 泛函的极小值点，并计算两个相关量：一个边缘保持因子和一个阶梯效应度量。\n\n首先，我们对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n-   未知信号是向量 $u \\in \\mathbb{R}^{6}$。\n-   观测向量是 $f \\in \\mathbb{R}^{6}$，由 $f = (0, \\varepsilon, 2\\varepsilon, M, M+\\varepsilon, M+2\\varepsilon)$ 给出，其中 $M0$ 且 $\\varepsilon0$。\n-   待最小化的目标泛函是 $J(u) = \\frac{1}{2}\\sum_{i=1}^{6}(u_{i}-f_{i})^{2} + \\lambda\\sum_{i=1}^{5}|u_{i+1}-u_{i}|$。\n-   正则化参数 $\\lambda$ 为正，并满足条件 $3\\varepsilon  \\lambda  \\frac{3}{2}M$。\n-   需要计算的量是：\n    1.  边缘保持因子 $E = \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}$。\n    2.  左侧平台区上的均方平坦化 $P = \\frac{1}{3}\\sum_{i=1}^{3}(u_{i}-f_{i})^{2}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据**：该问题使用用于全变分 (TV) 正则化的标准 ROF 模型进行表述，这是现代信号和图像处理中解决反问题的基石。该表述在数学上和物理上都是合理的。\n-   **适定性**：泛函 $J(u)$ 是一个严格凸的可微函数（数据保真项 $\\frac{1}{2}\\|u-f\\|_2^2$）和一个凸的不可微函数（TV 半范数）之和。总和 $J(u)$ 是严格凸的，这保证了极小值点的存在性和唯一性。\n-   **目标**：问题以精确的数学术语陈述，所有变量和目标都已明确定义。\n-   **完备性**：问题提供了所有必要的信息，包括数据向量 $f$ 的显式形式和正则化参数 $\\lambda$ 的特定范围，这对于确定解的结构至关重要。整个设定是自洽和一致的。\n\n### 步骤 3：结论和行动\n问题是有效的。这是一个定义明确、自洽且有科学依据的优化问题。我们着手解决它。\n\n### 求解推导\n\n凸泛函 $J(u)$ 的极小值点 $u$ 由一阶最优性条件 $0 \\in \\partial J(u)$ 表征，其中 $\\partial J(u)$ 是 $J(u)$ 的次微分。该泛函为 $J(u) = F(u) + \\lambda G(u)$，其中 $F(u) = \\frac{1}{2}\\sum_{i=1}^{6}(u_i-f_i)^2$ 且 $G(u) = \\sum_{i=1}^{5}|u_{i+1}-u_i|$。\n$J(u)$ 的次微分是 $\\partial J(u) = \\nabla F(u) + \\lambda \\partial G(u)$。\n$F(u)$ 的梯度是 $\\nabla F(u) = u-f$。\n$G(u)$ 的次微分更复杂。对于每个分量 $u_k$，最优性条件是：\n$$ (u_k - f_k) + \\lambda \\partial_{u_k}G = 0 $$\n令 $p_i \\in \\partial|u_{i+1}-u_i|$，这意味着如果 $u_{i+1} \\neq u_i$，则 $p_i = \\text{sgn}(u_{i+1}-u_i)$；如果 $u_{i+1}=u_i$，则 $p_i \\in [-1, 1]$。$G(u)$ 的次梯度分量为：\n$\\partial_{u_1}G \\ni -p_1$\n$\\partial_{u_k}G \\ni p_{k-1} - p_k$ 对于 $k=2, \\ldots, 5$\n$\\partial_{u_6}G \\ni p_5$\n这给出了 $k=1, \\ldots, 6$ 的最优性条件系统：\n\\begin{align*} u_1 - f_1 - \\lambda p_1 = 0 \\\\ u_2 - f_2 + \\lambda p_1 - \\lambda p_2 = 0 \\\\ u_3 - f_3 + \\lambda p_2 - \\lambda p_3 = 0 \\\\ u_4 - f_4 + \\lambda p_3 - \\lambda p_4 = 0 \\\\ u_5 - f_5 + \\lambda p_4 - \\lambda p_5 = 0 \\\\ u_6 - f_6 + \\lambda p_5 = 0 \\end{align*}\n问题陈述暗示了解在索引 $3$ 处的边缘之外具有平台区。我们假设解具有 $u_1=u_2=u_3=c_1$ 和 $u_4=u_5=u_6=c_2$ 的结构，其中 $c_1$ 和 $c_2$ 为某些常数。\n这个假设意味着 $u_2-u_1=0$, $u_3-u_2=0$, $u_5-u_4=0$ 和 $u_6-u_5=0$。因此，次梯度变量 $p_1, p_2, p_4, p_5$ 必须位于区间 $[-1, 1]$ 内。\n数据 $f$ 有一个大的跳变 $f_4-f_3 = M-2\\varepsilon$。给定条件 $\\lambda  \\frac{3}{2}M$ 表明这个跳变将被保留，因此我们假设 $u_4 > u_3$。这意味着 $p_3 = \\text{sgn}(u_4-u_3) = 1$。\n\n我们可以通过对假设的平台区上的最优性方程求和来找到 $c_1$ 和 $c_2$。\n对前三个方程求和：\n$$ (u_1-f_1) + (u_2-f_2) + (u_3-f_3) - \\lambda p_3 = 0 $$\n代入 $u_1=u_2=u_3=c_1$、$p_3=1$ 以及 $f_1, f_2, f_3$ 的值：\n$$ 3c_1 - (f_1+f_2+f_3) - \\lambda = 0 $$\n$$ 3c_1 - (0+\\varepsilon+2\\varepsilon) - \\lambda = 0 \\implies 3c_1 - 3\\varepsilon - \\lambda = 0 $$\n$$ c_1 = \\varepsilon + \\frac{\\lambda}{3} $$\n对后三个方程求和：\n$$ (u_4-f_4) + (u_5-f_5) + (u_6-f_6) + \\lambda p_3 = 0 $$\n代入 $u_4=u_5=u_6=c_2$、$p_3=1$ 以及 $f_4, f_5, f_6$ 的值：\n$$ 3c_2 - (f_4+f_5+f_6) + \\lambda = 0 $$\n$$ 3c_2 - (M+M+\\varepsilon+M+2\\varepsilon) + \\lambda = 0 \\implies 3c_2 - (3M+3\\varepsilon) + \\lambda = 0 $$\n$$ c_2 = M + \\varepsilon - \\frac{\\lambda}{3} $$\n在找到 $c_1$ 和 $c_2$ 后，所提出的极小值点是 $u = (c_1, c_1, c_1, c_2, c_2, c_2)$。我们必须验证该解与所有次梯度条件一致。\n1.  检查 $u_4 > u_3$：$u_4-u_3 = c_2 - c_1 = (M+\\varepsilon-\\frac{\\lambda}{3}) - (\\varepsilon+\\frac{\\lambda}{3}) = M - \\frac{2\\lambda}{3}$。为了使其为正，我们需要 $M > \\frac{2\\lambda}{3}$，即 $\\lambda  \\frac{3}{2}M$。这在问题陈述中已给出。所以，$p_3=1$ 是正确的。\n2.  找出 $p_1, p_2, p_4, p_5$ 并检查它们是否在 $[-1, 1]$ 内。从各个最优性方程可得：\n    - $p_1 = \\frac{u_1-f_1}{\\lambda} = \\frac{c_1-0}{\\lambda} = \\frac{\\varepsilon + \\lambda/3}{\\lambda} = \\frac{1}{3} + \\frac{\\varepsilon}{\\lambda}$。\n    - $p_2-p_1 = \\frac{u_2-f_2}{\\lambda} = \\frac{c_1-\\varepsilon}{\\lambda} = \\frac{\\lambda/3}{\\lambda} = \\frac{1}{3} \\implies p_2 = p_1+\\frac{1}{3} = \\frac{2}{3} + \\frac{\\varepsilon}{\\lambda}$。\n    - $p_4-p_3 = \\frac{u_4-f_4}{\\lambda} = \\frac{c_2-M}{\\lambda} = \\frac{\\varepsilon-\\lambda/3}{\\lambda} = \\frac{\\varepsilon}{\\lambda}-\\frac{1}{3}$。由于 $p_3=1$，$p_4=1+\\frac{\\varepsilon}{\\lambda}-\\frac{1}{3} = \\frac{2}{3}+\\frac{\\varepsilon}{\\lambda}$。\n    - $p_5-p_4 = \\frac{u_5-f_5}{\\lambda} = \\frac{c_2-(M+\\varepsilon)}{\\lambda} = \\frac{-\\lambda/3}{\\lambda} = -\\frac{1}{3} \\implies p_5 = p_4-\\frac{1}{3} = \\frac{1}{3}+\\frac{\\varepsilon}{\\lambda}$。\n对于 $i \\in \\{1,2,4,5\\}$，条件 $|p_i| \\le 1$ 必须成立。由于 $\\varepsilon>0, \\lambda>0$，我们只需要检查上界。\n$|p_2| = |p_4| = \\frac{2}{3} + \\frac{\\varepsilon}{\\lambda} \\le 1 \\implies \\frac{\\varepsilon}{\\lambda} \\le \\frac{1}{3} \\implies \\lambda \\ge 3\\varepsilon$。问题陈述了更严格的条件 $\\lambda > 3\\varepsilon$，因此我们有 $|p_2|1$ 和 $|p_4|1$。\n$|p_1| = |p_5| = \\frac{1}{3} + \\frac{\\varepsilon}{\\lambda}$。由于 $\\lambda > 3\\varepsilon$，我们有 $\\frac{\\varepsilon}{\\lambda}  \\frac{1}{3}$，所以 $|p_1|  \\frac{1}{3}+\\frac{1}{3} = \\frac{2}{3}  1$。\n所有条件都已满足，所以我们假设的结构是正确的。唯一的极小值点是 $u_1=u_2=u_3=\\varepsilon+\\frac{\\lambda}{3}$ 和 $u_4=u_5=u_6=M+\\varepsilon-\\frac{\\lambda}{3}$。\n\n现在，我们计算所需的量 $E$ 和 $P$。\n\n1.  **边缘保持因子 $E$**：\n    $E = \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}$\n    重构信号中的跳变是 $u_4-u_3 = c_2 - c_1 = M - \\frac{2\\lambda}{3}$。\n    观测值中的跳变是 $f_4-f_3 = M - 2\\varepsilon$。\n    $$ E = \\frac{M - \\frac{2\\lambda}{3}}{M - 2\\varepsilon} $$\n\n2.  **阶梯效应度量 $P$**：\n    $P = \\frac{1}{3}\\sum_{i=1}^{3}(u_{i}-f_{i})^{2}$\n    由于 $u_1=u_2=u_3=c_1=\\varepsilon+\\frac{\\lambda}{3}$，我们计算残差：\n    $u_1-f_1 = c_1 - 0 = \\varepsilon + \\frac{\\lambda}{3}$\n    $u_2-f_2 = c_1 - \\varepsilon = \\frac{\\lambda}{3}$\n    $u_3-f_3 = c_1 - 2\\varepsilon = \\frac{\\lambda}{3} - \\varepsilon$\n    将这些代入 $P$ 的表达式中：\n    $$ P = \\frac{1}{3}\\left[ \\left(\\varepsilon + \\frac{\\lambda}{3}\\right)^2 + \\left(\\frac{\\lambda}{3}\\right)^2 + \\left(\\frac{\\lambda}{3} - \\varepsilon\\right)^2 \\right] $$\n    $$ P = \\frac{1}{3}\\left[ \\left(\\varepsilon^2 + \\frac{2\\varepsilon\\lambda}{3} + \\frac{\\lambda^2}{9}\\right) + \\frac{\\lambda^2}{9} + \\left(\\frac{\\lambda^2}{9} - \\frac{2\\varepsilon\\lambda}{3} + \\varepsilon^2\\right) \\right] $$\n    $$ P = \\frac{1}{3}\\left[ 2\\varepsilon^2 + 3 \\frac{\\lambda^2}{9} \\right] = \\frac{1}{3}\\left[ 2\\varepsilon^2 + \\frac{\\lambda^2}{3} \\right] $$\n    $$ P = \\frac{2}{3}\\varepsilon^2 + \\frac{\\lambda^2}{9} $$\n最终答案是以行矩阵形式表示的对 $(E, P)$。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{M - \\frac{2\\lambda}{3}}{M - 2\\varepsilon}  \\frac{2}{3}\\varepsilon^{2} + \\frac{\\lambda^{2}}{9} \\end{pmatrix} } $$", "id": "3420951"}, {"introduction": "为了将理论与实际应用联系起来，最后一个练习是一个完整的数值实现。你将为一个带有总变分惩罚项的一维平流系统的数据同化问题编写代码求解 [@problem_id:3420901]。通过这个编程实践，你将能直观地观察到，随着正则化参数 $\\lambda$ 的增大，平滑的信号是如何被逐渐转化为一系列分段常数平台，从而在实践中验证阶梯效应的影响。", "problem": "考虑一个长度为 $N = 64$ 的均匀网格上的一维周期性平流系统。设初始状态为一个向量 $x \\in \\mathbb{R}^{N}$。定义线性平流算子 $H \\in \\mathbb{R}^{N \\times N}$ 为周期性右移 $s = 9$ 个网格点，即对于所有索引 $i$，有 $(H x)_i = x_{(i - s) \\bmod N}$。我们通过以下方式生成一个确定性的合成真值 $x^{\\star} \\in \\mathbb{R}^{N}$：\n$$\nx^{\\star}_i = 0.4 + 0.3 \\sin\\left(\\frac{2\\pi i}{N}\\right) + 0.3 \\cos\\left(\\frac{4\\pi i}{N}\\right), \\quad \\text{for } i = 0, 1, \\dots, N-1,\n$$\n其中角度以弧度为单位。我们还定义了一个确定性的振荡扰动 $\\eta \\in \\mathbb{R}^{N}$：\n$$\n\\eta_i = \\sigma \\sin\\left(\\frac{2\\pi k i}{N}\\right), \\quad \\sigma = 0.05,\\; k = 7,\n$$\n同样，角度以弧度为单位。观测到的状态为\n$$\ny = H x^{\\star} + \\eta.\n$$\n\n我们提出以下带有全变分 (TV) 正则化惩罚项的数据同化问题：\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\; J_{\\lambda}(x) := \\frac{1}{2}\\,\\|H x - y\\|_2^2 + \\lambda \\sum_{i=0}^{N-2} \\left|x_{i+1} - x_i\\right|,\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，离散的各向异性全变分是一阶差分绝对值之和。算子 $H$ 是正交的，因为它是一个置换矩阵，即 $H^{\\top} H = I$。\n\n您的任务是，对于几个不同的 $\\lambda$ 值，计算 $J_{\\lambda}(x)$ 的最小值点 $x_{\\lambda}^{\\star}$，然后通过计算 $x_{\\lambda}^{\\star}$ 中分段常数平台的数量来量化阶梯效应。定义一个平台为最大的连续索引区间 $[i_{\\text{start}}, i_{\\text{end}}]$，使得在该区间内的所有连续索引，其绝对差满足 $\\left|x_{j+1} - x_j\\right| \\le \\varepsilon$，其中容差为 $\\varepsilon = 10^{-3}$。平台计数就是从左到右扫描 $x_{\\lambda}^{\\star}$ 时此类区间的数量。\n\n您必须使用基于第一性原理的有原则的优化方法来解决这个凸问题。您可以使用任何适用于凸复合优化的正确算法方法，只要它在该设置下是数学上合理的。所有角度都以弧度为单位，并且没有需要报告的物理单位。\n\n测试套件：\n- 使用上面定义的 $N = 64$、$s = 9$、$\\sigma = 0.05$、$k = 7$ 和 $\\varepsilon = 10^{-3}$。\n- 评估以下正则化强度的平台计数：\n$$\n\\lambda \\in \\{0,\\; 0.05,\\; 0.2,\\; 1.0,\\; 5.0\\}.\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含上述 $\\lambda$ 值的平台计数，按相同顺序排列，格式为用方括号括起来的逗号分隔列表，例如 $[c_1,c_2,c_3,c_4,c_5]$，其中每个 $c_j$ 是一个整数。", "solution": "该问题要求我们解决一个带全变分 (TV) 正则化惩罚项的一维数据同化问题。目标是找到一个目标函数的最小值点，然后通过计算分段常数平台的数量来分析其结构。\n\n首先，我们验证问题陈述。所有参数（$N=64$, $s=9$, $\\sigma=0.05$, $k=7$, $\\varepsilon=10^{-3}$）、定义（函数 $x^{\\star}$、$\\eta$；算子 $H$；目标函数 $J_{\\lambda}(x)$；平台定义）和测试用例（$\\lambda \\in \\{0, 0.05, 0.2, 1.0, 5.0\\}$）都已明确提供。该问题在反问题和凸优化这一成熟领域具有科学依据。该公式是一个标准公式，已知是适定的，对于任何 $\\lambda \\ge 0$都有唯一解。其语言客观且数学上精确。因此，该问题被认为是**有效的**。\n\n需要最小化的目标函数是：\n$$\nJ_{\\lambda}(x) = \\frac{1}{2}\\|H x - y\\|_2^2 + \\lambda \\sum_{i=0}^{N-2} |x_{i+1} - x_i|\n$$\n该函数是两个凸项的和：一个光滑的数据保真项 $f(x) = \\frac{1}{2}\\|H x - y\\|_2^2$ 和一个非光滑的正则化项 $g(x) = \\lambda \\sum_{i=0}^{N-2} |x_{i+1} - x_i|$。这种结构非常适合凸复合优化算法。我们将使用交替方向乘子法 (ADMM)，该方法非常适合这类问题。\n\n为了应用 ADMM，我们引入一个辅助变量 $z \\in \\mathbb{R}^{N-1}$ 和一个有限差分算子 $D \\in \\mathbb{R}^{(N-1) \\times N}$，其中 $(Dx)_i = x_{i+1} - x_i$。该优化问题被重新表述为：\n$$\n\\min_{x, z} \\; \\frac{1}{2}\\|H x - y\\|_2^2 + \\lambda \\|z\\|_1 \\quad \\text{约束条件为} \\quad Dx = z\n$$\n这个约束问题的增广拉格朗日函数是：\n$$\nL_{\\rho}(x, z, u) = \\frac{1}{2}\\|H x - y\\|_2^2 + \\lambda \\|z\\|_1 + \\frac{\\rho}{2}\\|Dx - z + u\\|_2^2 - \\frac{\\rho}{2}\\|u\\|_2^2\n$$\n其中 $\\rho  0$ 是一个惩罚参数，$u \\in \\mathbb{R}^{N-1}$ 是缩放后的对偶变量。ADMM 迭代地最小化关于 $x$ 和 $z$ 的 $L_{\\rho}$，然后更新 $u$。对于迭代次数 $k=0, 1, 2, \\dots$，步骤如下：\n\n1.  **$x$-更新**：我们通过最小化关于 $x$ 的 $L_{\\rho}(x, z^k, u^k)$ 来求解 $x^{k+1}$：\n    $$\n    x^{k+1} = \\arg\\min_x \\left( \\frac{1}{2}\\|Hx - y\\|_2^2 + \\frac{\\rho}{2}\\|Dx - z^k + u^k\\|_2^2 \\right)\n    $$\n    这是一个二次优化问题，通过将梯度设为零来找到最小值：\n    $$\n    H^{\\top}(Hx^{k+1} - y) + \\rho D^{\\top}(Dx^{k+1} - z^k + u^k) = 0\n    $$\n    利用给定的 $H$ 是正交的（$H^{\\top}H = I$，其中 $I$ 是单位矩阵）这一性质，我们得到线性系统：\n    $$\n    (I + \\rho D^{\\top}D) x^{k+1} = H^{\\top}y + \\rho D^{\\top}(z^k - u^k)\n    $$\n    矩阵 $(I + \\rho D^{\\top}D)$ 是对称、正定和三对角的，因为 $D$ 是一阶差分算子。这个系统可以在每次迭代中使用带状线性求解器高效求解。\n\n2.  **$z$-更新**：我们通过最小化关于 $z$ 的 $L_{\\rho}(x^{k+1}, z, u^k)$ 来求解 $z^{k+1}$：\n    $$\n    z^{k+1} = \\arg\\min_z \\left( \\lambda \\|z\\|_1 + \\frac{\\rho}{2}\\|z - (Dx^{k+1} + u^k)\\|_2^2 \\right)\n    $$\n    这是 $\\ell_1$-范数的近端算子，其闭式解由分量形式的软阈值函数 $S_{\\alpha}(\\cdot)$ 给出：\n    $$\n    z_i^{k+1} = S_{\\lambda/\\rho}\\left((Dx^{k+1} + u^k)_i\\right) = \\text{sign}\\left((Dx^{k+1} + u^k)_i\\right) \\max\\left(\\left|(Dx^{k+1} + u^k)_i\\right| - \\frac{\\lambda}{\\rho}, 0\\right)\n    $$\n\n3.  **$u$-更新**：更新对偶变量以强制执行约束 $Dx=z$：\n    $$\n    u^{k+1} = u^k + Dx^{k+1} - z^{k+1}\n    $$\n重复这些迭代，直到满足收敛准则或达到足够大的步数。\n\n对于 $\\lambda = 0$ 的特殊情况，问题简化为无正则化的最小二乘问题：\n$$\n\\min_x J_0(x) = \\frac{1}{2}\\|H x - y\\|_2^2\n$$\n将梯度 $\\nabla J_0(x) = H^{\\top}(Hx - y)$ 设为零并使用 $H^{\\top}H=I$ 得到显式解 $x_0^{\\star} = H^{\\top}y$。由于 $H$ 是周期性右移 $s$ 个单位，其转置 $H^{\\top}$ 是周期性左移 $s$ 个单位。\n\n最后，为了量化阶梯效应，我们计算优化后信号 $x_{\\lambda}^{\\star}$ 中的平台数量。一个平台被定义为信号的一个最大连续块，其中相邻值是“接近的”，即 $|x_{i+1} - x_i| \\le \\varepsilon$。一种等效的计数方法是识别所有发生“跳跃”的位置 $i$，即 $|x_{i+1} - x_i|  \\varepsilon$。平台的数量比跳跃的数量多一。该算法是计算 $x_{\\lambda}^{\\star}$ 相邻元素之间的绝对差，统计有多少个差值超过容差 $\\varepsilon$，然后将此计数加 1。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the TV-regularized data assimilation problem and counts plateaus.\n    \"\"\"\n    # Define problem parameters\n    N = 64\n    s = 9\n    sigma = 0.05\n    k = 7\n    epsilon = 1e-3\n    lambda_values = [0.0, 0.05, 0.2, 1.0, 5.0]\n\n    # Generate synthetic data\n    i = np.arange(N)\n    x_star = 0.4 + 0.3 * np.sin(2 * np.pi * i / N) + 0.3 * np.cos(4 * np.pi * i / N)\n    eta = sigma * np.sin(2 * np.pi * k * i / N)\n    \n    # Advection operator H (right shift) and its transpose H.T (left shift)\n    def op_H(vec):\n        return np.roll(vec, s)\n\n    def op_HT(vec):\n        return np.roll(vec, -s)\n\n    y = op_H(x_star) + eta\n    \n    # Finite difference operator D and its transpose D.T\n    D = np.zeros((N - 1, N))\n    for j in range(N - 1):\n        D[j, j] = -1\n        D[j, j + 1] = 1\n    \n    DT = D.T\n    \n    # ADMM parameters\n    rho = 1.0\n    max_iter = 2000\n\n    results = []\n    for lam in lambda_values:\n        if lam == 0.0:\n            # For lambda=0, the solution is x = H.T @ y\n            x_sol = op_HT(y)\n        else:\n            # ADMM for lam  0\n            # Pre-build the tridiagonal matrix for the x-update solve\n            # A = I + rho * D.T @ D\n            # A is tridiagonal: diag is [1+rho, 1+2*rho, ..., 1+2*rho, 1+rho]\n            # off-diag is -rho\n            ab = np.zeros((3, N))\n            main_diag = 1.0 + rho * 2.0 * np.ones(N)\n            main_diag[0] = 1.0 + rho\n            main_diag[N-1] = 1.0 + rho\n            off_diag_val = -rho\n            \n            ab[0, 1:] = off_diag_val  # Super-diagonal\n            ab[1, :] = main_diag     # Main-diagonal\n            ab[2, :-1] = off_diag_val # Sub-diagonal\n            l_and_u = (1, 1)\n\n            # Initialize ADMM variables\n            x = np.zeros(N)\n            z = np.zeros(N - 1)\n            u = np.zeros(N - 1)\n            \n            # Pre-calculate parts of the RHS for x-update\n            HTy = op_HT(y)\n            \n            for _ in range(max_iter):\n                # x-update: solve (I + rho * D.T @ D)x = HTy + rho * D.T @ (z - u)\n                rhs = HTy + rho * (DT @ (z - u))\n                x = solve_banded(l_and_u, ab, rhs)\n\n                # z-update: soft thresholding\n                v = (D @ x) + u\n                z = np.sign(v) * np.maximum(np.abs(v) - lam / rho, 0)\n                \n                # u-update\n                u = u + (D @ x) - z\n            \n            x_sol = x\n\n        # Count plateaus in the solution x_sol\n        diffs = np.abs(np.diff(x_sol))\n        num_jumps = np.sum(diffs  epsilon)\n        plateau_count = num_jumps + 1\n        results.append(plateau_count)\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3420901"}]}