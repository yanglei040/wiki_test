{"hands_on_practices": [{"introduction": "伴随方法是在大规模变分问题中计算梯度的基石。本练习将重点关注“先离散后优化”的范式，即我们从一个*离散*的模型算子出发来推导其伴随。我们将通过一个简单的线性平流方程，来理解伴随推导的力学过程，并关键地分析由此产生的伴随模型的数值稳定性，这是稳健资料同化系统的一个核心考量因素。[@problem_id:3408488]", "problem": "考虑在周期性域 $x \\in [0,L]$ 上，具有恒定波速 $a > 0$ 的线性平流方程 $u_{t} + a\\,u_{x} = 0$。设空间网格是均匀的，有 $N_{x}$ 个点，间距为 $\\Delta x = L/N_{x}$，并采用周期性索引。设时间步长为 $\\Delta t$，并定义 Courant 数 $C = a\\,\\Delta t/\\Delta x$。考虑使用空间上一阶迎风有限差分和时间上向前欧拉法进行状态更新：\n$$\nu^{n+1}_{j} = u^{n}_{j} - C\\left(u^{n}_{j} - u^{n}_{j-1}\\right), \\quad j = 0,\\dots,N_{x}-1,\n$$\n其中 $j-1$ 采用周期性索引。假设在一个四维变分 (4D-Var) 数据同化设定中，一个二次代价泛函依赖于初始条件 $u^{0}$ 和在最终时间层 $n = N_{t}$ 的模型状态，该依赖关系通过一个线性观测算子 $H$ 和正定协方差矩阵实现，但除了线性和正定性外，不对这些项的具体形式做任何假设。使用离散内积 $\\langle p, q\\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_{x}-1} p_{j} q_{j}$。\n\n任务：\n1. 从一个离散拉格朗日量的定义出发，该拉格朗日量使用拉格朗日乘子 $\\lambda^{n+1}$ 在每个时间步强制执行模型更新约束 $u^{n+1} - M u^{n} = 0$（其中 $M$ 表示由上述格式定义的线性更新算子），推导关于内积 $\\langle \\cdot, \\cdot\\rangle_{\\Delta x}$ 的 $\\lambda^{n}$ 的离散伴随递推关系。将伴随更新明确地表示为 $\\lambda^{n}_{j}$ 关于 $\\lambda^{n+1}$ 的逐点模板。\n2. 通过考虑形如 $\\varphi_{j}(\\theta) = \\exp(i \\theta j)$（其中 $\\theta \\in [-\\pi,\\pi]$）的离散傅里叶模，对周期性边界条件下的齐次伴随递推（无源项）进行 von Neumann 分析。推导伴随放大因子 $G_{\\mathrm{adj}}(\\theta; C)$ 及其大小 $|G_{\\mathrm{adj}}(\\theta; C)|$ 作为 $\\theta$ 和 $C$ 的函数。\n3. 使用大小 $|G_{\\mathrm{adj}}(\\theta; C)|$，确定 $C$ 需满足的条件，使得对于所有傅里叶模，伴随更新在由 $\\langle \\cdot, \\cdot\\rangle_{\\Delta x}$ 导出的离散 $\\ell^{2}$ 范数下是非扩张的，即对所有 $\\theta \\in [-\\pi,\\pi]$ 都有 $|G_{\\mathrm{adj}}(\\theta; C)| \\leq 1$。\n4. 满足此非扩张条件的最大允许 Courant 数 $C_{\\max}$ 是多少？将 $C_{\\max}$ 报告为单个无单位的实数。\n\n你的最终答案必须仅为数字 $C_{\\max}$。无需四舍五入。", "solution": "用户提供了一个来自数值分析和数据同化领域的有效、适定的问题陈述。该问题具有科学依据，并包含了推导出唯一解所需的所有信息。以下步骤将导向最终答案。\n\n### 任务1：离散伴随递推的推导\n\n正向模型是线性平流方程 $u_t + a u_x = 0$ 的一个离散近似。该离散化由空间上的一阶迎风格式和时间上的向前欧拉法给出：\n$$\nu^{n+1}_{j} = u^{n}_{j} - C \\left(u^{n}_{j} - u^{n}_{j-1}\\right)\n$$\n其中 $C = a \\Delta t / \\Delta x$ 是 Courant 数，$u^n_j$ 是在时间层 $n$ 和空间格点 $j$ 的状态。我们可以将其重写为一个线性更新规则：\n$$\nu^{n+1}_{j} = (1-C) u^{n}_{j} + C u^{n}_{j-1}\n$$\n以向量形式，这可以写作 $u^{n+1} = M u^n$，其中 $u^n$ 是在时间 $n$ 的所有空间值的向量，而 $M$ 是一个线性算子，其对向量 $v$ 的作用由 $(Mv)_j = (1-C)v_j + C v_{j-1}$ 给出。\n\n问题指定了一个 4D-Var 设定，其中模型方程被视为约束。构造一个拉格朗日量 $\\mathcal{L}$，它包含一个代价函数 $J(u^0, u^{N_t})$ 和在每个时间步由拉格朗日乘子 $\\lambda^{n+1}$ 强制执行的约束项：\n$$\n\\mathcal{L}(u^0, \\dots, u^{N_t}; \\lambda^1, \\dots, \\lambda^{N_t}) = J(u^0, u^{N_t}) + \\sum_{n=0}^{N_t-1} \\langle \\lambda^{n+1}, u^{n+1} - M u^n \\rangle_{\\Delta x}\n$$\n内积定义为 $\\langle p, q\\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_{x}-1} p_{j} q_{j}$。伴随方程是通过对 $\\mathcal{L}$ 关于状态变量 $u^n$（对于 $n = 1, \\dots, N_t-1$）求 Fréchet 导数并令其为零来找到的。对于一个给定的中间状态 $u^n$，它出现在求和的两项中：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u^n} = \\frac{\\partial}{\\partial u^n} \\left( \\langle \\lambda^n, u^n - M u^{n-1} \\rangle_{\\Delta x} + \\langle \\lambda^{n+1}, u^{n+1} - M u^n \\rangle_{\\Delta x} \\right) = 0\n$$\n$\\langle p, q \\rangle$ 关于 $q$ 的导数是 $p$。因此，第一项关于 $u^n$ 的导数是 $\\lambda^n$。对于第二项，$\\langle \\lambda^{n+1}, -M u^n \\rangle_{\\Delta x} = \\langle -M^* \\lambda^{n+1}, u^n \\rangle_{\\Delta x}$，其中 $M^*$ 是 $M$ 关于内积 $\\langle \\cdot, \\cdot \\rangle_{\\Delta x}$ 的伴随算子。这一项关于 $u^n$ 的导数是 $-M^* \\lambda^{n+1}$。将总导数设为零，得到伴随递推关系：\n$$\n\\lambda^n - M^* \\lambda^{n+1} = 0 \\quad \\implies \\quad \\lambda^n = M^* \\lambda^{n+1}\n$$\n为了找到伴随算子 $M^*$ 的显式模板，我们使用它的定义：对于任意向量 $p, q$，$\\langle M^* p, q \\rangle_{\\Delta x} = \\langle p, M q \\rangle_{\\Delta x}$。\n$$\n\\langle p, M q \\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_x-1} p_j (Mq)_j = \\Delta x \\sum_{j=0}^{N_x-1} p_j \\left( (1-C)q_j + C q_{j-1} \\right)\n$$\n$$\n= \\Delta x \\sum_{j=0}^{N_x-1} (1-C) p_j q_j + \\Delta x \\sum_{j=0}^{N_x-1} C p_j q_{j-1}\n$$\n我们对第二个和式进行变址。令 $k = j-1$。由于周期性边界条件，$j=k+1$，并且对 $k$ 从 $0$ 到 $N_x-1$ 的求和等价于对 $j$ 的求和。\n$$\n= \\Delta x \\sum_{j=0}^{N_x-1} (1-C) p_j q_j + \\Delta x \\sum_{k=0}^{N_x-1} C p_{k+1} q_k = \\Delta x \\sum_{j=0}^{N_x-1} \\left( (1-C)p_j + C p_{j+1} \\right) q_j\n$$\n将此与 $\\langle M^* p, q \\rangle_{\\Delta x} = \\Delta x \\sum_{j=0}^{N_x-1} (M^*p)_j q_j$ 进行比较，我们确定伴随算子的作用：\n$$\n(M^*p)_j = (1-C)p_j + C p_{j+1}\n$$\n因此，表示为 $\\lambda_j^n$ 的逐点模板的伴随递推关系是：\n$$\n\\lambda^n_j = (1-C)\\lambda^{n+1}_j + C \\lambda^{n+1}_{j+1}\n$$\n这是一个一阶下风格式，它是一阶迎风正向模型的伴随。\n\n### 任务2：伴随递推的 Von Neumann 分析\n\n为了进行 von Neumann 稳定性分析，我们考察单个离散傅里叶模 $\\lambda_j = \\exp(i\\theta j)$ 在一个从 $n+1$ 到 $n$ 的后向时间步长内的放大情况。令 $\\lambda_j^{n+1} = \\exp(i\\theta j)$ 和 $\\lambda_j^n = G_{\\mathrm{adj}}(\\theta; C) \\exp(i\\theta j)$，其中 $G_{\\mathrm{adj}}(\\theta; C)$ 是伴随放大因子。将这些代入伴随递推关系中：\n$$\nG_{\\mathrm{adj}}(\\theta; C) \\exp(i\\theta j) = (1-C) \\exp(i\\theta j) + C \\exp(i\\theta(j+1))\n$$\n两边同除以 $\\exp(i\\theta j)$，得到放大因子的表达式：\n$$\nG_{\\mathrm{adj}}(\\theta; C) = (1-C) + C \\exp(i\\theta) = (1-C) + C(\\cos\\theta + i\\sin\\theta)\n$$\n$$\nG_{\\mathrm{adj}}(\\theta; C) = (1-C+C\\cos\\theta) + i(C\\sin\\theta)\n$$\n放大因子的模的平方是 $|G_{\\mathrm{adj}}(\\theta; C)|^2$：\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = (1-C+C\\cos\\theta)^2 + (C\\sin\\theta)^2\n$$\n$$\n= (1-C)^2 + 2C(1-C)\\cos\\theta + C^2\\cos^2\\theta + C^2\\sin^2\\theta\n$$\n$$\n= (1-C)^2 + 2C(1-C)\\cos\\theta + C^2(\\cos^2\\theta+\\sin^2\\theta)\n$$\n$$\n= 1 - 2C + C^2 + (2C-2C^2)\\cos\\theta + C^2 = 1 - 2C + 2C^2 + (2C-2C^2)\\cos\\theta\n$$\n$$\n= 1 + (2C^2-2C) - (2C^2-2C)\\cos\\theta = 1 + 2C(C-1)(1-\\cos\\theta)\n$$\n使用半角恒等式 $1-\\cos\\theta = 2\\sin^2(\\theta/2)$，我们得到：\n$$\n|G_{\\mathrm{adj}}(\\theta; C)|^2 = 1 + 4C(C-1)\\sin^2(\\theta/2)\n$$\n其大小为 $|G_{\\mathrm{adj}}(\\theta; C)| = \\sqrt{1 + 4C(C-1)\\sin^2(\\theta/2)}$。\n\n### 任务3：非扩张性条件\n\n如果对于所有傅里叶模，放大因子的大小小于或等于 $1$，即对于所有 $\\theta \\in [-\\pi, \\pi]$ 都有 $|G_{\\mathrm{adj}}(\\theta; C)| \\le 1$，则伴随更新是非扩张的。这等价于 $|G_{\\mathrm{adj}}(\\theta; C)|^2 \\le 1$。\n$$\n1 + 4C(C-1)\\sin^2(\\theta/2) \\leq 1\n$$\n两边减 $1$ 得到：\n$$\n4C(C-1)\\sin^2(\\theta/2) \\leq 0\n$$\n项 $\\sin^2(\\theta/2)$ 对所有实数 $\\theta$ 都是非负的。常数 $4$ 是正的。问题陈述 $a>0$，这意味着 $C = a\\Delta t/\\Delta x > 0$，因为 $\\Delta t$ 和 $\\Delta x$ 都是正的。为了使这个不等式对所有 $\\theta$（包括那些 $\\sin^2(\\theta/2) > 0$ 的情况）都成立，我们必须有：\n$$\nC(C-1) \\leq 0\n$$\n由于 $C > 0$，我们可以除以 $C$ 而不改变不等式的方向：\n$$\nC - 1 \\leq 0 \\implies C \\leq 1\n$$\n将此与物理约束 $C > 0$ 相结合，伴随更新为非扩张的条件是 $0 < C \\leq 1$。\n\n### 任务4：最大允许 Courant 数\n\n非扩张性的条件是 Courant 数 $C$ 必须在区间 $(0, 1]$ 内。满足此条件的最大 $C$ 值是 $1$。\n因此，最大允许 Courant 数 $C_{\\max}$ 是 $1$。", "answer": "$$\\boxed{1}$$", "id": "3408488"}, {"introduction": "一旦我们推导并编写了伴随模型（特别是对于复杂的非线性系统），验证其正确性就至关重要。梯度计算中的一个微小错误可能导致优化算法收敛失败或得到次优解。本练习将介绍“梯度检验”或泰勒余项检验，这是一个强大的数值工具，用以确认我们实现的梯度是否与理论预测的行为一致。[@problem_id:3408591]", "problem": "构建一个完整的、可运行的程序，使用泰勒余项检验来验证一个变分数据同化目标的伴随派生梯度的二阶一致性。从强约束变分数据同化目标的基本定义和标量场的泰勒定理出发。您必须仅使用核心定义和链式法则推导梯度的算法形式，然后设计数值实验，以验证当梯度正确时泰勒余项预期的二阶行为，并在梯度被有意扰动时检测到其精度的损失。\n\n给定以下设置。设状态维度为 $n=5$，观测维度为 $m=3$。定义背景项，其背景态为 $x_b \\in \\mathbb{R}^n$，对称正定背景协方差为 $B \\in \\mathbb{R}^{n \\times n}$。定义前向模型（单步模型算子）$M:\\mathbb{R}^n \\to \\mathbb{R}^n$ 和观测算子 $H:\\mathbb{R}^n \\to \\mathbb{R}^m$。设观测值为 $y \\in \\mathbb{R}^m$，对称正定观测协方差为 $R \\in \\mathbb{R}^{m \\times m}$。考虑目标函数\n$$\nJ(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big).\n$$\n使用以下具体的、完全指定的要素：\n\n- 维度：$n=5$, $m=3$。\n- 时间步长：$\\Delta t = 0.1$。\n- 背景和观测协方差：按如下方式确定性地构造 $B$ 和 $R$。设 $L_B \\in \\mathbb{R}^{n \\times n}$ 是一个下三角矩阵，其对角线元素为 $[1.0,\\,1.2,\\,1.4,\\,1.6,\\,1.8]$，次对角线元素为 $0.05$（所有其他元素为 $0$）。设置 $B = L_B L_B^\\top$。设 $L_R \\in \\mathbb{R}^{m \\times m}$ 是一个下三角矩阵，其对角线元素为 $[0.7,\\,0.9,\\,1.1]$，次对角线元素为 $0.02$，并设置 $R = L_R L_R^\\top$。\n- 背景态：$x_b = [0.5,\\,-0.3,\\,0.8,\\,-1.0,\\,0.2]^\\top$。\n- 真值和观测：设 $x_{\\text{true}} = [-0.4,\\,0.7,\\,-0.2,\\,0.3,\\,-0.6]^\\top$，定义 $y = H(M(x_{\\text{true}}))$，不添加噪声。\n- 前向模型：对任意 $x \\in \\mathbb{R}^n$，定义 $f(x) = \\sin(x) + 0.1\\,x \\odot x$，其中 $\\sin(\\cdot)$ 和 $\\odot$ 逐元素作用，并设置 $M(x) = x + \\Delta t\\, f(x)$。\n- 观测算子：设 $C \\in \\mathbb{R}^{m \\times n}$ 为\n$$\nC = \\begin{bmatrix}\n1  & 0  & 0.2 & 0  & 0\\\\\n0  & 0.5 & 0  & 0.1 & 0\\\\\n0  & 0  & 0  & 0.3 & 1\n\\end{bmatrix},\n$$\n对任意 $z \\in \\mathbb{R}^n$，定义 $H(z) \\in \\mathbb{R}^m$ 为 $H(z) = C z + 0.05\\, [z_1^2, z_2^2, \\dots, z_m^2]^\\top$。\n- 评估状态：定义 $x_0 = [0.1,\\, -0.2,\\, 0.3,\\, -0.4,\\, 0.5]^\\top$。\n- 方向向量：设 $p$ 为一个确定性的单位向量，定义为 $p = \\frac{1}{\\|v\\|}v$，其中 $v = [1.0,\\, -2.0,\\, 3.0,\\, -4.0,\\, 5.0]^\\top$。\n\n使用基础的矩阵微积分和链式法则，将梯度 $\\nabla J(x)$ 表示为 $B^{-1}$、$R^{-1}$ 以及 $M$ 和 $H$ 的雅可比矩阵的函数。然后，基于标量场泰勒定理的恒等式实现泰勒余项检验：\n$$\nJ(x + \\epsilon p) - J(x) \\;=\\; \\epsilon\\, \\nabla J(x)^\\top p \\;+\\; \\mathcal{O}(\\epsilon^2),\n$$\n对于小的 $|\\epsilon|$，使用一系列 $\\epsilon$ 值。对于每次检验，计算一阶余项\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x + \\epsilon p) - J(x) - \\epsilon\\, \\nabla J(x)^\\top p\\right|.\n$$\n通过对 $\\log r_1(\\epsilon)$ 与 $\\log \\epsilon$ 的关系进行直线拟合来估计经验收敛率 $\\alpha$，并报告其斜率 $\\alpha$，对于正确的梯度，该值应约等于 $2$。\n\n设计并执行以下测试套件，该套件必须内置于程序中，无需用户输入：\n\n- 测试 A（非线性，正确梯度）：使用上面定义的 $M$ 和 $H$ 以及解析推导的正确梯度 $\\nabla J(x)$。在 $x=x_0$ 处，沿方向 $p$，使用一组在 $[10^{-1},\\, 10^{-5}]$ 内递减的 $\\epsilon$ 值来评估斜率 $\\alpha_A$。该组值应包括 $10^{-k}$ 以及在适用情况下近似几何中间值，如 $3\\times 10^{-k}$（对于整数 $k$）。\n- 测试 B（非线性，有意错误的梯度）：重复测试 A，但用一个特意不正确的变体替换伴随传播步骤，该变体在梯度组装中需要 $M'(x)^\\top$ 的地方使用了 $M'(x)$。将得到的斜率记为 $\\alpha_B$；它应该接近 $1$ 而不是 $2$。\n- 测试 C（线性二次参考）：定义一个线性模型和观测算子 $M_{\\text{lin}}(x) = A x$ 和 $H_{\\text{lin}}(z) = C z$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是\n$$\nA = I + \\Delta t\\, \\mathrm{diag}([0.2,\\,-0.1,\\,0.05,\\,-0.2,\\,0.1]),\n$$\n其中 $I$ 是单位矩阵，$\\mathrm{diag}(\\cdot)$ 是具有给定条目的对角矩阵。使用相同的 $C$、$B$、$R$、$x_b$、$x_0$ 和 $p$，并定义 $y_{\\text{lin}} = H_{\\text{lin}}(M_{\\text{lin}}(x_{\\text{true}}))$。在这种线性二次设置中使用精确梯度计算斜率 $\\alpha_C$；在舍入误差占主导地位之前，它应该接近 $2$。\n- 测试 D（正交方向）：回到非线性的 $M$ 和 $H$，并通过将 $p$ 投影到与 $\\nabla J(x_0)$ 正交的方向并进行归一化，构造一个单位方向 $p_\\perp$ 使得 $\\nabla J(x_0)^\\top p_\\perp = 0$ 在数值上成立。使用 $r_1(\\epsilon)$ 计算斜率 $\\alpha_D$；它应该接近 $2$，因为一阶项在这个方向上消失了。\n\n您的程序必须：\n\n- 仅使用基本线性代数和应用于指定 $M$ 和 $H$ 的链式法则来实现 $J(x)$ 及其梯度。\n- 通过在伴随累积中用 $M'(x)$ 替换 $M'(x)^\\top$ 来实现测试 B 的错误梯度。\n- 使用一个固定的 $\\epsilon$ 值序列，包含 $[10^{-1},\\,3\\cdot 10^{-2},\\,10^{-2},\\,3\\cdot 10^{-3},\\,10^{-3},\\,3\\cdot 10^{-4},\\,10^{-4},\\,3\\cdot 10^{-5},\\,10^{-5}]$。\n- 通过对整个序列中 $\\log r_1(\\epsilon)$ 对 $\\log \\epsilon$ 的最小二乘线性回归来估计斜率 $\\alpha_A, \\alpha_B, \\alpha_C, \\alpha_D$，排除任何因 $r_1(\\epsilon)$ 在浮点运算中下溢为零的 $\\epsilon$ 值。\n\n最终输出格式：您的程序应生成单行输出，其中包含四个估计的斜率，四舍五入到三位小数，格式为用方括号括起来的逗号分隔列表，顺序为 $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$。不涉及任何物理单位。所有角度（如有）必须被视为无量纲实数值。输出是实数。", "solution": "该问题要求构建一个数值实验，以验证一个变分数据同化目标函数的伴随派生梯度的正确性。验证将通过泰勒余项检验来执行，该检验依赖于一阶泰勒展开中余项的二阶行为。分析从梯度的形式化推导开始，然后进行四个特定数值测试的设计和实现。\n\n首先，我们建立理论基础。目标函数 $J(x)$ 被定义为背景项 $J_b(x)$ 和观测项 $J_o(x)$ 的和：\n$$\nJ(x) = J_b(x) + J_o(x) = \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) \\;+\\; \\tfrac{1}{2}\\,\\big(H(M(x)) - y\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n其中 $x \\in \\mathbb{R}^n$ 是控制变量（模型状态）。其他量在问题描述中已定义：$x_b$ 是背景态，$B$ 是背景误差协方差，$y$ 是观测向量，$R$ 是观测误差协方差，$M$ 是前向模型，$H$ 是观测算子。\n\n验证方法基于标量场 $J:\\mathbb{R}^n \\to \\mathbb{R}$ 的泰勒定理。将 $J(x)$ 在点 $x_0$ 处沿方向 $p \\in \\mathbb{R}^n$ 展开：\n$$\nJ(x_0 + \\epsilon p) = J(x_0) + \\epsilon\\, \\nabla J(x_0)^\\top p + \\tfrac{\\epsilon^2}{2} p^\\top \\nabla^2 J(x_0) p + \\mathcal{O}(\\epsilon^3)\n$$\n其中 $\\epsilon$ 是一个小的标量，$\\nabla J(x_0)$ 是 $J$ 在 $x_0$ 处的梯度，$\\nabla^2 J(x_0)$ 是海森矩阵。如果我们计算的梯度是正确的，那么定义为\n$$\nr_1(\\epsilon) \\;=\\; \\left|J(x_0 + \\epsilon p) - J(x_0) - \\epsilon\\, \\nabla J(x_0)^\\top p\\right|\n$$\n的一阶余项必须由二阶项主导：\n$$\nr_1(\\epsilon) = \\left|\\tfrac{\\epsilon^2}{2} p^\\top \\nabla^2 J(x_0) p + \\mathcal{O}(\\epsilon^3)\\right| \\approx C \\epsilon^2\n$$\n对于某个常数 $C$（假设 $p^\\top \\nabla^2 J(x_0) p \\neq 0$）。取对数后，我们发现 $\\log r_1(\\epsilon) \\approx \\log C + 2 \\log \\epsilon$。这意味着 $\\log r_1(\\epsilon)$ 对 $\\log \\epsilon$ 的图像应该是一条斜率近似为 $2$ 的直线。如果计算的梯度不正确，那么 $\\epsilon\\, \\nabla J(x_0)^\\top p$ 项将不能正确抵消 $J$ 的一阶变化，余项 $r_1(\\epsilon)$ 将是 $\\mathcal{O}(\\epsilon)$ 阶的，导致斜率近似为 $1$。\n\n接下来，我们推导梯度 $\\nabla J(x)$。背景项 $J_b(x)$ 的梯度是二次型的标准形式：\n$$\n\\nabla J_b(x) = B^{-1}(x - x_b)\n$$\n对于观测项 $J_o(x)$，我们应用链式法则。设 $d(x) = H(M(x)) - y$。$d(x)$ 关于 $x$ 的雅可比矩阵，根据链式法则，是 $H$（在 $M(x)$ 处求值）和 $M$（在 $x$ 处求值）的雅可比矩阵的乘积：\n$$\n\\frac{\\partial d}{\\partial x} = H'(M(x)) M'(x)\n$$\n$J_o(x) = \\frac{1}{2} d(x)^\\top R^{-1} d(x)$ 的梯度则为：\n$$\n\\nabla J_o(x) = \\left(\\frac{\\partial d}{\\partial x}\\right)^\\top R^{-1} d(x) = \\big(H'(M(x)) M'(x)\\big)^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n利用属性 $(AB)^\\top = B^\\top A^\\top$，我们得到伴随公式：\n$$\n\\nabla J_o(x) = (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n总梯度是这两个分量的和：\n$$\n\\nabla J(x) = B^{-1}(x - x_b) + (M'(x))^\\top (H'(M(x)))^\\top R^{-1} \\big(H(M(x)) - y\\big)\n$$\n为了实现这一点，我们需要特定模型函数的雅可比矩阵。\n前向模型是 $M(x) = x + \\Delta t\\, f(x)$，其中 $f(x)_i = \\sin(x_i) + 0.1\\, x_i^2$。由于 $f$ 逐元素作用，其雅可比矩阵 $f'(x)$ 是一个对角矩阵，对角线元素为 $\\frac{\\partial f_i}{\\partial x_i} = \\cos(x_i) + 0.2\\, x_i$。因此，$M(x)$ 的雅可比矩阵是：\n$$\nM'(x) = I + \\Delta t\\, \\mathrm{diag}\\big(\\cos(x) + 0.2\\, x\\big)\n$$\n观测算子由 $H(z)_i = (C z)_i + 0.05 z_i^2$ 给出，其中 $i=1, \\dots, m$。其雅可比矩阵 $H'(z) \\in \\mathbb{R}^{m \\times n}$ 的元素为：\n$$\n[H'(z)]_{ij} = \\frac{\\partial H_i}{\\partial z_j} = C_{ij} + 0.1 z_i \\delta_{ij}\n$$\n其中 $\\delta_{ij}$ 是克罗内克 delta 符号。这意味着 $H'(z)$ 是矩阵 $C$ 加上一个在其前 $m$ 个对角线元素上包含 $0.1 z_i$ 的对角矩阵。\n\n该问题指定了四个测试：\n**测试 A（非线性，正确梯度）：** 此测试实现推导出的梯度 $\\nabla J(x_0)$，预计将产生一个收敛率 $\\alpha_A \\approx 2$。\n\n**测试 B（非线性，错误梯度）：** 此测试旨在通过将 $(M'(x))^\\top$ 替换为 $M'(x)$ 来使用不正确的梯度。然而，一个关键的观察是，指定的模型 $M(x)$ 导出的雅可比矩阵是对称的，$M'(x) = (M'(x))^\\top$，因为它是单位矩阵和一个对角矩阵的和。因此，这个“错误的”梯度与正确的梯度是相同的。问题中预期的 $\\alpha_B \\approx 1$ 将不会实现；相反，我们必须预测 $\\alpha_B \\approx 2$。这一结果突显了验证的一个重要方面：如果特定的数据或模型具有掩盖错误的对称性，测试可能无法揭示错误。\n\n**测试 C（线性二次参考）：** 对于线性模型 $M_{\\text{lin}}(x)=Ax$ 和算子 $H_{\\text{lin}}(z)=Cz$，目标函数是纯二次的。泰勒展开到二阶是精确的，意味着 $r_1(\\epsilon) = |\\frac{\\epsilon^2}{2} p^\\top \\nabla^2 J p|$。$\\log r_1(\\epsilon)$ 对 $\\log \\epsilon$ 的关系应该是完全线性的，斜率恰好为 $2$，仅受浮点精度限制。这为“完美”的二阶收敛提供了一个基准。\n\n**测试 D（正交方向）：** 在此测试中，方向向量 $p_\\perp$ 被构造成与梯度正交，即 $\\nabla J(x_0)^\\top p_\\perp = 0$。泰勒展开随后简化为 $J(x_0 + \\epsilon p_\\perp) - J(x_0) = \\mathcal{O}(\\epsilon^2)$，余项变为 $r_1(\\epsilon) = |J(x_0 + \\epsilon p_\\perp) - J(x_0)|$。此余项仍然是 $\\mathcal{O}(\\epsilon^2)$ 阶的，因此斜率 $\\alpha_D$ 预计接近 $2$。此测试证实了即使当一阶项恒为零时，函数在二次行为上的表现也能被正确捕捉。\n\n程序通过定义 $J$、$\\nabla J$、模型及其雅可比矩阵的函数来实现这些推导和测试。然后，它为四个测试中的每一个遍历指定的 $\\epsilon$ 值，计算余项，并使用 `numpy.polyfit` 对数据的对数执行最小二乘线性回归，以估计收敛斜率 $\\alpha_A、\\alpha_B、\\alpha_C$ 和 $\\alpha_D$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and executes a test suite to verify an adjoint-derived gradient\n    for a variational data assimilation objective function using Taylor remainder tests.\n    \"\"\"\n    # --- Problem Setup: Constants and Givens ---\n    n = 5  # State dimension\n    m = 3  # Observation dimension\n    delta_t = 0.1\n\n    # Background state\n    xb = np.array([0.5, -0.3, 0.8, -1.0, 0.2])\n    # True state for generating observations\n    xtrue = np.array([-0.4, 0.7, -0.2, 0.3, -0.6])\n    # State for evaluation\n    x0 = np.array([0.1, -0.2, 0.3, -0.4, 0.5])\n\n    # Direction vector p\n    v = np.array([1.0, -2.0, 3.0, -4.0, 5.0])\n    p = v / np.linalg.norm(v)\n\n    # Covariance matrices B and R\n    L_B = np.diag([1.0, 1.2, 1.4, 1.6, 1.8])\n    for i in range(1, n):\n        L_B[i, i - 1] = 0.05\n    B = L_B @ L_B.T\n    B_inv = np.linalg.inv(B)\n\n    L_R = np.diag([0.7, 0.9, 1.1])\n    for i in range(1, m):\n        L_R[i, i - 1] = 0.02\n    R = L_R @ L_R.T\n    R_inv = np.linalg.inv(R)\n\n    # Observation operator matrix C\n    C = np.array([\n        [1, 0, 0.2, 0, 0],\n        [0, 0.5, 0, 0.1, 0],\n        [0, 0, 0, 0.3, 1]\n    ])\n\n    # Linear model matrix A for Test C\n    A = np.eye(n) + delta_t * np.diag([0.2, -0.1, 0.05, -0.2, 0.1])\n    \n    # Epsilon values for Taylor test\n    epsilons = np.array([\n        1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5, 1e-5\n    ])\n\n    # --- Model and Operator Definitions ---\n\n    # Nonlinear forward model M(x)\n    def M_nl(x):\n        f = np.sin(x) + 0.1 * x**2\n        return x + delta_t * f\n\n    # Nonlinear observation operator H(z)\n    def H_nl(z):\n        h = C @ z\n        h_nonlinear_part = np.zeros_like(h)\n        h_nonlinear_part[:m] = 0.05 * z[:m]**2\n        return h + h_nonlinear_part\n\n    # Jacobian of M_nl(x)\n    def M_nl_jac(x):\n        diag_f_prime = np.cos(x) + 0.2 * x\n        return np.eye(n) + delta_t * np.diag(diag_f_prime)\n\n    # Jacobian of H_nl(z)\n    def H_nl_jac(z):\n        jac = np.copy(C)\n        jac[:m, :m] += np.diag(0.1 * z[:m])\n        return jac\n\n    # Linear model and operator for Test C\n    def M_lin(x):\n        return A @ x\n    def H_lin(z):\n        return C @ z\n\n    # --- Objective Function and Gradient ---\n\n    def objective_J(x, y, M_func, H_func, B_inv, R_inv, xb):\n        J_b = 0.5 * (x - xb).T @ B_inv @ (x - xb)\n        hx = H_func(M_func(x))\n        J_o = 0.5 * (hx - y).T @ R_inv @ (hx - y)\n        return J_b + J_o\n\n    def gradient_J(x, y, M_func, H_func, B_inv, R_inv, xb, M_jac_T_provider, H_jac_func):\n        # Background gradient component\n        grad_b = B_inv @ (x - xb)\n        \n        # Observation gradient component (adjoint formulation)\n        z = M_func(x)\n        hx = H_func(z)\n        H_jac_val = H_jac_func(z) # H'(M(x))\n        M_jac_T_val = M_jac_T_provider(x) # M'(x)^T (or faulty version)\n        \n        # Adjoint variable: (H'(z))^T * R^-1 * (H(z) - y)\n        adj = H_jac_val.T @ (R_inv @ (hx - y))\n        # Propagate adjoint variable back through model: (M'(x))^T * adj\n        grad_o = M_jac_T_val @ adj\n        \n        return grad_b + grad_o\n\n    # --- Test Execution ---\n    \n    def run_taylor_test(x0_test, p_test, grad_j_val, M_func, H_func, y_test, B_inv, R_inv, xb):\n        J0 = objective_J(x0_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n        gradJ_p = grad_j_val.T @ p_test\n        \n        log_eps, log_remainders = [], []\n        for eps in epsilons:\n            J_eps = objective_J(x0_test + eps * p_test, y_test, M_func, H_func, B_inv, R_inv, xb)\n            remainder = abs(J_eps - J0 - eps * gradJ_p)\n            if remainder > 1e-15: # Avoid log(0)\n                log_eps.append(np.log(eps))\n                log_remainders.append(np.log(remainder))\n        \n        # Perform linear regression to find slope\n        if len(log_eps) > 1:\n            slope, _ = np.polyfit(log_eps, log_remainders, 1)\n            return slope\n        return np.nan\n\n    all_slopes = []\n\n    # Generate observation data `y` for nonlinear tests\n    y_nl = H_nl(M_nl(xtrue))\n\n    # --- Test A: Nonlinear, Correct Gradient ---\n    grad_A = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x).T, H_nl_jac)\n    alpha_A = run_taylor_test(x0, p, grad_A, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_A)\n\n    # --- Test B: Nonlinear, Faulty Gradient ---\n    # Faulty implementation using M'(x) instead of M'(x)^T\n    # Note: For this problem's M, M'(x) is symmetric, so this \"faulty\"\n    # gradient is identical to the correct one. Expected slope is ~2.\n    grad_B = gradient_J(x0, y_nl, M_nl, H_nl, B_inv, R_inv, xb, \n                        lambda x: M_nl_jac(x), H_nl_jac) # Using jacobian, not its transpose\n    alpha_B = run_taylor_test(x0, p, grad_B, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_B)\n\n    # --- Test C: Linear-Quadratic Reference ---\n    y_lin = H_lin(M_lin(xtrue))\n    \n    def M_lin_jac_T(x): # M_lin'(x) = A, so M_lin'(x)^T = A^T\n        return A.T\n    def H_lin_jac(z): # H_lin'(z) = C\n        return C\n        \n    grad_C = gradient_J(x0, y_lin, M_lin, H_lin, B_inv, R_inv, xb, M_lin_jac_T, H_lin_jac)\n    alpha_C = run_taylor_test(x0, p, grad_C, M_lin, H_lin, y_lin, B_inv, R_inv, xb)\n    all_slopes.append(alpha_C)\n    \n    # --- Test D: Orthogonal Direction ---\n    g_nl = grad_A # Use correct nonlinear gradient\n    # Project p to be orthogonal to the gradient g_nl\n    p_ortho = p - (g_nl.T @ p / (g_nl.T @ g_nl)) * g_nl\n    p_ortho_norm = p_ortho / np.linalg.norm(p_ortho)\n    \n    alpha_D = run_taylor_test(x0, p_ortho_norm, g_nl, M_nl, H_nl, y_nl, B_inv, R_inv, xb)\n    all_slopes.append(alpha_D)\n\n    # --- Final Output ---\n    formatted_slopes = [f\"{s:.3f}\" for s in all_slopes]\n    print(f\"[{','.join(formatted_slopes)}]\")\n\nsolve()\n```", "id": "3408591"}, {"introduction": "有了经过验证的梯度，我们现在可以在优化算法中使用它了。对于增量式四维变分法中出现的大型二次子问题，共轭梯度（CG）方法是首选的求解器。本练习模拟了业务化资料同化中的一项关键任务：使用“无矩阵”方法实现CG算法的一个迭代步，其中Hessian矩阵的作用是通过连续调用正向和伴随模型来计算的，而无需显式地构建该矩阵。[@problem_id:3408588]", "problem": "给定一个在单次内循环迭代中，以预处理控制变量表示的线性化增量四维变分（4D-Var）数据同化子问题。设状态增量为 $\\delta x \\in \\mathbb{R}^n$，预处理控制变量为 $v \\in \\mathbb{R}^n$，其定义为 $\\delta x = B^{1/2} v$，其中 $B \\in \\mathbb{R}^{n \\times n}$ 是背景误差协方差，$B^{1/2}$ 是其对称正定平方根。令 $M \\in \\mathbb{R}^{n \\times n}$ 表示数据同化窗口内的切线性模型算子，$H \\in \\mathbb{R}^{m \\times n}$ 表示线性化观测算子，$R \\in \\mathbb{R}^{m \\times m}$ 表示观测误差协方差，其对称正定平方根为 $R^{1/2}$，逆平方根为 $R^{-1/2}$。以预处理变量表示的背景项为 $\\frac{1}{2}\\lVert v - v_b \\rVert_2^2$，观测失配为 $\\frac{1}{2}\\lVert R^{-1/2}(H M B^{1/2} v - d)\\rVert_2^2$，其中 $v_b \\in \\mathbb{R}^n$ 和 $d \\in \\mathbb{R}^m$ 是给定的。\n\n从 $v$ 的二次目标函数出发，内循环线性系统的一阶最优性（正规）方程形式为\n$$\nA v = b,\n$$\n其中\n$$\nA \\equiv I + Z^\\top Z,\\quad Z \\equiv R^{-1/2} H M B^{1/2},\\quad b \\equiv v_b + Z^\\top R^{-1/2} d,\n$$\n且 $I$ 是 $\\mathbb{R}^n$ 上的单位矩阵。根据构造，矩阵 $A$ 是对称正定的。\n\n你的任务是编写一个程序，该程序仅使用由给定算子 $B^{-1/2}, M, H, R^{-1/2}$ 及隐含的逆 $B^{1/2} = \\left(B^{-1/2}\\right)^{-1}$ 构造的算子作用，对求解对称正定线性系统 $A v = b$ 执行一步共轭梯度（CG）法（由于测试用例维度较低，你可以通过对 $B^{-1/2}$ 求逆来数值计算 $B^{1/2}$）。具体而言，给定一个初始猜测 $v_0$，执行以下单次 CG 迭代：\n- 计算初始残差 $r_0 = b - A v_0$。\n- 设置初始搜索方向 $p_0 = r_0$。\n- 计算步长\n$$\n\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}.\n$$\n- 更新迭代解 $v_1 = v_0 + \\alpha_0 p_0$。\n- 更新残差 $r_1 = r_0 - \\alpha_0 A p_0$。\n报告每个测试用例在此单次 CG 步骤后的欧几里得范数 $\\lVert r_1 \\rVert_2$。\n\n算子实现要求：你不能显式地组装 $A$。相反，对于任意 $x \\in \\mathbb{R}^n$，你必须通过由 $Z$ 和 $Z^\\top$ 导出的复合运算来实现算子作用 $y = A x$，即：\n$$\nA x = x + Z^\\top (Z x),\\quad Z x = R^{-1/2} H M B^{1/2} x,\\quad Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y.\n$$\n所有转置均为通常的欧几里得伴随。\n\n测试套件。对于下述每种情况，$n$ 和 $m$ 分别表示状态和观测维度。所有矩阵都是小型的实值矩阵，并且指定的平方根矩阵是对称正定的。请严格使用给定的参数。计算并返回每个测试用例的单步残差范数 $\\lVert r_1 \\rVert_2$，结果为浮点数。\n\n- 情况 1（理想路径，混合强度观测）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(2, 1, 1/2)$。\n  - $M = \\begin{bmatrix}1  & 0 & 0\\\\ 0 & 1 & 0\\\\ 0 & 0 & 1\\end{bmatrix}$。\n  - $H = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 1\\end{bmatrix}$。\n  - $R^{-1/2} = \\mathrm{diag}(1, 2)$。\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}1\\\\-1\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 情况 2（无观测；仅背景）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(1, 1, 1)$。\n  - $M = I$（3 阶）。\n  - $H = \\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0\\end{bmatrix}$。\n  - $R^{-1/2} = I$（2 阶）。\n  - $v_b = \\begin{bmatrix}1\\\\-1\\\\1/2\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}3\\\\-2\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 情况 3（观测极弱；几乎仅背景）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(4/5, 6/5, 1)$。\n  - $M = I$（3 阶）。\n  - $H = \\begin{bmatrix}1 & 0 & 0\\\\ 0 & 0 & 1\\end{bmatrix}$。\n  - $R^{-1/2} = \\mathrm{diag}(10^{-3}, 2 \\cdot 10^{-3})$。\n  - $v_b = \\begin{bmatrix}1/5\\\\-1/10\\\\1/20\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}1/2\\\\-1\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 情况 4（动力学和完全观测）：\n  - $n = 3$, $m = 3$。\n  - $B^{-1/2} = \\mathrm{diag}(3/2, 3/4, 1/2)$。\n  - $M = \\begin{bmatrix}1 & 1 & 0\\\\ 0 & 1 & 1\\\\ 0 & 0 & 1\\end{bmatrix}$。\n  - $H = I$（3 阶）。\n  - $R^{-1/2} = \\mathrm{diag}(1, 1/2, 1/4)$。\n  - $v_b = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}1/2\\\\-1/2\\\\1\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}$。\n\n- 情况 5（具有非零初始猜测的一般情况）：\n  - $n = 3$, $m = 2$。\n  - $B^{-1/2} = \\mathrm{diag}(6/5, 9/10, 7/10)$。\n  - $M = \\begin{bmatrix}1 & 1/5 & 0\\\\ 0 & 1 & 3/10\\\\ 0 & 0 & 1\\end{bmatrix}$。\n  - $H = \\begin{bmatrix}1/2 & -1/2 & 0\\\\ 0 & 1 & -1\\end{bmatrix}$。\n  - $R^{-1/2} = \\mathrm{diag}(7/10, 13/10)$。\n  - $v_b = \\begin{bmatrix}1/10\\\\-1/5\\\\3/10\\end{bmatrix}$。\n  - $d = \\begin{bmatrix}-1/2\\\\4/5\\end{bmatrix}$。\n  - $v_0 = \\begin{bmatrix}1/20\\\\1/20\\\\-1/10\\end{bmatrix}$。\n\n你的程序必须：\n- 对每种情况，通过数值方法构造 $B^{1/2} = \\left(B^{-1/2}\\right)^{-1}$。\n- 仅使用 $B^{1/2}, M, H, R^{-1/2}$ 及其所需的转置进行算子复合来实现 $y = A x$。\n- 对每种情况，完全按照上述规定执行一步共轭梯度法，并计算 $\\lVert r_1 \\rVert_2$。\n- 输出一行，其中包含五个残差范数的列表，格式为方括号内的逗号分隔列表，例如 $[a_1,a_2,a_3,a_4,a_5]$，其中每个 $a_i$ 都是一个浮点数。\n\n本问题不涉及物理单位或角度。最终输出为不带单位的实数。", "solution": "该问题是有效的。\n\n### 步骤1：提取既定条件\n- **问题领域**：线性化增量四维变分（4D-Var）资料同化子问题。\n- **待求解系统**：一个对称正定线性系统 $A v = b$，其中 $v \\in \\mathbb{R}^n$ 是预处理后的控制变量。\n- **矩阵和向量定义**:\n  - $A \\equiv I + Z^\\top Z$，其中 $I$ 是 $\\mathbb{R}^n$ 上的单位矩阵。\n  - $Z \\equiv R^{-1/2} H M B^{1/2}$。\n  - $b \\equiv v_b + Z^\\top R^{-1/2} d$。\n- **组成算子**:\n  - $B^{1/2} \\in \\mathbb{R}^{n \\times n}$：背景误差协方差 $B$ 的对称正定平方根。问题提供了其逆矩阵 $B^{-1/2}$。\n  - $M \\in \\mathbb{R}^{n \\times n}$：切线性模型算子。\n  - $H \\in \\mathbb{R}^{m \\times n}$：线性化观测算子。\n  - $R^{-1/2} \\in \\mathbb{R}^{m \\times m}$：观测误差协方差 $R$ 的对称正定逆平方根。\n- **给定数据**:\n  - $v_b \\in \\mathbb{R}^n$：一个与背景状态相关的向量。\n  - $d \\in \\mathbb{R}^m$：一个与观测-模型不匹配（新息向量）相关的向量。\n  - $v_0 \\in \\mathbb{R}^n$：解 $v$ 的初始猜测。\n- **任务**：执行单步共轭梯度（CG）算法。\n  1. 计算初始残差: $r_0 = b - A v_0$。\n  2. 设置初始搜索方向: $p_0 = r_0$。\n  3. 计算步长: $\\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0}$。\n  4. 更新迭代解: $v_1 = v_0 + \\alpha_0 p_0$。\n  5. 更新残差: $r_1 = r_0 - \\alpha_0 A p_0$。\n- **输出要求**：为五个测试用例中的每一个报告更新后的残差的欧几里得范数 $\\|r_1\\|_2$。\n- **实现约束**：矩阵 $A$ 不能显式组装。对向量 $x$ 的矩阵作用，即 $A x$，必须通过给定算子的复合来计算：$A x = x + Z^\\top (Z x)$，其中 $Z x = R^{-1/2} H M B^{1/2} x$ 且 $Z^\\top y = B^{1/2} M^\\top H^\\top R^{-1/2} y$。\n\n### 步骤2：验证既定条件\n- **科学基础（关键）**：该问题是在数值天气预报和地球物理资料同化中的标准提法。使用预处理的CG方法求解二次代价函数的正规方程是该领域的一项基石技术。所有概念都基于成熟的线性代数、优化和反演问题理论。该问题在科学上是合理的。\n- **适定性**：系统矩阵 $A = I + Z^\\top Z$ 是对称且正定的。矩阵 $Z^\\top Z$ 是对称半正定的，加上单位矩阵 $I$ 保证了 $A$ 是严格正定的。因此，线性系统 $Av=b$ 有唯一解。共轭梯度算法是求解此类系统的明确定义且收敛的方法。问题是适定的。\n- **客观性（关键）**：问题通过精确的数学符号指定，并为所有测试用例提供了明确的数值数据。没有歧义或主观性。\n- **不完整或矛盾的设定**：问题是自包含的。执行计算所需的所有矩阵、向量和初始条件都为每个测试用- 例提供了。所有矩阵和向量的维度都是一致的。$A$ 和 $b$ 的推导与标准的4D-Var二次代价函数的最小化是一致的。\n- **不切实际或不可行**：维度和数值是为了计算方便而选择的，但在离散化模型的背景下，它们在物理上并非不可能或科学上不合理。\n- **结论**：问题是有效的、适定的，并且解决它所需的所有信息都已提供。\n\n### 步骤3：裁决与行动\n问题有效。将提供完整的解决方案。\n\n### 基于原则的设计\n这个问题的核心是求解线性系统 $A v = b$，它代表了4D-Var资料同化中出现的二次最小化问题的一阶最优性条件（或正规方程）。共轭梯度（CG）方法是一种迭代算法，非常适合求解大规模对称正定（SPD）系统，特别是当系统矩阵 $A$ 不是显式可用，但其对向量的作用可以高效计算时。这被称为“无矩阵”方法。\n\n我们的解决方案通过严格遵守无矩阵约束，实现CG算法的单步迭代。\n\n**1. 算子定义与作用**\n\n系统矩阵定义为 $A \\equiv I + Z^\\top Z$，其中 $Z \\equiv R^{-1/2} H M B^{1/2}$。我们不构造稠密矩阵 $A$，而是实现一个函数，对于任意给定向量 $x$，计算乘积 $y = Ax$。这是通过复合组成算子的作用来实现的：\n- $Z$ 对向量 $x$ 的作用通过一系列矩阵-向量乘积计算：$Z x = R^{-1/2}(H(M(B^{1/2} x)))$。\n- 伴随算子 $Z^\\top$ 对向量 $y$ 的作用也类似计算：$Z^\\top y = B^{1/2}(M^\\top(H^\\top(R^{-1/2} y)))$。注意，由于 $B^{1/2}$ 和 $R^{-1/2}$ 是对称的，它们的转置就是它们自身。\n- $A$ 对 $x$ 的完整作用是 $A x = x + Z^\\top(Z x)$。\n\n这些操作需要矩阵 $B^{1/2}$。它是通过对给定的 $B^{-1/2}$ 进行数值求逆来为每个测试用例计算一次的，即 $B^{1/2} = (B^{-1/2})^{-1}$。\n\n**2. 右端项的构造**\n\n右端向量 $b$ 定义为 $b \\equiv v_b + Z^\\top R^{-1/2} d$。这也是用算子作用来计算的。首先，计算向量 $y_d = R^{-1/2} d$。然后，将 $Z^\\top$ 的作用应用于 $y_d$，结果加到 $v_b$ 上：\n$$ b = v_b + Z^\\top(y_d) = v_b + B^{1/2}(M^\\top(H^\\top(R^{-1/2} d))) $$\n\n**3. 单次CG迭代**\n\n有了计算 $Ax$ 和 $b$ 的能力，我们从初始猜测 $v_0$ 开始执行一次CG算法迭代。\n\n- **初始残差**：算法首先计算初始残差，它衡量初始猜测满足方程的程度：\n$$ r_0 = b - A v_0 $$\n乘积 $A v_0$ 使用上面描述的无矩阵函数计算。\n\n- **初始搜索方向**：在CG算法中，第一个搜索方向就是初始残差：\n$$ p_0 = r_0 $$\n\n- **最优步长**：我们通过计算在A-范数下最小化残差的步长 $\\alpha_0$ 来确定沿搜索方向 $p_0$ 移动的距离。公式为：\n$$ \\alpha_0 = \\frac{r_0^\\top r_0}{p_0^\\top A p_0} $$\n分母项 $p_0^\\top A p_0$ 首先计算向量 $w = A p_0$，然后取点积 $p_0^\\top w$。\n\n- **更新迭代解和残差**：最后，我们更新解向量和残差。问题只要求新残差 $r_1$ 的范数。更新残差的公式比重新计算 $r_1 = b - A v_1$ 的计算成本更低：\n$$ r_1 = r_0 - \\alpha_0 A p_0 $$\n\n- **最终结果**：计算并报告新残差的欧几里得范数 $\\|r_1\\|_2 = \\sqrt{r_1^\\top r_1}$，用于每个测试用例。该值表示单次CG步骤实现的误差减少量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_single_cg_step(case_data):\n    \"\"\"\n    Performs one step of the Conjugate Gradient method for a given test case.\n\n    Args:\n        case_data (dict): A dictionary containing all matrices and vectors for the test case.\n\n    Returns:\n        float: The Euclidean norm of the residual after one CG step, ||r_1||_2.\n    \"\"\"\n    # Unpack all data for the current case\n    B_inv_sqrt = case_data[\"B_inv_sqrt\"]\n    M = case_data[\"M\"]\n    H = case_data[\"H\"]\n    R_inv_sqrt = case_data[\"R_inv_sqrt\"]\n    v_b = case_data[\"v_b\"]\n    d = case_data[\"d\"]\n    v0 = case_data[\"v0\"]\n\n    # 1. Compute B_sqrt and required transposes\n    # As per the problem, B^{1/2} = (B^{-1/2})^{-1}\n    B_sqrt = np.linalg.inv(B_inv_sqrt)\n    M_T = M.T\n    H_T = H.T\n    # R_inv_sqrt is symmetric, so its transpose is itself.\n\n    # 2. Define the matrix-free operator actions\n    def Z_action(x):\n        \"\"\"Computes Zx = R^{-1/2} H M B^{1/2} x.\"\"\"\n        res = B_sqrt @ x\n        res = M @ res\n        res = H @ res\n        res = R_inv_sqrt @ res\n        return res\n\n    def Z_transpose_action(y):\n        \"\"\"Computes Z^T y = B^{1/2} M^T H^T R^{-1/2} y.\"\"\"\n        res = R_inv_sqrt @ y\n        res = H_T @ res\n        res = M_T @ res\n        res = B_sqrt @ res\n        return res\n\n    def A_action(x):\n        \"\"\"Computes Ax = (I + Z^T Z)x = x + Z^T(Z(x)).\"\"\"\n        return x + Z_transpose_action(Z_action(x))\n\n    # 3. Compute the right-hand side vector b\n    # b = v_b + Z^T (R^{-1/2} d)\n    term_in_Z_T = R_inv_sqrt @ d\n    b = v_b + Z_transpose_action(term_in_Z_T)\n\n    # 4. Perform one step of the Conjugate Gradient algorithm\n    # Compute the initial residual: r_0 = b - A v_0\n    Av0 = A_action(v0)\n    r0 = b - Av0\n    \n    # If the initial residual is zero, the initial guess is the exact solution.\n    # The next residual will also be zero.\n    if np.linalg.norm(r0)  1e-15:\n        return 0.0\n\n    # Set the initial search direction: p_0 = r_0\n    p0 = r0\n    \n    # Compute the matrix-vector product A*p_0 needed for alpha_0 and r_1\n    Ap0 = A_action(p0)\n    \n    # Compute the step size: alpha_0 = (r_0^T r_0) / (p_0^T A p_0)\n    r0_dot_r0 = np.dot(r0, r0)\n    p0_dot_Ap0 = np.dot(p0, Ap0)\n\n    # Since A is SPD, p0_dot_Ap0 is zero iff p0 is zero.\n    # This is handled by the initial check on norm(r0).\n    # This check is for robustness.\n    if abs(p0_dot_Ap0)  1e-15:\n        alpha0 = 0.0\n    else:\n        alpha0 = r0_dot_r0 / p0_dot_Ap0\n    \n    # Update the residual: r_1 = r_0 - alpha_0 * A * p_0\n    r1 = r0 - alpha0 * Ap0\n    \n    # 5. Compute and return the Euclidean norm of the new residual, ||r_1||\n    norm_r1 = np.linalg.norm(r1)\n    \n    return norm_r1\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, mixed-strength observations):\n        {\n            \"B_inv_sqrt\": np.diag([2.0, 1.0, 0.5]),\n            \"M\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1.0, 2.0]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([1.0, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 2 (no observations; background-only):\n        {\n            \"B_inv_sqrt\": np.diag([1.0, 1.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.zeros((2, 3)),\n            \"R_inv_sqrt\": np.identity(2),\n            \"v_b\": np.array([1.0, -1.0, 0.5]),\n            \"d\": np.array([3.0, -2.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 3 (very weak observations; nearly background-only):\n        {\n            \"B_inv_sqrt\": np.diag([4.0/5.0, 6.0/5.0, 1.0]),\n            \"M\": np.identity(3),\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            \"R_inv_sqrt\": np.diag([1e-3, 2e-3]),\n            \"v_b\": np.array([1.0/5.0, -1.0/10.0, 1.0/20.0]),\n            \"d\": np.array([0.5, -1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 4 (dynamics and full observations):\n        {\n            \"B_inv_sqrt\": np.diag([3.0/2.0, 3.0/4.0, 1.0/2.0]),\n            \"M\": np.array([[1.0, 1.0, 0.0], [0.0, 1.0, 1.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.identity(3),\n            \"R_inv_sqrt\": np.diag([1.0, 0.5, 0.25]),\n            \"v_b\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.5, -0.5, 1.0]),\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n        },\n        # Case 5 (general case with nonzero initial guess):\n        {\n            \"B_inv_sqrt\": np.diag([6.0/5.0, 9.0/10.0, 7.0/10.0]),\n            \"M\": np.array([[1.0, 1.0/5.0, 0.0], [0.0, 1.0, 3.0/10.0], [0.0, 0.0, 1.0]]),\n            \"H\": np.array([[0.5, -0.5, 0.0], [0.0, 1.0, -1.0]]),\n            \"R_inv_sqrt\": np.diag([7.0/10.0, 13.0/10.0]),\n            \"v_b\": np.array([1.0/10.0, -2.0/10.0, 3.0/10.0]),\n            \"d\": np.array([-0.5, 4.0/5.0]),\n            \"v0\": np.array([1.0/20.0, 1.0/20.0, -1.0/10.0]),\n        }\n    ]\n\n    results = []\n    for case_data in test_cases:\n        norm_r1 = run_single_cg_step(case_data)\n        results.append(norm_r1)\n\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3408588"}]}