{"hands_on_practices": [{"introduction": "在尝试求解一个估计问题之前，我们必须首先确定其解是否存在且唯一。本练习从代数角度探讨了*可观测性*这一基本概念，它将平滑问题构建为最小化一个二次目标函数。该目标函数的Hessian矩阵（即后验精度矩阵）直接揭示了系统的状态是否能被现有信息完全确定。通过分析Hessian矩阵的零空间，我们能够精确定位并量化问题中不可观测的模式。[@problem_id:3406074]", "problem": "要求您通过构造具有大零空间的示例，来形式化并计算线性高斯平滑目标中的不可观测性，然后提出并验证能够正则化后验的最少额外观测。整个过程完全在有限维线性代数中进行，将完整的时空轨迹视为单个堆叠向量，并将所有惩罚项视为二次型。\n\n考虑一个离散时间状态序列 $\\{x_t\\}_{t=0}^T$，其中 $x_t \\in \\mathbb{R}^n$。将轨迹堆叠为 $z \\in \\mathbb{R}^{n(T+1)}$，其中 $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top$。一个二次平滑目标可以写为\n$$\nJ(z) \\;=\\; \\tfrac{1}{2}\\,\\|L z\\|_2^2 \\;+\\; \\tfrac{1}{2}\\,\\|C z - y\\|_2^2,\n$$\n其中 $L$ 编码了平滑或动态惩罚，$C$ 编码了可用的线性观测。在线性高斯贝叶斯解释下，最大后验 (MAP) 估计量与 $J(z)$ 的最小值点重合，其黑塞矩阵 (后验精度矩阵) 为\n$$\n\\mathcal{I} \\;=\\; L^\\top L \\;+\\; C^\\top C.\n$$\n当 $\\mathcal{I}$ 是奇异矩阵时，存在不可观测性；其零空间 $\\mathcal{N}(\\mathcal{I})$ 标识了那些既不受动态/平滑惩罚也不受观测惩罚的方向。$\\mathcal{N}(\\mathcal{I})$ 的维度是 $\\mathcal{I}$ 的零度，等价于 $n(T+1) - \\mathrm{rank}(\\mathcal{I})$。\n\n您将研究两种产生大零空间的典型平滑算子：\n\n- 一阶差分 (随机游走) 平滑：对于 $t \\in \\{0,\\dots,T-1\\}$，惩罚 $x_{t+1} - x_t$。这对应于 $L \\in \\mathbb{R}^{nT \\times n(T+1)}$，其块结构使得第 $t$ 个行块等于 $[0,\\dots,0,-I_n, I_n, 0,\\dots,0]$，作用于 $[x_t^\\top, x_{t+1}^\\top]^\\top$。\n- 二阶差分 (加速度) 平滑：对于 $t \\in \\{1,\\dots,T-1\\}$，惩罚 $x_{t+1} - 2 x_t + x_{t-1}$。这对应于 $L \\in \\mathbb{R}^{n(T-1) \\times n(T+1)}$，其块结构使得第 $t$ 个行块等于 $[0,\\dots,0, I_n, -2 I_n, I_n, 0,\\dots,0]$，作用于 $[x_{t-1}^\\top, x_t^\\top, x_{t+1}^\\top]^\\top$。\n\n为简单起见，假设所有惩罚都是各向同性的，权重为单位矩阵，因此上述的 $2$-范数是欧几里得范数。观测是逐点分量测量：如果在时间 $t$ 观测到 $x_t$ 的分量索引 $S \\subset \\{0,\\dots,n-1\\}$，则对于每个 $(t,i) \\in \\{t\\} \\times S$，$C$ 中包含一行 $e_{t,i}^\\top$，其中 $e_{t,i} \\in \\mathbb{R}^{n(T+1)}$ 是选择时间 $t$ 的第 $i$ 个分量的标准基向量（也就是说，它在对应于 $(t,i)$ 的位置上为 $1$，其他位置为 $0$）。设测量噪声精度等于单位矩阵，因此 $C^\\top C$ 直接贡献于 $\\mathcal{I}$。\n\n您的程序必须为每个指定的测试用例执行以下操作：\n- 针对给定的 $n$ 和 $T$，构造一阶差分或二阶差分算子 $L$。\n- 根据在指定时间观测到的分量集合来构造 $C$。\n- 计算 $\\mathcal{I} = L^\\top L + C^\\top C$ 的零度 $k_0$。\n- 提出一组最少的额外线性观测来正则化后验。您必须通过计算 $\\mathcal{N}(\\mathcal{I})$ 的一组基 $\\{u_j\\}_{j=1}^{k_0}$ 来实现这一点，并将额外观测算子 $C_{\\mathrm{add}}$ 的行设为 $u_j^\\top$。这个选择使用了 $k_0$ 个标量观测，并保证更新后的精度矩阵\n$$\n\\mathcal{I}_{\\mathrm{new}} \\;=\\; \\mathcal{I} \\;+\\; C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n$$\n在 $k_0 > 0$ 时是正定的，在 $k_0 = 0$ 时保持不变。\n- 设 $k_{\\min} = k_0$ 为理论上所需的最少额外标量观测数，并通过检查 $\\mathcal{I}_{\\mathrm{new}}$ 的零度是否为 $0$ 来验证成功。\n\n实现的数值线性代数提示（非解题捷径）：使用 $\\mathcal{I}$ 的对称特征分解来获取其特征值和特征向量，并使用一个数值上合理的阈值来定义“零”特征值。\n\n测试套件。您的程序必须解决以下情况：\n\n- 情况 A：一阶差分，$n = 3$，$T = 5$，无观测。\n- 情况 B：一阶差分，$n = 3$，$T = 5$，在时间 $t = 0$ 观测分量 $\\{0,1\\}$。\n- 情况 C：二阶差分，$n = 2$，$T = 6$，无观测。\n- 情况 D：二阶差分，$n = 2$，$T = 6$，在时间 $t = 0$ 观测分量 $\\{0,1\\}$。\n- 情况 E：二阶差分，$n = 2$，$T = 3$，在时间 $t = 0$ 和 $t = 3$ 观测分量 $\\{0\\}$（即在两个端点都观测分量 $0$）。\n- 情况 F：二阶差分，$n = 1$，$T = 4$，在时间 $t = 0$ 和 $t = 4$ 观测分量 $\\{0\\}$。\n\n所需输出。对于每种情况，输出一个列表 $[k_0, k_{\\min}, s]$，其中 $k_0$ 是计算出的 $\\mathcal{I}$ 的零度，$k_{\\min}$ 是您提出的最少额外标量观测数（根据构造等于 $k_0$），$s$ 是一个布尔值，指示更新后的精度矩阵 $\\mathcal{I}_{\\mathrm{new}}$ 是否数值上满秩（零度等于 $0$）。您的程序应生成单行输出，其中包含情况 A–F 的结果，形式为这些列表的逗号分隔列表，并用方括号括起来；例如，一个有效的格式是 $[[r_1],[r_2],\\dots]$，其中每个 $[r_j]$ 是按指定顺序 A–F 的情况 $j$ 的三元组。", "solution": "该问题要求通过构造和正则化具有奇异后验精度矩阵的系统，来分析线性平滑问题中的不可观测性。状态的整个时空轨迹 $\\{x_t\\}_{t=0}^T$（其中 $x_t \\in \\mathbb{R}^n$）被表示为一个单一的堆叠向量 $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top \\in \\mathbb{R}^{n(T+1)}$。\n\n该平滑问题被构建为最小化一个二次目标函数 $J(z)$：\n$$\nJ(z) = \\tfrac{1}{2} \\|L z\\|_2^2 + \\tfrac{1}{2} \\|C z - y\\|_2^2\n$$\n第一项对应于先验或平滑惩罚，由矩阵 $L$ 编码。第二项表示与一组线性观测 $y$ 不匹配的惩罚，由矩阵 $C$ 编码。在贝叶斯背景下，这对应于为线性高斯模型寻找最大后验 (MAP) 估计。后验精度矩阵，即 $J(z)$ 的黑塞矩阵，由下式给出：\n$$\n\\mathcal{I} = L^\\top L + C^\\top C\n$$\n当 $\\mathcal{I}$ 是奇异矩阵时，就会出现不可观测性，这意味着它有一个非平凡的零空间 $\\mathcal{N}(\\mathcal{I})$。这个零空间的维度，即其零度，量化了状态空间中既不受动态（平滑）也不受观测约束的独立方向的数量。任何向量 $v \\in \\mathcal{N}(\\mathcal{I})$ 都可以加到解 $z^*$ 上而不改变成本 $J$，因为对于任何 $v \\in \\mathcal{N}(\\mathcal{I})$，$v$ 必须同时在 $\\mathcal{N}(L)$ 和 $\\mathcal{N}(C)$ 中（为进行零空间分析，假设 $y=0$），因此 $L(\\alpha v)=0$ 且 $C(\\alpha v)=0$。\n\n解决每个测试用例的步骤如下：\n\n1.  **构造先验精度矩阵 $L^\\top L$**：\n    问题的总维度是 $d = n(T+1)$。矩阵 $L^\\top L$ 是一个 $d \\times d$ 矩阵。\n    -   对于**一阶差分**平滑器，惩罚项是 $x_{t+1} - x_t$（对于 $t=0, \\dots, T-1$）。这意味着 $L$ 的零空间由所有 $t$ 的 $x_t$ 都为常数的轨迹组成。具体来说，如果 $z \\in \\mathcal{N}(L)$，则 $x_0 = x_1 = \\dots = x_T$。这是一个 $n$ 维零空间，由形如 $[v^\\top, v^\\top, \\dots, v^\\top]^\\top$ 的轨迹张成，其中 $v \\in \\mathbb{R}^n$。矩阵 $L$ 可以构造成具有 $n T$ 行，其中对应于时间 $t$ 的每个 $n$ 行块在列块 $t$ 处有一个 $-I_n$ 块，在列块 $t+1$ 处有一个 $I_n$ 块。\n    -   对于**二阶差分**平滑器，惩罚项是 $x_{t+1} - 2x_t + x_{t-1}$（对于 $t=1, \\dots, T-1$）。$L$ 的零空间由随时间线性演化的轨迹组成，即 $x_t = x_0 + t(x_1 - x_0)$。这样的轨迹完全由最初的两个状态 $x_0$ 和 $x_1$ 决定。这构成了一个 $2n$ 维零空间。矩阵 $L$ 被构造成具有 $n(T-1)$ 行，其中对应于时间 $t$ 的每个 $n$ 行块在列块 $t-1, t, t+1$ 处分别有块 $I_n, -2I_n, I_n$。\n    在两种情况下，我们都计算 $L$，然后形成先验精度矩阵 $L^\\top L$。\n\n2.  **构造观测精度矩阵 $C^\\top C$**：\n    观测是特定分量的逐点测量。对 $x_t$ 的第 $i$ 个分量的观测对应于 $C$ 中的一行 $e_{t,i}^\\top$，其中 $e_{t,i}$ 是 $\\mathbb{R}^{n(T+1)}$ 中的标准基向量，在对应于 $x_t$ 的第 $i$ 个分量的位置上为 $1$，其他位置为零。在测量噪声精度为单位矩阵的情况下，观测对后验精度的总贡献是 $C^\\top C = \\sum_{(t,i) \\in \\text{obs}} e_{t,i} e_{t,i}^\\top$。这是一个对角矩阵，在与观测分量对应的索引处为 $1$。我们将此矩阵加到 $L^\\top L$ 上，以形成完整的后验精度矩阵 $\\mathcal{I}$。状态 $x_t$ 的分量 $i$ 的全局索引是 $t \\cdot n + i$。\n\n3.  **计算初始零度 ($k_0$)**：\n    后验精度矩阵 $\\mathcal{I}$ 根据构造是实对称的。我们使用对称特征求解器（`numpy.linalg.eigh`）计算其特征值和特征向量。零度 $k_0$ 是数值上接近于零的特征值的数量，通过一个小的容差（例如，$10^{-9}$）来判断。相应的特征向量 $\\{u_j\\}_{j=1}^{k_0}$ 构成了零空间 $\\mathcal{N}(\\mathcal{I})$ 的一组标准正交基。\n\n4.  **提出并应用正则化**：\n    为了使问题适定，我们必须添加新的观测来惩罚未被观测的模式。问题指定了一种最小正则化策略。我们引入 $k_{\\min} = k_0$ 个新的标量观测。新的观测算子 $C_{\\mathrm{add}}$ 的定义是将其行取为零空间的基向量，即 $C_{\\mathrm{add}} = [u_1, u_2, \\dots, u_{k_0}]^\\top$。对精度矩阵的贡献是 $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}$。由于基向量 $\\{u_j\\}$ 是标准正交的，所以 $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}} = \\sum_{j=1}^{k_0} u_j u_j^\\top$，这是到 $\\mathcal{N}(\\mathcal{I})$ 上的投影矩阵。\n    新的、正则化后的精度矩阵是：\n    $$\n    \\mathcal{I}_{\\mathrm{new}} = \\mathcal{I} + C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n    $$\n\n5.  **验证正则化**：\n    我们通过计算 $\\mathcal{I}_{\\mathrm{new}}$ 的零度来验证正则化的成功。对于原始零空间中的任何向量 $v = \\sum_k \\alpha_k u_k$，有 $\\mathcal{I}v=0$。新矩阵的作用如下：\n    $$\n    \\mathcal{I}_{\\mathrm{new}} v = (\\mathcal{I} + \\sum_{j=1}^{k_0} u_j u_j^\\top) (\\sum_{k=1}^{k_0} \\alpha_k u_k) = 0 + \\sum_{j,k} \\alpha_k u_j (u_j^\\top u_k) = \\sum_{j,k} \\alpha_k u_j \\delta_{jk} = \\sum_k \\alpha_k u_k = v\n    $$\n    这表明张成原始零空间的向量现在是 $\\mathcal{I}_{\\mathrm{new}}$ 的特征向量，特征值为 $1$。对于 $\\mathcal{I}$ 的任何具有非零特征值 $\\lambda$ 的特征向量 $w$（因此 $w \\perp \\mathcal{N}(\\mathcal{I})$），我们有 $\\mathcal{I}_{\\mathrm{new}} w = \\mathcal{I}w + (\\sum u_j u_j^\\top)w = \\lambda w + 0 = \\lambda w$。其他特征值和特征向量保持不变。因此，$\\mathcal{I}_{\\mathrm{new}}$ 没有零特征值并且是正定的。我们通过计算 $\\mathcal{I}_{\\mathrm{new}}$ 的特征值并检查其零度是否为 $0$ 来数值地确认这一点。成功与否被记录为一个布尔值 $s$。\n\n为每个指定的测试用例实现了这一完整过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of test cases for analyzing unobservability\n    in linear-Gaussian smoothing problems.\n    \"\"\"\n    \n    # Test suite as specified in the problem statement\n    test_cases = [\n        # Case A: first-difference, n=3, T=5, no observations\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': []},\n        # Case B: first-difference, n=3, T=5, obs at t=0 of components {0,1}\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': [(0, [0, 1])]},\n        # Case C: second-difference, n=2, T=6, no observations\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': []},\n        # Case D: second-difference, n=2, T=6, obs at t=0 of components {0,1}\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': [(0, [0, 1])]},\n        # Case E: second-difference, n=2, T=3, obs at t=0, t=3 of component {0}\n        {'type': 'second_difference', 'n': 2, 'T': 3, 'obs': [(0, [0]), (3, [0])]},\n        # Case F: second-difference, n=1, T=4, obs at t=0, t=4 of component {0}\n        {'type': 'second_difference', 'n': 1, 'T': 4, 'obs': [(0, [0]), (4, [0])]},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(case['type'], case['n'], case['T'], case['obs'])\n        results.append(result)\n\n    # Format the final output string exactly as required\n    # e.g., [[k0_A,kmin_A,s_A],[k0_B,kmin_B,s_B],...]\n    result_strings = [f\"[{r[0]},{r[1]},{str(r[2]).lower()}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef _solve_case(smoother_type, n, T, observations):\n    \"\"\"\n    Solves a single instance of the observability problem.\n    \"\"\"\n    # Total dimension of the stacked state vector z\n    dim = n * (T + 1)\n    \n    # Numerical tolerance for identifying zero eigenvalues\n    TOL = 1e-9\n\n    # 1. Construct the smoothing operator L\n    if smoother_type == 'first_difference':\n        # L has n*T rows and n*(T+1) columns\n        L = np.zeros((n * T, dim))\n        identity_n = np.eye(n)\n        for t in range(T):\n            row_slice = slice(t * n, (t + 1) * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_t1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_t] = -identity_n\n            L[row_slice, col_slice_t1] = identity_n\n    \n    elif smoother_type == 'second_difference':\n        # L has n*(T-1) rows and n*(T+1) columns\n        L = np.zeros((n * (T - 1), dim))\n        identity_n = np.eye(n)\n        for t_idx, t in enumerate(range(1, T)):\n            row_slice = slice(t_idx * n, (t_idx + 1) * n)\n            col_slice_tm1 = slice((t - 1) * n, t * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_tp1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_tm1] = identity_n\n            L[row_slice, col_slice_t] = -2 * identity_n\n            L[row_slice, col_slice_tp1] = identity_n\n    else:\n        raise ValueError(\"Unknown smoother type\")\n\n    # 2. Construct the posterior precision matrix I\n    # Start with the prior precision L^T L\n    I = L.T @ L\n    \n    # Add observation precision C^T C\n    for t, components in observations:\n        for i in components:\n            idx = t * n + i\n            I[idx, idx] += 1.0\n\n    # 3. Compute initial nullity k0 and find nullspace basis\n    eigenvalues, eigenvectors = np.linalg.eigh(I)\n    is_zero_eigenvalue = np.abs(eigenvalues)  TOL\n    k0 = int(np.sum(is_zero_eigenvalue))\n\n    # 4. Propose and apply regularization\n    k_min = k0\n    \n    if k0 == 0:\n        I_new = I\n    else:\n        nullspace_basis = eigenvectors[:, is_zero_eigenvalue]\n        # C_add.T @ C_add = sum(u_j @ u_j.T)\n        C_add_T_C_add = nullspace_basis @ nullspace_basis.T\n        I_new = I + C_add_T_C_add\n        \n    # 5. Verify successful regularization\n    new_eigenvalues, _ = np.linalg.eigh(I_new)\n    new_nullity = np.sum(np.abs(new_eigenvalues)  TOL)\n    s = (new_nullity == 0)\n    \n    return [k0, k_min, s]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3406074"}, {"introduction": "真实世界的系统很少是完全线性的。本练习将带领我们从线性高斯模型的理想凸优化世界，进入非线性估计的复杂领域。通过一个包含二次非线性观测模型的简单例子，您会发现非线性如何为状态轨迹创造出多个貌似合理的解（即局部最小值）。您将实现如同伦（或称连续法）这样的高级优化策略来探索这个复杂的目标函数地貌，并使用拉普拉斯近似方法来评估和比较不同解的统计证据。[@problem_id:3406013]", "problem": "考虑一个用于在时间 $t = 0, 1, \\dots, T$ 上进行固定区间平滑的离散时间、标量、非线性状态空间模型。隐状态序列为 $x_{0:T} \\equiv (x_0, x_1, \\dots, x_T) \\in \\mathbb{R}^{T+1}$。该模型由以下要素指定：\n\n- 初始状态的高斯先验：$x_0 \\sim \\mathcal{N}(m_0, P_0)$，其中先验均值为 $m_0 \\in \\mathbb{R}$，先验方差为 $P_0 \\in \\mathbb{R}_{0}$。\n- 一个马尔可夫动态模型：$x_{t+1} = f(x_t) + w_t$，其中对于给定的 $a \\in \\mathbb{R}$，$f(x) = a x$，且 $w_t \\sim \\mathcal{N}(0, q)$，过程噪声方差为 $q \\in \\mathbb{R}_{0}$，在时间 $t$ 上独立。\n- 一个非线性观测模型：$y_t = h(x_t) + v_t$，其中 $h(x) = x^2$ 且 $v_t \\sim \\mathcal{N}(0, r)$，观测噪声方差为 $r \\in \\mathbb{R}_{0}$，在时间 $t$ 上独立且与过程噪声独立。\n\n所有分布均被理解为根据上述结构条件独立。令 $y_{0:T} \\equiv (y_0, y_1, \\dots, y_T)$ 表示观测数据。\n\n您的任务是：\n\n1. 从贝叶斯定理和上述高斯假设下的联合概率密度定义出发，推导固定区间平滑目标函数 $J(x_{0:T})$，该函数为后验密度 $p(x_{0:T} \\mid y_{0:T})$ 的负对数，忽略一个不依赖于 $x_{0:T}$ 的加法常数。用模型参数以及状态序列与模型/观测之间的残差来表示 $J(x_{0:T})$。不要引用未经证明的快捷公式；从联合密度的分解开始，并通过高斯密度的性质进行推导。\n2. 对于 $f(x) = a x$ 和 $h(x) = x^2$ 的特定选择，推导关于 $x_{0:T}$ 的精确梯度 $\\nabla J(x_{0:T})$ 和精确Hessian矩阵 $\\nabla^2 J(x_{0:T})$。您的推导必须明确包含 $a$、$q$、$r$、$m_0$、$P_0$ 和 $y_{0:T}$。\n3. 一个局部极小值点 $x_{0:T}^\\star$ 对应于最大后验 (MAP) 估计的一个候选项。拉普拉斯近似通过对 $J(x_{0:T})$ 进行二阶泰勒展开，在局部极小值点附近构建后验的高斯近似。使用您的Hessian矩阵，写出高斯近似以及对模型证据的局部贡献的相应近似，其对数由下式给出：\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{(T+1)}{2} \\log(2\\pi),\n$$\n注意最后一项来自于对一个 $(T+1)$ 维高斯密度的归一化。\n4. 通过引入标量参数 $\\lambda \\in (0, 1]$ 并定义 $r(\\lambda) = r_{\\text{base}} / \\lambda$（其中 $r_{\\text{base}} \\in \\mathbb{R}_{0}$ 为固定值），提出一个关于观测噪声方差 $r$ 的同伦。解释为什么减小 $\\lambda$ 会增加 $r(\\lambda)$（削弱观测影响），而增加 $\\lambda$ 会减小 $r(\\lambda)$（增强观测影响）。\n5. 在 $\\lambda$ 中实现一个延拓策略，以追踪一系列从弱观测状态（较小的 $\\lambda$）到目标状态 $\\lambda = 1$ 的局部极小值点。在指定离散时间表上的每个 $\\lambda$ 处，使用带回溯线搜索和Levenberg–Marquardt式阻尼的牛頓法来寻找局部极小值点，并从前一个 $\\lambda$ 的解进行初始化。通过验证Hessian矩阵是正定的，确保在最终的 $\\lambda = 1$ 处，您的候選項是一个局部最小值。\n\n为了进行数值研究和自动化测试，请使用以下测试套件。对于每种情况，您必须：\n\n- 为给定参数构建平滑目标函数。\n- 在 $\\lambda = 1$ 时使用两种多起点初始化：一种是对于所有 $t$，$x_t^{(0)} = +\\sqrt{y_t}$；另一种是对于所有 $t$，$x_t^{(0)} = -\\sqrt{y_t}$。从两种初始化开始运行牛顿法，以寻找不同的局部最小值。通过 $\\ell_2$距离阈值 $10^{-3}$ 来识别不同的最小值，并仅保留那些Hessian矩阵为正定的最小值。\n- 使用您的Hessian矩阵计算每个不同局部最小值的拉普拉斯近似对数证据。\n- 从较小的 $\\lambda$ 运行同伦追踪至 $\\lambda = 1$，从最小 $\\lambda$ 处的正初始化开始，并将最终追踪解的时间平均值的符号报告为 $+1$（如果平均值为正）或 $-1$（如果平均值为负）。\n- 在最终的同伦步骤，将Hessian矩阵的条件数计算为其最大特征值与最小特征值之比。\n\n测试套件：\n\n- 案例 #1（对称，双最小值状态）：\n  - $T = 4$，\n  - $a = 0.9$，\n  - $q = 0.05$，\n  - $r_{\\text{base}} = 0.02$，\n  - $m_0 = 0.0$，\n  - $P_0 = 1.0$，\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$，\n  - 同伦时间表 $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$，因此 $r(\\lambda) = r_{\\text{base}}/\\lambda$。\n- 案例 #2（先验打破对称性，可能为单最小值状态）：\n  - $T = 4$，\n  - $a = 0.9$，\n  - $q = 0.05$，\n  - $r_{\\text{base}} = 0.02$，\n  - $m_0 = 0.5$，\n  - $P_0 = 0.01$，\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$，\n  - 同伦时间表 $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$。\n- 案例 #3（弱动态耦合，多个近乎解耦的模态）：\n  - $T = 4$，\n  - $a = 0.9$，\n  - $q = 10.0$，\n  - $r_{\\text{base}} = 0.02$，\n  - $m_0 = 0.0$，\n  - $P_0 = 1.0$，\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$，\n  - 同伦时间表 $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$。\n\n每个案例所需的最终输出是包含四个值的列表：\n- 在 $\\lambda = 1$ 时，从两种规定的初始化中找到的不同局部最小值的整数数量。\n- 在 $\\lambda = 1$ 时，正、负吸引盆地的极小值之间的拉普拉斯近似对数证据的浮点数差异，计算为 $\\log Z_{\\text{Lap}}^{(+)} - \\log Z_{\\text{Lap}}^{(-)}$。如果只找到一个最小值，则报告 $0.0$。\n- 在 $\\lambda = 1$ 时，最终同伦追踪解的时间平均值的整数符号，报告为 $+1$ 或 $-1$。\n- 在 $\\lambda = 1$ 时，最终同伦追踪解处Hessian矩阵的浮点条件数。\n\n您的程序应生成一行输出，其中包含三个案例的结果，以逗号分隔的每个案例列表的形式，无空格，并用方括号括起来。例如：$[\\,[1,0.0,1,12.345678],[\\dots],[\\dots]\\,]$。此问题不涉及物理单位，所有数字应打印为普通小数。", "solution": "此问题要求推导并实现一个用于非线性状态空间模型的固定区间平滑算法。解决方案涉及通过最小化一个成本函数来寻找状态轨迹的最大后验 (MAP) 估计，该成本函数对应于负对数后验概率密度。具体模型涉及线性动态和二次观测，这导致了一个可能具有多个局部最小值的非凸优化问题。\n\n### 任务1：平滑目标函数 $J(x_{0:T})$ 的推导\n\n目标是找到状态轨迹 $x_{0:T} = (x_0, x_1, \\dots, x_T)$ 以最大化后验概率密度 $p(x_{0:T} \\mid y_{0:T})$。根据贝叶斯定理，后验与似然和先验的乘积成正比：\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto p(y_{0:T} \\mid x_{0:T}) p(x_{0:T})\n$$\n最大化后验等价于最大化其对数，或最小化其负对数。我们将目标函数 $J(x_{0:T})$ 定义为负对数后验，不考虑一个不依赖于状态序列 $x_{0:T}$ 的加法常数。\n$$\nJ(x_{0:T}) = -\\log p(x_{0:T} \\mid y_{0:T}) + C = -\\log p(y_{0:T} \\mid x_{0:T}) - \\log p(x_{0:T}) + C'\n$$\n状态空间模型的结构意味着条件独立性，这使我们能够分解似然项和先验项。\n状态轨迹的先验 $p(x_{0:T})$ 由马尔可夫性质给出：\n$$\np(x_{0:T}) = p(x_0) \\prod_{t=0}^{T-1} p(x_{t+1} \\mid x_t)\n$$\n给定状态 $x_{0:T}$，观测 $y_{0:T}$ 是条件独立的：\n$$\np(y_{0:T} \\mid x_{0:T}) = \\prod_{t=0}^{T} p(y_t \\mid x_t)\n$$\n将这些代入 $J(x_{0:T})$ 的表达式中得到：\n$$\nJ(x_{0:T}) = -\\log p(x_0) - \\sum_{t=0}^{T-1} \\log p(x_{t+1} \\mid x_t) - \\sum_{t=0}^{T} \\log p(y_t \\mid x_t) + C'\n$$\n我们现在代入给定的高斯概率密度：\n-   先验：$p(x_0) = \\mathcal{N}(x_0; m_0, P_0) \\propto \\exp\\left(-\\frac{1}{2P_0}(x_0 - m_0)^2\\right)$\n-   动态：$p(x_{t+1} \\mid x_t) = \\mathcal{N}(x_{t+1}; f(x_t), q) = \\mathcal{N}(x_{t+1}; ax_t, q) \\propto \\exp\\left(-\\frac{1}{2q}(x_{t+1} - ax_t)^2\\right)$\n-   观测：$p(y_t \\mid x_t) = \\mathcal{N}(y_t; h(x_t), r) = \\mathcal{N}(y_t; x_t^2, r) \\propto \\exp\\left(-\\frac{1}{2r}(y_t - x_t^2)^2\\right)$\n\n对每一项取负对数并求和，同时忽略归一化常数（例如，$1/\\sqrt{2\\pi\\sigma^2}$ 因子），我们得到平滑目标函数：\n$$\nJ(x_{0:T}) = \\frac{1}{2P_0}(x_0 - m_0)^2 + \\sum_{t=0}^{T-1} \\frac{1}{2q}(x_{t+1} - ax_t)^2 + \\sum_{t=0}^{T} \\frac{1}{2r}(y_t - x_t^2)^2\n$$\n这是一个标准的最小二乘目标函数，由一个惩罚偏离先验均值的先验项、一个惩罚偏离状态转移模型的动态项和一个惩罚与测量值不匹配的观测项组成。\n\n### 任务2：梯度 $\\nabla J(x_{0:T})$ 和Hessian矩阵 $\\nabla^2 J(x_{0:T})$\n\n为了使用像牛顿法这样的基于梯度的方法来最小化 $J(x_{0:T})$，我们需要其梯度向量 $\\nabla J(x_{0:T})$ 和Hessian矩阵 $\\nabla^2 J(x_{0:T})$。状态向量是 $x = (x_0, \\dots, x_T)^T$。\n\n梯度是偏导数向量 $\\frac{\\partial J}{\\partial x_k}$，其中 $k=0, \\dots, T$。\n-   对于 $k=0$：\n    $$\n    \\frac{\\partial J}{\\partial x_0} = \\frac{1}{P_0}(x_0 - m_0) + \\frac{1}{q}(x_{1} - ax_0)(-a) + \\frac{1}{r}(y_0 - x_0^2)(-2x_0) = \\frac{x_0 - m_0}{P_0} - \\frac{a}{q}(x_1 - ax_0) - \\frac{2x_0}{r}(y_0 - x_0^2)\n    $$\n-   对于 $k \\in \\{1, \\dots, T-1\\}$：\n    $$\n    \\frac{\\partial J}{\\partial x_k} = \\frac{1}{q}(x_k - ax_{k-1}) + \\frac{1}{q}(x_{k+1} - ax_k)(-a) + \\frac{1}{r}(y_k - x_k^2)(-2x_k) = \\frac{1}{q}(x_k - ax_{k-1}) - \\frac{a}{q}(x_{k+1} - ax_k) - \\frac{2x_k}{r}(y_k - x_k^2)\n    $$\n-   对于 $k=T$：\n    $$\n    \\frac{\\partial J}{\\partial x_T} = \\frac{1}{q}(x_T - ax_{T-1}) + \\frac{1}{r}(y_T - x_T^2)(-2x_T) = \\frac{1}{q}(x_T - ax_{T-1}) - \\frac{2x_T}{r}(y_T - x_T^2)\n    $$\n\nHessian矩阵是二阶偏导数的对称矩阵 $H_{ij} = \\frac{\\partial^2 J}{\\partial x_i \\partial x_j}$。由于动态的马尔可夫结构，该矩阵是三对角的。\n对角元素 $H_{k,k}$ 是：\n-   对于 $k=0$：\n    $$\n    \\frac{\\partial^2 J}{\\partial x_0^2} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{\\partial}{\\partial x_0}\\left(-\\frac{2x_0y_0}{r} + \\frac{2x_0^3}{r}\\right) = \\frac{1}{P_0} + \\frac{a^2}{q} - \\frac{2y_0}{r} + \\frac{6x_0^2}{r} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{2}{r}(3x_0^2 - y_0)\n    $$\n-   对于 $k \\in \\{1, \\dots, T-1\\}$：\n    $$\n    \\frac{\\partial^2 J}{\\partial x_k^2} = \\frac{1}{q} + \\frac{a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k) = \\frac{1+a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k)\n    $$\n-   对于 $k=T$：\n    $$\n    \\frac{\\partial^2 J}{\\partial x_T^2} = \\frac{1}{q} + \\frac{2}{r}(3x_T^2 - y_T)\n    $$\n非对角元素 $H_{k, k+1} = H_{k+1, k}$ 对于 $k=0, \\dots, T-1$ 是：\n$$\n\\frac{\\partial^2 J}{\\partial x_k \\partial x_{k+1}} = \\frac{\\partial}{\\partial x_{k+1}}\\left( \\dots - \\frac{a}{q}(x_{k+1} - ax_k) \\dots \\right) = -\\frac{a}{q}\n$$\n所有其他非对角元素 $H_{i,j}$ 其中 $|i-j|>1$ 均为零。\n\n### 任务3：拉普拉斯近似\n\n拉普拉斯近似在MAP估计 $x_{0:T}^\\star$（即 $J(x_{0:T})$ 的一个局部最小值）附近提供了对后验分布的高斯近似。目标函数在 $x_{0:T}^\\star$ 周围展开到二阶：\n$$\nJ(x_{0:T}) \\approx J(x_{0:T}^\\star) + \\nabla J(x_{0:T}^\\star)^T (x_{0:T} - x_{0:T}^\\star) + \\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T \\nabla^2 J(x_{0:T}^\\star) (x_{0:T} - x_{0:T}^\\star)\n$$\n由于 $x_{0:T}^\\star$ 是一个最小值，$\\nabla J(x_{0:T}^\\star) = 0$。后验则近似为：\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto \\exp(-J(x_{0:T})) \\approx \\exp(-J(x_{0:T}^\\star)) \\exp\\left(-\\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T H^\\star (x_{0:T} - x_{0:T}^\\star)\\right)\n$$\n其中 $H^\\star = \\nabla^2 J(x_{0:T}^\\star)$。这表明后验近似为高斯分布：$\\mathcal{N}(x_{0:T}^\\star, (H^\\star)^{-1})$。模型证据（或边际似然）$p(y_{0:T}) = \\int p(y_{0:T}, x_{0:T}) dx_{0:T}$ 可以通过对未归一化的后验密度积分来近似。如题目所给，对数证据的局部贡献是：\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{T+1}{2} \\log(2\\pi)\n$$\n该公式结合了后验峰值的高度（通过 $-J(x_{0:T}^\\star)$）和峰值的体积（通过 $-\\frac{1}{2}\\log\\det H^\\star$，衡量其“宽度”）。在比较不同局部最小值的证据时，不依赖于 $x_{0:T}^\\star$ 的项会抵消，因此在 $J$ 中包含哪些常数对于相对比较而言并不重要。\n\n### 任务4：观测噪声方差的同伦\n\n提出了一种同伦（或延拓方法）来解决该优化问题。观测噪声方差 $r$ 由 $\\lambda \\in (0, 1]$ 参数化为 $r(\\lambda) = r_{\\text{base}} / \\lambda$。\n-   当 $\\lambda$ 很小（接近0）时，$r(\\lambda)$ 变得非常大。大的观测方差 $r$ 意味着观测 $y_t$ 信息量不足。目标函数中的项 $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ 变得很小，因此后验由先验和动态主导，这定义了一个凸问题。这个“弱观测”问题更容易解决，并且通常在先验均值附近有一个单一的最小值。\n-   随着 $\\lambda$ 增加到1，$r(\\lambda)$ 减小，接近目标值 $r_{\\text{base}}$。非凸观测项 $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ 的影响增加，目标函数从简单的、类凸的形状变形为复杂的、可能多模态的目标形状。\n\n策略是从小 $\\lambda$ 处的简单问题的解开始，并将其用作稍大 $\\lambda$ 的初始猜测。通过将 $\\lambda$ 从一个小值逐步增加到1，我们可以通过变形来追踪一个局部最小值，这通常比直接在 $\\lambda=1$ 处开始优化更鲁棒。\n\n### 任务5：数值实现\n\n实现的核心是使用阻尼牛顿法来寻找 $J(x_{0:T})$ 的局部最小值。\n-   **牛顿步**：在每次迭代中，我们求解线性系统 $H_k p_k = -g_k$ 以获得牛顿步 $p_k$，其中 $g_k$ 和 $H_k$ 是当前估计 $x_k$ 处的梯度和Hessian矩阵。\n-   **阻尼**：在远离最小值的地方，Hessian矩阵可能不是正定的。使用了Levenberg-Marquardt风格的修改：我们求解 $(H_k + \\mu I) p_k = -g_k$，其中 $\\mu \\ge 0$ 是一个阻尼参数。如果 $H_k$ 是正定的，则选择 $\\mu=0$；否则增加 $\\mu$ 直到 $(H_k + \\mu I)$ 是正定的，以确保一个下降方向。\n-   **线搜索**：使用回溯线搜索来确定步长 $\\alpha$。我们从 $\\alpha=1$ 开始，并减小它直到满足Armijo条件 $J(x_k + \\alpha p_k) \\le J(x_k) + c \\alpha g_k^T p_k$（对于一个小的常数 $c$）。\n-   **多起点和同伦**：这个牛顿求解器用于两个目的：\n    1.  在 $\\lambda=1$ 时，它从两个不同的初始猜测（$x_t^{(0)} = \\pm\\sqrt{y_t}$）运行，以寻找可能不同的局部最小值。\n    2.  它在同伦时间表的每一步中使用，来自前一个 $\\lambda$ 值的解为当前 $\\lambda$ 提供初始猜测。\n\n对每个识别出的最小值的最后一步包括检查Hessian矩阵是否正定（所有特征值均为正），然后计算所需的量：拉普拉斯对数证据、平均状态的符号和Hessian条件数。", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the smoothing problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case #1 (symmetric, two-minima regime)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #2 (prior breaks symmetry, single-minimum regime likely)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.5, \"P0\": 0.01,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #3 (weak dynamics coupling, multiple nearly-decoupled modes)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 10.0, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p = case\n        dim = p[\"T\"] + 1\n\n        def J_objective(x, r, p):\n            term_prior = 0.5 * (x[0] - p[\"m0\"])**2 / p[\"P0\"]\n            term_dyn = 0.5 * np.sum((x[1:] - p[\"a\"] * x[:-1])**2) / p[\"q\"]\n            term_obs = 0.5 * np.sum((p[\"y\"] - x**2)**2) / r\n            return term_prior + term_dyn + term_obs\n\n        def grad_J(x, r, p):\n            grad = np.zeros_like(x)\n            # Observation part\n            grad += -2 * x * (p[\"y\"] - x**2) / r\n            \n            # Prior part\n            grad[0] += (x[0] - p[\"m0\"]) / p[\"P0\"]\n            \n            # Dynamics part\n            dyn_res = x[1:] - p[\"a\"] * x[:-1]\n            grad[1:] += dyn_res / p[\"q\"]\n            grad[:-1] -= p[\"a\"] * dyn_res / p[\"q\"]\n            \n            return grad\n\n        def hess_J(x, r, p):\n            hess = np.zeros((dim, dim))\n            \n            # Diagonal\n            diag = np.zeros(dim)\n            diag += 2 * (3 * x**2 - p[\"y\"]) / r\n            diag[0] += 1 / p[\"P0\"] + p[\"a\"]**2 / p[\"q\"]\n            diag[1:p[\"T\"]] += (1 + p[\"a\"]**2) / p[\"q\"]\n            diag[p[\"T\"]] += 1 / p[\"q\"]\n            np.fill_diagonal(hess, diag)\n            \n            # Off-diagonal\n            off_diag_val = -p[\"a\"] / p[\"q\"]\n            np.fill_diagonal(hess[1:], off_diag_val)\n            np.fill_diagonal(hess[:, 1:], off_diag_val)\n            \n            return hess\n\n        def newton_solver(x_init, lam, p, max_iter=100, tol=1e-8):\n            x = x_init.copy()\n            r = p[\"r_base\"] / lam\n            \n            for _ in range(max_iter):\n                g = grad_J(x, r, p)\n                if np.linalg.norm(g)  tol:\n                    break\n                \n                H = hess_J(x, r, p)\n                \n                # Levenberg-Marquardt Damping\n                mu = 0\n                while True:\n                    try:\n                        H_lm = H + mu * np.eye(dim)\n                        L = scipy.linalg.cholesky(H_lm, lower=True)\n                        break\n                    except np.linalg.LinAlgError:\n                        if mu == 0:\n                            mu = 1e-6\n                        else:\n                            mu *= 10\n                \n                # Solve linear system H_lm * step = -g\n                step = scipy.linalg.cho_solve((L, True), -g)\n\n                if np.linalg.norm(step)  tol:\n                    break\n\n                # Backtracking Line Search\n                alpha = 1.0\n                c = 1e-4\n                J_current = J_objective(x, r, p)\n                while alpha > 1e-8:\n                    x_new = x + alpha * step\n                    if J_objective(x_new, r, p) = J_current + c * alpha * np.dot(g, step):\n                        break\n                    alpha *= 0.5\n                \n                x = x + alpha * step\n\n            return x\n\n        # 1. Multi-start Newton's at lambda=1\n        x_init_pos = np.sqrt(np.maximum(p[\"y\"], 0))\n        x_init_neg = -np.sqrt(np.maximum(p[\"y\"], 0))\n        \n        x_sol_pos = newton_solver(x_init_pos, 1.0, p)\n        x_sol_neg = newton_solver(x_init_neg, 1.0, p)\n\n        minima = []\n        r_final = p[\"r_base\"]\n        \n        # Check first minimum\n        H_pos = hess_J(x_sol_pos, r_final, p)\n        try:\n            eigvals_pos = np.linalg.eigvalsh(H_pos)\n            if np.all(eigvals_pos > 0):\n                minima.append({'x': x_sol_pos, 'H': H_pos, 'origin': 'pos', 'eigvals':eigvals_pos})\n        except np.linalg.LinAlgError:\n            pass\n\n        # Check second minimum\n        is_new = True\n        for m in minima:\n            if np.linalg.norm(m['x'] - x_sol_neg)  1e-3:\n                is_new = False\n                break\n        \n        if is_new:\n            H_neg = hess_J(x_sol_neg, r_final, p)\n            try:\n                eigvals_neg = np.linalg.eigvalsh(H_neg)\n                if np.all(eigvals_neg > 0):\n                    minima.append({'x': x_sol_neg, 'H': H_neg, 'origin': 'neg', 'eigvals':eigvals_neg})\n            except np.linalg.LinAlgError:\n                pass\n        \n        num_minima = len(minima)\n\n        # 2. Laplace log-evidence difference\n        log_evidence_diff = 0.0\n        if num_minima == 2:\n            m_pos = next(m for m in minima if m['origin'] == 'pos')\n            m_neg = next(m for m in minima if m['origin'] == 'neg')\n\n            J_pos = J_objective(m_pos['x'], r_final, p)\n            logdet_pos = np.sum(np.log(m_pos['eigvals']))\n            \n            J_neg = J_objective(m_neg['x'], r_final, p)\n            logdet_neg = np.sum(np.log(m_neg['eigvals']))\n            \n            log_z_pos = -J_pos - 0.5 * logdet_pos\n            log_z_neg = -J_neg - 0.5 * logdet_neg\n            \n            log_evidence_diff = log_z_pos - log_z_neg\n\n        # 3. Homotopy tracking\n        x_homotopy = np.sqrt(np.maximum(p[\"y\"], 0))\n        for lam in p[\"lambda_schedule\"]:\n            x_homotopy = newton_solver(x_homotopy, lam, p)\n        \n        sign_avg_homotopy = int(np.sign(np.mean(x_homotopy)))\n\n        # 4. Condition number\n        H_homotopy = hess_J(x_homotopy, r_final, p)\n        cond_num = np.inf\n        try:\n            eigvals_homotopy = np.linalg.eigvalsh(H_homotopy)\n            if np.all(eigvals_homotopy > 0):\n                cond_num = eigvals_homotopy[-1] / eigvals_homotopy[0]\n        except np.linalg.LinAlgError:\n            pass\n        \n        all_results.append(f\"[{num_minima},{log_evidence_diff:.8f},{sign_avg_homotopy},{cond_num:.8f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3406013"}, {"introduction": "对于机器人学或天气预报等领域中的大规模状态估计问题，计算效率至关重要。本练习引入了因子图这一强大的工具来表示平滑问题的目标函数，它清晰地揭示了问题固有的稀疏结构。您将亲身体会到，在求解高斯-牛顿法中的线性系统时，变量的消元顺序如何显著影响计算成本，这为平滑算法的实际应用提供了宝贵的经验。[@problem_id:3406056]", "problem": "考虑一个离散时间状态序列 $\\{x_k\\}_{k=0}^{N-1}$ 的一维固定区间平滑问题，该序列具有一阶马尔可夫先验和非线性观测。过程模型为 $f(x_k, u_k) = x_k + u_k$，带有方差为 $q$ 的加性高斯噪声；观测模型为 $h(x_k) = \\sin(x_k)$，带有方差为 $r$ 的加性高斯噪声。设未知量为状态向量 $x \\in \\mathbb{R}^N$。设控制输入 $u_k$ 和观测值 $z_k$ 为给定的常数。状态估计和平滑的目标是通过最小化由过程项和观测项构成的负对数后验（等价于加权平方和目标）来计算最大后验（MAP）估计。\n\n从基本定义出发，将平滑目标构建为一个成对因子图，其中每个过程因子连接 $(x_k, x_{k+1})$，每个观测因子连接 $x_k$ 到数据点 $z_k$。使用高斯-牛顿（GN）方法，在当前估计值 $\\hat{x}$ 处对残差进行线性化，以获得带有正规方程的二次近似。正规方程的系数矩阵等于高斯-牛顿海森近似，右侧项等于雅可比矩阵转置乘以残差向量，两者都由逆噪声方差加权。稀疏 Cholesky 分解的复杂度取决于变量消元顺序。对于下面定义的每个测试用例，您的任务是执行以下操作：\n\n1. 在给定的当前估计值 $\\hat{x}$ 处构建高斯-牛顿线性化。执行单步高斯-牛顿法，求解线性化正规方程得到 $\\Delta x$，从而获得更新后的估计值 $x^{+} = \\hat{x} + \\Delta x$。\n2. 通过仅考虑由成对过程因子所隐含的非对角结构（观测因子仅对对角线元素有贡献，因此不产生非对角边），构建在 $\\hat{x}$ 处的高斯-牛顿海森近似的稀疏图。对于两种特定的消元顺序，通过在稀疏图上模拟稀疏消元过程中的团（clique）填充，计算 Cholesky 因子 $L$ 中非零元素的确切数量（包括对角线元素）：\n   - 时间顺序：按自然时间顺序 $[0,1,2,\\dots,N-1]$ 消去变量。\n   - 中心优先顺序：令 $m = \\lfloor N/2 \\rfloor$，按顺序 $[m, m-1, m+1, m-2, m+2, \\dots]$ 消元，跳过出现的超出范围的索引。\n   计算 $L$ 中的非零元素数量，其值为按消元顺序对每个变量求和，每项为 1（代表对角线）加上该消元步骤中其剩余的当前邻居数量。邻居团必须作为填充模拟的一部分形成。\n3. 在更新后的估计值 $x^{+}$ 处评估真实的非线性目标函数，其定义为\n$$\nJ(x) = \\frac{1}{2} \\sum_{k=0}^{N-2} \\frac{\\left(x_{k+1} - x_k - u_k\\right)^2}{q} \\;+\\; \\frac{1}{2} \\sum_{k=0}^{N-1} \\frac{\\left(z_k - \\sin(x_k)\\right)^2}{r}.\n$$\n报告此值，四舍五入到 $6$ 位小数。\n\n对于每个测试用例，按顺序输出三个值：时间顺序的 Cholesky 非零元素整数计数，中心优先顺序的整数计数，以及 $J(x^{+})$ 的值（四舍五入到 $6$ 位小数的浮点数）。\n\n最后，将所有测试用例的结果汇总到单行输出中，形式为方括号内以逗号分隔的列表，并按测试用例的顺序排列。即，如果有 $T$ 个测试用例，输出列表的长度必须为 $3T$，包含 $[\\text{nnz\\_time}^{(1)}, \\text{nnz\\_center}^{(1)}, J^{(1)}, \\dots, \\text{nnz\\_time}^{(T)}, \\text{nnz\\_center}^{(T)}, J^{(T)}]$。\n\n使用以下测试套件。对于所有情况，使用长度适当的零向量作为高斯-牛顿法的当前估计值 $\\hat{x}$。\n\n测试用例 1：\n- $N = 5$， $q = 0.05$， $r = 0.02$，\n- 控制 $u \\in \\mathbb{R}^{4}$：\n$$\nu = [0.1,\\; 0.0,\\; -0.2,\\; 0.1]\n$$\n- 观测 $z \\in \\mathbb{R}^{5}$：\n$$\nz = [0.3005202067,\\; 0.3854183423,\\; 0.3914183423,\\; 0.1956693308,\\; 0.2955202067]\n$$\n\n测试用例 2：\n- $N = 10$， $q = 0.01$， $r = 0.05$，\n- 控制 $u \\in \\mathbb{R}^{9}$：\n$$\nu = [0.05,\\; 0.03,\\; 0.0,\\; 0.02,\\; -0.01,\\; 0.0,\\; 0.04,\\; -0.02,\\; 0.01]\n$$\n- 观测 $z \\in \\mathbb{R}^{10}$：\n$$\nz = [-0.1966693308,\\; -0.1504381325,\\; -0.1197122073,\\; -0.1167122073,\\; -0.1018334166,\\; -0.1057783008,\\; -0.1107783008,\\; -0.0699428473,\\; -0.0878785492,\\; -0.0829146940]\n$$\n\n测试用例 3：\n- $N = 15$， $q = 0.1$， $r = 0.02$，\n- 控制 $u \\in \\mathbb{R}^{14}$：\n$$\nu = [0.1,\\; 0.1,\\; -0.05,\\; 0.0,\\; 0.02,\\; -0.03,\\; 0.05,\\; 0.0,\\; -0.02,\\; 0.01,\\; 0.0,\\; -0.04,\\; 0.03,\\; 0.01]\n$$\n- 观测 $z \\in \\mathbb{R}^{15}$：\n$$\nz = [0.0000000000,\\; 0.1018334166,\\; 0.1976693308,\\; 0.1524381325,\\; 0.1494381325,\\; 0.1671823499,\\; 0.1405431146,\\; 0.1858588900,\\; 0.1908588900,\\; 0.1691823499,\\; 0.1780295734,\\; 0.1830295734,\\; 0.1375431146,\\; 0.1701823499,\\; 0.1790295734]\n$$\n\n角度单位为弧度。输出中不包含物理单位。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\dots]$，其中的条目按上述规定排序。每个测试用例报告的三个值必须分别为整数、整数和浮点数（四舍五入到 $6$ 位小数）类型。", "solution": "用户希望使用高斯-牛顿法解决一个固定区间平滑问题。\n\n### 步骤 1：提取给定信息\n- **状态序列**：$\\{x_k\\}_{k=0}^{N-1}$，状态向量为 $x \\in \\mathbb{R}^N$。\n- **过程模型**：$f(x_k, u_k) = x_k + u_k$，带有方差为 $q$ 的加性高斯噪声。这定义了过程残差 $e_{p,k}(x_k, x_{k+1}) = x_{k+1} - x_k - u_k$。\n- **观测模型**：$h(x_k) = \\sin(x_k)$，带有方差为 $r$ 的加性高斯噪声。这定义了观测残差 $e_{o,k}(x_k) = z_k - \\sin(x_k)$。\n- **目标函数**：通过最小化负对数后验来找到最大后验（MAP）估计，这等价于最小化加权平方和目标函数 $J(x)$：\n$$\nJ(x) = \\frac{1}{2} \\sum_{k=0}^{N-2} \\frac{\\left(x_{k+1} - x_k - u_k\\right)^2}{q} \\;+\\; \\frac{1}{2} \\sum_{k=0}^{N-1} \\frac{\\left(z_k - \\sin(x_k)\\right)^2}{r}\n$$\n- **方法**：使用单步高斯-牛顿（GN）法。\n- **初始估计**：GN 方法的线性化点是 $\\hat{x} = 0 \\in \\mathbb{R}^N$。\n- **任务 1**：计算更新后的估计值 $x^{+} = \\hat{x} + \\Delta x$，其中 $\\Delta x$ 是线性化正规方程的解。\n- **任务 2**：对于两种变量消元顺序，计算高斯-牛顿海森矩阵稀疏图的 Cholesky 因子 $L$ 中的非零元素（nnz）数量：\n    1.  时间顺序：$[0, 1, 2, \\dots, N-1]$\n    2.  中心优先顺序：$[m, m-1, m+1, \\dots]$，其中 $m = \\lfloor N/2 \\rfloor$。\n- **任务 3**：在更新后的估计值 $x^{+}$ 处评估非线性目标函数 $J(x)$，并四舍五入到 6 位小数。\n- **测试用例**：提供了三个测试用例，包含 $N, q, r, u, z$ 的具体值。\n\n### 步骤 2：使用提取的给定信息进行验证\n该问题经过严格的验证过程。\n- **科学依据**：该问题是非线性最小二乘优化在状态估计中的一个标准应用，特别是固定区间平滑，这是机器人学、控制理论和数据同化中的一个基本课题。所用的模型、目标函数和高斯-牛顿法都是标准且成熟的。\n- **适定性**：问题定义清晰。对于高斯-牛顿步骤，任务是求解一个线性系统 $H_{GN} \\Delta x = b$。海森近似矩阵 $H_{GN}$ 是从雅可比矩阵导出的外积之和。对于此问题，保证其为对称正定矩阵，从而确保存在唯一的解 $\\Delta x$。模拟 Cholesky 填充的过程在算法上是明确的。\n- **客观性**：问题使用了数值优化和稀疏线性代数中的精确、标准术语。没有主观或含糊不清的陈述。\n- **完整性和一致性**：为每个测试用例提供了所有必要的数据（$N, q, r, u, z$）和初始条件（$\\hat{x}=0$）。向量的维度与定义一致。\n- **结论**：该问题是**有效的**，因为它在科学上是合理的、适定的、客观的和完整的。\n\n### 步骤 3：判定与行动\n问题有效。将提供完整的解决方案。\n\n### 基于原则的解决方案设计\n\n该解决方案通过为每个测试用例实现问题中指定的三个任务来进行。\n\n**任务 1：高斯-牛顿步骤**\n高斯-牛顿法在当前估计值 $\\hat{x}$ 周围用一个二次函数来近似目标函数 $J(x)$，并求解使该二次函数最小化的步长 $\\Delta x$。这导出了正规方程：\n$$ H_{GN} \\Delta x = b $$\n其中 $H_{GN}$ 是高斯-牛顿海森近似，而 $b$ 是右侧向量。\n\n海森矩阵 $H_{GN}$ 是所有过程项和观测项贡献的总和。\n- 一个方差为 $q$ 的过程项 $e_{p,k}(x) = (x_{k+1} - x_k - u_k)$ 对与变量 $(x_k, x_{k+1})$ 对应的子矩阵贡献了 $\\frac{1}{q}\\begin{psmallmatrix} 1  -1 \\\\ -1  1 \\end{psmallmatrix}$。\n- 一个观测项 $e_{o,k}(x) = z_k - \\sin(x_k)$ 是非线性的。它关于 $x_k$ 的雅可比矩阵是 $-\\cos(x_k)$。在线性化点 $\\hat{x}=0$ 处，该雅可比矩阵为 $-\\cos(0) = -1$。对海森矩阵位置 $(k,k)$ 的贡献是 $\\frac{1}{r}(-1)(-1) = \\frac{1}{r}$。\n\n将这些贡献相加，$H_{GN}$ 是一个对称三对角矩阵，其元素为：\n- $H_{k,k} = \\frac{2}{q} + \\frac{1}{r}$ for $k \\in [1, N-2]$\n- $H_{0,0} = H_{N-1, N-1} = \\frac{1}{q} + \\frac{1}{r}$\n- $H_{k, k+1} = H_{k+1, k} = -\\frac{1}{q}$ for $k \\in [0, N-2]$\n\n右侧向量为 $b = -\\nabla J(\\hat{x})$。在 $\\hat{x}=0$ 处的梯度 $\\nabla J(x)$ 的分量为：\n- $\\nabla J(\\hat{x})_k = \\frac{u_k - u_{k-1}}{q} - \\frac{z_k}{r}$ for $k \\in [1, N-2]$\n- $\\nabla J(\\hat{x})_0 = \\frac{u_0}{q} - \\frac{z_0}{r}$\n- $\\nabla J(\\hat{x})_{N-1} = -\\frac{u_{N-2}}{q} - \\frac{z_{N-1}}{r}$\n所以，$b = -\\nabla J(\\hat{x})$ 的分量为：\n- $b_k = \\frac{u_{k-1} - u_k}{q} + \\frac{z_k}{r}$ for $k \\in [1, N-2]$\n- $b_0 = -\\frac{u_0}{q} + \\frac{z_0}{r}$\n- $b_{N-1} = \\frac{u_{N-2}}{q} + \\frac{z_{N-1}}{r}$\n\n求解线性系统 $H_{GN} \\Delta x = b$ 以得到 $\\Delta x$。更新后的状态为 $x^{+} = \\hat{x} + \\Delta x = \\Delta x$。\n\n**任务 2：Cholesky 非零元素计数**\nCholesky 因子 $L$（其中 $H=LL^T$）中的非零元素数量取决于分解过程中发生的填充（fill-in），而填充又由变量消元顺序决定。我们在 $H_{GN}$ 的稀疏图上模拟此过程。$H_{GN}$ 的非对角稀疏性对应于一个路径图，其中节点 $k$ 连接到 $k-1$ 和 $k+1$。\n模拟过程如下：\n1. 初始化路径图的邻接表表示，并将总非零计数 `nnz` 设为 0。\n2. 对于指定消元顺序中的每个变量 $v$：\n    a. 识别出尚未被消元的 $v$ 的邻居集合。\n    b. 将 1（用于对角线元素）加上这些剩余邻居的数量加到 `nnz` 中。\n    c. 通过在 $v$ 的所有剩余邻居之间形成一个团（clique），向图中添加“填充”边。\n    d. 将 $v$ 标记为已消元。\n对时间顺序和中心优先顺序都执行此过程。\n\n**任务 3：评估目标函数**\n计算出更新后的状态向量 $x^{+}$ 后，通过将 $x^{+}$ 代入给定的 $J(x)$ 公式来计算真实的非线性目标函数 $J(x^{+})$ 的值。最终值四舍五入到 6 位小数。\n\n整个过程被封装在一个 Python 脚本中，该脚本遍历提供的测试用例，并将结果汇总成指定的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# scipy is listed as available, but not required for this implementation.\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the solver for each,\n    and print the aggregated results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 5, \"q\": 0.05, \"r\": 0.02,\n            \"u\": np.array([0.1, 0.0, -0.2, 0.1]),\n            \"z\": np.array([0.3005202067, 0.3854183423, 0.3914183423, 0.1956693308, 0.2955202067])\n        },\n        {\n            \"N\": 10, \"q\": 0.01, \"r\": 0.05,\n            \"u\": np.array([0.05, 0.03, 0.0, 0.02, -0.01, 0.0, 0.04, -0.02, 0.01]),\n            \"z\": np.array([-0.1966693308, -0.1504381325, -0.1197122073, -0.1167122073, -0.1018334166, -0.1057783008, -0.1107783008, -0.0699428473, -0.0878785492, -0.0829146940])\n        },\n        {\n            \"N\": 15, \"q\": 0.1, \"r\": 0.02,\n            \"u\": np.array([0.1, 0.1, -0.05, 0.0, 0.02, -0.03, 0.05, 0.0, -0.02, 0.01, 0.0, -0.04, 0.03, 0.01]),\n            \"z\": np.array([0.0000000000, 0.1018334166, 0.1976693308, 0.1524381325, 0.1494381325, 0.1671823499, 0.1405431146, 0.1858588900, 0.1908588900, 0.1691823499, 0.1780295734, 0.1830295734, 0.1375431146, 0.1701823499, 0.1790295734])\n        }\n    ]\n    \n    all_results = []\n    for params in test_cases:\n        case_results = process_one_case(**params)\n        all_results.extend(list(case_results))\n        \n    # Format results to match required output types (int, int, float)\n    formatted_results = []\n    for i in range(len(all_results)):\n        if (i % 3 == 2): # J(x+) value\n             formatted_results.append(f\"{all_results[i]:.6f}\")\n        else: # nnz counts\n             formatted_results.append(str(all_results[i]))\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef process_one_case(N, q, r, u, z):\n    \"\"\"\n    Processes a single test case: performs one Gauss-Newton step, simulates\n    Cholesky factorization for two orderings, and evaluates the final cost.\n    \n    Args:\n        N (int): Number of states.\n        q (float): Process noise variance.\n        r (float): Observation noise variance.\n        u (np.ndarray): Control inputs.\n        z (np.ndarray): Observations.\n\n    Returns:\n        tuple: (nnz_time, nnz_center, J_x_plus)\n    \"\"\"\n    # Task 1: Perform one Gauss-Newton step from x_hat = 0\n    inv_q = 1.0 / q\n    inv_r = 1.0 / r\n\n    # At x_hat = 0, Jacobians are: J_proc = [-1, 1], J_obs = -cos(0) = -1\n    # Build H_GN = J^T W J\n    H_GN = np.zeros((N, N))\n    \n    # Process factors (chain)\n    for i in range(N - 1):\n        H_GN[i, i] += inv_q\n        H_GN[i + 1, i + 1] += inv_q\n        H_GN[i, i + 1] -= inv_q\n        H_GN[i + 1, i] -= inv_q\n    # Observation factors\n    for i in range(N):\n        H_GN[i, i] += inv_r\n\n    # Build RHS b = J^T W E(x_hat)\n    # At x_hat = 0, E_proc = -u, E_obs = z - sin(0) = z\n    b = np.zeros(N)\n    \n    # Process factors\n    for k in range(N - 1):\n        # J_k = -1, J_{k+1} = 1. E_k = -u_k\n        # Contribution is inv_q * [-1, 1]^T * (-u_k)\n        b[k] += inv_q * u[k]\n        b[k+1] -= inv_q * u[k]\n    # Observation factors\n    for k in range(N):\n        # J_k = -1. E_k = z_k\n        # Contribution is inv_r * [-1]^T * z_k\n        b[k] -= inv_r * z[k]\n\n    # GN requires solving H * delta_x = -b\n    delta_x = np.linalg.solve(H_GN, -b)\n    x_plus = delta_x  # Since x_hat = 0, x_plus = 0 + delta_x\n\n    # Task 2: Compute Cholesky nonzeros (nnz) for two orderings\n    # Time ordering: [0, 1, ..., N-1]\n    time_ordering = list(range(N))\n    nnz_time = simulate_cholesky_elimination(N, time_ordering)\n    \n    # Center-first ordering: [m, m-1, m+1, ...]\n    m = N // 2\n    center_first_ordering = [m]\n    left, right = m - 1, m + 1\n    while left >= 0 or right  N:\n        if left >= 0:\n            center_first_ordering.append(left)\n            left -= 1\n        if right  N:\n            center_first_ordering.append(right)\n            right += 1\n    nnz_center = simulate_cholesky_elimination(N, center_first_ordering)\n\n    # Task 3: Evaluate the true nonlinear objective J at the updated estimate x_plus\n    process_error_sum = np.sum(((x_plus[1:] - x_plus[:-1] - u)**2) / q)\n    observation_error_sum = np.sum(((z - np.sin(x_plus))**2) / r)\n    J_x_plus = 0.5 * (process_error_sum + observation_error_sum)\n    \n    return nnz_time, nnz_center, J_x_plus\n\ndef simulate_cholesky_elimination(N, ordering):\n    \"\"\"\n    Simulates sparse Cholesky elimination on a path graph to count nonzeros\n    in the resulting factor L.\n\n    Args:\n        N (int): The number of nodes in the graph.\n        ordering (list): The variable elimination order.\n\n    Returns:\n        int: The total number of nonzeros in the Cholesky factor L.\n    \"\"\"\n    if N == 0:\n        return 0\n    \n    # Adjacency list for the initial path graph\n    adj = {i: set() for i in range(N)}\n    for i in range(N - 1):\n        adj[i].add(i + 1)\n        adj[i + 1].add(i)\n\n    nnz = 0\n    eliminated_nodes = [False] * N\n\n    for node_to_elim in ordering:\n        # Find neighbors that have not been eliminated yet\n        remaining_neighbors = {n for n in adj[node_to_elim] if not eliminated_nodes[n]}\n\n        # Add contributions from this column to nnz (1 for diagonal + off-diagonals)\n        nnz += 1 + len(remaining_neighbors)\n\n        # Add fill-in edges by forming a clique among remaining neighbors\n        remaining_neighbors_list = list(remaining_neighbors)\n        for i in range(len(remaining_neighbors_list)):\n            for j in range(i + 1, len(remaining_neighbors_list)):\n                u_node, v_node = remaining_neighbors_list[i], remaining_neighbors_list[j]\n                adj[u_node].add(v_node)\n                adj[v_node].add(u_node)\n\n        eliminated_nodes[node_to_elim] = True\n\n    return nnz\n\nsolve()\n```", "id": "3406056"}]}