## 引言
在现代科学与工程中，从[天气预报](@entry_id:270166)到机器学习的众多领域都面临着一个共同的挑战：如何高效地优化和校准包含数百万甚至数十亿变量的大规模动态系统？直接计算系统输出对这些变量的敏感性或梯度，在计算上往往是不可行的。伴随模型（Adjoint Model）为此提供了一个强大而优雅的解决方案，它是一种能够在不牺牲精度的前提下，以极高效率计算梯度的通用方法，已成为[大规模优化](@entry_id:168142)、数据同化和逆问题领域的基石。

本文旨在系统性地揭示伴随模型的推导过程、核心性质及其广泛应用。读者将通过本文的学习，从根本上理解为何伴随方法是解决高维[敏感性分析](@entry_id:147555)问题的关键。文章分为三个核心部分：

第一章，“原理与机制”，将深入探讨伴随算子的数学基础，从抽象的[希尔伯特空间](@entry_id:261193)定义到有限维和无限维的具体形式，阐明其作为梯度计算工具的深刻内涵。

第二章，“应用与[交叉](@entry_id:147634)学科联系”，将展示伴随方法如何在[地球科学](@entry_id:749876)、机器学习、[电力](@entry_id:262356)系统等不同学科中发挥关键作用，揭示其作为一种统一计算思想的普适性。

第三章，“动手实践”，将引导读者通过具体的编程练习，掌握伴随模型的推导、验证和实现中的关键技术，如检查点算法，从而将理论知识转化为实践能力。

通过这一结构化的学习路径，我们将从理论基础出发，逐步深入其应用场景，最终落脚于实践技能，全面掌握伴随模型这一强大的计算工具。

## 原理与机制

本章在前一章介绍的基础上，深入探讨伴随模型的数学原理和关键机制。我们将从其在[希尔伯特空间](@entry_id:261193)中的抽象定义出发，逐步过渡到其在有限维和无限维空间中的具体形式，并最终阐明其在求解[数据同化](@entry_id:153547)和逆问题中[大规模优化](@entry_id:168142)问题时的核心作用。本章的目标是不仅展示如何推导伴随模型，更要揭示其作为计算敏感性梯度的通用而高效工具的深刻内涵。

### 伴随算子的基本定义

在深入探讨伴随模型在动力系统中的应用之前，我们必须首先建立其坚实的数学基础。[伴随算子](@entry_id:140236)的概念源于[泛函分析](@entry_id:146220)，它在任何具备[内积](@entry_id:158127)结构的[线性空间](@entry_id:151108)中都有明确的定义。

#### [希尔伯特空间](@entry_id:261193)中的定义

从最普适的层面来讲，假设我们有一个实[希尔伯特空间](@entry_id:261193) $\mathcal{H}$，其[内积](@entry_id:158127)记作 $\langle \cdot, \cdot \rangle_{\mathcal{H}}$。对于一个定义在 $\mathcal{H}$ 的一个[稠密子集](@entry_id:264458)（其定义域）上的[有界线性算子](@entry_id:180446) $L: \mathcal{H} \to \mathcal{H}$，其**[伴随算子](@entry_id:140236)** (adjoint operator) $L^*$ 是唯一满足如下关系的算子：
$$
\langle L u, v \rangle_{\mathcal{H}} = \langle u, L^* v \rangle_{\mathcal{H}}, \quad \forall u, v \in \mathcal{H}
$$
这一定义是所有伴随方法的核心。它本质上提供了一种将算子 $L$ 在[内积](@entry_id:158127)中的作用从第一个参数“移动”到第二个参数上的方法，而这种“移动”正是通过其[伴随算子](@entry_id:140236) $L^*$ 来实现的。这种看似简单的代数技巧，在处理[微分](@entry_id:158718)和[积分算子](@entry_id:262332)时，通过[分部积分法](@entry_id:136350)展现出强大的威力，构成了计算复杂系统梯度的基石。

#### [有限维空间](@entry_id:151571)中的伴随

为了更直观地理解这一定义，让我们考虑一个有限维实[向量空间](@entry_id:151108) $\mathbb{R}^n$。最常见的[内积](@entry_id:158127)是标准的**欧几里得[内积](@entry_id:158127)**，$\langle x, y \rangle = x^\top y = \sum_{i=1}^n x_i y_i$。对于一个由矩阵 $A \in \mathbb{R}^{n \times n}$ 表示的线性算子，我们可以找到它的伴随。根据定义，我们需要满足：
$$
\langle Ax, y \rangle = \langle x, A^* y \rangle
$$
将[内积](@entry_id:158127)定义代入，我们得到：
$$
(Ax)^\top y = x^\top (A^* y)
$$
利用矩阵[转置的性质](@entry_id:148302) $(Ax)^\top = x^\top A^\top$，我们有：
$$
x^\top A^\top y = x^\top A^* y
$$
由于这个等式对所有向量 $x, y \in \mathbb{R}^n$ 都成立，我们必然得出 $A^\top = A^*$。因此，在线性代数的语境下，如果空间配备的是标准的欧几里得[内积](@entry_id:158127)，那么一个矩阵的**伴随**就是它的**[转置](@entry_id:142115)** (transpose)。

#### [加权内积](@entry_id:163877)与梯度

在数据同化和逆问题中，我们通常处理的不是标准的欧几里得[内积](@entry_id:158127)。例如，当我们评估状态向量与观测或背景先验的偏差时，我们往往需要根据不确定性（即[误差协方差](@entry_id:194780)）来加权。这自然地引出了**[加权内积](@entry_id:163877)** (weighted inner product) 的概念。给定一个对称正定 (Symmetric Positive Definite, SPD) 矩阵 $W \in \mathbb{R}^{n \times n}$，我们可以定义一个新的[内积](@entry_id:158127)：
$$
\langle x, y \rangle_W = x^\top W y
$$
在[数据同化](@entry_id:153547)中，$W$ 通常是背景或[观测误差协方差](@entry_id:752872)矩阵的逆，即 $W = B^{-1}$ 或 $W = R^{-1}$。这个[加权内积](@entry_id:163877)定义了一个新的几何结构，其中向量的“长度”和“角度”都发生了改变。

在这种新的几何结构下，算子 $A$ 的伴随也必须随之改变。我们再次从定义出发，寻求满足 $\langle Ax, y \rangle_W = \langle x, A^* y \rangle_W$ 的算子 $A^*$。展开这个表达式：
$$
(Ax)^\top W y = x^\top W (A^* y)
$$
$$
x^\top A^\top W y = x^\top W A^* y
$$
由于此式对所有 $x, y$ 均成立，我们得到矩阵关系 $A^\top W = W A^*$。因为 $W$ 是对称正定的，所以它可逆。我们可以从左侧乘以 $W^{-1}$ 来求解 $A^*$：
$$
A^* = W^{-1} A^\top W
$$
这个重要的结果 [@problem_id:3363603] [@problem_id:3363677] 表明，当[内积](@entry_id:158127)由 $W$ 定义时，伴随算子不再是简单的[转置](@entry_id:142115)，而是通过一个相似变换与[转置](@entry_id:142115) $A^\top$ 相关联。只有当 $W$ 是[单位矩阵](@entry_id:156724) $I$ 的倍数时（即 $\langle \cdot, \cdot \rangle_W$ 与欧几里得[内积](@entry_id:158127)成比例），我们才有 $A^* = A^\top$。

例如，考虑一个由矩阵 $A = \begin{pmatrix} 0  1 \\ 2  0 \end{pmatrix}$ 表示的算子，以及一个由 $W = \begin{pmatrix} 2  1 \\ 1  2 \end{pmatrix}$ 定义的[内积](@entry_id:158127)。其代数转置为 $A^\top = \begin{pmatrix} 0  2 \\ 1  0 \end{pmatrix}$。然而，根据上述公式，其[伴随算子](@entry_id:140236)为 $A^* = W^{-1} A^\top W = \frac{1}{3} \begin{pmatrix} 2  7 \\ 2  -2 \end{pmatrix}$。显然，$A^* \neq A^\top$ [@problem_id:3363603]。

这个区别至关重要，因为它直接影响梯度的计算。一个泛函 $J(x)$ 的**梯度** $\nabla J(x)$ 本身就依赖于[内积](@entry_id:158127)的定义。根据 Riesz [表示定理](@entry_id:637872)，梯度是唯一满足以下条件的向量：
$$
DJ(x)[\delta x] = \langle \nabla J(x), \delta x \rangle_W
$$
其中 $DJ(x)[\delta x]$ 是 $J$ 在 $x$ 点沿方向 $\delta x$ 的 Gâteaux 导数。如果使用标准的欧几里得[内积](@entry_id:158127)，我们会得到一个梯度；如果使用由 $W$ 定义的[加权内积](@entry_id:163877)，我们会得到另一个不同的[梯度向量](@entry_id:141180) [@problem_id:3363677]。因此，伴随和梯度都是与特定[内积空间](@entry_id:271570)紧密相关的概念。

### [微分算子](@entry_id:140145)的伴随

当我们将伴随的概念从[有限维向量空间](@entry_id:265491)推广到[函数空间](@entry_id:143478)（如 $L^2(\Omega)$）时，[微分算子](@entry_id:140145)的伴随可以通过分部积分来确定。这在处理由[偏微分方程](@entry_id:141332) (PDE) 约束的[优化问题](@entry_id:266749)时是核心步骤。

考虑一个定义在有界域 $\Omega \subset \mathbb{R}^d$ 上的泛函空间 $L^2(\Omega)$，其标准[内积](@entry_id:158127)为 $\langle f, g \rangle = \int_\Omega f(x)g(x) dx$。我们来考察一个一般的二阶[线性微分算子](@entry_id:174781) $L$：
$$
Lu = -\nabla \cdot (A \nabla u) + \mathbf{b} \cdot \nabla u + c u
$$
其中 $A(x)$ 是一个[扩散](@entry_id:141445)[系数矩阵](@entry_id:151473)，$\mathbf{b}(x)$ 是平流[速度场](@entry_id:271461)，$c(x)$ 是反应系数。为了确定其[伴随算子](@entry_id:140236) $L^*$，我们计算 $\langle Lu, v \rangle$ 并通过[分部积分](@entry_id:136350)将所有[微分](@entry_id:158718)操作从 $u$ 转移到 $v$ 上。

对[扩散](@entry_id:141445)项使用[格林第一恒等式](@entry_id:170345)：
$$
\int_\Omega (-\nabla \cdot (A \nabla u)) v \,dx = \int_\Omega (A \nabla u) \cdot (\nabla v) \,dx - \int_{\partial\Omega} v (A \nabla u) \cdot \mathbf{n} \,dS
$$
$$
= \int_\Omega u (-\nabla \cdot (A^\top \nabla v)) \,dx + \int_{\partial\Omega} u (A^\top \nabla v) \cdot \mathbf{n} \,dS - \int_{\partial\Omega} v (A \nabla u) \cdot \mathbf{n} \,dS
$$
对[平流](@entry_id:270026)项使用散度定理的推广：
$$
\int_\Omega (\mathbf{b} \cdot \nabla u) v \,dx = \int_\Omega \nabla \cdot (u v \mathbf{b}) \,dx - \int_\Omega u \nabla \cdot (v \mathbf{b}) \,dx = \int_{\partial\Omega} u v (\mathbf{b} \cdot \mathbf{n}) \,dS - \int_\Omega u \nabla \cdot (v \mathbf{b}) \,dx
$$
反应项是自伴的：$\int_\Omega (cu)v \,dx = \int_\Omega u(cv) \,dx$。

将所有项合并，我们得到一个形如 $\langle Lu, v \rangle = \langle u, L^*v \rangle + \text{边界项}$ 的表达式。为了使伴随关系成立，所有边界项之和必须为零。这正是**边界条件**发挥关键作用的地方。算子 $L$ 的定义域 $D(L)$ 包含了施加于 $u$ 上的边界条件。这些边界条件，会反过来决定施加于 $v$ 上的边界条件（即 $L^*$ 的定义域 $D(L^*)$），以确保边界项消失。

例如，如果 $L$ 的定义域包含齐次狄利克雷边界条件（$u|_{\partial\Omega}=0$），那么所有包含 $u$ 的边界积分项都会自动消失。为了消除剩余的边界项（如 $\int_{\partial\Omega} v (A \nabla u) \cdot \mathbf{n} \,dS$），我们必须要求 $v$ 也满足齐次[狄利克雷边界条件](@entry_id:173524)（$v|_{\partial\Omega}=0$）。在这种情况下，我们得到的[伴随算子](@entry_id:140236)为 [@problem_id:3363615]：
$$
L^* v = -\nabla \cdot (A^\top \nabla v) - \nabla \cdot (\mathbf{b} v) + c v
$$
其定义域 $D(L^*)$ 也包含齐次[狄利克雷边界条件](@entry_id:173524)。这个例子清晰地表明，算子的伴随不仅涉及到其形式表达式的变化（例如，$A \to A^\top$，以及[平流](@entry_id:270026)项的形式改变），还深刻地涉及到其定义域和相关边界条件的传递。

### 基于伴随方法的[敏感性分析](@entry_id:147555)

[伴随算子](@entry_id:140236)的真正威力在于它提供了一种极其高效的计算方法，用以求解一个标量输出（成本函数）相对于大量输入参数（如[初始条件](@entry_id:152863)或模型参数）的梯度。这种方法被称为**伴随方法** (adjoint method)。

考虑一个由动力系统 $x = \mathcal{M}(x_0)$ 约束的[优化问题](@entry_id:266749)，其中 $\mathcal{M}$ 是一个（通常是[非线性](@entry_id:637147)的）动力学模型（如 ODE 或 PDE 的解算子），$x_0$ 是[初始条件](@entry_id:152863)，我们希望最小化一个成本函数 $J(x_0) = \mathcal{J}(x(t))$。

根据链式法则，成本函数对于[初始条件](@entry_id:152863)的微小扰动 $\delta x_0$ 的一阶变化为：
$$
\delta J = \langle \nabla_{x} \mathcal{J}, \delta x \rangle = \langle \nabla_{x} \mathcal{J}, \mathcal{M}'(x_0) \delta x_0 \rangle
$$
其中 $\mathcal{M}'(x_0)$ 是模型算子 $\mathcal{M}$ 在 $x_0$ 处的 Fréchet 导数，也称为**[切线性模型](@entry_id:755808)** (Tangent Linear Model, TLM)。它描述了扰动如何在线性近似下随时间演化。根据梯度的定义，我们有 $\delta J = \langle \nabla_{x_0} J, \delta x_0 \rangle$。结合以上两式，我们得到：
$$
\langle \nabla_{x_0} J, \delta x_0 \rangle = \langle \nabla_{x} \mathcal{J}, \mathcal{M}'(x_0) \delta x_0 \rangle
$$
利用[伴随算子](@entry_id:140236)的定义，我们可以将 $\mathcal{M}'(x_0)$ 移到[内积](@entry_id:158127)的另一边：
$$
\langle \nabla_{x_0} J, \delta x_0 \rangle = \langle (\mathcal{M}'(x_0))^* \nabla_{x} \mathcal{J}, \delta x_0 \rangle
$$
由此我们得到了梯度的表达式：
$$
\nabla_{x_0} J = (\mathcal{M}'(x_0))^* \nabla_{x} \mathcal{J}
$$
这个公式是伴随方法的核心思想：**成本函数关于[初始条件](@entry_id:152863)的梯度，可以通过将[成本函数](@entry_id:138681)关于状态的梯度（$\nabla_{x} \mathcal{J}$）通过伴随的[切线性模型](@entry_id:755808) $(\mathcal{M}'(x_0))^*$ 从后向前传播得到**。

这个过程的计算效率极高。无论[初始条件](@entry_id:152863) $x_0$ 的维度多大，我们只需要：
1.  **一次正向积分**：从 $x_0$ 出发，求解动力学模型，得到整个时空轨迹 $x(t)$。
2.  **一次反向积分**：从[成本函数](@entry_id:138681)的梯度出发，求解**伴随模型**（即 $(\mathcal{M}'(x_0))^*$ 的具体实现），得到梯度 $\nabla_{x_0} J$。

相比之下，使用有限差分或多次运行[切线性模型](@entry_id:755808)来计算梯度，其计算成本与 $x_0$ 的维度成正比，对于高维系统（如天气预报模型）是不可行的。

#### [连续时间系统](@entry_id:276553)

对于由[非线性](@entry_id:637147) ODE $\dot{x} = f(x, t)$ 描述的[连续时间系统](@entry_id:276553)，我们可以通过引入时变的拉格朗日乘子（即伴随变量）$p(t)$ 来推导伴随方程。考虑一个典型的 4D-Var [成本函数](@entry_id:138681)：
$$
J(x_{0}) = \frac{1}{2} (x_{0} - x_{b})^{\top} B_{0}^{-1} (x_{0} - x_{b}) + \frac{1}{2} \int_{0}^{T} (H x(t) - y(t))^{\top} R^{-1} (H x(t) - y(t)) \, dt
$$
通过变分法或[拉格朗日乘子法](@entry_id:176596)，可以推导出伴随变量 $p(t)$ 满足的**伴随方程** [@problem_id:33649] [@problem_id:3363685]：
$$
-\dot{p}(t) = \left(\frac{\partial f(x(t), t)}{\partial x}\right)^{\top} p(t) - H^{\top} R^{-1} (H x(t) - y(t))
$$
这个方程是一个线性的 ODE，但其[系数矩阵](@entry_id:151473) $(\partial f / \partial x)^\top$ 依赖于正向模型的轨迹 $x(t)$。由于负号的存在，这个方程是**反向积分**的。它需要一个终点时刻的边界条件。如果成本函数没有在 $t=T$ 时刻直接依赖于状态（如上例），则**终端条件**为 $p(T)=0$。如果[成本函数](@entry_id:138681)包含一个终端项，例如 $J(x_T) = \frac{1}{2}\|x(T)-x_d\|^2$，那么终端条件就会变为 $p(T) = \nabla_{x(T)} J = x(T)-x_d$ [@problem_id:3363631]。

在求得 $p(t)$ 的整个轨迹后，最终的梯度由以下公式给出：
$$
\nabla_{x_{0}} J = B_{0}^{-1} (x_{0} - x_{b}) + p(0)
$$
（注意：符号可能因[拉格朗日乘子](@entry_id:142696)的定义而异，此处的 $p(0)$ 对应于一些文献中的 $-\lambda(0)$，但物理意义相同）。这个结果 [@problem_id:33649] 是[四维变分同化](@entry_id:749536)（4D-Var）的基石。

#### [离散时间系统](@entry_id:263935)

在实践中，模型通常是离散的。考虑一个离散时间[非线性系统](@entry_id:168347) $x_{k+1} = M_k(x_k)$（例如，通过对连续系统进行[时间离散化](@entry_id:169380)得到，如 $x_{k+1} = x_k + \Delta t f(x_k, t_k)$ [@problem_id:3363621]）。设[成本函数](@entry_id:138681)为：
$$
J(x_0) = \sum_{k=0}^{K} \mathcal{J}_k(x_k)
$$
同样使用[拉格朗日乘子法](@entry_id:176596)，我们可以为每个时间步引入一个伴随向量 $p_k$。通过设定拉格朗日函数对中间状态 $x_k$ ($k>0$) 的导数为零，我们得到**[离散伴随](@entry_id:748494)递归关系**：
$$
p_k = \left(\frac{\partial M_k}{\partial x_k}\right)^{\top} p_{k+1} + \nabla_{x_k} \mathcal{J}_k
$$
这是一个从 $k=K$ 开始反向进行的递归。其**终端条件**为 $p_{K+1}=0$（假设递归到 $K+1$）或直接由 $p_K$ 的方程给出，这取决于[成本函数](@entry_id:138681)的具体形式。例如，对于[成本函数](@entry_id:138681) $J(x_0) = \frac{1}{2}\sum_{k=0}^K \|Cx_k - y_k\|_{R^{-1}}^2$，伴随递归为 [@problem_id:3363628] [@problem_id:3363621]：
$$
p_k = M_k'(x_k)^\top p_{k+1} + C^\top R^{-1}(Cx_k-y_k), \quad \text{with } p_{K+1}=0
$$
最终，关于[初始条件](@entry_id:152863) $x_0$ 的梯度就是 $p_0$：
$$
\nabla_{x_0} J = p_0
$$
值得注意的是，伴随递归的每一步都涉及一个雅可比矩阵的[转置](@entry_id:142115)乘以一个向量。如果模型算子 $M_k$ 是稀疏的（例如，在 PDE 的有限差分或[有限元离散化](@entry_id:193156)中），那么雅可比矩阵 $M_k'(x_k)$ 也是稀疏的。这意味着伴随模型的计算成本可以与正向模型相当，这是伴随方法在计算上可行性的关键 [@problem_id:3363628]。

### 实践考量与高等主题

#### 伴随模型的稳定性与刚性

伴随模型的数值积分并非没有挑战。它的稳定性和刚性特征与正向模型密切相关。考虑一个[线性系统](@entry_id:147850) $\dot{x} = Ax$。其伴随方程为 $-\dot{p} = A^\top p$。
如果正向系统是[渐近稳定](@entry_id:168077)的，那么 $A$ 的所有[特征值](@entry_id:154894) $\lambda_i$ 都具有负实部 ($\text{Re}(\lambda_i)  0$)。伴随方程的[系统矩阵](@entry_id:172230)是 $-A^\top$，其[特征值](@entry_id:154894)为 $-\lambda_i$。由于 $\text{Re}(-\lambda_i) = -\text{Re}(\lambda_i) > 0$，这意味着伴随方程在**正向积[分时](@entry_id:274419)是不稳定的**。

然而，伴随方程是从终端条件 $p(T)$ 开始**反向积分**的。通过[时间变换](@entry_id:634205) $\tau = T-t$，反向积分的方程变为 $\frac{dp}{d\tau} = A^\top p(\tau)$。这个新系统的矩阵是 $A^\top$，其[特征值](@entry_id:154894)与 $A$ 相同，仍然具有负实部。因此，**如果正向模型是稳定的，那么其伴随模型在反向积分时也是稳定的** [@problem_id:3363629]。

此外，**刚性** (stiffness) 问题会从正向模型传递到伴随模型。如果正向模型是刚性的（即其[特征值](@entry_id:154894)的实部大小跨越多个[数量级](@entry_id:264888)），那么伴随模型在反向积[分时](@entry_id:274419)也是同样刚性的。例如，如果 $A$ 的[特征值](@entry_id:154894)为 $-1$ 和 $-1000$，这在数值积分中是一个典型的[刚性问题](@entry_id:142143)。反向积分的伴随模型，其动力学由[特征值](@entry_id:154894)同样为 $-1$ 和 $-1000$ 的 $A^\top$ 驱动，因此也存在相同的刚性问题。这意味着，对伴随模型进行[数值积分](@entry_id:136578)时，如果使用显式格式（如欧拉前向法），步长将受到最大幅度[特征值](@entry_id:154894)的严格限制（本例中为 $\Delta t \lesssim 1/1000$）。因此，高效求解伴随模型通常需要使用为刚性问题设计的[隐式数值方法](@entry_id:178288)（如欧拉后向法）[@problem_id:3363629]。

#### “先离散后伴随”与“先伴随后离散”

在处理由[连续模](@entry_id:158807)型（如 ODE/PDE）出发的[优化问题](@entry_id:266749)时，存在两种构建[离散伴随](@entry_id:748494)模型的策略：

1.  **先离散后伴随 (Discretize-then-Adjoint, DTA)**：首先对连续的[非线性](@entry_id:637147)正向模型进行[数值离散化](@entry_id:752782)，得到一个离散的[非线性模型](@entry_id:276864)。然后，为这个离散模型推导其精确的[离散伴随](@entry_id:748494)。
2.  **先伴随后离散 (Adjoint-then-Discretize, ATD)**：首先为原始的[连续模](@entry_id:158807)型推导其[连续伴随](@entry_id:747804)方程（一个线性的 ODE/PDE）。然后，对正向和伴随两个连续方程分别进行[数值离散化](@entry_id:752782)。

一个至关重要且时常微妙的结论是：**这两种方法得到的结果通常是不同的**。即，离散化和伴随这两个操作是**不可交换**的 [@problem_id:3363678]。

DTA 方法的产物是**离散[成本函数](@entry_id:138681)关于离散模型[控制变量](@entry_id:137239)的精确梯度**。这意味着，如果我们将这个梯度用于[基于梯度的优化](@entry_id:169228)算法（如 [L-BFGS](@entry_id:167263)），我们能够精确地最小化我们实际求解的离散成本函数。因此，DTA 是实践中（尤其是在[数值天气预报](@entry_id:191656)等领域）的标准做法，因为它保证了梯度与成本函数的一致性。

ATD 方法的产物是**连续梯度的一个离散近似**。它并不精确对应于任何一个离散成本函数的梯度。当离散化步长 $\Delta t \to 0$ 时，ATD 梯度和 DTA 梯度都会收敛到连续梯度，但对于有限的 $\Delta t$，它们之间存在差异。这个差异的大小取决于所使用的[离散化格式](@entry_id:153074)。例如，对于一个简单的线性 ODE $\dot{x}=\theta x$ 和欧拉前向离散化，DTA 和 ATD 梯度之间的差异是 $\mathcal{O}((\Delta t)^2)$ [@problem_id:3363678]。

理解 DTA 和 ATD 之间的区别对于正确实现和解释伴随模型的结果至关重要。在绝大多数应用中，忠实于离散模型的 DTA 方法是首选，因为它能确保[优化算法](@entry_id:147840)的收敛性和鲁棒性。