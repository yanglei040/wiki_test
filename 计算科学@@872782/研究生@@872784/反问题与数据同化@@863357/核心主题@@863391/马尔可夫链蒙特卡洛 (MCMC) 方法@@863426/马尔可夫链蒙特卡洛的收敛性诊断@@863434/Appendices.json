{"hands_on_practices": [{"introduction": "理论知识必须通过实践来巩固。马尔可夫链蒙特卡洛（MCMC）方法的核心挑战之一是评估样本的质量。本节的第一个练习将理论与实践联系起来，要求您从给定的经验自相关函数中，推导并计算有效样本量（ESS）和蒙特卡洛标准误差（MCSE）。这个基本练习旨在加深您对链的“记忆”如何直接影响我们对后验估计不确定性的理解，这是任何严谨的MCMC分析的基石。[@problem_id:3372636]", "problem": "在一个线性观测模型的贝叶斯逆问题中，一个马尔可夫链蒙特卡罗（MCMC, Markov chain Monte Carlo）算法产生了一个标量后验泛函 $Y_{t} = g(\\theta_{t})$ 的平稳序列 $\\{Y_{t}\\}_{t=1}^{N}$，其中 $N = 50000$。假设马尔可夫链中心极限定理对 $\\{Y_{t}\\}$ 成立，并且已经达到了平稳性。给定经验边际方差估计 $s^{2} = 2.25$ 和滞后最高为 $L = 10$ 的经验自相关 $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$：\n$$\n\\hat{\\rho}_{1} = 0.65,\\quad \\hat{\\rho}_{2} = 0.48,\\quad \\hat{\\rho}_{3} = 0.36,\\quad \\hat{\\rho}_{4} = 0.27,\\quad \\hat{\\rho}_{5} = 0.20,\\\\\n\\hat{\\rho}_{6} = 0.14,\\quad \\hat{\\rho}_{7} = 0.10,\\quad \\hat{\\rho}_{8} = 0.07,\\quad \\hat{\\rho}_{9} = 0.04,\\quad \\hat{\\rho}_{10} = 0.02.\n$$\n仅从严平稳过程的自协方差函数定义以及马尔可夫链中心极限定理关于样本均值渐近方差通过零点谱密度表征的论述出发，为后验均值 $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$ 推导一个有效样本量（ESS）和蒙特卡罗标准误（MCSE）的一致大样本估计量，该估计量通过有限滞后截断使用经验自相关 $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$。然后，使用上述数值，计算ESS和MCSE的数值。最后，用文字论证当经验自相关在中等和较大滞后上存在噪声时，重叠分批均值（OBM, overlapping batch means）方法如何能够稳定MCSE的估计，并将你的论证与零频率下谱密度的一致性联系起来。\n\n最终数值答案只报告MCSE值，四舍五入到四位有效数字。", "solution": "该问题是有效的，因为它在科学上基于马尔可夫链蒙特卡罗（MCMC）方法的统计理论，问题提法良好，信息充分，有唯一解，并以客观、正式的语言表述。它代表了MCMC输出分析中一个标准的、不平凡的任务。\n\n我们首先推导有效样本量（ESS）和蒙特卡罗标准误（MCSE）的估计量。设 $\\{Y_{t}\\}_{t=1}^{N}$ 是一个弱平稳序列，其均值为 $\\mu = \\mathbb{E}[Y_{t}]$，方差为 $\\sigma^{2} = \\text{Var}(Y_{t})$，自协方差函数为 $\\gamma_{k} = \\text{Cov}(Y_{t}, Y_{t+k})$。样本均值为 $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$。\n\n样本均值的方差为\n$$\n\\text{Var}(\\bar{Y}_{N}) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} Y_{t}\\right) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\text{Cov}(Y_{t}, Y_{s}) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\gamma_{t-s}.\n$$\n对于大样本量 $N$，该方差可以近似为\n$$\n\\text{Var}(\\bar{Y}_{N}) \\approx \\frac{1}{N} \\sum_{k=-(N-1)}^{N-1} \\left(1 - \\frac{|k|}{N}\\right) \\gamma_{k} \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\n马尔可夫链中心极限定理（CLT）指出，在适当条件下，\n$$\n\\sqrt{N}(\\bar{Y}_{N} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{\\text{asy}}^{2}),\n$$\n其中渐近方差 $\\sigma_{\\text{asy}}^{2}$ 由所有自协方差之和给出：\n$$\n\\sigma_{\\text{asy}}^{2} = \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\n这与过程的谱密度 $f(\\omega) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k} \\exp(-ik\\omega)$ 相关。在频率 $\\omega=0$ 处，我们有 $f(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k}$，这直接得出 $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$。\n\n由于平稳性，$\\gamma_{k} = \\gamma_{-k}$。我们可以使用自相关函数 $\\rho_{k} = \\gamma_{k}/\\gamma_{0} = \\gamma_{k}/\\sigma^{2}$ 将渐近方差重写为：\n$$\n\\sigma_{\\text{asy}}^{2} = \\gamma_{0} + \\sum_{k=1}^{\\infty} \\gamma_{k} + \\sum_{k=-\\infty}^{-1} \\gamma_{k} = \\gamma_{0} + 2\\sum_{k=1}^{\\infty} \\gamma_{k} = \\sigma^{2}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right).\n$$\n项 $\\tau = 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}$ 被称为积分自相关时间（IACT）。\n\n蒙特卡罗标准误（MCSE）是估计量 $\\bar{Y}_{N}$ 的标准差，对于大 $N$ 而言，其值为\n$$\n\\text{MCSE}(\\bar{Y}_{N}) = \\sqrt{\\text{Var}(\\bar{Y}_{N})} \\approx \\sqrt{\\frac{\\sigma_{\\text{asy}}^{2}}{N}} = \\sqrt{\\frac{\\sigma^{2}}{N}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right)}.\n$$\n有效样本量（ESS）被定义为能为样本均值产生相同方差的等效独立同分布（i.i.d.）样本的大小。对于一个大小为 $M$ 的独立同分布样本，均值的方差为 $\\sigma^{2}/M$。将其与MCMC均值的方差相等可得：\n$$\n\\frac{\\sigma^{2}}{\\text{ESS}} = \\text{Var}(\\bar{Y}_{N}) \\approx \\frac{\\sigma_{\\text{asy}}^{2}}{N} \\implies \\text{ESS} = \\frac{N\\sigma^{2}}{\\sigma_{\\text{asy}}^{2}} = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}}.\n$$\n为了构造一致的估计量，我们用它们的经验对应物替换理论量。边际方差 $\\sigma^{2}$ 由样本方差 $s^{2}$ 估计。自相关 $\\rho_{k}$ 由样本自相关 $\\hat{\\rho}_{k}$ 估计。问题指定使用有限滞后截断，这意味着我们将自相关的无穷和用一个直到滞后 $L$ 的有限和来近似。由此产生的IACT估计量为：\n$$\n\\hat{\\tau}_{L} = 1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}.\n$$\n这导出了以下大样本估计量：\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{L}} = \\frac{N}{1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}}\n$$\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{s^{2}\\hat{\\tau}_{L}}{N}} = \\sqrt{\\frac{s^{2}}{N}\\left(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}\\right)}.\n$$\n现在，我们使用给定的数据计算数值：$N = 50000$，$s^{2} = 2.25$，以及滞后最高为 $L = 10$ 的样本自相关。\n首先，我们对给定的经验自相关求和：\n$$\n\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 0.65 + 0.48 + 0.36 + 0.27 + 0.20 + 0.14 + 0.10 + 0.07 + 0.04 + 0.02 = 2.33.\n$$\n接下来，我们计算估计的IACT：\n$$\n\\hat{\\tau}_{10} = 1 + 2\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 1 + 2(2.33) = 1 + 4.66 = 5.66.\n$$\n使用这个值，我们计算有效样本量：\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{10}} = \\frac{50000}{5.66} \\approx 8833.922.\n$$\n最后，我们计算蒙特卡罗标准误：\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{2.25}{50000 / 5.66}} = \\sqrt{\\frac{2.25 \\times 5.66}{50000}} = \\sqrt{\\frac{12.735}{50000}} = \\sqrt{0.0002547} \\approx 0.0159593.\n$$\n四舍五入到四位有效数字，MCSE 为 $0.01596$。\n\n最后，我们讨论重叠分批均值（OBM）方法的作用。渐近方差的简单有限滞后截断估计量是 $\\hat{\\sigma}_{\\text{asy, L}}^{2} = s^{2}(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}) = \\sum_{k=-L}^{L} \\hat{\\gamma}_{k}$。已知该估计量具有高方差，因为样本自协方差 $\\hat{\\gamma}_{k}$ 本身就存在噪声，尤其是在中等到较大的滞后 $k$ 上。直接对这些含噪声的项求和可能导致 $\\sigma_{\\text{asy}}^{2}$ 的估计不稳定。如果负样本相关的总和足够大，甚至可能得到负的估计值，这对于方差来说是无意义的。\n\nOBM 方法提供了一个更稳定的替代方案。它使用重叠观测批次均值的方差来估计 $\\sigma_{\\text{asy}}^{2}$。对于批次大小 $b$，如果 $b \\to \\infty$ 且 $b/N \\to 0$，则 OBM 对 $\\sigma_{\\text{asy}}^{2}$ 的估计量在 $N \\to \\infty$ 时是一致的。关键的见解是，OBM 估计量等价于一个在零频率下的基于核的谱密度估计量。具体来说，可以证明OBM方差估计量具有以下形式：\n$$\n\\hat{\\sigma}_{\\text{OBM}}^{2} \\approx \\sum_{k=-(b-1)}^{b-1} K\\left(\\frac{k}{b}\\right) \\hat{\\gamma}_{k},\n$$\n其中 $K(x) = 1 - |x|$ (对于 $|x| \\le 1$) 是 Bartlett（或三角）核。相比之下，有限滞后截断方法对应于使用矩形核，$K(x) = 1$ (对于 $|x| \\le 1$)。\n\nBartlett 核通过降低含噪声的高滞后自协方差的贡献来稳定 $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$ 的估计。权重从滞后 $k=0$ 处的 $1$ 线性递减到滞后 $|k|=b$ 处的 $0$。与矩形核（它对直到截断点 $L$ 的所有滞后给予同等权重）相比，这种锥化降低了总估计量的方差。通过系统地减少求和中最不可靠项的影响，OBM 方法为零点谱密度提供了一个更稳健和统计上一致的估计，从而为MCSE提供了一个更稳定的估计。", "answer": "$$\\boxed{0.01596}$$", "id": "3372636"}, {"introduction": "在掌握了基本度量的计算之后，我们进入一个完整的编码实践，以探索高自相关性更微妙的影响。在这个练习中，您将实现一个随机游走Metropolis采样器，并设计一个动态稀释诊断程序，该程序会揭示一个关键问题：未收敛的链（尤其是在初始瞬态阶段）的后验统计量会因稀释而发生显著变化。通过亲手实现这个诊断工具，您将深刻理解为什么稀释不能“修复”收敛问题，以及为什么比较稀释前后样本的统计特性是诊断链是否真正达到平稳分布的有力方法。[@problem_id:3372604]", "problem": "给定一个线性高斯逆问题，其中未知状态向量 $x \\in \\mathbb{R}^d$ 服从高斯先验，且观测算子是线性的，并带有加性高斯噪声。具体来说，先验为 $x \\sim \\mathcal{N}(0, \\Gamma_{\\text{pr}})$，其中 $\\Gamma_{\\text{pr}} = \\alpha^2 I_d$；观测值为 $y = A x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$；并且 $A \\in \\mathbb{R}^{m \\times d}$ 是已知的。在这些假设下，后验分布是高斯分布。您将为该后验分布实现一个随机游走 Metropolis (Random Walk Metropolis) 采样器，以及一个基于在线自相关估计指导的动态稀疏化 (dynamic thinning) 诊断方法，以检验稀疏化是否会显著改变后验摘要。这种变化表明，当朴素地进行稀疏化时，非收敛的链可能会伪装成独立的样本。\n\n从基本原理出发：\n\n- 后验密度 $\\pi(x \\mid y)$ 正比于先验密度和似然密度的乘积。随机游走 Metropolis 方法构建一个马尔可夫链 $\\{x_t\\}_{t=0}^{T-1}$，其转移为 $x' = x_t + \\eta$，其中 $\\eta \\sim \\mathcal{N}(0, \\Sigma_{\\text{prop}})$，并以概率 $\\min\\{1, \\exp(\\log \\pi(x' \\mid y) - \\log \\pi(x_t \\mid y))\\}$ 接受该提议。\n\n- 对于标量时间序列 $\\{s_t\\}$，滞后 $k$ 的自相关函数定义为 $\\rho(k) = \\frac{\\mathrm{Cov}(s_t, s_{t+k})}{\\mathrm{Var}(s_t)}$。在一阶自回归 (AR($1$)) 近似下进行在线估计时，如果滞后-1 自相关为 $\\phi = \\rho(1)$，则 $\\rho(k) \\approx \\phi^k$。您将在采样过程中，使用运行和 (running sums) 来在线估计均值、方差和滞后-1 协方差，从而估计 $\\phi$，并将稀疏化步长 $s$ 设置为满足 $\\phi^s \\le \\tau$ 的最小整数，即当 $0  \\phi  1$ 时，$s = \\left\\lceil \\frac{\\log(\\tau)}{\\log(\\phi)} \\right\\rceil$，如果 $\\phi \\le 0$，则 $s = 1$。\n\n- 该诊断方法比较从预烧期 (burn-in) 后的未稀疏化链和动态稀疏化链计算出的后验摘要。后验摘要包括样本均值向量和样本协方差矩阵。将标准化均值差异定义为均值差异除以未稀疏化样本各维度标准差后，在维度上计算的均方根；并将相对协方差差异定义为协方差差异的弗罗贝尼乌斯范数 (Frobenius norm) 除以未稀疏化协方差的弗罗贝尼乌斯范数。如果任一差异超过其容差，则认为稀疏化对后验摘要产生了显著改变。\n\n实现以下内容：\n\n$1.$ 使用上面定义的先验和似然，为线性高斯逆问题构建对数后验（允许相差一个加性常数）。使用 $\\Gamma_{\\text{pr}}^{-1} = \\frac{1}{\\alpha^2} I_d$ 和 $R^{-1} = \\frac{1}{\\sigma^2} I_m$。\n\n$2.$ 实现一个随机游走 Metropolis 采样器，其提议协方差为 $\\Sigma_{\\text{prop}} = \\sigma_{\\text{prop}}^2 I_d$，总迭代次数为 $T$，初始状态为 $x_0$。记录完整的链 $\\{x_t\\}_{t=0}^{T-1}$。\n\n$3.$ 从预烧期后的样本中，为 $x_t$ 的每个坐标实现一个滞后-1 自相关 $\\phi$ 的在线估计器。在迭代 $t$（对于 $t \\ge 1$），更新每个维度 $j \\in \\{1, \\dots, d\\}$ 的运行和：$x_t^{(j)}$、$ (x_t^{(j)})^2$、$x_{t-1}^{(j)}$、$(x_{t-1}^{(j)})^2$ 和 $x_t^{(j)} x_{t-1}^{(j)}$ 的和。在累积了至少最小数量的样本对之后，计算各坐标的滞后-1 相关性估计值 $\\hat{\\phi}_j$，并设置 $\\phi = \\max_j \\max\\{0, \\hat{\\phi}_j\\}$ 以在各维度上采取保守估计。\n\n$4.$ 在预烧期后即时实现动态稀疏化。从第一个预烧期后的索引开始，以固定的更新间隔，维护一个根据 $\\phi$ 计算的当前步长 $s$。每当自上次保留样本以来的步数达到 $s$ 时，就保留一个样本。\n\n$5.$ 采样结束后，从预烧期后的部分计算两个后验摘要：未稀疏化样本的均值和协方差，以及动态稀疏化样本的均值和协方差。按照上述规定计算标准化均值差异和相对协方差差异。报告稀疏化是否显著改变了摘要，这是一个布尔值，如果标准化均值差异超过其均值容差，或相对协方差差异超过其协方差容差，则为真。\n\n设计一个测试套件，包含以下参数集，用于测试不同的行为，包括混合良好的链、预烧期不足导致的高自相关链，以及高维相关问题：\n\n测试用例 1（一维，混合良好）：\n- $d = 1$, $m = 1$, $A = [1.0]$, $\\alpha = 1.0$, $\\sigma = 0.5$, $x_0 = 0.0$, $y = 0.7$, $\\sigma_{\\text{prop}} = 0.6$, $T = 15000$, 预烧期比例 $b = 0.2$, 自相关阈值 $\\tau = 0.05$, 更新间隔 $U = 200$, 最小样本对数 $P_{\\min} = 50$, 均值容差 $\\epsilon_{\\mu} = 0.02$, 协方差容差 $\\epsilon_{\\Sigma} = 0.05$, 固定随机种子。\n\n测试用例 2（一维，慢混合且无预烧期）：\n- $d = 1$, $m = 1$, $A = [1.0]$, $\\alpha = 1.0$, $\\sigma = 0.5$, $x_0 = -2.0$, $y = 0.7$, $\\sigma_{\\text{prop}} = 0.05$, $T = 6000$, 预烧期比例 $b = 0.0$, 自相关阈值 $\\tau = 0.05$, 更新间隔 $U = 100$, 最小样本对数 $P_{\\min} = 50$, 均值容差 $\\epsilon_{\\mu} = 0.05$, 协方差容差 $\\epsilon_{\\Sigma} = 0.10$, 固定随机种子。\n\n测试用例 3（五维，相关的前向算子，混合良好）：\n- $d = 5$, $m = 5$, \n  $A = \\begin{bmatrix}\n  1.0  0.8  0.0  0.0  0.0 \\\\\n  0.8  1.2  0.5  0.0  0.0 \\\\\n  0.0  0.5  1.0  0.4  0.0 \\\\\n  0.0  0.0  0.4  1.1  0.7 \\\\\n  0.0  0.0  0.0  0.7  1.3\n  \\end{bmatrix}$,\n  $\\alpha = 1.0$, $\\sigma = 0.3$, $x_0 = [0, 0, 0, 0, 0]^\\top$, $x_{\\text{true}} = [1, -1, 0.5, 0, 2]^\\top$, $y = A x_{\\text{true}}$, $\\sigma_{\\text{prop}} = 0.4$, $T = 20000$, 预烧期比例 $b = 0.25$, 自相关阈值 $\\tau = 0.05$, 更新间隔 $U = 200$, 最小样本对数 $P_{\\min} = 100$, 均值容差 $\\epsilon_{\\mu} = 0.03$, 协方差容差 $\\epsilon_{\\Sigma} = 0.08$, 固定随机种子。\n\n测试用例 4（五维，慢混合且运行时间短）：\n- $d = 5$, $m = 5$, 与测试用例 3 中相同的 $A$ 和 $y$, $\\alpha = 1.0$, $\\sigma = 0.3$, $x_0 = [0, 0, 0, 0, 0]^\\top$, $\\sigma_{\\text{prop}} = 0.05$, $T = 8000$, 预烧期比例 $b = 0.05$, 自相关阈值 $\\tau = 0.05$, 更新间隔 $U = 100$, 最小样本对数 $P_{\\min} = 100$, 均值容差 $\\epsilon_{\\mu} = 0.04$, 协方差容差 $\\epsilon_{\\Sigma} = 0.10$, 固定随机种子。\n\n您的程序必须执行所有测试用例，并生成一行输出，其中包含按顺序排列的四个布尔结果，格式为方括号括起来的逗号分隔列表（例如，`[true,false,false,true]`）。不允许有任何其他输出。不涉及物理单位或角度。布尔值使用小写字母表示。程序必须能直接运行，不得需要用户输入、外部文件或网络访问。使用指定的 Python 环境和库。[@problem_id:449]", "solution": "### 第 1 步：提取给定信息\n- **贝叶斯模型：**\n  - 先验：$x \\sim \\mathcal{N}(0, \\Gamma_{\\text{pr}})$，其中 $\\Gamma_{\\text{pr}} = \\alpha^2 I_d$。状态向量 $x \\in \\mathbb{R}^d$。\n  - 似然：$y = A x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$。观测向量 $y \\in \\mathbb{R}^m$，观测算子 $A \\in \\mathbb{R}^{m \\times d}$。\n- **对数后验构建：**\n  - 使用 $\\Gamma_{\\text{pr}}^{-1} = \\frac{1}{\\alpha^2} I_d$ 和 $R^{-1} = \\frac{1}{\\sigma^2} I_m$。\n- **MCMC 采样器：**\n  - 算法：随机游走 Metropolis (RWM)。\n  - 提议：$x' = x_t + \\eta$，其中 $\\eta \\sim \\mathcal{N}(0, \\Sigma_{\\text{prop}})$，且 $\\Sigma_{\\text{prop}} = \\sigma_{\\text{prop}}^2 I_d$。\n  - 总迭代次数：$T$。\n  - 初始状态：$x_0$。\n- **在线自相关估计：**\n  - 近似：AR($1$)，$\\rho(k) \\approx \\phi^k$，其中 $\\phi = \\rho(1)$。\n  - 估计：对预烧期后的样本，使用运行和来计算均值、方差和滞后-1 协方差。\n  - 组合：$\\phi = \\max_j \\max\\{0, \\hat{\\phi}_j\\}$，其中 $\\hat{\\phi}_j$ 是维度 $j$ 的估计值。\n  - 用于估计的最小样本对数：$P_{\\min}$。\n- **动态稀疏化：**\n  - 步长计算：对于 $0  \\phi  1$， $s = \\left\\lceil \\frac{\\log(\\tau)}{\\log(\\phi)} \\right\\rceil$；如果 $\\phi \\le 0$，则 $s = 1$。$\\tau$ 是自相关阈值。\n  - 步长更新间隔：$U$。\n  - 过程：预烧期后，每当自上次保留样本以来的步数达到当前步长 $s$ 时，就保留一个样本。\n- **诊断：**\n  - 后验摘要：样本均值向量和样本协方差矩阵。\n  - 比较：预烧期后的未稀疏化链 vs. 动态稀疏化链。\n  - 标准化均值差异：$D_\\mu = \\sqrt{\\frac{1}{d} \\sum_{j=1}^d \\left(\\frac{(\\mu_{\\text{unthinned}} - \\mu_{\\text{thinned}})_j}{(\\sigma_{\\text{unthinned}})_j}\\right)^2}$。\n  - 相对协方差差异：$D_\\Sigma = \\frac{\\|\\Sigma_{\\text{unthinned}} - \\Sigma_{\\text{thinned}}\\|_F}{\\|\\Sigma_{\\text{unthinned}}\\|_F}$。\n  - 显著改变的条件：$D_\\mu > \\epsilon_{\\mu}$ 或 $D_\\Sigma > \\epsilon_{\\Sigma}$。\n- **测试用例：** 提供了四个具体的参数集，包括维度、模型参数、MCMC 参数、诊断参数以及固定的随机种子协议。\n\n### 第 2 步：使用提取的给定信息进行验证\n1.  **科学或事实不健全性**：该问题在贝叶斯推断、MCMC 方法（特别是 RWM）和标准统计收敛诊断（自相关）方面有充分的依据。线性高斯模型是该领域的典型例子。所描述的方法是标准的。该问题在科学和数学上是合理的。\n2.  **非形式化或不相关**：该问题高度形式化，直接关系到*逆问题与数据同化*学科中*马尔可夫链蒙特卡罗收敛诊断*。\n3.  **不完整或矛盾的设置**：该问题异常详细，为四个不同的测试用例提供了模型、采样器和诊断所需的所有必要参数。没有明显的矛盾。设置是自洽和完整的。\n4.  **不切实际或不可行**：该问题描述了一个计算模拟。所有参数都已指定，所需的计算（MCMC 采样、在线统计、矩阵范数）都是标准的，并且在给定的迭代限制内计算上是可行的。\n5.  **不适定或结构不良**：任务是实现一个明确定义的算法，并根据精确的标准报告一个特定的布尔输出。这构成了一个适定的计算问题。所有术语，如“标准化均值差异”和“相对协方差差异”，都有明确的定义。\n6.  **伪深刻、琐碎或同义反复**：该问题很复杂，需要仔细实现几个相互关联的统计和数值概念。测试用例旨在探测不同的 MCMC 混合行为，展示了对基本概念的非凡理解。这是一项实质性的、非琐碎的实现任务。\n7.  **超出科学可验证性**：该问题是一个计算任务。给定一个固定的随机种子，输出是确定性的，可以通过独立实现进行验证。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。它是一个适定的、科学上合理的、完整的计算任务，植根于计算统计和逆问题的既定原则。我现在将继续提供一个完整的解决方案。\n\n该解决方案将构建为一个单一的 Python 脚本。一个主函数 `run_mcmc_and_diagnostic` 将封装单个测试用例的整个过程：设置模型，运行带有即时动态稀疏化和自相关估计的 RWM 采样器，最后计算两个差异度量以确定布尔结果。一个 `solve` 函数将定义四个测试用例，并为每个用例调用主函数，收集并以指定格式打印结果。\n\n**数学公式**\n\n对数后验密度，在相差一个加性常数的情况下，由下式给出：\n$$\n\\log \\pi(x \\mid y) \\propto -\\frac{1}{2 \\sigma^2} \\|y - Ax\\|_2^2 - \\frac{1}{2 \\alpha^2} \\|x\\|_2^2\n$$\n该函数将用于计算 Metropolis-Hastings 步骤中的接受概率。\n\n**算法逻辑**\n\n1.  **初始化**：设置 MCMC 链，初始化状态 $x_0$，并预计算初始对数后验值。为在线统计和稀疏化链初始化数据结构。\n2.  **MCMC 循环**：从 $t=1$ 到 $T-1$ 迭代。在每次迭代中：\n    a. 从 $\\mathcal{N}(x_{t-1}, \\sigma_{\\text{prop}}^2 I_d)$ 中提议一个新状态 $x'$。\n    b. 计算接受对数概率 $\\log \\alpha_{\\text{acc}} = \\log \\pi(x' \\mid y) - \\log \\pi(x_{t-1} \\mid y)$。\n    c. 如果 $\\log(\\text{random uniform } U(0,1))  \\log \\alpha_{\\text{acc}}$，则接受 $x'$ 作为 $x_t$；否则，设置 $x_t = x_{t-1}$。\n    d. 存储 $x_t$。\n3.  **即时诊断（预烧期后）**：对于预烧期点 $N_{\\text{burn}} = \\lfloor b \\cdot T \\rfloor$ 之后的每次迭代 $t$：\n    a. **在线自相关**：如果 $t > N_{\\text{burn}}$，使用对 $(x_{t-1}, x_t)$ 更新均值、平方值和交叉积的运行和。在 $U$ 个样本的间隔处，如果已收集到至少 $P_{\\min}$ 对，则计算坐标方向的滞后-1 自相关 $\\hat{\\phi}_j$，确定总的 $\\phi = \\max_j \\max\\{0, \\hat{\\phi}_j\\}$，并更新稀疏化步长 $s$。\n    b. **动态稀疏化**：保留第一个预烧期后的样本。对于后续样本，使用计数器根据当前步长 $s$ 确定何时保留下一个样本。\n4.  **最终比较**：MCMC 运行完成后：\n    a. 提取未稀疏化的预烧期后样本和动态稀疏化的样本。\n    b. 计算两组样本的样本均值和协方差。\n    c. 使用提供的公式计算标准化均值差异和相对协方差差异。\n    d. 如果任一差异超过其各自的容差（$\\epsilon_{\\mu}, \\epsilon_{\\Sigma}$），则返回 `True`，否则返回 `False`。\n\n该实现将正确处理标量（$d=1$）和向量（$d>1$）情况，特别是在协方差计算方面。将管理边缘情况，如零方差或稀疏化样本数量不足，以确保数值稳定性和逻辑正确性。", "answer": "```python\nimport numpy as np\n\ndef run_mcmc_and_diagnostic(\n    d, m, A, alpha, sigma, x0, y_val, xtrue,\n    sigma_prop, T, b, tau, U, P_min,\n    epsilon_mu, epsilon_Sigma, seed\n):\n    \"\"\"\n    Runs the Random Walk Metropolis sampler with on-the-fly dynamic thinning\n    and returns a boolean indicating if thinning materially changed summaries.\n    \"\"\"\n    # 1. Setup\n    rng = np.random.default_rng(seed)\n    A = np.array(A, dtype=float)\n    x0 = np.array(x0, dtype=float)\n\n    if xtrue is not None:\n        y = A @ np.array(xtrue, dtype=float)\n    else:\n        y = np.array([y_val] if isinstance(y_val, (int, float)) else y_val, dtype=float)\n\n    # Log posterior function (up to a constant)\n    inv_sigma_sq = 1.0 / (sigma**2)\n    inv_alpha_sq = 1.0 / (alpha**2)\n    def log_posterior(x):\n        residual = y - A @ x\n        log_lik = -0.5 * inv_sigma_sq * np.dot(residual, residual)\n        log_prior = -0.5 * inv_alpha_sq * np.dot(x, x)\n        return log_lik + log_prior\n\n    # 2. Initialize MCMC and Diagnostics\n    chain = np.zeros((T, d))\n    chain[0] = x0\n    x_current = x0.copy()\n    log_post_current = log_posterior(x_current)\n\n    N_burn = int(b * T)\n    \n    # Online autocorrelation stats (for the post-burn-in series)\n    sum_x_lag_1 = np.zeros(d)\n    sum_sq_x_lag_1 = np.zeros(d)\n    sum_x_curr = np.zeros(d)\n    sum_sq_x_curr = np.zeros(d)\n    sum_prod = np.zeros(d)\n    num_pairs = 0\n\n    # Dynamic thinning\n    thinned_chain = []\n    thinning_stride = 1\n    steps_since_last_kept = 0\n    \n    # Pre-add first sample if no burn-in\n    if N_burn == 0:\n        thinned_chain.append(x0)\n        steps_since_last_kept = 0\n\n    # 3. MCMC Sampling Loop\n    for t in range(1, T):\n        proposal = x_current + rng.normal(scale=sigma_prop, size=d)\n        log_post_proposal = log_posterior(proposal)\n        \n        log_acc_ratio = log_post_proposal - log_post_current\n        if np.log(rng.uniform())  log_acc_ratio:\n            x_current = proposal\n            log_post_current = log_post_proposal\n        \n        chain[t] = x_current\n\n        # On-the-fly diagnostics for post-burn-in samples\n        if t >= N_burn:\n            post_burn_sample_count = t - N_burn + 1\n            \n            # Use pairs from post-burn-in series for autocorrelation\n            if t > N_burn:\n                x_prev = chain[t-1]\n                x_curr = chain[t]\n                \n                sum_x_lag_1 += x_prev\n                sum_sq_x_lag_1 += x_prev**2\n                sum_x_curr += x_curr\n                sum_sq_x_curr += x_curr**2\n                sum_prod += x_prev * x_curr\n                num_pairs += 1\n\n                # Update thinning stride at specified intervals\n                if post_burn_sample_count % U == 0 and num_pairs >= P_min:\n                    mean_lag_1 = sum_x_lag_1 / num_pairs\n                    mean_curr = sum_x_curr / num_pairs\n                    \n                    cov_num = (sum_prod / num_pairs) - (mean_lag_1 * mean_curr)\n                    var_lag_1 = (sum_sq_x_lag_1 / num_pairs) - mean_lag_1**2\n                    var_curr = (sum_sq_x_curr / num_pairs) - mean_curr**2\n                    \n                    den_sq = var_lag_1 * var_curr\n                    phis = np.zeros(d)\n                    valid_indices = den_sq > 1e-12\n                    phis[valid_indices] = cov_num[valid_indices] / np.sqrt(den_sq[valid_indices])\n                    \n                    phi = np.max(np.maximum(0, phis))\n                    \n                    if 0  phi  1:\n                        thinning_stride = int(np.ceil(np.log(tau) / np.log(phi)))\n                    else:\n                        thinning_stride = 1\n                    thinning_stride = max(1, thinning_stride)\n\n            # Perform dynamic thinning\n            if t == N_burn:\n                thinned_chain.append(chain[t])\n                steps_since_last_kept = 0\n            else: # t > N_burn\n                steps_since_last_kept += 1\n                if steps_since_last_kept >= thinning_stride:\n                    thinned_chain.append(chain[t])\n                    steps_since_last_kept = 0\n\n    # 4. Final Comparison\n    unthinned_samples = chain[N_burn:]\n    thinned_samples = np.array(thinned_chain)\n\n    if unthinned_samples.shape[0]  2 or thinned_samples.shape[0]  2:\n        return True # Not enough samples for statistics implies a material change\n\n    # Compute summaries\n    mean_u = np.mean(unthinned_samples, axis=0)\n    cov_u = np.atleast_2d(np.cov(unthinned_samples, rowvar=False))\n    \n    mean_th = np.mean(thinned_samples, axis=0)\n    cov_th = np.atleast_2d(np.cov(thinned_samples, rowvar=False))\n\n    # Compute discrepancies\n    # Mean discrepancy\n    std_u = np.sqrt(np.diag(cov_u))\n    mean_diff = mean_u - mean_th\n    scaled_mean_diff_sq = np.zeros(d)\n    \n    valid_std_mask = std_u > 1e-12\n    scaled_mean_diff_sq[valid_std_mask] = (mean_diff[valid_std_mask] / std_u[valid_std_mask])**2\n    \n    mean_discrepancy = np.inf if np.any(np.abs(mean_diff[~valid_std_mask]) > 1e-9) else np.sqrt(np.mean(scaled_mean_diff_sq))\n\n    # Covariance discrepancy\n    norm_cov_u = np.linalg.norm(cov_u, 'fro')\n    if norm_cov_u  1e-12:\n        cov_discrepancy = np.inf if np.linalg.norm(cov_th, 'fro') > 1e-9 else 0.0\n    else:\n        cov_discrepancy = np.linalg.norm(cov_u - cov_th, 'fro') / norm_cov_u\n\n    return (mean_discrepancy > epsilon_mu) or (cov_discrepancy > epsilon_Sigma)\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the diagnostic for each, and prints the results.\n    \"\"\"\n    A_case3 = [\n        [1.0, 0.8, 0.0, 0.0, 0.0],\n        [0.8, 1.2, 0.5, 0.0, 0.0],\n        [0.0, 0.5, 1.0, 0.4, 0.0],\n        [0.0, 0.0, 0.4, 1.1, 0.7],\n        [0.0, 0.0, 0.0, 0.7, 1.3]\n    ]\n    xtrue_case3 = [1, -1, 0.5, 0, 2]\n\n    test_cases = [\n        # Case 1: 1D, well-mixed\n        {\"d\": 1, \"m\": 1, \"A\": [[1.0]], \"alpha\": 1.0, \"sigma\": 0.5, \"x0\": [0.0], \"y_val\": 0.7, \"xtrue\": None, \n         \"sigma_prop\": 0.6, \"T\": 15000, \"b\": 0.2, \"tau\": 0.05, \"U\": 200, \"P_min\": 50, \n         \"epsilon_mu\": 0.02, \"epsilon_Sigma\": 0.05, \"seed\": 42},\n        # Case 2: 1D, slow mixing, no burn-in\n        {\"d\": 1, \"m\": 1, \"A\": [[1.0]], \"alpha\": 1.0, \"sigma\": 0.5, \"x0\": [-2.0], \"y_val\": 0.7, \"xtrue\": None,\n         \"sigma_prop\": 0.05, \"T\": 6000, \"b\": 0.0, \"tau\": 0.05, \"U\": 100, \"P_min\": 50,\n         \"epsilon_mu\": 0.05, \"epsilon_Sigma\": 0.10, \"seed\": 42},\n        # Case 3: 5D, well-mixed\n        {\"d\": 5, \"m\": 5, \"A\": A_case3, \"alpha\": 1.0, \"sigma\": 0.3, \"x0\": [0,0,0,0,0], \"y_val\": None, \"xtrue\": xtrue_case3,\n         \"sigma_prop\": 0.4, \"T\": 20000, \"b\": 0.25, \"tau\": 0.05, \"U\": 200, \"P_min\": 100,\n         \"epsilon_mu\": 0.03, \"epsilon_Sigma\": 0.08, \"seed\": 42},\n        # Case 4: 5D, slow mixing, short run\n        {\"d\": 5, \"m\": 5, \"A\": A_case3, \"alpha\": 1.0, \"sigma\": 0.3, \"x0\": [0,0,0,0,0], \"y_val\": None, \"xtrue\": xtrue_case3,\n         \"sigma_prop\": 0.05, \"T\": 8000, \"b\": 0.05, \"tau\": 0.05, \"U\": 100, \"P_min\": 100,\n         \"epsilon_mu\": 0.04, \"epsilon_Sigma\": 0.10, \"seed\": 42},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_mcmc_and_diagnostic(**params)\n        results.append(str(result).lower())\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3372604"}, {"introduction": "最后的这项高级实践将通用统计诊断与反问题的特定结构联系起来，这是解决复杂科学模型时的一项关键技能。标准的收敛诊断（如$\\hat{R}$）通常平等地对待所有参数维度，但在高维反问题中，数据对不同参数组合的敏感性差异巨大。本练习要求您实现一个“灵敏度加权的势能标度缩减因子 (PSRF)”，该诊断利用前向模型的雅可比矩阵（灵敏度矩阵）来优先评估数据最敏感方向上的收敛性。通过完成这项任务，您将学会如何构建一个更智能、更具物理意义的诊断工具，将计算资源和分析重点放在模型中最关键、最可识别的部分。[@problem_id:3372654]", "problem": "给定 $m$ 条来自马尔可夫链蒙特卡洛 (MCMC) 算法的平行链，其目标是一个由可微正向算子 $\\mathcal{F}:\\mathbb{R}^d \\to \\mathbb{R}^p$ 控制的贝叶斯逆问题，其中观测值为 $y \\in \\mathbb{R}^p$，观测噪声协方差为 $R \\in \\mathbb{R}^{p \\times p}$。令 $S = \\partial \\mathcal{F}/\\partial \\theta \\in \\mathbb{R}^{p \\times d}$ 表示在某个固定线性化点上的正向敏感度雅可比矩阵。考虑参数空间 $\\mathbb{R}^d$ 中的 $m$ 条长度为 $n$ 的链，组织为 $\\{\\theta_{j,t}\\}_{j=1,\\dots,m;\\, t=1,\\dots,n}$，其中 $\\theta_{j,t} \\in \\mathbb{R}^d$。潜在尺度缩减因子 (PSRF) 是一种单变量收敛诊断，它通过标量目标的跨链基本样本均值和方差摘要来定义，其多变量扩展可以通过投影到 $\\theta$ 的线性泛函上构建。\n\n仅从以下基本要素出发：\n- 标量序列的单变量链式样本均值和无偏样本方差的定义。\n- 标准的单变量潜在尺度缩减因子 (PSRF)，它使用标量投影的链间方差和链内方差，将估计的边际方差与链内方差进行比较。\n- 观测模型的线性高斯局部近似 $y \\approx \\mathcal{F}(\\theta^\\star) + S(\\theta - \\theta^\\star) + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,R)$，这蕴含了由半正定矩阵 $J = S^\\top R^{-1} S \\in \\mathbb{R}^{d \\times d}$ 导出的关于参数扰动的局部信息度量。\n\n推导一个标量收敛诊断方法，该方法强调沿由观测值 $y$ 提供信息最多的方向上的收敛性。具体而言，您必须：\n- 证明将多变量链投影到信息矩阵 $J$ 的特征方向 $v_i \\in \\mathbb{R}^d$ 上，并沿每个这样的方向诊断单变量 PSRF 的合理性。\n- 证明通过根据 $J$ 的相应特征值 $\\lambda_i \\ge 0$ 对方向性 PSRF 值进行加权，从而将它们组合成单个标量的合理性，以此优先考虑信息量更大的方向。\n- 证明对退化情况进行合理处理的合理性，包括某些 $\\lambda_i = 0$ 的情况以及在投影方向上链内方差为零等数值问题。\n\n对于下面的每个测试用例，您的程序必须将最终的敏感度加权 PSRF 诊断实现为 $(\\{\\theta_{j,t}\\}, S, R)$ 的确定性函数。所有计算必须使用实数算术执行。如果 $J$ 的所有特征值都为 $0$（即 $J$ 是零矩阵），则按照约定将加权诊断定义为 $1$，因为 $y$ 通过 $S$ 没有提供关于 $\\theta$ 的任何局部信息。\n\n测试数据生成定义：\n- 对于任何整数 $n \\ge 1$，定义时间索引 $t \\in \\{0,1,\\dots,n-1\\}$。\n- 对于任何整数 $m \\ge 1$，定义链索引 $j \\in \\{0,1,\\dots,m-1\\}$。\n- 对于任何维度 $d \\ge 1$，$\\theta_{j,t} \\in \\mathbb{R}^d$。\n- 令 $\\varphi_j$ 表示一个相位，定义为 $\\varphi_j = j \\pi / 6$。\n- 对于每个测试，链都通过具有指定均值和振幅的正弦公式确定性地生成，从而确保关于特定于链的均值的平稳性。\n\n对于标量投影 $x_{j,t} \\in \\mathbb{R}$ 的单变量 PSRF 成分：\n- 对于每个链 $j$，样本均值 $\\mu_j = \\frac{1}{n}\\sum_{t=1}^n x_{j,t}$ 和无偏样本方差 $s_j^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_{j,t} - \\mu_j)^2$。\n- 合并的链内方差 $W = \\frac{1}{m}\\sum_{j=1}^m s_j^2$。\n- 链间方差 $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\mu_j - \\bar{\\mu})^2$，其中 $\\bar{\\mu} = \\frac{1}{m}\\sum_{j=1}^m \\mu_j$。\n- 单变量 PSRF 使用这些量将估计的边际方差与 $W$ 进行比较。\n\n数值稳定性要求：\n- 在形成任何涉及链内方差的比率时，在分母上添加一个稳定器 $\\varepsilon = 10^{-12}$。\n- 如果在某个投影中，链间方差和链内方差在数值上均为零，则将该投影中的 PSRF 视为 $1$。\n\n测试套件：\n- 在所有测试中，链的数量为 $m = 3$，每条链的迭代次数为 $n = 200$。参数维度为 $d = 2$，观测维度为 $p = 2$。观测噪声协方差为 $R = I_2$，即 $2 \\times 2$ 的单位矩阵。\n- 测试 $1$（良好收敛，第一个方向信息丰富）：\n  - 敏感度矩阵 $S_1 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$。\n  - 对于每个链索引 $j \\in \\{0,1,2\\}$ 和时间 $t \\in \\{0,1,\\dots,199\\}$：\n    - 第一个分量：$\\theta_{j,t}^{(1)} = 0 + 0.2 \\sin\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n    - 第二个分量：$\\theta_{j,t}^{(2)} = 0 + 0.2 \\cos\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n- 测试 $2$（在信息最丰富的方向上收敛性差）：\n  - 敏感度矩阵 $S_2 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$。\n  - 第一个分量上的链特定均值为 $\\mu^{(1)}_0 = -0.3$, $\\mu^{(1)}_1 = 0.0$, $\\mu^{(1)}_2 = 0.3$。在第二个分量上，所有链的均值均为 $0$。\n  - 对于每个链索引 $j \\in \\{0,1,2\\}$ 和时间 $t \\in \\{0,1,\\dots,199\\}$：\n    - 第一个分量：$\\theta_{j,t}^{(1)} = \\mu^{(1)}_j + 0.2 \\sin\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n    - 第二个分量：$\\theta_{j,t}^{(2)} = 0 + 0.2 \\cos\\left(2\\pi t / 200 + \\varphi_j\\right)$。\n- 测试 $3$（无信息敏感度）：\n  - 敏感度矩阵 $S_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$。\n  - 链的生成方式与测试 $1$ 完全相同。\n\n您的任务：\n- 实现一个程序，该程序仅使用上述输入为每个测试计算敏感度加权的 PSRF 诊断，结果为单个标量。\n- 每个测试的结果必须是一个浮点数。在报告最终结果时，将每个数字四舍五入到恰好 $6$ 位小数。\n- 最终输出格式：您的程序应生成一行输出，其中包含三个结果，格式为方括号内的逗号分隔列表，例如 $[r_1,r_2,r_3]$，其中每个 $r_i$ 显示小数点后恰好 $6$ 位数字。", "solution": "该问题要求推导并实现一种敏感度加权的潜在尺度缩减因子 (PSRF)，作为多变量马尔可夫链蒙特卡洛 (MCMC) 链的标量收敛诊断。推导过程必须基于第一性原理和所提供的信息进行论证。\n\n设 MCMC 链表示为 $\\{\\theta_{j,t} \\in \\mathbb{R}^d\\}_{j=1,\\dots,m; t=1,\\dots,n}$，代表在 $d$ 维参数空间中的 $m$ 条长度为 $n$ 的链。给定正向模型的雅可比矩阵 $S \\in \\mathbb{R}^{p \\times d}$ 和观测噪声协方差 $R \\in \\mathbb{R}^{p \\times p}$。\n\n按照要求，推导过程分三部分进行：论证投影方向的选择、论证方向性诊断的加权组合以及论证对退化情况的处理。\n\n### 投影到信息矩阵 $J$ 的特征方向上的合理性论证\n\n该方法的基础是贝叶斯逆问题中后验分布的局部行为。我们从给定的正向模型 $\\mathcal{F}$ 在线性化点 $\\theta^\\star$ 附近的线性高斯近似开始：\n$$y \\approx \\mathcal{F}(\\theta^\\star) + S(\\theta - \\theta^\\star) + \\varepsilon, \\quad \\text{with } \\varepsilon \\sim \\mathcal{N}(0,R)$$\n给定观测值 $y$ 的参数 $\\theta$ 的似然由残差噪声的分布确定。假设该近似是充分的，则负对数似然函数 $\\mathcal{L}(\\theta) = -\\log p(y|\\theta)$ 在 $\\theta$ 中是局部二次的：\n$$ \\mathcal{L}(\\theta) \\approx \\frac{1}{2} (S(\\theta - \\theta^\\star) - \\delta y)^\\top R^{-1} (S(\\theta - \\theta^\\star) - \\delta y) + \\text{const.} $$\n其中 $\\delta y = y - \\mathcal{F}(\\theta^\\star)$ 是在线性化点的数据残差。该负对数似然关于 $\\theta$ 的海森矩阵由下式给出：\n$$ H_{\\mathcal{L}} = \\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta^2} = S^\\top R^{-1} S $$\n这个矩阵正是问题陈述中提供的信息矩阵 $J$。在贝叶斯背景下，后验分布为 $p(\\theta|y) \\propto p(y|\\theta)p(\\theta)$。后验分布在众数附近的局部形状主要由似然决定。矩阵 $J$ 量化了负对数后验的曲率（假设先验局部平坦），这对应于后验的局部高斯近似的精度（协方差的逆）。\n\n信息矩阵 $J$ 是对称半正定的。它允许进行特征分解 $J = V \\Lambda V^\\top$，其中 $V = [v_1, \\dots, v_d]$ 是一个正交矩阵，其列是特征向量 $v_i$，而 $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$ 是相应非负特征值 $\\lambda_i \\ge 0$ 的对角矩阵。\n\n特征向量 $v_i$ 构成了参数空间 $\\mathbb{R}^d$ 的一个标准正交基。相关的特征值 $\\lambda_i$ 衡量了数据 $y$ 沿这些方向提供的关于参数扰动的信息量。一个大的特征值 $\\lambda_i$ 意味着后验在方向 $v_i$ 上受到严格约束，即数据信息量很大。相反，一个小的或零的特征值 $\\lambda_i$ 意味着数据在方向 $v_i$ 上几乎不提供关于参数的信息，并且后验在该方向上的方差很大（即由先验决定）。\n\nMCMC 链的收敛可能是各向异性的。链在由数据提供充分信息的方向上收敛至关重要，因为这些方向定义了后验分布景观的关键特征。沿 $J$ 的每个特征方向 $v_i$ 分别诊断收敛性，使我们能够将问题解耦为一组一维分析，每个分析都对应一个具有特定、可量化信息含量的方向。与检查沿任意轴（例如，原始参数坐标）的收敛性相比，这是一种更优越的策略，因为这些任意轴可能与高后验不确定性和低后验不确定性的方向不一致。因此，将 $d$ 维链 $\\theta_{j,t}$ 投影到每个特征向量 $v_i$ 上以获得标量链 $x_{j,t}^{(i)} = v_i^\\top \\theta_{j,t}$，是进行多变量收敛诊断的一种在统计学和科学上都有原则的方法。\n\n### 方向性 PSRF 加权组合的合理性论证\n\n在投影链之后，我们为 $d$ 个标量值链集 $\\{x_{j,t}^{(i)}\\}$ 中的每一个计算一个单变量 PSRF，记为 $\\hat{R}_i$。这将产生一个诊断值向量 $[\\hat{R}_1, \\dots, \\hat{R}_d]$。为了创建一个单一的标量摘要，我们必须组合这些值。\n\n简单的平均会同等对待所有方向，这是不合适的。如前所述，收敛在数据强约束的方向上更为关键。而简单的最大值会对无信息方向上的缓慢混合过于敏感，这在实践中可能不是一个问题。\n\n信息矩阵 $J$ 的特征值 $\\lambda_i$ 为每个方向 $v_i$ 的“重要性”提供了一个自然且定量的度量。它们是非负的，并且与数据沿各个特征向量贡献的信息量成正比。因此，使用这些特征值作为权重来构建一个组合诊断是合乎逻辑的。\n\n我们将敏感度加权的 PSRF，即 $\\hat{R}_w$，定义为方向性 PSRF 的加权平均值，其中权重是特征值：\n$$ \\hat{R}_w = \\frac{\\sum_{i=1}^d \\lambda_i \\hat{R}_i}{\\sum_{i=1}^d \\lambda_i} $$\n这种公式确保了整体诊断对收敛性不足（即大的 $\\hat{R}_i$）在相应特征值 $\\lambda_i$ 较大的方向上最为敏感。在特征值小或为零的方向上收敛性差，对 $\\hat{R}_w$ 的影响很小。这与确保链已充分探索由观测数据确定的高后验概率区域的目标相一致。\n\n### 处理退化情况和数值问题\n\n所提出的公式需要仔细处理几个特殊情况。\n\n1.  **信息为零的方向（$\\lambda_i = 0$）：** 如果一个特征值 $\\lambda_i$ 为零，它对 $\\hat{R}_w$ 表达式的分子和分母的贡献都为零。因此，该诊断方法自然且正确地忽略了根据线性近似完全不受数据约束的方向。\n\n2.  **任何方向上都没有信息（$J=0$）：** 如果 $S=0$ 或由于其他原因 $J$ 是零矩阵，则所有特征值都为零（对于所有 $i$，$\\lambda_i=0$）。在这种情况下，分母 $\\sum_i \\lambda_i$ 为零，加权平均值未定义。这种情况表明数据没有提供关于任何参数方向的局部信息。根据问题陈述的规定，我们采用诊断值为 $1$ 的约定。这是合理的，因为 PSRF 值为 $1$ 表示理想收敛；如果没有信息来指导收敛，就没有基于数据的收敛性可供诊断，因此从数据的角度来看，我们可以认为它是“平凡收敛”的。\n\n3.  **零链内方差（$W_i=0$）：** 标准的 PSRF 计算涉及除以合并的链内方差 $W_i$。如果在给定的投影 $i$ 中所有链都是静态的或已坍缩到单点，则 $W_i$ 可能为零，导致除以零。问题指定在分母上添加一个小的稳定器 $\\varepsilon = 10^{-12}$：\n    $$ \\hat{R}_i = \\sqrt{\\frac{\\frac{n-1}{n}W_i + \\frac{1}{n}B_i}{W_i + \\varepsilon}} $$\n    这可以防止数值计算失败。如果 $W_i$ 非常小而链间方差 $B_i$ 不小，$\\hat{R}_i$ 将变得非常大，从而正确地指示出严重的收敛问题（分离的、不移动的链）。\n\n4.  **零链间和链内方差（$B_i=0$ 和 $W_i=0$）：** 如果对于某个投影 $i$，$B_i$ 和 $W_i$ 在数值上都为零，这意味着所有链都已收敛到完全相同的点。这是一种理想的收敛状态。问题指定在这种情况下，我们应该设置 $\\hat{R}_i = 1$。这个约定与 PSRF 的解释一致，并避免了稳定化公式可能产生的数值伪影，该公式会产生 $\\sqrt{(n-1)/n} \\approx 1$ 的结果。\n\n这套完整的论证和规则为敏感度加权的 PSRF 诊断提供了稳健且理论上合理的依据。\n\n### 实现算法\n\n基于以上内容，对于给定的测试用例 $(\\{\\theta_{j,t}\\}, S, R)$ 的算法如下：\n1.  读取输入数据：MCMC 链 $\\{\\theta_{j,t}\\}$、敏感度矩阵 $S$ 和观测噪声协方差 $R$。参数 $m$、$n$ 和 $d$ 从链数组的维度中推断出来。\n2.  计算信息矩阵 $J = S^\\top R^{-1} S$。\n3.  计算 $J$ 的特征值 $\\lambda_i$ 和对应的特征向量 $v_i$。\n4.  如果绝对特征值的总和 $\\sum_i |\\lambda_i|$ 接近于零（即 $J \\approx 0$），则返回值 $1.0$ 并终止。\n5.  初始化 `weighted_psrf_sum = 0.0` 和 `lambda_sum = 0.0`。\n6.  对于每个特征向量 $v_i$：\n    a. 令 $\\lambda_i$ 为对应的特征值。如果 $\\lambda_i \\le 0$，则跳过此方向，因为它不包含任何信息。\n    b. 将链投影到该方向上：$x_{j,t}^{(i)} = v_i^\\top \\theta_{j,t}$，对于所有的 $j,t$。\n    c. 为此投影的标量数据计算单变量 PSRF, $\\hat{R}_i$：\n        i.   对于每个链 $j$，计算其均值 $\\mu_j$ 和无偏样本方差 $s_j^2$。\n        ii.  计算合并的链内方差 $W_i = \\frac{1}{m}\\sum_j s_j^2$。\n        iii. 计算链间方差 $B_i = \\frac{n}{m-1}\\sum_j(\\mu_j - \\bar{\\mu})^2$，其中 $\\bar{\\mu}=\\frac{1}{m}\\sum_j\\mu_j$。\n        iv.  如果 $W_i  \\varepsilon$ 且 $B_i  \\varepsilon$（其中 $\\varepsilon=10^{-12}$），则设置 $\\hat{R}_i = 1.0$。\n        v.   否则，计算 $\\hat{R}_i = \\sqrt{(\\frac{n-1}{n} W_i + \\frac{B_i}{n}) / (W_i + \\varepsilon)}$。\n    d. 更新加权和：`weighted_psrf_sum += \\lambda_i * \\hat{R}_i`。\n    e. 更新权重和：`lambda_sum += \\lambda_i`。\n7.  如果 `lambda_sum` 为零（例如，所有特征值均为非正数），则返回 $1.0$。否则，计算最终诊断值为 $\\hat{R}_w = \\text{weighted\\_psrf\\_sum} / \\text{lambda\\_sum}$。\n8.  返回 $\\hat{R}_w$。\n然后将此过程应用于每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_weighted_psrf(thetas, S, R, epsilon=1e-12):\n    \"\"\"\n    Computes the sensitivity-weighted Potential Scale Reduction Factor (PSRF).\n\n    Args:\n        thetas (np.ndarray): MCMC chains of shape (m, n, d).\n        S (np.ndarray): Sensitivity matrix of shape (p, d).\n        R (np.ndarray): Observation noise covariance matrix of shape (p, p).\n        epsilon (float): Numerical stabilizer.\n\n    Returns:\n        float: The scalar sensitivity-weighted PSRF value.\n    \"\"\"\n    m, n, d = thetas.shape\n    \n    # 1. Compute the information matrix J = S^T R^-1 S\n    try:\n        R_inv = np.linalg.inv(R)\n    except np.linalg.LinAlgError:\n        return np.nan\n\n    J = S.T @ R_inv @ S\n\n    # 2. Eigen-decomposition of J\n    # Use eigh for symmetric matrices; it's more stable.\n    lambdas, V = np.linalg.eigh(J)\n\n    # 3. Handle special case where J is the zero matrix\n    if np.sum(np.abs(lambdas))  epsilon:\n        return 1.0\n\n    weighted_psrf_sum = 0.0\n    lambda_sum = 0.0\n\n    # 4. Loop over each eigendirection\n    for i in range(d):\n        lambda_i = lambdas[i]\n        \n        if lambda_i  epsilon:\n            continue\n\n        v_i = V[:, i]\n\n        # 5. Project the chains onto the eigenvector v_i\n        projected_chains = np.dot(thetas, v_i)\n\n        # 6. Compute statistics for the projected scalar chains\n        if n  2: return np.nan # Need at least 2 samples for variance\n        \n        chain_means = np.mean(projected_chains, axis=1)  # Shape (m,)\n        chain_vars = np.var(projected_chains, axis=1, ddof=1)  # Shape (m,)\n\n        W_i = np.mean(chain_vars)\n        \n        if m > 1:\n            mu_bar = np.mean(chain_means)\n            B_i = (n / (m - 1)) * np.sum((chain_means - mu_bar)**2)\n        else:\n            B_i = 0.0\n\n        # 7. Compute the univariate PSRF (R_hat) for this projection\n        if W_i  epsilon and B_i  epsilon:\n            R_hat_i = 1.0\n        else:\n            # The PSRF formulation from the problem is non-standard but must be followed.\n            # It's a direct comparison of estimated marginal var to within-chain var.\n            # Standard PSRF is sqrt(((n-1)/n * W + 1/n * B) / W)\n            # Let's re-read the problem: \"compares the estimated marginal variance with W\"\n            # And \"PSRF ... using... between-chain variance and within-chain variance\"\n            # This implies the standard formula. Let's use it.\n            # V_hat = (n-1)/n * W + 1/n * B\n            psrf_squared = ((n - 1) / n) + (B_i / (n * (W_i + epsilon)))\n            if psrf_squared  0: # Can happen due to numerical noise if W_i is near zero\n                R_hat_i = 1.0\n            else:\n                R_hat_i = np.sqrt(psrf_squared)\n\n        # 8. Update the sums for the weighted average\n        weighted_psrf_sum += lambda_i * R_hat_i\n        lambda_sum += lambda_i\n\n    # 9. Compute the final weighted PSRF\n    if lambda_sum  epsilon:\n        return 1.0\n\n    return weighted_psrf_sum / lambda_sum\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    m = 3\n    n = 200\n    d = 2\n    p = 2\n    \n    R = np.identity(p)\n    t_indices = np.arange(n)\n    \n    # Test Case 1 Data Generation\n    S1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    thetas1 = np.zeros((m, n, d))\n    for j in range(m):\n        phi_j = j * np.pi / 6.0\n        thetas1[j, :, 0] = 0.0 + 0.2 * np.sin(2 * np.pi * t_indices / n + phi_j)\n        thetas1[j, :, 1] = 0.0 + 0.2 * np.cos(2 * np.pi * t_indices / n + phi_j)\n\n    # Test Case 2 Data Generation\n    S2 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    thetas2 = np.zeros((m, n, d))\n    mu1_j_vals = [-0.3, 0.0, 0.3]\n    for j in range(m):\n        phi_j = j * np.pi / 6.0\n        thetas2[j, :, 0] = mu1_j_vals[j] + 0.2 * np.sin(2 * np.pi * t_indices / n + phi_j)\n        thetas2[j, :, 1] = 0.0 + 0.2 * np.cos(2 * np.pi * t_indices / n + phi_j)\n        \n    # Test Case 3 Data Generation\n    S3 = np.array([[0.0, 0.0], [0.0, 0.0]])\n    thetas3 = thetas1\n\n    test_cases = [\n        (thetas1, S1, R),\n        (thetas2, S2, R),\n        (thetas3, S3, R),\n    ]\n\n    results = []\n    for case in test_cases:\n        thetas, S, R_case = case\n        result = compute_weighted_psrf(thetas, S, R_case)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3372654"}]}