{"hands_on_practices": [{"introduction": "我们从MCMC中最基础的算法——随机游走Metropolis（RWM）——开始。虽然RWM算法原理简单，但其性能严重依赖于提议分布的步长。过大或过小的步长都会导致采样效率低下。本练习将引导你完成一个经典的推导，以理解在高维高斯问题中如何最优地缩放提议分布的协方差[@problem_id:3415063]。这个过程不仅揭示了著名的约 $0.234$ 目标接受率的理论来源，也是任何MCMC实践者都应掌握的基础技能。", "problem": "考虑一个具有高斯先验和加性高斯观测噪声的线性反问题，其状态向量 $x \\in \\mathbb{R}^{d}$ 的后验分布为高斯分布，均值为 $\\mu$，协方差矩阵为 $C \\in \\mathbb{R}^{d \\times d}$，即后验密度为 $\\pi(x) = \\mathcal{N}(\\mu, C)$。您希望使用马尔可夫链蒙特卡洛（MCMC）方法，特别是随机游走 Metropolis (RWM) 算法，从 $\\pi(x)$ 中进行采样，其对称高斯提议分布的形式为\n$$\nq(x' \\mid x) = \\mathcal{N}\\big(x, \\Sigma_{\\text{prop}}\\big), \\quad \\Sigma_{\\text{prop}} = s^{2} C,\n$$\n其中 $s  0$ 是一个标量步长，$C$ 作为预条件子，旨在匹配后验协方差结构。\n\n使用白化坐标变换 $u = C^{-1/2}(x - \\mu)$，使目标分布变为标准高斯分布 $\\mathcal{N}(0, I_{d})$，则白化坐标下的提议为 $u' = u + s z$，其中 $z \\sim \\mathcal{N}(0, I_{d})$。在高维情况下，为避免接受概率趋于零或爆炸，通常考虑缩放 $s = \\ell / \\sqrt{d}$，其中 $\\ell  0$ 是一个无量纲的步长常数。\n\n从随机游走 Metropolis 接受概率的基本定义和适用于大 $d$ 情况的渐近分布近似出发，推导当 $d$ 很大时接受概率关于 $\\ell$ 的近似函数，然后基于白化坐标中的期望平方跳跃距离（ESJD）定义一个效率度量。确定使此效率最大化的 $\\ell$ 值，从而获得原始坐标中作为 $d$ 和 $C$ 函数的最优提议协方差 $\\Sigma_{\\text{prop}}^{\\star}$。\n\n将 $\\Sigma_{\\text{prop}}^{\\star}$ 的最终答案表示为关于 $d$ 和 $C$ 的单个闭式表达式，其中最优步长常数 $\\ell^{\\star}$ 四舍五入到三位有效数字。最终表达式无需单位。", "solution": "我们首先回顾对称提议分布下的随机游走 Metropolis (RWM) 接受概率。给定当前状态 $x$ 和提议状态 $x'$，接受概率为\n$$\n\\alpha(x, x') = \\min\\Big(1, \\frac{\\pi(x')}{\\pi(x)}\\Big).\n$$\n对于高斯目标分布 $\\pi(x) = \\mathcal{N}(\\mu, C)$，在白化坐标 $u = C^{-1/2}(x - \\mu)$ 中处理会很方便，这样目标密度变为 $\\pi(u) \\propto \\exp\\big(-\\|u\\|^{2}/2\\big)$，即标准高斯分布 $\\mathcal{N}(0, I_{d})$。提议取为\n$$\nu' = u + s z, \\quad z \\sim \\mathcal{N}(0, I_{d}).\n$$\n为了在维度 $d$ 增长时获得接受概率的非退化极限，我们采用如下缩放\n$$\ns = \\frac{\\ell}{\\sqrt{d}},\n$$\n其中 $\\ell  0$ 是一个待定的无量纲步长常数。\n\n白化坐标中的接受率为\n$$\nr(u, u') = \\frac{\\pi(u')}{\\pi(u)} = \\exp\\Big(-\\frac{1}{2}\\big(\\|u'\\|^{2} - \\|u\\|^{2}\\big)\\Big).\n$$\n定义提议增量 $\\delta = u' - u = s z = \\frac{\\ell}{\\sqrt{d}} z$。那么\n$$\n\\|u'\\|^{2} - \\|u\\|^{2} = \\|u + \\delta\\|^{2} - \\|u\\|^{2} = 2 u \\cdot \\delta + \\|\\delta\\|^{2},\n$$\n因此，对数接受率为\n$$\nA = \\ln r(u, u') = - u \\cdot \\delta - \\frac{1}{2} \\|\\delta\\|^{2} = - \\frac{\\ell}{\\sqrt{d}} u \\cdot z - \\frac{1}{2} \\frac{\\ell^{2}}{d} \\|z\\|^{2}.\n$$\n我们在 $d \\to \\infty$ 的极限下分析 $A$。由于 $u \\sim \\mathcal{N}(0, I_{d})$ 和 $z \\sim \\mathcal{N}(0, I_{d})$ 是独立的，我们考虑随机和\n$$\n\\frac{1}{\\sqrt{d}} u \\cdot z = \\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} u_{i} z_{i}.\n$$\n根据独立性和标准高斯性质，项 $u_{i} z_{i}$ 是独立的、均值为零且方差为 $1$。根据中心极限定理 (CLT)，我们有\n$$\n\\frac{1}{\\sqrt{d}} u \\cdot z \\Rightarrow W \\sim \\mathcal{N}(0, 1),\n$$\n当 $d \\to \\infty$ 时。此外，根据大数定律，当 $d \\to \\infty$ 时，$\\|z\\|^{2}/d \\to 1$ 几乎必然成立。因此，在大 $d$ 极限下，对数接受率在分布上收敛于\n$$\nA \\Rightarrow - \\ell W - \\frac{\\ell^{2}}{2}, \\quad W \\sim \\mathcal{N}(0, 1).\n$$\n接受概率在 $u$ 的平稳分布和提议 $z$ 上取平均后，收敛于\n$$\n\\alpha(\\ell) = \\mathbb{E}\\big[ \\min\\big(1, \\exp(A)\\big) \\big] = \\mathbb{E}\\big[ \\min\\big(1, \\exp(- \\ell W - \\frac{\\ell^{2}}{2})\\big) \\big],\n$$\n其中 $W \\sim \\mathcal{N}(0, 1)$。我们显式地计算这个期望。令 $\\phi(w) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-w^{2}/2)$ 表示标准正态概率密度函数 (PDF)，$\\Phi$ 表示其累积分布函数 (CDF)。注意到\n$$\n\\min\\big(1, \\exp(- \\ell W - \\frac{\\ell^{2}}{2})\\big) =\n\\begin{cases}\n1,  \\text{若 } - \\ell W - \\frac{\\ell^{2}}{2} \\ge 0 \\iff W \\le - \\frac{\\ell}{2}, \\\\\n\\exp(- \\ell W - \\frac{\\ell^{2}}{2}),  \\text{若 } W  - \\frac{\\ell}{2}.\n\\end{cases}\n$$\n因此\n$$\n\\alpha(\\ell) = \\int_{-\\infty}^{- \\ell/2} \\phi(w) \\, dw + \\int_{- \\ell/2}^{\\infty} \\exp\\Big(- \\ell w - \\frac{\\ell^{2}}{2}\\Big) \\phi(w) \\, dw.\n$$\n注意到\n$$\n\\exp\\Big(- \\ell w - \\frac{\\ell^{2}}{2}\\Big) \\phi(w) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\Big( - \\frac{w^{2}}{2} - \\ell w - \\frac{\\ell^{2}}{2} \\Big) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\Big( - \\frac{(w + \\ell)^{2}}{2} \\Big) = \\phi(w + \\ell).\n$$\n因此，\n$$\n\\alpha(\\ell) = \\Phi\\Big( - \\frac{\\ell}{2} \\Big) + \\int_{- \\ell/2}^{\\infty} \\phi(w + \\ell) \\, dw = \\Phi\\Big( - \\frac{\\ell}{2} \\Big) + \\int_{\\ell/2}^{\\infty} \\phi(u) \\, du = \\Phi\\Big( - \\frac{\\ell}{2} \\Big) + \\big(1 - \\Phi(\\ell/2)\\big).\n$$\n利用对称性 $\\Phi(-a) = 1 - \\Phi(a)$，我们得到\n$$\n\\alpha(\\ell) = 2 \\Phi\\Big( - \\frac{\\ell}{2} \\Big).\n$$\n\n我们现在基于白化坐标中的期望平方跳跃距离（ESJD）来定义效率度量。平方跳跃长度为 $\\|\\delta\\|^{2} = \\frac{\\ell^{2}}{d} \\|z\\|^{2}$，且对于大 $d$ 有 $\\|z\\|^{2} \\approx d$。因此，白化坐标中的 ESJD 渐近为\n$$\nJ(\\ell) = \\mathbb{E}\\big[ \\|\\delta\\|^{2} \\, \\text{accept} \\big] \\approx \\ell^{2} \\, \\alpha(\\ell) = \\ell^{2} \\cdot 2 \\Phi\\Big( - \\frac{\\ell}{2} \\Big).\n$$\n我们寻求最大化 $J(\\ell)$ 的 $\\ell^{\\star}$。求导：\n$$\n\\alpha(\\ell) = 2 \\Phi\\Big( - \\frac{\\ell}{2} \\Big), \\quad \\alpha'(\\ell) = 2 \\cdot \\Big( - \\frac{1}{2} \\Big) \\phi\\Big( \\frac{\\ell}{2} \\Big) = - \\phi\\Big( \\frac{\\ell}{2} \\Big).\n$$\n因此\n$$\nJ'(\\ell) = 2 \\ell \\alpha(\\ell) + \\ell^{2} \\alpha'(\\ell) = 2 \\ell \\cdot 2 \\Phi\\Big( - \\frac{\\ell}{2} \\Big) - \\ell^{2} \\phi\\Big( \\frac{\\ell}{2} \\Big).\n$$\n令 $J'(\\ell) = 0$ 并除以 $\\ell  0$，得到一阶条件\n$$\n4 \\Phi\\Big( - \\frac{\\ell}{2} \\Big) = \\ell \\, \\phi\\Big( \\frac{\\ell}{2} \\Big).\n$$\n这个超越方程存在唯一的正解 $\\ell^{\\star}$，我们通过数值方法确定它。对\n$$\nf(\\ell) = \\ell \\, \\phi\\Big( \\frac{\\ell}{2} \\Big) - 4 \\Phi\\Big( - \\frac{\\ell}{2} \\Big)\n$$\n应用标准的求根方法（例如，牛顿法），我们得到\n$$\n\\ell^{\\star} \\approx 2.38,\n$$\n以及对应的最优接受率为\n$$\n\\alpha(\\ell^{\\star}) = 2 \\Phi\\Big( - \\frac{\\ell^{\\star}}{2} \\Big) \\approx 2 \\Phi(-1.19) \\approx 0.234,\n$$\n这是高维情况下 RWM 的一个众所周知的近最优接受率。\n\n映射回原始坐标，最优缩放为 $s^{\\star} = \\ell^{\\star}/\\sqrt{d}$，最优提议协方差为\n$$\n\\Sigma_{\\text{prop}}^{\\star} = (s^{\\star})^{2} C = \\frac{\\ell^{\\star 2}}{d} \\, C.\n$$\n将 $\\ell^{\\star}$ 四舍五入到三位有效数字，最优协方差为\n$$\n\\Sigma_{\\text{prop}}^{\\star} = \\frac{(2.38)^{2}}{d} \\, C.\n$$\n在最大化针对协方差为 $C$ 的 $d$ 维高斯后验分布的随机游走 Metropolis 算法的渐近 ESJD 的意义上，这个选择实现了近最优效率。", "answer": "$$\\boxed{\\frac{(2.38)^{2}}{d}\\,C}$$", "id": "3415063"}, {"introduction": "简单随机游走并非总是最高效的。为了更有效地探索参数空间，我们可以利用目标分布的梯度信息，例如在Metropolis调整的朗之万算法（MALA）中。然而，这种统计效率的提升通常伴随着计算成本的增加，尤其是在由偏微分方程（PDE）约束的复杂模型中。本练习要求你分析和量化这种权衡[@problem_id:3415120]，这是在科学计算中设计实用MCMC采样器时的一个关键考量。", "problem": "考虑一个受偏微分方程（PDE）约束的贝叶斯逆问题，其中未知参数场 $m$ 通过状态方程 $A(m) u = f$ 进入一个由算子 $A(m)$ 定义的正向模型，该方程在一个有界域上成立，并带有适当的边界条件。观测值由 $d = S u + \\varepsilon$ 给出，其中 $S$ 是一个线性观测算子，$\\varepsilon$ 是加性观测噪声，建模为均值为零、协方差矩阵为 $\\Gamma$ 的高斯噪声。关于 $m$ 的对数后验密度由对数先验和从高斯似然导出的数据失配项之和定义，因此，在给定的 $m$ 处评估对数后验需要求解正向PDE以获得 $u$。\n\n在马尔可夫链蒙特卡洛（MCMC）方法中，考虑使用两种提议机制在参数空间中生成新状态：\n\n- 随机游走提议，使用对称高斯增量，不使用梯度。\n- Metropolis调整的Langevin算法（MALA; Metropolis Adjusted Langevin Algorithm），它使用对数后验关于 $m$ 的梯度来构建一个漂移项。\n\n在PDE约束的设置中，为计算对数后验关于 $m$ 的梯度，假设采用标准的伴随状态法：给定当前参数 $m$ 处的正向解 $u$，求解伴随PDE以获得伴随状态 $p$，然后由 $p$ 组装出 $\\nabla \\log \\pi(m \\mid d)$。假设采用以下成本模型：\n\n- 单次正向PDE求解的计算成本为 $C_f$。\n- 单次伴随PDE求解的计算成本为 $C_a$。\n- 与PDE求解相比，所有其他操作（组装、向量代数、从高斯分布中抽样以及先验评估）的成本可以忽略不计。\n\n假设采用以下缓存策略和接受计算，与Metropolis-Hastings算法一致：\n\n- 在每次迭代中，当前参数 $m$ 处的正向解 $u$ 和对数后验值已从上一个接受的步骤中获得。\n- 对于随机游走提议，生成提议 $y$ 不需要梯度评估，接受概率取决于在 $y$ 处评估对数后验，这需要一次正向求解以获得 $u(y)$。\n- 对于步长为 $h$ 的MALA提议，提议按 $y = x + \\frac{h}{2} \\nabla \\log \\pi(x \\mid d) + \\sqrt{h} \\, \\eta$ 抽取，其中 $\\eta$ 是参数空间中的标准高斯变量。Metropolis-Hastings接受率涉及 $\\log \\pi(y \\mid d)$ 和反向提议密度 $q(x \\mid y)$，后者需要计算 $\\nabla \\log \\pi(y \\mid d)$，因此需要在 $y$ 处进行一次伴随求解。在当前状态计算 $\\nabla \\log \\pi(x \\mid d)$ 需要在给定缓存的正向解的情况下，在 $x$ 处进行一次伴随求解。\n\n在这些假设下，推导MALA的每个提议的计算成本与随机游走提议的每个提议的计算成本之比的闭式表达式，该表达式仅用 $C_f$ 和 $C_a$ 表示。请以单个解析表达式的形式给出最终答案。不需要四舍五入，最终答案中也不需要单位。", "solution": "问题陈述已经过验证，被认为是合理的。它在科学上基于计算统计学和逆问题的理论，问题设定良好，目标明确，信息充分，并使用客观、正式的语言。我们可以开始求解。\n\n目标是确定在PDE约束的贝叶斯逆问题背景下，Metropolis-Adjusted Langevin Algorithm（MALA）与标准随机游走（RW）Metropolis算法的每个提议的计算成本之比。成本需要用单次正向PDE求解的成本（记为 $C_f$）和单次伴随PDE求解的成本（记为 $C_a$）来表示。所有其他计算成本均假定可以忽略不计。\n\n首先，我们分析随机游走Metropolis算法每个提议的计算成本。设马尔可夫链的当前状态为 $m$。问题陈述指出，正向解 $u(m)$ 和对数后验 $\\log \\pi(m \\mid d)$ 在上一个接受的步骤中已经可用，因此是缓存的。\n1.  从一个对称分布生成一个提议 $y$，例如 $y = m + \\eta$，其中 $\\eta$ 是从一个零均值高斯分布中抽取的样本。根据问题陈述，此步骤的成本可以忽略不计。\n2.  为了计算Metropolis-Hastings接受概率 $\\alpha = \\min\\left(1, \\frac{\\pi(y \\mid d)}{\\pi(m \\mid d)}\\right)$，我们需要在提议状态 $y$ 处评估后验密度。这涉及到评估 $\\log \\pi(y \\mid d)$。\n3.  问题陈述指出，在给定参数处评估对数后验需要求解正向PDE。因此，为了计算 $\\log \\pi(y \\mid d)$，必须首先求解状态方程 $A(y)u = f$ 以得到状态 $u(y)$。\n4.  这次单次正向PDE求解的成本是 $C_f$。\n5.  所有后续操作（例如，计算数据失配、评估先验、为接受测试抽取随机数）均假定成本可以忽略不计。\n\n因此，随机游走Metropolis算法每个提议的总计算成本（记为 $\\text{Cost}_{\\text{RW}}$）主要由单次正向PDE求解决定。\n$$\n\\text{Cost}_{\\text{RW}} = C_f\n$$\n\n接下来，我们分析Metropolis-Adjusted Langevin Algorithm（MALA）每个提议的计算成本。设链的当前状态为 $x$。和之前一样，正向解 $u(x)$ 和对数后验 $\\log \\pi(x \\mid d)$ 是缓存的。\n1.  使用基于梯度的漂移项生成提议 $y$：$y = x + \\frac{h}{2} \\nabla \\log \\pi(x \\mid d) + \\sqrt{h} \\, \\eta$。为了构建此提议，必须计算当前状态的对数后验梯度 $\\nabla \\log \\pi(x \\mid d)$。\n2.  问题指定使用伴随状态法进行此梯度计算。由于正向解 $u(x)$ 已经可用（已缓存），计算梯度仅需要一次伴随PDE的求解。此操作的成本为 $C_a$。\n3.  计算梯度后，形成提议 $y$。此过程的代数运算成本可以忽略不计。\n4.  MALA的Metropolis-Hastings接受概率为 $\\alpha = \\min\\left(1, \\frac{\\pi(y \\mid d) q(x \\mid y)}{\\pi(x \\mid d) q(y \\mid x)}\\right)$。为了评估此比率，我们需要计算两项：提议处的对数后验 $\\log \\pi(y \\mid d)$ 和反向提议的密度 $q(x \\mid y)$。\n5.  为了评估 $\\log \\pi(y \\mid d)$，我们必须像随机游走情况一样，在 $y$ 处求解正向PDE以获得 $u(y)$。这次正向求解的成本是 $C_f$。\n6.  为了评估反向提议密度 $q(x \\mid y)$，我们必须计算从 $y$ 开始的提议的漂移项。该漂移项依赖于提议状态处的梯度 $\\nabla \\log \\pi(y \\mid d)$。\n7.  为了使用伴随状态法计算 $\\nabla \\log \\pi(y \\mid d)$，我们需要正向解 $u(y)$ 和相应的伴随解 $p(y)$。正向解 $u(y)$ 已在上一步中计算（成本为 $C_f$）。因此，我们现在只需要在状态 $y$ 处求解伴随PDE。第二次伴随求解的成本为 $C_a$。\n\n总结单个MALA提议的主要操作成本：\n- 在当前状态 $x$ 处进行一次伴随求解以计算提议漂移项：$C_a$。\n- 在提议状态 $y$ 处进行一次正向求解以评估后验：$C_f$。\n- 在提议状态 $y$ 处进行一次伴随求解以计算反向提议密度：$C_a$。\n\nMALA算法每个提议的总计算成本（记为 $\\text{Cost}_{\\text{MALA}}$）是这些成本的总和。\n$$\n\\text{Cost}_{\\text{MALA}} = C_a + C_f + C_a = C_f + 2C_a\n$$\n\n最后，我们计算MALA每个提议的计算成本与随机游走提议的计算成本之比。\n$$\n\\text{Ratio} = \\frac{\\text{Cost}_{\\text{MALA}}}{\\text{Cost}_{\\text{RW}}} = \\frac{C_f + 2C_a}{C_f}\n$$\n此表达式可以简化为：\n$$\n\\text{Ratio} = 1 + \\frac{2C_a}{C_f}\n$$\n然而，显示总成本之比的形式更直接。两者是等价的。我们将提供未简化的分数形式。", "answer": "$$\n\\boxed{\\frac{C_f + 2C_a}{C_f}}\n$$", "id": "3415120"}, {"introduction": "对于具有高度结构化特征的问题，例如数据同化中的时间序列分析，通用型提议分布往往效率低下。本高级计算练习将介绍一种强大的策略：构建一个能模仿后验分布自身结构的提议分布。你将为一个隐状态轨迹实现一个块采样器（block sampler），使用无迹卡尔曼平滑器（Unscented Kalman Smoother）来构造一个智能的、感知模型的提议分布[@problem_id:3415098]。这项技术是现代粒子MCMC方法的核心。", "problem": "考虑一个用于数据同化的一维非线性状态空间模型。潜在轨迹表示为 $x_{0:T} = (x_0, x_1, \\dots, x_T)$。潜在动态和观测模型指定如下。\n\n- 潜在动态：$x_{t+1} = f(x_t) + \\varepsilon_t$，其中 $\\varepsilon_t \\sim \\mathcal{N}(0, Q)$ 对 $t$ 独立，且 $x_0 \\sim \\mathcal{N}(m_0, P_0)$。这里 $\\mathcal{N}$ 表示高斯分布。\n- 观测模型：$y_t \\mid x_t \\sim \\text{Laplace}(h(x_t), b)$ 对 $t$ 独立，其概率密度函数为 $p(y_t \\mid x_t) = \\frac{1}{2 b} \\exp\\left(-\\frac{|y_t - h(x_t)|}{b}\\right)$，其中 $b  0$ 是拉普拉斯尺度参数。\n\n你需要使用无迹卡尔曼平滑器（Unscented Kalman Smoother, UKS）为 $x_{0:T}$ 构建一个独立块提议，方法如下。\n\n- 使用无迹卡尔曼滤波器（Unscented Kalman Filter, UKF），其中观测噪声由一个高斯分布近似，其方差与拉普拉斯方差相匹配。尺度为 $b$ 的拉普拉斯分布的方差为 $2 b^2$，因此在 UKF 的量测更新中使用高斯近似 $\\mathcal{N}(0, R)$，其中 $R = 2 b^2$。\n- 使用带参数 $\\alpha$、$\\beta$ 和 $\\kappa$ 的标准一维无迹变换（Unscented Transform）来计算 sigma 点、它们的权重以及传播后的均值和协方差。\n- 在无迹设置下执行 Rauch–Tung–Striebel 平滑步骤，以获得后向条件分布 $q(x_T \\mid y_{0:T}) = \\mathcal{N}(m_{T\\mid T}, P_{T\\mid T})$ 以及对于 $t = 0, \\dots, T-1$ 的条件密度 $q(x_t \\mid x_{t+1}, y_{0:T}) = \\mathcal{N}(\\mu_t + J_t (x_{t+1} - m_{t+1\\mid t}), S_t)$，其中 $J_t$ 是平滑增益，由时间 $t$ 的滤波协方差、时间 $t+1$ 的一步预测协方差及其互协方差计算得出。后向模拟提议 $q(x_{0:T} \\mid y_{0:T})$ 可分解为 $q(x_T \\mid y_{0:T}) \\prod_{t=0}^{T-1} q(x_t \\mid x_{t+1}, y_{0:T})$。\n\n使用此提议，考虑对整个块 $x_{0:T}$ 进行一次独立 Metropolis-Hastings 移动，其目标分布在非高斯似然下与真实后验密度成正比，\n$$\n\\pi(x_{0:T}) \\propto p(x_0) \\prod_{t=0}^{T-1} p(x_{t+1} \\mid x_t) \\prod_{t=0}^T p(y_t \\mid x_t),\n$$\n提议分布为如上所述的 $q(x_{0:T} \\mid y_{0:T})$。对于从 $q(\\cdot \\mid y_{0:T})$ 中提议的样本 $x'_{0:T}$ 和固定的当前轨迹 $x_{0:T}$，其接受概率是源于细致平衡的独立 Metropolis-Hastings 接受概率。\n\n你的程序必须：\n\n- 使用高斯近似 $R = 2 b^2$ 作为量测噪声方差，并使用参数 $\\alpha$、$\\beta$ 和 $\\kappa$，实现一维无迹卡尔曼滤波器和无迹 Rauch–Tung–Striebel 平滑器。\n- 实现后向模拟，使用提供的随机种子以保证可复现性，从 $q(x_{0:T} \\mid y_{0:T})$ 中抽取一个样本 $x'_{0:T}$。\n- 对于任何给定的轨迹 $x_{0:T}$，在真实模型下（使用拉普拉斯似然），计算后验密度 $\\log \\pi(x_{0:T})$ 的自然对数，计算须包含所有归一化常数。\n- 使用上述指定的后向分解，计算提议密度 $\\log q(x_{0:T} \\mid y_{0:T})$ 的自然对数。\n- 使用源于细致平衡和上述密度的精确公式，计算从固定的当前轨迹 $x_{0:T}$ 移动到提议的 $x'_{0:T}$ 的独立 Metropolis-Hastings 接受概率 $a(x_{0:T}, x'_{0:T})$。将接受概率表示为 $[0, 1]$ 内的一个实数。\n\n无需进一步推导，假设以下基本事实成立。\n\n- 用于后验分解的贝叶斯法则，以及关于时间和观测的独立性假设。\n- Metropolis-Hastings 细致平衡条件以及独立提议的接受概率形式。\n- 一维无迹变换，其中 $L = 1$，$\\lambda = \\alpha^2 (L + \\kappa) - L$，sigma 点为 $x^{(0)} = m$, $x^{(1)} = m + \\sqrt{(L + \\lambda) P}$, $x^{(2)} = m - \\sqrt{(L + \\lambda) P}$；均值的权重为 $W_0^{(m)} = \\frac{\\lambda}{L + \\lambda}$，$W_0^{(c)} = \\frac{\\lambda}{L + \\lambda} + (1 - \\alpha^2 + \\beta)$，以及对于 $i \\in \\{1, 2\\}$，$W_i^{(m)} = W_i^{(c)} = \\frac{1}{2(L + \\lambda)}$。\n\n使用以下参数集作为测试套件。在所有情况下，对所有 $t \\in \\{0, \\dots, T\\}$，使用当前轨迹 $x_{t} = 0$。\n\n- 测试 A（正常路径，恒等观测）:\n  - 时间范围：$T = 3$。\n  - 动态：$f(x) = 0.7 x + 0.3 \\sin(x)$，过程方差 $Q = 0.01$。\n  - 观测：$h(x) = x$，拉普拉斯尺度 $b = 0.3$。\n  - 先验：$m_0 = 0$, $P_0 = 1$。\n  - 无迹参数：$\\alpha = 0.8$, $\\beta = 2.0$, $\\kappa = 0$。\n  - 观测值：$y_{0:3} = [0.0, 0.2, -0.1, 0.1]$。\n  - 提议采样的随机种子：$s = 12345$。\n\n- 测试 B（边界情况 $T = 0$）:\n  - 时间范围：$T = 0$。\n  - 动态：$f(x) = 0.7 x + 0.3 \\sin(x)$，过程方差 $Q = 0.01$。\n  - 观测：$h(x) = x$，拉普拉斯尺度 $b = 0.1$。\n  - 先验：$m_0 = 0$, $P_0 = 1$。\n  - 无迹参数：$\\alpha = 0.5$, $\\beta = 2.0$, $\\kappa = 0$。\n  - 观测值：$y_{0} = [0.5]$。\n  - 提议采样的随机种子：$s = 42$。\n\n- 测试 C（非线性观测）:\n  - 时间范围：$T = 5$。\n  - 动态：$f(x) = 0.7 x + 0.3 \\sin(x)$，过程方差 $Q = 0.01$。\n  - 观测：$h(x) = 0.5 x^2$，拉普拉斯尺度 $b = 0.5$。\n  - 先验：$m_0 = 0$, $P_0 = 1$。\n  - 无迹参数：$\\alpha = 0.6$, $\\beta = 2.0$, $\\kappa = 0$。\n  - 观测值：$y_{0:5} = [0.2, 0.1, 0.15, 0.05, 0.0, 0.1]$。\n  - 提议采样的随机种子：$s = 2021$。\n\n对于每个测试，你的程序应生成独立 Metropolis-Hastings 接受概率 $a(x_{0:T}, x'_{0:T})$，结果为一个浮点数，四舍五入到六位小数。最终输出必须是单行，包含按测试 A、测试 B、测试 C 顺序排列的三个接受概率，格式为方括号内的逗号分隔列表，例如 $[a_A, a_B, a_C]$。\n\n不涉及物理单位。所有角度（如适用）均以弧度为单位。所有百分比（如有）必须表示为小数。\n\n你的程序不能要求任何输入，并且必须严格遵循上述输出格式。", "solution": "用户要求为一维非线性状态空间模型构建和评估一个独立 Metropolis-Hastings 提议。这包括实现一个无迹卡尔曼平滑器（UKS）来生成提议分布，从此提议中采样，并计算接受概率。\n\n### **1. 问题验证**\n问题陈述已经过严格验证，并被确定为**有效**。\n\n- **已知条件提取**：所有必要组件均已提供：\n    - 潜在动态函数 $f(x)$ 和观测函数 $h(x)$。\n    - 先验（$p(x_0)$）、过程噪声（$p(x_{t+1}|x_t)$）和观测似然（$p(y_t|x_t)$）的统计模型。\n    - 所有模型参数（$m_0, P_0, Q, b$）。\n    - 无迹变换参数（$\\alpha, \\beta, \\kappa$）。\n    - 针对三个不同测试用例的特定时间范围 $T$ 和观测序列 $y_{0:T}$。\n    - 对于所有 $t \\in \\{0, \\dots, T\\}$，固定的当前状态轨迹 $x_t=0$。\n    - 用于可复现提议采样的随机种子。\n- **验证清单**：\n    - **科学依据**：该问题使用了贝叶斯统计和信号处理中标准的、成熟的方法，包括无迹卡尔曼滤波器/平滑器和 Metropolis-Hastings 算法。使用高斯近似来处理非高斯似然的技术是近似推断中一种常见的启发式方法。\n    - **良态问题**：问题定义清晰，有唯一的可计算解。所有函数、参数和算法都得到了无歧义的指定。\n    - **客观性**：问题以精确的数学和算法术语陈述，没有主观性语言。\n    - **完整性和一致性**：问题是自包含的。数据维度与指定的时间范围一致。例如，对于时间范围 $T$，从 $t=0$到 $t=T$ 共有 $T+1$ 个观测值。\n    - **可行性**：所有指定的参数和函数在数值上都是良态的，所需的计算是可行的。\n\n验证确认了问题的合理性，可以进入解决阶段。\n\n### **2. 基于原理的解决方案设计**\n\n核心任务是为整个状态轨迹 $x_{0:T}$ 计算独立 Metropolis-Hastings (IMH) 更新的接受概率。\n\n**2.1. Metropolis-Hastings 框架**\n目标是从后验分布 $\\pi(x_{0:T} \\mid y_{0:T})$ 中采样，根据贝叶斯定理，该分布与似然和先验的乘积成正比：\n$$\n\\pi(x_{0:T} \\mid y_{0:T}) \\propto p(y_{0:T} \\mid x_{0:T}) p(x_{0:T})\n$$\n鉴于模型的条件独立结构，这可以展开为：\n$$\n\\pi(x_{0:T}) \\propto p(x_0) \\prod_{t=0}^{T-1} p(x_{t+1} \\mid x_t) \\prod_{t=0}^T p(y_t \\mid x_t)\n$$\n其中分量密度为：\n- 先验：$p(x_0) = \\mathcal{N}(x_0; m_0, P_0)$\n- 动态：$p(x_{t+1} \\mid x_t) = \\mathcal{N}(x_{t+1}; f(x_t), Q)$\n- 似然：$p(y_t \\mid x_t) = \\text{Laplace}(y_t; h(x_t), b)$\n\nIMH 算法从一个与当前状态 $x_{0:T}$ 无关的提议分布 $q(x'_{0:T} \\mid y_{0:T})$ 中生成一个候选状态 $x'_{0:T}$。该提议以如下概率被接受：\n$$\na(x_{0:T}, x'_{0:T}) = \\min\\left(1, \\frac{\\pi(x'_{0:T}) q(x_{0:T} \\mid y_{0:T})}{\\pi(x_{0:T}) q(x'_{0:T} \\mid y_{0:T})}\\right)\n$$\n为了使这个比率在数值上稳定，我们计算它的对数：\n$$\n\\log(\\text{ratio}) = \\left(\\log \\pi(x'_{0:T}) - \\log q(x'_{0:T} \\mid y_{0:T})\\right) - \\left(\\log \\pi(x_{0:T}) - \\log q(x_{0:T} \\mid y_{0:T})\\right)\n$$\n\n**2.2. 提议构建：无迹卡尔曼平滑器 (UKS)**\n一个好的提议分布应近似于目标后验。这里，我们使用 UKS 构建一个高斯近似。这涉及两个主要阶段：一个前向滤波过程和一个后向平滑/采样过程。\n\n**近似**：真实的观测模型具有拉普拉斯似然，这是非高斯的。为了使用卡尔曼滤波框架，我们将观测噪声近似为高斯分布 $\\mathcal{N}(0, R)$，选择方差 $R$ 与拉普拉斯分布的方差相匹配，即 $R = 2b^2$。\n\n**阶段 A：前向无迹卡尔曼滤波器 (UKF)**\nUKF 使用无迹变换（UT）将状态分布的均值和协方差通过非线性动态和观测模型进行传播。UT 确定性地在当前均值周围选择一组“sigma 点”，将它们通过非线性函数传播，并计算加权平均以获得新的均值和协方差。\n\n对于每个时间步 $t=0, \\dots, T$：\n1.  **预测（对于 $t  0$）**：上一步的滤波状态 $\\mathcal{N}(m_{t-1|t-1}, P_{t-1|t-1})$ 通过动态函数 $f(x)$ 使用 UT 进行传播。加上过程噪声协方差 $Q$ 后，得到预测的状态分布 $\\mathcal{N}(m_{t|t-1}, P_{t|t-1})$。\n2.  **更新**：预测的状态通过观测函数 $h(x)$ 使用 UT 进行传播，以找到预测观测值 $\\hat{y}_t$ 及其协方差 $P_{yy}$，以及状态-观测互协方差 $P_{xy}$。计算卡尔曼增益 $K_t$，并使用实际观测值 $y_t$ 更新状态均值和协方差，从而产生滤波分布 $\\mathcal{N}(m_{t|t}, P_{t|t})$。\n\n此前向过程提供了一系列滤波分布 $\\{ \\mathcal{N}(m_{t|t}, P_{t|t}) \\}_{t=0}^T$ 和预测分布 $\\{ \\mathcal{N}(m_{t+1|t}, P_{t+1|t}) \\}_{t=0}^{T-1}$。我们还存储了每个预测步骤中的互协方差 $C_t = \\text{Cov}(x_t, f(x_t))$，这对于平滑器至关重要。\n\n**阶段 B：后向模拟**\n提议分布 $q(x_{0:T} \\mid y_{0:T})$ 由基于平滑器的后向分解定义：\n$$\nq(x_{0:T} \\mid y_{0:T}) = q(x_T \\mid y_{0:T}) \\prod_{t=T-1}^{0} q(x_t \\mid x_{t+1}, y_{0:T})\n$$\n各项定义如下：\n- 最终状态从最终滤波分布中抽取：$q(x_T \\mid y_{0:T}) = \\mathcal{N}(x_T; m_{T|T}, P_{T|T})$。\n- 后向条件核也是高斯的，$q(x_t \\mid x_{t+1}, y_{0:T}) = \\mathcal{N}(x_t; \\tilde{m}_t, \\tilde{P}_t)$，其参数源自前向滤波器存储的值（这是一种 Rauch-Tung-Striebel 类型的后向递归）：\n    - **平滑增益**：$J_t = C_t / P_{t+1|t}$\n    - **平滑均值**：$\\tilde{m}_t = m_{t|t} + J_t (x_{t+1} - m_{t+1|t})$\n    - **平滑协方差**：$\\tilde{P}_t = P_{t|t} - J_t C_t^T$\n\n采样轨迹 $x'_{0:T}$ 的过程是：首先抽取 $x'_T \\sim \\mathcal{N}(m_{T|T}, P_{T|T})$，然后从 $t=T-1$ 到 $0$ 进行后向迭代，抽取 $x'_t \\sim \\mathcal{N}(\\tilde{m}_t(x'_{t+1}), \\tilde{P}_t)$。\n\n**2.3. 密度评估**\n为了计算接受概率，我们需要对当前轨迹 $x_{0:T}$（其中 $x_t=0$）和提议的轨迹 $x'_{0:T}$ 评估对数密度 $\\log \\pi(\\cdot)$ 和 $\\log q(\\cdot)$。\n\n- **对数目标密度 $\\log \\pi(x_{0:T})$**：这是真实模型各分量的对数概率密度函数（log-PDFs）之和，包括所有归一化常数：\n  $$\n  \\log \\pi(x_{0:T}) = \\log \\mathcal{N}(x_0; m_0, P_0) + \\sum_{t=0}^{T-1} \\log \\mathcal{N}(x_{t+1}; f(x_t), Q) + \\sum_{t=0}^T \\log \\text{Laplace}(y_t; h(x_t), b)\n  $$\n- **对数提议密度 $\\log q(x_{0:T} \\mid y_{0:T})$**：这是在给定轨迹上评估的高斯后向核的对数概率密度函数之和：\n  $$\n  \\log q(x_{0:T} \\mid y_{0:T}) = \\log \\mathcal{N}(x_T; m_{T|T}, P_{T|T}) + \\sum_{t=T-1}^{0} \\log \\mathcal{N}(x_t; \\tilde{m}_t(x_{t+1}), \\tilde{P}_t)\n  $$\n\n有了这四个对数密度值，接受概率 $a = \\min(1, \\exp(\\log(\\text{ratio})))$ 就很容易计算了。对于 $T=0$ 的情况，过程简化为单个 UKF 更新步骤，没有后向递归。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, laplace\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases and prints the final result.\n    \"\"\"\n\n    def unscented_transform(m, P, func, alpha, beta, kappa, func_args=None):\n        \"\"\"\n        Performs the 1D unscented transform.\n        \"\"\"\n        if func_args is None:\n            func_args = {}\n        \n        L = 1\n        lambda_ = alpha**2 * (L + kappa) - L\n        \n        # Sigma points\n        sqrt_term = np.sqrt((L + lambda_) * P) if P  0 else 0\n        sigmas = np.array([m, m + sqrt_term, m - sqrt_term])\n        \n        # Propagate sigma points\n        y_sigmas = np.array([func(s, **func_args) for s in sigmas])\n        \n        # Weights for mean\n        W_m = np.full(2 * L + 1, 1 / (2 * (L + lambda_)))\n        W_m[0] = lambda_ / (L + lambda_)\n        \n        # Weights for covariance\n        W_c = np.copy(W_m)\n        W_c[0] += (1 - alpha**2 + beta)\n        \n        # Propagated mean and covariance\n        y_hat = np.sum(W_m * y_sigmas)\n        P_yy = np.sum(W_c * (y_sigmas - y_hat)**2)\n        \n        # Cross-covariance\n        P_xy = np.sum(W_c * (sigmas - m) * (y_sigmas - y_hat))\n        \n        return y_hat, P_yy, P_xy\n\n    def run_ukf_smoother(y, f, h, Q, b, m0, P0, alpha, beta, kappa):\n        \"\"\"\n        Runs the Unscented Kalman Filter and stores parameters for the smoother.\n        \"\"\"\n        T = len(y) - 1\n        R = 2 * b**2\n\n        # Storage for filter outputs\n        m_filt = np.zeros(T + 1)\n        P_filt = np.zeros(T + 1)\n        m_pred = np.zeros(T) if T  0 else np.array([])\n        P_pred = np.zeros(T) if T  0 else np.array([])\n        C_cross = np.zeros(T) if T  0 else np.array([])\n\n        m_prior, P_prior = m0, P0\n\n        for t in range(T + 1):\n            # Update step\n            y_pred, P_yy, P_xy = unscented_transform(m_prior, P_prior, h, alpha, beta, kappa)\n            S_t = P_yy + R\n            K_t = P_xy / S_t if S_t != 0 else 0\n            \n            m_filt[t] = m_prior + K_t * (y[t] - y_pred)\n            P_filt[t] = P_prior - K_t * S_t * K_t\n            \n            # Prediction step for next time step (if not last step)\n            if t  T:\n                m_next_pred, P_next_pred_ff, C_t = unscented_transform(\n                    m_filt[t], P_filt[t], f, alpha, beta, kappa)\n                \n                m_pred[t] = m_next_pred\n                P_pred[t] = P_next_pred_ff + Q\n                C_cross[t] = C_t\n                \n                m_prior, P_prior = m_pred[t], P_pred[t]\n\n        return {\n            \"m_filt\": m_filt, \"P_filt\": P_filt,\n            \"m_pred\": m_pred, \"P_pred\": P_pred,\n            \"C_cross\": C_cross, \"T\": T\n        }\n\n    def backward_sample(proposal_params, seed):\n        \"\"\"\n        Draws one sample from the proposal distribution via backward simulation.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        T = proposal_params[\"T\"]\n        m_filt, P_filt = proposal_params[\"m_filt\"], proposal_params[\"P_filt\"]\n        m_pred, P_pred = proposal_params[\"m_pred\"], proposal_params[\"P_pred\"]\n        C_cross = proposal_params[\"C_cross\"]\n\n        x_prime = np.zeros(T + 1)\n        \n        # Sample final state\n        p_filt_T_safe = max(P_filt[T], 1e-12)\n        x_prime[T] = rng.normal(m_filt[T], np.sqrt(p_filt_T_safe))\n\n        # Backward sampling loop\n        for t in range(T - 1, -1, -1):\n            if P_pred[t] == 0:\n                J_t = 0\n            else:\n                J_t = C_cross[t] / P_pred[t]\n\n            mu_t = m_filt[t] + J_t * (x_prime[t+1] - m_pred[t])\n            S_t = P_filt[t] - J_t * C_cross[t]\n            S_t_safe = max(S_t, 1e-12)\n            x_prime[t] = rng.normal(mu_t, np.sqrt(S_t_safe))\n        \n        return x_prime\n\n    def log_target_pdf(x, y, f, h, Q, b, m0, P0):\n        \"\"\"\n        Computes the log of the true posterior density (up to a constant).\n        \"\"\"\n        T = len(x) - 1\n        log_p = norm.logpdf(x[0], loc=m0, scale=np.sqrt(P0))\n        \n        for t in range(T):\n            f_xt = f(x[t])\n            log_p += norm.logpdf(x[t+1], loc=f_xt, scale=np.sqrt(Q))\n        \n        for t in range(T + 1):\n            h_xt = h(x[t])\n            log_p += laplace.logpdf(y[t], loc=h_xt, scale=b)\n            \n        return log_p\n\n    def log_proposal_pdf(x, proposal_params):\n        \"\"\"\n        Computes the log of the proposal density.\n        \"\"\"\n        T = proposal_params[\"T\"]\n        m_filt, P_filt = proposal_params[\"m_filt\"], proposal_params[\"P_filt\"]\n        m_pred, P_pred = proposal_params[\"m_pred\"], proposal_params[\"P_pred\"]\n        C_cross = proposal_params[\"C_cross\"]\n        \n        p_filt_T_safe = max(P_filt[T], 1e-12)\n        log_q = norm.logpdf(x[T], loc=m_filt[T], scale=np.sqrt(p_filt_T_safe))\n\n        for t in range(T - 1, -1, -1):\n            if P_pred[t] == 0:\n                J_t = 0\n            else:\n                J_t = C_cross[t] / P_pred[t]\n            mu_t = m_filt[t] + J_t * (x[t+1] - m_pred[t])\n            S_t = P_filt[t] - J_t * C_cross[t]\n            S_t_safe = max(S_t, 1e-12)\n            log_q += norm.logpdf(x[t], loc=mu_t, scale=np.sqrt(S_t_safe))\n            \n        return log_q\n\n    def calculate_acceptance_prob(params):\n        \"\"\"\n        Orchestrates the calculation for a single test case.\n        \"\"\"\n        T = params[\"T\"]\n        x_current = np.zeros(T + 1)\n        \n        f = lambda x: params[\"f_params\"][\"a\"] * x + params[\"f_params\"][\"b\"] * np.sin(x)\n        h = params[\"h_func\"]\n\n        # 1. Run UKS to get proposal parameters\n        proposal_params = run_ukf_smoother(\n            params[\"y\"], f, h, params[\"Q\"], params[\"b\"],\n            params[\"m0\"], params[\"P0\"], params[\"alpha\"], params[\"beta\"], params[\"kappa\"]\n        )\n\n        # 2. Draw a sample from the proposal\n        x_proposed = backward_sample(proposal_params, params[\"seed\"])\n\n        # 3. Compute log target densities\n        log_pi_current = log_target_pdf(\n            x_current, params[\"y\"], f, h, params[\"Q\"], params[\"b\"], params[\"m0\"], params[\"P0\"]\n        )\n        log_pi_proposed = log_target_pdf(\n            x_proposed, params[\"y\"], f, h, params[\"Q\"], params[\"b\"], params[\"m0\"], params[\"P0\"]\n        )\n\n        # 4. Compute log proposal densities\n        log_q_current = log_proposal_pdf(x_current, proposal_params)\n        log_q_proposed = log_proposal_pdf(x_proposed, proposal_params)\n        \n        # 5. Compute acceptance probability\n        log_ratio = (log_pi_proposed - log_q_proposed) - (log_pi_current - log_q_current)\n        \n        acceptance_prob = min(1.0, np.exp(log_ratio))\n        return acceptance_prob\n\n    f_params = {\"a\": 0.7, \"b\": 0.3}\n\n    test_cases = [\n        # Test A\n        {\n            \"T\": 3, \"Q\": 0.01, \"h_func\": lambda x: x, \"b\": 0.3,\n            \"m0\": 0.0, \"P0\": 1.0, \"alpha\": 0.8, \"beta\": 2.0, \"kappa\": 0.0,\n            \"y\": np.array([0.0, 0.2, -0.1, 0.1]), \"seed\": 12345, \"f_params\": f_params\n        },\n        # Test B\n        {\n            \"T\": 0, \"Q\": 0.01, \"h_func\": lambda x: x, \"b\": 0.1,\n            \"m0\": 0.0, \"P0\": 1.0, \"alpha\": 0.5, \"beta\": 2.0, \"kappa\": 0.0,\n            \"y\": np.array([0.5]), \"seed\": 42, \"f_params\": f_params\n        },\n        # Test C\n        {\n            \"T\": 5, \"Q\": 0.01, \"h_func\": lambda x: 0.5 * x**2, \"b\": 0.5,\n            \"m0\": 0.0, \"P0\": 1.0, \"alpha\": 0.6, \"beta\": 2.0, \"kappa\": 0.0,\n            \"y\": np.array([0.2, 0.1, 0.15, 0.05, 0.0, 0.1]), \"seed\": 2021, \"f_params\": f_params\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        prob = calculate_acceptance_prob(case)\n        results.append(f\"{prob:.6f}\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3415098"}]}