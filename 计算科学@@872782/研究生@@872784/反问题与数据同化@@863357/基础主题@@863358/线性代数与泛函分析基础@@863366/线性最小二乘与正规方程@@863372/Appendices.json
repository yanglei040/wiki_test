{"hands_on_practices": [{"introduction": "线性最小二乘的核心几何意义在于将数据向量$b$投影到由观测算符$A$的列向量张成的子空间上。这个练习旨在夯实这一基本理解，通过明确计算投影算子并确定数据向量$b$在何种条件下完全位于$A$的列空间中，从而使得最小二乘残差为零。这不仅是一个计算练习，更是对最小二乘解存在性和唯一性根本条件的深入探讨[@problem_id:3398138]。", "problem": "考虑一个线性逆问题，其中观测算子由一个实矩阵 $A \\in \\mathbb{R}^{4 \\times 2}$ 表示，数据向量为 $b(\\alpha) \\in \\mathbb{R}^{4}$。设 $A$ 由以下给出的两列 $c_{1}$ 和 $c_{2}$ 构成\n$$\nc_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}, \\qquad c_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix},\n$$\n即\n$$\nA = \\begin{pmatrix}\n1   1 \\\\\n1   0 \\\\\n0   1 \\\\\n1   -1\n\\end{pmatrix}.\n$$\n设数据向量依赖于实数参数 $\\alpha \\in \\mathbb{R}$，形式如下\n$$\nb(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}.\n$$\n在线性最小二乘 (LS) 意义下，残差定义为 $r(\\alpha) = b(\\alpha) - A x(\\alpha)$，其中 $x(\\alpha) \\in \\mathbb{R}^{2}$ 的选择是为了最小化欧几里得范数 $ \\| r(\\alpha) \\|_{2}$。记 $P$ 为到 $A$ 的列空间 $\\mathcal{R}(A)$ 上的正交投影算子，记 $I$ 为 $\\mathbb{R}^{4}$ 上的单位矩阵。你的任务是：\n- 仅使用线性最小二乘和正规方程的基本定义，确定是否存在 $\\alpha$ 值使得 LS 残差可以为零。等价地，确定对于哪些 $\\alpha$，我们有 $b(\\alpha) \\in \\mathcal{R}(A)$。\n- 计算正交投影 $P\\,b(\\alpha)$ 和正交补 $(I-P)\\,b(\\alpha)$，将它们明确地表示为 $\\alpha$ 的函数。\n- 最后，报告使得 LS 残差为零的唯一实数值 $\\alpha$（如果存在）。\n\n最终答案必须是 $\\alpha$ 的单个值。不需要四舍五入，也不涉及单位。", "solution": "**问题验证**\n\n**第一步：提取已知条件**\n- 观测算子是一个实矩阵 $A \\in \\mathbb{R}^{4 \\times 2}$。\n- $A$ 的列是 $c_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$ 和 $c_{2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$。\n- 矩阵 $A$ 由 $A = \\begin{pmatrix} 1   1 \\\\ 1   0 \\\\ 0   1 \\\\ 1   -1 \\end{pmatrix}$ 给出。\n- 数据向量为 $b(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}$，其中 $\\alpha \\in \\mathbb{R}$。\n- 残差定义为 $r(\\alpha) = b(\\alpha) - A x(\\alpha)$，其中 $x(\\alpha) \\in \\mathbb{R}^{2}$ 的选择是为了最小化欧几里得范数 $\\| r(\\alpha) \\|_{2}$。\n- $P$ 是到 $A$ 的列空间 $\\mathcal{R}(A)$ 上的正交投影算子。\n- $I$ 是 $\\mathbb{R}^{4}$ 上的单位矩阵。\n\n**第二步：使用提取的已知条件进行验证**\n该问题具有科学依据，是线性代数中关于线性最小二乘的标准问题。问题是适定的，提供了所有必要信息（矩阵 $A$ 和参数化向量 $b(\\alpha)$）以确定所求量的唯一解。问题以客观和精确的数学语言表述。$A$ 的列是线性无关的，因为其中一列不是另一列的标量倍，这确保了矩阵 $A^T A$ 是可逆的，并且对于任何给定的 $b(\\alpha)$，都存在唯一的最小二乘解。该问题是完全自洽的，并且可以解析求解。\n\n**第三步：结论和行动**\n问题有效。将提供完整解答。\n\n**解题过程**\n\n该问题要求参数 $\\alpha$ 的值，使得线性系统 $A x = b(\\alpha)$ 有精确解，这等价于最小二乘残差为零。这种情况发生当且仅当数据向量 $b(\\alpha)$ 位于矩阵 $A$ 的列空间中，记为 $\\mathcal{R}(A)$。列空间 $\\mathcal{R}(A)$ 是 $A$ 的所有列向量的线性组合的集合。\n\n设 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$。条件 $b(\\alpha) \\in \\mathcal{R}(A)$ 意味着存在标量 $x_1$ 和 $x_2$ 使得 $x_1 c_1 + x_2 c_2 = b(\\alpha)$。这可以转化为以下线性方程组：\n$$\nx_1 \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 1 \\end{pmatrix} + x_2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix}\n$$\n这个向量方程等价于一个包含四个标量方程的方程组：\n$1.$ $x_1 + x_2 = 2$\n$2.$ $x_1 + 0 \\cdot x_2 = 1 \\implies x_1 = 1$\n$3.$ $0 \\cdot x_1 + x_2 = 1 \\implies x_2 = 1$\n$4.$ $x_1 - x_2 = \\alpha$\n\n从第二个和第三个方程，我们立即得出 $x_1 = 1$ 和 $x_2 = 1$。我们必须检查这些值是否与第一个方程相符。代入第一个方程得到 $1 + 1 = 2$，这是成立的。因此，前三个方程是相容的，并唯一确定了 $x_1=1$ 和 $x_2=1$。\n为了使整个系统相容，这些值也必须满足第四个方程。将 $x_1 = 1$ 和 $x_2 = 1$ 代入第四个方程，得到：\n$$\n1 - 1 = \\alpha \\implies \\alpha = 0\n$$\n因此，向量 $b(\\alpha)$ 属于 $A$ 的列空间当且仅当 $\\alpha = 0$。这是使最小二乘残差为零的唯一值。\n\n任务的第二部分是计算正交投影 $P\\,b(\\alpha)$ 及其补 $(I-P)\\,b(\\alpha)$。到列空间 $\\mathcal{R}(A)$ 的正交投影算子由公式 $P = A(A^T A)^{-1}A^T$ 给出。首先，我们计算矩阵 $A^T A$：\n$$\nA^T A = \\begin{pmatrix} 1   1   0   1 \\\\ 1   0   1   -1 \\end{pmatrix} \\begin{pmatrix} 1   1 \\\\ 1   0 \\\\ 0   1 \\\\ 1   -1 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1+1\\cdot1+0\\cdot0+1\\cdot1   1\\cdot1+1\\cdot0+0\\cdot1+1\\cdot(-1) \\\\ 1\\cdot1+0\\cdot1+1\\cdot0+(-1)\\cdot1   1\\cdot1+0\\cdot0+1\\cdot1+(-1)\\cdot(-1) \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 3   0 \\\\ 0   3 \\end{pmatrix} = 3I_2\n$$\n其中 $I_2$ 是 $2 \\times 2$ 的单位矩阵。$A^T A$ 是一个对角矩阵这一事实证实了 $A$ 的列是正交的。其逆矩阵很容易求得：\n$$\n(A^T A)^{-1} = \\frac{1}{3} I_2 = \\begin{pmatrix} \\frac{1}{3}   0 \\\\ 0   \\frac{1}{3} \\end{pmatrix}\n$$\n$b(\\alpha)$ 在 $\\mathcal{R}(A)$ 上的投影是向量 $A x(\\alpha)$，其中 $x(\\alpha)$ 是正规方程 $A^T A x(\\alpha) = A^T b(\\alpha)$ 的解。\n我们来求解 $x(\\alpha)$：\n$$\n3I_2 x(\\alpha) = A^T b(\\alpha) \\implies x(\\alpha) = \\frac{1}{3} A^T b(\\alpha)\n$$\n我们计算 $A^T b(\\alpha)$：\n$$\nA^T b(\\alpha) = \\begin{pmatrix} 1   1   0   1 \\\\ 1   0   1   -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix} = \\begin{pmatrix} 1\\cdot2+1\\cdot1+0\\cdot1+1\\cdot\\alpha \\\\ 1\\cdot2+0\\cdot1+1\\cdot1+(-1)\\cdot\\alpha \\end{pmatrix} = \\begin{pmatrix} 3+\\alpha \\\\ 3-\\alpha \\end{pmatrix}\n$$\n所以，最小二乘解向量为：\n$$\nx(\\alpha) = \\frac{1}{3} \\begin{pmatrix} 3+\\alpha \\\\ 3-\\alpha \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\end{pmatrix}\n$$\n现在我们可以计算投影 $P\\,b(\\alpha) = A x(\\alpha)$：\n$$\nP\\,b(\\alpha) = \\begin{pmatrix} 1   1 \\\\ 1   0 \\\\ 0   1 \\\\ 1   -1 \\end{pmatrix} \\begin{pmatrix} 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} (1+\\frac{\\alpha}{3}) + (1-\\frac{\\alpha}{3}) \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ (1+\\frac{\\alpha}{3}) - (1-\\frac{\\alpha}{3}) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ \\frac{2\\alpha}{3} \\end{pmatrix}\n$$\n正交补投影 $(I-P)b(\\alpha)$ 是残差向量 $r(\\alpha)$，由 $r(\\alpha) = b(\\alpha) - P\\,b(\\alpha)$ 给出：\n$$\n(I-P)\\,b(\\alpha) = \\begin{pmatrix} 2 \\\\ 1 \\\\ 1 \\\\ \\alpha \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1+\\frac{\\alpha}{3} \\\\ 1-\\frac{\\alpha}{3} \\\\ \\frac{2\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 2-2 \\\\ 1-(1+\\frac{\\alpha}{3}) \\\\ 1-(1-\\frac{\\alpha}{3}) \\\\ \\alpha - \\frac{2\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\end{pmatrix}\n$$\n当 $(I-P)b(\\alpha)$ 是零向量时，LS 残差为零。\n$$\n\\begin{pmatrix} 0 \\\\ -\\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\\\ \\frac{\\alpha}{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这要求 $-\\frac{\\alpha}{3} = 0$ 和 $\\frac{\\alpha}{3} = 0$，两者都意味着 $\\alpha=0$。这个结果证实了我们最初的发现。\n使得 LS 残差为零的唯一实数值 $\\alpha$ 是 $0$。", "answer": "$$\n\\boxed{0}\n$$", "id": "3398138"}, {"introduction": "在许多科学和工程应用中，待求解的状态变量通常需要满足某些物理定律或平衡条件，这些条件可以表示为线性约束。本练习介绍了一种处理此类约束最小二乘问题的强大方法：零空间法。通过将可行解集参数化，该方法将一个受约束的优化问题巧妙地转化为一个维度更低的无约束最小二乘问题，从而可以使用标准正规方程求解[@problem_id:3398146]。", "problem": "在线性数据同化背景下，考虑一个状态向量 $x \\in \\mathbb{R}^{3}$，它表示三个空间位置上的分析增量。一组 $4$ 个线性观测由一个线性算子 $A \\in \\mathbb{R}^{4 \\times 3}$ 和一个数据向量 $b \\in \\mathbb{R}^{4}$ 建模。分析增量需要满足一个由 $C \\in \\mathbb{R}^{1 \\times 3}$ 和 $d \\in \\mathbb{R}^{1}$ 表示的线性平衡约束。目标是找到唯一的向量 $x^{\\star}$，它在满足平衡约束的条件下，最小化观测失配的二范数平方：\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad C x = d.\n$$\n假设所有误差都不相关且方差相等，因此除了二范数外不需要其他加权。矩阵和向量如下：\n$$\nA = \\begin{pmatrix}\n1   0   1 \\\\\n0   1   1 \\\\\n1   1   0 \\\\\n2   1   1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\ 2 \\\\ 1 \\\\ 4\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1   -1   0\n\\end{pmatrix}, \\quad\nd = \\begin{pmatrix}\n1\n\\end{pmatrix}.\n$$\n从第一性原理（约束最小二乘的定义和线性子空间的基本性质）出发，使用 $C$ 的零空间的一组基和一个特解可行向量来参数化由线性约束定义的可行集，并将该约束问题简化为关于降维坐标的无约束最小二乘问题。然后，使用与简化问题相关的正规方程，计算唯一的约束最小化子 $x^{\\star}$。\n\n只报告约束最小二乘解的第二个分量 $x^{\\star}_{2}$。将您的最终答案表示为精确分数。无需单位。", "solution": "问题是求解向量 $x^{\\star} \\in \\mathbb{R}^{3}$，该向量是以下约束线性最小二乘问题的解：\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|A x - b\\|_{2}^{2} \\quad \\text{subject to} \\quad C x = d\n$$\n其中矩阵 $A$、$C$ 和向量 $b$、$d$ 如下所示：\n$$\nA = \\begin{pmatrix}\n1   0   1 \\\\\n0   1   1 \\\\\n1   1   0 \\\\\n2   1   1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\ 2 \\\\ 1 \\\\ 4\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n1   -1   0\n\\end{pmatrix}, \\quad\nd = \\begin{pmatrix}\n1\n\\end{pmatrix}.\n$$\n该问题要求使用基于可行集参数化的求解方法。可行集是满足线性约束 $C x = d$ 的所有向量 $x \\in \\mathbb{R}^{3}$ 的集合。该集合形成一个仿射子空间，可以表示为非齐次方程的一个特解与相应齐次方程的通解之和。\n\n设 $x \\in \\mathbb{R}^{3}$ 是可行集中的一个向量。它可以写成 $x = x_p + x_h$，其中 $x_p$ 是满足 $C x_p = d$ 的任意一个特解，而 $x_h$ 是 $C$ 的零空间中的一个向量，满足 $C x_h = 0$。\n\n首先，我们找一个特解 $x_p$。约束条件是 $x_1 - x_2 = 1$。一个简单的选择是将 $x_1 = 1$ 和 $x_2 = 0$，并将 $x_3$ 任意设为 $0$。\n$$\nx_p = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n我们验证 $C x_p = \\begin{pmatrix} 1   -1   0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1$，与 $d$ 相符。\n\n接下来，我们刻画 $C$ 的零空间，即所有满足 $C x_h = 0$ 的向量 $x_h = \\begin{pmatrix} x_{h1} \\\\ x_{h2} \\\\ x_{h3} \\end{pmatrix}$ 的集合。这给出了方程 $x_{h1} - x_{h2} = 0$，即 $x_{h1} = x_{h2}$。零空间中的向量形式为 $\\begin{pmatrix} s \\\\ s \\\\ t \\end{pmatrix}$，其中 $s, t \\in \\mathbb{R}$ 是任意标量。这个二维零空间的一组基可以由对应于 $(s=1, t=0)$ 和 $(s=0, t=1)$ 的向量构成：\n$$\nz_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad z_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n$C$ 的零空间中的任何向量 $x_h$ 都可以写成这些基向量的线性组合：$x_h = w_1 z_1 + w_2 z_2 = Z w$，其中 $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ 是降维坐标向量，$Z$ 是以这些基向量为列的矩阵：\n$$\nZ = \\begin{pmatrix} 1   0 \\\\ 1   0 \\\\ 0   1 \\end{pmatrix}.\n$$\n因此，任何可行向量 $x$ 都可以被参数化为：\n$$\nx = x_p + Z w = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1   0 \\\\ 1   0 \\\\ 0   1 \\end{pmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}.\n$$\n将此参数化代入目标函数 $\\|A x - b\\|_{2}^{2}$，我们得到一个关于 $w$ 的降维无约束最小二乘问题：\n$$\n\\min_{w \\in \\mathbb{R}^{2}} \\|A (x_p + Z w) - b\\|_{2}^{2} = \\min_{w \\in \\mathbb{R}^{2}} \\|(A Z) w - (b - A x_p)\\|_{2}^{2}.\n$$\n令 $\\hat{A} = A Z$ 和 $\\hat{b} = b - A x_p$。我们计算这些项。\n$$\n\\hat{A} = AZ = \\begin{pmatrix} 1   0   1 \\\\ 0   1   1 \\\\ 1   1   0 \\\\ 2   1   1 \\end{pmatrix} \\begin{pmatrix} 1   0 \\\\ 1   0 \\\\ 0   1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+0(1)+1(0)   1(0)+0(0)+1(1) \\\\ 0(1)+1(1)+1(0)   0(0)+1(0)+1(1) \\\\ 1(1)+1(1)+0(0)   1(0)+1(0)+0(1) \\\\ 2(1)+1(1)+1(0)   2(0)+1(0)+1(1) \\end{pmatrix} = \\begin{pmatrix} 1   1 \\\\ 1   1 \\\\ 2   0 \\\\ 3   1 \\end{pmatrix}.\n$$\n接下来，我们计算 $A x_p$：\n$$\nA x_p = \\begin{pmatrix} 1   0   1 \\\\ 0   1   1 \\\\ 1   1   0 \\\\ 2   1   1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}.\n$$\n现在我们计算 $\\hat{b}$：\n$$\n\\hat{b} = b - A x_p = \\begin{pmatrix} 2 \\\\ 2 \\\\ 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 2 \\end{pmatrix}.\n$$\n无约束问题 $\\min_{w} \\|\\hat{A} w - \\hat{b}\\|_{2}^{2}$ 的解 $w^{\\star}$ 可通过求解正规方程得到：\n$$\n(\\hat{A}^T \\hat{A}) w^{\\star} = \\hat{A}^T \\hat{b}.\n$$\n我们计算矩阵 $\\hat{A}^T \\hat{A}$ 和向量 $\\hat{A}^T \\hat{b}$。\n$$\n\\hat{A}^T \\hat{A} = \\begin{pmatrix} 1   1   2   3 \\\\ 1   1   0   1 \\end{pmatrix} \\begin{pmatrix} 1   1 \\\\ 1   1 \\\\ 2   0 \\\\ 3   1 \\end{pmatrix} = \\begin{pmatrix} 1+1+4+9   1+1+0+3 \\\\ 1+1+0+3   1+1+0+1 \\end{pmatrix} = \\begin{pmatrix} 15   5 \\\\ 5   3 \\end{pmatrix}.\n$$\n$$\n\\hat{A}^T \\hat{b} = \\begin{pmatrix} 1   1   2   3 \\\\ 1   1   0   1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1(1)+1(2)+2(0)+3(2) \\\\ 1(1)+1(2)+0(0)+1(2) \\end{pmatrix} = \\begin{pmatrix} 1+2+0+6 \\\\ 1+2+0+2 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix}.\n$$\n正规方程是关于 $w^{\\star}$ 的一个 $2 \\times 2$ 系统：\n$$\n\\begin{pmatrix} 15   5 \\\\ 5   3 \\end{pmatrix} w^{\\star} = \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix}.\n$$\n为了求解 $w^{\\star}$，我们求矩阵 $\\hat{A}^T \\hat{A}$ 的逆。行列式为 $\\det(\\hat{A}^T \\hat{A}) = (15)(3) - (5)(5) = 45 - 25 = 20$。\n逆矩阵为：\n$$\n(\\hat{A}^T \\hat{A})^{-1} = \\frac{1}{20} \\begin{pmatrix} 3   -5 \\\\ -5   15 \\end{pmatrix}.\n$$\n现在我们可以求解 $w^{\\star}$：\n$$\nw^{\\star} = \\frac{1}{20} \\begin{pmatrix} 3   -5 \\\\ -5   15 \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 5 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 3(9) - 5(5) \\\\ -5(9) + 15(5) \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 27 - 25 \\\\ -45 + 75 \\end{pmatrix} = \\frac{1}{20} \\begin{pmatrix} 2 \\\\ 30 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{20} \\\\ \\frac{30}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\n所以，$w_1^{\\star} = \\frac{1}{10}$ 且 $w_2^{\\star} = \\frac{3}{2}$。\n最后，我们使用最优降维坐标 $w^{\\star}$ 计算约束最小化子 $x^{\\star}$：\n$$\nx^{\\star} = x_p + Z w^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1   0 \\\\ 1   0 \\\\ 0   1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{11}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}.\n$$\n解向量为 $x^{\\star} = \\begin{pmatrix} \\frac{11}{10} \\\\ \\frac{1}{10} \\\\ \\frac{3}{2} \\end{pmatrix}$。问题要求的是该向量的第二个分量 $x^{\\star}_{2}$。\n$$\nx^{\\star}_{2} = \\frac{1}{10}.\n$$", "answer": "$$ \\boxed{\\frac{1}{10}} $$", "id": "3398146"}, {"introduction": "本练习将我们从确定性的最小二乘框架提升到概率性的贝叶斯视角，这在现代数据同化中至关重要。我们将看到，经典的最小二乘代价函数可以被解释为在高斯假设下的负对数后验概率，其最小值对应于最大后验 (MAP) 估计。此练习的核心是探究一个关键的实际问题：当我们对观测误差的统计特性（即协方差矩阵$R$）做出不准确的假设时，如何影响我们对分析结果不确定性的估计[@problem_id:3398168]。", "problem": "考虑一个在变分数据同化背景下的线性高斯逆问题。设未知状态为 $x \\in \\mathbb{R}^{n}$，其高斯先验为 $x \\sim \\mathcal{N}(x_{b}, B)$，其中 $x_{b} \\in \\mathbb{R}^{n}$ 是先验均值，$B \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。观测值 $y \\in \\mathbb{R}^{m}$ 由线性正向算子 $H \\in \\mathbb{R}^{m \\times n}$ 给出，并带有加性高斯噪声，$y = H x + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$ 且 $R_{\\text{true}} \\in \\mathbb{R}^{m \\times m}$ 是对称正定矩阵。一位实践者使用一个错误指定的观测噪声协方差 $R \\in \\mathbb{R}^{m \\times m}$ 进行最大后验（MAP）估计，假设二次代价泛函为\n$$\nJ_{R}(x) = \\frac{1}{2} \\| y - H x \\|_{R^{-1}}^{2} + \\frac{1}{2} \\| x - x_{b} \\|_{B^{-1}}^{2},\n$$\n其中对于对称正定矩阵 $M$，有 $\\| v \\|_{M}^{2} \\equiv v^{\\top} M v$。\n\n从高斯先验和似然的定义以及相关的最小二乘代价 $J_{R}(x)$ 出发，推导在假定 $R$ 下 MAP 估计的正规方程，并推导相应的后验协方差矩阵 $A(R)$，其为 $J_{R}(x)$ 的 Hessian 矩阵的逆。\n\n现在假设真实的观测协方差是假定协方差的标量倍数，$R_{\\text{true}} = \\eta R$，其中 $\\eta  1$，这表示在同化过程中低估了观测噪声。令 $A(R)$ 和 $A(R_{\\text{true}})$ 分别表示使用 $R$ 和 $R_{\\text{true}}$ 时得到的后验协方差矩阵。定义先验预处理的高斯-牛顿矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 为\n$$\nK = B^{1/2} H^{\\top} R^{-1} H B^{1/2},\n$$\n其中 $B^{1/2}$ 是 $B$ 的任意对称矩阵平方根。考虑对错误指定的后验协方差进行形式为 $\\alpha \\, A(R)$ 的标量膨胀，其中 $\\alpha  0$ 的选择旨在通过匹配广义后验体积来校正不确定性的低估，即强制满足\n$$\n\\det\\big( \\alpha \\, A(R) \\big) = \\det\\big( A(R_{\\text{true}}) \\big).\n$$\n用 $K$ 的特征值 $\\{ \\lambda_{i} \\}_{i=1}^{n}$ 和标量 $\\eta$ 以闭式形式表示膨胀因子 $\\alpha$。您的最终答案必须是单一的闭式解析表达式。不需要数值近似。", "solution": "分析始于二次代价泛函 $J_{R}(x)$ 的定义，通过最小化该泛函来找到状态 $x$ 的最大后验（MAP）估计。该代价泛函结合了来自状态的先验分布和观测的信息。其形式如下：\n$$\nJ_{R}(x) = \\frac{1}{2} \\| y - H x \\|_{R^{-1}}^{2} + \\frac{1}{2} \\| x - x_{b} \\|_{B^{-1}}^{2}\n$$\n使用定义 $\\| v \\|_{M}^{2} = v^{\\top} M v$，我们展开代价泛函：\n$$\nJ_{R}(x) = \\frac{1}{2} (y - H x)^{\\top} R^{-1} (y - H x) + \\frac{1}{2} (x - x_{b})^{\\top} B^{-1} (x - x_{b})\n$$\n为了找到 MAP 估计，我们必须找到使 $J_{R}(x)$ 最小化的 $x$ 值。由于 $J_{R}(x)$ 是一个凸函数，其最小值可以通过将其关于 $x$ 的梯度设为零来找到。梯度为：\n$$\n\\nabla_{x} J_{R}(x) = \\frac{d}{dx} \\left[ \\frac{1}{2} (x^{\\top} H^{\\top} R^{-1} H x - 2 y^{\\top} R^{-1} H x) + \\frac{1}{2} (x^{\\top} B^{-1} x - 2 x_{b}^{\\top} B^{-1} x) + \\text{const} \\right]\n$$\n$$\n\\nabla_{x} J_{R}(x) = H^{\\top} R^{-1} H x - H^{\\top} R^{-1} y + B^{-1} x - B^{-1} x_{b}\n$$\n将梯度设为零，$\\nabla_{x} J_{R}(x) = 0$，得到 MAP 估计的正规方程，我们将其表示为 $x_{a}$：\n$$\n(B^{-1} + H^{\\top} R^{-1} H) x_{a} = B^{-1} x_{b} + H^{\\top} R^{-1} y\n$$\n接下来，我们推导后验协方差矩阵 $A(R)$。在高斯假设的背景下，后验分布也是高斯的，其协方差矩阵由负对数后验的 Hessian 矩阵的逆给出，这等价于代价泛函 $J_{R}(x)$ 的 Hessian 矩阵。Hessian 矩阵 $\\mathcal{H}$ 是 $J_{R}(x)$ 关于 $x$ 的二阶导数：\n$$\n\\mathcal{H} = \\nabla_{x}^{2} J_{R}(x) = \\frac{d}{dx^{\\top}} (\\nabla_{x} J_{R}(x)) = B^{-1} + H^{\\top} R^{-1} H\n$$\n后验协方差矩阵 $A(R)$ 是 Hessian 矩阵的逆：\n$$\nA(R) = \\mathcal{H}^{-1} = (B^{-1} + H^{\\top} R^{-1} H)^{-1}\n$$\n问题将先验预处理的高斯-牛顿矩阵定义为 $K = B^{1/2} H^{\\top} R^{-1} H B^{1/2}$，其中 $B^{1/2}$ 是 $B$ 的一个对称矩阵平方根。我们可以用 $K$ 来表示 $A(R)$。让我们处理 Hessian 矩阵 $\\mathcal{H}$：\n$$\n\\mathcal{H} = B^{-1/2} B^{-1/2} + H^{\\top} R^{-1} H = B^{-1/2} (I + B^{1/2} H^{\\top} R^{-1} H B^{1/2}) B^{-1/2}\n$$\n$$\n\\mathcal{H} = B^{-1/2} (I + K) B^{-1/2}\n$$\n取逆以求得 $A(R)$：\n$$\nA(R) = \\mathcal{H}^{-1} = (B^{-1/2} (I + K) B^{-1/2})^{-1} = B^{1/2} (I + K)^{-1} B^{1/2}\n$$\n现在，我们考虑真实观测误差协方差为 $R_{\\text{true}} = \\eta R$ 的情况，其中标量 $\\eta  1$。相应的后验协方差 $A(R_{\\text{true}})$ 是通过在公式中用 $R_{\\text{true}} = \\eta R$ 替换 $R$ 得到的。这会影响项 $H^{\\top} R^{-1} H$，它变为 $H^{\\top} (\\eta R)^{-1} H = \\frac{1}{\\eta} H^{\\top} R^{-1} H$。\n相应的高斯-牛顿矩阵，我们称之为 $K_{\\text{true}}$，是：\n$$\nK_{\\text{true}} = B^{1/2} H^{\\top} R_{\\text{true}}^{-1} H B^{1/2} = B^{1/2} \\left(\\frac{1}{\\eta} H^{\\top} R^{-1} H \\right) B^{1/2} = \\frac{1}{\\eta} K\n$$\n因此，真实的后验协方差矩阵是：\n$$\nA(R_{\\text{true}}) = B^{1/2} (I + K_{\\text{true}})^{-1} B^{1/2} = B^{1/2} \\left(I + \\frac{1}{\\eta} K \\right)^{-1} B^{1/2}\n$$\n问题要求我们找到一个膨胀因子 $\\alpha$，使得膨胀后的错误指定协方差的行列式与真实协方差的行列式相匹配：\n$$\n\\det\\big( \\alpha \\, A(R) \\big) = \\det\\big( A(R_{\\text{true}}) \\big)\n$$\n对于矩阵 $M \\in \\mathbb{R}^{n \\times n}$，使用性质 $\\det(cM) = c^{n} \\det(M)$：\n$$\n\\alpha^{n} \\det(A(R)) = \\det(A(R_{\\text{true}}))\n$$\n求解 $\\alpha^{n}$：\n$$\n\\alpha^{n} = \\frac{\\det(A(R_{\\text{true}}))}{\\det(A(R))}\n$$\n让我们计算行列式。使用性质 $\\det(XYZ) = \\det(X)\\det(Y)\\det(Z)$ 和 $\\det(X^{-1}) = (\\det(X))^{-1}$：\n$$\n\\det(A(R)) = \\det(B^{1/2} (I + K)^{-1} B^{1/2}) = \\det(B^{1/2}) \\det((I + K)^{-1}) \\det(B^{1/2}) = \\det(B) \\frac{1}{\\det(I+K)}\n$$\n$$\n\\det(A(R_{\\text{true}})) = \\det(B^{1/2} \\left(I + \\frac{1}{\\eta} K \\right)^{-1} B^{1/2}) = \\det(B) \\frac{1}{\\det(I + \\frac{1}{\\eta} K)}\n$$\n将这些代入 $\\alpha^{n}$ 的表达式中：\n$$\n\\alpha^{n} = \\frac{\\det(B) / \\det(I + \\frac{1}{\\eta} K)}{\\det(B) / \\det(I+K)} = \\frac{\\det(I+K)}{\\det(I + \\frac{1}{\\eta} K)}\n$$\n一个矩阵的行列式是其特征值的乘积。设 $\\{\\lambda_{i}\\}_{i=1}^{n}$ 是 $K$ 的特征值。\n矩阵 $(I+K)$ 的特征值是 $\\{1+\\lambda_{i}\\}_{i=1}^{n}$。\n矩阵 $(I + \\frac{1}{\\eta} K)$ 的特征值是 $\\{1 + \\frac{\\lambda_{i}}{\\eta}\\}_{i=1}^{n}$。\n因此，我们可以将行列式写成这些特征值的乘积：\n$$\n\\det(I+K) = \\prod_{i=1}^{n} (1+\\lambda_{i})\n$$\n$$\n\\det\\left(I + \\frac{1}{\\eta} K\\right) = \\prod_{i=1}^{n} \\left(1+\\frac{\\lambda_{i}}{\\eta}\\right)\n$$\n将这些乘积代入 $\\alpha^{n}$ 的方程中：\n$$\n\\alpha^{n} = \\frac{\\prod_{i=1}^{n} (1+\\lambda_{i})}{\\prod_{i=1}^{n} (1+\\frac{\\lambda_{i}}{\\eta})} = \\prod_{i=1}^{n} \\frac{1+\\lambda_{i}}{1+\\frac{\\lambda_{i}}{\\eta}}\n$$\n简化乘积内部的项：\n$$\n\\frac{1+\\lambda_{i}}{1+\\frac{\\lambda_{i}}{\\eta}} = \\frac{1+\\lambda_{i}}{\\frac{\\eta + \\lambda_{i}}{\\eta}} = \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}}\n$$\n所以，$\\alpha^{n}$ 的表达式变为：\n$$\n\\alpha^{n} = \\prod_{i=1}^{n} \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}}\n$$\n最后，通过取 $n$ 次根求解 $\\alpha$，得到所需的闭式表达式：\n$$\n\\alpha = \\left( \\prod_{i=1}^{n} \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}} \\right)^{\\frac{1}{n}}\n$$", "answer": "$$\n\\boxed{\\left( \\prod_{i=1}^{n} \\frac{\\eta(1+\\lambda_{i})}{\\eta+\\lambda_{i}} \\right)^{\\frac{1}{n}}}\n$$", "id": "3398168"}]}