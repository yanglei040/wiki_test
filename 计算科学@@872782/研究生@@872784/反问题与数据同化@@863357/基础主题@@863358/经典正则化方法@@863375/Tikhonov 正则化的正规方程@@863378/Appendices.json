{"hands_on_practices": [{"introduction": "深入理解吉洪诺夫正则化的第一步是掌握其核心数学结构。本练习引导你从第一性原理出发，为一个具体的逆问题推导其正则化正规方程组，并分析解的唯一性。通过直接求解这个例子，你将巩固对正则化方法基本理论和代数运算的理解。[@problem_id:3405686]", "problem": "考虑一个线性逆模型，其未知状态向量为 $x \\in \\mathbb{R}^{3}$，观测算子为 $A \\in \\mathbb{R}^{2 \\times 3}$，数据为 $b \\in \\mathbb{R}^{2}$。设\n$$\nA \\;=\\; \\begin{pmatrix}\n1  -1  0 \\\\\n0  1  -1\n\\end{pmatrix}, \\qquad\nb \\;=\\; \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix},\n$$\n并假设我们使用一个由下式给出的线性算子 $L \\in \\mathbb{R}^{2 \\times 3}$ 进行正则化\n$$\nL \\;=\\; \\begin{pmatrix}\n1  0  0 \\\\\n0  1  -1\n\\end{pmatrix}.\n$$\n矩阵 $A$ 有一个非平凡零空间，而矩阵 $L$ 惩罚与该零空间互补的方向。对于一个固定的正则化参数 $\\lambda > 0$，考虑 Tikhonov 泛函\n$$\nJ(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda \\,\\|L x\\|_{2}^{2}.\n$$\n从线性代数和微积分的基本原理出发，不援引任何预先给出的最优性公式，执行以下操作：\n- 通过计算梯度 $\\nabla J(x)$ 并令其为零，推导极小化子 $x_{\\lambda}$ 的一阶最优性条件。通过分析 $A$ 和 $L$ 的零空间，论证对于所有 $\\lambda > 0$，所得的线性系统都是非奇异的。\n- 使用该最优性条件，显式求解 $x_{\\lambda}$，得到一个关于 $\\lambda$ 的闭式表达式。\n\n你的最终答案必须是 $x_{\\lambda}$ 的显式表达式，写成单个行向量的形式。无需四舍五入，不涉及单位。请以精确的符号形式表示最终答案。", "solution": "Tikhonov 泛函由下式给出\n$$ J(x) = \\|A x - b\\|_{2}^{2} + \\lambda \\|L x\\|_{2}^{2} $$\n其中 $x \\in \\mathbb{R}^{3}$，$A \\in \\mathbb{R}^{2 \\times 3}$，$b \\in \\mathbb{R}^{2}$，$L \\in \\mathbb{R}^{2 \\times 3}$，且 $\\lambda > 0$。我们已知具体的矩阵和向量为\n$$\nA = \\begin{pmatrix}\n1  -1  0 \\\\\n0  1  -1\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n2 \\\\\n1\n\\end{pmatrix}, \\quad\nL = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  -1\n\\end{pmatrix}.\n$$\n\n为求 $J(x)$ 的极小化子 $x_{\\lambda}$，我们首先推导一阶最优性条件，该条件通过将 $J(x)$ 关于 $x$ 的梯度设为零得到。我们首先使用欧几里得范数的平方的定义 $\\|v\\|_2^2 = v^T v$ 来展开泛函。\n$$ J(x) = (Ax - b)^T (Ax - b) + \\lambda (Lx)^T (Lx) $$\n使用转置的性质 $(MN)^T=N^T M^T$，我们展开各项：\n$$ J(x) = (x^T A^T - b^T)(Ax - b) + \\lambda (x^T L^T L x) $$\n$$ J(x) = x^T A^T A x - x^T A^T b - b^T A x + b^T b + \\lambda x^T L^T L x $$\n由于 $b^T A x$ 是一个标量，它等于其自身的转置 $(b^T A x)^T = x^T A^T b$。因此，我们可以合并线性项：\n$$ J(x) = x^T (A^T A + \\lambda L^T L) x - 2 b^T A x + b^T b $$\n这是关于 $x$ 的一个二次型。为了求梯度 $\\nabla J(x) = \\frac{dJ}{dx}$，我们使用向量微积分中的标准结论：$\\nabla_x (x^T M x) = (M + M^T)x$ 和 $\\nabla_x (c^T x) = c$。\n矩阵 $H = A^T A + \\lambda L^T L$ 是对称的，因为 $(A^T A)^T = A^T (A^T)^T = A^T A$ 且 $(L^T L)^T = L^T (L^T)^T = L^T L$。因此，二次项的梯度是 $2(A^T A + \\lambda L^T L)x$。\n线性项可以写成 $-2(A^T b)^T x$，所以其梯度是 $-2A^T b$。\n项 $b^T b$ 关于 $x$ 是常数，所以其梯度为零。\n综合这些结果，泛函的梯度为：\n$$ \\nabla J(x) = 2(A^T A + \\lambda L^T L)x - 2A^T b $$\n一阶最优性条件是 $\\nabla J(x) = 0$：\n$$ 2(A^T A + \\lambda L^T L)x - 2A^T b = 0 $$\n$$ (A^T A + \\lambda L^T L)x = A^T b $$\n这就是 Tikhonov 正则化问题的正规方程组。\n\n接下来，我们必须证明对于任意 $\\lambda > 0$，系统矩阵 $H(\\lambda) = A^T A + \\lambda L^T L$ 都是非奇异的。一个矩阵是非奇异的，当且仅当其零空间（或核）只包含零向量。设 $v$ 是 $H(\\lambda)$ 零空间中的一个向量，则 $H(\\lambda)v = 0$。\n$$ (A^T A + \\lambda L^T L)v = 0 $$\n从左侧乘以 $v^T$：\n$$ v^T(A^T A + \\lambda L^T L)v = v^T 0 = 0 $$\n$$ v^T A^T A v + \\lambda v^T L^T L v = 0 $$\n$$ (Av)^T(Av) + \\lambda (Lv)^T(Lv) = 0 $$\n$$ \\|Av\\|_2^2 + \\lambda \\|Lv\\|_2^2 = 0 $$\n由于范数是非负的，并且已知 $\\lambda > 0$，这个和为零的唯一可能是两项都各自为零：\n$$ \\|Av\\|_2^2 = 0 \\quad \\text{且} \\quad \\|Lv\\|_2^2 = 0 $$\n这意味着 $Av = 0$ 且 $Lv = 0$。因此，$H(\\lambda)$ 零空间中的向量 $v$ 必须同时位于 $A$ 的零空间和 $L$ 的零空间中。换句话说，$\\ker(H(\\lambda)) = \\ker(A) \\cap \\ker(L)$。\n我们来求这些零空间。\n对于 $\\ker(A)$，我们求解 $Ax=0$，其中 $x = (x_1, x_2, x_3)^T$：\n$$ \\begin{pmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这得到 $x_1 - x_2 = 0 \\implies x_1 = x_2$ 以及 $x_2 - x_3 = 0 \\implies x_2 = x_3$。因此，$x_1=x_2=x_3$。$\\ker(A)$ 中的任意向量形式为 $c(1, 1, 1)^T$，其中 $c$ 为某个标量。所以，$\\ker(A) = \\text{span}\\left\\{(1, 1, 1)^T\\right\\}$。\n\n对于 $\\ker(L)$，我们求解 $Lx=0$：\n$$ \\begin{pmatrix} 1  0  0 \\\\ 0  1  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这得到 $x_1=0$ 以及 $x_2 - x_3 = 0 \\implies x_2=x_3$。$\\ker(L)$ 中的任意向量形式为 $c(0, 1, 1)^T$，其中 $c$ 为某个标量。所以，$\\ker(L) = \\text{span}\\left\\{(0, 1, 1)^T\\right\\}$。\n\n现在我们求交集 $\\ker(A) \\cap \\ker(L)$。交集中的向量 $v$ 必须同时是 $(1, 1, 1)^T$ 的标量倍数和 $(0, 1, 1)^T$ 的标量倍数。设 $v = c_1(1, 1, 1)^T = c_2(0, 1, 1)^T$。比较第一个分量，我们得到 $c_1 \\cdot 1 = c_2 \\cdot 0$，这意味着 $c_1=0$。如果 $c_1=0$，那么 $v = 0 \\cdot (1, 1, 1)^T = (0, 0, 0)^T$。该交集只包含零向量：$\\ker(A) \\cap \\ker(L) = \\{0\\}$。\n由于 $\\ker(H(\\lambda))=\\{0\\}$，矩阵 $H(\\lambda)$ 对于所有 $\\lambda > 0$ 都是非奇异的，并且存在唯一解 $x_{\\lambda}$。\n\n最后，我们通过求解该线性系统来解出 $x_{\\lambda}$。首先，我们计算矩阵 $A^T A$ 和 $L^T L$，以及向量 $A^T b$。\n$$ A^T = \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{pmatrix}, \\quad L^T = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix} $$\n$$ A^T A = \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1  -1  0 \\\\ 0  1  -1 \\end{pmatrix} = \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix} $$\n$$ L^T L = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  -1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  -1 \\\\ 0  -1  1 \\end{pmatrix} $$\n系统矩阵为：\n$$ H(\\lambda) = A^T A + \\lambda L^T L = \\begin{pmatrix} 1+\\lambda  -1  0 \\\\ -1  2+\\lambda  -1-\\lambda \\\\ 0  -1-\\lambda  1+\\lambda \\end{pmatrix} $$\n右端项为：\n$$ A^T b = \\begin{pmatrix} 1  0 \\\\ -1  1 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\n需求解的关于 $x_{\\lambda} = (x_1, x_2, x_3)^T$ 的系统是：\n$$ \\begin{pmatrix} 1+\\lambda  -1  0 \\\\ -1  2+\\lambda  -1-\\lambda \\\\ 0  -1-\\lambda  1+\\lambda \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ -1 \\end{pmatrix} $$\n这对应于以下方程组：\n1) $(1+\\lambda)x_1 - x_2 = 2$\n2) $-x_1 + (2+\\lambda)x_2 - (1+\\lambda)x_3 = -1$\n3) $-(1+\\lambda)x_2 + (1+\\lambda)x_3 = -1$\n\n由方程(3)，因为 $\\lambda > 0$，所以 $1+\\lambda \\ne 0$，我们可以用它来除：\n$$ -x_2 + x_3 = -\\frac{1}{1+\\lambda} \\implies x_3 = x_2 - \\frac{1}{1+\\lambda} $$\n将这个 $x_3$ 的表达式代入方程(2)：\n$$ -x_1 + (2+\\lambda)x_2 - (1+\\lambda)\\left(x_2 - \\frac{1}{1+\\lambda}\\right) = -1 $$\n$$ -x_1 + (2+\\lambda)x_2 - (1+\\lambda)x_2 + 1 = -1 $$\n$$ -x_1 + (2+\\lambda - 1-\\lambda)x_2 = -2 $$\n$$ -x_1 + x_2 = -2 $$\n现在我们得到了一个关于 $x_1$ 和 $x_2$ 的二元方程组：\n(a) $-x_1 + x_2 = -2$\n(b) $(1+\\lambda)x_1 - x_2 = 2$ (来自方程1)\n\n将方程(a)和(b)相加：\n$$ (-x_1 + x_2) + ((1+\\lambda)x_1 - x_2) = -2 + 2 $$\n$$ -x_1 + (1+\\lambda)x_1 = 0 $$\n$$ \\lambda x_1 = 0 $$\n由于 $\\lambda > 0$，我们必有 $x_1 = 0$。\n\n将 $x_1=0$ 代回方程(a)：\n$$ -0 + x_2 = -2 \\implies x_2 = -2 $$\n最后，将 $x_2=-2$ 代入 $x_3$ 的表达式中：\n$$ x_3 = -2 - \\frac{1}{1+\\lambda} = \\frac{-2(1+\\lambda) - 1}{1+\\lambda} = \\frac{-2-2\\lambda-1}{1+\\lambda} = -\\frac{2\\lambda+3}{\\lambda+1} $$\n解向量为：\n$$ x_{\\lambda} = \\begin{pmatrix} 0 \\\\ -2 \\\\ -\\frac{2\\lambda+3}{\\lambda+1} \\end{pmatrix} $$", "answer": "$$ \\boxed{ \\begin{pmatrix} 0  -2  -\\frac{2\\lambda+3}{\\lambda+1} \\end{pmatrix} } $$", "id": "3405686"}, {"introduction": "在推导出正规方程后，理解正则化参数$\\lambda$如何精确地影响解至关重要。本练习利用强大的奇异值分解（SVD）工具，将正则化过程分解为对解的谱分量的滤波操作。通过计算一个关键的“衰减因子”，你将直观地看到正则化如何选择性地抑制噪声放大的不稳定模式。[@problem_id:3405692]", "problem": "考虑一个数据同化中的线性反问题，其中观测数据向量 $\\mathbf{y} \\in \\mathbb{R}^{m}$ 被建模为 $\\mathbf{y} = A \\mathbf{x}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$，以及 $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{m}$ 代表观测噪声。为稳定反演过程，使用零阶吉洪诺夫正则化（Tikhonov regularization），并将估计量 $\\mathbf{x}_{\\lambda}$ 定义为泛函\n$$\nJ(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}\n$$\n的最小化子，其中 $\\lambda > 0$。从核心定义出发，推导与该最小化问题相关的正规方程，然后将解在奇异值分解（SVD）的基下表示，其中 $A = U \\Sigma V^{\\top}$，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交的，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，其非负对角元素为 $\\sigma_{1}, \\dots, \\sigma_{r}$，其中 $r = \\operatorname{rank}(A)$ 且对于 $i \\le r$ 有 $\\sigma_{i} > 0$。假设数据 $\\mathbf{y}$ 完全投影到单个左奇异向量 $u_{i}$ 上，即，对于某个索引 $i \\in \\{1, \\dots, r\\}$ 和标量 $\\beta_{i} \\in \\mathbb{R}$，有 $\\mathbf{y} = \\beta_{i} u_{i}$，所有其他分量为零。设 $\\sigma_{i}$ 为严格正数，且与 $A$ 的最大奇异值相比很小。\n\n将衰减因子 $f(\\sigma_{i}, \\lambda)$ 定义为 $\\mathbf{x}_{\\lambda}$ 中 $v_{i}$ 的系数与数据系数 $\\beta_{i}$ 之间的比率。以 $\\sigma_{i}$ 和 $\\lambda$ 的形式，计算 $f(\\sigma_{i}, \\lambda)$ 的闭式解析表达式。最终答案必须是单个解析表达式。不需要四舍五入，也不应报告单位。", "solution": "该问题旨在为一个与线性反问题的吉洪诺夫正则化解相关的衰减因子寻找一个闭式表达式。过程始于验证问题陈述。\n\n### 步骤 1：提取已知条件\n-   **线性模型**：观测数据向量 $\\mathbf{y} \\in \\mathbb{R}^{m}$ 通过 $\\mathbf{y} = A \\mathbf{x}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$ 与真实状态 $\\mathbf{x}_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ 相关，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{m}$ 是观测噪声。\n-   **吉洪诺夫泛函**：对于正则化参数 $\\lambda > 0$，正则化解 $\\mathbf{x}_{\\lambda}$ 是泛函 $J(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}$ 的最小化子。\n-   **奇异值分解 (SVD)**：矩阵 $A$ 的 SVD 分解为 $A = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵。矩阵 $\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是对角矩阵，其主对角线上的非负项为 $\\sigma_{1}, \\dots, \\sigma_{r}$，其中 $r = \\operatorname{rank}(A)$ 且对于 $i \\le r$ 有 $\\sigma_{i} > 0$。\n-   **数据条件**：数据向量被指定为 $\\mathbf{y} = \\beta_{i} u_{i}$，其中 $i \\in \\{1, \\dots, r\\}$ 为某个索引，$\\beta_{i} \\in \\mathbb{R}$ 为一个标量系数。此处，$u_i$ 是 $U$ 的第 $i$ 列。\n-   **奇异值条件**：与数据对应的奇异值 $\\sigma_{i}$ 是严格为正的，并且与 $A$ 的最大奇异值相比很小。\n-   **衰减因子的定义**：因子 $f(\\sigma_{i}, \\lambda)$ 被定义为解 $\\mathbf{x}_{\\lambda}$ 中 $v_{i}$ 的系数与数据系数 $\\beta_{i}$ 的比率。\n\n### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据**：该问题是反问题理论中的一个标准练习，特别是关于吉洪诺夫正则化。使用 SVD 是分析此类问题的典型方法。该设定在根本上是合理的。\n-   **适定性**：泛函 $J(\\mathbf{x})$ 是两个平方范数之和。由于 $A^{\\top}A$ 是半正定的，对于任何 $\\lambda > 0$，矩阵 $A^{\\top}A + \\lambda I$ 都是正定的。这保证了 $J(\\mathbf{x})$ 是严格凸的，并有唯一最小值。该问题是适定的。\n-   **目标**：问题以精确的数学语言陈述，所有术语和变量都已明确定义。\n-   **完整性**：推导解所需的所有必要信息都已提供。将数据向量简化为单个 SVD 分量是分离和分析正则化器效果的标准技术，而不是一个缺陷。\n\n### 步骤 3：结论与行动\n问题有效。我们开始求解。\n\n吉洪诺夫正则化解 $\\mathbf{x}_{\\lambda}$ 是使以下泛函最小化的向量 $\\mathbf{x}$：\n$$J(\\mathbf{x}) = \\|A \\mathbf{x} - \\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{2}^{2}$$\n这可以用内积写成：\n$$J(\\mathbf{x}) = (A \\mathbf{x} - \\mathbf{y})^{\\top}(A \\mathbf{x} - \\mathbf{y}) + \\lambda \\mathbf{x}^{\\top}\\mathbf{x}$$\n展开各项，我们得到：\n$$J(\\mathbf{x}) = \\mathbf{x}^{\\top}A^{\\top}A\\mathbf{x} - 2\\mathbf{y}^{\\top}A\\mathbf{x} + \\mathbf{y}^{\\top}\\mathbf{y} + \\lambda \\mathbf{x}^{\\top}\\mathbf{x}$$\n为了找到最小值，我们计算 $J(\\mathbf{x})$ 关于 $\\mathbf{x}$ 的梯度并将其设为零。梯度为：\n$$\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = 2A^{\\top}A\\mathbf{x} - 2A^{\\top}\\mathbf{y} + 2\\lambda\\mathbf{x}$$\n令 $\\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{0}$ 得：\n$$A^{\\top}A\\mathbf{x}_{\\lambda} - A^{\\top}\\mathbf{y} + \\lambda\\mathbf{x}_{\\lambda} = \\mathbf{0}$$\n整理各项，我们得到吉洪诺夫正则化的正规方程：\n$$(A^{\\top}A + \\lambda I)\\mathbf{x}_{\\lambda} = A^{\\top}\\mathbf{y}$$\n其中 $I$ 是 $n \\times n$ 单位矩阵。由于 $\\lambda > 0$，矩阵 $(A^{\\top}A + \\lambda I)$ 是可逆的。解的形式上由下式给出：\n$$\\mathbf{x}_{\\lambda} = (A^{\\top}A + \\lambda I)^{-1}A^{\\top}\\mathbf{y}$$\n为了分析这个解，我们引入 $A$ 的 SVD 分解，即 $A = U \\Sigma V^{\\top}$。其转置为 $A^{\\top} = V \\Sigma^{\\top} U^{\\top}$。我们将这些代入 $\\mathbf{x}_{\\lambda}$ 的表达式中。\n首先，我们计算 $A^{\\top}A$ 项：\n$$A^{\\top}A = (V \\Sigma^{\\top} U^{\\top})(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} (U^{\\top}U) \\Sigma V^{\\top} = V (\\Sigma^{\\top}\\Sigma) V^{\\top}$$\n这里利用了 $U$ 的正交性 ($U^{\\top}U = I_m$)。\n正规方程变为：\n$$(V \\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda I)\\mathbf{x}_{\\lambda} = V \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\n由于 $I = VV^{\\top}$，我们可以分解出 $V$ 和 $V^{\\top}$：\n$$V(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}\\mathbf{x}_{\\lambda} = V \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\n左乘 $V^{\\top}$ 并利用 $V^{\\top}V = I_n$：\n$$(\\Sigma^{\\top}\\Sigma + \\lambda I)V^{\\top}\\mathbf{x}_{\\lambda} = \\Sigma^{\\top} U^{\\top}\\mathbf{y}$$\n矩阵 $(\\Sigma^{\\top}\\Sigma + \\lambda I)$ 是一个 $n \\times n$ 的对角矩阵。其对角线元素对于 $j=1, \\dots, r$ 是 $(\\sigma_{j}^{2} + \\lambda)$，对于 $j > r$ 是 $\\lambda$。这个矩阵是可逆的。\n让我们在 SVD 基中定义解和数据：$\\hat{\\mathbf{x}} = V^{\\top}\\mathbf{x}_{\\lambda}$ 和 $\\hat{\\mathbf{y}} = U^{\\top}\\mathbf{y}$。方程变为：\n$$(\\Sigma^{\\top}\\Sigma + \\lambda I)\\hat{\\mathbf{x}} = \\Sigma^{\\top}\\hat{\\mathbf{y}}$$\n该向量方程的第 $j$ 个分量是：\n$$(\\sigma_{j}^{2} + \\lambda)\\hat{x}_{j} = \\sigma_{j}\\hat{y}_{j}$$\n其中 $\\hat{x}_j$ 和 $\\hat{y}_j$ 分别是 $\\hat{\\mathbf{x}}$ 和 $\\hat{\\mathbf{y}}$ 的第 $j$ 个分量，并且我们定义当 $j > r$ 时 $\\sigma_j=0$。这给出了分量形式的解：\n$$\\hat{x}_{j} = \\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}\\hat{y}_{j} = \\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(u_{j}^{\\top}\\mathbf{y})$$\n完整的解 $\\mathbf{x}_{\\lambda}$ 可以通过从 $V$ 基变换回来重建：\n$$\\mathbf{x}_{\\lambda} = V\\hat{\\mathbf{x}} = \\sum_{j=1}^{n} \\hat{x}_{j} v_{j} = \\sum_{j=1}^{r} \\left(\\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(u_{j}^{\\top}\\mathbf{y})\\right)v_{j}$$\n现在我们使用问题中给出的数据向量的具体形式：$\\mathbf{y} = \\beta_{i} u_{i}$，对于固定的索引 $i \\in \\{1, \\dots, r\\}$。\n我们计算内积 $u_{j}^{\\top}\\mathbf{y}$：\n$$u_{j}^{\\top}\\mathbf{y} = u_{j}^{\\top}(\\beta_{i}u_{i}) = \\beta_{i}(u_{j}^{\\top}u_{i})$$\n由于 $U$ 的列是正交的，我们有 $u_{j}^{\\top}u_{i} = \\delta_{ij}$，即克罗内克（Kronecker）delta。因此，仅当 $j = i$ 时内积非零：\n$$u_{j}^{\\top}\\mathbf{y} = \\beta_{i}\\delta_{ij}$$\n将此代入 $\\mathbf{x}_{\\lambda}$ 的表达式中：\n$$\\mathbf{x}_{\\lambda} = \\sum_{j=1}^{r} \\left(\\frac{\\sigma_{j}}{\\sigma_{j}^{2} + \\lambda}(\\beta_{i}\\delta_{ij})\\right)v_{j}$$\n克罗内克 delta 使求和坍缩为 $j=i$ 的单项：\n$$\\mathbf{x}_{\\lambda} = \\left(\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}\\right)v_{i}$$\n问题将衰减因子 $f(\\sigma_{i}, \\lambda)$ 定义为 $\\mathbf{x}_{\\lambda}$ 中 $v_{i}$ 的系数与数据系数 $\\beta_{i}$ 之间的比率。\n从 $\\mathbf{x}_{\\lambda}$ 的表达式可知，$v_{i}$ 的系数是 $\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}$。\n数据系数被给出为 $\\beta_{i}$。\n比率为：\n$$f(\\sigma_{i}, \\lambda) = \\frac{\\text{coefficient of } v_{i}}{\\text{data coefficient } \\beta_{i}} = \\frac{\\frac{\\sigma_{i}\\beta_{i}}{\\sigma_{i}^{2} + \\lambda}}{\\beta_{i}}$$\n假设 $\\beta_{i} \\neq 0$ (否则问题是平凡的)，我们从分子和分母中消去 $\\beta_{i}$：\n$$f(\\sigma_{i}, \\lambda) = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda}$$\n这就是所指定衰减因子的最终闭式解析表达式。", "answer": "$$\\boxed{\\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda}}$$", "id": "3405692"}, {"introduction": "从理论到实践，解决大规模逆问题需要高效且稳健的数值算法。本练习聚焦于正规方程的数值预处理，特别是在存在异方差噪声的实际情境中。你将实现“预白化”和行均衡策略，并通过编程来量化这些技术如何改善系统矩阵的条件数，为使用迭代法求解铺平道路。[@problem_id:3405684]", "problem": "给定一个带有异方差噪声的线性逆问题，该问题被建模为带 Tikhonov 正则化的加权最小二乘问题。设 $A \\in \\mathbb{R}^{m \\times n}$ 为正向算子，$y \\in \\mathbb{R}^{m}$ 为数据，$R = \\operatorname{diag}(r_i^2) \\in \\mathbb{R}^{m \\times m}$ 为数据误差协方差，其中 $r_i > 0$ 表示第 $i$ 次观测中噪声的标准差。考虑零阶 Tikhonov 正则化，其参数为 $\\alpha > 0$，正则化算子为 $L = I_n$（$n \\times n$ 的单位矩阵）。加权 Tikhonov 泛函为\n$$\nJ(x) = \\|y - A x\\|_{R^{-1}}^2 + \\alpha \\|x\\|_2^2 = (y - A x)^\\top R^{-1} (y - A x) + \\alpha x^\\top x.\n$$\n任务：\n1. 从加权欧几里得范数和 Tikhonov 泛函的定义出发，推导 $J(x)$ 的最小化子 $x^\\star$ 所满足的正规方程，并用 $A$、$R$ 和 $\\alpha$ 表示。\n\n2. 定义预白化（行缩放）算子 $S = R^{-1/2} = \\operatorname{diag}(1/r_i)$ 和预白化系统 $(\\tilde{A}, \\tilde{y})$，其中 $\\tilde{A} = S A$ 和 $\\tilde{y} = S y$。证明正规方程可以等价地用预白化量来表示，并解释为什么当噪声是异方差时，预白化是合适的。\n\n3. 为预白化矩阵 $\\tilde{A}$ 设计一个对角行均衡化算子 $D = \\operatorname{diag}(d_i)$，使得 $D \\tilde{A}$ 的行 2-范数近似相等。使用以下策略：令 $n_i = \\|\\tilde{A}_{i,:}\\|_2$ 表示行范数；定义 $c = \\exp\\left(\\frac{1}{m} \\sum_{i=1}^m \\log(\\max(n_i, \\varepsilon))\\right)$，其中 $\\varepsilon > 0$ 是一个小的保护值；设置 $d_i = c / \\max(n_i, \\varepsilon)$。使用范数不等式和直观的谱论证来证明，均衡化 $\\tilde{A}$ 的行范数可以降低正规方程矩阵的条件数。\n\n4. 实现一个程序，对于下面指定的每个测试用例，该程序构建 $A$、$R$，计算预白化矩阵 $\\tilde{A}$ 和均衡化矩阵 $\\hat{A} = D \\tilde{A}$，形成对称正定矩阵\n$$\nH_{\\text{before}} = \\tilde{A}^\\top \\tilde{A} + \\alpha I_n,\\qquad H_{\\text{after}} = \\hat{A}^\\top \\hat{A} + \\alpha I_n,\n$$\n并返回条件数之比 $\\kappa(H_{\\text{before}})/\\kappa(H_{\\text{after}})$，其中 $\\kappa(\\cdot)$ 表示 2-范数条件数，计算为最大特征值与最小特征值之比。使用适用于对称正定矩阵的特征值计算方法。程序应使用测试套件中定义的确切参数，并且不得读取任何输入。\n\n测试套件：\n- 用例 $1$（理想情况，异方差且中度病态）：$m = 60$，$n = 8$，$\\alpha = 10^{-6}$，随机种子为 $0$。构建 $A$ 为一个标准正态矩阵，其列向量按 $s_j = 10^{-j/(n-1)}$（列索引 $j = 0, 1, \\dots, n-1$）进行缩放。构建 $R$，其中 $r_i$ 在 $10^0$到 $10^2$ 之间对数等距分布（$i = 1, \\dots, m$）。\n\n- 用例 $2$（同方差噪声的边界情况）：$m = 60$，$n = 8$，$\\alpha = 10^{-6}$，随机种子为 $1$。按用例 1 的方式构建 $A$。构建 $R$，其中对所有 $i$ 都有 $r_i = 1$。\n\n- 用例 $3$（强异方差离群值的边缘情况）：$m = 80$，$n = 10$，$\\alpha = 10^{-6}$，随机种子为 $2$。按用例 1 的方式构建 $A$，并使用相同的列缩放公式（使用 $n = 10$）。构建 $R$，使得前 $5$ 行的 $r_i = 100$，其余 $75$ 行的 $r_i = 0.1$。\n\n- 用例 $4$（正则化起主导作用的边界情况）：$m = 60$，$n = 8$，$\\alpha = 1$，随机种子为 $3$。完全按照用例 1 的方式构建 $A$ 和 $R$。\n\n算法与数值细节：\n- 在 $c$ 和 $d_i$ 的定义中使用 $\\varepsilon = 10^{-12}$ 以避免除以零和对零取对数。\n- 使用 $H$ 的最大特征值与最小特征值之比来计算条件数，这是有效的，因为当 $\\alpha > 0$ 时 $H$ 是对称正定的。\n- 每个测试用例的结果是一个等于 $\\kappa(H_{\\text{before}})/\\kappa(H_{\\text{after}})$ 的浮点数。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，结果为逗号分隔的列表，并用方括号括起来，顺序为用例 1、用例 2、用例 3、用例 4。例如，输出必须类似于 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$，其中每个 $\\text{result}_k$ 都是一个浮点数。", "solution": "我们从加权最小二乘 Tikhonov 泛函开始\n$$\nJ(x) = (y - A x)^\\top R^{-1} (y - A x) + \\alpha x^\\top x,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^m$，$R = \\operatorname{diag}(r_i^2) \\succ 0$ 且 $\\alpha > 0$。关于 $x$ 的梯度可以通过应用标准的二次型微分法获得。对于任意对称矩阵 $W$，使用恒等式 $\\nabla_x \\left( (y - A x)^\\top W (y - A x) \\right) = -2 A^\\top W (y - A x)$ 以及 $\\nabla_x (\\alpha x^\\top x) = 2 \\alpha x$，我们得到\n$$\n\\nabla_x J(x) = -2 A^\\top R^{-1} (y - A x) + 2 \\alpha x.\n$$\n将梯度设为零，得到最优性条件\n$$\nA^\\top R^{-1} (y - A x^\\star) = \\alpha x^\\star,\n$$\n整理后可得正规方程\n$$\n\\left( A^\\top R^{-1} A + \\alpha I_n \\right) x^\\star = A^\\top R^{-1} y.\n$$\n因为 $R \\succ 0$ 且 $\\alpha > 0$，矩阵 $A^\\top R^{-1} A + \\alpha I_n$ 是对称正定的，这保证了最小化子 $x^\\star$ 的唯一性。\n\n预白化是处理异方差噪声的一种有原则的方法。定义对角缩放\n$$\nS = R^{-1/2} = \\operatorname{diag}\\left( \\frac{1}{r_i} \\right),\n$$\n和预白化量\n$$\n\\tilde{A} = S A, \\qquad \\tilde{y} = S y.\n$$\n于是泛函变为\n$$\nJ(x) = \\|\\tilde{y} - \\tilde{A} x\\|_2^2 + \\alpha \\|x\\|_2^2,\n$$\n相应的正规方程为\n$$\n\\left( \\tilde{A}^\\top \\tilde{A} + \\alpha I_n \\right) x^\\star = \\tilde{A}^\\top \\tilde{y}.\n$$\n由于 $\\tilde{A}^\\top \\tilde{A} = A^\\top R^{-1} A$ 和 $\\tilde{A}^\\top \\tilde{y} = A^\\top R^{-1} y$，这些方程与加权正规方程在代数上是等价的。预白化是合适的，因为它在该模型下将异方差噪声转换为具有单位协方差的同方差噪声，这是非加权最小二乘法的经典设置。\n\n接下来，我们为 $\\tilde{A}$ 设计一个行均衡化缩放，以使其行范数更加均匀。令\n$$\nn_i = \\|\\tilde{A}_{i,:}\\|_2, \\quad i = 1, \\dots, m,\n$$\n并通过几何平均值定义一个稳定的中心趋势\n$$\nc = \\exp\\left( \\frac{1}{m} \\sum_{i=1}^m \\log( \\max(n_i, \\varepsilon) ) \\right),\n$$\n其中有一个小的保护值 $\\varepsilon > 0$ 以避免未定义的对数。设置\n$$\nd_i = \\frac{c}{\\max(n_i, \\varepsilon)}, \\qquad D = \\operatorname{diag}(d_i),\n$$\n并考虑均衡化矩阵\n$$\n\\hat{A} = D \\tilde{A}.\n$$\n于是 $\\hat{A}$ 的行范数满足 $\\|\\hat{A}_{i,:}\\|_2 = d_i n_i \\approx c$，在保护值范围内各行范数均等化。正规方程矩阵变为\n$$\nH_{\\text{after}} = \\hat{A}^\\top \\hat{A} + \\alpha I_n = \\tilde{A}^\\top D^\\top D \\tilde{A} + \\alpha I_n.\n$$\n虽然用 $D$ 乘以残差会修改权重，但在数值线性代数中，这种对角均衡化用于改善格拉姆矩阵的条件，进而可以改善迭代求解器的收敛性。直观地看，考虑格拉姆矩阵 $G = \\tilde{A}^\\top \\tilde{A}$。其 $(p,q)$ 项是 $\\tilde{A}$ 列向量的内积，它聚合了所有行的贡献。如果某些行因其范数较大而占主导地位，格拉姆矩阵可能会呈现强各向异性，从而扩大谱展。通过均衡化行范数，各行的贡献变得更加均匀，这会收紧 $G$ 的盖尔圆盘，并启发式地降低 $\\kappa(G)$。更具体地说，由于 $G = \\sum_{i=1}^m \\tilde{a}_i \\tilde{a}_i^\\top$，其中 $\\tilde{a}_i^\\top$ 表示第 $i$ 行，缩放 $\\tilde{a}_i \\mapsto d_i \\tilde{a}_i$ 平衡了此和中秩一更新的量级。因为 $H = G + \\alpha I_n$ 的特征值为 $\\lambda_j(H) = \\lambda_j(G) + \\alpha$，所以减小 $\\lambda_j(G)$ 的谱展会降低 $\\kappa(H) = \\frac{\\lambda_{\\max}(G) + \\alpha}{\\lambda_{\\min}(G) + \\alpha}$，除非 $\\alpha$ 大到已经起主导作用。\n\n每个测试用例的算法：\n1. 使用固定的随机种子构建 $A$，并应用列缩放 $s_j = 10^{-j/(n-1)}$ 以引入轻微的病态性。这种缩放使得后面的列更小，增加了 $A^\\top A$ 的各向异性。\n2. 根据指定的配置构建 $R$，然后计算 $S = \\operatorname{diag}(1/r_i)$ 和 $\\tilde{A} = S A$。\n3. 计算 $\\tilde{A}$ 的行范数 $n_i$，使用几何平均值策略和 $\\varepsilon = 10^{-12}$ 形成 $D$，并获得 $\\hat{A} = D \\tilde{A}$。\n4. 形成 $H_{\\text{before}} = \\tilde{A}^\\top \\tilde{A} + \\alpha I_n$ 和 $H_{\\text{after}} = \\hat{A}^\\top \\hat{A} + \\alpha I_n$。\n5. 通过最大与最小特征值之比计算它们的条件数 $\\kappa(H_{\\text{before}})$ 和 $\\kappa(H_{\\text{after}})$（为保证数值稳定性，请使用对称特征求解器）。\n6. 输出比率 $\\kappa(H_{\\text{before}})/\\kappa(H_{\\text{after}})$。\n\n测试套件中的预期行为：\n- 用例 1：异方差性与各向异性的列相结合，在均衡化后会导致条件数显著降低，因此比率应大于 $1$。\n- 用例 2：对于同方差噪声，预白化是单位操作，均衡化仅处理来自 $A$ 的各向异性，因此改善效果是中等的；比率仍大于 $1$。\n- 用例 3：强异方差离群值会加剧不平衡；均衡化应产生比用例 2 更大的改善，从而增加比率。\n- 用例 4：当 $\\alpha = 1$ 时，正则化占主导作用，缩小了谱展；比率应接近 $1$。\n\n程序必须生成单行输出，其中包含按所述用例顺序排列的四个比率，以逗号分隔并用方括号括起来。", "answer": "```python\nimport numpy as np\n\ndef construct_A(m, n, seed):\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((m, n))\n    # Column scaling to induce anisotropy: s_j = 10^{-j/(n-1)} for j = 0..n-1\n    if n > 1:\n        scales = 10.0 ** (-np.arange(n) / (n - 1))\n    else:\n        scales = np.array([1.0])\n    A = A * scales  # broadcast column-wise\n    return A\n\ndef construct_R(profile, m):\n    if profile == \"logspace_1_to_100\":\n        # r_i logarithmically spaced from 10^0 to 10^2\n        r = 10.0 ** np.linspace(0.0, 2.0, m)\n    elif profile == \"uniform_1\":\n        r = np.ones(m)\n    elif profile == \"outliers\":\n        # first 5 rows have 100, remaining have 0.1\n        r = np.ones(m) * 0.1\n        r[:5] = 100.0\n    else:\n        raise ValueError(\"Unknown R profile\")\n    return r\n\ndef prewhiten(A, r):\n    # S = diag(1/r_i): scale rows of A by 1/r_i\n    inv_r = 1.0 / r\n    tilde_A = A * inv_r[:, None]\n    return tilde_A\n\ndef equilibrate_rows(tilde_A, eps=1e-12):\n    # Row norms\n    row_norms = np.linalg.norm(tilde_A, axis=1)\n    # Geometric mean of row norms with safeguard\n    safe_norms = np.maximum(row_norms, eps)\n    c = np.exp(np.mean(np.log(safe_norms)))\n    d = c / safe_norms\n    # Avoid scaling rows that are exactly zero (if any)\n    d = np.where(row_norms > 0.0, d, 1.0)\n    D_tilde_A = tilde_A * d[:, None]\n    return D_tilde_A\n\ndef gram_with_tikhonov(mat, alpha, n):\n    # H = mat^T mat + alpha * I\n    H = mat.T @ mat\n    H[np.diag_indices(n)] += alpha\n    return H\n\ndef cond_spd(H):\n    # For SPD, condition number in 2-norm is ratio of largest to smallest eigenvalues\n    # Use eigh for symmetric matrices\n    w = np.linalg.eigvalsh(H)\n    w_min = np.min(w)\n    w_max = np.max(w)\n    # Guard against numerical issues (shouldn't happen with alpha > 0, but good practice)\n    if w_min = 0.0:\n        # If numerical zero or negative due to rounding, set a tiny floor\n        w_min = max(w_min, 1e-300)\n    return w_max / w_min\n\ndef run_case(m, n, alpha, seed, r_profile):\n    A = construct_A(m, n, seed)\n    r = construct_R(r_profile, m)\n    tilde_A = prewhiten(A, r)\n    H_before = gram_with_tikhonov(tilde_A, alpha, n)\n    cond_before = cond_spd(H_before)\n    eq_A = equilibrate_rows(tilde_A, eps=1e-12)\n    H_after = gram_with_tikhonov(eq_A, alpha, n)\n    cond_after = cond_spd(H_after)\n    ratio = cond_before / cond_after\n    return float(ratio)\n\ndef solve():\n    test_cases = [\n        # Case 1: m=60, n=8, alpha=1e-6, seed=0, R logspace from 10^0 to 10^2\n        (60, 8, 1e-6, 0, \"logspace_1_to_100\"),\n        # Case 2: m=60, n=8, alpha=1e-6, seed=1, R uniform 1\n        (60, 8, 1e-6, 1, \"uniform_1\"),\n        # Case 3: m=80, n=10, alpha=1e-6, seed=2, R outliers\n        (80, 10, 1e-6, 2, \"outliers\"),\n        # Case 4: m=60, n=8, alpha=1.0, seed=3, R logspace as Case 1\n        (60, 8, 1.0, 3, \"logspace_1_to_100\"),\n    ]\n\n    results = []\n    for m, n, alpha, seed, r_profile in test_cases:\n        ratio = run_case(m, n, alpha, seed, r_profile)\n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3405684"}]}