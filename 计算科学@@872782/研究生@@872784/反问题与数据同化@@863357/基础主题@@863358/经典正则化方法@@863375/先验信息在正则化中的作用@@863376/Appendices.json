{"hands_on_practices": [{"introduction": "本练习旨在建立贝叶斯推断与正则化之间的根本联系。通过一个带有高斯假设的简单线性反问题，您将分析推导出后验分布如何融合来自数据和先验的信息。这个实践明确地展示了 Tikhonov 正则化项如何直接源于高斯先验，并且正则化强度由先验方差控制，从而巩固了“正则化是贝叶斯推断的一种表现形式”这一核心概念 ([@problem_id:3418467])。", "problem": "考虑一个被构建为贝叶斯数据同化任务的一维线性反演问题。设未知状态为 $x \\in \\mathbb{R}$。单个观测值 $y \\in \\mathbb{R}$ 通过模型 $y = h x + \\varepsilon$ 与状态相关，其中 $h \\in \\mathbb{R}$ 是线性正向算子，观测误差 $\\varepsilon$ 是均值为零、方差为 $\\sigma^{2} > 0$ 的高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$。关于 $x$ 的先验信息是均值为 $\\mu \\in \\mathbb{R}$、方差为 $C > 0$ 的高斯分布，即 $x \\sim \\mathcal{N}(\\mu, C)$。贝叶斯后验由贝叶斯定理定义：后验密度与似然和先验的乘积成正比。\n\n现在定义一个缩放的先验方差为 $C \\mapsto \\alpha C$，其中 $\\alpha > 0$ 是一个标量。将 $\\alpha$ 视为一个可调参数，用于调整先验不确定性的水平。从高斯似然和高斯先验的定义出发，使用贝叶斯定理，推导出后验均值和后验方差，使其成为以 $\\alpha$、$h$、$\\sigma^{2}$、$\\mu$、$C$ 和 $y$ 表示的闭式解析表达式。然后，通过将后验与负对数后验（这是一个关于 $x$ 的二次准则）的最小化器联系起来，解释 $\\alpha$ 作为由先验不确定性所确定的正则化强度的作用。根据你推导的表达式，解释改变 $\\alpha$ 如何改变数据失配项和先验项之间的平衡。\n\n将你的最终答案表示为一个包含两个元素的行矩阵，依次为后验均值和后验方差。无需四舍五入。最终答案中无需单位。", "solution": "该问题要求推导线性反演问题的后验均值和方差，并解释先验不确定性缩放参数 $\\alpha$ 的作用。\n\n首先，我们建立概率框架。状态是一个随机变量 $x \\in \\mathbb{R}$，观测值是一个随机变量 $y \\in \\mathbb{R}$。\n\n关于状态 $x$ 的先验信息由一个高斯分布给出，其均值为 $\\mu$，方差被缩放为 $\\alpha C$，其中 $\\alpha > 0$ 且 $C > 0$。$x$ 的先验概率密度函数 (PDF) 为：\n$$p(x) = \\mathcal{N}(x; \\mu, \\alpha C) = \\frac{1}{\\sqrt{2\\pi \\alpha C}} \\exp\\left( -\\frac{(x - \\mu)^2}{2 \\alpha C} \\right)$$\n\n正向模型通过线性方程 $y = hx + \\varepsilon$ 将状态 $x$ 与观测值 $y$ 联系起来。观测误差 $\\varepsilon$ 从均值为零、方差为 $\\sigma^2 > 0$ 的高斯分布中抽取，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。这定义了似然函数，即给定状态 $x$ 时观测值 $y$ 的条件概率。给定一个 $x$ 值，$y$ 的分布是均值为 $hx$、方差为 $\\sigma^2$ 的高斯分布。似然PDF为：\n$$p(y|x) = \\mathcal{N}(y; hx, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y - hx)^2}{2\\sigma^2} \\right)$$\n\n根据贝叶斯定理，后验PDF $p(x|y)$ 与似然和先验的乘积成正比：\n$$p(x|y) \\propto p(y|x) p(x)$$\n代入似然和先验的表达式，我们得到：\n$$p(x|y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y - hx)^2}{2\\sigma^2} \\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi \\alpha C}} \\exp\\left( -\\frac{(x - \\mu)^2}{2 \\alpha C} \\right) \\right]$$\n由于归一化常数不依赖于 $x$，我们可以合并指数项：\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2}\\left[ \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C} \\right] \\right)$$\n指数部分是 $x$ 的二次函数，这表明后验分布也是高斯分布。设后验分布为 $p(x|y) = \\mathcal{N}(x; \\mu_{\\text{post}}, C_{\\text{post}})$，其形式为：\n$$p(x|y) \\propto \\exp\\left( -\\frac{(x - \\mu_{\\text{post}})^2}{2 C_{\\text{post}}} \\right)$$\n为了求出后验均值 $\\mu_{\\text{post}}$ 和后验方差 $C_{\\text{post}}$，我们可以展开 $p(x|y)$ 表达式中指数的参数，并对 $x$ 进行配方。设 $Q(x)$ 为方括号中的项：\n$$Q(x) = \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C}$$\n$$Q(x) = \\frac{y^2 - 2yhx + h^2x^2}{\\sigma^2} + \\frac{x^2 - 2x\\mu + \\mu^2}{\\alpha C}$$\n我们按 $x$ 的幂次对各项进行分组：\n$$Q(x) = x^2 \\left( \\frac{h^2}{\\sigma^2} + \\frac{1}{\\alpha C} \\right) - 2x \\left( \\frac{yh}{\\sigma^2} + \\frac{\\mu}{\\alpha C} \\right) + \\left( \\frac{y^2}{\\sigma^2} + \\frac{\\mu^2}{\\alpha C} \\right)$$\n将其与标准二次型 $\\frac{(x - \\mu_{\\text{post}})^2}{C_{\\text{post}}} = \\frac{1}{C_{\\text{post}}}x^2 - \\frac{2\\mu_{\\text{post}}}{C_{\\text{post}}}x + \\frac{\\mu_{\\text{post}}^2}{C_{\\text{post}}}$ 进行比较，我们可以确定系数。\n$x^2$ 的系数给出了后验方差的倒数：\n$$\\frac{1}{C_{\\text{post}}} = \\frac{h^2}{\\sigma^2} + \\frac{1}{\\alpha C} = \\frac{h^2 \\alpha C + \\sigma^2}{\\sigma^2 \\alpha C}$$\n解出后验方差 $C_{\\text{post}}$：\n$$C_{\\text{post}} = \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}$$\n$x$ 的系数给出了后验均值与后验方差的比值：\n$$\\frac{\\mu_{\\text{post}}}{C_{\\text{post}}} = \\frac{yh}{\\sigma^2} + \\frac{\\mu}{\\alpha C} = \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C}$$\n解出后验均值 $\\mu_{\\text{post}}$：\n$$\\mu_{\\text{post}} = C_{\\text{post}} \\left( \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C} \\right)$$\n代入 $C_{\\text{post}}$ 的表达式：\n$$\\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2} \\right) \\left( \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C} \\right)$$\n$$\\mu_{\\text{post}} = \\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}$$\n\n现在，我们通过将贝叶斯公式与正则化框架联系起来，来解释 $\\alpha$ 的作用。$x$ 的最大后验 (MAP) 估计是使 $p(x|y)$ 最大化的值，这等价于最小化负对数后验。负对数后验与我们之前定义的二次函数 $Q(x)$ 成正比，该函数通常被称为代价函数 $J(x)$：\n$$J(x) = \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C}$$\n该代价函数由两项组成：\n1.  数据失配项：项 $\\frac{(y - hx)^2}{\\sigma^2}$ 衡量了预测值 $hx$ 和观测值 $y$ 之间的平方差，并由观测误差方差的倒数加权。它量化了状态 $x$ 拟合数据的程度。\n2.  先验/正则化项：项 $\\frac{(x - \\mu)^2}{\\alpha C}$ 惩罚状态 $x$ 对先验均值 $\\mu$ 的偏离，并由先验方差 $\\alpha C$ 的倒数加权。\n\n参数 $\\alpha$ 直接控制先验分布 $\\mathcal{N}(\\mu, \\alpha C)$ 的方差。因此，它调节了对先验信息的置信水平，并充当正则化参数。\n-   当 $\\alpha \\to \\infty$ 时，先验方差变为无穷大，代表了最大不确定性状态或无信息先验。正则化项 $\\frac{(x - \\mu)^2}{\\alpha C} \\to 0$，代价函数由数据失配项主导。后验均值趋近于 $\\lim_{\\alpha \\to \\infty} \\mu_{\\text{post}} = \\frac{yhC}{h^2C} = \\frac{y}{h}$，即最大似然估计。解完全由数据驱动。\n-   当 $\\alpha \\to 0$ 时，先验方差趋于零，代表了 $x = \\mu$ 的绝对确定状态。对于任何 $x \\neq \\mu$，正则化项 $\\frac{(x - \\mu)^2}{\\alpha C}$ 会变得无限大，迫使解趋向于先验均值。后验均值趋近于 $\\lim_{\\alpha \\to 0} \\mu_{\\text{post}} = \\frac{\\mu\\sigma^2}{\\sigma^2} = \\mu$。解完全由先验信息驱动，而忽略了数据。\n\n总而言之，$\\alpha$ 调整了拟合数据与遵循先验信念之间的平衡。较大的 $\\alpha$ 表示对先验的置信度较低（较弱的正则化），使得后验估计更多地受到观测值 $y$ 的影响。较小的 $\\alpha$ 表示对先验的置信度较高（较强的正则化），将估计值拉向先验均值 $\\mu$。这展示了先验在反演问题中既是信息来源又是正则化机制的双重作用。\n\n最终答案包括推导出的后验均值和后验方差。\n后验均值：$\\mu_{\\text{post}} = \\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}$\n后验方差：$C_{\\text{post}} = \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2} & \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}\n\\end{pmatrix}\n}\n$$", "id": "3418467"}, {"introduction": "超越平滑性假设，科学与工程中的许多信号以其稀疏性而著称。本练习将探讨如何使用拉普拉斯分布来编码这种结构化的先验信息。您将看到，在该设定下，最大化后验概率等价于最小化一个带有 $L_1$ 惩罚项的成本函数，并推导出著名的软阈值算子作为其解，这是诸如 LASSO 等现代稀疏恢复算法的基石 ([@problem_id:3418473])。", "problem": "考虑一个单系数线性观测模型，其中未知标量状态 $x$ 通过单个带有加性噪声的线性测量 $y$ 进行观测，即 $y = a x + \\varepsilon$。假设噪声 $\\varepsilon$ 是均值为零、方差为 $\\sigma^{2}$ 的高斯噪声，并且 $x$ 的先验是拉普拉斯分布，其密度正比于 $\\exp(-\\lambda |x|)$，其中 $\\lambda > 0$ 是一个固定的惩罚权重。您的任务是：\n\n1) 仅从适定、下半连续凸函数 $f$ 的近端算子定义 $\\operatorname{prox}_{\\gamma f}(v) = \\arg\\min_{x} \\left\\{ \\frac{1}{2} |x - v|^{2} + \\gamma f(x) \\right\\}$ (对于任意 $\\gamma > 0$) 出发，并使用凸分析中的次微分最优性条件，推导出一维情况下 $\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v)$ 的闭式表达式。\n\n2) 使用贝叶斯定理以及高斯似然和拉普拉斯先验的定义，写出此模型中给定 $y$ 时 $x$ 的负对数后验（不计一个无关的加法常数），并将最大后验（MAP）估计量刻画为该负对数后验的最小化子。通过配方法和重新缩放，将 MAP 问题简化为一个与第一部分相匹配的近端问题，然后得到 MAP 估计量关于 $a$、$\\sigma^{2}$、$\\lambda$ 和 $y$ 的显式闭式表达式。\n\n3) 对于 $a = 1.6$，$\\sigma^{2} = 0.8$，$\\lambda = 0.3$ 和 $y = 0.2$，评估您的闭式表达式。报告 $x$ 的 MAP 估计量的最终数值，四舍五入到四位有效数字。无需单位。", "solution": "问题分为三个部分。我们将按顺序进行解答。\n\n**第1部分：近端算子的推导**\n\n我们被要求推导近端算子 $\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v)$ 的闭式表达式。令阈值参数表示为 $\\alpha = \\gamma\\lambda$。因为 $\\gamma > 0$ 且 $\\lambda > 0$，所以我们有 $\\alpha > 0$。函数为 $f(x) = |x|$。根据所提供的定义，近端算子由以下最小化问题的解给出：\n$$\n\\operatorname{prox}_{\\alpha |\\cdot|}(v) = \\arg\\min_{x} \\left\\{ J(x) = \\frac{1}{2} (x - v)^{2} + \\alpha |x| \\right\\}\n$$\n目标函数 $J(x)$ 是一个严格凸的可微函数（$\\frac{1}{2}(x-v)^2$）与一个凸函数（$\\alpha|x|$）的和。因此，$J(x)$ 是严格凸的，并有唯一的最小化子，我们将其表示为 $x^*$。\n\n我们使用次微分最优性条件，该条件表明 $x^*$ 是最小化子当且仅当 $0$ 属于 $J(x)$ 在 $x^*$ 处的次微分。\n$$\n0 \\in \\partial J(x^*)\n$$\n$J(x)$ 的次微分由其各组成部分的次微分之和给出：\n$$\n\\partial J(x) = \\partial \\left( \\frac{1}{2}(x - v)^2 \\right) + \\partial(\\alpha |x|)\n$$\n第一项是可微的，所以其次微分就是其导数：$x - v$。第二项的次微分是 $\\alpha \\partial|x|$。绝对值函数 $\\partial|x|$ 的次微分定义为：\n$$\n\\partial |x| =\n\\begin{cases}\n    \\{1\\} & \\text{if } x > 0 \\\\\n    \\{-1\\} & \\text{if } x  0 \\\\\n    [-1, 1]  \\text{if } x = 0\n\\end{cases}\n$$\n所以，最小化子 $x^*$ 的最优性条件是：\n$$\n0 \\in (x^* - v) + \\alpha \\partial|x^*|\n$$\n这可以重写为：\n$$\nv - x^* \\in \\alpha \\partial|x^*|\n$$\n我们通过考虑 $x^*$ 的三种情况来分析这个条件：\n\n情况1：$x^* > 0$。\n在这种情况下，$\\partial|x^*| = \\{1\\}$。条件变为 $v - x^* = \\alpha(1)$，这意味着 $x^* = v - \\alpha$。为使此解与假设 $x^* > 0$ 一致，我们必须有 $v - \\alpha > 0$，即 $v > \\alpha$。\n\n情况2：$x^*  0$。\n在这种情况下，$\\partial|x^*| = \\{-1\\}$。条件变为 $v - x^* = \\alpha(-1)$，这意味着 $x^* = v + \\alpha$。为使此解与假设 $x^*  0$ 一致，我们必须有 $v + \\alpha  0$，即 $v  -\\alpha$。\n\n情况3：$x^* = 0$。\n在这种情况下，$\\partial|x^*| = [-1, 1]$。条件变为 $v - 0 \\in \\alpha[-1, 1]$，这等价于 $v \\in [-\\alpha, \\alpha]$，即 $|v| \\le \\alpha$。\n\n综合这三种情况，我们得到 $x^*$ 的完整解：\n$$\nx^* =\n\\begin{cases}\n    v - \\alpha  \\text{if } v > \\alpha \\\\\n    v + \\alpha  \\text{if } v  -\\alpha \\\\\n    0  \\text{if } |v| \\le \\alpha\n\\end{cases}\n$$\n这个分段函数被称为软阈值算子，通常表示为 $\\mathcal{S}_{\\alpha}(v)$。它可以写成一个紧凑的形式：\n$$\nx^* = \\operatorname{sgn}(v) \\max(0, |v| - \\alpha)\n$$\n用原始参数 $\\gamma\\lambda$ 替换 $\\alpha$，我们得到近端算子的最终表达式：\n$$\n\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v) = \\operatorname{sgn}(v) \\max(0, |v| - \\gamma\\lambda)\n$$\n\n**第2部分：MAP 估计量的推导**\n\n给定线性观测模型 $y = ax + \\varepsilon$，其中噪声 $\\varepsilon$ 是均值为 $0$、方差为 $\\sigma^2$ 的高斯噪声，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。这意味着给定状态 $x$ 的观测值 $y$ 的似然也是高斯分布：$y|x \\sim \\mathcal{N}(ax, \\sigma^2)$。该似然的概率密度函数（PDF）为：\n$$\np(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2}\\right)\n$$\n$x$ 的先验是拉普拉斯分布，其密度正比于 $\\exp(-\\lambda|x|)$，其中 $\\lambda > 0$：\n$$\np(x) \\propto \\exp(-\\lambda|x|)\n$$\n根据贝叶斯定理，$x$ 给定 $y$ 的后验分布正比于似然与先验的乘积：\n$$\np(x|y) \\propto p(y|x) p(x) \\propto \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2}\\right) \\exp(-\\lambda|x|) = \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2} - \\lambda|x|\\right)\n$$\n最大后验（MAP）估计量 $\\hat{x}_{\\text{MAP}}$ 是使后验概率 $p(x|y)$ 最大化的 $x$ 的值。这等价于最小化负对数后验。负对数后验（不计一个无关的加法常数）为：\n$$\nJ_{\\text{MAP}}(x) = \\frac{(y - ax)^2}{2\\sigma^2} + \\lambda|x|\n$$\n因此，MAP 估计量由下式给出：\n$$\n\\hat{x}_{\\text{MAP}} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}(y - ax)^2 + \\lambda|x| \\right\\}\n$$\n为了将其与第1部分的近端算子问题联系起来，我们必须对目标函数 $J_{\\text{MAP}}(x)$ 进行处理，使其变为 $\\frac{1}{2}(x-v)^2 + \\alpha'|x|$ 的形式。我们首先对包含 $x$ 的二次项进行配方：\n$$\n\\frac{1}{2\\sigma^2}(y - ax)^2 = \\frac{1}{2\\sigma^2}(a^2x^2 - 2axy + y^2) = \\frac{a^2}{2\\sigma^2} \\left(x^2 - 2\\frac{y}{a}x + \\left(\\frac{y}{a}\\right)^2\\right) = \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2\n$$\n将此代回目标函数：\n$$\nJ_{\\text{MAP}}(x) = \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2 + \\lambda|x|\n$$\n最小化此函数等价于最小化一个乘以正常数的相同函数，因为缩放不会改变最小点的位置。我们乘以 $\\frac{\\sigma^2}{a^2}$ 以使平方项的系数等于 $\\frac{1}{2}$：\n$$\n\\hat{x}_{\\text{MAP}} = \\arg\\min_x \\left\\{ \\frac{\\sigma^2}{a^2} \\left[ \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2 + \\lambda|x| \\right] \\right\\} = \\arg\\min_x \\left\\{ \\frac{1}{2} \\left(x - \\frac{y}{a}\\right)^2 + \\frac{\\sigma^2\\lambda}{a^2}|x| \\right\\}\n$$\n这个表达式现在是第1部分中近端问题的形式 $\\arg\\min_x \\{ \\frac{1}{2}(x-v)^2 + \\alpha'|x| \\}$，其中对应关系为：\n$$\nv = \\frac{y}{a} \\quad \\text{和} \\quad \\alpha' = \\frac{\\sigma^2\\lambda}{a^2}\n$$\n使用第1部分中推导出的软阈值解，阈值参数为 $\\alpha'$：\n$$\n\\hat{x}_{\\text{MAP}} = \\mathcal{S}_{\\alpha'}(v) = \\operatorname{sgn}(v) \\max(0, |v| - \\alpha')\n$$\n将 $v$ 和 $\\alpha'$ 的表达式代回，我们得到 MAP 估计量的显式闭式表达式：\n$$\n\\hat{x}_{\\text{MAP}} = \\operatorname{sgn}\\left(\\frac{y}{a}\\right) \\max\\left(0, \\left|\\frac{y}{a}\\right| - \\frac{\\sigma^2\\lambda}{a^2}\\right)\n$$\n\n**第3部分：数值计算**\n\n我们被要求用给定值计算 $\\hat{x}_{\\text{MAP}}$ 的闭式表达式：$a = 1.6$，$\\sigma^2 = 0.8$，$\\lambda = 0.3$ 和 $y = 0.2$。\n\n首先，我们计算表达式中的各项：\n$$\n\\frac{y}{a} = \\frac{0.2}{1.6} = \\frac{2}{16} = \\frac{1}{8} = 0.125\n$$\n接下来，我们计算阈值参数：\n$$\n\\frac{\\sigma^2\\lambda}{a^2} = \\frac{(0.8)(0.3)}{(1.6)^2} = \\frac{0.24}{2.56} = \\frac{24}{256} = \\frac{3}{32} = 0.09375\n$$\n现在，我们将这些值代入 $\\hat{x}_{\\text{MAP}}$ 的表达式中：\n$$\n\\hat{x}_{\\text{MAP}} = \\operatorname{sgn}(0.125) \\max(0, |0.125| - 0.09375)\n$$\n由于 $0.125 > 0.09375$，$\\max$ 函数内的项为正。由于 $\\operatorname{sgn}(0.125) = 1$，我们得到：\n$$\n\\hat{x}_{\\text{MAP}} = 1 \\times (0.125 - 0.09375) = 0.03125\n$$\n问题要求结果四舍五入到四位有效数字。数字 $0.03125$ 正好有四位有效数字（$3, 1, 2, 5$）。因此，无需进一步舍入。\n$x$ 的 MAP 估计量的数值是 $0.03125$。", "answer": "$$\n\\boxed{0.03125}\n$$", "id": "3418473"}, {"introduction": "在应用正则化时，一个关键步骤是为正则化参数 $\\lambda$ 选择一个合适的值，以平衡数据保真度和先验信念的影响。L 曲线法是完成此任务的一种广泛使用且直观的启发式方法。本动手编程实践将指导您实现 L 曲线准则，通过在解范数与残差范数的对数-对数图上定位最大曲率点来寻找最优的 $\\lambda$ ([@problem_id:3418439])。", "problem": "考虑一个线性反问题，其观测模型为 $b = A x_{\\mathrm{true}} + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x_{\\mathrm{true}} \\in \\mathbb{R}^n$，加性噪声 $\\varepsilon \\in \\mathbb{R}^m$。假设噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 服从高斯似然，且 $x$ 的高斯先验均值为 $\\mu \\in \\mathbb{R}^n$，精度算子为 $\\alpha^2 L^{\\top} L$，其中 $L \\in \\mathbb{R}^{p \\times n}$。最大后验 (MAP) 估计通过最小化负对数后验得到，这等价于 Tikhonov 正则化：最小化泛函 $J_{\\lambda}(x) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert L (x - \\mu) \\rVert_2^2$，其中 $\\lambda = \\sigma^2 \\alpha^2 > 0$。这通过选择 $L$ 和 $\\mu$ 来编码先验信息。对于每个 $\\lambda > 0$，最小化子 $x_{\\lambda}$ 满足正规方程 $(A^{\\top} A + \\lambda L^{\\top} L) x_{\\lambda} = A^{\\top} b + \\lambda L^{\\top} L \\mu$。\n\nL-曲线准则通过考虑参数曲线 $\\mathcal{C}(\\lambda) = (\\xi(\\lambda), \\eta(\\lambda))$ 来评估数据失配和先验罚项之间的权衡，其中 $\\xi(\\lambda) = \\log \\lVert A x_{\\lambda} - b \\rVert_2$ 且 $\\eta(\\lambda) = \\log \\lVert L (x_{\\lambda} - \\mu) \\rVert_2$。L-曲线的“拐角”通常对应于拟合与正则化之间的良好平衡，可以通过最大化曲率来定位。为了获得尺度不变的参数化，定义 $t = \\log \\lambda$ 并考虑平面曲线 $c(t) = (\\xi(t), \\eta(t))$，其中 $\\lambda = e^t$。平面曲线 $c(t) = (x(t), y(t))$ 的曲率 $\\kappa$ 由经过充分检验的公式给出\n$$\n\\kappa(t) = \\frac{\\lvert x'(t) y''(t) - y'(t) x''(t) \\rvert}{\\left( x'(t)^2 + y'(t)^2 \\right)^{3/2}}.\n$$\n在此设定中，$x(t) = \\xi(t)$ 且 $y(t) = \\eta(t)$，导数是关于 $t = \\log \\lambda$ 的。在数值上，$\\kappa(t)$ 可以通过在 $t$ 的均匀网格上使用有限差分来近似。\n\n你的任务是构建此准则并实现一个程序，该程序针对一小组但多样化的测试用例，在 $t = \\log \\lambda$ 的均匀网格上计算 $\\kappa(t)$，定位最大化点 $t_{\\star}$（不包括端点），并报告每个测试用例的 $\\lambda_{\\star} = e^{t_{\\star}}$。\n\n使用以下测试套件、通用定义和数值细节：\n\n所有测试用例的通用定义：\n- 维度 $n = 20$ 和网格点 $s_i = \\frac{i}{n+1}$，其中 $i = 1, 2, \\dots, n$。\n- 定义 $\\Delta = \\frac{1}{n}$ 和核宽度 $w = 0.08$。\n- 通过以下公式定义高斯模糊矩阵 $A_0 \\in \\mathbb{R}^{n \\times n}$\n$$\n(A_0)_{ij} = \\exp\\!\\left( - \\frac{(s_i - s_j)^2}{2 w^2} \\right) \\, \\Delta \\quad \\text{for } 1 \\le i,j \\le n.\n$$\n- 通过 $x_{\\mathrm{true}, i} = \\sin(2 \\pi s_i) + 0.5 \\, s_i$ 定义“真实”状态，其中 $i = 1, 2, \\dots, n$。\n- 对于给定的噪声水平 $\\sigma_{\\mathrm{noise}} > 0$，通过 $\\varepsilon_i = \\sigma_{\\mathrm{noise}} \\, \\frac{\\cos(7 i)}{i}$ 定义一个确定性噪声向量 $\\varepsilon \\in \\mathbb{R}^n$，其中 $i = 1, 2, \\dots, n$。\n- 对于每个测试用例，数据为 $b = A x_{\\mathrm{true}} + \\varepsilon$，使用指定的 $A$ 和 $\\sigma_{\\mathrm{noise}}$。\n\n定义以下正则化算子和先验均值选项：\n- 单位算子 $L = I_n$ (因此 $p = n$)。\n- 一阶差分算子 $D \\in \\mathbb{R}^{(n-1) \\times n}$，其元素为 $D_{k,k} = 1$，$D_{k,k+1} = -1$（对于 $k = 1, 2, \\dots, n-1$），其他位置为零。\n- 先验均值 $\\mu \\in \\mathbb{R}^n$ 如下指定（$\\mu = 0$ 或非零趋势）。\n\n测试套件（四个用例）：\n1. 用例 1 (病态正向映射，单位先验): $A = A_0$, $L = I_n$, $\\mu = 0$, $\\sigma_{\\mathrm{noise}} = 10^{-3}$。\n2. 用例 2 (病态正向映射，带非零均值的光滑先验): $A = A_0$, $L = D$, $\\mu_i = 0.25 \\, s_i$, $\\sigma_{\\mathrm{noise}} = 10^{-3}$。\n3. 用例 3 (良态正向映射，光滑先验): $A = I_n$, $L = D$, $\\mu = 0$, $\\sigma_{\\mathrm{noise}} = 5 \\cdot 10^{-2}$。\n4. 用例 4 (秩亏正向映射，单位先验): 从 $A_0$ 开始，将其最后一列设置为与第一列相同，以获得 $A \\in \\mathbb{R}^{n \\times n}$，其中 $A_{:,n} := A_{:,1}$，然后使用 $L = I_n$, $\\mu = 0$, $\\sigma_{\\mathrm{noise}} = 10^{-3}$。\n\n对于每个用例：\n- 在由 $t = \\log \\lambda$ 的均匀网格定义的 $\\lambda$ 值网格上求解 $x_{\\lambda}$，其中 $t \\in [\\log(10^{-10}), \\log(10^2)]$ 且有 $M = 201$ 个网格点。对于每个 $\\lambda = e^t$，计算残差范数 $\\lVert A x_{\\lambda} - b \\rVert_2$ 和惩罚项范数 $\\lVert L (x_{\\lambda} - \\mu) \\rVert_2$，然后计算L曲线的坐标 $\\xi(t)$ 和 $\\eta(t)$（即相应范数的自然对数）。\n- 通过在均匀的 $t$-网格上使用中心有限差分，数值近似导数 $\\xi'(t)$, $\\xi''(t)$, $\\eta'(t)$ 和 $\\eta''(t)$。使用以下公式计算曲率\n$$\n\\kappa(t) = \\frac{\\left| \\xi'(t) \\, \\eta''(t) - \\eta'(t) \\, \\xi''(t) \\right|}{\\left( \\xi'(t)^2 + \\eta'(t)^2 \\right)^{3/2}}.\n$$\n- 在内部网格点（不包括端点）上，找出使 $\\kappa(t)$ 最大化的索引 $k^{\\star}$。报告 $\\lambda_{\\star} = \\exp(t_{k^{\\star}})$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序对应于用例 1 到 4 的四个 $\\lambda_{\\star}$ 值，四舍五入到六位小数，以逗号分隔列表的形式并用方括号括起来（例如，$[0.123456,0.000789,1.234568,0.010000]$）。\n- 不涉及物理单位。不出现角度。不使用百分比。\n- 答案必须是浮点数。\n\n您的实现必须是一个完整的、可运行的程序，该程序能够构建矩阵，执行上述计算，并以所需格式输出结果，无需任何外部输入。", "solution": "该问题被评估为有效，因为它科学上基于成熟的反问题和 Tikhonov 正则化理论，在数学和计算上是适定的，并为其求解提供了一套完整且一致的参数和指令。\n\n目标是通过实施 L-曲线准则，为四个不同的线性反问题找到最优正则化参数 $\\lambda_{\\star}$。L-曲线方法为平衡对观测数据的保真度与对解的先验信息的遵循度之间的权衡提供了一种启发式方法。\n\n一个线性反问题由方程 $b = A x_{\\mathrm{true}} + \\varepsilon$ 建模，其中 $b \\in \\mathbb{R}^m$ 是观测值，$A \\in \\mathbb{R}^{m \\times n}$ 是正向算子，$x_{\\mathrm{true}} \\in \\mathbb{R}^n$ 是未知的真实状态，而 $\\varepsilon \\in \\mathbb{R}^m$ 是测量噪声。在贝叶斯框架下，这对应于高斯似然 $\\mathcal{N}(b | A x, \\sigma^2 I)$。我们对解施加一个高斯先验 $x \\sim \\mathcal{N}(\\mu, (\\alpha^2 L^{\\top} L)^{-1})$，其中 $\\mu$ 是先验均值，$L$ 是一个编码结构信息（例如，平滑度）的算子。\n\n最大后验 (MAP) 估计是通过最小化负对数后验得到的，这产生了 Tikhonov 正则化泛函：\n$$\nJ_{\\lambda}(x) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert L (x - \\mu) \\rVert_2^2\n$$\n在这里，正则化参数 $\\lambda > 0$ 平衡了数据失配项 $\\lVert A x - b \\rVert_2^2$ 和正则化（或惩罚）项 $\\lVert L (x - \\mu) \\rVert_2^2$。该泛函的最小化子，记为 $x_{\\lambda}$，是正则化解。对 $J_{\\lambda}(x)$ 关于 $x$ 求梯度并将其设为零，得到正规方程，这是一个关于 $x_{\\lambda}$ 的线性方程组：\n$$\n(A^{\\top} A + \\lambda L^{\\top} L) x_{\\lambda} = A^{\\top} b + \\lambda L^{\\top} L \\mu\n$$\n对于每个候选的 $\\lambda$ 值，都必须求解这个系统。\n\nL-曲线准则基于在一系列 $\\lambda$ 值下，惩罚项的对数与失配项的对数的参数图。该曲线定义为 $\\mathcal{C}(\\lambda) = (\\log \\lVert A x_\\lambda - b \\rVert_2, \\log \\lVert L(x_\\lambda - \\mu) \\rVert_2)$。这条曲线通常呈“L”形。这个L形的拐角被认为代表了一个最优的平衡，此时失配项和惩罚项都不会过大。这个拐角对应于曲线上曲率最大的点。\n\n为了计算这一点，我们使用 $t = \\log \\lambda$ 对曲线进行重新参数化，因此曲线由 $c(t) = (\\xi(t), \\eta(t))$ 给出，其中：\n$$\n\\xi(t) = \\log \\lVert A x_{\\lambda(t)} - b \\rVert_2 \\quad \\text{and} \\quad \\eta(t) = \\log \\lVert L (x_{\\lambda(t)} - \\mu) \\rVert_2, \\quad \\text{with } \\lambda(t) = e^t\n$$\n这个平面曲线 $c(t)$ 的曲率 $\\kappa$ 由以下公式给出：\n$$\n\\kappa(t) = \\frac{\\lvert \\xi'(t) \\eta''(t) - \\eta'(t) \\xi''(t) \\rvert}{\\left( \\xi'(t)^2 + \\eta'(t)^2 \\right)^{3/2}}\n$$\n其中导数是关于 $t$ 的。\n\n计算步骤如下：\n1.  对于每个测试用例，按规定构造矩阵 $A$ 和 $L$，以及向量 $b$ 和 $\\mu$。\n2.  在区间 $[\\log(10^{-10}), \\log(10^2)]$ 上定义一个包含 $M=201$ 个点的 $t = \\log \\lambda$ 的均匀网格。设该网格为 $\\{t_k\\}_{k=0}^{M-1}$，步长为 $h = t_{k+1} - t_k$。\n3.  对于网格中的每个 $t_k$：\n    a. 计算 $\\lambda_k = e^{t_k}$。\n    b. 求解正规方程 $(A^{\\top} A + \\lambda_k L^{\\top} L) x_{\\lambda_k} = A^{\\top} b + \\lambda_k L^{\\top} L \\mu$ 以得到 $x_{\\lambda_k}$。\n    c. 计算失配范数 $\\rho_k = \\lVert A x_{\\lambda_k} - b \\rVert_2$ 和惩罚范数 $\\eta_k = \\lVert L (x_{\\lambda_k} - \\mu) \\rVert_2$。为避免对数运算的数值问题，在运算前加上机器 epsilon。\n    d. 计算对数-对数坐标：$\\xi_k = \\log(\\rho_k + \\epsilon_{\\text{mach}})$ 和 $\\eta_k = \\log(\\eta_k + \\epsilon_{\\text{mach}})$。\n4.  使用中心有限差分近似内部网格点（$k=1, \\dots, M-2$）上 $\\xi(t)$ 和 $\\eta(t)$ 的一阶和二阶导数：\n    $$\n    f'(t_k) \\approx \\frac{f_{k+1} - f_{k-1}}{2h}, \\quad f''(t_k) \\approx \\frac{f_{k+1} - 2f_k + f_{k-1}}{h^2}\n    $$\n5.  将这些数值导数代入曲率公式，计算每个内部点的 $\\kappa(t_k)$。\n6.  找出在内部点上使曲率 $\\kappa$ 最大化的索引 $k^{\\star}$。最优参数则为 $\\lambda_{\\star} = e^{t_{k^{\\star}}}$。\n\n此过程应用于四个测试用例中的每一个，这些用例探索了不同类型的正向算子（病态的、良态的、秩亏的）和先验信息（单位先验 vs. 光滑先验，零均值 vs. 非零均值）。最终输出将是计算出的四个 $\\lambda_{\\star}$ 值的列表。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve_for_lambda_star(A, L, b, mu, t_grid):\n    \"\"\"\n    Solves for the optimal lambda using the L-curve curvature criterion.\n    \"\"\"\n    n, p_dim = L.shape[0], L.shape[1]\n    \n    # Pre-compute constant matrices and vectors\n    AtA = A.T @ A\n    LtL = L.T @ L\n    Atb = A.T @ b\n    LtLmu = LtL @ mu\n    \n    misfit_norms = np.zeros(len(t_grid))\n    penalty_norms = np.zeros(len(t_grid))\n    \n    # Epsilon for numerical stability of log\n    mach_eps = np.finfo(float).eps\n\n    for i, t in enumerate(t_grid):\n        lambda_val = np.exp(t)\n        \n        # Solve the normal equations: (AtA + lambda*LtL)x = Atb + lambda*LtLmu\n        lhs_matrix = AtA + lambda_val * LtL\n        rhs_vector = Atb + lambda_val * LtLmu\n        \n        try:\n            x_lambda = linalg.solve(lhs_matrix, rhs_vector, assume_a='sym')\n        except linalg.LinAlgError:\n            # Fallback to pseudo-inverse for singular matrices if necessary\n            x_lambda = linalg.lstsq(lhs_matrix, rhs_vector)[0]\n\n        # Calculate misfit and penalty norms\n        misfit_norms[i] = linalg.norm(A @ x_lambda - b)\n        penalty_norms[i] = linalg.norm(L @ (x_lambda - mu))\n\n    # Compute log-log coordinates\n    xi = np.log(misfit_norms + mach_eps)\n    eta = np.log(penalty_norms + mach_eps)\n    \n    # Calculate derivatives using centered finite differences for interior points\n    h = t_grid[1] - t_grid[0]\n    \n    # First derivatives\n    xi_p = (xi[2:] - xi[:-2]) / (2 * h)\n    eta_p = (eta[2:] - eta[:-2]) / (2 * h)\n    \n    # Second derivatives\n    xi_pp = (xi[2:] - 2 * xi[1:-1] + xi[:-2]) / (h**2)\n    eta_pp = (eta[2:] - 2 * eta[1:-1] + eta[:-2]) / (h**2)\n    \n    # Calculate curvature for interior points\n    numerator = np.abs(xi_p * eta_pp - eta_p * xi_pp)\n    denominator = (xi_p**2 + eta_p**2)**(3/2)\n    \n    # Avoid division by zero\n    kappa = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator > mach_eps)\n    \n    # Find the index of maximum curvature among interior points\n    # Interior points' indices in the original grid are 1, 2, ..., M-2\n    # The argmax gives an index relative to the kappa array (0, 1, ..., M-3)\n    k_star_interior_idx = np.argmax(kappa)\n    \n    # Map back to the index in the original t_grid\n    t_star_idx = k_star_interior_idx + 1\n    \n    lambda_star = np.exp(t_grid[t_star_idx])\n    \n    return lambda_star\n\ndef solve():\n    # --- Common definitions for all test cases ---\n    n = 20\n    s = np.array([i / (n + 1) for i in range(1, n + 1)])\n    delta = 1.0 / n\n    w = 0.08\n    \n    # Gaussian blur matrix A0\n    i_minus_j = s[:, np.newaxis] - s[np.newaxis, :]\n    A0 = np.exp(-i_minus_j**2 / (2 * w**2)) * delta\n    \n    # True state x_true\n    x_true = np.sin(2 * np.pi * s) + 0.5 * s\n    \n    # Regularization operators\n    L_I = np.identity(n)\n    L_D = np.eye(n - 1, n, k=0) - np.eye(n - 1, n, k=1)\n\n    # Lambda grid\n    t_grid = np.linspace(np.log(10**-10), np.log(10**2), 201)\n    \n    test_cases_params = [\n        # Case 1: ill-conditioned, identity prior\n        {'A': A0, 'L': L_I, 'mu': np.zeros(n), 'sigma_noise': 1e-3},\n        # Case 2: ill-conditioned, smoothness prior, non-zero mean\n        {'A': A0, 'L': L_D, 'mu': 0.25 * s, 'sigma_noise': 1e-3},\n        # Case 3: well-conditioned, smoothness prior\n        {'A': np.identity(n), 'L': L_D, 'mu': np.zeros(n), 'sigma_noise': 5e-2},\n        # Case 4: rank-deficient, identity prior\n        {'A': A0.copy(), 'L': L_I, 'mu': np.zeros(n), 'sigma_noise': 1e-3},\n    ]\n    # Modify A for case 4\n    test_cases_params[3]['A'][:, -1] = test_cases_params[3]['A'][:, 0]\n\n    results = []\n    \n    i_vals = np.arange(1, n + 1)\n        \n    for params in test_cases_params:\n        A = params['A']\n        L = params['L']\n        mu = params['mu']\n        sigma_noise = params['sigma_noise']\n        \n        # Noise vector\n        epsilon = sigma_noise * (np.cos(7 * i_vals) / i_vals)\n        \n        # Data vector b\n        b = A @ x_true + epsilon\n        \n        lambda_star = solve_for_lambda_star(A, L, b, mu, t_grid)\n        results.append(lambda_star)\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3418439"}]}