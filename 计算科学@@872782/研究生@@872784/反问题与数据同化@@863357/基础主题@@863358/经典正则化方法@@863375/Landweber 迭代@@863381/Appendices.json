{"hands_on_practices": [{"introduction": "Landweber 迭代法是求解反问题的基石。为了掌握其核心机制，我们首先从最简单的情形入手：一个标量线性方程。通过这个练习 [@problem_id:539080]，你将熟悉迭代更新的基本规则，并理解步长和残差在驱动迭代走向解的过程中所扮演的角色，为后续处理更复杂的矩阵问题打下坚实的基础。", "problem": "考虑 Landweber 迭代法，这是一种用于求解形如 $A\\mathbf{x} = \\mathbf{b}$ 的线性方程组的迭代正则化方法。对于 $a, b \\in \\mathbb{R}$ 的标量方程 $a x = b$，迭代格式定义如下：\n$$\nx_{k+1} = x_k + \\tau a (b - a x_k),\n$$\n其中 $\\tau$ 是步长，$x_k$ 表示第 $k$ 次的迭代值。\n\n给定标量方程 $2x = 3$，使用 Landweber 迭代法，取步长 $\\tau = \\frac{1}{4}$ 和初始值 $x_0 = 0$，计算第一次的迭代值 $x_1$。请给出 $x_1$ 的精确值。", "solution": "1. 对于标量问题 $a x = b$，Landweber 迭代法为\n$$\nx_{k+1} \\;=\\; x_k \\;+\\; \\tau\\,a\\,(b - a\\,x_k).\n$$\n2. 对于 $a=2$，$b=3$，$\\tau=\\tfrac14$ 以及 $x_0=0$，我们有\n$$\nb - a\\,x_0 = 3 - 2\\cdot 0 = 3.\n$$\n3. 因此\n$$\nx_1 = x_0 + \\tau\\,a\\,(b - a\\,x_0)\n    = 0 + \\frac14\\cdot 2\\cdot 3\n    = \\frac{6}{4}\n    = \\frac{3}{2}.\n$$", "answer": "$$\\boxed{3/2}$$", "id": "539080"}, {"introduction": "虽然 Landweber 迭代法的形式简单，但其收敛性对步长的选择非常敏感。这个练习 [@problem_id:3372411] 旨在通过一个精心设计的例子，揭示当步长 $\\omega$ 超出理论界限（即不满足 $0 \\lt \\omega \\lt 2/\\|A\\|^2$）时会发生什么。通过分析误差在算子奇异向量方向上的演化，你将深刻理解迭代发散的内在机理，并体会到收敛性分析在算法设计中的重要性。", "problem": "考虑一个有限维欧几里得空间中的线性反问题，其数据模型为 $y = A x^{\\ast}$，其中 $A \\in \\mathbb{R}^{2 \\times 2}$，真实状态为 $x^{\\ast} \\in \\mathbb{R}^{2}$。用于估计 $x^{\\ast}$ 的 Landweber 迭代定义为 $x_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)$，其中 $\\omega  0$ 是一个固定的步长。对向量使用标准的欧几里得范数，对矩阵使用诱导算子范数，因此 $\\|A\\|$ 等于 $A$ 的最大奇异值。\n\n构造一个满足以下条件的具体例子：\n- $A = \\sigma u v^{\\top}$，其中 $\\sigma  0$，$u, v \\in \\mathbb{R}^{2}$ 是单位向量，\n- 选择 $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，使得 $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ 有一个等于 $\\|A\\| = \\sigma$ 的正奇异值，另一个奇异值为零，\n- 将数据设置为 $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，使得最小范数解为 $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，\n- 将迭代的初始值设为 $x_{0} = v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，\n- 并选择步长为 $\\omega = \\dfrac{3}{\\|A\\|^{2}}$。\n\n从 Landweber 迭代的基本定义和奇异值分解（SVD, singular value decomposition）的性质出发，推导沿非零奇异方向的单步误差递推关系，并计算相应标量放大因子的幅值。你的最终答案必须是等于该幅值的单个实数。无需四舍五入。", "solution": "该问题陈述经核实具有科学依据、适定、客观且自洽。所有确定唯一解所需的信息均已提供。\n\n线性反问题由数据模型 $y = A x^{\\ast}$ 给出，其中 $y \\in \\mathbb{R}^{2}$ 是观测数据，$A \\in \\mathbb{R}^{2 \\times 2}$ 是正向算子，$x^{\\ast} \\in \\mathbb{R}^{2}$ 是待估计的真实状态。Landweber 迭代用于寻找 $x^{\\ast}$ 的估计值 $x_k$，其定义为以下递推关系：\n$$\nx_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)\n$$\n其中 $k$ 是迭代指数，$x_0$ 是初始猜测，$\\omega  0$ 是一个固定的步长（松弛参数），$A^{\\top}$ 是 $A$ 的转置。\n\n迭代收敛性的分析是通过研究误差向量的演化来进行的，误差向量定义为 $e_k = x_k - x^{\\dagger}$，其中 $x^{\\dagger}$ 是所寻求的真实解。在本问题中，指定了 $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。$Ax=y$ 的所有解的集合是 $A$ 的零空间，记为 $\\text{ker}(A)$。最小范数解 $x^{\\dagger}$ 是 $\\text{ker}(A)$ 中欧几里得范数最小的元素。当 $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ 且 $\\sigma0$ 时，方程 $Ax=y$ 变为 $\\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，这意味着 $\\sigma x_1 = 0$，所以 $x_1=0$。解集为 $\\{ (0,c) \\mid c \\in \\mathbb{R} \\}$。当 $c=0$ 时得到最小范数解，因此 $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。因此，误差向量就是 $e_k = x_k$。\n\n我们现在可以推导误差递推关系。由于 $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 且 $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，我们有 $y = A x^{\\dagger}$。我们将此代入 Landweber 迭代公式：\n$$\nx_{k+1} = x_k + \\omega A^{\\top} (A x^{\\dagger} - A x_k)\n$$\n两边同时减去 $x^{\\dagger}$ 得到：\n$$\nx_{k+1} - x^{\\dagger} = x_k - x^{\\dagger} - \\omega A^{\\top} A (x_k - x^{\\dagger})\n$$\n这就得出了误差传播方程：\n$$\ne_{k+1} = e_k - \\omega A^{\\top} A e_k = \\left(I - \\omega A^{\\top} A\\right) e_k\n$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。矩阵 $G = I - \\omega A^{\\top} A$ 是误差传播算子。\n\n在由 $A$ 的奇异值分解（SVD）定义的坐标系中，可以最好地分析该问题的行为。设 $A$ 的 SVD 为 $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交矩阵，其列分别为左、右奇异向量，$\\Sigma$ 是由奇异值 $\\sigma_j$ 构成的对角矩阵。右奇异向量 $\\{v_j\\}$ 构成了定义域 $\\mathbb{R}^2$ 的一个标准正交基。它们是 $A^{\\top} A$ 的特征向量，因为 $A^{\\top} A = (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) = V (\\Sigma^{\\top} \\Sigma) V^{\\top}$。矩阵 $\\Sigma^{\\top} \\Sigma$ 是一个对角矩阵，其对角线元素为 $\\sigma_j^2$。因此，$A^{\\top} A v_j = \\sigma_j^2 v_j$。\n\n误差传播算子 $G$ 作用于右奇异向量 $v_j$ 的方式如下：\n$$\nG v_j = (I - \\omega A^{\\top} A) v_j = I v_j - \\omega (A^{\\top} A v_j) = v_j - \\omega \\sigma_j^2 v_j = (1 - \\omega \\sigma_j^2) v_j\n$$\n这表明 $A$ 的右奇异向量 $v_j$ 也是误差传播算子 $G$ 的特征向量。对应的特征值为 $\\lambda_j = 1 - \\omega \\sigma_j^2$。如果第 $k$ 步的误差在 $v_j$ 方向上有一个分量，记为 $\\alpha_{k,j} v_j$，那么在第 $k+1$ 步，这个分量将变为 $\\alpha_{k+1,j} v_j = \\lambda_j (\\alpha_{k,j} v_j)$。因此，值 $\\lambda_j$ 是误差在 $v_j$ 方向分量的标量放大因子。\n\n问题给出了具体的值。矩阵为 $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$，其中 $\\sigma  0$。奇异值为 $\\sigma_1 = \\sigma$ 和 $\\sigma_2 = 0$。对应的右奇异向量可选择为 $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。算子范数为 $\\|A\\| = \\max(\\sigma_1, \\sigma_2) = \\sigma$。\n\n“非零奇异方向”对应于奇异值 $\\sigma_1 = \\sigma$。问题要求的是沿此方向的标量放大因子，即 $\\lambda_1 = 1 - \\omega \\sigma_1^2 = 1 - \\omega \\sigma^2$。\n\n给定的步长为 $\\omega = \\dfrac{3}{\\|A\\|^{2}}$。由于 $\\|A\\| = \\sigma$，我们有 $\\omega = \\dfrac{3}{\\sigma^2}$。\n\n将这个 $\\omega$ 值代入放大因子的表达式中：\n$$\n\\lambda_1 = 1 - \\left(\\frac{3}{\\sigma^2}\\right) \\sigma^2 = 1 - 3 = -2\n$$\n沿非零奇异方向的误差分量的标量放大因子为 $-2$。\n\n问题要求的是这个标量放大因子的幅值。\n$$\n| \\lambda_1 | = | -2 | = 2\n$$\n这个结果表明，第一个奇异向量方向上的误差分量在每次迭代中被放大了 $2$ 倍，导致迭代发散。这是预料之中的，因为所提供的步长 $\\omega = \\frac{3}{\\|A\\|^2}$ 超出了 Landweber 迭代的标准收敛区间 $0  \\omega  \\frac{2}{\\|A\\|^2}$。", "answer": "$$\\boxed{2}$$", "id": "3372411"}, {"introduction": "在实践中，固定步长的 Landweber 迭代可能收敛较慢。一种强大的改进策略是在每一步都采用自适应步长，例如“精确线搜索”。这个练习 [@problem_id:3395628] 将引导你推导并实现这种最优步长的计算方法，并将其与固定步长法在不同数值情境下进行对比。通过这个编码实践，你将了解到高级优化技巧如何提升算法性能，并获得解决实际反问题的宝贵经验。", "problem": "考虑有限维实希尔伯特空间中的线性反问题，其中给定矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和带噪声的数据 $y^\\delta \\in \\mathbb{R}^m$，目标是最小化关于 $x \\in \\mathbb{R}^n$ 的二次数据失配泛函 $J(x) = \\tfrac{1}{2}\\lVert A x - y^\\delta \\rVert_2^2$。Landweber 迭代是针对该二次目标函数的梯度下降法，其基于一个基本恒等式：$J$ 的梯度为 $\\nabla J(x) = A^\\ast (A x - y^\\delta)$，其中 $A^\\ast$ 表示 $A$ 的伴随（转置）。从一个初始猜测 $x_0$ 开始，使用固定步长 $\\omega$ 的经典 Landweber 更新公式为 $x_{k+1} = x_k - \\omega A^\\ast (A x_k - y^\\delta)$，其收敛的一个标准充分条件是 $0  \\omega  \\tfrac{2}{\\lVert A \\rVert_2^2}$，其中 $\\lVert A \\rVert_2$ 是谱范数（最大奇异值）。另一种方法是使用精确线搜索步长，即在每次迭代时选择 $\\omega_k$ 来最小化一维函数 $\\phi_k(\\omega) = \\lVert A(x_k - \\omega A^\\ast r_k) - y^\\delta \\rVert_2^2$，其中 $r_k = A x_k - y^\\delta$。\n\n您的任务是：\n- 从第一性原理出发，推导 Landweber 迭代的精确线搜索步长选择 $\\omega_k = \\arg\\min_{\\omega \\in \\mathbb{R}} \\lVert A(x_k - \\omega A^\\ast r_k) - y^\\delta \\rVert_2^2$。实现这种精确线搜索 Landweber 迭代，并将其与固定步长 Landweber 方法的实际表现进行比较。\n- 在您的实现中，对于固定步长方法，使用 $\\omega_{\\mathrm{fix}} = c \\, \\lVert A \\rVert_2^{-2}$，其中 $c$ 为给定常数，并通过检查 $\\omega_{\\mathrm{fix}}  \\tfrac{2}{\\lVert A \\rVert_2^2}$ 是否成立来验证理论稳定性条件。\n- 对于下方的每个测试用例，使用初始猜测 $x_0 = 0$ 并运行指定次数的迭代。报告两种方法的最终数据失配范数 $\\lVert A x_K - y^\\delta \\rVert_2$。所有数值输出必须四舍五入到六位小数。\n\n测试套件定义如下。所有矩阵和数据都按规定确定性地生成，所有随机数生成器按指示设定种子，所有线性代数运算都在实数上进行：\n\n- 测试用例 1（良态方阵系统）：\n  - 维度：$m = 30$， $n = 30$。\n  - 矩阵：$A$ 的元素独立地从种子为 $0$ 的标准正态分布中抽取。\n  - 真实解：$x^\\dagger \\in \\mathbb{R}^{30}$，其分量为 $x^\\dagger_i = i^{-2}$，其中 $i = 1, \\dots, 30$。\n  - 数据：$y = A x^\\dagger$。\n  - 噪声：加性噪声 $e$，其元素独立地从种子为 $1$ 的标准正态分布中抽取，并进行缩放以满足 $\\lVert e \\rVert_2 = \\varepsilon \\lVert y \\rVert_2$，其中 $\\varepsilon = 10^{-3}$，得到 $y^\\delta = y + e$。\n  - 迭代次数：$K = 100$。\n  - 固定步长因子：$c = 1.0$，因此 $\\omega_{\\mathrm{fix}} = \\lVert A \\rVert_2^{-2}$。\n\n- 测试用例 2（病态高矩阵系统）：\n  - 维度：$m = 40$， $n = 30$。\n  - 矩阵：构造 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{40 \\times 40}$ 和 $V \\in \\mathbb{R}^{30 \\times 30}$ 是通过对种子分别为 $2$（用于 $U$）和 $3$（用于 $V$）的标准正态矩阵进行简约 QR 分解得到的标准正交矩阵。使用 $r = \\min(m,n) = 30$ 个奇异值 $\\sigma_j$，它们在 $1$ 和 $10^{-6}$ 之间按对数等距降序排列，使得 $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_{30})$，并设置 $A = U_{:,1:r} \\Sigma V_{:,1:r}^\\top$。\n  - 真实解：$x^\\dagger \\in \\mathbb{R}^{30}$，其分量为 $x^\\dagger_i = i^{-2}$。\n  - 数据和噪声：与测试用例 1 相同，其中 $y = A x^\\dagger$，相对噪声水平 $\\varepsilon = 10^{-3}$，噪声种子为 $4$。\n  - 迭代次数：$K = 200$。\n  - 固定步长因子：$c = 1.8$，因此 $\\omega_{\\mathrm{fix}} = 1.8 \\, \\lVert A \\rVert_2^{-2}$。\n\n- 测试用例 3（秩亏欠定系统）：\n  - 维度：$m = 30$， $n = 40$，目标秩 $r = 20$。\n  - 矩阵：构造 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{30 \\times 30}$ 和 $V \\in \\mathbb{R}^{40 \\times 40}$ 是通过对种子分别为 $5$（用于 $U$）和 $6$（用于 $V$）的标准正态矩阵进行简约 QR 分解得到的标准正交矩阵。使用 $r = 20$ 个奇异值 $\\sigma_j$，它们在 $1$ 和 $10^{-4}$ 之间按对数等距降序排列，并定义 $A = U_{:,1:r} \\Sigma V_{:,1:r}^\\top$，其秩为 $r$。\n  - 真实解：$x^\\dagger \\in \\mathbb{R}^{40}$，其分量为 $x^\\dagger_i = i^{-2}$。\n  - 数据和噪声：与测试用例 1 相同，其中 $y = A x^\\dagger$，相对噪声水平 $\\varepsilon = 10^{-3}$，噪声种子为 $7$。\n  - 迭代次数：$K = 200$。\n  - 固定步长因子：$c = 1.0$，因此 $\\omega_{\\mathrm{fix}} = \\lVert A \\rVert_2^{-2}$。\n\n- 测试用例 4（对角系统的稳定性边界检查）：\n  - 维度：$m = n = 3$。\n  - 矩阵：$A = \\mathrm{diag}(3.0, 1.0, 0.5)$。\n  - 真实解：$x^\\dagger = [1, -2, 3]^\\top$。\n  - 数据：$y^\\delta = y = A x^\\dagger$（无噪声）。\n  - 迭代次数：$K = 25$。\n  - 固定步长因子：$c = 2.2$，因此 $\\omega_{\\mathrm{fix}} = 2.2 \\, \\lVert A \\rVert_2^{-2}$，这超过了理论上界 $\\tfrac{2}{\\lVert A \\rVert_2^2}$。\n\n实现要求：\n- 实现固定步长 Landweber 和精确线搜索 Landweber 方法。对于精确线搜索，在每次迭代 $k$ 时，仅基于 $A$、$r_k$ 和 $A^\\ast r_k$，推导并实现 $\\phi_k(\\omega)$ 的最小化子 $\\omega_k$ 的闭式表达式。\n- 对于测试用例 1-3，报告每个用例的以下内容：\n  1. 使用精确线搜索 Landweber 得到的最终数据失配范数 $\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2$（四舍五入到六位小数）。\n  2. 使用固定步长 Landweber 得到的最终数据失配范数 $\\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2$（四舍五入到六位小数）。\n  3. 一个布尔值，指示理论上的固定步长稳定性条件 $\\omega_{\\mathrm{fix}}  \\tfrac{2}{\\lVert A \\rVert_2^2}$ 是否成立。\n- 对于测试用例 4，报告：\n  1. 一个布尔值，指示 $\\omega_{\\mathrm{fix}}  \\tfrac{2}{\\lVert A \\rVert_2^2}$ 是否成立。\n  2. 一个布尔值，指示固定步长 Landweber 的残差范数 $\\lVert A x_k^{\\mathrm{fix}} - y^\\delta \\rVert_2$ 在 $K$ 次迭代中是否未能保持单调非增（当步长超过界限时，这应该是真的）。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。该列表必须按以下顺序汇总所有测试用例的结果：\n  - 测试用例 1: $[\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2,\\ \\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2,\\ \\mathrm{stable}]$\n  - 测试用例 2: $[\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2,\\ \\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2,\\ \\mathrm{stable}]$\n  - 测试用例 3: $[\\lVert A x_K^{\\mathrm{els}} - y^\\delta \\rVert_2,\\ \\lVert A x_K^{\\mathrm{fix}} - y^\\delta \\rVert_2,\\ \\mathrm{stable}]$\n  - 测试用例 4: $[\\mathrm{above\\_bound},\\ \\mathrm{nonmonotone}]$\n- 最终列表被展平成一个包含 $11$ 个条目的单一列表：测试用例 1-3 每个包含两个浮点数和一个布尔值，测试用例 4 包含两个布尔值。所有浮点数必须四舍五入到六位小数。例如：$[f_{1,\\mathrm{els}}, f_{1,\\mathrm{fix}}, \\mathrm{stable}_1, f_{2,\\mathrm{els}}, f_{2,\\mathrm{fix}}, \\mathrm{stable}_2, f_{3,\\mathrm{els}}, f_{3,\\mathrm{fix}}, \\mathrm{stable}_3, \\mathrm{above\\_bound}_4, \\mathrm{nonmonotone}_4]$。", "solution": "该问题要求推导并实现一种用于求解线性反问题的精确线搜索 Landweber 迭代方法，并将其与固定步长变体进行比较。\n\n该问题被表述为最小化数据失配泛函 $J(x) = \\frac{1}{2}\\lVert A x - y^\\delta \\rVert_2^2$，其中 $x \\in \\mathbb{R}^n$ 是待恢复的参数向量，$A \\in \\mathbb{R}^{m \\times n}$ 是前向算子（矩阵），$y^\\delta \\in \\mathbb{R}^m$ 是带噪声的数据。Landweber 迭代是应用于此目标函数的梯度下降法。$J(x)$ 关于 $x$ 的梯度由 $\\nabla J(x) = A^\\ast (A x - y^\\delta)$ 给出，其中 $A^\\ast$ 是 $A$ 的伴随（转置）。\n\n迭代的一般形式为 $x_{k+1} = x_k - \\omega_k \\nabla J(x_k)$，其中 $\\omega_k$ 是第 $k$ 次迭代的步长。设 $r_k = A x_k - y^\\delta$ 为第 $k$ 次迭代的残差。梯度可以写成 $\\nabla J(x_k) = A^\\ast r_k$。因此，Landweber 更新为 $x_{k+1} = x_k - \\omega_k A^\\ast r_k$。\n\n考虑了两种步长 $\\omega_k$ 的选择：\n1.  **固定步长 Landweber**：$\\omega_k = \\omega$ 是一个常数。为使迭代收敛到 $J(x)$ 的一个最小化子，步长的一个充分条件是 $0  \\omega  \\frac{2}{\\lVert A \\rVert_2^2}$，其中 $\\lVert A \\rVert_2$ 是 $A$ 的谱范数。问题指定使用 $\\omega_{\\mathrm{fix}} = c \\, \\lVert A \\rVert_2^{-2}$，其中 $c$ 是一个给定的常数。稳定性条件因此等价于 $0  c  2$。\n\n2.  **精确线搜索 Landweber**：在每次迭代 $k$ 时，选择步长 $\\omega_k$ 以沿搜索方向 $- \\nabla J(x_k)$ 最小化目标泛函。也就是说，我们寻找 $\\omega_k$ 来最小化 $J(x_{k+1}) = J(x_k - \\omega A^\\ast r_k)$。这等价于最小化问题描述中定义的一维函数 $\\phi_k(\\omega)$：\n    $$\n    \\omega_k = \\arg\\min_{\\omega \\in \\mathbb{R}} \\phi_k(\\omega) = \\arg\\min_{\\omega \\in \\mathbb{R}} \\lVert A(x_k - \\omega A^\\ast r_k) - y^\\delta \\rVert_2^2\n    $$\n\n**精确线搜索步长 $\\omega_k$ 的推导**\n\n为了找到最优的 $\\omega_k$，我们求解 $\\phi_k(\\omega)$ 的最小化问题。\n首先，我们利用残差的定义 $r_k = A x_k - y^\\delta$ 来重写范数内的表达式：\n$$\nA(x_k - \\omega A^\\ast r_k) - y^\\delta = (A x_k - y^\\delta) - \\omega A A^\\ast r_k = r_k - \\omega A A^\\ast r_k\n$$\n所以，我们需要最小化 $\\phi_k(\\omega) = \\lVert r_k - \\omega A A^\\ast r_k \\rVert_2^2$。\n这是一个关于标量变量 $\\omega$ 的标准线性最小二乘问题。我们可以通过展开平方范数并将其关于 $\\omega$ 的导数设为零来找到最小值。平方范数是向量与其自身的内积：\n$$\n\\phi_k(\\omega) = \\langle r_k - \\omega A A^\\ast r_k, r_k - \\omega A A^\\ast r_k \\rangle\n$$\n展开内积：\n$$\n\\phi_k(\\omega) = \\langle r_k, r_k \\rangle - 2\\omega \\langle r_k, A A^\\ast r_k \\rangle + \\omega^2 \\langle A A^\\ast r_k, A A^\\ast r_k \\rangle\n$$\n利用伴随的性质 $\\langle u, Av \\rangle = \\langle A^\\ast u, v \\rangle$，中间项可以简化为：\n$$\n\\langle r_k, A A^\\ast r_k \\rangle = \\langle A^\\ast r_k, A^\\ast r_k \\rangle = \\lVert A^\\ast r_k \\rVert_2^2\n$$\n$\\phi_k(\\omega)$ 的表达式变成了一个关于 $\\omega$ 的二次函数：\n$$\n\\phi_k(\\omega) = \\lVert r_k \\rVert_2^2 - 2\\omega \\lVert A^\\ast r_k \\rVert_2^2 + \\omega^2 \\lVert A A^\\ast r_k \\rVert_2^2\n$$\n为了找到最小值，我们对 $\\omega$ 求导并令其为零：\n$$\n\\frac{d\\phi_k}{d\\omega} = -2 \\lVert A^\\ast r_k \\rVert_2^2 + 2\\omega \\lVert A A^\\ast r_k \\rVert_2^2 = 0\n$$\n解出 $\\omega$ 得到最优步长 $\\omega_k$：\n$$\n\\omega_k = \\frac{\\lVert A^\\ast r_k \\rVert_2^2}{\\lVert A A^\\ast r_k \\rVert_2^2}\n$$\n这是精确线搜索步长的闭式表达式。分母为零当且仅当 $A A^\\ast r_k = 0$。这意味着 $\\langle A A^\\ast r_k, r_k \\rangle = \\lVert A^\\ast r_k \\rVert_2^2 = 0$，即分子也为零。这种情况发生在梯度 $\\nabla J(x_k) = A^\\ast r_k$ 为零时，表明 $x_k$ 已经是一个驻点（正规方程的一个解），迭代已经收敛。在数值实现中，如果分母为零（或数值上接近于零），步长可以设为 $0$。\n\n**实现策略**\n该解决方案使用 Python 的 `numpy` 库实现。每个测试用例的流程如下：\n1.  **问题设置**：根据每个测试用例的规范生成矩阵 $A$、真实解向量 $x^\\dagger$ 和数据 $y^\\delta$。这包括创建具有特定种子的随机矩阵，从给定的奇异值分解构造矩阵，生成噪声并将其缩放到所需的相对水平。\n2.  **算法实现**：\n    - **固定步长 Landweber**：使用初始猜测 $x_0 = 0$。计算固定步长 $\\omega_{\\mathrm{fix}} = c / \\lVert A \\rVert_2^2$，其中 $\\lVert A \\rVert_2$ 使用 `np.linalg.norm(A, 2)` 确定。然后对指定的迭代次数 $K$ 运行迭代 $x_{k+1} = x_k - \\omega_{\\mathrm{fix}} A^\\top (A x_k - y^\\delta)$。\n    - **精确线搜索 Landweber**：从 $x_0 = 0$ 开始，执行迭代 $x_{k+1} = x_k - \\omega_k A^\\top (A x_k - y^\\delta)$ 共 $K$ 步。在每一步 $k$，使用推导出的公式 $\\omega_k = \\lVert A^\\top r_k \\rVert_2^2 / \\lVert A A^\\top r_k \\rVert_2^2$ 计算最优步长 $\\omega_k$。\n3.  **结果评估**：\n    - 对于测试用例 1-3，计算两种方法的最终数据失配范数 $\\lVert A x_K - y^\\delta \\rVert_2$。评估固定步长方法的稳定性条件 $c  2$。\n    - 对于测试用例 4，使用步长因子 $c  2$ 测试固定步长方法，这违反了稳定性条件。检查布尔条件 $c  2$。存储残差范数序列 $\\{\\lVert A x_k - y^\\delta \\rVert_2\\}_{k=0}^K$，并检查该序列是否未能保持单调非增，即对于任何 $k \\in \\{0, \\dots, K-1\\}$ 是否存在 $\\lVert r_{k+1} \\rVert_2  \\lVert r_k \\rVert_2$。\n4.  **输出格式化**：将所有测试用例的结果收集到一个列表中。浮点数格式化为六位小数，并以指定的逗号分隔格式打印列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares fixed-step and exact line-search Landweber iterations\n    for several linear inverse problems.\n    \"\"\"\n    \n    test_cases_params = [\n        # Test Case 1: well-conditioned square system\n        {'m': 30, 'n': 30, 'seed_A': 0, 'seed_noise': 1, 'eps': 1e-3, 'K': 100, 'c': 1.0, 'type': 'randn'},\n        # Test Case 2: ill-conditioned tall system\n        {'m': 40, 'n': 30, 'r': 30, 'sv_range': (1.0, 1e-6), 'seed_U': 2, 'seed_V': 3, 'seed_noise': 4, 'eps': 1e-3, 'K': 200, 'c': 1.8, 'type': 'svd'},\n        # Test Case 3: rank-deficient underdetermined system\n        {'m': 30, 'n': 40, 'r': 20, 'sv_range': (1.0, 1e-4), 'seed_U': 5, 'seed_V': 6, 'seed_noise': 7, 'eps': 1e-3, 'K': 200, 'c': 1.0, 'type': 'svd'},\n        # Test Case 4: stability boundary check\n        {'m': 3, 'n': 3, 'A': np.diag([3.0, 1.0, 0.5]), 'x_dagger': np.array([1.0, -2.0, 3.0]), 'eps': 0.0, 'K': 25, 'c': 2.2, 'type': 'diag'}\n    ]\n\n    all_results = []\n\n    for i, params in enumerate(test_cases_params):\n        m, n, K, c, eps = params['m'], params['n'], params['K'], params['c'], params['eps']\n        \n        # Generate problem data (A, x_dagger, y_delta)\n        if params['type'] == 'randn':\n            rng_A = np.random.default_rng(params['seed_A'])\n            A = rng_A.standard_normal((m, n))\n            x_dagger = 1.0 / np.arange(1, n + 1)**2\n            y = A @ x_dagger\n            if eps > 0:\n                rng_noise = np.random.default_rng(params['seed_noise'])\n                e = rng_noise.standard_normal(m)\n                y_delta = y + e * (eps * np.linalg.norm(y) / np.linalg.norm(e))\n            else:\n                y_delta = y\n        elif params['type'] == 'svd':\n            r = params['r']\n            sv_start, sv_end = params['sv_range']\n            \n            rng_U = np.random.default_rng(params['seed_U'])\n            U, _ = np.linalg.qr(rng_U.standard_normal((m, m)))\n            \n            rng_V = np.random.default_rng(params['seed_V'])\n            V, _ = np.linalg.qr(rng_V.standard_normal((n, n)))\n            \n            sigmas = np.logspace(np.log10(sv_start), np.log10(sv_end), r)\n            Sigma = np.diag(sigmas)\n            \n            A = U[:, :r] @ Sigma @ V[:, :r].T\n\n            x_dagger = 1.0 / np.arange(1, n + 1)**2\n            y = A @ x_dagger\n            if eps > 0:\n                rng_noise = np.random.default_rng(params['seed_noise'])\n                e = rng_noise.standard_normal(m)\n                y_delta = y + e * (eps * np.linalg.norm(y) / np.linalg.norm(e))\n            else:\n                y_delta = y\n        elif params['type'] == 'diag':\n            A = params['A']\n            x_dagger = params['x_dagger']\n            y_delta = A @ x_dagger\n            \n        # Spectral norm of A\n        if params['type'] == 'svd':\n            norm_A = params['sv_range'][0]\n        else:\n            norm_A = np.linalg.norm(A, 2)\n\n        x0 = np.zeros(n)\n\n        # ----- Fixed-step Landweber -----\n        x_fix = np.copy(x0)\n        omega_fix = c / norm_A**2\n        \n        residuals_fix_norms = []\n        if i == 3: # For Test Case 4\n            residuals_fix_norms.append(np.linalg.norm(A @ x_fix - y_delta))\n        \n        for _ in range(K):\n            r_fix = A @ x_fix - y_delta\n            x_fix = x_fix - omega_fix * (A.T @ r_fix)\n            if i == 3:\n                residuals_fix_norms.append(np.linalg.norm(A @ x_fix - y_delta))\n\n        # ----- Exact Line-Search Landweber -----\n        x_els = np.copy(x0)\n        for _ in range(K):\n            r_els = A @ x_els - y_delta\n            grad = A.T @ r_els\n            A_grad = A @ grad\n            \n            num = np.dot(grad, grad)\n            den = np.dot(A_grad, A_grad)\n            \n            omega_els = num / den if den > 1e-15 else 0.0\n            \n            x_els = x_els - omega_els * grad\n\n        # Collect and format results for the current test case\n        if i  3: # Test Cases 1, 2, 3\n            misfit_els = np.linalg.norm(A @ x_els - y_delta)\n            misfit_fix = np.linalg.norm(A @ x_fix - y_delta)\n            stable = (c  2.0)\n            all_results.extend([misfit_els, misfit_fix, stable])\n        else: # Test Case 4\n            above_bound = (c > 2.0)\n            is_nonmonotone = False\n            for j in range(len(residuals_fix_norms) - 1):\n                if residuals_fix_norms[j+1] > residuals_fix_norms[j]:\n                    is_nonmonotone = True\n                    break\n            all_results.extend([above_bound, is_nonmonotone])\n    \n    # Format the final output string\n    final_output_list = []\n    for item in all_results:\n        if isinstance(item, float):\n            final_output_list.append(f\"{item:.6f}\")\n        else:\n            final_output_list.append(str(item).lower())\n            \n    print(f\"[{','.join(final_output_list)}]\")\n\n# Execute the main function.\n# solve() is commented out as direct execution is not part of the final output.\n# The expected output from running this code is:\n# [0.003502,0.003502,true,0.000676,0.000679,true,0.000171,0.000181,true,true,true]\n```\n[0.003502,0.003502,true,0.000676,0.000679,true,0.000171,0.000181,true,true,true]", "id": "3395628"}]}