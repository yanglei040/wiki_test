## 引言
在科学与工程的众多领域，从数据中提取有意义的模型参数是一个核心任务，而最小二乘法为此提供了一个强大而普适的框架。然而，当问题规模庞大或本质上是病态时，直接求解变得不可行，迭代方法应运而生。最速下降法，作为梯度[优化方法](@entry_id:164468)的基石，以其概念上的简洁和直观性，为求解复杂的[最小二乘问题](@entry_id:164198)提供了切入点。尽管它广为人知，但其在实际应用中缓慢的收敛速度暴露了理论与实践间的鸿沟，这正是本文旨在深入探讨的问题。

本文将系统性地剖析[最速下降法](@entry_id:140448)在求解最小二乘问题时的原理、性能瓶颈及其改进策略。通过学习本文，您将不仅掌握算法的数学细节，更能理解其在不同学科背景下的应用之道。文章分为三个核心部分：首先，在“原理与机制”一章中，我们将深入推导算法，分析其收敛特性，并揭示其作为正则化工具的内在机制。接着，在“应用与跨学科联系”一章中，我们将展示该方法如何被应用于数据分析、[地球物理反演](@entry_id:749866)乃至机器学习等前沿领域，并探讨领域知识如何指导算法的改进。最后，通过“动手实践”部分提供的一系列编程练习，您将有机会亲手实现并验证这些理论，将抽象的数学概念转化为解决实际问题的能力。

## 原理与机制

本章在前一章介绍的背景下，深入探讨了求解[最小二乘问题](@entry_id:164198)的最速下降法的核心原理与机制。我们将从算法的基本形式出发，系统地推导其关键公式，分析其收敛特性，并将其置于[逆问题](@entry_id:143129)与数据同化的正则化框架中进行审视。最后，我们将介绍该方法的一些重要扩展与变体，以展示其在处理更复杂问题时的灵活性。

### [最速下降法](@entry_id:140448)的基本形式

在许多[数据同化](@entry_id:153547)和[逆问题](@entry_id:143129)中，我们的目标是最小化一个二次目标函数，该函数衡量模型状态与观测数据之间的失配程度。一个典型的形式是线性最小二乘问题，其[目标函数](@entry_id:267263) $J(x)$ 定义为：
$$
J(x) = \frac{1}{2} \|Ax - b\|^2
$$
其中 $x \in \mathbb{R}^n$ 是待估计的状态向量， $b \in \mathbb{R}^m$ 是观测数据向量， $A \in \mathbb{R}^{m \times n}$ 是将[状态空间](@entry_id:177074)映射到观测空间的正演算子（或称[观测算子](@entry_id:752875)）。这里的范数 $\|\cdot\|$ 是欧几里得范数。

为了找到最小化 $J(x)$ 的 $x$，我们可以采用迭代方法。**[最速下降法](@entry_id:140448)**（Steepest Descent Method）是最直观的一种迭代策略。其思想是从当前点 $x_k$ 出发，沿着使[目标函数](@entry_id:267263)下降最快的方向移动一小步，以期到达函数值更低的新点 $x_{k+1}$。在欧几里得空间中，函数下降最快的方向是其梯度的反方向。

首先，我们需要计算目标函数 $J(x)$ 的梯度。通过展开范数平方并对 $x$ 求导，我们得到：
$$
J(x) = \frac{1}{2}(Ax - b)^{\top}(Ax - b) = \frac{1}{2}(x^{\top}A^{\top}Ax - 2b^{\top}Ax + b^{\top}b)
$$
其梯度为：
$$
\nabla J(x) = A^{\top}Ax - A^{\top}b = A^{\top}(Ax - b)
$$
因此，在第 $k$ 次迭代中，[最速下降](@entry_id:141858)方向为 $p_k = -\nabla J(x_k)$。迭代更新的公式为：
$$
x_{k+1} = x_k + \alpha_k p_k = x_k - \alpha_k \nabla J(x_k)
$$
其中 $\alpha_k > 0$ 是步长，它决定了我们沿着负梯度方向走多远。

一个关键问题是如何选择最优的步长 $\alpha_k$。**精确[线性搜索](@entry_id:633982)**（Exact Line Search）是一种策略，它选择的 $\alpha_k$ 能够在[一维搜索](@entry_id:172782)方向上使[目标函数](@entry_id:267263)达到最小值。换言之，$\alpha_k$ 是以下一元函数 $\varphi(\alpha)$ 的最小化子：
$$
\varphi(\alpha) = J(x_k - \alpha \nabla J(x_k)) = \frac{1}{2} \|A(x_k - \alpha \nabla J(x_k)) - b\|^2
$$
为了求解[最优步长](@entry_id:143372)，我们可以将 $\varphi(\alpha)$ 对 $\alpha$ 求导并令其为零。为了方便，我们记梯度 $g_k = \nabla J(x_k)$ 和数据空间中的残差 $r_k^{\text{data}} = Ax_k - b$。注意到 $g_k = A^{\top}r_k^{\text{data}}$。
$$
\varphi(\alpha) = \frac{1}{2} \|(Ax_k - b) - \alpha Ag_k\|^2 = \frac{1}{2} \|r_k^{\text{data}} - \alpha Ag_k\|^2
$$
对 $\alpha$ 求导得到：
$$
\frac{d\varphi}{d\alpha} = (r_k^{\text{data}} - \alpha Ag_k)^{\top}(-Ag_k) = 0
$$
整理后可得：
$$
\alpha_k (Ag_k)^{\top}(Ag_k) = (r_k^{\text{data}})^{\top}(Ag_k)
$$
这给出了精确[线性搜索](@entry_id:633982)步长的表达式：
$$
\alpha_k = \frac{(r_k^{\text{data}})^{\top}Ag_k}{\|Ag_k\|^2}
$$
利用 $g_k = A^{\top}r_k^{\text{data}}$ 的关系，以及标量等于其[转置的性质](@entry_id:148302)，分子 $(r_k^{\text{data}})^{\top}Ag_k$ 可以重写为 $(Ag_k)^{\top}r_k^{\text{data}} = g_k^{\top}A^{\top}r_k^{\text{data}}$。由于 $A^{\top}r_k^{\text{data}} = g_k$，该表达式简化为 $g_k^{\top}g_k = \|g_k\|^2$。于是，我们得到了一个更简洁且常用的形式 [@problem_id:3422243]：
$$
\alpha_k = \frac{\|g_k\|^2}{\|Ag_k\|^2}
$$
这个步长公式具有深刻的几何意义。[最优性条件](@entry_id:634091) $\frac{d\varphi}{d\alpha} = 0$ 意味着更新后的残差 $r_{k+1}^{\text{data}} = r_k^{\text{data}} - \alpha_k Ag_k$ 与搜索向量在数据空间的映射 $Ag_k$ 正交。这表明，每一步迭代都在数据空间中，将当前残差投影到某个方向上，以最大程度地减小其分量。

然而，当算子 $A$ 是**[秩亏](@entry_id:754065)**（rank-deficient）的，即 $A$ 的列向量[线性相关](@entry_id:185830)时，[最速下降法](@entry_id:140448)可能会遇到问题。如果 $Ag_k = 0$ 且 $g_k \neq 0$，则步长公式的分母为零。这种情况发生在 $g_k$ 位于 $A$ 的[零空间](@entry_id:171336)（Null Space）中。从梯度的定义 $g_k = A^{\top}(Ax_k - b)$ 来看，$g_k$ 总是位于 $A^{\top}$ 的值域（Range）中。因此，只有当 $A^{\top}$ 的值域和 $A$ 的零空间有非零交集时，才会出现 $Ag_k = 0$ 且 $g_k \neq 0$ 的情况。更常见的情形是，如果数据残差 $Ax_k-b$ 的分量恰好与 $A$ 的值域正交，即 $Ax_k-b \in \mathrm{Null}(A^{\top})$，那么梯度 $g_k = A^{\top}(Ax_k - b)$ 将为零，迭代会提前终止，即使此时的解 $x_k$ 可能远非[最小范数解](@entry_id:751996) [@problem_id:3422243]。这反映了数据对参数 $x$ 的某些分量不具有[可辨识性](@entry_id:194150)。

### [收敛性分析](@entry_id:151547)

理解最速下降法的性能至关重要，特别是它为什么在某些情况下收敛缓慢。

#### 连续梯度流视角

我们可以将离散的迭代过程与一个连续的[时间演化](@entry_id:153943)系统联系起来，这为我们提供了深刻的洞察。考虑如下的**[梯度流](@entry_id:635964)**（gradient flow）[常微分方程](@entry_id:147024)（ODE）：
$$
\frac{dx(t)}{dt} = -\nabla J(x(t))
$$
这个方程描述了一个点 $x(t)$ 在[目标函数](@entry_id:267263) $J(x)$ 的“地形”上沿着最陡峭的下坡路径连续滑动的过程。对于我们的二次目标函数，$\nabla J(x) = Hx - c$，其中 $H = A^{\top}A$ 是Hessian矩阵，$c = A^{\top}b$。梯度流方程变为：
$$
\dot{x}(t) = -(Hx(t) - c)
$$
[最速下降法](@entry_id:140448)迭代 $x_{k+1} = x_k - \alpha \nabla J(x_k)$（这里暂时考虑常数步长 $\alpha$）可以被看作是使用**前向欧拉法**（Forward Euler method）以时间步长 $\Delta t = \alpha$ 对上述[梯度流](@entry_id:635964)ODE进行的离散化 [@problem_id:3422239]。

这个联系不仅仅是形式上的。ODE数值积分的[稳定性理论](@entry_id:149957)可以被直接用于分析[最速下降法](@entry_id:140448)的收敛性。对于一个线性ODE $\dot{e} = -He$，[前向欧拉法](@entry_id:141238)的稳定性要求步长 $\alpha$ 必须满足 $|1 - \alpha \lambda_i|  1$ 对 $H$ 的所有[特征值](@entry_id:154894) $\lambda_i$ 成立。由于 $H = A^{\top}A$ 是对称半正定的，其[特征值](@entry_id:154894) $\lambda_i \ge 0$。稳定性条件简化为 $\alpha \lambda_i  2$，即 $\alpha  2/\lambda_i$。为了保证对所有模式都稳定，步长必须受限于 $H$ 的最大[特征值](@entry_id:154894) $\lambda_{\max}$：
$$
0  \alpha  \frac{2}{\lambda_{\max}(H)} = \frac{2}{\|A\|_2^2}
$$
其中 $\|A\|_2$ 是矩阵 $A$ 的[谱范数](@entry_id:143091)（最大[奇异值](@entry_id:152907)）。这为常数步长最速下降法的收敛提供了一个严格的理论[上界](@entry_id:274738)。

#### [收敛率](@entry_id:146534)与[病态问题](@entry_id:137067)

最速下降法虽然保证收敛（在合适的步长下），但其收敛速度可能非常缓慢。其收敛性能与Hessian矩阵 $H = A^{\top}A$ 的**[条件数](@entry_id:145150)**（condition number）$\kappa(H)$ 密切相关。[条件数](@entry_id:145150)定义为最大[特征值](@entry_id:154894)与[最小特征值](@entry_id:177333)之比：$\kappa(H) = \lambda_{\max}(H) / \lambda_{\min}(H)$。

对于二次目标函数，使用精确[线性搜索](@entry_id:633982)的[最速下降法](@entry_id:140448)，目标函数值的误差 $J(x_k) - J(x^\star)$ 每一步迭代的收缩因子（contraction factor）最坏情况下由以下公式给出：
$$
\frac{J(x_{k+1}) - J(x^\star)}{J(x_k) - J(x^\star)} \le \left(\frac{\kappa(H) - 1}{\kappa(H) + 1}\right)^2
$$
这个经典结果（源于Kantorovich不等式）揭示了一个残酷的现实：当 $\kappa(H)$ 很大时，收缩因子 $\left(\frac{\kappa-1}{\kappa+1}\right)^2$ 非常接近1，这意味着收敛会非常缓慢。一个很大的[条件数](@entry_id:145150)通常被称为**病态**（ill-conditioned）问题。在逆问题中，由于物理过程的平滑效应，算子 $A$ 往往具有快速衰减的奇异值，导致 $H=A^\top A$ 的[特征值](@entry_id:154894)谱范围极广，从而条件数极大。

例如，假设一个问题的算子 $A$ 的最大奇异值为 $\sigma_{\max}=10$，最小奇异值为 $\sigma_{\min}=1$。那么Hessian矩阵 $H$ 的[特征值](@entry_id:154894)在 $\lambda_{\min}=1^2=1$ 和 $\lambda_{\max}=10^2=100$ 之间，[条件数](@entry_id:145150)为 $\kappa(H)=100$。此时，最坏情况下的收缩因子为 $(\frac{100-1}{100+1})^2 = (\frac{99}{101})^2 \approx 0.96$。这意味着每一步迭代，误差最多只能减少约4%。若要将误差降低 $10^8$ 倍，需要的迭代次数 $m$ 大约满足 $(0.96)^m \le 10^{-8}$，这需要数百次迭代 [@problem_id:3422273]。这正是[最速下降法](@entry_id:140448)在许多实际应用中效率低下的根源。

### 作为[正则化方法](@entry_id:150559)的[最速下降法](@entry_id:140448)

在处理[逆问题](@entry_id:143129)时，尤其是在数据含有噪声的情况下，我们通常不希望迭代至完全收敛。这是因为迭代过程[后期](@entry_id:165003)主要是在拟合数据中的噪声，导致解的范数过大且出现不真实的[振荡](@entry_id:267781)。从这个角度看，最速下降法本身可以被视为一种[正则化技术](@entry_id:261393)。

#### [迭代正则化](@entry_id:750895)与提前终止

对于[病态问题](@entry_id:137067)，[最速下降法](@entry_id:140448)的迭代过程有一个有趣的特性：在初始几次迭代中，算法主要重构由 $A$ 的较大奇异值（对应 $H$ 的较大[特征值](@entry_id:154894)）所张成的信号分量。随着迭代次数 $k$ 的增加，算法才逐渐开始拟合与较小[奇异值](@entry_id:152907)相关的分量。而这些与小[奇异值](@entry_id:152907)相关的分量往往被噪声严重污染。

因此，通过在迭代早期（即在噪声被显著放大之前）停止迭代，我们可以得到一个正则化的解。这种技术被称为**提前终止**（Early Stopping），而迭代次数 $k$ 则扮演了**正则化参数**的角色。选择一个合适的 $k$ 可以在拟合数据和控制解的稳定性之间取得平衡。

#### [终止准则](@entry_id:136282)：差异性原理

那么，如何确定何时“提前”终止呢？如果关于数据噪声水平 $\delta = \|\varepsilon\|_2$ 的信息是已知的，**差异性原理**（Discrepancy Principle）提供了一个有效的准则。该原理指出，一个好的解 $x_k$ 应该使得其预测的数据 $Ax_k$ 与实际观测数据 $b$ 之间的[残差范数](@entry_id:754273)与噪声水平相当，即：
$$
\|Ax_k - b\|_2 \le \tau \delta
$$
其中 $\tau > 1$ 是一个安全因子，通常取略大于1的值（如1.01或1.2）。算法在第一次满足该条件的迭代 $k$ 处终止。这个准则的直观意义是：我们不应该试图将[数据拟合](@entry_id:149007)得比噪声水平更精确，因为这样做无异于拟合噪声 [@problem_id:3422232]。

除了差异性原理，其他常用的[终止准则](@entry_id:136282)还包括：
- **梯度范数减小**：当梯度范数 $\|g_k\|_2$ 相对于初始梯度范数 $\|g_0\|_2$ 足够小时（例如，$\|g_k\|_2 \le \epsilon \|g_0\|_2$），表示接近一个驻点。
- **最大迭代次数**：为算法设置一个迭代上限 $k_{\max}$，以防止无限循环并控制计算成本。

在实践中，这些准则常常被结合使用，并设定优先级。

#### [谱滤波](@entry_id:755173)与偏置-[方差](@entry_id:200758)权衡

为了更深刻地理解提前终止的正则化效应，我们可以借助奇异值分解（SVD）$A = U\Sigma V^\top$ 进行谱分析。在SVD基下，最小二乘问题的“理想”解（无噪声且 $A$ 可逆时）的分量为 $\frac{u_i^\top b}{\sigma_i}$。[正则化方法](@entry_id:150559)可以被看作是对这些分量应用了一个**[谱滤波](@entry_id:755173)器**（spectral filter）$\varphi_i$。

对于提前 $k$ 步终止的[最速下降法](@entry_id:140448)（也称为[Landweber迭代](@entry_id:751130)），其等效的[谱滤波](@entry_id:755173)器因子为 [@problem_id:3422247]：
$$
\varphi_i^{\text{ES}}(k, \alpha) = 1 - (1 - \alpha \sigma_i^2)^k
$$
而对于经典的**[吉洪诺夫正则化](@entry_id:140094)**（Tikhonov regularization），其解为 $x_\lambda = \arg\min_x \|Ax-b\|^2 + \lambda \|x\|^2$，对应的滤波器为：
$$
\varphi_i^{\text{Tik}}(\lambda) = \frac{\sigma_i^2}{\sigma_i^2 + \lambda}
$$
- 当[奇异值](@entry_id:152907) $\sigma_i$ 很大时，两种方法的 $\varphi_i$ 都接近1，意味着信号分量被保留。
- 当 $\sigma_i$ 很小时，$\varphi_i^{\text{Tik}} \to 0$，有效地抑制了噪声。对于提前终止法，只要 $k$ 不是太大，$(1 - \alpha \sigma_i^2)^k$ 也接近1，所以 $\varphi_i^{\text{ES}} \approx k \alpha \sigma_i^2 \to 0$，同样起到了抑制作用。

这种滤波效应可以用**偏置-[方差](@entry_id:200758)权衡**（Bias-Variance Tradeoff）来量化。一个正则化解的均方误差（MSE）可以分解为偏置的平方和[方差](@entry_id:200758)之和。
- **偏置**（Bias）是正则化解的期望与真实解 $x^\dagger$ 之间的差异，主要由对小奇异值分量的抑制引起（因为 $\varphi_i  1$）。
- **[方差](@entry_id:200758)**（Variance）是解因数据中的噪声而产生的波动，主要由噪声通过 $\frac{\varphi_i}{\sigma_i}$ 项的放大引起。

对于第 $i$ 个奇异分量，[均方误差](@entry_id:175403)为：
$$
\text{MSE}_i = \underbrace{((1 - \varphi_i)x_i^\dagger)^2}_{\text{偏置}^2} + \underbrace{\left(\frac{\varphi_i}{\sigma_i}\right)^2 \sigma_\varepsilon^2}_{\text{方差}}
$$
其中 $x_i^\dagger$ 是真实解的谱系数，$\sigma_\varepsilon^2$ 是噪声[方差](@entry_id:200758)。[正则化参数](@entry_id:162917)（$k$ 或 $\lambda$）的选择就是在减小[方差](@entry_id:200758)（通过使 $\varphi_i/\sigma_i$ 变小）和增加偏置（通过使 $1-\varphi_i$ 变大）之间做出权衡，以最小化总的均方误差 [@problem_id:3422247]。

### 扩展与变体

虽然基础的最速下降法存在收敛慢的问题，但其核心思想可以被扩展和改进，以适应更广泛的应用场景。

#### [预处理](@entry_id:141204)

既然收敛速度受制于Hessian矩阵 $H$ 的[条件数](@entry_id:145150)，一个自然的想法是“改造”问题，使其具有更好的[条件数](@entry_id:145150)。这就是**预处理**（Preconditioning）的思想。它通过一个[变量替换](@entry_id:141386) $x = M\hat{x}$ 来实现，其中 $M$ 是一个可逆的[预处理](@entry_id:141204)矩阵。原始问题 $\min \|Ax-b\|^2$ 变为 $\min \|AM\hat{x}-b\|^2$。新的Hessian矩阵变为 $\hat{H} = M^{\top}A^{\top}AM = M^{\top}HM$。我们的目标是选择 $M$ 使得 $\kappa(\hat{H}) \ll \kappa(H)$。

一个简单而有效的预处理策略是**[对角缩放](@entry_id:748382)**（diagonal scaling）。如果 $A$ 的列[向量范数](@entry_id:140649)差异很大，会导致 $H=A^\top A$ 的对角元素大小不一，通常这是病态的一个信号。例如，如果 $A = \begin{pmatrix} 1  0 \\ 0  \epsilon \end{pmatrix}$ 且 $\epsilon \ll 1$，则 $H = \begin{pmatrix} 1  0 \\ 0  \epsilon^2 \end{pmatrix}$，[条件数](@entry_id:145150)为 $1/\epsilon^2$，非常大。我们可以选择一个对角预处理器 $M$ 来均衡 $A$ 的列范数。令 $S$ 为一个对角矩阵，其对角元素等于 $A$ 相应列的范数，然后令 $M=S^{-1}$。这样，新的矩阵 $A' = AS^{-1}$ 的列都具有单位范数，通常会显著改善问题的[条件数](@entry_id:145150)，从而加速收敛 [@problem_id:3422227]。

#### [约束优化](@entry_id:635027)：投影最速下降法

在许多应用中，待求参数 $x$ 需要满足一些物理约束，例如 $c^\top x = d$ 这样的[线性等式约束](@entry_id:637994)。[最速下降法](@entry_id:140448)可以被修改以处理这类问题。**投影最速下降法**（Projected Steepest Descent）是一种常用方法。

其核心思想是，在每一步迭代中，将原始的负梯度方向 $p_k = -g_k$ **投影**（project）到约束所定义的可行域的切空间上。对于线性约束 $c^\top x = 1$，其切空间是所有满足 $c^\top v = 0$ 的向量 $v$ 构成的[子空间](@entry_id:150286)。将 $-g_k$ 正交投影到该[子空间](@entry_id:150286)上，得到一个既能保证函数下降又能维持可行性的搜索方向 $d_k$。然后，沿着这个投影方向 $d_k$ 进行[线性搜索](@entry_id:633982)以更新解。由于搜索方向位于切空间内，只要初始点是可行的，任何步长的移动都将保持可行性 [@problem_id:3422262]。

#### [块坐标下降法](@entry_id:636917)

当状态向量 $x$ 的维度 $n$ 非常巨大时，计算完整的梯度 $g_k = A^\top(Ax_k-b)$ 可能成本高昂。**[块坐标下降法](@entry_id:636917)**（Block-Coordinate Descent）提供了一种替代方案。它将向量 $x$ 分解成若干个块（或单个坐标），然后在每次迭代中，只选择一个块进行优化，同时保持其他块不变。

例如，对于 $x=(x_1, x_2)$，一个循环可以包括：
1. 固定 $x_2$，求解关于 $x_1$ 的最小化问题。
2. 固定更新后的 $x_1$，求解关于 $x_2$ 的最小化问题。

对于二次问题，每个子问题都是一个更小维度的[最小二乘问题](@entry_id:164198)，求解成本更低。这种方法的收敛性取决于变量块之间的耦合程度。如果耦合很弱（在Hessian矩阵 $H$ 中表现为非对角块的范数很小），[块坐标下降法](@entry_id:636917)可能非常有效。但如果耦合很强，其[收敛速度](@entry_id:636873)可能会远慢于全梯度方法 [@problem_id:3422272]。

#### [混合方法](@entry_id:163463)：信赖域与[共轭梯度](@entry_id:145712)

最速下降法还可以与**信赖域**（Trust Region, TR）方法结合。TR方法在每一步迭代中，定义一个以当前点 $x_k$ 为中心的“信赖域”（通常是一个球体 $\|s\| \le \Delta_k$），并在这个区域内求解一个二次模型 $m_k(s)$ 的近似最小化问题来获得步长 $s$。如果实际函数下降量与模型预测的下降量比值 $\rho_k$ 令人满意，则接受该步，并可能扩大信赖域半径 $\Delta_{k+1}$；否则，拒绝该步并缩小半径。将[最速下降](@entry_id:141858)方向与信赖域约束结合，可以得到一种稳健的[混合算法](@entry_id:171959)，它通过动态调整步长上界（即信赖域半径）来确保[全局收敛性](@entry_id:635436) [@problem_id:3422271]。

最后，值得强调的是，尽管最速下降法是理解梯度[优化方法](@entry_id:164468)的基础，但对于无约束的二次[优化问题](@entry_id:266749)，**[共轭梯度法](@entry_id:143436)**（Conjugate Gradient, CG）在理论和实践上都远优于最速下降法。CG方法通过巧妙地构造一系列相互A-正交的搜索方向，能够在至多 $n$ 次迭代内（在精确算术下）找到 $n$ 维二次函数的精确最小值。它避免了[最速下降法](@entry_id:140448)的锯齿状收敛路径，其[收敛率](@entry_id:146534)同样与条件数有关，但关系为 $\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}$，这比最速下降法的 $\frac{\kappa-1}{\kappa+1}$ 有了巨大的改进 [@problem_id:3422258]。在许多数据同化和逆问题的求解器中，CG及其变体（如PCG，[预处理共轭梯度法](@entry_id:753674)）是核心的迭代引擎。