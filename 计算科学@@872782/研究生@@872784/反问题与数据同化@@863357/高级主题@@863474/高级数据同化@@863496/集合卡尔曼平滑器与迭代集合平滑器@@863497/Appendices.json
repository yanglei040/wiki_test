{"hands_on_practices": [{"introduction": "本节的第一个实践将通过实现经典的双时间集合卡尔曼平滑器 (EnKS) 来巩固我们的理解。通过为一个简单的线性系统编写更新代码，你将直接体验未来观测的信息如何被同化以改进过去状态的估计，这正是平滑的本质。这个练习聚焦于信息在时间上向后传播的核心机制 [@problem_id:3379469]。", "problem": "为一个线性标量状态空间系统实现一个双时间集合卡尔曼平滑器（EnKS），并在吸收下一个时刻的单个观测值后，计算初始时刻的平滑集合均值。使用以下基础和定义来推导您的算法。\n\n该系统是线性的、标量的和高斯的：\n- 状态演化：$x_1 = a \\, x_0 + w_0$，其中 $w_0 \\sim \\mathcal{N}(0, q)$，$x_0$ 表示时间 $t = 0$ 时的状态，$x_1$ 表示时间 $t = 1$ 时的状态。\n- 观测：$y_1 = H \\, x_1 + v_1$，其中 $H = 1$，$v_1 \\sim \\mathcal{N}(0, r)$，$y_1$ 是时间 $t = 1$ 时的观测值。\n\n假设使用一个大小为 $N$ 的集合来表示时间 $t=0$ 时的先验，其先验分布为 $x_0 \\sim \\mathcal{N}(m_0, P_0)$。目标是使用双时间 EnKS，在吸收时间 $t=1$ 的观测值 $y_1$ 后，计算时间 $t=0$ 时的平滑集合均值。\n\n使用以下基本且经过充分检验的定义：\n- 向量 $\\{z^{(n)}\\}_{n=1}^N$ 的样本均值为 $\\bar{z} = \\frac{1}{N} \\sum_{n=1}^N z^{(n)}$。\n- 集合向量 $\\{x^{(n)}\\}_{n=1}^N$ 和 $\\{y^{(n)}\\}_{n=1}^N$ 之间的无偏样本协方差为 $\\operatorname{Cov}(x, y) = \\frac{1}{N - 1} \\sum_{n=1}^N \\left(x^{(n)} - \\bar{x}\\right)\\left(y^{(n)} - \\bar{y}\\right)$，无偏样本方差为 $\\operatorname{Var}(x) = \\operatorname{Cov}(x, x)$。\n- 随机集合卡尔曼滤波器（EnKF）对每个集合成员独立使用扰动观测值 $\\tilde{y}^{(n)} = y_1 + \\epsilon^{(n)}$（其中 $\\epsilon^{(n)} \\sim \\mathcal{N}(0, r)$），以确保在线性高斯情况下后验方差的正确性。\n\n根据这些基本原则，实现以下算法纲要：\n- 在时间 $t=0$，从 $\\mathcal{N}(m_0, P_0)$ 中抽取一个初始集合 $\\{x_0^{(n)}\\}_{n=1}^N$。\n- 将每个集合成员向前传播，以获得时间 $t=1$ 时的预报集合：$x_1^{f,(n)} = a \\, x_0^{(n)} + w_0^{(n)}$，其中 $w_0^{(n)} \\sim \\mathcal{N}(0, q)$ 且相互独立。\n- 在时间 $t=1$ 使用扰动观测值执行随机 EnKF 分析，通过基于集合协方差的最佳线性更新，从 $\\{x_1^{f,(n)}\\}_{n=1}^N$ 获得分析集合 $\\{x_1^{a,(n)}\\}_{n=1}^N$。\n- 应用双时间 EnKS 更新，使用基于 $\\{x_0^{(n)}\\}_{n=1}^N$ 和 $\\{x_1^{f,(n)}\\}_{n=1}^N$ 之间集合互协方差的最佳线性预测器，将时间 $t=1$ 的分析增量映射回时间 $t=0$。如果时间 $t=1$ 的样本方差在数值上为零，则将平滑增益设为 $0$。\n- 输出平滑集合均值 $\\bar{x}_0^{s} = \\frac{1}{N} \\sum_{n=1}^N x_0^{s,(n)}$。\n\n使用固定参数 $a = 0.9$、$q = 0.1$、$r = 0.4$、$N = 10$ 和 $H = 1$。为保证可复现性，对于索引为 $i$（从 $i = 0$ 开始）的每个测试用例，将随机种子设置为 $1000 + i$。当从 $\\mathcal{N}(m_0, P_0)$ 采样时，如果 $P_0 = 0$，则将所有集合成员精确地设为 $m_0$。\n\n测试套件。对于下面的每个参数集，计算吸收 $y_1$ 后时间 $t=0$ 时的平滑集合均值：\n- 情况 1：$(m_0, P_0, y_1) = (1.0, 1.0, 0.5)$，种子 $= 1000$。\n- 情况 2：$(m_0, P_0, y_1) = (0.0, 0.0, 0.0)$，种子 $= 1001$。\n- 情况 3：$(m_0, P_0, y_1) = (-2.0, 4.0, 1.0)$，种子 $= 1002$。\n- 情况 4：$(m_0, P_0, y_1) = (0.5, 0.5, 0.45)$，种子 $= 1003$。\n- 情况 5：$(m_0, P_0, y_1) = (0.0, 1.0, 5.0)$，种子 $= 1004$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个包含 $5$ 个平滑均值的列表，四舍五入到 $6$ 位小数，格式为用方括号括起来的逗号分隔列表（例如，$[x_1, x_2, x_3, x_4, x_5]$）。", "solution": "该问题要求为一个线性、标量、高斯状态空间系统实现一个双时间集合卡尔曼平滑器（EnKS）。目标是在吸收时间 $t=1$ 的单个观测值后，计算初始时间 $t=0$ 时状态的平滑集合均值。该解决方案源于基于集合的数据同化的基本原理。\n\n该系统定义如下：\n1.  状态演化模型：$x_1 = a \\, x_0 + w_0$，其中 $x_k$ 是时间 $t=k$ 时的状态，$a$ 是一个常数系数，$w_0$ 是过程噪声，从正态分布 $w_0 \\sim \\mathcal{N}(0, q)$ 中抽取。\n2.  观测模型：$y_1 = H \\, x_1 + v_1$，其中 $y_1$ 是 $t=1$ 时的观测值，$H$ 是观测算子（给定为 $H=1$），$v_1$ 是观测噪声，$v_1 \\sim \\mathcal{N}(0, r)$。\n\n该算法分几个步骤进行，从一个代表 $t=0$ 时状态先验知识的初始集合开始，并根据 $t=1$ 时的观测值顺序更新这些知识。\n\n**步骤 1：集合初始化**\n关于时间 $t=0$ 时状态的先验知识由一个正态分布 $x_0 \\sim \\mathcal{N}(m_0, P_0)$ 给出。该分布由一个包含 $N$ 个成员的集合 $\\{x_0^{(n)}\\}_{n=1}^N$ 来表示。每个成员都是从该分布中抽取的样本。对于非零的先验方差 $P_0 > 0$，样本生成为 $x_0^{(n)} \\sim \\mathcal{N}(m_0, P_0)$。在先验方差为零的特殊情况下，即 $P_0=0$，状态是完全已知的，所有集合成员都设置为均值，$x_0^{(n)} = m_0$ 对所有 $n \\in \\{1, \\dots, N\\}$ 成立。\n\n**步骤 2：预报至时间 $t=1$**\n初始集合的每个成员都使用状态演化模型在时间上向前传播。为每个成员添加一个独立实现的过程噪声 $w_0^{(n)} \\sim \\mathcal{N}(0, q)$。这就产生了在 $t=1$ 时的预报集合，表示为 $\\{x_1^{f,(n)}\\}_{n=1}^N$：\n$$x_1^{f,(n)} = a \\, x_0^{(n)} + w_0^{(n)}$$\n\n**步骤 3：时间 $t=1$ 的分析**\n分析步骤使用观测值 $y_1$ 来更新预报集合。这里采用随机集合卡尔曼滤波器（EnKF）。其核心思想是从样本统计量中计算出类似卡尔曼的增益，并更新每个集合成员。为了确保后验集合具有统计上正确的方差，使用了扰动观测值。\n\n每个集合成员的分析更新公式为：\n$$x_1^{a,(n)} = x_1^{f,(n)} + K_1 \\left( \\tilde{y}^{(n)} - H x_1^{f,(n)} \\right)$$\n其中 $\\{x_1^{a,(n)}\\}_{n=1}^N$ 是 $t=1$ 时的分析集合。\n\n此更新的组成部分是：\n-   **扰动观测值**：$\\tilde{y}^{(n)} = y_1 + \\epsilon^{(n)}$，其中每个 $\\epsilon^{(n)}$ 是从观测噪声分布 $\\epsilon^{(n)} \\sim \\mathcal{N}(0, r)$ 中独立抽取的。\n-   **预报观测值**：由于 $H=1$，每个成员的预报观测值为 $y_1^{f,(n)} = H x_1^{f,(n)} = x_1^{f,(n)}$。\n-   **卡尔曼增益 $K_1$**：该增益使用样本协方差计算。它是状态与观测值的样本协方差与包含观测误差的预报观测值的样本方差之比。\n    $$K_1 = \\frac{\\operatorname{Cov}(x_1^f, y_1^f)}{\\operatorname{Var}(y_1^f) + r}$$\n    使用无偏样本方差算子 $\\operatorname{Var}(\\cdot)$ 且 $H=1$，上式变为：\n    $$K_1 = \\frac{\\operatorname{Var}(x_1^f)}{\\operatorname{Var}(x_1^f) + r}$$\n    其中 $\\operatorname{Var}(x_1^f) = \\frac{1}{N-1} \\sum_{n=1}^N \\left(x_1^{f,(n)} - \\bar{x}_1^f\\right)^2$，而 $\\bar{x}_1^f$ 是预报集合的均值。\n\n综合这些，分析更新公式为：\n$$x_1^{a,(n)} = x_1^{f,(n)} + \\frac{\\operatorname{Var}(x_1^f)}{\\operatorname{Var}(x_1^f) + r} \\left( y_1 + \\epsilon^{(n)} - x_1^{f,(n)} \\right)$$\n\n**步骤 4：平滑至时间 $t=0$**\n集合卡尔曼平滑器（EnKS）更新 $t=0$ 时的初始集合，使其与 $t=1$ 时的分析结果保持一致。这是通过将 $t=1$ 时的分析增量 $(x_1^{a,(n)} - x_1^{f,(n)})$ 回归到 $t=0$ 时的状态来实现的。该回归基于 $t=0$ 和 $t=1$ 时状态之间的互协方差。\n\n$t=0$ 时的平滑集合 $\\{x_0^{s,(n)}\\}_{n=1}^N$ 由下式给出：\n$$x_0^{s,(n)} = x_0^{(n)} + K_0 \\left( x_1^{a,(n)} - x_1^{f,(n)} \\right)$$\n\n-   **平滑增益 $K_0$**：该增益是从集合中计算出的最佳线性预测器，它将 $t=1$ 时的异常映射到 $t=0$ 时的异常。\n    $$K_0 = \\frac{\\operatorname{Cov}(x_0, x_1^f)}{\\operatorname{Var}(x_1^f)}$$\n    无偏样本互协方差为 $\\operatorname{Cov}(x_0, x_1^f) = \\frac{1}{N-1} \\sum_{n=1}^N \\left(x_0^{(n)} - \\bar{x}_0\\right)\\left(x_1^{f,(n)} - \\bar{x}_1^f\\right)$。\n    \n这里处理一个特殊情况：如果 $\\operatorname{Var}(x_1^f)$ 在数值上为零，这意味着预报集合没有离散度，无法执行更新。在这种情况下，平滑增益 $K_0$ 设置为 $0$。如果初始集合没有离散度（$P_0=0$），也会发生这种情况，这将导致 $\\operatorname{Cov}(x_0, x_1^f) = 0$，因此 $K_0 = 0$。\n\n**步骤 5：最终平滑均值**\n最终结果是 $t=0$ 时平滑集合的均值：\n$$\\bar{x}_0^{s} = \\frac{1}{N} \\sum_{n=1}^N x_0^{s,(n)}$$\n\n该算法将针对每个测试用例实现，使用指定的参数：$a=0.9$、$q=0.1$、$r=0.4$、$N=10$、$H=1$。为保证可复现性，随机数生成器将被设定种子。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a two-time Ensemble Kalman Smoother (EnKS) for a linear scalar system\n    and computes the smoothed ensemble mean at the initial time for several test cases.\n    \"\"\"\n    # Fixed parameters from the problem statement\n    a = 0.9\n    q = 0.1\n    r = 0.4\n    N = 10\n    H = 1.0  # Observation operator\n\n    # Test cases from the problem statement\n    test_cases = [\n        (1.0, 1.0, 0.5),   # Case 1: (m0, P0, y1)\n        (0.0, 0.0, 0.0),   # Case 2\n        (-2.0, 4.0, 1.0),  # Case 3\n        (0.5, 0.5, 0.45),  # Case 4\n        (0.0, 1.0, 5.0),   # Case 5\n    ]\n\n    results = []\n    for i, (m0, P0, y1) in enumerate(test_cases):\n        # Set the random seed for reproducibility\n        seed = 1000 + i\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Ensemble Initialization at t=0\n        if P0 == 0.0:\n            x0_ensemble = np.full(N, m0)\n        else:\n            x0_ensemble = rng.normal(loc=m0, scale=np.sqrt(P0), size=N)\n\n        # Step 2: Forecast to t=1\n        w0_noise = rng.normal(loc=0.0, scale=np.sqrt(q), size=N)\n        x1_forecast_ensemble = a * x0_ensemble + w0_noise\n\n        # Step 3: Analysis at t=1 (Stochastic EnKF)\n        # Calculate sample variance of the forecast ensemble\n        var_x1_forecast = np.var(x1_forecast_ensemble, ddof=1)\n\n        # Calculate Kalman gain K1\n        # Denominator is Var(H*x_f) + r = Var(x_f) + r, since H=1\n        K1 = var_x1_forecast / (var_x1_forecast + r)\n        \n        # Perturb observations\n        obs_perturbations = rng.normal(loc=0.0, scale=np.sqrt(r), size=N)\n        perturbed_obs = y1 + obs_perturbations\n        \n        # Apply analysis update to each ensemble member\n        # Forecast observations are H * x1_forecast_ensemble = x1_forecast_ensemble\n        innovation = perturbed_obs - x1_forecast_ensemble\n        x1_analysis_ensemble = x1_forecast_ensemble + K1 * innovation\n        \n        # Step 4: Smoothing to t=0 (EnKS)\n        # Calculate smoother gain K0\n        # K0 = Cov(x0, x1_f) / Var(x1_f)\n        if np.isclose(var_x1_forecast, 0.0):\n            K0 = 0.0\n        else:\n            # np.cov returns a 2x2 matrix for 2 1D inputs\n            # [[Var(x0), Cov(x0, x1f)], [Cov(x0, x1f), Var(x1f)]]\n            cov_matrix = np.cov(x0_ensemble, x1_forecast_ensemble, ddof=1)\n            cov_x0_x1f = cov_matrix[0, 1]\n            K0 = cov_x0_x1f / var_x1_forecast\n\n        # Apply smoothing update\n        analysis_increment = x1_analysis_ensemble - x1_forecast_ensemble\n        x0_smoothed_ensemble = x0_ensemble + K0 * analysis_increment\n        \n        # Step 5: Compute Smoothed Mean\n        smoothed_mean_x0 = np.mean(x0_smoothed_ensemble)\n        \n        # Round result to 6 decimal places and store\n        results.append(np.round(smoothed_mean_x0, 6))\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3379469"}, {"introduction": "现实世界中的系统通常是非线性的，标准的 EnKS 可能不足以应对。本练习通过实现单次高斯-牛顿 (Gauss-Newton) 迭代来介绍迭代式集合平滑器 (IES) 框架。你将处理一个带有非线性观测模型的问题，学习如何在当前估计处对模型进行线性化，并利用该近似来迭代地改进解，从而逐步逼近真实的后验模式 [@problem_id:3379443]。", "problem": "在一个迭代集成平滑器 (IES) 框架内，为非线性标量观测模型实现单次高斯-牛顿迭代。基本方法必须从具有高斯先验和高斯数据误差模型的贝叶斯逆问题公式出发。具体来说，考虑一个参数 $x \\in \\mathbb{R}$，其先验为 $x \\sim \\mathcal{N}(0,1)$，观测算子为 $h(x) = x^2$。观测数据为 $y \\in \\mathbb{R}$，数据误差协方差为标量 $R > 0$。根据高斯先验和高斯似然定义负对数后验目标函数 $J(x)$。通过在当前集成均值处对 $h(x)$ 进行线性化，并使用得到的近似梯度和海森矩阵计算一个单一更新，该更新将统一应用于所有集成成员，从而实现一次高斯-牛顿迭代。您必须从先验 $x \\sim \\mathcal{N}(0,1)$ 中独立抽取一个大小为 $N$ 的集成，计算当前集成均值 $m$，基于在 $m$ 处的线性化执行一个高斯-牛顿步骤，通过将计算出的步长平移每个成员来更新集成，并报告更新后的集成均值 $m_{\\text{updated}}$。\n\n计算过程必须是自包含且可复现的。为了可复现性，为每个测试用例使用一个指定的伪随机数生成器种子。程序必须通过以下步骤实现迭代集成平滑器 (IES) 步骤：\n- 使用给定的种子从先验 $\\mathcal{N}(0,1)$ 中抽取 $N$ 个独立样本。\n- 计算当前集成均值 $m$。\n- 在 $m$ 处对 $h(x)$ 进行线性化，并从贝叶斯目标构建高斯-牛顿搜索方向，然后通过向每个成员添加相同的标量增量来更新所有集成成员。\n- 在这次单次迭代后报告更新后的集成均值 $m_{\\text{updated}}$。\n\n本问题不涉及物理单位。不使用角度。不使用百分比。目标输出为实数。\n\n测试套件：\n提供以下参数集 $(y, R, N, \\text{seed})$ 的结果：\n1. $(1.0, 0.1, 20, 0)$\n2. $(1.0, 1.0, 20, 1)$\n3. $(0.0, 0.1, 20, 2)$\n4. $(1.0, 0.001, 20, 3)$\n5. $(4.0, 0.1, 3, 4)$\n6. $(1.0, 0.1, 1, 5)$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含这六个测试用例的更新后集成均值，以逗号分隔的列表形式并用方括号括起来。每个数字必须四舍五入到六位小数。例如：\"[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901]\"。", "solution": "该解决方案遵循以下逻辑步骤：\n\n1.  将问题介绍为贝叶斯逆问题。\n2.  定义负对数后验目标函数 $J(x)$。展示其从先验和似然的推导过程。\n3.  针对此目标函数推导高斯-牛顿方法。\n    -   陈述高斯-牛顿更新的一般形式。\n    -   计算梯度 $g(x) = \\nabla J(x)$。\n    -   计算近似高斯-牛顿海森矩阵 $H_{GN}(x)$。\n4.  描述迭代集成平滑器 (IES) 步骤。\n    -   解释线性化是在集成均值 $m$ 处执行的。\n    -   展示如何使用 $g(m)$ 和 $H_{GN}(m)$ 计算更新步长 $\\delta m$。\n    -   解释此更新统一应用于所有集成成员，因此更新后的均值即为 $m_{\\text{updated}} = m + \\delta m$。\n5.  总结给定测试用例的最终算法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a single Gauss-Newton iteration for an Iterative Ensemble Smoother (IES)\n    to solve a nonlinear scalar inverse problem.\n    \"\"\"\n    \n    test_cases = [\n        (1.0, 0.1, 20, 0),\n        (1.0, 1.0, 20, 1),\n        (0.0, 0.1, 20, 2),\n        (1.0, 0.001, 20, 3),\n        (4.0, 0.1, 3, 4),\n        (1.0, 0.1, 1, 5)\n    ]\n\n    results = []\n    \n    for y, R, N, seed in test_cases:\n        # Step 1: Draw an ensemble from the prior distribution.\n        # The prior is x ~ N(0, 1).\n        rng = np.random.default_rng(seed)\n        ensemble_x = rng.standard_normal(N)\n\n        # Step 2: Compute the current ensemble mean.\n        m = np.mean(ensemble_x)\n\n        # Step 3: Compute the Gauss-Newton update step based on the ensemble mean.\n        # The update step delta_m = -[H_GN(m)]^-1 * g(m),\n        # where g is the gradient and H_GN is the Gauss-Newton approximation of the Hessian\n        # of the negative log-posterior objective function J(x).\n        \n        # Gradient of J(x) at m: g(m) = m - 2*m/R * (y - m^2)\n        gradient_at_m = m - (2.0 * m / R) * (y - m**2)\n\n        # Gauss-Newton approximation of the Hessian of J(x) at m: H_GN(m) = 1 + 4*m^2/R\n        hessian_approx_at_m = 1.0 + (4.0 * m**2) / R\n        \n        # Check for division by zero, although it is mathematically impossible here\n        # since R > 0 implies hessian_approx_at_m >= 1.\n        if hessian_approx_at_m == 0:\n            # This case should not be reached. If it is, no update is performed.\n            delta_m = 0.0\n        else:\n            delta_m = -gradient_at_m / hessian_approx_at_m\n\n        # Step 4: Compute the updated ensemble mean.\n        # A uniform update is applied to all ensemble members: x_i_new = x_i + delta_m.\n        # The new mean is therefore m_updated = mean(x_i + delta_m) = mean(x_i) + delta_m = m + delta_m.\n        m_updated = m + delta_m\n        \n        results.append(m_updated)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3379443"}, {"introduction": "对于强非线性问题，即使是迭代方法也可能难以收敛。这个实践深入探讨了一种先进且广泛应用的技术：似然退火 (likelihood tempering)，它通常通过多重资料同化集合平滑器 (ES-MDA) 实现。你将推导退火与观测误差协方差膨胀之间的数学等价性，然后确定一个应用这些更新的最优策略，从而深入了解如何设计能够平衡准确性和稳定性的鲁棒平滑器 [@problem_id:3379440]。", "problem": "考虑一个贝叶斯逆问题，其状态向量为 $x \\in \\mathbb{R}^{n}$，先验密度为 $p(x) \\propto \\exp\\!\\big(-\\tfrac{1}{2}(x-m^{b})^{\\top}(C^{b})^{-1}(x-m^{b})\\big)$，均值为 $m^{b} \\in \\mathbb{R}^{n}$，协方差为 $C^{b} \\in \\mathbb{R}^{n \\times n}$，观测模型为 $y = h(x) + \\xi$，其中 $h:\\mathbb{R}^{n} \\to \\mathbb{R}^{p}$ 可能非线性，观测噪声 $\\xi \\sim \\mathcal{N}(0,R)$，$R \\in \\mathbb{R}^{p \\times p}$ 为正定矩阵，实现的观测数据为 $y^{\\mathrm{obs}} \\in \\mathbb{R}^{p}$。在迭代集合平滑器中，似然回火将似然函数提升到 $\\beta \\in (0,1]$ 的幂次。请基于以下基本依据进行推导：高斯似然函数 $p(y^{\\mathrm{obs}} \\mid x) \\propto \\exp\\!\\big(-\\tfrac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\big)$，贝叶斯定理，以及对于线性 $h$ 的卡尔曼分析会最小化一个由先验项和数据失配项组成的二次代价函数。\n\n任务 A. 从贝叶斯定理和高斯形式的似然函数出发，推导如何通过因子 $\\beta \\in (0,1]$ 进行回火等价于在似然函数中将观测误差协方差 $R$ 替换为 $R/\\beta$。解释在多重数据同化集合平滑器 (ES-MDA) 中，这种等价性如何意味着在同化步骤 $m \\in \\{1,\\dots,M\\}$ 使用膨胀的协方差 $\\alpha_{m} R$ 实现了一个回火指数 $\\beta_{m} = 1/\\alpha_{m}$，并证明条件 $\\sum_{m=1}^{M} \\beta_{m} = 1$ 是在 $M$ 步后恢复未回火后验分布所必需的。\n\n任务 B. 假设存在一个包含 $M$ 次回火 ES-MDA 更新的序列，其指数为 $\\{\\beta_{m}\\}_{m=1}^{M}$，对应的协方差为 $\\{R/\\beta_{m}\\}_{m=1}^{M}$。将每一步 $m$ 的竞争误差建模如下：\n- 局部线性化（建模）误差贡献，其上界为 $c_{L}\\,\\beta_{m}^{2}$，其中常数 $c_{L} > 0$ 不依赖于 $m$。\n- 随机集合更新中的采样误差贡献，其上界为 $c_{S}/(N\\,\\beta_{m})$，其中常数 $c_{S} > 0$ 不依赖于 $m$，$N \\in \\mathbb{N}$ 是集合大小。\n\n在这些建模假设下，确定回火方案 $\\{\\beta_{m}\\}_{m=1}^{M}$，以最小化总误差上界 $\\sum_{m=1}^{M}\\big(c_{L}\\,\\beta_{m}^{2} + c_{S}/(N\\,\\beta_{m})\\big)$，并满足约束条件 $\\beta_{m} > 0$（对所有 $m$）和 $\\sum_{m=1}^{M} \\beta_{m} = 1$。将你的方案表示为每个 $\\beta_{m}$ 仅关于 $M$ 的单个闭式解析表达式。说明你的答案在何种条件下有效，并简要证明为什么它能在这个理想化模型中平衡线性化误差和采样误差。\n\n你的最终答案必须是每个 $\\beta_{m}$ 仅关于 $M$ 的单个闭式解析表达式。不需要数值四舍五入。不涉及物理单位。除了所要求的表达式外，不要在最终答案中包含任何额外的评论。", "solution": "该问题被验证为具有科学依据、良态且客观的。它建立在贝叶斯数据同化的标准数学框架内，并提出了一个可解的约束优化问题。所有定义和条件都是自洽和一致的。\n\n### 任务 A：回火、协方差膨胀和 ES-MDA\n\n我们首先分析回火似然的结构及其与多重数据同化集合平滑器 (ES-MDA) 框架的联系。\n\n后验概率密度函数 $p(x \\mid y^{\\mathrm{obs}})$ 由贝叶斯定理给出：\n$$\np(x \\mid y^{\\mathrm{obs}}) \\propto p(y^{\\mathrm{obs}} \\mid x) \\, p(x)\n$$\n其中 $p(y^{\\mathrm{obs}} \\mid x)$ 是似然函数，$p(x)$ 是先验。问题指定了高斯先验和高斯似然。似然函数由下式给出：\n$$\np(y^{\\mathrm{obs}} \\mid x) \\propto \\exp\\left(-\\frac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\n$$\n似然回火涉及将似然函数提升到 $\\beta \\in (0,1]$ 的幂次。因此，回火后验分布正比于 $[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} p(x)$。我们来考察回火似然项：\n$$\n[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} \\propto \\left[\\exp\\left(-\\frac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\\right]^{\\beta}\n$$\n利用性质 $(\\exp(a))^b = \\exp(ab)$，上式变为：\n$$\n[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} \\propto \\exp\\left(-\\frac{\\beta}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}R^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\n$$\n为了证明这等价于将观测误差协方差 $R$ 替换为一个新的协方差 $R_{\\text{new}}$，我们需要将指数写成 $-\\frac{1}{2}(\\dots)^{\\top}R_{\\text{new}}^{-1}(\\dots)$ 的形式。我们可以将缩放后的逆协方差重写为：\n$$\n\\beta R^{-1} = (R/\\beta)^{-1}\n$$\n这里我们使用了矩阵恒等式 $(cA)^{-1} = c^{-1}A^{-1}$，其中 $c$ 是一个标量。将此代入回火似然的表达式中，得到：\n$$\n[p(y^{\\mathrm{obs}} \\mid x)]^{\\beta} \\propto \\exp\\left(-\\frac{1}{2}(y^{\\mathrm{obs}} - h(x))^{\\top}(R/\\beta)^{-1}(y^{\\mathrm{obs}} - h(x))\\right)\n$$\n该表达式具有高斯似然的精确函数形式，只是观测误差协方差被替换为 $R/\\beta$。这完成了任务 A 的第一部分。\n\n对于第二部分，ES-MDA 执行 $M$ 次类卡尔曼更新序列。在每一步 $m \\in \\{1, \\dots, M\\}$，更新都是使用一个膨胀的观测误差协方差矩阵 $\\alpha_m R$ 来执行的，其中 $\\alpha_m \\geq 1$。根据刚刚推导出的等价关系，使用协方差 $\\alpha_m R$ 等价于使用一个回火指数 $\\beta_m$，使得：\n$$\n\\alpha_m R = \\frac{R}{\\beta_m}\n$$\n由于 $R$ 是可逆的，我们可以在右侧乘以 $R^{-1}$ 得到 $\\alpha_m I = \\beta_m^{-1} I$，这意味着 $\\beta_m = 1/\\alpha_m$。\n\n最后，我们证明为什么需要条件 $\\sum_{m=1}^{M} \\beta_{m} = 1$。ES-MDA 过程通过使用修改后的似然函数多次同化相同的数据 $y^{\\mathrm{obs}}$ 来顺序更新状态。设第 $m=1$ 步的先验为 $p_0(x) = p(x)$。第一步之后的后验 $p_1(x)$ 是使用回火指数 $\\beta_1$ 得到的：\n$$\np_1(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1} p_0(x) = [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1} p(x)\n$$\n这个后验 $p_1(x)$ 接着作为第二步的先验。第二步之后的后验 $p_2(x)$ 为：\n$$\np_2(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_2} p_1(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_2} \\left( [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1} p(x) \\right) = [p(y^{\\mathrm{obs}} \\mid x)]^{\\beta_1+\\beta_2} p(x)\n$$\n通过归纳法，经过 $M$ 次这样的步骤后，最终的后验分布 $p_M(x)$ 由下式给出：\n$$\np_M(x) \\propto [p(y^{\\mathrm{obs}} \\mid x)]^{\\sum_{m=1}^{M} \\beta_m} p(x)\n$$\n为了恢复真实、未回火的贝叶斯后验 $p(x \\mid y^{\\mathrm{obs}})$，似然函数上的总指数必须等于 $1$。因此，回火指数序列必须满足条件：\n$$\n\\sum_{m=1}^{M} \\beta_{m} = 1\n$$\n\n### 任务 B：最优回火方案\n\n我们的任务是找到回火指数序列 $\\{\\beta_m\\}_{m=1}^M$，以最小化总误差上界，并满足约束条件。要最小化的目标函数是：\n$$\nJ(\\beta_1, \\dots, \\beta_M) = \\sum_{m=1}^{M}\\left(c_{L}\\,\\beta_{m}^{2} + \\frac{c_{S}}{N\\,\\beta_{m}}\\right)\n$$\n最小化需满足约束条件 $\\beta_m > 0$（对所有 $m \\in \\{1, \\dots, M\\}$）以及在任务 A 中推导的一致性条件：\n$$\n\\sum_{m=1}^{M} \\beta_{m} = 1\n$$\n这是一个约束优化问题。我们可以使用拉格朗日乘数法来解决它。拉格朗日函数 $\\mathcal{L}$ 定义为：\n$$\n\\mathcal{L}(\\beta_1, \\dots, \\beta_M, \\lambda) = \\sum_{m=1}^{M}\\left(c_{L}\\,\\beta_{m}^{2} + \\frac{c_{S}}{N\\,\\beta_{m}}\\right) - \\lambda \\left(\\sum_{m=1}^{M} \\beta_{m} - 1\\right)\n$$\n为了找到最小值，我们将 $\\mathcal{L}$ 对每个 $\\beta_k$（$k = 1, \\dots, M$）的偏导数设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\beta_k} = \\frac{\\partial}{\\partial \\beta_k} \\left(c_{L}\\,\\beta_{k}^{2} + \\frac{c_{S}}{N\\,\\beta_{k}}\\right) - \\lambda = 0\n$$\n$$\n2c_{L}\\,\\beta_{k} - \\frac{c_{S}}{N\\,\\beta_{k}^{2}} - \\lambda = 0\n$$\n这个方程必须对每个 $k \\in \\{1, \\dots, M\\}$ 都成立。该方程的形式对所有 $\\beta_k$ 都是相同的。这意味着在最优点，所有的 $\\beta_k$ 都必须相等。我们用 $\\beta$ 表示这个共同的值。\n$$\n\\beta_1 = \\beta_2 = \\dots = \\beta_M = \\beta\n$$\n一个更严谨的理由是，目标函数是关于每个 $\\beta_m$ 的相同、严格凸函数的和（因为对于 $\\beta_m > 0$，二阶导数 $2c_L + 2c_S/(N\\beta_m^3)$ 在 $c_L>0, c_S>0, N>0$ 的条件下是严格为正的），并且约束条件相对于 $\\beta_m$ 的排列是对称的。对于这类对称问题，唯一的最小值必然出现在所有变量都相等的对称点上。\n\n将 $\\beta_m = \\beta$ 代入约束方程中：\n$$\n\\sum_{m=1}^{M} \\beta = M\\beta = 1\n$$\n解出 $\\beta$，我们得到每个指数的最优值：\n$$\n\\beta = \\frac{1}{M}\n$$\n因此，最优方案是对于所有 $m = 1, \\dots, M$，$\\beta_m = 1/M$。这个解与常数 $c_L$、$c_S$ 和 $N$ 无关，正如问题陈述所要求的那样。\n\n该答案在问题给出的条件下有效：$c_L > 0$，$c_S > 0$，$N \\in \\mathbb{N}$ 是一个正整数，以及 $M \\in \\mathbb{N}$ 也是一个正整数。约束 $\\beta_m > 0$ 得到满足，因为 $M \\geq 1$。\n\n这个恒定的方案 $\\beta_m = 1/M$ 在以下意义上平衡了两个相互竞争的误差源。最优性条件要求总误差相对于任何 $\\beta_k$ 的边际变化，即 $2c_{L}\\,\\beta_{k} - c_{S}/(N\\,\\beta_{k}^{2})$，对所有 $k$ 都必须相同。通过将所有 $\\beta_k$ 都设置为相同的值 $1/M$，这个条件得到了满足。这意味着在最优点，不可能通过略微增加一个 $\\beta_k$ 同时减少另一个 $\\beta_j$ 来维持和的约束，从而降低总误差。该解决方案均衡了将回火“预算”分配给任何特定步骤的边际成本（或收益），从而为总误差上界实现了最优平衡。", "answer": "$$\\boxed{\\frac{1}{M}}$$", "id": "3379440"}]}