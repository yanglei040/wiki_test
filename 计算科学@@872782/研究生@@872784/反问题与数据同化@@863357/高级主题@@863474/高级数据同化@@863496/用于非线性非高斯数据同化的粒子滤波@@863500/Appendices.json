{"hands_on_practices": [{"introduction": "在任何粒子滤波器中，重采样都是一个对抗权重退化、保持粒子多样性的关键步骤。这个练习将引导你亲手完成分层重采样 (stratified resampling) 的整个过程，这是一种高效且广泛应用的技术。通过手动计算，你将具体地理解权重如何归一化、粒子如何被选择以及后代粒子如何生成，从而对粒子群的“优胜劣汰”机制建立起扎实的直观认识。[@problem_id:3409843]", "problem": "考虑一个在序贯蒙特卡洛（SMC）框架内使用粒子滤波器（PF）估计的非线性、非高斯状态空间模型。在给定的同化周期，假设有 $N$ 个粒子，其未归一化的重要性权重为 $\\tilde{w}_{1},\\ldots,\\tilde{w}_{N}$，其中 $N=10$ 且\n$$\n\\tilde{w}_{1}=2,\\ \\tilde{w}_{2}=1,\\ \\tilde{w}_{3}=1,\\ \\tilde{w}_{4}=1,\\ \\tilde{w}_{5}=5,\\ \\tilde{w}_{6}=2,\\ \\tilde{w}_{7}=3,\\ \\tilde{w}_{8}=1,\\ \\tilde{w}_{9}=3,\\ \\tilde{w}_{10}=1.\n$$\n为了减轻权重退化并减少抽样方差，您使用分层重采样执行重采样步骤，该方法在 $[0,1)$ 上的每个等宽层中抽取一个均匀变量。\n\n从SMC中重要性采样和重采样的核心定义出发，将权重归一化以获得 $w_{j}=\\tilde{w}_{j}/\\sum_{k=1}^{N}\\tilde{w}_{k}$，构建累积分布函数（CDF）$C_{j}=\\sum_{k=1}^{j}w_{k}$（其中 $j=1,\\ldots,N$），并使用以下在每个层中的特定均匀样本进行分层重采样：\n$$\nu_{i}=\\frac{i-\\tfrac{1}{2}}{N},\\quad i=1,\\ldots,N,\n$$\n具体为\n$$\nu_{1}=0.05,\\ u_{2}=0.15,\\ u_{3}=0.25,\\ u_{4}=0.35,\\ u_{5}=0.45,\\ u_{6}=0.55,\\ u_{7}=0.65,\\ u_{8}=0.75,\\ u_{9}=0.85,\\ u_{10}=0.95.\n$$\n使用逆CDF法则来分配祖先索引 $a_{i}$，定义为使得 $u_{i}\\leq C_{j}$ 的最小 $j$。然后计算后代计数 $n_{j}$，定义为索引 $j$ 在 $\\{a_{1},\\ldots,a_{N}\\}$ 中出现的次数。\n\n请提供一个单行矩阵作为最终答案，该矩阵首先串联后代计数 $(n_{1},\\ldots,n_{N})$，然后是祖先索引 $(a_{1},\\ldots,a_{N})$，按此顺序排列。不需要四舍五入，也不涉及单位。您的推理应从SMC的基本原理开始，而不是使用快捷公式。", "solution": "问题要求我们为粒子滤波器（PF）执行一个分层重采样步骤，并报告得到的后代计数和祖先索引。该过程是用于非线性、非高斯系统中状态估计的序贯蒙特卡洛（SMC）方法论的一部分。重采样的核心思想是解决权重退化问题，即在少数同化周期后，大多数粒子的重要性权重变得可以忽略不计。重采样通过复制高权重的粒子并丢弃低权重的粒子来更新粒子集，从而有效地将计算资源集中在状态空间中更可能的区域。\n\n首先，我们验证问题陈述。问题提供了一组明确定义的未归一化重要性权重、一个特定的重采样方案（分层重采样）、一个清晰的执行算法以及一个精确的输出格式。所有定义在数据同化领域都是标准的。所提供的数据是自包含的、一致的且在数值上是合理的。该问题具有科学依据、提法恰当、客观且可验证。因此，该问题是有效的，我们可以继续进行求解。\n\n该过程始于给定的 $N=10$ 个未归一化重要性权重集合，$\\tilde{w}_{j}$ 对于 $j=1,\\ldots,10$：\n$$\n\\tilde{w}_{1}=2,\\ \\tilde{w}_{2}=1,\\ \\tilde{w}_{3}=1,\\ \\tilde{w}_{4}=1,\\ \\tilde{w}_{5}=5,\\ \\tilde{w}_{6}=2,\\ \\tilde{w}_{7}=3,\\ \\tilde{w}_{8}=1,\\ \\tilde{w}_{9}=3,\\ \\tilde{w}_{10}=1.\n$$\n第一步是将这些权重归一化，使其总和为 $1$。未归一化权重的总和是：\n$$\n\\sum_{k=1}^{10} \\tilde{w}_{k} = 2+1+1+1+5+2+3+1+3+1 = 20.\n$$\n归一化权重 $w_{j}$ 通过将每个未归一化权重除以这个总和得到：$w_{j} = \\tilde{w}_{j} / \\sum_{k=1}^{10} \\tilde{w}_{k}$。\n$$\n\\begin{aligned}\nw_{1} = \\frac{2}{20} = 0.1 \\\\\nw_{2} = \\frac{1}{20} = 0.05 \\\\\nw_{3} = \\frac{1}{20} = 0.05 \\\\\nw_{4} = \\frac{1}{20} = 0.05 \\\\\nw_{5} = \\frac{5}{20} = 0.25 \\\\\nw_{6} = \\frac{2}{20} = 0.1 \\\\\nw_{7} = \\frac{3}{20} = 0.15 \\\\\nw_{8} = \\frac{1}{20} = 0.05 \\\\\nw_{9} = \\frac{3}{20} = 0.15 \\\\\nw_{10} = \\frac{1}{20} = 0.05\n\\end{aligned}\n$$\n这些归一化权重定义了粒子索引上的一个离散概率分布。下一步是构建累积分布函数（CDF），$C_{j} = \\sum_{k=1}^{j} w_{k}$。\n$$\n\\begin{aligned}\nC_{1} = w_{1} = 0.1 \\\\\nC_{2} = C_{1} + w_{2} = 0.1 + 0.05 = 0.15 \\\\\nC_{3} = C_{2} + w_{3} = 0.15 + 0.05 = 0.20 \\\\\nC_{4} = C_{3} + w_{4} = 0.20 + 0.05 = 0.25 \\\\\nC_{5} = C_{4} + w_{5} = 0.25 + 0.25 = 0.50 \\\\\nC_{6} = C_{5} + w_{6} = 0.50 + 0.1 = 0.60 \\\\\nC_{7} = C_{6} + w_{7} = 0.60 + 0.15 = 0.75 \\\\\nC_{8} = C_{7} + w_{8} = 0.75 + 0.05 = 0.80 \\\\\nC_{9} = C_{8} + w_{9} = 0.80 + 0.15 = 0.95 \\\\\nC_{10} = C_{9} + w_{10} = 0.95 + 0.05 = 1.00\n\\end{aligned}\n$$\n重采样步骤涉及从当前粒子集中抽取 $N$ 个新粒子，其中抽取粒子 $j$ 的概率为 $w_{j}$。分层重采样将区间 $[0,1)$ 分割成 $N$ 个相等的层，即 $[\\frac{i-1}{N}, \\frac{i}{N})$，其中 $i=1,\\ldots,N$。从每个层中抽取一个均匀随机样本。问题指定了要使用的确切样本：\n$$\nu_{i} = \\frac{i - \\frac{1}{2}}{N} \\quad \\text{for } i=1,\\ldots,10.\n$$\n给定的样本是：\n$$\nu_{1}=0.05,\\ u_{2}=0.15,\\ u_{3}=0.25,\\ u_{4}=0.35,\\ u_{5}=0.45,\\ u_{6}=0.55,\\ u_{7}=0.65,\\ u_{8}=0.75,\\ u_{9}=0.85,\\ u_{10}=0.95.\n$$\n我们使用逆CDF方法为每个样本 $u_i$ 找到祖先索引 $a_i$。规则是找到满足 $u_i \\leq C_j$ 的最小索引 $j$。这等价于找到哪个区间 $(C_{j-1}, C_j]$（其中 $C_0=0$）包含 $u_i$。\n\n- 对于 $u_1=0.05$：我们需要满足 $0.05 \\le C_j$ 的最小 $j$。因为 $C_1=0.1$，所以条件对 $j=1$ 成立。因此，$a_1=1$。\n- 对于 $u_2=0.15$：我们需要满足 $0.15 \\le C_j$ 的最小 $j$。$C_1=0.1  0.15$，但 $C_2=0.15 \\ge 0.15$。满足条件的最小 $j$ 是 $2$。因此，$a_2=2$。\n- 对于 $u_3=0.25$：我们需要满足 $0.25 \\le C_j$ 的最小 $j$。$C_3=0.20  0.25$，但 $C_4=0.25 \\ge 0.25$。满足条件的最小 $j$ 是 $4$。因此，$a_3=4$。\n- 对于 $u_4=0.35$：我们需要满足 $0.35 \\le C_j$ 的最小 $j$。$C_4=0.25  0.35$，但 $C_5=0.50 \\ge 0.35$。满足条件的最小 $j$ 是 $5$。因此，$a_4=5$。\n- 对于 $u_5=0.45$：我们需要满足 $0.45 \\le C_j$ 的最小 $j$。$C_4=0.25  0.45$，但 $C_5=0.50 \\ge 0.45$。满足条件的最小 $j$ 是 $5$。因此，$a_5=5$。\n- 对于 $u_6=0.55$：我们需要满足 $0.55 \\le C_j$ 的最小 $j$。$C_5=0.50  0.55$，但 $C_6=0.60 \\ge 0.55$。满足条件的最小 $j$ 是 $6$。因此，$a_6=6$。\n- 对于 $u_7=0.65$：我们需要满足 $0.65 \\le C_j$ 的最小 $j$。$C_6=0.60  0.65$，但 $C_7=0.75 \\ge 0.65$。满足条件的最小 $j$ 是 $7$。因此，$a_7=7$。\n- 对于 $u_8=0.75$：我们需要满足 $0.75 \\le C_j$ 的最小 $j$。$C_6=0.60  0.75$，但 $C_7=0.75 \\ge 0.75$。满足条件的最小 $j$ 是 $7$。因此，$a_8=7$。\n- 对于 $u_9=0.85$：我们需要满足 $0.85 \\le C_j$ 的最小 $j$。$C_8=0.80  0.85$，但 $C_9=0.95 \\ge 0.85$。满足条件的最小 $j$ 是 $9$。因此，$a_9=9$。\n- 对于 $u_{10}=0.95$：我们需要满足 $0.95 \\le C_j$ 的最小 $j$。$C_8=0.80  0.95$，但 $C_9=0.95 \\ge 0.95$。满足条件的最小 $j$ 是 $9$。因此，$a_{10}=9$。\n\n祖先索引的集合是 $\\{a_1,\\ldots,a_{10}\\} = \\{1, 2, 4, 5, 5, 6, 7, 7, 9, 9\\}$。新的粒子集将由这些祖先粒子的副本组成。\n\n最后，我们计算后代计数 $n_j$，即每个原始粒子索引 $j$ 被选为祖先的次数。我们计算集合 $\\{a_1, \\ldots, a_{10}\\}$ 中每个索引的出现次数。\n- $n_1$（$1$的计数）: $1$\n- $n_2$（$2$的计数）: $1$\n- $n_3$（$3$的计数）: $0$\n- $n_4$（$4$的计数）: $1$\n- $n_5$（$5$的计数）: $2$\n- $n_6$（$6$的计数）: $1$\n- $n_7$（$7$的计数）: $2$\n- $n_8$（$8$的计数）: $0$\n- $n_9$（$9$的计数）: $2$\n- $n_{10}$（$10$的计数）: $0$\n\n后代计数的向量是 $(n_1,\\ldots,n_{10}) = (1, 1, 0, 1, 2, 1, 2, 0, 2, 0)$。作为检验，后代计数的总和必须为 $N$：$1+1+0+1+2+1+2+0+2+0=10$。\n祖先索引的向量是 $(a_1,\\ldots,a_{10}) = (1, 2, 4, 5, 5, 6, 7, 7, 9, 9)$。\n\n最终答案是后代计数和祖先索引串联成的一个单行矩阵。\n$$\n(n_1,\\ldots,n_{10}, a_1,\\ldots,a_{10}) = (1, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 2, 4, 5, 5, 6, 7, 7, 9, 9).\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  0  1  2  1  2  0  2  0  1  2  4  5  5  6  7  7  9  9\n\\end{pmatrix}\n}\n$$", "id": "3409843"}, {"introduction": "在粒子滤波器普及之前，扩展卡尔曼滤波器 (EKF) 是解决非线性数据同化问题的常用方法。本编程练习旨在揭示 EKF 在强非线性或非高斯噪声情景下的局限性。通过将 EKF 的近似结果与基于数值积分的精确贝叶斯更新进行对比，你将清晰地看到为何以及何时需要粒子滤波器这类更为强大的方法。[@problem_id:3409807]", "problem": "考虑一个用于数据同化的标量隐马尔可夫模型，其观测算子为非线性，观测噪声为非高斯。设时间 $t$ 的隐藏状态为 $x_t \\in \\mathbb{R}$。假设时间 $t$ 的先验（也称为一步预测）是高斯分布，均值为 $m_t^{-}$，方差为 $P_t^{-}$，即 $p(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t \\mid m_t^{-}, P_t^{-})$。时间 $t$ 的观测值为 $y_t \\in \\mathbb{R}$，通过一个已知的非线性函数 $h(\\cdot)$ 和加性噪声与状态相关联，$y_t = h(x_t) + \\varepsilon_t$，其中噪声 $\\varepsilon_t$ 服从自由度为 $\\nu$、尺度为 $s$ 的 Student t分布，其概率密度函数为\n$$\np_{\\varepsilon}(e) = \\frac{\\Gamma\\!\\left(\\frac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\, s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{e}{s}\\right)^2\\right)^{-\\frac{\\nu + 1}{2}}.\n$$\n观测算子由下式给出\n$$\nh(x) = x + a \\sin(b x),\n$$\n其中参数 $a \\in \\mathbb{R}$ 和 $b \\in \\mathbb{R}$。对于此问题，假设所有量均为无量纲，不涉及物理单位。\n\n你的任务是：\n\n1.  使用贝叶斯定理以及给定的先验、似然和观测算子，定义滤波后验密度\n$$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) \\, p(x_t \\mid y_{1:t-1}),\n$$\n并对下面的每个测试用例，数值计算其前两阶矩，即后验均值\n$$\n\\mu_t = \\mathbb{E}[x_t \\mid y_{1:t}],\n$$\n和后验方差\n$$\n\\sigma_t^2 = \\mathbb{V}[x_t \\mid y_{1:t}]。\n$$\n数值计算必须通过对 $x \\in \\mathbb{R}$ 上的未归一化后验进行直接数值积分来执行，不使用任何高斯近似或线性化。\n\n2.  计算扩展卡尔曼滤波器（EKF）预测的矩，在此问题背景下其定义如下。使用一阶泰勒展开，在预测均值 $m_t^{-}$ 处对观测算子进行线性化，\n$$\nh(x) \\approx h(m_t^{-}) + H_t (x - m_t^{-}), \\quad \\text{其中 } H_t = \\left.\\frac{dh}{dx}\\right|_{x = m_t^{-}} = 1 + a b \\cos(b m_t^{-}).\n$$\n将观测噪声近似为与 Student t分布具有相同方差的高斯分布，即\n$$\nR = \\frac{\\nu}{\\nu - 2} s^2 \\quad \\text{对于 } \\nu  2.\n$$\n然后，使用标准的标量卡尔曼滤波器公式计算 EKF 对均值和方差的更新，\n$$\nS_t = H_t^2 P_t^{-} + R, \\quad K_t = \\frac{P_t^{-} H_t}{S_t}, \\quad m_t^{\\text{EKF}} = m_t^{-} + K_t \\left(y_t - h(m_t^{-})\\right), \\quad P_t^{\\text{EKF}} = \\left(1 - K_t H_t\\right) P_t^{-}.\n$$\n\n对于每个测试用例，你的程序必须生成一对数值后验矩 $\\mu_t$ 和 $\\sigma_t^2$，EKF矩 $m_t^{\\text{EKF}}$ 和 $P_t^{\\text{EKF}}$，以及绝对差 $\\lvert \\mu_t - m_t^{\\text{EKF}} \\rvert$ 和 $\\lvert \\sigma_t^2 - P_t^{\\text{EKF}} \\rvert$。\n\n使用以下参数值 $(m_t^{-}, P_t^{-}, y_t, a, b, \\nu, s)$ 的测试套件，它涵盖了一个典型情况、一个线性化预期准确的小方差情况、一个高度非线性和离散先验情况以及一个重尾异常值情况：\n\n-   测试用例1：$(0.0, 1.0, 1.0, 0.5, 1.0, 5.0, 0.5)$。\n-   测试用例2：$(0.5, 0.01, 0.2, 0.8, 2.0, 7.0, 0.3)$。\n-   测试用例3：$(-1.0, 4.0, 2.5, 1.2, 2.0, 4.0, 0.7)$。\n-   测试用例4：$(0.0, 1.5, 4.0, 0.7, 1.5, 3.1, 1.0)$。\n\n对于数值积分，你必须在 $x \\in (-\\infty, \\infty)$ 上进行积分，并计算归一化常数、一阶原始矩和二阶原始矩，然后通过减法获得方差。所有积分都必须使用标准的自适应求积法计算到合理的数值精度；不要对线进行粗略离散化。\n\n最终输出格式：你的程序应生成一行输出，包含一个用方括号括起来的逗号分隔的结果列表。对于四个测试用例，该列表必须按以下固定顺序展平：对于按上述顺序给出的每个测试用例，输出 $[\\mu_t, \\sigma_t^2, m_t^{\\text{EKF}}, P_t^{\\text{EKF}}, \\lvert \\mu_t - m_t^{\\text{EKF}} \\rvert, \\lvert \\sigma_t^2 - P_t^{\\text{EKF}} \\rvert]$，然后将所有测试用例的这些结果连接成一个单一列表。例如，输出应类似于 $[\\text{case1\\_mu}, \\text{case1\\_var}, \\text{case1\\_m\\_ekf}, \\text{case1\\_P\\_ekf}, \\text{case1\\_dmu}, \\text{case1\\_dvar}, \\text{case2\\_mu}, \\ldots, \\text{case4\\_dvar}]$，其中每个占位符都是一个实数。", "solution": "该问题是有效的，因为它在科学上基于贝叶斯滤波和数据同化的原理，是适定的（提供了所有必要信息），并且其表述是客观的。我们将提供一个完整解答。\n\n问题的核心是为时间 $t$ 的标量隐藏状态 $x_t \\in \\mathbb{R}$ 执行贝叶斯更新。该更新将关于状态的先验信念与来自观测 $y_t$ 的新信息相结合。给定截至时间 $t$ 的所有观测值（记为 $y_{1:t}$），$x_t$ 的后验概率密度函数（PDF）由贝叶斯定理给出：\n$$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) \\, p(x_t \\mid y_{1:t-1})\n$$\n此处，$p(x_t \\mid y_{1:t-1})$ 是先验PDF（来自前一时间步的预测），而 $p(y_t \\mid x_t)$ 是似然函数，它量化了在给定特定状态 $x_t$ 的情况下观测到 $y_t$ 的概率。\n\n问题指定先验分布为高斯分布：\n$$\np(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t \\mid m_t^{-}, P_t^{-}) = \\frac{1}{\\sqrt{2 \\pi P_t^{-}}} \\exp\\left(-\\frac{(x_t - m_t^{-})^2}{2 P_t^{-}}\\right)\n$$\n其中 $m_t^{-}$ 是先验均值，$P_t^{-}$ 是先验方差。\n\n观测模型为 $y_t = h(x_t) + \\varepsilon_t$，具有非线性观测算子 $h(x) = x + a \\sin(b x)$ 和加性噪声 $\\varepsilon_t$。噪声服从具有 $\\nu$ 自由度和尺度参数 $s$ 的Student t分布。因此，似然函数是噪声PDF在残差 $e = y_t - h(x_t)$ 处的取值：\n$$\np(y_t \\mid x_t) = p_{\\varepsilon}(y_t - h(x_t)) = \\frac{\\Gamma\\!\\left(\\frac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\, s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y_t - h(x_t)}{s}\\right)^2\\right)^{-\\frac{\\nu + 1}{2}}\n$$\n\n未归一化的后验密度，我们称之为 $q(x_t)$，是先验和似然的乘积。对于数值计算，我们可以使用一个与真实未归一化后验成比例的 $q(x_t)$ 版本，方法是省略常数因子：\n$$\nq(x_t) \\propto \\exp\\left(-\\frac{(x_t - m_t^{-})^2}{2 P_t^{-}}\\right) \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y_t - (x_t + a \\sin(b x_t))}{s}\\right)^2\\right)^{-\\frac{\\nu + 1}{2}}\n$$\n\n**1. 后验矩的数值计算**\n为了找到精确的后验均值 $\\mu_t$ 和方差 $\\sigma_t^2$，我们必须执行数值积分。我们首先计算未归一化后验 $q(x_t)$ 所需的原始矩：\n归一化常数（零阶矩）：\n$$\nZ = \\int_{-\\infty}^{\\infty} q(x_t) \\, dx_t\n$$\n一阶原始矩：\n$$\nM_1 = \\int_{-\\infty}^{\\infty} x_t \\, q(x_t) \\, dx_t\n$$\n二阶原始矩：\n$$\nM_2 = \\int_{-\\infty}^{\\infty} x_t^2 \\, q(x_t) \\, dx_t\n$$\n这些积分使用在 $(-\\infty, \\infty)$ 域上的自适应求积法计算。然后，后验均值和方差计算如下：\n$$\n\\mu_t = \\mathbb{E}[x_t \\mid y_{1:t}] = \\frac{M_1}{Z}\n$$\n$$\n\\sigma_t^2 = \\mathbb{V}[x_t \\mid y_{1:t}] = \\mathbb{E}[x_t^2 \\mid y_{1:t}] - (\\mathbb{E}[x_t \\mid y_{1:t}])^2 = \\frac{M_2}{Z} - \\mu_t^2\n$$\n\n**2. 扩展卡尔曼滤波器（EKF）近似**\nEKF通过两个关键近似简化了问题：\n1.  非线性观测算子 $h(x_t)$ 在先验均值 $m_t^{-}$ 附近使用一阶泰勒展开进行线性化：\n$$\nh(x_t) \\approx h(m_t^{-}) + H_t (x_t - m_t^{-})\n$$\n其中 $H_t$ 是 $h$ 在 $m_t^{-}$ 处求值的雅可比矩阵。对于给定的标量函数，这是其导数：\n$$\nH_t = \\left.\\frac{dh}{dx}\\right|_{x = m_t^{-}} = 1 + a b \\cos(b m_t^{-})\n$$\n2.  非高斯观测噪声 $\\varepsilon_t$ 被近似为与Student t分布具有相同方差的零均值高斯分布。对于 $\\nu > 2$，该方差为：\n$$\nR = \\frac{\\nu}{\\nu - 2} s^2\n$$\n有了这些近似，问题就变成了一个标准的线性高斯更新，卡尔曼滤波器方程为此提供了精确的后验矩。这些就是EKF近似矩：\n新息协方差：\n$$\nS_t = H_t^2 P_t^{-} + R\n$$\n卡尔曼增益：\n$$\nK_t = \\frac{P_t^{-} H_t}{S_t}\n$$\nEKF后验均值：\n$$\nm_t^{\\text{EKF}} = m_t^{-} + K_t \\left(y_t - h(m_t^{-})\\right)\n$$\nEKF后验方差：\n$$\nP_t^{\\text{EKF}} = \\left(1 - K_t H_t\\right) P_t^{-}\n$$\n下面的程序为每个指定的测试用例，实现了用于精确矩的数值积分和用于近似矩的EKF方程。然后计算所需的绝对差 $\\lvert \\mu_t - m_t^{\\text{EKF}} \\rvert$ 和 $\\lvert \\sigma_t^2 - P_t^{\\text{EKF}} \\rvert$ 以比较这两种方法。", "answer": "```python\nimport numpy as np\nfrom scipy import integrate\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Computes and compares numerical Bayesian and EKF updates for a scalar HMM.\n    \"\"\"\n    test_cases = [\n        # (m_minus, P_minus, y_t, a, b, nu, s)\n        (0.0, 1.0, 1.0, 0.5, 1.0, 5.0, 0.5),  # Case 1: Typical case\n        (0.5, 0.01, 0.2, 0.8, 2.0, 7.0, 0.3),  # Case 2: Small prior variance\n        (-1.0, 4.0, 2.5, 1.2, 2.0, 4.0, 0.7), # Case 3: High nonlinearity\n        (0.0, 1.5, 4.0, 0.7, 1.5, 3.1, 1.0),   # Case 4: Heavy-tailed outlier\n    ]\n\n    results = []\n\n    def h_func(x, a, b):\n        \"\"\" The nonlinear observation operator h(x). \"\"\"\n        return x + a * np.sin(b * x)\n\n    def unnormalized_posterior(x, m_minus, P_minus, y, a, b, nu, s):\n        \"\"\"\n        Computes the unnormalized posterior density q(x) = p(y|x)p(x).\n        \"\"\"\n        # Likelihood term p(y|x)\n        residual = y - h_func(x, a, b)\n        likelihood = stats.t.pdf(residual, df=nu, loc=0, scale=s)\n        \n        # Prior term p(x)\n        prior = stats.norm.pdf(x, loc=m_minus, scale=np.sqrt(P_minus))\n        \n        return likelihood * prior\n\n    for case in test_cases:\n        m_minus, P_minus, y, a, b, nu, s = case\n        params = (m_minus, P_minus, y, a, b, nu, s)\n\n        # 1. Numerical Bayesian Update\n        \n        # Define integrands for moments\n        integrand_Z = lambda x, *p: unnormalized_posterior(x, *p)\n        integrand_M1 = lambda x, *p: x * unnormalized_posterior(x, *p)\n        integrand_M2 = lambda x, *p: x**2 * unnormalized_posterior(x, *p)\n\n        # Numerically integrate using adaptive quadrature\n        Z, _ = integrate.quad(integrand_Z, -np.inf, np.inf, args=params)\n        M1, _ = integrate.quad(integrand_M1, -np.inf, np.inf, args=params)\n        M2, _ = integrate.quad(integrand_M2, -np.inf, np.inf, args=params)\n\n        # Calculate posterior mean and variance\n        mu_t = M1 / Z\n        sigma2_t = M2 / Z - mu_t**2\n\n        # 2. Extended Kalman Filter (EKF) Update\n        \n        # Linearization\n        h_m_minus = h_func(m_minus, a, b)\n        H_t = 1.0 + a * b * np.cos(b * m_minus)\n        \n        # Observation noise variance approximation\n        R = (nu / (nu - 2.0)) * s**2\n        \n        # Standard Kalman filter update equations\n        S_t = H_t**2 * P_minus + R\n        K_t = (P_minus * H_t) / S_t\n        m_ekf = m_minus + K_t * (y - h_m_minus)\n        P_ekf = (1.0 - K_t * H_t) * P_minus\n        \n        # 3. Compute Absolute Differences\n        d_mu = np.abs(mu_t - m_ekf)\n        d_sigma2 = np.abs(sigma2_t - P_ekf)\n        \n        results.extend([mu_t, sigma2_t, m_ekf, P_ekf, d_mu, d_sigma2])\n\n    # Format and print the final output string\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3409807"}, {"introduction": "本章的最后一个练习将挑战你处理由随机微分方程 (SDE) 描述的连续时间系统。你将需要使用欧拉-丸山 (Euler-Maruyama) 方法来推进粒子，并计算一个关键的校正项——重要性权重，它用于弥补模型真实动态与我们所用提议动态之间的不匹配。这个练习将帮助你将在离散时间中学到的滤波知识扩展到连续时间领域，这是解决许多现实世界应用问题的必备技能。[@problem_id:3409822]", "problem": "您需要为一个非线性、非高斯数据同化问题，实现连续时间自助粒子滤波器的一个单时间步长。潜在状态根据一个具有恒定扩散和非线性漂移的随机微分方程（SDE, stochastic differential equation）演化。一组粒子在时间 $t$ 近似过滤分布，并且必须使用欧拉-丸山离散化将其传播到时间 $t + \\Delta t$，并重新加权以同时考虑模型转换和一个带有重尾噪声的观测。\n\n基本设置：\n- 潜在状态 $x_t \\in \\mathbb{R}$ 根据SDE演化\n$$\n\\mathrm{d}x_t = f(x_t) \\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t,\n$$\n其中 $W_t$ 是布朗运动，$\\sigma  0$ 是一个已知的标量扩散系数，$f$ 是一个非线性漂移。用于粒子传播的提议由一个可能与真实漂移 $f$ 不同的漂移 $g$ 给出。\n- 用于提议的欧拉-丸山（EM, Euler-Maruyama）离散化计算，对于每个粒子 $x^{(i)}_t$，\n$$\nx^{(i)}_{t+\\Delta t} = x^{(i)}_t + g\\big(x^{(i)}_t\\big)\\,\\Delta t + \\sigma \\,\\Delta W^{(i)},\n$$\n其中布朗增量 $\\Delta W^{(i)}$ 是从一个均值为 $0$、方差为 $\\Delta t$ 的正态随机变量中抽取的。\n- 在时间 $t+\\Delta t$ 的观测通过以下方式提供\n$$\ny = h\\big(x_{t+\\Delta t}\\big) + v,\n$$\n其中 $h$ 是非线性的，$v$ 是从一个自由度为 $\\nu$、尺度为 $s  0$ 的学生t分布中抽取的。观测噪声是重尾的，不是高斯噪声。\n\n您的单时间步长任务：\n1. 在时间 $t$ 从一个指定的分布初始化 $N$ 个粒子，并赋予均匀权重。\n2. 使用带有漂移 $g$ 和扩散 $\\sigma$ 的EM提议将粒子传播到时间 $t+\\Delta t$。\n3. 基于SDE中恒定扩散下测度变换的基本原理，计算由于目标模型转换（带有漂移 $f$）和提议转换（带有漂移 $g$）在时间步长上的不匹配所导致的重要性权重校正。使用对单步连续时间拉东-尼科迪姆导数的有原则的离散化：它必须依赖于漂移差异、扩散、时间步长以及每个粒子实现的布朗增量。\n4. 结合带有学生t噪声的非线性测量 $h$ 的观测似然。使用似然时忽略常数因子是可以接受的，因为归一化的重要性权重会消除所有粒子共有的常数。\n5. 归一化更新后的权重并计算有效样本量（ESS, effective sample size），定义为\n$$\n\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N \\big(w^{(i)}\\big)^2},\n$$\n其中 $w^{(i)}$ 是在时间 $t+\\Delta t$ 的归一化权重。\n6. 不执行重采样；仅报告ESS。\n\n模型规格：\n- 漂移 $f$ 是\n$$\nf(x) = \\alpha \\,\\tanh(x),\n$$\n带有一个已知的标量参数 $\\alpha  0$。\n- 提议漂移 $g$ 是以下之一：\n  - $g(x) = f(x)$（无不匹配），\n  - $g(x) = \\alpha x$（线性化不匹配），\n  - $g(x) = 0$（零漂移不匹配）。\n- 测量函数是\n$$\nh(x) = x^3.\n$$\n\n粒子初始化：\n- 粒子数量 $N = 250$。\n- 初始粒子 $x^{(i)}_t$ 是独立同分布的，服从均值为 $0$、方差为 $1$ 的标准正态分布，使用固定的随机种子生成以保证可复现性。\n- 初始权重是均匀的 $w^{(i)}_t = \\frac{1}{N}$。\n\n随机性与可复现性：\n- 您必须使用指定的随机种子来初始化粒子和在每个测试用例中生成布朗增量。\n- 每个测试用例为其布朗增量使用自己的种子；初始粒子使用指定的种子生成一次，并在所有用例中重复使用。\n\n测试套件与参数：\n- 所有测试用例的通用参数：$\\alpha = 1.3$, $\\sigma = 0.6$, $N = 250$, 初始粒子种子 $= 2025$, $h(x) = x^3$, 自由度 $\\nu = 5$, 尺度 $s = 0.7$。\n- 测试用例 A（理想路径）：\n  - 时间步长 $\\Delta t = 0.1$，\n  - 提议漂移类型：$g(x) = f(x)$（无不匹配），\n  - 观测值 $y = 1.0$，\n  - 布朗增量种子 $= 10$。\n- 测试用例 B（动力学显著不匹配）：\n  - 时间步长 $\\Delta t = 0.1$，\n  - 提议漂移类型：$g(x) = 0$（零漂移），\n  - 观测值 $y = 1.0$，\n  - 布朗增量种子 $= 20$。\n- 测试用例 C（小时间步长边界）：\n  - 时间步长 $\\Delta t = 10^{-3}$，\n  - 提议漂移类型：$g(x) = \\alpha x$（线性化），\n  - 观测值 $y = 1.0$，\n  - 布朗增量种子 $= 30$。\n- 测试用例 D（异常观测）：\n  - 时间步长 $\\Delta t = 0.1$，\n  - 提议漂移类型：$g(x) = \\alpha x$（线性化），\n  - 观测值 $y = 8.0$，\n  - 布朗增量种子 $= 40$。\n\n算法约束：\n- 动力学的重要性权重校正必须从适用于恒定扩散SDE的第一性原理推导得出，确保科学真实性，并针对单个欧拉-丸山步骤及实现的布朗增量进行实现。\n- 观测似然必须反映 $y - h(x_{t+\\Delta t})$ 的学生t噪声，具有自由度 $\\nu$ 和尺度 $s$，忽略常数因子。\n- 所有计算必须是数值稳定的；例如，在适当时在对数域中执行权重计算。\n\n要求的最终输出格式：\n- 您的程序应产生单行输出，包含一个用方括号括起来的逗号分隔列表，即按顺序排列的测试用例A、B、C和D的四个计算出的 $\\mathrm{ESS}$ 值，均为浮点数。例如，输出格式为\n$$\n[\\mathrm{ESS}_A,\\mathrm{ESS}_B,\\mathrm{ESS}_C,\\mathrm{ESS}_D].\n$$\n不应打印任何额外文本。\n\n您的实现必须是一个完整的、可运行的程序，使用Python 3.12版本，仅使用标准库、NumPy 1.23.5版本和SciPy 1.11.4版本（无其他库）。程序必须是自包含的，没有外部输入或文件。", "solution": "用户提供的问题陈述被认为是有效的。它在科学上基于随机微分方程和粒子滤波理论，问题设定良好，定义了所有必要的参数和条件，并且其表述是客观的。因此，我们可以着手提供完整的解决方案。\n\n该问题要求为在连续时间内定义的状态空间模型实现自助粒子滤波器的一个单时间步长。潜在状态 $x_t \\in \\mathbb{R}$ 根据随机微分方程（SDE）演化，观测是非线性的，并受到重尾噪声的污染。任务的核心是传播一个粒子集合，并根据模型动力学和新的观测更新它们的重要性权重。\n\n从时间 $t$ 到 $t+\\Delta t$ 的单时间步长的整体过程如下：\n1.  **传播 (Propagation)：** 每个粒子 $x^{(i)}_t$ 通过一个提议分布被推进到一个新状态 $x^{(i)}_{t+\\Delta t}$。\n2.  **加权 (Weighting)：** 每个粒子的重要性权重被更新，以反映它与真实模型动力学和在时间 $t+\\Delta t$ 的观测的匹配程度。\n3.  **评估 (Evaluation)：** 从新的、归一化的权重计算有效样本量（ESS），以量化权重退化。\n\n现在我们根据提供的模型规格详细说明每个步骤。\n\n**1. 粒子传播**\n\n每个粒子 $i \\in \\{1, \\dots, N\\}$ 的状态使用提议SDE的欧拉-丸山离散化从时间 $t$ 传播到 $t+\\Delta t$。提议动力学使用一个漂移函数 $g(x)$，它可能与真实模型漂移 $f(x)$ 不同。\n\n粒子 $i$ 的更新规则是：\n$$\nx^{(i)}_{t+\\Delta t} = x^{(i)}_t + g\\big(x^{(i)}_t\\big)\\,\\Delta t + \\sigma \\,\\Delta W^{(i)}\n$$\n其中 $\\Delta W^{(i)}$ 是一个随机样本，表示长度为 $\\Delta t$ 的区间上的布朗增量。这些增量是为每个粒子独立地从均值为 $0$、方差为 $\\Delta t$ 的正态分布中抽取的，即 $\\Delta W^{(i)} \\sim \\mathcal{N}(0, \\Delta t)$。\n\n**2. 重要性权重更新**\n\n给定在时间 $t$ 的均匀权重 $w^{(i)}_t = 1/N$，在时间 $t+\\Delta t$ 的新的、未归一化的权重 $\\tilde{w}^{(i)}_{t+\\Delta t}$ 由动力学校正因子和观测似然项的乘积给出：\n$$\n\\tilde{w}^{(i)}_{t+\\Delta t} \\propto w_{\\text{dyn}}^{(i)} \\times L^{(i)}\n$$\n为了数值稳定性，计算在对数域中进行：\n$$\n\\log \\tilde{w}^{(i)}_{t+\\Delta t} = \\log w_{\\text{dyn}}^{(i)} + \\log L^{(i)} + C\n$$\n其中 $C$ 是一个任意常数。\n\n**2.1. 动力学校正权重 ($w_{\\text{dyn}}^{(i)}$)**\n\n此项校正提议动力学（带漂移 $g$）与目标动力学（带漂移 $f$）之间的差异。重要性权重是目标转移概率密度与提议转移概率密度的比值，$w_{\\text{dyn}}^{(i)} = p(x^{(i)}_{t+\\Delta t} | x^{(i)}_t) / q(x^{(i)}_{t+\\Delta t} | x^{(i)}_t)$。\n\n对于具有恒定扩散 $\\sigma$ 的欧拉-丸山方案，两种密度都是高斯的：\n-   目标：$p(x_{t+\\Delta t} | x_t) = \\mathcal{N}(x_{t+\\Delta t} | x_t + f(x_t)\\Delta t, \\sigma^2 \\Delta t)$\n-   提议：$q(x_{t+\\Delta t} | x_t) = \\mathcal{N}(x_{t+\\Delta t} | x_t + g(x_t)\\Delta t, \\sigma^2 \\Delta t)$\n\n这两个密度之比的对数，在消去公共项后，得到对数重要性权重。这个结果是吉尔萨诺夫定理的拉东-尼科迪姆导数的一阶离散化。对于粒子 $i$，动力学校正的对数权重是：\n$$\n\\log w_{\\text{dyn}}^{(i)} = \\frac{f(x_t^{(i)}) - g(x_t^{(i)})}{\\sigma^2} \\left( \\sigma \\Delta W^{(i)} \\right) - \\frac{1}{2} \\left( \\frac{f(x_t^{(i)}) - g(x_t^{(i)})}{\\sigma} \\right)^2 \\Delta t\n$$\n这里，$f(x) = \\alpha \\tanh(x)$ 是真实漂移，$g(x)$ 是测试用例指定的提议漂移，而 $\\sigma \\Delta W^{(i)} = x^{(i)}_{t+\\Delta t} - x^{(i)}_t - g(x_t^{(i)})\\Delta t$ 是在提议步骤中实现的随机分量。如果提议漂移与真实漂移匹配（$g=f$），此项为零，无需校正。\n\n**2.2. 观测似然 ($L^{(i)}$)**\n\n观测模型是 $y = h(x_{t+\\Delta t}) + v$，其中噪声 $v$ 服从自由度为 $\\nu$、尺度参数为 $s$ 的学生t分布。这种分布的概率密度函数（PDF）是：\n$$\np(v; \\nu, s) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\pi\\nu}s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{v}{s}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n$$\n给定其传播状态 $x^{(i)}_{t+\\Delta t}$，粒子 $i$ 的观测似然是在残差 $v^{(i)} = y - h(x^{(i)}_{t+\\Delta t})$ 处评估的PDF。因为最终权重会被归一化，所以似然中的任何常数乘法因子都是无关紧要的。因此，粒子 $i$ 的对数似然是：\n$$\n\\log L^{(i)} \\propto -\\frac{\\nu+1}{2} \\log\\left(1 + \\frac{1}{\\nu}\\left(\\frac{y - h\\left(x^{(i)}_{t+\\Delta t}\\right)}{s}\\right)^2\\right)\n$$\n这个表达式捕捉了对于每个粒子的提议状态，观测到 $y$ 的相对概率。\n\n**3. 权重归一化与ESS计算**\n\n总的未归一化对数权重 $\\log \\tilde{w}^{(i)}_{t+\\Delta t}$ 被合并，然后归一化以使总和为一。为防止数值下溢，采用log-sum-exp技巧。首先，我们找到最大对数权重，$C = \\max_i(\\log \\tilde{w}^{(i)}_{t+\\Delta t})$。然后，归一化权重 $w^{(i)}_{t+\\Delta t}$ 计算如下：\n$$\nw^{(i)}_{t+\\Delta t} = \\frac{\\exp\\left(\\log \\tilde{w}^{(i)}_{t+\\Delta t} - C\\right)}{\\sum_{j=1}^{N} \\exp\\left(\\log \\tilde{w}^{(j)}_{t+\\Delta t} - C\\right)}\n$$\n最后，计算有效样本量（ESS）以评估粒子权重的退化程度。一个低的ESS表明少数粒子具有非常高的权重，而其余的可以忽略不计，这意味着粒子对分布的表示效果很差。ESS由下式给出：\n$$\n\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N \\left(w^{(i)}_{t+\\Delta t}\\right)^2}\n$$\n一个接近粒子总数 $N$ 的ESS值表明权重几乎是均匀的，这是理想情况。一个接近 $1$ 的ESS值表示极端退化。对四个指定的测试用例中的每一个都执行此分析。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Implements a single time-step of a bootstrap particle filter for a nonlinear,\n    non-Gaussian data assimilation problem and computes the Effective Sample Size (ESS)\n    for four different test cases.\n    \"\"\"\n    \n    # Common parameters across all test cases\n    alpha = 1.3\n    sigma = 0.6\n    N = 250\n    initial_particle_seed = 2025\n    nu = 5.0\n    s_scale = 0.7\n\n    # Define the core model functions\n    f_drift = lambda x: alpha * np.tanh(x)\n    h_obs = lambda x: x**3\n\n    # Define the parameters for each test case\n    test_cases_params = [\n        {'dt': 0.1, 'g_type': 'identity', 'y_obs': 1.0, 'brownian_seed': 10}, # Case A\n        {'dt': 0.1, 'g_type': 'zero', 'y_obs': 1.0, 'brownian_seed': 20},     # Case B\n        {'dt': 1e-3, 'g_type': 'linear', 'y_obs': 1.0, 'brownian_seed': 30},  # Case C\n        {'dt': 0.1, 'g_type': 'linear', 'y_obs': 8.0, 'brownian_seed': 40},   # Case D\n    ]\n\n    # --- Step 1: Initialize particles (done once for all cases) ---\n    # These are the particles at time t, x_t.\n    rng_initial = np.random.default_rng(initial_particle_seed)\n    x_t = rng_initial.normal(loc=0.0, scale=1.0, size=N)\n\n    results_ess = []\n    \n    for case_params in test_cases_params:\n        dt = case_params['dt']\n        g_type = case_params['g_type']\n        y_obs = case_params['y_obs']\n        brownian_seed = case_params['brownian_seed']\n\n        # Define the proposal drift function g(x) based on the test case\n        if g_type == 'identity':\n            g_drift = f_drift\n        elif g_type == 'zero':\n            g_drift = lambda x: np.zeros_like(x)\n        elif g_type == 'linear':\n            g_drift = lambda x: alpha * x\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown proposal drift type: {g_type}\")\n\n        # --- Step 2: Propagate particles (Prediction) ---\n        # Generate Brownian increments for this specific case\n        rng_brownian = np.random.default_rng(brownian_seed)\n        delta_W = rng_brownian.normal(loc=0.0, scale=np.sqrt(dt), size=N)\n        \n        g_val_t = g_drift(x_t)\n        # These are the propagated particles at time t+dt, x_{t+dt}\n        x_t_plus_dt = x_t + g_val_t * dt + sigma * delta_W\n\n        # --- Step 3  4: Compute unnormalized log weights (Update) ---\n        \n        # Part 1: Log importance weight correction for dynamics mismatch\n        f_val_t = f_drift(x_t)\n        drift_diff = f_val_t - g_val_t\n        \n        log_dyn_correction = (drift_diff / sigma**2) * (sigma * delta_W) - 0.5 * (drift_diff / sigma)**2 * dt\n\n        # Part 2: Log likelihood from the observation\n        h_val_t_plus_dt = h_obs(x_t_plus_dt)\n        log_obs_likelihood = student_t.logpdf(y_obs, df=nu, loc=h_val_t_plus_dt, scale=s_scale)\n\n        # Combine log weights\n        log_weights_unnorm = log_dyn_correction + log_obs_likelihood\n\n        # --- Step 5: Normalize weights and compute ESS ---\n        \n        # Use logsumexp for robust normalization\n        log_sum_weights = logsumexp(log_weights_unnorm)\n        log_normalized_weights = log_weights_unnorm - log_sum_weights\n        normalized_weights = np.exp(log_normalized_weights)\n        \n        # Compute Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n        results_ess.append(ess)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results_ess))}]\")\n\nsolve()\n\n```", "id": "3409822"}]}