{"hands_on_practices": [{"introduction": "信息滤波器的一个核心优势在于其更新步骤的简洁性：先验信息与新观测信息简单相加。本练习将带您从第一性原理出发，通过基本的代数推导，为标量和批量观测导出信息更新规则。通过一个具体的数值算例 [@problem_id:3390747]，您将亲手验证顺序更新和批量更新的等价性，从而加深对贝叶斯推断在信息空间中可加性的理解。", "problem": "考虑一个信息形式的线性高斯逆问题，其状态向量为 $x \\in \\mathbb{R}^{2}$，先验密度与 $\\exp\\!\\big(-\\tfrac{1}{2}\\,x^{\\top} Y x + y^{\\top} x\\big)$ 成正比，其中先验信息矩阵和先验信息向量为\n$$\nY \\,=\\, \\begin{pmatrix} 3  & 1 \\\\ 1  & 2 \\end{pmatrix}, \\qquad y \\,=\\, \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}.\n$$\n您收到两个形式为 $z_{i} = h_{i}^{\\top} x + v_{i}$ 的独立标量观测，其中 $v_{i} \\sim \\mathcal{N}(0, r_{i})$，数据如下：\n$$\nh_{1} \\,=\\, \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad r_{1} \\,=\\, \\tfrac{1}{2}, \\quad z_{1} \\,=\\, \\tfrac{3}{2}; \n\\qquad\nh_{2} \\,=\\, \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad r_{2} \\,=\\, 2, \\quad z_{2} \\,=\\, -\\tfrac{1}{2}.\n$$\n从贝叶斯法则和高斯似然出发，并仅使用基本的代数操作（例如，展开和配方法）作为基础，完成以下任务：\n\n1. 对于单个标量观测 $z = h^{\\top} x + v$（其中 $v \\sim \\mathcal{N}(0, r)$），推导其信息形式的后验更新。将更新后的信息矩阵 $Y^{+}$ 和更新后的信息向量 $y^{+}$ 表示为 $Y$、$y$、$h$、$r$ 和 $z$ 的函数。\n\n2. 对上述两个观测序贯地应用您的标量更新，以获得 $Y_{\\mathrm{seq}}$ 和 $y_{\\mathrm{seq}}$。\n\n3. 现在建立多观测模型 $z = H x + v$，其中\n$$\nH \\,=\\, \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix} \\,=\\, \\begin{pmatrix} 1  & 1 \\\\ 2  & -1 \\end{pmatrix}, \n\\qquad \nR \\,=\\, \\operatorname{diag}(r_{1}, r_{2}) \\,=\\, \\begin{pmatrix} \\tfrac{1}{2}  & 0 \\\\ 0  & 2 \\end{pmatrix}, \n\\qquad \nz \\,=\\, \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} \\,=\\, \\begin{pmatrix} \\tfrac{3}{2} \\\\ -\\tfrac{1}{2} \\end{pmatrix}.\n$$\n再次从高斯似然和配方法出发，推导批量信息形式的后验更新，以获得 $Y_{\\mathrm{bat}}$ 和 $y_{\\mathrm{bat}}$。\n\n最后，定义标量\n$$\nS \\,=\\, \\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} \\;+\\; \\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2},\n$$\n其中 $\\|\\cdot\\|_{F}$ 是弗罗贝尼乌斯范数，$\\|\\cdot\\|_{2}$ 是欧几里得范数。精确计算 $S$。将您的最终结果以单个实数的形式给出。无需四舍五入，也无需单位。", "solution": "该问题要求推导并应用线性高斯系统的信息形式更新，包括序贯和批量两种形式，然后比较结果。状态向量为 $x \\in \\mathbb{R}^{2}$。\n\n分析从对数形式的贝叶斯法则开始，该法则表明对数后验与对数似然和对数先验之和成正比，相差一个加法常数：\n$$\n\\ln p(x|\\text{data}) = \\ln p(\\text{data}|x) + \\ln p(x) + C\n$$\n先验概率密度 $p(x)$ 以信息形式给出，与 $\\exp(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x)$ 成正比。指数的参数，我们记为 $J_{\\text{prior}}(x)$，是：\n$$\nJ_{\\text{prior}}(x) = -\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\n$$\n\n1.  标量更新法则的推导。\n\n对于单个标量观测 $z = h^{\\top} x + v$，其中噪声 $v$ 服从分布 $\\mathcal{N}(0, r)$，其似然函数 $p(z|x)$ 是一个高斯函数：\n$$\np(z|x) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\left(-\\frac{1}{2r}(z - h^{\\top} x)^2\\right)\n$$\n忽略与 $x$ 无关的常数，对数似然与指数的参数成正比，我们称之为 $J_{\\text{like}}(x)$：\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2r}(z - h^{\\top} x)^2\n$$\n展开这个二次型：\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2r}(z^2 - 2z h^{\\top} x + (h^{\\top} x)^2) = -\\frac{1}{2r}(z^2 - 2z x^{\\top} h + x^{\\top} h h^{\\top} x)\n$$\n去掉与 $x$ 无关的项 $-\\frac{z^2}{2r}$，我们得到：\n$$\nJ_{\\text{like}}(x) = \\frac{z}{r} x^{\\top} h - \\frac{1}{2r} x^{\\top} h h^{\\top} x = -\\frac{1}{2} x^{\\top} \\left( \\frac{1}{r} h h^{\\top} \\right) x + \\left( \\frac{z}{r} h \\right)^{\\top} x\n$$\n后验对数密度参数 $J_{\\text{post}}(x)$ 是先验和似然参数之和：\n$$\nJ_{\\text{post}}(x) = J_{\\text{prior}}(x) + J_{\\text{like}}(x) = \\left(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\\right) + \\left(-\\frac{1}{2} x^{\\top} \\left(\\frac{1}{r} h h^{\\top}\\right) x + \\left(\\frac{z}{r} h\\right)^{\\top} x\\right)\n$$\n对 $x$ 的二次项和一次项进行分组：\n$$\nJ_{\\text{post}}(x) = -\\frac{1}{2} x^{\\top} \\left(Y + \\frac{1}{r} h h^{\\top}\\right) x + \\left(y + \\frac{z}{r} h\\right)^{\\top} x\n$$\n通过将其与通用信息形式 $-\\frac{1}{2}x^{\\top} Y^{+} x + (y^{+})^{\\top} x$ 进行比较，我们确定更新后的信息矩阵 $Y^{+}$ 和信息向量 $y^{+}$：\n$$\nY^{+} = Y + \\frac{1}{r} h h^{\\top}\n$$\n$$\ny^{+} = y + \\frac{z}{r} h\n$$\n\n2.  序贯应用标量更新。\n\n我们从先验信息 $Y = \\begin{pmatrix} 3  & 1 \\\\ 1  & 2 \\end{pmatrix}$ 和 $y = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ 开始。\n\n第一次观测更新：$h_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, r_{1} = \\frac{1}{2}, z_{1} = \\frac{3}{2}$。\n来自此观测的信息贡献是 $\\frac{1}{r_1}h_{1}h_{1}^{\\top}$ 和 $\\frac{z_1}{r_1}h_1$。\n$$\n\\frac{1}{r_1}h_{1}h_{1}^{\\top} = \\frac{1}{1/2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1  & 1 \\end{pmatrix} = 2\\begin{pmatrix} 1  & 1 \\\\ 1  & 1 \\end{pmatrix} = \\begin{pmatrix} 2  & 2 \\\\ 2  & 2 \\end{pmatrix}\n$$\n$$\n\\frac{z_1}{r_1}h_1 = \\frac{3/2}{1/2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 3\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\n中间后验信息矩阵 $Y_{1}$ 和向量 $y_{1}$ 是：\n$$\nY_{1} = Y + \\frac{1}{r_1}h_{1}h_{1}^{\\top} = \\begin{pmatrix} 3  & 1 \\\\ 1  & 2 \\end{pmatrix} + \\begin{pmatrix} 2  & 2 \\\\ 2  & 2 \\end{pmatrix} = \\begin{pmatrix} 5  & 3 \\\\ 3  & 4 \\end{pmatrix}\n$$\n$$\ny_{1} = y + \\frac{z_1}{r_1}h_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\n$$\n第二次观测更新：$h_{2} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, r_{2} = 2, z_{2} = -\\frac{1}{2}$。\n此更新应用于中间后验 $(Y_1, y_1)$。\n$$\n\\frac{1}{r_2}h_{2}h_{2}^{\\top} = \\frac{1}{2}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 2  & -1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 4  & -2 \\\\ -2  & 1 \\end{pmatrix} = \\begin{pmatrix} 2  & -1 \\\\ -1  & \\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\frac{z_2}{r_2}h_2 = \\frac{-1/2}{2}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = -\\frac{1}{4}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}\n$$\n最终的序贯后验信息矩阵 $Y_{\\mathrm{seq}}$ 和向量 $y_{\\mathrm{seq}}$ 是：\n$$\nY_{\\mathrm{seq}} = Y_{1} + \\frac{1}{r_2}h_{2}h_{2}^{\\top} = \\begin{pmatrix} 5  & 3 \\\\ 3  & 4 \\end{pmatrix} + \\begin{pmatrix} 2  & -1 \\\\ -1  & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 7  & 2 \\\\ 2  & \\frac{9}{2} \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{seq}} = y_{1} + \\frac{z_2}{r_2}h_2 = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix}\n$$\n\n3.  批量更新的推导与应用。\n\n对于多观测模型 $z = Hx + v$（其中 $v \\sim \\mathcal{N}(0, R)$），忽略常数，对数似然参数为：\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}(z - Hx)^{\\top} R^{-1} (z - Hx)\n$$\n展开此表达式：\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}(z^{\\top}R^{-1}z - z^{\\top}R^{-1}Hx - x^{\\top}H^{\\top}R^{-1}z + x^{\\top}H^{\\top}R^{-1}Hx)\n$$\n去掉项 $-\\frac{1}{2}z^{\\top}R^{-1}z$ 并合并两个线性项（它们是标量且互为转置）：\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}x^{\\top}(H^{\\top}R^{-1}H)x + (H^{\\top}R^{-1}z)^{\\top}x\n$$\n批量后验指数为 $J_{\\text{post}}(x) = J_{\\text{prior}}(x) + J_{\\text{like}}(x)$：\n$$\nJ_{\\text{post}}(x) = \\left(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\\right) + \\left(-\\frac{1}{2}x^{\\top}(H^{\\top}R^{-1}H)x + (H^{\\top}R^{-1}z)^{\\top}x\\right)\n$$\n$$\nJ_{\\text{post}}(x) = -\\frac{1}{2}x^{\\top}(Y + H^{\\top}R^{-1}H)x + (y + H^{\\top}R^{-1}z)^{\\top}x\n$$\n这给出了批量更新法则：\n$$\nY_{\\mathrm{bat}} = Y + H^{\\top}R^{-1}H\n$$\n$$\ny_{\\mathrm{bat}} = y + H^{\\top}R^{-1}z\n$$\n现在我们使用给定的批量数据应用此法则：$H = \\begin{pmatrix} 1  & 1 \\\\ 2  & -1 \\end{pmatrix}$， $R = \\begin{pmatrix} \\frac{1}{2}  & 0 \\\\ 0  & 2 \\end{pmatrix}$， $z = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n协方差矩阵的逆是 $R^{-1} = \\begin{pmatrix} 2  & 0 \\\\ 0  & \\frac{1}{2} \\end{pmatrix}$。\n首先，我们计算信息贡献矩阵 $H^{\\top}R^{-1}H$：\n$$\nH^{\\top}R^{-1}H = \\begin{pmatrix} 1  & 2 \\\\ 1  & -1 \\end{pmatrix} \\begin{pmatrix} 2  & 0 \\\\ 0  & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1  & 1 \\\\ 2  & -1 \\end{pmatrix} = \\begin{pmatrix} 2  & 1 \\\\ 2  & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1  & 1 \\\\ 2  & -1 \\end{pmatrix} = \\begin{pmatrix} 4  & 1 \\\\ 1  & \\frac{5}{2} \\end{pmatrix}\n$$\n然后，我们计算信息贡献向量 $H^{\\top}R^{-1}z$：\n$$\nH^{\\top}R^{-1}z = \\begin{pmatrix} 1  & 2 \\\\ 1  & -1 \\end{pmatrix} \\begin{pmatrix} 2  & 0 \\\\ 0  & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1  & 2 \\\\ 1  & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 3-\\frac{1}{2} \\\\ 3+\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{13}{4} \\end{pmatrix}\n$$\n现在，我们求出批量后验信息矩阵 $Y_{\\mathrm{bat}}$ 和向量 $y_{\\mathrm{bat}}$：\n$$\nY_{\\mathrm{bat}} = Y + H^{\\top}R^{-1}H = \\begin{pmatrix} 3  & 1 \\\\ 1  & 2 \\end{pmatrix} + \\begin{pmatrix} 4  & 1 \\\\ 1  & \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} 7  & 2 \\\\ 2  & \\frac{9}{2} \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{bat}} = y + H^{\\top}R^{-1}z = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{13}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{2+5}{2} \\\\ \\frac{-8+13}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix}\n$$\n\n最后，我们计算标量 $S$。\n通过比较序贯更新和批量更新的结果，我们发现它们是相同的：\n$$\nY_{\\mathrm{seq}} = \\begin{pmatrix} 7  & 2 \\\\ 2  & \\frac{9}{2} \\end{pmatrix} = Y_{\\mathrm{bat}}\n$$\n$$\ny_{\\mathrm{seq}} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix} = y_{\\mathrm{bat}}\n$$\n因此，差值为零矩阵和零向量：\n$$\nY_{\\mathrm{seq}} - Y_{\\mathrm{bat}} = \\begin{pmatrix} 0  & 0 \\\\ 0  & 0 \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{seq}} - y_{\\mathrm{bat}} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n标量 $S$ 定义为 $S = \\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} \\;+\\; \\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2}$。\n零矩阵的弗罗贝尼乌斯范数平方是：\n$$\n\\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} = 0^2 + 0^2 + 0^2 + 0^2 = 0\n$$\n零向量的欧几里得范数平方是：\n$$\n\\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2} = 0^2 + 0^2 = 0\n$$\n因此，$S$ 的值为：\n$$\nS = 0 + 0 = 0\n$$\n这个结果证明了线性高斯系统的一个基本性质：序贯贝叶斯更新等价于同时整合所有数据的单次批量更新。", "answer": "$$\\boxed{0}$$", "id": "3390747"}, {"introduction": "在数据同化中，对观测误差的准确建模至关重要，但我们常常简化假设，例如忽略误差之间的相关性。本练习 [@problem_id:3390743] 构建了一个警示性的反例，旨在量化这种模型误设定的后果。您将推导并比较在正确和错误（对角化）的误差协方差假设下，后验不确定性的差异，从而揭示忽略正相关误差如何导致对估计结果的过度自信。", "problem": "考虑一个一维线性高斯数据同化设置，其标量状态变量为 $x \\in \\mathbb{R}$。设先验（背景）分布为 $x \\sim \\mathcal{N}(x_b, \\sigma_b^{2})$，其中 $\\sigma_b^{2} = 1$。两个传感器通过一个线性观测算子 $H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 观测同一状态 $x$，根据 $y = H x + \\varepsilon$ 产生一个观测向量 $y \\in \\mathbb{R}^{2}$，其中观测误差 $\\varepsilon$ 是联合高斯分布，均值为零。\n\n实际上，观测误差是相关的，其协方差为\n$$\nR_{\\text{true}} = \\sigma_o^{2} \\begin{pmatrix} 1  & \\rho \\\\ \\rho  & 1 \\end{pmatrix},\n$$\n其中 $\\sigma_o^{2} = 1$，相关系数为 $\\rho = \\frac{1}{2}$。然而，假设一位分析师错误地假定误差不相关，并使用对角近似\n$$\nR_{\\text{diag}} = \\sigma_o^{2} I_{2} = \\begin{pmatrix} 1  & 0 \\\\ 0  & 1 \\end{pmatrix}.\n$$\n\n从高斯变量的贝叶斯法则和线性高斯模型中后验的定义出发，推导正确指定情况（使用 $R_{\\text{true}}$）和错误指定情况（使用 $R_{\\text{diag}}$）下的后验协方差。由此，求出相应的后验信息矩阵（后验协方差的逆）。\n\n然后计算以下两个量，它们共同构成一个具体的反例：\n\n1. 错误指定的后验方差与正确指定的后验方差之比，即 $\\frac{P_{\\text{diag}}}{P_{\\text{true}}}$。\n2. 错误指定的后验信息与正确指定的后验信息之差，即 $J_{\\text{diag}} - J_{\\text{true}}$。\n\n将最终答案表示为使用精确值（不四舍五入）的单行矩阵。不需要单位。", "solution": "在线性高斯模型中，后验信息矩阵（后验协方差的逆）由先验信息和观测信息之和给出：\n$$\nJ_a = P_a^{-1} = P_b^{-1} + H^T R^{-1} H\n$$\n其中 $P_b$ 是先验协方差，$R$ 是观测误差协方差。在本题中，状态 $x$ 是一个标量，因此先验协方差 $P_b = \\sigma_b^2 = 1$，先验信息 $P_b^{-1} = 1$。观测算子为 $H = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n**1. 正确指定的情况（使用 $R_{\\text{true}}$）**\n\n真实的观测误差协方差为 $R_{\\text{true}} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix}$。\n其逆矩阵为：\n$$\nR_{\\text{true}}^{-1} = \\frac{1}{1 - (1/2)^2} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 1 \\end{pmatrix} = \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix}\n$$\n来自观测的信息贡献为：\n$$\nH^T R_{\\text{true}}^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 4/3 & -2/3 \\\\ -2/3 & 4/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2/3 & 2/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{4}{3}\n$$\n因此，正确指定的后验信息和后验方差为：\n$$\nJ_{\\text{true}} = P_b^{-1} + H^T R_{\\text{true}}^{-1} H = 1 + \\frac{4}{3} = \\frac{7}{3}\n$$\n$$\nP_{\\text{true}} = J_{\\text{true}}^{-1} = \\frac{3}{7}\n$$\n\n**2. 错误指定的情况（使用 $R_{\\text{diag}}$）**\n\n错误指定的观测误差协方差为 $R_{\\text{diag}} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$，其逆矩阵 $R_{\\text{diag}}^{-1} = I_2$。\n来自观测的信息贡献为：\n$$\nH^T R_{\\text{diag}}^{-1} H = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 + 1 = 2\n$$\n因此，错误指定的后验信息和后验方差为：\n$$\nJ_{\\text{diag}} = P_b^{-1} + H^T R_{\\text{diag}}^{-1} H = 1 + 2 = 3\n$$\n$$\nP_{\\text{diag}} = J_{\\text{diag}}^{-1} = \\frac{1}{3}\n$$\n\n**3. 最终计算**\n\n我们计算所要求的量：\n1.  后验方差之比：\n    $$\n    \\frac{P_{\\text{diag}}}{P_{\\text{true}}} = \\frac{1/3}{3/7} = \\frac{1}{3} \\times \\frac{7}{3} = \\frac{7}{9}\n    $$\n2.  后验信息之差：\n    $$\n    J_{\\text{diag}} - J_{\\text{true}} = 3 - \\frac{7}{3} = \\frac{9-7}{3} = \\frac{2}{3}\n    $$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{7}{9}  \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "3390743"}, {"introduction": "信息矩阵不仅用于状态估计，更是指导实验设计的有力工具。它的特征值和特征向量揭示了状态空间中不确定性的大小和方向。本练习 [@problem_id:3390762] 将引导您分析如何通过增加一个精心设计的传感器来减少系统中最不确定的方向上的误差。您将计算新传感器如何改变信息矩阵的最小特征值，并直观地理解如何利用这一洞察力来策略性地获取信息，以最高效地降低整体不确定性。", "problem": "考虑一个数据同化背景下的状态向量 $x \\in \\mathbb{R}^{3}$ 的线性高斯逆问题。在同化了初始的一组传感器后，当前的后验信息（精度）矩阵 $Y_{\\text{post}}$ 具有一个标准正交特征基 $\\{v_{1}, v_{2}, v_{3}\\}$，其谱分解为 $Y_{\\text{post}} = U \\,\\operatorname{diag}(\\lambda_{1}, \\lambda_{2}, \\lambda_{3})\\, U^{\\top}$，其中 $U = [v_{1}\\ v_{2}\\ v_{3}]$，特征值为 $\\lambda_{1} = 9$，$\\lambda_{2} = 2$ 和 $\\lambda_{3} = 0.08$。你正考虑通过增加一个额外的标量传感器来增强传感网络，该传感器产生 $y_{\\star}$，由线性观测算子 $H_{\\star}$ 和独立高斯噪声 $\\varepsilon_{\\star}$ 建模：\n$$\ny_{\\star} = H_{\\star} x + \\varepsilon_{\\star}, \\quad \\varepsilon_{\\star} \\sim \\mathcal{N}(0, R_{\\star}),\n$$\n其中 $H_{\\star} = \\gamma\\, v_{3}^{\\top}$，$\\gamma = 0.2$，$R_{\\star} = 0.04$。从线性高斯模型的第一性原理和对数后验海森矩阵的定义出发，推导增加的传感器如何改变后验信息矩阵的最小特征值，并计算其精确数值。然后，定性地解释这一变化如何对应于沿观测不足方向 $v_{3}$ 的不确定性减少。\n\n你的最终答案必须是新的后验信息矩阵的最小特征值的单个数值。无需四舍五入。", "solution": "该问题要求我们确定新的传感器测量对状态向量 $x \\in \\mathbb{R}^{3}$ 的后验信息矩阵的影响。我们必须首先推导信息矩阵的更新规则，将其应用于给定问题，计算新的最小特征值，然后提供定性解释。\n\n在同化新测量值 $y_{\\star}$ 后，状态 $x$ 的后验概率密度函数 (PDF) 由贝叶斯定理给出：\n$$\np(x | y_{\\star}, \\text{previous data}) \\propto p(y_{\\star} | x) \\, p(x | \\text{previous data})\n$$\n项 $p(x | \\text{previous data})$ 表示当前的后验信念，它作为此次更新的先验。在线性高斯框架中，该分布是高斯分布，$x \\sim \\mathcal{N}(\\mu_{\\text{post}}, P_{\\text{post}})$，其中后验协方差为 $P_{\\text{post}} = Y_{\\text{post}}^{-1}$。信息矩阵 $Y_{\\text{post}}$ 是先前同化步骤中负对数后验的Hessian矩阵。PDF与 $\\exp(-\\frac{1}{2}(x - \\mu_{\\text{post}})^{\\top} Y_{\\text{post}} (x - \\mu_{\\text{post}}))$ 成正比。\n\n项 $p(y_{\\star} | x)$ 是新测量的似然，由传感器模型 $y_{\\star} = H_{\\star}x + \\varepsilon_{\\star}$ 给出，噪声 $\\varepsilon_{\\star} \\sim \\mathcal{N}(0, R_{\\star})$。这对应于 PDF $p(y_{\\star} | x) \\propto \\exp(-\\frac{1}{2}(y_{\\star} - H_{\\star}x)^{\\top} R_{\\star}^{-1} (y_{\\star} - H_{\\star}x))$。\n\n因此，新的后验PDF与这些指数项的乘积成正比。新后验的负对数，记为 $J_{\\text{new}}(x)$，（在忽略一个加性常数的情况下）为：\n$$\nJ_{\\text{new}}(x) = \\frac{1}{2}(x - \\mu_{\\text{post}})^{\\top} Y_{\\text{post}} (x - \\mu_{\\text{post}}) + \\frac{1}{2}(y_{\\star} - H_{\\star}x)^{\\top} R_{\\star}^{-1} (y_{\\star} - H_{\\star}x)\n$$\n新的后验信息矩阵 $Y_{\\text{new}}$ 定义为该负对数后验的Hessian矩阵，$Y_{\\text{new}} = \\nabla_x^2 J_{\\text{new}}(x)$。为计算Hessian矩阵，我们首先求梯度 $\\nabla_x J_{\\text{new}}(x)$：\n$$\n\\nabla_x J_{\\text{new}}(x) = Y_{\\text{post}}(x - \\mu_{\\text{post}}) + H_{\\star}^{\\top} R_{\\star}^{-1} (H_{\\star}x - y_{\\star})\n$$\n再次对 $x$ 求导得到Hessian矩阵：\n$$\n\\nabla_x^2 J_{\\text{new}}(x) = Y_{\\text{post}} + H_{\\star}^{\\top}R_{\\star}^{-1}H_{\\star}\n$$\n因此，信息更新规则为：\n$$\nY_{\\text{new}} = Y_{\\text{post}} + H_{\\star}^{\\top}R_{\\star}^{-1}H_{\\star}\n$$\n现在，我们将此规则应用于问题的具体细节。给定当前后验信息矩阵的谱分解：\n$$\nY_{\\text{post}} = \\sum_{i=1}^{3} \\lambda_i v_i v_i^{\\top} = \\lambda_1 v_1 v_1^{\\top} + \\lambda_2 v_2 v_2^{\\top} + \\lambda_3 v_3 v_3^{\\top}\n$$\n其特征值为 $\\lambda_1 = 9$，$\\lambda_2 = 2$ 和 $\\lambda_3 = 0.08$。特征向量 $\\{v_1, v_2, v_3\\}$ 构成一个标准正交基。\n新传感器的观测模型定义为 $H_{\\star} = \\gamma v_3^{\\top}$，其中 $\\gamma = 0.2$，噪声方差为 $R_{\\star} = 0.04$。由于 $R_{\\star}$ 是标量，其逆为 $R_{\\star}^{-1} = 1/R_{\\star}$。\n\n更新项是一个秩为1的矩阵：\n$$\nH_{\\star}^{\\top}R_{\\star}^{-1}H_{\\star} = (\\gamma v_3) (R_{\\star}^{-1}) (\\gamma v_3^{\\top}) = \\frac{\\gamma^2}{R_{\\star}} v_3 v_3^{\\top}\n$$\n将此代入更新规则：\n$$\nY_{\\text{new}} = \\left( \\lambda_1 v_1 v_1^{\\top} + \\lambda_2 v_2 v_2^{\\top} + \\lambda_3 v_3 v_3^{\\top} \\right) + \\frac{\\gamma^2}{R_{\\star}} v_3 v_3^{\\top}\n$$\n我们可以合并与每个特征向量相关的项：\n$$\nY_{\\text{new}} = \\lambda_1 v_1 v_1^{\\top} + \\lambda_2 v_2 v_2^{\\top} + \\left(\\lambda_3 + \\frac{\\gamma^2}{R_{\\star}}\\right) v_3 v_3^{\\top}\n$$\n这个表达式是新信息矩阵 $Y_{\\text{new}}$ 的谱分解。特征向量不变，仍为 $v_1$，$v_2$ 和 $v_3$。新的特征值，记为 $\\lambda_i^{\\text{new}}$，是 $v_i v_i^{\\top}$ 项的系数：\n$$\n\\lambda_1^{\\text{new}} = \\lambda_1 = 9\n$$\n$$\n\\lambda_2^{\\text{new}} = \\lambda_2 = 2\n$$\n$$\n\\lambda_3^{\\text{new}} = \\lambda_3 + \\frac{\\gamma^2}{R_{\\star}}\n$$\n现在我们计算 $\\lambda_3^{\\text{new}}$ 的数值。给定 $\\lambda_3 = 0.08$，$\\gamma = 0.2$ 和 $R_{\\star} = 0.04$。\n$$\n\\frac{\\gamma^2}{R_{\\star}} = \\frac{(0.2)^2}{0.04} = \\frac{0.04}{0.04} = 1\n$$\n因此，新的第三个特征值为：\n$$\n\\lambda_3^{\\text{new}} = 0.08 + 1 = 1.08\n$$\n新的特征值集合为 $\\{9, 2, 1.08\\}$。因此，新的后验信息矩阵的最小特征值为 $1.08$。\n\n定性地看，信息矩阵的特征值与协方差矩阵的特征值成反比 ($Y = P^{-1}$)。协方差矩阵的一个特征值代表了状态估计在相应特征向量方向上的方差（不确定性的度量）。因此，信息矩阵的一个小特征值 $\\lambda_i$ 对应于 $v_i$ 方向上的大方差（高不确定性）。\n最初，最小的特征值是 $\\lambda_3 = 0.08$，这表明 $v_3$ 方向是观测最差的方向，即它具有最高的后验不确定性。\n新传感器的观测算子为 $H_{\\star} = \\gamma v_3^{\\top}$，这意味着它测量状态向量 $x$ 沿 $v_3$ 方向的分量（因为 $H_{\\star}x = \\gamma v_3^{\\top}x$）。该传感器被专门设计用来收集关于现有最大不确定性方向上状态的信息。\n对该测量的同化增加了信息，特别是沿 $v_3$ 方向的信息。这在数学上反映为只有 $\\lambda_3$ 被更新，而 $\\lambda_1$ 和 $\\lambda_2$ 保持不变。新的特征值 $\\lambda_3^{\\text{new}} = 1.08$ 显著大于原始的 $\\lambda_3 = 0.08$。信息特征值的这一增加对应于 $v_3$ 方向后验方差（不确定性）的显著减少。该传感器成功地减少了先前最不确定方向上的不确定性。更新后系统的最小特征值现在是 $1.08$，相比之前的 $0.08$ 是一个实质性的改进。", "answer": "$$\n\\boxed{1.08}\n$$", "id": "3390762"}]}