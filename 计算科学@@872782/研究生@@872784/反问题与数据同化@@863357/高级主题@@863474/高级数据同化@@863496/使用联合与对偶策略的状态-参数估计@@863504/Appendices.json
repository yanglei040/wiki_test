{"hands_on_practices": [{"introduction": "在深入研究复杂的计算实现之前，我们首先通过一个理论练习来建立对联合与对偶策略之间关系的基本理解。这个练习 [@problem_id:3421586] 在一个简化的线性高斯框架内，旨在从数学上证明，在特定条件下，对增广状态进行一步联合更新与先更新状态再通过回归更新参数的两步对偶方法是等价的。完成这个推导有助于揭示这两种策略背后的核心机制，并为后续更复杂的应用奠定坚实的基础。", "problem": "考虑一个增广空间中的线性高斯状态-参数估计问题，其增广状态为 $z = (x,\\theta)^{\\top} \\in \\mathbb{R}^{2}$。预报（背景）分布为高斯分布，其均值为 $m^{f} = (m_{x}^{f}, m_{\\theta}^{f})^{\\top}$，协方差为\n$$\nP^{f} \\;=\\; \\begin{pmatrix}\np_{x} & c \\\\\nc & p_{\\theta}\n\\end{pmatrix},\n$$\n其中 $p_{x} > 0$, $p_{\\theta} > 0$, 且 $|c|  \\sqrt{p_{x} p_{\\theta}}$。观测模型为 $y = H z + v$，其中 $H = (1,\\,0)$，因此测量仅依赖于状态分量 $x$；且 $v \\sim \\mathcal{N}(0,R)$，其中 $R = r > 0$。假设在大样本、线性高斯极限下，集合变换卡尔曼滤波器 (Ensemble Transform Kalman Filter, ETKF) 的均值更新与卡尔曼滤波器 (Kalman filter, KF) 的均值更新一致。\n\n定义增广空间中到状态和参数子空间的正交投影为\n$$\nP_{x} \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  0\n\\end{pmatrix}, \n\\qquad\nP_{\\theta} \\;=\\; \\begin{pmatrix}\n0  0 \\\\\n0  1\n\\end{pmatrix}.\n$$\n\n比较以下两种计算参数 $\\theta$ 的分析均值的策略：\n\n1. 对增广状态 $z$ 进行联合 ETKF，即在增广空间中使用 $H$ 进行单次分析更新。\n\n2. 两步 ETKF（对偶策略）：首先仅使用 $y$ 更新状态 $x$；然后利用背景交叉协方差将 $\\theta$ 回归到 $x$ 上来更新 $\\theta$，即使用由正交投影耦合 $G = P_{\\theta} P^{f} P_{x} \\left(P_{x} P^{f} P_{x}\\right)^{-1}$ 导出的线性映射，使得参数均值按 $m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + G\\left(m_{x}^{a} - m_{x}^{f}\\right)$ 增量更新。\n\n从高斯-贝叶斯/卡尔曼更新原理和上述投影定义出发，推导参数分析均值之差的闭式表达式\n$$\n\\Delta m_{\\theta} \\;=\\; m_{\\theta}^{a,\\text{dual}} - m_{\\theta}^{a,\\text{joint}}\n$$\n该表达式应由 $m_{x}^{f}$、 $m_{\\theta}^{f}$、 $y$、 $p_{x}$、 $c$ 和 $r$ 表示。请将最终答案表示为单个简化的解析表达式。不需要数值四舍五入。此处不适用角度或物理单位。", "solution": "该问题要求在线性高斯框架内比较两种状态-参数估计策略：联合更新和对偶（两步）更新。我们需要求解这两种方法产生的参数分析均值之差。此分析是在集合变换卡尔曼滤波器 (ETKF) 的均值更新等同于标准卡尔曼滤波器 (KF) 更新的假设下进行的。\n\n状态向量 $z$ 的通用卡尔曼滤波器分析均值更新由下式给出：\n$$m^{a} = m^{f} + K(y - H m^{f})$$\n其中 $m^{f}$ 是预报（背景）均值，$y$ 是观测值，$H$ 是观测算子，$K$ 是卡尔曼增益，定义为：\n$$K = P^{f} H^{\\top} (H P^{f} H^{\\top} + R)^{-1}$$\n此处，$P^{f}$ 是预报误差协方差，$R$ 是观测误差协方差。\n\n给定以下信息：\n增广状态为 $z = (x, \\theta)^{\\top}$。\n预报均值为 $m^{f} = (m_{x}^{f}, m_{\\theta}^{f})^{\\top}$。\n预报协方差为 $P^{f} = \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix}$。\n观测模型为 $y = H z + v$，其中 $H = (1, 0)$ 且 $v \\sim \\mathcal{N}(0, r)$，因此 $R=r$。\n\n首先，我们使用联合更新策略推导参数 $\\theta$ 的分析均值，记为 $m_{\\theta}^{a,\\text{joint}}$。\n\n**1. 联合 ETKF/KF 更新**\n\n在此策略中，更新应用于整个增广状态 $z$。我们首先计算卡尔曼增益 $K$ 的分量。\n观测算子为 $H = (1, 0)$，其转置为 $H^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n$H P^{f} H^{\\top}$ 项为：\n$$H P^{f} H^{\\top} = (1, 0) \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (p_{x}, c) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = p_{x}$$\n因此，$(H P^{f} H^{\\top} + R)^{-1}$ 项为 $(p_{x} + r)^{-1}$。\n\n$P^{f} H^{\\top}$ 项为：\n$$P^{f} H^{\\top} = \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix}$$\n现在，我们可以计算卡尔曼增益 $K$：\n$$K = \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix} (p_{x} + r)^{-1} = \\frac{1}{p_{x} + r} \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix}$$\n\n新息项为 $y - H m^{f}$：\n$$y - H m^{f} = y - (1, 0) \\begin{pmatrix} m_{x}^{f} \\\\ m_{\\theta}^{f} \\end{pmatrix} = y - m_{x}^{f}$$\n增广均值向量 $m^{a,\\text{joint}}$ 的分析更新为：\n$$m^{a,\\text{joint}} = \\begin{pmatrix} m_{x}^{a,\\text{joint}} \\\\ m_{\\theta}^{a,\\text{joint}} \\end{pmatrix} = \\begin{pmatrix} m_{x}^{f} \\\\ m_{\\theta}^{f} \\end{pmatrix} + \\frac{1}{p_{x} + r} \\begin{pmatrix} p_{x} \\\\ c \\end{pmatrix} (y - m_{x}^{f})$$\n从该表达式中，我们提取参数分量 $\\theta$ 的分析均值：\n$$m_{\\theta}^{a,\\text{joint}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n\n接下来，我们使用对偶策略推导参数 $\\theta$ 的分析均值 $m_{\\theta}^{a,\\text{dual}}$。\n\n**2. 对偶（两步）ETKF/KF 更新**\n\n此策略包含两个步骤。\n\n**步骤 2a：更新状态分量 $x$。**\n这是一个标量卡尔曼滤波器问题。状态为 $x$，预报均值为 $m_{x}^{f}$，预报方差为 $p_{x}$。观测为 $y = x + v$，观测算子为 $H_{x}=1$，观测误差方差为 $r$。\n标量卡尔曼增益 $K_{x}$ 为：\n$$K_{x} = p_{x} (H_{x} p_{x} H_{x}^{\\top} + r)^{-1} = p_{x} (1 \\cdot p_{x} \\cdot 1 + r)^{-1} = \\frac{p_{x}}{p_{x} + r}$$\n$x$ 的分析均值（记为 $m_{x}^{a}$）为：\n$$m_{x}^{a} = m_{x}^{f} + K_{x} (y - H_{x} m_{x}^{f}) = m_{x}^{f} + \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f})$$\n\n**步骤 2b：通过回归更新参数分量 $\\theta$。**\n问题指明了参数均值的更新规则：\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + G(m_{x}^{a} - m_{x}^{f})$$\n其中 $G$ 是回归系数，表示 $x$ 的更新对 $\\theta$ 的影响。该系数由背景协方差结构导出。对于多元高斯分布，给定 $x$ 时 $\\theta$ 的条件期望为 $\\mathbb{E}[\\theta|x] = m_{\\theta}^{f} + \\text{Cov}(\\theta, x) \\text{Var}(x)^{-1} (x - m_{x}^{f})$。因此，回归系数为 $G = \\text{Cov}(\\theta, x) \\text{Var}(x)^{-1}$。从背景协方差矩阵 $P^{f}$ 中，我们得到 $\\text{Cov}(\\theta, x) = c$ 和 $\\text{Var}(x) = p_{x}$。\n因此，回归系数为 $G = \\frac{c}{p_{x}}$。\n\n问题提供了使用投影算子的 $G$ 的形式化定义，$G = P_{\\theta} P^{f} P_{x} (P_{x} P^{f} P_{x})^{-1}$。我们来验证这是否产生相同的标量系数。该算子将 $x$ 的子空间映射到 $\\theta$ 的子空间。\n$P_{x} P^{f} P_{x} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} p_{x}  0 \\\\ 0  0 \\end{pmatrix}$。\n在相关子空间上的逆（或伪逆）是 $(P_{x} P^{f} P_{x})^{\\dagger} = \\begin{pmatrix} p_{x}^{-1}  0 \\\\ 0  0 \\end{pmatrix}$。\n$P_{\\theta} P^{f} P_{x} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} p_{x}  c \\\\ c  p_{\\theta} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix}$。\n完整的算子是 $G_{op} = \\begin{pmatrix} 0  0 \\\\ c  0 \\end{pmatrix} \\begin{pmatrix} p_{x}^{-1}  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ c p_{x}^{-1}  0 \\end{pmatrix}$。该算子将增量向量 $(m_x^a - m_x^f, 0)^{\\top}$ 映射到 $(0, \\frac{c}{p_x}(m_x^a - m_x^f))^{\\top}$。标量回归系数 $G$ 确实是 $\\frac{c}{p_{x}}$。\n\n现在我们将步骤 2a 中的状态增量 $(m_{x}^{a} - m_{x}^{f})$ 代入回归公式：\n$$m_{x}^{a} - m_{x}^{f} = \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f})$$\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\left(\\frac{c}{p_{x}}\\right) \\left( \\frac{p_{x}}{p_{x} + r} (y - m_{x}^{f}) \\right)$$\n分子和分母中的 $p_{x}$ 项相互抵消：\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n\n**3. 比较与最终差异**\n\n我们现在得到了两种策略下参数分析均值的表达式：\n$$m_{\\theta}^{a,\\text{joint}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n$$m_{\\theta}^{a,\\text{dual}} = m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f})$$\n这两个表达式是相同的。这表明对于观测仅依赖于状态分量的线性高斯系统，参数的联合更新等同于先更新状态，然后使用背景统计量通过回归更新参数的两步过程。\n\n因此，参数分析均值之差为：\n$$\\Delta m_{\\theta} = m_{\\theta}^{a,\\text{dual}} - m_{\\theta}^{a,\\text{joint}}$$\n$$\\Delta m_{\\theta} = \\left( m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f}) \\right) - \\left( m_{\\theta}^{f} + \\frac{c}{p_{x} + r} (y - m_{x}^{f}) \\right)$$\n$$\\Delta m_{\\theta} = 0$$\n\n该差值的闭式表达式就是数字零。", "answer": "$$\\boxed{0}$$", "id": "3421586"}, {"introduction": "从理论基础出发，我们现在转向一个更具挑战性的实际应用。本练习 [@problem_id:3421568] 要求您在一个经典的混沌系统——洛伦兹-96模型上，实现并比较联合与对偶集合卡尔曼滤波器（EnKF）。通过这个编码实践，您将获得在动态、非线性系统中同时估计状态和时变参数的实践经验。此外，本练习的核心目标是分析滤波器稳定性如何依赖于对参数变化（即过程噪声 $Q_{\\theta}$）的假设，这是数据同化领域一个关键的性能考量因素。", "problem": "您需要实现、比较和分析两种集合卡尔曼滤波器（EnKF）策略（联合策略和双重策略）在离散时间 Lorenz-96 系统上进行同步状态-参数估计时的稳定性。Lorenz-96 动力学由以下常微分方程给出：\n$$\n\\frac{dx_i}{dt} = \\left( x_{i+1} - x_{i-2} \\right) x_{i-1} - x_i + F,\n$$\n其中索引是循环的，通过固定步长的四阶龙格-库塔格式进行离散化，以定义映射：\n$$\nx_{t+1} = M(x_t, \\theta_t) + \\eta_t,\n$$\n其中 $x_t \\in \\mathbb{R}^N$ 是状态向量，$\\theta_t \\in \\mathbb{R}$ 是一个标量时变参数，扮演强迫项 $F$ 的角色，$M(\\cdot)$ 是单步数值积分器，$\\eta_t$ 是加性模型噪声。参数演化过程为随机游走：\n$$\n\\theta_{t+1} = \\theta_t + \\xi_t,\n$$\n其中 $\\xi_t$ 是方差为 $Q_\\theta$ 的高斯噪声。观测是线性的和部分的：\n$$\ny_t = H x_t + \\varepsilon_t,\n$$\n其中 $H \\in \\mathbb{R}^{m \\times N}$ 是一个选择算子，$\\varepsilon_t$ 是协方差为 $R$ 的高斯观测噪声。\n\n解决方案的基本基础包括：(i) 状态空间模型的定义；(ii) 针对线性观测算子的高斯贝叶斯更新，\n$$\nx^a = x^f + K(y - H x^f), \\quad K = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\n其中 $x^f$ 表示预报，$x^a$ 表示分析，$P_{xy}$ 是状态（或增广状态）与预报观测之间的互协方差，$P_{yy}$ 是预报观测的协方差；以及 (iii) 集合卡尔曼滤波器原理，该原理使用集合的样本协方差来近似协方差。\n\n您必须实现两种策略：\n- 联合 EnKF：用参数增广状态，形成 $z_t = [x_t; \\theta_t]$，用各自的动力学模型对两者进行预报，并仅使用状态分量上的线性观测算子，对增广集合进行单次卡尔曼分析。\n- 双重 EnKF：每个时间步执行两次序贯分析。首先，仅使用状态-观测互协方差更新状态。其次，仅使用参数-观测互协方差更新参数。每次更新都使用与 $R$ 一致的独立扰动的观测。\n\n使用以下固定设置：\n- 状态维度 $N=10$。\n- 时间步长 $dt=0.05$。\n- 同化步数 $T=120$。\n- 观测算子 $H$ 从索引 0 开始选择每隔一个状态分量，因此 $m=5$。\n- 真实初始参数 $\\theta_0 = 8.0$。\n- 真实初始状态 $x_0$ 按 $x_{0,i} = \\theta_0 + \\delta_i$ 抽取，其中 $\\delta_i \\sim \\mathcal{N}(0,1)$ 对 $i=1,\\dots,N$ 独立。\n- 状态模型噪声方差 $Q_x = 0.01$（对角协方差）。\n- 观测噪声协方差 $R = \\sigma_{\\text{obs}}^2 I_m$，其中 $\\sigma_{\\text{obs}} = 1.0$。\n- 集合大小 $N_e = 25$。\n- 初始集合均值猜测：$x_0^{\\text{guess}} = x_0 + \\epsilon$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,1)$，以及 $\\theta_0^{\\text{guess}} = \\theta_0 - 1.0$，状态的集合离散度为 $1.0$，参数的集合离散度为 $0.5$。\n\n将参数过程噪声方差 $Q_\\theta$ 视为研究变量。对于下面给出的测试套件中的每个 $Q_\\theta$ 值，在其真实参数演化中使用该 $Q_\\theta$ 生成一条“真实”轨迹和相应的观测序列。然后，使用在其参数预报模型中相同的 $Q_\\theta$ 运行两种滤波器。对于每种策略和每个 $Q_\\theta$，计算同化窗口内状态的时间平均均方根误差 (RMSE)：\n$$\n\\mathrm{RMSE}_x = \\frac{1}{T} \\sum_{t=1}^{T} \\left\\| x_t - \\bar{x}_t \\right\\|_2 \\big/ \\sqrt{N},\n$$\n和参数的 RMSE：\n$$\n\\mathrm{RMSE}_\\theta = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\theta_t - \\bar{\\theta}_t \\right|,\n$$\n其中 $\\bar{x}_t$ 和 $\\bar{\\theta}_t$ 是分析后时刻 $t$ 的集合均值。\n\n对于给定的 $Q_\\theta$，一个滤波器的稳定性定义如下：如果滤波器在所有时刻都产生有限值，并且满足 $\\mathrm{RMSE}_x \\le \\tau_x$ 和 $\\mathrm{RMSE}_\\theta \\le \\tau_\\theta$，则该滤波器是稳定的，其中阈值 $\\tau_x = 3.0$ 和 $\\tau_\\theta = 2.0$。在测试套件上，一个策略的稳定性阈值定义为套件中使滤波器保持稳定的最大 $Q_\\theta$ 值。\n\n参数过程噪声方差 $Q_\\theta$ 的测试套件：\n- 情况 1：$Q_\\theta = 0.0$。\n- 情况 2：$Q_\\theta = 0.001$。\n- 情况 3：$Q_\\theta = 0.01$。\n- 情况 4：$Q_\\theta = 0.05$。\n- 情况 5：$Q_\\theta = 0.1$。\n- 情况 6：$Q_\\theta = 0.5$。\n\n您的程序必须：\n- 实现具有循环索引和给定 $dt$ 的四阶龙格-库塔法的 Lorenz-96 模型积分 $M(\\cdot)$。\n- 实现如上所述的联合和双重 EnKF 策略，使用带扰动观测的随机 EnKF。\n- 对于测试套件中的每个 $Q_\\theta$，计算联合和双重滤波器的稳定性布尔值。\n- 计算两种滤波器的稳定性阈值，即套件中使每个滤波器保持稳定的最大 $Q_\\theta$；如果没有稳定的情况，则报告 $0.0$。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表，格式如下：\n$$\n[\\text{JT}, \\text{DT}, [s^{(J)}_1,\\dots,s^{(J)}_6], [s^{(D)}_1,\\dots,s^{(D)}_6]],\n$$\n其中 $\\text{JT}$ 是联合滤波器的稳定性阈值（浮点数），$\\text{DT}$ 是双重滤波器的稳定性阈值（浮点数），$s^{(J)}_k$ 是联合滤波器在测试用例 $k$ 的布尔稳定性，$s^{(D)}_k$ 是双重滤波器对应的布尔值。不应打印任何其他文本。", "solution": "Lorenz-96 系统是一个用于数据同化研究的典型混沌模型。我们首先指定动力学模型和观测模型。离散时间映射 $M(\\cdot)$ 是通过使用四阶龙格-库塔格式对常微分方程\n$$\n\\frac{dx_i}{dt} = \\left( x_{i+1} - x_{i-2} \\right) x_{i-1} - x_i + F\n$$\n进行时间步长 $dt$ 的积分得到的。索引是循环的，即 $x_{-1} = x_{N-1}$ 和 $x_{N} = x_0$ 等，确保了周期性边界条件。\n\n离散时间下的随机状态空间模型为\n$$\nx_{t+1} = M(x_t, \\theta_t) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}\\left(0, Q_x I_N\\right),\n$$\n参数演化过程为\n$$\n\\theta_{t+1} = \\theta_t + \\xi_t, \\quad \\xi_t \\sim \\mathcal{N}(0, Q_\\theta).\n$$\n观测由以下公式给出\n$$\ny_t = H x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}\\left(0, R\\right),\n$$\n其中 $H \\in \\mathbb{R}^{m \\times N}$ 选择分量的一个子集。我们通过选择索引 $0,2,4,\\dots$ 来取 $m = N/2$。\n\n集合卡尔曼滤波器（EnKF）基于线性观测算子的高斯贝叶斯更新，其中的协方差是从一个集合中估计的。在经典的线性高斯情况下，卡尔曼分析满足\n$$\nx^a = x^f + K(y - H x^f), \\quad K = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\n其中 $P_{xy}$ 是预报状态与预报观测之间的互协方差，$P_{yy}$ 是预报观测的协方差。在EnKF中，给定一个集合 $\\{x^{f,(k)}\\}_{k=1}^{N_e}$，这些协方差通过样本协方差来近似\n$$\n\\bar{x}^f = \\frac{1}{N_e} \\sum_{k=1}^{N_e} x^{f,(k)}, \\quad X' = \\left[x^{f,(1)} - \\bar{x}^f, \\dots, x^{f,(N_e)} - \\bar{x}^f\\right],\n$$\n$$\ny^{f,(k)} = H x^{f,(k)}, \\quad \\bar{y}^f = \\frac{1}{N_e} \\sum_{k=1}^{N_e} y^{f,(k)}, \\quad Y' = \\left[y^{f,(1)} - \\bar{y}^f, \\dots, y^{f,(N_e)} - \\bar{y}^f\\right],\n$$\n$$\nP_{xy} \\approx \\frac{1}{N_e-1} X' (Y')^\\top, \\quad P_{yy} \\approx \\frac{1}{N_e-1} Y' (Y')^\\top.\n$$\n在随机EnKF中，分析是针对每个成员使用扰动观测值进行的\n$$\ny^{(k)} = y + \\varepsilon^{(k)}, \\quad \\varepsilon^{(k)} \\sim \\mathcal{N}(0,R),\n$$\n每个成员通过以下方式更新\n$$\nx^{a,(k)} = x^{f,(k)} + K\\left(y^{(k)} - y^{f,(k)}\\right).\n$$\n\n对于状态-参数联合估计，我们考虑增广状态\n$$\nz_t = \\begin{bmatrix} x_t \\\\ \\theta_t \\end{bmatrix} \\in \\mathbb{R}^{N+1},\n$$\n其预报集合成员通过应用各自的动力学模型得到：\n$$\nx_{t+1}^{f,(k)} = M\\left(x_t^{a,(k)}, \\theta_t^{a,(k)}\\right) + \\eta_t^{(k)}, \\quad \\theta_{t+1}^{f,(k)} = \\theta_t^{a,(k)} + \\xi_t^{(k)}.\n$$\n我们像之前一样形成增广距平和观测距平，然后计算\n$$\nP_{zy} \\approx \\frac{1}{N_e-1} Z' (Y')^\\top, \\quad P_{yy} \\approx \\frac{1}{N_e-1} Y' (Y')^\\top,\n$$\n以及增广卡尔曼增益\n$$\nK_z = P_{zy}\\left(P_{yy} + R\\right)^{-1}.\n$$\n每个集合成员的联合分析更新为\n$$\nz^{a,(k)} = z^{f,(k)} + K_z\\left(y^{(k)} - y^{f,(k)}\\right),\n$$\n其中 $y^{f,(k)} = H x^{f,(k)}$ 且观测扰動 $\\varepsilon^{(k)}$ 是独立抽取的。\n\n对于双重 EnKF，我们执行两次序贯分析。首先，使用状态-观测互协方差进行仅状态分析\n$$\nP_{xy} \\approx \\frac{1}{N_e-1} X' (Y')^\\top, \\quad K_x = P_{xy}\\left(P_{yy} + R\\right)^{-1},\n$$\n成员更新为\n$$\nx^{a,(k)} = x^{f,(k)} + K_x\\left(y^{(k)} - y^{f,(k)}\\right),\n$$\n在此步骤中保持 $\\theta^{f,(k)}$ 不变。其次，由参数-观测互协方差驱动的仅参数分析\n$$\nP_{\\theta y} \\approx \\frac{1}{N_e-1} \\Theta' (Y')^\\top,\n$$\n其中 $\\Theta' = [\\theta^{f,(1)} - \\bar{\\theta}^f, \\dots, \\theta^{f,(N_e)} - \\bar{\\theta}^f]$ 是参数值的 $1 \\times N_e$ 距平向量。参数增益为\n$$\nK_\\theta = P_{\\theta y}\\left(P_{yy} + R\\right)^{-1},\n$$\n成员参数更新为\n$$\n\\theta^{a,(k)} = \\theta^{f,(k)} + K_\\theta\\left(y^{(k)} - y^{f,(k)}\\right).\n$$\n在这种双重方法中，预报观测 $y^{f,(k)}$ 及其距平是从预报状态计算的；将它们用于状态和参数更新可确保一致地使用相同的新息。\n\n为评估稳定性，我们计算时间平均均方根误差：\n$$\n\\mathrm{RMSE}_x = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\left\\| x_t - \\bar{x}_t \\right\\|_2}{\\sqrt{N}}, \\quad \\mathrm{RMSE}_\\theta = \\frac{1}{T} \\sum_{t=1}^{T} \\left| \\theta_t - \\bar{\\theta}_t \\right|.\n$$\n如果所有集合分析均产生有限数值（没有非数值或无穷大），并且满足阈值 $\\tau_x = 3.0$ 和 $\\tau_\\theta = 2.0$：\n$$\n\\mathrm{RMSE}_x \\le \\tau_x, \\quad \\mathrm{RMSE}_\\theta \\le \\tau_\\theta,\n$$\n我们则宣布滤波器在给定的 $Q_\\theta$ 下是稳定的。在测试套件上的稳定性阈值定义为保持稳定的测试值中的最大 $Q_\\theta$。\n\n算法设计细节：\n- Lorenz-96 的右端项通过循环索引进行评估以保持动力学特性。\n- 状态的预报噪声使用 $Q_x I_N$，在数值积分后添加，参数预报使用 $Q_\\theta$ 作为随机游走的方差。\n- 随机 EnKF 使用扰动观测 $\\varepsilon^{(k)} \\sim \\mathcal{N}(0,R)$，每次更新和每个成员都独立抽取。\n- 样本协方差通过距平计算，观测协方差 $P_{yy}$ 由正定的 $R$ 隐式正则化；如有必要，可向对角线添加少量数值抖动以提高数值稳定性。\n- 观测算子 $H$ 是一个选择一半状态分量的线性选择器，确保了一个非平凡的部分观测情景。\n- 程序遍历指定的 $Q_\\theta$ 值，为每种情况生成一致的真实值和观测序列，运行两种滤波器，并汇总稳定性结果。\n\n最后，程序以要求的确切格式打印一行，包含联合滤波器阈值、双重滤波器阈值以及两个布尔值列表，分别标记每个测试用例下的稳定性：\n$$\n[\\text{JT}, \\text{DT}, [s^{(J)}_1,\\dots,s^{(J)}_6], [s^{(D)}_1,\\dots,s^{(D)}_6]].\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lorenz96_rhs(x, F):\n    \"\"\"\n    Compute the Lorenz-96 right-hand side for state x and forcing F.\n    x: array of shape (N,)\n    F: scalar forcing\n    Returns dx/dt: array of shape (N,)\n    \"\"\"\n    N = x.shape[0]\n    dx = np.empty_like(x)\n    # cyclic indices: x_{i+1}, x_{i-2}, x_{i-1}\n    for i in range(N):\n        xp1 = x[(i + 1) % N]\n        xm2 = x[(i - 2) % N]\n        xm1 = x[(i - 1) % N]\n        dx[i] = (xp1 - xm2) * xm1 - x[i] + F\n    return dx\n\ndef rk4_step(x, F, dt):\n    \"\"\"\n    One RK4 step for Lorenz-96.\n    \"\"\"\n    k1 = lorenz96_rhs(x, F)\n    k2 = lorenz96_rhs(x + 0.5 * dt * k1, F)\n    k3 = lorenz96_rhs(x + 0.5 * dt * k2, F)\n    k4 = lorenz96_rhs(x + dt * k3, F)\n    return x + (dt / 6.0) * (k1 + 2*k2 + 2*k3 + k4)\n\ndef build_observation_matrix(N):\n    \"\"\"\n    Build H selecting every other component starting at index 0.\n    Returns H of shape (m, N), where m = N//2 for even N.\n    \"\"\"\n    indices = np.arange(0, N, 2)\n    m = len(indices)\n    H = np.zeros((m, N))\n    for row, idx in enumerate(indices):\n        H[row, idx] = 1.0\n    return H\n\ndef generate_truth(N, T, dt, F0, Q_theta, Q_x, H, R, rng):\n    \"\"\"\n    Generate true trajectory (X_true, Theta_true) and observations Y.\n    \"\"\"\n    X_true = np.zeros((T+1, N))\n    Theta_true = np.zeros(T+1)\n    # Initial true state around F0\n    X_true[0] = F0 + rng.normal(0.0, 1.0, size=N)\n    Theta_true[0] = F0\n\n    for t in range(T):\n        F_t = Theta_true[t]\n        x_next = rk4_step(X_true[t], F_t, dt)\n        # Add model noise\n        x_next = x_next + rng.normal(0.0, np.sqrt(Q_x), size=N)\n        X_true[t+1] = x_next\n        # Parameter random walk\n        Theta_true[t+1] = Theta_true[t] + rng.normal(0.0, np.sqrt(Q_theta))\n\n    # Observations for t=1..T corresponding to X_true[t]\n    m = H.shape[0]\n    Y = np.zeros((T, m))\n    for t in range(1, T+1):\n        y = H @ X_true[t]\n        y = y + rng.normal(0.0, np.sqrt(np.diag(R)), size=m)\n        Y[t-1] = y\n    return X_true, Theta_true, Y\n\ndef enkf_joint(N, T, dt, Q_theta, Q_x, H, R, Y, X_true, Theta_true, x0_guess, theta0_guess, Ne, rng):\n    \"\"\"\n    Joint EnKF on augmented state [x; theta].\n    Returns (rmse_x, rmse_theta, stable_boolean).\n    \"\"\"\n    m = H.shape[0]\n    # Initialize ensemble\n    x_ens = x0_guess + rng.normal(0.0, 1.0, size=(Ne, N))\n    theta_ens = theta0_guess + rng.normal(0.0, 0.5, size=Ne)\n\n    # Storage for ensemble means\n    x_mean = np.zeros((T+1, N))\n    theta_mean = np.zeros(T+1)\n    x_mean[0] = np.mean(x_ens, axis=0)\n    theta_mean[0] = np.mean(theta_ens)\n\n    # Precompute sqrt covariances for obs perturbations\n    obs_std = np.sqrt(np.diag(R))\n\n    stable = True\n    eps_jitter = 1e-9\n\n    for t in range(T):\n        # Forecast step\n        x_f = np.zeros_like(x_ens)\n        theta_f = np.zeros_like(theta_ens)\n        for k in range(Ne):\n            x_f[k] = rk4_step(x_ens[k], theta_ens[k], dt)\n            x_f[k] += rng.normal(0.0, np.sqrt(Q_x), size=N)\n            theta_f[k] = theta_ens[k] + rng.normal(0.0, np.sqrt(Q_theta))\n        # Predicted observations\n        y_pred = (H @ x_f.T).T  # shape (Ne, m)\n\n        # Compute anomalies for augmented state and obs\n        x_f_mean = np.mean(x_f, axis=0)\n        theta_f_mean = np.mean(theta_f)\n        y_mean = np.mean(y_pred, axis=0)\n\n        X_anom = x_f - x_f_mean  # (Ne, N)\n        theta_anom = theta_f - theta_f_mean  # (Ne,)\n        # Build augmented anomalies Z' of shape (Ne, N+1)\n        Z_anom = np.hstack([X_anom, theta_anom[:, None]])\n        Y_anom = y_pred - y_mean  # (Ne, m)\n\n        # Covariances\n        P_zy = (Z_anom.T @ Y_anom) / (Ne - 1)  # (N+1, m)\n        P_yy = (Y_anom.T @ Y_anom) / (Ne - 1) + R + eps_jitter * np.eye(m)  # (m, m)\n\n        # Solve for gain K = P_zy @ inv(P_yy)\n        try:\n            K = np.linalg.solve(P_yy.T, P_zy.T).T  # more stable than inv; solve P_yy^T * X^T = P_zy^T\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        # Analysis with perturbed observations\n        x_a = np.zeros_like(x_f)\n        theta_a = np.zeros_like(theta_f)\n        y_obs = Y[t]  # observation at time t+1\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            update = K @ innov  # (N+1,)\n            x_a[k] = x_f[k] + update[:N]\n            theta_a[k] = theta_f[k] + update[N]\n\n        # Check for numerical issues\n        if not (np.isfinite(x_a).all() and np.isfinite(theta_a).all()):\n            stable = False\n            break\n\n        # Prepare for next step\n        x_ens = x_a\n        theta_ens = theta_a\n\n        # Save means\n        x_mean[t+1] = np.mean(x_ens, axis=0)\n        theta_mean[t+1] = np.mean(theta_ens)\n\n    # Compute RMSEs\n    if not stable:\n        return np.inf, np.inf, False\n\n    # Time-averaged RMSE over t=1..T\n    rmse_x = 0.0\n    rmse_theta = 0.0\n    for t in range(1, T+1):\n        rmse_x += np.linalg.norm(X_true[t] - x_mean[t]) / np.sqrt(N)\n        rmse_theta += abs(Theta_true[t] - theta_mean[t])\n    rmse_x /= T\n    rmse_theta /= T\n\n    # Stability thresholds\n    tau_x = 3.0\n    tau_theta = 2.0\n    stable = np.isfinite(rmse_x) and np.isfinite(rmse_theta) and (rmse_x = tau_x) and (rmse_theta = tau_theta)\n    return rmse_x, rmse_theta, stable\n\ndef enkf_dual(N, T, dt, Q_theta, Q_x, H, R, Y, X_true, Theta_true, x0_guess, theta0_guess, Ne, rng):\n    \"\"\"\n    Dual EnKF: sequential state update then parameter update per time step.\n    Returns (rmse_x, rmse_theta, stable_boolean).\n    \"\"\"\n    m = H.shape[0]\n    # Initialize ensemble\n    x_ens = x0_guess + rng.normal(0.0, 1.0, size=(Ne, N))\n    theta_ens = theta0_guess + rng.normal(0.0, 0.5, size=Ne)\n\n    x_mean = np.zeros((T+1, N))\n    theta_mean = np.zeros(T+1)\n    x_mean[0] = np.mean(x_ens, axis=0)\n    theta_mean[0] = np.mean(theta_ens)\n\n    obs_std = np.sqrt(np.diag(R))\n    stable = True\n    eps_jitter = 1e-9\n\n    for t in range(T):\n        # Forecast step\n        x_f = np.zeros_like(x_ens)\n        theta_f = np.zeros_like(theta_ens)\n        for k in range(Ne):\n            x_f[k] = rk4_step(x_ens[k], theta_ens[k], dt)\n            x_f[k] += rng.normal(0.0, np.sqrt(Q_x), size=N)\n            theta_f[k] = theta_ens[k] + rng.normal(0.0, np.sqrt(Q_theta))\n\n        y_pred = (H @ x_f.T).T  # (Ne, m)\n        y_mean = np.mean(y_pred, axis=0)\n        Y_anom = y_pred - y_mean  # (Ne, m)\n\n        # State update\n        x_f_mean = np.mean(x_f, axis=0)\n        X_anom = x_f - x_f_mean  # (Ne, N)\n        P_xy = (X_anom.T @ Y_anom) / (Ne - 1)  # (N, m)\n        P_yy = (Y_anom.T @ Y_anom) / (Ne - 1) + R + eps_jitter * np.eye(m)\n        try:\n            K_x = np.linalg.solve(P_yy.T, P_xy.T).T  # (N, m)\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        y_obs = Y[t]\n        x_a = np.zeros_like(x_f)\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            x_a[k] = x_f[k] + K_x @ innov\n\n        # Parameter update using forecast obs anomalies (dual strategy)\n        theta_f_mean = np.mean(theta_f)\n        theta_anom = theta_f - theta_f_mean  # (Ne,)\n        # P_{theta y}: (1 x m)\n        P_thetay = (theta_anom[:, None].T @ Y_anom) / (Ne - 1)  # (1, m)\n        try:\n            K_theta = np.linalg.solve(P_yy.T, P_thetay.T).T  # (1, m)\n        except np.linalg.LinAlgError:\n            stable = False\n            break\n\n        theta_a = np.zeros_like(theta_f)\n        for k in range(Ne):\n            eps = rng.normal(0.0, obs_std, size=m)\n            innov = (y_obs + eps) - y_pred[k]\n            theta_a[k] = theta_f[k] + float(K_theta @ innov)\n\n        # Check for numerical issues\n        if not (np.isfinite(x_a).all() and np.isfinite(theta_a).all()):\n            stable = False\n            break\n\n        # Prepare next step\n        x_ens = x_a\n        theta_ens = theta_a\n\n        x_mean[t+1] = np.mean(x_ens, axis=0)\n        theta_mean[t+1] = np.mean(theta_ens)\n\n    if not stable:\n        return np.inf, np.inf, False\n\n    rmse_x = 0.0\n    rmse_theta = 0.0\n    for t in range(1, T+1):\n        rmse_x += np.linalg.norm(X_true[t] - x_mean[t]) / np.sqrt(N)\n        rmse_theta += abs(Theta_true[t] - theta_mean[t])\n    rmse_x /= T\n    rmse_theta /= T\n\n    tau_x = 3.0\n    tau_theta = 2.0\n    stable = np.isfinite(rmse_x) and np.isfinite(rmse_theta) and (rmse_x = tau_x) and (rmse_theta = tau_theta)\n    return rmse_x, rmse_theta, stable\n\ndef solve():\n    rng = np.random.default_rng(42)\n\n    # Fixed settings\n    N = 10\n    dt = 0.05\n    T = 120\n    Ne = 25\n    Q_x = 0.01\n    sigma_obs = 1.0\n    R = (sigma_obs ** 2) * np.eye(N // 2)\n    H = build_observation_matrix(N)\n    F0 = 8.0\n\n    # Test suite of Q_theta values\n    test_cases = [0.0, 0.001, 0.01, 0.05, 0.1, 0.5]\n\n    stable_joint = []\n    stable_dual = []\n\n    # For threshold computation\n    joint_stable_Qs = []\n    dual_stable_Qs = []\n\n    # Run through test cases\n    for Q_theta in test_cases:\n        # Re-seed for reproducibility per case\n        case_rng = np.random.default_rng(1000 + int(Q_theta * 1e6))\n\n        # Generate truth and observations\n        X_true, Theta_true, Y = generate_truth(\n            N=N, T=T, dt=dt, F0=F0, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, rng=case_rng\n        )\n\n        # Initial guesses\n        x0_guess = X_true[0] + rng.normal(0.0, 1.0, size=N)\n        theta0_guess = F0 - 1.0\n\n        # Joint EnKF\n        rmse_x_j, rmse_th_j, st_j = enkf_joint(\n            N=N, T=T, dt=dt, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, Y=Y,\n            X_true=X_true, Theta_true=Theta_true,\n            x0_guess=x0_guess, theta0_guess=theta0_guess, Ne=Ne, rng=np.random.default_rng(2000 + int(Q_theta * 1e6))\n        )\n        stable_joint.append(st_j)\n        if st_j:\n            joint_stable_Qs.append(Q_theta)\n\n        # Dual EnKF\n        rmse_x_d, rmse_th_d, st_d = enkf_dual(\n            N=N, T=T, dt=dt, Q_theta=Q_theta, Q_x=Q_x, H=H, R=R, Y=Y,\n            X_true=X_true, Theta_true=Theta_true,\n            x0_guess=x0_guess, theta0_guess=theta0_guess, Ne=Ne, rng=np.random.default_rng(3000 + int(Q_theta * 1e6))\n        )\n        stable_dual.append(st_d)\n        if st_d:\n            dual_stable_Qs.append(Q_theta)\n\n    # Stability thresholds (maximum Q_theta in test suite with stability)\n    JT = max(joint_stable_Qs) if joint_stable_Qs else 0.0\n    DT = max(dual_stable_Qs) if dual_stable_Qs else 0.0\n\n    # Final print statement in the exact required format.\n    # Convert booleans to 'True'/'False' strings automatically by str().\n    print(f\"[{JT},{DT},[{','.join(map(str, stable_joint))}],[{','.join(map(str, stable_dual))}]]\")\n\nsolve()\n```", "id": "3421568"}, {"introduction": "在解决状态-参数估计问题时，我们常常会遇到数值上的挑战，特别是当模型非线性且变量尺度差异巨大时。最后一个练习 [@problem_id:3421627] 聚焦于优化求解过程，采用高斯-牛顿法来解决非线性最小二乘问题。您将探索如何通过预处理技术（具体为列缩放）来改善联合求解器的数值条件，从而提高收敛速度和稳定性。通过将其与一个更简单的无标度交替优化策略进行对比，本练习突显了在实际反问题中，高级数值方法对于获得可靠解的重要性。", "problem": "考虑一个非线性状态-参数估计问题，其中状态向量 $x \\in \\mathbb{R}^n$ 和参数向量 $\\theta \\in \\mathbb{R}^m$ 必须通过最小化一个由非线性模型残差和观测残差构成的正则化最小二乘目标函数来联合估计。令模型映射按分量定义为 $F_i(x,\\theta) = \\theta_1 \\exp(x_i) + \\theta_2 x_i^2$，其中 $i = 1,\\dots,n$，并定义模型残差 $R_d(x,\\theta) = F(x,\\theta) - c$，其中 $c \\in \\mathbb{R}^n$ 是一个给定向量。令观测残差为 $R_o(x) = H x - y$，其中 $H \\in \\mathbb{R}^{p \\times n}$ 是一个已知的观测矩阵，而 $y \\in \\mathbb{R}^p$ 是一个给定的观测向量。为正常数权重 $\\lambda_d$ 和 $\\lambda_o$ 定义加权残差向量 $r(x,\\theta) = \\begin{bmatrix} \\sqrt{\\lambda_d}\\, R_d(x,\\theta) \\\\ \\sqrt{\\lambda_o}\\, R_o(x) \\end{bmatrix}$。目标是通过近似求解最小化 $\\frac{1}{2} \\| r(x,\\theta) \\|_2^2$ 的非线性最小二乘问题来估计 $(x,\\theta)$。\n\n本问题侧重于比较两种策略：\n- 一种针对增广变量 $(x,\\theta)$ 的联合高斯-牛顿策略，使用列缩放（预处理）矩阵来处理病态单位问题。\n- 一种未缩放的对偶交替策略，该策略在固定 $\\theta$ 更新 $x$ 和固定 $x$ 更新 $\\theta$ 之间交替进行。\n\n从一阶泰勒线性化和高斯-牛顿近似出发，联合策略应在每次迭代中，通过使用 $r(x,\\theta)$ 关于 $(x,\\theta)$ 的雅可比矩阵，求解一个关于 $(x,\\theta)$ 增量的线性最小二乘问题。为处理由异构单位和尺度引起的病态问题，引入对角缩放矩阵 $S_x \\in \\mathbb{R}^{n \\times n}$ 和 $S_\\theta \\in \\mathbb{R}^{m \\times m}$，并在缩放变量中参数化增量，即令 $u = S_x x$ 和 $v = S_\\theta \\theta$，从而使得在线性化步骤中使用的有效雅可比矩阵被 $\\mathrm{diag}(S_x^{-1}, S_\\theta^{-1})$ 进行列缩放。未缩放的对偶交替策略应对 $x$ 和 $\\theta$ 执行独立的高斯-牛顿更新，不进行任何缩放，并交替进行直至收敛。\n\n实现这两种策略，并量化联合策略中的预处理相对于未缩放的对偶交替策略所获得的改进。使用以下基本定义和事实作为推导基础：\n- 高斯-牛顿法使用残差的一阶泰勒近似，并用 $J^\\top J$ 替代精确的海森矩阵，其中 $J$ 是残差的雅可比矩阵。\n- 使用 $S_x$ 和 $S_\\theta$ 进行列缩放等同于在缩放变量中求解线性化系统，然后将增量映射回原始变量。\n- 交替最小化（对偶策略）在保持另一组变量固定的同时更新一组变量，并重复此过程。\n\n您的程序必须：\n- 实现残差 $r(x,\\theta)$ 及其关于增广变量 $(x,\\theta)$ 的雅可比矩阵。\n- 实现一个使用 $S_x$ 和 $S_\\theta$ 进行列缩放的联合高斯-牛顿求解器，包括一个简单的回溯线搜索以确保下降，以及在正规方程中加入一个小的正对角阻尼项以保证数值稳定性。\n- 实现一个未缩放的对偶交替求解器，该求解器交替进行对 $x$ 和 $\\theta$ 的高斯-牛顿更新，每次更新都有其自己的回溯线搜索和用于稳定性的相同阻尼。\n- 使用基于增量相对范数的收敛准则。当 $\\max\\left(\\frac{\\|\\Delta x\\|_2}{\\|x\\|_2 + 1}, \\frac{\\|\\Delta \\theta\\|_2}{\\|\\theta\\|_2 + 1}\\right)  \\varepsilon$ 时停止，其中 $\\varepsilon$ 是一个预设的容差。\n- 每个测试用例使用两个指标来量化改进：每种策略收敛所需的迭代次数，以及在初始迭代点处增广正规方程的谱条件数（在欧几里得范数下）。对于联合策略，报告缩放后增广正规方程的条件数；对于对偶策略，报告未缩放增广正规方程的条件数。条件数必须计算为在初始迭代点处 $J^\\top J$ 的最大奇异值与最小奇异值之比。\n\n使用以下测试套件，它探索了不同的病态情况。在所有情况下，设置 $n = 3$, $m = 2$，并使用相同的观测矩阵 $H$，其条目为\n$H = \\begin{bmatrix} 1  0.1  0 \\\\ 0  1  0.01 \\\\ 0  0.5  1 \\end{bmatrix}$。\n设置权重 $\\lambda_d = 1$ 和 $\\lambda_o = 1$。对于每个情况，使用一个综合真实值 $(x^\\star, \\theta^\\star)$ 来构建 $c$ 和 $y$，使得 $c = F(x^\\star,\\theta^\\star)$ 和 $y = H x^\\star$。\n\n测试用例 1（中度病态）：\n- 真实值：$x^\\star = [1.0,\\; 0.01,\\; 0.0001]$，$\\theta^\\star = [0.01,\\; 100.0]$。\n- 初始猜测：$x_0 = [0.0,\\; 0.1,\\; 0.001]$，$\\theta_0 = [0.02,\\; 10.0]$。\n- 缩放：$S_x = \\mathrm{diag}([1.0,\\; 100.0,\\; 10000.0])$，$S_\\theta = \\mathrm{diag}([100.0,\\; 0.01])$。\n\n测试用例 2（严重病态）：\n- 真实值：$x^\\star = [10.0,\\; 10^{-4},\\; 10^{-8}]$，$\\theta^\\star = [10^{-6},\\; 10^{6}]$。\n- 初始猜测：$x_0 = [1.0,\\; 10^{-3},\\; 10^{-7}]$，$\\theta_0 = [2 \\times 10^{-6},\\; 5 \\times 10^{5}]$。\n- 缩放：$S_x = \\mathrm{diag}([1.0,\\; 10^{4},\\; 10^{8}])$，$S_\\theta = \\mathrm{diag}([10^{6},\\; 10^{-6}])$。\n\n测试用例 3（近线性、良态缩放）：\n- 真实值：$x^\\star = [0.1,\\; 0.2,\\; 0.3]$，$\\theta^\\star = [0.5,\\; 0.1]$。\n- 初始猜测：$x_0 = [0.05,\\; 0.1,\\; 0.15]$，$\\theta_0 = [0.8,\\; 0.05]$。\n- 缩放：$S_x = \\mathrm{diag}([1.0,\\; 1.0,\\; 1.0])$，$S_\\theta = \\mathrm{diag}([1.0,\\; 1.0])$。\n\n数值规格：\n- 使用加到正规方程对角线上的阻尼 $\\alpha = 10^{-8}$。\n- 使用回溯线搜索，几何缩减因子为 $0.5$，直到目标函数减小或步长低于 $10^{-6}$。\n- 两个求解器均使用容差 $\\varepsilon = 10^{-10}$ 和最大迭代次数 $N_{\\max} = 150$。\n\n对于每个测试用例，您的程序必须生成列表 $[N_{\\mathrm{joint}}, N_{\\mathrm{dual}}, \\kappa_{\\mathrm{joint}}, \\kappa_{\\mathrm{dual}}, \\mathrm{improved}]$，其中 $N_{\\mathrm{joint}}$ 和 $N_{\\mathrm{dual}}$ 分别是联合预处理和对偶未缩放策略的迭代次数，$\\kappa_{\\mathrm{joint}}$ 是初始迭代点处缩放后增广正规方程的条件数，$\\kappa_{\\mathrm{dual}}$ 是初始迭代点处未缩放增广正规方程的条件数，$\\mathrm{improved}$ 是一个布尔值，指示联合预处理策略是否在收敛迭代次数更少和初始条件数更低两个方面都优于未缩放的对偶设置。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为由逗号分隔的各用例列表，并用方括号括起来。例如，形式为 $[[\\cdot],[\\cdot],[\\cdot]]$，每个数字以标准十进制形式打印，每个布尔值打印为 $\\mathrm{True}$ 或 $\\mathrm{False}$。", "solution": "用户在数值优化反问题领域提供了一个有效的、适定的问题。任务是为一个非线性状态-参数估计问题实现并比较两种不同的策略：一种是带预处理的联合高斯-牛顿法，另一种是不带预处理的对偶交替策略。\n\n### 1. 数学公式\n\n问题是找到状态向量 $x \\in \\mathbb{R}^n$ 和参数向量 $\\theta \\in \\mathbb{R}^m$，使得正则化的最小二乘目标函数最小化：\n$$ J(x, \\theta) = \\frac{1}{2} \\| r(x, \\theta) \\|_2^2 $$\n其中 $r(x, \\theta)$ 是加权残差向量，定义为：\n$$ r(x,\\theta) = \\begin{bmatrix} \\sqrt{\\lambda_d}\\, R_d(x,\\theta) \\\\ \\sqrt{\\lambda_o}\\, R_o(x) \\end{bmatrix} $$\n残差的组成部分是模型残差 $R_d(x,\\theta) = F(x,\\theta) - c$ 和观测残差 $R_o(x) = H x - y$。模型映射按分量由 $F_i(x,\\theta) = \\theta_1 \\exp(x_i) + \\theta_2 x_i^2$ 给出。权重 $\\lambda_d, \\lambda_o  0$ 平衡了每个残差项的贡献。\n\n高斯-牛顿法通过迭代求解一系列线性最小二乘问题来近似解决这个非线性问题。在每次迭代 $k$，给定当前估计 $(x_k, \\theta_k)$，我们通过线性化残差来找到一个增量 $(\\Delta x_k, \\Delta \\theta_k)$：\n$$ r(x_k + \\Delta x, \\theta_k + \\Delta \\theta) \\approx r(x_k, \\theta_k) + J(x_k, \\theta_k) \\begin{bmatrix} \\Delta x \\\\ \\Delta \\theta \\end{bmatrix} $$\n其中 $J(x_k, \\theta_k)$ 是 $r(x, \\theta)$ 在 $(x_k, \\theta_k)$ 处求值的雅可比矩阵。增量通过求解正规方程得到，该方程为实现正则化和数值稳定性而增加了一个阻尼项 $\\alpha  0$（一种 Levenberg-Marquardt 方法）：\n$$ (J_k^\\top J_k + \\alpha I) \\begin{bmatrix} \\Delta x_k \\\\ \\Delta \\theta_k \\end{bmatrix} = -J_k^\\top r(x_k, \\theta_k) $$\n然后状态被更新为 $x_{k+1} = x_k + \\beta \\Delta x_k$ 和 $\\theta_{k+1} = \\theta_k + \\beta \\Delta \\theta_k$，其中 $\\beta \\in (0, 1]$ 是通过线搜索过程确定的步长，以确保目标函数值的减小。\n\n### 2. 雅可比矩阵计算\n\n雅可比矩阵 $J = \\frac{\\partial r}{\\partial (x, \\theta)}$ 是一个大小为 $(n+p) \\times (n+m)$ 的块矩阵：\n$$ J(x, \\theta) = \\begin{bmatrix} \\frac{\\partial r_d}{\\partial x}  \\frac{\\partial r_d}{\\partial \\theta} \\\\ \\frac{\\partial r_o}{\\partial x}  \\frac{\\partial r_o}{\\partial \\theta} \\end{bmatrix} = \\begin{bmatrix} \\sqrt{\\lambda_d} \\frac{\\partial F}{\\partial x}  \\sqrt{\\lambda_d} \\frac{\\partial F}{\\partial \\theta} \\\\ \\sqrt{\\lambda_o} H  0 \\end{bmatrix} $$\n其组成块是：\n- $\\frac{\\partial F}{\\partial x}$: 一个 $n \\times n$ 的对角矩阵，其条目为 $(\\frac{\\partial F}{\\partial x})_{ii} = \\theta_1 \\exp(x_i) + 2 \\theta_2 x_i$。\n- $\\frac{\\partial F}{\\partial \\theta}$: 一个 $n \\times m$ 的矩阵，其列是 $F$ 关于 $\\theta$ 各分量的导数。对于 $m=2$，其列为 $\\frac{\\partial F}{\\partial \\theta_1} = [\\exp(x_1), \\dots, \\exp(x_n)]^\\top$ 和 $\\frac{\\partial F}{\\partial \\theta_2} = [x_1^2, \\dots, x_n^2]^\\top$。\n- $H$: $p \\times n$ 的观测矩阵。\n- $0$: 一个 $p \\times m$ 的零矩阵。\n\n### 3. 算法策略\n\n**A. 带列缩放的联合高斯-牛顿法**\n\n该策略同时求解 $\\Delta x$ 和 $\\Delta \\theta$。为处理变量不同尺度可能导致的病态问题，我们引入对角缩放矩阵 $S_x$ 和 $S_\\theta$，并求解缩放后的增量 $\\Delta u = S_x \\Delta x$ 和 $\\Delta v = S_\\theta \\Delta \\theta$。正规方程用这些缩放后的变量表示：\n$$ (\\tilde{J}_k^\\top \\tilde{J}_k + \\alpha I) \\begin{bmatrix} \\Delta u_k \\\\ \\Delta v_k \\end{bmatrix} = -\\tilde{J}_k^\\top r(x_k, \\theta_k) $$\n其中缩放后的雅可比矩阵 $\\tilde{J}_k$ 由 $\\tilde{J}_k = J_k S^{-1}$ 给出，其中 $S^{-1} = \\mathrm{diag}(S_x^{-1}, S_\\theta^{-1})$。求解出 $(\\Delta u_k, \\Delta v_k)$ 后，通过 $\\Delta x_k = S_x^{-1} \\Delta u_k$ 和 $\\Delta \\theta_k = S_\\theta^{-1} \\Delta v_k$ 恢复未缩放的增量。这种预处理有效地重新缩放了雅可比矩阵的列，可以显著改善正规方程矩阵 $\\tilde{J}_k^\\top \\tilde{J}_k$ 的条件数。该策略的条件数是针对初始迭代点处的缩放正规矩阵计算的：$\\kappa_{\\text{joint}} = \\kappa((\\tilde{J}_0)^\\top \\tilde{J}_0)$。\n\n**B. 未缩放的对偶交替策略**\n\n该策略是块坐标下降的一种形式，它避免了构建完整的 $(n+m) \\times (n+m)$ 线性系统。相反，它在固定 $\\theta$ 更新 $x$ 和固定 $x$ 更新 $\\theta$ 之间交替进行。\n1.  **x-更新**: 给定 $(x_k, \\theta_k)$，通过最小化 $\\|r(x_k + \\Delta x, \\theta_k)\\|_2^2$ 来求解 $\\Delta x_k$。相应的正规方程为：\n    $$ (J_{x,k}^\\top J_{x,k} + \\alpha I) \\Delta x_k = -J_{x,k}^\\top r(x_k, \\theta_k) $$\n    其中 $J_{x,k} = \\frac{\\partial r}{\\partial x}|_{(x_k, \\theta_k)}$。线搜索产生更新后的状态 $x_{k+1}$。\n2.  **$\\theta$-更新**: 给定 $(x_{k+1}, \\theta_k)$，通过最小化 $\\|r(x_{k+1}, \\theta_k + \\Delta\\theta)\\|_2^2$ 来求解 $\\Delta \\theta_k$。正规方程为：\n    $$ (J_{\\theta,k}^\\top J_{\\theta,k} + \\alpha I) \\Delta \\theta_k = -J_{\\theta,k}^\\top r(x_{k+1}, \\theta_k) $$\n    其中 $J_{\\theta,k} = \\frac{\\partial r}{\\partial \\theta}|_{(x_{k+1}, \\theta_k)}$。线搜索产生更新后的参数 $\\theta_{k+1}$。\n\n对偶策略的单次迭代包含一次 x-更新和一次 $\\theta$-更新。此方法在没有缩放的情况下实现。其条件通过初始迭代点处未缩放的完整增广正规矩阵的条件数来评估：$\\kappa_{\\text{dual}} = \\kappa(J_0^\\top J_0)$。\n\n### 4. 数值实现\n\n两种算法都采用回溯线搜索，将步长 $\\beta$ 以 $0.5$ 的因子缩减，直到目标函数值减小。当实际步长的最大相对范数低于容差 $\\varepsilon = 10^{-10}$ 时，宣告收敛：\n$$ \\max\\left(\\frac{\\|x_{k+1}-x_k\\|_2}{\\|x_k\\|_2 + 1}, \\frac{\\|\\theta_{k+1}-\\theta_k\\|_2}{\\|\\theta_k\\|_2 + 1}\\right)  \\varepsilon $$\n这为两种策略提供了一个鲁棒且一致的终止准则。如果某个算法在 $N_{\\max}=150$ 次迭代内未能收敛，则终止该算法，并报告迭代次数为 $N_{\\max}$。\n\n两种策略之间的比较通过收敛所需的迭代次数（$N_{\\text{joint}}$，$N_{\\mathrm{dual}}$）以及各自正规方程矩阵的初始条件数（$\\kappa_{\\text{joint}}$，$\\kappa_{\\text{dual}}$）来量化。一个布尔标志 `improved` 指示联合预处理策略是否在迭代次数更少和初始条件数更低两个指标上均优于未缩放的对偶设置。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares joint and dual state-parameter estimation strategies.\n    \"\"\"\n    \n    # Define problem constants\n    H_MATRIX = np.array([[1.0, 0.1, 0.0], [0.0, 1.0, 0.01], [0.0, 0.5, 1.0]])\n    LAMBDA_D = 1.0\n    LAMBDA_O = 1.0\n    ALPHA = 1e-8\n    TOL = 1e-10\n    MAX_ITER = 150\n    LINE_SEARCH_MIN_BETA = 1e-6\n    LINE_SEARCH_FACTOR = 0.5\n    N_DIM = 3\n    M_DIM = 2\n    P_DIM = 3\n\n    # Define helper functions inside the solve scope\n    \n    def F_model(x, theta):\n        return theta[0] * np.exp(x) + theta[1] * x**2\n\n    def residual(x, theta, c, y):\n        Rd = F_model(x, theta) - c\n        Ro = H_MATRIX @ x - y\n        return np.concatenate([np.sqrt(LAMBDA_D) * Rd, np.sqrt(LAMBDA_O) * Ro])\n\n    def objective(r):\n        return 0.5 * np.dot(r, r)\n\n    def jacobian_full(x, theta):\n        dF_dx_diag = theta[0] * np.exp(x) + 2 * theta[1] * x\n        dRd_dx = np.sqrt(LAMBDA_D) * np.diag(dF_dx_diag)\n        \n        dF_dtheta1 = np.exp(x)\n        dF_dtheta2 = x**2\n        dRd_dtheta = np.sqrt(LAMBDA_D) * np.vstack([dF_dtheta1, dF_dtheta2]).T\n        \n        dRo_dx = np.sqrt(LAMBDA_O) * H_MATRIX\n        dRo_dtheta = np.zeros((P_DIM, M_DIM))\n\n        J_top = np.hstack([dRd_dx, dRd_dtheta])\n        J_bottom = np.hstack([dRo_dx, dRo_dtheta])\n        return np.vstack([J_top, J_bottom])\n\n    def jacobian_x(x, theta):\n        dF_dx_diag = theta[0] * np.exp(x) + 2 * theta[1] * x\n        dRd_dx = np.sqrt(LAMBDA_D) * np.diag(dF_dx_diag)\n        dRo_dx = np.sqrt(LAMBDA_O) * H_MATRIX\n        return np.vstack([dRd_dx, dRo_dx])\n\n    def jacobian_theta(x, theta):\n        dF_dtheta1 = np.exp(x)\n        dF_dtheta2 = x**2\n        dRd_dtheta = np.sqrt(LAMBDA_D) * np.vstack([dF_dtheta1, dF_dtheta2]).T\n        dRo_dtheta = np.zeros((P_DIM, M_DIM))\n        return np.vstack([dRd_dtheta, dRo_dtheta])\n\n    def joint_gn_solver(x0, theta0, c, y, Sx_diag, S_theta_diag):\n        x, theta = x0.copy(), theta0.copy()\n        S_inv_diag = np.concatenate([1.0/Sx_diag, 1.0/S_theta_diag])\n        S_inv = np.diag(S_inv_diag)\n\n        for i in range(MAX_ITER):\n            x_old, theta_old = x.copy(), theta.copy()\n            r_k = residual(x, theta, c, y)\n            J_k = jacobian_full(x, theta)\n            J_tilde = J_k @ S_inv\n            \n            grad = J_tilde.T @ r_k\n            H_gn = J_tilde.T @ J_tilde + ALPHA * np.eye(N_DIM + M_DIM)\n            \n            try:\n                delta_uv = np.linalg.solve(H_gn, -grad)\n            except np.linalg.LinAlgError:\n                return x, theta, MAX_ITER\n\n            delta_x = S_inv[:N_DIM, :N_DIM] @ delta_uv[:N_DIM]\n            delta_theta = S_inv[N_DIM:, N_DIM:] @ delta_uv[N_DIM:]\n            \n            beta = 1.0\n            obj_old_val = objective(r_k)\n            \n            while beta > LINE_SEARCH_MIN_BETA:\n                x_new = x_old + beta * delta_x\n                theta_new = theta_old + beta * delta_theta\n                if objective(residual(x_new, theta_new, c, y))  obj_old_val:\n                    x, theta = x_new, theta_new\n                    break\n                beta *= LINE_SEARCH_FACTOR\n            else: # Line search failed\n                return x_old, theta_old, MAX_ITER\n\n            step_x = x - x_old\n            step_theta = theta - theta_old\n            rel_norm_x = np.linalg.norm(step_x) / (np.linalg.norm(x_old) + 1.0)\n            rel_norm_theta = np.linalg.norm(step_theta) / (np.linalg.norm(theta_old) + 1.0)\n            \n            if max(rel_norm_x, rel_norm_theta)  TOL:\n                return x, theta, i + 1\n                \n        return x, theta, MAX_ITER\n\n    def dual_alternation_solver(x0, theta0, c, y):\n        x, theta = x0.copy(), theta0.copy()\n        \n        for i in range(MAX_ITER):\n            x_old, theta_old = x.copy(), theta.copy()\n\n            # --- x-update ---\n            r_x_k = residual(x, theta, c, y)\n            J_x = jacobian_x(x, theta)\n            grad_x = J_x.T @ r_x_k\n            H_gn_x = J_x.T @ J_x + ALPHA * np.eye(N_DIM)\n            try:\n                delta_x = np.linalg.solve(H_gn_x, -grad_x)\n            except np.linalg.LinAlgError:\n                return x, theta, MAX_ITER\n\n            beta_x = 1.0\n            obj_old_x = objective(r_x_k)\n            while beta_x > LINE_SEARCH_MIN_BETA:\n                x_new = x + beta_x * delta_x\n                if objective(residual(x_new, theta, c, y))  obj_old_x:\n                    x = x_new\n                    break\n                beta_x *= LINE_SEARCH_FACTOR\n            else: # Line search for x failed\n                return x_old, theta_old, MAX_ITER\n            \n            # --- theta-update ---\n            r_theta_k = residual(x, theta, c, y)\n            J_theta = jacobian_theta(x, theta)\n            grad_theta = J_theta.T @ r_theta_k\n            H_gn_theta = J_theta.T @ J_theta + ALPHA * np.eye(M_DIM)\n            try:\n                delta_theta = np.linalg.solve(H_gn_theta, -grad_theta)\n            except np.linalg.LinAlgError:\n                return x, theta, MAX_ITER\n            \n            beta_theta = 1.0\n            obj_old_theta = objective(r_theta_k)\n            while beta_theta > LINE_SEARCH_MIN_BETA:\n                theta_new = theta + beta_theta * delta_theta\n                if objective(residual(x, theta_new, c, y))  obj_old_theta:\n                    theta = theta_new\n                    break\n                beta_theta *= LINE_SEARCH_FACTOR\n            else: # Line search for theta failed\n                 return x, theta_old, MAX_ITER\n\n            step_x = x - x_old\n            step_theta = theta - theta_old\n            rel_norm_x = np.linalg.norm(step_x) / (np.linalg.norm(x_old) + 1.0)\n            rel_norm_theta = np.linalg.norm(step_theta) / (np.linalg.norm(theta_old) + 1.0)\n            \n            if max(rel_norm_x, rel_norm_theta)  TOL:\n                return x, theta, i + 1\n                \n        return x, theta, MAX_ITER\n\n    test_cases = [\n        {'x_star': np.array([1.0, 0.01, 0.0001]), 'theta_star': np.array([0.01, 100.0]),\n         'x0': np.array([0.0, 0.1, 0.001]), 'theta0': np.array([0.02, 10.0]),\n         'Sx_diag': np.array([1.0, 100.0, 10000.0]), 'S_theta_diag': np.array([100.0, 0.01])},\n        {'x_star': np.array([10.0, 1e-4, 1e-8]), 'theta_star': np.array([1e-6, 1e6]),\n         'x0': np.array([1.0, 1e-3, 1e-7]), 'theta0': np.array([2e-6, 5e5]),\n         'Sx_diag': np.array([1.0, 1e4, 1e8]), 'S_theta_diag': np.array([1e6, 1e-6])},\n        {'x_star': np.array([0.1, 0.2, 0.3]), 'theta_star': np.array([0.5, 0.1]),\n         'x0': np.array([0.05, 0.1, 0.15]), 'theta0': np.array([0.8, 0.05]),\n         'Sx_diag': np.array([1.0, 1.0, 1.0]), 'S_theta_diag': np.array([1.0, 1.0])}\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        x_star, theta_star = case['x_star'], case['theta_star']\n        x0, theta0 = case['x0'], case['theta0']\n        Sx_diag, S_theta_diag = case['Sx_diag'], case['S_theta_diag']\n        \n        c = F_model(x_star, theta_star)\n        y = H_MATRIX @ x_star\n        \n        J0 = jacobian_full(x0, theta0)\n        \n        # Condition number for dual (unscaled)\n        N_dual_unscaled = J0.T @ J0\n        kappa_dual = np.linalg.cond(N_dual_unscaled)\n        \n        # Condition number for joint (scaled)\n        S_inv = np.diag(np.concatenate([1.0/Sx_diag, 1.0/S_theta_diag]))\n        J0_tilde = J0 @ S_inv\n        N_joint_scaled = J0_tilde.T @ J0_tilde\n        kappa_joint = np.linalg.cond(N_joint_scaled)\n        \n        _, _, N_joint = joint_gn_solver(x0, theta0, c, y, Sx_diag, S_theta_diag)\n        _, _, N_dual = dual_alternation_solver(x0, theta0, c, y)\n\n        improved = (N_joint  N_dual) and (kappa_joint  kappa_dual)\n        \n        all_results.append([N_joint, N_dual, kappa_joint, kappa_dual, improved])\n\n    case_strs = []\n    for res in all_results:\n        item_strs = map(str, res)\n        case_strs.append(f\"[{','.join(item_strs)}]\")\n    final_output = f\"[{','.join(case_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3421627"}]}