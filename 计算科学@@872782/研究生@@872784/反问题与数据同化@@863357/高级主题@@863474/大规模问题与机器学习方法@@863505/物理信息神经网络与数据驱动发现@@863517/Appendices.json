{"hands_on_practices": [{"introduction": "在信任任何复杂的数值求解器（包括物理信息神经网络）之前，我们必须验证其实现的正确性。本练习将引导您使用“可控解方法”（method of manufactured solutions），这是一种标准的验证技术，通过构建一个具有已知精确解的基准问题，让您能够严格测试物理损失函数及其所需导数的计算是否准确无误([@problem_id:3410533])。这对于确保您的 PINN 模型从根本上正确地编码了物理定律至关重要。", "problem": "考虑在单位正方形域 $\\Omega = (0,1)\\times(0,1)$ 上的泊松方程，其在边界 $\\partial\\Omega$ 上具有齐次狄利克雷边界条件。其控制方程为在 $\\Omega$ 内的泊松方程 $-\\Delta u = f$，其中拉普拉斯算子定义为 $\\Delta u = u_{xx} + u_{yy}$，狄利克雷边界条件为在 $\\partial\\Omega$ 上 $u = 0$。在物理信息神经网络（PINNs）中，物理损失项通常涉及逐点残差 $r_{\\mathrm{PDE}}(x,y) = -\\Delta u(x,y) - f(x,y)$，而边界损失项则在 $\\partial\\Omega$ 上强制 $r_{\\mathrm{BC}}(x,y) = u(x,y)$。为验证自动微分所需的梯度包括 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$，以及物理残差的输入梯度 $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y), \\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y)\\right)$。\n\n给定解析解 $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$，要求设计一个规范测试来验证 PINN 实现。从拉普拉斯算子和狄利克雷边界条件的基本定义出发，推导源项 $f(x,y)$，使得 $u$ 在 $\\Omega$ 内满足 $-\\Delta u = f$，并推导精确残差 $r_{\\mathrm{PDE}}(x,y)$ 和 $r_{\\mathrm{BC}}(x,y)$，以及由此选择所隐含的精确一阶和二阶导数 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$ 和梯度 $\\nabla r_{\\mathrm{PDE}}(x,y)$。\n\n对于给定的 $u$，哪个选项正确地指定了 $f(x,y)$、$r_{\\mathrm{PDE}}(x,y)$、$r_{\\mathrm{BC}}(x,y)$、$u_x(x,y)$、$u_y(x,y)$、$u_{xx}(x,y)$、$u_{yy}(x,y)$ 和 $\\nabla r_{\\mathrm{PDE}}(x,y)$？\n\nA. $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 0$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\cos(\\pi x)\\sin(\\pi y)$, $u_y(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$.\n\nB. $f(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 4\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\sin(\\pi x)\\sin(\\pi y)$, $u_y(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = \\pi^2 \\cos(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = \\pi^2 \\sin(\\pi x)\\cos(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(4\\pi^3 \\cos(\\pi x)\\sin(\\pi y), 4\\pi^3 \\sin(\\pi x)\\cos(\\pi y)\\right)$.\n\nC. $f(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{BC}}(x,y) = 0$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\cos(\\pi x)\\sin(\\pi y)$, $u_y(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\pi^3 \\cos(\\pi x)\\sin(\\pi y), \\pi^3 \\sin(\\pi x)\\cos(\\pi y)\\right)$.\n\nD. $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $r_{\\mathrm{PDE}}(x,y) = 0$, $r_{\\mathrm{BC}}(x,y) = \\partial_n u(x,y)$ on $\\partial\\Omega$, $u_x(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$, $u_y(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$, $u_{xx}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, $u_{yy}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$, and $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$.", "solution": "我们从基本定义开始。拉普拉斯算子定义为 $\\Delta u = u_{xx} + u_{yy}$，泊松方程为在 $\\Omega = (0,1)\\times(0,1)$ 内的 $-\\Delta u = f$，其在边界 $\\partial\\Omega$ 上具有齐次狄利克雷边界条件 $u = 0$。对于给定的解析解 $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$，我们使用链式法则和标准的正弦、余弦函数微分法则来计算所需的导数。\n\n首先，计算一阶导数：\n$u_x(x,y) = \\frac{\\partial}{\\partial x}\\left[\\sin(\\pi x)\\sin(\\pi y)\\right] = \\pi \\cos(\\pi x)\\sin(\\pi y)$,\n$u_y(x,y) = \\frac{\\partial}{\\partial y}\\left[\\sin(\\pi x)\\sin(\\pi y)\\right] = \\pi \\sin(\\pi x)\\cos(\\pi y)$.\n\n其次，计算二阶导数：\n$u_{xx}(x,y) = \\frac{\\partial}{\\partial x}\\left[\\pi \\cos(\\pi x)\\sin(\\pi y)\\right] = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$,\n$u_{yy}(x,y) = \\frac{\\partial}{\\partial y}\\left[\\pi \\sin(\\pi x)\\cos(\\pi y)\\right] = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n\n因此，拉普拉斯算子为\n$\\Delta u(x,y) = u_{xx}(x,y) + u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - \\pi^2 \\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n\n根据控制方程 $-\\Delta u = f$，我们得到\n$f(x,y) = -\\Delta u(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$.\n\n定义物理残差 $r_{\\mathrm{PDE}}(x,y) = -\\Delta u(x,y) - f(x,y)$。代入上述表达式，\n$r_{\\mathrm{PDE}}(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) = 0$，\n因此，当使用精确解 $u$ 和与之对应的 $f$ 时，残差恒等于零。\n\n对于齐次狄利克雷条件，边界残差为在 $\\partial\\Omega$ 上的 $r_{\\mathrm{BC}}(x,y) = u(x,y)$。因为当 $x=0$ 或 $x=1$ 时 $\\sin(\\pi x) = 0$，且当 $y=0$ 或 $y=1$ 时 $\\sin(\\pi y) = 0$，所以我们有在 $\\partial\\Omega$ 上 $u=0$，因此在 $\\partial\\Omega$ 上 $r_{\\mathrm{BC}}(x,y) = 0$。\n\n最后，物理残差相对于输入的梯度为\n$\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y), \\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y)\\right)$.\n由于 $r_{\\mathrm{PDE}}(x,y) \\equiv 0$，我们有 $\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y) = 0$ 和 $\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y) = 0$，因此 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$。\n\n我们现在评估每个选项：\n\n选项A：它陈述了 $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{PDE}}(x,y) = 0$，在 $\\partial\\Omega$ 上 $r_{\\mathrm{BC}}(x,y) = 0$，并且列出的 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$ 与上面计算的完全一致。它也正确地指出了 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$，因为残差恒等于零。判定 — 正确。\n\n选项B：它设定 $f(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，这与 $f = -\\Delta u = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$ 相矛盾。使用这个错误的 $f$，它接着给出 $r_{\\mathrm{PDE}}(x,y) = 4\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，这是由符号错误产生的非零残差。此外，它错误地列出了 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$：例如，$u_x$ 被给出为 $\\pi \\sin(\\pi x)\\sin(\\pi y)$，但正确的 $u_x$ 是 $\\pi \\cos(\\pi x)\\sin(\\pi y)$；类似地，$u_{xx}$ 不是 $\\pi^2 \\cos(\\pi x)\\sin(\\pi y)$ 而是 $-\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。所陈述的 $\\nabla r_{\\mathrm{PDE}}$ 也与正确的残差不一致。判定 — 错误。\n\n选项C：它设定 $f(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，缺少了一个因子 $2$。因此，$r_{\\mathrm{PDE}}(x,y)$ 被给出为 $\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，这是非零的，并与精确残差相矛盾。虽然该选项列出了正确的 $u_x$、$u_y$、$u_{xx}$ 和 $u_{yy}$，但它从其错误的残差中错误地计算了 $\\nabla r_{\\mathrm{PDE}}$。精确残差的正确梯度是零向量。判定 — 错误。\n\n选项D：它给出了正确的 $f(x,y)$ 并正确设置了 $r_{\\mathrm{PDE}}(x,y) = 0$，但它将边界残差定义为在 $\\partial\\Omega$ 上的 $r_{\\mathrm{BC}}(x,y) = \\partial_n u(x,y)$，这不是狄利克雷边界残差；对于齐次狄利克雷条件，$r_{\\mathrm{BC}}(x,y)$ 应该是 $u(x,y)$，而不是法向导数。此外，它错误地将 $u_x$ 和 $u_y$ 都列为 $\\pi \\cos(\\pi x)\\cos(\\pi y)$，这与正确的一阶导数不相等，并且它错误地将 $u_{xx}$ 和 $u_{yy}$ 的符号定为正的 $\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$ 而不是负的。尽管它给出了 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$，但其他不一致之处使该选项无效。判定 — 错误。\n\n因此，正确选项是 A。", "answer": "$$\\boxed{A}$$", "id": "3410533"}, {"introduction": "一个可靠的科学模型不仅应提供预测，还应告知我们其预测的可信度。本练习将带领您超越简单的点估计，探索物理信息神经网络中的不确定性量化（UQ）。您将学习如何应用蒙特卡洛 Dropout 这一实用的贝叶斯近似技术，使您的 PINN 能够评估自身预测的不确定性，并学习如何通过“经验覆盖率”来校准和验证这些不确定性估计的质量([@problem_id:3410639])。", "problem": "实现一个完整的程序，该程序构建一个带蒙特卡洛 dropout 的物理信息神经网络 (PINN) 用于求解一维热方程，并通过在留出数据上的经验覆盖率来校准预测不确定性。控制偏微分方程是热方程 $u_t = \\alpha u_{xx}$，定义在单位域 $(x,t) \\in [0,1] \\times [0,1]$ 上，其狄利克雷边界条件为 $u(0,t) = 0$，$u(1,t) = 0$，初始条件为 $u(x,0) = \\sin(\\pi x)$。此处 $u(x,t)$ 是温度场，$\\alpha > 0$ 是热扩散系数。您必须从第一性原理出发实现以下组件：一个具有双曲正切激活函数的单隐藏层全连接神经网络 $u_\\theta(x,t)$；一个使用热方程残差以及边界和初始条件的物理信息损失；以及在推理时使用蒙特卡洛 dropout 来获得预测不确定性。仅使用一致有效的解析解 $u^\\star(x,t) = \\exp(-\\alpha \\pi^2 t)\\sin(\\pi x)$ 来生成用于覆盖率评估的留出测试目标。所有量都是无量纲的，因此不需要物理单位转换。\n\n您的设计应基于以下原则和定义：\n- 热方程 $u_t = \\alpha u_{xx}$ 编码了热量守恒和傅里叶扩散定律；物理信息方法在配置点上对该定律的均方残差进行惩罚。\n- 物理信息神经网络 (PINN) 是一个神经网络 $u_\\theta(x,t)$，它通过最小化一个聚合了数据失配和物理残差的损失函数进行训练。对于这个问题，使用损失\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}} \\, \\mathbb{E}_{(x,t)\\in\\mathcal{C}} \\left[ \\left( \\partial_t u_\\theta(x,t) - \\alpha \\, \\partial_{xx} u_\\theta(x,t) \\right)^2 \\right] + \\lambda_{\\mathrm{ic}} \\, \\mathbb{E}_{x\\in\\mathcal{I}} \\left[ \\left( u_\\theta(x,0) - \\sin(\\pi x) \\right)^2 \\right] + \\lambda_{\\mathrm{bc}} \\, \\left( \\mathbb{E}_{t\\in\\mathcal{B}} \\left[ u_\\theta(0,t)^2 \\right] + \\mathbb{E}_{t\\in\\mathcal{B}} \\left[ u_\\theta(1,t)^2 \\right] \\right),\n$$\n其中 $\\lambda_{\\mathrm{pde}}, \\lambda_{\\mathrm{ic}}, \\lambda_{\\mathrm{bc}}$ 为非负权重，$\\mathcal{C}, \\mathcal{I}, \\mathcal{B}$ 分别为配置点、初始点和边界采样点的有限集合。\n- 蒙特卡洛 dropout 在推理时使用随机 dropout 掩码来导出预测分布。对于一个具有单隐藏层和应用于隐藏层激活值 $a = \\tanh(z)$ 的逐元素 dropout 掩码 $m$ 的网络，采用反向缩放，一次前向传播返回 $u_\\theta(x,t)$；重复此过程 $M$ 次可得到样本，用于计算每个留出点的预测均值和标准差。\n\n需要实现的神经网络模型和导数：\n- 设网络为 $u_\\theta(x,t) = \\mathbf{w}_2^\\top \\left( m \\odot \\tanh(\\mathbf{W}_1 [x,t]^\\top + \\mathbf{b}_1) \\right) + b_2$，其中 $\\mathbf{W}_1 \\in \\mathbb{R}^{H \\times 2}$，$\\mathbf{b}_1 \\in \\mathbb{R}^H$，$\\mathbf{w}_2 \\in \\mathbb{R}^H$，$b_2 \\in \\mathbb{R}$ 且 $H$ 是隐藏层宽度，而 $m \\in \\mathbb{R}^H$ 是一个随机掩码，其条目对于 dropout 概率 $p \\in [0,1)$ 为 $m_i \\in \\{0, 1/(1-p)\\}$。在蒙特卡洛推理期间，为每个输入样本抽取一个新的 $m$。\n- 物理残差所需的导数必须通过链式法则进行解析计算。记 $s = \\mathbf{W}_1 [x,t]^\\top + \\mathbf{b}_1$，$a = \\tanh(s)$，$\\operatorname{sech}^2(s) = 1 - \\tanh^2(s)$，并将 $\\mathbf{W}_1$ 的列分别记为 $\\mathbf{W}_{1x}$ 和 $\\mathbf{W}_{1t}$，对应 $x$ 和 $t$。那么对于给定的 dropout 掩码 $m$ 和单隐藏层，实现\n$$\n\\partial_x u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\operatorname{sech}^2(s) \\odot \\mathbf{W}_{1x} \\right), \\quad\n\\partial_t u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\operatorname{sech}^2(s) \\odot \\mathbf{W}_{1t} \\right),\n$$\n$$\n\\partial_{xx} u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\left( -2 \\, \\operatorname{sech}^2(s) \\odot \\tanh(s) \\right) \\odot \\mathbf{W}_{1x} \\odot \\mathbf{W}_{1x} \\right).\n$$\n\n训练和评估协议：\n- 通过最小化 $\\mathcal{L}(\\theta)$ 来训练 PINN，在训练期间禁用 dropout 以确保目标函数是确定性的。使用小型网络和适度的样本量，以使优化在合理时间内收敛。\n- 在留出的测试点网格 $(x,t)$ 上，使用解析解 $u^\\star(x,t)$ 作为目标，评估标称高斯置信区间的经验覆盖率。对于标称水平 $q \\in \\{0.68, 0.95\\}$，使用 z-分数 $z_{0.68} = 1.0$ 和 $z_{0.95} = 1.96$。对于每个测试点，计算蒙特卡洛预测均值 $\\mu$ 和标准差 $\\sigma$，并检查是否 $|u^\\star - \\mu| \\le z \\sigma$。经验覆盖率是满足此不等式的留出点的比例。如果 $\\sigma = 0$，则将区间视为退化区间；在实现中使用一个小的正 $\\varepsilon$ 抖动以避免数值问题。\n\n测试套件和要求的输出：\n- 使用以下三个测试用例。在所有情况下，时空域为 $[0,1]\\times[0,1]$，隐藏层宽度为 $H = 10$，损失权重为 $\\lambda_{\\mathrm{pde}} = 1$，$\\lambda_{\\mathrm{ic}} = 100$，$\\lambda_{\\mathrm{bc}} = 100$。留出测试网格应为 x 轴上 $N_x = 20$ 个点和 t 轴上 $N_t = 20$ 个点（总共 $400$ 个点）的均匀网格。对于蒙特卡洛方法，每次前向传播中每个样本使用独立的掩码，并采用反向 dropout 缩放。三个用例如下：\n    - 用例 A (happy path): $\\alpha = 0.1$，dropout 概率 $p = 0.1$，蒙特卡洛样本数 $M = 100$。使用 $N_{\\mathcal{C}} = 64$ 个配置点、$N_{\\mathcal{I}} = 32$ 个初始点、$N_{\\mathcal{B}} = 32$ 个边界点进行训练，并优化 $80$ 次迭代。\n    - 用例 B (dropout 敏感性): 重用用例 A 训练好的权重，不进行进一步优化。使用 dropout 概率 $p = 0.3$，蒙特卡洛样本数 $M = 100$ 进行评估。\n    - 用例 C (变化的扩散系数和有限数据): $\\alpha = 0.2$，dropout 概率 $p = 0.2$，蒙特卡洛样本数 $M = 100$。使用 $N_{\\mathcal{C}} = 32$ 个配置点、$N_{\\mathcal{I}} = 16$ 个初始点、$N_{\\mathcal{B}} = 16$ 个边界点进行训练，并优化 $60$ 次迭代。\n\n您的程序必须：\n- 实现带有解析导数和物理信息损失的 PINN，并进行确定性训练。\n- 对于每个测试用例，计算两个浮点数：$q=0.68$ 和 $q=0.95$ 的经验覆盖率，按此顺序。\n- 生成一行输出，包含六个覆盖率结果，格式为方括号内的逗号分隔列表，顺序为 [A-0.68, A-0.95, B-0.68, B-0.95, C-0.68, C-0.95]。\n\n不允许用户输入；所有常量和随机种子应在程序内部固定以保证可复现性。只能使用执行环境中指定的 Python 标准库、NumPy 和 SciPy。在代码中清晰地记录 dropout 的处理、通过链式法则计算导数以及覆盖率指标。本问题陈述中出现的每个数学符号、函数、运算符和数字都以 LaTeX 书写，以确保清晰和精确。", "solution": "用户提供的问题是科学机器学习领域中一个定义明确且内容充实的任务。它要求实现一个物理信息神经网络 (PINN) 来求解一维热方程，其中包括一个使用蒙特卡洛 (MC) dropout 进行不确定性量化的机制。问题陈述在科学上是合理的、内部一致且完整的。所有必要的方程、参数和评估协议都已提供，使得该问题完全可形式化且计算上可解。\n\n根据对指定标准的严格检查，该问题是有效的：\n- **科学基础：** 问题的基础是热方程（物理学的基石）和 PINNs（计算科学中广泛认可的方法论）。所提供的解析解和导数公式是正确的。\n- **适定性：** 问题提供了明确的目标（最小化一个已定义的损失函数）和精确的评估程序（经验覆盖率）。三个测试用例的所有必要数据和约束都已指定。\n- **客观性：** 问题使用精确的数学符号和无歧义的指令进行描述，没有任何主观因素。\n- **完整性与一致性：** 虽然没有明确指定优化器的选择，但允许使用 `scipy.optimize`，像 L-BFGS-B 这样的标准选择适用于这类问题。迭代次数作为一个明确的停止准则。问题是自包含的，没有矛盾。对“迭代次数”的规定被解释为所选 `scipy` 优化器中的 `maxiter` 选项。\n- **可行性：** 网络规模（$H=10$）、数据集大小和优化迭代次数被有意设计得很小，以确保計算可以在标准硬件上于合理的时间内完成，正如一个自包含脚本所预期的那样。\n\n因此，该问题被认定为**有效**，并将构建一个完整的解决方案。\n\n### 算法设计与原则\n\n该解决方案围绕一个核心 Python 类 `PINN` 构建，该类封装了神经网络模型及其相关的物理信息逻辑。\n\n1.  **神经网络架构：**\n    PINN 的核心是一个单隐藏层、全连接的神经网络 $u_\\theta(x,t)$，它带有一个双曲正切 ($\\tanh$) 激活函数。参数 $\\theta = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ 使用标准方案（Glorot 初始化）进行初始化，以促进稳定的训练。为了与 `scipy.optimize.minimize`（它对单个变量向量进行操作）兼容，实现了辅助方法来将网络的权重矩阵和偏置向量“打包”到一个扁平的 1D NumPy 数组中，然后再“解包”回来。\n\n2.  **物理信息损失函数：**\n    训练目标是最小化复合损失函数 $\\mathcal{L}(\\theta)$，它是三个均方误差项的加权和：\n    - **PDE 残差损失 ($\\mathcal{L}_{\\mathrm{pde}}$):** 此项强制执行控制物理 $u_t - \\alpha u_{xx} = 0$。它在一组从时空域 $[0,1] \\times [0,1]$ 内部采样的配置点 $\\mathcal{C}$ 上进行评估。\n    - **初始条件损失 ($\\mathcal{L}_{\\mathrm{ic}}$):** 此项惩罚与已知初始状态 $u(x,0) = \\sin(\\pi x)$ 的偏差，作用于时间 $t=0$ 的一组点 $\\mathcal{I}$ 上。\n    - **边界条件损失 ($\\mathcalL_{\\mathrm{bc}}$):** 此项强制执行狄利克雷边界条件 $u(0,t)=0$ 和 $u(1,t)=0$，作用于沿空间边界 $x=0$ 和 $x=1$ 采样的点上。\n\n    PDE 残差所需的导数 $\\partial_t u_\\theta$ 和 $\\partial_{xx} u_\\theta$ 按照问题陈述中的规定，使用链式法则进行解析计算。这些计算在一个专门的方法中实现，确保了正确性和效率。在训练期间计算损失时禁用 dropout，以按要求为优化器提供一个确定性的目标函数。\n\n3.  **优化：**\n    使用 L-BFGS-B 算法进行优化，这是一种在 `scipy.optimize.minimize` 中可用的拟牛顿法。它非常适合此问题中的少量参数（$4H+1 = 41$），并且对于 PINNs 来说，通常比一阶方法表现出更好的收敛特性。优化器迭代地调整扁平化的参数向量以最小化损失函数 $\\mathcal{L}(\\theta)$。迭代次数由 `maxiter` 选项控制， sesuai dengan yang ditentukan untuk setiap kasus uji。\n\n4.  **通过蒙特卡洛 dropout 进行不确定性量化：**\n    训练后，使用 MC dropout 评估模型的预测不确定性。在推理时，启用 dropout。对于留出测试网格上的每个点，执行 $M$ 次随机前向传播。在每次传播中，将一个不同的随机 dropout 掩码 $m$ 应用于隐藏层的激活值。这个过程为每个测试点生成一个包含 $M$ 个预测的集成。\n    - **预测均值** ($\\mu$) 是 $M$ 个预测的平均值。\n    - **预测标准差** ($\\sigma$) 量化了模型的不确定性，由集成中的变异性表示。\n\n5.  **经验覆盖率评估：**\n    不确定性估计的质量通过计算经验覆盖率来校准。对于测试网格上的每个点，我们构建一个标称置信区间 $[\\mu - z\\sigma, \\mu + z\\sigma]$，其中 $z$ 是对应于所需置信水平 $q$（例如，$q=0.68$ 时 $z=1.0$）的标准正态分布的临界值。然后我们检查真实的解析解 $u^\\star(x,t)$ 是否落在此区间内。经验覆盖率是满足此条件的测试点的比例。该指标评估模型报告的不确定性是否良好校准；例如，一个 95% 的标称区间理想情况下应在 95% 的时间内包含真值。在 $\\sigma$ 上添加一个小的抖动 $\\varepsilon$，以防止在预测方差为零的情况下出现除以零的错误。\n\n该实现遵循三个指定的测试用例，在热扩散系数 ($\\alpha$)、dropout 概率 ($p$) 和训练数据大小等不同条件下训练和/或评估模型，并以要求的精确格式报告所得的覆盖率指标。固定的随机种子确保了所有随机操作（包括权重初始化、数据采样和 dropout 掩码）的可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements a PINN for the 1D heat equation, trains it, and evaluates\n    predictive uncertainty calibration using Monte Carlo dropout.\n    \"\"\"\n    # Fix random seed for reproducibility of weight initialization, data sampling, and dropout masks.\n    np.random.seed(42)\n\n    class PINN:\n        \"\"\"\n        Physics-Informed Neural Network for the 1D Heat Equation.\n        Implements the network architecture, analytical derivatives, loss function,\n        and prediction with Monte Carlo dropout.\n        \"\"\"\n        def __init__(self, H=10):\n            self.H = H\n            # Glorot initialization for weights\n            self.W1 = np.random.randn(self.H, 2) * np.sqrt(2 / (2 + self.H))\n            self.b1 = np.zeros((1, self.H))\n            self.w2 = np.random.randn(self.H, 1) * np.sqrt(2 / (self.H + 1))\n            self.b2 = np.zeros((1, 1))\n\n        def pack_params(self):\n            \"\"\"Flattens all model parameters into a single 1D array for the optimizer.\"\"\"\n            return np.concatenate([self.W1.flatten(), self.b1.flatten(), self.w2.flatten(), self.b2.flatten()])\n\n        def unpack_params(self, params_vec):\n            \"\"\"Unpacks a 1D array of parameters back into the model's weight/bias attributes.\"\"\"\n            ptr = 0\n            self.W1 = params_vec[ptr:ptr + self.H * 2].reshape(self.H, 2)\n            ptr += self.H * 2\n            self.b1 = params_vec[ptr:ptr + self.H].reshape(1, self.H)\n            ptr += self.H\n            self.w2 = params_vec[ptr:ptr + self.H].reshape(self.H, 1)\n            ptr += self.H\n            self.b2 = params_vec[ptr:ptr + 1].reshape(1, 1)\n\n        def forward(self, x, t, p_drop=0.0):\n            \"\"\"\n            Performs a forward pass through the network.\n            Handles training (p_drop=0) and inference with MC dropout (p_drop>0).\n            \"\"\"\n            X = np.hstack((x, t))\n            N = X.shape[0]\n            \n            s = X @ self.W1.T + self.b1\n            a = np.tanh(s)\n            \n            # Apply dropout if in inference mode (p_drop > 0)\n            if p_drop > 0:\n                # Inverted dropout: scale by 1/(1-p) during training is standard,\n                # but here we apply it at inference time as per the problem.\n                # A fresh mask is drawn for each sample in the batch.\n                scale = 1.0 / (1.0 - p_drop)\n                mask = np.random.binomial(1, 1.0 - p_drop, size=(N, self.H)) * scale\n                a = a * mask\n            \n            # If p_drop is 0 (training), the mask is effectively all ones.\n            u = a @ self.w2 + self.b2\n            return u.flatten()\n\n        def get_derivatives(self, x, t):\n            \"\"\"\n            Computes the network output and its analytical derivatives w.r.t. x and t.\n            Dropout is disabled (mask 'm' is effectively all ones).\n            \"\"\"\n            X = np.hstack((x, t))\n            s = X @ self.W1.T + self.b1\n            a = np.tanh(s)\n            sech2_s = 1.0 - a**2\n\n            # Extract columns of W1 corresponding to x and t\n            W1x = self.W1[:, 0:1].T  # Shape (1, H)\n            W1t = self.W1[:, 1:2].T  # Shape (1, H)\n\n            # First derivatives (chain rule)\n            u_t = (sech2_s * W1t) @ self.w2\n            u_x = (sech2_s * W1x) @ self.w2\n\n            # Second derivative (chain rule)\n            term_xx = -2.0 * sech2_s * a\n            u_xx = (term_xx * W1x * W1x) @ self.w2\n\n            return u_t.flatten(), u_x.flatten(), u_xx.flatten()\n\n        def loss_function(self, params_vec, alpha, points, lambdas):\n            \"\"\"\n            The objective function for the optimizer.\n            Calculates the weighted sum of PDE, initial, and boundary condition losses.\n            \"\"\"\n            self.unpack_params(params_vec)\n            \n            # Unpack points\n            x_c, t_c = points['c']\n            x_i, t_i = points['i']\n            x_b, t_b = points['b']\n\n            # Unpack loss weights\n            lambda_pde, lambda_ic, lambda_bc = lambdas\n\n            # 1. PDE Residual Loss\n            u_t_c, _, u_xx_c = self.get_derivatives(x_c, t_c)\n            pde_residual = u_t_c - alpha * u_xx_c\n            loss_pde = lambda_pde * np.mean(pde_residual**2)\n\n            # 2. Initial Condition Loss\n            u_i = self.forward(x_i, t_i, p_drop=0.0)\n            ic_target = np.sin(np.pi * x_i.flatten())\n            loss_ic = lambda_ic * np.mean((u_i - ic_target)**2)\n            \n            # 3. Boundary Condition Loss\n            u_b = self.forward(x_b, t_b, p_drop=0.0)\n            loss_bc = lambda_bc * np.mean(u_b**2)\n            \n            return loss_pde + loss_ic + loss_bc\n\n    def train_pinn(H, alpha, N_c, N_i, N_b, iterations, lambdas):\n        \"\"\"\n        Initializes and trains a PINN model.\n        \n        Returns:\n            The trained model parameters as a 1D vector.\n        \"\"\"\n        # Initialize the model\n        model = PINN(H=H)\n        \n        # Generate training points\n        # Collocation points (domain interior)\n        x_c = np.random.rand(N_c, 1)\n        t_c = np.random.rand(N_c, 1)\n        \n        # Initial condition points (t=0)\n        x_i = np.random.rand(N_i, 1)\n        t_i = np.zeros_like(x_i)\n        \n        # Boundary condition points (x=0 and x=1)\n        t_b0 = np.random.rand(N_b // 2, 1)\n        t_b1 = np.random.rand(N_b // 2, 1)\n        x_b = np.vstack([np.zeros_like(t_b0), np.ones_like(t_b1)])\n        t_b = np.vstack([t_b0, t_b1])\n\n        points = {'c': (x_c, t_c), 'i': (x_i, t_i), 'b': (x_b, t_b)}\n        \n        # Initial parameter vector\n        p0 = model.pack_params()\n        \n        # Optimizer call\n        res = minimize(\n            fun=model.loss_function,\n            x0=p0,\n            args=(alpha, points, lambdas),\n            method='L-BFGS-B',\n            options={'maxiter': iterations}\n        )\n        return res.x\n        \n    def evaluate_coverage(params_vec, H, alpha, p_drop, M):\n        \"\"\"\n        Evaluates the empirical coverage of the model's uncertainty estimates.\n        \"\"\"\n        # Create a model with the trained parameters\n        model = PINN(H=H)\n        model.unpack_params(params_vec)\n        \n        # Create the held-out test grid\n        N_x, N_t = 20, 20\n        x_space = np.linspace(0, 1, N_x)\n        t_space = np.linspace(0, 1, N_t)\n        x_grid, t_grid = np.meshgrid(x_space, t_space)\n        x_flat, t_flat = x_grid.flatten()[:, None], t_grid.flatten()[:, None]\n        \n        # Perform Monte Carlo predictions\n        predictions = np.zeros((M, x_flat.shape[0]))\n        for i in range(M):\n            predictions[i, :] = model.forward(x_flat, t_flat, p_drop=p_drop)\n            \n        # Compute predictive mean and standard deviation\n        mu = np.mean(predictions, axis=0)\n        # Add a small epsilon for numerical stability if sigma is zero\n        sigma = np.std(predictions, axis=0) + 1e-8 \n        \n        # Get analytical solution for comparison\n        u_star = np.exp(-alpha * np.pi**2 * t_flat.flatten()) * np.sin(np.pi * x_flat.flatten())\n        \n        # Calculate empirical coverage for two confidence levels\n        z_scores = {'0.68': 1.0, '0.95': 1.96}\n        abs_error = np.abs(u_star - mu)\n        \n        coverage_68 = np.mean(abs_error = z_scores['0.68'] * sigma)\n        coverage_95 = np.mean(abs_error = z_scores['0.95'] * sigma)\n        \n        return coverage_68, coverage_95\n\n    # --- Test Case Execution ---\n    \n    H = 10\n    lambdas = (1.0, 100.0, 100.0) # (lambda_pde, lambda_ic, lambda_bc)\n    results = []\n\n    # Case A: Happy path\n    alpha_A = 0.1\n    p_A = 0.1\n    M_A = 100\n    trained_params_A = train_pinn(H, alpha_A, N_c=64, N_i=32, N_b=32, iterations=80, lambdas=lambdas)\n    cov_68_A, cov_95_A = evaluate_coverage(trained_params_A, H, alpha_A, p_A, M_A)\n    results.extend([cov_68_A, cov_95_A])\n    \n    # Case B: Dropout sensitivity (uses weights from A)\n    alpha_B = 0.1 # alpha is the same for the physics\n    p_B = 0.3\n    M_B = 100\n    cov_68_B, cov_95_B = evaluate_coverage(trained_params_A, H, alpha_B, p_B, M_B)\n    results.extend([cov_68_B, cov_95_B])\n\n    # Case C: Changed diffusivity and limited data\n    alpha_C = 0.2\n    p_C = 0.2\n    M_C = 100\n    trained_params_C = train_pinn(H, alpha_C, N_c=32, N_i=16, N_b=16, iterations=60, lambdas=lambdas)\n    cov_68_C, cov_95_C = evaluate_coverage(trained_params_C, H, alpha_C, p_C, M_C)\n    results.extend([cov_68_C, cov_95_C])\n    \n    # Print the final results in the required format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "3410639"}, {"introduction": "物理信息机器学习最前沿的应用之一，是从数据中直接发现未知的物理定律。本练习将您的角色从求解已知方程的工程师，转变为探索未知规律的科学家。您将使用基于物理的预测器和交叉验证这一强大的统计工具，从一系列候选模型中识别出最能描述观测数据的真实动力学方程，亲身体验数据驱动科学发现的核心过程([@problem_id:3410562])。", "problem": "构建一个完整的程序，用于在标量常微分方程 (ODE) 的物理信息、数据驱动发现设置中，评估交叉验证 (CV) 是否能够检测出模型误设。此问题的基础是一系列经过充分检验的原则：一个确定性的 ODE 控制标量状态的演化，显式欧拉法通过离散时间更新来逼近连续时间动力学，而 CV 则通过比较候选模型间的泛化误差来选择模型结构。物理信息神经网络 (PINN) 传统上是针对神经函数逼近器最小化物理残差；在这里，您将采用相同的残差最小化原则，但将其应用于参数化的 ODE 右端项，而非自由形式的网络，这保留了物理信息设计，同时能够使用允许的库进行可处理的推断。\n\n考虑遵循逻辑斯谛增长的真实动力学：\n$$\n\\frac{d u}{d t} = r u - \\frac{r}{K} u^2,\n$$\n其中 $u(t)$ 是标量状态，$r$ 是正常数增长率，$K$ 是正常数环境承载力。对于初始条件 $u(0) = u_0$，$u(t)$ 的精确解为\n$$\nu(t) = \\frac{K}{1 + \\left( \\frac{K}{u_0} - 1 \\right) e^{-r t}}.\n$$\n通过在有限时间范围内，在均匀间隔的时间点 $t_i$ 对该精确解进行采样，并为每个样本添加标准差已知的独立同分布高斯噪声，来生成合成观测数据。用于发现的候选模型类包含一个额外的三次项：\n$$\n\\frac{d u}{d t} = r u - \\frac{r}{K} u^2 + \\gamma u^3,\n$$\n其中 $\\gamma$ 是一个实标量系数。真实动力学省略了 $\\gamma u^3$ 项，这意味着正确设定的模型具有 $\\gamma = 0$。\n\n您的程序必须实现以下方法：\n- 定义两个候选模型：正确设定的模型 $\\mathcal{M}_0$，其参数为 $(r, K)$ 且 $\\gamma = 0$；以及误设的模型 $\\mathcal{M}_1$，其参数为 $(r, K, \\gamma)$。\n- 使用显式欧拉更新，在时间 $t_{i+1}$ 构建一个单步物理信息预测器：\n$$\n\\widehat{u}_{i+1} = u_i + \\Delta t_i \\, f(u_i; \\theta),\n$$\n其中 $f(u; \\theta)$ 是带有参数 $\\theta$ 的模型右端项，$u_i$ 是在时间 $t_i$ 观测到的状态，而 $\\Delta t_i = t_{i+1} - t_i$。\n- 通过最小化在训练索引上的单步预测误差平方和来估计参数 $\\theta$，并辅以一个小的 $\\ell_2$ (岭) 正则化：\n$$\n\\min_{\\theta} \\sum_{i \\in \\mathcal{I}_{\\text{train}}} \\left( u_i + \\Delta t_i \\, f(u_i; \\theta) - u_{i+1} \\right)^2 + \\lambda \\, \\lVert \\theta \\rVert_2^2,\n$$\n其中 $\\lambda > 0$ 是一个固定的较小值。\n- 使用时间索引上的连续折执行 K 折交叉验证。在每一折 $k$ 中，对完全位于验证段之外且时间上相邻的索引对 $(i,i+1)$ 所构成的训练对拟合 $\\theta$，然后计算完全位于验证段内的对的验证均方误差 (MSE)：\n$$\n\\text{MSE}_k(\\mathcal{M}) = \\frac{1}{|\\mathcal{I}^{(k)}_{\\text{val}}|} \\sum_{i \\in \\mathcal{I}^{(k)}_{\\text{val}}} \\left( u_i + \\Delta t_i \\, f(u_i; \\widehat{\\theta}_k) - u_{i+1} \\right)^2,\n$$\n其中 $\\widehat{\\theta}_k$ 是在第 $k$ 折的训练部分上拟合的参数估计值。模型 $\\mathcal{M}$ 的总体 CV 分数是 $\\text{MSE}_k(\\mathcal{M})$ 在 $k = 1, \\dots, K$ 上的平均值。\n- 定义模型误设检测的决策规则：如果简单模型的泛化能力至少与复杂模型一样好，则选择简单模型：\n$$\n\\text{Choose } \\mathcal{M}_0 \\text{ if } \\overline{\\text{MSE}}(\\mathcal{M}_0) \\le \\overline{\\text{MSE}}(\\mathcal{M}_1),\n$$\n当两者相等时，选择 $\\mathcal{M}_0$。\n\n通过评估逻辑斯谛精确解并使用固定的随机种子添加噪声来实现合成数据生成，以确保可复现性。在时间网格上使用连续的 K 折交叉验证。通过优化参数的对数来确保优化过程中 $r  0$ 和 $K  0$ 的约束；$\\gamma$ 无约束。对优化的变量直接应用一个小的岭回归正则化权重 $\\lambda$ (无量纲)。\n\n您必须在以下参数集的测试套件上评估该检测程序。对于所有情况，最终答案必须以布尔值的形式表示，指明交叉验证是否正确选择了较简单的模型 $\\mathcal{M}_0$；结果必须打印在单行中，形式为用方括号括起来的逗号分隔列表，例如 $[{\\tt True},{\\tt False},{\\tt True}]$。\n\n测试套件（所有量均为无量纲）：\n- 案例 1 (理想情况): $r = 1.0$, $K = 1.0$, $u_0 = 0.2$, 总时间范围 $T = 4.0$, 样本数 $N = 201$, 噪声标准差 $\\sigma = 0.02$, 折数 $K_{\\text{CV}} = 5$。\n- 案例 2 (中等噪声): $r = 1.0$, $K = 2.0$, $u_0 = 0.1$, $T = 4.0$, $N = 201$, $\\sigma = 0.08$, $K_{\\text{CV}} = 5$。\n- 案例 3 (短时间范围边界): $r = 1.5$, $K = 1.0$, $u_0 = 0.05$, $T = 1.0$, $N = 101$, $\\sigma = 0.01$, $K_{\\text{CV}} = 5$。\n- 案例 4 (接近饱和): $r = 0.5$, $K = 1.0$, $u_0 = 0.9$, $T = 6.0$, $N = 301$, $\\sigma = 0.02$, $K_{\\text{CV}} = 5$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果 (例如, $[{\\tt True},{\\tt False},{\\tt True},{\\tt True}]$)。不得打印任何其他文本。不涉及角度单位。不适用物理单位；所有变量均为无量纲。在给定固定随机种子的情况下，所有计算必须是自包含且确定性的。", "solution": "该问题是有效的。这是一个定义明确的计算实验，其基础是微分方程、数值方法和统计模型选择的既定原则。所有必要的参数和程序都已指定，从而能够得到唯一且可验证的解。\n\n目标是构建一个使用 K 折交叉验证 (CV) 的程序，以确定一个正确设定的逻辑斯谛增长模型是否能与一个误设的模型区分开来。此任务涉及合成数据生成、两个相互竞争的常微分方程 (ODE) 模型的参数估计，以及基于 CV 分数的模型选择决策。\n\n### 1. 系统动力学与数据生成\n\n真实的底层系统遵循逻辑斯谛增长模型，这是一个一阶非线性 ODE：\n$$\n\\frac{d u}{d t} = r u \\left( 1 - \\frac{u}{K} \\right) = r u - \\frac{r}{K} u^2\n$$\n其中 $u(t)$ 是状态变量，$r > 0$ 是内禀增长率，$K > 0$ 是环境承载力。给定初始条件 $u(0) = u_0$，其精确解为：\n$$\nu(t) = \\frac{K}{1 + \\left( \\frac{K}{u_0} - 1 \\right) e^{-r t}}\n$$\n合成数据是通过首先在区间 $[0, T]$ 上的 $N$ 个离散、均匀间隔的时间点 $t_i$ 计算精确解 $u(t_i)$，然后对每个点添加独立同分布 (i.i.d.) 的高斯噪声来生成的：\n$$\nu_{\\text{obs}}(t_i) = u(t_i) + \\epsilon_i, \\quad \\text{where} \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n$$\n使用固定的随机种子来确保噪声生成过程的可复现性。\n\n### 2. 候选模型\n\n考虑使用两个候选模型进行数据驱动的发现：\n\n- **模型 $\\mathcal{M}_0$ (正确设定):** 该模型具有与真实动力学相同的形式。其参数为 $\\theta_0 = (r, K)$。\n  $$\n  f_0(u; \\theta_0) = r u - \\frac{r}{K} u^2\n  $$\n\n- **模型 $\\mathcal{M}_1$ (误设):** 该模型包含一个额外的、无关的三次项，代表一个结构性错误。其参数为 $\\theta_1 = (r, K, \\gamma)$。真实动力学对应于 $\\gamma=0$。\n  $$\n  f_1(u; \\theta_1) = r u - \\frac{r}{K} u^2 + \\gamma u^3\n  $$\n\n### 3. 通过单步预测进行参数估计\n\n参数估计的核心是最小化源自显式欧拉法的单步前向预测器的误差。给定在时间 $t_i$ 的观测值 $u_i$，在下一个时间步 $t_{i+1}$ 的状态预测为：\n$$\n\\widehat{u}_{i+1} = u_i + \\Delta t_i \\, f(u_i; \\theta)\n$$\n其中 $\\Delta t_i = t_{i+1} - t_i$ 是时间步长，$f(u_i; \\theta)$ 是 ODE 模型 ($\\mathcal{M}_0$ 或 $\\mathcal{M}_1$) 的右端项。\n\n模型参数 $\\theta$ 是通过在一组训练数据对 $(u_i, u_{i+1})$ 上最小化一个正则化的误差平方和 (SSE) 成本函数来估计的。\n$$\nJ(\\theta_{\\text{opt}}) = \\sum_{i \\in \\mathcal{I}_{\\text{train}}} \\left( (u_i + \\Delta t_i \\, f(u_i; \\theta)) - u_{i+1} \\right)^2 + \\lambda \\lVert \\theta_{\\text{opt}} \\rVert_2^2\n$$\n此处，$\\lambda$ 是一个小的、正的岭回归正则化系数，用于增强数值稳定性。对 $r$ 和 $K$ 的正性约束通过优化它们的对数来强制执行，即对于 $\\mathcal{M}_0$ 有 $\\theta_{\\text{opt}} = (\\log r, \\log K)$，对于 $\\mathcal{M}_1$ 有 $\\theta_{\\text{opt}} = (\\log r, \\log K, \\gamma)$。该优化问题使用 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 算法求解。\n\n### 4. 用于模型选择的 K 折交叉验证\n\n为评估每个模型的泛化性能，我们实现了一个 K 折交叉验证程序。所有可用的数据对 $(u_i, u_{i+1})$（其中 $i=0, \\dots, N-2$）的集合被划分为 $K_{\\text{CV}}$ 个连续、不重叠的块（折）。\n\n对于每一折 $k \\in \\{1, \\dots, K_{\\text{CV}}\\}$：\n1.  第 $k$ 折被指定为验证集。所有其他 $K_{\\text{CV}}-1$ 折的并集构成训练集。\n2.  模型参数（记为 $\\widehat{\\theta}_k$）通过仅使用训练集最小化成本函数 $J$ 来估计。\n3.  通过计算均方误差 (MSE) 来评估拟合模型在留出的验证集上的性能，该误差不包括正则化项：\n    $$\n    \\text{MSE}_k(\\mathcal{M}) = \\frac{1}{|\\mathcal{I}^{(k)}_{\\text{val}}|} \\sum_{i \\in \\mathcal{I}^{(k)}_{\\text{val}}} \\left( (u_i + \\Delta t_i \\, f(u_i; \\widehat{\\theta}_k)) - u_{i+1} \\right)^2\n    $$\n模型 $\\mathcal{M}$ 的总体 CV 分数是所有 $K_{\\text{CV}}$ 折的 MSE 的平均值：\n$$\n\\overline{\\text{MSE}}(\\mathcal{M}) = \\frac{1}{K_{\\text{CV}}} \\sum_{k=1}^{K_{\\text{CV}}} \\text{MSE}_k(\\mathcal{M})\n$$\n\n### 5. 决策规则\n\n最终的模型选择基于对 CV 分数的比较。如果较简单模型 $\\mathcal{M}_0$ 的平均泛化误差小于或等于较复杂模型 $\\mathcal{M}_1$ 的误差，则优先选择前者。该规则体现了奥卡姆剃刀的一种形式。\n$$\n\\text{Decision: Choose }\n\\begin{cases}\n    \\mathcal{M}_0  \\text{if } \\overline{\\text{MSE}}(\\mathcal{M}_0) \\le \\overline{\\text{MSE}}(\\mathcal{M}_1) \\\\\n    \\mathcal{M}_1  \\text{if } \\overline{\\text{MSE}}(\\mathcal{M}_0) > \\overline{\\text{MSE}}(\\mathcal{M}_1)\n\\end{cases}\n$$\n对于每个测试案例，如果 $\\mathcal{M}_0$ 被正确选择，程序报告 `True`，否则报告 `False`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the cross-validation experiment for model selection.\n    \"\"\"\n    \n    # Global constants for the experiment\n    RANDOM_SEED = 42\n    LAMBDA_REG = 1e-6\n\n    def f_m0(u, params):\n        \"\"\"Right-hand side of the correctly specified model M0.\"\"\"\n        r, K = params\n        return r * u - (r / K) * u**2\n\n    def f_m1(u, params):\n        \"\"\"Right-hand side of the misspecified model M1.\"\"\"\n        r, K, gamma = params\n        return r * u - (r / K) * u**2 + gamma * u**3\n\n    def cost_function(opt_params, u_obs, dt, train_indices, model_type):\n        \"\"\"\n        Cost function for parameter optimization, including squared error and L2 regularization.\n        \"\"\"\n        if model_type == 'm0':\n            r = np.exp(opt_params[0])\n            K = np.exp(opt_params[1])\n            params = (r, K)\n            f_model = f_m0\n        elif model_type == 'm1':\n            r = np.exp(opt_params[0])\n            K = np.exp(opt_params[1])\n            gamma = opt_params[2]\n            params = (r, K, gamma)\n            f_model = f_m1\n        \n        u_i = u_obs[train_indices]\n        u_i_plus_1 = u_obs[train_indices + 1]\n        \n        # One-step Euler prediction\n        u_pred = u_i + dt * f_model(u_i, params)\n        errors = u_pred - u_i_plus_1\n        \n        sse = np.sum(errors**2)\n        reg_term = LAMBDA_REG * np.sum(np.array(opt_params)**2)\n        \n        return sse + reg_term\n\n    def k_fold_cv(u_obs, t, K_cv, model_type):\n        \"\"\"\n        Performs K-fold cross-validation for a given model type.\n        \"\"\"\n        num_points = len(u_obs)\n        num_pairs = num_points - 1\n        pair_indices = np.arange(num_pairs)\n        \n        if num_pairs  K_cv:\n            raise ValueError(\"Number of data pairs must be greater than or equal to K_cv.\")\n\n        folds = np.array_split(pair_indices, K_cv)\n        \n        dt = t[1] - t[0]\n        \n        fold_mses = []\n        \n        for k in range(K_cv):\n            val_indices = folds[k]\n            train_indices_list = [f for i, f in enumerate(folds) if i != k]\n            train_indices = np.concatenate(train_indices_list)\n\n            # Define initial guess for optimization\n            if model_type == 'm0':\n                initial_guess = np.array([0.0, 0.0])  # log(r=1), log(K=1)\n                f_model = f_m0\n            else:  # m1\n                initial_guess = np.array([0.0, 0.0, 0.0])  # log(r=1), log(K=1), gamma=0\n                f_model = f_m1\n\n            res = minimize(\n                cost_function,\n                initial_guess,\n                args=(u_obs, dt, train_indices, model_type),\n                method='BFGS',\n                options={'gtol': 1e-6, 'disp': False}\n            )\n            opt_params_k = res.x\n            \n            # Unpack estimated parameters for validation\n            if model_type == 'm0':\n                r_hat = np.exp(opt_params_k[0])\n                K_hat = np.exp(opt_params_k[1])\n                params_hat = (r_hat, K_hat)\n            else:  # m1\n                r_hat = np.exp(opt_params_k[0])\n                K_hat = np.exp(opt_params_k[1])\n                gamma_hat = opt_params_k[2]\n                params_hat = (r_hat, K_hat, gamma_hat)\n            \n            # Compute validation MSE\n            if len(val_indices) > 0:\n                u_i_val = u_obs[val_indices]\n                u_i_plus_1_val = u_obs[val_indices + 1]\n                u_pred_val = u_i_val + dt * f_model(u_i_val, params_hat)\n                val_errors = u_pred_val - u_i_plus_1_val\n                val_mse = np.mean(val_errors**2)\n                fold_mses.append(val_mse)\n\n        return np.mean(fold_mses) if fold_mses else np.inf\n\n    test_cases = [\n        # (r, K, u0, T, N, sigma, K_cv)\n        (1.0, 1.0, 0.2, 4.0, 201, 0.02, 5),\n        (1.0, 2.0, 0.1, 4.0, 201, 0.08, 5),\n        (1.5, 1.0, 0.05, 1.0, 101, 0.01, 5),\n        (0.5, 1.0, 0.9, 6.0, 301, 0.02, 5),\n    ]\n\n    results = []\n    rng = np.random.RandomState(RANDOM_SEED)\n\n    for r_true, K_true, u0, T, N, sigma, K_cv in test_cases:\n        # Generate synthetic data\n        t = np.linspace(0, T, N)\n        u_exact = K_true / (1 + (K_true / u0 - 1) * np.exp(-r_true * t))\n        noise = rng.normal(0, sigma, N)\n        u_obs = u_exact + noise\n\n        # Run CV for both models\n        mse_m0 = k_fold_cv(u_obs, t, K_cv, 'm0')\n        mse_m1 = k_fold_cv(u_obs, t, K_cv, 'm1')\n        \n        # Apply decision rule: prefer simpler model on tie or better performance\n        is_correctly_identified = mse_m0 = mse_m1\n        results.append(is_correctly_identified.item())\n        \n    print(f\"[{','.join(map(lambda x: str(x).title(), results))}]\")\n\nsolve()\n```", "id": "3410562"}]}