{"hands_on_practices": [{"introduction": "高斯过程回归的核心在于利用协方差矩阵来描述数据点之间的关系。对数边缘似然的计算和从过程中进行高效采样都严重依赖于对这个矩阵的数值操作。这项练习旨在通过一个简单的二维示例，让您亲手实践两个最基本的计算：计算协方差矩阵的对数行列式和其Cholesky分解，这是保证数值稳定性和计算效率的关键步骤。[@problem_id:3423975]", "problem": "考虑一个数据同化背景下的线性观测模型，其中潜状态被赋予了高斯过程（GP）先验。在高斯过程回归（GPR）中，两个输入位置上两个带噪声的观测函数值的协方差是先验协方差与一个独立的观测噪声方差之和。具体而言，设先验协方差矩阵为 $K=\\begin{pmatrix}1  & 0.9\\\\ 0.9  & 1\\end{pmatrix}$，观测噪声方差为 $\\sigma^{2}=0.1$，因此观测数据的总协方差为 $A=K+\\sigma^{2}I$。为了在反问题中进行代理建模和超参数推断，需要两个计算基元：行列式的自然对数 $\\ln\\det(A)$ 和满足 $A=L\\,L^{\\top}$ 且对角线元素为正的下三角 Cholesky 因子 $L$。\n\n从行列式的定义和基本性质以及对称正定矩阵的 Cholesky 分解出发，显式计算给定 $K$ 和 $\\sigma^{2}$ 下的 $\\ln\\det(K+\\sigma^{2}I)$ 和 Cholesky 因子 $L$。使用自然对数并提供精确的代数表达式（不要进行四舍五入）。你的最终答案必须包含这两个量，其中 Cholesky 因子由其非零元素指定。", "solution": "用户提供了一个来自数据同化和机器学习领域的有效且适定的问题陈述。\n\n问题要求计算高斯过程回归的两个关键量：协方差矩阵行列式的自然对数 $\\ln\\det(A)$，以及该矩阵的 Cholesky 因子 $L$。\n\n给定的量是：\n先验协方差矩阵 $K = \\begin{pmatrix} 1  & 0.9 \\\\ 0.9  & 1 \\end{pmatrix}$。\n观测噪声方差 $\\sigma^{2}=0.1$。\n观测数据的总协方差由模型 $A=K+\\sigma^{2}I$ 给出，其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n\n首先，我们构建矩阵 $A$。\n$$\nI = \\begin{pmatrix} 1  & 0 \\\\ 0  & 1 \\end{pmatrix}\n$$\n$$\nA = K + \\sigma^{2}I = \\begin{pmatrix} 1  & 0.9 \\\\ 0.9  & 1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 1  & 0 \\\\ 0  & 1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1  & 0.9 \\\\ 0.9  & 1 \\end{pmatrix} + \\begin{pmatrix} 0.1  & 0 \\\\ 0  & 0.1 \\end{pmatrix} = \\begin{pmatrix} 1.1  & 0.9 \\\\ 0.9  & 1.1 \\end{pmatrix}\n$$\n为保持精确性，我们可以将这些小数值表示为分数：\n$$\nA = \\begin{pmatrix} \\frac{11}{10}  & \\frac{9}{10} \\\\ \\frac{9}{10}  & \\frac{11}{10} \\end{pmatrix}\n$$\n\n接下来，我们计算第一个所需量 $\\ln\\det(A)$。\n一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  & b \\\\ c  & d \\end{pmatrix}$ 的行列式由公式 $ad - bc$ 给出。\n$$\n\\det(A) = \\left(\\frac{11}{10}\\right) \\left(\\frac{11}{10}\\right) - \\left(\\frac{9}{10}\\right) \\left(\\frac{9}{10}\\right) = \\frac{121}{100} - \\frac{81}{100} = \\frac{121 - 81}{100} = \\frac{40}{100} = \\frac{2}{5}\n$$\n因此，行列式的自然对数是：\n$$\n\\ln\\det(A) = \\ln\\left(\\frac{2}{5}\\right)\n$$\n\n第二，我们计算 Cholesky 因子 $L$。$A$ 是一个对称正定矩阵，因此存在一个唯一的、对角线元素为正的下三角矩阵 $L$，使得 $A = L L^{\\top}$。设 $L$ 为：\n$$\nL = \\begin{pmatrix} L_{11}  & 0 \\\\ L_{21}  & L_{22} \\end{pmatrix}\n$$\n条件 $A=LL^{\\top}$ 给出：\n$$\n\\begin{pmatrix} A_{11}  & A_{12} \\\\ A_{21}  & A_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}  & 0 \\\\ L_{21}  & L_{22} \\end{pmatrix} \\begin{pmatrix} L_{11}  & L_{21} \\\\ 0  & L_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}^2  & L_{11}L_{21} \\\\ L_{21}L_{11}  & L_{21}^2 + L_{22}^2 \\end{pmatrix}\n$$\n我们通过令矩阵的元素相等来求解 $L$ 的各项：\n$(1,1)$ 元素给出 $A_{11} = L_{11}^2$：\n$$\nL_{11}^2 = \\frac{11}{10} \\implies L_{11} = \\sqrt{\\frac{11}{10}}\n$$\n根据 Cholesky 分解的要求，我们取正根。\n\n$(2,1)$ 元素给出 $A_{21} = L_{21}L_{11}$：\n$$\nL_{21} = \\frac{A_{21}}{L_{11}} = \\frac{9/10}{\\sqrt{11/10}} = \\frac{9}{10} \\frac{\\sqrt{10}}{\\sqrt{11}} = \\frac{9}{\\sqrt{10}\\sqrt{11}} = \\frac{9}{\\sqrt{110}}\n$$\n\n$(2,2)$ 元素给出 $A_{22} = L_{21}^2 + L_{22}^2$：\n$$\nL_{22}^2 = A_{22} - L_{21}^2 = \\frac{11}{10} - \\left(\\frac{9}{\\sqrt{110}}\\right)^2 = \\frac{11}{10} - \\frac{81}{110}\n$$\n为了相减，我们找到一个公分母：\n$$\nL_{22}^2 = \\frac{11 \\times 11}{110} - \\frac{81}{110} = \\frac{121 - 81}{110} = \\frac{40}{110} = \\frac{4}{11}\n$$\n取正平方根：\n$$\nL_{22} = \\sqrt{\\frac{4}{11}} = \\frac{2}{\\sqrt{11}}\n$$\n因此，Cholesky 因子是：\n$$\nL = \\begin{pmatrix} \\sqrt{\\frac{11}{10}}  & 0 \\\\ \\frac{9}{\\sqrt{110}}  & \\frac{2}{\\sqrt{11}} \\end{pmatrix}\n$$\n问题要求两个计算出的量：$\\ln\\det(A)$ 和 Cholesky 因子 $L$ 的非零元素，即 $L_{11}$、$L_{21}$ 和 $L_{22}$。\n\n最终值为：\n$\\ln\\det(A) = \\ln(\\frac{2}{5})$\n$L_{11} = \\sqrt{\\frac{11}{10}}$\n$L_{21} = \\frac{9}{\\sqrt{110}}$\n$L_{22} = \\frac{2}{\\sqrt{11}}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln\\left(\\frac{2}{5}\\right) & \\sqrt{\\frac{11}{10}} & \\frac{9}{\\sqrt{110}} & \\frac{2}{\\sqrt{11}} \\end{pmatrix}}\n$$", "id": "3423975"}, {"introduction": "在处理真实世界的实验数据时，一个常见的挑战是观测噪声并非均匀的，即存在异方差性。虽然标准的高斯过程模型通常假设噪声方差恒定，但我们可以通过巧妙的统计方法来处理这个问题。这项练习将指导您如何利用重复测量数据来推导局部噪声方差的无偏估计量，并说明如何将此估计整合到高斯过程的观测协方差中，这是实验数据建模中的一项重要技能。[@problem_id:3423985]", "problem": "考虑一个标量前向模型，其中未知函数 $f:\\mathbb{R}^{d}\\to\\mathbb{R}$ 被赋予了均值为 $0$、协方差核为 $k(\\cdot,\\cdot)$ 的高斯过程（GP）先验。在输入 $x_{i}\\in\\mathbb{R}^{d}$ 处，您执行了 $2$ 次独立的重复测量\n$$\ny_{i1}=f(x_{i})+\\varepsilon_{i1},\\qquad y_{i2}=f(x_{i})+\\varepsilon_{i2},\n$$\n其中，观测误差在给定 $f$ 的条件下是条件独立的、高斯的、零均值的，并且是异方差的，即 $\\varepsilon_{ij}\\,|\\,f\\sim\\mathcal{N}(0,\\sigma^{2}(x_{i}))$，其方差依赖于 $x_{i}$，但在 $x_{i}$ 处的两次重复测量中是相同的。假设在 $x_{i}$ 处的重复测量期间，$f(x_{i})$ 是一个未知但固定的值。\n\n从这些定义和独立高斯随机变量的性质出发，仅使用两次重复测量值 $y_{i1}$ 和 $y_{i2}$ 推导局部噪声方差 $\\sigma^{2}(x_{i})$ 的无偏估计量。然后解释如何将此估计量纳入高斯过程回归（GPR）中用于后验预测的观测协方差。具体来说，假设您将 $x_{i}$ 处的两次重复测量值压缩为其算术平均值 $\\bar{y}_{i}=\\frac{1}{2}(y_{i1}+y_{i2})$，并将 $\\bar{y}_{i}$ 视为在 $x_{i}$ 处的单个观测值。给出应在后验计算中使用的、$x_{i}$ 处观测协方差矩阵 $\\Sigma$ 的最终估计对角线元素。\n\n您的最终答案必须是关于 $\\Sigma$ 在 $x_{i}$ 处的这个估计对角线元素的单一闭式解析表达式，用 $y_{i1}$ 和 $y_{i2}$ 表示。不需要四舍五入，也不涉及单位。", "solution": "问题陈述经评估有效。它具有科学依据，提法恰当，客观，并包含足够的信息，可以在高斯过程回归和统计估计的背景下推导出唯一且有意义的解。该问题没有矛盾、歧义或事实错误。\n\n该问题要求推导两个相关的量。首先，使用两次重复测量值 $y_{i1}$ 和 $y_{i2}$，为局部异方差噪声方差 $\\sigma^{2}(x_{i})$ 推导一个无偏估计量。其次，推导观测噪声协方差矩阵 $\\Sigma$ 中对应于这两次重复测量的算术平均值 $\\bar{y}_{i}$ 的估计对角线元素，该元素将用于高斯过程回归（GPR）模型。\n\n我们首先推导 $\\sigma^{2}(x_{i})$ 的无偏估计量。在位置 $x_{i}$ 处的两次重复测量的测量模型由下式给出：\n$$ y_{i1} = f(x_{i}) + \\varepsilon_{i1} $$\n$$ y_{i2} = f(x_{i}) + \\varepsilon_{i2} $$\n观测误差 $\\varepsilon_{i1}$ 和 $\\varepsilon_{i2}$ 被规定为来自均值为零、方差为 $\\sigma^{2}(x_{i})$ 的高斯分布的独立抽样，即 $\\varepsilon_{i1}, \\varepsilon_{i2} \\sim \\mathcal{N}(0, \\sigma^{2}(x_{i}))$。函数值 $f(x_{i})$ 是一个未知量，为了从局部重复测量中估计噪声方差，我们可以将其视为一个固定的、未观测到的参数。\n\n为了从未知函数值 $f(x_{i})$ 中分离出噪声项，我们考虑两次测量值之间的差值：\n$$ d_{i} = y_{i1} - y_{i2} = (f(x_{i}) + \\varepsilon_{i1}) - (f(x_{i}) + \\varepsilon_{i2}) = \\varepsilon_{i1} - \\varepsilon_{i2} $$\n随机变量 $d_{i}$ 是两个独立高斯随机变量的线性组合，因此它本身也是高斯分布的。其期望值为：\n$$ E[d_{i}] = E[\\varepsilon_{i1} - \\varepsilon_{i2}] = E[\\varepsilon_{i1}] - E[\\varepsilon_{i2}] = 0 - 0 = 0 $$\n$d_{i}$ 的方差是利用独立随机变量的和或差的方差等于其方差之和的性质计算的：\n$$ \\text{Var}(d_{i}) = \\text{Var}(\\varepsilon_{i1} - \\varepsilon_{i2}) = \\text{Var}(\\varepsilon_{i1}) + \\text{Var}(\\varepsilon_{i2}) = \\sigma^{2}(x_{i}) + \\sigma^{2}(x_{i}) = 2\\sigma^{2}(x_{i}) $$\n对于任何随机变量 $X$，其方差通过 $\\text{Var}(X) = E[X^{2}] - (E[X])^{2}$ 与其一阶矩和二阶矩相关。由于 $E[d_{i}] = 0$， $d_{i}$ 的方差就是其二阶矩：\n$$ \\text{Var}(d_{i}) = E[d_{i}^{2}] = 2\\sigma^{2}(x_{i}) $$\n代入 $d_{i} = y_{i1} - y_{i2}$，我们有：\n$$ E[(y_{i1} - y_{i2})^{2}] = 2\\sigma^{2}(x_{i}) $$\n这个方程为 $\\sigma^{2}(x_{i})$ 提供了一个无偏估计量。设该估计量为 $\\hat{\\sigma}^{2}(x_{i})$。通过重新整理方程，我们将估计量定义为：\n$$ \\hat{\\sigma}^{2}(x_{i}) = \\frac{1}{2}(y_{i1} - y_{i2})^{2} $$\n为了验证该估计量确实是无偏的，我们计算其期望值：\n$$ E[\\hat{\\sigma}^{2}(x_{i})] = E\\left[\\frac{1}{2}(y_{i1} - y_{i2})^{2}\\right] = \\frac{1}{2}E[(y_{i1} - y_{i2})^{2}] = \\frac{1}{2}(2\\sigma^{2}(x_{i})) = \\sigma^{2}(x_{i}) $$\n该估计量的期望值等于真实参数，这证实了它是局部噪声方差 $\\sigma^{2}(x_{i})$ 的一个无偏估计量。\n\n接下来，我们解决问题的第二部分。将两次重复测量值压缩为单个数据点，即它们的算术平均值：\n$$ \\bar{y}_{i} = \\frac{1}{2}(y_{i1} + y_{i2}) $$\n代入 $y_{i1}$ 和 $y_{i2}$ 的测量模型：\n$$ \\bar{y}_{i} = \\frac{1}{2}\\left( (f(x_{i}) + \\varepsilon_{i1}) + (f(x_{i}) + \\varepsilon_{i2}) \\right) = f(x_{i}) + \\frac{1}{2}(\\varepsilon_{i1} + \\varepsilon_{i2}) $$\n这表明，使用 $\\bar{y}_{i}$ 作为 $f(x_{i})$ 的观测值，等价于一个模型 $\\bar{y}_{i} = f(x_{i}) + \\bar{\\varepsilon}_{i}$，其中有效噪声项为 $\\bar{\\varepsilon}_{i} = \\frac{1}{2}(\\varepsilon_{i1} + \\varepsilon_{i2})$。\n\n在 GPR 中，观测噪声协方差矩阵 $\\Sigma$ 的对角线元素对应于每个观测值的噪声项的方差。对于复合观测值 $\\bar{y}_{i}$，相应的对角线元素（记为 $\\Sigma_{ii}$）是其有效噪声项的方差，即 $\\text{Var}(\\bar{\\varepsilon}_{i})$。我们现在计算这个方差：\n$$ \\Sigma_{ii} = \\text{Var}(\\bar{\\varepsilon}_{i}) = \\text{Var}\\left(\\frac{1}{2}(\\varepsilon_{i1} + \\varepsilon_{i2})\\right) = \\left(\\frac{1}{2}\\right)^{2} \\text{Var}(\\varepsilon_{i1} + \\varepsilon_{i2}) $$\n再次利用 $\\varepsilon_{i1}$ 和 $\\varepsilon_{i2}$ 的独立性：\n$$ \\text{Var}(\\varepsilon_{i1} + \\varepsilon_{i2}) = \\text{Var}(\\varepsilon_{i1}) + \\text{Var}(\\varepsilon_{i2}) = 2\\sigma^{2}(x_{i}) $$\n将此结果代回 $\\Sigma_{ii}$ 的表达式中：\n$$ \\Sigma_{ii} = \\frac{1}{4}(2\\sigma^{2}(x_{i})) = \\frac{1}{2}\\sigma^{2}(x_{i}) $$\n这个结果是直观的：对两个方差为 $\\sigma^{2}$ 的独立测量值求平均，会使测量噪声的方差减少一个因子 $2$，这遵循了均值标准误公式 $\\frac{\\sigma^{2}}{n}$（其中 $n=2$）。\n\n量 $\\Sigma_{ii} = \\frac{1}{2}\\sigma^{2}(x_{i})$ 是平均观测值的真实方差，但它依赖于未知的真实方差 $\\sigma^{2}(x_{i})$。问题要求的是 $\\Sigma$ 的*估计*对角线元素，我们通过用无偏估计量 $\\hat{\\sigma}^{2}(x_{i})$ 代替真实值 $\\sigma^{2}(x_{i})$ 来获得它。我们将这个估计量表示为 $\\hat{\\Sigma}_{ii}$：\n$$ \\hat{\\Sigma}_{ii} = \\frac{1}{2}\\hat{\\sigma}^{2}(x_{i}) $$\n现在，我们代入我们的估计量表达式 $\\hat{\\sigma}^{2}(x_{i}) = \\frac{1}{2}(y_{i1} - y_{i2})^{2}$：\n$$ \\hat{\\Sigma}_{ii} = \\frac{1}{2}\\left[\\frac{1}{2}(y_{i1} - y_{i2})^{2}\\right] = \\frac{1}{4}(y_{i1} - y_{i2})^{2} $$\n这就是仅从两次重复测量值 $y_{i1}$ 和 $y_{i2}$ 推导出的、在位置 $x_{i}$ 处的观测协方差矩阵 $\\Sigma$ 的估计对角线元素的最终闭式表达式。该值将用于 GPR 后验计算中噪声协方差矩阵的对角线上。", "answer": "$$\\boxed{\\frac{1}{4}(y_{i1} - y_{i2})^{2}}$$", "id": "3423985"}, {"introduction": "高斯过程模型的预测能力在很大程度上取决于其超参数的选择，例如核函数参数和噪声水平。最大似然估计（MLE）是确定这些超参数的主要方法之一。这项练习将引导您完成对噪声超参数进行最大似然估计的全过程，从推导对数似然函数的梯度到实现数值求解器来寻找最优值，这是任何希望有效使用高斯过程的人都必须掌握的核心技能。[@problem_id:3423961]", "problem": "考虑一个用于逆问题和数据同化背景下计算昂贵的正向模型的一维高斯过程（GP）回归代理模型。给定一个大小为 $n=5$ 的小数据集，包含输入位置和相应的标量输出。输出之间的协方差由平方指数（SE）核函数 $k_{\\mathrm{SE}}$ 建模，其信号方差和长度尺度超参数是固定的。假设存在加性独立同分布的高斯观测噪声，其方差 $\\sigma^2$ 未知。您的目标是通过对精确边缘似然进行微分并数值求解一阶条件（FOC），来获得 $\\sigma^2$ 的最大似然估计（MLE）。\n\n从高斯过程回归的基本定义和多元正态分布的精确边缘似然出发，仅使用矩阵微积分的成熟性质（如对数行列式的导数和矩阵逆相对于标量参数的导数），推导出 $\\sigma^2$ 的一阶最优性条件。不要假设任何跳过此类推导的快捷公式。然后，实现一个数值求解器来找到满足所推导出的 FOC 的最大似然估计 $\\hat{\\sigma}^2$。如果 FOC 在开区间 $\\sigma^2>0$ 内没有解，则通过正确分析边界行为来确定最大化者。\n\n使用以下固定的核函数和数据规范。对于每个测试用例，协方差矩阵定义为\n$$\nK_{ij} = k_{\\mathrm{SE}}(x_i, x_j) = \\alpha^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{|x_i - x_j|}{\\ell}\\right)^2\\right),\n$$\n其中 $\\alpha^2$ 是信号方差，$\\ell$ 是长度尺度。观测模型为 $y \\sim \\mathcal{N}(0, K + \\sigma^2 I)$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n实现一个程序，对于每个测试用例，通过以下步骤计算 MLE $\\hat{\\sigma}^2$：\n- 使用 $k_{\\mathrm{SE}}$ 和给定的 $(\\alpha^2, \\ell)$ 构建协方差矩阵 $K$，\n- 形成 $y$ 在 $K + \\sigma^2 I$ 下的精确对数边缘似然，\n- 对 $\\sigma^2$ 求导以获得 FOC，\n- 对 $\\sigma^2 > 0$ 数值求解 FOC；如果不存在内部解，则选择正确的边界最大化者。\n\n将每个 $\\hat{\\sigma}^2$ 报告为四舍五入到六位小数的浮点数。\n\n测试套件：\n- 情况 1（一般情况）：$x = [0.0, 0.25, 0.5, 0.75, 1.0]$，$y = [0.05, 0.6, 0.9, 0.6, 0.0]$，$\\alpha^2 = 1.2$，$\\ell = 0.3$。\n- 情况 2（边界情况，信号接近于零）：$x = [0.0, 0.1, 0.4, 0.8, 1.2]$，$y = [0.0, 0.0, 0.0, 0.0, 0.0]$，$\\alpha^2 = 0.8$，$\\ell = 0.5$。\n- 情况 3（不同的尺度和曲率）：$x = [0.0, 0.2, 0.5, 0.9, 1.4]$，$y = [1.0, 0.9, 0.4, -0.2, -0.7]$，$\\alpha^2 = 1.5$，$\\ell = 0.4$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[\\text{result1},\\text{result2},\\text{result3}]$），每个 $\\hat{\\sigma}^2$ 四舍五入到六位小数。此问题不涉及物理单位，所有角度（如有）均默认为无量纲实数。最终输出必须是浮点数。", "solution": "用户希望在一个高斯过程（GP）回归模型中，找到噪声方差（$\\sigma^2$）的最大似然估计（MLE）。该过程包括两个主要阶段：首先，从基本原理出发，对 $\\sigma^2$ 的 MLE 进行一阶条件（FOC）的形式化推导；其次，实现一个数值求解器，以找到满足此条件的 $\\sigma^2$ 值，用于给定的数据集。\n\n### 第 1 部分：一阶条件的推导\n\n问题指定了一个用于输入位置 $X$ 处的一组观测值 $y$ 的 GP 模型。假设观测值服从零均值的多元正态分布：\n$$\ny \\sim \\mathcal{N}(0, K_y)\n$$\n其中 $K_y$ 是带噪声观测值的协方差矩阵。它被定义为信号协方差矩阵 $K$ 和噪声协方差矩阵之和。信号协方差由平方指数（SE）核函数给出，噪声被假设为方差为 $\\sigma^2$ 的独立同分布高斯噪声。因此，总协方差矩阵为：\n$$\nK_y = K + \\sigma^2 I\n$$\n其中 $K_{ij} = k_{\\mathrm{SE}}(x_i, x_j) = \\alpha^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{|x_i - x_j|}{\\ell}\\right)^2\\right)$ 且 $I$ 是 $n \\times n$ 的单位矩阵，其中 $n$ 是数据点的数量。\n\n此多元正态分布的概率密度函数（PDF）为：\n$$\np(y | X, \\alpha^2, \\ell, \\sigma^2) = \\frac{1}{(2\\pi)^{n/2} (\\det(K_y))^{1/2}} \\exp\\left(-\\frac{1}{2} y^T K_y^{-1} y\\right)\n$$\n对数边缘似然，记为 $\\mathcal{L}(\\sigma^2)$，是该 PDF 的自然对数。我们将 $\\sigma^2$ 视为待优化的变量，而其他参数是固定的。\n$$\n\\mathcal{L}(\\sigma^2) = \\log p(y | \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\det(K_y)) - \\frac{1}{2} y^T K_y^{-1} y\n$$\n为了找到 $\\sigma^2$ 的 MLE，我们必须最大化 $\\mathcal{L}(\\sigma^2)$。这等同于最大化不含常数项 $-\\frac{n}{2}\\log(2\\pi)$ 的表达式。为简化符号，我们定义 $\\theta = \\sigma^2$。目标函数变为：\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{2}\\log(\\det(K + \\theta I)) - \\frac{1}{2} y^T (K + \\theta I)^{-1} y\n$$\n我们通过对 $\\mathcal{L}(\\theta)$ 关于 $\\theta$ 求导并令其为零来找到最大值。这就是一阶条件（FOC）。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = -\\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ \\log(\\det(K + \\theta I)) \\right] - \\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ y^T (K + \\theta I)^{-1} y \\right] = 0\n$$\n为计算这个导数，我们使用矩阵微积分中的两个标准恒等式。对于一个作为标量 $t$ 函数的对称矩阵 $A(t)$：\n1.  对数行列式的导数：$\\frac{\\partial}{\\partial t} \\log(\\det(A(t))) = \\text{tr}\\left(A(t)^{-1} \\frac{\\partial A(t)}{\\partial t}\\right)$\n2.  逆的导数：$\\frac{\\partial}{\\partial t} A(t)^{-1} = -A(t)^{-1} \\frac{\\partial A(t)}{\\partial t} A(t)^{-1}$\n\n在我们的问题中，矩阵是 $A(\\theta) = K + \\theta I$。它关于 $\\theta$ 的导数是：\n$$\n\\frac{\\partial A(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(K + \\theta I) = I\n$$\n因为 $K$ 相对于 $\\theta$ 是常数。\n\n现在，我们将这些恒等式应用于 $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$ 的每一项：\n\n对于第一项（对数行列式）：\n$$\n\\frac{\\partial}{\\partial \\theta} \\log(\\det(K + \\theta I)) = \\text{tr}\\left((K + \\theta I)^{-1} \\frac{\\partial(K + \\theta I)}{\\partial \\theta}\\right) = \\text{tr}\\left((K + \\theta I)^{-1} I\\right) = \\text{tr}((K + \\theta I)^{-1})\n$$\n\n对于第二项（二次型）：\n$$\n\\frac{\\partial}{\\partial \\theta} \\left[ y^T (K + \\theta I)^{-1} y \\right] = y^T \\left( \\frac{\\partial}{\\partial \\theta} (K + \\theta I)^{-1} \\right) y\n$$\n使用逆的导数恒等式：\n$$\n\\frac{\\partial}{\\partial \\theta} (K + \\theta I)^{-1} = -(K + \\theta I)^{-1} \\left(\\frac{\\partial(K + \\theta I)}{\\partial \\theta}\\right) (K + \\theta I)^{-1} = -(K + \\theta I)^{-1} I (K + \\theta I)^{-1} = -(K + \\theta I)^{-2}\n$$\n将其代回，二次项的导数是：\n$$\ny^T \\left( -(K + \\theta I)^{-2} \\right) y = -y^T (K + \\theta I)^{-2} y\n$$\n\n结合两项的导数，我们得到对数边缘似然的导数：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = -\\frac{1}{2} \\left[ \\text{tr}((K + \\theta I)^{-1}) \\right] - \\frac{1}{2} \\left[ -y^T (K + \\theta I)^{-2} y \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{1}{2} \\left[ y^T (K + \\theta I)^{-2} y - \\text{tr}((K + \\theta I)^{-1}) \\right]\n$$\n将此导数设为零，得到 MLE $\\hat{\\theta} = \\hat{\\sigma}^2$ 的 FOC：\n$$\ny^T (K + \\hat{\\sigma}^2 I)^{-2} y = \\text{tr}((K + \\hat{\\sigma}^2 I)^{-1})\n$$\n这是一个关于 $\\hat{\\sigma}^2$ 的非线性标量方程，必须进行数值求解。\n\n### 第 2 部分：数值求解策略\n\nFOC 要求找到函数的一个根 $\\hat{\\sigma}^2 > 0$：\n$$\nf(\\sigma^2) = y^T (K + \\sigma^2 I)^{-2} y - \\text{tr}((K + \\sigma^2 I)^{-1})\n$$\n解的搜索必须小心处理，因为最大化者可能位于有效域的边界上，即 $\\hat{\\sigma}^2 = 0$。对数似然函数 $\\mathcal{L}(\\sigma^2)$ 的行为由其导数的符号决定，即 $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} = \\frac{1}{2}f(\\sigma^2)$。\n\n我们分析导数在边界 $\\sigma^2 \\to 0^+$ 处的符号：\n$$\n\\lim_{\\sigma^2 \\to 0^+} \\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} = \\frac{1}{2} \\left[ y^T K^{-2} y - \\text{tr}(K^{-1}) \\right]\n$$\n（这假设 $K$ 是可逆的，对于具有不同输入的 SE 核函数，这是成立的）。\n\n1.  如果在 $\\sigma^2 = 0^+$ 处 $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} \\le 0$，则对数似然函数初始是非增的。由于对于大的 $\\sigma^2$，$\\mathcal{L}(\\sigma^2) \\to -\\infty$，函数在边界处达到最大值。因此，MLE 为 $\\hat{\\sigma}^2 = 0$。\n\n2.  如果在 $\\sigma^2 = 0^+$ 处 $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} > 0$，则对数似然函数初始是递增的。由于它最终必须减小到 $-\\infty$，所以在域的内部必须存在一个最大值，即在某个 $\\hat{\\sigma}^2 > 0$ 处。这个内部最大值对应于 FOC 的一个根，$f(\\hat{\\sigma}^2)=0$。\n\n数值算法如下：\n1.  对于给定的测试用例，构建核矩阵 $K$。\n2.  对于一个小的正值，例如 $\\sigma^2 = 10^{-9}$，数值评估 $f(\\sigma^2)$ 的符号，以近似边界处的行为。\n3.  如果 $f(10^{-9}) \\le 0$，则 MLE 为 $\\hat{\\sigma}^2 = 0$。\n4.  如果 $f(10^{-9}) > 0$，则存在一个内部根。我们可以使用像 `scipy.optimize.brentq` 这样的数值求解器来找到这个根。这需要对根进行区间限定。我们知道 $f(\\sigma^2)$ 在零附近是正的。对于大的 $\\sigma^2$，项 $-\\text{tr}((K + \\sigma^2 I)^{-1}) \\approx -n/\\sigma^2$ 主导项 $y^T (K + \\sigma^2 I)^{-2} y \\approx (y^T y)/\\sigma^4$，所以 $f(\\sigma^2)$ 变为负值。这保证了可以找到一个括号区间 $[a, b]$，使得 $f(a) > 0$ 且 $f(b)  < 0$，从而让求根器收敛到唯一的内部最大值。\n\n下面的 Python 代码实现了这一逻辑。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes the Maximum Likelihood Estimate (MLE) for the noise variance sigma^2\n    in a Gaussian Process regression model for several test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"x\": np.array([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"y\": np.array([0.05, 0.6, 0.9, 0.6, 0.0]),\n            \"alpha_sq\": 1.2,\n            \"ell\": 0.3\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.4, 0.8, 1.2]),\n            \"y\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"alpha_sq\": 0.8,\n            \"ell\": 0.5\n        },\n        {\n            \"x\": np.array([0.0, 0.2, 0.5, 0.9, 1.4]),\n            \"y\": np.array([1.0, 0.9, 0.4, -0.2, -0.7]),\n            \"alpha_sq\": 1.5,\n            \"ell\": 0.4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mle_sigma_sq = compute_mle_for_case(\n            case[\"x\"], case[\"y\"], case[\"alpha_sq\"], case[\"ell\"]\n        )\n        results.append(f\"{mle_sigma_sq:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef build_se_kernel(x, alpha_sq, ell):\n    \"\"\"Constructs the Squared Exponential kernel matrix K.\"\"\"\n    n = len(x)\n    K = np.zeros((n, n))\n    l_sq = ell**2\n    for i in range(n):\n        for j in range(n):\n            dist_sq = (x[i] - x[j])**2\n            K[i, j] = alpha_sq * np.exp(-0.5 * dist_sq / l_sq)\n    return K\n\ndef foc_function(sigma_sq, K, y_col):\n    \"\"\"\n    Computes the value of the First-Order Condition (FOC) function for a given sigma^2.\n    The function is f(sigma^2) = y^T(K+sigma^2*I)^(-2)y - tr((K+sigma^2*I)^(-1)).\n    The MLE is a root of this function.\n    \n    Args:\n        sigma_sq (float): The noise variance parameter sigma^2.\n        K (np.ndarray): The signal covariance matrix.\n        y_col (np.ndarray): The observation vector (as a column vector).\n\n    Returns:\n        float: The value of the FOC function.\n    \"\"\"\n    if sigma_sq  0:\n        return np.inf  # Enforce non-negativity constraint.\n    \n    n = K.shape[0]\n    I = np.eye(n)\n    K_y = K + sigma_sq * I\n    \n    try:\n        inv_K_y = np.linalg.inv(K_y)\n    except np.linalg.LinAlgError:\n        # If matrix is singular, return a large value to repel the solver.\n        return np.inf\n        \n    # Efficient computation of the quadratic form term\n    # alpha = inv(K_y) @ y\n    # y.T @ inv(K_y)^2 @ y = y.T @ inv(K_y).T @ inv(K_y) @ y = (inv(K_y) @ y).T @ (inv(K_y) @ y)\n    alpha = inv_K_y @ y_col\n    quadratic_term = alpha.T @ alpha\n    \n    # Trace term\n    trace_term = np.trace(inv_K_y)\n    \n    return float(quadratic_term - trace_term)\n\ndef compute_mle_for_case(x, y, alpha_sq, ell):\n    \"\"\"\n    Determines the MLE of sigma^2 for a single case.\n    \n    It first checks the derivative of the log-likelihood at the boundary (sigma^2=0).\n    If it's non-positive, the maximum is at the boundary (sigma^2=0).\n    If it's positive, an interior maximum exists and is found using a numerical root-finder.\n    \"\"\"\n    K = build_se_kernel(x, alpha_sq, ell)\n    y_col = y.reshape(-1, 1)\n\n    # The sign of the FOC function f(sigma^2) is the sign of the derivative of the log-likelihood.\n    # Check the sign at a small positive value to determine behavior at the boundary.\n    epsilon = 1e-9\n    \n    try:\n        foc_at_zero = foc_function(epsilon, K, y_col)\n    except np.linalg.LinAlgError:\n        # This handles the rare case where K itself is singular.\n        # This implies likelihood at sigma^2=0 is -inf, so derivative must be positive.\n        foc_at_zero = np.inf\n\n    # If the derivative is non-positive at the boundary, max is at sigma^2 = 0.\n    if foc_at_zero = 0:\n        return 0.0\n    else:\n        # Interior solution exists. Find a bracketing interval for the root-finder.\n        # The FOC function is positive near zero and becomes negative for large sigma^2.\n        lower_bound = epsilon\n        upper_bound = 1.0\n        \n        # Search for an upper bound where the function is negative.\n        for _ in range(10): # Limit iterations to prevent infinite loop\n            if foc_function(upper_bound, K, y_col)  0:\n                break\n            upper_bound *= 10\n        else:\n            # This should not be reached in a well-behaved problem.\n            # It indicates the derivative doesn't become negative within a reasonable range.\n            # Returning a sentinel value would be appropriate in a general library,\n            # but for this specific problem, the root is expected to be found.\n            # Assume it will be found for these test cases.\n            pass\n\n        # Use Brent's method to find the root in the established bracket.\n        mle_sigma_sq = brentq(foc_function, lower_bound, upper_bound, args=(K, y_col))\n        return mle_sigma_sq\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3423961"}]}