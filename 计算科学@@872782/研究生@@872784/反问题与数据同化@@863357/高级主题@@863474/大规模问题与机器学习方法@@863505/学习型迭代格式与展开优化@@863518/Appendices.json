{"hands_on_practices": [{"introduction": "我们首先通过一个实践来直接比较经典的优化算法（如近端梯度下降法）和一个简单的学习型展开方案。这个练习将让我们具体分析迭代方法的一个关键属性：稳定性。通过计算迭代矩阵的谱半径，我们可以判断这些方案是否收敛到一个不动点，这为理解和调试学习模型提供了一个基本工具。[@problem_id:3396250]", "problem": "考虑一个带有加性噪声的线性逆问题，其观测数据建模为 $y = A x + \\epsilon$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是已知的系统矩阵，$x \\in \\mathbb{R}^{n}$ 是未知状态，$\\epsilon \\in \\mathbb{R}^{n}$ 是加性噪声。为了恢复 $x$，通常会最小化数据失配项和正则化项之和，定义一个如下形式的目标函数：\n$$\nF(x) = \\frac{1}{2} \\| A x - y \\|_2^2 + \\lambda R(x),\n$$\n其中 $\\lambda > 0$ 是正则化参数，$R(x)$ 是一个惩罚项。在本问题中，惩罚项是二次的，由下式给出：\n$$\nR(x) = \\frac{1}{2} \\| L x \\|_2^2,\n$$\n其中 $L$ 是一个已知的线性算子。\n\n邻近梯度下降（Proximal Gradient Descent, PGD）方法根据以下规则生成迭代序列：\n$$\nx^{k+1} = \\operatorname{prox}_{\\lambda R}\\!\\left( x^k - \\tau A^\\top (A x^k - y) \\right),\n$$\n其中 $\\tau > 0$ 是步长，$\\operatorname{prox}_{\\lambda R}$ 表示与 $\\lambda R$ 相关联的邻近算子。一个学习型展开方案将邻近算子替换为一个经过训练的学习算子，例如，训练该算子以近似邻近映射的作用。在本问题中，假定学习到的邻近算子是线性的，并用矩阵 $M \\in \\mathbb{R}^{n \\times n}$ 表示，因此学习型更新规则为：\n$$\nx^{k+1} = M \\left( x^k - \\tau A^\\top (A x^k - y) \\right).\n$$\n\n你的任务是：\n- 从第一性原理和上述核心定义出发，推导带有二次惩罚项的经典PGD方法和带有线性训练邻近算子的学习型展开方案的线性更新算子。将每次更新表示为仿射映射 $x^{k+1} = G x^k + c$，并根据 $G$ 的谱给出不动点存在且稳定的条件。\n- 对于下面提供的每个测试用例，计算每个更新算子的特征值、它们的模、谱半径，并判断不动点是否线性稳定。当且仅当谱半径严格小于 $1$ 时，不动点被认为是线性稳定的。\n\n使用以下参数值测试套件。在所有情况下，$n = 3$，范数 $\\| A \\|_2$ 表示 $A$ 的谱范数。\n\n案例 $\\mathbf{1}$ (良态，非扩张性学习型邻近算子):\n- $$A_1 = \\begin{bmatrix} 1  0.2  0 \\\\ 0.2  0.8  0.1 \\\\ 0  0.1  0.5 \\end{bmatrix}, \\quad L_1 = I_3, \\quad \\lambda_1 = 0.5, \\quad M_1 = 0.9 I_3,$$\n- $$\\tau_1 = 0.9 \\cdot \\frac{2}{\\|A_1\\|_2^2}.$$\n\n案例 $\\mathbf{2}$ (接近阈值的步长，扩张性学习型邻近算子):\n- $$A_2 = \\begin{bmatrix} 1  0.3  0 \\\\ 0.3  0.7  0.2 \\\\ 0  0.2  0.4 \\end{bmatrix}, \\quad L_2 = I_3, \\quad \\lambda_2 = 0.4, \\quad M_2 = 1.1 I_3,$$\n- $$\\tau_2 = 0.99 \\cdot \\frac{2}{\\|A_2\\|_2^2}.$$\n\n案例 $\\mathbf{3}$ (病态系统，精确的经典邻近算子):\n- $$A_3 = \\begin{bmatrix} 1.5  -0.9  0 \\\\ -0.9  0.6  0.3 \\\\ 0  0.3  0.2 \\end{bmatrix}, \\quad L_3 = \\begin{bmatrix} -1  1  0 \\\\ 0  -1  1 \\end{bmatrix}, \\quad \\lambda_3 = 0.8,$$\n- 对于学习型邻近算子，设置 $$M_3 = \\left(I_3 + \\lambda_3 L_3^\\top L_3 \\right)^{-1},$$\n- $$\\tau_3 = 0.7 \\cdot \\frac{2}{\\|A_3\\|_2^2}.$$\n\n案例 $\\mathbf{4}$ (无正则化，精确阈值，临界经典稳定性):\n- $$A_4 = \\begin{bmatrix} 0.9  0.2  0.1 \\\\ 0.2  0.5  0.05 \\\\ 0.1  0.05  0.3 \\end{bmatrix}, \\quad L_4 = 0_{3 \\times 3}, \\quad \\lambda_4 = 0.5, \\quad M_4 = 0.95 I_3,$$\n- $$\\tau_4 = 1.0 \\cdot \\frac{2}{\\|A_4\\|_2^2}.$$\n\n对于每个案例，构建经典PGD更新矩阵\n$$\nG_{\\text{classical}} = S \\left( I_3 - \\tau A^\\top A \\right), \\quad \\text{其中} \\quad S = \\left( I_3 + \\lambda L^\\top L \\right)^{-1},\n$$\n和学习型更新矩阵\n$$\nG_{\\text{learned}} = M \\left( I_3 - \\tau A^\\top A \\right).\n$$\n\n对于每个 $G_{\\text{classical}}$ 和 $G_{\\text{learned}}$，计算：\n- 按降序排列的特征值模长列表。\n- 谱半径，定义为特征值中的最大模长。\n- 一个指示不动点根据谱半径条件是否线性稳定的布尔值。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含所有用例的结果，形式为用方括号括起来的逗号分隔列表。每个用例的结果必须是以下形式的列表\n$$\n[\\text{eig\\_mags\\_classical}, \\text{eig\\_mags\\_learned}, \\rho_{\\text{classical}}, \\rho_{\\text{learned}}, \\text{stable}_{\\text{classical}}, \\text{stable}_{\\text{learned}}],\n$$\n其中 $\\text{eig\\_mags\\_classical}$ 和 $\\text{eig\\_mags\\_learned}$ 是浮点数列表。例如，整个输出应如下所示\n$$\n[[\\ldots],[\\ldots],\\ldots].\n$$\n不应打印任何额外文本。不涉及角度，也不需要物理单位。所有数值都应根据需要提供为浮点数或布尔值。", "solution": "该问题要求对求解线性逆问题的两种迭代方案进行推导和稳定性分析：经典的邻近梯度下降（PGD）和一种学习型展开变体。\n\n### 更新算子的推导\n\n如果一个形如 $x^{k+1} = f(x^k)$ 的迭代方案可以表示为 $x^{k+1} = G x^k + c$（其中 $G$ 为常数矩阵，$c$ 为常数向量），那么它就是一个仿射映射。我们将为经典方法和学习方法推导这种形式。\n\n#### 1. 经典邻近梯度下降（PGD）\n\nPGD 更新规则由下式给出：\n$$\nx^{k+1} = \\operatorname{prox}_{\\lambda R}\\!\\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\n令 $z^k = x^k - \\tau A^\\top (A x^k - y)$。则更新为 $x^{k+1} = \\operatorname{prox}_{\\lambda R}(z^k)$。\n\n邻近算子 $\\operatorname{prox}_{\\gamma g}(v)$ 定义为函数 $u \\mapsto g(u) + \\frac{1}{2\\gamma} \\|u - v\\|_2^2$ 的唯一最小化子。在我们的例子中，要最小化的函数是 $J(u)$，其中 $\\gamma=1$ 且 $g(u) = \\lambda R(u) = \\frac{\\lambda}{2} \\|Lu\\|_2^2$：\n$$\n\\operatorname{prox}_{\\lambda R}(z^k) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left( \\frac{\\lambda}{2} \\|Lu\\|_2^2 + \\frac{1}{2} \\|u - z^k\\|_2^2 \\right)\n$$\n这是一个二次严格凸优化问题，因此存在唯一的最小化子。我们通过将关于 $u$ 的梯度设为零来找到它：\n$$\n\\nabla_u J(u) = \\nabla_u \\left( \\frac{\\lambda}{2} u^\\top L^\\top L u + \\frac{1}{2} (u - z^k)^\\top (u - z^k) \\right) = 0\n$$\n$$\n\\lambda L^\\top L u + (u - z^k) = 0\n$$\n对 $u$ 进行整理：\n$$\n(\\lambda L^\\top L + I) u = z^k\n$$\n其中 $I$ 是单位矩阵。由于 $L^\\top L$ 是半正定的，对于 $\\lambda > 0$，矩阵 $(I + \\lambda L^\\top L)$ 是正定的，因此它是可逆的。解是：\n$$\nu = (I + \\lambda L^\\top L)^{-1} z^k\n$$\n我们定义矩阵 $S = (I + \\lambda L^\\top L)^{-1}$。因此，邻近算子是一个线性变换：$\\operatorname{prox}_{\\lambda R}(z^k) = S z^k$。\n\n现在，我们将其代回 PGD 更新方程中：\n$$\nx^{k+1} = S z^k = S \\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\n分配各项以分离 $x^k$ 和 $y$：\n$$\nx^{k+1} = S \\left( x^k - \\tau A^\\top A x^k + \\tau A^\\top y \\right)\n$$\n$$\nx^{k+1} = S (I - \\tau A^\\top A) x^k + S \\tau A^\\top y\n$$\n这是一个仿射映射 $x^{k+1} = G_{\\text{classical}} x^k + c_{\\text{classical}}$，其更新矩阵和常数向量由下式给出：\n$$\nG_{\\text{classical}} = S (I - \\tau A^\\top A) = (I + \\lambda L^\\top L)^{-1} (I - \\tau A^\\top A)\n$$\n$$\nc_{\\text{classical}} = S \\tau A^\\top y = (I + \\lambda L^\\top L)^{-1} \\tau A^\\top y\n$$\n$G_{\\text{classical}}$ 的表达式与问题描述中提供的一致。\n\n#### 2. 学习型展开方案\n\n学习型方案的更新规则直接给出如下：\n$$\nx^{k+1} = M \\left( x^k - \\tau A^\\top (A x^k - y) \\right)\n$$\n其中 $M$ 是一个学习到的矩阵。通过分配 $M$，我们可以立即识别出仿射形式：\n$$\nx^{k+1} = M \\left( (I - \\tau A^\\top A) x^k + \\tau A^\\top y \\right)\n$$\n$$\nx^{k+1} = M (I - \\tau A^\\top A) x^k + M \\tau A^\\top y\n$$\n这是一个仿射映射 $x^{k+1} = G_{\\text{learned}} x^k + c_{\\text{learned}}$，其中：\n$$\nG_{\\text{learned}} = M (I - \\tau A^\\top A)\n$$\n$$\nc_{\\text{learned}} = M \\tau A^\\top y\n$$\n$G_{\\text{learned}}$ 的表达式也与问题描述中提供的一致。\n\n### 不动点稳定性分析\n\n对于任意初始猜测 $x^0$，迭代方法 $x^{k+1} = G x^k + c$ 收敛到一个唯一不动点 $x^*$ 的充分必要条件是迭代算子 $G$ 是一个压缩映射。不动点 $x^*$ 满足方程 $x^* = G x^* + c$。\n\n为了分析线性稳定性，我们考虑误差的演化，$e^k = x^k - x^*$。从更新方程中减去不动点方程得到：\n$$\nx^{k+1} - x^* = (G x^k + c) - (G x^* + c) = G (x^k - x^*)\n$$\n$$\ne^{k+1} = G e^k\n$$\n这意味着 $e^k = G^k e^0$。对于任意初始误差 $e^0$，误差 $e^k$ 收敛到零的充分必要条件是，当 $k \\to \\infty$ 时，矩阵的幂 $G^k$ 收敛到零矩阵。此条件被满足的充分必要条件是 $G$ 的谱半径（记作 $\\rho(G)$）严格小于 $1$。谱半径定义为 $G$ 的特征值模长的最大值：\n$$\n\\rho(G) = \\max_{i} |\\mu_i|\n$$\n其中 $\\mu_i$ 是 $G$ 的特征值。\n\n因此，当且仅当 $\\rho(G)  1$ 时，迭代方案的不动点是线性稳定的。每个案例的数值计算将基于构建各自的 $G$ 矩阵并评估它们的谱半径。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the stability of classical and learned proximal\n    gradient descent for four test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[1, 0.2, 0], [0.2, 0.8, 0.1], [0, 0.1, 0.5]]),\n            \"L\": np.eye(3),\n            \"lambda_val\": 0.5,\n            \"M\": 0.9 * np.eye(3),\n            \"tau_prefactor\": 0.9,\n        },\n        {\n            \"A\": np.array([[1, 0.3, 0], [0.3, 0.7, 0.2], [0, 0.2, 0.4]]),\n            \"L\": np.eye(3),\n            \"lambda_val\": 0.4,\n            \"M\": 1.1 * np.eye(3),\n            \"tau_prefactor\": 0.99,\n        },\n        {\n            \"A\": np.array([[1.5, -0.9, 0], [-0.9, 0.6, 0.3], [0, 0.3, 0.2]]),\n            \"L\": np.array([[-1, 1, 0], [0, -1, 1]]),\n            \"lambda_val\": 0.8,\n            \"M\": None,  # M is derived from L and lambda\n            \"tau_prefactor\": 0.7,\n        },\n        {\n            \"A\": np.array([[0.9, 0.2, 0.1], [0.2, 0.5, 0.05], [0.1, 0.05, 0.3]]),\n            \"L\": np.zeros((3, 3)),\n            \"lambda_val\": 0.5,\n            \"M\": 0.95 * np.eye(3),\n            \"tau_prefactor\": 1.0,\n        },\n    ]\n\n    results_for_all_cases = []\n\n    for i, case in enumerate(test_cases):\n        A = case[\"A\"]\n        L = case[\"L\"]\n        lambda_val = case[\"lambda_val\"]\n        tau_prefactor = case[\"tau_prefactor\"]\n        n = A.shape[0]\n        I_n = np.eye(n)\n\n        # Calculate step size tau\n        norm_A_sq = np.linalg.norm(A, 2)**2\n        tau = tau_prefactor * (2 / norm_A_sq)\n\n        # Common term in both G matrices\n        common_term = I_n - tau * A.T @ A\n        \n        # --- Classical PGD ---\n        # Construct S = (I + lambda * L^T * L)^-1\n        L_T_L = L.T @ L\n        S = np.linalg.inv(I_n + lambda_val * L_T_L)\n        \n        # Construct G_classical\n        G_classical = S @ common_term\n        \n        # Analyze G_classical\n        eigvals_c = np.linalg.eigvals(G_classical)\n        mags_c = np.abs(eigvals_c)\n        mags_c_sorted = -np.sort(-mags_c) # Sort descending\n        rho_c = mags_c_sorted[0]\n        stable_c = rho_c  1.0\n\n        # --- Learned Scheme ---\n        # Construct M\n        if i == 2:  # Case 3\n            M = S\n        else:\n            M = case[\"M\"]\n        \n        # Construct G_learned\n        G_learned = M @ common_term\n        \n        # Analyze G_learned\n        eigvals_l = np.linalg.eigvals(G_learned)\n        mags_l = np.abs(eigvals_l)\n        mags_l_sorted = -np.sort(-mags_l) # Sort descending\n        rho_l = mags_l_sorted[0]\n        stable_l = rho_l  1.0\n        \n        # Format the output for the current case\n        mags_c_str = f\"[{','.join([f'{m:.8f}' for m in mags_c_sorted])}]\"\n        mags_l_str = f\"[{','.join([f'{m:.8f}' for m in mags_l_sorted])}]\"\n        \n        current_case_str = (\n            f\"[{mags_c_str},\"\n            f\"{mags_l_str},\"\n            f\"{rho_c:.8f},\"\n            f\"{rho_l:.8f},\"\n            f\"{str(stable_c).lower()},\"\n            f\"{str(stable_l).lower()}]\"\n        )\n        results_for_all_cases.append(current_case_str)\n        \n    print(f\"[{','.join(results_for_all_cases)}]\")\n\nsolve()\n```", "id": "3396250"}, {"introduction": "许多现实世界中的反问题都包含物理或逻辑约束，例如解的非负性或边界。本实践将演示如何使用可微障碍函数将此类不等式约束融入学习型迭代方案中。你将实现一个学习到的近端映射，它能“软性”地强制解的可行性，这是构建物理信息神经网络的一种强大技术。[@problem_id:3396268]", "problem": "请考虑在展开优化（unrolled optimization）设置下的带线性不等式约束的等式约束二次逆问题。任务是设计并分析一个学习的近端映射（learned proximal mapping），该映射在固定深度的展开方案中使用可微障碍（differentiable barrier）来强制施加不等式约束，并评估在迭代过程中可行性（feasibility）的保持情况。\n\n从以下基础出发：\n- 线性最小二乘数据拟合的目标由函数 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$ 表示，其中 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 是一个测量算子，$\\mathbf{y} \\in \\mathbb{R}^m$ 是一个给定的数据向量。\n- 约束集由 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 定义，其中 $\\mathbf{C} \\in \\mathbb{R}^{p \\times n}$ 且 $\\mathbf{d} \\in \\mathbb{R}^p$。\n- 使用一个可微障碍来软性强制施加不等式约束。对于标量参数 $u$，一个平滑障碍由 $b_\\tau(u) = \\tau \\log\\big(1 + \\exp(u/\\tau)\\big)$ 给出，其温度参数 $\\tau  0$。该函数处处可微，并且当 $\\tau \\to 0$ 时近似于正部函数。\n\n您必须设计一个固定深度为 $K$ 的展开迭代方案，该方案首先对数据保真项执行梯度下降步骤，然后应用一个学习的近端映射，该映射结合了障碍以抑制对 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 的违反。具体来说，在每次迭代 $k$ 中：\n- 使用步长 $\\alpha  0$ 对 $f(\\mathbf{x})$ 进行梯度下降更新，以获得一个中间点 $\\mathbf{z}^{(k)}$。\n- 将一个由 $\\boldsymbol{\\theta}$ 参数化的学习近端映射 $P_{\\boldsymbol{\\theta}}$ 应用于 $\\mathbf{z}^{(k)}$，以获得旨在强制执行约束的 $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$。\n\n学习的近端映射 $P_{\\boldsymbol{\\theta}}$ 的构造应使其与通过链式法则沿与线性约束相关的障碍函数下降保持一致。您必须实现一个仅使用线性算子和逐元素平滑函数的映射，并且其参数 $\\boldsymbol{\\theta}$ 在展开迭代中被视为固定的学习值。初始迭代点必须是 $\\mathbf{x}^{(0)} = \\mathbf{0}$。\n\n将第 $k$ 次迭代的可行性定义为 $\\mathbf{C}\\mathbf{x}^{(k)} - \\mathbf{d}$ 的所有分量都小于或等于一个容差 $\\varepsilon  0$ 的条件。对于一个给定的测试用例，跨迭代的可行性保持是一个布尔值，当且仅当对于 $k = 1, 2, \\ldots, K$ 的每个迭代点 $\\mathbf{x}^{(k)}$ 都根据此容差是可行的时，该值为真。此外，将最终最大违规量定义为非负标量 $\\max\\big\\{0, \\max_i\\big((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i\\big)\\big\\}$。\n\n您的程序必须实现上述展开方案，并为每个测试用例生成一个结果，该结果包含可行性保持布尔值和最终最大违规量。将所有测试用例的结果汇总到单行输出中，该输出包含一个用方括号括起来的逗号分隔列表，其中每个测试用例的结果都是 $[\\text{boolean}, \\text{float}]$ 形式的双元素列表。\n\n使用以下测试套件，其中所有矩阵和向量都明确指定。在所有情况下，维度为 $n = m = 3$，约束数量 $p$ 不定，迭代次数为 $K = 25$，可行性容差为 $\\varepsilon = 10^{-8}$。\n\n- 测试用例 1（理想路径）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$，$\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\mathbf{I}_3$，$\\mathbf{d} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.15$，学习的近端参数 $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau) = (0.35, 1.0, 0.1)$。\n\n- 测试用例 2（更紧的箱式约束）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$，$\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\mathbf{I}_3$，$\\mathbf{d} = \\begin{bmatrix} 0.2 \\\\ 0.2 \\\\ 0.05 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.15$，学习的近端参数 $\\boldsymbol{\\theta} = (0.5, 2.0, 0.08)$。\n\n- 测试用例 3（混合约束，包括一个和约束）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$，$\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$，$\\mathbf{d} = \\begin{bmatrix} 0.4 \\\\ 0.4 \\\\ 0.4 \\\\ 0.6 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.2$，学习的近端参数 $\\boldsymbol{\\theta} = (0.4, 1.5, 0.07)$。\n\n- 测试用例 4（激进的数据步）：\n  - $\\mathbf{A} = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.2  1.5  0.1 \\\\ 0.0  0.1  2.0 \\end{bmatrix}$，$\\mathbf{y} = \\begin{bmatrix} 0.4 \\\\ 0.6 \\\\ -0.1 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\mathbf{I}_3$，$\\mathbf{d} = \\begin{bmatrix} 0.3 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.65$，学习的近端参数 $\\boldsymbol{\\theta} = (0.45, 2.0, 0.05)$。\n\n- 测试用例 5（病态前向算子与单个聚合约束）：\n  - $\\mathbf{A} = \\begin{bmatrix} 3.0  2.5  0.0 \\\\ 2.5  2.8  1.2 \\\\ 0.0  1.2  0.5 \\end{bmatrix}$，$\\mathbf{y} = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}$。\n  - $\\mathbf{C} = \\begin{bmatrix} 1  1  1 \\end{bmatrix}$，$\\mathbf{d} = \\begin{bmatrix} 0.7 \\end{bmatrix}$。\n  - 步长 $\\alpha = 0.3$，学习的近端参数 $\\boldsymbol{\\theta} = (0.6, 3.0, 0.1)$。\n\n程序必须：\n- 实现具有 $K = 25$ 次迭代和可行性容差 $\\varepsilon = 10^{-8}$ 的展开方案。\n- 对于每个测试用例，计算序列 $\\{\\mathbf{x}^{(k)}\\}_{k=1}^{K}$ 并评估可行性保持布尔值和最终最大违规量。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，其中每个内部测试用例的结果本身就是一个 $[\\text{boolean}, \\text{float}]$ 形式的列表。例如，一个有效的输出格式是 $[[\\text{True}, 0.0],[\\text{False}, 0.1234],\\ldots]$。\n\n本问题不涉及物理单位、角度单位或百分比单位。所有量均为无量纲实数。", "solution": "用户提供了一个问题，要求为约束逆问题设计和实现一个学习的、展开的优化方案。解决方案必须按照特定格式呈现，从问题验证开始。\n\n### 问题验证\n\n**第1步：提取已知信息**\n\n- **目标函数**：$f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$，其中 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 且 $\\mathbf{y} \\in \\mathbb{R}^m$。\n- **不等式约束**：$\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$，其中 $\\mathbf{C} \\in \\mathbb{R}^{p \\times n}$ 且 $\\mathbf{d} \\in \\mathbb{R}^p$。\n- **平滑障碍函数**：对于标量参数 $u$，障碍为 $b_\\tau(u) = \\tau \\log\\big(1 + \\exp(u/\\tau)\\big)$，温度参数为 $\\tau  0$。\n- **展开迭代方案**：一个固定深度为 $K$ 次迭代的方案。\n  - 初始迭代点：$\\mathbf{x}^{(0)} = \\mathbf{0}$。\n  - 对于 $k = 0, \\dots, K-1$：\n    1.  **梯度下降更新**：计算一个中间点 $\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)})$，步长为 $\\alpha  0$。\n    2.  **学习的近端映射**：应用一个映射 $P_{\\boldsymbol{\\theta}}$ 以得到 $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$。\n- **近端映射规范**：$P_{\\boldsymbol{\\theta}}$ 由 $\\boldsymbol{\\theta}$ 参数化，与通过链式法则沿与约束相关的障碍函数下降一致，并且只使用线性算子和逐元素平滑函数。\n- **可行性度量**：\n  - **第 k 次迭代的可行性**：$\\mathbf{C}\\mathbf{x}^{(k)} - \\mathbf{d}$ 的所有分量都小于或等于一个容差 $\\varepsilon  0$。\n  - **可行性保持**：一个布尔值，当且仅当对于 $k = 1, 2, \\ldots, K$ 的每个迭代点 $\\mathbf{x}^{(k)}$ 都是可行的时，该值为真。\n  - **最终最大违规量**：非负标量 $\\max\\big\\{0, \\max_i\\big((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i\\big)\\big\\}$。\n- **全局参数**：$n = m = 3$, $K = 25$, $\\varepsilon = 10^{-8}$。\n- **测试用例**：提供了五个测试用例，每个都指定了矩阵 $\\mathbf{A}$、$\\mathbf{C}$，向量 $\\mathbf{y}$、$\\mathbf{d}$，以及参数 $\\alpha$ 和 $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau)$。\n\n**第2步：使用提取的已知信息进行验证**\n\n1.  **科学依据**：该问题在数值优化、逆问题和机器学习领域有坚实的基础。最小二乘法、线性约束、障碍方法（特别是 softplus 函数）以及展开的基于梯度的方案都是标准且严谨的概念。\n2.  **适定性**：该问题是适定的。它提供了所有必要的数学定义、算法步骤和数值数据，以得出一个唯一的计算结果。对学习的近端映射 $P_{\\boldsymbol{\\theta}}$ 的描述有足够的约束，以推导出一个具体的、可形式化的结构（对障碍惩罚项的梯度下降步骤）。\n3.  **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观声明。\n4.  **完整性与一致性**：问题陈述是自包含且内部一致的。每个测试用例中矩阵和向量的维度都是兼容的。\n5.  **相关性与可形式化性**：该问题与指定的主题“学习的迭代方案和展开优化”直接相关，并且完全可以形式化为一个计算算法。\n\n**第3步：结论与行动**\n\n该问题被判定为**有效**。将提供一个完整、合理的解决方案。\n\n### 基于原则的解决方案设计\n\n该问题要求构建一个展开的迭代算法来解决一个约束优化问题。总体目标是在约束 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 下最小化数据保真项 $f(\\mathbf{x})$。展开方案交替执行最小化数据误差的步骤和强制执行约束的步骤。\n\n**1. 展开迭代方案**\n\n该算法从一个初始估计 $\\mathbf{x}^{(0)} = \\mathbf{0}$ 开始，并对其进行固定次数 $K$ 的迭代优化。每次迭代 $k$（从 $k=0$ 到 $k=K-1$）包括两个主要步骤：\n\n**步骤 A：数据保真度更新**\n\n此步骤对数据保真度目标函数 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x} - \\mathbf{y}\\|_2^2$ 执行梯度下降。此函数关于 $\\mathbf{x}$ 的梯度是：\n$$\n\\nabla f(\\mathbf{x}) = \\mathbf{A}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{y})\n$$\n从当前迭代点 $\\mathbf{x}^{(k)}$ 开始，此步骤的更新规则是：\n$$\n\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\nabla f(\\mathbf{x}^{(k)}) = \\mathbf{x}^{(k)} - \\alpha \\mathbf{A}^T(\\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{y})\n$$\n其中 $\\alpha  0$ 是步长。所得向量 $\\mathbf{z}^{(k)}$ 是一个中间估计，它更接近于最小化数据失配，但可能违反约束。\n\n**步骤 B：通过学习的近端映射强制执行约束**\n\n此步骤 $\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)})$ 旨在将中间估计 $\\mathbf{z}^{(k)}$ 拉回到可行域。约束 $\\mathbf{C}\\mathbf{x} \\le \\mathbf{d}$ 等价于对所有 $i$ 都有 $(\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i \\le 0$。当 $(\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i  0$ 时，发生违规。\n\n我们使用提供的平滑障碍函数 $b_\\tau(u) = \\tau \\log(1 + \\exp(u/\\tau))$ 来惩罚此类违规。此函数近似于正部函数 $\\max(0, u)$。对于给定的 $\\mathbf{x}$，总障碍惩罚是每个约束的惩罚之和，并由参数 $\\beta$ 缩放：\n$$\nB(\\mathbf{x}) = \\sum_{i=1}^{p} \\beta \\cdot b_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big)\n$$\n近端映射 $P_{\\boldsymbol{\\theta}}$ 被构造为从此障碍函数上的梯度下降步骤，从点 $\\mathbf{z}^{(k)}$ 开始。首先，我们推导 $B(\\mathbf{x})$ 的梯度。障碍函数的导数是 sigmoid 函数：\n$$\nb'_\\tau(u) = \\frac{d}{du} \\left[\\tau \\log(1+e^{u/\\tau})\\right] = \\frac{1}{1+e^{-u/\\tau}} =: \\text{sigm}_\\tau(u)\n$$\n使用链式法则，总障碍惩罚 $B(\\mathbf{x})$ 的梯度是：\n$$\n\\nabla B(\\mathbf{x}) = \\sum_{i=1}^{p} \\beta \\cdot b'_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big) \\cdot \\nabla_x\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big) = \\beta \\mathbf{C}^T \\mathbf{s}(\\mathbf{x})\n$$\n其中 $\\mathbf{s}(\\mathbf{x})$ 是一个向量，其第 $i$ 个分量为 $s_i(\\mathbf{x}) = \\text{sigm}_\\tau\\big((\\mathbf{C}\\mathbf{x} - \\mathbf{d})_i\\big)$。\n\n学习的近端映射 $P_{\\boldsymbol{\\theta}}$ 在障碍惩罚上执行单步梯度下降，步长为 $\\sigma$，从点 $\\mathbf{z}^{(k)}$ 开始：\n$$\n\\mathbf{x}^{(k+1)} = P_{\\boldsymbol{\\theta}}(\\mathbf{z}^{(k)}) = \\mathbf{z}^{(k)} - \\sigma \\nabla_z B(\\mathbf{z}^{(k)})\n$$\n代入梯度的表达式，我们得到此步骤的最终更新规则：\n$$\n\\mathbf{x}^{(k+1)} = \\mathbf{z}^{(k)} - \\sigma \\beta \\mathbf{C}^T \\text{sigm}_\\tau(\\mathbf{C}\\mathbf{z}^{(k)} - \\mathbf{d})\n$$\n参数 $\\boldsymbol{\\theta} = (\\sigma, \\beta, \\tau)$ 被视为固定的“学习”值，它们控制约束执行步骤的行为。$\\sigma$ 控制步长，$\\beta$ 缩放障碍的影响，$\\tau$ 控制 soft-plus 障碍的锐度。这个构造满足了问题中仅使用线性算子（$\\mathbf{C}$、$\\mathbf{C}^T$）和逐元素平滑函数（$\\text{sigm}_\\tau$）的要求。\n\n**2. 算法总结与评估**\n\n完整的算法展开如下：\n1.  初始化 $\\mathbf{x}^{(0)} = \\mathbf{0}$。\n2.  对于 $k = 0, 1, \\dots, K-1$：\n    a. 计算 $\\mathbf{z}^{(k)} = \\mathbf{x}^{(k)} - \\alpha \\mathbf{A}^T(\\mathbf{A}\\mathbf{x}^{(k)} - \\mathbf{y})$。\n    b. 计算 $\\mathbf{x}^{(k+1)} = \\mathbf{z}^{(k)} - \\sigma \\beta \\mathbf{C}^T \\text{sigm}_\\tau(\\mathbf{C}\\mathbf{z}^{(k)} - \\mathbf{d})$。\n    c. 检查可行性：验证 $\\mathbf{C}\\mathbf{x}^{(k+1)} - \\mathbf{d}$ 的所有分量是否 $\\le \\varepsilon$。\n3.  在 $K$ 次迭代后，从 $\\mathbf{x}^{(K)}$ 计算最终最大违规量。\n\n当且仅当步骤 2.c 中的检查对所有 $k=0, \\dots, K-1$ 都通过时，可行性保持布尔值为真。最终最大违规量计算为 $\\max\\{0, \\max_i ((\\mathbf{C}\\mathbf{x}^{(K)} - \\mathbf{d})_i)\\}$。这为最终迭代点满足约束的程度提供了一个定量的度量。", "answer": "```python\nimport numpy as np\n\ndef run_unrolled_scheme(A, y, C, d, alpha, theta, K, eps):\n    \"\"\"\n    Implements the unrolled optimization scheme for a given test case.\n\n    Args:\n        A (np.ndarray): Measurement operator.\n        y (np.ndarray): Data vector.\n        C (np.ndarray): Constraint matrix.\n        d (np.ndarray): Constraint vector.\n        alpha (float): Step size for data fidelity term.\n        theta (tuple): Parameters (sigma, beta, tau) for the proximal mapping.\n        K (int): Number of iterations.\n        eps (float): Feasibility tolerance.\n\n    Returns:\n        list: A list containing the feasibility maintenance boolean and the \n              final maximum violation float.\n    \"\"\"\n    sigma, beta, tau = theta\n    n = A.shape[1]\n    x = np.zeros(n)\n    feasibility_maintained = True\n\n    # Note: problem states feasibility for k=1...K. x(0) is always feasible.\n    # The loop runs K times, producing x(1) to x(K).\n    for _ in range(K):\n        # Step 1: Gradient descent on the data fidelity term f(x)\n        grad_f = A.T @ (A @ x - y)\n        z = x - alpha * grad_f\n\n        # Step 2: Apply the learned proximal mapping to enforce constraints\n        # Argument of the sigmoid function\n        u = C @ z - d\n        \n        # Elementwise sigmoid function: 1 / (1 + exp(-u/tau))\n        # Add a small constant to tau to prevent division by zero, though not strictly needed for given test cases\n        s = 1.0 / (1.0 + np.exp(-u / (tau + 1e-12)))\n        \n        # Gradient of the barrier function B(z)\n        grad_b = beta * (C.T @ s)\n        \n        # Update x for the next iteration\n        x_next = z - sigma * grad_b\n        \n        # Check feasibility for the new iterate x^(k+1)\n        violations = C @ x_next - d\n        if np.any(violations > eps):\n            feasibility_maintained = False\n        \n        x = x_next\n        \n    # After all K iterations, calculate the final maximum violation\n    final_violations = C @ x - d\n    final_max_violation = np.maximum(0.0, np.max(final_violations))\n    \n    return [feasibility_maintained, final_max_violation]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the unrolled optimization scheme for each,\n    then prints the results in the specified format.\n    \"\"\"\n    \n    # Common parameters\n    K = 25\n    eps = 1e-8\n    \n    # Test Suite\n    A_common = np.array([[1.0, 0.2, 0.0], [0.2, 1.5, 0.1], [0.0, 0.1, 2.0]])\n    y_common = np.array([0.4, 0.6, -0.1])\n    \n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.5, 0.5, 0.5]),\n            \"alpha\": 0.15, \"theta\": (0.35, 1.0, 0.1)\n        },\n        # Test case 2 (tighter box constraints)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.2, 0.2, 0.05]),\n            \"alpha\": 0.15, \"theta\": (0.5, 2.0, 0.08)\n        },\n        # Test case 3 (mixed constraints)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"d\": np.array([0.4, 0.4, 0.4, 0.6]),\n            \"alpha\": 0.2, \"theta\": (0.4, 1.5, 0.07)\n        },\n        # Test case 4 (aggressive data step)\n        {\n            \"A\": A_common, \"y\": y_common,\n            \"C\": np.eye(3), \"d\": np.array([0.3, 0.3, 0.3]),\n            \"alpha\": 0.65, \"theta\": (0.45, 2.0, 0.05)\n        },\n        # Test case 5 (ill-conditioned operator)\n        {\n            \"A\": np.array([[3.0, 2.5, 0.0], [2.5, 2.8, 1.2], [0.0, 1.2, 0.5]]),\n            \"y\": np.array([2.0, -1.0, 0.5]),\n            \"C\": np.array([[1, 1, 1]]), \"d\": np.array([0.7]),\n            \"alpha\": 0.3, \"theta\": (0.6, 3.0, 0.1)\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_unrolled_scheme(\n            case[\"A\"], case[\"y\"], case[\"C\"], case[\"d\"],\n            case[\"alpha\"], case[\"theta\"], K, eps\n        )\n        results.append(result)\n\n    # Convert a list of lists like [[True, 0.0], [False, 0.123]]\n    # into the string \"[[True,0.0],[False,0.123]]\"\n    result_strings = [\n        f\"[{str(res[0]).lower()},{res[1]:.4e}]\" if isinstance(res[1], float) else str(res) for res in results\n    ]\n    # Re-format to match expected output style more closely\n    output_str = \"[\" + \",\".join([f\"[{str(r[0]).lower()}, {r[1]}]\" for r in results]) + \"]\"\n    print(output_str.replace(\" \", \"\"))\n\nsolve()\n```", "id": "3396268"}, {"introduction": "随着展开网络的层数 $K$ 变得很深，为反向传播存储中间激活所需的内存可能成为一个瓶颈。本练习探讨了一种缓解此问题的关键架构选择：可逆网络。通过推导标准设计和可逆设计的内存与时间复杂度，你将量化内存效率和计算成本之间的基本权衡，这是在有限硬件资源上训练大规模模型时的一个关键考量。[@problem_id:3396297]", "problem": "考虑训练一个$K$层展开的迭代收缩阈值算法 (ISTA) 网络，用于解决线性逆问题 $y = A x + \\varepsilon$。其中，前向算子 $A \\in \\mathbb{R}^{n \\times n}$，潜向量 $x \\in \\mathbb{R}^{n}$ 经过$\\ell_{1}$-正则化以促进稀疏性。在该展开式架构中，每一层都对数据保真项执行一步梯度下降，然后进行一次逐项收缩映射。假设以下科学标准的建模选择和实现惯例，您必须将其用作推导的基础：\n\n- 每一层对当前的稀疏迭代量 $x^{k}$ 执行一次与 $A$ 的乘法，对一个稠密残差向量执行一次与 $A^{\\top}$ 的乘法，然后进行一次逐元素收缩。也就是说，每层的前向计算工作量是以下各项的总和：\n  1) 在 $A x^{k}$ 中访问 $x^{k}$ 的 $s$ 个非零项，\n  2) 对于稠密残差 $r^{k}$，在 $A^{\\top} r^{k}$ 中访问 $n$ 个项，\n  3) $n$ 次标量收缩操作，以及\n  4) 在 $n$ 维数组上进行的线性时间向量加法，您可以将其吸收到 $n$ 项中。\n- 对于所有层 $k=1,\\dots,K$，稀疏迭代量 $x^{k}$ 恰好有 $s$ 个非零项（各层稀疏度恒定），并以压缩稀疏表示法存储。该表示法为每个非零项记录一个浮点值和一个整数索引。在计算内存时，将每个记录的值或索引视为一个单位。\n- 在使用反向传播（backprop）进行训练期间，所有中间层的激活值 $\\{x^{1},\\dots,x^{K}\\}$ 都被保留。此外，峰值时需要 $2$ 个长度为 $n$ 的稠密工作缓冲区（例如，一个残差缓冲区和一个梯度携带缓冲区），您必须将其计入峰值内存。忽略参数内存和与批处理相关的开销；重点关注峰值时的激活值和工作缓冲区内存。\n- 在可逆设计中，通过标准的加性耦合使 $K$ 层变换成为双射，从而保留信息，使得反向传播无需存储中间激活值。在这种可逆模式下，仅保留终端激活值 $x^{K}$（以相同的稀疏格式）以及峰值时相同的 $2$ 个长度为 $n$ 的稠密工作缓冲区。\n- 对于每个训练样本的时间核算，将通过线性算子和逐元素收缩的反向传播过程建模为与前向传播过程相同序列操作的一次额外应用等效的工作量。在可逆设计中，由于不存储中间激活值，反向传播过程必须首先重新计算它们，这在执行反向等效过程之前会产生一次额外的前向等效过程。因此，每层使用以下前向等效单元：\n  反向传播：每层 $2$ 个前向等效单元（一个前向，一个反向等效），\n  可逆：每层 $3$ 个前向等效单元（一个前向，一个重新计算，一个反向等效）。\n\n在这些假设下，导出以下各项关于 $n$、$s$ 和 $K$ 的符号表达式：\n1) 反向传播训练期间的峰值激活内存，以及\n2) 可逆训练期间的峰值激活内存。\n然后，使用上述前向等效核算，推导可逆设计的每个样本总训练时间与反向传播设计的总训练时间之比。\n\n将您的最终答案表示为一个单行向量，包含三个表达式 $[M_{\\mathrm{bp}}(n,s,K),\\,M_{\\mathrm{rev}}(n,s,K),\\,R_{T}]$，其中 $M_{\\mathrm{bp}}$ 和 $M_{\\mathrm{rev}}$ 分别表示反向传播和可逆设计下的峰值激活内存，$R_{T}$ 是可逆与反向传播的训练时间比。不需要进行数值舍入。不需要单位。", "solution": "问题陈述已分析并确定有效。它在计算科学和深度学习领域具有科学依据，特别是在逆问题和展开式优化方面。该问题是适定的，为得到唯一且有意义的解提供了所有必要的假设和定义。语言客观，设置内部一致且可形式化。因此，我们可以进行推导。\n\n任务是推导与一个展开的 $K$ 层 ISTA 网络相关的三个量：标准反向传播训练的峰值内存（$M_{\\mathrm{bp}}$）、可逆设计训练的峰值内存（$M_{\\mathrm{rev}}$）以及总训练时间之比（$R_{T}$）。\n\n**1. 反向传播训练的峰值内存 ($M_{\\mathrm{bp}}$)**\n\n根据问题陈述，使用标准反向传播进行训练需要保留所有中间层的激活值 $\\{x^{1}, \\dots, x^{K}\\}$。\n- 每个激活向量 $x^{k}$ 都是稀疏的，恰好包含 $s$ 个非零项。\n- 每个稀疏向量的存储格式是一种压缩表示，其中每个非零项需要存储一个浮点值和一个整数索引。这相当于每个非零项需要 $2$ 个内存单位。\n- 因此，存储单个稀疏激活向量 $x^{k}$ 所需的内存为 $2 \\times s = 2s$ 个单位。\n- 由于所有 $K$ 个中间激活值都被存储，这些激活值的总内存为 $K \\times (2s) = 2Ks$ 个单位。\n- 除了激活值，问题还指明有 $2$ 个长度为 $n$ 的稠密工作缓冲区。这些缓冲区的内存为 $2 \\times n = 2n$ 个单位。\n- 峰值内存 $M_{\\mathrm{bp}}$ 是存储的激活值和工作缓冲区的内存之和。\n\n$$M_{\\mathrm{bp}}(n,s,K) = (\\text{激活内存}) + (\\text{工作缓冲区内存})$$\n$$M_{\\mathrm{bp}}(n,s,K) = 2Ks + 2n$$\n\n**2. 可逆训练的峰值内存 ($M_{\\mathrm{rev}}$)**\n\n在可逆设计中，无需存储所有中间激活值。\n- 问题陈述指出，只保留终端激活值 $x^{K}$。\n- 该向量以与之前相同的稀疏格式存储。因此，存储 $x^{K}$ 的内存成本为 $2s$ 个单位。\n- 问题还指出，峰值时需要相同的 $2$ 个长度为 $n$ 的稠密工作缓冲区。这些缓冲区的内存成本仍然是 $2n$ 个单位。\n- 可逆设计的峰值内存 $M_{\\mathrm{rev}}$ 是单个存储的激活值和工作缓冲区的内存之和。\n\n$$M_{\\mathrm{rev}}(n,s,K) = (\\text{终端激活内存}) + (\\text{工作缓冲区内存})$$\n$$M_{\\mathrm{rev}}(n,s,K) = 2s + 2n$$\n\n**3. 训练时间之比 ($R_{T}$)**\n\n问题提供了一个训练计算成本模型，以每层的“前向等效单元”来衡量。设 $T_{\\mathrm{fwd}}$ 表示单层一次前向传播的计算工作量。\n\n- **反向传播训练时间 ($T_{\\mathrm{bp}}$):**\n  - 对于 $K$ 层中的每一层，成本被指定为 $2$ 个前向等效单元。这包括一次前向传播和一次反向传播（假设其成本等效）。\n  - 整个 $K$ 层网络的总训练时间是每层成本的总和。\n  $$T_{\\mathrm{bp}} = \\sum_{k=1}^{K} (2 \\cdot T_{\\mathrm{fwd}}) = K \\cdot (2 T_{\\mathrm{fwd}}) = 2K T_{\\mathrm{fwd}}$$\n\n- **可逆训练时间 ($T_{\\mathrm{rev}}$):**\n  - 对于可逆设计，中间激活值不被存储，必须在反向传播过程中重新计算。\n  - 每层的成本被指定为 $3$ 个前向等效单元。这包括原始的前向传播、一次重新计算过程（另一次前向传播）和反向传播。\n  - $K$ 层网络的总训练时间为：\n  $$T_{\\mathrm{rev}} = \\sum_{k=1}^{K} (3 \\cdot T_{\\mathrm{fwd}}) = K \\cdot (3 T_{\\mathrm{fwd}}) = 3K T_{\\mathrm{fwd}}$$\n\n- **时间之比 ($R_T$):**\n  - 比率 $R_{T}$ 是可逆设计的总训练时间除以反向传播设计的总训练时间。\n  $$R_{T} = \\frac{T_{\\mathrm{rev}}}{T_{\\mathrm{bp}}} = \\frac{3K T_{\\mathrm{fwd}}}{2K T_{\\mathrm{fwd}}}$$\n  - $K$ 和 $T_{\\mathrm{fwd}}$ 项相互抵消，得到一个常数比率。\n  $$R_{T} = \\frac{3}{2}$$\n\n结合这三个结果，我们得到反向传播的峰值内存为 $M_{\\mathrm{bp}}(n,s,K) = 2n + 2Ks$，可逆设计的峰值内存为 $M_{\\mathrm{rev}}(n,s,K) = 2n + 2s$，训练时间比为 $R_{T} = \\frac{3}{2}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2n + 2Ks  2n + 2s  \\frac{3}{2}\n\\end{pmatrix}\n}\n$$", "id": "3396297"}]}