## 引言
在科学与工程的众多领域中，从间接且带噪的观测中恢复真实信号的逆问题是一个核心挑战。传统的变分[优化方法](@entry_id:164468)为此提供了坚实的理论框架，但其迭代求解过程往往收敛缓慢且依赖于繁琐的手动参数调整。这引出了一个关键的知识缺口：我们能否构建一种既保留经典方法的可解释性与模型结构，又能利用数据驱动的优势以实现更快、更精确求解的自动化方案？

学习迭代格式（Learned Iterative Schemes）与[展开优化](@entry_id:756343)（Unrolled Optimization）正是为解决这一挑战而生。本文将系统性地引导读者探索这一融合了经典[数值优化](@entry_id:138060)与[深度学习](@entry_id:142022)的强大[范式](@entry_id:161181)。在接下来的内容中，您将学习到：

- 在“原理与机制”一章中，我们将深入剖析如何将一个经典的迭代[优化算法](@entry_id:147840)（如ISTA）“展开”为一个具有特定结构的[深度神经网络](@entry_id:636170)，并详解其背后的优化原理和训练机制。
- 在“应用与[交叉](@entry_id:147634)学科联系”一章中，我们将展示该框架如何增强经典算法、与物理模型深度融合，并与[元学习](@entry_id:635305)、[自监督学习](@entry_id:173394)等前沿机器学习理念建立联系。
- 最后，通过“动手实践”部分，您将有机会亲手实现和分析学习迭代格式的关键组件，从而将理论知识转化为实践技能。

通过这一系列的学习，您将掌握一种将模型驱动的先验知识与数据驱动的学习能力相结合的尖端技术，为解决复杂的逆问题开辟新的道路。

## 原理与机制

本章在前一章介绍的背景基础上，深入探讨学习迭代格式（Learned Iterative Schemes）背后的核心原理与机制。我们将从[逆问题](@entry_id:143129)的经典变分方法出发，揭示这些方法的优化基础。随后，我们将阐述如何将传统的迭代优化算法“展开”为深度神经网络结构，并详细介绍其训练机制。最后，我们将探讨几种高级模型变体及其相关的理论考量，包括加速格式、即插即用先验（Plug-and-Play Priors）、[参数可辨识性](@entry_id:197485)（identifiability）以及训练的计算复杂性。

### [逆问题](@entry_id:143129)的变分优化基础

许多科学与工程领域的逆问题旨在从间接且通常带有噪声的观测数据 $y$ 中恢复未知的信号或图像 $x$。一个经典的线性模型可以表示为：

$y = Ax + e$

其中 $A$ 是描述物理过程的正演算子，$x$ 是待恢复的未知状态，$y$ 是观测数据，$e$ 是[加性噪声](@entry_id:194447)。这类问题通常是 **不适定 (ill-posed)** 的。一个问题若要被视为适定的（in the sense of Hadamard），其解必须存在、唯一且稳定地依赖于输入数据。若其中任一条件不满足，该问题即为不适定。对于[线性逆问题](@entry_id:751313)，当算子 $A$ 的[奇异值](@entry_id:152907)趋近于零时，问题会表现出不稳定性：数据 $y$ 的微小扰动可能导致解 $x$ 发生巨大变化。这可以通过[奇异值分解](@entry_id:138057)（SVD）来理解，微小的[奇异值](@entry_id:152907)会导致[伪逆](@entry_id:140762) $A^\dagger$ 中出现巨大的元素，从而放大噪声 [@problem_id:3396223]。

为了解决[不适定性](@entry_id:635673)，**正则化 (regularization)** 方法被引入。其核心思想是在求解过程中融入关于未知信号 $x$ 的 **先验知识 (prior knowledge)**。一个强大且广泛应用的方法是[变分正则化](@entry_id:756446)，它将逆问题转化为一个[优化问题](@entry_id:266749)，即最小化一个目标函数 $J(x)$。该[目标函数](@entry_id:267263)通常由两部分组成：

$J(x) = \mathcal{D}(Ax, y) + \lambda \mathcal{R}(x)$

其中：
- $\mathcal{D}(Ax, y)$ 是 **数据保真项 (data fidelity term)**，用于度量经正演算子 $A$ 投影后的估计 $Ax$ 与观测数据 $y$ 之间的一致性。一个常见的选择是基于[高斯噪声](@entry_id:260752)假设的二次范数，即 $\mathcal{D}(Ax, y) = \frac{1}{2}\|Ax - y\|_2^2$。
- $\mathcal{R}(x)$ 是 **正则化项 (regularization term)** 或 **先验项 (prior term)**，它编码了关于解 $x$ 的预期属性，例如平滑性或[稀疏性](@entry_id:136793)。
- $\lambda > 0$ 是 **[正则化参数](@entry_id:162917)**，用于权衡数据保真度与先验知识的重要性。

值得强调的是，数据保真项确保解与观测数据一致，而正则化项则引入[先验信息](@entry_id:753750)以稳定求解过程并选择出符合预期的解。将这两者的角色混淆是一个常见的概念错误 [@problem_id:3396223]。

为了确保[优化问题](@entry_id:266749)有一个令人满意的解，目标函数 $J(x)$ 的 **[凸性](@entry_id:138568) (convexity)** 至关重要。如果 $J(x)$ 是 **严格凸 (strictly convex)** 的，那么它最多只有一个[全局最小值](@entry_id:165977)。对于上述形式的[目标函数](@entry_id:267263)，其[严格凸性](@entry_id:193965)可以通过以下两种主要方式保证：
1.  如果数据保真项是严格凸的，并且正则化项是凸的。例如，当数据保真项为 $\frac{1}{2}\|Ax - y\|_2^2$ 且算子 $A$ 具有[满列秩](@entry_id:749628)时，其Hessian矩阵 $A^\top A$ 是正定的，从而使数据项严格凸。此时，只要 $\mathcal{R}(x)$ 是凸函数，整个目标函数 $J(x)$ 对于任意 $\lambda \ge 0$ 都是严格凸的。
2.  即使数据保真项不是严格凸的（例如，当 $A$ 是[秩亏](@entry_id:754065)的），如果正则化项 $\lambda \mathcal{R}(x)$ 是严格凸的（即 $\mathcal{R}(x)$ 严格凸且 $\lambda > 0$），整个目标函数 $J(x)$ 仍然是严格凸的。一个典型的例子是经典的[吉洪诺夫正则化](@entry_id:140094)（Tikhonov regularization），其中 $\mathcal{R}(x) = \frac{1}{2}\|x\|_2^2$。在这种情况下，$J(x)$ 的Hessian矩阵为 $A^\top A + \lambda I$，对于任何 $\lambda > 0$，该矩阵都是正定的，从而确保了唯一解的存在性 [@problem_id:3396223]。

### 从经典迭代算法到展开网络

一旦[逆问题](@entry_id:143129)被表述为[优化问题](@entry_id:266749)，我们就需要算法来求解它。对于由光滑项（如数据保真项）和可能的非光滑凸项（如[L1范数正则化](@entry_id:751087)器）组成的 **[复合优化](@entry_id:165215)问题 (composite optimization problem)**，形如 $F(x) = g(x) + h(x)$，[近端梯度法](@entry_id:634891)（Proximal Gradient Method）是一个非常有效的工具。

#### [近端梯度法](@entry_id:634891)（ISTA）

[近端梯度法](@entry_id:634891)，也称为前向-后向分裂算法（Forward-Backward Splitting），其迭代格式如下：

$x^{k+1} = \mathrm{prox}_{\alpha h}(x^k - \alpha \nabla g(x^k))$

这个迭代步骤可以直观地理解为两步：
1.  **前向步骤 (Forward Step)**：对光滑项 $g(x)$ 执行一次显式的梯度下降：$v^k = x^k - \alpha \nabla g(x^k)$。
2.  **后向步骤 (Backward Step)**：对非光滑项 $h(x)$ 应用 **[近端算子](@entry_id:635396) (proximal operator)**：$x^{k+1} = \mathrm{prox}_{\alpha h}(v^k)$。

[近端算子](@entry_id:635396)的定义如下：

$\mathrm{prox}_{\phi}(v) = \arg\min_{z \in \mathbb{R}^n} \left\{ \phi(z) + \frac{1}{2}\|z - v\|^2 \right\}$

这个算子可以看作是在点 $v$ 附近寻找一个点 $z$，该点既要使函数 $\phi(z)$ 的值较小，又要与 $v$ 保持接近。例如，当 $\phi(x) = \lambda \|x\|_1$ 时，[近端算子](@entry_id:635396)就是众所周知的 **[软阈值算子](@entry_id:755010) (soft-thresholding operator)** $S_{\lambda}$。此时，该算法被称为 **[迭代软阈值算法](@entry_id:750899) (Iterative Shrinkage-Thresholding Algorithm, ISTA)** [@problem_id:3396290]。

为了保证算法的收敛性，步长 $\alpha$ 的选择至关重要。从[单调算子](@entry_id:637459)理论的第一性原理出发，可以证明，如果 $\nabla g$ 是 $L$-[Lipschitz连续的](@entry_id:267396)，即其满足 **余强制性 (cocoercivity)** [@problem_id:3396236]，那么只要步长 $\alpha$ 满足 $0  \alpha  2/L$，ISTA迭代序列就会收敛到目标函数的[最小值点](@entry_id:634980)。

#### [算法展开](@entry_id:746359)与学习

经典迭代算法（如ISTA）虽然强大，但其[收敛速度](@entry_id:636873)可能很慢，且其性能高度依赖于手工选择的参数（如 $\lambda$ 和 $\alpha$）。**[算法展开](@entry_id:746359) (Algorithm unrolling)** 的思想是将一个[迭代算法](@entry_id:160288)的固定 $K$ 次迭代过程“展开”成一个具有 $K$ 层的深度神经网络。

以ISTA为例，其展开形式，通常称为 **[学习型ISTA (LISTA)](@entry_id:751213)**，可以表示为：

$x^{k+1} = S_{\theta_k}\left(x^k - \gamma_k A^\top(Ax^k - y)\right), \quad k=0, \dots, K-1$

其中，原始的固定步长 $\alpha$ 和阈值 $\lambda\alpha$ 被替换为逐层可学习的参数 $\gamma_k$ 和 $\theta_k$。这个结构本质上是一个[循环神经网络](@entry_id:171248)（RNN），但具有由优化算法赋予的特定结构。模型的输入是观测数据 $y$，输出是经过 $K$ 次迭代后的估计 $x_K$。然后，我们可以通过最小化一个[损失函数](@entry_id:634569)（例如 $\mathcal{L} = \frac{1}{2}\|x_K - x^\star\|_2^2$，其中 $x^\star$ 是真实的信号）来端到端地训练所有可学习的参数。

学习这些逐层参数的动机是多方面的 [@problem_id:3396273]：
- **加速收敛**：通过学习优化的参数序列，展开的算法可以在固定的迭代次数（层数）内达到比固定参数的经典算法高得多的精度。
- **减少偏置**：在[稀疏恢复](@entry_id:199430)问题中，固定的阈值会导致对非零系数的持续性收缩，从而引入所谓的“收缩偏置”。通过学习一个递减的阈值序列 $\theta_k$，模型可以在早期迭代中使用较大的阈值来快速识别信号的支撑集（support），而在[后期](@entry_id:165003)迭代中使用较小的阈值来减小对已识别系数的偏置。
- **改善稳定性和[收缩性](@entry_id:162795)**：通过学习，模型可以自动调整每一步的收缩强度，以在稳定性和收缩之间取得最佳平衡。

一个极具启发性的例子展示了学习的威力 [@problem_id:3396289]。在理想条件下（例如，当 $A^\top A$ 是[对角矩阵](@entry_id:637782)时），通过精心选择第一层的参数 $\alpha$ 和 $\theta$，单层LISTA网络甚至可以一步精确地恢复出特定的[稀疏信号](@entry_id:755125)。例如，对于一个幅度为 $m$ 的理想[稀疏信号](@entry_id:755125)，一步恢复的条件要求增益 $\alpha = 1 + \theta/m$。这清晰地表明，学习到的参数能够编码关于信号结构（如幅度）和噪声水平的信息，从而实现远超传统方法的性能。

### 训练机制：通过时间的[反向传播](@entry_id:199535)

学习迭代格式的参数是通过[基于梯度的优化](@entry_id:169228)方法（如SGD或Adam）来训练的。为了计算[损失函数](@entry_id:634569)关于网络参数（如LISTA中的 $W_k, U_k, \theta_k$）的梯度，我们需要应用 **通过时间的[反向传播](@entry_id:199535) (Backpropagation Through Time, [BPTT](@entry_id:633900))** 算法。

考虑一个通用的LISTA层 [@problem_id:3396240]：
$z_k = W_k x_{k-1} + U_k y$
$x_k = \mathcal{S}_{\theta_k}(z_k)$

其中 $W_k$ 和 $U_k$ 是可学习的权重矩阵。训练的目标是最小化关于最终输出 $x_K$ 的损失 $\mathcal{L} = \frac{1}{2}\|x_K - x^\star\|_2^2$。

[BPTT](@entry_id:633900)的核心是链式法则。我们从最后一层开始，反向计算损失对每一层变量和参数的梯度。首先，我们定义 **敏感度 (sensitivities)** $\delta_k = \frac{\partial \mathcal{L}}{\partial z_k}$，它表示损失对第 $k$ 层预激活变量 $z_k$ 的梯度。

[反向传播](@entry_id:199535)过程如下 [@problem_id:3396240]：
1.  **初始化 (在第 $K$ 层)**：
    $\delta_K = \frac{\partial \mathcal{L}}{\partial x_K} \odot \mathcal{S}'_{\theta_K}(z_K) = (x_K - x^\star) \odot \mathbf{1}_{|z_K| > \theta_K}$
    其中 $\odot$ 表示逐元素乘积，$\mathcal{S}'$ 是软[阈值函数](@entry_id:272436)的导数（在非零点处为1，在零点处为0）。

2.  **反向递推 (从 $k$ 到 $k-1$)**：
    $\frac{\partial \mathcal{L}}{\partial x_{k-1}} = W_k^\top \delta_k$
    $\delta_{k-1} = \frac{\partial \mathcal{L}}{\partial z_{k-1}} = \left(\frac{\partial \mathcal{L}}{\partial x_{k-1}}\right) \odot \mathcal{S}'_{\theta_{k-1}}(z_{k-1}) = (W_k^\top \delta_k) \odot \mathbf{1}_{|z_{k-1}| > \theta_{k-1}}$

3.  **参数梯度计算**：
    一旦计算出每一层的敏感度 $\delta_k$，我们就可以计算该层参数的梯度：
    $\frac{\partial \mathcal{L}}{\partial W_k} = \delta_k (x_{k-1})^\top$
    $\frac{\partial \mathcal{L}}{\partial U_k} = \delta_k y^\top$
    $\frac{\partial \mathcal{L}}{\partial \theta_k} = -\delta_k^\top \mathrm{sign}(z_k)$

这个过程为我们提供了一种系统性的方法来计算损失函数关于所有可学习参数的梯度，从而可以通过梯度下降来优化网络。需要注意的是，标准的[BPTT](@entry_id:633900)要求存储[前向传播](@entry_id:193086)过程中的所有中间激活值，这会导致内存消耗随层数 $K$ 线性增长 [@problem_id:3396249]。

### 高级模型与理论考量

基于[算法展开](@entry_id:746359)的基本思想，研究者们开发了多种更复杂的模型，并深入探讨了相关的理论问题。

#### 惯性方法与[Lyapunov稳定性](@entry_id:147734)分析

为了进一步加速收敛，可以在迭代中引入 **惯性项 (inertial term)** 或 **动量 (momentum)**，这受到了经典[Nesterov加速](@entry_id:752419)梯度法（FISTA）的启发。一个典型的惯性前向-后向分裂层可以写为 [@problem_id:3396266]：

$x_{k+1} = \mathrm{prox}_{\tau g}\left(x_k - \tau \nabla f(x_k) + \beta (x_k - x_{k-1})\right)$

其中 $\beta$ 是动量参数。学习这些惯性模型中的参数（如 $\tau$ 和 $\beta$）需要保证迭代的稳定性。一种强大的分析工具是构建 **Lyapunov函数 (Lyapunov function)**，并证明它在每次迭代中单调递减。例如，可以构建一个形如 $\Phi_k = F(x_k) + \frac{\sigma}{2} \|x_k - x_{k-1}\|^2$ 的[Lyapunov函数](@entry_id:273986)。通过推导 $\Phi_{k+1} \le \Phi_k$ 的条件，可以得到对可学习参数的约束。对于上述惯性格式，可以证明为了保证收敛，动量参数 $\beta$ 必须满足 $\beta \le 1 - \frac{L\tau}{2}$，其中 $L$ 是 $\nabla f$ 的[Lipschitz常数](@entry_id:146583)，$\tau$ 是步长 [@problem_id:3396266]。这为在学习过程中约束参数以确保稳定性提供了理论依据。

#### 即插即用先验（PnP）与[隐式正则化](@entry_id:187599)

**即插即用 (Plug-and-Play, PnP)** 方法将展开算法的思想推向了另一个层次的模块化。它将近端步骤中的特定正则化算子（如[软阈值](@entry_id:635249)）替换为一个通用的 **[去噪](@entry_id:165626)器 (denoiser)** $D$。这个去噪器可以是一个经典的图像处理算法（如BM3D），也可以是一个预训练的深度神经网络。PnP-ISTA的迭代格式如下 [@problem_id:3396307]：

$x^{k+1} = D\left(x^k - \alpha A^\top(Ax^k - y)\right)$

一个核心的理论问题是：在什么条件下，这个通用的去噪器 $D$ 仍可以被解释为某个（可能是隐式的）正则化函数 $\mathcal{R}$ 的[近端算子](@entry_id:635396)？答案来自[单调算子](@entry_id:637459)理论。一个算子 $D$ 是某个凸函数 $\mathcal{R}$ 的[近端算子](@entry_id:635396)，当且仅当 $D$ 是 **紧[不动点](@entry_id:156394)非扩张的 (firmly nonexpansive)**，并且其关联的算子 $D^{-1} - I$ 是 **最大循环单调的 (maximal cyclically monotone)**。如果去噪器 $D$ 满足这些条件，那么PnP迭代就等价于最小化一个显式的变分目标函数 $f(x) + (\lambda/\alpha)\mathcal{R}(x)$，其收敛性可以得到保证 [@problem_id:3396307]。这为使用强大的、可能是黑箱的去噪模型作为复杂[逆问题](@entry_id:143129)的先验提供了坚实的理论基础。

#### [参数可辨识性](@entry_id:197485)

当学习一个参数化的迭代格式时，一个重要但微妙的问题是 **[参数可辨识性](@entry_id:197485) (parameter identifiability)**。即，是否存在多组不同的参数可以产生完全相同的网络映射？如果存在，那么学习过程可能会不稳定，并且学到的参数可能难以解释。

考虑一个“分析形式”的LISTA，其中使用了一个可学习的[分析算子](@entry_id:746429) $W$：

$x^{k+1} = W^{-1} S_{\theta}\left( W (x^k - \tau A^\top(Ax^k - y)) \right)$

可以证明，这个映射对于 $W$ 的行向量的尺度和符号变换是不变的。具体来说，对于任何可逆对角矩阵 $P$，参数组 $(W, \theta)$ 和 $(PW, |P|\theta)$ 会产生完全相同的网络函数 [@problem_id:3396259]。这意味着[损失函数](@entry_id:634569)的表面在参数空间中存在“平坦”的山谷，使得[梯度下降](@entry_id:145942)无法确定唯一的解。

为了解决这个问题，必须引入额外的正则化或约束来打破这种模糊性。一个有效的方法是约束 $W$ 的每一行具有单位范数（例如 $\|w_i\|_2=1$），并为每一行规定一个符号约定（例如，第一个非零元素为正）。这些约束从每个[等价类](@entry_id:156032)中选择一个唯一的、规范的代表，从而使学习问题适定，并使学到的阈值 $\theta$ 具有作为在归一化分析域中的收缩水平的清晰物理解释 [@problem_id:3396259]。

#### 训练策略：展开[BPTT](@entry_id:633900) vs. 隐式[微分](@entry_id:158718)

最后，对于具有大量层数（$K \to \infty$）或达到[不动点](@entry_id:156394)（平衡状态）的迭代格式，训练的计算成本成为一个关键问题。除了通过展开进行[BPTT](@entry_id:633900)之外，还有一种替代方法：**隐式[微分](@entry_id:158718) (implicit differentiation)**。

这种方法首先通过迭代直到收敛找到[不动点](@entry_id:156394) $x^\star(\theta)$，然后利用[隐函数定理](@entry_id:147247)直接计算损失关于参数 $\theta$ 的梯度，而无需存储整个[前向计算](@entry_id:193086)历史。这通常需要求解一个[线性系统](@entry_id:147850)。

两种策略的计算和内存成本有显著差异 [@problem_id:3396249]：
- **展开[BPTT](@entry_id:633900) (Strategy U)**：
    - 计算成本: $C_U = O(K(c_f + c_b))$，其中 $c_f$ 和 $c_b$ 分别是单步前向和后向计算的成本。
    - 内存成本: $M_U = O(K m_x)$，其中 $m_x$ 是存储单步状态所需的内存。

- **隐式[微分](@entry_id:158718) (Strategy I)**：
    - 计算成本: $C_I = K c_f + S c_{imp}$，其中 $S c_{imp}$ 是[求解线性系统](@entry_id:146035)的成本（$S$ 为Krylov[子空间方法](@entry_id:200957)的迭代次数）。
    - 内存成本: $M_I = O(m_x)$，内存消耗是常数级别的，与 $K$ 无关。

比较可知，隐式[微分](@entry_id:158718)在内存方面具有巨大优势，使其成为训练非常深或[平衡态](@entry_id:168134)模型的唯一可行选择。在计算成本方面，当迭代次数 $K$ 足够大，以至于[反向传播](@entry_id:199535)的成本 $K c_b$ 超过[求解线性系统](@entry_id:146035)的成本 $S c_{imp}$ 时，即 $K > S \frac{c_{imp}}{c_b}$，隐式[微分](@entry_id:158718)在计算上也变得更高效 [@problem_id:3396249]。这两种训练[范式](@entry_id:161181)之间的权衡是设计和实现现代[学习型优化](@entry_id:751216)算法的核心考量之一。