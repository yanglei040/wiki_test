{"hands_on_practices": [{"introduction": "对于任何逆问题求解器而言，一个基本要求是稳定性，即对输入数据中的微小扰动（如噪声）不产生过度敏感的响应。本练习将探讨如何通过对学习到的逆映射（一个神经网络）施加全局利普希茨约束来保证这种稳定性。通过这个实践，你将把一种常见的深度学习正则化技术——谱归一化——与逆问题理论中的一个核心要求联系起来，并量化可实现的最优稳定性界限。[@problem_id:3399532]", "problem": "给定一个线性正演模型，其观测值由 $y = A x$ 定义，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是已知的。考虑使用一个 $K$ 层前馈神经网络学习一个逆映射 $\\Phi: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$，其形式为\n$$\n\\Phi(y) \\;=\\; W_{K} \\,\\rho\\!\\left(W_{K-1} \\,\\rho\\!\\left(\\cdots \\rho\\!\\left(W_{1} y\\right)\\cdots\\right)\\right),\n$$\n其中每个 $W_{k} \\in \\mathbb{R}^{d_{k} \\times d_{k-1}}$ 是一个线性层，$\\rho$ 是一个 1-利普希茨（Lipschitz）的逐点非线性函数（例如，整流线性单元 (ReLU)）。我们施加谱归一化，使得每一层的谱范数（算子2-范数）对于已知的正标量 $s_{k}$ 都满足 $\\|W_{k}\\|_{2} \\le s_{k}$。\n\n1. 仅使用利普希茨连续性、算子范数的定义以及它们在函数复合下的性质，推导出一个紧上界 $L_{\\mathrm{sn}}$，使得对于所有的 $y_{1},y_{2} \\in \\mathbb{R}^{m}$，\n$$\n\\|\\Phi(y_{1})-\\Phi(y_{2})\\|_{2} \\;\\le\\; L_{\\mathrm{sn}} \\,\\|y_{1}-y_{2}\\|_{2}.\n$$\n将 $L_{\\mathrm{sn}}$ 表示为 $\\{s_{k}\\}_{k=1}^{K}$ 的函数。\n\n2. 设 $A$ 的奇异值分解（SVD）为 $A = U \\Sigma V^{\\top}$，其奇异值为 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r}  0$，秩为 $r$。假设可识别性被限制在与大于等于阈值 $\\tau0$ 的奇异值相关联的右奇异向量所张成的子空间中，即子空间 $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$。仅使用 SVD 和 Moore-Penrose 伪逆（MPP）的性质，确定在 $\\mathcal{V}_{\\mathrm{id}}$ 上的任何精确右逆的最小利普希茨常数，并将其表示为 $\\mathcal{V}_{\\mathrm{id}}$ 内最小奇异值 $\\sigma_{\\min}^{\\mathrm{id}}$ 的函数。\n\n3. 在实践中，为确保条件稳定性，同时不超过由正演算子 $A$ 引起的不可避免的放大效应，我们将 $\\Phi$ 的全局利普希茨界限定在以下水平\n$$\nL_{\\star} \\;=\\; \\min\\!\\big\\{L_{\\mathrm{sn}},\\, L_{\\mathrm{id}}\\big\\},\n$$\n其中 $L_{\\mathrm{id}}$ 是在第2部分中找到的在 $\\mathcal{V}_{\\mathrm{id}}$ 上的精确右逆的最小利普希茨常数。给定以下具体值：\n- $K=3$，$s_{1}=2$，$s_{2}=1.5$，$s_{3}=1.2$，\n- $A$ 的奇异值为 $\\{10,\\, 2,\\, 0.5,\\, 0.01\\}$，可识别性阈值为 $\\tau=0.4$，\n计算 $L_{\\star}$。\n\n将您的最终答案表示为一个纯数字（无单位），并四舍五入到四位有效数字。", "solution": "该问题经验证是自洽的，其科学基础在于线性代数和神经网络理论的数学，并且是良定的。解题过程按要求分为三部分进行。\n\n### 第1部分：推导利普希茨界 $L_{\\mathrm{sn}}$\n\n目标是找到神经网络 $\\Phi(y)$ 的利普希茨常数的一个紧上界。该网络是函数的复合。我们为每一层定义函数。对于 $k \\in \\{1, 2, \\dots, K-1\\}$，设第 k 层的变换为 $f_k(z) = \\rho(W_k z)$。对于最后一层，设 $f_K(z) = W_K z$。那么该网络就是复合函数 $\\Phi(y) = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(y)$。\n\n复合函数的利普希茨常数受其各个函数利普希茨常数乘积的限制。即 $L(g \\circ h) \\le L(g)L(h)$。我们将递归地应用此性质。\n\n首先，我们确定每个层函数 $f_k$ 的利普希茨常数。\n\n对于一个中间层 $k \\in \\{1, 2, \\dots, K-1\\}$，我们有 $f_k(z) = \\rho(W_k z)$。设 $z_1, z_2$ 是 $f_k$ 定义域中的两个向量。我们分析其差的范数：\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 = \\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2\n$$\n问题陈述中指出，非线性函数 $\\rho$ 是一个逐点 1-利普希茨函数。对于任意两个向量 $u, v$，一个逐点 1-利普希茨函数满足 $\\|\\rho(u) - \\rho(v)\\|_2 \\le \\|u - v\\|_2$。令 $u = W_k z_1$，$v = W_k z_2$。应用此性质，我们得到：\n$$\n\\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2 \\le \\|W_k z_1 - W_k z_2\\|_2\n$$\n利用 $W_k$ 的线性性质和算子2-范数（谱范数）的定义，我们有：\n$$\n\\|W_k z_1 - W_k z_2\\|_2 = \\|W_k (z_1 - z_2)\\|_2 \\le \\|W_k\\|_2 \\|z_1 - z_2\\|_2\n$$\n问题中规定施加了谱归一化，使得 $\\|W_k\\|_2 \\le s_k$。因此，函数 $f_k$ 的利普希茨常数 $L_k$ 受 $s_k$ 的限制：\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 \\le s_k \\|z_1 - z_2\\|_2 \\implies L_k \\le s_k\n$$\n\n对于最后一层 $k=K$，函数为 $f_K(z) = W_K z$。这是一个线性映射。它的利普希茨常数恰好是其算子范数，$L_K = \\|W_K\\|_2$。我们已知界 $\\|W_K\\|_2 \\le s_K$，所以 $L_K \\le s_K$。\n\n现在，我们复合这些层来求整个网络 $\\Phi$ 的利普希茨常数。\n$$\nL_{\\Phi} = L(f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1) \\le L_K \\cdot L_{K-1} \\cdots L_1\n$$\n使用我们为每一层推导出的界：\n$$\nL_{\\Phi} \\le s_K \\cdot s_{K-1} \\cdots s_1 = \\prod_{k=1}^{K} s_k\n$$\n这个上界被记为 $L_{\\mathrm{sn}}$。该界被认为是紧的，因为对于特定的权重 $W_k$（其中 $\\|W_k\\|_2=s_k$）和特定的输入（例如，如果 $\\rho$ 是恒等函数，并且权重矩阵的主奇异向量对齐），它是可以达到的。\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{K} s_k\n$$\n\n### 第2部分：推导最小利普希茨常数 $L_{\\mathrm{id}}$\n\n我们要求的是在子空间 $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$ 上任何精确右逆的最小利普希茨常数。设这样一个逆映射为 $\\mathcal{G}: \\mathbb{R}^m \\to \\mathbb{R}^n$。在 $\\mathcal{V}_{\\mathrm{id}}$ 上是精确右逆的条件意味着，对于任何向量 $x \\in \\mathcal{V}_{\\mathrm{id}}$，如果 $y = Ax$，那么 $\\mathcal{G}(y) = x$。\n\n设 $L_{\\mathcal{G}}$ 是这样一个映射 $\\mathcal{G}$ 的利普希茨常数。设 $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$，并设 $v_j$ 是与该奇异值对应的右奇异向量，因此 $Av_j = \\sigma_j u_j$，其中 $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$。由于 $\\sigma_j \\ge \\tau$，所以 $v_j \\in \\mathcal{V}_{\\mathrm{id}}$。\n让我们用两个特定的点来检验利普希茨条件。设 $x_1 = v_j$ 和 $x_2=0$。两者都在 $\\mathcal{V}_{\\mathrm{id}}$ 中。相应的观测值为 $y_1 = Ax_1 = Av_j = \\sigma_j u_j$ 和 $y_2 = Ax_2 = 0$。\n右逆性质要求 $\\mathcal{G}(y_1) = x_1 = v_j$ 且 $\\mathcal{G}(y_2) = x_2 = 0$。\n利普希茨常数的定义要求：\n$$\n\\|\\mathcal{G}(y_1) - \\mathcal{G}(y_2)\\|_2 \\le L_{\\mathcal{G}} \\|y_1 - y_2\\|_2\n$$\n代入向量：\n$$\n\\|v_j - 0\\|_2 \\le L_{\\mathcal{G}} \\|\\sigma_j u_j - 0\\|_2\n$$\n奇异向量是标准正交的，所以 $\\|v_j\\|_2 = 1$ 且 $\\|u_j\\|_2 = 1$。不等式变为：\n$$\n1 \\le L_{\\mathcal{G}} |\\sigma_j| \\cdot \\|u_j\\|_2 = L_{\\mathcal{G}} \\sigma_j\n$$\n由于 $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$，我们得到了对于*任何*此类逆映射 $\\mathcal{G}$ 的利普希茨常数的一个下界：\n$$\nL_{\\mathcal{G}} \\ge \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\n这表明最小可能的利普希茨常数至少是 $1/\\sigma_{\\min}^{\\mathrm{id}}$。为了证明这个最小值是可以达到的，我们必须构造一个满足右逆性质并且其利普希茨常数恰好是这个值的映射。\n\n考虑截断奇异值分解（TSVD）逆，其定义为：\n$$\n\\mathcal{G}_{\\mathrm{TSVD}} = \\sum_{i : \\sigma_i \\ge \\tau} \\frac{1}{\\sigma_i} v_i u_i^{\\top}\n$$\n这是一个从 $\\mathbb{R}^m$ 到 $\\mathbb{R}^n$ 的线性映射。其算子范数（也就是它的利普希茨常数）由该映射的最大奇异值给出，即 $\\max_{i:\\sigma_i \\ge \\tau} (1/\\sigma_i) = 1/\\min_{i:\\sigma_i \\ge \\tau} (\\sigma_i) = 1/\\sigma_{\\min}^{\\mathrm{id}}$。\n让我们验证它在 $\\mathcal V_{\\mathrm{id}}$ 上是一个精确右逆。任何 $x \\in \\mathcal V_{\\mathrm{id}}$ 都可以写成 $x = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j$。那么 $y = Ax = \\sum_{j : \\sigma_j \\ge \\tau} c_j A v_j = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j$。\n将 $\\mathcal{G}_{\\mathrm{TSVD}}$ 应用于 $y$：\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\mathcal{G}_{\\mathrm{TSVD}}\\left(\\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\mathcal{G}_{\\mathrm{TSVD}}(u_j)\n$$\n根据 $\\mathcal{G}_{\\mathrm{TSVD}}$ 的定义和 $\\{u_i\\}$ 的正交性，$\\mathcal{G}_{\\mathrm{TSVD}}(u_j) = (1/\\sigma_j) v_j$。\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\left(\\frac{1}{\\sigma_j} v_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j = x\n$$\n该映射满足条件。由于我们找到了一个达到下界的映射，因此这就是最小可能的利普希茨常数。\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\n\n### 第3部分：计算 $L_{\\star}$\n\n我们已知具体值，需要计算 $L_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\}$。\n\n首先，计算 $L_{\\mathrm{sn}}$：\n- $K=3$\n- $s_1=2$, $s_2=1.5$, $s_3=1.2$\n- 使用第1部分的公式：\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{3} s_k = s_1 \\cdot s_2 \\cdot s_3 = 2 \\times 1.5 \\times 1.2 = 3 \\times 1.2 = 3.6\n$$\n\n接下来，计算 $L_{\\mathrm{id}}$：\n- $A$ 的奇异值：$\\{10, 2, 0.5, 0.01\\}$\n- 可识别性阈值：$\\tau = 0.4$\n- 我们必须找到 $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$。\n- 大于或等于 $\\tau=0.4$ 的奇异值是 $\\{10, 2, 0.5\\}$。\n- 这些值中的最小值是 $\\sigma_{\\min}^{\\mathrm{id}} = 0.5$。\n- 使用第2部分的公式：\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}} = \\frac{1}{0.5} = 2\n$$\n\n最后，计算 $L_{\\star}$：\n$$\nL_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\} = \\min\\{3.6, 2\\} = 2\n$$\n问题要求答案四舍五入到四位有效数字。\n$$\nL_{\\star} = 2.000\n$$", "answer": "$$\\boxed{2.000}$$", "id": "3399532"}, {"introduction": "一种强大的、融合领域知识的神经网络设计范式是算法展开，它将经典的迭代优化算法的步骤展开成一个深度网络层。本练习将指导你构建一个基于近端梯度法的展开网络，并深入分析其内在的正则化机制。通过推导其等效的谱滤波器，你将理解学习到的参数（如步长）如何隐式地控制解的平滑度，并学会如何设计一个基于数据不符原则的早停准则以保证稳定性。[@problem_id:3399544]", "problem": "考虑线性逆问题 $y = A x^{\\star} + \\eta$，其中 $A \\in \\mathbb{R}^{m \\times n}$，未知目标为 $x^{\\star} \\in \\mathbb{R}^{n}$，加性噪声 $\\eta \\in \\mathbb{R}^{m}$ 满足已知界 $\\|\\eta\\|_{2} \\leq \\delta$。假设数据保真目标由残差平方和二次惩罚项导出，因此正则化代价为 $J(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\mu \\|x\\|_{2}^{2}$，其中正则化权重 $\\mu  0$。考虑一个具有 $L$ 层的展开式近端梯度（PG）网络，其第 $k$ 层执行更新\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right), \\quad x^{0} = 0,\n$$\n其中 $\\alpha_{k}  0$ 是一个可训练的步长，$\\mathrm{prox}_{\\gamma R}(v)$ 表示在 $v$ 处求值的 $\\gamma R$ 的近端算子。你可以认为近端算子的定义和奇异值分解（SVD）的标准性质是已知的。目标是分析由可训练参数引起的隐式谱滤波，并推导出一个有原则的早停规则。\n\n你的任务是：\n- 从近端微积分和线性代数的第一性原理出发，推导用对称矩阵 $M = A^{\\top} A$ 表示的逐层更新，并证明该网络定义了一个从 $A^{\\top} y$ 到 $x^{L}$ 的线性映射。\n- 利用 $M$ 的特征分解和网络的线性性质，推导标量谱滤波器 $g_{L}(\\lambda)$，使得对于 $M$ 的任意特征对 $(\\lambda, u)$ 且 $M u = \\lambda u$， $x^{L}$ 沿 $u$ 的分量为 $g_{L}(\\lambda)\\,(u^{\\top} A^{\\top} y)$。在此部分，假设可训练参数被选择为逐层常数，即 $\\alpha_{k} \\equiv \\alpha$，并且 $\\mu$ 在各层中是固定的。\n- 从数据模型和迭代值的谱表示出发，阐述一个用于早停的差异性原则。也就是说，提出一个规则来选择最小的 $L$，使得残差 $\\|A x^{L} - y\\|_{2}$ 近似匹配噪声水平乘以一个安全因子 $\\tau  1$，并解释为什么这能保证对于 $y$ 中扰动的稳定性。\n- 在逐层常数选择 $\\alpha_{k} \\equiv \\alpha$ 和固定 $\\mu$ 的条件下，以 $g_{L}(\\lambda)$ 的闭式表达式形式给出你的最终答案。无需进行数值计算。\n\n所有数学表达式均以 LaTeX 形式表示。最终答案必须是单个闭式解析表达式。不要包含任何单位。", "solution": "该问题按要求分为四个部分进行分析和求解。\n\n首先，我们推导用矩阵 $M = A^{\\top} A$ 表示的逐层更新，并证明网络映射的线性性。\n第 $k$ 层的更新由下式给出\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right)\n$$\n近端算子 $\\mathrm{prox}_{\\gamma R}(v)$ 被定义为一个优化问题的解：\n$$\n\\mathrm{prox}_{\\gamma R}(v) = \\arg\\min_{z} \\left\\{ \\gamma R(z) + \\frac{1}{2}\\|z-v\\|_{2}^{2} \\right\\}\n$$\n在我们的例子中，正则化函数是 $R(x) = \\|x\\|_{2}^{2}$，参数是 $\\gamma = \\alpha_k \\mu$。最小化目标为\n$$\nf(z) = \\alpha_{k} \\mu \\|z\\|_{2}^{2} + \\frac{1}{2}\\|z-v\\|_{2}^{2}\n$$\n这是一个严格凸函数。我们通过将关于 $z$ 的梯度设为零来找到最小值：\n$$\n\\nabla_{z} f(z) = 2 \\alpha_{k} \\mu z + (z - v) = 0\n$$\n$$\n(1 + 2 \\alpha_{k} \\mu) z = v \\implies z = \\frac{1}{1 + 2 \\alpha_{k} \\mu} v\n$$\n因此，近端算子是一个简单的缩放：$\\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}(v) = (1 + 2 \\alpha_{k} \\mu)^{-1} v$。\n将此代回 $x^{k+1}$ 的更新规则中，我们得到：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} A^{\\top}(A x^{k} - y)\\right)\n$$\n让我们引入对称矩阵 $M = A^{\\top} A$。更新变为：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} (M x^{k} - A^{\\top}y)\\right)\n$$\n重新整理各项，我们得到用 $M$ 表示的更新规则：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left( (I - \\alpha_{k} M) x^{k} + \\alpha_{k} A^{\\top}y \\right)\n$$\n这是一个仿射递推关系。为了证明最终输出 $x^L$ 是 $A^{\\top}y$ 的线性函数，我们从初始条件 $x^0 = 0$ 开始展开递推：\n对于 $k=0$：\n$$\nx^{1} = \\frac{1}{1 + 2 \\alpha_{0} \\mu} \\left( (I - \\alpha_{0} M) x^{0} + \\alpha_{0} A^{\\top}y \\right) = \\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\n$$\n对于 $k=1$：\n$$\nx^{2} = \\frac{1}{1 + 2 \\alpha_{1} \\mu} \\left( (I - \\alpha_{1} M) x^{1} + \\alpha_{1} A^{\\top}y \\right) = \\frac{I - \\alpha_{1} M}{1 + 2 \\alpha_{1} \\mu} \\left(\\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\\right) + \\frac{\\alpha_{1}}{1 + 2 \\alpha_{1} \\mu} A^{\\top}y\n$$\n这可以写成 $x^{2} = (Q_1 + Q_2) A^{\\top}y$，其中 $Q_1$ 和 $Q_2$ 是矩阵。将此过程继续到第 $L$ 层，$x^L$ 将是一些项的和，其中每一项都是包含 $M$ 和 $\\alpha_k$ 的矩阵乘积，作用于一个与 $A^{\\top}y$ 成比例的向量。$x^L$ 的最终表达式形式为：\n$$\nx^L = G_L(\\alpha_0, \\dots, \\alpha_{L-1}, \\mu, M) A^{\\top}y\n$$\n其中 $G_L$ 是一个关于参数的矩阵值函数。这证明了对于固定的参数，展开的网络定义了一个从 $A^{\\top}y$ 到 $x^L$ 的线性映射。\n\n第二，我们推导在逐层常数参数（即 $\\alpha_k \\equiv \\alpha$）情况下的标量谱滤波器 $g_{L}(\\lambda)$。\n更新规则简化为：\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha \\mu} \\left( (I - \\alpha M) x^{k} + \\alpha A^{\\top}y \\right)\n$$\n令 $C = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)$ 且 $d = \\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y$。更新为 $x^{k+1} = C x^{k} + d$。\n从 $x^0=0$ 开始，我们有 $x^L = \\left(\\sum_{k=0}^{L-1} C^k\\right) d$。\n设 $(\\lambda, u)$ 是 $M$ 的一个特征对，因此 $M u = \\lambda u$。向量 $u$ 也是 $C$ 的一个特征向量：\n$$\nC u = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)u = \\frac{1}{1 + 2 \\alpha \\mu}(u - \\alpha \\lambda u) = \\left(\\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}\\right) u\n$$\n让我们将 $C$ 对应于 $\\lambda$ 的特征值表示为 $\\gamma(\\lambda) = \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}$。\n算子 $\\sum_{k=0}^{L-1} C^k$ 是一个矩阵的几何级数。它对特征向量 $u$ 的作用是乘以标量 $\\sum_{k=0}^{L-1} \\gamma(\\lambda)^k = \\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}$。\n让我们将方程投影到特征向量 $u$ 上。$x^L$ 沿 $u$ 的分量是 $u^{\\top}x^L$。\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}\\right) u^{\\top}d\n$$\n代入 $d$ 和 $\\gamma(\\lambda)$ 的表达式：\n$$\nu^{\\top}d = u^{\\top} \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y\\right) = \\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\n$$\n$$\n1 - \\gamma(\\lambda) = 1 - \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu} = \\frac{(1 + 2 \\alpha \\mu) - (1 - \\alpha \\lambda)}{1 + 2 \\alpha \\mu} = \\frac{\\alpha \\lambda + 2 \\alpha \\mu}{1 + 2 \\alpha \\mu} = \\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}\n$$\n结合这些，$x^L$ 沿 $u$ 的分量是：\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{\\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}}\\right) \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\\right) = (1 - \\gamma(\\lambda)^L) \\frac{1}{\\lambda + 2\\mu} (u^{\\top} A^{\\top}y)\n$$\n问题要求的是标量滤波器 $g_L(\\lambda)$，使得 $x^L$ 沿 $u$ 的分量为 $g_L(\\lambda) (u^{\\top}A^{\\top}y)$。从推导出的表达式中，我们可以确定该滤波器为：\n$$\ng_L(\\lambda) = \\frac{1}{\\lambda + 2\\mu} \\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)\n$$\n\n第三，我们阐述一个用于早停的差异性原则，并解释它所提供的稳定性。\nMorozov 差异性原则是选择正则化参数的一种成熟方法。在迭代方法的背景下，迭代次数 $L$ 充当正则化参数。\n提出的规则是：给定数据 $y$、噪声界 $\\delta$ 和安全因子 $\\tau  1$，选择层数（迭代次数）$L$ 为满足残差范数条件的最小整数 $k \\geq 1$：\n$$\n\\|A x^{k} - y\\|_{2} \\leq \\tau \\delta\n$$\n该规则通过计算网络每一层的残差并在满足此标准的第一个层停止来实现。\n\n这个原则保证稳定性的原因如下：\n逆问题是不适定的，这意味着 $A$ 的小奇异值会在朴素求逆中导致噪声放大。展开网络的迭代过程充当了一种正则化方案。迭代次数 $L$ 控制着正则化的程度。\n在初始迭代中（$L$ 较小），该方法重建解 $x^{\\star}$ 中对应于 $A$ 的较大奇异值的分量，这些分量受噪声影响较小。在此阶段，残差 $\\|A x^{k} - y\\|_{2}$ 很大，主要由信号中尚未重建的部分 $A(x^{\\star}-x^k)$ 主导。\n随着 $k$ 的增加，$x^k$ 越来越接近 $x^{\\star}$，残差范数减小，最终接近噪声的范数，因为 $Ax^k - y = A(x^k - x^{\\star}) - \\eta$。当残差变得与噪声水平 $\\delta$ 相当时，差异性原则停止迭代。\n如果迭代继续超过这一点，算法将试图进一步减小残差。由于残差此时已主要由噪声 $\\eta$ 主导，这将意味着将模型拟合到噪声上。拟合噪声需要算法重建对应于小奇异值的分量，而这些分量被严重污染。这会导致解 $x^k$ 中噪声分量的大幅放大，从而破坏重建并导致不稳定。\n通过早停，差异性原则防止算法进入这种噪声拟合的状态。对于所选的 $L$，有效谱滤波器 $g_L(\\lambda)$ 会适当地衰减对应于小奇异值（其中 $\\lambda = \\sigma^2$ 很小）的分量，从而防止噪声的放大。这确保了从受扰动的数据 $y$ 到解 $x^L$ 的映射是一个有界算子，而这正是稳定性的定义。\n\n最后，谱滤波器 $g_{L}(\\lambda)$ 的闭式表达式作为最终答案给出。", "answer": "$$\n\\boxed{\\frac{1}{\\lambda + 2\\mu}\\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)}\n$$", "id": "3399544"}, {"introduction": "在许多实际应用中，逆问题的设置（如传感器几何形状或材料属性）可能会发生变化，为每个新设置重新训练一个模型是不切实际的。超网络（Hypernetwork）通过学习一个从任务描述符到反演网络权重的映射，为这个问题提供了一个优雅的解决方案。本练习将带你深入了解训练超网络的核心机制，通过运用多元微积分和链式法则，推导出验证损失相对于超网络参数的梯度。[@problem_id:3399497]", "problem": "考虑一个监督线性逆问题，其中前向算子依赖于一个传感器几何任务描述符。设任务描述符为一个向量 $s \\in \\mathbb{R}^{p}$，它确定了一个已知的前向算子 $A(s) \\in \\mathbb{R}^{m \\times n}$。对于给定的任务描述符 $s$，假设有一个验证集，包含由 $y_i = A(s) x_i^{\\star} + \\varepsilon_i$ 生成的数对 $\\{(x_i^{\\star}, y_i)\\}_{i=1}^{M}$，其中 $\\varepsilon_i$ 是零均值噪声。一个由权重矩阵 $W \\in \\mathbb{R}^{n \\times m}$ 参数化的逆网络产生重建结果 $\\hat{x}_i = \\sigma(W y_i)$，其中 $\\sigma$ 是一个可微的、逐分量应用于 $\\mathbb{R}^{n}$ 中向量的逐元素非线性函数。对于固定的任务描述符 $s$ 和权重 $W$，验证损失为\n$$\n\\mathcal{L}_{\\mathrm{val}}(W; s) \\;=\\; \\frac{1}{2M} \\sum_{i=1}^{M} \\left\\| \\sigma(W y_i) - x_i^{\\star} \\right\\|_{2}^{2}.\n$$\n一个带有参数 $\\phi \\in \\mathbb{R}^{d_{\\phi}}$ 的超网络 (HN) 通过一个可微映射 $h_{\\phi} : \\mathbb{R}^{p} \\to \\mathbb{R}^{nm}$ 从任务描述符生成逆网络的权重，其中 $W(\\phi, s)$ 通过重塑 $\\mathrm{vec}(W(\\phi, s)) = h_{\\phi}(s)$ 来定义。将生成的权重关于超网络参数的雅可比矩阵表示为\n$$\nJ_{\\phi}(s) \\;=\\; \\frac{\\partial \\,\\mathrm{vec}(W(\\phi, s))}{\\partial \\phi} \\;\\in\\; \\mathbb{R}^{nm \\times d_{\\phi}}.\n$$\n仅使用多元微积分的基本法则，包括链式法则和标准矩阵微积分恒等式（例如 $\\mathrm{vec}(uv^{\\top}) = v \\otimes u$）以及哈达玛积的定义，推导验证损失关于超网络参数的梯度的闭式解析表达式，\n$$\n\\nabla_{\\phi} \\,\\mathcal{L}_{\\mathrm{val}}(\\phi; s) \\;=\\; \\frac{\\partial \\,\\mathcal{L}_{\\mathrm{val}}(W(\\phi, s); s)}{\\partial \\phi},\n$$\n该表达式需用 $J_{\\phi}(s)$、验证数据 $\\{(x_i^{\\star}, y_i)\\}_{i=1}^{M}$、非线性函数 $\\sigma$ 及其导数 $\\sigma'$ 以及生成的权重 $W(\\phi, s)$ 来表示。你的最终答案必须是单一的闭式解析表达式。定义你在推导过程中引入的任何辅助量。不需要进行数值计算。", "solution": "该问题陈述已经过验证，被认为是机器学习和逆问题领域中一个适定且具有科学依据的问题。我现在将开始推导指定的梯度。\n\n目标是推导验证损失 $\\mathcal{L}_{\\mathrm{val}}$ 关于超网络参数 $\\phi$ 的梯度的闭式表达式，记为 $\\nabla_{\\phi} \\mathcal{L}_{\\mathrm{val}}(\\phi; s)$。验证损失是逆网络权重 $W$ 的函数，而权重 $W$ 又是由超网络根据其参数 $\\phi$ 和任务描述符 $s$ 生成的。这种嵌套依赖关系 $\\phi \\to W(\\phi, s) \\to \\mathcal{L}_{\\mathrm{val}}(W;s)$，使得使用多元链式法则成为必要。\n\n为方便起见，在中间步骤中我们将用 $W$ 代替 $W(\\phi, s)$，并在最终结果中重新引入其显式依赖关系。将关于 $\\phi$ 的梯度与关于权重 $W$ 的梯度联系起来的链式法则，可以最方便地使用权重的向量化形式 $\\mathrm{vec}(W) \\in \\mathbb{R}^{nm}$ 来表示。关于超网络参数向量 $\\phi \\in \\mathbb{R}^{d_{\\phi}}$ 的梯度由下式给出：\n$$\n\\nabla_{\\phi} \\mathcal{L}_{\\mathrm{val}} = \\left( \\frac{\\partial \\mathrm{vec}(W)}{\\partial \\phi} \\right)^T \\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}}\n$$\n问题陈述将向量化权重关于超网络参数的雅可比矩阵定义为 $J_{\\phi}(s) = \\frac{\\partial \\mathrm{vec}(W)}{\\partial \\phi}$。因此，方程变为：\n$$\n\\nabla_{\\phi} \\mathcal{L}_{\\mathrm{val}} = J_{\\phi}(s)^T \\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}}\n$$\n推导的核心是求出损失关于向量化权重的梯度，即 $\\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}}$。我们将首先计算关于矩阵 $W \\in \\mathbb{R}^{n \\times m}$ 的梯度，记为 $\\nabla_W \\mathcal{L}_{\\mathrm{val}}$，然后将结果向量化。\n\n验证损失定义为：\n$$\n\\mathcal{L}_{\\mathrm{val}}(W; s) = \\frac{1}{2M} \\sum_{i=1}^{M} \\left\\| \\sigma(W y_i) - x_i^{\\star} \\right\\|_{2}^{2}\n$$\n根据梯度算子的线性性质，我们可以单独计算求和中的每一项的梯度：\n$$\n\\nabla_W \\mathcal{L}_{\\mathrm{val}} = \\frac{1}{2M} \\sum_{i=1}^{M} \\nabla_W \\left\\| \\sigma(W y_i) - x_i^{\\star} \\right\\|_{2}^{2}\n$$\n我们先关注样本 $i$ 的单项，并令 $\\mathcal{L}_i(W) = \\left\\| \\sigma(W y_i) - x_i^{\\star} \\right\\|_{2}^{2}$。为简化推导，我们为样本 $i$ 引入以下辅助量：\n1. 预激活向量：$z_i = W y_i \\in \\mathbb{R}^n$。\n2. 网络的重建结果：$\\hat{x}_i = \\sigma(z_i) \\in \\mathbb{R}^n$。\n3. 重建误差向量：$\\delta_i = \\hat{x}_i - x_i^{\\star} \\in \\mathbb{R}^n$。\n\n根据这些定义，单样本损失为 $\\mathcal{L}_i = \\| \\delta_i \\|_2^2 = \\delta_i^T \\delta_i$。我们使用微分法来求梯度。$\\mathcal{L}_i$ 的微分为：\n$$\nd\\mathcal{L}_i = 2 \\delta_i^T d\\delta_i\n$$\n误差的微分 $d\\delta_i$ 就是 $d\\hat{x}_i$，因为 $x_i^{\\star}$ 是一个常数。重建结果的微分 $d\\hat{x}_i = d\\sigma(z_i)$，通过应用链式法则求得。由于非线性函数 $\\sigma$ 是逐元素应用的，其关于其向量输入 $z_i$ 的雅可比矩阵是一个对角矩阵，即 $J_{\\sigma}(z_i) = \\mathrm{diag}(\\sigma'(z_i))$，其中 $\\sigma'(z_i)$ 是 $\\sigma$ 的导数在 $z_i$ 的每个分量上求值得到的向量。因此，$d\\hat{x}_i = J_{\\sigma}(z_i) dz_i$。将此代入 $d\\mathcal{L}_i$ 的表达式中可得：\n$$\nd\\mathcal{L}_i = 2 \\delta_i^T J_{\\sigma}(z_i) dz_i\n$$\n接下来，我们将预激活的微分 $dz_i = d(W y_i)$ 表示为 $dz_i = (dW) y_i$。将其代入 $d\\mathcal{L}_i$ 的表达式，得到关于 $W$ 变化的完整微分：\n$$\nd\\mathcal{L}_i = 2 \\delta_i^T J_{\\sigma}(z_i) (dW) y_i\n$$\n为了从微分 $d\\mathcal{L}_i$ 中确定梯度矩阵 $\\nabla_W \\mathcal{L}_i$，我们使用标准矩阵微积分恒等式 $df = \\mathrm{Tr}(G^T dX)$，它意味着 $\\nabla_X f = G$。$d\\mathcal{L}_i$ 的表达式是一个标量，因此等于其自身的迹。我们应用迹的循环性质 $\\mathrm{Tr}(ABC) = \\mathrm{Tr}(CAB)$ 来分离出 $dW$ 项：\n$$\nd\\mathcal{L}_i = \\mathrm{Tr}\\left( 2 \\delta_i^T J_{\\sigma}(z_i) (dW) y_i \\right) = \\mathrm{Tr}\\left( y_i \\left(2 \\delta_i^T J_{\\sigma}(z_i)\\right) dW \\right)\n$$\n将此与恒等式 $d\\mathcal{L}_i = \\mathrm{Tr}((\\nabla_W \\mathcal{L}_i)^T dW)$ 进行比较，我们可以确定梯度的转置：\n$$\n(\\nabla_W \\mathcal{L}_i)^T = 2 y_i \\delta_i^T J_{\\sigma}(z_i)\n$$\n对两边进行转置，得到第 $i$ 个样本损失的梯度矩阵：\n$$\n\\nabla_W \\mathcal{L}_i = \\left( 2 y_i \\delta_i^T J_{\\sigma}(z_i) \\right)^T = 2 J_{\\sigma}(z_i)^T \\delta_i y_i^T\n$$\n由于 $J_{\\sigma}(z_i)$ 是一个对角矩阵，它是对称的，即 $J_{\\sigma}(z_i)^T = J_{\\sigma}(z_i)$。因此：\n$$\n\\nabla_W \\mathcal{L}_i = 2 J_{\\sigma}(z_i) \\delta_i y_i^T\n$$\n对角矩阵 $J_{\\sigma}(z_i)$ 和向量 $\\delta_i$ 的乘积可以使用哈达玛（逐元素）积 $\\odot$ 表示为 $\\sigma'(z_i) \\odot \\delta_i$。代入 $z_i$ 和 $\\delta_i$ 的定义：\n$$\n\\nabla_W \\mathcal{L}_i = 2 \\left( \\sigma'(W y_i) \\odot (\\sigma(W y_i) - x_i^{\\star}) \\right) y_i^T\n$$\n现在，我们对所有样本求和并引入归一化因子，以求得总验证损失关于 $W$ 的梯度：\n$$\n\\nabla_W \\mathcal{L}_{\\mathrm{val}} = \\frac{1}{2M} \\sum_{i=1}^{M} \\nabla_W \\mathcal{L}_i = \\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\left( \\sigma'(W y_i) \\odot (\\sigma(W y_i) - x_i^{\\star}) \\right) y_i^T \\right]\n$$\n下一步是通过对 $\\nabla_W \\mathcal{L}_{\\mathrm{val}}$ 应用向量化算子 $\\mathrm{vec}(\\cdot)$ 来获得 $\\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}}$。由于向量化是线性运算，我们可以写出：\n$$\n\\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}} = \\frac{1}{M} \\sum_{i=1}^{M} \\mathrm{vec}\\left( \\left( \\sigma'(W y_i) \\odot (\\sigma(W y_i) - x_i^{\\star}) \\right) y_i^T \\right)\n$$\n我们使用所提供的外积向量化恒等式 $\\mathrm{vec}(uv^T) = v \\otimes u$，其中 $\\otimes$ 表示克罗内克积。对于求和中的每一项，我们设 $u_i = \\sigma'(W y_i) \\odot (\\sigma(W y_i) - x_i^{\\star})$ 和 $v_i = y_i$。这得到：\n$$\n\\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}} = \\frac{1}{M} \\sum_{i=1}^{M} \\left[ y_i \\otimes \\left( \\sigma'(W y_i) \\odot (\\sigma(W y_i) - x_i^{\\star}) \\right) \\right]\n$$\n最后，我们将此表达式代回到链式法则方程 $\\nabla_{\\phi} \\mathcal{L}_{\\mathrm{val}} = J_{\\phi}(s)^T \\nabla_{\\mathrm{vec}(W)} \\mathcal{L}_{\\mathrm{val}}$ 中。我们明确写出 $W$ 对 $\\phi$ 和 $s$ 的依赖关系，从而得到验证损失关于超网络参数的梯度的最终闭式表达式：\n$$\n\\nabla_{\\phi} \\mathcal{L}_{\\mathrm{val}}(\\phi; s) = \\frac{1}{M} J_{\\phi}(s)^T \\sum_{i=1}^{M} \\left[ y_i \\otimes \\left( \\sigma'(W(\\phi, s) y_i) \\odot (\\sigma(W(\\phi, s) y_i) - x_i^{\\star}) \\right) \\right]\n$$\n此表达式完全由问题陈述中指定的量来表示，符合要求。", "answer": "$$\n\\boxed{\\frac{1}{M} J_{\\phi}(s)^T \\sum_{i=1}^{M} \\left[ y_i \\otimes \\left( \\sigma'(W(\\phi, s) y_i) \\odot (\\sigma(W(\\phi, s) y_i) - x_i^{\\star}) \\right) \\right]}\n$$", "id": "3399497"}]}