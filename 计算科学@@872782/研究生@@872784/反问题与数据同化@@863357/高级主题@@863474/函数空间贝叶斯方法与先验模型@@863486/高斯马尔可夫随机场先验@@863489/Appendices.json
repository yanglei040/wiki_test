{"hands_on_practices": [{"introduction": "理解任何概率模型的关键一步是能够从中抽取样本。本练习将为您提供从高斯马尔可夫随机场（GMRF）进行抽样的标准方法的实践机会，该方法依赖于对稀疏精度矩阵进行高效的 Cholesky 分解 [@problem_id:3384809]。掌握这项技术对于不确定性量化和模型检验等任务至关重要。", "problem": "考虑一个未知向量 $x \\in \\mathbb{R}^n$ 的高斯马尔可夫随机场（GMRF）先验，该先验由一个零均值多元正态分布定义，其精度矩阵 $Q \\in \\mathbb{R}^{n \\times n}$ 是稀疏、对称正定（SPD）的，即 $x \\sim \\mathcal{N}(0, Q^{-1})$。仅使用以下基本事实作为出发点：(i) SPD 矩阵的 Cholesky 分解存在且唯一，即存在一个对角线元素为正的下三角矩阵 $L$，使得 $Q = L L^\\top$；(ii) 如果 $z \\sim \\mathcal{N}(0, I)$，那么对于任意固定矩阵 $A$，随机向量 $A z$ 是均值为 $0$、协方差为 $A A^\\top$ 的高斯向量；以及 (iii) 对于下三角矩阵，求解三角线性系统在数值上是稳定且高效的。基于这些基础，推导一种抽样方法，通过涉及 $Q$ 的 Cholesky 因子的运算，对一个独立同分布（i.i.d.）的标准正态随机向量进行变换，以生成 $x \\sim \\mathcal{N}(0, Q^{-1})$。您的推导必须确定该变换，并根据其产生的协方差来证明其正确性。\n\n在一个程序中实现所推导的抽样方法，该程序对代表 GMRF 先验的三个 SPD 精度矩阵执行蒙特卡洛验证。对于每种情况，生成 $N_s$ 个 i.i.d. 样本，计算经验均值和二阶矩矩阵，并量化样本与目标分布的匹配程度。具体来说，计算经验均值的欧几里得范数，并使用经验二阶矩代替精确期望，计算矩恒等式 $Q \\, \\mathbb{E}[x x^\\top] = I$ 的相对 Frobenius 范数误差。对于标量情况，还需计算经验二阶矩与精确方差 $Q^{-1}$ 之间的绝对误差。所有数值结果必须报告为无单位的浮点数。\n\n您的程序必须生成单行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，“[result1,result2,...]”），并且对于下面的测试套件，结果的顺序如下：对于每个矩阵，输出经验均值范数，后跟矩恒等式的相对误差；对于标量情况，还需附加方差的绝对误差。具体而言，输出必须是列表 $[m_1, e_1, m_2, e_2, m_3, e_3]$，其中 $m_i$ 是经验均值范数，$e_i$ 是案例 $i$ 的指定误差，而 $e_3$ 是标量方差的绝对误差。\n\n使用以下具有固定种子和样本数量的测试套件：\n\n- 案例 1（一维链状图）：设 $n = 5$。定义图拉普拉斯矩阵 $L_g \\in \\mathbb{R}^{n \\times n}$，其元素为：对于节点度 $\\deg(i)$，$[L_g]_{ii} = \\deg(i)$；对于连续节点之间的边，$[L_g]_{i,i+1} = [L_g]_{i+1,i} = -1$；其他位置为零。用 $\\gamma = 1$ 构建 SPD 精度矩阵 $Q = L_g + \\gamma I$。使用 $N_s = 20000$ 和固定的伪随机种子 $42$。\n\n- 案例 2（二维网格图）：设网格为 $3 \\times 3$，因此 $n = 9$。按行主序对节点进行索引，并定义二维网格拉普拉斯矩阵 $L_g \\in \\mathbb{R}^{n \\times n}$，其中 $[L_g]_{ii} = \\deg(i)$ 等于节点 $i$ 的网格内邻居数量，对于与水平或垂直邻居的每个邻接关系，非对角线元素为 $-1$，其他位置为零。用 $\\gamma = 1$ 构建 $Q = L_g + \\gamma I$。使用 $N_s = 20000$ 和固定的伪随机种子 $123$。\n\n- 案例 3（标量精度）：设 $n = 1$ 且 $Q = [3]$。使用 $N_s = 20000$ 和固定的伪随机种子 $7$。\n\n对于每种情况，计算：\n- 经验均值 $\\hat{\\mu} = \\frac{1}{N_s} \\sum_{k=1}^{N_s} x^{(k)}$ 并将其欧几里得范数 $\\|\\hat{\\mu}\\|_2$ 报告为浮点数。\n- 经验二阶矩矩阵 $\\hat{S} = \\frac{1}{N_s} \\sum_{k=1}^{N_s} x^{(k)} (x^{(k)})^\\top$，并为案例 1 和 2 报告相对 Frobenius 范数误差 $\\frac{\\|Q \\hat{S} - I\\|_F}{\\|I\\|_F}$ 为浮点数。\n- 对于案例 3，还需报告绝对误差 $|\\hat{v} - Q^{-1}|$，其中 $\\hat{v} = \\frac{1}{N_s} \\sum_{k=1}^{N_s} (x^{(k)})^2$，并将其报告为浮点数。\n\n您的程序必须按此确切顺序和格式生成单行 $[m_1,e_1,m_2,e_2,m_3,e_3]$。角度和物理单位不适用；所有结果都是无单位的浮点值。", "solution": "我们的目标是推导一种方法，用于从一个由其精度矩阵定义的零均值多元正态分布 $x \\sim \\mathcal{N}(0, Q^{-1})$ 中生成向量 $x \\in \\mathbb{R}^n$ 的随机样本。精度矩阵 $Q \\in \\mathbb{R}^{n \\times n}$ 被给定为稀疏、对称和正定（SPD）。该推导将基于一组指定的基本原则构建。\n\n核心策略是将一个独立同分布（i.i.d.）的标准正态随机变量向量变换为一个具有期望分布特性的向量。设 $z \\in \\mathbb{R}^n$ 是一个随机向量，其分量独立地从标准正态分布中抽取，即 $z \\sim \\mathcal{N}(0, I)$，其中 $I$ 是 $n \\times n$ 的单位矩阵。我们假设一个由常数矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 定义的线性变换，使得变换后的向量 $x = Az$ 服从目标分布 $\\mathcal{N}(0, Q^{-1})$。\n\n我们首先引用所提供的事实 (ii)，该事实阐述了高斯随机向量线性变换的分布。根据该原则，如果 $z \\sim \\mathcal{N}(0, I)$，则向量 $x = Az$ 也是高斯分布的。其均值为 $\\mathbb{E}[x] = \\mathbb{E}[Az] = A\\mathbb{E}[z] = A \\cdot 0 = 0$。这证实了其零均值特性。$x$ 的协方差矩阵由 $\\text{Cov}(x) = \\mathbb{E}[xx^\\top] = \\mathbb{E}[(Az)(Az)^\\top] = \\mathbb{E}[Azz^\\top A^\\top] = A\\mathbb{E}[zz^\\top]A^\\top$ 给出。由于 $z$ 的分量是独立同分布的标准正态变量，其协方差矩阵是单位矩阵，即 $\\mathbb{E}[zz^\\top] = I$。因此，$x$ 的协方差为 $\\text{Cov}(x) = AIA^\\top = AA^\\top$。\n\n要使 $x$ 服从分布 $\\mathcal{N}(0, Q^{-1})$，其协方差矩阵必须等于 $Q^{-1}$。这对我们的变换矩阵 $A$ 施加了以下条件：\n$$AA^\\top = Q^{-1}$$\n\n为了找到一个合适的矩阵 $A$，我们现在利用事实 (i)。该事实指出，任何 SPD 矩阵，例如我们的精度矩阵 $Q$，都存在唯一的 Cholesky 分解形式 $Q = LL^\\top$，其中 $L$ 是一个对角线元素为正的下三角矩阵。通过对该分解求矩阵的逆，我们得到所需协方差矩阵 $Q^{-1}$ 的表达式：\n$$Q^{-1} = (LL^\\top)^{-1} = (L^\\top)^{-1}L^{-1}$$\n\n通过将条件 $AA^\\top = Q^{-1}$ 与推导出的表达式 $Q^{-1} = (L^\\top)^{-1}L^{-1}$ 进行比较，我们可以确定变换矩阵 $A$ 的一个有效选择。通过观察，我们可以设定 $A = (L^\\top)^{-1}$。采用此选择，变换变为 $x = (L^\\top)^{-1}z$。\n\n这个关于 $x$ 的方程不应通过显式计算 $L^\\top$ 的逆来实现。相反，我们可以将其重写为一个等价的线性系统：\n$$L^\\top x = z$$\n\n因此，最终的抽样算法描述如下。要从 $\\mathcal{N}(0, Q^{-1})$ 生成一个样本 $x$：\n1.  计算精度矩阵 $Q$ 的 Cholesky 因子 $L$，使得 $Q = LL^\\top$。\n2.  从标准正态分布 $z \\sim \\mathcal{N}(0, I)$ 中生成一个样本向量 $z \\in \\mathbb{R}^n$。\n3.  求解线性方程组 $L^\\top x = z$ 以得到未知向量 $x$。\n\n此过程在计算上是可靠的。由于 $L$ 是一个下三角矩阵，其转置 $L^\\top$ 是一个上三角矩阵。正如事实 (iii) 所证实的，求解三角线性系统是一种数值稳定且高效的操作，通常通过回代法完成。\n\n这完成了对高斯马尔可夫随机场的一种正确且高效的抽样方法的推导，该推导严格基于所提供的原则。实现将遵循此推导出的算法进行蒙特卡洛验证。", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Derives and implements a GMRF sampling method, and runs Monte Carlo verification.\n    \"\"\"\n\n    def analyze_case(Q, n, Ns, seed, case_id):\n        \"\"\"\n        Analyzes a single GMRF case by sampling and computing statistics.\n\n        Args:\n            Q (np.ndarray): The precision matrix.\n            n (int): The dimension of the state vector.\n            Ns (int): The number of samples to generate.\n            seed (int): The seed for the random number generator.\n            case_id (int): An identifier for the case (1, 2, or 3).\n\n        Returns:\n            tuple: A tuple containing the empirical mean norm and the relevant error metric.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Compute the Cholesky factor L of the precision matrix Q such that Q = L L^T.\n        # The method is based on the derivation that if z ~ N(0, I), then x solved from\n        # L.T @ x = z will have the distribution N(0, Q^-1).\n        L = scipy.linalg.cholesky(Q, lower=True)\n\n        # Step 2: Generate Ns samples from a standard normal distribution.\n        # Z is a matrix of size n x Ns, where each column is an independent sample z.\n        Z = rng.standard_normal(size=(n, Ns))\n\n        # Step 3: Solve L.T @ x = z for each sample z to get samples x from N(0, Q^-1).\n        # solve_triangular is used for efficiency as L.T is upper-triangular.\n        X = scipy.linalg.solve_triangular(L.T, Z, lower=False)\n\n        # Step 4: Compute empirical mean and its norm.\n        mu_hat = np.mean(X, axis=1)\n        mean_norm = np.linalg.norm(mu_hat)\n\n        # Step 5: Compute empirical second-moment matrix.\n        S_hat = (X @ X.T) / Ns\n\n        # Step 6: Compute the specified error metric for the case.\n        if case_id in [1, 2]:\n            # For cases 1 and 2, compute the relative Frobenius-norm error of the moment identity.\n            I = np.identity(n)\n            # The identity is Q * E[x x^T] = Q * Q^-1 = I.\n            # We check ||Q @ S_hat - I||_F / ||I||_F\n            error_numerator = np.linalg.norm(Q @ S_hat - I, ord='fro')\n            # ||I||_F = sqrt(n)\n            error_denominator = np.sqrt(n)\n            error = error_numerator / error_denominator\n        elif case_id == 3:\n            # For scalar case 3, compute the absolute error of the variance.\n            v_hat = S_hat[0, 0]\n            # Exact variance is Q^-1.\n            variance_exact = 1.0 / Q[0, 0]\n            error = np.abs(v_hat - variance_exact)\n        else:\n            # This path should not be reached with the given problem statement.\n            error = np.nan\n\n        return mean_norm, error\n\n    def build_q_case1(n, gamma):\n        \"\"\"Constructs the precision matrix Q for Case 1 (1D chain graph).\"\"\"\n        Lg = np.zeros((n, n))\n        diag = 2.0 * np.ones(n)\n        diag[0], diag[-1] = 1.0, 1.0\n        np.fill_diagonal(Lg, diag)\n        off_diag = -1.0 * np.ones(n - 1)\n        Lg += np.diag(off_diag, k=1)\n        Lg += np.diag(off_diag, k=-1)\n        return Lg + gamma * np.identity(n)\n\n    def build_q_case2(n, gamma):\n        \"\"\"Constructs the precision matrix Q for Case 2 (2D grid graph).\"\"\"\n        dim = int(np.sqrt(n))\n        Lg = np.zeros((n, n))\n        for r in range(dim):\n            for c in range(dim):\n                i = r * dim + c\n                degree = 0\n                for dr, dc in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n                    nr, nc = r + dr, c + dc\n                    if 0 = nr  dim and 0 = nc  dim:\n                        degree += 1\n                        j = nr * dim + nc\n                        Lg[i, j] = -1.0\n                Lg[i, i] = degree\n        return Lg + gamma * np.identity(n)\n\n    # Define test cases from the problem statement\n    test_cases = [\n        {'id': 1, 'n': 5, 'Ns': 20000, 'seed': 42, 'gamma': 1.0, 'q_builder': build_q_case1},\n        {'id': 2, 'n': 9, 'Ns': 20000, 'seed': 123, 'gamma': 1.0, 'q_builder': build_q_case2},\n        {'id': 3, 'n': 1, 'Ns': 20000, 'seed': 7, 'Q': np.array([[3.0]])},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['id'] in [1, 2]:\n            Q = case['q_builder'](case['n'], case['gamma'])\n        else: # Case 3\n            Q = case['Q']\n        \n        m, e = analyze_case(Q=Q, n=case['n'], Ns=case['Ns'], seed=case['seed'], case_id=case['id'])\n        results.extend([m, e])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3384809"}, {"introduction": "GMRF 先验通常源于对施加平滑性的微分算子的离散化。本练习将指导您使用有限元方法，在非均匀网格上构建这样一个先验，这在科学计算中是一种常见情景 [@problem_id:3384822]。接着，您将探索该先验与观测数据的结合如何决定解的可辨识性，这是逆问题中的一个核心概念。", "problem": "要求您构建一个精度矩阵 $Q$，该矩阵是通过在非均匀网格上使用混合狄利克雷（Dirichlet）和诺伊曼（Neumann）边界条件离散化负一维拉普拉斯算子 $-\\Delta$ 而得到的。同时，当该精度矩阵被用作线性逆问题中的高斯马尔可夫随机场（GMRF）先验时，您需要分析其后验零空间。请使用以下原则和规范。\n\n按如下方式构建离散精度矩阵。考虑一个一维域 $[0, L]$，其网格节点位于 $x_0, x_1, \\ldots, x_M$ 处，其中 $x_0 = 0$，$x_M = L$，且网格为非均匀的，意味着对于 $i = 0, 1, \\ldots, M-1$，单元长度 $h_i = x_{i+1} - x_i$ 不必相等。使用混合边界条件离散化 $-\\Delta$：左边界为狄利克雷（Dirichlet）条件，即 $u(0) = 0$；右边界为诺伊曼（Neumann）条件，即 $u'(L) = 0$。使用标准线性有限元方法，通过对每个单元的贡献求和来组装刚度矩阵 $K \\in \\mathbb{R}^{(M+1) \\times (M+1)}$。对于每个长度为 $h_i$ 的单元 $e_i = [x_i, x_{i+1}]$，其局部刚度矩阵为\n$$\nK^{(e_i)} = \\frac{1}{h_i}\n\\begin{bmatrix}\n1  -1 \\\\\n-1  1\n\\end{bmatrix},\n$$\n而 $K$ 是通过将 $K^{(e_i)}$ 加到全局节点 $i$ 和 $i+1$ 的相应条目中得到的。通过移除 $K$ 中与左边界节点 $x_0$ 对应的行和列来施加狄利克雷边界条件 $u(0)=0$，从而得到关于未知量 $x = [u(x_1), \\ldots, u(x_M)]^\\top$ 的精度矩阵 $Q \\in \\mathbb{R}^{M \\times M}$。$x_M$ 处的诺伊曼边界对此算子是自然边界条件，不需要对组装好的刚度矩阵进行显式修改。\n\n将 $Q$ 解释为 $x$ 上的高斯马尔可夫随机场（GMRF）先验的先验精度。考虑线性逆问题\n$$\ny = H x + \\varepsilon,\n$$\n其中 $y \\in \\mathbb{R}^m$ 是观测值，$H \\in \\mathbb{R}^{m \\times M}$ 是观测算子，$\\varepsilon \\sim \\mathcal{N}(0, R)$ 是协方差为 $R = \\sigma^2 I$ 的零均值高斯噪声。令 $\\alpha  0$ 表示先验的标量精度缩放因子。在这些假设下，后验精度为\n$$\n\\Lambda = \\alpha Q + H^\\top R^{-1} H = \\alpha Q + \\frac{1}{\\sigma^2} H^\\top H.\n$$\n\n将后验零空间维度 $d$ 定义为 $\\Lambda$ 的特征值中严格小于给定数值阈值 $\\tau  0$ 的数量。该数值判据捕捉了后验分布中因精确或近奇异的精度而实际上无信息量的方向。使用 $\\tau = 10^{-12}$。\n\n您的程序必须：\n- 根据所述的有限元方法，从给定的非均匀网格组装精度矩阵 $Q$。\n- 对每个测试用例，构建 $H$，设置 $\\sigma$ 和 $\\alpha$，形成 $\\Lambda$，计算其特征值，并统计严格小于 $\\tau$ 的特征值数量以获得 $d$。\n\n使用以下测试套件，其旨在探查不同的行为：\n- 测试用例 1 (理想路径，完全观测)：网格节点为 $[0.0, 0.2, 0.5, 0.75, 1.0]$，因此 $M = 4$。使用 $\\alpha = 1.0$，$\\sigma = 0.2$ 和 $H = I_{4}$（$4 \\times 4$ 单位矩阵）。预期 $\\Lambda$ 是严格正定的，且 $d = 0$。\n- 测试用例 2 (仅有似然，秩亏)：使用相同网格。使用 $\\alpha = 0.0$，$\\sigma = 0.3$，以及一个有 $m = 2$ 行的 $H$ 来选择 $x$ 的第一个和第三个分量，具体为 $H = \\begin{bmatrix} 1  0  0  0 \\\\ 0  0  1  0 \\end{bmatrix}$。$H$ 的秩为 $2$，因此 $H^\\top H$ 的秩也为 $2$，在使用阈值 $\\tau$ 计数时，零空间维度应为 $M - 2 = 2$。\n- 测试用例 3 (因先验消失而近奇异，低秩数据)：使用相同网格。使用 $\\alpha = 10^{-16}$，$\\sigma = 0.1$，以及一个所有元素都为 1 的单行算子 $H = \\begin{bmatrix} 1  1  1  1 \\end{bmatrix}$。此时 $H^\\top H$ 的秩为 $1$，且先验的贡献 $\\alpha Q$ 极小，因此使用阈值 $\\tau$ 时，有效零空间维度应为 $M - 1 = 3$。\n\n最终输出格式要求：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[d_1,d_2,d_3]$，其中每个 $d_i$ 是第 $i$ 个测试用例的零空间维度。输出必须是整数，并且必须遵循上面列出的测试用例的顺序。", "solution": "该问题要求从具有混合边界条件的非均匀网格上的一维负拉普拉斯算子 $-\\Delta$ 的有限元离散化中，构建一个先验精度矩阵 $Q$。然后，该精度矩阵被用于贝叶斯线性逆问题中，以形成后验精度矩阵 $\\Lambda$。目标是确定后验零空间的维度，其在数值上定义为 $\\Lambda$ 的特征值中小于给定阈值 $\\tau$ 的数量。\n\n解决方案分四步进行：1) 为算子 $-\\Delta$ 组装全局刚度矩阵 $K$；2) 通过施加边界条件推导出先验精度矩阵 $Q$；3) 通过将先验与似然信息相结合来形成后验精度矩阵 $\\Lambda$；4) 通过特征值分析计算后验零空间维度 $d$。\n\n**第 1 步：组装全局刚度矩阵 $K$**\n\n域是一个一维区间 $[0, L]$，由 $M+1$ 个节点 $x_0, x_1, \\ldots, x_M$ 进行离散化，其中 $x_0 = 0$ 且 $x_M = L$。网格是非均匀的，单元长度为 $h_i = x_{i+1} - x_i$，其中 $i = 0, \\ldots, M-1$。\n\n负拉普拉斯算子使用线性有限元方法进行离散化。$-\\Delta u = f$ 的变分或“弱”形式涉及找到 $u$，使得对于所有测试函数 $v$，$\\int_0^L u'(x) v'(x) dx = \\int_0^L f(x) v(x) dx$ 成立，并满足边界条件。等式左边产生了刚度矩阵。\n\n每个单元 $e_i = [x_i, x_{i+1}]$ 对全局刚度矩阵的贡献由局部刚度矩阵 $K^{(e_i)}$ 给出。对于线性基函数，其形式为：\n$$\nK^{(e_i)} = \\frac{1}{h_i}\n\\begin{bmatrix}\n1  -1 \\\\\n-1  1\n\\end{bmatrix}\n$$\n全局刚度矩阵 $K \\in \\mathbb{R}^{(M+1) \\times (M+1)}$ 的组装方式是：首先将其初始化为零矩阵，然后累加所有单元的贡献。对于每个单元 $e_i$，$K^{(e_i)}$ 的条目被加到 $K$ 中对应的全局位置。具体而言，$2 \\times 2$ 的矩阵 $K^{(e_i)}$ 被加到 $K$ 中对应于节点 $i$ 和 $i+1$ 的子矩阵上。\n\n这个组装过程会得到一个对称的三对角矩阵 $K$。对于一个内部节点 $j \\in \\{1, \\ldots, M-1\\}$，其对角线元素为 $K_{j,j} = \\frac{1}{h_{j-1}} + \\frac{1}{h_j}$，非对角线元素为 $K_{j, j-1} = -\\frac{1}{h_{j-1}}$ 和 $K_{j, j+1} = -\\frac{1}{h_j}$。在边界处，$K_{0,0} = \\frac{1}{h_0}$ 和 $K_{M,M} = \\frac{1}{h_{M-1}}$。\n\n**第 2 步：构建先验精度矩阵 $Q$**\n\n问题指定了混合边界条件：左边界为狄利克雷（Dirichlet）条件 $u(0)=0$，右边界为诺伊曼（Neumann）条件 $u'(L)=0$。\n\n狄利克雷条件 $u(0)=0$ 固定了节点 $x_0$ 处的值。在离散系统中，这意味着与 $u(x_0)$ 相关的自由度被移除了。这是通过从全局刚度矩阵 $K$ 中消除对应于节点 $x_0$ 的行和列（即第一行和第一列，索引为0）来实现的。得到的矩阵即为关于未知节点值向量 $x = [u(x_1), \\ldots, u(x_M)]^\\top$ 的精度矩阵 $Q \\in \\mathbb{R}^{M \\times M}$。\n\n诺伊曼条件 $u'(L)=0$ 在这种有限元公式中是一个“自然”边界条件。它在组装过程中被自动满足，不需要对节点 $x_M$ 处的矩阵进行显式修改。\n\n得到的矩阵 $Q$ 是对称正定的，这是正态高斯分布的精度矩阵所必需的属性。其正定性源于一端的狄利克雷条件阻止了“浮动”或常数零空间模态的出现，而这种模态在两端都是纯诺伊曼条件时会存在。\n\n**第 3 步：形成后验精度矩阵 $\\Lambda$**\n\n该问题被置于贝叶斯框架内。未知向量 $x \\in \\mathbb{R}^M$ 上的 GMRF 先验由概率密度函数 $p(x) \\propto \\exp(-\\frac{\\alpha}{2} x^\\top Q x)$ 给出，其中 $Q$ 是上面推导出的先验精度矩阵，$\\alpha  0$ 是一个控制先验强度的标量超参数。\n\n数据通过线性模型 $y = Hx + \\varepsilon$ 与未知量相关联，其中 $H \\in \\mathbb{R}^{m \\times M}$ 是观测算子，$\\varepsilon$ 是高斯噪声，满足 $\\varepsilon \\sim \\mathcal{N}(0, R)$，其中 $R = \\sigma^2 I$。似然函数为 $p(y|x) \\propto \\exp(-\\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx))$。\n\n根据贝叶斯定理，后验概率密度与似然和先验的乘积成正比，即 $p(x|y) \\propto p(y|x)p(x)$。对于高斯分布，后验密度的指数是似然和先验指数的和。后验指数中关于 $x$ 的二次项定义了后验精度矩阵 $\\Lambda$。\n$$\n\\text{exponent} = -\\frac{1}{2} \\left( (y-Hx)^\\top R^{-1} (y-Hx) + \\alpha x^\\top Q x \\right)\n$$\n展开该项并收集关于 $x$ 的二次项可得：\n$$\n-\\frac{1}{2} \\left( x^\\top H^\\top R^{-1} H x + \\alpha x^\\top Q x - 2y^\\top R^{-1} Hx + y^\\top R^{-1} y \\right) = -\\frac{1}{2} x^\\top (\\alpha Q + H^\\top R^{-1} H) x + \\dots\n$$\n因此，后验精度矩阵为：\n$$\n\\Lambda = \\alpha Q + H^\\top R^{-1} H = \\alpha Q + \\frac{1}{\\sigma^2} H^\\top H\n$$\n该矩阵 $\\Lambda$ 是对称半正定的，因为它是两个对称半正定矩阵的和（并且如果 $\\alpha  0$，则为正定，因为 $Q$ 是正定的）。\n\n**第 4 步：计算后验零空间维度 $d$**\n\n后验零空间维度 $d$ 是衡量参数空间中未被数据或先验所告知的方向数量的指标。在数值上，它被定义为后验精度矩阵 $\\Lambda$ 的特征值中严格小于一个小的正阈值 $\\tau = 10^{-12}$ 的数量。小于此阈值的特征值表示后验分布在该方向上极其平坦，对应于非常高的不确定性。\n\n计算过程包括：\n1.  对每个测试用例，使用指定的参数 $\\alpha$、$\\sigma$、$Q$ 和 $H$ 构建矩阵 $\\Lambda$。\n2.  计算 $\\Lambda$ 的所有特征值。由于 $\\Lambda$ 是对称的，可以使用专门且数值稳定的算法。\n3.  统计计算出的特征值 $\\lambda_i$ 中满足 $\\lambda_i  \\tau$ 的数量。这个计数就是所求的维度 $d$。\n\n所提供的三个测试用例旨在探查不同的情景：一个良定系统（测试用例 1），一个仅由数据决定的欠定系统（测试用例 2），以及一个非常弱的先验未能正则化秩亏数据项的系统（测试用例 3）。每个案例的分析都遵循上述步骤。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of finding the posterior nullspace dimension for three\n    different linear inverse problem setups with a GMRF prior.\n    \"\"\"\n    \n    # Numerical threshold for counting eigenvalues in the nullspace.\n    tau = 1e-12\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (mesh_nodes, alpha, sigma, H_definition).\n    # H_definition is a lambda function to construct H for a given M.\n    test_cases = [\n        # Test case 1: happy path, fully observed\n        (\n            np.array([0.0, 0.2, 0.5, 0.75, 1.0]),\n            1.0,\n            0.2,\n            lambda M: np.identity(M)\n        ),\n        # Test case 2: likelihood-only, rank-deficient\n        (\n            np.array([0.0, 0.2, 0.5, 0.75, 1.0]),\n            0.0,\n            0.3,\n            lambda M: np.array([[1., 0., 0., 0.], [0., 0., 1., 0.]])\n        ),\n        # Test case 3: near-singular due to vanishing prior, low-rank data\n        (\n            np.array([0.0, 0.2, 0.5, 0.75, 1.0]),\n            1e-16,\n            0.1,\n            lambda M: np.array([[1., 1., 1., 1.]])\n        ),\n    ]\n\n    results = []\n    for nodes, alpha, sigma, get_H in test_cases:\n        # Determine the number of unknown nodes M\n        M = len(nodes) - 1\n\n        # Calculate element lengths h_i\n        h = np.diff(nodes)\n\n        # Assemble the (M+1)x(M+1) global stiffness matrix K\n        K = np.zeros((M + 1, M + 1))\n        for i in range(M):\n            h_i = h[i]\n            local_K = (1 / h_i) * np.array([[1, -1], [-1, 1]])\n            K[i:i+2, i:i+2] += local_K\n\n        # Enforce Dirichlet BC u(0)=0 by removing the first row and column\n        # to get the M x M prior precision matrix Q.\n        Q = K[1:, 1:]\n\n        # Construct the observation operator H for the current M\n        H = get_H(M)\n\n        # Form the posterior precision matrix Lambda\n        # Lambda = alpha * Q + (1/sigma^2) * H.T @ H\n        HTH = H.T @ H\n        Lambda = alpha * Q + (1 / sigma**2) * HTH\n\n        # Compute eigenvalues of the symmetric matrix Lambda.\n        # eigvalsh is efficient for symmetric matrices.\n        eigenvalues = np.linalg.eigvalsh(Lambda)\n\n        # Count the number of eigenvalues strictly less than the threshold tau.\n        # This is the posterior nullspace dimension d.\n        d = np.sum(eigenvalues  tau)\n        \n        results.append(int(d))\n\n    # Print the final results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3384822"}, {"introduction": "在复杂的逆问题中，先验和似然函数之间的相互作用可能导致意想不到的行为。这个高级练习深入探讨了“混叠”（aliasing）现象，即观测算子对某些变化模式不敏感，而这些模式在先验中也存在模糊性，这可能导致后验分布非单峰 [@problem_id:3384858]。通过构建和分析这种情况，您将对诊断和规避建模陷阱培养更深刻的直觉。", "problem": "考虑一个线性逆问题，其潜藏场为 $x \\in \\mathbb{R}^n$，观测算子为 $H \\in \\mathbb{R}^{m \\times n}$，观测数据为 $y \\in \\mathbb{R}^m$。数据模型为 $y = H x + \\eta$，其中 $\\eta \\sim \\mathcal{N}(0, R)$，噪声协方差 $R \\in \\mathbb{R}^{m \\times m}$ 是一个已知的对称正定矩阵。$x$ 的先验是一个高斯马尔可夫随机场 (GMRF)，其精度矩阵 $Q \\in \\mathbb{R}^{n \\times n}$ 由一个包含 $n$ 个节点的周期性一维晶格（环）的图拉普拉斯算子 $L \\in \\mathbb{R}^{n \\times n}$ 构建：$Q = \\tau L + \\varepsilon I_n$。其中 $\\tau  0$ 是一个平滑尺度，$\\varepsilon  0$ 是一个小的岭项，用于使 GMRF 成为一个真分布，而 $I_n$ 是单位矩阵。拉普拉斯算子 $L$ 由具有周期性边界条件的常用二阶差分模板定义。设 $v \\in \\mathbb{R}^n$ 为归一化常数向量（所有元素等于 $1/\\sqrt{n}$），它是 $L$ 最平滑的模式，并满足 $L v = 0$。\n\n您的任务是通过构建一个零空间包含 $Q$ 的平滑模式的 $H$ 来研究 $H$ 和 $Q$ 之间的混叠现象，并展示在非高斯先验下这将如何产生多峰后验。具体来说，考虑一个针对 $x$ 的混合 GMRF 先验，该先验由两个等权重、以 $\\pm \\alpha v$ 为中心的分量组成，两个分量共享相同的精度矩阵 $Q$：\n$$\np(x) = \\tfrac{1}{2}\\,\\mathcal{N}(x; +\\alpha v, Q^{-1}) + \\tfrac{1}{2}\\,\\mathcal{N}(x; -\\alpha v, Q^{-1}),\n$$\n其中 $\\alpha  0$ 控制了混合模式沿 $v$ 方向的分离程度。对于给定的 $(H, Q, R)$ 和观测到的 $y$，后验是根据贝叶斯法则得到的高斯混合分布：\n$$\np(x \\mid y) \\propto p(y \\mid x) \\, p(x),\n$$\n其中 $p(y \\mid x) = \\mathcal{N}(y; Hx, R)$。\n\n您必须：\n\n1. 根据贝叶斯法则和多元正态分布的基本性质，推导混合后验中每个高斯分量的后验精度矩阵和均值。您的推导应完全用 $H$、$Q$、$R$、$y$ 以及分量均值 $\\pm \\alpha v$ 来表示。不要跳过步骤，除了这些基本定义之外，不要使用任何快捷公式。\n\n2. 构建一个零空间包含 $v$ 的 $H$，具体为环上的一阶差分算子，定义为 $(H x)_i = x_{(i+1) \\bmod n} - x_i$，对于 $i = 0, \\ldots, n-1$。解释为什么 $H v = 0$，并说明这种混叠如何意味着在无噪声情况下，似然函数 $p(y \\mid x)$ 对于 $x$ 沿 $v$ 的任何标量倍数平移是不变的，从而当先验是集中在 $\\pm \\alpha v$ 的混合分布时，允许多峰后验行为的出现。\n\n3. 提出合理的先验来打破这种简并性，并解释它们恢复单峰性的机制。您必须至少考虑一种以 $0$ 为中心的单峰 GMRF 先验，并可选择性地讨论增加先验精度矩阵以惩罚零空间方向的方法。\n\n4. 实现一个程序，对于以下参数值的测试套件，构建 $(H, Q, R)$，模拟无噪声数据 $y = H x_{\\text{true}}$，计算混合后验分量均值、给定 $y$ 时两个分量的混合责任度，并根据以下标准输出一个布尔值，指示后验是否呈现多峰性：两个分量的责任度均至少为 $0.3$，并且两个分量后验均值之间的欧几里得距离严格大于一个固定的阈值 $d_{\\text{thr}} = 0.5$。如果两个条件都成立，则返回 true；否则，返回 false。\n\n使用以下测试套件，该套件探讨了混叠、打破简并性的先验以及非混叠观测：\n\n- 测试用例 1（混合先验下的混叠）：\n  - $n = 16$，$\\tau = 1.0$，$\\varepsilon = 10^{-6}$，$\\alpha = 3.0$，$\\sigma^2 = 10^{-3}$，$R = \\sigma^2 I_m$，$H$ 是环上的一阶差分算子（因此 $m = n$），$x_{\\text{true}} = 0$，$y = H x_{\\text{true}}$。\n\n- 测试用例 2（打破简并性的先验）：\n  - $n$、$\\tau$、$\\varepsilon$、$\\sigma^2$、$H$ 和 $R$ 与测试用例 1 相同，但使用单个单峰 GMRF 先验 $\\mathcal{N}(x; 0, Q^{-1})$（即非混合）。令 $x_{\\text{true}} = 0$，$y = H x_{\\text{true}}$。\n\n- 测试用例 3（混合先验下的非混叠观测）：\n  - $n$、$\\tau$、$\\varepsilon$、$\\alpha$、$\\sigma^2$ 与测试用例 1 相同，但取 $H = I_n$（单位矩阵，因此 $m = n$），$x_{\\text{true}} = +\\alpha v$，$y = H x_{\\text{true}}$。\n\n对于每个测试用例，您的程序必须根据所述标准判断后验是否为多峰，并按顺序输出这三个测试用例的布尔值。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[true,false,false]”）。\n\n此问题中不需要物理单位、角度单位或百分比。所有量都是无量纲的纯数学量。\n\n您的程序必须是一个完整、可运行的实现，能够执行上述计算并以所需格式打印最终结果。程序不得依赖外部输入。它必须使用基于上述定义和性质的标准线性代数技术，并且必须避免使用任何未从这些基本基础上推导出的快捷方式。", "solution": "该问题陈述经过仔细审查，并被确定为有效。它在贝叶斯统计和逆问题领域具有科学依据，问题设定良好，提供了所有必要信息，并使用客观、精确的语言进行表述。所有有效问题的条件均已满足。我们现在开始进行解答。\n\n该问题要求一个由三部分组成的理论阐述，以及一个计算实现。我们将按顺序处理每个部分。\n\n### 1. 混合后验的推导\n\n我们给定一个具有高斯似然和混合 GMRF 先验的线性逆问题。\n似然函数指定为 $p(y \\mid x) = \\mathcal{N}(y; Hx, R)$，就其概率密度函数而言，它正比于：\n$$\np(y \\mid x) \\propto \\exp\\left(-\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\\right)\n$$\n先验是两个权重相等（$w_1 = w_2 = \\frac{1}{2}$）的高斯分量的混合：\n$$\np(x) = \\frac{1}{2}\\,\\mathcal{N}(x; \\mu_1, Q^{-1}) + \\frac{1}{2}\\,\\mathcal{N}(x; \\mu_2, Q^{-1})\n$$\n其中 $\\mu_1 = +\\alpha v$ 和 $\\mu_2 = -\\alpha v$。对于 $k \\in \\{1, 2\\}$，第 $k$ 个先验分量是 $p_k(x) = \\mathcal{N}(x; \\mu_k, Q^{-1})$，它正比于：\n$$\np_k(x) \\propto \\exp\\left(-\\frac{1}{2}(x - \\mu_k)^T Q (x - \\mu_k)\\right)\n$$\n后验分布 $p(x \\mid y)$ 通过贝叶斯法则获得：\n$$\np(x \\mid y) = \\frac{p(y \\mid x) p(x)}{p(y)} \\propto p(y \\mid x) p(x)\n$$\n代入混合先验，我们得到：\n$$\np(x \\mid y) \\propto p(y \\mid x) \\left(\\frac{1}{2} p_1(x) + \\frac{1}{2} p_2(x)\\right) = \\frac{1}{2} p(y \\mid x) p_1(x) + \\frac{1}{2} p(y \\mid x) p_2(x)\n$$\n这表明后验也是两个分量的混合。让我们找出第 $k$ 个未归一化的后验分量 $p(y \\mid x) p_k(x)$ 的形式。它的对数是该分量的对数似然和对数先验之和：\n$$\n\\log(p(y \\mid x) p_k(x)) = C - \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx) - \\frac{1}{2}(x - \\mu_k)^T Q (x - \\mu_k)\n$$\n其中 $C$ 是一个常数。我们展开二次型：\n$$\n= C - \\frac{1}{2}(y^T R^{-1} y - 2y^T R^{-1} Hx + x^T H^T R^{-1} Hx) - \\frac{1}{2}(x^T Qx - 2x^T Q\\mu_k + \\mu_k^T Q\\mu_k)\n$$\n按 $x$ 的幂次分组各项：\n$$\n= C' - \\frac{1}{2} \\left[ x^T(H^T R^{-1} H + Q)x - 2(y^T R^{-1} H + \\mu_k^T Q)x \\right]\n$$\n这是 $x$ 的一个二次函数，表明分量 $p(y \\mid x) p_k(x)$ 与高斯密度成正比。我们可以通过配方法来确定其精度矩阵和均值。高斯对数密度的形式是 $-\\frac{1}{2}(x - \\mu')^T Q' (x - \\mu') + C'' = -\\frac{1}{2}(x^T Q'x - 2x^T Q'\\mu') + C'''$。\n\n通过比较 $x$ 的二次项和线性项，我们可以找到第 $k$ 个分量的后验精度矩阵 $Q'$ 和后验均值向量 $\\mu'_k$。\n\n后验精度矩阵 $Q'$ 是二次项 $x^T(\\cdot)x$ 的矩阵：\n$$\nQ' = H^T R^{-1} H + Q\n$$\n这个精度矩阵对于两个后验分量是共同的，因为它不依赖于先验均值 $\\mu_k$。\n\n后验均值 $\\mu'_k$ 通过 equating 线性项找到：\n$$\n2x^T Q' \\mu'_k = 2(y^T R^{-1} H + \\mu_k^T Q)x\n$$\n取 $x^T$ 系数的转置：\n$$\nQ' \\mu'_k = (y^T R^{-1} H)^T + ( \\mu_k^T Q)^T = H^T R^{-1} y + Q \\mu_k\n$$\n解出 $\\mu'_k$ 得到分量 $k$ 的后验均值：\n$$\n\\mu'_k = (Q')^{-1} (H^T R^{-1} y + Q \\mu_k)\n$$\n因此，后验是一个混合分布 $p(x \\mid y) = w'_1 \\mathcal{N}(x; \\mu'_1, (Q')^{-1}) + w'_2 \\mathcal{N}(x; \\mu'_2, (Q')^{-1})$，其中 $w'_1$ 和 $w'_2$ 是后验权重，或称责任度，我们不需要为问题的这一部分推导它们。\n\n### 2. $H$ 和 $Q$ 之间的混叠\n\n问题指定了观测算子 $H$ 的构建，即周期性 $n$ 节点晶格上的一阶差分算子：\n$$\n(Hx)_i = x_{(i+1) \\bmod n} - x_i \\quad \\text{for } i = 0, \\dots, n-1\n$$\n向量 $v \\in \\mathbb{R}^n$ 是归一化常数向量，其所有元素等于 $1/\\sqrt{n}$。让我们证明 $v$ 位于 $H$ 的零空间中：\n$$\n(Hv)_i = v_{(i+1) \\bmod n} - v_i = \\frac{1}{\\sqrt{n}} - \\frac{1}{\\sqrt{n}} = 0\n$$\n由于这对所有 $i = 0, \\dots, n-1$ 都成立，我们有 $Hv = 0$。\n\n这个性质导致了混叠。似然函数 $p(y \\mid x)$ 量化了给定状态 $x$ 对数据 $y$ 的解释程度。考虑一个状态 $x$ 和一个新状态 $x_{\\text{shifted}} = x + c v$（对于某个标量 $c \\in \\mathbb{R}$）。应用于这个平移后状态的观测算子是：\n$$\nH x_{\\text{shifted}} = H(x + c v) = Hx + c(Hv) = Hx + c \\cdot 0 = Hx\n$$\n预测的观测值 $Hx$ 不因 $x$ 沿 $v$ 方向的平移而改变。因此，似然函数也是不变的：\n$$\np(y \\mid x_{\\text{shifted}}) = \\mathcal{N}(y; Hx_{\\text{shifted}}, R) = \\mathcal{N}(y; Hx, R) = p(y \\mid x)\n$$\n数据没有提供任何信息来区分 $x$ 和直线 $x+cv$ 上的任何点。算子 $H$ 对 $x$ 沿 $v$ 方向的分量是“盲”的。\n\n先验精度矩阵为 $Q = \\tau L + \\varepsilon I_n$。给定 $Lv=0$，我们有 $Qv = \\tau(Lv) + \\varepsilon(I_n v) = \\varepsilon v$。在 $v$ 方向上的精度是 $\\varepsilon$，这是一个很小的正数。这意味着该方向上的先验方差（与 $1/\\varepsilon$ 成正比）非常大。因此，先验在 $v$ 方向上非常不确定或“平坦”。\n\n混合 GMRF 先验是双峰的，中心位于 $\\mu_1 = +\\alpha v$ 和 $\\mu_2 = -\\alpha v$。这些模式之间的分离完全位于向量 $v$ 上。当我们将这个先验与似然函数结合时，似然函数沿 $v$ 的不变性意味着它不能偏好任何一个模式。如果数据 $y$ 与 $H$ 的零空间一致（例如 $y=0$，这与 $H\\mu_1=0$ 和 $H\\mu_2=0$ 都一致），后验分布将继承先验的双峰性。数据未能解决由先验引入的模糊性，这种现象称为混叠。\n\n### 3. 打破简并性的合理先验\n\n混叠和由此产生的后验多峰性之所以出现，是因为数据在 $H$ 的零空间中不包含任何信息，而先验在同一方向上是无信息的（或模糊的）。可以通过使用在零空间中提供明确信息的先验来打破这种简并性。\n\n**方法1：单峰 GMRF 先验**\n最简单的方法是用一个以单一、无歧义值为中心的单峰先验替换双峰混合先验。一个标准的选择是零均值 GMRF：\n$$\np(x) = \\mathcal{N}(x; 0, Q^{-1})\n$$\n这个先验在 $x=0$ 处有一个单一的峰值。当与高斯似然函数结合时，得到的后验也是一个单一的高斯分布。$+\\alpha v$ 和 $-\\alpha v$ 之间的模糊性得到了解决，因为先验明确表示了对接近原点的解的偏好。后验均值变为 $\\mu' = (H^T R^{-1} H + Q)^{-1} H^T R^{-1} y$，这是一个唯一的解（线性高斯模型的标准 MAP 估计）。这通过在数据未约束的方向上将解拉向先验均值，有效地对逆问题进行了正则化。\n\n**方法2：增强先验精度矩阵**\n一种更具针对性的方法是修改精度矩阵 $Q$ 以专门惩罚零空间方向 $v$。原始精度矩阵 $Q = \\tau L + \\varepsilon I_n$ 在 $v$ 方向上非常弱，因为 $Qv = \\varepsilon v$。我们可以通过向 $Q$ 添加一个秩一更新来引入更强的惩罚：\n$$\nQ_{\\text{new}} = Q + \\gamma v v^T = \\tau L + \\varepsilon I_n + \\gamma v v^T\n$$\n其中 $\\gamma  0$ 是一个控制惩罚强度的超参数。项 $v v^T$ 是到由 $v$ 张成的子空间上的投影算子。这个新精度矩阵对 $v$ 的作用是：\n$$\nQ_{\\text{new}} v = (\\tau L + \\varepsilon I_n + \\gamma v v^T) v = \\tau Lv + \\varepsilon v + \\gamma v(v^T v)\n$$\n由于 $v$ 是归一化的（$v^T v = 1$）且 $Lv=0$，这可以简化为：\n$$\nQ_{\\text{new}} v = (\\varepsilon + \\gamma) v\n$$\n通过选择 $\\gamma \\gg \\varepsilon$，我们可以显著增加先验在 $v$ 方向上的精度（即减小方差）。使用零均值先验 $\\mathcal{N}(x; 0, Q_{\\text{new}}^{-1})$，这个先验强烈抑制了在 $v$ 方向上有大分量的解。它有效地施加了一个“弱约束”，即场 $x$ 的平均值（与 $x \\cdot v$ 成正比）应接近于 $0$，从而解决了 $\\pm\\alpha v$ 模式之间的模糊性并恢复了后验的单峰性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1: Aliasing with mixture prior\n        {\n            \"n\": 16, \"tau\": 1.0, \"eps\": 1e-6, \"alpha\": 3.0, \"sigma2\": 1e-3,\n            \"prior_type\": \"mixture\", \"H_type\": \"diff\", \"xtrue_type\": \"zero\"\n        },\n        # Test Case 2: Degeneracy-breaking prior\n        {\n            \"n\": 16, \"tau\": 1.0, \"eps\": 1e-6, \"alpha\": 3.0, \"sigma2\": 1e-3,\n            \"prior_type\": \"unimodal\", \"H_type\": \"diff\", \"xtrue_type\": \"zero\"\n        },\n        # Test Case 3: Non-aliased observation with mixture prior\n        {\n            \"n\": 16, \"tau\": 1.0, \"eps\": 1e-6, \"alpha\": 3.0, \"sigma2\": 1e-3,\n            \"prior_type\": \"mixture\", \"H_type\": \"identity\", \"xtrue_type\": \"alphav\"\n        },\n    ]\n\n    results = []\n    d_thr = 0.5\n    resp_thr = 0.3\n\n    for case in test_cases:\n        # Unpack parameters\n        n, tau, eps, alpha, sigma2 = case[\"n\"], case[\"tau\"], case[\"eps\"], case[\"alpha\"], case[\"sigma2\"]\n        prior_type, H_type, xtrue_type = case[\"prior_type\"], case[\"H_type\"], case[\"xtrue_type\"]\n\n        # --- Problem Setup ---\n        \n        # 1. Define vectors and base matrices\n        v = np.ones(n) / np.sqrt(n)\n        Id_n = np.identity(n)\n\n        # 2. Construct prior precision Q\n        L = np.zeros((n, n))\n        for i in range(n):\n            L[i, i] = 2.0\n            L[i, (i - 1 + n) % n] = -1.0\n            L[i, (i + 1) % n] = -1.0\n        Q = tau * L + eps * Id_n\n\n        # 3. Construct observation operator H and noise covariance R\n        if H_type == \"diff\":\n            m = n\n            H = np.zeros((n, n))\n            for i in range(n):\n                H[i, i] = -1.0\n                H[i, (i + 1) % n] = 1.0\n        elif H_type == \"identity\":\n            m = n\n            H = np.identity(n)\n        \n        R = sigma2 * np.identity(m)\n        R_inv = (1.0 / sigma2) * np.identity(m)\n\n        # 4. Simulate true state and observed data y\n        if xtrue_type == \"zero\":\n            x_true = np.zeros(n)\n        elif xtrue_type == \"alphav\":\n            x_true = alpha * v\n        \n        y = H @ x_true # Noiseless data\n\n        # --- Posterior Analysis ---\n\n        if prior_type == \"unimodal\":\n            # A unimodal prior results in a unimodal posterior.\n            # The criterion for multimodality requires two components.\n            results.append(False)\n            continue\n\n        # For mixture prior:\n        # 1. Define prior component means\n        mu1 = alpha * v\n        mu2 = -alpha * v\n\n        # 2. Compute posterior precision (common to both components)\n        Q_post = H.T @ R_inv @ H + Q\n        Q_post_inv = linalg.inv(Q_post)\n\n        # 3. Compute posterior means for each component\n        mu1_post = Q_post_inv @ (H.T @ R_inv @ y + Q @ mu1)\n        mu2_post = Q_post_inv @ (H.T @ R_inv @ y + Q @ mu2)\n\n        # 4. Compute posterior component responsibilities (weights)\n        # Log of marginal likelihood for component k is proportional to -1/2 * J_k\n        # J_k = y'R_inv*y + mu_k'*Q*mu_k - (H'R_inv*y+Q*mu_k)'*Q_post_inv*(H'R_inv*y+Q*mu_k)\n        \n        term_y = y.T @ R_inv @ y\n        \n        # Component 1\n        term_mu1 = mu1.T @ Q @ mu1\n        linear_term1 = H.T @ R_inv @ y + Q @ mu1\n        term_post1 = linear_term1.T @ Q_post_inv @ linear_term1\n        J1 = term_y + term_mu1 - term_post1\n        \n        # Component 2\n        term_mu2 = mu2.T @ Q @ mu2\n        linear_term2 = H.T @ R_inv @ y + Q @ mu2\n        term_post2 = linear_term2.T @ Q_post_inv @ linear_term2\n        J2 = term_y + term_mu2 - term_post2\n\n        # Responsibilities are computed from the log evidence difference\n        # log(Z1/Z2) = -0.5 * (J1 - J2)\n        # w1 = Z1/(Z1+Z2) = 1 / (1 + Z2/Z1) = 1 / (1 + exp(-log(Z1/Z2)))\n        log_Z_ratio = -0.5 * (J1 - J2)\n        \n        # Use log-sum-exp trick for numerical stability, though simpler form works here\n        w1_post = 1.0 / (1.0 + np.exp(-log_Z_ratio))\n        w2_post = 1.0 - w1_post\n        \n        # --- Check multimodality criterion ---\n        \n        # Condition 1: Responsibilities\n        is_resp_balanced = (w1_post >= resp_thr) and (w2_post >= resp_thr)\n        \n        # Condition 2: Distance between posterior means\n        dist_means = linalg.norm(mu1_post - mu2_post)\n        is_dist_large = dist_means > d_thr\n\n        is_multimodal = is_resp_balanced and is_dist_large\n        results.append(is_multimodal)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\".lower())\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3384858"}]}