{"hands_on_practices": [{"introduction": "构建最大熵先验并不仅仅是一个机械的数学过程，我们施加的约束必须在数学上与变量的支撑集和基本测度相容。这个练习旨在挑战你的批判性思维，让你思考在何种情况下，一组约束才是“有意义”的。通过分析不同的情景，你将学会判断约束的有效性，并理解它对最终得到的分布（例如其尾部行为）意味着什么。[@problem_id:3401709]", "problem": "考虑在一个反问题和数据同化（DA）的背景下，为一个标量状态变量 $x$ 构建一个最大熵先验。设基测度 $\\mu$ 是预先指定且固定的。一个密度 $p$ 是可接受的，当且仅当它相对于 $\\mu$ 是绝对连续的，并且其支撑集为该状态规定的支撑集 $S$。需要最大化的熵是相对于基测度的香农熵，对于连续变量 $x$ 其表达式为 $H(p) = -\\int_{S} p(x)\\log\\left(\\frac{p(x)}{d\\mu/dx}\\right)\\,d\\mu(x)$（其中 $d\\mu/dx$ 被理解为拉东-尼科迪姆导数），对于计数测度则有其离散形式的类比。约束通常是形如 $\\int_{S} f_i(x)\\,p(x)\\,d\\mu(x) = c_i$ 的线性期望约束，其中的常数 $c_i$ 由物理或统计知识提供。一个约束有意义，当且仅当其定义积分收敛且可行集非空。\n\n要求您根据这些原则，推断约束与支撑集和基测度的相容性，并通过一个积分 $\\int_{S} x^2 p(x)\\,d\\mu(x)$ 是有限还是无限的案例来说明。考虑在勒贝格(Lebesgue)基测度下 $\\mathbb{R}$ 上的两个候选先验：一个柯西(Cauchy)先验 $p_1(x) = \\frac{1}{\\pi}\\frac{1}{1+x^2}$ 和一个高斯(Gaussian)先验 $p_2(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{x^2}{2\\sigma^2}\\right)$，其中 $\\sigma>0$。同时考虑物理约束 $x \\ge 0$ 且基测度为 $[0,\\infty)$ 上的勒贝格测度的情况，以及在 $\\mathbb{Z}$ 上取整数值的状态变量和计数测度的情况。\n\n选择所有正确的陈述：\n\nA. 在 $\\mathbb{R}$ 上的勒贝格基测度下，施加约束 $\\int_{\\mathbb{R}} x^2 p(x)\\,dx   \\infty$ 会排除任何具有渐近幂律尾部 $p(x) \\sim c\\,|x|^{-\\alpha}$（当 $|x|\\to\\infty$ 时）且 $\\alpha \\le 3$ 的先验；特别地，有限二阶矩要求 $\\alpha3$。\n\nB. 对于 $\\mathbb{R}$ 上的柯西(Cauchy)先验 $p_1(x)$，积分 $\\int_{\\mathbb{R}} x^2 p_1(x)\\,dx$ 发散；因此，约束 $\\int_{\\mathbb{R}} x^2 p(x)\\,dx = m$（其中 $m  \\infty$）与选择 $p_1$ 作为先验是不相容的。\n\nC. 如果物理约束 $x \\ge 0$ 且基测度是 $[0,\\infty)$ 上的勒贝格测度，那么约束 $\\int_{0}^{\\infty} x\\,p(x)\\,dx = 0$ 与支撑集不相容，除非 $p$ 是在 $x=0$ 处的狄拉克(Dirac) delta函数，而它相对于基测度不是绝对连续的。\n\nD. 在具有计数测度的 $\\mathbb{Z}$ 上，任何概率质量函数 $p(k)$ 都自动满足 $\\sum_{k\\in\\mathbb{Z}} k^2 p(k)  \\infty$（一个有限的二阶矩），因此对于二阶矩约束不存在相容性问题。\n\nE. 当基测度是 $\\mathbb{R}$ 上的勒贝格测度，且唯一的约束是 $\\int_{\\mathbb{R}} x^2 p(x)\\,dx = \\sigma^2$（其中 $\\sigma^2  \\infty$）时，最大熵解必然比任何高斯分布都具有更重的尾部，因此不可能是高斯分布。", "solution": "问题陈述在科学上是合理的、适定的和客观的。它提供了一个用于选择先验概率分布的标准信息论框架，并要求基于这些原则对几个陈述进行评估。所有术语在数学和统计学中都有明确的定义。所呈现的场景是标准的，并可作为这些概念的有效说明。因此，该问题是有效的，我将继续进行分析。\n\n对各项陈述的分析：\n\nA. 该陈述涉及具有幂律尾部的先验具有有限二阶矩的条件。约束为 $\\int_{\\mathbb{R}} x^2 p(x)\\,dx   \\infty$。该先验的渐近行为为 $p(x) \\sim c\\,|x|^{-\\alpha}$（当 $|x|\\to\\infty$ 时），其中常数 $c  0$。对于大的 $|x|$，被积函数的行为如同 $x^2 p(x) \\sim c\\,x^2 |x|^{-\\alpha} = c\\,|x|^{2-\\alpha}$。\n为了使积分 $\\int_{-\\infty}^{\\infty} |x|^{2-\\alpha}\\,dx$ 收敛，被积函数必须足够快地衰减。根据在 $(-\\infty, \\infty)$ 上的反常积分的p-判别法，指数必须严格小于 $-1$。\n因此，我们需要条件 $2 - \\alpha  -1$，可简化为 $3  \\alpha$。\n如果 $\\alpha \\le 3$，此条件不满足，二阶矩的积分发散。例如，如果 $\\alpha = 3$，被积函数的行为如同 $|x|^{-1}$，其积分是对数形式的，会发散。如果 $\\alpha  3$，被积函数衰减得更慢，积分也会发散。\n因此，施加有限二阶矩约束会排除尾部行为为 $p(x) \\sim c\\,|x|^{-\\alpha}$ 且 $\\alpha \\le 3$ 的先验。该陈述正确地指出，有限二阶矩要求 $\\alpha  3$。\n结论：**正确**。\n\nB. 该陈述考察了柯西(Cauchy)分布与有限二阶矩约束的相容性。柯西先验由 $p_1(x) = \\frac{1}{\\pi(1+x^2)}$ 给出。我们必须计算其二阶矩，即 $\\int_{\\mathbb{R}} x^2 p_1(x)\\,dx$。\n该积分为：\n$$ \\int_{-\\infty}^{\\infty} x^2 p_1(x)\\,dx = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\pi(1+x^2)}\\,dx = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{x^2}{1+x^2}\\,dx $$\n被积函数 $\\frac{x^2}{1+x^2}$ 可以重写为 $\\frac{1+x^2-1}{1+x^2} = 1 - \\frac{1}{1+x^2}$。\n当 $|x| \\to \\infty$ 时，被积函数的极限是 $\\lim_{|x|\\to\\infty} \\frac{x^2}{1+x^2} = 1$。\n由于当 $|x|\\to\\infty$ 时被积函数不趋于 $0$，在无限域上的积分必然发散。因此，柯西分布的二阶矩是无限的。\n问题陈述中提到，一个约束要有意义，其定义积分必须收敛。约束 $\\int_{\\mathbb{R}} x^2 p(x)\\,dx = m  \\infty$ 定义了一个具有有限二阶矩的分布 $p$ 的可行集。柯西分布 $p_1(x)$ 不属于这个集合。因此，如果施加此约束，柯西分布就不是最大熵先验的可接受候选者。该陈述是对这种不相容性的正确评估。\n结论：**正确**。\n\nC. 该陈述考虑了对状态变量的非负约束 $x \\ge 0$，基测度为 $[0, \\infty)$ 上的勒贝格测度。约束是关于一阶矩的：$\\int_{0}^{\\infty} x\\,p(x)\\,dx = 0$。\n概率密度函数 $p(x)$ 在其支撑集 $[0, \\infty)$ 中的所有 $x$ 处都必须是非负的，即 $p(x) \\ge 0$。积分变量 $x$ 在这个区间上也是非负的。因此，被积函数 $x\\,p(x)$ 对于所有 $x \\in [0, \\infty)$ 都是一个非负函数。\n一个非负函数在其定义域上的积分等于零，当且仅当该函数几乎处处为零。这意味着我们必须有 $x\\,p(x) = 0$ 对几乎所有的 $x \\in [0, \\infty)$ 成立。\n对于任何 $x  0$，这意味着 $p(x) = 0$。为了使总概率 $\\int_0^\\infty p(x)\\,dx$ 等于 $1$，全部的概率质量必须集中在单一点 $x=0$ 上。唯一满足此条件的“分布”是狄拉克(Dirac) delta分布，即 $p(x) = \\delta(x)$。\n然而，问题规定一个可接受的密度 $p$ 必须“相对于 $\\mu$ 是绝对连续的”，这里的 $\\mu$ 是勒贝格测度。狄拉克delta分布相对于勒贝格测度不是绝对连续的；它甚至不是传统意义上的函数，其累积分布函数也不可微。\n因此，没有可接受的密度函数能够满足约束 $\\int_{0}^{\\infty} x\\,p(x)\\,dx = 0$。该约束与问题对可接受性的要求不相容。该陈述正确地指出了这种情况。\n结论：**正确**。\n\nD. 该陈述声称，对于整数集 $\\mathbb{Z}$ 上的任何概率质量函数（PMF）$p(k)$（使用计数测度），其二阶矩 $\\sum_{k\\in\\mathbb{Z}} k^2 p(k)$ 自动是有限的。这是一个错误的断言。我们可以构造一个反例。\n一个有效的PMF需要满足 $p(k) \\ge 0$ 和 $\\sum_{k\\in\\mathbb{Z}} p(k) = 1$。这个级数的收敛性并不能保证二阶矩级数的收敛性。\n考虑一个具有幂律尾部的PMF。为了使PMF可归一化，即 $\\sum_{k\\ne 0} |k|^{-\\alpha}  \\infty$，我们需要 $\\alpha > 1$（通过与p级数类比）。\n二阶矩是 $\\sum_{k\\in\\mathbb{Z}} k^2 p(k)$。如果 $p(k) \\propto |k|^{-\\alpha}$，则二阶矩级数中的项的行为如同 $k^2 |k|^{-\\alpha} = |k|^{2-\\alpha}$。为了使该级数收敛，我们需要指数小于 $-1$，即 $2-\\alpha  -1$，这意味着 $\\alpha > 3$。\n因此，如果我们选择一个满足 $1  \\alpha \\le 3$ 的PMF，它将是可归一化的，但其二阶矩将是无限的。例如，令 $p(k) = \\frac{C}{|k|^3}$（对于 $k \\ne 0$）和 $p(0) = 1-2C\\zeta(3)$，其中 $\\zeta(s)$ 是黎曼(Riemann) zeta函数，而 $C = (2\\zeta(3))^{-1}$ 是一个归一化常数，其选择是为了使非零整数上的和接近1。\n对于此PMF，其二阶矩的级数为 $\\sum_k k^2 p(k) = C \\sum_{k\\ne 0} k^2 |k|^{-3} = C \\sum_{k\\ne 0} |k|^{-1} = 2C \\sum_{k=1}^\\infty \\frac{1}{k}$。这是调和级数的一个倍数，而调和级数是发散的。\n由于一个有效的PMF可以有无限的二阶矩，所以该陈述是错误的。\n结论：**错误**。\n\nE. 该陈述涉及在 $\\mathbb{R}$ 上使用勒贝格基测度，并在给定二阶矩约束 $\\int_{\\mathbb{R}} x^2 p(x)\\,dx = \\sigma^2  \\infty$ 的条件下最大化熵的结果。\n这是一个最大熵变分法中的经典问题。我们需要在约束 $\\int p(x)\\,dx = 1$ 和 $\\int x^2 p(x)\\,dx = \\sigma^2$ 下，最大化 $H(p) = -\\int p(x)\\log p(x)\\,dx$。\n使用拉格朗日乘子法，解出的 $p(x)$ 具有以下形式：\n$$ p(x) = \\exp(-\\lambda_0 - 1 - \\lambda_1 x^2) = C \\exp(-\\lambda_1 x^2) $$\n其中 $C$ 和 $\\lambda_1$ 是由约束决定的常数。这是一个中心化高斯分布的函数形式。为了使二阶矩有限，我们必须有 $\\lambda_1  0$。归一化约束 $\\int p(x)\\,dx = 1$ 给出 $C\\sqrt{\\pi/\\lambda_1} = 1$。二阶矩约束 $\\int x^2 p(x)\\,dx = \\sigma^2$ 给出 $C\\frac{\\sqrt{\\pi}}{2\\lambda_1^{3/2}} = \\sigma^2$。解这些方程得到 $\\lambda_1 = \\frac{1}{2\\sigma^2}$ 和 $C = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}$。最终的最大熵分布是：\n$$ p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) $$\n这是均值为 $0$、方差为 $\\sigma^2$ 的高斯分布的概率密度函数。\n该陈述声称解“不可能是高斯分布”并且比“任何高斯分布都具有更重的尾部”。这与正确结果完全相反。高斯分布正是在给定方差下使熵最大化的分布。其尾部以 $\\exp(-x^2)$ 的形式衰减，这被认为是“轻尾”而非“重尾”。\n结论：**错误**。", "answer": "$$\\boxed{ABC}$$", "id": "3401709"}, {"introduction": "对于简单的约束，我们可以解析地推导出最大熵先验，但大多数拥有多个复杂约束的现实世界问题则需要依赖数值方法。本练习将为你提供一个实现广义迭代尺度法（Generalized Iterative Scaling, GIS）的动手经验。这是一个经典且强大的算法，用于寻找离散最大熵模型的参数 $\\lambda$，以使其满足一组给定的矩约束。[@problem_id:3401753]", "problem": "在一个离散样本空间上，提出了一个带有最大熵先验的有限逆问题。设离散空间为 $\\mathcal{X} = \\{1, \\dots, n\\}$。给定一个严格为正的基测度 $h : \\mathcal{X} \\to \\mathbb{R}_{0}$ 和 $m$ 个非负特征的集合 $f_i : \\mathcal{X} \\to \\mathbb{R}_{\\ge 0}$（$i \\in \\{1, \\dots, m\\}$）。考虑在 $\\mathcal{X}$ 上由 $\\lambda \\in \\mathbb{R}^m$ 索引的对数线性先验族 $p_\\lambda$，其形式为\n$$\np_\\lambda(x) = \\frac{h(x)\\exp\\left(\\sum_{i=1}^m \\lambda_i f_i(x)\\right)}{Z(\\lambda)}, \\quad Z(\\lambda) = \\sum_{x \\in \\mathcal{X}} h(x)\\exp\\left(\\sum_{i=1}^m \\lambda_i f_i(x)\\right).\n$$\n给定目标特征期望 $c_i \\in \\mathbb{R}_{0}$（$i \\in \\{1,\\dots,m\\}$），目标是调整参数向量 $\\lambda$，使得对于每个特征，在 $p_\\lambda$ 下的模型期望满足\n$$\n\\mathbb{E}_{p_\\lambda}[f_i] = \\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x) = c_i.\n$$\n假设特征是有界的，并且它们在 $\\mathcal{X}$ 上的和是常数，即存在一个常数 $C \\in \\mathbb{R}_{0}$ 使得\n$$\n\\sum_{i=1}^m f_i(x) = C \\quad \\text{for all } x \\in \\mathcal{X}.\n$$\n在此有界性假设下，实现广义迭代尺度法（Generalized Iterative Scaling, GIS），该方法以一种强制期望值逼近目标值 $c_i$ 的方式更新参数向量 $\\lambda$。您的实现必须：\n- 按照指定方式构造 $p_\\lambda$。\n- 迭代更新参数，直到最大绝对约束违反量\n$$\nr = \\max_{1\\le i \\le m} \\left| \\mathbb{E}_{p_\\lambda}[f_i] - c_i \\right|\n$$\n小于一个容差或达到最大迭代次数。\n- 对 $p_\\lambda$ 使用数值稳定的归一化。\n- 对每个测试用例，返回 $r$ 的最终值。\n\n在您的解决方案中，解释为什么该算法在凸性和有界性假设下会收敛，以及为什么即使在特征线性相关的情况下，解在导出的先验 $p_\\lambda$ 方面仍然是唯一的。\n\n实现您的程序来解决以下测试套件。对于每个测试用例，程序必须计算 $r$ 的最终值，并按照下面指定的格式将结果聚合到单行输出中。\n\n测试用例 $1$（正常情况）：\n- 规模：$n = 5$， $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：将 $\\mathcal{X}$ 划分为三组，使用独热指示函数，得到恒定和 $C = 1$。具体地，\n  - 当 $x \\in \\{1,2\\}$ 时，$f_1(x) = 1$，否则 $f_1(x) = 0$。\n  - 当 $x \\in \\{3,4\\}$ 时，$f_2(x) = 1$，否则 $f_2(x) = 0$。\n  - 当 $x \\in \\{5\\}$ 时，$f_3(x) = 1$，否则 $f_3(x) = 0$。\n- 目标值：$c = [\\,0.4,\\,0.4,\\,0.2\\,]$。\n\n测试用例 $2$（边界与极端目标质量）：\n- 规模：$n = 5$， $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：与测试用例 $1$ 相同，因此 $C = 1$。\n- 目标值：$c = [\\,10^{-6},\\,0.999,\\,0.000999\\,]$。\n\n测试用例 $3$（具有恒定和的冗余特征）：\n- 规模：$n = 4$， $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：两个独热指示函数和一个冗余特征，确保恒定和 $C = 2$：\n  - 当 $x \\in \\{1,2\\}$ 时，$f_1(x) = 1$，否则 $f_1(x) = 0$。\n  - 当 $x \\in \\{3,4\\}$ 时，$f_2(x) = 1$，否则 $f_2(x) = 0$。\n  - 对于所有 $x \\in \\{1,2,3,4\\}$，$f_3(x) = 1$。\n- 目标值：$c = [\\,0.5,\\,0.5,\\,1.0\\,]$。\n\n测试用例 $4$（不可行约束）：\n- 规模：$n = 5$， $m = 3$。\n- 基测度：对于所有 $x$，$h(x) = 1$。\n- 特征：与测试用例 $1$ 相同，因此 $C = 1$。\n- 目标值：$c = [\\,0.3,\\,0.3,\\,0.3\\,]$。\n\n实现细节：\n- 在 $\\lambda = \\mathbf{0} \\in \\mathbb{R}^m$ 处初始化 $\\lambda$。\n- 对 $r$ 使用 $10^{-10}$ 的停止容差，最大迭代次数为 $5000$ 次。\n- 使用数值稳定的归一化方法构造 $p_\\lambda$，以避免溢出或下溢。\n- 对于每个测试用例，计算迭代后的最终 $r$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。例如，如果有四个测试用例，输出形式应为 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是一个浮点数，表示测试用例 $i$ 的最终最大绝对约束违反量。", "solution": "手头的问题是，在一个离散空间 $\\mathcal{X} = \\{1, \\dots, n\\}$ 上，找到最大熵先验分布 $p_\\lambda$ 的参数 $\\lambda \\in \\mathbb{R}^m$，使得一组特征 $\\{f_i\\}_{i=1}^m$ 的模型期望与一组给定的目标期望 $\\{c_i\\}_{i=1}^m$ 相匹配。这是统计推断中的一个经典逆问题。规定的分布形式为\n$$\np_\\lambda(x) = \\frac{h(x)\\exp\\left(\\sum_{i=1}^m \\lambda_i f_i(x)\\right)}{Z(\\lambda)}\n$$\n其中 $h(x)$ 是一个基测度，$Z(\\lambda)$ 是归一化常数或配分函数。该分布是在线性约束 $\\mathbb{E}_p[f_i] = c_i$（$i=1, \\dots, m$）下，最大化相对熵（或称 Kullback-Leibler 散度）$S(p \\| h) = -\\sum_{x \\in \\mathcal{X}} p(x) \\log(p(x)/h(x))$ 这一优化问题的唯一解。\n\n这个约束优化问题可以通过考虑其拉格朗日对偶问题来解决。寻找分布 $p$ 的任务被转化为一个关于拉格朗日乘子 $\\lambda$ 的无约束优化问题。在对偶问题中需要最大化的目标函数是对数似然函数或对数配分函数，不包括与 $\\lambda$ 无关的项。具体来说，我们寻求找到 $\\lambda \\in \\mathbb{R}^m$ 来解这个包含 $m$ 个非线性方程的方程组：\n$$\n\\mathbb{E}_{p_\\lambda}[f_i] = \\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x) = c_i, \\quad \\text{for } i=1, \\dots, m.\n$$\n该方程的左侧与对数配分函数 $\\log Z(\\lambda)$ 的梯度有关：\n$$\n\\frac{\\partial}{\\partial \\lambda_i} \\log Z(\\lambda) = \\mathbb{E}_{p_\\lambda}[f_i].\n$$\n因此，解约束方程等价于找到对偶势函数 $\\Phi(\\lambda) = \\log Z(\\lambda) - \\sum_{i=1}^m \\lambda_i c_i$ 的一个驻点。\n\n解分布 $p_\\lambda$ 的唯一性和迭代算法的收敛性由该势函数的凸性保证。$\\log Z(\\lambda)$ 的海森矩阵（Hessian）由下式给出：\n$$\n\\frac{\\partial^2 \\log Z(\\lambda)}{\\partial \\lambda_j \\partial \\lambda_i} = \\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x) f_j(x) - \\left(\\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_i(x)\\right)\\left(\\sum_{x \\in \\mathcal{X}} p_\\lambda(x) f_j(x)\\right) = \\text{Cov}_{p_\\lambda}(f_i, f_j).\n$$\n这个海森矩阵是在分布 $p_\\lambda$ 下特征的协方差矩阵。作为协方差矩阵，它总是半正定的。这确保了 $\\log Z(\\lambda)$ 是一个凸函数，因此对偶势 $\\Phi(\\lambda)$ 也是凸的。这种凸性保证了如果存在最小值，则最小化子的集合是一个凸集。此外，该集合中的任意两组参数 $\\lambda_A$ 和 $\\lambda_B$ 都将产生相同的期望向量 $(\\mathbb{E}[f_1], \\dots, \\mathbb{E}[f_m])$，从而产生相同的概率分布 $p_\\lambda$。因此，如果约束 $\\{c_i\\}$ 是可行的（即，它们位于特征向量的凸包内），则存在唯一的解分布。\n\n如果特征 $\\{f_i\\}$ 是线性相关的，即存在一个非零向量 $\\alpha$，使得对于所有 $x$，$\\sum_{i=1}^m \\alpha_i f_i(x)$ 是一个常数，那么海森矩阵将是奇异的。在这种情况下，$\\Phi(\\lambda)$ 不是严格凸的，解该方程组的参数向量 $\\lambda$ 不是唯一的。如果 $\\lambda^*$ 是一个解，那么对于任何标量 $t$，$\\lambda^* + t\\alpha$ 也是一个解，因为指数中的变化在归一化过程中被抵消了。这正是测试用例 3 的情况。然而，最终得到的分布 $p_\\lambda$ 仍然是唯一的。\n\n该问题指定使用广义迭代尺度法（Generalized Iterative Scaling, GIS）算法，该算法在给定条件下适用：非负特征 $f_i(x) \\ge 0$ 以及对于所有 $x \\in \\mathcal{X}$，特征和为常数 $\\sum_{i=1}^m f_i(x) = C  0$。GIS 提供了一个乘性更新，可以在 $\\lambda$ 的对数参数空间中表示为加性更新。给定第 $k$ 次迭代时的参数 $\\lambda^{(k)}$，对每个分量 $\\lambda_i$ 的更新为：\n$$\n\\lambda_i^{(k+1)} = \\lambda_i^{(k)} + \\frac{1}{C} \\log\\left(\\frac{c_i}{\\mathbb{E}_{p_{\\lambda^{(k)}}}[f_i]}\\right).\n$$\n这里，$\\mathbb{E}_{p_{\\lambda^{(k)}}}[f_i]$ 表示特征 $i$ 的当前模型期望。项 $1/C$ 充当学习率。只要约束是可行的，这个迭代过程保证收敛到满足约束的唯一参数（在線性相關的意義上）。其收敛性证明依赖于表明算法的每一步都严格减小了当前分布与唯一目标分布之间的 Kullback-Leibler 散度，这一性质由 Darroch 和 Ratcliff (1972) 以及 Csiszár (1989) 建立。如果约束是不可行的（如测试用例 4 中所示），算法仍然会收敛到指数族内一个明确定义的分布，但最终的约束违反量 $r$ 将保持非零，表示在所选散度度量下与目标约束的最小可能偏差。\n\n对于数值实现，计算 $p_\\lambda(x)$ 需要小心处理指数项，以防止数值上溢或下溢。log-sum-exp 技巧是用于此目的的标准方法。令 $s_x = \\log h(x) + \\sum_{i=1}^m \\lambda_i f_i(x)$。我们计算 $S_{\\max} = \\max_{x \\in \\mathcal{X}} s_x$，然后按如下方式计算概率：\n$$\np_\\lambda(x) = \\frac{\\exp(s_x - S_{\\max})}{\\sum_{y \\in \\mathcal{X}} \\exp(s_y - S_{\\max})}.\n$$\n这确保了指数函数的最大参数为 0，从而防止了上溢，而可能下溢的项会正确地计算为 0。\n\n该算法的流程是：首先初始化 $\\lambda = \\mathbf{0}$，然后迭代计算当前分布 $p_{\\lambda^{(k)}}$、期望值 $\\mathbb{E}_{p_{\\lambda^{(k)}}}[f_i]$、残差 $r$，并更新 $\\lambda^{(k+1)}$，直到 $r$ 小于指定的容差或达到最大迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of maximum entropy problems using Generalized Iterative Scaling.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"n\": 5, \"m\": 3,\n            \"h\": np.ones(5),\n            \"F\": np.array([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0], [0, 0, 0, 0, 1]], dtype=float),\n            \"c\": np.array([0.4, 0.4, 0.2]),\n            \"C\": 1.0,\n        },\n        {\n            \"n\": 5, \"m\": 3,\n            \"h\": np.ones(5),\n            \"F\": np.array([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0], [0, 0, 0, 0, 1]], dtype=float),\n            \"c\": np.array([1e-6, 0.999, 0.000999]),\n            \"C\": 1.0,\n        },\n        {\n            \"n\": 4, \"m\": 3,\n            \"h\": np.ones(4),\n            \"F\": np.array([[1, 1, 0, 0], [0, 0, 1, 1], [1, 1, 1, 1]], dtype=float),\n            \"c\": np.array([0.5, 0.5, 1.0]),\n            \"C\": 2.0,\n        },\n        {\n            \"n\": 5, \"m\": 3,\n            \"h\": np.ones(5),\n            \"F\": np.array([[1, 1, 0, 0, 0], [0, 0, 1, 1, 0], [0, 0, 0, 0, 1]], dtype=float),\n            \"c\": np.array([0.3, 0.3, 0.3]),\n            \"C\": 1.0,\n        },\n    ]\n\n    results = []\n    tol = 1e-10\n    max_iter = 5000\n\n    for case in test_cases:\n        n, m = case[\"n\"], case[\"m\"]\n        h, F, c, C = case[\"h\"], case[\"F\"], case[\"c\"], case[\"C\"]\n\n        lambda_vec = np.zeros(m)\n        log_h = np.log(h)\n        \n        final_r = -1.0 # Placeholder\n\n        for k in range(max_iter):\n            # Calculate unnormalized log probabilities\n            # lambda_vec: (m,), F: (m, n) - s: (n,)\n            s = lambda_vec @ F + log_h\n\n            # Numerically stable calculation of p_lambda (softmax)\n            s_max = np.max(s)\n            exp_s = np.exp(s - s_max)\n            p_lambda = exp_s / np.sum(exp_s)\n            \n            # Calculate current expectations\n            # F: (m, n), p_lambda: (n,) - current_c: (m,)\n            current_c = F @ p_lambda\n            \n            # Calculate maximum absolute constraint violation\n            r = np.max(np.abs(current_c - c))\n            final_r = r\n\n            # Check for convergence\n            if r  tol:\n                break\n            \n            # GIS update step for lambda\n            # Note: current_c components are  0 because h0, f_i=0, c_i0,\n            # and each f_i is non-zero for at least one x.\n            delta_lambda = (1.0 / C) * np.log(c / current_c)\n            lambda_vec += delta_lambda\n\n        results.append(final_r)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3401753"}]}