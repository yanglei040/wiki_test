{"hands_on_practices": [{"introduction": "在设计实验时，一个核心任务不仅是学习模型参数，还包括表征我们测量中的不确定性。这个练习从第一性原理出发，要求学生动手推导一个关键超参数——观测噪声方差——的期望信息增益（EIG）。通过解决这个问题 [@problem_id:3380369]，你将能够巩固对于 EIG 作为熵减的定义以及其在共轭贝叶斯模型中应用的理解，并深入思考何时应优先学习噪声特性而非模型参数本身。", "problem": "考虑一个数据同化工作流中的校准实验，其唯一目标是学习未知的观测噪声方差（超参数）$\\sigma^{2}$。您收集了 $r$ 个独立的校准残差 $\\{y_{i}\\}_{i=1}^{r}$，其信号已知为零均值，建模为 $y_{i} \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2})$，$i=1,\\dots,r$。假设方差的共轭先验为 $\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a, b)$，其中形状参数 $a>0$ 且尺度参数 $b>0$，其概率密度函数为 $p(\\sigma^{2}) = \\frac{b^{a}}{\\Gamma(a)}(\\sigma^{2})^{-(a+1)} \\exp\\left(-\\frac{b}{\\sigma^{2}}\\right)$。\n\n采用期望信息增益（EIG）的定义，即数据与感兴趣参数之间的互信息，这等价于后验分布到先验分布的期望Kullback–Leibler散度（KLD）。也就是说，对于数据向量 $Y = (y_{1},\\dots,y_{r})$，$I(Y;\\sigma^{2}) = \\mathbb{E}_{Y}\\!\\left[ \\mathrm{KLD}\\big(p(\\sigma^{2}\\mid Y)\\,\\|\\,p(\\sigma^{2})\\big) \\right]$，其中 $\\mathrm{KLD}$ 表示 Kullback–Leibler 散度。\n\n从基本定义和共轭性出发，推导 EIG $I(Y;\\sigma^{2})$ 作为 $a$ 和 $r$ 的函数的闭式解析表达式。最终答案必须是单个解析表达式。然后，基于反问题和数据同化的第一性原理，论证在何种条件下，设计实验来学习 $\\sigma^{2}$ 而非正演模型参数 $\\theta$ 是有益的（假设 $\\theta$ 仅出现在非校准实验中，并具有适当的先验）。EIG 不需要数值近似；您的最终答案必须是精确的。如果您引入任何缩写词，请在首次使用时定义它们。", "solution": "首先根据所需标准验证问题陈述。\n\n### 问题验证\n\n**第1步：提取已知条件**\n\n- **目标**：学习未知的观测噪声方差 $\\sigma^{2}$。\n- **数据**：$r$ 个独立的校准残差 $\\{y_{i}\\}_{i=1}^{r}$。\n- **数据模型（似然）**：$y_{i} \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2})$，$i=1,\\dots,r$。数据向量为 $Y = (y_{1},\\dots,y_{r})$。\n- **先验分布**：$\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a, b)$，其中形状参数 $a>0$ 且尺度参数 $b>0$。\n- **先验概率密度函数（PDF）**：$p(\\sigma^{2}) = \\frac{b^{a}}{\\Gamma(a)}(\\sigma^{2})^{-(a+1)} \\exp\\left(-\\frac{b}{\\sigma^{2}}\\right)$。\n- **期望信息增益（EIG）的定义**：$I(Y;\\sigma^{2}) = \\mathbb{E}_{Y}\\!\\left[ \\mathrm{KLD}\\big(p(\\sigma^{2}\\mid Y)\\,\\|\\,p(\\sigma^{2})\\big) \\right]$，其中 KLD 是 Kullback–Leibler 散度。\n- **任务1**：推导 $I(Y;\\sigma^{2})$ 作为 $a$ 和 $r$ 的函数的闭式解析表达式。\n- **任务2**：论证何时设计实验来学习 $\\sigma^{2}$ 而非正演模型参数 $\\theta$ 是有益的。\n\n**第2步：使用提取的已知条件进行验证**\n\n- **科学依据**：该问题基于贝叶斯推断的标准、公认的原理。对残差使用正态似然以及对方差使用共轭逆伽马先验是统计学和机器学习中的教科书式示例。将 EIG 定义为数据和参数之间的互信息也是信息论和贝叶斯实验设计中的一个基本概念。\n- **适定性**：问题陈述清晰，并提供了推导所需表达式的所有必要信息。使用共轭先验确保了后验分布是可解析处理的，从而可以对 EIG 进行闭式推导。问题没有歧义，且存在唯一解。\n- **客观性**：问题以精确、客观的数学语言陈述，没有主观或带有偏见的术语。\n\n该问题没有说明中列出的任何缺陷（例如，科学上不健全、不完整、有歧义）。这是贝叶斯统计学中一个标准的、非平凡的问题。\n\n**第3步：结论与行动**\n\n问题有效。将提供完整的解答。\n\n### 解答推导\n\n按要求，解答分为两部分进行推导。\n\n**第1部分：期望信息增益（EIG）的推导**\n\n期望信息增益（EIG）是数据 $Y$ 和参数 $\\sigma^2$ 之间的互信息，可以用两种等价的方式表示：\n$$I(Y; \\sigma^{2}) = H(Y) - \\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$$\n$$I(Y; \\sigma^{2}) = H(\\sigma^{2}) - \\mathbb{E}_{Y}[H(\\sigma^{2} \\mid Y)]$$\n其中 $H(\\cdot)$ 表示微分熵。第一种形式涉及数据的熵，对本问题而言更为直接。我们将分别计算每一项。\n\n1.  **数据的条件熵**：$H(Y \\mid \\sigma^{2})$\n    给定 $\\sigma^{2}$，数据 $Y = (y_1, \\dots, y_r)$ 由 $r$ 个独立同分布的随机变量组成，每个 $y_i \\sim \\mathcal{N}(0, \\sigma^2)$。因此，向量 $Y$ 服从多元正态分布，$Y \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2}I_{r})$，其中 $I_{r}$ 是 $r \\times r$ 的单位矩阵。\n    $k$ 维多元正态分布 $\\mathcal{N}(\\mu, \\Sigma)$ 的微分熵为 $\\frac{1}{2}\\ln\\left((2\\pi e)^{k}\\det(\\Sigma)\\right)$。\n    对于 $Y \\mid \\sigma^{2}$，我们有维度 $k=r$ 和协方差矩阵 $\\Sigma = \\sigma^2 I_r$，所以 $\\det(\\Sigma) = (\\sigma^2)^r$。\n    条件熵为：\n    $$H(Y \\mid \\sigma^{2}) = \\frac{1}{2}\\ln\\left((2\\pi e)^{r}(\\sigma^{2})^{r}\\right) = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\ln(\\sigma^{2})$$\n\n2.  **期望条件熵**：$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$\n    我们对 $H(Y \\mid \\sigma^{2})$ 关于 $\\sigma^2$ 的先验分布（即 $\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a, b)$）取期望。\n    $$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})] = \\mathbb{E}_{\\sigma^{2}}\\left[\\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\ln(\\sigma^{2})\\right] = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\mathbb{E}_{\\sigma^{2}}[\\ln(\\sigma^{2})]$$\n    对于一个随机变量 $X \\sim \\mathrm{Inverse\\text{-}Gamma}(\\alpha, \\beta)$，其对数的期望为 $\\mathbb{E}[\\ln(X)] = \\ln(\\beta) - \\psi(\\alpha)$，其中 $\\psi(\\alpha) = \\frac{d}{d\\alpha}\\ln\\Gamma(\\alpha)$ 是双伽玛函数(digamma function)。\n    对于我们的先验，有 $\\alpha=a$ 和 $\\beta=b$，我们得到 $\\mathbb{E}_{\\sigma^{2}}[\\ln(\\sigma^{2})] = \\ln(b) - \\psi(a)$。\n    将此代入期望条件熵的表达式，得到：\n    $$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})] = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}(\\ln(b) - \\psi(a)) = \\frac{r}{2}(\\ln(2\\pi) + 1) + \\frac{r}{2}\\ln(b) - \\frac{r}{2}\\psi(a)$$\n\n3.  **数据的边缘熵**：$H(Y)$\n    为了求得 $H(Y)$，我们首先需要 $Y$ 的边缘分布，通过对 $\\sigma^2$ 积分得到：$p(Y) = \\int_{0}^{\\infty} p(Y \\mid \\sigma^{2}) p(\\sigma^{2}) d\\sigma^{2}$。\n    这是一个标准的分层模型。正态似然与方差的逆伽马先验结合，会得到一个多元学生t分布作为边缘分布。\n    具体来说，如果 $Y \\mid \\sigma^2 \\sim \\mathcal{N}(0, \\sigma^2 I_r)$ 且 $\\sigma^2 \\sim \\mathrm{IG}(a, b)$，那么 $Y$ 的边缘分布是多元学生t分布，$Y \\sim t_r(\\mu, \\Sigma, \\nu)$，其参数为：\n    - 自由度：$\\nu = 2a$\n    - 位置：$\\mu = 0$\n    - 尺度矩阵：$\\Sigma = \\frac{b}{a}I_r$\n    一个维度为 $k$、自由度为 $\\nu$、尺度矩阵为 $\\Sigma$ 的多元t分布的微分熵是：\n    $$H = \\ln\\left(\\frac{\\Gamma(\\nu/2)}{\\Gamma((\\nu+k)/2)}\\right) + \\frac{k}{2}\\ln(\\nu\\pi) + \\frac{1}{2}\\ln\\det(\\Sigma) + \\frac{\\nu+k}{2}\\left[\\psi\\left(\\frac{\\nu+k}{2}\\right) - \\psi\\left(\\frac{\\nu}{2}\\right)\\right]$$\n    代入我们的参数 $k=r$, $\\nu=2a$ 和 $\\det(\\Sigma) = \\det(\\frac{b}{a}I_r) = (\\frac{b}{a})^r$：\n    $$H(Y) = \\ln\\left(\\frac{\\Gamma(a)}{\\Gamma(a+r/2)}\\right) + \\frac{r}{2}\\ln(2a\\pi) + \\frac{1}{2}\\ln\\left(\\left(\\frac{b}{a}\\right)^{r}\\right) + \\frac{2a+r}{2}\\left[\\psi\\left(a+\\frac{r}{2}\\right) - \\psi(a)\\right]$$\n    简化对数项：\n    $$H(Y) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}(\\ln(2\\pi)+\\ln a) + \\frac{r}{2}(\\ln b - \\ln a) + \\left(a+\\frac{r}{2}\\right)\\left[\\psi\\left(a+\\frac{r}{2}\\right) - \\psi(a)\\right]$$\n    $$H(Y) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}\\ln(2\\pi) + \\frac{r}{2}\\ln b + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a)$$\n\n4.  **计算 EIG**：$I(Y; \\sigma^{2}) = H(Y) - \\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$\n    现在我们从边缘熵中减去期望条件熵的表达式：\n    $$I(Y; \\sigma^{2}) = \\left[ \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}\\ln(2\\pi) + \\frac{r}{2}\\ln b + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a) \\right] - \\left[ \\frac{r}{2}(\\ln(2\\pi) + 1) + \\frac{r}{2}\\ln(b) - \\frac{r}{2}\\psi(a) \\right]$$\n    我们将各项分组以进行消去：\n    $$I(Y; \\sigma^{2}) = (\\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right)) + \\left(\\frac{r}{2}\\ln(2\\pi) - \\frac{r}{2}\\ln(2\\pi)\\right) + \\left(\\frac{r}{2}\\ln b - \\frac{r}{2}\\ln b\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a) + \\frac{r}{2}\\psi(a) - \\frac{r}{2}$$\n    $\\ln(2\\pi)$ 和 $\\ln b$ 项相互抵消。合并 $\\psi(a)$ 项：\n    $$-(a+r/2)\\psi(a) + (r/2)\\psi(a) = -a\\psi(a) - (r/2)\\psi(a) + (r/2)\\psi(a) = -a\\psi(a)$$\n    因此，EIG 的最终表达式为：\n    $$I(Y; \\sigma^{2}) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - a\\psi(a) - \\frac{r}{2}$$\n    此表达式仅依赖于先验形状参数 $a$ 和数据点数量 $r$，而不依赖于先验尺度参数 $b$，这与量纲分析的考虑相符。\n\n**第2部分：学习 $\\sigma^2$ 的理由**\n\n在反问题和数据同化的背景下，主要目标通常是从观测数据 $d$ 中推断一组正演模型参数，记为 $\\theta$。这种关系通常被建模为 $d = G(\\theta) + \\epsilon$，其中 $G$ 是正演模型，$\\epsilon$ 是观测噪声，通常假设为均值为零、方差为 $\\sigma^2$ 的高斯噪声。在这种情况下，$\\sigma^2$ 是一个超参数，它决定了在为 $\\theta$ 进行反演时对数据的信任程度。$\\theta$ 的后验分布由贝叶斯定理给出，$p(\\theta \\mid d) \\propto p(d \\mid \\theta) p(\\theta)$，其中似然项 $p(d \\mid \\theta)$ 关键性地依赖于 $\\sigma^2$。\n\n基于第一性原理，在以下条件下，投入实验精力来学习 $\\sigma^2$ 而不是（或在学习 $\\theta$ 之前）学习 $\\theta$ 是有益的：\n\n1.  **当观测误差的先验知识贫乏时：** 如果 $\\sigma^2$ 的初始先验分布模糊或高度不确定（例如，逆伽马先验中的形状参数 $a$ 很小），这种不确定性会直接传播到对 $\\theta$ 的推断中。一个不确定的 $\\sigma^2$ 意味着对 $\\theta$ 的先验信念和来自数据 $d$ 的信息之间的相对权重定义不清。这会导致 $\\theta$ 的后验分布可能过宽（如果 $\\sigma^2$ 被高估）或过窄且可能存在偏差（如果 $\\sigma^2$ 被低估）。如问题中所述的专门校准实验，旨在减少 $\\sigma^2$ 的不确定性，为其产生一个信息更丰富的后验分布。然后，这个后验分布在旨在推断 $\\theta$ 的主要实验中充当一个更紧凑、更可靠的 $\\sigma^2$ 先验，确保数据被正确定权，并且对 $\\theta$ 的推断更加稳健和高效。\n\n2.  **为正确定量不确定性并避免模型-数据不匹配：** 残差项 $\\|d - G(\\theta)\\|^2$ 混合了观测误差（随机噪声）和模型不足（正演模型 $G$ 是对现实的不完美表示）的影响。没有对观测噪声方差 $\\sigma^2$ 的可靠估计，就不可能区分这两种误差来源。一个校准实验，其中“信号”$G(\\theta)$ 是已知的（例如，如问题陈述中为零），可以专门分离并量化 $\\sigma^2$。这是许多数据同化系统中的关键第一步，这些系统必须明确指定观测误差协方差矩阵（通常表示为 $R$）。一个准确的 $R$ 是正确推断模型误差统计量（通常表示为 $Q$）以及为参数 $\\theta$ 获得可靠的后验不确定性量化的先决条件。\n\n3.  **为了实验设计和成本效益：** 为复杂模型参数 $\\theta$ 设计信息量最大化的实验可能会耗费大量资源且成本高昂。这类实验对于约束噪声方差 $\\sigma^2$ 也可能是次优的。通常，首先进行一个更简单、更便宜的校准实验，专门用于表征测量仪器的噪声特性 ($\\sigma^2$)，会更具成本效益。通过预先“确定”这个讨厌的参数，后续针对 $\\theta$ 的更昂贵的实验可以被更高效地设计和分析，从而最大化科研投资回报。来自昂贵数据的信息随后被用于约束主要感兴趣的参数 $\\theta$，而不是被部分消耗用于同时学习噪声水平。\n\n总之，学习 $\\sigma^2$ 不仅仅是一项次要任务；它是一个基础步骤，能够为主要模型参数 $\\theta$ 实现稳健而高效的推断。当关于噪声水平的初始不确定性很高，并且当为了科学目标而解开不同误差来源至关重要时，这样做最为有益。", "answer": "$$\n\\boxed{\\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - a\\psi(a) - \\frac{r}{2}}\n$$", "id": "3380369"}, {"introduction": "一旦我们掌握了如何计算 EIG，一个关键问题随之而来：我们应该为*什么*来获取信息增益？这个练习探讨了为学习模型的所有参数而设计实验与为学习一个特定的“感兴趣量”（Quantity of Interest, QoI）而设计实验之间的重要区别。通过一个具体的例子 [@problem_id:3380354]，该问题明确地展示了这两个目标为何会导致不同的最优实验设计，从而引入了“目标导向的实验设计”这一核心概念。", "problem": "考虑一个线性高斯贝叶斯逆问题和实验设计设置，其中未知参数是向量 $\\theta \\in \\mathbb{R}^{2}$，其高斯先验为 $\\theta \\sim \\mathcal{N}(0, \\Sigma_{\\theta})$，其中\n$$\n\\Sigma_{\\theta} \\;=\\;\n\\begin{pmatrix}\n4  3 \\\\\n3  4\n\\end{pmatrix}.\n$$\n通过测量模型，使用设计参数 $d \\in [0, 2\\pi)$ 进行单次标量观测\n$$\nY \\,\\big|\\, \\theta, d \\;\\sim\\; \\mathcal{N}\\!\\big(h(d)^{\\top}\\theta,\\, \\sigma^{2}\\big), \\quad \\text{其中} \\quad h(d) \\;=\\; \\begin{pmatrix}\\cos d \\\\ \\sin d\\end{pmatrix}, \\quad \\sigma^{2} \\;=\\; 1.\n$$\n感兴趣的量是线性泛函 $\\psi = a^{\\top}\\theta$，其中 $a = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$。\n\n仅使用信息论和高斯概率的基本定义（特别是互信息 $I(X;Y\\,|\\,d) = H(X) - H(X\\,|\\,Y,d)$ 的定义和多元高斯的熵），完成以下任务：\n\n1. 推导作为 $d$ 的函数的期望信息增益 $I(\\theta; Y \\mid d)$ 和 $I(\\psi; Y \\mid d)$。\n2. 对于每个信息目标，在 $h(d)$ 所隐含的单位范数约束下，确定最大化 $I(\\theta; Y \\mid d)$ 的最优设计 $d_{\\theta}^{\\star}$ 和最大化 $I(\\psi; Y \\mid d)$ 的最优设计 $d_{\\psi}^{\\star}$。\n3. 证明对于给定的 $\\Sigma_{\\theta}$ 和 $\\sigma^{2}$，$d_{\\theta}^{\\star} \\neq d_{\\psi}^{\\star}$。\n4. 解释当 $\\psi$ 是 $\\theta$ 的一个非可逆函数时，这两个最优设计通常会不同的结构性原因，解释应基于定义和高斯推断的几何学。\n\n最后，计算两个单位范数最优测量方向 $h(d_{\\theta}^{\\star})$ 和 $h(d_{\\psi}^{\\star})$ 之间的锐角（以弧度为单位），并以单一精确解析表达式给出答案。角度以弧度表示。不要对答案进行四舍五入。", "solution": "该问题在贝叶斯逆问题和最优实验设计的框架内是适定的且具有科学依据。所有必要的参数和定义都已提供。我们可以开始求解。\n\n解答根据问题陈述中指定的四个任务进行组织，最终计算出两个最优测量方向之间的夹角。\n\n### 1. 期望信息增益的推导\n\n两个随机变量 $X$ 和 $Y$ 之间的互信息由 $I(X; Y) = H(X) - H(X|Y)$ 给出，其中 $H(\\cdot)$ 表示微分熵。对于高斯分布，一个等价且通常更方便的表达式是 $I(X; Y) = H(Y) - H(Y|X)$。我们使用一个设计参数 $d$，因此所有量都以 $d$ 为条件。一个 $k$ 维高斯随机变量 $Z \\sim \\mathcal{N}(\\mu, \\Sigma)$ 的熵是 $H(Z) = \\frac{1}{2}\\ln(\\det(2\\pi e \\Sigma))$。\n\n**$I(\\theta; Y \\mid d)$ 的推导**\n\n我们计算 $I(\\theta; Y \\mid d) = H(Y \\mid d) - H(Y \\mid \\theta, d)$。\n\n首先，我们考虑在参数 $\\theta$ 和设计 $d$ 条件下的观测 $Y$ 的熵。问题陈述 $Y \\mid \\theta, d \\sim \\mathcal{N}(h(d)^\\top \\theta, \\sigma^2)$。由于这是一个方差为 $\\sigma^2 = 1$ 的标量高斯分布，其熵为：\n$$\nH(Y \\mid \\theta, d) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2) = \\frac{1}{2}\\ln(2\\pi e)\n$$\n\n接下来，我们求仅以 $d$ 为条件的 $Y$ 的边际分布。由于 $\\theta \\sim \\mathcal{N}(0, \\Sigma_\\theta)$ 且观测模型在 $\\theta$ 上是线性的，因此 $Y \\mid d$ 也是高斯分布。\n其均值为 $\\mathbb{E}[Y \\mid d] = \\mathbb{E}[h(d)^\\top \\theta] = h(d)^\\top \\mathbb{E}[\\theta] = 0$。\n其方差为 $\\text{Var}(Y \\mid d) = \\text{Var}(h(d)^\\top \\theta + \\epsilon) = \\text{Var}(h(d)^\\top \\theta) + \\text{Var}(\\epsilon)$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 是观测噪声。\n$$\n\\text{Var}(Y \\mid d) = h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2\n$$\n因此，$Y \\mid d \\sim \\mathcal{N}(0, h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)$。其熵为：\n$$\nH(Y \\mid d) = \\frac{1}{2}\\ln(2\\pi e (\\text{Var}(Y \\mid d))) = \\frac{1}{2}\\ln(2\\pi e (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2))\n$$\n期望信息增益是这些熵之间的差值：\n$$\nI(\\theta; Y \\mid d) = H(Y \\mid d) - H(Y \\mid \\theta, d) = \\frac{1}{2}\\ln\\left(\\frac{2\\pi e (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}{2\\pi e \\sigma^2}\\right) = \\frac{1}{2}\\ln\\left(1 + \\frac{h(d)^\\top \\Sigma_\\theta h(d)}{\\sigma^2}\\right)\n$$\n当 $\\sigma^2=1$ 时，这简化为 $I(\\theta; Y \\mid d) = \\frac{1}{2}\\ln(1 + h(d)^\\top \\Sigma_\\theta h(d))$。\n\n**$I(\\psi; Y \\mid d)$ 的推导**\n\n我们计算 $I(\\psi; Y \\mid d) = H(\\psi) - H(\\psi \\mid Y, d)$，其中 $\\psi = a^\\top \\theta$。最大化这个量等价于最小化后验熵 $H(\\psi \\mid Y, d)$。\n$\\psi$ 的先验方差是 $\\text{Var}(\\psi) = \\text{Var}(a^\\top\\theta) = a^\\top \\Sigma_\\theta a$。\n观测到 $Y$ 后 $\\theta$ 的后验协方差矩阵由线性高斯模型的贝叶斯更新公式给出：\n$$\n\\Sigma_{\\theta|Y,d} = \\left(\\Sigma_\\theta^{-1} + \\frac{h(d)h(d)^\\top}{\\sigma^2}\\right)^{-1}\n$$\n使用 Sherman-Morrison-Woodbury 公式，我们得到：\n$$\n\\Sigma_{\\theta|Y,d} = \\Sigma_\\theta - \\frac{\\Sigma_\\theta h(d) h(d)^\\top \\Sigma_\\theta}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\n$$\n$\\psi = a^\\top \\theta$ 的后验分布是高斯分布，$\\psi \\mid Y,d \\sim \\mathcal{N}(a^\\top \\mu_{\\theta|Y,d}, a^\\top \\Sigma_{\\theta|Y,d} a)$，其中 $\\mu_{\\theta|Y,d}$ 是后验均值。$\\psi$ 的后验方差 $\\text{Var}(\\psi \\mid Y, d) = a^\\top \\Sigma_{\\theta|Y,d} a$ 不依赖于测量值 $Y$。\n$$\n\\text{Var}(\\psi \\mid Y, d) = a^\\top \\left(\\Sigma_\\theta - \\frac{\\Sigma_\\theta h(d) h(d)^\\top \\Sigma_\\theta}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\\right) a = a^\\top\\Sigma_\\theta a - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}\n$$\n信息增益是 $\\psi$ 从先验到后验的熵减少量：\n$$\nI(\\psi; Y \\mid d) = H(\\psi) - H(\\psi \\mid Y, d) = \\frac{1}{2}\\ln(2\\pi e \\text{Var}(\\psi)) - \\frac{1}{2}\\ln(2\\pi e \\text{Var}(\\psi \\mid Y, d))\n$$\n$$\nI(\\psi; Y \\mid d) = \\frac{1}{2}\\ln\\left(\\frac{\\text{Var}(\\psi)}{\\text{Var}(\\psi \\mid Y, d)}\\right) = \\frac{1}{2}\\ln\\left(\\frac{a^\\top\\Sigma_\\theta a}{a^\\top\\Sigma_\\theta a - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2}}\\right)\n$$\n这可以重写为：\n$$\nI(\\psi; Y \\mid d) = -\\frac{1}{2}\\ln\\left(1 - \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{(a^\\top \\Sigma_\\theta a) (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}\\right)\n$$\n\n### 2. 最优设计的确定\n\n**最优设计 $d_{\\theta}^{\\star}$**\n\n为了最大化 $I(\\theta; Y \\mid d) = \\frac{1}{2}\\ln(1 + h(d)^\\top \\Sigma_\\theta h(d))$，我们必须在 $h(d)$ 是单位向量（即 $h(d)^\\top h(d) = 1$）的约束下最大化二次型 $h(d)^\\top \\Sigma_\\theta h(d)$。这是一个瑞利商最大化问题。最大值是 $\\Sigma_\\theta$ 的最大特征值，最优向量 $h(d)$ 是对应的特征向量。\n\n先验协方差为 $\\Sigma_\\theta = \\begin{pmatrix} 4  3 \\\\ 3  4 \\end{pmatrix}$。其特征方程为 $\\det(\\Sigma_\\theta - \\lambda I) = (4-\\lambda)^2 - 9 = 0$，解得特征值为 $\\lambda_1 = 7$ 和 $\\lambda_2 = 1$。最大特征值是 $\\lambda_{\\max} = 7$。\n$\\lambda_{\\max}=7$ 对应的特征向量通过求解 $(\\Sigma_\\theta - 7I)v = 0$ 得到：\n$$\n\\begin{pmatrix} -3  3 \\\\ 3  -3 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies v_1 = v_2\n$$\n归一化的特征向量是 $\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。我们设 $h(d_{\\theta}^{\\star}) = \\begin{pmatrix} \\cos d_{\\theta}^{\\star} \\\\ \\sin d_{\\theta}^{\\star} \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。这得到 $\\cos d_{\\theta}^{\\star} = \\sin d_{\\theta}^{\\star} = 1/\\sqrt{2}$，因此一个最优设计是 $d_{\\theta}^{\\star} = \\pi/4$。\n\n**最优设计 $d_{\\psi}^{\\star}$**\n\n要最大化 $I(\\psi; Y \\mid d)$，我们必须最大化对数的参数，这等价于最大化 $\\psi$ 和 $Y$ 之间的相关系数平方（给定 $d$）：\n$$\n\\rho^2(d) = \\frac{(a^\\top \\Sigma_\\theta h(d))^2}{(a^\\top \\Sigma_\\theta a) (h(d)^\\top \\Sigma_\\theta h(d) + \\sigma^2)}\n$$\n我们来计算各个部分，其中 $a = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$h(d) = \\begin{pmatrix} \\cos d \\\\ \\sin d \\end{pmatrix}$，且 $\\sigma^2=1$：\n- $a^\\top \\Sigma_\\theta = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 4  3 \\\\ 3  4 \\end{pmatrix} = \\begin{pmatrix} 4  3 \\end{pmatrix}$\n- $a^\\top \\Sigma_\\theta a = 4$\n- $a^\\top \\Sigma_\\theta h(d) = 4\\cos d + 3\\sin d$\n- $h(d)^\\top \\Sigma_\\theta h(d) = 4\\cos^2 d + 6\\sin d \\cos d + 4\\sin^2 d = 4 + 3\\sin(2d)$\n\n需要最大化的目标函数是：\n$$\n\\rho^2(d) = \\frac{(4\\cos d + 3\\sin d)^2}{4(4 + 3\\sin(2d) + 1)} = \\frac{(4\\cos d + 3\\sin d)^2}{4(5 + 3\\sin(2d))}\n$$\n将关于 $d$ 的导数设为零，得到条件 $9\\cos(2d) - 7\\sin(2d) = 4.2$。\n我们从这个方程和恒等式 $\\cos^2(2d) + \\sin^2(2d) = 1$ 中解出 $\\cos(2d)$ 和 $\\sin(2d)$。这会产生两对解：\n1. $\\cos(2d) = 56/65$, $\\sin(2d) = 33/65$\n2. $\\cos(2d) = -7/25$, $\\sin(2d) = -24/25$\n评估目标函数 $\\rho^2(d)$ 表明，第二组解给出 $\\rho^2=0$（最小值），而第一组解给出最大值。\n对于最优设计，我们使用恒等式 $\\cos^2 d = \\frac{1+\\cos(2d)}{2}$ 和 $\\sin^2 d = \\frac{1-\\cos(2d)}{2}$：\n$$\n\\cos^2(d_{\\psi}^{\\star}) = \\frac{1 + 56/65}{2} = \\frac{121/65}{2} = \\frac{121}{130} \\implies |\\cos(d_{\\psi}^{\\star})| = \\frac{11}{\\sqrt{130}}\n$$\n$$\n\\sin^2(d_{\\psi}^{\\star}) = \\frac{1 - 56/65}{2} = \\frac{9/65}{2} = \\frac{9}{130} \\implies |\\sin(d_{\\psi}^{\\star})| = \\frac{3}{\\sqrt{130}}\n$$\n由于 $\\sin(2d_{\\psi}^{\\star}) = 2\\sin(d_{\\psi}^{\\star})\\cos(d_{\\psi}^{\\star}) = 33/65 > 0$，$\\sin(d_{\\psi}^{\\star})$ 和 $\\cos(d_{\\psi}^{\\star})$ 必须同号。我们选择正解，这给出了最优测量方向向量：\n$$\nh(d_{\\psi}^{\\star}) = \\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\n这对应于角度 $d_{\\psi}^{\\star} = \\arctan(3/11)$。\n\n### 3. 最优设计不等的证明\n\n总信息的最优设计是 $d_{\\theta}^{\\star} = \\pi/4$。\n特定感兴趣量(QoI)信息的最优设计是 $d_{\\psi}^{\\star} = \\arctan(3/11)$。\n由于 $\\tan(d_{\\theta}^{\\star}) = \\tan(\\pi/4) = 1$ 且 $\\tan(d_{\\psi}^{\\star}) = 3/11$，并且 $1 \\neq 3/11$，因此证明了 $d_{\\theta}^{\\star} \\neq d_{\\psi}^{\\star}$。\n\n### 4. 差异的结构性原因\n\n最大化 $I(\\theta; Y \\mid d)$ 和最大化 $I(\\psi; Y \\mid d)$ 这两个目标是根本不同的。\n- 最大化 $I(\\theta; Y \\mid d)$ 旨在减少参数向量 $\\theta$ 的整体不确定性。这是通过在先验不确定性最大的方向上进行测量来实现的。先验不确定性的几何结构由协方差矩阵 $\\Sigma_\\theta$ 描述。最大方差的方向是 $\\Sigma_\\theta$ 的主特征向量。因此，$h(d_\\theta^\\star)$ 与该特征向量对齐。这是一个非定向的，或“参数聚焦”的设计准则。\n- 最大化 $I(\\psi; Y \\mid d)$ 旨在减少特定感兴趣量 $\\psi=a^\\top\\theta$ 的不确定性，这是一个 $\\theta$ 的非可逆线性投影。最优设计必须平衡目标函数 $\\rho^2(d)$ 中可见的两个竞争因素：\n  1. 分子 $(a^\\top \\Sigma_\\theta h)^2 = \\text{Cov}(\\psi, h^\\top\\theta)^2$ 鼓励将测量方向 $h$ 与向量 $\\Sigma_\\theta a$ 对齐。这个向量表示感兴趣量(QoI)和 $\\theta$ 各分量之间的协方差，有效地映射了 $\\theta$ 的变化如何影响 $\\psi$。与此方向相关的测量对于 $\\psi$ 的信息量最大。\n  2. 分母 $h^\\top \\Sigma_\\theta h + \\sigma^2$ 是观测的先验预测方差。该项惩罚在先验方差高的方向（即 $h^\\top \\Sigma_\\theta h$ 较大）进行测量，因为由于 $\\theta$ 自身的不确定性，这类测量本质上更“嘈杂”。\n因此，最优设计 $h(d_\\psi^\\star)$ 代表了一种折衷：它从最大先验方差的方向（由 $d_\\theta^\\star$ 选择）被拉向一个为估计 $\\psi$ 提供更好信噪比的方向。这两个方向仅在特殊情况下重合，例如，如果感兴趣量(QoI)向量 $a$ 恰好是 $\\Sigma_\\theta$ 的一个特征向量。在这个问题中，$a=(1,0)^\\top$ 不是一个特征向量，所以最优设计不同。\n\n### 最终计算：最优方向之间的锐角\n\n我们需要找到两个最优单位范数测量向量之间的锐角 $\\phi$：\n$$\nh(d_{\\theta}^{\\star}) = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\quad \\text{和} \\quad h(d_{\\psi}^{\\star}) = \\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\n两个单位向量之间夹角的余弦值是它们的点积：\n$$\n\\cos \\phi = h(d_{\\theta}^{\\star}) \\cdot h(d_{\\psi}^{\\star}) = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right) \\cdot \\left(\\frac{1}{\\sqrt{130}}\\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\\right)\n$$\n$$\n\\cos \\phi = \\frac{1}{\\sqrt{260}} (1 \\cdot 11 + 1 \\cdot 3) = \\frac{14}{\\sqrt{260}} = \\frac{14}{\\sqrt{4 \\cdot 65}} = \\frac{14}{2\\sqrt{65}} = \\frac{7}{\\sqrt{65}}\n$$\n由于 $\\cos \\phi > 0$，该角是锐角。该角度为：\n$$\n\\phi = \\arccos\\left(\\frac{7}{\\sqrt{65}}\\right)\n$$\n这就是所要求的精确解析表达式。", "answer": "$$\n\\boxed{\\arccos\\left(\\frac{7}{\\sqrt{65}}\\right)}\n$$", "id": "3380354"}, {"introduction": "在许多实际问题中，模型往往过于复杂，无法得到期望信息增益的完全解析解，这为从理论到实践架起了桥梁。这个练习 [@problem_id:3380344] 介绍了一种强大而实用的技术：将讨厌参数（nuisance parameters）的解析边缘化与蒙特卡洛估计相结合。你将推导出一个高效的计算估计量，这是将最优实验设计思想应用于复杂分层模型的关键技能。", "problem": "考虑一个分层线性高斯数据模型，其中讨厌参数仅通过噪声分布进入模型。令感兴趣的参数为向量 $\\psi \\in \\mathbb{R}^{n_{\\psi}}$，其先验密度为 $p(\\psi)$，且可从中采样。对于一个固定的设计 $d$，数据 $Y \\in \\mathbb{R}^{n_{y}}$ 通过以下方式生成：\n- 前向映射 $H(d) \\in \\mathbb{R}^{n_{y} \\times n_{\\psi}}$，即 $Y \\mid \\psi, \\phi, d \\sim \\mathcal{N}\\!\\left(H(d)\\,\\psi,\\;\\phi^{-1}\\,\\Sigma(d)\\right)$，其中 $\\Sigma(d) \\in \\mathbb{R}^{n_{y} \\times n_{y}}$ 是一个已知的正定矩阵，以及\n- 噪声精度 $\\phi$ 服从Gamma先验 $\\phi \\sim \\mathrm{Gamma}\\!\\left(\\frac{\\nu}{2},\\,\\frac{\\nu}{2}\\right)$，其形状参数为 $\\frac{\\nu}{2}$，率参数为 $\\frac{\\nu}{2}$，对于一个固定的 $\\nu > 0$。\n\n假设 $\\phi$ 是一个没有推断兴趣的讨厌参数，并且仅如上所述进入噪声模型，同时 $p(\\psi)$ 与 $\\phi$ 相互独立。期望信息增益 $I(\\psi; Y \\mid d)$ 定义为在该分层模型下，以 $d$为条件时 $\\psi$ 和 $Y$ 之间的互信息。\n\n从互信息的定义和概率论的第一性原理（例如全概率定律和贝叶斯法则）出发，推导 $I(\\psi; Y \\mid d)$ 的一个蒙特卡洛估计量，该估计量在似然函数中精确地边缘化讨厌参数 $\\phi$，并避免在关于合成数据的内循环中对 $\\phi$ 进行采样。您的推导必须：\n- 从定义 $I(\\psi; Y \\mid d) = \\mathbb{E}_{\\psi}\\,\\mathbb{E}_{Y \\mid \\psi, d}\\!\\left[\\ln p(Y \\mid \\psi, d) - \\ln p(Y \\mid d)\\right]$ 开始，\n- 对于给定的分层模型，以闭式形式求得 $\\phi$-边缘化数据似然 $p(Y \\mid \\psi, d)$，以及\n- 生成一个基于 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^{M}$ 的单循环、样本重用的蒙特卡洛估计量，该估计量仅依赖于 $\\phi$-边缘化似然的评估值，并且不在关于 $Y$ 的内循环中对 $\\phi$ 进行采样。\n\n将您最终的估计量表示为关于 $H(d)$、$\\Sigma(d)$、$\\nu$ 和多元学生t分布密度的一个单一、显式的解析表达式，并确保每个数学符号都得到清晰的说明。最终答案必须是一个闭式表达式。不需要进行数值评估。", "solution": "问题陈述经过严格验证，具有科学依据、适定性、客观性和完整性。它描述了一个标准的分层贝叶斯模型，其中期望信息增益是一个明确定义的量。前提条件是一致的，任务是在逆问题和数据同化领域内进行形式化推导。因此，该问题被认为是有效的。\n\n目标是为期望信息增益（EIG）或互信息 $I(\\psi; Y \\mid d)$ 推导一个单循环、样本重用的蒙特卡洛估计量，该估计量通过解析方法将讨厌参数 $\\phi$ 边缘化掉。\n\nEIG定义为：\n$$\nI(\\psi; Y \\mid d) = \\mathbb{E}_{\\psi, Y \\mid d}\\! \\left[ \\ln\\frac{p(Y \\mid \\psi, d)}{p(Y \\mid d)} \\right] = \\mathbb{E}_{\\psi}\\,\\mathbb{E}_{Y \\mid \\psi, d}\\!\\left[\\ln p(Y \\mid \\psi, d) - \\ln p(Y \\mid d)\\right]\n$$\n期望是关于联合分布 $p(\\psi, Y \\mid d) = p(Y \\mid \\psi, d) p(\\psi)$ 的。通过从该联合分布中抽取 $M$ 个样本 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^{M}$ 来构建该期望的蒙特卡洛近似。具体方法是，首先从 $p(\\psi)$ 中抽取 $\\psi^{(i)}$，然后从 $p(Y \\mid \\psi^{(i)}, d)$ 中抽取 $Y^{(i)}$。\n\n推导过程主要分为三个步骤：\n1.  推导 $\\phi$-边缘化数据似然 $p(Y \\mid \\psi, d)$ 的解析形式。\n2.  构建避免对 $\\phi$ 采样的蒙特卡洛采样过程。\n3.  使用推导出的似然函数和针对证据项 $p(Y \\mid d)$ 的样本重用策略，构建 $I(\\psi; Y \\mid d)$ 的最终估计量。\n\n**步骤 1：边缘似然 $p(Y \\mid \\psi, d)$ 的推导**\n\n边缘似然 $p(Y \\mid \\psi, d)$ 是通过将完整似然 $p(Y, \\phi \\mid \\psi, d)$ 对讨厌参数 $\\phi$ 进行积分得到的。根据概率的链式法则以及 $\\psi$ 和 $\\phi$ 的先验独立性，我们有：\n$$\np(Y \\mid \\psi, d) = \\int_{0}^{\\infty} p(Y, \\phi \\mid \\psi, d) \\, d\\phi = \\int_{0}^{\\infty} p(Y \\mid \\psi, \\phi, d) \\, p(\\phi \\mid \\psi, d) \\, d\\phi = \\int_{0}^{\\infty} p(Y \\mid \\psi, \\phi, d) \\, p(\\phi) \\, d\\phi\n$$\n问题指明了这两种密度的形式：\n数据似然是一个多元正态分布：\n$$\np(Y \\mid \\psi, \\phi, d) = \\mathcal{N}(Y; H(d)\\psi, \\phi^{-1}\\Sigma(d)) = \\frac{(\\det(\\phi^{-1}\\Sigma(d)))^{-1/2}}{(2\\pi)^{n_y/2}} \\exp\\left(-\\frac{1}{2} (Y-H(d)\\psi)^T (\\phi^{-1}\\Sigma(d))^{-1} (Y-H(d)\\psi)\\right)\n$$\n$$\np(Y \\mid \\psi, \\phi, d) = \\frac{\\phi^{n_y/2}}{ (2\\pi)^{n_y/2} \\det(\\Sigma(d))^{1/2}} \\exp\\left(-\\frac{\\phi}{2} (Y-H(d)\\psi)^T \\Sigma(d)^{-1} (Y-H(d)\\psi)\\right)\n$$\n噪声精度 $\\phi$ 的先验是一个 Gamma 分布：\n$$\np(\\phi) = \\mathrm{Gamma}\\left(\\phi; \\frac{\\nu}{2}, \\frac{\\nu}{2}\\right) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\phi^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\phi\\right)\n$$\n现在，我们计算 $p(Y \\mid \\psi, d)$ 的积分：\n$$\np(Y \\mid \\psi, d) = \\int_{0}^{\\infty} \\left[ \\frac{\\phi^{n_y/2} \\exp\\left(-\\frac{\\phi}{2} \\delta^2(Y, \\psi)\\right)}{(2\\pi)^{n_y/2} \\det(\\Sigma(d))^{1/2}} \\right] \\left[ \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\phi^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\phi\\right) \\right] d\\phi\n$$\n其中 $\\delta^2(Y, \\psi) = (Y-H(d)\\psi)^T \\Sigma(d)^{-1} (Y-H(d)\\psi)$ 是马氏距离的平方。\n合并与 $\\phi$ 相关的项：\n$$\np(Y \\mid \\psi, d) = \\frac{(\\nu/2)^{\\nu/2}}{(2\\pi)^{n_y/2} \\det(\\Sigma(d))^{1/2} \\Gamma(\\nu/2)} \\int_{0}^{\\infty} \\phi^{\\frac{n_y+\\nu}{2}-1} \\exp\\left(-\\frac{\\phi}{2}(\\delta^2(Y, \\psi) + \\nu)\\right) d\\phi\n$$\n该积分是 Gamma 分布的核。使用恒等式 $\\int_0^\\infty x^{a-1} \\exp(-bx) dx = \\frac{\\Gamma(a)}{b^a}$，并令 $a = \\frac{n_y+\\nu}{2}$ 和 $b = \\frac{\\delta^2(Y, \\psi) + \\nu}{2}$，积分的计算结果为：\n$$\n\\int_{0}^{\\infty} \\dots d\\phi = \\frac{\\Gamma\\left(\\frac{n_y+\\nu}{2}\\right)}{\\left(\\frac{\\delta^2(Y, \\psi) + \\nu}{2}\\right)^{\\frac{n_y+\\nu}{2}}}\n$$\n将此结果代回 $p(Y \\mid \\psi, d)$ 的表达式中：\n$$\np(Y \\mid \\psi, d) = \\frac{(\\nu/2)^{\\nu/2} \\Gamma\\left(\\frac{n_y+\\nu}{2}\\right)}{(2\\pi)^{n_y/2} \\det(\\Sigma(d))^{1/2} \\Gamma(\\nu/2)} \\frac{2^{\\frac{n_y+\\nu}{2}}}{(\\delta^2(Y, \\psi) + \\nu)^{\\frac{n_y+\\nu}{2}}}\n$$\n简化常数项可得：\n$$\np(Y \\mid \\psi, d) = \\frac{\\Gamma\\left(\\frac{\\nu+n_y}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) (\\nu\\pi)^{n_y/2} \\det(\\Sigma(d))^{1/2}} \\left(1 + \\frac{\\delta^2(Y, \\psi)}{\\nu}\\right)^{-\\frac{\\nu+n_y}{2}}\n$$\n这是多元学生t分布的概率密度函数（PDF）。令 $t_{n_y}(y; \\mu, S, \\nu_{df})$ 表示随机向量 $y \\in \\mathbb{R}^{n_y}$ 的PDF，其位置参数为 $\\mu$，尺度矩阵为 $S$，自由度为 $\\nu_{df}$。那么我们可以写成：\n$$\np(Y \\mid \\psi, d) = t_{n_y}(Y; H(d)\\psi, \\Sigma(d), \\nu)\n$$\n\n**步骤 2：蒙特卡洛采样过程**\n\n用于采样的联合分布为 $p(\\psi, Y \\mid d) = p(Y \\mid \\psi, d) p(\\psi)$。我们通过祖先采样生成 $M$ 个样本 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^M$：\n1.  对于 $i=1, \\dots, M$，从其先验分布中抽取参数样本 $\\psi^{(i)}$：$\\psi^{(i)} \\sim p(\\psi)$。\n2.  对于每个 $\\psi^{(i)}$，从上面推导出的边缘似然中抽取对应的数据样本 $Y^{(i)}$：$Y^{(i)} \\sim p(Y \\mid \\psi^{(i)}, d) = t_{n_y}(\\cdot; H(d)\\psi^{(i)}, \\Sigma(d), \\nu)$。\n此过程不需要对讨厌参数 $\\phi$进行采样。\n\n**步骤 3：单循环估计量的构建**\n\nEIG 可以通过生成的样本 $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^M$ 估计如下：\n$$\n\\hat{I}_M(\\psi; Y \\mid d) = \\frac{1}{M} \\sum_{i=1}^M \\left[ \\ln p(Y^{(i)} \\mid \\psi^{(i)}, d) - \\ln p(Y^{(i)} \\mid d) \\right]\n$$\n第一项 $p(Y^{(i)} \\mid \\psi^{(i)}, d)$ 可使用学生t分布的PDF直接计算：\n$$\np(Y^{(i)} \\mid \\psi^{(i)}, d) = t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu)\n$$\n第二项，即证据 $p(Y^{(i)} \\mid d)$，难以直接计算。它被定义为似然函数在 $\\psi$ 的先验上的期望：\n$$\np(Y^{(i)} \\mid d) = \\int p(Y^{(i)} \\mid \\psi', d) \\, p(\\psi') \\, d\\psi' = \\mathbb{E}_{\\psi'} \\left[ t_{n_y}(Y^{(i)}; H(d)\\psi', \\Sigma(d), \\nu) \\right]\n$$\n我们可以使用蒙特卡洛近似来估计这个期望。构建一个高效估计量的一个关键思想是，重用同一组参数样本 $\\{\\psi^{(j)}\\}_{j=1}^M$ 来估计每个数据点 $Y^{(i)}$ 的证据。这给出了证据的估计量：\n$$\n\\hat{p}(Y^{(i)} \\mid d) = \\frac{1}{M} \\sum_{j=1}^M p(Y^{(i)} \\mid \\psi^{(j)}, d) = \\frac{1}{M} \\sum_{j=1}^M t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu)\n$$\n将这些表达式代回 EIG 估计量，得到最终的单循环、样本重用的蒙特卡洛估计量：\n$$\n\\hat{I}_M(\\psi; Y \\mid d) = \\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\ln t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu) - \\ln \\left( \\frac{1}{M} \\sum_{j=1}^{M} t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu) \\right) \\right]\n$$\n该表达式仅依赖于 $\\phi$-边缘化似然的评估值。\n\n为清晰起见，对于向量 $y \\in \\mathbb{R}^{n_y}$，其位置参数为 $\\mu$，尺度矩阵为 $S$，自由度为 $\\nu_{df}$ 的多元学生t分布密度函数 $t_{n_y}(y; \\mu, S, \\nu_{df})$ 定义为：\n$$\nt_{n_y}(y; \\mu, S, \\nu_{df}) = \\frac{\\Gamma\\left(\\frac{\\nu_{df}+n_y}{2}\\right)}{\\Gamma\\left(\\frac{\\nu_{df}}{2}\\right) (\\nu_{df}\\pi)^{n_y/2} \\det(S)^{1/2}} \\left(1 + \\frac{1}{\\nu_{df}}(y-\\mu)^T S^{-1} (y-\\mu)\\right)^{-\\frac{\\nu_{df}+n_y}{2}}\n$$\n在我们的情况下，参数为 $y=Y^{(i)}$、$\\mu=H(d)\\psi^{(j)}$、$S=\\Sigma(d)$ 和 $\\nu_{df}=\\nu$。", "answer": "$$\n\\boxed{\\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\ln t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu) - \\ln \\left( \\frac{1}{M} \\sum_{j=1}^{M} t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu) \\right) \\right]}\n$$", "id": "3380344"}]}