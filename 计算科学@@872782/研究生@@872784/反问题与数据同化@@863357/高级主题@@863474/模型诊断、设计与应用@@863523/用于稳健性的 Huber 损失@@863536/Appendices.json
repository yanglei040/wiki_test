{"hands_on_practices": [{"introduction": "第一个练习将通过一个具体实例，为你建立对Huber估计器如何在存在异常值时工作的基本理解。通过手动计算并与传统的$L_2$和$L_1$估计器进行比较，你将直观地感受到其稳健性以及它作为两者之间折衷的角色。在深入研究更复杂的算法解决方案之前，这是至关重要的一步。[@problem_id:3389451]", "problem": "考虑一个数据同化（DA）背景下的标量逆问题，其中未知状态是一个实数参数 $x \\in \\mathbb{R}$。通过单位观测算子可以获得两个独立的点观测值，因此每个观测值都直接测量 $x$。一个观测值是干净的，另一个则被一个大误差污染。数值上，观测值为 $y_{1} = 1.2$（干净）和 $y_{2} = 9.0$（受污染）。为了反映不同的信任程度，为 $y_{1}$ 分配观测权重 $w_{1} = 2$，为 $y_{2}$ 分配观测权重 $w_{2} = 0.5$。设 Huber 损失阈值为 $\\delta = 1$。\n\n将两个观测值的残差定义为 $r_{1}(x) = x - y_{1}$ 和 $r_{2}(x) = x - y_{2}$。考虑与三种数据失配模型相对应的三个估计器：\n- $L_{2}$（最小二乘）估计器最小化加权二次失配 $\\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$。\n- $L_{1}$（最小绝对偏差）估计器最小化加权绝对失配 $\\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$。\n- Huber 估计器最小化加权 Huber 损失 $\\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$，阈值为 $\\delta$，其中 $\\rho_{\\delta}$ 是 Huber 损失。\n\n从这些损失函数的核心定义和凸最小化的一阶最优性条件出发，推导针对这个双观测标量问题的每个估计器。为给定的数值显式计算这三个估计值，并对它们进行数值比较。仅报告 Huber 估计值 $\\hat{x}_{\\mathrm{Huber}}$ 的数值。无需四舍五入。未知状态 $x$ 是无量纲的，因此最终答案应表示为不带单位的纯数。", "solution": "在尝试求解之前，对问题陈述的有效性进行严格评估。\n\n### 第1步：提取已知条件\n- 未知状态：一个实数参数 $x \\in \\mathbb{R}$。\n- 观测值：$y_{1} = 1.2$（干净），$y_{2} = 9.0$（受污染）。\n- 观测算子：单位算子。\n- 观测权重：$w_{1} = 2$, $w_{2} = 0.5$。\n- Huber 损失阈值：$\\delta = 1$。\n- 残差：$r_{1}(x) = x - y_{1}$, $r_{2}(x) = x - y_{2}$。\n- $L_{2}$ 估计器最小化：$J_2(x) = \\sum_{i=1}^{2} w_{i} \\, r_{i}(x)^{2}$。\n- $L_{1}$ 估计器最小化：$J_1(x) = \\sum_{i=1}^{2} w_{i} \\, |r_{i}(x)|$。\n- Huber 估计器最小化：$J_H(x) = \\sum_{i=1}^{2} w_{i} \\, \\rho_{\\delta}(r_{i}(x))$，其中 $\\rho_{\\delta}$ 是 Huber 损失函数。\n\n### 第2步：使用提取的已知条件进行验证\n根据预定义标准评估问题：\n- **科学依据：** 该问题是统计估计和优化中的一个标准练习，特别是在应用于逆问题和数据同化的鲁棒统计背景下。$L_1$、$L_2$ 和 Huber 损失的概念是这些领域的基础。该问题的设定在科学上和数学上都是合理的。\n- **适定性：** 对于标量变量 $x$，定义的代价函数（$L_1$、$L_2$、Huber）都是凸函数。在 $\\mathbb{R}$ 上的凸函数具有非空的全局最小化子集，对于这些严格凸（或分段严格凸）函数，最小化子是唯一的。该问题是适定的。\n- **客观性：** 问题使用精确的数学定义和数值进行陈述。没有主观或含糊不清的术语。\n- **完整性和一致性：** 提供了所有必要的数据（$y_1, y_2, w_1, w_2, \\delta$）和定义。设定是自洽的，没有矛盾。\n- **现实性和可行性：** 拥有干净数据和受污染数据混合的场景是数据分析中的一个典型问题，这使得该问题具有现实性。所选的数值旨在清晰地说明不同估计器的行为差异。\n\n### 第3步：结论与行动\n该问题是**有效的**。这是一个适定的、有科学依据的问题，可以使用既定的数学原理来解决。我现在将开始推导解答。\n\n目标是找到最小化各自代价函数的估计值 $\\hat{x}_{L_2}$、$\\hat{x}_{L_1}$ 和 $\\hat{x}_{\\text{Huber}}$，然后报告 $\\hat{x}_{\\text{Huber}}$ 的数值。\n\n**1. $L_2$ (最小二乘) 估计器**\n$L_2$ 代价函数由下式给出：\n$$J_2(x) = w_1 (x-y_1)^2 + w_2 (x-y_2)^2$$\n这是一个关于 $x$ 的二次函数，其最小值可以通过将其关于 $x$ 的一阶导数设为零来找到。\n$$\\frac{dJ_2(x)}{dx} = 2w_1(x-y_1) + 2w_2(x-y_2) = 0$$\n求解 $x$：\n$$w_1 x - w_1 y_1 + w_2 x - w_2 y_2 = 0$$\n$$(w_1 + w_2)x = w_1 y_1 + w_2 y_2$$\n$L_2$ 估计器 $\\hat{x}_{L_2}$ 是观测值的加权平均值：\n$$\\hat{x}_{L_2} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2}$$\n代入给定的数值：$y_1=1.2$，$y_2=9.0$，$w_1=2$ 和 $w_2=0.5$。\n$$\\hat{x}_{L_2} = \\frac{2(1.2) + 0.5(9.0)}{2 + 0.5} = \\frac{2.4 + 4.5}{2.5} = \\frac{6.9}{2.5} = 2.76$$\n\n**2. $L_1$ (最小绝对偏差) 估计器**\n$L_1$ 代价函数为：\n$$J_1(x) = w_1|x-y_1| + w_2|x-y_2|$$\n该函数是凸函数，但在 $x=y_1$ 和 $x=y_2$ 处不可微。最小化子是观测值的加权中位数。一阶最优性条件是 $0 \\in \\partial J_1(x)$，其中 $\\partial J_1(x)$ 是次梯度。$J_1(x)$ 的导数（在存在的地方）是：\n$$\\frac{dJ_1(x)}{dx} = w_1 \\text{sgn}(x-y_1) + w_2 \\text{sgn}(x-y_2)$$\n让我们根据 $x$ 相对于 $y_1=1.2$ 和 $y_2=9.0$ 的值来分析导数：\n- 对于 $x  1.2$：$\\frac{dJ_1}{dx} = -w_1 - w_2 = -2 - 0.5 = -2.5  0$。函数是递减的。\n- 对于 $1.2  x  9.0$：$\\frac{dJ_1}{dx} = w_1 - w_2 = 2 - 0.5 = 1.5 > 0$。函数是递增的。\n- 对于 $x > 9.0$：$\\frac{dJ_1}{dx} = w_1 + w_2 = 2 + 0.5 = 2.5 > 0$。函数是递增的。\n导数在 $x = y_1 = 1.2$ 处从负变正。因此，$J_1(x)$ 的最小值出现在这一点。\n$$\\hat{x}_{L_1} = y_1 = 1.2$$\n\n**3. Huber 估计器**\nHuber 代价函数由 $J_H(x) = w_1 \\rho_{\\delta}(x-y_1) + w_2 \\rho_{\\delta}(x-y_2)$ 给出，其中 Huber 损失函数 $\\rho_{\\delta}(r)$ 定义为：\n$$\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2}r^2  \\text{if } |r| \\le \\delta \\\\ \\delta|r| - \\frac{1}{2}\\delta^2  \\text{if } |r| > \\delta \\end{cases}$$\n这个函数是凸函数且连续可微。其导数是 Huber 分数函数 $\\psi_{\\delta}(r) = \\rho'_{\\delta}(r)$：\n$$\\psi_{\\delta}(r) = \\begin{cases} r  \\text{if } |r| \\le \\delta \\\\ \\delta \\cdot \\text{sgn}(r)  \\text{if } |r| > \\delta \\end{cases} $$\n$J_H(x)$ 的最小值可以通过将其导数设为零来找到：\n$$\\frac{dJ_H(x)}{dx} = w_1 \\psi_{\\delta}(x-y_1) + w_2 \\psi_{\\delta}(x-y_2) = 0$$\n使用给定的值 $w_1=2$，$w_2=0.5$，$y_1=1.2$，$y_2=9.0$ 和 $\\delta=1$，方程变为：\n$$2 \\cdot \\psi_{1}(x-1.2) + 0.5 \\cdot \\psi_{1}(x-9.0) = 0$$\n基于其他估计值和权重，我们预期解 $\\hat{x}_{\\text{Huber}}$ 会接近 $y_1=1.2$ 但不完全相同。这表明第一个观测值的残差 $r_1 = x-y_1$ 很小，而第二个观测值的残差 $r_2 = x-y_2$ 很大。我们假设 $|x-y_1| \\le \\delta$ 和 $|x-y_2| > \\delta$。\n- 如果 $|r_1| = |x-1.2| \\le 1$，则 $\\psi_{1}(x-1.2) = x-1.2$。\n- 如果 $|r_2| = |x-9.0| > 1$，则 $\\psi_{1}(x-9.0) = 1 \\cdot \\text{sgn}(x-9.0)$。由于我们预期 $x$ 接近 $1.2$，所以 $x-9.0$ 会是负数，因此 $\\text{sgn}(x-9.0)=-1$。\n将这些代入最优性条件：\n$$2(x-1.2) + 0.5(-1) = 0$$\n$$2x - 2.4 - 0.5 = 0$$\n$$2x = 2.9$$\n$$x = 1.45$$\n我们现在必须验证这个解是否与我们的初始假设一致。\n- 对于 $x=1.45$，第一个残差是 $r_1 = 1.45 - 1.2 = 0.25$。我们检查是否 $|r_1| \\le \\delta$：$|0.25| \\le 1$。这是成立的。\n- 第二个残差是 $r_2 = 1.45 - 9.0 = -7.55$。我们检查是否 $|r_2| > \\delta$：$|-7.55| > 1$。这也是成立的。\n假设成立，因此 Huber 估计值为：\n$$\\hat{x}_{\\text{Huber}} = 1.45$$\n\n**估计值比较**\n- $\\hat{x}_{L_2} = 2.76$：这个估计值受到离群值 $y_2=9.0$ 的强烈影响。对大残差 $r_2$ 的二次惩罚将估计值显著地拉离干净的观测值 $y_1=1.2$，显示了最小二乘法的不鲁棒性。\n- $\\hat{x}_{L_1} = 1.2$：这个估计值，即加权中位数，完全舍弃了离群值的信息，并因其权重较大而锁定在干净的观测值上。这展示了极强的鲁棒性。\n- $\\hat{x}_{\\text{Huber}} = 1.45$：这个估计值代表了一种折衷。它接近干净的观测值 $y_1=1.2$，显示出对离群值 $y_2=9.0$ 的鲁棒性。然而，与 $L_1$ 估计不同，它仍然受到离群值的轻微影响，因为 Huber 损失的线性惩罚部分仍然允许离群值有一个小的、恒定的梯度贡献。它位于 $L_1$ 和 $L_2$ 估计值之间。\n\n问题只要求 Huber 估计的数值。", "answer": "$$\n\\boxed{1.45}\n$$", "id": "3389451"}, {"introduction": "在建立了对Huber估计器的概念性理解之后，这个实践将引导你进入一个常见的算法解决方案：迭代重加权最小二乘法（IRLS）。你将为一个线性模型手动完成一次迭代计算，亲眼见证Huber损失函数如何转化为一个能够系统性地降低异常值权重的加权方案。这个练习连接了损失函数的定义与其在实践中的优化过程。[@problem_id:3389420]", "problem": "考虑一个线性逆问题，其中状态向量 $\\theta \\in \\mathbb{R}^{2}$ 参数化一个线性观测算子 $g(x;\\theta) = \\theta_{1} x + \\theta_{2}$。给定一组 $4$ 个标量观测值 $\\{(x_{i}, d_{i})\\}_{i=1}^{4}$，其设计点和数据由 $(x_{1}, d_{1}) = (0, 1)$、$(x_{2}, d_{2}) = (1, 3)$、$(x_{3}, d_{3}) = (2, 5)$ 和 $(x_{4}, d_{4}) = (3, 40)$ 给出。第四个数据点被一个严重误差污染，产生了一个离群值。将使用Huber损失进行稳健估计。带有阈值 $\\delta  0$ 的Huber损失 $\\rho_{\\delta}: \\mathbb{R} \\to \\mathbb{R}$ 定义为\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2},  |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  |r| > \\delta,\n\\end{cases}\n$$\n其中 $r$ 是一个残差。初始参数为 $\\theta^{(0)} = (\\theta_{1}^{(0)}, \\theta_{2}^{(0)}) = (1.5, 0.5)$，阈值为 $\\delta = 2$。\n\n从稳健M估计和Huber损失的核心定义出发，推导残差 $r_{i}^{(0)} = d_{i} - (\\theta_{1}^{(0)} x_{i} + \\theta_{2}^{(0)})$ 的迭代重加权最小二乘法 (IRLS; Iteratively Reweighted Least Squares) 权重，为该双参数线性模型构建相应的加权正规方程，并精确求解它们以获得下一次迭代的参数 $\\theta^{(1)}$。最后，将参数更新 $\\Delta \\theta = \\theta^{(1)} - \\theta^{(0)}$ 计算为精确有理数。使用 `\\texttt{pmatrix}` 环境将最终答案表示为一个双分量行向量。无需进行四舍五入。", "solution": "该问题要求使用迭代重加权最小二乘法 (IRLS) 算法，对一个带有Huber损失函数的M估计，为一个线性模型求取第一次参数更新 $\\Delta \\theta$。\n\n线性模型由 $g(x;\\theta) = \\theta_{1} x + \\theta_{2}$ 给出，其中 $\\theta = (\\theta_{1}, \\theta_{2})^T \\in \\mathbb{R}^{2}$。\n目标是最小化残差的Huber损失之和：\n$$\nJ(\\theta) = \\sum_{i=1}^{4} \\rho_{\\delta}(r_i) = \\sum_{i=1}^{4} \\rho_{\\delta}(d_i - g(x_i; \\theta))\n$$\nHuber损失函数 $\\rho_{\\delta}(r)$ 定义为：\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2},  |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  |r| > \\delta.\n\\end{cases}\n$$\n为了最小化 $J(\\theta)$，我们将其关于 $\\theta$ 的梯度设为零。$\\rho_{\\delta}(r)$ 关于 $r$ 的导数是影响函数 $\\psi_{\\delta}(r)$：\n$$\n\\psi_{\\delta}(r) = \\frac{d\\rho_{\\delta}}{dr}(r) = \n\\begin{cases}\nr,  |r| \\leq \\delta, \\\\\n\\delta \\, \\text{sgn}(r),  |r| > \\delta.\n\\end{cases}\n$$\n代价函数的梯度为：\n$$\n\\nabla_{\\theta} J(\\theta) = \\sum_{i=1}^{4} \\frac{\\partial \\rho_{\\delta}(r_i)}{\\partial r_i} \\frac{\\partial r_i}{\\partial \\theta} = \\sum_{i=1}^{4} \\psi_{\\delta}(r_i) (-\\nabla_{\\theta} g(x_i; \\theta)) = 0.\n$$\n由于 $g(x_i; \\theta) = \\theta_1 x_i + \\theta_2$，我们有 $\\nabla_{\\theta} g(x_i; \\theta) = (x_i, 1)^T$。设其为设计矩阵 $G$ 的第 $i$ 行的转置。\n方程组变为：\n$$\n\\sum_{i=1}^{4} \\psi_{\\delta}(d_i - g(x_i; \\theta)) \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nIRLS 算法通过定义一个权重函数 $w(r) = \\psi_{\\delta}(r)/r$ 来线性化这个系统。\n$$\nw(r) = \n\\begin{cases}\n1,  |r| \\leq \\delta, \\\\\n\\frac{\\delta}{|r|},  |r| > \\delta.\n\\end{cases}\n$$\n使用这个权重函数，方程组可以写为：\n$$\n\\sum_{i=1}^{4} w(r_i) r_i \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\sum_{i=1}^{4} w(r_i) (d_i - (\\theta_1 x_i + \\theta_2)) \\begin{pmatrix} x_i \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\n在每次迭代 $k$ 中，我们根据前一次的残差 $r_i^{(k-1)} = d_i - g(x_i; \\theta^{(k-1)})$ 计算权重 $w_i^{(k)} = w(r_i^{(k-1)})$，然后为 $\\theta^{(k)}$ 求解以下加权最小二乘问题：\n$$\n\\theta^{(k)} = \\arg\\min_{\\theta} \\sum_{i=1}^{4} w_i^{(k)} (d_i - g(x_i; \\theta))^2.\n$$\n这导出了加权正规方程：\n$$\n(G^T W^{(k)} G) \\theta^{(k)} = G^T W^{(k)} d,\n$$\n其中 $G$ 是设计矩阵，$d$ 是观测向量，$W^{(k)}$ 是权重 $w_i^{(k)}$ 构成的对角矩阵。\n\n首先，我们使用初始参数 $\\theta^{(0)} = (1.5, 0.5) = (3/2, 1/2)$ 和模型 $g(x_i; \\theta^{(0)}) = \\frac{3}{2}x_i + \\frac{1}{2}$ 来计算初始残差 $r_i^{(0)}$。数据点为 $(x_1, d_1) = (0, 1)$、$(x_2, d_2) = (1, 3)$、$(x_3, d_3) = (2, 5)$ 和 $(x_4, d_4) = (3, 40)$。\n$r_1^{(0)} = 1 - (\\frac{3}{2}(0) + \\frac{1}{2}) = 1 - \\frac{1}{2} = \\frac{1}{2} = 0.5$。\n$r_2^{(0)} = 3 - (\\frac{3}{2}(1) + \\frac{1}{2}) = 3 - 2 = 1$。\n$r_3^{(0)} = 5 - (\\frac{3}{2}(2) + \\frac{1}{2}) = 5 - \\frac{7}{2} = \\frac{3}{2} = 1.5$。\n$r_4^{(0)} = 40 - (\\frac{3}{2}(3) + \\frac{1}{2}) = 40 - 5 = 35$。\n\n接下来，我们使用阈值 $\\delta = 2$ 计算第一次迭代的权重 $w_i^{(1)}$。\n$|r_1^{(0)}| = 0.5 \\leq 2 \\implies w_1^{(1)} = 1$。\n$|r_2^{(0)}| = 1 \\leq 2 \\implies w_2^{(1)} = 1$。\n$|r_3^{(0)}| = 1.5 \\leq 2 \\implies w_3^{(1)} = 1$。\n$|r_4^{(0)}| = 35 > 2 \\implies w_4^{(1)} = \\frac{\\delta}{|r_4^{(0)}|} = \\frac{2}{35}$。\n\n现在，我们为 $\\theta^{(1)}$ 建立加权正规方程。设计矩阵 $G$、数据向量 $d$ 和权重矩阵 $W^{(1)}$ 为：\n$$\nG = \\begin{pmatrix} 0  1 \\\\ 1  1 \\\\ 2  1 \\\\ 3  1 \\end{pmatrix}, \\quad d = \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 40 \\end{pmatrix}, \\quad W^{(1)} = \\text{diag}\\left(1, 1, 1, \\frac{2}{35}\\right).\n$$\n我们需要计算 $G^T W^{(1)} G$ 和 $G^T W^{(1)} d$。\n$$\nG^T W^{(1)} = \\begin{pmatrix} 0  1  2  3 \\\\ 1  1  1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  \\frac{2}{35} \\end{pmatrix} = \\begin{pmatrix} 0  1  2  \\frac{6}{35} \\\\ 1  1  1  \\frac{2}{35} \\end{pmatrix}.\n$$\n然后，\n$$\nG^T W^{(1)} G = \\begin{pmatrix} 0  1  2  \\frac{6}{35} \\\\ 1  1  1  \\frac{2}{35} \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  1 \\\\ 2  1 \\\\ 3  1 \\end{pmatrix} = \\begin{pmatrix} 1+4+\\frac{18}{35}  1+2+\\frac{6}{35} \\\\ 1+2+\\frac{6}{35}  1+1+1+\\frac{2}{35} \\end{pmatrix}\n\\\\ = \\begin{pmatrix} 5+\\frac{18}{35}  3+\\frac{6}{35} \\\\ 3+\\frac{6}{35}  3+\\frac{2}{35} \\end{pmatrix} = \\begin{pmatrix} \\frac{193}{35}  \\frac{111}{35} \\\\ \\frac{111}{35}  \\frac{107}{35} \\end{pmatrix} = \\frac{1}{35}\\begin{pmatrix} 193  111 \\\\ 111  107 \\end{pmatrix}.\n$$\n以及，\n$$\nG^T W^{(1)} d = \\begin{pmatrix} 0  1  2  \\frac{6}{35} \\\\ 1  1  1  \\frac{2}{35} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 3 \\\\ 5 \\\\ 40 \\end{pmatrix} = \\begin{pmatrix} 3+10+\\frac{240}{35} \\\\ 1+3+5+\\frac{80}{35} \\end{pmatrix} = \\begin{pmatrix} 13+\\frac{48}{7} \\\\ 9+\\frac{16}{7} \\end{pmatrix}\n\\\\ = \\begin{pmatrix} \\frac{91+48}{7} \\\\ \\frac{63+16}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{139}{7} \\\\ \\frac{79}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{695}{35} \\\\ \\frac{395}{35} \\end{pmatrix} = \\frac{1}{35}\\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}.\n$$\n正规方程为 $\\frac{1}{35}\\begin{pmatrix} 193  111 \\\\ 111  107 \\end{pmatrix} \\theta^{(1)} = \\frac{1}{35}\\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}$，简化为：\n$$\n\\begin{pmatrix} 193  111 \\\\ 111  107 \\end{pmatrix} \\begin{pmatrix} \\theta_1^{(1)} \\\\ \\theta_2^{(1)} \\end{pmatrix} = \\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}.\n$$\n我们求解这个关于 $\\theta^{(1)}$ 的 $2 \\times 2$ 系统。设矩阵为 $A$。$A$ 的行列式为：\n$$\n\\det(A) = (193)(107) - (111)^2 = 20651 - 12321 = 8330.\n$$\n$A$ 的逆矩阵为：\n$$\nA^{-1} = \\frac{1}{8330} \\begin{pmatrix} 107  -111 \\\\ -111  193 \\end{pmatrix}.\n$$\n现在我们求 $\\theta^{(1)} = A^{-1} \\begin{pmatrix} 695 \\\\ 395 \\end{pmatrix}$：\n$$\n\\begin{pmatrix} \\theta_1^{(1)} \\\\ \\theta_2^{(1)} \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} (107)(695) - (111)(395) \\\\ -(111)(695) + (193)(395) \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} 74365 - 43845 \\\\ -77145 + 76235 \\end{pmatrix} = \\frac{1}{8330} \\begin{pmatrix} 30520 \\\\ -910 \\end{pmatrix}.\n$$\n我们化简 $\\theta^{(1)}$ 的分量：\n$$\n\\theta_1^{(1)} = \\frac{30520}{8330} = \\frac{3052}{833} = \\frac{436 \\times 7}{119 \\times 7} = \\frac{436}{119}.\n$$\n$$\n\\theta_2^{(1)} = \\frac{-910}{8330} = \\frac{-91}{833} = \\frac{-13 \\times 7}{119 \\times 7} = -\\frac{13}{119}.\n$$\n所以，$\\theta^{(1)} = (\\frac{436}{119}, -\\frac{13}{119})^T$。\n\n最后，我们计算参数更新 $\\Delta \\theta = \\theta^{(1)} - \\theta^{(0)}$：\n$$\n\\theta^{(0)} = \\left(\\frac{3}{2}, \\frac{1}{2}\\right)^T.\n$$\n$$\n\\Delta \\theta_1 = \\theta_1^{(1)} - \\theta_1^{(0)} = \\frac{436}{119} - \\frac{3}{2} = \\frac{2 \\times 436 - 3 \\times 119}{2 \\times 119} = \\frac{872 - 357}{238} = \\frac{515}{238}.\n$$\n$$\n\\Delta \\theta_2 = \\theta_2^{(1)} - \\theta_2^{(0)} = -\\frac{13}{119} - \\frac{1}{2} = \\frac{-13 \\times 2 - 1 \\times 119}{2 \\times 119} = \\frac{-26 - 119}{238} = -\\frac{145}{238}.\n$$\n参数更新为 $\\Delta \\theta = (\\frac{515}{238}, -\\frac{145}{238})^T$。\n这些分数是不可约的，因为 $515=5 \\times 103$，$145=5 \\times 29$，而 $238=2 \\times 7 \\times 17$。没有公因数。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{515}{238}  -\\frac{145}{238} \\end{pmatrix}}\n$$", "id": "3389420"}, {"introduction": "在现代数据同化中，尤其是在处理高维状态空间时，计算效率至关重要。这个编程练习将通过伴随方法（adjoint method）来计算梯度，从而解决在大规模优化框架中实现Huber损失的实际问题。使用有限差分验证梯度，并检查导数的饱和行为，是开发稳健可靠的科学计算代码所必需的关键技能。[@problem_id:3389414]", "problem": "考虑反问题和数据同化中的一个线性观测算子，其中未知状态向量为 $x \\in \\mathbb{R}^n$，观测向量为 $y \\in \\mathbb{R}^m$，正演算子为矩阵 $A \\in \\mathbb{R}^{m \\times n}$。观测-模型残差定义为 $r(x) = A x - y$。为实现对离群值的稳健性，数据失配采用带阈值 $\\delta  0$ 的 Huber 损失来衡量，该损失由函数 $\\rho_{\\delta} : \\mathbb{R} \\to \\mathbb{R}$ 定义，对于小残差，它与二次损失一致，对于大残差，则过渡到线性损失。总目标函数为 $J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x))$，其中 $r_i(x)$ 表示残差的第 $i$ 个分量。\n\n您的任务是从 $r(x)$ 和 Huber 损失 $\\rho_{\\delta}$ 的定义出发，使用链式法则实现 $J(x)$ 关于 $x$ 的基于伴随的梯度，并通过一个基于中心有限差分的稳健梯度检验程序来验证其正确性。此外，您必须明确验证 Huber 损失关于残差分量的导数所预期的饱和行为：对于满足 $|r_i(x)|  \\delta$ 的残差分量，其关于 $r_i$ 的对应导数必须取恒定幅值 $\\delta$，其符号与 $\\operatorname{sign}(r_i)$ 匹配；对于满足 $|r_i(x)| \\le \\delta$ 的残差分量，其导数应与残差本身一致。您的程序必须在包括中等残差、恰好在 $\\pm \\delta$ 的边界残差以及严重离群值的合成案例上评估这些性质。\n\n从残差 $r(x) = A x - y$ 和 Huber 损失 $\\rho_{\\delta}$（阈值内为二次，阈值外为线性）的基本定义出发，推导并实现：\n- 通过链式法则得到 $J(x)$ 关于 $x$ 的伴随梯度，用 $A$ 和 Huber 损失关于 $r$ 的导数表示。\n- 一个使用步长 $\\varepsilon  0$ 的中心差分梯度检验器，用于计算 $J(x)$ 梯度的每个分量的有限差分近似。\n- 一个饱和验证器，对于每个残差分量 $r_i(x)$，检查其关于 $r_i$ 的导数在 $|r_i(x)|  \\delta$ 时是否饱和于 $\\pm \\delta$，在 $|r_i(x)|  \\delta$ 时是否等于 $r_i(x)$，以及在边界 $r_i(x) = \\pm \\delta$ 时是否等于 $\\pm \\delta$。\n\n通过要求伴随梯度和中心差分梯度之间的最大绝对差小于给定容差 $\\tau  0$，来数值上判定梯度的正确性。饱和验证器应在边界处使用严格的数值相等检查，并在其他地方酌情使用基于容差的检查。\n\n实现以下测试套件，将每个案例的布尔结果计算为以下两项的逻辑与：\n- 梯度检查在容差标准下通过，以及\n- 所有残差分量的饱和检查均通过。\n\n测试案例：\n1. 使用单位正演算子的理想情况：\n   - 维度：$n = 5$, $m = 5$。\n   - $A = I_5$（$5 \\times 5$ 单位矩阵）。\n   - $x = [0.0, 1.0, -2.0, 0.5, -0.5]$。\n   - $y = [0.0, -1.0, 10.0, 0.4, -0.9]$。\n   - $\\delta = 0.7$。\n   - 有限差分步长：$\\varepsilon = 10^{-6}$。\n   - 梯度检查容差：$\\tau = 10^{-7}$。\n\n2. 在拐点处的边界情况：\n   - 维度：$n = 5$, $m = 5$。\n   - $A = I_5$。\n   - 选择残差 $r = [0.5, -0.5, 0.4999999, -0.5000001, 10.0]$ 并令 $x = [0, 0, 0, 0, 0]$，$y = -r$，从而使得 $r(x) = A x - y = r$ 精确成立。\n   - $\\delta = 0.5$。\n   - $\\varepsilon = 10^{-8}$。\n   - $\\tau = 10^{-6}$。\n\n3. 一般的非单位正演算子：\n   - 维度：$n = 4$, $m = 4$。\n   - $A = \\begin{bmatrix} 1.0  0.2  0.0  -0.1 \\\\ 0.0  1.5  -0.5  0.0 \\\\ 0.0  0.0  -1.0  2.0 \\\\ 0.3  0.0  0.0  0.7 \\end{bmatrix}$。\n   - $x = [0.2, -0.3, 1.0, -1.2]$。\n   - $y = [2.0, -1.0, 10.0, 0.0]$。\n   - $\\delta = 1.0$。\n   - $\\varepsilon = 10^{-6}$。\n   - $\\tau = 10^{-6}$。\n\n对于测试案例 2 中涉及边界 $r_i(x) = \\pm \\delta$ 处相等性的数值检查，请按定义严格处理相等性（无容差），而对于其他饱和检查，请使用 $10^{-12}$ 的小数值容差以考虑浮点运算。\n\n您的程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表（例如，“[result1,result2,result3]”），每个结果都是一个布尔值，指示相应测试案例的梯度检查和饱和检查条件是否都已通过。不允许外部输入，本问题中不出现物理单位；未使用角度，因此无需指定单位。最终输出必须严格遵循指定格式，通过单个打印语句输出。", "solution": "该问题要求推导和实现一个基于 Huber 损失的目标函数的伴随梯度，然后对其正确性和饱和性质进行数值验证。解决方案分三个阶段进行：数学推导、数值验证程序的描述和实现细节。\n\n首先，我们将问题形式化。状态向量为 $x \\in \\mathbb{R}^n$，观测向量为 $y \\in \\mathbb{R}^m$，线性正演算子为 $A \\in \\mathbb{R}^{m \\times n}$。残差向量定义为 $r(x) = A x - y$。目标函数 $J(x)$ 是应用于残差每个分量的 Huber 损失之和：\n$$ J(x) = \\sum_{i=1}^m \\rho_{\\delta}(r_i(x)) $$\n带阈值 $\\delta  0$ 的 Huber 损失函数 $\\rho_{\\delta}(z)$ 在小值时的二次（$L_2$）损失和大值时的线性（$L_1$）损失之间提供了一个过渡，从而增强了对离群值的稳健性。其标准定义确保了函数及其一阶导数的连续性，即：\n$$ \\rho_{\\delta}(z) = \\begin{cases} \\frac{1}{2} z^2,  \\text{若 } |z| \\le \\delta \\\\ \\delta |z| - \\frac{1}{2} \\delta^2,  \\text{若 } |z| > \\delta \\end{cases} $$\nHuber 损失关于其自变量的导数，记为 $\\psi(z) = \\frac{d\\rho_{\\delta}}{dz}(z)$，是梯度计算的关键组成部分。它由下式给出：\n$$ \\psi(z) = \\begin{cases} z,  \\text{若 } |z| \\le \\delta \\\\ \\delta \\cdot \\operatorname{sign}(z),  \\text{若 } |z| > \\delta \\end{cases} $$\n这个函数也被称为软阈值函数或收缩函数。\n\n主要任务是求 $J(x)$ 关于 $x$ 的梯度，记为 $\\nabla_x J(x)$。我们应用链式法则。梯度的第 $j$ 个分量是：\n$$ (\\nabla_x J(x))_j = \\frac{\\partial J}{\\partial x_j} = \\sum_{i=1}^m \\frac{d \\rho_{\\delta}}{d r_i}(r_i(x)) \\frac{\\partial r_i}{\\partial x_j} $$\n残差的第 $i$ 个分量 $r_i(x) = (Ax - y)_i = \\sum_{k=1}^n A_{ik} x_k - y_i$ 关于 $x_j$ 的偏导数是：\n$$ \\frac{\\partial r_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^n A_{ik} x_k - y_i \\right) = \\sum_{k=1}^n A_{ik} \\frac{\\partial x_k}{\\partial x_j} = \\sum_{k=1}^n A_{ik} \\delta_{kj} = A_{ij} $$\n其中 $\\delta_{kj}$ 是克罗内克 δ。\n将此代入梯度表达式并使用 $\\psi$ 的定义，我们得到：\n$$ (\\nabla_x J(x))_j = \\sum_{i=1}^m \\psi(r_i(x)) A_{ij} = \\sum_{i=1}^m (A^T)_{ji} \\psi(r_i(x)) $$\n此方程表示矩阵-向量积 $A^T \\psi(r(x))$ 的第 $j$ 个分量，其中 $\\psi(r(x))$ 是一个分量为 $\\psi(r_i(x))$ 的向量。因此，完整的梯度向量是：\n$$ \\nabla_x J(x) = A^T \\psi(A x - y) $$\n这就是基于伴随的梯度。它的计算效率很高，因为它需要一次与 $A$ 的矩阵-向量积来计算残差，然后应用逐元素函数 $\\psi$，最后再进行一次与转置（伴随）矩阵 $A^T$ 的矩阵-向量积。\n\n为验证这个解析推导出的梯度的正确性，我们采用中心有限差分近似。对于每个分量 $j \\in \\{1, \\dots, n\\}$，偏导数 $\\frac{\\partial J}{\\partial x_j}$ 近似为：\n$$ (\\nabla_x J(x))_j \\approx \\frac{J(x + \\varepsilon e_j) - J(x - \\varepsilon e_j)}{2\\varepsilon} $$\n其中 $e_j$ 是第 $j$ 个典范基向量，$\\varepsilon$ 是一个很小的步长。一个稳健的实现需要为梯度的每个分量计算两次完整的目​​标函数 $J$。梯度正确性的检查包括将基于伴随的梯度与此数值近似进行比较。如果两个梯度向量之间的最大绝对差低于指定的容差 $\\tau$，则检查通过：\n$$ \\max_{j} \\left| (\\nabla_x J(x))_{\\text{adjoint}, j} - (\\nabla_x J(x))_{\\text{fd}, j} \\right|  \\tau $$\n尽管在某些 $i$ 使得 $|r_i(x)| = \\delta$ 的点上 $J(x)$ 的二阶导数没有定义，但中心差分近似仍然是良态的，并提供了一个有效次梯度的良好估计，这在我们的公式中对应于解析梯度。\n\n最后，我们必须验证 Huber 导数 $\\psi(r_i)$ 的饱和行为。这涉及检查计算出的 $\\psi(r_i)$ 值是否对所有残差分量 $r_i$ 都符合其分段定义。检查内容如下：\n1. 对于线性区域中的残差分量 $|r_i| > \\delta$，我们验证 $\\psi(r_i)$ 是否在 $10^{-12}$ 的小数值容差内等于 $\\delta \\cdot \\operatorname{sign}(r_i)$。\n2. 对于二次区域中的残差分量 $|r_i|  \\delta$，我们验证 $\\psi(r_i)$ 是否在相同的数值容差内等于 $r_i$。\n3. 对于恰好在边界上的残差分量 $|r_i| = \\delta$，我们执行严格的相等性检查，即 $\\psi(r_i)$ 是否等于 $r_i$（即 $\\pm\\delta$）。这是有效的，因为我们对 $\\psi(z)$ 的定义是连续的，并且在 $\\psi(z)=z$ 的二次情况下包含了端点。\n\n每个测试案例的总体结果是一个布尔值，表示梯度检查通过和所有饱和检查通过的逻辑与，从而为实现提供了全面的验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy\n#       version: 1.23.5\n#     - name: scipy\n#       version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving, implementing, and verifying the adjoint-based gradient \n    of a Huber loss objective function for a set of predefined test cases.\n    \"\"\"\n\n    def huber_objective(x_vec, A_mat, y_vec, delta):\n        \"\"\"Computes the total Huber loss J(x).\"\"\"\n        residuals = A_mat @ x_vec - y_vec\n        loss = 0.0\n        for r_i in residuals:\n            abs_r_i = np.abs(r_i)\n            if abs_r_i = delta:\n                loss += 0.5 * r_i**2\n            else:\n                loss += delta * abs_r_i - 0.5 * delta**2\n        return loss\n\n    def huber_derivative(residuals, delta):\n        \"\"\"Computes the derivative of the Huber loss with respect to residuals, psi(r).\"\"\"\n        psi = np.zeros_like(residuals, dtype=float)\n        for i, r_i in enumerate(residuals):\n            if np.abs(r_i) = delta:\n                psi[i] = r_i\n            else:\n                psi[i] = delta * np.sign(r_i)\n        return psi\n\n    def run_test_case(params):\n        \"\"\"\n        Runs a single test case, performing adjoint gradient calculation, saturation checks, \n        and finite-difference gradient verification.\n        \"\"\"\n        A = params[\"A\"]\n        x = params[\"x\"]\n        y = params[\"y\"]\n        delta = params[\"delta\"]\n        eps = params[\"eps\"]\n        tau = params[\"tau\"]\n        sat_tol = 1e-12\n\n        # 1. Adjoint-based gradient calculation\n        residuals = A @ x - y\n        psi_r = huber_derivative(residuals, delta)\n        grad_adj = A.T @ psi_r\n\n        # 2. Saturation verifier\n        saturation_ok = True\n        for i in range(len(residuals)):\n            r_i = residuals[i]\n            psi_i = psi_r[i]\n            \n            check_passed = False\n            # Strict equality check for boundary cases, as required\n            if r_i == delta:\n                if psi_i == delta:\n                    check_passed = True\n            elif r_i == -delta:\n                if psi_i == -delta:\n                    check_passed = True\n            # Tolerance-based checks for non-boundary cases\n            elif np.abs(r_i)  delta:\n                if np.abs(psi_i - delta * np.sign(r_i))  sat_tol:\n                    check_passed = True\n            elif np.abs(r_i)  delta:\n                if np.abs(psi_i - r_i)  sat_tol:\n                    check_passed = True\n            \n            if not check_passed:\n                saturation_ok = False\n                break\n        \n        # 3. Central finite-difference gradient checker\n        n = len(x)\n        grad_fd = np.zeros(n)\n        for j in range(n):\n            x_plus = x.copy()\n            x_minus = x.copy()\n            x_plus[j] += eps\n            x_minus[j] -= eps\n            \n            J_plus = huber_objective(x_plus, A, y, delta)\n            J_minus = huber_objective(x_minus, A, y, delta)\n            \n            grad_fd[j] = (J_plus - J_minus) / (2 * eps)\n            \n        # 4. Gradient correctness check\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        gradient_ok = max_abs_diff  tau\n        \n        # 5. Final result is a conjunction of both checks\n        return gradient_ok and saturation_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 1.0, -2.0, 0.5, -0.5]),\n            \"y\": np.array([0.0, -1.0, 10.0, 0.4, -0.9]),\n            \"delta\": 0.7,\n            \"eps\": 1e-6,\n            \"tau\": 1e-7,\n        },\n        {\n            \"A\": np.identity(5),\n            \"x\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"y\": -np.array([0.5, -0.5, 0.4999999, -0.5000001, 10.0]),\n            \"delta\": 0.5,\n            \"eps\": 1e-8,\n            \"tau\": 1e-6,\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.2, 0.0, -0.1],\n                [0.0, 1.5, -0.5, 0.0],\n                [0.0, 0.0, -1.0, 2.0],\n                [0.3, 0.0, 0.0, 0.7]\n            ]),\n            \"x\": np.array([0.2, -0.3, 1.0, -1.2]),\n            \"y\": np.array([2.0, -1.0, 10.0, 0.0]),\n            \"delta\": 1.0,\n            \"eps\": 1e-6,\n            \"tau\": 1e-6,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case)\n        results.append(str(result).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3389414"}]}