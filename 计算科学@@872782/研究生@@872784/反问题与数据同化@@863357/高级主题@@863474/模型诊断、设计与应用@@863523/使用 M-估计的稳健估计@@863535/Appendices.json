{"hands_on_practices": [{"introduction": "M-估计器实现稳健性的核心思想是降低离群值在求解过程中的影响。本练习将通过一个基础性的计算任务来阐明这一思想，即推导并计算Huber损失函数在迭代重加权最小二乘（IRLS）算法中所对应的权重[@problem_id:3418133]。通过亲手计算这些权重，您将直观地理解M-估计器如何根据残差的大小来区分和处理正常数据点与离群值，从而为更复杂的算法打下坚实的基础。", "problem": "考虑一个数据同化中的线性反问题，其目标是通过一个线性观测算子 $H$ 从观测值 $y$ 估计状态向量 $x$，使得残差为 $r=y-Hx$。为了提高对离群值的稳健性，假设目标是最小化残差的稳健损失之和，具体来说，是使用调整常数 $c>0$ 的Huber损失。Huber损失定义为\n$$\n\\rho_{c}(r)=\\begin{cases}\n\\frac{1}{2}r^{2}, & |r|\\leq c,\\\\\nc|r|-\\frac{1}{2}c^{2}, & |r|>c.\n\\end{cases}\n$$\n在用于稳健$M$估计的迭代重加权最小二乘（IRLS）方案中，稳健目标 $\\sum_{i}\\rho_{c}(r_{i})$ 关于 $x$ 的梯度是使用得分函数 $\\psi_{c}(r)=\\frac{d}{dr}\\rho_{c}(r)$ 计算的，并且我们寻求一个加权最小二乘系统，其正规方程与给定迭代点附近的稳健目标的正规方程相匹配。从这些基本定义出发，通过令稳健目标和加权最小二乘目标的梯度相等，推导与Huber损失相关的IRLS权重函数 $w(r)$。然后，对于残差 $r=\\begin{pmatrix}-5 & -1 & 0.2 & 3\\end{pmatrix}$ 和调整常数 $c=1.5$，计算每个残差的Huber IRLS权重 $w(r_{i})$。最后，简要解释这些权重在一次高斯-牛顿步骤中对每次观测对更新的贡献所产生的影响。将最终权重表示为一个行向量。无需四舍五入；提供精确值。", "solution": "问题要求推导Huber损失的迭代重加权最小二乘（IRLS）权重函数，计算一组特定残差的这些权重，并对结果进行解释。\n\n首先，我们推导与Huber损失 $\\rho_c(r)$ 相关的IRLS权重函数 $w(r)$。M估计旨在最小化目标函数 $J_{robust}(x) = \\sum_{i} \\rho_{c}(r_i)$，其中 $r = y - Hx$ 是残差向量。该目标函数相对于状态向量 $x$ 的梯度可使用链式法则求得：\n$$ \\nabla_x J_{robust}(x) = \\sum_i \\frac{d\\rho_c(r_i)}{dr_i} \\nabla_x r_i $$\n残差分量 $r_i = y_i - \\sum_k H_{ik} x_k$ 相对于 $x$ 的导数是 $\\nabla_x r_i = -h_i^T$，其中 $h_i$ 是矩阵 $H$ 的第 $i$ 行。损失函数 $\\frac{d\\rho_c(r)}{dr}$ 的导数定义为得分函数 $\\psi_c(r)$。让我们根据Huber损失的定义来计算 $\\psi_c(r)$：\n$$ \\rho_{c}(r)=\\begin{cases} \\frac{1}{2}r^{2}, & |r|\\leq c\\\\ c|r|-\\frac{1}{2}c^{2}, & |r|>c \\end{cases} $$\n对于 $|r| \\leq c$，导数为 $\\psi_c(r) = \\frac{d}{dr} \\left( \\frac{1}{2}r^2 \\right) = r$。\n对于 $|r| > c$，我们考虑两种情况。如果 $r > c$，则 $\\rho_c(r) = cr - \\frac{1}{2}c^2$，因此 $\\psi_c(r) = c$。如果 $r < -c$，则 $\\rho_c(r) = -cr - \\frac{1}{2}c^2$，因此 $\\psi_c(r) = -c$。在这两种情况下，都有 $\\psi_c(r) = c \\cdot \\text{sgn}(r)$。\n综合以上，得分函数为：\n$$ \\psi_c(r) = \\begin{cases} r, & |r| \\leq c \\\\ c \\cdot \\text{sgn}(r), & |r| > c \\end{cases} $$\n因此，稳健目标函数的梯度为 $\\nabla_x J_{robust}(x) = -\\sum_i h_i^T \\psi_c(r_i) = -H^T \\vec{\\psi}_c(r)$，其中 $\\vec{\\psi}_c(r)$ 是元素为 $\\psi_c(r_i)$ 的列向量。最小值的必要条件是梯度为零，这给出了M估计的正规方程：\n$$ H^T \\vec{\\psi}_c(r) = 0 $$\nIRLS算法通过迭代求解加权最小二乘问题来求解这个非线性系统。加权最小二乘问题的目标函数是 $J_{WLS}(x) = \\frac{1}{2} \\sum_i w_i r_i^2 = \\frac{1}{2}(y-Hx)^T W (y-Hx)$，其中 $W$ 是由权重 $w_i$ 构成的对角矩阵。该目标的梯度为：\n$$ \\nabla_x J_{WLS}(x) = -H^T W (y-Hx) = -H^T W r $$\n相应的正规方程为：\n$$ H^T W r = 0 $$\nIRLS的核心原理是定义权重 $w_i$，使得加权最小二乘（WLS）问题的正规方程与M估计的正规方程相匹配。通过比较这两组正规方程，我们令乘以 $H^T$ 的项相等：\n$$ W r = \\vec{\\psi}_c(r) $$\n这个等价关系必须对每个分量 $i$ 都成立，因此 $w_i r_i = \\psi_c(r_i)$。权重函数 $w(r)$ 因此可以定义为：\n$$ w(r) = \\frac{\\psi_c(r)}{r} $$\n代入 $\\psi_c(r)$ 的表达式：\n对于 $|r| \\leq c$（且 $r \\neq 0$），$w(r) = \\frac{r}{r} = 1$。对于 $r=0$，我们可以定义 $w(0) = \\lim_{r \\to 0} \\frac{r}{r} = 1$。\n对于 $|r| > c$，$w(r) = \\frac{c \\cdot \\text{sgn}(r)}{r} = \\frac{c}{|r|}$。\n因此，Huber损失的IRLS权重函数为：\n$$ w(r) = \\begin{cases} 1, & |r| \\leq c \\\\ \\frac{c}{|r|}, & |r| > c \\end{cases} $$\n这可以更紧凑地写成 $w(r) = \\min(1, \\frac{c}{|r|})$。\n\n接下来，我们计算给定残差 $r=\\begin{pmatrix}-5 & -1 & 0.2 & 3\\end{pmatrix}$ 和调整常数 $c=1.5$ 时的权重。我们将推导出的权重函数应用于每个残差分量 $r_i$。\n\n对于 $r_1 = -5$：其绝对值为 $|r_1|=5$。因为 $5 > 1.5$，所以这是一个离群值。\n$w(r_1) = \\frac{c}{|r_1|} = \\frac{1.5}{5} = \\frac{3/2}{5} = \\frac{3}{10}$。\n\n对于 $r_2 = -1$：其绝对值为 $|r_2|=1$。因为 $1 \\leq 1.5$，所以这是一个内点值。\n$w(r_2) = 1$。\n\n对于 $r_3 = 0.2$：其绝对值为 $|r_3|=0.2$。因为 $0.2 \\leq 1.5$，所以这是一个内点值。\n$w(r_3) = 1$。\n\n对于 $r_4 = 3$：其绝对值为 $|r_4|=3$。因为 $3 > 1.5$，所以这是一个离群值。\n$w(r_4) = \\frac{c}{|r_4|} = \\frac{1.5}{3} = \\frac{1}{2}$。\n\n计算出的权重为 $w(r_1)=\\frac{3}{10}$，$w(r_2)=1$，$w(r_3)=1$ 和 $w(r_4)=\\frac{1}{2}$。\n\n最后，关于这些权重的解释：IRLS算法通过求解一系列加权最小二乘问题来迭代地逼近M估计问题的解。权重 $w_i$ 决定了每次观测 $y_i$ 在每一步对解的更新的影响程度。\n具有小残差 ($|r_i| \\le c$) 的观测被视为内点值。对于这些观测，Huber权重为 $w_i=1$，这意味着它们以全部强度对最小二乘拟合作出贡献，就像在标准的（非加权的）最小二乘问题中一样。在我们的例子中，对应于残差 $r_2=-1$ 和 $r_3=0.2$ 的观测被视为内点值。\n具有大残差 ($|r_i| > c$) 的观测被标记为离群值。它们的权重被设置为 $w_i = c/|r_i| < 1$。这降低了它们对成本函数的贡献，从而减小了它们对 $x$ 的解的影响。残差越大，权重越小。对于我们的数据，残差为 $r_4=3$ 的观测被适度地降权至 $\\frac{1}{2}$，而具有最大残差 $r_1=-5$ 的观测则被更显著地降权至 $\\frac{3}{10}$。这种机制确保了潜在的离群值不会主导估计过程，使得对 $x$ 的最终估计对数据 $y$ 中的严重错误更具稳健性。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{3}{10} & 1 & 1 & \\frac{1}{2} \\end{pmatrix} } $$", "id": "3418133"}, {"introduction": "现代优化算法通常依赖于将复杂问题分解为一系列更简单的子问题，而邻近算子（proximal operator）正是解决这些子问题的关键构件。本练习将指导您为几种不同的M-估计器（包括凸的Huber损失和非凸的Welsch及Geman-McClure损失）推导并实现其邻近算子[@problem_id:3418049]。掌握这项技能对于在实际中应用基于代理的分割算法（如ADMM或ISTA）解决稳健逆问题至关重要，因为它将抽象的损失函数转化为了具体的计算步骤。", "problem": "考虑一个反问题，其中数据同化步骤被建模为对一个标量残差的稳健邻近更新。对于给定的残差 $r \\in \\mathbb{R}$、正步长参数 $\\tau \\in \\mathbb{R}_{+}$ 以及一个稳健罚函数 $\\rho : \\mathbb{R} \\to \\mathbb{R}_{+}$（一个 M-估计量），邻近映射被定义为唯一的最小化子\n$$\n\\operatorname{prox}_{\\tau \\rho}(r) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right\\}.\n$$\n该邻近映射是用于稳健反演和数据同化的邻近分裂方法中的一个基本构建块。从可微的 $\\rho$ 的一阶最优性条件出发，即目标函数关于 $x$ 的导数为零，为以下三种 M-估计量推导 $\\operatorname{prox}_{\\tau \\rho}(r)$ 的可实现表达式或算法：\n\n- Huber 罚，阈值为 $\\delta \\in \\mathbb{R}_{+}$：\n$$\n\\rho_{\\mathrm{Huber}}(x) = \\begin{cases}\n\\frac{1}{2} x^2, & \\text{if } |x| \\le \\delta, \\\\\n\\delta |x| - \\frac{1}{2} \\delta^2, & \\text{if } |x| > \\delta,\n\\end{cases}\n$$\n其中 $|\\,\\cdot\\,|$ 表示绝对值。\n\n- Welsch (Leclerc) 罚，尺度为 $c \\in \\mathbb{R}_{+}$：\n$$\n\\rho_{\\mathrm{Welsch}}(x) = \\frac{c^2}{2}\\left(1 - \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right).\n$$\n\n- Geman–McClure 罚，尺度为 $c \\in \\mathbb{R}_{+}$：\n$$\n\\rho_{\\mathrm{GM}}(x) = \\frac{1}{2} \\frac{x^2}{1 + \\left(\\frac{x}{c}\\right)^2}.\n$$\n\n你的程序必须通过运用一阶最优性条件，为上述每种 $\\rho$ 实现一个邻近算子 $x = \\operatorname{prox}_{\\tau \\rho}(r)$。在存在闭式解的情况下，使用它来获得一个精确的分段定义表达式。在不存在初等闭式解的情况下，基于最优性方程推导一个适定的标量求根方案，并实现一个稳健的数值求解器（带有收敛容差和安全保障措施），该求解器能为所提供的测试套件中的任何输入可靠地返回最小化子。推导和实现必须是通用的和纯数学的；不涉及任何物理单位。\n\n测试套件。使用以下参数值和残差来测试一般行为、边界条件和边缘情况：\n\n- Huber 罚，使用 $\\delta = 1.25$ 和 $\\tau = 0.75$，在残差 $r \\in \\{-3.00, -\\delta(1+\\tau), 0.00, \\delta(1+\\tau), 2.70\\}$ 处进行评估，即 $r \\in \\{-3.00, -1.25(1+0.75), 0.00, 1.25(1+0.75), 2.70\\}$。\n\n- Welsch 罚，使用 $c = 1.00$ 和 $\\tau = 0.50$，在残差 $r \\in \\{-2.00, -1.00, 0.00, 1.00, 2.00\\}$ 处进行评估。\n\n- Geman–McClure 罚，使用 $c = 1.50$ 和 $\\tau = 0.80$，在残差 $r \\in \\{-3.00, -1.50, 0.00, 1.50, 3.00\\}$ 处进行评估。\n\n最终输出格式。你的程序应生成单行输出，其中包含按上述顺序列出的邻近值（首先是所有 Huber 的结果，按指定的 $r$ 顺序排列，然后是所有 Welsch 的结果，最后是所有 Geman–McClure 的结果），格式为用方括号括起来的逗号分隔列表，例如 $[x_1,x_2,\\dots,x_{15}]$。每个条目 $x_i$ 必须是一个实数（浮点数）。不应打印任何额外文本。", "solution": "任务是找到目标函数 $J(x)$ 的最小化子 $x^\\star$：\n$$\nx^\\star = \\operatorname{prox}_{\\tau \\rho}(r) = \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ J(x) = \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right\\}\n$$\n对于给定的残差 $r$、步长 $\\tau > 0$ 和稳健罚函数 $\\rho(x)$。假设 $\\rho(x)$ 在最小化子处可微，一阶最优性条件是目标函数 $J'(x)$ 的导数必须为零：\n$$\nJ'(x) = \\frac{d}{dx} \\left( \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right) = (x - r) + \\tau \\, \\rho'(x) = 0\n$$\n这个最优性方程 $x + \\tau \\rho'(x) = r$，构成了为每种指定的 M-估计量推导邻近算子的基础。\n\n### 1. Huber 罚\n\n由阈值 $\\delta > 0$ 参数化的 Huber 罚函数由下式给出：\n$$\n\\rho_{\\mathrm{Huber}}(x) = \\begin{cases}\n\\frac{1}{2} x^2, & \\text{if } |x| \\le \\delta, \\\\\n\\delta |x| - \\frac{1}{2} \\delta^2, & \\text{if } |x| > \\delta.\n\\end{cases}\n$$\n该函数是凸的且连续可微。其导数 $\\psi_{\\mathrm{Huber}}(x) = \\rho'_{\\mathrm{Huber}}(x)$ 为：\n$$\n\\psi_{\\mathrm{Huber}}(x) = \\begin{cases}\nx, & \\text{if } |x| \\le \\delta, \\\\\n\\delta \\operatorname{sgn}(x), & \\text{if } |x| > \\delta.\n\\end{cases}\n$$\n最优性条件 $x - r + \\tau \\psi_{\\mathrm{Huber}}(x) = 0$ 根据 $x$ 的值分三种情况进行分析。\n\n情况1：$|x| \\le \\delta$。最优性条件为 $x - r + \\tau x = 0$，得出 $(1+\\tau)x = r$，即 $x = \\frac{r}{1+\\tau}$。如果 $|x| \\le \\delta$，即 $|\\frac{r}{1+\\tau}| \\le \\delta$ 或 $|r| \\le \\delta(1+\\tau)$，则该解是自洽的。\n\n情况2：$x > \\delta$。最优性条件为 $x - r + \\tau \\delta = 0$，得出 $x = r - \\tau\\delta$。如果 $x > \\delta$，即 $r - \\tau\\delta > \\delta$ 或 $r > \\delta(1+\\tau)$，则该解是一致的。\n\n情况3：$x < -\\delta$。最优性条件为 $x - r - \\tau \\delta = 0$，得出 $x = r + \\tau\\delta$。如果 $x < -\\delta$，即 $r + \\tau\\delta < -\\delta$ 或 $r < -\\delta(1+\\tau)$，则该解是一致的。\n\n结合这些互斥的情况，可以得到 Huber 罚的邻近算子的完整闭式解：\n$$\n\\operatorname{prox}_{\\tau \\rho_{\\mathrm{Huber}}}(r) = \\begin{cases}\nr + \\tau\\delta, & \\text{if } r < -\\delta(1+\\tau) \\\\\n\\frac{r}{1+\\tau}, & \\text{if } |r| \\le \\delta(1+\\tau) \\\\\nr - \\tau\\delta, & \\text{if } r > \\delta(1+\\tau)\n\\end{cases}\n$$\n这个表达式是精确的，将直接被实现。\n\n### 2. Welsch (Leclerc) 罚\n\nWelsch 罚，其尺度参数为 $c > 0$，定义如下：\n$$\n\\rho_{\\mathrm{Welsch}}(x) = \\frac{c^2}{2}\\left(1 - \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right)\n$$\n该函数是光滑但非凸的。其导数为：\n$$\n\\psi_{\\mathrm{Welsch}}(x) = \\rho'_{\\mathrm{Welsch}}(x) = x \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\n$$\n一阶最优性条件是 $x - r + \\tau \\psi_{\\mathrm{Welsch}}(x) = 0$，这导出了以下非线性方程：\n$$\nx \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) = r\n$$\n这个方程没有关于 $x$ 的初等闭式解。我们必须找到函数 $g(x) = x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) - r$ 的根。注意，当且仅当 $r=0$ 时，$x=0$ 是解。函数 $h(x) = x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right)$ 是奇函数，因此 $\\operatorname{prox}(-r) = -\\operatorname{prox}(r)$。我们可以求解 $r>0$ 的情况，然后扩展该解。对于提供的测试用例参数（$\\tau=0.5$），函数 $h(x)$ 是严格单调的，保证了对于任何 $r$ 都存在唯一解。\n\n我们采用牛顿-拉弗森方法来求解 $g(x)=0$。迭代更新公式为 $x_{k+1} = x_k - g(x_k)/g'(x_k)$。导数 $g'(x)$ 为：\n$$\ng'(x) = \\frac{d}{dx} \\left[ x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) - r \\right] = 1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right) \\left(1 - \\frac{2x^2}{c^2}\\right)\n$$\n一个好的初始猜测值是 $x_0 = r$，因为对于大的 $|x|$，指数项会消失，方程近似为 $x=r$。算法通过迭代进行，直到 $x$ 的变化量低于指定的容差为止。\n\n### 3. Geman–McClure 罚\n\nGeman–McClure 罚，其尺度参数为 $c > 0$，为：\n$$\n\\rho_{\\mathrm{GM}}(x) = \\frac{1}{2} \\frac{x^2}{1 + \\left(\\frac{x}{c}\\right)^2} = \\frac{c^2}{2} \\frac{x^2}{c^2 + x^2}\n$$\n该函数也是光滑且非凸的。其导数为：\n$$\n\\psi_{\\mathrm{GM}}(x) = \\rho'_{\\mathrm{GM}}(x) = \\frac{c^4 x}{(c^2+x^2)^2}\n$$\n最优性条件 $x - r + \\tau \\psi_{\\mathrm{GM}}(x) = 0$ 给出以下非线性方程：\n$$\nx \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) = r\n$$\n与 Welsch 罚一样，这需要数值求解。我们定义一个函数 $g(x) = x \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) - r$ 并求其根。其性质与 Welsch 的情况相似：当 $r=0$ 时，$x=0$ 是解，映射是奇函数，并且对于给定的测试参数（$\\tau=0.8$），存在唯一解。\n\n我们再次使用牛顿-拉弗森方法。导数 $g'(x)$ 为：\n$$\ng'(x) = \\frac{d}{dx} \\left[ x \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) - r \\right] = 1 + \\frac{\\tau c^4 (c^2 - 3x^2)}{(c^2+x^2)^3}\n$$\n初始猜测值再次取为 $x_0=r$。数值实现将使用此迭代方案找到 $g(x)$ 的根。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef prox_huber(r, tau, delta):\n    \"\"\"\n    Computes the proximal operator for the Huber penalty.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_huber(x) }\n    \n    This has a closed-form solution.\n    \"\"\"\n    threshold = delta * (1.0 + tau)\n    if r  -threshold:\n        return r + tau * delta\n    elif r > threshold:\n        return r - tau * delta\n    else:\n        return r / (1.0 + tau)\n\ndef prox_welsch(r, tau, c, tol=1e-12, max_iter=100):\n    \"\"\"\n    Computes the proximal operator for the Welsch penalty using Newton's method.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_welsch(x) }\n    \n    The optimality condition is g(x) = x * (1 + tau*exp(-(x/c)^2)) - r = 0.\n    \"\"\"\n    if r == 0.0:\n        return 0.0\n\n    # The function is odd, so we can solve for abs(r) and apply the sign.\n    r_abs = abs(r)\n    sign = np.sign(r)\n\n    # Initial guess x0 = r. Since we solve for r_abs, x0 = r_abs.\n    x = r_abs\n    c_sq = c * c\n\n    for _ in range(max_iter):\n        x_sq_over_c_sq = (x / c)**2\n        exp_term = np.exp(-x_sq_over_c_sq)\n        \n        # g(x) = x * (1 + tau * exp_term) - r_abs\n        g_x = x * (1.0 + tau * exp_term) - r_abs\n        \n        if abs(g_x)  tol:\n            return x * sign\n\n        # g'(x) = 1 + tau * exp_term * (1 - 2 * (x/c)^2)\n        gp_x = 1.0 + tau * exp_term * (1.0 - 2.0 * x_sq_over_c_sq)\n        \n        # Newton-Raphson update step\n        # Safeguard: gp_x is positive for the problem's parameters.\n        if gp_x == 0.0:\n            break\n        x = x - g_x / gp_x\n\n    return x * sign\n\ndef prox_geman_mcclure(r, tau, c, tol=1e-12, max_iter=100):\n    \"\"\"\n    Computes the proximal operator for the Geman-McClure penalty using Newton's method.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_gm(x) }\n    \n    The optimality condition is g(x) = x * (1 + tau*c^4 / (c^2+x^2)^2) - r = 0.\n    \"\"\"\n    if r == 0.0:\n        return 0.0\n\n    r_abs = abs(r)\n    sign = np.sign(r)\n    \n    # Initial guess\n    x = r_abs\n    c_sq = c * c\n    tau_c4 = tau * c * c * c * c\n\n    for _ in range(max_iter):\n        x_sq = x * x\n        denom = c_sq + x_sq\n        denom_sq = denom * denom\n        \n        # g(x) = x * (1 + tau*c^4 / (c^2+x^2)^2) - r_abs\n        g_x = x * (1.0 + tau_c4 / denom_sq) - r_abs\n        \n        if abs(g_x)  tol:\n            return x * sign\n        \n        # g'(x) = 1 + tau*c^4 * (c^2 - 3*x^2) / (c^2+x^2)^3\n        denom_cub = denom * denom_sq\n        gp_x = 1.0 + tau_c4 * (c_sq - 3.0 * x_sq) / denom_cub\n\n        # Safeguard: gp_x is positive for the problem's parameters.\n        if gp_x == 0.0:\n            break\n            \n        x = x - g_x / gp_x\n\n    return x * sign\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the test suite for each M-estimator.\n    \"\"\"\n    results = []\n    \n    # Huber penalty test cases\n    delta_huber = 1.25\n    tau_huber = 0.75\n    r_huber_vals = [-3.00, -delta_huber * (1.0 + tau_huber), 0.00, delta_huber * (1.0 + tau_huber), 2.70]\n    for r in r_huber_vals:\n        results.append(prox_huber(r, tau_huber, delta_huber))\n        \n    # Welsch penalty test cases\n    c_welsch = 1.00\n    tau_welsch = 0.50\n    r_welsch_vals = [-2.00, -1.00, 0.00, 1.00, 2.00]\n    for r in r_welsch_vals:\n        results.append(prox_welsch(r, tau_welsch, c_welsch))\n        \n    # Geman-McClure penalty test cases\n    c_gm = 1.50\n    tau_gm = 0.80\n    r_gm_vals = [-3.00, -1.50, 0.00, 1.50, 3.00]\n    for r in r_gm_vals:\n        results.append(prox_geman_mcclure(r, tau_gm, c_gm))\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3418049"}, {"introduction": "在解决了M-估计器的核心概念和关键计算模块（邻近算子）之后，我们现在将它们整合到一个完整的框架中，以解决一个带有正则化项的实际逆问题。本练习要求您使用交替方向乘子法（ADMM）来求解一个结合了稳健M-估计数据拟合项和二次正则化项的优化问题[@problem_id:3418091]。通过推导ADMM的迭代步骤，您将学会如何巧妙地运用变量分裂技术，将一个看似棘手的耦合问题分解为两个独立的、更易于处理的子问题，其中一个恰好可以通过前一个练习中学习的邻近算子来解决。", "problem": "考虑一个数据同化中的线性逆问题，其中状态向量 $x \\in \\mathbb{R}^{n}$ 是通过一个已知的线性观测算子 $A \\in \\mathbb{R}^{m \\times n}$ 从带噪声的观测值 $y \\in \\mathbb{R}^{m}$ 中推断出来的。为了增强对观测误差中离群值的稳健性，数据失配项由一类最大似然型估计量（M估计量）中的稳健损失函数建模，该函数记为逐元素应用的凸、真、下半连续函数 $\\rho:\\mathbb{R} \\to \\mathbb{R}$。通过强度为 $\\lambda  0$ 的线性正则化算子 $L \\in \\mathbb{R}^{p \\times n}$ 施加二次正则化。该稳健公式为\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\sum_{i=1}^{m} \\rho\\!\\left((A x - y)_{i}\\right) + \\lambda \\|L x\\|_{2}^{2}.\n$$\n引入一个分裂变量 $z \\in \\mathbb{R}^{m}$ 来表示残差，其约束为 $z = A x - y$。将交替方向乘子法（ADMM；Alternating Direction Method of Multipliers）应用于该约束重构问题。使用尺度对偶ADMM，其罚参数为 $\\mu  0$，与约束 $A x - y - z = 0$ 相关的尺度对偶变量为 $u \\in \\mathbb{R}^{m}$。\n\n从等式约束凸优化的增广拉格朗日量以及真、凸、下半连续函数的近端算子的核心定义出发，推导显式的ADMM迭代过程。你的推导必须：\n- 建立与约束 $A x - y - z = 0$ 对应的尺度增广拉格朗日量。\n- 通过关于 $x$ 最小化增广拉格朗日量来获得 $x$ 的更新，并将其表示为一个线性系统的闭式解。\n- 通过关于 $z$ 最小化来获得 $z$ 的更新，并使用 $\\rho$ 的近端算子来表示它。\n- 陈述尺度对偶变量的更新。\n\n用 $A$, $L$, $y$, $\\lambda$, $\\mu$ 和当前迭代值 $(x^{k}, z^{k}, u^{k})$ 表示这三个更新步骤。你的最终答案必须是一个单一的闭式解析表达式，将 $x$ 更新、 $z$ 更新和尺度对偶更新的右侧项紧凑地列为一个行向量。不需要数值近似，也不涉及单位。不要在最终的方框答案中包含任何方程或不等式；只按要求呈现更新的表达式。", "solution": "我们首先通过引入辅助变量 $z \\in \\mathbb{R}^{m}$ 来表示观测残差，从而对原始的稳健目标函数进行等式约束重构。问题变为\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{m}} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\lambda \\|L x\\|_{2}^{2} \\right] \\quad \\text{subject to} \\quad A x - y - z = 0.\n$$\n定义函数 $f(x) = \\lambda \\|L x\\|_{2}^{2}$ 和 $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$。交替方向乘子法（ADMM；Alternating Direction Method of Multipliers）的尺度对偶形式引入了与线性约束相关的尺度对偶变量 $u \\in \\mathbb{R}^{m}$，并使用一个增广拉格朗日罚参数 $\\mu  0$。根据经过充分检验的凸优化原理，构建尺度增广拉格朗日量：\n$$\n\\mathcal{L}_{\\mu}^{\\text{scaled}}(x, z, u) = f(x) + g(z) + \\frac{\\mu}{2} \\left\\|A x - y - z + u \\right\\|_{2}^{2} - \\frac{\\mu}{2} \\|u\\|_{2}^{2}.\n$$\nADMM迭代过程交替地对 $\\mathcal{L}_{\\mu}^{\\text{scaled}}$ 分别关于 $x$ 和 $z$ 进行最小化，然后进行尺度对偶变量的更新。\n\n对于 $x$ 的更新，我们固定 $(z^{k}, u^{k})$，并关于 $x$ 进行最小化：\n$$\nx^{k+1} \\in \\arg\\min_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} + \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right].\n$$\n两项都是关于 $x$ 的凸且可微函数。一阶最优性条件（通过将梯度设为零得到）得出\n$$\n\\nabla_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} \\right] + \\nabla_{x} \\left[ \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right] = 0,\n$$\n化简为\n$$\n2 \\lambda L^{\\top} L x + \\mu A^{\\top} \\left( A x - y - z^{k} + u^{k} \\right) = 0.\n$$\n合并关于 $x$ 的项，我们得到正规方程\n$$\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right) x = \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\n假设矩阵 $\\mu A^{\\top} A + 2 \\lambda L^{\\top} L$ 是可逆的，则 $x$ 的更新由以下闭式解给出\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\n\n对于 $z$ 的更新，我们固定 $(x^{k+1}, u^{k})$，并关于 $z$ 进行最小化：\n$$\nz^{k+1} \\in \\arg\\min_{z} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\frac{\\mu}{2} \\left\\|A x^{k+1} - y - z + u^{k} \\right\\|_{2}^{2} \\right].\n$$\n记 $v^{k} = A x^{k+1} - y + u^{k} \\in \\mathbb{R}^{m}$。由于 $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$ 是可分的，并且二次惩罚项也是可分的，因此该问题在各个坐标上解耦。根据真、凸、下半连续函数 $h:\\mathbb{R}^{m} \\to \\mathbb{R}$ 的近端算子的定义，\n$$\n\\operatorname{prox}_{\\tau h}(v) := \\arg\\min_{z} \\left\\{ h(z) + \\frac{1}{2 \\tau} \\|z - v\\|_{2}^{2} \\right\\},\n$$\n我们可以将 $z$ 的更新重写为一个步长为 $\\tau = \\frac{1}{\\mu}$ 的近端步骤：\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu} g}\\!\\left( v^{k} \\right) = \\operatorname{prox}_{\\frac{1}{\\mu} \\sum_{i=1}^{m} \\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right).\n$$\n因为 $g$ 是可分的，所以这个近端算子是逐元素作用的，并且可以等价地表示为标量近端算子 $\\operatorname{prox}_{\\frac{1}{\\mu}\\rho}$ 的坐标级应用：\n$$\nz^{k+1} = \\left[ \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( (A x^{k+1} - y + u^{k})_{i} \\right) \\right]_{i=1}^{m}.\n$$\n\n最后，$u$ 的尺度对偶变量更新直接遵循标准尺度ADMM对于等式约束 $A x - y - z = 0$ 的定义：\n$$\nu^{k+1} = u^{k} + \\left( A x^{k+1} - y - z^{k+1} \\right).\n$$\n\n汇总这三个更新步骤，我们得到了用当前迭代值 $(x^{k}, z^{k}, u^{k})$、算子 $A$、$L$、数据 $y$ 以及参数 $(\\lambda, \\mu)$ 紧凑表示的ADMM迭代过程：\n- $x$ 更新：\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right),\n$$\n- $z$ 更新：\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right),\n$$\n- 尺度对偶更新：\n$$\nu^{k+1} = u^{k} + A x^{k+1} - y - z^{k+1}.\n$$\n这些公式完成了在分裂 $z = A x - y$ 条件下，针对带有二次正则化的稳健M估计量数据失配问题的ADMM推导。", "answer": "$$\\boxed{\\begin{pmatrix}\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right)  \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right)  u^{k} + A x^{k+1} - y - z^{k+1}\n\\end{pmatrix}}$$", "id": "3418091"}]}