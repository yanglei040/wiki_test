{"hands_on_practices": [{"introduction": "理论知识只有通过实践才能真正内化。这个练习将引导我们从最基本的贝叶斯推断原理出发，为一个简单的线性动力学系统推导出模型分辨率矩阵。通过求解这个理想化的位置-速度追踪问题，我们将亲手揭示分辨率矩阵如何将真实状态与我们通过数据同化得到的期望估计联系起来，从而加深对其物理意义的理解。[@problem_id:3403416]", "problem": "考虑一个线性离散时间二维状态模型，其初始状态 $x_{0} = \\begin{pmatrix}p_{0} \\\\ v_{0}\\end{pmatrix} \\in \\mathbb{R}^{2}$ 未知，其中 $p_{0}$ 表示初始位置，$v_{0}$ 表示初始速度。在均匀时间步长 $\\Delta t > 0$ 上的确定性时间演化由线性算子 $M = \\begin{pmatrix}1  \\Delta t \\\\ 0  1\\end{pmatrix}$ 给出，因此在观测索引 $k \\in \\mathbb{N}$ 处有 $x_{k} = M^{k} x_{0}$。在时间索引 $k$ 处，通过线性观测算子 $H = \\begin{pmatrix}1  0\\end{pmatrix}$ 进行单个标量观测，该观测带有均值为零、方差为 $\\sigma_{y}^{2} > 0$ 的加性高斯噪声，即 $y_{k} = H x_{k} + \\eta_{k}$，其中 $\\eta_{k} \\sim \\mathcal{N}(0, \\sigma_{y}^{2})$。假设 $x_{0}$ 服从零均值高斯先验，其协方差为 $C_{0} = \\operatorname{diag}(\\sigma_{p}^{2}, \\sigma_{v}^{2})$，其中给定了 $\\sigma_{p}^{2} > 0$ 和 $\\sigma_{v}^{2} > 0$。\n\n仅使用上述基本设置（线性动力学、线性观测、高斯先验和噪声），推导模型分辨率矩阵，该矩阵定义为线性算子 $\\mathcal{R}(k)$，它在同化时间索引 $k$ 处的单个观测时，将真实初始状态 $x_{0}^{\\star}$ 映射到期望后验均值估计 $\\mathbb{E}[\\hat{x}_{0} \\mid x_{0}^{\\star}]$。将 $\\mathcal{R}(k)$ 表示为 $k$、$\\Delta t$、$\\sigma_{p}^{2}$、$\\sigma_{v}^{2}$ 和 $\\sigma_{y}^{2}$ 的闭式函数。您的最终答案必须是单一的精确解析矩阵表达式，并使用标准矩阵表示法书写。不需要进行数值舍入。", "solution": "该问题是适定的，有科学依据且自成体系。所有推导所述线性高斯系统的模型分辨率矩阵所需的信息均已提供。我们开始进行求解。\n\n问题要求推导模型分辨率矩阵 $\\mathcal{R}(k)$，它由关系式 $\\mathbb{E}[\\hat{x}_{0} \\mid x_{0}^{\\star}] = \\mathcal{R}(k) x_{0}^{\\star}$ 定义，其中 $x_{0}^{\\star}$ 是真实初始状态，$\\hat{x}_{0}$ 是基于数据的初始状态估计值。估计值 $\\hat{x}_{0}$ 取为后验均值，对于线性高斯系统，这等同于最大后验概率 (MAP) 估计。\n\n我们首先建立统计框架。待估计的状态是初始状态 $x_{0} \\in \\mathbb{R}^{2}$。\n$x_{0}$ 的先验分布是零均值高斯分布，协方差为 $C_{0}$：\n$$p(x_{0}) \\propto \\exp\\left(-\\frac{1}{2} x_{0}^{T} C_{0}^{-1} x_{0}\\right)$$\n其中 $C_{0} = \\operatorname{diag}(\\sigma_{p}^{2}, \\sigma_{v}^{2})$。\n\n在时间索引 $k$ 处进行单个标量观测 $y_k$。观测模型为 $y_{k} = H x_{k} + \\eta_{k}$，其中 $x_k = M^k x_0$ 且 $\\eta_k \\sim \\mathcal{N}(0, \\sigma_y^2)$。我们可以将观测直接写为初始状态 $x_0$ 的函数：\n$$y_k = H M^k x_0 + \\eta_k$$\n将初始状态映射到（无噪声）观测的前向算子是 $G_k = H M^k$。\n似然函数是给定状态 $x_0$ 观测到 $y_k$ 的概率，表示为：\n$$p(y_k \\mid x_0) \\propto \\exp\\left(-\\frac{1}{2\\sigma_y^2} (y_k - H M^k x_0)^2\\right)$$\n\n根据贝叶斯定理，$x_0$ 在给定 $y_k$ 条件下的后验概率密度与似然和先验的乘积成正比：$p(x_0 \\mid y_k) \\propto p(y_k \\mid x_0) p(x_0)$。MAP 估计 $\\hat{x}_0$ 是使该后验概率最大化的 $x_0$ 值，等价于最小化负对数后验，这定义了代价函数 $J(x_0)$：\n$$J(x_0) = \\frac{1}{2\\sigma_y^2}(y_k - H M^k x_0)^2 + \\frac{1}{2}x_0^T C_0^{-1} x_0$$\n注意，由于 $y_k$ 和 $H M^k x_0$ 都是标量，项 $(y_k - H M^k x_0)^T (\\sigma_y^2)^{-1} (y_k - H M^k x_0)$ 可简化为上述表达式。\n\n为了找到最小值，我们计算 $J(x_0)$ 关于 $x_0$ 的梯度，并令其为零。我们将行向量 $H M^k$ 记为 $G_k$。那么 $H M^k x_0 = G_k x_0$。\n$$\\nabla_{x_0} J(x_0) = \\frac{1}{2\\sigma_y^2} \\nabla_{x_0} \\left( y_k^2 - 2y_k G_k x_0 + (G_k x_0)^2 \\right) + \\frac{1}{2} \\nabla_{x_0} (x_0^T C_0^{-1} x_0)$$\n$$\\nabla_{x_0} J(x_0) = \\frac{1}{2\\sigma_y^2} \\left( -2y_k G_k^T + 2G_k^T G_k x_0 \\right) + C_0^{-1} x_0$$\n将梯度设为零以求得估计值 $\\hat{x}_0$：\n$$\\frac{1}{\\sigma_y^2}(-y_k G_k^T + G_k^T G_k \\hat{x}_0) + C_0^{-1} \\hat{x}_0 = 0$$\n$$\\left( \\frac{1}{\\sigma_y^2} G_k^T G_k + C_0^{-1} \\right) \\hat{x}_0 = \\frac{1}{\\sigma_y^2} G_k^T y_k$$\n求解 $\\hat{x}_0$：\n$$\\hat{x}_0 = \\left( \\frac{1}{\\sigma_y^2} (H M^k)^T (H M^k) + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (H M^k)^T y_k$$\n此方程给出了作为观测值 $y_k$ 函数的后验均值估计 $\\hat{x}_0$。\n\n现在，我们使用模型分辨率矩阵的定义。我们令 $y_k$ 由真实状态 $x_0^\\star$ 生成，所以 $y_k = H M^k x_0^\\star + \\eta_k$。然后，我们对 $\\hat{x}_0$ 关于 $\\eta_k$ 的噪声分布求期望，条件是真实状态 $x_0^\\star$。由于 $\\mathbb{E}[\\eta_k] = 0$，我们有 $\\mathbb{E}[y_k \\mid x_0^\\star] = H M^k x_0^\\star$。\n$$\\mathbb{E}[\\hat{x}_0 \\mid x_0^\\star] = \\mathbb{E}\\left[ \\left( \\frac{1}{\\sigma_y^2} (H M^k)^T H M^k + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (H M^k)^T y_k \\mid x_0^\\star \\right]$$\n矩阵项是确定性的，所以我们可以将期望算子移到内部：\n$$\\mathbb{E}[\\hat{x}_0 \\mid x_0^\\star] = \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (M^k)^T H^T \\mathbb{E}[y_k \\mid x_0^\\star]$$\n$$\\mathbb{E}[\\hat{x}_0 \\mid x_0^\\star] = \\left[ \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1} \\right)^{-1} \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k \\right] x_0^\\star$$\n根据定义，方括号中的表达式即为模型分辨率矩阵 $\\mathcal{R}(k)$：\n$$\\mathcal{R}(k) = \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1} \\right)^{-1} \\left(\\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k \\right)$$\n\n为了获得闭式表达式，我们必须计算每个组成部分。\n首先，我们求 $M^k$。给定 $M = \\begin{pmatrix} 1  \\Delta t \\\\ 0  1 \\end{pmatrix}$，我们可以找到其幂次方的规律：\n$M^2 = \\begin{pmatrix} 1  2\\Delta t \\\\ 0  1 \\end{pmatrix}$，$M^3 = \\begin{pmatrix} 1  3\\Delta t \\\\ 0  1 \\end{pmatrix}$。通过归纳法，我们确定：\n$$M^k = \\begin{pmatrix} 1  k \\Delta t \\\\ 0  1 \\end{pmatrix}$$\n项 $(M^k)^T H^T H M^k$ 计算如下：\n$H = \\begin{pmatrix} 1  0 \\end{pmatrix}$，所以 $H^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n$(M^k)^T H^T = \\begin{pmatrix} 1  0 \\\\ k\\Delta t  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ k\\Delta t \\end{pmatrix}$。\n然后，$(M^k)^T H^T H M^k = \\begin{pmatrix} 1 \\\\ k\\Delta t \\end{pmatrix} \\begin{pmatrix} 1  k\\Delta t \\end{pmatrix} = \\begin{pmatrix} 1  k\\Delta t \\\\ k\\Delta t  (k\\Delta t)^2 \\end{pmatrix}$。\n\n先验协方差的逆是 $C_0^{-1} = \\operatorname{diag}(\\sigma_p^{-2}, \\sigma_v^{-2}) = \\begin{pmatrix} 1/\\sigma_p^2  0 \\\\ 0  1/\\sigma_v^2 \\end{pmatrix}$。\n\n令 $A_k = \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k + C_0^{-1}$。这是需要求逆的矩阵。\n$$A_k = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1  k\\Delta t \\\\ k\\Delta t  k^2(\\Delta t)^2 \\end{pmatrix} + \\begin{pmatrix} 1/\\sigma_p^2  0 \\\\ 0  1/\\sigma_v^2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2}  \\frac{k\\Delta t}{\\sigma_y^2} \\\\ \\frac{k\\Delta t}{\\sigma_y^2}  \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2} \\end{pmatrix}$$\n$A_k$ 的行列式是：\n$$\\det(A_k) = \\left(\\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2}\\right) \\left(\\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2}\\right) - \\left(\\frac{k\\Delta t}{\\sigma_y^2}\\right)^2$$\n$$\\det(A_k) = \\frac{k^2(\\Delta t)^2}{\\sigma_y^4} + \\frac{1}{\\sigma_y^2\\sigma_v^2} + \\frac{k^2(\\Delta t)^2}{\\sigma_y^2\\sigma_p^2} + \\frac{1}{\\sigma_p^2\\sigma_v^2} - \\frac{k^2(\\Delta t)^2}{\\sigma_y^4}$$\n$$\\det(A_k) = \\frac{1}{\\sigma_y^2\\sigma_v^2} + \\frac{k^2(\\Delta t)^2}{\\sigma_y^2\\sigma_p^2} + \\frac{1}{\\sigma_p^2\\sigma_v^2} = \\frac{\\sigma_p^2 + k^2(\\Delta t)^2\\sigma_v^2 + \\sigma_y^2}{\\sigma_y^2\\sigma_p^2\\sigma_v^2}$$\n为简洁起见，令 $D_k = \\sigma_p^2 + k^2(\\Delta t)^2\\sigma_v^2 + \\sigma_y^2$。那么 $\\det(A_k) = \\frac{D_k}{\\sigma_y^2\\sigma_p^2\\sigma_v^2}$。\n\n$A_k$ 的逆是：\n$$A_k^{-1} = \\frac{1}{\\det(A_k)} \\begin{pmatrix} \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2}  -\\frac{k\\Delta t}{\\sigma_y^2} \\\\ -\\frac{k\\Delta t}{\\sigma_y^2}  \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} \\end{pmatrix}$$\n现在，我们计算 $\\mathcal{R}(k) = A_k^{-1} \\left( \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k \\right)$。\n令 $B_k = \\frac{1}{\\sigma_y^2} (M^k)^T H^T H M^k = \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1  k\\Delta t \\\\ k\\Delta t  k^2(\\Delta t)^2 \\end{pmatrix}$。\n$$\\mathcal{R}(k) = \\frac{\\sigma_y^2\\sigma_p^2\\sigma_v^2}{D_k} \\begin{pmatrix} \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2}  -\\frac{k\\Delta t}{\\sigma_y^2} \\\\ -\\frac{k\\Delta t}{\\sigma_y^2}  \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} \\end{pmatrix} \\frac{1}{\\sigma_y^2} \\begin{pmatrix} 1  k\\Delta t \\\\ k\\Delta t  k^2(\\Delta t)^2 \\end{pmatrix}$$\n让我们计算这两个矩阵的乘积：\n$$ \\begin{pmatrix} \\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2}  -\\frac{k\\Delta t}{\\sigma_y^2} \\\\ -\\frac{k\\Delta t}{\\sigma_y^2}  \\frac{1}{\\sigma_y^2} + \\frac{1}{\\sigma_p^2} \\end{pmatrix} \\begin{pmatrix} 1  k\\Delta t \\\\ k\\Delta t  k^2(\\Delta t)^2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sigma_v^2}  \\frac{k\\Delta t}{\\sigma_v^2} \\\\ \\frac{k\\Delta t}{\\sigma_p^2}  \\frac{k^2(\\Delta t)^2}{\\sigma_p^2} \\end{pmatrix} $$\n(1,1) 元素是 $(\\frac{k^2(\\Delta t)^2}{\\sigma_y^2} + \\frac{1}{\\sigma_v^2})(1) - (\\frac{k\\Delta t}{\\sigma_y^2})(k\\Delta t) = \\frac{1}{\\sigma_v^2}$。\n其他元素的计算方式类似。\n将此结果代回到 $\\mathcal{R}(k)$ 的表达式中：\n$$\\mathcal{R}(k) = \\frac{\\sigma_y^2\\sigma_p^2\\sigma_v^2}{D_k} \\frac{1}{\\sigma_y^2} \\begin{pmatrix} \\frac{1}{\\sigma_v^2}  \\frac{k\\Delta t}{\\sigma_v^2} \\\\ \\frac{k\\Delta t}{\\sigma_p^2}  \\frac{k^2(\\Delta t)^2}{\\sigma_p^2} \\end{pmatrix}$$\n$$\\mathcal{R}(k) = \\frac{\\sigma_p^2\\sigma_v^2}{D_k} \\begin{pmatrix} \\frac{1}{\\sigma_v^2}  \\frac{k\\Delta t}{\\sigma_v^2} \\\\ \\frac{k\\Delta t}{\\sigma_p^2}  \\frac{k^2(\\Delta t)^2}{\\sigma_p^2} \\end{pmatrix}$$\n最后，将标量乘入矩阵，得到模型分辨率矩阵的闭式表达式：\n$$\\mathcal{R}(k) = \\frac{1}{D_k} \\begin{pmatrix} \\sigma_p^2  k\\Delta t \\sigma_p^2 \\\\ k\\Delta t \\sigma_v^2  k^2(\\Delta t)^2 \\sigma_v^2 \\end{pmatrix}$$\n代入 $D_k$ 的表达式：\n$$\\mathcal{R}(k) = \\frac{1}{\\sigma_p^2 + k^2(\\Delta t)^2\\sigma_v^2 + \\sigma_y^2} \\begin{pmatrix} \\sigma_p^2  k\\Delta t \\sigma_p^2 \\\\ k\\Delta t \\sigma_v^2  k^2(\\Delta t)^2 \\sigma_v^2 \\end{pmatrix}$$\n这就是模型分辨率矩阵作为给定参数函数的最终解析表达式。", "answer": "$$\n\\boxed{\n\\frac{1}{\\sigma_{p}^{2} + k^{2} (\\Delta t)^{2} \\sigma_{v}^{2} + \\sigma_{y}^{2}}\n\\begin{pmatrix}\n\\sigma_{p}^{2}  & k \\Delta t \\sigma_{p}^{2} \\\\\nk \\Delta t \\sigma_{v}^{2}  & k^{2} ( \\Delta t )^{2} \\sigma_{v}^{2}\n\\end{pmatrix}\n}\n$$", "id": "3403416"}, {"introduction": "模型分辨率矩阵最直观的应用之一是评估反演过程引入的“模糊”效应。本练习引入了点扩展函数（Point-Spread Function, PSF）的概念，即分辨率矩阵的一行或一列，它形象地展示了单个点状真实信号在反演结果中是如何被“涂抹”开的。通过一个计算任务，我们将量化这种模糊的程度（即PSF的宽度），并探索正则化参数 $\\lambda$ 如何直接影响分辨率与解的平滑性之间的权衡。[@problem_id:3403453]", "problem": "考虑一个一维离散线性反演问题，该问题定义在一个包含 $N$ 个未知数 $\\{m_k\\}_{k=0}^{N-1}$ 的均匀网格上，网格间距为单位长度。设正演算子为单位算子，因此在没有噪声的情况下，数据等于模型，并假设数据协方差为单位矩阵。一个带有一次差分稳定器的标准 Tikhonov 正则化估计器通过最小化一个二次目标函数来求解 $\\hat{m}$，该目标函数惩罚由一次差分算子测量的粗糙度。模型分辨率矩阵编码了在无噪声数据的期望下，真实模型如何映射到估计模型。索引 $i$ 处的点扩散函数是模型分辨率矩阵的第 $i$ 行；它量化了索引 $i$ 处的一个单位脉冲被估计器模糊的程度。您的任务是推导、计算和量化该点扩散函数的宽度，将其作为正则化强度的函数。\n\n从 Tikhonov 正则化和模型分辨率矩阵的基本定义出发，执行以下操作：\n\n- 推导在正演算子为单位算子、数据协方差为单位矩阵的特殊情况下的模型分辨率矩阵。正则化算子 $L$ 是一次差分算子，其元素为 $(L)_{r,r}=-1$ 和 $(L)_{r,r+1}=1$（对于 $r \\in \\{0,\\dots,N-2\\}$），所有其他元素为零。不要先验地假设分辨率矩阵的任何表达式；从定义 Tikhonov 估计器的变分公式以及在无噪声情况下从真实模型到期望估计的映射关系的分辨率矩阵定义开始。\n\n- 解释如何在不显式构造完整逆矩阵的情况下，仅使用线性代数恒等式和系统的对称性质，计算第 $i$ 个点扩散函数（定义为模型分辨率矩阵的第 $i$ 行）。\n\n- 使用二阶中心矩为以索引 $i$ 为中心的离散点扩散函数 $p=\\{p_k\\}_{k=0}^{N-1}$ 定义一个定量的宽度度量。具体而言，设 $\\alpha=\\sum_{k=0}^{N-1} p_k$，定义方差 $\\sigma^2=\\frac{1}{\\alpha}\\sum_{k=0}^{N-1} (k-i)^2 p_k$，宽度为 $w=2\\sqrt{\\sigma^2}$。所有距离都以网格索引单位表示，此处为无量纲单位。\n\n然后，实现一个程序，对下面测试套件中的每一组参数，构造 $L$，计算给定索引 $i$ 和正则化参数 $\\lambda$ 对应的点扩散函数，并输出其如上定义的宽度 $w(\\lambda)$。使用网格索引为从 $0$ 到 $N-1$ 的整数的约定。\n\n测试套件（每个元组为 $(N,i,\\lambda)$）：\n\n- 情况 A（内部索引，平滑度增加）：\n  - $(N,i,\\lambda)=(51,25,0)$\n  - $(N,i,\\lambda)=(51,25,0.2)$\n  - $(N,i,\\lambda)=(51,25,1.0)$\n  - $(N,i,\\lambda)=(51,25,5.0)$\n- 情况 B（左边界索引）：\n  - $(N,i,\\lambda)=(51,0,0)$\n  - $(N,i,\\lambda)=(51,0,1.0)$\n  - $(N,i,\\lambda)=(51,0,5.0)$\n- 情况 C（不同网格尺寸，内部索引）：\n  - $(N,i,\\lambda)=(21,10,0)$\n  - $(N,i,\\lambda)=(21,10,0.5)$\n  - $(N,i,\\lambda)=(21,10,2.0)$\n\n您的程序应生成单行输出，其中包含按上述顺序列出的 10 个计算宽度，四舍五入到 6 位小数，格式为用方括号括起来的逗号分隔列表（例如，$[0.000000,0.123456,\\dots]$）。不应打印任何其他文本。所有计算都是无量纲的；不需要物理单位。不涉及角度。", "solution": "该问题被评估为有效，因为它科学地基于线性反演理论的既定原则，具有充分且一致的信息，问题设定适定，并且表述客观。我们可以开始求解。\n\n任务是为 Tikhonov 正则化反演问题推导和计算模型点扩散函数（PSF）的宽度。求解过程包括三个主要阶段：首先，从基本原理推导模型分辨率矩阵；其次，描述一种计算单个 PSF（分辨率矩阵的一行）的高效方法；第三，量化计算出的 PSF 的宽度。\n\n**1. 模型分辨率矩阵 ($R$) 的推导**\n\n模型向量 $m$ 的 Tikhonov 正则化估计 $\\hat{m}$ 是使二次目标函数 $J(m)$ 最小化的向量：\n$$\nJ(m) = \\| Gm - d \\|^2_{C_d^{-1}} + \\lambda^2 \\| Lm \\|^2\n$$\n这里，$G$ 是正演算子，$d$ 是数据向量，$C_d$ 是数据协方差矩阵，$L$ 是正则化算子（或稳定器），$\\lambda$ 是在数据保真度与模型简单性（例如，平滑度）之间取得平衡的正则化参数。加权范数的平方定义为 $\\|v\\|^2_W = v^T W v$。\n\n问题指明正演算子是单位算子，$G=I$，数据协方差也是单位矩阵，$C_d=I$。将这些代入目标函数可得：\n$$\nJ(m) = \\| Im - d \\|^2_{I^{-1}} + \\lambda^2 \\| Lm \\|^2 = (m - d)^T (m - d) + \\lambda^2 (Lm)^T (Lm)\n$$\n展开此表达式，我们得到：\n$$\nJ(m) = m^T m - 2m^T d + d^T d + \\lambda^2 m^T L^T L m\n$$\n为了找到最小化 $J(m)$ 的模型估计 $\\hat{m}$，我们计算 $J(m)$ 关于 $m$ 的梯度并将其设为零。梯度为：\n$$\n\\nabla_m J(m) = 2m - 2d + 2\\lambda^2 L^T L m\n$$\n令 $\\nabla_m J(m) = 0$ 可得：\n$$\n2\\hat{m} - 2d + 2\\lambda^2 L^T L \\hat{m} = 0\n$$\n$$\n(I + \\lambda^2 L^T L) \\hat{m} = d\n$$\n求解正则化估计 $\\hat{m}$，我们得到：\n$$\n\\hat{m} = (I + \\lambda^2 L^T L)^{-1} d\n$$\n模型分辨率矩阵 $R$ 由从真实模型 $m_{\\text{true}}$ 到估计模型期望值 $E[\\hat{m}]$ 的线性映射定义。在无噪声数据的条件下，观测到的数据向量 $d$ 就是 $d = G m_{\\text{true}}$。给定 $G=I$，这变为 $d = m_{\\text{true}}$。将此代入 $\\hat{m}$ 的表达式，得到无噪声情况下的关系：\n$$\n\\hat{m} = (I + \\lambda^2 L^T L)^{-1} m_{\\text{true}}\n$$\n通过将其与定义 $\\hat{m} = R m_{\\text{true}}$ 进行比较，我们确定模型分辨率矩阵为：\n$$\nR = (I + \\lambda^2 L^T L)^{-1}\n$$\n这就是从变分公式推导出的模型分辨率矩阵所需表达式。矩阵 $L$ 是 $(N-1) \\times N$ 的一次差分算子，而 $I$ 是 $N \\times N$ 的单位矩阵。\n\n**2. 点扩散函数的计算**\n\n索引 $i$ 处的点扩散函数（PSF），记为 $p^{(i)}$，定义为模型分辨率矩阵 $R$ 的第 $i$ 行。直接计算需要构造矩阵 $A = I + \\lambda^2 L^T L$，然后计算其完整的逆矩阵 $R = A^{-1}$，这是一个复杂度为 $O(N^3)$ 的操作，对于大的 $N$ 来说计算效率低下。\n\n一种更有效的方法利用了矩阵 $A$ 的性质。矩阵 $L^T L$ 是对称的，因为 $(L^T L)^T = L^T (L^T)^T = L^T L$。因此，矩阵 $A = I + \\lambda^2 L^T L$ 也是对称的 ($A^T=A$)。对称矩阵的逆矩阵也是对称的，所以 $R$ 是对称的 ($R^T=R$)。\n\n$R$ 的第 $i$ 行可以写成 $e_i^T R$，其中 $e_i$ 是一个在索引 $i$ 处为 1、其他地方为 0 的列向量（第 $i$ 个标准基向量）。由于 $R$ 的对称性，第 $i$ 行是第 $i$ 列的转置：\n$$\n(i\\text{-th row of } R) = e_i^T R = (R^T e_i)^T = (R e_i)^T\n$$\n向量 $R e_i$ 是 $R$ 的第 $i$ 列。设这个列向量为 $x$。那么 $x = R e_i = A^{-1} e_i$。为了求 $x$，我们可以解等价的线性方程组：\n$$\nAx = e_i\n$$\n解向量 $x$ 包含了第 $i$ 个 PSF $p^{(i)}$ 的元素。这种求解单个线性方程组的方法，其计算成本远低于对矩阵 $A$ 求逆。由于 $A$ 是对称正定（且是带状）矩阵，可以采用专门的高效线性求解器，其复杂度通常接近 $O(N)$。\n\n因此，计算步骤如下：\n1. 构造 $(N-1) \\times N$ 的一次差分矩阵 $L$。\n2. 构造 $N \\times N$ 的矩阵 $A = I + \\lambda^2 L^T L$。\n3. 定义 $N \\times 1$ 的基向量 $e_i$。\n4. 求解线性方程组 $Ax = e_i$ 以获得 PSF 向量 $x$。\n\n**3. PSF 宽度的量化**\n\n问题为以索引 $i$ 为中心的离散 PSF $p = \\{p_k\\}_{k=0}^{N-1}$ 的宽度提供了一个具体的度量标准。该宽度基于 $p$ 中数值分布的二阶中心矩（方差）来定义。\n\n首先，我们计算 PSF 各分量的和：\n$$\n\\alpha = \\sum_{k=0}^{N-1} p_k\n$$\n这个和作为一个归一化因子。对于 $\\lambda=0$，$R=I$，所以 $p^{(i)} = e_i^T$ 且 $\\alpha=1$。对于 $\\lambda > 0$，PSF 变宽，$\\alpha$ 可能会偏离 1。\n\n接下来，方差 $\\sigma^2$ 被计算为与中心 $i$ 的距离平方的加权平均值：\n$$\n\\sigma^2 = \\frac{1}{\\alpha} \\sum_{k=0}^{N-1} (k-i)^2 p_k\n$$\n最后，宽度 $w$ 定义为标准差的两倍：\n$$\nw = 2\\sqrt{\\sigma^2}\n$$\n距离 $(k-i)$ 以无量纲的网格索引为单位，因此得到的宽度 $w$ 也使用这些单位。在 $\\lambda=0$ 的特殊情况下，PSF 是一个完美的脉冲（$p_k = \\delta_{ik}$），导致 $\\sigma^2=0$，因此 $w=0$，表示完美的分辨率。随着 $\\lambda$ 的增加，正则化效应增强，PSF 变宽，宽度 $w$ 预计会增加。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the widths of point-spread functions for a series of test cases\n    in a Tikhonov-regularized inverse problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (N, i, lambda), where:\n    # N is the number of grid points (size of the model vector).\n    # i is the index for which the point-spread function is computed.\n    # lambda is the regularization parameter.\n    test_cases = [\n        # Case A (interior index, increasing smoothing)\n        (51, 25, 0.0),\n        (51, 25, 0.2),\n        (51, 25, 1.0),\n        (51, 25, 5.0),\n        # Case B (left boundary index)\n        (51, 0, 0.0),\n        (51, 0, 1.0),\n        (51, 0, 5.0),\n        # Case C (different grid size, interior index)\n        (21, 10, 0.0),\n        (21, 10, 0.5),\n        (21, 10, 2.0),\n    ]\n\n    results = []\n    for N, i, lambda_reg in test_cases:\n        # For lambda = 0, the resolution matrix is the identity.\n        # The PSF is a Kronecker delta, so its width is exactly 0.\n        if lambda_reg == 0.0:\n            results.append(0.0)\n            continue\n            \n        # STEP 1: Construct the (N-1)xN first-difference operator L.\n        L = np.zeros((N - 1, N))\n        rows = np.arange(N - 1)\n        L[rows, rows] = -1.0\n        L[rows, rows + 1] = 1.0\n        \n        # STEP 2: Form the matrix A = I + lambda^2 * L^T * L.\n        # This matrix is symmetric and positive definite.\n        LTL = L.T @ L\n        A = np.identity(N) + (lambda_reg**2) * LTL\n        \n        # STEP 3: Define the standard basis vector e_i.\n        e_i = np.zeros(N)\n        e_i[i] = 1.0\n        \n        # STEP 4: Solve the linear system A*p = e_i to find the PSF p.\n        # np.linalg.solve is efficient for such systems.\n        p = np.linalg.solve(A, e_i)\n        \n        # STEP 5: Calculate the width of the PSF p.\n        # The width is defined as 2 * sqrt(variance).\n        \n        # Sum of the PSF components (normalization factor).\n        alpha = np.sum(p)\n\n        # Vector of grid indices [0, 1, ..., N-1].\n        k = np.arange(N)\n        \n        # Variance of the PSF around its center i.\n        # The sum is a weighted average of squared distances from the center.\n        variance = np.sum(((k - i)**2) * p) / alpha\n        \n        # The variance can be slightly negative due to floating-point inaccuracies\n        # if the true value is near zero. np.abs() provides robustness.\n        width = 2.0 * np.sqrt(np.abs(variance))\n        \n        results.append(width)\n\n    # Print the final results in the specified format:\n    # A single line, comma-separated list of widths, rounded to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3403453"}, {"introduction": "一个好的反演不仅要能精确地重构模型（由分辨率矩阵的对角元素反映），还应能清晰地区分不同模型参数。本练习将我们的注意力从对角线转移到非对角元素 $R_{ij}$，这些元素量化了参数间的“串扰”（cross-talk）。通过计算一系列不同设置下的串扰系数，我们将学会如何评估一个真实模型参数的变化会在多大程度上“泄漏”并污染另一个参数的估计值，这对于理解复杂反演问题的内在不确定性至关重要。[@problem_id:3403399]", "problem": "考虑一个线性反演问题，其正演算子为 $G \\in \\mathbb{R}^{m \\times n}$，模型向量为 $m \\in \\mathbb{R}^{n}$，数据向量为 $d \\in \\mathbb{R}^{m}$，满足 $d = G m + \\varepsilon$。其中 $\\varepsilon$ 是均值为零、协方差矩阵为对称正定矩阵 $C_d \\in \\mathbb{R}^{m \\times m}$ 的加性噪声。令估计量 $\\hat{m}$ 定义为严格凸二次目标函数的极小化子：\n$$\nJ(m) = \\| C_d^{-1/2} (G m - d)\\|_2^2 + \\| L m \\|_2^2,\n$$\n其中 $L \\in \\mathbb{R}^{r \\times n}$ 是一个固定的正则化算子（任何所需的缩放已并入 $L$ 中）。矩阵 $L$ 可以是矩形的，并假设乘积 $L^\\top L$ 是良定义的。假设 $G$、$C_d$ 和 $L$ 能确保极小化子是唯一的。\n\n从这个定义和标准的最小二乘最优性条件出发，定义模型分辨率矩阵 $R \\in \\mathbb{R}^{n \\times n}$，通过矩阵元素 $R_{ij}$ 解释参数 $i$ 和 $j$ 之间的串扰系数，并计算在理想化的无噪声设置 $d = G m_{\\text{true}}$ 下，第 $i$ 个估计模型分量 $\\hat{m}_i$ 对施加于第 $j$ 个真实模型分量 $m_j$ 的单位扰动的响应。对整数 $i$ 和 $j$ 使用从零开始的索引约定。\n\n您的程序必须：\n- 仅从所述目标函数的最优性条件推导 $R$ 的表达式，除了给定假设外，不假设 $G$、$C_d$ 或 $L$ 具有任何特定结构。\n- 实现一个函数，给定 $G$、$C_d$、$L$ 和索引 $(i,j)$，返回标量串扰系数 $R_{ij}$，该系数等于当 $m_{\\text{true}}$ 的所有其他分量为零时，$\\hat{m}_i$ 对 $m_j$ 处单位扰动的响应。\n- 使用以下测试套件，其中每个情况都明确指定 $(G, C_d, L, i, j)$ 为实值数值数组和整数。所有数组都是无量纲的，不涉及物理单位。\n\n测试套件（从零开始的索引）：\n1. 情况 A（单位分辨率，对角线上）：\n   - $G = I_3$\n   - $C_d = I_3$\n   - $L = 0_{3 \\times 3}$（$3 \\times 3$ 的零矩阵）\n   - $i = 1$, $j = 1$\n2. 情况 B（单位分辨率，非对角线上）：\n   - $G = I_3$\n   - $C_d = I_3$\n   - $L = 0_{3 \\times 3}$\n   - $i = 0$, $j = 2$\n3. 情况 C（带有一阶差分正则化的模糊正演算子）：\n   - $G = \\begin{bmatrix}\n     1  0.5  0  0 \\\\\n     0.5  1  0.5  0 \\\\\n     0  0.5  1  0.5 \\\\\n     0  0  0.5  1\n     \\end{bmatrix}$\n   - $C_d = I_4$\n   - $L = \\begin{bmatrix}\n     -1  1  0  0 \\\\\n     0  -1  1  0 \\\\\n     0  0  -1  1\n     \\end{bmatrix}$\n   - $i = 1$, $j = 2$\n4. 情况 D（带有各向异性数据协方差和一阶差分正则化的单位正演算子）：\n   - $G = I_4$\n   - $C_d = \\mathrm{diag}(0.04, 1.0, 0.25, 9.0)$\n   - $L = \\begin{bmatrix}\n     -1  1  0  0 \\\\\n     0  -1  1  0 \\\\\n     0  0  -1  1\n     \\end{bmatrix}$\n   - $i = 3$, $j = 0$\n5. 情况 E（由单位正则化稳定的秩亏正演算子）：\n   - $G = \\begin{bmatrix}\n     1  1 \\\\\n     2  2\n     \\end{bmatrix}$\n   - $C_d = I_2$\n   - $L = I_2$\n   - $i = 0$, $j = 1$\n6. 情况 F（强单位正则化抑制分辨率）：\n   - $G = I_2$\n   - $C_d = I_2$\n   - $L = 100\\, I_2$\n   - $i = 1$, $j = 1$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序对应于情况 A–F 的六个标量结果，格式为逗号分隔的列表，并用方括号括起来，例如 `[rA,rB,rC,rD,rE,rF]`。每个条目必须是实数。", "solution": "我们从所述的二次目标函数开始\n$$\nJ(m) = \\| C_d^{-1/2} (G m - d) \\|_2^2 + \\| L m \\|_2^2,\n$$\n其中 $G \\in \\mathbb{R}^{m \\times n}$，$C_d \\in \\mathbb{R}^{m \\times m}$ 是对称正定矩阵，$L \\in \\mathbb{R}^{r \\times n}$ 是正则化算子。极小化子 $\\hat{m}$ 满足通过将 $J(m)$ 对 $m$ 的梯度设为零得到的一阶最优性条件。使用二次型的标准性质，\n$$\n\\nabla_m J(m) = 2 G^\\top C_d^{-1} (G m - d) + 2 L^\\top (L m).\n$$\n令 $\\nabla_m J(\\hat{m}) = 0$ 得到正规方程\n$$\n\\left(G^\\top C_d^{-1} G + L^\\top L \\right) \\hat{m} = G^\\top C_d^{-1} d.\n$$\n定义对称正定矩阵\n$$\nA \\equiv G^\\top C_d^{-1} G + L^\\top L \\in \\mathbb{R}^{n \\times n},\n$$\n在给定假设下，该矩阵是可逆的，因为即使 $G^\\top C_d^{-1} G$ 是秩亏的，$L^\\top L$ 也能提供稳定性；如果 $G^\\top C_d^{-1} G$ 本身就是正定的，则不需要 $L^\\top L$。于是\n$$\n\\hat{m} = A^{-1} G^\\top C_d^{-1} d.\n$$\n为了分析模型分辨率，我们考虑理想化的无噪声设置，其中对于某个固定的真实模型 $m_{\\text{true}}$，有 $d = G m_{\\text{true}}$。代入 $d$，\n$$\n\\hat{m} = A^{-1} G^\\top C_d^{-1} G \\, m_{\\text{true}}.\n$$\n这表明估计量是 $m_{\\text{true}}$ 的一个线性映射。因此，模型分辨率矩阵定义为\n$$\nR \\equiv A^{-1} G^\\top C_d^{-1} G = \\left(G^\\top C_d^{-1} G + L^\\top L\\right)^{-1} G^\\top C_d^{-1} G \\in \\mathbb{R}^{n \\times n}.\n$$\n根据构造，\n$$\n\\hat{m} = R \\, m_{\\text{true}}.\n$$\n为了解释串扰，考虑在真实模型的第 $j$ 个分量上施加一个单位扰动，同时保持所有其他分量为零：\n$$\nm_{\\text{true}} = e_j,\n$$\n其中 $e_j$ 是 $\\mathbb{R}^{n}$ 中的第 $j$ 个标准基向量。则\n$$\n\\hat{m} = R e_j,\n$$\n估计量的第 $i$ 个分量变为\n$$\n\\hat{m}_i = e_i^\\top \\hat{m} = e_i^\\top R e_j = R_{ij}.\n$$\n因此，从参数 $j$ 到估计参数 $i$ 的串扰系数恰好是矩阵元素 $R_{ij}$。特别地，$R_{ij}$ 量化了 $\\hat{m}_i$ 对 $m_j$ 处单位扰动的响应。\n\n算法计算过程如下：\n1. 给定 $G$、$C_d$ 和 $L$，通过对对称正定矩阵 $C_d$ 求逆来形成 $C_d^{-1}$。\n2. 形成 $A = G^\\top C_d^{-1} G + L^\\top L$ 和 $B = G^\\top C_d^{-1} G$。\n3. 使用稳定的线性求解器求解线性矩阵方程\n$$\nA R = B\n$$\n以得到 $R$；这等效于 $R = A^{-1} B$ 而无需显式地计算 $A$ 的逆矩阵。\n4. 在所请求的从零开始的索引 $(i, j)$ 处提取标量 $R_{ij}$。\n\n测试套件讨论：\n- 在情况 A 中，$G = I_3$，$C_d = I_3$，$L = 0_{3 \\times 3}$，我们有 $A = I_3$ 和 $B = I_3$，因此 $R = I_3$。对角线上的响应 $R_{11}$ 等于 $1$。\n- 在情况 B 中，使用相同的算子，非对角线元素 $R_{02}$ 等于 $0$。\n- 在情况 C 中，正演算子 $G$ 耦合了相邻参数，而一阶差分正则化 $L$ 对粗糙度进行惩罚，导致一个非平凡的 $R$，其对角线和非对角线上都有效应；$R_{12}$ 捕捉了从参数 2 到估计分量 1 的串扰。\n- 在情况 D 中，即使 $G = I_4$，各向异性的数据协方差 $C_d$ 也会对数据进行不均等加权，而粗糙度惩罚 $L$ 通过 $L^\\top L$ 耦合了参数，从而产生如 $R_{30}$ 这样的非对角线分辨率元素。\n- 在情况 E 中，$G$ 是秩亏的，但通过 $L = I_2$ 进行的稳定性处理产生了一个良定义的 $R$；值 $R_{01}$ 表明了第二个真实参数如何影响第一个估计参数。\n- 在情况 F 中，对于强正则化 $L = 100 I_2$，我们有 $A \\approx 10000 I_2 + I_2$，所以 $R \\approx (10001 I_2)^{-1} I_2$，这使得对角线元素接近 $1/10001$，而非对角线元素接近零，反映了对分辨率的强烈抑制。\n\n该程序实现了概述的步骤，计算了情况 A–F 的六个所请求的串扰系数 $R_{ij}$，并将其作为逗号分隔的列表，用方括号括起来，打印在单行上。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_resolution_entry(G: np.ndarray, Cd: np.ndarray, L: np.ndarray, i: int, j: int) -> float:\n    \"\"\"\n    Compute the cross-talk coefficient R_{ij} for the regularized least-squares estimator:\n        R = (G^T Cd^{-1} G + L^T L)^{-1} G^T Cd^{-1} G\n    Returns the scalar entry at zero-based indices (i, j).\n    \"\"\"\n    # Invert data covariance (assumed symmetric positive definite)\n    Cd_inv = np.linalg.inv(Cd)\n\n    # Assemble A and B\n    GT_CdInv = G.T @ Cd_inv\n    B = GT_CdInv @ G\n    A = B + (L.T @ L)\n\n    # Solve A R = B for R without explicitly inverting A\n    R = np.linalg.solve(A, B)\n\n    return float(R[i, j])\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A: Identity resolution, on-diagonal\n    G_A = np.eye(3)\n    Cd_A = np.eye(3)\n    L_A = np.zeros((3, 3))\n    i_A, j_A = 1, 1\n\n    # Case B: Identity resolution, off-diagonal\n    G_B = np.eye(3)\n    Cd_B = np.eye(3)\n    L_B = np.zeros((3, 3))\n    i_B, j_B = 0, 2\n\n    # Case C: Blurred forward operator with first-difference regularization\n    G_C = np.array([\n        [1.0, 0.5, 0.0, 0.0],\n        [0.5, 1.0, 0.5, 0.0],\n        [0.0, 0.5, 1.0, 0.5],\n        [0.0, 0.0, 0.5, 1.0]\n    ])\n    Cd_C = np.eye(4)\n    L_C = np.array([\n        [-1.0,  1.0,  0.0,  0.0],\n        [ 0.0, -1.0,  1.0,  0.0],\n        [ 0.0,  0.0, -1.0,  1.0]\n    ])\n    i_C, j_C = 1, 2\n\n    # Case D: Identity forward operator, anisotropic Cd, first-difference L\n    G_D = np.eye(4)\n    Cd_D = np.diag([0.04, 1.0, 0.25, 9.0])\n    L_D = np.array([\n        [-1.0,  1.0,  0.0,  0.0],\n        [ 0.0, -1.0,  1.0,  0.0],\n        [ 0.0,  0.0, -1.0,  1.0]\n    ])\n    i_D, j_D = 3, 0\n\n    # Case E: Rank-deficient G, stabilized by identity L\n    G_E = np.array([\n        [1.0, 1.0],\n        [2.0, 2.0]\n    ])\n    Cd_E = np.eye(2)\n    L_E = np.eye(2)\n    i_E, j_E = 0, 1\n\n    # Case F: Strong identity regularization suppressing resolution\n    G_F = np.eye(2)\n    Cd_F = np.eye(2)\n    L_F = 100.0 * np.eye(2)\n    i_F, j_F = 1, 1\n\n    test_cases = [\n        (G_A, Cd_A, L_A, i_A, j_A),\n        (G_B, Cd_B, L_B, i_B, j_B),\n        (G_C, Cd_C, L_C, i_C, j_C),\n        (G_D, Cd_D, L_D, i_D, j_D),\n        (G_E, Cd_E, L_E, i_E, j_E),\n        (G_F, Cd_F, L_F, i_F, j_F),\n    ]\n\n    results = []\n    for G, Cd, L, i, j in test_cases:\n        res_ij = compute_resolution_entry(G, Cd, L, i, j)\n        results.append(res_ij)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3403399"}]}