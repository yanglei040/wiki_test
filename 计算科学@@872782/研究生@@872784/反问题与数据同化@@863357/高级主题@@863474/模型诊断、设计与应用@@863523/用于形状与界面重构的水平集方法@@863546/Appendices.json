{"hands_on_practices": [{"introduction": "基于梯度的优化是求解形状反问题的核心方法。然而，当问题由偏微分方程（PDE）约束时，计算目标函数相对于整个水平集函数 $\\phi$ 的梯度可能成本极高。本练习将引导你推导并实现伴随状态法（Adjoint-State Method），这是在此类问题中高效计算形状梯度的黄金标准。通过这个实践 [@problem_id:3396641]，你将掌握一个基本工具，并学会通过与有限差分近似进行比较来验证其正确性。", "problem": "考虑一个二维方形域 $\\Omega = [0,1] \\times [0,1]$，其具有齐次狄利克雷边界条件。一个分段常数扩散系数 $a(\\phi)$ 依赖于一个水平集函数 $\\phi:\\Omega \\to \\mathbb{R}$，该函数表示一个未知形状 $\\mathcal{D} = \\{ x \\in \\Omega \\mid \\phi(x) > 0 \\}$。正演模型是变系数泊松方程\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega,\n$$\n其中 $f:\\Omega \\to \\mathbb{R}$ 是一个已知的源项。数据同化目标是通过优化水平集函数 $\\phi$ 来重建形状 $\\mathcal{D}$，以最小化数据失配泛函\n$$\nJ(\\phi) = \\tfrac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\n其中 $u_{\\mathrm{obs}}:\\Omega \\to \\mathbb{R}$ 是从一个不同的、真实的形状 $\\phi_{\\mathrm{true}}$ 生成的观测状态。系数 $a(\\phi)$ 使用平滑的亥维赛函数定义，以确保可微性，\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi),\n$$\n其中\n$$\nH_{\\varepsilon}(s) = \\tfrac{1}{2} + \\frac{1}{\\pi}\\arctan\\!\\Big(\\frac{s}{\\varepsilon}\\Big),\n$$\n这里 $a_{\\mathrm{in}} > 0$ 和 $a_{\\mathrm{out}} > 0$ 是常数，$\\varepsilon > 0$ 是一个平滑参数。与 $H_{\\varepsilon}$ 相关联的平滑狄拉克δ函数是\n$$\n\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}.\n$$\n目标是，从第一性原理出发，推导伴随状态公式以及在水平集参数化下关于 $\\phi$ 的形状梯度。仅从正演模型的强形式、数据失配泛函的定义以及标准的变分法出发，推导伴随方程和形状梯度 $G(\\phi)$ 的逐点表达式，使得对于任何足够光滑的扰动 $\\eta:\\Omega \\to \\mathbb{R}$，一阶方向导数满足\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x.\n$$\n您必须接着实现一个程序，该程序：\n- 在一个具有 $N \\times N$ 个节点和网格间距 $h = 1/(N-1)$ 的均匀笛卡尔网格上离散化 $\\Omega$。\n- 使用 $a$ 的面心算术平均值和齐次狄利克雷边界条件，为 $-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla (\\cdot)\\big)$ 以通量形式组装离散算子。\n- 求解正演问题以计算 $u(\\phi)$。\n- 通过使用真实形状 $\\phi_{\\mathrm{true}}$ 求解相同的正演问题来构建 $u_{\\mathrm{obs}}$。\n- 求解由数据失配残差驱动的伴随问题，以获得伴随变量。\n- 使用推导出的、与水平集参数化和 $a(\\phi)$ 的链式法则一致的逐点表达式，在网格节点上计算形状梯度 $G(\\phi)$。\n- 通过将预测的方向导数 $\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$ 与一个小的步长 $t$ 的有限差分近似 $\\big(J(\\phi + t\\,\\eta) - J(\\phi)\\big)/t$ 进行比较，来验证伴随状态恒等式。\n\n使用以下确定性规范：\n- 源项 $f(x,y) = 1$ 对所有 $(x,y) \\in \\Omega$ 成立。\n- 观测状态 $u_{\\mathrm{obs}}$ 是用一个由圆形给出的真实形状计算的：$\\phi_{\\mathrm{true}}(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_{\\mathrm{true}}$，其中 $r_{\\mathrm{true}} = 0.25$。\n- 扰动是 $\\eta(x,y) = \\sin(2\\pi x)\\sin(2\\pi y)$。\n- 初始水平集函数 $\\phi$ 是一个圆形 $\\phi(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_0$ 或一个常数函数 $\\phi(x,y) \\equiv c_0$，由每个测试用例指定。\n- 平滑的亥维赛函数是 $H_{\\varepsilon}(-\\phi)$，其关于 $\\phi$ 的导数是 $-\\delta_{\\varepsilon}(\\phi)$，其中 $\\delta_{\\varepsilon}(\\phi)$ 如上定义。\n- 内积和积分必须通过网格上的标准黎曼和来近似，每个节点的权重为 $h^2$。\n\n您的程序必须为每个测试用例计算伴随预测的方向导数和有限差分近似之间的相对一致性误差，\n$$\nE = \\frac{\\left|\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x - \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t}\\right|}{\\max\\!\\Big(\\lvert \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x \\rvert + \\left\\lvert \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t} \\right\\rvert, \\,10^{-12}\\Big)}.\n$$\n\n测试套件：\n- 用例 1：$N = 64$，$\\varepsilon = 0.02$， $t = 10^{-3}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是一个半径为 $r_0 = 0.35$ 的圆形。\n- 用例 2：$N = 64$，$\\varepsilon = 0.005$，$t = 10^{-3}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是一个半径为 $r_0 = 0.35$ 的圆形。\n- 用例 3：$N = 16$，$\\varepsilon = 0.02$， $t = 10^{-3}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是一个半径为 $r_0 = 0.35$ 的圆形。\n- 用例 4：$N = 64$，$\\varepsilon = 0.02$， $t = 10^{-4}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是常数 $c_0 = 0.5$（因此 $a(\\phi)$ 是均匀的），对于足够小的 $t$，这应该产生一个接近于零的方向导数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含测试套件的四个相对误差，以逗号分隔的列表形式并用方括号括起来（例如，$\\texttt{[e1,e2,e3,e4]}$）。不应打印任何其他文本。", "solution": "用户提供的问题是偏微分方程约束优化和反演问题领域中一个适定且科学合理的问题。它要求推导由变系数泊松方程控制的形状优化问题的伴随状态方程，然后进行数值实现以验证推导出的梯度。该问题是自洽的，所有参数、控制方程和数值规范都已明确定义。它遵循了变分法和数值分析的既定原则。因此，该问题被认为是**有效的**。\n\n### 使用伴随方法的形状梯度推导\n\n目标是找到泛函 $J(\\phi)$ 相对于水平集函数 $\\phi$ 的梯度。该泛函为\n$$\nJ(\\phi) = \\frac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\n其中状态变量 $u(\\phi)$ 是正演问题的解：\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega.\n$$\n正演问题的弱形式是找到 $u \\in H^1_0(\\Omega)$，使得对于所有测试函数 $v \\in H^1_0(\\Omega)$：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} f\\,v \\,\\mathrm{d}x.\n$$\n\n我们寻求 $J(\\phi)$ 在任意方向 $\\eta$ 上的 Gâteaux 导数，其定义为\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\lim_{t \\to 0} \\frac{J(\\phi + t\\eta) - J(\\phi)}{t}.\n$$\n对 $J(\\phi)$ 的定义应用链式法则，我们得到：\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)\\,\\dot{u} \\,\\mathrm{d}x,\n$$\n其中 $\\dot{u} = \\frac{\\mathrm{d}u}{\\mathrm{d}\\phi}[\\eta]$ 是状态 $u$ 相对于 $\\phi$ 在方向 $\\eta$ 上变化的敏感度。\n\n为了找到 $\\dot{u}$ 的方程，我们对状态方程的弱形式关于 $\\phi$ 在方向 $\\eta$ 上求导。右侧与 $\\phi$ 无关，因此其导数为零。对左侧求导得到：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\int_{\\Omega} a(\\phi+t\\eta)\\,\\nabla u(\\phi+t\\eta) \\cdot \\nabla v \\,\\mathrm{d}x \\right) \\bigg|_{t=0} = 0.\n$$\n应用乘法法则，我们得到切线方程：\n$$\n\\int_{\\Omega} \\dot{a}\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x + \\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = 0,\n$$\n其中 $\\dot{a} = \\frac{\\mathrm{d}a}{\\mathrm{d}\\phi}[\\eta]$。$\\dot{a}$ 项通过对 $a(\\phi)$ 的定义应用链式法则得到：\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi).\n$$\n平滑亥维赛函数 $H_{\\varepsilon}(s)$ 的导数是平滑狄拉克δ函数 $\\delta_{\\varepsilon}(s)$。如规范所述，$H_{\\varepsilon}(-\\phi)$ 关于 $\\phi$ 的导数是 $-\\delta_{\\varepsilon}(\\phi)$，其中 $\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}$。因此，\n$$\n\\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} = (a_{\\mathrm{in}} - a_{\\mathrm{out}}) \\frac{\\mathrm{d}}{\\mathrm{d}\\phi} H_{\\varepsilon}(-\\phi) = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi).\n$$\n这意味着 $\\dot{a} = \\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} \\eta = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta$。\n将此代入切线方程得到：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla v) \\,\\mathrm{d}x.\n$$\n\n引入伴随方法是为了避免显式求解 $\\dot{u}$。我们定义一个伴随状态 $p \\in H^1_0(\\Omega)$ 作为以下伴随方程的解：找到 $p$，使得对于所有测试函数 $w \\in H^1_0(\\Omega)$，\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla w \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,w \\,\\mathrm{d}x.\n$$\n伴随方程的强形式是：\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla p\\big) = u - u_{\\mathrm{obs}} \\quad \\text{in } \\Omega, \\qquad p = 0 \\quad \\text{on } \\partial\\Omega.\n$$\n注意该微分算子是自伴随的。伴随方程的源项是数据失配的残差。\n\n现在，我们策略性地选择伴随方程弱形式中的测试函数 $w$ 为状态敏感度，即 $w=\\dot{u}$。这给出：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,\\dot{u} \\,\\mathrm{d}x.\n$$\n右侧恰好是方向导数 $\\mathrm{D}J(\\phi)[\\eta]$ 的表达式。因此，\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x.\n$$\n因为双线性形式是对称的，我们可以写成 $\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x$。\n我们现在可以使用切线方程，并选择测试函数为伴随状态，即 $v=p$：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla p) \\,\\mathrm{d}x.\n$$\n通过将方向导数的表达式等同起来，我们得到最终形式：\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\Big( (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p) \\Big)\\,\\eta \\,\\mathrm{d}x.\n$$\n该表达式具有所需的形式 $\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$。因此，我们可以确定逐点形状梯度 $G(\\phi)$ 为：\n$$\nG(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).\n$$\n\n### 用于梯度计算的伴随状态系统摘要\n1.  **求解正演问题以得到 $u$**：给定 $\\phi$，求解 $u$：\n    $$-\\nabla \\cdot (a(\\phi)\\nabla u) = f, \\quad u|_{\\partial\\Omega}=0.$$\n2.  **求解伴随问题以得到 $p$**：使用步骤1中的解 $u$，求解 $p$：\n    $$-\\nabla \\cdot (a(\\phi)\\nabla p) = u - u_{\\mathrm{obs}}, \\quad p|_{\\partial\\Omega}=0.$$\n3.  **计算梯度 $G(\\phi)$**：使用 $u$ 和 $p$，计算梯度：\n    $$G(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).$$\n推导到此结束。接下来的 Python 代码以数值方式实现了这个三步过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main driver function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'N': 64, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 2\n        {'N': 64, 'eps': 0.005, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 3\n        {'N': 16, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 4\n        {'N': 64, 'eps': 0.02, 't': 1e-4, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'const', 'phi_param': 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_case(**params)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\ndef run_case(N, eps, t, a_in, a_out, phi_type, phi_param):\n    \"\"\"\n    Executes a single test case for the adjoint verification.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    y = np.linspace(0, 1, N)\n    X, Y = np.meshgrid(x, y)\n\n    # Helper functions for level set, Heaviside, and delta\n    def get_phi(X, Y, type, param):\n        if type == 'circle':\n            radius = param\n            return np.sqrt((X - 0.5)**2 + (Y - 0.5)**2) - radius\n        elif type == 'const':\n            c0 = param\n            return np.full_like(X, c0)\n        \n    def H_eps(s, eps_val):\n        return 0.5 + (1.0 / np.pi) * np.arctan(s / eps_val)\n\n    def delta_eps(s, eps_val):\n        return (1.0 / np.pi) * eps_val / (eps_val**2 + s**2)\n\n    def get_a(phi, a_in_val, a_out_val, eps_val):\n        return a_out_val + (a_in_val - a_out_val) * H_eps(-phi, eps_val)\n\n    # --- Finite Difference Poisson Solver ---\n    def solve_poisson(a_coeff, rhs, h_val, N_val):\n        num_interior = (N_val - 2)**2\n        A = lil_matrix((num_interior, num_interior), dtype=np.float64)\n        \n        # Assemble matrix A using a 5-point stencil for -div(a grad(u))\n        for i in range(1, N_val - 1):\n            for j in range(1, N_val - 1):\n                k = (i - 1) * (N_val - 2) + (j - 1)\n\n                # Face-centered arithmetic means for 'a'\n                a_south = (a_coeff[i-1, j] + a_coeff[i, j]) / 2.0\n                a_north = (a_coeff[i+1, j] + a_coeff[i, j]) / 2.0\n                a_west  = (a_coeff[i, j-1] + a_coeff[i, j]) / 2.0\n                a_east  = (a_coeff[i, j+1] + a_coeff[i, j]) / 2.0\n                \n                A[k, k] = -(a_north + a_south + a_east + a_west)\n\n                if i > 1:\n                    A[k, k - (N_val - 2)] = a_south # South neighbor\n                if i  N_val - 2:\n                    A[k, k + (N_val - 2)] = a_north # North neighbor\n                if j > 1:\n                    A[k, k - 1] = a_west           # West neighbor\n                if j  N_val - 2:\n                    A[k, k + 1] = a_east           # East neighbor\n\n        A /= h_val**2\n        \n        # Flatten the right-hand side for interior nodes\n        b = rhs[1:-1, 1:-1].flatten()\n        \n        # Solve the linear system\n        u_interior = spsolve(A.tocsr(), b)\n        \n        # Embed solution back into the full grid with boundary conditions\n        u_full = np.zeros((N_val, N_val))\n        u_full[1:-1, 1:-1] = u_interior.reshape((N_val - 2, N_val - 2))\n        return u_full\n\n    # Function to compute the objective functional J\n    def compute_J(u, u_obs, h_val):\n        return 0.5 * np.sum((u - u_obs)**2) * h_val**2\n\n    # --- Main Calculation Steps ---\n    \n    # 1. Compute observed state u_obs\n    f_source = np.ones((N, N))\n    r_true = 0.25\n    phi_true = get_phi(X, Y, 'circle', r_true)\n    a_true = get_a(phi_true, a_in, a_out, eps)\n    u_obs = solve_poisson(a_true, f_source, h, N)\n    \n    # 2. Define initial state phi and perturbation eta\n    phi = get_phi(X, Y, phi_type, phi_param)\n    eta = np.sin(2 * np.pi * X) * np.sin(2 * np.pi * Y)\n\n    # 3. Solve forward problem for u(phi) and compute J(phi)\n    a_phi = get_a(phi, a_in, a_out, eps)\n    u_phi = solve_poisson(a_phi, f_source, h, N)\n    J_phi = compute_J(u_phi, u_obs, h)\n    \n    # 4. Compute finite difference approximation of the directional derivative\n    phi_pert = phi + t * eta\n    a_pert = get_a(phi_pert, a_in, a_out, eps)\n    u_pert = solve_poisson(a_pert, f_source, h, N)\n    J_pert = compute_J(u_pert, u_obs, h)\n    \n    DJ_fd = (J_pert - J_phi) / t\n\n    # 5. Compute adjoint-based directional derivative\n    # 5a. Solve adjoint equation: -div(a grad(p)) = u - u_obs\n    adjoint_rhs = u_phi - u_obs\n    p = solve_poisson(a_phi, adjoint_rhs, h, N)\n\n    # 5b. Compute gradients of u and p\n    grad_u_y, grad_u_x = np.gradient(u_phi, h)\n    grad_p_y, grad_p_x = np.gradient(p, h)\n    \n    # 5c. Compute the shape gradient G(phi)\n    grad_u_dot_grad_p = grad_u_x * grad_p_x + grad_u_y * grad_p_y\n    G_phi = (a_in - a_out) * delta_eps(phi, eps) * grad_u_dot_grad_p\n    \n    # 5d. Compute the directional derivative via inner product\n    DJ_adj = np.sum(G_phi * eta) * h**2\n    \n    # 6. Calculate the relative consistency error\n    numerator = np.abs(DJ_adj - DJ_fd)\n    denominator = max(np.abs(DJ_adj) + np.abs(DJ_fd), 1e-12)\n    error = numerator / denominator\n\n    return error\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3396641"}, {"introduction": "反问题通常是病态的，这意味着微小的数据扰动可能导致解的巨大变化。正则化是克服这一挑战的关键技术，但它引入了一个必须仔细选择的正则化参数 $\\alpha$。本练习 [@problem_id:3396597] 探讨了在水平集框架下如何使用周长正则化，并实现经典的 Morozov 差异原则（Morozov Discrepancy Principle）来自动确定最佳的 $\\alpha$ 值。这将使你能够平衡数据拟合与解的平滑性，从而获得稳定且有物理意义的重构结果。", "problem": "考虑一个形状重建反问题，其中矩形域内的一个未知子集被隐式地表示为一个标量场的零水平集。设域为一个大小为 $N \\times N$ 的离散网格，其中 $N = 48$。一个形状由一个水平集函数 $\\phi : \\{1,\\dots,N\\} \\times \\{1,\\dots,N\\} \\to \\mathbb{R}$ 表示，使得重建的指示场为 $u = H_{\\varepsilon}(\\phi)$，其中 $H_{\\varepsilon}$ 是一个平滑的亥维赛德函数，平滑参数为 $\\varepsilon  0$。观测值由一个已知的线性对称正向算子 $A$ 应用于 $u$ 生成，并被标准差为 $\\sigma$ 的加性独立高斯噪声污染。重建问题被描述为一个类吉洪诺夫泛函的最小化问题，该泛函带有一个在水平集框架下表示的周长先验。\n\n使用以下基本基础：\n- 通过亥维赛德函数 $H_{\\varepsilon}(\\phi)$ 及其平滑的狄拉克-德尔塔函数 $\\delta_{\\varepsilon}(\\phi) = H_{\\varepsilon}'(\\phi)$ 对二值形状进行水平集表示。\n- Morozov 差异原则：选择正则化参数，使得数据失配在适当的范数下等于噪声水平。\n- 利用标准变分法，从能量泛函的变分导数中得到的梯度流。\n\n设正向算子 $A$ 是一个高斯模糊，其标准差为 $\\sigma_{b} = 1.2$ 个网格单位，并采用反射边界条件。设平滑的亥维赛德函数和平滑的狄拉克-德尔塔函数定义为\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right), \\quad\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\tfrac{\\varepsilon}{\\varepsilon^{2} + x^{2}},\n$$\n其中 $\\varepsilon = 1.0$。\n\n设真实形状为一个半径为 $r_{\\mathrm{true}} = 12$ 像素、中心在 $(c_x,c_y) = (24,30)$ 的圆盘，由符号距离水平集 $\\phi_{\\mathrm{true}}(i,j) = \\sqrt{(i-c_x)^2 + (j-c_y)^2} - r_{\\mathrm{true}}$ 表示。初始猜测为一个半径为 $r_{0} = 10$ 像素、中心在 $(c_{0x},c_{0y}) = (16,18)$ 的圆盘，即 $\\phi_{0}(i,j) = \\sqrt{(i-c_{0x})^2 + (j-c_{0y})^2} - r_{0}$。无噪声数据为 $y_{\\mathrm{clean}} = A(H_{\\varepsilon}(\\phi_{\\mathrm{true}}))$，观测数据为 $y = y_{\\mathrm{clean}} + \\eta$，其中 $\\eta$ 的分量是均值为零、标准差为 $\\sigma$ 的独立高斯分布。数据点的数量为 $m = N^{2}$。\n\n对于给定的正则化参数 $\\alpha  0$，考虑能量泛函\n$$\nJ(\\phi;\\alpha) = \\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2} + \\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x,\n$$\n其中 $\\Omega$ 是离散网格，$\\lVert \\cdot \\rVert_{2}$ 是 $\\mathbb{R}^{m}$ 上的欧几里得范数。利用变分法和链式法则，$\\phi$ 的梯度下降法（时间步长为 $\\Delta t$）可以写成离散形式\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\delta_{\\varepsilon}(\\phi^{k}) \\cdot A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) - \\alpha \\, \\delta_{\\varepsilon}(\\phi^{k}) \\, \\kappa(\\phi^{k}) \\right),\n$$\n其中 $A^{\\top} = A$（高斯模糊的对称性），$\\kappa(\\phi)$ 是零水平集的曲率，由下式给出\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right),\n$$\n其中有一个小的稳定项 $\\beta = 10^{-8}$。空间导数使用中心差分和反射边界条件进行近似。对于给定的 $\\alpha$，使用 $\\Delta t = 0.2$ 并在每次评估 $J$ 时执行 $K = 80$ 次迭代。\n\nMorozov 差异原则规定选择 $\\alpha$，使得残差范数在一个乘法因子范围内等于噪声水平：选择 $\\alpha$ 以使得\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} \\approx \\tau \\, \\sigma \\, \\sqrt{m},\n$$\n其中 $\\phi_{\\alpha}^{\\star}$ 是从 $\\phi_{0}$ 开始，经过 $K$ 次梯度下降迭代后的结果。对 $\\alpha$ 实现一个区间套定和二分法程序，使用初始区间 $[\\alpha_{\\min},\\alpha_{\\max}] = [10^{-6}, 1.0]$，将 $\\alpha_{\\max}$ 翻倍，直到在 $\\alpha_{\\max}$ 处的残差不小于目标差异，或者直到 $\\alpha_{\\max}$ 超过 $10^{6}$。在二分法中对差异使用相对容差 $\\rho = 10^{-2}$；如果目标无法被区间套住，则选择产生最接近目标残差的端点。\n\n测试组。对于以下每个案例，使用固定的伪随机种子 $s_{0} + i$ 生成噪声，其中 $s_{0} = 12345$，$i$ 是测试案例的从 0 开始的索引。对于每个案例，按照所述的 Morozov 差异原则计算选定的正则化参数 $\\alpha^{\\star}$。这些案例是：\n- 案例 1：$(\\sigma,\\tau) = (0.05, 1.0)$。\n- 案例 2：$(\\sigma,\\tau) = (0.01, 1.0)$。\n- 案例 3：$(\\sigma,\\tau) = (0.10, 1.1)$。\n- 案例 4：$(\\sigma,\\tau) = (0.08, 0.9)$。\n\n最终输出格式。您的程序应生成单行输出，其中包含每个案例选定的 $\\alpha^{\\star}$，格式为用方括号括起来的逗号分隔列表，每个数字四舍五入到 $6$ 位小数（例如，\"[0.123456,0.234567,0.345678,0.456789]\"）。不应打印任何额外文本。所有角度（如有）均以弧度为单位。除了离散网格间距外，没有其他物理单位；请将所要求的值报告为无量纲数。确保程序是完全确定性的，并且不需要任何用户输入。", "solution": "用户提供的问题是一个适定且自洽的计算反问题练习，特别关注使用基于水平集方法进行形状重建。该问题在科学上是合理的，所有组成部分——物理模型、数学公式和数值算法——都是标准的且定义清晰。任务是使用 Morozov 差异原则为四种不同情景确定最优正则化参数 $\\alpha$。这将通过为水平集函数实现梯度下降优化和为参数 $\\alpha$ 实现二分搜索来完成。\n\n首先，我们对问题进行形式化。未知形状由一个定义在大小为 $N \\times N$（其中 $N=48$）的离散网格 $\\Omega$ 上的函数 $\\phi(x,y)$ 的零水平集表示。形状的指示函数 $u$（在形状内部为 $1$，外部为 $0$）由一个平滑的亥维赛德函数 $H_{\\varepsilon}(\\phi)$ 近似。问题指定：\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right)\n$$\n平滑宽度为 $\\varepsilon = 1.0$。该函数的导数是平滑的狄拉克-德尔塔函数 $\\delta_{\\varepsilon}(x) = H_{\\varepsilon}'(x)$，由下式给出：\n$$\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\frac{\\varepsilon}{\\varepsilon^{2} + x^{2}}\n$$\n该函数将计算局部化到界面附近（其中 $\\phi \\approx 0$）。\n\n数据 $y$ 是通过将一个线性正向算子 $A$ 应用于真实的指示函数 $u_{\\mathrm{true}} = H_{\\varepsilon}(\\phi_{\\mathrm{true}})$ 并加上高斯噪声 $\\eta$ 生成的。算子 $A$ 是一个标准差为 $\\sigma_b = 1.2$ 个网格单位的高斯模糊，并且是对称的 ($A=A^\\top$)。真实形状是一个半径为 $r_{\\mathrm{true}} = 12$、中心在 $(c_x, c_y) = (24, 30)$ 的圆盘，而重建的初始猜测是一个半径为 $r_0 = 10$、中心在 $(c_{0x}, c_{0y}) = (16, 18)$ 的圆盘。噪声 $\\eta$ 从均值为 $0$、标准差为 $\\sigma$ 的高斯分布中抽取。\n\n重建的 $\\phi$ 通过最小化类吉洪诺夫能量泛函 $J(\\phi;\\alpha)$ 来找到：\n$$\nJ(\\phi;\\alpha) = \\underbrace{\\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2}}_{\\text{数据保真项}} + \\underbrace{\\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x}_{\\text{周长正则化项}}\n$$\n第一项确保重建的形状在模糊后与观测数据 $y$ 匹配。第二项是周长惩罚项，它通过偏好具有较短边界长度的形状来对问题进行正则化，从而促进更平滑、更紧凑的重建。$\\alpha0$ 是平衡这两个相互竞争目标的正则化参数。\n\n为了最小化 $J(\\phi;\\alpha)$，我们使用梯度下降算法。$\\phi$ 在一个虚拟时间 $t$ 上的演化遵循能量的负梯度。问题提供了离散更新规则：\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\frac{\\delta J}{\\delta \\phi}(\\phi^k) \\right)\n$$\n其中使用了变分导数 $\\frac{\\delta J}{\\delta \\phi}$ 的标准简化形式，得到更新方程：\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( -\\delta_{\\varepsilon}(\\phi^{k}) A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) - \\alpha \\delta_{\\varepsilon}(\\phi^{k}) \\kappa(\\phi^{k}) \\right)\n$$\n这里，$\\Delta t = 0.2$ 是时间步长。括号内的第一项是数据驱动力，它拉动水平集以匹配数据。第二项是几何正则化力，它根据水平集的平均曲率 $\\kappa(\\phi)$ 移动水平集，从而有效地平滑边界。曲率由下式给出：\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right)\n$$\n其中有一个小的稳定项 $\\beta=10^{-8}$。所有空间导数（梯度 $\\nabla$ 和散度 $\\nabla \\cdot$）都使用中心差分和反射边界条件进行近似，这是通过在微分前对网格进行填充来实现的。\n\n问题的核心是选择参数 $\\alpha$。为此采用了 Morozov 差异原则。它规定正则化参数 $\\alpha$ 的选择应使得最优解 $\\phi^{\\star}_{\\alpha}$ 的数据残差的欧几里得范数与噪声的期望范数相匹配。目标公式化为：\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} = \\tau \\, \\sigma \\, \\sqrt{m}\n$$\n其中 $m = N^2 = 48^2$ 是数据点（像素）的总数，$\\tau$ 是一个接近 $1$ 的给定因子。项 $\\sigma\\sqrt{m}$ 是噪声向量的期望 $L_2$-范数。对于每个给定的 $(\\sigma, \\tau)$，我们必须找到相应的 $\\alpha^{\\star}$。\n\n为了找到 $\\alpha^{\\star}$，我们定义一个函数 $D(\\alpha) = \\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2}$，其中 $\\phi_{\\alpha}^{\\star}$ 是使用参数 $\\alpha$ 运行梯度下降 $K=80$ 次迭代后的结果。$D(\\alpha)$ 函数通常随 $\\alpha$ 单调递增。寻找 $\\alpha^{\\star}$ 的问题简化为求解标量方程 $D(\\alpha) = \\tau \\sigma \\sqrt{m}$。这通过一个区间套定和二分算法进行数值求解：\n1.  **区间套定**：建立一个初始搜索区间 $[\\alpha_{\\min}, \\alpha_{\\max}] = [10^{-6}, 1.0]$。如果在 $\\alpha_{\\max}$ 处的差异小于目标，则将 $\\alpha_{\\max}$ 反复加倍，直到目标被套住（即 $D(\\alpha_{\\min})  \\text{目标}  D(\\alpha_{\\max})$）或 $\\alpha_{\\max}$ 超过上限 $10^6$。如果区间套定失败，则选择搜索范围中产生最接近目标差异的端点。\n2.  **二分法**：一旦找到一个区间，就使用二分法来缩小该区间，迭代地将其减半，直到中点 $\\alpha_m$ 处的差异在目标值的相对容差 $\\rho=10^{-2}$ 之内。\n\n每个测试案例的总体算法如下：\n1.  生成真实的符号距离函数 $\\phi_{\\mathrm{true}}$、真实的指示函数 $u_{\\mathrm{true}}$ 和干净的数据 $y_{\\mathrm{clean}} = A(u_{\\mathrm{true}})$。\n2.  使用指定的标准差 $\\sigma$ 和随机种子生成噪声向量 $\\eta$，并构成带噪声的数据 $y = y_{\\mathrm{clean}} + \\eta$。\n3.  计算目标差异 $T = \\tau \\sigma \\sqrt{m}$。\n4.  执行对 $\\alpha$ 的区间套定和二分搜索，以找到在指定容差内解出 $D(\\alpha) = T$ 的值 $\\alpha^{\\star}$。\n5.  存储得到的 $\\alpha^{\\star}$ 并为下一个测试案例重复此过程。\n最后，所有四个案例计算出的 $\\alpha^{\\star}$ 值将按指定格式报告。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import ndimage\n\ndef solve():\n    \"\"\"\n    Solves the level set shape reconstruction problem by finding the optimal\n    regularization parameter alpha using the Morozov discrepancy principle.\n    \"\"\"\n    \n    # Constants from the problem statement\n    N = 48\n    EPSILON = 1.0\n    SIGMA_B = 1.2\n    R_TRUE = 12.0\n    CX_TRUE, CY_TRUE = 23, 29  # 0-based from 1-based (24, 30)\n    R0 = 10.0\n    CX0, CY0 = 15, 17  # 0-based from 1-based (16, 18)\n    BETA = 1e-8\n    DT = 0.2\n    K = 80\n    ALPHA_MIN_INIT = 1e-6\n    ALPHA_MAX_INIT = 1.0\n    ALPHA_MAX_CAP = 1e6\n    RHO = 1e-2\n    S0 = 12345\n    M = N * N\n\n    # 1. Mathematical Functions\n    def smoothed_heaviside(x):\n        \"\"\" Smoothed Heaviside function H_epsilon(x). \"\"\"\n        return 0.5 + (1.0 / np.pi) * np.arctan(x / EPSILON)\n\n    def smoothed_delta(x):\n        \"\"\" Smoothed Dirac delta function delta_epsilon(x). \"\"\"\n        return (1.0 / np.pi) * (EPSILON / (EPSILON**2 + x**2))\n\n    def create_sdf_disk(n_grid, cx, cy, r):\n        \"\"\" Creates a signed distance function for a disk. \"\"\"\n        y_coords, x_coords = np.mgrid[0:n_grid, 0:n_grid]\n        return np.sqrt((x_coords - cx)**2 + (y_coords - cy)**2) - r\n\n    # 2. Operators and Derivatives\n    def forward_operator_A(u):\n        \"\"\" Forward operator A: Gaussian blur with reflective boundaries. \"\"\"\n        return ndimage.gaussian_filter(u, sigma=SIGMA_B, mode='reflect')\n\n    def gradient_cd_reflect(f):\n        \"\"\" Computes gradient (fy, fx) using central differences and reflective padding. \"\"\"\n        f_padded = np.pad(f, pad_width=1, mode='reflect')\n        fy = (f_padded[2:, 1:-1] - f_padded[:-2, 1:-1]) / 2.0\n        fx = (f_padded[1:-1, 2:] - f_padded[1:-1, :-2]) / 2.0\n        return fy, fx\n\n    def divergence_cd_reflect(fy, fx):\n        \"\"\" Computes divergence of a vector field (fy, fx) using central differences. \"\"\"\n        fy_padded = np.pad(fy, pad_width=1, mode='reflect')\n        fx_padded = np.pad(fx, pad_width=1, mode='reflect')\n        fyy = (fy_padded[2:, 1:-1] - fy_padded[:-2, 1:-1]) / 2.0\n        fxx = (fx_padded[1:-1, 2:] - fx_padded[1:-1, :-2]) / 2.0\n        return fxx + fyy\n\n    def curvature(phi):\n        \"\"\" Computes curvature kappa(phi) of the level set. \"\"\"\n        phi_y, phi_x = gradient_cd_reflect(phi)\n        grad_phi_norm = np.sqrt(phi_x**2 + phi_y**2 + BETA)\n        g_x = phi_x / grad_phi_norm\n        g_y = phi_y / grad_phi_norm\n        return divergence_cd_reflect(g_y, g_x)\n\n    # 3. Optimization and Parameter Search\n    def run_gradient_descent(phi_init, y, alpha):\n        \"\"\" Performs K iterations of gradient descent for a given alpha. \"\"\"\n        phi = phi_init.copy()\n        for _ in range(K):\n            u_k = smoothed_heaviside(phi)\n            delta_k = smoothed_delta(phi)\n            residual = forward_operator_A(u_k) - y\n            data_term_grad = -delta_k * forward_operator_A(residual) # A is symmetric\n            kappa_k = curvature(phi)\n            reg_term_grad = alpha * delta_k * kappa_k\n            phi += DT * (data_term_grad - reg_term_grad)\n        return phi\n\n    def get_discrepancy(alpha, phi_init, y, memo):\n        \"\"\" Calculates the residual norm for a given alpha, with memoization. \"\"\"\n        if alpha in memo:\n            return memo[alpha]\n        phi_star = run_gradient_descent(phi_init, y, alpha)\n        u_star = smoothed_heaviside(phi_star)\n        discrepancy = np.linalg.norm(forward_operator_A(u_star) - y)\n        memo[alpha] = discrepancy\n        return discrepancy\n\n    def find_alpha_morozov(y, phi_0, sigma, tau):\n        \"\"\" Finds the optimal alpha using Morozov's principle with bisection. \"\"\"\n        target_discrepancy = tau * sigma * np.sqrt(M)\n        memo = {}\n        \n        alpha_min, alpha_max = ALPHA_MIN_INIT, ALPHA_MAX_INIT\n        d_min = get_discrepancy(alpha_min, phi_0, y, memo)\n        \n        if d_min > target_discrepancy:\n            # The discrepancy is already too large even for smallest alpha.\n            # This can happen if the number of iterations is too small or step size too large.\n            # As per instruction to choose closest endpoint, alpha_min is the choice.\n            return alpha_min\n\n        d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n\n        # Bracketing\n        while d_max  target_discrepancy:\n            alpha_max *= 2.0\n            if alpha_max > ALPHA_MAX_CAP:\n                d_cap = get_discrepancy(ALPHA_MAX_CAP, phi_0, y, memo)\n                # If target cannot be bracketed, choose endpoint that gives closest discrepancy\n                if abs(d_cap - target_discrepancy)  abs(d_max/2.0 - target_discrepancy):\n                    return ALPHA_MAX_CAP\n                else:\n                    return alpha_max / 2.0\n            d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n            \n        # Bisection\n        alpha_l, alpha_u = alpha_min, alpha_max\n        for _ in range(50): # 50 iterations are more than enough for convergence\n            alpha_m = (alpha_l + alpha_u) / 2.0\n            if alpha_m == alpha_l or alpha_m == alpha_u:\n                break # Reached precision limit\n            d_m = get_discrepancy(alpha_m, phi_0, y, memo)\n            if abs(d_m - target_discrepancy)  RHO * target_discrepancy:\n                return alpha_m\n            if d_m  target_discrepancy:\n                alpha_l = alpha_m\n            else:\n                alpha_u = alpha_m\n        \n        # Return the one that is closer to target\n        d_l = get_discrepancy(alpha_l, phi_0, y, memo)\n        d_u = get_discrepancy(alpha_u, phi_0, y, memo)\n        return alpha_l if abs(d_l - target_discrepancy)  abs(d_u - target_discrepancy) else alpha_u\n\n    # 4. Main Execution Loop\n    phi_true = create_sdf_disk(N, CX_TRUE, CY_TRUE, R_TRUE)\n    u_true = smoothed_heaviside(phi_true)\n    y_clean = forward_operator_A(u_true)\n    phi_0 = create_sdf_disk(N, CX0, CY0, R0)\n    \n    test_cases = [\n        (0.05, 1.0), (0.01, 1.0), (0.10, 1.1), (0.08, 0.9)\n    ]\n    \n    results = []\n    for i, (sigma, tau) in enumerate(test_cases):\n        seed = S0 + i\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(0, sigma, (N, N))\n        y = y_clean + noise\n        \n        alpha_star = find_alpha_morozov(y, phi_0, sigma, tau)\n        results.append(\"{:.6f}\".format(alpha_star))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3396597"}, {"introduction": "在反问题中，我们通常假设测量误差服从高斯分布，这对应于最小二乘数据拟合项。然而，真实世界的测量数据常常包含离群点或具有“重尾”分布，使得最小二乘法的结果变得不可靠。本练习 [@problem_id:3396667] 将指导你从贝叶斯理论出发，为不同的噪声模型（高斯、拉普拉斯和学生t分布）推导相应的“稳健”拟合泛函。通过比较它们在含有离群点数据下的表现，你将深刻理解选择合适的数据拟合项对于提高重构质量的重要性。", "problem": "考虑一个在水平集方法框架下构建的二维形状重建问题，其中一个二值形状由一个水平集函数表示。设未知形状由圆的参数 $\\theta = (c_x, c_y, r)$ 参数化，并且水平集函数定义为 $\\phi_\\theta(x,y)$，使得其零水平集描绘了界面。正向算子通过平滑与 $\\phi_\\theta$ 相关的二值指示函数，并将其与一个已知的点扩散函数进行卷积，从而将 $\\phi_\\theta$ 映射到一个预测的观测图像。观测值被噪声所污染，该噪声不一定是高斯噪声，且可能包含稀疏离群值。\n\n从以下三点出发：(i) 将似然与负对数似然失配联系起来的贝叶斯范式，(ii) 几何形状的水平集函数的定义，以及 (iii) 与已知的高斯点扩散函数（PSF）的卷积，推导出对应于三种噪声模型（高斯、拉普拉斯和具有固定自由度的学生t分布）的鲁棒负对数似然失配。您的程序必须实现这些推导出的失配函数，并在形状参数空间上进行离散搜索，以针对每个测试案例和每种噪声模型估计半径 $r$。\n\n程序必须遵循以下建模和算法规范：\n\n- 水平集表示：\n  - 在一个大小为 $N \\times N$ 的均匀网格上，使用水平集函数 $\\phi_\\theta(x,y)$ 来表示一个圆，其中像素坐标为 $(x,y) \\in \\{0,1,\\dots,N-1\\}^2$。\n  - 通过一个平滑参数为 $\\epsilon  0$ 的光滑亥维赛德函数 $H_\\epsilon(\\phi_\\theta)$ 来使用二值指示函数的光滑近似。\n\n- 正向算子：\n  - 预测图像是通过将 $H_\\epsilon(\\phi_\\theta)$ 与一个标准差为 $\\sigma_{\\text{psf}}$ 的高斯PSF进行卷积得到的。\n\n- 噪声模型和鲁棒失配：\n  - 从第一性原理出发，为以下三种噪声模型推导负对数似然失配：\n    - 标准差为 $\\sigma$ 的高斯模型。\n    - 尺度参数为 $b$ 的拉普拉斯模型。\n    - 自由度为 $\\nu$、尺度参数为 $\\sigma_t$ 的学生t模型。\n  - 将这些失配实现为观测图像与预测图像之间图像残差的函数。\n\n- 估计策略：\n  - 对于每个测试案例，通过在真实参数 $\\theta^\\star = (c_x^\\star, c_y^\\star, r^\\star)$ 处评估正向算子，加入所述标准差的零均值独立高斯噪声，然后注入指定数量和幅度的稀疏离群值，来生成合成观测数据。\n  - 对于每种噪声模型，在以下参数空间上进行离散搜索\n    $$\n    c_x \\in \\{c_x^\\star - 2, c_x^\\star - 1, c_x^\\star, c_x^\\star + 1, c_x^\\star + 2\\},\\quad\n    c_y \\in \\{c_y^\\star - 2, c_y^\\star - 1, c_y^\\star, c_y^\\star + 1, c_y^\\star + 2\\},\\quad\n    r \\in \\{r^\\star - 2, r^\\star - 1, r^\\star, r^\\star + 1, r^\\star + 2\\},\n    $$\n    将索引裁剪到有效的图像边界内，并将 $r$ 限制为正整数，然后选择使相应负对数似然失配最小化的参数。\n  - 仅报告每个测试案例中每种噪声模型估计出的半径 $r$。\n\n测试套件和参数：\n\n- 所有角度（如有）均以弧度处理。不涉及物理单位，所有输出均为无量纲。\n\n- 使用以下三个测试案例（所有整数值均为精确值，必须如此处理）：\n\n  1. 案例 $1$（基线，近乎理想）：\n     - $N = 64$, $c_x^\\star = 32$, $c_y^\\star = 32$, $r^\\star = 12$, $\\epsilon = 1.0$, $\\sigma_{\\text{psf}} = 1.2$,\n     - 观测噪声标准差 $\\sigma_{\\text{obs}} = 0.05$，离群值数量 $n_{\\text{out}} = 0$，离群值幅度 $a_{\\text{out}} = 0.0$。\n\n  2. 案例 $2$（重尾污染）：\n     - $N = 64$, $c_x^\\star = 32$, $c_y^\\star = 32$, $r^\\star = 12$, $\\epsilon = 1.0$, $\\sigma_{\\text{psf}} = 1.2$,\n     - 观测噪声标准差 $\\sigma_{\\text{obs}} = 0.05$，离群值数量 $n_{\\text{out}} = 300$，离群值幅度 $a_{\\text{out}} = 0.5$。\n\n  3. 案例 $3$（近边界几何）：\n     - $N = 64$, $c_x^\\star = 54$, $c_y^\\star = 10$, $r^\\star = 9$, $\\epsilon = 1.0$, $\\sigma_{\\text{psf}} = 1.2$,\n     - 观测噪声标准差 $\\sigma_{\\text{obs}} = 0.04$，离群值数量 $n_{\\text{out}} = 40$，离群值幅度 $a_{\\text{out}} = 0.3$。\n\n- 对于失配参数化，使用：\n  - 高斯模型：对于每个案例，$\\sigma = \\sigma_{\\text{obs}}$。\n  - 拉普拉斯模型：对于每个案例，$b = \\sigma_{\\text{obs}} / \\sqrt{2}$。\n  - 学生t模型：$\\nu = 3$（对所有案例固定），对于每个案例，$\\sigma_t = \\sigma_{\\text{obs}}$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、由逗号分隔的列表组成的列表，每个内部列表对应一个测试案例，并按 [高斯, 拉普拉斯, 学生t] 的顺序包含三个估计的整数半径。例如：“[[r1G,r1L,r1T],[r2G,r2L,r2T],[r3G,r3L,r3T]]”。", "solution": "我们从反问题的贝叶斯框架开始。设观测图像为 $y \\in \\mathbb{R}^{N \\times N}$，将参数 $\\theta$ 映射到预测图像 $f(\\theta) \\in \\mathbb{R}^{N \\times N}$ 的正向算子。在基于似然的方法下，后验密度与先验密度和似然的乘积成正比。在没有信息先验的情况下，最大后验 (MAP) 估计器简化为最大似然 (ML) 估计器，最小化负对数似然等价于最大化似然。\n\n残差图像定义为 $r(\\theta) = y - f(\\theta)$。负对数似然失配是根据独立同分布噪声假设下逐像素应用的噪声模型的概率密度函数 (PDF) 推导出来的。我们推导三种失配函数：\n\n1. 高斯噪声模型：\n   标准差为 $\\sigma$ 的高斯噪声模型，其每个残差 $r_i$ 的PDF由下式给出\n   $$\n   p(r_i \\mid \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right).\n   $$\n   在不考虑与 $\\theta$ 无关的加性常数的情况下，对所有像素聚合的负对数似然为\n   $$\n   \\mathcal{J}_{\\text{Gaussian}}(\\theta) = \\sum_i \\frac{r_i(\\theta)^2}{2\\sigma^2}.\n   $$\n   这是我们熟悉的、由方差缩放的最小二乘失配，由于对大残差施加二次惩罚，因此对离群值敏感。\n\n2. 拉普拉斯（双指数）噪声模型：\n   尺度参数为 $b$ 的拉普拉斯噪声模型，其PDF为\n   $$\n   p(r_i \\mid b) = \\frac{1}{2b} \\exp\\left(-\\frac{|r_i|}{b}\\right).\n   $$\n   忽略常数，负对数似然变为\n   $$\n   \\mathcal{J}_{\\text{Laplace}}(\\theta) = \\sum_i \\frac{|r_i(\\theta)|}{b}.\n   $$\n   这是一种 $\\ell_1$ 型失配，它比高斯模型对离群值更具鲁棒性，因为其惩罚随残差幅度线性增长。\n\n3. 学生t噪声模型：\n   自由度为 $\\nu$、尺度为 $\\sigma_t$ 的学生t分布，其PDF为\n   $$\n   p(r_i \\mid \\nu, \\sigma_t) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma_t} \\left(1 + \\frac{r_i^2}{\\nu \\sigma_t^2}\\right)^{-\\frac{\\nu+1}{2}}.\n   $$\n   在不考虑与 $\\theta$ 无关的加性常数的情况下，负对数似然为\n   $$\n   \\mathcal{J}_{\\text{Student}}(\\theta) = \\sum_i \\frac{\\nu+1}{2} \\log\\left(1 + \\frac{r_i(\\theta)^2}{\\nu \\sigma_t^2}\\right).\n   $$\n   这种失配对离群值具有强鲁棒性，因为其有效惩罚从零附近的二次惩罚过渡到大残差的亚二次惩罚。\n\n通过水平集方法构建正向算子：\n\n- 我们用水平集函数表示一个圆\n  $$\n  \\phi_\\theta(x,y) = r - \\sqrt{(x - c_x)^2 + (y - c_y)^2},\n  $$\n  其中 $(x,y)$ 是网格 $\\{0,1,\\dots,N-1\\} \\times \\{0,1,\\dots,N-1\\}$ 上的像素坐标，$\\theta = (c_x, c_y, r)$。\n\n- 为了获得一个可微的指示函数，我们使用一个平滑参数为 $\\epsilon  0$ 的光滑亥维赛德函数 $H_\\epsilon(\\cdot)$。一个常用的光滑近似是基于双曲正切函数：\n  $$\n  H_\\epsilon(\\phi) = \\frac{1}{2} \\left( 1 + \\tanh\\left(\\frac{\\phi}{\\epsilon}\\right) \\right),\n  $$\n  它在宽度与 $\\epsilon$ 成正比的界面区域内从0平滑过渡到1。\n\n- 预测图像通过将 $H_\\epsilon(\\phi_\\theta)$ 与标准差为 $\\sigma_{\\text{psf}}$ 的高斯点扩散函数 (PSF) 进行卷积来建模。用 $\\ast$ 表示卷积，我们有\n  $$\n  f(\\theta) = \\text{PSF}_{\\sigma_{\\text{psf}}} \\ast H_\\epsilon(\\phi_\\theta).\n  $$\n  在实践中，我们通过对图像应用高斯滤波器来计算它。\n\n合成观测数据：\n\n- 对于每个测试案例，我们在真实参数 $\\theta^\\star$ 处计算 $f(\\theta^\\star)$，以获得一个无噪声的预测图像。\n\n- 然后，我们向每个像素添加标准差为 $\\sigma_{\\text{obs}}$ 的独立零均值高斯噪声。接下来，我们通过在图像上均匀选择 $n_{\\text{out}}$ 个随机像素位置，并向这些像素添加一个具有随机符号的固定幅度 $a_{\\text{out}}$，来加入 $n_{\\text{out}}$ 个离群值。这样就产生了观测图像 $y$。\n\n离散搜索与估计：\n\n- 对于每种噪声模型，我们计算离散集合中所有候选参数的失配 $\\mathcal{J}(\\theta)$\n  $$\n  c_x \\in \\{c_x^\\star - 2, c_x^\\star - 1, c_x^\\star, c_x^\\star + 1, c_x^\\star + 2\\},\\quad\n  c_y \\in \\{c_y^\\star - 2, c_y^\\star - 1, c_y^\\star, c_y^\\star + 1, c_y^\\star + 2\\},\\quad\n  r \\in \\{r^\\star - 2, r^\\star - 1, r^\\star, r^\\star + 1, r^\\star + 2\\},\n  $$\n  将 $c_x$ 和 $c_y$ 的值裁剪到有效边界内，并将 $r$ 限制为正整数。我们选择使 $\\mathcal{J}(\\theta)$ 最小化的参数 $\\theta$。\n\n- 然后，我们仅报告每个测试案例中每种噪声模型估计出的半径 $r$。\n\n噪声模型参数：\n\n- 对于高斯失配，设置 $\\sigma = \\sigma_{\\text{obs}}$。\n\n- 对于拉普拉斯失配，设置 $b = \\sigma_{\\text{obs}} / \\sqrt{2}$，这样做的目的是对齐尺度，使得拉普拉斯方差 $2b^2$ 近似匹配高斯方差。\n\n- 对于学生t失配，固定 $\\nu = 3$ 并设置 $\\sigma_t = \\sigma_{\\text{obs}}$。\n\n输出：\n\n- 程序必须以以下格式输出单行结果\n  $$\n  [[r_{1,\\text{G}}, r_{1,\\text{L}}, r_{1,\\text{T}}], [r_{2,\\text{G}}, r_{2,\\text{L}}, r_{2,\\text{T}}], [r_{3,\\text{G}}, r_{3,\\text{L}}, r_{3,\\text{T}}]],\n  $$\n  其中 $r_{i,\\cdot}$ 是表示测试案例 $i \\in \\{1,2,3\\}$ 和高斯(G)、拉普拉斯(L)及学生t(T)噪声模型的估计半径的整数。\n\n该设计测试以下几点：(a) 从其概率密度函数正确推导和实现鲁棒性失配，(b) 正确的水平集正向建模，(c) 在干净、离群值污染和近边界场景下的鲁棒性差异，以及 (d) 在指定格式下的一致性最终结果聚合。", "answer": "```python\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\n\ndef smooth_heaviside(phi, eps):\n    # Smooth Heaviside using tanh\n    return 0.5 * (1.0 + np.tanh(phi / eps))\n\ndef level_set_circle(N, cx, cy, r):\n    # Create level set phi = r - distance to center\n    y_indices, x_indices = np.meshgrid(np.arange(N), np.arange(N), indexing='ij')\n    dist = np.sqrt((x_indices - cx)**2 + (y_indices - cy)**2)\n    phi = r - dist\n    return phi\n\ndef forward_operator(N, theta, eps, psf_sigma):\n    cx, cy, r = theta\n    phi = level_set_circle(N, cx, cy, r)\n    H = smooth_heaviside(phi, eps)\n    pred = gaussian_filter(H, psf_sigma, mode='reflect')\n    return pred\n\ndef generate_observation(N, true_theta, eps, psf_sigma, sigma_obs, n_outliers, outlier_amp, rng):\n    clean = forward_operator(N, true_theta, eps, psf_sigma)\n    noise = rng.normal(0.0, sigma_obs, size=(N, N))\n    obs = clean + noise\n    # Inject sparse outliers with random sign\n    if n_outliers > 0 and outlier_amp > 0.0:\n        total_pixels = N * N\n        idx = rng.choice(total_pixels, size=min(n_outliers, total_pixels), replace=False)\n        signs = rng.choice(np.array([-1.0, 1.0]), size=idx.shape[0])\n        obs.flat[idx] += signs * outlier_amp\n    return obs\n\ndef nll_gaussian(residual, sigma):\n    # Sum of 0.5 * (r/sigma)^2 over pixels\n    return 0.5 * np.sum((residual / sigma) ** 2)\n\ndef nll_laplace(residual, b):\n    # Sum of |r|/b over pixels\n    return np.sum(np.abs(residual) / b)\n\ndef nll_student_t(residual, nu, sigma_t):\n    # Sum of 0.5*(nu+1)*log(1 + r^2/(nu*sigma_t^2)) over pixels\n    # ignoring additive constants\n    return 0.5 * (nu + 1.0) * np.sum(np.log1p((residual ** 2) / (nu * (sigma_t ** 2))))\n\ndef clip_center(N, c):\n    return int(np.clip(c, 0, N - 1))\n\ndef candidate_grid(cx_true, cy_true, r_true, N):\n    cx_candidates = [clip_center(N, cx_true + d) for d in (-2, -1, 0, 1, 2)]\n    cy_candidates = [clip_center(N, cy_true + d) for d in (-2, -1, 0, 1, 2)]\n    r_candidates = [ri for ri in (r_true - 2, r_true - 1, r_true, r_true + 1, r_true + 2) if ri > 0]\n    return cx_candidates, cy_candidates, r_candidates\n\ndef estimate_radius_for_models(N, obs, eps, psf_sigma, cx_true, cy_true, r_true, sigma_obs):\n    cx_cand, cy_cand, r_cand = candidate_grid(cx_true, cy_true, r_true, N)\n    # Misfit parameters\n    sigma_gauss = sigma_obs\n    b_laplace = sigma_obs / np.sqrt(2.0)\n    nu = 3\n    sigma_t = sigma_obs\n\n    best_gauss = (np.inf, None)\n    best_laplace = (np.inf, None)\n    best_student = (np.inf, None)\n\n    # Use a cache for forward model evaluations\n    pred_cache = {}\n\n    for cx in cx_cand:\n        for cy in cy_cand:\n            for r in r_cand:\n                theta = (cx, cy, r)\n                if theta not in pred_cache:\n                    pred_cache[theta] = forward_operator(N, theta, eps, psf_sigma)\n                \n                pred = pred_cache[theta]\n                residual = obs - pred\n\n                jg = nll_gaussian(residual, sigma_gauss)\n                if jg  best_gauss[0]:\n                    best_gauss = (jg, r)\n\n                jl = nll_laplace(residual, b_laplace)\n                if jl  best_laplace[0]:\n                    best_laplace = (jl, r)\n\n                jt = nll_student_t(residual, nu, sigma_t)\n                if jt  best_student[0]:\n                    best_student = (jt, r)\n\n    return [int(best_gauss[1]), int(best_laplace[1]), int(best_student[1])]\n\ndef solve():\n    rng = np.random.default_rng(12345)\n    test_cases = [\n        # Case 1: Baseline, near-ideal\n        {\n            \"N\": 64, \"cx_true\": 32, \"cy_true\": 32, \"r_true\": 12,\n            \"eps\": 1.0, \"psf_sigma\": 1.2,\n            \"sigma_obs\": 0.05, \"n_outliers\": 0, \"outlier_amp\": 0.0\n        },\n        # Case 2: Heavy-tailed contamination\n        {\n            \"N\": 64, \"cx_true\": 32, \"cy_true\": 32, \"r_true\": 12,\n            \"eps\": 1.0, \"psf_sigma\": 1.2,\n            \"sigma_obs\": 0.05, \"n_outliers\": 300, \"outlier_amp\": 0.5\n        },\n        # Case 3: Near-boundary geometry\n        {\n            \"N\": 64, \"cx_true\": 54, \"cy_true\": 10, \"r_true\": 9,\n            \"eps\": 1.0, \"psf_sigma\": 1.2,\n            \"sigma_obs\": 0.04, \"n_outliers\": 40, \"outlier_amp\": 0.3\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        cx_true = case[\"cx_true\"]\n        cy_true = case[\"cy_true\"]\n        r_true = case[\"r_true\"]\n        eps = case[\"eps\"]\n        psf_sigma = case[\"psf_sigma\"]\n        sigma_obs = case[\"sigma_obs\"]\n        n_outliers = case[\"n_outliers\"]\n        outlier_amp = case[\"outlier_amp\"]\n\n        obs = generate_observation(\n            N,\n            (cx_true, cy_true, r_true),\n            eps,\n            psf_sigma,\n            sigma_obs,\n            n_outliers,\n            outlier_amp,\n            rng\n        )\n        est_radii = estimate_radius_for_models(\n            N, obs, eps, psf_sigma, cx_true, cy_true, r_true, sigma_obs\n        )\n        results.append(est_radii)\n\n    # Print single line in the required format\n    # Nested lists with integers\n    def format_nested(lst):\n        return \"[\" + \",\".join(\"[\" + \",\".join(str(x) for x in inner) + \"]\" for inner in lst) + \"]\"\n\n    print(format_nested(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3396667"}]}