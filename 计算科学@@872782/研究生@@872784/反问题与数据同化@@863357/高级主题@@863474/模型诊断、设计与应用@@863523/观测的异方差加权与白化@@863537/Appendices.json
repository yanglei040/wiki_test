{"hands_on_practices": [{"introduction": "白化变换不仅仅是一种数学技巧，它为我们清晰地揭示了观测信息如何被整合到最终的估计中。这项实践将带你深入数据同化的核心——不确定性量化。你将推导并实现计算后验协方差的方法，以验证白化虽然简化了问题形式，但底层的后验不确定性是等价的，并进一步探索改变观测误差协方差$R$如何通过观测算子$J$来重新分配不同参数的不确定性。[@problem_id:3388459]", "problem": "给定一个具有高斯先验和高斯观测误差的线性反演问题。正向模型是从参数向量到观测向量的线性映射。设参数维度为 $n=3$，观测维度为 $m=5$。问题在于量化后验不确定性，并研究观测白化和异方差性（空间变化的观测误差协方差）如何影响后验协方差。\n\n使用的基本原理：\n- 观测模型为 $y = J x + \\varepsilon$，其中 $x \\in \\mathbb{R}^n$ 是参数，$y \\in \\mathbb{R}^m$ 是观测值，$\\varepsilon \\sim \\mathcal{N}(0, R)$，其中 $R \\in \\mathbb{R}^{m \\times m}$ 是对称正定的观测协方差。\n- 先验为 $x \\sim \\mathcal{N}(x_b, \\Gamma)$，其中 $\\Gamma \\in \\mathbb{R}^{n \\times n}$ 是对称正定的先验协方差。\n- 使用高斯条件化定律和线性代数恒等式，根据 $J$、$R$ 和 $\\Gamma$ 推导给定 $y$ 时 $x$ 的后验协方差。\n- 对于白化，使用基于 Cholesky 分解的因式分解：如果 $R = L L^\\top$，$L$ 为下三角矩阵，则定义白化量 $\\tilde{y} = L^{-1} y$ 和 $\\tilde{J} = L^{-1} J$，并在白化空间中推导后验协方差。证明原始公式和白化公式中的后验协方差相等。\n\n你的程序必须：\n1. 实现一种数值稳定的方法，使用原始（未白化）公式计算后验协方差，避免形成任何不必要的矩阵逆。你可以使用三角求解和线性系统求解。\n2. 使用 Cholesky 因子的逆实现白化变换，并计算白化空间中的后验协方差。\n3. 对下述每个测试用例，计算：\n   - 由未白化和白化公式计算出的后验协方差之差的 Frobenius 范数（如果实现正确，该值在数值上应接近于零）。\n   - 后验边际方差向量，由后验协方差矩阵的对角线给出。\n   - 后验协方差矩阵的迹。\n4. 通过比较各测试用例的后验方差，分析异方差观测协方差的影响，并解释空间变化的 $R$ 如何通过观测算子 $J$ 在参数间重新分配不确定性。\n5. 对每个测试用例，输出一个浮点数列表，格式为 $[d, v_1, v_2, v_3, t]$，其中 $d$ 是 Frobenius 范数差，$v_i$ 是后验方差（后验协方差的对角线元素），$i \\in \\{1,2,3\\}$，$t$ 是后验协方差的迹。为确保输出的确定性，将每个浮点数四舍五入到10位小数。\n\n所有测试用例使用以下固定矩阵：\n- 观测算子 $J \\in \\mathbb{R}^{5 \\times 3}$:\n$$\nJ =\n\\begin{bmatrix}\n1.0  0.2  0.0 \\\\\n0.0  1.0  0.3 \\\\\n0.5  0.1  1.0 \\\\\n1.0  -0.5  0.2 \\\\\n0.0  0.7  -0.3\n\\end{bmatrix}.\n$$\n- 先验协方差 $\\Gamma \\in \\mathbb{R}^{3 \\times 3}$（对角矩阵）：\n$$\n\\Gamma = \\mathrm{diag}\\left(1.0, 4.0, 0.25\\right).\n$$\n\n测试套件（$R$ 的四种情况）：\n- 情况 A（同方差，对角矩阵）：\n$$\nR_A = 0.5 \\, I_5,\n$$\n其中 $I_5$ 是 $5 \\times 5$ 的单位矩阵。\n- 情况 B（异方差，对角矩阵）：\n$$\nR_B = \\mathrm{diag}\\left(0.1, 2.0, 0.5, 1.5, 0.3\\right).\n$$\n- 情况 C（强异方差，对角矩阵）：\n$$\nR_C = \\mathrm{diag}\\left(0.01, 10.0, 0.2, 5.0, 0.05\\right).\n$$\n- 情况 D（异方差且相关）：\n构造 $R_D$ 为 $R_D = D^{1/2} \\, C \\, D^{1/2}$，其中\n$$\nD = \\mathrm{diag}\\left(0.5, 1.0, 0.2, 2.0, 0.8\\right),\n$$\n且 $C \\in \\mathbb{R}^{5 \\times 5}$ 的元素为 $C_{ij} = \\rho^{|i-j|}$，其中 $\\rho = 0.3$。\n\n不涉及角度单位。此问题中没有物理单位。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是按上述顺序排列的列表。例如，最后一行应如下所示\n\"[[d_A,v1_A,v2_A,v3_A,t_A],[d_B,v1_B,v2_B,v3_B,t_B],[d_C,v1_C,v2_C,v3_C,t_C],[d_D,v1_D,v2_D,v3_D,t_D]]\"\n每个值都四舍五入到10位小数。", "solution": "用户提供的问题经过严格验证，被确定为**有效**。这是一个贝叶斯反演理论和数据同化领域的适定问题，具有完整且一致的设定。所有提供的数据和定义在科学上都是合理的，并且可以进行数学形式化。\n\n### 理论推导\n\n问题是确定在给定观测向量 $y \\in \\mathbb{R}^m$ 的条件下，参数向量 $x \\in \\mathbb{R}^n$ 的后验协方差。该关系由一个带有加性高斯噪声的线性正向模型和一个关于参数的高斯先验定义。\n\n观测模型为 $y = J x + \\varepsilon$，其中观测误差 $\\varepsilon$ 服从高斯分布 $\\mathcal{N}(0, R)$，其协方差矩阵 $R \\in \\mathbb{R}^{m \\times m}$ 是对称正定（SPD）的。因此，似然函数为 $p(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Jx)^\\top R^{-1} (y - Jx)\\right)$。\n\n参数的先验分布为 $x \\sim \\mathcal{N}(x_b, \\Gamma)$，其均值为 $x_b$，SPD 协方差矩阵为 $\\Gamma \\in \\mathbb{R}^{n \\times n}$。先验概率密度函数为 $p(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_b)^\\top \\Gamma^{-1} (x - x_b)\\right)$。\n\n根据贝叶斯定理，后验分布 $p(x|y)$ 与似然和先验的乘积成正比，$p(x|y) \\propto p(y|x)p(x)$。由于两个分布都是高斯分布，后验分布也是高斯分布，记为 $x|y \\sim \\mathcal{N}(x_a, P_a)$。后验密度与 $\\exp(-\\mathcal{J}(x))$ 成正比，其中 $\\mathcal{J}(x)$ 是代价函数：\n$$\n\\mathcal{J}(x) = \\frac{1}{2} (x - x_b)^\\top \\Gamma^{-1} (x - x_b) + \\frac{1}{2} (y - Jx)^\\top R^{-1} (y - Jx)\n$$\n后验协方差 $P_a$ 是 $\\mathcal{J}(x)$ 关于 $x$ 的 Hessian 矩阵的逆。通过求二阶导数可以找到 Hessian 矩阵：\n$$\n\\nabla_x^2 \\mathcal{J}(x) = \\Gamma^{-1} + J^\\top R^{-1} J\n$$\n因此，后验协方差 $P_a$ 由下式给出：\n$$\nP_a = \\left( \\Gamma^{-1} + J^\\top R^{-1} J \\right)^{-1}\n$$\n\n### 数值稳定的计算（未白化）\n\n直接实现上述公式在数值上可能不稳定且效率低下，尤其是因为涉及矩阵求逆。一个更稳定的流程如下：\n1.  计算 $\\Gamma^{-1}$。由于 $\\Gamma$ 是对角矩阵，这很简单：$(\\Gamma^{-1})_{ii} = 1/\\Gamma_{ii}$。\n2.  为计算 $J^\\top R^{-1} J$ 项，我们避免直接构造 $R^{-1}$。我们首先找到 $R$ 的 Cholesky 分解 $R = LL^\\top$，其中 $L$ 是一个下三角矩阵。\n3.  我们求解矩阵 $Z = R^{-1}J$ 的线性系统。这通过使用 Cholesky 因子分两步完成：\n    a. 通过前向替换求解 $LK = J$ 得到 $K$ ($K=L^{-1}J$)。\n    b. 通过后向替换求解 $L^\\top Z = K$ 得到 $Z$ ($Z=(L^\\top)^{-1}K = (LL^\\top)^{-1}J = R^{-1}J$)。\n4.  计算矩阵乘积 $J^\\top Z$，其结果等于 $J^\\top R^{-1} J$。\n5.  构建 Hessian 矩阵 $H = \\Gamma^{-1} + J^\\top R^{-1} J$。\n6.  通过对小的 $n \\times n$ Hessian 矩阵求逆来计算后验协方差：$P_a = H^{-1}$。当 $n=3$ 时，这在数值上是安全的。\n\n### 白化变换\n\n白化变换简化了观测误差的结构。利用 $R$ 的 Cholesky 因子 $L$，我们通过将观测方程左乘 $L^{-1}$ 来定义白化量：\n$$\nL^{-1}y = L^{-1}Jx + L^{-1}\\varepsilon \\implies \\tilde{y} = \\tilde{J}x + \\tilde{\\varepsilon}\n$$\n这里，$\\tilde{y} = L^{-1}y$ 是白化观测值，$\\tilde{J} = L^{-1}J$ 是白化观测算子，$\\tilde{\\varepsilon} = L^{-1}\\varepsilon$ 是白化误差。白化误差的协方差为：\n$$\n\\tilde{R} = \\mathbb{E}[\\tilde{\\varepsilon}\\tilde{\\varepsilon}^\\top] = L^{-1}\\mathbb{E}[\\varepsilon\\varepsilon^\\top](L^{-1})^\\top = L^{-1}R(L^\\top)^{-1} = L^{-1}(LL^\\top)(L^\\top)^{-1} = I_m\n$$\n白化问题的观测误差协方差等于单位矩阵。将后验协方差公式应用于白化系统，得到：\n$$\nP_{a, \\text{white}} = \\left( \\Gamma^{-1} + \\tilde{J}^\\top \\tilde{R}^{-1} \\tilde{J} \\right)^{-1} = \\left( \\Gamma^{-1} + \\tilde{J}^\\top I_m^{-1} \\tilde{J} \\right)^{-1} = \\left( \\Gamma^{-1} + \\tilde{J}^\\top \\tilde{J} \\right)^{-1}\n$$\n两种公式的等价性通过代入 $\\tilde{J} = L^{-1}J$ 来证明：\n$$\n\\tilde{J}^\\top \\tilde{J} = (L^{-1}J)^\\top(L^{-1}J) = J^\\top(L^{-1})^\\top L^{-1}J = J^\\top(L^\\top)^{-1}L^{-1}J = J^\\top(LL^\\top)^{-1}J = J^\\top R^{-1} J\n$$\n这证明了 Hessian 矩阵是相同的，因此 $P_a = P_{a, \\text{white}}$。通过 Frobenius 范数测量的两种计算结果之间的数值差异，可作为对实现的检验。\n\n### 实现与分析\n\n程序实现了这两种方法。对于由不同观测协方差矩阵 $R$ 定义的每个测试用例，它计算后验协方差 $P_a$。$P_a$ 的对角线元素是参数的后验方差（$v_1, v_2, v_3$），量化了每个参数估计中的不确定性。$P_a$ 的迹是总后验方差。\n- **情况 A（同方差）**提供了一个具有均匀观测误差的基准。\n- **情况 B 和 C（异方差）**具有非均匀的对角矩阵 $R$。方差较小（精度较高）的观测会贡献更多信息。矩阵 $J$ 将这些信息引导至约束特定的参数。例如，在情况 C 中，对应于 $J$ 的第1行和第5行的非常精确的观测（$R_{11}=0.01$, $R_{55}=0.05$）将导致它们敏感的参数的后验方差显著减小。\n- **情况 D（相关）**在 $R$ 中引入了非对角项，模拟了相关的观测误差。白化变换在此处尤其具有洞察力，因为 $L^{-1}$ 混合了 $J$ 的各行，形成了一个对应于不相关“伪观测”的新算子 $\\tilde{J}$。\n比较这些情况下的后验方差和迹，揭示了观测质量和结构（异方差性和相关性）如何影响参数的不确定性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular, toeplitz\n\ndef solve():\n    \"\"\"\n    Computes and analyzes posterior covariance for a linear inverse problem\n    under different observation error covariance structures.\n    \"\"\"\n\n    # Define the fixed matrices for the problem.\n    J = np.array([\n        [1.0, 0.2, 0.0],\n        [0.0, 1.0, 0.3],\n        [0.5, 0.1, 1.0],\n        [1.0, -0.5, 0.2],\n        [0.0, 0.7, -0.3]\n    ], dtype=np.float64)\n\n    Gamma = np.diag([1.0, 4.0, 0.25]).astype(np.float64)\n\n    # Define the test cases for the observation covariance matrix R.\n    # Case A: Homoscedastic, diagonal\n    R_A = 0.5 * np.identity(5, dtype=np.float64)\n\n    # Case B: Heteroscedastic, diagonal\n    R_B = np.diag([0.1, 2.0, 0.5, 1.5, 0.3]).astype(np.float64)\n\n    # Case C: Strongly heteroscedastic, diagonal\n    R_C = np.diag([0.01, 10.0, 0.2, 5.0, 0.05]).astype(np.float64)\n\n    # Case D: Heteroscedastic and correlated\n    D = np.diag([0.5, 1.0, 0.2, 2.0, 0.8]).astype(np.float64)\n    rho = 0.3\n    # First column of the Toeplitz correlation matrix C\n    c_col = np.array([rho**i for i in range(5)], dtype=np.float64)\n    C = toeplitz(c_col)\n    D_sqrt = np.sqrt(D)\n    R_D = D_sqrt @ C @ D_sqrt\n\n    test_cases = [R_A, R_B, R_C, R_D]\n    \n    all_results = []\n\n    for R in test_cases:\n        # --- Unwhitened Formulation ---\n        # P_a = (Gamma_inv + J.T @ R_inv @ J)^-1\n        Gamma_inv = np.diag(1.0 / np.diag(Gamma))\n        \n        # Stably compute J.T @ R_inv @ J\n        L_unwhitened = cholesky(R, lower=True)\n        # Solve R @ Z = J for Z = R_inv @ J using two triangular solves\n        K = solve_triangular(L_unwhitened, J, lower=True)\n        Z = solve_triangular(L_unwhitened.T, K, lower=False)\n        \n        Hessian_unwhitened = Gamma_inv + J.T @ Z\n        Pa_unwhitened = np.linalg.inv(Hessian_unwhitened)\n\n        # --- Whitened Formulation ---\n        # P_a = (Gamma_inv + J_tilde.T @ J_tilde)^-1 where J_tilde = L_inv @ J\n        L_whitened = cholesky(R, lower=True) # Same as cholesky(R, lower=True)\n        # Compute J_tilde by solving L @ J_tilde = J\n        J_tilde = solve_triangular(L_whitened, J, lower=True)\n        \n        Hessian_whitened = Gamma_inv + J_tilde.T @ J_tilde\n        Pa_whitened = np.linalg.inv(Hessian_whitened)\n\n        # --- Compute required output values ---\n        # 1. Frobenius norm of the difference between formulations\n        d = np.linalg.norm(Pa_unwhitened - Pa_whitened, 'fro')\n\n        # 2. Posterior marginal variances (diagonal of posterior covariance)\n        variances = np.diag(Pa_unwhitened)\n        v1, v2, v3 = variances[0], variances[1], variances[2]\n        \n        # 3. Trace of the posterior covariance\n        t = np.trace(Pa_unwhitened)\n        \n        # Collect results for this case. Rounding is done here.\n        case_result = [\n            round(d, 10),\n            round(v1, 10),\n            round(v2, 10),\n            round(v3, 10),\n            round(t, 10)\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string to match the required format exactly.\n    # The string representation of a list includes spaces, which we must remove\n    # by building the string manually.\n    output_parts = []\n    for res_list in all_results:\n        # Creates a string like \"[v1,v2,v3,v4,v5]\"\n        case_str = f\"[{','.join(map(str, res_list))}]\"\n        output_parts.append(case_str)\n    \n    # Joins the case strings into the final format \"[[...],[...],...]\"\n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3388459"}, {"introduction": "从理想情况转向现实世界的挑战，我们有时会遇到不切实际的微小观测误差估计，这会赋予某些数据点过度的影响力，可能导致结果出现偏差。这项练习介绍了一种名为“截断”的实用技术来正则化该问题。你将研究这种“特设”调整带来的后果，权衡在降低后验方差和引入潜在偏差之间的利弊，这是业务化数据同化中的一个关键考量。[@problem_id:3388489]", "problem": "考虑一个线性高斯逆问题，其中未知状态向量 $x \\in \\mathbb{R}^3$ 通过一个已知的观测算子 $H \\in \\mathbb{R}^{5 \\times 3}$ 和加性噪声与观测值 $y \\in \\mathbb{R}^5$ 相关联。假设一个高斯先验 $x \\sim \\mathcal{N}(x_b, B)$，其先验均值为 $x_b \\in \\mathbb{R}^3$，先验协方差为对称正定矩阵 $B \\in \\mathbb{R}^{3 \\times 3}$。观测值满足 $y = H x + \\varepsilon$，其中观测噪声 $\\varepsilon \\sim \\mathcal{N}(0, R)$ 具有一个异方差协方差矩阵 $R \\in \\mathbb{R}^{5 \\times 5}$，该矩阵为对角矩阵，其对角线元素 $\\sigma_i^2$ 随 $i$ 的不同而有很大差异。为避免具有不切实际的小 $\\sigma_i^2$ 的测量值产生过度主导作用，我们通过将 $R$ 替换为 $R_{\\mathrm{clip}}$ 来强制执行削波，其对角线元素定义为 $\\max(\\sigma_i^2, \\sigma_{\\min}^2)$，其中 $\\sigma_{\\min}^2$ 是选定的最小方差阈值。\n\n对于对角观测协方差 $R_{\\mathrm{clip}}$，定义白化变换 $W$ 为 $W = R_{\\mathrm{clip}}^{-1/2}$，得到白化后的数据 $y_{\\mathrm{w}} = W y$ 和白化后的观测算子 $H_{\\mathrm{w}} = W H$，这会产生一个具有单位观测协方差的模型。\n\n你的任务是：\n- 从基本的贝叶斯线性高斯原理出发，不使用任何预先提供的公式，为未削波的异方差情况和削波后的情况推导最大后验 (MAP) 估计的后验均值 $x_a$ 和后验协方差 $A$。\n- 实现白化并验证使用白化模型（单位观测协方差）进行数据同化，与直接使用削波后协方差进行同化所产生的后验结果相同（在数值容差范围内）。\n- 通过为每个削波水平 $\\sigma_{\\min}^2$ 计算以下指标，来量化削波对后验可信区间和潜在偏差的影响：偏差的欧几里得范数 $\\lVert x_a - x^\\ast \\rVert_2$，其中 $x^\\ast$ 是固定的参考真值；$x$ 的每个分量的 $95\\%$ 边际可信区间宽度；以及覆盖计数，定义为 $x^\\ast$ 的分量落在其各自 $95\\%$ 可信区间内的数量。\n\n在所有计算中使用以下固定的、科学上合理的测试配置：\n- 状态维数 $n = 3$ 和观测维数 $m = 5$。\n- 真值状态 $x^\\ast = [2.0, -1.0, 0.5]^\\top$。\n- 先验均值 $x_b = [-0.5, 0.5, 0.0]^\\top$ 和先验协方差 $B = \\mathrm{diag}(4.0, 2.25, 9.0)$。\n- 观测算子\n$$\nH = \\begin{bmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1 \\\\\n1  1  0 \\\\\n0  1  1\n\\end{bmatrix}.\n$$\n- 基线异方差观测方差 $\\sigma^2 = [0.01, 0.09, 4.0, 0.0025, 1.0]$，因此 $R = \\mathrm{diag}(\\sigma^2)$。\n- 确定性观测噪声分量 $\\varepsilon_i$ 通过固定乘数 $[1.0, -0.5, 0.3, 2.0, -0.7]$ 按 $\\sqrt{\\sigma_i^2}$ 缩放，即 $\\varepsilon = [0.1, -0.15, 0.6, 0.1, -0.7]^\\top$，以及观测值 $y = H x^\\ast + \\varepsilon$。\n\n为削波阈值测试集计算后验量\n$$\n\\sigma_{\\min}^2 \\in \\{0.0, 0.04, 0.25, 4.0, 100.0\\}.\n$$\n\n对于测试集中的每个 $\\sigma_{\\min}^2$，报告一个包含以下内容的结果列表：\n- 后验偏差的欧几里得范数 $\\lVert x_a - x^\\ast \\rVert_2$，以浮点数形式表示。\n- $x$ 的每个分量的 $95\\%$ 边际可信区间宽度，以三个浮点数表示，根据后验协方差 $A$ 计算为 $2 \\cdot 1.96 \\cdot \\sqrt{A_{ii}}$，其中 $i = 1,2,3$。\n- 覆盖计数，以整数表示，等于分量 $i$ 满足 $x^\\ast_i$ 位于 $x_{a,i} \\pm 1.96 \\sqrt{A_{ii}}$ 区间内的数量。\n- 一个布尔标志，指示通过白化同化得到的后验是否与通过直接削波同化得到的后验完全匹配（在数值容差范围内），定义为均值和协方差的最大绝对差小于 $10^{-10}$。\n\n最终输出格式：\n你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果，其中每个元素本身都是一个形式为\n$[$bias\\_norm, width\\_1, width\\_2, width\\_3, coverage\\_count, equivalence\\_flag$]$\n的列表，按指定顺序对应每个 $\\sigma_{\\min}^2$。例如，一个有效的输出形状是\n$[[b_1,w_{11},w_{12},w_{13},c_1,e_1],[b_2,w_{21},w_{22},w_{23},c_2,e_2],\\dots]$。", "solution": "我们从具有先验 $x \\sim \\mathcal{N}(x_b, B)$ 和似然 $y \\mid x \\sim \\mathcal{N}(H x, R)$ 的线性高斯逆问题开始，其中 $B \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$R \\in \\mathbb{R}^{m \\times m}$ 是对称正定矩阵。对于高斯分布，Bayes 法则意味着后验分布 $p(x \\mid y)$ 也是高斯的。由于后验分布是高斯的，最大后验 (MAP) 估计与后验均值一致。\n\n推导的基础是联合高斯模型以及负对数后验是一个二次型这一事实。具体来说，负对数后验（不计与 $x$ 无关的加性常数）是先验项和观测项之和：\n$$\nJ(x) = \\tfrac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2} (y - H x)^\\top R^{-1} (y - H x).\n$$\n关于 $x$ 最小化 $J(x)$，通过将梯度设为零，得到一阶最优性条件：\n$$\n\\nabla J(x) = B^{-1} (x - x_b) - H^\\top R^{-1} (y - H x) = 0.\n$$\n重新整理各项，得到线性系统\n$$\n\\left(B^{-1} + H^\\top R^{-1} H\\right) x = B^{-1} x_b + H^\\top R^{-1} y.\n$$\n定义对称正定矩阵\n$$\nS = B^{-1} + H^\\top R^{-1} H.\n$$\n那么后验均值为\n$$\nx_a = S^{-1} \\left( B^{-1} x_b + H^\\top R^{-1} y \\right),\n$$\n且后验协方差为\n$$\nA = S^{-1}.\n$$\n这些表达式可以直接通过对高斯后验的指数部分进行配方，或者通过最小化二次目标函数 $J(x)$ 得到。\n\n现在考虑异方差情况，其中 $R$ 是对角矩阵，其对角线元素 $\\sigma_i^2$ 随 $i$ 而变化。为避免过度主导的测量，我们通过定义 $R_{\\mathrm{clip}}$（其对角线元素为 $\\max(\\sigma_i^2, \\sigma_{\\min}^2)$）来强制执行削波。在上述推导中用 $R_{\\mathrm{clip}}$ 替换 $R$，得到削波后的后验均值和协方差：\n$$\nS_{\\mathrm{clip}} = B^{-1} + H^\\top R_{\\mathrm{clip}}^{-1} H, \\quad\nx_{a,\\mathrm{clip}} = S_{\\mathrm{clip}}^{-1} \\left( B^{-1} x_b + H^\\top R_{\\mathrm{clip}}^{-1} y \\right), \\quad\nA_{\\mathrm{clip}} = S_{\\mathrm{clip}}^{-1}.\n$$\n观测值的白化通过变换 $W = R_{\\mathrm{clip}}^{-1/2}$ 执行，这是良定义的，因为 $R_{\\mathrm{clip}}$ 是具有正对角线元素的对角矩阵。定义白化后的量\n$$\ny_{\\mathrm{w}} = W y, \\quad H_{\\mathrm{w}} = W H.\n$$\n白化后的似然具有单位协方差，即 $y_{\\mathrm{w}} \\mid x \\sim \\mathcal{N}(H_{\\mathrm{w}} x, I)$。相应的负对数后验（不计加性常数）变为\n$$\nJ_{\\mathrm{w}}(x) = \\tfrac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2} \\lVert y_{\\mathrm{w}} - H_{\\mathrm{w}} x \\rVert_2^2,\n$$\n其最小化器满足\n$$\n\\left(B^{-1} + H_{\\mathrm{w}}^\\top H_{\\mathrm{w}}\\right) x = B^{-1} x_b + H_{\\mathrm{w}}^\\top y_{\\mathrm{w}}.\n$$\n代入 $H_{\\mathrm{w}} = W H$ 和 $y_{\\mathrm{w}} = W y$ 得到\n$$\nB^{-1} + H_{\\mathrm{w}}^\\top H_{\\mathrm{w}} = B^{-1} + H^\\top W^\\top W H = B^{-1} + H^\\top R_{\\mathrm{clip}}^{-1} H = S_{\\mathrm{clip}},\n$$\n以及\n$$\nB^{-1} x_b + H_{\\mathrm{w}}^\\top y_{\\mathrm{w}} = B^{-1} x_b + H^\\top W^\\top W y = B^{-1} x_b + H^\\top R_{\\mathrm{clip}}^{-1} y.\n$$\n因此，白化后的后验与削波后的后验相同：\n$$\nx_{a,\\mathrm{w}} = x_{a,\\mathrm{clip}}, \\quad A_{\\mathrm{w}} = A_{\\mathrm{clip}}.\n$$\n\n为了评估可信区间和潜在偏差，请注意每个分量 $x_i$ 的后验边际分布是均值为 $x_{a,i}$、方差为 $A_{ii}$ 的高斯分布。$95\\%$ 边际可信区间为\n$$\n\\left[x_{a,i} - 1.96 \\sqrt{A_{ii}}, \\, x_{a,i} + 1.96 \\sqrt{A_{ii}}\\right],\n$$\n其宽度为\n$$\n2 \\cdot 1.96 \\cdot \\sqrt{A_{ii}}.\n$$\n定义分量 $i$ 的覆盖指示符：如果 $x^\\ast_i$ 位于其可信区间内，则为 $1$，否则为 $0$；覆盖计数则为 $i = 1,2,3$ 的指示符之和。后验偏差范数为 $\\lVert x_a - x^\\ast \\rVert_2$。\n\n对测试集中的每个削波阈值 $\\sigma_{\\min}^2$ 需要实现的算法步骤：\n1. 构造 $R_{\\mathrm{clip}} = \\mathrm{diag}(\\max(\\sigma_i^2, \\sigma_{\\min}^2))$。\n2. 计算 $S_{\\mathrm{clip}} = B^{-1} + H^\\top R_{\\mathrm{clip}}^{-1} H$，然后计算 $A_{\\mathrm{clip}} = S_{\\mathrm{clip}}^{-1}$。\n3. 计算 $x_{a,\\mathrm{clip}} = A_{\\mathrm{clip}} \\left( B^{-1} x_b + H^\\top R_{\\mathrm{clip}}^{-1} y \\right)$。\n4. 计算 $95\\%$ 可信区间宽度 $2 \\cdot 1.96 \\cdot \\sqrt{A_{ii}}$，其中 $i=1,2,3$。\n5. 根据 $x^\\ast_i$ 是否位于其可信区间内，计算每个 $i$ 的覆盖计数。\n6. 计算偏差范数 $\\lVert x_{a,\\mathrm{clip}} - x^\\ast \\rVert_2$。\n7. 通过构造 $W = R_{\\mathrm{clip}}^{-1/2}$、$H_{\\mathrm{w}} = W H$、$y_{\\mathrm{w}} = W y$，然后计算 $S_{\\mathrm{w}} = B^{-1} + H_{\\mathrm{w}}^\\top H_{\\mathrm{w}}$、$A_{\\mathrm{w}} = S_{\\mathrm{w}}^{-1}$ 和 $x_{a,\\mathrm{w}} = A_{\\mathrm{w}} \\left( B^{-1} x_b + H_{\\mathrm{w}}^\\top y_{\\mathrm{w}} \\right)$ 来计算白化后的后验。通过检查均值和协方差的最大绝对差是否小于 $10^{-10}$ 来验证等价性。\n\n将上述步骤应用于指定的确定性测试配置，对于每个 $\\sigma_{\\min}^2$，会产生一个形式为 $[$bias\\_norm, width\\_1, width\\_2, width\\_3, coverage\\_count, equivalence\\_flag$]$ 的结果列表。将这些列表按有序测试集聚合，即可生成所需的单行程序输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef posterior_with_clipping(H, B, x_b, R_base_diag, y, sigma_min_sq, x_true):\n    \"\"\"\n    Compute posterior mean and covariance under clipping, credible widths,\n    coverage count, bias norm, and check equivalence with whitening.\n    \"\"\"\n    # Clip the observation variances.\n    R_diag = np.maximum(R_base_diag, sigma_min_sq)\n    R = np.diag(R_diag)\n\n    # Precompute inverses of diagonal matrices efficiently.\n    B_inv = np.diag(1.0 / np.diag(B))\n    R_inv = np.diag(1.0 / R_diag)\n\n    # Assemble S = B^{-1} + H^T R^{-1} H.\n    S = B_inv + H.T @ R_inv @ H\n\n    # Compute A = S^{-1}.\n    A = np.linalg.inv(S)\n\n    # Compute posterior mean: x_a = A * (B^{-1} x_b + H^T R^{-1} y).\n    rhs = B_inv @ x_b + H.T @ (R_inv @ y)\n    x_a = A @ rhs\n\n    # Credible widths: 2 * 1.96 * sqrt(diag(A)).\n    stds = np.sqrt(np.clip(np.diag(A), 0.0, np.inf))\n    widths = 2.0 * 1.96 * stds\n\n    # Coverage count: number of components where x_true_i in [mean - 1.96*std, mean + 1.96*std].\n    lower = x_a - 1.96 * stds\n    upper = x_a + 1.96 * stds\n    coverage_flags = (x_true >= lower) & (x_true <= upper)\n    coverage_count = int(np.sum(coverage_flags))\n\n    # Bias norm.\n    bias_norm = float(np.linalg.norm(x_a - x_true))\n\n    # Whitening equivalence check.\n    # W = R^{-1/2}; for diagonal R, W is diag(1/sigma_i).\n    sigma = np.sqrt(R_diag)\n    W = np.diag(1.0 / sigma)\n    H_w = W @ H\n    y_w = W @ y\n\n    S_w = B_inv + H_w.T @ H_w\n    A_w = np.linalg.inv(S_w)\n    rhs_w = B_inv @ x_b + H_w.T @ y_w\n    x_a_w = A_w @ rhs_w\n\n    # Check equivalence within tolerance.\n    tol = 1e-10\n    cov_diff = np.max(np.abs(A - A_w))\n    mean_diff = np.max(np.abs(x_a - x_a_w))\n    equivalence_flag = (cov_diff < tol) and (mean_diff < tol)\n\n    return [bias_norm, widths[0], widths[1], widths[2], coverage_count, equivalence_flag]\n\ndef solve():\n    # Define the test configuration as specified in the problem statement.\n    n = 3\n    m = 5\n\n    x_true = np.array([2.0, -1.0, 0.5])\n    x_b = np.array([-0.5, 0.5, 0.0])\n    B = np.diag([4.0, 2.25, 9.0])\n\n    H = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 1.0]\n    ])\n\n    # Baseline heteroscedastic variances and deterministic noise multipliers.\n    R_base_diag = np.array([0.01, 0.09, 4.0, 0.0025, 1.0])\n    # Multipliers applied to sqrt(variance) to form epsilon.\n    multipliers = np.array([1.0, -0.5, 0.3, 2.0, -0.7])\n    epsilon = multipliers * np.sqrt(R_base_diag)\n\n    # Observations y = H x_true + epsilon.\n    y = H @ x_true + epsilon\n\n    # Test suite of clipping thresholds sigma_min^2.\n    test_cases = [0.0, 0.04, 0.25, 4.0, 100.0]\n\n    results = []\n    for sigma_min_sq in test_cases:\n        result = posterior_with_clipping(H, B, x_b, R_base_diag, y, sigma_min_sq, x_true)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3388489"}, {"introduction": "最后的这项实践将超越单个参数的方差，转而审视它们之间的相互依赖关系。当参数在前向模型中耦合时，来自单个观测的信息会影响多个参数的不确定性。本练习使用舒尔补（Schur complement）这一强大的线性代数工具，来分析通过增大观测误差方差来降低某些观测的权重，会如何改变参数之间的后验相关性，从而让你对$R$的结构如何影响参数的可识别性有一个更深刻的理解。[@problem_id:3388480]", "problem": "考虑一个具有高斯先验和异方差观测误差的线性反演问题。状态向量 $x \\in \\mathbb{R}^n$ 通过一个线性映射进行观测，观测值之间相互不相关但方差不等。必须处理观测的白化和适当的加权，以量化对某些观测分量（通过大方差）的降权如何影响耦合参数的可辨识性和后验相关性。您的任务是推导、实现并评估一个基于舒尔补的分析方法，以量化在观测误差协方差的特定扰动下，所选参数之间后验相关性的变化。\n\n从线性高斯贝叶斯反演问题的基本定义以及由先验和似然构造二次代价函数出发，完成以下任务，不要预先假设任何目标公式：\n\n- 仅使用高斯先验和高斯似然的定义，推导以后验精度、观测算子和观测误差协方差表示的后验信息矩阵。证明使用观测误差协方差的逆平方根进行白化不会改变后验，但会改变正规方程，并用白化坐标表示后验信息。\n- 将后验信息矩阵划分为两个块，分别对应于参数的目标子集及其补集。使用对称正定分块矩阵的舒尔补，推导出目标子集的边缘后验协方差的显式表达式。解释为什么无论是在白化坐标还是非白化坐标中计算，这个边缘后验协方差都是不变的。\n- 根据目标参数的边缘后验协方差矩阵的元素，定义两个目标参数之间的后验相关系数，并说明当观测误差协方差发生扰动时，该相关性如何变化。\n\n然后，将您的推导应用于以下具体实例。状态维度为 $n = 3$，观测维度为 $m = 3$。先验均值是任意的，与协方差计算无关。先验协方差矩阵 $B \\in \\mathbb{R}^{3 \\times 3}$ 和观测算子 $H \\in \\mathbb{R}^{3 \\times 3}$ 固定如下：\n- $B = \\begin{bmatrix} 1.0  0.6  0.2 \\\\ 0.6  1.5  0.4 \\\\ 0.2  0.4  1.2 \\end{bmatrix}$，\n- $H = \\begin{bmatrix} 1.0  1.0  0.0 \\\\ 0.0  1.0  1.0 \\\\ 1.0  0.0  1.0 \\end{bmatrix}$。\n\n观测误差协方差 $R \\in \\mathbb{R}^{3 \\times 3}$ 是对角且异方差的，即 $R = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2)$。用 $W = R^{-1/2}$ 表示白化矩阵。参数的目标子集是 $x$ 的前两个分量，记为 $x_a = [x_1,x_2]^T$，其补集为 $x_b = [x_3]$。\n\n定义后验信息矩阵 $J \\in \\mathbb{R}^{3 \\times 3}$，并将其分块为\n$J = \\begin{bmatrix} J_{aa}  J_{ab} \\\\ J_{ba}  J_{bb} \\end{bmatrix}$，其中 $J_{aa} \\in \\mathbb{R}^{2 \\times 2}$，$J_{bb} \\in \\mathbb{R}^{1 \\times 1}$，以及共形的非对角块。利用 $J$ 中 $J_{bb}$ 的舒尔补来获得 $x_a$ 的边缘后验协方差 $S_{aa} \\in \\mathbb{R}^{2 \\times 2}$，并计算 $x_1$ 和 $x_2$ 之间的后验相关系数 $\\rho_{12}$ 作为 $R$ 的函数。\n\n实现一个程序，针对以下每个测试用例，计算在 $R$ 的指定扰动前后的后验相关系数 $\\rho_{12}$，以及定义为扰动后与基线相关性之差的变化量 $\\Delta \\rho_{12}$。所有计算都必须使用上面推导的基于舒尔补的 $S_{aa}$ 表达式来执行。\n\n测试套件：\n- 情况 A (理想路径)：基线 $R_0 = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2)$，其中 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.2, 0.5, 1.0)$。扰动：将第三个方差增加 $\\Delta = 4.0$，即 $R_{\\text{pert}} = \\operatorname{diag}(0.2^2, 0.5^2, 1.0^2 + 4.0)$。\n- 情况 B (边界降权)：基线 $R_0$ 与情况 A 相同。扰动：将第三个方差设置为一个非常大的值，以模拟近乎完全的降权，即 $R_{\\text{pert}} = \\operatorname{diag}(0.2^2, 0.5^2, 10^6)$。\n- 情况 C (边缘信息分量)：基线 $R_0 = \\operatorname{diag}(0.5^2, 0.5^2, 0.5^2)$。扰动：将第二个方差减小到一个信息量很高的水平，即 $R_{\\text{pert}} = \\operatorname{diag}(0.5^2, 0.1^2, 0.5^2)$。\n\n对于每种情况，定义来自 $R_0$ 的 $\\rho_{12}^{\\text{base}}$、来自 $R_{\\text{pert}}$ 的 $\\rho_{12}^{\\text{pert}}$，以及 $\\Delta \\rho_{12} = \\rho_{12}^{\\text{pert}} - \\rho_{12}^{\\text{base}}$。不涉及物理单位。不涉及角度。\n\n最终输出格式：\n- 您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。该列表必须按此顺序包含九个浮点数：\n  - 情况 A：$\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$，\n  - 情况 B：$\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$，\n  - 情况 C：$\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$。\n- 将所有浮点输出四舍五入到六位小数。\n\n您的实现必须是一个完整、可运行的程序，通过线性代数和舒尔补在计算上执行推导，并完全按照指定格式打印最终列表。不需要用户输入。该分析必须普遍适用于任何现代编程语言；对于此任务，您必须用 Python 实现它，并使用适当的线性代数例程确保矩阵求逆的数值稳定性。", "solution": "此问题被评估为有效。它在科学上以贝叶斯反演理论为基础，问题设定良好，具有完整且一致的定义和数据，并且表述客观。它提出了一个非平凡的挑战，需要理论推导和数值实现，直接涉及异方差加权和后验不确定性量化的核心概念。\n\n### 理论推导\n\n#### 1. 后验信息矩阵与白化\n\n在线性贝叶斯反演问题中，状态向量 $x \\in \\mathbb{R}^n$ 通过模型 $d = Hx + \\epsilon$ 与观测值 $d \\in \\mathbb{R}^m$ 相关联，其中 $H$ 是线性观测算子，$\\epsilon$ 是观测误差。\n\n贝叶斯框架将关于 $x$ 的先验知识与来自观测 $d$ 的信息相结合，从而得出 $x$ 的后验概率分布。根据贝叶斯定理，后验概率密度函数 (PDF) $p(x|d)$ 与似然 $p(d|x)$ 和先验 $p(x)$ 的乘积成正比：\n$$p(x|d) \\propto p(d|x) p(x)$$\n\n问题陈述了 $x$ 的高斯先验和高斯观测误差。\n先验分布为 $x \\sim \\mathcal{N}(x_{prior}, B)$，其中 $x_{prior}$ 是先验均值，$B$ 是先验协方差矩阵。其 PDF 为：\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x - x_{prior})^T B^{-1} (x - x_{prior})\\right)$$\n矩阵 $B^{-1}$ 是先验精度矩阵。\n\n观测误差为 $\\epsilon \\sim \\mathcal{N}(0, R)$，其中 $R$ 是观测误差协方差矩阵。对于给定的状态 $x$，观测值 $d$ 的分布因此为 $d \\sim \\mathcal{N}(Hx, R)$。似然函数为：\n$$p(d|x) \\propto \\exp\\left(-\\frac{1}{2}(d - Hx)^T R^{-1} (d - Hx)\\right)$$\n矩阵 $R^{-1}$ 是观测误差精度矩阵。\n\n结合先验和似然，后验 PDF 为：\n$$p(x|d) \\propto \\exp\\left(-\\frac{1}{2}\\left[ (x - x_{prior})^T B^{-1} (x - x_{prior}) + (d - Hx)^T R^{-1} (d - Hx) \\right]\\right)$$\n指数中的表达式是 $x$ 的二次函数。这意味着后验分布也是高斯的，$x|d \\sim \\mathcal{N}(x_{post}, P)$，其中 $x_{post}$ 是后验均值，$P$ 是后验协方差。后验 PDF 可以写成：\n$$p(x|d) \\propto \\exp\\left(-\\frac{1}{2}(x - x_{post})^T P^{-1} (x - x_{post})\\right)$$\n矩阵 $J = P^{-1}$ 是后验信息矩阵，也称为后验精度矩阵。它是负对数后验（也称为代价函数 $\\mathcal{J}(x) = \\frac{1}{2}(x - x_{prior})^T B^{-1} (x - x_{prior}) + \\frac{1}{2}(d - Hx)^T R^{-1} (d - Hx)$）关于 $x$ 的Hessian矩阵。\n\n为了找到Hessian矩阵，我们计算 $\\mathcal{J}(x)$ 关于 $x$ 的二阶导数。\n$$\\nabla_x \\mathcal{J}(x) = B^{-1}(x - x_{prior}) - H^T R^{-1} (d - Hx)$$\n$$\\nabla_x^2 \\mathcal{J}(x) = B^{-1} + H^T R^{-1} H$$\n因此，后验信息矩阵 $J$ 是：\n$$J = B^{-1} + H^T R^{-1} H$$\n\n现在，考虑白化的影响。白化使用矩阵 $W = R^{-1/2}$（即（通常为对角的）观测误差协方差 $R$ 的逆矩阵平方根）来变换观测值和观测算子。变换后的观测值为 $d' = Wd$，变换后的算子为 $H' = WH$。变换后的观测误差 $\\epsilon' = W\\epsilon$ 的协方差为 $E[\\epsilon'(\\epsilon')^T] = W E[\\epsilon \\epsilon^T] W^T = W R W^T = R^{-1/2} R R^{-1/2} = I$，即单位矩阵。\n\n代价函数中的似然项，用白化坐标表示为 $\\frac{1}{2}(d' - H'x)^T (d' - H'x)$。代入 $d'$ 和 $H'$ 的定义：\n$$\\frac{1}{2}(Wd - WHx)^T (Wd - WHx) = \\frac{1}{2}(W(d - Hx))^T (W(d - Hx)) = \\frac{1}{2}(d - Hx)^T W^T W (d - Hx)$$\n由于 $R$ 是对称的， $W=R^{-1/2}$ 也是对称的。因此，$W^T = W$，且 $W^T W = W^2 = (R^{-1/2})^2 = R^{-1}$。因此，似然项为 $\\frac{1}{2}(d-Hx)^T R^{-1} (d-Hx)$，这与非白化形式相同。因为代价函数 $\\mathcal{J}(x)$ 保持不变，所以后验分布 $p(x|d)$ 也保持不变。\n\n然而，通过设置 $\\nabla_x \\mathcal{J}(x) = 0$ 以求得后验均值的正规方程，其形式发生了变化。\n在非白化坐标中：\n$$(B^{-1} + H^T R^{-1} H)x_{post} = B^{-1}x_{prior} + H^T R^{-1} d$$\n在白化坐标中，代价函数写为 $\\mathcal{J}(x) = \\frac{1}{2}(x - x_{prior})^T B^{-1} (x - x_{prior}) + \\frac{1}{2}(d' - H'x)^T (d' - H'x)$。其梯度为 $\\nabla_x \\mathcal{J}(x) = B^{-1}(x - x_{prior}) - (H')^T(d' - H'x)$。令其为零可得：\n$$(B^{-1} + (H')^T H')x_{post} = B^{-1}x_{prior} + (H')^T d'$$\n尽管最终解 $x_{post}$ 是相同的，但线性系统的矩阵和向量分量是由不同的量（$H, R, d$ vs. $H', d'$）构造的。\n\n#### 2. 通过舒尔补计算边缘后验协方差\n\n将状态向量 $x$ 分块为目标子集 $x_a$ 及其补集 $x_b$，使得 $x = \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix}$。后验信息矩阵 $J$ 进行共形分块：\n$$J = \\begin{bmatrix} J_{aa}  J_{ab} \\\\ J_{ba}  J_{bb} \\end{bmatrix}$$\n后验协方差矩阵 $P = J^{-1}$ 也进行分块：\n$$P = \\begin{bmatrix} P_{aa}  P_{ab} \\\\ P_{ba}  P_{bb} \\end{bmatrix}$$\n块 $P_{aa}$ 表示目标子集 $x_a$ 的边缘后验协方差。利用对称分块矩阵的求逆公式，$P_{aa}$ 可以用 $J$ 的块来表示：\n$$P_{aa} = (J_{aa} - J_{ab} J_{bb}^{-1} J_{ba})^{-1}$$\n矩阵 $S = J_{aa} - J_{ab} J_{bb}^{-1} J_{ba}$ 是矩阵 $J$ 中块 $J_{bb}$ 的舒尔补。因此，目标参数 $x_a$ 的边缘后验协方差（问题中记为 $S_{aa}$）是这个舒尔补的逆。\n$$S_{aa} \\equiv P_{aa} = (J_{aa} - J_{ab} J_{bb}^{-1} J_{ba})^{-1}$$\n无论是在白化坐标还是非白化坐标中计算，该边缘协方差都是不变的。如前一节所述，后验信息矩阵 $J$ 本身对白化是不变的。由于 $J$ 在两种坐标系中是相同的，它的子块（$J_{aa}, J_{ab}, J_{ba}, J_{bb}$）也相同。因此，从这些块导出的任何量，包括舒尔补及其逆（$S_{aa}$），也必须是不变的。\n\n#### 3. 后验相关系数\n\n给定目标参数 $x_a = [x_1, x_2]^T$ 的边缘后验协方差矩阵 $S_{aa} \\in \\mathbb{R}^{2 \\times 2}$：\n$$S_{aa} = \\begin{bmatrix} (S_{aa})_{11}  (S_{aa})_{12} \\\\ (S_{aa})_{21}  (S_{aa})_{22} \\end{bmatrix}$$\n对角元素 $(S_{aa})_{11}$ 和 $(S_{aa})_{22}$ 分别是 $x_1$ 和 $x_2$ 的边缘后验方差。非对角元素 $(S_{aa})_{12}$（由于 $S_{aa}$ 是对称的，等于 $(S_{aa})_{21}$）是 $x_1$ 和 $x_2$ 之间的边缘后验协方差。\n\n$x_1$ 和 $x_2$ 之间的后验相关系数 $\\rho_{12}$ 定义为归一化协方差：\n$$\\rho_{12} = \\frac{(S_{aa})_{12}}{\\sqrt{(S_{aa})_{11} (S_{aa})_{22}}}$$\n当观测误差协方差 $R$ 发生扰动时，例如从 $R_0$ 变为 $R_{\\text{pert}}$，后验信息矩阵 $J$ 根据公式 $J = B^{-1} + H^T R^{-1} H$ 发生变化。这种变化会传播到 $J$ 的分块，从而改变舒尔补、边缘后验协方差 $S_{aa}$，并最终改变后验相关系数 $\\rho_{12}$ 的值。相关性的变化通过使用 $R_0$ 和 $R_{\\text{pert}}$ 分别计算 $\\rho_{12}$ 的表达式并求其差值来得到。\n\n### 具体实例应用\n\n问题提供了以下信息：\n$n=3$, $m=3$。目标子集 $x_a = [x_1, x_2]^T$，补集 $x_b = [x_3]$。\n先验协方差 $B = \\begin{bmatrix} 1.0  0.6  0.2 \\\\ 0.6  1.5  0.4 \\\\ 0.2  0.4  1.2 \\end{bmatrix}$。\n观测算子 $H = \\begin{bmatrix} 1.0  1.0  0.0 \\\\ 0.0  1.0  1.0 \\\\ 1.0  0.0  1.0 \\end{bmatrix}$。\n观测误差协方差 $R = \\operatorname{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2)$。\n\n计算流程如下：\n1.  给定 $R$，计算其逆 $R^{-1} = \\operatorname{diag}(1/\\sigma_1^2, 1/\\sigma_2^2, 1/\\sigma_3^2)$。\n2.  计算先验精度 $B^{-1}$。\n3.  计算后验信息矩阵 $J = B^{-1} + H^T R^{-1} H$。\n4.  将 $J$ 分块为对应于 $x_a$ 和 $x_b$ 的块：$J_{aa} \\in \\mathbb{R}^{2 \\times 2}$, $J_{ab} \\in \\mathbb{R}^{2 \\times 1}$, $J_{ba} \\in \\mathbb{R}^{1 \\times 2}$ 和 $J_{bb} \\in \\mathbb{R}^{1 \\times 1}$。\n5.  计算 $J_{bb}$ 的舒尔补：$S = J_{aa} - J_{ab} J_{bb}^{-1} J_{ba}$。由于 $J_{bb}$ 是一个标量，其逆就是其倒数。\n6.  计算 $x_a$ 的边缘后验协方差：$S_{aa} = S^{-1}$。\n7.  根据 $S_{aa}$ 的元素，使用公式 $\\rho_{12} = (S_{aa})_{12} / \\sqrt{(S_{aa})_{11}(S_{aa})_{22}}$ 计算后验相关系数 $\\rho_{12}$。\n\n对每个测试用例实施此流程，以找到基线和扰动后的相关系数及其差值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inverse problem to calculate posterior correlations.\n    \"\"\"\n\n    # --- Given Fixed Parameters ---\n    # Prior covariance matrix B\n    B = np.array([\n        [1.0, 0.6, 0.2],\n        [0.6, 1.5, 0.4],\n        [0.2, 0.4, 1.2]\n    ])\n\n    # Observation operator H\n    H = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.0, 1.0]\n    ])\n\n    # Pre-compute the inverse of the prior covariance matrix (prior precision)\n    try:\n        B_inv = np.linalg.inv(B)\n    except np.linalg.LinAlgError:\n        # This case should not be reached with the given B\n        print(\"Error: Prior covariance matrix B is singular.\")\n        return\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        # Case A: Happy path\n        {\n            \"name\": \"Case A\",\n            \"R0_variances\": np.array([0.2**2, 0.5**2, 1.0**2]),\n            \"Rpert_variances\": np.array([0.2**2, 0.5**2, 1.0**2 + 4.0])\n        },\n        # Case B: Boundary downweighting\n        {\n            \"name\": \"Case B\",\n            \"R0_variances\": np.array([0.2**2, 0.5**2, 1.0**2]),\n            \"Rpert_variances\": np.array([0.2**2, 0.5**2, 10.0**6])\n        },\n        # Case C: Edge informative component\n        {\n            \"name\": \"Case C\",\n            \"R0_variances\": np.array([0.5**2, 0.5**2, 0.5**2]),\n            \"Rpert_variances\": np.array([0.5**2, 0.1**2, 0.5**2])\n        }\n    ]\n\n    def compute_posterior_correlation(R_variances, B_inv_matrix, H_matrix):\n        \"\"\"\n        Computes the posterior correlation rho_12 for a given R.\n\n        Args:\n            R_variances (np.ndarray): Diagonal elements (variances) of R.\n            B_inv_matrix (np.ndarray): The inverse of the prior covariance matrix B.\n            H_matrix (np.ndarray): The observation operator H.\n\n        Returns:\n            float: The posterior correlation coefficient rho_12.\n        \"\"\"\n        # 1. Observation error precision R_inv\n        # R is diagonal, so its inverse is a diagonal matrix of reciprocal variances.\n        R_inv_diag = 1.0 / R_variances\n        R_inv = np.diag(R_inv_diag)\n\n        # 2. Posterior information matrix J\n        # J = B_inv + H^T * R_inv * H\n        J = B_inv_matrix + H_matrix.T @ R_inv @ H_matrix\n\n        # 3. Partition J\n        # The state is x = [x1, x2, x3]^T.\n        # Target subset xa = [x1, x2]^T. Complement xb = [x3].\n        # Partition is after the 2nd row/col.\n        J_aa = J[0:2, 0:2]  # 2x2\n        J_ab = J[0:2, 2:3]  # 2x1\n        J_ba = J[2:3, 0:2]  # 1x2\n        J_bb = J[2:3, 2:3]  # 1x1\n\n        # 4. Compute Schur complement of J_bb in J\n        # S = J_aa - J_ab * J_bb^-1 * J_ba\n        # J_bb is 1x1, so its inverse is just the reciprocal of its element.\n        J_bb_inv_scalar = 1.0 / J_bb[0, 0]\n        schur_complement = J_aa - (J_ab * J_bb_inv_scalar) @ J_ba\n\n        # 5. Compute marginal posterior covariance S_aa\n        # S_aa is the inverse of the Schur complement.\n        try:\n            S_aa = np.linalg.inv(schur_complement)\n        except np.linalg.LinAlgError:\n            # Should not happen with well-posed problems\n            return np.nan\n\n        # 6. Compute posterior correlation coefficient rho_12\n        # rho_12 = S_aa(1,2) / sqrt(S_aa(1,1) * S_aa(2,2))\n        S_11 = S_aa[0, 0]\n        S_22 = S_aa[1, 1]\n        S_12 = S_aa[0, 1]\n        \n        if S_11 <= 0 or S_22 <= 0: # Variances must be positive\n             return np.nan\n             \n        rho_12 = S_12 / np.sqrt(S_11 * S_22)\n        return rho_12\n\n    results = []\n    for case in test_cases:\n        # Compute baseline correlation\n        rho_base = compute_posterior_correlation(case[\"R0_variances\"], B_inv, H)\n        \n        # Compute perturbed correlation\n        rho_pert = compute_posterior_correlation(case[\"Rpert_variances\"], B_inv, H)\n        \n        # Compute the change\n        delta_rho = rho_pert - rho_base\n        \n        # Append formatted results\n        results.extend([f\"{rho_base:.6f}\", f\"{rho_pert:.6f}\", f\"{delta_rho:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3388480"}]}