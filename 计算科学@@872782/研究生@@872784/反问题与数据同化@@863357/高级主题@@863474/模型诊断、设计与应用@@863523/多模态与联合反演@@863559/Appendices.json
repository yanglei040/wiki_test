{"hands_on_practices": [{"introduction": "联合反演中的一个核心挑战是如何恰当地组合来自不同来源的数据。一种常见的方法是构建一个联合目标函数，该函数是各个数据模态的失配项的加权和。这个练习旨在阐明，这些权重并非随意设定，而是深深植根于数据误差的统计特性。通过一个具体的计算，我们将看到数据误差协方差如何决定性地影响每种数据模态对总目标函数的贡献，这正是基于统计的反演问题的基石 [@problem_id:3404765]。", "problem": "考虑一个联合反演问题，其包含两种由 $i \\in \\{1,2\\}$ 索引的数据模态。正演映射为 $F_1(m)$ 和 $F_2(m)$，与线性观测算子 $H_1$ 和 $H_2$ 复合，得到残差 $r_i(m) = H_i F_i(m) - d_i$。假设每种模态都存在加性、零均值的高斯数据误差，其协方差矩阵分别为 $C_1$ 和 $C_2$，且均为已知正定矩阵。在此假设下，联合目标函数的数据失配部分是根据高斯误差模型所蕴含的适当协方差加权来构建的。\n\n在一个特定的候选模型 $m^\\star$ 处，假设协方差为 $C_1 = 9\\,I$ 和 $C_2 = I$，其中 $I$ 表示维度兼容的单位矩阵，且欧几里得残差范数分别为 $\\|r_1(m^\\star)\\|_2 = 3$ 和 $\\|r_2(m^\\star)\\|_2 = 1$。使用基于高斯噪声的数据失配目标函数构建方法，并在各模态间使用一个公共的标量前置因子，计算在 $m^\\star$ 处每种模态对总数据失配的相对贡献，即分数 $J_1(m^\\star)/J_{\\mathrm{data}}(m^\\star)$ 和 $J_2(m^\\star)/J_{\\mathrm{data}}(m^\\star)$，其中 $J_{\\mathrm{data}}(m^\\star) = J_1(m^\\star) + J_2(m^\\star)$ 表示总数据失配。\n\n将你的最终答案以一个双元素行向量的形式报告，按顺序包含这两个相对贡献。无需单位。无需四舍五入。", "solution": "该问题要求计算在特定候选模型 $m^\\star$ 处，两种数据模态对总数据失配目标函数的相对贡献。目标函数的构建基于存在加性、零均值、协方差矩阵已知的高斯误差的假设。\n\n对于给定的模型 $m$，模态 $i$ 的数据 $d_i$ 的概率密度函数基于误差项的高斯分布，该误差项等同于残差 $r_i(m) = H_i F_i(m) - d_i$。模态 $i$ 的似然函数由下式给出：\n$$L_i(m) \\propto \\exp\\left(-\\frac{1}{2} r_i(m)^T C_i^{-1} r_i(m)\\right)$$\n其中 $C_i$ 是数据误差协方差矩阵。\n\n对于一个具有两种统计独立数据模态的联合反演问题，总似然是个体似然的乘积：\n$$L_{\\mathrm{total}}(m) = L_1(m) \\cdot L_2(m) \\propto \\exp\\left(-\\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m) - \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)\\right)$$\n\n数据失配目标函数 $J_{\\mathrm{data}}(m)$ 通常通过取负对数似然得到，并忽略任何不依赖于模型 $m$ 的常数项。这会产生一个二次型之和：\n$$J_{\\mathrm{data}}(m) = \\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m) + \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)$$\n问题陈述中提到“一个公共的标量前置因子”，这与两项中都出现的因子 $\\frac{1}{2}$ 是一致的。我们可以将单个失配贡献定义为：\n$$J_1(m) = \\frac{1}{2} r_1(m)^T C_1^{-1} r_1(m)$$\n$$J_2(m) = \\frac{1}{2} r_2(m)^T C_2^{-1} r_2(m)$$\n使得 $J_{\\mathrm{data}}(m) = J_1(m) + J_2(m)$。\n\n我们需要在特定模型 $m^\\star$ 处评估这些贡献。用于此评估的已知条件是：\n- 模态 1 的协方差：$C_1 = 9\\,I$\n- 模态 2 的协方差：$C_2 = I$\n- 模态 1 的残差范数：$\\|r_1(m^\\star)\\|_2 = 3$\n- 模态 2 的残差范数：$\\|r_2(m^\\star)\\|_2 = 1$\n\n首先，我们求逆协方差矩阵：\n$$C_1^{-1} = (9\\,I)^{-1} = \\frac{1}{9}I^{-1} = \\frac{1}{9}I$$\n$$C_2^{-1} = I^{-1} = I$$\n\n现在，我们计算 $J_1(m^\\star)$ 的值。我们将 $C_1^{-1}$ 代入 $J_1(m)$ 的表达式中：\n$$J_1(m^\\star) = \\frac{1}{2} r_1(m^\\star)^T \\left(\\frac{1}{9}I\\right) r_1(m^\\star)$$\n$r_1(m^\\star)^T I r_1(m^\\star)$ 项等价于点积 $r_1(m^\\star) \\cdot r_1(m^\\star)$，也就是欧几里得范数的平方 $\\|r_1(m^\\star)\\|_2^2$。\n$$J_1(m^\\star) = \\frac{1}{2} \\cdot \\frac{1}{9} \\|r_1(m^\\star)\\|_2^2 = \\frac{1}{18} \\|r_1(m^\\star)\\|_2^2$$\n代入给定值 $\\|r_1(m^\\star)\\|_2 = 3$：\n$$J_1(m^\\star) = \\frac{1}{18} (3^2) = \\frac{9}{18} = \\frac{1}{2}$$\n\n接下来，我们计算 $J_2(m^\\star)$ 的值。我们将 $C_2^{-1}$ 代入 $J_2(m)$ 的表达式中：\n$$J_2(m^\\star) = \\frac{1}{2} r_2(m^\\star)^T (I) r_2(m^\\star)$$\n这可以简化为：\n$$J_2(m^\\star) = \\frac{1}{2} \\|r_2(m^\\star)\\|_2^2$$\n代入给定值 $\\|r_2(m^\\star)\\|_2 = 1$：\n$$J_2(m^\\star) = \\frac{1}{2} (1^2) = \\frac{1}{2}$$\n\n在 $m^\\star$ 处的总数据失配是各项贡献之和：\n$$J_{\\mathrm{data}}(m^\\star) = J_1(m^\\star) + J_2(m^\\star) = \\frac{1}{2} + \\frac{1}{2} = 1$$\n\n最后，我们计算每种模态对总数据失配的相对贡献。\n模态 1 的相对贡献是：\n$$\\frac{J_1(m^\\star)}{J_{\\mathrm{data}}(m^\\star)} = \\frac{1/2}{1} = \\frac{1}{2}$$\n模态 2 的相对贡献是：\n$$\\frac{J_2(m^\\star)}{J_{\\mathrm{data}}(m^\\star)} = \\frac{1/2}{1} = \\frac{1}{2}$$\n\n问题要求将答案表示为一个双元素行向量，按顺序包含这两个分数。这个结果展示了逆协方差加权原理：模态 1 的原始残差较大，但由于其相关的数据误差具有更大的方差（$9$ 对比 $1$），因此其对目标函数的贡献被相应地降低了权重，从而导致在 $m^\\star$ 点处两种模态的贡献相等。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{2} & \\frac{1}{2} \\end{pmatrix}}$$", "id": "3404765"}, {"introduction": "一旦定义了联合目标函数，我们通常会采用基于梯度的优化算法来寻找能够最小化该函数的模型。对于复杂的大规模反演问题，高效地计算目标函数相对于模型参数的梯度至关重要。此练习将引导您使用伴随状态法——这是计算此类梯度的标准且最高效的方法——来推导一个通用的多模态目标函数的梯度 [@problem_id:3404713]。这个推导过程揭示了梯度是如何由每个数据模态的“伴随源”贡献相加而成，为实际的优化计算提供了理论基础。", "problem": "考虑一个具有多种数据模态的联合反演设定，其中模态由 $k \\in \\{1,\\dots,K\\}$ 索引。令 $\\mathcal{M}$ 是一个由模型参数 $m \\in \\mathcal{M}$ 组成的希尔伯特空间。对于每个模态 $k$，令 $\\mathcal{U}_k$ 表示正演模型状态的希尔伯特空间，$\\mathcal{Y}_k$ 表示数据的希尔伯特空间。假设：\n- 一个可微的正演映射 $F_k:\\mathcal{M} \\to \\mathcal{U}_k$，其 Fréchet 导数为 $F_k^{\\prime}(m):\\mathcal{M} \\to \\mathcal{U}_k$。\n- 一个有界线性观测映射 $H_k:\\mathcal{U}_k \\to \\mathcal{Y}_k$，其希尔伯特伴随为 $H_k^{*}:\\mathcal{Y}_k \\to \\mathcal{U}_k$。\n- 一个对称正定的数据失配权重（噪声协方差）$C_k:\\mathcal{Y}_k \\to \\mathcal{Y}_k$，其中 $C_k^{-1}$ 和 $C_k^{-1/2}$ 是良定义且有界的。\n- 观测数据 $d_k \\in \\mathcal{Y}_k$。\n\n定义联合目标泛函为\n$$\nJ(m) \\;=\\; \\frac{1}{2} \\sum_{k=1}^{K} \\big\\|\\, C_k^{-1/2} \\big(H_k F_k(m) - d_k\\big) \\big\\|_{\\mathcal{Y}_k}^{2}.\n$$\n假设所有空间都是具有典范内积的实希尔伯特空间，并且伴随是关于这些内积定义的。$J$ 沿方向 $\\delta m \\in \\mathcal{M}$ 的 Gâteaux 导数定义为\n$$\n\\delta J[m;\\delta m] \\;=\\; \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, J(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0}.\n$$\n\n严格地从 Gâteaux 导数的定义、Fréchet 导数的链式法则、希尔伯特空间伴随的性质以及关联范数和内积的极化恒等式出发，推导 Gâteaux 导数 $\\delta J[m;\\delta m]$ 的一个伴随状态表达式，该表达式将 $\\delta m$ 分离到 $\\mathcal{M}$ 上的单个内积中。在您的推导中，用残差和给定的算子明确指出每个模态的伴随源。您最终的 $\\delta J[m;\\delta m]$ 表达式必须仅用 $F_k^{\\prime}(m)^{*}$、 $H_k^{*}$、 $C_k^{-1}$、 $H_kF_k(m)$ 和 $d_k$ 来表示，并且所有算子和残差都清晰地组合成 $\\mathcal{M}$ 上的单个内积。\n\n您的最终答案必须是 $\\delta J[m;\\delta m]$ 的单个闭式解析表达式。无需进行数值计算。", "solution": "问题陈述在反演问题领域内提供了一个良定的数学问题。所有术语都有明确定义，前提在科学上和数学上都是合理的，目标是使用所提供的定义和性质进行标准推导。因此，该问题是有效的。\n\n我们的任务是推导联合目标泛函 $J(m)$ 的 Gâteaux 导数的伴随状态表达式。该泛函由下式给出\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\|\\, C_k^{-1/2} \\big(H_k F_k(m) - d_k\\big) \\big\\|_{\\mathcal{Y}_k}^{2}\n$$\n其中 $m \\in \\mathcal{M}$ 是模型参数，对于每个模态 $k$，$F_k$ 是正演模型，$H_k$ 是观测算子，$d_k \\in \\mathcal{Y}_k$ 是观测数据，$C_k$ 是对称正定的数据失配权重，而 $\\mathcal{M}$ 和 $\\mathcal{Y}_k$ 是实希尔伯特空间。\n\n首先，我们使用希尔伯特空间 $\\mathcal{Y}_k$ 上的内积来表示范数平方项。对于任意向量 $y \\in \\mathcal{Y}_k$，范数定义为 $\\|y\\|_{\\mathcal{Y}_k}^2 = \\langle y, y \\rangle_{\\mathcal{Y}_k}$。\n将此应用于目标泛函，得到：\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\langle C_k^{-1/2} (H_k F_k(m) - d_k), C_k^{-1/2} (H_k F_k(m) - d_k) \\big\\rangle_{\\mathcal{Y}_k}\n$$\n算子 $C_k$ 是对称正定的，这意味着它的逆 $C_k^{-1}$ 和平方根 $C_k^{-1/2}$ 也是对称正定的，因而在实希尔伯特空间 $\\mathcal{Y}_k$ 中是自伴的。也就是说，$(C_k^{-1/2})^{*} = C_k^{-1/2}$。使用伴随算子的定义 $\\langle u, Av \\rangle = \\langle A^*u, v \\rangle$，我们可以将一个 $C_k^{-1/2}$ 算子移动到内积的第一个参数上：\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\langle (C_k^{-1/2})^{*} C_k^{-1/2} (H_k F_k(m) - d_k), (H_k F_k(m) - d_k) \\big\\rangle_{\\mathcal{Y}_k}\n$$\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\big\\langle C_k^{-1} (H_k F_k(m) - d_k), (H_k F_k(m) - d_k) \\big\\rangle_{\\mathcal{Y}_k}\n$$\n为方便起见，令模态 $k$ 的数据残差表示为 $r_k(m) = H_k F_k(m) - d_k$。泛函变为：\n$$\nJ(m) = \\frac{1}{2} \\sum_{k=1}^{K} \\langle C_k^{-1} r_k(m), r_k(m) \\rangle_{\\mathcal{Y}_k}\n$$\n\n$J$ 在 $m$ 点沿方向 $\\delta m \\in \\mathcal{M}$ 的 Gâteaux 导数定义为：\n$$\n\\delta J[m;\\delta m] = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, J(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0}\n$$\n代入 $J$ 的内积形式：\n$$\n\\delta J[m;\\delta m] = \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, \\left( \\frac{1}{2} \\sum_{k=1}^{K} \\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} \\right) \\right|_{\\epsilon=0}\n$$\n由于导数的线性和有限和，我们可以交换它们的顺序：\n$$\n\\delta J[m;\\delta m] = \\frac{1}{2} \\sum_{k=1}^{K} \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon}\\, \\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} \\right|_{\\epsilon=0}\n$$\n我们应用微分内积 $\\langle u(\\epsilon), v(\\epsilon) \\rangle$ 的乘法法则：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\langle u, v \\rangle = \\langle \\frac{\\mathrm{d}u}{\\mathrm{d}\\epsilon}, v \\rangle + \\langle u, \\frac{\\mathrm{d}v}{\\mathrm{d}\\epsilon} \\rangle\n$$\n这里，$u(\\epsilon) = C_k^{-1} r_k(m+\\epsilon\\,\\delta m)$ 且 $v(\\epsilon) = r_k(m+\\epsilon\\,\\delta m)$。算子 $C_k^{-1}$ 关于 $\\epsilon$ 是常数。令 $r_k'(\\epsilon)$ 表示 $\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)$。内积项的导数是：\n$$\n\\langle C_k^{-1} r_k'(\\epsilon), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} + \\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k'(\\epsilon) \\rangle_{\\mathcal{Y}_k}\n$$\n由于我们处于实希尔伯特空间中，内积是对称的（$\\langle a,b \\rangle = \\langle b,a \\rangle$）。此外，$C_k^{-1}$ 是自伴的。因此，第二项可以重写为：\n$$\n\\langle C_k^{-1} r_k(m+\\epsilon\\,\\delta m), r_k'(\\epsilon) \\rangle_{\\mathcal{Y}_k} = \\langle r_k'(\\epsilon), (C_k^{-1})^{*} r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} = \\langle r_k'(\\epsilon), C_k^{-1} r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k}\n$$\n这两项是相同的。因此它们的和是：\n$$\n2 \\langle C_k^{-1} r_k'(\\epsilon), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k}\n$$\n将此代回到 $\\delta J[m;\\delta m]$ 的表达式中，得到：\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\left. \\langle C_k^{-1} \\left(\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)\\right), r_k(m+\\epsilon\\,\\delta m) \\rangle_{\\mathcal{Y}_k} \\right|_{\\epsilon=0}\n$$\n在 $\\epsilon=0$ 处求值：\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle C_k^{-1} \\left( \\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0} \\right), r_k(m) \\rangle_{\\mathcal{Y}_k}\n$$\n接下来，我们计算残差 $r_k(m+\\epsilon\\,\\delta m) = H_k F_k(m+\\epsilon\\,\\delta m) - d_k$ 的导数。使用 Fréchet 导数的链式法则，并注意到 $H_k$ 是有界线性算子且 $d_k$ 是常数：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m) = H_k \\left( \\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} F_k(m+\\epsilon\\,\\delta m) \\right) = H_k \\left( F_k^{\\prime}(m+\\epsilon\\,\\delta m)[\\delta m] \\right)\n$$\n在 $\\epsilon=0$ 处求值，我们得到复合正演算子 $H_k F_k$ 的 Gâteaux 导数：\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} r_k(m+\\epsilon\\,\\delta m)\\right|_{\\epsilon=0} = H_k (F_k^{\\prime}(m)[\\delta m])\n$$\n将此结果代入 $\\delta J[m;\\delta m]$ 的表达式中：\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle C_k^{-1} (H_k F_k^{\\prime}(m)[\\delta m]), H_k F_k(m) - d_k \\rangle_{\\mathcal{Y}_k}\n$$\n问题要求一个将 $\\delta m$ 分离到 $\\mathcal{M}$ 上的单个内积中的表达式。我们通过系统地应用算子的希尔伯特伴随的定义来实现这一点。对于希尔伯特空间 $X, Y$ 之间的任意线性算子 $A:X \\to Y$，其伴随 $A^*:Y \\to X$ 对所有 $x \\in X, y \\in Y$ 满足 $\\langle Ax, y \\rangle_Y = \\langle x, A^*y \\rangle_X$。\n首先，我们使用 $C_k^{-1}$ 的自伴性质：\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle H_k F_k^{\\prime}(m)[\\delta m], C_k^{-1} (H_k F_k(m) - d_k) \\rangle_{\\mathcal{Y}_k}\n$$\n作用于 $\\delta m$ 的算子是复合算子 $H_k F_k^{\\prime}(m)$，它将 $\\mathcal{M}$ 映射到 $\\mathcal{Y}_k$。它的伴随是 $(H_k F_k^{\\prime}(m))^* = (F_k^{\\prime}(m))^* H_k^*$，它将 $\\mathcal{Y}_k$ 映射到 $\\mathcal{M}$。将这个复合算子移动到内积的另一边：\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle \\delta m, (H_k F_k^{\\prime}(m))^* [C_k^{-1} (H_k F_k(m) - d_k)] \\rangle_{\\mathcal{M}}\n$$\n$$\n\\delta J[m;\\delta m] = \\sum_{k=1}^{K} \\langle \\delta m, (F_k^{\\prime}(m))^* H_k^* [C_k^{-1} (H_k F_k(m) - d_k)] \\rangle_{\\mathcal{M}}\n$$\n根据内积在其第二个参数上的线性性质，我们可以将求和符号移入内积内部：\n$$\n\\delta J[m;\\delta m] = \\bigg\\langle \\delta m, \\sum_{k=1}^{K} (F_k^{\\prime}(m))^* H_k^* [C_k^{-1} (H_k F_k(m) - d_k)] \\bigg\\rangle_{\\mathcal{M}}\n$$\n最后，由于我们处于实希尔伯特空间中，内积是对称的。我们将表达式写成常规形式 $\\langle \\nabla J(m), \\delta m \\rangle_{\\mathcal{M}}$：\n$$\n\\delta J[m;\\delta m] = \\bigg\\langle \\sum_{k=1}^{K} (F_k^{\\prime}(m))^{*} H_k^{*} C_k^{-1} (H_k F_k(m) - d_k), \\delta m \\bigg\\rangle_{\\mathcal{M}}\n$$\n这就是 Gâteaux 导数的最终伴随状态表达式。项 $s_k = C_k^{-1} (H_k F_k(m) - d_k) \\in \\mathcal{Y}_k$ 是加权残差，它作为模态 $k$ 的“伴随源”。伴随算子 $(F_k^{\\prime}(m))^{*} H_k^{*}$ 将此源从数据空间 $\\mathcal{Y}_k$ 反向传播到模型空间 $\\mathcal{M}$，以形成目标泛函的梯度。", "answer": "$$\n\\boxed{\\bigg\\langle \\sum_{k=1}^{K} (F_k^{\\prime}(m))^{*} H_k^{*} C_k^{-1} \\big(H_k F_k(m) - d_k\\big), \\delta m \\bigg\\rangle_{\\mathcal{M}}}\n$$", "id": "3404713"}, {"introduction": "联合反演的本质是一个多目标优化问题，我们试图同时最小化多个（通常是相互冲突的）目标函数。这个更广阔的视角引出了帕累托最优（Pareto optimality）的概念，即所有最优权衡解的集合。本练习探讨了在联合反演中广泛使用的加权和方法（标量化）与帕累托最优解集之间的深刻联系，并强调了凸性在确保我们能够通过改变权重来探索整个帕累托前沿中的重要作用 [@problem_id:3404773]。", "problem": "考虑一个联合反演问题，其中模型向量 $m \\in \\mathcal{M}$ 必须解释两种模态的数据，产生两个凸失配目标函数 $\\phi_1(m)$ 和 $\\phi_2(m)$，其中 $\\mathcal{M}$ 是一个非空凸可行集。假设每种模态的正演算子是线性的，数据保真项是二次的，因此每个 $\\phi_i(m)$ 在 $\\mathcal{M}$ 上是凸的。定义该双目标问题的帕累托集，并分析该帕累托集与以下形式的标量化优化问题之间的关系\n$$\n\\min_{m \\in \\mathcal{M}} \\ \\alpha \\, \\phi_1(m) + (1-\\alpha) \\, \\phi_2(m), \\quad \\text{其中 } \\alpha \\in (0,1).\n$$\n您的回答应基于凸分析和多目标最优性的基本原理。\n\n在给定的凸性假设下，选择最准确的陈述：\n\nA. 帕累托集是能够同时最小化 $\\phi_1(m)$ 和 $\\phi_2(m)$ 的所有 $m \\in \\mathcal{M}$ 的集合，并且任何 $\\alpha \\in (0,1)$ 的选择都能恢复所有的帕累托点（每个帕累托点都是某个 $\\alpha$ 的标量化问题的解），即使可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是非凸的。\n\nB. 帕累托集是非支配的 $m \\in \\mathcal{M}$ 的集合，即不存在 $m' \\in \\mathcal{M}$ 使得对于 $i=1,2$ 都有 $\\phi_i(m') \\le \\phi_i(m)$ 且至少在一个目标上是严格不等式；对于任何 $\\alpha \\in (0,1)$，$\\alpha \\phi_1 + (1-\\alpha)\\phi_2$ 的每个最小化子都是帕累托最优的，并且改变 $\\alpha$ 可以恢复所有支撑的（弱）帕累托点。如果可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是凸的，那么每个帕累托最优点都是被支撑的，并且可以通过某个 $\\alpha \\in (0,1)$ 恢复。\n\nC. 帕累托集由所有使 $\\phi_1(m) + \\phi_2(m)$ 最小化的 $m \\in \\mathcal{M}$ 组成，并且即使当 $\\phi_1$ 和 $\\phi_2$ 在凸集 $\\mathcal{M}$ 上是凸的，使用 $\\alpha \\in (0,1)$ 进行标量化也可能返回非帕累托点。\n\nD. 帕累托集等于交集 $\\bigcap_{\\alpha \\in (0,1)} \\arg\\min_{m \\in \\mathcal{M}} \\alpha \\phi_1(m) + (1-\\alpha) \\phi_2(m)$，因此标量化问题等价于多目标最优性，没有信息损失，无论可达目标集的几何形状如何。", "solution": "在尝试解答之前，将对问题陈述进行验证。\n\n### 第一步：提取已知条件\n\n-   模型向量：$m \\in \\mathcal{M}$\n-   可行集：$\\mathcal{M}$ 是一个非空凸集。\n-   失配目标函数：$\\phi_1(m)$ 和 $\\phi_2(m)$，代表两种数据模态。\n-   函数性质：$\\phi_1(m)$ 和 $\\phi_2(m)$ 是在 $\\mathcal{M}$ 上的凸函数。这被陈述为线性正演算子和二次数据保真项的结果。\n-   标量化优化问题：$\\min_{m \\in \\mathcal{M}} \\ \\alpha \\, \\phi_1(m) + (1-\\alpha) \\, \\phi_2(m)$，其中 $\\alpha \\in (0,1)$。\n-   任务：分析双目标问题的帕累托集与标量化问题解之间的关系，并从选项中选择最准确的陈述。\n\n### 第二步：使用提取的已知条件进行验证\n\n-   **科学性（关键）**：该问题牢固地植根于多目标优化和凸分析的数学理论，这些是反演问题和数据同化领域的标准工具。帕累托最优性、凸函数和标量化（加权和方法）等概念定义明确且是基础性的。该设置在科学上是合理的。\n-   **适定性**：该问题要求在指定的数学条件下，对不同类型解集之间的关系进行理论分析。所提供的信息（函数和可行集的凸性）足以基于已建立的定理进行此分析。该问题是适定的。\n-   **客观性（关键）**：该问题使用精确、形式化的数学语言陈述。诸如“凸集”、“凸函数”和“帕累托集”之类的术语具有明确的定义。该问题是客观的。\n-   **不完整或矛盾的设置**：设置是自洽的，并提供了对问题进行推理所必需的假设（凸性）。没有矛盾之处。\n-   **不切实际或不可行**：所描述的设置是多模态联合反演中的一个标准且广泛使用的模型，其目的是寻找单个模型来解释来自不同物理测量（例如地球物理学中的地震和重力数据）的数据。在此理论背景下，这些假设是现实的。\n-   **未检测到其他缺陷**：该问题并非无关紧要、不适定或隐喻性的。它解决的是多目标优化中的一个核心概念挑战。\n\n### 第三步：结论与行动\n\n问题陈述是**有效的**。将基于基本原理推导解决方案。\n\n### 推导\n\n问题的核心在于多目标优化问题的解与相关的单目标标量化问题的解之间的关系。\n\n**1. 定义**\n\n-   **多目标问题**：问题是在概念上为 $m \\in \\mathcal{M}$ 同时“最小化”$\\phi_1(m)$ 和 $\\phi_2(m)$。我们可以将其写作：\n    $$ \\min_{m \\in \\mathcal{M}} \\ (\\phi_1(m), \\phi_2(m)) $$\n-   **帕累托支配**：一个模型 $m' \\in \\mathcal{M}$ *支配* 一个模型 $m \\in \\mathcal{M}$，如果对于 $i=1, 2$ 都有 $\\phi_i(m') \\leq \\phi_i(m)$，并且存在至少一个索引 $j \\in \\{1, 2\\}$ 使得 $\\phi_j(m') < \\phi_j(m)$。\n-   **帕累托最优性**：一个模型 $m^* \\in \\mathcal{M}$ 是**帕累托最优的**（或非支配的、或有效的），如果没有其他模型 $m' \\in \\mathcal{M}$ 支配它。所有这些点的集合就是**帕累托集**。\n-   **标量化问题（加权和方法）**：对于给定的权重 $\\alpha \\in (0,1)$，标量化问题是：\n    $$ P(\\alpha): \\min_{m \\in \\mathcal{M}} \\Phi_\\alpha(m) \\text{ 其中 } \\Phi_\\alpha(m) = \\alpha \\, \\phi_1(m) + (1-\\alpha) \\, \\phi_2(m) $$\n    目标函数 $\\Phi_\\alpha(m)$ 是两个凸函数（$\\phi_1, \\phi_2$）的正加权和，所以 $\\Phi_\\alpha(m)$ 也是凸的。最小化是在一个凸集 $\\mathcal{M}$ 上进行的。这是一个标准的凸优化问题。\n\n**2. 标量化与帕累托最优性之间的关系**\n\n-   **第一部分：对于 $\\alpha \\in (0,1)$，$P(\\alpha)$ 的任何解都是帕累托最优的。**\n    设 $m^*$ 是某个 $\\alpha \\in (0,1)$ 的 $\\Phi_\\alpha(m)$ 的一个最小化子。为进行反证，假设 $m^*$ 不是帕累托最优的。那么存在一个 $m' \\in \\mathcal{M}$ 支配 $m^*$。根据支配的定义，这意味着：\n    1.  $\\phi_1(m') \\leq \\phi_1(m^*)$\n    2.  $\\phi_2(m') \\leq \\phi_2(m^*)$\n    3.  这些不等式中至少有一个是严格的。\n    \n    由于 $\\alpha > 0$ 和 $1-\\alpha > 0$，我们可以将这些不等式乘以权重：\n    1.  $\\alpha \\phi_1(m') \\leq \\alpha \\phi_1(m^*)$\n    2.  $(1-\\alpha) \\phi_2(m') \\leq (1-\\alpha) \\phi_2(m^*)$\n\n    将它们相加得到：\n    $$ \\alpha \\phi_1(m') + (1-\\alpha) \\phi_2(m') \\leq \\alpha \\phi_1(m^*) + (1-\\alpha) \\phi_2(m^*) $$\n    $$ \\implies \\Phi_\\alpha(m') \\leq \\Phi_\\alpha(m^*) $$\n    此外，由于初始不等式中有一个是严格的（例如，$\\phi_1(m') < \\phi_1(m^*)$），且其对应的权重（$\\alpha$）是严格为正的，因此最终的不等式也必须是严格的：\n    $$ \\Phi_\\alpha(m') < \\Phi_a(m^*) $$\n    这与 $m^*$ 是 $\\Phi_\\alpha(m)$ 的最小化子的假设相矛盾。因此，最初的假设必定是错误的， $m^*$ 必须是帕累托最优的。这对权重在 $(0,1)$ 内的标量化问题的任何最小化子都成立。\n\n-   **第二部分：每个帕累托最优点都是某个 $\\alpha$ 的 $P(\\alpha)$ 的解吗？**\n    这在一般情况下是不能保证的。答案取决于**可达目标集**的几何形状，该集合定义为 $Y = \\{(\\phi_1(m), \\phi_2(m)) \\mid m \\in \\mathcal{M}\\}$。对于所有 $\\alpha \\in [0,1]$，标量化问题的解对应于 $Y$ 的凸包边界上的点。这些点被称为**支撑的帕累托最优点**。\n    即使模型空间 $\\mathcal{M}$ 是凸的，目标函数 $\\phi_i$ 也是凸的，映射 $m \\mapsto (\\phi_1(m), \\phi_2(m))$ 是一个向量值函数，一个凸集在该映射下的像通常**不是**凸的。\n    如果 $Y$ 不是凸的，就可能存在不被支撑的帕累托最优点。这些“非支撑”点位于帕累托前沿的非凸“凹陷”处，无法通过加权和方法找到。\n    然而，有一个关键定理：**如果可达目标集 $Y$ 是凸的，那么每个帕累托最优点都是一个支撑点。**因此，对于任何帕累托最优点 $m^*$，存在一个 $\\alpha \\in [0,1]$，使得 $m^*$ 是 $P(\\alpha)$ 的一个解。\n\n### 逐项分析\n\n-   **A. 帕累托集是能够同时最小化 $\\phi_1(m)$ 和 $\\phi_2(m)$ 的所有 $m \\in \\mathcal{M}$ 的集合，并且任何 $\\alpha \\in (0,1)$ 的选择都能恢复所有的帕累托点（每个帕累托点都是某个 $\\alpha$ 的标量化问题的解），即使可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是非凸的。**\n    陈述的第一部分给出了帕累托集的错误定义。同时最小化两个目标的点的集合在目标空间中被称为“乌托邦点”，而在实践中，由于需要权衡，实现这一点的模型通常不存在。帕累托集是所有非支配的权衡解的完整集合。第二部分也是错误的。如果可达目标集是非凸的，加权和方法（标量化）不保证能恢复所有帕累托点；它只能恢复被支撑的点。\n    **结论：错误。**\n\n-   **B. 帕累托集是非支配的 $m \\in \\mathcal{M}$ 的集合，即不存在 $m' \\in \\mathcal{M}$ 使得对于 $i=1,2$ 都有 $\\phi_i(m') \\le \\phi_i(m)$ 且至少在一个目标上是严格不等式；对于任何 $\\alpha \\in (0,1)$，$\\alpha \\phi_1 + (1-\\alpha)\\phi_2$ 的每个最小化子都是帕累托最优的，并且改变 $\\alpha$ 可以恢复所有支撑的（弱）帕累托点。如果可达目标集 $\\{(\\phi_1(m),\\phi_2(m)) : m \\in \\mathcal{M}\\}$ 是凸的，那么每个帕累托最优点都是被支撑的，并且可以通过某个 $\\alpha \\in (0,1)$ 恢复。**\n    该陈述由几个正确的主张组成。\n    1. 它通过非支配性给出了帕累托集的正确定义。\n    2. 它正确地指出，对于 $\\alpha \\in (0,1)$，标量化问题的任何最小化子都是帕累托最优的（如上所证）。\n    3. 它正确地将通过改变 $\\alpha$ 找到的点集描述为“支撑的”帕累托点。\n    4. 它正确地陈述了基本结果：如果可达目标集是凸的，则支撑点和非支撑点之间的区别消失，并且（几乎所有）帕累托点都可以通过标量化找到。使用 $\\alpha \\in (0,1)$ 在技术上排除了可能唯一最小化某个目标并需要 $\\alpha=0$ 或 $\\alpha=1$ 的极端端点，但这是一个次要细节，该陈述是迄今为止最准确和最全面的。\n    **结论：正确。**\n\n-   **C. 帕累托集由所有使 $\\phi_1(m) + \\phi_2(m)$ 最小化的 $m \\in \\mathcal{M}$ 组成，并且即使当 $\\phi_1$ 和 $\\phi_2$ 在凸集 $\\mathcal{M}$ 上是凸的，使用 $\\alpha \\in (0,1)$ 进行标量化也可能返回非帕累托点。**\n    第一部分是错误的。最小化和 $\\phi_1(m) + \\phi_2(m)$ 对应于帕累托前沿上的单个点（或点集）（具体来说，对应于 $\\alpha=1/2$）。这并非整个帕累托集。第二部分也是错误的。如推导所示，在指定条件下（$\\phi_i$ 是凸的，`\\mathcal{M}` 是凸的）且 $\\alpha \\in (0,1)$ 时，最小化子总是帕累托最优的。\n    **结论：错误。**\n\n-   **D. 帕累托集等于交集 $\\bigcap_{\\alpha \\in (0,1)} \\arg\\min_{m \\in \\mathcal{M}} \\alpha \\phi_1(m) + (1-\\alpha) \\phi_2(m)$，因此标量化问题等价于多目标最优性，没有信息损失，无论可达目标集的几何形状如何。**\n    这个陈述有根本性的缺陷。帕累托集与不同 $\\alpha$ 下的解集的*并集*有关，而不是*交集*。交集将只包含对所有权重同时最优的模型，这是一个极其严格的条件，通常结果是空集，或者如果存在乌托邦点，则为单个乌托邦点。此外，“无论可达目标集的几何形状如何”都等价的说法是错误的。该集合的几何形状，特别是其凸性，是决定标量化是否能恢复整个帕累托集的关键因素。\n    **结论：错误。**", "answer": "$$\\boxed{B}$$", "id": "3404773"}]}