{"hands_on_practices": [{"introduction": "本练习的核心是为时移全波形反演问题推导梯度。理解伴随状态法至关重要，因为它为根据观测数据有效优化大规模模型提供了计算基础。通过这个练习 [@problem_id:3427743]，你将巩固对在基于物理波动方程的反演中如何计算模型更新的理解。", "problem": "考虑一个在有界开域 $\\Omega \\subset \\mathbb{R}^{3}$ 上、时间区间为 $[0,T]$、在边界 $\\partial \\Omega$ 上具有齐次吸收边界条件的恒定密度声波传播模型。设标量参数场为 $m(\\mathbf{x}) = c(\\mathbf{x})^{-2}$，其中 $c(\\mathbf{x})$ 是波速。给定一个基线参数 $m_{0}(\\mathbf{x})$ 和一个小的时移微扰 $\\delta m(\\mathbf{x})$，对于震源指数 $s = 1,\\dots,N_{s}$，基线正演波场 $u_{0}^{(s)}(\\mathbf{x},t)$ 由以下初值问题定义\n$$\n\\begin{cases}\nm_{0}(\\mathbf{x}) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t) - \\nabla^{2} u_{0}^{(s)}(\\mathbf{x},t) = q^{(s)}(\\mathbf{x},t),  (\\mathbf{x},t) \\in \\Omega \\times (0,T], \\\\\nu_{0}^{(s)}(\\mathbf{x},0) = 0, \\quad \\partial_{t} u_{0}^{(s)}(\\mathbf{x},0) = 0,  \\mathbf{x} \\in \\Omega,\n\\end{cases}\n$$\n其中 $q^{(s)}(\\mathbf{x},t)$ 是在 $\\Omega$ 内有支撑的已知震源项。线性化 (Born) 增量正演波场 $u_{1}^{(s)}(\\mathbf{x},t)$ 定义为以下问题的解\n$$\n\\begin{cases}\nm_{0}(\\mathbf{x}) \\, \\partial_{tt} u_{1}^{(s)}(\\mathbf{x},t) - \\nabla^{2} u_{1}^{(s)}(\\mathbf{x},t) = -\\delta m(\\mathbf{x}) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t),  (\\mathbf{x},t) \\in \\Omega \\times (0,T], \\\\\nu_{1}^{(s)}(\\mathbf{x},0) = 0, \\quad \\partial_{t} u_{1}^{(s)}(\\mathbf{x},0) = 0,  \\mathbf{x} \\in \\Omega.\n\\end{cases}\n$$\n令检波器采样算子 $P$ 将一个波场映射到每个震源 $s$ 的检波点位置 $\\{\\mathbf{r}_{j}^{(s)}\\}_{j=1}^{N_{r}^{(s)}}$ 处的值，使得 $(P u)(\\mathbf{r}_{j}^{(s)},t) = u(\\mathbf{r}_{j}^{(s)},t)$。令震源 $s$ 的观测时移数据差为 $\\delta d_{\\mathrm{obs}}^{(s)}(\\mathbf{r}_{j}^{(s)},t)$，并令 $W$ 是一个对称正定加权算子，以标量 $\\{w_{j}^{(s)} > 0\\}$ 逐点作用于检波器。考虑最小二乘失配泛函\n$$\nJ(\\delta m) = \\frac{1}{2} \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\sum_{j=1}^{N_{r}^{(s)}} w_{j}^{(s)} \\left( u_{1}^{(s)}(\\mathbf{r}_{j}^{(s)},t) - \\delta d_{\\mathrm{obs}}^{(s)}(\\mathbf{r}_{j}^{(s)},t) \\right)^{2} \\, dt.\n$$\n仅使用伴随状态法和偏微分方程变分法的基本原理，推导 $J$ 相对于 $\\delta m(\\mathbf{x})$ 的 Fréchet 梯度。您的推导必须从所述的控制方程、初始条件以及 $J(\\delta m)$ 的定义出发，并使用分部积分法和互易性来消去波场的变化量，以伴随场代之。证明该计算一旦生成基线正演波场 $u_{0}^{(s)}$，就不再需要重新计算它们，并将最终梯度表示为适当定义的正演和伴随微扰场之间的时间内积。请以 $\\nabla J(\\delta m)(\\mathbf{x})$ 的单个闭式解析表达式的形式给出最终答案。不需要进行数值计算，最终表达式中也不应包含任何单位。", "solution": "目标是推导最小二乘泛函 $J(\\delta m)$ 关于时移模型微扰 $\\delta m(\\mathbf{x})$ 的 Fréchet 梯度。该梯度记为 $\\nabla J(\\delta m)$，是与 $\\delta m$ 处于同一空间的函数，并通过 Fréchet 导数定义。$J$ 在 $\\delta m$ 处沿任意方向 $h(\\mathbf{x})$ 的方向导数由内积给出：\n$$ \\delta J[\\delta m; h] = \\lim_{\\epsilon \\to 0} \\frac{J(\\delta m + \\epsilon h) - J(\\delta m)}{\\epsilon} = \\int_{\\Omega} \\nabla J(\\delta m)(\\mathbf{x}) h(\\mathbf{x}) \\,d\\mathbf{x} $$\n我们的目标是通过处理 $\\delta J$ 的表达式来找到 $\\nabla J(\\delta m)(\\mathbf{x})$ 的表达式。\n\n该泛函由下式给出：\n$$ J(\\delta m) = \\frac{1}{2} \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\sum_{j=1}^{N_{r}^{(s)}} w_{j}^{(s)} \\left( u_{1}^{(s)}[\\delta m](\\mathbf{r}_{j}^{(s)},t) - \\delta d_{\\mathrm{obs}}^{(s)}(\\mathbf{r}_{j}^{(s)},t) \\right)^{2} \\, dt $$\n这里，我们明确地将 $u_{1}^{(s)}$ 对 $\\delta m$ 的依赖表示为 $u_{1}^{(s)}[\\delta m]$。微扰 $\\delta m \\rightarrow \\delta m + \\epsilon h$ 会在线性化波场 $u_1^{(s)}$ 中引起一个微扰。令 $\\dot{u}_1^{(s)}$ 为 $u_1^{(s)}$ 关于 $\\delta m$ 在方向 $h$ 上的方向导数：\n$$ \\dot{u}_1^{(s)} = \\lim_{\\epsilon \\to 0} \\frac{u_{1}^{(s)}[\\delta m + \\epsilon h] - u_{1}^{(s)}[\\delta m]}{\\epsilon} $$\n对 $J$ 关于 $\\epsilon$ 求导并在 $\\epsilon=0$ 处求值，我们得到：\n$$ \\delta J[\\delta m; h] = \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\sum_{j=1}^{N_{r}^{(s)}} w_{j}^{(s)} \\left( u_{1}^{(s)}(\\mathbf{r}_{j}^{(s)},t) - \\delta d_{\\mathrm{obs}}^{(s)}(\\mathbf{r}_{j}^{(s)},t) \\right) \\dot{u}_{1}^{(s)}(\\mathbf{r}_{j}^{(s)},t) \\, dt $$\n我们可以使用 Dirac delta 函数 $\\delta(\\mathbf{x})$ 来表示对检波器的求和，并在域 $\\Omega$ 上积分。让我们将震源 $s$ 在检波器处的数据残差定义为伴随方程的震源项：\n$$ q_{\\text{adj}}^{(s)}(\\mathbf{x},t) = \\sum_{j=1}^{N_{r}^{(s)}} w_{j}^{(s)} \\left( u_{1}^{(s)}(\\mathbf{r}_{j}^{(s)},t) - \\delta d_{\\mathrm{obs}}^{(s)}(\\mathbf{r}_{j}^{(s)},t) \\right) \\delta(\\mathbf{x} - \\mathbf{r}_{j}^{(s)}) $$\n根据这个定义，泛函的变分变为：\n$$ \\delta J = \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\int_{\\Omega} q_{\\text{adj}}^{(s)}(\\mathbf{x},t) \\dot{u}_{1}^{(s)}(\\mathbf{x},t) \\, d\\mathbf{x} \\, dt $$\n这个 $\\delta J$ 的表达式依赖于状态变量的变分 $\\dot{u}_{1}^{(s)}$。伴随状态法使我们能够消除这种依赖性，并直接用模型微扰 $h$ 来表示 $\\delta J$。\n\n接下来，我们寻找 $\\dot{u}_{1}^{(s)}$ 的控制方程。$u_{1}^{(s)}[\\delta m + \\epsilon h]$ 的方程是：\n$$ m_{0}(\\mathbf{x}) \\, \\partial_{tt} u_{1}^{(s)}[\\delta m + \\epsilon h] - \\nabla^{2} u_{1}^{(s)}[\\delta m + \\epsilon h] = -(\\delta m(\\mathbf{x}) + \\epsilon h(\\mathbf{x})) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t) $$\n减去 $u_{1}^{(s)}[\\delta m]$ 的方程，除以 $\\epsilon$，并取 $\\epsilon \\to 0$ 的极限，得到 $\\dot{u}_{1}^{(s)}$ 的偏微分方程：\n$$\n\\begin{cases}\nm_{0}(\\mathbf{x}) \\, \\partial_{tt} \\dot{u}_{1}^{(s)}(\\mathbf{x},t) - \\nabla^{2} \\dot{u}_{1}^{(s)}(\\mathbf{x},t) = -h(\\mathbf{x}) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t),  (\\mathbf{x},t) \\in \\Omega \\times (0,T], \\\\\n\\dot{u}_{1}^{(s)}(\\mathbf{x},0) = 0, \\quad \\partial_{t} \\dot{u}_{1}^{(s)}(\\mathbf{x},0) = 0,  \\mathbf{x} \\in \\Omega.\n\\end{cases}\n$$\n我们将正演波算子定义为 $L_0 = m_0 \\partial_{tt} - \\nabla^2$。$\\dot{u}_{1}^{(s)}$ 的方程为 $L_0 \\dot{u}_{1}^{(s)} = -h \\partial_{tt} u_{0}^{(s)}$。\n\n现在，我们为每个震源 $s$ 引入伴随状态（或对偶场）$v^{(s)}(\\mathbf{x},t)$。我们通过让 $L_0$ 作用于 $v^{(s)}$ 来定义伴随方程，其中伴随源 $q_{\\text{adj}}^{(s)}$作为强迫项：\n$$ L_0 v^{(s)} = m_{0}(\\mathbf{x}) \\, \\partial_{tt} v^{(s)}(\\mathbf{x},t) - \\nabla^{2} v^{(s)}(\\mathbf{x},t) = q_{\\text{adj}}^{(s)}(\\mathbf{x},t) $$\n为了确保算子是自伴的，我们施加适当的边界条件和终值条件。我们选择齐次终值条件 $v^{(s)}(\\mathbf{x},T) = 0$ 和 $\\partial_t v^{(s)}(\\mathbf{x},T) = 0$。这意味着伴随方程从 $t=T$ 到 $t=0$ 沿时间反向求解。$v^{(s)}$ 在 $\\partial\\Omega$ 上的边界条件选择为与正演问题相同（例如，齐次 Dirichlet 或 Neumann 条件，或相应的伴随吸收条件），这使得算子的空间部分是自伴的。\n\n有了这个定义，我们可以重写 $\\delta J$：\n$$ \\delta J = \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\int_{\\Omega} (L_0 v^{(s)}) \\dot{u}_{1}^{(s)} \\, d\\mathbf{x} \\, dt $$\n我们现在使用分部积分将算子 $L_0$ 从 $v^{(s)}$ 移到 $\\dot{u}_{1}^{(s)}$ 上。这是互易性的一种体现。\n$$ \\int_0^T \\int_\\Omega (m_0 \\partial_{tt} v^{(s)}) \\dot{u}_{1}^{(s)} \\,d\\mathbf{x}dt = \\int_\\Omega \\left[ m_0 \\partial_t v^{(s)} \\dot{u}_{1}^{(s)} - m_0 v^{(s)} \\partial_t \\dot{u}_{1}^{(s)} \\right]_0^T \\,d\\mathbf{x} + \\int_0^T \\int_\\Omega (m_0 v^{(s)}) \\partial_{tt} \\dot{u}_{1}^{(s)} \\,d\\mathbf{x}dt $$\n由于 $v^{(s)}$ 的终值条件，在 $t=T$ 处的时间边界项为零；由于 $\\dot{u}_{1}^{(s)}$ 的初值条件，在 $t=0$ 处的时间边界项也为零。\n\n对于空间部分，使用格林第二公式：\n$$ \\int_\\Omega (-\\nabla^2 v^{(s)}) \\dot{u}_{1}^{(s)} \\, d\\mathbf{x} = \\int_\\Omega v^{(s)} (-\\nabla^2 \\dot{u}_{1}^{(s)}) \\, d\\mathbf{x} + \\int_{\\partial\\Omega} \\left( v^{(s)} \\frac{\\partial \\dot{u}_1^{(s)}}{\\partial n} - \\dot{u}_1^{(s)} \\frac{\\partial v^{(s)}}{\\partial n} \\right) dS $$\n由于 $\\dot{u}_{1}^{(s)}$ 和 $v^{(s)}$ 都满足齐次吸收边界条件，在 $\\partial\\Omega$ 上的边界积分为零。\n\n因此，算子 $L_0$ 在满足这些条件的场空间上是自伴的，我们有：\n$$ \\delta J = \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\int_{\\Omega} v^{(s)} (L_0 \\dot{u}_{1}^{(s)}) \\, d\\mathbf{x} \\, dt $$\n现在代入 $L_0 \\dot{u}_{1}^{(s)}$ 的表达式：\n$$ \\delta J = \\sum_{s=1}^{N_{s}} \\int_{0}^{T} \\int_{\\Omega} v^{(s)}(\\mathbf{x},t) \\left( -h(\\mathbf{x}) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t) \\right) \\, d\\mathbf{x} \\, dt $$\n由于 $h(\\mathbf{x})$ 与时间和震源指数 $s$ 无关，我们可以重排积分：\n$$ \\delta J = \\int_{\\Omega} h(\\mathbf{x}) \\left( - \\sum_{s=1}^{N_{s}} \\int_{0}^{T} v^{(s)}(\\mathbf{x},t) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t) \\, dt \\right) d\\mathbf{x} $$\n与定义 $\\delta J = \\int_{\\Omega} \\nabla J(\\delta m)(\\mathbf{x}) h(\\mathbf{x}) \\,d\\mathbf{x}$ 相比较，我们可以识别出 Fréchet 梯度：\n$$ \\nabla J(\\delta m)(\\mathbf{x}) = - \\sum_{s=1}^{N_{s}} \\int_{0}^{T} v^{(s)}(\\mathbf{x},t) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t) \\, dt $$\n这个最终表达式是伴随场 $v^{(s)}$ 与基线场 $u_0^{(s)}$ 的时间二阶导数之间的零延迟互相关（或时间上的内积），并对所有震源求和。\n\n对于给定的 $\\delta m$，计算梯度的步骤如下：\n1. 对每个震源 $s$，求解基线波场 $u_0^{(s)}$ 并存储其在整个区域和时间间隔上的时间二阶导数 $\\partial_{tt} u_0^{(s)}$。此步骤与 $\\delta m$ 无关。\n2. 对每个震源 $s$，使用存储的 $\\partial_{tt} u_0^{(s)}$ 和当前估计的 $\\delta m$ 来求解线性化波场 $u_1^{(s)}$。\n3. 对每个震源 $s$，计算检波器处的数据残差，并用它们来构建伴随源 $q_{\\text{adj}}^{(s)}$。\n4. 对每个震源 $s$，从 $t=T$ 到 $t=0$ 反向求解伴随方程，得到 $v^{(s)}$。\n5. 对每个震源 $s$，在计算伴随场 $v^{(s)}$ 的同时，将其与预先存储的场 $\\partial_{tt} u_0^{(s)}$ 进行相关运算，并对时间进行积分。\n6. 将所有震源的结果相加，形成梯度 $\\nabla J(\\delta m)(\\mathbf{x})$。\n\n该过程证实，基线波场 $u_0^{(s)}$（或其导数）只需计算一次，存储后即可重复使用。为不同的 $\\delta m$ 计算梯度时，不需要重新计算 $u_0^{(s)}$。梯度的最终表达式是一个用正演场和伴随场表示的闭式解析表达式。", "answer": "$$ \\boxed{- \\sum_{s=1}^{N_{s}} \\int_{0}^{T} v^{(s)}(\\mathbf{x},t) \\, \\partial_{tt} u_{0}^{(s)}(\\mathbf{x},t) \\, dt} $$", "id": "3427743"}, {"introduction": "时移数据可以通过多种策略进行反演，每种策略都有其自身的假设和权衡。这个综合性练习 [@problem_id:3427721] 要求你实现并比较三种主要方法：稳定化差异反演、联合反演和四维变分同化 (4D-Var)。通过分析误差、分辨率和不确定性等量化指标，你将对这些不同框架的行为和相对优点获得实践性的见解。", "problem": "考虑以下线性、高斯、一维时移反问题，该问题具有静态观测算子和简单的恒等动力学模型。设空间网格大小为 $N$，空间坐标 $s_i \\in [0,1]$ 对于 $i \\in \\{1,\\dots,N\\}$ 均匀分布，时间步数为 $T$。时刻 $k$ 的未知状态为 $x_k \\in \\mathbb{R}^N$，时刻 $k$ 的观测数据为 $y_k \\in \\mathbb{R}^M$，由以下线性正演模型给出\n$$\ny_k = H x_k + \\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0, R),\n$$\n其中 $H \\in \\mathbb{R}^{M \\times N}$ 不随时间变化，$R = \\sigma^2 I_M$ 是观测噪声协方差，且 $\\sigma > 0$。假设一个简单的恒等动力学模型 $F = I_N$，因此模型演化在模型误差范围内为 $x_{k+1} \\approx x_k$。令 $D \\in \\mathbb{R}^{N \\times N}$ 表示空间网格上的一阶周期差分算子，即 $(D x)_i = x_i - x_{i-1}$ 且 $x_0 \\equiv x_N$，并通过 $D^T D$ 定义空间粗糙度惩罚。令 $T_{\\text{op}} \\in \\mathbb{R}^{N(T-1) \\times NT}$ 表示作用于堆叠状态 $X = [x_1^T,\\dots,x_T^T]^T$ 的时间一阶差分算子，即 $T_{\\text{op}} X = [ (x_2 - x_1)^T, \\dots, (x_T - x_{T-1})^T ]^T$。\n\n您将需要在一个固定的网格上生成合成真值 $x_k^{\\text{true}}$ 及相应的带噪观测数据，然后使用三种方法计算并比较重建结果：稳定化差异反演（stabilized difference inversion）、联合反演（joint inversion）和四维变分数据同化（four-dimensional variational data assimilation (4D-Var)）。对于每种方法，您必须从第一性原理推导出相应的线性估计器，并实现它以获得最终时刻状态 $\\hat{x}_T$ 的重建。然后，您必须报告最终时刻的三个定量指标：$\\hat{x}_T$ 和 $x_T^{\\text{true}}$ 之间的均方根误差（RMSE）、根据最终时刻分辨率矩阵计算的平均分辨率宽度，以及根据最终时刻后验协方差计算的平均后验不确定性。\n\n需要使用的基本原理包括：线性高斯模型及其与 Tikhonov 正则化的等价性、一阶空间和时间差分算子的性质，以及联合反演和四维变分数据同化的变分形式。\n\n定义和要求的构造：\n- 合成真值生成：设 $N = 20$ 且 $T = 3$。定义 $s_i = \\frac{i-1}{N-1}$，其中 $i \\in \\{1,\\dots,N\\}$。定义\n$$\nx_1^{\\text{true}}(s) = 0.1 \\sin(2 \\pi s) + \\exp\\!\\left(-\\frac{(s-0.30)^2}{2 \\cdot 0.05^2}\\right),\n$$\n$$\nx_2^{\\text{true}}(s) = 0.1 \\sin(2 \\pi s) + 0.8\\,\\exp\\!\\left(-\\frac{(s-0.50)^2}{2 \\cdot 0.05^2}\\right),\n$$\n$$\nx_3^{\\text{true}}(s) = 0.1 \\sin(2 \\pi s) + 0.6\\,\\exp\\!\\left(-\\frac{(s-0.70)^2}{2 \\cdot 0.05^2}\\right),\n$$\n并通过在 $s_i$ 处采样来设置离散向量 $x_k^{\\text{true}} \\in \\mathbb{R}^N$。设观测矩阵 $H \\in \\mathbb{R}^{M \\times N}$ 通过以 $r_m = \\frac{m-1}{M-1}$（$m \\in \\{1,\\dots,M\\}$）为中心的点扩散行定义，其中\n$$\nH_{m i} = \\exp\\!\\left(-\\frac{(s_i - r_m)^2}{2 \\cdot \\sigma_h^2}\\right), \\quad \\sigma_h = 0.05.\n$$\n生成带噪数据 $y_k = H x_k^{\\text{true}} + \\varepsilon_k$，其中 $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2 I_M)$ 在时间上独立抽取。\n\n- 稳定化差异反演（SDI）：首先，通过最小化 Tikhonov 泛函来估计 $x_1$\n$$\nJ_{\\text{SDI},1}(x) = \\| y_1 - H x \\|_{R^{-1}}^2 + \\lambda_s \\| D x \\|_2^2,\n$$\n然后对于 $k \\in \\{2,\\dots,T\\}$，通过最小化以下泛函来估计变化量 $\\Delta x_k$\n$$\nJ_{\\text{SDI},\\Delta}(\\Delta x) = \\| (y_k - y_{k-1}) - H \\Delta x \\|_{R^{-1}}^2 + \\lambda_{\\Delta} \\| D \\Delta x \\|_2^2.\n$$\n将 $\\hat{x}_1$ 设为 $J_{\\text{SDI},1}$ 的最小化子，将 $\\widehat{\\Delta x}_k$ 设为 $J_{\\text{SDI},\\Delta}$ 的最小化子，并定义最终估计 $\\hat{x}_T = \\hat{x}_1 + \\sum_{k=2}^T \\widehat{\\Delta x}_k$。对于 SDI 分辨率宽度指标，使用时刻 $T$ 的最终变化估计器的模型分辨率，即从 $y_T - y_{T-1}$到 $\\widehat{\\Delta x}_T$ 的线性估计器的分辨率矩阵。\n\n- 联合反演（JI）：将状态堆叠成 $X = [x_1^T,\\dots,x_T^T]^T \\in \\mathbb{R}^{NT}$ 并最小化\n$$\nJ_{\\text{JI}}(X) = \\sum_{k=1}^T \\| y_k - H x_k \\|_{R^{-1}}^2 + \\lambda_s \\sum_{k=1}^T \\| D x_k \\|_2^2 + \\lambda_t \\sum_{k=1}^{T-1} \\| x_{k+1} - x_k \\|_2^2.\n$$\n设 $\\hat{X}$ 表示最小化子，并提取其最后的 N 维块作为 $\\hat{x}_T$。\n\n- 四维变分数据同化（4D-Var）：如上堆叠 $X$ 并最小化\n$$\nJ_{\\text{4D}}(X) = \\sum_{k=1}^T \\| y_k - H x_k \\|_{R^{-1}}^2 + \\beta \\| D x_1 \\|_2^2 + \\sum_{k=1}^{T-1} \\| x_{k+1} - x_k \\|_{Q^{-1}}^2,\n$$\n其中 $Q = q I_N$，因此 $Q^{-1} = \\frac{1}{q} I_N$，$\\beta > 0$，且 $q > 0$。\n\n在最终时刻 $T$ 需要计算的指标：\n- RMSE:\n$$\n\\text{RMSE} = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{x}_{T,i} - x_{T,i}^{\\text{true}} \\right)^2 }.\n$$\n- 平均分辨率宽度：令 $W$ 表示将堆叠数据 $Y = [y_1^T,\\dots,y_T^T]^T$ 映射到所讨论方法的堆叠估计 $\\hat{X}$ 的线性估计器，令 $G$ 表示将 $X$ 映射到 $Y$ 的块观测算子。模型分辨率矩阵为 $R_{\\text{mod}} = W G$。提取将真实 $x_T$ 映射到估计 $\\hat{x}_T$ 的 $N \\times N$ 最终时刻子块 $R_T$。对于每一行 $i$，构建非负权重 $a_{i j} = |(R_T)_{i j}|$，计算二阶中心矩\n$$\nw_i = \\sqrt{ \\frac{\\sum_{j=1}^N a_{i j} (s_j - s_i)^2}{\\sum_{j=1}^N a_{i j}} },\n$$\n并报告平均宽度 $\\frac{1}{N} \\sum_{i=1}^N w_i$。对于 SDI，使用 $R_T$ 作为最终变化估计器 $\\widehat{\\Delta x}_T$ 相对于 $x_T$ 的分辨率（通过 $y_T - y_{T-1}$）。\n- 平均后验不确定性：在线性高斯设定中，堆叠状态的后验协方差为\n$$\n\\Sigma_{\\text{post}} = \\left(G^T R^{-1} G + \\Gamma \\right)^{-1},\n$$\n其中 $\\Gamma$ 是每种方法所隐含的先验精度。提取最终时刻的 $N \\times N$ 子块，并报告平均边际后验标准差，即\n$$\n\\frac{1}{N} \\sum_{i=1}^N \\sqrt{ (\\Sigma_{\\text{post},T})_{i i} }.\n$$\n对于 SDI，将最终时刻的后验方差近似为 $\\hat{x}_1$ 和 $k \\in \\{2,\\dots,T\\}$ 的 $\\widehat{\\Delta x}_k$ 的边际后验方差之和，这是基于独立数据导致这些高斯估计器相互独立的假设；报告此总和对角线元素的平方根的平均值。\n\n您的程序必须通过构建并求解从上述最小化子推导出的相应正规方程来实现这三个估计器。使用以下三个测试用例，每个用例由 $(M, \\sigma, \\lambda_s, \\lambda_{\\Delta}, \\lambda_t, \\beta, q)$ 指定：\n- 测试用例 1: $(20, 0.01, 0.01, 0.01, 0.01, 0.01, 0.005)$。\n- 测试用例 2: $(20, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01)$。\n- 测试用例 3: $(10, 0.02, 0.01, 0.01, 0.005, 0.01, 0.005)$。\n\n所有量都是无量纲的，因此不需要进行物理单位转换。角度不出现。百分比不出现。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的汇总结果。对于每个测试用例（按 1、2、3 的顺序），按此顺序附加九个浮点数：$\\text{RMSE}_{\\text{SDI}}$、$\\text{ResWidth}_{\\text{SDI}}$、$\\text{PostStd}_{\\text{SDI}}$、$\\text{RMSE}_{\\text{JI}}$、$\\text{ResWidth}_{\\text{JI}}$、$\\text{PostStd}_{\\text{JI}}$、$\\text{RMSE}_{\\text{4D}}$、$\\text{ResWidth}_{\\text{4D}}$、$\\text{PostStd}_{\\text{4D}}$。因此，最终输出行在一个列表中包含 27 个浮点数，顺序完全符合规定。", "solution": "问题的核心在于找到三种不同的 Tikhonov 型二次代价函数的最小化子，这对应于求解一个线性正规方程组。从这些解中，我们推导出估计器以及相关的统计和分辨率指标。\n\n设堆叠状态向量为 $X = [x_1^T, \\dots, x_T^T]^T \\in \\mathbb{R}^{NT}$，堆叠数据向量为 $Y = [y_1^T, \\dots, y_T^T]^T \\in \\mathbb{R}^{MT}$。正演模型可以写成 $Y = \\mathcal{H} X + \\mathcal{E}$，其中 $\\mathcal{H} = I_T \\otimes H$ 是块对角观测算子，$\\mathcal{E}$ 是堆叠噪声向量，其协方差为 $\\mathcal{R} = I_T \\otimes R = \\sigma^2 I_{MT}$。\n\n在线性高斯框架中，后验概率密度函数的一般形式为 $p(X|Y) \\propto \\exp(-\\frac{1}{2} J(X))$，其中 $J(X)$ 是目标函数。目标函数是数据失配项和先验项的和：\n$$\nJ(X) = (Y - \\mathcal{H}X)^T \\mathcal{R}^{-1} (Y - \\mathcal{H}X) + (X - X_{\\text{prior}})^T \\Gamma (X - X_{\\text{prior}})\n$$\n其中 $X_{\\text{prior}}$ 是先验均值（此处假定为零），$\\Gamma$ 是先验精度（协方差之逆）矩阵。$J(X)$ 的最小化子，即最大后验 (MAP) 估计 $\\hat{X}$，可通过将梯度 $\\nabla_X J(X)$ 设为零求得。这引出了正规方程：\n$$\n(\\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H} + \\Gamma) \\hat{X} = \\mathcal{H}^T \\mathcal{R}^{-1} Y\n$$\n$\\frac{1}{2} J(X)$ 的 Hessian 矩阵是后验精度矩阵 $\\Sigma_{\\text{post}}^{-1} = (\\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H} + \\Gamma)$。后验协方差是其逆矩阵，$\\Sigma_{\\text{post}}$。在没有噪声的情况下，将真实状态 $X^{\\text{true}}$ 映射到估计 $\\hat{X}$ 的模型分辨率矩阵是 $R_{\\text{mod}} = \\Sigma_{\\text{post}} \\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H}$。\n\n我们现在将此框架应用于这三种方法。\n\n**1. 稳定化差异反演 (SDI)**\n\nSDI 按顺序操作。首先，计算初始状态 $x_1$ 的估计。然后，为后续时刻计算变化量 $\\Delta x_k = x_k - x_{k-1}$ 的估计。\n\n初始状态 $x_1$ 的代价函数为：\n$$\nJ_{\\text{SDI},1}(x_1) = \\| y_1 - H x_1 \\|_{R^{-1}}^2 + \\lambda_s \\| D x_1 \\|_2^2 = \\frac{1}{\\sigma^2} \\| y_1 - H x_1 \\|_2^2 + \\lambda_s x_1^T D^T D x_1\n$$\n这对应于一个先验精度 $\\Gamma_1 = \\lambda_s D^T D$。正规方程是：\n$$\n\\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_s D^T D\\right) \\hat{x}_1 = \\frac{1}{\\sigma^2} H^T y_1\n$$\n$x_1$ 的后验协方差是 $\\Sigma_{\\text{post},1} = \\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_s D^T D\\right)^{-1}$。\n\n对于 $k \\in \\{2, \\dots, T\\}$，变化量 $\\Delta x_k$ 通过最小化以下函数来估计：\n$$\nJ_{\\text{SDI},\\Delta}(\\Delta x) = \\| (y_k - y_{k-1}) - H \\Delta x \\|_{R^{-1}}^2 + \\lambda_{\\Delta} \\| D \\Delta x \\|_2^2 = \\frac{1}{\\sigma^2} \\| \\Delta y_k - H \\Delta x \\|_2^2 + \\lambda_{\\Delta} \\Delta x^T D^T D \\Delta x\n$$\n其中 $\\Delta y_k = y_k - y_{k-1}$。估计 $\\widehat{\\Delta x}_k$ 的正规方程是：\n$$\n\\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_{\\Delta} D^T D\\right) \\widehat{\\Delta x}_k = \\frac{1}{\\sigma^2} H^T \\Delta y_k\n$$\n$\\Delta x_k$ 的后验协方差是 $\\Sigma_{\\text{post},\\Delta} = \\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_{\\Delta} D^T D\\right)^{-1}$。注意，此公式隐式地假设 $\\Delta y_k$ 上的噪声协方差为 $R$，而实际上是 $2R$。我们遵循问题的定义。\n\n最终状态估计是 $\\hat{x}_T = \\hat{x}_1 + \\sum_{k=2}^T \\widehat{\\Delta x}_k$。\n最终时刻的后验协方差通过将独立估计器的协方差相加来近似：\n$$\n\\Sigma_{\\text{post},T}^{\\text{SDI}} \\approx \\Sigma_{\\text{post},1} + \\sum_{k=2}^T \\Sigma_{\\text{post},\\Delta} = \\Sigma_{\\text{post},1} + (T-1)\\Sigma_{\\text{post},\\Delta}\n$$\n最终变化估计器 $\\widehat{\\Delta x}_T$ 相对于 $x_T$ 的模型分辨率矩阵是：\n$$\nR_T^{\\text{SDI}} = \\Sigma_{\\text{post},\\Delta} \\left(\\frac{1}{\\sigma^2} H^T H\\right)\n$$\n\n**2. 联合反演 (JI)**\n\nJI 同时估计所有状态 $X = [x_1^T, \\dots, x_T^T]^T$。代价函数是：\n$$\nJ_{\\text{JI}}(X) = \\sum_{k=1}^T \\frac{1}{\\sigma^2}\\| y_k - H x_k \\|_2^2 + \\lambda_s \\sum_{k=1}^T \\| D x_k \\|_2^2 + \\lambda_t \\sum_{k=1}^{T-1} \\| x_{k+1} - x_k \\|_2^2\n$$\n这可以写成矩阵形式：\n$$\nJ_{\\text{JI}}(X) = \\frac{1}{\\sigma^2}\\|Y - \\mathcal{H}X\\|_2^2 + \\lambda_s X^T (I_T \\otimes D^T D) X + \\lambda_t X^T (T_{\\text{op}}^T T_{\\text{op}}) X\n$$\n其中 $T_{\\text{op}}$ 是时间一阶差分算子。先验精度矩阵是 $\\Gamma_{\\text{JI}} = \\lambda_s (I_T \\otimes D^T D) + \\lambda_t (T_{\\text{op}}^T T_{\\text{op}})$。正规方程是：\n$$\n\\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{JI}} \\right) \\hat{X} = \\frac{1}{\\sigma^2} \\mathcal{H}^T Y\n$$\n令 $\\mathcal{A}_{\\text{JI}} = \\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{JI}} \\right)$。整个状态历史的后验协方差是 $\\Sigma_{\\text{post}}^{\\text{JI}} = \\mathcal{A}_{\\text{JI}}^{-1}$。模型分辨率矩阵是 $R_{\\text{mod}}^{\\text{JI}} = \\Sigma_{\\text{post}}^{\\text{JI}} \\left(\\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H}\\right)$。我们从 $\\Sigma_{\\text{post}}^{\\text{JI}}$ 和 $R_{\\text{mod}}^{\\text{JI}}$ 中提取最终时刻的 $N \\times N$ 子块来计算指标。\n\n**3. 四维变分数据同化 (4D-Var)**\n\n4D-Var 与 JI 类似，但使用不同的先验结构，通常由状态空间模型提供信息：\n$$\nJ_{\\text{4D}}(X) = \\sum_{k=1}^T \\frac{1}{\\sigma^2}\\| y_k - H x_k \\|_2^2 + \\beta \\| D x_1 \\|_2^2 + \\sum_{k=1}^{T-1} \\frac{1}{q} \\| x_{k+1} - x_k \\|_2^2\n$$\n先验惩罚了初始状态 $x_1$ 的粗糙度以及时间变化 $x_{k+1}-x_k$。在矩阵形式中，先验精度矩阵是 $\\Gamma_{\\text{4D}} = \\beta \\text{diag}(D^T D, 0, \\dots, 0) + \\frac{1}{q} T_{\\text{op}}^T T_{\\text{op}}$。正规方程是：\n$$\n\\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{4D}} \\right) \\hat{X} = \\frac{1}{\\sigma^2} \\mathcal{H}^T Y\n$$\n令 $\\mathcal{A}_{\\text{4D}} = \\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{4D}} \\right)$。后验协方差是 $\\Sigma_{\\text{post}}^{\\text{4D}} = \\mathcal{A}_{\\text{4D}}^{-1}$，模型分辨率矩阵是 $R_{\\text{mod}}^{\\text{4D}} = \\Sigma_{\\text{post}}^{\\text{4D}} \\left(\\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H}\\right)$。同样，我们提取最终时刻的子块进行指标计算。\n\n**指标计算**\n\n对于每种方法，为最终时间步 $T$ 计算三个要求的指标：\n- **RMSE**：使用估计的最终状态 $\\hat{x}_T$ 和真实状态 $x_T^{\\text{true}}$直接计算。\n- **平均分辨率宽度**：对于给定的最终时刻模型分辨率矩阵 $R_T$，每个网格点 $i$ 的宽度计算为网格坐标的加权标准差，其中权重是 $R_T$ 第 $i$ 行的绝对值。最终指标是这些宽度在所有网格点上的平均值。\n- **平均后验不确定性**：对于给定的最终时刻后验协方差矩阵 $\\Sigma_{\\text{post}, T}$，每个网格点 $i$ 的不确定性是相应对角元素（边际后验方差）的平方根。最终指标是这些标准差的平均值。\n\n以下 Python 程序实现了这些推导，以求解估计器并为每个测试用例计算指定的指标。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef calculate_average_resolution_width(R_T, s):\n    \"\"\"Computes the average resolution width.\"\"\"\n    N = R_T.shape[0]\n    widths = np.zeros(N)\n    for i in range(N):\n        abs_row = np.abs(R_T[i, :])\n        denominator = np.sum(abs_row)\n        if denominator > np.finfo(float).eps:\n            s_i = s[i]\n            numerator = np.sum(abs_row * (s - s_i)**2)\n            widths[i] = np.sqrt(numerator / denominator)\n        else:\n            widths[i] = 0.0\n    return np.mean(widths)\n\ndef calculate_average_posterior_std(Sigma_T):\n    \"\"\"Computes the average posterior standard deviation.\"\"\"\n    variances = np.diag(Sigma_T)\n    # Ensure variances are non-negative before taking sqrt\n    return np.mean(np.sqrt(np.maximum(0, variances)))\n\ndef solve():\n    \"\"\"\n    Main function to solve the time-lapse inverse problem for the given test cases.\n    \"\"\"\n    N = 20\n    T = 3\n    sigma_h = 0.05\n    \n    # Use a fixed seed for reproducibility of synthetic noise\n    rng = np.random.default_rng(0)\n\n    # Spatial grid\n    s = np.linspace(0, 1, N)\n\n    # True model generation\n    x_true = np.zeros((T, N))\n    x_true[0, :] = 0.1 * np.sin(2 * np.pi * s) + np.exp(-(s - 0.3)**2 / (2 * 0.05**2))\n    x_true[1, :] = 0.1 * np.sin(2 * np.pi * s) + 0.8 * np.exp(-(s - 0.5)**2 / (2 * 0.05**2))\n    x_true[2, :] = 0.1 * np.sin(2 * np.pi * s) + 0.6 * np.exp(-(s - 0.7)**2 / (2 * 0.05**2))\n\n    # Spatial difference operator D\n    D = np.eye(N) - np.roll(np.eye(N), 1, axis=0)\n    D[0, N-1] = -1\n    DtD = D.T @ D\n    \n    # Temporal difference operator T_op\n    T_op_blocks = []\n    for k in range(T - 1):\n        row = [np.zeros((N, N))] * T\n        row[k] = -np.eye(N)\n        row[k+1] = np.eye(N)\n        T_op_blocks.append(np.hstack(row))\n    T_op = np.vstack(T_op_blocks)\n    T_op_t_T_op = T_op.T @ T_op\n\n    test_cases = [\n        # (M, sigma, lambda_s, lambda_delta, lambda_t, beta, q)\n        (20, 0.01, 0.01, 0.01, 0.01, 0.01, 0.005),\n        (20, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01),\n        (10, 0.02, 0.01, 0.01, 0.005, 0.01, 0.005),\n    ]\n\n    results = []\n\n    for M, sigma, lambda_s, lambda_delta, lambda_t, beta, q in test_cases:\n        # Observation operator H\n        r = np.linspace(0, 1, M)\n        H = np.exp(-(s[np.newaxis, :] - r[:, np.newaxis])**2 / (2 * sigma_h**2))\n        HtH = H.T @ H\n\n        # Generate noisy data\n        y = np.zeros((T, M))\n        for k in range(T):\n            noise = rng.normal(0, sigma, M)\n            y[k, :] = H @ x_true[k, :] + noise\n\n        # --- 1. Stabilized Difference Inversion (SDI) ---\n        # Estimators\n        A1_sdi = (1/sigma**2) * HtH + lambda_s * DtD\n        A_delta_sdi = (1/sigma**2) * HtH + lambda_delta * DtD\n        \n        inv_A1_sdi = np.linalg.inv(A1_sdi)\n        inv_A_delta_sdi = np.linalg.inv(A_delta_sdi)\n        \n        rhs1 = (1/sigma**2) * H.T @ y[0]\n        x1_hat_sdi = inv_A1_sdi @ rhs1\n        \n        delta_x_hat_sum = np.zeros(N)\n        for k in range(1, T):\n            delta_y = y[k] - y[k-1]\n            rhs_delta = (1/sigma**2) * H.T @ delta_y\n            delta_x_hat_k = inv_A_delta_sdi @ rhs_delta\n            delta_x_hat_sum += delta_x_hat_k\n            \n        xT_hat_sdi = x1_hat_sdi + delta_x_hat_sum\n        \n        # Metrics for SDI\n        rmse_sdi = np.sqrt(np.mean((xT_hat_sdi - x_true[T-1, :])**2))\n        \n        Sigma_post_1_sdi = inv_A1_sdi\n        Sigma_post_delta_sdi = inv_A_delta_sdi\n        Sigma_post_T_sdi = Sigma_post_1_sdi + (T-1) * Sigma_post_delta_sdi\n        post_std_sdi = calculate_average_posterior_std(Sigma_post_T_sdi)\n\n        R_T_sdi = Sigma_post_delta_sdi @ ((1/sigma**2) * HtH)\n        res_width_sdi = calculate_average_resolution_width(R_T_sdi, s)\n        \n        results.extend([rmse_sdi, res_width_sdi, post_std_sdi])\n        \n        # --- Common matrices for JI and 4D-Var ---\n        NT = N * T\n        H_cal = block_diag(*([H] * T))\n        H_cal_T_H_cal = block_diag(*([HtH] * T))\n        H_cal_T_y = np.concatenate([H.T @ yk for yk in y])\n\n        # --- 2. Joint Inversion (JI) ---\n        Gamma_JI = lambda_s * block_diag(*([DtD] * T)) + lambda_t * T_op_t_T_op\n        A_JI = (1/sigma**2) * H_cal_T_H_cal + Gamma_JI\n        rhs_JI = (1/sigma**2) * H_cal_T_y\n        \n        X_hat_ji = np.linalg.solve(A_JI, rhs_JI)\n        xT_hat_ji = X_hat_ji[(T-1)*N:]\n        \n        # Metrics for JI\n        rmse_ji = np.sqrt(np.mean((xT_hat_ji - x_true[T-1,:])**2))\n        \n        Sigma_post_JI = np.linalg.inv(A_JI)\n        Sigma_post_T_ji = Sigma_post_JI[(T-1)*N:, (T-1)*N:]\n        post_std_ji = calculate_average_posterior_std(Sigma_post_T_ji)\n        \n        R_mod_JI = Sigma_post_JI @ ((1/sigma**2) * H_cal_T_H_cal)\n        R_TT_ji = R_mod_JI[(T-1)*N:, (T-1)*N:]\n        res_width_ji = calculate_average_resolution_width(R_TT_ji, s)\n        \n        results.extend([rmse_ji, res_width_ji, post_std_ji])\n        \n        # --- 3. Four-Dimensional Variational (4D-Var) ---\n        D1tD1 = np.zeros((NT, NT))\n        D1tD1[:N, :N] = DtD\n        Gamma_4D = beta * D1tD1 + (1/q) * T_op_t_T_op\n        A_4D = (1/sigma**2) * H_cal_T_H_cal + Gamma_4D\n        rhs_4D = (1/sigma**2) * H_cal_T_y\n\n        X_hat_4d = np.linalg.solve(A_4D, rhs_4D)\n        xT_hat_4d = X_hat_4d[(T-1)*N:]\n\n        # Metrics for 4D-Var\n        rmse_4d = np.sqrt(np.mean((xT_hat_4d - x_true[T-1,:])**2))\n\n        Sigma_post_4D = np.linalg.inv(A_4D)\n        Sigma_post_T_4d = Sigma_post_4D[(T-1)*N:, (T-1)*N:]\n        post_std_4d = calculate_average_posterior_std(Sigma_post_T_4d)\n        \n        R_mod_4D = Sigma_post_4D @ ((1/sigma**2) * H_cal_T_H_cal)\n        R_TT_4d = R_mod_4D[(T-1)*N:, (T-1)*N:]\n        res_width_4d = calculate_average_resolution_width(R_TT_4d, s)\n        \n        results.extend([rmse_4d, res_width_4d, post_std_4d])\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3427721"}, {"introduction": "除了简单地对地下进行成像，时移数据通常还用于回答一个具体问题：在感兴趣的区域是否发生了显著变化？这个练习 [@problem_id:3427712] 引入了一种强大的概率方法，使用贝叶斯模型选择来量化变化的证据。你将推导并计算贝叶斯因子——一个用于比较假设的关键指标，并探索它在不同噪声水平和先验假设下的行为。", "problem": "考虑一个线性化的时移反演设定，其中两次采集时间之间的差异数据 $\\mathbf{d} \\in \\mathbb{R}^{n}$ 被建模为 $\\mathbf{d} = \\mathbf{G}_S \\mathbf{u} + \\mathbf{G}_C \\mathbf{v} + \\boldsymbol{\\eta}$，其中 $\\mathbf{G}_S \\in \\mathbb{R}^{n \\times p_S}$ 映射子区域参数 $\\mathbf{u} \\in \\mathbb{R}^{p_S}$，$\\mathbf{G}_C \\in \\mathbb{R}^{n \\times p_C}$ 映射互补区域参数 $\\mathbf{v} \\in \\mathbb{R}^{p_C}$，并且 $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$ 是加性高斯噪声，其协方差 $\\mathbf{R} \\in \\mathbb{R}^{n \\times n}$ 是对称正定的。我们为子区域的变化检测定义了两个假设：$H_0$ 强制子区域中的 $\\Delta m_t$ 精确为零，这等同于 $\\mathbf{u} = \\mathbf{0}$，而在 $H_1$ 下，子区域中的 $\\Delta m_t$ 不被约束为零，这等同于 $\\mathbf{u}$ 是未知的。在这两种假设下，互补区域的变化 $\\mathbf{v}$ 被视为一个无关参数，并被建模为一个先验分布为 $\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C}_v)$ 的高斯随机向量，其中 $\\mathbf{C}_v \\in \\mathbb{R}^{p_C \\times p_C}$ 是对称正定的。在 $H_1$ 下，子区域的变化由高斯先验 $\\mathbf{u} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C}_u)$ 建模，其中 $\\mathbf{C}_u \\in \\mathbb{R}^{p_S \\times p_S}$ 是对称正定的。在 $H_0$ 下，$\\mathbf{u}$ 被固定为 $\\mathbf{0}$（即一个集中在零点的退化先验）。假设线性正演模型和高斯先验均有效，使用贝叶斯模型选择来推导贝叶斯因子 $\\mathcal{K}$，其定义为边际似然的比值 $\\mathcal{K} = \\dfrac{p(\\mathbf{d} \\mid H_1)}{p(\\mathbf{d} \\mid H_0)}$，通过在适用情况下对无关参数 $\\mathbf{u}$ 和 $\\mathbf{v}$ 进行解析积分。然后实现一个程序，为以下测试套件计算 $\\mathcal{K}$，并讨论贝叶斯因子如何随逆噪声协方差 $\\mathbf{R}^{-1}$ 伸缩。\n\n您必须从线性高斯模型和贝叶斯证据的基本原则出发，而不使用超出这些基础的专门捷径。所给的数量、维度和数值在科学上是合理的且自洽的。此问题不涉及物理单位，因此无需进行单位转换。\n\n为每个测试用例定义贝叶斯因子，参数集如下。所有矩阵都已明确给出。为清晰起见，所有情况下的维度均为 $n = 3$，$p_S = 1$，$p_C = 2$。\n\n- 测试用例 $1$（中等噪声，信息丰富的先验）：\n  $$\\mathbf{G}_S = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix}, \\quad \\mathbf{G}_C = \\begin{bmatrix} 0.2  0.1 \\\\ 0.0  1.0 \\\\ 0.5  0.0 \\end{bmatrix}, \\quad \\mathbf{C}_u = \\begin{bmatrix} 0.25 \\end{bmatrix}, \\quad \\mathbf{C}_v = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.4 \\end{bmatrix}, $$\n  $$\\mathbf{R} = \\begin{bmatrix} 0.1  0.0  0.0 \\\\ 0.0  0.1  0.0 \\\\ 0.0  0.0  0.1 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0.45 \\\\ 0.02 \\\\ 0.26 \\end{bmatrix}.$$\n\n- 测试用例 $2$（高噪声，其余与用例 $1$ 相同）：\n  $$\\mathbf{G}_S = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix}, \\quad \\mathbf{G}_C = \\begin{bmatrix} 0.2  0.1 \\\\ 0.0  1.0 \\\\ 0.5  0.0 \\end{bmatrix}, \\quad \\mathbf{C}_u = \\begin{bmatrix} 0.25 \\end{bmatrix}, \\quad \\mathbf{C}_v = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.4 \\end{bmatrix}, $$\n  $$\\mathbf{R} = \\begin{bmatrix} 10.0  0.0  0.0 \\\\ 0.0  10.0  0.0 \\\\ 0.0  0.0  10.0 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0.45 \\\\ 0.02 \\\\ 0.26 \\end{bmatrix}.$$\n\n- 测试用例 $3$（对子区域变化的先验非常紧）：\n  $$\\mathbf{G}_S = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix}, \\quad \\mathbf{G}_C = \\begin{bmatrix} 0.2  0.1 \\\\ 0.0  1.0 \\\\ 0.5  0.0 \\end{bmatrix}, \\quad \\mathbf{C}_u = \\begin{bmatrix} 0.01 \\end{bmatrix}, \\quad \\mathbf{C}_v = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.4 \\end{bmatrix}, $$\n  $$\\mathbf{R} = \\begin{bmatrix} 0.1  0.0  0.0 \\\\ 0.0  0.1  0.0 \\\\ 0.0  0.0  0.1 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0.45 \\\\ 0.02 \\\\ 0.26 \\end{bmatrix}.$$\n\n- 测试用例 $4$（相关噪声协方差）：\n  $$\\mathbf{G}_S = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix}, \\quad \\mathbf{G}_C = \\begin{bmatrix} 0.2  0.1 \\\\ 0.0  1.0 \\\\ 0.5  0.0 \\end{bmatrix}, \\quad \\mathbf{C}_u = \\begin{bmatrix} 0.25 \\end{bmatrix}, \\quad \\mathbf{C}_v = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.4 \\end{bmatrix}, $$\n  $$\\mathbf{R} = \\begin{bmatrix} 0.2  0.05  0.0 \\\\ 0.05  0.2  0.05 \\\\ 0.0  0.05  0.2 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0.45 \\\\ 0.02 \\\\ 0.26 \\end{bmatrix}.$$\n\n- 测试用例 $5$（不可观测的子区域：子区域中的正演灵敏度为零）：\n  $$\\mathbf{G}_S = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad \\mathbf{G}_C = \\begin{bmatrix} 0.2  0.1 \\\\ 0.0  1.0 \\\\ 0.5  0.0 \\end{bmatrix}, \\quad \\mathbf{C}_u = \\begin{bmatrix} 0.25 \\end{bmatrix}, \\quad \\mathbf{C}_v = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.4 \\end{bmatrix}, $$\n  $$\\mathbf{R} = \\begin{bmatrix} 0.1  0.0  0.0 \\\\ 0.0  0.1  0.0 \\\\ 0.0  0.0  0.1 \\end{bmatrix}, \\quad \\mathbf{d} = \\begin{bmatrix} 0.30 \\\\ -0.10 \\\\ 0.05 \\end{bmatrix}.$$\n\n您的程序必须通过根据线性模型和先验解析地积分掉高斯潜变量来为每个测试用例计算贝叶斯因子 $\\mathcal{K}$。最终输出格式必须是单行，包含一个用方括号括起来的逗号分隔列表的结果，例如 $\\left[ r_1, r_2, r_3, r_4, r_5 \\right]$，其中每个 $r_i$ 是测试用例 $i$ 的贝叶斯因子，表示为标准十进制浮点数。\n\n测试套件涵盖：\n- 一个具有中等噪声和信息丰富先验的一般情况。\n- 一个具有非常高噪声水平的边界条件。\n- 一个对子区域变化具有高度约束性先验的情况。\n- 一个具有相关噪声协方差的情况。\n- 一个重要的边缘情况，即子区域不可观测，这意味着 $\\mathcal{K}$ 应简化为 $1$。\n\n您的程序应生成单行输出，其中包含如上所述的用方括号括起来的逗号分隔列表的结果。", "solution": "问题的核心是计算贝叶斯因子 $\\mathcal{K}$，其定义为两个竞争假设 $H_1$ 和 $H_0$ 的边际似然（或证据）之比：\n$$\n\\mathcal{K} = \\frac{p(\\mathbf{d} \\mid H_1)}{p(\\mathbf{d} \\mid H_0)}\n$$\n对于给定的假设，边际似然是通过在整个参数空间上对数据和参数的联合概率进行积分得到的。\n\n首先，我们推导原假设 $H_0$ 的边际似然。在此假设下，子区域的变化为零，因此模型简化为：\n$$\nH_0: \\mathbf{d} = \\mathbf{G}_C \\mathbf{v} + \\boldsymbol{\\eta}\n$$\n需要积分掉的参数是无关参数 $\\mathbf{v}$，它们被建模为具有先验协方差 $\\mathbf{C}_v$ 的零均值高斯随机向量。噪声 $\\boldsymbol{\\eta}$ 是一个独立的零均值高斯随机向量，协方差为 $\\mathbf{R}$。因此，数据向量 $\\mathbf{d}$ 是独立的零均值高斯变量的线性组合，其本身也是一个零均值高斯变量。其协方差 $\\mathbf{\\Sigma}_0$ 由下式给出：\n$$\n\\mathbb{E}[\\mathbf{d} \\mathbf{d}^T \\mid H_0] = \\mathbb{E}[(\\mathbf{G}_C \\mathbf{v} + \\boldsymbol{\\eta})(\\mathbf{G}_C \\mathbf{v} + \\boldsymbol{\\eta})^T]\n$$\n$$\n= \\mathbb{E}[\\mathbf{G}_C \\mathbf{v} \\mathbf{v}^T \\mathbf{G}_C^T] + \\mathbb{E}[\\boldsymbol{\\eta} \\boldsymbol{\\eta}^T] + \\mathbb{E}[\\mathbf{G}_C \\mathbf{v} \\boldsymbol{\\eta}^T] + \\mathbb{E}[\\boldsymbol{\\eta} \\mathbf{v}^T \\mathbf{G}_C^T]\n$$\n由于 $\\mathbf{v}$ 和 $\\boldsymbol{\\eta}$ 是独立且零均值的，交叉项为零。这简化为：\n$$\n\\mathbf{\\Sigma}_0 = \\mathbf{G}_C \\mathbb{E}[\\mathbf{v} \\mathbf{v}^T] \\mathbf{G}_C^T + \\mathbb{E}[\\boldsymbol{\\eta} \\boldsymbol{\\eta}^T] = \\mathbf{G}_C \\mathbf{C}_v \\mathbf{G}_C^T + \\mathbf{R}\n$$\n$H_0$ 的边际似然是在观测数据 $\\mathbf{d}$ 处评估的该分布的概率密度：\n$$\np(\\mathbf{d} \\mid H_0) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\mathbf{\\Sigma}_0)}} \\exp\\left(-\\frac{1}{2} \\mathbf{d}^T \\mathbf{\\Sigma}_0^{-1} \\mathbf{d}\\right)\n$$\n其中 $n$ 是数据向量 $\\mathbf{d}$ 的维度。\n\n接下来，我们推导备择假设 $H_1$ 的边际似然。在此假设下，子区域变化 $\\mathbf{u}$ 是未知的，并被视为一个随机变量。模型是：\n$$\nH_1: \\mathbf{d} = \\mathbf{G}_S \\mathbf{u} + \\mathbf{G}_C \\mathbf{v} + \\boldsymbol{\\eta}\n$$\n需要积分掉的参数是 $\\mathbf{u}$ 和 $\\mathbf{v}$。$\\mathbf{u}$ 的先验是协方差为 $\\mathbf{C}_u$ 的零均值高斯分布。与 $H_0$ 一样，$\\mathbf{d}$ 是三个独立的零均值高斯向量（$\\mathbf{G}_S\\mathbf{u}$、$\\mathbf{G}_C\\mathbf{v}$ 和 $\\boldsymbol{\\eta}$）的线性组合，因此它本身也是一个零均值高斯分布。其协方差 $\\mathbf{\\Sigma}_1$ 是通过将独立分量的协方差相加得到的：\n$$\n\\mathbf{\\Sigma}_1 = \\text{Cov}(\\mathbf{G}_S \\mathbf{u}) + \\text{Cov}(\\mathbf{G}_C \\mathbf{v}) + \\text{Cov}(\\boldsymbol{\\eta})\n$$\n$$\n\\mathbf{\\Sigma}_1 = \\mathbf{G}_S \\mathbf{C}_u \\mathbf{G}_S^T + \\mathbf{G}_C \\mathbf{C}_v \\mathbf{G}_C^T + \\mathbf{R} = \\mathbf{\\Sigma}_0 + \\mathbf{G}_S \\mathbf{C}_u \\mathbf{G}_S^T\n$$\n因此，$H_1$ 的边际似然是：\n$$\np(\\mathbf{d} \\mid H_1) = \\frac{1}{\\sqrt{(2\\pi)^n \\det(\\mathbf{\\Sigma}_1)}} \\exp\\left(-\\frac{1}{2} \\mathbf{d}^T \\mathbf{\\Sigma}_1^{-1} \\mathbf{d}\\right)\n$$\n\n最后，我们通过取两个边际似然的比值来计算贝叶斯因子 $\\mathcal{K}$：\n$$\n\\mathcal{K} = \\frac{p(\\mathbf{d} \\mid H_1)}{p(\\mathbf{d} \\mid H_0)} = \\frac{\\frac{1}{\\sqrt{(2\\pi)^n \\det(\\mathbf{\\Sigma}_1)}} \\exp\\left(-\\frac{1}{2} \\mathbf{d}^T \\mathbf{\\Sigma}_1^{-1} \\mathbf{d}\\right)}{\\frac{1}{\\sqrt{(2\\pi)^n \\det(\\mathbf{\\Sigma}_0)}} \\exp\\left(-\\frac{1}{2} \\mathbf{d}^T \\mathbf{\\Sigma}_0^{-1} \\mathbf{d}\\right)}\n$$\n$(2\\pi)^n$因子抵消，得到最终表达式：\n$$\n\\mathcal{K} = \\sqrt{\\frac{\\det(\\mathbf{\\Sigma}_0)}{\\det(\\mathbf{\\Sigma}_1)}} \\exp\\left(-\\frac{1}{2} \\left[ \\mathbf{d}^T \\mathbf{\\Sigma}_1^{-1} \\mathbf{d} - \\mathbf{d}^T \\mathbf{\\Sigma}_0^{-1} \\mathbf{d} \\right]\\right)\n$$\n这个公式由两个主要部分组成。项 $\\sqrt{\\det(\\mathbf{\\Sigma}_0)/\\det(\\mathbf{\\Sigma}_1)}$ 是一个复杂度惩罚（一个“奥卡姆因子”）。由于 $\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_0 + \\mathbf{G}_S \\mathbf{C}_u \\mathbf{G}_S^T$ 且 $\\mathbf{G}_S \\mathbf{C}_u \\mathbf{G}_S^T$ 是半正定的，因此 $\\det(\\mathbf{\\Sigma}_1) \\geq \\det(\\mathbf{\\Sigma}_0)$，使得该项小于或等于 $1$。它因模型 $H_1$ 具有额外的灵活性而对其进行惩罚。指数项衡量了两个模型对数据的相对拟合优度。如果数据 $\\mathbf{d}$ 能被 $H_1$ 更好地解释，那么项 $\\mathbf{d}^T \\mathbf{\\Sigma}_1^{-1} \\mathbf{d}$ 将小于 $\\mathbf{d}^T \\mathbf{\\Sigma}_0^{-1} \\mathbf{d}$，使得指数为正，从而支持 $H_1$。\n\n关于 $\\mathcal{K}$ 随逆噪声协方差 $\\mathbf{R}^{-1}$ 的伸缩关系：随着数据精度的增加（即噪声方差减小，因此 $\\mathbf{R}^{-1}$ 增大），对更优模型的证据会被放大。如果数据强烈支持 $H_1$，拟合优度项将占主导地位，$\\mathcal{K}$ 将会增加，可能趋向于无穷大。相反，如果数据与 $H_0$ 一致，拟合优度项将接近或小于 $1$，复杂度惩罚将导致 $\\mathcal{K}$ 小于 $1$（并可能趋近于零）。在噪声非常大（$\\mathbf{R}$ 大，$\\mathbf{R}^{-1}$ 小）的极限情况下，$\\mathbf{\\Sigma}_0$ 和 $\\mathbf{\\Sigma}_1$ 都由 $\\mathbf{R}$ 主导，因此 $\\mathbf{\\Sigma}_1 \\approx \\mathbf{\\Sigma}_0$。因此，行列式比值趋近于 $1$，二次型也变得相似，导致 $\\mathcal{K} \\to 1$。这表明含噪数据无法提供区分模型间的证据。\n\n一个关键的边缘情况是当子区域不可观测时，即 $\\mathbf{G}_S = \\mathbf{0}$。在这种情况下，$\\mathbf{\\Sigma}_1 = \\mathbf{G}_S \\mathbf{C}_u \\mathbf{G}_S^T + \\mathbf{\\Sigma}_0 = \\mathbf{0} + \\mathbf{\\Sigma}_0 = \\mathbf{\\Sigma}_0$。行列式相等，逆协方差矩阵也相同。$\\mathcal{K}$ 的公式正确地简化为 $\\sqrt{1} \\exp(0) = 1$。这在逻辑上是必要的，因为对感兴趣参数不敏感的数据不能提供支持或反对它们的证据。\n\n提供的Python代码实现了这个推导出的公式，用以为每个测试用例计算 $\\mathcal{K}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayes factor K for a series of test cases in a linearized\n    time-lapse inversion problem.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1: moderate noise, informative priors\n        {\n            \"Gs\": np.array([[1.0], [0.0], [0.5]]),\n            \"Gc\": np.array([[0.2, 0.1], [0.0, 1.0], [0.5, 0.0]]),\n            \"Cu\": np.array([[0.25]]),\n            \"Cv\": np.array([[0.5, 0.1], [0.1, 0.4]]),\n            \"R\": np.array([[0.1, 0.0, 0.0], [0.0, 0.1, 0.0], [0.0, 0.0, 0.1]]),\n            \"d\": np.array([[0.45], [0.02], [0.26]]),\n        },\n        # Test Case 2: high noise\n        {\n            \"Gs\": np.array([[1.0], [0.0], [0.5]]),\n            \"Gc\": np.array([[0.2, 0.1], [0.0, 1.0], [0.5, 0.0]]),\n            \"Cu\": np.array([[0.25]]),\n            \"Cv\": np.array([[0.5, 0.1], [0.1, 0.4]]),\n            \"R\": np.array([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]),\n            \"d\": np.array([[0.45], [0.02], [0.26]]),\n        },\n        # Test Case 3: very tight prior on subregion change\n        {\n            \"Gs\": np.array([[1.0], [0.0], [0.5]]),\n            \"Gc\": np.array([[0.2, 0.1], [0.0, 1.0], [0.5, 0.0]]),\n            \"Cu\": np.array([[0.01]]),\n            \"Cv\": np.array([[0.5, 0.1], [0.1, 0.4]]),\n            \"R\": np.array([[0.1, 0.0, 0.0], [0.0, 0.1, 0.0], [0.0, 0.0, 0.1]]),\n            \"d\": np.array([[0.45], [0.02], [0.26]]),\n        },\n        # Test Case 4: correlated noise covariance\n        {\n            \"Gs\": np.array([[1.0], [0.0], [0.5]]),\n            \"Gc\": np.array([[0.2, 0.1], [0.0, 1.0], [0.5, 0.0]]),\n            \"Cu\": np.array([[0.25]]),\n            \"Cv\": np.array([[0.5, 0.1], [0.1, 0.4]]),\n            \"R\": np.array([[0.2, 0.05, 0.0], [0.05, 0.2, 0.05], [0.0, 0.05, 0.2]]),\n            \"d\": np.array([[0.45], [0.02], [0.26]]),\n        },\n        # Test Case 5: unobservable subregion\n        {\n            \"Gs\": np.array([[0.0], [0.0], [0.0]]),\n            \"Gc\": np.array([[0.2, 0.1], [0.0, 1.0], [0.5, 0.0]]),\n            \"Cu\": np.array([[0.25]]),\n            \"Cv\": np.array([[0.5, 0.1], [0.1, 0.4]]),\n            \"R\": np.array([[0.1, 0.0, 0.0], [0.0, 0.1, 0.0], [0.0, 0.0, 0.1]]),\n            \"d\": np.array([[0.30], [-0.10], [0.05]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        Gs = case[\"Gs\"]\n        Gc = case[\"Gc\"]\n        Cu = case[\"Cu\"]\n        Cv = case[\"Cv\"]\n        R = case[\"R\"]\n        d = case[\"d\"]\n\n        # Calculate Sigma_0 = Gc * Cv * Gc^T + R\n        Sigma_0 = Gc @ Cv @ Gc.T + R\n\n        # Calculate Sigma_1 = Gs * Cu * Gs^T + Sigma_0\n        Sigma_1 = Gs @ Cu @ Gs.T + Sigma_0\n\n        # Calculate the determinants\n        det_Sigma_0 = np.linalg.det(Sigma_0)\n        det_Sigma_1 = np.linalg.det(Sigma_1)\n        \n        # Check for numerical stability if determinants are close to zero\n        if det_Sigma_1 == 0:\n            # This would imply a singular predictive covariance under H1.\n            # Depending on context, could be an error or lead to K=0 or infinity.\n            # For this problem, assume matrices are well-behaved.\n            # In a floating point comparison, we might use a small epsilon.\n            # if the ratio is infinity, the value of K would be determined by the exponential.\n            # If Gs=0, det_Sigma_1 == det_Sigma_0, so this path is not taken.\n            # If det_Sigma_0 is also zero, K is indeterminate, but problem setup prevents this.\n            # if only det_Sigma_1 is zero, ratio is infinite, K is likely infinite if exp part > 0.\n            # We will proceed without a special numeric handler as test cases are well-posed.\n            pass\n\n        # Calculate the ratio of determinants term\n        det_ratio = np.sqrt(det_Sigma_0 / det_Sigma_1)\n\n        # Calculate the quadratic forms for the exponential term\n        # Using np.linalg.solve is more numerically stable than direct inversion\n        # For d.T @ inv(A) @ d, we can compute inv(A) @ d first using solve\n        x0 = np.linalg.solve(Sigma_0, d)\n        quad_form_0 = d.T @ x0\n        \n        x1 = np.linalg.solve(Sigma_1, d)\n        quad_form_1 = d.T @ x1\n\n        # Calculate the exponential term\n        exponent = -0.5 * (quad_form_1 - quad_form_0)\n        exp_term = np.exp(exponent)\n\n        # Calculate the Bayes Factor K\n        K = det_ratio * exp_term\n\n        # The quadratic forms are (1,1) arrays, so we extract the scalar\n        results.append(K.item())\n\n    # Format the final output string\n    formatted_results = f\"[{','.join([f'{r:.8f}' for r in results])}]\"\n    print(formatted_results)\n\nsolve()\n```", "id": "3427712"}]}