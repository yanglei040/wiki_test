{"hands_on_practices": [{"introduction": "莫罗佐夫差异原则 (Morozov Discrepancy Principle) 是一种经典的后验参数选择方法，它利用已知的噪声水平来确定正则化程度。[@problem_id:3361699] 这个练习通过一个具体的低维问题，让您亲手计算不同正则化参数 $\\alpha$ 下的吉洪诺夫解 (Tikhonov solution)，并直观地感受残差范数与 $\\alpha$ 之间的关系。这个基础计算旨在巩固您对差异原则核心思想的理解，并为更复杂的数值实现打下坚实基础。", "problem": "考虑由 $y^{\\delta} = A x^{\\dagger} + \\eta$ 描述的带观测噪声的线性反问题，其中 $A \\in \\mathbb{R}^{2 \\times 2}$ 是病态的，$x^{\\dagger} \\in \\mathbb{R}^{2}$ 是未知的真实状态，$\\eta \\in \\mathbb{R}^{2}$ 是噪声。令测量矩阵为\n$$\nA = \\begin{pmatrix}\n1  0 \\\\\n0  10^{-2}\n\\end{pmatrix},\n$$\n且带噪数据为\n$$\ny^{\\delta} = \\begin{pmatrix}\n1 \\\\\n0.2\n\\end{pmatrix}.\n$$\n对于使用单位罚项的吉洪诺夫正则化 (Tikhonov regularization)，定义对于任意 $\\alpha > 0$ 的正则化估计量 $x_{\\alpha}^{\\delta}$ 为\n$$\n\\|A x - y^{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2}\n$$\n的最小化子，并回顾 $x_{\\alpha}^{\\delta}$ 满足正规方程 (normal equations)\n$$\n(A^{\\top} A + \\alpha I) x_{\\alpha}^{\\delta} = A^{\\top} y^{\\delta}.\n$$\n给定正则化参数的候选集\n$$\n\\mathcal{A} = \\{10^{-6}, 10^{-4}, 3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}.\n$$\n执行以下任务：\n- 对于每个 $\\alpha \\in \\mathcal{A}$，计算 $x_{\\alpha}^{\\delta}$ 和残差范数 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$。\n- 使用第一性原理和针对 $A$ 的奇异值分解 (singular value decomposition, SVD) 框架，讨论 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ 是否关于 $\\alpha$ 单调，并严格证明你的结论。\n\n假设已知噪声界 $\\|\\eta\\|_{2} \\le \\delta$，其中 $\\delta = 0.135$，并使用安全因子 $\\tau = 1.1$ 的莫罗佐夫差异原理 (Morozov discrepancy principle)，即选择 $\\alpha^{\\star}$ 使得 $\\|A x_{\\alpha^{\\star}}^{\\delta} - y^{\\delta}\\|_{2} \\approx \\tau \\delta$，并且在离散集 $\\mathcal{A}$ 中，选择满足 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2} \\ge \\tau \\delta$ 的最小 $\\alpha$。将选定的值 $\\alpha^{\\star}$ 作为你的最终答案。最终答案必须是一个实数值。", "solution": "问题陈述经审查有效。其科学基础是成熟的线性反问题吉洪诺夫正则化理论，问题本身是适定的 (well-posed)，提供了所有必要信息，并以客观、数学上精确的语言表述。\n\n问题的核心是分析线性系统 $y^{\\delta} = A x^{\\dagger} + \\eta$ 的吉洪诺夫正则化解 $x_{\\alpha}^{\\delta}$。正则化解是泛函 $\\|A x - y^{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2}$ 的最小化子，并由正规方程 $(A^{\\top} A + \\alpha I) x_{\\alpha}^{\\delta} = A^{\\top} y^{\\delta}$ 的解给出。\n\n**第一部分：正则化解和残差的计算**\n\n首先，我们根据问题陈述定义矩阵和向量。\n矩阵 $A$ 和数据向量 $y^{\\delta}$ 为：\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  10^{-2} \\end{pmatrix}, \\quad y^{\\delta} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\n$$\n由于 $A$ 是对称矩阵，所以 $A^{\\top} = A$。我们计算正规方程的各个组成部分：\n$$\nA^{\\top}A = A^2 = \\begin{pmatrix} 1^2  0 \\\\ 0  (10^{-2})^2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  10^{-4} \\end{pmatrix}\n$$\n$$\nA^{\\top}y^{\\delta} = Ay^{\\delta} = \\begin{pmatrix} 1  0 \\\\ 0  10^{-2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n正规方程变为：\n$$\n\\left( \\begin{pmatrix} 1  0 \\\\ 0  10^{-4} \\end{pmatrix} + \\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) x_{\\alpha}^{\\delta} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1+\\alpha  0 \\\\ 0  10^{-4}+\\alpha \\end{pmatrix} x_{\\alpha}^{\\delta} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n对于任意 $\\alpha > 0$，左侧的矩阵是对角且可逆的，从而得到解：\n$$\nx_{\\alpha}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+\\alpha} \\end{pmatrix}\n$$\n残差为 $r_{\\alpha}^{\\delta} = A x_{\\alpha}^{\\delta} - y^{\\delta}$。残差的平方范数为：\n$$\n\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{1}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-5}}{10^{-4}+\\alpha} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{-\\alpha}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-5} - 0.2(10^{-4}+\\alpha)}{10^{-4}+\\alpha} \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{-\\alpha}{1+\\alpha} \\\\ \\frac{-0.2\\alpha}{10^{-4}+\\alpha} \\end{pmatrix}\\right\\|_{2}^{2}\n$$\n$$\n\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}^{2} = \\alpha^2 \\left( \\frac{1}{(1+\\alpha)^2} + \\frac{0.04}{(10^{-4}+\\alpha)^2} \\right)\n$$\n我们对候选集 $\\mathcal{A} = \\{10^{-6}, 10^{-4}, 3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}$ 中的每个 $\\alpha$ 计算 $x_{\\alpha}^{\\delta}$ 和 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$。\n\n对于 $\\alpha = 10^{-6}$：\n$x_{10^{-6}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-6}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-6}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.000001} \\\\ \\frac{200}{10.1} \\end{pmatrix}$。\n$\\|A x_{10^{-6}}^{\\delta} - y^{\\delta}\\|_{2} \\approx 0.00198$。\n\n对于 $\\alpha = 10^{-4}$：\n$x_{10^{-4}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-4}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.0001} \\\\ 10 \\end{pmatrix}$。\n$\\|A x_{10^{-4}}^{\\delta} - y^{\\delta}\\|_{2} \\approx 0.10000005$。\n\n对于 $\\alpha = 3 \\times 10^{-4}$：\n$x_{3 \\times 10^{-4}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+3 \\times 10^{-4}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+3 \\times 10^{-4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.0003} \\\\ 5 \\end{pmatrix}$。\n$\\|A x_{3 \\times 10^{-4}}^{\\delta} - y^{\\delta}\\|_{2} \\approx 0.1500003$。\n\n对于 $\\alpha = 10^{-3}$：\n$x_{10^{-3}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-3}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.001} \\\\ \\frac{20}{11} \\end{pmatrix}$。\n$\\|A x_{10^{-3}}^{\\delta} - y^{\\delta}\\|_{2} \\approx 0.1818$。\n\n对于 $\\alpha = 10^{-2}$：\n$x_{10^{-2}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-2}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.01} \\\\ \\frac{2}{10.1} \\end{pmatrix}$。\n$\\|A x_{10^{-2}}^{\\delta} - y^{\\delta}\\|_{2} \\approx 0.1983$。\n\n**第二部分：残差范数的单调性**\n\n我们严格证明残差范数 $\\rho(\\alpha) = \\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ 是 $\\alpha$ 的单调函数。考虑 $A$ 的奇异值分解 $A = U \\Sigma V^{\\top}$，其中奇异值为 $\\sigma_i > 0$。残差范数的平方为 $\\rho(\\alpha)^2 = \\alpha^2 \\sum_{i} \\frac{(u_i^{\\top} y^{\\delta})^2}{(\\sigma_i^2+\\alpha)^2}$。对 $\\alpha$ 求导：\n$$\n\\frac{d}{d\\alpha} \\rho(\\alpha)^2 = \\sum_i (u_i^{\\top} y^{\\delta})^2 \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} \\right)\n$$\n内项的导数为：\n$$\n\\frac{d}{d\\alpha} \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} = \\frac{2\\alpha(\\sigma_i^2+\\alpha)^2 - \\alpha^2 \\cdot 2(\\sigma_i^2+\\alpha)(1)}{(\\sigma_i^2+\\alpha)^4} = \\frac{2\\alpha(\\sigma_i^2+\\alpha) - 2\\alpha^2}{(\\sigma_i^2+\\alpha)^3} = \\frac{2\\alpha\\sigma_i^2}{(\\sigma_i^2+\\alpha)^3}\n$$\n因此，残差范数平方的导数为：\n$$\n\\frac{d}{d\\alpha} \\rho(\\alpha)^2 = \\sum_i (u_i^{\\top} y^{\\delta})^2 \\frac{2\\alpha\\sigma_i^2}{(\\sigma_i^2+\\alpha)^3}\n$$\n对于 $\\alpha > 0$，此和中的每一项都是非负的。只要至少存在一个索引 $i$ 使得 $\\sigma_i > 0$ 且数据分量 $u_i^{\\top} y^{\\delta} \\neq 0$，该导数就严格为正。在本问题中，$A$ 是对角矩阵，因此其奇异值为 $\\sigma_1=1$ 和 $\\sigma_2=10^{-2}$，且奇异向量 $u_i$ 是标准基向量 $e_i$。数据分量为 $e_1^{\\top} y^{\\delta} = 1 \\neq 0$ 和 $e_2^{\\top} y^{\\delta} = 0.2 \\neq 0$。因此，对于所有 $\\alpha > 0$，都有 $\\frac{d}{d\\alpha} \\rho(\\alpha)^2 > 0$。\n这证明了 $\\rho(\\alpha)^2$ 是关于 $\\alpha$ 的严格单调递增函数。由于 $\\rho(\\alpha)$ 是非负的，且平方根函数对于非负参数是严格递增的，因此 $\\rho(\\alpha)$ 也是关于 $\\alpha$ 的严格单调递增函数。\n\n**第三部分：莫罗佐夫差异原理**\n\n莫罗佐夫差异原理要求从离散集 $\\mathcal{A}$ 中选择满足 $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2} \\ge \\tau \\delta$ 的最小 $\\alpha$。\n给定 $\\delta = 0.135$ 和 $\\tau = 1.1$，目标差异值为：\n$$\n\\tau \\delta = 1.1 \\times 0.135 = 0.1485\n$$\n我们对计算出的残差范数检查此条件：\n- $\\alpha = 10^{-6}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.00198  0.1485$。\n- $\\alpha = 10^{-4}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.10000005  0.1485$。\n- $\\alpha = 3 \\times 10^{-4}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1500003 \\ge 0.1485$。\n- $\\alpha = 10^{-3}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1818 \\ge 0.1485$。\n- $\\alpha = 10^{-2}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1983 \\ge 0.1485$。\n\n满足条件的参数是 $\\{3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}$。规则是选择其中最小的一个。\n因此，选定的正则化参数是 $\\alpha^{\\star} = 3 \\times 10^{-4}$。", "answer": "$$\\boxed{3 \\times 10^{-4}}$$", "id": "3361699"}, {"introduction": "在许多实际问题中，噪声水平是未知的，这使得差异原则无法直接应用。L-曲线法是一种流行的启发式方法，它通过在解范数与残差范数的对数-对数图中寻找“拐角”来选择参数，以达到数据拟合和解稳定性之间的最佳平衡。[@problem_id:3361732] 在这个编码实践中，您将实现L-曲线的构建，并通过计算曲率来定位其“拐角”，从而选定正则化参数 $\\alpha$。通过将L-曲线法的结果与差异原则的结果进行对比，您将能更深刻地理解这两种方法的优势与局限。", "problem": "给定一个合成的小型线性逆问题族，其具有预先设定的奇异值结构和系数，需通过吉洪诺夫正则化（Tikhonov regularization）进行处理。对于每种情况，您必须计算 L 曲线，估计吉洪诺夫正则化参数的 L 曲线角点，并将其与通过莫洛佐夫差异原则（Morozov Discrepancy Principle, MDP）获得的选择进行比较。\n\n所有问题都在以下纯数学背景下定义。考虑一个具有已知奇异值分解（SVD）基的线性系统，为简化起见，该基特化为单位矩阵，因此前向算子是对角的。设前向算子由一个对角矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 给出，其对角元等于预设的奇异值 $\\{\\sigma_i\\}_{i=1}^n$，并设真实未知向量在右奇异向量基中以系数 $\\{c_i\\}_{i=1}^n$ 表示。那么，干净数据由 $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$ 给出，其中 $x_{\\mathrm{true}}$ 在该基中的坐标为 $c_i$。定义一个确定性噪声方向 $d \\in \\mathbb{R}^n$ 和一个噪声水平 $\\delta  0$。设 $b = b_{\\mathrm{clean}} + \\eta$，其中 $\\eta = \\delta \\, d / \\lVert d \\rVert_2$，因此 $\\lVert \\eta \\rVert_2 = \\delta$。\n\n对于使用单位罚项的吉洪诺夫正则化，给定参数 $\\alpha  0$ 的解 $x_\\alpha$ 最小化 $\\lVert A x - b \\rVert_2^2 + \\alpha^2 \\lVert x \\rVert_2^2$。在左右奇异向量矩阵均为单位矩阵的 SVD 基中，解可以使用数据坐标 $\\beta_i = b_i$ 表示为\n$$\nx_\\alpha = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} \\, \\beta_i \\, e_i,\n$$\n其中 $e_i$ 是第 $i$ 个标准基向量。残差范数和解范数可以通过以下方式计算\n$$\n\\lVert A x_\\alpha - b \\rVert_2^2 = \\sum_{i=1}^n \\left(\\frac{\\alpha^2}{\\sigma_i^2 + \\alpha^2}\\right)^2 \\beta_i^2, \\quad\n\\lVert x_\\alpha \\rVert_2^2 = \\sum_{i=1}^n \\left(\\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2}\\right)^2 \\beta_i^2.\n$$\n\nL 曲线是当 $\\alpha$ 变化时，点 $\\left(\\log \\lVert A x_\\alpha - b \\rVert_2, \\log \\lVert x_\\alpha \\rVert_2\\right)$ 的参数曲线。L 曲线的“角点”可以通过最大化该平面参数曲线的曲率来估计。如果我们定义参数化 $t = \\log \\alpha$，$X(t) = \\log \\lVert A x_{e^t} - b \\rVert_2$ 和 $Y(t) = \\log \\lVert x_{e^t} \\rVert_2$，则在 $t$ 处的曲率为\n$$\n\\kappa(t) = \\frac{X'(t)Y''(t) - Y'(t)X''(t)}{\\left( \\left(X'(t)\\right)^2 + \\left(Y'(t)\\right)^2 \\right)^{3/2}}.\n$$\n在实践中，您应使用二阶中心有限差分在 $t$ 值的离散网格上评估曲率，并选择具有最大绝对曲率的网格点上的参数 $\\alpha$。\n\n莫洛佐夫差异原则（MDP）选择正则化参数 $\\alpha$，使得残差范数与噪声水平相匹配，即\n$$\n\\lVert A x_\\alpha - b \\rVert_2 = \\delta.\n$$\n对于此处考虑的满列秩对角算子，残差范数是 $\\alpha \\in [0,\\infty)$ 上的严格递增函数，其极限在 $\\alpha \\to 0^+$ 时为 $0$，在 $\\alpha \\to \\infty$ 时为 $\\lVert b \\rVert_2$。因此，对于任何 $\\delta \\in (0, \\lVert b \\rVert_2)$，都存在唯一的 $\\alpha$。\n\n您的任务是实现一个程序，对下面的每个测试用例执行以下步骤：\n1. 使用指定的 $\\{\\sigma_i\\}$、$\\{c_i\\}$、噪声方向 $d$ 和噪声分数 $f$（其中 $\\delta = f \\, \\lVert b_{\\mathrm{clean}} \\rVert_2$）构造 $A$、$x_{\\mathrm{true}}$、$b_{\\mathrm{clean}}$ 和 $b = b_{\\mathrm{clean}} + \\eta$，且 $\\lVert \\eta \\rVert_2 = \\delta$。\n2. 在指定的 $\\alpha_{\\min}$ 和 $\\alpha_{\\max}$ 之间，在 $\\log_{10} \\alpha$ 上均匀间隔的 $\\alpha$ 值网格上计算 L 曲线，该网格具有指定的点数 $N_\\alpha$。使用关于 $t = \\log \\alpha$ 的离散二阶中心差分来近似导数，在内部点上评估曲率，并选择与最大绝对曲率对应的 $\\alpha$ 作为 L 曲线角点估计 $\\alpha_{\\mathrm{LC}}$。\n3. 通过对 $\\alpha$ 求解 $\\lVert A x_\\alpha - b \\rVert_2 = \\delta$ 来计算 MDP 选择的 $\\alpha_{\\mathrm{MDP}}$，使用稳健的区间求根方法。如有必要，指数级扩大区间直到找到符号变化。选择最小的正根（在此处是唯一的）。\n4. 对于每个测试用例，生成实数三元组 $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$，其中 $\\mathrm{rel\\_err} = \\lvert \\alpha_{\\mathrm{LC}} - \\alpha_{\\mathrm{MDP}} \\rvert / \\alpha_{\\mathrm{MDP}}$，以浮点数形式返回。\n\n重要的实现细节和约束：\n- 所有计算都是无量纲的；不涉及物理单位。\n- 在计算范数的对数时，确保参数为严格正数。如果发生数值下溢，在取对数前将范数裁剪到一个正的下限值（如 $10^{-300}$），以避免未定义值。\n- 仅使用所描述的数据和方法；不假设任何外部提示或公式。\n- 程序必须是自包含的，并且不得读取输入或写入文件。\n\n测试套件：\n使用以下四个测试用例。对于每个案例，$n = 6$，噪声方向为 $d = [1,-1,1,-1,1,-1]^T$。\n- 案例 1（中等噪声，几何衰减）：\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 0.05$\n  - 网格：$\\alpha_{\\min} = 10^{-6}$，$\\alpha_{\\max} = 10^{1}$，$N_\\alpha = 400$\n- 案例 2（极小噪声，几何衰减）：\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 10^{-6}$\n  - 网格：$\\alpha_{\\min} = 10^{-8}$，$\\alpha_{\\max} = 10^{0}$，$N_\\alpha = 400$\n- 案例 3（大噪声，几何衰减）：\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 0.4$\n  - 网格：$\\alpha_{\\min} = 10^{-6}$，$\\alpha_{\\max} = 10^{2}$，$N_\\alpha = 600$\n- 案例 4（严重病态谱，中等噪声）：\n  - $\\sigma = [1, 10^{-2}, 10^{-4}, 10^{-6}, 10^{-8}, 10^{-10}]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - 噪声分数 $f = 10^{-3}$\n  - 网格：$\\alpha_{\\min} = 10^{-10}$，$\\alpha_{\\max} = 10^{2}$，$N_\\alpha = 600$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个类似 JSON 的列表，列表包含四个项目，每个测试用例一个，每个项目都是列表 $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$。例如：\n[[aL1,aM1,e1],[aL2,aM2,e2],[aL3,aM3,e3],[aL4,aM4,e4]]\n每个 $aLk$、$aMk$ 和 $ek$ 都必须是浮点数。不应打印额外的文本或行。", "solution": "所呈现的问题是逆问题数值分析领域一个有效且适定的练习。它要求为吉洪诺夫正则化实现并比较两种标准的*后验*正则化参数选择准则——L 曲线准则和莫洛佐夫差异原则。该问题具有科学依据、自包含，并且在算法上是明确的。我们将继续提供完整的解决方案。\n\n核心任务是为一个线性系统 $Ax = b$ 确定吉洪诺夫正则化参数 $\\alpha$，其中前向算子 $A$ 和真实解 $x_{\\mathrm{true}}$ 已知，但数据 $b$ 被噪声污染。\n\n**1. 控制方程与问题表述**\n\n我们考虑一个由系统 $Ax = b$ 定义的 $\\mathbb{R}^n$ 中的线性逆问题。前向算子 $A \\in \\mathbb{R}^{n \\times n}$ 被给定为一个对角矩阵，其对角元对应一组奇异值 $\\{\\sigma_i\\}_{i=1}^n$，其中 $\\sigma_i  0$。\n$$\nA = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\n$$\n真实未知解向量 $x_{\\mathrm{true}}$ 由其在标准基中的系数 $\\{c_i\\}_{i=1}^n$ 指定，使得 $x_{\\mathrm{true}} = [c_1, c_2, \\dots, c_n]^T$。干净数据向量 $b_{\\mathrm{clean}}$ 则由下式给出：\n$$\nb_{\\mathrm{clean}} = A x_{\\mathrm{true}}, \\quad \\text{其分量为} \\quad (b_{\\mathrm{clean}})_i = \\sigma_i c_i\n$$\n观测数据 $b$ 是干净数据被加性噪声 $\\eta$ 扰动的结果，即 $b = b_{\\mathrm{clean}} + \\eta$。噪声向量 $\\eta$ 被构造成具有特定的幅值 $\\delta  0$ 和方向 $d \\in \\mathbb{R}^n$：\n$$\n\\eta = \\delta \\frac{d}{\\lVert d \\rVert_2}, \\quad \\text{因此} \\quad \\lVert \\eta \\rVert_2 = \\delta\n$$\n噪声水平 $\\delta$ 被定义为干净数据范数的一个分数 $f$：\n$$\n\\delta = f \\cdot \\lVert b_{\\mathrm{clean}} \\rVert_2\n$$\n问题在于如何从噪声数据 $b$ 中找到 $x_{\\mathrm{true}}$ 的一个稳定近似解。\n\n**2. 吉洪诺夫正则化及相关范数**\n\n吉洪诺夫正则化通过寻找一个最小化复合目标函数的解 $x_\\alpha$ 来解决这个病态问题，该函数平衡了数据保真度与解的稳定性。对于一个正则化参数 $\\alpha  0$，解 $x_\\alpha$ 是以下函数的最小化子：\n$$\nJ_\\alpha(x) = \\lVert Ax - b \\rVert_2^2 + \\alpha^2 \\lVert x \\rVert_2^2\n$$\n由于 $A$ 是一个对角矩阵，问题得以解耦。正则化解的第 $i$ 个分量 $(x_\\alpha)_i$ 为：\n$$\n(x_\\alpha)_i = \\frac{\\sigma_i b_i}{\\sigma_i^2 + \\alpha^2}\n$$\n正则化解的质量通过残差范数 $\\lVert Ax_\\alpha - b \\rVert_2$ 和解范数 $\\lVert x_\\alpha \\rVert_2$ 来评估。它们的平方值，我们分别表示为 $\\rho(\\alpha)$ 和 $\\xi(\\alpha)$，具有方便的闭式表达式：\n$$\n\\rho(\\alpha) = \\lVert Ax_\\alpha - b \\rVert_2^2 = \\sum_{i=1}^n \\left( \\sigma_i (x_\\alpha)_i - b_i \\right)^2 = \\sum_{i=1}^n \\left( \\frac{\\alpha^2 b_i}{\\sigma_i^2 + \\alpha^2} \\right)^2\n$$\n$$\n\\xi(\\alpha) = \\lVert x_\\alpha \\rVert_2^2 = \\sum_{i=1}^n \\left( (x_\\alpha)_i \\right)^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i b_i}{\\sigma_i^2 + \\alpha^2} \\right)^2\n$$\n核心挑战是为 $\\alpha$ 选择一个合适的值。\n\n**3. L 曲线参数选择准则**\n\nL 曲线是解范数的对数与残差范数的对数的参数图。如问题所述，该曲线由 $\\alpha  0$ 参数化：\n$$\n\\left( \\log \\lVert A x_\\alpha - b \\rVert_2, \\log \\lVert x_\\alpha \\rVert_2 \\right) = \\left( \\frac{1}{2}\\log \\rho(\\alpha), \\frac{1}{2}\\log \\xi(\\alpha) \\right)\n$$\n该曲线通常具有特征性的“L”形。 “L”的角点通常被认为代表了在最小化残差（数据保真度）和最小化解范数（正则性）之间的良好平衡。我们通过寻找最大曲率点来识别这个角点。\n\n为计算曲率，我们使用 $t = \\log \\alpha$ 对曲线进行重新参数化。设 $X(t) = \\log \\lVert A x_{e^t} - b \\rVert_2$ 和 $Y(t) = \\log \\lVert x_{e^t} \\rVert_2$。曲率 $\\kappa(t)$ 由平面曲线的标准公式给出：\n$$\n\\kappa(t) = \\frac{X'(t)Y''(t) - Y'(t)X''(t)}{\\left( (X'(t))^2 + (Y'(t))^2 \\right)^{3/2}}\n$$\n我们通过执行以下步骤来计算 $\\alpha_{\\mathrm{LC}}$：\n1.  在 $\\alpha_{\\min}$ 和 $\\alpha_{\\max}$ 之间生成一个包含 $N_\\alpha$ 个 $\\alpha$ 值的对数间隔网格。这对应于一个 $t = \\log \\alpha$ 的均匀网格，步长为 $\\Delta t$。\n2.  对于网格上的每个 $\\alpha$，计算 $\\rho(\\alpha)$ 和 $\\xi(\\alpha)$，然后计算函数 $X(t)$ 和 $Y(t)$。为防止数值错误，对数函数的参数使用 $10^{-300}$ 作为下限。\n3.  使用二阶中心有限差分在网格的内部点上近似一阶和二阶导数 $X'(t)$、$X''(t)$、$Y'(t)$ 和 $Y''(t)$：\n    $$\n    F'(t_j) \\approx \\frac{F(t_{j+1}) - F(t_{j-1})}{2 \\Delta t}, \\quad F''(t_j) \\approx \\frac{F(t_{j+1}) - 2F(t_j) + F(t_{j-1})}{(\\Delta t)^2}\n    $$\n4.  将这些近似导数代入曲率公式 $\\kappa(t)$。\n5.  正则化参数 $\\alpha_{\\mathrm{LC}}$ 被选为与最大绝对曲率 $|\\kappa(t)|$ 对应的 $\\alpha$ 网格值。\n\n**4. 莫洛佐夫差异原则 (MDP)**\n\n莫洛佐夫差异原则提供了另一种选择 $\\alpha$ 的准则。它的前提是，一个理想的正则化解应该仅在噪声水平内再现数据。在数学上，它选择满足以下条件的参数 $\\alpha$：\n$$\n\\lVert A x_\\alpha - b \\rVert_2 = \\delta \\quad \\text{或等价地} \\quad \\rho(\\alpha) = \\delta^2\n$$\n为了找到参数 $\\alpha_{\\mathrm{MDP}}$，我们必须求解非线性标量方程 $g(\\alpha) = 0$，其中 $g(\\alpha) = \\rho(\\alpha) - \\delta^2$。对于 $\\alpha  0$，函数 $\\rho(\\alpha)$ 是一个严格单调递增函数，其范围从 $\\rho(0)=0$ 到 $\\rho(\\infty) = \\lVert b \\rVert_2^2$。因此，对于任何 $\\delta \\in (0, \\lVert b \\rVert_2)$，都存在一个唯一的正解 $\\alpha_{\\mathrm{MDP}}$。这个解可以通过使用稳健的区间求根算法（如 Brent-Dekker 方法）进行数值求解。如果初始搜索区间没有包围一个根，则指数级扩大该区间，直到检测到 $g(\\alpha)$ 的符号变化。\n\n**5. 算法实现摘要**\n\n对于每个提供的测试用例，执行以下计算过程：\n1.  使用问题参数 $\\{\\sigma_i\\}$、$\\{c_i\\}$、$d$ 和 $f$ 来构造向量 $x_{\\mathrm{true}}$、$b_{\\mathrm{clean}}$ 和 $b$，以及标量噪声水平 $\\delta$。\n2.  为了找到 $\\alpha_{\\mathrm{LC}}$，在指定的 $\\alpha$ 对数网格上计算 $\\rho(\\alpha)$ 和 $\\xi(\\alpha)$。使用有限差分在内部网格点上计算导数和曲率，并选择与最大绝对曲率对应的 $\\alpha$。\n3.  为了找到 $\\alpha_{\\mathrm{MDP}}$，定义函数 $g(\\alpha) = \\rho(\\alpha) - \\delta^2$。使用求根算法来定位该函数的唯一正根，从而得到 $\\alpha_{\\mathrm{MDP}}$。\n4.  通过计算相对误差 $\\mathrm{rel\\_err} = |\\alpha_{\\mathrm{LC}} - \\alpha_{\\mathrm{MDP}}| / \\alpha_{\\mathrm{MDP}}$ 来比较这两个参数估计值。\n5.  得到的三元组 $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$ 构成了该测试用例的输出。\n整个过程被封装在一个自包含的程序中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the regularization parameter selection problem for a series of test cases.\n    For each case, it calculates the L-curve corner alpha, the Morozov Discrepancy\n    Principle alpha, and their relative error.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 0.05,\n            \"grid\": (1e-6, 1e1, 400)\n        },\n        {\n            \"name\": \"Case 2\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 1e-6,\n            \"grid\": (1e-8, 1e0, 400)\n        },\n        {\n            \"name\": \"Case 3\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 0.4,\n            \"grid\": (1e-6, 1e2, 600)\n        },\n        {\n            \"name\": \"Case 4\",\n            \"sigma\": np.array([1, 1e-2, 1e-4, 1e-6, 1e-8, 1e-10]),\n            \"c\": np.ones(6),\n            \"f\": 1e-3,\n            \"grid\": (1e-10, 1e2, 600)\n        }\n    ]\n\n    results = []\n    \n    d = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0])\n    d_normalized = d / np.linalg.norm(d)\n    \n    # Positive floor for log calculation to avoid undefined values.\n    LOG_FLOOR = 1e-300\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        c = case[\"c\"]\n        f = case[\"f\"]\n        alpha_min, alpha_max, n_alpha = case[\"grid\"]\n\n        # 1. Construct b_clean, delta, and b\n        x_true = c\n        b_clean = sigma * x_true\n        norm_b_clean = np.linalg.norm(b_clean)\n        delta = f * norm_b_clean\n        eta = delta * d_normalized\n        b = b_clean + eta\n\n        # 2. Compute L-curve and estimate the corner alpha_LC\n        alphas = np.logspace(np.log10(alpha_min), np.log10(alpha_max), n_alpha)\n        \n        # Vectorized calculation of norms\n        sigma_sq = sigma**2\n        b_sq = b**2\n        alphas_sq = alphas[:, np.newaxis]**2\n        \n        denominators = sigma_sq + alphas_sq\n        \n        # Residual norm squared rho(alpha)\n        rho_terms = (alphas_sq**2 / denominators**2) * b_sq\n        rho_sq_vals = np.sum(rho_terms, axis=1)\n\n        # Solution norm squared xi(alpha)\n        xi_terms = (sigma_sq / denominators**2) * b_sq\n        xi_sq_vals = np.sum(xi_terms, axis=1)\n\n        # L-curve coordinates in log-log scale\n        X = 0.5 * np.log(np.maximum(rho_sq_vals, LOG_FLOOR)) # X = log(residual norm)\n        Y = 0.5 * np.log(np.maximum(xi_sq_vals, LOG_FLOOR))  # Y = log(solution norm)\n        \n        # Curvature calculation using central differences\n        t = np.log(alphas)\n        delta_t = t[1] - t[0]\n\n        # Derivatives for interior points (from index 1 to n_alpha-2)\n        Xp = (X[2:] - X[:-2]) / (2 * delta_t)\n        Yp = (Y[2:] - Y[:-2]) / (2 * delta_t)\n        Xpp = (X[2:] - 2 * X[1:-1] + X[:-2]) / (delta_t**2)\n        Ypp = (Y[2:] - 2 * Y[1:-1] + Y[:-2]) / (delta_t**2)\n\n        numerator = Xp * Ypp - Yp * Xpp\n        denominator = (Xp**2 + Yp**2)**1.5\n\n        kappa = np.zeros_like(numerator)\n        # Avoid division by zero for points where derivatives might be nil.\n        valid_denom = denominator > LOG_FLOOR\n        kappa[valid_denom] = numerator[valid_denom] / denominator[valid_denom]\n\n        # Find alpha corresponding to max absolute curvature\n        max_kappa_idx = np.argmax(np.abs(kappa))\n        # The index must be shifted by 1 to map back to the original alpha grid\n        alpha_lc = alphas[max_kappa_idx + 1]\n\n        # 3. Compute MDP selection alpha_MDP\n        def rho_sq_func(alpha):\n            alpha_sq = alpha**2\n            den = sigma_sq + alpha_sq\n            terms = (alpha_sq**2 / den**2) * b_sq\n            return np.sum(terms)\n            \n        def g(alpha):\n            return rho_sq_func(alpha) - delta**2\n            \n        # Find a bracket for the root-finding algorithm\n        a, b_root = alpha_min, alpha_max\n        try:\n            val_a = g(a)\n            val_b = g(b_root)\n            # Exponentially expand bracket until a sign change is found\n            while val_a * val_b > 0 and b_root  1e100 and a > 1e-100:\n                if val_b  0:\n                    b_root *= 10\n                    val_b = g(b_root)\n                elif val_a > 0:\n                    a /= 10\n                    val_a = g(a)\n                else:\n                    # This case should not be reached with monotonic rho\n                    break\n            alpha_mdp = brentq(g, a, b_root, xtol=1e-15, rtol=1e-15)\n        except (ValueError, OverflowError):\n             # Fallback if bracketing fails\n            alpha_mdp = np.nan\n\n\n        # 4. Compute relative error and store results\n        rel_err = np.abs(alpha_lc - alpha_mdp) / alpha_mdp if alpha_mdp is not np.nan and alpha_mdp != 0 else np.nan\n        results.append([alpha_lc, alpha_mdp, rel_err])\n\n    # Final print statement in the exact required format.\n    # Produces for example: [[aL1,aM1,e1],[aL2,aM2,e2],...]\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3361732"}, {"introduction": "广义交叉验证 (Generalized Cross-Validation, GCV) 是另一种功能强大的参数选择技术，它同样不依赖于对噪声水平的精确了解。然而，深刻理解一个方法的适用范围和潜在缺陷同样重要。[@problem_id:3361679] 这个高级练习将引导您探索标准GCV在严重不适定问题下的一个典型失效模式，并将其选择的参数与无法在实践中获得的“神谕” (oracle) 最优解进行对比。此外，您还将实现一个加权GCV (weighted GCV, wGCV) 作为补救措施，这不仅能加深您对GCV内在机理的理解，也展示了如何通过改进算法来提升其在挑战性场景下的鲁棒性。", "problem": "考虑由奇异值坐标下的离散正演模型定义的加性噪声线性反问题。设 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对角矩阵，其对角线元素 $s_i$ 呈几何衰减，$s_i = \\rho^{i-1}$，其中 $i = 1,\\dots,n$，$\\rho \\in (0,1)$ 为固定值。设未知量为 $x_{\\star} \\in \\mathbb{R}^n$，其在 $A$ 的右奇异向量基中的分量为 $\\theta_i = (-1)^{i-1} i^{-2}$（即 $x_{\\star}$ 在使 $A$ 对角化的基中的表示）。观测值为 $y = A x_{\\star} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2 I)$ 是零均值高斯噪声，其方差 $\\sigma^2$ 已知。\n\n您将使用 Tikhonov 正则化和选择正则化参数的后验准则。设给定参数 $\\alpha  0$ 的 Tikhonov 估计由平衡数据失配和正则化的变分问题的解定义，并将相应的线性估计器表示为 $x_{\\alpha}$。在奇异值分解基中，该估计器通过依赖于 $\\alpha$ 和奇异值 $s_i$ 的滤波因子逐分量作用。\n\n您的任务如下：\n\n1. 从 $A$ 的奇异值分解和 Tikhonov 正则化解的正规方程出发，推导 $x_{\\alpha}$ 在滤波因子方面的分量表示，并由此推导出期望均方误差（风险）$\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$ 作为 $\\alpha$、$s_i$、$\\theta_i$ 和 $\\sigma^2$ 的函数。仅使用 Tikhonov 正则化的标准定义和高斯噪声下线性估计器的性质作为基本依据。\n2. 将预言参数 $\\alpha_{\\mathrm{oracle}}$ 定义为期望均方误差 $\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$ 在合适的 $\\alpha$ 值搜索域上的最小值。此预言参数在实践中无法获得，但将作为基准。\n3. 从线性平滑器的留一法预测损失定义出发，推导线性估计器的广义交叉验证（GCV）目标函数，并获得仅使用 $y$ 和与 Tikhonov 正则化相关的线性平滑器的标准 GCV 准则来选择 $\\alpha$。展示如何从残差和平滑器的有效自由度计算 GCV 目标函数。\n4. 通过选择一个具有小 $\\rho$ 的几何衰减谱，构建一个严重病态性下的后验反例，并数值证明标准的广义交叉验证（GCV）选择的 $\\alpha$ 相对于 $\\alpha_{\\mathrm{oracle}}$ 过小，即 $|\\log_{10}(\\alpha_{\\mathrm{GCV}}/\\alpha_{\\mathrm{oracle}})|$ 很大，且在 $\\alpha_{\\mathrm{GCV}}$ 处的期望均方误差大于在 $\\alpha_{\\mathrm{oracle}}$ 处的期望均方误差。\n5. 提出一个加权广义交叉验证（GCV）准则，该准则在左奇异向量基中的残差上引入非负谱权重 $w_i$。从加权预测损失和平滑器的线性性质出发，推导出依赖于权重 $w_i$、残差和加权有效自由度的相应加权 GCV 目标函数。然后，设置 $w_i = s_i^2$ 并解释为什么此选择能缓解标准 GCV 在严重病态性下的失效模式。实现加权 GCV 准则，并计算最小化此目标函数的 $\\alpha_{\\mathrm{wGCV}}$。\n6. 对于下方的每个测试用例，计算三元组 $(\\alpha_{\\mathrm{GCV}}, \\alpha_{\\mathrm{wGCV}}, \\alpha_{\\mathrm{oracle}})$ 和相应的期望风险，然后记录一个整数结果，如果以下两个条件同时成立，则结果为 $1$：\n   (a) $|\\log_{10}(\\alpha_{\\mathrm{wGCV}}/\\alpha_{\\mathrm{oracle}})|  |\\log_{10}(\\alpha_{\\mathrm{GCV}}/\\alpha_{\\mathrm{oracle}})|$，\n   以及\n   (b) 在 $\\alpha_{\\mathrm{wGCV}}$ 处的期望均方误差小于或等于在 $\\alpha_{\\mathrm{GCV}}$ 处的期望均方误差；否则记录 $0$。\n\n实现要求和细节：\n\n- 模型规范。使用 $U = I$ 和 $V = I$，使得 $A = \\mathrm{diag}(s_1,\\dots,s_n)$，并且数据空间与奇异向量基重合。使用分量为 $\\theta_i = (-1)^{i-1} i^{-2}$ 的 $x_{\\star}$，并生成分量独立服从 $\\mathcal{N}(0,\\sigma^2)$ 分布的噪声 $\\varepsilon$。相应地构造 $y = A x_{\\star} + \\varepsilon$。\n- Tikhonov 估计器。使用正规方程所隐含的滤波因子，在奇异值坐标中实现估计器。\n- 预言参数。通过在覆盖广泛范围的对数间隔的 $\\alpha$ 值网格上最小化解析推导出的 $\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|^2$ 表达式来计算 $\\alpha_{\\mathrm{oracle}}$。您的程序必须根据模型参数精确计算此期望值，无需蒙特卡洛平均。\n- GCV 和加权 GCV。使用 Tikhonov 平滑器的残差和有效自由度实现标准广义交叉验证（GCV）选择器，以及使用在奇异向量基中计算的谱权重 $w_i = s_i^2$ 实现加权广义交叉验证（wGCV）选择器。两个选择器都应在用于预言参数的相同搜索网格上最小化它们各自的目标函数。\n- 严重病态性反例。使用几何衰减的奇异值 $s_i = \\rho^{i-1}$ 来展示一个严重情况（小 $\\rho$），在该情况下，与预言参数相比，标准 GCV 选择的 $\\alpha$ 过小，并产生较大的期望风险。\n\n测试套件：\n\n定义并求解以下四个测试用例。对每个用例，使用指定的 $(n,\\rho,\\sigma,\\text{seed})$ 来构造 $A$、$x_{\\star}$ 和 $y$ 的一次实现：\n- 用例 1 (严重): $n=80$, $\\rho=0.30$, $\\sigma=10^{-3}$, seed $=10$。\n- 用例 2 (严重但噪声较低): $n=80$, $\\rho=0.30$, $\\sigma=5 \\times 10^{-4}$, seed $=11$。\n- 用例 3 (较温和): $n=80$, $\\rho=0.80$, $\\sigma=10^{-3}$, seed $=12$。\n- 用例 4 (小维度，非常严重): $n=20$, $\\rho=0.20$, $\\sigma=10^{-3}$, seed $=13$。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含按测试用例顺序排列的结果，形式为方括号括起来的逗号分隔的整数列表，其中每个整数为 $1$ 或 $0$，如上定义（例如，$\\left[1,0,1,1\\right]$）。不应打印任何其他文本。所有数值都必须使用标准实数算术计算。", "solution": "### 1. Tikhonov 估计器与期望均方误差（风险）\n\n对于问题 $y \\approx Ax$，Tikhonov 正则化解 $x_{\\alpha}$ 是以下泛函的最小化子：\n$$\nJ_{\\alpha}(x) = \\|Ax - y\\|_2^2 + \\alpha \\|x\\|_2^2\n$$\n其中 $\\alpha  0$ 是正则化参数。令梯度 $\\nabla_x J_{\\alpha}(x)$ 为零，得到正规方程：\n$$\n(A^T A + \\alpha I) x_{\\alpha} = A^T y\n$$\n鉴于 $A$ 是一个对角矩阵 $A = S = \\mathrm{diag}(s_1, \\dots, s_n)$，其转置为 $A^T=A=S$。方程简化为：\n$$\n(S^2 + \\alpha I) x_{\\alpha} = S y\n$$\n由于 $S^2 + \\alpha I$ 是一个对角元素为 $s_i^2 + \\alpha$ 的对角矩阵，解 $x_{\\alpha}$ 可以逐分量求得：\n$$\n(s_i^2 + \\alpha) (x_{\\alpha})_i = s_i y_i \\implies (x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha} y_i\n$$\n数据 $y$ 由 $y = Ax_{\\star} + \\varepsilon$ 给出，其分量形式为 $y_i = s_i \\theta_i + \\varepsilon_i$，其中 $\\theta_i = (x_{\\star})_i$，$\\varepsilon_i$ 是噪声向量 $\\varepsilon$ 的分量。将此代入 $(x_{\\alpha})_i$ 的表达式中：\n$$\n(x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha} (s_i \\theta_i + \\varepsilon_i) = \\frac{s_i^2}{s_i^2 + \\alpha} \\theta_i + \\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i\n$$\n项 $f_i(\\alpha) = \\frac{s_i^2}{s_i^2 + \\alpha}$ 是 Tikhonov 滤波器的滤波因子。第 $i$ 个分量的估计误差为：\n$$\n(x_{\\alpha})_i - \\theta_i = \\left(\\frac{s_i^2}{s_i^2 + \\alpha} - 1\\right)\\theta_i + \\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i = \\frac{-\\alpha}{s_i^2 + \\alpha} \\theta_i + \\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i\n$$\n期望均方误差（风险）是 $\\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|_2^2 = \\sum_{i=1}^n \\mathbb{E}[((x_{\\alpha})_i - \\theta_i)^2]$。由于 $\\mathbb{E}[\\varepsilon_i] = 0$ 且 $\\mathbb{E}[\\varepsilon_i \\varepsilon_j] = \\sigma^2 \\delta_{ij}$，交叉项的期望为零。\n$$\n\\mathbb{E}[((x_{\\alpha})_i - \\theta_i)^2] = \\mathbb{E}\\left[\\left(\\frac{-\\alpha}{s_i^2 + \\alpha} \\theta_i\\right)^2 + 2(\\dots)\\varepsilon_i + \\left(\\frac{s_i}{s_i^2 + \\alpha} \\varepsilon_i\\right)^2\\right] = \\left(\\frac{\\alpha}{s_i^2 + \\alpha}\\right)^2 \\theta_i^2 + \\left(\\frac{s_i}{s_i^2 + \\alpha}\\right)^2 \\mathbb{E}[\\varepsilon_i^2]\n$$\n由于 $\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2$，风险是偏差平方和方差之和：\n$$\nR(\\alpha) = \\mathbb{E}\\|x_{\\alpha} - x_{\\star}\\|_2^2 = \\sum_{i=1}^n \\underbrace{\\left(\\frac{\\alpha \\theta_i}{s_i^2 + \\alpha}\\right)^2}_{\\text{偏差平方}} + \\sum_{i=1}^n \\underbrace{\\frac{s_i^2 \\sigma^2}{(s_i^2 + \\alpha)^2}}_{\\text{方差}}\n$$\n\n### 2. 预言正则化参数\n\n预言参数 $\\alpha_{\\mathrm{oracle}}$ 是使真实风险 $R(\\alpha)$ 最小化的 $\\alpha$ 值。它定义为：\n$$\n\\alpha_{\\mathrm{oracle}} = \\arg\\min_{\\alpha  0} R(\\alpha)\n$$\n这个值在实践中是无法获得的，因为 $R(\\alpha)$ 依赖于未知的真实解 $x_{\\star}$（通过其分量 $\\theta_i$）。然而，它可作为任何实用参数选择准则的理论基准。\n\n### 3. 广义交叉验证 (GCV)\n\nGCV 是一种用于选择 $\\alpha$ 的数据驱动方法。它源于留一法交叉验证（LOOCV）。正则化数据估计为 $\\hat{y}_{\\alpha} = Ax_{\\alpha} = A(S^2+\\alpha I)^{-1}S y = H(\\alpha)y$，其中 $H(\\alpha) = S(S^2+\\alpha I)^{-1}S$ 是影响矩阵或平滑矩阵。在这种对角情况下，$H(\\alpha) = \\mathrm{diag}(h_1(\\alpha), \\dots, h_n(\\alpha))$，其中 $h_i(\\alpha) = \\frac{s_i^2}{s_i^2+\\alpha}$。\n\nLOOCV 分数是平方预测误差的平均值，其中每个点 $y_i$ 都是使用拟合了除该点以外数据的模型来预测的。对于线性平滑器，该分数有一个方便的闭合形式：\n$$\nL(\\alpha) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\alpha,i}}{1 - h_{ii}(\\alpha)} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - h_i(\\alpha)y_i}{1 - h_i(\\alpha)} \\right)^2\n$$\nGCV 通过将分母中单个的对角元素 $h_i(\\alpha)$ 替换为其平均值 $\\frac{1}{n}\\mathrm{Tr}(H(\\alpha))$ 来近似 LOOCV。需要最小化的 GCV 函数是：\n$$\nV_{GCV}(\\alpha) = \\frac{\\frac{1}{n}\\|y - \\hat{y}_{\\alpha}\\|_2^2}{\\left(1 - \\frac{1}{n}\\mathrm{Tr}(H(\\alpha))\\right)^2}\n$$\n最小化 $V_{GCV}(\\alpha)$ 等价于最小化更简单的泛函 $G(\\alpha)$：\n$$\nG(\\alpha) = \\frac{\\|y - \\hat{y}_{\\alpha}\\|_2^2}{(\\mathrm{Tr}(I - H(\\alpha)))^2}\n$$\n分子是残差向量 $r(\\alpha) = y - \\hat{y}_{\\alpha} = (I - H(\\alpha))y$ 的平方范数。项 $\\mathrm{Tr}(H(\\alpha)) = \\sum_{i=1}^n h_i(\\alpha)$ 是平滑器的有效自由度。在我们的分量形式下：\n$$\nG(\\alpha) = \\frac{\\sum_{i=1}^n \\left( (1-h_i(\\alpha))y_i \\right)^2}{\\left( \\sum_{i=1}^n (1-h_i(\\alpha)) \\right)^2} = \\frac{\\sum_{i=1}^n \\left( \\frac{\\alpha}{s_i^2+\\alpha} y_i \\right)^2}{\\left( \\sum_{i=1}^n \\frac{\\alpha}{s_i^2+\\alpha} \\right)^2}\n$$\nGCV 参数选择为 $\\alpha_{\\mathrm{GCV}} = \\arg\\min_{\\alpha  0} G(\\alpha)$。\n\n### 4. GCV 在严重病态性下的失效\n\n对于严重病态问题（例如，小的 $\\rho$，此时奇异值 $s_i$ 衰减非常快），GCV 函数 $G(\\alpha)$ 可能会在一个非常小的 $\\alpha$ 处出现一个误导性的、虚假的最小值。这个 $\\alpha_{\\mathrm{GCV}}$ 通常远小于 $\\alpha_{\\mathrm{oracle}}$，导致正则化不足、充满噪声的解，且风险较大。数值实验将展示这一现象。\n\n### 5. 加权广义交叉验证 (wGCV)\n\n为了缓解 GCV 的失效问题，可以引入加权版本。加权 LOOCV 分数应为 $L_w(\\alpha) = \\frac{1}{n} \\sum_{i} w_i \\left( \\frac{r_i(\\alpha)}{1 - h_i(\\alpha)} \\right)^2$。应用 GCV 的近似逻辑，可得到 wGCV 泛函 $G_w(\\alpha)$：\n$$\nG_w(\\alpha) = \\frac{\\sum_{i=1}^n w_i (r_i(\\alpha))^2}{\\left(\\sum_{i=1}^n w_i (1-h_i(\\alpha))\\right)^2} = \\frac{\\sum_{i=1}^n w_i \\left( \\frac{\\alpha}{s_i^2+\\alpha} y_i \\right)^2}{\\left( \\sum_{i=1}^n w_i \\frac{\\alpha}{s_i^2+\\alpha} \\right)^2}\n$$\n问题指定权重为 $w_i = s_i^2$。让我们分析这个选择。当 $\\alpha \\to 0$ 时，$1 - h_i(\\alpha) = \\frac{\\alpha}{s_i^2+\\alpha} \\approx \\frac{\\alpha}{s_i^2}$。该泛函的行为如下：\n$$\nG_w(\\alpha) \\approx \\frac{\\sum_{i=1}^n s_i^2 \\left( \\frac{\\alpha}{s_i^2} y_i \\right)^2}{\\left( \\sum_{i=1}^n s_i^2 \\frac{\\alpha}{s_i^2} \\right)^2} = \\frac{\\alpha^2 \\sum_{i=1}^n y_i^2}{\\left( \\sum_{i=1}^n \\alpha \\right)^2} = \\frac{\\sum_{i=1}^n y_i^2}{n^2}\n$$\n这与标准 GCV 的行为不同，后者在 $\\alpha \\to 0$ 时分母的行为类似于 $(\\sum_i \\alpha/s_i^2)^2$，对于小的 $s_i$ 会发散，导致在 $\\alpha=0$ 附近出现虚假的最小值。权重 $w_i=s_i^2$ 给对应于小奇异值（噪声放大最严重的分量）的残差分量赋予了较小的权重，从而稳定了 GCV 泛函，防止其在 $\\alpha \\to 0$ 时被噪声主导的分量所驱动而失效。参数选择则为 $\\alpha_{\\mathrm{wGCV}} = \\arg\\min_{\\alpha  0} G_w(\\alpha)$。\n\n### 6. 数值计算与比较\n\n对每个测试用例，实施以下程序：\n1.  使用给定的参数 $n, \\rho, \\sigma, \\text{seed}$，构造奇异值 $s_i$、真实解分量 $\\theta_i$ 和数据 $y_i$ 的一次实现。\n2.  定义一个广泛的候选 $\\alpha$ 值的对数网格。\n3.  对于网格上的每个 $\\alpha$，评估风险 $R(\\alpha)$、GCV 泛函 $G(\\alpha)$ 和 wGCV 泛函 $G_w(\\alpha)$。\n4.  通过找到这些函数的最小值点，确定 $\\alpha_{\\mathrm{oracle}}$、$\\alpha_{\\mathrm{GCV}}$ 和 $\\alpha_{\\mathrm{wGCV}}$。\n5.  计算在 $\\alpha_{\\mathrm{GCV}}$ 和 $\\alpha_{\\mathrm{wGCV}}$ 处的期望风险 $R(\\alpha_{\\mathrm{GCV}})$ 和 $R(\\alpha_{\\mathrm{wGCV}})$。\n6.  检查两个条件：\n    (a) wGCV 在对数尺度上是否更接近预言参数：$|\\log_{10}(\\alpha_{\\mathrm{wGCV}}/\\alpha_{\\mathrm{oracle}})|  |\\log_{10}(\\alpha_{\\mathrm{GCV}}/\\alpha_{\\mathrm{oracle}})|$？\n    (b) wGCV 是否产生更好或相等的风险：$R(\\alpha_{\\mathrm{wGCV}}) \\le R(\\alpha_{\\mathrm{GCV}})$？\n7.  如果两个条件都为真，则结果为 $1$；否则为 $0$。对所有四个测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(n, rho, sigma, seed):\n    \"\"\"\n    Solves a single test case for comparing GCV, wGCV, and Oracle parameter selection.\n\n    Args:\n        n (int): Dimension of the problem.\n        rho (float): Decay rate for singular values.\n        sigma (float): Standard deviation of the Gaussian noise.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        int: 1 if wGCV is superior to GCV by the specified criteria, 0 otherwise.\n    \"\"\"\n    # 1. Model and data setup\n    # Construct singular values s_i = rho^(i-1)\n    i = np.arange(n, dtype=np.float64)\n    s = rho**i\n\n    # Construct true solution components theta_i = (-1)^(i) * (i+1)^-2\n    theta = ((-1.0)**i) * ((i + 1.0)**(-2.0))\n\n    # Generate data y = A*x_star + epsilon\n    rng = np.random.default_rng(seed)\n    epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = s * theta + epsilon\n\n    # 2. Define search grid for alpha\n    alphas = np.logspace(-20, 5, 2000)\n\n    # 3. Vectorized evaluation of risk and GCV functions\n    s2 = s**2\n\n    # Use broadcasting to compute for all alphas at once\n    # Shape will be (n_alphas, n) after transposing for easier sum\n    s2a = s2[np.newaxis, :] + alphas[:, np.newaxis]\n\n    # Expected Mean Squared Error (Risk) R(alpha)\n    risk_bias_sq = (alphas[:, np.newaxis]**2 * theta[np.newaxis, :]**2) / (s2a**2)\n    risk_variance = (s2[np.newaxis, :] * sigma**2) / (s2a**2)\n    risk_values = np.sum(risk_bias_sq + risk_variance, axis=1)\n\n    # Standard Generalized Cross-Validation (GCV)\n    # h_i are the diagonal elements of the influence matrix H\n    h = s2[np.newaxis, :] / s2a\n    one_minus_h = 1.0 - h\n    # Residual r = (I-H)y\n    r = one_minus_h * y[np.newaxis, :]\n    gcv_numerator = np.sum(r**2, axis=1)\n    gcv_denominator = np.sum(one_minus_h, axis=1)\n    # Add a small epsilon to denominator to avoid division by zero far from minimum\n    gcv_values = gcv_numerator / (gcv_denominator**2 + 1e-30)\n\n    # Weighted Generalized Cross-Validation (wGCV) with w_i = s_i^2\n    w = s2\n    wgcv_numerator = np.sum(w[np.newaxis, :] * r**2, axis=1)\n    wgcv_denominator = np.sum(w[np.newaxis, :] * one_minus_h, axis=1)\n    wgcv_values = wgcv_numerator / (wgcv_denominator**2 + 1e-30)\n\n    # 4. Find the optimal alpha values\n    alpha_oracle = alphas[np.argmin(risk_values)]\n    alpha_gcv = alphas[np.argmin(gcv_values)]\n    alpha_wgcv = alphas[np.argmin(wgcv_values)]\n\n    # 5. Compute risks at the selected alpha values\n    def calculate_risk(alpha, s_vec, theta_vec, sigma_val):\n        s2_vec = s_vec**2\n        s2a_vec = s2_vec + alpha\n        risk_b_sq = (alpha**2 * theta_vec**2) / (s2a_vec**2)\n        risk_var = (s2_vec * sigma_val**2) / (s2a_vec**2)\n        return np.sum(risk_b_sq + risk_var)\n\n    risk_at_gcv = calculate_risk(alpha_gcv, s, theta, sigma)\n    risk_at_wgcv = calculate_risk(alpha_wgcv, s, theta, sigma)\n\n    # 6. Check the two conditions for the final result\n    # Condition (a): wGCV is closer to oracle in log-scale\n    log_dist_gcv = np.abs(np.log10(alpha_gcv / alpha_oracle)) if alpha_gcv > 0 and alpha_oracle > 0 else np.inf\n    log_dist_wgcv = np.abs(np.log10(alpha_wgcv / alpha_oracle)) if alpha_wgcv > 0 and alpha_oracle > 0 else np.inf\n    cond_a = log_dist_wgcv  log_dist_gcv\n\n    # Condition (b): wGCV yields lower or equal risk\n    cond_b = risk_at_wgcv = risk_at_gcv\n\n    result = 1 if cond_a and cond_b else 0\n    return result\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test cases from the problem statement\n    test_cases = [\n        # (n, rho, sigma, seed)\n        (80, 0.30, 1e-3, 10),            # Case 1 (severe)\n        (80, 0.30, 5e-4, 11),            # Case 2 (severe but lower noise)\n        (80, 0.80, 1e-3, 12),            # Case 3 (milder)\n        (20, 0.20, 1e-3, 13),            # Case 4 (small dimension, very severe)\n    ]\n\n    results = []\n    for case in test_cases:\n        n, rho, sigma, seed = case\n        result = solve_case(n, rho, sigma, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3361679"}]}