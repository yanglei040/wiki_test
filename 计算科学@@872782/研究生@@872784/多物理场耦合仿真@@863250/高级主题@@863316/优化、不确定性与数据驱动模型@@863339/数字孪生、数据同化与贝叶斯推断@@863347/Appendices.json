{"hands_on_practices": [{"introduction": "在构建基于集合的数据同化系统（如集合卡尔曼滤波器 EnKF）时，一个基本问题是如何评估其性能。我们如何确定预测集合的离散度是否恰当地代表了真实的预测不确定性？本练习介绍了一种关键的诊断工具——秩统计直方图，它为验证集合预报的统计一致性提供了一种强大的可视化和定量方法[@problem_id:3502567]。通过从第一性原理出发构建和检验秩统计直方图，你将掌握诊断和调试数字孪生中数据同化模块的基本技能。", "problem": "要求您构建一个独立的程序，为集合卡尔曼滤波 (EnKF) 构建基于新息的排序直方图，并在关于模型和噪声设定的不同假设下评估其均匀性。其背景是在一个多物理场数字孪生中的标量、线性观测模型，其中集合预报和贝叶斯推断被用于数据同化。推导和算法必须仅从概率论基本原理、线性高斯模型定义以及独立同分布变量的可交换性定义出发。\n\n考虑以下设定。真实状态 $x$ 是一个服从高斯先验分布 $x \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}})$ 的标量随机变量。观测算子是单位算子，因此观测到的量为 $y = x + \\varepsilon$，其中观测噪声 $\\varepsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$ 且独立于 $x$。集合卡尔曼滤波 (EnKF) 生成一个预报集合 $\\{x_f^{(i)}\\}_{i=1}^m$，其中 $m$ 是集合大小。EnKF 使用一个假定的高斯预报分布 $x_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}})$ 和一个用于扰动观测的假定观测噪声 $e^{(i)} \\sim \\mathcal{N}(0, R_{\\text{assumed}})$，所有这些在 $i$ 上相互独立，并且独立于 $x$ 和 $\\varepsilon$。集合预报观测值为 $y_f^{(i)} = x_f^{(i)} + e^{(i)}$。\n\n按如下方式定义基于新息的预报排序。在每个同化循环中，抽取一个 $y$ 的实现，抽取一个集合 $\\{y_f^{(i)}\\}_{i=1}^m$，并计算 $y$ 在这 $m$ 个独立同分布的值 $\\{y_f^{(i)}\\}_{i=1}^m$ 中的排序 $r \\in \\{0,1,\\dots,m\\}$，即 $r$ 是集合预报观测值中严格小于 $y$ 的数量。在完美的模型和噪声调整下，变量 $\\{y, y_f^{(1)},\\dots,y_f^{(m)}\\}$ 是独立同分布的，并且排序 $r$ 期望在 $\\{0,\\dots,m\\}$ 上均匀分布。\n\n您的任务是：\n- 通过重复上述实验 $N$ 个独立的同化循环，并统计每个排序 $r \\in \\{0,\\dots,m\\}$ 的频率，来构建排序直方图。\n- 使用显著性水平为 $\\alpha$ 的卡方拟合优度检验，定量地检验直方图的均匀性。其原假设是直方图是均匀的，每个区间（bin）的概率等于 $1/(m+1)$。使用卡方统计量，并与具有 $m$ 个自由度的参考分布计算 $p$ 值。\n- 对于测试套件中的每个测试用例，输出一个布尔值，指示在水平 $\\alpha$ 下是否接受均匀性的原假设。\n\n您必须从第一性原理出发解决此问题。推导和算法逻辑应从基本的概率定律、高斯模型以及顺序统计量和可交换性的性质开始，而不是从超出这些基础的任何预先指定的排序直方图公式开始。\n\n此问题中没有物理单位。不使用角度。所有概率必须表示为 $[0,1]$ 区间内的实数。\n\n需要实现的测试套件：\n- 情况 A（完美调整，大集合和大样本量）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n- 情况 B（离散度不足的预报集合和噪声）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 0.25$, $R_{\\text{assumed}} = 0.25$\n- 情况 C（离散度过大的预报集合和噪声）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 4.0$, $R_{\\text{assumed}} = 4.0$\n- 情况 D（有偏的预报均值）：\n  - $m = 20$, $N = 10000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.5$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n- 情况 E（小集合的边界情况）：\n  - $m = 3$, $N = 2000$, $\\alpha = 0.01$\n  - $\\mu_{\\text{true}} = 0.0$, $P_{\\text{true}} = 1.0$, $R_{\\text{true}} = 1.0$\n  - $\\mu_{\\text{assumed}} = 0.0$, $P_{\\text{assumed}} = 1.0$, $R_{\\text{assumed}} = 1.0$\n\n实现要求：\n- 对于每种情况，生成 $N$ 个独立的 $y$ 抽样和每次抽样对应的 $m$ 个成员的集合 $\\{y_f^{(i)}\\}_{i=1}^m$，计算排序 $r \\in \\{0,\\dots,m\\}$，并累积直方图计数。\n- 在 $m+1$ 个区间中，以期望频率 $N/(m+1)$ 进行卡方拟合优度检验。使用得到的 $p$ 值来决定在水平 $\\alpha$ 下是接受还是拒绝均匀性。\n- 使用固定的随机种子，以确保结果是可复现的。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按情况 A, B, C, D, E 的顺序排列，例如 $[b_A,b_B,b_C,b_D,b_E]$，其中每个 $b_{\\cdot}$ 是字符串 $\\texttt{True}$ 或 $\\texttt{False}$，指示该情况在水平 $\\alpha$ 下是否接受均匀性。\n\n您的程序必须是完整的，并且可以直接运行，无需任何用户输入或外部文件。", "solution": "该问题要求在不同的模型和噪声设定情景下，为集合卡尔曼滤波 (EnKF) 构建基于新息的排序直方图并进行统计验证。推导和实现将从概率论的第一性原理出发。\n\n首先，我们定义主导该问题的概率分布。“真实”状态 $x$ 是从高斯分布中抽取的标量随机变量，$x \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}})$。通过单位观测算子获得一个观测值 $y$，并带有加性高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, R_{\\text{true}})$，该噪声独立于 $x$。因此，$y = x + \\varepsilon$。作为两个独立高斯随机变量的和，$y$ 也是一个高斯随机变量。其分布，我们可以称之为观测的“真实预报分布”，推导如下：\n均值为 $\\mathbb{E}[y] = \\mathbb{E}[x + \\varepsilon] = \\mathbb{E}[x] + \\mathbb{E}[\\varepsilon] = \\mu_{\\text{true}} + 0 = \\mu_{\\text{true}}$。\n方差为 $\\text{Var}(y) = \\text{Var}(x + \\varepsilon) = \\text{Var}(x) + \\text{Var}(\\varepsilon) = P_{\\text{true}} + R_{\\text{true}}$，因为 $x$ 和 $\\varepsilon$ 是独立的。\n因此，任何单个真实观测值 $y$ 都是从分布 $y \\sim \\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}} + R_{\\text{true}})$ 中抽取的样本。\n\n接下来，我们定义基于集合的预报观测的分布。EnKF 使用一个假定的预报分布，从中抽取一个状态集合 $\\{x_f^{(i)}\\}_{i=1}^m$，每个成员 $x_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}})$。观测值被假定为受到来自 $\\mathcal{N}(0, R_{\\text{assumed}})$ 分布的噪声污染。为了构建预报观测集合，每个预报状态 $x_f^{(i)}$ 都被一个从假定噪声分布中抽取的随机数 $e^{(i)}$ 进行扰动，$e^{(i)} \\sim \\mathcal{N}(0, R_{\\text{assumed}})$。得到的集合预报观测值为 $y_f^{(i)} = x_f^{(i)} + e^{(i)}$。\n与真实观测类似，每个 $y_f^{(i)}$ 都是两个独立高斯变量的和。其分布推导如下：\n均值为 $\\mathbb{E}[y_f^{(i)}] = \\mathbb{E}[x_f^{(i)} + e^{(i)}] = \\mathbb{E}[x_f^{(i)}] + \\mathbb{E}[e^{(i)}] = \\mu_{\\text{assumed}} + 0 = \\mu_{\\text{assumed}}$。\n方差为 $\\text{Var}(y_f^{(i)}) = \\text{Var}(x_f^{(i)} + e^{(i)}) = \\text{Var}(x_f^{(i)}) + \\text{Var}(e^{(i)}) = P_{\\text{assumed}} + R_{\\text{assumed}}$。\n因此，对于 $i \\in \\{1, \\dots, m\\}$，每个集合预报观测值 $y_f^{(i)}$ 都是从分布 $y_f^{(i)} \\sim \\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}} + R_{\\text{assumed}})$ 中抽取的独立同分布 (i.i.d.) 样本。\n\n排序直方图背后的基本原理基于可交换性的概念。如果一组随机变量的联合概率分布在变量的任何排列下都保持不变，则这组变量是可交换的。一组独立同分布的随机变量总是可交换的。\n考虑这组 $m+1$ 个随机变量 $\\{y, y_f^{(1)}, \\dots, y_f^{(m)}\\}$。如果滤波器被完美调整，那么观测的“真实预报分布”必须与“集合预报分布”相同。此条件要求：\n1. 均值相等：$\\mu_{\\text{true}} = \\mu_{\\text{assumed}}$。\n2. 方差相等：$P_{\\text{true}} + R_{\\text{true}} = P_{\\text{assumed}} + R_{\\text{assumed}}$。\n\n当这些条件满足时，所有 $m+1$ 个变量都是独立同分布的，因此是可交换的。对于任何这样一组独立同分布的连续随机变量，如果我们将它们排序，任何特定的变量（在此例中为 $y$）出现在排序后列表的 $m+1$ 个可能位置中的任何一个位置的概率都是相等的。\n排序 $r$ 定义为严格小于观测值 $y$ 的集合成员 $y_f^{(i)}$ 的数量。这等价于说，在组合集合 $\\{y, y_f^{(1)}, \\dots, y_f^{(m)}\\}$ 的排序列表中，$y$ 占据第 $(r+1)$ 个位置。因此，在完美模型的假设下，排序 $r$ 必须在整数 $\\{0, 1, \\dots, m\\}$ 上均匀分布。任何特定排序 $k$ 的概率为 $P(r=k) = \\frac{1}{m+1}$，对于 $k \\in \\{0, 1, \\dots, m\\}$。\n\n构建和检验排序直方图的算法如下：\n1. 初始化一个大小为 $m+1$ 的整数计数数组 `rank_counts`，所有元素为零。\n2. 对于 $N$ 个独立的同化循环中的每一个：\n    a. 从 $\\mathcal{N}(\\mu_{\\text{true}}, P_{\\text{true}} + R_{\\text{true}})$ 中抽取一个单一的真实观测值 $y$。\n    b. 从 $\\mathcal{N}(\\mu_{\\text{assumed}}, P_{\\text{assumed}} + R_{\\text{assumed}})$ 中抽取一个包含 $m$ 个预报观测值的集合 $\\{y_f^{(i)}\\}_{i=1}^m$。\n    c. 计算排序 $r = \\sum_{i=1}^{m} \\mathbb{I}(y_f^{(i)}  y)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数，如果条件为真则为 $1$，否则为 $0$。排序 $r$ 将是一个介于 $0$ 和 $m$ 之间的整数。\n    d. 增加相应的计数器：`rank_counts[r] = rank_counts[r] + 1`。\n3. 经过 $N$ 个循环后，数组 `rank_counts` 包含了每个排序的观测频率 $\\{O_k\\}_{k=0}^m$。\n\n为了检验均匀性，我们采用卡方 ($\\chi^2$) 拟合优度检验。\n原假设 $H_0$ 是排序服从均匀分布。\n在 $H_0$ 下，每个排序区间 $k$ 的期望频率是 $E_k = N \\times P(r=k) = \\frac{N}{m+1}$。\n$\\chi^2$ 检验统计量的计算公式为：\n$$ \\chi^2 = \\sum_{k=0}^{m} \\frac{(O_k - E_k)^2}{E_k} $$\n将此统计量与卡方分布进行比较。自由度 ($df$) 的数量是区间数减一，因为分布已由原假设完全指定。因此，$df = (m+1) - 1 = m$。\n$p$ 值是在假设 $H_0$ 为真的情况下，获得至少与观测到的检验统计量一样极端的检验统计量的概率。其计算方式为 $p = P(\\chi^2_{df=m} \\geq \\chi^2_{\\text{observed}}) = 1 - F_{\\chi^2_m}(\\chi^2_{\\text{observed}})$，其中 $F_{\\chi^2_m}$ 是具有 $m$ 个自由度的卡方分布的累积分布函数 (CDF)。\n决策规则是：如果计算出的 $p$ 值小于指定的显著性水平 $\\alpha$，我们拒绝原假设 $H_0$。否则，我们不拒绝（即接受）$H_0$。如果 $p \\ge \\alpha$，检验结果为 `True`；如果 $p  \\alpha$，则为 `False`。\n\n将此完整过程应用于问题陈述中指定的每个测试用例。固定的随机种子确保了蒙特卡洛模拟的可复现性。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Constructs and validates innovation-based rank histograms for an EnKF.\n    \"\"\"\n    # A fixed random seed is used for reproducibility.\n    rng = np.random.default_rng(42)\n\n    # Test suite parameters for Cases A through E.\n    test_cases = [\n        {\n            \"name\": \"Case A\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        },\n        {\n            \"name\": \"Case B\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 0.25, \"R_assumed\": 0.25\n        },\n        {\n            \"name\": \"Case C\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 4.0, \"R_assumed\": 4.0\n        },\n        {\n            \"name\": \"Case D\", \"m\": 20, \"N\": 10000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.5, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        },\n        {\n            \"name\": \"Case E\", \"m\": 3, \"N\": 2000, \"alpha\": 0.01,\n            \"mu_true\": 0.0, \"P_true\": 1.0, \"R_true\": 1.0,\n            \"mu_assumed\": 0.0, \"P_assumed\": 1.0, \"R_assumed\": 1.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m = case[\"m\"]\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n\n        # Parameters for the \"true\" predictive observation distribution\n        mu_y_true = case[\"mu_true\"]\n        var_y_true = case[\"P_true\"] + case[\"R_true\"]\n        std_y_true = np.sqrt(var_y_true)\n\n        # Parameters for the \"ensemble\" predictive observation distribution\n        mu_y_assumed = case[\"mu_assumed\"]\n        var_y_assumed = case[\"P_assumed\"] + case[\"R_assumed\"]\n        std_y_assumed = np.sqrt(var_y_assumed)\n\n        # Initialize the histogram for ranks {0, 1, ..., m}\n        # rank_counts has m+1 bins\n        rank_counts = np.zeros(m + 1, dtype=int)\n\n        # Run N independent assimilation cycles\n        for _ in range(N):\n            # 1. Generate one \"true\" observation y\n            y_true = rng.normal(loc=mu_y_true, scale=std_y_true)\n\n            # 2. Generate an m-member ensemble of predictive observations {y_f}\n            y_f_ensemble = rng.normal(loc=mu_y_assumed, scale=std_y_assumed, size=m)\n\n            # 3. Compute the rank of y_true among the ensemble\n            # The rank is the number of ensemble members strictly less than y_true.\n            rank = np.sum(y_f_ensemble  y_true)\n            \n            # 4. Increment the count for the computed rank\n            rank_counts[rank] += 1\n        \n        # Perform the Chi-squared goodness-of-fit test for uniformity\n        \n        # The null hypothesis H0 is that the ranks are uniformly distributed.\n        # The expected count in each of the m+1 bins is N / (m+1).\n        expected_count = N / (m + 1)\n        \n        # Calculate the chi-squared statistic\n        # chi2_stat = sum_{k=0 to m} ( (observed_k - expected_k)^2 / expected_k )\n        chi2_stat = np.sum((rank_counts - expected_count)**2 / expected_count)\n\n        # The degrees of freedom is the number of bins minus 1.\n        # df = (m + 1) - 1 = m\n        df = m\n        \n        # Calculate the p-value.\n        # p_value = P(X^2_df >= chi2_stat), where X^2_df is a chi-squared random variable.\n        # This is 1 - CDF(chi2_stat).\n        p_value = 1.0 - chi2.cdf(chi2_stat, df)\n        \n        # Decision: Accept the null hypothesis (uniformity) if p_value >= alpha.\n        is_uniform = p_value >= alpha\n        results.append(is_uniform)\n\n    # Format the final output as a comma-separated list of booleans\n    # The output format must be exactly \"[True,False,False,False,True]\" (example)\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3502567"}, {"introduction": "尽管高斯假设在贝叶斯推断中无处不在，但真实的传感器数据常常受到离群值或“重尾”噪声的污染，这会严重影响标准滤波器的性能。本练习将探讨如何通过更换似然函数来构建对异常数据具有鲁棒性的推断模型[@problem_id:3502607]。你将使用学生$t$分布作为高斯分布的替代，并实现迭代重加权最小二乘（IRLS）算法来计算最大后验（MAP）估计，从而亲身体验当数据不完美时，选择合适的统计模型对提升数字孪生可靠性的巨大价值。", "problem": "考虑一个简化的热致裂纹扩展数字孪生，其中传感器记录的声发射振幅被建模为由多物理场耦合模拟器产生的潜在断裂活动的线性图像。设传感器读数的正向模型为 $y_i = A s_i + \\varepsilon_i$，其中 $s_i$ 是由数字孪生预测的已知确定性信号，$A$ 是一个待推断的未知标量振幅参数。假设先验为高斯分布 $A \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$。考虑两种观测模型：(i) 具有尺度参数 $\\sigma$ 的正态（高斯）似然，以及 (ii) 具有自由度 $\\nu$ 和相同尺度参数 $\\sigma$ 的学生t似然。学生t似然用于测试对重尾传感器噪声的鲁棒性。仅使用贝叶斯定理以及正态和学生t分布的定义作为唯一起点。除了可以从这些定义中推导出的后验，不要假设任何其他闭式后验。使用从学生t分布的尺度混合表示推导出的迭代重加权最小二乘原理或等效的第一性原理论证来实现鲁棒推断。\n\n任务是计算两种似然下的最大后验 (MAP) 估计量 $\\hat{A}$，并通过将 MAP 误差与已知的真实值进行比较来测试对异常值的鲁棒性。对于学生t模型，还需计算拉普拉斯近似曲率（负对数后验在 MAP 处的 Hessian 矩阵），并检查其正性，这是有效的局部高斯近似所必需的。\n\n所有量均为无量纲，所有输出必须报告为无量纲值。\n\n数据和参数的定义如下。对于所有测试用例，先验参数和正向模型信号是固定的：\n- 先验均值 $\\mu_0 = 0.0$ 和先验方差 $\\tau_0^2 = 10.0$。\n- 噪声尺度 $\\sigma = 0.1$。\n- 真实振幅 $A_{\\text{true}} = 2.0$。\n- 时间索引 $i \\in \\{1,2,\\dots,25\\}$ 和正向信号 $s_i = \\exp(-0.05 i)\\left(1 + 0.5 \\sin(0.3 i)\\right)$。\n- 基线确定性伪噪声项 $n_i = 0.05 \\sin(0.7 i)$。\n\n每个测试用例的观测数据 $y_i$ 定义为 $y_i = A_{\\text{true}} s_i + n_i + o_i$，其中 $o_i$ 是每个测试中不同的异常值项。学生t分布的自由度参数 $\\nu$ 也因测试而异。\n\n测试套件：\n- 测试用例 1 (理想情况): 无异常值，即对所有 $i$，有 $o_i = 0$。学生t似然使用 $\\nu = 5$。\n- 测试用例 2 (重尾污染): 在索引 $i \\in \\{5,12,20\\}$ 处有异常值，分别为 $o_5 = 1.5$、$o_{12} = -2.0$、$o_{20} = 2.5$，其他情况下 $o_i = 0$。使用 $\\nu = 3$。\n- 测试用例 3 (极端异常值): 在索引 $i = 8$ 处有一个极端异常值，为 $o_8 = 6.0$，其他情况下 $o_i = 0$。使用 $\\nu = 1$。\n\n对于每个测试用例，执行以下操作：\n1. 仅使用第一性原理和尺度混合或等效推导得到算法，计算学生t似然下的 MAP 估计量 $\\hat{A}_{t}$。计算负对数后验在 $\\hat{A}_t$ 处的曲率 $H_t$（在这个一维参数问题中为标量 Hessian 矩阵）。报告一个布尔值，指示是否 $H_t > 0$。\n2. 仅使用第一性原理计算正态似然下的 MAP 估计量 $\\hat{A}_{g}$。\n3. 计算绝对误差 $e_t = |\\hat{A}_t - A_{\\text{true}}|$ 和 $e_g = |\\hat{A}_g - A_{\\text{true}}|$。\n4. 报告一个布尔值，指示学生t的 MAP 是否严格比正态的 MAP 更接近真实值，即是否 $e_t  e_g$。\n\n您的程序必须以完全确定的方式实现计算，不含任何随机性。对于三个测试用例中的每一个，程序应返回一个形式为 $[\\hat{A}_t,\\hat{A}_g,e_t,e_g,\\text{is\\_robust},\\text{hessian\\_positive}]$ 的列表，其中 $\\text{is\\_robust}$ 是用于 $e_t  e_g$ 的布尔值，$\\text{hessian\\_positive}$ 是用于 $H_t > 0$ 的布尔值。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个含有三个按用例划分的列表的列表，不含空格，格式为 $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$。所有数字均为无量纲。三角函数内的角度以弧度为单位。输出中不需要也不允许使用物理单位。", "solution": "该问题要求在线性模型中，在两种不同的似然假设（正态（高斯）和学生t）下，计算标量参数$A$的最大后验 (MAP) 估计量，并进行比较。这些估计量对异常值的鲁棒性将得到评估。所有推导都将从第一性原理出发。\n\n问题的核心在于贝叶斯推断。根据贝叶斯定理，给定观测数据 $\\mathbf{y} = \\{y_i\\}_{i=1}^{N}$，参数$A$的后验概率分布与似然和先验的乘积成正比：\n$$\np(A | \\mathbf{y}, \\mathbf{s}) \\propto p(\\mathbf{y} | A, \\mathbf{s}) p(A)\n$$\n其中 $\\mathbf{s} = \\{s_i\\}_{i=1}^{N}$ 是已知信号。MAP 估计 $\\hat{A}$ 是使该后验概率最大化的 $A$ 的值。最大化后验等价于最小化其负对数。我们定义负对数后验为 $L(A) = - \\ln p(\\mathbf{y} | A, \\mathbf{s}) - \\ln p(A)$，忽略不依赖于 $A$ 的常数项。\n\n模型组件定义如下：\n- **正向模型**：$y_i = A s_i + \\varepsilon_i$，其中 $\\varepsilon_i$ 是噪声项。\n- **先验分布**：关于$A$的先验信念被建模为高斯分布，$A \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$。其概率密度函数 (PDF) 为 $p(A) \\propto \\exp\\left(-\\frac{(A - \\mu_0)^2}{2\\tau_0^2}\\right)$。负对数先验为 $\\ln p(A) = - \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + \\text{const}$。\n- **似然函数**：假设给定$A$时观测值$y_i$条件独立，总似然是单个似然的乘积，$p(\\mathbf{y} | A, \\mathbf{s}) = \\prod_{i=1}^{N} p(y_i | A, s_i)$。我们考虑两种 $p(y_i | A, s_i)$ 的形式：\n    1.  **正态似然**：$p(y_i | A, s_i) = \\mathcal{N}(y_i | A s_i, \\sigma^2) \\propto \\exp\\left(-\\frac{(y_i - A s_i)^2}{2\\sigma^2}\\right)$。这对应于假设噪声 $\\varepsilon_i$ 是高斯的。\n    2.  **学生t似然**：$p(y_i | A, s_i) = \\text{St}(y_i | A s_i, \\sigma^2, \\nu) \\propto \\left(1 + \\frac{(y_i - A s_i)^2}{\\nu \\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}$。这对应于假设噪声 $\\varepsilon_i$ 服从学生t分布，其尾部比高斯分布更重，因此对异常值更具鲁棒性。\n\n### 1. 使用正态似然进行 MAP 估计\n\n对于正态似然，负对数后验 $L_g(A)$ 为：\n$$\nL_g(A) = - \\sum_{i=1}^{N} \\ln \\mathcal{N}(y_i | A s_i, \\sigma^2) - \\ln \\mathcal{N}(A | \\mu_0, \\tau_0^2) + C_g\n$$\n$$\nL_g(A) = \\sum_{i=1}^{N} \\frac{(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + C'_g\n$$\n为找到 MAP 估计 $\\hat{A}_g$，我们将 $L_g(A)$ 对 $A$ 求导并令结果为零：\n$$\n\\frac{d L_g}{d A} = \\sum_{i=1}^{N} \\frac{-s_i(y_i - A s_i)}{\\sigma^2} + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n整理各项以求解 $A$：\n$$\n\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} (A s_i^2 - s_i y_i) + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n$$\nA \\left( \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i^2 + \\frac{1}{\\tau_0^2} \\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}\n$$\n这为高斯 MAP 估计 $\\hat{A}_g$ 提供了一个闭式解：\n$$\n\\hat{A}_g = \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} s_i^2 + \\frac{1}{\\tau_0^2}}\n$$\n该表达式表示最大似然估计和先验均值的精度加权平均。\n\n### 2. 使用学生t似然进行 MAP 估计 (IRLS)\n\n对于学生t似然，负对数后验 $L_t(A)$ 为：\n$$\nL_t(A) = - \\sum_{i=1}^{N} \\ln \\text{St}(y_i | A s_i, \\sigma^2, \\nu) - \\ln \\mathcal{N}(A | \\mu_0, \\tau_0^2) + C_t\n$$\n$$\nL_t(A) = \\sum_{i=1}^{N} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - A s_i)^2}{\\nu \\sigma^2}\\right) + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} + C'_t\n$$\n对 $A$ 求导得到一个非线性方程：\n$$\n\\frac{d L_t}{d A} = \\sum_{i=1}^{N} \\frac{(\\nu+1) s_i (A s_i - y_i)}{\\nu \\sigma^2 + (y_i - A s_i)^2} + \\frac{A - \\mu_0}{\\tau_0^2} = 0\n$$\n$A$不存在闭式解。我们根据问题要求，基于学生t分布的尺度混合表示推导一个迭代算法。一个学生t变量可以表示为一个其方差在伽马分布上积分的高斯变量。具体来说，$y_i \\sim \\text{St}(A s_i, \\sigma^2, \\nu)$ 等价于分层模型：\n$$\ny_i | \\lambda_i \\sim \\mathcal{N}(A s_i, \\sigma^2/\\lambda_i) \\quad \\text{with} \\quad \\lambda_i \\sim \\text{Gamma}(\\nu/2, \\nu/2)\n$$\n这里，$\\lambda_i$是潜在精度变量。$A$和$\\{\\lambda_i\\}$的完整数据负对数后验为：\n$$\nL_t(A, \\{\\lambda_i\\}) = \\sum_{i=1}^{N} \\frac{\\lambda_i(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} - \\sum_{i=1}^{N} \\ln p(\\lambda_i) + \\text{const}\n$$\n迭代重加权最小二乘 (IRLS) 算法是一种期望最大化 (EM) 变体，它迭代地更新 $A$。\n- **E-步**：计算给定当前$A$的估计值$A^{(k)}$下，潜在变量$\\lambda_i$的期望。这个期望作为每个数据点的权重。$\\lambda_i$的后验是 $\\text{Gamma}(\\lambda_i | \\frac{\\nu+1}{2}, \\frac{\\nu\\sigma^2 + (y_i-A^{(k)}s_i)^2}{2\\sigma^2})$。期望值为：\n$$\nw_i^{(k+1)} = E[\\lambda_i | y_i, A^{(k)}] = \\frac{(\\nu+1)/2}{(\\nu\\sigma^2 + (y_i-A^{(k)}s_i)^2)/(2\\sigma^2)} = \\frac{\\nu+1}{\\nu + \\frac{(y_i - A^{(k)}s_i)^2}{\\sigma^2}}\n$$\n- **M-步**：通过最小化期望的完整数据负对数后验来更新$A$的估计，这是一个加权最小二乘问题：\n$$\nA^{(k+1)} = \\arg\\min_A \\left( \\sum_{i=1}^{N} \\frac{w_i^{(k+1)}(y_i - A s_i)^2}{2\\sigma^2} + \\frac{(A - \\mu_0)^2}{2\\tau_0^2} \\right)\n$$\n该最小化问题的解在形式上与高斯情况相同，但似然和的每一项都应用了权重 $w_i$：\n$$\nA^{(k+1)} = \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} w_i^{(k+1)} s_i y_i + \\frac{\\mu_0}{\\tau_0^2}}{\\frac{1}{\\sigma^2} \\sum_{i=1}^{N} w_i^{(k+1)} s_i^2 + \\frac{1}{\\tau_0^2}}\n$$\n从一个初始猜测（例如，$A^{(0)} = \\hat{A}_g$）开始，迭代此过程，直到 $A^{(k)}$ 收敛到学生t的 MAP 估计 $\\hat{A}_t$。\n\n### 3. 负对数后验的曲率\n\n在 MAP 估计处的负对数后验曲率是二阶导数，$H_t = \\frac{d^2 L_t}{dA^2}\\Big|_{A=\\hat{A}_t}$。这个值是拉普拉斯近似（一个以 $\\hat{A}_t$ 为中心的高斯分布）方差的倒数。正性 ($H_t > 0$) 是 $\\hat{A}_t$ 成为局部最小值以及拉普拉斯近似成为有效高斯分布（即具有正方差）的必要条件。\n\n将 $\\frac{d L_t}{d A}$ 对 $A$ 求导可得：\n$$\nH_t(A) = \\frac{d^2 L_t}{d A^2} = \\sum_{i=1}^{N} (\\nu+1)s_i^2 \\frac{\\nu\\sigma^2 - (y_i - A s_i)^2}{(\\nu\\sigma^2 + (y_i - A s_i)^2)^2} + \\frac{1}{\\tau_0^2}\n$$\n我们在 $A = \\hat{A}_t$ 处计算此表达式的值，以求得所需的曲率 $H_t$。\n\n### 4. 计算步骤\n对于每个测试用例：\n1.  使用给定的参数 $A_{\\text{true}}$、$s_i$、$n_i$ 和 $o_i$ 构建数据向量 $\\mathbf{y}$。\n2.  使用其闭式解析解直接计算 $\\hat{A}_g$。\n3.  通过执行 IRLS 算法固定次数的迭代来计算 $\\hat{A}_t$，以确保确定性收敛。\n4.  使用其解析公式在 $\\hat{A}_t$ 处计算曲率 $H_t$。检查条件 $H_t > 0$。\n5.  计算绝对误差 $e_g = |\\hat{A}_g - A_{\\text{true}}|$ 和 $e_t = |\\hat{A}_t - A_{\\text{true}}|$。\n6.  评估鲁棒性条件 $e_t  e_g$。\n然后收集结果并按规定格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes MAP estimators for a linear model with Gaussian and Student-t likelihoods\n    and assesses their robustness to outliers.\n    \"\"\"\n    # Fixed parameters\n    mu0 = 0.0\n    tau0_sq = 10.0\n    sigma = 0.1\n    sigma_sq = sigma**2\n    A_true = 2.0\n    N = 25\n    IRLS_ITERATIONS = 100\n\n    # Generate forward model signal and baseline noise\n    i_vals = np.arange(1, N + 1)\n    s = np.exp(-0.05 * i_vals) * (1.0 + 0.5 * np.sin(0.3 * i_vals))\n    n = 0.05 * np.sin(0.7 * i_vals)\n\n    # Define test cases\n    test_cases = [\n        # Case 1: No outliers, nu=5\n        {'nu': 5.0, 'outliers': {}},\n        # Case 2: Heavy-tailed contamination, nu=3\n        {'nu': 3.0, 'outliers': {5: 1.5, 12: -2.0, 20: 2.5}},\n        # Case 3: Extreme outlier, nu=1\n        {'nu': 1.0, 'outliers': {8: 6.0}},\n    ]\n\n    results = []\n\n    def compute_gaussian_map(y, s_vec, mu0_val, tau0_sq_val, sigma_sq_val):\n        \"\"\"Computes the MAP estimate for A under a Gaussian likelihood.\"\"\"\n        s_sq_sum = np.sum(s_vec**2)\n        sy_sum = np.sum(s_vec * y)\n        \n        inv_sigma_sq = 1.0 / sigma_sq_val\n        inv_tau0_sq = 1.0 / tau0_sq_val\n\n        numerator = inv_sigma_sq * sy_sum + inv_tau0_sq * mu0_val\n        denominator = inv_sigma_sq * s_sq_sum + inv_tau0_sq\n        \n        return numerator / denominator\n\n    def compute_student_t_map(y, s_vec, mu0_val, tau0_sq_val, sigma_sq_val, nu_val, A_init):\n        \"\"\"Computes the MAP estimate for A under a Student-t likelihood using IRLS.\"\"\"\n        A_k = A_init\n        inv_sigma_sq = 1.0 / sigma_sq_val\n        inv_tau0_sq = 1.0 / tau0_sq_val\n\n        for _ in range(IRLS_ITERATIONS):\n            residuals_sq = (y - A_k * s_vec)**2\n            weights = (nu_val + 1.0) / (nu_val + residuals_sq * inv_sigma_sq)\n            \n            w_s_sq_sum = np.sum(weights * s_vec**2)\n            w_sy_sum = np.sum(weights * s_vec * y)\n            \n            numerator = inv_sigma_sq * w_sy_sum + inv_tau0_sq * mu0_val\n            denominator = inv_sigma_sq * w_s_sq_sum + inv_tau0_sq\n            \n            A_k = numerator / denominator\n\n        A_hat_t = A_k\n\n        # Compute curvature (Hessian of negative log-posterior) at A_hat_t\n        residuals = y - A_hat_t * s_vec\n        \n        term1_num = nu_val * sigma_sq_val - residuals**2\n        term1_den = (nu_val * sigma_sq_val + residuals**2)**2\n        \n        hessian_sum = np.sum((nu_val + 1.0) * s_vec**2 * term1_num / term1_den)\n        H_t = hessian_sum + inv_tau0_sq\n        \n        return A_hat_t, H_t\n\n    for case in test_cases:\n        # Construct outlier vector and observed data y\n        o = np.zeros(N)\n        for idx, val in case['outliers'].items():\n            o[idx - 1] = val  # Convert 1-based index to 0-based\n        \n        y = A_true * s + n + o\n        nu = case['nu']\n\n        # 1. Compute MAP for Normal likelihood\n        A_hat_g = compute_gaussian_map(y, s, mu0, tau0_sq, sigma_sq)\n\n        # 2. Compute MAP and Hessian for Student-t likelihood\n        # Initialize IRLS with the Gaussian MAP estimate\n        A_hat_t, H_t = compute_student_t_map(y, s, mu0, tau0_sq, sigma_sq, nu, A_hat_g)\n\n        # 3. Compute errors\n        e_t = abs(A_hat_t - A_true)\n        e_g = abs(A_hat_g - A_true)\n        \n        # 4. Check robustness and Hessian positivity\n        is_robust = e_t  e_g\n        hessian_positive = H_t > 0.0\n        \n        # Collect results for this case\n        case_results = [A_hat_t, A_hat_g, e_t, e_g, is_robust, hessian_positive]\n        results.append(case_results)\n\n    # Format final output string as a list of lists with no spaces\n    # Convert bools to lowercase 'true'/'false' as per python str() default\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```", "id": "3502607"}, {"introduction": "一个真正“鲜活”的数字孪生不仅能追踪系统的当前状态，还应能从数据中学习并优化其自身的底层模型参数。本练习将带你挑战状态与参数联合估计这一核心问题，通过设计一个交替进行的状态估计和参数学习方案来解决它[@problem_id:3502593]。你将把这个迭代过程构建为一个仿射迭代，并从理论上分析其收敛性，这是开发可靠、自适应数字孪生时必须考虑的关键问题，它深刻地联系了优化、贝叶斯估计和数值分析。", "problem": "一个开发人员的任务是为一个多物理场耦合仿真中的数字孪生设计一个训练-同化方案，该方案在参数学习和状态估计之间交替进行，并使用贝叶斯推断和数据同化，在收缩假设下分析其向不动点的收敛性。考虑一个定义如下的线性高斯模型。状态为 $x \\in \\mathbb{R}^n$，标量参数为 $\\theta \\in \\mathbb{R}$，观测值为 $y \\in \\mathbb{R}^n$。观测模型为 $y = C x + \\varepsilon$，其中 $C \\in \\mathbb{R}^{n \\times n}$，高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, R)$，且 $R \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。给定参数的状态先验为 $x \\mid \\theta \\sim \\mathcal{N}(B \\theta, P)$，其中 $B \\in \\mathbb{R}^{n \\times 1}$，且 $P \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。参数的先验为 $\\theta \\sim \\mathcal{N}(\\theta_{\\text{prior}}, S)$，其中 $S \\in \\mathbb{R}_{0}$。\n\n将最大后验（MAP）状态估计器定义为二次目标函数\n$$\nJ(x \\mid \\theta) = (y - C x)^\\top R^{-1} (y - C x) + (x - B \\theta)^\\top P^{-1} (x - B \\theta),\n$$\n的最小化子 $x_{\\text{MAP}}(\\theta)$，并将MAP参数估计器 $\\theta_{\\text{MAP}}(x)$ 定义为\n$$\nL(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + (\\theta - \\theta_{\\text{prior}})^\\top S^{-1} (\\theta - \\theta_{\\text{prior}}).\n$$\n的最小化子。考虑一个交替的、欠松弛的训练-同化方案\n$$\nx_{k+1} = x_k + \\alpha_x \\left( x_{\\text{MAP}}(\\theta_k) - x_k \\right),\n\\quad\n\\theta_{k+1} = \\theta_k + \\alpha_\\theta \\left( \\theta_{\\text{MAP}}(x_{k+1}) - \\theta_k \\right),\n$$\n松弛系数为 $ \\alpha_x \\in \\mathbb{R}$ 和 $ \\alpha_\\theta \\in \\mathbb{R}$。证明该方案可以写成关于增广变量 $z_k \\in \\mathbb{R}^{n+1}$（定义为 $z_k = [x_k; \\theta_k]$）的单个仿射迭代：\n$$\nz_{k+1} = A z_k + b,\n$$\n对于某个依赖于 $C, R, P, B, S, y, \\theta_{\\text{prior}}, \\alpha_x, \\alpha_\\theta$ 的矩阵 $A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ 和向量 $b \\in \\mathbb{R}^{n+1}$。从高斯后验的核心定义和线性最小二乘法的正规方程出发，推导出 $A$ 和 $b$ 的显式表达式。然后，使用有限维空间中的巴拿赫不动点原理和诺伊曼级数特征，根据 $A$ 的谱半径分析收敛条件。具体来说，证明如果谱半径 $\\rho(A)  1$，则存在唯一的不动点 $z^\\star$ 满足\n$$\nz^\\star = A z^\\star + b,\n$$\n并且对于任何初始值 $z_0$，迭代都收敛于 $z^\\star$。当 $\\rho(A)  1$ 时，推导出闭式不动点 $z^\\star = (I - A)^{-1} b$。\n\n您的程序必须：\n- 根据给定的模型参数构造 $A$ 和 $b$。\n- 计算谱半径 $\\rho(A)$。\n- 从指定的初始条件 $z_0$ 开始，运行迭代 $z_{k+1} = A z_k + b$，直至达到固定的最大迭代次数，如果迭代差满足 $\\|z_{k+1} - z_k\\|_2  \\varepsilon$（其中 $\\varepsilon$ 为容差），则提前停止。\n- 如果 $\\rho(A)  1$，计算不动点 $z^\\star = (I - A)^{-1} b$ 并报告最终误差 $e = \\|z_N - z^\\star\\|_2$，其中 $z_N$ 是最后一次迭代的结果。\n- 如果 $\\rho(A) \\ge 1$，将误差 $e$ 设置为 $+\\infty$。\n- 报告一个收敛指标 $c$，它是一个十进制数。如果 $ \\rho(A)  1$ 且最终误差 $e$ 小于容差 $\\varepsilon$，则其值为 $1.0$，否则为 $0.0$。\n\n在所有情况下，使用以下测试套件，其中 $n = 2$，并使用指定的数值。下面提供的所有数字都必须完全按照给定的方式实现：\n- 测试用例 1 (正常路径):\n  - $C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $P = \\begin{bmatrix} 10  0 \\\\ 0  10 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 3 \\\\ 3 \\end{bmatrix}$,\n  - $S = 0.2$,\n  - $y = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$,\n  - $\\theta_{\\text{prior}} = 0.0$,\n  - $\\alpha_x = 0.6$,\n  - $\\alpha_\\theta = 0.4$,\n  - $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- 测试用例 2 (近边界收缩):\n  - $C = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $P = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}$,\n  - $B = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$,\n  - $S = 1000000.0$,\n  - $y = \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix}$,\n  - $\\theta_{\\text{prior}} = 0.0$,\n  - $\\alpha_x = 1.0$,\n  - $\\alpha_\\theta = 1.0$,\n  - $z_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- 测试用例 3 (通过过松弛导致不收缩):\n  - 与测试用例 2 相同，除了 $\\alpha_x = 1.2$ 和 $\\alpha_\\theta = 1.0$。\n\n角度单位不适用。不涉及物理单位，因此以数学单位作答即可。最大迭代次数为 $N_{\\max} = 200$，容差为 $\\varepsilon = 10^{-8}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序和类型如下：\n$$\n[\\rho_1, c_1, e_1, \\rho_2, c_2, e_2, \\rho_3, c_3, e_3],\n$$\n其中 $\\rho_i$ 是测试用例 $i$ 的谱半径（浮点数），$c_i$ 是测试用例 $i$ 的收敛指标（浮点数，等于 $1.0$ 或 $0.0$），$e_i$ 是测试用例 $i$ 的最终误差（浮点数；如果 $\\rho_i \\ge 1$，则使用 $+\\infty$）。输出必须是这种确切格式的单行文本。", "solution": "该问题是有效的。这是一个在数值分析、贝叶斯推断和数据同化领域中提法恰当、有科学依据的问题，并提供了所有必要的数据和定义。我们将着手解决。\n\n目标是为耦合的训练-同化方案推导出仿射迭代 $z_{k+1} = A z_k + b$，然后对其收敛性进行数值分析。增广状态为 $z_k = [x_k^\\top, \\theta_k]^\\top \\in \\mathbb{R}^{n+1}$。\n\n首先，我们推导最大后验（MAP）估计器 $x_{\\text{MAP}}(\\theta)$ 和 $\\theta_{\\text{MAP}}(x)$ 的显式形式。通过最小化它们各自的二次目标函数来找到这些估计器，这可以通过将其梯度设置为零来实现。\n\n状态 $x$ 的目标函数为：\n$$\nJ(x \\mid \\theta) = (y - C x)^\\top R^{-1} (y - C x) + (x - B \\theta)^\\top P^{-1} (x - B \\theta)\n$$\n关于 $x$ 的梯度为：\n$$\n\\nabla_x J(x \\mid \\theta) = -2 C^\\top R^{-1} (y - C x) + 2 P^{-1} (x - B \\theta) = 0\n$$\n$$\nC^\\top R^{-1} C x - C^\\top R^{-1} y + P^{-1}x - P^{-1}B\\theta = 0\n$$\n$$\n(C^\\top R^{-1} C + P^{-1}) x = C^\\top R^{-1} y + P^{-1} B \\theta\n$$\n由于 $R$ 和 $P$ 是对称正定（SPD）矩阵，因此 $R^{-1}$ 和 $P^{-1}$ 也是。矩阵 $H_x = C^\\top R^{-1} C + P^{-1}$ 是一个半正定矩阵与一个正定矩阵之和，因此是正定且可逆的。求解 $x$ 得到MAP估计器：\n$$\nx_{\\text{MAP}}(\\theta) = (C^\\top R^{-1} C + P^{-1})^{-1} (C^\\top R^{-1} y + P^{-1} B \\theta)\n$$\n我们定义 $M_x = (C^\\top R^{-1} C + P^{-1})^{-1}$。$x_{\\text{MAP}}(\\theta)$ 的表达式是 $\\theta$ 的一个仿射函数：\n$$\nx_{\\text{MAP}}(\\theta) = (M_x P^{-1} B) \\theta + (M_x C^\\top R^{-1} y)\n$$\n\n接下来，我们求解标量参数 $\\theta$ 的MAP估计器。目标函数为：\n$$\nL(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + (\\theta - \\theta_{\\text{prior}})^\\top S^{-1} (\\theta - \\theta_{\\text{prior}})\n$$\n由于 $\\theta$ 是一个标量，所以该函数为 $L(\\theta \\mid x) = (x - B \\theta)^\\top P^{-1} (x - B \\theta) + S^{-1} (\\theta - \\theta_{\\text{prior}})^2$。\n关于 $\\theta$ 的导数为：\n$$\n\\frac{dL}{d\\theta} = -2 B^\\top P^{-1} (x - B \\theta) + 2 S^{-1} (\\theta - \\theta_{\\text{prior}}) = 0\n$$\n$$\n-B^\\top P^{-1} x + (B^\\top P^{-1} B) \\theta + S^{-1} \\theta - S^{-1} \\theta_{\\text{prior}} = 0\n$$\n$$\n(B^\\top P^{-1} B + S^{-1}) \\theta = B^\\top P^{-1} x + S^{-1} \\theta_{\\text{prior}}\n$$\n标量项 $H_\\theta = B^\\top P^{-1} B + S^{-1}$ 是严格为正的，因为 $P^{-1}$ 是对称正定的且 $S > 0$。因此，它是可逆的。求解 $\\theta$ 得到MAP估计器：\n$$\n\\theta_{\\text{MAP}}(x) = (B^\\top P^{-1} B + S^{-1})^{-1} (B^\\top P^{-1} x + S^{-1} \\theta_{\\text{prior}})\n$$\n我们定义标量 $M_\\theta = (B^\\top P^{-1} B + S^{-1})^{-1}$。$\\theta_{\\text{MAP}}(x)$ 的表达式是 $x$ 的一个仿射函数：\n$$\n\\theta_{\\text{MAP}}(x) = (M_\\theta B^\\top P^{-1}) x + (M_\\theta S^{-1} \\theta_{\\text{prior}})\n$$\n\n现在，我们将这些估计器代入欠松弛迭代方案中：\n1. $x_{k+1} = x_k + \\alpha_x ( x_{\\text{MAP}}(\\theta_k) - x_k ) = (1-\\alpha_x)x_k + \\alpha_x x_{\\text{MAP}}(\\theta_k)$\n2. $\\theta_{k+1} = \\theta_k + \\alpha_\\theta ( \\theta_{\\text{MAP}}(x_{k+1}) - \\theta_k ) = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta \\theta_{\\text{MAP}}(x_{k+1})$\n\n将推导出的 $x_{\\text{MAP}}(\\theta_k)$ 表达式代入第一个方程：\n$$\nx_{k+1} = (1-\\alpha_x)x_k + \\alpha_x \\left[ (M_x P^{-1} B) \\theta_k + (M_x C^\\top R^{-1} y) \\right]\n$$\n$$\nx_{k+1} = (1-\\alpha_x)I x_k + (\\alpha_x M_x P^{-1} B) \\theta_k + (\\alpha_x M_x C^\\top R^{-1} y)\n$$\n这提供了我们的仿射系统 $z_{k+1}=Az_k+b$ 的顶部块。\n\n接下来，我们将 $x_{k+1}$ 的表达式代入 $\\theta_{k+1}$ 的方程中：\n$$\n\\theta_{k+1} = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta \\left[ (M_\\theta B^\\top P^{-1}) x_{k+1} + (M_\\theta S^{-1} \\theta_{\\text{prior}}) \\right]\n$$\n$$\n\\theta_{k+1} = (1-\\alpha_\\theta)\\theta_k + \\alpha_\\theta M_\\theta B^\\top P^{-1} \\left[ (1-\\alpha_x)I x_k + (\\alpha_x M_x P^{-1} B) \\theta_k + (\\alpha_x M_x C^\\top R^{-1} y) \\right] + \\alpha_\\theta M_\\theta S^{-1} \\theta_{\\text{prior}}\n$$\n我们按 $x_k$、$\\theta_k$ 和常数项对各项进行分组：\n- 含 $x_k$ 的项： $(\\alpha_\\theta (1-\\alpha_x) M_\\theta B^\\top P^{-1}) x_k$\n- 含 $\\theta_k$ 的项： $\\left( (1-\\alpha_\\theta) + \\alpha_\\theta \\alpha_x M_\\theta B^\\top P^{-1} M_x P^{-1} B \\right) \\theta_k$\n- 常数项： $\\alpha_\\theta M_\\theta \\alpha_x B^\\top P^{-1} M_x C^\\top R^{-1} y + \\alpha_\\theta M_\\theta S^{-1} \\theta_{\\text{prior}}$\n\n根据这些表达式，我们可以构造分块形式的矩阵 $A \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ 和向量 $b \\in \\mathbb{R}^{n+1}$，其中 $z_k = [x_k^\\top, \\theta_k]^\\top$：\n$$\nA = \\begin{bmatrix} A_{xx}  A_{x\\theta} \\\\ A_{\\theta x}  A_{\\theta\\theta} \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_x \\\\ b_\\theta \\end{bmatrix}\n$$\n这些块是：\n$A_{xx} = (1 - \\alpha_x) I_{n \\times n}$\n$A_{x\\theta} = \\alpha_x M_x P^{-1} B$\n$A_{\\theta x} = \\alpha_\\theta (1 - \\alpha_x) M_\\theta B^\\top P^{-1}$\n$A_{\\theta\\theta} = (1 - \\alpha_\\theta) + \\alpha_\\theta \\alpha_x M_\\theta (B^\\top P^{-1} M_x P^{-1} B)$\n\n$b_x = \\alpha_x M_x C^\\top R^{-1} y$\n$b_\\theta = \\alpha_\\theta M_\\theta ( \\alpha_x B^\\top P^{-1} M_x C^\\top R^{-1} y + S^{-1} \\theta_{\\text{prior}})$\n\n迭代 $z_{k+1} = A z_k + b$ 是一个仿射变换。根据巴拿赫不动点定理，对于任何起始点 $z_0$，此迭代收敛到唯一不动点 $z^\\star$ 的充分必要条件是 $A$ 是一个压缩映射。对于有限维空间中的线性算子，此条件等价于其谱半径小于1：$\\rho(A)  1$。谱半径定义为 $\\rho(A) = \\max_i |\\lambda_i|$，其中 $\\lambda_i$ 是 $A$ 的特征值。\n\n如果 $\\rho(A)  1$，不动点 $z^\\star$ 满足 $z^\\star = A z^\\star + b$，可以重排为 $(I - A) z^\\star = b$。由于 $\\rho(A)  1$，$1$ 不是 $A$ 的特征值，所以 $(I-A)$ 是可逆的。唯一不动点由 $z^\\star = (I - A)^{-1} b$ 给出。逆矩阵 $(I-A)^{-1}$ 可以用收敛的诺伊曼级数 $\\sum_{k=0}^{\\infty} A^k$ 来表示。\n\n数值实现将构造 $A$ 和 $b$，计算 $\\rho(A)$，执行迭代，并计算相对于解析确定的不动点 $z^\\star$ 的最终误差。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It orchestrates the calculation for each case and prints the final result.\n    \"\"\"\n\n    # Constants\n    N_max = 200\n    epsilon = 1e-8\n    n = 2\n\n    # Test Case 1: Happy path\n    case1 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[10.0, 0.0], [0.0, 10.0]]),\n        \"B\": np.array([[3.0], [3.0]]),\n        \"S\": 0.2,\n        \"y\": np.array([[1.0], [-1.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 0.6,\n        \"alpha_theta\": 0.4,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    # Test Case 2: Near-boundary contraction\n    case2 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        \"B\": np.array([[10.0], [10.0]]),\n        \"S\": 1000000.0,\n        \"y\": np.array([[2.0], [-2.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 1.0,\n        \"alpha_theta\": 1.0,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    # Test Case 3: Non-contraction via over-relaxation\n    case3 = {\n        \"C\": np.eye(2),\n        \"R\": np.eye(2),\n        \"P\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        \"B\": np.array([[10.0], [10.0]]),\n        \"S\": 1000000.0,\n        \"y\": np.array([[2.0], [-2.0]]),\n        \"theta_prior\": 0.0,\n        \"alpha_x\": 1.2,\n        \"alpha_theta\": 1.0,\n        \"z0\": np.zeros((n + 1, 1))\n    }\n\n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case_params in test_cases:\n        rho, c, e = process_case(case_params, n, N_max, epsilon)\n        results.extend([rho, c, e])\n\n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef process_case(params, n, N_max, epsilon):\n    \"\"\"\n    Processes a single test case: constructs A and b, computes convergence\n    properties, runs the iteration, and calculates the final error.\n    \"\"\"\n    # Unpack parameters\n    C, R, P, B, S, y = params[\"C\"], params[\"R\"], params[\"P\"], params[\"B\"], params[\"S\"], params[\"y\"]\n    theta_prior, alpha_x, alpha_theta = params[\"theta_prior\"], params[\"alpha_x\"], params[\"alpha_theta\"]\n    z0 = params[\"z0\"]\n\n    # Compute matrix inverses\n    try:\n        R_inv = np.linalg.inv(R)\n        P_inv = np.linalg.inv(P)\n    except np.linalg.LinAlgError:\n        # Handle singular matrices, though not expected from problem spec\n        return np.inf, 0.0, np.inf\n\n    # Compute intermediate quantities M_x and M_theta\n    Mx_inv = C.T @ R_inv @ C + P_inv\n    Mx = np.linalg.inv(Mx_inv)\n    \n    M_theta_inv_val = (B.T @ P_inv @ B)[0, 0] + 1.0/S\n    M_theta = 1.0 / M_theta_inv_val\n\n    # Construct the iteration matrix A\n    A_xx = (1.0 - alpha_x) * np.eye(n)\n    A_xtheta = alpha_x * Mx @ P_inv @ B\n    A_thetax = alpha_theta * (1.0 - alpha_x) * M_theta * (B.T @ P_inv)\n    A_thetatheta_val = (1.0 - alpha_theta) + alpha_theta * alpha_x * M_theta * (B.T @ P_inv @ Mx @ P_inv @ B)[0, 0]\n    A_thetatheta = np.array([[A_thetatheta_val]])\n    A = np.block([[A_xx, A_xtheta], [A_thetax, A_thetatheta]])\n\n    # Construct the constant vector b\n    b_x = alpha_x * Mx @ C.T @ R_inv @ y\n    b_theta_val = alpha_theta * M_theta * (alpha_x * (B.T @ P_inv @ Mx @ C.T @ R_inv @ y)[0, 0] + (1.0/S) * theta_prior)\n    b_theta = np.array([[b_theta_val]])\n    b = np.vstack((b_x, b_theta))\n\n    # Compute spectral radius of A\n    eigenvalues = np.linalg.eigvals(A)\n    rho = np.max(np.abs(eigenvalues))\n\n    # Perform the iteration\n    z = z0.copy()\n    z_N = z0\n    for _ in range(N_max):\n        z_next = A @ z + b\n        diff = np.linalg.norm(z_next - z)\n        z = z_next\n        if diff  epsilon:\n            break\n    z_N = z\n\n    # Compute fixed point and final error\n    if rho >= 1:\n        e = np.inf\n        c = 0.0\n    else:\n        # Compute the true fixed point z_star\n        I_mat = np.eye(n + 1)\n        z_star = np.linalg.inv(I_mat - A) @ b\n        \n        # Compute the final error\n        e = np.linalg.norm(z_N - z_star)\n        \n        # Determine convergence indicator\n        c = 1.0 if e  epsilon else 0.0\n\n    return rho, c, e\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3502593"}]}