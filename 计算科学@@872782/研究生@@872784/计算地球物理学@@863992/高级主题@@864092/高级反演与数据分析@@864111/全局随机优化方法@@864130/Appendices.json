{"hands_on_practices": [{"introduction": "在深入研究复杂的全局优化方法之前，理解其必要性至关重要。本练习旨在通过一个基础的编码实践，揭示局部优化方法在处理非凸问题时的局限性。我们将对比一个简单的随机采样全局搜索与一个标准的基于梯度的局部搜索方法，以具体展示“盆地陷阱”现象，即算法被困在次优局部最小值中。通过这个练习 [@problem_id:3600613]，你将直观地感受到为何需要更复杂的全局搜索策略来处理地球物理反演中常见的复杂失配函数。", "problem": "您需要在一个一维非凸目标函数上实现并比较两种全局随机优化策略。该目标函数通常在计算地球物理学中用作盆地陷阱的玩具模型。考虑在有界域 $$\\mathcal{X}=[-4,4]$$ 上定义的标量目标函数 $$f(x)=\\sin(5x)+0.1\\,x^2$$，其中所有角度均以弧度为单位，且 $x$ 是无量纲的。您的目标是近似求解一个全局最小值点，并说明随机全局搜索与局部、单点启动的梯度下降法之间的差异。\n\n从以下基本原理开始：\n- 全局最小化问题是计算 $$\\min_{x\\in\\mathcal{X}} f(x)$$。\n- 区间上的独立同分布均匀采样使用伪随机数生成器抽取样本 $$x_i\\sim \\mathrm{Uniform}([-4,4])$$，并根据目标函数选择最佳样本。\n- 在一维情况下，使用恒定步长的梯度下降法采用更新规则 $$x_{k+1}=\\Pi_{\\mathcal{X}}\\bigl(x_k-\\alpha\\,f'(x_k)\\bigr)$$，其中 $$\\Pi_{\\mathcal{X}}$$ 表示到区间 $$\\mathcal{X}$$ 上的欧几里得投影（即，将值钳位到 $$[-4,4]$$），$$\\alpha>0$$ 是一个固定的步长，$$f'(x)$$ 是使用链式法则和微分的线性性质计算出的一阶导数。\n\n需要实现的任务：\n1) 使用导数定义和链式法则，为给定的 $$f(x)$$ 推导并实现其导数 $$f'(x)$$。\n2) 实现一个随机全局搜索方法，该方法使用一个带有指定整数种子 $$s$$ 的伪随机数生成器，在 $$[-4,4]$$ 上抽取 $$n$$ 个独立均匀样本，对每个样本计算 $$f(x)$$ 的值，并返回具有最小目标值的样本位置以及该最小值。\n3) 实现一个单点启动的投影梯度下降法，给定初始点 $$x_0\\in[-4,4]$$、恒定步长 $$\\alpha>0$$ 和固定迭代次数 $$K\\in\\mathbb{N}$$。每次更新后，通过 $$\\Pi_{\\mathcal{X}}(x)=\\min(\\max(x,-4),4)$$ 将迭代点投影回 $$[-4,4]$$。\n\n比较指标和决策规则：\n- 对于每个测试用例，计算随机采样得到的近似最优值 $$f_{\\mathrm{rand}}:=\\min_{i=1,\\dots,n} f(x_i)$$，（在对应的点 $$x_{\\mathrm{rand}}$$ 处），以及梯度下降后的最终值 $$f_{\\mathrm{gd}}:=f(x_K)$$，（在最终点 $$x_{\\mathrm{gd}}$$ 处）。\n- 定义 $$\\Delta:=f_{\\mathrm{gd}}-f_{\\mathrm{rand}}$$。 如果 $$\\Delta>\\tau$$，则认为梯度下降法陷入了次优盆地，其中 $$\\tau=10^{-3}$$。\n\n数值和输出要求：\n- 所有三角函数求值均使用弧度。没有物理单位。\n- 对于每个测试用例，返回一个列表 $$[x_{\\mathrm{rand}}, f_{\\mathrm{rand}}, x_{\\mathrm{gd}}, f_{\\mathrm{gd}}, \\text{trapped}]$$，其中前四个条目是四舍五入到六位小数的浮点数，最后一个条目是使用阈值 $$\\tau=10^{-3}$$ 计算出的布尔值。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表按顺序汇总了所有测试用例的结果，例如 $$[r_1,r_2,\\dots,r_m]$$。\n\n需要实现和评估的测试套件：\n- 用例 A（在最优盆地附近的基线成功案例）：$$s=123$$, $$n=500$$, $$x_0=0.0$$, $$\\alpha=0.05$$, $$K=200$$。\n- 用例 B（在远处的局部盆地中陷入）：$$s=7$$, $$n=2000$$, $$x_0=2.8$$, $$\\alpha=0.05$$, $$K=200$$。\n- 用例 C（欠采样压力测试）：$$s=42$$, $$n=20$$, $$x_0=-3.5$$, $$\\alpha=0.02$$, $$K=150$$。\n- 用例 D（零迭代梯度下降边界情况）：$$s=0$$, $$n=3000$$, $$x_0=1.0$$, $$\\alpha=0.05$$, $$K=0$$。\n\n您的程序必须精确实现上述逻辑，并以 $$[[x_{\\mathrm{rand}}^{(A)},f_{\\mathrm{rand}}^{(A)},x_{\\mathrm{gd}}^{(A)},f_{\\mathrm{gd}}^{(A)},\\mathrm{trapped}^{(A)}],\\dots,[x_{\\mathrm{rand}}^{(D)},f_{\\mathrm{rand}}^{(D)},x_{\\mathrmgpd}}^{(D)},f_{\\mathrm{gd}}^{(D)},\\mathrm{trapped}^{(D)}]]$$ 的格式打印单行的汇总结果。", "solution": "该问题是有效的，因为它在数值优化领域提出了一个定义明确、自洽且科学上合理的计算任务。它要求在一个指定的非凸函数上实现和比较两种标准优化算法：随机全局搜索和梯度下降。所有参数、条件和评估指标都已提供，确保了问题的客观性，并允许得到一个唯一、可验证的解。\n\n问题的核心是在一个多峰目标函数上比较全局优化策略与局部优化策略。选定的函数 $$f(x)=\\sin(5x)+0.1\\,x^2$$ 定义在紧致域 $$\\mathcal{X}=[-4,4]$$ 上，是非凸的。这意味着它拥有多个局部极小值点，使其成为一个合适的测试用例，用以说明局部搜索方法会“陷入”次优吸引盆地的现象，而全局搜索则有更高的概率识别出全局最小值点。\n\n方法步骤如下：首先，我们推导函数导数的解析形式，这是梯度下降算法所必需的。其次，我们规范化随机搜索和投影梯度下降的流程。最后，我们将这些算法应用于指定的测试用例，并使用定义的比较指标来评估它们的性能。\n\n1.  **目标函数及其导数**\n目标函数为：\n$$f(x)=\\sin(5x)+0.1\\,x^2$$\n为了实现梯度下降，我们必须计算其一阶导数 $$f'(x)$$。我们应用微分的线性性质（即和的导数等于导数的和）以及对三角函数项应用链式法则。\n第一项 $$\\sin(5x)$$ 关于 $$x$$ 的导数使用链式法则 $$d/dx(\\sin(u)) = \\cos(u) \\cdot du/dx$$ 求得。这里，$$u=5x$$，所以 $$du/dx=5$$。\n$$\\frac{d}{dx}\\left(\\sin(5x)\\right) = \\cos(5x) \\cdot 5 = 5\\cos(5x)$$\n第二项 $$0.1x^2$$ 的导数使用幂法则求得：\n$$\\frac{d}{dx}\\left(0.1x^2\\right) = 0.1 \\cdot 2x = 0.2x$$\n结合这些结果，得到完整的导数：\n$$f'(x) = 5\\cos(5x) + 0.2x$$\n按规定，所有三角计算都以弧度为单位进行。\n\n2.  **随机全局搜索：均匀随机采样**\n该方法对搜索空间 $$\\mathcal{X}=[-4,4]$$ 进行全局探索。其原理基于概率性覆盖。通过抽取足够大量的、独立同分布的样本 $$x_i \\sim \\mathrm{Uniform}([-4,4])$$（数量为 $$n$$），该算法很可能将至少一个样本置于全局最小值的吸引盆地内。该方法对函数的局部结构（如其梯度）是“盲目”的，纯粹依赖于在随机点上的函数求值。算法如下：\n-   用指定的整数种子 $$s$$ 初始化一个伪随机数生成器，以确保可复现性。\n-   从区间 $$[-4,4]$$ 中均匀生成 $$n$$ 个随机样本 $$\\{x_1, x_2, \\dots, x_n\\}$$。\n-   对每个样本 $$x_i$$ 计算 $$f(x_i)$$。\n-   确定产生最小函数值 $$f_{\\mathrm{rand}} = \\min_{i=1,\\dots,n} f(x_i)$$ 的样本 $$x_{\\mathrm{rand}}$$。\n\n3.  **局部搜索：投影梯度下降**\n这是一种一阶迭代优化算法，旨在寻找局部最小值。它从一个初始点 $$x_0$$ 开始，并沿着负梯度方向迭代移动，该方向是局部最陡峭的下降方向。\n迭代更新规则为：\n$$x_{k+1} = x_k - \\alpha f'(x_k)$$\n其中 $$k$$ 是迭代次数，$$x_k$$ 是当前点，$$f'(x_k)$$ 是在该点的梯度，而 $$\\alpha>0$$ 是一个控制每一步大小的恒定步长。\n由于问题被约束在域 $$\\mathcal{X}=[-4,4]$$ 上，我们必须确保每个新的迭代点 $$x_{k+1}$$ 都保持在该域内。这是通过在每次更新后应用一个欧几里得投影算子 $$\\Pi_{\\mathcal{X}}$$ 来实现的。对于一维区间，这个投影等价于对值进行钳位：\n$$\\Pi_{\\mathcal{X}}(z) = \\min(\\max(z,-4),4)$$\n完整的算法如下：\n-   用起始点 $$x_0$$ 初始化迭代点 $$x$$。\n-   对于固定的迭代次数 $$K$$：\n    1.  计算梯度 $$g_k = f'(x_k)$$。\n    2.  执行下降步：$$x_{\\text{temp}} = x_k - \\alpha g_k$$。\n    3.  将结果投影回域中：$$x_{k+1} = \\Pi_{\\mathcal{X}}(x_{\\text{temp}})$$.\n-   最终迭代点 $$x_{\\mathrm{gd}} = x_K$$ 及其对应的函数值 $$f_{\\mathrm{gd}} = f(x_K)$$ 是算法的结果。\n\n4.  **比较和决策规则**\n比较的目的是判断局部梯度下降法是否“陷入”了一个显著差于全局随机搜索找到的最优最小值的局部最小值中。性能差异由 $$\\Delta = f_{\\mathrm{gd}} - f_{\\mathrm{rand}}$$ 量化。\n一个正的 $$\\Delta$$ 表明随机搜索找到了比梯度下降更好的解（一个更低的最小值）。问题为陷入定义了一个具体标准：如果这个差异超过了给定的阈值 $$\\tau=10^{-3}$$，则认为梯度下降被陷入。\n决策规则是：\n$$\\text{trapped} = (\\Delta > \\tau)$$\n这个布尔结果，连同两种方法的坐标和函数值，为每个测试用例提供了完整的概况。\n对于用例 D，其中迭代次数 $$K=0$$，梯度下降过程会立即终止。因此，其最终点 $$x_{\\mathrm{gd}}$$ 就是其初始点 $$x_0$$，并且 $$f_{\\mathrm{gd}} = f(x_0)$$。这个用例实际上是将一个预先确定的点与大规模随机搜索进行比较。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares stochastic global search and projected gradient descent\n    on a nonconvex objective function for several test cases.\n    \"\"\"\n\n    # Define the objective function and its derivative.\n    # f(x) = sin(5x) + 0.1 * x^2\n    def f(x: np.ndarray | float) - np.ndarray | float:\n        return np.sin(5 * x) + 0.1 * x**2\n\n    # f'(x) = 5*cos(5x) + 0.2*x\n    def df(x: np.ndarray | float) - np.ndarray | float:\n        return 5 * np.cos(5 * x) + 0.2 * x\n\n    def stochastic_search(n: int, s: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs a stochastic global search using uniform random sampling.\n\n        Args:\n            n: Number of samples.\n            s: Seed for the pseudorandom number generator.\n            domain: The search interval (min, max).\n\n        Returns:\n            A tuple (x_rand, f_rand) containing the location and value of the\n            best sample found.\n        \"\"\"\n        rng = np.random.default_rng(s)\n        samples = rng.uniform(domain[0], domain[1], n)\n        values = f(samples)\n        min_index = np.argmin(values)\n        x_rand = samples[min_index]\n        f_rand = values[min_index]\n        return x_rand, f_rand\n\n    def gradient_descent(x0: float, alpha: float, K: int, domain: tuple[float, float]):\n        \"\"\"\n        Performs projected gradient descent.\n\n        Args:\n            x0: Initial point.\n            alpha: Constant step size.\n            K: Number of iterations.\n            domain: The search interval (min, max) for projection.\n\n        Returns:\n            A tuple (x_gd, f_gd) containing the final location and value.\n        \"\"\"\n        x_k = x0\n        for _ in range(K):\n            grad = df(x_k)\n            x_k = x_k - alpha * grad\n            # Project back onto the domain (clamping)\n            x_k = np.clip(x_k, domain[0], domain[1])\n        \n        x_gd = x_k\n        f_gd = f(x_gd)\n        return x_gd, f_gd\n\n    # Define common parameters and test cases.\n    domain = (-4.0, 4.0)\n    tau = 1e-3\n\n    test_cases = [\n        # Case A (baseline success near the best basin)\n        {'s': 123, 'n': 500, 'x0': 0.0, 'alpha': 0.05, 'K': 200},\n        # Case B (entrapment in a distant local basin)\n        {'s': 7, 'n': 2000, 'x0': 2.8, 'alpha': 0.05, 'K': 200},\n        # Case C (undersampling stress test)\n        {'s': 42, 'n': 20, 'x0': -3.5, 'alpha': 0.02, 'K': 150},\n        # Case D (zero-iteration gradient descent boundary)\n        {'s': 0, 'n': 3000, 'x0': 1.0, 'alpha': 0.05, 'K': 0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Run stochastic search\n        x_rand, f_rand = stochastic_search(case['n'], case['s'], domain)\n\n        # Run gradient descent\n        x_gd, f_gd = gradient_descent(case['x0'], case['alpha'], case['K'], domain)\n\n        # Compare results and determine if trapped\n        delta = f_gd - f_rand\n        trapped = delta  tau\n\n        # Format the result list for this case\n        result_list = [\n            round(float(x_rand), 6),\n            round(float(f_rand), 6),\n            round(float(x_gd), 6),\n            round(float(f_gd), 6),\n            trapped\n        ]\n        all_results.append(result_list)\n    \n    # Print the aggregated results in the specified format\n    # The format is a string representation of a Python list of lists.\n    # e.g., [[-1.427..., -0.893..., ...], [...]]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3600613"}, {"introduction": "模拟退火（Simulated Annealing, SA）是解决全局优化问题的一种经典且强大的随机方法，其灵感来源于冶金学中的退火过程。该算法成功的关键在于其“冷却方案”$T_k$，它控制着接受“上坡”移动（即目标函数值增大的移动）的概率，从而使算法能够跳出局部极小值盆地。这个理论练习 [@problem_id:3600630] 要求你推导出一个临界冷却速率，以保证算法在理论上能够持续探索整个搜索空间，从而深入理解在“探索”（exploration）与“利用”（exploitation）之间取得平衡的核心原理。", "problem": "在计算地球物理学中，一个典型的玩具失配（toy misfit）问题展示了由一个能量壁垒分隔的两种可能的模型解释。考虑由双势阱能量建模的标量目标函数\n$$\nf(x)=x^{4}-2x^{2}\n$$，它有两个吸引盆，对应两个相互竞争的地球物理模型。您应用Metropolis模拟退火（一种马尔可夫链蒙特卡洛（MCMC）方法），其温度序列 $\\{T_{k}\\}_{k\\geq 1}$ 递减。在每次迭代 $k$ 中，使用一个对称的局部提议，该提议以一个由常数 $q\\in(0,1]$ 作为下界的概率，提出一个尝试从当前势阱爬升至势垒顶部，然后可能下降到另一个势阱的移动。在温度 $T$ 下，对于一个使目标函数增加 $\\Delta f$ 的提议移动，其Metropolis接受概率为\n$$\n\\alpha(\\Delta f, T)=\\exp\\!\\left(-\\frac{\\Delta f}{T}\\right)\n$$。\n\n1) 计算 $f(x)$ 的势垒高度 $B$，其定义为鞍点（势垒顶部）的能量与任一极小值点能量之差。\n\n2) 令 $N(n)$ 表示前 $n$ 次迭代中被接受的越过势垒的移动（即至少一次越过势垒顶部的被接受移动）的期望次数。仅使用Metropolis接受概率的定义和级数的标准比较判别法，推导出一个关于降温方案 $\\{T_{k}\\}$ 的条件，在该条件下 $\\lim_{n\\to\\infty}\\mathbb{E}[N(n)]$ 不收敛于一个有限的极限。换句话说，找到一个渐近最小的降温方案，以确保当 $k\\to\\infty$ 时，跨越势垒事件的期望累积次数为非零（即随 $n$ 无界）。\n\n对于上面给出的特定 $f(x)$，将您的最终答案表示为关于 $k$ 的单个闭式渐近表达式 $T_{k}$。无需四舍五入。", "solution": "根据要求，该问题的分析分两部分进行。首先，计算势能函数的势垒高度。其次，推导使期望跨越势垒次数发散的降温方案条件。\n\n第1部分：势垒高度 $B$ 的计算\n\n目标函数为 $f(x) = x^{4} - 2x^{2}$。为了找到极小值点和鞍点，我们必须通过将 $f(x)$ 对 $x$ 的一阶导数设为零来求得临界点。\n\n一阶导数为：\n$$\nf'(x) = \\frac{d}{dx}(x^{4} - 2x^{2}) = 4x^{3} - 4x\n$$\n令 $f'(x) = 0$ 以求得临界点：\n$$\n4x^{3} - 4x = 0\n$$\n$$\n4x(x^{2} - 1) = 0\n$$\n$$\n4x(x - 1)(x + 1) = 0\n$$\n临界点为 $x=0$，$x=1$ 和 $x=-1$。\n\n为对这些临界点进行分类，我们使用二阶导数检验。$f(x)$ 的二阶导数为：\n$$\nf''(x) = \\frac{d}{dx}(4x^{3} - 4x) = 12x^{2} - 4\n$$\n我们在每个临界点处计算 $f''(x)$ 的值：\n- 对于 $x=0$：$f''(0) = 12(0)^{2} - 4 = -4$。由于 $f''(0)  0$，点 $x=0$ 是一个局部极大值点，对应于能量势垒的顶部（在更高维度上是一个鞍点）。\n- 对于 $x=1$：$f''(1) = 12(1)^{2} - 4 = 8$。由于 $f''(1) > 0$，点 $x=1$ 是一个局部极小值点。\n- 对于 $x=-1$：$f''(-1) = 12(-1)^{2} - 4 = 8$。由于 $f''(-1) > 0$，点 $x=-1$ 也是一个局部极小值点。\n\n这两个极小值点对应于两个吸引盆。现在，我们计算在这些点上的能量 $f(x)$：\n- 鞍点处的能量：$f_{\\text{saddle}} = f(0) = 0^{4} - 2(0)^{2} = 0$。\n- 极小值点处的能量：$f_{\\text{min}} = f(1) = 1^{4} - 2(1)^{2} = 1 - 2 = -1$。并且 $f(-1) = (-1)^{4} - 2(-1)^{2} = 1 - 2 = -1$。\n\n势垒高度 $B$ 定义为鞍点处的能量与任一极小值点处的能量之差。\n$$\nB = f_{\\text{saddle}} - f_{\\text{min}} = 0 - (-1) = 1\n$$\n因此，势垒高度为 $B=1$。\n\n第2部分：降温方案的推导\n\n令 $A_{k}$ 为在第 $k$ 次迭代中，一个越过势垒的移动被提议并接受的事件。根据期望的线性性质，前 $n$ 次迭代中此类事件的期望次数为：\n$$\n\\mathbb{E}[N(n)] = \\mathbb{E}\\left[\\sum_{k=1}^{n} I(A_{k})\\right] = \\sum_{k=1}^{n} \\mathbb{E}[I(A_{k})] = \\sum_{k=1}^{n} P(A_{k})\n$$\n其中 $I(A_{k})$ 是事件 $A_{k}$ 的指示函数。\n\n问题要求找到一个关于降温方案 $\\{T_{k}\\}$ 的条件，使得 $\\lim_{n\\to\\infty} \\mathbb{E}[N(n)]$ 不收敛于有限极限。这等价于概率级数发散的条件：\n$$\n\\sum_{k=1}^{\\infty} P(A_{k}) = \\infty\n$$\n事件 $A_k$ 的概率是提议一个越过势垒的移动的概率与接受该移动的概率的乘积。问题陈述，一个尝试爬升势垒的移动被提议的概率 $p_k$ 有一个常数下界 $q > 0$。这样一个从极小值点附近开始的移动，必须使其能量至少增加势垒高度 $B$ 才能到达鞍点。我们考虑最小的这种增加量，即 $\\Delta f = B$。在温度 $T_k$ 下，对此移动的Metropolis接受概率为 $\\alpha(B, T_k) = \\exp(-B/T_k)$。\n\n因此，在第 $k$ 步提议并接受这样一个移动的概率有如下下界：\n$$\nP(A_k) \\ge q \\exp\\left(-\\frac{B}{T_k}\\right)\n$$\n根据级数的比较判别法，如果由下界构成的级数发散，则原级数也发散。由于 $q$ 是一个正常数，我们需要找到使下式成立的关于 $\\{T_k\\}$ 的条件：\n$$\n\\sum_{k=1}^{\\infty} \\exp\\left(-\\frac{B}{T_k}\\right) = \\infty\n$$\n这是模拟退火理论中保证收敛到全局最优解的经典条件，因为它确保了从任何局部极小值点逃逸的期望次数是无限的。\n\n为了找到满足此条件的 $T_k$ 的渐近形式，我们可以使用积分判别法。数列项 $a_k = \\exp(-B/T_k)$ 是正的。由于 $\\{T_k\\}_{k\\ge 1}$ 是一个趋近于 $0$ 的递减序列，所以 $1/T_k$ 是一个递增序列，$-B/T_k$ 是一个递减序列，因此 $a_k = \\exp(-B/T_k)$ 是一个递减序列。积分判别法适用。级数的发散等价于相应积分的发散：\n$$\n\\int_{1}^{\\infty} \\exp\\left(-\\frac{B}{T_x}\\right) dx = \\infty\n$$\n在收敛的临界边界上，降温方案的一个标准选择是形式为 $T_k = \\frac{C}{\\ln k}$，其中 $C$ 是某个常数且 $k$ 较大。我们来分析这种形式。将 $T_x = \\frac{C}{\\ln x}$ 代入项中可得：\n$$\n\\exp\\left(-\\frac{B}{T_x}\\right) = \\exp\\left(-\\frac{B}{C/\\ln x}\\right) = \\exp\\left(-\\frac{B}{C}\\ln x\\right) = \\exp\\left(\\ln x^{-B/C}\\right) = x^{-B/C}\n$$\n该级数变为 $\\sum_{k=k_0}^{\\infty} k^{-B/C}$，这是一个p级数，其指数为 $p = B/C$。p级数发散当且仅当其指数 $p \\le 1$。因此，为了使期望跨越次数为无限，我们需要：\n$$\n\\frac{B}{C} \\le 1 \\implies C \\ge B\n$$\n问题要求找到确保这种发散的“渐近最小方案”。这对应于仍然满足条件的最快降温（最小的 $T_k$）。较小的 $T_k$ 对应于较小的常数 $C$。满足 $C \\ge B$ 的 $C$ 的最小值为 $C = B$。\n\n这给出了边界情况，即临界降温方案：\n$$\nT_k = \\frac{B}{\\ln k}\n$$\n使用在第1部分计算出的值 $B=1$，具体的渐近方案为：\n$$\nT_k = \\frac{1}{\\ln k}\n$$\n该方案代表了形式为 $C/\\ln(k)$ 的最慢降温速率，它不能保证从任何局部极小值有无限次期望跨越。任何比这更慢的降温方案（即，渐近地有 $T_k' \\ge T_k$，意味着 $C' \\ge B$）也将满足该条件。问题要求的是确保非零（无界）计数的最小方案，即这个边界情况。", "answer": "$$\n\\boxed{\\frac{1}{\\ln(k)}}\n$$", "id": "3600630"}, {"introduction": "从模拟退火的单一温度参数，我们现在转向一种更先进、适应性更强的策略。协方差矩阵自适应演化策略（Covariance Matrix Adaptation Evolution Strategy, CMA-ES）是一种前沿的全局优化器，它能够学习并调整其搜索分布（由协方差矩阵 $\\mathbf{C}_k$ 定义）以适应目标函数景观的局部拓扑结构。本练习 [@problem_id:3600595] 将引导你通过解析分析，剖析其核心的协方差更新机制。你将看到，在一个简化的主导搜索方向场景下，该算法如何沿着有希望的方向拉伸其搜索椭球，从而高效地在复杂、病态或旋转的地球物理失配函数中导航。", "problem": "在被视为失配泛函的黑箱最小化问题的全局地震波形反演中，一个常用的全局随机优化器是协方差矩阵自适应演化策略 (CMA-ES)。考虑一个简化的设定，其中在第 $k$ 次迭代时的搜索分布是一个高斯分布，其均值为 $\\mathbf{m}_{k} \\in \\mathbb{R}^{2}$，协方差为 $\\mathbf{C}_{k} \\in \\mathbb{R}^{2 \\times 2}$。协方差自适应结合了基于演化路径 $\\mathbf{p}_{c,k}$ 的秩一更新和基于所选步长 $\\{\\mathbf{y}_{i,k}\\}_{i=1}^{\\mu}$ 的加权样本协方差的秩$\\mu$更新，其中正权重 $\\{w_{i}\\}_{i=1}^{\\mu}$ 的和为1。从协方差作为外积的期望和指数移动平均作为凸组合的基本定义出发，推导当存在单一主导搜索方向时，秩一更新和秩$\\mu$更新的相互作用如何沿着其特征基修改 $\\mathbf{C}_{k}$。\n\n假设以下科学上真实且内部一致的情景，该情景捕捉了由数据失配景观引起的主导灵敏度方向：\n- 当前协方差 $\\mathbf{C}_{k}$ 具有标准正交特征向量 $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ 和特征值 $\\lambda_{1}$ 和 $\\lambda_{2}$，因此 $\\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$。\n- 在第 $k$ 次迭代时，所选步长的加权二阶矩集中在单位向量 $\\mathbf{u} = \\mathbf{e}_{1}$ 方向上，即 $\\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top}$，其中 $\\alpha > 0$。\n- 演化路径与 $\\mathbf{u}$ 对齐，即 $\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u}$。\n\n设秩一和秩$\\mu$分量的学习率分别为 $c_{1}$ 和 $c_{\\mu}$，其中 $c_{1} > 0$, $c_{\\mu} > 0$, 且 $c_{1} + c_{\\mu}  1$。通过从凸组合和协方差作为外积平均的定义出发进行推导，在上述主导方向假设下，用 $\\lambda_{1}$、$\\lambda_{2}$、$c_{1}$、$c_{\\mu}$、$\\|\\mathbf{p}_{c,k}\\|^{2}$ 和 $\\alpha$ 表示下一个协方差 $\\mathbf{C}_{k+1}$ 的更新后的特征值 $\\lambda_{1}^{\\prime}$ 和 $\\lambda_{2}^{\\prime}$。\n\n然后，对于具体参数值 $c_{1} = \\tfrac{1}{5}$、$c_{\\mu} = \\tfrac{3}{10}$、$\\lambda_{1} = 2$、$\\lambda_{2} = 1$、$\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$ 和 $\\alpha = \\tfrac{5}{2}$，计算各向异性比 $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$ 的精确值。以单个精确值的形式提供最终结果，不进行四舍五入，不带单位。", "solution": "用户要求我解决一个关于协方差矩阵自适应演化策略 (CMA-ES) 的问题。\n\n### 问题验证\n\n我将首先验证问题陈述。\n\n**步骤1：提取已知条件**\n- 第 $k$ 次迭代时的搜索分布是均值为 $\\mathbf{m}_{k} \\in \\mathbb{R}^{2}$、协方差为 $\\mathbf{C}_{k} \\in \\mathbb{R}^{2 \\times 2}$ 的高斯分布。\n- 协方差自适应结合了秩一更新（来自演化路径 $\\mathbf{p}_{c,k}$）和秩$\\mu$更新（来自所选步长 $\\{\\mathbf{y}_{i,k}\\}_{i=1}^{\\mu}$）。\n- 权重 $\\{w_{i}\\}_{i=1}^{\\mu}$ 是正数且和为1。\n- 当前协方差矩阵具有谱分解 $\\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$，其中 $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ 是标准正交特征向量，$\\lambda_{1}, \\lambda_{2}$ 是特征值。\n- 所选步长的加权二阶矩由 $\\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top}$ 给出，其中 $\\alpha > 0$。\n- 假设存在一个主导搜索方向：$\\mathbf{u} = \\mathbf{e}_{1}$。\n- 演化路径与此主导方向对齐：$\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u}$。\n- 秩一和秩$\\mu$更新的学习率分别为 $c_{1}$ 和 $c_{\\mu}$，其中 $c_{1} > 0$, $c_{\\mu} > 0$, 且 $c_{1} + c_{\\mu}  1$。\n- 给定了具体的参数值：$c_{1} = \\tfrac{1}{5}$、$c_{\\mu} = \\tfrac{3}{10}$、$\\lambda_{1} = 2$、$\\lambda_{2} = 1$、$\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$ 和 $\\alpha = \\tfrac{5}{2}$。\n- 任务是推导新协方差矩阵 $\\mathbf{C}_{k+1}$ 的更新后的特征值 $\\lambda_{1}^{\\prime}$、$\\lambda_{2}^{\\prime}$，然后为给定的参数计算各向异性比 $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学依据**：该问题描述了CMA-ES算法的核心机制，这是一种先进的随机优化方法。其更新规则及其组成部分（秩一和秩$\\mu$更新）是该算法的标准部分。假设存在主导搜索方向是一种常用的简化，用于解析地研究算法的行为，特别是其使协方差矩阵适应目标函数局部拓扑结构的能力。这是一个合理且相关的科学设定。\n- **适定性**：问题陈述清晰。所提供的信息充分且一致，可以导出一个关于更新后的特征值及其比率的唯一解。\n- **客观性**：问题以精确、客观的数学语言陈述。\n- **其他缺陷**：该问题没有说明中列出的使其无效的缺陷。它是可形式化的、切题的（因为它明确关于一种全局随机优化方法）、完整的、一致的，并且在其数学背景下是可行的。这是一个非平凡的问题，考验了对CMA-ES更新机制的理解。\n\n**步骤3：结论与行动**\n- 问题是**有效的**。我将进行完整解答。\n\n### 解题推导\n\n该问题要求在特定假设下，推导CMA-ES框架中协方差矩阵 $\\mathbf{C}_{k}$ 的更新后的特征值。从迭代 $k$到 $k+1$ 的协方差矩阵更新规则是CMA-ES的基石。它被表述为一种指数移动平均（一种凸组合形式），并融合了有关搜索步长的信息。\n\n协方差矩阵 $\\mathbf{C}_{k+1}$ 的通用更新方程为：\n$$ \\mathbf{C}_{k+1} = (1 - c_{1} - c_{\\mu}) \\mathbf{C}_{k} + c_{1} \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top} + c_{\\mu} \\sum_{i=1}^{\\mu} w_{i} \\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} $$\n该方程符合问题的描述。项 $(1 - c_{1} - c_{\\mu}) \\mathbf{C}_{k}$ 是“记忆”或“遗忘”部分，保留了前一个协方差矩阵的一部分。项 $c_{1} \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top}$ 是秩一更新，它融合了来自演化路径 $\\mathbf{p}_{c,k}$ 的信息，这是搜索方向的长期记忆。项 $c_{\\mu} \\sum_{i=1}^{\\mu} w_{i} \\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top}$ 是秩$\\mu$更新，它是当前代中最成功的搜索步长的外积的加权平均。该项可以解释为成功步长的基于样本的协方差估计，直接关联到协方差作为外积期望的定义。条件 $c_{1} > 0, c_{\\mu} > 0, c_{1} + c_{\\mu}  1$ 确保了这是一个凸组合，其中如果更新项是半正定的（它们作为外积的构造确实是半正定的），则新矩阵 $\\mathbf{C}_{k+1}$ 保持半正定。\n\n我们现在将给定的假设代入这个通用方程。\n初始协方差矩阵为：\n$$ \\mathbf{C}_{k} = \\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top} $$\n秩$\\mu$更新项由以下公式给出：\n$$ \\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{u}\\mathbf{u}^{\\top} $$\n在主导方向 $\\mathbf{u} = \\mathbf{e}_{1}$ 下，这变成：\n$$ \\sum_{i=1}^{\\mu} w_{i} \\,\\mathbf{y}_{i,k}\\mathbf{y}_{i,k}^{\\top} = \\alpha\\, \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} $$\n演化路径与同一方向对齐：$\\mathbf{p}_{c,k} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{u} = \\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{e}_{1}$。因此，秩一更新项为：\n$$ \\mathbf{p}_{c,k}\\mathbf{p}_{c,k}^{\\top} = (\\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{e}_{1}) (\\|\\mathbf{p}_{c,k}\\|\\, \\mathbf{e}_{1})^{\\top} = \\|\\mathbf{p}_{c,k}\\|^{2} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} $$\n现在，我们将这些表达式代回 $\\mathbf{C}_{k+1}$ 的更新方程：\n$$ \\mathbf{C}_{k+1} = (1 - c_{1} - c_{\\mu}) (\\lambda_{1} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\lambda_{2} \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}) + c_{1} (\\|\\mathbf{p}_{c,k}\\|^{2} \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}) + c_{\\mu} (\\alpha \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}) $$\n我们可以根据标准正交投影算子 $\\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top}$ 和 $\\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top}$ 对各项进行分组：\n$$ \\mathbf{C}_{k+1} = \\left[ (1 - c_{1} - c_{\\mu})\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha \\right] \\mathbf{e}_{1}\\mathbf{e}_{1}^{\\top} + \\left[ (1 - c_{1} - c_{\\mu})\\lambda_{2} \\right] \\mathbf{e}_{2}\\mathbf{e}_{2}^{\\top} $$\n这个表达式是新协方差矩阵 $\\mathbf{C}_{k+1}$ 的谱分解。在这些特定的简化假设下，特征向量 $\\{\\mathbf{e}_{1}, \\mathbf{e}_{2}\\}$ 保持不变。投影算子的系数是新的特征值 $\\lambda_{1}^{\\prime}$ 和 $\\lambda_{2}^{\\prime}$。\n\n因此，更新后的特征值为：\n$$ \\lambda_{1}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha $$\n$$ \\lambda_{2}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{2} $$\n这个结果显示了协方差矩阵如何沿着其特征基被修改。与主导方向 $\\mathbf{e}_{1}$ 相关联的特征值 $\\lambda_{1}^{\\prime}$ 因演化路径和成功步长的贡献而增加。与正交方向 $\\mathbf{e}_{2}$ 相关联的特征值 $\\lambda_{2}^{\\prime}$ 仅被“遗忘因子” $(1 - c_{1} - c_{\\mu})$ 缩小。这种机制使得CMA-ES能够沿着前进的方向拉长搜索分布。\n\n现在，我们代入给定的数值来计算各向异性比 $r = \\dfrac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}}$。\n参数为：$c_{1} = \\tfrac{1}{5}$、$c_{\\mu} = \\tfrac{3}{10}$、$\\lambda_{1} = 2$、$\\lambda_{2} = 1$、$\\|\\mathbf{p}_{c,k}\\|^{2} = \\tfrac{3}{2}$ 和 $\\alpha = \\tfrac{5}{2}$。\n\n首先，我们计算遗忘因子：\n$$ 1 - c_{1} - c_{\\mu} = 1 - \\frac{1}{5} - \\frac{3}{10} = \\frac{10}{10} - \\frac{2}{10} - \\frac{3}{10} = \\frac{5}{10} = \\frac{1}{2} $$\n接下来，我们计算新的特征值 $\\lambda_{1}^{\\prime}$：\n$$ \\lambda_{1}^{\\prime} = \\left(\\frac{1}{2}\\right)\\lambda_{1} + c_{1}\\|\\mathbf{p}_{c,k}\\|^{2} + c_{\\mu}\\alpha $$\n$$ \\lambda_{1}^{\\prime} = \\left(\\frac{1}{2}\\right)(2) + \\left(\\frac{1}{5}\\right)\\left(\\frac{3}{2}\\right) + \\left(\\frac{3}{10}\\right)\\left(\\frac{5}{2}\\right) $$\n$$ \\lambda_{1}^{\\prime} = 1 + \\frac{3}{10} + \\frac{15}{20} = 1 + \\frac{3}{10} + \\frac{3}{4} $$\n为了将这些分数相加，我们找到一个公分母，即 $20$：\n$$ \\lambda_{1}^{\\prime} = \\frac{20}{20} + \\frac{6}{20} + \\frac{15}{20} = \\frac{20+6+15}{20} = \\frac{41}{20} $$\n接下来，我们计算新的特征值 $\\lambda_{2}^{\\prime}$：\n$$ \\lambda_{2}^{\\prime} = (1 - c_{1} - c_{\\mu})\\lambda_{2} = \\left(\\frac{1}{2}\\right)(1) = \\frac{1}{2} $$\n最后，我们计算各向异性比 $r$：\n$$ r = \\frac{\\lambda_{1}^{\\prime}}{\\lambda_{2}^{\\prime}} = \\frac{41/20}{1/2} = \\frac{41}{20} \\times \\frac{2}{1} = \\frac{41}{10} $$\n分析表明，各向异性在单步内显著增加，从 $\\lambda_1/\\lambda_2 = 2/1 = 2$ 增加到 $\\lambda_1'/\\lambda_2' = 41/10 = 4.1$，这展示了搜索分布的快速自适应能力。", "answer": "$$\\boxed{\\frac{41}{10}}$$", "id": "3600595"}]}