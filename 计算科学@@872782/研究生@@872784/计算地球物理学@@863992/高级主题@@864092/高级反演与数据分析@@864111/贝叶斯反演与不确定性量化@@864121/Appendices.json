{"hands_on_practices": [{"introduction": "基于梯度的优化和采样算法是现代大规模反演问题的核心。这些方法的效率和正确性关键取决于目标函数梯度的精确计算。本练习 ([@problem_id:3577517]) 将指导您实现用于梯度计算的伴随状态法并对其进行验证，这是所有计算地球物理学家都应掌握的一项基础技能。", "problem": "给定一个线性的、一维的走时层析成像设置，它在计算地球物理学中用作一个简化的正演模型。未知的地下慢度模型由向量 $m \\in \\mathbb{R}^N$ 表示，其中每个分量参数化一个单位长度的单元格。每个数据点都是一个走时测量值，其模型是某个连续单元格块上的总和。这给出了一个线性正演算子 $A \\in \\mathbb{R}^{M \\times N}$，其中每一行选择 $m$ 的一个连续块，并以权重 $1$ 对其条目求和。观测值的生成方式为 $d = A m_{\\mathrm{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_d^2 I_M)$。\n\n假设高斯先验 $m \\sim \\mathcal{N}(m_0, C_m)$，其中 $m_0 \\in \\mathbb{R}^N$ 且 $C_m = \\sigma_m^2 I_N$。对于高斯似然和先验，负对数后验（不考虑与 $m$ 无关的加性常数）为\n$$\n\\Phi(m) = \\frac{1}{2} \\left\\| \\frac{A m - d}{\\sigma_d} \\right\\|_2^2 + \\frac{1}{2} \\left\\| \\frac{m - m_0}{\\sigma_m} \\right\\|_2^2.\n$$\n您的任务是根据第一性原理，实现并验证此目标函数 $\\Phi(m)$ 的伴随方法和梯度一致性。\n\n使用的基本原理：\n- 在各种高斯假设下，关联后验、似然和先验的贝叶斯定理。\n- 将负对数后验定义为推断的目标函数。\n- 通过弗雷歇导数（Fréchet derivative）定义标量泛函的梯度。\n- 伴随算子 $A^\\ast$ 对于欧几里得内积的定义，其特征为对于所有 $x, y$，均有 $\\langle A x, y \\rangle = \\langle x, A^\\ast y \\rangle$。\n- 方向导数的中心差分近似：\n$$\n\\frac{\\partial \\Phi(m)}{\\partial s} \\approx \\frac{\\Phi(m + h s) - \\Phi(m - h s)}{2 h},\n$$\n其中 $s$ 是方向，$h$ 是小步长。\n\n在完整、可运行的程序中实现以下内容：\n1. 根据指定的随机种子构建正演算子 $A \\in \\mathbb{R}^{M \\times N}$，通过生成 $M$ 条射线；每条射线对应一个连续的索引块。对于每一行 $i$，从 $\\{1, 2, \\dots, \\lfloor N/2 \\rfloor\\}$ 中均匀抽取一个块长度 $\\ell_i$，然后从 $\\{0, 1, \\dots, N - \\ell_i\\}$ 中均匀抽取一个起始索引 $b_i$，如果 $j \\in \\{b_i, b_i+1, \\dots, b_i+\\ell_i-1\\}$，则设置 $A_{i,j} = 1$，否则设置 $A_{i,j} = 0$。\n2. 一个函数，用于在给定 $A$、$d$、$m_0$、$\\sigma_d$ 和 $\\sigma_m$ 的情况下计算 $\\Phi(m)$。\n3. 一个函数，仅使用上述定义和伴随算子特性来计算梯度 $\\nabla \\Phi(m)$。您不能硬编码一个预先推导出的封闭形式梯度；相反，应使用链式法则和伴随概念将残差映射回参数空间。\n4. 一个伴随测试，用于对随机向量 $x \\in \\mathbb{R}^N$ 和 $y \\in \\mathbb{R}^M$ 验证属性 $\\langle A x, y \\rangle = \\langle x, A^\\ast y \\rangle$，使用欧几里得内积 $\\langle u, v \\rangle = u^\\top v$。报告在多次随机抽样中的最大相对差异：\n$$\n\\mathrm{err}_{\\mathrm{adj}} = \\max_{k} \\frac{|\\langle A x_k, y_k \\rangle - \\langle x_k, A^\\ast y_k \\rangle|}{\\max\\{1, |\\langle A x_k, y_k \\rangle|, |\\langle x_k, A^\\ast y_k \\rangle|\\}}.\n$$\n5. 一个梯度一致性测试，用于检查对于一个随机的单位范数方向 $s \\in \\mathbb{R}^N$ 和一组步长 $\\{h_j\\}$，中心差分方向导数是否与内积 $s^\\top \\nabla \\Phi(m)$ 匹配。对于每个 $h_j$，计算\n$$\n\\mathrm{err}_j = \\frac{\\left| \\frac{\\Phi(m + h_j s) - \\Phi(m - h_j s)}{2 h_j} - s^\\top \\nabla \\Phi(m) \\right|}{\\max\\{1, \\left| \\frac{\\Phi(m + h_j s) - \\Phi(m - h_j s)}{2 h_j} \\right|, |s^\\top \\nabla \\Phi(m)|\\}},\n$$\n并报告 $\\min_j \\mathrm{err}_j$ 作为梯度一致性误差。\n\n测试套件和要求的行为：\n实现三个独立的测试用例。对于每个案例，独立构建 $A$，抽取真实基准 $m_{\\mathrm{true}}$ 和噪声以生成 $d$，并选择一个测试点 $m$ 和一个方向 $s$。除非另有说明，否则设置 $m_0 = 0$。使用以下参数集：\n\n- 案例 1（一般情况）：\n  - $N = 64$, $M = 48$。\n  - 随机种子：$A$：$1$，$m_{\\mathrm{true}}$：$2$，噪声：$3$，梯度方向和测试点：$4$。\n  - 噪声标准差：$\\sigma_d = 0.05$。\n  - 先验标准差：$\\sigma_m = 0.5$。\n  - 中心差分步长：$[10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}]$。\n  - 容差：$\\mathrm{tol}_{\\mathrm{adj}} = 10^{-12}$，$\\mathrm{tol}_{\\mathrm{grad}} = 10^{-6}$。\n\n- 案例 2（更大规模，更强正则化）：\n  - $N = 128$, $M = 96$。\n  - 随机种子：$A$：$10$，$m_{\\mathrm{true}}$：$20$，噪声：$30$，梯度方向和测试点：$40$。\n  - 噪声标准差：$\\sigma_d = 0.10$。\n  - 先验标准差：$\\sigma_m = 0.2$。\n  - 中心差分步长：$[10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}]$。\n  - 容差：$\\mathrm{tol}_{\\mathrm{adj}} = 10^{-11}$，$\\mathrm{tol}_{\\mathrm{grad}} = 5 \\cdot 10^{-6}$。\n\n- 案例 3（数据少、正则化较弱的边缘情况）：\n  - $N = 32$, $M = 8$。\n  - 随机种子：$A$：$100$，$m_{\\mathrm{true}}$：$200$，噪声：$300$，梯度方向和测试点：$400$。\n  - 噪声标准差：$\\sigma_d = 0.20$。\n  - 先验标准差：$\\sigma_m = 1.0$。\n  - 中心差分步长：$[10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}]$。\n  - 容差：$\\mathrm{tol}_{\\mathrm{adj}} = 10^{-12}$，$\\mathrm{tol}_{\\mathrm{grad}} = 10^{-5}$。\n\n对于每个案例：\n- 使用 $K = 5$ 次对 $x$ 和 $y$ 的独立抽样（例如，标准正态分布条目）进行伴随测试。\n- 对于梯度测试，用标准正态分布条目独立抽取 $m$ 和 $s$，然后设置 $s \\leftarrow s / \\|s\\|_2$。\n\n每个案例的验收标准：\n- 如果 $\\mathrm{err}_{\\mathrm{adj}} \\le \\mathrm{tol}_{\\mathrm{adj}}$，伴随测试通过。\n- 如果 $\\min_j \\mathrm{err}_j \\le \\mathrm{tol}_{\\mathrm{grad}}$，梯度测试通过。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来。该列表必须按以下顺序包含六个布尔值：$[\\text{adj\\_pass\\_1}, \\text{grad\\_pass\\_1}, \\text{adj\\_pass\\_2}, \\text{grad\\_pass\\_2}, \\text{adj\\_pass\\_3}, \\text{grad\\_pass\\_3}]$。", "solution": "问题陈述已经过仔细审查，被认为是有效的。它在科学上基于贝叶斯反演问题和数值线性代数的原理，定义和参数完整一致，问题是适定的，并且可以形式化为一个具体的计算任务。该问题要求实现和验证基本的数值方法——特别是用于梯度计算的伴随方法及其通过泰勒检验的验证——这些方法是计算地球物理学领域的核心。\n\n解决方案首先从第一性原理推导出必要的数学表达式，然后在一个结构化程序中实现它们，以执行指定的测试用例。\n\n**1. 理论公式**\n\n任务的核心是计算负对数后验目标函数 $\\Phi(m)$ 的梯度，并验证其正确性。目标函数由以下公式给出：\n$$\n\\Phi(m) = \\frac{1}{2} \\left\\| \\frac{A m - d}{\\sigma_d} \\right\\|_2^2 + \\frac{1}{2} \\left\\| \\frac{m - m_0}{\\sigma_m} \\right\\|_2^2\n$$\n该函数可以分解为两部分：似然项 $\\Phi_{\\text{like}}(m)$ 和先验项 $\\Phi_{\\text{prior}}(m)$。\n\n$\\Phi_{\\text{like}}(m) = \\frac{1}{2\\sigma_d^2} \\| A m - d \\|_2^2$\n$\\Phi_{\\text{prior}}(m) = \\frac{1}{2\\sigma_m^2} \\| m - m_0 \\|_2^2$\n\n梯度 $\\nabla \\Phi(m)$ 是这两项梯度之和：$\\nabla \\Phi(m) = \\nabla \\Phi_{\\text{like}}(m) + \\nabla \\Phi_{\\text{prior}}(m)$。\n\n**先验项的梯度**\n\n先验项是关于 $m$ 的一个简单二次函数。其梯度可直接求得：\n$\\nabla \\Phi_{\\text{prior}}(m) = \\nabla_m \\left( \\frac{1}{2\\sigma_m^2} (m - m_0)^\\top(m - m_0) \\right) = \\frac{1}{2\\sigma_m^2} \\cdot 2(m-m_0) = \\frac{1}{\\sigma_m^2} (m - m_0)$。\n\n**通过伴随方法计算似然项的梯度**\n\n似然项的梯度是使用链式法则和伴随算子的定义推导出来的。$\\Phi_{\\text{like}}(m)$ 在方向 $s \\in \\mathbb{R}^N$ 上的弗雷歇导数由方向导数给出：\n$$\nD\\Phi_{\\text{like}}(m)[s] = \\lim_{h \\to 0} \\frac{\\Phi_{\\text{like}}(m + hs) - \\Phi_{\\text{like}}(m)}{h}\n$$\n根据定义，这个方向导数等于内积 $\\langle \\nabla \\Phi_{\\text{like}}(m), s \\rangle$。令数据残差为 $r(m) = Am - d$。则 $\\Phi_{\\text{like}}(m) = \\frac{1}{2\\sigma_d^2} \\langle r(m), r(m) \\rangle$。\n使用链式法则，$\\Phi_{\\text{like}}$ 的导数由范数平方函数的导数和残差函数 $r(m)$ 的导数复合而成。$r(m)$ 关于 $m$ 在方向 $s$ 上的导数是 $A s$。\n$$\nD\\Phi_{\\text{like}}(m)[s] = \\frac{1}{\\sigma_d^2} \\langle r(m), A s \\rangle = \\frac{1}{\\sigma_d^2} \\langle Am - d, A s \\rangle\n$$\n“伴随方法”包括利用伴随算子 $A^\\ast$ 的性质，将算子 $A$ 从内积的第二个参数移动到第一个参数。对于标准欧几里得内积，伴随算子 $A^\\ast$ 是转置 $A^\\top$。\n$$\n\\langle Am - d, A s \\rangle = \\langle A^\\ast (Am - d), s \\rangle = \\langle A^\\top (Am - d), s \\rangle\n$$\n因此，我们有：\n$$\n\\langle \\nabla \\Phi_{\\text{like}}(m), s \\rangle = D\\Phi_{\\text{like}}(m)[s] = \\frac{1}{\\sigma_d^2} \\langle A^\\top (Am - d), s \\rangle\n$$\n由于此等式必须对所有方向 $s$ 成立，我们可以确定梯度为：\n$$\n\\nabla \\Phi_{\\text{like}}(m) = \\frac{1}{\\sigma_d^2} A^\\top (Am - d)\n$$\n此推导阐明了原理：为了找到梯度在参数空间 ($m$) 上的作用，我们首先在数据空间中计算残差 ($Am - d$)，然后使用伴随算子 ($A^\\top$) 将其映射回参数空间。\n\n**总梯度**\n\n结合这两个分量，完整的梯度为：\n$$\n\\nabla \\Phi(m) = \\frac{1}{\\sigma_d^2} A^\\top (Am - d) + \\frac{1}{\\sigma_m^2} (m - m_0)\n$$\n实现将使用函数 `apply_A(m)` 进行正向操作 $Am$，并使用 `apply_A_adjoint(y)` 进行伴随操作 $A^\\top y$，遵循“伴随状态”方法学。\n\n**2. 数值验证协议**\n\n**伴随测试**\n\n此测试验证所实现的伴随算子 `apply_A_adjoint` 确实是正演算子 `apply_A` 的伴随。对于实数矩阵和欧几里得内积（$\\langle u, v \\rangle = u^\\top v$），伴随是转置，因此我们必须验证 $\\langle Ax, y \\rangle = \\langle x, A^\\top y \\rangle$。该测试对 $K=5$ 对随机向量 $x_k \\in \\mathbb{R}^N$ 和 $y_k \\in \\mathbb{R}^M$ 进行。如果最大相对差异低于给定的容差 $\\mathrm{tol}_{\\mathrm{adj}}$，则测试通过。\n$$\n\\mathrm{err}_{\\mathrm{adj}} = \\max_{k} \\frac{|\\langle Ax_k, y_k \\rangle - \\langle x_k, A^\\top y_k \\rangle|}{\\max\\{1, |\\langle Ax_k, y_k \\rangle|, |\\langle x_k, A^\\top y_k \\rangle|\\}} \\le \\mathrm{tol}_{\\mathrm{adj}}\n$$\n接近机器精度的 $\\mathrm{err}_{\\mathrm{adj}}$ 值证实了伴随实现的正确性。\n\n**梯度一致性测试（泰勒检验）**\n\n此测试验证所实现的梯度函数 `evaluate_grad_Phi` 是正确的。它将解析方向导数 $g_{\\text{analytic}} = s^\\top \\nabla \\Phi(m)$ 与通过中心有限差分获得的数值近似值 $g_{\\text{fd}}$ 进行比较。\n$$\ng_{\\text{fd}}(h) = \\frac{\\Phi(m + h s) - \\Phi(m - h s)}{2 h}\n$$\n中心差分格式的截断误差为 $O(h^2)$ 阶。对于非常小的 $h$，浮点运算的舍入误差将占主导地位。该测试计算一系列步长 $h_j$ 的相对误差，并报告找到的最小误差。\n$$\n\\mathrm{err}_{\\text{grad}} = \\min_{j} \\frac{| g_{\\text{fd}}(h_j) - g_{\\text{analytic}} |}{\\max\\{1, |g_{\\text{fd}}(h_j)|, |g_{\\text{analytic}}|\\}}\n$$\n如果这个最小误差低于指定的容差 $\\mathrm{tol}_{\\mathrm{grad}}$，则测试通过。一个很小的误差证实了推导出的梯度表达式已正确实现。\n\n下面的实现遵循此协议，用于三个指定的测试用例。", "answer": "```python\nimport numpy as np\n\ndef construct_A(N, M, seed):\n    \"\"\"\n    Constructs the forward operator matrix A.\n    \n    Args:\n        N (int): Number of model parameters (columns).\n        M (int): Number of data points (rows).\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        np.ndarray: The MxN forward operator matrix A.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    A = np.zeros((M, N))\n    max_len = N // 2\n    for i in range(M):\n        length = rng.integers(1, max_len + 1)\n        start = rng.integers(0, N - length + 1)\n        A[i, start : start + length] = 1.0\n    return A\n\ndef apply_A(A, m):\n    \"\"\"Applies the forward operator A to a model vector m.\"\"\"\n    return A @ m\n\ndef apply_A_adjoint(A, y):\n    \"\"\"Applies the adjoint of the operator A to a data vector y.\"\"\"\n    return A.T @ y\n\ndef test_adjoint(A, K, seed):\n    \"\"\"\n    Performs the adjoint test for the operator A and its adjoint.\n\n    Args:\n        A (np.ndarray): The MxN forward operator matrix.\n        K (int): Number of random trials.\n        seed (int): Random seed for generating vectors.\n\n    Returns:\n        float: The maximum relative discrepancy found.\n    \"\"\"\n    M, N = A.shape\n    rng = np.random.default_rng(seed)\n    max_rel_err = 0.0\n    \n    for _ in range(K):\n        x = rng.standard_normal(size=N)\n        y = rng.standard_normal(size=M)\n\n        lhs = np.dot(y, apply_A(A, x))\n        rhs = np.dot(x, apply_A_adjoint(A, y))\n\n        diff = np.abs(lhs - rhs)\n        denom = max(1.0, np.abs(lhs), np.abs(rhs))\n        rel_err = diff / denom\n        \n        if rel_err > max_rel_err:\n            max_rel_err = rel_err\n            \n    return max_rel_err\n\ndef evaluate_Phi(m, A, d, m0, sigma_d, sigma_m):\n    \"\"\"\n    Evaluates the negative log-posterior objective function Phi(m).\n    \"\"\"\n    residual_data = apply_A(A, m) - d\n    term_like = 0.5 * np.sum((residual_data / sigma_d)**2)\n    \n    residual_prior = m - m0\n    term_prior = 0.5 * np.sum((residual_prior / sigma_m)**2)\n    \n    return term_like + term_prior\n\ndef evaluate_grad_Phi(m, A, d, m0, sigma_d, sigma_m):\n    \"\"\"\n    Evaluates the gradient of Phi(m) using the adjoint method.\n    \"\"\"\n    # Gradient of likelihood term\n    residual_data = apply_A(A, m) - d\n    grad_like = apply_A_adjoint(A, residual_data) / (sigma_d**2)\n    \n    # Gradient of prior term\n    residual_prior = m - m0\n    grad_prior = residual_prior / (sigma_m**2)\n    \n    return grad_like + grad_prior\n\ndef test_gradient(A, d, m0, sigma_d, sigma_m, m_test, s, h_values):\n    \"\"\"\n    Performs the gradient consistency test (Taylor test).\n\n    Returns:\n        float: The minimum relative error found.\n    \"\"\"\n    # Analytical directional derivative\n    grad_val = evaluate_grad_Phi(m_test, A, d, m0, sigma_d, sigma_m)\n    grad_proj = np.dot(s, grad_val)\n\n    min_rel_err = np.inf\n    \n    for h in h_values:\n        phi_plus = evaluate_Phi(m_test + h * s, A, d, m0, sigma_d, sigma_m)\n        phi_minus = evaluate_Phi(m_test - h * s, A, d, m0, sigma_d, sigma_m)\n        \n        fd_approx = (phi_plus - phi_minus) / (2 * h)\n        \n        diff = np.abs(fd_approx - grad_proj)\n        denom = max(1.0, np.abs(fd_approx), np.abs(grad_proj))\n        rel_err = diff / denom\n        \n        if rel_err  min_rel_err:\n            min_rel_err = rel_err\n            \n    return min_rel_err\n\ndef run_case(params):\n    \"\"\"\n    Runs a single test case with given parameters.\n    \"\"\"\n    N, M = params[\"N\"], params[\"M\"]\n    seeds = params[\"seeds\"]\n    sigma_d, sigma_m = params[\"sigma_d\"], params[\"sigma_m\"]\n    h_values = params[\"h_values\"]\n    tol_adj, tol_grad = params[\"tol_adj\"], params[\"tol_grad\"]\n\n    # 1. Construct A\n    A = construct_A(N, M, seeds[\"A\"])\n\n    # 2. Adjoint test\n    # The seed for adjoint test vectors is part of the grad/test point seed dict key\n    err_adj = test_adjoint(A, K=5, seed=seeds[\"grad\"])\n    adj_pass = err_adj = tol_adj\n\n    # 3. Generate data\n    rng_m_true = np.random.default_rng(seeds[\"m_true\"])\n    m_true = rng_m_true.standard_normal(size=N)\n    \n    rng_noise = np.random.default_rng(seeds[\"noise\"])\n    noise = rng_noise.standard_normal(size=M) * sigma_d\n\n    d = apply_A(A, m_true) + noise\n    m0 = np.zeros(N)\n\n    # 4. Gradient test\n    rng_grad = np.random.default_rng(seeds[\"grad\"])\n    m_test = rng_grad.standard_normal(size=N)\n    s = rng_grad.standard_normal(size=N)\n    s /= np.linalg.norm(s)\n\n    err_grad = test_gradient(A, d, m0, sigma_d, sigma_m, m_test, s, h_values)\n    grad_pass = err_grad = tol_grad\n    \n    return adj_pass, grad_pass\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 64, \"M\": 48,\n            \"seeds\": {\"A\": 1, \"m_true\": 2, \"noise\": 3, \"grad\": 4},\n            \"sigma_d\": 0.05, \"sigma_m\": 0.5,\n            \"h_values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n            \"tol_adj\": 1e-12, \"tol_grad\": 1e-6\n        },\n        {\n            \"N\": 128, \"M\": 96,\n            \"seeds\": {\"A\": 10, \"m_true\": 20, \"noise\": 30, \"grad\": 40},\n            \"sigma_d\": 0.10, \"sigma_m\": 0.2,\n            \"h_values\": [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n            \"tol_adj\": 1e-11, \"tol_grad\": 5e-6\n        },\n        {\n            \"N\": 32, \"M\": 8,\n            \"seeds\": {\"A\": 100, \"m_true\": 200, \"noise\": 300, \"grad\": 400},\n            \"sigma_d\": 0.20, \"sigma_m\": 1.0,\n            \"h_values\": [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n            \"tol_adj\": 1e-12, \"tol_grad\": 1e-5\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        adj_pass, grad_pass = run_case(case_params)\n        results.extend([adj_pass, grad_pass])\n    \n    # Format the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3577517"}, {"introduction": "在实际应用中，由于求解器误差或各种近似，反演算法中使用的梯度往往并非完全精确。本练习 ([@problem_id:3577488]) 旨在探究此类不精确性在哈密顿蒙特卡洛 (Hamiltonian Monte Carlo, HMC) 框架下所带来的后果。通过向梯度中引入一个受控的偏差，您将深入探索Metropolis-Hastings校正步骤在确保采样器仍能收敛至正确后验分布方面所起的关键作用。", "problem": "考虑一个计算地球物理学中的贝叶斯反演问题，其未知模型向量为 $m \\in \\mathbb{R}^2$，通过一个线性正演算子进行观测。设观测数据向量为 $d \\in \\mathbb{R}^2$，正演算子为矩阵 $G \\in \\mathbb{R}^{2 \\times 2}$，数据噪声协方差为对称正定矩阵 $C_n \\in \\mathbb{R}^{2 \\times 2}$，$m$ 上的高斯先验均值为 $m_0 \\in \\mathbb{R}^2$，协方差为 $C_m \\in \\mathbb{R}^{2 \\times 2}$。后验密度 $p(m \\mid d)$ 正比于 $\\exp(-\\Phi(m))$，其中势函数由数据失配和先验惩罚之和定义，两者均源于高斯假设的二次型形式。将哈密顿蒙特卡罗（HMC）方法记为一种马尔可夫链蒙特卡罗方法，它通过数值积分涉及梯度 $\\nabla \\Phi(m)$ 的哈密顿动力学来提议新状态，并结合使用精确势函数 $\\Phi(m)$ 的 Metropolis 接受准则。\n\n在实际的计算地球物理学工作流程中，梯度 $\\nabla \\Phi(m)$ 通常通过伴随求解器计算。为评估伴随求解器误差对后验准确性和马尔可夫链平稳性的影响，在 HMC 积分器使用的梯度中引入一个受控偏差 $\\eta \\in \\mathbb{R}^2$，使得积分器使用带偏梯度 $\\tilde{\\nabla}\\Phi(m) = \\nabla\\Phi(m) + \\eta$。此外，考虑一个随机偏差模型，其中每次评估 $\\tilde{\\nabla}\\Phi(m)$ 时都包含一个可加的高斯扰动 $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$，该扰动在各次梯度调用之间独立，因此 $\\tilde{\\nabla}\\Phi(m) = \\nabla\\Phi(m) + \\xi$。在所有情况下，Metropolis 接受步骤必须使用精确势函数 $\\Phi(m)$，而不是带偏梯度。\n\n仅从上述基本定义出发，推导线性高斯情况下的后验密度以及相应的势函数 $\\Phi(m)$ 及其精确梯度 $\\nabla \\Phi(m)$。利用这些，实现采用标准蛙跳积分的 HMC，其中蛙跳更新步骤使用为各测试案例指定的 $\\tilde{\\nabla}\\Phi(m)$，而 Metropolis 接受概率使用由 $\\Phi(m)$ 和单位质量矩阵下的动能定义的精确哈密顿量。量化梯度偏差对以下方面的影响：\n- 链的平稳性，通过一种分布稳定性诊断实现，该诊断比较预烧期后样本的前半部分和后半部分，对 $m$ 的每个坐标使用单变量双样本分布距离，并报告所有坐标中的最大值。\n- 采样后验 $p(m \\mid d)$ 的准确性，通过将 HMC 样本的经验均值和协方差与解析推导的后验均值和协方差进行比较。\n\n对于数值实验，使用以下固定的问题设置：\n- $G = \\begin{bmatrix} 1.0  0.5 \\\\ 0.3  1.2 \\end{bmatrix}$，\n- $d = \\begin{bmatrix} 1.2 \\\\ -0.7 \\end{bmatrix}$，\n- $C_n = \\mathrm{diag}(0.1^2, 0.2^2)$，\n- $m_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$，\n- $C_m = \\mathrm{diag}(0.5^2, 0.5^2)$。\n\n使用 HMC，参数为：单位质量矩阵，蛙跳步长 $\\epsilon = 0.2$，蛙跳步数 $L = 20$，总样本数 $N = 2000$，预烧期长度 $N_{\\mathrm{burn}} = 500$。在先验均值 $m_0$ 处初始化链。为保证可复现性，使用固定的随机种子。\n\n设计一个包含五个案例的测试套件，以探究确定性梯度偏差和随机梯度偏差：\n1. 无偏积分器：$\\eta = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\sigma = 0$。\n2. 确定性小偏差：$\\eta = \\begin{bmatrix} 0.05 \\\\ -0.05 \\end{bmatrix}$，$\\sigma = 0$。\n3. 确定性大偏差：$\\eta = \\begin{bmatrix} 0.2 \\\\ -0.2 \\end{bmatrix}$，$\\sigma = 0$。\n4. 随机小噪声：$\\eta = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\sigma = 0.05$。\n5. 随机大噪声：$\\eta = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$\\sigma = 0.2$。\n\n对于每个测试案例，计算：\n- 接受率，一个在 $[0,1]$ 区间内的浮点数，\n- $m$ 的经验均值相对于精确后验均值的误差的欧几里得范数，\n- $m$ 的经验协方差相对于精确后验协方差的误差的弗罗贝尼乌斯范数，\n- 预烧期后样本的前半部分和后半部分之间，在两个坐标上计算的双样本 Kolmogorov–Smirnov 距离的最大值，\n- 一个布尔平稳性标志，指示最大 Kolmogorov–Smirnov 距离是否低于阈值 $0.08$。\n\n您的程序应生成单行输出，其中包含所有测试案例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个测试案例的结果本身是按上述顺序排列的列表。例如，输出格式必须完全如下：\n[[acc1,mean_err1,cov_err1,ks1,flag1],[acc2,mean_err2,cov_err2,ks2,flag2],...]\n此问题不涉及任何物理单位或角度单位，所有数值答案必须按规定表示为浮点数或布尔值。", "solution": "我们从线性高斯模型的基本贝叶斯反演设置开始。正演算子将模型向量 $m \\in \\mathbb{R}^2$ 映射到预测数据 $Gm \\in \\mathbb{R}^2$。观测数据 $d \\in \\mathbb{R}^2$ 被建模为 $d = Gm + \\varepsilon$，其中数据噪声 $\\varepsilon$ 是均值为零、协方差为 $C_n$ 的高斯噪声，即 $\\varepsilon \\sim \\mathcal{N}(0, C_n)$。$m$ 的先验是均值为 $m_0$、协方差为 $C_m$ 的高斯分布，即 $m \\sim \\mathcal{N}(m_0, C_m)$。\n\n根据贝叶斯法则，后验密度与似然乘以先验成正比，$p(m \\mid d) \\propto p(d \\mid m)\\, p(m)$。在高斯假设下，似然为 $p(d \\mid m) \\propto \\exp\\left(-\\tfrac{1}{2} (d - Gm)^\\top C_n^{-1} (d - Gm)\\right)$，先验为 $p(m) \\propto \\exp\\left(-\\tfrac{1}{2} (m - m_0)^\\top C_m^{-1} (m - m_0)\\right)$。因此，后验密度为\n$$\np(m \\mid d) \\propto \\exp\\left(-\\Phi(m)\\right),\n$$\n其中势函数（负对数后验，相差一个加性常数）为\n$$\n\\Phi(m) = \\tfrac{1}{2} (d - Gm)^\\top C_n^{-1} (d - Gm) + \\tfrac{1}{2} (m - m_0)^\\top C_m^{-1} (m - m_0).\n$$\n势函数的梯度通过对二次型关于 $m$ 求导得到。使用恒等式 $\\nabla_m \\tfrac{1}{2} (Ax - b)^\\top W (Ax - b) = A^\\top W (Ax - b)$（对于合适的矩阵 $A$ 和 $W$），我们有\n$$\n\\nabla \\Phi(m) = -G^\\top C_n^{-1} (d - Gm) + C_m^{-1} (m - m_0).\n$$\n整理线性项，可以表示为\n$$\n\\nabla \\Phi(m) = \\left(G^\\top C_n^{-1} G + C_m^{-1}\\right)m - \\left(G^\\top C_n^{-1} d + C_m^{-1} m_0\\right).\n$$\n由于似然和先验都是高斯分布，且正演算子是线性的，后验也服从高斯分布，$p(m \\mid d) = \\mathcal{N}(\\mu_{\\text{post}}, C_{\\text{post}})$，其中后验协方差和均值可以通过在指数上配方法得到：\n$$\nC_{\\text{post}} = \\left(G^\\top C_n^{-1} G + C_m^{-1}\\right)^{-1},\n\\quad\n\\mu_{\\text{post}} = C_{\\text{post}} \\left(G^\\top C_n^{-1} d + C_m^{-1} m_0\\right).\n$$\n\n我们现在概述使用蛙跳积分的哈密顿蒙特卡罗（HMC）方法。引入一个辅助动量 $p \\in \\mathbb{R}^2$，其密度为 $\\mathcal{N}(0, M)$，其中 $M$ 是质量矩阵。我们取 $M = I$（单位矩阵）。哈密顿量为\n$$\nH(m, p) = \\Phi(m) + \\tfrac{1}{2} p^\\top M^{-1} p = \\Phi(m) + \\tfrac{1}{2} p^\\top p.\n$$\n精确的连续时间动力学满足 $\\dot{m} = \\partial H / \\partial p = p$ 和 $\\dot{p} = - \\partial H / \\partial m = - \\nabla \\Phi(m)$。HMC 通过蛙跳积分器使用时间步长 $\\epsilon$ 和 $L$ 步来近似模拟这些动力学，从一个初始状态 $(m, p)$ 生成一个提议状态 $(m^\\ast, p^\\ast)$，然后以概率\n$$\n\\alpha = \\min\\left\\{1, \\exp\\left( -H(m^\\ast, p^\\ast) + H(m, p) \\right)\\right\\}\n$$\n接受或拒绝这个提议。当积分器是保体积和可逆的时，这个接受步骤能恢复关于目标密度的细致平衡。\n\n在我们的研究中，蛙跳更新不使用精确梯度 $\\nabla \\Phi(m)$，而是使用一个带偏梯度 $\\tilde{\\nabla}\\Phi(m) = \\nabla \\Phi(m) + \\eta$（其中 $\\eta$ 是一个确定性偏差向量），或一个随机带偏梯度 $\\tilde{\\nabla}\\Phi(m) = \\nabla \\Phi(m) + \\xi$（其中 $\\xi \\sim \\mathcal{N}(0, \\sigma^2 I)$ 是在每次梯度评估时添加的独立高斯噪声）。接受概率使用精确势函数 $\\Phi(m)$。\n\n使用带偏梯度的蛙跳方案如下。给定当前 $m$ 和一个新采样的 $p \\sim \\mathcal{N}(0, I)$，执行：\n1. $p \\leftarrow p - \\tfrac{\\epsilon}{2} \\tilde{\\nabla}\\Phi(m)$，\n2. 对于 $l = 1, \\ldots, L$：\n   - $m \\leftarrow m + \\epsilon p$，\n   - 如果 $l  L$：$p \\leftarrow p - \\epsilon \\tilde{\\nabla}\\Phi(m)$，\n   - 否则：$p \\leftarrow p - \\tfrac{\\epsilon}{2} \\tilde{\\nabla}\\Phi(m)$。\n3. 将 $p$ 取反以强制对称性：$p \\leftarrow -p$。\n使用精确的 $\\Phi(m)$ 计算 $H(m, p)$ 和 $H(m^\\ast, p^\\ast)$，并以此计算接受概率。\n\n对于给定的 $G, d, C_n, m_0, C_m$ 的固定问题设置，我们通过上述公式计算 $\\mu_{\\text{post}}$ 和 $C_{\\text{post}}$。然后，我们运行 HMC $N = 2000$ 次迭代，预烧期为 $N_{\\mathrm{burn}} = 500$，步长为 $\\epsilon = 0.2$，蛙跳步数为 $L = 20$，从 $m_0$ 开始初始化。我们分析五个测试案例：\n1. 无偏积分器：$\\eta = 0, \\sigma = 0$。\n2. 确定性小偏差：$\\eta = [0.05, -0.05]^\\top, \\sigma = 0$。\n3. 确定性大偏差：$\\eta = [0.2, -0.2]^\\top, \\sigma = 0$。\n4. 随机小噪声：$\\eta = 0, \\sigma = 0.05$。\n5. 随机大噪声：$\\eta = 0, \\sigma = 0.2$。\n\n对于每个案例，我们收集以下诊断指标：\n- 接受率：被接受的提议所占的比例。\n- 后验均值误差：$\\|\\bar{m} - \\mu_{\\text{post}}\\|_2$，其中 $\\bar{m}$ 是预烧期后样本的经验均值。\n- 后验协方差误差：$\\| \\widehat{C} - C_{\\text{post}} \\|_F$，其中 $\\widehat{C}$ 是预烧期后样本的经验协方差，$\\|\\cdot\\|_F$ 表示弗罗贝尼乌斯范数。\n- 平稳性诊断：我们将预烧期后的样本分成两半，对 $m$ 的每个坐标，计算两半样本间的双样本 Kolmogorov–Smirnov 统计量，并报告所有坐标中的最大值。我们还提供一个布尔平稳性标志，指示该最大值是否低于阈值 $0.08$。\n\n这些诊断指标的理由如下。如果积分器使用精确梯度并应用了接受步骤，HMC 应该能高效地从精确后验分布中采样；在我们的二次型案例中，蛙跳积分非常准确，接受率预计接近于 1，均值和协方差误差很小，且链在前后两半表现出平稳性。当在积分器使用的梯度中注入确定性偏差 $\\eta$ 时，提议动力学会偏离真实的哈密顿流。尽管在固定的确定性偏差下蛙跳积分仍是保体积和可逆的（因此平稳性通常可以通过 Metropolis 步骤恢复），但引起的能量误差会降低接受率并损害采样效率，可能放大有限长度链的经验矩的偏差。在随机梯度噪声下，积分器评估在各步之间随机变化，违反了严格的可逆性。Metropolis 校正仍然适用，但可能面临显著的能量误差，导致接受率降低，并可能引起前后两半样本的分布差异，我们通过 Kolmogorov–Smirnov 统计量来量化这种差异。\n\n所需的程序实现了 $\\Phi(m)$ 和 $\\nabla \\Phi(m)$ 的推导，计算了精确的后验参数 $(\\mu_{\\text{post}}, C_{\\text{post}})$，运行了带偏梯度的 HMC，计算了每个测试案例的诊断指标，并按规定的单行格式输出了所有结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ks_2samp\n\ndef posterior_params(G, Cn, m0, Cm, d):\n    Cn_inv = np.linalg.inv(Cn)\n    Cm_inv = np.linalg.inv(Cm)\n    A = G.T @ Cn_inv @ G + Cm_inv\n    b = G.T @ Cn_inv @ d + Cm_inv @ m0\n    C_post = np.linalg.inv(A)\n    mu_post = C_post @ b\n    return mu_post, C_post, A, b, Cn_inv, Cm_inv\n\ndef phi(m, G, Cn_inv, m0, Cm_inv, d):\n    r = d - G @ m\n    term1 = 0.5 * (r.T @ Cn_inv @ r)\n    dm = m - m0\n    term2 = 0.5 * (dm.T @ Cm_inv @ dm)\n    return float(term1 + term2)\n\ndef grad_phi(m, A, b):\n    # Exact gradient of Phi(m) = 0.5*(d - Gm)^T Cn^{-1} (d - Gm) + 0.5*(m - m0)^T Cm^{-1} (m - m0)\n    # grad = A m - b\n    return A @ m - b\n\ndef leapfrog_with_bias(m, p, epsilon, L, grad_fun, bias_type, eta, sigma, rng):\n    # Perform L leapfrog steps using biased gradients; return proposed (m_prop, p_prop)\n    # bias_type: \"deterministic\" or \"stochastic\" or \"none\"\n    # eta: deterministic bias vector\n    # sigma: std dev for stochastic bias\n    def biased_grad(x):\n        g = grad_fun(x)\n        if bias_type == \"deterministic\":\n            return g + eta\n        elif bias_type == \"stochastic\":\n            noise = rng.normal(loc=0.0, scale=sigma, size=g.shape)\n            return g + noise\n        else:\n            return g\n\n    p = p - 0.5 * epsilon * biased_grad(m)\n    for l in range(1, L + 1):\n        m = m + epsilon * p\n        if l  L:\n            p = p - epsilon * biased_grad(m)\n        else:\n            p = p - 0.5 * epsilon * biased_grad(m)\n    p = -p  # Momentum flip for reversibility\n    return m, p\n\ndef hmc_run(G, Cn, m0, Cm, d, epsilon, L, N, burn_in, bias_type, eta, sigma, seed=12345):\n    rng = np.random.default_rng(seed)\n    mu_post, C_post, A, b, Cn_inv, Cm_inv = posterior_params(G, Cn, m0, Cm, d)\n    def grad_fun(x):\n        return grad_phi(x, A, b)\n    def phi_fun(x):\n        return phi(x, G, Cn_inv, m0, Cm_inv, d)\n\n    dim = G.shape[1]\n    m_current = m0.copy()\n    samples = []\n    accepted = 0\n\n    for i in range(N):\n        p_current = rng.normal(loc=0.0, scale=1.0, size=dim)\n        current_H = phi_fun(m_current) + 0.5 * np.dot(p_current, p_current)\n\n        m_prop, p_prop = leapfrog_with_bias(\n            m_current.copy(),\n            p_current.copy(),\n            epsilon,\n            L,\n            grad_fun,\n            bias_type,\n            eta,\n            sigma,\n            rng\n        )\n\n        proposed_H = phi_fun(m_prop) + 0.5 * np.dot(p_prop, p_prop)\n        # Metropolis acceptance with exact Hamiltonian\n        accept_prob = np.exp(min(0.0, current_H - proposed_H))\n        if rng.uniform()  accept_prob:\n            m_current = m_prop\n            accepted += 1\n\n        samples.append(m_current.copy())\n\n    samples = np.array(samples)\n    post_samples = samples[burn_in:]\n\n    # Diagnostics\n    acc_rate = accepted / float(N)\n\n    emp_mean = np.mean(post_samples, axis=0)\n    mean_err_norm = float(np.linalg.norm(emp_mean - mu_post))\n\n    # Empirical covariance\n    centered = post_samples - emp_mean\n    emp_cov = (centered.T @ centered) / (post_samples.shape[0] - 1)\n    cov_err_fro = float(np.linalg.norm(emp_cov - C_post, ord='fro'))\n\n    # Stationarity diagnostic: KS between first and second halves (per coordinate), report max\n    half = post_samples.shape[0] // 2\n    first_half = post_samples[:half]\n    second_half = post_samples[half:]\n    ks_stats = []\n    for k in range(first_half.shape[1]):\n        stat, _ = ks_2samp(first_half[:, k], second_half[:, k], alternative='two-sided', mode='auto')\n        ks_stats.append(stat)\n    ks_max = float(np.max(ks_stats))\n    stationary_flag = (ks_max  0.08)\n\n    return [acc_rate, mean_err_norm, cov_err_fro, ks_max, stationary_flag]\n\ndef solve():\n    # Fixed problem setup\n    G = np.array([[1.0, 0.5],\n                  [0.3, 1.2]])\n    d = np.array([1.2, -0.7])\n    Cn = np.diag([0.1**2, 0.2**2])\n    m0 = np.array([0.0, 0.0])\n    Cm = np.diag([0.5**2, 0.5**2])\n\n    # HMC parameters\n    epsilon = 0.2\n    L = 20\n    N = 2000\n    burn_in = 500\n\n    # Test suite\n    test_cases = [\n        # bias_type, eta, sigma\n        (\"none\", np.array([0.0, 0.0]), 0.0),\n        (\"deterministic\", np.array([0.05, -0.05]), 0.0),\n        (\"deterministic\", np.array([0.2, -0.2]), 0.0),\n        (\"stochastic\", np.array([0.0, 0.0]), 0.05),\n        (\"stochastic\", np.array([0.0, 0.0]), 0.2),\n    ]\n\n    results = []\n    # Use distinct seeds for each case to avoid identical stochastic sequences where needed\n    base_seed = 20231010\n    for idx, (bias_type, eta, sigma) in enumerate(test_cases):\n        res = hmc_run(\n            G, Cn, m0, Cm, d,\n            epsilon, L, N, burn_in,\n            bias_type, eta, sigma,\n            seed=base_seed + idx\n        )\n        results.append(res)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3577488"}, {"introduction": "地球物理建模常常需要在模型准确性与计算成本之间做出权衡，从而产生具有不同保真度的多种正演模型。本练习 ([@problem_id:3577515]) 介绍了一个原则性的贝叶斯框架，用于融合来自高保真度（精确但昂贵）和低保真度（快速但近似）数据源的信息。您将学习如何构建一个包含偏差项的增广模型，从而实现一种能够利用所有可用信息且结果协调一致的不确定性量化方法。", "problem": "要求您在一个代表计算地球物理学的线性化设定中，形式化并实现一个基于数学原理的多保真度贝叶斯反演。该反演需包含一个显式的保真度指示器和一个差异先验，以避免得出过于自信的后验分布。本问题中的所有量均为无量纲。您必须从第一性原理出发，推导后验分布和预测分布，并实现一个程序，为提供的测试套件计算指定的标量摘要。您的最终程序必须完全按照本问题末尾的规定输出单行结果。\n\n场景如下。我们考虑一个参数向量 $\\mathbf{m} \\in \\mathbb{R}^2$ 和两个正演求解器，一个精细求解器 $G_f$ 和一个粗略求解器 $G_c$，两者均是关于 $\\mathbf{m}$ 的线性求解器。对于 $i \\in \\{1,\\dots,n\\}$ 和一个保真度指示器 $z_i \\in \\{0,1\\}$，定义观测模型为\n$$\ny_i = \n\\begin{cases}\n\\mathbf{a}_{f,i}^\\top \\mathbf{m} + \\varepsilon_i,  \\text{if } z_i = 1 \\,\\,(\\text{精细}),\\\\\n\\mathbf{a}_{c,i}^\\top \\mathbf{m} + b + \\varepsilon_i,  \\text{if } z_i = 0 \\,\\,(\\text{粗略}),\n\\end{cases}\n$$\n其中 $b \\in \\mathbb{R}$ 是一个标量差异项，用于建模粗略求解器的保真度偏差。随机噪声 $\\varepsilon_i$ 在不同 $i$ 之间是独立的，其分布为\n$$\n\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_f^2) \\text{ if } z_i=1,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_c^2) \\text{ if } z_i=0,\n$$\n其中 $\\sigma_f^2 > 0$ 和 $\\sigma_c^2 > 0$ 是已知的超参数。\n\n先验。假设独立的多元高斯先验\n$$\n\\mathbf{m} \\sim \\mathcal{N}(\\mathbf{m}_0,\\mathbf{C}_0), \\qquad b \\sim \\mathcal{N}(0,s_b^2).\n$$\n\n数据与算子。使用 $n=4$ 个观测值和以下正演算子：\n- 精细正演算子矩阵 $A_f \\in \\mathbb{R}^{4\\times 2}$，其行向量 $\\mathbf{a}_{f,i}^\\top$ 如下所示：\n$$\nA_f = \\begin{bmatrix}\n1.0  0.5\\\\\n0.5  1.0\\\\\n1.0  -0.2\\\\\n0.3  0.8\n\\end{bmatrix}.\n$$\n- 粗略正演算子矩阵 $A_c \\in \\mathbb{R}^{4\\times 2}$，其行向量 $\\mathbf{a}_{c,i}^\\top$ 如下所示：\n$$\nA_c = \\begin{bmatrix}\n0.9  0.5\\\\\n0.5  0.9\\\\\n0.95  -0.15\\\\\n0.32  0.78\n\\end{bmatrix}.\n$$\n\n使用以下观测数据向量 $\\mathbf{y} \\in \\mathbb{R}^4$：\n$$\n\\mathbf{y} = \\begin{bmatrix} 1.57 \\\\ 1.29 \\\\ 1.06 \\\\ 0.935 \\end{bmatrix}.\n$$\n\n使用以下高斯先验：\n$$\n\\mathbf{m}_0 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}, \\quad \\mathbf{C}_0 = \\mathrm{diag}(0.25, 0.25), \\quad s_b^2 = 0.09.\n$$\n\n定义用于预测的新的粗略设计向量为：\n$$\n\\mathbf{a}_{\\text{new},c} = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}.\n$$\n\n任务。\n1) 推导。从贝叶斯法则和给定的高斯假设出发，推导在给定数据 $\\mathbf{y}$、任意保真度指示向量 $\\mathbf{z} \\in \\{0,1\\}^n$ 以及已知的 $\\sigma_f^2,\\sigma_c^2$ 条件下，增广参数 $\\boldsymbol{\\theta} = [\\mathbf{m}^\\top, b]^\\top \\in \\mathbb{R}^3$ 的后验分布。您的推导必须将后验分布表示为闭合形式的多元高斯分布，其均值和协方差需要用以下各项表示：\n- $\\boldsymbol{\\theta}$ 的块对角先验均值和协方差，\n- 结合了 $A_f$、$A_c$ 以及由 $\\mathbf{z}$ 隐含的偏差列的设计矩阵，\n- 按行编码了 $\\sigma_f^2$ 和 $\\sigma_c^2$ 的对角噪声协方差矩阵。\n\n2) 后验预测。推导在设计向量 $\\mathbf{a}_{\\text{new},c}$ 和保真度指示器 $z_{\\text{new}} = 0$ 条件下，新的粗略保真度观测值 $y_{\\text{new}}$ 的后验预测分布。将预测均值和方差用 $\\boldsymbol{\\theta}$ 的后验均值和协方差以及已知的噪声方差 $\\sigma_c^2$ 显式表示。\n\n3) 实现。实现一个程序，使用您推导的公式，为下面测试套件中的每个测试用例计算以下输出：\n- 第一个分量 $m_1$ 的后验均值（一个浮点数），\n- 在 $\\mathbf{a}_{\\text{new},c}$ 处且 $z_{\\text{new}}=0$ 时，$y_{\\text{new}}$ 的后验预测方差（一个浮点数）。\n\n您的程序必须使用标准的稠密运算实现所有线性代数，并且不得依赖任何随机数生成。\n\n测试套件。使用上面给定的固定的 $\\mathbf{y}$、$A_f$、$A_c$、$\\mathbf{m}_0$、$\\mathbf{C}_0$、$s_b^2$ 和 $\\mathbf{a}_{\\text{new},c}$。对以下三种情况中的每一种，评估两个输出：\n- 情况 1 (混合保真度)：\n  - $\\mathbf{z} = [1,0,1,0]$,\n  - $\\sigma_f^2 = 0.01$, $\\sigma_c^2 = 0.04$.\n- 情况 2 (全为粗略，粗略方差较大)：\n  - $\\mathbf{z} = [0,0,0,0]$,\n  - $\\sigma_f^2 = 0.01$, $\\sigma_c^2 = 0.09$.\n- 情况 3 (全为精细，精细方差较小)：\n  - $\\mathbf{z} = [1,1,1,1]$,\n  - $\\sigma_f^2 = 0.0025$, $\\sigma_c^2 = 0.04$.\n\n最终输出格式。您的程序必须生成单行输出，其中包含三个测试用例的结果，格式为一个由方括号括起来的逗号分隔列表。每个测试用例的结果本身是一个包含两个元素的列表，顺序如上所述。每个浮点数必须四舍五入到六位小数。具体来说，程序必须打印形如下面的单行：\n$$\n[[m1_case1,vpred_case1],[m1_case2,vpred_case2],[m1_case3,vpred_case3]],\n$$\n例如，\n$$\n[[0.123456,0.654321],[\\dots],[\\dots]],\n$$\n除了此处显示的逗号和方括号外，不应有任何额外的空白字符。所有量均为无量纲，并且必须报告为四舍五入到小数点后六位的普通十进制数。", "solution": "用户提供的问题是有效的。这是一个在计算地球物理学和不确定性量化领域的适定问题，没有科学或数学上的不一致之处。所有必需的数据和定义均已提供。\n\n### 1. 问题形式化与贝叶斯模型建立\n\n该问题要求对一个增广参数向量 $\\boldsymbol{\\theta} \\in \\mathbb{R}^3$ 进行贝叶斯反演，该向量同时包含了物理参数 $\\mathbf{m} \\in \\mathbb{R}^2$ 和标量偏差项 $b \\in \\mathbb{R}$。我们将此增广向量定义为 $\\boldsymbol{\\theta} = [\\mathbf{m}^\\top, b]^\\top$。\n\n**先验分布：** 问题陈述了 $\\mathbf{m}$ 和 $b$ 的独立高斯先验：\n$$\n\\mathbf{m} \\sim \\mathcal{N}(\\mathbf{m}_0, \\mathbf{C}_0) \\quad \\text{and} \\quad b \\sim \\mathcal{N}(0, s_b^2).\n$$\n这意味着增广参数向量 $\\boldsymbol{\\theta}$ 上的联合高斯先验为：\n$$\np(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_0, \\mathbf{C}_{\\theta,0}),\n$$\n其中先验均值 $\\boldsymbol{\\theta}_0$ 和先验协方差 $\\mathbf{C}_{\\theta,0}$ 由下式给出：\n$$\n\\boldsymbol{\\theta}_0 = \\begin{bmatrix} \\mathbf{m}_0 \\\\ 0 \\end{bmatrix}, \\qquad\n\\mathbf{C}_{\\theta,0} = \\begin{bmatrix} \\mathbf{C}_0  \\mathbf{0} \\\\ \\mathbf{0}^\\top  s_b^2 \\end{bmatrix}.\n$$\n\n**似然函数：** 观测模型取决于保真度指示器 $z_i \\in \\{0, 1\\}$。我们可以为所有 $n$ 个观测值将其表示为一个统一的线性形式。\n令 $\\mathbf{y} = [y_1, \\dots, y_n]^\\top$ 为观测向量。第 $i$ 个观测的模型是：\n$$\ny_i =\n\\begin{cases}\n[\\mathbf{a}_{f,i}^\\top, 0] \\boldsymbol{\\theta} + \\varepsilon_i,  \\text{if } z_i=1 \\\\\n[\\mathbf{a}_{c,i}^\\top, 1] \\boldsymbol{\\theta} + \\varepsilon_i,  \\text{if } z_i=0\n\\end{cases}\n$$\n这可以为整个数据向量 $\\mathbf{y}$ 紧凑地写成一个单一的线性模型：\n$$\n\\mathbf{y} = \\mathbf{G} \\boldsymbol{\\theta} + \\boldsymbol{\\varepsilon},\n$$\n其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C}_\\varepsilon)$ 是噪声向量，$\\mathbf{G}$ 是一个 $n \\times 3$ 的设计矩阵。\n\n$\\mathbf{G}$ 的第 $i$ 行，记为 $\\mathbf{g}_i^\\top$，是根据保真度指示器 $z_i$ 构建的：\n$$\n\\mathbf{g}_i^\\top = z_i [\\mathbf{a}_{f,i}^\\top, 0] + (1-z_i) [\\mathbf{a}_{c,i}^\\top, 1].\n$$\n噪声协方差矩阵 $\\mathbf{C}_\\varepsilon$ 是对角的，因为噪声项 $\\varepsilon_i$ 是独立的。其对角线元素由 $z_i$ 决定：\n$$\n(\\mathbf{C}_\\varepsilon)_{ii} = z_i \\sigma_f^2 + (1-z_i) \\sigma_c^2.\n$$\n因此，似然函数由多元高斯分布的概率密度函数给出：\n$$\np(\\mathbf{y}|\\boldsymbol{\\theta}) = \\mathcal{N}(\\mathbf{y} | \\mathbf{G}\\boldsymbol{\\theta}, \\mathbf{C}_\\varepsilon).\n$$\n\n### 2. 后验分布的推导\n\n我们应用贝叶斯法则来找到后验分布 $p(\\boldsymbol{\\theta}|\\mathbf{y}) \\propto p(\\mathbf{y}|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})$。由于先验和似然都是高斯分布，因此 $\\boldsymbol{\\theta}$ 的后验分布也是一个多元高斯分布，我们记为：\n$$\np(\\boldsymbol{\\theta}|\\mathbf{y}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_{\\text{post}}, \\mathbf{C}_{\\theta,\\text{post}}).\n$$\n后验协方差矩阵 $\\mathbf{C}_{\\theta,\\text{post}}$ 和后验均值向量 $\\boldsymbol{\\theta}_{\\text{post}}$ 由贝叶斯线性回归的标准公式给出：\n后验精度矩阵是先验精度与数据精度（来自似然函数）之和：\n$$\n\\mathbf{C}_{\\theta,\\text{post}}^{-1} = \\mathbf{C}_{\\theta,0}^{-1} + \\mathbf{G}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{G}.\n$$\n后验协方差是后验精度的逆：\n$$\n\\mathbf{C}_{\\theta,\\text{post}} = (\\mathbf{C}_{\\theta,0}^{-1} + \\mathbf{G}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{G})^{-1}.\n$$\n后验均值是先验均值和数据的精度加权平均：\n$$\n\\boldsymbol{\\theta}_{\\text{post}} = \\mathbf{C}_{\\theta,\\text{post}} (\\mathbf{C}_{\\theta,0}^{-1} \\boldsymbol{\\theta}_0 + \\mathbf{G}^\\top \\mathbf{C}_{\\varepsilon}^{-1} \\mathbf{y}).\n$$\n这些公式以问题陈述中定义的量，为后验分布提供了闭合形式的表达式。向量 $\\boldsymbol{\\theta}_{\\text{post}}$ 的第一个分量是 $m_1$ 的后验均值，这是要求的输出之一。\n\n### 3. 后验预测分布的推导\n\n我们需要推导在设计向量 $\\mathbf{a}_{\\text{new},c}$ （即 $z_{\\text{new}}=0$）处的一个新的粗略保真度观测值 $y_{\\text{new}}$ 的分布。这个新观测值的模型是：\n$$\ny_{\\text{new}} = \\mathbf{a}_{\\text{new},c}^\\top \\mathbf{m} + b + \\varepsilon_{\\text{new}},\n$$\n其中 $\\varepsilon_{\\text{new}} \\sim \\mathcal{N}(0, \\sigma_c^2)$。用增广参数 $\\boldsymbol{\\theta}$ 表示，即为：\n$$\ny_{\\text{new}} = \\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta} + \\varepsilon_{\\text{new}}, \\quad \\text{where} \\quad \\mathbf{g}_{\\text{new}}^\\top = [\\mathbf{a}_{\\text{new},c}^\\top, 1].\n$$\n后验预测分布 $p(y_{\\text{new}}|\\mathbf{y})$ 是通过对 $\\boldsymbol{\\theta}$ 的后验分布进行边缘化得到的：\n$$\np(y_{\\text{new}}|\\mathbf{y}) = \\int p(y_{\\text{new}}|\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}|\\mathbf{y}) d\\boldsymbol{\\theta}.\n$$\n这是两个高斯分布的卷积，其结果是另一个高斯分布。我们可以求出其均值和方差。\n\n**预测均值：** 根据全期望定律：\n$$\n\\mathbb{E}[y_{\\text{new}}|\\mathbf{y}] = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbb{E}[y_{\\text{new}}|\\boldsymbol{\\theta}]] = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta}] = \\mathbf{g}_{\\text{new}}^\\top \\mathbb{E}[\\boldsymbol{\\theta}|\\mathbf{y}] = \\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta}_{\\text{post}}.\n$$\n\n**预测方差：** 根据全方差定律：\n$$\n\\text{Var}(y_{\\text{new}}|\\mathbf{y}) = \\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\text{Var}(y_{\\text{new}}|\\boldsymbol{\\theta})] + \\text{Var}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbb{E}(y_{\\text{new}}|\\boldsymbol{\\theta})].\n$$\n第一项是期望的观测方差：$\\mathbb{E}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\sigma_c^2] = \\sigma_c^2$。\n第二项是由于 $\\boldsymbol{\\theta}$ 的不确定性导致的预测均值的方差：\n$$\n\\text{Var}_{\\boldsymbol{\\theta}|\\mathbf{y}}[\\mathbf{g}_{\\text{new}}^\\top \\boldsymbol{\\theta}] = \\mathbf{g}_{\\text{new}}^\\top \\text{Var}(\\boldsymbol{\\theta}|\\mathbf{y}) \\mathbf{g}_{\\text{new}} = \\mathbf{g}_{\\text{new}}^\\top \\mathbf{C}_{\\theta,\\text{post}} \\mathbf{g}_{\\text{new}}.\n$$\n综合这两项，后验预测方差 $V_{\\text{pred}}$ 为：\n$$\nV_{\\text{pred}} = \\mathbf{g}_{\\text{new}}^\\top \\mathbf{C}_{\\theta,\\text{post}} \\mathbf{g}_{\\text{new}} + \\sigma_c^2.\n$$\n这是第二个要求的输出。\n\n### 4. 实现策略\n\n对三个测试用例中的每一个，实现将按以下步骤进行：\n1.  初始化给定的常量矩阵和向量：$A_f, A_c, \\mathbf{y}, \\mathbf{m}_0, \\mathbf{C}_0, s_b^2, \\mathbf{a}_{\\text{new},c}$。\n2.  构建增广先验均值 $\\boldsymbol{\\theta}_0$ 和先验协方差 $\\mathbf{C}_{\\theta,0}$。\n3.  对于由 $(\\mathbf{z}, \\sigma_f^2, \\sigma_c^2)$ 指定的给定测试用例：\n    a. 根据保真度向量 $\\mathbf{z}$，通过从 $A_f$ 和 $A_c$ 中选择并增广行来构建 $4 \\times 3$ 的设计矩阵 $\\mathbf{G}$。\n    b. 根据 $\\mathbf{z}$ 的指定，使用 $\\sigma_f^2$ 和 $\\sigma_c^2$ 构建 $4 \\times 4$ 的对角噪声协方差矩阵 $\\mathbf{C}_\\varepsilon$。\n    c. 计算逆矩阵 $\\mathbf{C}_{\\theta,0}^{-1}$ 和 $\\mathbf{C}_\\varepsilon^{-1}$。\n    d. 使用推导出的公式计算后验协方差 $\\mathbf{C}_{\\theta,\\text{post}}$。\n    e. 使用推导出的公式计算后验均值 $\\boldsymbol{\\theta}_{\\text{post}}$。\n    f. 提取 $\\boldsymbol{\\theta}_{\\text{post}}$ 的第一个元素作为 $m_1$ 的后验均值。\n    g. 构建新的设计向量 $\\mathbf{g}_{\\text{new}} = [\\mathbf{a}_{\\text{new},c}^\\top, 1]^\\top$。\n    h. 使用推导出的公式计算后验预测方差 $V_{\\text{pred}}$，其中附加的噪声方差对应于粗略模型，即 $\\sigma_c^2$。\n4.  按规定收集并格式化结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes posterior and predictive quantities for a multi-fidelity Bayesian inversion problem.\n    \"\"\"\n    \n    # --- 1. Define constants from the problem statement ---\n    \n    # Forward operator matrices\n    A_f = np.array([\n        [1.0, 0.5],\n        [0.5, 1.0],\n        [1.0, -0.2],\n        [0.3, 0.8]\n    ])\n    A_c = np.array([\n        [0.9, 0.5],\n        [0.5, 0.9],\n        [0.95, -0.15],\n        [0.32, 0.78]\n    ])\n    \n    # Observed data vector\n    y_obs = np.array([1.57, 1.29, 1.06, 0.935])\n    \n    # Prior on m\n    m0 = np.array([1.0, 1.0])\n    C0 = np.diag([0.25, 0.25])\n    \n    # Prior on b\n    s_b_sq = 0.09\n    \n    # New design vector for prediction\n    a_new_c = np.array([0.6, 0.4])\n\n    # --- 2. Construct augmented prior for theta = [m, b] ---\n    \n    # Augmented prior mean theta_0 = [m0, 0]\n    theta_0 = np.append(m0, 0.0)\n    \n    # Augmented prior covariance C_theta_0\n    C_theta_0 = np.block([\n        [C0, np.zeros((2, 1))],\n        [np.zeros((1, 2)), [[s_b_sq]]]\n    ])\n    \n    # Pre-compute prior precision and related term for posterior mean calculation\n    C_theta_0_inv = np.linalg.inv(C_theta_0)\n    C_theta_0_inv_theta_0 = C_theta_0_inv @ theta_0\n    \n    # Define test cases\n    test_cases = [\n        # Case 1: Mixed fidelity\n        {'z': np.array([1, 0, 1, 0]), 'sigma_f_sq': 0.01, 'sigma_c_sq': 0.04},\n        # Case 2: All coarse\n        {'z': np.array([0, 0, 0, 0]), 'sigma_f_sq': 0.01, 'sigma_c_sq': 0.09},\n        # Case 3: All fine\n        {'z': np.array([1, 1, 1, 1]), 'sigma_f_sq': 0.0025, 'sigma_c_sq': 0.04},\n    ]\n\n    all_results = []\n    \n    # --- 3. Process each test case ---\n    \n    for case in test_cases:\n        z = case['z']\n        sigma_f_sq = case['sigma_f_sq']\n        sigma_c_sq = case['sigma_c_sq']\n        \n        # --- a. Construct case-specific matrices G and C_epsilon ---\n        \n        # Design matrix G (4x3)\n        G = np.zeros((4, 3))\n        for i in range(4):\n            if z[i] == 1:\n                G[i, :2] = A_f[i, :]\n                G[i, 2] = 0.0\n            else: # z[i] == 0\n                G[i, :2] = A_c[i, :]\n                G[i, 2] = 1.0\n        \n        # Noise covariance C_epsilon (diagonal)\n        diag_C_eps = z * sigma_f_sq + (1 - z) * sigma_c_sq\n        C_eps_inv = np.diag(1.0 / diag_C_eps)\n\n        # --- b. Calculate posterior distribution for theta ---\n        \n        # Posterior precision matrix for theta\n        G_T_C_eps_inv_G = G.T @ C_eps_inv @ G\n        C_theta_post_inv = C_theta_0_inv + G_T_C_eps_inv_G\n        \n        # Posterior covariance matrix for theta\n        C_theta_post = np.linalg.inv(C_theta_post_inv)\n        \n        # Posterior mean for theta\n        G_T_C_eps_inv_y = G.T @ C_eps_inv @ y_obs\n        theta_post = C_theta_post @ (C_theta_0_inv_theta_0 + G_T_C_eps_inv_y)\n        \n        # Extract posterior mean of m1\n        m1_post_mean = theta_post[0]\n        \n        # --- c. Calculate posterior predictive variance ---\n        \n        # New design vector g_new for coarse prediction\n        g_new = np.append(a_new_c, 1.0)\n        \n        # Predictive variance V_pred = g_new^T * C_theta_post * g_new + sigma_c^2\n        V_pred = g_new.T @ C_theta_post @ g_new + sigma_c_sq\n        \n        all_results.append([m1_post_mean, V_pred])\n\n    # --- 4. Format and print the final output ---\n    \n    # Format each result pair into \"[val1,val2]\"\n    formatted_results = [f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in all_results]\n    \n    # Join the pairs and enclose in outer brackets\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3577515"}]}