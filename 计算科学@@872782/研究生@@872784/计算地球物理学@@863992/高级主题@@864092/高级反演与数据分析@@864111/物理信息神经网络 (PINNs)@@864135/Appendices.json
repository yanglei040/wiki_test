{"hands_on_practices": [{"introduction": "在我们要求神经网络遵循某个物理定律之前，我们首先需要一种方法来量化一个给定函数对该定律的“违背程度”。这个练习提供了一个具体的实践，让你亲手计算这种违背量，即“强形式残差”。通过解决一个简单的扩散问题，你将掌握这个核心概念的计算，这是理解物理约束神经网络（PINN）最小化目标的入门基础。[@problem_id:3612737]", "problem": "考虑一个计算地球物理学中的稳态扩散模型，其中方形域 $\\Omega=[0,1]\\times[0,1]$ 上的一个标量场 $u(x,y)$ 在无储存项的情况下满足守恒定律，这可简化为椭圆型偏微分方程 $-\\nabla\\cdot\\left(\\kappa(x,y)\\nabla u(x,y)\\right)=f(x,y)$，其中 $\\kappa(x,y)$ 是传导率，$f(x,y)$ 是源项。在物理信息神经网络 (PINN) 中，逐点强残差定义为 $r(x,y)=-\\nabla\\cdot\\left(\\kappa(x,y)\\nabla u(x,y)\\right)-f(x,y)$，它衡量了候选场 $u$ 在每个点上对控制方程的满足程度的偏差。令 $\\kappa(x,y)=1$，$f(x,y)\\equiv 0$，$u(x,y)=\\sin(\\pi x)\\sin(\\pi y)$。仅从守恒定律和标准矢量微积分运算出发，推导逐点强残差 $r(x,y)$，然后计算其 $L^{2}$ 范数 $\\|r\\|_{L^{2}(\\Omega)}=\\left(\\int_{0}^{1}\\int_{0}^{1}|r(x,y)|^{2}\\,\\mathrm{d}x\\,\\mathrm{d}y\\right)^{1/2}$。请将你的最终答案表示为精确的解析表达式。无需四舍五入，且此无量纲设置不涉及物理单位。", "solution": "该问题被验证为自洽的、有科学依据且适定的。所有必要信息均已提供，且所要求的计算是矢量微积分和泛函分析中的标准步骤。\n\n第一步是推导逐点强残差 $r(x,y)$ 的表达式。问题将残差定义为：\n$$r(x,y) = -\\nabla\\cdot\\left(\\kappa(x,y)\\nabla u(x,y)\\right)-f(x,y)$$\n给定传导率 $\\kappa(x,y) = 1$ 和源项 $f(x,y) = 0$。将这些代入定义中可得：\n$$r(x,y) = -\\nabla\\cdot\\left(1 \\cdot \\nabla u(x,y)\\right) - 0 = -\\nabla\\cdot\\left(\\nabla u(x,y)\\right)$$\n一个标量场的梯度的散度是该场的拉普拉斯算子，记作 $\\nabla^2 u$。因此，残差可简化为 $u(x,y)$ 的负拉普拉斯：\n$$r(x,y) = -\\nabla^2 u(x,y)$$\n在二维笛卡尔坐标系 $(x,y)$ 中，拉普拉斯算子定义为 $\\nabla^2 = \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$。因此，我们有：\n$$r(x,y) = -\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right)$$\n给定的候选场为 $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$。我们现在计算它的二阶偏导数。\n\n首先，我们计算关于 $x$ 的偏导数：\n$$\\frac{\\partial u}{\\partial x} = \\frac{\\partial}{\\partial x}\\left(\\sin(\\pi x)\\sin(\\pi y)\\right) = \\pi\\cos(\\pi x)\\sin(\\pi y)$$\n$$\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\left(\\pi\\cos(\\pi x)\\sin(\\pi y)\\right) = -\\pi^2\\sin(\\pi x)\\sin(\\pi y)$$\n\n接下来，我们计算关于 $y$ 的偏导数：\n$$\\frac{\\partial u}{\\partial y} = \\frac{\\partial}{\\partial y}\\left(\\sin(\\pi x)\\sin(\\pi y)\\right) = \\pi\\sin(\\pi x)\\cos(\\pi y)$$\n$$\\frac{\\partial^2 u}{\\partial y^2} = \\frac{\\partial}{\\partial y}\\left(\\pi\\sin(\\pi x)\\cos(\\pi y)\\right) = -\\pi^2\\sin(\\pi x)\\sin(\\pi y)$$\n\n现在，我们将二阶偏导数相加，求出 $u$ 的拉普拉斯：\n$$\\nabla^2 u(x,y) = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - \\pi^2\\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2\\sin(\\pi x)\\sin(\\pi y)$$\n\n最后，我们将此结果代入残差的表达式中：\n$$r(x,y) = -\\nabla^2 u(x,y) = -(-2\\pi^2\\sin(\\pi x)\\sin(\\pi y)) = 2\\pi^2\\sin(\\pi x)\\sin(\\pi y)$$\n这是第一个要求的结果。\n\n第二步是计算残差在域 $\\Omega=[0,1]\\times[0,1]$ 上的 $L^2$ 范数 $\\|r\\|_{L^2(\\Omega)}$。定义如下：\n$$\\|r\\|_{L^2(\\Omega)} = \\left(\\int_{0}^{1}\\int_{0}^{1}|r(x,y)|^2\\,\\mathrm{d}x\\,\\mathrm{d}y\\right)^{1/2}$$\n首先，我们计算残差的模的平方 $|r(x,y)|^2$：\n$$|r(x,y)|^2 = \\left|2\\pi^2\\sin(\\pi x)\\sin(\\pi y)\\right|^2 = 4\\pi^4\\sin^2(\\pi x)\\sin^2(\\pi y)$$\n现在我们计算该表达式在域 $\\Omega$ 上的积分：\n$$\\|r\\|_{L^2(\\Omega)}^2 = \\int_{0}^{1}\\int_{0}^{1} 4\\pi^4\\sin^2(\\pi x)\\sin^2(\\pi y)\\,\\mathrm{d}x\\,\\mathrm{d}y$$\n被积函数是可分离的，因此我们可以将二重积分写成两个单变量积分的乘积：\n$$\\|r\\|_{L^2(\\Omega)}^2 = 4\\pi^4 \\left(\\int_{0}^{1}\\sin^2(\\pi x)\\,\\mathrm{d}x\\right) \\left(\\int_{0}^{1}\\sin^2(\\pi y)\\,\\mathrm{d}y\\right)$$\n我们来计算关于 $x$ 的积分。我们使用降幂恒等式 $\\sin^2(\\theta) = \\frac{1-\\cos(2\\theta)}{2}$：\n$$\\int_{0}^{1}\\sin^2(\\pi x)\\,\\mathrm{d}x = \\int_{0}^{1}\\frac{1-\\cos(2\\pi x)}{2}\\,\\mathrm{d}x$$\n$$= \\frac{1}{2}\\left[x - \\frac{\\sin(2\\pi x)}{2\\pi}\\right]_{0}^{1}$$\n$$= \\frac{1}{2}\\left(\\left(1 - \\frac{\\sin(2\\pi)}{2\\pi}\\right) - \\left(0 - \\frac{\\sin(0)}{2\\pi}\\right)\\right)$$\n由于 $\\sin(2\\pi)=0$ 和 $\\sin(0)=0$，该表达式简化为：\n$$\\int_{0}^{1}\\sin^2(\\pi x)\\,\\mathrm{d}x = \\frac{1}{2}\\left((1-0) - (0-0)\\right) = \\frac{1}{2}$$\n关于 $y$ 的积分在形式和值上都是相同的：\n$$\\int_{0}^{1}\\sin^2(\\pi y)\\,\\mathrm{d}y = \\frac{1}{2}$$\n将这些结果代回 $\\|r\\|_{L^2(\\Omega)}^2$ 的表达式中：\n$$\\|r\\|_{L^2(\\Omega)}^2 = 4\\pi^4 \\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right) = 4\\pi^4\\left(\\frac{1}{4}\\right) = \\pi^4$$\n$L^2$ 范数是该值的平方根：\n$$\\|r\\|_{L^2(\\Omega)} = \\sqrt{\\pi^4} = \\pi^2$$\n这是第二个要求的结果。\n\n两个最终的解析表达式是 $r(x,y)=2\\pi^2\\sin(\\pi x)\\sin(\\pi y)$ 和 $\\|r\\|_{L^2(\\Omega)} = \\pi^2$。", "answer": "$$\\boxed{\\begin{pmatrix} 2\\pi^{2}\\sin(\\pi x)\\sin(\\pi y)  \\pi^{2} \\end{pmatrix}}$$", "id": "3612737"}, {"introduction": "现在我们已经理解了残差的概念，接下来我们可以构建驱动PINN的核心引擎：损失函数。这个实践将指导你将一个偏微分方程（泊松方程）及其边界条件转化为一个可优化的单一目标函数。这是神经网络“学习”背后物理原理的基本机制，也是所有PINN应用的核心。[@problem_id:3430996]", "problem": "考虑方形域 $\\Omega = [0,1]^{2}$ 上的泊松方程，其边界条件为齐次狄利克雷边界条件，\n$$\n-\\Delta u = f \\quad \\text{in } \\Omega, \n\\qquad \nu = 0 \\quad \\text{on } \\partial \\Omega,\n$$\n其中 $u:\\mathbb{R}^{2}\\to\\mathbb{R}$，$f:\\Omega\\to\\mathbb{R}$ 是一个给定的源项。设 $u_{\\theta}:\\mathbb{R}^{2}\\to\\mathbb{R}$ 是一个具有参数 $\\theta$ 的足够光滑的神经网络拟设。在物理信息神经网络（PINN）中，通过将控制微分算子应用于 $u_{\\theta}$ 并减去已知的右端项来构造强形式残差。给定两组配置点：内部点集 $\\{(x_{i},y_{i})\\}_{i=1}^{M}\\subset \\Omega^{\\circ}$ 和边界点集 $\\{(x^{(b)}_{j},y^{(b)}_{j})\\}_{j=1}^{N}\\subset \\partial\\Omega$。$u_{\\theta}$ 关于 $x$ 和 $y$ 的导数可以通过自动微分获得。\n\n从加权残差法和最小二乘原理出发，并且除了上述定义之外不假设任何特殊结构，推导：\n\n1. 泊松问题的显式强形式残差 $r_{\\theta}(x,y)$，用 $u_{\\theta}$、其偏导数和 $f$ 表示。\n2. 一个惩罚内部残差和边界条件违反的均方经验配置损失。引入一个正的边界加权参数 $\\lambda_{b} > 0$，并对给定的配置点集使用均匀权重。\n\n将你的最终答案表示为待最小化的总损失 $L(\\theta)$ 的单个闭式解析表达式，用 $u_{\\theta}$、$f$、点集、$M$、$N$ 和 $\\lambda_{b}$ 书写。不要计算任何数值。你的最终答案必须是单个解析表达式，而不是一个不等式或待解方程。", "solution": "第一步是推导强形式残差 $r_{\\theta}(x,y)$。给定的控制偏微分方程是 $-\\Delta u = f$，可以重写为 $\\Delta u + f = 0$。强形式残差是当我们将近似解 $u_{\\theta}$ 代入该方程时得到的表达式。在笛卡尔坐标系中，拉普拉斯算子 $\\Delta$ 展开为 $\\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2}$。因此，区域内部的强形式残差为：\n$$r_{\\theta}(x,y) = \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x,y) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x,y) + f(x,y)$$\n\n第二步是根据最小二乘原理构建复合损失函数 $L(\\theta)$。总损失是来自 PDE 内部的损失 $L_{PDE}(\\theta)$ 和来自边界条件的损失 $L_{BC}(\\theta)$ 的加权和：\n$$L(\\theta) = L_{PDE}(\\theta) + \\lambda_b L_{BC}(\\theta)$$\n\n内部损失 $L_{PDE}(\\theta)$ 是在 $M$ 个内部配点 $\\{(x_i, y_i)\\}_{i=1}^{M}$ 上评估的 PDE 残差的均方误差：\n$$L_{PDE}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} [r_{\\theta}(x_i, y_i)]^2 = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2$$\n\n边界损失 $L_{BC}(\\theta)$ 惩罚对齐次狄利克雷条件 $u=0$ 在 $\\partial\\Omega$ 上的违反。它是网络输出在 $N$ 个边界点 $\\{(x^{(b)}_{j}, y^{(b)}_{j})\\}_{j=1}^{N}$ 上的均方误差：\n$$L_{BC}(\\theta) = \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) - 0 \\right)^2 = \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$$\n\n最后，将各部分组合起来，得到待最小化的总损失函数。注意，在最终表达式中，我们将 $L_{BC}$ 的权重 $\\lambda_b$ 与求和前的系数 $\\frac{1}{N}$ 结合。\n$$L(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\frac{\\lambda_b}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$$\n该表达式符合问题的要求，并与最终答案中的公式一致。", "answer": "$$ \\boxed{ \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\frac{\\lambda_b}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2 } $$", "id": "3430996"}, {"introduction": "许多地球物理问题，从流体流动到热传导，都由其边界上的通量（即诺伊曼条件）定义。这个练习旨在解决将这些更复杂的边界条件整合到PINN损失函数中的实际挑战。你将学习如何利用自动微分得到的梯度来计算法向导数，这是将PINN应用于具有复杂几何形状和现实物理约束问题的关键技术。[@problem_id:3431045]", "problem": "考虑一个由物理信息神经网络 (PINN) 表示的标量场 $u_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$，该网络经过训练以近似求解有界域 $\\Omega\\subset\\mathbb{R}^{d}$（其边界为 $\\partial\\Omega$）内的偏微分方程 (PDE)。在边界上，模型必须满足诺伊曼边界条件 $\\partial_{n}u=h$ on $\\partial\\Omega$，其中 $h:\\partial\\Omega\\to\\mathbb{R}$ 是一个指定的通量，$\\partial_{n}u$ 表示外法向导数。边界 $\\partial\\Omega$ 由一个光滑函数 $\\phi:\\mathbb{R}^{d}\\to\\mathbb{R}$ 的零水平集隐式给出，其中对于所有 $x\\in\\partial\\Omega$ 都有 $\\nabla\\phi(x)\\neq 0$，单位外法向量为 $n(x)=\\nabla\\phi(x)/\\|\\nabla\\phi(x)\\|$。给定一个边界求积法则 $\\{(x_{i}^{b},w_{i})\\}_{i=1}^{N_{b}}$，其中 $x_{i}^{b}\\in\\partial\\Omega$ 且权重 $w_{i}>0$，该求积法则用于近似 $\\partial\\Omega$ 上的曲面积分。PINN的训练通过最小化一个损失泛函来进行，该泛函包含一个边界惩罚项，用于在平方 $L^{2}$ 意义下强制施加诺伊曼条件，并通过给定的求积法则进行近似。可以使用自动微分 (AD) 来计算 $u_{\\theta}$ 相对于其输入的空间导数，并且 $\\phi$ 的实现方式也使其梯度可以被计算。\n\n从法向导数的定义 $\\partial_{n}u(x)=n(x)^{\\top}\\nabla_{x}u(x)$ 和 $L^{2}$ 边界失配出发，推导边界损失 $L_{N}(\\theta)$ 的闭式解析表达式。该损失用于惩罚 $\\partial\\Omega$ 上 $\\partial_{n}u_{\\theta}$ 与 $h$ 之间的差异，并使用所提供的求积法则。将法向导数完全用可通过自动微分 (AD) 计算的量和水平集几何来表示，并将损失写为边界样本上的归一化加权和。\n\n你的最终答案必须是关于 $\\{x_{i}^{b},w_{i}\\}_{i=1}^{N_{b}}$、$u_{\\theta}$、$\\phi$ 和 $h$ 的 $L_{N}(\\theta)$ 的单个显式解析表达式。最终答案中不要包含任何解释性文字。", "solution": "该问题要求推导边界损失 $L_{N}(\\theta)$ 的闭式解析表达式，该损失用于为物理信息神经网络 (PINN) 施加诺伊曼边界条件。推导过程从边界条件在适当函数空间中的失配的基本定义开始，然后进行离散化。\n\n首先，我们定义诺伊曼边界条件的残差。该条件为边界 $\\partial\\Omega$ 上的 $\\partial_{n}u = h$。对于神经网络近似解 $u_{\\theta}$，在点 $x \\in \\partial\\Omega$ 处的边界残差由下式给出：\n$$\nR_{N}(x; \\theta) = \\partial_{n}u_{\\theta}(x) - h(x)\n$$\n问题指明边界惩罚是在平方 $L^{2}$ 意义下施加的。边界 $\\partial\\Omega$ 上残差的平方 $L^{2}$ 范数由以下曲面积分定义：\n$$\n\\|R_{N}(\\cdot; \\theta)\\|_{L^{2}(\\partial\\Omega)}^{2} = \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\n其中 $dS(x)$ 是微分曲面元。在机器学习和数值优化的背景下，通常使用平均误差，这使得损失项与域的大小无关。这可以通过除以边界的总表面积 $|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x)$ 来实现。因此，均方边界误差为：\n$$\n\\mathcal{E}_{N}(\\theta) = \\frac{1}{|\\partial\\Omega|} \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\n下一步是将法向导数 $\\partial_{n}u_{\\theta}(x)$ 表示为可以用自动微分 (AD) 计算的量。问题给出了法向导数的定义，即 $u_{\\theta}$ 的梯度在单位外法向量 $n(x)$ 上的投影：\n$$\n\\partial_{n}u_{\\theta}(x) = n(x)^{\\top}\\nabla_{x}u_{\\theta}(x)\n$$\n单位外法向量 $n(x)$ 是使用水平集函数 $\\phi(x)$ 定义的，其中 $\\partial\\Omega = \\{x \\in \\mathbb{R}^{d} | \\phi(x)=0\\}$。其公式为：\n$$\nn(x) = \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|}\n$$\n梯度的欧几里得范数为 $\\|\\nabla\\phi(x)\\| = \\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}$。将 $n(x)$ 的表达式代入法向导数的定义中，得到：\n$$\n\\partial_{n}u_{\\theta}(x) = \\left( \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|} \\right)^{\\top} \\nabla_{x}u_{\\theta}(x) = \\frac{\\nabla\\phi(x)^{\\top} \\nabla_{x}u_{\\theta}(x)}{\\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}}\n$$\n根据问题陈述，梯度 $\\nabla_{x}u_{\\theta}(x)$ 和 $\\nabla\\phi(x)$ 都可以通过自动微分 (AD) 计算。\n\n最后一步是使用给定的边界求积法则 $\\{(x_{i}^{b}, w_{i})\\}_{i=1}^{N_{b}}$ 来近似连续的均方误差 $\\mathcal{E}_{N}(\\theta)$。求积法则将曲面 $\\mathcal{S}$ 上的积分近似为 $\\int_{\\mathcal{S}} f(x) dS(x) \\approx \\sum_{i} w_{i} f(x_{i})$。将此应用于我们的分子和分母：\n平方残差的积分近似为：\n$$\n\\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x) \\approx \\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}\n$$\n总表面积近似为：\n$$\n|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x) \\approx \\sum_{j=1}^{N_{b}} w_{j}\n$$\n损失 $L_{N}(\\theta)$ 是均方误差 $\\mathcal{E}_{N}(\\theta)$ 的离散近似。结合这些近似，我们得到归一化加权和：\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\n将每个边界点 $x_{i}^{b}$ 处可通过 AD 计算的法向导数表达式代入此公式，即可得到边界损失的最终闭式表达式：\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\n此表达式仅依赖于网络输出 $u_{\\theta}$ 和水平集函数 $\\phi$（及其梯度）、边界点 $x_{i}^{b}$ 和权重 $w_{i}$，以及指定的通量函数 $h$，所有这些都是已知或可计算的。", "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}}\n$$", "id": "3431045"}]}