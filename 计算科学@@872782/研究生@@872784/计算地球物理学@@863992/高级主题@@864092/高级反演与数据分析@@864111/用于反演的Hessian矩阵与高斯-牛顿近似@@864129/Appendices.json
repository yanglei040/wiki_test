{"hands_on_practices": [{"introduction": "理解优化算法中每一步的行为至关重要。本练习将精确牛顿法和高斯-牛顿法置于一个具有挑战性的场景——非凸区域——中进行直接对比。通过分析一个简单的一维模型 [@problem_id:3603042]，我们将深入了解这两种方法的基本属性，并揭示为何高斯-牛顿近似在正演模型灵敏度较低的点附近可能会失效。", "problem": "在一个代表计算地球物理学中局部振幅校准子问题的单参数非线性反问题中，考虑正演模型 $F(m) = \\sin(m)$，其中 $m$ 以弧度为单位。观测到单个数据 $d \\in \\mathbb{R}$，残差和最小二乘目标函数分别定义为 $r(m) = F(m) - d$ 和 $\\phi(m) = \\tfrac{1}{2} r(m)^{2}$。取 $d = \\tfrac{1}{2}$ 并记 $m^{\\star} = \\tfrac{\\pi}{2}$。注意到 $F^{\\prime}(m^{\\star}) = 0$ 且 $F^{\\prime\\prime}(m^{\\star}) = -1$，因此 $\\phi(m)$ 在 $m^{\\star}$ 处有一个非凸驻点，该点处的精确曲率为负。令 $m = m^{\\star} + \\varepsilon$，其中 $|\\varepsilon| \\ll 1$。\n\n仅从 $r(m)$ 和 $\\phi(m)$ 的定义出发，并利用 $F$ 及其导数在 $m^{\\star}$ 附近的 Taylor 展开，完成以下任务：\n\n1. 用 $F^{\\prime}(m)$、$F^{\\prime\\prime}(m)$ 和 $r(m)$ 推导梯度 $g(m) = \\nabla \\phi(m)$ 和精确 Hessian 矩阵 $H(m) = \\nabla^{2} \\phi(m)$ 的表达式。\n\n2. 使用 $\\phi$ 在 $m$ 处的二阶 Taylor 模型，最小化该模型以获得作为 $\\varepsilon$ 函数的精确牛顿步长 $\\delta m_{\\mathrm{N}}$。\n\n3. 使用残差的一阶线性化 $r(m + \\delta m) \\approx r(m) + F^{\\prime}(m)\\,\\delta m$，最小化相应的二次模型以获得作为 $\\varepsilon$ 函数的高斯-牛顿步长 $\\delta m_{\\mathrm{GN}}$。\n\n4. 计算 $\\delta m_{\\mathrm{N}}(\\varepsilon)$ 和 $\\delta m_{\\mathrm{GN}}(\\varepsilon)$ 在 $\\varepsilon = 0$ 附近的双项渐近展开式，对 $\\delta m_{\\mathrm{N}}$ 保留至 $\\varepsilon^{3}$ 阶（含），对 $\\delta m_{\\mathrm{GN}}$ 保留至 $\\varepsilon^{-1}$ 阶和 $\\varepsilon^{1}$ 阶（含）。\n\n将最终结果以单个 $1 \\times 2$ 行向量 $\\big[\\delta m_{\\mathrm{N}}(\\varepsilon),\\, \\delta m_{\\mathrm{GN}}(\\varepsilon)\\big]$ 的形式给出。无需单位。角度以弧度为单位。", "solution": "该问题经验证是自洽的、一致的且科学上合理的。这是一个应用于地球物理反演的数值优化领域中的适定问题。\n\n最小二乘目标函数为 $\\phi(m) = \\frac{1}{2}r(m)^2$，其中残差为 $r(m) = F(m) - d$。正演模型为 $F(m) = \\sin(m)$，数据为 $d = \\frac{1}{2}$。展开点为 $m^{\\star} = \\frac{\\pi}{2}$，我们在 $m = m^{\\star} + \\varepsilon = \\frac{\\pi}{2} + \\varepsilon$（其中 $|\\varepsilon| \\ll 1$）处分析此问题。\n\n**1. 梯度与精确 Hessian 矩阵**\n\n首先，我们推导目标函数 $\\phi(m)$ 的梯度 $g(m)$ 和 Hessian 矩阵 $H(m)$ 的一般表达式。\n梯度 $g(m) = \\phi^{\\prime}(m)$ 是通过使用链式法则对 $\\phi(m)$ 关于 $m$ 求导得到的：\n$$\ng(m) = \\frac{d}{dm} \\left( \\frac{1}{2} r(m)^2 \\right) = \\frac{1}{2} \\cdot 2 r(m) \\cdot r^{\\prime}(m) = r(m) r^{\\prime}(m)\n$$\n由于 $r(m) = F(m) - d$，其导数为 $r^{\\prime}(m) = F^{\\prime}(m)$。代入此式可得梯度：\n$$\ng(m) = r(m) F^{\\prime}(m)\n$$\nHessian 矩阵 $H(m) = \\phi^{\\prime\\prime}(m)$ 是梯度 $g(m)$ 关于 $m$ 的导数。使用乘法法则：\n$$\nH(m) = \\frac{d}{dm} \\left( r(m) F^{\\prime}(m) \\right) = r^{\\prime}(m) F^{\\prime}(m) + r(m) F^{\\prime\\prime}(m)\n$$\n代入 $r^{\\prime}(m) = F^{\\prime}(m)$ 可得精确 Hessian 矩阵：\n$$\nH(m) = (F^{\\prime}(m))^2 + r(m) F^{\\prime\\prime}(m)\n$$\n此表达式由两项组成。第一项 $(F^{\\prime}(m))^2$ 是 Hessian 矩阵的高斯-牛顿近似。第二项 $r(m) F^{\\prime\\prime}(m)$ 在实践中常被忽略，尤其是在残差 $r(m)$ 很小或模型接近线性（即 $F^{\\prime\\prime}(m)$ 很小）的情况下。\n\n**2. 精确牛顿步长 $\\delta m_{\\mathrm{N}}$**\n\n牛顿步长 $\\delta m_{\\mathrm{N}}$ 是通过最小化 $\\phi(m)$ 在当前点 $m$ 附近的二阶 Taylor 展开式获得的：\n$$\n\\phi(m + \\delta m) \\approx \\phi(m) + g(m) \\delta m + \\frac{1}{2} H(m) (\\delta m)^2\n$$\n为求驻点，将上式关于 $\\delta m$ 的导数设为零，可得：\n$$\ng(m) + H(m) \\delta m = 0 \\implies \\delta m_{\\mathrm{N}} = -H(m)^{-1} g(m) = -\\frac{g(m)}{H(m)}\n$$\n为将其表示为 $\\varepsilon$ 的函数，我们在 $m = \\frac{\\pi}{2} + \\varepsilon$ 处计算 $F(m)$、$F^{\\prime}(m)$、$F^{\\prime\\prime}(m)$ 和 $r(m)$ 的值：\n$F(m) = \\sin(\\frac{\\pi}{2} + \\varepsilon) = \\cos(\\varepsilon)$\n$F^{\\prime}(m) = \\cos(\\frac{\\pi}{2} + \\varepsilon) = -\\sin(\\varepsilon)$\n$F^{\\prime\\prime}(m) = -\\sin(\\frac{\\pi}{2} + \\varepsilon) = -\\cos(\\varepsilon)$\n$r(m) = F(m) - d = \\cos(\\varepsilon) - \\frac{1}{2}$\n\n现在，我们用 $\\varepsilon$ 来表示 $g(m)$ 和 $H(m)$：\n$g(m) = (\\cos(\\varepsilon) - \\frac{1}{2})(-\\sin(\\varepsilon))$\n$H(m) = (-\\sin(\\varepsilon))^2 + (\\cos(\\varepsilon) - \\frac{1}{2})(-\\cos(\\varepsilon)) = \\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)$\n\n将这些代入牛顿步长的公式中，可得：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = -\\frac{(\\cos(\\varepsilon) - \\frac{1}{2})(-\\sin(\\varepsilon))}{\\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)} = \\frac{(\\cos(\\varepsilon) - \\frac{1}{2})\\sin(\\varepsilon)}{\\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon)}\n$$\n\n**3. 高斯-牛顿步长 $\\delta m_{\\mathrm{GN}}$**\n\n高斯-牛顿步长是通过最小化由残差的一阶线性化 $r(m + \\delta m) \\approx r(m) + F^{\\prime}(m) \\delta m$ 构成的目标函数推导出来的。相应的二次模型为 $\\phi_{\\mathrm{lin}}(\\delta m) = \\frac{1}{2}(r(m) + F^{\\prime}(m) \\delta m)^2$。对此模型关于 $\\delta m$ 进行最小化：\n$$\n\\frac{d\\phi_{\\mathrm{lin}}}{d(\\delta m)} = (r(m) + F^{\\prime}(m) \\delta m) F^{\\prime}(m) = 0\n$$\n这可导出 $r(m)F'(m) + (F'(m))^2 \\delta m = 0$，这等价于求解 $g(m) + H_{\\mathrm{GN}}(m)\\delta m = 0$，其中 $H_{\\mathrm{GN}}(m) = (F^{\\prime}(m))^2$。因此，高斯-牛顿步长为：\n$$\n\\delta m_{\\mathrm{GN}} = -\\frac{g(m)}{H_{\\mathrm{GN}}(m)} = -\\frac{r(m) F^{\\prime}(m)}{(F^{\\prime}(m))^2} = -\\frac{r(m)}{F^{\\prime}(m)}\n$$\n代入用 $\\varepsilon$ 表示的表达式：\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = -\\frac{\\cos(\\varepsilon) - \\frac{1}{2}}{-\\sin(\\varepsilon)} = \\frac{\\cos(\\varepsilon) - \\frac{1}{2}}{\\sin(\\varepsilon)}\n$$\n\n**4. 渐近展开**\n\n我们使用以下关于 $\\varepsilon = 0$ 的 Taylor 级数展开：\n$\\sin(\\varepsilon) = \\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)$\n$\\cos(\\varepsilon) = 1 - \\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^4}{24} + \\mathcal{O}(\\varepsilon^6)$\n\n**$\\delta m_{\\mathrm{N}}(\\varepsilon)$ 的展开：**\n分子为 $N(\\varepsilon) = (\\cos(\\varepsilon) - \\frac{1}{2})\\sin(\\varepsilon) = (\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4))(\\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)) = \\frac{1}{2}\\varepsilon - \\frac{\\varepsilon^3}{12} - \\frac{\\varepsilon^3}{2} + \\mathcal{O}(\\varepsilon^5) = \\frac{1}{2}\\varepsilon - \\frac{7}{12}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)$。\n分母为 $D(\\varepsilon) = \\sin^2(\\varepsilon) - \\cos^2(\\varepsilon) + \\frac{1}{2}\\cos(\\varepsilon) = (\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)) - (1 - \\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)) + \\frac{1}{2}(1 - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)) = -\\frac{1}{2} + \\frac{7}{4}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)$。\n进行级数除法：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = \\frac{\\frac{1}{2}\\varepsilon - \\frac{7}{12}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)}{-\\frac{1}{2} + \\frac{7}{4}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)} = \\left(-\\varepsilon + \\frac{7}{6}\\varepsilon^3\\right) \\frac{1}{1 - \\frac{7}{2}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)}\n$$\n使用几何级数展开 $(1-x)^{-1} = 1+x+x^2+\\dots$：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = \\left(-\\varepsilon + \\frac{7}{6}\\varepsilon^3\\right) \\left(1 + \\frac{7}{2}\\varepsilon^2 + \\mathcal{O}(\\varepsilon^4)\\right) = -\\varepsilon - \\frac{7}{2}\\varepsilon^3 + \\frac{7}{6}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5) = -\\varepsilon - \\frac{14}{6}\\varepsilon^3 + \\mathcal{O}(\\varepsilon^5)\n$$\n因此，牛顿步长的双项展开式为：\n$$\n\\delta m_{\\mathrm{N}}(\\varepsilon) = -\\varepsilon - \\frac{7}{3}\\varepsilon^3\n$$\n\n**$\\delta m_{\\mathrm{GN}}(\\varepsilon)$ 的展开：**\n分子为 $N(\\varepsilon) = \\cos(\\varepsilon) - \\frac{1}{2} = \\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\frac{\\varepsilon^4}{24} + \\mathcal{O}(\\varepsilon^6)$。\n分母为 $D(\\varepsilon) = \\sin(\\varepsilon) = \\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)$。\n进行级数除法：\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)}{\\varepsilon - \\frac{\\varepsilon^3}{6} + \\mathcal{O}(\\varepsilon^5)} = \\frac{1}{\\varepsilon} \\frac{\\frac{1}{2} - \\frac{\\varepsilon^2}{2} + \\mathcal{O}(\\varepsilon^4)}{1 - \\frac{\\varepsilon^2}{6} + \\mathcal{O}(\\varepsilon^4)}\n$$\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} - \\frac{\\varepsilon^2}{2}\\right)\\left(1 + \\frac{\\varepsilon^2}{6}\\right) + \\mathcal{O}(\\varepsilon^3) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} + \\frac{\\varepsilon^2}{12} - \\frac{\\varepsilon^2}{2}\\right) + \\mathcal{O}(\\varepsilon^3) = \\frac{1}{\\varepsilon} \\left(\\frac{1}{2} - \\frac{5}{12}\\varepsilon^2\\right) + \\mathcal{O}(\\varepsilon^3)\n$$\n因此，高斯-牛顿步长的双项展开式为：\n$$\n\\delta m_{\\mathrm{GN}}(\\varepsilon) = \\frac{1}{2}\\varepsilon^{-1} - \\frac{5}{12}\\varepsilon\n$$\n\n牛顿步长将当前位置 $m = m^\\star+\\varepsilon$ 修正了大约 $-\\varepsilon$，这表明它试图返回到驻点 $m^\\star$。高阶项显示了曲率的影响。当 $\\varepsilon \\to 0$ 时，高斯-牛顿步长变为奇异的，这是当 $F^{\\prime}(m) \\to 0$ 时的典型行为。\n\n最终结果是包含这两个渐近展开式的行向量。", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\varepsilon - \\frac{7}{3}\\varepsilon^{3} & \\frac{1}{2}\\varepsilon^{-1} - \\frac{5}{12}\\varepsilon \\end{pmatrix}}\n$$", "id": "3603042"}, {"introduction": "在进行了理论上的分析对比后，本练习将转向计算验证。我们将开发一个数值工具来量化高斯-牛顿近似所引入的误差。通过显式计算泰勒余项 [@problem_id:3603122]，我们可以精确地观察到被忽略的二阶导数项如何影响二次模型的准确性，以及该误差如何随步长和残差大小的变化而变化。", "problem": "考虑一个计算地球物理学中的反演问题，其中预测数据由一个非线性正演算子生成。设模型向量为 $m \\in \\mathbb{R}^n$，预测数据为 $F(m) \\in \\mathbb{R}^p$，观测数据为 $d \\in \\mathbb{R}^p$。定义最小二乘数据失配目标函数为\n$$\n\\phi(m) = \\frac{1}{2} \\lVert F(m) - d \\rVert_2^2,\n$$\n其中残差为 $r(m) = F(m) - d$。令 $J(m) \\in \\mathbb{R}^{p \\times n}$ 表示 $F(m)$ 的雅可比矩阵（一阶导数），并令 $\\nabla^2 F_i(m) \\in \\mathbb{R}^{n \\times n}$ 表示第 $i$ 个分量 $F_i(m)$ 的海森矩阵（二阶导数）。\n\n在 $m$ 处的失配梯度为 $g = J(m)^\\top r(m)$。高斯-牛顿海森近似为 $H_{GN}(m) = J(m)^\\top J(m)$。对于一个步长 $v \\in \\mathbb{R}^n$，定义关于高斯-牛顿二次模型的泰勒余项为\n$$\nR(v) = \\phi(m + v) - \\phi(m) - g^\\top v - \\frac{1}{2} v^\\top H_{GN}(m) \\, v.\n$$\n\n您的任务是实现一个程序，针对一个特定的非线性正演算子 $F(m)$，计算 $R(v)$，并将其与 $\\phi(m)$ 的海森矩阵中被忽略的 $F(m)$ 的二阶导数所产生的贡献进行定量关联。使用以下正演算子，该算子在模拟指数衰减的地球物理响应中很常见：\n$$\nF(m) = \\exp(A m),\n$$\n其中指数运算是逐分量应用的，而 $A \\in \\mathbb{R}^{p \\times n}$ 是一个固定矩阵。这个 $F(m)$ 是一个光滑的非线性映射。对于此 $F(m)$，其雅可比矩阵和逐分量海森矩阵存在且连续。\n\n定义以下数值测试配置：\n- 模型维度 $n = 3$ 和数据维度 $p = 4$。\n- 矩阵 $A \\in \\mathbb{R}^{4 \\times 3}$ 由下式给出\n$$\nA = \\begin{bmatrix}\n0.5 & -0.2 & 0.3 \\\\\n-0.1 & 0.4 & 0.6 \\\\\n0.3 & 0.1 & -0.5 \\\\\n0.0 & -0.3 & 0.2 \\\\\n\\end{bmatrix}.\n$$\n- 真实模型 $m_{\\mathrm{true}} \\in \\mathbb{R}^3$ 由下式给出\n$$\nm_{\\mathrm{true}} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}.\n$$\n- 观测数据 $d \\in \\mathbb{R}^4$ 是无噪声的，并由 $d = F(m_{\\mathrm{true}})$ 给出。\n- 考虑三个测试用例，每个用例指定一个基准模型 $m$ 和一个步长 $v$：\n  1. 用例 1 (中等残差，中等步长): \n     $$\n     m = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n     v = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.04 \\end{bmatrix}.\n     $$\n  2. 用例 2 (中等残差，非常小的步长):\n     $$\n     m = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n     v = \\begin{bmatrix} 0.00005 \\\\ -0.00002 \\\\ 0.00004 \\end{bmatrix}.\n     $$\n  3. 用例 3 (基准点处残差为零，中等步长):\n     $$\n     m = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}, \\quad\n     v = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ -0.05 \\end{bmatrix}.\n     $$\n\n对于每个用例，计算：\n- 如上定义的泰勒余项 $R(v)$。\n- 在 $m$ 处，由 $\\phi(m)$ 的精确海森矩阵中被忽略的 $F(m)$ 的二阶导数产生的主二次项，定义为\n$$\nB(v) = \\frac{1}{2} \\sum_{i=1}^{p} r_i(m) \\, v^\\top \\left( \\nabla^2 F_i(m) \\right) v,\n$$\n其中 $r_i(m)$ 是 $r(m)$ 的第 $i$ 个分量。\n- 相对差异\n$$\n\\varepsilon(v) = \\frac{\\lvert R(v) - B(v) \\rvert}{\\max\\left( \\lvert B(v) \\rvert, 10^{-12} \\right)}.\n$$\n\n您的程序应：\n- 为指定的 $F(m) = \\exp(A m)$ 实现 $F(m)$、$J(m)$ 和逐分量海森矩阵 $\\nabla^2 F_i(m)$。\n- 为三个测试用例中的每一个计算 $\\phi(m)$、$g$、$H_{GN}(m)$、$R(v)$、$B(v)$ 和 $\\varepsilon(v)$。\n- 生成单行输出，其中包含结果，格式为一个包含三个子列表（每个测试用例一个）的逗号分隔列表，每个子列表包含作为浮点数的 $R(v)$、$B(v)$ 和 $\\varepsilon(v)$。输出格式必须严格如下：\n$$\n\\left[ [R_1,B_1,\\varepsilon_1],[R_2,B_2,\\varepsilon_2],[R_3,B_3,\\varepsilon_3] \\right],\n$$\n其中每个浮点数必须以科学记数法书写，小数点后有十二位数字，并使用字母 $e$ 表示指数，且该行中任何地方都不能有空格。\n\n注意：\n- 所有计算都以纯数学术语进行；不涉及物理单位。\n- 此问题中不出现角度。\n- 预期输出为实数（浮点值）。", "solution": "该问题是有效的，因为它是数学上适定的，其科学基础根植于计算地球物理学和数值优化的原理，并为所需的计算提供了完整且一致的数据集和定义。\n\n以下是所需量和求解方法的逐步推导。\n\n### 1. 数学公式\n\n该问题围绕最小二乘目标函数 $\\phi(m) = \\frac{1}{2} \\lVert F(m) - d \\rVert_2^2$ 的泰勒级数展开。$\\phi(m+v)$ 在 $m$ 附近的展开式由下式给出：\n$$\n\\phi(m+v) = \\phi(m) + \\nabla\\phi(m)^\\top v + \\frac{1}{2} v^\\top \\nabla^2\\phi(m) v + O(\\lVert v \\rVert^3)\n$$\n需要梯度 $\\nabla\\phi(m)$ 和Hessian矩阵 $\\nabla^2\\phi(m)$。令 $r(m) = F(m) - d$。\n\n梯度由下式给出：\n$$\n\\nabla\\phi(m) = J(m)^\\top r(m) = g\n$$\n其中 $J(m)$ 是 $F(m)$ 的雅可比矩阵。\n\n精确Hessian矩阵由下式给出：\n$$\n\\nabla^2\\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^{p} r_i(m) \\nabla^2 F_i(m)\n$$\n其中 $\\nabla^2 F_i(m)$ 是正演算子 $F(m)$ 第 $i$ 个分量的Hessian矩阵。\n\nHessian矩阵的高斯-牛顿近似 $H_{GN}(m)$ 忽略了涉及 $F$ 的二阶导数的第二项：\n$$\nH_{GN}(m) = J(m)^\\top J(m)\n$$\n基于高斯-牛顿近似的 $\\phi(m+v)$ 的二次模型是：\n$$\n\\phi(m+v) \\approx \\phi(m) + g^\\top v + \\frac{1}{2} v^\\top H_{GN}(m) v\n$$\n问题将余项 $R(v)$ 定义为真实值 $\\phi(m+v)$ 与此二次模型之间的差：\n$$\nR(v) = \\phi(m + v) - \\left( \\phi(m) + g^\\top v + \\frac{1}{2} v^\\top H_{GN}(m) v \\right)\n$$\n通过将完整泰勒级数与 $R(v)$ 的定义进行比较，我们可以看到 $R(v)$ 捕获了所有高阶项，其主项是涉及Hessian矩阵被忽略部分的二次型：\n$$\nR(v) = \\frac{1}{2} v^\\top \\left( \\sum_{i=1}^{p} r_i(m) \\nabla^2 F_i(m) \\right) v + O(\\lVert v \\rVert^3)\n$$\n问题将量 $B(v)$ 定义为此主项：\n$$\nB(v) = \\frac{1}{2} \\sum_{i=1}^{p} r_i(m) v^\\top \\left( \\nabla^2 F_i(m) \\right) v\n$$\n因此，对于小的步长向量 $v$，我们期望 $R(v) \\approx B(v)$，并且差值 $R(v) - B(v)$ 是 $O(\\lVert v \\rVert^3)$ 阶的。\n\n### 2. 特定正演算子的导数\n\n正演算子是 $F(m) = \\exp(A m)$，其中指数运算是逐分量的。令 $u = Am$，则 $F_i(m) = \\exp(u_i) = \\exp(\\sum_j A_{ij} m_j)$。\n\n**雅可比矩阵 $J(m)$**：元素 $J_{ik}(m)$ 是 $\\frac{\\partial F_i}{\\partial m_k}$。\n$$\nJ_{ik}(m) = \\frac{\\partial}{\\partial m_k} \\exp\\left(\\sum_{j=1}^n A_{ij} m_j\\right) = \\exp\\left(\\sum_{j=1}^n A_{ij} m_j\\right) \\cdot A_{ik} = F_i(m) A_{ik}\n$$\n使用对角矩阵 $\\text{diag}(F(m))$，其矩阵形式为：\n$$\nJ(m) = \\text{diag}(F(m)) A\n$$\n\n**分量Hessian矩阵 $\\nabla^2 F_i(m)$**：元素 $(\\nabla^2 F_i(m))_{kl}$ 是 $\\frac{\\partial^2 F_i}{\\partial m_l \\partial m_k}$。\n$$\n(\\nabla^2 F_i(m))_{kl} = \\frac{\\partial}{\\partial m_l} (F_i(m) A_{ik}) = A_{ik} \\frac{\\partial F_i}{\\partial m_l} = A_{ik} (F_i(m) A_{il}) = F_i(m) A_{ik} A_{il}\n$$\n这是外积的第 $(k,l)$ 个元素。令 $a_{i \\cdot}$ 为 $A$ 的第 $i$ 行（作为一个 $1 \\times n$ 的行向量）。那么：\n$$\n\\nabla^2 F_i(m) = F_i(m) (a_{i \\cdot}^\\top a_{i \\cdot})\n$$\n\n### 3. 计算策略\n\n对于每个测试用例，我们执行以下计算：\n1.  **定义常量**：按规定设置矩阵 $A$、真实模型 $m_{\\mathrm{true}}$ 和测试用例 $(m, v)$。\n2.  **计算观测数据**：计算 $d = F(m_{\\mathrm{true}}) = \\exp(A m_{\\mathrm{true}})$。\n3.  **在基准点 $m$ 处求值**：\n    -   $F(m) = \\exp(Am)$\n    -   $r(m) = F(m) - d$\n    -   $\\phi(m) = \\frac{1}{2} r(m)^\\top r(m)$\n    -   $J(m) = \\text{diag}(F(m)) A$\n    -   $g = J(m)^\\top r(m)$\n    -   $H_{GN}(m) = J(m)^\\top J(m)$\n4.  **在扰动点 $m+v$ 处求值**：\n    -   $\\phi(m+v) = \\frac{1}{2} \\lVert \\exp(A(m+v)) - d \\rVert_2^2$\n5.  **计算 $R(v)$**：使用定义 $R(v) = \\phi(m + v) - \\phi(m) - g^\\top v - \\frac{1}{2} v^\\top H_{GN}(m) v$。\n6.  **计算 $B(v)$**：将分量Hessian矩阵的特定形式代入 $B(v)$ 的定义中。\n    $$\n    v^\\top (\\nabla^2 F_i(m)) v = v^\\top (F_i(m) a_{i \\cdot}^\\top a_{i \\cdot}) v = F_i(m) (v^\\top a_{i \\cdot}^\\top) (a_{i \\cdot} v) = F_i(m) (a_{i \\cdot} v)^2\n    $$\n    这将 $B(v)$ 的计算简化为：\n    $$\n    B(v) = \\frac{1}{2} \\sum_{i=1}^{p} r_i(m) F_i(m) (a_{i \\cdot} v)^2\n    $$\n    这种形式在计算上比显式构造每个Hessian矩阵更有效率。\n7.  **计算 $\\varepsilon(v)$**：使用公式 $\\varepsilon(v) = \\frac{\\lvert R(v) - B(v) \\rvert}{\\max\\left( \\lvert B(v) \\rvert, 10^{-12} \\right)}$，其中分母中的小数 $10^{-12}$ 是为了防止除以零，特别是对于用例3，其中 $r(m)=0$ 从而 $B(v)=0$。\n8.  **格式化输出**：根据严格指定的格式，将每个用例的最终结果 $[R(v), B(v), \\varepsilon(v)]$ 格式化为单行文本。\n\n下面的Python代码中实现了这种结构化方法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Taylor remainder R(v), the leading neglected term B(v),\n    and their relative discrepancy for three test cases in a nonlinear\n    least-squares problem.\n    \"\"\"\n    # --- Problem Definition ---\n\n    # Model and data dimensions\n    n = 3\n    p = 4\n\n    # Fixed matrix A for the forward operator F(m) = exp(A m)\n    A = np.array([\n        [0.5, -0.2, 0.3],\n        [-0.1, 0.4, 0.6],\n        [0.3, 0.1, -0.5],\n        [0.0, -0.3, 0.2],\n    ], dtype=np.float64)\n\n    # True model parameters\n    m_true = np.array([0.2, -0.1, 0.3], dtype=np.float64)\n\n    # Test cases: each is a tuple of (base model m, step v)\n    test_cases = [\n        (  # Case 1: moderate residual, moderate step\n            np.array([0.0, 0.0, 0.0], dtype=np.float64),\n            np.array([0.05, -0.02, 0.04], dtype=np.float64)\n        ),\n        (  # Case 2: moderate residual, very small step\n            np.array([0.0, 0.0, 0.0], dtype=np.float64),\n            np.array([0.00005, -0.00002, 0.00004], dtype=np.float64)\n        ),\n        (  # Case 3: zero residual at base point, moderate step\n            np.array([0.2, -0.1, 0.3], dtype=np.float64),\n            np.array([0.1, 0.1, -0.05], dtype=np.float64)\n        ),\n    ]\n\n    # --- Calculations ---\n\n    def F_op(m, A_mat):\n        \"\"\"Computes the forward operator F(m) = exp(A m).\"\"\"\n        return np.exp(A_mat @ m)\n\n    # Generate noise-free observed data d = F(m_true)\n    d = F_op(m_true, A)\n\n    all_results = []\n    for m, v in test_cases:\n        # --- Evaluate quantities at the base point m ---\n\n        # Forward operator, residual, and objective function\n        Fm = F_op(m, A)\n        r = Fm - d\n        phi_m = 0.5 * (r @ r)\n\n        # Jacobian J(m) = diag(F(m)) * A\n        Jm = np.diag(Fm) @ A\n\n        # Gradient g = J(m)^T * r(m)\n        g = Jm.T @ r\n\n        # Gauss-Newton Hessian H_GN(m) = J(m)^T * J(m)\n        H_GN = Jm.T @ Jm\n\n        # --- Evaluate objective function at the perturbed point m+v ---\n        F_m_plus_v = F_op(m + v, A)\n        r_m_plus_v = F_m_plus_v - d\n        phi_m_plus_v = 0.5 * (r_m_plus_v @ r_m_plus_v)\n\n        # --- Compute R(v) ---\n        # R(v) = phi(m+v) - (phi(m) + g^T*v + 0.5*v^T*H_GN*v)\n        Rv = phi_m_plus_v - phi_m - (g @ v) - 0.5 * (v.T @ H_GN @ v)\n\n        # --- Compute B(v) ---\n        # B(v) = 0.5 * sum_i r_i(m) * v^T * (nabla^2 F_i(m)) * v\n        # where v^T * (nabla^2 F_i(m)) * v = F_i(m) * (a_i . v)^2\n        Bv_sum = 0.0\n        Av = A @ v\n        for i in range(p):\n            Bv_sum += r[i] * Fm[i] * (Av[i] ** 2)\n        Bv = 0.5 * Bv_sum\n\n        # --- Compute relative discrepancy epsilon(v) ---\n        # epsilon(v) = |R(v) - B(v)| / max(|B(v)|, 1e-12)\n        epsilon_v = np.abs(Rv - Bv) / np.maximum(np.abs(Bv), 1e-12)\n\n        all_results.append((Rv, Bv, epsilon_v))\n\n    # --- Format final output string ---\n    sublist_strs = []\n    for R_val, B_val, e_val in all_results:\n        r_str = f\"{R_val:.12e}\"\n        b_str = f\"{B_val:.12e}\"\n        e_str = f\"{e_val:.12e}\"\n        sublist_strs.append(f\"[{r_str},{b_str},{e_str}]\")\n\n    final_output = f\"[{','.join(sublist_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3603122"}, {"introduction": "最后的练习将这些概念整合到一个实用的优化算法中：信赖域方法。我们将构建一个精确Hessian矩阵具有负曲率的情景，在这种情况下，朴素的牛顿法可能会失败。通过比较使用精确Hessian矩阵和始终为正半定的高斯-牛顿Hessian矩阵所计算出的步长 [@problem_id:3603124]，我们将展示高斯-牛顿法广受欢迎的一个关键原因：即使在高度非线性区域，它也能可靠地生成下降方向，从而增强算法的稳健性。", "problem": "要求您构建并分析一个在精确Hessian矩阵中表现出负曲率的单参数非线性最小二乘反演示例，并比较使用精确Hessian矩阵与高斯-牛顿近似计算出的信赖域步长。其基础是标准的非线性最小二乘目标函数和链式法则微积分。您必须实现一个程序，该程序针对一个指定的合成残差模型和一个小型测试套件，计算两种Hessian矩阵选择下的信赖域步长，并报告目标函数的预测减少量和实际减少量，以量化高斯-牛顿近似在强非线性区域中何时能避免有害的步长。\n\n反演设置如下。考虑一个标量模型参数 $m \\in \\mathbb{R}$。定义一个双分量残差向量 $r(m) = [r_1(m), r_2(m)]^\\top$，其中\n- $r_1(m) = a \\exp(-b m^2) - d$，\n- $r_2(m) = c m - e$，\n\n其中 $a$、$b$、$c$、$d$ 和 $e$ 是固定的正常数。目标函数为非线性最小二乘失配\n$$\n\\phi(m) = \\tfrac{1}{2} \\|r(m)\\|_2^2 = \\tfrac{1}{2}\\big(r_1(m)^2 + r_2(m)^2\\big).\n$$\n梯度由链式法则定义为\n$$\ng(m) = \\nabla \\phi(m) = J(m)^\\top r(m),\n$$\n其中 $J(m)$ 是 $r(m)$ 的Jacobian矩阵。精确Hessian矩阵为\n$$\nH(m) = \\nabla^2 \\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m),\n$$\n而高斯-牛顿 (GN) 近似使用\n$$\nH_{\\mathrm{GN}}(m) = J(m)^\\top J(m),\n$$\n也就是说，它忽略了二阶导数项 $\\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m)$。\n\n在迭代点 $m$ 处的信赖域 (TR) 步长 $p$ 被定义为一维二次子问题的解\n$$\n\\min_{p \\in \\mathbb{R}} \\; q(p) = g(m)\\,p + \\tfrac{1}{2} H_\\star(m)\\,p^2 \\quad \\text{约束条件为} \\quad |p| \\le \\Delta,\n$$\n其中 $H_\\star(m)$ 表示精确Hessian矩阵 $H(m)$ 或高斯-牛顿Hessian矩阵 $H_{\\mathrm{GN}}(m)$，且 $\\Delta > 0$ 是信赖域半径。对于一维问题，该子问题有一个初等的闭式解：如果无约束最小化子 $p^\\mathrm{N} = -g/H_\\star$ 位于区间 $[-\\Delta,\\Delta]$ 内且 $H_\\star > 0$，则 $p = p^\\mathrm{N}$；否则，$p$ 是 $\\{-\\Delta, +\\Delta\\}$ 中使 $q(p)$ 最小化的边界点。\n\n对于任何计算出的步长 $p$，定义：\n- 使用模型 $q(p)$ 的预测减少量为\n$$\n\\mathrm{pred} = -\\big(g(m)\\,p + \\tfrac{1}{2} H_\\star(m)\\,p^2\\big),\n$$\n- 使用真实目标函数的实际减少量为\n$$\n\\mathrm{ared} = \\phi(m) - \\phi(m + p),\n$$\n- 比率\n$$\n\\rho = \\frac{\\mathrm{ared}}{\\mathrm{pred}},\n$$\n约定如果分母为0，则将该比率视为0。\n\n您的程序必须仅使用上述定义中的标准微积分来实现以下内容：\n- 计算 $r_1(m)$ 和 $r_2(m)$，\n- 计算 $J(m)$、$g(m)$、$H(m)$ 和 $H_{\\mathrm{GN}}(m)$，\n- 对 $H_\\star \\in \\{H, H_{\\mathrm{GN}}\\}$ 这两种选择求解一维信赖域子问题，\n- 计算两种选择下的 $\\mathrm{pred}$、$\\mathrm{ared}$ 和 $\\rho$。\n\n使用以下固定参数和测试套件：\n- 常数：$a = 1.0$, $b = 5.0$, $c = 1.0$, $d = 0.1$, $e = 0.0$，\n- 测试用例为有序对 $(m, \\Delta)$：\n  - 用例1：$(m, \\Delta) = (0.1, 0.05)$，\n  - 用例2：$(m, \\Delta) = (0.1, 0.4)$，\n  - 用例3：$(m, \\Delta) = (0.1, 1.0)$，\n  - 用例4：$(m, \\Delta) = (0.3, 0.2)$。\n\n所有量纲均为无量纲。对于每个用例，您的程序必须按顺序输出一个包含六个实数的列表\n$$\n[p_H, \\; p_{\\mathrm{GN}}, \\; \\mathrm{ared}_H, \\; \\mathrm{ared}_{\\mathrm{GN}}, \\; \\rho_H, \\; \\rho_{\\mathrm{GN}}],\n$$\n其中 $p_H$ 和 $p_{\\mathrm{GN}}$ 分别是使用 $H$ 和 $H_{\\mathrm{GN}}$ 计算的信赖域步长。最终输出必须是包含这些列表的单行列表，每个测试用例一个列表，顺序与上文相同。每个实数必须四舍五入到六位小数。例如，语法模板为\n$$\n\\big[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\big].\n$$", "solution": "用户提供了一个关于计算地球物理学中非线性最小二乘反演的有效问题陈述。任务是分析一个单参数模型，比较使用精确Hessian矩阵与其高斯-牛顿近似计算的信赖域步长。此分析需要推导必要的数学量，实现信赖域步长计算，并通过目标函数的预测减少量和实际减少量来评估步长的质量。\n\n问题的核心在于标量参数 $m \\in \\mathbb{R}$ 的非线性最小二乘目标函数 $\\phi(m)$：\n$$\n\\phi(m) = \\frac{1}{2} \\|r(m)\\|_2^2 = \\frac{1}{2}\\left(r_1(m)^2 + r_2(m)^2\\right)\n$$\n其中残差分量由下式给出：\n$$\n\\begin{aligned}\nr_1(m) = a \\exp(-b m^2) - d \\\\\nr_2(m) = c m - e\n\\end{aligned}\n$$\n其中 $a$、$b$、$c$、$d$ 和 $e$ 是固定的正常数。\n\n为实现信赖域方法，我们必须首先计算目标函数 $\\phi(m)$ 的梯度 $g(m)$ 和Hessian矩阵 $H(m)$。梯度由链式法则给出：\n$$\ng(m) = \\nabla \\phi(m) = J(m)^\\top r(m)\n$$\n其中 $J(m)$ 是残差向量 $r(m)$ 的Jacobian矩阵。对于标量参数 $m$，Jacobian矩阵是一个 $2 \\times 1$ 的矩阵，其元素是残差分量对 $m$ 的导数：\n$$\nJ(m) = \\begin{pmatrix} \\frac{\\partial r_1}{\\partial m} \\\\ \\frac{\\partial r_2}{\\partial m} \\end{pmatrix} = \\begin{pmatrix} -2abm \\exp(-b m^2) \\\\ c \\end{pmatrix}\n$$\n我们将Jacobian矩阵的分量表示为 $J_1(m) = -2abm \\exp(-b m^2)$ 和 $J_2(m) = c$。梯度则是一个标量：\n$$\ng(m) = J_1(m) r_1(m) + J_2(m) r_2(m)\n$$\n\n精确Hessian矩阵 $H(m)$ 由下式给出：\n$$\nH(m) = \\nabla^2 \\phi(m) = J(m)^\\top J(m) + \\sum_{i=1}^2 r_i(m)\\,\\nabla^2 r_i(m)\n$$\n第一项 $J(m)^\\top J(m)$ 是Hessian矩阵的高斯-牛顿 (GN) 近似：\n$$\nH_{\\mathrm{GN}}(m) = J_1(m)^2 + J_2(m)^2 = \\left(-2abm \\exp(-b m^2)\\right)^2 + c^2 = 4a^2b^2m^2 \\exp(-2bm^2) + c^2\n$$\n注意，$H_{\\mathrm{GN}}(m)$ 总是非负的，如果 $c \\neq 0$，则为严格正。\n\n精确Hessian矩阵中的第二项需要残差的二阶导数：\n$$\n\\frac{d^2 r_1}{dm^2} = \\frac{d}{dm}\\left(-2abm \\exp(-b m^2)\\right) = -2ab \\exp(-b m^2) (1 - 2bm^2)\n$$\n$$\n\\frac{d^2 r_2}{dm^2} = \\frac{d}{dm}(c) = 0\n$$\n因此，精确Hessian矩阵为：\n$$\nH(m) = H_{\\mathrm{GN}}(m) + r_1(m) \\frac{d^2 r_1}{dm^2}\n$$\n第二项 $r_1(m) \\frac{d^2 r_1}{dm^2}$ 可以为负。如果该项为足够大的负值，整个Hessian矩阵 $H(m)$ 可能变为负值，这种情况称为负曲率。这表明目标函数是局部凹的，而作为正定矩阵的高斯-牛顿近似提供了一个很差的局部模型。\n\n信赖域 (TR) 步长 $p$ 是一维二次子问题的解：\n$$\n\\min_{p \\in \\mathbb{R}} \\; q(p) = g p + \\frac{1}{2} H_\\star p^2 \\quad \\text{约束条件为} \\quad |p| \\le \\Delta\n$$\n其中 $H_\\star$ 是 $H(m)$ 或 $H_{\\mathrm{GN}}(m)$，$\\Delta > 0$ 是信赖域半径。该子问题的求解策略取决于 $H_\\star$ 的符号。\n1.  如果 $H_\\star > 0$，则二次模型 $q(p)$ 是凸的。无约束最小化子为 $p^\\mathrm{N} = -g(m)/H_\\star$。如果 $|p^\\mathrm{N}| \\le \\Delta$，最优步长为 $p=p^\\mathrm{N}$。如果 $|p^\\mathrm{N}| > \\Delta$，步长位于信赖域边界上，$p = \\Delta \\cdot \\mathrm{sign}(p^\\mathrm{N}) = \\mathrm{copysign}(\\Delta, p^\\mathrm{N})$。\n2.  如果 $H_\\star \\le 0$，则二次模型是线性的或凹的。在区间 $[-\\Delta, \\Delta]$ 上的最小值必定出现在边界上。为了最小化 $q(p)$，我们选择使主导线性项 $g(m)p$ 最小（即最负）的边界点。因此，步长为 $p = -\\Delta \\cdot \\mathrm{sign}(g(m)) = \\mathrm{copysign}(\\Delta, -g(m))$。\n\n寻找步长 $p$ 的统一算法如下：\n- 如果 $H_\\star > 0$，计算无约束步长 $p^\\mathrm{N} = -g/H_\\star$。如果 $|p^\\mathrm{N}| \\le \\Delta$，则 $p = p^\\mathrm{N}$。\n- 否则（如果 $H_\\star \\le 0$ 或 $|p^\\mathrm{N}| > \\Delta$），步长在边界上：$p = \\mathrm{copysign}(\\Delta, -g)$。\n\n计算出步长 $p$ 后，我们使用两个指标来评估其质量：\n- 预测减少量，基于二次模型：$\\mathrm{pred} = -\\left(g(m)\\,p + \\frac{1}{2} H_\\star(m)\\,p^2\\right)$。\n- 实际减少量，基于真实目标函数：$\\mathrm{ared} = \\phi(m) - \\phi(m + p)$。\n\n比率 $\\rho = \\mathrm{ared}/\\mathrm{pred}$ 衡量了模型与实际函数之间的一致性。接近1的 $\\rho$ 值表示局部近似效果极好。\n\n对于每个测试用例 $(m, \\Delta)$，程序将执行以下步骤：\n1.  使用推导的公式和提供的常数，计算 $g(m)$、$H(m)$ 和 $H_{\\mathrm{GN}}(m)$ 的数值。\n2.  使用 $H_\\star = H(m)$ 求解TR子问题得到 $p_H$。\n3.  使用 $H_\\star = H_{\\mathrm{GN}}(m)$ 求解TR子问题得到 $p_{\\mathrm{GN}}$。\n4.  对于每一步（$p_H$ 和 $p_{\\mathrm{GN}}$），计算相应的 $\\mathrm{ared}$ 和 $\\rho$ 值。\n5.  收集六个结果值：$p_H, p_{\\mathrm{GN}}, \\mathrm{ared}_H, \\mathrm{ared}_{\\mathrm{GN}}, \\rho_H, \\rho_{\\mathrm{GN}}$。\n6.  按要求格式化结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes trust-region steps and performance metrics for a nonlinear\n    least-squares problem, comparing the exact Hessian vs. the Gauss-Newton\n    approximation.\n    \"\"\"\n\n    # --- Problem Definition ---\n    # Constants for the residual model\n    A, B, C, D, E = 1.0, 5.0, 1.0, 0.1, 0.0\n\n    # Test cases: tuples of (m, delta)\n    test_cases = [\n        (0.1, 0.05),\n        (0.1, 0.4),\n        (0.1, 1.0),\n        (0.3, 0.2),\n    ]\n\n    def r1(m):\n        \"\"\"Computes the first residual component.\"\"\"\n        return A * np.exp(-B * m**2) - D\n\n    def r2(m):\n        \"\"\"Computes the second residual component.\"\"\"\n        return C * m - E\n\n    def phi(m):\n        \"\"\"Computes the least-squares objective function.\"\"\"\n        return 0.5 * (r1(m)**2 + r2(m)**2)\n\n    def compute_derivatives(m):\n        \"\"\"\n        Computes the gradient (g), exact Hessian (h), and\n        Gauss-Newton Hessian (h_gn) at a given point m.\n        \"\"\"\n        r1_val = r1(m)\n        r2_val = r2(m)\n        \n        # First derivatives (Jacobian components)\n        exp_term = np.exp(-B * m**2)\n        J1 = -2 * A * B * m * exp_term\n        J2 = C\n        \n        # Gradient\n        g = J1 * r1_val + J2 * r2_val\n        \n        # Gauss-Newton Hessian\n        h_gn = J1**2 + J2**2\n        \n        # Second derivative of r1\n        d2r1_dm2 = -2 * A * B * exp_term * (1 - 2 * B * m**2)\n        # d2r2_dm2 is 0\n        \n        # Exact Hessian\n        h = h_gn + r1_val * d2r1_dm2\n        \n        return g, h, h_gn\n\n    def solve_tr_subproblem(g, h_star, delta):\n        \"\"\"\n        Solves the 1D trust-region subproblem for step p.\n        min_p q(p) = g*p + 0.5*h_star*p^2  s.t. |p| = delta\n        \"\"\"\n        if h_star > 0:\n            p_unconstrained = -g / h_star\n            if abs(p_unconstrained) = delta:\n                return p_unconstrained\n        \n        # Fall-through for boundary step:\n        # This case is reached if h_star = 0 (concave/linear model) or\n        # if h_star > 0 but the unconstrained step is outside the trust region.\n        # The step is taken along the steepest descent direction (-g) to the boundary.\n        return np.copysign(delta, -g)\n\n    def compute_metrics(m, p, g, h_star):\n        \"\"\"\n        Computes the actual and predicted reductions for a given step p.\n        \"\"\"\n        phi_m = phi(m)\n        phi_m_plus_p = phi(m + p)\n        \n        ared = phi_m - phi_m_plus_p\n        pred = -(g * p + 0.5 * h_star * p**2)\n        \n        # Per problem specification, rho = 0 if pred = 0.\n        if pred == 0:\n            rho = 0.0\n        else:\n            rho = ared / pred\n            \n        return ared, rho\n\n    all_results = []\n    for m, delta in test_cases:\n        g, h_exact, h_gn = compute_derivatives(m)\n        \n        # --- Exact Hessian (H) calculations ---\n        p_h = solve_tr_subproblem(g, h_exact, delta)\n        ared_h, rho_h = compute_metrics(m, p_h, g, h_exact)\n        \n        # --- Gauss-Newton (GN) calculations ---\n        p_gn = solve_tr_subproblem(g, h_gn, delta)\n        ared_gn, rho_gn = compute_metrics(m, p_gn, g, h_gn)\n        \n        case_results = [p_h, p_gn, ared_h, ared_gn, rho_h, rho_gn]\n        all_results.append(case_results)\n\n    # --- Format the output as a single-line list of lists ---\n    case_strings = []\n    for result_list in all_results:\n        # Format each number to six decimal places\n        num_strings = [f\"{val:.6f}\" for val in result_list]\n        case_strings.append(f\"[{', '.join(num_strings)}]\")\n    \n    final_output_string = f\"[{', '.join(case_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3603124"}]}