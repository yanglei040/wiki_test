## 引言
在[计算地球物理学](@entry_id:747618)领域，我们致力于通过分析地表或井中采集的间接测量数据来揭示地下世界的奥秘。这一过程，即[地球物理反演](@entry_id:749866)，其核心挑战在于许多反演问题本质上是“不适定的”（ill-posed）。这意味着即使数据中存在微小的噪声，也可能导致反演结果产生巨大的、不符合物理实际的偏差。为了克服这一根本性难题，研究者们引入了[正则化方法](@entry_id:150559)，通过融入关于地下结构的先验知识来约束解空间，从而获得稳定且有意义的模型。

本文聚焦于两种极其强大且广泛应用的[正则化技术](@entry_id:261393)：l1范数[稀疏正则化](@entry_id:755137)与全变分（Total Variation, TV）正则化。这些方法通过假设地下模型或其梯度具有“稀疏”或“块状”的特性，极大地提升了反演成像的质量。本文将系统地引导您深入理解这些前沿方法，内容组织为三个核心章节：

-   在“原理与机制”一章中，我们将深入剖析[不适定问题](@entry_id:182873)的数学根源，并从几何直观、解析机制到贝叶斯概率视角，全面揭示l1范数和[TV正则化](@entry_id:756242)促进[稀疏性](@entry_id:136793)与分段常数结构的基本原理。
-   接下来的“应用与[交叉](@entry_id:147634)学科联系”一章将展示这些理论在[重力反演](@entry_id:750042)、[多物理场](@entry_id:164478)[联合反演](@entry_id:750950)等实际地球物理问题中的应用，并探讨其与信号处理、压缩感知、统计学等领域的深刻联系。
-   最后，通过“动手实践”部分，您将有机会通过具体的计算问题，将理论知识转化为解决实际挑战的能力。

通过学习本文，您将掌握利用l1范数和[TV正则化](@entry_id:756242)解决复杂[地球物理反演](@entry_id:749866)问题的核心思想与关键技术。

## 原理与机制

在[计算地球物理学](@entry_id:747618)的许多领域，我们的目标是从间接且通常带有噪声的测量数据中恢复地下介质的物理属性。这个过程在数学上被构建为一个逆问题。正如前一章所述，这些问题往往是**不适定的 (ill-posed)**，这意味着解可能不存在、不唯一，或对数据中的微小扰动极其敏感。本章深入探讨了两种强有力的[正则化技术](@entry_id:261393)——$\ell_1$范数[稀疏正则化](@entry_id:755137)和全变分 (Total Variation, TV) 正则化——的原理与机制，它们通过引入先验知识来克服[不适定性](@entry_id:635673)，从而获得稳定且符合物理实际的解。

### [不适定问题](@entry_id:182873)的挑战：噪声放大

为了理解正则化的必要性，我们首先要分析为什么标准的反演方法，如[最小二乘法](@entry_id:137100)，在[不适定问题](@entry_id:182873)中会失效。考虑一个[线性逆问题](@entry_id:751313)，其模型可以表示为：

$$A x \approx b$$

其中，$x \in \mathbb{R}^{n}$ 是我们希望恢复的离散化地下模型参数（例如，速度或密度），$b \in \mathbb{R}^{m}$ 是观测数据，而 $A \in \mathbb{R}^{m \times n}$ 是描述物理过程的**正演算子 (forward operator)**。在实践中，数据总是被[噪声污染](@entry_id:188797)的，因此更精确的模型是 $b = A x_{\text{true}} + \varepsilon$，其中 $x_{\text{true}}$ 是真实模型，$\varepsilon$ 是测量噪声。

无正则化的最小二乘法旨在寻找一个模型 $\hat{x}$，使数据残差的欧几里得范数最小化，即 $\min_x \|Ax - b\|_{2}^{2}$。该问题的解由**[伪逆](@entry_id:140762) (pseudoinverse)** $A^{\dagger}$ 给出，即 $\hat{x}_{LS} = A^{\dagger} b$。

为了揭示其内在的不稳定性，我们使用**奇异值分解 (Singular Value Decomposition, SVD)** 来[分析算子](@entry_id:746429) $A$。任何矩阵 $A$ 都可以分解为 $A = U \Sigma V^{\top}$，其中 $U$ 和 $V$ 是正交矩阵，$\Sigma$ 是一个[对角矩阵](@entry_id:637782)，其对角线上的元素是**[奇异值](@entry_id:152907) (singular values)** $\sigma_{1} \ge \sigma_{2} \ge \dots \ge \sigma_{r} > 0$，其中 $r$ 是矩阵 $A$ 的秩。$U$ 的列向量 $u_i$ 称为[左奇异向量](@entry_id:751233)，构成了数据空间的一组基；$V$ 的列向量 $v_i$ 称为[右奇异向量](@entry_id:754365)，构成了模型空间的一组基。

利用SVD，[伪逆](@entry_id:140762)解可以表示为：

$$\hat{x}_{LS} = A^{\dagger} b = (V \Sigma^{\dagger} U^{\top}) (A x_{\text{true}} + \varepsilon) = V \Sigma^{\dagger} U^{\top} A x_{\text{true}} + V \Sigma^{\dagger} U^{\top} \varepsilon$$

将 $A = U \Sigma V^{\top}$ 代入，上式简化为：

$$\hat{x}_{LS} = (V \Sigma^{\dagger} \Sigma V^{\top}) x_{\text{true}} + \sum_{i=1}^{r} \frac{u_i^{\top} \varepsilon}{\sigma_i} v_i$$

解的误差主要来自于噪声项。我们可以看到，噪声向量 $\varepsilon$ 在第 $i$ 个[左奇异向量](@entry_id:751233) $u_i$ 上的投影 $(u_i^{\top} \varepsilon)$，在模型空间中被因子 $1/\sigma_i$ 放大，并沿着相应的[右奇异向量](@entry_id:754365) $v_i$ 的方向贡献给最终解 [@problem_id:3606214]。

在许多地球物理问题中，正演算子 $A$ 是**病态的 (ill-conditioned)**，这意味着它的[奇异值](@entry_id:152907)衰减得非常快，许多[奇异值](@entry_id:152907)非常接近于零。当一个奇异值 $\sigma_i$ 很小时，即使噪声分量 $u_i^{\top} \varepsilon$ 很小，其在解中的贡献也会被极大地放大，从而导致解中出现由噪声主导的、剧烈[振荡](@entry_id:267781)的伪影。如果 $A$ 是**[秩亏](@entry_id:754065)的 (rank-deficient)**，它甚至存在奇异值为零的情况，对应于模型的某些分量（即**[零空间](@entry_id:171336) (nullspace)**）对数据完全没有影响，使得解不唯一。这种对噪声的极端敏感性正是[逆问题](@entry_id:143129)的不稳定性所在，因此必须引入正则化来加以抑制 [@problem_id:3606222]。

### [稀疏性](@entry_id:136793)促进原理：$\ell_1$范数

正则化的核心思想是在最小化数据残差的同时，对解的某些属性施加惩罚。一个通用的正则化目标函数形式如下：

$$\min_{x} \frac{1}{2} \|A x - b\|_{2}^{2} + \lambda R(x)$$

其中，$R(x)$ 是**正则化项 (regularizer)**，而 $\lambda > 0$ 是**正则化参数 (regularization parameter)**，它权衡了[数据拟合](@entry_id:149007)与解的先验约束之间的重要性。

一种经典的[正则化方法](@entry_id:150559)是吉洪诺夫 (Tikhonov) 正则化，它使用解的 $\ell_2$ 范数的平方作为惩罚，即 $R(x) = \|x\|_{2}^{2} = \sum_i x_i^2$。这种方法倾向于产生模长较小且平滑的解。然而，在许多地球物理场景中，我们期望的地下结构是“简单”的，但不是全局平滑的。例如，[地震反射](@entry_id:754645)剖面可能由少数几个强的反射界面构成，而其他地方则很安静；或者，地层模型可能由几块具有恒定属性的“块体”组成。这些结构在数学上被称为**稀疏的 (sparse)** 或**可压缩的 (compressible)**。

**$\ell_1$范数**，$R(x) = \|x\|_{1} = \sum_i |x_i|$，是促进稀疏性的关键工具。与惩罚平方和的 $\ell_2$ 范数不同，$\ell_1$ 范数惩罚[绝对值](@entry_id:147688)之和。这种看似微小的差异导致了截然不同的结果。

#### 几何直观

理解 $\ell_1$ 范数为何能诱导稀疏性，最直观的方法是通过几何形状。考虑一个简单的二维情况，并比较 $\ell_1$ 和 $\ell_2$ 范数的[单位球](@entry_id:142558)，即所有满足 $\|x\| \le 1$ 的点的集合。

-   $\ell_2$ [单位球](@entry_id:142558) ($\{x \in \mathbb{R}^2 : \sqrt{x_1^2 + x_2^2} \le 1\}$) 是一个圆形。它的边界是平滑的，没有任何“角”。
-   $\ell_1$ 单位球 ($\{x \in \mathbb{R}^2 : |x_1| + |x_2| \le 1\}$) 是一个旋转了45度的正方形，其顶点位于坐标轴上 ($(1,0), (0,1), (-1,0), (0,-1)$)。它的边界在顶点处是“尖锐”的。

现在，考虑一个欠定问题，我们试图在满足约束 $Ax=y$ 的所有解中，寻找范数最小的那个。这等价于将范数球逐渐“吹大”，直到它第一次接触到由约束定义的超平面。

-   对于 $\ell_2$ 范数，由于其[单位球](@entry_id:142558)是光滑的，与一个通用的超平面相切的点通常位于球面的某个“普通”位置，该点的所有坐标分量都不为零。因此，$\ell_2$ 最小化倾向于产生**稠密 (dense)** 的解。
-   对于 $\ell_1$ 范数，当其[多面体](@entry_id:637910)单位球膨胀时，极有可能首先在某个顶点或边上接触到约束[超平面](@entry_id:268044)。这些顶点和边上的点，其坐标必然包含零值（例如，顶点 $(1,0)$ 的第二个分量为零）。因此，$\ell_1$ 最小化在几何上天然地倾向于选择那些坐标分量恰好为零的解，即稀疏解 [@problem_id:3606231]。

#### 解析机制

从解析的角度看，$\ell_1$ 范数的稀疏促进能力源于其在零点的不[可微性](@entry_id:140863)。一个函数在某点可微，意味着在该点存在一个唯一的梯度，指示了[函数增长](@entry_id:267648)最快的方向。而[绝对值函数](@entry_id:160606) $|t|$ 在 $t=0$ 处是不可微的。取而代之的是**次梯度 (subgradient)** 的概念。在 $t \neq 0$ 时，[次梯度](@entry_id:142710)是唯一的，等于 $\text{sgn}(t)$（即-1或1）。但在 $t=0$ 时，任何在 $[-1, 1]$ 区间内的值都是有效的次梯度。

对于 $\ell_1$ 正则化问题 $\min_{x} f(x) + \lambda \|x\|_1$，其[最优性条件](@entry_id:634091)（[KKT条件](@entry_id:185881)）要求在解 $\hat{x}$ 处，平滑项的负梯度 $-\nabla f(\hat{x})$ 必须落在 $\lambda \partial \|\hat{x}\|_1$ 这个集合中。对于解的第 $i$ 个分量 $\hat{x}_i$：

-   如果 $\hat{x}_i \neq 0$，则该分量上的[最优性条件](@entry_id:634091)是 $-(\nabla f(\hat{x}))_i = \lambda \cdot \text{sgn}(\hat{x}_i)$。这意味着梯度的分量必须达到一个精确的阈值。
-   如果 $\hat{x}_i = 0$，则条件变为 $|-(\nabla f(\hat{x}))_i| \le \lambda$。这意味着梯度分量只要落在 $[-\lambda, \lambda]$ 这个区间内，$\hat{x}_i=0$ 就是一个合法的解。

相比于必须精确达到某个值的条件，落在一个区间内的条件显然更容易满足。这个在零点处“扩大”了的[最优性条件](@entry_id:634091)，正是 $\ell_1$ 正则化能够将许多系数精确地“压缩”到零的解析根源 [@problem_id:3606231]。

### 从稀疏到结构：[全变分(TV)正则化](@entry_id:756067)

虽然模型本身的[稀疏性](@entry_id:136793)在某些应用（如[反射系数](@entry_id:194350)反演）中有用，但在其他情况下，我们期望模型是**分段常数 (piecewise-constant)** 的，例如表示具有清晰边界的不同地质单元的速度模型。这种模型本身不是稀疏的，但它的**梯度是稀疏的**。

**全变分 (Total Variation, TV)** 正则化正是利用了这一思想。它将 $\ell_1$ 范数的稀疏促进能力应用于模型的梯度，而非模型本身。一维信号 $x \in \mathbb{R}^n$ 的（各向异性）TV定义为其相邻元素差分的[绝对值](@entry_id:147688)之和：

$$\mathrm{TV}(x) = \sum_{i=1}^{n-1} |x_{i+1} - x_i|$$

这可以更紧凑地写成 $\mathrm{TV}(x) = \|Dx\|_1$，其中 $D$ 是一个**[一阶差分](@entry_id:275675)算子 (first-order difference operator)**。通过惩罚梯度的 $\ell_1$ 范数，[TV正则化](@entry_id:756242)鼓励解的梯度向量是稀疏的，即大部分差分项 $x_{i+1} - x_i$ 都等于零。这直接导致了解 $x$ 具有大段的平坦区域（常数段），仅在少数位置发生跳跃，从而形成了分段常数的结构 [@problem_id:3606258]。

一个经典的例子是TV去噪问题，其[目标函数](@entry_id:267263)为 $J(x) = \frac{1}{2}\|x - z\|_{2}^{2} + \lambda \mathrm{TV}(x)$，其中 $z$ 是带噪观测。对于一个只有两个点的简单信号 $x=(x_1, x_2)$，可以精确地证明，解的差值满足：

$$x_2 - x_1 = \text{sign}(z_2 - z_1) \max(|z_2 - z_1| - 2\lambda, 0)$$

这个表达式被称为**[软阈值算子](@entry_id:755010) (soft-thresholding operator)**。它表明，如果原始数据中的跳跃 $|z_2 - z_1|$ 小于阈值 $2\lambda$，那么在去噪后的解中，这个跳跃将被完全抑制为零 ($x_2 - x_1 = 0$)。这清晰地揭示了[TV正则化](@entry_id:756242)如何通过消除小的梯度来保留大的、显著的边缘，从而实现分段平滑的效果 [@problem_id:3606258]。

#### 高维度的TV公式化

在二维或三维中，TV的概念被自然地推广。对于一个在二维网格上离散化的模型 $x_{i,j}$，我们可以定义水平和垂直方向的[离散梯度](@entry_id:171970)。例如，使用[前向差分](@entry_id:173829)：

$$(\nabla_x x)_{i,j} = \frac{x_{i,j+1} - x_{i,j}}{\Delta x}, \quad (\nabla_y x)_{i,j} = \frac{x_{i+1,j} - x_{i,j}}{\Delta y}$$

其中 $\Delta x$ 和 $\Delta y$ 是网格间距。基于这些[离散梯度](@entry_id:171970)，可以定义两种主要的TV形式：

1.  **各向异性TV (Anisotropic TV)**: 它是梯度分量[绝对值](@entry_id:147688)之和，即 $\ell_1$ 范数：
    $$\mathrm{TV}_{\text{aniso}}(x) = \sum_{i,j} \left( |(\nabla_x x)_{i,j}| + |(\nabla_y x)_{i,j}| \right)$$
    这可以被看作是梯度向量的 $\ell_1$ 范数 $\sum_{i,j} \|\nabla x_{i,j}\|_1$。

2.  **各向同性TV (Isotropic TV)**: 它是每个点上梯度向量的[欧几里得范数](@entry_id:172687)（$\ell_2$ 范数）之和：
    $$\mathrm{TV}_{\text{iso}}(x) = \sum_{i,j} \sqrt{(\nabla_x x)_{i,j}^2 + (\nabla_y x)_{i,j}^2}$$
    这可以被看作是梯度向量的 $\ell_{2,1}$ 混合范数。

这两种形式的主要区别在于它们的**[旋转不变性](@entry_id:137644) (rotational invariance)**。各向同性TV，作为[梯度向量](@entry_id:141180)[欧几里得范数](@entry_id:172687)的积分（离散求和）的近似，在连续域中是完全旋转不变的。这意味着它对模型中边缘的方向没有偏好。相比之下，各向异性TV在连续域中不是旋转不变的；它倾向于产生与坐标轴对齐的边缘，因为对于一个对角线方向的梯度，其$\ell_1$范数要大于一个轴向对齐但长度相同的梯度。在离散网格上，这种差异依然存在，使得各向异性TV表现出与角度相关的偏置，而各向同性TV则（在[离散化误差](@entry_id:748522)范围内）更接近于旋转不变 [@problem_id:3606251]。

在实际应用中，[离散梯度](@entry_id:171970)算子的定义还需要考虑**边界条件 (boundary conditions)**。例如，在定义边界处的梯度时，我们需要假设模型在域外的行为。
- **狄利克雷 (Dirichlet) 边界条件**：假设模型在域外为某个固定值（如零）。这会在TV惩罚项中引入边界值本身，从而倾向于将解的边界值拉向该固定值。例如，在右边界，$(D_x x)_{N_x,j} = (0 - x_{N_x,j})/\Delta x = -x_{N_x,j}/\Delta x$。
- **诺伊曼 (Neumann) 边界条件**：假设模型在边界处的[法向导数](@entry_id:169511)为零，通常通过镜像延拓实现。这导致边界处的法向差分为零，例如，$(D_x x)_{N_x,j} = (x_{N_x,j} - x_{N_x,j})/\Delta x = 0$。因此，这种边界条件不会对边界值本身施加直接的惩罚。
选择哪种边界条件取决于具体的物理问题和先验假设 [@problem_id:3606218] [@problem_id:3606281]。

### 概率视角：贝叶斯最大后验估计

[正则化方法](@entry_id:150559)有一个深刻的概率解释，即**[最大后验概率](@entry_id:268939) (Maximum A Posteriori, MAP)** 估计。在贝叶斯框架下，我们不仅考虑数据告诉我们的信息（[似然](@entry_id:167119)），还考虑我们对解的[先验信念](@entry_id:264565)（先验）。根据[贝叶斯定理](@entry_id:151040)，后验概率为：

$$p(x|b) \propto p(b|x) p(x)$$

其中，$p(b|x)$ 是**似然函数 (likelihood function)**，$p(x)$ 是**[先验概率](@entry_id:275634)[分布](@entry_id:182848) (prior probability distribution)**。[MAP估计](@entry_id:751667)旨在寻找使后验概率最大化的模型 $x$。这等价于最小化负对数[后验概率](@entry_id:153467)：

$$\hat{x}_{\text{MAP}} = \arg\min_x [-\ln p(b|x) - \ln p(x)]$$

将这个表达式与正则化目标函数进行比较，我们发现：
-   [数据拟合](@entry_id:149007)项 $\frac{1}{2}\|Ax - b\|_2^2$ 对应于[负对数似然](@entry_id:637801)。具体来说，如果假设[测量噪声](@entry_id:275238) $\varepsilon$ 是[独立同分布](@entry_id:169067)的高斯噪声，即 $\varepsilon \sim \mathcal{N}(0, \sigma^2 I)$，那么[负对数似然](@entry_id:637801)恰好是 $\frac{1}{2\sigma^2}\|Ax - b\|_2^2$ (加上一个常数)。
-   正则化项 $R(x)$ 对应于负对数先验。

对于[TV正则化](@entry_id:756242)，如果假设模型的梯度分量 $u = Dx$ 是[独立同分布](@entry_id:169067)的，且服从**[拉普拉斯分布](@entry_id:266437) (Laplace distribution)** $p(u_i) \propto \exp(-|u_i|/b)$，那么负对数先验就是 $\frac{1}{b} \|Dx\|_1$。因此，[TV正则化](@entry_id:756242)等价于假设模型梯度服从拉普拉斯先验的[MAP估计](@entry_id:751667) [@problem_id:3606241]。

这个视角不仅为正则化提供了理论基础，还将正则化参数 $\lambda$ 与物理和统计量联系起来：$\lambda$ 正比于噪声[方差](@entry_id:200758) $\sigma^2$ 与[先验分布](@entry_id:141376)[尺度参数](@entry_id:268705) $b$ 的比值。它量化了我们对数据和先验信念的相对信任程度。值得注意的是，由于[梯度算子](@entry_id:275922) $D$ 的[零空间](@entry_id:171336)（常数向量）是非平凡的，这种对梯度施加的先验会导致对模型 $x$ 本身的**[非正常先验](@entry_id:166066) (improper prior)**，因为模型可以加上任意常数而不改变其[先验概率](@entry_id:275634)。这在理论分析和某些[算法设计](@entry_id:634229)中需要特别注意 [@problem_id:3606241]。

### 理论考量与唯一性

一个关键的理论问题是：[TV正则化](@entry_id:756242)问题的解是否唯一？一个凸[优化问题](@entry_id:266749)的解是唯一的，如果其目标函数是**严格凸 (strictly convex)** 的。

$$J(m) = \frac{1}{2}\|Am - d\|_{2}^{2} + \lambda \mathrm{TV}(m)$$

$J(m)$ 的两个组成部分都是凸函数，因此它们的和也是[凸函数](@entry_id:143075)，这保证了任何局部最小值都是[全局最小值](@entry_id:165977)。然而，它们不总是严格凸的。

-   数据拟合项 $\frac{1}{2}\|Am - d\|_{2}^{2}$ 是严格凸的，当且仅当其Hessian矩阵 $A^\top A$ 是正定的，这等价于 $A$ 具有[满列秩](@entry_id:749628)（即 $\text{null}(A) = \{0\}$）。
-   TV项 $\lambda\mathrm{TV}(m)$ 从来不是严格凸的，因为对于任何在[梯度算子](@entry_id:275922)零空间中的非零向量 $h \in \text{null}(D)$（例如，常数向量），我们总有 $\mathrm{TV}(m+h) = \mathrm{TV}(m)$。

因此，唯一性取决于这两部分是否能“互补”。
1.  如果 $A$ 具有[满列秩](@entry_id:749628)，那么[数据拟合](@entry_id:149007)项是严格凸的。一个严格凸函数与一个凸函数之和仍然是严格凸的。因此，当 $A$ [满列秩](@entry_id:749628)时，[TV正则化](@entry_id:756242)问题的解是唯一的 [@problem_id:3606270]。
2.  如果 $A$ 是[秩亏](@entry_id:754065)的，情况就变得复杂。如果 $A$ 的零空间与 $D$ 的[零空间](@entry_id:171336)存在非零交集，即 $\text{null}(A) \cap \text{null}(D) \neq \{0\}$，那么解就不是唯一的。例如，如果正演算子对模型的常数偏移不敏感（即 $A\mathbf{1}=0$），而 $D$ 的[零空间](@entry_id:171336)恰好是常数向量，那么若 $m^*$ 是一个解，则 $m^* + c\mathbf{1}$（其中 $c$ 是任意常数）也是解，因为加上 $c\mathbf{1}$ 不会改变[数据拟合](@entry_id:149007)项和TV项的值。这就导致了无穷多个解 [@problem_id:3606270]。

保证[解的唯一性](@entry_id:143619)对于结果的[可解释性](@entry_id:637759)和[可重复性](@entry_id:194541)至关重要。在实践中，即使理论上不唯一，[数值算法](@entry_id:752770)也可能收敛到某个特定的解（例如，[最小范数解](@entry_id:751996)），但理解这种潜在的不唯一性是进行可靠反演的前提。

### 算法机制简介

解决[TV正则化](@entry_id:756242)这类[非光滑优化](@entry_id:167581)问题需要专门的算法。这类问题被称为**复合[凸优化](@entry_id:137441) (composite convex optimization)**，因为[目标函数](@entry_id:267263)是一个光滑项与一个非光滑但结构简单（“近端友好”）的项之和。

两种主流的算法是**[近端梯度法](@entry_id:634891) (Proximal Gradient Method)** 和 **[交替方向乘子法](@entry_id:163024) (Alternating Direction Method of Multipliers, [ADMM](@entry_id:163024))**。

-   **[近端梯度法](@entry_id:634891)**：其迭代格式为 $$x^{k+1} = \text{prox}_{\tau \lambda R}(x^k - \tau \nabla f(x^k))$$。这一步分为两部分：一个标准的光滑项梯度下降步，和一个对结果应用**[近端算子](@entry_id:635396) (proximal operator)** 的步骤。对于[TV正则化](@entry_id:756242) $R(x)=\|Dx\|_1$，其[近端算子](@entry_id:635396)没有[闭式](@entry_id:271343)解，但可以被高效地分解为一个更简单的[对偶问题](@entry_id:177454)来求解 [@problem_id:3606267]。

-   **ADMM**：该方法通过引入一个分[裂变](@entry_id:261444)量 $z$ 来解耦问题。原问题被重写为 $\min_{x,z} f(x) + \lambda\|z\|_1$ s.t. $Dx - z = 0$。[ADMM](@entry_id:163024)通过[交替最小化](@entry_id:198823)关于 $x$ 和 $z$ 的增广[拉格朗日函数](@entry_id:174593)来求解。这导致了一系列更简单的子问题：
    1.  **$x$-更新**：求解一个形如 $(A^\top A + \rho D^\top D)x^{k+1} = \dots$ 的线性系统。值得注意的是，增广项 $\rho D^\top D$ 起到了对子问题的内部正则化作用，即使 $A^\top A$ 是病态或奇异的，它也能改善子问题的[条件数](@entry_id:145150)，使得求解更加稳定 [@problem_id:3606242]。
    2.  **$z$-更新**：这个子问题是一个关于 $z$ 的 $\ell_1$ 范数[近端算子](@entry_id:635396)，其解就是简单的**[软阈值](@entry_id:635249)**操作，计算非常快速。
    3.  **对偶变量更新**：一个简单的加法步骤。

ADMM的强大之处在于它将一个困难的非光滑问题分解为一系列（通常是）更容易求解的子问题（如[线性系统](@entry_id:147850)求解和[软阈值](@entry_id:635249)）。通过自适应地调整惩罚参数 $\rho$，可以进一步优化收敛性能 [@problem_id:3606242]。这些算法的出现，使得大规模的[TV正则化](@entry_id:756242)反演在计算上变得可行。