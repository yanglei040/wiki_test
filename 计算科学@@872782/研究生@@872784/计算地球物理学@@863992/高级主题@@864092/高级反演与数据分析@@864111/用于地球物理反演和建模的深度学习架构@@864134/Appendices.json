{"hands_on_practices": [{"introduction": "本练习旨在搭建起地球物理反演中经典正则化方法与深度学习模型之间的桥梁。通过分析一个一维反褶积问题，您将推导出经典吉洪诺夫（Tikhonov）正则化的解，并揭示其在特定统计假设下等价于最优线性维纳（Wiener）滤波器。更重要的是，该练习将展示这个最优滤波器如何能被诠释为一个简单的单层卷积神经网络，从而为理解和设计更复杂的数据驱动反演方法奠定坚实的理论基础 [@problem_id:3583423]。", "problem": "考虑一个在一维周期性网格上的循环卷积前向模型，网格长度为 $N \\in \\mathbb{N}$，其中数据 $\\mathbf{d} \\in \\mathbb{C}^{N}$ 由模型 $\\mathbf{m} \\in \\mathbb{C}^{N}$ 生成，关系为 $\\mathbf{d} = \\mathbf{F}\\mathbf{m} + \\mathbf{n}$，其中 $\\mathbf{F}$ 是由与核 $\\mathbf{h} \\in \\mathbb{C}^{N}$ 进行卷积所生成的循环矩阵，$\\mathbf{n}$ 是加性噪声。令 $\\mathbf{U} \\in \\mathbb{C}^{N \\times N}$ 表示酉离散傅里叶变换（DFT）矩阵，使得 $\\mathbf{U}^{*}\\mathbf{F}\\mathbf{U} = \\operatorname{diag}(\\hat{h}(k))$ 为对角矩阵，其对角元为 $\\hat{h}(k)$，即 $\\mathbf{h}$ 的DFT。考虑带有单位惩罚项的 Tikhonov 正则化，即最小化目标函数 $J(\\mathbf{m}) = \\|\\mathbf{F}\\mathbf{m} - \\mathbf{d}\\|_{2}^{2} + \\lambda \\|\\mathbf{m}\\|_{2}^{2}$，其中标量 $\\lambda > 0$。\n\n- 推导正规方程，并利用 DFT 对循环算子的对角化，以 $\\hat{h}(k)$、$\\hat{d}(k)$ 和 $\\lambda$ 表示傅里叶域中的解 $\\hat{m}(k)$。\n\n- 将核的功率谱定义为 $|\\hat{h}(k)|^{2}$。用 $|\\hat{h}(k)|^{2}$ 的极值和 $\\lambda$ 来推导正规矩阵 $\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I}$ 的条件数，并解释当 $\\lambda$ 变化时，较小的 $|\\hat{h}(k)|^{2}$ 值如何影响反演的稳定性。\n\n- 假设真实模型 $\\mathbf{m}_{\\mathrm{true}}$ 和噪声 $\\mathbf{n}$ 是相互独立、零均值、宽义平稳的白噪声，其功率谱为常数，分别为 $S_{m}(k) \\equiv \\sigma_{m}^{2} > 0$ 和 $S_{n}(k) \\equiv \\sigma_{n}^{2} > 0$。计算每个频率的期望均方误差 $\\mathbb{E}\\left[|\\hat{m}(k) - \\hat{m}_{\\mathrm{true}}(k)|^{2}\\right]$，将其表示为 $|\\hat{h}(k)|^{2}$ 和 $\\lambda$ 的函数，并确定使该误差在所有频率上的平均值最小的常数正则化参数 $\\lambda^{\\star}$。\n\n- 现在将 Tikhonov 正则化解释为卷积神经网络（CNN）的单个线性层，这是一个在数据域上作用的、具有单个线性卷积层的卷积神经网络（CNN）。约束该线性层具有频率响应 $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\varphi\\left(|\\hat{h}(k)|^{2}\\right)$，其中 $\\varphi:[0,\\infty)\\to\\mathbb{R}$ 是一个仅依赖于 $s = |\\hat{h}(k)|^{2}$ 的标量函数，$\\overline{\\hat{h}(k)}$ 表示复共轭。在对 $\\mathbf{m}_{\\mathrm{true}}$ 和 $\\mathbf{n}$ 相同的统计假设下，确定使估计量 $\\hat{m}(k) = \\hat{G}(k)\\hat{d}(k)$ 的期望均方误差最小化的函数 $\\varphi(s)$。将最终结果以单行矩阵的形式给出，即 $(\\lambda^{\\star}, \\varphi(s))$ 对。无需四舍五入，也无需单位。", "solution": "我们从 Tikhonov 目标函数 $J(\\mathbf{m}) = \\|\\mathbf{F}\\mathbf{m} - \\mathbf{d}\\|_{2}^{2} + \\lambda \\|\\mathbf{m}\\|_{2}^{2}$ 开始，对于 $\\lambda > 0$，这是一个严格凸的二次泛函。通过将梯度设为零，可得一阶最优性条件（正规方程），即\n$$(\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I})\\mathbf{m} = \\mathbf{F}^{*}\\mathbf{d}.$$\n利用酉离散傅里叶变换（DFT）矩阵 $\\mathbf{U}$ 对循环矩阵进行对角化，我们有 $\\mathbf{F} = \\mathbf{U}^{*}\\operatorname{diag}(\\hat{h}(k))\\mathbf{U}$，因此\n$$\\mathbf{F}^{*}\\mathbf{F} = \\mathbf{U}^{*}\\operatorname{diag}(|\\hat{h}(k)|^{2})\\mathbf{U}.$$\n用 $\\mathbf{U}$ 左乘正规方程，并定义 $\\hat{m} = \\mathbf{U}\\mathbf{m}$ 和 $\\hat{d} = \\mathbf{U}\\mathbf{d}$，可得\n$$\\left[\\operatorname{diag}(|\\hat{h}(k)|^{2}) + \\lambda \\mathbf{I}\\right]\\hat{m} = \\operatorname{diag}(\\overline{\\hat{h}(k)})\\hat{d}.$$\n这在频率 $k$ 上解耦，给出谱域解\n$$\\hat{m}(k) = \\frac{\\overline{\\hat{h}(k)}\\,\\hat{d}(k)}{|\\hat{h}(k)|^{2} + \\lambda}.$$\n\n接下来，我们分析条件数。正规矩阵为 $\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I} = \\mathbf{U}^{*}\\operatorname{diag}(|\\hat{h}(k)|^{2} + \\lambda)\\mathbf{U}$，因此其特征值为 $\\{|\\hat{h}(k)|^{2} + \\lambda\\}_{k}$。因此，2-范数条件数为\n$$\\kappa_{2}(\\mathbf{F}^{*}\\mathbf{F} + \\lambda \\mathbf{I}) = \\frac{\\max_{k}\\left(|\\hat{h}(k)|^{2}\\right) + \\lambda}{\\min_{k}\\left(|\\hat{h}(k)|^{2}\\right) + \\lambda}.$$\n当 $\\lambda$ 很小时，较小的 $|\\hat{h}(k)|^{2}$ 值会增​​大条件数，导致噪声放大和不稳定性。增加 $\\lambda$ 会将最小特征值提升 $\\lambda$，从而减小 $\\kappa_{2}$ 并提高稳定性，但代价是引入偏差。\n\n现在我们在假设的随机模型下计算期望均方误差。设真实模型 $\\mathbf{m}_{\\mathrm{true}}$ 和噪声 $\\mathbf{n}$ 是独立、零均值、白噪声，且具有恒定的功率谱 $S_{m}(k) \\equiv \\sigma_{m}^{2}$ 和 $S_{n}(k) \\equiv \\sigma_{n}^{2}$。在傅里叶域中，\n$$\\hat{d}(k) = \\hat{h}(k)\\,\\hat{m}_{\\mathrm{true}}(k) + \\hat{n}(k).$$\nTikhonov 估计量是\n$$\\hat{m}(k) = \\frac{\\overline{\\hat{h}(k)}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{d}(k) = \\frac{|\\hat{h}(k)|^{2}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{m}_{\\mathrm{true}}(k) + \\frac{\\overline{\\hat{h}(k)}}{|\\hat{h}(k)|^{2} + \\lambda}\\,\\hat{n}(k).$$\n为方便表示，定义 $s = |\\hat{h}(k)|^{2} \\geq 0$。频率 $k$ 处的估计误差为\n$$\\hat{e}(k) = \\hat{m}(k) - \\hat{m}_{\\mathrm{true}}(k) = -\\frac{\\lambda}{s + \\lambda}\\,\\hat{m}_{\\mathrm{true}}(k) + \\frac{\\overline{\\hat{h}(k)}}{s + \\lambda}\\,\\hat{n}(k).$$\n根据独立性和零均值假设，每个频率的期望均方误差为\n$$\\mathbb{E}\\left[|\\hat{e}(k)|^{2}\\right] = \\left(\\frac{\\lambda}{s + \\lambda}\\right)^{2}\\mathbb{E}\\left[|\\hat{m}_{\\mathrm{true}}(k)|^{2}\\right] + \\frac{|\\hat{h}(k)|^{2}}{(s + \\lambda)^{2}}\\mathbb{E}\\left[|\\hat{n}(k)|^{2}\\right] = \\frac{\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}}{(s + \\lambda)^{2}}.$$\n由于问题在 $k$ 上是可分的，该表达式对 $k$ 的平均值可通过最小化每个 $k$ 的项来最小化。对 $\\lambda$ 求导，\n$$\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}\\left[\\frac{\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}}{(s + \\lambda)^{2}}\\right] = \\frac{2(s + \\lambda)\\left[\\lambda \\sigma_{m}^{2}(s + \\lambda) - (\\lambda^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2})\\right]}{(s + \\lambda)^{4}} = \\frac{2s\\left(\\lambda \\sigma_{m}^{2} - \\sigma_{n}^{2}\\right)}{(s + \\lambda)^{3}}.$$\n将导数设为零并注意到 $s \\geq 0$，得到唯一的最小化子\n$$\\lambda^{\\star} = \\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}}.$$\n\n最后，将 Tikhonov 解释为作用于数据域的卷积神经网络（CNN）的单个线性层，其频率响应为 $\\hat{G}(k) = \\overline{\\hat{h}(k)}\\,\\varphi(s)$，其中 $s = |\\hat{h}(k)|^{2}$。估计量变为\n$$\\hat{m}(k) = \\hat{G}(k)\\hat{d}(k) = \\overline{\\hat{h}(k)}\\,\\varphi(s)\\left[\\hat{h}(k)\\hat{m}_{\\mathrm{true}}(k) + \\hat{n}(k)\\right] = s\\,\\varphi(s)\\,\\hat{m}_{\\mathrm{true}}(k) + \\overline{\\hat{h}(k)}\\,\\varphi(s)\\,\\hat{n}(k).$$\n误差为\n$$\\hat{e}(k) = \\left[s\\,\\varphi(s) - 1\\right]\\hat{m}_{\\mathrm{true}}(k) + \\overline{\\hat{h}(k)}\\,\\varphi(s)\\,\\hat{n}(k),$$\n其期望功率为\n$$\\mathbb{E}\\left[|\\hat{e}(k)|^{2}\\right] = \\left|s\\,\\varphi(s) - 1\\right|^{2}\\sigma_{m}^{2} + s\\,|\\varphi(s)|^{2}\\sigma_{n}^{2}.$$\n对于每个固定的 $s \\geq 0$，对实数 $\\varphi(s)$ 最小化该式（该表达式对于 $\\varphi$ 是凸的）得到一阶条件\n$$2\\left[s\\,\\varphi(s) - 1\\right]s\\,\\sigma_{m}^{2} + 2s\\,\\varphi(s)\\,\\sigma_{n}^{2} = 0,$$\n化简为\n$$\\varphi(s)\\left[s^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}\\right] = s\\,\\sigma_{m}^{2}.$$\n因此最优的标量函数是\n$$\\varphi(s) = \\frac{s\\sigma_{m}^{2}}{s^{2}\\sigma_{m}^{2} + s\\,\\sigma_{n}^{2}} = \\frac{\\sigma_{m}^{2}}{s\\sigma_{m}^{2} + \\sigma_{n}^{2}}.$$\n通过此选择，线性 CNN 实现了维纳滤波器 $\\hat{G}(k) = \\frac{\\overline{\\hat{h}(k)}\\sigma_{m}^{2}}{s\\sigma_{m}^{2} + \\sigma_{n}^{2}}$，当 $\\lambda = \\sigma_{n}^{2}/\\sigma_{m}^{2}$ 时，它与 Tikhonov 正则化一致。\n\n总之，最优正则化参数和最优 CNN 标量频率映射分别为 $\\lambda^{\\star} = \\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}}$ 和 $\\varphi(s) = \\frac{\\sigma_{m}^{2}}{s\\sigma_{m}^{2} + \\sigma_{n}^{2}}$。", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\sigma_{n}^{2}}{\\sigma_{m}^{2}} & \\frac{\\sigma_{m}^{2}}{s\\sigma_{m}^{2} + \\sigma_{n}^{2}}\\end{pmatrix}}$$", "id": "3583423"}, {"introduction": "在了解了单层网络的理论基础后，我们转向一种在地球物理成像中被广泛应用的深度多尺度架构——U-Net。此练习的核心是分析一个对网络设计至关重要的概念：感受野（receptive field）。通过从第一性原理出发，推导网络深度、卷积核大小等超参数如何共同决定U-Net的有效感受野，您将获得关于如何设计网络以捕捉特定尺度地质构造的实用见解 [@problem_id:3583499]。", "problem": "考虑一个U-Net风格的卷积神经网络 (CNN) 的一维变体，它沿着地震时间轴应用于将炮集输入映射到一维垂直地震速度剖面。假设采用以下架构和约定，这些在实践中是标准的，并且当偏移距不是限制因素时，对于垂直剖面分析是科学上合理的：\n\n- 网络严格沿时间一维，输入采样间隔为 $\\Delta t$ (单位：秒)。\n- 编码器有 $L$ 个分辨率降低阶段。在每个编码器阶段 $l \\in \\{1,\\dots,L\\}$，有两个卷积层，其卷积核大小为 $k$ (奇数，步长为 $1$，膨胀为 $1$，并选择零填充以保持长度)，然后是一个最大池化层，其池化核大小为 $2$，步长为 $2$。\n- 在瓶颈层（经过 $L$ 次池化后），有两个卷积层，其卷积核大小为 $k$ (步长为 $1$，膨胀为 $1$，相同填充)。\n- 解码器有 $L$ 个分辨率提升阶段。每个解码器阶段使用最近邻上采样（无核）进行因子为 $2$ 的上采样，与相应的编码器特征图（跳跃连接）进行拼接，然后应用两个卷积层，其卷积核大小为 $k$ (步长为 $1$，膨胀为 $1$，相同填充)。\n- 最后的 $1 \\times 1$ 卷积（卷积核大小为 $1$）将特征映射到输出。\n- 所有卷积都是线性和时不变的；没有大于 $1$ 的膨胀。\n\n您可以使用以下经过充分验证的事实：\n\n- 对于一个按前向顺序由 $i$ 索引的层序列，其中每一层的卷积核大小为 $k_i$，步长为 $s_i$，一个输出样本的理论感受野长度（以输入样本为单位）由基础值 $1$ 加上每一层的贡献给出，该贡献等于其卷积核贡献的增量跨度，再乘以其之前所有层的步长之积。\n- 一个卷积核大小为 $2$、步长为 $2$ 的最大池化层，其贡献的感受野增量等于当前的有效输入步长，并在此后将该步长加倍。\n- 因子为 $2$ 的最近邻上采样本身不扩展感受野，但它将传播到后续层的有效输入步长减半。\n\n假设背景速度 $v_0$ (单位：米/秒) 为常数，并采用近正入射单次散射（Born近似），因此双程走时 $t$ 和深度 $z$ 通过 $t = \\frac{2 z}{v_0}$ 相关联。在此映射下，深度波长为 $\\lambda$ 的正弦速度扰动对应于双程时间中周期为 $T = \\frac{2 \\lambda}{v_0}$ 的正弦波。为了让网络能从炮集输入中捕获该分量的至少一个完整周期，沿时间的理论感受野必须跨越至少一个周期，即 $R_t \\, \\Delta t \\geq T$，其中 $R_t$ 是沿时间的感受野（以输入样本为单位）。\n\n任务：\n\n- 根据上述U-Net架构，从第一性原理推导单个输出样本相对于输入的理论感受野 $R_t(L,k)$（以输入样本为单位）的显式封闭形式表达式。\n- 使用时间-深度关系 $t = \\frac{2 z}{v_0}$ 和单周期准则，将最大可恢复深度波长 $\\lambda_{\\max}$ (单位：米) 表示为关于 $L$, $k$, $\\Delta t$ 和 $v_0$ 的封闭形式解析函数。\n\n陈述您需要的任何额外最小假设。以米为单位表示最终波长。最终答案必须是单个封闭形式表达式。无需四舍五入。", "solution": "问题陈述已经过分析，并被确定为有效。它有科学依据、适定、内部一致，并包含推导唯一解所需的所有必要信息。语言精确，物理和计算模型是该领域使用的标准简化方法。\n\n对于带跳跃连接的U-Net架构中的感受野计算，需要一个最小假设：输出神经元的理论感受野由来自输入的最长影响路径决定。在指定的U-Net架构中，这对应于贯穿编码器、瓶颈层和解码器整个深度的路径。在每个解码器阶段，一个上采样的特征图与一个来自跳跃连接的特征图进行拼接。后续卷积层中一个神经元的感受野是它相对于两个拼接输入的感受野的并集。由于上采样路径更深，其感受野严格大于并包含跳跃连接路径的感受野。因此，总的感受野由追溯这条最深路径确定，这是标准的解释。\n\n第一个任务是推导理论感受野 $R_t(L,k)$（以输入样本为单位）的表达式。我们使用提供的感受野增长公式：总感受野是 $1$ 加上所有层的增量贡献之和。一个卷积核大小为 $k_i$ 的层 $i$ 的贡献是 $(k_i-1)$ 乘以其之前所有层的步长之积 $S_{i-1}$。\n$R_t = 1 + \\sum_{i} (k_i - 1) S_{i-1}$。\n\n让我们沿着网络的最长路径追踪感受野的增长和累积步长。\n\n$1$. **编码器**：编码器有 $L$ 个阶段。对于每个阶段 $l \\in \\{1, \\dots, L\\}$：\n阶段 $l$ 输入处的累积步长为 $S_{\\text{in},l} = 2^{l-1}$。\n每个阶段有两个卷积核大小为 $k$、步长为 $1$ 的卷积层，以及一个池化核大小为 $2$、步长为 $2$ 的最大池化层。\n- 第一个卷积层（步长为 $1$）贡献 $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$。\n- 第二个卷积层（步长为 $1$）贡献 $(k-1)S_{\\text{in},l} = (k-1)2^{l-1}$。\n- 最大池化层（步长为 $2$）贡献 $(2-1)S_{\\text{in},l} = 2^{l-1}$。\n编码器阶段 $l$ 的总贡献是 $\\Delta R_l^{\\text{enc}} = (k-1)2^{l-1} + (k-1)2^{l-1} + 2^{l-1} = (2k-1)2^{l-1}$。\n所有 $L$ 个编码器阶段的总贡献是对 $l$ 的求和：\n$$ \\Delta R_{\\text{enc}} = \\sum_{l=1}^{L} (2k-1)2^{l-1} = (2k-1) \\sum_{l=1}^{L} 2^{l-1} $$\n这是一个几何级数 $\\sum_{j=0}^{L-1} 2^j = \\frac{2^L-1}{2-1} = 2^L - 1$。\n$$ \\Delta R_{\\text{enc}} = (2k-1)(2^L - 1) $$\n经过 $L$ 个编码器阶段后，累积步长为 $2^L$。\n\n$2$. **瓶颈层**：瓶颈层在编码器之后。其输入处的累积步长是 $S_{\\text{bottle}} = 2^L$。\n它由两个卷积层组成，每个卷积核大小为 $k$，步长为 $1$。\n- 第一个卷积贡献 $(k-1)S_{\\text{bottle}} = (k-1)2^L$。\n- 第二个卷积贡献 $(k-1)S_{\\text{bottle}} = (k-1)2^L$。\n瓶颈层的总贡献是：\n$$ \\Delta R_{\\text{bottle}} = 2(k-1)2^L $$\n步长在通过瓶颈层时不变。\n\n$3$. **解码器**：解码器有 $L$ 个阶段。我们将其索引为 $l \\in \\{1, \\dots, L\\}$，从最接近瓶颈层的阶段到最接近输出的阶段。\n对于每个解码器阶段 $l$，有一个上采样步骤，后跟两个卷积层。\n进入解码器阶段 $l$ 的累积步长是 $2^{L-l+1}$。因子为 $2$ 的上采样将此步长减小为 $S_{\\text{dec},l} = \\frac{1}{2} \\cdot 2^{L-l+1} = 2^{L-l}$。此步长适用于后续的卷积。\n两个卷积层中的每一个都有卷积核大小 $k$ 和步长 $1$。\n解码器阶段 $l$ 的总贡献是：\n$\\Delta R_l^{\\text{dec}} = (k-1)S_{\\text{dec},l} + (k-1)S_{\\text{dec},l} = 2(k-1)2^{L-l}$。\n所有 $L$ 个解码器阶段的总贡献是：\n$$ \\Delta R_{\\text{dec}} = \\sum_{l=1}^{L} 2(k-1)2^{L-l} = 2(k-1) \\sum_{l=1}^{L} 2^{L-l} $$\n令 $j=L-l$。求和变为 $\\sum_{j=0}^{L-1} 2^j = 2^L - 1$。\n$$ \\Delta R_{\\text{dec}} = 2(k-1)(2^L - 1) $$\n\n$4$. **最终卷积**：应用一个最后的 $1 \\times 1$ 卷积（卷积核大小为 $1$，步长为 $1$）。进入该层的步长为 $1$。其对感受野的贡献是 $(1-1) \\cdot 1 = 0$。\n\n$5$. **总感受野**：总感受野 $R_t(L,k)$ 是初始值 $1$ 和所有贡献的总和：\n$$ R_t(L,k) = 1 + \\Delta R_{\\text{enc}} + \\Delta R_{\\text{bottle}} + \\Delta R_{\\text{dec}} $$\n$$ R_t(L,k) = 1 + (2k-1)(2^L-1) + 2(k-1)2^L + 2(k-1)(2^L-1) $$\n我们可以将含有 $(2^L-1)$ 的项分组：\n$$ R_t(L,k) = 1 + \\left( (2k-1) + 2(k-1) \\right)(2^L-1) + 2(k-1)2^L $$\n$$ R_t(L,k) = 1 + (2k-1+2k-2)(2^L-1) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 + (4k-3)(2^L-1) + (2k-2)2^L $$\n展开各项：\n$$ R_t(L,k) = 1 + (4k-3)2^L - (4k-3) + (2k-2)2^L $$\n$$ R_t(L,k) = 1 - 4k + 3 + (4k-3 + 2k-2)2^L $$\n$$ R_t(L,k) = 4 - 4k + (6k-5)2^L $$\n$$ R_t(L,k) = (6k-5)2^L - 4(k-1) $$\n这就是理论感受野的封闭形式表达式。\n\n第二个任务是找到最大可恢复深度波长 $\\lambda_{\\max}$。\n捕获时间周期为 $T$ 的特征的条件是，时间上的感受野 $R_t \\cdot \\Delta t$ 必须至少为一个周期：$R_t \\Delta t \\ge T$。\n网络能分辨的最大周期 $T_{\\max}$ 受其感受野的限制：\n$$ T_{\\max} = R_t(L,k) \\Delta t $$\n问题提供了双程走时周期 $T$ 和速度扰动的深度波长 $\\lambda$ 之间的物理关系：$T = \\frac{2\\lambda}{v_0}$。\n因此，最大可恢复波长 $\\lambda_{\\max}$ 对应于 $T_{\\max}$：\n$$ T_{\\max} = \\frac{2\\lambda_{\\max}}{v_0} $$\n将 $T_{\\max}$ 的两个表达式相等：\n$$ \\frac{2\\lambda_{\\max}}{v_0} = R_t(L,k) \\Delta t $$\n求解 $\\lambda_{\\max}$：\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} R_t(L,k) $$\n代入推导出的 $R_t(L,k)$ 表达式：\n$$ \\lambda_{\\max} = \\frac{v_0 \\Delta t}{2} \\left[ (6k-5)2^L - 4(k-1) \\right] $$\n该表达式也可以写成：\n$$ \\lambda_{\\max} = v_0 \\Delta t \\left[ (6k-5)2^{L-1} - 2(k-1) \\right] $$\n这就是最大可恢复深度波长的最终封闭形式解析函数。", "answer": "$$\\boxed{\\frac{v_0 \\Delta t}{2} \\left( (6k-5)2^L - 4(k-1) \\right)}$$", "id": "3583499"}, {"introduction": "最后的实践将探索一种更为前沿的架构范式：模仿并学习迭代优化过程的展开网络（unrolled network）。与直接的端到端映射不同，展开网络将梯度下降等经典算法的结构融入网络设计中，使其每层执行一次类优化的更新。本练习要求您首先从理论上推导这类网络层必须满足的下降和稳定性条件，然后通过数值实验验证这些理论，从而深入理解优化理论与可学习组件之间的相互作用 [@problem_id:3583447]。", "problem": "为全波形反演设计一个数学上精确的、展开的类梯度架构，其中每一层执行一步更新 $$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k),$$ 目标函数为 $$J(m) = \\lVert F(m) - d \\rVert_2^2 + \\lambda R(m),$$ 假设正演算子和正则化项使得 $J$ 的梯度在欧几里得范数下是全局利普希茨连续的，且利普希茨常数为 $L$。矩阵 $P_k$ 表示学习到的线性预条件子，标量 $\\alpha_k$ 是学习到的步长。使用具有 $L$-利普希茨梯度的函数的基本平滑性不等式、对称半正定矩阵的性质以及算子范数界作为您推导的基础。\n\n您必须执行以下操作。\n\n1) 从 $L$-平滑函数的平滑性不等式 $$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2$$ 出发，为更新 $$m^{+} = m - \\alpha P \\nabla J(m)$$ 推导一个充分下降条件，该条件表示为一个耦合了步长 $\\alpha$、学习到的矩阵 $P$ 和利普希茨常数 $L$ 的显式不等式。您的推导必须通过处理其对称部分 $$S \\equiv \\frac{P + P^\\top}{2}$$ 来处理 $P$ 不一定对称的一般情况，并且必须以一个可实现的界的形式结束：\n$$\\text{若 } \\mu \\equiv \\lambda_{\\min}(S) > 0 \\text{ 且 } M \\equiv \\lVert P \\rVert_2, \\text{ 则只要 } 0 < \\alpha < \\frac{2\\mu}{L M^2}，\\text{就有 } J(m^{+}) < J(m)。$$\n\n2) 为单层映射 $$T(m) \\equiv m - \\alpha P \\nabla J(m)$$ 提供一个利普希茨稳定性界。具体来说，推导 $T$ 在欧几里得范数下的利普希茨常数的一个充分上界，形式为 $$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2,$$ 并用 $\\alpha$、$L$ 和 $\\lVert P \\rVert_2$ 显式地表示 $K$。清楚地说明任何使该界为有限的额外充分条件，以及（如果可用）任何使 $T$ 在适当范数下成为非扩张或收缩映射的保守参数范围。\n\n3) 在一个二次代理上实现并评估该理论，该代理在线性化全波形反演与 Tikhonov 正则化中是标准的，即 $$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2,$$ 其中 $A \\in \\mathbb{R}^{n \\times n}$，$b \\in \\mathbb{R}^n$，$L \\in \\mathbb{R}^{n \\times n}$ 和 $\\lambda > 0$ 是固定的。对于此二次函数，其海森矩阵是常数，$$H = \\nabla^2 J(m) = 2\\left(A^\\top A + \\lambda L^\\top L\\right),$$ 因此 $\\nabla J$ 的全局利普希茨常数等于 $$L = \\lVert H \\rVert_2.$$\n\n对于数值评估，使用维度 $n = 3$ 和以下测试套件（这些选择是无量纲的，因此不需要物理单位）：\n\n- 所有情况的共享数据：\n  - $$A = \\begin{bmatrix} 1.0 & 0.2 & 0.0 \\\\ 0.2 & 1.0 & 0.3 \\\\ 0.0 & 0.3 & 1.5 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}, \\quad L = \\begin{bmatrix} 1.0 & 0.0 & 0.0 \\\\ 0.0 & 1.0 & -1.0 \\\\ 0.0 & 0.0 & 1.0 \\end{bmatrix}, \\quad \\lambda = 0.1,$$\n  - 初始模型 $$m^{0} = \\begin{bmatrix} -0.2 \\\\ 0.5 \\\\ 1.0 \\end{bmatrix}。$$\n- 四个学习到的层配置：\n  - 情况 1：$$P = \\operatorname{diag}(0.5, 1.0, 2.0), \\quad \\alpha = \\frac{1}{4} \\cdot \\frac{2 \\mu}{L M^2}。$$\n  - 情况 2：$P$ 与情况 1 相同，$$\\alpha = \\frac{2 \\mu}{L M^2}。$$\n  - 情况 3：$P$ 与情况 1 相同，$$\\alpha = 1.1 \\cdot \\frac{2 \\mu}{L M^2}。$$\n  - 情况 4：$$P = \\begin{bmatrix} 1.0 & 0.5 & 0.0 \\\\ 0.0 & 1.0 & 0.2 \\\\ 0.0 & 0.0 & 0.8 \\end{bmatrix}, \\quad \\alpha = \\frac{1}{2} \\cdot \\frac{2 \\mu}{L M^2},$$ 其中对于非对称的 $P$，$\\mu = \\lambda_{\\min}\\!\\left(\\frac{P + P^\\top}{2}\\right)$ 且 $M = \\lVert P \\rVert_2。$\n\n对于每种情况：\n  - 计算理论阈值 $$\\alpha_\\star = \\frac{2 \\mu}{L M^2}。$$\n  - 报告理论严格下降条件的布尔值，即 $$\\alpha < \\alpha_\\star \\text{ 且 } \\mu > 0。$$\n  - 执行一次更新 $$m^{1} = m^{0} - \\alpha P \\nabla J(m^{0})$$ 并报告实际严格减小的布尔值，即 $$J(m^{1}) < J(m^{0})。$$\n  - 报告保守利普希茨界 $$K_{\\mathrm{bound}} = 1 + \\alpha L \\lVert P \\rVert_2。$$\n  - 对于这个二次函数 $J$，单层映射是线性的：$$T(m) = \\left(I - \\alpha P H\\right)m + \\alpha P \\cdot \\mathrm{const},$$ 所以其精确的欧几里得利普希茨常数等于 $$\\lVert I - \\alpha P H \\rVert_2。$$ 报告此值。\n\n您的程序必须输出一行，其中包含连接成一个列表的所有按情况排列的结果，顺序完全如下\n$$[\\text{theory\\_descent}_1, \\text{actual\\_descent}_1, K_{\\mathrm{bound},1}, K_{\\mathrm{actual},1}, \\; \\ldots \\;, \\text{theory\\_descent}_4, \\text{actual\\_descent}_4, K_{\\mathrm{bound},4}, K_{\\mathrm{actual},4}],$$\n其中布尔值为小写，浮点数是四舍五入到 $6$ 位小数的十进制数。输出必须是单行，严格为一个用方括号括起来的逗号分隔列表，不得包含额外文本。", "solution": "该问题是有效的，因为它在科学上基于优化理论，具备所有必要的数据和条件，问题设定良好，并且是客观陈述的。它提出了一个对学习型优化算法进行分析的标准而严谨的练习。\n\n解答按要求分三部分进行。\n\n### 第 1 部分：充分下降条件的推导\n\n我们从具有 $L$-利普希茨连续梯度 $\\nabla J$ 的函数 $J(m)$ 的基本不等式开始，该不等式对其定义域中的任意两点 $x$ 和 $y$ 都成立：\n$$J(y) \\le J(x) + \\nabla J(x)^\\top (y-x) + \\frac{L}{2} \\lVert y-x \\rVert_2^2.$$\n我们关注的是预处理类梯度方法的单步更新，其形式为\n$$m^{k+1} = m^{k} - \\alpha_k P_k \\nabla J(m^k).$$\n为简化单步的符号表示，令 $m = m^k$，$m^{+} = m^{k+1}$，$\\alpha = \\alpha_k$ 以及 $P = P_k$。更新为 $m^{+} = m - \\alpha P \\nabla J(m)$。\n\n我们将 $x = m$ 和 $y = m^{+}$ 代入平滑性不等式：\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (m^{+} - m) + \\frac{L}{2} \\lVert m^{+} - m \\rVert_2^2.$$\n位移向量是 $m^{+} - m = -\\alpha P \\nabla J(m)$。将此表达式代入不等式，得到：\n$$J(m^{+}) \\le J(m) + \\nabla J(m)^\\top (-\\alpha P \\nabla J(m)) + \\frac{L}{2} \\lVert -\\alpha P \\nabla J(m) \\rVert_2^2.$$\n为简洁起见，令 $g \\equiv \\nabla J(m)$。不等式变为：\n$$J(m^{+}) \\le J(m) - \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2.$$\n为实现严格下降，我们需要 $J(m^{+}) < J(m)$。如果右侧最后两项之和严格为负，则可以保证这一点：\n$$- \\alpha g^\\top P g + \\frac{L \\alpha^2}{2} \\lVert P g \\rVert_2^2 < 0.$$\n假设步长 $\\alpha > 0$ 且我们不处于驻点（即 $g \\neq 0$），我们可以除以 $\\alpha$：\n$$- g^\\top P g + \\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < 0,$$\n可以重排为：\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top P g.$$\n项 $g^\\top P g$ 是一个二次型。由于预条件子 $P$ 不保证是对称的，该二次型与 $P$ 的特征值没有直接关系。然而，对于任意实矩阵 $P$ 和向量 $g$，该二次型由 $P$ 的对称部分决定。令 $S \\equiv \\frac{P + P^\\top}{2}$。则：\n$$g^\\top S g = g^\\top \\left(\\frac{P + P^\\top}{2}\\right) g = \\frac{1}{2} (g^\\top P g + g^\\top P^\\top g) = \\frac{1}{2} (g^\\top P g + (P g)^\\top g)。$$\n由于 $(P g)^\\top g$ 是一个标量，它等于其转置，即 $(P g)^\\top g = g^\\top P g$。因此，$g^\\top S g = g^\\top P g$。\n\n下降不等式可以使用 $S$ 重写为：\n$$\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top S g.$$\n为了建立一个仅依赖于 $P$ 的性质而不依赖于具体梯度 $g$ 的充分条件，我们使用给定的界。我们已知 $\\mu \\equiv \\lambda_{\\min}(S) > 0$，其中 $\\lambda_{\\min}(S)$ 是 $P$ 的对称部分的最小特征值。根据对称矩阵 $S$ 的瑞利商定理，我们有：\n$$g^\\top S g \\ge \\lambda_{\\min}(S) \\lVert g \\rVert_2^2 = \\mu \\lVert g \\rVert_2^2.$$\n我们还已知 $M \\equiv \\lVert P \\rVert_2$，即 $P$ 的谱范数。根据诱导矩阵范数的定义：\n$$\\lVert P g \\rVert_2 \\le \\lVert P \\rVert_2 \\lVert g \\rVert_2 = M \\lVert g \\rVert_2 \\implies \\lVert P g \\rVert_2^2 \\le M^2 \\lVert g \\rVert_2^2.$$\n为保证不等式 $\\frac{L \\alpha}{2} \\lVert P g \\rVert_2^2 < g^\\top S g$ 对任意 $g \\neq 0$ 成立，只需满足一个更保守的不等式，该不等式是通过将左边替换为一个上界，右边替换为一个下界得到的：\n$$\\frac{L \\alpha}{2} (M^2 \\lVert g \\rVert_2^2) < \\mu \\lVert g \\rVert_2^2.$$\n因为我们假设 $g \\neq 0$，所以可以除以正标量 $\\lVert g \\rVert_2^2$：\n$$\\frac{L \\alpha M^2}{2} < \\mu.$$\n解出 $\\alpha$，我们得到关于步长的充分条件：\n$$\\alpha < \\frac{2\\mu}{L M^2}.$$\n将此与 $\\alpha > 0$以及二次型的下界为正（即 $\\mu > 0$）的要求相结合，我们得出保证严格下降的最终条件：\n$$J(m^{+}) < J(m) \\text{ 只要 } \\mu > 0 \\text{ 且 } 0 < \\alpha < \\frac{2\\mu}{L M^2}。$$\n\n### 第 2 部分：利普希茨稳定性界\n\n我们的目标是为单层映射 $T(m) \\equiv m - \\alpha P \\nabla J(m)$ 在欧几里得范数下找到一个利普希茨常数 $K$。也就是说，我们寻求一个界 $K$，使得对于任意两点 $x$ 和 $y$：\n$$\\lVert T(x) - T(y) \\rVert_2 \\le K \\lVert x - y \\rVert_2.$$\n我们来分析差值 $T(x) - T(y)$：\n$$T(x) - T(y) = (x - \\alpha P \\nabla J(x)) - (y - \\alpha P \\nabla J(y)) = (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)).$$\n取 $L_2$-范数并应用三角不等式：\n$$\\lVert T(x) - T(y) \\rVert_2 = \\lVert (x - y) - \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2.$$\n我们使用矩阵和向量范数的性质来分析第二项：\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 = |\\alpha| \\cdot \\lVert P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot \\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2.$$\n梯度 $\\nabla J$ 是全局利普希茨连续的，常数为 $L$，这意味着 $\\lVert \\nabla J(x) - \\nabla J(y) \\rVert_2 \\le L \\lVert x - y \\rVert_2$。将此代入我们的表达式中：\n$$\\lVert \\alpha P (\\nabla J(x) - \\nabla J(y)) \\rVert_2 \\le |\\alpha| \\cdot \\lVert P \\rVert_2 \\cdot (L \\lVert x - y \\rVert_2).$$\n假设步长 $\\alpha > 0$ 为正，我们将其代回主不等式：\n$$\\lVert T(x) - T(y) \\rVert_2 \\le \\lVert x - y \\rVert_2 + \\alpha L \\lVert P \\rVert_2 \\lVert x - y \\rVert_2.$$\n提出因子 $\\lVert x-y \\rVert_2$：\n$$\\lVert T(x) - T(y) \\rVert_2 \\le (1 + \\alpha L \\lVert P \\rVert_2) \\lVert x - y \\rVert_2.$$\n这就建立了一个 $T$ 的利普希茨常数的充分上界，由下式给出\n$$K = 1 + \\alpha L \\lVert P \\rVert_2.$$\n只要步长 $\\alpha$、梯度的利普希茨常数 $L$ 和预条件子的范数 $\\lVert P \\rVert_2$ 都是有限的，这个界就是有限的。\n这个特定的界源于三角不等式，因此是保守的。由于 $\\alpha > 0$、$L > 0$ 且 $\\lVert P \\rVert_2 \\ge 0$，这个界 $K$ 总是大于或等于 $1$。因此，它不能用来证明映射 $T$ 是一个收缩映射（$K < 1$）。要证明收缩性，需要更紧密的分析，这通常与问题的结构有关。对于第 3 部分中 $J$ 是二次函数的特殊情况，映射 $T$ 变为仿射映射，其精确的利普希茨常数可以计算出来，并且确实可能小于 $1$。\n\n### 第 3 部分：数值评估设置\n\n对于数值评估，我们给定二次目标函数：\n$$J(m) = \\lVert A m - b \\rVert_2^2 + \\lambda \\lVert L m \\rVert_2^2.$$\n这可以展开为 $J(m) = (A m - b)^\\top(A m - b) + \\lambda (L m)^\\top(L m)$，简化为标准的二次型：\n$$J(m) = m^\\top A^\\top A m - 2b^\\top A m + b^\\top b + \\lambda m^\\top L^\\top L m = m^\\top(A^\\top A + \\lambda L^\\top L) m - 2 (A^\\top b)^\\top m + b^\\top b.$$\n梯度 $\\nabla J(m)$ 通过对 $m$ 求导得到：\n$$\\nabla J(m) = 2(A^\\top A + \\lambda L^\\top L)m - 2A^\\top b.$$\n海森矩阵 $\\nabla^2 J(m)$ 是梯度的导数：\n$$H \\equiv \\nabla^2 J(m) = 2(A^\\top A + \\lambda L^\\top L).$$\n由于海森矩阵 $H$ 是一个常数矩阵，梯度 $\\nabla J(m)$ 是 $m$ 的一个线性映射加上一个常数，这使其成为全局利普希茨的。$\\nabla J(m)$ 的利普希茨常数是其导数矩阵的算子范数，也就是 $H$。因此，全局利普希茨常数 $L$ 由下式给出：\n$$L = \\lVert H \\rVert_2 = \\lVert 2(A^\\top A + \\lambda L^\\top L) \\rVert_2.$$\n单层映射 $T(m)$ 成为一个仿射变换：\n$$T(m) = m - \\alpha P \\nabla J(m) = m - \\alpha P (H m - 2A^\\top b) = (I - \\alpha P H) m + 2\\alpha P A^\\top b.$$\n仿射映射 $f(x) = M x + c$ 的利普希茨常数是 $\\lVert M \\rVert_2$。因此，我们的映射 $T(m)$ 的精确利普希茨常数是：\n$$K_{\\mathrm{actual}} = \\lVert I - \\alpha P H \\rVert_2.$$\n下面的实现将使用这些公式为每种指定情况计算这些量。", "answer": "[true,true,1.139626,0.957635,false,true,1.558504,0.830541,false,false,1.614355,1.018903,true,true,1.380775,0.840742]", "id": "3583447"}]}