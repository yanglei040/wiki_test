## 引言
[基于梯度的优化](@entry_id:169228)方法是现代[计算地球物理学](@entry_id:747618)的支柱，为从地表观测中推断复杂地下结构的反演问题提供了数学框架。然而，实际的[地球物理反演](@entry_id:749866)问题往往涉及数百万个参数、海量数据和高度[非线性](@entry_id:637147)的物理过程，这使得寻找一个准确且符合地质规律的模型解成为一项巨大的挑战。本文旨在系统性地攻克这一难题。我们将首先在“原理与机制”一章中，深入剖析梯度计算的效率问题以及从最速下降法到先进[自适应算法](@entry_id:142170)的演进。随后，在“应用与[交叉](@entry_id:147634)学科联系”一章，我们将展示这些理论如何在[全波形反演](@entry_id:749622)、[联合反演](@entry_id:750950)等前沿应用中发挥作用，并揭示其与机器学习等领域的深刻联系。最后，通过“动手实践”部分，您将直面并解决实践中的关键技术细节。让我们从构建这些强大算法的基础——梯度计算的原理与机制——开始。

## 原理与机制

本章旨在系统性地阐述[基于梯度的优化](@entry_id:169228)方法的核心原理与基本机制。梯度优化是求解[地球物理反演](@entry_id:749866)问题的基石，它为我们提供了一套系统化的框架，用于迭代地改进模型参数，以最小化模型预测与观测数据之间的差异。我们将从最基本的概念——梯度——出发，深入探讨在[计算地球物理学](@entry_id:747618)的大规模问题中如何高效地计算梯度。随后，我们将系统地梳理一系列[优化算法](@entry_id:147840)，从经典的最速下降法到更先进的[动量法](@entry_id:177862)、[自适应学习率](@entry_id:634918)方法以及二阶方法。我们不仅会阐述这些方法的数学构造，更将重点揭示其背后的理论依据、收敛特性以及在实践中面临的挑战与权衡。

### 梯度的计算：效率与精度的权衡

在[基于梯度的优化](@entry_id:169228)中，第一步也是最关键的一步是计算目标函数 $J(m)$ 关于模型参数 $m \in \mathbb{R}^n$ 的梯度 $\nabla J(m)$。梯度向量指向[目标函数](@entry_id:267263)值增长最快的方向，因此其负方向 $-\nabla J(m)$ 便是**[最速下降](@entry_id:141858)方向**，为我们改进模型提供了最直接的指引。然而，在典型的[地球物理反演](@entry_id:749866)问题中，[目标函数](@entry_id:267263) $J(m)$ 通常是通过求解一个复杂的[偏微分方程](@entry_id:141332)（PDE）来与模型参数 $m$ 关联的，这使得梯度的计算远非易事。

#### 有限差分法：简单但昂贵

一个直观的计算梯度分量 $\frac{\partial J}{\partial m_i}$ 的方法是**有限差分法**（Finite Difference, FD）。例如，[前向差分](@entry_id:173829)格式定义为：
$$
\frac{\partial J}{\partial m_i} \approx \frac{J(m + h e_i) - J(m)}{h}
$$
其中 $e_i$ 是第 $i$ 个[标准基向量](@entry_id:152417)，$h$ 是一个很小的步长。这种方法的概念简单，易于实现。然而，它存在两个主要的缺点。

首先是**计算成本**。为了计算完整的[梯度向量](@entry_id:141180) $\nabla J(m)$，我们需要对 $n$ 个模型参数中的每一个都进行一次扰动，并重新计算目标函数。这意味着除了计算一次未扰动的 $J(m)$ 外，还需要额外计算 $n$ 次扰动后的[目标函数](@entry_id:267263)值。由于每次计算 $J$ 都需要求解一次正演PDE，计算整个梯度的总成本大约是 $n+1$ 次PDE求解。在现代[地球物理反演](@entry_id:749866)中，模型参数的数量 $n$ 可以轻易达到数百万甚至更多，这样的计算代价是无法承受的。尽管这 $n$ 次扰动计算是相互独立的，可以并行执行，但其总计算量仍然与 $n$ 成正比 [@problem_id:3600999]。

其次是**精度问题**。[有限差分近似](@entry_id:749375)引入了两种类型的误差。第一种是**截断误差**，它源于用[有限差分](@entry_id:167874)代替导数的数学近似。对于一个二阶连续可微的目标函数，泰勒展开显示[前向差分](@entry_id:173829)的截断误差是 $O(h)$ [@problem_id:3600999]。为了减小截断误差，我们希望 $h$ 尽可能小。然而，第二种误差——**舍入误差**——却限制了 $h$ 的下界。当 $h$ 非常小时，$J(m + h e_i)$ 和 $J(m)$ 的值会非常接近，它们的相减操作会受到[浮点数](@entry_id:173316)精度 $\varepsilon$ 的严重影响，导致舍入误差被 $h$ 放大，其量级为 $O(\varepsilon / h)$。总误差是这两者之和，为了最小化总误差，[最优步长](@entry_id:143372)的选择存在一个权衡，其量级约为 $h \sim \sqrt{\varepsilon}$。这使得在实践中选择一个合适的 $h$ 变得非常棘手 [@problem_id:3600999]。

#### 伴随状态法：高效且精确

为了克服有限差分法的局限性，[计算地球物理学](@entry_id:747618)广泛采用**伴随状态法**（Adjoint-State Method）。这种方法源于[约束优化](@entry_id:635027)的拉格朗日乘子理论，它允许我们以极高的效率计算[目标函数](@entry_id:267263)关于所有模型参数的精确梯度。

伴随状态法的核心思想是引入一个伴随变量（或称拉格朗日乘子）$\lambda$，并求解一个额外的**伴随方程**。对于一个受PDE约束的[优化问题](@entry_id:266749)，计算梯度的过程通常包括以下三步：
1.  给定当前模型 $m$，求解一次**正演PDE**，得到[状态变量](@entry_id:138790)（例如，波场）$u$。
2.  利用正演波场 $u$ 和观测数据 $d$ 计算残差，并将其作为源项，求解一次**伴随PDE**，得到伴随状态变量 $\lambda$。伴随PDE的算子通常是正演PDE算子的[共轭转置](@entry_id:147909)。
3.  通过对正演场 $u$ 和伴随场 $\lambda$ 进行互相关（通常是一个积分或[内积](@entry_id:158127)），一次性地计算出目标函数对**所有**模型参数的梯度 $\nabla J(m)$。

伴随状态法的惊人之处在于，其计算总成本主要由一次正演求解和一次伴随求解构成，总共只需两次PDE求解，这个成本**与模型参数的数量 $n$ 无关**。与有限差分法 $O(n)$ 的成本相比，其优势是压倒性的。此外，伴随法计算得到的是离散化问题下的**解析梯度**，它没有有限差分引入的截断误差，其精度仅受限于[PDE求解器](@entry_id:753289)的[数值精度](@entry_id:173145)和机器精度 $\varepsilon$ [@problem_id:3600999]。因此，伴随状态法已成为大规模[地球物理反演](@entry_id:749866)中计算梯度的标准方法。

### 一阶[优化方法](@entry_id:164468)

获得梯度后，我们便可以利用它来迭代地更新模型。一阶[优化方法](@entry_id:164468)仅利用目标函数的梯度信息来确定搜索方向。

#### [最速下降法](@entry_id:140448)与线搜索

最基本的一阶方法是**[最速下降法](@entry_id:140448)**（Steepest Descent）。其迭代格式为：
$$
m_{k+1} = m_k - \alpha_k \nabla J(m_k)
$$
其中 $m_k$ 是第 $k$ 次迭代的模型，$\nabla J(m_k)$ 是在 $m_k$ 处的梯度，而 $\alpha_k > 0$ 是**步长**（或称学习率），它决定了我们沿着负梯度方向前进的距离。

步长 $\alpha_k$ 的选择至关重要。一个理想的步长应该能使[目标函数](@entry_id:267263)得到充分的下降。**线搜索**（Line Search）就是一系列用于确定合适步长的策略。

一个重要的理论工具是目标函数的**[L-光滑性](@entry_id:635414)**（$L$-smoothness），即梯度 $\nabla J$ 满足 $L$-Lipschitz 连续性条件：
$$
\|\nabla J(x) - \nabla J(y)\| \le L \|x - y\|
$$
对于任意 $x, y$。这个性质意味着[函数的曲率](@entry_id:173664)有一个上界 $L$。它导出一个非常有用的不等式，称为**[下降引理](@entry_id:636345)**（Descent Lemma）：
$$
J(y) \le J(x) + \nabla J(x)^\top (y-x) + \frac{L}{2}\|y-x\|^2
$$
这个引理表明，函数 $J(y)$ 被一个以 $x$ 为中心的二次函数所[上界](@entry_id:274738)。利用这个性质，我们可以证明，如果选取一个固定的步长 $\alpha = 1/L$，[最速下降法](@entry_id:140448)能保证目标函数在每一步都下降 [@problem_id:3601038]：
$$
J(m_{k+1}) \le J(m_k) - \frac{1}{2L}\|\nabla J(m_k)\|^2
$$
这为选择步长提供了一个理论上的参考。然而，在实践中，估计 $L$ 的值可能很困难，而且固定的步长可能过于保守。

更实用的方法是执行一个动态的[线搜索](@entry_id:141607)。**[精确线搜索](@entry_id:170557)**（Exact Line Search）旨在找到[最优步长](@entry_id:143372) $\alpha_\star$，它能精确地最小化一维函数 $\phi(\alpha) = J(m_k - \alpha \nabla J(m_k))$。对于一些特殊情况，如目标函数是二次型函数时，$\alpha_\star$ 存在一个解析解，可以通过一次Hessian-[向量积](@entry_id:156672)来计算 [@problem_id:3601044]。

然而，在更一般的[非线性](@entry_id:637147)问题中，[精确线搜索](@entry_id:170557)的代价很高。因此，**[非精确线搜索](@entry_id:637270)**（Inexact Line Search）更为常用。**[Wolfe条件](@entry_id:171378)**是其中最流行的一套准则，它要求一个可接受的步长 $\alpha$ 必须同时满足两个条件：
1.  **[Armijo条件](@entry_id:169106)**（或称充分下降条件）：确保步长能带来目标函数的显著减小，避免步子太长。
    $$
    J(m_k + \alpha p_k) \le J(m_k) + c_1 \alpha \nabla J(m_k)^\top p_k
    $$
    其中 $p_k = -\nabla J(m_k)$ 是搜索方向，$c_1 \in (0, 1)$ 是一个很小的常数（例如 $10^{-4}$）。
2.  **曲率条件**：确保步长不会太小，使得梯度在新点处相比原点有显著变化。[强Wolfe条件](@entry_id:173436)要求：
    $$
    \left| \nabla J(m_k + \alpha p_k)^\top p_k \right| \le c_2 \left| \nabla J(m_k)^\top p_k \right|
    $$
    其中 $0  c_1  c_2  1$（例如 $c_2 = 0.9$）。

同时满足这两个条件的步长构成一个可接受的区间。例如，在一个二次型目标函数上，这个区间可以被精确地计算出来 [@problem_id:3601044]。[Wolfe条件](@entry_id:171378)通过几次[目标函数](@entry_id:267263)和梯度的计算就能找到一个“足够好”的步长，避免了[精确线搜索](@entry_id:170557)或Hessian计算的高昂代价，从而在理论保证和计算效率之间取得了良好平衡 [@problem_id:3601044]。

#### 最速下降法的[收敛速度](@entry_id:636873)与[条件数](@entry_id:145150)

[最速下降法](@entry_id:140448)的收敛速度严重依赖于目标函数的几何形态，这一形态可以通过其Hessian矩阵 $\nabla^2 J$ 的**条件数** $\kappa$ 来量化。对于一个正定的Hessian矩阵 $H$，其[特征值](@entry_id:154894)[谱分布](@entry_id:158779)在 $[\mu, L]$ 区间内，其中 $0  \mu \le L$ 分别是最小和最大[特征值](@entry_id:154894)。[条件数](@entry_id:145150)定义为 $\kappa = L/\mu$。

在一个二次型[目标函数](@entry_id:267263)上，最速下降法（使用[精确线搜索](@entry_id:170557)）的收敛是**[线性收敛](@entry_id:163614)**，即每一步的误差（以[能量范数](@entry_id:274966)衡量）会乘以一个小于1的常数因子 $q$。这个收敛因子由条件数决定 [@problem_id:3601010]：
$$
q = \frac{\kappa - 1}{\kappa + 1}
$$
当 $\kappa=1$ 时，目标函数的等值线是完美的圆形，[最速下降法](@entry_id:140448)一步即可到达最小值。但当 $\kappa \gg 1$ 时，等值线呈狭长的椭球状，此时 $q \to 1$，收敛变得极其缓慢，算法会在狭长的山谷中呈现典型的“之”字形蹒跚前进。达到一定精度所需的迭代次数正比于 $\mathcal{O}(\kappa)$。

在[地球物理反演](@entry_id:749866)中，**病态问题**（ill-conditioned problem）非常普遍，即Hessian矩阵的条件数 $\kappa$ 巨大。这主要源于以下几个方面 [@problem_id:3601010]：
*   **数据不完备**：地震勘探中有限的炮点和检波器覆盖（有限的采集孔径）以及震源子波的频带限制，导致数据对地下介质的某些部分或某些特征（如深部结构、高[波数](@entry_id:172452)成分）不敏感。这些“照明不足”的区域对应于目标函数中的平坦方向，即Hessian矩阵的极小[特征值](@entry_id:154894) $\mu \approx 0$，从而导致巨大的 $\kappa$。
*   **参数尺度差异**：当同时反演具有不同物理单位和量纲的多种参数时（例如，密度和纵波慢度），如果不对参数进行适当的缩放或[预处理](@entry_id:141204)，Hessian矩阵对角线元素的值可能会有[数量级](@entry_id:264888)的差异，这同样会扩大[特征值](@entry_id:154894)的[分布](@entry_id:182848)范围，增大 $\kappa$。
*   **正则化的作用**：正则化是处理病态问题的关键。例如，二次正则化项 $\frac{\lambda}{2} \|Rm\|^2$ 会给Hessian增加一个[半正定矩阵](@entry_id:155134) $\lambda R^\top R$。这个正则化项在数据不敏感的方向上（即 $H_{misfit}$ 的零空间或[近零空间](@entry_id:752382)中）提供了曲率，有效地将[最小特征值](@entry_id:177333) $\mu$ 从接近零的位置抬高，从而显著**减小**[条件数](@entry_id:145150) $\kappa$，加速收敛。

#### 加速收敛：[动量法](@entry_id:177862)

为了克服最速下降法的缓慢收敛，研究者们引入了**动量**（Momentum）的概念，旨在让迭代过程“记住”之前的更新方向，从而在狭长的山谷中抑制[振荡](@entry_id:267781)，加速前进。

**[重球法](@entry_id:637899)**（Heavy-ball method），由 Boris Polyak 提出，是最早的[动量法](@entry_id:177862)之一。它在梯度更新项之外，额外增加了一项与上一步更新量成正比的“动量项”：
$$
m_{k+1} = m_k - \alpha \nabla J(m_k) + \beta (m_k - m_{k-1})
$$
其中 $\beta \in [0, 1)$ 是动量参数。这个更新可以被看作一个在目标函数表面上滚动的“重球”，它会积累速度，从而更快地穿过平坦区域。对于强凸二次型函数，通过精心调节 $\alpha$ 和 $\beta$（依赖于 $\mu$ 和 $L$），[重球法](@entry_id:637899)可以实现比[最速下降法](@entry_id:140448)更快的[线性收敛](@entry_id:163614)速度。然而，对于一般的凸函数，它缺乏严格的[收敛速度](@entry_id:636873)保证，且对参数选择敏感，可能出现[振荡](@entry_id:267781)甚至不收敛 [@problem_id:3601011]。

**[Nesterov加速](@entry_id:752419)梯度法**（Nesterov's Accelerated Gradient, NAG）是一种更巧妙的[动量法](@entry_id:177862)。其关键思想是“先走一步再看梯度”。它首先根据之前的动量方向，计算一个“展望点”（lookahead point）$y_k$，然后在这个展望点上计算梯度并进行更新。其一种常见的更新形式为 [@problem_id:3601011]：
$$
\begin{align*}
y_k = m_k + \beta_k (m_k - m_{k-1}) \\
m_{k+1} = y_k - \alpha \nabla J(y_k)
\end{align*}
$$
NAG的神奇之处在于，对于一般的L-光滑凸函数，它能达到 $O(1/k^2)$ 的收敛速度，这在理论上被证明是一阶方法能达到的最优[收敛率](@entry_id:146534)。这比最速下降法 $O(1/k)$ 的[收敛率](@entry_id:146534)有了显著的提升。需要注意的是，NAG通常不是一个下降算法，即目标函数值 $J(m_k)$ 不一定是单调递减的。它的“超前”行为可能导致暂时的函数值上升，但总体上能更快地逼近最小值 [@problem_id:3601011]。

#### 处理海量数据：[随机梯度下降](@entry_id:139134)

在许多地球物理问题（如[全波形反演](@entry_id:749622)）中，[目标函数](@entry_id:267263)是所有炮集（shot records）数据拟合误差的总和：
$$
J(m) = \frac{1}{N} \sum_{i=1}^{N} \ell_i(m)
$$
其中 $\ell_i(m)$ 是第 $i$ 炮的损失函数，$N$ 是总炮数。计算完整梯度需要处理所有 $N$ 炮数据，当 $N$ 很大时，每一步迭代的成本都非常高。

**[随机梯度下降](@entry_id:139134)**（Stochastic Gradient Descent, SGD）通过在每次迭代中只使用一炮或一小批（mini-batch）炮的数据来估计梯度，从而极大地降低了单次迭代的成本。设 $g_k = \nabla \ell_{i_k}(m_k)$ 是在第 $k$ 步随机选取的第 $i_k$ 炮的梯度，SGD的更新规则为：
$$
m_{k+1} = m_k - \alpha_k g_k
$$
随机梯度 $g_k$ 是真实梯度 $\nabla J(m_k)$ 的一个有噪声的估计。它的关键性质是**无偏性**，即其期望等于真实梯度 $\mathbb{E}[g_k] = \nabla J(m_k)$。为了保证SGD收敛，步长序列 $(\alpha_k)$ 必须满足著名的**[Robbins-Monro条件](@entry_id:634006)** [@problem_id:3601030]：
$$
\sum_{k=0}^{\infty} \alpha_k = \infty \quad \text{and} \quad \sum_{k=0}^{\infty} \alpha_k^2  \infty
$$
第一个条件 $\sum \alpha_k = \infty$ 保证了算法有足够的能力跨越任意距离到达最小值，不会过早“停滞”。第二个条件 $\sum \alpha_k^2  \infty$ 保证了步长最终会衰减到零，从而能够抑制随机梯度带来的噪声，使得迭代[序列收敛](@entry_id:143579)到最优解，而不是在一个“噪声球”内[振荡](@entry_id:267781)。像 $\alpha_k = a/(b+k)$ 这样的步长序列就满足这些条件。相比之下，使用一个固定的步长 $\alpha$ 会导致算法在最优解附近的一个邻域内持续[振荡](@entry_id:267781)，无法精确收敛 [@problem_id:3601030]。

#### [自适应学习率](@entry_id:634918)方法

在SGD和[最速下降法](@entry_id:140448)中，所有参数都共享同一个学习率。然而，在地球物理模型中，不同位置或不同类型的参数可能具有非常不同的尺度和敏感度。**[自适应学习率](@entry_id:634918)方法**为每个参数独立地调整学习率，从而实现更高效的优化。

**RMSProp** (Root Mean Square Propagation) 是一种流行的[自适应学习率](@entry_id:634918)方法。它维护一个梯度平方的指数移动平均值 $G_k$，并用它来对学习率进行缩放 [@problem_id:3601034]：
$$
\begin{align*}
G_k = \rho G_{k-1} + (1-\rho) (g_k \odot g_k) \\
m_{k+1} = m_k - \frac{\alpha}{\sqrt{G_k} + \epsilon} \odot g_k
\end{align*}
$$
其中 $\odot$ 表示逐元素乘积，$\rho$ 是衰减率，$\epsilon$ 是为了防止分母为零的微小稳定项。其思想是：如果某个参数的梯度一直很大，那么 $G_k$ 中对应元素也会很大，从而使得该参数的有效[学习率](@entry_id:140210)减小；反之，梯度小的参数会获得更大的有效学习率。

**Adam** (Adaptive Moment Estimation) 是另一种更复杂的自适应方法，它结合了RMSProp的[自适应学习率](@entry_id:634918)和动量的思想。Adam同时维护梯度的一阶矩（均值，即动量）$m_k$ 和二阶矩（未中心化的[方差](@entry_id:200758)）$v_k$ 的指数移动平均 [@problem_id:3601034]：
$$
\begin{align*}
m_k = \beta_1 m_{k-1} + (1-\beta_1) g_k \\
v_k = \beta_2 v_{k-1} + (1-\beta_2)(g_k \odot g_k)
\end{align*}
$$
由于 $m_k$ 和 $v_k$ 初始化为零，它们在早期迭代中会偏向于零。Adam通过**偏差修正**来解决这个问题：
$$
\hat{m}_k = \frac{m_k}{1-\beta_1^k}, \quad \hat{v}_k = \frac{v_k}{1-\beta_2^k}
$$
最终的更新规则为：
$$
m_{k+1} = m_k - \alpha \frac{\hat{m}_k}{\sqrt{\hat{v}_k} + \epsilon}
$$
尽管Adam在实践中（尤其是在深度学习中）非常成功，但其理论性质比较复杂。在确定性（非随机）优化设置下，原始的Adam算法甚至被证明在某些简单的凸问题上不收敛。其主要问题在于[自适应学习率](@entry_id:634918)可能不会单调递减。后续改进如 **AMSGrad** 通过确保分母项非递减，修复了这一收敛性问题 [@problem_id:3601034]。在[地球物理反演](@entry_id:749866)的确定性优化框架中，一种稳健的做法是将这些自适应方法生成的[缩放矩阵](@entry_id:188350)视为一个**预条件子**，并结合传统的[线搜索](@entry_id:141607)（如Armijo线搜索）来保证算法的稳定下降和收敛性 [@problem_id:3601034]。

### 二阶与拟牛顿方法

二阶方法不仅使用梯度信息，还利用了目标函数的[二阶导数](@entry_id:144508)（Hessian矩阵），从而包含了关于函数曲率的更丰富信息。

#### [牛顿法](@entry_id:140116)

**[牛顿法](@entry_id:140116)**（Newton's Method）通过在当前点 $m_k$ 处用一个二次函数来近似[目标函数](@entry_id:267263) $J(m)$，并直接跳到这个二次模型的[最小值点](@entry_id:634980)。这个二次模型由 $J(m)$ 在 $m_k$ 处的二阶泰勒展开给出：
$$
J(m_k + \Delta m) \approx J(m_k) + \nabla J(m_k)^\top \Delta m + \frac{1}{2} \Delta m^\top \nabla^2 J(m_k) \Delta m
$$
最小化这个关于步长 $\Delta m$ 的二次函数，我们得到**牛顿系统** [@problem_id:3601006]：
$$
\nabla^2 J(m_k) \Delta m = - \nabla J(m_k)
$$
解出 $\Delta m$ 即为[牛顿步](@entry_id:177069)，更新规则为 $m_{k+1} = m_k + \Delta m$。

牛顿法的主要优点是其极快的**局部二次收敛速度**。一旦迭代点进入到最优解 $m^*$ 的一个足够小的邻域内，每一步迭代的误差大约是上一步误差的平方。这种快速收敛的前提是，在 $m^*$ 附近，Hessian矩阵 $\nabla^2 J(m^*)$ 是[对称正定](@entry_id:145886)的（SPD）。一个正定的Hessian确保了二次模型是严格凸的，其最小值是唯一的，并且[牛顿步](@entry_id:177069)是一个下降方向，从而保证了算法的稳定性和快速收敛性 [@problem_id:3601006]。

然而，牛顿法在实践中的应用受到极大限制，因为它要求计算、存储和求逆一个 $n \times n$ 的Hessian矩阵，当 $n$ 巨大时，这在计算上是不可行的。

#### [高斯-牛顿法](@entry_id:173233)

对于在[地球物理反演](@entry_id:749866)中极为常见的**[非线性](@entry_id:637147)[最小二乘问题](@entry_id:164198)** $J(m) = \frac{1}{2}\|r(m)\|_2^2$，其中 $r(m) = F(m) - d$ 是[残差向量](@entry_id:165091)，存在一种对牛顿法的有效近似，即**[高斯-牛顿法](@entry_id:173233)**（Gauss-Newton Method）。

$J(m)$ 的精确Hessian矩阵可以被写为 [@problem_id:3601051]：
$$
\nabla^2 J(m) = J_r(m)^\top J_r(m) + \sum_{i=1}^{p} r_i(m) \nabla^2 r_i(m)
$$
其中 $J_r(m)$ 是[残差向量](@entry_id:165091) $r(m)$ 的[雅可比矩阵](@entry_id:264467)（在地球物理中常称为Fréchet导数或敏感度矩阵）。[高斯-牛顿法](@entry_id:173233)的核心思想是**忽略**Hessian中的第二项。这个近似在两种情况下是合理的：(1) 当算法接近解时，残差 $r_i(m)$ 很小；(2) 当正演问题 $F(m)$ 近似线性时，[二阶导数](@entry_id:144508) $\nabla^2 r_i(m)$ 很小。

忽略第二项后，Hessian被近似为 $\nabla^2 J(m) \approx J_r(m)^\top J_r(m)$。这个近似Hessian的优点是它总是半正定的，并且它只依赖于一阶导数 $J_r(m)$，避免了计算复杂的[二阶导数](@entry_id:144508)。将这个近似Hessian代入牛顿系统，我们得到高斯-牛顿系统：
$$
(J_r(m)^\top J_r(m)) \Delta m = -J_r(m)^\top r(m)
$$
这恰好是线性最小二乘问题 $\min_{\Delta m} \|r(m) + J_r(m) \Delta m\|_2^2$ 的[正规方程](@entry_id:142238)。[高斯-牛顿法](@entry_id:173233)通过在每一步求解一个线性化的子问题来逼近原[非线性](@entry_id:637147)问题的解。它在许多反演应用中取得了巨大成功，构成了许多实用算法的基础。高斯-[牛顿步](@entry_id:177069)与真实[牛顿步](@entry_id:177069)之间的差异，主要就来自于被忽略的Hessian第二项，这个差异的大小直接影响了算法的收敛速度 [@problem_id:3601051]。

### 处理[非光滑优化](@entry_id:167581)问题

许多先进的[地球物理反演方法](@entry_id:749867)为了获得具有特定结构（例如，块状或稀疏）的模型，会采用非光滑的正则化项，例如**全变分**（Total Variation, TV）正则化：
$$
R(m) = \int_{\Omega} \|\nabla m(x)\| \,dx
$$
[TV正则化](@entry_id:756242)能够很好地保护模型中的锐利边界，但它在梯度为零的点是不可微的，这给[基于梯度的优化](@entry_id:169228)带来了挑战。

对于这类问题，梯度的概念被推广为**次梯度**（Subgradient）。一个[凸函数](@entry_id:143075) $J$ 在点 $m$ 的次梯度是任何一个向量 $g$，使得对于所有的 $y$，都满足 $J(y) \ge J(m) + g^\top(y-m)$。对于[可微函数](@entry_id:144590)，次梯度就是梯度；对于不可微点，次梯度可能是一个集合。例如，TV泛函的次梯度可以形式化地写为 $-\nabla \cdot \left(\frac{\nabla m}{\|\nabla m\|}\right)$，但在 $\|\nabla m\|=0$ 的地方，这个表达式没有定义，体现了其非[光滑性](@entry_id:634843) [@problem_id:3601015]。

直接使用次梯度进行优化的[次梯度法](@entry_id:164760)收敛通常很慢。一个更实用和流行的方法是**平滑化**（Smoothing）。我们可以用一个光滑的函数 $R_\varepsilon(m)$ 来近似非光滑的 $R(m)$，例如：
$$
R_\varepsilon(m) = \int_{\Omega} \sqrt{\|\nabla m(x)\|^2 + \varepsilon^2} \,dx
$$
其中 $\varepsilon > 0$ 是一个很小的平滑参数。这个平滑后的泛函 $R_\varepsilon(m)$ 在任何地方都是可微的，其梯度为 $-\nabla \cdot \left(\frac{\nabla m}{\sqrt{\|\nabla m\|^2 + \varepsilon^2}}\right)$ [@problem_id:3601015]。这个梯度在 $\|\nabla m\| \to 0$ 时也是良定义的，因为它受到 $\varepsilon$ 的保护。

当 $\varepsilon \to 0$ 时，平滑泛函的梯度会[逐点收敛](@entry_id:145914)到原始TV泛函的[次梯度](@entry_id:142710) [@problem_id:3601015]。这提供了一个强大的策略：我们可以用标准的梯度优化算法（如[L-BFGS](@entry_id:167263)等拟牛顿法）来求解一个由光滑[目标函数](@entry_id:267263) $J_\varepsilon(m) = \Phi(m) + \lambda R_\varepsilon(m)$ 构成的序列问题，并通过逐步减小 $\varepsilon$ 来逼近原始的非光滑问题的解。这种方法在实践中被证明非常有效。