## 引言
在[计算地球物理学](@entry_id:747618)中，从地表或井中收集的有限、含噪数据中反演出地下的物理属性模型，是勘探与研究的核心任务。然而，获得一个看似合理的模型图像仅仅是第一步。一个更为关键且具挑战性的问题是：这个反演得到的模型在多大程度上是可信的？它的哪些特征是地下真实结构的可靠反映，而哪些又可能只是数据噪声或反演算法引入的假象？

若缺乏对解的质量进行定量评估的严谨方法，我们可能会错误地解释地质构造、高估储层参数或对监测结果产生误判。因此，我们需要一个数学框架来穿透反演这个“黑箱”，精确地量化分辨率、不确定性和参数间的耦合关系。

[模型分辨率矩阵](@entry_id:752083)与[数据分辨率矩阵](@entry_id:748215)正是为此而生的核心分析工具。它们提供了一套强大的语言，让我们能够深刻理解数据如何塑造模型、正则化如何影响解的特性。本文将系统性地引导你掌握分辨率分析。在第一章“原理与机制”中，我们将深入其数学基础，推导模型与[数据分辨率矩阵](@entry_id:748215)，并解释其每个元素的物理含义。接下来的“应用与交叉学科联系”章节将展示如何将这些理论应用于模型评估、实验设计和时序监测等实际场景。最后，在“动手实践”部分，你将通过具体的计算练习来巩固所学知识。

现在，让我们从最基本的问题出发，深入探索分辨率分析的原理与机制，揭示它们如何将真实模型与我们最终的估计联系起来。

## 原理与机制

在[地球物理反演](@entry_id:749866)中，获得模型估计只是求解过程的一部分。同样重要的是，我们需要对解的质量和可靠性进行量化评估。模型和[数据分辨率矩阵](@entry_id:748215)为此提供了一个严谨的数学框架，使我们能够探究反演过程如何将“真实”的地下模型转化为我们得到的估计模型，以及观测数据在多大程度上影响了最终的拟合结果。本章将深入探讨这些[分辨率矩阵](@entry_id:754282)的原理、推导及其解释，揭示它们在诊断反演问题和指导勘探设计中的核心作用。

### 线性反演的基本框架

我们从一个一般的线性反演问题出发。在许多地球物理场景中，观测数据向量 $\mathbf{d} \in \mathbb{R}^{n}$ 与未知的地下模型参数向量 $\mathbf{m} \in \mathbb{R}^{p}$ 之间的关系可以通过一个线性或线性化的正演算子 $\mathbf{G} \in \mathbb{R}^{n \times p}$ 来近似描述：

$$
\mathbf{d} = \mathbf{G} \mathbf{m} + \boldsymbol{\epsilon}
$$

在这个方程中：
- $\mathbf{m}$ 是我们希望推断的模型参数向量，例如地下介质的速度、密度或[电导率](@entry_id:137481)[分布](@entry_id:182848)。
- $\mathbf{d}$ 是我们在地表或井中观测到的数据向量，例如地震波走时、[重力异常](@entry_id:750038)或[电磁场](@entry_id:265881)值。
- $\mathbf{G}$ 是正演算子或敏感度矩阵（也称为[雅可比矩阵](@entry_id:264467)或Frechét导数），它根据物理定律描述了数据对模型参数变化的敏感度。$\mathbf{G}$ 的每一列对应一个模型参数，描述该参数的单位变化对所有数据点的影响。[@problem_id:3613747]
- $\boldsymbol{\epsilon}$ 是观测噪声和模型[线性化误差](@entry_id:751298)的总和，通常假设为一个均值为零的随机向量。

为了从含噪数据 $\mathbf{d}$ 中估计模型 $\mathbf{m}$，一个常见的方法是最小化一个目标函数。这个目标函数不仅要惩罚数据与模型预测之间的失配（misfit），还要包含一个正则化项（regularization term）以保证解的稳定性和唯一性，特别是当问题是病态（ill-conditioned）或欠定（underdetermined）时。一个广泛应用的目标函数是[Tikhonov正则化](@entry_id:140094)的广义最小二乘（Generalized Least Squares, GLS）形式：

$$
J(\mathbf{m}) = (\mathbf{d} - \mathbf{G} \mathbf{m})^T \mathbf{C}_d^{-1} (\mathbf{d} - \mathbf{G} \mathbf{m}) + \lambda \|\mathbf{L} \mathbf{m}\|_2^2
$$

这里：
- $\mathbf{C}_d \in \mathbb{R}^{n \times n}$ 是[数据协方差](@entry_id:748192)矩阵，它描述了噪声的统计特性（[方差](@entry_id:200758)和相关性）。它的逆 $\mathbf{C}_d^{-1}$ 起到了对数据进行加权的作用：[方差](@entry_id:200758)较小（更可信）的数据点在失配项中获得更大的权重。[@problem_id:3613747]
- $\lambda \ge 0$ 是[正则化参数](@entry_id:162917)，它控制着数据拟合与模型平滑性之间的权衡。
- $\mathbf{L} \in \mathbb{R}^{q \times p}$ 是一个正则化算子（或稳定器），它对模型施加先验约束。例如，$\mathbf{L}$ 可以是单位矩阵（惩罚模型范数）、[一阶差分](@entry_id:275675)算子（惩罚模型梯度，促进平滑）或二阶差分算子（惩罚模型曲率，促进线性变化）。[@problem_id:3613733]

通过求解 $\nabla_{\mathbf{m}} J(\mathbf{m}) = \mathbf{0}$，我们得到正则化的GLS估计量 $\hat{\mathbf{m}}$：

$$
\hat{\mathbf{m}} = (\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G} + \lambda \mathbf{L}^T \mathbf{L})^{-1} \mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{d}
$$

这个估计量是后续所有分辨率分析的出发点。

### [模型分辨率矩阵](@entry_id:752083) $R_m$

[模型分辨率矩阵](@entry_id:752083) $\mathbf{R}_m$ 回答了一个核心问题：我们得到的估计模型 $\hat{\mathbf{m}}$ 在多大程度上是对真实模型 $\mathbf{m}_{\text{true}}$ 的一个无偏、无失真的重构？

#### 定义与推导

为了定义 $\mathbf{R}_m$，我们考察估计量 $\hat{\mathbf{m}}$ 的[期望值](@entry_id:153208)。将真实数据产生的过程 $\mathbf{d} = \mathbf{G} \mathbf{m}_{\text{true}} + \boldsymbol{\epsilon}$ 代入估计量表达式，并利用噪声均值为零（$\mathbb{E}[\boldsymbol{\epsilon}] = \mathbf{0}$）的假设，我们得到：

$$
\mathbb{E}[\hat{\mathbf{m}}] = \mathbb{E}[(\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G} + \lambda \mathbf{L}^T \mathbf{L})^{-1} \mathbf{G}^T \mathbf{C}_d^{-1} (\mathbf{G} \mathbf{m}_{\text{true}} + \boldsymbol{\epsilon})]
$$
$$
\mathbb{E}[\hat{\mathbf{m}}] = \underbrace{(\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G} + \lambda \mathbf{L}^T \mathbf{L})^{-1} \mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G}}_{\mathbf{R}_m} \mathbf{m}_{\text{true}}
$$

因此，**[模型分辨率矩阵](@entry_id:752083)** $\mathbf{R}_m \in \mathbb{R}^{p \times p}$ 定义为：

$$
\mathbf{R}_m = (\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G} + \lambda \mathbf{L}^T \mathbf{L})^{-1} \mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G}
$$

它将真实模型[线性映射](@entry_id:185132)到估计模型的[期望值](@entry_id:153208)。

#### 理想分辨率与[无偏估计](@entry_id:756289)

在一种理想化的情景下，假设问题是超定的且良态的，以至于我们不需要正则化（$\lambda=0$），并且矩阵 $\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G}$ 是可逆的。在这种情况下，[模型分辨率矩阵](@entry_id:752083)简化为：

$$
\mathbf{R}_m = (\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G})^{-1} (\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G}) = \mathbf{I}_p
$$

其中 $\mathbf{I}_p$ 是 $p \times p$ 的[单位矩阵](@entry_id:156724)。[@problem_id:3613664] 此时，$\mathbb{E}[\hat{\mathbf{m}}] = \mathbf{I}_p \mathbf{m}_{\text{true}} = \mathbf{m}_{\text{true}}$。这意味着估计量是**无偏**的：平均而言，我们的估计恰好等于真实模型。因此，单位矩阵 $\mathbf{I}_p$ 代表了**完美的分辨率**。在实践中，由于正则化或数据覆盖不足，$\mathbf{R}_m$ 很少是单位矩阵。$\mathbf{R}_m$ 与单位矩阵的偏离程度，恰恰量化了我们反演结果的局限性。

#### 解读 $R_m$ 的结构：自解析度与弥散

$\mathbf{R}_m$ 的威力在于其结构揭示了反演过程如何“扭曲”真实世界。关系式 $\mathbb{E}[\hat{\mathbf{m}}] = \mathbf{R}_m \mathbf{m}_{\text{true}}$ 可以按分量写出：

$$
\mathbb{E}[\hat{m}_i] = \sum_{j=1}^{p} (\mathbf{R}_m)_{ij} (m_{\text{true}})_j
$$

这个表达式告诉我们，第 $i$ 个模型参数的估计值 $\hat{m}_i$（的期望），是所有真实模型参数 $(m_{\text{true}})_j$ 的加权平均。[@problem_id:3613748]

- **对角元素 $(\mathbf{R}_m)_{ii}$**：称为第 $i$ 个参数的**自解析度**（self-resolution）。它量化了真实参数 $(m_{\text{true}})_i$ 对其自身估计值 $\hat{m}_i$ 的贡献。如果 $(\mathbf{R}_m)_{ii}=1$，则意味着 $\hat{m}_i$ 完美地恢复了 $(m_{\text{true}})_i$ 的值（假设无其他参数的贡献）。如果 $(\mathbf{R}_m)_{ii}  1$，则表示估计值被压制或阻尼了。

- **非对角元素 $(\mathbf{R}_m)_{ij}$ ($i \neq j$)**：量化了**参数弥散**（smearing）或**串扰**（cross-talk）。它表示第 $j$ 个真实参数 $(m_{\text{true}})_j$ 的值在多大程度上“泄露”或“弥散”到了第 $i$ 个参数的估计值 $\hat{m}_i$ 中。

考虑一个具体的 $3 \times 3$ [模型分辨率矩阵](@entry_id:752083)示例 [@problem_id:3613748]：
$$
\mathbf{R}_m = \begin{pmatrix} 0.82  0.10  0.05 \\ 0.08  0.65  0.20 \\ 0.03  0.12  0.90 \end{pmatrix}
$$
- 参数1的估计值 $\hat{m}_1$ 的自解析度为 $0.82$，意味着它主要恢复了真实值 $m_{\text{true},1}$ 的 $82\%$，但同时也被 $m_{\text{true},2}$ 和 $m_{\text{true},3}$ 分别以 $10\%$ 和 $5\%$ 的权重污染了。
- 参数2的估计值 $\hat{m}_2$ 的自解析度最差，仅为 $0.65$。同时，它受到来自 $m_{\text{true},3}$ 的显著弥散影响（系数为 $0.20$）。
- 参数3的自解析度最好（$0.90$），且受到的弥散效应相对较小。

如果 $\mathbf{R}_m$ 是[单位矩阵](@entry_id:156724)，则所有非对角元素都为零，不存在弥散，且所有对角元素都为1，自解析度完美。[@problem_id:3613748]

### 分辨率核与点扩展函数

$\mathbf{R}_m$ 的每一行都可以被看作一个**分辨率核**（averaging kernel）或**点扩展函数**（Point Spread Function, PSF）。第 $i$ 行，记为 $\mathbf{r}^{(i)}$，描述了估计值 $\hat{m}_i$ 是如何通过对整个真实模型空间进行加权平均而形成的。[@problem_id:3613657] [@problem_id:3613748]

一个理想的PSF应该是一个只在第 $i$ 个位置为1，其余位置为0的[脉冲函数](@entry_id:273257)（Kronecker delta）。然而在实践中，PSF通常是一个在中心达到峰值，并向两侧衰减的函数。这个函数的形态直接反映了分辨率的质量。

考虑一个一维[空间离散化](@entry_id:172158)模型，其中第3个参数的PSF为 [@problem_id:3613657]：
$$
\mathbf{r}^{(3)} = [0.05, 0.15, 0.60, 0.15, 0.05]
$$
- **中心峰值**：中心值 $0.60$ 是自解析度，表明估计值 $\hat{m}_3$ 主要由真实值 $m_{\text{true},3}$ 决定，但被衰减到其真实值的 $60\%$。
- **[旁瓣](@entry_id:270334)**：旁边的非零值（$0.15$ 和 $0.05$）表示弥散效应。估计值 $\hat{m}_3$ 也包含了来自邻近真实参数 $m_{\text{true},2}$、$m_{\text{true},4}$ 甚至更远的 $m_{\text{true},1}$ 和 $m_{\text{true},5}$ 的贡献。
- **分辨率长度**：我们可以用PSF的二阶矩来定义一个**分辨率长度** $l_{\text{res}}$，它量化了PSF的空间展宽程度。对于上述例子，分辨率长度约为 $\sqrt{0.7} \Delta x \approx 0.84 \Delta x$（其中 $\Delta x$ 是网格间距），表明分辨率的有效宽度大约是一个网格单元。[@problem_id:3613657]
- **对不同尺度特征的响应**：
    - 如果PSF的行和为1（如本例），则对于一个常数模型 $m_{\text{true}, i} = c$，估计值将是完美的：$\hat{m}_3 = \sum_i r^{(3)}_i c = c \sum_i r^{(3)}_i = c$。这说明反演能准确恢复模型的直流（DC）分量。[@problem_id:3613657]
    - 然而，对于高频[振荡](@entry_id:267781)的特征，PSF的平均效应会起到低通滤波的作用，显著衰减其振幅。例如，一个波长为 $2\Delta x$ 的交错模式会被衰减到其真实振幅的 $40\%$。[@problem_id:3613657]

### [数据分辨率矩阵](@entry_id:748215) $R_d$

与[模型分辨率矩阵](@entry_id:752083)从[模型空间](@entry_id:635763)视角进行分析不同，[数据分辨率矩阵](@entry_id:748215) $\mathbf{R}_d$ 从数据空间视角回答了另一个关键问题：我们通过反演得到的**预测数据** $\hat{\mathbf{d}} = \mathbf{G}\hat{\mathbf{m}}$ 与**观测数据** $\mathbf{d}$ 之间的关系是怎样的？

#### 定义与属性

通过将 $\hat{\mathbf{m}}$ 的表达式代入 $\hat{\mathbf{d}} = \mathbf{G}\hat{\mathbf{m}}$，我们得到 $\hat{\mathbf{d}}$ 与 $\mathbf{d}$ 的线性关系：
$$
\hat{\mathbf{d}} = \underbrace{\mathbf{G} (\mathbf{G}^T \mathbf{C}_d^{-1} \mathbf{G} + \lambda \mathbf{L}^T \mathbf{L})^{-1} \mathbf{G}^T \mathbf{C}_d^{-1}}_{\mathbf{R}_d} \mathbf{d}
$$

这个 $n \times n$ 的矩阵 $\mathbf{R}_d$ 就是**[数据分辨率矩阵](@entry_id:748215)**，在统计学中也常被称为“[帽子矩阵](@entry_id:174084)”（hat matrix），因为它给数据“戴上了一顶帽子”（$\hat{\mathbf{d}}$）。

在无正则化的理想情况下（$\lambda=0$），$\mathbf{R}_d$ 具有一些优美的性质 [@problem_id:3613668]：
- **[幂等性](@entry_id:190768)**（Idempotent）：$\mathbf{R}_d^2 = \mathbf{R}_d$。这意味着它是一个**[投影算子](@entry_id:154142)**。将数据投影一次和投影两次的效果是相同的。
- **对称性**（Symmetric）：仅当[数据协方差](@entry_id:748192)是各向同性的（即 $\mathbf{C}_d = \sigma^2 \mathbf{I}_n$）时，$\mathbf{R}_d$ 才是对称的（$\mathbf{R}_d^T = \mathbf{R}_d$）。在这种情况下，它是一个**[正交投影](@entry_id:144168)算子**，将数据向量[正交投影](@entry_id:144168)到由 $\mathbf{G}$ 的列向量张成的空间上。如果 $\mathbf{C}_d$ 不是[单位矩阵](@entry_id:156724)的倍数，可以通过[数据白化](@entry_id:636289)（whitening）变换来恢复此对称性。[@problem_id:3613747]
- **迹**（Trace）：在无正则化且 $\mathbf{G}$ 列满秩的情况下，$\mathbf{R}_d$ 的迹等于模型参数的数量 $p$：$\text{tr}(\mathbf{R}_d) = p$。这在统计学上被解释为模型拟合消耗的“自由度”数量。[@problem_id:3613668]

当引入正则化（$\lambda0$）时，$\mathbf{R}_d$ 不再是幂等的，它变成了一个收缩算子而非投影算子。

#### [杠杆值](@entry_id:172567)

$\mathbf{R}_d$ 的对角元素 $h_{ii} = (\mathbf{R}_d)_{ii}$ 有一个特殊的名称和含义：第 $i$ 个数据的**[杠杆值](@entry_id:172567)**（leverage）。从 $\hat{d}_i = \sum_j (\mathbf{R}_d)_{ij} d_j$ 出发，我们可以看到：
$$
h_{ii} = \frac{\partial \hat{d}_i}{\partial d_i}
$$
[杠杆值](@entry_id:172567) $h_{ii}$ 直接量化了观测数据点 $d_i$ 对其自身预测值 $\hat{d}_i$ 的影响力。[@problem_id:3613740]
- **高[杠杆值](@entry_id:172567)**：一个接近1的杠杆值意味着 $\hat{d}_i$ 几乎完全由 $d_i$ 决定。这个数据点对模型有很强的“拉动”作用。这样的点可能是异常值，也可能是对模型约束特别强的关键数据。
- **低[杠杆值](@entry_id:172567)**：一个接近0的杠杆值意味着 $\hat{d}_i$ 主要由其他数据点决定，而 $d_i$ 本身几乎不起作用。这通常发生在给该数据点的权重很小（即其假设的[方差](@entry_id:200758)很大）的情况下。增加数据点的[方差](@entry_id:200758)（降低其可信度）会*降低*其[杠杆值](@entry_id:172567)。[@problem_id:3613740]

在标准的普通最小二乘（OLS）设置中，杠杆值满足 $0 \le h_{ii} \le 1$，且所有杠杆值之和等于模型参数的个数。有趣的是，一个数据点对自身拟合值的高[杠杆作用](@entry_id:172567)，必然意味着它对其他拟合值的影响力 collectively 很小。[@problem_id:3613740]

### 正则化的作用：权衡与选择

正则化是反演的核心，它通过引入[先验信息](@entry_id:753750)来稳定解，但代价是引入了偏差。[模型分辨率矩阵](@entry_id:752083)是理解这种权衡的关键工具。

#### 分辨率-[方差](@entry_id:200758)权衡

正则化参数 $\lambda$ 的选择是一个典型的**分辨率-[方差](@entry_id:200758)权衡**（resolution-variance tradeoff），也即统计学中的**偏差-方差权衡**（bias-variance tradeoff）。[@problem_id:3613697]

- **小 $\lambda$**：当我们减小 $\lambda$ 时，我们更加信任数据。
    - **分辨率提高**：$\mathbf{R}_m$ 更接近单位矩阵，对角元素趋向于1，弥散减小。反演的偏差减小。
    - **[方差](@entry_id:200758)增大**：估计模型对数据中的噪声变得极其敏感。$\mathbf{G}$ 中与小[奇异值](@entry_id:152907)相关的部分（确定性差的模式）会将噪声极大地放大，导致模型估计的[方差](@entry_id:200758)急剧增加。

- **大 $\lambda$**：当我们增大 $\lambda$ 时，我们更加依赖先验约束。
    - **分辨率降低**：$\mathbf{R}_m$ 趋向于[零矩阵](@entry_id:155836)（如果 $\mathbf{L}$ 是可逆的）或一个投影到 $\mathbf{L}$ 的[零空间](@entry_id:171336)的算子。这意味着估计模型被强烈地拉向先验模型（通常是[零模型](@entry_id:181842)或一个平[滑模](@entry_id:263630)型），与真实模型的关联减弱，偏差增大。[@problem_id:3613747]
    - **[方差](@entry_id:200758)减小**：正则化项有效地抑制了噪声的放大，使得估计模型更加稳定，[方差](@entry_id:200758)减小。

这种权衡可以通过对 $\mathbf{G}$ 进行[奇异值分解](@entry_id:138057)（SVD）来精确地理解。正则化过程等效于对模型的不同奇异向量分量应用一个依赖于 $\lambda$ 和[奇异值](@entry_id:152907)大小的[谱滤波](@entry_id:755173)器。减小 $\lambda$ 相当于让滤波器通过更多的分量（包括那些被[噪声污染](@entry_id:188797)的），从而提高分辨率但增加[方差](@entry_id:200758)。[@problem_id:3613697]

#### 正则化算子 $L$ 的选择

正则化算子 $\mathbf{L}$ 的选择将我们的[先验信念](@entry_id:264565)（例如，模型应该是平滑的）编码到反演中，并直接塑造了分辨率核的形态。[@problem_id:3613733]

考虑一个简单的情景 $\mathbf{G}=\mathbf{I}$，此时 $\mathbf{R}_m = (\mathbf{I} + \lambda^2 \mathbf{L}^T \mathbf{L})^{-1}$。
- 如果 $\mathbf{L}=\mathbf{I}$（范数正则化），$\mathbf{L}^T\mathbf{L}$ 是[单位矩阵](@entry_id:156724)，$\mathbf{R}_m$ 是一个对角矩阵。每个估计参数只依赖于其对应的真实参数，但会被一个因子 $1/(1+\lambda^2)$ 衰减。这是一种均匀的阻尼，没有空间弥散。
- 如果 $\mathbf{L}$ 是[一阶差分](@entry_id:275675)（梯度）算子，$\mathbf{L}^T\mathbf{L}$ 是一个[三对角矩阵](@entry_id:138829)，代表离散的[二阶导数](@entry_id:144508)。此时，$\mathbf{R}_m$ 是一个具有局部正权重、从对角线向外快速衰减的[稠密矩阵](@entry_id:174457)。这对应于一个**局部的空间平均核**，它将真实模型在局部进行平滑，以得到估计模型。
- 如果 $\mathbf{L}$ 是二阶差分（拉普拉斯）算子，$\mathbf{L}^T\mathbf{L}$ 是一个五[对角矩阵](@entry_id:637782)。这会导致一个比[梯度算子](@entry_id:275922)**更宽的空间平均核**，因为它在施加平滑性时考虑了更远的邻居。

此外，如果正则化算子 $\mathbf{L}$ 的[零空间](@entry_id:171336)包含常数向量（例如，对于梯度和拉普拉斯算子），那么 $\mathbf{R}_m$ 的每一行之和将等于1。这保证了模型估计能准确地恢复真实模型的平均值。[@problem_id:3613733]

### 实践中的分辨率：与物理和几何的联系

[分辨率矩阵](@entry_id:754282)不仅是数学抽象，它与实际的地球物理勘探设计和[数据质量](@entry_id:185007)紧密相连。一个典型的例子是[地震层析成像](@entry_id:754649)。[@problem_id:3613676]

假设在一个[层析成像](@entry_id:756051)模型中，某个子区域 $\mathcal{S}$ 的射线覆盖非常稀疏，且仅有少量近乎平行的射线穿过。这种不佳的采集几何会导致：
1.  **敏感度矩阵 $\mathbf{G}$ 的变化**：对应于 $\mathcal{S}$ 区域内模型单元的 $\mathbf{G}$ 的列向量，其范数会很小（因为射线路径短），并且彼此之间高度相关（因为射线路径相似）。
2.  **[分辨率矩阵](@entry_id:754282) $\mathbf{R}_m$ 的结构**：由于 $\mathbf{G}$ 无法区分该区域内沿射线方向不同单元的贡献，反演过程会将它们“模糊”在一起。这在 $\mathbf{R}_m$ 中表现为：
    - 对应于 $\mathcal{S}$ 区域的**对角元素显著小于1**，表明自解析度很差。
    - 沿平行射线方向连接相邻单元的**非对角元素变得相对较大且为正**。
    - 最终，分辨率核（$\mathbf{R}_m$ 的行）在射线方向上被拉长，形成**弥散条纹**。

这个例子生动地说明了，通过分析[模型分辨率矩阵](@entry_id:752083)，我们可以预见或诊断出由于采集几何的局限性而导致的图像伪影和分辨率损失。

### 高级主题：含约束反演的分辨率

在许多实际问题中，我们需要对模型参数施加边界约束，例如 $m_{\text{min}} \le m_i \le m_{\text{max}}$（比如，速度或密度必须为正）。这些约束使得反演问题变成了一个[非线性](@entry_id:637147)问题，因为最终解中哪些参数会“触及”边界（即所谓的**活动集**）取决于数据本身。[@problem_id:3613719]

由于这种[非线性](@entry_id:637147)，一个单一的、全局适用的[模型分辨率矩阵](@entry_id:752083)不再存在。但是，我们可以在一个给定的解 $\hat{\mathbf{m}}$ 附近进行**[局部线性化](@entry_id:169489)分辨率分析**。其核心思想是：
- 将那些在解中已经达到上或下边界的**活动参数**（active parameters）视为固定的。它们对真实模型或数据的微小变化的响应为零，因此它们的局部分辨率为零。
- 对于那些位于边界内部的**自由参数**（free parameters），我们可以求解一个缩减的、仅涉及这些参数的线性反演问题。
- 基于这个缩减的线性问题，我们可以推导出一个**局部[模型分辨率矩阵](@entry_id:752083)** $\mathbf{R}_m^{\text{loc}}$ 和**局部[数据分辨率矩阵](@entry_id:748215)** $\mathbf{R}_d^{\text{loc}}$。这些矩阵的结构反映了在当前活动集下的分辨率特性。例如，$\mathbf{R}_m^{\text{loc}}$ 中与活动参数对应的行将全为零，表明这些参数被“锁定”，失去了对真实模型变化的响应能力。[@problem_id:3613719]

这种局部化的分析虽然比全局线性分析复杂，但它为理解和评估[非线性](@entry_id:637147)或含约束反演结果的可靠性提供了一个强大而必要的工具。