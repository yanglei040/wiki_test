{"hands_on_practices": [{"introduction": "在应用最小二乘法时，一个核心的见解是并非所有数据点对最终解的贡献都是均等的。某些观测值由于其在实验设计中的独特位置，对模型拟合具有不成比例的影响力，这种影响力通过“杠杆值”(leverage)来量化。本练习将通过一个简洁、可解析的折射实验算例，引导您亲手计算杠杆值，直观地感受高杠杆数据点如何支配普通最小二乘（OLS）的解，并探索如何通过加权最小二乘（WLS）来缓解这一问题 [@problem_id:3606778]。", "problem": "在计算地球物理学的一维折射实验中，假设存在一个未知的恒定地下慢度 $s$（单位为秒/公里，$\\mathrm{s/km}$）。一组震源-接收器偏移距 $L_i$（单位为公里）和相应的观测走时 $t_i$（单位为秒）遵循线性数据模型 $t_i = L_i s + \\epsilon_i$，其中 $\\epsilon_i$ 是独立的、零均值、有限方差的误差。考虑设计矩阵 $G \\in \\mathbb{R}^{N \\times 1}$（其第 $i$ 行为 $[L_i]$）和数据向量 $d \\in \\mathbb{R}^{N}$（其分量为 $t_i$）。给定以下 $N=4$ 的具体数据集：\n- 偏移距: $L = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$,\n- 观测时间: $t = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$。\n\n原则上将该问题视为一个 $N \\gg 1$ 的超定系统，但为了便于解析处理，此处使用 $N=4$。从普通最小二乘法（OLS）作为残差平方和的最小化器的定义以及帽子矩阵 $H = G\\,(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$ 的定义出发，完成以下任务：\n\n1. 直接通过最小化残差平方和 $\\sum_{i=1}^{N} (t_i - L_i s)^2$ 来推导这个单参数问题的 OLS 估计值 $\\hat{s}$。使用你的结果计算给定数据的 $\\hat{s}$。\n\n2. 使用帽子矩阵的定义，计算杠杆值 $h_{ii}$ 并解释该数据集按行的杠杆作用。以小数形式量化与第四行相关的总对角杠杆的分数。\n\n3. 为了减轻高杠杆行的主导作用，考虑使用对角权重 $W = \\mathrm{diag}(w_1,\\dots,w_N)$ 的加权最小二乘法（WLS），该方法最小化 $\\sum_{i=1}^{N} w_i (t_i - L_i s)^2$。选择权重 $w_1 = w_2 = w_3 = 1$ 和 $w_4 = 0.01$ 以降低第四个观测值的权重。推导 WLS 估计量 $\\hat{s}_w$ 并计算其在给定数据下的值。\n\n4. 使用带有 Huber 损失的 M 估计量的方程，简要解释迭代重加权最小二乘法（IRLS）过程如何通过分配一个较小的有效权重来自适应地减小像第四个数据点这样的离群残差的影响，而无需硬编码 $w_4$。你不需要执行迭代。\n\n作为你的最终答案，提供加权估计值 $\\hat{s}_w$ 的数值。以 $\\mathrm{s/km}$ 为单位表示你的结果，并四舍五入到四位有效数字。", "solution": "问题陈述已经过验证，被认为是有效的。它科学地基于应用于地球物理学的线性反演理论原理，是适定的，并使用了客观、正式的语言。所有必要的数据都已提供且一致。我们可以开始求解。\n\n该问题要求使用一个特定的数据集对一个简单的线性反演问题进行四部分分析。走时 $t$ 和偏移距 $L$ 之间的关系被建模为一条通过原点的直线，$t = Ls$，其中 $s$ 是慢度。\n\n**1. 普通最小二乘法（OLS）估计**\n\n参数 $s$ 的普通最小二乘法（OLS）估计值是使残差平方和 $S(s)$ 最小化的值 $\\hat{s}$。第 $i$ 次测量的残差是 $r_i = t_i - L_i s$。目标函数为：\n$$\nS(s) = \\sum_{i=1}^{N} r_i^2 = \\sum_{i=1}^{N} (t_i - L_i s)^2\n$$\n为了找到最小值，我们计算 $S(s)$ 关于 $s$ 的导数并将其设为零：\n$$\n\\frac{dS}{ds} = \\sum_{i=1}^{N} \\frac{d}{ds} (t_i - L_i s)^2 = \\sum_{i=1}^{N} 2(t_i - L_i s)(-L_i) = -2 \\sum_{i=1}^{N} (L_i t_i - L_i^2 s)\n$$\n令 $\\frac{dS}{ds} = 0$：\n$$\n\\sum_{i=1}^{N} (L_i t_i - L_i^2 s) = 0 \\implies \\sum_{i=1}^{N} L_i t_i - s \\sum_{i=1}^{N} L_i^2 = 0\n$$\n解出 $s$ 得到 OLS 估计值 $\\hat{s}$：\n$$\n\\hat{s} = \\frac{\\sum_{i=1}^{N} L_i t_i}{\\sum_{i=1}^{N} L_i^2}\n$$\n这是一参数线性拟合通过原点的通用公式。对于给定的数据集，$L = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$ 和 $t = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$。我们计算和：\n$$\n\\sum_{i=1}^{4} L_i t_i = (1)(0.5) + (1)(0.5) + (1)(0.5) + (10)(6.0) = 0.5 + 0.5 + 0.5 + 60.0 = 61.5\n$$\n$$\n\\sum_{i=1}^{4} L_i^2 = 1^2 + 1^2 + 1^2 + 10^2 = 1 + 1 + 1 + 100 = 103\n$$\n因此，OLS 估计值为：\n$$\n\\hat{s} = \\frac{61.5}{103} \\approx 0.597087\\; \\mathrm{s/km}\n$$\n\n**2. 帽子矩阵和杠杆值**\n\n帽子矩阵 $H$ 将观测数据向量 $d$ 映射到预测数据向量 $\\hat{d}$，即 $\\hat{d} = Hd$。对于一般的线性模型 $d = Gm$，帽子矩阵定义为 $H = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$。$H$ 的对角元素，记为 $h_{ii}$，是杠杆值。\n\n在这个问题中，模型参数向量 $m$ 就是标量 $s$。设计矩阵 $G$ 是偏移距 $L_i$ 的列向量：\n$$\nG = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix}\n$$\n首先，我们计算 $G^{\\mathsf{T}}G$：\n$$\nG^{\\mathsf{T}}G = \\begin{pmatrix} 1  1  1  10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} = 1^2 + 1^2 + 1^2 + 10^2 = 103\n$$\n这是一个标量，所以它的逆就是：\n$$\n(G^{\\mathsf{T}}G)^{-1} = \\frac{1}{103}\n$$\n现在我们构造帽子矩阵 $H$：\n$$\nH = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} \\left(\\frac{1}{103}\\right) \\begin{pmatrix} 1  1  1  10 \\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1\\cdot1  1\\cdot1  1\\cdot1  1\\cdot10 \\\\\n1\\cdot1  1\\cdot1  1\\cdot1  1\\cdot10 \\\\\n1\\cdot1  1\\cdot1  1\\cdot1  1\\cdot10 \\\\\n10\\cdot1  10\\cdot1  10\\cdot1  10\\cdot10\n\\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1  1  1  10 \\\\\n1  1  1  10 \\\\\n1  1  1  10 \\\\\n10  10  10  100\n\\end{pmatrix}\n$$\n杠杆值 $h_{ii}$ 是 $H$ 的对角元素：\n$h_{11} = \\frac{1}{103}$，$h_{22} = \\frac{1}{103}$，$h_{33} = \\frac{1}{103}$，以及 $h_{44} = \\frac{100}{103}$。\n\n杠杆值 $h_{ii}$ 量化了第 $i$ 个观测值 $t_i$ 对其自身预测值 $\\hat{t}_i$ 的影响。一个接近 1 的值表示该观测值具有高影响力，有效地将模型拟合拉向其自身。在这里，前三个数据点的杠杆值非常低（$h_{11}=h_{22}=h_{33} \\approx 0.0097$），而第四个数据点的杠杆值极高（$h_{44} \\approx 0.9709$）。这意味着 OLS 解几乎完全由 $L_4=10$ 处的第四个测量值决定。\n\n帽子矩阵的对角元素之和，即 $\\mathrm{Tr}(H)$，等于模型参数的数量，本例中为 1。$\\sum_{i=1}^{4} h_{ii} = \\frac{1}{103} + \\frac{1}{103} + \\frac{1}{103} + \\frac{100}{103} = \\frac{103}{103} = 1$。与第四行相关的总对角杠杆的分数是 $h_{44} / \\mathrm{Tr}(H) = h_{44} / 1 = h_{44}$。以小数表示，即为：\n$$\n\\frac{100}{103} \\approx 0.9709\n$$\n\n**3. 加权最小二乘法（WLS）估计**\n\n为了减轻高杠杆第四点的影响，我们使用加权最小二乘法（WLS）。WLS 估计值 $\\hat{s}_w$ 最小化加权残差平方和：\n$$\nS_w(s) = \\sum_{i=1}^{N} w_i (t_i - L_i s)^2\n$$\n遵循与 OLS 相同的微分过程，我们找到 WLS 估计量：\n$$\n\\hat{s}_w = \\frac{\\sum_{i=1}^{N} w_i L_i t_i}{\\sum_{i=1}^{N} w_i L_i^2}\n$$\n使用指定的权重 $w_1 = w_2 = w_3 = 1$ 和 $w_4 = 0.01$，我们计算加权和：\n$$\n\\sum_{i=1}^{4} w_i L_i t_i = (1)(1)(0.5) + (1)(1)(0.5) + (1)(1)(0.5) + (0.01)(10)(6.0) = 0.5 + 0.5 + 0.5 + 0.6 = 2.1\n$$\n$$\n\\sum_{i=1}^{4} w_i L_i^2 = (1)(1^2) + (1)(1^2) + (1)(1^2) + (0.01)(10^2) = 1 + 1 + 1 + (0.01)(100) = 3 + 1 = 4\n$$\n因此，WLS 估计值为：\n$$\n\\hat{s}_w = \\frac{2.1}{4} = 0.525\\; \\mathrm{s/km}\n$$\n这个值远接近于前三个点所暗示的慢度 $0.5\\; \\mathrm{s/km}$，表明成功地降低了高杠杆第四点的权重。\n\n**4. M估计量和迭代重加权最小二乘法（IRLS）**\n\nM估计量旨在最小化一个更一般的目标函数 $\\sum_{i=1}^{N} \\rho(r_i)$，其中 $r_i$ 是残差，$\\rho(r)$ 是一个稳健的损失函数，对于大的 $r$ 值，其增长速度比 $r^2$ 慢。Huber 损失函数是一个常见的选择：\n$$\n\\rho(r) = \\begin{cases}\n  \\frac{1}{2} r^2  \\text{if } |r| \\le \\delta \\\\\n  \\delta(|r| - \\frac{1}{2}\\delta)  \\text{if } |r|  \\delta\n\\end{cases}\n$$\n其中 $\\delta$ 是一个调节参数。最小化此目标函数会导致非线性估计方程 $\\sum_{i=1}^{N} L_i \\psi(r_i) = 0$，其中 $\\psi(r) = \\rho'(r)$是影响函数。\n\n迭代重加权最小二乘法（IRLS）是求解该方程的一个过程。它通过将估计方程重构为一个加权最小二乘问题来工作，其中权重取决于残差本身。我们定义一个自适应权重函数 $W(r) = \\psi(r)/r$。对于 Huber 损失，相关的权重函数是：\n$$\nW(r) = \\frac{\\psi(r)}{r} = \\begin{cases}\n  1  \\text{if } |r| \\le \\delta \\\\\n  \\frac{\\delta}{|r|}  \\text{if } |r|  \\delta\n\\end{cases}\n$$\nIRLS 过程如下：\n1.  获得一个初始估计，例如 OLS 估计 $\\hat{s}^{(0)}$。\n2.  对于迭代 $k=1, 2, ...$：\n    a. 计算残差：$r_i^{(k-1)} = t_i - L_i \\hat{s}^{(k-1)}$。\n    b. 计算新权重：$w_i^{(k)} = W(r_i^{(k-1)})$。具有大残差（$|r_i|  \\delta$）的数据点将被赋予小于 1 的权重。\n    c. 用这些新权重解决 WLS 问题以获得更新的估计：\n       $$\n       \\hat{s}^{(k)} = \\frac{\\sum_{i=1}^N w_i^{(k)} L_i t_i}{\\sum_{i=1}^N w_i^{(k)} L_i^2}\n       $$\n3.  重复直到估计值 $\\hat{s}^{(k)}$ 收敛。\n\n在这个问题的背景下，如果一个数据的残差相对于数据的某种稳健尺度度量来说很大，那么它就被认为是离群值。前三个点意味着慢度为 $s=0.5\\;\\mathrm{s/km}$。相对于这个趋势，第四个点产生了一个很大的残差：$r_4 = 6.0 - 10(0.5) = 1.0$。一个 IRLS 程序，在初始步骤之后，会识别出这个大残差，并赋予一个较小的有效权重 $w_4 = \\delta/|r_4|  1$，然后重新计算解。这个过程自适应地、自动地减少任何与大部分数据不一致的数据点（即离群值）的影响，而不需要像在 WLS 中那样由用户硬编码权重。", "answer": "$$\\boxed{0.5250}$$", "id": "3606778"}, {"introduction": "除了单个数据点的影响，地球物理反演问题的稳定性更深层次地取决于整个正演算子 $G$ 的内在结构。当实验设计导致 $G$ 的列向量近似线性相关时，系统便处于“病态”或“近秩亏”状态，这会在解空间中形成一个“扁平而狭长的峡谷”，其中许多差异巨大的模型都能以几乎相同的程度拟合数据。本练习将引导您从奇异值分解（SVD）的几何视角出发，揭示病态问题如何将微小的观测噪声放大为巨大的解方差，从而深化对解稳定性和实验设计重要性的理解 [@problem_id:3606781]。", "problem": "在双单元介质的线性化走时层析成像中，假设数据残差向量 $d \\in \\mathbb{R}^{3}$ 建模为 $d = G(\\varepsilon) \\, m_{\\mathrm{true}} + n$，其中 $m_{\\mathrm{true}} \\in \\mathbb{R}^{2}$ 是以缩放单位表示的单元慢度扰动，$n$ 是观测噪声。考虑设计矩阵\n$$\nG(\\varepsilon) \\;=\\;\n\\begin{pmatrix}\n1  1 \\\\\n1  1+\\varepsilon \\\\\n0  \\varepsilon\n\\end{pmatrix},\n$$\n其中 $\\varepsilon > 0$ 很小，表示三条不同的射线，其路径长度敏感度使得当 $\\varepsilon \\to 0$ 时，$G(\\varepsilon)$ 的两列近似共线。假设 $n$ 是零均值高斯噪声，其分量独立且方差相等，即对于某个 $\\sigma^{2} > 0$，$n \\sim \\mathcal{N}(0, \\sigma^{2} I)$。\n\n普通最小二乘 (OLS) 估计 $\\hat{m}(\\varepsilon)$ 最小化残差范数的平方 $J(m) = \\| G(\\varepsilon) \\, m - d \\|_{2}^{2}$。在极限 $\\varepsilon \\to 0$ 下，矩阵 $G(\\varepsilon)$ 趋于秩亏，因此失配景观会形成一条由近似等价的极小值点构成的长而浅的谷。\n\n任务：\n- 仅使用最小二乘的定义和奇异值分解（SVD）导出的几何结构，给出一个构造，以展示 $G(\\varepsilon)$ 的近秩亏性如何产生多个近似等价的极小值点。特别地，论证存在一个单位方向 $v_{\\varepsilon} \\in \\mathbb{R}^{2}$，沿此方向 $J$ 在极小值点处的二阶方向导数为 $O(\\varepsilon^{2})$，并找出一对显式的模型 $m_{\\star}$ 和 $m_{\\star} + \\delta v_{\\varepsilon}$（其中 $\\delta$ 与 $\\varepsilon$ 无关），使得它们的失配差异在 $\\varepsilon \\to 0$ 时为 $O(\\varepsilon^{2})$。\n- 在上述假设下，从第一性原理（OLS 和协方差为 $\\sigma^{2} I$ 的高斯噪声的定义）出发，推导无量纲方差放大因子\n$$\nA(\\varepsilon) \\;=\\; \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{m}(\\varepsilon)]\\big),\n$$\n作为仅关于 $\\varepsilon$ 的函数的精确表达式。\n\n给出 $A(\\varepsilon)$ 的精确闭式解析表达式作为你的最终答案。不需要进行数值舍入，所求量为无量纲量，因此无需报告单位。", "solution": "首先对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- 数据向量：$d \\in \\mathbb{R}^{3}$。\n- 模型向量：$m_{\\mathrm{true}} \\in \\mathbb{R}^{2}$。\n- 线性模型：$d = G(\\varepsilon) \\, m_{\\mathrm{true}} + n$。\n- 设计矩阵：$G(\\varepsilon) = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix}$。\n- 参数 $\\varepsilon > 0$ 很小。\n- 噪声向量 $n$ 的分布为 $\\mathcal{N}(0, \\sigma^{2} I)$，其中 $\\sigma^{2} > 0$ 且 $I$ 是 $3 \\times 3$ 单位矩阵。\n- 普通最小二乘 (OLS) 估计 $\\hat{m}(\\varepsilon)$ 最小化成本函数 $J(m) = \\| G(\\varepsilon) \\, m - d \\|_{2}^{2}$。\n- 任务 1：使用 SVD 原理构造一个论证，说明 $G(\\varepsilon)$ 的近秩亏性如何导致近似等价的极小值点。确定一个单位方向 $v_{\\varepsilon}$，使得 $J$ 在极小值点沿该方向的二阶方向导数为 $O(\\varepsilon^2)$。找出一对模型 $m_{\\star}$ 和 $m_{\\star} + \\delta v_{\\varepsilon}$，其失配差异为 $O(\\varepsilon^2)$。\n- 任务 2：推导方差放大因子 $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{m}(\\varepsilon)]\\big)$ 的精确表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题是线性代数和统计学中一个明确定义的练习，特别是在地球物理学和其他实验科学中常见的线性反演理论中的应用。\n\n- **科学依据**：该问题是分析病态线性系统 $Gm=d$ 的一个典型例子。矩阵 $G(\\varepsilon)$ 代表了层析成像中一个简化但物理上合理的场景，其中射线以非常相似的敏感度对两个单元进行采样，导致 $G$ 的列向量近似线性相关。对估计量协方差的分析是反演问题理论的一个标准和基本部分。\n- **适定性**：该问题在数学上是适定的。它提供了所有必要的定义和矩阵。任务规定明确，要求一个论证性构造和一个具体推导。$A(\\varepsilon)$ 存在唯一的解析解。\n- **客观性**：该问题以客观的数学语言陈述，没有歧义或主观内容。\n\n满足有效问题的所有标准。未发现任何缺陷。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供完整解答。\n\n### 解答推导\n\n解答分为两部分，对应两个任务。\n\n#### 第一部分：失配景观与近似等价的极小值点\n\n需要最小化的成本函数是残差向量的 $L_2$ 范数的平方：\n$$\nJ(m) = \\| G(\\varepsilon)m - d \\|_{2}^{2} = (G(\\varepsilon)m - d)^T (G(\\varepsilon)m - d)\n$$\n这是关于模型参数 $m$ 的二次函数。$J(m)$ 的梯度和黑塞矩阵是：\n$$\n\\nabla J(m) = 2 G(\\varepsilon)^T (G(\\varepsilon)m - d)\n$$\n$$\n\\nabla^2 J(m) = 2 G(\\varepsilon)^T G(\\varepsilon)\n$$\n黑塞矩阵是常数，意味着失配曲面是一个抛物碗形。这个碗在任何方向的“平坦度”由二阶方向导数决定。对于一个单位方向向量 $v \\in \\mathbb{R}^2$，$J$ 在任意点 $m_0$ 的二阶方向导数由下式给出：\n$$\nD_v^2 J(m_0) = v^T (\\nabla^2 J) v = 2 v^T G(\\varepsilon)^T G(\\varepsilon) v = 2 \\|G(\\varepsilon)v\\|_2^2\n$$\n问题陈述中关于“长而浅的谷”对应于在模型空间中找到一个方向 $v$，使得该二阶导数非常小。这意味着我们正在寻找一个方向 $v$，使得 $G(\\varepsilon)v$ 的范数很小。这样一个向量 $v$ 是 $G(\\varepsilon)$ 的近似零空间中的一个元素。\n\n$G(\\varepsilon)$ 的奇异值分解 (SVD) 为 $G(\\varepsilon) = U \\Sigma V^T$，其中 $V = [v_1, v_2]$ 的列是右奇异向量，构成模型空间 $\\mathbb{R}^2$ 的一个标准正交基。奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge 0$ 是 $\\Sigma$ 的对角线元素。$G(\\varepsilon)$ 在这些基向量上的作用是 $G(\\varepsilon)v_i = \\sigma_i u_i$，其中 $u_i$ 是对应的左奇异向量。\n\n在奇异向量 $v_i$ 方向上的二阶方向导数为：\n$$\nD_{v_i}^2 J = 2 \\|G(\\varepsilon)v_i\\|_2^2 = 2 \\|\\sigma_i u_i\\|_2^2 = 2 \\sigma_i^2 \\|u_i\\|_2^2 = 2 \\sigma_i^2\n$$\n因此，最浅谷的方向是对应于最小奇异值 $\\sigma_2$ 的右奇异向量 $v_2$。平坦度与 $\\sigma_2^2$ 成正比。\n\n我们来分析当 $\\varepsilon \\to 0$ 时 $G(\\varepsilon)$ 的情况。其列向量为 $g_1 = \\begin{pmatrix} 1  1  0 \\end{pmatrix}^T$ 和 $g_2 = \\begin{pmatrix} 1  1+\\varepsilon  \\varepsilon \\end{pmatrix}^T$。当 $\\varepsilon \\to 0$ 时，$g_2 \\to g_1$，因此列向量变得线性相关。近似零空间中的一个向量将满足 $m_1 g_1 + m_2 g_2 \\approx 0$，对于小的 $\\varepsilon$ 这意味着 $m_1 \\approx -m_2$。这表明该方向接近 $(1, -1)^T$。\n\n我们测试单位向量 $v = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$。注意这个向量与 $\\varepsilon$ 无关。\n$$\nG(\\varepsilon)v = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1-1 \\\\ 1-(1+\\varepsilon) \\\\ 0-\\varepsilon \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0 \\\\ -\\varepsilon \\\\ -\\varepsilon \\end{pmatrix}\n$$\n范数的平方是：\n$$\n\\|G(\\varepsilon)v\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 (0^2 + (-\\varepsilon)^2 + (-\\varepsilon)^2) = \\frac{1}{2} (2\\varepsilon^2) = \\varepsilon^2\n$$\n在此方向上的二阶方向导数为 $D_v^2 J = 2\\varepsilon^2$。这是 $O(\\varepsilon^2)$ 阶的，符合要求。因此，我们可以选择 $v_{\\varepsilon} = v = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$。\n\n现在，我们来构造这对模型。OLS 极小值点 $\\hat{m}(\\varepsilon)$ 满足 $\\nabla J(\\hat{m}) = 0$。考虑 $J(m)$ 在 $\\hat{m}(\\varepsilon)$ 附近的泰勒展开：\n$$\nJ(m) = J(\\hat{m}) + \\nabla J(\\hat{m})^T (m-\\hat{m}) + \\frac{1}{2} (m-\\hat{m})^T (\\nabla^2 J) (m-\\hat{m}) + \\dots\n$$\n由于黑塞矩阵是常数，该展开是精确的。当 $\\nabla J(\\hat{m}) = 0$ 时：\n$$\nJ(m) - J(\\hat{m}) = \\frac{1}{2} (m-\\hat{m})^T (2 G^T G) (m-\\hat{m}) = (m-\\hat{m})^T G^T G (m-\\hat{m}) = \\|G(m-\\hat{m})\\|_2^2\n$$\n我们选择模型对为 $m_{\\star} = \\hat{m}(\\varepsilon)$ 和扰动后的模型 $m_{\\star} + \\delta v_{\\varepsilon}$，其中 $\\delta$ 是一个与 $\\varepsilon$ 无关的标量常数，且 $v_{\\varepsilon} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$。\n它们的失配差异是：\n$$\nJ(m_{\\star} + \\delta v_{\\varepsilon}) - J(m_{\\star}) = J(\\hat{m} + \\delta v_{\\varepsilon}) - J(\\hat{m}) = \\|G(\\delta v_{\\varepsilon})\\|_2^2 = \\delta^2 \\|G v_{\\varepsilon}\\|_2^2\n$$\n使用我们之前计算的结果 $\\|G v_{\\varepsilon}\\|_2^2 = \\varepsilon^2$，我们有：\n$$\nJ(m_{\\star} + \\delta v_{\\varepsilon}) - J(m_{\\star}) = \\delta^2 \\varepsilon^2\n$$\n这个差异是 $O(\\varepsilon^2)$ 阶的。因此，对于一个大小为 $\\delta$ 的固定模型扰动，当 $\\varepsilon \\to 0$ 时，失配的增加是极小的。这证实了沿方向 $v_{\\varepsilon}$ 存在一个由近似等价的极小值点构成的长而浅的谷。\n\n#### 第二部分：方差放大因子\n\n无量纲方差放大因子是 $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{m}(\\varepsilon)]\\big)$。我们必须首先推导 OLS 估计量的协方差矩阵 $\\operatorname{Cov}[\\hat{m}(\\varepsilon)]$。为简洁起见，我们将 $\\hat{m}$ 记为 $\\hat{m}(\\varepsilon)$，$G$ 记为 $G(\\varepsilon)$。\n\nOLS 估计量由正规方程给出：\n$$\n\\hat{m} = (G^T G)^{-1} G^T d\n$$\n我们将数据模型 $d = G m_{\\mathrm{true}} + n$ 代入：\n$$\n\\hat{m} = (G^T G)^{-1} G^T (G m_{\\mathrm{true}} + n) = (G^T G)^{-1} G^T G m_{\\mathrm{true}} + (G^T G)^{-1} G^T n = m_{\\mathrm{true}} + (G^T G)^{-1} G^T n\n$$\n估计量的期望是：\n$$\nE[\\hat{m}] = E[m_{\\mathrm{true}} + (G^T G)^{-1} G^T n] = m_{\\mathrm{true}} + (G^T G)^{-1} G^T E[n]\n$$\n因为噪声是零均值的 ($E[n]=0$)，所以估计量是无偏的：$E[\\hat{m}] = m_{\\mathrm{true}}$。\n\n估计量的协方差矩阵定义为 $\\operatorname{Cov}[\\hat{m}] = E[(\\hat{m} - E[\\hat{m}])(\\hat{m} - E[\\hat{m}])^T]$。\n使用上面的表达式：\n$$\n\\hat{m} - E[\\hat{m}] = (G^T G)^{-1} G^T n\n$$\n所以，\n$$\n\\operatorname{Cov}[\\hat{m}] = E\\Big[ \\big( (G^T G)^{-1} G^T n \\big) \\big( (G^T G)^{-1} G^T n \\big)^T \\Big] = E\\Big[ (G^T G)^{-1} G^T n n^T G (G^T G)^{-T} \\Big]\n$$\n由于 $G$ 是一个相对于期望而言的常数矩阵，并且 $(G^T G)^T = G^T G$，我们可以写出：\n$$\n\\operatorname{Cov}[\\hat{m}] = (G^T G)^{-1} G^T E[n n^T] G (G^T G)^{-1}\n$$\n项 $E[n n^T]$ 是噪声的协方差矩阵 $\\operatorname{Cov}[n]$。已知 $n \\sim \\mathcal{N}(0, \\sigma^2 I)$，所以 $\\operatorname{Cov}[n] = \\sigma^2 I$。\n$$\n\\operatorname{Cov}[\\hat{m}] = (G^T G)^{-1} G^T (\\sigma^2 I) G (G^T G)^{-1} = \\sigma^2 (G^T G)^{-1} (G^T G) (G^T G)^{-1} = \\sigma^2 (G^T G)^{-1}\n$$\n现在我们可以表示放大因子 $A(\\varepsilon)$：\n$$\nA(\\varepsilon) = \\sigma^{-2} \\operatorname{tr}(\\operatorname{Cov}[\\hat{m}]) = \\sigma^{-2} \\operatorname{tr}(\\sigma^2 (G^T G)^{-1}) = \\operatorname{tr}((G^T G)^{-1})\n$$\n为了计算这个，我们首先求矩阵 $G^T G$：\n$$\nG(\\varepsilon)^T G(\\varepsilon) = \\begin{pmatrix} 1  1  0 \\\\ 1  1+\\varepsilon  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 1+1  1+(1+\\varepsilon) \\\\ 1+(1+\\varepsilon)  1+(1+\\varepsilon)^2+\\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 2  2+\\varepsilon \\\\ 2+\\varepsilon  2+2\\varepsilon+2\\varepsilon^2 \\end{pmatrix}\n$$\n对于一个一般的可逆 $2 \\times 2$ 矩阵 $M = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵是 $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。逆矩阵的迹是 $\\operatorname{tr}(M^{-1}) = \\frac{a+d}{\\det(M)} = \\frac{\\operatorname{tr}(M)}{\\det(M)}$。\n将此性质应用于 $M = G^T G$：\n$$\n\\operatorname{tr}((G^T G)^{-1}) = \\frac{\\operatorname{tr}(G^T G)}{\\det(G^T G)}\n$$\n我们计算 $G^T G$ 的迹和行列式：\n$$\n\\operatorname{tr}(G^T G) = 2 + (2+2\\varepsilon+2\\varepsilon^2) = 4+2\\varepsilon+2\\varepsilon^2\n$$\n$$\n\\det(G^T G) = 2(2+2\\varepsilon+2\\varepsilon^2) - (2+\\varepsilon)^2 = (4+4\\varepsilon+4\\varepsilon^2) - (4+4\\varepsilon+\\varepsilon^2) = 3\\varepsilon^2\n$$\n将这些代入 $A(\\varepsilon)$ 的表达式中：\n$$\nA(\\varepsilon) = \\frac{4+2\\varepsilon+2\\varepsilon^2}{3\\varepsilon^2}\n$$\n这就是无量纲方差放大因子的精确闭式解析表达式。", "answer": "$$\n\\boxed{\\frac{4+2\\varepsilon+2\\varepsilon^{2}}{3\\varepsilon^{2}}}\n$$", "id": "3606781"}, {"introduction": "本综合实践练习将前述的杠杆效应和解方差概念融入一个更真实的地球物理计算模拟中。您将通过编程实现一个旅行时层析成像场景，主动识别并处理高杠杆数据点，并在此过程中探索数据分析中的一个经典两难问题：偏差-方差权衡（bias-variance tradeoff）。通过蒙特卡洛模拟，您将定量地证明，移除高杠杆（通常也是高噪声）的数据点虽然能够有效降低解的方差，但往往会以引入或增大系统性模型偏差为代价，这对于在实际工作中做出稳健的数据处理决策至关重要 [@problem_id:3606791]。", "problem": "考虑一个层状介质中的一维垂直走时实验，其中从地表到深度 $z$ 处接收器的走时等于沿路径的慢度线积分。其基本依据是走时的路径积分定义，即\n$$\nT(z) \\equiv \\int_{0}^{z} s(\\zeta)\\, d\\zeta,\n$$\n其中 $s(\\zeta)$ 是慢度场，单位为秒/米。在计算地球物理学中，通常将未知慢度按层离散化为分段常数，从而得到一个线性模型 $t \\approx G m$，其中 $t \\in \\mathbb{R}^{R}$ 是 $R$ 个接收器测得的走时向量，$G \\in \\mathbb{R}^{R \\times L}$ 是每层路径长度的设计矩阵，$m \\in \\mathbb{R}^{L}$ 是层慢度向量（单位为秒/米）。对于超定系统，最小二乘解是 $\\|G m - t\\|_2^2$ 的最小化子，当 $G^{\\top} G$ 可逆时，得到估计值\n$$\n\\hat{m} = (G^{\\top} G)^{-1} G^{\\top} t,\n$$\n杠杆分数（leverage scores）量化了拟合值对观测值的敏感度，其计算公式为\n$$\n\\ell_i = g_i^{\\top} (G^{\\top} G)^{-1} g_i,\n$$\n其中 $g_i^{\\top}$ 是 $G$ 的第 $i$ 行。\n\n您将为一个具有从地表到接收器的直线路径和分层参数化的垂直实验构建 $G$。设存在 $L$ 个层，厚度为 $h_j$（$j=1,\\dots,L$），第 $j$ 层的顶部深度为 $z_{j}^{\\mathrm{top}}$，底部深度为 $z_{j}^{\\mathrm{bot}}$。对于深度为 $z_i$ 的接收器，穿过第 $j$ 层的路径长度为 $\\ell_{ij}^{\\mathrm{path}} = \\max\\big(0, \\min(z_i, z_{j}^{\\mathrm{bot}}) - z_{j}^{\\mathrm{top}}\\big)$，因此 $G_{ij} = \\ell_{ij}^{\\mathrm{path}}$。设连续慢度为\n$$\ns(z) = s_0 + g z,\n$$\n其中 $s_0$ 和 $g$ 为常数，因此无噪声走时为\n$$\nT_i = T(z_i) = \\int_0^{z_i} (s_0 + g \\zeta) \\, d\\zeta = s_0 z_i + \\tfrac{1}{2} g z_i^2.\n$$\n将层平均目标慢度（分段常数的粗尺度真实值）定义为\n$$\nm^{\\mathrm{coarse}}_j = \\frac{1}{h_j} \\int_{z_{j}^{\\mathrm{top}}}^{z_{j}^{\\mathrm{bot}}} s(z) \\, dz = s_0 + g \\frac{z_{j}^{\\mathrm{top}} + z_{j}^{\\mathrm{bot}}}{2}.\n$$\n您将模拟每个数据点上的加性噪声，其异方差性与杠杆分数相关联，\n$$\n\\varepsilon_i \\sim \\mathcal{N}\\Big(0, \\sigma^2 \\big(1 + \\gamma \\, \\ell_i\\big)\\Big),\n$$\n以反映在实际走时数据集中，高杠杆拾取点具有更高的拾取不确定性。通过 $t_i = T_i + \\varepsilon_i$ 形成含噪数据，并使用普通最小二乘法对完整数据集和移除了最高杠杆拾取点的数据集分别估计 $\\hat{m}$。\n\n您的任务：\n- 计算 $G$ 所有行的杠杆分数 $\\ell_i$。\n- 使用指定的移除比例 $p$ 识别高杠杆拾取点：移除按 $\\ell_i$ 排序的前 $\\lceil p R \\rceil$ 行。\n- 对完整数据集和移除了高杠杆拾取点的数据集，进行 $K$ 次数据模拟和最小二乘反演的蒙特卡洛重复实验。\n- 通过分量样本方差之和来量化估计参数的方差，\n$$\n\\mathrm{VarSum} = \\sum_{j=1}^{L} \\widehat{\\mathrm{Var}}(\\hat{m}_j),\n$$\n单位为 $(\\mathrm{s}/\\mathrm{m})^2$。\n- 通过平均估计器误差的欧几里得范数来量化相对于粗尺度真实值的偏差，\n$$\n\\mathrm{BiasNorm} = \\left\\| \\mathbb{E}[\\hat{m}] - m^{\\mathrm{coarse}} \\right\\|_2,\n$$\n单位为 $\\mathrm{s}/\\mathrm{m}$，通过重复实验的样本均值来近似。\n- 通过模拟证明，移除高杠杆拾取点会降低 $\\mathrm{VarSum}$，同时增加 $\\mathrm{BiasNorm}$。\n\n使用以下科学上合理的配置和测试套件。所有量必须以国际单位制（SI）表示。慢度的单位是秒/米，深度和厚度的单位是米，走时的单位是秒。不涉及角度。测试套件指定了三种具有不同噪声尺度和移除比例的情况：\n- 所有情况共享的几何和物理参数：\n  - 层：$L = 5$，厚度 $h = [200, 200, 200, 200, 200]$ 米。\n  - 接收器深度：$R = 25$ 个接收器，深度 $z_i$ 从 40 米到 1000 米均匀分布。\n  - 连续慢度：$s_0 = 5 \\times 10^{-4}$ 秒/米，$g = 1 \\times 10^{-7}$ 秒/米$^2$。\n  - 重复次数：$K = 400$。\n- 情况 1（理想情况）：噪声尺度 $\\sigma = 1 \\times 10^{-4}$ 秒，异方差因子 $\\gamma = 20$，移除比例 $p = 0.20$。\n- 情况 2（边界条件：最小移除）：噪声尺度 $\\sigma = 5 \\times 10^{-5}$ 秒，异方差因子 $\\gamma = 10$，移除比例 $p = 0.04$。\n- 情况 3（边缘情况：强异方差性）：噪声尺度 $\\sigma = 2 \\times 10^{-4}$ 秒，异方差因子 $\\gamma = 30$，移除比例 $p = 0.30$。\n\n对于每种情况，计算：\n- $\\mathrm{VarSum}_{\\mathrm{full}}$ 和 $\\mathrm{VarSum}_{\\mathrm{removed}}$，单位为 $(\\mathrm{s}/\\mathrm{m})^2$。\n- $\\mathrm{BiasNorm}_{\\mathrm{full}}$ 和 $\\mathrm{BiasNorm}_{\\mathrm{removed}}$，单位为 $\\mathrm{s}/\\mathrm{m}$。\n- 移除的拾取点数量 $N_{\\mathrm{removed}} = \\lceil p R \\rceil$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素对应一个测试用例，并且本身也是一个列表\n$$\n[\\mathrm{VarSum}_{\\mathrm{full}}, \\mathrm{VarSum}_{\\mathrm{removed}}, \\mathrm{BiasNorm}_{\\mathrm{full}}, \\mathrm{BiasNorm}_{\\mathrm{removed}}, N_{\\mathrm{removed}}].\n$$\n例如，输出将类似于\n$[[x_1,x_2,x_3,x_4,n_1],[y_1,y_2,y_3,y_4,n_2],[z_1,z_2,z_3,z_4,n_3]]$,\n其中所有 $x_k$、$y_k$、$z_k$ 均为浮点数，$n_k$ 为整数，并以指定的国际单位制单位报告。不应打印任何额外文本。", "solution": "我们从连续变化介质中走时的路径积分定律开始，该定律指出，到深度 $z$ 处接收器的走时 $T(z)$ 等于沿路径的慢度积分，$T(z) = \\int_{0}^{z} s(\\zeta) \\, d\\zeta$，其中 $s(\\zeta)$ 是慢度，单位为秒/米。对于分层离散化，其中第 $j$ 层的慢度为分段常数 $m_j$，且路径为从地表开始的垂直路径，到接收器 $i$ 的测量走时被建模为线性组合 $t_i \\approx \\sum_{j=1}^{L} G_{ij} m_j$，其中 $G_{ij}$ 是到接收器 $i$ 的路径穿过第 $j$ 层的路径长度。由于路径是垂直且直的，$G_{ij}$ 是区间 $[0, z_i]$ 与区间 $[z_j^{\\mathrm{top}}, z_j^{\\mathrm{bot}}]$ 的交集长度，即 $G_{ij} = \\max\\big(0, \\min(z_i, z_{j}^{\\mathrm{bot}}) - z_{j}^{\\mathrm{top}}\\big)$。将所有接收器堆叠起来，得到 $t \\in \\mathbb{R}^{R}$ 和 $G \\in \\mathbb{R}^{R \\times L}$。\n\n最小二乘解是通过最小化二次目标函数 $J(m) = \\|G m - t\\|_2^2$ 得到的，其关于 $m$ 的梯度为 $\\nabla J(m) = 2 G^{\\top} (G m - t)$。令梯度为零，得到正规方程组 $G^{\\top} G \\, \\hat{m} = G^{\\top} t$。当 $G^{\\top} G$ 可逆时，唯一的最小化子是 $\\hat{m} = (G^{\\top} G)^{-1} G^{\\top} t$。当线性模型被正确指定且噪声均值为零时，该估计量是无偏的。然而，在计算地球物理学中，由于对连续场进行粗略参数化，模型误设是很常见的。在我们的设置中，连续慢度为 $s(z) = s_0 + g z$，因此无噪声走时为 $T(z) = s_0 z + \\frac{1}{2} g z^2$。粗尺度层平均慢度（物理目标）为 $m^{\\mathrm{coarse}}_j = \\frac{1}{h_j} \\int_{z_j^{\\mathrm{top}}}^{z_j^{\\mathrm{bot}}} (s_0 + g z) \\, dz = s_0 + g \\, \\frac{z_j^{\\mathrm{top}} + z_j^{\\mathrm{bot}}}{2}$。\n\n因为连续走时 $T$ 并不精确地位于每层 $m$ 为常数的 $G m$ 的列空间中（除非 $g = 0$），所以在无噪声数据下的最小二乘解返回一个伪真参数 $m^{\\star}$，它求解 $G^{\\top} G \\, m^{\\star} = G^{\\top} T$，而这通常与 $m^{\\mathrm{coarse}}$ 不同。这种差异是由离散化和接收器采样引起的结构性偏差。每个观测值的杠杆分数（leverage score）量化了其对拟合值的影响，对于 $G$ 的第 $i$ 行 $g_i^{\\top}$，其定义为 $\\ell_i = g_i^{\\top} (G^{\\top} G)^{-1} g_i$。投影（帽子）矩阵 $H = G (G^{\\top} G)^{-1} G^{\\top}$ 的对角线包含了这些分数，高杠杆行是那些 $\\ell_i$ 相对于其他行较大的行。在走时反演中，更深的接收器具有更长的路径，并且可能是高杠杆的，因为它们的 $g_i$ 向量具有较大的量级和跨越各层的不同方向贡献。\n\n为了将杠杆分数与不确定性联系起来，我们考虑异方差拾取噪声，其方差随杠杆分数增加而增加：$\\varepsilon_i \\sim \\mathcal{N}\\big(0, \\sigma^2 (1 + \\gamma \\ell_i)\\big)$。当 $T$ 平均上精确等于 $G m$ 时，普通最小二乘法下的估计量对于线性模型仍然是无偏的，但存在结构性误设（对于任何 $m$，$T \\neq G m$）时，期望估计量等于 $m^{\\star}$，而不是 $m^{\\mathrm{coarse}}$。此外，重尾或异方差噪声会使 $\\hat{m}$ 的抽样方差膨胀，这尤其归因于高杠杆观测值。因此，移除高杠杆行可以减少 $\\hat{m}$ 的方差，因为它消除了同时具有大杠杆值和大噪声方差的观测值；然而，移除操作降低了设计矩阵 $G$ 的多样性，通常会通过改变 $G^{\\top} T$ 和 $G^{\\top} G$ 的方式恶化结构性偏差，从而降低对更深层的敏感度。这种权衡是模型误设的超定最小二乘问题中经典的方差-偏差相互作用。\n\n算法步骤：\n1. 为 $L = 5$ 且每层厚度 $h_j = 200$ 米构建地层深度，因此 $z_1^{\\mathrm{top}} = 0$，$z_1^{\\mathrm{bot}} = 200$，$z_2^{\\mathrm{top}} = 200$，以此类推，直到 $z_5^{\\mathrm{bot}} = 1000$ 米。构建 $R = 25$ 个接收器深度 $z_i$，从 40 米到 1000 米均匀分布。\n2. 构建矩阵 $G$，其元素为 $G_{ij} = \\max\\big(0, \\min(z_i, z_j^{\\mathrm{bot}}) - z_j^{\\mathrm{top}}\\big)$。\n3. 计算所有行的杠杆分数 $\\ell_i = g_i^{\\top} (G^{\\top} G)^{-1} g_i$。\n4. 对于每个测试用例的参数 $(\\sigma, \\gamma, p)$，确定 $N_{\\mathrm{removed}} = \\lceil p R \\rceil$ 并移除 $\\ell_i$ 最大的行以创建 $G_{\\mathrm{removed}}$ 和相应的索引集。\n5. 对于 $K = 400$ 次蒙特卡洛重复实验，为完整数据集和移除行的数据集生成含噪数据 $t = T + \\varepsilon$（在完整集上重用相同的 $\\varepsilon$ 实现，并为移除集对其进行子选择）。\n6. 拟合 $\\hat{m}_{\\mathrm{full}} = (G^{\\top} G)^{-1} G^{\\top} t$ 和 $\\hat{m}_{\\mathrm{removed}} = (G_{\\mathrm{removed}}^{\\top} G_{\\mathrm{removed}})^{-1} G_{\\mathrm{removed}}^{\\top} t_{\\mathrm{removed}}$。\n7. 计算每个分量在重复实验中的样本方差，并求和得到 $\\mathrm{VarSum}_{\\mathrm{full}}$ 和 $\\mathrm{VarSum}_{\\mathrm{removed}}$，单位为 $(\\mathrm{s}/\\mathrm{m})^2$。\n8. 计算完整集和移除集在重复实验中 $\\hat{m}$ 的样本均值，并计算其与 $m^{\\mathrm{coarse}}$ 之差的欧几里得范数，以获得 $\\mathrm{BiasNorm}_{\\mathrm{full}}$ 和 $\\mathrm{BiasNorm}_{\\mathrm{removed}}$，单位为 $\\mathrm{s}/\\mathrm{m}$。\n\n测试套件详情：\n- 共享的几何和物理参数：\n  - $L = 5$，$h = [200, 200, 200, 200, 200]$ 米。\n  - $R = 25$ 个接收器，$z_i$ 从 40 米到 1000 米均匀分布。\n  - $s_0 = 5 \\times 10^{-4}$ 秒/米，$g = 1 \\times 10^{-7}$ 秒/米$^2$。\n  - 重复次数：$K = 400$。\n- 情况 1：$\\sigma = 1 \\times 10^{-4}$ 秒，$\\gamma = 20$，$p = 0.20$。\n- 情况 2：$\\sigma = 5 \\times 10^{-5}$ 秒，$\\gamma = 10$，$p = 0.04$。\n- 情况 3：$\\sigma = 2 \\times 10^{-4}$ 秒，$\\gamma = 30$，$p = 0.30$。\n\n最终输出规范：\n您的程序应生成单行输出，格式为\n$[[\\mathrm{VarSum}_{\\mathrm{full}},\\mathrm{VarSum}_{\\mathrm{removed}},\\mathrm{BiasNorm}_{\\mathrm{full}},\\mathrm{BiasNorm}_{\\mathrm{removed}},N_{\\mathrm{removed}}],\\dots]$\n针对三种情况，按情况 1、情况 2、情况 3 的顺序排列。所有值必须使用上述指定的国际单位制单位，方差和偏差报告为浮点数，计数 $N_{\\mathrm{removed}}$ 报告为整数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_layers(thicknesses):\n    \"\"\"Return arrays of layer top and bottom depths.\"\"\"\n    z_top = np.cumsum([0] + thicknesses[:-1])\n    z_bot = np.cumsum(thicknesses)\n    return z_top.astype(float), z_bot.astype(float)\n\ndef build_design_matrix(z_receivers, z_top, z_bot):\n    \"\"\"\n    Build G with G[i, j] = intersection length of [0, z_i] with [z_top[j], z_bot[j]].\n    \"\"\"\n    R = len(z_receivers)\n    L = len(z_top)\n    G = np.zeros((R, L), dtype=float)\n    for i, z in enumerate(z_receivers):\n        # intersection length: max(0, min(z, z_bot[j]) - z_top[j])\n        G[i, :] = np.maximum(0.0, np.minimum(z, z_bot) - z_top)\n    return G\n\ndef true_travel_times(z_receivers, s0, g):\n    \"\"\"Compute T_i = s0*z_i + 0.5*g*z_i^2.\"\"\"\n    z = np.asarray(z_receivers, dtype=float)\n    return s0 * z + 0.5 * g * z**2\n\ndef coarse_layer_slowness(z_top, z_bot, s0, g):\n    \"\"\"Compute m_coarse_j = s0 + g * (z_top_j + z_bot_j)/2.\"\"\"\n    return s0 + g * (z_top + z_bot) / 2.0\n\ndef leverage_scores(G):\n    \"\"\"Compute leverage scores l_i = g_i^T (G^T G)^{-1} g_i.\"\"\"\n    GTG = G.T @ G\n    # Use a robust inverse; GTG should be SPD; fallback to pseudo-inverse for numerical robustness\n    GTG_inv = np.linalg.pinv(GTG)\n    # row-wise leverage: diag(G @ GTG_inv @ G^T)\n    H = G @ GTG_inv @ G.T\n    return np.diag(H)\n\ndef ls_estimate(G, t):\n    \"\"\"Ordinary least-squares estimate (G^T G)^{-1} G^T t.\"\"\"\n    GTG = G.T @ G\n    GTt = G.T @ t\n    # Use pseudo-inverse for robustness\n    m_hat = np.linalg.pinv(GTG) @ GTt\n    return m_hat\n\ndef simulate_case(G, T_true, leverages, m_coarse, sigma, gamma, frac_remove, K, rng):\n    \"\"\"\n    Simulate K replicates for full and removed datasets.\n    Returns VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed.\n    \"\"\"\n    R = G.shape[0]\n    L = G.shape[1]\n    # Determine removal set: top ceil(frac_remove * R) by leverage\n    N_removed = int(np.ceil(frac_remove * R))\n    order = np.argsort(-leverages)  # descending by leverage\n    remove_idx = order[:N_removed]\n    keep_mask = np.ones(R, dtype=bool)\n    keep_mask[remove_idx] = False\n    G_removed = G[keep_mask, :]\n\n    # Preallocate arrays to store estimates\n    m_full_all = np.zeros((K, L), dtype=float)\n    m_removed_all = np.zeros((K, L), dtype=float)\n\n    # Noise std per observation (heteroscedastic)\n    std_i = sigma * np.sqrt(1.0 + gamma * leverages)\n\n    for k in range(K):\n        eps = rng.normal(loc=0.0, scale=std_i, size=R)\n        t_noisy = T_true + eps\n        # Full estimate\n        m_full = ls_estimate(G, t_noisy)\n        m_full_all[k, :] = m_full\n        # Removed estimate\n        t_removed = t_noisy[keep_mask]\n        m_removed = ls_estimate(G_removed, t_removed)\n        m_removed_all[k, :] = m_removed\n\n    # Variance sums across components\n    var_full = m_full_all.var(axis=0, ddof=1)  # sample variance per component\n    var_removed = m_removed_all.var(axis=0, ddof=1)\n    VarSum_full = float(var_full.sum())\n    VarSum_removed = float(var_removed.sum())\n\n    # Bias norms: norm of mean(m_hat) - m_coarse\n    mean_full = m_full_all.mean(axis=0)\n    mean_removed = m_removed_all.mean(axis=0)\n    BiasNorm_full = float(np.linalg.norm(mean_full - m_coarse))\n    BiasNorm_removed = float(np.linalg.norm(mean_removed - m_coarse))\n\n    return VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed\n\ndef solve():\n    # Geometry and physics shared across cases\n    thicknesses = [200, 200, 200, 200, 200]  # meters\n    L = len(thicknesses)\n    z_top, z_bot = build_layers(thicknesses)\n    # Receiver depths: 25 receivers from 40 m to 1000 m\n    R = 25\n    z_receivers = np.linspace(40.0, 1000.0, R)\n    # Continuous slowness parameters\n    s0 = 5e-4  # s/m\n    g = 1e-7   # s/m^2\n    # True travel-times and coarse slowness\n    T_true = true_travel_times(z_receivers, s0, g)\n    m_coarse = coarse_layer_slowness(z_top, z_bot, s0, g)\n    # Design matrix\n    G = build_design_matrix(z_receivers, z_top, z_bot)\n    # Leverage scores\n    leverages = leverage_scores(G)\n\n    # Replicates\n    K = 400\n    rng = np.random.default_rng(seed=42)\n\n    # Test cases: (sigma, gamma, frac_remove)\n    test_cases = [\n        (1e-4, 20.0, 0.20),  # Case 1: happy path\n        (5e-5, 10.0, 0.04),  # Case 2: boundary minimal removal\n        (2e-4, 30.0, 0.30),  # Case 3: strong heteroscedasticity\n    ]\n\n    results = []\n    for sigma, gamma, frac_remove in test_cases:\n        VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed = simulate_case(\n            G, T_true, leverages, m_coarse, sigma, gamma, frac_remove, K, rng\n        )\n        results.append([VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed])\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\"'\", '\"'))\n\nsolve()\n```", "id": "3606791"}]}