## 引言
在[计算地球物理学](@entry_id:747618)中，反演问题是从间接的、带有噪声的观测数据中推断地球内部物理属性的核心任务。然而，这些问题在数学上常常是“不适定的”，直接求解往往导致不稳定且缺乏物理意义的结果。这构成了一个关键的知识鸿沟：我们如何才能约束这些[不适定问题](@entry_id:182873)，以获得既能拟合数据又符合地质现实的可靠模型？本文旨在系统性地解答这一问题，聚焦于[吉洪诺夫正则化](@entry_id:140094)及其相关的阻尼策略——这是解决[不适定问题](@entry_id:182873)最强大、最基础的工具之一。通过本文的学习，读者将深入理解如何利用[先验信息](@entry_id:753750)来稳定反演过程。文章结构如下：第一章“原理与机制”将深入剖析[不适定性](@entry_id:635673)的本质，并从数学、贝叶斯和[谱域](@entry_id:755169)滤波等多个角度阐述[吉洪诺夫正则化](@entry_id:140094)的核心思想。第二章“应用与跨学科联系”将展示正则化在[地球物理联合反演](@entry_id:750951)、[时移](@entry_id:261541)监测等前沿应用，并揭示其与[机器人学](@entry_id:150623)、[量子化学](@entry_id:140193)等领域的深刻联系。最后，在“动手实践”部分，读者将通过具体编程练习，将理论知识转化为解决实际问题的能力。

## 原理与机制

在上一章中，我们介绍了[地球物理反演](@entry_id:749866)问题的基本框架，即从观测数据中推断地球内部的模型参数。然而，许多这类问题本质上是“不适定的”，这意味着对解的直接求解不仅困难，而且常常导致无物理意义的结果。本章将深入探讨[不适定性](@entry_id:635673)的根源，并系统地阐述[吉洪诺夫正则化](@entry_id:140094)（Tikhonov regularization）作为一种核心的阻尼策略，如何从原理上应对这些挑战，并提供稳定且有意义的解。

### [不适定问题](@entry_id:182873)的挑战

一个数学问题，若其解存在、唯一，且连续依赖于输入数据，则称之为**适定的（well-posed）**。法国数学家 Hadamard 提出的这三条标准——**存在性 (existence)**、**唯一性 (uniqueness)** 和 **稳定性 (stability)**——构成了衡量问题“良性”程度的基石。反之，任何一条标准未能满足，问题即为**不适定的（ill-posed）**。[@problem_id:3617437]

在[地球物理反演](@entry_id:749866)中，我们求解的[线性系统](@entry_id:147850) $G m \approx d$ 常常违反一条或多条[适定性](@entry_id:148590)准则：

1.  **存在性**：由于测量噪声的存在，观测数据 $d$ 很可能不精确地位于算子 $G$ 的值域（[列空间](@entry_id:156444)）内。因此，严格意义上的解 $m$（使得 $G m = d$）可能不存在。我们通常退而求其次，寻找一个[最小二乘解](@entry_id:152054)，使数据残差 $\|G m - d\|$ 最小化。

2.  **唯一性**：地球物理观测往往是有限且不完整的（例如，有限的台站覆盖、有限的采集孔径）。这导致模型参数的数量（$m$ 的维度）常常超过独立观测的数量（$G$ 的秩）。此时，$G$ 拥有一个非平凡的**零空间（null space）**，即存在非零模型 $m_0$ 使得 $G m_0 = 0$。若 $m^*$ 是一个解，那么 $m^* + m_0$ 同样是一个能完美拟合数据的解，从而破坏了[解的唯一性](@entry_id:143619)。

3.  **稳定性**：这是[地球物理反演](@entry_id:749866)中最棘手的问题。许多物理过程，如重力、[磁场](@entry_id:153296)或热扩散，其数学描述都是[积分变换](@entry_id:186209)。这些变换具有天然的**平滑效应**：模型 $m$ 中的高频（短波长）细节在传播到观测数据 $d$ 的过程中会被强烈衰减。这意味着正演算子 $G$ 是一个“平滑”算子，其数学本质是一个**[紧算子](@entry_id:139189)（compact operator）**。

为了更深刻地理解稳定性的缺失，我们可以借助**奇异值分解（Singular Value Decomposition, SVD）**来审视算子 $G$。离散算子 $G$ 可被分解为 $G = U \Sigma V^T$，其中 $U$ 和 $V$ 是正交矩阵，$\Sigma$ 是包含奇异值 $\sigma_i$ 的对角矩阵。[奇异值](@entry_id:152907)按大小[排列](@entry_id:136432)，$\sigma_1 \ge \sigma_2 \ge \dots \ge 0$。[紧算子](@entry_id:139189)的一个决定性特征是其奇异值会衰减并趋向于零。

形式上，反演问题的解可以表示为 $m = G^\dagger d$，其中 $G^\dagger = V \Sigma^\dagger U^T$ 是 $G$ 的[伪逆](@entry_id:140762)。$m$ 的展开式为：
$$ m = \sum_i \frac{u_i^T d}{\sigma_i} v_i $$
其中 $u_i$ 和 $v_i$ 分别是 $U$ 和 $V$ 的列向量。当数据 $d$ 受到微小扰动（噪声）$\delta d$ 时，解的扰动为 $\delta m = \sum_i \frac{u_i^T \delta d}{\sigma_i} v_i$。即使噪声 $\delta d$ 的幅度很小，如果它包含了与小[奇异值](@entry_id:152907) $\sigma_i$ 对应的分量，那么 $1/\sigma_i$ 这一项会变得极大，从而导致解 $m$ 发生剧烈变化。这就是稳定性的失效：**数据中的微小噪声被小奇异值极度放大，污染了解**。[@problem_id:3617437] 这种对噪声的敏感性使得直接反演的结果充满了高频[振荡](@entry_id:267781)，缺乏物理意义。

### [吉洪诺夫正则化](@entry_id:140094)：核心思想

为了克服[不适定性](@entry_id:635673)，我们需要引入额外信息来约束解空间，使其稳定下来。[吉洪诺夫正则化](@entry_id:140094)是一种强大而经典的方法，其核心思想是在最小化数据 misfit 的同时，增加一个惩罚项来约束模型的某些属性。这种方法在统计学中也称为**岭回归（ridge regression）**。

最基本的**零阶[吉洪诺夫正则化](@entry_id:140094)（zero-order Tikhonov regularization）**的[目标函数](@entry_id:267263) $\Phi(m)$ 定义为：
$$ \min_{m} \Phi(m) = \| G m - d \|_{2}^{2} + \lambda^{2} \| m \|_{2}^{2} $$
这个目标函数由两部分组成：
*   **数据 misfit 项**：$\| G m - d \|_{2}^{2}$，衡量模型预测数据 $G m$ 与观测数据 $d$ 之间的一致性。
*   **模型惩罚项（或正则化项）**：$\| m \|_{2}^{2}$，惩罚模型向量 $m$ 的[欧几里得范数](@entry_id:172687)。它体现了一种先验假设：我们偏好一个“小”的解。

**[正则化参数](@entry_id:162917)（regularization parameter）** $\lambda > 0$ 是一个关键的**权衡参数（trade-off parameter）**。
*   当 $\lambda \to 0$ 时，目标函数退化为无约束的[最小二乘问题](@entry_id:164198)，解会过度拟合数据，包含大量噪声。
*   当 $\lambda \to \infty$ 时，为了使目标函数最小，模型惩罚项必须趋于零，导致解 $m \to 0$，完全忽略了数据。

通过选择一个合适的 $\lambda$，我们可以在数据拟合与模型简约性之间取得平衡。

更一般的形式是引入数据和模型的权重，以及一个**参考模型（reference model）** $m_{\mathrm{ref}}$。[@problem_id:3617418] 此时，目标函数变为：
$$ \min_{m} \Phi(m) = \| W_d (G m - d) \|_{2}^{2} + \lambda^{2} \| W_m (m - m_{\mathrm{ref}}) \|_{2}^{2} $$
其中，$W_d$ 是**[数据加权](@entry_id:635715)矩阵**，通常用于处理非均匀的数据噪声（例如 $W_d^T W_d = C_d^{-1}$，其中 $C_d$ 是[数据协方差](@entry_id:748192)矩阵）。$W_m$ 是**模型加权矩阵**，用于对不同模型参数施加不同的惩罚。$m_{\mathrm{ref}}$ 代表我们对模型的先验猜测。在这种形式下，正则化项惩罚的是模型与参考模型之间的偏差。

求解上述[目标函数](@entry_id:267263)的最小值，可通过令其关于 $m$ 的梯度为零得到**正则化[正规方程](@entry_id:142238)（regularized normal equations）**。对于 $W_d = I$, $W_m = I$, $m_{\mathrm{ref}} = 0$ 的简单情况，我们得到：
$$ (G^T G + \lambda^2 I) m_{\lambda} = G^T d $$
这里的 $m_\lambda$ 表示依赖于 $\lambda$ 的正则化解。与原始[正规方程](@entry_id:142238) $(G^T G) m = G^T d$ 相比，正则化的关键在于向矩阵 $G^T G$ 的对角线添加了一个正的“阻尼”项 $\lambda^2 I$。即使 $G^T G$ 是奇[异或](@entry_id:172120)病态的（含有零或非常小的[特征值](@entry_id:154894)），$G^T G + \lambda^2 I$ 也是可逆且良态的，从而保证了解的稳定存在。

### 正则化的贝叶斯基础

[吉洪诺夫正则化](@entry_id:140094)看似一种巧妙的数学技巧，但它背后有深刻的[概率论基础](@entry_id:158925)，可以通过**贝叶斯（Bayesian）**反演框架来理解。[@problem_id:3617510] 在贝叶斯观点中，我们不仅考虑数据如何由模型生成，还为模型本身赋予一个[概率分布](@entry_id:146404)，称为**先验（prior）**。

根据[贝叶斯定理](@entry_id:151040)，模型 $m$ 的**[后验概率](@entry_id:153467)[分布](@entry_id:182848)（posterior probability distribution）** $p(m|d)$ 正比于**[似然函数](@entry_id:141927)（likelihood）** $p(d|m)$ 与**先验概率[分布](@entry_id:182848)（prior probability distribution）** $p(m)$ 的乘积：
$$ p(m|d) \propto p(d|m) p(m) $$
**最大后验估计（Maximum A Posteriori, MAP）**旨在寻找使后验概率最大化的模型 $m_{\text{MAP}}$。这等价于最小化负对数后验概率：
$$ m_{\text{MAP}} = \arg\min_{m} [ - \ln p(d|m) - \ln p(m) ] $$
现在我们来考察[高斯假设](@entry_id:170316)下的具体形式：
1.  **[高斯噪声](@entry_id:260752)假设（似然）**：假设数据噪声 $\varepsilon = d - Gm$ 服从均值为零、协[方差](@entry_id:200758)为 $C_d$ 的[高斯分布](@entry_id:154414)，即 $\varepsilon \sim \mathcal{N}(0, C_d)$。[似然函数](@entry_id:141927)为：
    $$ p(d|m) \propto \exp\left(-\frac{1}{2} (Gm - d)^T C_d^{-1} (Gm - d)\right) $$
    其负对数（忽略常数）为 $\frac{1}{2} \| C_d^{-1/2} (Gm - d) \|_{2}^{2}$。这正是加权的数据 misfit 项。

2.  **[高斯先验](@entry_id:749752)假设（先验）**：假设模型 $m$ 本身服从一个以 $m_0$ 为均值、协[方差](@entry_id:200758)为 $C_m$ 的[高斯先验](@entry_id:749752)[分布](@entry_id:182848)，即 $m \sim \mathcal{N}(m_0, C_m)$。先验概率密度为：
    $$ p(m) \propto \exp\left(-\frac{1}{2} (m - m_0)^T C_m^{-1} (m - m_0)\right) $$
    其负对数（忽略常数）为 $\frac{1}{2} (m - m_0)^T C_m^{-1} (m - m_0)$。

将这两项结合，MAP [目标函数](@entry_id:267263)变为：
$$ J(m) = (Gm - d)^T C_d^{-1} (Gm - d) + (m - m_0)^T C_m^{-1} (m - m_0) $$
通过对比这个 MAP [目标函数](@entry_id:267263)与广义[吉洪诺夫正则化](@entry_id:140094)[目标函数](@entry_id:267263)，我们可以建立一个精确的对应关系。如果 Tikhonov 惩罚项写为 $\lambda^2 \| L(m - m_0) \|_{2}^{2} = \lambda^2 (m-m_0)^T L^T L (m-m_0)$，那么我们得到：
$$ C_m^{-1} = \lambda^2 L^T L $$
这个等式意义重大：它表明[吉洪诺夫正则化](@entry_id:140094)等价于假设模型服从一个[高斯先验](@entry_id:749752)[分布](@entry_id:182848)，其**先验[精度矩阵](@entry_id:264481)（inverse prior covariance）**由 $\lambda^2 L^T L$ 定义。[@problem_id:3617510] 这种对应关系将正则化从一个纯粹的数值稳定技巧提升为一个基于[概率推理](@entry_id:273297)的 principled 方法。它赋予了正则化算子 $L$ 和参数 $\lambda$ 清晰的统计学含义：它们共同定义了我们对模型结构和确定性的先验信念。

此外，在这个框架下，模型的后验不确定性也可以被量化。解的**[后验协方差矩阵](@entry_id:753631)** $C_{\text{post}}$ 由目标函数二次型的 Hessian 矩阵的逆给出：
$$ C_{\text{post}} = (G^T C_d^{-1} G + C_m^{-1})^{-1} = (G^T C_d^{-1} G + \lambda^2 L^T L)^{-1} $$
这个矩阵的对角线元素给出了每个模型参数的后验[方差](@entry_id:200758)，为我们评估解的可靠性提供了定量依据。

### 广义正则化与 L 算子的作用

标准形式的[吉洪诺夫正则化](@entry_id:140094)（$L=I$）惩罚模型的范数，偏好“小”模型。然而，我们往往拥有关于模型结构的更具体先验知识，例如模型应该是平滑的。**广义[吉洪诺夫正则化](@entry_id:140094)（General-form Tikhonov regularization）**通过引入一个[线性算子](@entry_id:149003) $L$ 来编码这些结构性先验。

*   当 $L=I$（[单位矩阵](@entry_id:156724)）时，为**零阶正则化**，惩罚模型本身的大小。
*   当 $L$ 是一个[一阶导数](@entry_id:749425)算子（例如[一阶差分](@entry_id:275675)算子）时, 为**一阶正则化**, 惩罚模型的梯度, 从而偏好**平[滑模](@entry_id:263630)型**。
*   当 $L$ 是[拉普拉斯算子](@entry_id:146319)（二阶差分算子）时, 为**二阶正则化**, 惩罚模型的曲率, 偏好**线性变化**的模型。

$L$ 算子的选择直接影响着正则化如何解决唯一性问题。任何模型 $m$ 都可以被[正交分解](@entry_id:148020)为两部分：一部分位于 $G^T$ 的值域 $\mathcal{R}(G^T)$，另一部分位于 $G$ 的[零空间](@entry_id:171336) $\mathcal{N}(G)$。数据 misfit 项 $\|Gm-d\|_2^2$ 只对 $\mathcal{R}(G^T)$ 中的分量敏感，而对[零空间](@entry_id:171336)分量完全“盲目”。因此，约束零空间分量的任务完全落在了正则化项 $\|L m\|_2^2$ 的肩上。[@problem_id:3617531]

正则化项通过在 $G$ 的[零空间](@entry_id:171336)中寻找一个使 $\|L m\|_2^2$ 最小的分量来确定唯一的解。这要求 $L$ 算子能够“看见”$G$ 所“看不见”的模型部分。数学上，要保证吉洪诺夫问题有唯一解，必须满足条件：
$$ \mathcal{N}(G) \cap \mathcal{N}(L) = \{ 0 \} $$
这意味着 $G$ 和 $L$ 不能有共同的[零空间](@entry_id:171336)向量。换言之，任何不影响数据的模型变化（即在 $\mathcal{N}(G)$ 中），都必须被正则化项 $L$ 所惩罚。[@problem_id:3617531] 例如，如果 $G$ 对模型的常数偏移不敏感（即常数向量在 $\mathcal{N}(G)$ 中），而我们选择 $L$ 为[梯度算子](@entry_id:275922)，由于常数[向量的梯度](@entry_id:188005)为零（即常数向量也在 $\mathcal{N}(L)$ 中），那么解的常数偏移依然无法确定，唯一性问题并未完全解决。

### [谱域](@entry_id:755169)视角：作为滤波器的正则化

理解正则化作用的另一个强大视角是将其视为一个**[谱域](@entry_id:755169)滤波器**。[@problem_id:3617459] 让我们回到 SVD 分解。对于 $L=I$ 的情况，Tikhonov 解 $m_\lambda$ 在 SVD 基下的系数可以表示为：
$$ m_{\lambda} = \sum_i \phi_i(\lambda) \left( \frac{u_i^T d}{\sigma_i} \right) v_i $$
其中，$\phi_i(\lambda)$ 被称为**滤波因子（filter factors）**：
$$ \phi_i(\lambda) = \frac{\sigma_i^2}{\sigma_i^2 + \lambda^2} $$
这个表达式非常直观：
*   对于与大奇异值 $\sigma_i \gg \lambda$ 相关联的模式，$\phi_i(\lambda) \approx 1$。这些模式被认为是信号主导的，几乎被完整保留。
*   对于与小奇异值 $\sigma_i \ll \lambda$ 相关联的模式，$\phi_i(\lambda) \approx \sigma_i^2 / \lambda^2 \approx 0$。这些模式被认为是噪声主导的，被强烈抑制。

因此，[吉洪诺夫正则化](@entry_id:140094)本质上是一个**低通滤波器**。它允许由数据良好约束的“信号”分量通过，同时衰减由数据弱约束并被[噪声污染](@entry_id:188797)的“噪声”分量。正则化参数 $\lambda$ 的作用就是设定这个滤波器的**[截止频率](@entry_id:276383)**。$\lambda$ 越大，[截止频率](@entry_id:276383)越低，滤波作用越强，得到的模型越平滑，分辨率越低。

以位场[向上延拓](@entry_id:756371)为例，正演算子在傅里叶域的表现为 $\widehat{G}(k) = \exp(-|k|h)$，其中 $k$ 是波数，$h$ 是延拓高度。高[波数](@entry_id:172452)（高频）信息被指数衰减，对应于 SVD 中的小[奇异值](@entry_id:152907)。零阶正则化（$L=I$）的滤波因子变为 $\varphi(k) = \frac{\exp(-2|k|h)}{\exp(-2|k|h) + \lambda^2}$。通过设定一个截止阈值（如[半功率点](@entry_id:267416) $\varphi(k_c) = 1/2$），我们可以直接将 $\lambda$ 与一个**截止[波数](@entry_id:172452)** $k_c$ 或空间**分辨率** $\ell_c \approx 2\pi/k_c$ 联系起来。例如，可以推导出 $k_c = \frac{1}{h} \ln(1/\lambda)$。这为选择 $\lambda$ 提供了一个物理上的直观感受：选择 $\lambda$ 就等同于选择你信任的模型解的分辨率。[@problem_id:3617459] [@problem_id:3617467]

### 选择阻尼参数 λ

如何选择最优的 $\lambda$ 是正则化实践中的核心问题。选择过小会导致噪声放大，选择过大则会[过度平滑](@entry_id:634349)，丢失真实信号。下面介绍两种经典方法。

#### L-曲线法 (L-curve)

L-曲线是一种广泛应用的[启发式方法](@entry_id:637904)。它在一个对数-对数[坐标系](@entry_id:156346)中，绘制出一系列不同 $\lambda$ 值对应的解的正则化项范数 $\|L m_\lambda\|_2$（y轴）与数据 misfit 范数 $\|G m_\lambda - d\|_2$（x轴）之间的关系。[@problem_id:3617467]

这条曲线通常呈现出特征性的 “L” 形。
*   **曲线的垂直部分**：对应于小的 $\lambda$ 值。此时，解过度拟合噪声，数据 misfit 很小，但模型范数很大且变化剧烈。
*   **曲线的水平部分**：对应于大的 $\lambda$ 值。此时，解被[过度平滑](@entry_id:634349)，模型范数很小，但与数据的拟合很差。
*   **曲线的拐角（corner）**：L-曲线的拐角处被认为是最佳的[平衡点](@entry_id:272705)。在这个点上，数据 misfit 和模型范数都相对较小。从这个点出发，试图进一步减小数据 misfit（向左移动）会导致模型范数的急剧增加；而试图进一步减小模型范数（向下移动）会导致数据 misfit 的急剧增加。因此，拐角代表了**[偏差-方差权衡](@entry_id:138822)（bias-variance trade-off）**的最佳位置。

L-曲线法的好处在于它不需要关于数据噪声的先验知识，仅依赖于解的几何特性。

#### 莫洛佐夫差异原理 (Morozov's Discrepancy Principle)

与 L-曲线的几何启发不同，差异原理是一种基于统计的方法，它要求我们对数据噪声的水平有一个估计。[@problem_id:3617478] 其核心思想是：一个好的解不应该拟合数据“好于”噪声水平。如果数据 misfit 远小于噪声水平，说明模型正在拟合噪声，即过拟合。

假设数据噪声 $\varepsilon$ 服从 $\mathcal{N}(0, C_d)$ [分布](@entry_id:182848)。我们可以通过[白化变换](@entry_id:637327) $\tilde{\varepsilon} = C_d^{-1/2} \varepsilon$ 得到一个服从标准正态分布 $\mathcal{N}(0, I)$ 的白化噪声向量。其平方范数的[期望值](@entry_id:153208)为：
$$ E[\| \tilde{\varepsilon} \|_{2}^{2}] = E[\| C_d^{-1/2} \varepsilon \|_{2}^{2}] = N $$
其中 $N$ 是数据的数量。这是因为 $\| \tilde{\varepsilon} \|_{2}^{2}$ 是 $N$ 个独立的标准正态[随机变量](@entry_id:195330)的平方和，服从自由度为 $N$ 的[卡方分布](@entry_id:165213)（$\chi_N^2$）。

差异原理即选择 $\lambda$ 使得白化残差的范数与这个[期望值](@entry_id:153208)相等：
$$ \| C_d^{-1/2} (G m_{\lambda} - d) \|_{2}^{2} \approx N $$
这个准则确保了解对数据的拟合程度与已知的噪声统计特性相符，从而有效避免了过拟合。

### 实用阻尼策略与扩展

#### [联合反演](@entry_id:750950)中的参数缩放

在**[联合反演](@entry_id:750950)（joint inversion）**中，我们常常需要同时求解具有不同物理单位和数值尺度的参数，例如电导率 $\sigma$（单位 S/m）和密度 $\rho$（单位 kg/m³）。[@problem_id:3617422] 在这种情况下，直接应用 Tikhonov 正则化是不恰当的，因为一个统一的 $\lambda$ 会不成比例地惩罚数值较大的参数。

为了实现平衡的正则化，必须对目标函数进行**[无量纲化](@entry_id:136704)（nondimensionalization）**。这通常通过在 $L$ 算子中引入参数特定的缩放因子来实现。一个科学合理的策略是将正则化项构建为无量纲量。例如，对于一阶正则化，我们可以定义一个分块的 $L$ 算子：
$$ L = \begin{bmatrix} \alpha_\sigma D  0 \\ 0  \alpha_\rho D \end{bmatrix}, \quad \text{其中 } \alpha_i = \frac{\ell_i}{s_i} $$
这里，$D$ 是[梯度算子](@entry_id:275922)（单位 1/m），$s_i$ 是参数 $i$ 的先验标准差（单位与参数相同），$\ell_i$ 是其先验[相关长度](@entry_id:143364)（单位 m）。这样一来，$\alpha_i$ 的单位恰好是（米/参数单位），使得 $\alpha_i D m_i$ 成为无量纲量。通过这种方式，总的正则化项 $\|L(m-m_{\text{ref}})\|_2^2$ 成为一个无量纲的、在物理上可比的量，允许我们使用一个单一的、无量纲的 $\lambda$ 来控制整体的正则化强度。

#### [非线性](@entry_id:637147)问题：Levenberg-Marquardt 算法

对于[非线性](@entry_id:637147)反演问题 $d = F(m) + \varepsilon$，我们通常采用迭代方法求解。**Levenberg-Marquardt (LM) 算法**可以被看作是在每一步迭代中应用 Tikhonov 正则化。[@problem_id:3617455]

在第 $k$ 次迭代，LM 算法通过求解一个线性化的子问题来[计算模型](@entry_id:152639)更新步长 $\Delta m$：
$$ \Delta m_k = \arg\min_{\Delta m} \| J \Delta m + r_k \|_{2}^{2} + \lambda^2 \| \Delta m \|_{2}^{2} $$
其中 $r_k = F(m_k) - d$ 是当前残差，$J$ 是 $F(m)$ 在 $m_k$ 处的雅可比矩阵。该子问题的解由以下[线性系统](@entry_id:147850)给出：
$$ (J^T J + \lambda^2 I) \Delta m = - J^T r_k $$
这里的正则化是施加在**模型更新步长** $\Delta m$ 上，而非模型本身。参数 $\lambda$ 在这里扮演着**信赖域（trust region）**半径的角色：
*   当 $\lambda$ 很小时，LM 步长趋近于**高斯-牛顿（Gauss-Newton）**步长，收敛速度快但可能不稳定。
*   当 $\lambda$ 很大时，LM 步长趋近于一个非常小的**[最速下降](@entry_id:141858)（steepest descent）**步长，收敛慢但保证稳定。
LM 算法通过自适应地调整 $\lambda$ 来在速度和稳定性之间取得平衡，是求解[非线性](@entry_id:637147)[最小二乘问题](@entry_id:164198)最强大和最常用的方法之一。

#### 超越吉洪诺夫：$\ell_1$ 正则化与[稀疏性](@entry_id:136793)

[吉洪诺夫正则化](@entry_id:140094)基于 $\ell_2$ 范数，其对应的[贝叶斯先验](@entry_id:183712)是高斯分布，它偏好产生**平滑**的解。然而，在许多地质环境中，我们期望模型具有**稀疏（sparse）**或**块状（blocky）**的特征，例如清晰的断层、岩性界面或孤立的矿体。[@problem_id:3617492]

为了促进这类解，我们可以使用**$\ell_1$ 范数**替代 $\ell_2$ 范数作为正则化项：
$$ \min_{m} \| G m - d \|_{2}^{2} + \lambda \| L m \|_{1} $$
$\ell_1$ 范数 $\|z\|_1 = \sum_i |z_i|$。在贝叶斯框架下，这对应于对 $Lm$ 的分量施加**拉普拉斯（Laplace）**[先验分布](@entry_id:141376)。与高斯分布的钟形不同，[拉普拉斯分布](@entry_id:266437)在零点处有一个尖峰，这使得它更倾向于产生许多恰好为零的值。
*   **几何上**，$\ell_2$ 范数的[等值面](@entry_id:196027)是光滑的球面，而 $\ell_1$ 范数的[等值面](@entry_id:196027)是带有尖角的菱形（超八面体）。[最小二乘解](@entry_id:152054)的椭球[等值面](@entry_id:196027)更容易与 $\ell_1$ 球的尖角或棱边相切，从而产生[稀疏解](@entry_id:187463)。
*   **地质应用**：当 $L$ 是[梯度算子](@entry_id:275922)时，$\ell_1$ 正则化（也称为**[全变差](@entry_id:140383)正则化，Total Variation regularization**）倾向于产生**分段常数**的模型，非常适合恢复具有清晰边界的地质体。当 $L=I$ 时，$\ell_1$ 正则化（也称为 **LASSO**）则直接[促进模型](@entry_id:147560)参数本身的稀疏性，适用于寻找孤立的异常体。

总而言之，$\ell_2$ 正则化是平滑化的通用工具，而 $\ell_1$ 正则化则是获取稀疏或块状解的有力工具。选择哪种正则化取决于我们对地下真实模型结构的先验地质认识。