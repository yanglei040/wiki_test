{"hands_on_practices": [{"introduction": "为奠定坚实的基础，我们从牛顿法的理想情景——最小化一个二次函数——开始。该练习将展示牛顿法卓越的效率，无论从哪个初始点出发，它都能在单次迭代中精确找到最小值点。掌握这个基本案例，有助于揭示为何牛顿类型的方法在众多优化问题中如此强大 ([@problem_id:3611910])。", "problem": "在线性化地震走时层析成像中，带有 Tikhonov 正则化的数据失配的一个常见二次近似被建模为形式为 $J(m)=\\tfrac{1}{2} m^{\\top} H m - b^{\\top} m$ 的目标函数，其中 $m \\in \\mathbb{R}^{n}$ 表示模型参数，$H \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定矩阵，编码了正规方程和正则化曲率，而 $b \\in \\mathbb{R}^{n}$ 编码了投影数据。假设 $H \\succ 0$。考虑从一个任意的初始猜测 $m_{0} \\in \\mathbb{R}^{n}$ 开始，应用牛顿法来最小化 $J(m)$。\n\n仅使用标量场的梯度和 Hessian 矩阵的标准定义以及作为牛顿法基础的二阶泰勒展开，执行以下操作：\n\n- 通过最小化 $J$ 在 $m_{0}$ 附近的二阶泰勒模型，推导 $J(m)$ 在 $m_{0}$ 处的牛顿搜索方向 $p_{0}$。\n- 从 $p_{0}$ 出发，构建更新后的迭代点 $m_{1} = m_{0} + p_{0}$，并证明 $m_{1}$ 等于 $J(m)$ 的唯一全局最小化子且与 $m_{0}$ 无关。\n- 仅用 $H$ 和 $b$ 提供 $m_{1}$ 的最终显式表达式。\n\n您的最终答案必须是 $m_{1}$ 的单个闭式解析表达式。不包含单位。不提供任何中间表达式。不要对您的答案进行四舍五入或近似。", "solution": "首先对问题进行验证，以确保其具有科学依据、是良构的且客观的。\n\n### 步骤 1：提取已知条件\n- 目标函数：$J(m)=\\tfrac{1}{2} m^{\\top} H m - b^{\\top} m$\n- 模型参数：$m \\in \\mathbb{R}^{n}$\n- 矩阵 $H$：$H \\in \\mathbb{R}^{n \\times n}$，对称正定（$H \\succ 0$）\n- 向量 $b$：$b \\in \\mathbb{R}^{n}$\n- 初始猜测：$m_{0} \\in \\mathbb{R}^{n}$\n- 方法：牛顿法\n- 任务 1：通过最小化 $J(m)$ 在 $m_{0}$ 附近的二阶泰勒模型，推导在 $m_{0}$ 处的牛顿搜索方向 $p_{0}$。\n- 任务 2：构建更新后的迭代点 $m_{1} = m_{0} + p_{0}$ 并证明 $m_{1}$ 是 $J(m)$ 的唯一全局最小化子。\n- 任务 3：仅用 $H$ 和 $b$ 提供 $m_{1}$ 的最终表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题是数值线性代数中的一个标准优化问题，常见于地震层析成像等反演问题中。\n- **科学依据：** 该问题使用了一个二次目标函数，这是线性化反演理论和优化的基石。梯度、Hessian 矩阵和牛顿法等概念是基础性的，且应用正确。其前提在科学上和数学上都是合理的。\n- **良构性：** 目标函数 $J(m)$ 是一个严格凸函数。这一点可以得到保证，因为它的 Hessian 矩阵，即矩阵 $H$，被给定为对称正定。定义在 $\\mathbb{R}^{n}$ 上的严格凸函数有唯一的全局最小化子。该问题要求找到这个最小化子，因此是良构的。\n- **客观性：** 该问题使用精确的数学语言陈述，没有歧义或主观因素。\n\n该问题没有上述缺陷。它是完整的、一致的且可形式化的。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整解。\n\n### 解题推导\n待最小化的目标函数是关于模型参数 $m$ 的二次函数：\n$$\nJ(m) = \\frac{1}{2} m^{\\top} H m - b^{\\top} m\n$$\n其中 $m \\in \\mathbb{R}^{n}$，$H \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$b \\in \\mathbb{R}^{n}$。\n\n牛顿法通过在当前迭代点为函数创建一个二次模型，然后最小化该模型来迭代地寻找函数的最小值。最小化该模型的步长被用来寻找下一个迭代点。\n\n首先，我们计算目标函数 $J(m)$ 的梯度和 Hessian 矩阵。根据向量微积分的标准法则，$J(m)$ 关于 $m$ 的梯度为：\n$$\n\\nabla J(m) = \\frac{1}{2} (H^{\\top}m + Hm) - b\n$$\n由于 $H$ 是对称的（$H = H^{\\top}$），梯度简化为：\n$$\n\\nabla J(m) = \\frac{1}{2} (2Hm) - b = Hm - b\n$$\n$J(m)$ 的 Hessian 矩阵是 $\\nabla J(m)$ 的梯度：\n$$\n\\nabla^2 J(m) = \\nabla (\\nabla J(m)) = \\nabla (Hm - b) = H\n$$\nHessian 矩阵是常数矩阵 $H$。\n\n牛顿法使用二阶泰勒展开在迭代点 $m_k$ 附近近似 $J(m)$。设 $p = m - m_k$ 是从当前迭代点 $m_k$ 开始的步长。$J$ 在 $m_k$ 处的二阶泰勒模型为：\n$$\nJ(m_k + p) \\approx J(m_k) + \\nabla J(m_k)^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^2 J(m_k) p\n$$\n对于像 $J(m)$ 这样的二次函数，这个展开不是近似，而是精确的。设步长 $p$ 的这个二次模型表示为 $Q_k(p)$。在初始猜测 $m_{0}$ 处，该模型为：\n$$\nQ_{0}(p) = J(m_{0}) + \\nabla J(m_{0})^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^2 J(m_{0}) p\n$$\n代入在 $m_{0}$ 处求得的梯度和 Hessian 矩阵的表达式：\n$$\n\\nabla J(m_{0}) = Hm_{0} - b\n$$\n$$\n\\nabla^2 J(m_{0}) = H\n$$\n模型变为：\n$$\nQ_{0}(p) = J(m_{0}) + (Hm_{0} - b)^{\\top} p + \\frac{1}{2} p^{\\top} H p\n$$\n牛顿搜索方向 $p_{0}$ 是最小化此二次模型 $Q_{0}(p)$ 的向量 $p$。为了找到这个最小值，我们求 $Q_{0}(p)$ 关于 $p$ 的梯度，并将其设为零向量。\n$$\n\\nabla_p Q_{0}(p) = (Hm_{0} - b) + Hp\n$$\n将梯度设为零以找到最优步长 $p_{0}$：\n$$\nHp_{0} + (Hm_{0} - b) = 0\n$$\n$$\nHp_{0} = -(Hm_{0} - b) = b - Hm_{0}\n$$\n由于 $H$ 是正定的（$H \\succ 0$），它是可逆的。我们可以通过乘以 $H$ 的逆矩阵来求解 $p_{0}$：\n$$\np_{0} = H^{-1}(b - Hm_{0})\n$$\n这是在初始点 $m_{0}$ 处的牛顿搜索方向。\n\n下一步是计算新的迭代点 $m_{1}$ 并证明它是全局最小化子。牛顿更新规则是 $m_{1} = m_{0} + p_{0}$。\n$$\nm_{1} = m_{0} + p_{0} = m_{0} + H^{-1}(b - Hm_{0})\n$$\n分配 $H^{-1}$：\n$$\nm_{1} = m_{0} + H^{-1}b - H^{-1}Hm_{0}\n$$\n由于 $H^{-1}H = I$，其中 $I$ 是单位矩阵：\n$$\nm_{1} = m_{0} + H^{-1}b - Im_{0} = m_{0} + H^{-1}b - m_{0}\n$$\n$$\nm_{1} = H^{-1}b\n$$\n$m_{1}$ 的这个表达式与初始猜测 $m_{0}$ 无关。为了证明 $m_{1}$ 是 $J(m)$ 的唯一全局最小化子，我们必须验证最优性的一阶和二阶条件。\n\n最小值的一阶必要条件是目标函数在该点的梯度为零。让我们在 $m = m_{1}$ 处计算 $\\nabla J(m)$：\n$$\n\\nabla J(m_{1}) = H m_{1} - b\n$$\n代入 $m_{1} = H^{-1}b$：\n$$\n\\nabla J(m_{1}) = H(H^{-1}b) - b = (HH^{-1})b - b = Ib - b = b - b = 0\n$$\n在 $m_{1}$ 处的梯度确实是零向量。\n\n唯一全局最小值的二阶充分条件是目标函数的 Hessian 矩阵是正定的。$J(m)$ 的 Hessian 矩阵是 $\\nabla^2 J(m) = H$。根据题目陈述，$H$ 是一个对称正定矩阵（$H \\succ 0$）。\n\n由于 $J(m)$ 在 $m_{1}$ 处的梯度为零，且其 Hessian 矩阵处处正定，因此点 $m_{1} = H^{-1}b$ 是严格凸函数 $J(m)$ 的唯一全局最小化子。这个结果表明，对于一个二次目标函数，牛顿法可以在单次迭代中找到精确的最小化子，而与起始点 $m_{0}$ 无关。\n\n$m_{1}$（即唯一全局最小化子）的最终表达式仅用 $H$ 和 $b$ 表示。", "answer": "$$\n\\boxed{H^{-1} b}\n$$", "id": "3611910"}, {"introduction": "从理想走向现实，我们来应对地球物理学中的一个普遍挑战：不适定性。本练习通过分析海森矩阵的特征值，探讨了吉洪诺夫正则化（Tikhonov regularization）如何影响牛顿系统的条件数。通过求解能最小化条件数的最优正则化权重 $\\lambda$，您将亲身体会到在数据拟合与解的稳定性之间取得关键平衡的真谛 ([@problem_id:3611922])。", "problem": "考虑线性重力反演问题，其中正演算子将离散化的地下密度向量映射到地表重力异常。目标函数是 Tikhonov 正则化最小二乘泛函\n$$\nJ(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2},\n$$\n其中 $m$ 是模型，$G$ 是线性正演算子，$d$ 是观测数据，$D$ 是线性正则化算子（例如，离散梯度），$\\lambda$ 是无量纲正则化权重。从 $J(m)$ 的定义和无约束优化的牛顿法出发，推导牛顿系统以及相应曲率（海森）算子的条件数作为 $\\lambda$ 的函数。假设离散化和边界条件使得 $G^{\\top}G$ 和 $D^{\\top}D$ 是对称半正定且对易的，因此它们共享一个特征向量的标准正交基。在一个简化的、代表重力反演中主曲率方向的双模态谱模型中，假设 $G^{\\top}G$ 沿着相同的两个特征向量具有特征值 $a_{1} = 100$ 和 $a_{2} = 1$，而 $D^{\\top}D$ 具有特征值 $b_{1} = 0.01$ 和 $b_{2} = 10$。\n\n利用这些假设，构建牛顿系统的条件数 $\\kappa(\\lambda)$，即其最大特征值与最小特征值之比，并通过解析地平衡两个主模态上的数据失配曲率和正则化曲率，确定使 $\\kappa(\\lambda)$ 最小化的 $\\lambda$ 值。将最终答案表示为一个无量纲数，并四舍五入到四位有效数字。", "solution": "用户希望我解决一个与计算地球物理学中的拟牛顿优化方法相关的问题。\n\n我的计划如下：\n1.  **验证问题陈述：**\n    *   **第 1 步：提取已知条件：** 我将逐字列出所有给定的信息。\n        *   目标函数：$J(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2}$。\n        *   $m$：模型向量。\n        *   $G$：线性正演算子。\n        *   $d$：观测数据向量。\n        *   $D$：线性正则化算子。\n        *   $\\lambda$：无量纲正则化权重。\n        *   上下文是无约束优化的牛顿法。\n        *   假设：$G^{\\top}G$ 和 $D^{\\top}D$ 是对称半正定且对易的。它们共享一个特征向量的标准正交基。\n        *   简化的双模态谱模型：\n            *   $G^{\\top}G$ 特征值：$a_1 = 100$, $a_2 = 1$。\n            *   $D^{\\top}D$ 特征值：$b_1 = 0.01$, $b_2 = 10$。\n            *   这些特征值对应于相同的两个特征向量。\n    *   **第 2 步：使用提取的已知条件进行验证：**\n        *   **科学依据：** 该问题使用 Tikhonov 正则化，这是反演问题中的一种标准技术，尤其是在地球物理学（如重力反演）中。目标函数的公式是正确的。使用牛顿法也是标准的。像 $G^{\\top}G$（与法方程相关）和 $D^{\\top}D$（与正则化相关）这类算子的特征值和特征向量的概念是分析此类问题的核心。假设它们对易并共享特征向量是一种简化，但为了分析上的易处理性是有效的，常用于教科书示例以说明主要原理。这是计算科学与工程中一个公认的问题结构。它在科学上是合理的。\n        *   **适定性：** 问题要求找到使海森矩阵条件数最小化的 $\\lambda$ 值。这是一个定义明确的数学优化问题。所提供的信息（分量矩阵的特征值）足以构建海森矩阵的特征值及其作为 $\\lambda$ 函数的条件数。可以研究该条件数最小值的存在性和唯一性。\n        *   **客观性：** 语言精确且数学化。没有主观陈述。\n        *   **无效性检查：**\n            1.  **科学上不合理：** 无。所涉物理和数学都是标准的。\n            2.  **无法形式化/不相关：** 问题是高度可形式化的，并且与所述主题（地球物理学中的牛顿法）直接相关。\n            3.  **不完整/矛盾：** 设置是完整的。$G^{\\top}G$ 和 $D^{\\top}D$ 对易并共享特征向量的假设是关键。特征值对 $(a_1, b_1)$ 和 $(a_2, b_2)$ 与相同的特征向量相关联。这一点是清楚的。\n            4.  **不切实际/不可行：** 虽然双模态模型是一种简化，但它是一种标准的分析工具，并非物理上不可能或不一致。在实际问题中，$G$ 的奇异值谱通常快速衰减，而正则化算子的选择是为了惩罚模型的粗糙度，这通常对应于 $G$ 灵敏度较低的方向。将大的 $a_i$ 与小的 $b_i$ 配对，反之亦然，代表了这种权衡。这里，$a_1  a_2$ 且 $b_1  b_2$。这是一个合理的场景，其中第一种模态被数据很好地解析但是光滑的（低正则化惩罚），而第二种模态解析度差但是粗糙的（高正则化惩罚）。这是一个为探索正则化的平衡作用而精心设计的设置。没有不一致之处。\n            5.  **不适定：** 找到使 $\\kappa(\\lambda)$ 最小化的 $\\lambda$ 的问题是适定的。\n            6.  **伪深刻/琐碎：** 问题并非琐碎。它需要推导海森矩阵、其特征值、条件数，然后最小化该函数。它测试了反演问题背景下的数值优化和线性代数的核心概念。它没有被过度简化到琐碎的程度。\n            7.  **超出科学可验证性范围：** 结果是可以通过数学推导和验证的。\n\n    *   **第 3 步：结论与行动：** 问题是**有效的**。我将着手解决它。\n\n2.  **解决问题：**\n    *   **第 1 部分：推导牛顿系统。**\n        *   目标函数是 $J(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2}$。\n        *   展开范数：$J(m) = \\frac{1}{2}(G m - d)^{\\top}(G m - d) + \\frac{\\lambda}{2}(D m)^{\\top}(D m)$。\n        *   $J(m) = \\frac{1}{2}(m^{\\top}G^{\\top}G m - m^{\\top}G^{\\top}d - d^{\\top}G m + d^{\\top}d) + \\frac{\\lambda}{2}m^{\\top}D^{\\top}D m$。\n        *   由于 $m^{\\top}G^{\\top}d$ 是一个标量，它等于其转置 $d^{\\top}G m$。\n        *   $J(m) = \\frac{1}{2}m^{\\top}G^{\\top}G m - d^{\\top}G m + \\frac{1}{2}d^{\\top}d + \\frac{\\lambda}{2}m^{\\top}D^{\\top}D m$。\n        *   $J(m)$ 关于 $m$ 的梯度是 $\\nabla J(m)$。\n        *   $\\nabla J(m) = \\frac{1}{2}(2 G^{\\top}G m) - G^{\\top}d + \\frac{\\lambda}{2}(2 D^{\\top}D m) = G^{\\top}G m - G^{\\top}d + \\lambda D^{\\top}D m$。\n        *   $\\nabla J(m) = (G^{\\top}G + \\lambda D^{\\top}D)m - G^{\\top}d$。\n        *   $J(m)$ 的海森矩阵是二阶导数矩阵 $\\nabla^2 J(m)$，我将其称为 $H$。\n        *   $H = \\nabla (\\nabla J(m)^{\\top}) = G^{\\top}G + \\lambda D^{\\top}D$。\n        *   用于找到 $J(m)$ 最小值的牛顿法涉及迭代更新模型 $m_k$：$m_{k+1} = m_k - (\\nabla^2 J(m_k))^{-1} \\nabla J(m_k)$。\n        *   每次迭代的牛顿系统是 $\\nabla^2 J(m_k) \\Delta m_k = - \\nabla J(m_k)$，其中 $\\Delta m_k = m_{k+1} - m_k$。\n        *   由于 $J(m)$ 是 $m$ 的二次函数，其海森矩阵相对于 $m$ 是常数。所以，$\\nabla^2 J(m) = H = G^{\\top}G + \\lambda D^{\\top}D$。\n        *   牛顿系统是 $(G^{\\top}G + \\lambda D^{\\top}D) \\Delta m = - \\nabla J(m)$。该系统的矩阵是海森矩阵 $H$。问题中称其为“曲率算子”。\n\n    *   **第 2 部分：分析海森矩阵 H 的条件数。**\n        *   海森矩阵是 $H(\\lambda) = G^{\\top}G + \\lambda D^{\\top}D$。\n        *   条件数 $\\kappa(H)$ 是其最大特征值与最小特征值之比：$\\kappa(H) = \\frac{\\mu_{max}}{\\mu_{min}}$。\n        *   我们已知 $G^{\\top}G$ 和 $D^{\\top}D$ 是对称半正定且对易的。这是一条至关重要的信息。\n        *   因为它们对易，所以它们可以同时对角化。这意味着它们共享一个共同的特征向量基。\n        *   设 $\\{v_i\\}$ 是共同的特征向量标准正交基。\n        *   设 $G^{\\top}G v_i = a_i v_i$ 和 $D^{\\top}D v_i = b_i v_i$。\n        *   现在，我们来求 $H(\\lambda)$ 的特征值。设 $\\mu_i$ 是 $H(\\lambda)$ 对应于特征向量 $v_i$ 的特征值。\n        *   $H(\\lambda) v_i = (G^{\\top}G + \\lambda D^{\\top}D) v_i = G^{\\top}G v_i + \\lambda D^{\\top}D v_i = a_i v_i + \\lambda b_i v_i = (a_i + \\lambda b_i) v_i$。\n        *   所以，$H(\\lambda)$ 的特征值是 $\\mu_i(\\lambda) = a_i + \\lambda b_i$。\n        *   问题提供了一个简化的双模态模型。这意味着我们只需要考虑两个特征向量 $v_1$ 和 $v_2$。\n        *   对应的特征值是：\n            *   对于 $v_1$：$a_1 = 100$, $b_1 = 0.01$。$H$ 的特征值是 $\\mu_1(\\lambda) = a_1 + \\lambda b_1 = 100 + 0.01 \\lambda$。\n            *   对于 $v_2$：$a_2 = 1$, $b_2 = 10$。$H$ 的特征值是 $\\mu_2(\\lambda) = a_2 + \\lambda b_2 = 1 + 10 \\lambda$。\n        *   牛顿系统（海森矩阵）的特征值是 $\\mu_1(\\lambda)$ 和 $\\mu_2(\\lambda)$。\n\n    *   **第 3 部分：构建条件数 $\\kappa(\\lambda)$。**\n        *   条件数是 $\\kappa(\\lambda) = \\frac{\\mu_{max}}{\\mu_{min}} = \\frac{\\max(\\mu_1(\\lambda), \\mu_2(\\lambda))}{\\min(\\mu_1(\\lambda), \\mu_2(\\lambda))}$。\n        *   我们有 $\\mu_1(\\lambda) = 100 + 0.01 \\lambda$ 和 $\\mu_2(\\lambda) = 1 + 10 \\lambda$。\n        *   由于 $\\lambda$ 是一个正则化权重，它必须是非负的，即 $\\lambda \\ge 0$。\n        *   当 $\\lambda = 0$ 时，$\\mu_1(0)=100$ 且 $\\mu_2(0)=1$。所以 $\\mu_1(0) > \\mu_2(0)$。\n        *   我们来看看它们是否以及何时相交。$\\mu_1(\\lambda) = \\mu_2(\\lambda)$。\n        *   $100 + 0.01 \\lambda = 1 + 10 \\lambda$。\n        *   $99 = (10 - 0.01) \\lambda = 9.99 \\lambda$。\n        *   $\\lambda = \\frac{99}{9.99} = \\frac{9900}{999} = \\frac{1100}{111} \\approx 9.9099$。\n        *   我们称这个交叉点为 $\\lambda_c = \\frac{99}{9.99}$。\n        *   对于 $0 \\le \\lambda  \\lambda_c$，$\\mu_2$ 的斜率远大于 $\\mu_1$ 的斜率。由于 $\\mu_1(0) > \\mu_2(0)$，我们在此区间内有 $\\mu_1(\\lambda) > \\mu_2(\\lambda)$。\n        *   对于 $\\lambda > \\lambda_c$，我们有 $\\mu_2(\\lambda) > \\mu_1(\\lambda)$。\n        *   所以，条件数 $\\kappa(\\lambda)$ 是一个分段函数：\n            *   如果 $0 \\le \\lambda \\le \\frac{99}{9.99}$：$\\kappa(\\lambda) = \\frac{\\mu_1(\\lambda)}{\\mu_2(\\lambda)} = \\frac{100 + 0.01 \\lambda}{1 + 10 \\lambda}$。\n            *   如果 $\\lambda > \\frac{99}{9.99}$：$\\kappa(\\lambda) = \\frac{\\mu_2(\\lambda)}{\\mu_1(\\lambda)} = \\frac{1 + 10 \\lambda}{100 + 0.01 \\lambda}$。\n        *   注意，当 $\\lambda=\\lambda_c$ 时，两个表达式都得出 $\\kappa(\\lambda_c) = 1$。函数 $\\kappa(\\lambda)$ 是连续的。\n\n    *   **第 4 部分：最小化 $\\kappa(\\lambda)$。**\n        *   问题要求找到使 $\\kappa(\\lambda)$ 最小化的 $\\lambda$ 值。\n        *   条件数 $\\kappa(\\lambda)$ 是两个正特征值之比，所以 $\\kappa(\\lambda) \\ge 1$。\n        *   条件数的最小可能值为 $1$。\n        *   我们发现，在 $\\lambda = \\lambda_c = \\frac{99}{9.99}$ 处，两个特征值相等，即 $\\mu_1(\\lambda_c) = \\mu_2(\\lambda_c)$。\n        *   在这一点上，$\\kappa(\\lambda_c) = \\frac{\\mu_1(\\lambda_c)}{\\mu_2(\\lambda_c)} = 1$。\n        *   由于对所有的 $\\lambda$ 都有 $\\kappa(\\lambda) \\ge 1$，因此 $\\kappa(\\lambda)$ 的最小值是 $1$，并且在 $\\lambda = \\lambda_c$ 时达到。\n        *   这对应于问题条件最佳的点（在这个双模态子空间中）。\n        *   其解释是，这个 $\\lambda$ 值平衡了两种模态的“曲率”。问题陈述中说“通过解析地平衡两个主模态上的数据失配曲率和正则化曲率”。\n        *   我们再读一遍：“确定使 $\\kappa(\\lambda)$ 最小化的 $\\lambda$ 值”。\n        *   我的分析表明，最小值出现在特征值相等的点。\n        *   为了证实这一点，考虑导数。对于 $0 \\le \\lambda \\le \\lambda_c$，设 $f_1(\\lambda) = \\kappa(\\lambda)$。导数 $f_1'(\\lambda) = \\frac{0.01(1 + 10\\lambda) - 10(100 + 0.01\\lambda)}{(1 + 10\\lambda)^2} = \\frac{-999.99}{(1 + 10\\lambda)^2}$ 总是负的。因此，$\\kappa(\\lambda)$ 在此区间上递减。对于 $\\lambda > \\lambda_c$，设 $f_2(\\lambda) = \\kappa(\\lambda)$。导数 $f_2'(\\lambda) = \\frac{10(100 + 0.01\\lambda) - 0.01(1 + 10\\lambda)}{(100 + 0.01\\lambda)^2} = \\frac{999.99}{(100 + 0.01\\lambda)^2}$ 总是正的。因此，$\\kappa(\\lambda)$ 在此区间上递增。因此，$\\kappa(\\lambda)$ 的最小值必然出现在过渡点 $\\lambda_c$，即特征值相等的点。\n        *   所以使 $\\kappa(\\lambda)$ 最小化的 $\\lambda$ 值是 $\\lambda_c = \\frac{99}{9.99}$。\n\n    *   **第 5 部分：计算数值。**\n        *   $\\lambda = \\frac{99}{9.99} = \\frac{99}{999/100} = \\frac{9900}{999}$。\n        *   分子分母同除以 $9$ 得到 $\\frac{1100}{111}$。\n        *   数值上，$\\lambda = \\frac{1100}{111} \\approx 9.9099099...$。\n        *   问题要求四舍五入到四位有效数字。前四位有效数字是 $9, 9, 0, 9$。第五位数字是 $9$。由于 $9 \\ge 5$，我们将第四位有效数字向上取整。\n        *   $9.909...$ 变成 $9.910$。结尾的零是有效数字，是必需的。\n\n这个详细的计划涵盖了问题的所有方面，并遵守了指定的约束。我准备好写解答了。", "answer": "$$\n\\boxed{9.910}\n$$", "id": "3611922"}, {"introduction": "对于地球物理学中常见的大规模反演问题，构建和存储海森矩阵在计算上是不可行的。这一挑战推动了诸如L-BFGS等拟牛顿方法的使用。本练习将“有限内存”这一抽象概念具体化，要求您计算大规模反演中L-BFGS算法的内存需求，并分析曲率近似质量与可用计算资源之间的权衡 ([@problem_id:3611901])。", "problem": "在计算地震学中，一个大规模全波形反演（Full Waveform Inversion, FWI）问题旨在通过使用拟牛顿法迭代更新一个具有 $N$ 个参数的模型，来最小化一个数据失配目标函数。在限制内存的Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 算法中，逆海森矩阵的近似是由固定数量 $m$ 的近期校正对构建的：步长向量 $s_{i} = x_{i+1} - x_{i}$ 和梯度差 $y_{i} = g_{i+1} - g_{i}$，每个都以双精度存储。假设以下科学上真实的条件：\n\n- 参数维度为 $N = 10^{8}$。\n- 双精度数组每个条目存储 $8$ 字节。\n- L-BFGS 实现存储 $m$ 个校正对 $\\{s_{i}, y_{i}\\}_{i=1}^{m}$（每个长度为 $N$）、当前模型 $x \\in \\mathbb{R}^{N}$、当前梯度 $g \\in \\mathbb{R}^{N}$，以及一个在双循环递归中使用的长度为 $N$ 的额外工作向量。它还存储两个用于双循环递归的长度为 $m$ 的标量数组。\n- 可用的内存预算为 $64$ 吉字节（GB），吉字节定义为 $10^{9}$ 字节。\n\n作为一个曲率保真度的第一性原理模型，假设在此反演中高斯-牛顿海森矩阵的谱是强病态的，且具有幂律衰减 $\\lambda_{i} = \\lambda_{1} i^{-\\alpha}$，其中 $\\alpha = 1.5$。我们将一个 $m$ 对 L-BFGS 近似的曲率保真度定义为所捕获的迹的比例，\n$$\nF(m) = \\frac{\\sum_{i=1}^{m} \\lambda_{i}}{\\sum_{i=1}^{\\infty} \\lambda_{i}} = \\frac{\\sum_{i=1}^{m} i^{-\\alpha}}{\\sum_{i=1}^{\\infty} i^{-\\alpha}},\n$$\n这说明了随着 $m$ 增加，收益会递减。利用这一点来定性讨论增加 $m$（提高曲率保真度）与内存使用（随 $m$ 线性增加）之间的权衡。\n\n任务：\n1. 根据上述定义，推导 L-BFGS 使用 $m$ 个校正对所需的总内存 $M(m)$（以 GB 为单位）的解析表达式，包括所有明确列出的数组。\n2. 在 $64$ GB 内存预算下，确定最大整数校正对数 $m^{\\star}$，使得 $M(m^{\\star}) \\leq 64$ GB。\n\n您最终报告的量必须仅为整数 $m^{\\star}$。无需四舍五入到有效数字；报告确切的整数。使用 $1\\,\\mathrm{GB} = 10^{9}$ 字节来表示任何中间的内存表达式（以吉字节 GB 为单位）。", "solution": "首先将根据指定标准对问题进行验证。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n- **主题：** 计算地球物理学中的牛顿法和拟牛顿优化方法。\n- **算法：** 限制内存的Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) 算法。\n- **模型维度：** $N = 10^{8}$ 个参数。\n- **数据存储：** 双精度数组每个条目存储 $8$ 字节。\n- **L-BFGS 存储项：**\n    1.  $m$ 个校正对 $\\{s_{i}, y_{i}\\}_{i=1}^{m}$，其中 $s_i, y_i \\in \\mathbb{R}^{N}$。\n    2.  当前模型向量 $x \\in \\mathbb{R}^{N}$。\n    3.  当前梯度向量 $g \\in \\mathbb{R}^{N}$。\n    4.  一个额外的长度为 $N$ 的工作向量。\n    5.  两个长度为 $m$ 的标量数组。\n- **内存预算：** $64$ 吉字节 (GB)，其中 $1\\,\\text{GB} = 10^{9}$ 字节。\n- **曲率保真度模型（供参考）：** $F(m) = \\frac{\\sum_{i=1}^{m} i^{-\\alpha}}{\\sum_{i=1}^{\\infty} i^{-\\alpha}}$，其中 $\\alpha = 1.5$。\n- **任务：**\n    1.  推导总内存 $M(m)$（以 GB 为单位）的解析表达式。\n    2.  确定最大整数 $m^{\\star}$，使得 $M(m^{\\star}) \\leq 64$ GB。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n- **科学上合理：** 该问题设置在全波形反演的背景下，这是地球物理学中一个标准且计算密集的问题。L-BFGS 是解决此类大规模优化问题的常用算法。给定的参数，如模型大小 ($N=10^8$) 和内存限制 ($64$ GB)，对于高性能计算场景是现实的。内存核算是科学计算中的一个标准练习。曲率模型是拟牛顿法中内存与精度之间权衡的一个合理（尽管简化）的表示。该问题没有违反任何科学原理。\n- **定义明确：** 该问题提供了构建和求解所需量的所有必要数据。任务是具体的，并导向一个唯一的整数解。\n- **客观：** 该问题以精确、量化和无偏见的语言陈述。\n\n**步骤 3：结论与行动**\n\n该问题被认为是**有效的**，因为它是科学合理的、定义明确的、客观且自洽的。现在开始求解过程。\n\n### 求解推导\n\n如问题陈述中所指定的，求解过程包括两个部分。首先，我们推导出总内存消耗 $M(m)$ 作为存储校正对数量 $m$ 的函数的表达式。其次，我们使用此表达式和给定的内存预算来计算 $m$ 的最大允许整数值，记为 $m^{\\star}$。\n\n**第 1 部分：内存使用量 $M(m)$ 的解析表达式**\n\n所需的总内存是所有存储组件所用内存的总和。每个组件都是一个双精度数数组，其中每个数字占用 $8$ 字节。\n\n各组件及其大小（以双精度条目数量计）如下：\n1.  **校正对：** 有 $m$ 对向量 $\\{s_{i}, y_{i}\\}$。每个向量的维度为 $N$。这相当于 $2 \\times m$ 个向量，每个长度为 $N$。\n    - 条目数：$2 \\times m \\times N = 2mN$。\n2.  **当前模型向量：** 向量 $x$ 的长度为 $N$。\n    - 条目数：$N$。\n3.  **当前梯度向量：** 向量 $g$ 的长度为 $N$。\n    - 条目数：$N$。\n4.  **额外的工作向量：** 使用一个长度为 $N$ 的向量。\n    - 条目数：$N$。\n5.  **标量数组：** 两个数组，每个存储 $m$ 个标量。我们假设这些标量也以双精度数存储以保持一致性，这是标准做法。\n    - 条目数：$2 \\times m = 2m$。\n\n需要存储的双精度值的总数，我们称之为 $D(m)$，是所有这些条目的总和：\n$$D(m) = 2mN + N + N + N + 2m$$\n$$D(m) = 2mN + 3N + 2m$$\n这可以根据 $m$ 和 $N$ 进行因式分解：\n$$D(m) = (2N + 2)m + 3N$$\n总内存使用量（以字节为单位）是 $8 \\times D(m)$，因为每个双精度值需要 $8$ 字节。\n$$\\text{内存（字节）} = 8 \\times ((2N + 2)m + 3N)$$\n问题要求内存表达式 $M(m)$ 以吉字节 (GB) 为单位，定义为 $1\\,\\text{GB} = 10^{9}$ 字节。\n$$M(m) = \\frac{8 \\times ((2N + 2)m + 3N)}{10^9}$$\n这是总内存（以 GB 为单位）的解析表达式。\n\n**第 2 部分：最大校正对数 $m^{\\star}$ 的计算**\n\n我们给定的内存预算为 $64$ GB。我们必须找到满足条件 $M(m^{\\star}) \\leq 64$ 的最大整数 $m = m^{\\star}$。\n$$\\frac{8 \\times ((2N + 2)m + 3N)}{10^9} \\leq 64$$\n我们已知 $N = 10^{8}$。将此值代入不等式：\n$$\\frac{8 \\times ((2 \\times 10^8 + 2)m + 3 \\times 10^8)}{10^9} \\leq 64$$\n为了解出 $m$，我们首先简化不等式。两边同除以 $8$：\n$$\\frac{(2 \\times 10^8 + 2)m + 3 \\times 10^8}{10^9} \\leq 8$$\n两边同乘以 $10^9$：\n$$(2 \\times 10^8 + 2)m + 3 \\times 10^8 \\leq 8 \\times 10^9$$\n分离包含 $m$ 的项：\n$$(2 \\times 10^8 + 2)m \\leq 8 \\times 10^9 - 3 \\times 10^8$$\n为了方便右侧的减法，我们将 $8 \\times 10^9$ 写成 $80 \\times 10^8$：\n$$(2 \\times 10^8 + 2)m \\leq 80 \\times 10^8 - 3 \\times 10^8$$\n$$(2 \\times 10^8 + 2)m \\leq (80 - 3) \\times 10^8$$\n$$(2 \\times 10^8 + 2)m \\leq 77 \\times 10^8$$\n现在，解出 $m$：\n$$m \\leq \\frac{77 \\times 10^8}{2 \\times 10^8 + 2}$$\n我们来计算这个分数：\n$$m \\leq \\frac{7,700,000,000}{200,000,002}$$\n执行除法：\n$$m \\leq 38.499999615...$$\n由于 $m$ 必须是表示校正对数量的整数，我们必须对该值取底。\n$$m^{\\star} = \\lfloor 38.499999615... \\rfloor = 38$$\n因此，在 $64$ GB 内存预算内可以存储的最大整数校正对数是 $38$。\n关于曲率保真度 $F(m)$ 的定性背景说明，虽然更大的 $m$ 通常对优化算法的收敛速度更有利，但它会带来线性的内存成本。值 $m^{\\star}=38$ 代表了在这种特定场景下可用硬件资源施加的硬性限制。", "answer": "$$\\boxed{38}$$", "id": "3611901"}]}