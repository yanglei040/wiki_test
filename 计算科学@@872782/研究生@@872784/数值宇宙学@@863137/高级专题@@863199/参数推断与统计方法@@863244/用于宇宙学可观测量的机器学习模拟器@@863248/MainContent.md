## 引言
随着宇宙学进入精确测量时代，理论预测与观测数据的比对日益依赖于计算成本高昂的数值模拟。[机器学习模拟器](@entry_id:751586)通过构建传统模拟的快速、精确替代模型，为解决这一计算瓶颈提供了革命性的方案。然而，构建一个物理上可靠且统计上稳健的模拟器并非易事，它需要在模型选择、训练策略和不确定性量化方面做出审慎的决策。本文旨在系统性地填补这一知识鸿沟，为研究者提供一份从原理到实践的综合指南。

在接下来的内容中，我们将分三步深入探讨这一主题。**第一章：原理与机制**将剖析构建模拟器的核心技术，包括如何选择和表征模拟目标、主流模拟器架构（如高斯过程和[神经网](@entry_id:276355)络）的内在原理，以及高级训练与验证策略。**第二章：应用与[交叉](@entry_id:147634)学科联系**将展示模拟器在真实科研场景中的强大功能，从处理高维复杂的[可观测量](@entry_id:267133)，到赋能先进的[贝叶斯推断](@entry_id:146958)框架，再到主动指导实验设计。最后，**第三章：动手实践**将通过一系列精心设计的问题，帮助读者将理论知识转化为解决实际问题的能力。通过这一结构化的学习路径，读者将全面掌握为[宇宙学可观测量](@entry_id:747921)构建和应用[机器学习模拟器](@entry_id:751586)的前沿技术。

## 原理与机制

在本章中，我们将深入探讨为[宇宙学可观测量](@entry_id:747921)构建[机器学习模拟器](@entry_id:751586)的核心原理和关键机制。这些工具的开发不仅是计算上的便利，更是[现代宇宙学](@entry_id:752086)数据分析中不可或缺的一环，它要求在[计算效率](@entry_id:270255)、[模型灵活性](@entry_id:637310)和物理保真度之间进行审慎的权衡。我们将从模拟目标的选取开始，逐步剖析不同的模拟策略、主流的模拟器架构，并最终讨论高级训练和验证技术，以确保模拟器的鲁棒性和可靠性。

### 模拟目标的选取与表征

在构建模拟器之前，首要任务是明确我们究竟要模拟什么。[宇宙学模拟](@entry_id:747928)的输出，如[物质功率谱](@entry_id:161407) $P(k,z)$ 或[角功率谱](@entry_id:161125) $C_\ell$，通常是定义在连续变量（如波数 $k$、[红移](@entry_id:159945) $z$ 或[多极矩](@entry_id:191120) $\ell$）上的高维函数或向量。直接模拟这些高维对象可能既不高效也非最优。因此，选择合适的目标变量并对其进行有效表征是至关重要的第一步。

#### [对数空间](@entry_id:270258)模拟：一个实用的[范式](@entry_id:161181)

宇宙学中的许多[可观测量](@entry_id:267133)，特别是[功率谱](@entry_id:159996)，本质上是平方振幅的系综平均，因此是物理上非负的量 [@problem_id:3478322] [@problem_id:3478338]。例如，[物质功率谱](@entry_id:161407) $P(k,z)$ 和[角功率谱](@entry_id:161125) $C_\ell$ 分别定义为密度场傅里叶模式和球谐系数的平方模长的[期望值](@entry_id:153208)，即 $P(k,z) \propto \langle |\delta_{\mathbf{k}}(z)|^{2} \rangle$ 和 $C_\ell \propto \sum_m \langle |a_{\ell m}|^{2} \rangle$。这一非负性约束对模拟器的设计提出了挑战。一个未经约束的模拟器（如具有线性输出层的[神经网](@entry_id:276355)络）可能会产生无物理意义的负值预测。

一个简洁而强大的解决方案是选择模拟目标量的对数，例如 $\log P(k,z)$ 或 $\log C_\ell$。这种选择有几个显著的优势 [@problem_id:3478322]：

1.  **自动保证正定性**：模拟器本身可以自由输出任何实数值，记为 $\widehat{\log y}$。通过简单的指数变换 $\hat{y} = \exp(\widehat{\log y})$，最终的物理量预测值 $\hat{y}$ 被自然地约束在正数范围内，从而无需在模型架构或训练过程中施加复杂的约束。

2.  **契合科学目标的误差度量**：宇宙学功率谱通常跨越数个[数量级](@entry_id:264888)。在科学实践中，我们更关心[相对误差](@entry_id:147538)而非[绝对误差](@entry_id:139354)。例如，无论功率谱的值是 $10^4$ 还是 $10^{-2}$，一个 $1\%$ 的误差通常被认为具有同等的重要性。在[对数空间](@entry_id:270258)中最小化[均方误差](@entry_id:175403)（MSE），即 $(\log \hat{y} - \log y)^2 = (\log(\hat{y}/y))^2$，对于小的[相对误差](@entry_id:147538) $\epsilon = (\hat{y}-y)/y \ll 1$，近似等于最小化相对误差的平方，因为 $\log(1+\epsilon) \approx \epsilon$。因此，在[对数空间](@entry_id:270258)训练能使[损失函数](@entry_id:634569)更好地反映科学目标，避免模型性能被高功率区域主导而忽略低功率区域。

3.  **稳定[方差](@entry_id:200758)和误差建模**：对于[高斯随机场](@entry_id:749757)，功率谱[估计量的[方](@entry_id:167223)差](@entry_id:200758)（即宇宙学[方差](@entry_id:200758)）与其自身的值成正比，例如 $\mathrm{Var}(\hat{C}_\ell) \approx \frac{2}{2\ell+1} C_\ell^2$。这种依赖于信号强度的[方差](@entry_id:200758)（[异方差性](@entry_id:136378)）在统计上处理起来很棘手。通过[误差传播公式](@entry_id:275155)，我们可以看到 $\log \hat{C}_\ell$ 的[方差](@entry_id:200758)变为 $\mathrm{Var}(\log \hat{C}_\ell) \approx \frac{\mathrm{Var}(\hat{C}_\ell)}{C_\ell^2} \approx \frac{2}{2\ell+1}$，它近似独立于 $C_\ell$ 的幅度。这种[方差](@entry_id:200758)稳定化效应使得在对数空间中更容易建立和处理[统计模型](@entry_id:165873)。此外，如果模拟器的不确定性在对数空间中被建模为[高斯分布](@entry_id:154414)，那么在原始空间中对应的就是[对数正态分布](@entry_id:261888)，这自然地描述了[乘性](@entry_id:187940)误差，更符合物理直觉。因此，性能评估也应采用相对度量，如对数空间的[均方根误差](@entry_id:170440)（RMSE）或相对误差的[均方根](@entry_id:263605) [@problem_id:3478322] [@problem_id:3478394]。

综上所述，选择对数功率谱作为模拟目标是一种在实践中被广泛采用的明智策略，它简化了物理约束的实施，并使训练目标和误差模型与科学需求更加一致。

#### 基于[主成分分析](@entry_id:145395)的降维

即使确定了模拟目标，例如在离散波数网格上的 $\log P(k)$ 向量，其维度 $p$ 依然可能很高。然而，这些高维向量中的信息通常是高度冗余的。[宇宙学参数](@entry_id:161338)的变化只会激发[向量空间](@entry_id:151108)中少数几个主导的变化模式。**主成分分析 (Principal Component Analysis, PCA)** 提供了一种系统性的方法来识别这些模式并进行线性[降维](@entry_id:142982) [@problem_id:3478388]。

PCA的目标是找到一个低维的正交基，使得数据投影到该基上时保留的[方差](@entry_id:200758)最大。给定一个包含 $n$ 个样本的训练集 $\{y^{(j)}\}_{j=1}^n$，其中 $y^{(j)} \in \mathbb{R}^p$ 是一个离散化的谱向量，我们可以计算其样本均值 $\bar{y}$ 和样本协方差矩阵 $S = \frac{1}{n-1} \sum_{j=1}^n (y^{(j)}-\bar{y})(y^{(j)}-\bar{y})^T$。PCA的核心是协方差矩阵的[特征分解](@entry_id:181333) $S = V \Lambda V^T$，其中 $V$ 是由[特征向量](@entry_id:151813) $v_i$ 组成的[标准正交矩阵](@entry_id:169220)，$\Lambda$ 是由相应[特征值](@entry_id:154894) $\lambda_i$ 组成的[对角矩阵](@entry_id:637782)。按照惯例，[特征值](@entry_id:154894)按降序[排列](@entry_id:136432)，$\lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_p \ge 0$。

[特征向量](@entry_id:151813) $v_i$ 构成了数据变化的主方向，而[特征值](@entry_id:154894) $\lambda_i$ 则量化了沿该方向的[方差](@entry_id:200758)大小。为了将数据从 $p$ 维压缩到 $m$ 维（$m \ll p$），我们选取前 $m$ 个最重要的主成分，即与最大 $m$ 个[特征值](@entry_id:154894)对应的[特征向量](@entry_id:151813)，构成一个[投影矩阵](@entry_id:154479) $W_m = [v_1 | v_2 | \dots | v_m] \in \mathbb{R}^{p \times m}$。

对于任意一个谱向量 $y$，其压缩表示（即PCA系数）$c_m \in \mathbb{R}^m$ 通过投影得到：
$c_m = W_m^T (y - \bar{y})$

而从[压缩系数](@entry_id:272630) $c_m$ 重建原始谱向量的近似值 $\hat{y}_m$ 为：
$\hat{y}_m = \bar{y} + W_m c_m = \bar{y} + W_m W_m^T (y - \bar{y})$

这种重建的期望[均方误差](@entry_id:175403)由被舍弃的[特征值](@entry_id:154894)之和给出：
$\mathbb{E}[\|y - \hat{y}_m\|_2^2] = \sum_{j=m+1}^p \lambda_j$

归一化的重建误差，即被舍弃的[方差](@entry_id:200758)占总[方差](@entry_id:200758)的比例，则为：
$\varepsilon_m = \frac{\sum_{j=m+1}^p \lambda_j}{\sum_{j=1}^p \lambda_j}$

在实践中，我们通常会发现少数几个主成分（例如，$m$ 为5到10）就足以捕获绝大部分（如 $99.9\%$）的[方差](@entry_id:200758)，这意味着我们可以用一个低维系数向量 $c_m$ 来精确地代表原始的高维谱向量。例如，在一组典型的[功率谱](@entry_id:159996)数据上，仅用3个主成分可能就足以将归一化重建误差降低到 $1.5\%$ 以下 [@problem_id:3478388]。此时，模拟器的任务就从学习一个从参数 $\theta$ 到高维向量 $y$ 的复杂映射，转变为学习一个从 $\theta$ 到低维系数向量 $c_m$ 的更简单的映射。

### 基本模拟策略：前向模型与[似然](@entry_id:167119)

在确定了（可能经过降维的）模拟目标 $y$ 之后，我们需要决定模拟器在整个推理流程中扮演的角色。这里存在两种主要的策略：模拟**前向模型**或模拟**似然函数** [@problem_id:3478382]。

1.  **前向[模型模拟](@entry_id:752073) (Forward Model Emulation)**：这种策略的目标是模拟从[宇宙学参数](@entry_id:161338) $\theta$ 到可观测量（或其摘要统计量）[期望值](@entry_id:153208)的确定性映射，即 $y(\theta) = \mathbb{E}[s(d) | \theta]$，其中 $d$ 是原始[高维数据](@entry_id:138874)，$s(d)$ 是摘要统计量（如功率谱）。模拟器，无论是[神经网](@entry_id:276355)络、高斯过程还是其他模型，学习这个回归任务。随后，在进行[参数推断](@entry_id:753157)时，我们将模拟器的快速预测 $\hat{y}(\theta)$ 与一个解析的、计算上易于处理的[似然](@entry_id:167119)模型相结合。最常见的选择是多元高斯似然：
    $p(s | \theta) \approx \mathcal{N}(s | \hat{y}(\theta), C(\theta))$
    其中 $C(\theta)$ 是摘要统计量 $s$ 的协方差矩阵。

    这种方法在以下条件下是首选：
    *   摘要统计量 $s(d)$ 对于参数 $\theta$ 是**近似充分的** (nearly sufficient)。这意味着从原始数据 $d$ 压缩到 $s(d)$ 的过程中几乎没有损失关于 $\theta$ 的信息。对于许多宇宙学问题，[功率谱](@entry_id:159996)等二点统计量在弱非高斯尺度上是近似充分的。
    *   $s(d)$ 的[采样分布](@entry_id:269683)是**近似高斯的**。[中心极限定理](@entry_id:143108)为这一假设提供了理论基础，因为许多摘要统计量是大量独立模式的平均。
    *   [协方差矩阵](@entry_id:139155) $C(\theta)$ 对参数 $\theta$ 的**依赖性较弱**，或者可以被廉价地估计或建模。如果 $C$ 不随 $\theta$ 变化，我们就可以预先计算一次，极大地加速了马尔可夫链蒙特卡洛（MCMC）等采样过程中的似然评估。

    在这些条件下，前向[模型模拟](@entry_id:752073)将复杂的模拟任务分解为一个相对简单的回归问题（学习均值）和一个独立的（或简单的）[协方差估计](@entry_id:145514)问题，计算效率极高 [@problem_id:3478382]。

2.  **似然模拟 (Likelihood Emulation)**：与前一种策略不同，似然模拟尝试直接学习整个[条件概率分布](@entry_id:163069) $p(s|\theta)$，甚至是 $p(d|\theta)$。这通常采用神经[密度估计](@entry_id:634063)（Neural Density Estimation）技术，如[归一化流](@entry_id:272573)（Normalizing Flows）。这类模型不预设似然的函数形式（如高斯），而是直接从模拟数据对 $(\theta, s)$ 中学习其复杂的形状。

    这种策略在以下情况下更具优势：
    *   摘要统计量 $s(d)$ **不是充分的**，并且丢弃了重要的非高斯信息。例如，在[弱引力透镜](@entry_id:158468)分析中，宇宙网中的纤维状结构等相位信息包含了重要的宇宙学信息，而这些信息在[功率谱](@entry_id:159996)中会丢失。直接模拟数据（或信息更丰富的摘要）的似然可以捕获这些信息。
    *   摘要统计量 $s(d)$ 的[分布](@entry_id:182848)是**高度非高斯**的。
    *   [协方差矩阵](@entry_id:139155) $C(\theta)$ **强烈且复杂地依赖于**参数 $\theta$。在这种情况下，为每个新的 $\theta$ 计算 $C(\theta)$ 及其[逆矩阵](@entry_id:140380)会非常昂贵。一个灵活的条件[密度估计](@entry_id:634063)器可以自动地、隐式地学习均值、协[方差](@entry_id:200758)乃至更[高阶矩](@entry_id:266936)如何随 $\theta$ 变化，从而绕过了显式计算协[方差](@entry_id:200758)的难题 [@problem_id:3478382]。

总而言之，前向[模型模拟](@entry_id:752073)是一种高效、实用的“[近似贝叶斯计算](@entry_id:746494)”（Approximate Bayesian Computation, ABC）方法，适用于问题结构良好、[高斯和](@entry_id:196588)充分性假设近似成立的情况。而似然模拟则是一种更强大、更通用的方法，能够应对更复杂的数据[分布](@entry_id:182848)和信息损失问题，但其代价是需要更灵活的模型和通常更大的训练数据集。

### 模拟器架构及其先验

选定策略后，我们需要选择具体的机器学习模型来实现它。每种架构都带有其固有的假设或“[归纳偏置](@entry_id:137419)”，这些偏置可以被看作是关于待学习函数性质的先验知识。

#### [多项式混沌展开](@entry_id:162793)

**[多项式混沌展开](@entry_id:162793) (Polynomial Chaos Expansion, PCE)** 是一种经典的、基于函数展开的模拟方法，尤其适用于输入参数的[概率分布](@entry_id:146404)已知的情况 [@problem_id:3478356]。其核心思想是将模型的输出（一个[随机变量](@entry_id:195330)，因为其输入是[随机变量](@entry_id:195330)）展开为一组关于输入[随机变量](@entry_id:195330)的正交多项式基的级数。

对于一个输出量 $Y = f(\boldsymbol{\xi})$，其中 $\boldsymbol{\xi} = (\xi_1, \dots, \xi_d)$ 是一个由[独立随机变量](@entry_id:273896)组成的向量，其PCE形式为：
$Y \approx \sum_{\alpha \in \mathcal{J}} c_{\alpha} \Psi_{\alpha}(\boldsymbol{\xi})$
这里，$\{\Psi_{\alpha}\}$ 是一组多元[正交多项式](@entry_id:146918)基，$\alpha$ 是一个多维索引，$\mathcal{J}$ 是一个截断的索引集（例如，总阶数小于等于某个值 $p$），$c_{\alpha}$ 是待确定的展开系数。

多项式基的选择取决于输入变量的[概率分布](@entry_id:146404)，以确保正交性。根据Wiener-[Askey方案](@entry_id:187960)，对于标准高斯输入的变量，应选择**[Hermite多项式](@entry_id:153594)**；对于[均匀分布](@entry_id:194597)的变量，应选择**[Legendre多项式](@entry_id:141510)**。例如，如果[宇宙学参数](@entry_id:161338)被转换为独立的标准[高斯变量](@entry_id:276673) $\xi$ 和 $\eta$，则二维[Hermite基函数](@entry_id:167915)形如 $\psi_{\alpha}(\xi, \eta) = \psi_{\alpha_1}(\xi)\psi_{\alpha_2}(\eta)$，其中 $\psi_n(x)$ 是归一化的概率论者[Hermite多项式](@entry_id:153594)，例如 $\psi_0(x)=1$, $\psi_1(x)=x$, $\psi_2(x)=(x^2-1)/\sqrt{2}$ [@problem_id:3478356]。

系数 $c_{\alpha}$ 可以通过**投影**或**回归**来确定。由于基[函数的正交性](@entry_id:160337)，即 $\mathbb{E}[\Psi_{\alpha}(\boldsymbol{\xi}) \Psi_{\beta}(\boldsymbol{\xi})] = \delta_{\alpha\beta}$（经过归一化），系数可以通过[Galerkin投影](@entry_id:145611)简单地计算出来：
$c_{\alpha} = \mathbb{E}[f(\boldsymbol{\xi}) \Psi_{\alpha}(\boldsymbol{\xi})]$
在实践中，这个期望可以通过[数值积分](@entry_id:136578)（如[稀疏网格](@entry_id:139655)或高斯求积）或[蒙特卡洛采样](@entry_id:752171)来估计。或者，也可以通过在一个设计好的参数点集上运行模拟，然后通过[最小二乘回归](@entry_id:262382)来拟合系数。对于[正交基](@entry_id:264024)，这两种方法是等价的 [@problem_id:3478356]。

PCE的优点在于其坚实的数学基础、收敛性保证以及能够直接提供[全局敏感性分析](@entry_id:171355)（通过系数 $c_{\alpha}$）的能力。其缺点是，对于高维输入或高度[非线性](@entry_id:637147)的函数，可能需要非常高阶的多项式才能达到所需的精度，这会受到“[维度灾难](@entry_id:143920)”的影响。

#### [高斯过程](@entry_id:182192)

**高斯过程 (Gaussian Process, GP)** 是一种非参数的贝叶斯方法，它将函数本身视为一个[随机变量](@entry_id:195330)，并直接在[函数空间](@entry_id:143478)中进行推断 [@problem_id:3478385]。一个高斯过程由其[均值函数](@entry_id:264860) $m(\theta)$ 和[协方差函数](@entry_id:265031)（或称**[核函数](@entry_id:145324)**）$k(\theta, \theta')$ 完全定义。它假设函数在任何一组有限输入点 $\theta_1, \dots, \theta_N$ 上的取值 $(f(\theta_1), \dots, f(\theta_N))$ 服从一个多元[高斯分布](@entry_id:154414)。

GP的强大之处在于核函数。核函数 $k(\theta, \theta')$ 编码了关于待学习函数性质的[先验信念](@entry_id:264565)，例如光滑度、周期性或其它结构。

一个常见的选择是**[平方指数核](@entry_id:191141) (Squared Exponential, SE)**：
$k_{SE}(\theta, \theta') = \sigma_f^2 \exp\left(-\frac{\|\theta - \theta'\|^2}{2\ell^2}\right)$
其中 $\sigma_f^2$ 是信号[方差](@entry_id:200758)，$\ell$ 是长度尺度。SE核对应的函数样本路径是无限可微的（解析的）。这是一种非常强的光滑性先验。然而，许多[物理可观测量](@entry_id:154692)虽然光滑，但并非无限可微。例如，[物质功率谱](@entry_id:161407)中的**[重子声学振荡](@entry_id:158848) (Baryon Acoustic Oscillations, BAO)** 特征是[准周期性](@entry_id:272343)的、且被[非线性](@entry_id:637147)效应和丝绸阻尼所抑制，这导致了函数在特定尺度上具有额外的“高频”内容。使用SE核可能会[过度平滑](@entry_id:634349)这些重要特征 [@problem_id:3478385]。

一个更灵活的选择是**Matérn核**族，它有一个额外的光滑度参数 $\nu$：
$k_{\text{Matérn}}(\tau; \nu, \ell) = \sigma_f^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left(\frac{\sqrt{2\nu}\tau}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}\tau}{\ell}\right)$
其中 $\tau = \|\theta - \theta'\|$，$K_\nu$ 是第二类修正的[Bessel函数](@entry_id:265752)。Matérn核的关键特性是，它产生的函数样本路径是 $\lfloor \nu - 1/2 \rfloor$ 次均方可微的。这允许我们精确地[控制函数](@entry_id:183140)的先验光滑度。
*   $\nu \to \infty$ 时，Matérn核趋于SE核。
*   $\nu=1/2$ 时，得到指数核，其样本路径[连续但不可微](@entry_id:261860)。
*   $\nu=3/2$ 或 $\nu=5/2$ 是常见的选择。例如，$\nu=5/2$ 对应的函数是二次均方可微的。这足以满足大多数物理模型的[光滑性](@entry_id:634843)要求，同时其谱密度具有比SE核更重的尾部，使其能够更好地拟合像BAO这样具有有限带宽[振荡](@entry_id:267781)的特征，而不会[过度平滑](@entry_id:634349) [@problem_id:3478385]。

GP的主要缺点在于其计算复杂度。对于 $N$ 个训练点，训练（最大化[边际似然](@entry_id:636856)）和精确预测的计算成本分别为 $O(N^3)$ 和 $O(N)$，内存成本为 $O(N^2)$。这使得标准GP难以扩展到数千个训练点以上。为了解决这个问题，发展了多种**稀疏GP**方法，如使用 $M \ll N$ 个**诱导点 (inducing points)** 的方法（如FITC或[变分方法](@entry_id:163656)）。这些方法将计算复杂度降低到大约 $O(NM^2)$，从而能够处理更大的数据集 [@problem_id:3478340]。

#### [神经网](@entry_id:276355)络

**[神经网](@entry_id:276355)络 (Neural Networks, NNs)**，特别是[深度前馈网络](@entry_id:635356)，是强大的[通用函数逼近器](@entry_id:637737)。其理论基础是**通用逼近定理 (Universal Approximation Theorem, UAT)**，该定理指出，一个具有足够宽度的单隐藏层网络，只要其激活函数是连续且非多项式的（如Sigmoid或ReLU），就可以以任意精度逼近任何[紧集上的连续函数](@entry_id:146442) [@problem_id:3478363]。

然而，UAT只是一个[存在性定理](@entry_id:261096)，它并不保证我们能够通过训练找到这个网络。在实践中，构建有效的NN模拟器需要考虑以下几点：

*   **[维度灾难](@entry_id:143920)与深度**：尽管UAT在理论上成立，但对于高维输入空间，为了达到给定的逼近精度，所需的神经元数量（宽度）可能会随维度 $d_\theta$ [指数增长](@entry_id:141869)，这就是所谓的“维度灾难”。理论和实践都表明，对于某些类型的函数（特别是具有层次化或组合结构的目标函数），增加网络的深度比增加宽度在参数效率上要高得多。深度网络能够以更少的参数学习复杂的特征，从而在一定程度上缓解维度灾难 [@problem_id:3478363]。

*   **编码物理先验**：与GP不同，标准NN的[归纳偏置](@entry_id:137419)较弱。为了构建物理上合理的模拟器，我们需要主动地将物理知识编码到模型中 [@problem_id:3478338]。
    *   **保证非负性**：如前所述，对于 $C_\ell$ 等非负量，可以通过在网络输出层使用[指数函数](@entry_id:161417) $C_\ell = \exp(g(\ell, \theta))$ 或softplus函数来实现。另一种优雅的方法是将输出建模为一个潜在光滑函数的平方，$C_\ell = (f(\ell, \theta))^2$。
    *   **保证[光滑性](@entry_id:634843)**：可以通过多种方式促进输出关于 $\ell$ 或 $k$ 的[光滑性](@entry_id:634843)。一种方法是将输出表示为一组光滑[基函数](@entry_id:170178)（如[B样条](@entry_id:172303)或高斯包）的[线性组合](@entry_id:154743)，网络只学习组合系数。另一种方法是在损失函数中加入一个粗糙度惩罚项，如 $\int (\partial_\ell^2 C_\ell)^2 d\ell$。更进一步，可以将混合模型思想融入其中，例如，对[基函数](@entry_id:170178)的系数施加一个高斯过程先验，利用GP的核函数来强制[光滑性](@entry_id:634843) [@problem_id:3478338]。

通过这些精心设计的架构和损失函数，NN可以从“黑箱”模型转变为能够尊重基本物理原理的、强大的科学工具。

### 高级训练与验证策略

构建了模拟器架构后，如何有效地训练它并验证其可靠性，是决定其最终成败的关键。

#### 多保真度模拟与[迁移学习](@entry_id:178540)

生成高质量的[宇宙学模拟](@entry_id:747928)数据（高保真度数据，$\mathcal{D}_{\mathrm{HF}}$）通常极为昂贵。然而，我们往往可以廉价地生成大量近似的、但保真度较低的数据（$\mathcal{D}_{\mathrm{LF}}$）。**[迁移学习](@entry_id:178540)**提供了一个框架来有效利用这两种数据 [@problem_id:3478321]。

一个标准的策略是：
1.  **预训练**：在一个大型的低保真度数据集 $\mathcal{D}_{\mathrm{LF}}$ 上训练[神经网](@entry_id:276355)络。这一阶段的目标是让网络学习函数的大范围、全局的结构。为了提高预训练的有效性，可以采用**物理信息损失函数**。例如，在模拟[物质功率谱](@entry_id:161407)时，损失函数可以包含两部分：一部分是与低保真度数据的拟合项，另一部分是正则项，它惩罚模型在低 $k$ 区域偏离已知的线性理论预测 $P_{\mathrm{lin}}(k,z)$。这有助于将模型“锚定”在正确的物理基础上。

2.  **微调**：在预训练模型的基础上，使用少量的高保真度数据 $\mathcal{D}_{\mathrm{HF}}$ 进行进一步的训练（微调）。通常，整个网络的所有层都会被微调，但使用较小的学习率，以避免“[灾难性遗忘](@entry_id:636297)”并防止对小数据集的过拟合。微调阶段的[损失函数](@entry_id:634569)应尽可能地精确，例如，使用考虑了数据点间相关性的高斯[负对数似然](@entry_id:637801) $\ell = (\hat{\mathbf{y}}-\mathbf{y}^{\mathrm{HF}})^{\top}\boldsymbol{\Sigma}^{-1}(\hat{\mathbf{y}}-\mathbf{y}^{\mathrm{HF}})$ [@problem_id:3478321]。

然而，[迁移学习](@entry_id:178540)并非总能成功。如果低保真度数据的物理模型与高保真度相差太远，预训练可能会引入错误的偏置，从而损害最终在高保真度任务上的性能。这种现象被称为**[负迁移](@entry_id:634593)**。由于高保真度数据稀缺，诊断[负迁移](@entry_id:634593)需要非常谨慎。一个鲁棒的方法是使用**K-折[交叉验证](@entry_id:164650)**。在$\mathcal{D}_{\mathrm{HF}}$的每一折上，我们成对地训练两个模型：一个从头随机初始化，另一个从预训练模型初始化。然后比较它们在留出[验证集](@entry_id:636445)上的性能。通过对多折的成对性能差异进行统计检验（如自助法构造[置信区间](@entry_id:142297)），我们可以判断预训练是否带来了显著的性能提升或下降 [@problem_id:3478321]。

#### 模拟器残差的严谨建模

任何模拟器都不可避免地存在误差，即其预测值与真实物理值之间的差异，这被称为**模拟器残差** $r(k,z) = p_{\mathrm{true}}(k,z; \theta) - p_{\mathrm{em}}(k,z; \theta)$。在进行高精度的[宇宙学参数](@entry_id:161338)推断时，忽略这一不确定性来源会导致过于乐观、甚至有偏的参数约束。

一个严谨的处理方法是将模拟器残差本身建模为一个[随机过程](@entry_id:159502)，通常是一个**[高斯过程](@entry_id:182192)** [@problem_id:3478394]。这意味着我们假设残差在 $(k,z)$ 平面上是相关的，其相关性结构由一个GP核函数（如[平方指数核](@entry_id:191141)）描述。这个残差GP有其自身的超参数，如残差的典型幅度 $\sigma_r$ 和相关长度 $\ell_k, \ell_z$。这些超参数可以通过专门的验证集来校准。

在贝叶斯推断中，这个残差模型被直接整合到似然函数中。数据模型变为：
$\mathbf{y}_{\mathrm{obs}} = \mathbf{p}_{\mathrm{em}}(\theta) + \mathbf{r} + \mathbf{n}$
其中 $\mathbf{n}$ 是观测噪声，$\mathbf{r}$ 是服从GP先验的[残差向量](@entry_id:165091)。总的噪声协[方差](@entry_id:200758)变为 $\boldsymbol{\Sigma}_{\mathrm{tot}} = \boldsymbol{\Sigma}_{\mathrm{noise}} + \boldsymbol{\Sigma}_{\mathrm{residual}}$。这里，$\boldsymbol{\Sigma}_{\mathrm{residual}}$ 是由残差GP的[核函数](@entry_id:145324)在数据点上计算得到的协方差矩阵。

引入相关的残差模型通常会使总[协方差矩阵](@entry_id:139155)的非对角项增大，从而降低数据的Fisher信息量。这会导致参数的后验分布变宽，即置信区间扩大。这种扩大是诚实地反映了我们模拟器不完美所带来的额外不确定性。研究表明，残差的幅度和相关长度对后验区间的膨胀有显著影响：残差越大、相关性越强，参数约束就越弱 [@problem_id:3478394]。这种严谨的误差建模是确保最终科学结论鲁棒性的关键步骤。