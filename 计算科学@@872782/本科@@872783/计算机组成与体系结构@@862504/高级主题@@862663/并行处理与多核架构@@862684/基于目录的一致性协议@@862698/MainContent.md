## 引言
在多核处理器已成为计算核心的时代，确保所有核心的内存视图保持一致是系统正确运行的基石。[缓存一致性协议](@entry_id:747051)正是为此而生。虽然简单的监听（snooping）协议在小型系统中行之有效，但其固有的广播机制使其在核心数量增多时迅速成为性能瓶颈，从而限制了系统的[可扩展性](@entry_id:636611)。为了构建拥有数十乃至数千个核心的大规模[并行系统](@entry_id:271105)，我们迫切需要一种更高效的替代方案，这便是本文的主角——**基于目录（directory-based）的一致性协议**。

本文将系统性地引导你深入理解这一关键技术。在第一章“**原理与机制**”中，我们将从根本上探讨目录协议为何能实现[可扩展性](@entry_id:636611)，剖析一次典型写事务的完整消息流，并分析[目录结构](@entry_id:748458)设计中的存储与性能权衡。接着，在第二章“**应用与跨学科关联**”中，我们将视野扩展到实际应用，探索目录协议如何支持软件同步、实现[内存模型](@entry_id:751871)，以及它在[异构计算](@entry_id:750240)、[虚拟化](@entry_id:756508)和安[全等](@entry_id:273198)前沿领域扮演的关键角色。最后，通过第三章“**动手实践**”中的一系列分析性练习，你将有机会亲手应用所学知识，量化协议性能，从而将理论理解转化为解决实际设计问题的能力。

## 原理与机制

在[多核处理器](@entry_id:752266)架构中，确保所有核心看到的内存视图保持一致性是至关重要的。前一章我们介绍了[缓存一致性](@entry_id:747053)的基本概念，并了解到监听（snooping）协议是一种在小型多核系统中广泛采用的有效方法。然而，随着核心数量的增长，监听协议固有的广播通信模式会成为系统性能和[功耗](@entry_id:264815)的瓶颈。为了构建拥有数十、数百甚至数千个核心的可扩展系统，我们需要一种更为高效的机制。本章将深入探讨**基于目录（directory-based）的一致性协议**的原理与机制，它通过消除广播，为[大规模并行计算](@entry_id:268183)提供了坚实的基础。

### 可扩展性的迫切需求：监听协议与目录协议的对比

监听协议的核心思想是让每个缓存控制器“监听”[共享总线](@entry_id:177993)或[互连网络](@entry_id:750720)上的所有内存事务。当一个核心需要修改某个缓存行时，它会广播一个无效化（invalidation）或所有权请求。系统中的其他每个核心都必须处理这个广播消息，以确定是否需要使自己缓存中的相应副本无效。

这种方法的优势在于其简单性和[分布](@entry_id:182848)式特性。然而，它的根本弱点在于其通信模式。对于一个拥有 $N$ 个核心的系统，每次写未命中（write miss）到一个被多个核心共享的缓存行时，都需要向所有其他 $N-1$ 个核心广播消息。这意味着一致性[通信开销](@entry_id:636355)至少与核心数量成线性关系，即 $O(N)$。随着 $N$ 的增大，广播风暴（broadcast storm）会迅速饱和网络带宽，导致延迟急剧增加，从而限制了系统的可扩展性。

为了克服这一限制，**[基于目录的协议](@entry_id:748456)**应运而生。其核心思想是引入一个集中的（或[分布](@entry_id:182848)式的）[数据结构](@entry_id:262134)——**目录（directory）**——来取代广播。对于内存中的每一个缓存行，目录中都有一个对应的条目，精确地记录着该缓存行当前的状态以及在哪些核心的缓存中存在副本（即**共享者集合**，sharer set）。当一个核心需要对某缓存行进行写操作时，它不再向所有核心广播，而是向负责该缓存行的目录发送一个点对点（point-to-point）的请求。目录根据记录的共享者信息，只向那些真正持有该缓存行副本的核心发送精准的、目标明确的无效化消息。

让我们通过一个量化模型来比较这两种协议的性能差异 [@problem_id:3684761]。假设在一个拥有 $N$ 个核心的系统中，一次写未命中到一个被广泛共享的缓存行，我们需要评估其服务时间。服务时间主要由两部分构成：网络消息传输的总开销和协议处理的固定开销。

在**监听协议**中，主要的[通信开销](@entry_id:636355)来自一次向 $N-1$ 个核心的多播无效化消息。若该消息大小为 $m$ flits（flit是网络流控的基本单位），则总共需要投递 $(N-1)m$ 个flits。此外，还需要一个大小为 $L$ flits的数据响应消息。若每个flit的平均网络服务时间为 $\tau$，并存在一个固定的全局仲裁开销 $\theta$，则总服务时间可以建模为：
$T_{\text{snoop}}(N) = \tau \big( (N-1)m + L \big) + \theta$

在**目录协议**中，假设平均只有 $s$ 个核心共享该缓存行（由于程序局部性原理， $s$ 通常远小于 $N$ 且不随 $N$ 显著增长）。其消息流如下：
1.  请求者向目录发送一个大小为 $m$ 的请求。
2.  目录向 $s$ 个共享者发送大小为 $m$ 的无效化消息。
3.  这 $s$ 个共享者各自向目录回复一个大小为 $a$ 的确认（acknowledgment）消息。
4.  目录（或原始数据持有者）向请求者发送大小为 $L$ 的数据响应。

此外，目录查找本身也存在一个固定的处理开销 $\delta$。因此，总服务时间为：
$T_{\text{dir}} = \tau \big( m + sm + sa + L \big) + \delta$

可以看到，$T_{\text{snoop}}$ 是 $N$ 的线性函数，而 $T_{\text{dir}}$ 是一个与 $N$ 无关的常数（在 $s$ 保持稳定的前提下）。这意味着必然存在一个核心数量的[临界点](@entry_id:144653)，超过该点，目录协议的效率将开始超越监听协议。例如，使用一组符合现代[多核处理器](@entry_id:752266)特征的典型参数（如 $m=2$, $L=16$, $a=1$, $s=4$, $\tau=1$, $\theta=6$, $\delta=18$），我们可以通过求解不等式 $T_{\text{dir}}  T_{\text{snoop}}(N)$ 发现，当核心数 $N$ 大于14时，目录协议的单次未命中处理延迟就更低了 [@problem_id:3684761]。这清晰地揭示了为何对于追求更高核心数量的现代处理器而言，[基于目录的协议](@entry_id:748456)是实现可扩展[缓存一致性](@entry_id:747053)的必然选择。

### 核心机制：一次典型的目录事务剖析

理解了目录协议的“为何”，接下来我们深入其“如何”工作。我们将以一个经典的**MESI（Modified, Exclusive, Shared, Invalid）协议**为例，剖析一次写未命中（write miss）事务的完[整流](@entry_id:197363)程。MESI是四态协议，分别代表修改、独占、共享和无效。

假设系统遵循**[顺序一致性](@entry_id:754699)（Sequential Consistency）**模型，该模型要求所有处理器以相同的顺序观察到所有写操作，且一个写操作必须在获得独占所有权后才能对其他处理器可见。

考虑这样一个场景：某个核心（请求者）希望对一个缓存行执行写操作，但其本地缓存中没有该行的有效副本（即写未命中）。此时，该缓存行正被 $m$ 个其他核心以**共享（Shared, S）**状态持有。在S状态下，缓存行的数据是“干净”的，意味着其内容与[主存](@entry_id:751652)一致。

以下是处理此事务的最小化消息交互序列 [@problem_id:3635593]：

1.  **请求所有权 (Request for Ownership)**：请求者向该缓存行归属的目录节点发送一个**读所有权请求（Read-For-Ownership, GETM）**消息。此消息表明：“我需要该行的数据，并打算修改它。”

2.  **目录处理与无效化 (Directory Processing and Invalidation)**：目录收到 `GETM` 请求后，查询对应条目。它发现该行处于共享状态，并且共享者列表中有 $m$ 个核心。为了将独占所有权授予请求者，目录必须首先收回所有其他副本。于是，目录向这 $m$ 个共享者中的每一个都发送一条**无效化（Invalidation, INV）**消息。

3.  **共享者响应 (Sharer Response)**：每个收到 `INV` 消息的共享者核心，会将其本地缓存中对应行的状态从 S 变为**无效（Invalid, I）**。完成状态转换后，它必须向目录回复一条**无效化确认（Invalidation Acknowledgment, ACK）**消息，告知目录：“我已经完成无效化操作。”

4.  **序列化与等待 (Serialization and Waiting)**：这是保证一致性的关键一步。由于系统遵循[顺序一致性](@entry_id:754699)，目录在授予请求者写权限之前，**必须**确保所有旧的共享副本都已失效。这意味着目录必须等待，直到它收集到所有 $m$ 个 `ACK` 消息。如果目录在收到所有确认前就授权写入，那么尚未完成无效化的核心仍可能读取到旧数据，而此时请求者可能已经写入了新数据，从而破坏了一致性视图。

5.  **授予权限与数据传输 (Granting Permission and Data Transfer)**：当第 $m$ 个 `ACK` 到达后，目录确认全局范围内除了即将成为新所有者的请求者外，已无其他核心持有该行的有效副本。此时，它可以安全地完成事务的最后步骤：
    *   由于原始状态是 S（干净），主存中的数据是最新的。目录从主存获取数据。
    *   目录向请求者发送一个**数据（DATA）**消息，其中包含了缓存行的数据。同时，它将**授权（GRANT）**信息“捎带”在该消息中。
    *   目录更新自己的记录：将该行的所有者更新为请求者，状态标记为**修改（Modified, M）**，并清空共享者列表。

6.  **请求者完成写入 (Requester Completes Write)**：请求者收到 `DATA + GRANT` 消息后，将数据载入其缓存，将该行状态置为 M，然后执行写操作。

至此，一次完整的、保证[顺序一致性](@entry_id:754699)的写未命中事务处理完毕。整个过程涉及 $1 + m + m + 1 = 2m+2$ 条点对点消息，避免了任何形式的全局广播。

### 目录的组织结构与存储开销

目录本身是一个需要存储在硬件中的[数据结构](@entry_id:262134)，其设计直接影响到系统的成本和性能。一个目录条目至少需要存储两类信息：该缓存行的一致性**状态**（如M, S, I），以及它的**共享者信息**。

最直观的实现方式是**全[位向量](@entry_id:746852)（full sharer bit-vector）**方案 [@problem_id:3635575]。对于一个拥有 $N$ 个核心的系统，每个目录条目中都包含一个 $N$ 位的向量。如果第 $i$ 位被置为1，就表示第 $i$ 个核心的缓存中存有该行的副本。

这种设计的优点是信息完备且查找迅速。然而，它的存储开销巨大。如果一个系统的末级缓存（LLC）有 $M$ 个缓存行，那么目录的总存储开销将是 $M \times (N + \text{状态位} + \text{标签位})$。当核心数量 $N$ 变得非常大时（例如，成百上千），仅仅是[位向量](@entry_id:746852)本身就会占据惊人的存储空间，这在成本和功耗上往往是不可接受的。

为了解决这个问题，研究人员提出了多种压缩方案，其中最常见的是**有限指针（limited-pointer）**方案。这种方案不再为每个核心预留一位，而是在每个目录条目中设置固定数量的“指针槽”，例如 $k$ 个。每个指针槽用来存储一个共享者核心的ID。要唯一标识 $N$ 个核心，每个ID需要 $\lceil \log_2(N) \rceil$ 位。

这种方案的存储开销不再与 $N$ 线性相关，而是与指针数量 $k$ 相关。当然，这种效率是以牺牲通用性为代价的：如果共享一个缓存行的核心数量超过了 $k$，就会发生**目录[溢出](@entry_id:172355)（directory overflow）**。我们将在后续小节讨论如何处理[溢出](@entry_id:172355)。

我们可以精确地分析这两种方案的存储成本。全[位向量](@entry_id:746852)方案中，每个条目的共享者信息占用 $N$ 位。在有限指针方案中，每个指针需要 $\lceil \log_2(N) \rceil$ 位来表示核心ID，再加上一个有效位来标记该指针槽是否被使用，共计 $\lceil \log_2(N) \rceil + 1$ 位。如果有 $k$ 个指针，则总共占用 $k \times (\lceil \log_2(N) \rceil + 1)$ 位。通过令两者相等，我们可以找到一个临界指针容量 $k^*$，在该点上两种方案的存储开销相同 [@problem_id:3635575]：
$k^* = \frac{N}{\lceil\log_2(N)\rceil + 1}$

例如，在一个64核系统中（$N=64$），$\lceil \log_2(64) \rceil = 6$。此时 $k^* = 64 / (6+1) \approx 9.14$。这意味着，只要一个目录条目平均需要跟踪的共享者数量少于9个，使用有限指针方案就比全[位向量](@entry_id:746852)方案更节省空间。鉴于大多数应用中广泛共享的数据块相对较少，有限指针方案通常是更具成本效益的选择。

### 管理有限的目录资源

采用有限指针方案或其他压缩目录方案虽然节省了空间，但也引入了新的挑战：如何处理资源耗尽的情况。这主要表现为两种[溢出](@entry_id:172355)：单个缓存行的共享者数量超出指针容量（sharer overflow），以及目录本身没有足够条目来跟踪新的活动缓存行（directory entry overflow）。

#### 共享者[溢出](@entry_id:172355)

当一个缓存行的共享者数量超过了指针容量 $k$ 时，目录将无法精确记录所有共享者的ID。这时必须启动一个**回退机制（fallback mechanism）**。常见的处理方式包括：
-   **粗粒度[位向量](@entry_id:746852)（Coarse-grained Vector）**：将 $N$ 个核心分组，用一位代表一个核心组。
-   **广播（Broadcast）**：放弃精确跟踪，在该行需要无效化时，退化为向所有核心广播无效化消息。
-   **基于区域的广播**：如果系统物理布局支持，可以向可能包含共享者的一个或多个区域进行广播。

选择哪种回退机制取决于对性能和复杂性的权衡。预测共享者[溢出](@entry_id:172355)发生的频率对于系统设计至关重要，它帮助架构师决定 $k$ 的取值。我们可以通过概率模型来估算溢出概率 [@problem_id:3635518]。假设一个缓存行的并发共享者数量 $X$ 服从均值为 $s$ 的泊松分布，那么[溢出](@entry_id:172355)（即 $X > k$）的概率为：
$P(\text{overflow}) = P(X > k) = 1 - P(X \le k) = 1 - \sum_{i=0}^{k} \frac{e^{-s} s^i}{i!}$

例如，在一个系统中，如果平均共享者数量 $s=5$，而目录指针容量 $k=8$，通过计算可以得出溢出概率约为 $0.068$。这意味着大约有 6.8% 的情况下，系统需要启用较慢的回退机制。这个概率可以指导架构师在存储成本和平均性能之间做出合理的权衡。

#### 目录条目[溢出](@entry_id:172355)

目录本身通常也作为一种缓存（Directory Cache）来实现，其条目数量远少于主存中的缓存行总数。当一个请求访问一个当前未被目录跟踪的缓存行，而目录又已满时，就必须**驱逐（evict）**一个现有的目录条目来为新行腾出空间。

驱逐目录条目是一个极其精细且关键的操作，错误的实现会严重破坏一致性。一个绝对不能犯的错误是**静默驱逐（silent eviction）** [@problem_id:3635532]，即在不通知任何核心的情况下，直接丢弃一个目录条目。
-   如果被驱逐的行处于 M 状态，其最新数据仅存在于所有者缓存中。静默驱逐将导致系统“遗忘”这个最新数据的位置，后续的读请求会从[主存](@entry_id:751652)获取到**陈旧数据（stale data）**，同时所有者的修改也会**丢失（lost write）**。
-   如果被驱逐的行处于 S 状态，静默驱逐将导致目录“遗忘”共享者列表。这些共享者对此一无所知，继续持有它们认为有效的只读副本。当一个新的写请求到来时，目录因为没有记录，会直接授予新请求者 M 状态，但却无法向旧的共享者发送无效化消息。这将导致系统中同时存在一个写者和多个持有旧数据的读者，严重违反了**单写多读（Single-Writer/Multiple-Readers, SWMR）**不变式。

正确的驱逐过程，通常称为**干净驱逐（clean eviction）**，必须在释放条目之前，将被驱逐的缓存行恢复到一个已知的、安全的状态。这个过程如下 [@problem_id:3635532]：
1.  **静默化（Quiesce）**：首先，目录暂时拒绝（例如通过发送NACK消息）所有新到达的、针对该行的请求，以防止在驱逐过程中发生状态改变。
2.  **回收（Recall）**：
    *   如果行处于 M 状态，目录向所有者发送一个**回写（write-back）**请求，强制其将脏数据[写回](@entry_id:756770)[主存](@entry_id:751652)，并将其本地状态置为 I。
    *   如果行处于 S 状态，目录向其记录的所有共享者发送无效化（INV）消息。
3.  **等待确认（Wait for Acknowledgments）**：目录必须等待所有者的回写完成或所有共享者的 `ACK` 消息返回。
4.  **释放条目（Free Entry）**：确认所有缓存副本都已被清除，且主存数据是最新版本后，目录才可以安全地释放该条目，用于跟踪新的缓存行。

更高级的系统可能会使用**溢出结构（overflow structures）**，如[受害者缓存](@entry_id:756499)（victim cache）或[布隆过滤器](@entry_id:636496)（Bloom filter），来保存被驱逐条目的部分信息，从而在不进行完全回收的情况下维护一致性，但这大大增加了协议的复杂性 [@problem_id:3635532]。

### [性能优化](@entry_id:753341)与考量

一个正确实现的目录协议只是基础，为了追求更高的性能，架构师们发展了多种[优化技术](@entry_id:635438)。

#### Cache-to-Cache 传输与 MOESI 协议

在基础的 MESI 协议中，当一个核心（读者）试图读取一个被另一个核心（所有者）以 M 状态持有的缓存行时，通常会发生以下低效的交互：目录指示所有者将脏数据[写回](@entry_id:756770)[主存](@entry_id:751652)，然后读者再从[主存](@entry_id:751652)读取数据。这个“所有者 → 内存 → 读者”的两步过程引入了不必要的内存访问延迟。

为了优化这一点，许多现代系统采用了 **MOESI 协议**，它在 MESI 的基础上增加了一个**持有（Owned, O）**状态 [@problem_id:3635556]。O 状态的含义是：该缓存行的数据是脏的（即与主存不一致），但可能存在多个共享副本。持有 O 状态的核心是“所有者”，它有责任在未来某个时刻将数据[写回](@entry_id:756770)[主存](@entry_id:751652)，并为新的读请求提供数据。

有了 O 状态，上述场景的处理流程变为：
1.  读者向目录发送读请求。
2.  目录发现所有者持有 M 状态的行，于是向所有者转发该请求。
3.  所有者直接将数据发送给读者（**Cache-to-Cache 传输**），同时将自己的状态从 M 变为 O。
4.  读者接收数据，并将自己的状态置为 S。
5.  目录更新记录，将所有者标记为 O 状态，并将读者加入共享者列表。

这个过程避免了对[主存](@entry_id:751652)的访问，显著降低了读未命中延迟。通过一个具体的读写序列模型可以发现，对于“一个核心写，多个核心读”的模式，[MOESI协议](@entry_id:752105)相比MESI能有效减少消息数量，其根源就在于避免了第一次读脏数据时的强制回写操作 [@problem_id:3635556]。

Cache-to-Cache 传输的优势可以通过一个细致的性能模型来量化 [@problem_id:3635488]。读未命中的总延迟主要由网络[传输延迟](@entry_id:274283)（与消息大小和跳数相关）和处理延迟（如目录查找、DRAM访问）组成。由于[片上网络](@entry_id:752421)（NoC）的带宽远高于D[RAM](@entry_id:173159)带宽，且DRAM访问延迟（通常为数十到上百纳秒）远高于缓存访问延迟，直接从所有者缓存获取数据几乎总是比通过内存更快。当存在大量独立的读未命中时，系统的[吞吐量](@entry_id:271802)将受限于数据源的带宽。如果数据源是所有者缓存，[吞吐量](@entry_id:271802)受限于NoC带宽；如果是内存，则受限于较慢的D[RAM](@entry_id:173159)带宽。因此，Cache-to-Cache 传输不仅降低了延迟，还提升了系统的整体数据供给能力。

#### [伪共享](@entry_id:634370)与分区缓存

[缓存一致性协议](@entry_id:747051)是在**缓存行（cache line）**粒度上工作的。一个标准的缓存行大小为64字节或128字节。如果两个核心频繁地、独立地写入同一个缓存行的不同部分（例如，核心A写第1个字，核心B写第10个字），就会产生**[伪共享](@entry_id:634370)（false sharing）**问题 [@problem_id:3635514]。

在这种情况下，尽管两个核心的写操作在逻辑上并不冲突，但由于它们位于同一个物理缓存行上，一致性协议会认为这是一个写-写冲突。结果是，该缓存行的所有权会在两个核心之间来回“乒乓”，每次所有权转移都伴随着一次昂贵的无效化事务。这种由协议粒度与实际数据共享粒度不匹配造成的性能下降，就是[伪共享](@entry_id:634370)。

解决[伪共享](@entry_id:634370)的一种硬件方法是采用**分区缓存（sectored cache）**或**子块（sub-blocking）**。其思想是将一个缓存行划分为多个更小的、可独立跟踪一致性状态的扇区（sector）或子块。例如，一个64字节的行可以被划分为4个16字节的子块。这样，只有当两个核心的写操作落入同一个子块时，才会发生一致[性冲突](@entry_id:152298)。

我们可以量化这种优化带来的好处。假设一个缓存行有 $N$ 个字，被划分为 $k$ 个子块（每个子块大小为 $S=N/k$）。如果两个核心随机写入该行的两个不同位置，它们落入同一个子块的概率为 $(S-1)/(N-1)$。无效化率与此概率成正比。当不分子块时（$S=N$），冲突概率为1。当子块大小减半时，冲突概率也近似减半。通过减小子块大小，可以显著降低因[伪共享](@entry_id:634370)导致的无效化流量 [@problem_id:3635514]。

#### 序列化与阻塞

目录作为每个缓存行的集中仲裁点，其本身也可能成为性能瓶颈。所有针对同一行的请求都必须在目录处排队序列化。例如，当目录正在处理一个耗时较长的[写回](@entry_id:756770)操作时，新到达的针对该行的读请求就必须**阻塞（stall）**等待 [@problem_id:3635571]。

我们可以使用排队论来对此进行建模。将目录条目视为一个服务台，[写回](@entry_id:756770)请求以泊松过程到达，其服务时间为某个随机[分布](@entry_id:182848)。根据[排队论](@entry_id:274141)中的PASTA（Poisson Arrivals See Time Averages）特性，一个泊松到达的读请求发现目录正忙于处理写回的概率，等于目录在长时间内处于忙碌状态的时间比例。这个比例就是写回请求对目录造成的**利用率（utilization）**，即 $\rho_w = \lambda_w \mathbb{E}[S_w]$，其中 $\lambda_w$ 是写回的到达率，$\mathbb{E}[S_w]$ 是平均服务时间。这个简单的模型揭示了系统中的不同操作流是如何通过共享资源（目录）相互影响的。

### 在并发环境中确保正确性

真实的硬件系统充满了并发和竞争。网络消息的[传输延迟](@entry_id:274283)是不可预测的，可能导致消息[乱序](@entry_id:147540)到达。一个健壮的目录协议必须在所有可能的事件交错（interleaving）下都能维持一致性。

考虑一个经典的竞争场景：核心A正在驱逐它持有的缓存行 $x$（发送`Put`消息），而与此同时，核心B正在请求对 $x$ 的写权限（发送`Upgrade`消息） [@problem_id:3635573]。这两个消息可能以任意顺序到达目录。

为了在这种情况下保证正确性，协议必须遵循严格的排序约束：
1.  **等待所有确认**：如前所述，目录在授予写权限之前，绝对不能“抄近路”。它必须等待所有其他共享者（包括可能正在驱逐自己的核心A）的无效化确认或驱逐通知都已收到。这是维护SWMR不变式的基本要求。
2.  **处理陈旧[写回](@entry_id:756770)**：一个更微妙的问题是**陈旧[写回](@entry_id:756770)（stale writeback）**。设想核心A是行 $x$ 的所有者。目录向A发送了一个回写请求，但该请求在网络中延迟了。目录可能超时并假定A已宕机，于是将所有权授予了B。很久以后，A的旧的回写消息才到达[内存控制器](@entry_id:167560)。如果这个陈旧的数据被写入内存，它将覆盖B写入的新数据，导致[数据损坏](@entry_id:269966)。

为了解决这个问题，许多系统采用了**版本号（versioning）**或**纪元（epoch）**机制。目录为每个缓存行的每次独占授权分配一个单调递增的版本号。所有者在回写数据时，必须附上它所知的版本号。[内存控制器](@entry_id:167560)只接受与其记录的当前版本号相匹配的[写回](@entry_id:756770)请求，并丢弃所有来自旧版本的陈旧写回。这个机制确保了即使在消息严重[乱序](@entry_id:147540)的情况下，内存的状态也能正确演进 [@problem_id:3635573]。

总而言之，基于目录的一致性协议通过用精确的点对点通信取代全局广播，为构建大规模多核系统提供了可扩展的基础。然而，这种[可扩展性](@entry_id:636611)并非没有代价。它要求精心的设计来管理目录的存储开销、处理资源溢出，并通过各种[优化技术](@entry_id:635438)来提升性能，同时还需要复杂的机制来保证在高度并发和不可靠的网络环境中的绝对正确性。这些原理与机制共同构成了现代[高性能计算](@entry_id:169980)架构的基石。