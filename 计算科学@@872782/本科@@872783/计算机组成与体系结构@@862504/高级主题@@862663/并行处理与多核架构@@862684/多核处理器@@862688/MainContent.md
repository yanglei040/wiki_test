## 引言
在现代计算领域，多核处理器已从前沿技术演变为无处不在的基础设施，为从智能手机到超级计算机的一切设备提供动力。然而，这一转变的背后，是单核性能扩展走到了物理极限的深刻危机。简单地增加核心数量并不能自动转化为性能的提升，反而引入了数据共享、通信和同步等一系列前所未有的复杂挑战。理解并驾驭这些挑战，已成为现代计算机科学家和工程师的必备技能。

本文旨在系统性地揭示多核处理器世界的复杂性与精妙之处。我们将带领读者穿越硬件与软件的层层迷雾，从根本上回答“为什么需要多核”以及“如何有效利用多核”这两个核心问题。

在接下来的内容中，我们将分三个部分展开：第一章“原理与机制”将深入剖析[多核架构](@entry_id:752264)的基石，从功率墙问题到[Amdahl定律](@entry_id:137397)，再到[缓存一致性](@entry_id:747053)与[内存模型](@entry_id:751871)等核心机制；第二章“应用与跨学科连接”将展示这些原理如何在高性能软件设计、[操作系统调度](@entry_id:753016)以及前沿[科学计算](@entry_id:143987)等真实场景中发挥作用；最后，在“动手实践”部分，你将通过解决具体问题，将理论知识转化为解决[伪共享](@entry_id:634370)、保证[内存顺序](@entry_id:751873)和设计[可扩展算法](@entry_id:163158)的实践能力。通过这段旅程，你将建立起对多核处理一个坚实而全面的理解。

## 原理与机制

继上一章介绍了多核处理器的历史背景和发展趋势之后，本章将深入探讨其工作的核心原理与关键机制。我们将从[多核架构](@entry_id:752264)出现的根本原因——功率墙问题——出发，系统性地分析其性能模型、[微架构](@entry_id:751960)设计权衡、片上通信以及[数据一致性](@entry_id:748190)挑战。本章旨在为读者构建一个关于多核处理器如何工作以及为何如此设计的严谨知识框架。

### 单核性能的终结：功率墙与多核的兴起

在二十一世纪初，[处理器设计](@entry_id:753772)遭遇了一个根本性的物理限制，即所谓的**功率墙 (power wall)**。数十年来，[处理器性能](@entry_id:177608)的提升主要依赖于 Dennard 缩放定律，该定律指出，随着晶体管尺寸的缩小，其[功耗](@entry_id:264815)密度保持不变。这使得我们可以在提高[时钟频率](@entry_id:747385)的同时，保持芯片[功耗](@entry_id:264815)在可控范围内。然而，大约在2005年之后，Dennard 缩放定律失效。晶体管尺寸虽然仍在缩小，但其漏电流（leakage current）变得不可忽略，导致无法按比例降低[阈值电压](@entry_id:273725)和供电电压。

处理器的[功耗](@entry_id:264815)主要由两部分组成：**动态功耗 (dynamic power)** 和**[静态功耗](@entry_id:174547) (static power)**，后者主要是漏[电功](@entry_id:273970)耗。动态[功耗](@entry_id:264815)是晶体管在开关时消耗的能量，其模型可以近似为 $P_{dyn} = \alpha C V^{2} f$，其中 $\alpha$ 是活动因子，C 是负载电容，$V$ 是供电电压，$f$ 是[时钟频率](@entry_id:747385)。[静态功耗](@entry_id:174547)则是在晶体管不开关时由于漏电产生的，可以近似为与电压 $V$ 成正比的关系，即 $P_{leak} = \beta V$。

从动态[功耗](@entry_id:264815)公式可见，功耗对电压 $V$ 的依赖是二次方的，而频率 $f$ 与电压 $V$ 密切相关。为了维持时序正确性，电压不能无限降低，存在一个最小安全工作电压 $V_{min}$。当电压缩放走到尽头时，继续提高频率 $f$ 将导致功耗呈指数级增长，产生的热量超出了芯片封装的散热能力。这就是功率墙的本质。

面对这一挑战，业界的设计思路发生了根本性转变：既然无法让单个核心变得更快，那么就在一个芯片上集成多个相对简单、低[功耗](@entry_id:264815)的核心，通过并行处理来提升总体[吞吐量](@entry_id:271802)。这就是多核处理器的由来。然而，这种转变并非没有代价。在一个固定的[功耗](@entry_id:264815)预算 $P_{cap}$ 下，我们虽然可以在芯片上集成大量的核心，但往往无法同时点亮所有核心。

我们可以通过一个模型来理解这个现象 [@problem_id:3639338]。假设一个多核处理器拥有 $n$ 个核心，每个核心的[功耗](@entry_id:264815)为 $P_{core}(V, f) = \alpha C V^{2} f + \beta V$。为了在[功耗](@entry_id:264815)上限 $P_{cap}$ 内激活尽可能多的核心，我们必须在保证系统正常工作的前提下，让每个核心的[功耗](@entry_id:264815)最低。由于核心功耗 $P_{core}(V)$ 是电压 $V$ 的单调递增函数，最低[功耗](@entry_id:264815)必然在最低允许电压 $V_{min}$ 下取得，即 $P_{core,min} = P_{core}(V_{min})$。因此，在[功耗](@entry_id:264815)预算内能同时激活的最大核心数 $n_{max\_active} = \frac{P_{cap}}{P_{core,min}}$。如果一个芯片上物理集成的核心数 $n$ 超过了 $n_{max\_active}$，那么至少会有一个核心因为功耗限制而必须保持关闭状态。例如，在一个具体的设计场景中，若 $P_{cap} = 95$ 瓦，计算得出在 $V_{min}$ 下单个核心的最小[功耗](@entry_id:264815)为 $0.595$ 瓦，那么最多可以同时激活 $\lfloor 95 / 0.595 \rfloor = 159$ 个核心。一旦芯片设计集成到 $160$ 个核心，就必然会出现至少一个核心无法被供电的情况。

这种“虽然存在但无法使用”的晶体管资源被称为**[暗硅](@entry_id:748171) (dark silicon)**。[暗硅](@entry_id:748171)问题是现代多核[处理器设计](@entry_id:753772)的一个核心约束，它意味着性能的提升不再仅仅是增加核心数量，而是在[功耗](@entry_id:264815)、核心数量和单核性能之间进行复杂的权衡与调度。

### 多核时代的性能：并行性及其限制

转向[多核架构](@entry_id:752264)意味着性能的来源从提高单核频率转变为挖掘**[线程级并行](@entry_id:755943)性 (Thread-Level Parallelism, TLP)**。然而，仅仅增加核心数量并不总能带来线性的性能提升。几个基本的理论模型揭示了[多核性能](@entry_id:752230)扩展的内在限制。

#### 修正的 Amdahl 定律

经典的 **Amdahl 定律** 指出，程序加速比受限于其固有的串行部分。对于一个串行部分占比为 $s$ 的程序，使用 $N$ 个核心的理论加速比为 $S(N) = \frac{1}{s + \frac{1-s}{N}}$。当 $N \to \infty$ 时，最[大加速](@entry_id:198882)比为 $\frac{1}{s}$。

在多核时代，这个模型需要修正，因为它忽略了功率约束。如前所述，激活更多核心通常需要降低所有核心的共享时钟频率以维持在[功耗](@entry_id:264815)预算内。考虑一个更实际的频率缩放模型，其中当 $N$ 个核心被激活时，频率变为 $f(N) = \frac{f_0}{\sqrt{N}}$，其中 $f_0$ 是单核工作时的最高频率 [@problem_id:3620126]。在这种情况下，程序的执行时间由两部分组成：串行部分必须在单个核心上以较低的频率 $f(N)$ 运行，其时间会变长；并行部分则由 $N$ 个核心以 $f(N)$ 的频率分担。推导出的新加速比公式变为 $S(N) = \frac{\sqrt{N}}{sN + 1-s}$。对这个函数求导并寻找其最大值，可以发现存在一个最优的核心数量 $N^{\star} = \frac{1-s}{s}$。例如，如果一个程序有 $20\%$ 的串行部分 ($s=0.2$)，那么最优的核心数是 $N^{\star} = \frac{1-0.2}{0.2} = 4$。超过这个数量，由于频率下降带来的性能损失将超过并行化带来的收益，总性能反而会下降。这揭示了一个深刻的结论：对于给定的程序，并不是核心越多越好。

#### 并行开销的影响

另一个限制来自于并行执行本身引入的**开销 (overhead)**。这些开销包括[线程同步](@entry_id:755949)、[数据通信](@entry_id:272045)、[缓存一致性](@entry_id:747053)维护等，并且通常随着核心数量 $N$ 的增加而增长。我们可以将这种开销建模为总工作量的一部分，即总工作量变为 $W + o(N)$，其中 $W$ 是原始单核工作量，而 $o(N)$ 是随 $N$ 变化的开销函数 [@problem_id:3660955]。例如，一个包含线性和二次项的开销模型 $o(N) = aN + bN^2$ 可以分别代表每个核心的管理开销和核心间两两交互的[通信开销](@entry_id:636355)。

在这种模型下，加速比为 $S(N) = \frac{NW}{W + o(N)} = \frac{NW}{W + aN + bN^2}$。同样，通过微积分可以找到一个使加速比最大化的最优核心数 $N^{\star} = \sqrt{\frac{W}{b}}$。这表明，当并行开销（特别是像 $bN^2$ 这样快速增长的开销）变得显著时，无限制地增加核心数量将导致开销淹没有效计算，从而降低整体性能。一个具体的计算例子显示，在特定的工作负载和开销参数下，最优核心数可能只有 $10$ 个，而此时的最[大加速](@entry_id:198882)比远低于 $10$ [@problem_id:3660955]。

#### Roofline 模型

**Roofline 模型**是一个直观的性能分析工具，它将处理器的性能上限与内存带宽限制结合起来，以预测特定应用在给定硬件上的[可达性](@entry_id:271693)能。该模型的[横轴](@entry_id:177453)是**[运算强度](@entry_id:752956) (Arithmetic Intensity)**，定义为程序总的[浮点运算次数](@entry_id:749457)除以总的内存访问字节数，单位是 $FLOPs/Byte$。纵轴是性能，单位是 $GFLOP/s$ (每秒十亿次[浮点运算](@entry_id:749454))。

一个多核处理器的理论峰值计算性能 $P_{peak}$ 由其所有核心的计算能力总和决定，这是一个水平的“屋顶”。同时，系统还有一个共享的内存带宽 $B$，它与[运算强度](@entry_id:752956) $I$ 的乘积 $B \times I$ 构成了一个倾斜的“屋脊”。一个应用的可达性能 $P_{attainable}$ 受限于这两个边界，即 $P_{attainable} = \min(P_{peak}, B \times I)$。

在一个拥有 $N$ 个核心的系统中，峰值计算性能 $P_{peak}(N)$ 随 $N$ [线性增长](@entry_id:157553)，而[内存带宽](@entry_id:751847) $B$ 通常是一个固定的共享资源 [@problem_id:3660963]。这意味着随着核心数的增加，水平的计算屋顶会不断升高，但倾斜的内存屋脊保持不变。对于[运算强度](@entry_id:752956)较低（即需要大量内存访问）的应用，其性能很快就会撞上内存屋脊，成为**内存密集型 (memory-bound)** 应用。此时，即使增加再多的核心，性能也无法提升，因为瓶颈在于内存系统无法足够快地供应数据。例如，一个拥有6个核心的系统，其理论计算峰值可达 $268.8$ GFLOP/s，但如果运行一个[运算强度](@entry_id:752956)为 $0.65$ FLOP/Byte 的应用，在 $170$ GB/s 的[内存带宽](@entry_id:751847)下，其性能上限被限制在 $170 \times 0.65 = 110.5$ GFLOP/s。这清晰地表明，共享资源（尤其是[内存带宽](@entry_id:751847)）是多核扩展性的一个关键瓶颈。

### [微架构](@entry_id:751960)设计与资源共享

在宏观的性能模型之下，多核处理器的性能还取决于微观层面的设计决策，包括核心本身的结构以及缓存系统的策略。

#### 同步[多线程](@entry_id:752340) (SMT) 与多核心 (CMP)

在芯片面积和[功耗](@entry_id:264815)预算固定的前提下，设计者面临一个经典选择：是构建少数几个功能强大、采用**同步[多线程](@entry_id:752340) (Simultaneous Multithreading, SMT)** 技术的宽发射核心，还是构建大量相对简单的单线程核心？SMT技术允许一个核心在同一时钟周期内发射来自多个硬件线程的指令，其目的是利用**[指令级并行](@entry_id:750671)性 (Instruction-Level Parallelism, ILP)** 不足时产生的[指令流水线](@entry_id:750685)空闲气泡，通过填充其他线程的指令来提高核心的执行单元利用率。

这两种设计（SMT vs. CMP，即Chip Multiprocessing）的性能优劣取决于工作负载的特性 [@problem_id:3661045]。

*   **计算密集型 (Compute-bound) 负载**：如果每个线程都有很高的ILP（例如，每个[时钟周期](@entry_id:165839)可独立执行的指令数接近核心的发射宽度），那么两个独立的物理核心（CMP）通常能提供更高的总吞吐量。例如，若每个线程的ILP为3，在一个4发射宽度的核心上，单个线程无法完全利用核心资源。两个这样的独立核心可以提供 $3+3=6$ 的总IPC（每周期指令数）。而一个支持两线程的SMT核心，即使能混合两个线程的指令，由于资源（如发射队列、寄存器文件）的争用，其总IPC可能只能达到发射宽度的 $90\%$，即 $0.9 \times 4 = 3.6$。在这种情况下，CMP胜出。

*   **内存密集型 (Memory-bound) 负载**：如果线程性能受限于内存访问延迟（例如，缓存未命中导致流水线频繁停顿），SMT的优势就体现出来。当一个线程因等待数据而停顿时，SMT核心可以立即切换到执行另一个线程的指令，从而有效隐藏[内存延迟](@entry_id:751862)。然而，在多核系统中，最终的瓶颈往往是共享的片外[内存带宽](@entry_id:751847)。如果两个线程（无论是在SMT核心上还是在两个CMP核心上）产生的内存请求总量都足以使内存带宽饱和，那么两种设计的最终性能将趋于一致，都被内存系统的“屋顶”所限制。

#### 多核环境下的[缓存策略](@entry_id:747066)

私有缓存在多核处理器中至关重要，但它们的管理策略也面临新的挑战。

*   **写策略：[写回](@entry_id:756770) vs. 写直通**：**写直通 (Write-Through)** 策略在每次写操作时都同时更新缓存和[主存](@entry_id:751652)。**写回 (Write-Back)** 策略则只在缓存中标记被修改的行（脏行），直到该行被替换出缓存时才[写回](@entry_id:756770)[主存](@entry_id:751652)。在单核系统中，写回因利用了**[时间局部性](@entry_id:755846)**（对同一地址的多次写操作只需一次最终的内存写入）而显著减少了内存写带宽的消耗。在多核环境中，这一优势更加关键 [@problem_id:3660962]。一个写密集的应用，如果采用[写回](@entry_id:756770)策略，可以将一个缓存行上的连续多次写操作合并为一次内存写。相比之下，写直通会为每一次写操作都产生内存流量，这不仅增加了处理器写操作的平均延迟（因为需要等待内存响应），更严重的是它会大量消耗宝贵的共享内存带宽，从而影响到所有其他核心的性能。因此，对于写密集型负载，写回策略几乎总是更优的选择。

*   **缓存行大小的权衡**：缓存行（Cache Line）是缓存与主存之间[数据传输](@entry_id:276754)的最小单位。选择合适的缓存行大小 $L$ 是一个微妙的权衡 [@problem_id:3661000]。
    *   **优点（[空间局部性](@entry_id:637083)）**：较大的 $L$ 能更好地利用**空间局部性 (spatial locality)**。当程序访问一个地址时，它很可能在不久的将来访问其附近的地址。一次性取回一个较大的[数据块](@entry_id:748187)（一个缓存行），可以把后续的访问变为缓存命中，从而降低平均访存延迟。对于具有连续访问模式的程序（如数组遍历），这种效应尤其明显，其未命中率与 $L$ 成反比。
    *   **缺点（[伪共享](@entry_id:634370)）**：在多核系统中，较大的 $L$ 会增加**[伪共享](@entry_id:634370) (false sharing)** 的风险。[伪共享](@entry_id:634370)发生于两个或多个核心需要访问同一缓存行内的不同数据字时。例如，核心A写地址 `0x100`，核心B写地址 `0x104`。如果缓存行大小为64字节，这两个地址位于同一缓存行。根据[缓存一致性协议](@entry_id:747051)（后文将详述），当核心A写入时，它必须获得该缓存行的独占所有权，这将导致核心B持有的该行副本失效。随后，当核心B要写入时，它又需要从核心A那里抢夺所有权，导致核心A的副本失效。这种由于访问不同数据但恰好在同一缓存行而导致的缓存行在核心间来回“颠簸”的现象，就是[伪共享](@entry_id:634370)。它会引入大量的额外通信和延迟，而这些通信在逻辑上本是不必要的。[伪共享](@entry_id:634370)的发生概率与缓存行大小 $L$ 成正比。

因此，存在一个最优的缓存行大小 $L^{\star}$，它在利用[空间局部性](@entry_id:637083)的收益和避免[伪共享](@entry_id:634370)的代价之间取得了最佳平衡。这个最优值取决于具体的工作负载特性，可以通过建立总停顿时间的数学模型（一个关于 $1/L$ 的项和另一个关于 $L$ 的项）并求其最小值来确定。

### 通信与一致性：[互连网络](@entry_id:750720)与[内存模型](@entry_id:751871)

多核处理器的核心挑战在于如何高效、正确地管理核心之间的通信与数据共享。这涉及到物理层面的[互连网络](@entry_id:750720)和逻辑层面的[数据一致性](@entry_id:748190)协议。

#### [片上网络](@entry_id:752421) (NoC)

核心之间的通信依赖于**[片上网络](@entry_id:752421) (Network-on-Chip, NoC)**。选择何种网络拓扑结构直接影响了通信的延迟、带宽和[功耗](@entry_id:264815)。常见的三种拓扑结构各有优劣 [@problem_id:3660956]：

*   **环形 (Ring)**：将所有核心连接成一个环。其优点是结构简单，每个节点的度数低（通常为2），易于实现和控制。但其**[网络直径](@entry_id:752428)**（任意两点间最大[最短路径](@entry_id:157568)）与核心数 $N$ 成正比 ($O(N)$)，这意味着长距离通信延迟高。其**对剖带宽**（将网络一分为二的[最小割集](@entry_id:191824)上的总带宽）是一个常数 ($O(1)$)，扩展性很差。因此，环形结构通常只适用于核心数较少的系统（例如 $N \le 16$）。

*   **二维网格 (2D Mesh)**：将核心[排列](@entry_id:136432)成一个二维网格，每个核心连接其上下左右的邻居。这种拓扑结构非常适合平面芯片布局，所有连接都是短距离的本地连接，布线规整。其[网络直径](@entry_id:752428)与 $N$ 的平方根成正比 ($O(\sqrt{N})$)，对剖带宽也与 $N$ 的平方根成正比 ($O(\sqrt{N})$)，提供了比环形好得多的扩展性。路由算法（如简单的XY维度顺序路由）也非常高效且易于实现。因此，二维网格是目前大规模多核处理器中最主流的拓扑结构。

*   **树形 (Tree)**：将核心作为叶节点，通过交换节点组织成一棵树。平衡[二叉树](@entry_id:270401)的直径与 $N$ 的对数成正比 ($O(\log N)$)，提供了最低的通信延迟，非常适合广播、归约等全局通信模式。然而，标准树形结构的根节点是巨大的瓶颈，其对剖带宽为常数 ($O(1)$)。为了解决这个问题，实际中常采用**胖树 (Fat-Tree)**，即越靠近根节点的链路带宽越宽。此外，树形结构在二维平面上的物理布局非常困难，会导致大量长距离、不规则的连线，增加功耗和延迟。

#### [缓存一致性协议](@entry_id:747051)

当多个核心拥有同一内存地址的私有缓存副本时，必须确保对该地址的任何修改最终能被所有核心以一致的方式观察到。这就是**[缓存一致性](@entry_id:747053) (Cache Coherence)** 问题。解决方案是[缓存一致性协议](@entry_id:747051)，主要分为两大类 [@problem_id:3661005]：

*   **监听协议 (Snooping Protocol)**：这种协议通常用于核心数较少的系统中。所有核心连接到一个[共享总线](@entry_id:177993)（或等效的广播网络）上。每个核心的缓存控制器都会“监听”总线上的所有事务。当一个核心需要对某个缓存行进行写操作时，它会向总线广播一个请求。其他核心的控制器监听到这个请求后，会检查自己的缓存状态并做出相应的响应（例如，使自己的副本失效）。监听协议的优点是实现简单，响应快速。但其主要缺点是，每次缓存未命中或写请求都需要向所有 $N-1$ 个其他核心进行广播，这导致控制流量与核心数 $N$ 成正比。当 $N$ 增大时，总线会迅速成为瓶颈。

*   **目录协议 (Directory-based Protocol)**：为了解决监听协议的扩展性问题，核心数较多的系统通常采用目录协议。在这种协议中，系统维护一个中心化的**目录 (directory)**，记录了每个内存块的状态以及哪些核心正在共享该数据。当一个核心发生缓存未命中时，它不再广播，而是向目录发送一个点对点的请求。目录根据记录的信息，向拥有该数据的核心（或主存）转发请求，并只向持有该数据副本的核心发送必要的失效或更新消息。目录协议的[控制流](@entry_id:273851)量与共享该数据副本的核心数 $k$ 成正比，而不是总核心数 $N$。通常情况下，$k$ 远小于 $N$。因此，尽管目录协议的基线延迟（请求-目录-转发）可能更高，但其出色的扩展性使其成为大规模多核系统（例如 $N>16$）的事实标准。通过建立流量模型可以精确计算出，随着 $N$ 的增加，必然存在一个[交叉点](@entry_id:147634)，超过该点后目录协议产生的总流量将低于监听协议。

#### [内存一致性模型](@entry_id:751852)

[缓存一致性](@entry_id:747053)保证了对**单个**内存地址的读写最终是一致的，而**[内存一致性模型](@entry_id:751852) (Memory Consistency Model)** 则定义了对**不同**内存地址的读写操作的顺序。这是[并行编程](@entry_id:753136)中最微妙也最关键的概念之一。

*   **[顺序一致性](@entry_id:754699) (Sequential Consistency, SC)**：这是最直观也最严格的模型。它要求所有核心上的所有内存操作看起来像是按照某个单一的全局顺序执行的，并且每个核心内部的操作顺序与程序代码顺序一致。SC模型简单易懂，但其严格的顺序约束在现代[乱序执行](@entry_id:753020)处理器上实现代价极高，会牺牲大量性能。

*   **松弛一致性模型 (Relaxed Consistency Models)**：为了性能，现代处理器几乎都采用松弛一致性模型。其中，**[总存储顺序](@entry_id:756066) (Total Store Order, TSO)** 是一个被广泛应用的例子（例如在[x86架构](@entry_id:756791)中）。TS[O模](@entry_id:186318)型的核心特点是引入了**[写缓冲](@entry_id:756779) (Store Buffer)**。当一个核心执行写操作时，数据被写入[写缓冲](@entry_id:756779)，然后处理器可以继续执行后续指令，而不必等待写操作完成并对所有其他核心可见。特别是，一个后续的**读操作**可以绕过[写缓冲](@entry_id:756779)中对**不同地址**的写操作，直接从缓存或内存中读取数据。

这种“写-读”重排序会导致在SC模型下不可能出现的结果。考虑以下经典的 litmus test 程序 [@problem_id:3660986]，其中变量 $x$ 和 $y$ 初始值为0：

- **核心 P0**: `store x - 1`; `load r0 - y`
- **核心 P1**: `store y - 1`; `load r1 - x`

在SC模型下，结果 $(r_0, r_1) = (0, 0)$ 是不可能的。因为要得到 $r_0=0$，P0的读必须在P1的写之前；要得到 $r_1=0$，P1的读必须在P0的写之前。这会形成一个 $S_x \to L_y \to S_y \to L_x \to S_x$ 的环形依赖，这在任何全局线性顺序中都是不可能的。

然而，在TS[O模](@entry_id:186318)型下，$(0,0)$ 的结果是可能的：
1. P0 执行 `store x - 1`，但这个写操作进入了P0的[写缓冲](@entry_id:756779)，对P1不可见。
2. P0 继续执行 `load r0 - y`，它绕过了[写缓冲](@entry_id:756779)，从内存读到 $y$ 的初始值0。于是 $r_0 = 0$。
3. 同时，P1 执行 `store y - 1`，这个写操作也进入了P1的[写缓冲](@entry_id:756779)，对P0不可见。
4. P1 继续执行 `load r1 - x`，它绕过了自己的[写缓冲](@entry_id:756779)，从内存读到 $x$ 的初始值0。于是 $r_1 = 0$。

这个例子清晰地展示了松弛[内存模型](@entry_id:751871)如何引入非直观的行为。为了在需要时强制恢复顺序，程序员必须使用**[内存屏障](@entry_id:751859) (Memory Fence)** 或[内存栅栏](@entry_id:751859)指令。一个[内存屏障](@entry_id:751859)可以确保其之前的所有内存操作都完成后，才允许执行其之后的操作。在上述例子中，如果在每个核心的写操作和读操作之间插入一个[内存屏障](@entry_id:751859)，就可以阻止“写-读”重排序，从而禁止 $(0,0)$ 结果的出现，使系统在该段代码上的行为恢复为[顺序一致性](@entry_id:754699)。理解并正确使用[内存屏障](@entry_id:751859)是编写正确的[多线程](@entry_id:752340)程序的关键。