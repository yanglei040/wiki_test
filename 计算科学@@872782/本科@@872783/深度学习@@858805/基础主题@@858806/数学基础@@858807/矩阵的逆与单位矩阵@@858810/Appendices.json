{"hands_on_practices": [{"introduction": "在许多深度学习的优化问题中，我们都会遇到形如 $Ax=b$ 的线性系统。一种看似直接的方法是显式地计算矩阵的逆 $A^{-1}$，但这种方法在数值上通常是不稳定的，特别是对于病态矩阵。本实践将引导您通过一个动手实验，来量化直接求逆的缺陷，并展示添加一个微小的单位矩阵倍数（一种称为吉洪诺夫正则化的技术）如何显著提高数值稳定性 [@problem_id:3147728]。", "problem": "给定一系列线性系统，这些系统是在深度训练循环中对内部迭代进行建模的，其中在每一步，一个对称半正定矩阵 $A$ 由局部二次近似产生，并且通过求解 $A x = b$ 来计算参数更新。你的任务是在受控的合成实验中，量化三种方法的数值稳定性，并确定使用单位矩阵移位系统如何提高鲁棒性：显式矩阵求逆以计算 $A^{-1} b$，直接线性求解 $A x = b$，以及使用 $\\lambda > 0$ 的移位线性求解 $(A + \\lambda I) x = b$。你必须采用编程方法，以双精度实现所有计算，并将所有测试用例的结果汇总到单个、机器可读的行中。\n\n此问题的基本基础：\n- 线性系统 $A x = b$ 和单位矩阵 $I$。\n- $2$-范数 $\\| \\cdot \\|_2$ 和 $2$-范数条件数 $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$。对于对称正定矩阵 $A$，$\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$，其中 $\\lambda_{\\max}(A)$ 和 $\\lambda_{\\min}(A)$ 分别是 $A$ 的最大和最小特征值。\n- 对于对称矩阵，$A + \\lambda I$ 的特征值是 $\\lambda_i(A) + \\lambda$，因此 $\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$，其中 $\\lambda > 0$ 会减小条件数。\n- 通过高斯消去法或 Cholesky 分解求解线性系统的后向稳定性，相对于在浮点运算中显式构造 $A^{-1}$ 的不稳定性。\n\n待计算的度量定义：\n- 对于一个方法，给定噪声向量 $\\eta$ 和真实值 $x^\\star$，放大因子 $g$ 定义为 $g = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2}$，其中 $x$ 是该方法在带噪声的右侧项 $b = A x^\\star + \\eta$ 上生成的估计值。\n- 条件数 $\\kappa_2(A)$ 和 $\\kappa_2(A + \\lambda I)$。\n- 在干净的右侧项 $b_0 = A x^\\star$ 上，显式求逆和直接求解之间的算法差异 $\\delta_{\\text{inv}}$ 定义为 $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$，其中 $x_{\\text{inv,clean}} = A^{-1} b_0$，而 $x_{\\text{solve,clean}}$ 是通过直接求解器求解 $A x = b_0$ 得到的。\n\n实验设置：\n- 所有随机数必须由使用指定种子初始化的可复现伪随机数生成器生成。\n- 对于每个测试用例，构造矩阵 $A$，采样一个其条目在 $[-1,1]$ 内均匀分布的真实值向量 $x^\\star$，形成干净的右侧项 $b_0 = A x^\\star$，从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取一个具有独立分量的高斯噪声向量 $\\eta$，并设置 $b = b_0 + \\eta$。\n- 通过显式求逆计算 $x_{\\text{inv}} = A^{-1} b$，通过求解 $A x = b$ 计算 $x_{\\text{solve}}$，以及通过求解 $(A + \\lambda I) x = b$ 计算 $x_{\\text{shift}}$。\n- 对于因奇异性或数值崩溃而失败的方法，将相应的度量设置为 $+\\infty$。\n- 对于每个测试用例，报告列表 $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$，其中 $\\text{is\\_superior}$ 是 $g_{\\text{shift}}  \\min(g_{\\text{solve}}, g_{\\text{inv}})$ 的布尔值。\n\n包含参数值的测试套件：\n- 用例 1（随机对称正定）：维度 $n = 10$，构造 $A = Q^\\top Q + \\alpha I$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 的条目在 $[-1,1]$ 内均匀分布，使用种子 $7$，设置 $\\alpha = 10^{-2}$，真实值种子 $11$，噪声种子 $13$，噪声标准差 $\\sigma = 10^{-6}$，移位 $\\lambda = 10^{-1}$。\n- 用例 2（希尔伯特矩阵）：维度 $n = 12$，构造希尔伯特矩阵 $A$，其条目为 $A_{ij} = \\frac{1}{i + j + 1}$，其中索引 $i, j \\in \\{0, 1, \\dots, n-1\\}$，真实值种子 $17$，噪声种子 $19$，噪声标准差 $\\sigma = 10^{-4}$，移位 $\\lambda = 10^{-1}$。\n- 用例 3（秩亏对称半正定）：维度 $n = 10$，构造 $B \\in \\mathbb{R}^{r \\times n}$，其中 $r = 8$，条目在 $[-1,1]$ 内均匀分布，使用种子 $23$，并设置 $A = B^\\top B$，真实值种子 $29$，噪声种子 $31$，噪声标准差 $\\sigma = 10^{-6}$，移位 $\\lambda = 1$。\n- 用例 4（预定谱）：维度 $n = 12$，构造 $A = U \\operatorname{diag}(e) U^\\top$，其中 $U$ 是从使用种子 $37$ 生成的高斯矩阵的 $\\operatorname{QR}$ 分解中获得的，特征值 $e_i$ 满足 $\\log_{10}(e_i)$ 在 $-8$ 和 $0$ 之间线性间隔，即 $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$ 对于 $i \\in \\{0, 1, \\dots, n - 1\\}$，真实值种子 $41$，噪声种子 $43$，噪声标准差 $\\sigma = 10^{-5}$，移位 $\\lambda = 10^{-2}$。\n\n你的程序必须：\n- 按照指定顺序为每个用例实现上述构造和度量。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身也是一个形如 $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$ 的方括号逗号分隔列表。", "solution": "我们从基础线性代数原理开始。对于线性系统 $A x = b$，当 $A$ 非奇异时，在精确算术中，解映射为 $b \\mapsto x = A^{-1} b$。此映射对扰动的敏感性由条件数 $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$ 决定。在浮点运算中，显式求逆会放大舍入误差，因为计算 $A^{-1}$ 本身是一个数值敏感的操作，然后乘以 $b$ 会引入进一步的误差。相比之下，基于高斯消去法或 Cholesky 分解的直接线性求解器是后向稳定的：它们计算的是一个轻微扰动系统 $(A + \\Delta A) x = b$ 的精确解，其中 $\\|\\Delta A\\|$ 与机器精度成正比。因此，显式构造 $A^{-1}$ 并使用它比求解 $A x = b$ 的稳定性差。\n\n对于对称正定矩阵 $A$，$2$-范数等于最大奇异值，也等于最大特征值，而 $\\|A^{-1}\\|_2$ 等于最小特征值的倒数。因此，$\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$。考虑移位系统 $(A + \\lambda I) x = b$，其中 $\\lambda > 0$。$A + \\lambda I$ 的特征值是 $\\lambda_i(A) + \\lambda$，对应于 $A$ 的每个特征值 $\\lambda_i(A)$。所以，\n$$\n\\kappa_2(A + \\lambda I) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}.\n$$\n假设 $0  \\lambda_{\\min}(A) \\le \\lambda_{\\max}(A)$。对于 $\\lambda > 0$，定义 $f(\\lambda) = \\frac{\\lambda_{\\max}(A) + \\lambda}{\\lambda_{\\min}(A) + \\lambda}$。其导数为\n$$\nf'(\\lambda) = \\frac{\\lambda_{\\min}(A) - \\lambda_{\\max}(A)}{(\\lambda_{\\min}(A) + \\lambda)^2} \\le 0,\n$$\n这意味着 $f$ 是严格递减的。因此，加上 $\\lambda I$ 会减小条件数。由于 $(A + \\lambda I)^{-1}$ 的算子范数是 $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$，移位系统减小了 $b$ 中扰动的潜在放大。\n\n在存在带噪声的右侧项 $b = A x^\\star + \\eta$ 的情况下，解 $x = A^{-1} b$ 的误差 $\\|x - x^\\star\\|_2$ 在一阶扰动分析中，当 $A$ 条件良好且算法后向稳定时，其大小与 $\\|A^{-1}\\|_2 \\|\\eta\\|_2$ 相当。对于病态的 $A$，$\\|A^{-1}\\|_2$ 很大，噪声会被显著放大。使用 $(A + \\lambda I)^{-1}$ 将 $\\|A^{-1}\\|_2$ 减小到 $\\|(A + \\lambda I)^{-1}\\|_2$，从而抑制了噪声放大，但代价是使解偏离 $x^\\star$。在许多深度学习的背景下，这种移位对应于 Levenberg–Marquardt 阻尼或 Tikhonov 型正则化，通过控制步长来提高鲁棒性。我们使用的度量，即放大因子\n$$\ng = \\frac{\\|x - x^\\star\\|_2}{\\|\\eta\\|_2},\n$$\n捕捉了这种权衡，较小的 $g$ 表示对噪声有更好的鲁棒性。\n\n算法设计：\n- 对于每个测试用例，按照指定的配方确定性地构造 $A$：\n  - 用例 1：$A = Q^\\top Q + \\alpha I$，其中 $Q$ 在 $[-1,1]$ 内均匀分布。\n  - 用例 2：希尔伯特矩阵 $A_{ij} = \\frac{1}{i + j + 1}$。\n  - 用例 3：$A = B^\\top B$，其中 $B$ 在 $[-1,1]$ 内均匀分布，且秩 $r  n$。\n  - 用例 4：$A = U \\operatorname{diag}(e) U^\\top$，其中标准正交矩阵 $U$ 来自 $\\operatorname{QR}$ 分解，特征值 $e_i = 10^{(-8 + 8 \\frac{i}{n - 1})}$。\n- 使用指定的种子生成在 $[-1,1]$ 内均匀分布的 $x^\\star$ 和噪声 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$。计算 $b_0 = A x^\\star$ 和 $b = b_0 + \\eta$。\n- 使用 $2$-范数计算条件数 $\\kappa_2(A)$ 和 $\\kappa_2(A + \\lambda I)$。\n- 计算解：\n  - 显式逆解 $x_{\\text{inv}} = A^{-1} b$ 和在干净右侧项上的 $x_{\\text{inv,clean}} = A^{-1} b_0$。\n  - 求解 $A x = b$ 的直接解 $x_{\\text{solve}}$ 和在干净右侧项上的 $x_{\\text{solve,clean}}$。\n  - 求解 $(A + \\lambda I) x = b$ 的移位解 $x_{\\text{shift}}$。\n- 计算度量：\n  - 放大因子 $g_{\\text{inv}} = \\frac{\\|x_{\\text{inv}} - x^\\star\\|_2}{\\|\\eta\\|_2}$，$g_{\\text{solve}} = \\frac{\\|x_{\\text{solve}} - x^\\star\\|_2}{\\|\\eta\\|_2}$，以及 $g_{\\text{shift}} = \\frac{\\|x_{\\text{shift}} - x^\\star\\|_2}{\\|\\eta\\|_2}$。\n  - 算法差异 $\\delta_{\\text{inv}} = \\frac{\\|x_{\\text{inv,clean}} - x_{\\text{solve,clean}}\\|_2}{\\|x_{\\text{solve,clean}}\\|_2}$。\n- 将优越性确定为 $g_{\\text{shift}}  \\min(g_{\\text{solve}}, g_{\\text{inv}})$ 的布尔值。\n- 通过捕获线性代数错误来处理奇异或近奇异情况；如果求逆或求解失败，则将相应的度量设置为 $+\\infty$。\n- 将每个用例的结果汇总到指定的单行输出格式中。\n\n为何这能证明其优越性：\n- 对于对称正定矩阵 $A$，加上 $\\lambda I$ 会一致地增加所有特征值，从而收紧谱并减小 $\\kappa_2$。逆的算子范数从 $\\frac{1}{\\lambda_{\\min}(A)}$ 减小到 $\\frac{1}{\\lambda_{\\min}(A) + \\lambda}$，从而减少了噪声放大。根据经验，$g_{\\text{shift}}$ 应该小于 $g_{\\text{inv}}$ 和 $g_{\\text{solve}}$，尤其是在病态或秩亏的情况下。\n- 显式求逆往往有更大的 $\\delta_{\\text{inv}}$，特别是对于病态的 $A$，这揭示了其相比于直接求解器较差的数值稳定性。\n- 该测试套件涵盖：\n  - 一个中等条件数的系统，其中所有方法表现相当，移位提供了轻微的鲁棒性。\n  - 一个条件数非常大的希尔伯特矩阵，其中移位显著提高了稳定性。\n  - 一个秩亏的对称半正定系统，其中未移位的方法可能会失败或产生非常大的放大，而移位对问题进行了正则化。\n  - 一个具有跨越多个数量级预定谱的系统，用以对条件数进行压力测试并显示移位的好处。\n\n最终输出格式是一个单行，包含一个方括号括起来的、由每个用例列表 $[g_{\\text{inv}}, g_{\\text{solve}}, g_{\\text{shift}}, \\kappa_2(A), \\kappa_2(A + \\lambda I), \\delta_{\\text{inv}}, \\text{is\\_superior}]$ 组成的逗号分隔列表，顺序与四个用例的顺序一致。", "answer": "```python\n# Python 3.12, numpy 1.23.5, scipy 1.11.4 (not used), standard library only.\nimport numpy as np\n\ndef hilbert(n: int) - np.ndarray:\n    i = np.arange(n).reshape(-1, 1)\n    j = np.arange(n).reshape(1, -1)\n    return 1.0 / (i + j + 1.0)\n\ndef make_spd_via_Q(n: int, alpha: float, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    Q = rng.uniform(-1.0, 1.0, size=(n, n))\n    A = Q.T @ Q + alpha * np.eye(n)\n    return A\n\ndef make_rank_def_spd(n: int, r: int, seed: int) - np.ndarray:\n    rng = np.random.default_rng(seed)\n    B = rng.uniform(-1.0, 1.0, size=(r, n))\n    A = B.T @ B\n    return A\n\ndef make_spd_with_eigs(n: int, seed: int) - np.ndarray:\n    # Create orthonormal U via QR of a Gaussian matrix\n    rng = np.random.default_rng(seed)\n    G = rng.normal(0.0, 1.0, size=(n, n))\n    Q, _ = np.linalg.qr(G)\n    # Prescribed eigenvalues: log10 spaced from -8 to 0\n    exponents = -8.0 + 8.0 * (np.arange(n) / (n - 1))\n    eigs = 10.0 ** exponents\n    A = Q @ np.diag(eigs) @ Q.T\n    return A\n\ndef amplification_factor(x_est: np.ndarray, x_true: np.ndarray, noise: np.ndarray) - float:\n    num = np.linalg.norm(x_est - x_true, ord=2)\n    den = np.linalg.norm(noise, ord=2)\n    # Avoid division by zero; if no noise, define amplification as +inf\n    if den == 0.0:\n        return float('inf')\n    return num / den\n\ndef safe_inv_solve(A: np.ndarray, b: np.ndarray):\n    # Returns solution via explicit inverse, or raises/returns inf if fails\n    try:\n        A_inv = np.linalg.inv(A)\n        return A_inv @ b\n    except Exception:\n        return None\n\ndef safe_direct_solve(A: np.ndarray, b: np.ndarray):\n    try:\n        return np.linalg.solve(A, b)\n    except Exception:\n        return None\n\ndef compute_case(case):\n    case_type = case['type']\n    if case_type == 'spd_q':\n        A = make_spd_via_Q(case['n'], case['alpha'], case['seed_A'])\n    elif case_type == 'hilbert':\n        A = hilbert(case['n'])\n    elif case_type == 'rank_def_spd':\n        A = make_rank_def_spd(case['n'], case['rank'], case['seed_A'])\n    elif case_type == 'spd_eigs':\n        A = make_spd_with_eigs(case['n'], case['seed_A'])\n    else:\n        raise ValueError(\"Unknown case type\")\n\n    n = case['n']\n    # Ground truth and noise\n    rng_x = np.random.default_rng(case['seed_x'])\n    x_true = rng_x.uniform(-1.0, 1.0, size=n)\n    b_clean = A @ x_true\n\n    rng_noise = np.random.default_rng(case['seed_noise'])\n    noise = rng_noise.normal(0.0, case['noise_std'], size=n)\n    b_noisy = b_clean + noise\n\n    # Condition numbers\n    try:\n        cond_A = np.linalg.cond(A)  # 2-norm condition number\n    except Exception:\n        cond_A = float('inf')\n\n    A_shift = A + case['lambda'] * np.eye(n)\n    try:\n        cond_A_shift = np.linalg.cond(A_shift)\n    except Exception:\n        cond_A_shift = float('inf')\n\n    # Solutions\n    x_inv_noisy = safe_inv_solve(A, b_noisy)\n    x_solve_noisy = safe_direct_solve(A, b_noisy)\n    x_shift_noisy = safe_direct_solve(A_shift, b_noisy)\n\n    # Clean RHS for algorithmic discrepancy\n    x_inv_clean = safe_inv_solve(A, b_clean)\n    x_solve_clean = safe_direct_solve(A, b_clean)\n\n    # Amplification factors\n    if x_inv_noisy is None:\n        g_inv = float('inf')\n    else:\n        g_inv = amplification_factor(x_inv_noisy, x_true, noise)\n\n    if x_solve_noisy is None:\n        g_solve = float('inf')\n    else:\n        g_solve = amplification_factor(x_solve_noisy, x_true, noise)\n\n    if x_shift_noisy is None:\n        g_shift = float('inf')\n    else:\n        g_shift = amplification_factor(x_shift_noisy, x_true, noise)\n\n    # Algorithmic discrepancy\n    if (x_inv_clean is None) or (x_solve_clean is None):\n        delta_inv = float('inf')\n    else:\n        denom = np.linalg.norm(x_solve_clean, ord=2)\n        if denom == 0.0:\n            delta_inv = float('inf')\n        else:\n            delta_inv = np.linalg.norm(x_inv_clean - x_solve_clean, ord=2) / denom\n\n    is_superior = g_shift  min(g_solve, g_inv)\n\n    return [g_inv, g_solve, g_shift, cond_A, cond_A_shift, delta_inv, is_superior]\n\ndef solve():\n    test_cases = [\n        {\n            'type': 'spd_q',\n            'n': 10,\n            'alpha': 1e-2,\n            'seed_A': 7,\n            'seed_x': 11,\n            'seed_noise': 13,\n            'noise_std': 1e-6,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'hilbert',\n            'n': 12,\n            'seed_A': None,  # Deterministic\n            'seed_x': 17,\n            'seed_noise': 19,\n            'noise_std': 1e-4,\n            'lambda': 1e-1,\n        },\n        {\n            'type': 'rank_def_spd',\n            'n': 10,\n            'rank': 8,\n            'seed_A': 23,\n            'seed_x': 29,\n            'seed_noise': 31,\n            'noise_std': 1e-6,\n            'lambda': 1.0,\n        },\n        {\n            'type': 'spd_eigs',\n            'n': 12,\n            'seed_A': 37,\n            'seed_x': 41,\n            'seed_noise': 43,\n            'noise_std': 1e-5,\n            'lambda': 1e-2,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(compute_case(case))\n\n    # Print single-line, bracket-enclosed, comma-separated list of per-case lists\n    # Ensure booleans and floats are printed via default str\n    def format_item(item):\n        if isinstance(item, list):\n            return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n        else:\n            return str(item)\n\n    print(\"[\" + \",\".join(format_item(r) for r in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3147728"}, {"introduction": "基于梯度的优化方法其收敛速度在很大程度上取决于损失函数的地形“形状”，该形状由曲率矩阵（Hessian 矩阵）描述。预处理技术旨在重塑此地形，使其更加均匀，从而加速收敛。本练习将让您探索不同的预处理器——从简单的单位矩阵（无预处理）到理想的逆 Hessian 矩阵——如何重塑优化问题并改善其条件数 [@problem_id:3147698]。", "problem": "给定一个源于深度学习中二次训练目标的对称正定（SPD）曲率矩阵族。考虑一个使用平方损失和岭回归正则化训练的线性模型。在某个参数向量处的曲率由海森矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 建模，该矩阵由数据矩阵 $X \\in \\mathbb{R}^{m \\times n}$ 和正则化参数 $\\lambda > 0$ 构建，公式为 $A = X^{\\top}X + \\lambda I$，其中 $I$ 是单位矩阵。预处理通过将梯度左乘一个矩阵 $M$ 来修改基于梯度的优化，这改变了算法所见的有效曲率。对于一个 SPD 预处理器 $M$，相关的变换后曲率为 $B = M^{1/2} A M^{1/2}$。\n\n您的任务是实现一个完整的、可运行的程序，该程序：\n- 使用 $A = X^{\\top}X + \\lambda I$ 为每个测试用例构建 $A$。\n- 比较三种预处理器：$M = I$（单位预处理器）、$M = D^{-1}$（逆对角预处理器，其中 $D = \\operatorname{diag}(A)$）和 $M = A^{-1}$（完全逆预处理器）。\n- 定义使用预处理后曲率 $B$ 的条件数 $\\kappa(B)$ 来量化优化行为的指标。具体来说，计算：\n  1. 每种预处理器的条件数 $\\kappa(B)$。\n  2. 对于曲率为 $B$ 的二次目标函数，固定步长梯度下降的理论最优线性收缩因子，表示为 $\\kappa(B)$ 的函数。\n  3. 当应用于曲率 $B$ 时，保证梯度下降稳定性的最大固定步长。\n  4. 使用 $M = D^{-1}$ 和 $M = A^{-1}$ 相对于 $M = I$ 时，条件数的改善因子。\n  5. 对于 $M = D^{-1}$ 和 $M = A^{-1}$ 中的每一个，一个布尔值，指示其条件数是否比 $M = I$ 的条件数严格减小。\n\n程序必须为以下每个确定性测试用例计算这些指标。在每个用例中，$X$ 是通过从标准正态分布中抽取条目并使用固定的随机种子来构建的，然后使用提供的向量 $s \\in \\mathbb{R}^{n}$ 通过列的逐元素相乘来缩放列：\n- 测试用例 1（一般良态场景）：\n  - $m = 64$，$n = 10$，$\\lambda = 10^{-2}$，随机种子 $1$，\n  - 列缩放因子 $s = [1,1,1,1,1,1,1,1,1,1]$。\n- 测试用例 2（通过正则化缓解的近奇异基线）：\n  - $m = 16$，$n = 10$，$\\lambda = 10^{-8}$，随机种子 $2$，\n  - 列缩放因子 $s = [1,1,1,1,1,10^{-6},10^{-6},10^{-6},1,1]$。\n- 测试用例 3（高度各向异性的特征）：\n  - $m = 128$，$n = 10$，$\\lambda = 10^{-1}$，随机种子 $3$，\n  - 列缩放因子 $s$ 在 10 列上构成从 $1$ 到 $1000$ 的等比数列，即 $s_j = 10^{\\frac{3(j-1)}{9}}$ 对于 $j = 1,\\dots,10$。\n\n科学和算法要求：\n- 使用 $A = X^{\\top}X + \\lambda I$ 作为曲率模型。将所有矩阵视为实对称矩阵，并确保所有计算都尊重 SPD 属性。\n- 对于每个预处理器 $M \\in \\{I, D^{-1}, A^{-1}\\}$，形成预处理后的曲率 $B = M^{1/2} A M^{1/2}$ 并计算其条件数 $\\kappa(B)$，即最大特征值与最小特征值之比。预处理器 $M = D^{-1}$ 使用 $D = \\operatorname{diag}(A)$ 并且 $D^{-1/2}$ 是逐元素定义的。\n- 以 $B$ 的谱量表示理论最优线性收缩因子和最大稳定步长，但除非从第一性原理推导，否则在实现中不要假设任何特定的快捷公式。\n- 所有指标对于 SPD 矩阵都必须是数值上良定义的。\n\n最终输出格式：\n- 对于每个测试用例，将指标按以下顺序汇总到一个列表中：\n  1. $M = I$ 时的 $\\kappa(B)$，\n  2. $M = D^{-1}$ 时的 $\\kappa(B)$，\n  3. $M = A^{-1}$ 时的 $\\kappa(B)$，\n  4. $M = I$ 时的最优线性收缩因子，\n  5. $M = D^{-1}$ 时的最优线性收缩因子，\n  6. $M = A^{-1}$ 时的最优线性收缩因子，\n  7. $M = I$ 时的最大稳定步长，\n  8. $M = D^{-1}$ 时的最大稳定步长，\n  9. $M = A^{-1}$ 时的最大稳定步长，\n  10. 改善因子 $\\kappa(B_{I})/\\kappa(B_{D^{-1}})$，\n  11. 改善因子 $\\kappa(B_{I})/\\kappa(B_{A^{-1}})$，\n  12. 指示 $\\kappa(B_{D^{-1}})  \\kappa(B_{I})$ 是否为真的布尔值，\n  13. 指示 $\\kappa(B_{A^{-1}})  \\kappa(B_{I})$ 是否为真的布尔值。\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为逗号分隔的列表的列表，不含空格，并用方括号括起来。例如，它看起来应该像：[[case1_metrics],[case2_metrics],[case3_metrics]]。\n\n不涉及物理单位或角度单位；除布尔值外，所有输出都是无量纲的实数。程序必须是自包含的，不需要任何输入，并使用指定的运行时环境。", "solution": "经评估，用户提供的问题是有效的。它在科学上植根于应用于机器学习的数值线性代数和优化理论领域。该问题是适定的，提供了所有必要的数据、定义和约束，以便为每个测试用例得出唯一的、确定性的解决方案。其语言客观且无歧义。\n\n问题的核心是分析预处理对与正则化线性回归问题相关的曲率矩阵的影响。曲率由对称正定（SPD）矩阵 $A$ 表示，它决定了优化景观的几何形状。基于梯度的优化方法在条件更好的景观上收敛更快。预处理旨在通过变换问题来改善这种条件状况。\n\n二次优化目标可以写成 $f(w) = \\frac{1}{2} w^{\\top}A w - b^{\\top}w + c$。梯度是 $\\nabla f(w) = Aw - b$。一个预处理的梯度下降步骤由 $w_{k+1} = w_k - \\alpha M \\nabla f(w_k)$ 给出，其中 $M$ 是预处理器。这等效于对一个变换后的问题应用标准梯度下降。如果 $M$ 是 SPD 矩阵，它有一个唯一的 SPD 平方根 $M^{1/2}$。更新可以看作是在一个变换后的坐标系 $\\hat{w} = M^{-1/2}w$ 中进行。在这些坐标中，目标函数成为 $\\hat{w}$ 的函数，其海森矩阵是 $B = M^{1/2} A M^{1/2}$。优化的收敛性质现在由这个新曲率矩阵 $B$ 的谱性质决定。\n\n条件数 $\\kappa(B) = \\frac{\\lambda_{\\max}(B)}{\\lambda_{\\min}(B)}$（其中 $\\lambda_{\\max}(B)$ 和 $\\lambda_{\\min}(B)$ 分别是 $B$ 的最大和最小特征值）是衡量优化难度的关键指标。$\\kappa(B) = 1$ 是理想情况，对应于一个完美的球形景观，而大的值表示一个病态的、拉长的景观，在这种情况下，简单的梯度下降表现不佳。\n\n对于海森矩阵为 $B$ 的二次目标，梯度下降更新 $w_{k+1} = w_k - \\alpha \\nabla f(w_k)$ 收敛，如果步长 $\\alpha$ 的选择使得迭代矩阵 $(I - \\alpha B)$ 的谱半径小于 1。这引出了以下关键指标：\n1.  **最大稳定步长**：为使迭代稳定，我们需要对所有特征值 $\\lambda_i(B)$ 满足 $|1 - \\alpha \\lambda_i(B)|  1$。由于 $B$ 是 SPD 矩阵，$\\lambda_i(B) > 0$。该条件简化为 $\\alpha \\lambda_{\\max}(B)  2$，从而得出最大稳定步长：\n    $$ \\alpha_{\\text{stable}} = \\frac{2}{\\lambda_{\\max}(B)} $$\n2.  **最优线性收缩因子**：收敛速率受限于收敛最慢的特征分量。最优步长 $\\alpha_{\\text{opt}} = \\frac{2}{\\lambda_{\\max}(B) + \\lambda_{\\min}(B)}$，最小化了所有模式下的最大收缩。这产生了由下式给出的最优线性收缩因子 $\\rho_{\\text{opt}}$：\n    $$ \\rho_{\\text{opt}} = \\frac{\\lambda_{\\max}(B) - \\lambda_{\\min}(B)}{\\lambda_{\\max}(B) + \\lambda_{\\min}(B)} = \\frac{\\kappa(B) - 1}{\\kappa(B) + 1} $$\n    一个更小的 $\\rho_{\\text{opt}}$ 表示更快的收敛。\n\n该问题要求评估三种特定的 SPD 预处理器：\n1.  **单位预处理器 ($M=I$)**：这对应于没有预处理。有效曲率就是 $B_I = I^{1/2} A I^{1/2} = A$。这提供了一个比较的基准。曲率矩阵是 $A = X^{\\top}X + \\lambda I$。由于 $X^{\\top}X$ 是半正定的且 $\\lambda > 0$，因此 $A$ 保证是对称正定矩阵。\n\n2.  **逆对角预处理器 ($M=D^{-1}$)**：这里，$D = \\operatorname{diag}(A)$ 是一个只包含 $A$ 对角线元素的矩阵。由于 $A$ 是 SPD 矩阵，其对角线元素 $A_{ii} = (X^{\\top}X)_{ii} + \\lambda = \\sum_{k=1}^{m} X_{ki}^2 + \\lambda$ 严格为正。因此，$D$ 及其逆 $D^{-1}$ 都是良定义的 SPD 矩阵。平方根 $M^{1/2} = (D^{-1})^{1/2} = D^{-1/2}$ 是一个对角矩阵，其对角线元素为 $1/\\sqrt{A_{ii}}$。变换后的曲率是 $B_{D^{-1}} = D^{-1/2} A D^{-1/2}$。这相当于雅可比（Jacobi）或对角预处理，它重新缩放问题，使得有效海森矩阵的对角线元素都等于 1。当特征（$X$ 的列）具有差异巨大的尺度时，这通常非常有效。\n\n3.  **完全逆预处理器 ($M=A^{-1}$)**：这是理论上理想的预处理器，对应于牛顿法。由于 $A$ 是 SPD 矩阵，其逆 $A^{-1}$ 存在并且也是 SPD 矩阵。$M=A^{-1}$ 的唯一 SPD 平方根是 $A^{-1/2}$。变换后的曲率为：\n    $$ B_{A^{-1}} = (A^{-1})^{1/2} A (A^{-1})^{1/2} = A^{-1/2} A A^{-1/2} $$\n    由于 $A = A^{1/2} A^{1/2}$，我们有：\n    $$ B_{A^{-1}} = A^{-1/2} (A^{1/2} A^{1/2}) A^{-1/2} = (A^{-1/2} A^{1/2}) (A^{1/2} A^{-1/2}) = I \\cdot I = I $$\n    有效曲率是单位矩阵 $I$。在这种情况下，所有特征值都是 1，所以 $\\lambda_{\\max}(B_{A^{-1}}) = \\lambda_{\\min}(B_{A^{-1}}) = 1$。这得出 $\\kappa(I)=1$，$\\rho_{\\text{opt}}=0$，以及 $\\alpha_{\\text{stable}}=2$。这代表了完美的条件状况，对于二次问题，优化可以在一步内收敛。在实践中，计算 $A^{-1}$ 通常与解决原始问题的代价一样高，但它是一个重要的理论基准。\n\n实现将首先为每个测试用例构建数据矩阵 $X$ 和曲率 $A$。然后，对于每个预处理器，将形成相应的矩阵 $B$，使用 `numpy.linalg.eigvalsh`（适用于对称矩阵）计算其特征值，并根据上述推导的公式计算所需指标。对于 $M=A^{-1}$ 的情况，其结果将被设置为它们的精确理论值。最后，所有指标将被汇总并格式化为所需的输出字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes preconditioning metrics for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"m\": 64, \"n\": 10, \"lambd\": 1e-2, \"seed\": 1,\n            \"scales\": np.ones(10)\n        },\n        {\n            \"m\": 16, \"n\": 10, \"lambd\": 1e-8, \"seed\": 2,\n            \"scales\": np.array([1,1,1,1,1,1e-6,1e-6,1e-6,1,1])\n        },\n        {\n            \"m\": 128, \"n\": 10, \"lambd\": 1e-1, \"seed\": 3,\n            # s_j = 10^(3(j-1)/9) for j=1..10. This is a logspace from 10^0 to 10^3.\n            \"scales\": np.logspace(0, 3, 10, base=10)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        m, n, lambd, seed, s = case[\"m\"], case[\"n\"], case[\"lambd\"], case[\"seed\"], case[\"scales\"]\n\n        # 1. Construct matrices\n        rng = np.random.default_rng(seed)\n        X_rand = rng.standard_normal((m, n))\n        X = X_rand * s  # Apply column scaling\n        A = X.T @ X + lambd * np.eye(n)\n\n        # Container for metrics for the current test case\n        case_metrics = []\n        \n        # --- Preconditioner M = I ---\n        B_I = A\n        eigvals_I = np.linalg.eigvalsh(B_I)\n        lmax_I, lmin_I = eigvals_I[-1], eigvals_I[0]\n        \n        kappa_I = lmax_I / lmin_I\n        rho_I = (kappa_I - 1) / (kappa_I + 1)\n        alpha_I = 2 / lmax_I\n\n        # --- Preconditioner M = D^-1 ---\n        diag_A = np.diag(A)\n        # D_inv_sqrt is a diagonal matrix with 1/sqrt(A_ii) on the diagonal\n        D_inv_sqrt = np.diag(1.0 / np.sqrt(diag_A))\n        B_D = D_inv_sqrt @ A @ D_inv_sqrt\n        \n        eigvals_D = np.linalg.eigvalsh(B_D)\n        lmax_D, lmin_D = eigvals_D[-1], eigvals_D[0]\n\n        kappa_D = lmax_D / lmin_D\n        rho_D = (kappa_D - 1) / (kappa_D + 1)\n        alpha_D = 2 / lmax_D\n\n        # --- Preconditioner M = A^-1 ---\n        # Analytically, B = I, so kappa=1, rho=0, alpha_stable=2.\n        kappa_A = 1.0\n        rho_A = 0.0\n        alpha_A = 2.0\n        \n        # --- Aggregate metrics in the specified order ---\n        # 1-3. Condition numbers\n        case_metrics.append(kappa_I)\n        case_metrics.append(kappa_D)\n        case_metrics.append(kappa_A)\n        \n        # 4-6. Optimal contraction factors\n        case_metrics.append(rho_I)\n        case_metrics.append(rho_D)\n        case_metrics.append(rho_A)\n\n        # 7-9. Largest stable step sizes\n        case_metrics.append(alpha_I)\n        case_metrics.append(alpha_D)\n        case_metrics.append(alpha_A)\n\n        # 10. Improvement factor kappa(B_I)/kappa(B_D^-1)\n        case_metrics.append(kappa_I / kappa_D)\n        \n        # 11. Improvement factor kappa(B_I)/kappa(B_A^-1)\n        case_metrics.append(kappa_I / kappa_A)\n        \n        # 12. Boolean: kappa(B_D^-1)  kappa(B_I)\n        case_metrics.append(kappa_D  kappa_I)\n        \n        # 13. Boolean: kappa(B_A^-1)  kappa(B_I)\n        case_metrics.append(kappa_A  kappa_I)\n\n        all_results.append(case_metrics)\n\n    # Format the final output string as a list of lists.\n    case_strings = []\n    for res_list in all_results:\n        # Convert all items to string, including booleans.\n        str_list = [str(item) for item in res_list]\n        case_strings.append(f\"[{','.join(str_list)}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3147698"}, {"introduction": "虽然对一般矩阵求逆的计算成本很高（复杂度为 $O(n^3)$），但深度学习中的许多矩阵具有可以利用的特殊结构以提高效率。本实践侧重于一种重要的“低秩加单位矩阵”结构，$A = I + UV^\\top$，这种结构出现在诸如注意力机制等场景中。您将基于 Sherman-Morrison-Woodbury 公式实现一种高效的求逆算法，并分析其相比于直接方法所具有的显著计算优势 [@problem_id:3147731]。", "problem": "给定形式为 $A = I + U V^\\top$ 的方阵，其中 $I$ 是大小为 $n \\times n$ 的单位矩阵，而 $U, V \\in \\mathbb{R}^{n \\times k}$ 定义了一个低秩更新。这种低秩加单位矩阵的结构在深度学习的计算线性代数中，当近似某些核化或预处理变换时会出现，并且当该结构在注意力机制中是块局部或头局部时尤其重要。您的任务是实现并评估一个算法，该算法通过从 $A_0 = I$ 开始，使用重复的秩-1更新来求 $A$ 的逆矩阵，并逐个地并入每个列向量对 $(u_i, v_i)$，其中 $u_i$ 和 $v_i$ 分别是 $U$ 和 $V$ 的第 $i$ 列。您不得预先假设任何公式；相反，您必须从矩阵逆的定义出发，从第一性原理推导出所需的更新规则，并加以实现。\n\n您可以使用的基本知识包括：矩阵逆和单位矩阵的定义与性质、外积和矩阵向量乘法的代数、范数以及标准的分块矩阵操作规则。特别是，您可以依赖以下事实：如果 $A$ 可逆，则 $A A^{-1} = I$ 且 $A^{-1} A = I$，并且当逆矩阵存在时，它是唯一的。\n\n构造矩阵的说明：\n- 对于每个测试，通过从标准正态分布中抽取元素来构造 $U$，然后按因子 $\\gamma / \\sqrt{n}$ 进行缩放，其中每个测试用例会提供 $\\gamma$。为保证更新是良态且非奇异的，请设置 $V = U$，这样对于任何实数 $\\gamma$，$A = I + U U^\\top$ 都是对称正定的。\n- 这些矩阵是纯数学的，不涉及任何物理单位。\n\n算法任务：\n1. 从 $A_0 = I$ 及其逆 $A_0^{-1} = I$ 开始，顺序地并入每个秩-1项 $u_i v_i^\\top$ 以获得 $A_i = A_{i-1} + u_i v_i^\\top$，并仅使用矩阵向量乘积、向量外积、标量运算和矩阵加法来更新一个显式的逆矩阵 $A_i^{-1}$。从逆的定义出发，推导出您需要的更新规则。\n2. 使用基于鲁棒数值线性代数算法的直接方法计算参考逆矩阵 $A_{\\mathrm{ref}}^{-1}$。\n3. 对于每个测试用例，计算您的迭代法逆矩阵与参考逆矩阵之间的相对弗罗贝尼乌斯误差：\n   $$ \\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}. $$\n4. 在以下标准化模型下，根据标量乘加运算的数量分析计算成本：\n   - 假设通过使用三角分解求解具有 $n$ 个右侧项的方程组 $A X = I$ 来直接计算 $A^{-1}$ 的成本为\n     $$ C_{\\text{direct}}(n) = \\frac{5}{3} \\, n^3. $$\n   - 假设您的迭代逆矩阵更新中每次秩-1更新（从 $A_{i-1}^{-1}$ 到 $A_i^{-1}$）的成本为\n     $$ C_{\\text{rank-1}}(n) = 4 n^2, $$\n     计入两个矩阵向量乘积、一个外积和一个矩阵更新的主导阶成本，并忽略低阶项。对于 $k$ 次更新，总成本为\n     $$ C_{\\text{iter}}(n,k) = 4 k n^2. $$\n   - 报告成本比率\n     $$ \\rho = \\frac{C_{\\text{iter}}(n,k)}{C_{\\text{direct}}(n)}. $$\n5. 您的程序必须实现以上内容，并精确地按照所述使用 $V = U$。\n\n测试套件：\n- 测试 1 (一般小型): $n = 8$, $k = 3$, $\\gamma = 0.2$, 随机种子 $= 0$。\n- 测试 2 (中等大小): $n = 64$, $k = 4$, $\\gamma = 0.05$, 随机种子 $= 1$。\n- 测试 3 (单位矩阵边缘情况): $n = 5$, $k = 0$, $\\gamma = 0.0$, 随机种子 $= 2$。\n- 测试 4 (更大且更具挑战性): $n = 50$, $k = 10$, $\\gamma = 0.5$, 随机种子 $= 3$。\n\n要求的最终输出：\n- 对于每个测试用例，按给定顺序输出两个值：首先是相对弗罗贝尼乌斯误差 $\\varepsilon$（一个浮点数），然后是成本比率 $\\rho$（一个浮点数）。\n- 将所有结果聚合到一个单行字符串中，格式化为Python风格的列表，使用逗号分隔值且不含空格，例如 $[\\varepsilon_1,\\rho_1,\\varepsilon_2,\\rho_2,\\dots]$。\n\n您的程序必须以此格式生成恰好一行作为其唯一输出。不涉及物理单位或角度，因此不需要特殊的单位处理。", "solution": "对形如 $A = I + U V^\\top$ 的矩阵求逆是数值线性代数中的一个经典问题。Sherman-Morrison-Woodbury 公式为该逆矩阵提供了一个直接的表达式。然而，当前任务要求我们从第一性原理出发，为一系列秩-1修正推导更新规则，我们现在就来进行这项工作。\n\n设 $A_{i-1} \\in \\mathbb{R}^{n \\times n}$ 是一个可逆矩阵，其逆矩阵 $A_{i-1}^{-1}$ 已知。我们希望找到矩阵 $A_i$ 的逆，该矩阵由一次秩-1更新形成：\n$$ A_i = A_{i-1} + u_i v_i^\\top $$\n其中 $u_i, v_i \\in \\mathbb{R}^n$ 是列向量。我们将所求的逆矩阵记为 $A_i^{-1}$。\n\n根据矩阵逆的定义，我们必须有 $A_i A_i^{-1} = I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。代入 $A_i$ 的表达式：\n$$ (A_{i-1} + u_i v_i^\\top) A_i^{-1} = I $$\n假设 $A_{i-1}$ 可逆，我们可以在等式左侧乘以 $A_{i-1}^{-1}$：\n$$ A_{i-1}^{-1} (A_{i-1} + u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} I $$\n$$ (I + A_{i-1}^{-1} u_i v_i^\\top) A_i^{-1} = A_{i-1}^{-1} $$\n现在，我们可以通过对括号中的项求逆来分离出 $A_i^{-1}$：\n$$ A_i^{-1} = (I + A_{i-1}^{-1} u_i v_i^\\top)^{-1} A_{i-1}^{-1} $$\n问题现在简化为求矩阵 $M = I + w v_i^\\top$ 的逆，其中我们定义了列向量 $w = A_{i-1}^{-1} u_i$。\n\n我们假设 $M$ 的逆具有相似的结构，即 $M^{-1} = I + \\alpha w v_i^\\top$，其中标量 $\\alpha$ 待定。为了求出 $\\alpha$，我们施加条件 $M M^{-1} = I$：\n$$ (I + w v_i^\\top)(I + \\alpha w v_i^\\top) = I $$\n使用矩阵乘法的分配律展开左侧：\n$$ I(I + \\alpha w v_i^\\top) + w v_i^\\top(I + \\alpha w v_i^\\top) = I $$\n$$ I + \\alpha w v_i^\\top + w v_i^\\top + w v_i^\\top (\\alpha w v_i^\\top) = I $$\n项 $v_i^\\top w$ 是一个标量，它是 $v_i$ 和 $w$ 的内积。我们可以重新组合各项：\n$$ I + (\\alpha w v_i^\\top + w v_i^\\top) + \\alpha (w v_i^\\top w v_i^\\top) = I $$\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha w (v_i^\\top w) v_i^\\top = I $$\n由于 $v_i^\\top w$ 是一个标量，我们可以交换它的位置：\n$$ I + (\\alpha+1) w v_i^\\top + \\alpha (v_i^\\top w) w v_i^\\top = I $$\n提出外积 $w v_i^\\top$：\n$$ I + [(\\alpha+1) + \\alpha(v_i^\\top w)] w v_i^\\top = I $$\n为使此方程对任意非零向量 $w$ 和 $v_i$ 均成立，项 $w v_i^\\top$ 的标量系数必须为零：\n$$ (\\alpha+1) + \\alpha(v_i^\\top w) = 0 $$\n$$ \\alpha(1 + v_i^\\top w) = -1 $$\n解出 $\\alpha$，我们得到：\n$$ \\alpha = -\\frac{1}{1 + v_i^\\top w} $$\n该表达式在分母 $1 + v_i^\\top w \\neq 0$ 的条件下有效。如果此条件成立，逆矩阵为：\n$$ (I + w v_i^\\top)^{-1} = I - \\frac{w v_i^\\top}{1 + v_i^\\top w} $$\n将此结果代回到我们关于 $A_i^{-1}$ 的表达式中：\n$$ A_i^{-1} = \\left(I - \\frac{w v_i^\\top}{1 + v_i^\\top w}\\right) A_{i-1}^{-1} $$\n将 $w = A_{i-1}^{-1} u_i$ 代回：\n$$ A_i^{-1} = \\left(I - \\frac{(A_{i-1}^{-1} u_i) v_i^\\top}{1 + v_i^\\top (A_{i-1}^{-1} u_i)}\\right) A_{i-1}^{-1} $$\n最后，将右侧的 $A_{i-1}^{-1}$ 项分配进去：\n$$ A_i^{-1} = A_{i-1}^{-1} - \\frac{(A_{i-1}^{-1} u_i) (v_i^\\top A_{i-1}^{-1})}{1 + v_i^\\top A_{i-1}^{-1} u_i} $$\n这就是 Sherman-Morrison 公式，它为逆矩阵提供了顺序更新规则。\n\n对于这个具体问题，给定 $V = U$，这意味着对所有 $i = 1, \\dots, k$ 都有 $v_i = u_i$。所构造的矩阵为 $A_0 = I$ 以及当 $i > 0$ 时 $A_i = A_{i-1} + u_i u_i^\\top$。由于 $A_0=I$ 是对称正定 (SPD) 的，并且每次更新 $u_i u_i^\\top$ 都是对称半正定的，因此所有矩阵 $A_i$ 都是对称正定的。因此，它们的逆 $A_i^{-1}$ 也是对称正定的。这确保了对于任何非零向量 $u_i$，二次型 $u_i^\\top A_{i-1}^{-1} u_i > 0$。因此，分母 $1 + u_i^\\top A_{i-1}^{-1} u_i$ 总是严格大于 $1$，从而避免了任何除以零的情况。\n\n因此，算法实现将按以下步骤进行：\n1. 初始化逆矩阵为 $A_0^{-1} = I$。\n2. 对于每个步骤 $i = 1, \\dots, k$，使用推导出的公式（其中 $v_i=u_i$）将当前逆矩阵 $A_{i-1}^{-1}$ 更新为 $A_i^{-1}$。这就构成了迭代法求得的逆矩阵 $A_{\\mathrm{iter}}^{-1}$。\n3. 直接构造最终矩阵 $A = I + U U^\\top$，并使用一个标准的数值库函数计算其逆矩阵 $A_{\\mathrm{ref}}^{-1}$，该函数通常依赖于像LU分解这样的鲁棒分解方法。\n4. 计算相对弗罗贝尼乌斯误差 $\\varepsilon = \\frac{\\lVert A_{\\mathrm{iter}}^{-1} - A_{\\mathrm{ref}}^{-1} \\rVert_F}{\\lVert A_{\\mathrm{ref}}^{-1} \\rVert_F}$。\n5. 使用提供的成本模型计算成本比率 $\\rho = C_{\\text{iter}}(n,k) / C_{\\text{direct}}(n)$：\n   $C_{\\text{iter}}(n,k) = 4 k n^2$\n   $C_{\\text{direct}}(n) = \\frac{5}{3} n^3$\n   该比率可简化为 $\\rho = \\frac{4 k n^2}{(5/3) n^3} = \\frac{12k}{5n}$。这个比率表明，当更新次数 $k$相对于矩阵维度 $n$ 较小时，迭代方法更高效，特别是在 $k  \\frac{5}{12}n$ 的情况下。\n\n实现将针对所提供的每个测试用例遵循这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of iteratively inverting a matrix of the form A = I + UU^T\n    and compares it against a direct inversion method.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, gamma, random_seed)\n        (8, 3, 0.2, 0),\n        (64, 4, 0.05, 1),\n        (5, 0, 0.0, 2),\n        (50, 10, 0.5, 3),\n    ]\n\n    results = []\n    for n, k, gamma, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # Construct matrix U. Handle the k=0 edge case.\n        if k > 0:\n            scale_factor = gamma / np.sqrt(n)\n            U = np.random.randn(n, k) * scale_factor\n        else:\n            # An n x 0 matrix\n            U = np.zeros((n, 0))\n        \n        # As per instructions, V = U\n        V = U\n\n        # 1. Iterative inverse calculation using the derived rank-1 update rule.\n        # Start with the inverse of A_0 = I, which is I.\n        A_inv_iter = np.eye(n)\n        \n        # Sequentially apply k rank-1 updates.\n        for i in range(k):\n            u_i = U[:, i:i+1] # Get i-th column as a (n, 1) vector\n            v_i = V[:, i:i+1] # v_i is u_i\n            \n            # Sherman-Morrison formula:\n            # A_inv_new = A_inv - (A_inv @ u @ v.T @ A_inv) / (1 + v.T @ A_inv @ u)\n            \n            # Pre-calculate terms for clarity and efficiency\n            w = A_inv_iter @ u_i # This is the vector (A_{i-1}^{-1} u_i)\n            \n            # Denominator: 1 + v_i^T * (A_{i-1}^{-1} u_i)\n            denominator = 1.0 + (v_i.T @ w).item()\n            \n            # Since A_inv_iter is symmetric and v_i=u_i, the numerator term\n            # (A_inv @ u) @ (v.T @ A_inv) simplifies to w @ w.T\n            numerator_outer_product = w @ w.T\n            \n            # Update the inverse\n            A_inv_iter -= numerator_outer_product / denominator\n\n        # 2. Compute the reference inverse using a direct method.\n        # First, construct the full matrix A = I + U U^T\n        A = np.eye(n) + U @ U.T\n        \n        # Use a robust direct solver\n        A_inv_ref = np.linalg.inv(A)\n\n        # 3. Compute the relative Frobenius error.\n        norm_ref = np.linalg.norm(A_inv_ref, 'fro')\n        \n        if norm_ref == 0:\n            # This is unlikely for an invertible matrix, but for completeness:\n            # if reference is zero matrix, error is 0 if iterative is also zero, else infinity\n            epsilon = 0.0 if np.linalg.norm(A_inv_iter, 'fro') == 0.0 else np.inf\n        else:\n            norm_diff = np.linalg.norm(A_inv_iter - A_inv_ref, 'fro')\n            epsilon = norm_diff / norm_ref\n\n        # 4. Compute the computational cost ratio.\n        c_direct = (5.0 / 3.0) * (n**3)\n        c_iter = 4.0 * k * (n**2)\n        \n        # The ratio rho simplifies to 12*k / (5*n)\n        if c_direct == 0:\n            # Handle n=0 case to avoid division by zero\n            rho = np.inf if c_iter > 0 else 0.0\n        else:\n            rho = c_iter / c_direct\n            \n        results.extend([epsilon, rho])\n\n    # Final print statement in the exact required format.\n    # The format is a string representing a list, with no spaces.\n    formatted_results = [f\"{val:.12g}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3147731"}]}