{"hands_on_practices": [{"introduction": "香农熵为我们提供了一种衡量数据源不确定性或信息量的基本方法。这个练习将这一概念应用于深度学习中一种常见的数据类型：图像。通过比较灰度像素（由单个随机变量 $X$ 建模）和彩色像素（由三个独立的随机变量 $(X_R, X_G, X_B)$ 建模）的熵，我们可以定量地理解为什么彩色图像本质上更复杂。更重要的是，这个练习将熵与一个实际的机器学习挑战——“维度灾难”——联系起来，通过一个与字母表大小 $|\\mathcal{A}|$ 相关的样本复杂度公式，揭示了为什么更高维的数据需要更多的样本才能进行可靠的估计 [@problem_id:3174049]。", "problem": "给定两类用于深度学习图像分类的数据集：一类是具有单个通道的灰度图像，另一类是具有三个通道（红、绿、蓝）的彩色图像。每个像素被建模为一个在有限字母表上取值的离散随机变量。在灰度图像的情况下，一个像素由单个离散随机变量 $X$ 建模，其概率质量函数分布在 $m$ 个强度区间上。在彩色图像的情况下，一个像素由一个离散随机变量的三元组 $(X_{R}, X_{G}, X_{B})$ 建模，每个通道对应一个变量。在整个问题中，假设通道在像素级别上是统计独立的。\n\n从基本定义出发，推理并实现灰度图像和彩色图像在像素上的不确定性有何不同，以及这种差异如何影响可靠估计像素分布（以一种对训练深度分类器有用的方式）所需的样本数量。具体来说：\n\n- 使用离散随机变量的香农熵的标准定义（以比特为单位，即使用以2为底的对数）作为每个像素不确定性的度量。对于彩色数据集，仅使用熵的独立性属性，从各通道的熵中获得每个像素的联合熵。\n- 对于样本复杂度，使用一个来自经验分布估计的成熟结论：为确保一个多项式随机变量的经验分布的期望 $\\ell_{1}$ 误差最多为容差 $\\varepsilon$，所需独立样本数 $n$ 必须满足形式为 $n \\geq c \\cdot \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}}$ 的不等式，其中 $|\\mathcal{A}|$ 是字母表大小， $c$ 是一个不依赖于 $|\\mathcal{A}|$ 或 $\\varepsilon$ 的常数。为解决此问题，取 $c = 1$ 以获得一个保守的标度律，并将其视为深度分类器将像素级分布估计到所需容差所需的标记样本数量的下界近似。您的程序应通过向上取整到最近的整数来计算 $n$。\n\n您的任务是编写一个完整的程序，该程序：\n1.  计算灰度数据集和彩色数据集（假设通道间独立）的每像素熵（以比特为单位）。\n2.  使用边界 $n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil$ 计算灰度数据集和彩色数据集的样本复杂度代理值 $n$。\n\n所有数学量必须精确处理，并遵循以下约定：\n- 对于熵，使用以2为底的对数，单位为比特。\n- 使用约定 $0 \\log 0 = 0$。\n- 对于灰度字母表大小，使用 $m = | \\text{bins for } X |$。\n- 对于彩色字母表大小，使用 $m_{\\text{color}} = | \\text{bins for } X_{R} | \\cdot | \\text{bins for } X_{G} | \\cdot | \\text{bins for } X_{B} |$。\n\n实现以下参数值的测试套件。每个测试用例都指定了灰度图像和每个彩色通道的概率质量函数，以及一个容差 $\\varepsilon$：\n- 测试用例1（正常路径，均匀分布）：灰度 $p = [0.25, 0.25, 0.25, 0.25]$，彩色通道与灰度相同，$\\varepsilon = 0.05$。\n- 测试用例2（边界情况，退化灰度）：灰度 $p = [1.0, 0.0, 0.0, 0.0]$，彩色 $p_{R} = p_{G} = p_{B} = [0.97, 0.01, 0.01, 0.01]$，$\\varepsilon = 0.05$。\n- 测试用例3（异构通道）：灰度 $p = [0.10, 0.20, 0.30, 0.40]$，彩色 $p_{R} = [0.70, 0.10, 0.10, 0.10]$，$p_{G} = [0.50, 0.20, 0.20, 0.10]$，$p_{B} = [0.25, 0.25, 0.25, 0.25]$，$\\varepsilon = 0.02$。\n- 测试用例4（小容差，相同分布）：灰度 $p = [0.40, 0.40, 0.10, 0.10]$，彩色通道与灰度相同，$\\varepsilon = 0.01$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是一个列表 $[H_{\\text{gray}}, H_{\\text{color}}, n_{\\text{gray}}, n_{\\text{color}}]$，其中 $H_{\\text{gray}}$ 和 $H_{\\text{color}}$ 是浮点数（单位为比特），$n_{\\text{gray}}$ 和 $n_{\\text{color}}$ 是整数。例如：$[[h_{1}, h_{1}^{c}, n_{1}, n_{1}^{c}], [h_{2}, h_{2}^{c}, n_{2}, n_{2}^{c}], \\dots]$。不应打印单位，但熵必须以比特为单位计算，$n$ 是一个无量纲的样本计数。", "solution": "该问题是有效的，因为它在科学上基于信息论和统计学，问题陈述清晰，提供了所有必要信息，且其表述是客观的。我们将逐步进行求解。\n\n该问题要求我们从信息论和统计估计的角度比较灰度图像和彩色图像数据的复杂性。我们将像素值建模为离散随机变量，使用香农熵作为不确定性的度量，并使用一个派生公式来计算估计底层概率分布所需的样本复杂度。\n\n### 1. 每像素不确定性：香农熵\n\n对于具有字母表 $\\mathcal{X}$ 和概率质量函数（PMF）$p(x) = P(X=x)$ 的离散随机变量 $X$，其不确定性或“信息内容”的基本度量是香农熵 $H(X)$。当使用以2为底的对数时，熵的单位是比特。定义如下：\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} p(x) \\log_2 p(x) $$\n按照惯例，我们定义 $0 \\log_2 0 = 0$ 来处理概率为零的结果。\n\n#### 灰度图像熵 ($H_{\\text{gray}}$)\n一个灰度像素由一个在 $m$ 个强度区间的字母表上的单个离散随机变量 $X$ 建模。给定一个PMF $p_{\\text{gray}} = [p_1, p_2, \\ldots, p_m]$，每像素的熵直接根据定义计算：\n$$ H_{\\text{gray}} = H(X) = - \\sum_{i=1}^{m} p_i \\log_2 p_i $$\n\n#### 彩色图像熵 ($H_{\\text{color}}$)\n一个彩色像素被建模为一个由三个随机变量组成的向量 $(X_R, X_G, X_B)$，每个颜色通道一个。问题陈述了一个关键假设：各通道是统计独立的。对于独立的随机变量，联合熵就是它们各自熵的和：\n$$ H(X_R, X_G, X_B) = H(X_R) + H(X_G) + H(X_B) $$\n因此，彩色图像的每像素熵 $H_{\\text{color}}$ 由每个通道的PMF（$p_R$, $p_G$, $p_B$）的熵之和给出：\n$$ H_{\\text{color}} = \\left( - \\sum_{i} p_{R,i} \\log_2 p_{R,i} \\right) + \\left( - \\sum_{j} p_{G,j} \\log_2 p_{G,j} \\right) + \\left( - \\sum_{k} p_{B,k} \\log_2 p_{B,k} \\right) $$\n\n### 2. 分布估计的样本复杂度\n\n问题提供了一个代理指标，用于衡量可靠估计多项式随机变量PMF所需的样本数量 $n$。这个量被称为样本复杂度，由以下公式给出：\n$$ n = \\left\\lceil \\frac{|\\mathcal{A}| - 1}{\\varepsilon^{2}} \\right\\rceil $$\n其中 $|\\mathcal{A}|$ 是随机变量的字母表大小（可能结果的数量），$\\varepsilon$ 是期望 $\\ell_{1}$ 误差所需的目标容差，$\\lceil \\cdot \\rceil$ 是向上取整函数（ceil function），它将数值向上舍入到最近的整数。\n\n这个公式揭示了所需样本数量与字母表大小 $|\\mathcal{A}|$ 呈线性增长关系。\n\n#### 灰度图像的样本复杂度 ($n_{\\text{gray}}$)\n对于灰度像素，随机变量是 $X$，其字母表大小 $|\\mathcal{A}_{\\text{gray}}|$ 是强度区间的数量，即 $m$。\n$$ n_{\\text{gray}} = \\left\\lceil \\frac{m - 1}{\\varepsilon^{2}} \\right\\rceil $$\n\n#### 彩色图像的样本复杂度 ($n_{\\text{color}}$)\n对于彩色像素，随机变量是向量 $(X_R, X_G, X_B)$。我们正在估计这个向量的联合分布。联合分布的字母表由所有可能的通道值三元组构成。如果各通道分别有 $m_R$、$m_G$ 和 $m_B$ 个区间，那么联合字母表的大小是这些单个大小的乘积：\n$$ |\\mathcal{A}_{\\text{color}}| = m_R \\cdot m_G \\cdot m_B $$\n样本复杂度则为：\n$$ n_{\\text{color}} = \\left\\lceil \\frac{(m_R \\cdot m_G \\cdot m_B) - 1}{\\varepsilon^{2}} \\right\\rceil $$\n对于高维数据，字母表大小的这种乘法增长是“维度灾难”的一种体现。它表明，在相同的容差 $\\varepsilon$ 下，估计彩色像素的联合分布比估计单个灰度通道需要大得多的样本数量。对于独立变量而言，熵是可加的，其增长速度远慢于样本复杂度，后者依赖于乘法增长的字母表大小。这一区别是本分析的核心要点。\n\n### 每个测试用例的计算摘要\n1.  **计算 $H_{\\text{gray}}$**：将熵公式应用于灰度PMF。\n2.  **计算 $H_{\\text{color}}$**：为每个颜色通道PMF计算熵，并将结果相加。\n3.  **计算 $n_{\\text{gray}}$**：从灰度PMF的长度确定字母表大小 $m$，并应用样本复杂度公式。\n4.  **计算 $n_{\\text{color}}$**：从各通道PMF的长度确定字母表大小 $m_R, m_G, m_B$，计算联合字母表大小 $m_R \\cdot m_G \\cdot m_B$，并应用样本复杂度公式。\n\n这些步骤将为提供的每个测试用例系统地实现。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes per-pixel entropy and sample complexity for grayscale and color image models\n    across a suite of test cases.\n    \"\"\"\n\n    # Test cases: (p_gray, p_red, p_green, p_blue, epsilon)\n    # The structure allows for cases where channel PMFs are distinct or identical to grayscale.\n    p1 = [0.25, 0.25, 0.25, 0.25]\n    p2_gray = [1.0, 0.0, 0.0, 0.0]\n    p2_color_channel = [0.97, 0.01, 0.01, 0.01]\n    p3_gray = [0.10, 0.20, 0.30, 0.40]\n    p3_r = [0.70, 0.10, 0.10, 0.10]\n    p3_g = [0.50, 0.20, 0.20, 0.10]\n    p3_b = [0.25, 0.25, 0.25, 0.25]\n    p4 = [0.40, 0.40, 0.10, 0.10]\n    \n    test_cases = [\n        (p1, p1, p1, p1, 0.05),\n        (p2_gray, p2_color_channel, p2_color_channel, p2_color_channel, 0.05),\n        (p3_gray, p3_r, p3_g, p3_b, 0.02),\n        (p4, p4, p4, p4, 0.01),\n    ]\n\n    results = []\n\n    def calculate_entropy(pmf: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Shannon entropy for a given probability mass function.\n        Handles the 0*log(0) = 0 case.\n        \"\"\"\n        # Filter out zero probabilities to avoid log(0)\n        pmf_nz = pmf[pmf > 0]\n        if pmf_nz.size == 0:\n            return 0.0\n        return -np.sum(pmf_nz * np.log2(pmf_nz))\n\n    def calculate_sample_complexity(alphabet_size: int, epsilon: float) -> int:\n        \"\"\"\n        Calculates the sample complexity n based on the given formula.\n        \"\"\"\n        if alphabet_size == 1:\n            return 0\n        n = math.ceil((alphabet_size - 1) / (epsilon**2))\n        return int(n)\n\n    for case in test_cases:\n        p_gray_list, p_r_list, p_g_list, p_b_list, epsilon = case\n        \n        p_gray = np.array(p_gray_list, dtype=np.float64)\n        p_r = np.array(p_r_list, dtype=np.float64)\n        p_g = np.array(p_g_list, dtype=np.float64)\n        p_b = np.array(p_b_list, dtype=np.float64)\n\n        # 1. Grayscale Calculations\n        h_gray = calculate_entropy(p_gray)\n        m_gray = len(p_gray)\n        n_gray = calculate_sample_complexity(m_gray, epsilon)\n\n        # 2. Color Calculations\n        h_r = calculate_entropy(p_r)\n        h_g = calculate_entropy(p_g)\n        h_b = calculate_entropy(p_b)\n        h_color = h_r + h_g + h_b\n\n        m_r, m_g, m_b = len(p_r), len(p_g), len(p_b)\n        m_color = m_r * m_g * m_b\n        n_color = calculate_sample_complexity(m_color, epsilon)\n        \n        results.append([h_gray, h_color, n_gray, n_color])\n\n    # Format the final output string exactly as required, handling list-of-lists.\n    # The standard str() representation of a list includes spaces, which is fine here.\n    # The example f-string correctly joins string representations of the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3174049"}, {"introduction": "除了分析原始数据，熵还可以用来描述数据集层面的特性，例如类别平衡度。一个类别分布极不平衡的数据集（例如在某些物种稀有的生态调查中）其类别先验分布的熵 $H(Y)$ 会很低，这给训练出无偏见的模型带来了挑战。这个练习 [@problem_id:3174037] 探讨了一个假设情景，其中利用数据集的先验熵来调整模型的输出，具体来说，通过一个等于 $1/H(Y)$ 的缩放因子来调整 logits。这个实践旨在研究这种调整对少数类别的识别性能和模型校准度（以期望校准误差 ECE 来衡量）的影响，突出了熵作为诊断和潜在缓解数据集问题的工具所扮演的角色。", "problem": "给定一个受生态物种鉴定启发的分类场景。一个固定的物종类别集合由随机变量 $Y$ 表示，其取值范围为 $\\{0,1,\\dots,K-1\\}$。对于每个测试用例，您将获得每个类别的样本数量、每个样本的模型 logit 矩阵以及相应的真实标签。您的任务是计算类别先验的香农熵 $H(Y)$，并检验通过逆熵对 logit 进行重加权是否能改善少数类别的识别并校准模型的预测概率。\n\n从以下基本原理开始：\n- 定义类别 $Y$ 上离散分布的概率公理，其中类别先验概率 $p_k$ 满足 $p_k \\ge 0$ 且 $\\sum_{k=0}^{K-1} p_k = 1$。\n- 结果 $y$ 的自信息的运算定义为其先验似然的倒数的对数，而香农熵 $H(Y)$ 是先验分布下类别标签的期望自信息。\n- 通过归一化指数函数从 logit 到概率的标准映射，这为每个样本生成一个有效的分类分布。\n\n您必须实现以下计算，使用自然对数，以便所有信息量都以奈特 (nats) 为单位：\n1. 根据提供的计数 $\\mathbf{c} = (c_0,\\dots,c_{K-1})$，通过将非负计数归一化使其总和为一来计算类别先验概率 $\\mathbf{p} = (p_0,\\dots,p_{K-1})$。然后计算先验分布的香农熵 $H(Y)$，单位为奈特。在数值上处理计数为零的类别时，应与熵的极限行为保持一致。\n2. 使用为每个样本提供的 logit，通过归一化指数函数将 logit 转换为预测的类别概率。将每个样本的预测类别定义为最大概率的索引。计算两个指标：\n   - 少数类别召回率 (Minority recall)：将少数类别集合定义为 $\\{k : p_k < 1/K\\}$。计算每个少数类别 $k$ 的召回率，即真实标签为 $k$ 的样本被正确预测为类别 $k$ 的比例，并报告少数类别上的宏平均值。如果少数类别集合为空，则将少数类别召回率定义为所有类别上的宏平均召回率。\n   - 期望校准误差 (Expected Calibration Error, ECE)：将区间 $[0,1]$ 划分为 $B=10$ 个等宽的区间。对于每个区间，计算其预测的 top-1 置信度落入该区间的样本比例、它们的经验准确率以及它们的平均置信度。ECE 是各区间的样本比例乘以准确率与平均置信度之差的绝对值的总和。\n3. 执行“逆熵重加权”，即在将 logit 转换为概率之前，将每个样本的 logit 乘以标量 $s = 1/H(Y)$。在此缩放后重新计算少数类别召回率和 ECE。确定少数类别召回率是否有所改善，以及 ECE 是否有所改善（严格减小），这些都相对于未缩放的基线而言。\n\n测试套件：\n对于每个测试用例，$K=3$。您将获得类别计数 $\\mathbf{c}$、形状为 $(N,K)$ 的 logit 矩阵 $\\mathbf{L}$ 以及长度为 $N$ 的真实标签向量 $\\mathbf{y}$，其中 $N = \\sum_k c_k$。\n\n- 测试用例 1 (平衡先验):\n  - $\\mathbf{c} = [4,4,4]$。\n  - $\\mathbf{L}$ 的行（每行是一个长度为 3 的 logit 向量）：\n    $[3.0,1.0,0.0]$, $[2.5,2.0,1.0]$, $[1.0,1.2,0.8]$, $[2.2,-0.5,0.1]$,\n    $[0.5,2.8,1.0]$, $[1.0,2.2,2.0]$, $[0.2,1.5,1.6]$, $[0.1,2.5,-0.3]$,\n    $[0.0,0.5,2.5]$, $[1.0,0.9,1.2]$, $[-0.2,1.3,1.1]$, $[0.3,-0.4,2.0]$。\n  - $\\mathbf{y}$ （$\\mathbf{L}$ 每行对应的真实标签）：$[0,0,0,0,1,1,1,1,2,2,2,2]$。\n\n- 测试用例 2 (中度不平衡先验):\n  - $\\mathbf{c} = [8,3,1]$。\n  - $\\mathbf{L}$ 的行：\n    $[3.4,0.1,-0.2]$, $[2.8,0.5,0.0]$, $[3.1,0.3,0.2]$, $[2.0,1.1,0.9]$,\n    $[1.8,1.5,1.2]$, $[2.5,1.6,1.6]$, $[1.7,1.9,1.6]$, $[2.6,0.8,0.7]$,\n    $[1.5,2.7,1.0]$, $[2.2,1.8,0.9]$, $[2.0,2.1,1.9]$, $[2.4,1.3,1.6]$。\n  - $\\mathbf{y}$：$[0,0,0,0,0,0,0,0,1,1,1,2]$。\n\n- 测试用例 3 (高度不平衡先验):\n  - $\\mathbf{c} = [10,1,1]$。\n  - $\\mathbf{L}$ 的行：\n    $[3.5,0.2,0.1]$, $[3.0,0.5,0.4]$, $[2.9,0.1,-0.2]$, $[2.3,0.7,0.6]$,\n    $[2.6,1.0,0.9]$, $[3.2,0.8,0.7]$, $[2.8,0.4,0.3]$, $[3.1,1.2,1.0]$,\n    $[2.7,1.4,1.3]$, $[2.4,1.6,1.5]$, $[2.5,2.2,2.1]$, $[2.5,2.1,2.2]$。\n  - $\\mathbf{y}$：$[0,0,0,0,0,0,0,0,0,0,1,2]$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含三个测试用例的结果，格式为一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是一个列表，顺序如下：\n$[H(Y), \\text{minority\\_recall\\_before}, \\text{minority\\_recall\\_after}, \\text{ECE\\_before}, \\text{ECE\\_after}, \\text{improved\\_recall}, \\text{improved\\_ECE}]$，\n其中前五个条目是浮点数（$H(Y)$ 的单位是奈特；使用自然对数），最后两个条目是布尔值。例如，整体输出应如下所示：\n$[[h_1,r^{\\text{min}}_{1,\\text{before}},r^{\\text{min}}_{1,\\text{after}},e_{1,\\text{before}},e_{1,\\text{after}},b^{\\text{recall}}_1,b^{\\text{ECE}}_1],[\\dots],[\\dots]]$。", "solution": "该问题要求通过计算香农熵、少数类别召回率和期望校准误差 (ECE) 来分析分类模型的输出，然后在提出的 logit 缩放程序之后重新评估这些指标。解决方案是通过系统地应用概率论、信息论和模型评估的基本原理来构建的。\n\n首先，我们定义类别先验概率和相关的香农熵。给定来自总共 $N$ 个样本的 $K$ 个类别的一组类别计数 $\\mathbf{c} = (c_0, c_1, \\dots, c_{K-1})$，其中 $N = \\sum_{k=0}^{K-1} c_k$，每个类别 $k$ 的先验概率 $p_k$ 被估计为经验频率：\n$$p_k = \\frac{c_k}{N}$$\n这组概率 $\\mathbf{p} = (p_0, \\dots, p_{K-1})$ 构成一个离散概率分布。随机变量 $Y$（代表类别标签）的香农熵 $H(Y)$ 是该分布不确定性的度量。使用自然对数（单位为奈特），它被定义为期望自信息：\n$$H(Y) = -\\sum_{k=0}^{K-1} p_k \\ln(p_k)$$\n在此计算中，我们遵循极限 $\\lim_{x\\to 0^+} x \\ln(x) = 0$，因此计数为零的类别对熵没有贡献。\n\n接下来，我们形式化地描述从模型 logit 到预测的转换。对于给定的样本，模型生成一个 logit 向量 $\\mathbf{l} = (l_0, l_1, \\dots, l_{K-1})$。这些实值分数通过 softmax 函数转换为关于类别的有效概率分布：\n$$P(Y=k|\\mathbf{l}) = \\frac{\\exp(l_k)}{\\sum_{j=0}^{K-1} \\exp(l_j)}$$\n预测类别 $\\hat{y}$ 是对应于最大概率的类别，$\\hat{y} = \\arg\\max_k P(Y=k|\\mathbf{l})$，而此预测的置信度是这个最大概率，$\\text{conf} = \\max_k P(Y=k|\\mathbf{l})$。\n\n有了这些预测，我们就可以评估模型的性能和校准。\n1.  **少数类别召回率**：此指标评估模型正确识别代表性不足类别实例的能力。如果一个类别 $k$ 的先验概率 $p_k$ 小于均匀先验，即 $p_k < 1/K$，则该类别被定义为少数类别。特定类别 $k$ 的召回率 $\\text{Recall}_k$ 是类别 $k$ 的实际实例中被正确预测为类别 $k$ 的比例。整体的少数类别召回率是所有少数类别上 $\\text{Recall}_k$ 的宏平均值。如果少数类别集合为空（即数据集平衡），则该指标计算为所有类别上的宏平均召回率。\n\n2.  **期望校准误差 (ECE)**：该指标量化了模型的预测置信度在多大程度上反映了其真实准确率。置信度范围 $[0, 1]$ 被划分为 $B$ 个等宽的区间（这里，$B=10$）。对于每个区间 $m$，我们找到所有预测置信度落入该区间的样本。然后我们计算该区间内样本的平均准确率 $\\text{acc}(b_m)$ 和平均置信度 $\\text{conf}(b_m)$。ECE 是这些数量之间绝对差的加权平均值：\n    $$ \\text{ECE} = \\sum_{m=1}^{B} \\frac{|b_m|}{N} |\\text{acc}(b_m) - \\text{conf}(b_m)| $$\n    其中 $|b_m|$ 是区间 $m$ 中的样本数，而 $N$ 是总样本数。较低的 ECE 意味着更好的校准。\n\n问题的核心是检验“逆熵重加权”的假设。这涉及到将每个样本的 logit 向量 $\\mathbf{l}$ 乘以一个标量 $s = 1/H(Y)$。新的 logit 是 $\\mathbf{l}' = s \\cdot \\mathbf{l}$。所有的指标——概率、预测、少数类别召回率和 ECE——都使用这些缩放后的 logit 重新计算。此操作类似于将 softmax 函数的温度调整为 $T=H(Y)$。高熵（不确定）的先验导致高温，从而“软化”概率；而低熵（尖锐）的先验导致低温，从而“锐化”概率。\n\n最后一步是比较缩放前后的性能。如果 $\\text{recall}_{\\text{after}} > \\text{recall}_{\\text{before}}$，则记录召回率有所改善；如果 $\\text{ECE}_{\\text{after}} < \\text{ECE}_{\\text{before}}$，则记录校准有所改善。将此过程应用于每个测试用例，以生成所需的结果。", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax\n\ndef calculate_metrics(logits, y_true, K, minority_classes_idx, n_bins=10):\n    \"\"\"\n    Calculates minority recall and ECE for a given set of logits and labels.\n    \"\"\"\n    N = len(y_true)\n    if N == 0:\n        return 0.0, 0.0\n\n    # 1. Get predictions and confidences\n    probs = softmax(logits, axis=1)\n    confidences = np.max(probs, axis=1)\n    y_pred = np.argmax(probs, axis=1)\n    correct = (y_pred == y_true)\n\n    # 2. Calculate Minority Recall\n    recall_set = minority_classes_idx\n    if not recall_set:  # If no minority classes, use all classes\n        recall_set = list(range(K))\n\n    recalls = []\n    for k in recall_set:\n        class_mask = (y_true == k)\n        n_class_samples = np.sum(class_mask)\n        if n_class_samples > 0:\n            tp = np.sum(correct[class_mask])\n            recall_k = tp / n_class_samples\n            recalls.append(recall_k)\n\n    minority_recall = np.mean(recalls) if recalls else 0.0\n\n    # 3. Calculate ECE\n    ece = 0.0\n    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n    \n    for i in range(n_bins):\n        lower, upper = bin_boundaries[i], bin_boundaries[i+1]\n        \n        # The first bin is inclusive of 0.\n        if i == 0:\n             in_bin = (confidences >= lower) & (confidences <= upper)\n        else:\n             in_bin = (confidences > lower) & (confidences <= upper)\n        \n        num_in_bin = np.sum(in_bin)\n\n        if num_in_bin > 0:\n            accuracy_in_bin = np.mean(correct[in_bin])\n            confidence_in_bin = np.mean(confidences[in_bin])\n            ece += (num_in_bin / N) * np.abs(accuracy_in_bin - confidence_in_bin)\n\n    return minority_recall, ece\n\ndef process_case(c, L, y, K, n_bins=10):\n    \"\"\"\n    Processes a single test case according to the problem description.\n    \"\"\"\n    c = np.array(c, dtype=float)\n    L = np.array(L, dtype=float)\n    y = np.array(y, dtype=int)\n    \n    # 1. Compute prior probabilities and Shannon entropy H(Y)\n    N = np.sum(c)\n    p = c / N\n    p_nonzero = p[p > 0]\n    H_Y = -np.sum(p_nonzero * np.log(p_nonzero))\n\n    # 2. Identify minority classes\n    uniform_prob = 1.0 / K\n    minority_classes_idx = [k for k, pk in enumerate(p) if pk < uniform_prob]\n\n    # 3. Compute baseline metrics\n    recall_before, ece_before = calculate_metrics(L, y, K, minority_classes_idx, n_bins)\n    \n    # 4. Perform reweighting and recompute metrics\n    if H_Y == 0:\n        s = 1.0  # Avoid division by zero, effectively no scaling\n    else:\n        s = 1.0 / H_Y\n    \n    L_after = L * s\n    recall_after, ece_after = calculate_metrics(L_after, y, K, minority_classes_idx, n_bins)\n\n    # 5. Determine if metrics improved\n    improved_recall = recall_after > recall_before\n    improved_ece = ece_after < ece_before\n\n    return [H_Y, recall_before, recall_after, ece_before, ece_after, improved_recall, improved_ece]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"c\": [4, 4, 4],\n            \"L\": [\n                [3.0, 1.0, 0.0], [2.5, 2.0, 1.0], [1.0, 1.2, 0.8], [2.2, -0.5, 0.1],\n                [0.5, 2.8, 1.0], [1.0, 2.2, 2.0], [0.2, 1.5, 1.6], [0.1, 2.5, -0.3],\n                [0.0, 0.5, 2.5], [1.0, 0.9, 1.2], [-0.2, 1.3, 1.1], [0.3, -0.4, 2.0]\n            ],\n            \"y\": [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2],\n            \"K\": 3\n        },\n        {\n            \"c\": [8, 3, 1],\n            \"L\": [\n                [3.4, 0.1, -0.2], [2.8, 0.5, 0.0], [3.1, 0.3, 0.2], [2.0, 1.1, 0.9],\n                [1.8, 1.5, 1.2], [2.5, 1.6, 1.6], [1.7, 1.9, 1.6], [2.6, 0.8, 0.7],\n                [1.5, 2.7, 1.0], [2.2, 1.8, 0.9], [2.0, 2.1, 1.9], [2.4, 1.3, 1.6]\n            ],\n            \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2],\n            \"K\": 3\n        },\n        {\n            \"c\": [10, 1, 1],\n            \"L\": [\n                [3.5, 0.2, 0.1], [3.0, 0.5, 0.4], [2.9, 0.1, -0.2], [2.3, 0.7, 0.6],\n                [2.6, 1.0, 0.9], [3.2, 0.8, 0.7], [2.8, 0.4, 0.3], [3.1, 1.2, 1.0],\n                [2.7, 1.4, 1.3], [2.4, 1.6, 1.5], [2.5, 2.2, 2.1], [2.5, 2.1, 2.2]\n            ],\n            \"y\": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2],\n            \"K\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['c'], case['L'], case['y'], case['K'])\n        results.append(result)\n    \n    # Format the final output string exactly as specified\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]},{str(res[5]).lower()},{str(res[6]).lower()}]\" \n        for res in results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3174037"}, {"introduction": "现在，我们将注意力从分析数据本身转移到分析模型学到的内部表征。在训练过程中，并非所有学到的特征都同等重要。一个直观的想法是，那些激活值非常稳定、变化单一的特征（例如卷积神经网络中的通道 $c$）所携带的信息量较少，其激活值 $Z_c$ 的熵 $H(Z_c)$ 也因此较低。这个练习 [@problem_id:3174074] 实现了一种基于此思想的剪枝策略：当一个通道的激活熵 $H(Z_c)$ 低于某个阈值 $\\tau$ 时，该通道被认为是冗余的并被移除。这提供了一个动手实践的机会，让我们了解信息论如何指导模型的优化和压缩，将抽象的理论应用于具体的工程挑战。", "problem": "考虑一个在加拿大高等研究院（CIFAR）$10$分类数据集上训练的卷积神经网络（CNN）。设通道 $c$ 在一组样本上的激活为一个实值随机变量 $Z_c$。根据信息论的第一性原理，一个具有概率 $\\{p_i\\}_{i=1}^B$ 的离散随机变量 $Z$ 的香农熵定义为 $$H(Z) = -\\sum_{i=1}^{B} p_i \\log_2 p_i,$$ 单位为比特（bits）。在实践中，激活是连续值的；为了估计熵，可以将 $Z_c$ 离散化到 $B$ 个区间（bin）中，并将经验区间频率视为概率。剪枝的核心思想是移除那些激活熵较低的通道，其假设是低熵通道携带的信息很少，因此是冗余的。\n\n您的任务是实现并评估一个基于熵的剪枝标准，这纯粹是一个基于上述定义的计算过程，无需访问任何外部数据。具体来说：\n\n1. 给定一个单层的一组通道激活，将每个通道的激活值离散化到 $B$ 个区间中，这些区间覆盖该通道的观测值范围，并使用以2为底的对数计算估计的香农熵 $H(Z_c)$（单位为比特）。在归一化之前，通过向每个区间的计数中添加一个小的常数 $\\varepsilon$ 来使用加性平滑，以避免零概率。\n\n2. 当且仅当 $H(Z_c) < \\tau$ 时，剪枝通道 $c$，其中 $\\tau$ 是用户指定的阈值（单位为比特）。剪枝决策是严格不等式，因此 $H(Z_c) = \\tau$ 的通道不被剪枝。\n\n3. 将压缩量化为被移除通道的比例，即 $$\\text{compression\\_fraction} = \\frac{\\text{被剪枝的通道数}}{\\text{总通道数}}.$$ 该值为无量纲量。\n\n4. 估计剪枝后在 CIFAR-$10$ 上的准确率（表示为 $0$ 到 $1$ 之间的小数），方法是从给定的基线准确率 $A_0$（剪枝前）开始，减去一个与移除的归一化信息成比例的惩罚项。对于 $B$ 个区间，每个通道可能的最大熵为 $$H_{\\max} = \\log_2(B).$$ 设 $\\eta$ 为一个无量纲的比例常数。剪枝后的预测准确率定义为 $$A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\\right)\\right),$$ 其中 $\\mathcal{P}$ 是被剪枝通道的集合。这将预测准确率限制在区间 $[0,1]$ 内。以小数形式报告准确率，不使用百分号。\n\n5. 为了将剪枝与冗余度联系起来，还需报告被剪枝通道的平均熵，$$\\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c),  \\text{若 } |\\mathcal{P}| > 0,\\\\ 0,  \\text{其他情况。}\\end{cases}$$\n\n实现细节：\n- 对每个通道使用 $B$ 个区间，在该通道的激活值范围内进行等宽区间划分。\n- 在归一化之前，对所有区间使用加性平滑常数 $\\varepsilon$。\n- 使用以2为底的对数计算熵，以比特为单位报告数值。\n\n测试套件：\n实现并评估以下四个测试用例。在每个用例中，通道由确定性序列定义，因此您的程序会产生固定的输出。\n\n情况1（正常路径，中等阈值）：\n- 每个通道的样本数 $N = 100$。\n- 通道（共6个）：\n  - 通道 $0$：对于 $n = 1, \\dots, N$，$z_n = 0$。\n  - 通道 $1$：周期性小数值 $z_n \\in \\{-0.02, 0, 0.02\\}$，重复至长度为 $N$。\n  - 通道 $2$：均匀序列 $z_n = -1 + \\frac{2(n-1)}{N-1}$，对于 $n = 1, \\dots, N$。\n  - 通道 $3$：由两个簇组成的双峰序列：前 $50$ 个样本在 $[-0.6,-0.4]$ 内线性间隔，后 $50$ 个样本在 $[0.4,0.6]$ 内线性间隔。\n  - 通道 $4$：对于所有 $n$，$z_n = 0.1$ 为恒定小数值。\n  - 通道 $5$：正弦序列 $z_n = 0.5 \\sin\\left(\\frac{8\\pi (n-1)}{N-1}\\right)$，对于 $n = 1, \\dots, N$。\n- 区间数 $B = 8$，阈值 $\\tau = 0.5$ 比特，平滑常数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.92$，比例常数 $\\eta = 0.03$。\n\n情况2（边界阈值，无剪枝）：\n- 通道和 $N$ 与情况1相同。\n- 区间数 $B = 8$，阈值 $\\tau = 0$，平滑常数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.92$，比例常数 $\\eta = 0.03$。\n\n情况3（激进阈值，大量剪枝）：\n- 通道和 $N$ 与情况1相同。\n- 区间数 $B = 8$，阈值 $\\tau = 2.5$ 比特，平滑常数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.92$，比例常数 $\\eta = 0.03$。\n\n情况4（不同的分布形状和区间数量）：\n- 每个通道的样本数 $N = 120$。\n- 通道（共4个）：\n  - 通道 $0$：对于所有 $n$，$z_n = 0$。\n  - 通道 $1$：均匀序列 $z_n = -2 + \\frac{4(n-1)}{N-1}$，对于 $n = 1, \\dots, N$。\n  - 通道 $2$：重尾混合：前 $60$ 个样本在 $[-0.1,0.1]$ 内线性间隔，接下来 $30$ 个样本在 $[-3,-2]$ 内，最后 $30$ 个样本在 $[2,3]$ 内。\n  - 通道 $3$：近乎恒定但有偶尔变化：前 $100$ 个样本 $z_n = 1$，后 $20$ 个样本在 $[0.9,1.1]$ 内线性间隔。\n- 区间数 $B = 4$，阈值 $\\tau = 1.0$ 比特，平滑常数 $\\varepsilon = 10^{-12}$，基线准确率 $A_0 = 0.90$，比例常数 $\\eta = 0.025$。\n\n所需输出：\n- 对每个用例，按顺序输出三个浮点数：compression\\_fraction, predicted\\_accuracy, average\\_entropy\\_pruned。\n- 您的程序应生成单行输出，其中包含以逗号分隔并用方括号括起来的结果列表（例如，对于4个用例，每个用例3个浮点数，按情况1、情况2、情况3、情况4的顺序，结果为 $[r_1,r_2,\\dots,r_{12}]$）。\n\n不允许使用外部文件、用户输入或网络访问。仅可使用指定的运行时环境。", "solution": "该问题要求基于香农熵原理，实现一个用于神经网络通道剪枝的确定性计算过程。该过程涉及计算通道激活的熵，应用剪枝规则，并报告几个派生指标：压缩率、预测的剪枝后准确率以及被移除通道的平均熵。\n\n对问题陈述的验证确认了其科学性、良构性和客观性。它基于信息论中香农熵的标准定义。对于连续变量估计熵的方法——通过区间划分进行离散化——是一种常用且有效的近似技术。派生指标的公式在数学上是合理的且无歧义。测试用例使用确定性序列构建，确保问题具有唯一、可验证的解。因此，该问题是有效的，我们可以继续进行求解。\n\n通过对每个提供的测试用例执行一系列步骤来实现解决方案。\n\n首先，对于每个通道，我们必须按规定生成激活值。问题将这些激活定义为确定性序列，而非随机样本。对于给定的通道 $c$，我们有一组 $N$ 个激活值 $\\{z_n\\}_{n=1}^N$。\n\n其次，我们估计每个通道激活变量 $Z_c$ 的香农熵 $H(Z_c)$。由于激活是连续值的，我们必须首先将其离散化。该过程规定在特定通道的观测激活值范围内使用 $B$ 个等宽区间。设通道 $c$ 的最小和最大激活值分别为 $z_{\\min, c} = \\min_n(z_n)$ 和 $z_{\\max, c} = \\max_n(z_n)$。范围 $[z_{\\min, c}, z_{\\max, c}]$ 被划分为 $B$ 个等宽的区间，宽度为 $w = (z_{\\max, c} - z_{\\min, c}) / B$。然后我们统计落入每个区间 $i$ 的激活数量 $k_i$（$i = 1, \\dots, B$）。如果一个通道中的所有激活都是恒定的，即 $z_{\\min, c} = z_{\\max, c}$，则会出现一个特殊情况。在这种情况下，所有 $N$ 个激活都落入单个区间，因此其计数为 $N$，而其他所有 $B-1$ 个区间的计数为 $0$。\n\n第三，为了计算熵公式所需的概率，我们必须处理零计数的可能性，因为这会使对数未定义。问题要求使用加性平滑（也称为拉普拉斯平滑）。将一个小的常数 $\\varepsilon$ 添加到每个区间的计数中。区间 $i$ 的平滑计数为 $k'_i = k_i + \\varepsilon$。平滑后的总计数为 $N' = \\sum_{i=1}^{B} k'_i = (\\sum k_i) + B\\varepsilon = N + B\\varepsilon$。然后，每个区间的概率估计为 $p_i = k'_i / N'$。\n\n有了这些概率，使用以2为底的对数计算通道 $c$ 的香农熵（单位为比特）：\n$$\nH(Z_c) = -\\sum_{i=1}^{B} p_i \\log_2 p_i\n$$\n对层中的每个通道都执行此计算。\n\n第四，我们应用剪枝标准。当且仅当一个通道 $c$ 的计算熵 $H(Z_c)$ 严格小于给定的阈值 $\\tau$ 时，该通道被标记为待剪枝。我们确定被剪枝通道的集合，记为 $\\mathcal{P}$。\n$$\n\\mathcal{P} = \\{c \\mid H(Z_c) < \\tau\\}\n$$\n\n第五，我们基于被剪枝通道的集合 $\\mathcal{P}$ 计算三个所需的输出指标。\n\n1.  **压缩率**：这是被剪枝通道数与总通道数 $C$ 的比率。\n    $$\n    \\text{compression\\_fraction} = \\frac{|\\mathcal{P}|}{C}\n    $$\n\n2.  **被剪枝通道的平均熵**：该指标量化了被视为冗余的通道的平均信息含量。其定义为：\n    $$\n    \\overline{H}_{\\text{pruned}} = \\begin{cases}\\frac{1}{|\\mathcal{P}|} \\sum_{c \\in \\mathcal{P}} H(Z_c),  \\text{若 } |\\mathcal{P}| > 0,\\\\ 0,  \\text{其他情况。}\\end{cases}\n    $$\n    $|\\mathcal{P}|=0$ 的情况确保了在没有通道被剪枝时有明确的输出。\n\n3.  **预测的剪枝后准确率**：这是一个基于惩罚模型的估计值。该模型从基线准确率 $A_0$ 开始，减去一个与剪枝移除的总归一化信息成比例的惩罚项。对于 $B$ 个区间，一个分布的最大可能熵是 $H_{\\max} = \\log_2(B)$，这在分布是均匀时发生。预测准确率 $A_{\\text{pred}}$ 由下式给出：\n    $$\n    A_{\\text{pred}} = A_0 - \\eta \\sum_{c \\in \\mathcal{P}} \\frac{H(Z_c)}{H_{\\max}}\n    $$\n    其中 $\\eta$ 是一个无量纲的比例常数。最终值被限制在区间 $[0, 1]$ 内，以确保其为有效的准确率值：\n    $$\n    A_{\\text{pred}} = \\max\\left(0, \\min\\left(1, A_{\\text{pred}}\\right)\\right)\n    $$\n\n通过系统地将这些步骤应用于问题陈述中定义的四个测试用例，我们可以生成所需的数值结果。每个步骤都是对所提供公式和规则的直接实现。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the channel pruning problem for all test cases.\n    \"\"\"\n\n    def compute_metrics(activations_list, B, tau, epsilon, A0, eta):\n        \"\"\"\n        Computes entropy for each channel, prunes, and calculates metrics.\n        \"\"\"\n        num_channels = len(activations_list)\n        entropies = []\n        N = len(activations_list[0]) if num_channels > 0 else 0\n\n        for act in activations_list:\n            min_val, max_val = np.min(act), np.max(act)\n            \n            if min_val == max_val:\n                # All values are constant, they fall into a single bin.\n                counts = np.zeros(B)\n                counts[0] = N\n            else:\n                # Use numpy's histogram function for equal-width binning.\n                # The range is inclusive of min_val but exclusive of max_val,\n                # except for the last bin which is inclusive of max_val.\n                counts, _ = np.histogram(act, bins=B, range=(min_val, max_val))\n            \n            # Additive smoothing\n            smoothed_counts = counts.astype(np.float64) + epsilon\n            \n            # Normalize to get probabilities\n            total_smoothed_count = np.sum(smoothed_counts)\n            probs = smoothed_counts / total_smoothed_count\n            \n            # Calculate Shannon entropy in bits (log base 2)\n            # We filter out zero probabilities, though smoothing prevents this.\n            # This is just for robustness.\n            non_zero_probs = probs[probs > 0]\n            entropy = -np.sum(non_zero_probs * np.log2(non_zero_probs))\n            entropies.append(entropy)\n\n        # Identify pruned channels\n        pruned_indices = [i for i, h in enumerate(entropies) if h < tau]\n        pruned_entropies = [entropies[i] for i in pruned_indices]\n        \n        num_pruned = len(pruned_indices)\n        \n        # 1. Compression Fraction\n        compression_fraction = num_pruned / num_channels if num_channels > 0 else 0.0\n\n        # 2. Average Entropy of Pruned Channels\n        avg_entropy_pruned = np.mean(pruned_entropies) if num_pruned > 0 else 0.0\n\n        # 3. Predicted Post-Pruning Accuracy\n        H_max = np.log2(B)\n        # Avoid division by zero if B = 1, though problem B >= 4\n        if H_max == 0:\n            penalty = 0\n        else:\n            total_info_removed = np.sum(pruned_entropies)\n            penalty = eta * total_info_removed / H_max\n        \n        predicted_accuracy = A0 - penalty\n        predicted_accuracy = np.clip(predicted_accuracy, 0.0, 1.0)\n        \n        return compression_fraction, predicted_accuracy, avg_entropy_pruned\n\n    # --- Test Cases Definition ---\n    test_cases = [\n        {\n            \"id\": 1,\n            \"N\": 100, \"B\": 8, \"tau\": 0.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 2,\n            \"N\": 100, \"B\": 8, \"tau\": 0.0, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 3,\n            \"N\": 100, \"B\": 8, \"tau\": 2.5, \"epsilon\": 1e-12, \"A0\": 0.92, \"eta\": 0.03,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.tile(np.array([-0.02, 0, 0.02]), n // 3 + 1)[:n],\n                lambda n: np.linspace(-1.0, 1.0, n),\n                lambda n: np.concatenate([np.linspace(-0.6, -0.4, n // 2), np.linspace(0.4, 0.6, n // 2)]),\n                lambda n: np.full(n, 0.1),\n                lambda n: 0.5 * np.sin(8 * np.pi * np.arange(n) / (n - 1)),\n            ]\n        },\n        {\n            \"id\": 4,\n            \"N\": 120, \"B\": 4, \"tau\": 1.0, \"epsilon\": 1e-12, \"A0\": 0.90, \"eta\": 0.025,\n            \"channels_def\": [\n                lambda n: np.zeros(n),\n                lambda n: np.linspace(-2.0, 2.0, n),\n                lambda n: np.concatenate([\n                    np.linspace(-0.1, 0.1, n // 2), \n                    np.linspace(-3.0, -2.0, n // 4), \n                    np.linspace(2.0, 3.0, n // 4)\n                ]),\n                lambda n: np.concatenate([\n                    np.full(100, 1.0),\n                    np.linspace(0.9, 1.1, 20)\n                ]),\n            ]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        activations_list = [gen(N) for gen in case[\"channels_def\"]]\n        \n        results = compute_metrics(\n            activations_list,\n            case[\"B\"],\n            case[\"tau\"],\n            case[\"epsilon\"],\n            case[\"A0\"],\n            case[\"eta\"]\n        )\n        all_results.extend(results)\n\n    print(f\"[{','.join(f'{r:.7f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "3174074"}]}