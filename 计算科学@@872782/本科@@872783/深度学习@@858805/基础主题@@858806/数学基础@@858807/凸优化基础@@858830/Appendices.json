{"hands_on_practices": [{"introduction": "梯度下降法是深度学习优化的核心，但其性能在很大程度上取决于步长（学习率）$ \\alpha $ 的选择。一个固定的、过大的 $ \\alpha $ 可能导致算法发散，而过小的 $ \\alpha $ 则会使收敛速度变得极其缓慢。本练习将指导你从凸优化第一性原理出发，基于 $ L $-光滑 (L-smoothness) 性质，推导出一个被称为回溯线搜索 (backtracking line search) 的自适应步长选择规则。通过完成这个练习 [@problem_id:3126951]，你不仅能理解这一常用启发式方法背后的理论依据，还能亲手将其应用于逻辑回归模型，从而真正掌握理论与实践的联系。", "problem": "您正在研究用于训练二元逻辑回归模型的基于梯度的优化方法，并希望从凸优化的第一性原理出发，为一种实用的步长选择规则提供理论依据。考虑一个可微凸函数 $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$，其梯度是 $L$-Lipschitz 连续的（即，$f$ 是 $L$-光滑的），这意味着对于所有 $x,y\\in\\mathbb{R}^{d}$，梯度满足 $\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\|$。从这个定义和标准微积分出发，为梯度步长 $x^{+}=x-\\alpha \\nabla f(x)$ 推导出一个仅依赖于 $f(x)$、$\\nabla f(x)$ 和 $\\alpha$ 的充分下降条件，然后用它构建一个回溯线搜索规则，该规则保证在 $\\alpha\\leq 1/L$ 时步长被接受。\n\n将您推導出的规则应用于一个不带截距项的二元逻辑回归问题，其损失函数为二元交叉熵 (BCE) 损失，数据集包含 $n=3$ 个样本：\n- $x_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，其 $y_{1}=1$，\n- $x_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}$，其 $y_{2}=0$，\n- $x_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}$，其 $y_{3}=1$，\n其中模型为 $p_{\\theta}(y=1\\mid x)=\\sigma(\\theta^{\\top}x)$，逻辑 sigmoid 函数为 $\\sigma(t)=\\frac{1}{1+\\exp(-t)}$，经验损失为\n$$\nf(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\Big(-y_{i}\\ln \\sigma(\\theta^{\\top}x_{i})-(1-y_{i})\\ln(1-\\sigma(\\theta^{\\top}x_{i}))\\Big).\n$$\n从 $\\theta_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ 开始梯度法，并使用您推导的充分下降规则，沿着最陡下降方向 $-\\nabla f(\\theta_{0})$ 执行回溯线搜索。使用以下回溯参数：初始步长 $\\alpha_{0}=1$，收缩因子 $\\tau=\\frac{1}{2}$，以及充分下降常数 $c=\\frac{1}{4}$。确定在第一次迭代中线搜索接受的步长 $\\alpha$。\n\n将最终答案表示为一个无量纲数。将您的答案四舍五入到四位有效数字。", "solution": "该问题包含两部分。首先，我们必须为一个 $L$-光滑凸函数上的梯度下降推导一个充分下降条件，并用它来构建一个回溯线搜索规则。其次，我们必须将此规则应用于一个特定的二元逻辑回归问题。\n\n第1部分：充分下降条件和回溯规则的推导\n\n设 $f:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ 是一个可微凸函数，其梯度是 $L$-Lipschitz 连续的。梯度的 $L$-Lipschitz 连续性，也称为 $L$-光滑性，由以下不等式定义：\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|\\leq L\\|x-y\\| \\quad \\forall x,y\\in\\mathbb{R}^{d}\n$$\n其中 $L  0$ 是 Lipschitz 常数。该性质的一个关键推论是下降引理，或称函数 $f$ 的二次上界。我们首先推导这个引理。\n\n考虑函数 $g(t) = f(x+t(y-x))$，其中 $t \\in [0,1]$。根据链式法则，其导数为 $g'(t) = \\nabla f(x+t(y-x))^{\\top}(y-x)$。微积分基本定理表明 $g(1) - g(0) = \\int_{0}^{1} g'(t) dt$。用 $f$ 表示，这可以写成：\n$$\nf(y) - f(x) = \\int_{0}^{1} \\nabla f(x+t(y-x))^{\\top}(y-x) dt\n$$\n我们从两侧减去项 $\\nabla f(x)^{\\top}(y-x)$，该项可以写成 $\\int_{0}^{1} \\nabla f(x)^{\\top}(y-x) dt$：\n$$\nf(y) - f(x) - \\nabla f(x)^{\\top}(y-x) = \\int_{0}^{1} (\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) dt\n$$\n对被积函数应用 Cauchy-Schwarz 不等式，我们得到：\n$$\n(\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) \\leq \\|\\nabla f(x+t(y-x)) - \\nabla f(x)\\| \\|y-x\\|\n$$\n对梯度项使用 $L$-光滑性性质：\n$$\n\\|\\nabla f(x+t(y-x)) - \\nabla f(x)\\| \\leq L\\|(x+t(y-x)) - x\\| = L\\|t(y-x)\\| = Lt\\|y-x\\|\n$$\n因为 $t \\geq 0$。结合这些不等式，我们对被积函数进行限定：\n$$\n(\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) \\leq Lt\\|y-x\\|^{2}\n$$\n将此界限从 $t=0$ 积分到 $t=1$：\n$$\n\\int_{0}^{1} (\\nabla f(x+t(y-x)) - \\nabla f(x))^{\\top}(y-x) dt \\leq \\int_{0}^{1} Lt\\|y-x\\|^{2} dt = L\\|y-x\\|^{2} \\int_{0}^{1} t dt = \\frac{L}{2}\\|y-x\\|^{2}\n$$\n这就得到了 $f(y)$ 的二次上界：\n$$\nf(y) \\leq f(x) + \\nabla f(x)^{\\top}(y-x) + \\frac{L}{2}\\|y-x\\|^{2}\n$$\n现在，我们考虑一个梯度下降步，其中下一个迭代点是 $x^{+} = x - \\alpha \\nabla f(x)$，步长为 $\\alpha  0$。我们在上面的不等式中令 $y=x^{+}$，因此 $y-x = -\\alpha\\nabla f(x)$。\n$$\nf(x^{+}) \\leq f(x) + \\nabla f(x)^{\\top}(-\\alpha\\nabla f(x)) + \\frac{L}{2}\\|-\\alpha\\nabla f(x)\\|^{2}\n$$\n$$\nf(x^{+}) \\leq f(x) - \\alpha\\|\\nabla f(x)\\|^{2} + \\frac{L\\alpha^{2}}{2}\\|\\nabla f(x)\\|^{2}\n$$\n这个不等式 $f(x - \\alpha\\nabla f(x)) \\leq f(x) - \\alpha(1 - \\frac{L\\alpha}{2})\\|\\nabla f(x)\\|^2$ 是一个从第一性原理推导出的充分下降条件。\n\n接下来，我们构建一个回溯线搜索规则。标准规则，即 Armijo-Goldstein 条件，要求步长 $\\alpha$ 满足：\n$$\nf(x - \\alpha\\nabla f(x)) \\leq f(x) - c\\alpha\\|\\nabla f(x)\\|^{2}\n$$\n对于一个常数 $c \\in (0,1)$。线搜索过程从一个初始步长 $\\alpha$ 开始，并以因子 $\\tau \\in (0,1)$ 不断减小它，直到满足此条件。\n\n为保证此搜索终止，我们必须证明对于足够小的 $\\alpha$，该条件最终会被满足。从我们推导出的二次上界来看，如果满足以下条件，Armijo 条件就会成立：\n$$\nf(x) - \\alpha\\|\\nabla f(x)\\|^{2} + \\frac{L\\alpha^{2}}{2}\\|\\nabla f(x)\\|^{2} \\leq f(x) - c\\alpha\\|\\nabla f(x)\\|^{2}\n$$\n假设 $\\nabla f(x) \\neq 0$，我们可以将其简化为：\n$$\n-1 + \\frac{L\\alpha}{2} \\leq -c \\quad \\implies \\quad \\frac{L\\alpha}{2} \\leq 1-c \\quad \\implies \\quad \\alpha \\leq \\frac{2(1-c)}{L}\n$$\n问题要求一个在 $\\alpha \\leq 1/L$ 时保证接受的规则。为了让我们的条件对任何 $\\alpha \\leq 1/L$ 都成立，我们需要区间 $[0, 1/L]$ 是 $[0, 2(1-c)/L]$ 的子集。这要求：\n$$\n\\frac{1}{L} \\leq \\frac{2(1-c)}{L} \\quad \\implies \\quad 1 \\leq 2(1-c) \\quad \\implies \\quad \\frac{1}{2} \\leq 1-c \\quad \\implies \\quad c \\leq \\frac{1}{2}\n$$\n因此，使用带有任何参数 $c \\in (0, 1/2]$ 的 Armijo 条件的回溯线搜索规则是一种有效的构造，因为它保证任何步长 $\\alpha \\leq 1/L$ 都将被接受。\n\n第2部分：应用于二元逻辑回归\n\n经验损失函数是在 $n=3$ 个样本上的平均二元交叉熵 (BCE)：\n$$\nf(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n}\\Big(-y_{i}\\ln \\sigma(\\theta^{\\top}x_{i})-(1-y_{i})\\ln(1-\\sigma(\\theta^{\\top}x_{i}))\\Big)\n$$\n单个样本 $i$ 的损失函数可以等价地写为 $\\text{loss}_i(\\theta) = \\ln(1+e^{\\theta^T x_i}) - y_i \\theta^T x_i$。\n\n让我们使用给定的数据：\n- $x_{1}=\\begin{pmatrix}1\\\\0\\end{pmatrix}, y_{1}=1 \\implies \\text{loss}_1(\\theta) = \\ln(1+e^{\\theta_1})-\\theta_1 = \\ln(1+e^{-\\theta_1})$\n- $x_{2}=\\begin{pmatrix}0\\\\1\\end{pmatrix}, y_{2}=0 \\implies \\text{loss}_2(\\theta) = \\ln(1+e^{\\theta_2})$\n- $x_{3}=\\begin{pmatrix}1\\\\1\\end{pmatrix}, y_{3}=1 \\implies \\text{loss}_3(\\theta) = \\ln(1+e^{\\theta_1+\\theta_2})-(\\theta_1+\\theta_2) = \\ln(1+e^{-(\\theta_1+\\theta_2)})$\n\n总损失函数为：\n$$\nf(\\theta) = \\frac{1}{3} \\left( \\ln(1+e^{-\\theta_1}) + \\ln(1+e^{\\theta_2}) + \\ln(1+e^{-(\\theta_1+\\theta_2)}) \\right)\n$$\n我们从 $\\theta_0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$ 开始。初始损失为：\n$$\nf(\\theta_0) = \\frac{1}{3}(\\ln(1+e^0) + \\ln(1+e^0) + \\ln(1+e^0)) = \\frac{1}{3}(3\\ln 2) = \\ln 2\n$$\nBCE 损失的梯度是 $\\nabla f(\\theta) = \\frac{1}{n}\\sum_{i=1}^{n}(\\sigma(\\theta^{\\top}x_i)-y_i)x_i$。在 $\\theta_0 = \\begin{pmatrix}0\\\\0\\end{pmatrix}$ 处，对于所有的 $i$ 都有 $\\theta_0^{\\top}x_i=0$，并且 $\\sigma(0) = \\frac{1}{1+e^0} = \\frac{1}{2}$。\n$$\n\\nabla f(\\theta_0) = \\frac{1}{3} \\left( \\left(\\frac{1}{2}-1\\right)x_1 + \\left(\\frac{1}{2}-0\\right)x_2 + \\left(\\frac{1}{2}-1\\right)x_3 \\right)\n$$\n$$\n\\nabla f(\\theta_0) = \\frac{1}{3} \\left( -\\frac{1}{2}\\begin{pmatrix}1\\\\0\\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix}0\\\\1\\end{pmatrix} - \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}-1/2 - 1/2 \\\\ 1/2 - 1/2\\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix}-1\\\\0\\end{pmatrix} = \\begin{pmatrix}-1/3\\\\0\\end{pmatrix}\n$$\n梯度的平方范数是 $\\|\\nabla f(\\theta_0)\\|^2 = (-1/3)^2 + 0^2 = 1/9$。\n\n我们使用初始步长 $\\alpha_0=1$、收缩因子 $\\tau=1/2$ 和常数 $c=1/4$ 执行回溯线搜索。需要检查的条件是：\n$$\nf(\\theta_0 - \\alpha \\nabla f(\\theta_0)) \\leq f(\\theta_0) - c \\alpha \\|\\nabla f(\\theta_0)\\|^2\n$$\n让我们测试初始步长 $\\alpha=1$。候选点是：\n$$\n\\theta_1 = \\theta_0 - (1)\\nabla f(\\theta_0) = \\begin{pmatrix}0\\\\0\\end{pmatrix} - \\begin{pmatrix}-1/3\\\\0\\end{pmatrix} = \\begin{pmatrix}1/3\\\\0\\end{pmatrix}\n$$\n条件的左侧 (LHS) 是在这个新点的损失值：\n$$\nf(\\theta_1) = f\\left(\\begin{pmatrix}1/3\\\\0\\end{pmatrix}\\right) = \\frac{1}{3} \\left( \\ln(1+e^{-1/3}) + \\ln(1+e^{0}) + \\ln(1+e^{-(1/3+0)}) \\right)\n$$\n$$\nf(\\theta_1) = \\frac{1}{3} \\left( 2\\ln(1+e^{-1/3}) + \\ln 2 \\right)\n$$\n条件的右侧 (RHS) 是：\n$$\nf(\\theta_0) - c\\alpha\\|\\nabla f(\\theta_0)\\|^2 = \\ln 2 - \\frac{1}{4}(1)\\left(\\frac{1}{9}\\right) = \\ln 2 - \\frac{1}{36}\n$$\n$\\alpha=1$ 的条件是：\n$$\n\\frac{1}{3} \\left( 2\\ln(1+e^{-1/3}) + \\ln 2 \\right) \\leq \\ln 2 - \\frac{1}{36}\n$$\n两边乘以 $3$ 并重新整理：\n$$\n2\\ln(1+e^{-1/3}) + \\ln 2 \\leq 3\\ln 2 - \\frac{3}{36} \\implies 2\\ln(1+e^{-1/3}) \\leq 2\\ln 2 - \\frac{1}{12}\n$$\n$$\n\\ln(1+e^{-1/3}) \\leq \\ln 2 - \\frac{1}{24}\n$$\n数值上，$\\ln(2) \\approx 0.693147$ 且 $1/24 \\approx 0.041667$，所以 RHS 约等于 $0.65148$。\n对于 LHS，$e^{-1/3} \\approx 0.716531$，所以 $1+e^{-1/3} \\approx 1.716531$。那么 $\\ln(1.716531) \\approx 0.540316$。\n不等式 $0.540316 \\leq 0.65148$ 成立。\n\n由于初始步长 $\\alpha_0=1$ 满足条件，线搜索立即终止并接受 $\\alpha=1$。\n问题要求将答案四舍五入到四位有效数字。精确答案是 $1$，表示为四位有效数字是 $1.000$。", "answer": "$$\\boxed{1.000}$$", "id": "3126951"}, {"introduction": "在许多机器学习和现实世界的问题中，模型的参数需要满足特定约束，例如，权重非负或概率分布的和为一。处理这类约束优化问题是优化理论的一个重要分支。本练习将介绍 Frank-Wolfe 算法，这是一种优雅的“免投影” (projection-free) 方法，特别适用于具有复杂约束集的问题 [@problem_id:3126940]。你将为带单纯形约束的多项式逻辑回归问题，推导该算法的关键组成部分——线性最小化预言机 (Linear Minimization Oracle, LMO)，并动手实现完整的算法，从而学习一种处理约束问题的有效范式。", "problem": "本题要求您为深度学习中的一个凸学习问题——在每个类别权重向量上带有单纯形约束的多项式逻辑回归——推导、设计并实现一个完整的 Frank-Wolfe (FW) 优化方法。您的最终答案必须是一个单一、可运行的 Python 程序，该程序能为固定的测试套件计算结果，并按指定格式在一行中打印出来。\n\n问题设定。考虑一个数据集，其特征矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，标签为 $y \\in \\{1,2,\\dots,C\\}^n$。模型参数为一个权重矩阵 $W \\in \\mathbb{R}^{d \\times C}$，其列向量被约束在概率单纯形内。也就是说，对于每个类别 $c \\in \\{1,2,\\dots,C\\}$，列向量 $W_{:,c}$ 满足对所有 $i \\in \\{1,2,\\dots,d\\}$ 都有 $W_{i,c} \\ge 0$ 且 $\\sum_{i=1}^d W_{i,c} = 1$。令 $Z = X W \\in \\mathbb{R}^{n \\times C}$ 为 logit 矩阵，并逐行定义 softmax 概率为 $p_{k,c}(W) = \\exp(Z_{k,c}) \\big/ \\sum_{j=1}^C \\exp(Z_{k,j})$，其中 $k \\in \\{1,\\dots,n\\}$ 为样本索引，$c \\in \\{1,\\dots,C\\}$ 为类别索引。目标函数是平均多项式交叉熵损失\n$$\nf(W) = \\frac{1}{n}\\sum_{k=1}^n \\left(- \\log p_{k,y_k}(W) \\right).\n$$\n您必须推导梯度并证明其凸性，以证明在可行集 $\\mathcal{D} = \\Delta^d \\times \\cdots \\times \\Delta^d$（$C$ 个单纯形的笛卡尔积）上使用 Frank-Wolfe 方法是合理的，其中 $\\Delta^d = \\{u \\in \\mathbb{R}^d \\mid u \\ge 0, \\ \\sum_{i=1}^d u_i = 1 \\}$。\n\n缩略语定义。Frank-Wolfe (FW) 方法是一种用于约束凸优化的无投影算法。线性最小化预言机 (Linear Minimization Oracle, LMO) 用于求解给定梯度矩阵 $G$ 的 $\\arg\\min_{S \\in \\mathcal{D}} \\langle S, G \\rangle$ 问题，其中 $\\langle \\cdot,\\cdot\\rangle$ 表示弗罗贝尼乌斯内积。多项式逻辑回归 (Multinomial Logistic Regression, MLR) 使用 softmax 进行多类别分类。\n\n您必须完成以下任务。\n\n$1.$ 从 softmax 和交叉熵的定义出发，使用基础微积分和链式法则推导关于 $W$ 的梯度 $\\nabla f(W)$。请勿假设任何预先记下的梯度公式。\n\n$2.$ 通过引用 log-sum-exp 函数的凸性以及复合仿射映射保持凸性的事实，证明 $f(W)$ 相对于 $W$ 是凸的。\n\n$3.$ 推导可行集 $\\mathcal{D} = \\Delta^d \\times \\cdots \\times \\Delta^d$ 的线性最小化预言机 (LMO)。证明在一个单纯形上最小化线性函数会选择一个极点（一个标准基向量），并由此得出如何为给定的 $G \\in \\mathbb{R}^{d \\times C}$ 构建使 $\\langle S, G \\rangle$ 最小化的 $S \\in \\mathbb{R}^{d \\times C}$。\n\n$4.$ 实现包含以下组件的 Frank-Wolfe 方法：\n- 初始化 $W_0$，使每个列向量均为均匀分布，即 $W_{0,i,c} = 1/d$。\n- 在第 $t$ 次迭代中，计算梯度 $G_t = \\nabla f(W_t)$。\n- 调用 LMO 得到 $S_t \\in \\arg\\min_{S \\in \\mathcal{D}} \\langle S, G_t \\rangle$。\n- 构建 FW 方向 $D_t = S_t - W_t$。\n- 对 $\\gamma_t \\in [0,1]$ 进行精确一维线搜索，以最小化 $\\phi_t(\\gamma) = f(W_t + \\gamma D_t)$。\n- 更新 $W_{t+1} = W_t + \\gamma_t D_t$。\n- 计算 Frank-Wolfe 对偶间隙 $g_t = \\langle W_t - S_t, G_t \\rangle$，如果 $g_t \\le \\varepsilon$ 或达到最大迭代次数，则停止。\n\n数值稳定性要求。通过在进行指数化之前从 logit 中减去行最大值，以数值稳定的方式实现 softmax。\n\n测试套件与参数。您的程序必须在以下固定的测试用例上运行 FW 算法，并汇总指定的输出。\n\n- 线性最小化预言机测试：\n  - 测试 A (正常情况)：对于 $\\mathbb{R}^5$ 中的向量 $c = [1.5, -2.0, 0.0, 3.0, -1.0]$，在 $\\Delta^5$ 上的 LMO 必须返回最小分量处的极点，即在索引 $\\arg\\min_i c_i$ 处的标准基向量。将返回的索引记录为整数（在代码中使用从零开始的索引）。\n  - 测试 B (平局/边界情况)：对于 $\\mathbb{R}^3$ 中的向量 $c = [2.0, 2.0, 2.0]$，存在多个最小化点。您的实现必须确定性地选择最小化点中最小的索引。将该索引记录为整数（从零开始）。\n\n- Frank-Wolfe 优化测试：\n  - 数据。使用 $n = 6$, $d = 3$, $C = 3$。令\n    $$\n    X = \\begin{bmatrix}\n    1  0  0\\\\\n    0  1  0\\\\\n    0  0  1\\\\\n    1  0  0\\\\\n    0  1  0\\\\\n    0  0  1\n    \\end{bmatrix}, \\quad\n    y = \\begin{bmatrix}\n    1\\\\2\\\\3\\\\1\\\\2\\\\3\n    \\end{bmatrix}.\n    $$\n    本问题不使用角度，因此不需要角度单位。不涉及物理单位。\n  - 算法设置。使用最多 $T = 200$ 次迭代和容忍度 $\\varepsilon = 10^{-6}$。初始化 $W_0$，使其每个列向量都等于 $\\Delta^3$ 中的均匀向量。\n  - 需要记录的输出：\n    - 测试 C：运行 FW 后，记录最终对偶间隙 $g_T \\le 5 \\times 10^{-4}$ 是否成立，结果为布尔值。\n    - 测试 D：将最终目标值 $f(W_T)$ 记录为浮点数，四舍五入到六位小数。\n    - 测试 E：记录 $f(W_T)  f(W_0)$ 是否成立，结果为布尔值。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，结果按以下顺序排列：\n$[ \\text{测试 A 整数索引}, \\text{测试 C 布尔值}, \\text{测试 B 整数索引}, \\text{测试 D 浮点数}, \\text{测试 E 布尔值} ]$。\n例如，一个有效的输出应类似于 $[1,True,0,0.551445,True]$。\n\n约束与指南。\n- 仅使用执行环境中指定的标准数值库。\n- 确保您的代码是自包含的，不使用外部输入，并严格按照要求格式打印一行输出。\n- 您的解答中的推导必须从上面提供的定义以及诸如 log-sum-exp 的凸性和仿射函数的性质等标准、广为接受的事实出发，而不是从目标公式本身出发。", "solution": "### 1. 目标函数的梯度推导\n\n目标函数是平均多项式交叉熵损失函数：\n$$\nf(W) = \\frac{1}{n}\\sum_{k=1}^n L_k(W) \\quad \\text{其中} \\quad L_k(W) = -\\log p_{k,y_k}(W)\n$$\n参数是权重矩阵 $W \\in \\mathbb{R}^{d \\times C}$。logit 是 $Z = XW$。softmax 概率是 $p_{k,c}(W) = \\exp(Z_{k,c}) \\big/ \\sum_{j=1}^C \\exp(Z_{k,j})$。\n\n我们展开单个数据样本 $k$ 的损失项 $L_k(W)$：\n$$\nL_k(W) = -\\log \\left( \\frac{\\exp(Z_{k,y_k})}{\\sum_{j=1}^C \\exp(Z_{k,j})} \\right) = -Z_{k,y_k} + \\log\\left(\\sum_{j=1}^C \\exp(Z_{k,j})\\right)\n$$\n为了找到梯度 $\\nabla f(W)$，我们计算关于矩阵 $W$ 的每个元素 $W_{ab}$ 的偏导数，其中 $a \\in \\{1, \\dots, d\\}$ 且 $b \\in \\{1, \\dots, C\\}$。\n$$\n\\frac{\\partial f(W)}{\\partial W_{ab}} = \\frac{1}{n} \\sum_{k=1}^n \\frac{\\partial L_k(W)}{\\partial W_{ab}}\n$$\n我们应用链式法则。损失 $L_k$ 通过 logit $Z_{kc}$ 依赖于 $W_{ab}$。\n$$\n\\frac{\\partial L_k(W)}{\\partial W_{ab}} = \\sum_{c=1}^C \\frac{\\partial L_k(W)}{\\partial Z_{kc}} \\frac{\\partial Z_{kc}}{\\partial W_{ab}}\n$$\n首先，我们求一个 logit 相对于一个权重的导数。logit $Z_{kc}$ 定义为 $Z_{kc} = \\sum_{m=1}^d X_{km} W_{mc}$。\n$$\n\\frac{\\partial Z_{kc}}{\\partial W_{ab}} = \\frac{\\partial}{\\partial W_{ab}} \\left( \\sum_{m=1}^d X_{km} W_{mc} \\right) = X_{ka} \\delta_{cb}\n$$\n其中 $\\delta_{cb}$ 是克罗内克 delta 符号。仅当类别 $c=b$ 时，导数非零。\n\n接下来，我们求 $L_k$ 相对于一个 logit $Z_{kc}$ 的导数。\n$$\n\\frac{\\partial L_k(W)}{\\partial Z_{kc}} = \\frac{\\partial}{\\partial Z_{kc}} \\left( -Z_{ky_k} + \\log\\left(\\sum_{j=1}^C \\exp(Z_{kj})\\right) \\right) = -\\delta_{c,y_k} + \\frac{\\exp(Z_{kc})}{\\sum_{j=1}^C \\exp(Z_{kj})} = p_{kc} - \\delta_{c,y_k}\n$$\n这里我们用 $y_k$ 表示样本 $k$ 的真实类别标签，如果 $c=y_k$ 则 $\\delta_{c,y_k}$ 为 1，否则为 0。\n\n使用链式法则结合这些结果：\n$$\n\\frac{\\partial L_k(W)}{\\partial W_{ab}} = \\sum_{c=1}^C (p_{kc} - \\delta_{c,y_k})(X_{ka}\\delta_{cb}) = (p_{kb} - \\delta_{b,y_k})X_{ka}\n$$\n最后，我们对所有 $n$ 个样本求平均，得到梯度矩阵的分量：\n$$\n(\\nabla f(W))_{ab} = \\frac{\\partial f(W)}{\\partial W_{ab}} = \\frac{1}{n} \\sum_{k=1}^n X_{ka} (p_{kb} - \\delta_{b,y_k})\n$$\n这个表达式可以紧凑地写成矩阵形式。令 $P$ 为元素为 $p_{kc}$ 的 $n \\times C$ 矩阵，令 $Y$ 为标签的 $n \\times C$ one-hot 编码矩阵，其中 $Y_{kc} = \\delta_{c,y_k}$。则梯度为：\n$$\n\\nabla f(W) = \\frac{1}{n} X^T (P - Y)\n$$\n\n### 2. 目标函数的凸性\n\n我们需要证明 $f(W)$ 是 $W$ 的凸函数。由于 $f(W)$ 是每个样本损失函数 $L_k(W)$ 的正权重和，只需证明每个 $L_k(W)$ 对 $W$ 是凸的即可。\n$L_k(W)$ 函数可以看作是函数的复合。令 $Z_{k,:} \\in \\mathbb{R}^C$ 为样本 $k$ 的 logit 向量。\n$L_k(Z_{k,:}) = \\log\\left(\\sum_{j=1}^C \\exp(Z_{kj})\\right) - Z_{ky_k}$。\n函数 $\\text{LSE}(\\mathbf{z}) = \\log(\\sum_j \\exp(z_j))$ 是 log-sum-exp 函数，它是 $\\mathbb{R}^C$ 上的一个著名凸函数。项 $-Z_{ky_k}$ 是 $Z_{k,:}$ 的一个线性函数，因此也是凸的（也是凹的）。两个凸函数的和是凸函数，所以 $L_k$ 是 logit 向量 $Z_{k,:}$ 的一个凸函数。\n\nlogit 与模型参数 $W$ 之间的关系由 $Z = XW$ 给出。对于单个样本 $k$，logit 向量 $Z_{k,:}$ 是 $Z$ 的第 $k$ 行：\n$$\nZ_{k,:} = X_{k,:} W\n$$\n其中 $X_{k,:}$ 是 $X$ 的第 $k$ 行。这是一个将 $W \\in \\mathbb{R}^{d \\times C}$ 映射到 $Z_{k,:} \\in \\mathbb{R}^C$ 的仿射变换。\n\n凸函数的一个基本性质指出，一个凸函数与一个仿射映射的复合函数是凸的。由于 $L_k(Z_{k,:})$ 对 $Z_{k,:}$ 是凸的，且映射 $W \\mapsto Z_{k,:}$ 是仿射的，所以复合函数 $L_k(W) = L_k(X_{k,:}W)$ 对 $W$ 是凸的。\n由于 $f(W)$ 是这些凸函数的和，所以 $f(W)$ 本身对 $W$ 是凸的。\n\n### 3. 线性最小化预言机 (LMO)\n\nFrank-Wolfe 算法需要在每次迭代中求解一个线性最小化预言机 (LMO)：\n$$\nS_t = \\arg\\min_{S \\in \\mathcal{D}} \\langle S, G_t \\rangle\n$$\n其中 $G_t = \\nabla f(W_t)$ 是当前迭代的梯度，$\\langle \\cdot, \\cdot \\rangle$ 是弗罗贝尼乌斯内积。可行集是 $C$ 个标准 $d$ 维单纯形的笛卡尔积：$\\mathcal{D} = \\Delta^d \\times \\cdots \\times \\Delta^d$。\n\n弗罗贝尼乌斯内积可以写成对列的求和：\n$$\n\\langle S, G_t \\rangle = \\sum_{i=1}^d \\sum_{c=1}^C S_{ic} (G_t)_{ic} = \\sum_{c=1}^C \\langle S_{:,c}, (G_t)_{:,c} \\rangle_{\\mathbb{R}^d}\n$$\n其中 $S_{:,c}$ 和 $(G_t)_{:,c}$ 分别是 $S$ 和 $G_t$ 的第 $c$ 列。$S \\in \\mathcal{D}$ 的约束意味着每个列向量 $S_{:,c}$ 必须独立地位于单纯形 $\\Delta^d$ 中。\n因此，这个最小化问题可以分解为 $C$ 个独立的子问题，每个列向量一个：\n$$\n\\min_{S \\in \\mathcal{D}} \\sum_{c=1}^C \\langle S_{:,c}, (G_t)_{:,c} \\rangle = \\sum_{c=1}^C \\min_{s_c \\in \\Delta^d} \\langle s_c, (G_t)_{:,c} \\rangle\n$$\n对于每个类别 $c \\in \\{1, \\dots, C\\}$，我们必须求解以下线性规划问题：\n$$\n\\min_{s \\in \\Delta^d} \\langle s, g \\rangle \\quad \\text{其中 } g = (G_t)_{:,c} \\text{ 且 } \\Delta^d = \\left\\{u \\in \\mathbb{R}^d \\mid u \\ge 0, \\sum_{i=1}^d u_i = 1\\right\\}\n$$\n可行集 $\\Delta^d$ 是一个多胞体（一个紧凸集）。在一个多胞体上，线性函数的最小值总是在其某个顶点（极点）上达到。标准 $d$-单纯形 $\\Delta^d$ 的顶点是标准基向量 $e_1, e_2, \\dots, e_d \\in \\mathbb{R}^d$。\n\n为了找到最小值，我们在每个顶点上评估目标函数 $\\langle s, g \\rangle$：\n$$\n\\langle e_j, g \\rangle = \\sum_{i=1}^d (e_j)_i g_i = g_j\n$$\n最小值就是向量 $g$ 的最小分量。如果我们令 $j^* = \\arg\\min_{j \\in \\{1, \\dots, d\\}} g_j$，那么子问题的解就是 $s^* = e_{j^*}$。\n\n因此，为了构建完整的 LMO 解 $S_t \\in \\mathbb{R}^{d \\times C}$，我们对每一列 $c$ 执行以下步骤：\n1.  提取梯度的第 $c$ 列，$g_c = (G_t)_{:,c}$。\n2.  找到 $g_c$ 最小分量的索引 $j_c^*$。\n3.  将 $S_t$ 的第 $c$ 列设置为标准基向量 $e_{j_c^*}$。\n\n这就完全定义了指定可行集的 LMO。\n\n### 4. Frank-Wolfe 算法总结\n\n实现的 Frank-Wolfe 算法流程如下：\n1.  **初始化**：权重矩阵 $W_0$ 被初始化，使其每个列向量均为 $\\Delta^d$ 中的均匀向量，即对所有 $i,c$ 都有 $W_{0,i,c} = 1/d$。\n2.  **迭代**：对于 $t = 0, 1, 2, \\dots, T-1$：\n    a.  **梯度**：使用第 1 部分推导的公式计算梯度 $G_t = \\nabla f(W_t)$。\n    b.  **LMO**：使用第 3 部分的方法求解 $S_t = \\arg\\min_{S \\in \\mathcal{D}} \\langle S, G_t \\rangle$。\n    c.  **对偶间隙**：计算 FW 对偶间隙 $g_t = \\langle G_t, W_t - S_t \\rangle$。如果 $g_t \\le \\varepsilon$，则终止。\n    d.  **方向**：确定更新方向 $D_t = S_t - W_t$。\n    e.  **线搜索**：通过精确线搜索找到步长 $\\gamma_t \\in [0,1]$，该搜索最小化一维凸函数 $\\phi(\\gamma) = f(W_t + \\gamma D_t)$。这通过数值优化例程（`scipy.optimize.minimize_scalar`）实现。\n    f.  **更新**：更新权重：$W_{t+1} = W_t + \\gamma_t D_t$。\n最终输出基于算法终止后的状态。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Derives, designs, and implements the Frank-Wolfe method for\n    multinomial logistic regression with simplex constraints.\n    \"\"\"\n\n    # --- Test Cases ---\n\n    # Test A: LMO on a simple vector\n    c_A = np.array([1.5, -2.0, 0.0, 3.0, -1.0])\n    res_A = np.argmin(c_A)\n\n    # Test B: LMO with a tie\n    c_B = np.array([2.0, 2.0, 2.0])\n    # np.argmin deterministically returns the first index in case of a tie.\n    res_B = np.argmin(c_B)\n\n    # --- Frank-Wolfe Optimization Setup ---\n    \n    # Data and parameters for tests C, D, E\n    n, d, C = 6, 3, 3\n    X = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    # Labels are 1-based in the problem, convert to 0-based for indexing\n    y_1based = np.array([1, 2, 3, 1, 2, 3])\n    y_0based = y_1based - 1\n\n    # Create one-hot encoding for y\n    Y_one_hot = np.zeros((n, C))\n    Y_one_hot[np.arange(n), y_0based] = 1.0\n\n    # Algorithm settings\n    T_max = 200\n    epsilon = 1e-6\n\n    # --- Helper Functions ---\n\n    def compute_f_and_grad(W, X_data, Y_one_hot_data):\n        \"\"\"Computes the objective function value and its gradient.\"\"\"\n        n_samples = X_data.shape[0]\n        \n        # Logits\n        Z = X_data @ W\n        \n        # Numerically stable softmax\n        Z_max = Z.max(axis=1, keepdims=True)\n        exp_Z = np.exp(Z - Z_max)\n        P = exp_Z / exp_Z.sum(axis=1, keepdims=True)\n        \n        # Loss (cross-entropy)\n        # Using direct indexing for correctness\n        log_likelihood = -np.log(P[np.arange(n_samples), Y_one_hot_data.argmax(axis=1)] + 1e-12)\n        loss = np.mean(log_likelihood)\n        \n        # Gradient\n        grad = (1 / n_samples) * X_data.T @ (P - Y_one_hot_data)\n        \n        return loss, grad\n\n    def lmo_product_simplex(G):\n        \"\"\"Linear Minimization Oracle for the product of simplicies.\"\"\"\n        d_dim, C_dim = G.shape\n        S = np.zeros_like(G)\n        # For each column, find the index of the minimum element\n        indices = np.argmin(G, axis=0)\n        # Create a one-hot vector in each column of S at that index\n        S[indices, np.arange(C_dim)] = 1.0\n        return S\n\n    # --- Frank-Wolfe Algorithm Implementation ---\n\n    # 1. Initialization\n    W = np.full((d, C), 1.0 / d)\n    \n    f_initial, _ = compute_f_and_grad(W, X, Y_one_hot)\n    \n    final_gap = np.inf\n    final_f = f_initial\n    \n    for t in range(T_max):\n        # 2. Compute gradient\n        f_t, G_t = compute_f_and_grad(W, X, Y_one_hot)\n\n        # 3. Call LMO to find the FW vertex\n        S_t = lmo_product_simplex(G_t)\n        \n        # 4. Compute Frank-Wolfe duality gap and check for convergence\n        gap = np.sum((W - S_t) * G_t)\n        final_gap = gap\n        \n        if gap = epsilon:\n            break\n            \n        # 5. Determine FW direction\n        D_t = S_t - W\n        \n        # 6. Exact line search for step size gamma\n        def phi(gamma, W_curr, D_curr, X_data, Y_one_hot_data):\n            f_val, _ = compute_f_and_grad(W_curr + gamma * D_curr, X_data, Y_one_hot_data)\n            return f_val\n        \n        res = minimize_scalar(\n            phi, \n            bounds=(0, 1), \n            method='bounded', \n            args=(W, D_t, X, Y_one_hot)\n        )\n        gamma_t = res.x\n        \n        # 7. Update weights\n        W = W + gamma_t * D_t\n\n    # After loop, compute final objective value\n    final_f, _ = compute_f_and_grad(W, X, Y_one_hot)\n    \n    # --- Collect and Format Results ---\n\n    # Test C: Is the final duality gap below the threshold?\n    res_C = final_gap = 5e-4\n    \n    # Test D: What is the final objective value?\n    res_D = round(final_f, 6)\n    \n    # Test E: Did the objective value decrease from the initial value?\n    res_E = final_f  f_initial\n    \n    # Final print statement in the exact required format.\n    # Order: [Test A integer, Test C boolean, Test B integer, Test D float, Test E boolean]\n    print(f\"[{res_A},{res_C},{res_B},{res_D},{res_E}]\")\n\nsolve()\n```", "id": "3126940"}, {"introduction": "有时候，直接求解一个优化问题（即“原始问题”）可能非常困难。对偶理论为我们提供了另一种视角，它将原始问题转化为一个“对偶问题”，后者可能在结构上更简单、更易于分析或求解。本练习将带你深入探索这一强大工具，推导弹性网络 (elastic net) 正则化问题的 Fenchel 对偶形式 [@problem_id:3126974]。在这个过程中，你将发现一个深刻而优美的关系：原始问题中的强凸性 (strong convexity) 对应于其对偶问题中的光滑性 (smoothness)，这一洞察对于设计和理解高级优化算法至关重要。", "problem": "考虑在深度学习流程中使用弹性网络正则化器训练一个线性预测器。令 $A \\in \\mathbb{R}^{n \\times d}$ 为数据矩阵，$b \\in \\mathbb{R}^{n}$ 为目标向量，且令 $\\lambda_{1}  0$ 和 $\\lambda_{2}  0$ 为固定标量。弹性网络正则化的最小二乘原问题是\n$$\n\\min_{x \\in \\mathbb{R}^{d}} \\; \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda_{1} \\|x\\|_{1} + \\frac{\\lambda_{2}}{2} \\|x\\|_{2}^{2}.\n$$\n从凸分析和优化的第一性原理出发，推导该问题的Fenchel对偶问题，其形式需明确地用 $A$、$b$、$\\lambda_{1}$ 和 $\\lambda_{2}$ 表示。然后，利用强凸性的定义和凸共轭的性质，确定原问题中由 $\\frac{\\lambda_{2}}{2} \\|x\\|_{2}^{2}$ 项所赋予的强凸性如何转化为对偶目标函数的光滑性。具体来说，计算对偶目标函数梯度的利普希茨常数，其为一个包含谱范数 $\\|A\\|_{2}$ 和 $\\lambda_{2}$ 的闭式表达式。\n\n将您的最终答案表示为一个单行矩阵，其第一个元素是显式的对偶目标函数（作为对偶变量的闭式解析表达式），第二个元素是其梯度的相应利普希茨常数。无需四舍五入，也不涉及单位。", "solution": "原问题由下式给出：\n$$\nP_0 = \\min_{x \\in \\mathbb{R}^{d}} \\; \\frac{1}{2} \\|A x - b\\|_{2}^{2} + \\lambda_{1} \\|x\\|_{1} + \\frac{\\lambda_{2}}{2} \\|x\\|_{2}^{2}\n$$\n其中 $A \\in \\mathbb{R}^{n \\times d}$，$b \\in \\mathbb{R}^{n}$，且 $\\lambda_{1}, \\lambda_{2}  0$。\n\n为了推导Fenchel对偶，我们将问题构造为 $\\min_{x} f(Kx) + g(x)$ 的形式，其中 $K$ 是一个线性算子。\n我们定义：\n- $f(z) = \\frac{1}{2} \\|z - b\\|_{2}^{2}$，其中 $z \\in \\mathbb{R}^n$。\n- $g(x) = \\lambda_{1} \\|x\\|_{1} + \\frac{\\lambda_{2}}{2} \\|x\\|_{2}^{2}$，其中 $x \\in \\mathbb{R}^d$。\n- 线性算子为 $K = A$。\n\n原问题可以写成：\n$$\nP_0 = \\min_{x \\in \\mathbb{R}^{d}} f(Ax) + g(x)\n$$\n根据Fenchel-Rockafellar对偶定理，对偶问题由下式给出：\n$$\nD_0 = \\max_{y \\in \\mathbb{R}^{n}} -f^*(y) - g^*(-A^T y)\n$$\n其中 $f^*$ 和 $g^*$ 分别是 $f$ 和 $g$ 的凸共轭（或Fenchel共轭）。对偶变量为 $y \\in \\mathbb{R}^n$。为了找到对偶问题的显式形式，我们必须计算 $f^*$ 和 $g^*$。\n\n首先，我们计算 $f(z)$ 的共轭：\n$$\nf^*(y) = \\sup_{z \\in \\mathbb{R}^n} \\{ y^T z - f(z) \\} = \\sup_{z \\in \\mathbb{R}^n} \\left\\{ y^T z - \\frac{1}{2} \\|z - b\\|_{2}^{2} \\right\\}\n$$\n上确界内的表达式是关于 $z$ 的凹二次函数。当其关于 $z$ 的梯度为零时，达到最大值：\n$$\n\\nabla_z \\left( y^T z - \\frac{1}{2} (z - b)^T(z - b) \\right) = y - (z - b) = 0\n$$\n这给出了最优解 $z^* = y + b$。将此代回表达式中：\n$$\nf^*(y) = y^T(y+b) - \\frac{1}{2} \\|(y+b) - b\\|_{2}^{2} = y^T y + y^T b - \\frac{1}{2} \\|y\\|_{2}^{2} = \\frac{1}{2} \\|y\\|_{2}^{2} + b^T y\n$$\n\n接下来，我们计算 $g(x)$ 的共轭。函数 $g(x)$ 是两个函数的和：$g_1(x) = \\lambda_1 \\|x\\|_1$ 和 $g_2(x) = \\frac{\\lambda_2}{2} \\|x\\|_2^2$。和的共轭是它们各自共轭的下确界卷积：\n$$\ng^*(v) = (g_1 + g_2)^*(v) = \\inf_{w \\in \\mathbb{R}^d} \\{ g_1^*(w) + g_2^*(v-w) \\}\n$$\n我们来计算 $g_1^*$ 和 $g_2^*$：\n- $g_1^*(w) = (\\lambda_1 \\|\\cdot\\|_1)^*(w)$。缩放范数 $\\lambda_1 \\|\\cdot\\|_1$ 的共轭是对偶范数球的示性函数。$\\ell_1$-范数的对偶范数是 $\\ell_\\infty$-范数。因此：\n$$\ng_1^*(w) = I_{\\{w' : \\|w'\\|_\\infty \\le \\lambda_1\\}}(w) = \\begin{cases} 0  \\text{如果 } \\|w\\|_\\infty \\le \\lambda_1 \\\\ +\\infty  \\text{否则} \\end{cases}\n$$\n- $g_2^*(u) = (\\frac{\\lambda_2}{2} \\|\\cdot\\|_2^2)^*(u)$。这是一个缩放二次函数的共轭。对于一般的二次函数 $\\frac{\\gamma}{2}\\|x\\|_2^2$，其共轭为 $\\frac{1}{2\\gamma}\\|u\\|_2^2$。这里，$\\gamma = \\lambda_2$，所以：\n$$\ng_2^*(u) = \\frac{1}{2\\lambda_2}\\|u\\|_2^2\n$$\n现在我们计算 $g^*(v)$ 的下确界卷积：\n$$\ng^*(v) = \\inf_{\\|w\\|_\\infty \\le \\lambda_1} \\left\\{ 0 + \\frac{1}{2\\lambda_2} \\|v - w\\|_{2}^{2} \\right\\}\n$$\n这等价于求从 $v$ 到半径为 $\\lambda_1$ 的 $\\ell_\\infty$-球的欧几里得距离的平方，再按 $\\frac{1}{2\\lambda_2}$ 进行缩放。最小化子 $w^*$ 是 $v$ 在该球上的投影，$w^* = \\text{proj}_{B_{\\infty}(\\lambda_1)}(v)$。到 $\\ell_\\infty$-球上的投影是一个分量级操作：$(w^*)_i = \\text{sign}(v_i)\\min(|v_i|, \\lambda_1)$。\n我们来分析项 $v - w^* = v - \\text{proj}_{B_{\\infty}(\\lambda_1)}(v)$。其第 $i$ 个分量是：\n$$\n(v - w^*)_i = v_i - \\text{sign}(v_i)\\min(|v_i|, \\lambda_1) = \\begin{cases} 0  \\text{如果 } |v_i| \\le \\lambda_1 \\\\ v_i - \\text{sign}(v_i)\\lambda_1 = \\text{sign}(v_i)(|v_i|-\\lambda_1)  \\text{如果 } |v_i|  \\lambda_1 \\end{cases}\n$$\n该分量的绝对值为 $\\max\\{0, |v_i| - \\lambda_1\\}$。因此，其范数的平方是：\n$$\n\\|v - \\text{proj}_{B_{\\infty}(\\lambda_1)}(v)\\|_2^2 = \\sum_{i=1}^d (\\max\\{0, |v_i| - \\lambda_1\\})^2\n$$\n我们定义向量软阈值算子 $S_{\\lambda_1}(v)$，其分量为 $(S_{\\lambda_1}(v))_i = \\text{sign}(v_i)\\max\\{0, |v_i| - \\lambda_1\\}$。注意 $(v - w^*)_i = (S_{\\lambda_1}(v))_i$。因此，$\\|v - \\text{proj}_{B_{\\infty}(\\lambda_1)}(v)\\|_2^2 = \\|S_{\\lambda_1}(v)\\|_2^2$。于是共轭函数为：\n$$\ng^*(v) = \\frac{1}{2\\lambda_2}\\|S_{\\lambda_1}(v)\\|_2^2 = \\frac{1}{2\\lambda_2} \\sum_{i=1}^d (\\max\\{0, |v_i| - \\lambda_1\\})^2\n$$\n我们需要计算 $g^*(-A^T y)$。注意 $|(-A^T y)_i| = |(A^T y)_i|$，所以：\n$$\ng^*(-A^T y) = \\frac{1}{2\\lambda_2} \\sum_{i=1}^d (\\max\\{0, |(-A^T y)_i| - \\lambda_1\\})^2 = \\frac{1}{2\\lambda_2} \\sum_{i=1}^d (\\max\\{0, |(A^T y)_i| - \\lambda_1\\})^2\n$$\n将 $f^*$ 和 $g^*$ 代入对偶问题表达式 $D_0 = \\max_{y} -f^*(y) - g^*(-A^T y)$，需要最大化的对偶目标函数是：\n$$\nD(y) = -\\left(\\frac{1}{2} \\|y\\|_{2}^{2} + b^T y\\right) - \\frac{1}{2\\lambda_2} \\sum_{i=1}^d (\\max\\{0, |(A^T y)_i| - \\lambda_1\\})^2\n$$\n通常将对偶问题表述为一个最小化问题，即最小化 $-D(y)$。需要最小化的对偶目标函数是：\n$$\nD_{min}(y) = \\frac{1}{2} \\|y\\|_{2}^{2} + b^T y + \\frac{1}{2\\lambda_2} \\sum_{i=1}^d (\\max\\{0, |(A^T y)_i| - \\lambda_1\\})^2\n$$\n这是答案的第一部分。\n\n对于第二部分，我们需要求对偶目标函数 $D_{min}(y)$ 梯度的利普希茨常数。这个性质被称为光滑性。如果一个可微函数 $\\phi$ 的梯度 $\\nabla \\phi$ 是 $L$-利普希茨连续的，那么该函数是 $L$-光滑的。\n原目标函数的一部分 $g(x) = \\lambda_1 \\|x\\|_1 + \\frac{\\lambda_2}{2}\\|x\\|_2^2$ 是 $\\lambda_2$-强凸的，这是因为项 $\\frac{\\lambda_2}{2}\\|x\\|_2^2$ 的存在（因为 $\\lambda_2  0$）。\n凸分析中的一个基本结果指出，一个函数 $\\psi$ 是 $\\mu$-强凸的，当且仅当其共轭函数 $\\psi^*$ 是 $1/\\mu$-光滑的。\n将此应用于 $g(x)$，由于它是 $\\lambda_2$-强凸的，其共轭函数 $g^*(v)$ 必须是 $1/\\lambda_2$-光滑的。这意味着 $\\nabla g^*(v)$ 是 $1/\\lambda_2$-利普希茨连续的。\n\n需要最小化的对偶目标函数是 $D_{min}(y) = f^*(y) + h(y)$，其中 $h(y) = g^*(-A^T y)$。\n对偶目标函数的梯度是 $\\nabla D_{min}(y) = \\nabla f^*(y) + \\nabla h(y)$。\n- $f^*(y) = \\frac{1}{2}\\|y\\|_2^2 + b^T y$ 的梯度是 $\\nabla f^*(y) = y + b$。其Hessian矩阵是单位矩阵 $I$。这个梯度是 $1$-利普希茨的：$\\|\\nabla f^*(y_1) - \\nabla f^*(y_2)\\|_2 = \\|(y_1+b)-(y_2+b)\\|_2 = \\|y_1 - y_2\\|_2$。\n- 对于 $h(y) = g^*(-A^T y)$，我们使用链式法则：$\\nabla h(y) = -A \\nabla g^*(-A^T y)$。我们需要求出 $\\nabla h(y)$ 的利普希茨常数。\n我们考虑两点 $y_1, y_2$：\n$$\n\\|\\nabla h(y_1) - \\nabla h(y_2)\\|_2 = \\|(-A \\nabla g^*(-A^T y_1)) - (-A \\nabla g^*(-A^T y_2))\\|_2 = \\|A (\\nabla g^*(-A^T y_2) - \\nabla g^*(-A^T y_1))\\|_2\n$$\n使用谱范数 $\\|A\\|_2$ 的定义以及 $\\nabla g^*$ 是 $1/\\lambda_2$-利普希茨连续的事实：\n\\begin{align*}\n\\|\\nabla h(y_1) - \\nabla h(y_2)\\|_2 \\le \\|A\\|_2 \\|\\nabla g^*(-A^T y_2) - \\nabla g^*(-A^T y_1)\\|_2 \\\\\n\\le \\|A\\|_2 \\left( \\frac{1}{\\lambda_2} \\|(-A^T y_2) - (-A^T y_1)\\|_2 \\right) \\\\\n= \\frac{\\|A\\|_2}{\\lambda_2} \\|A^T (y_1 - y_2)\\|_2 \\\\\n\\le \\frac{\\|A\\|_2}{\\lambda_2} \\|A^T\\|_2 \\|y_1 - y_2\\|_2\n\\end{align*}\n由于 $\\|A^T\\|_2 = \\|A\\|_2$，$\\nabla h(y)$ 的利普希茨常数是 $\\frac{\\|A\\|_2^2}{\\lambda_2}$。\n\n现在，对于整个对偶目标函数的梯度 $\\nabla D_{min}(y) = \\nabla f^*(y) + \\nabla h(y)$，我们使用三角不等式：\n$$\n\\|\\nabla D_{min}(y_1) - \\nabla D_{min}(y_2)\\|_2 \\le \\|\\nabla f^*(y_1) - \\nabla f^*(y_2)\\|_2 + \\|\\nabla h(y_1) - \\nabla h(y_2)\\|_2\n$$\n代入各自的利普希茨常数：\n$$\n\\|\\nabla D_{min}(y_1) - \\nabla D_{min}(y_2)\\|_2 \\le \\left(1 \\cdot \\|y_1 - y_2\\|_2\\right) + \\left(\\frac{\\|A\\|_2^2}{\\lambda_2} \\|y_1 - y_2\\|_2\\right) = \\left(1 + \\frac{\\|A\\|_2^2}{\\lambda_2}\\right) \\|y_1 - y_2\\|_2\n$$\n因此，对偶目标函数梯度的利普希茨常数是 $L = 1 + \\frac{\\|A\\|_2^2}{\\lambda_2}$。\n\n最终答案包括需要最小化的对偶目标函数及其梯度的利普希茨常数。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} \\|y\\|_{2}^{2} + b^T y + \\frac{1}{2\\lambda_2} \\sum_{i=1}^d \\left(\\max\\{0, |(A^T y)_i| - \\lambda_1\\}\\right)^2  1 + \\frac{\\|A\\|_{2}^{2}}{\\lambda_2}\n\\end{pmatrix}\n}\n$$", "id": "3126974"}]}