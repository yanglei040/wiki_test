## 引言
尽管[深度学习模型](@entry_id:635298)在结构上看似是确定的，但其行为和性能的根基却深植于概率论与统计学的沃土之中。从数据抽样的随机性，到[权重初始化](@entry_id:636952)的不确定性，再到优化过程的随机探索，随机性无处不在。然而，许多从业者常常将这些现象视为孤立的“技巧”或难以捉摸的“玄学”，缺乏一个统一的理论框架来理解其背后的深刻联系。本文旨在填补这一认知空白，通过将[深度学习](@entry_id:142022)中的核心元素——数据、权重、激活乃至损失函数——统一在**[随机变量](@entry_id:195330)**的视角下，揭示其内在的随机机制。

通过本文的学习，你将掌握一套强大的分析工具，从而能够从第一性原理出发理解深度学习的运作方式。
*   在**原理与机制**一章中，我们将探讨如何将训练循环中的损失和梯度量化为[随机变量](@entry_id:195330)，分析其均值与[方差](@entry_id:200758)如何影响优化过程。我们还将深入网络内部，揭示信号[方差](@entry_id:200758)的传播规律如何决定了有效的[权重初始化](@entry_id:636952)策略和归一化技术，并阐述[泛化理论](@entry_id:635655)是如何通过[概率不等式](@entry_id:202750)来界定模型学习能力的边界。
*   接着，在**应用与交叉学科联系**一章中，我们将展示这些基本原理如何在优化算法（如Adam）、[生成模型](@entry_id:177561)（如VAE）、前沿架构（如Transformer）以及[强化学习](@entry_id:141144)和可信AI等多个领域中得到具体应用和扩展，凸显其作为连接不同学科的桥梁作用。
*   最后，**动手实践**部分将提供精选的编程练习，让你亲手实现和验证这些理论概念，将抽象的数学原理转化为可操作的工程洞见。

让我们一同开启这段旅程，用概率的语言重新解读[深度学习](@entry_id:142022)，洞悉其表象之下的随机之美。

## 原理与机制

在[深度学习](@entry_id:142022)的实践中，我们经常与看似确定的算法和架构打交道。然而，支撑其有效性的核心原理在很大程度上植根于概率论和统计学。通过将[深度学习](@entry_id:142022)中的关键元素——如数据、权重、激活和损失——视为[随机变量](@entry_id:195330)，我们可以获得深刻的洞见，从而能够系统地分析和设计模型。本章将深入探讨这些基本原理，阐明深度学习模型训练、设计和泛化能力背后的随机机制。

### 训练循环中的[随机变量](@entry_id:195330)

[深度学习](@entry_id:142022)的训练过程本质上是一个[随机过程](@entry_id:159502)。无论是数据的随机抽样，还是权重的随机初始化，随机性无处不在。将这些[随机过程](@entry_id:159502)中的核心量化为[随机变量](@entry_id:195330)，是理解其行为的第一步。

#### 损失作为[随机变量](@entry_id:195330)

在监督学习中，我们的目标是学习一个从输入 $X$ 到输出 $Y$ 的映射。输入和输出本身是从某个未知的联[合数](@entry_id:263553)据生成[分布](@entry_id:182848) $\mathcal{D}$ 中抽取的[随机变量](@entry_id:195330)。对于一个由参数 $\theta$ 确定的模型 $f_\theta$，其在单个数据点 $(X, Y)$ 上的性能由损失函数 $\ell(f_\theta(X), Y)$ 来衡量。由于 $(X, Y)$ 是随机的，因此单一样本的损失 $Z = \ell(f_\theta(X), Y)$ 本身也是一个[随机变量](@entry_id:195330)。

训练的最终目标是找到一组参数 $\theta$，以最小化**总体风险**（population risk），即在整个数据[分布](@entry_id:182848)上的期望损失：
$$
L = \mathbb{E}_{(X, Y) \sim \mathcal{D}}[Z] = \mathbb{E}[\ell(f_\theta(X), Y)]
$$
由于我们无法访问整个[分布](@entry_id:182848) $\mathcal{D}$，我们转而使用从该[分布](@entry_id:182848)中独立同分布（i.i.d.）抽样得到的一个有限训练集。在[随机梯度下降](@entry_id:139134)（SGD）中，我们甚至只在每一步使用一小批（mini-batch）数据。这一小批数据的平均损失，即**[经验风险](@entry_id:633993)**（empirical risk），是对总体风险的一个估计量。给定一个大小为 $m$ 的小批量，损失估计量 $\hat{L}_m$ 是 $m$ 个独立的单一样本损失 $Z_1, \dots, Z_m$ 的样本均值：
$$
\hat{L}_m = \frac{1}{m}\sum_{i=1}^{m} Z_i
$$
根据中心极限定理，这个估计量是总体风险 $L$ 的一个[无偏估计](@entry_id:756289)，即 $\mathbb{E}[\hat{L}_m] = L$。其[方差](@entry_id:200758)为 $\mathrm{Var}(\hat{L}_m) = \frac{\sigma^2}{m}$，其中 $\sigma^2 = \mathrm{Var}(Z)$ 是单个样本损失的[方差](@entry_id:200758)。这个简单的 $1/m$ 关系表明，增加[批量大小](@entry_id:174288)可以降低损失估计的噪声，这是[大批量训练](@entry_id:636067)稳定性的一个基本原因。

然而，在实践中，小批量内的样本损失可能并非完全独立。例如，当使用共享的[数据增强](@entry_id:266029)或[预处理](@entry_id:141204)策略时，样本之间会引入相关性。假设批量中的任意两个不同样本的损失 $Z_i$ 和 $Z_j$ 之间存在一个共同的相关系数 $\rho$。在这种情况下，小批量损失[估计量的方差](@entry_id:167223)变为 [@problem_id:3166676]：
$$
\mathrm{Var}(\hat{L}_m) = \frac{\sigma^2}{m} (1 + (m-1)\rho)
$$
这个表达式揭示了一个重要的现象：当 $\rho > 0$ 时，[方差](@entry_id:200758)的减小速度会慢于 $1/m$。在极限情况下，如果样本完全相关（$\rho \to 1$），[方差](@entry_id:200758)将接近 $\sigma^2$ 而不随 $m$ 减小。这从数学上解释了为什么当[数据增强](@entry_id:266029)引入了强烈的批内相关性时，仅仅增加[批量大小](@entry_id:174288)可能无法有效降低训练噪声。

#### 随机梯度及其噪声

类似地，随机梯度本身也是一个[随机变量](@entry_id:195330)。在第 $t$ 次迭代中，基于小批量的随机梯度 $g_t$ 可以被建模为真实梯度（在整个数据集上的梯度）$\nabla L(\theta_t)$ 与一个零均值噪声项 $\xi_t$ 的和：
$$
g_t = \nabla L(\theta_t) + \xi_t
$$
这个噪声 $\xi_t$ 是由于我们用一个小批量代替整个数据集而产生的。分析这个噪声的特性对于理解 SGD 的动态至关重要。与通常的假设不同，标准的 SGD 算法从大小为 $N$ 的有限数据集中进行**无放回**抽样。

考虑梯度向量的第 $k$ 个坐标。我们可以推导出在这种[无放回抽样](@entry_id:276879)下，[梯度噪声](@entry_id:165895)[方差](@entry_id:200758)的精确表达式 [@problem_id:3166780]：
$$
\mathrm{Var}(\xi_{t,k}) = S_k^2 \left( \frac{1}{B} - \frac{1}{N} \right)
$$
其中 $B$ 是小[批量大小](@entry_id:174288)，$N$ 是总数据集大小，$S_k^2$ 是数据集中所有样本的第 $k$ 个梯度坐标的（样本）[方差](@entry_id:200758)。这个公式非常富有启发性。首先，它包含一个**有限总体修正因子**（finite population correction）。当 $B \ll N$ 时，[方差近似](@entry_id:268585)为 $S_k^2/B$，这与[有放回抽样](@entry_id:274194)的情况类似。然而，当 $B$ 接近 $N$ 时，[方差](@entry_id:200758)会减小，并且当 $B=N$ 时，[方差](@entry_id:200758)为零，因为此时随机梯度就是真实梯度，没有噪声。

此外，根据适用于有限总体抽样的[中心极限定理](@entry_id:143108)，当[批量大小](@entry_id:174288) $B$ 足够大且采样比例 $B/N$ 足够小时，[梯度噪声](@entry_id:165895)向量 $\xi_t$ 的[分布](@entry_id:182848)可以近似为多元[高斯分布](@entry_id:154414) [@problem_id:3166780]。这个[高斯近似](@entry_id:636047)是许多关于 SGD 动态理论分析的出发点。

### [信号传播](@entry_id:165148)与网络设计

将网络的权重和激活值视为[随机变量](@entry_id:195330)，使我们能够分析信号（即信息）在网络中逐层传播时的统计特性。这种分析是现代[深度神经网络](@entry_id:636170)设计的理论基石，尤其是在[权重初始化](@entry_id:636952)和网络架构规范化方面。

#### [方差](@entry_id:200758)传播与[权重初始化](@entry_id:636952)

一个未经良好初始化的深度网络在训练时很容易遇到梯度消失或[梯度爆炸](@entry_id:635825)的问题。这两种现象的根本原因在于，随着信号在层间传播，激活值的[方差](@entry_id:200758)（可以看作信号的“强度”）要么指数级衰减至零，要么指数级增长至溢出。理想情况下，我们希望信号的[方差](@entry_id:200758)在各层之间保持大致恒定。

让我们来分析一个[全连接层](@entry_id:634348)。其对于第 $i$ 个神经元的预激活值（pre-activation）为 $z_i = \sum_{j=1}^{n} w_{ij} a_j$，其中 $a_j$ 是来自前一层的 $n$ 个激活输出（fan-in），$w_{ij}$ 是权重。假设输入激活 $a_j$ 和权重 $w_{ij}$ 都是零均值的[独立随机变量](@entry_id:273896)。那么，$z_i$ 的[方差](@entry_id:200758)为 [@problem_id:3166688]：
$$
\mathrm{Var}(z_i) = \sum_{j=1}^n \mathrm{Var}(w_{ij} a_j) = \sum_{j=1}^n \mathbb{E}[w_{ij}^2]\mathbb{E}[a_j^2] = n \cdot \mathrm{Var}(w_{ij}) \cdot \mathrm{Var}(a_j)
$$
（此处利用了 $\mathbb{E}[w]=0$ 和 $\mathbb{E}[a]=0$）

为了使输出激活的[方差](@entry_id:200758)与输入激活的[方差保持](@entry_id:634352)一致，即 $\mathrm{Var}(a_i') \approx \mathrm{Var}(a_j)$，我们需要仔细选择权重的[方差](@entry_id:200758) $\mathrm{Var}(w_{ij})$。当网络层很宽（即 $n$ 很大）时，根据中心极限定理，$z_i$ 作为大量[独立同分布随机变量](@entry_id:270381)的加权和，其[分布](@entry_id:182848)会趋近于[高斯分布](@entry_id:154414) [@problem_id:3166773]。这为我们分析[非线性激活函数](@entry_id:635291)（如ReLU）提供了便利。

对于 ReLU [激活函数](@entry_id:141784) $a_i' = \max\{0, z_i\}$，如果 $z_i$ 是一个均值为 0，[方差](@entry_id:200758)为 $q=\mathrm{Var}(z_i)$ 的[高斯变量](@entry_id:276673)，我们可以精确计算出输出激活的二阶矩 [@problem_id:3166688]：
$$
\mathbb{E}[(a_i')^2] = \frac{1}{2} \mathrm{Var}(z_i)
$$
在实践中，为了简化设计，通常用二阶矩 $\mathbb{E}[(a_i')^2]$ 来近似[方差](@entry_id:200758) $\mathrm{Var}(a_i')$。为了保持信号强度，我们设定 $\mathbb{E}[(a_i')^2] \approx \mathrm{Var}(a_j)$。结合前面的[方差](@entry_id:200758)传播公式，我们得到：
$$
\mathrm{Var}(a_j) \approx \frac{1}{2} \mathrm{Var}(z_i) = \frac{1}{2} n \cdot \mathrm{Var}(w_{ij}) \cdot \mathrm{Var}(a_j)
$$
这导出了著名的 **He 初始化** 条件 [@problem_id:3166688]：
$$
\mathrm{Var}(w_{ij}) = \frac{2}{n}
$$
这个简单的结果深刻地展示了如何通过概率论的视角，从第一性原理出发，推导出保障深度网络可训练性的关键设计准则。权重[方差](@entry_id:200758)需要与网络层的输入维度（fan-in）成反比，以抵消求和运算带来的[方差](@entry_id:200758)累积效应。

#### [归一化层](@entry_id:636850)中的[统计估计](@entry_id:270031)

另一种维持信号稳定性的强大技术是**[批量归一化](@entry_id:634986)（Batch Normalization, BN）**。BN 在每个训练步骤中，对一个小批量内的预激活值进行标准化，使其具有零均值和单位[方差](@entry_id:200758)。具体来说，它使用该小批量的均值 $\bar{X}$ 和[方差](@entry_id:200758) $\hat{\sigma}^2$ 来估计整个数据[分布](@entry_id:182848)上该激活通道的真实均值 $\mu$ 和[方差](@entry_id:200758) $\sigma^2$。

$\bar{X} = \frac{1}{m}\sum_{i=1}^{m} X_i$ 本身是一个[随机变量](@entry_id:195330)，其随机性来自小批量的抽样。中心极限定理（CLT）为 BN 的有效性提供了理论解释。根据 CLT，当[批量大小](@entry_id:174288) $m$ 增大时，样本均值 $\bar{X}$ 的[分布](@entry_id:182848)会向一个以真实均值 $\mu$ 为中心、[方差](@entry_id:200758)为 $\sigma^2/m$ 的高斯分布收敛。这意味着 $\bar{X}$ 会集中在 $\mu$ 附近。

更精确地说，对于任意小的正数 $\epsilon$，$\bar{X}$ 落在 $\mu$ 的 $\epsilon$-邻域内的概率可以通过高斯累积分布函数 $\Phi$ 来近似 [@problem_id:3166692]：
$$
\mathbb{P}(|\bar{X} - \mu| \le \epsilon) \approx 2\Phi\left(\frac{\epsilon\sqrt{m}}{\sigma}\right) - 1
$$
这个概率随着 $\sqrt{m}$ 的增长而迅速趋近于 1。这表明，只要[批量大小](@entry_id:174288) $m$ 合理，BN 所使用的批量统计量就是对全局统计量的可靠估计，从而能够有效地稳定激活值的[分布](@entry_id:182848)，加速[模型收敛](@entry_id:634433)。

### 泛化与不确定性

[深度学习模型](@entry_id:635298)为何能泛化到未见过的数据上？我们如何量化模型预测的不确定性？[随机变量](@entry_id:195330)的语言为回答这些深刻的问题提供了强大的理论框架。

#### 从[集中不等式](@entry_id:273366)到[泛化界](@entry_id:637175)

一个模型**泛化**得好，意味着它在[训练集](@entry_id:636396)上的表现（[经验风险](@entry_id:633993) $\hat{L}_n$）与在整个数据[分布](@entry_id:182848)上的表现（总体风险 $L$）相近。它们的差值 $|\hat{L}_n - L|$ 被称为**[泛化误差](@entry_id:637724)**（generalization error）。由于[训练集](@entry_id:636396)是随机抽样的，[经验风险](@entry_id:633993) $\hat{L}_n$ 是一个[随机变量](@entry_id:195330)，而[泛化误差](@entry_id:637724)也是一个随机量。

**[集中不等式](@entry_id:273366)**（Concentration inequalities）是界定一个[随机变量](@entry_id:195330)（如 $\hat{L}_n$）偏离其期望（如 $L$）的概率的数学工具。一个强大的例子是**Bernstein 不等式**。假设[损失函数](@entry_id:634569)的值有界，即 $0 \le \ell(\cdot) \le B$，且其[方差](@entry_id:200758)为 $\sigma^2$。Bernstein 不等式给出了[泛化误差](@entry_id:637724)超过某个值 $\epsilon$ 的概率的一个[上界](@entry_id:274738) [@problem_id:3166697]：
$$
\mathbb{P}(|\hat{L}_n - L| \ge \epsilon) \le 2\exp\left(-\frac{n\epsilon^2}{2\sigma^2 + \frac{2}{3}B\epsilon}\right)
$$
这个界限告诉我们，随着样本量 $n$ 的增加，模型在[训练集](@entry_id:636396)上的表现以指数级的速度收敛到其在真实[分布](@entry_id:182848)上的表现。这个界限同时考虑了损失的[方差](@entry_id:200758) $\sigma^2$ 和它的绝对边界 $B$，比只考虑边界的 Hoeffding 不等式更为精细。[方差](@entry_id:200758)越小，收敛越快。

#### 度量[模型复杂度](@entry_id:145563)：[Rademacher 复杂度](@entry_id:634858)

上述界限只针对一个**固定**的模型。然而，在训练过程中，我们是在一个巨大的函数类别 $\mathcal{F}$（由[神经网络架构](@entry_id:637524)定义）中搜索最佳模型。我们需要一个能够同时对类别中**所有**函数都成立的**[一致收敛](@entry_id:146084)**界限。

**[Rademacher 复杂度](@entry_id:634858)**是衡量一个函数类别 $\mathcal{F}$ “丰富度”或“复杂度”的强大工具。其核心思想是，一个复杂的函数类别能够很好地拟合纯粹的随机噪声。经验 [Rademacher 复杂度](@entry_id:634858) $\hat{\mathfrak{R}}_n(\mathcal{G})$ 定义为损失函数类别 $\mathcal{G}$ 在给定数据集上与随机 Rademacher 变量（即在 $\{-1, 1\}$ 中均匀取值的[随机变量](@entry_id:195330) $\sigma_i$）的最大相关性 [@problem_id:3166736]：
$$
\hat{\mathfrak{R}}_n(\mathcal{G};S) = \mathbb{E}_{\sigma}\left[\sup_{g\in\mathcal{G}}\frac{1}{n}\sum_{i=1}^n \sigma_i g(x_i,y_i)\right]
$$
一个函数类别的 [Rademacher 复杂度](@entry_id:634858)越高，它就越有可能“记住”训练数据中的噪声，从而导致过拟合。基于 [Rademacher 复杂度](@entry_id:634858)的标准[泛化理论](@entry_id:635655)给出了一个关于一致[泛化误差](@entry_id:637724)的[上界](@entry_id:274738)。以高概率（至少 $1-\delta$）成立的界限形式如下 [@problem_id:3166736]：
$$
\sup_{f\in\mathcal{F}}\Big(L(f)-\hat{L}_S(f)\Big) \le 2\hat{\mathfrak{R}}_n(\mathcal{G};S) + C(\delta, n)
$$
其中 $C(\delta, n)$ 是一个只依赖于[置信度](@entry_id:267904) $\delta$ 和样本量 $n$ 的小项。这个不等式清晰地表明，模型的[泛化差距](@entry_id:636743)受到其所属函数类别的复杂度的控制。

#### 贝叶斯视角：PAC-Bayes 框架

PAC-Bayes 理论提供了另一种理解泛化的深刻视角。它不关注单个最优模型，而是考虑一个在模型[参数空间](@entry_id:178581)上的**后验分布** $Q$。这个后验分布是在看到了训练数据 $S$ 之后，从一个数据无关的**[先验分布](@entry_id:141376)** $P$ 更新而来的。最终的分类器是一个[随机过程](@entry_id:159502)：首先从后验 $Q$ 中采样一组权重 $W \sim Q$，然后用模型 $f_W$ 进行预测。

该框架的核心目标是为这个随机分类器的期望总体风险 $\mathbb{E}_{W \sim Q}[R_{\mathcal{D}}(f_W)]$ 提供一个高概率[上界](@entry_id:274738) [@problem_id:3166750]。其[泛化界](@entry_id:637175)的形式通常如下：
$$
\mathbb{E}_{W \sim Q}[R_{\mathcal{D}}(f_W)] \le \mathbb{E}_{W \sim Q}[R_S(f_W)] + \sqrt{\frac{\mathrm{KL}(Q \| P) + \ln(n/\delta)}{2n}}
$$
这个界限有几个关键组成部分 [@problem_id:3166750]：
1.  **KL 散度作为复杂度惩罚**：$\mathrm{KL}(Q \| P)$ 度量了后验 $Q$ 相对于先验 $P$ 的“[信息增益](@entry_id:262008)”。如果为了完美拟合训练数据，后验 $Q$ 需要变得与简单先验 $P$ 非常不同，那么 KL 散度就会很大，导致[泛化界](@entry_id:637175)变差。这可以看作是一种对[模型复杂度](@entry_id:145563)的惩罚。
2.  **对随机分类器的界定**：该理论界定的对象是[后验分布](@entry_id:145605)下模型的平均性能，而非单个模型的性能。
3.  **先验的独立性**：理论的有效性严格依赖于先验 $P$ 是在观察数据**之前**固定的。如果先验依赖于数据，整个概率论证将失效。
4.  **样本量的作用**：与所有[泛化理论](@entry_id:635655)一样，界限随着样本量 $n$ 的增加而收紧，这反映了[经验风险](@entry_id:633993)围绕总体风险的集中现象。

#### 分解预测的不确定性

除了泛化能力，量化模型的**不确定性**也至关重要。一个好的模型不仅应该给出准确的预测，还应该知道自己何时可能出错。我们可以将总的预测[不确定性分解](@entry_id:183314)为两个部分：**[认知不确定性](@entry_id:149866)（epistemic uncertainty）**和**[偶然不确定性](@entry_id:154011)（aleatoric uncertainty）**。

**[深度集成](@entry_id:636362)（Deep Ensembles）**是一种强大的实用技术，它通过训练多个具有不同随机初始化的相同架构的模型，并对它们的预测进行平均来实现。我们可以将模型选择（由随机种子 $S$ 决定）本身看作一个[随机变量](@entry_id:195330)。利用**[全方差公式](@entry_id:177482)（Law of Total Variance）**，我们可以将目标变量 $Y$ 的总预测[方差分解](@entry_id:272134)为 [@problem_id:3166725]：
$$
\mathrm{Var}(Y) = \underbrace{\mathrm{Var}_S(\mathbb{E}[Y \mid S])}_{\text{认知不确定性}} + \underbrace{\mathbb{E}_S[\mathrm{Var}(Y \mid S)]}_{\text{偶然不确定性}}
$$
- **[认知不确定性](@entry_id:149866)**是集成中不同模型预测均值之间的[方差](@entry_id:200758)。它反映了模型本身由于数据有限而产生的不确定性。如果所有模型都给出非常不同的预测，说明模型对这个输入“没有把握”。这种不确定性可以通过增加训练数据来降低。
- **[偶然不确定性](@entry_id:154011)**是每个模型自身预测[方差](@entry_id:200758)的期望。它代表了数据生成过程中固有的、不可约减的噪声。即使拥有无限的数据，这种不确定性也无法消除。

这种分解为我们提供了一种诊断模型和理解预测可靠性的实用工具。

#### 学习的终局：SGD 的[平稳分布](@entry_id:194199)

最后，让我们回到训练过程本身。SGD 不会收敛到一个点，而是在一个区域内[持续随机游走](@entry_id:189741)。参数 $\boldsymbol{\theta}_t$ 的轨迹是一个[随机过程](@entry_id:159502)。在二次[损失函数](@entry_id:634569) $f(\boldsymbol{\theta}) = \frac{1}{2}\boldsymbol{\theta}^{\top}\mathbf{H}\boldsymbol{\theta}$ 的简化模型下，我们可以精确分析 SGD 的行为。

当 SGD 达到平稳状态时，参数 $\boldsymbol{\theta}_{\infty}$ 会在一个以损失函数[最小值点](@entry_id:634980)为中心的[分布](@entry_id:182848)中波动。这个[分布](@entry_id:182848)的形状（即[方差](@entry_id:200758)）取决于[优化景观](@entry_id:634681)的曲率（由 Hessian 矩阵 $\mathbf{H}$ 描述）和[梯度噪声](@entry_id:165895)的结构（由其[协方差矩阵](@entry_id:139155) $\boldsymbol{\Sigma}_g$ 描述）。

在一个解耦的二维情况下，其中 $\mathbf{H} = \mathrm{diag}(h_1, h_2)$ 且噪声协[方差](@entry_id:200758)的期望为 $\mathbb{E}[\boldsymbol{\Sigma}_g] = \mathrm{diag}(\mathbb{E}[\lambda_1], \mathbb{E}[\lambda_2])$，参数在每个坐标轴上的平稳[方差](@entry_id:200758)为 [@problem_id:3166770]：
$$
\mathrm{Var}(\theta_{i, \infty}) \propto \frac{\mathbb{E}[\lambda_i]}{h_i(2-\eta h_i)}
$$
这个结果表明，SGD 在[损失景观](@entry_id:635571)中**更平坦**的方向（$h_i$ 较小）和/或[梯度噪声](@entry_id:165895)**更大**的方向（$\mathbb{E}[\lambda_i]$ 较大）探索的范围更广（[方差](@entry_id:200758)更大）。这为“SGD 偏好宽阔平坦的最小值”这一广为流传的观点提供了数学支持，因为这些区域通常对应于更好的泛化性能。通过将 SGD 的输出视为一个[随机变量](@entry_id:195330)，我们揭示了[优化算法](@entry_id:147840)、[损失景观](@entry_id:635571)和[梯度噪声](@entry_id:165895)之间深刻的相互作用，这种相互作用共同决定了[深度学习模型](@entry_id:635298)的最终性能。