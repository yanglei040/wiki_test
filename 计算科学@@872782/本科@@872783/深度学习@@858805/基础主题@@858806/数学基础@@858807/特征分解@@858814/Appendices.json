{"hands_on_practices": [{"introduction": "为了利用甚至改变损失函数的几何形状，我们首先需要学会如何精确地度量它。这个练习将向您介绍瑞利商 (Rayleigh quotient)，这是一种与尺度无关的度量方向曲率的标准化方法。通过这个实践，您将建立起一个核心直觉：Hessian 矩阵的特征值决定了某点所有方向上曲率的范围，为理解优化过程和模型剪枝等高级技术奠定了基础。[@problem_id:3120472]", "problem": "给定一个对称实矩阵，它表示深度学习中使用的局部二次近似中一个二阶可导损失函数的 Hessian 矩阵。考虑一个非零参数方向向量及其在该方向上的关联曲率。您的任务是：从第一性原理推导出一个尺度不变的方向曲率度量，将其与特征分解联系起来，并用它来设计一个剪枝规则。\n\n从以下基本基础开始：\n- 对于一个二阶可导的标量损失函数，在某点周围的二阶泰勒展开表明，对于沿方向 $\\mathbf{v}$ 大小为 $\\alpha$ 的小步长，其二次变化由二次型 $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ 决定，其中 $\\mathbf{H}$ 是在展开点的对称 Hessian 矩阵。\n- 一个二阶可导标量损失在某点的 Hessian 矩阵 $\\mathbf{H}$ 是一个实对称矩阵，因此它允许使用标准正交特征基和实特征值进行实特征分解。\n\n您的程序必须：\n- 从二次型构造一个尺度不变的方向曲率度量，并将其用作剪枝的曲率重要性分数。在您的方法设计中，从第一性原理明确证明其尺度不变性。\n- 实现剪枝规则：当且仅当一个方向的曲率重要性分数小于或等于给定阈值 $t$ 时，才剪枝该方向。为保证数值鲁棒性，如果对于一个给定的 $\\varepsilon$ 有 $\\mathbf{v}^{\\top}\\mathbf{v}  \\varepsilon$，则按约定将曲率重要性分数定义为 $0$。\n- 使用特征分解计算 $\\mathbf{H}$ 的最小和最大特征值，并对每个给定的非零方向，验证所实现的曲率重要性分数位于从最小特征值到最大特征值的闭区间内。在检查区间成员资格时，使用数值容差 $\\tau$，并且如果 $\\lambda_{\\min} - \\tau \\le r \\le \\lambda_{\\max} + \\tau$，则将分数 $r$ 视为在界限内。\n\n测试套件和要求输出：\n在以下测试用例上实现您的解决方案。在每个用例中，都给定一个对称矩阵 $\\mathbf{H}$、一组方向向量、一个阈值 $t$、一个小范数截断值 $\\varepsilon$ 以及一个边界检查容差 $\\tau$。\n\n- 案例 1 (正常路径，正定矩阵):\n  - $\\mathbf{H}_1 = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$.\n  - 方向: $\\mathbf{v}_{1,1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{1,2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $\\mathbf{v}_{1,3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n  - 阈值: $t_1 = 3.5$.\n  - 小范数截断值: $\\varepsilon = 10^{-12}$.\n  - 边界容差: $\\tau = 10^{-10}$.\n\n- 案例 2 (边界相等，重复特征值):\n  - $\\mathbf{H}_2 = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}$.\n  - 方向: $\\mathbf{v}_{2,1} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $\\mathbf{v}_{2,2} = \\begin{bmatrix} -3 \\\\ 4 \\end{bmatrix}$.\n  - 阈值: $t_2 = 2$.\n  - 小范数截断值: $\\varepsilon = 10^{-12}$.\n  - 边界容差: $\\tau = 10^{-10}$.\n\n- 案例 3 (半正定，带零空间和近零向量):\n  - $\\mathbf{H}_3 = \\begin{bmatrix} 1  0  0 \\\\ 0  0  0 \\\\ 0  0  3 \\end{bmatrix}$.\n  - 方向: $\\mathbf{v}_{3,1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{3,2} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{3,3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $\\mathbf{v}_{3,4} = \\begin{bmatrix} 10^{-13} \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - 阈值: $t_3 = 0.5$.\n  - 小范数截断值: $\\varepsilon = 10^{-12}$.\n  - 边界容差: $\\tau = 10^{-10}$.\n\n- 案例 4 (不定曲率，鞍点状):\n  - $\\mathbf{H}_4 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$.\n  - 方向: $\\mathbf{v}_{4,1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $\\mathbf{v}_{4,2} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$, $\\mathbf{v}_{4,3} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n  - 阈值: $t_4 = -0.5$.\n  - 小范数截断值: $\\varepsilon = 10^{-12}$.\n  - 边界容差: $\\tau = 10^{-10}$.\n\n对于每个案例 $k \\in \\{1,2,3,4\\}$，您的程序必须计算：\n- 在“当且仅当曲率重要性 $\\le t_k$ 时剪枝”的规则下，被剪枝方向的整数计数 $c_k$。\n- 一个布尔标志 $b_k$，当且仅当该案例中计算出的每个曲率重要性分数都在特征值区间 $[\\lambda_{\\min}(\\mathbf{H}_k), \\lambda_{\\max}(\\mathbf{H}_k)]$ 内（在容差 $\\tau$ 范围内）时，该标志为真。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含结果，格式为方括号括起来的逗号分隔列表，顺序为 $[c_1, b_1, c_2, b_2, c_3, b_3, c_4, b_4]$。", "solution": "该问题是有效的。它在科学上基于多元微积分和线性代数的原理，并应用于深度学习中的优化，特别是关于通过 Hessian 矩阵分析损失地貌。该问题是适定的、客观的，并包含推导唯一、可验证解所需的所有信息。\n\n核心任务是开发并应用一种尺度不变的方向曲率度量。从点 $\\mathbf{w}_0$ 出发，沿方向 $\\mathbf{v}$ 迈出大小为 $\\alpha$ 的一小步，损失函数 $L$ 的变化由二阶泰勒展开近似：\n$$ L(\\mathbf{w}_0 + \\alpha\\mathbf{v}) \\approx L(\\mathbf{w}_0) + \\alpha \\nabla L(\\mathbf{w}_0)^{\\top}\\mathbf{v} + \\frac{\\alpha^2}{2} \\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v} $$\n在梯度 $\\nabla L(\\mathbf{w}_0)$ 为零的临界点，局部损失的变化由二次型 $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ 决定，其中 $\\mathbf{H}$ 是在 $\\mathbf{w}_0$ 处求值的 Hessian 矩阵。这一项描述了损失曲面在 $\\mathbf{v}$ 方向上的曲率。\n\n**1. 尺度不变曲率重要性分数的推导**\n\n二次型 $\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}$ 本身并不是一个合适的方向曲率度量，因为它依赖于方向向量 $\\mathbf{v}$ 的大小。如果我们将向量乘以一个非零常数 $k$，二次型会发生变化：\n$$ (k\\mathbf{v})^{\\top}\\mathbf{H}(k\\mathbf{v}) = k^2(\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}) $$\n该度量随 $k^2$ 缩放，意味着它反映的是向量 $\\mathbf{v}$ 的“长度”，而不仅仅是其方向。为了创建一个仅与方向本身内禀相关的度量，我们必须对这个量进行归一化。一个自然的选择是使用向量的欧几里得范数的平方 $\\|\\mathbf{v}\\|^2 = \\mathbf{v}^{\\top}\\mathbf{v}$ 进行归一化。这引出了**瑞利商**，我们将其定义为曲率重要性分数 $s(\\mathbf{v})$：\n$$ s(\\mathbf{v}) = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} $$\n对于任何非零标量 $k$，该分数都是尺度不变的：\n$$ s(k\\mathbf{v}) = \\frac{(k\\mathbf{v})^{\\top}\\mathbf{H}(k\\mathbf{v})}{(k\\mathbf{v})^{\\top}(k\\mathbf{v})} = \\frac{k^2(\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v})}{k^2(\\mathbf{v}^{\\top}\\mathbf{v})} = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} = s(\\mathbf{v}) $$\n这个性质使得 $s(\\mathbf{v})$ 成为一个纯粹衡量 $\\mathbf{v}$ 方向曲率的度量，与其大小无关。为保证数值稳定性，根据问题陈述，如果范数的平方 $\\mathbf{v}^{\\top}\\mathbf{v}$ 小于一个小的正常数 $\\varepsilon$，我们将分数定义为 $0$。这个约定将大小可忽略的方向视为具有零曲率重要性。\n\n**2. 与特征分解和有界性的联系**\n\nHessian 矩阵 $\\mathbf{H}$ 是实对称的。这类矩阵的一个基本性质是它们可被正交对角化并拥有实特征值。瑞利-里兹定理建立了瑞利商与 $\\mathbf{H}$ 的特征值之间的直接联系。该定理指出，对于任何非零向量 $\\mathbf{v}$，瑞利商的值都由 $\\mathbf{H}$ 的最小和最大特征值界定：\n$$ \\lambda_{\\min}(\\mathbf{H}) \\le s(\\mathbf{v}) = \\frac{\\mathbf{v}^{\\top}\\mathbf{H}\\mathbf{v}}{\\mathbf{v}^{\\top}\\mathbf{v}} \\le \\lambda_{\\max}(\\mathbf{H}) $$\n最小值 $\\lambda_{\\min}(\\mathbf{H})$ 在 $\\mathbf{v}$ 是对应于最小特征值的特征向量时达到。类似地，最大值 $\\lambda_{\\max}(\\mathbf{H})$ 在 $\\mathbf{v}$ 是对应于最大特征值的特征向量时达到。该定理为分析可能曲率的范围提供了一个强大的工具，并构成了问题所要求的验证步骤的基础。对于每个计算出的分数 $s$，我们必须验证它位于区间 $[\\lambda_{\\min}(\\mathbf{H}) - \\tau, \\lambda_{\\max}(\\mathbf{H}) + \\tau]$ 内，其中 $\\tau$ 是给定的数值容差。\n\n**3. 剪枝规则和验证程序**\n\n为每个测试用例实现的程序遵循以下逻辑：\n\n**对于每个案例 $k$，给定 Hessian 矩阵 $\\mathbf{H}_k$、方向 $\\{\\mathbf{v}_{k,i}\\}$、阈值 $t_k$、截断值 $\\varepsilon$ 和容差 $\\tau$：**\n1.  初始化一个剪枝方向的计数器 $c_k = 0$，以及一个用于边界验证的布尔标志 $b_k = \\text{true}$。\n2.  对对称矩阵 $\\mathbf{H}_k$ 进行特征分解以找到其实特征值。确定最小特征值 $\\lambda_{\\min}$ 和最大特征值 $\\lambda_{\\max}$。\n3.  对于每个方向向量 $\\mathbf{v}_{k,i}$：\n    a. 计算范数的平方 $n_i = \\mathbf{v}_{k,i}^{\\top}\\mathbf{v}_{k,i}$。\n    b. 计算曲率重要性分数 $s_{k,i}$。如果 $n_i  \\varepsilon$，则设置 $s_{k,i} = 0$。否则，计算 $s_{k,i} = (\\mathbf{v}_{k,i}^{\\top}\\mathbf{H}_k\\mathbf{v}_{k,i}) / n_i$。\n    c. 应用剪枝规则：如果 $s_{k,i} \\le t_k$，则增加剪枝计数 $c_k$。\n    d. 对照谱界验证分数：如果条件 $\\lambda_{\\min} - \\tau \\le s_{k,i} \\le \\lambda_{\\max} + \\tau$ 不成立，则设置 $b_k = \\text{false}$。\n4.  该案例的最终结果是序对 $(c_k, b_k)$。\n\n此程序被系统地应用于所有提供的测试用例。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing each test case according to the\n    derived methodology for curvature calculation, pruning, and verification.\n    \"\"\"\n\n    def process_case(H, directions, t, eps, tau):\n        \"\"\"\n        Calculates the number of pruned directions and verifies that all curvature\n        scores lie within the eigenvalue bounds for a single test case.\n\n        Args:\n            H (np.ndarray): The symmetric Hessian matrix.\n            directions (list of np.ndarray): A list of direction vectors.\n            t (float): The pruning threshold.\n            eps (float): The small-norm cutoff for vectors.\n            tau (float): The tolerance for eigenvalue bound checking.\n\n        Returns:\n            tuple: A tuple (pruned_count, all_scores_in_bounds) containing\n                   the integer count of pruned directions and a boolean flag.\n        \"\"\"\n        pruned_count = 0\n        all_scores_in_bounds = True\n\n        # For a real symmetric matrix, eigh is preferred.\n        # It returns sorted eigenvalues and is numerically stable.\n        eigenvalues = np.linalg.eigh(H)[0]\n        lambda_min = eigenvalues[0]\n        lambda_max = eigenvalues[-1]\n\n        for v in directions:\n            v_dot_v = v.T @ v\n            \n            # Use single element from 1x1 array if necessary\n            if isinstance(v_dot_v, np.ndarray):\n                v_dot_v = v_dot_v.item()\n\n            score = 0.0\n            if v_dot_v >= eps:\n                # Rayleigh quotient calculation\n                numerator = v.T @ H @ v\n                if isinstance(numerator, np.ndarray):\n                    numerator = numerator.item()\n                score = numerator / v_dot_v\n\n            # Apply pruning rule\n            if score = t:\n                pruned_count += 1\n\n            # Verify score is within spectral bounds (Rayleigh-Ritz theorem)\n            if not (lambda_min - tau = score = lambda_max + tau):\n                all_scores_in_bounds = False\n\n        return pruned_count, all_scores_in_bounds\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, positive definite)\n        {\n            \"H\": np.array([[4, 1], [1, 3]]),\n            \"directions\": [np.array([[1], [0]]), np.array([[0], [1]]), np.array([[1], [1]])],\n            \"t\": 3.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 2 (boundary equality, repeated eigenvalue)\n        {\n            \"H\": np.array([[2, 0], [0, 2]]),\n            \"directions\": [np.array([[1], [2]]), np.array([[-3], [4]])],\n            \"t\": 2.0,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 3 (positive semidefinite with a nullspace and a near-zero vector)\n        {\n            \"H\": np.array([[1, 0, 0], [0, 0, 0], [0, 0, 3]]),\n            \"directions\": [\n                np.array([[0], [1], [0]]),\n                np.array([[1], [0], [0]]),\n                np.array([[1], [1], [0]]),\n                np.array([[1e-13], [0], [0]])\n            ],\n            \"t\": 0.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        },\n        # Case 4 (indefinite curvature, saddle-like)\n        {\n            \"H\": np.array([[0, 1], [1, 0]]),\n            \"directions\": [np.array([[1], [1]]), np.array([[1], [-1]]), np.array([[1], [0]])],\n            \"t\": -0.5,\n            \"eps\": 1e-12,\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        c, b = process_case(case[\"H\"], case[\"directions\"], case[\"t\"], case[\"eps\"], case[\"tau\"])\n        results.extend([c, b])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120472"}, {"introduction": "既然我们已经学会了如何度量曲率，那么当我们发现一个负曲率方向时该怎么办？这个练习将理论付诸实践，展示了如何利用 Hessian 矩阵的特征分解来设计一个更智能的优化算法。您将亲手实现一个能够利用与负特征值相关联的特征向量来逃离鞍点的程序，而鞍点正是梯度下降等一阶方法经常陷入的困境。[@problem_id:3120493]", "problem": "给定一个确定性的玩具训练目标，该目标捕捉了深度学习中常见的非凸现象：具有负曲率的静止鞍点。考虑在维度 $2$ 上为参数 $\\theta = (x,y) \\in \\mathbb{R}^{2}$ 定义的损失函数\n$$\nf(\\theta) = \\frac{1}{4}\\left(x^{4}+y^{4}\\right) - \\frac{1}{2}x^{2} + \\frac{1}{2}y^{2}.\n$$\n该目标函数具有同时包含正曲率和负曲率分量的静止点。您的任务是设计并实现一个程序，在基于梯度的训练过程中跟踪海森矩阵 $H(\\theta)$ 的负特征值 $\\lambda_{i}  0$，并在一次信号较弱时，通过沿与 $\\lambda_{i}  0$ 对应的特征向量 $v_{i}$ 移动来逃离鞍点行为。\n\n从以下基本概念开始：\n- 梯度 $\\nabla f(\\theta)$ 是 $f$ 关于 $(x,y)$ 的偏导数向量。\n- 海森矩阵 $H(\\theta)$ 是二阶偏导数的 $2 \\times 2$ 对称矩阵。\n- $H(\\theta)$ 的一个特征对 $(\\lambda, v)$ 满足 $H(\\theta)\\,v=\\lambda\\,v$，其中 $v \\neq 0$。\n- 标准的梯度下降步骤使用 $\\theta_{t+1}=\\theta_{t}-\\alpha\\,\\nabla f(\\theta_{t})$，其中步长 $\\alpha>0$。\n- $f$ 在 $\\theta$ 点沿方向 $v$、标量步长为 $\\varepsilon$ 的二阶泰勒展开式为\n$$\nf(\\theta+\\varepsilon v)\\approx f(\\theta)+\\varepsilon\\,\\nabla f(\\theta)^{\\top}v+\\frac{1}{2}\\varepsilon^{2}v^{\\top}H(\\theta)v.\n$$\n当 $v$ 是一个特征值为 $\\lambda0$ 的特征向量且 $\\|\\nabla f(\\theta)\\|$ 足够小时，二次项占主导地位，小的非零 $\\varepsilon$ 可以使 $f$ 减小。\n\n问题要求：\n- 实现一个迭代训练过程，该过程在标准梯度下降和有原则的负曲率逃逸步骤之间交替。在迭代 $t$ 时，计算 $\\nabla f(\\theta_{t})$ 和 $H(\\theta_{t})$，对 $H(\\theta_{t})$ 进行特征分解，并识别最小特征值 $\\lambda_{\\min}$ 及其对应的单位特征向量 $v_{\\min}$。\n- 使用一个简单的决策规则：如果 $\\lambda_{\\min}  -\\lambda_{\\mathrm{thresh}}$ 并且 $\\|\\nabla f(\\theta_{t})\\| \\le g_{\\mathrm{thresh}}$，则执行曲率逃逸步骤 $\\theta_{t+1}=\\theta_{t}+s_{\\mathrm{esc}}\\,\\tilde{v}$，其中 $\\tilde{v}$ 是与 $\\lambda_{\\min}$ 相关的单位特征向量；否则执行梯度下降步骤 $\\theta_{t+1}=\\theta_{t}-\\alpha\\,\\nabla f(\\theta_{t})$。\n- 逃逸步长应仅使用特征分解中可用的量，根据局部曲率的大小进行缩放，并根据二阶泰勒模型选择一个安全的小步长。您必须确保在需要时通过小的正则化使步长在所有状态下保持有限。\n- 跟踪执行了多少次逃逸步骤。\n\n测试套件和超参数：\n- 对所有测试用例使用相同的超参数：步长 $\\alpha = 0.1$，最大迭代次数 $T=200$，梯度范数阈值 $g_{\\mathrm{thresh}}=10^{-6}$，负曲率阈值 $\\lambda_{\\mathrm{thresh}}=10^{-9}$，基线逃逸幅度 $s_{0}=0.1$，以及一个小的曲率正则化项 $\\varepsilon=10^{-12}$（仅在您的逃逸步长缩放中使用，如有）。\n- 测试用例是四个初始化点 $\\theta_{0}$：\n    $$\n    \\theta_{0}^{(1)} = [0,0],\\quad\n    \\theta_{0}^{(2)} = [0.2,0],\\quad\n    \\theta_{0}^{(3)} = [0,0.5],\\quad\n    \\theta_{0}^{(4)} = [1.5,0.1].\n    $$\n- 对于每个测试用例，运行迭代过程最多 $T$ 步，并报告：\n    $1)$ 逃逸次数（整数），\n    $2)$ 指示最终海森矩阵是否具有负特征值的布尔值，\n    $3)$ 最终损失值 $f(\\theta_{T})$（实数），\n    $4)$ 最终参数向量 $\\theta_{T}$（实数列表）。\n- 四舍五入：将所有实数值输出四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果是按上述顺序排列的列表。例如：\n$$\n[\\,[\\text{escape\\_count}_{1},\\text{has\\_neg}_{1},f_{1},[\\text{theta}_{1,1},\\text{theta}_{1,2}]],\\dots,[\\text{escape\\_count}_{4},\\text{has\\_neg}_{4},f_{4},[\\text{theta}_{4,1},\\text{theta}_{4,2}]]\\,].\n$$\n此问题不涉及单位。不使用角度。不使用百分比。确保打印的行与指定格式匹配，无任何附加文本。", "solution": "用户要求设计并实现一个优化程序，以处理非凸目标函数中的鞍点，这是训练深度学习模型时的一个常见挑战。该问题定义明确，具有科学依据，并为完整解决方案提供了所有必要信息。\n\n需要最小化的目标函数由 $f: \\mathbb{R}^{2} \\to \\mathbb{R}$ 给出，为参数 $\\theta = (x,y)$ 定义为：\n$$\nf(\\theta) = f(x,y) = \\frac{1}{4}\\left(x^{4}+y^{4}\\right) - \\frac{1}{2}x^{2} + \\frac{1}{2}y^{2}\n$$\n我们的任务是实现一个迭代算法，该算法从一个初始点 $\\theta_0$ 开始，生成一个迭代序列 $\\theta_t$，使其收敛到一个局部最小值，并有效逃离遇到的任何鞍点。\n\n首先，我们必须推导损失函数的一阶和二阶导数，它们分别是梯度 $\\nabla f(\\theta)$ 和海森矩阵 $H(\\theta)$。这些对于指定的优化算法至关重要。\n\n梯度是偏导数的向量：\n$$\n\\nabla f(\\theta) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} x^3 - x \\\\ y^3 + y \\end{pmatrix}\n$$\n海森矩阵是二阶偏导数的矩阵：\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2}  \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x}  \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 3x^2 - 1  0 \\\\ 0  3y^2 + 1 \\end{pmatrix}\n$$\n海森矩阵是一个对角矩阵，这简化了其分析。其特征值是对角线元素：$\\lambda_1 = 3x^2 - 1$ 和 $\\lambda_2 = 3y^2 + 1$。对应的特征向量是标准基向量，$v_1 = (1,0)^\\top$ 和 $v_2 = (0,1)^\\top$。\n\n静止点是梯度为零的位置，即 $\\nabla f(\\theta) = 0$。对于我们的函数，这发生在 $x^3 - x = 0$ 和 $y^3 + y = 0$ 时。第一个方程得出 $x \\in \\{-1, 0, 1\\}$，第二个方程要求 $y=0$。因此，静止点为 $(-1,0)$、$(0,0)$ 和 $(1,0)$。\n\n这些静止点的性质由在这些点处计算的海森矩阵的特征值决定：\n- 在 $\\theta = (\\pm 1, 0)$ 处：$H = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$。特征值为 $\\lambda_1=2$ 和 $\\lambda_2=1$。两者均为正，因此这些点是局部最小值。在这些最小值处的损失为 $f(\\pm 1, 0) = \\frac{1}{4} - \\frac{1}{2} = -0.25$。\n- 在 $\\theta = (0, 0)$ 处：$H = \\begin{pmatrix} -1  0 \\\\ 0  1 \\end{pmatrix}$。特征值为 $\\lambda_1=-1$ 和 $\\lambda_2=1$。由于有一个负特征值和一个正特征值，该点是一个鞍点。负曲率方向沿着x轴，与特征向量 $v_1=(1,0)^\\top$ 相关联。\n\n所提出的算法是一种混合策略。在每次迭代 $t$ 中，使用当前参数 $\\theta_t$：\n$1.$ 我们计算梯度 $\\nabla f(\\theta_t)$、其范数 $\\|\\nabla f(\\theta_t)\\|$ 和海森矩阵 $H(\\theta_t)$。\n$2.$ 我们对 $H(\\theta_t)$ 进行特征分解，以找到其最小特征值 $\\lambda_{\\min}$ 和相应的单位特征向量 $v_{\\min}$。\n$3.$ 根据两个阈值做出决策，梯度范数阈值 $g_{\\mathrm{thresh}}$ 和负曲率阈值 $\\lambda_{\\mathrm{thresh}}$。\n\n更新规则如下：\n- **如果 $\\lambda_{\\min}  -\\lambda_{\\mathrm{thresh}}$ 并且 $\\|\\nabla f(\\theta_t)\\| \\le g_{\\mathrm{thresh}}$**：此条件表明我们接近一个静止点（梯度很弱），该点具有显著的负曲率方向。标准的梯度下降步骤会非常慢。为了逃离鞍点区域，我们执行一个“负曲率逃逸”步骤。此步骤将参数沿对应于负特征值的特征向量方向移动。更新规则是：\n$$\n\\theta_{t+1} = \\theta_t + s_{\\mathrm{esc}} v_{\\min}\n$$\n问题要求设计一个由曲率大小缩放的步长 $s_{\\mathrm{esc}}$。一个受信任域方法启发的有原则的选择是，使步长与负曲率的大小成反比，同时使用基线大小 $s_0$ 和一个小的正则化常数 $\\varepsilon$ 来防止除以零。因此，我们将逃逸步长定义为：\n$$\ns_{\\mathrm{esc}} = \\frac{s_0}{|\\lambda_{\\min}| + \\varepsilon}\n$$\n这种设计确保了在曲率接近平坦（$|\\lambda_{\\min}|$ 较小）时采取较大的步长，而在曲率较陡（$|\\lambda_{\\min}|$ 较大）时采取较小、更安全的步长。方向取自特征分解程序返回的单位特征向量 $v_{\\min}$。\n\n- **否则**：如果梯度足够大或者没有显著的负曲率，标准的梯度下降更新更为有效。此更新为：\n$$\n\\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t)\n$$\n其中 $\\alpha$ 是学习率（步长）。\n\n对于每个指定的初始点 $\\theta_0$，此迭代过程将重复最多 $T=200$ 次迭代。我们将跟踪所采取的逃逸步骤数。在最后一次迭代之后，我们报告所需的指标：总逃逸次数、最终海森矩阵是否具有负特征值、最终损失值和最终参数向量，所有实数值均四舍五入到 $6$ 位小数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests a hybrid optimization algorithm that combines\n    gradient descent with a negative curvature escape mechanism to avoid\n    saddle points.\n    \"\"\"\n    # Hyperparameters\n    alpha = 0.1\n    T = 200\n    g_thresh = 1e-6\n    lambda_thresh = 1e-9\n    s0 = 0.1\n    epsilon = 1e-12\n\n    # Test cases: initial parameter vectors theta_0\n    thetas0 = [\n        np.array([0.0, 0.0]),\n        np.array([0.2, 0.0]),\n        np.array([0.0, 0.5]),\n        np.array([1.5, 0.1]),\n    ]\n\n    def f(theta):\n        \"\"\"Computes the loss function f(theta).\"\"\"\n        x, y = theta\n        return 0.25 * (x**4 + y**4) - 0.5 * x**2 + 0.5 * y**2\n\n    def grad_f(theta):\n        \"\"\"Computes the gradient of f(theta).\"\"\"\n        x, y = theta\n        return np.array([x**3 - x, y**3 + y])\n\n    def hess_f(theta):\n        \"\"\"Computes the Hessian of f(theta).\"\"\"\n        x, y = theta\n        # The Hessian is diagonal for this specific problem.\n        return np.array([[3 * x**2 - 1, 0.0], [0.0, 3 * y**2 + 1]])\n\n    all_results = []\n\n    for theta0 in thetas0:\n        theta = theta0.copy()\n        escape_count = 0\n\n        for _ in range(T):\n            # Compute gradient, Hessian, and its eigendecomposition\n            grad = grad_f(theta)\n            hess = hess_f(theta)\n            \n            # np.linalg.eigh is for symmetric matrices and returns sorted eigenvalues\n            eigenvalues, eigenvectors = np.linalg.eigh(hess)\n            lambda_min = eigenvalues[0]\n            v_min = eigenvectors[:, 0]\n            \n            grad_norm = np.linalg.norm(grad)\n\n            # Decision rule for which step to take\n            if lambda_min  -lambda_thresh and grad_norm = g_thresh:\n                # Curvature escape step\n                escape_count += 1\n                \n                # Scaled escape step size as per problem design principles\n                s_esc = s0 / (abs(lambda_min) + epsilon)\n                \n                # Update along the eigenvector of minimum eigenvalue\n                theta = theta + s_esc * v_min\n            else:\n                # Standard gradient descent step\n                theta = theta - alpha * grad\n        \n        # After T iterations, collect and format results\n        final_loss = f(theta)\n        \n        # Check final Hessian for negative eigenvalues\n        final_hess = hess_f(theta)\n        final_eigenvalues, _ = np.linalg.eigh(final_hess)\n        has_neg_eigenvalue = final_eigenvalues[0]  0\n\n        # Round all specified real-valued outputs to 6 decimal places\n        final_loss_rounded = round(final_loss, 6)\n        final_theta_rounded = [round(c, 6) for c in theta]\n        \n        result_for_case = [\n            escape_count,\n            bool(has_neg_eigenvalue), # Cast to standard Python boolean\n            final_loss_rounded,\n            final_theta_rounded\n        ]\n        all_results.append(result_for_case)\n\n    # Manually format the output string to match the exact requirement,\n    # avoiding extra spaces and using standard boolean/list representations.\n    result_strings = []\n    for res in all_results:\n        escape_count, has_neg, loss, theta_vec = res\n        theta_str = f\"[{theta_vec[0]},{theta_vec[1]}]\"\n        has_neg_str = 'True' if has_neg else 'False'\n        res_str = f\"[{escape_count},{has_neg_str},{loss},{theta_str}]\"\n        result_strings.append(res_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3120493"}, {"introduction": "我们已经学会了度量和利用曲率，最后一个练习将探讨我们如何主动地 *控制* 曲率。我们将深入研究 $L_2$ 正则化的几何效应，它是现代机器学习的基石之一。通过理论推导和编码实验，您将验证 $L_2$ 正则化如何通过系统性地增加所有 Hessian 特征值来重塑损失曲面，从而使优化过程更加稳定和高效。[@problem_id:3120548]", "problem": "考虑在标签位于 $\\{0,1\\}$ 内的二元分类问题，使用经验风险最小化（ERM）和 logistic 损失函数。令 $X \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，其行是样本；$y \\in \\{0,1\\}^{n}$ 为标签向量；$w \\in \\mathbb{R}^{d}$ 为参数向量。定义 logistic 函数 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$ 和经验风险 $R(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(w; x_i, y_i)$，其中 $\\ell(w; x_i, y_i) = - y_i \\log \\sigma(x_i^\\top w) - (1 - y_i) \\log(1 - \\sigma(x_i^\\top w))$。定义强度为 $\\lambda \\ge 0$ 的平方欧几里得范数惩罚（L2）为 $P_\\lambda(w) = \\frac{\\lambda}{2} \\lVert w \\rVert_2^2$，以及正则化风险 $R_\\lambda(w) = R(w) + P_\\lambda(w)$。一个二阶可微函数 $F$ 在 $w$ 处的 Hessian 矩阵是其二阶偏导数矩阵 $\\nabla^2 F(w)$。仅使用 $R(w)$、$P_\\lambda(w)$ 的定义和微分法则，解析地推导出 $R_\\lambda(w)$ 的 Hessian 矩阵与 $R(w)$ 的 Hessian 矩阵的关系，并确定惩罚强度 $\\lambda$ 如何影响 Hessian 矩阵的特征值。然后，设计并实现一个数值实验，以在训练动态中隔离这种效应，这意味着您必须在无正则化的梯度下降轨迹上，于相同的参数迭代点 $w$ 评估曲率效应，以避免不同轨迹造成的混淆。\n\n您的程序必须从第一性原理出发实现以下步骤：\n- 生成合成数据集 $(X,y)$，通过对 $X$ 进行独立同分布的标准正态分布采样，采样一个基准真相参数 $w_\\star$，计算 logits $z = X w_\\star$，采样 $y_i \\sim \\mathrm{Bernoulli}(\\sigma(z_i))$，并返回 $(X,y)$。通过固定的随机种子确保可复现性。\n- 对无正则化的经验风险 $R(w)$ 运行全批量梯度下降（GD），从初始化 $w_0 = 0$ 开始，共进行 $K$ 步，使用固定的学习率 $\\eta  0$。\n- 在每个 GD 迭代点 $w_k$ 处，计算 Hessian 矩阵 $H_{\\text{unreg}}(w_k) = \\nabla^2 R(w_k)$，以及在同一点的正则化 Hessian 矩阵 $H_{\\text{reg}}(w_k,\\lambda) = \\nabla^2 R_\\lambda(w_k)$。\n- 计算两个 Hessian 矩阵的特征分解以获得它们的谱。为了隔离惩罚项的谱效应，通过计算排序后特征值的差异来比较在同一 $w_k$ 处的谱。对于每个测试用例，聚合所有特征值和所有迭代中的最大绝对偏差：\n$$\n\\varepsilon_{\\max} = \\max_{0 \\le k  K} \\left( \\max_j \\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\Delta \\, \\right| \\right),\n$$\n其中 $\\Delta$ 是您解析地识别出的恒定谱位移，$\\lambda_j(\\cdot)$ 表示按升序排列的第 $j$ 个特征值。\n- 对每个测试用例，报告 $\\varepsilon_{\\max}$ 作为一个非负实数。\n\n实现以下测试套件，以涵盖一般情况、边界条件和秩亏的边缘情况：\n- 情况 A（正常路径，满秩）：$n = 200$, $d = 5$, 种子 $= 7$, $\\lambda = 0.05$, $K = 8$, $\\eta = 0.1$, 满秩 $X$。\n- 情况 B（边界，零惩罚）：$n = 200$, $d = 5$, 种子 $= 7$, $\\lambda = 0$, $K = 8$, $\\eta = 0.1$, 满秩 $X$。\n- 情况 C（边缘，秩亏且惩罚适中）：$n = 150$, $d = 6$, 种子 $= 11$, $\\lambda = 0.05$, $K = 8$, $\\eta = 0.1$, 构造秩减少 $1$ 的 $X$（两列相同）。\n- 情况 D（边缘，秩亏且惩罚较大）：$n = 150$, $d = 6$, 种子 $= 11$, $\\lambda = 10.0$, $K = 8$, $\\eta = 0.1$, 与情况 C 相同的秩亏构造。\n- 情况 E（小规模可变性检查）：$n = 60$, $d = 3$, 种子 $= 3$, $\\lambda = 0.2$, $K = 10$, $\\eta = 0.2$, 满秩 $X$。\n\n角度单位和物理单位不适用于此问题。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 [情况 A, 情况 B, 情况 C, 情况 D, 情况 E]。每个条目必须是该情况下如上定义的实数 $\\varepsilon_{\\max}$。例如，格式必须与 $[\\varepsilon_A,\\varepsilon_B,\\varepsilon_C,\\varepsilon_D,\\varepsilon_E]$ 完全一样。", "solution": "该问题陈述经评估有效，因为它在科学上基于优化和机器学习的原理，问题设定良好，具有清晰且可通过数值验证的目标，并且没有矛盾或含糊之处。\n\n此处提供了一个完整的解决方案，包括一个解析推导，随后是一个数值实验的设计。\n\n### Hessian 矩阵及其谱位移的解析推导\n\n目标是推导 L2 正则化 logistic 回归风险 $R_\\lambda(w)$ 的 Hessian 矩阵，并确定其特征值如何受正则化参数 $\\lambda$ 的影响。\n\n正则化风险定义为经验风险 $R(w)$ 与惩罚项 $P_\\lambda(w)$ 之和：\n$$\nR_\\lambda(w) = R(w) + P_\\lambda(w)\n$$\n其中 $R(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(w; x_i, y_i)$ 且 $P_\\lambda(w) = \\frac{\\lambda}{2} \\lVert w \\rVert_2^2$。\n\nHessian 算子 $\\nabla^2$ 是一个线性算子。因此，正则化风险的 Hessian 矩阵是经验风险的 Hessian 矩阵与惩罚项的 Hessian 矩阵之和：\n$$\n\\nabla^2 R_\\lambda(w) = \\nabla^2 R(w) + \\nabla^2 P_\\lambda(w)\n$$\n\n我们来计算惩罚项的 Hessian 矩阵 $\\nabla^2 P_\\lambda(w)$。惩罚项为 $P_\\lambda(w) = \\frac{\\lambda}{2} \\sum_{j=1}^{d} w_j^2$。\n关于向量 $w$ 的一个分量 $w_k$ 的一阶偏导数为：\n$$\n\\frac{\\partial P_\\lambda(w)}{\\partial w_k} = \\frac{\\partial}{\\partial w_k} \\left( \\frac{\\lambda}{2} \\sum_{j=1}^{d} w_j^2 \\right) = \\frac{\\lambda}{2} \\cdot (2 w_k) = \\lambda w_k\n$$\n这意味着梯度为 $\\nabla P_\\lambda(w) = \\lambda w$。\n\n二阶偏导数构成了 Hessian 矩阵的元素。第 $(j,k)$ 个元素是：\n$$\n(\\nabla^2 P_\\lambda(w))_{jk} = \\frac{\\partial^2 P_\\lambda(w)}{\\partial w_j \\partial w_k} = \\frac{\\partial}{\\partial w_j} (\\lambda w_k)\n$$\n如果 $j=k$，该导数为 $\\lambda$；如果 $j \\neq k$，则为 $0$。这可以用 Kronecker delta $\\delta_{jk}$ 来表示。\n$$\n(\\nabla^2 P_\\lambda(w))_{jk} = \\lambda \\delta_{jk}\n$$\n这是矩阵 $\\lambda I_d$ 的定义，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。因此，$\\nabla^2 P_\\lambda(w) = \\lambda I_d$。\n\n将此结果代回，我们得到正则化与非正则化 Hessian 矩阵之间的关系：\n$$\n\\nabla^2 R_\\lambda(w) = \\nabla^2 R(w) + \\lambda I_d\n$$\n我们将非正则化 Hessian 矩阵记为 $H_{\\text{unreg}}(w) = \\nabla^2 R(w)$，正则化 Hessian 矩阵记为 $H_{\\text{reg}}(w, \\lambda) = \\nabla^2 R_\\lambda(w)$。它们的关系是：\n$$\nH_{\\text{reg}}(w, \\lambda) = H_{\\text{unreg}}(w) + \\lambda I_d\n$$\n现在，我们分析这种关系对特征值的影响。设 $v$ 是非正则化 Hessian 矩阵 $H_{\\text{unreg}}(w)$ 的一个特征向量，其对应的特征值为 $\\mu$。根据定义：\n$$\nH_{\\text{unreg}}(w) v = \\mu v\n$$\n将正则化 Hessian 矩阵 $H_{\\text{reg}}(w, \\lambda)$ 应用于该特征向量 $v$：\n$$\nH_{\\text{reg}}(w, \\lambda) v = (H_{\\text{unreg}}(w) + \\lambda I_d) v = H_{\\text{unreg}}(w) v + \\lambda I_d v = \\mu v + \\lambda v = (\\mu + \\lambda) v\n$$\n这表明 $v$ 也是 $H_{\\text{reg}}(w, \\lambda)$ 的一个特征向量，其对应的特征值为 $\\mu + \\lambda$。由于 Hessian 矩阵是实对称的，它们拥有一组完备的标准正交特征向量基。该性质对所有特征向量都成立，这意味着正则化 Hessian 矩阵的整个谱相对于非正则化 Hessian 矩阵的谱移动了一个恒定值 $\\lambda$。\n\n因此，所确定的恒定谱位移为 $\\Delta = \\lambda$。数值实验旨在验证量 $\\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\lambda \\, \\right|$ 接近于零，其中与零的差异可归因于浮点精度误差。\n\n### 数值实验设计\n\n该数值实验的结构旨在隔离并验证这一分析发现。\n\n1.  **数据生成**：使用固定的随机种子生成合成数据集 $(X, y)$ 以保证可复现性。特征矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 从标准正态分布中采样。同时采样一个基准真相参数向量 $w_\\star \\in \\mathbb{R}^d$。计算 Logits $z = X w_\\star$，然后通过 logistic 函数 $\\sigma(t) = (1+e^{-t})^{-1}$ 得到概率 $p_i = \\sigma(z_i)$。接着从伯努利分布中采样二元标签 $y_i$，即 $y_i \\sim \\text{Bernoulli}(p_i)$。对于秩亏情况，将 $X$ 的两列设为相同，使其秩至少减少一。\n\n2.  **梯度下降轨迹**：从 $w_0 = 0$ 开始，执行全批量梯度下降（GD）$K$ 次迭代。关键在于，更新使用的是*无正则化*风险 $R(w)$ 的梯度，即 $\\nabla R(w) = \\frac{1}{n} X^\\top (\\sigma(Xw) - y)$。这确保了迭代序列 $w_0, w_1, \\dots, w_{K-1}$ 与 $\\lambda$ 无关，从而实现受控比较。\n\n3.  **Hessian 评估**：在该轨迹的每个迭代点 $w_k$ 处，计算非正则化和正则化 Hessian 矩阵。logistic 风险的非正则化 Hessian 矩阵由以下公式给出：\n    $$\n    H_{\\text{unreg}}(w_k) = \\nabla^2 R(w_k) = \\frac{1}{n} X^\\top S_k X\n    $$\n    其中 $S_k$ 是一个对角矩阵，其对角线元素为 $S_{ii} = p_{k,i}(1-p_{k,i})$，且 $p_k = \\sigma(Xw_k)$。然后通过 $H_{\\text{reg}}(w_k, \\lambda) = H_{\\text{unreg}}(w_k) + \\lambda I_d$ 求得正则化 Hessian 矩阵。\n\n4.  **谱分析与度量计算**：对于在步骤 $k$ 计算的每对 Hessian 矩阵，使用针对对称矩阵的数值特征求解器计算它们的特征值，求解器会按升序返回这些值。设这些排序后的特征值为 $\\lambda_j(H_{\\text{unreg}}(w_k))$ 和 $\\lambda_j(H_{\\text{reg}}(w_k, \\lambda))$。分析的核心是计算与理论期望的偏差：\n    $$\n    d_{k,j} = \\left| \\, \\lambda_j\\big(H_{\\text{reg}}(w_k,\\lambda)\\big) - \\lambda_j\\big(H_{\\text{unreg}}(w_k)\\big) - \\lambda \\, \\right|\n    $$\n    最终报告的度量 $\\varepsilon_{\\max}$ 是这些偏差在所有特征值 $j$ 和所有 GD 步骤 $k$ 上的最大值：\n    $$\n    \\varepsilon_{\\max} = \\max_{0 \\le k  K} \\left( \\max_j d_{k,j} \\right)\n    $$\n    这个值量化了数值结果与分析理论之间的最大差异，其数量级应为机器精度。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef run_experiment(n, d, seed, lambd, K, eta, rank_deficient):\n    \"\"\"\n    Runs one instance of the numerical experiment to measure the spectral shift\n    of the Hessian due to L2 regularization.\n\n    Args:\n        n (int): Number of samples.\n        d (int): Number of features.\n        seed (int): Random seed for reproducibility.\n        lambd (float): L2 regularization strength.\n        K (int): Number of Gradient Descent steps.\n        eta (float): Learning rate for Gradient Descent.\n        rank_deficient (bool): If True, the data matrix X is made rank-deficient.\n\n    Returns:\n        float: The maximum absolute deviation, epsilon_max.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Generate synthetic dataset\n    X = np.random.randn(n, d)\n    if rank_deficient:\n        # Induce rank deficiency by making two columns identical.\n        X[:, -1] = X[:, -2]\n\n    # Sample a ground-truth parameter vector\n    w_star = np.random.randn(d, 1)\n\n    # Define logistic function with clipping for numerical stability\n    def sigma(t):\n        t = np.clip(t, -500, 500)\n        return 1 / (1 + np.exp(-t))\n\n    # Compute true probabilities and sample binary labels\n    z_true = X @ w_star\n    p_true = sigma(z_true)\n    y = (np.random.rand(n, 1)  p_true).astype(float)\n\n    # 2. Run Gradient Descent on the UNREGULARIZED risk R(w)\n    w = np.zeros((d, 1))\n    max_deviations_across_steps = []\n\n    for k in range(K):\n        # 3. At each iterate w_k, compute and analyze Hessians\n        \n        # Predictions at current iterate\n        z_k = X @ w\n        p_k = sigma(z_k)\n\n        # Compute the unregularized Hessian: H_unreg = (1/n) * X^T * S * X\n        # where S is a diagonal matrix with S_ii = p_k_i * (1 - p_k_i)\n        s_k = p_k * (1 - p_k)  # Element-wise product, shape (n, 1)\n        # Efficient computation using broadcasting: (X.T * s_k.T) is shape (d, n)\n        H_unreg = (X.T * s_k.T) @ X / n\n\n        # Compute the regularized Hessian: H_reg = H_unreg + lambda * I\n        H_reg = H_unreg + lambd * np.eye(d)\n\n        # 4. Compute eigendecompositions and find deviations\n        # linalg.eigh returns eigenvalues sorted in ascending order.\n        eigvals_unreg = linalg.eigh(H_unreg, eigvals_only=True)\n        eigvals_reg = linalg.eigh(H_reg, eigvals_only=True)\n        \n        # The theoretical spectral shift is Delta = lambda.\n        Delta = lambd\n        \n        # Deviations from the theoretical shift\n        deviations = np.abs(eigvals_reg - eigvals_unreg - Delta)\n        \n        # Store the maximum deviation for the current step k\n        if deviations.size > 0:\n            max_deviations_across_steps.append(np.max(deviations))\n        else: # Handle d=0 case, though not used in test suite\n            max_deviations_across_steps.append(0.0)\n\n        # 5. Perform GD update using the gradient of the UNREGULARIZED risk\n        grad = X.T @ (p_k - y) / n\n        w = w - eta * grad\n        \n    # Aggregate result: find the maximum deviation over all steps\n    if not max_deviations_across_steps:\n        return 0.0\n    epsilon_max = np.max(max_deviations_across_steps)\n    return epsilon_max\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, d, seed, lambda, K, eta, rank_deficient_flag)\n        (200, 5, 7, 0.05, 8, 0.1, False),   # Case A: happy path, full rank\n        (200, 5, 7, 0.0, 8, 0.1, False),    # Case B: boundary, zero penalty\n        (150, 6, 11, 0.05, 8, 0.1, True),   # Case C: edge, rank-deficient\n        (150, 6, 11, 10.0, 8, 0.1, True),   # Case D: edge, rank-deficient, large lambda\n        (60, 3, 3, 0.2, 10, 0.2, False)     # Case E: small-scale check\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_experiment(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120548"}]}