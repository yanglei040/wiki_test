{"hands_on_practices": [{"introduction": "皮尔逊相关系数是衡量两个变量之间线性关系的常用工具，但其应用范围有限。当变量间的关系是非线性的，相关系数可能会给出误导性的结论，例如值为零。本练习旨在通过一个思想实验，挑战您对统计依赖性的理解：当两个变量的皮尔逊相关系数接近于零，而互信息却很高时，这揭示了何种深刻的非线性结构？通过分析这一情景，您将建立起对更普适依赖度量需求的直观认识[@problem_id:1462533]。", "problem": "一位生物信息学家正在分析来自一个细胞群的大规模基因表达数据集，以推断调控网络。他们专注于两个特定基因的表达水平，我们称之为基因Alpha（$G_{\\alpha}$）和基因Beta（$G_{\\beta}$）。基因的表达水平是其活性的度量，量化为其相应信使RNA的浓度。\n\n统计分析得出两个关键结果：\n1. 皮尔逊相关系数，用于衡量两个变量之间*线性*关系的强度，计算得出其值对于$G_{\\alpha}$和$G_{\\beta}$的表达水平而言约等于零（$r \\approx 0$）。\n2. 互信息，用于衡量两个变量之间的一般统计依赖性（包括线性和非线性），发现对于同一对基因而言，其值非常高。高互信息值表明，知道一个基因的表达水平可以提供大量关于另一个基因表达水平的信息。\n\n基于这两项统计观察，以下哪种生物学情景最能合理解释基因Alpha和基因Beta之间的关系？\n\nA. 由$G_{\\alpha}$产生的蛋白质是一个简单的转录因子，它直接且线性地激活$G_{\\beta}$的表达。\n\nB. $G_{\\alpha}$和$G_{\\beta}$的表达彼此完全独立。\n\nC. 由$G_{\\alpha}$产生的蛋白质是一个简单的转录因子，它直接且线性地抑制$G_{\\beta}$的表达。\n\nD. 由$G_{\\alpha}$产生的蛋白质对$G_{\\beta}$具有复杂的、非单调的调控效应。例如，它可能在低浓度时适度激活$G_{\\beta}$，但在高浓度时强烈抑制它，从而形成一种高度结构化但非线性的关系。\n\nE. 实验中的技术错误导致$G_{\\alpha}$的测量数据被随机打乱，使得任何真实关系都无法被检测到。", "solution": "设$X$表示$G_{\\alpha}$的表达水平，$Y$表示$G_{\\beta}$的表达水平。皮尔逊相关系数为\n$$\nr=\\frac{\\operatorname{Cov}(X,Y)}{\\sigma_{X}\\sigma_{Y}},\n$$\n因此$r\\approx 0$意味着$\\operatorname{Cov}(X,Y)\\approx 0$，这排除了强线性关联的可能性，但并不意味着独立。\n\n互信息为\n$$\nI(X;Y)=\\iint p_{X,Y}(x, y)\\,\\ln\\!\\left(\\frac{p_{X,Y}(x, y)}{p_{X}(x)p_{Y}(y)}\\right)\\,\\mathrm{d}x\\,\\mathrm{d}y.\n$$\n$I(X;Y)$的显著高值表明$X$和$Y$之间存在显著的统计依赖性（线性或非线性）；特别地，$I(X;Y)=0$当且仅当$X$和$Y$是独立的。\n\n根据这两个约束条件评估各个选项：\n\n1. 选项A和C假设存在一种简单的直接线性调控效应，可以建模为$Y=aX+b+\\varepsilon$，其中$a>0$（激活）或$a<0$（抑制），$\\varepsilon$是独立于$X$的噪声。在这种情况下，只要信号项$aX$不可忽略，$\\operatorname{Cov}(X,Y)$的符号就与$a$相同，且$|r|$的量级通常很大。这与观察到的$r\\approx 0$相矛盾。此外，如果噪声大到足以使$r\\approx 0$，那么互信息也应随之降低，而不是显著地高。因此，A和C与($r \\approx 0$, $I(X;Y)$ 高)这对观察结果不一致。\n\n2. 选项B断言独立性。独立性意味着$I(X;Y)=0$，这与观察到的高互信息相矛盾。因此B是无效的。\n\n3. 选项E描述了对$X$测量值的随机打乱，这会破坏任何依赖关系，同样会导致$I(X;Y)\\approx 0$，再次与观察到的高互信息相矛盾。因此E是无效的。\n\n4. 选项D假设存在一种复杂的、非单调的调控关系。一个典型的例子是双相或U形依赖关系，例如$Y=f(X)$，其中$f$是非单调且对称的。例如，如果$f(x)=x^2$并且$X$服从一个均值为零且方差有限的对称分布，那么\n$$\n\\operatorname{Cov}(X,Y)=\\operatorname{Cov}(X,X^{2})=\\mathbb{E}[X^3]-\\mathbb{E}[X]\\mathbb{E}[X^2]=0,\n$$\n所以$r=0$，而$X$和$Y$显然是相关的。事实上，对于一个确定性的非恒定映射$Y=f(X)$，我们有$I(X;Y)=H(Y)>0$，因为$H(Y|X)=0$，所以尽管线性相关为零，互信息仍然可以很大。在生物学上，一个在低浓度时轻度激活、在高浓度时又进行抑制的调控因子，会产生这样一种结构化的、非线性的、非单调的关系，这与观察结果相符。\n\n因此，唯一与$r\\approx 0$和高$I(X;Y)$一致的选项是，$G_{\\alpha}$对$G_{\\beta}$施加了一种复杂的、非单调的调控效应。", "answer": "$$\\boxed{D}$$", "id": "1462533"}, {"introduction": "在认识到线性相关性的局限后，我们自然会问：如何量化任意形式的统计依赖关系？本实践将指导您实现两种强大的非参数工具：距离相关性（distance correlation）与希尔伯特-施密特独立性准则（HSIC）。这些方法无需对数据关系的形式做任何预设，能够捕捉从简单线性到复杂非线性的各种依赖结构。您不仅将通过编码比较它们在不同数据集上的表现，还将从第一性原理出发，推导两者在特定条件下的数学联系，从而深刻理解理论与实践的结合[@problem_id:3149079]。", "problem": "您的任务是实现并比较两种在深度学习中常用的统计依赖性度量：距离相关性（distance correlation）和希尔伯特-施密特独立性准则（Hilbert-Schmidt Independence Criterion, HSIC）。目标是从第一性原理出发，推导这些度量在何种条件下一致，并使用一个完整的、可运行的程序在非线性合成数据上测试它们的行为。\n\n定义与计算任务：\n- 设 $\\{(x_i,y_i)\\}_{i=1}^n$ 为 $n$ 组成对的观测值，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\mathbb{R}$。定义中心化矩阵 $H = I_n - \\frac{1}{n}\\mathbf{1}\\mathbf{1}^\\top$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{1}$ 是 $n$ 维的全1向量。\n- 距离相关性：\n  - 对于指数 $\\alpha \\in (0,2]$，定义成对距离矩阵 $D^{(\\alpha)}_X$ 和 $D^{(\\alpha)}_Y$ 分别为 $(D^{(\\alpha)}_X)_{ij} = \\|x_i - x_j\\|^\\alpha$ 和 $(D^{(\\alpha)}_Y)_{ij} = \\|y_i - y_j\\|^\\alpha$。计算它们的双重中心化形式 $A_X = H D^{(\\alpha)}_X H$ 和 $A_Y = H D^{(\\alpha)}_Y H$。\n  - 样本距离协方差的平方为 $$d\\mathrm{Cov}^2_{\\alpha}(X,Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (A_X)_{ij} (A_Y)_{ij}$$。样本距离协方差为 $d\\mathrm{Cov}_{\\alpha}(X,Y) = \\sqrt{d\\mathrm{Cov}^2_{\\alpha}(X,Y)}$。\n  - 样本距离方差为 $d\\mathrm{Var}_{\\alpha}(X) = d\\mathrm{Cov}_{\\alpha}(X,X)$，类似地，$d\\mathrm{Var}_{\\alpha}(Y) = d\\mathrm{Cov}_{\\alpha}(Y,Y)$。样本距离相关性为 $d\\mathrm{Cor}_{\\alpha}(X,Y) = \\frac{d\\mathrm{Cov}_{\\alpha}(X,Y)}{\\sqrt{d\\mathrm{Var}_{\\alpha}(X)\\, d\\mathrm{Var}_{\\alpha}(Y)}}$，约定如果分母为 $0$，则 $d\\mathrm{Cor}_{\\alpha}(X,Y)$ 为 $0$。\n- 希尔伯特-施密特独立性准则 (HSIC)：\n  - 考虑在 $\\mathcal{X}$ 上的再生核希尔伯特空间 (RKHS) 核函数 $k_X$ 和在 $\\mathcal{Y}$ 上的 $k_Y$。通过 $K_{ij} = k_X(x_i,x_j)$ 和 $L_{ij} = k_Y(y_i,y_j)$ 构建Gram矩阵 $K$ 和 $L$，然后通过 $K_c = H K H$ 和 $L_c = H L H$ 将它们中心化。\n  - 有偏样本HSIC为 $$\\mathrm{HSIC}(X,Y;k_X,k_Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (K_c)_{ij} (L_c)_{ij}$$\n  - 需要实现的核函数：\n    1. 线性核：$k_{\\mathrm{lin}}(x,x') = x x'$。\n    2. 高斯核：$k_{\\mathrm{gauss}}(x,x';\\sigma) = \\exp\\!\\left(-\\frac{\\|x-x'\\|^2}{2\\sigma^2}\\right)$，带宽参数为 $\\sigma > 0$。\n    3. 拉普拉斯核：$k_{\\mathrm{lap}}(x,x';\\beta) = \\exp\\!\\left(-\\frac{\\|x-x'\\|}{\\beta}\\right)$，尺度参数为 $\\beta > 0$。\n\n推导要求：\n- 从上述定义出发，用代数方法推导条件，在此条件下，使用线性核的 $\\mathrm{HSIC}$ 与指数 $\\alpha = 2$ 的平方距离协方差一致（相差一个常数因子）。具体来说，证明当 $k_X$ 和 $k_Y$ 是 $\\mathbb{R}$ 上的线性核时，有 $H D^{(2)}_X H = -2 H K H$ 和 $H D^{(2)}_Y H = -2 H L H$，这意味着 $d\\mathrm{Cov}^2_{2}(X,Y) = 4\\,\\mathrm{HSIC}(X,Y;k_{\\mathrm{lin}},k_{\\mathrm{lin}})$。您的程序应通过数值方法验证此比例关系，对每个测试用例，在严格的容差范围内使用布尔检查进行验证。\n\n合成数据与参数（为保证可复现性，请使用以下确切设置）：\n- 所有角度均为实数值；不涉及角度单位。\n- 不涉及物理单位。\n- 使用以下包含5个数据集的测试套件，每个数据集都有固定的随机种子和样本量：\n  1. 独立噪声：$n = 300$，种子 $= 42$。独立抽取 $x_i \\sim \\mathcal{N}(0,1)$ 和 $y_i \\sim \\mathcal{N}(0,1)$，其中 $i=1,\\dots,n$。\n  2. 带噪声的线性依赖：$n = 300$，种子 $= 123$。抽取 $x_i \\sim \\mathcal{N}(0,1)$，且 $y_i = 2 x_i + 0.1 \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 且与 $x_i$ 独立。\n  3. 非线性正弦依赖：$n = 300$，种子 $= 7$。抽取 $x_i \\sim \\mathrm{Uniform}(-3,3)$，且 $y_i = \\sin(x_i) + 0.2 \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 且与 $x_i$ 独立。\n  4. 非线性二次，小样本（边界情况）：$n = 5$，种子 $= 999$。抽取 $x_i \\sim \\mathrm{Uniform}(-2,2)$，且 $y_i = x_i^2 + 0.05 \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$。\n  5. 恒定响应（边缘情况）：$n = 100$，种子 $= 2024$。抽取 $x_i \\sim \\mathcal{N}(0,1)$，并对所有 $i$ 设置 $y_i = 0$。\n\n待比较的核参数：\n- 高斯核带宽 $\\sigma = 0.5$ 和 $\\sigma = 2.0$。\n- 拉普拉斯核尺度 $\\beta = 0.5$。\n\n程序要求：\n- 对每个数据集，计算：\n  1. $d\\mathrm{Cor}_{1}(X,Y)$。\n  2. $d\\mathrm{Cov}^2_{2}(X,Y)$。\n  3. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{lin}},k_{\\mathrm{lin}})$。\n  4. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}0.5),k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}0.5))$。\n  5. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}2.0),k_{\\mathrm{gauss}}(\\cdot;\\sigma{=}2.0))$。\n  6. $\\mathrm{HSIC}(X,Y;k_{\\mathrm{lap}}(\\cdot;\\beta{=}0.5),k_{\\mathrm{lap}}(\\cdot;\\beta{=}0.5))$。\n  7. 一个布尔值，指示在容差 $\\mathrm{rtol} = 10^{-10}$ 和 $\\mathrm{atol} = 10^{-10}$ 内，$4 \\times \\mathrm{HSIC}(X,Y;k_{\\mathrm{lin}},k_{\\mathrm{lin}})$ 是否在数值上等于 $d\\mathrm{Cov}^2_{2}(X,Y)$。\n- 数值稳定性：如果 $d\\mathrm{Var}_{1}(Y) = 0$ 或 $d\\mathrm{Var}_{1}(X) = 0$，则对该数据集返回 $d\\mathrm{Cor}_{1}(X,Y) = 0$。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个包含5个内部列表的列表（每个数据集一个）。每个内部列表必须按给定顺序包含上述7个值，形式为用方括号括起来的逗号分隔列表。例如，一个有效的输出格式为 `[[v_{1,1},...,v_{1,7}],[v_{2,1},...,v_{2,7}],...,[v_{5,1},...,v_{5,7}]]`, 其中每个 $v_{i,j}$ 是布尔值或浮点数。", "solution": "用户提供的问题具有科学依据、提法明确，并包含了完整解答所需的所有必要信息。距离相关性和希尔伯特-施密特独立性准则 (HSIC) 的定义是统计学和机器学习领域的标准定义。关于推导平方距离协方差与使用线性核的HSIC之间关系的请求，是一项有效且具有指导意义的理论练习。数值部分被详细说明，包括数据集、参数和随机种子，以确保可复现性。因此，该问题被认定为有效。\n\n根据要求，解决方案包含两部分：一个形式化的数学推导和一个用于数值验证的完整 Python 程序。\n\n### 第一部分：推导\n\n目标是在指数 $\\alpha=2$ 的样本距离协方差的平方（记为 $d\\mathrm{Cov}^2_{2}(X,Y)$）和使用线性核的样本希尔伯特-施密特独立性准则（$\\mathrm{HSIC}(X,Y; k_{\\mathrm{lin}}, k_{\\mathrm{lin}})$）之间建立直接的比例关系。我们将从为 $\\mathbb{R}$ 中数据提供的定义开始。\n\n设我们的数据为 $n$ 对 $\\{(x_i, y_i)\\}_{i=1}^n$，其中每个 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\mathbb{R}$。设数据向量为 $\\mathbf{x} = (x_1, \\dots, x_n)^\\top$ 和 $\\mathbf{y} = (y_1, \\dots, y_n)^\\top$。\n\n**步骤1：分析距离矩阵 $D^{(2)}_X$。**\n指数 $\\alpha=2$ 的平方距离协方差涉及成对距离矩阵 $D^{(2)}_X$，其元素定义为 $(D^{(2)}_X)_{ij} = \\|x_i - x_j\\|^2$。由于 $x_i, x_j \\in \\mathbb{R}$，这可以简化为：\n$$ (D^{(2)}_X)_{ij} = (x_i - x_j)^2 = x_i^2 - 2x_i x_j + x_j^2 $$\n我们可以用数据向量 $\\mathbf{x}$ 来表示矩阵 $D^{(2)}_X$。设 $\\mathbf{x}^{\\circ 2}$ 为逐元素平方的向量，即 $\\mathbf{x}^{\\circ 2} = (x_1^2, x_2^2, \\dots, x_n^2)^\\top$。设 $\\mathbf{1}$ 为 $n$ 维全1列向量。\n对于所有 $j=1, \\dots, n$，项 $x_i^2$ 对应于矩阵 $\\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top$。\n对于所有 $i=1, \\dots, n$，项 $x_j^2$ 对应于矩阵 $\\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top$。\n项 $-2x_i x_j$ 对应于矩阵 $-2 \\mathbf{x} \\mathbf{x}^\\top$。\n因此，距离矩阵可以写为：\n$$ D^{(2)}_X = \\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top + \\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top - 2 \\mathbf{x} \\mathbf{x}^\\top $$\n\n**步骤2：将 $D^{(2)}_X$ 与线性核的Gram矩阵 $K$ 联系起来。**\n对于线性核 $k_{\\mathrm{lin}}(x, x') = x x'$，Gram矩阵 $K$ 由 $K_{ij} = x_i x_j$ 给出。以矩阵形式表示，即 $K = \\mathbf{x} \\mathbf{x}^\\top$。\n将此代入 $D^{(2)}_X$ 的表达式中，我们得到：\n$$ D^{(2)}_X = \\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top + \\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top - 2 K $$\n\n**步骤3：应用双重中心化操作。**\n双重中心化距离矩阵定义为 $A_X = H D^{(2)}_X H$，其中 $H = I_n - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^\\top$ 是中心化矩阵。$H$ 的一个关键性质是它会消除常数向量，即 $H \\mathbf{1} = \\mathbf{0}$ 和 $\\mathbf{1}^\\top H = \\mathbf{0}^\\top$。\n将此应用于我们的 $D^{(2)}_X$ 表达式：\n$$ A_X = H \\left( \\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top + \\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top - 2 K \\right) H $$\n使用矩阵乘法的分配律：\n$$ A_X = H(\\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top)H + H(\\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top)H - 2 H K H $$\n现在我们使用性质 $H\\mathbf{1}=\\mathbf{0}$ 来计算前两项：\n$$ H(\\mathbf{x}^{\\circ 2} \\mathbf{1}^\\top)H = (H \\mathbf{x}^{\\circ 2})(\\mathbf{1}^\\top H) = (H \\mathbf{x}^{\\circ 2}) \\mathbf{0}^\\top = \\mathbf{0} $$\n$$ H(\\mathbf{1} (\\mathbf{x}^{\\circ 2})^\\top)H = (H \\mathbf{1})((\\mathbf{x}^{\\circ 2})^\\top H) = \\mathbf{0} ((\\mathbf{x}^{\\circ 2})^\\top H) = \\mathbf{0} $$\n前两项是零矩阵。这使得我们得到：\n$$ A_X = -2 H K H $$\n根据定义，项 $H K H$ 是中心化的Gram矩阵 $K_c$。因此，我们证明了：\n$$ A_X = H D^{(2)}_X H = -2 K_c $$\n对于变量 $Y$，一个相同的推导也成立，得出 $A_Y = H D^{(2)}_Y H = -2 L_c$，其中 $L_c = H L H$ 且 $L$ 是 $Y$ 使用线性核的Gram矩阵。\n\n**步骤4：建立最终的比例关系。**\n样本距离协方差的平方定义为：\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (A_X)_{ij} (A_Y)_{ij} $$\n代入我们关于 $A_X$ 和 $A_Y$ 的结果：\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (-2 (K_c)_{ij}) (-2 (L_c)_{ij}) $$\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = \\frac{4}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (K_c)_{ij} (L_c)_{ij} $$\n有偏样本 HSIC 的定义是：\n$$ \\mathrm{HSIC}(X,Y; k_X, k_Y) = \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n (K_c)_{ij} (L_c)_{ij} $$\n通过比较这两个表达式，我们得出期望的结论：\n$$ d\\mathrm{Cov}^2_{2}(X,Y) = 4\\,\\mathrm{HSIC}(X,Y; k_{\\mathrm{lin}}, k_{\\mathrm{lin}}) $$\n推导至此完成。对于一维数据，指数 $\\alpha=2$ 的平方距离协方差正好是使用线性核的 HSIC 的四倍。这个关系将在实现中进行数值验证。", "answer": "```python\nimport numpy as np\n\ndef generate_data(case_id, n, seed):\n    \"\"\"Generates synthetic data for a given test case.\"\"\"\n    rng = np.random.default_rng(seed)\n    if case_id == 1:  # Independent noise\n        x = rng.normal(0, 1, size=n)\n        y = rng.normal(0, 1, size=n)\n    elif case_id == 2:  # Linear dependence\n        x = rng.normal(0, 1, size=n)\n        eps = rng.normal(0, 1, size=n)\n        y = 2 * x + 0.1 * eps\n    elif case_id == 3:  # Nonlinear sine dependence\n        x = rng.uniform(-3, 3, size=n)\n        eps = rng.normal(0, 1, size=n)\n        y = np.sin(x) + 0.2 * eps\n    elif case_id == 4:  # Nonlinear quadratic\n        x = rng.uniform(-2, 2, size=n)\n        eps = rng.normal(0, 1, size=n)\n        y = x**2 + 0.05 * eps\n    elif case_id == 5:  # Constant response\n        x = rng.normal(0, 1, size=n)\n        y = np.zeros(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n    return x.reshape(-1, 1), y.reshape(-1, 1)\n\ndef double_center(M):\n    \"\"\"Performs double centering on a matrix M efficiently.\"\"\"\n    mean_r = M.mean(axis=1, keepdims=True)\n    mean_c = M.mean(axis=0, keepdims=True)\n    mean_all = M.mean()\n    return M - mean_r - mean_c + mean_all\n\ndef d_cov_sq(X, Y, alpha):\n    \"\"\"Computes the squared sample distance covariance.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n    \n    dist_X = np.abs(X - X.T)\n    dist_Y = np.abs(Y - Y.T)\n\n    D_X_alpha = dist_X ** alpha\n    D_Y_alpha = dist_Y ** alpha\n\n    A_X = double_center(D_X_alpha)\n    A_Y = double_center(D_Y_alpha)\n    \n    return np.sum(A_X * A_Y) / (n ** 2)\n\ndef d_cor(X, Y, alpha):\n    \"\"\"Computes the sample distance correlation.\"\"\"\n    d_cov_sq_xy = d_cov_sq(X, Y, alpha)\n    d_cov_sq_xx = d_cov_sq(X, X, alpha)\n    d_cov_sq_yy = d_cov_sq(Y, Y, alpha)\n\n    d_cov_xy = np.sqrt(max(0, d_cov_sq_xy))\n    d_var_x = np.sqrt(max(0, d_cov_sq_xx))\n    d_var_y = np.sqrt(max(0, d_cov_sq_yy))\n\n    denom = np.sqrt(d_var_x * d_var_y)\n    \n    if np.isclose(denom, 0.0):\n        return 0.0\n    \n    return d_cov_xy / denom\n\ndef kernel_linear(X1, X2):\n    \"\"\"Linear kernel for (n, 1) data.\"\"\"\n    return X1 @ X2.T\n\ndef kernel_gaussian(X1, X2, sigma):\n    \"\"\"Gaussian RBF kernel for (n, 1) data.\"\"\"\n    dist_sq = (X1 - X2.T)**2\n    return np.exp(-dist_sq / (2 * sigma**2))\n\ndef kernel_laplacian(X1, X2, beta):\n    \"\"\"Laplacian kernel for (n, 1) data.\"\"\"\n    dist = np.abs(X1 - X2.T)\n    return np.exp(-dist / beta)\n\ndef hsic(X, Y, kernel, kernel_params={}):\n    \"\"\"Computes the biased sample HSIC.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n        \n    K = kernel(X, X, **kernel_params)\n    L = kernel(Y, Y, **kernel_params)\n    \n    K_c = double_center(K)\n    L_c = double_center(L)\n    \n    return np.sum(K_c * L_c) / (n ** 2)\n\ndef solve():\n    \"\"\"Main function to run the analysis.\"\"\"\n    test_cases = [\n        {'id': 1, 'n': 300, 'seed': 42},\n        {'id': 2, 'n': 300, 'seed': 123},\n        {'id': 3, 'n': 300, 'seed': 7},\n        {'id': 4, 'n': 5, 'seed': 999},\n        {'id': 5, 'n': 100, 'seed': 2024},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        X, Y = generate_data(case['id'], case['n'], case['seed'])\n        \n        # 1. dCor_1(X, Y)\n        dcor1 = d_cor(X, Y, alpha=1)\n        \n        # 2. dCov^2_2(X, Y)\n        dcov_sq2 = d_cov_sq(X, Y, alpha=2)\n        \n        # 3. HSIC with linear kernel\n        hsic_lin = hsic(X, Y, kernel_linear)\n        \n        # 4. HSIC with Gaussian kernel (sigma=0.5)\n        hsic_gauss_05 = hsic(X, Y, kernel_gaussian, kernel_params={'sigma': 0.5})\n        \n        # 5. HSIC with Gaussian kernel (sigma=2.0)\n        hsic_gauss_20 = hsic(X, Y, kernel_gaussian, kernel_params={'sigma': 2.0})\n\n        # 6. HSIC with Laplacian kernel (beta=0.5)\n        hsic_lap_05 = hsic(X, Y, kernel_laplacian, kernel_params={'beta': 0.5})\n\n        # 7. Numerical check of the derived proportionality\n        is_proportional = np.allclose(dcov_sq2, 4 * hsic_lin, rtol=1e-10, atol=1e-10)\n\n        case_results = [\n            dcor1,\n            dcov_sq2,\n            hsic_lin,\n            hsic_gauss_05,\n            hsic_gauss_20,\n            hsic_lap_05,\n            is_proportional\n        ]\n        all_results.append(case_results)\n    \n    # Print the final list of lists as a string\n    print(str(all_results))\n\nsolve()\n```", "id": "3149079"}, {"introduction": "本章的最终实践将统计依赖性的概念应用于深度学习的前沿领域：表示学习。我们将从衡量两个变量间的依赖，拓展到衡量一组变量内部的总体相互依赖性，即总相关性（Total Correlation）。这个练习模拟了在β-变分自编码器（β-VAE）中，如何通过惩罚潜变量的总相关性来鼓励模型学习“解耦”的、有意义的特征表示。通过亲手实现并观察解耦度量（如MIG）的变化，您将具体地理解统计依赖度量在塑造深度模型内部表征中的关键作用[@problem_id:3149107]。", "problem": "您将执行一项模拟任务，研究一种名为“总相关性”的统计依赖性度量在深度学习潜变量背景下及其与解耦的关系。考虑一个由 β-变分自编码器（β-Variational Autoencoder (β-VAE)）的假设编码器生成的、包含 $d$ 个变量 $Z = (Z_1, Z_2, Z_3)$ 的潜表示。该编码器的聚合后验被建模为独立真实因子的线性高斯映射。设 $V = (V_1, V_2, V_3)$ 是一个独立因子向量，其中每个 $V_i$ 服从标准正态分布。定义一个混合矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$ 和加性高斯噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2))$。潜变量由 $N$ 个独立样本生成，公式为 $Z = A V + \\varepsilon$。\n\n需要估计的统计依赖性度量是总相关性 $TC(Z)$，它通过微分熵从第一性原理定义为\n$$TC(Z) = \\sum_{i=1}^{3} H(Z_i) - H(Z),$$\n其中 $H(\\cdot)$ 表示微分熵。在经过充分检验的高斯近似下，一个协方差为 $\\Sigma$ 的 $d$ 维多元正态分布的微分熵为\n$$H(Z) = \\frac{1}{2} \\log\\!\\big( (2\\pi e)^d \\det(\\Sigma) \\big),$$\n而方差为 $\\sigma_i^2$ 的标量正态分布的边缘熵为\n$$H(Z_i) = \\frac{1}{2} \\log\\!\\big( 2\\pi e \\, \\sigma_i^2 \\big).$$\n这就得出了一个关于 $Z$ 的样本协方差矩阵 $\\Sigma_Z$ 及其对角元素的可计算表达式 $TC(Z)$。\n\n为了将 $TC(Z)$ 与解耦联系起来，您还需要计算潜变量与真实因子之间的互信息间隙（Mutual Information Gap (MIG)）。对于每个因子 $V_k$ 和每个潜变量 $Z_i$，在联合高斯模型下，使用 $Z_i$ 和 $V_k$ 之间的相关系数 $\\rho_{ik}$ 来估计互信息 $I(Z_i; V_k)$：\n$$I(Z_i; V_k) = -\\frac{1}{2} \\log(1 - \\rho_{ik}^2).$$\nMIG 则定义为所有因子中最大互信息值与第二大互信息值之间差值的平均值：\n$$\\mathrm{MIG} = \\frac{1}{3} \\sum_{k=1}^{3} \\Big( \\max_{i} I(Z_i; V_k) - \\max_{i \\neq i^*(k)} I(Z_i; V_k) \\Big),$$\n其中 $i^*(k)$ 是因子 $k$ 达到最大值时的索引。\n\n为了模拟类似于在 β-VAE 中增加 $\\beta$ 参数的总相关性惩罚，对 $Z$ 应用一个部分白化变换，以逐步消除相关性：\n$$Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2} \\, (Z - \\mathbb{E}[Z]),$$\n其中 $\\alpha \\in [0,1]$ 是一个惩罚强度。当 $\\alpha = 0$ 时，$Z^{(0)} = Z - \\mathbb{E}[Z]$；当 $\\alpha = 1$ 时，$Z^{(1)}$ 被完全白化并具有单位协方差，这在理想情况下、高斯假设下会使 $TC(Z)$ 最小化。您将计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$，并将其与原始的 $TC(Z)$ 和 $\\mathrm{MIG}(Z, V)$ 进行比较。\n\n所有信息量必须以奈特（自然对数底）为单位表示。\n\n实现一个完整、可运行的程序，该程序能够：\n- 模拟 $N$ 个 $V \\sim \\mathcal{N}(0, I_3)$ 的样本。\n- 使用指定的对角标准差，通过 $Z = A V + \\varepsilon$ 生成潜变量，其中 $\\varepsilon$ 是独立高斯噪声。\n- 使用高斯微分熵公式，根据 $Z$ 的样本协方差计算 $TC(Z)$。\n- 使用联合高斯互信息公式 $I(Z_i; V_k) = -\\frac{1}{2}\\log(1-\\rho_{ik}^2)$ 计算 $\\mathrm{MIG}(Z, V)$，其中 $\\rho_{ik}$ 是 $Z_i$ 和 $V_k$ 之间的经验相关性。\n- 对指定的 $\\alpha$ 值应用部分白化变换 $Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2}(Z - \\mathbb{E}[Z])$，并重新计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$。\n\n使用以下参数值的测试套件来覆盖不同的情况：\n- 测试用例 1 (理想路径): \n$$ A = \\begin{bmatrix} 1.0 & 0.6 & 0.0 \\\\ 0.4 & 1.0 & 0.5 \\\\ 0.0 & 0.3 & 1.0 \\end{bmatrix} $$\n噪声标准差 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.1, 0.1, 0.1)$, $N = 20000$, $\\alpha \\in \\{0.5, 1.0\\}$。\n- 测试用例 2 (边界情况，已分解): \n$$ A = I_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$\n噪声标准差 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.05, 0.05, 0.05)$, $N = 20000$, $\\alpha \\in \\{0.5, 1.0\\}$。\n- 测试用例 3 (边缘情况，强混合): \n$$ A = \\begin{bmatrix} 1.0 & 1.1 & 1.2 \\\\ 0.9 & 1.0 & 1.1 \\\\ 1.1 & 1.2 & 1.3 \\end{bmatrix} $$\n噪声标准差 $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.2, 0.2, 0.2)$, $N = 20000$, $\\alpha \\in \\{0.5, 1.0\\}$。\n\n对于每个测试用例，您的程序必须输出一个列表，其中包含六个以奈特为单位的浮点数：\n$[TC(Z), \\mathrm{MIG}(Z,V), TC(Z^{(0.5)}), \\mathrm{MIG}(Z^{(0.5)},V), TC(Z^{(1.0)}), \\mathrm{MIG}(Z^{(1.0)},V)]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表包含在方括号中，没有空格。每个元素对应一个测试用例的六个数字列表。例如，结构必须是 `[[r_{1,1},r_{1,2},r_{1,3},r_{1,4},r_{1,5},r_{1,6}],[r_{2,1},...,r_{2,6}],[r_{3,1},...,r_{3,6}]]`。\n-所有值必须以奈特（自然对数底）为单位。\n- 不需要用户输入；使用固定的随机种子以确保可复现性。", "solution": "这个问题是有效的，因为它在信息论和统计学方面有科学依据，定义明确且包含所有必要的参数和定义，并提出了一个与深度学习研究相关的、计算上可行的模拟任务。\n\n该问题要求模拟一个线性高斯潜变量模型 $Z = AV + \\varepsilon$，并计算两个关键指标：总相关性（$TC$）和互信息间隙（$MIG$）。然后，在对潜变量应用部分白化变换后，重新评估这些指标。整个过程将针对三个不同的测试用例实施。\n\n首先，我们来确定模型的统计特性。真实因子 $V$ 是一个 $d=3$ 维的随机向量，服从标准多元正态分布，$V \\sim \\mathcal{N}(0, I_3)$，其中 $I_3$ 是 $3 \\times 3$ 的单位矩阵。加性噪声 $\\varepsilon$ 也是一个 $d=3$ 维的高斯向量，$\\varepsilon \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$，其中 $\\Sigma_\\varepsilon = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2, \\sigma_3^2)$ 是一个对角协方差矩阵。由于 $Z$ 是独立高斯向量 $V$ 和 $\\varepsilon$ 的线性变换，因此 $Z$ 本身也是一个多元高斯变量。\n$Z$ 的均值为 $\\mathbb{E}[Z] = \\mathbb{E}[AV + \\varepsilon] = A\\mathbb{E}[V] + \\mathbb{E}[\\varepsilon] = A \\cdot 0 + 0 = 0$。\n$Z$ 的协方差，记为 $\\Sigma_Z$，是：\n$$ \\Sigma_Z = \\operatorname{Cov}(AV + \\varepsilon) = \\operatorname{Cov}(AV) + \\operatorname{Cov}(\\varepsilon) $$\n由于 $V$ 和 $\\varepsilon$ 的独立性。各项为：\n$$ \\operatorname{Cov}(AV) = A \\operatorname{Cov}(V) A^T = A I_3 A^T = AA^T $$\n$$ \\operatorname{Cov}(\\varepsilon) = \\Sigma_\\varepsilon $$\n因此，$Z$ 的理论协方差为 $\\Sigma_Z = AA^T + \\Sigma_\\varepsilon$。在模拟中，我们将使用从 $N$ 个生成样本中估计出的样本协方差矩阵 $\\hat{\\Sigma}_Z$。\n\n第一个指标，总相关性，衡量 $Z$ 各分量之间的总统计依赖性。它被定义为联合分布 $p(Z)$ 与其边缘分布的乘积 $\\prod_i p(Z_i)$ 之间的库尔贝克-莱布勒散度。对于连续变量，这用微分熵表示为：\n$$ TC(Z) = \\sum_{i=1}^{3} H(Z_i) - H(Z) $$\n在高斯假设下，协方差为 $\\Sigma$ 的 $d$ 维变量的微分熵为 $H(Z) = \\frac{1}{2} \\log\\!\\big( (2\\pi e)^d \\det(\\Sigma) \\big)$，而方差为 $\\sigma_{Z_i}^2$ 的单个分量 $Z_i$ 的熵为 $H(Z_i) = \\frac{1}{2} \\log\\!\\big( 2\\pi e \\, \\sigma_{Z_i}^2 \\big)$。方差 $\\sigma_{Z_i}^2$ 对应于协方差矩阵 $\\Sigma_Z$ 的第 $i$ 个对角元素，即 $\\sigma_{Z_i}^2 = (\\Sigma_Z)_{ii}$。\n将这些代入 $TC$ 公式并简化，我们得到了一个基于协方差矩阵的可计算表达式：\n$$ TC(Z) = \\frac{1}{2} \\left[ \\log\\left(\\prod_{i=1}^{3} (\\Sigma_Z)_{ii}\\right) - \\log(\\det(\\Sigma_Z)) \\right] $$\n这个公式将应用于样本协方差矩阵 $\\hat{\\Sigma}_Z$。\n\n第二个指标，互信息间隙，量化了学习到的潜表示 $Z$ 与真实因子 $V$ 之间的解耦程度。它需要计算每个潜变量 $Z_i$ 和每个因子 $V_k$ 之间的成对互信息 $I(Z_i; V_k)$。由于 $(Z, V)$ 的联合分布是高斯的，互信息可以根据它们的相关系数 $\\rho_{ik} = \\operatorname{Corr}(Z_i, V_k)$ 计算得出：\n$$ I(Z_i; V_k) = -\\frac{1}{2} \\log(1 - \\rho_{ik}^2) $$\n相关系数将根据生成的 $Z$ 和 $V$ 样本进行经验估计。\n然后，$MIG$ 通过对所有因子 $V_k$ 求平均值来计算，这个平均值是最大互信息值与第二大互信息值之间的差：\n$$ \\mathrm{MIG} = \\frac{1}{3} \\sum_{k=1}^{3} \\Big( \\max_{i} I(Z_i; V_k) - \\max_{i \\neq i^*(k)} I(Z_i; V_k) \\Big) $$\n其中 $i^*(k)$ 是与因子 $V_k$ 具有最大互信息的潜变量的索引。高 $MIG$ 值表示每个因子都被一个单一、独立的潜变量清晰地捕获。\n\n研究的核心涉及到一个部分白化变换，旨在减少 $Z$ 内部的相关性。变换后的变量 $Z^{(\\alpha)}$ 由下式给出：\n$$ Z^{(\\alpha)} = \\Sigma_Z^{-\\alpha/2} (Z - \\mathbb{E}[Z]) $$\n其中 $\\alpha \\in [0, 1]$ 是惩罚强度。矩阵幂 $\\Sigma_Z^{-\\alpha/2}$ 使用 $\\Sigma_Z$ 的特征分解来计算。如果 $\\Sigma_Z = UDU^T$，其中 $D$ 是特征值的对角矩阵，$U$ 是特征向量矩阵，那么 $\\Sigma_Z^p = UD^p U^T$。在实现中，我们将使用样本协方差 $\\hat{\\Sigma}_Z$ 和样本均值 $\\bar{Z}$，并数值计算变换矩阵。`scipy.linalg.fractional_matrix_power` 函数提供了一种稳健的计算方法。\n将变换应用于生成的样本 $Z$ 以获得 $Z^{(\\alpha)}$ 的样本后，我们将使用上述相同的方法重新计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$。对于 $\\alpha=0.0$，$Z^{(0)}$ 只是 $Z$ 的中心化版本，因此 $TC(Z^{(0)}) = TC(Z)$ 且 $\\mathrm{MIG}(Z^{(0)},V) = \\mathrm{MIG}(Z,V)$。对于 $\\alpha=1.0$，变量 $Z^{(1)}$被完全白化，其协方差矩阵是单位矩阵，理论上将总相关性最小化到 $0$。\n\n每个测试用例的整体算法如下：\n1.  使用固定种子初始化随机数生成器以保证可复现性。\n2.  生成 $N=20000$ 个因子向量 $V \\sim \\mathcal{N}(0, I_3)$ 的样本。\n3.  生成 $N=20000$ 个噪声向量 $\\varepsilon \\sim \\mathcal{N}(0, \\mathrm{diag}(\\sigma_i^2))$ 的样本。\n4.  通过 $Z = AV + \\varepsilon$ 计算潜变量样本。数据矩阵的形状为 $(N, 3)$。\n5.  根据样本数据计算 $TC(Z)$ 和 $\\mathrm{MIG}(Z,V)$。\n6.  对于每个指定的 $\\alpha \\in \\{0.5, 1.0\\}$ 值：\n    a.  计算原始数据 $Z$ 的样本协方差矩阵 $\\hat{\\Sigma}_Z$。\n    b.  计算变换矩阵 $M = \\hat{\\Sigma}_Z^{-\\alpha/2}$。\n    c.  中心化数据 $Z_{cent} = Z - \\bar{Z}$。\n    d.  应用变换：$Z^{(\\alpha)} = Z_{cent} M$。由于对于对称的 $\\hat{\\Sigma}_Z$，$M$ 是对称的，这等价于 $(M Z_{cent}^T)^T$。\n    e.  使用变换后的样本 $Z^{(\\alpha)}$ 和原始因子样本 $V$ 计算 $TC(Z^{(\\alpha)})$ 和 $\\mathrm{MIG}(Z^{(\\alpha)}, V)$。\n7.  收集六个得到的浮点数：$[TC(Z), \\mathrm{MIG}(Z,V), TC(Z^{(0.5)}), \\mathrm{MIG}(Z^{(0.5)},V), TC(Z^{(1.0)}), \\mathrm{MIG}(Z^{(1.0)},V)]$ 用于最终输出。\n对于所提供的所有三个测试用例，将重复此过程。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    # A fixed random seed for reproducibility is required.\n    RANDOM_SEED = 42\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.6, 0.0], [0.4, 1.0, 0.5], [0.0, 0.3, 1.0]]),\n            \"noise_stds\": np.array([0.1, 0.1, 0.1]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        },\n        {\n            \"A\": np.identity(3),\n            \"noise_stds\": np.array([0.05, 0.05, 0.05]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        },\n        {\n            \"A\": np.array([[1.0, 1.1, 1.2], [0.9, 1.0, 1.1], [1.1, 1.2, 1.3]]),\n            \"noise_stds\": np.array([0.2, 0.2, 0.2]),\n            \"N\": 20000,\n            \"alphas\": [0.5, 1.0]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        noise_stds = case[\"noise_stds\"]\n        N = case[\"N\"]\n        alphas = case[\"alphas\"]\n        \n        # Step 1: Generate data\n        d = A.shape[0]\n        V = rng.standard_normal(size=(N, d))\n        epsilon = rng.normal(loc=0.0, scale=noise_stds, size=(N, d))\n        Z = V @ A.T + epsilon\n\n        # Step 2: Calculate metrics for original Z\n        case_results = []\n        tc_z, mig_z = _calculate_metrics(Z, V)\n        case_results.extend([tc_z, mig_z])\n        \n        # Step 3: Apply partial-whitening transform and recalculate metrics\n        Z_centered = Z - Z.mean(axis=0)\n        cov_Z = np.cov(Z, rowvar=False)\n\n        for alpha in alphas:\n            # Compute the transformation matrix Sigma_Z^(-alpha/2)\n            # Use real part to discard negligible imaginary parts from numerical errors\n            transform_M = np.real(linalg.fractional_matrix_power(cov_Z, -alpha / 2.0))\n            \n            # Apply the transformation\n            Z_alpha = Z_centered @ transform_M\n            \n            # Calculate metrics for the transformed data\n            tc_alpha, mig_alpha = _calculate_metrics(Z_alpha, V)\n            case_results.extend([tc_alpha, mig_alpha])\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    outer_parts = []\n    for res_list in all_results:\n        # Round to a reasonable number of digits for consistent output\n        inner_str = f\"[{','.join(f'{x:.6f}' for x in res_list)}]\"\n        outer_parts.append(inner_str)\n    final_str = f\"[{','.join(outer_parts)}]\"\n    print(final_str)\n\ndef _calculate_metrics(Z_data, V_data):\n    \"\"\"\n    Calculates Total Correlation (TC) and Mutual Information Gap (MIG).\n    \"\"\"\n    # Calculate Total Correlation (TC)\n    cov_Z = np.cov(Z_data, rowvar=False)\n    # Use slogdet for numerical stability\n    s, logdet_cov = np.linalg.slogdet(cov_Z)\n    if s == 0: # Handle non-positive definite cases, though unlikely with N >> d\n        tc = np.nan\n    else:\n        log_prod_diag = np.sum(np.log(np.diag(cov_Z)))\n        tc = 0.5 * (log_prod_diag - logdet_cov)\n\n    # Calculate Mutual Information Gap (MIG)\n    d = Z_data.shape[1]\n    combined_data = np.hstack((Z_data, V_data))\n    corr_matrix = np.corrcoef(combined_data, rowvar=False)\n    \n    # Extract the correlation submatrix between Z and V\n    rho_ZV = corr_matrix[0:d, d:]\n\n    # Clip to avoid log(0) for perfect correlations\n    rho_ZV_sq = np.clip(rho_ZV**2, 0.0, 1.0 - 1e-12)\n    mi_matrix = -0.5 * np.log(1.0 - rho_ZV_sq)\n    \n    gaps = []\n    for k in range(d): # Iterate over factors V_k\n        mi_for_factor = mi_matrix[:, k]\n        sorted_mi = np.sort(mi_for_factor)\n        gap = sorted_mi[-1] - sorted_mi[-2]\n        gaps.append(gap)\n        \n    mig = np.mean(gaps)\n    \n    return tc, mig\n\nsolve()\n```", "id": "3149107"}]}