{"hands_on_practices": [{"introduction": "理解偏差-方差权衡始于分析单个模型。我们将使用岭回归（一种基础技术）来从数学上推导，添加惩罚项（正则化）如何在模型的估计中引入少量偏差。这个练习将揭示为何这种刻意引入的偏差有助于降低整体预测误差，特别是通过控制模型的方差。[@problem_id:3180590]", "problem": "考虑一个带有岭正则化的固定设计线性回归模型。设 $X \\in \\mathbb{R}^{n \\times p}$ 表示一个确定性设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是未知参数矢量， $y \\in \\mathbb{R}^{n}$ 是由模型 $y = X \\beta + \\epsilon$ 生成的观测响应，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ 是一个与 $X$ 无关的零均值高斯噪声矢量。带有正则化参数 $\\lambda > 0$ 的岭回归估计量定义为 $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} y$。\n\n设 $x_{0} \\in \\mathbb{R}^{p}$ 是一个新的协变量矢量，我们希望在该点预测 $y_{0} = x_{0}^{\\top} \\beta + \\epsilon_{0}$，其中 $\\epsilon_{0} \\sim \\mathcal{N}(0, \\sigma^{2})$ 与 $\\epsilon$ 无关。在 $x_{0}$ 处的预测为 $\\hat{y}_{0} = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$。在 $x_{0}$ 处的期望平方预测误差可以进行偏差-方差-噪声分解，分解为偏差平方、方差和不可约噪声之和。\n\n假设奇异值分解 (SVD) 为 $X = U D V^{\\top}$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{p \\times p}$ 是正交矩阵，$D \\in \\mathbb{R}^{n \\times p}$ 是对角矩阵，其对角线上有非负奇异值 $d_{1}, \\dots, d_{r} > 0$，其余为零，其中 $r = \\operatorname{rank}(X)$。$V$ 的列 $v_{1}, \\dots, v_{p}$ 是主轴（$X^{\\top} X$ 的特征向量）。定义对齐系数 $a_{i} = v_{i}^{\\top} x_{0}$ 和 $b_{i} = v_{i}^{\\top} \\beta$，其中 $i = 1, \\dots, p$。\n\n从关于期望、线性算子和奇异值分解的基本定义和事实出发，推导岭估计量 $\\hat{\\beta}_{\\lambda}$ 的偏差矢量，并使用上述主轴表示，计算该偏差对 $x_{0}$ 处期望平方预测误差的贡献，该贡献仅用 $\\{d_{i}\\}_{i=1}^{p}$、$\\lambda$、$\\{a_{i}\\}_{i=1}^{p}$ 和 $\\{b_{i}\\}_{i=1}^{p}$ 表示。你的最终答案必须是偏差平方项的单一闭式解析表达式。不需要数值近似，也不涉及单位。", "solution": "该问题要求推导岭回归模型中期望平方预测误差 (ESPE) 的偏差平方项。对于在点 $x_{0}$ 处的新观测 $y_{0} = x_{0}^{\\top}\\beta + \\epsilon_{0}$ 的预测 $\\hat{y}_{0}$，其 ESPE 由 $E[(y_{0} - \\hat{y}_{0})^{2}]$ 给出。这可以分解为三个分量：偏差平方、方差和不可约误差。\n\n预测值为 $\\hat{y}_{0} = x_{0}^{\\top}\\hat{\\beta}_{\\lambda}$。我们试图估计的真实值是条件均值 $f(x_{0}) = E[y_{0} | x_{0}] = x_{0}^{\\top}\\beta$。误差项 $\\epsilon_{0}$ 被认为是不可约噪声。ESPE 的偏差-方差分解为：\n$$\nE[(y_{0} - \\hat{y}_{0})^{2}] = (E[\\hat{y}_{0}] - x_{0}^{\\top}\\beta)^{2} + E[(\\hat{y}_{0} - E[\\hat{y}_{0}])^{2}] + E[\\epsilon_{0}^{2}]\n$$\n该表达式对应于：\n$$\n\\text{ESPE}(x_{0}) = (\\text{Bias}[\\hat{y}_{0}])^{2} + \\text{Var}[\\hat{y}_{0}] + \\sigma^{2}\n$$\n问题要求的是偏差平方项，$(\\text{Bias}[\\hat{y}_{0}])^{2}$。\n\n首先，我们推导岭估计量 $\\hat{\\beta}_{\\lambda}$ 的偏差。对于参数 $\\theta$ 的估计量 $\\hat{\\theta}$，其偏差的定义是 $\\text{Bias}[\\hat{\\theta}] = E[\\hat{\\theta}] - \\theta$。这里，参数是矢量 $\\beta$。\n岭估计量由 $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} y$ 给出。我们通过代入真实模型 $y = X \\beta + \\epsilon$ 来计算其期望：\n$$\nE[\\hat{\\beta}_{\\lambda}] = E[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta + \\epsilon) ]\n$$\n由于设计矩阵 $X$ 是确定性的，并利用期望的线性性质：\n$$\nE[\\hat{\\beta}_{\\lambda}] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta + (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} E[\\epsilon]\n$$\n鉴于噪声矢量 $\\epsilon$ 的均值为零，即 $E[\\epsilon] = 0$，第二项消失。\n$$\nE[\\hat{\\beta}_{\\lambda}] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta\n$$\n因此，估计量 $\\hat{\\beta}_{\\lambda}$ 的偏差是：\n$$\n\\text{Bias}[\\hat{\\beta}_{\\lambda}] = E[\\hat{\\beta}_{\\lambda}] - \\beta = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - \\beta = \\left( (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_{p} \\right) \\beta\n$$\n现在，我们使用 $X$ 的奇异值分解 (SVD)，即 $X = U D V^{\\top}$。项 $X^{\\top} X$ 可以表示为：\n$$\nX^{\\top} X = (U D V^{\\top})^{\\top} (U D V^{\\top}) = V D^{\\top} U^{\\top} U D V^{\\top}\n$$\n由于 $U$ 是正交矩阵，所以 $U^{\\top} U = I_{n}$。\n$$\nX^{\\top} X = V D^{\\top} D V^{\\top}\n$$\n矩阵 $D^{\\top}D$ 是一个 $p \\times p$ 的对角矩阵，其对角线元素是奇异值的平方，即 $(D^{\\top}D)_{ii} = d_{i}^{2}$，其中 $i=1, \\dots, p$。请注意，根据问题定义，当 $i>r$ 时 $d_i=0$。我们将此对角矩阵表示为 $\\Sigma_{p}^{2} = \\text{diag}(d_{1}^{2}, \\dots, d_{p}^{2})$。因此，$X^{\\top} X = V \\Sigma_{p}^{2} V^{\\top}$。\n\n我们将此代入偏差的表达式中。首先，考虑项 $(X^{\\top} X + \\lambda I_{p})$：\n$$\nX^{\\top} X + \\lambda I_{p} = V \\Sigma_{p}^{2} V^{\\top} + \\lambda I_{p} = V \\Sigma_{p}^{2} V^{\\top} + \\lambda V V^{\\top} = V (\\Sigma_{p}^{2} + \\lambda I_{p}) V^{\\top}\n$$\n其逆矩阵为：\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} = (V (\\Sigma_{p}^{2} + \\lambda I_{p}) V^{\\top})^{-1} = V (\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} V^{\\top}\n$$\n矩阵 $(\\Sigma_{p}^{2} + \\lambda I_{p})$ 是对角矩阵，其元素为 $d_{i}^{2} + \\lambda$，因此其逆矩阵也是对角矩阵，元素为 $1 / (d_{i}^{2} + \\lambda)$。\n\n现在，我们可以简化乘积 $(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X$：\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X = \\left( V (\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} V^{\\top} \\right) \\left( V \\Sigma_{p}^{2} V^{\\top} \\right) = V (\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} \\Sigma_{p}^{2} V^{\\top}\n$$\n中间的矩阵 $(\\Sigma_{p}^{2} + \\lambda I_{p})^{-1} \\Sigma_{p}^{2}$ 是对角矩阵，其元素为 $\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda}$。\n将此代回 $\\hat{\\beta}_{\\lambda}$ 的偏差表达式中：\n$$\n\\text{Bias}[\\hat{\\beta}_{\\lambda}] = \\left( V \\text{diag}\\left(\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda}\\right) V^{\\top} - V I_{p} V^{\\top} \\right) \\beta = V \\left( \\text{diag}\\left(\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda}\\right) - I_{p} \\right) V^{\\top} \\beta\n$$\n括号内的对角矩阵的元素为 $\\frac{d_{i}^{2}}{d_{i}^{2} + \\lambda} - 1 = \\frac{d_{i}^{2} - (d_{i}^{2} + \\lambda)}{d_{i}^{2} + \\lambda} = \\frac{-\\lambda}{d_{i}^{2} + \\lambda}$。\n因此，岭估计量的偏差矢量为：\n$$\n\\text{Bias}[\\hat{\\beta}_{\\lambda}] = V \\text{diag}\\left(\\frac{-\\lambda}{d_{i}^{2} + \\lambda}\\right) V^{\\top} \\beta\n$$\n接下来，我们计算预测值 $\\hat{y}_{0} = x_{0}^{\\top}\\hat{\\beta}_{\\lambda}$ 的偏差。被预测的真实值是 $x_{0}^{\\top}\\beta$。\n$$\n\\text{Bias}[\\hat{y}_{0}] = E[\\hat{y}_{0}] - x_{0}^{\\top}\\beta = E[x_{0}^{\\top}\\hat{\\beta}_{\\lambda}] - x_{0}^{\\top}\\beta = x_{0}^{\\top}E[\\hat{\\beta}_{\\lambda}] - x_{0}^{\\top}\\beta = x_{0}^{\\top}(E[\\hat{\\beta}_{\\lambda}] - \\beta) = x_{0}^{\\top}\\text{Bias}[\\hat{\\beta}_{\\lambda}]\n$$\n代入 $\\text{Bias}[\\hat{\\beta}_{\\lambda}]$ 的表达式：\n$$\n\\text{Bias}[\\hat{y}_{0}] = x_{0}^{\\top} V \\text{diag}\\left(\\frac{-\\lambda}{d_{i}^{2} + \\lambda}\\right) V^{\\top} \\beta\n$$\n这可以写成三个项的乘积：$(x_{0}^{\\top} V)$、对角矩阵和 $(V^{\\top} \\beta)$。\n让我们使用给定的定义来分析这些项。矢量 $V^{\\top}x_{0}$ 的分量为 $(V^{\\top}x_{0})_{i} = v_{i}^{\\top}x_{0} = a_{i}$。因此，$x_{0}^{\\top}V = (V^{\\top}x_{0})^{\\top} = [a_{1}, a_{2}, \\dots, a_{p}]$。\n类似地，矢量 $V^{\\top}\\beta$ 的分量为 $(V^{\\top}\\beta)_{i} = v_{i}^{\\top}\\beta = b_{i}$。因此，$V^{\\top}\\beta = [b_{1}, b_{2}, \\dots, b_{p}]^{\\top}$。\n\n预测的偏差成为一个行向量、一个对角矩阵和一个列向量的乘积：\n$$\n\\text{Bias}[\\hat{y}_{0}] = [a_{1} \\dots a_{p}]\n\\begin{pmatrix}\n\\frac{-\\lambda}{d_{1}^{2} + \\lambda} & 0 & \\dots \\\\\n0 & \\frac{-\\lambda}{d_{2}^{2} + \\lambda} & \\\\\n\\vdots &  & \\ddots\n\\end{pmatrix}\n\\begin{pmatrix}\nb_{1} \\\\\n\\vdots \\\\\nb_{p}\n\\end{pmatrix}\n$$\n这得到一个和式：\n$$\n\\text{Bias}[\\hat{y}_{0}] = \\sum_{i=1}^{p} a_{i} \\left(\\frac{-\\lambda}{d_{i}^{2} + \\lambda}\\right) b_{i} = -\\lambda \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda}\n$$\n问题要求的是此偏差对期望平方预测误差的贡献，即偏差的平方，$(\\text{Bias}[\\hat{y}_{0}])^{2}$。\n$$\n(\\text{Bias}[\\hat{y}_{0}])^{2} = \\left(-\\lambda \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda}\\right)^{2} = \\lambda^{2} \\left( \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda} \\right)^{2}\n$$\n这就是偏差平方的最终表达式，仅用指定的变量表示。", "answer": "$$\n\\boxed{\\lambda^{2} \\left( \\sum_{i=1}^{p} \\frac{a_{i} b_{i}}{d_{i}^{2} + \\lambda} \\right)^{2}}\n$$", "id": "3180590"}, {"introduction": "在单个模型的基础上，我们现在探索将它们组合成“集成”的力量。这个练习将剖析一个堆叠集成模型的误差，展示最终的偏差是个体模型偏差的加权平均。更重要的是，它揭示了集成的方差不仅取决于个体方差，还关键地取决于模型之间的协方差，为“多样化的模型构成强大的集成”这一原则提供了量化依据。[@problem_id:3180603]", "problem": "考虑一个标量响应的监督学习场景，其中在固定输入 $x$ 处的数据生成过程为 $y = f(x) + \\varepsilon$，其中 $\\varepsilon$ 是一个零均值随机噪声项，独立于训练数据和学习到的预测器，其方差为 $\\operatorname{Var}(\\varepsilon) = \\sigma^{2}$。三个基学习器生成预测 $\\hat{f}_{1}(x)$、$\\hat{f}_{2}(x)$ 和 $\\hat{f}_{3}(x)$。将学习器 $i$ 在 $x$ 处的预测误差定义为 $e_{i}(x) = \\hat{f}_{i}(x) - f(x)$，其在 $x$ 处的偏差定义为 $b_{i}(x) = \\mathbb{E}[\\hat{f}_{i}(x)] - f(x) = \\mathbb{E}[e_{i}(x)]$。一个堆叠集成使用和为一的非负权重，具体为 $w = (w_{1}, w_{2}, w_{3}) = (0.5, 0.3, 0.2)$，生成集成预测器 $\\hat{f}_{w}(x) = \\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)$。\n\n在所考虑的点 $x$ 处，假设基学习器的偏差向量为\n$$\nb(x) = \\begin{pmatrix} 0.20 \\\\ -0.10 \\\\ 0.05 \\end{pmatrix},\n$$\n基学习器误差的协方差矩阵为\n$$\n\\Sigma(x) = \\begin{pmatrix}\n0.25 & 0.10 & 0.05 \\\\\n0.10 & 0.36 & 0.12 \\\\\n0.05 & 0.12 & 0.49\n\\end{pmatrix}.\n$$\n假设不可约噪声方差为 $\\sigma^{2} = 0.04$。\n\n从偏差的基本定义 $\\mathbb{E}[\\hat{f}(x)] - f(x)$、方差的基本定义 $\\operatorname{Var}(\\hat{f}(x))$ 以及期望平方预测误差 $\\mathbb{E}\\big[(\\hat{f}(x) - y)^{2}\\big]$ 出发，首先推导出堆叠集成在 $x$ 处的偏差和方差关于 $w$、$b(x)$ 和 $\\Sigma(x)$ 的符号表达式。然后，使用所提供的数值，计算该集成在 $x$ 处的期望平方预测误差。\n\n将最终答案表示为单个实数。将答案四舍五入到四位有效数字。不需要单位。", "solution": "问题要求计算堆叠集成预测器在特定点 $x$ 处的期望平方预测误差。数据生成过程由 $y = f(x) + \\varepsilon$ 给出，其中 $\\varepsilon$ 是一个方差为 $\\sigma^2$ 的零均值噪声项。期望平方预测误差 $\\text{Error}(x)$ 定义为 $\\mathbb{E}\\big[(\\hat{f}(x) - y)^{2}\\big]$。\n\n我们首先分解这个误差表达式。令 $\\hat{f}_{w}(x)$ 为集成预测器。\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - y)^{2}\\big]\n$$\n代入 $y = f(x) + \\varepsilon$：\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - (f(x) + \\varepsilon))^{2}\\big] = \\mathbb{E}\\big[((\\hat{f}_{w}(x) - f(x)) - \\varepsilon)^{2}\\big]\n$$\n展开平方项：\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2} - 2\\varepsilon(\\hat{f}_{w}(x) - f(x)) + \\varepsilon^{2}\\big]\n$$\n根据期望的线性性质：\n$$\n\\text{Error}(x) = \\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2}\\big] - 2\\mathbb{E}\\big[\\varepsilon(\\hat{f}_{w}(x) - f(x))\\big] + \\mathbb{E}[\\varepsilon^{2}]\n$$\n预测器 $\\hat{f}_{w}(x)$ 是训练数据的函数，对于新的观测值，它独立于噪声项 $\\varepsilon$。因此，$\\mathbb{E}\\big[\\varepsilon(\\hat{f}_{w}(x) - f(x))\\big] = \\mathbb{E}[\\varepsilon]\\mathbb{E}[\\hat{f}_{w}(x) - f(x)]$。由于 $\\mathbb{E}[\\varepsilon]=0$，该项为零。\n最后一项是 $\\mathbb{E}[\\varepsilon^{2}] = \\operatorname{Var}(\\varepsilon) + (\\mathbb{E}[\\varepsilon])^2 = \\sigma^2 + 0^2 = \\sigma^2$。\n\n第一项 $\\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2}\\big]$ 是预测器的均方误差 (MSE)。它可以被进一步分解。对于任意随机变量 $Z$，$\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2$。令 $Z = \\hat{f}_{w}(x) - f(x)$。\n$$\n\\mathbb{E}\\big[(\\hat{f}_{w}(x) - f(x))^{2}\\big] = \\operatorname{Var}(\\hat{f}_{w}(x) - f(x)) + (\\mathbb{E}[\\hat{f}_{w}(x) - f(x)])^2\n$$\n由于 $f(x)$ 是一个固定的、非随机的量，$\\operatorname{Var}(\\hat{f}_{w}(x) - f(x)) = \\operatorname{Var}(\\hat{f}_{w}(x))$。根据定义，$\\mathbb{E}[\\hat{f}_{w}(x) - f(x)]$ 项是集成预测器的偏差。\n令 $\\text{Bias}(\\hat{f}_{w}(x)) = \\mathbb{E}[\\hat{f}_{w}(x)] - f(x)$。\n所以，完整分解为：\n$$\n\\text{Error}(x) = (\\text{Bias}(\\hat{f}_{w}(x)))^2 + \\operatorname{Var}(\\hat{f}_{w}(x)) + \\sigma^2\n$$\n这就是偏差-方差-噪声分解。我们的任务是为给定的集成计算这三项中的每一项。\n\n首先，我们推导集成偏差的符号表达式。集成预测器是 $\\hat{f}_{w}(x) = \\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)$。\n$$\n\\text{Bias}(\\hat{f}_{w}(x)) = \\mathbb{E}\\left[\\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)\\right] - f(x)\n$$\n利用期望的线性性质以及 $\\sum_{i=1}^{3} w_i = 1$：\n$$\n\\text{Bias}(\\hat{f}_{w}(x)) = \\sum_{i=1}^{3} w_{i}\\,\\mathbb{E}[\\hat{f}_{i}(x)] - \\left(\\sum_{i=1}^{3} w_i\\right)f(x) = \\sum_{i=1}^{3} w_{i} (\\mathbb{E}[\\hat{f}_{i}(x)] - f(x))\n$$\n这可以简化为各个偏差的加权和，$b_i(x) = \\mathbb{E}[\\hat{f}_{i}(x)] - f(x)$：\n$$\n\\text{Bias}(\\hat{f}_{w}(x)) = \\sum_{i=1}^{3} w_{i}\\,b_{i}(x) = w^T b(x)\n$$\n\n接下来，我们推导集成方差的符号表达式。\n$$\n\\operatorname{Var}(\\hat{f}_{w}(x)) = \\operatorname{Var}\\left(\\sum_{i=1}^{3} w_{i}\\,\\hat{f}_{i}(x)\\right)\n$$\n使用随机变量加权和的方差的通用公式：\n$$\n\\operatorname{Var}(\\hat{f}_{w}(x)) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} w_{i} w_{j} \\operatorname{Cov}(\\hat{f}_{i}(x), \\hat{f}_{j}(x))\n$$\n问题提供了基学习器误差的协方差矩阵 $\\Sigma(x)$，其元素为 $\\Sigma_{ij}(x) = \\operatorname{Cov}(e_i(x), e_j(x))$。由于 $e_i(x) = \\hat{f}_i(x) - f(x)$ 且 $f(x)$ 是一个常数，所以 $\\operatorname{Cov}(e_i(x), e_j(x)) = \\operatorname{Cov}(\\hat{f}_i(x), \\hat{f}_j(x))$。因此，$\\Sigma(x)$ 是预测器 $\\hat{f}_i(x)$ 的协方差矩阵。集成的方差可以写成矩阵形式：\n$$\n\\operatorname{Var}(\\hat{f}_{w}(x)) = w^T \\Sigma(x) w\n$$\n\n现在，我们代入给定的数值来计算总误差。\n权重为 $w = \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix}$。\n偏差向量为 $b(x) = \\begin{pmatrix} 0.20 \\\\ -0.10 \\\\ 0.05 \\end{pmatrix}$。\n协方差矩阵为 $\\Sigma(x) = \\begin{pmatrix} 0.25 & 0.10 & 0.05 \\\\ 0.10 & 0.36 & 0.12 \\\\ 0.05 & 0.12 & 0.49 \\end{pmatrix}$。\n不可约噪声方差为 $\\sigma^2 = 0.04$。\n\n1.  计算集成偏差的平方：\n    $$\n    \\text{Bias}(\\hat{f}_{w}(x)) = w^T b(x) = \\begin{pmatrix} 0.5 & 0.3 & 0.2 \\end{pmatrix} \\begin{pmatrix} 0.20 \\\\ -0.10 \\\\ 0.05 \\end{pmatrix}\n    $$\n    $$\n    \\text{Bias}(\\hat{f}_{w}(x)) = (0.5)(0.20) + (0.3)(-0.10) + (0.2)(0.05) = 0.10 - 0.03 + 0.01 = 0.08\n    $$\n    $$\n    (\\text{Bias}(\\hat{f}_{w}(x)))^2 = (0.08)^2 = 0.0064\n    $$\n\n2.  计算集成的方差：\n    $$\n    \\operatorname{Var}(\\hat{f}_{w}(x)) = w^T \\Sigma(x) w\n    $$\n    我们首先计算向量 $\\Sigma(x)w$：\n    $$\n    \\Sigma(x)w = \\begin{pmatrix} 0.25 & 0.10 & 0.05 \\\\ 0.10 & 0.36 & 0.12 \\\\ 0.05 & 0.12 & 0.49 \\end{pmatrix} \\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} (0.25)(0.5) + (0.10)(0.3) + (0.05)(0.2) \\\\ (0.10)(0.5) + (0.36)(0.3) + (0.12)(0.2) \\\\ (0.05)(0.5) + (0.12)(0.3) + (0.49)(0.2) \\end{pmatrix}\n    $$\n    $$\n    \\Sigma(x)w = \\begin{pmatrix} 0.125 + 0.030 + 0.010 \\\\ 0.050 + 0.108 + 0.024 \\\\ 0.025 + 0.036 + 0.098 \\end{pmatrix} = \\begin{pmatrix} 0.165 \\\\ 0.182 \\\\ 0.159 \\end{pmatrix}\n    $$\n    现在，我们计算二次型 $w^T(\\Sigma(x)w)$：\n    $$\n    \\operatorname{Var}(\\hat{f}_{w}(x)) = \\begin{pmatrix} 0.5 & 0.3 & 0.2 \\end{pmatrix} \\begin{pmatrix} 0.165 \\\\ 0.182 \\\\ 0.159 \\end{pmatrix}\n    $$\n    $$\n    \\operatorname{Var}(\\hat{f}_{w}(x)) = (0.5)(0.165) + (0.3)(0.182) + (0.2)(0.159) = 0.0825 + 0.0546 + 0.0318 = 0.1689\n    $$\n\n3.  将各分量相加求得总期望误差：\n    $$\n    \\text{Error}(x) = (\\text{Bias}(\\hat{f}_{w}(x)))^2 + \\operatorname{Var}(\\hat{f}_{w}(x)) + \\sigma^2\n    $$\n    $$\n    \\text{Error}(x) = 0.0064 + 0.1689 + 0.04\n    $$\n    $$\n    \\text{Error}(x) = 0.2153\n    $$\n结果 $0.2153$ 已经表示为四位有效数字。", "answer": "$$\n\\boxed{0.2153}\n$$", "id": "3180603"}]}