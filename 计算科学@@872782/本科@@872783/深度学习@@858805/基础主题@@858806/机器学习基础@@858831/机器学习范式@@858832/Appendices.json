{"hands_on_practices": [{"introduction": "自监督学习（Self-supervised learning, SSL）是一种强大的范式，它能够从未标记的数据中学习有效的特征表示。其核心思想是通过创建“代理任务” (pretext task)，让数据自身为学习过程提供监督信号。本练习将通过一个简化的数学模型，深入探讨两种经典的SSL代理任务——图像旋转预测与拼图游戏 [@problem_id:3160860]。通过量化计算每种任务在多大程度上保留了关于物体方向的核心信息，您将建立起对不同代理任务如何塑造所学表征并引入特定“隐式不变性” (implicit invariance) 的直观理解，从而领悟设计高效自监督方法背后的关键权衡。", "problem": "你将通过一个数学上精确的代理模型来分析深度学习中的两种自监督学习范式。潜变量是一个连续的平面方向角 $\\theta \\in [0,2\\pi)$，以弧度为单位。下游任务是仅使用代理任务所保留的信息来估计 $\\theta$。你将比较两种定义了不同隐式不变性的代理任务，并量化哪一种与下游几何结构更吻合。\n\n定义与设置：\n- 设 $\\theta$ 是一个在区间 $[a,b] \\subseteq [0,2\\pi)$ 上服从均匀分布的随机变量（角度以弧度为单位）。\n- 设 $p$ 是一个离散的排列标签，它独立于 $\\theta$。\n- 旋转预测代理任务通过将 $\\theta$ 在 $[0,2\\pi)$ 上分箱到 $K$ 个等宽的类别中来创建一个标签 $Y_{\\mathrm{rot}}$：$Y_{\\mathrm{rot}} = \\left\\lfloor K \\cdot \\theta / (2\\pi) \\right\\rfloor \\in \\{0,1,\\dots,K-1\\}$。\n- 拼图代理任务创建一个独立于 $\\theta$ 的标签 $Y_{\\mathrm{jig}} = p$。\n\n使用的基本原理：\n- 在平方误差损失下，给定观测变量 $Y$ 的目标变量 $Z$ 的贝叶斯估计量是条件均值 $\\hat{Z}(Y) = \\mathbb{E}[Z \\mid Y]$，相应的最小均方误差（贝叶斯风险）是 $\\mathcal{R}^\\star(Y) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z \\mid Y])^2\\big] = \\mathbb{E}\\big[\\mathrm{Var}(Z \\mid Y)\\big]$。\n- 互信息 (MI) 定义为 $I(U;V) = \\mathbb{E}\\left[\\log \\frac{p_{U,V}(U,V)}{p_U(U) p_V(V)}\\right]$，并满足数据处理不等式。角度必须以弧度处理。\n\n你的目标：\n- 对下面的每个测试用例，计算以下两个量：\n  1. 对齐分数 $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$，其中 $\\mathcal{R}^\\star(Y)$ 是仅使用 $Y$ 估计 $\\theta$ 的贝叶斯最优均方误差。正值表示对于平方误差估计，旋转预测比拼图保留了更多关于 $\\theta$ 的信息。\n  2. 互信息 $I(\\theta; Y_{\\mathrm{rot}})$，以奈特（nats）为单位。\n- 仅使用上述数学原理。角度必须以弧度为单位。\n\n测试套件（角度以弧度为单位）：\n- 用例 1：$K=4$, $[a,b]=[0,2\\pi)$。\n- 用例 2：$K=8$, $[a,b]=[0,2\\pi)$。\n- 用例 3：$K=4$, $[a,b]=[0,\\pi)$。\n- 用例 4：$K=1$, $[a,b]=[0,2\\pi)$。\n- 用例 5：$K=4$, $[a,b]=[0,\\pi/8)$。\n\n最终输出格式：\n- 你的程序应产生单行输出，其中包含一个逗号分隔的对列表形式的结果，每对的形式为 $[\\Delta_{\\mathrm{MSE}}, I(\\theta; Y_{\\mathrm{rot}})]$，按上述用例的顺序排列，所有内容都包含在一对最外层的方括号中。例如，输出应类似于 [[x1,y1],[x2,y2],...,[x5,y5]]，其中每个 $x_i$ 和 $y_i$ 都是浮点数值。", "solution": "在尝试求解之前，评估问题陈述的有效性。\n\n### 步骤 1：提取已知信息\n- **潜变量**：一个连续的平面方向角 $\\theta \\in [0,2\\pi)$，以弧度为单位。\n- **$\\theta$ 的分布**：在区间 $[a,b] \\subseteq [0,2\\pi)$ 上服从均匀分布。\n- **独立变量**：一个离散的排列标签 $p$，独立于 $\\theta$。\n- **旋转代理任务标签**：$Y_{\\mathrm{rot}} = \\left\\lfloor K \\cdot \\theta / (2\\pi) \\right\\rfloor \\in \\{0,1,\\dots,K-1\\}$。\n- **拼图代理任务标签**：$Y_{\\mathrm{jig}} = p$。\n- **贝叶斯估计量**：对于给定的观测变量 $Y$ 的目标变量 $Z$，在平方误差损失下的贝叶斯估计量是条件均值 $\\hat{Z}(Y) = \\mathbb{E}[Z \\mid Y]$。\n- **贝叶斯风险**：最小均方误差是 $\\mathcal{R}^\\star(Y) = \\mathbb{E}\\big[(Z - \\mathbb{E}[Z \\mid Y])^2\\big] = \\mathbb{E}\\big[\\mathrm{Var}(Z \\mid Y)\\big]$。\n- **互信息 (MI)**：$I(U;V) = \\mathbb{E}\\left[\\log \\frac{p_{U,V}(U,V)}{p_U(U) p_V(V)}\\right]$。\n- **目标**：计算 $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 和 $I(\\theta; Y_{\\mathrm{rot}})$，其中目标变量是 $\\theta$。\n- **测试套件**：\n    - 用例 1：$K=4$, $[a,b]=[0,2\\pi)$。\n    - 用例 2：$K=8$, $[a,b]=[0,2\\pi)$。\n    - 用例 3：$K=4$, $[a,b]=[0,\\pi)$。\n    - 用例 4：$K=1$, $[a,b]=[0,2\\pi)$。\n    - 用例 5：$K=4$, $[a,b]=[0,\\pi/8)$。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据验证标准对问题进行评估。\n\n- **科学依据**：问题使用了概率论和信息论中的标准、正确的定义（贝叶斯估计量、贝叶斯风险、互信息）。该设置是一个简化的数学模型，或称代理模型，用于比较自监督学习中两种已建立的范式（旋转预测和拼图）。这种建模方法是理论机器学习中一种有效且常见的技术，用以获得形式化的见解。核心假设，例如拼图标签与方向无关，是对现实世界方法的合理抽象。该问题在科学上是合理的。\n- **良构性**：所有变量、分布和待计算的量都有精确的定义。每个测试用例的参数都已明确给出。问题是自洽的，并为推导每个所需值的唯一数学解提供了足够的信息。\n- **客观性**：问题以正式的数学语言陈述，没有任何主观性、模糊性或基于观点的论断。\n\n该问题未表现出任何无效性缺陷。它并非科学上不合理、不可形式化、不完整、自相矛盾、不切实际、非良构、无意义或超出科学可验证的范畴。\n\n### 步骤 3：结论与行动\n该问题是**有效**的。将提供一个分步的解决方案。\n\n### 基于原理的解决方案\n目标是为每个测试用例计算两个量：对齐分数 $\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 和互信息 $I(\\theta; Y_{\\mathrm{rot}})$。估计的目标变量是角度 $\\theta$。\n\n$\\theta$ 在 $[a,b]$上均匀分布，其概率密度函数 (PDF) 如下：\n$$ p(\\theta) = \\begin{cases} \\frac{1}{b-a}  \\text{if } a \\le \\theta \\le b \\\\ 0  \\text{otherwise} \\end{cases} $$\n\n**1. 计算 $\\mathcal{R}^\\star(Y_{\\mathrm{jig}})$**\n拼图代理任务的贝叶斯风险是 $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\mathbb{E}[\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{jig}})]$。问题陈述中说明，拼图标签 $p$（即 $Y_{\\mathrm{jig}}$）独立于 $\\theta$。以独立变量为条件不提供任何信息，因此给定 $Y_{\\mathrm{jig}}$ 的 $\\theta$ 的条件分布与其边际分布相同。\n$$\np(\\theta \\mid Y_{\\mathrm{jig}}) = p(\\theta)\n$$\n因此，条件方差等于边际方差：\n$$\n\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{jig}}) = \\mathrm{Var}(\\theta)\n$$\n由于 $\\mathrm{Var}(\\theta)$ 是一个常数，其期望就是其自身。区间 $[a,b]$ 上的连续均匀分布的方差是 $\\frac{(b-a)^2}{12}$。\n$$\n\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\mathrm{Var}(\\theta) = \\frac{(b-a)^2}{12}\n$$\n这一项代表了 $\\theta$ 的总方差；拼图代理任务没有保留任何关于 $\\theta$ 的信息，因此估计误差是最大的。\n\n**2. 计算 $\\mathcal{R}^\\star(Y_{\\mathrm{rot}})$**\n旋转代理任务的贝叶斯风险是 $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\mathbb{E}[\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}})]$。这可以使用全期望定律来计算：\n$$\n\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\sum_{k=0}^{K-1} P(Y_{\\mathrm{rot}}=k) \\cdot \\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}}=k)\n$$\n条件 $Y_{\\mathrm{rot}}=k$ 意味着 $\\lfloor K \\theta / (2\\pi) \\rfloor = k$，这将 $\\theta$ 限制在分箱 $B_k = [\\frac{2\\pi k}{K}, \\frac{2\\pi (k+1)}{K})$ 中。$\\theta$ 的原始分布是在 $[a,b]$ 上的均匀分布。因此，给定 $Y_{\\mathrm{rot}}=k$，我们知道 $\\theta$ 必须位于交集 $I_k = [a,b] \\cap B_k$ 中。给定 $Y_{\\mathrm{rot}}=k$ 的 $\\theta$ 的条件分布是在区间 $I_k$ 上的均匀分布。\n\n设 $L_k$ 是区间 $I_k$ 的长度。观测到 $Y_{\\mathrm{rot}}=k$ 的概率是 $\\theta$ 落入 $I_k$ 的概率：\n$$\nP(Y_{\\mathrm{rot}}=k) = \\int_{I_k} p(\\theta) d\\theta = \\frac{L_k}{b-a}\n$$\n在一个长度为 $L_k$ 的区间上的均匀分布的方差是 $\\frac{L_k^2}{12}$。因此，$\\mathrm{Var}(\\theta \\mid Y_{\\mathrm{rot}}=k) = \\frac{L_k^2}{12}$。\n\n将这些代入 $\\mathcal{R}^\\star(Y_{\\mathrm{rot}})$ 的公式中：\n$$\n\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\sum_{k=0}^{K-1} \\left( \\frac{L_k}{b-a} \\right) \\left( \\frac{L_k^2}{12} \\right) = \\frac{1}{12(b-a)} \\sum_{k=0}^{K-1} L_k^3\n$$\n\n**3. 计算 $\\Delta_{\\mathrm{MSE}}$**\n对齐分数是两个风险之差：\n$$\n\\Delta_{\\mathrm{MSE}} = \\mathcal{R}^\\star(Y_{\\mathrm{jig}}) - \\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{(b-a)^2}{12} - \\frac{1}{12(b-a)} \\sum_{k=0}^{K-1} L_k^3\n$$\n\n**4. 计算 $I(\\theta; Y_{\\mathrm{rot}})$**\n互信息 $I(\\theta; Y_{\\mathrm{rot}})$ 可以表示为 $I(\\theta; Y_{\\mathrm{rot}}) = H(Y_{\\mathrm{rot}}) - H(Y_{\\mathrm{rot}} \\mid \\theta)$，其中 $H$ 表示熵。由于 $Y_{\\mathrm{rot}}$ 是 $\\theta$ 的一个确定性函数（即 $Y_{\\mathrm{rot}} = g(\\theta)$），在给定 $\\theta$ 的情况下，$Y_{\\mathrm{rot}}$ 没有不确定性。因此，条件熵 $H(Y_{\\mathrm{rot}} \\mid \\theta) = 0$。\n互信息简化为标签 $Y_{\\mathrm{rot}}$ 的熵：\n$$\nI(\\theta; Y_{\\mathrm{rot}}) = H(Y_{\\mathrm{rot}})\n$$\n对于离散随机变量，熵由香农公式给出。设 $p_k = P(Y_{\\mathrm{rot}}=k) = \\frac{L_k}{b-a}$。\n$$\nI(\\theta; Y_{\\mathrm{rot}}) = -\\sum_{k=0}^{K-1} p_k \\log(p_k) = -\\sum_{k \\text{ s.t. } L_k > 0} \\frac{L_k}{b-a} \\log\\left(\\frac{L_k}{b-a}\\right)\n$$\n对数是自然对数（底为 $e$），单位是奈特（nats）。\n\n**每个测试用例的计算摘要**\n通过确定分箱交集长度 $L_k$，将上述公式应用于每个用例。\n\n- **用例 1**：$K=4$, $[a,b]=[0,2\\pi)$。\n  $b-a=2\\pi$。分箱宽度为 $\\pi/2$。所有 4 个分箱都完全包含在 $[0,2\\pi)$ 中。所以，$L_0=L_1=L_2=L_3=\\pi/2$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\frac{(2\\pi)^2}{12} = \\frac{\\pi^2}{3}$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{1}{12(2\\pi)} \\cdot 4 \\cdot (\\frac{\\pi}{2})^3 = \\frac{\\pi^2}{48}$。\n  $\\Delta_{\\mathrm{MSE}} = \\frac{\\pi^2}{3} - \\frac{\\pi^2}{48} = \\frac{15\\pi^2}{48} = \\frac{5\\pi^2}{16}$。\n  概率为 $p_k = \\frac{\\pi/2}{2\\pi} = 1/4$（对所有 $k$）。$I(\\theta; Y_{\\mathrm{rot}}) = -\\sum_{k=0}^3 \\frac{1}{4}\\log(\\frac{1}{4}) = \\log(4)$。\n\n- **用例 2**：$K=8$, $[a,b]=[0,2\\pi)$。\n  $b-a=2\\pi$。分箱宽度为 $\\pi/4$。所有 8 个分箱都完全包含在内。$L_k=\\pi/4$（对 $k=0,\\dots,7$）。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\frac{\\pi^2}{3}$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{1}{12(2\\pi)} \\cdot 8 \\cdot (\\frac{\\pi}{4})^3 = \\frac{\\pi^2}{192}$。\n  $\\Delta_{\\mathrm{MSE}} = \\frac{\\pi^2}{3} - \\frac{\\pi^2}{192} = \\frac{63\\pi^2}{192} = \\frac{21\\pi^2}{64}$。\n  概率为 $p_k = \\frac{\\pi/4}{2\\pi} = 1/8$（对所有 $k$）。$I(\\theta; Y_{\\mathrm{rot}}) = \\log(8)$。\n\n- **用例 3**：$K=4$, $[a,b]=[0,\\pi)$。\n  $b-a=\\pi$。分箱宽度为 $\\pi/2$。区间 $[0,\\pi)$ 完全覆盖前两个分箱。$L_0=\\pi/2$, $L_1=\\pi/2$, $L_2=0$, $L_3=0$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\frac{\\pi^2}{12}$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{1}{12\\pi} ((\\frac{\\pi}{2})^3 + (\\frac{\\pi}{2})^3) = \\frac{\\pi^2}{48}$。\n  $\\Delta_{\\mathrm{MSE}} = \\frac{\\pi^2}{12} - \\frac{\\pi^2}{48} = \\frac{3\\pi^2}{48} = \\frac{\\pi^2}{16}$。\n  概率为 $p_0=p_1=\\frac{\\pi/2}{\\pi} = 1/2$。$I(\\theta; Y_{\\mathrm{rot}}) = -2 \\cdot \\frac{1}{2}\\log(\\frac{1}{2}) = \\log(2)$。\n\n- **用例 4**：$K=1$, $[a,b]=[0,2\\pi)$。\n  $b-a=2\\pi$。只有一个分箱 $B_0=[0,2\\pi)$。$L_0=2\\pi$。\n  这意味着 $Y_{\\mathrm{rot}}$ 总是 $0$，不提供任何信息。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\frac{(2\\pi)^2}{12} = \\frac{\\pi^2}{3}$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{1}{12(2\\pi)}(2\\pi)^3 = \\frac{\\pi^2}{3}$。\n  $\\Delta_{\\mathrm{MSE}} = 0$。\n  $p_0 = \\frac{2\\pi}{2\\pi}=1$。$I(\\theta; Y_{\\mathrm{rot}}) = -1\\log(1) = 0$。\n\n- **用例 5**：$K=4$, $[a,b]=[0,\\pi/8)$。\n  $b-a=\\pi/8$。分箱宽度为 $\\pi/2$。区间 $[0,\\pi/8)$ 完全包含在第一个分箱 $B_0=[0,\\pi/2)$ 内。\n  $L_0=\\pi/8$, $L_1=L_2=L_3=0$。\n  对于这个 $\\theta$ 的分布，$Y_{\\mathrm{rot}}$ 总是 $0$，不提供任何信息。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{jig}}) = \\frac{(\\pi/8)^2}{12} = \\frac{\\pi^2}{768}$。\n  $\\mathcal{R}^\\star(Y_{\\mathrm{rot}}) = \\frac{1}{12(\\pi/8)}(\\pi/8)^3 = \\frac{\\pi^2}{768}$。\n  $\\Delta_{\\mathrm{MSE}} = 0$。\n  $p_0=\\frac{\\pi/8}{\\pi/8}=1$。$I(\\theta; Y_{\\mathrm{rot}}) = -1\\log(1) = 0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_metrics(K, a, b):\n    \"\"\"\n    Calculates the alignment score Delta_MSE and mutual information I(theta; Y_rot).\n\n    Args:\n        K (int): The number of bins for rotation prediction.\n        a (float): The start of the uniform interval for theta.\n        b (float): The end of the uniform interval for theta.\n\n    Returns:\n        list: A list containing [Delta_MSE, I(theta; Y_rot)].\n    \"\"\"\n    # Edge case: if a=b, theta is a constant. All variances and MIs are 0.\n    if np.isclose(a, b):\n        return [0.0, 0.0]\n    \n    b_minus_a = b - a\n\n    # Calculate Risk for Jigsaw Pretext\n    # R_jig = Var(theta) for theta ~ U(a, b)\n    risk_jig = (b_minus_a)**2 / 12.0\n\n    # Calculate Risk for Rotation Pretext\n    sum_L_cubed = 0.0\n    L_values = []\n    \n    for k in range(K):\n        bin_start = 2.0 * np.pi * k / K\n        bin_end = 2.0 * np.pi * (k + 1.0) / K\n        \n        # Calculate intersection of [a, b] and [bin_start, bin_end)\n        intersect_start = max(a, bin_start)\n        intersect_end = min(b, bin_end)\n        \n        Lk = max(0.0, intersect_end - intersect_start)\n        \n        sum_L_cubed += Lk**3\n        L_values.append(Lk)\n\n    # R_rot = E[Var(theta | Y_rot)]\n    risk_rot = sum_L_cubed / (12.0 * b_minus_a)\n    \n    # Calculate Delta_MSE\n    delta_mse = risk_jig - risk_rot\n\n    # Calculate Mutual Information I(theta; Y_rot)\n    # I(theta; Y_rot) = H(Y_rot)\n    mutual_info = 0.0\n    for Lk in L_values:\n        if Lk  0:\n            pk = Lk / b_minus_a\n            # Ensure pk is not slightly  1 due to float precision\n            pk = min(pk, 1.0) \n            mutual_info -= pk * np.log(pk)\n\n    return [delta_mse, mutual_info]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, computes results, and prints them in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 0.0, 2.0 * np.pi),  # Case 1\n        (8, 0.0, 2.0 * np.pi),  # Case 2\n        (4, 0.0, np.pi),        # Case 3\n        (1, 0.0, 2.0 * np.pi),  # Case 4\n        (4, 0.0, np.pi / 8.0)    # Case 5\n    ]\n\n    results_str = []\n    for case in test_cases:\n        K, a, b = case\n        result = calculate_metrics(K, a, b)\n        # Format each pair as [x,y] without spaces\n        results_str.append(f\"[{result[0]},{result[1]}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3160860"}, {"introduction": "在许多现实世界的分类问题中，类别标签并非一个扁平的列表，而是天然地呈现出层级结构，例如生物学中的物种分类树。标准的分类损失函数（如交叉熵）通常会忽略这种结构，将所有类型的误分类同等对待。本练习旨在引导您实践一种更先进的范式——层次感知风险最小化 (hierarchy-aware risk minimization)，它通过精心设计损失函数，将关于标签结构的先验知识融入学习过程 [@problem_id:3160884]。通过亲手实现并比较惩罚不同层级错误的损失函数，您将具体地体会到，如何引导模型做出语义上更合理的预测，例如，将模型混淆两个相似犬种的代价设置得远低于将犬类误判为车辆的代价。", "problem": "考虑一个分层分类任务，其中类别是分类树的叶节点。设该分类树为一个有根树，具有单一根节点和代表类别的内部节点。学习范式是经验风险最小化：给定一个输入，模型会输出一个实值分数（logits）向量，该向量通过 Softmax 函数转换为叶节点类别的概率分布。目标是评估损失函数塑造（loss shaping）的选择如何反映分类体系，并将其与扁平交叉熵（flat Cross-Entropy）进行比较，从而分析一种层次感知风险。\n\n基本原理：\n- 经验风险定义为损失函数在数据分布上的期望。对于真实叶节点类别索引为 $y$、预测的叶节点类别 $k$ 的概率为 $p(k)$ 的单个样本，瞬时风险是所选的损失 $\\ell(y, p)$。\n- Softmax 函数使用自然对数底数 $e$ 将 logits $z(k)$ 转换为概率 $p(k) = \\exp(z(k)) \\big/ \\sum_{j} \\exp(z(j))$。\n- 扁平交叉熵（在叶节点级别定义）为 $\\ell_{\\mathrm{CE}}(y, p) = -\\log p(y)$。\n- 层次感知期望代价是根据叶节点类别之间的分类距离函数 $d(y, k)$ 构建的，得到 $\\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k} p(k) \\, d(y, k)$。\n- 层次感知路径交叉熵（Hierarchy-aware path Cross-Entropy）塑造损失函数，以将概率质量分配到从根到真实叶节点的路径上。设 $P(n)$ 为内部或叶节点 $n$ 的概率质量，定义为其后代叶节点概率的总和。设 $\\mathrm{path}(y)$ 表示从根到 $y$ 的唯一路径上的节点集合。排除根节点（其概率质量恒为 $1$），定义 $\\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n)$。与那些在正确分支内混淆同级节点的预测相比，这种损失函数更严厉地惩罚将概率质量分配到正确分支之外的预测。\n\n分类体系：\n- 该分类体系有一个根节点和两个内部节点，每个内部节点有两个叶节点：\n  - 内部节点 $A$，其叶节点为 $A1$ 和 $A2$。\n  - 内部节点 $B$，其叶节点为 $B1$ 和 $B2$。\n- 叶节点的索引为：\n  - $0 \\leftrightarrow A1$，$1 \\leftrightarrow A2$，$2 \\leftrightarrow B1$，$3 \\leftrightarrow B2$。\n- 叶节点 $i$ 和 $j$ 之间的分类距离 $d(i, j)$ 是树中两个叶节点之间最短路径的长度（以边数计算）。因此：\n  - 对所有 $i$，$d(i, i) = 0$。\n  - 对于同一内部节点下的同级叶节点，$d(0, 1) = 2$ 和 $d(2, 3) = 2$。\n  - 对于不同内部节点下的叶节点，$d(0, 2) = 4$，$d(0, 3) = 4$，$d(1, 2) = 4$，$d(1, 3) = 4$。\n\n测试套件：\n每个测试用例由一对 $(\\text{logits}, y)$ 组成，其中 logits 是一个包含 $4$ 个实数的列表， $y$ 是真实的叶节点索引。在评估损失之前，必须先从 logits 计算出 Softmax 概率 $p(k)$。使用自然对数，并将所有损失计算为实数。将每个损失四舍五入到 $6$ 位小数。\n\n- 用例 $1$（理想情况，正确的叶节点得分很高）：logits $[4, 1, -1, -2]$，$y = 0$。\n- 用例 $2$（$A$ 内部的同级混淆）：logits $[1, 3.5, -1, -2]$，$y = 0$。\n- 用例 $3$（跨分支混淆到 $B$）：logits $[-1, -1.5, 3.0, 2.5]$，$y = 0$。\n- 用例 $4$（无信息量的均匀 logits）：logits $[0, 0, 0, 0]$，$y = 1$。\n- 用例 $5$（极端错误分支集中）：logits $[10, -10, -10, -10]$，$y = 3$。\n\n对于每个测试用例，计算：\n- 扁平交叉熵 $\\ell_{\\mathrm{CE}}(y, p)$。\n- 层次感知期望代价 $\\ell_{\\mathrm{HEC}}(y, p)$。\n- 层次感知路径交叉熵 $\\ell_{\\mathrm{HPCE}}(y, p)$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个逗号分隔的所有测试用例结果列表，并用方括号括起来。每个测试用例的结果本身必须是包含三个浮点数的列表，顺序为 $[\\ell_{\\mathrm{CE}}, \\ell_{\\mathrm{HEC}}, \\ell_{\\mathrm{HPCE}}]$，每个浮点数都四舍五入到 $6$ 位小数。最终输出必须如下所示：\n$[[\\ell_{\\mathrm{CE}}^{(1)}, \\ell_{\\mathrm{HEC}}^{(1)}, \\ell_{\\mathrm{HPCE}}^{(1)}],[\\ell_{\\mathrm{CE}}^{(2)}, \\ell_{\\mathrm{HEC}}^{(2)}, \\ell_{\\mathrm{HPCE}}^{(2)}],\\dots]$。", "solution": "该问题要求计算和比较三种用于分层分类任务的不同损失函数。该任务是在一个特定的树形结构分类体系上定义的。这三种损失函数分别是标准的扁平交叉熵（$\\ell_{\\mathrm{CE}}$）、层次感知期望代价（$\\ell_{\\mathrm{HEC}}$）和层次感知路径交叉熵（$\\ell_{\\mathrm{HPCE}}$）。目标是针对由测试用例表示的五种不同情景评估这些损失，每个测试用例包含一个 logits 向量和-个真实类别标签。\n\n首先，我们形式化问题的各个组成部分。给定一个 logits 向量 $z = [z(0), z(1), z(2), z(3)]$，通过 Softmax 函数可以得到 $4$ 个叶节点类别的概率分布 $p = [p(0), p(1), p(2), p(3)]$：\n$$ p(k) = \\frac{\\exp(z(k))}{\\sum_{j=0}^{3} \\exp(z(j))} \\quad \\text{其中 } k \\in \\{0, 1, 2, 3\\} $$\n在整个计算过程中使用自然对数 $\\log$。\n\n该分类体系是一个具有两级分支的有根树。根节点有两个子节点，即内部节点 $A$ 和 $B$。节点 $A$ 有两个叶子节点，分别是 $A1$（索引 $0$）和 $A2$（索引 $1$）。节点 $B$ 有两个叶子节点，分别是 $B1$（索引 $2$）和 $B2$（索引 $3$）。\n\n对于给定的真实叶节点索引 $y$ 和预测的概率分布 $p$，三种损失函数的定义如下：\n\n$1$. **扁平交叉熵 ($\\ell_{\\mathrm{CE}}$)**：这是真实类别的负对数似然。它将所有类别视为独立的，不考虑层次结构。\n$$ \\ell_{\\mathrm{CE}}(y, p) = -\\log p(y) $$\n\n$2$. **层次感知期望代价 ($\\ell_{\\mathrm{HEC}}$)**：该损失函数将风险定义为真实类别与预测类别之间距离的期望值，其中期望是基于模型的预测概率分布计算的。\n$$ \\ell_{\\mathrm{HEC}}(y, p) = \\sum_{k=0}^{3} p(k) \\, d(y, k) $$\n距离 $d(i, j)$ 是叶节点 $i$ 和叶节点 $j$ 之间最短路径上的边数。对于我们的分类体系，距离矩阵 $D$（其中 $D_{ij}=d(i,j)$）为：\n$$\nD = \\begin{pmatrix}\n0  2  4  4 \\\\\n2  0  4  4 \\\\\n4  4  0  2 \\\\\n4  4  2  0\n\\end{pmatrix}\n$$\n该损失函数根据误分类在层次结构中的距离进行惩罚，同级节点间的混淆（例如，$A1$ 与 $A2$，距离为 $2$）比跨分支的混淆（例如，$A1$ 与 $B1$，距离为 $4$）受到的惩罚更轻。\n\n$3$. **层次感知路径交叉熵 ($\\ell_{\\mathrm{HPCE}}$)**：该损失函数鼓励模型将概率质量分配到从根到真实叶节点的正确路径上。它通过对该路径上所有节点（不包括根节点）的负对数概率求和来计算。\n$$ \\ell_{\\mathrm{HPCE}}(y, p) = -\\sum_{n \\in \\mathrm{path}(y) \\setminus \\{\\mathrm{root}\\}} \\log P(n) $$\n$P(n)$ 是节点 $n$ 的总概率质量，通过对 $n$ 的所有后代叶节点的概率求和得到。对于我们的分类体系：\n- 内部节点 $A$ 的概率：$P(A) = p(0) + p(1)$。\n- 内部节点 $B$ 的概率：$P(B) = p(2) + p(3)$。\n- 叶节点（例如 $P(A1)$）的概率就是它自身的概率，$p(0)$。\n\n对于每个真实类别 $y$，$\\ell_{\\mathrm{HPCE}}$ 的具体公式如下：\n- 若 $y=0$（叶节点 $A1$）：$\\mathrm{path}(0) = \\{\\text{root}, A, A1\\}$。\n    $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log P(A) + \\log P(A1)] = -[\\log(p(0)+p(1)) + \\log p(0)]$。\n- 若 $y=1$（叶节点 $A2$）：$\\mathrm{path}(1) = \\{\\text{root}, A, A2\\}$。\n    $\\ell_{\\mathrm{HPCE}}(1, p) = -[\\log P(A) + \\log P(A2)] = -[\\log(p(0)+p(1)) + \\log p(1)]$。\n- 若 $y=2$（叶节点 $B1$）：$\\mathrm{path}(2) = \\{\\text{root}, B, B1\\}$。\n    $\\ell_{\\mathrm{HPCE}}(2, p) = -[\\log P(B) + \\log P(B1)] = -[\\log(p(2)+p(3)) + \\log p(2)]$。\n- 若 $y=3$（叶节点 $B2$）：$\\mathrm{path}(3) = \\{\\text{root}, B, B2\\}$。\n    $\\ell_{\\mathrm{HPCE}}(3, p) = -[\\log P(B) + \\log P(B2)] = -[\\log(p(2)+p(3)) + \\log p(3)]$。\n\n让我们逐步计算用例 1：logits $[4, 1, -1, -2]$，$y = 0$。\n- **步骤 1：计算概率。**\n    logits 的指数为 $[\\exp(4), \\exp(1), \\exp(-1), \\exp(-2)] \\approx [54.598, 2.718, 0.368, 0.135]$。\n    总和约为 $57.820$。\n    概率向量为 $p \\approx [54.598/57.820, 2.718/57.820, 0.368/57.820, 0.135/57.820] \\approx [0.944285, 0.047012, 0.006363, 0.002341]$。\n- **步骤 2：计算损失。**\n    - $\\ell_{\\mathrm{CE}}(0, p) = -\\log(p(0)) = -\\log(0.944285) \\approx 0.057310$。\n    - $\\ell_{\\mathrm{HEC}}(0, p) = \\sum_k p(k) d(0, k) = p(0) \\cdot 0 + p(1) \\cdot 2 + p(2) \\cdot 4 + p(3) \\cdot 4 \\approx (0.047012 \\cdot 2) + (0.006363 \\cdot 4) + (0.002341 \\cdot 4) \\approx 0.094024 + 0.025452 + 0.009364 \\approx 0.128840$。\n    - $\\ell_{\\mathrm{HPCE}}(0, p) = -[\\log(p(0)+p(1)) + \\log(p(0))] = -[\\log(0.944285+0.047012) + \\log(0.944285)] = -[\\log(0.991297) - 0.057310] \\approx -[-0.008745 - 0.057310] \\approx 0.066055$。\n\n这些结果展示了每种损失函数的特性。例如，在用例 2（同级混淆）中，$\\ell_{\\mathrm{HEC}}$ 显著低于用例 3（跨分支混淆），这反映了错误的分类距离较小。在用例 2 中，$\\ell_{\\mathrm{HPCE}}$ 比 $\\ell_{\\mathrm{CE}}$ 更为宽容，因为大部分概率质量都保持在正确的超类中，但在用例 3 中，由于概率质量泄漏到了一个完全不同的分类体系分支，其值很高。以下 Python 代码系统地将此方法应用于所有测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes three different loss functions for hierarchical classification\n    based on a defined taxonomy and a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ([4, 1, -1, -2], 0),   # Case 1: Happy path\n        ([1, 3.5, -1, -2], 0), # Case 2: Sibling confusion\n        ([-1, -1.5, 3.0, 2.5], 0), # Case 3: Cross-branch confusion\n        ([0, 0, 0, 0], 1),      # Case 4: Uninformative uniform\n        ([10, -10, -10, -10], 3)    # Case 5: Extreme wrong branch\n    ]\n\n    # Taxonomy distance matrix d(i, j)\n    # Leaves: 0=A1, 1=A2, 2=B1, 3=B2\n    # d(i,i)=0\n    # d(0,1)=2, d(2,3)=2\n    # d(0,2)=4, d(0,3)=4, d(1,2)=4, d(1,3)=4\n    distance_matrix = np.array([\n        [0, 2, 4, 4],\n        [2, 0, 4, 4],\n        [4, 4, 0, 2],\n        [4, 4, 2, 0]\n    ])\n\n    results = []\n    \n    for logits, y in test_cases:\n        # Convert logits to numpy array for vectorized operations\n        z = np.array(logits, dtype=np.float64)\n\n        # Compute probabilities using the Softmax function\n        # A numerically stable version of Softmax is used: exp(z - max(z))\n        exps = np.exp(z - np.max(z))\n        p = exps / np.sum(exps)\n\n        # --- Loss Calculation ---\n\n        # 1. Flat Cross-Entropy (l_ce)\n        l_ce = -np.log(p[y])\n\n        # 2. Hierarchy-aware Expected Cost (l_hec)\n        l_hec = np.sum(p * distance_matrix[y])\n\n        # 3. Hierarchy-aware Path Cross-Entropy (l_hpce)\n        # Taxonomy: root - {A, B}; A - {0, 1}; B - {2, 3}\n        if y in [0, 1]:  # True class is in group A\n            # P(A) = p(0) + p(1)\n            p_group = p[0] + p[1]\n            # l_hpce = -(log(P(A)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n        else:  # True class is in group B (y in [2, 3])\n            # P(B) = p(2) + p(3)\n            p_group = p[2] + p[3]\n            # l_hpce = -(log(P(B)) + log(P(leaf)))\n            l_hpce = -(np.log(p_group) + np.log(p[y]))\n            \n        # Round results to 6 decimal places as required\n        case_result = [\n            round(l_ce, 6),\n            round(l_hec, 6),\n            round(l_hpce, 6)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) correctly formats the inner lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160884"}, {"introduction": "由于深度神经网络的高度非线性与高维特性，理解其训练动态是深度学习理论中的一个核心难题。神经正切核 (Neural Tangent Kernel, NTK) 理论提供了一个强大的分析范式，它揭示了在网络宽度趋于无穷大的极限情况下，复杂的神经网络训练动态可以被一个简单的线性模型精确描述。本练习为您提供了一个绝佳的动手实践机会，让您连接抽象理论与具体代码，亲手验证这一核心思想 [@problem_id:3160899]。通过分别实现一个真实神经网络的梯度下降过程以及其对应的NTK线性化动态，您将能够量化比较两者的行为差异，并凭经验探究在何种条件下（例如网络宽度、学习率），这种优雅的线性化范式能够忠实地作为深度学习的近似模型。", "problem": "你需要验证双层神经网络上的参数空间梯度下降与其由神经正切核 (NTK) 控制的函数空间线性化之间的联系。在纯数学和算法的设定下，于监督回归的框架内进行。你的程序必须实现实际的训练动态和线性化的 NTK 近似，比较它们的训练损失轨迹，并在一个小规模测试集上输出它们差异的量化度量。\n\n从以下基本原理出发：\n- 带均方误差的监督学习经验风险：对于预测值 $\\mathbf{p} \\in \\mathbb{R}^{n}$ 和目标值 $\\mathbf{y} \\in \\mathbb{R}^{n}$，经验损失为 $L(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\lVert \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y} \\rVert_2^2$。\n- 参数空间中的全批量梯度下降：$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}_t)$，其中 $\\eta$ 是学习率。\n- 在初始化 $\\boldsymbol{\\theta}_0$ 处计算的雅可比矩阵 $\\mathbf{J}_0 = \\left.\\frac{\\partial \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X})}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0}$，以及定义为 $\\mathbf{K} = \\mathbf{J}_0 \\mathbf{J}_0^\\top$ 的经验神经正切核 (NTK)。\n- 模型输出在初始化点附近的线性化（一阶泰勒展开）：$\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) \\approx \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X}) + \\mathbf{J}_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)$。\n\n问题说明：\n- 数据生成：\n  - 固定整数 $n = 20$ 和 $d = 5$。生成一个输入矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其元素独立从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取。为保证可复现性，使用确定性随机种子 $s_{\\text{data}} = 123$。\n  - 使用相同的种子 $s_{\\text{data}}$ 生成一个教师向量 $\\mathbf{q} \\in \\mathbb{R}^{d}$，其元素独立从 $\\mathcal{N}(0,1)$ 中抽取。通过 $y_i = \\tanh(\\mathbf{q}^\\top \\mathbf{x}_i)$ 来定义目标值，其中 $i \\in \\{1,\\dots,n\\}$，$\\mathbf{x}_i$ 是 $\\mathbf{X}$ 的第 $i$ 行，$\\tanh(\\cdot)$ 是双曲正切函数。\n- 模型：\n  - 考虑一个具有宽度为 $m$ 的单隐藏层和标量输出的双层神经网络，\n    $$f_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^m a_j \\, \\phi(\\mathbf{w}_j^\\top \\mathbf{x}),$$\n    其中 $\\phi(z) = \\tanh(z)$，$\\mathbf{w}_j \\in \\mathbb{R}^{d}$ 和 $a_j \\in \\mathbb{R}$ 是可训练参数，参数向量为 $\\boldsymbol{\\theta} = \\{\\mathbf{W} \\in \\mathbb{R}^{m \\times d}, \\mathbf{a} \\in \\mathbb{R}^{m}\\}$。使用确定性种子 $s_{\\text{param}} = 10 m$ 初始化 $\\mathbf{W}$ 和 $\\mathbf{a}$，其元素为独立的标准正态分布（这样具有相同 $m$ 的测试用例将共享相同的初始化）。\n- 实际训练动态：\n  - 使用学习率 $\\eta$ 和指定的步数 $T$，对 $L(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\lVert \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y} \\rVert_2^2$ 进行全批量梯度下降。在每一步，计算并存储训练损失。\n- NTK 线性化动态：\n  - 仅从上述基本原理出发，推导将模型在 $\\boldsymbol{\\theta}_0$ 附近线性化，并对线性化模型应用参数空间梯度下降步骤所得到的离散时间函数空间更新规则。实现此更新，以在相同的初始化、学习率 $\\eta$ 和步数 $T$ 的条件下，生成在线性化动态下的训练损失预测序列。\n  - 根据雅可比矩阵的定义，在初始化时计算经验 NTK $\\mathbf{K}$（不要假设任何超出定义的封闭形式）。使用它来驱动你推导出的线性化函数空间动态。\n- 比较度量：\n  - 对于选定的 $(m,\\eta,T)$，将实际训练与线性化训练之间的差异定义为\n    $$\\Delta(m,\\eta,T) = \\frac{1}{T+1} \\sum_{t=0}^{T} \\left| L_{\\text{actual}}^{(t)} - L_{\\text{lin}}^{(t)} \\right|.$$\n  - 你的程序必须为下面的每个测试用例计算 $\\Delta(m,\\eta,T)$，并将这些值作为浮点数输出。\n\n测试集：\n- 对所有测试用例使用上述固定数据集，并遵循每个测试的初始化规则 $s_{\\text{param}} = 10 m$。\n- 测试用例是以下元组 $(m,\\eta,T)$：\n  1. $(m=\\;512,\\; \\eta=\\;0.1,\\; T=\\;200)$: 大宽度，中等学习率，较长训练时间。\n  2. $(m=\\;64,\\; \\eta=\\;0.1,\\; T=\\;200)$: 小宽度，相同学习率，较长训练时间。\n  3. $(m=\\;512,\\; \\eta=\\;0.5,\\; T=\\;200)$: 大宽度，较大学习率，较长训练时间。\n  4. $(m=\\;128,\\; \\eta=\\;0.5,\\; T=\\;5)$: 中等宽度，较大学习率，极短训练时间。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含按上述测试集顺序排列的结果，形式为方括号内由逗号分隔的列表，例如：$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。\n- 每个 $\\Delta_i$ 必须以浮点数形式打印。\n\n科学真实性和推导要求：\n- 你的推导必须仅基于所述的基本定义（经验风险、梯度下降、雅可比矩阵、线性化）和标准微积分。不要在问题陈述中假设任何未经证实的简化公式。\n- 角度、物理单位或百分比单位不适用于此问题；不涉及此类单位。\n- 答案必须是一个完整的、可运行的程序，该程序能够完全按照规定格式生成所需的输出。[@problem_id:126]", "solution": "用户要求对双层神经网络在梯度下降下的训练动态与其由神经正切核 (NTK) 描述的线性化之间的一致性进行数值验证。这涉及实现两种动态并比较它们的损失轨迹。该问题在科学上是有效的、适定的，并且所有必要的参数都已提供。\n\n### 基于原理的设计与推导\n\n解决方案基于以下原理和推导：\n\n1.  **模型与损失**：模型是一个带有 `tanh` 激活函数的双层神经网络，由下式给出：\n    $$f_{\\boldsymbol{\\theta}}(\\mathbf{x}) = \\frac{1}{\\sqrt{m}} \\sum_{j=1}^m a_j \\, \\tanh(\\mathbf{w}_j^\\top \\mathbf{x})$$\n    其中参数为 $\\boldsymbol{\\theta} = \\{\\mathbf{W} \\in \\mathbb{R}^{m \\times d}, \\mathbf{a} \\in \\mathbb{R}^{m}\\}$。对于一个有 $n$ 个样本的数据集 $(\\mathbf{X}, \\mathbf{y})$，模型输出的向量是 $\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) \\in \\mathbb{R}^n$。均方误差损失为：\n    $$L(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\lVert \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y} \\rVert_2^2$$\n\n2.  **实际训练动态（参数空间梯度下降）**：参数通过全批量梯度下降进行更新：\n    $$\\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t - \\eta \\nabla_{\\boldsymbol{\\theta}} L(\\boldsymbol{\\theta}_t)$$\n    为了实现这一点，我们需要损失函数关于参数 $\\mathbf{W}$ 和 $\\mathbf{a}$ 的梯度。令 $\\mathbf{r}_t = \\mathbf{f}_{\\boldsymbol{\\theta}_t}(\\mathbf{X}) - \\mathbf{y}$ 为第 $t$ 步的残差向量。令 $z_{ij}^{(t)} = (\\mathbf{w}_j^{(t)})^\\top \\mathbf{x}_i$。梯度通过链式法则推导得出：\n    \n    -   **关于权重 $\\mathbf{a}$ 的梯度**：梯度向量 $\\nabla_{\\mathbf{a}} L \\in \\mathbb{R}^m$ 的分量为：\n        $$\\frac{\\partial L}{\\partial a_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\frac{\\partial f_i}{\\partial a_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\left(\\frac{1}{\\sqrt{m}} \\tanh(z_{ik}^{(t)})\\right)$$\n        以矩阵形式表示，即 $\\nabla_{\\mathbf{a}} L = \\frac{1}{n\\sqrt{m}} \\mathbf{\\Phi}_{t}^\\top \\mathbf{r}_t$，其中 $(\\mathbf{\\Phi}_t)_{ik} = \\tanh(z_{ik}^{(t)})$。\n\n    -   **关于权重 $\\mathbf{W}$ 的梯度**：梯度矩阵 $\\nabla_{\\mathbf{W}} L \\in \\mathbb{R}^{m \\times d}$ 的行为 $\\frac{\\partial L}{\\partial \\mathbf{w}_k}$：\n        $$\\frac{\\partial L}{\\partial \\mathbf{w}_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\frac{\\partial f_i}{\\partial \\mathbf{w}_k} = \\frac{1}{n} \\sum_{i=1}^n r_i^{(t)} \\left(\\frac{1}{\\sqrt{m}} a_k^{(t)} (1 - \\tanh^2(z_{ik}^{(t)})) \\mathbf{x}_i^\\top\\right)$$\n        以矩阵形式表示，这可以写作 $\\nabla_{\\mathbf{W}} L = \\frac{1}{n\\sqrt{m}} (\\mathbf{S}_t^\\top \\mathbf{X})$，其中矩阵 $\\mathbf{S}_t \\in \\mathbb{R}^{n \\times m}$ 的元素为 $(S_t)_{ik} = r_i^{(t)} a_k^{(t)} (1 - \\tanh^2(z_{ik}^{(t)}))$。\n\n3.  **线性化动态（函数空间）**：NTK 框架将模型的输出在其初始参数 $\\boldsymbol{\\theta}_0$ 附近进行线性化。\n    $$\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X}) \\approx \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X}) + \\mathbf{J}_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_0)$$\n    其中 $\\mathbf{J}_0 = \\left.\\frac{\\partial \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{X})}{\\partial \\boldsymbol{\\theta}}\\right|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_0}$ 是在初始化时的雅可比矩阵。\n    \n    对线性化模型的损失应用参数空间梯度步骤，会产生一个函数空间中的更新规则。线性化模型输出 $\\mathbf{f}_{\\text{lin}, t}$ 的演化由下式给出：\n    $$\\mathbf{f}_{\\text{lin}, t+1} = \\mathbf{f}_{\\text{lin}, t} - \\frac{\\eta}{n} \\mathbf{K} (\\mathbf{f}_{\\text{lin}, t} - \\mathbf{y})$$\n    其中 $\\mathbf{K} = \\mathbf{J}_0 \\mathbf{J}_0^\\top$ 是经验神经正切核，一个 $n \\times n$ 的矩阵。模拟从 $\\mathbf{f}_{\\text{lin}, 0} = \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X})$ 开始。\n\n4.  **NTK 计算**：我们可以直接计算 $\\mathbf{K}$，而不是显式地构造巨大的雅可比矩阵 $\\mathbf{J}_0$。核可以分解为来自每层参数贡献的总和：$\\mathbf{K} = \\mathbf{K}_{\\mathbf{a}} + \\mathbf{K}_{\\mathbf{W}}$。\n    \n    -   **来自层 $\\mathbf{a}$ 的核**：\n        $$(\\mathbf{K}_{\\mathbf{a}})_{ij} = \\sum_{k=1}^m \\frac{\\partial f_i}{\\partial a_k} \\frac{\\partial f_j}{\\partial a_k} = \\frac{1}{m} \\sum_{k=1}^m \\tanh(\\mathbf{w}_k^\\top \\mathbf{x}_i) \\tanh(\\mathbf{w}_k^\\top \\mathbf{x}_j)$$\n        这在初始化 $\\boldsymbol{\\theta}_0$ 处进行评估。\n\n    -   **来自层 $\\mathbf{W}$ 的核**：\n        $$(\\mathbf{K}_{\\mathbf{W}})_{ij} = \\sum_{k=1}^m \\sum_{l=1}^d \\frac{\\partial f_i}{\\partial w_{kl}} \\frac{\\partial f_j}{\\partial w_{kl}} = \\left(\\frac{1}{m} \\sum_{k=1}^m a_k^2 (1 - \\tanh^2(\\mathbf{w}_k^\\top \\mathbf{x}_i))(1 - \\tanh^2(\\mathbf{w}_k^\\top \\mathbf{x}_j))\\right)(\\mathbf{x}_i^\\top \\mathbf{x}_j)$$\n        这同样在初始化 $\\boldsymbol{\\theta}_0$ 处进行评估。\n\n### 实现\n\n对于每个测试用例 $(m, \\eta, T)$，算法按以下步骤进行：\n\n1.  **设置**：使用固定种子 $s_{\\text{data}}$ 生成数据 $(\\mathbf{X}, \\mathbf{y})$。使用种子 $s_{\\text{param}} = 10m$ 初始化模型参数 $(\\mathbf{W}_0, \\mathbf{a}_0)$。\n\n2.  **实际训练**：\n    -   初始化 $(\\mathbf{W}, \\mathbf{a}) = (\\mathbf{W}_0, \\mathbf{a}_0)$。\n    -   迭代 $T$ 次：\n        -   计算模型输出 $\\mathbf{f}_{\\boldsymbol{\\theta}_t}(\\mathbf{X})$ 并存储损失 $L^{(t)}_{\\text{actual}}$。\n        -   计算梯度 $\\nabla_{\\mathbf{a}} L$ 和 $\\nabla_{\\mathbf{W}} L$。\n        -   更新参数：$\\mathbf{a}_{t+1} = \\mathbf{a}_t - \\eta \\nabla_{\\mathbf{a}} L$ 和 $\\mathbf{W}_{t+1} = \\mathbf{W}_t - \\eta \\nabla_{\\mathbf{W}} L$。\n    -   存储最终损失 $L^{(T)}_{\\text{actual}}$。这将产生一个包含 $T+1$ 个损失值的轨迹。\n\n3.  **线性化训练**：\n    -   计算初始输出 $\\mathbf{f}_{\\text{lin}, 0} = \\mathbf{f}_{\\boldsymbol{\\theta}_0}(\\mathbf{X})$。\n    -   使用初始参数 $(\\mathbf{W}_0, \\mathbf{a}_0)$ 计算 NTK 矩阵 $\\mathbf{K} = \\mathbf{K}_{\\mathbf{a}} + \\mathbf{K}_{\\mathbf{W}}$。\n    -   迭代 $T$ 次：\n        -   基于当前的 $\\mathbf{f}_{\\text{lin}, t}$ 存储损失 $L^{(t)}_{\\text{lin}}$。\n        -   更新函数输出：$\\mathbf{f}_{\\text{lin}, t+1} = \\mathbf{f}_{\\text{lin}, t} - \\frac{\\eta}{n} \\mathbf{K} (\\mathbf{f}_{\\text{lin}, t} - \\mathbf{y})$。\n    -   存储最终损失 $L^{(T)}_{\\text{lin}}$。这同样会产生一个包含 $T+1$ 个损失值的轨迹。\n\n4.  **比较**：计算差异度量 $\\Delta = \\frac{1}{T+1} \\sum_{t=0}^{T} |L^{(t)}_{\\text{actual}} - L^{(t)}_{\\text{lin}}|$。最终输出是所有测试用例的这些 $\\Delta$ 值的列表。", "answer": "```python\nimport numpy as np\n\ndef generate_data(n, d, seed):\n    \"\"\"\n    Generates the input data matrix X and target vector y.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.standard_normal((n, d), dtype=np.float64)\n    q = rng.standard_normal((d,), dtype=np.float64)\n    y = np.tanh(X @ q)\n    return X, y\n\ndef initialize_params(m, d, seed):\n    \"\"\"\n    Initializes the model parameters W and a.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal((m, d), dtype=np.float64)\n    a = rng.standard_normal((m,), dtype=np.float64)\n    return W, a\n\ndef model_forward(X, W, a):\n    \"\"\"\n    Computes the forward pass of the two-layer neural network.\n    \"\"\"\n    m = W.shape[0]\n    sqrt_m = np.sqrt(m)\n    z = X @ W.T\n    phi_z = np.tanh(z)\n    # Broadcasting of a: (m,) becomes (1, m) and applied to each row of phi_z\n    f_X = (1.0 / sqrt_m) * np.sum(a * phi_z, axis=1)\n    return f_X\n\ndef loss_fn(f_X, y):\n    \"\"\"\n    Computes the mean squared error loss.\n    \"\"\"\n    n = len(y)\n    return (1.0 / (2.0 * n)) * np.sum((f_X - y)**2)\n\ndef actual_training(X, y, W0, a0, eta, T):\n    \"\"\"\n    Performs training using standard full-batch gradient descent.\n    \"\"\"\n    n, d = X.shape\n    m = W0.shape[0]\n    sqrt_m = np.sqrt(m)\n    \n    W, a = W0.copy(), a0.copy()\n    \n    losses = []\n    \n    for t in range(T + 1):\n        # Forward pass\n        z = X @ W.T\n        phi_z = np.tanh(z)\n        f_X = (1.0 / sqrt_m) * np.sum(a * phi_z, axis=1)\n        \n        # Compute and store loss\n        current_loss = loss_fn(f_X, y)\n        losses.append(current_loss)\n        \n        if t == T:\n            break\n            \n        # Backward pass (gradients)\n        residual = f_X - y\n        \n        # Gradient w.r.t. a\n        grad_a = (1.0 / (n * sqrt_m)) * (phi_z.T @ residual)\n        \n        # Gradient w.r.t. W\n        phi_prime_z = 1.0 - phi_z**2\n        S = residual[:, np.newaxis] * phi_prime_z * a[np.newaxis, :]\n        grad_W = (1.0 / (n * sqrt_m)) * (S.T @ X)\n        \n        # Parameter update\n        W -= eta * grad_W\n        a -= eta * grad_a\n        \n    return losses\n    \ndef ntk_linearized_training(X, y, W0, a0, eta, T):\n    \"\"\"\n    Simulates training using the linearized NTK dynamics.\n    \"\"\"\n    n, d = X.shape\n    m = W0.shape[0]\n    \n    # Compute NTK at initialization\n    z0 = X @ W0.T\n    phi_z0 = np.tanh(z0)\n    phi_prime_z0 = 1.0 - phi_z0**2\n    \n    # Kernel part from the second layer (a)\n    K_a = (1.0 / m) * (phi_z0 @ phi_z0.T)\n    \n    # Kernel part from the first layer (W)\n    dot_prods_X = X @ X.T\n    K_W_factor = (1.0 / m) * ((phi_prime_z0 * (a0**2)) @ phi_prime_z0.T)\n    K_W = K_W_factor * dot_prods_X\n    \n    K = K_a + K_W\n    \n    # Linearized dynamics simulation\n    f_lin = model_forward(X, W0, a0)\n    losses = []\n    \n    for t in range(T + 1):\n        # Compute and store loss for the current function estimate\n        current_loss = loss_fn(f_lin, y)\n        losses.append(current_loss)\n        \n        if t == T:\n            break\n            \n        # Update function output in function space\n        residual = f_lin - y\n        f_lin -= (eta / n) * (K @ residual)\n        \n    return losses\n\ndef compute_discrepancy(losses_actual, losses_lin):\n    \"\"\"\n    Computes the average absolute difference between two loss trajectories.\n    \"\"\"\n    return np.mean(np.abs(np.array(losses_actual) - np.array(losses_lin)))\n\ndef solve():\n    \"\"\"\n    Main solver function to run the test suite and print results.\n    \"\"\"\n    # Fixed parameters for data generation\n    n = 20\n    d = 5\n    s_data = 123\n    \n    # Generate data once for all test cases\n    X, y = generate_data(n, d, s_data)\n\n    # Test suite from the problem statement\n    test_cases = [\n        (512, 0.1, 200),\n        (64, 0.1, 200),\n        (512, 0.5, 200),\n        (128, 0.5, 5)\n    ]\n\n    results = []\n    for m, eta, T in test_cases:\n        # Per-test-case parameter initialization seed\n        s_param = 10 * m\n        W0, a0 = initialize_params(m, d, s_param)\n        \n        # Run both actual and linearized training\n        losses_actual = actual_training(X, y, W0, a0, eta, T)\n        losses_lin = ntk_linearized_training(X, y, W0, a0, eta, T)\n        \n        # Compute and store the discrepancy\n        discrepancy = compute_discrepancy(losses_actual, losses_lin)\n        results.append(discrepancy)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160899"}]}