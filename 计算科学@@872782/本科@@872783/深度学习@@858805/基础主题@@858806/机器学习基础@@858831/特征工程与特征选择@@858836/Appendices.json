{"hands_on_practices": [{"introduction": "本章的动手实践将理论付诸于行，从基础的特征构建到高级的流程诊断。第一个练习 [@problem_id:2389794] 将带您深入特征工程的核心，您需要利用原始的生物学数据——蛋白质的氨基酸序列及其溶剂可及性——来构建一个具有明确科学意义的特征。这项任务旨在展示如何运用领域知识，将复杂的数据源提炼为捕捉关键属性（如蛋白质表面电荷平衡）的简洁数值，这对于理解其生物功能至关重要。", "problem": "你的任务是为蛋白质设计一个标量特征，该特征能够捕捉其溶剂暴露表面上带电残基与疏水残基的比例。请使用以下定义和约定。\n\n- 带电残基的集合为 $\\mathcal{C}=\\{R,K,D,E\\}$。\n- 疏水残基的集合为 $\\mathcal{H}=\\{A,V,L,I,P,F,M,W\\}$。\n- 一个蛋白质由长度为 $n$ 的单字母氨基酸序列 $s$ 和一个相应的相对溶剂可及性值向量 $a=(a_1,\\dots,a_n)$ 表示，其中每个 $a_i$ 是在闭区间 $[0,1]$ 内的相对溶剂可及性 (RSA)。相对溶剂可及性定义为一个残基的溶剂可及表面积 (Solvent Accessible Surface Area (SASA)) 除以该残基类型的最大 SASA。\n- 如果 $a_i \\ge \\tau$，则索引为 $i$ 的残基被认为是表面暴露的，其中 $\\tau \\in [0,1]$ 是一个给定的阈值。\n\n设表面索引的集合为 $S=\\{i \\in \\{1,\\dots,n\\}\\,:\\,a_i \\ge \\tau\\}$。将分子 $n_c$ 和分母 $n_h$ 定义为\n$$\nn_c = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{C}\\}, \\quad\nn_h = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{H}\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。所求的特征是比率\n$$\nr =\n\\begin{cases}\n\\frac{n_c}{n_h},  \\text{if } n_h > 0,\\\\\n+\\infty,  \\text{if } n_h = 0 \\text{ and } n_c > 0,\\\\\n0,  \\text{if } n_h = 0 \\text{ and } n_c = 0.\n\\end{cases}\n$$\n任何不属于标准20种氨基酸的残基符号，或模糊/未知符号（例如，$X$、$B$、$Z$、$J$），在计数时必须被忽略；无论其相对溶剂可及性如何，这些残基对 $n_c$ 或 $n_h$ 均无贡献。\n\n请实现一个程序，对下面的每个测试用例，计算所定义的 $r$。对于有限的 $r$，四舍五入到六位小数。如果 $r=+\\infty$，则将其输出为字符串 inf。最终输出必须是单行，包含所有测试用例的结果，按顺序排列，格式为方括号内以逗号分隔的列表，例如 $[x_1,x_2,x_3]$。\n\n测试套件：\n- 用例1：$s=$ \"ACDEFGHIKLMNPQ\"，$a=$ [$0.8$, $0.1$, $0.9$, $0.6$, $0.3$, $0.2$, $0.7$, $0.75$, $0.05$, $0.4$, $0.9$, $0.12$, $0.8$, $0.2$]，$\\tau = 0.5$。\n- 用例2：$s=$ \"RKDEAVILMWFP\"，$a=$ [$0.9$, $0.8$, $0.7$, $0.31$, $0.1$, $0.2$, $0.29$, $0.0$, $0.05$, $0.2$, $0.1$, $0.28$]，$\\tau = 0.3$。\n- 用例3：$s=$ \"AVLIWFPAMGTC\"，$a=$ [$0.8$, $0.6$, $0.7$, $0.9$, $0.2$, $0.55$, $0.52$, $0.1$, $0.51$, $0.05$, $0.2$, $0.49$]，$\\tau = 0.5$。\n- 用例4：$s=$ \"RKDEAVXGHNQ\"，$a=$ [$0.1$, $0.2$, $0.3$, $0.4$, $0.6$, $0.5$, $1.0$, $0.8$, $0.9$, $0.94$, $0.2$]，$\\tau = 0.95$。\n- 用例5：$s=$ \"BEZJRPFA\"，$a=$ [$0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$]，$\\tau = 0.5$。\n\n你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 对应给定顺序中用例 $i$ 的比率。对于有限值，精确打印小数点后六位数字；对于无穷大值，打印 inf。", "solution": "问题陈述经过验证。\n\n**第一步：提取已知信息**\n-   带电残基的集合为 $\\mathcal{C}=\\{R,K,D,E\\}$。\n-   疏水残基的集合为 $\\mathcal{H}=\\{A,V,L,I,P,F,M,W\\}$。\n-   蛋白质由长度为 $n$ 的序列 $s$ 和相对溶剂可及性 (RSA) 值向量 $a=(a_1,\\dots,a_n)$ 描述，其中 $a_i \\in [0,1]$。\n-   索引为 $i$ 的残基在 $a_i \\ge \\tau$ 时为表面暴露，其中 $\\tau \\in [0,1]$ 是一个阈值。\n-   表面暴露索引的集合为 $S=\\{i \\in \\{1,\\dots,n\\}\\,:\\,a_i \\ge \\tau\\}$。\n-   表面暴露的带电残基计数为 $n_c = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{C}\\}$。\n-   表面暴露的疏水残基计数为 $n_h = \\sum_{i \\in S} \\mathbf{1}\\{s_i \\in \\mathcal{H}\\}$。\n-   特征比率 $r$ 定义为：\n    $$\n    r =\n    \\begin{cases}\n    \\frac{n_c}{n_h},  \\text{if } n_h > 0,\\\\\n    +\\infty,  \\text{if } n_h = 0 \\text{ and } n_c > 0,\\\\\n    0,  \\text{if } n_h = 0 \\text{ and } n_c = 0.\n    \\end{cases}\n    $$\n-   任何不属于标准20种氨基酸的残基符号，或模糊符号（例如，$X, B, Z, J$），都将被忽略。\n-   提供了带有序列 $s$、RSA向量 $a$ 和阈值 $\\tau$ 的测试用例。\n-   最终值的格式要求对有限值四舍五入到六位小数，并将 $+\\infty$ 表示为字符串 `inf`。\n\n**第二步：使用提取的已知信息进行验证**\n根据既定标准对问题进行评估：\n-   **科学性**：该问题基于结构生物信息学中的基本概念。将氨基酸分为带电和疏水基团是标准做法。相对溶剂可及性 (RSA) 是量化残基暴露程度的公认度量。使用 RSA 阈值区分表面残基和内部残基是一种常见且有效的简化方法。所设计的特征，即表面带电残基与疏水残基的比率，是蛋白质某些生物物理性质（如与极性溶剂的相互作用）的合理代表。该问题具有科学合理性。\n-   **良构性**：该问题以数学精度陈述。每个测试用例的所有输入（$s, a, \\tau$）都已明确指定。计算计数 $n_c$ 和 $n_h$ 的过程是明确的。比率 $r$ 的定义，包括处理分母 $n_h$ 为零的情况的条件逻辑，是详尽的，并确保对任何有效输入都存在唯一、稳定且有意义的解。\n-   **客观性**：问题描述使用客观、正式的语言。所有术语都得到了明确定义。没有主观陈述或观点。\n\n该问题被确定为完整、一致、现实且结构良好。它没有任何可能导致其无效的缺陷。\n\n**第三步：结论与行动**\n问题有效。将提供一个合理的解决方案。\n\n**解题步骤**\n\n目标是计算一个标量特征 $r$，表示蛋白质溶剂暴露表面上带电残基与疏水残基的比率。对于任何给定的测试用例，包括一个序列 $s$、一个 RSA 向量 $a$ 和一个阈值 $\\tau$，其步骤如下。\n\n1.  **识别表面暴露的残基**：首先，我们确定对应于蛋白质表面残基的索引集合 $S$。如果索引为 $i$（如问题文本中使用的基于1的索引）的氨基酸的 RSA 值 $a_i$ 大于或等于给定的阈值 $\\tau$，则认为该氨基酸是表面暴露的。即，$S = \\{i \\in \\{1, \\dots, n\\} \\mid a_i \\ge \\tau\\}$，其中 $n$ 是序列 $s$ 的长度。\n\n2.  **统计残基类型**：接下来，我们遍历集合 $S$ 中的索引。对于每个索引 $i$，我们检查氨基酸 $s_i$。\n    -   我们初始化两个计数器：$n_c = 0$ 用于带电残基，$n_h = 0$ 用于疏水残基。\n    -   带电残基的集合为 $\\mathcal{C} = \\{R, K, D, E\\}$。\n    -   疏水残基的集合为 $\\mathcal{H} = \\{A, V, L, I, P, F, M, W\\}$。\n    -   对于每个 $i \\in S$，如果残基 $s_i \\in \\mathcal{C}$，我们增加 $n_c$。如果 $s_i \\in \\mathcal{H}$，我们增加 $n_h$。不属于这两个集合的残基，包括模糊或非标准符号如 $X$、$B$、$Z$ 和 $J$，将被忽略，对两个计数均无贡献。\n\n3.  **计算比率 $r$**：最后，我们根据 $n_c$ 和 $n_h$ 的值，遵循指定的三部分逻辑计算比率 $r$：\n    -   如果 $n_h > 0$，比率为 $r = \\frac{n_c}{n_h}$。\n    -   如果 $n_h = 0$ 且 $n_c > 0$，比率定义为 $r = +\\infty$。\n    -   如果 $n_h = 0$ 且 $n_c = 0$，比率定义为 $r = 0$。\n\n此过程应用于每个测试用例。\n\n**测试用例分析**\n\n-   **用例1**：$s=$ \"ACDEFGHIKLMNPQ\"，$a=$ [$0.8$, $0.1$, $0.9$, $0.6$, $0.3$, $0.2$, $0.7$, $0.75$, $0.05$, $0.4$, $0.9$, $0.12$, $0.8$, $0.2$]，$\\tau = 0.5$。\n    -   表面索引 $S$（$a_i \\ge 0.5$ 的索引 $i$）：{$1, 3, 4, 7, 8, 11, 13$}。\n    -   表面残基：$s_1=A, s_3=D, s_4=E, s_7=H, s_8=I, s_{11}=M, s_{13}=P$。\n    -   表面带电残基 ($s_i \\in \\mathcal{C}$)：$D, E$。因此，$n_c = 2$。\n    -   表面疏水残基 ($s_i \\in \\mathcal{H}$)：$A, I, M, P$。因此，$n_h = 4$。\n    -   由于 $n_h > 0$，$r = \\frac{n_c}{n_h} = \\frac{2}{4} = 0.5$。\n\n-   **用例2**：$s=$ \"RKDEAVILMWFP\"，$a=$ [$0.9$, $0.8$, $0.7$, $0.31$, $0.1$, $0.2$, $0.29$, $0.0$, $0.05$, $0.2$, $0.1$, $0.28$]，$\\tau = 0.3$。\n    -   表面索引 $S$（$a_i \\ge 0.3$ 的索引 $i$）：{$1, 2, 3, 4$}。\n    -   表面残基：$s_1=R, s_2=K, s_3=D, s_4=E$。\n    -   表面带电残基：$R, K, D, E$。因此，$n_c = 4$。\n    -   表面疏水残基：无。因此，$n_h = 0$。\n    -   由于 $n_h = 0$ 且 $n_c > 0$，$r = +\\infty$。\n\n-   **用例3**：$s=$ \"AVLIWFPAMGTC\"，$a=$ [$0.8$, $0.6$, $0.7$, $0.9$, $0.2$, $0.55$, $0.52$, $0.1$, $0.51$, $0.05$, $0.2$, $0.49$]，$\\tau = 0.5$。\n    -   表面索引 $S$（$a_i \\ge 0.5$ 的索引 $i$）：{$1, 2, 3, 4, 6, 7, 9$}。\n    -   表面残基：$s_1=A, s_2=V, s_3=L, s_4=I, s_6=F, s_7=P, s_9=A$。\n    -   表面带电残基：无。因此，$n_c = 0$。\n    -   表面疏水残基：$A, V, L, I, F, P, A$。因此，$n_h = 7$。\n    -   由于 $n_h > 0$，$r = \\frac{n_c}{n_h} = \\frac{0}{7} = 0$。\n\n-   **用例4**：$s=$ \"RKDEAVXGHNQ\"，$a=$ [$0.1$, $0.2$, $0.3$, $0.4$, $0.6$, $0.5$, $1.0$, $0.8$, $0.9$, $0.94$, $0.2$]，$\\tau = 0.95$。\n    -   表面索引 $S$（$a_i \\ge 0.95$ 的索引 $i$）：{$7$}。\n    -   表面残基：$s_7=X$。\n    -   残基 $X$ 是非标准的，应被忽略。它对 $n_c$ 和 $n_h$ 均无贡献。\n    -   表面带电残基：无。因此，$n_c = 0$。\n    -   表面疏水残基：无。因此，$n_h = 0$。\n    -   由于 $n_h = 0$ 且 $n_c = 0$，$r = 0$。\n\n-   **用例5**：$s=$ \"BEZJRPFA\"，$a=$ [$0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$, $0.5$]，$\\tau = 0.5$。\n    -   表面索引 $S$（$a_i \\ge 0.5$ 的索引 $i$）：{$1, 2, 3, 4, 5, 6, 7, 8$}。所有残基都在表面上。\n    -   表面残基：$B, E, Z, J, R, P, F, A$。\n    -   模糊/非标准残基 $B, Z, J$ 被忽略。\n    -   从剩余集合 $\\{E, R, P, F, A\\}$ 中找到的表面带电残基：$E, R$。因此，$n_c = 2$。\n    -   从剩余集合中找到的表面疏水残基：$P, F, A$。因此，$n_h = 3$。\n    -   由于 $n_h > 0$，$r = \\frac{n_c}{n_h} = \\frac{2}{3} \\approx 0.666667$。\n\n实现将精确遵循此逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the ratio of charged to hydrophobic \n    residues on the solvent-exposed surface of proteins for a given test suite.\n    \"\"\"\n    \n    test_cases = [\n        {\"s\": \"ACDEFGHIKLMNPQ\", \"a\": [0.8, 0.1, 0.9, 0.6, 0.3, 0.2, 0.7, 0.75, 0.05, 0.4, 0.9, 0.12, 0.8, 0.2], \"tau\": 0.5},\n        {\"s\": \"RKDEAVILMWFP\", \"a\": [0.9, 0.8, 0.7, 0.31, 0.1, 0.2, 0.29, 0.0, 0.05, 0.2, 0.1, 0.28], \"tau\": 0.3},\n        {\"s\": \"AVLIWFPAMGTC\", \"a\": [0.8, 0.6, 0.7, 0.9, 0.2, 0.55, 0.52, 0.1, 0.51, 0.05, 0.2, 0.49], \"tau\": 0.5},\n        {\"s\": \"RKDEAVXGHNQ\", \"a\": [0.1, 0.2, 0.3, 0.4, 0.6, 0.5, 1.0, 0.8, 0.9, 0.94, 0.2], \"tau\": 0.95},\n        {\"s\": \"BEZJRPFA\", \"a\": [0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], \"tau\": 0.5},\n    ]\n\n    charged_residues = {'R', 'K', 'D', 'E'}\n    hydrophobic_residues = {'A', 'V', 'L', 'I', 'P', 'F', 'M', 'W'}\n\n    results = []\n    \n    for case in test_cases:\n        s = case[\"s\"]\n        a = np.array(case[\"a\"])\n        tau = case[\"tau\"]\n        \n        n_c = 0.0  # Numerator: Count of charged surface residues\n        n_h = 0.0  # Denominator: Count of hydrophobic surface residues\n        \n        # Identify surface-exposed indices\n        surface_indices = np.where(a >= tau)[0]\n        \n        for i in surface_indices:\n            residue = s[i]\n            if residue in charged_residues:\n                n_c += 1\n            elif residue in hydrophobic_residues:\n                n_h += 1\n        \n        if n_h > 0:\n            ratio = n_c / n_h\n            results.append(f\"{ratio:.6f}\")\n        elif n_h == 0 and n_c > 0:\n            results.append(\"inf\")\n        elif n_h == 0 and n_c == 0:\n            ratio = 0.0\n            results.append(f\"{ratio:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2389794"}, {"introduction": "在掌握了基础的特征构建后，我们将探索一种更精巧的方法：利用统计模型来生成特征。在本练习 [@problem_id:2389767] 中，您将使用生物信息学中的一个核心工具——位置权重矩阵（Position Weight Matrix, PWM），来计算一个DNA序列的转录因子结合亲和力得分。这个过程清晰地展示了如何将一个预先存在的模型作为强大的特征提取器，把复杂的序列数据转化为一个信息密集的单一数值。", "problem": "给定一个特定转录因子的位置权重矩阵 (PWM) 和一个背景核苷酸分布。对于任何脱氧核糖核酸 (DNA) 序列，定义一个等于预测结合亲和力的单一数值特征，该亲和力计算为在所有窗口和两条链上的最大对数优势 PWM 分数。请使用以下精确的数学定义。\n\n设字母表为核苷酸集合 $\\{A,C,G,T\\}$。令 $L$ 表示基序长度，令 $P \\in \\mathbb{R}^{4 \\times L}$ 是一个按列的概率矩阵，其行按 $(A,C,G,T)$ 排序，使得对于每个列索引 $i \\in \\{0,1,\\dots,L-1\\}$，都有 $\\sum_{b \\in \\{A,C,G,T\\}} P[b,i] = 1$。令 $Q \\in \\mathbb{R}^{4}$ 为核苷酸的背景分布，满足 $\\sum_{b \\in \\{A,C,G,T\\}} Q[b] = 1$ 且对于所有 $b$，都有 $Q[b] > 0$。\n\n定义对数优势权重矩阵 $W \\in \\mathbb{R}^{4 \\times L}$ 为\n$$\nW[b,i] = \\ln\\left(\\frac{P[b,i]}{Q[b]}\\right),\n$$\n其中 $\\ln$ 表示自然对数。\n\n对于一个长度为 $n$、符号在 $\\{A,C,G,T\\}$ 中的 DNA 序列 $S$，以及任何窗口起始位置 $t \\in \\{0,1,\\dots,n-L\\}$，定义正向窗口分数为\n$$\n\\mathrm{score}_{\\mathrm{fwd}}(S,t) = \\sum_{j=0}^{L-1} W[S_{t+j}, j],\n$$\n以及反向窗口分数（扫描反向互补链）为\n$$\n\\mathrm{score}_{\\mathrm{rev}}(S,t) = \\sum_{j=0}^{L-1} W[\\mathrm{comp}(S_{t+L-1-j}), j],\n$$\n其中 $\\mathrm{comp}(\\cdot)$ 将 $A \\leftrightarrow T$ 和 $C \\leftrightarrow G$ 进行映射。\n\n一个窗口 $S[t..t+L-1]$ 是有效的，当且仅当其所有符号都在 $\\{A,C,G,T\\}$ 中。对于任何序列 $S$，定义预测结合亲和力特征为\n$$\n\\phi(S) = \\max\\left\\{ \\mathrm{score}_{\\mathrm{fwd}}(S,t), \\mathrm{score}_{\\mathrm{rev}}(S,t) \\,:\\, t \\in \\{0,\\dots,n-L\\}, \\, S[t..t+L-1]\\ \\text{valid} \\right\\}.\n$$\n如果没有有效的窗口（包括 $n  L$ 的情况），则设 $\\phi(S) = -\\infty$。\n\n使用以下固定参数：\n- 基序长度 $L = 4$。\n- 背景分布 $Q$，按 $(A,C,G,T)$ 排序：\n$$\nQ = [\\,0.3,\\,0.2,\\,0.2,\\,0.3\\,].\n$$\n- 位置权重矩阵 $P$，行按 $(A,C,G,T)$ 排序，列索引为 $i=0,1,2,3$：\n$$\nP = \\begin{bmatrix}\n0.7  0.1  0.1  0.1 \\\\\n0.1  0.1  0.1  0.1 \\\\\n0.1  0.7  0.7  0.1 \\\\\n0.1  0.1  0.1  0.7 \\\\\n\\end{bmatrix}.\n$$\n\n测试套件。按给定顺序为以下每个序列计算 $\\phi(S)$：\n1. $S_1 =$ TTTAGGTAA\n2. $S_2 =$ AG\n3. $S_3 =$ GGGACCTCCC\n4. $S_4 =$ NAGGTN\n5. $S_5 =$ CCCCCCCC\n\n所有扫描都必须遵循上述定义。包含任何在 $\\{A,C,G,T\\}$ 之外的字符的窗口都是无效的，必须被忽略。如果一个序列不存在有效的窗口，则该序列的输出为 $-\\infty$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与测试用例相同，例如 $[\\phi(S_1),\\phi(S_2),\\phi(S_3),\\phi(S_4),\\phi(S_5)]$。这些值必须是自然对数单位的实数，在适用时允许使用 $-\\infty$。不应打印任何额外的文本。", "solution": "该问题陈述已经过严格验证，并被确定为是合理的。它在科学上基于生物信息学的原理，特别是在转录因子结合位点的建模方面。所有参数、定义和数学公式都是适定的、完整的且无歧义的。与已建立的科学理论没有矛盾、事实错误或时代错误。因此，可以推导出一个确定的解决方案。\n\n任务是为几个给定的脱氧核糖核酸 (DNA) 序列计算一个数值特征，记为 $\\phi(S)$。这个特征表示一个转录因子与该序列的预测结合亲和力，定义为通过在正向和反向互补链上用位置权重矩阵 (PWM) 扫描序列所获得的最大对数优势分数。\n\n这个问题的基本组成部分是核苷酸字母表 $\\mathcal{A} = \\{A, C, G, T\\}$、长度为 $L=4$ 的基序、一个位置特异性概率矩阵 $P$ 和一个背景核苷酸分布 $Q$。\n\n给定的参数是：\n- 基序长度 $L=4$。\n- 碱基 $(A,C,G,T)$ 的背景分布 $Q$：\n$$\nQ = [\\,0.3,\\,0.2,\\,0.2,\\,0.3\\,].\n$$\n- 位置权重矩阵 $P$，行为 $(A,C,G,T)$，列为位置 $i \\in \\{0, 1, 2, 3\\}$：\n$$\nP = \\begin{bmatrix}\n0.7  0.1  0.1  0.1 \\\\\n0.1  0.1  0.1  0.1 \\\\\n0.1  0.7  0.7  0.1 \\\\\n0.1  0.1  0.1  0.7 \\\\\n\\end{bmatrix}.\n$$\n\n第一步是根据 $P$ 和 $Q$ 构建对数优势权重矩阵 $W$。每个元素 $W[b,i]$ 计算为基序中位置 $i$ 处碱基 $b$ 的概率与其背景概率之比的自然对数。公式为：\n$$\nW[b,i] = \\ln\\left(\\frac{P[b,i]}{Q[b]}\\right).\n$$\n我们建立一个从核苷酸到行索引的映射：$A \\to 0$, $C \\to 1$, $G \\to 2$, $T \\to 3$。得到的矩阵 $W$ 是：\n$$\nW = \\begin{bmatrix}\n\\ln(0.7/0.3)  \\ln(0.1/0.3)  \\ln(0.1/0.3)  \\ln(0.1/0.3) \\\\\n\\ln(0.1/0.2)  \\ln(0.1/0.2)  \\ln(0.1/0.2)  \\ln(0.1/0.2) \\\\\n\\ln(0.1/0.2)  \\ln(0.7/0.2)  \\ln(0.7/0.2)  \\ln(0.1/0.2) \\\\\n\\ln(0.1/0.3)  \\ln(0.1/0.3)  \\ln(0.1/0.3)  \\ln(0.7/0.3) \\\\\n\\end{bmatrix}\n\\approx\n\\begin{bmatrix}\n 0.8473  -1.0986  -1.0986  -1.0986 \\\\\n-0.6931  -0.6931  -0.6931  -0.6931 \\\\\n-0.6931   1.2528   1.2528  -0.6931 \\\\\n-1.0986  -1.0986  -1.0986   0.8473\n\\end{bmatrix}.\n$$\n\n第二步是处理每个长度为 $n$ 的序列 $S$。我们在序列上滑动一个长度为 $L=4$ 的窗口。对于每个起始位置 $t \\in \\{0, 1, \\dots, n-L\\}$，我们考虑窗口 $S[t..t+L-1]$。一个窗口是有效的，仅当它完全由集合 $\\{A,C,G,T\\}$ 中的字符组成。对于每个有效窗口，我们计算两个分数：\n1.  正向分数，$\\mathrm{score}_{\\mathrm{fwd}}(S,t) = \\sum_{j=0}^{L-1} W[S_{t+j}, j]$。\n2.  反向分数，$\\mathrm{score}_{\\mathrm{rev}}(S,t) = \\sum_{j=0}^{L-1} W[\\mathrm{comp}(S_{t+L-1-j}), j]$，其中 $\\mathrm{comp}(\\cdot)$ 是互补映射 ($A \\leftrightarrow T$, $C \\leftrightarrow G$)。这等同于用 $W$ 对窗口序列的反向互补序列进行评分。\n\n特征 $\\phi(S)$ 是为所有有效窗口计算的所有分数中的最大值。如果没有有效的窗口（例如，如果 $n  L$），则 $\\phi(S) = -\\infty$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the predicted binding affinity for a set of DNA sequences\n    based on a log-odds Position Weight Matrix (PWM) score.\n    \"\"\"\n    # Define fixed parameters as specified in the problem statement.\n    L = 4\n    P = np.array([\n        [0.7, 0.1, 0.1, 0.1],  # Row for 'A'\n        [0.1, 0.1, 0.1, 0.1],  # Row for 'C'\n        [0.1, 0.7, 0.7, 0.1],  # Row for 'G'\n        [0.1, 0.1, 0.1, 0.7]   # Row for 'T'\n    ])\n    Q = np.array([0.3, 0.2, 0.2, 0.3]) # Background for A, C, G, T\n\n    # Calculate the log-odds weight matrix W.\n    # The division is broadcasted: each column of P is divided by the vector Q.\n    # Q[:, np.newaxis] reshapes Q to a column vector for division.\n    W = np.log(P / Q[:, np.newaxis])\n\n    # Helper dictionaries for mapping and sequence manipulation.\n    nuc_to_idx = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n    comp_map = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}\n    valid_nucs = set(nuc_to_idx.keys())\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        \"TTTAGGTAA\",\n        \"AG\",\n        \"GGGACCTCCC\",\n        \"NAGGTN\",\n        \"CCCCCCCC\"\n    ]\n\n    results = []\n    for S in test_cases:\n        n = len(S)\n        max_score = -np.inf\n\n        # Iterate through all possible window starting positions.\n        # The loop range will be empty if n  L, correctly handling such cases.\n        for t in range(n - L + 1):\n            window = S[t:t+L]\n            \n            # A window is valid only if all its characters are in {A, C, G, T}.\n            if not all(c in valid_nucs for c in window):\n                continue\n\n            # Calculate the forward score for the window.\n            fwd_score = sum(W[nuc_to_idx[window[j]], j] for j in range(L))\n            \n            # Calculate the reverse-complement score. This is done by scoring\n            # the reverse complement of the window against the PWM.\n            rev_comp_window = \"\".join(comp_map[c] for c in reversed(window))\n            rev_score = sum(W[nuc_to_idx[rev_comp_window[j]], j] for j in range(L))\n\n            # Update the maximum score found so far for the sequence.\n            current_max = max(fwd_score, rev_score)\n            if current_max > max_score:\n                max_score = current_max\n        \n        results.append(max_score)\n\n    # Final print statement in the exact required format.\n    # The str() function correctly converts float('-inf') to '-inf'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2389767"}, {"introduction": "确保机器学习流程的稳健性是特征工程中的一项高级技能。最后一个练习 [@problem_id:3124151] 将引导您应对一个微妙但极为关键的挑战：标签泄露（label leakage）。您将通过一个模拟实验来理解泄露的信息是如何“污染”并误导模型的，并亲手实现一种基于“哨兵特征”（sentinel features）的统计保障机制，学会如何自动检测并惩罚这些有问题的特征，从而保护您的模型免受此类常见错误的干扰。", "problem": "您将实现一个完整、确定性的程序，通过有意添加泄漏特征、执行特征选择、计算置换重要性以及应用一个惩罚可疑高重要性的保障措施，来检测用于二元分类器的特征工程中的标签泄漏。该程序必须是自包含的，并为给定的测试套件生成结果。本问题中所有的数学符号都以 LaTeX 格式指定，每个符号、变量、函数、运算符和数字都使用行内数学模式编写。\n\n本问题的基础是用于训练的经验风险最小化 (ERM) 定义、用于衡量分类误差的二元交叉熵 (BCE) 以及用于特征相关性的基于置换的重要性，所有这些都是机器学习中经过充分检验的原则。\n\n按如下方式构建一个合成的二元分类数据集。对于每个测试用例，有 $n$ 个样本和三个特征块：\n- 维度为 $d_b$ 的基础特征，对标签具有信息量。\n- 维度为 $d_n$ 的噪声哨兵特征，与标签无关。\n- 维度为 $d_\\ell$ 的泄漏特征，被有意构造成直接携带有关标签的信息。\n\n数据生成：\n1. 为每个样本索引 $i \\in \\{1,\\dots,n\\}$，独立地从 $\\mathcal{N}(\\mathbf{0}, I)$ 分布中抽取基础特征 $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$。\n2. 从一个伪随机种子确定性地抽取一个真实权重向量 $\\mathbf{w}^\\star \\in \\mathbb{R}^{d_b}$ 和偏置 $b^\\star \\in \\mathbb{R}$，并定义 logit $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$。定义标签概率 $p_i = \\sigma(z_i)$，其中 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$，并从参数为 $p_i$ 的伯努利分布中采样二元标签 $y_i \\in \\{0,1\\}$。\n3. 独立地从 $\\mathcal{N}(\\mathbf{0}, I)$ 分布中抽取噪声哨兵特征 $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$，所有这些特征都与 $\\mathbf{x}_i^b$ 和 $y_i$ 无关。\n4. 构造泄漏特征 $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$ 为 $x_{i,j}^\\ell = y_i + \\eta_{i,j}$，适用于每个泄漏特征索引 $j \\in \\{1,\\dots,d_\\ell\\}$，其中 $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$ 且 $s_\\ell$ 是一个给定的泄漏噪声标准差。较小的 $s_\\ell$ 会产生更强的泄漏。\n5. 构建完整的特征向量 $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$，其中 $d = d_b + d_n + d_\\ell$。\n\n将数据集分割为训练集和验证集，使用 $0.7$ 的训练集比例，使得训练集有 $\\lfloor 0.7n \\rfloor$ 个样本，验证集有 $n - \\lfloor 0.7n \\rfloor$ 个样本。使用训练集统计数据对每个特征维度进行标准化：对于每个特征索引 $j \\in \\{1,\\dots,d\\}$，计算训练集均值 $m_j$ 和标准差 $s_j$，然后通过 $\\tilde{x}_{i,j} = \\frac{x_{i,j} - m_j}{s_j}$ 转换训练和验证特征。\n\n训练一个带有一个隐藏层的前馈人工神经网络 (ANN) 分类器：\n- 隐藏层宽度为固定常数 $h$。\n- 隐藏层激活函数是修正线性单元 (ReLU)，$\\text{ReLU}(u) = \\max(0,u)$。\n- 输出是带有 logistic 激活函数 $\\sigma$ 的单个 logit，以产生 $\\hat{y}_i \\in (0,1)$。\n- 在训练集上最小化二元交叉熵 (BCE) 经验风险：\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right),\n$$\n使用全批量梯度下降法，训练固定的周期数和固定的学习率，两者都在程序中指定。\n\n在验证集上计算置换重要性，以量化对每个特征坐标 $j$ 的依赖程度：\n1. 使用训练好的 ANN 和未经置换的验证特征计算基线验证损失 $L_0$。\n2. 对于每个特征索引 $j \\in \\{1,\\dots,d\\}$，独立地对验证特征的第 $j$ 列在样本间进行置换以产生一个扰动过的验证集，计算损失 $L_{(j)}$，并定义重要性：\n$$\nI_j = L_{(j)} - L_0.\n$$\n较大的 $I_j$ 表示对特征 $j$ 的依赖性更强。\n\n使用噪声哨兵特征作为参考零分布，定义一个防止泄漏的保障措施：\n- 设 $J_n \\subset \\{1,\\dots,d\\}$ 表示对应于噪声哨兵特征（基础特征之后的块）的索引集合。\n- 计算重要性的噪声参考均值和标准差：\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j,\n\\qquad\n\\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2}.\n$$\n- 定义一个阈值 $\\tau = \\mu_n + z \\sigma_n$，其中 $z$ 为固定值，惩罚系数 $\\lambda  0$。\n- 为每个特征索引 $j$ 定义惩罚后重要性：\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau).\n$$\n\n在有和没有保障措施的情况下执行选择：\n- 未惩罚选择通过降序排列 $I_j$ 来选择前 $k$ 个索引。\n- 惩罚后选择通过降序排列 $P_j$ 来选择前 $k$ 个索引。\n- 如果 $I_j  \\tau$，则特征索引 $j$ 被标记为可疑。\n- 将标签泄漏检测定义为一个布尔值，如果至少有一个被标记为可疑的特征会被未惩罚选择选中，但被惩罰后选择排除，则该值为真：\n$$\n\\text{detected} = \\left( \\left\\{ j \\mid I_j  \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset,\n$$\n其中 $\\text{TopK}(I)$ 是 $I$ 中 $k$ 个最大条目的索引集合，$\\text{TopK}(P)$ 也类似。\n\n您的程序必须实现上述过程，并评估以下测试套件。每个测试用例是一个包含 $(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$ 的元组：\n- 无泄漏的正常路径：$(42, 600, 5, 5, 0, 0.0, 3)$\n- 轻度泄漏：$(43, 600, 5, 5, 1, 0.9, 3)$\n- 强泄漏：$(44, 600, 5, 5, 2, 0.05, 3)$\n- 边界情况（样本更少，噪声哨兵更多，单个强泄漏）：$(45, 200, 5, 10, 1, 0.1, 3)$\n\n对 ANN 使用固定的超参数：隐藏层宽度 $h = 12$，学习率 $\\alpha = 0.1$，训练周期数 $300$。对保障措施使用 $z = 3$ 和 $\\lambda = 0.8$。所有随机操作必须使用每个测试用例提供的种子以确保确定性。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的检测布尔值，形式为用方括号括起来的逗号分隔列表（例如，$[b_1,b_2,b_3,b_4]$），其中每个 $b_i$ 为 $True$ 或 $False$。", "solution": "用户提供的问题已经过严格验证，并被确定为是合理的。它在科学上基于已建立的机器学习原则，定义明确，具有清晰、确定性的程序，并且表述客观。因此，这里提供一个完整的解决方案。\n\n目标是构建一个确定性程序来检测特征工程中的标签泄漏。这是通过创建一个包含有意泄漏特征的合成数据集，训练一个神经网络分类器，使用置换方法评估特征重要性，并应用一种统计保障措施来识别和惩罚相对于已知无信息特征而言重要性可疑地高的特征来实现的。\n\n该过程的结构如下：\n\n首先，为每个测试用例生成一个合成的二元分类数据集。每个测试用例由一个参数元组定义：$(\\text{seed}, n, d_b, d_n, d_\\ell, s_\\ell, k)$，这些参数控制随机种子、样本数量和特征块的维度。所有随机操作都使用种子以确保确定性。\n-   从标准正态分布 $\\mathcal{N}(\\mathbf{0}, I)$ 中抽取对标签有信息量的基础特征 $\\mathbf{x}_i^b \\in \\mathbb{R}^{d_b}$。\n-   使用确定性生成的权重向量 $\\mathbf{w}^\\star$ 和偏置 $b^\\star$ 定义一个真实线性模型 $z_i = \\mathbf{w}^{\\star \\top}\\mathbf{x}_i^b + b^\\star$。正类的真实概率由 logistic 函数给出，$p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$。然后从参数为 $p_i$ 的伯努利分布中采样二元标签 $y_i \\in \\{0, 1\\}$。\n-   从 $\\mathcal{N}(\\mathbf{0}, I)$ 中抽取无信息量的噪声哨兵特征 $\\mathbf{x}_i^n \\in \\mathbb{R}^{d_n}$，这些特征在统计上与基础特征和标签都无关。\n-   通过用高斯噪声直接扰乱标签来构造潜在的泄漏特征 $\\mathbf{x}_i^\\ell \\in \\mathbb{R}^{d_\\ell}$：$x_{i,j}^\\ell = y_i + \\eta_{i,j}$，其中 $\\eta_{i,j} \\sim \\mathcal{N}(0, s_\\ell^2)$。参数 $s_\\ell$ 控制泄漏的强度；较小的 $s_\\ell$ 意味着更强、更明显的泄漏。如果 $d_\\ell=0$，则此块为空。\n-   每个样本 $i$ 的最终特征向量是拼接而成的 $\\mathbf{x}_i = [\\mathbf{x}_i^b, \\mathbf{x}_i^n, \\mathbf{x}_i^\\ell] \\in \\mathbb{R}^{d}$，其中总维度为 $d = d_b + d_n + d_\\ell$。\n\n接下来，对数据进行预处理。数据集按顺序分割为一个包含前 $\\lfloor 0.7n \\rfloor$ 个样本的训练集和一个包含其余 $n - \\lfloor 0.7n \\rfloor$ 个样本的验证集。为防止验证集的信息泄漏，特征标准化仅在训练集上进行，通过计算每个特征 $j$ 的均值 $m_j$ 和标准差 $s_j$ 来完成。这些统计数据随后用于转换训练和验证数据，根据公式 $\\tilde{x}_{i,j} = (x_{i,j} - m_j) / s_j$。为防止常量特征导致除以零，向 $s_j$ 添加一个小的 epsilon 值。\n\n然后，在标准化后的训练数据上训练一个人工神经网络 (ANN)。该网络具有一个固定宽度 $h=12$ 的单个隐藏层，使用修正线性单元 (ReLU) 激活函数 $\\text{ReLU}(u) = \\max(0,u)$。输出层由一个带有 logistic 激活函数 $\\sigma(u)$ 的神经元组成，以预测概率 $\\hat{y}_i$。训练模型以最小化训练集上的二元交叉熵 (BCE) 损失：\n$$\n\\mathcal{L}_{\\text{BCE}} = -\\frac{1}{n_{\\text{train}}}\\sum_{i=1}^{n_{\\text{train}}}\\left( y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i) \\right).\n$$\n使用全批量梯度下降法进行训练，训练周期为 $300$ 次，学习率为 $\\alpha = 0.1$。\n\n训练后，使用验证集上的置换重要性方法来量化特征的重要性。该方法衡量模型对每个特征的依赖程度。\n-   首先，在原始（未置换的）验证集上计算基线损失 $L_0$。\n-   然后，对于每个特征索引 $j \\in \\{1, \\dots, d\\}$，验证特征矩阵的第 $j$ 列的值被随机置换。在这个扰动过的数据集上重新计算损失 $L_{(j)}$。\n-   特征 $j$ 的置換重要性定义为损失的增加量：$I_j = L_{(j)} - L_0$。一个大的正值 $I_j$ 表明模型在做出准确预测时严重依赖特征 $j$。\n\n实施一种统计保障措施以区分合法的高重要性特征和可能指示泄漏的可疑重要特征。此保障措施使用噪声哨兵特征作为零参考。\n-   重要性的均值 $\\mu_n$ 和标准差 $\\sigma_n$ 仅从噪声哨兵特征集计算得出：\n$$\n\\mu_n = \\frac{1}{|J_n|}\\sum_{j \\in J_n} I_j, \\qquad \\sigma_n = \\sqrt{\\frac{1}{|J_n|}\\sum_{j \\in J_n}(I_j - \\mu_n)^2},\n$$\n其中 $J_n$ 是噪声特征的索引集合。\n-   使用 z-分数乘数 $z=3$ 定义一个统计阈值 $\\tau = \\mu_n + z \\sigma_n$。任何重要性 $I_j  \\tau$ 的特征 $j$ 都会被标记为“可疑”，因为其重要性显著高于一个无信息特征的预期值。\n-   然后为每个特征计算一个惩罚后重要性分数 $P_j$，以折减任何可疑特征的重要性：\n$$\nP_j = I_j - \\lambda \\max(0, I_j - \\tau),\n$$\n惩罚系数为 $\\lambda = 0.8$。这种惩罚降低了重要性超过阈值 $\\tau$ 的特征的分数，使它们不太可能被选中。\n\n最后，通过比较使用和不使用保障措施所做的特征选择来应用泄漏检测逻辑。\n-   未惩罚选择 $\\text{TopK}(I)$ 是按原始重要性 $I_j$ 排名的前 $k$ 个特征的索引集合。\n-   惩罚后选择 $\\text{TopK}(P)$ 是按惩罚后重要性 $P_j$ 排名的前 $k$ 个特征的索引集合。\n-   如果存在至少一个特征同时满足以下条件，则检测到泄漏（评估为 True）：(a) 被标记为可疑（即 $I_j  \\tau$），(b) 是未惩罚 top-k 选择的一部分，以及 (c) 被排除在惩罚后 top-k 选择之外。形式上，如果以下集合非空，则发生检测：\n$$\n\\left( \\left\\{ j \\mid I_j  \\tau \\right\\} \\cap \\text{TopK}(I) \\right) \\setminus \\text{TopK}(P) \\neq \\emptyset.\n$$\n程序对套件中的每个测试用例执行这整个过程，并报告最终的布尔检测结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a deterministic procedure to detect label leakage in feature engineering.\n    \"\"\"\n\n    test_cases = [\n        # (seed, n, d_b, d_n, d_ell, s_ell, k)\n        (42, 600, 5, 5, 0, 0.0, 3),   # Happy path with no leak\n        (43, 600, 5, 5, 1, 0.9, 3),   # Mild leak\n        (44, 600, 5, 5, 2, 0.05, 3),  # Strong leak\n        (45, 200, 5, 10, 1, 0.1, 3), # Boundary case\n    ]\n\n    results = []\n    \n    # Fixed hyperparameters\n    h = 12\n    learning_rate = 0.1\n    epochs = 300\n    z_score_threshold = 3.0\n    penalty_lambda = 0.8\n    train_frac = 0.7\n\n    for case in test_cases:\n        seed, n, d_b, d_n, d_ell, s_ell, k = case\n        \n        # Set seed for determinism in all random operations\n        np.random.seed(seed)\n        \n        # --- 1. Data Generation ---\n        # Ground-truth weights and bias\n        w_star = np.random.randn(d_b, 1)\n        b_star = np.random.randn(1)\n        \n        # Base features\n        X_b = np.random.randn(n, d_b)\n        \n        # Labels\n        z_logits = X_b @ w_star + b_star\n        p_labels = 1 / (1 + np.exp(-z_logits))\n        y = (np.random.rand(n, 1)  p_labels).astype(float)\n        \n        # Noise sentinel features\n        X_n = np.random.randn(n, d_n)\n        \n        # Leaked features\n        if d_ell > 0:\n            eta = np.random.randn(n, d_ell) * s_ell\n            X_ell = y + eta\n        else:\n            X_ell = np.empty((n, 0))\n            \n        # Concatenate all feature blocks\n        X = np.hstack([X_b, X_n, X_ell])\n        d = d_b + d_n + d_ell\n\n        # --- 2. Data Preprocessing ---\n        n_train = int(np.floor(train_frac * n))\n        X_train, X_val = X[:n_train], X[n_train:]\n        y_train, y_val = y[:n_train], y[n_train:]\n\n        train_mean = np.mean(X_train, axis=0)\n        train_std = np.std(X_train, axis=0)\n        # Add epsilon for numerical stability if a feature has zero standard deviation\n        train_std[train_std == 0] = 1e-8\n        \n        X_train_std = (X_train - train_mean) / train_std\n        X_val_std = (X_val - train_mean) / train_std\n\n        # --- 3. ANN Model and Training ---\n        # Initialize weights with Xavier/Glorot scaling\n        W1 = np.random.randn(d, h) * np.sqrt(2.0 / (d + h))\n        b1 = np.zeros(h)\n        W2 = np.random.randn(h, 1) * np.sqrt(2.0 / (h + 1))\n        b2 = np.zeros(1)\n\n        def bce_loss(y_true, y_pred, epsilon=1e-9):\n            y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n            return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n        # Full-batch gradient descent\n        for _ in range(epochs):\n            # Forward pass\n            z1 = X_train_std @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            y_hat = 1 / (1 + np.exp(-z2))\n\n            # Backward pass\n            d_z2 = (y_hat - y_train) / n_train\n            d_W2 = a1.T @ d_z2\n            d_b2 = np.sum(d_z2, axis=0)\n            \n            d_a1 = d_z2 @ W2.T\n            d_z1 = d_a1 * (z1 > 0)\n            d_W1 = X_train_std.T @ d_z1\n            d_b1 = np.sum(d_z1, axis=0)\n\n            # Update weights\n            W1 -= learning_rate * d_W1\n            b1 -= learning_rate * d_b1\n            W2 -= learning_rate * d_W2\n            b2 -= learning_rate * d_b2\n        \n        def predict(X_in):\n            z1 = X_in @ W1 + b1\n            a1 = np.maximum(0, z1)\n            z2 = a1 @ W2 + b2\n            return 1 / (1 + np.exp(-z2))\n\n        # --- 4. Permutation Importance ---\n        baseline_loss = bce_loss(y_val, predict(X_val_std))\n        importances = np.zeros(d)\n        \n        for j in range(d):\n            X_val_perm = X_val_std.copy()\n            # Shuffle in place using the global RNG stream\n            np.random.shuffle(X_val_perm[:, j])\n            permuted_loss = bce_loss(y_val, predict(X_val_perm))\n            importances[j] = permuted_loss - baseline_loss\n\n        # --- 5. Safeguard and Detection Logic ---\n        I = importances\n        noise_indices = range(d_b, d_b + d_n)\n        \n        if d_n > 1:\n            I_noise = I[noise_indices]\n            mu_n = np.mean(I_noise)\n            sigma_n = np.std(I_noise)\n        elif d_n == 1:\n            mu_n = I[noise_indices[0]]\n            sigma_n = 0.0 # Std of a single point is 0\n        else: # d_n == 0\n            mu_n = 0.0\n            sigma_n = 0.0 # Cannot compute threshold without sentinels\n\n        tau = mu_n + z_score_threshold * sigma_n\n        \n        # Penalized importance\n        P = I - penalty_lambda * np.maximum(0, I - tau)\n        \n        # Get top-k indices\n        topk_I_indices = np.argsort(I)[-k:][::-1]\n        topk_P_indices = np.argsort(P)[-k:][::-1]\n        \n        # Convert to sets for formal comparison\n        set_topk_I = set(topk_I_indices)\n        set_topk_P = set(topk_P_indices)\n        \n        # Identify suspicious features\n        suspicious_indices = {j for j, imp in enumerate(I) if imp > tau}\n        \n        # Find suspicious features that are selected by I but deselected by P\n        suspiciously_selected_by_I = suspicious_indices.intersection(set_topk_I)\n        deselected_by_P = suspiciously_selected_by_I.difference(set_topk_P)\n        \n        is_detected = len(deselected_by_P) > 0\n        results.append(is_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3124151"}]}