{"hands_on_practices": [{"introduction": "微调的核心挑战在于如何在适应新任务的同时，避免“灾难性遗忘”预训练模型学到的通用知识。一种关键技术是在优化过程中引入一个机制，鼓励新模型参数不要偏离其预训练的初始状态。\n\n本练习将深入探讨 L2-SP 正则化，这是一种通过惩罚参数与初始值之间距离来实现此目标的方法。通过在一个简化的线性模型上推导其闭式解，你将从数学上直观地理解微调如何在这种“引力”作用下，找到一个平衡新旧知识的最佳点 [@problem_id:3195259]。", "problem": "考虑迁移学习，其中模型首先在源数据集上训练以获得预训练参数 $\\theta_{0} \\in \\mathbb{R}^{d}$，然后使用起点（SP）正则化（也称为 L2 起点（L2-SP）正则化）在目标数据集上进行微调。设目标数据集由设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和目标响应向量 $y \\in \\mathbb{R}^{n}$ 给出。考虑一个线性模型 $f(x) = x^{\\top}\\theta$，并将在目标数据上的均方误差（MSE）经验风险定义为\n$$\n\\mathcal{L}_{\\text{target}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}.\n$$\n使用 L2-SP 正则化进行微调的目标函数为\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\mathcal{L}_{\\text{target}}(\\theta) + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2},\n$$\n其中 $\\lambda > 0$ 是正则化强度。从经验风险最小化和凸二次函数性质的第一性原理出发，解释为什么 L2-SP 项会使微调后的解偏向预训练参数 $\\theta_{0}$，并推导在线性模型下 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的闭式解最小化子。假设 $X$ 和 $\\lambda$ 使得最小化子是唯一的。你的最终答案必须是 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的最小化子 $\\theta^{\\star}$ 的单一闭式解析表达式。不需要数值近似。", "solution": "问题要求两件事：首先，从概念上解释 L2 起点（L2-SP）正则化如何使微调后的参数偏向预训练参数；其次，在线性回归的背景下推导微调参数的闭式解。\n\n首先，我们来解决概念上的解释。使用 L2-SP 正则化进行微调的目标函数由下式给出：\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\mathcal{L}_{\\text{target}}(\\theta) + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}\n$$\n这个目标函数由两项组成。第一项 $\\mathcal{L}_{\\text{target}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}$ 是经验风险，具体来说是目标数据集上的均方误差（MSE）。这一项衡量了参数为 $\\theta$ 的模型对目标数据的拟合程度。单独最小化这一项会使参数趋向于一个能最好地解释目标数据的解，而完全不考虑预训练参数 $\\theta_{0}$。\n\n第二项 $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$ 是 L2-SP 正则化惩罚项。这一项衡量了当前参数 $\\theta$ 和预训练参数 $\\theta_{0}$ 之间的平方欧几里得距离，并由一个正则化超参数 $\\lambda > 0$ 进行缩放。当 $\\theta = \\theta_{0}$ 时，该惩罚项最小。随着 $\\theta$ 远离 $\\theta_{0}$，它会二次方地增加。\n\n最小化总目标函数 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的过程涉及到在这两个相互竞争的目标之间进行权衡，而这个权衡由正则化强度 $\\lambda$ 控制。\n-   为了最小化 $\\mathcal{L}_{\\text{target}}(\\theta)$，优化过程必须调整 $\\theta$ 以减少在目标数据集上的预测误差。\n-   为了最小化 $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$，优化过程必须使 $\\theta$ 尽可能接近起始点 $\\theta_{0}$。\n\n最终的解 $\\theta^{\\star}$ 将是平衡这两种压力的一个点。如果解 $\\theta$ 为了在目标数据上获得更好的拟合而偏离 $\\theta_{0}$ 太远，惩罚项就会增大，从而增加总损失。因此，优化过程不仅受目标数据误差景观的引导，还受到一股将解拉向 $\\theta_{0}$ 的“引力”的影响。这就是为什么说 L2-SP 项会使微调后的解偏向预训练参数。这种偏向的程度由 $\\lambda$ 决定。当 $\\lambda \\to \\infty$ 时，惩罚项占主导地位，解 $\\theta^{\\star}$ 将被迫非常接近 $\\theta_{0}$，即 $\\theta^{\\star} \\to \\theta_{0}$。相反，当 $\\lambda \\to 0$ 时，正则化效应消失，解会收敛到目标数据的标准经验风险最小化子。\n\n接下来，我们推导 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 的闭式解最小化子。目标函数是：\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2} + \\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}\n$$\n函数 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 是两个凸函数之和。项 $\\frac{1}{2n}\\|X\\theta - y\\|_{2}^{2}$ 是凸的，并且由于 $\\lambda > 0$，项 $\\lambda \\|\\theta - \\theta_{0}\\|_{2}^{2}$ 是严格凸的。因此，它们的和是严格凸的，这保证了存在唯一的最小化子。为了找到这个最小化子，我们计算 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 关于 $\\theta$ 的梯度，并将其设为零向量。\n\n首先，我们使用内积的定义 $\\|v\\|_{2}^{2} = v^{\\top}v$ 来展开平方范数项：\n$$\n\\|X\\theta - y\\|_{2}^{2} = (X\\theta - y)^{\\top}(X\\theta - y) = (\\theta^{\\top}X^{\\top} - y^{\\top})(X\\theta - y) = \\theta^{\\top}X^{\\top}X\\theta - 2y^{\\top}X\\theta + y^{\\top}y\n$$\n$$\n\\|\\theta - \\theta_{0}\\|_{2}^{2} = (\\theta - \\theta_{0})^{\\top}(\\theta - \\theta_{0}) = \\theta^{\\top}\\theta - 2\\theta_{0}^{\\top}\\theta + \\theta_{0}^{\\top}\\theta_{0}\n$$\n将这些代回目标函数：\n$$\n\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}(\\theta^{\\top}X^{\\top}X\\theta - 2y^{\\top}X\\theta + y^{\\top}y) + \\lambda(\\theta^{\\top}\\theta - 2\\theta_{0}^{\\top}\\theta + \\theta_{0}^{\\top}\\theta_{0})\n$$\n现在，我们计算梯度 $\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta)$。我们使用以下标准矩阵微积分恒等式：$\\nabla_{z}(z^{\\top}Az) = (A+A^{\\top})z$ 和 $\\nabla_{z}(b^{\\top}z) = b$。由于 $X^{\\top}X$ 和单位矩阵 $I$ 是对称的，我们有 $\\nabla_{\\theta}(\\theta^{\\top}X^{\\top}X\\theta) = 2X^{\\top}X\\theta$ 和 $\\nabla_{\\theta}(\\theta^{\\top}\\theta) = 2\\theta$。\n\n逐项对 $\\mathcal{L}_{\\text{ft}}(\\theta)$ 求导：\n$$\n\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{2n}(2X^{\\top}X\\theta - 2X^{\\top}y) + \\lambda(2\\theta - 2\\theta_{0})\n$$\n$$\n\\nabla_{\\theta}\\mathcal{L}_{\\text{ft}}(\\theta) = \\frac{1}{n}(X^{\\top}X\\theta - X^{\\top}y) + 2\\lambda(\\theta - \\theta_{0})\n$$\n为了找到最小化子 $\\theta^{\\star}$，我们将梯度设为零向量：\n$$\n\\frac{1}{n}(X^{\\top}X\\theta^{\\star} - X^{\\top}y) + 2\\lambda(\\theta^{\\star} - \\theta_{0}) = 0\n$$\n现在，我们求解 $\\theta^{\\star}$。首先，乘以 $n$ 以消去分数：\n$$\nX^{\\top}X\\theta^{\\star} - X^{\\top}y + 2n\\lambda(\\theta^{\\star} - \\theta_{0}) = 0\n$$\n$$\nX^{\\top}X\\theta^{\\star} - X^{\\top}y + 2n\\lambda\\theta^{\\star} - 2n\\lambda\\theta_{0} = 0\n$$\n将包含 $\\theta^{\\star}$ 的项组合在一起：\n$$\n(X^{\\top}X + 2n\\lambda I)\\theta^{\\star} = X^{\\top}y + 2n\\lambda\\theta_{0}\n$$\n其中 $I$ 是 $d \\times d$ 的单位矩阵。问题假设最小化子是唯一的，这意味着矩阵 $(X^{\\top}X + 2n\\lambda I)$ 是可逆的。这一点可以得到保证，因为 $X^{\\top}X$ 是半正定的，并且由于 $n>0$ 和 $\\lambda>0$，$2n\\lambda I$ 是正定的。一个半正定矩阵和一个正定矩阵的和是正定的，因此是可逆的。\n\n最后，我们通过左乘 $(X^{\\top}X + 2n\\lambda I)$ 的逆矩阵来找到 $\\theta^{\\star}$ 的闭式表达式：\n$$\n\\theta^{\\star} = (X^{\\top}X + 2n\\lambda I)^{-1}(X^{\\top}y + 2n\\lambda\\theta_{0})\n$$\n这个表达式就是线性模型下 L2-SP 正则化微调目标的闭式解最小化子。", "answer": "$$\n\\boxed{(X^{\\top}X + 2n\\lambda I)^{-1}(X^{\\top}y + 2n\\lambda\\theta_{0})}\n$$", "id": "3195259"}, {"introduction": "在掌握了基本的微调正则化方法后，我们将转向一个常见的实践陷阱：训练不稳定性。当一次性解冻所有预训练层并使用较大的学习率时，模型损失常常会突然飙升，尤其是在网络的较深层（靠近输入的层）出现梯度爆炸现象。\n\n这个练习模拟了这一真实场景，挑战你分析梯度在深层网络中通过链式法则被逐层放大的根本原因。通过评估不同的解决方案，你将学会识别并运用如分层学习率和逐步解冻等关键策略，以确保微调过程的稳定与高效 [@problem_id:3185080]。", "problem": "考虑一个在源数据集上预训练过的、具有 $L$ 层的深度前馈网络。在迁移到目标数据集时，采用了常见的层冻结方法：在 $E$ 个周期（epoch）内，只微调最终的分类头，而较低的层保持固定。在 $E$ 个周期后，所有层同时解冻并进行联合训练。假设观察到以下现象：\n\n- 在仅训练头的阶段，$\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$ 的学习率（LR）能带来平滑的损失下降。\n- 解冻时，对所有层应用了 $\\alpha = 10^{-2}$ 的统一学习率，训练损失出现激增；在早期层中测得的梯度表现出大范数和快速振荡。\n- 该网络有 $L = 12$ 层；中间雅可比矩阵（将激活值从一层映射到下一层）的经验算子范数在几个连续层中满足 $\\lVert J_k \\rVert \\approx 1.3$ 到 $1.6$。\n\n使用梯度下降更新的定义，即第 $l$ 层的参数更新幅度与 $\\lVert \\Delta \\theta_l \\rVert \\propto \\alpha_l \\lVert g_l \\rVert$ 成比例，以及反向传播链式法则，该法则意味着上游梯度范数可以按层间雅可比矩阵范数的乘积增长。假设目标数据集与源数据集存在中度偏移，因此先前冻结的层最初产生的激活值与新的头尚未良好对齐。\n\n哪一项干预措施最能直接缓解在解冻时刻观察到的梯度爆炸的根本原因，同时保留迁移学习的好处？\n\nA. 一次性解冻所有层，并保持一个较高的统一学习率 $\\alpha = 10^{-2}$，以加速整个网络的适应过程。\n\nB. 逐步解冻各层（从顶层到底层），并应用随深度衰减的分层学习率缩放 $\\alpha_l$（例如，对较低层使用几何衰减），并进行短暂的学习率预热，使 $\\alpha_l$ 从一个较小的值开始缓慢增加。\n\nC. 将优化器更换为不带动量的随机梯度下降（SGD），但在解冻时对所有层保持统一的学习率 $\\alpha = 10^{-2}$。\n\nD. 增加批量大小以减少梯度方差，并将学习率加倍至 $\\alpha = 2 \\times 10^{-2}$ 以维持吞吐量。\n\nE. 在保持 $\\alpha = 10^{-2}$ 并一次性解冻所有层的同时，应用一个非常高的阈值进行梯度裁剪。", "solution": "首先将验证问题陈述的科学性和逻辑合理性。\n\n### 步骤 1：提取已知条件\n- 一个具有 $L=12$ 层的深度前馈网络在源数据集上进行了预训练。\n- 该网络使用层冻结技术迁移到目标数据集。\n- 在 $E$ 个周期内，仅微调最终的分类头（仅训练头阶段）。\n- 在 $E$ 个周期后，所有层被解冻并联合训练。\n- 在仅训练头阶段，$\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$ 的学习率（LR）可以平滑地降低损失。\n- 在解冻所有层后，应用了 $\\alpha = 10^{-2}$ 的统一学习率。\n- 观察：训练损失激增，早期层的梯度表现出大范数和快速振荡。\n- 观察：几个连续层的雅可比矩阵的经验算子范数满足 $\\lVert J_k \\rVert \\approx 1.3$ 到 $1.6$。\n- 定义：第 $l$ 层的参数更新幅度按 $\\lVert \\Delta \\theta_l \\rVert \\propto \\alpha_l \\lVert g_l \\rVert$ 缩放，其中 $g_l$ 是关于参数 $\\theta_l$ 的梯度。\n- 反向传播链式法则意味着上游梯度范数可以按雅可比矩阵范数的乘积增长。\n- 假设：目标数据集与源数据集存在中度偏移，导致冻结的主干网络激活值与新训练的头之间存在初始错位。\n- 问题：确定哪一项干预措施最能直接缓解在解冻时刻观察到的梯度爆炸的根本原因，同时保留迁移学习的好处。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述描述了深度神经网络微调中一个常见且现实的场景。\n- **科学依据：**所提出的概念——迁移学习、层冻结、微调、学习率、梯度爆炸、反向传播和雅可比范数——都是深度学习领域基本且公认的原则。大于 $1$ 的雅可比范数与梯度爆炸可能性之间的关系是链式法则的直接推论。提供的数值是合理的。\n- **适定性：**该问题提供了一个清晰的因果场景，并要求从一组选项中选出最有效的干预措施。它的结构旨在基于已建立的优化和迁移学习理论，得出一个唯一的、先验的最佳答案。\n- **客观性：**语言技术性强、精确，且没有主观或模糊的术语。\n\n问题内部是一致的。用低学习率（$\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$）成功训练头部的初始阶段与对整个网络使用高得多的统一学习率（$\\alpha = 10^{-2}$）时观察到的不稳定性形成对比。这种对比是核心的诊断线索，而非矛盾。问题陈述是有效的。\n\n### 步骤 3：推导与选项分析\n\n该问题描述了在微调预训练网络过程中出现不稳定性的一个典型案例。让我们根据所提供的信息分析其根本原因。\n\n损失 $\\mathcal{L}$ 相对于第 $l-1$ 层激活值 $a_{l-1}$ 的梯度是通过反向传播，由损失相对于第 $l$ 层激活值 $a_l$ 的梯度计算得出的，如下所示：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial a_l} \\frac{\\partial a_l}{\\partial a_{l-1}} = \\frac{\\partial \\mathcal{L}}{\\partial a_l} J_l\n$$\n其中 $J_l$ 是第 $l$ 层变换的雅可比矩阵。因此，梯度的范数受以下公式约束：\n$$\n\\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_{l-1}} \\right\\rVert \\le \\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_l} \\right\\rVert \\left\\lVert J_l \\right\\rVert\n$$\n将此从顶层（$L-1$）反向传播到底层 $l$，梯度范数可能会被放大。跨越 $m$ 层，放大因子可以大到雅可比范数的乘积：\n$$\n\\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_{l-1}} \\right\\rVert \\le \\left\\lVert \\frac{\\partial \\mathcal{L}}{\\partial a_{L-1}} \\right\\rVert \\prod_{k=l}^{L-1} \\left\\lVert J_k \\right\\rVert\n$$\n问题陈述指出 $\\lVert J_k \\rVert \\approx 1.3$ 到 $1.6$。在多层上传播时，这些大于 $1$ 的数的乘积将导致梯度范数的指数级增长。例如，在平均雅可比范数为 $1.4$ 的 $10$ 层上，梯度幅度可能被放大 $1.4^{10} \\approx 28.9$ 倍。这种现象被称为**梯度爆炸问题**。\n\n关于第 $l$ 层参数 $\\theta_l$ 的梯度（表示为 $g_l$）是从 $\\frac{\\partial \\mathcal{L}}{\\partial a_l}$ 导出的。因此，参数梯度 $\\lVert g_l \\rVert$ 的范数在较低层中也会很大。参数更新为 $\\Delta \\theta_l = -\\alpha_l g_l$，其幅度为 $\\lVert \\Delta \\theta_l \\rVert = \\alpha_l \\lVert g_l \\rVert$。\n\n观察到的不稳定性的**根本原因**是两个因素的结合：\n1.  **梯度爆炸**：预训练网络的架构和权重导致雅可比范数 $>1$，使得梯度信号在传播到较低层时被放大。\n2.  **不当的学习率**：一个较大的、统一的学习率 $\\alpha = 10^{-2}$ 被应用于所有层。当这个大的 $\\alpha$ 与较低层中已经很大的梯度范数 $\\lVert g_l \\rVert$ 相乘时，得到的参数更新 $\\Delta \\theta_l$ 是巨大的。这个大的更新步长会破坏在预训练期间学到的、经过精细调整的通用特征的稳定性，导致损失激增和“灾难性遗忘”。分类头可以用小得多的 $\\alpha_{\\mathrm{head}} = 5 \\times 10^{-4}$（小 20 倍）进行训练，这一事实强烈表明 $\\alpha=10^{-2}$ 过于激进，特别是对于敏感的较低层。\n\n理想的干预措施必须解决更新步长与损失函数局部曲率之间的这种根本性不匹配，特别是对于较低层。\n\n**逐项分析：**\n\n**A. 一次性解冻所有层，并保持一个较高的统一学习率 $\\alpha = 10^{-2}$，以加速整个网络的适应过程。**\n这恰恰是导致训练不稳定的操作。它直接引发了问题，而不是缓解问题。对一个已经是功能良好的特征提取器一部分的参数，施加一个大的学习步长，而这些参数正在接收一个被大幅放大的误差信号，这是导致发散的根源。\n**结论：错误。**\n\n**B. 逐步解冻各层（从顶层到底层），并应用随深度衰减的分层学习率缩放 $\\alpha_l$（例如，对较低层使用几何衰减），并进行短暂的学习率预热，使 $\\alpha_l$ 从一个较小的值开始缓慢增加。**\n这个选项提出了一个直接针对根本原因的多方面策略。\n- **分层学习率缩放（判别性学习率）：** 对较低层应用较小的学习率 $\\alpha_l$（例如，$\\alpha_{l}  \\alpha_{l+1}$）可以直接抵消大的梯度范数 $\\lVert g_l \\rVert$。更新幅度 $\\lVert \\Delta \\theta_l \\rVert = \\alpha_l \\lVert g_l \\rVert$ 可以保持稳定且适当小，从而保留早期层中有价值的通用特征。这是修正不当更新步长的最直接方法。\n- **逐步解冻：** 这使得网络能更温和地适应。更具任务特异性的上层首先被调整。这在高度敏感的较低层变得可训练之前，减小了向后传播的误差信号的幅度。\n- **学习率预热：** 将一个已经很小的 $\\alpha_l$ 从一个更小的值开始，并逐渐增加它，可以防止解冻时的初始冲击，让优化器在采取较大步骤之前找到一个稳定的下降方向。\n这种组合是现代迁移学习技术（例如，由 ULMFiT 推广）的主要方法，专门为解决所述的不稳定性问题而设计。它直接解决了根本原因：较低层中参数更新的破坏性幅度。\n**结论：正确。**\n\n**C. 将优化器更换为不带动量的随机梯度下降（SGD），但在解冻时对所有层保持统一的学习率 $\\alpha = 10^{-2}$。**\n虽然像 Adam 或带动量的 SGD 这样的自适应优化器有时可能更具侵略性并导致过冲，但这里的主要问题是更新幅度，它是学习率和梯度范数的乘积。简单地切换到普通的 SGD 并不能改变学习率 $\\alpha = 10^{-2}$ 对于梯度爆炸的较低层来说太大的事实。$\\lVert \\Delta \\theta_l \\rVert$ 过大的根本问题仍未解决。\n**结论：错误。**\n\n**D. 增加批量大小以减少梯度方差，并将学习率加倍至 $\\alpha = 2 \\times 10^{-2}$ 以维持吞吐量。**\n增加批量大小可以减少梯度估计的方差（噪声），这可能具有稳定效果。然而，所描述的问题是梯度*幅度*（范数）的问题，而不是方差的问题。梯度是持续性地大，而不仅仅是噪声大。此外，将学习率*加倍*至 $\\alpha = 2 \\times 10^{-2}$ 的建议将灾难性地恶化情况，因为已经过大的更新步长会变得更大。\n**结论：错误。**\n\n**E. 在保持 $\\alpha = 10^{-2}$ 并一次性解冻所有层的同时，应用一个非常高的阈值进行梯度裁剪。**\n梯度裁剪是一种机制，当梯度的范数超过指定阈值时，它会重新缩放梯度。它是一种反应性措施，治疗的是*症状*（大梯度范数）而不是*原因*。产生大梯度的根本动态（大雅可比矩阵和特征不匹配）没有改变。优化器仍会尝试迈出一大步（由于高学习率 $\\alpha = 10^{-2}$），但步长被人为地限制了。虽然这可以防止损失发散到无穷大，但它不是一种最优的训练方式。较低层的梯度很可能在每一步都被裁剪，这意味着它们的更新方向被保留，但所有幅度信息都丢失了。这导致了低效和次优的学习。选项 B 更优，因为它从一开始就通过使用适当的学习率来解决根本原因。\n**结论：错误。**", "answer": "$$\\boxed{B}$$", "id": "3185080"}, {"introduction": "在许多现实世界的应用中，模型优化的目标不仅仅是最小化预测误差。我们可能还需要模型满足特定的外部约束，例如确保其输出具有单调性以保证可解释性，或满足某些安全条件。\n\n本练习将指导你实现一种更高级的优化技术——投影梯度下降法，以在微调过程中强制施加这类线性约束。通过完成这个编码任务，你不仅能掌握这项强大的技术，还能探索它在减少负迁移方面的潜力，从而为构建更可靠、更值得信赖的AI模型打下基础 [@problem_id:3195172]。", "problem": "考虑一个线性模型，其参数向量为 $\\theta \\in \\mathbb{R}^d$，用于在目标数据集 $(X, y)$ 上进行微调，其中 $X \\in \\mathbb{R}^{n \\times d}$ 且 $y \\in \\mathbb{R}^{n}$。目标函数为均方误差 (MSE)，定义为\n$$\nL(\\theta) = \\frac{1}{2n} \\left\\|X\\theta - y\\right\\|_2^2,\n$$\n其中 $\\left\\|\\cdot\\right\\|_2$ 表示欧几里得范数。从一个预训练的源参数 $\\theta_{\\text{src}}$ 开始的微调是使用梯度下降法执行的。为了在微调期间施加安全性或单调性约束，我们引入线性等式约束 $C\\theta = d$，其中 $C \\in \\mathbb{R}^{m \\times d}$ 且 $d \\in \\mathbb{R}^m$。使用投影梯度更新到仿射约束集上。\n\n从以下基本原理出发：\n- 梯度下降更新定义为 $\\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)$，其中 $\\alpha  0$ 是步长，$\\nabla L(\\theta)$ 是损失函数关于 $\\theta$ 的梯度。\n- 任何 $u \\in \\mathbb{R}^d$ 到仿射集 $\\{\\theta: C\\theta = d\\}$ 上的欧几里得投影定义为在约束 $Cz = d$ 下 $\\left\\|z - u\\right\\|_2^2$ 的最小化子。\n\n实现两种微调程序：\n- 从 $\\theta_{\\text{src}}$ 开始的无约束梯度下降。\n- 从 $\\theta_{\\text{src}}$ 开始的投影梯度下降，其中每个梯度步骤之后都进行到 $\\{\\theta \\in \\mathbb{R}^d : C\\theta = d\\}$ 上的欧几里得投影。\n同时，通过从 $\\theta_{\\text{scratch}} = 0$ 开始的无约束梯度下降，计算一个从零开始的基线。\n\n对于一个初始化为 $\\theta_{\\text{init}}$ 的方法，定义其负迁移量为\n$$\n\\operatorname{NT}(\\theta_{\\text{init}}) = L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{init}})\\bigr) - L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{scratch}})\\bigr),\n$$\n其中 $\\theta_{\\text{final}}(\\cdot)$ 表示经过固定数量的梯度步数后的参数。通过检查以下不等式来测试约束是否减少了负迁移：\n$$\n\\operatorname{NT}_{\\text{constrained}}  \\operatorname{NT}_{\\text{unconstrained}}.\n$$\n\n使用以下固定的目标数据集和超参数：\n- 维度 $d = 3$。\n- 样本数 $n = 4$。\n- 设计矩阵\n$$\nX = \\begin{bmatrix}\n1  0  1 \\\\\n0  1  1 \\\\\n1  1  1 \\\\\n2  0  1\n\\end{bmatrix},\n$$\n和目标标签\n$$\ny = \\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 2\n\\end{bmatrix}.\n$$\n- 梯度下降步长 $\\alpha = 0.1$ 和总步数 $T = 5$。\n\n测试套件（每个测试用例指定 $(\\theta_{\\text{src}}, C, d)$）：\n1. 理想情况（对一个虚假的源特征施加安全约束）：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$，\n   - $C = \\begin{bmatrix} 0  0  1 \\end{bmatrix}$，\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$。\n2. 边界情况（无约束）：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$，\n   - $C \\in \\mathbb{R}^{0 \\times 3}$（$0 \\times 3$ 的空矩阵），\n   - $d \\in \\mathbb{R}^{0}$（空向量）。\n3. 边缘情况（不相关或有害的校准约束）：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 0.5 \\\\ 0.5 \\\\ 5.0 \\end{bmatrix}$，\n   - $C = \\begin{bmatrix} 1  1  0 \\end{bmatrix}$，\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$。\n4. 正迁移场景（源已与目标对齐）：\n   - $\\theta_{\\text{src}} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 0.0 \\end{bmatrix}$，\n   - $C = \\begin{bmatrix} 0  0  1 \\end{bmatrix}$，\n   - $d = \\begin{bmatrix} 0 \\end{bmatrix}$。\n\n您的程序必须：\n- 实现用于 MSE 目标的梯度下降法。\n- 实现投影梯度下降法，在每个梯度步骤后使用欧几里得投影到 $C\\theta = d$ 上。\n- 对每个测试用例，计算布尔值\n$$\nb = \\left( \\operatorname{NT}_{\\text{constrained}}  \\operatorname{NT}_{\\text{unconstrained}} \\right).\n$$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目必须是对应于上述顺序的一个测试用例的布尔值，例如 $\\left[\\text{True},\\text{False},\\text{True},\\text{False}\\right]$。此问题不涉及物理单位或角度单位。所有布尔结果明确表示为 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题要求使用基于梯度的方法，对有约束和无约束的线性模型微调进行比较。目标是确定施加线性等式约束是否能减轻负迁移。评估方法是，将有约束微调模型的最终损失与无约束微调模型的最终损失进行比较，两者都相对于一个从零开始训练的基线模型。\n\n首先，我们定义问题的数学组成部分。模型是其参数 $\\theta \\in \\mathbb{R}^d$ 的线性函数，其在具有 $n$ 个样本的目标数据集 $(X, y)$ 上的性能由均方误差 (MSE) 损失函数度量，其中 $X \\in \\mathbb{R}^{n \\times d}$ 且 $y \\in \\mathbb{R}^n$：\n$$\nL(\\theta) = \\frac{1}{2n} \\|X\\theta - y\\|_2^2 = \\frac{1}{2n} (X\\theta - y)^T(X\\theta - y)\n$$\n这是一个关于 $\\theta$ 的凸且可微的函数。\n\n为了执行梯度下降，我们需要损失函数关于参数 $\\theta$ 的梯度。展开损失函数可得：\n$$\nL(\\theta) = \\frac{1}{2n} (\\theta^T X^T X \\theta - 2y^T X \\theta + y^T y)\n$$\n对 $\\theta$ 求导，得到梯度：\n$$\n\\nabla L(\\theta) = \\frac{1}{2n} (2 X^T X \\theta - 2 X^T y) = \\frac{1}{n} X^T (X\\theta - y)\n$$\n\n问题指定了三种训练程序，都使用固定的步长 $\\alpha  0$ 进行总共 $T$ 次迭代。\n\n1.  **无约束梯度下降**：这个标准算法被用于“从零开始”的基线（从 $\\theta_{\\text{scratch}} = 0$ 开始）和“无约束”的微调（从预训练的 $\\theta_{\\text{src}}$ 开始）。在每一步 $k$ 的更新规则是：\n    $$\n    \\theta_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)\n    $$\n\n2.  **投影梯度下降**：该算法用于“有约束”的微调。它确保参数向量 $\\theta$ 始终满足线性等式约束 $C\\theta = d$，其中 $C \\in \\mathbb{R}^{m \\times d}$ 且 $d \\in \\mathbb{R}^m$。每次迭代包括两个步骤：\n    a. 一个标准的梯度下降步骤，以找到一个中间点 $u_{k+1}$：\n    $$\n    u_{k+1} = \\theta_k - \\alpha \\nabla L(\\theta_k)\n    $$\n    b. 将 $u_{k+1}$ 投影到仿射约束集 $\\mathcal{A} = \\{\\theta \\in \\mathbb{R}^d : C\\theta = d\\}$ 上，以获得下一个迭代值 $\\theta_{k+1}$。该投影表示为 $P_{\\mathcal{A}}(u_{k+1})$，是在欧几里得意义上 $\\mathcal{A}$ 中离 $u_{k+1}$ 最近的点：\n    $$\n    \\theta_{k+1} = P_{\\mathcal{A}}(u_{k+1}) = \\arg\\min_{z \\in \\mathcal{A}} \\|z - u_{k+1}\\|_2^2\n    $$\n\n为了推导投影算子 $P_{\\mathcal{A}}(u)$ 的公式，我们求解约束优化问题 $\\min_z \\frac{1}{2} \\|z - u\\|_2^2$ subject to $Cz = d$。我们使用拉格朗日乘子法。拉格朗日函数为：\n$$\n\\mathcal{L}(z, \\lambda) = \\frac{1}{2} (z - u)^T(z - u) + \\lambda^T(Cz - d)\n$$\n其中 $\\lambda \\in \\mathbb{R}^m$ 是拉格朗日乘子向量。一阶最优性条件为：\n$$\n\\nabla_z \\mathcal{L} = z - u + C^T \\lambda = 0 \\implies z = u - C^T \\lambda\n$$\n将此代入约束 $Cz = d$ 中：\n$$\nC(u - C^T \\lambda) = d \\implies Cu - CC^T \\lambda = d \\implies CC^T \\lambda = Cu - d\n$$\n假设约束矩阵 $C$ 具有满行秩（即其行是线性无关的），则矩阵 $CC^T \\in \\mathbb{R}^{m \\times m}$ 是可逆的。我们可以解出 $\\lambda$：\n$$\n\\lambda = (CC^T)^{-1} (Cu - d)\n$$\n将 $\\lambda$ 代回 $z$ 的表达式，得到投影公式：\n$$\nP_{\\mathcal{A}}(u) = z = u - C^T (CC^T)^{-1} (Cu - d)\n$$\n在没有约束的特殊情况下 ($m=0$)，矩阵 $C$ 是空的 ($C \\in \\mathbb{R}^{0 \\times d}$)。约束集是整个空间 $\\mathbb{R}^d$，投影就是恒等算子，$P_{\\mathcal{A}}(u) = u$。\n\n评估指标是负迁移量 $\\operatorname{NT}(\\theta_{\\text{init}})$，定义为从 $\\theta_{\\text{init}}$ 微调的模型的最终损失与从零开始训练的模型的最终损失之差，即 $\\theta_{\\text{scratch}}$：\n$$\n\\operatorname{NT}(\\theta_{\\text{init}}) = L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{init}})\\bigr) - L\\bigl(\\theta_{\\text{final}}(\\theta_{\\text{scratch}})\\bigr)\n$$\n我们必须测试约束是否减少了负迁移，这被表述为不等式：\n$$\n\\operatorname{NT}_{\\text{constrained}}  \\operatorname{NT}_{\\text{unconstrained}}\n$$\n令 $\\theta_{\\text{final, constrained}}$ 为从投影梯度下降得到的最终参数，$\\theta_{\\text{final, unconstrained}}$ 为从无约束梯度下降得到的最终参数，两者都从 $\\theta_{\\text{src}}$ 开始。不等式变为：\n$$\nL(\\theta_{\\text{final, constrained}}) - L(\\theta_{\\text{final, scratch}})  L(\\theta_{\\text{final, unconstrained}}) - L(\\theta_{\\text{final, scratch}})\n$$\n这可以简化为对两种微调方法的最终损失的直接比较：\n$$\nL(\\theta_{\\text{final, constrained}})  L(\\theta_{\\text{final, unconstrained}})\n$$\n\n为了解决该问题，我们使用所提供的数据 ($X, y$)、超参数 ($\\alpha=0.1, T=5$) 以及每个测试用例的具体信息 $(\\theta_{\\text{src}}, C, d)$ 来实现这三种梯度下降程序。对于每个案例，我们计算最终损失 $L(\\theta_{\\text{final, constrained}})$ 和 $L(\\theta_{\\text{final, unconstrained}})$ 并评估上述不等式的布尔真值。\n\n使用的值为：\n$d = 3$, $n = 4$。\n$X = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  1 \\\\ 2  0  1 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 2 \\end{bmatrix}$。\n$\\alpha = 0.1$, $T = 5$。\n$\\theta_{\\text{scratch}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating constrained fine-tuning against negative transfer.\n    \"\"\"\n    \n    # Fixed target dataset and hyperparameters\n    X = np.array([\n        [1, 0, 1],\n        [0, 1, 1],\n        [1, 1, 1],\n        [2, 0, 1]\n    ])\n    y = np.array([1, 2, 3, 2]).reshape(-1, 1)\n    \n    n, d_dim = X.shape\n    alpha = 0.1\n    T = 5\n    theta_scratch = np.zeros((d_dim, 1))\n\n    # Test Suite (theta_src, C, d)\n    test_cases = [\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.array([[0, 0, 1]]),\n            np.array([[0]])\n        ),\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.empty((0, d_dim)),\n            np.empty((0, 1))\n        ),\n        (\n            np.array([0.5, 0.5, 5.0]).reshape(-1, 1),\n            np.array([[1, 1, 0]]),\n            np.array([[0]])\n        ),\n        (\n            np.array([1.0, 2.0, 0.0]).reshape(-1, 1),\n            np.array([[0, 0, 1]]),\n            np.array([[0]])\n        )\n    ]\n\n    def compute_loss(theta):\n        \"\"\"Computes the MSE loss.\"\"\"\n        return (1 / (2 * n)) * np.sum((X @ theta - y)**2)\n\n    def compute_gradient(theta):\n        \"\"\"Computes the gradient of the MSE loss.\"\"\"\n        return (1 / n) * X.T @ (X @ theta - y)\n\n    def project(u, C, d_vec):\n        \"\"\"Projects a vector u onto the affine set {z : C z = d}.\"\"\"\n        if C.shape[0] == 0:  # No constraints\n            return u\n        \n        # u - C.T @ inv(C @ C.T) @ (C @ u - d)\n        C_T = C.T\n        CC_T = C @ C_T\n        CC_T_inv = np.linalg.inv(CC_T)\n        \n        projection_offset = C_T @ CC_T_inv @ (C @ u - d_vec)\n        return u - projection_offset\n\n    def unconstrained_gd(theta_init):\n        \"\"\"Performs unconstrained gradient descent.\"\"\"\n        theta = theta_init.copy()\n        for _ in range(T):\n            grad = compute_gradient(theta)\n            theta -= alpha * grad\n        return theta\n\n    def projected_gd(theta_init, C, d_vec):\n        \"\"\"Performs projected gradient descent.\"\"\"\n        theta = theta_init.copy()\n        for _ in range(T):\n            grad = compute_gradient(theta)\n            u = theta - alpha * grad\n            theta = project(u, C, d_vec)\n        return theta\n\n    # Calculate baseline loss (from scratch) once\n    theta_final_scratch = unconstrained_gd(theta_scratch)\n    L_final_scratch = compute_loss(theta_final_scratch)\n\n    results = []\n    for theta_src, C, d_vec in test_cases:\n        # Unconstrained fine-tuning\n        theta_final_unconstrained = unconstrained_gd(theta_src)\n        L_final_unconstrained = compute_loss(theta_final_unconstrained)\n        \n        # Constrained fine-tuning\n        theta_final_constrained = projected_gd(theta_src, C, d_vec)\n        L_final_constrained = compute_loss(theta_final_constrained)\n\n        # Negative transfer definitions\n        # NT_unconstrained = L_final_unconstrained - L_final_scratch\n        # NT_constrained = L_final_constrained - L_final_scratch\n        \n        # Check if NT_constrained  NT_unconstrained\n        # This simplifies to L_final_constrained  L_final_unconstrained\n        test_result = L_final_constrained  L_final_unconstrained\n        results.append(str(test_result))\n        \n    # Format and print the final output\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3195172"}]}