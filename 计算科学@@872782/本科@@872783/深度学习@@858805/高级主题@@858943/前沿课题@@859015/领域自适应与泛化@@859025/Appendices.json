{"hands_on_practices": [{"introduction": "领域偏移会以微妙但重要的方式影响神经网络的标准组件。我们将从量化批归一化（Batch Normalization）中源域和目标域统计数据的不匹配如何影响模型性能开始。这个练习 [@problem_id:3117605] 提供了一个清晰的解析视角，让你了解分布偏移是如何直接导致预测误差增加的，从而为理解“为何需要自适应”奠定基础。", "problem": "要求您编写一个完整且可运行的程序，量化在测试时使用不同的批量归一化（Batch Normalization, BN）统计量如何影响偏移目标域上的总体风险，以及输入白化变换是否能减轻这种影响。批量归一化（BN）在推理时应用逐特征的仿射变换 $$y_i = \\gamma_i \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\varepsilon}} + \\beta_i,$$ 其中 $x_i$ 是输入特征，$\\mu_i$ 和 $\\sigma_i$ 是归一化均值和标准差，$\\gamma_i$ 和 $\\beta_i$ 是学习到的缩放和移位参数。目标域上的总体风险定义为 $$R_T(f) = \\mathbb{E}_{(x,y) \\sim T}\\left[\\ell(f(x),y)\\right],$$ 其中损失函数为 $0$-$1$ 损失 $\\ell(\\hat{y},y)=\\mathbf{1}\\{\\hat{y}\\neq y\\}$。\n\n基本设置和假设：\n- 输入 $x \\in \\mathbb{R}^d$ 来自一个各坐标独立的多元正态分布，并且只有第一个坐标决定标签。具体来说，$$x_1 \\sim \\mathcal{N}(\\mu,\\sigma^2), \\quad x_j \\sim \\mathcal{N}(0,1) \\text{ for } j \\in \\{2,\\dots,d\\},$$ 且各坐标之间相互独立。\n- 真实标签为 $$y = \\mathrm{sign}(x_1)$$，并以概率 $p \\in [0,1)$ 独立地翻转（标签噪声），因此，干净标签是 $y_{\\mathrm{clean}}=\\mathrm{sign}(x_1)$，观测到的标签以 $1-p$ 的概率为 $y=y_{\\mathrm{clean}}$，以 $p$ 的概率为 $y=-y_{\\mathrm{clean}}$。\n- 训练好的预测器是一个单层模型，它首先对每个特征使用参数 $(\\mu_{\\mathrm{used}},\\sigma_{\\mathrm{used}},\\gamma,\\beta)$ 应用 BN，然后对第一个归一化坐标应用一个单位权重、零偏置的分类器，$$f(x) = \\mathrm{sign}\\!\\left(\\gamma \\frac{x_1 - \\mu_{\\mathrm{used}}}{\\sigma_{\\mathrm{used}}} + \\beta\\right).$$ 我们考虑 $\\gamma=1$ 和 $\\beta=0$ 的特殊情况，此时分类器简化为 $$f(x)=\\mathrm{sign}\\!\\left(\\frac{x_1 - \\mu_{\\mathrm{used}}}{\\sigma_{\\mathrm{used}}}\\right) = \\mathrm{sign}(x_1 - \\mu_{\\mathrm{used}}),$$ 其中符号不受任何正数缩放的影响。因此，在原始特征轴上，有效的决策阈值为 $\\tau=\\mu_{\\mathrm{used}}$。\n\n- 源域 $S$ 的参数为 $(\\mu_S,\\sigma_S)$。目标域 $T$ 的参数为 $(\\mu_T,\\sigma_T)$。在目标域上进行测试时，我们比较三种推理流程：\n  1. 冻结 BN：使用 $(\\mu_{\\mathrm{used}},\\sigma_{\\mathrm{used}}) = (\\mu_S,\\sigma_S)$，得到决策 $f_{\\mathrm{frozen}}(x) = \\mathrm{sign}(x_1 - \\mu_S)$。\n  2. 在 $T$ 上更新 BN：使用 $(\\mu_{\\mathrm{used}},\\sigma_{\\mathrm{used}}) = (\\mu_T,\\sigma_T)$，得到决策 $f_{\\mathrm{updated}}(x) = \\mathrm{sign}(x_1 - \\mu_T)$。\n  3. 白化后冻结 BN：应用一个逐特征的仿射映射，将目标域第一个坐标的矩与源域对齐，$$x_1' = \\mu_S + \\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T),$$ 然后将 $x'$ 输入到具有 $(\\mu_S,\\sigma_S)$ 和相同 $(\\gamma,\\beta)=(1,0)$ 的冻结 BN 模型中。这种组合产生的有效决策与更新 BN 相同，因为 $$f_{\\mathrm{whiten}\\to\\mathrm{frozen}}(x) = \\mathrm{sign}\\!\\left(\\frac{x_1' - \\mu_S}{\\sigma_S}\\right) = \\mathrm{sign}\\!\\left(\\frac{x_1 - \\mu_T}{\\sigma_T}\\right) = \\mathrm{sign}(x_1 - \\mu_T).$$\n\n根据以上定义和 $x_1$ 的高斯模型，带有标签噪声 $p$ 和 $x_1$ 上的阈值 $\\tau$ 的总体风险可以表示为 $$R_T(\\tau) = p + (1-2p)\\,\\mathbb{P}_{x_1 \\sim \\mathcal{N}(\\mu_T,\\sigma_T^2)}\\!\\left[\\mathrm{sign}(x_1)\\neq \\mathrm{sign}(x_1 - \\tau)\\right].$$ 不一致事件 $\\{\\mathrm{sign}(x_1)\\neq \\mathrm{sign}(x_1 - \\tau)\\}$ 仅在 $x_1$ 位于 $0$ 和 $\\tau$ 之间时发生。因此，令 $\\Phi$ 表示标准正态累积分布函数，则 $$\\mathbb{P}\\!\\left[\\mathrm{sign}(x_1)\\neq \\mathrm{sign}(x_1 - \\tau)\\right] = \\left|\\Phi\\!\\left(\\frac{\\max\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right) - \\Phi\\!\\left(\\frac{\\min\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right)\\right|.$$\n\n您的程序必须：\n- 通过将冻结 BN 的 $\\tau=\\mu_S$ 和更新 BN 及白化后冻结 BN 的 $\\tau=\\mu_T$ 代入上述公式，并使用高斯累积分布函数，实现对三种流程的 $R_T(f)$ 的精确计算。\n- 使用下面的测试套件，计算每种情况下的三种风险，并将每个风险值四舍五入到 $6$ 位小数。\n\n所有测试中保持不变的参数：\n- 源域统计量 $(\\mu_S,\\sigma_S) = (\\,0.5,\\,1.0\\,)$。\n- 标签噪声 $p = 0.1$。\n- 维度 $d = 5$ （只有第一个坐标影响标签；其他坐标是无关的独立标准正态变量，不影响计算）。\n\n测试套件（每个项目指定目标域的 $(\\mu_T,\\sigma_T)$）：\n- 情况 A：$(\\,0.5,\\,1.0\\,)$。\n- 情况 B：$(\\,0.0,\\,1.0\\,)$。\n- 情况 C：$(\\,0.5,\\,0.5\\,)$。\n- 情况 D：$(\\,-1.0,\\,2.0\\,)$。\n\n对于每种情况，按顺序 $[R_T(\\text{冻结 BN}), R_T(\\text{更新 BN}), R_T(\\text{白化后冻结})]$ 输出一个包含三个浮点数的列表。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表，每个子列表对应一个测试用例，并用方括号括起来。例如，格式必须像 $[[r_{A,1},r_{A,2},r_{A,3}],[r_{B,1},r_{B,2},r_{B,3}],\\dots]$，其中每个 $r_{\\cdot,\\cdot}$ 都是四舍五入到 $6$ 位小数的浮点数。", "solution": "该问题要求针对一个简单的分类器，在三种不同的批量归一化（BN）参数测试时策略下，计算其在目标域上的总体风险。该问题定义明确，具有科学依据，并为得出唯一解提供了所有必要信息。\n\n预测器 $f$ 在目标域 $T$ 上的总体风险由 $R_T(f) = \\mathbb{E}_{(x,y) \\sim T}\\left[\\ell(f(x),y)\\right]$ 给出，其中损失是 $0$-$1$ 损失，$\\ell(\\hat{y},y)=\\mathbf{1}\\{\\hat{y}\\neq y\\}$。数据生成过程指定，在目标域上，输入坐标 $x_1$ 从正态分布 $\\mathcal{N}(\\mu_T, \\sigma_T^2)$ 中抽取，真实标签为 $y_{\\mathrm{clean}} = \\mathrm{sign}(x_1)$。观测到的标签 $y$ 受到对称标签噪声的影响，以概率 $p$ 翻转。在 $\\gamma=1$ 和 $\\beta=0$ 的特殊情况下，简化 BN 变换后的预测器为 $f(x) = \\mathrm{sign}(x_1 - \\tau)$，其中 $\\tau$ 是一个有效的决策阈值。\n\n总风险可以根据标签噪声进行分解。错分类的概率为：\n$$R_T(f) = \\mathbb{P}(f(x) \\neq y) = (1-p) \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}}) + p \\mathbb{P}(f(x) = y_{\\mathrm{clean}})$$\n由于 $\\mathbb{P}(f(x) = y_{\\mathrm{clean}}) = 1 - \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}})$，我们可以将风险重写为：\n$$R_T(f) = (1-p) \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}}) + p (1 - \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}}))$$\n$$R_T(f) = p + (1-2p) \\mathbb{P}(f(x) \\neq y_{\\mathrm{clean}})$$\n不一致事件 $f(x) \\neq y_{\\mathrm{clean}}$ 对应于 $\\mathrm{sign}(x_1-\\tau) \\neq \\mathrm{sign}(x_1)$。这恰好在 $x_1$ 落在 $0$ 和 $\\tau$ 之间的区间时发生。对于 $x_1 \\sim \\mathcal{N}(\\mu_T, \\sigma_T^2)$，此事件的概率由高斯概率密度函数在该区间上的积分给出。这可以用标准正态分布的累积分布函数（CDF）表示，记为 $\\Phi(\\cdot)$。\n$$\\mathbb{P}(\\mathrm{sign}(x_1) \\neq \\mathrm{sign}(x_1 - \\tau)) = \\mathbb{P}(\\min(0,\\tau)  x_1  \\max(0,\\tau))$$\n$$= \\Phi\\left(\\frac{\\max(0,\\tau) - \\mu_T}{\\sigma_T}\\right) - \\Phi\\left(\\frac{\\min(0,\\tau) - \\mu_T}{\\sigma_T}\\right)$$\n由于概率必须为非负数，且 $\\max$ 和 $\\min$ 的顺序取决于 $\\tau$ 的符号，因此表达式如题目所提供，使用了绝对值：\n$$\\mathbb{P}(\\text{disagreement}) = \\left|\\Phi\\left(\\frac{\\max\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right) - \\Phi\\left(\\frac{\\min\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right)\\right|$$\n因此，目标域上决策阈值为 $\\tau$ 的总体风险的完整表达式为：\n$$R_T(\\tau) = p + (1-2p)\\left|\\Phi\\left(\\frac{\\max\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right) - \\Phi\\left(\\frac{\\min\\{0,\\tau\\}-\\mu_T}{\\sigma_T}\\right)\\right|$$\n我们给定了固定参数：源域统计量 $(\\mu_S, \\sigma_S) = (0.5, 1.0)$ 和标签噪声概率 $p=0.1$。因子 $(1-2p)$ 为 $1 - 2(0.1) = 0.8$。\n\n三种推理流程对应于阈值 $\\tau$ 的不同选择：\n1.  **冻结 BN**：使用源域的 BN 统计量，因此 $(\\mu_{\\mathrm{used}}, \\sigma_{\\mathrm{used}}) = (\\mu_S, \\sigma_S)$。分类器变为 $f(x) = \\mathrm{sign}(x_1 - \\mu_S)$。有效阈值为 $\\tau_{\\mathrm{frozen}} = \\mu_S = 0.5$。\n2.  **在 T 上更新 BN**：在目标域上重新计算 BN 统计量，因此 $(\\mu_{\\mathrm{used}}, \\sigma_{\\mathrm{used}}) = (\\mu_T, \\sigma_T)$。分类器为 $f(x) = \\mathrm{sign}(x_1 - \\mu_T)$。有效阈值为 $\\tau_{\\mathrm{updated}} = \\mu_T$。\n3.  **白化后冻结 BN**：首先应用一个初始仿射变换 $x_1' = \\mu_S + \\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T)$，然后是冻结的 BN 模型。决策函数为 $f(x) = \\mathrm{sign}\\left(\\frac{x_1' - \\mu_S}{\\sigma_S}\\right)$。代入 $x_1'$ 的表达式得到：\n$$ \\mathrm{sign}\\left(\\frac{\\left(\\mu_S + \\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T)\\right) - \\mu_S}{\\sigma_S}\\right) = \\mathrm{sign}\\left(\\frac{\\frac{\\sigma_S}{\\sigma_T}(x_1 - \\mu_T)}{\\sigma_S}\\right) = \\mathrm{sign}\\left(\\frac{x_1 - \\mu_T}{\\sigma_T}\\right) $$\n由于 $\\sigma_T  0$，这等价于 $\\mathrm{sign}(x_1 - \\mu_T)$。因此，有效阈值为 $\\tau_{\\mathrm{whiten}} = \\mu_T$，与更新 BN 的情况相同。因此，它们的总体风险将始终相等。\n\n我们现在计算每个测试用例的风险。\n\n**情况 A：** 目标域参数 $(\\mu_T, \\sigma_T) = (0.5, 1.0)$。\n- 冻结 BN：$\\tau = 0.5$。风险参数为 $\\left|\\Phi\\left(\\frac{0.5 - 0.5}{1.0}\\right) - \\Phi\\left(\\frac{0 - 0.5}{1.0}\\right)\\right| = |\\Phi(0) - \\Phi(-0.5)| \\approx 0.191462$。\n$R_T = 0.1 + 0.8 \\times 0.191462 = 0.253170$。\n- 更新 BN：$\\tau = \\mu_T = 0.5$。计算与冻结 BN 相同。$R_T = 0.253170$。\n- 白化后冻结 BN：风险与更新 BN 相同。$R_T = 0.253170$。\n\n**情况 B：** 目标域参数 $(\\mu_T, \\sigma_T) = (0.0, 1.0)$。\n- 冻结 BN：$\\tau = 0.5$。风险参数为 $\\left|\\Phi\\left(\\frac{0.5 - 0.0}{1.0}\\right) - \\Phi\\left(\\frac{0 - 0.0}{1.0}\\right)\\right| = |\\Phi(0.5) - \\Phi(0)| \\approx 0.191462$。\n$R_T = 0.1 + 0.8 \\times 0.191462 = 0.253170$。\n- 更新 BN：$\\tau = \\mu_T = 0.0$。不一致区间为从 $0$ 到 $0$，其测度为零。不一致概率为 $0$。\n$R_T = 0.1 + 0.8 \\times 0 = 0.100000$。\n- 白化后冻结 BN：风险与更新 BN 相同。$R_T = 0.100000$。\n\n**情况 C：** 目标域参数 $(\\mu_T, \\sigma_T) = (0.5, 0.5)$。\n- 冻结 BN：$\\tau = 0.5$。风险参数为 $\\left|\\Phi\\left(\\frac{0.5-0.5}{0.5}\\right) - \\Phi\\left(\\frac{0-0.5}{0.5}\\right)\\right| = |\\Phi(0) - \\Phi(-1.0)| \\approx 0.341345$。\n$R_T = 0.1 + 0.8 \\times 0.341345 = 0.373076$。\n- 更新 BN：$\\tau = \\mu_T = 0.5$。计算与冻结 BN 相同。$R_T = 0.373076$。\n- 白化后冻结 BN：风险与更新 BN 相同。$R_T = 0.373076$。\n\n**情况 D：** 目标域参数 $(\\mu_T, \\sigma_T) = (-1.0, 2.0)$。\n- 冻结 BN：$\\tau = 0.5$。风险参数为 $\\left|\\Phi\\left(\\frac{0.5 - (-1.0)}{2.0}\\right) - \\Phi\\left(\\frac{0 - (-1.0)}{2.0}\\right)\\right| = |\\Phi(0.75) - \\Phi(0.5)| \\approx 0.081911$。\n$R_T = 0.1 + 0.8 \\times 0.081911 = 0.165529$。\n- 更新 BN：$\\tau = \\mu_T = -1.0$。风险参数为 $\\left|\\Phi\\left(\\frac{0 - (-1.0)}{2.0}\\right) - \\Phi\\left(\\frac{-1.0 - (-1.0)}{2.0}\\right)\\right| = |\\Phi(0.5) - \\Phi(0)| \\approx 0.191462$。\n$R_T = 0.1 + 0.8 \\times 0.191462 = 0.253170$。\n- 白化后冻结 BN：风险与更新 BN 相同。$R_T = 0.253170$。\n\n这些计算在所提供的程序中实现。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes population risks for three Batch Normalization inference strategies\n    across a suite of test cases.\n    \"\"\"\n\n    # --- Fixed Parameters ---\n    # Source domain statistics\n    mu_S = 0.5\n    # sigma_S is not directly used in the simplified risk calculation,\n    # as it's only a positive scaling factor for the sign function's argument.\n    # sigma_S = 1.0\n\n    # Label noise probability\n    p = 0.1\n\n    # --- Test Cases ---\n    # Each case is a tuple (mu_T, sigma_T) for the target domain.\n    test_cases = [\n        (0.5, 1.0),   # Case A\n        (0.0, 1.0),   # Case B\n        (0.5, 0.5),   # Case C\n        (-1.0, 2.0)   # Case D\n    ]\n\n    def calculate_risk(tau, mu_T, sigma_T, p):\n        \"\"\"\n        Calculates the population risk R_T(tau).\n\n        Args:\n            tau (float): The decision threshold on the original feature x_1.\n            mu_T (float): The mean of x_1 in the target domain.\n            sigma_T (float): The standard deviation of x_1 in the target domain.\n            p (float): The label noise probability.\n\n        Returns:\n            float: The calculated population risk.\n        \"\"\"\n        # The disagreement event {sign(x_1) != sign(x_1 - tau)} occurs when x_1 is\n        # between 0 and tau.\n        # We calculate the probability of this event using the standard normal CDF.\n        z_arg1 = (max(0, tau) - mu_T) / sigma_T\n        z_arg2 = (min(0, tau) - mu_T) / sigma_T\n        \n        # The absolute value handles cases where tau  0.\n        disagreement_prob = abs(norm.cdf(z_arg1) - norm.cdf(z_arg2))\n        \n        # The risk is calculated based on the derived formula.\n        risk = p + (1 - 2 * p) * disagreement_prob\n        return risk\n\n    all_results = []\n    for mu_T, sigma_T in test_cases:\n        case_results = []\n\n        # 1. Frozen BN: uses source mean as threshold.\n        tau_frozen = mu_S\n        risk_frozen = calculate_risk(tau_frozen, mu_T, sigma_T, p)\n        case_results.append(risk_frozen)\n\n        # 2. Updated BN on T: uses target mean as threshold.\n        tau_updated = mu_T\n        risk_updated = calculate_risk(tau_updated, mu_T, sigma_T, p)\n        case_results.append(risk_updated)\n\n        # 3. Whitening then Frozen BN: effective threshold is also the target mean.\n        # Therefore, its risk is identical to Updated BN.\n        risk_whiten = risk_updated\n        case_results.append(risk_whiten)\n        \n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists, with each risk\n    # value formatted to 6 decimal places.\n    output_parts = []\n    for res_list in all_results:\n        formatted_nums = [f\"{r:.6f}\" for r in res_list]\n        output_parts.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3117605"}, {"introduction": "既然我们已经看到了领域偏移如何损害性能，下一步自然是尝试修改我们的训练目标以增强模型的鲁棒性。这个实践 [@problem_id:3117567] 将探索一种基于正则化的方法，通过惩罚模型对特定领域特征的依赖来实现。通过推导源域和目标域风险作为自适应强度的函数，你将揭示出在源域专业化与向目标域泛化之间的基本权衡。", "problem": "给定一个深度学习中的领域自适应和泛化场景，该场景使用经验风险最小化 (ERM)。考虑一个带有参数向量 $w \\in \\mathbb{R}^2$ 和平方损失的线性预测器。源域，记为 $S$，其输入 $x \\in \\mathbb{R}^2$ 服从均值为 $\\mu_S \\in \\mathbb{R}^2$、协方差为单位矩阵 $I_2$ 的高斯分布。目标域，记为 $T$，其输入 $x \\in \\mathbb{R}^2$ 服从均值为 $\\mu_T \\in \\mathbb{R}^2$、协方差为单位矩阵 $I_2$ 的高斯分布。观测噪声的方差为 $\\sigma^2$ 且与 $x$ 独立。源域中的标签遵循线性关系 $y_S = w_S^{\\top} x + \\varepsilon$，目标域中的标签遵循 $y_T = w_T^{\\top} x + \\varepsilon$，其中 $\\varepsilon$ 是均值为零、方差为 $\\sigma^2$ 的噪声。定义一个单位方向 $d \\in \\mathbb{R}^2$ 来捕捉领域差异所在的轴，以及一个与 $d$ 正交的领域不变方向 $u \\in \\mathbb{R}^2$。设 $w_S = u + a d$ 和 $w_T = u$，其中 $a \\in \\mathbb{R}$ 控制源域中一个伪（spurious）、领域特定相关性的强度。源域和目标域的均值满足 $\\mu_S = [m, 0]^{\\top}$ 和 $\\mu_T = [-m, 0]^{\\top}$ (对于给定的 $m \\in \\mathbb{R}$)，并设 $d = [1, 0]^{\\top}$ 和 $u = [0, 1]^{\\top}$，因此有 $d^{\\top} u = 0$。\n\n将在源域上训练 $w$ 的自适应正则化目标定义为最小化预期源域风险，外加一个抑制沿 $d$ 方向敏感度的惩罚项：\n$$\nJ_{\\lambda}(w) = R_S(w) + \\lambda \\,(w^{\\top} d)^2,\n$$\n其中 $R_S(w)$ 是在源域分布和标签规则 $y_S = w_S^{\\top} x + \\varepsilon$ 下的源域预期平方损失，$\\lambda \\ge 0$ 是自适应强度。目标域预期平方损失 $R_T(w)$ 的定义类似，使用目标域分布和标签规则 $y_T = w_T^{\\top} x + \\varepsilon$。\n\n你的任务是：\n- 从高斯分布（具有单位协方差和非零均值）下的预期平方损失的 ERM 定义出发，推导出 $J_{\\lambda}(w)$ 的最小化器 $w_{\\lambda}$ 关于 $\\mu_S$、$d$、$u$、$a$ 和 $\\lambda$ 的显式闭式表达式。\n- 使用推导出的 $w_{\\lambda}$，推导出 $R_S(w_{\\lambda})$ 和 $R_T(w_{\\lambda})$ 关于 $\\mu_S$、$\\mu_T$、$w_S$、$w_T$、$\\sigma^2$ 和 $\\lambda$ 的显式公式。\n- 利用所提供的构造 $w_S = u + a d$ 和 $w_T = u$，以及给定的 $\\mu_S$ 和 $\\mu_T$，设计一个场景，在该场景中增加 $\\lambda$ 会损害域内源域性能 $R_S(w_{\\lambda})$，同时改善目标域性能 $R_T(w_{\\lambda})$。通过改变 $\\lambda$ 来分析 $R_S(w_{\\lambda})$ 和 $R_T(w_{\\lambda})$ 之间的权衡边界。\n\n实现一个完整、可运行的程序，该程序：\n- 使用以下参数值测试套件 $(m, a, \\sigma^2, \\lambda)$：\n    $$\n    (1.0, 1.5, 0.01, 0.0), (1.0, 1.5, 0.01, 0.5), (1.0, 1.5, 0.01, 2.0), (1.0, 1.5, 0.01, 100.0), (1.0, 0.0, 0.01, 10.0).\n    $$\n- 对于每个参数元组，根据推导出的解析公式精确计算 $w_{\\lambda}$，然后评估 $R_S(w_{\\lambda})$ 和 $R_T(w_{\\lambda})$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个双元素列表 $[R_S, R_T]$，其中每个风险值都报告为四舍五入到 $6$ 位小数的浮点数。例如，输出格式应为\n$$\n\\text{[ [r_{S,1}, r_{T,1}], [r_{S,2}, r_{T,2}], \\dots ]}\n$$\n不带任何附加文本。\n\n该测试套件涵盖了 $\\lambda$ 递增的一般情况、$\\lambda$ 非常大的极端自适应情况，以及 $a = 0$ 的边界条件，在此条件下，由于 $w_S = w_T$ 且两者均与 $d$ 正交，自适应预计不会损害域内性能。\n\n此问题不涉及任何物理单位或角度单位。所有答案必须报告为四舍五入到 $6$ 位小数的浮点数。", "solution": "该问题要求在领域自适应的正则化目标下推导最优线性预测器，然后分析其在源域和目标域上的性能。我们将首先推导预期平方损失的一般表达式，然后解决具体的优化问题，最后推导风险公式以评估性能权衡。\n\n首先，我们建立预期平方损失（或风险）$R(w)$ 的一般形式。设数据生成方式为 $x \\sim \\mathcal{N}(\\mu, I)$，其中 $I$ 是单位矩阵，标签遵循 $y = w_*^{\\top} x + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立噪声。预测器 $w$ 的风险为 $R(w) = \\mathbb{E}_{x, \\varepsilon} [(w^{\\top}x - y)^2]$。\n代入 $y$，我们得到：\n$$\nR(w) = \\mathbb{E}_{x, \\varepsilon} [(w^{\\top}x - (w_*^{\\top}x + \\varepsilon))^2] = \\mathbb{E}_{x, \\varepsilon} [((w - w_*)^{\\top}x - \\varepsilon)^2]\n$$\n由于 $\\varepsilon$ 是零均值且与 $x$ 独立，涉及 $\\varepsilon$ 的交叉项在求期望后消失：\n$$\nR(w) = \\mathbb{E}_x [((w - w_*)^{\\top}x)^2] + \\mathbb{E}_{\\varepsilon}[\\varepsilon^2]\n$$\n令 $\\Delta w = w - w_*$。第一项是 $\\mathbb{E}_x [(\\Delta w^{\\top}x)^2]$。由于 $x \\sim \\mathcal{N}(\\mu, I)$，随机变量 $Z = \\Delta w^{\\top}x$ 是高斯分布的，其均值为 $\\mathbb{E}[Z] = \\Delta w^{\\top}\\mathbb{E}[x] = \\Delta w^{\\top}\\mu$，方差为 $\\text{Var}(Z) = \\Delta w^{\\top}\\text{Cov}(x)\\Delta w = \\Delta w^{\\top}I\\Delta w = \\|\\Delta w\\|^2$。\n因此，期望为 $\\mathbb{E}[Z^2] = (\\mathbb{E}[Z])^2 + \\text{Var}(Z) = (\\Delta w^{\\top}\\mu)^2 + \\|\\Delta w\\|^2$。\n第二项是 $\\mathbb{E}_{\\varepsilon}[\\varepsilon^2] = \\sigma^2$。\n结合这些，通用风险公式为：\n$$\nR(w) = ((w - w_*)^{\\top}\\mu)^2 + \\|w - w_*\\|^2 + \\sigma^2\n$$\n\n问题使用 $x \\sim \\mathcal{N}(\\mu_S, I_2)$ 和 $y_S = w_S^{\\top}x + \\varepsilon$ 定义了源域风险 $R_S(w)$。正则化目标函数为：\n$$\nJ_{\\lambda}(w) = R_S(w) + \\lambda (w^{\\top}d)^2\n$$\n为找到最小化器 $w_{\\lambda}$，我们可以将 $J_{\\lambda}(w)$ 表示为 $w$ 的二次函数，并将其梯度设为零。\n源域风险可以写为：\n$$\nR_S(w) = \\mathbb{E}_{x \\sim \\mathcal{N}(\\mu_S, I_2)} [(w^{\\top}x - w_S^{\\top}x)^2] + \\sigma^2 = w^{\\top}\\mathbb{E}[xx^{\\top}]w - 2w^{\\top}\\mathbb{E}[xx^{\\top}]w_S + w_S^{\\top}\\mathbb{E}[xx^{\\top}]w_S + \\sigma^2\n$$\n其中 $\\mathbb{E}[xx^{\\top}] = \\text{Cov}(x) + \\mathbb{E}[x]\\mathbb{E}[x]^{\\top} = I_2 + \\mu_S\\mu_S^{\\top}$。\n我们记 $Q_S = I_2 + \\mu_S\\mu_S^{\\top}$。目标函数变为：\n$$\nJ_{\\lambda}(w) = w^{\\top}Q_S w - 2w^{\\top}Q_S w_S + \\text{const} + \\lambda w^{\\top}(dd^{\\top})w\n$$\n$$\nJ_{\\lambda}(w) = w^{\\top}(Q_S + \\lambda dd^{\\top})w - 2w^{\\top}Q_S w_S + \\text{const}\n$$\n关于 $w$ 的梯度是：\n$$\n\\nabla_w J_{\\lambda}(w) = 2(Q_S + \\lambda dd^{\\top})w - 2Q_S w_S\n$$\n将梯度设为零得到最优的 $w_{\\lambda}$：\n$$\n(Q_S + \\lambda dd^{\\top})w_{\\lambda} = Q_S w_S \\implies w_{\\lambda} = (Q_S + \\lambda dd^{\\top})^{-1} Q_S w_S\n$$\n给定 $\\mu_S = [m, 0]^{\\top}$，$d = [1, 0]^{\\top}$，$u = [0, 1]^{\\top}$，$w_S = u + ad$ 和 $w_T = u$。注意 $\\mu_S = md$。\n让我们将这些代入矩阵中：\n$\\mu_S\\mu_S^{\\top} = (md)(md)^{\\top} = m^2 dd^{\\top}$。\n$Q_S = I_2 + m^2 dd^{\\top}$。\n$w_{\\lambda}$ 的表达式变为：\n$$\nw_{\\lambda} = (I_2 + m^2 dd^{\\top} + \\lambda dd^{\\top})^{-1} (I_2 + m^2 dd^{\\top}) w_S = (I_2 + (m^2+\\lambda)dd^{\\top})^{-1} (I_2 + m^2 dd^{\\top}) w_S\n$$\n使用具体向量 $d=[1,0]^{\\top}$ 和 $u=[0,1]^{\\top}$：\n$dd^{\\top} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$。\n这些矩阵是对角矩阵：\n$I_2 + (m^2+\\lambda)dd^{\\top} = \\begin{pmatrix} 1 + m^2 + \\lambda  0 \\\\ 0  1 \\end{pmatrix}$。\n其逆矩阵为 $\\begin{pmatrix} (1 + m^2 + \\lambda)^{-1}  0 \\\\ 0  1 \\end{pmatrix}$。\n$I_2 + m^2 dd^{\\top} = \\begin{pmatrix} 1+m^2  0 \\\\ 0  1 \\end{pmatrix}$。\n$w_S = u + ad = [a, 1]^{\\top}$。\n乘积 $(I_2 + m^2 dd^{\\top}) w_S$ 为 $\\begin{pmatrix} 1+m^2  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} a \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} a(1+m^2) \\\\ 1 \\end{pmatrix}$。\n最后，我们计算 $w_{\\lambda}$：\n$$\nw_{\\lambda} = \\begin{pmatrix} (1 + m^2 + \\lambda)^{-1}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} a(1+m^2) \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{a(1+m^2)}{1+m^2+\\lambda} \\\\ 1 \\end{pmatrix} = \\frac{a(1+m^2)}{1+m^2+\\lambda} d + u\n$$\n这就是最小化器 $w_{\\lambda}$ 的闭式表达式。\n\n接下来，我们使用通用的风险公式推导源域风险 $R_S(w_{\\lambda})$ 和目标域风险 $R_T(w_{\\lambda})$ 的显式公式。\n对于源域风险，$w=w_{\\lambda}$，$w_*=w_S$，且 $\\mu=\\mu_S=md$：\n差分向量是 $\\Delta w_S = w_{\\lambda} - w_S = \\left(\\frac{a(1+m^2)}{1+m^2+\\lambda} d + u\\right) - (ad + u) = a\\left(\\frac{1+m^2}{1+m^2+\\lambda} - 1\\right)d = \\frac{-a\\lambda}{1+m^2+\\lambda}d$。\n风险的各项是：\n1. $\\|\\Delta w_S\\|^2 = \\left(\\frac{-a\\lambda}{1+m^2+\\lambda}\\right)^2 \\|d\\|^2 = \\frac{a^2\\lambda^2}{(1+m^2+\\lambda)^2}$。\n2. $((\\Delta w_S)^{\\top}\\mu_S)^2 = \\left(\\left(\\frac{-a\\lambda}{1+m^2+\\lambda}d\\right)^{\\top}(md)\\right)^2 = \\left(\\frac{-a\\lambda m}{1+m^2+\\lambda}\\right)^2 = \\frac{a^2\\lambda^2m^2}{(1+m^2+\\lambda)^2}$。\n源域风险是这两项之和再加上 $\\sigma^2$：\n$$\nR_S(w_{\\lambda}) = \\frac{a^2\\lambda^2}{(1+m^2+\\lambda)^2} + \\frac{a^2\\lambda^2m^2}{(1+m^2+\\lambda)^2} + \\sigma^2 = \\frac{a^2\\lambda^2(1+m^2)}{(1+m^2+\\lambda)^2} + \\sigma^2\n$$\n对于目标域风险，$w=w_{\\lambda}$，$w_*=w_T=u$，且 $\\mu=\\mu_T=-md$：\n差分向量是 $\\Delta w_T = w_{\\lambda} - w_T = \\left(\\frac{a(1+m^2)}{1+m^2+\\lambda} d + u\\right) - u = \\frac{a(1+m^2)}{1+m^2+\\lambda}d$。\n风险的各项是：\n1. $\\|\\Delta w_T\\|^2 = \\left(\\frac{a(1+m^2)}{1+m^2+\\lambda}\\right)^2 \\|d\\|^2 = \\frac{a^2(1+m^2)^2}{(1+m^2+\\lambda)^2}$。\n2. $((\\Delta w_T)^{\\top}\\mu_T)^2 = \\left(\\left(\\frac{a(1+m^2)}{1+m^2+\\lambda}d\\right)^{\\top}(-md)\\right)^2 = \\left(\\frac{-a m(1+m^2)}{1+m^2+\\lambda}\\right)^2 = \\frac{a^2m^2(1+m^2)^2}{(1+m^2+\\lambda)^2}$。\n目标域风险是这两项之和再加上 $\\sigma^2$：\n$$\nR_T(w_{\\lambda}) = \\frac{a^2(1+m^2)^2}{(1+m^2+\\lambda)^2} + \\frac{a^2m^2(1+m^2)^2}{(1+m^2+\\lambda)^2} + \\sigma^2 = \\frac{a^2(1+m^2)^2(1+m^2)}{(1+m^2+\\lambda)^2} + \\sigma^2 = \\frac{a^2(1+m^2)^3}{(1+m^2+\\lambda)^2} + \\sigma^2\n$$\n这些公式显示了权衡关系。对于 $\\lambda  0$，$R_S(w_{\\lambda})$ 是 $\\lambda$ 的增函数，从其最小值 $R_S(w_0) = \\sigma^2$ 开始。这表明正则化损害了域内性能。相反，$R_T(w_{\\lambda})$ 是 $\\lambda$ 的减函数，从其最大值 $R_T(w_0) = a^2(1+m^2)+\\sigma^2$ 开始，并在 $\\lambda \\to \\infty$ 时逼近可能的最小风险 $R_T(w_T)=\\sigma^2$。这表明正则化改善了域外（目标域）性能。当 $a=0$ 时，$w_S=w_T=u$，伪相关性不存在。对于所有 $\\lambda \\ge 0$，两种风险都变为 $R_S(w_\\lambda) = R_T(w_\\lambda) = \\sigma^2$，不存在权衡。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the domain adaptation problem by applying the derived analytical formulas\n    for source and target risks.\n    \"\"\"\n    \n    # Test suite of parameter values (m, a, sigma^2, lambda)\n    test_cases = [\n        # (m, a, sigma_sq, lam)\n        (1.0, 1.5, 0.01, 0.0),\n        (1.0, 1.5, 0.01, 0.5),\n        (1.0, 1.5, 0.01, 2.0),\n        (1.0, 1.5, 0.01, 100.0),\n        (1.0, 0.0, 0.01, 10.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        m, a, sigma_sq, lam = case\n        \n        # Derived analytical formulas for the risks.\n        # Let C1 = 1 + m^2\n        # R_S(w_lambda) = (a^2 * lambda^2 * C1) / (C1 + lambda)^2 + sigma^2\n        # R_T(w_lambda) = (a^2 * C1^3) / (C1 + lambda)^2 + sigma^2\n        \n        c1 = 1.0 + m**2\n        \n        # Numerators for the risk components\n        rs_num = (a**2) * (lam**2) * c1\n        rt_num = (a**2) * (c1**3)\n        \n        # Denominator for both risks\n        denominator = (c1 + lam)**2\n        \n        # Calculate source and target risks\n        if denominator == 0:\n            # This case should not happen for lambda = 0\n            # but as a safeguard\n            rs = float('inf')\n            rt = float('inf')\n        else:\n            rs = (rs_num / denominator) + sigma_sq\n            rt = (rt_num / denominator) + sigma_sq\n            \n        results.append([rs, rt])\n\n    # Format the output string as per the problem specification.\n    # Each risk value is rounded to 6 decimal places.\n    # The final output is of the form [[r_S1, r_T1], [r_S2, r_T2], ...]\n    output_parts = []\n    for rs, rt in results:\n        # Using format string for precise decimal representation\n        output_parts.append(f\"[{rs:.6f}, {rt:.6f}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    \n    # The final print statement must produce ONLY the single-line format specified.\n    print(final_output)\n\nsolve()\n```", "id": "3117567"}, {"introduction": "除了简单地惩罚特定领域的特征，一种更直接的方法是显式地对齐源域和目标域的特征分布。这个练习 [@problem_id:3117509] 将指导你实现并比较两种强大的对齐技术：最优传输（Optimal Transport, OT）和最大均值差异（Maximum Mean Discrepancy, MMD）。你将获得关于Sinkhorn-Knopp算法的实践经验，并看到特征空间变换如何能显著提高模型将知识迁移到新领域的能力。", "problem": "您将处理一个二元分类域适应任务，其框架是使用最优传输 (Optimal Transport, OT) 和最大均值差异 (Maximum Mean Discrepancy, MMD) 在源域和目标域之间进行特征对齐。两个域都由有限样本表示，其中源域样本带标签，目标域样本仅在评估时使用标签。特征提取器是恒等映射，因此特征与输入向量相同。目标是实现具有均匀边缘分布的熵正则化最优传输，从得到的传输方案构建重心对齐，在对齐后的源特征上微调一个分类器，并将其在目标域上的准确率与使用基于线性核的 MMD 对齐方法微调的分类器进行比较。最终输出应汇总多个测试用例中 OT 相对于 MMD 的准确率提升。\n\n使用的基本原理和定义：\n- 该任务设定在域适应的背景下。源域提供带标签的样本，而目标域提供用于对齐的无标签样本和用于评估的带标签样本。\n- 恒等特征提取器表示为 $f_{\\theta}(x) = x$。在此任务中，参数向量 $\\theta$ 不会被显式优化。\n- 最优传输对齐解决以下熵正则化运输问题：给定源特征 $\\{z_{i}^{S}\\}_{i=1}^{n_{S}}$、目标特征 $\\{z_{j}^{T}\\}_{j=1}^{n_{T}}$、均匀边缘分布 $a \\in \\mathbb{R}^{n_{S}}$ 和 $b \\in \\mathbb{R}^{n_{T}}$ (其中 $a_{i} = \\frac{1}{n_{S}}$ 且 $b_{j} = \\frac{1}{n_{T}}$)，以及由 $C_{ij} = \\lVert z_{i}^{S} - z_{j}^{T} \\rVert_{2}^{2}$ 定义的成本矩阵 $C \\in \\mathbb{R}^{n_{S} \\times n_{T}}$，找到一个传输方案 $\\pi \\in \\mathbb{R}^{n_{S} \\times n_{T}}$，在满足边缘约束的条件下最小化正则化目标函数：\n$$\n\\min_{\\pi} \\sum_{i=1}^{n_{S}} \\sum_{j=1}^{n_{T}} \\pi_{ij} \\lVert z_{i}^{S} - z_{j}^{T} \\rVert_{2}^{2} + \\varepsilon \\sum_{i=1}^{n_{S}} \\sum_{j=1}^{n_{T}} \\pi_{ij} \\log \\pi_{ij}\n$$\n约束条件为 $\\sum_{j=1}^{n_{T}} \\pi_{ij} = a_{i}$ 和 $\\sum_{i=1}^{n_{S}} \\pi_{ij} = b_{j}$。熵正则化参数为 $\\varepsilon  0$。\n- 使用 Sinkhorn-Knopp 迭代缩放方法（此处未提供明确公式；您必须推导并实现该过程）来解决熵正则化问题，并获得满足边缘约束的传输方案 $\\pi$。\n- 使用传输方案构建源特征相对于目标特征的重心对齐。具体来说，通过构建由传输方案和源边缘分布加权的目标特征的重心组合，生成对齐后的源特征 $\\{z_{i}^{S,\\mathrm{OT}}\\}_{i=1}^{n_{S}}$。\n- 使用源标签，在对齐后的源特征上微调一个二元分类器。该分类器必须实现为逻辑回归，并使用梯度下降法在交叉熵损失上进行训练。\n- 最大均值差异 (MMD) 是一种基于核均值嵌入的分布间差异度量。使用线性核 $k(x,y) = x^{\\top}y$。在此核下，推导并实现源特征上的对齐变换，以最小化对齐后的源特征与目标特征之间的经验 MMD。使用源标签，在经过 MMD 对齐的源特征上微调同一个逻辑回归分类器。\n- 使用真实的目标标签，在原始目标特征上评估这两个分类器。计算 OT 对齐分类器和 MMD 对齐分类器的准确率。对于每个测试用例，报告准确率提升，其计算方式为标量 $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$，其中 $A_{\\mathrm{OT}}$ 和 $A_{\\mathrm{MMD}}$ 分别是两种方法在目标域上实现的准确率。\n\n数据生成协议（所有角度均以弧度为单位）：\n- 为两个类别生成样本量相等的源数据（二元标签）。令 $d = 2$。类别条件下的源均值为 $\\mu_{0}^{S} = [-2, 0]$ 和 $\\mu_{1}^{S} = [2, 0]$。对于源样本，从均值为零、协方差为 $\\sigma_{S}^{2} I_{2}$ 的独立高斯噪声中抽取样本，其中 $I_{2}$ 是 $2 \\times 2$ 的单位矩阵。源标签是已知的。\n- 通过对源类别均值应用旋转和平移来生成目标数据，从而产生目标类别均值。设旋转角度为 $\\alpha$，平移向量为 $t = [t_{x}, t_{y}]$。目标类别均值为 $\\mu_{c}^{T} = R(\\alpha) \\mu_{c}^{S} + t$，其中 $c \\in \\{0,1\\}$，$R(\\alpha)$ 是角度为 $\\alpha$ 弧度的 $2 \\times 2$ 旋转矩阵。\n- 围绕每个目标类别均值，从均值为零、协方差为 $\\sigma_{T}^{2} I_{2}$ 的独立高斯噪声中抽取样本。目标标签仅用于评估。\n\n实现要求：\n- 使用 $f_{\\theta}(x) = x$。\n- 使用均匀边缘分布 $a_{i} = \\frac{1}{n_{S}}$ 和 $b_{j} = \\frac{1}{n_{T}}$。\n- 使用 Sinkhorn-Knopp 方法解决给定 $\\varepsilon$ 的熵正则化 OT 问题。\n- 使用传输方案和目标特征构建源特征的重心对齐。\n- 使用测试套件中指定的固定迭代次数和学习率，通过梯度下降法训练逻辑回归。使用交叉熵损失，并通过将逻辑函数阈值设为 $0.5$ 来进行预测。\n- 对于基于线性核的 MMD 对齐，推导能够最小化对齐后源特征与目标特征之间经验 MMD 的对齐方式。实现此对齐，然后在对齐后的源特征上微调逻辑回归。\n- 在目标域上评估准确率，并为每个案例计算准确率提升 $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$。\n\n测试套件：\n- 所有随机抽样必须使用指定的种子以确保可复现性。每个测试用例使用独立的种子：种子 $= 10 + \\text{case\\_index}$，其中四个用例的 case index 分别为 $0,1,2,3$。\n- 提供四个测试用例，参数为 $(n_{S}, n_{T}, \\sigma_{S}, \\sigma_{T}, \\alpha, t_{x}, t_{y}, \\varepsilon, \\text{epochs}, \\text{learning\\_rate})$：\n    1. 案例 $0$ (理想情况): $(n_{S}, n_{T}) = (60, 60)$, $\\sigma_{S} = 0.4$, $\\sigma_{T} = 0.4$, $\\alpha = 0.5$, $t_{x} = 1.0$, $t_{y} = 0.5$, $\\varepsilon = 0.2$, epochs $= 200$, learning rate $= 0.1$。\n    2. 案例 $1$ (无偏移边界条件): $(n_{S}, n_{T}) = (60, 60)$, $\\sigma_{S} = 0.4$, $\\sigma_{T} = 0.4$, $\\alpha = 0.0$, $t_{x} = 0.0$, $t_{y} = 0.0$, $\\varepsilon = 0.2$, epochs $= 200$, learning rate $= 0.1$。\n    3. 案例 $2$ (剧烈偏移): $(n_{S}, n_{T}) = (80, 80)$, $\\sigma_{S} = 0.4$, $\\sigma_{T} = 0.6$, $\\alpha = 1.0$, $t_{x} = 2.0$, $t_{y} = -1.0$, $\\varepsilon = 0.3$, epochs $= 250$, learning rate $= 0.08$。\n    4. 案例 $3$ (小样本量边缘情况): $(n_{S}, n_{T}) = (12, 12)$, $\\sigma_{S} = 0.5$, $\\sigma_{T} = 0.5$, $\\alpha = 0.8$, $t_{x} = 1.5$, $t_{y} = 1.5$, $\\varepsilon = 0.25$, epochs $= 300$, learning rate $= 0.12$。\n\n答案类型和最终输出格式：\n- 对于每个测试用例，计算浮点数 $A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$，其中 $A_{\\mathrm{OT}}$ 和 $A_{\\mathrm{MMD}}$ 是在 $[0,1]$ 区间内以小数表示的目标域准确率。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个浮点数按测试用例的顺序格式化为四位小数（例如，$[0.1234,0.0000,-0.0567,0.0321]$）。", "solution": "该问题要求对两种域适应技术进行比较分析，一种基于最优传输 (OT)，另一种基于最大均值差异 (MMD)，用于一个二元分类任务。目标是在一个特征空间中，将带标签的源域分布与无标签的目标域分布对齐。在本例中，由于特征提取器是恒等映射 $f_{\\theta}(x) = x$，该特征空间就是输入空间本身。我们将实现这两种对齐方法，在对齐后的源数据上训练一个逻辑回归分类器，在目标域上评估其性能，并计算基于 OT 的方法相对于基于 MMD 的方法的准确率提升。\n\n首先，我们定义数据生成过程。源数据由 $d=2$ 维空间中两个均衡类别的 $n_S$ 个样本组成。类别条件分布是高斯分布。均值为 $\\mu_{0}^{S} = [-2, 0]^{\\top}$ 和 $\\mu_{1}^{S} = [2, 0]^{\\top}$。样本生成方式为 $z_i^S \\sim \\mathcal{N}(\\mu_{y_i^S}^S, \\sigma_S^2 I_2)$，其中 $y_i^S \\in \\{0, 1\\}$ 是源标签，$I_2$ 是 $2 \\times 2$ 的单位矩阵。目标域数据由 $n_T$ 个样本组成，这些样本是通过对源类别均值应用旋转和平移生成的。目标类别均值为 $\\mu_{c}^{T} = R(\\alpha) \\mu_{c}^{S} + t$，其中 $c \\in \\{0,1\\}$，$R(\\alpha)$ 是角度为 $\\alpha$ 的旋转矩阵，$t = [t_x, t_y]^\\top$ 是一个平移向量。目标样本生成方式为 $z_j^T \\sim \\mathcal{N}(\\mu_{y_j^T}^T, \\sigma_T^2 I_2)$，其中 $y_j^T$ 是目标标签，仅用于评估。\n\n第一种对齐方法的核心是熵正则化最优传输。我们的目标是找到一个传输方案 $\\pi \\in \\mathbb{R}^{n_S \\times n_T}$，以最小化正则化的传输成本。源特征 $z_i^S$ 和目标特征 $z_j^T$ 之间的成本是平方欧氏距离，$C_{ij} = \\lVert z_i^S - z_j^T \\rVert_2^2$。优化问题为：\n$$\n\\min_{\\pi} \\sum_{i=1}^{n_{S}} \\sum_{j=1}^{n_{T}} \\pi_{ij} C_{ij} - \\varepsilon H(\\pi)\n$$\n其中 $H(\\pi) = -\\sum_{ij} \\pi_{ij} \\log \\pi_{ij}$ 是方案的熵，$\\varepsilon  0$ 是正则化强度。最小化受边缘约束 $\\pi \\mathbf{1}_{n_T} = a$ 和 $\\pi^\\top \\mathbf{1}_{n_S} = b$ 的限制，其中 $\\mathbf{1}_k$ 是一个大小为 $k$ 的全一向量。边缘分布是均匀的，即 $a_i = 1/n_S$ 和 $b_j = 1/n_T$。\n\n这个问题可以使用 Sinkhorn-Knopp 算法高效解决。解 $\\pi$可以表示为 $\\pi = \\text{diag}(u) K \\text{diag}(v)$，其中 $K_{ij} = e^{-C_{ij}/\\varepsilon}$，$u, v$ 是缩放向量。该算法迭代更新 $u$ 和 $v$ 以满足边缘约束：\n$$\nu \\leftarrow a \\oslash (K v) \\quad \\text{and} \\quad v \\leftarrow b \\oslash (K^\\top u)\n$$\n其中 $\\oslash$ 表示逐元素除法。从对 $v$ 的一个初始猜测（例如，$v = \\mathbf{1}_{n_T}$）开始，重复这些更新直到收敛。\n\n一旦找到最优传输方案 $\\pi$，我们对源特征进行重心对齐。对齐后的源特征 $z_i^{S,\\mathrm{OT}}$ 是 $z_i^S$ 映射到的目标特征的期望，由条件传输概率加权。$z_i^S$ 映射到 $z_j^T$ 的条件概率是 $p(j|i) = \\pi_{ij} / a_i = n_S \\pi_{ij}$。因此，对齐后的特征是：\n$$\nz_i^{S,\\mathrm{OT}} = \\sum_{j=1}^{n_T} p(j|i) z_j^T = n_S \\sum_{j=1}^{n_T} \\pi_{ij} z_j^T\n$$\n用矩阵表示法，即为 $Z_{S,\\mathrm{OT}} = (n_S \\pi) Z_T$，其中 $Z_T$ 是目标特征矩阵。\n\n第二种对齐方法基于最小化使用线性核 $k(x,y) = x^\\top y$ 的最大均值差异 (MMD)。一组变换后的源特征 $\\{\\mathcal{T}(z_i^S)\\}_{i=1}^{n_S}$ 和目标特征 $\\{z_j^T\\}_{j=1}^{n_T}$ 之间的经验 MMD 平方是：\n$$\n\\text{MMD}^2 = \\left\\lVert \\frac{1}{n_S} \\sum_{i=1}^{n_S} \\phi(\\mathcal{T}(z_i^S)) - \\frac{1}{n_T} \\sum_{j=1}^{n_T} \\phi(z_j^T) \\right\\rVert_{\\mathcal{H}}^2\n$$\n对于线性核，特征映射 $\\phi$ 是恒等映射，即 $\\phi(x) = x$。我们选择一个简单的对齐变换类别：全局平移，$\\mathcal{T}(z) = z + v$。目标是找到最小化以下表达式的向量 $v$：\n$$\n\\min_v \\left\\lVert \\frac{1}{n_S} \\sum_{i=1}^{n_S} (z_i^S + v) - \\frac{1}{n_T} \\sum_{j=1}^{n_T} z_j^T \\right\\rVert_2^2 = \\min_v \\left\\lVert (\\bar{z}^S + v) - \\bar{z}^T \\right\\rVert_2^2\n$$\n其中 $\\bar{z}^S$ 和 $\\bar{z}^T$ 分别是源特征和目标特征的经验均值。当 $v = \\bar{z}^T - \\bar{z}^S$ 时，该表达式最小化。因此，基于 MMD 的对齐是一种均值匹配变换：\n$$\nz_i^{S,\\mathrm{MMD}} = z_i^S + (\\bar{z}^T - \\bar{z}^S)\n$$\n这将源分布的质心与目标分布的质心对齐。\n\n在使用 OT ($Z_{S,\\mathrm{OT}}$) 和 MMD ($Z_{S,\\mathrm{MMD}}$) 对源特征进行对齐后，我们训练两个独立的二元逻辑回归分类器。模型预测类别 1 的概率为 $\\hat{y} = \\sigma(z^\\top w + b)$，其中 $\\sigma(s) = 1/(1+e^{-s})$ 是 sigmoid 函数，$w \\in \\mathbb{R}^2$ 是权重，$b \\in \\mathbb{R}$ 是偏置。分类器通过使用梯度下降法在固定的迭代次数和给定的学习率下最小化二元交叉熵损失函数进行训练。对于大小为 $N$、特征为 $X$、标签为 $Y$ 的数据集，损失 $L$ 的梯度为：\n$$\n\\nabla_w L = \\frac{1}{N} X^\\top (\\hat{Y} - Y), \\quad \\nabla_b L = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n$$\n权重和偏置按 $w \\leftarrow w - \\eta \\nabla_w L$ 和 $b \\leftarrow b - \\eta \\nabla_b L$ 更新，其中 $\\eta$ 是学习率。\n\n最后，我们评估这两个分类器的性能。在 $Z_{S,\\mathrm{OT}}$ 上训练的分类器和在 $Z_{S,\\mathrm{MMD}}$ 上训练的分类器都在原始、未对齐的目标特征 $Z_T$ 及其真实标签 $Y_T$ 上进行测试。如果输出概率大于 $0.5$，则预测分类为 $1$，否则为 $0$。准确率是正确分类的目标样本的比例。每个测试用例的最终结果是准确率之差：$A_{\\mathrm{OT}} - A_{\\mathrm{MMD}}$。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_data(n_s, n_t, sigma_s, sigma_t, alpha, t_x, t_y, seed):\n    \"\"\"Generates source and target domain data.\"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Source domain data\n    mu_s0 = np.array([-2.0, 0.0])\n    mu_s1 = np.array([2.0, 0.0])\n    \n    n_s_half = n_s // 2\n    z_s0 = rng.multivariate_normal(mu_s0, sigma_s**2 * np.eye(2), size=n_s_half)\n    z_s1 = rng.multivariate_normal(mu_s1, sigma_s**2 * np.eye(2), size=n_s - n_s_half)\n    \n    Z_s = np.vstack((z_s0, z_s1))\n    Y_s = np.hstack((np.zeros(n_s_half), np.ones(n_s - n_s_half)))\n\n    # Target domain data\n    R = np.array([[np.cos(alpha), -np.sin(alpha)], [np.sin(alpha), np.cos(alpha)]])\n    t = np.array([t_x, t_y])\n    \n    mu_t0 = R @ mu_s0 + t\n    mu_t1 = R @ mu_s1 + t\n    \n    n_t_half = n_t // 2\n    z_t0 = rng.multivariate_normal(mu_t0, sigma_t**2 * np.eye(2), size=n_t_half)\n    z_t1 = rng.multivariate_normal(mu_t1, sigma_t**2 * np.eye(2), size=n_t - n_t_half)\n    \n    Z_t = np.vstack((z_t0, z_t1))\n    Y_t = np.hstack((np.zeros(n_t_half), np.ones(n_t - n_t_half)))\n    \n    return Z_s, Y_s, Z_t, Y_t\n\ndef sinkhorn_knopp(C, epsilon, n_s, n_t, num_iter=100):\n    \"\"\"Solves entropically regularized OT using Sinkhorn-Knopp.\"\"\"\n    a = np.full(n_s, 1.0 / n_s)\n    b = np.full(n_t, 1.0 / n_t)\n    \n    K = np.exp(-C / epsilon)\n    v = np.ones(n_t)\n    \n    for _ in range(num_iter):\n        u = a / (K @ v)\n        v = b / (K.T @ u)\n        \n    pi = np.diag(u) @ K @ np.diag(v)\n    return pi\n\ndef ot_align(Z_s, Z_t, epsilon):\n    \"\"\"Aligns source features to target features using Optimal Transport.\"\"\"\n    n_s = Z_s.shape[0]\n    n_t = Z_t.shape[0]\n    \n    C = cdist(Z_s, Z_t, 'sqeuclidean')\n    pi = sinkhorn_knopp(C, epsilon, n_s, n_t)\n    \n    Z_s_ot = (n_s * pi) @ Z_t\n    return Z_s_ot\n\ndef mmd_align(Z_s, Z_t):\n    \"\"\"Aligns source features to target features by matching means (linear MMD).\"\"\"\n    mean_s = np.mean(Z_s, axis=0)\n    mean_t = np.mean(Z_t, axis=0)\n    \n    translation = mean_t - mean_s\n    Z_s_mmd = Z_s + translation\n    return Z_s_mmd\n\ndef train_logistic_regression(X, y, epochs, lr):\n    \"\"\"Trains a logistic regression classifier using gradient descent.\"\"\"\n    n_samples, n_features = X.shape\n    w = np.zeros(n_features)\n    b = 0.0\n    \n    y = y.reshape(-1, 1)\n\n    for _ in range(epochs):\n        linear_model = (X @ w + b).reshape(-1, 1)\n        y_pred = 1 / (1 + np.exp(-linear_model))\n        \n        dw = (1 / n_samples) * (X.T @ (y_pred - y))\n        db = (1 / n_samples) * np.sum(y_pred - y)\n        \n        w -= lr * dw.flatten()\n        b -= lr * db\n\n    return w, b\n\ndef predict_and_evaluate(X, y, w, b):\n    \"\"\"Makes predictions and computes accuracy.\"\"\"\n    linear_model = (X @ w + b).reshape(-1, 1)\n    y_pred_prob = 1 / (1 + np.exp(-linear_model))\n    y_pred_class = (y_pred_prob  0.5).astype(int)\n    \n    accuracy = np.mean(y_pred_class.flatten() == y.flatten())\n    return accuracy\n\ndef solve():\n    test_cases = [\n        (60, 60, 0.4, 0.4, 0.5, 1.0, 0.5, 0.2, 200, 0.1),\n        (60, 60, 0.4, 0.4, 0.0, 0.0, 0.0, 0.2, 200, 0.1),\n        (80, 80, 0.4, 0.6, 1.0, 2.0, -1.0, 0.3, 250, 0.08),\n        (12, 12, 0.5, 0.5, 0.8, 1.5, 1.5, 0.25, 300, 0.12),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        n_s, n_t, sigma_s, sigma_t, alpha, t_x, t_y, epsilon, epochs, lr = params\n        seed = 10 + i\n        \n        Z_s, Y_s, Z_t, Y_t = generate_data(n_s, n_t, sigma_s, sigma_t, alpha, t_x, t_y, seed)\n        \n        # OT path\n        Z_s_ot = ot_align(Z_s, Z_t, epsilon)\n        w_ot, b_ot = train_logistic_regression(Z_s_ot, Y_s, epochs, lr)\n        acc_ot = predict_and_evaluate(Z_t, Y_t, w_ot, b_ot)\n        \n        # MMD path\n        Z_s_mmd = mmd_align(Z_s, Z_t)\n        w_mmd, b_mmd = train_logistic_regression(Z_s_mmd, Y_s, epochs, lr)\n        acc_mmd = predict_and_evaluate(Z_t, Y_t, w_mmd, b_mmd)\n        \n        accuracy_improvement = acc_ot - acc_mmd\n        results.append(accuracy_improvement)\n\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```", "id": "3117509"}]}