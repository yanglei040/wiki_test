## 引言
在数据驱动的时代，我们面临着一个核心矛盾：一方面，大规模数据是驱动机器学习模型性能提升的关键燃料；另一方面，对个人隐私和数据主权的日益关切，为传统的数据集中式处理方法带来了巨大挑战。联邦学习 (Federated Learning, FL) 作为一种新兴的[分布](@entry_id:182848)式[机器学习范式](@entry_id:637731)应运而生，它旨在打破数据孤岛，允许各方在不共享其本地私有数据的情况下，共同训练一个强大的[机器学习模型](@entry_id:262335)。这种“数据不动，模型动”的理念，为在金融、医疗、物联网等隐私敏感领域进行协作学习提供了革命性的解决方案。

然而，将联邦学习从一个优雅的概念转变为一个实用、高效且安全的系统，需要克服一系列独特的理论与工程挑战。本文旨在系统性地剖析联邦学习，解决“如何在现实世界的复杂约束下成功部署联邦学习”这一关键问题。

为实现这一目标，本文将引导您完成三个层次的探索。首先，在“原理与机制”一章中，我们将深入联邦学习的内核，从基础的[联邦平均](@entry_id:634153)算法出发，剖析其在面对数据异质性、隐私风险、通信瓶颈和安全威胁时所面临的挑战，并介绍相应的先进解决机制。接着，在“应用与跨学科连接”一章中，我们将理论与实践相结合，展示联邦学习如何在医疗保健、边缘计算等多个领域解决实际问题，并揭示其与[算法公平性](@entry_id:143652)、强化学习等前沿方向的深刻联系。最后，通过“动手实践”部分，您将有机会将所学知识付诸实践，通过具体的编程练习来解决[资源优化](@entry_id:172440)、系统效率和模型个性化等核心问题，从而获得对联邦学习的直观理解和操作经验。

## 原理与机制

本章在前一章介绍性概述的基础上，深入探讨驱动联邦学习 (FL) 的核心原理与关键机制。我们将从[联邦平均](@entry_id:634153) (Federated Averaging, [FedAvg](@entry_id:634153)) 算法的基本构造入手，剖析其在理想与现实场景下的行为。随后，我们将系统性地审视联邦学习面临的核心挑战，如统计异质性、隐私泄露、通信瓶颈和系统稳健性等。针对每一项挑战，我们将介绍并分析学术界与工业界提出的前沿解决方案，包括 FedProx、[安全聚合](@entry_id:754615)、[差分隐私](@entry_id:261539)、模型量化以及[拜占庭容错](@entry_id:747029)聚合规则。最后，本章将把联邦学习置于更广阔的[分布](@entry_id:182848)式学习背景下，对比其在不同部署场景（跨孤岛与跨设备）下的特性，并与其他[分布](@entry_id:182848)式[范式](@entry_id:161181)（如分裂学习）进行比较。

### 核心机制：[联邦平均](@entry_id:634153) ([FedAvg](@entry_id:634153))

联邦学习的核心思想是在不直接访问客户端原始数据的情况下，协作训练一个共享的全局模型。迄今为止，**[联邦平均](@entry_id:634153) (Federated Averaging, [FedAvg](@entry_id:634153))** 算法是实现这一目标的最具[代表性](@entry_id:204613)也最为基础的方法。一个典型的 [FedAvg](@entry_id:634153) 训练轮次包含以下三个步骤：

1.  **分发 (Broadcast):** 中央服务器将当前的全局模型参数 $\mathbf{w}_t$ 分发给一个被选中的客户端[子集](@entry_id:261956) $\mathcal{S}_t$。

2.  **本地训练 (Local Training):** 每个参与的客户端 $i \in \mathcal{S}_t$ 以接收到的模型 $\mathbf{w}_t$ 为起点，使用其本地私有数据集 $\mathcal{D}_i$ 执行多步（例如 $E$ 步）[随机梯度下降](@entry_id:139134) (SGD) 或其变体，从而得到一个更新后的本地模型 $\mathbf{w}^{(i)}_{t+1}$。

3.  **聚合 (Aggregation):** 服务器收集来自所有参与客户端的更新模型 $\mathbf{w}^{(i)}_{t+1}$。随后，服务器通过加权平均的方式将这些本地模型聚合成一个新的全局模型 $\mathbf{w}_{t+1}$，以供下一轮训练使用。

聚合步骤的数学表达式为：
$$
\mathbf{w}_{t+1} = \sum_{i \in \mathcal{S}_t} \alpha_i \mathbf{w}^{(i)}_{t+1}
$$
其中，$\alpha_i$ 是分配给客户端 $i$ 的非负聚合权重，且满足 $\sum_{i \in \mathcal{S}_t} \alpha_i = 1$。一个最自然且最常用的权重选择是按客户端数据量大小进行加权，即**数据比例加权 (data-proportional weighting)**。如果客户端 $i$ 的本地数据集大小为 $n_i$，则其权重为 $\alpha_i = \frac{n_i}{\sum_{j \in \mathcal{S}_t} n_j}$。这种加权方式旨在使聚合结果更好地逼近在所有参与客户端的数据上定义的全局[损失函数](@entry_id:634569)。

为了深刻理解 [FedAvg](@entry_id:634153) 的本质，我们可以考察一个特殊情况：当每个客户端仅执行一步本地更新 ($E=1$)，并且所有客户端都参与训练时。在这种设定下，本地更新可以近似写为 $\mathbf{w}^{(i)}_{t+1} \approx \mathbf{w}_t - \eta \mathbf{g}^{(i)}_t$，其中 $\mathbf{g}^{(i)}_t$ 是在 $\mathbf{w}_t$ 上计算的关于客户端 $i$ 本地损失的随机梯度。此时，聚合后的全局模型更新为：
$$
\mathbf{w}_{t+1} \approx \sum_{i=1}^{N} p_i (\mathbf{w}_t - \eta \mathbf{g}^{(i)}_t) = \mathbf{w}_t - \eta \sum_{i=1}^{N} p_i \mathbf{g}^{(i)}_t
$$
其中 $p_i$ 是客户端 $i$ 在整个群体中的数据比例。我们发现，聚合梯度 $\sum p_i \mathbf{g}^{(i)}_t$ 是全局损失函数真实梯度的一个无偏估计。因此，在这种 $E=1$ 的理想条件下，[FedAvg](@entry_id:634153) 在算法上等价于一种大规模的、[数据并行](@entry_id:172541)的同步[随机梯度下降](@entry_id:139134) (Data-Parallel SGD)。[@problem_id:3124695]

然而，联邦学习的真正威力与复杂性体现在 $E \gt 1$ 的情况。允许多步本地更新可以显著减少所需的通信轮次，这是联邦学习相比于传统[分布](@entry_id:182848)式训练的一大优势。但是，当客户端数据[分布](@entry_id:182848)并非**[独立同分布](@entry_id:169067) (Independent and Identically Distributed, IID)** 时，多步本地更新会引入一个关键的挑战：[客户端漂移](@entry_id:634167)。

### 异质性的挑战：[客户端漂移](@entry_id:634167)

在现实世界的联邦学习应用中，不同客户端的本地数据[分布](@entry_id:182848)通常存在显著差异，即数据是**非独立同分布 (non-IID)** 的。这意味着每个客户端的本地[损失函数](@entry_id:634569) $F_i(\mathbf{w})$ 的最小值点（最优模型）各不相同，且都偏离全局损失函数 $F(\mathbf{w}) = \sum p_i F_i(\mathbf{w})$ 的全局最优解。

当每个客户端在自己的本地损失函数上执行多步 ($E \gt 1$) 更新时，其本地模型会朝着各自的局部最优点移动。这个过程被称为**[客户端漂移](@entry_id:634167) (client drift)**。当服务器对这些已经“漂移”的本地模型进行平均时，得到的全局模型可能偏离了真正的全局最优路径。

我们可以通过一个具体的例子来观察这种现象。考虑一个简单的二次损失函数场景，其中每个客户端 $i$ 的本地目标是 $f_i(w) = \frac{1}{2} (w - a_i)^\top H_i (w - a_i)$，这里的 $a_i$ 代表客户端 $i$ 的局部最优点。在 non-IID 设定下，各客户端的 $a_i$ 和 $H_i$ 是不同的。如果从一个共同的起点 $w$ 开始，经过 $E$ 步本地[梯度下降](@entry_id:145942)后，客户端 $i$ 的模型 $w_i$ 将会移向其局部最优点 $a_i$。服务器聚合的是在这些漂移后的点 $w_i$ 上计算的梯度（或者等效于聚合模型 $w_i$ 本身），即 $g_{\mathrm{fed}} = \sum_i p_i \nabla f_i(w_i)$。这个聚合梯度与在初始点 $w$ 处的真实全局梯度 $\nabla F(w) = \sum_i p_i \nabla f_i(w)$ 之间存在一个偏差 $b = g_{\mathrm{fed}} - \nabla F(w)$。只有在 $E=0$（无本地更新）或[学习率](@entry_id:140210) $\eta=0$（无移动）的平凡情况下，这个偏差才为零。在所有其他 non-IID 情况下，该偏差通常非零，表明[客户端漂移](@entry_id:634167)导致聚合更新偏离了真实梯度方向。[@problem_id:3124661]

我们可以通过[泰勒展开](@entry_id:145057)来更深入地分析这个偏差。对于在本地更新后的模型 $w_i = w - \eta \nabla f_i(w)$ （考虑 $E=1$ 的单步情况进行分析），在 $w_i$ 处评估的梯度与在 $w$ 处评估的梯度之间的差异可以一阶近似为：
$$
\nabla f_i(w_i) - \nabla f_i(w) \approx \nabla^2 f_i(w) (w_i - w) = -\eta \nabla^2 f_i(w) \nabla f_i(w)
$$
其中 $\nabla^2 f_i(w)$ 是客户端 $i$ 本地损失函数的[海森矩阵](@entry_id:139140) (Hessian)。因此，聚合梯度与真实全局梯度之间的偏差（即漂移）的领先项为：
$$
R(w, \eta) = \sum_{i=1}^{m} p_i (\nabla f_i(w_i) - \nabla f_i(w)) \approx -\eta \sum_{i=1}^{m} p_i \nabla^2 f_i(w) \nabla f_i(w)
$$
这个表达式 [@problem_id:3124710] 揭示了[客户端漂移](@entry_id:634167)的根源：它与本地更新的步长 $\eta$、本地梯度的方向 $\nabla f_i(w)$ 以及本地损失[函数的曲率](@entry_id:173664) $\nabla^2 f_i(w)$ 直接相关。在数据[异质性](@entry_id:275678)高（即 $\nabla f_i$ 和 $\nabla^2 f_i$ 在客户端之间差异很大）的区域，[客户端漂移](@entry_id:634167)问题会尤为严重，可能导致[模型收敛](@entry_id:634433)速度变慢、精度下降甚至发散。

### 应对核心挑战：变体与增强机制

为了将联邦学习从理论模型转变为能够应对现实世界复杂性的实用技术，研究人员开发了一系列增强机制。这些机制分别针对异质性、隐私、通信和安[全等](@entry_id:273198)核心挑战。

#### 减轻异质性：FedProx

**联邦[近端算法](@entry_id:174451) (Federated Proximal, FedProx)** 是应对[客户端漂移](@entry_id:634167)问题的一种著名方法。其核心思想是在每个客户端的本地优化目标中增加一个**近端项 (proximal term)**，以限制本地模型偏离当前全局模型的程度。

具体而言，在 FedProx 中，客户端 $i$ 在本地训练时优化的目标函数不再是 $f_i(w)$，而是：
$$
\phi_i(w) = f_i(w) + \frac{\lambda}{2} \|w - w_t\|^2
$$
其中 $w_t$ 是[本轮](@entry_id:169326)开始时的全局模型，$\lambda \ge 0$ 是一个控制近端项强度的超参数。这个二次惩罚项将本地更新“[拉回](@entry_id:160816)”到初始的全局模型 $w_t$ 附近。当 $\lambda=0$ 时，FedProx 退化为 [FedAvg](@entry_id:634153)。

近端项的引入改变了本地[目标函数](@entry_id:267263)的性质。如果原始本地目标 $f_i(w)$ 是 $\mu$-强凸和 $L$-光滑的，那么加入了近端项的新目标 $\phi_i(w)$ 将变为 $(\mu+\lambda)$-强凸和 $(L+\lambda)$-光滑的。这意味着本地[优化问题](@entry_id:266749)变得“更简单”，收敛性更好。更强的[凸性](@entry_id:138568)可以有效抑制由统计[异质性](@entry_id:275678)（由梯度差异范数 $\Delta$ 界定）引起的本地模型漂移。具体而言，本地最优解与一个理想的中心化最优解之间的漂移距离被一个与 $\frac{\Delta}{\mu + \lambda}$ 成比例的量所约束。[@problem_id:3124719]

参数 $\lambda$ 的选择体现了一种权衡：
*   **较大的 $\lambda$**：更强地约束本地模型，有效减少[客户端漂移](@entry_id:634167)，[增强算法](@entry_id:635795)在高度 non-IID 数据下的稳定性。但它也限制了本地模型在每轮训练中可以取得的进展，可能减慢整体[收敛速度](@entry_id:636873)。
*   **较小的 $\lambda$**：允许本地模型进行更大幅度的更新，可能在 IID 或轻度 non-IID 环境下收敛更快，但在高度异质的环境中可能不稳定。

选择合适的 $\lambda$ 对于 FedProx 的性能至关重要。它需要与本地学习率 $\eta$ 等其他超参数协同调整，以在抑制漂移和保证收敛进展之间取得平衡。[@problem_id:3124719]

#### 确保隐私：[安全聚合](@entry_id:754615)与[差分隐私](@entry_id:261539)

隐私是联邦学习的基石。然而，标准的 [FedAvg](@entry_id:634153) 算法并不能提供严格的隐私保证，因为它要求客户端将模型更新（例如梯度）明文发送给中央服务器。这些更新可能被恶意服务器或窃听者用来推断有关客户端本地数据的敏感信息。为了解决这个问题，研究者引入了[密码学](@entry_id:139166)和统计学技术。

**[安全聚合](@entry_id:754615) (Secure Aggregation, SecAgg)** 是一种密码学方法，其目标是让服务器只能得到所有客户端更新的总和，而无法看到任何单个客户端的更新。一个简化的 SecAgg 方案 [@problem_id:3124667] 可以通过**成对抵消掩码 (pairwise-canceling masks)** 来实现：

1.  在每一轮开始前，每对客户端 $(i, j)$ 通过一个安全信道协商一对随机生成的掩码向量 $r_{ij}$ 和 $r_{ji}$，它们满足 $r_{ij} = -r_{ji}$。这个掩码只有客户端 $i$ 和 $j$ 知道。
2.  客户端 $i$ 计算其本地更新 $g_i$，然后用它与所有其他客户端共享的掩码来混淆它，得到要发送的报告 $x_i = g_i + \sum_{j \ne i} r_{ij}$。
3.  所有客户端将它们的混淆报告 $x_i$ 发送给服务器。
4.  服务器计算所有报告的总和：$\sum_i x_i = \sum_i g_i + \sum_{i \ne j} r_{ij}$。由于 $r_{ij} + r_{ji} = 0$，所有掩码项在总和中成对抵消，最终服务器得到精确的聚合更新 $\sum_i g_i$，但无法从单个 $x_i$ 中分离出 $g_i$。

这种基于[秘密共享](@entry_id:274559)的方案依赖于模运算（例如，在有限域 $\mathbb{Z}_p$ 中）来实现[信息论安全](@entry_id:140051)，并且需要所有参与客户端都成功完成协议。在实践中，需要更复杂的协议来处理客户端掉线等问题。[@problem_id:3124667]

**[差分隐私](@entry_id:261539) (Differential Privacy, DP)** 提供了另一种更强的、基于统计的隐私保证。DP 的目标是确保从聚合结果中无法推断出任何单个客户端是否参与了[本轮](@entry_id:169326)训练。这通常通过**[差分隐私](@entry_id:261539)[随机梯度下降](@entry_id:139134) (DP-SGD)** 实现，其核心是在聚合之前对客户端更新进行处理：

1.  **[梯度裁剪](@entry_id:634808) (Gradient Clipping):** 每个客户端在本地计算其梯度（或模型更新）后，会根据一个预设的裁剪阈值 $C$ 来缩放其范数。如果梯度的 $L_2$ 范数 $\|g_i\|_2$ 超过 $C$，则梯度被缩放为 $g_i' = g_i \cdot \frac{C}{\|g_i\|_2}$。这一步的目的是限制单个客户端对聚合结果的最大可能贡献，即**敏感度 (sensitivity)**。
2.  **添加噪声 (Noise Addition):** 在服务器端聚合了所有裁剪后的梯度后，再向聚合结果的每个坐标上添加独立采样的、来自已知[分布](@entry_id:182848)（通常是高斯分布）的噪声。

噪声的大小取决于所需的隐私级别（由[隐私预算](@entry_id:276909) $(\varepsilon, \delta)$ 指定）、裁剪阈值 $C$ 和客户端数量。为了在满足给定[隐私预算](@entry_id:276909) $(\varepsilon, \delta)$ 的同时最大化模型效用，需要仔细选择裁剪阈值。例如，可以设置与客户端梯度范数成比例的个性化裁剪阈值 $C_i = \lambda \|\mathbf{g}_i\|_2$，并通过求解[优化问题](@entry_id:266749)来找到在满足隐私约束下的最大保留比例 $\lambda$。这允许系统在保护隐私的同时，更公平地对待那些梯度范数天然较大的客户端。[@problem_id:3124646]

#### 提升通信效率：模型量化

在联邦学习中，特别是对于大型[深度学习模型](@entry_id:635298)，模型更新的维度可能达到数百万甚至数十亿。将这些高维向量在每轮训练中从大量客户端传输到服务器，会产生巨大的[通信开销](@entry_id:636355)，成为系统瓶颈。**模型量化 (Model Quantization)** 是一种有效的应对策略。

量化的核心思想是用较低比特数的离散值来表示高精度的[浮点数](@entry_id:173316)（例如 32 位）。例如，一个 $b$-bit 的[均匀量化器](@entry_id:192441)可以将一个范围为 $[-s, s]$ 内的数值映射到 $2^b$ 个离散级别中的一个。为了在降低通信量的同时不引入系统性偏差，通常采用**[随机舍入](@entry_id:164336) (stochastic rounding)**：一个值被随机地量化到其相邻的两个量化级别之一，概率与其离这两个级别的距离成反比。这种方法保证了量化后的值在期望上等于原始值，即量化器是**无偏**的。[@problem_id:3124717]

然而，通信效率的提升并非没有代价。尽管随机量化是无偏的，它向梯度更新中注入了额外的**量化噪声**。这个噪声的[方差](@entry_id:200758)（期望平方误差）与量化区间的范围 $s$ 的平方成正比，与量化比特数 $b$ 的指数成反比。一个典型的结果是，期望平方[量化误差](@entry_id:196306)为 $\mathbb{E}\|Q(g) - g\|^2 = \frac{2ds^2}{3(2^b - 1)^2}$，其中 $d$ 是模型维度。[@problem_id:3124717]

在[随机梯度下降](@entry_id:139134)的[收敛性分析](@entry_id:151547)中，算法最终能达到的精度受限于[梯度估计](@entry_id:164549)的总[方差](@entry_id:200758)。量化噪声与SGD固有的随机[梯度噪声](@entry_id:165895)（由小批量采样引起）叠加，增大了总[方差](@entry_id:200758)。这导致了使用量化通信的联邦学习算法的**误差下界 (error floor)** 会高于使用精确通信的算法。换言之，[模型收敛](@entry_id:634433)到的最终精度会更差。因此，在量化方案的设计中，存在一个关键的权衡：使用更少的比特数可以节省更多通信，但会引入更大的[方差](@entry_id:200758)，损害模型性能。

#### 增强稳健性：[拜占庭容错](@entry_id:747029)聚合

标准的联邦学习系统假设所有参与的客户端都是诚实且正常工作的。然而，在开放或敌对的环境中，一些客户端可能是**拜占庭节点 (Byzantine nodes)**——它们可能因为故障或恶意企图而向服务器发送任意设计的、有害的更新，企图破坏全局模型的训练过程。

[FedAvg](@entry_id:634153) 使用的算术平均聚合规则对此类攻击极其脆弱。一个单一的拜占庭客户端只需发送一个具有极大范数的向量，就可以将聚合结果任意拉向任何方向，彻底破坏[本轮](@entry_id:169326)更新。在[鲁棒统计](@entry_id:270055)学中，我们用**击溃点 (breakdown point)** 来衡量一个估计器对异常值的容忍能力。击溃点是指能够使估计器产生任意大误差所需的最小污染数据比例。[算术平均值](@entry_id:165355)的击溃点为 0，意味着它无法容忍任何一个异常值。[@problem_id:3124668]

为了构建对拜占庭攻击具有稳健性的联邦学习系统，必须采用**鲁棒聚合规则 (robust aggregation rules)**。这些规则通常基于排序统计量，而不是简单的求和。常见的例子包括（通常是逐坐标应用）：

*   **坐标[中位数](@entry_id:264877) (Coordinate-wise Median):** 对于模型的每一个坐标，服务器不计算均值，而是计算所有客户端在该坐标上报告值的[中位数](@entry_id:264877)。中位数的击溃点为 0.5，这意味着只要少于一半的客户端是拜占庭节点，聚合结果的每个坐标都会被限制在诚实客户端报告值的范围内。[@problem_id:3124668]
*   **坐标截尾均值 (Coordinate-wise Trimmed Mean):** 对于每个坐标，服务器首先对所有报告值进行排序，然后丢弃掉最小的 $\tau$ 比例和最大的 $\tau$ 比例的值，最后计算剩余值的均值。截尾均值的击溃点为 $\tau$。通过选择合适的 $\tau$（例如，略大于预期的拜占庭节点比例），可以确保恶意值被“裁剪”掉，从而不影响最终的聚合结果。[@problem_id:3124668]

这些鲁棒聚合器以牺牲部分[统计效率](@entry_id:164796)为代价，换取了对恶意攻击的抵抗能力，是构建安全可靠联邦学习系统的关键组成部分。

### 联邦学习的场景与比较

理解联邦学习的原理与机制，还需要将其置于具体的应用场景和与其他相关技术的比较中。

#### 跨孤岛与跨设备场景

联邦学习的部署模式通常分为两大类，它们在系统特性和设计考量上存在显著差异：

*   **跨孤岛 (Cross-Silo) 联邦学习:** 参与者通常是少数（例如 $2 \sim 100$ 个）大型组织或机构（如医院、银行）。这些参与者通常一直在线，计算和网络资源稳定可靠，数据量大且质量高。
*   **跨设备 (Cross-Device) 联邦学习:** 参与者是海量的（例如数百万）个人终端设备（如手机、物联网设备）。这些设备计算能力、网络连接和电量都受限，参与训练是[间歇性](@entry_id:275330)的、不可靠的，且其本地数据量相对较小。

这两种场景的一个关键区别在于**客户端[参与率](@entry_id:197893) (client participation rate)** 对训练稳定性的影响。我们可以通过一个简化的模型来分析。假设聚合梯度的噪声[标准差](@entry_id:153618)需要小于真实梯度大小的一个比例 $\alpha$ 以保证稳定学习。当服务器从 $n$ 个客户端的总体中无放回地随机抽样 $m$ 个客户端时，聚合[梯度估计](@entry_id:164549)的[方差](@entry_id:200758)不仅与样本量 $m$ 有关，还受到**有限总体修正 (Finite Population Correction, FPC)** 因子 $(n-m)/(n-1)$ 的影响。[@problem_id:3124636]

分析表明 [@problem_id:3124636]：
*   在**跨孤岛**场景中，由于总客户端数 $n$ 很小（例如 $n=10$），FPC 因子变得非常重要。为了将聚合梯度的[方差](@entry_id:200758)降低到足够小的水平，通常需要极高的[参与率](@entry_id:197893)（例如，接近 $100\%$）。
*   在**跨设备**场景中，$n$ 极大（例如 $n=10^4$），FPC 因子接近 1，可以忽略。此时，聚合梯度的[方差](@entry_id:200758)几乎只反比于参与的客户端数量 $m$。即使[参与率](@entry_id:197893) $p = m/n$ 很低（例如 $10\% \sim 15\%$），只要绝对数量 $m$ 足够大，就可以有效地将[方差](@entry_id:200758)控制在所需范围内。

这一分析为跨设备联邦学习中采用低[参与率](@entry_id:197893)的常见做法提供了理论依据，也凸显了为不同场景量身定制系统设计的重要性。

#### 与其他[分布](@entry_id:182848)式学习[范式](@entry_id:161181)的比较：分裂学习

联邦学习是[分布](@entry_id:182848)式机器学习的[范式](@entry_id:161181)之一，但并非唯一。**分裂学习 (Split Learning, SL)** 是另一个值得关注的保护隐私的[分布](@entry_id:182848)式学习框架。与联邦学习中每个客户端持有完整模型进行训练不同，分裂学习将模型本身“分裂”成多个部分，由不同参与方分别持有和计算。

在一个典型的 SL 序列管道设置中 [@problem_id:3124634]，数据持有方（客户端 1）只持有模型的前几层。它完成[前向传播](@entry_id:193086)计算后，将输出的激活值（称为**切片数据 (smashed data)**）发送给下一个参与方（例如客户端 2，或直接是服务器），后者继续进行后续层的计算。当整个[前向传播](@entry_id:193086)完成后，梯度以相反的顺序在管道中[反向传播](@entry_id:199535)。

将 SL 与 FL 进行比较，可以揭示它们在架构和特性上的根本差异 [@problem_id:3124634]：
*   **并行性与延迟:** FL 是高度并行的，所有参与客户端可以同时进行本地计算。而 SL（在基本管道设置中）是串行的，一个批次的数据必须依次通过每个参与方，导致其端到端延迟通常远高于 FL。
*   **隐私暴露面:** 在 FL 中，服务器会看到所有参与者的模型更新，但客户端之间不直接交互。在 SL 中，服务器可能只看到模型最后几层的梯度，但相邻的客户端会看到彼此之间传递的激活值和梯度。这些中间激活值可能比模型梯度更接近原始数据，因此 SL 的隐私暴露面与 FL 不同，且可能在客户端之间引入新的隐私风险。
*   **通信模式:** FL 的通信模式是“星型”的，客户端与服务器之间双向通信。SL 的通信模式是“链式”或更复杂的图结构，数据和梯度在参与方之间传递。

总而言之，联邦学习通过本地化计算和[模型平均](@entry_id:635177)，在去中心化数据上实现了并行训练。但要构建一个实用、高效且安全的联邦学习系统，必须综合考虑并解决由数据异质性、隐私要求、通信限制和潜在的恶意行为带来的多重挑战。本章介绍的原理和机制，为理解和应对这些挑战提供了坚实的基础。