{"hands_on_practices": [{"introduction": "机器学习中的许多任务都涉及从一组候选项中选择“最佳”的一项，例如挑选最佳模型。指数机制提供了一个符合差分隐私原则的方法，使得我们更有可能选择效用得分高的选项，同时保证隐私。本练习将指导您从零开始实现这一基础机制，深入理解如何在保障隐私的前提下做出最优选择。[@problem_id:3165692]", "problem": "您的任务是构建一个选择机制，在根据实值效用函数从有限的候选集中选择单个模型时，满足差分隐私（DP）。请仅从以下基础出发：$\\epsilon$-差分隐私（DP）的定义和效用函数的有界敏感度。如果对于所有相差一条记录的邻近数据集 $D$ 和 $D'$，以及对于所有可测的输出子集 $S$，以下不等式成立，则称随机化机制 $\\mathcal{M}$ 满足 $\\epsilon$-差分隐私：\n$$\n\\Pr[\\mathcal{M}(D) \\in S] \\le e^{\\epsilon} \\Pr[\\mathcal{M}(D') \\in S].\n$$\n假设有一个实值效用函数 $u(D, r)$，用于输出 $r$，其已知的全局敏感度为 $\\Delta u$：\n$$\n\\Delta u = \\max_{D \\sim D'} \\max_{r} |u(D, r) - u(D', r)|.\n$$\n构建一个用于选择单个模型的机制，该机制对于 $D$ 是差分隐私的，并且其选择概率偏向于具有更高效用值的模型。请仅使用上述基本定义推导出该机制的选择规则，然后实现它。\n\n您的程序必须实现从第一性原理推导出的指数机制。给定：\n- $k$ 个候选模型的效用列表 $[u_0, u_1, \\dots, u_{k-1}]$，\n- 隐私参数 $\\epsilon > 0$，\n- 效用敏感度 $\\Delta u > 0$，\n输出您的机制所隐含的候选模型的归一化选择概率，然后为一组测试用例计算指定的属性。\n\n在您的最终输出中不要使用任何随机性；请确定性地计算精确概率。使用数值稳定的计算方法。模型索引使用零基索引。\n\n测试套件。对于下面的每个测试用例，精确计算所要求的量。所有相等性检查的公差均为 $10^{-12}$ 的绝对公差。\n- 测试 A (正常情况)：效用值 $[1.0, 2.0, 0.0]$，$\\epsilon = 1.0$，$\\Delta u = 1.0$。按顺序输出两个量：\n  1) 选择具有最大效用值的模型的概率（浮点数），四舍五入到 $6$ 位小数，\n  2) 在您的机制下，具有最大选择概率的模型的零基索引（整数）。\n- 测试 B (边界情况 $\\epsilon = 0$)：效用值 $[3.0, -1.0, 7.0, 7.0]$，$\\epsilon = 0.0$，$\\Delta u = 1.0$。输出一个布尔值，当且仅当生成的分布在公差范围内是均匀的，即所有概率在指定公差内等于 $1/k$ 时，该值为真。\n- 测试 C (平局公平性)：效用值 $[0.5, 0.5, 0.0]$，$\\epsilon = 2.0$，$\\Delta u = 1.0$。输出一个布尔值，当且仅当相同的效用值在公差范围内为对应的索引带来相等的选择概率时，该值为真。\n- 测试 D (邻近数据集上的隐私比率检查，两个输出)：效用值 $u = [0.0, 1.0]$，邻近效用值 $u' = [0.2, 0.8]$，$\\epsilon = 0.5$，$\\Delta u = 0.2$。由于只有 $2$ 个输出，验证单元素事件的不等式意味着它对所有可测子集都成立。输出一个布尔值，当且仅当\n  $$\n  \\max\\left\\{\\max_{i \\in \\{0,1\\}} \\frac{p_i(u)}{p_i(u')}, \\max_{i \\in \\{0,1\\}} \\frac{p_i(u')}{p_i(u)}\\right\\} \\le e^{\\epsilon} + 10^{-12},\n  $$\n  成立时，该值为真，其中 $p_i(\\cdot)$ 表示在相应效用向量下索引 $i$ 的选择概率。\n- 测试 E (大 $\\epsilon$ 下的集中性)：效用值 $[0.0, 2.0, 1.0]$，$\\epsilon = 10.0$，$\\Delta u = 1.0$。输出选择具有最大效用值的模型的概率（浮点数），四舍五入到 $6$ 位小数。\n- 测试 F (敏感度的影响)：效用值 $[0.0, 2.0, 1.0]$，$\\epsilon = 1.0$，比较 $\\Delta u_1 = 1.0$ 与 $\\Delta u_2 = 2.0$。输出一个布尔值，当且仅当使用 $\\Delta u_1$ 选择最佳效用模型的概率在公差范围内大于或等于使用 $\\Delta u_2$ 的概率时，该值为真。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，结果按以下顺序排列：\n$$\n[\\text{测试 A 概率}, \\text{测试 A 索引}, \\text{测试 B 布尔值}, \\text{测试 C 布尔值}, \\text{测试 D 布尔值}, \\text{测试 E 概率}, \\text{测试 F 布尔值}]\n$$\n对于概率，在输出前四舍五入到 $6$ 位小数。索引是整数，布尔值是编程语言的原生布尔格式。不需要外部输入；请在程序中硬编码上述测试套件参数。本问题不涉及物理单位或角度。所有十进制数量均以小数形式表示，而不是百分比。", "solution": "任务是推导并实现一个差分隐私机制，用于从有限的候选模型集中根据实值效用函数选择一个模型。推导必须从 $\\epsilon$-差分隐私（$\\epsilon$-DP）和全局敏感度的基本定义开始。\n\n设 $R$ 为候选模型的有限集。设 $u(D, r)$ 是一个实值效用函数，它根据数据集 $D$ 为模型 $r \\in R$ 打分。$u$ 的全局敏感度定义为 $\\Delta u = \\max_{D \\sim D'} \\max_{r \\in R} |u(D, r) - u(D', r)|$，其中 $D$ 和 $D'$ 是相差一条记录的邻近数据集。\n\n我们的目标是构建一个随机化机制 $\\mathcal{M}$，该机制在给定数据集 $D$ 的情况下，以概率 $\\Pr[\\mathcal{M}(D) = r]$ 输出一个模型 $r \\in R$。该机制必须满足 $\\epsilon$-DP，即对于任何一对邻近数据集 $D, D'$ 和任何输出子集 $S \\subseteq R$，我们有 $\\Pr[\\mathcal{M}(D) \\in S] \\le e^{\\epsilon} \\Pr[\\mathcal{M}(D') \\in S]$。只需对所有单元素集 $S = \\{r\\}$ 证明这一点即可。\n\n为了偏向具有更高效用值的模型，我们提议选择模型 $r$ 的概率与其效用的指数函数成正比。这是一个常见的选择，因为它为更高的效用值提供了强烈的偏好，同时在数学上易于处理。设未归一化的分数为 $\\exp(c \\cdot u(D, r))$，其中 $c > 0$ 是一个待定常数。\n\n为了形成一个有效的概率分布，我们将这些分数在所有模型 $R$ 上进行归一化：\n$$\n\\Pr[\\mathcal{M}(D) = r] = \\frac{\\exp(c \\cdot u(D, r))}{\\sum_{r' \\in R} \\exp(c \\cdot u(D, r'))}\n$$\n现在，我们必须找到能确保该机制满足 $\\epsilon$-DP 的 $c$ 值。我们分析在两个邻近数据集 $D$ 和 $D'$ 上，对于任意输出 $r$ 的概率比率：\n$$\n\\frac{\\Pr[\\mathcal{M}(D) = r]}{\\Pr[\\mathcal{M}(D') = r]} = \\frac{\\frac{\\exp(c \\cdot u(D, r))}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j))}}{\\frac{\\exp(c \\cdot u(D', r))}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))}} = \\frac{\\exp(c \\cdot u(D, r))}{\\exp(c \\cdot u(D', r))} \\cdot \\frac{\\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j))}\n$$\n我们使用全局敏感度 $\\Delta u$ 的定义来约束这个比率。我们可以分别约束乘积中的两项。\n\n对于第一项，根据敏感度的定义，$u(D, r) \\le u(D', r) + \\Delta u$。由于 $c > 0$，这意味着：\n$$\n\\exp(c \\cdot u(D, r)) \\le \\exp(c \\cdot (u(D', r) + \\Delta u)) = \\exp(c \\cdot u(D', r)) \\cdot \\exp(c \\cdot \\Delta u)\n$$\n因此，第一项受 $\\exp(c \\cdot \\Delta u)$ 的约束。\n\n对于第二项，即归一化常数的比率，我们为其分母建立一个下界。对于任何模型 $r_j \\in R$，我们有 $u(D, r_j) \\ge u(D', r_j) - \\Delta u$。因此：\n$$\n\\exp(c \\cdot u(D, r_j)) \\ge \\exp(c \\cdot (u(D', r_j) - \\Delta u)) = \\exp(c \\cdot u(D', r_j)) \\cdot \\exp(-c \\cdot \\Delta u)\n$$\n对所有 $r_j \\in R$求和，我们得到整个和的界限：\n$$\n\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j)) \\ge \\sum_{r_j \\in R} \\left( \\exp(c \\cdot u(D', r_j)) \\cdot \\exp(-c \\cdot \\Delta u) \\right) = \\exp(-c \\cdot \\Delta u) \\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))\n$$\n这给了我们和的倒数的上界：\n$$\n\\frac{1}{\\sum_{r_j \\in R} \\exp(c \\cdot u(D, r_j))} \\le \\frac{1}{\\exp(-c \\cdot \\Delta u) \\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))}\n$$\n将此乘以第二项的分子 $\\sum_{r_j \\in R} \\exp(c \\cdot u(D', r_j))$，我们发现第二项受 $\\exp(c \\cdot \\Delta u)$ 的约束。\n\n结合两项的界限，我们得到概率比率的上界：\n$$\n\\frac{\\Pr[\\mathcal{M}(D) = r]}{\\Pr[\\mathcal{M}(D') = r]} \\le \\exp(c \\cdot \\Delta u) \\cdot \\exp(c \\cdot \\Delta u) = \\exp(2c \\cdot \\Delta u)\n$$\n为了满足 $\\epsilon$-DP 约束，这个界限不能大于 $e^\\epsilon$：\n$$ \\exp(2c \\cdot \\Delta u) \\le e^{\\epsilon} \\implies 2c \\cdot \\Delta u \\le \\epsilon $$\n为了提供最强的效用保证（即，使选择对效用尽可能敏感，同时满足隐私要求），我们应该选择 $c$ 的最大可能值，这对应于将不等式设为等式：$2c \\cdot \\Delta u = \\epsilon$。这得出了 $c$ 的值：\n$$\nc = \\frac{\\epsilon}{2 \\Delta u}\n$$\n将此代入我们的概率公式，得到模型选择的指数机制的最终形式：\n$$\np_i = \\Pr[\\mathcal{M}(D) = r_i] = \\frac{\\exp\\left(\\frac{\\epsilon u_i}{2 \\Delta u}\\right)}{\\sum_{j=0}^{k-1} \\exp\\left(\\frac{\\epsilon u_j}{2 \\Delta u}\\right)}\n$$\n其中 $u_i$ 是 $u(D, r_i)$ 的简写。为了实现，为避免计算指数时出现数值溢出，我们使用 log-sum-exp 稳定化技巧。令 $s_i = \\frac{\\epsilon u_i}{2 \\Delta u}$ 和 $s_{\\max} = \\max_j s_j$，则概率计算如下：\n$$\np_i = \\frac{\\exp(s_i - s_{\\max})}{\\sum_{j=0}^{k-1} \\exp(s_j - s_{\\max})}\n$$\n这种形式在数值上是稳定的，因为指数函数的参数都是非正数。", "answer": "```python\nimport numpy as np\n\ndef run_exponential_mechanism(utilities, epsilon, delta_u):\n    \"\"\"\n    Computes the selection probabilities using the Exponential Mechanism.\n    \n    Args:\n        utilities (list or np.ndarray): A list of utility scores for k models.\n        epsilon (float): The privacy parameter epsilon.\n        delta_u (float): The global sensitivity of the utility function.\n        \n    Returns:\n        np.ndarray: An array of k selection probabilities.\n    \"\"\"\n    utilities = np.array(utilities, dtype=np.float64)\n    k = len(utilities)\n    \n    # Handle the boundary case of epsilon = 0, which implies a uniform distribution.\n    if epsilon == 0.0:\n        return np.full(k, 1.0 / k)\n    \n    # The problem specifies epsilon > 0 and delta_u > 0 for the main derivation,\n    # but Test B uses epsilon=0. delta_u=0 would be an issue, but is not tested.\n    if delta_u == 0:\n        raise ValueError(\"delta_u must be positive.\")\n\n    # Calculate scaled utilities as per the derived formula.\n    scaling_factor = epsilon / (2.0 * delta_u)\n    scaled_utilities = scaling_factor * utilities\n    \n    # Use the log-sum-exp trick for numerical stability.\n    # Subtracting the max value from each scaled utility before exponentiating\n    # prevents overflow and mitigates underflow.\n    max_scaled_utility = np.max(scaled_utilities)\n    exp_utilities = np.exp(scaled_utilities - max_scaled_utility)\n    \n    sum_exp_utilities = np.sum(exp_utilities)\n    \n    probabilities = exp_utilities / sum_exp_utilities\n    return probabilities\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    results = []\n    TOL = 1e-12\n\n    # --- Test A ---\n    u_A = [1.0, 2.0, 0.0]\n    eps_A = 1.0\n    du_A = 1.0\n    probs_A = run_exponential_mechanism(u_A, eps_A, du_A)\n    best_utility_idx_A = np.argmax(u_A)\n    prob_of_best_A = probs_A[best_utility_idx_A]\n    max_prob_idx_A = np.argmax(probs_A)\n    results.append(round(prob_of_best_A, 6))\n    results.append(int(max_prob_idx_A))\n\n    # --- Test B ---\n    u_B = [3.0, -1.0, 7.0, 7.0]\n    eps_B = 0.0\n    du_B = 1.0\n    probs_B = run_exponential_mechanism(u_B, eps_B, du_B)\n    k_B = len(u_B)\n    is_uniform_B = np.all(np.abs(probs_B - 1.0/k_B)  TOL)\n    results.append(is_uniform_B)\n    \n    # --- Test C ---\n    u_C = [0.5, 0.5, 0.0]\n    eps_C = 2.0\n    du_C = 1.0\n    probs_C = run_exponential_mechanism(u_C, eps_C, du_C)\n    # indices 0 and 1 have tied utilities\n    tied_probs_equal_C = np.abs(probs_C[0] - probs_C[1])  TOL\n    results.append(tied_probs_equal_C)\n\n    # --- Test D ---\n    u_D = [0.0, 1.0]\n    u_prime_D = [0.2, 0.8]\n    eps_D = 0.5\n    du_D = 0.2\n    probs_D = run_exponential_mechanism(u_D, eps_D, du_D)\n    probs_prime_D = run_exponential_mechanism(u_prime_D, eps_D, du_D)\n    \n    # Prevent division by zero, although not expected here\n    # Adding a small constant is one way, but given the problem setup,\n    # probabilities should be non-zero.\n    ratio1 = probs_D / probs_prime_D\n    ratio2 = probs_prime_D / probs_D\n    max_ratio = np.max(np.concatenate([ratio1, ratio2]))\n    \n    dp_holds_D = max_ratio = np.exp(eps_D) + TOL\n    results.append(dp_holds_D)\n\n    # --- Test E ---\n    u_E = [0.0, 2.0, 1.0]\n    eps_E = 10.0\n    du_E = 1.0\n    probs_E = run_exponential_mechanism(u_E, eps_E, du_E)\n    best_utility_idx_E = np.argmax(u_E)\n    prob_of_best_E = probs_E[best_utility_idx_E]\n    results.append(round(prob_of_best_E, 6))\n\n    # --- Test F ---\n    u_F = [0.0, 2.0, 1.0]\n    eps_F = 1.0\n    du1_F = 1.0\n    du2_F = 2.0\n    \n    probs1_F = run_exponential_mechanism(u_F, eps_F, du1_F)\n    probs2_F = run_exponential_mechanism(u_F, eps_F, du2_F)\n    \n    best_utility_idx_F = np.argmax(u_F)\n    prob_best1_F = probs1_F[best_utility_idx_F]\n    prob_best2_F = probs2_F[best_utility_idx_F]\n    \n    concentration_check_F = prob_best1_F >= prob_best2_F - TOL\n    results.append(concentration_check_F)\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3165692"}, {"introduction": "差分隐私训练通常旨在保护整个数据样本，但有时我们更关心保护敏感的标签信息，而非特征本身。本练习将引导您实现一种只为标签提供隐私保障的训练方法，并通过实施模型反演攻击，直观地检验这种方法在多大程度上能够防止敏感特征信息的泄露。这个实践将加深您对不同隐私模型之间细微差别的理解。[@problem_id:3165689]", "problem": "给定一个监督二元分类问题，其中线性模型通过随机梯度下降（Stochastic Gradient Descent, SGD）进行训练。数据域包含一个指定的敏感特征。您的任务是实现仅对标签应用隐私保护的训练，然后执行模型反演攻击以评估敏感特征的泄露情况。分析必须在 $(\\epsilon,\\delta)$-差分隐私（Differential Privacy, DP）的框架下进行。\n\n构建一个包含 $N$ 个样本和 $d$ 个特征的合成数据集，其中第一个特征是敏感特征。对于每个特征向量为 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 的样本索引 $i$，将敏感坐标 $x_{i,0} \\in \\{-1, +1\\}$ 定义为中心化的伯努利随机变量，其余坐标 $x_{i,1},\\dots,x_{i,d-1}$ 定义为独立的、经过缩放的高斯变量，以使每个样本的向量范数上限为一个常数 $C_x$。根据逻辑斯谛生成过程生成标签 $y_i \\in \\{0,1\\}$，其中真实的 logit 强烈依赖于敏感坐标，而极弱地依赖于非敏感坐标，并带有加性的小高斯噪声。您必须确保每个 $\\mathbf{x}_i$ 满足 $\\lVert \\mathbf{x}_i \\rVert_2 \\leq C_x$。\n\n使用从零初始化的 SGD 的一个梯度步骤来训练一个线性模型，其参数为 $\\mathbf{w} \\in \\mathbb{R}^d$，偏置为 $b \\in \\mathbb{R}$，学习率为 $\\eta$。令模型对输入 $\\mathbf{x}$ 的预测概率为 $\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$，其中 $\\sigma$ 是逻辑斯谛 sigmoid 函数。计算每个样本的残差为 $r_i = \\sigma(\\mathbf{w}^\\top \\mathbf{x}_i + b) - y_i$，并将每个 $r_i$ 裁剪到区间 $[-c, c]$ 内，其中 $c$ 的选择应使得更改一个标签最多能使 $r_i$ 改变 1。将关于 $\\mathbf{w}$ 的平均梯度构建为批次上 $\\mathbf{x}_i r_i$ 的均值。为了仅实现标签隐私，通过高斯机制对聚合梯度添加独立的高斯噪声，该机制应用于标签的函数，并将特征视为固定的和有界的。噪声的方差需要经过校准。平均梯度对于改变单个标签的敏感度必须从界限 $\\lVert \\mathbf{x}_i \\rVert_2 \\leq C_x$ 和 $r_i$ 的裁剪中推导得出；校准高斯机制以保证标签的 $(\\epsilon,\\delta)$-DP。将相同的标签私有高斯机制应用于从平均 $r_i$ 计算出的偏置梯度。\n\n在这个单一的带噪梯度步骤之后，执行一次模型反演攻击，寻找一个输入 $\\hat{\\mathbf{x}}$，该输入在最大化模型 logit 的同时，通过二次正则化项惩罚大的输入。具体来说，对于选定的 $\\lambda  0$，求解目标函数 $J(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x} + b - \\lambda \\lVert \\mathbf{x} \\rVert_2^2$ 的最大化器。使用得到的 $\\hat{\\mathbf{x}}$ 来定义一个泄露度量，该度量比较敏感坐标的量级与非敏感坐标的量级：\n$$\nR = \\frac{\\left| \\hat{x}_0 \\right|}{\\frac{1}{d-1} \\sum_{j=1}^{d-1} \\left| \\hat{x}_j \\right|}.\n$$\n如果 $R  \\tau$（对于一个选定的阈值 $\\tau  0$），则声明存在泄露，否则声明不存在泄露。\n\n您的程序必须实现上述流程，并运行以下测试套件，每个测试由元组 $(N, d, \\epsilon, \\delta, \\lambda, \\tau)$ 指定：\n- 测试 $1$ (正常路径，无隐私噪声): $(N=\\;200,\\; d=\\;4,\\; \\epsilon=\\;\\text{None},\\; \\delta=\\;10^{-5},\\; \\lambda=\\;0.1,\\; \\tau=\\;1.5)$。\n- 测试 $2$ (显著隐私，中等批量): $(N=\\;50,\\; d=\\;4,\\; \\epsilon=\\;0.1,\\; \\delta=\\;10^{-5},\\; \\lambda=\\;0.1,\\; \\tau=\\;1.5)$。\n- 测试 $3$ (边界条件，非常强的隐私): $(N=\\;20,\\; d=\\;4,\\; \\epsilon=\\;0.01,\\; \\delta=\\;10^{-5},\\; \\lambda=\\;0.1,\\; \\tau=\\;1.5)$。\n\n在所有测试中，使用学习率 $\\eta=\\;1.0$，单个样本残差裁剪参数 $c=\\;0.5$（这样翻转一个标签最多使 $r_i$ 改变 1），以及通过对 $\\mathbf{x}_i$ 进行逐样本裁剪实现的特征范数界限 $C_x=\\;1.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述测试的顺序排列结果，其中每个元素是一个布尔值，指示该测试是否存在泄露（例如，$\\left[\\text{True},\\text{False},\\text{False}\\right]$）。不应打印任何其他文本。", "solution": "该问题被评估为有效，因为它在科学上基于差分隐私和机器学习的原理，问题设定良好，目标明确，并提供了一套足够完整、一致且可形式化的指令。\n\n解决方案通过遵循三个主要步骤的顺序来实现：合成数据生成、标签私有的随机梯度下降（SGD）单步训练，以及用于量化信息泄露的模型反演攻击。\n\n### 步骤 1：合成数据生成\n我们构建了一个包含 $N$ 个样本的合成数据集，每个样本有 $d$ 个特征，由矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ 和标签向量 $\\mathbf{y} \\in \\{0, 1\\}^N$ 表示。\n\n1.  **特征生成**：\n    第一个特征 $\\mathbf{x}_{\\cdot, 0}$ 被指定为敏感特征。其值从中心化的伯努利分布中抽取，即对每个样本 $i=1, \\dots, N$，$x_{i,0} \\in \\{-1, +1\\}$。其余的 $d-1$ 个非敏感特征 $x_{i,j}$（其中 $j=1, \\dots, d-1$）从标准正态分布 $\\mathcal{N}(0,1)$ 中抽取。\n\n2.  **特征范数裁剪**：\n    为了在后续的隐私分析中限制梯度的敏感度，每个特征向量 $\\mathbf{x}_i$ 都经过归一化，以确保其 $L_2$ 范数不超过常数 $C_x=1.0$。具体来说，如果 $\\lVert \\mathbf{x}_i \\rVert_2  C_x$，则该向量会乘以一个因子 $C_x / \\lVert \\mathbf{x}_i \\rVert_2$。在问题描述中，此操作被称为逐样本裁剪。\n\n3.  **标签生成**：\n    标签 $y_i \\in \\{0, 1\\}$ 通过一个逻辑斯谛模型生成。我们定义一个真实权重向量 $\\mathbf{w}_{\\text{true}}$，以建立对敏感特征的强依赖和对非敏感特征的弱依赖。我们选择 $\\mathbf{w}_{\\text{true}} = [10, 0.1, \\dots, 0.1]^\\top$。样本 $i$ 的真实 logit 计算为 $z_i = \\mathbf{w}_{\\text{true}}^\\top \\mathbf{x}_i + n_i$，其中 $n_i \\sim \\mathcal{N}(0, 0.1^2)$ 是少量高斯噪声。标签 $y_i=1$ 的概率由逻辑斯谛 sigmoid 函数给出，$p_i = \\sigma(z_i) = (1 + e^{-z_i})^{-1}$。最终的标签从伯努利分布中抽取，$y_i \\sim \\text{Bernoulli}(p_i)$。\n\n### 步骤 2：差分隐私 SGD\n我们对一个线性模型执行单步梯度下降，从零初始化开始，即 $\\mathbf{w}_0 = \\mathbf{0}$ 和 $b_0 = 0$。隐私保证是关于标签的 $(\\epsilon, \\delta)$-差分隐私 (DP)。\n\n1.  **梯度计算**：模型对输入 $\\mathbf{x}$ 的预测是 $\\sigma(\\mathbf{w}^\\top \\mathbf{x} + b)$。对于第一步，当 $\\mathbf{w}=\\mathbf{w}_0$ 和 $b=b_0$ 时，对任何输入的预测都是 $\\sigma(0) = 0.5$。每个样本的残差是 $r_i = \\sigma(\\mathbf{w}_0^\\top \\mathbf{x}_i + b_0) - y_i = 0.5 - y_i$。这些残差被裁剪到范围 $[-c, c]$ 内，其中 $c=0.5$。由于 $y_i \\in \\{0,1\\}$，$r_i$ 的值要么是 $0.5$，要么是 $-0.5$，所以裁剪没有效果。关于 $\\mathbf{w}$ 和 $b$ 的平均梯度是：\n    $$\n    \\nabla_{\\mathbf{w}} L = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{x}_i r_i \\quad , \\quad \\nabla_{b} L = \\frac{1}{N} \\sum_{i=1}^N r_i\n    $$\n\n2.  **敏感度分析**：对于标签隐私，我们分析平均梯度对单个标签 $y_k$ 变化的敏感度。如果 $y_k$ 翻转，相应的残差 $r_k$ 变化量为 $\\Delta r_k = y_k - y'_k = \\pm 1$。$\\mathbf{w}$ 的平均梯度的最大变化是 $\\Delta(\\nabla_{\\mathbf{w}} L) = \\frac{1}{N} \\mathbf{x}_k (\\Delta r_k)$。$L_2$ 敏感度是这个变化量的最大 $L_2$ 范数：\n    $$\n    \\mathcal{S}_{\\mathbf{w}} = \\sup \\lVert \\Delta(\\nabla_{\\mathbf{w}} L) \\rVert_2 = \\sup \\frac{1}{N} \\lVert \\mathbf{x}_k \\rVert_2 |\\Delta r_k| = \\frac{C_x}{N}\n    $$\n    类似地，偏置梯度的敏感度是：\n    $$\n    \\mathcal{S}_{b} = \\sup |\\Delta(\\nabla_{b} L)| = \\sup \\frac{1}{N} |\\Delta r_k| = \\frac{1}{N}\n    $$\n\n3.  **高斯机制**：为确保 $(\\epsilon, \\delta)$-DP，我们向梯度中添加高斯噪声。噪声的标准差根据敏感度进行校准。对于一个 $L_2$ 敏感度为 $\\mathcal{S}$ 的函数，噪声标准差为 $\\sigma_{\\text{noise}} = \\frac{\\mathcal{S} \\sqrt{2 \\ln(1.25/\\delta)}}{\\epsilon}$。\n    - 对于 $\\nabla_{\\mathbf{w}} L$：$\\sigma_{\\mathbf{w}} = \\frac{C_x \\sqrt{2 \\ln(1.25/\\delta)}}{N \\epsilon}$。我们添加噪声 $\\mathbf{n}_{\\mathbf{w}} \\sim \\mathcal{N}(0, \\sigma_{\\mathbf{w}}^2 \\mathbf{I})$。\n    - 对于 $\\nabla_{b} L$：$\\sigma_{b} = \\frac{\\sqrt{2 \\ln(1.25/\\delta)}}{N \\epsilon}$。我们添加噪声 $n_b \\sim \\mathcal{N}(0, \\sigma_{b}^2)$。\n    对于 $\\epsilon$ 为 `None` 的测试用例，不添加噪声 ($\\sigma_{\\mathbf{w}}=0, \\sigma_{b}=0$)。\n\n4.  **模型更新**：权重和偏置使用带噪梯度和学习率 $\\eta=1.0$ 进行更新：\n    $$\n    \\mathbf{w}_1 = \\mathbf{w}_0 - \\eta(\\nabla_{\\mathbf{w}} L + \\mathbf{n}_{\\mathbf{w}}) = -\\eta(\\nabla_{\\mathbf{w}} L + \\mathbf{n}_{\\mathbf{w}})\n    $$\n    $$\n    b_1 = b_0 - \\eta(\\nabla_{b} L + n_b) = -\\eta(\\nabla_{b} L + n_b)\n    $$\n\n### 步骤 3：模型反演和泄露评估\n在单步训练之后，我们执行模型反演攻击，从学习到的参数 $\\mathbf{w}_1$ 和 $b_1$ 中恢复有关训练数据的信息。\n\n1.  **攻击的构建**：攻击者寻求一个输入 $\\hat{\\mathbf{x}}$，它能最大化模型的 logit，并通过一个 $L_2$ 正则化项进行惩罚以防止平凡解。目标函数是：\n    $$\n    J(\\mathbf{x}) = \\mathbf{w}_1^\\top \\mathbf{x} + b_1 - \\lambda \\lVert \\mathbf{x} \\rVert_2^2\n    $$\n    为了找到最大化器 $\\hat{\\mathbf{x}}$，我们将关于 $\\mathbf{x}$ 的梯度设为零：\n    $$\n    \\nabla_{\\mathbf{x}} J(\\mathbf{x}) = \\mathbf{w}_1 - 2\\lambda \\mathbf{x} = 0\n    $$\n    求解 $\\mathbf{x}$ 得到重建的输入：\n    $$\n    \\hat{\\mathbf{x}} = \\frac{1}{2\\lambda} \\mathbf{w}_1\n    $$\n\n2.  **泄露度量**：重建的向量 $\\hat{\\mathbf{x}}$ 预计会反映训练数据的属性。由于标签与敏感特征 $x_0$ 强相关，因此在不使用隐私保护时，梯度的第一个分量（也就是 $\\mathbf{w}_1$ 和 $\\hat{\\mathbf{x}}$ 的第一个分量）的量级应该很大。我们使用比率 $R$ 来衡量这种差异：\n    $$\n    R = \\frac{\\left| \\hat{x}_0 \\right|}{\\frac{1}{d-1} \\sum_{j=1}^{d-1} \\left| \\hat{x}_j \\right|}\n    $$\n    该比率比较了重建的敏感坐标的量级与非敏感坐标的平均量级。一个大的 $R$ 值表明模型不成比例地编码了关于敏感特征的信息。如果 $R  \\tau$，则声明存在泄露。\n\n该实现为每个测试用例执行了整个流程。在没有隐私噪声的情况下，$R$ 预计会很大，表明存在泄露。当有足够的隐私噪声时（即 $\\epsilon$ 很小），梯度中的信号被掩盖，导致 $\\hat{\\mathbf{x}}$ 的所有分量具有相似的量级（由各向同性噪声驱动），从而得到 $R \\approx 1$ 且未检测到泄露。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the label-private SGD and model inversion attack pipeline.\n    \"\"\"\n    # Use a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # --- Fixed Parameters ---\n    # Learning rate for SGD\n    ETA = 1.0\n    # Per-example residual clipping parameter\n    C_RESIDUAL = 0.5\n    # Per-example feature norm bound\n    C_X = 1.0\n    \n    # --- Test Suite ---\n    test_cases = [\n        # (N, d, epsilon, delta, lambda, tau)\n        (200, 4, None, 1e-5, 0.1, 1.5),  # Test 1: No privacy\n        (50, 4, 0.1, 1e-5, 0.1, 1.5),    # Test 2: Significant privacy\n        (20, 4, 0.01, 1e-5, 0.1, 1.5),   # Test 3: Very strong privacy\n    ]\n\n    results = []\n\n    for N, d, epsilon, delta, lambda_reg, tau in test_cases:\n        # --- Step 1: Data Generation ---\n        \n        # Sensitive feature: centered Bernoulli\n        sensitive_feature = 2 * np.random.binomial(1, 0.5, size=(N, 1)) - 1\n        \n        # Non-sensitive features: Gaussian\n        other_features = np.random.randn(N, d - 1)\n        \n        # Combine features\n        X = np.hstack([sensitive_feature, other_features])\n        \n        # Clip feature vectors to have L2 norm at most C_X\n        norms = np.linalg.norm(X, axis=1, keepdims=True)\n        # Add a small constant to avoid division by zero for zero-norm vectors\n        X /= np.maximum(1.0, norms / C_X) \n\n        # Generate labels from a logistic model\n        # True weights with strong dependency on the sensitive feature\n        w_true = np.array([10.0] + [0.1] * (d - 1))\n        # Add small Gaussian noise to true logits\n        logits_true = X @ w_true + np.random.normal(0, 0.1, size=N)\n        probs = 1 / (1 + np.exp(-logits_true))\n        y = np.random.binomial(1, probs).astype(float)\n\n        # --- Step 2: Differentially Private SGD Step ---\n        \n        # Start from zero initialization for weights and bias\n        w0 = np.zeros(d)\n        b0 = 0.0\n        \n        # With zero-init model, prediction is sigma(0) = 0.5 for all inputs\n        pred = 0.5\n        \n        # Calculate per-example residuals\n        residuals = pred - y  # Values are either +0.5 or -0.5\n        \n        # The problem requires clipping, but residuals are already within [-0.5, 0.5]\n        # np.clip(residuals, -C_RESIDUAL, C_RESIDUAL, out=residuals)\n        \n        # Calculate average gradients (non-private)\n        grad_w = (X.T @ residuals) / N\n        grad_b = np.mean(residuals)\n        \n        noisy_grad_w = grad_w\n        noisy_grad_b = grad_b\n\n        # Add Gaussian noise for differential privacy if epsilon is specified\n        if epsilon is not None and epsilon > 0:\n            # L2 sensitivity for label privacy\n            sensitivity_w = C_X / N\n            sensitivity_b = 1.0 / N\n            \n            # Calculate noise standard deviation for the Gaussian mechanism\n            # Common factor for both noise scales\n            privacy_term = np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n            \n            sigma_w = sensitivity_w * privacy_term\n            sigma_b = sensitivity_b * privacy_term\n            \n            # Add noise to gradients\n            noise_w = np.random.normal(0, sigma_w, size=d)\n            noise_b = np.random.normal(0, sigma_b)\n            \n            noisy_grad_w += noise_w\n            noisy_grad_b += noise_b\n            \n        # Perform a single SGD update step\n        w1 = w0 - ETA * noisy_grad_w\n        b1 = b0 - ETA * noisy_grad_b\n        \n        # --- Step 3: Model Inversion Attack and Leakage Assessment ---\n        \n        # Attacker reconstructs input by maximizing the regularized logit\n        # J(x) = w1.T @ x + b1 - lambda * ||x||^2\n        # Setting dJ/dx = 0 -> w1 - 2 * lambda * x = 0 -> x = w1 / (2 * lambda)\n        if lambda_reg == 0:\n            raise ValueError(\"Regularization parameter lambda must be positive.\")\n        x_hat = w1 / (2 * lambda_reg)\n        \n        # Calculate leakage metric R\n        x_hat_0_abs = np.abs(x_hat[0])\n        x_hat_rest_abs_mean = np.mean(np.abs(x_hat[1:]))\n\n        # Handle potential division by zero if d=1, though problem states d=4\n        if x_hat_rest_abs_mean > 1e-12:\n            R = x_hat_0_abs / x_hat_rest_abs_mean\n        else: # If non-sensitive components are all zero, leakage is maximal\n            R = np.inf\n            \n        # Declare leakage if R exceeds the threshold tau\n        leakage_detected = R > tau\n        results.append(leakage_detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3165689"}, {"introduction": "一个理论上安全的差分隐私机制，如果实现不当，也可能存在隐私漏洞。因此，对隐私实现进行审计至关重要。本练习将教您如何设计并实现一个量化审计流程，通过模拟成员推断攻击来估算经验隐私损失，并将其与理论隐私预算 $\\epsilon$ 进行比较，从而检测出实现中可能存在的偏差。[@problem_id:3165736]", "problem": "你被派去在一个程式化的成员推断设定中，对一个差分隐私机制进行量化审计。该审计应将经验攻击成功率与仅由隐私审计员的参数 $ \\epsilon $ 所隐含的理论上合理的上界进行比较，并判断是否存在可检测的差异，该差异表明存在潜在的实现错误。\n\n从以下基本原理开始：\n\n- 一个随机化机制 $ M $ 满足 $(\\epsilon, 0)$-差分隐私（DP），如果对于所有相邻数据集 $ D $ 和 $ D' $（相差一个个体）以及所有可测事件 $ S $，都有 $ \\mathbb{P}(M(D) \\in S) \\leq e^{\\epsilon} \\, \\mathbb{P}(M(D') \\in S) $。\n- 在等先验概率下，最优成员推断攻击者是最小化误差的贝叶斯最优检验，该检验由似然比检验给出，其成功概率为 $ \\frac{1 + \\mathrm{TV}(P, Q)}{2} $，其中 $ \\mathrm{TV}(P, Q) $ 是两个输出分布 $ P $ 和 $ Q $ 之间的全变差距离。\n- 向全局敏感度为 $ 1 $ 的查询添加噪声 $ \\eta \\sim \\mathrm{Laplace}(0, b) $ 的拉普拉斯机制，在 $ \\epsilon = \\frac{1}{b} $ 时实现 $(\\epsilon, 0)$-DP。\n\n你的程序必须实现以下审计程序：\n\n1. 对于一个固定的整数计数 $ c_{\\mathrm{in}} $ 及其相邻计数 $ c_{\\mathrm{out}} = c_{\\mathrm{in}} - 1 $，定义一个发布 $ Y = c + \\eta $ 的机制，其中如果个体在数据集中，则 $ c $ 等于 $ c_{\\mathrm{in}} $，否则等于 $ c_{\\mathrm{out}} $。噪声 $ \\eta $ 从拉普拉斯分布中抽取，其尺度为 $ b = \\frac{\\mathrm{factor}}{\\epsilon} $，其中 $ \\epsilon $ 是隐私审计员的参数，而 $ \\mathrm{factor} $ 是一个乘法实现因子。正确的实现应有 $ \\mathrm{factor} = 1 $，而偏差则模拟了潜在的实现错误。\n\n2. 为了估计经验攻击成功率，模拟 $ N $ 次独立的试验，成员身份的先验概率相等，为 $ \\frac{1}{2} $。在每次试验中，抽取一个概率为 $ \\frac{1}{2} $ 的伯努利变量，以决定输出是来自“in”分布 $ \\mathrm{Laplace}(c_{\\mathrm{in}}, b) $ 还是“out”分布 $ \\mathrm{Laplace}(c_{\\mathrm{out}}, b) $。使用专门针对这对分布的贝叶斯最优决策规则：如果 $ Y \\geq \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} $，则判定为“in”，否则判定为“out”。经验成功率是在 $ N $ 次试验中正确决策的比例。\n\n3. 从 $(\\epsilon, 0)$-DP 的定义中，推导出一个关于等先验概率下贝叶斯最优成功概率的严格上界，该上界仅是 $ \\epsilon $ 的函数，不参考机制的内部结构。使用此上界作为“审计员预测”的上限。\n\n4. 为了避免因抽样变异性导致的假阳性，使用霍夫丁不等式为伯努利试验计算一个非渐近容差。对于用户指定的置信参数 $ \\alpha \\in (0, 1) $，经验成功率 $ \\hat{p} $ 与真实成功概率 $ p $ 的偏差最多为 $$ \\tau = \\sqrt{\\frac{\\ln\\left(\\frac{2}{\\alpha}\\right)}{2 N}} $$ 的概率至少为 $ 1 - \\alpha $。如果 $ \\hat{p} - \\tau $ 严格超过审计员预测的上界，则标记为存在差异。\n\n你的程序必须为以下测试套件实现上述审计，每个测试用例指定为一个元组 $ (\\epsilon, \\mathrm{factor}, N, \\alpha, \\mathrm{seed}) $：\n\n- 测试 $ 1 $: $ (\\epsilon = 1.0, \\mathrm{factor} = 1.0, N = 100000, \\alpha = 10^{-6}, \\mathrm{seed} = 42) $。\n- 测试 $ 2 $: $ (\\epsilon = 1.0, \\mathrm{factor} = 0.5, N = 50000, \\alpha = 10^{-4}, \\mathrm{seed} = 7) $。\n- 测试 $ 3 $: $ (\\epsilon = 0.001, \\mathrm{factor} = 1.0, N = 200000, \\alpha = 10^{-6}, \\mathrm{seed} = 2023) $。\n- 测试 $ 4 $: $ (\\epsilon = 5.0, \\mathrm{factor} = 1.0, N = 50000, \\alpha = 10^{-6}, \\mathrm{seed} = 123456) $。\n- 测试 $ 5 $: $ (\\epsilon = 1.0, \\mathrm{factor} = 0.8, N = 500, \\alpha = 0.05, \\mathrm{seed} = 99) $。\n\n实现细节：\n\n- 使用固定的整数 $ c_{\\mathrm{in}} = 10 $ 并设置 $ c_{\\mathrm{out}} = 9 $。中点决策阈值为 $ \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} = 9.5 $。\n- 使用提供的随机种子以确保可复现性。\n\n最终输出规范：\n\n- 对于每个测试用例，输出一个布尔值，指示是否检测到差异。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $ [\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3,\\mathrm{result}_4,\\mathrm{result}_5] $，其中每个 $ \\mathrm{result}_i $ 是 $ \\mathrm{True} $ 或 $ \\mathrm{False} $。", "solution": "任务是实现对差分隐私机制的量化审计。该审计将经验测量的攻击成功率与从隐私参数 $\\epsilon$ 推导出的理论上界进行比较。如果经验率在统计上显著高于理论界限，则标记为存在差异，这表明可能存在实现错误。该过程包括三个主要步骤：推导理论界限、模拟攻击以获得经验成功率，以及应用统计检验来做出决策。\n\n### 1. 源于差分隐私的理论上界\n\n一个机制 $M$ 提供 $(\\epsilon, 0)$-差分隐私（DP），如果对于任意两个相邻数据集 $D$ 和 $D'$，以及任意事件 $S$，输出分布 $P=M(D)$ 和 $Q=M(D')$ 满足 $\\mathbb{P}(P \\in S) \\leq e^{\\epsilon} \\mathbb{P}(Q \\in S)$。\n\n问题指出，对于等先验概率，最优成员推断攻击者的成功概率为 $p_{succ} = \\frac{1 + \\mathrm{TV}(P, Q)}{2}$，其中 $\\mathrm{TV}(P, Q)$ 是输出分布之间的全变差距离。为了找到“审计员预测”的上界，我们必须在 $(\\epsilon, 0)$-DP 约束下，找到 $\\mathrm{TV}(P, Q)$ 的最大可能值，而无需考虑具体机制。\n\n差分隐私中的一个标准结果是，对于任何满足 $(\\epsilon, 0)$-DP 条件的两个分布 $P$ 和 $Q$，它们的全变差距离有界：\n$$\n\\mathrm{TV}(P, Q) \\leq \\frac{e^\\epsilon - 1}{e^\\epsilon + 1}\n$$\n这个界是紧的；存在一对满足 $(\\epsilon, 0)$-DP 的分布可以达到这个最大的全变差距离。\n\n将这个最大可能的全变差距离代入攻击者成功概率的公式，得到理论上界，我们将其表示为 $p_{bound}$：\n$$\np_{succ} \\leq \\frac{1}{2} \\left( 1 + \\frac{e^\\epsilon - 1}{e^\\epsilon + 1} \\right)\n$$\n简化此表达式可得出审计员预测的成功概率上界：\n$$\np_{bound} = \\frac{1}{2} \\left( \\frac{e^\\epsilon + 1 + e^\\epsilon - 1}{e^\\epsilon + 1} \\right) = \\frac{1}{2} \\left( \\frac{2e^\\epsilon}{e^\\epsilon + 1} \\right) = \\frac{e^\\epsilon}{e^\\epsilon + 1}\n$$\n此界限仅取决于所声称的隐私参数 $\\epsilon$。\n\n### 2. 经验攻击成功率模拟\n\n该审计模拟一个特定的成员推断攻击，以估计该机制的真实攻击成功概率。\n- 该机制从拉普拉斯分布中添加噪声，$Y = c + \\eta$，其中 $\\eta \\sim \\mathrm{Laplace}(0, b)$。\n- 尺度参数为 $b = \\frac{\\mathrm{factor}}{\\epsilon}$。对于全局敏感度为 $1$ 的查询的正确实现，我们应该有 $\\mathrm{factor}=1$。全局敏感度为 $\\Delta c = |c_{\\mathrm{in}} - c_{\\mathrm{out}}| = |10 - 9| = 1$。\n- 考虑两个分布：$P_{in}$ 对应于 $c=c_{\\mathrm{in}}=10$，因此 $Y \\sim \\mathrm{Laplace}(10, b)$；$P_{out}$ 对应于 $c=c_{\\mathrm{out}}=9$，因此 $Y \\sim \\mathrm{Laplace}(9, b)$。\n- 用于区分这两个具有相等先验概率的对称分布的贝叶斯最优决策规则是比较似然，这可以简化为检查输出 $Y$ 更接近哪个均值。决策边界是中点 $T = \\frac{c_{\\mathrm{in}} + c_{\\mathrm{out}}}{2} = 9.5$。规则是：如果 $Y \\geq 9.5$，则判定为“in”，否则判定为“out”。\n\n模拟运行 $N$ 次试验。在每次试验中：\n1. 以 $\\frac{1}{2}$ 的概率选择一个真实状态（“in”或“out”）。\n2. 从相应的分布 $\\mathrm{Laplace}(c_{\\mathrm{in}}, b)$ 或 $\\mathrm{Laplace}(c_{\\mathrm{out}}, b)$ 生成一个带噪声的输出 $Y$。\n3. 将决策规则应用于 $Y$。\n4. 将决策与真实状态进行比较，以检查其是否正确。\n\n经验成功率 $\\hat{p}$ 是正确决策的总数除以试验总数 $N$。这个 $\\hat{p}$ 是对此特定机制上此特定攻击的真实成功概率 $p_{true}$ 的估计。\n\n为了完整性， $p_{true}$ 的解析值是从中点分类器在两个拉普拉斯分布上的成功概率推导出来的。正确决策的概率是 $p_{true} = \\frac{1}{2} \\mathbb{P}(\\text{正确} | \\text{in}) + \\frac{1}{2} \\mathbb{P}(\\text{正确} | \\text{out})$。根据对称性，这两个条件概率相等：$\\mathbb{P}(Y \\geq T | Y \\sim \\mathrm{Laplace}(c_{\\mathrm{in}}, b)) = \\mathbb{P}(Y  T | Y \\sim \\mathrm{Laplace}(c_{\\mathrm{out}}, b))$。使用拉普拉斯分布的累积分布函数（CDF），这个概率是 $1 - \\frac{1}{2} \\exp\\left(-\\frac{c_{\\mathrm{in}}-T}{b}\\right)$。这导致：\n$$\np_{true} = 1 - \\frac{1}{2} \\exp\\left(-\\frac{\\Delta c}{2b}\\right) = 1 - \\frac{1}{2} \\exp\\left(-\\frac{\\epsilon}{2 \\cdot \\mathrm{factor}}\\right)\n$$\n\n### 3. 用于差异检测的统计检验\n\n经验率 $\\hat{p}$ 是一个随机变量。为了解释抽样变异性并避免误报，我们使用霍夫丁不等式。对于具有真实概率 $p$ 的 $N$ 次伯努利试验，经验估计 $\\hat{p}$ 与 $p$ 的偏差以高概率最多为容差 $\\tau$：\n$$\n\\mathbb{P}(|\\hat{p} - p| \\geq \\tau) \\leq 2e^{-2N\\tau^2}\n$$\n将右侧设为小的置信参数 $\\alpha$，我们可以解出单边容差 $\\tau$：\n$$\n\\alpha = 2e^{-2N\\tau^2} \\implies \\frac{\\alpha}{2} = e^{-2N\\tau^2} \\implies \\ln\\left(\\frac{2}{\\alpha}\\right) = 2N\\tau^2 \\implies \\tau = \\sqrt{\\frac{\\ln\\left(\\frac{2}{\\alpha}\\right)}{2 N}}\n$$\n以至少 $1-\\alpha$ 的概率，真实成功率 $p_{true}$ 不超过 $\\hat{p}+\\tau$。如果实现有缺陷导致隐私性降低（例如 $\\mathrm{factor}  1$），$p_{true}$ 可能会超过审计员的界限 $p_{bound}$。如果我们有强有力的统计证据表明 $p_{true} > p_{bound}$，我们就标记为存在差异。如果我们的 $p_{true}$ 置信区间的下限，即 $\\hat{p} - \\tau$，严格大于理论最大值 $p_{bound}$，则可以确定存在差异。\n\n最终的审计条件是，当且仅当以下情况成立时，标记为存在差异：\n$$\n\\hat{p} - \\tau > p_{bound}\n$$\n\n### 审计算法摘要\n对于每个测试用例 $(\\epsilon, \\mathrm{factor}, N, \\alpha, \\mathrm{seed})$：\n1.  计算拉普拉斯尺度参数 $b = \\frac{\\mathrm{factor}}{\\epsilon}$。\n2.  使用给定的 `seed` 设置随机数生成器。\n3.  模拟 $N$ 次成员推断攻击试验，以计算经验成功率 $\\hat{p}$。\n4.  计算审计员预测的上界：$p_{bound} = \\frac{e^\\epsilon}{e^\\epsilon + 1}$。\n5.  计算统计容差：$\\tau = \\sqrt{\\frac{\\ln(2/\\alpha)}{2N}}$。\n6.  将 $\\hat{p} - \\tau$ 与 $p_{bound}$进行比较。如果 $\\hat{p} - \\tau > p_{bound}$，则检测到差异（True）。否则，未检测到差异（False）。\n7.  收集所有测试用例的布尔结果。", "answer": "```python\nimport numpy as np\n\ndef run_audit(epsilon, factor, N, alpha, seed):\n    \"\"\"\n    Performs a quantitative audit of a differentially private mechanism.\n\n    Args:\n        epsilon (float): The claimed privacy parameter epsilon.\n        factor (float): The implementation factor for the Laplace scale.\n        N (int): The number of simulation trials.\n        alpha (float): The confidence parameter for Hoeffding's inequality.\n        seed (int): The random seed for reproducibility.\n\n    Returns:\n        bool: True if a discrepancy is detected, False otherwise.\n    \"\"\"\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # --- Step 1: Mechanism and Attack Setup ---\n    c_in = 10.0\n    c_out = 9.0\n    decision_threshold = (c_in + c_out) / 2.0\n    \n    # The global sensitivity of the count query is |c_in - c_out| = 1.\n    # The Laplace scale b should be GS/epsilon = 1/epsilon for a correct implementation.\n    # A 'factor' is introduced to model implementation errors.\n    if epsilon == 0:\n        # Avoid division by zero, although not in test cases.\n        # Infinite noise means output is always 0, attack success is 0.5.\n        b = float('inf')\n    else:\n        b = factor / epsilon\n\n    # --- Step 2: Estimate Empirical Attack Success Rate ---\n    correct_decisions = 0\n    \n    # Generate all random choices at once for efficiency\n    true_labels_are_in = rng.random(size=N)  0.5\n    \n    # Generate Laplace noise for both 'in' and 'out' cases\n    # Note: np.random.laplace takes scale parameter b\n    laplace_noise = rng.laplace(loc=0.0, scale=b, size=N)\n\n    # Calculate noisy outputs\n    outputs_in = c_in + laplace_noise\n    outputs_out = c_out + laplace_noise\n    \n    # Combine outputs based on true labels\n    Y = np.where(true_labels_are_in, outputs_in, outputs_out)\n    \n    # Apply the Bayes-optimal decision rule\n    decisions_are_in = Y >= decision_threshold\n    \n    # Count correct decisions\n    correct_decisions = np.sum(decisions_are_in == true_labels_are_in)\n\n    # Empirical success rate\n    p_hat = correct_decisions / N\n\n    # --- Step 3: Theoretical Bound ---\n    # The upper bound on success probability is derived from the TV-distance bound of (eps, 0)-DP\n    # p_bound = (1 + (e^eps - 1)/(e^eps + 1)) / 2 = e^eps / (e^eps + 1)\n    p_bound = np.exp(epsilon) / (np.exp(epsilon) + 1.0)\n\n    # --- Step 4: Statistical Test ---\n    # Calculate the tolerance tau using Hoeffding's inequality\n    tau = np.sqrt(np.log(2.0 / alpha) / (2.0 * N))\n    \n    # A discrepancy is flagged if the empirical rate, adjusted for sampling error,\n    # is strictly greater than the theoretical bound.\n    discrepancy_detected = (p_hat - tau) > p_bound\n\n    return discrepancy_detected\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the audit for each test case.\n    \"\"\"\n    test_cases = [\n        # (epsilon, factor, N, alpha, seed)\n        (1.0, 1.0, 100000, 1e-6, 42),\n        (1.0, 0.5, 50000, 1e-4, 7),\n        (0.001, 1.0, 200000, 1e-6, 2023),\n        (5.0, 1.0, 50000, 1e-6, 123456),\n        (1.0, 0.8, 500, 0.05, 99)\n    ]\n\n    results = []\n    for case in test_cases:\n        epsilon, factor, N, alpha, seed = case\n        result = run_audit(epsilon, factor, N, alpha, seed)\n        results.append(result)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3165736"}]}