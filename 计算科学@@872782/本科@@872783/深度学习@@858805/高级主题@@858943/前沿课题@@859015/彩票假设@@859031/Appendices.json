{"hands_on_practices": [{"introduction": "要亲手实践彩票假设，第一步是掌握其核心的“训练-剪枝-回卷”流程。这项练习将指导你在一个可控的线性回归环境中，从零开始实现这一流程。通过比较迭代式幅度剪枝（IMP）与单次剪枝，你将能深入理解为何寻找“中奖彩票”的迭代过程通常更为有效，并探索子网络结构与早期训练轨迹之间的深刻联系。", "problem": "要求您形式化并实现一种受控比较，比较对象是在相同目标稀疏度下的带回卷的迭代幅度剪枝（IMP）与单次剪枝，以检验重复回卷到固定的训练迭代步骤是否提供了一种可归因于优化轨迹对齐的独特优势。该实验必须在一个完全指定、纯数学的监督学习代理模型中进行，其中所有组件均被解析定义。\n\n考虑一个线性模型，其参数为 $w \\in \\mathbb{R}^d$，数据矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，目标向量为 $y \\in \\mathbb{R}^n$。目标函数是正则化最小二乘损失\n$$\nL(w) \\;=\\; \\frac{1}{2n}\\,\\lVert X w - y \\rVert_2^2 \\;+\\; \\frac{\\lambda}{2}\\,\\lVert w \\rVert_2^2,\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$n$ 和 $d$ 是给定的。训练通过全批量梯度下降进行，步长 $\\eta  0$ 固定：\n$$\nw_{t+1} \\;=\\; w_t \\;-\\; \\eta \\,\\nabla L(w_t), \\quad \\text{其中} \\quad \\nabla L(w) \\;=\\; \\frac{1}{n}\\,X^\\top\\!\\left(X w - y\\right) \\;+\\; \\lambda w.\n$$\n假设步长的选择需满足光滑凸函数上梯度下降的标准稳定性条件，即：\n$$\n0 \\;\\; \\eta \\;\\le\\; \\frac{1}{L_{\\max}}, \\quad \\text{其中} \\quad L_{\\max} \\;\\text{是}\\; \\frac{1}{n}X^\\top X + \\lambda I_d \\;\\text{的最大特征值}。\n$$\n\n定义一个二元掩码 $m \\in \\{0,1\\}^d$ 来指示哪些参数被保留（1）或被剪枝（0）。当使用掩码 $m$ 进行训练时，更新会被投影到被掩码的坐标上，以使被剪枝的坐标保持为零。具体来说，从一个初始化 $w_{\\text{init}}$ 开始，一步带掩码的梯度下降是：\n$$\nw^{+} \\;=\\; m \\odot \\bigl(w - \\eta \\,\\nabla L(w)\\bigr),\n$$\n其中 $\\odot$ 表示逐元素乘法，且在任何给定的带掩码训练阶段，$m$ 保持固定。\n\n在目标稀疏度 $s \\in [0,1)$ 下的幅度剪枝会保留绝对值最大的 $K$ 个坐标，并将其余坐标设为零，其中\n$$\nK \\;=\\; \\left\\lceil (1 - s)\\,d \\right\\rceil.\n$$\n单次剪枝通过从 $w_0$ 开始训练密集模型 $T$ 步得到 $w_T$，然后保留 $|w_T|$ 中最大的 $K$ 个坐标来计算一个掩码 $m_{\\text{SS}}$。然后，单次剪枝解从回卷点 $w_k$（从 $w_0$ 开始训练 $k$ 步后的密集参数）开始，进行 $T$ 步带掩码的训练，即在 $m_{\\text{SS}} \\odot w_k$ 处初始化。带回卷的迭代幅度剪枝（IMP）执行 $R$ 轮，轮次由 $r \\in \\{1,\\dots,R\\}$ 索引：在每一轮中，从 $m^{(r-1)} \\odot w_k$（其中 $m^{(0)} = \\mathbf{1}$）开始，进行 $T$ 步带掩码的训练得到 $w_T^{(r)}$，然后在仍未被剪枝的坐标中，剪掉 $|w_T^{(r)}|$ 中绝对值最小的额外坐标以更新 $m^{(r)}$，并重复此过程，直到最终掩码 $m_{\\text{IMP}} = m^{(R)}$ 恰好有 $K$ 个保留坐标。在最后一轮之后，从 $m_{\\text{IMP}} \\odot w_k$ 开始进行 $T$ 步带掩码的训练，以产生 IMP 解。\n\n将掩码 $m$ 在回卷时间 $k$ 的轨迹对齐分数定义为在 $w_k$ 处的梯度能量被保留坐标捕获的比例：\n$$\n\\mathcal{A}(m; k) \\;=\\; \\frac{\\left\\| m \\odot \\nabla L(w_k) \\right\\|_2^2}{\\left\\| \\nabla L(w_k) \\right\\|_2^2},\n$$\n约定如果 $\\left\\| \\nabla L(w_k) \\right\\|_2 = 0$，则 $\\mathcal{A}(m;k) = 1$。\n\n您的任务是实现这两种程序，并为每个测试用例报告一个包含四个实数的列表\n$$\n\\bigl[L_{\\text{IMP}},\\,L_{\\text{SS}},\\,\\mathcal{A}(m_{\\text{IMP}};k),\\,\\mathcal{A}(m_{\\text{SS}};k)\\bigr],\n$$\n其中 $L_{\\text{IMP}}$ 和 $L_{\\text{SS}}$ 分别是 IMP 解和单次剪枝解的最终损失，这些损失是在回卷点 $w_k$ 开始，使用各自的最终掩码进行最后的 $T$ 步训练后计算得出的，而 $\\mathcal{A}$ 是上面定义的对齐分数。\n\n实现要求：\n- 使用上面定义的确切的梯度下降更新。\n- 对每个测试用例，使用稳定性步长 $\\eta = 1 / L_{\\max}$，其中 $L_{\\max}$ 由提供的 $X$ 和 $\\lambda$ 计算得出。\n- 在 IMP 中，设置每轮的剪枝，使得最终保留的坐标数量恰好为 $K$。您可以每轮剪枝一个固定的比例，但必须校正舍入误差，以确保 $R$ 轮后恰好剩下 $K$ 个。在每一轮中，仅在仍未被剪枝的坐标中进行剪枝，选择那些在该轮 $T$ 步带掩码训练结束时绝对值最小的坐标。\n- 在单次剪枝中，从 $w_0$ 开始进行 $T$ 步密集训练结束时的幅值中选择 $m_{\\text{SS}}$，然后回卷到 $w_k$，应用掩码，并进行 $T$ 步带掩码的训练。\n\n测试套件：\n对于每个测试用例，都给定了 $X, y, w_0, \\lambda, s, k, T$ 和 $R$。所有数字都是实数且维度一致。矩阵和向量如下：\n\n- 测试用例 1：\n  - $n = 6$, $d = 8$,\n  $$\n  X = \\begin{bmatrix}\n  0.7  -0.2  0.3  0.0  0.5  -0.1  0.2  0.4 \\\\\n  -0.1  0.8  -0.3  0.6  0.0  0.2  -0.5  0.1 \\\\\n  0.4  -0.6  0.9  -0.2  0.1  0.3  0.0  -0.4 \\\\\n  0.3  0.1  0.2  0.7  -0.3  0.0  0.6  -0.2 \\\\\n  -0.5  0.4  -0.1  0.3  0.2  -0.7  0.1  0.0 \\\\\n  0.2  -0.3  0.5  -0.4  0.6  0.1  -0.2  0.3\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 0.9 \\\\ -0.2 \\\\ 0.5 \\\\ 0.7 \\\\ -0.6 \\\\ 0.1 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\\\ 0.01 \\\\ -0.04 \\\\ 0.02 \\\\ 0.0 \\\\ -0.01 \\end{bmatrix},\n  \\quad \\lambda = 0.1, \\quad s = 0.5, \\quad k = 5, \\quad T = 200, \\quad R = 4.\n  $$\n\n- 测试用例 2：\n  - $n = 8$, $d = 8$,\n  $$\n  X = I_8,\n  \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.5 \\\\ 0.8 \\\\ -1.2 \\\\ 0.3 \\\\ 0.0 \\\\ 0.7 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ -0.03 \\\\ 0.01 \\\\ 0.0 \\\\ 0.02 \\\\ -0.02 \\\\ 0.01 \\end{bmatrix},\n  \\quad \\lambda = 0.0, \\quad s = 0.1, \\quad k = 0, \\quad T = 100, \\quad R = 3.\n  $$\n\n- 测试用例 3：\n  - $n = 10$, $d = 8$,\n  $$\n  X = \\begin{bmatrix}\n  1.0  0.95  0.1  -0.1  0.0  0.2  -0.2  0.1 \\\\\n  0.9  0.85  -0.1  0.0  0.1  -0.1  0.2  -0.2 \\\\\n  1.1  1.05  0.0  0.1  -0.1  0.0  0.1  0.2 \\\\\n  0.8  0.75  0.2  -0.2  0.2  -0.2  0.0  -0.1 \\\\\n  1.2  1.15  -0.2  0.2  -0.2  0.1  -0.1  0.0 \\\\\n  1.0  0.95  0.1  -0.2  0.2  0.0  0.2  -0.2 \\\\\n  0.95  0.9  0.0  0.2  -0.1  0.1  -0.2  0.2 \\\\\n  1.05  1.0  -0.1  0.0  0.0  -0.1  0.1  -0.1 \\\\\n  0.85  0.8  0.2  -0.1  0.1  0.0  0.0  0.1 \\\\\n  1.15  1.1  -0.2  0.1  -0.1  0.1  -0.1  0.0\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 1.2 \\\\ 0.6 \\\\ 1.4 \\\\ 1.0 \\\\ 0.9 \\\\ 1.1 \\\\ 0.7 \\\\ 1.3 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ 0.01 \\\\ -0.02 \\\\ 0.03 \\\\ -0.01 \\\\ 0.0 \\end{bmatrix},\n  \\quad \\lambda = 0.05, \\quad s = 0.8, \\quad k = 10, \\quad T = 300, \\quad R = 5.\n  $$\n\n- 测试用例 4：\n  - $n = 5$, $d = 6$,\n  $$\n  X = \\begin{bmatrix}\n  0.6  -0.1  0.2  0.0  0.3  -0.2 \\\\\n  0.0  0.5  -0.3  0.4  -0.1  0.2 \\\\\n  0.2  -0.4  0.6  -0.2  0.0  0.1 \\\\\n  -0.3  0.2  0.1  0.5  -0.2  0.0 \\\\\n  0.1  0.0  -0.2  0.3  0.4  -0.1\n  \\end{bmatrix},\n  \\quad\n  y = \\begin{bmatrix} 0.4 \\\\ -0.1 \\\\ 0.5 \\\\ 0.3 \\\\ -0.2 \\end{bmatrix},\n  $$\n  $$\n  w_0 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ 0.01 \\\\ -0.02 \\\\ 0.0 \\\\ 0.03 \\end{bmatrix},\n  \\quad \\lambda = 0.15, \\quad s = 0.0, \\quad k = 7, \\quad T = 150, \\quad R = 4.\n  $$\n\n您的程序必须按 $1,2,3,4$ 的顺序为每个测试用例计算四元组 $\\bigl[L_{\\text{IMP}},\\,L_{\\text{SS}},\\,\\mathcal{A}(m_{\\text{IMP}};k),\\,\\mathcal{A}(m_{\\text{SS}};k)\\bigr]$，并生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个用例的结果本身是一个包含四个浮点数的列表。例如，最终输出的形式应为\n$$\n\\bigl[ [a_1,b_1,c_1,d_1], [a_2,b_2,c_2,d_2], [a_3,b_3,c_3,d_3], [a_4,b_4,c_4,d_4] \\bigr],\n$$\n不含任何额外文本。您必须以普通实数形式输出数值；此任务不涉及单位或百分比。如果您愿意，只要输出的是实数，就可以四舍五入到固定的小数位数。", "solution": "用户提供了一个定义明确的计算问题，要求在正则化线性回归的受控环境中，对两种神经网络剪枝策略——迭代幅度剪枝（IMP）和单次（SS）剪枝——进行形式化和比较。该问题具有科学依据、数学上精确且算法上明确，因此适合于一个完整的解决方案。\n\n问题的核心是模拟两种不同的剪枝流程，并从最终性能（损失）和一个提出的“轨迹对齐”指标两方面对它们进行评估。该指标假设剪枝掩码的成功与它在训练早期阶段（“回卷”点）保留梯度方向的好坏程度有关。\n\n该解决方案将组织为一系列函数，用以实现所需的数学和算法组件，然后将这些函数编排起来以处理每个测试用例。\n\n**1. 数学预备知识**\n\n损失函数是正则化最小二乘目标：\n$$\nL(w) = \\frac{1}{2n}\\,\\lVert X w - y \\rVert_2^2 + \\frac{\\lambda}{2}\\,\\lVert w \\rVert_2^2\n$$\n其梯度（梯度下降所需）为：\n$$\n\\nabla L(w) = \\frac{1}{n}\\,X^\\top(X w - y) + \\lambda w\n$$\n训练算法是使用固定步长 $\\eta$ 的梯度下降。为了在这种凸设定下保证稳定性和收敛性，步长必须小于或等于梯度利普希茨常数 $L_{\\text{max}}$ 的倒数。梯度的利普希茨常数是海森矩阵 $\\nabla^2 L(w) = \\frac{1}{n}X^\\top X + \\lambda I_d$ 的最大特征值。我们将使用上界 $\\eta = 1/L_{\\max}$。\n\n**2. 算法实现步骤**\n\n每个测试用例的总体流程包含以下几个阶段：\n\n**阶段 I：密集预训练和设置**\n首先，我们为两种剪枝方法设置通用参数。\n- 计算要保留的参数数量，$K = \\lceil (1-s)d \\rceil$，其中 $s$ 是目标稀疏度，$d$ 是模型维度。\n- 计算海森矩阵 $H = \\frac{1}{n}X^\\top X + \\lambda I_d$。\n- 计算 $H$ 的最大特征值 $L_{\\max}$，以确定稳定的步长 $\\eta = 1/L_{\\max}$。\n- 从初始权重 $w_0$ 开始，执行一个完整的密集训练过程，共 $T$ 步。我们必须存储权重向量的整个轨迹 $\\{w_0, w_1, \\dots, w_T\\}$，以便访问回卷权重 $w_k$ 和用于生成单次剪枝掩码的权重 $w_T$。\n- 计算回卷点处的梯度 $\\nabla L(w_k)$，这对于之后计算轨迹对齐分数至关重要。\n\n**阶段 II：单次（SS）剪枝**\n单次剪枝的程序如下：\n1.  **掩码生成**：掩码 $m_{\\text{SS}}$ 由 $T$ 步后的密集权重 $w_T$ 确定。我们识别出 $w_T$ 中对应最大绝对值的 $K$ 个位置，并将其对应的掩码条目设为 1，其余所有条目设为 0。\n2.  **回卷和训练**：模型被回卷到步骤 $k$。权重初始化为 $w_{\\text{SS, init}} = m_{\\text{SS}} \\odot w_k$。使用带掩码的梯度下降执行一个为期 $T$ 步的最终训练阶段：$w^{+} = m_{\\text{SS}} \\odot (w - \\eta \\nabla L(w))$。\n3.  **评估**：使用最终权重 $w_{\\text{SS, final}}$ 计算最终损失 $L_{\\text{SS}}$。对齐分数 $\\mathcal{A}(m_{\\text{SS}}; k)$ 计算为 $\\frac{\\|m_{\\text{SS}} \\odot \\nabla L(w_k)\\|_2^2}{\\|\\nabla L(w_k)\\|_2^2}$。\n\n**阶段 III：迭代幅度剪枝（IMP）**\nIMP 涉及一个多轮剪枝过程：\n1.  **剪枝计划**：我们定义一个计划，在 $R$ 轮内剪枝权重以达到最终目标数量 $K$。一个线性的计划是鲁棒且易于实现的：在每一轮 $r \\in \\{1, \\dots, R\\}$ 中，我们将目标剪枝权重总数计算为 $\\text{round}(r \\cdot (d-K)/R)$（在第 $R$ 轮进行最终校正，以确保总共恰好剪枝 $d-K$ 个）。这决定了在当前轮次需要额外剪枝多少权重。\n2.  **迭代剪枝和训练**：对于每一轮 $r=1, \\dots, R$：\n    a. 模型被回卷到 $w_k$ 并使用当前掩码 $m^{(r-1)}$ 进行掩蔽，以获得起始权重 $w_{\\text{start}}^{(r)} = m^{(r-1)} \\odot w_k$。\n    b. 使用带有 $m^{(r-1)}$ 的带掩码梯度下降法对模型进行 $T$ 步训练，以获得权重 $w_T^{(r)}$。\n    c. 通过从当前活动集合（其中 $m^{(r-1)}=1$）中剪枝掉在 $w_T^{(r)}$ 中具有最小绝对值的计划数量的权重，来生成一个新的掩码 $m^{(r)}$。\n3.  **最终训练**：经过 $R$ 轮后，获得最终掩码 $m_{\\text{IMP}} = m^{(R)}$。模型再次回卷到 $w_k$，初始化为 $w_{\\text{IMP, init}} = m_{\\text{IMP}} \\odot w_k$，并进行最后的 $T$ 步训练。\n4.  **评估**：计算最终损失 $L_{\\text{IMP}}$。对齐分数 $\\mathcal{A}(m_{\\text{IMP}}; k)$ 的计算方式与 SS 情况类似，使用最终掩码 $m_{\\text{IMP}}$。\n\n**阶段 IV：报告**\n对于每个测试用例，收集四个计算出的值 $[L_{\\text{IMP}}, L_{\\text{SS}}, \\mathcal{A}(m_{\\text{IMP}};k), \\mathcal{A}(m_{\\text{SS}};k)]$。最终输出将这些四元组聚合成一个列表的列表。一个特殊情况逻辑处理 $\\|\\nabla L(w_k)\\|_2 = 0$ 的情况，将对齐分数设置为 1。\n\n这种结构化的方法确保了两种剪枝策略都完全按照规定实现，并且比较是公平和受控的。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It orchestrates the setup, simulation, and result collection.\n    \"\"\"\n    \n    # Define helper functions for loss, gradient, and training\n    def get_loss(w, X, y, lambda_reg):\n        n = X.shape[0]\n        if n == 0:\n            return 0.5 * lambda_reg * np.dot(w, w)\n        residual = X @ w - y\n        loss = (0.5 / n) * np.dot(residual, residual) + 0.5 * lambda_reg * np.dot(w, w)\n        return loss\n\n    def get_grad(w, X, y, lambda_reg):\n        n = X.shape[0]\n        if n == 0:\n            return lambda_reg * w\n        grad = (1.0 / n) * X.T @ (X @ w - y) + lambda_reg * w\n        return grad\n\n    def train_masked_gd(w_init, mask, T, eta, X, y, lambda_reg):\n        w = np.copy(w_init)\n        for _ in range(T):\n            grad = get_grad(w, X, y, lambda_reg)\n            w = mask * (w - eta * grad)\n        return w\n\n    def get_alignment_score(mask, grad_at_wk):\n        norm_grad_sq = np.dot(grad_at_wk, grad_at_wk)\n        if norm_grad_sq  1e-12: # Handle case where gradient is zero\n            return 1.0\n        masked_grad_norm_sq = np.linalg.norm(mask * grad_at_wk)**2\n        return masked_grad_norm_sq / norm_grad_sq\n\n    # Define test cases as specified in the problem\n    test_cases = [\n        # Test case 1\n        {\n            \"X\": np.array([\n                [0.7, -0.2, 0.3, 0.0, 0.5, -0.1, 0.2, 0.4], [-0.1, 0.8, -0.3, 0.6, 0.0, 0.2, -0.5, 0.1],\n                [0.4, -0.6, 0.9, -0.2, 0.1, 0.3, 0.0, -0.4], [0.3, 0.1, 0.2, 0.7, -0.3, 0.0, 0.6, -0.2],\n                [-0.5, 0.4, -0.1, 0.3, 0.2, -0.7, 0.1, 0.0], [0.2, -0.3, 0.5, -0.4, 0.6, 0.1, -0.2, 0.3]\n            ]),\n            \"y\": np.array([0.9, -0.2, 0.5, 0.7, -0.6, 0.1]),\n            \"w0\": np.array([0.05, -0.02, 0.03, 0.01, -0.04, 0.02, 0.0, -0.01]),\n            \"lambda_reg\": 0.1, \"s\": 0.5, \"k\": 5, \"T\": 200, \"R\": 4\n        },\n        # Test case 2\n        {\n            \"X\": np.identity(8),\n            \"y\": np.array([1.0, 0.5, -0.5, 0.8, -1.2, 0.3, 0.0, 0.7]),\n            \"w0\": np.array([-0.01, 0.02, -0.03, 0.01, 0.0, 0.02, -0.02, 0.01]),\n            \"lambda_reg\": 0.0, \"s\": 0.1, \"k\": 0, \"T\": 100, \"R\": 3\n        },\n        # Test case 3\n        {\n            \"X\": np.array([\n                [1.0, 0.95, 0.1, -0.1, 0.0, 0.2, -0.2, 0.1], [0.9, 0.85, -0.1, 0.0, 0.1, -0.1, 0.2, -0.2],\n                [1.1, 1.05, 0.0, 0.1, -0.1, 0.0, 0.1, 0.2], [0.8, 0.75, 0.2, -0.2, 0.2, -0.2, 0.0, -0.1],\n                [1.2, 1.15, -0.2, 0.2, -0.2, 0.1, -0.1, 0.0], [1.0, 0.95, 0.1, -0.2, 0.2, 0.0, 0.2, -0.2],\n                [0.95, 0.9, 0.0, 0.2, -0.1, 0.1, -0.2, 0.2], [1.05, 1.0, -0.1, 0.0, 0.0, -0.1, 0.1, -0.1],\n                [0.85, 0.8, 0.2, -0.1, 0.1, 0.0, 0.0, 0.1], [1.15, 1.1, -0.2, 0.1, -0.1, 0.1, -0.1, 0.0]\n            ]),\n            \"y\": np.array([1.0, 0.8, 1.2, 0.6, 1.4, 1.0, 0.9, 1.1, 0.7, 1.3]),\n            \"w0\": np.array([0.02, -0.01, 0.0, 0.01, -0.02, 0.03, -0.01, 0.0]),\n            \"lambda_reg\": 0.05, \"s\": 0.8, \"k\": 10, \"T\": 300, \"R\": 5\n        },\n        # Test case 4\n        {\n            \"X\": np.array([\n                [0.6, -0.1, 0.2, 0.0, 0.3, -0.2], [0.0, 0.5, -0.3, 0.4, -0.1, 0.2],\n                [0.2, -0.4, 0.6, -0.2, 0.0, 0.1], [-0.3, 0.2, 0.1, 0.5, -0.2, 0.0],\n                [0.1, 0.0, -0.2, 0.3, 0.4, -0.1]\n            ]),\n            \"y\": np.array([0.4, -0.1, 0.5, 0.3, -0.2]),\n            \"w0\": np.array([-0.01, 0.02, 0.01, -0.02, 0.0, 0.03]),\n            \"lambda_reg\": 0.15, \"s\": 0.0, \"k\": 7, \"T\": 150, \"R\": 4\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        X, y, w0, lambda_reg, s, k, T, R = case.values()\n        n, d = X.shape\n        \n        # --- Stage I: Setup and Dense Pre-training ---\n        K = math.ceil((1 - s) * d)\n        \n        H = (1.0 / n) * (X.T @ X) + lambda_reg * np.eye(d)\n        L_max = np.max(np.linalg.eigvalsh(H))\n        eta = 1.0 / L_max\n        \n        w_dense_traj = [np.copy(w0)]\n        w = np.copy(w0)\n        for _ in range(T):\n            grad = get_grad(w, X, y, lambda_reg)\n            w -= eta * grad\n            w_dense_traj.append(np.copy(w))\n            \n        w_k = w_dense_traj[k]\n        w_T_dense = w_dense_traj[T]\n        grad_at_wk = get_grad(w_k, X, y, lambda_reg)\n\n        # --- Stage II: Single-Shot Pruning (SS) ---\n        m_ss = np.zeros(d, dtype=float)\n        if K > 0:\n            indices_to_keep = np.argpartition(np.abs(w_T_dense), -K)[-K:]\n            m_ss[indices_to_keep] = 1.0\n\n        w_ss_init = m_ss * w_k\n        w_ss_final = train_masked_gd(w_ss_init, m_ss, T, eta, X, y, lambda_reg)\n        L_ss = get_loss(w_ss_final, X, y, lambda_reg)\n        A_ss = get_alignment_score(m_ss, grad_at_wk)\n\n        # --- Stage III: Iterative Magnitude Pruning (IMP) ---\n        m_imp = np.ones(d, dtype=float)\n        pruned_count = 0\n        \n        for r in range(1, R + 1):\n            w_init_round = m_imp * w_k\n            w_trained_round = train_masked_gd(w_init_round, m_imp, T, eta, X, y, lambda_reg)\n            \n            if r  R:\n                target_pruned_total = round(r * (d - K) / R)\n            else: # Final round\n                target_pruned_total = d - K\n\n            num_to_prune_now = int(target_pruned_total - pruned_count)\n            \n            if num_to_prune_now > 0:\n                active_indices = np.where(m_imp == 1.0)[0]\n                active_weights_abs = np.abs(w_trained_round[active_indices])\n                \n                if num_to_prune_now >= len(active_indices):\n                    indices_to_prune_global = active_indices\n                else:\n                    prune_indices_local = np.argpartition(active_weights_abs, num_to_prune_now)[:num_to_prune_now]\n                    indices_to_prune_global = active_indices[prune_indices_local]\n\n                m_imp[indices_to_prune_global] = 0.0\n\n            pruned_count = d - np.sum(m_imp)\n\n        w_imp_init = m_imp * w_k\n        w_imp_final = train_masked_gd(w_imp_init, m_imp, T, eta, X, y, lambda_reg)\n        L_imp = get_loss(w_imp_final, X, y, lambda_reg)\n        A_imp = get_alignment_score(m_imp, grad_at_wk)\n        \n        all_results.append([L_imp, L_ss, A_imp, A_ss])\n\n    print(all_results)\n\nsolve()\n```", "id": "3188076"}, {"introduction": "现实世界中的神经网络，如卷积网络（CNN）和Transformer，其参数并非杂乱无章，而是以滤波器、注意力头等结构化形式存在。[@problem_id:3188072] 在这项练习中，你将探索结构化剪枝（移除整个通道或头）与非结构化剪枝（移除单个权重）之间的差异。通过在简化的类CNN和类Transformer模型上进行实验，你将亲身体会到两者在寻找有效子网络时的不同表现，并理解在硬件效率和模型性能之间的权衡。", "problem": "您的任务是通过编程方式测试彩票假说（LTH），具体方法是在两个简化的网络族上，以相等的稀疏度比较结构化剪枝和非结构化剪枝。这两个网络族分别捕捉了卷积神经网络（CNN）和Transformer的分组逻辑。目标是确定在每种情况下，是否存在一个通过幅度剪枝和回卷到原始初始化获得的稀疏子网络（即“彩票”），该子网络在使用相同的优化器从相同的初始化状态开始训练时，其验证准确率能在指定容差范围内匹配密集模型的验证准确率。\n\n从以下机器学习基本概念开始：\n\n- 经验风险最小化：给定标记数据 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N$，通过调整参数 $\\theta$ 来最小化经验风险 $\\frac{1}{N}\\sum_{i=1}^N \\ell(f_\\theta(\\mathbf{x}_i), y_i)$。\n- 梯度下降：学习率为 $\\eta$，参数通过 $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathbb{E}[\\ell(f_\\theta(\\mathbf{x}), y)]$ 进行更新，该期望值由数据集上的经验均值近似。\n- 具有softmax输出的 $K=2$ 类的交叉熵：如果 logits 为 $\\mathbf{z} \\in \\mathbb{R}^2$，且 $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$，则对于独热（one-hot）标签 $\\mathbf{y}$ 的损失为 $\\ell(\\mathbf{z}, \\mathbf{y}) = -\\sum_{k=1}^2 y_k \\log p_k$。\n- ReLU激活函数：$\\mathrm{ReLU}(a) = \\max(0,a)$，按元素应用。\n- 稀疏度 $s \\in [0,1]$：剪枝后权重被设置为零的比例。\n- 幅度剪枝：通过对权重（或整组权重）的绝对值进行排序，并将最小的权重置零，直到达到目标稀疏度 $s$。\n- 回卷（Rewinding）：在根据训练好的密集模型的权重幅度进行剪枝后，将幸存的权重重置为其原始初始化值并重新训练。\n\n您将从基本原理出发，实现两种玩具架构和训练过程：\n\n- 类CNN分组线性网络：输入维度 $D=16$，通道数 $C=8$，类别数 $K=2$。\n  - 参数：一个带掩码的线性层，其权重 $W \\in \\mathbb{R}^{C \\times D}$ 受一个固定的连接掩码约束，该掩码为每个通道强制执行大小为 $D/C=2$ 的不相交感受野；然后是ReLU，最后是一个线性分类器 $V \\in \\mathbb{R}^{C \\times K}$。\n  - 对于一个批次 $X \\in \\mathbb{R}^{N \\times D}$ 的前向传播：$H = \\mathrm{ReLU}(X W^\\top)$，logits $Z = H V$，通过softmax得到概率，并计算交叉熵损失。\n  - 滤波器/通道级别的结构化剪枝：移除整个通道，即在 $W$ 中将对应的行（在其感受野段内）和在 $V$ 中对应的行置零。在相等的稀疏度 $s$下，选择要剪枝的通道数量，使得剪枝的权重总比例恰好等于 $s$。\n  - 非结构化剪枝：移除 $W$（仅在感受野段内）和 $V$ 中的单个权重，以达到精确的稀疏度 $s$。\n\n- 类Transformer分组线性网络：输入维度 $D=16$，头数 $H=4$，每头投影维度 $d_h=4$，类别数 $K=2$。\n  - 参数：每个头的投影 $P_h \\in \\mathbb{R}^{d_h \\times D}$（其中 $h \\in \\{1,\\dots,H\\}$），各头输出拼接成一个在 $\\mathbb{R}^{H \\cdot d_h}$ 中的向量，然后是一个线性分类器 $U \\in \\mathbb{R}^{H \\cdot d_h \\times K}$。在线性投影后，对每个头应用ReLU。\n  - 对于一个批次 $X \\in \\mathbb{R}^{N \\times D}$ 的前向传播：为每个头计算 $Z_h = \\mathrm{ReLU}(X P_h^\\top)$，拼接得到 $Z = [Z_1,\\dots,Z_H]$，然后计算 logits $L = Z U$，最后通过softmax计算概率和交叉熵损失。\n  - 头级别的结构化剪枝：移除整个头，即将所有的 $P_h$ 以及 $U$ 中对应的行段置零。在相等的稀疏度 $s$ 下，选择要剪枝的头数量，使得剪枝的权重总比例恰好等于 $s$。\n  - 非结构化剪枝：移除所有 $P_h$ 和 $U$ 中的单个权重，以达到精确的稀疏度 $s$。\n\n训练和评估协议必须实现以下步骤：\n\n1. 数据生成：对于每个测试用例，通过采样 $N_{\\text{train}}$ 个训练点和 $N_{\\text{val}}$ 个验证点，创建一个在 $\\mathbb{R}^{D}$ 中的平衡二分类数据集。令 $u \\in \\mathbb{R}^{D}$ 为一个随机单位向量，并定义类别均值 $\\mu_+ = +a u$ 和 $\\mu_- = -a u$，其中 $a  0$。对于类别 $+1$，从 $\\mathcal{N}(\\mu_+, \\sigma^2 I)$ 中采样 $\\mathbf{x}$；对于类别 $-1$，从 $\\mathcal{N}(\\mu_-, \\sigma^2 I)$ 中采样。使用独热标签，并将准确率计算为验证集上正确预测的比例。不使用物理单位；所有量均为无量纲。\n2. 初始化：用独立的高斯分布项初始化所有权重，并进行缩放以保持梯度稳定。对于类CNN的 $W$，在初始化时通过将感受野之外的条目置零来强制执行连接掩码。\n3. 密集模型训练：使用全批量梯度下降法、学习率 $\\eta$、交叉熵损失和指定的ReLU激活函数，对每个密集模型（类CNN和类Transformer）训练 $T$ 个周期。记录每个架构的密集模型验证准确率 $A_{\\text{dense}}$。\n4. 剪枝：\n   - 非结构化剪枝：在训练好的密集权重上，剪枝掉绝对值最小的权重，以在所有可训练权重上达到精确的稀疏度 $s$。生成一个幸存权重的二元掩码。\n   - 结构化剪枝：在训练好的密集权重上，为每个通道（类CNN）或头（类Transformer）计算一个组重要性得分，该得分是该组内所有权重绝对值的总和。剪枝掉得分最低的组，以达到精确的稀疏度 $s$。生成一个幸存组的二元掩码。\n5. 回卷与重新训练：对于每种剪枝方法和架构，将幸存的权重回卷到其原始初始化值，将被剪枝的权重设置为零，并使用相同的优化器和学习率重新训练 $T$ 个周期，同时强制应用掩码以确保被剪枝的权重保持为零。重新训练后计算验证准确率 $A_{\\text{ticket}}$。\n6. 彩票检验：对于每种情况，输出一个布尔值，指示是否满足 $A_{\\text{ticket}} \\geq A_{\\text{dense}} - \\varepsilon$，其中 $\\varepsilon$ 是测试用例中指定的容差。\n\n测试套件和要求输出：\n\n- 使用 $D=16$, $C=8$, $H=4$, $d_h=4$, $K=2$, $N_{\\text{train}}=256$, $N_{\\text{val}}=256$, $a=2$, $\\sigma=0.5$, $T=200$, 和学习率 $\\eta=0.05$。\n- 测试套件必须包含三个具有不同稀疏度 $s$ 和随机种子的案例，以测试不同的情况：\n  1. 案例1：seed $=7$, $s=0.0$, 容差 $\\varepsilon=0.005$。\n  2. 案例2：seed $=13$, $s=0.5$, 容差 $\\varepsilon=0.03$。\n  3. 案例3：seed $=42$, $s=0.75$, 容差 $\\varepsilon=0.05$。\n- 对于每个案例，按以下顺序计算四个布尔值：\n  - 类CNN结构化剪枝彩票存在，\n  - 类CNN非结构化剪枝彩票存在，\n  - 类Transformer结构化剪枝彩票存在，\n  - 类Transformer非结构化剪枝彩票存在。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个列表的列表，每个子列表对应一个测试用例，并按指定顺序包含四个布尔值，例如 $[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}]]$，其中每个 $b_{ij}$ 为 $\\mathrm{True}$ 或 $\\mathrm{False}$。\n\n您的程序必须是一个完整、可运行的程序，仅使用Python标准库以及执行环境中指定的NumPy和SciPy库，从头开始执行所有计算。不需要用户输入。程序必须仅以指定的精确格式打印单行输出。", "solution": "该问题要求对彩票假说（LTH）进行编程验证，方法是在两个简化的、具有组结构的神经网络架构上比较结构化剪枝和非结构化剪枝。LTH假设一个密集的、随机初始化的神经网络包含一个稀疏子网络（“中奖彩票”），当该子网络从相同的初始状态被单独训练时，其性能可以匹配原始密集网络的性能。我们将遵循“剪枝、回卷、重训”的方法论，实现从数据生成到模型训练和评估的整个实验流程。\n\n**1. 模型和学习的数学公式化**\n\n任务的核心是使用基于梯度的优化来最小化经验风险函数。给定一个数据集 $\\{\\left(\\mathbf{x}_i, y_i\\right)\\}_{i=1}^N$，我们寻求找到参数 $\\theta$ 来最小化平均损失：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\ell(f_{\\theta}(\\mathbf{x}_i), y_i)\n$$\n其中 $f_{\\theta}$ 是模型，$\\ell$ 是损失函数。我们将使用全批量梯度下降，其中每个周期的参数更新由下式给出：\n$$\n\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)\n$$\n其中 $\\eta$ 是学习率。\n\n损失函数是针对 $K=2$ 类的交叉熵。对于单个数据点 $(\\mathbf{x}, \\mathbf{y})$，其独热标签为 $\\mathbf{y} \\in \\{[1, 0]^\\top, [0, 1]^\\top\\}$，模型 logits 为 $\\mathbf{z} = f_{\\theta}(\\mathbf{x}) \\in \\mathbb{R}^2$，通过 softmax 函数计算概率 $\\mathbf{p} = \\mathrm{softmax}(\\mathbf{z})$，其中 $p_k = e^{z_k} / \\sum_{j=1}^2 e^{z_j}$。损失为：\n$$\n\\ell(\\mathbf{z}, \\mathbf{y}) = -\\sum_{k=1}^2 y_k \\log p_k\n$$\n在数值计算上，为了保持稳定性，它被计算为 $\\ell(\\mathbf{z}, \\mathbf{y}) = \\log\\left(\\sum_{k=1}^2 e^{z_k}\\right) - \\mathbf{y} \\cdot \\mathbf{z}$。对于一个包含 $N$ 个样本的批次，损失相对于 logits 的梯度是 $\\nabla_{\\mathbf{Z}} \\mathcal{L} = \\frac{1}{N}(\\mathbf{P} - \\mathbf{Y})$，其中 $\\mathbf{P}$ 和 $\\mathbf{Y}$ 分别是预测概率矩阵和独热标签矩阵。\n\n我们考虑两种使用 $\\mathrm{ReLU}(a) = \\max(0, a)$ 激活函数的架构。\n\n**1.1. 类CNN分组线性网络**\n该模型简化了卷积层的逐通道、局部感受野结构。\n- 参数：$\\theta_{CNN} = \\{W, V\\}$，其中 $W \\in \\mathbb{R}^{C \\times D}$ 和 $V \\in \\mathbb{R}^{C \\times K}$。\n- 维度：输入 $D=16$，通道 $C=8$，类别 $K=2$。\n- 结构约束：权重矩阵 $W$ 受一个固定的二元掩码 $M_W$ 约束，该掩码强制实现 $C$ 个大小为 $D/C=2$ 的不相交感受野。具体来说，$M_W[c, j] = 1$ 仅当 $2c \\le j  2(c+1)$，否则为 $0$。可训练的权重实际上是 $W \\odot M_W$，其中 $\\odot$ 是逐元素乘积。\n- 前向传播：对于输入批次 $X \\in \\mathbb{R}^{N \\times D}$，隐藏表示为 $H = \\mathrm{ReLU}(X (W \\odot M_W)^\\top)$，logits 为 $Z = H V$。\n- 梯度：使用链式法则，$\\nabla_V \\mathcal{L} = H^\\top (\\nabla_Z \\mathcal{L})$ 和 $\\nabla_W \\mathcal{L} = ((\\nabla_Z \\mathcal{L} V^\\top) \\odot \\mathbb{I}[X (W \\odot M_W)^\\top > 0])^\\top X \\odot M_W$，其中 $\\mathbb{I}[\\cdot]$ 是指示函数。\n\n**1.2. 类Transformer分组线性网络**\n该模型捕捉了注意力机制的多头结构，其中输入由多个“头”独立投影。\n- 参数：$\\theta_{Trans} = \\{P_1, \\dots, P_H, U\\}$，其中每个 $P_h \\in \\mathbb{R}^{d_h \\times D}$ 且 $U \\in \\mathbb{R}^{(H \\cdot d_h) \\times K}$。\n- 维度：输入 $D=16$，头数 $H=4$，每头维度 $d_h=4$，类别 $K=2$。\n- 前向传播：对于 $X \\in \\mathbb{R}^{N \\times D}$，每个头计算一个表示 $Z_h = \\mathrm{ReLU}(X P_h^\\top)$。这些表示被拼接成 $Z_{cat} = [Z_1 | Z_2 | \\dots | Z_H] \\in \\mathbb{R}^{N \\times (H \\cdot d_h)}$。最终的 logits 是 $L = Z_{cat} U$。\n- 梯度：$\\nabla_U \\mathcal{L} = Z_{cat}^\\top (\\nabla_L \\mathcal{L})$。相对于拼接后隐藏状态的梯度 $\\nabla_{Z_{cat}} \\mathcal{L} = \\nabla_L \\mathcal{L} U^\\top$ 被分割，以找到每个头参数的梯度：$\\nabla_{P_h} \\mathcal{L} = ((\\nabla_{Z_h} \\mathcal{L}) \\odot \\mathbb{I}[X P_h^\\top > 0])^\\top X$。\n\n**2. 数据生成**\n\n生成一个合成的、线性可分的二分类数据集。\n- 选择一个随机单位向量 $\\mathbf{u} \\in \\mathbb{R}^D$。\n- 两个类别的均值定义为 $\\boldsymbol{\\mu}_+ = a \\mathbf{u}$ 和 $\\boldsymbol{\\mu}_- = -a \\mathbf{u}$。\n- 训练和验证数据从各向同性高斯分布中采样：$\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_y, \\sigma^2 I)$，其中 $y \\in \\{+1, -1\\}$ 是类别标签。对于本实验，$N_{\\text{train}}=256$, $N_{\\text{val}}=256$, $a=2$, and $\\sigma=0.5$。\n\n**3. LTH实验协议**\n\n对于每个架构和测试用例（由随机种子、稀疏度 $s$ 和容差 $\\varepsilon$ 定义）：\n1.  **初始化**：所有可训练权重均通过从高斯分布 $\\mathcal{N}(0, \\sigma_{init}^2)$ 采样来初始化，其中 $\\sigma_{init}^2$ 根据层的扇入（fan-in）进行缩放，以确保梯度稳定。例如，对于权重矩阵为 $W_{\\text{layer}}$ 且扇入为 $d_{in}$ 的层，权重从 $\\mathcal{N}(0, 1/d_{in})$ 中抽取。这些初始参数 $\\theta_0$ 被存储起来。\n2.  **密集模型训练**：使用全批量梯度下降法，以学习率 $\\eta=0.05$ 训练完整（密集）模型 $T=200$ 个周期。得到的训练后参数为 $\\theta_T$。在留出集上计算验证准确率并存储为 $A_{\\text{dense}}$。\n3.  **剪枝**：使用训练好的权重 $\\theta_T$ 生成一个二元掩码 $M_{prune}$，以达到目标稀疏度水平 $s$。\n    - **非结构化剪枝**：收集所有参数张量中的所有单个权重。对其绝对幅度 $|\\theta_{T,i}|$ 进行排序。找到一个阈值，将低于此阈值的权重设为零，可以使所有权重中恰好有比例为 $s$ 的权重为零。掩码 $M_{prune}$ 对幸存的权重为 $1$，否则为 $0$。\n    - **结构化剪枝**：按通道（类CNN）或头（类Transformer）对权重进行分组。为每个组计算一个重要性得分，定义为该组内所有权重绝对幅度的总和。得分最低的组被完全剪枝，直到被剪枝的参数总比例恰好等于 $s$。掩码 $M_{prune}$ 将属于被剪枝组的所有权重置零。\n4.  **回卷与重新训练**：将幸存的权重重置为它们在 $\\theta_0$ 中的初始值。这创建了“中奖彩票”的初始化 $\\theta'_{0} = \\theta_0 \\odot M_{prune}$。然后，对这个稀疏子网络进行 $T=200$ 个周期的重新训练，每次梯度更新后都应用剪枝掩码 $M_{prune}$，以确保被剪枝的权重保持为零。\n5.  **评估**：计算重新训练后的子网络的验证准确率 $A_{\\text{ticket}}$。\n6.  **彩票检验**：如果其性能接近密集模型，即 $A_{\\text{ticket}} \\geq A_{\\text{dense}} - \\varepsilon$，则认为存在中奖彩票。此检验针对架构和剪枝方法的每种组合进行。\n\n该过程针对三个具有不同稀疏度 $s \\in \\{0.0, 0.5, 0.75\\}$ 的测试用例执行，并将结果编译为指定的输出格式。$s=0.0$ 的情况作为基线，此时不发生剪枝，因此 $A_{\\text{ticket}}$ 预计与 $A_{\\text{dense}}$ 相同，确保对于任何 $\\varepsilon > 0$，检验都会通过。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, sparsity_level, tolerance)\n        (7, 0.0, 0.005),\n        (13, 0.5, 0.03),\n        (42, 0.75, 0.05),\n    ]\n    \n    # Global parameters\n    D = 16  # Input dimension\n    K = 2   # Number of classes\n    N_TRAIN = 256\n    N_VAL = 256\n    A = 2.0\n    SIGMA = 0.5\n    T_EPOCHS = 200\n    ETA = 0.05\n    \n    # CNN-like parameters\n    C = 8\n    \n    # Transformer-like parameters\n    H = 4\n    D_H = 4\n\n    results = []\n    for seed, s, eps in test_cases:\n        case_results = []\n        \n        # Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # 1. Data Generation\n        u = np.random.randn(D)\n        u /= np.linalg.norm(u)\n        mu_plus = A * u\n        mu_minus = -A * u\n        \n        # Training data\n        x_train_plus = np.random.multivariate_normal(mu_plus, SIGMA**2 * np.eye(D), N_TRAIN // 2)\n        x_train_minus = np.random.multivariate_normal(mu_minus, SIGMA**2 * np.eye(D), N_TRAIN // 2)\n        x_train = np.vstack((x_train_plus, x_train_minus))\n        y_train_labels = np.array([1] * (N_TRAIN // 2) + [0] * (N_TRAIN // 2))\n        y_train = np.eye(K)[y_train_labels]\n\n        # Validation data\n        x_val_plus = np.random.multivariate_normal(mu_plus, SIGMA**2 * np.eye(D), N_VAL // 2)\n        x_val_minus = np.random.multivariate_normal(mu_minus, SIGMA**2 * np.eye(D), N_VAL // 2)\n        x_val = np.vstack((x_val_plus, x_val_minus))\n        y_val_labels = np.array([1] * (N_VAL // 2) + [0] * (N_VAL // 2))\n        y_val = np.eye(K)[y_val_labels]\n\n        # --- Helper functions ---\n        def accuracy(y_pred_logits, y_true_one_hot):\n            pred_labels = np.argmax(y_pred_logits, axis=1)\n            true_labels = np.argmax(y_true_one_hot, axis=1)\n            return np.mean(pred_labels == true_labels)\n\n        # --- CNN-like Architecture ---\n        def run_cnn_experiment():\n            # Init\n            w_init = np.random.randn(C, D) * np.sqrt(1.0 / D)\n            v_init = np.random.randn(C, K) * np.sqrt(1.0 / C)\n            \n            w_connectivity_mask = np.zeros_like(w_init)\n            receptive_field_size = D // C\n            for i in range(C):\n                w_connectivity_mask[i, i*receptive_field_size:(i+1)*receptive_field_size] = 1.0\n            \n            w_init *= w_connectivity_mask\n\n            def forward_cnn(x, w, v):\n                h = np.maximum(0, x @ w.T)\n                z = h @ v\n                return z, h\n\n            def gradients_cnn(x, y, w, v, w_mask):\n                n_samples = x.shape[0]\n                z, h = forward_cnn(x, w, v)\n                log_p = z - logsumexp(z, axis=1, keepdims=True)\n                \n                dz = (np.exp(log_p) - y) / n_samples\n                \n                dv = h.T @ dz\n                dh = dz @ v.T\n                dh[h = 0] = 0\n                dw = (dh.T @ x) * w_mask\n\n                return dw, dv\n\n            def train(init_params, masks, grad_fn, T, eta, x, y, w_mask=None):\n                w, v = [p.copy() for p in init_params]\n                w_prune_mask, v_prune_mask = masks\n                \n                for _ in range(T):\n                    dw, dv = grad_fn(x, y, w, v, w_mask if w_mask is not None else np.ones_like(w))\n                    w -= eta * dw\n                    v -= eta * dv\n                    w *= w_prune_mask\n                    v *= v_prune_mask\n                return w, v\n\n            # Dense model training\n            dense_masks = (np.ones_like(w_init), np.ones_like(v_init))\n            w_dense, v_dense = train((w_init, v_init), dense_masks, gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_dense, _ = forward_cnn(x_val, w_dense, v_dense)\n            acc_dense = accuracy(logits_dense, y_val)\n            \n            # Structured Pruning\n            if s > 0:\n                weights_per_channel = (D // C) + K\n                total_weights = C * weights_per_channel\n                num_ch_to_prune = int(round(s * total_weights / weights_per_channel))\n                \n                channel_scores = np.sum(np.abs(w_dense), axis=1) + np.sum(np.abs(v_dense), axis=1)\n                pruned_channels_indices = np.argsort(channel_scores)[:num_ch_to_prune]\n                \n                w_mask_struct = np.ones_like(w_init)\n                v_mask_struct = np.ones_like(v_init)\n                w_mask_struct[pruned_channels_indices, :] = 0\n                v_mask_struct[pruned_channels_indices, :] = 0\n            else:\n                w_mask_struct, v_mask_struct = dense_masks\n\n            w_ticket_s, v_ticket_s = train((w_init, v_init), (w_mask_struct * w_connectivity_mask, v_mask_struct), gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_ticket_s, _ = forward_cnn(x_val, w_ticket_s, v_ticket_s)\n            acc_ticket_s = accuracy(logits_ticket_s, y_val)\n            lth_check_struct = acc_ticket_s >= acc_dense - eps\n\n            # Unstructured Pruning\n            if s > 0:\n                all_weights = np.concatenate([w_dense[w_connectivity_mask == 1], v_dense.flatten()])\n                threshold = np.percentile(np.abs(all_weights), s * 100)\n                \n                w_mask_unstruct = w_connectivity_mask.copy()\n                w_mask_unstruct[w_connectivity_mask == 1] = (np.abs(w_dense[w_connectivity_mask == 1]) >= threshold).astype(float)\n                v_mask_unstruct = (np.abs(v_dense) >= threshold).astype(float)\n            else:\n                w_mask_unstruct, v_mask_unstruct = dense_masks\n\n            w_ticket_u, v_ticket_u = train((w_init, v_init), (w_mask_unstruct, v_mask_unstruct), gradients_cnn, T_EPOCHS, ETA, x_train, y_train, w_connectivity_mask)\n            logits_ticket_u, _ = forward_cnn(x_val, w_ticket_u, v_ticket_u)\n            acc_ticket_u = accuracy(logits_ticket_u, y_val)\n            lth_check_unstruct = acc_ticket_u >= acc_dense - eps\n            \n            return lth_check_struct, lth_check_unstruct\n\n        cnn_struct, cnn_unstruct = run_cnn_experiment()\n        case_results.extend([cnn_struct, cnn_unstruct])\n        \n        # --- Transformer-like Architecture ---\n        def run_transformer_experiment():\n            # Init\n            p_list_init = [np.random.randn(D_H, D) * np.sqrt(1.0 / D) for _ in range(H)]\n            u_init = np.random.randn(H * D_H, K) * np.sqrt(1.0 / (H * D_H))\n\n            def forward_transformer(x, p_list, u):\n                z_h_list = [np.maximum(0, x @ p.T) for p in p_list]\n                z_cat = np.concatenate(z_h_list, axis=1)\n                l = z_cat @ u\n                return l, z_h_list, z_cat\n\n            def gradients_transformer(x, y, p_list, u):\n                n_samples = x.shape[0]\n                l, z_h_list, z_cat = forward_transformer(x, p_list, u)\n                log_p = l - logsumexp(l, axis=1, keepdims=True)\n\n                dl = (np.exp(log_p) - y) / n_samples\n                \n                du = z_cat.T @ dl\n                dz_cat = dl @ u.T\n                \n                dp_list = []\n                for h in range(H):\n                    dz_h = dz_cat[:, h*D_H:(h+1)*D_H]\n                    dz_h[z_h_list[h] = 0] = 0\n                    dp_h = dz_h.T @ x\n                    dp_list.append(dp_h)\n                \n                return dp_list, du\n\n            def train_t(init_params, masks, T, eta, x, y):\n                p_list, u = [p.copy() for p in init_params[0]], init_params[1].copy()\n                p_masks, u_mask = masks\n                \n                for _ in range(T):\n                    dp_list, du = gradients_transformer(x, y, p_list, u)\n                    for h in range(H):\n                        p_list[h] -= eta * dp_list[h]\n                        p_list[h] *= p_masks[h]\n                    u -= eta * du\n                    u *= u_mask\n                return p_list, u\n            \n            # Dense model training\n            dense_p_masks = [np.ones_like(p) for p in p_list_init]\n            dense_u_mask = np.ones_like(u_init)\n            \n            p_list_dense, u_dense = train_t((p_list_init, u_init), (dense_p_masks, dense_u_mask), T_EPOCHS, ETA, x_train, y_train)\n            logits_dense_t, _, _ = forward_transformer(x_val, p_list_dense, u_dense)\n            acc_dense_t = accuracy(logits_dense_t, y_val)\n            \n            # Structured pruning\n            if s > 0:\n                weights_per_head = (D_H * D) + (D_H * K)\n                total_weights = H * weights_per_head\n                num_heads_to_prune = int(round(s * total_weights / weights_per_head))\n                \n                head_scores = [np.sum(np.abs(p_list_dense[h])) + np.sum(np.abs(u_dense[h*D_H:(h+1)*D_H, :])) for h in range(H)]\n                pruned_head_indices = np.argsort(head_scores)[:num_heads_to_prune]\n                \n                p_masks_struct = [np.ones_like(p) for p in p_list_init]\n                u_mask_struct = np.ones_like(u_init)\n                for h_idx in pruned_head_indices:\n                    p_masks_struct[h_idx] = 0.\n                    u_mask_struct[h_idx*D_H:(h_idx+1)*D_H, :] = 0.\n            else:\n                p_masks_struct, u_mask_struct = dense_p_masks, dense_u_mask\n            \n            p_list_ticket_s, u_ticket_s = train_t((p_list_init, u_init), (p_masks_struct, u_mask_struct), T_EPOCHS, ETA, x_train, y_train)\n            logits_ticket_s, _, _ = forward_transformer(x_val, p_list_ticket_s, u_ticket_s)\n            acc_ticket_s_t = accuracy(logits_ticket_s, y_val)\n            lth_check_struct_t = acc_ticket_s_t >= acc_dense_t - eps\n\n            # Unstructured pruning\n            if s > 0:\n                all_weights = np.concatenate([p.flatten() for p in p_list_dense] + [u_dense.flatten()])\n                threshold = np.percentile(np.abs(all_weights), s * 100)\n\n                p_masks_unstruct = [(np.abs(p) >= threshold).astype(float) for p in p_list_dense]\n                u_mask_unstruct = (np.abs(u_dense) >= threshold).astype(float)\n            else:\n                p_masks_unstruct, u_mask_unstruct = dense_p_masks, dense_u_mask\n\n            p_list_ticket_u, u_ticket_u = train_t((p_list_init, u_init), (p_masks_unstruct, u_mask_unstruct), T_EPOCHS, ETA, x_train, y_train)\n            logits_ticket_u, _, _ = forward_transformer(x_val, p_list_ticket_u, u_ticket_u)\n            acc_ticket_u_t = accuracy(logits_ticket_u, y_val)\n            lth_check_unstruct_t = acc_ticket_u_t >= acc_dense_t - eps\n            \n            return lth_check_struct_t, lth_check_unstruct_t\n\n        trans_struct, trans_unstruct = run_transformer_experiment()\n        case_results.extend([trans_struct, trans_unstruct])\n        \n        results.append(case_results)\n\n    # Convert boolean arrays to Python booleans for standard JSON-like output\n    final_results = [[bool(val) for val in row] for row in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"{final_results}\".replace(\"'\", \"\"))\n\nsolve()\n```", "id": "3188072"}, {"introduction": "传统的幅度剪枝是一种“硬性”方法，在模型训练后根据权重大小做出不可逆的离散决策。是否存在一种更平滑、更具适应性的“软性”方法来发现子网络？[@problem_id:3187989] 本练习将引导你探索使用可学习的“门控”变量来替代固定的二进制掩码。通过实现一个带有温度退火的门控机制，你将对比硬掩码与软掩码在相同稀疏度预算下的性能，从而理解如何通过梯度下降来动态地“生长”出一个中奖彩票。", "problem": "您被要求形式化并实现一个可复现的实验，该实验在一个线性模型上，基于经验风险最小化（ERM）和梯度下降的第一性原理，比较硬剪枝与回卷掩码和退火软门在彩票假说框架下的表现。该实验必须是完全确定性的，并在数学上进行明确规定，以使最终程序可被测试且与语言无关。\n\n基本原理。考虑一个线性模型，其参数为 $w \\in \\mathbb{R}^D$，数据为 $(X, y)$，其中 $X \\in \\mathbb{R}^{N \\times D}$，$y \\in \\mathbb{R}^N$。在均方误差下的经验风险为\n$$\nL_{\\text{data}}(w) \\;=\\; \\frac{1}{N}\\,\\|Xw - y\\|_2^2.\n$$\n数据损失关于 $w$ 的梯度为\n$$\n\\nabla_w L_{\\text{data}}(w) \\;=\\; \\frac{2}{N}\\,X^\\top(Xw - y).\n$$\n使用随机梯度下降（SGD）的全批量形式（即标准梯度下降），学习率为 $\\eta$ 的一步更新为\n$$\nw \\leftarrow w - \\eta \\,\\nabla_w L_{\\text{data}}(w).\n$$\n\n设置。您将使用一个完全指定的合成回归任务，其中\n- $D = N$ 且 $X = I_D$（$D \\times D$ 单位矩阵），因此 $X^\\top X = I_D$，数据损失简化为\n  $$\n  L_{\\text{data}}(w) \\;=\\; \\frac{1}{N}\\,\\|w - y\\|_2^2, \\qquad \\nabla_w L_{\\text{data}}(w) \\;=\\; \\frac{2}{N}\\,(w - y).\n  $$\n- 真实目标向量 $y \\in \\mathbb{R}^D$ 是 $K_{\\text{true}}$-稀疏的：其最后 $K_{\\text{true}}$ 个坐标等于一个常数 $c$，其余坐标等于 $0$。形式上，对于 $i \\in \\{0,1,\\dots,D-1\\}$，\n  $$\n  y_i \\;=\\;\n  \\begin{cases}\n  c,   i \\in \\{D-K_{\\text{true}},\\,\\dots,\\,D-1\\},\\\\\n  0,   \\text{其他情况}.\n  \\end{cases}\n  $$\n- 回卷初始化向量 $w^{(0)} \\in \\mathbb{R}^D$ 是确定性的，由下式给出\n  $$\n  w^{(0)}_i \\;=\\; (-1)^i \\,\\bigl(1 - 0.01\\,i\\bigr), \\qquad i \\in \\{0,1,\\dots,D-1\\}.\n  $$\n\n通过单步预训练构建掩码。为导出一个貌似次优的硬剪枝掩码，您将首先从 $w^{(0)}$ 开始，使用学习率 $\\eta_{\\text{pre}}$ 执行恰好一个全批量梯度下降步骤以获得 $w^{\\text{pre}}$：\n$$\nw^{\\text{pre}} \\;=\\; w^{(0)} \\;-\\; \\eta_{\\text{pre}} \\cdot \\frac{2}{N}\\,(w^{(0)} - y).\n$$\n令 $K_{\\text{keep}}$ 为要保留的连接数。定义二元掩码 $m \\in \\{0,1\\}^D$，对于 $\\lvert w^{\\text{pre}} \\rvert$ 中 $K_{\\text{keep}}$ 个最大值的索引，设置 $m_i = 1$（平局时按索引递增顺序打破），否则设置 $m_i = 0$。\n\n回卷后的两种训练方案。两种方案在训练前都回卷回 $w^{(0)}$。\n\n1) 硬彩票（二元掩码）训练。\n- 将有效权重参数化为 $w = m \\odot v$，其中 $v \\in \\mathbb{R}^D$ 是可训练的，$\\odot$ 是逐元素乘法。\n- 初始化 $v \\leftarrow w^{(0)}$ 并对 $L_{\\text{data}}(w)$ 执行 T 步梯度下降，学习率为 $\\eta$，使用链式法则 $\\nabla_v L_{\\text{data}} = m \\odot \\nabla_w L_{\\text{data}}$。\n\n2) 软彩票（退火门）训练。\n- 引入门的对数几率 $s \\in \\mathbb{R}^D$ 和门 $g \\in [0,1]^D$，通过一个温度控制的 sigmoid 函数定义\n  $$\n  g_i(t) \\;=\\; \\sigma\\!\\left(\\frac{s_i(t)}{\\tau(t)}\\right), \\qquad \\sigma(z) = \\frac{1}{1 + e^{-z}},\n  $$\n  其中温度 $\\tau(t)$ 采用线性退火策略 $\\tau(t) = \\tau_0 - (\\tau_0 - \\tau_{\\min})\\,\\frac{t}{T-1}$（对于 $t \\in \\{0,1,\\dots,T-1\\}$），如果 $T = 1$，则 $\\tau(t) = \\tau_0$。\n- 将 $w$ 参数化为 $w = g \\odot v$，其中 $v \\in \\mathbb{R}^D$ 可训练，初始化为 $v \\leftarrow w^{(0)}$。\n- 初始化对数几率以反映硬掩码：对于给定的 $s_{\\text{init}}  0$，设置\n  $$\n  s_i(0) \\;=\\; \n  \\begin{cases}\n  +s_{\\text{init}},   m_i = 1,\\\\\n  -s_{\\text{init}},   m_i = 0.\n  \\end{cases}\n  $$\n- 通过最小化增广目标进行训练\n  $$\n  L_{\\text{soft}}(v,s,t) \\;=\\; L_{\\text{data}}(g(t) \\odot v) \\;+\\; \\beta \\,\\bigl(\\tfrac{1}{D}\\,\\mathbf{1}^\\top g(t) \\;-\\; \\tfrac{K_{\\text{keep}}}{D}\\bigr)^2,\n  $$\n  其中 $\\beta \\ge 0$ 强制使平均门预算接近所需保留比例。使用全批量梯度下降，学习率为 $\\eta$，进行 T 步训练。根据链式法则，设 $r = w - y$ 且 $w = g \\odot v$，\n  $$\n  \\nabla_w L_{\\text{data}} = \\tfrac{2}{N}\\,r, \\quad\n  \\nabla_v L_{\\text{data}} = g \\odot \\nabla_w L_{\\text{data}}, \\quad\n  \\nabla_g L_{\\text{data}} = v \\odot \\nabla_w L_{\\text{data}}.\n  $$\n  预算惩罚项贡献\n  $$\n  \\nabla_g \\Bigl[\\beta \\bigl(\\tfrac{1}{D}\\,\\mathbf{1}^\\top g - \\tfrac{K_{\\text{keep}}}{D}\\bigr)^2\\Bigr] \\;=\\; \\tfrac{2\\beta}{D}\\,\\Bigl(\\tfrac{1}{D}\\,\\mathbf{1}^\\top g - \\tfrac{K_{\\text{keep}}}{D}\\Bigr)\\,\\mathbf{1}.\n  $$\n  结合并应用 sigmoid 链式法则可得\n  $$\n  \\nabla_v L_{\\text{soft}} \\;=\\; g \\odot \\nabla_w L_{\\text{data}}, \\qquad\n  \\nabla_s L_{\\text{soft}} \\;=\\; \\Bigl(\\nabla_g L_{\\text{data}} \\;+\\; \\tfrac{2\\beta}{D}\\,\\Bigl(\\tfrac{1}{D}\\,\\mathbf{1}^\\top g - \\tfrac{K_{\\text{keep}}}{D}\\Bigr)\\,\\mathbf{1} \\Bigr) \\odot g \\odot (1-g)\\,/\\,\\tau(t).\n  $$\n\n评估。对于每种方案，报告 T 步后的最终数据损失 $L_{\\text{data}}$（报告的损失中不包括预算惩罚项）。对于每个测试用例，输出标量差\n$$\n\\Delta \\;=\\; L_{\\text{data}}^{\\text{hard}} \\;-\\; L_{\\text{data}}^{\\text{soft}}.\n$$\n正的 $\\Delta$ 表示在相同预算下，退火软门获得了比硬彩票更低的经验风险。\n\n数据和参数。在所有测试用例中使用以下固定的问题数据：\n- $D = N = 20$, $X = I_{20}$, $K_{\\text{true}} = 5$, $c = 0.2$, $w^{(0)}_i = (-1)^i\\,(1 - 0.01\\,i)$。\n- 预训练：使用 $\\eta_{\\text{pre}} = 0.05$ 执行恰好 1 步以获得 $w^{\\text{pre}}$，然后通过保留 $w^{\\text{pre}}$ 的 $K_{\\text{keep}}$ 个最大绝对值来构建掩码 $m$。\n- 训练：按前述方式初始化 $v \\leftarrow w^{(0)}$ 和 $s$（使用 $s_{\\text{init}} = 4.0$），使用学习率 $\\eta$ 训练 T 步。\n\n测试套件。在以下测试用例上运行您的实现。每个用例是一个元组 $(K_{\\text{keep}}, \\beta, \\tau_0, \\tau_{\\min}, \\eta, T)$：\n- 用例 1: $(5,\\,50.0,\\,2.0,\\,0.2,\\,0.2,\\,120)$。\n- 用例 2: $(20,\\,50.0,\\,2.0,\\,0.2,\\,0.2,\\,120)$。\n- 用例 3: $(3,\\,50.0,\\,2.0,\\,0.2,\\,0.2,\\,120)$。\n\n要求的最终输出格式。您的程序应生成单行输出，其中包含三个 $\\Delta$ 值，每个值四舍五入到 6 位小数，格式为逗号分隔的列表并用方括号括起（例如，“[$x_1,x_2,x_3$]”，其中每个 $x_j$ 在小数点后恰好有 6 位数字）。不应打印任何其他文本。\n\n本问题不涉及角度单位和物理单位。\n\n科学真实性和约束条件。所有计算都已精确指定；不涉及随机性或外部数据。您的实现必须仅使用指定的批量梯度下降。退火策略和惩罚项确保软门可以在与硬掩码相当的情况下重新分配有效稀疏性预算。该实验是自洽的，旨在测试在受控条件下，退火门是否能比硬掩码找到更好的“软彩票”。", "solution": "用户提供了一个定义明确的计算问题，要求形式化并实现一个实验，以在彩票假说的框架内比较两种网络剪枝方法——硬掩码与退火软门。该问题科学上基于成熟的机器学习原理，在数学和算法上进行了详尽的规定，并且其表述是客观的。所有参数、初始条件和评估指标都已明确定义，使得问题具有确定性和可复现性。该设置为一个受控的合成实验，这是一种有效的科学方法。因此，该问题被认为是有效的。\n\n解决方案将通过遵循问题陈述中指定的精确计算序列来实现。\n\n首先，我们确定实验的常量参数和初始条件。\n问题的维度是 $D=20$。我们使用 $N=D=20$ 个数据点。特征矩阵是单位矩阵 $X = I_{20}$。\n真实目标向量 $y \\in \\mathbb{R}^{20}$ 是 $K_{\\text{true}}=5$ 稀疏的。其最后 $5$ 个坐标等于常数 $c=0.2$，其余为 $0$。\n$$\ny_i = \\begin{cases} 0.2,  15 \\le i \\le 19 \\\\ 0,  0 \\le i \\le 14 \\end{cases}\n$$\n初始权重向量 $w^{(0)} \\in \\mathbb{R}^{20}$ 根据公式 $w^{(0)}_i = (-1)^i (1 - 0.01i)$ 确定性地设置，其中 $i \\in \\{0, 1, \\dots, 19\\}$。\n\n其次，我们构建二元剪枝掩码 $m \\in \\{0,1\\}^{20}$。这需要一个“预训练”步骤。我们从 $w^{(0)}$ 开始，对数据损失 $L_{\\text{data}}(w) = \\frac{1}{N}\\|w - y\\|_2^2$ 执行一步梯度下降。梯度为 $\\nabla_w L_{\\text{data}}(w) = \\frac{2}{N}(w-y)$。预训练学习率为 $\\eta_{\\text{pre}} = 0.05$，预训练后的权重 $w^{\\text{pre}}$ 为：\n$$\nw^{\\text{pre}} = w^{(0)} - \\eta_{\\text{pre}} \\frac{2}{N}(w^{(0)} - y) = w^{(0)} - 0.05 \\cdot \\frac{2}{20}(w^{(0)} - y) = 0.995 w^{(0)} + 0.005 y\n$$\n然后，通过识别 $w^{\\text{pre}}$ 中绝对值最大的 $K_{\\text{keep}}$ 个条目的索引来定义掩码 $m$。平局时优先选择较小的索引。这通过基于键值 $(-\\lvert w^{\\text{pre}}_i \\rvert, i)$ 进行排序来实现。对于这前 $K_{\\text{keep}}$ 个索引，掩码条目 $m_i$ 设为 $1$，否则为 $0$。\n\n第三，我们从回卷状态开始，模拟两种训练方案进行 T 步。两种方案的可训练参数都基于 $w^{(0)}$进行初始化。\n\n1.  **硬彩票训练**：\n    有效权重为 $w = m \\odot v$，其中 $v$ 是可训练的参数向量，初始化为 $v \\leftarrow w^{(0)}$。使用学习率 $\\eta$ 进行 T 步梯度下降训练。$v$ 的更新规则源自链式法则：\n    $$\n    v \\leftarrow v - \\eta \\nabla_v L_{\\text{data}}(m \\odot v) = v - \\eta \\left(m \\odot \\nabla_w L_{\\text{data}}(m \\odot v)\\right)\n    $$\n    T 步之后，我们得到最终参数向量 $v_{\\text{final}}^{\\text{hard}}$。最终损失计算为 $L_{\\text{data}}^{\\text{hard}} = \\frac{1}{N}\\|m \\odot v_{\\text{final}}^{\\text{hard}} - y\\|_2^2$。\n\n2.  **软彩票训练**：\n    有效权重为 $w = g \\odot v$，其中 $v$ 是一个可训练的参数向量，$g$ 是可训练的门。门 $g_i(t) = \\sigma(s_i(t)/\\tau(t))$ 由可训练的对数几率 $s$ 和一个退火温度 $\\tau(t)$ 控制。\n    参数初始化为 $v \\leftarrow w^{(0)}$，并且如果 $m_i=1$，则 $s_i(0) = +s_{\\text{init}}$，如果 $m_i=0$，则 $s_i(0) = -s_{\\text{init}}$，其中 $s_{\\text{init}}=4.0$。\n    温度 $\\tau(t)$ 在 T-1 步内从 $\\tau_0$ 线性退火到 $\\tau_{\\min}$。\n    训练过程最小化增广损失 $L_{\\text{soft}} = L_{\\text{data}}(g \\odot v) + \\beta (\\frac{1}{D}\\mathbf{1}^\\top g - \\frac{K_{\\text{keep}}}{D})^2$。参数 $v$ 和 $s$ 使用学习率 $\\eta$ 通过梯度下降同时更新 T 步。梯度为：\n    $$\n    \\nabla_v L_{\\text{soft}} = g \\odot \\nabla_w L_{\\text{data}}(g \\odot v)\n    $$\n    $$\n    \\nabla_s L_{\\text{soft}} = \\left(v \\odot \\nabla_w L_{\\text{data}}(g \\odot v) + \\frac{2\\beta}{D}\\left(\\frac{1}{D}\\mathbf{1}^\\top g - \\frac{K_{\\text{keep}}}{D}\\right)\\mathbf{1} \\right) \\odot \\frac{g \\odot (1-g)}{\\tau(t)}\n    $$\n    T 步之后，我们得到最终参数 $v_{\\text{final}}^{\\text{soft}}$ 和 $s_{\\text{final}}$。最终损失仅为数据损失部分，使用最终参数和最小温度 $\\tau_{\\min}$ 进行评估：\n    $$\n    g_{\\text{final}, i} = \\sigma(s_{\\text{final}, i} / \\tau_{\\min})\n    $$\n    $$\n    L_{\\text{data}}^{\\text{soft}} = \\frac{1}{N}\\|g_{\\text{final}} \\odot v_{\\text{final}}^{\\text{soft}} - y\\|_2^2\n    $$\n\n最后，对于每个由元组 $(K_{\\text{keep}}, \\beta, \\tau_0, \\tau_{\\min}, \\eta, T)$ 定义的测试用例，我们计算差值 $\\Delta = L_{\\text{data}}^{\\text{hard}} - L_{\\text{data}}^{\\text{soft}}$ 并报告结果。该实现将为三个指定的测试用例执行此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_experiment(case_params):\n    \"\"\"\n    Runs a single experimental case comparing hard and soft tickets,\n    following the exact mathematical specification from the problem.\n    \"\"\"\n    # Unpack test case parameters\n    K_keep, beta, tau_0, tau_min, eta, T = case_params\n\n    # Fixed problem parameters\n    D = 20\n    N = 20  # D = N is specified\n    K_true = 5\n    c = 0.2\n    eta_pre = 0.05\n    s_init = 4.0\n\n    # Part 0: Setup and Initialization\n    # Ground-truth target vector y\n    y = np.zeros(D, dtype=np.float64)\n    y[D - K_true:] = c\n\n    # Initial weights w^(0)\n    indices_arr = np.arange(D, dtype=np.float64)\n    w0 = ((-1) ** indices_arr) * (1.0 - 0.01 * indices_arr)\n\n    # Part 1: Mask Construction\n    # Perform one full-batch gradient descent step to get w_pre\n    grad_w0_data = (2.0 / N) * (w0 - y)\n    w_pre = w0 - eta_pre * grad_w0_data\n    \n    # Create mask m by keeping the K_keep largest magnitudes of w_pre.\n    # Ties are broken by increasing index. This is handled by np.lexsort,\n    # which uses the index as a secondary, ascending sort key.\n    magnitudes = np.abs(w_pre)\n    indices_sorted = np.lexsort((indices_arr, -magnitudes))\n    top_k_indices = indices_sorted[:K_keep]\n\n    m = np.zeros(D, dtype=np.float64)\n    m[top_k_indices] = 1.0\n\n    # Part 2: Hard Ticket Training\n    v_hard = w0.copy()\n    for _ in range(T):\n        w_hard = m * v_hard\n        grad_w_data = (2.0 / N) * (w_hard - y)\n        grad_v = m * grad_w_data\n        v_hard -= eta * grad_v\n\n    w_final_hard = m * v_hard\n    L_hard = (1.0 / N) * np.sum((w_final_hard - y) ** 2)\n\n    # Part 3: Soft Ticket Training\n    v_soft = w0.copy()\n    s = np.full(D, -s_init, dtype=np.float64)\n    s[m == 1.0] = s_init\n\n    # Temperature annealing schedule\n    if T > 1:\n        t_vals = np.arange(T, dtype=np.float64)\n        tau_schedule = tau_0 - (tau_0 - tau_min) * t_vals / (T - 1.0)\n    else:\n        tau_schedule = np.array([tau_0])\n    \n    for t in range(T):\n        tau_t = tau_schedule[t]\n        \n        # Calculate gates g from logits s\n        z = s / tau_t\n        g = 1.0 / (1.0 + np.exp(-z))\n\n        # Current weights for the soft ticket\n        w_soft = g * v_soft\n\n        # Common gradient term (gradient of data loss w.r.t w)\n        grad_w_data_soft = (2.0 / N) * (w_soft - y)\n\n        # Update trainable weights v_soft\n        grad_v_data = g * grad_w_data_soft\n        v_soft -= eta * grad_v_data\n\n        # Update gate logits s\n        # Gradient of data loss w.r.t gates g\n        grad_g_data = v_soft * grad_w_data_soft\n        # Gradient of budget penalty w.r.t gates g\n        grad_g_penalty = (2.0 * beta / D) * (np.mean(g) - K_keep / D) * np.ones(D)\n        \n        # Total gradient w.r.t g\n        grad_g_soft = grad_g_data + grad_g_penalty\n        \n        # Gradient w.r.t s via chain rule\n        grad_s = grad_g_soft * g * (1.0 - g) / tau_t\n        s -= eta * grad_s\n\n    # Final loss calculation for soft ticket\n    # Use final parameters and final (minimum) temperature\n    tau_final = tau_min\n    g_final = 1.0 / (1.0 + np.exp(-s / tau_final))\n    w_final_soft = g_final * v_soft\n    L_soft = (1.0 / N) * np.sum((w_final_soft - y) ** 2)\n\n    # Part 4: Final Calculation\n    delta = L_hard - L_soft\n    return delta\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (5, 50.0, 2.0, 0.2, 0.2, 120),    # Case 1\n        (20, 50.0, 2.0, 0.2, 0.2, 120),   # Case 2\n        (3, 50.0, 2.0, 0.2, 0.2, 120),    # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_experiment(case)\n        # Format to exactly 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3187989"}]}