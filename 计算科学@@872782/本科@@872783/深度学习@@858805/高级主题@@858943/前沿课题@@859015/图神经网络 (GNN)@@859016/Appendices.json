{"hands_on_practices": [{"introduction": "图神经网络的核心在于其消息传递机制，即节点通过聚合邻居的信息来更新自身的表示。本实践练习 [@problem_id:3106157] 将带你亲手实现并比较几种经典的基于信息传播的半监督学习方法。通过在一个精心设计的图上进行实验，你将直观地理解标签信息是如何从少数已标记节点扩散到整个网络的，同时也会发现当传播步数过多时，模型会面临一个被称为“过平滑”的关键挑战。", "problem": "给定一个具有弱节点特征和稀疏监督的无向简单图。您的目标是实现三种基于传播的方法，在一组受控参数下比较它们的测试准确率，并检验当传播步数增加时的边界行为。这些方法是：使用归一化邻接矩阵幂次的标签传播、简化图卷积 (SGC) 和近似个性化神经预测传播 (APPNP)。您必须从核心定义推导每种方法，并将其实现为一个无需用户输入即可生成所要求输出格式的完整程序。\n\n从以下基本概念开始：\n- 图由一个邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 表示，其中如果节点 $i$ 和节点 $j$ 之间存在一条边，则 $A_{ij} = 1$，否则 $A_{ij} = 0$。令 $I$ 表示单位矩阵。定义 $\\tilde{A} = A + I$ 以包含自环。通过 $\\tilde{D}_{ii} = \\sum_{j} \\tilde{A}_{ij}$ 定义度矩阵 $\\tilde{D}$。对称归一化邻接矩阵为 $\\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2}$。\n- 对于任何非负整数 $K$，通过 $\\hat{A}$ 进行的 $K$ 步传播是 $\\hat{A}^K$。对于一个连通图，其中 $\\hat{A}$ 是对称的，并且由带有自环的 $\\tilde{A}$ 暗示了随机归一化，其谱半径满足 $\\rho(\\hat{A}) = 1$。当 $K \\to \\infty$ 时，在标准条件下（例如，由自环确保的非周期性），$\\hat{A}^K$ 会收敛到一个秩为1的投影算子，该算子投影到主特征向量方向上。\n\n图、标签、特征和划分：\n- 节点数 $n = 8$，节点索引为 $\\{0,1,2,3,4,5,6,7\\}$。\n- 边：存在两个大小为4的密集簇，由一座桥连接。具体来说：\n  - 对于所有不同的 $i,j \\in \\{0,1,2,3\\}$，包含边 $\\{i,j\\}$。\n  - 对于所有不同的 $i,j \\in \\{4,5,6,7\\}$，包含边 $\\{i,j\\}$。\n  - 包含桥边 $\\{3,4\\}$。\n- 真实类别：节点 $\\{0,1,2,3\\}$ 属于类别 $0$，节点 $\\{4,5,6,7\\}$ 属于类别 $1$。令 $C = 2$ 为类别数。令 $Y \\in \\mathbb{R}^{n \\times C}$ 为真实标签的独热编码，即如果节点 $i$ 属于类别 $c$，则 $Y_{ic} = 1$，否则 $Y_{ic} = 0$。\n- 带标签的训练集索引 $L = \\{0,7\\}$ 和测试集索引 $T = \\{1,2,3,4,5,6\\}$。定义半监督标签种子矩阵 $Y_{0} \\in \\mathbb{R}^{n \\times C}$，其中如果 $i \\in L$，则 $Y_{0i:} = Y_{i:}$，如果 $i \\notin L$，则 $Y_{0i:} = 0$。\n- 弱节点特征：特征维度 $d = 3$。令 $X \\in \\mathbb{R}^{n \\times d}$，其条目为 $X_{ij} = 1 + \\epsilon_{ij} + s_{i} \\cdot \\delta \\cdot \\mathbb{1}\\{j=0\\}$，其中 $\\epsilon_{ij} \\sim \\mathcal{N}(0,\\sigma^{2})$ 是独立的噪声项，$\\sigma = 0.02$，如果节点 $i$ 属于类别 $0$，则 $s_{i} = +1$，如果节点 $i$ 属于类别 $1$，则 $s_{i} = -1$，且 $\\delta = 0.05$。这产生了一个弱信息性的第一个特征和其余几乎恒定的特征。\n\n需要实现的方法：\n- 标签传播基线：对于给定的整数 $K \\ge 0$，计算 $Y_{\\text{LP}} = \\hat{A}^{K} Y_{0}$，并通过 $\\arg\\max_{c} (Y_{\\text{LP}})_{ic}$ 对每个节点 $i$ 进行分类。仅通过与真实标签 Y 比较，在测试集索引 T 上评估准确率。\n- 简化图卷积 (SGC)：对于给定的整数 $K \\ge 0$，计算 $\\tilde{X} = \\hat{A}^{K} X$，通过在带标签的节点 $L$ 上进行多输出岭回归来拟合一个线性分类器，以获得权重 $W \\in \\mathbb{R}^{d \\times C}$，该权重最小化 $\\|\\tilde{X}_{L} W - Y_{L}\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2}$，其中 $\\lambda = 10^{-2}$。然后预测 $Z = \\tilde{X} W$ 并使用 $\\arg\\max_{c} Z_{ic}$ 进行分类。在 $T$ 上评估准确率。\n- 近似个性化神经预测传播 (APPNP)：初始化 $H^{(0)} = Y_{0}$，并对 $t = 0,1,\\dots,T_{\\max}-1$ 迭代更新 $H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)}$，其中传送参数 $\\alpha = 0.15$，迭代次数 $T_{\\max} = 100$。然后使用 $\\arg\\max_{c} H^{(T_{\\max})}_{ic}$ 进行分类，并在 $T$ 上评估准确率。\n\n需要研究的边界分析：\n- 对于标签传播和 SGC 平滑操作 $\\hat{A}^{K}$ 中 $K \\to \\infty$ 的情况，论证类别分数会发生什么变化（从 $\\hat{A}$ 的谱的角度），以及当标签少、特征弱时，这会如何影响准确率。\n\n测试套件参数：\n- 使用上面指定的共享图、标签、特征和划分。\n- 对于标签传播和 SGC 方法，在以下传播指数 $K \\in \\{0,1,5,200\\}$ 上评估这三种方法，而 APPNP 在所有情况下均使用固定的 $\\alpha = 0.15$ 和 $T_{\\max} = 100$。\n\n要求的输出：\n- 对于测试套件中的每个 $K$，计算测试准确率的元组 $(a_{\\text{LP}}, a_{\\text{SGC}}, a_{\\text{APPNP}})$，其中每个 $a$ 是测试集中被正确分类的节点比例，表示为四舍五入到三位小数的小数。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素按 $K \\in \\{0,1,5,200\\}$ 的顺序对应一个测试用例，并且本身是一个包含三个浮点数的列表 $[a_{\\text{LP}},a_{\\text{SGC}},a_{\\text{APPNP}}]$，四舍五入到三位小数。例如，格式必须类似于 $[[x_{1},y_{1},z_{1}],[x_{2},y_{2},z_{2}],\\dots]$。\n\n所有随机量必须使用固定的种子生成，以确保确定性的结果。本问题不涉及物理单位。未使用角度。最终输出必须与指定的单行格式完全匹配。", "solution": "用户提供了一个有效的问题陈述。\n\n该问题要求在图上实现和比较三种半监督学习方法：标签传播 (LP)、简化图卷积 (SGC) 和近似个性化神经预测传播 (APPNP)。我们将首先建立通用的数学框架，然后推导每种方法，分析它们在无限传播步数极限下的行为，并最终实现它们以获得指定的测试准确率。\n\n### 1. 图和数据表示\n\n该图有 $n=8$ 个节点，由两个4节点集团 $\\{0,1,2,3\\}$ 和 $\\{4,5,6,7\\}$ 组成，通过一条边 $\\{3,4\\}$ 连接。邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示此结构。为了引入节点自连接（这是图神经网络 (GNN) 中的标准做法，以确保非周期性并将节点自身的特征包含在其更新中），我们定义 $\\tilde{A} = A + I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n传播动态由对称归一化邻接矩阵控制。我们将 $\\tilde{A}$ 的度矩阵 $\\tilde{D}$ 定义为一个对角矩阵，其条目为 $\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$。归一化邻接矩阵则为：\n$$ \\hat{A} = \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} $$\n该矩阵是对称的，其特征值 $\\lambda$ 有界，即 $|\\lambda| \\le 1$。操作 $H' = \\hat{A} H$ 可以解释为从邻近节点聚合信息，其中 $H$ 可以是节点特征或标签的矩阵。\n\n数据包括：\n- **节点特征** $X \\in \\mathbb{R}^{n \\times d}$：一个矩阵，其中第 $i$ 行是节点 $i$ 的特征向量。这些特征被设计为弱信息性的。\n- **真实标签** $Y \\in \\{0,1\\}^{n \\times C}$：一个真实类别分配的独热编码矩阵，其中有 $C=2$ 个类别。\n- **种子标签** $Y_0 \\in \\mathbb{R}^{n \\times C}$：一个稀疏矩阵，包含训练节点集 $L=\\{0,7\\}$ 的独热标签，而所有其他节点均为零。该矩阵为传播过程提供种子。\n\n### 2. 基于传播的方法\n\n**方法1：标签传播 (LP)**\nLP 是一种简单的半监督算法，它将标签信息从带标签的节点扩散到整个图中。其核心思想是相连的节点可能共享相同的标签。通过将种子标签矩阵 $Y_0$ 乘以归一化邻接矩阵 $\\hat{A}$ 的 $K$ 次方来实现 $K$ 步传播。\n\n$K$ 步后的预测标签分数为：\n$$ Y_{\\text{LP}} = \\hat{A}^K Y_0 $$\n节点 $i$ 的最终类别预测是使其分数最大化的那个类别：$\\arg\\max_c (Y_{\\text{LP}})_{ic}$。当 $K=0$ 时，$\\hat{A}^0 = I$，因此只有初始带标签的节点有非零分数，所有未标记的节点都被预测为类别 0（由于 `argmax` 的平局决胜规则）。\n\n**方法2：简化图卷积 (SGC)**\nSGC 通过将特征传播与非线性变换解耦来简化 GNN。它包括两个阶段：\n1.  **特征传播**：通过应用传播矩阵 $\\hat{A}^K$ 对节点特征 $X$ 进行平滑处理：\n    $$ \\tilde{X} = \\hat{A}^K X $$\n    此步骤通过从节点的 $K$ 跳邻域聚合特征信息，为每个节点有效地创建了新特征。\n2.  **线性分类**：在带标签节点的传播特征上训练一个线性分类器。我们寻求一个权重矩阵 $W \\in \\mathbb{R}^{d \\times C}$，以最小化岭回归目标函数：\n    $$ \\underset{W}{\\text{minimize}} \\quad \\|\\tilde{X}_L W - Y_L\\|_F^2 + \\lambda \\|W\\|_F^2 $$\n    其中 $\\tilde{X}_L$ 和 $Y_L$ 分别是 $\\tilde{X}$ 和 $Y$ 中对应于带标签集 $L$ 的行，$\\| \\cdot \\|_F$ 是弗罗贝尼乌斯范数。正则化参数给定为 $\\lambda=10^{-2}$。这个优化问题有一个闭式解：\n    $$ W = (\\tilde{X}_L^T \\tilde{X}_L + \\lambda I_d)^{-1} \\tilde{X}_L^T Y_L $$\n    其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n然后计算所有节点的预测 $Z = \\tilde{X} W$，节点 $i$ 的类别为 $\\arg\\max_c Z_{ic}$。\n\n**方法3：近似个性化神经预测传播 (APPNP)**\nAPPNP 的灵感来自于个性化 PageRank，旨在解决深度 GNN 中固有的过平滑问题。它在传播邻域信息和保留初始“根”信息之间保持平衡。虽然原始 APPNP 使用神经网络的输出作为根信息，但这里我们使用种子标签 $Y_0$。\n\n从 $H^{(0)} = Y_0$ 开始，APPNP 对 $t=0, 1, \\dots, T_{\\max}-1$ 执行以下迭代更新：\n$$ H^{(t+1)} = (1-\\alpha) \\hat{A} H^{(t)} + \\alpha H^{(0)} $$\n这里，$\\alpha \\in [0,1]$ 是传送概率，给定为 $\\alpha=0.15$。在每一步中，该过程都涉及一步邻域聚合（$\\hat{A}H^{(t)}$ 项）和一次“传送”回初始预测（$H^{(0)}$ 项）。这确保了来自种子节点的影响永远不会完全消失。最终预测取自收敛或接近收敛的状态，这里是 $H^{(T_{\\max})}$，其中 $T_{\\max}=100$。通过 $\\arg\\max_c (H^{(T_{\\max})})_{ic}$ 进行分类。\n\n### 3. $K \\to \\infty$ 的边界分析\n\nLP 和 SGC 在大量传播步数 $K$ 下的行为由 $\\hat{A}$ 的谱特性决定。由于图是连通且非周期的（因为有自环），根据 Perron-Frobenius 定理，$\\hat{A}$ 的最大特征值为 $\\lambda_1 = 1$，且其重数为1。所有其他特征值 $|\\lambda_i|  1$。当 $K \\to \\infty$ 时，$\\hat{A}$ 的幂收敛到一个秩为1的投影算子，该算子投影到主特征向量 $u_1$ 上：\n$$ \\lim_{K \\to \\infty} \\hat{A}^K = u_1 u_1^T $$\n其中 $u_1$ 是对应于 $\\lambda_1=1$ 的归一化特征向量。对于对称归一化，该特征向量由 $(u_1)_i \\propto \\sqrt{\\tilde{D}_{ii}}$ 给出。\n\n**LP 过平滑**：\n对于大的 $K$，LP 的预测变为：\n$$ Y_{\\text{LP}} \\approx (u_1 u_1^T) Y_0 = u_1 (u_1^T Y_0) $$\n项 $v^T = u_1^T Y_0$ 是一个 $1 \\times C$ 的常数行向量。分数矩阵 $Y_{\\text{LP}}$ 变为 $u_1 v^T$，这意味着节点 $i$ 的分数向量是 $(u_1)_i v^T$。由于 $(u_1)_i  0$，分类决策 $\\arg\\max_c ((u_1)_i v_c)$ 简化为 $\\arg\\max_c v_c$，这与节点 $i$ 无关。因此，所有节点都会得到相同的预测。这种现象被称为“过平滑”。测试准确率将骤降至属于这个单一预测类别的测试节点的比例。在这个问题中，节点0和7的度相等，因此它们对最终聚合分数的贡献权重相等，导致平局，而`argmax`会选择类别0。这导致准确率为 $3/6 = 0.5$。\n\n**SGC 过平滑**：\nSGC 也面临类似的命运。对于大的 $K$，平滑后的特征变为：\n$$ \\tilde{X} \\approx (u_1 u_1^T) X = u_1 (u_1^T X) $$\n项 $f^T = u_1^T X$ 是一个 $1 \\times d$ 的行向量。任何节点 $i$ 的特征向量变为 $(\\tilde{X})_{i:} = (u_1)_i f^T$。所有节点特征都变得共线，指向 $f^T$ 方向。在这样的特征上训练的线性分类器将对每个节点做出相同的预测，因为分离超平面会划分特征空间，而所有特征向量都位于同一条线上。这同样导致准确率为 $0.5$。\n\n**APPNP 对过平滑的抵抗能力**：\n由于其传送机制，APPNP 避免了这个问题。APPNP 的收敛解是 $H^{(\\infty)} = \\alpha(I - (1-\\alpha)\\hat{A})^{-1} H^{(0)}$。单位矩阵 $I$ 和传送概率 $\\alpha$ 的存在，防止了传播过程完全冲刷掉 $H^{(0)}$ 中的初始信息。它确保了最终的预测对于初始种子节点保持“个性化”，从而即使在容易出现过平滑的结构上也能保持高准确率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates Label Propagation, SGC, and APPNP on a\n    specified graph and dataset.\n    \"\"\"\n\n    def build_graph():\n        \"\"\"Constructs the adjacency matrix for the specified graph.\"\"\"\n        n = 8\n        A = np.zeros((n, n), dtype=np.float64)\n        # Clique on {0, 1, 2, 3}\n        for i in range(4):\n            for j in range(i + 1, 4):\n                A[i, j] = A[j, i] = 1.0\n        # Clique on {4, 5, 6, 7}\n        for i in range(4, 8):\n            for j in range(i + 1, 8):\n                A[i, j] = A[j, i] = 1.0\n        # Bridge {3, 4}\n        A[3, 4] = A[4, 3] = 1.0\n        return A\n\n    def normalize_adjacency(A):\n        \"\"\"Computes the symmetrically normalized adjacency matrix.\"\"\"\n        A_tilde = A + np.eye(A.shape[0])\n        D_tilde_diag = A_tilde.sum(axis=1)\n        # Replace zeros in D_tilde_diag to avoid division by zero, though not expected here.\n        D_tilde_diag[D_tilde_diag == 0] = 1e-12\n        D_tilde_inv_sqrt = np.diag(np.power(D_tilde_diag, -0.5))\n        A_hat = D_tilde_inv_sqrt @ A_tilde @ D_tilde_inv_sqrt\n        return A_hat\n\n    def generate_data(n, seed):\n        \"\"\"Generates labels, features, and data splits.\"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Ground truth labels\n        y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n        C = 2\n        Y = np.eye(C, dtype=np.float64)[y_true]\n        \n        # Labeled and test sets\n        L = [0, 7]\n        T = [1, 2, 3, 4, 5, 6]\n        \n        # Seed labels (for LP and APPNP)\n        Y_0 = np.zeros_like(Y)\n        Y_0[L] = Y[L]\n        \n        # Node features\n        d = 3\n        sigma = 0.02\n        delta = 0.05\n        s = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n        \n        noise = rng.normal(0, sigma, size=(n, d))\n        signal = np.zeros((n, d))\n        signal[:, 0] = s * delta\n        \n        X = 1.0 + noise + signal\n        \n        return X, Y, y_true, Y_0, L, T\n\n    def solve_lp(A_hat, Y_0, K, T, y_true):\n        \"\"\"Calculates test accuracy for Label Propagation.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n        \n        Y_lp = A_hat_K @ Y_0\n        preds = np.argmax(Y_lp, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_sgc(A_hat, X, Y, K, T, y_true, L, reg_lambda):\n        \"\"\"Calculates test accuracy for Simplified Graph Convolution.\"\"\"\n        if K == 0:\n            A_hat_K = np.eye(A_hat.shape[0])\n        else:\n            A_hat_K = np.linalg.matrix_power(A_hat, K)\n            \n        X_tilde = A_hat_K @ X\n        \n        # Ridge regression\n        X_L = X_tilde[L]\n        Y_L = Y[L]\n        \n        d = X.shape[1]\n        term1 = X_L.T @ X_L + reg_lambda * np.eye(d)\n        term2 = X_L.T @ Y_L\n        W = np.linalg.solve(term1, term2)\n        \n        Z = X_tilde @ W\n        preds = np.argmax(Z, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    def solve_appnp(A_hat, Y_0, T, y_true, alpha, T_max):\n        \"\"\"Calculates test accuracy for APPNP.\"\"\"\n        H = Y_0.copy()\n        for _ in range(T_max):\n            H = (1 - alpha) * (A_hat @ H) + alpha * Y_0\n        \n        preds = np.argmax(H, axis=1)\n        \n        accuracy = np.mean(preds[T] == y_true[T])\n        return accuracy\n\n    # Main execution logic\n    n_nodes = 8\n    random_seed = 42 # For reproducibility\n    \n    A = build_graph()\n    A_hat = normalize_adjacency(A)\n    X, Y, y_true, Y_0, L, T = generate_data(n=n_nodes, seed=random_seed)\n    \n    test_suite_K = [0, 1, 5, 200]\n    sgc_lambda = 1e-2\n    appnp_alpha = 0.15\n    appnp_T_max = 100\n    \n    all_results = []\n    \n    # APPNP result is constant for all K\n    acc_appnp = solve_appnp(A_hat, Y_0, T, y_true, appnp_alpha, appnp_T_max)\n    \n    for K in test_suite_K:\n        acc_lp = solve_lp(A_hat, Y_0, K, T, y_true)\n        acc_sgc = solve_sgc(A_hat, X, Y, K, T, y_true, L, sgc_lambda)\n        \n        all_results.append([acc_lp, acc_sgc, acc_appnp])\n\n    # Format output string without spaces\n    result_strings = []\n    for r in all_results:\n        inner_str = '[' + ','.join([f'{val:.3f}' for val in r]) + ']'\n        result_strings.append(inner_str)\n    \n    final_output = '[' + ','.join(result_strings) + ']'\n    print(final_output)\n\nsolve()\n\n```", "id": "3106157"}, {"introduction": "除了节点分类，图神经网络在另一项关键图任务——链接预测（Link Prediction）中也大有可为。本实践练习 [@problem_id:3131905] 将指导你使用一个无参数的GNN模型来推断网络中缺失的连接。该方法的核心思想是，共享相似邻里结构的节点更有可能被连接，而GNN的消息传递机制正是捕捉这种结构相似性的强大工具。", "problem": "你的任务是设计一个纯算法的、基于图神经网络（GNN）的程序，该程序利用类似于三元闭包的局部结构先验来推断无向图中的缺失边。该程序必须源于图论和消息传递的基本定义，并且必须使用一个有原则的聚合度量来量化精确率-召回率的权衡。最终结果必须针对一个固定的测试套件进行计算，并以精确指定的格式输出。\n\n推导的基本基础如下。\n- 一个图由一个无向邻接矩阵 $A \\in \\{0,1\\}^{n \\times n}$ 表示，其中 $A_{ij} = A_{ji}$ 且 $A_{ii} = 0$。节点集由 $\\{0,1,\\dots,n-1\\}$ 索引，边集为 $E = \\{(i,j) \\mid i \\lt j, A_{ij} = 1\\}$。\n- 图神经网络（GNN）中的消息传递会聚合邻近节点的特征。添加自环后，增广邻接矩阵为 $\\tilde{A} = A + I$，度矩阵为 $\\tilde{D} = \\operatorname{diag}(\\sum_j \\tilde{A}_{ij})$，对称归一化得到 $\\hat{A} = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$。\n- 从单位节点特征 $X = I \\in \\mathbb{R}^{n \\times n}$ 开始，一个无学习参数的两步邻域聚合产生表示 $H = \\hat{A}^2 X = \\hat{A}^2$。这捕捉了三元闭包所使用的二阶邻域信息，因为 $H$ 中较大的值反映了通过共享邻居实现的更强连接性。\n\n你的程序必须实现以下流程。\n1. 对于每个测试图，提供两个矩阵：观测到的邻接矩阵 $A^{\\text{obs}}$ 和真实的邻接矩阵 $A^{\\text{true}}$。观测到的邻接矩阵 $A^{\\text{obs}}$ 是GNN必须从中推断缺失边的输入图。真实的邻接矩阵 $A^{\\text{true}}$ 决定了哪些缺失的边在底层图中是真实存在的，哪些是真实不存在的。\n2. 根据 $A^{\\text{obs}}$ 计算对称归一化邻接矩阵 $\\hat{A}^{\\text{obs}}$，使用 $\\tilde{A}^{\\text{obs}} = A^{\\text{obs}} + I$ 和 $\\hat{A}^{\\text{obs}} = (\\tilde{D}^{\\text{obs}})^{-1/2}\\tilde{A}^{\\text{obs}}(\\tilde{D}^{\\text{obs}})^{-1/2}$，其中 $\\tilde{D}^{\\text{obs}} = \\operatorname{diag}(\\sum_j \\tilde{A}^{\\text{obs}}_{ij})$。\n3. 计算两层消息传递表示 $H = (\\hat{A}^{\\text{obs}})^2$。\n4. 对于 $A^{\\text{obs}}$ 中每一个非边（即 $A^{\\text{obs}}_{ij} = 0$）的无序节点对 $(i,j)$（其中 $i \\lt j$），使用节点表示的内积后跟一个逻辑斯蒂压缩来计算一个标量链接分数 $s(i,j)$：\n   $$ s(i,j) = \\sigma\\big(H_{i,:}^\\top H_{j,:}\\big), \\quad \\sigma(z) = \\frac{1}{1 + e^{-z}}. $$\n5. 对每个候选对定义二元标签 $y(i,j)$ 如下：\n   - 如果 $(i,j)$ 是 $A^{\\text{true}}$ 中的一条边（即 $A^{\\text{true}}_{ij} = 1$），则 $y(i,j) = 1$，该对是要恢复的真正缺失的边。\n   - 如果 $(i,j)$ 不是 $A^{\\text{true}}$ 中的一条边（即 $A^{\\text{true}}_{ij} = 0$），则 $y(i,j) = 0$，该对是应不被提出的真正不存在的边。\n6. 通过计算平均精度（AP）来衡量精确率-召回率的权衡。AP定义为通过将候选对按 $s(i,j)$ 降序排列所产生的精确率-召回率曲线下的面积。假设总共有 $m$ 个正标签。如果排序后的候选列表的标签为 $\\{y_1, y_2, \\dots, y_N\\}$，并且在排名 $k$ 处的精确率为 $P(k) = \\frac{1}{k}\\sum_{i=1}^k y_i$，那么平均精度为\n   $$ \\mathrm{AP} = \\begin{cases}\n   \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot \\mathbb{1}\\{y_k = 1\\},  \\text{若 } m  0, \\\\\n   0,  \\text{若 } m = 0,\n   \\end{cases} $$\n   这是一个标准的、无阈值的精确率-召回率权衡的总结。精确率定义为 $ \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FP}} $，召回率定义为 $ \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} $，其中 $\\mathrm{TP}$、$\\mathrm{FP}$ 和 $\\mathrm{FN}$ 是在给定阈值下的真正例、假正例和假反例的数量。\n\n测试套件包含三个无向图，每个图由 $(A^{\\text{obs}}, A^{\\text{true}})$ 指定，节点索引从 $0$ 开始。在所有情况下，图都是简单的（无平行边）、无权的且对称的。\n- 测试用例 $1$（有利于三元闭包的稀疏图）：\n  - 节点数 $n = 6$。\n  - 观测到的边 $E^{\\text{obs}} = \\{(0,1),(1,2),(2,3),(3,4),(4,5)\\}$。\n  - 真实的边 $E^{\\text{true}} = E^{\\text{obs}} \\cup \\{(0,2),(2,4)\\}$。\n  - 候选对是所有满足 $i \\lt j$ 且 $(i,j) \\notin E^{\\text{obs}}$ 的 $(i,j)$。其中，真正缺失的边是 $(0,2)$ 和 $(2,4)$，而所有其他候选对都是真正不存在的。\n- 测试用例 $2$（没有初始三角形的边界情况）：\n  - 节点数 $n = 5$。\n  - 观测到的边 $E^{\\text{obs}} = \\{(0,1),(1,2),(2,3),(3,4)\\}$。\n  - 真实的边 $E^{\\text{true}} = E^{\\text{obs}} \\cup \\{(1,3)\\}$。\n  - 候选对是所有满足 $i \\lt j$ 且 $(i,j) \\notin E^{\\text{obs}}$ 的 $(i,j)$。其中，真正缺失的边是 $(1,3)$，而所有其他候选对都是真正不存在的。\n- 测试用例 $3$（近完全图，混合了真正缺失和真正不存在的边）：\n  - 节点数 $n = 6$。\n  - 真实的边 $E^{\\text{true}}$ 是所有满足 $i \\lt j$ 的对 $(i,j)$，除了 $(0,2)$，因此底层的真实图是近完全图，恰好有一条真正不存在的边 $(0,2)$。\n  - 观测到的边 $E^{\\text{obs}} = E^{\\text{true}} \\setminus \\{(0,5),(1,3)\\}$，因此观测图除了真正不存在的边 $(0,2)$ 之外，还缺失了两条真正存在的边 $(0,5)$ 和 $(1,3)$。\n  - 候选对是在 $E^{\\text{obs}}$ 中不存在的三个对：$(0,2)$、$(0,5)$ 和 $(1,3)$。其中，$(0,5)$ 和 $(1,3)$ 是真正缺失的，而 $(0,2)$ 是真正不存在的。\n\n你的程序必须：\n- 为每个测试用例实现上述基于GNN的评分和平均精度计算。\n- 生成单行输出，其中包含按顺序排列的三个测试用例的平均精度值，格式为用方括号括起来的逗号分隔列表，例如 $[\\mathrm{AP}_1,\\mathrm{AP}_2,\\mathrm{AP}_3]$。此问题不涉及单位。数字应为标准十进制浮点数，不带附加文本。\n\n你的解决方案必须是自包含的，并且不得读取任何输入或访问外部文件或网络。所有计算都必须使用上面指定的算法步骤执行。", "solution": "该问题在图深度学习领域提出了一个有效的、定义明确的任务。它要求实现一个特定的、无参数的图神经网络（GNN）算法用于链接预测，并使用平均精度（AP）指标在一组定义的测试用例上对其进行评估。所有定义、约束和数据都已提供，它们在科学上是合理的，并且没有歧义或矛盾。因此，我们可以着手进行正式的解决方案。\n\n核心目标是利用观测图（由邻接矩阵 $A^{\\text{obs}}$ 表示）的局部结构来推断其中的缺失边。此推断任务的基准事实由一个真实的邻接矩阵 $A^{\\text{true}}$ 提供。该方法基于三元闭包原理，该原理假定共享一个共同邻居的两个节点更有可能被连接。GNN通过消息传递为形式化这种直觉提供了一个自然的框架。\n\n规定的算法操作如下：\n\n首先，对于一个具有 $n$ 个节点和观测邻接矩阵 $A^{\\text{obs}} \\in \\{0,1\\}^{n \\times n}$ 的输入图，我们计算邻接矩阵的对称归一化版本。这是图卷积网络（GCNs）中的标准做法，以确保节点表示在传播过程中不会爆炸或消失，并能恰当地平均来自邻居节点的特征。该过程包括三个步骤：\n1.  向图中添加自环，得到增广邻接矩阵 $\\tilde{A}^{\\text{obs}} = A^{\\text{obs}} + I$，其中 $I$ 是 $n \\times n$ 的单位矩阵。这确保了节点在聚合过程中包含其自身的特征。\n2.  计算增广度矩阵 $\\tilde{D}^{\\text{obs}} = \\operatorname{diag}(\\sum_j \\tilde{A}^{\\text{obs}}_{ij})$。每个对角线元素 $(\\tilde{D}^{\\text{obs}})_{ii}$ 对应于带自环图中节点 $i$ 的度。\n3.  进行对称归一化以获得 $\\hat{A}^{\\text{obs}} = (\\tilde{D}^{\\text{obs}})^{-1/2}\\tilde{A}^{\\text{obs}}(\\tilde{D}^{\\text{obs}})^{-1/2}$。矩阵 $(\\tilde{D}^{\\text{obs}})^{-1/2}$ 是一个对角矩阵，其每个元素是 $\\tilde{D}^{\\text{obs}}$ 中相应对角线元素的逆平方根。\n\n其次，我们执行一个没有任何可训练参数的两层消息传递操作。从单位特征 $X=I$ 开始，一层传播得到 $\\hat{A}^{\\text{obs}}X = \\hat{A}^{\\text{obs}}$。第二层得到 $H = \\hat{A}^{\\text{obs}}(\\hat{A}^{\\text{obs}}X) = (\\hat{A}^{\\text{obs}})^2$。得到的矩阵 $H \\in \\mathbb{R}^{n \\times n}$ 包含节点表示，其中每行 $H_{i,:}$ 是节点 $i$ 的特征向量。一个元素 $H_{ij}$ 聚合了关于节点 $i$ 和 $j$ 之间长度最多为 $2$ 的路径信息，有效地编码了它们的2跳邻域结构。\n\n第三，我们识别所有用于链接预测的候选对。这些是无序节点对 $(i,j)$，其中 $i  j$ 且在观测图中未连接，即 $A^{\\text{obs}}_{ij} = 0$。\n\n第四，对于每个候选对 $(i,j)$，我们计算一个链接分数 $s(i,j)$。该分数源于节点表示的相似性。具体来说，我们计算它们特征向量的内积 $H_{i,:}^\\top H_{j,:}$，这衡量了它们2跳邻域的相似性。然后将此点积通过一个逻辑斯蒂S型函数 $\\sigma(z) = (1 + e^{-z})^{-1}$，将分数压缩到 $(0,1)$ 范围内：\n$$ s(i,j) = \\sigma\\big(H_{i,:}^\\top H_{j,:}\\big) $$\n更高的分数表示更强的结构相似性，因此预测的链接可能性也更高。\n\n第五，为了评估我们评分函数的性能，我们为每个候选对分配一个真实标签 $y(i,j)$。如果边 $(i,j)$ 存在于真实图中（$A^{\\text{true}}_{ij} = 1$），则标签为 $y(i,j) = 1$，否则（$A^{\\text{true}}_{ij} = 0$）为 $y(i,j) = 0$。\n\n最后，我们使用平均精度（AP）指标来量化模型将真正缺失的边（正标签）排在真正不存在的边（负标签）之前的能力。流程如下：\n1.  根据所有候选对的分数 $s(i,j)$ 进行降序排序。让对应的真实标签序列为 $\\{y_1, y_2, \\dots, y_N\\}$，其中 $N$ 是候选对的总数。\n2.  计算正实例的总数，$m = \\sum_{k=1}^N y_k$。\n3.  如果 $m > 0$，AP被计算为在每个发现真正例的排名 $k$ 处的精确率值的平均值：\n    $$ \\mathrm{AP} = \\frac{1}{m}\\sum_{k=1}^N P(k)\\cdot \\mathbb{1}\\{y_k = 1\\} $$\n    其中 $P(k) = \\frac{1}{k}\\sum_{i=1}^k y_i$ 是排名 $k$ 处的精确率，而 $\\mathbb{1}\\{\\cdot\\}$ 是指示函数。该指标奖励那些将真正例置于排序列表顶部的模型。\n4.  如果 $m = 0$（没有缺失的边可找），AP定义为 $0$。\n\n这整个流程独立地应用于提供的三个测试用例中的每一个。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_adj_matrix(n, edges):\n    \"\"\"Creates an n x n symmetric adjacency matrix from a list of edges.\"\"\"\n    A = np.zeros((n, n), dtype=np.float64)\n    for i, j in edges:\n        A[i, j] = 1\n        A[j, i] = 1\n    return A\n\ndef compute_ap(A_obs, A_true):\n    \"\"\"\n    Computes the Average Precision for link prediction on a single graph.\n    \n    Args:\n        A_obs (np.ndarray): The observed adjacency matrix.\n        A_true (np.ndarray): The ground truth adjacency matrix.\n        \n    Returns:\n        float: The Average Precision score.\n    \"\"\"\n    n = A_obs.shape[0]\n\n    # Step 2: Compute the symmetric normalized adjacency matrix A_hat_obs\n    A_obs_tilde = A_obs + np.identity(n)\n    D_obs_tilde_vec = np.sum(A_obs_tilde, axis=1)\n    # Handle case of isolated nodes (degree 0) to avoid division by zero\n    with np.errstate(divide='ignore'):\n        D_inv_sqrt_vec = 1.0 / np.sqrt(D_obs_tilde_vec)\n    D_inv_sqrt_vec[np.isinf(D_inv_sqrt_vec)] = 0\n    D_inv_sqrt_mat = np.diag(D_inv_sqrt_vec)\n    \n    A_hat_obs = D_inv_sqrt_mat @ A_obs_tilde @ D_inv_sqrt_mat\n\n    # Step 3: Compute the two-layer message-passing representations H\n    H = A_hat_obs @ A_hat_obs\n\n    # Step 4, 5: Generate candidates, compute scores, and get labels\n    candidates = []\n    # Iterate over upper triangle to consider each unordered pair once\n    for i in range(n):\n        for j in range(i + 1, n):\n            if A_obs[i, j] == 0:\n                # Compute score s(i,j)\n                score_raw = H[i, :] @ H[j, :]\n                score = 1.0 / (1.0 + np.exp(-score_raw))\n                \n                # Get label y(i,j)\n                label = A_true[i, j]\n                \n                candidates.append({'score': score, 'label': label})\n\n    # Step 6: Compute Average Precision\n    # Sort candidates by score in descending order\n    candidates.sort(key=lambda x: x['score'], reverse=True)\n    \n    m = sum(c['label'] for c in candidates)\n    if m == 0:\n        return 0.0\n\n    ap_sum = 0.0\n    tp_count = 0\n    for k, candidate in enumerate(candidates, 1):\n        if candidate['label'] == 1:\n            tp_count += 1\n            precision_at_k = tp_count / k\n            ap_sum += precision_at_k\n            \n    return ap_sum / m\n\ndef solve():\n    \"\"\"\n    Defines the test suite, runs the GNN-based link prediction pipeline,\n    and prints the results in the specified format.\n    \"\"\"\n    \n    # Test Case 1\n    n1 = 6\n    E_obs1 = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n    E_true1 = E_obs1 + [(0, 2), (2, 4)]\n    A_obs1 = create_adj_matrix(n1, E_obs1)\n    A_true1 = create_adj_matrix(n1, E_true1)\n\n    # Test Case 2\n    n2 = 5\n    E_obs2 = [(0, 1), (1, 2), (2, 3), (3, 4)]\n    E_true2 = E_obs2 + [(1, 3)]\n    A_obs2 = create_adj_matrix(n2, E_obs2)\n    A_true2 = create_adj_matrix(n2, E_true2)\n\n    # Test Case 3\n    n3 = 6\n    # True graph is complete except for edge (0, 2)\n    all_edges_n3 = [(i, j) for i in range(n3) for j in range(i + 1, n3)]\n    E_true3_list = [edge for edge in all_edges_n3 if edge != (0, 2)]\n    A_true3 = create_adj_matrix(n3, E_true3_list)\n    # Observed graph is true graph minus edges (0, 5) and (1, 3)\n    missing_in_obs = {(0, 5), (1, 3)}\n    E_obs3_list = [edge for edge in E_true3_list if edge not in missing_in_obs]\n    A_obs3 = create_adj_matrix(n3, E_obs3_list)\n\n    test_cases = [\n        (A_obs1, A_true1),\n        (A_obs2, A_true2),\n        (A_obs3, A_true3),\n    ]\n\n    results = []\n    for A_obs, A_true in test_cases:\n        ap_score = compute_ap(A_obs, A_true)\n        results.append(ap_score)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3131905"}]}