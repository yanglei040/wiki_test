{"hands_on_practices": [{"introduction": "在多任务学习中，不同任务的梯度可能指向相互冲突的方向，产生“负迁移”或“梯度干扰”，从而减慢甚至阻碍训练进程。这个练习将带你亲手实践并深入理解这一基本挑战。你将实现并比较标准的梯度累加方法与一种“梯度整形”（gradient surgery）技术——该技术通过将冲突的梯度正交化来解决冲突——从而直接观察到管理梯度冲突如何能显著提高模型的收敛速度。[@problem_id:3155105]", "problem": "考虑一个深度学习中的双任务多任务学习问题，其中两个任务共享一个参数向量 $w \\in \\mathbb{R}^d$。每个任务 $i \\in \\{1,2\\}$ 都有一个在具有 $n_i$ 个样本的数据集 $(X_i, y_i)$ 上定义的均方误差 (MSE) 损失 $L_i(w)$，其中 $X_i \\in \\mathbb{R}^{n_i \\times d}$ 且 $y_i \\in \\mathbb{R}^{n_i}$。MSE 损失由下式给出\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2.\n$$\n梯度下降使用聚合梯度和固定的学习率 $\\eta  0$ 来更新参数向量。在标准聚合方案中，算法使用各任务梯度之和；而在正交化方案中，算法将一个任务的梯度替换为其与另一个任务梯度正交的分量，然后进行聚合。\n\n您的任务是实现以下两种训练方案，并比较它们的收敛速度：\n\n- 标准聚合梯度下降：在每次迭代 $t$ 时，计算各任务的梯度 $\\nabla L_1(w_t)$ 和 $\\nabla L_2(w_t)$，并更新\n$$\nw_{t+1} = w_t - \\eta \\left( \\nabla L_1(w_t) + \\nabla L_2(w_t) \\right).\n$$\n\n- 基于正交化的梯度手术：在每次迭代 $t$ 时，计算 $\\nabla L_1(w_t)$ 和 $\\nabla L_2(w_t)$，将任务 1 的梯度替换为其与任务 2 梯度正交的分量，然后使用修改后的任务 1 梯度和未经修改的任务 2 梯度的和进行更新。如果 $\\left\\|\\nabla L_2(w_t)\\right\\|_2 = 0$，则在聚合前保持 $\\nabla L_1(w_t)$ 不变。\n\n对于这两种方案，都从相同的初始参数向量 $w_0$ 开始，并运行迭代，直到首次满足停止准则\n$$\nL_1(w_t) + L_2(w_t) \\le \\varepsilon\n$$\n，或达到最大迭代次数 $T_{\\max}$。记录满足停止准则所用的迭代次数（如果在该限制内未满足，则记录 $T_{\\max}$）。\n\n实现程序以评估以下五个测试用例，每个用例由 $(X_1, y_1)$ 和 $(X_2, y_2)$ 指定，共享维度 $d = 2$，初始参数 $w_0$，学习率 $\\eta$，容差 $\\varepsilon$ 和最大迭代次数 $T_{\\max}$：\n\n- 测试用例 1 (中度冲突)：\n  - $d = 2$，\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$， $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$，\n  - $X_2 = \\begin{bmatrix} 2  1 \\\\ 1  2 \\\\ 1  -1 \\end{bmatrix}$， $y_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ -1 \\end{bmatrix}$。\n\n- 测试用例 2 (在 $w_0$ 处各任务梯度同向)：\n  - $d = 2$，\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$， $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$，\n  - $X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$， $y_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$。\n\n- 测试用例 3 (在 $w_0$ 处各任务梯度正交)：\n  - $d = 2$，\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$， $y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，\n  - $X_2 = \\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix}$， $y_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n- 测试用例 4 (在 $w_0$ 处各任务梯度反向平行)：\n  - $d = 2$，\n  - $X_1 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$， $y_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$，\n  - $X_2 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$， $y_2 = \\begin{bmatrix} -1 \\\\ -2 \\\\ -3 \\end{bmatrix}$。\n\n- 测试用例 5 (在 $w_0$ 处任务 2 的梯度为零)：\n  - $d = 2$，\n  - $X_1 = \\begin{bmatrix} 1  1 \\\\ 1  -1 \\end{bmatrix}$， $y_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，\n  - $X_2 = \\begin{bmatrix} 1  2 \\\\ 3  4 \\end{bmatrix}$， $y_2 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n\n所有测试的通用超参数：\n- $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，\n- $\\eta = 0.05$，\n- $\\varepsilon = 10^{-6}$，\n- $T_{\\max} = 10000$。\n\n您的程序必须对每个测试用例 $k \\in \\{1,2,3,4,5\\}$：\n- 运行标准聚合梯度下降并记录 $N^{(k)}_{\\mathrm{vanilla}}$，即达到 $L_1(w_t) + L_2(w_t) \\le \\varepsilon$ 所需的迭代次数，如果未达到则记录为 $T_{\\max}$。\n- 运行基于正交化的梯度手术（将任务 1 的梯度相对于任务 2 的梯度进行正交化）并记录 $N^{(k)}_{\\mathrm{ortho}}$，定义类似。\n- 计算整数差 $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含按测试用例索引 $k = 1,2,3,4,5$ 排序的结果，形式为方括号括起来的逗号分隔列表，即打印\n$$\n\\left[ \\Delta^{(1)}, \\Delta^{(2)}, \\Delta^{(3)}, \\Delta^{(4)}, \\Delta^{(5)} \\right].\n$$", "solution": "该问题在科学和数学上是适定的 (well-posed)，为求解提供了所有必要的信息。它基于应用于多任务学习的数值优化和线性代数的既定原则。\n\n目标是为一个双任务线性回归问题比较两种梯度下降算法的收敛速度。这种比较通过达到总损失特定容差所需的迭代次数来量化。\n\n首先，我们形式化两种算法所需的数学组件。每个任务 $i \\in \\{1, 2\\}$ 的损失函数是均方误差 (MSE)：\n$$\nL_i(w) = \\frac{1}{2 n_i} \\left\\| X_i w - y_i \\right\\|_2^2\n$$\n其中 $w \\in \\mathbb{R}^d$ 是共享参数向量，$X_i \\in \\mathbb{R}^{n_i \\times d}$ 是数据矩阵，$y_i \\in \\mathbb{R}^{n_i}$ 是任务 $i$ 的目标向量。\n\n任何梯度下降方法的核心都是损失函数的梯度。我们推导 $L_i(w)$ 关于 $w$ 的梯度。将平方范数重写为点积，$L_i(w) = \\frac{1}{2 n_i} (X_i w - y_i)^T (X_i w - y_i)$。展开得到：\n$$\nL_i(w) = \\frac{1}{2 n_i} (w^T X_i^T X_i w - 2 y_i^T X_i w + y_i^T y_i)\n$$\n使用标准矩阵微积分恒等式对 $w$ 求梯度，我们得到：\n$$\n\\nabla_w L_i(w) = \\frac{1}{2 n_i} (2 X_i^T X_i w - 2 X_i^T y_i) = \\frac{1}{n_i} X_i^T (X_i w - y_i)\n$$\n让我们将第 $t$ 次迭代时每个任务的梯度表示为 $g_1(w_t) = \\nabla_w L_1(w_t)$ 和 $g_2(w_t) = \\nabla_w L_2(w_t)$。\n\n定义了梯度之后，我们来指定两种算法方案。\n\n**1. 标准聚合梯度下降**\n这是多任务学习中的标准方法，即所有任务的梯度相加形成更新方向。更新规则是：\n$$\nw_{t+1} = w_t - \\eta \\left( g_1(w_t) + g_2(w_t) \\right)\n$$\n该方法简单，但如果任务梯度相互冲突（即指向相反方向），则可能因聚合梯度幅度过小而导致收敛缓慢。\n\n**2. 基于正交化的梯度手术**\n该方法旨在缓解梯度冲突问题。一个任务（任务 1）的梯度通过移除其与另一个任务（任务 2）梯度平行的分量来进行修改。这确保了任务 1 的更新不会干扰任务 2 的梯度指示方向。\n\n$g_1(w_t)$ 平行于 $g_2(w_t)$ 的分量通过将 $g_1(w_t)$ 投影到 $g_2(w_t)$ 上得到：\n$$\n\\text{proj}_{g_2(w_t)}(g_1(w_t)) = \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\n任务 1 的修改后梯度，记为 $g_1^{\\perp}(w_t)$，是 $g_1(w_t)$ 正交于 $g_2(w_t)$ 的分量：\n$$\ng_1^{\\perp}(w_t) = g_1(w_t) - \\text{proj}_{g_2(w_t)}(g_1(w_t)) = g_1(w_t) - \\frac{g_1(w_t) \\cdot g_2(w_t)}{\\|g_2(w_t)\\|_2^2} g_2(w_t)\n$$\n如问题所述，如果 $\\|g_2(w_t)\\|_2 = 0$，则投影未定义。在这种情况下，我们不修改任务 1 的梯度，令 $g_1^{\\perp}(w_t) = g_1(w_t)$。更新规则变为：\n$$\nw_{t+1} = w_t - \\eta \\left( g_1^{\\perp}(w_t) + g_2(w_t) \\right)\n$$\n\n**计算流程**\n对五个测试用例中的每一个和两种算法中的每一种，都执行迭代优化。\n1. 初始化参数 $w_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n2. 对于从 $0$ 到 $T_{\\max} - 1$ 的每次迭代 $t$：\n   a. 计算总损失 $L_{total}(w_t) = L_1(w_t) + L_2(w_t)$。\n   b. 检查停止准则：如果 $L_{total}(w_t) \\le \\varepsilon = 10^{-6}$，循环终止，并将所用迭代次数记录为 $t$。\n   c. 计算每个任务的梯度 $g_1(w_t)$ 和 $g_2(w_t)$。\n   d. 根据特定算法（标准或正交化）的规则计算参数更新 $w_{t+1}$。\n3. 如果循环完成而未满足停止准则，则迭代次数记录为 $T_{\\max} = 10000$。\n\n令 $N^{(k)}_{\\mathrm{vanilla}}$ 和 $N^{(k)}_{\\mathrm{ortho}}$ 分别为测试用例 $k \\in \\{1, 2, 3, 4, 5\\}$ 中标准算法和正交化算法记录的迭代次数。每个用例的最终输出是整数差 $\\Delta^{(k)} = N^{(k)}_{\\mathrm{vanilla}} - N^{(k)}_{\\mathrm{ortho}}$。该值直接比较了它们的收敛速度。", "answer": "```python\nimport numpy as np\n\ndef loss_fn(X, y, w):\n    \"\"\"Computes the Mean Squared Error loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return 0.0\n    error = X @ w - y\n    return (1.0 / (2.0 * n)) * np.dot(error, error)\n\ndef grad_fn(X, y, w):\n    \"\"\"Computes the gradient of the MSE loss.\"\"\"\n    n = X.shape[0]\n    if n == 0:\n        return np.zeros_like(w)\n    error = X @ w - y\n    return (1.0 / n) * X.T @ error\n\ndef solve():\n    \"\"\"\n    Implements and compares vanilla and orthogonalized gradient descent for\n    several multi-task learning test cases.\n    \"\"\"\n    \n    # Common hyperparameters\n    w0 = np.array([0.0, 0.0])\n    eta = 0.05\n    epsilon = 1e-6\n    T_max = 10000\n\n    # Test cases definition\n    test_cases = [\n        # Test Case 1 (moderate conflict)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[2.0, 1.0], [1.0, 2.0], [1.0, -1.0]]),\n            \"y2\": np.array([0.0, 1.0, -1.0]),\n        },\n        # Test Case 2 (aligned gradients)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Test Case 3 (orthogonal gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[0.0, 1.0], [0.0, 1.0]]),\n            \"y2\": np.array([1.0, 1.0]),\n        },\n        # Test Case 4 (anti-parallel gradients at w0)\n        {\n            \"X1\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y1\": np.array([1.0, 2.0, 3.0]),\n            \"X2\": np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n            \"y2\": np.array([-1.0, -2.0, -3.0]),\n        },\n        # Test Case 5 (zero gradient for task 2 at w0)\n        {\n            \"X1\": np.array([[1.0, 1.0], [1.0, -1.0]]),\n            \"y1\": np.array([1.0, 0.0]),\n            \"X2\": np.array([[1.0, 2.0], [3.0, 4.0]]),\n            \"y2\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        X1, y1 = case[\"X1\"], case[\"y1\"]\n        X2, y2 = case[\"X2\"], case[\"y2\"]\n\n        # Run Vanilla Aggregated Gradient Descent\n        w = np.copy(w0)\n        n_vanilla = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_vanilla = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            w -= eta * (g1 + g2)\n\n        # Run Orthogonalization-based Gradient Surgery\n        w = np.copy(w0)\n        n_ortho = T_max\n        for t in range(T_max):\n            loss1 = loss_fn(X1, y1, w)\n            loss2 = loss_fn(X2, y2, w)\n            if loss1 + loss2 = epsilon:\n                n_ortho = t\n                break\n\n            g1 = grad_fn(X1, y1, w)\n            g2 = grad_fn(X2, y2, w)\n            \n            g2_norm_sq = np.dot(g2, g2)\n            \n            # Use a small tolerance for the zero-norm check for floating point stability\n            if g2_norm_sq > 1e-12:\n                g1_ortho = g1 - (np.dot(g1, g2) / g2_norm_sq) * g2\n            else:\n                g1_ortho = g1\n\n            w -= eta * (g1_ortho + g2)\n            \n        delta = n_vanilla - n_ortho\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3155105"}, {"introduction": "除了方向冲突，梯度的大小同样是一大挑战，尤其是当不同任务的数据集规模不均衡时。拥有更多数据的任务往往会产生更大的梯度，从而在学习过程中占据主导地位，使得模型忽略那些数据较少的任务。本练习将让你模拟这一场景，并实现“反频率加权”（inverse-frequency weighting），这是一种经典的启发式策略，用于重新平衡各任务的重要性。通过这个实践，你将看到策略性的损失加权如何带来更公平、更全面的模型性能。[@problem_id:3155106]", "problem": "考虑一个简化的双任务多任务学习（MTL）二元分类场景，其共享参数通过经验风险最小化（ERM）进行更新。聚合目标函数使用任务权重和小批量采样。令共享参数向量表示为 $\\theta$。现有两个任务 $t \\in \\{A, B\\}$，每个任务有大小为 $n_t$ 的小批量数据，沿共享更新方向的单样本梯度幅度为 $m_t$，以及一个对齐因子 $a_t \\in [0,1]$，该因子量化了共享更新转化为任务 $t$ 性能的有效程度。假设在学习率为 $\\eta$ 的单个梯度步骤中，共享更新的幅度（相关方向上更新范数的标量代理）由以下模型给出\n$$\nU(w) \\equiv \\eta \\sum_{t \\in \\{A,B\\}} w_t \\, n_t \\, m_t,\n$$\n其中 $w_t$ 表示任务权重。在均匀权重下，设置 $w_A = w_B = 1$。在反频率加权下，设置 $w_t = \\frac{1}{n_t}$。对于此步骤，假设 $\\eta$ 等于 $1$。\n\n每个任务 $t$ 都有一个初始混淆矩阵，其计数 $(\\mathrm{TP}_t, \\mathrm{FP}_t, \\mathrm{TN}_t, \\mathrm{FN}_t)$ 的总和为小批量大小 $n_t$。共享更新幅度的正向步骤倾向于增加预测的正例数，这对于每个任务 $t$，会根据灵敏度 $s^{\\mathrm{TP}}_t$ 和 $s^{\\mathrm{FP}}_t$（以每单位更新的计数为单位）来移动计数。每个任务的有效步骤为\n$$\n\\Delta_t \\equiv a_t \\, U(w).\n$$\n对于每个任务 $t$，混淆矩阵更新如下：\n- $\\Delta \\mathrm{TP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{TP}}_t \\, \\Delta_t\\right)$,\n- $\\Delta \\mathrm{FP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{FP}}_t \\, \\Delta_t\\right)$,\n- $\\mathrm{TP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{TP}_t + \\Delta \\mathrm{TP}_t, \\, 0, \\, \\mathrm{TP}_t + \\mathrm{FN}_t\\right)$,\n- $\\mathrm{FP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{FP}_t + \\Delta \\mathrm{FP}_t, \\, 0, \\, \\mathrm{FP}_t + \\mathrm{TN}_t\\right)$,\n- $\\mathrm{FN}_t^{\\mathrm{new}} = (\\mathrm{TP}_t + \\mathrm{FN}_t) - \\mathrm{TP}_t^{\\mathrm{new}}$,\n- $\\mathrm{TN}_t^{\\mathrm{new}} = (\\mathrm{FP}_t + \\mathrm{TN}_t) - \\mathrm{FP}_t^{\\mathrm{new}}$,\n其中 $\\mathrm{round}(\\cdot)$ 表示四舍五入到最近的整数，$\\mathrm{clip}(x, a, b)$ 将 $x$ 限制在范围 $[a,b]$ 内。这会保留类别总数并确保计数为有效的非负数。\n\n定义一个类别的F1分数（F1-score）为精确率（precision）和召回率（recall）的调和平均值。对于任务 $t$ 的正类，\n$$\n\\mathrm{precision}^+_t = \\frac{\\mathrm{TP}^{\\mathrm{new}}_t}{\\mathrm{TP}^{\\mathrm{new}}_t + \\mathrm{FP}^{\\mathrm{new}}_t}, \\quad\n\\mathrm{recall}^+_t = \\frac{\\mathrm{TP}^{\\mathrm{new}}_t}{\\mathrm{TP}^{\\mathrm{new}}_t + \\mathrm{FN}^{\\mathrm{new}}_t},\n$$\n约定任何分母为零的比率都定义为 $0$。那么\n$$\n\\mathrm{F1}^+_t = \n\\begin{cases}\n\\frac{2 \\cdot \\mathrm{precision}^+_t \\cdot \\mathrm{recall}^+_t}{\\mathrm{precision}^+_t + \\mathrm{recall}^+_t},  \\text{若 } \\mathrm{precision}^+_t + \\mathrm{recall}^+_t  0, \\\\\n0,  \\text{其他情况。}\n\\end{cases}\n$$\n对于负类（将“负”视作正标签），相应的计数为 $\\mathrm{TP}^{-}_t = \\mathrm{TN}^{\\mathrm{new}}_t$, $\\mathrm{FP}^{-}_t = \\mathrm{FN}^{\\mathrm{new}}_t$, $\\mathrm{FN}^{-}_t = \\mathrm{FP}^{\\mathrm{new}}_t$, 以及 $\\mathrm{TN}^{-}_t = \\mathrm{TP}^{\\mathrm{new}}_t$，得出\n$$\n\\mathrm{precision}^{-}_t = \\frac{\\mathrm{TN}^{\\mathrm{new}}_t}{\\mathrm{TN}^{\\mathrm{new}}_t + \\mathrm{FN}^{\\mathrm{new}}_t}, \\quad\n\\mathrm{recall}^{-}_t = \\frac{\\mathrm{TN}^{\\mathrm{new}}_t}{\\mathrm{TN}^{\\mathrm{new}}_t + \\mathrm{FP}^{\\mathrm{new}}_t},\n$$\n并且\n$$\n\\mathrm{F1}^{-}_t = \n\\begin{cases}\n\\frac{2 \\cdot \\mathrm{precision}^{-}_t \\cdot \\mathrm{recall}^{-}_t}{\\mathrm{precision}^{-}_t + \\mathrm{recall}^{-}_t},  \\text{若 } \\mathrm{precision}^{-}_t + \\mathrm{recall}^{-}_t  0, \\\\\n0,  \\text{其他情况。}\n\\end{cases}\n$$\n任务 $t$ 的宏平均F1分数则为\n$$\n\\mathrm{MacroF1}_t = \\frac{\\mathrm{F1}^+_t + \\mathrm{F1}^{-}_t}{2}.\n$$\n定义跨任务宏平均为\n$$\n\\mathrm{CrossMacroF1} = \\frac{\\mathrm{MacroF1}_A + \\mathrm{MacroF1}_B}{2}.\n$$\n\n为了分析标签频率不匹配的影响，计算两种加权方案下任务对共享更新的贡献比率如下\n$$\nR_{\\mathrm{unweighted}} = \\frac{n_A \\, m_A}{n_B \\, m_B}, \\quad\nR_{\\mathrm{weighted}} = \\frac{w_A \\, n_A \\, m_A}{w_B \\, n_B \\, m_B},\n$$\n对于无加权情况，$w_A = w_B = 1$；对于反频率加权情况，$w_t = \\frac{1}{n_t}$。\n\n您的任务是实现一个程序，对下面的每个测试用例执行以下操作：\n- 计算定义的 $R_{\\mathrm{unweighted}}$ 和 $R_{\\mathrm{weighted}}$。\n- 在两种加权方案下应用一个共享更新步骤，以获得任务 $A$ 和 $B$ 的更新后的混淆矩阵。\n- 计算两种方案下的 $\\mathrm{CrossMacroF1}$。\n- 如果反频率加权与无加权情况相比，严格增加了 $\\mathrm{CrossMacroF1}$，则输出布尔值 $\\mathrm{True}$，否则输出 $\\mathrm{False}$。\n\n测试套件（所有计数和参数均为整数或实数，视情况而定）：\n- 测试用例 1：\n  - $n_A = 100$, $n_B = 100$.\n  - $m_A = 1.0$, $m_B = 1.0$.\n  - $a_A = 0.8$, $a_B = 0.8$.\n  - 任务 $A$ 的初始混淆矩阵：$(\\mathrm{TP}_A, \\mathrm{FP}_A, \\mathrm{TN}_A, \\mathrm{FN}_A) = (50, 10, 30, 10)$，灵敏度 $s^{\\mathrm{TP}}_A = 0.05$, $s^{\\mathrm{FP}}_A = 0.02$。\n  - 任务 $B$ 的初始混淆矩阵：$(\\mathrm{TP}_B, \\mathrm{FP}_B, \\mathrm{TN}_B, \\mathrm{FN}_B) = (45, 15, 25, 15)$，灵敏度 $s^{\\mathrm{TP}}_B = 0.06$, $s^{\\mathrm{FP}}_B = 0.03$。\n- 测试用例 2：\n  - $n_A = 1000$, $n_B = 100$.\n  - $m_A = 0.8$, $m_B = 1.2$.\n  - $a_A = 0.6$, $a_B = 0.9$.\n  - 任务 $A$ 的初始混淆矩阵：$(\\mathrm{TP}_A, \\mathrm{FP}_A, \\mathrm{TN}_A, \\mathrm{FN}_A) = (400, 100, 450, 50)$，灵敏度 $s^{\\mathrm{TP}}_A = 0.02$, $s^{\\mathrm{FP}}_A = 0.01$。\n  - 任务 $B$ 的初始混淆矩阵：$(\\mathrm{TP}_B, \\mathrm{FP}_B, \\mathrm{TN}_B, \\mathrm{FN}_B) = (50, 20, 20, 10)$，灵敏度 $s^{\\mathrm{TP}}_B = 0.07$, $s^{\\mathrm{FP}}_B = 0.04$。\n- 测试用例 3：\n  - $n_A = 10000$, $n_B = 10$.\n  - $m_A = 0.5$, $m_B = 1.5$.\n  - $a_A = 0.5$, $a_B = 1.0$.\n  - 任务 $A$ 的初始混淆矩阵：$(\\mathrm{TP}_A, \\mathrm{FP}_A, \\mathrm{TN}_A, \\mathrm{FN}_A) = (4500, 500, 5000, 0)$，灵敏度 $s^{\\mathrm{TP}}_A = 0.005$, $s^{\\mathrm{FP}}_A = 0.002$。\n  - 任务 $B$ 的初始混淆矩阵：$(\\mathrm{TP}_B, \\mathrm{FP}_B, \\mathrm{TN}_B, \\mathrm{FN}_B) = (4, 1, 4, 1)$，灵敏度 $s^{\\mathrm{TP}}_B = 0.2$, $s^{\\mathrm{FP}}_B = 0.1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的扁平列表，按顺序为每个测试用例依次附加以下值：$R_{\\mathrm{unweighted}}$、$R_{\\mathrm{weighted}}$、均匀权重下的 $\\mathrm{CrossMacroF1}$、反频率权重下的 $\\mathrm{CrossMacroF1}$，以及布尔值改进指示符。例如，输出必须看起来像 $[r_1, r'_1, m_1, m'_1, b_1, r_2, r'_2, m_2, m'_2, b_2, r_3, r'_3, m_3, m'_3, b_3]$，其中每个 $r_i$ 和 $r'_i$ 是浮点数，每个 $m_i$ 和 $m'_i$ 是浮点数，每个 $b_i$ 是布尔值。不应打印任何额外文本。", "solution": "问题陈述已经过评估并被确定为**有效**。这是一个基于简化的多任务学习模型的、形式良好、自包含且具有科学依据的计算问题。所有必要的参数、公式和条件都已提供，从而可以得到唯一且可验证的解。该问题没有矛盾、歧义和非形式化的概念。\n\n求解过程首先建立一个系统化的计算程序，用以计算给定加权方案下的 `CrossMacroF1` 分数。然后，对每个测试用例应用该程序两次——一次使用均匀权重，一次使用反频率权重——以确定后者是否带来了严格的改进。\n\n计算的核心涉及双任务学习系统的一个更新步骤。程序如下：\n\n1.  **任务加权与共享更新：**\n    每个任务对共享参数更新的贡献取决于一个加权方案。问题定义了两种方案：均匀加权和反频率加权。\n    -   对于均匀加权，任务权重为 $w_A = 1$ 和 $w_B = 1$。\n    -   对于反频率加权，权重与小批量大小成反比：$w_t = \\frac{1}{n_t}$。\n\n    共享更新的总幅度 $U(w)$ 是任务特定梯度贡献的加权和，学习率为 $\\eta=1$：\n    $$U(w) = \\sum_{t \\in \\{A,B\\}} w_t \\, n_t \\, m_t$$\n    其中 $n_t$ 是小批量大小，$m_t$ 是任务 $t$ 的单样本梯度幅度。\n\n2.  **每任务有效更新：**\n    共享更新对每个任务的影响并不相同。一个对齐因子 $a_t \\in [0, 1]$ 模拟了共享更新转化为任务 $t$ 性能提升的有效程度。每任务的有效步骤 $\\Delta_t$ 为：\n    $$\\Delta_t = a_t \\, U(w)$$\n\n3.  **混淆矩阵更新：**\n    有效步骤 $\\Delta_t$ 会改变每个任务初始混淆矩阵 $(\\mathrm{TP}_t, \\mathrm{FP}_t, \\mathrm{TN}_t, \\mathrm{FN}_t)$ 中的计数。真正例和假正例计数的改变由任务特定的灵敏度 $s^{\\mathrm{TP}}_t$ 和 $s^{\\mathrm{FP}}_t$ 决定：\n    $$ \\Delta \\mathrm{TP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{TP}}_t \\, \\Delta_t\\right) $$\n    $$ \\Delta \\mathrm{FP}_t = \\mathrm{round}\\!\\left(s^{\\mathrm{FP}}_t \\, \\Delta_t\\right) $$\n    其中 $\\mathrm{round}(\\cdot)$ 四舍五入到最近的整数。\n\n    更新后的计数（用上标 `new` 表示）通过应用这些变化并使用裁剪函数确保它们保持在有效范围内来计算。实际正例的总数（$P_t = \\mathrm{TP}_t + \\mathrm{FN}_t$）和实际负例的总数（$N_t = \\mathrm{FP}_t + \\mathrm{TN}_t$）是守恒的。\n    $$ \\mathrm{TP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{TP}_t + \\Delta \\mathrm{TP}_t, \\, 0, \\, P_t\\right) $$\n    $$ \\mathrm{FP}_t^{\\mathrm{new}} = \\mathrm{clip}\\!\\left(\\mathrm{FP}_t + \\Delta \\mathrm{FP}_t, \\, 0, \\, N_t\\right) $$\n    剩余的混淆矩阵元素由类别总数守恒得出：\n    $$ \\mathrm{FN}_t^{\\mathrm{new}} = P_t - \\mathrm{TP}_t^{\\mathrm{new}} $$\n    $$ \\mathrm{TN}_t^{\\mathrm{new}} = N_t - \\mathrm{FP}_t^{\\mathrm{new}} $$\n\n4.  **F1分数计算：**\n    性能通过宏平均F1分数来衡量。这需要计算正类和负类的F1分数。对于任何类别，给定其对应的真正例（$tp$）、假正例（$fp$）和假负例（$fn$），精确率和召回率为：\n    $$ \\mathrm{precision} = \\frac{tp}{tp + fp}, \\quad \\mathrm{recall} = \\frac{tp}{tp + fn} $$\n    约定如果分母为 $0$，则比率为 $0$。F1分数是精确率和召回率的调和平均值：\n    $$ \\mathrm{F1} = \\begin{cases} \\frac{2 \\cdot \\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}},  \\text{若 } \\mathrm{precision} + \\mathrm{recall}  0 \\\\ 0,  \\text{其他情况} \\end{cases} $$\n    -   对于正类（$\\mathrm{F1}_t^+$），各项为 $tp = \\mathrm{TP}_t^{\\mathrm{new}}$，$fp = \\mathrm{FP}_t^{\\mathrm{new}}$，以及 $fn = \\mathrm{FN}_t^{\\mathrm{new}}$。\n    -   对于负类（$\\mathrm{F1}_t^-$），各项为 $tp = \\mathrm{TN}_t^{\\mathrm{new}}$，$fp = \\mathrm{FN}_t^{\\mathrm{new}}$，以及 $fn = \\mathrm{FP}_t^{\\mathrm{new}}$。\n\n5.  **聚合性能指标：**\n    每任务的宏平均F1分数是正类和负类F1分数的平均值：\n    $$ \\mathrm{MacroF1}_t = \\frac{\\mathrm{F1}_t^+ + \\mathrm{F1}_t^-}{2} $$\n    最终的性能指标是跨任务宏平均，它对两个任务的宏F1分数进行平均：\n    $$ \\mathrm{CrossMacroF1} = \\frac{\\mathrm{MacroF1}_A + \\mathrm{MacroF1}_B}{2} $$\n\n6.  **分析比率与最终比较：**\n    为分析任务贡献的平衡性，计算两个比率：\n    $$ R_{\\mathrm{unweighted}} = \\frac{n_A \\, m_A}{n_B \\, m_B} $$\n    $$ R_{\\mathrm{weighted}} = \\frac{w_A \\, n_A \\, m_A}{w_B \\, n_B \\, m_B} = \\frac{m_A}{m_B} \\quad (\\text{对于反频率加权}) $$\n    对于每个测试用例，我们计算 $R_{\\mathrm{unweighted}}$、$R_{\\mathrm{weighted}}$，以及均匀和反频率方案下的 $\\mathrm{CrossMacroF1}$。然后确定一个布尔值：如果反频率加权分数严格大于均匀加权分数，则为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。对每个测试用例都实现了这整个计算序列。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the multi-task learning problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"nA\": 100, \"nB\": 100,\n            \"mA\": 1.0, \"mB\": 1.0,\n            \"aA\": 0.8, \"aB\": 0.8,\n            \"cm_A\": (50, 10, 30, 10), \"sens_A\": (0.05, 0.02),\n            \"cm_B\": (45, 15, 25, 15), \"sens_B\": (0.06, 0.03),\n        },\n        {\n            \"nA\": 1000, \"nB\": 100,\n            \"mA\": 0.8, \"mB\": 1.2,\n            \"aA\": 0.6, \"aB\": 0.9,\n            \"cm_A\": (400, 100, 450, 50), \"sens_A\": (0.02, 0.01),\n            \"cm_B\": (50, 20, 20, 10), \"sens_B\": (0.07, 0.04),\n        },\n        {\n            \"nA\": 10000, \"nB\": 10,\n            \"mA\": 0.5, \"mB\": 1.5,\n            \"aA\": 0.5, \"aB\": 1.0,\n            \"cm_A\": (4500, 500, 5000, 0), \"sens_A\": (0.005, 0.002),\n            \"cm_B\": (4, 1, 4, 1), \"sens_B\": (0.2, 0.1),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # Calculate R_unweighted and R_weighted\n        r_unweighted = (case[\"nA\"] * case[\"mA\"]) / (case[\"nB\"] * case[\"mB\"])\n        r_weighted = case[\"mA\"] / case[\"mB\"]\n\n        # Calculate CrossMacroF1 for both weighting schemes\n        cmf1_uniform = run_simulation(case, \"uniform\")\n        cmf1_inv_freq = run_simulation(case, \"inv_freq\")\n\n        # Determine if inverse-frequency weighting provides a strict improvement\n        is_improvement = cmf1_inv_freq > cmf1_uniform\n\n        # Append results for this test case\n        results.extend([r_unweighted, r_weighted, cmf1_uniform, cmf1_inv_freq, is_improvement])\n\n    # Format and print the final output string\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_f1_score(tp, fp, tn, fn):\n    \"\"\"\n    Calculates the F1-score for a given set of confusion matrix counts.\n    Handles division by zero as per the problem specification.\n    \"\"\"\n    prec_denom = tp + fp\n    rec_denom = tp + fn\n\n    precision = tp / prec_denom if prec_denom > 0 else 0.0\n    recall = tp / rec_denom if rec_denom > 0 else 0.0\n\n    f1_denom = precision + recall\n    f1 = (2 * precision * recall) / f1_denom if f1_denom > 0 else 0.0\n\n    return f1\n\ndef run_simulation(params, weighting_scheme):\n    \"\"\"\n    Runs the simulation for a single test case and weighting scheme.\n    Returns the final CrossMacroF1 score.\n    \"\"\"\n    # Unpack parameters\n    nA, nB = params[\"nA\"], params[\"nB\"]\n    mA, mB = params[\"mA\"], params[\"mB\"]\n    aA, aB = params[\"aA\"], params[\"aB\"]\n    tpA, fpA, tnA, fnA = params[\"cm_A\"]\n    s_tpA, s_fpA = params[\"sens_A\"]\n    tpB, fpB, tnB, fnB = params[\"cm_B\"]\n    s_tpB, s_fpB = params[\"sens_B\"]\n    \n    # 1. Task Weighting\n    if weighting_scheme == \"uniform\":\n        wA, wB = 1.0, 1.0\n    elif weighting_scheme == \"inv_freq\":\n        wA, wB = 1.0 / nA, 1.0 / nB\n    else:\n        raise ValueError(\"Invalid weighting scheme\")\n\n    # 2. Shared Update Magnitude\n    U = (wA * nA * mA) + (wB * nB * mB) # eta is 1\n\n    # 3. Per-Task Effective Update\n    delta_A = aA * U\n    delta_B = aB * U\n\n    # --- Task A Calculations ---\n    # 4. Update Confusion Matrix for Task A\n    delta_tpA = np.round(s_tpA * delta_A)\n    delta_fpA = np.round(s_fpA * delta_A)\n    \n    total_pos_A = tpA + fnA\n    total_neg_A = fpA + tnA\n\n    tpA_new = np.clip(tpA + delta_tpA, 0, total_pos_A)\n    fpA_new = np.clip(fpA + delta_fpA, 0, total_neg_A)\n    fnA_new = total_pos_A - tpA_new\n    tnA_new = total_neg_A - fpA_new\n\n    # 5. F1-Score Calculation for Task A\n    f1_pos_A = compute_f1_score(tpA_new, fpA_new, tnA_new, fnA_new)\n    # For negative class, TN becomes TP, FN becomes FP, FP becomes FN\n    f1_neg_A = compute_f1_score(tnA_new, fnA_new, tpA_new, fpA_new)\n    \n    # 6. Aggregate Performance for Task A\n    macro_f1_A = (f1_pos_A + f1_neg_A) / 2.0\n\n    # --- Task B Calculations ---\n    # 4. Update Confusion Matrix for Task B\n    delta_tpB = np.round(s_tpB * delta_B)\n    delta_fpB = np.round(s_fpB * delta_B)\n\n    total_pos_B = tpB + fnB\n    total_neg_B = fpB + tnB\n\n    tpB_new = np.clip(tpB + delta_tpB, 0, total_pos_B)\n    fpB_new = np.clip(fpB + delta_fpB, 0, total_neg_B)\n    fnB_new = total_pos_B - tpB_new\n    tnB_new = total_neg_B - fpB_new\n\n    # 5. F1-Score Calculation for Task B\n    f1_pos_B = compute_f1_score(tpB_new, fpB_new, tnB_new, fnB_new)\n    # For negative class, TN becomes TP, FN becomes FP, FP becomes FN\n    f1_neg_B = compute_f1_score(tnB_new, fnB_new, tpB_new, fpB_new)\n\n    # 6. Aggregate Performance for Task B\n    macro_f1_B = (f1_pos_B + f1_neg_B) / 2.0\n\n    # 7. Final Cross-Task Metric\n    cross_macro_f1 = (macro_f1_A + macro_f1_B) / 2.0\n    \n    return cross_macro_f1\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3155106"}, {"introduction": "在探索了针对梯度冲突和不平衡问题的特定修正方法后，我们将转向一个植根于多目标优化理论的、更具原则性的解决方案。本练习将介绍帕累托最优（Pareto optimality）的概念以及多梯度下降算法（Multiple Gradient Descent Algorithm, MGDA）。MGDA旨在寻找一个能代表所有任务之间最佳折衷的更新方向。通过实现MGDA，你将学会如何求解梯度的最优凸组合，从而从简单的启发式方法迈向多任务优化的基础策略。[@problem_id:3155072]", "problem": "给定一个包含 $T$ 个任务的可微多任务学习问题，每个任务 $t \\in \\{1,\\dots,T\\}$ 都有一个标量损失 $L_t(\\mathbf{w})$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 是一个共享参数向量。每个任务在 $\\mathbf{w}$ 处的梯度表示为 $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) \\in \\mathbb{R}^d$。如果一个点 $\\mathbf{w}^\\star$ 满足以下条件，则称其为帕累托最优：不存在另一个点 $\\mathbf{w}$，使得对所有 $t$ 都有 $L_t(\\mathbf{w}) \\le L_t(\\mathbf{w}^\\star)$，且至少对一个 $t'$ 有 $L_{t'}(\\mathbf{w})  L_{t'}(\\mathbf{w}^\\star)$。基础优化理论指出，对于无约束、可微的优化问题，标量目标 $s(\\mathbf{w})$ 的任何局部极小值点都必须满足一阶必要条件 $\\nabla s(\\mathbf{w}) = \\mathbf{0}$。请利用此结论以及多目标优化中的其他公认事实来论证帕累托平稳性。\n\n任务 A（理论）。从上述定义出发，仅使用凸锥分离定理和无约束可微优化的一阶必要条件等基本原理，推导以下帕累托平稳性陈述：如果 $\\mathbf{w}^\\star$ 是帕累托最优的，则存在系数 $\\alpha_t \\ge 0$ 且 $\\sum_{t=1}^T \\alpha_t = 1$，使得\n$$\n\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n$$\n解释为什么这个条件可以从作用于 $\\{L_t(\\mathbf{w})\\}_{t=1}^T$ 像集上的支撑超平面/分离论证以及应用于适当标量化的一阶必要条件推导得出。\n\n任务 B（算法）。在给定的 $\\mathbf{w}$ 处，多梯度下降算法（MGDA）通过求解以下二次规划问题来选择系数 $\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_T)$：\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^T} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}) \\right\\|_2^2 \\quad \\text{约束条件为} \\quad \\alpha_t \\ge 0 \\; \\text{对所有 } t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\n这将产生任务梯度的最小范数凸组合。实现一个程序，针对下面的每个测试用例，通过使用闭式解（针对 $T=2$）和小规模情况推理（针对 $T=3$），精确地求解此问题以计算 $\\boldsymbol{\\alpha}$，要求不产生数值不稳定性，也不依赖外部数据。\n\n小模型和梯度。考虑一个共享线性模型，其参数为 $\\mathbf{w} \\in \\mathbb{R}^2$，以及任务特定的平方损失\n$$\nL_t(\\mathbf{w}) = \\frac{1}{2} \\| \\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t \\|_2^2,\n$$\n其中 $\\mathbf{A}_t \\in \\mathbb{R}^{2 \\times 2}$ 且 $\\mathbf{b}_t \\in \\mathbb{R}^2$。梯度为\n$$\n\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w}) = \\mathbf{A}_t^\\top (\\mathbf{A}_t \\mathbf{w} - \\mathbf{b}_t).\n$$\n在下面的所有测试用例中，对每个任务使用 $\\mathbf{w} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$ 和 $\\mathbf{A}_t = \\mathbf{I}_2$，因此 $\\mathbf{g}_t(\\mathbf{w}) = -\\mathbf{b}_t$。\n\n测试套件。对于每个测试用例，您将获得每个任务对应的 $\\mathbf{b}_t$ 向量列表，并且必须计算 MGDA 系数 $\\boldsymbol{\\alpha}$。\n\n- 情况 1（两个任务，存在内部解）：$T=2$，$\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{b}_2 = \\begin{bmatrix} -\\frac{1}{2} \\\\ 0 \\end{bmatrix}$。\n\n- 情况 2（两个任务，共线同向，边界解）：$T=2$，$\\mathbf{b}_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{b}_2 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$。\n\n- 情况 3（三个任务，所有系数均为正的严格内部解）：$T=3$，$\\mathbf{b}_1 = \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}$，$\\mathbf{b}_2 = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}$，$\\mathbf{b}_3 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$。\n\n对程序的要求。\n\n- 您的程序必须通过精确求解 $T \\in \\{2,3\\}$ 的 MGDA 二次规划问题来计算每个情况的 $\\boldsymbol{\\alpha}$，对 $T=2$ 使用闭式公式，对 $T=3$ 使用小规模情况枚举（如果可行，则为内部重心解；否则沿边和顶点进行最小化）。\n\n- 最终的数值答案必须四舍五入到 6 位小数。\n\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应测试用例的系数列表。例如，输出可能看起来像 $[[a_{1},a_{2}],[b_{1},b_{2}],[c_{1},c_{2},c_{3}]]$，不含空格。请将 $a_i,b_i,c_i$ 替换为您计算并四舍五入到 6 位小数的浮点数。\n\n- 唯一可接受的输出是浮点数列表；列表中不能包含布尔值或字符串。\n\n- 您的程序必须是自包含的，不需要任何输入。它不能读取或写入文件。它必须实现所描述的 MGDA 求解逻辑，并计算上述三个测试用例的系数。", "solution": "根据要求，问题分析分为两部分：帕累托平稳性条件的理论推导（任务 A）和求解多梯度下降算法（MGDA）系数的算法实现（任务 B）。\n\n### 任务 A：帕累托平稳性条件的推导\n\n此任务要求推导帕累托最优性的一阶必要条件。该条件也称为帕累托平稳性。推导过程依赖于帕累托最优性的定义和凸分析的基本结果，特别是分离定理。\n\n设 $T$ 个可微的任务特定损失函数集合为 $\\{L_t(\\mathbf{w})\\}_{t=1}^T$，其中 $\\mathbf{w} \\in \\mathbb{R}^d$ 是共享参数向量。每个损失的梯度为 $\\mathbf{g}_t(\\mathbf{w}) = \\nabla_{\\mathbf{w}} L_t(\\mathbf{w})$。\n\n1.  **下降方向的条件**：\n    如果沿向量 $\\mathbf{d} \\in \\mathbb{R}^d$ 方向前进一小步可以减少任务 $t$ 在点 $\\mathbf{w}$ 处的损失 $L_t$，则称 $\\mathbf{d}$ 为一个下降方向。基于一阶泰勒展开，该条件为 $\\mathbf{g}_t(\\mathbf{w})^\\top \\mathbf{d}  0$。\n\n2.  **帕累托最优性与下降方向**：\n    根据定义，如果不存在任何其他点 $\\mathbf{w}$，在不使任何其他任务恶化的情况下，至少对一个任务有严格的改善，则点 $\\mathbf{w}^\\star$ 是帕累托最优的。在局部改进方面，这意味着从 $\\mathbf{w}^\\star$ 出发，不可能存在一个下降方向 $\\mathbf{d}$，它能在不恶化任何其他任务损失的情况下，至少改善一个任务的损失。\n    形式上，如果 $\\mathbf{w}^\\star$ 是一个帕累托最优点，则不存在向量 $\\mathbf{d} \\in \\mathbb{R}^d$ 使得：\n    $$\n    \\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d} \\le 0 \\quad \\text{对所有 } t \\in \\{1, \\dots, T\\}\n    $$\n    且\n    $$\n    \\mathbf{g}_{t'}(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0 \\quad \\text{对至少一个 } t' \\in \\{1, \\dots, T\\}.\n    $$\n    如果存在这样的方向 $\\mathbf{d}$，则可以从 $\\mathbf{w}^\\star$ 沿该方向前进一小步 $\\epsilon\\mathbf{d}$ (其中 $\\epsilon > 0$)，得到一个新点 $\\mathbf{w}' = \\mathbf{w}^\\star + \\epsilon\\mathbf{d}$，该点将帕累托优于 $\\mathbf{w}^\\star$，这与其最优性相矛盾。一个更简单的直接推论是：不存在 $\\mathbf{d}$ 使得对所有 $t$ 都有 $\\mathbf{g}_t(\\mathbf{w}^\\star)^\\top \\mathbf{d}  0$。\n\n3.  **择一性定理（Gordan 引理）的应用**：\n    这种公共下降方向的不存在性可以通过择一性定理来正式确立。Gordan 引理是凸分析中的一个结果，它指出，对于任何向量集 $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_T\\}$，以下两个陈述中必有一个且仅有一个为真：\n    (i) 存在一个向量 $\\mathbf{d}$ 使得对所有 $t=1, \\dots, T$ 都有 $\\mathbf{v}_t^\\top \\mathbf{d}  0$。\n    (ii) 零向量 $\\mathbf{0}$ 是这些向量的非平凡非负组合，即存在不全为零的系数 $\\lambda_t \\ge 0$，使得 $\\sum_{t=1}^T \\lambda_t \\mathbf{v}_t = \\mathbf{0}$。\n    等价地，(ii) 指出 $\\mathbf{0}$ 位于 $\\{\\mathbf{v}_t\\}$ 的凸锥中。\n    该定理的一个更通用的版本涵盖了步骤 2 中的混合不等式（$\\le$ 和 $$），并得出相同的结论。\n\n4.  **推导帕累托平稳性条件**：\n    将此定理应用于我们的梯度集 $\\{\\mathbf{g}_t(\\mathbf{w}^\\star)\\}$，已确立的公共下降方向的不存在性（排除了陈述 (i)）意味着陈述 (ii) 必须为真。因此，必须存在系数 $\\lambda_t \\ge 0$（对于 $t=1, \\dots, T$），其中至少有一个 $\\lambda_t > 0$，使得：\n    $$\n    \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}.\n    $$\n    由于系数 $\\lambda_t$ 不全为零，它们的和 $S = \\sum_{j=1}^T \\lambda_j$ 严格为正。我们可以通过定义 $\\alpha_t = \\lambda_t / S$ 来归一化这些系数。这些新系数满足：\n    - 对所有 $t$，$\\alpha_t \\ge 0$。\n    - $\\sum_{t=1}^T \\alpha_t = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t = \\frac{S}{S} = 1$。\n    - $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\sum_{t=1}^T \\lambda_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\frac{1}{S} \\mathbf{0} = \\mathbf{0}$。\n    至此，帕累托平稳性条件的推导完成：\n    $$\n    \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}, \\quad \\text{其中 } \\alpha_t \\ge 0, \\sum_{t=1}^T \\alpha_t = 1.\n    $$\n\n5.  **与标量化和支撑超平面的联系**：\n    条件 $\\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t(\\mathbf{w}^\\star) = \\mathbf{0}$ 正是点 $\\mathbf{w}^\\star$ 成为标量化目标函数 $S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t L_t(\\mathbf{w})$ 局部极小值点的一阶必要条件。该标量化损失的梯度为 $\\nabla S(\\mathbf{w}) = \\sum_{t=1}^T \\alpha_t \\nabla L_t(\\mathbf{w})$。\n    在由 $(L_1, \\dots, L_T)$ 张成的损失空间中，从几何角度看，向量 $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_T)$ 定义了一个超平面。帕累托平稳性条件意味着，该超平面是可达损失向量集合在点 $(L_1(\\mathbf{w}^\\star), \\dots, L_T(\\mathbf{w}^\\star))$ 处的支撑超平面。\n\n### 任务 B：MGDA 算法实现\n\n目标是求解关于 $\\boldsymbol{\\alpha}$ 的二次规划（QP）问题：\n$$\n\\min_{\\boldsymbol{\\alpha}} \\; \\frac{1}{2} \\left\\| \\sum_{t=1}^T \\alpha_t \\, \\mathbf{g}_t \\right\\|_2^2 \\quad \\text{s.t.} \\quad \\alpha_t \\ge 0 \\; \\forall t, \\; \\sum_{t=1}^T \\alpha_t = 1.\n$$\n梯度由 $\\mathbf{g}_t = -\\mathbf{b}_t$ 给出。\n\n#### 情况 $T=2$\n问题是最小化 $f(\\alpha_1, \\alpha_2) = \\frac{1}{2}\\|\\alpha_1 \\mathbf{g}_1 + \\alpha_2 \\mathbf{g}_2\\|_2^2$，约束条件为 $\\alpha_1 + \\alpha_2 = 1$ 和 $\\alpha_1, \\alpha_2 \\ge 0$。代入 $\\alpha_2 = 1-\\alpha_1$ 将问题简化为最小化一个关于 $\\alpha_1 \\in [0, 1]$ 的二次函数。对 $\\alpha_1$ 求导并令其为零，得到无约束极小值点：\n$$\n\\alpha_1^* = \\frac{\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1)}{\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2}.\n$$\n有约束问题的解可以通过将 $\\alpha_1^*$ 投影到区间 $[0, 1]$ 上得到。因此，$\\alpha_1 = \\text{max}(0, \\text{min}(1, \\alpha_1^*))$，且 $\\alpha_2 = 1 - \\alpha_1$。\n\n- **情况 1：** $\\mathbf{g}_1 = [-1, 0]^\\top$，$\\mathbf{g}_2 = [0.5, 0]^\\top$。\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[-1.5, 0]^\\top\\|_2^2 = 2.25$。\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [0.5, 0] \\cdot [1.5, 0]^\\top = 0.75$。\n  $\\alpha_1^* = 0.75 / 2.25 = 1/3$。由于 $0 \\le 1/3 \\le 1$，我们得到 $\\alpha_1 = 1/3$ 和 $\\alpha_2 = 2/3$。\n\n- **情况 2：** $\\mathbf{g}_1 = [-1, 0]^\\top$，$\\mathbf{g}_2 = [-2, 0]^\\top$。\n  $\\|\\mathbf{g}_1 - \\mathbf{g}_2\\|_2^2 = \\|[1, 0]^\\top\\|_2^2 = 1$。\n  $\\mathbf{g}_2^\\top(\\mathbf{g}_2 - \\mathbf{g}_1) = [-2, 0] \\cdot [-1, 0]^\\top = 2$。\n  $\\alpha_1^* = 2 / 1 = 2$。由于 $2  1$，我们投影到边界：$\\alpha_1 = 1$ 和 $\\alpha_2 = 0$。\n\n#### 情况 $T=3$\n对于 $T=3$ 且梯度在 $\\mathbb{R}^2$ 中的情况，梯度的最小范数凸组合为零，当且仅当原点位于梯度的凸包（即它们形成的三角形）内。这可以通过求解原点的重心坐标 $(\\alpha_1, \\alpha_2, \\alpha_3)$ 来检验。如果所有 $\\alpha_t \\ge 0$，这就是解。这需要求解以下方程组：\n$$\n\\begin{pmatrix} g_{1x}  g_{2x}  g_{3x} \\\\ g_{1y}  g_{2y}  g_{3y} \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n如果失败（例如，原点在三角形外），解将位于单纯形的某条边上，这简化为求解三个独立的 $T=2$ 问题并选择最优的一个。\n\n- **情况 3：** $\\mathbf{g}_1 = [1, 0]^\\top$，$\\mathbf{g}_2 = [0, 1]^\\top$，$\\mathbf{g}_3 = [-1, -1]^\\top$。\n我们观察到 $\\mathbf{g}_1 + \\mathbf{g}_2 + \\mathbf{g}_3 = \\mathbf{0}$。一个能产生 $\\mathbf{0}$ 的凸组合是 $\\frac{1}{3}\\mathbf{g}_1 + \\frac{1}{3}\\mathbf{g}_2 + \\frac{1}{3}\\mathbf{g}_3 = \\mathbf{0}$。由于目标函数是范数的平方，其最小值为 $0$，通过该组合可以达到。这些系数是 $\\alpha_1 = 1/3$，$\\alpha_2 = 1/3$，$\\alpha_3 = 1/3$。它们都是非负的且和为 $1$，因此这是唯一的有效解。求解上述线性方程组可以证实这一点：\n$$\n\\begin{pmatrix} 1  0  -1 \\\\ 0  1  -1 \\\\ 1  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\implies \\begin{pmatrix} \\alpha_1 \\\\ \\alpha_2 \\\\ \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}.\n$$\n\n结果如下：\n- 情况 1：$(\\alpha_1, \\alpha_2) = (1/3, 2/3)$\n- 情况 2：$(\\alpha_1, \\alpha_2) = (1, 0)$\n- 情况 3：$(\\alpha_1, \\alpha_2, \\alpha_3) = (1/3, 1/3, 1/3)$\n下面将实现这些计算，以生成最终格式化的输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_t2(g1: np.ndarray, g2: np.ndarray) -> tuple[float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=2 tasks exactly.\n    \"\"\"\n    g1_minus_g2 = g1 - g2\n    g1_minus_g2_sq_norm = np.dot(g1_minus_g2, g1_minus_g2)\n\n    # Handle the degenerate case where g1 == g2\n    if np.isclose(g1_minus_g2_sq_norm, 0):\n        return 0.5, 0.5\n\n    # Compute unconstrained minimizer for alpha_1\n    alpha1_star = np.dot(g2, g2 - g1) / g1_minus_g2_sq_norm\n\n    # Project onto the interval [0, 1]\n    alpha1 = max(0.0, min(1.0, alpha1_star))\n    alpha2 = 1.0 - alpha1\n    \n    return alpha1, alpha2\n\ndef solve_t3(g1: np.ndarray, g2: np.ndarray, g3: np.ndarray) -> tuple[float, float, float]:\n    \"\"\"\n    Solves the MGDA QP for T=3 tasks exactly, as per problem specification.\n    \"\"\"\n    # First, check for an interior solution where the combination of gradients is zero.\n    # This happens if the origin is in the convex hull of the gradients.\n    # We solve for barycentric coordinates of the origin.\n    G_ext = np.array([\n        [g1[0], g2[0], g3[0]],\n        [g1[1], g2[1], g3[1]],\n        [1.0,   1.0,   1.0]\n    ])\n    \n    b_target = np.array([0.0, 0.0, 1.0])\n\n    # Check if a unique solution exists by checking if the matrix is invertible.\n    if abs(np.linalg.det(G_ext)) > 1e-9:\n        try:\n            alphas_int = np.linalg.solve(G_ext, b_target)\n            # If all coefficients are non-negative, this is the solution.\n            if np.all(alphas_int >= -1e-9):\n                return tuple(alphas_int)\n        except np.linalg.LinAlgError:\n            # Should not happen if det is non-zero, but for robustness.\n            pass\n\n    # If no interior solution exists, solve for solutions on the edges.\n    # Edge 1-2\n    a12 = solve_t2(g1, g2)\n    alpha_A = np.array([a12[0], a12[1], 0.0])\n    grad_A = alpha_A[0] * g1 + alpha_A[1] * g2\n    obj_A = 0.5 * np.dot(grad_A, grad_A)\n    \n    # Edge 1-3\n    a13 = solve_t2(g1, g3)\n    alpha_B = np.array([a13[0], 0.0, a13[1]])\n    grad_B = alpha_B[0] * g1 + alpha_B[2] * g3\n    obj_B = 0.5 * np.dot(grad_B, grad_B)\n\n    # Edge 2-3\n    a23 = solve_t2(g2, g3)\n    alpha_C = np.array([0.0, a23[0], a23[1]])\n    grad_C = alpha_C[1] * g2 + alpha_C[2] * g3\n    obj_C = 0.5 * np.dot(grad_C, grad_C)\n    \n    # Find the edge solution with the minimum objective value.\n    objectives = [obj_A, obj_B, obj_C]\n    candidates = [alpha_A, alpha_B, alpha_C]\n    best_alpha = candidates[np.argmin(objectives)]\n    \n    return tuple(best_alpha)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Gradients are g_t = -b_t.\n    test_cases = [\n        # Case 1 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([-0.5, 0.0])]},\n        # Case 2 (T=2)\n        {\"b_vectors\": [np.array([1.0, 0.0]), np.array([2.0, 0.0])]},\n        # Case 3 (T=3)\n        {\"b_vectors\": [np.array([-1.0, 0.0]), np.array([0.0, -1.0]), np.array([1.0, 1.0])]},\n    ]\n\n    results = []\n    \n    # Case 1\n    case1 = test_cases[0]\n    g_vectors1 = [-b for b in case1[\"b_vectors\"]]\n    result1 = solve_t2(g_vectors1[0], g_vectors1[1])\n    results.append(list(result1))\n    \n    # Case 2\n    case2 = test_cases[1]\n    g_vectors2 = [-b for b in case2[\"b_vectors\"]]\n    result2 = solve_t2(g_vectors2[0], g_vectors2[1])\n    results.append(list(result2))\n\n    # Case 3\n    case3 = test_cases[2]\n    g_vectors3 = [-b for b in case3[\"b_vectors\"]]\n    result3 = solve_t3(g_vectors3[0], g_vectors3[1], g_vectors3[2])\n    results.append(list(result3))\n\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for res_list in results:\n        # Format each float to 6 decimal places, then join into a string like \"[f1,f2,...]\"\n        formatted_list = \"[\" + \",\".join([f\"{x:.6f}\" for x in res_list]) + \"]\"\n        formatted_results.append(formatted_list)\n    \n    # Join all case results into the final output string.\n    final_output = \"[\" + \",\".join(formatted_results) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3155072"}]}