{"hands_on_practices": [{"introduction": "在基于度量的少样本学习中，类别原型是分类的核心，通常通过对支持集样本特征取平均来计算。但当支持集中存在离群点时，简单的均值原型会严重偏离，影响分类性能。本实践将指导你从概率建模的第一性原理出发，推导并实现一种更鲁棒的加权原型估计方法，亲身体验其在处理数据不平衡和离群点时的优势。[@problem_id:3125726]", "problem": "考虑一个双类别少样本分类任务，其中，一个基于度量的分类器通过比较查询的嵌入与类别原型来预测其类别。设嵌入函数表示为 $\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$。对于一个类别 $c\\in\\{0,1\\}$，其支持集嵌入为 $\\{y_{c,i}\\}_{i=1}^{k_c}$，其中 $y_{c,i}=\\phi(x_{c,i})$，一个标准的原型使用未加权均值 $\\mu_c=\\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$。在类别不平衡的少样本设置中，当 $k_1\\ll k_2$ 时，未加权均值可能会受到多数类中离散点的影响而产生偏差。为缓解此问题，考虑一个加权原型，形式为 $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$，其中 $\\alpha_{c,i}\\ge 0$ 且 $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$。\n\n从基础概率建模和经验风险最小化原则出发，推导出一个选择权重 $\\alpha_{c,i}$ 的有原则的规则，该规则能够减少离群点的影响，同时满足约束条件 $\\alpha_{c,i}\\ge 0$ 和 $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$。您的推导必须从一个关于嵌入空间中数据生成过程的适定假设开始，并逐步进行，得出一个具体的加权机制，而不能采用临时的启发式方法。然后实现所得的分类器，并在以下条件下评估其性能。\n\n使用一个二维嵌入，其固定的线性嵌入函数为 $\\phi(x)=W x$，其中\n$$\nW=\\begin{pmatrix}\n1.2  0.3\\\\\n-0.4  0.8\n\\end{pmatrix}.\n$$\n分类必须在嵌入空间中根据与原型的最小平方欧氏距离进行，若距离相等，则偏向于数值较小的类别索引。对于下面的每个测试用例，为每个类别计算两种原型：未加权均值和推导出的加权原型。对每个查询，使用每组原型预测其类别，并计算准确率，即正确分类的查询所占的比例，以小数表示。为了在您的加权规则所需的任何方差或尺度估计中保持数值稳定性，在任何可能发生除以零的地方加上一个很小的常数 $\\varepsilon=10^{-6}$。\n\n测试套件。对于每个测试用例，支持集和查询都在 $\\mathbb{R}^2$ 中指定。所有坐标均为无单位的数值。任务是为每个测试用例输出一个浮点数，该浮点数等于加权原型分类器的准确率减去未加权均值分类器的准确率，并四舍五入到三位小数。\n\n- 测试用例 A（中度不平衡，多数类簇分散）：\n    - 类别 $0$ 支持集 ($k_0=3$)：$(-0.1,\\,0.2)$、$(0.05,\\,-0.05)$、$(0.1,\\,0.0)$。\n    - 类别 $1$ 支持集 ($k_1=7$)：$(3.8,\\,0.1)$、$(4.1,\\,-0.2)$、$(4.0,\\,0.0)$、$(5.5,\\,2.0)$、$(3.9,\\,0.2)$、$(4.2,\\,0.1)$、$(4.1,\\,0.0)$。\n    - 查询与真实标签：\n        - $(-0.05,\\,0.0)\\rightarrow 0$、$(0.2,\\,-0.1)\\rightarrow 0$、$(4.05,\\,0.0)\\rightarrow 1$、$(3.7,\\,0.1)\\rightarrow 1$、$(2.1,\\,0.4)\\rightarrow 1$、$(2.0,\\,0.0)\\rightarrow 0$。\n\n- 测试用例 B（极端不平衡 $k_0=1\\ll k_1=10$）：\n    - 类别 $0$ 支持集 ($k_0=1$)：$(0.0,\\,0.0)$。\n    - 类别 $1$ 支持集 ($k_1=10$)：$(4.0,\\,0.0)$、$(4.1,\\,0.1)$、$(3.9,\\,-0.1)$、$(4.2,\\,0.0)$、$(3.8,\\,0.2)$、$(4.3,\\,-0.2)$、$(4.05,\\,0.05)$、$(4.2,\\,0.2)$、$(5.0,\\,1.5)$、$(3.7,\\,-0.3)$。\n    - 查询与真实标签：\n        - $(0.1,\\,0.0)\\rightarrow 0$、$(-0.2,\\,0.05)\\rightarrow 0$、$(4.05,\\,0.1)\\rightarrow 1$、$(3.9,\\,-0.05)\\rightarrow 1$、$(2.2,\\,0.2)\\rightarrow 1$、$(-1.0,\\,0.0)\\rightarrow 0$。\n\n- 测试用例 C（平衡但两个类别都有强离群点）：\n    - 类别 $0$ 支持集 ($k_0=5$)：$(0.0,\\,0.0)$、$(0.1,\\,-0.1)$、$(-0.05,\\,0.1)$、$(0.2,\\,0.05)$、$(-1.8,\\,-1.2)$。\n    - 类别 $1$ 支持集 ($k_1=5$)：$(3.9,\\,0.0)$、$(4.1,\\,0.1)$、$(4.0,\\,-0.1)$、$(4.2,\\,0.0)$、$(5.2,\\,2.5)$。\n    - 查询与真实标签：\n        - $(0.1,\\,0.0)\\rightarrow 0$、$(-0.2,\\,0.15)\\rightarrow 0$、$(4.1,\\,-0.05)\\rightarrow 1$、$(3.95,\\,0.2)\\rightarrow 1$、$(2.3,\\,0.5)\\rightarrow 1$、$(1.0,\\,-0.7)\\rightarrow 0$。\n\n您的程序必须：\n- 实现推导出的加权规则来计算 $\\alpha_{c,i}$ 并构建加权原型 $\\mu_c$。\n- 计算未加权原型（通过简单求均值）以进行比较。\n- 在嵌入空间中，根据最小平方欧氏距离，通过最近原型对每个查询进行分类。\n- 对每个测试用例，计算加权和未加权情况下的准确率。\n- 输出一行，其中包含一个用逗号分隔并用方括号括起来的列表，该列表按顺序包含测试用例A、B、C的准确率差异（加权减去未加权），每个差异都四舍五入到三位小数。例如，最终输出格式必须严格遵循 $\\left[\\text{result}_A,\\text{result}_B,\\text{result}_C\\right]$ 的形式。", "solution": "## 问题验证\n\n### 步骤 1：提取已知条件\n- **任务**：双类别少样本分类。\n- **分类器**：基于度量，通过比较查询嵌入与类别原型。\n- **类别**：$c \\in \\{0, 1\\}$。\n- **嵌入函数**：$\\phi:\\mathbb{R}^d\\rightarrow\\mathbb{R}^d$。\n- **支持集**：对于类别 $c$，$\\{x_{c,i}\\}_{i=1}^{k_c}$ 是输入点。\n- **支持集嵌入**：$y_{c,i} = \\phi(x_{c,i})$。\n- **未加权原型**：$\\mu_c = \\frac{1}{k_c}\\sum_{i=1}^{k_c}y_{c,i}$。\n- **加权原型**：$\\mu_c = \\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$，约束条件为 $\\alpha_{c,i} \\ge 0$ 和 $\\sum_{i=1}^{k_c}\\alpha_{c,i}=1$。\n- **推导目标**：从概率建模和经验风险最小化出发，推导出一个有原则的权重 $\\alpha_{c,i}$ 规则。\n- **嵌入矩阵**：对于实现部分，嵌入是线性的，$\\phi(x) = Wx$，其中 $d=2$ 且\n$$\nW=\\begin{pmatrix}\n1.2  0.3\\\\\n-0.4  0.8\n\\end{pmatrix}.\n$$\n- **分类度量**：基于最小平方欧氏距离的最近原型。\n- **平局打破规则**：若距离相等，则偏向于数值较小的类别索引（即类别 $0$）。\n- **数值稳定性**：在分母中加入 $\\varepsilon=10^{-6}$ 以防止除以零。\n- **测试用例**：提供了三个测试用例（A、B、C），每个用例都包含类别 $0$ 和类别 $1$ 的具体支持集，以及一组带真实标签的查询。\n- **输出**：对每个测试用例，计算加权原型分类器的准确率减去未加权均值分类器的准确率，结果四舍五入到三位小数。最终输出必须是一行：`[result_A,result_B,result_C]`。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准评估该问题：\n- **科学依据**：该问题牢固地植根于机器学习和统计学的既定原则。它要求基于概率建模和经验风险最小化进行推导，这些是开发算法的标准、严谨的框架。使用加权均值来创建稳健的估计量是稳健统计学中一个被充分研究的概念。该问题在科学上是合理的。\n- **适定性**：该问题定义清晰且自包含。它提供了所有必要的信息：数学公式、具体的嵌入函数、分类规则、平局打破规则、稳定性常数以及一套完整的测试数据。目标是明确的。\n- **客观性**：该问题以精确、正式的语言陈述，不含任何主观或基于意见的论断。所有数据和条件都是数值化和客观的。\n\n该问题没有任何无效性缺陷。它在科学上并非不合理、不可形式化、不完整、不切实际、非适定、过于简单或超出科学可验证的范围。\n\n### 步骤 3：结论与行动\n此问题是**有效的**。将提供一个解决方案。\n\n---\n## 加权规则的推导\n\n目标是从概率模型和经验风险最小化原则出发，为原型定义 $\\mu_c=\\sum_{i=1}^{k_c}\\alpha_{c,i}\\,y_{c,i}$ 推导出一个有原则的权重 $\\alpha_{c,i}$ 规则，该规则应能减轻离群点的影响。\n\n1.  **概率模型**：我们首先将类别 $c$ 的支持集嵌入 $\\{y_{c,i}\\}_{i=1}^{k_c}$ 建模为对一个未知的真实类别原型 $\\theta_c$ 的带噪声观测。为了考虑离群点，我们假设每个观测值 $y_{c,i}$ 都是从一个以 $\\theta_c$ 为中心但具有其各自方差 $\\sigma_{c,i}^2$ 的高斯分布中生成的。也就是说，$y_{c,i} \\sim \\mathcal{N}(\\theta_c, \\sigma_{c,i}^2 I)$，其中 $I$ 是单位矩阵。对于一个点 $y_{c,i}$，一个较大的方差 $\\sigma_{c,i}^2$ 表示我们对其是真实中心 $\\theta_c$ 的一个代表性样本的置信度较低，这为处理离群点提供了一个自然的机制。\n\n2.  **通过最大似然法实现经验风险最小化**：在此模型下，经验风险最小化原则对应于找到一个原型 $\\theta_c$，该原型能最大化观测到支持集的对数似然。假设观测是独立的，对数似然为：\n    $$ \\mathcal{L}(\\theta_c) = \\ln \\prod_{i=1}^{k_c} p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) = \\sum_{i=1}^{k_c} \\ln p(y_{c,i} | \\theta_c, \\sigma_{c,i}^2) $$\n    对于我们在 $d$ 维空间中的高斯模型，这变为：\n    $$ \\mathcal{L}(\\theta_c) = \\sum_{i=1}^{k_c} \\left( -\\frac{\\|y_{c,i} - \\theta_c\\|^2}{2\\sigma_{c,i}^2} - \\frac{d}{2}\\ln(2\\pi\\sigma_{c,i}^2) \\right) $$\n    相对于 $\\theta_c$ 最大化 $\\mathcal{L}(\\theta_c)$ 等价于最小化负对数似然。去掉不依赖于 $\\theta_c$ 的项后，我们剩下要最小化以下风险函数 $R(\\theta_c)$：\n    $$ R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\|y_{c,i} - \\theta_c\\|^2 $$\n    这是一个加权最小二乘问题，其中每个点对损失的贡献与其方差成反比。\n\n3.  **求解原型**：为了找到最优原型（我们对 $\\theta_c$ 的估计），我们将 $R(\\theta_c)$ 相对于 $\\theta_c$ 的梯度设为零：\n    $$ \\nabla_{\\theta_c} R(\\theta_c) = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\cdot 2(\\theta_c - y_{c,i}) = 0 $$\n    求解 $\\theta_c$ 可得：\n    $$ \\left( \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} \\right) \\theta_c = \\sum_{i=1}^{k_c} \\frac{1}{\\sigma_{c,i}^2} y_{c,i} $$\n    由此得到的原型估计值，我们记为 $\\mu_c$，是支持集嵌入的一个加权平均：\n    $$ \\mu_c = \\frac{\\sum_{i=1}^{k_c} (1/\\sigma_{c,i}^2) y_{c,i}}{\\sum_{j=1}^{k_c} (1/\\sigma_{c,j}^2)} $$\n    这对应于所期望的形式 $\\mu_c = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$，其中权重为 $\\alpha_{c,i} = \\frac{w_{c,i}}{\\sum_{j=1}^{k_c} w_{c,j}}$，且 $w_{c,i} = 1/\\sigma_{c,i}^2$。这些权重自动满足约束条件 $\\alpha_{c,i} \\ge 0$ 和 $\\sum_i \\alpha_{c,i}=1$。\n\n4.  **推导加权规则**：最后一步是定义未知的各个方差 $\\sigma_{c,i}^2$。一个有原则的、非迭代的方法是使用一个数据驱动的代理来表示每个点的可靠性。一个点与类别中心的初步估计之间的偏差是其潜在“离群性”的一个良好度量。标准的未加权均值 $\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_j y_{c,j}$，是这个初步估计最简单、最自然的选择（如果假设所有方差相等，它就是最大似然估计）。\n\n    因此，我们将每个点的方差 $\\sigma_{c,i}^2$ 建模为与其到这个未加权均值的平方欧氏距离成正比：\n    $$ \\sigma_{c,i}^2 \\propto \\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 $$\n    这提供了一个具体的规则：远离初始“质心”的点被认为可靠性较低（方差较大），因此在计算最终原型时被赋予较小的权重。未归一化的权重与该方差成反比：\n    $$ w_{c,i} \\propto \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2} $$\n    为了处理点可能恰好落在均值上的情况并确保数值稳定性，我们按照规定加上一个很小的常数 $\\varepsilon=10^{-6}$。未归一化权重的最终具体形式是：\n    $$ w_{c,i} = \\frac{1}{\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon} $$\n    然后将这些权重归一化以获得 $\\alpha_{c,i}$，用于计算最终的加权原型。对于 $k_c=1$ 的特殊情况，距离为 $0$，未归一化的权重为 $1/\\varepsilon$，归一化后的权重为 $1$，正确地得到该点本身作为原型。推导至此完成。\n\n## 算法与实现\n所推导的方法实现如下：\n1.  对每个类别 $c \\in \\{0, 1\\}$，取其支持点 $\\{x_{c,i}\\}$ 并应用嵌入函数 $\\phi(x) = Wx$ 以获得嵌入 $\\{y_{c,i}\\}$。\n2.  **未加权原型**：对每个类别，计算其嵌入的标准均值：$\\mu_{c, \\text{unw}} = \\frac{1}{k_c}\\sum_{i=1}^{k_c} y_{c,i}$。\n3.  **加权原型**：对每个类别：\n    a. 如果 $k_c = 1$，加权原型就是单个支持集嵌入本身。\n    b. 如果 $k_c  1$，首先计算未加权原型 $\\mu_{c, \\text{unw}}$。\n    c. 对每个支持集嵌入 $y_{c,i}$，计算一个未归一化的权重 $w_{c,i} = 1 / (\\|y_{c,i} - \\mu_{c, \\text{unw}}\\|^2 + \\varepsilon)$。\n    d. 归一化权重：$\\alpha_{c,i} = w_{c,i} / \\sum_{j=1}^{k_c} w_{c,j}$。\n    e. 计算加权原型：$\\mu_{c, \\text{w}} = \\sum_{i=1}^{k_c} \\alpha_{c,i} y_{c,i}$。\n4.  **分类与评估**：对每个测试用例：\n    a. 嵌入查询点 $\\{x_q\\}$。\n    b. 对每个查询嵌入 $y_q$，计算其到原型 $\\{\\mu_{0, \\text{unw}}, \\mu_{1, \\text{unw}}\\}$ 和 $\\{\\mu_{0, \\text{w}}, \\mu_{1, \\text{w}}\\}$ 的平方欧氏距离。\n    c. 对每种方法，将查询分配给最近原型的类别标签，若距离相等则偏向类别 $0$。\n    d. 通过将预测结果与真实标签进行比较，计算未加权和加权方法的准确率。\n    e. 计算差异：（加权准确率） - （未加权准确率）。\n5.  收集所有测试用例的准确率差异，并按要求格式化输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the few-shot classification problem by deriving and implementing a\n    principled weighting scheme for robust prototypes.\n    \"\"\"\n\n    # ---- Global parameters from the problem statement ----\n    W = np.array([[1.2, 0.3], [-0.4, 0.8]])\n    epsilon = 1e-6\n\n    # ---- Test cases ----\n    # Each case is a tuple: (support_set_dict, query_points_array, query_labels_array)\n    test_cases = [\n        (\n            {  # Support set for Test Case A\n                0: np.array([[-0.1, 0.2], [0.05, -0.05], [0.1, 0.0]]),\n                1: np.array([(3.8, 0.1), (4.1, -0.2), (4.0, 0.0), (5.5, 2.0), (3.9, 0.2), (4.2, 0.1), (4.1, 0.0)])\n            },\n            np.array([(-0.05, 0.0), (0.2, -0.1), (4.05, 0.0), (3.7, 0.1), (2.1, 0.4), (2.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case B\n                0: np.array([[0.0, 0.0]]),\n                1: np.array([(4.0, 0.0), (4.1, 0.1), (3.9, -0.1), (4.2, 0.0), (3.8, 0.2), (4.3, -0.2), (4.05, 0.05), (4.2, 0.2), (5.0, 1.5), (3.7, -0.3)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.05), (4.05, 0.1), (3.9, -0.05), (2.2, 0.2), (-1.0, 0.0)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        ),\n        (\n            {  # Support set for Test Case C\n                0: np.array([(0.0, 0.0), (0.1, -0.1), (-0.05, 0.1), (0.2, 0.05), (-1.8, -1.2)]),\n                1: np.array([(3.9, 0.0), (4.1, 0.1), (4.0, -0.1), (4.2, 0.0), (5.2, 2.5)])\n            },\n            np.array([(0.1, 0.0), (-0.2, 0.15), (4.1, -0.05), (3.95, 0.2), (2.3, 0.5), (1.0, -0.7)]),\n            np.array([0, 0, 1, 1, 1, 0])\n        )\n    ]\n\n    results = []\n\n    # ---- Helper functions ----\n    def embed_points(points, W_matrix):\n        if points.ndim == 1:\n            points = points.reshape(1, -1)\n        return (W_matrix @ points.T).T\n\n    def get_prototypes(support_embeddings, eps):\n        prototypes_unweighted = {}\n        prototypes_weighted = {}\n        \n        for c in sorted(support_embeddings.keys()):\n            points = support_embeddings[c]\n            k_c = len(points)\n            \n            # Unweighted prototype: simple mean\n            mu_unweighted = np.mean(points, axis=0)\n            prototypes_unweighted[c] = mu_unweighted\n            \n            # Weighted prototype: derived from the robust estimation principle\n            if k_c == 1:\n                mu_weighted = points[0]\n            else:\n                dists_sq = np.sum((points - mu_unweighted)**2, axis=1)\n                raw_weights = 1.0 / (dists_sq + eps)\n                normalized_weights = raw_weights / np.sum(raw_weights)\n                mu_weighted = np.sum(points * normalized_weights[:, np.newaxis], axis=0)\n            \n            prototypes_weighted[c] = mu_weighted\n            \n        return prototypes_unweighted, prototypes_weighted\n\n    def classify_queries(prototypes_dict, query_embeddings):\n        # Prototypes are already created for sorted class indices\n        prototypes = [prototypes_dict[i] for i in sorted(prototypes_dict.keys())]\n        predictions = []\n        for q in query_embeddings:\n            dists_sq = [np.sum((q - p)**2) for p in prototypes]\n            # np.argmin breaks ties by choosing the first occurrence, which\n            # corresponds to the smaller class index as required.\n            predictions.append(np.argmin(dists_sq))\n        return np.array(predictions)\n\n    # ---- Main loop for test case evaluation ----\n    for case in test_cases:\n        support_x, query_x, labels = case\n        \n        # 1. Embed all points using the given matrix W\n        support_y = {c: embed_points(pts, W) for c, pts in support_x.items()}\n        query_y = embed_points(query_x, W)\n        \n        # 2. Get both unweighted and weighted prototypes for each class\n        protos_unw, protos_w = get_prototypes(support_y, epsilon)\n        \n        # 3. Classify queries and compute accuracy for the unweighted method\n        preds_unw = classify_queries(protos_unw, query_y)\n        acc_unw = np.mean(preds_unw == labels)\n\n        # 4. Classify queries and compute accuracy for the weighted method\n        preds_w = classify_queries(protos_w, query_y)\n        acc_w = np.mean(preds_w == labels)\n        \n        # 5. Calculate and store the accuracy difference, rounded to 3 decimal places\n        accuracy_diff = acc_w - acc_unw\n        results.append(round(accuracy_diff, 3))\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125726"}, {"introduction": "在计算出查询样本与各类别原型的距离后，我们常使用带温度参数 $\\tau$ 的 softmax 函数将其转化为概率。这个温度参数不仅仅是一个需要手动调整的超参数；它背后蕴含着深刻的概率意义。本实践将引导你通过推导，揭示温度 $\\tau$ 与特征分布方差之间的直接联系，从而理解如何在特定假设下构建一个贝叶斯最优分类器。[@problem_id:3125741]", "problem": "考虑一个深度学习中的少样本分类场景，其中每个类别在嵌入空间中由一个原型表示。设嵌入映射表示为 $\\phi(\\cdot)$，并假设在 $\\mathbb{R}^{m}$ 中有 $C$ 个类别，其类别原型为 $\\mu_{1}, \\mu_{2}, \\dots, \\mu_{C}$。分类器通过计算输入 $x$ 到各类别原型的负距离的softmax来为其分配标签，其中温度参数 $\\tau  0$，具体为\n$$\np_{\\tau}(y=c \\mid x) \\;=\\; \\frac{\\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\!\\big(-\\tau\\, d\\!\\big(\\phi(x), \\mu_{k}\\big)\\big)},\n$$\n其中 $d\\!\\big(\\phi(x), \\mu_{c}\\big) = \\|\\phi(x) - \\mu_{c}\\|_{2}^{2}$ 是平方欧几里得距离。假设嵌入的类条件分布是高斯分布，且所有类别共享各向同性的协方差，即对于每个类别 $c$，给定 $y=c$ 时，随机向量 $z = \\phi(x)$ 服从均值为 $\\mu_{c}$、协方差矩阵为 $\\sigma^{2} I_{m}$ 的多元正态分布，记作 $z \\mid y=c \\sim \\mathcal{N}\\!\\big(\\mu_{c}, \\sigma^{2} I_{m}\\big)$，其中 $\\sigma^{2}  0$ 且 $I_{m}$ 是 $m \\times m$ 的单位矩阵。假设类别先验概率相等，即对于所有 $c \\in \\{1, 2, \\dots, C\\}$，有 $p(y=c) = \\frac{1}{C}$。\n\n从高斯概率密度函数和贝叶斯法则的定义出发，推导上述softmax分类器的温度参数 $\\tau$ 的值，使得在所述假设下，$p_{\\tau}(y=c \\mid x)$ 与贝叶斯最优后验概率 $p(y=c \\mid x)$ 相匹配。请将您的最终答案表示为仅含 $\\sigma^{2}$ 的单个闭式符号表达式。不需要进行数值近似或四舍五入。", "solution": "目标是确定温度参数 $\\tau$ 的值，使得在给定的建模假设下，softmax分类器的后验概率分布 $p_{\\tau}(y=c \\mid x)$ 与贝叶斯最优后验概率 $p(y=c \\mid x)$ 完全相同。\n\n首先，我们来推导贝叶斯最优后验概率 $p(y=c \\mid x)$。设 $z = \\phi(x)$ 是输入 $x$ 在 $\\mathbb{R}^{m}$ 中的嵌入。根据贝叶斯法则，给定嵌入 $z$ 时，类别 $c$ 的后验概率为：\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{p(z)}$$\n分母 $p(z)$ 是 $z$ 的边缘概率密度，可以通过全概率定律对所有可能的类别求和来表示：\n$$p(z) = \\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)$$\n将此代入贝叶斯法则的表达式中，得到：\n$$p(y=c \\mid z) = \\frac{p(z \\mid y=c) p(y=c)}{\\sum_{k=1}^{C} p(z \\mid y=k) p(y=k)}$$\n\n问题提供了以下信息：\n1.  对任意类别 $c$，嵌入 $z = \\phi(x)$ 的类条件分布是均值为 $\\mu_{c}$、协方差矩阵为 $\\sigma^{2} I_{m}$ 的多元正态分布。这被记为 $z \\mid y=c \\sim \\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$。\n2.  类别先验概率是均匀的，即对所有 $c \\in \\{1, 2, \\dots, C\\}$，有 $p(y=c) = \\frac{1}{C}$。\n\n对于向量 $z \\in \\mathbb{R}^{m}$，多元正态分布 $\\mathcal{N}(\\mu_{c}, \\sigma^{2} I_{m})$ 的概率密度函数 (PDF) 为：\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi)^{m/2} \\det(\\sigma^{2} I_{m})^{1/2}} \\exp\\left(-\\frac{1}{2} (z - \\mu_{c})^{\\top} (\\sigma^{2} I_{m})^{-1} (z - \\mu_{c})\\right)$$\n我们来简化此PDF中的各项。协方差矩阵的行列式为 $\\det(\\sigma^{2} I_{m}) = (\\sigma^{2})^{m} \\det(I_{m}) = (\\sigma^{2})^{m}$。协方差矩阵的逆为 $(\\sigma^{2} I_{m})^{-1} = \\frac{1}{\\sigma^{2}} I_{m}^{-1} = \\frac{1}{\\sigma^{2}} I_{m}$。\n指数中的二次型变为：\n$$(z - \\mu_{c})^{\\top} \\left(\\frac{1}{\\sigma^{2}} I_{m}\\right) (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} (z - \\mu_{c})^{\\top} (z - \\mu_{c}) = \\frac{1}{\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\n将这些代回PDF表达式，我们得到：\n$$p(z \\mid y=c) = \\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)$$\n\n现在，我们将PDF和均匀先验 $p(y=c) = \\frac{1}{C}$ 都代入贝叶斯最优后验概率的表达式中：\n$$p(y=c \\mid z) = \\frac{\\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}{\\sum_{k=1}^{C} \\left(\\frac{1}{(2\\pi\\sigma^{2})^{m/2}} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)\\right) \\left(\\frac{1}{C}\\right)}$$\n常数项 $\\frac{1}{(2\\pi\\sigma^{2})^{m/2}}$ 和 $\\frac{1}{C}$ 在分子和分母的求和项中都是公共的，因此它们可以被约去。贝叶斯最优后验概率的简化表达式为：\n$$p(y=c \\mid z) = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\n接下来，我们将此推导出的后验概率与问题陈述中给出的分类器模型进行比较。该模型的概率分布为：\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{c}\\big)\\big)}{\\sum_{k=1}^{C} \\exp\\big(-\\tau\\, d\\big(\\phi(x), \\mu_{k}\\big)\\big)}$$\n使用 $z = \\phi(x)$ 和距离度量的定义 $d(z, \\mu_c) = \\|z - \\mu_{c}\\|_{2}^{2}$，我们可以将分类器的概率重写为：\n$$p_{\\tau}(y=c \\mid x) = \\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n\n为了使分类器达到贝叶斯最优，对于所有的 $c$ 和 $z$，必须有 $p_{\\tau}(y=c \\mid x) = p(y=c \\mid z)$。\n$$\\frac{\\exp\\left(-\\tau \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\tau \\|z - \\mu_{k}\\|_{2}^{2}\\right)} = \\frac{\\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}\\right)}{\\sum_{k=1}^{C} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{k}\\|_{2}^{2}\\right)}$$\n两个表达式都是将softmax函数应用于一组分数的形式。要使对于任何原型集 $\\{\\mu_{k}\\}$ 和任何嵌入 $z$，输出都相同，分子和分母中相应指数函数的参数必须相等（最多相差一个与类别索引 $c$ 无关的加性常数，在本例中该常数为零）。\n通过比较类别 $c$ 的指数，我们得到等式：\n$$-\\tau \\|z - \\mu_{c}\\|_{2}^{2} = -\\frac{1}{2\\sigma^{2}} \\|z - \\mu_{c}\\|_{2}^{2}$$\n由于此式必须对任意 $z$ 成立，并且我们可以选择一个 $z$ 使得 $\\|z - \\mu_{c}\\|_{2}^{2} \\neq 0$，因此我们可以将等式两边同时除以 $-\\|z - \\mu_{c}\\|_{2}^{2}$：\n$$\\tau = \\frac{1}{2\\sigma^{2}}$$\n这就得到了用类条件嵌入分布的方差 $\\sigma^{2}$ 表示的温度参数 $\\tau$ 的值。条件 $\\sigma^{2}  0$ 确保了 $\\tau$ 是良定义且为正的，符合要求。", "answer": "$$\\boxed{\\frac{1}{2\\sigma^{2}}}$$", "id": "3125741"}, {"introduction": "除了基于度量的方法，另一种常见的少样本分类策略是在深度特征之上训练一个线性分类器。然而，在样本量极少的情况下，直接训练的分类器极易发生过拟合。本实践将带你深入研究经典的正则化技术——岭回归（ridge regression），通过推导其闭式解并分析其在单样本（1-shot）场景下的表现，让你具体地理解正则化是如何在数据稀缺时提升模型泛化能力的。[@problem_id:3125783]", "problem": "一个深度神经网络骨干产生 $\\mathbb{R}^{d}$ 中的特征向量，这些向量通过拟合一个线性头来执行少样本分类。共有 $C$ 个类别，每个类别有 $k$ 个带标签的样本，它们被排列成一个数据矩阵 $X \\in \\mathbb{R}^{d \\times Ck}$，其列是所有训练样本的骨干特征。设标签矩阵 $Y \\in \\mathbb{R}^{C \\times Ck}$ 收集了 $X$ 中每一列的 one-hot 类别指示向量，因此对于属于类别 $c$ 的每个训练样本，$Y$ 的对应列等于标准基向量 $e_{c} \\in \\mathbb{R}^{C}$。\n\n线性头是一个矩阵 $W \\in \\mathbb{R}^{C \\times d}$，它将一个特征 $x \\in \\mathbb{R}^{d}$ 映射到 $\\mathbb{R}^{C}$ 中的一个类别 logit 向量。该头通过最小化岭回归正则化的平方误差目标进行训练\n$$\n\\mathcal{L}(W) = \\|Y - W X\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2},\n$$\n其中 $\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数，$\\lambda  0$ 是正则化强度。仅从上述目标的定义以及矩阵微积分和线性代数的标准性质出发，推导出唯一的最小化子 $W^{\\star}$。\n\n然后，通过在以下骨干特征的几何模型下考虑 $k=1$ 的精确情况，分析单样本极限 $k \\to 1$。假设存在类别原型 $\\mu_{1}, \\dots, \\mu_{C} \\in \\mathbb{R}^{d}$，使得类别 $c$ 的单个训练特征等于 $\\mu_{c}$，并且这些原型在以下意义上是正交且等尺度的\n$$\n\\mu_{c}^{\\top} \\mu_{c'} = m^{2} \\, \\delta_{cc'}\n$$\n对于一个固定的标量 $m  0$，其中 $\\delta_{cc'}$ 是克罗内克 δ。令 $X = [\\mu_{1} \\,\\, \\mu_{2} \\,\\, \\cdots \\,\\, \\mu_{C}] \\in \\mathbb{R}^{d \\times C}$，并令 $Y$ 为 $\\mathbb{R}^{C \\times C}$ 中对应的 one-hot 标签矩阵。使用您推导出的 $W^{\\star}$，计算分类间隔。分类间隔定义为，当在某个 $t \\in \\{1, \\dots, C\\}$ 的测试特征 $x_{\\text{test}} = \\mu_{t}$ 上评估线性头时，真实类别的 logit 与所有不正确类别中最大 logit 之间的差值。将您的最终答案表示为关于 $m$ 和 $\\lambda$ 的单个闭式解析表达式，无需四舍五入，也没有单位。", "solution": "问题要求两个结果。首先，推导最小化岭回归正则化平方误差目标的最优权重矩阵 $W^{\\star}$。其次，在一个特定的单样本学习场景中计算分类间隔。\n\n第1部分：最优权重矩阵 $W^{\\star}$ 的推导。\n需要最小化的目标函数由下式给出\n$$\n\\mathcal{L}(W) = \\|Y - W X\\|_{F}^{2} + \\lambda \\|W\\|_{F}^{2}\n$$\n其中 $W \\in \\mathbb{R}^{C \\times d}$ 是权重矩阵，$X \\in \\mathbb{R}^{d \\times Ck}$ 是数据矩阵，$Y \\in \\mathbb{R}^{C \\times Ck}$ 是标签矩阵，$\\lambda  0$ 是正则化参数。$\\|\\cdot\\|_F$ 项表示弗罗贝尼乌斯范数。\n\n目标函数 $\\mathcal{L}(W)$ 是两个平方范数之和。弗罗贝尼乌斯范数是由内积诱导的范数，因此，它的平方是一个凸函数。两个凸函数之和是凸函数。此外，由于 $\\lambda  0$，项 $\\lambda \\|W\\|_{F}^{2}$ 是严格凸的。这确保了 $\\mathcal{L}(W)$ 是严格凸的，因此有唯一的全局最小值点。可以通过将 $\\mathcal{L}(W)$ 关于 $W$ 的梯度设为零来找到这个最小值点。\n\n我们将通过分别微分每一项来求梯度 $\\frac{\\partial \\mathcal{L}}{\\partial W}$。\n\n对于第一项，我们使用性质 $\\|A\\|_{F}^{2} = \\text{tr}(A^{\\top}A)$，其中 $\\text{tr}(\\cdot)$ 是迹算子。\n$$\n\\|Y - WX\\|_{F}^{2} = \\text{tr}\\left((Y - WX)^{\\top}(Y - WX)\\right)\n$$\n为了求梯度，我们可以使用矩阵微分。让我们求 $\\|Y - WX\\|_{F}^{2}$ 关于 $W$ 的微分变化 $dW$ 的微分 $d(\\|Y - WX\\|_{F}^{2})$。\n$$\nd(\\|Y - WX\\|_{F}^{2}) = \\text{tr}\\left( d((Y - WX)^{\\top})(Y - WX) + (Y - WX)^{\\top}d(Y - WX) \\right)\n$$\n由于 $d(Y - WX) = -dW X$，我们有 $d((Y - WX)^{\\top}) = -(dW X)^{\\top} = -X^{\\top}(dW)^{\\top}$。\n$$\nd(\\|Y - WX\\|_{F}^{2}) = \\text{tr}\\left(-X^{\\top}(dW)^{\\top}(Y - WX) - (Y - WX)^{\\top}(dW X)\\right)\n$$\n使用迹的循环性质 $\\text{tr}(AB) = \\text{tr}(BA)$ 和 $\\text{tr}(A^{\\top}) = \\text{tr}(A)$，我们可以重写迹中的第一项：\n$$\n\\text{tr}\\left(-X^{\\top}(dW)^{\\top}(Y - WX)\\right) = \\text{tr}\\left(-(Y - WX)^{\\top} dW X\\right)\n$$\n因此，\n$$\nd(\\|Y - WX\\|_{F}^{2}) = -2 \\, \\text{tr}\\left((Y - WX)^{\\top}dW X\\right) = -2 \\, \\text{tr}\\left(X (Y - WX)^{\\top} dW\\right)\n$$\n关于 $W$ 的梯度是在迹内部乘以 $dW$ 的矩阵的转置。\n$$\n\\frac{\\partial}{\\partial W} \\|Y - WX\\|_{F}^{2} = -2 (X (Y - WX)^{\\top})^{\\top} = -2 (Y - WX)X^{\\top}\n$$\n对于第二项，即正则化项 $\\lambda \\|W\\|_{F}^{2} = \\lambda \\, \\text{tr}(W^{\\top}W)$：\n$$\nd(\\lambda \\, \\text{tr}(W^{\\top}W)) = \\lambda \\, \\text{tr}(d(W^{\\top}W)) = \\lambda \\, \\text{tr}((dW)^{\\top}W + W^{\\top}dW) = \\lambda \\, \\text{tr}(W^{\\top}dW + W^{\\top}dW) = 2\\lambda \\, \\text{tr}(W^{\\top}dW)\n$$\n因此，梯度为：\n$$\n\\frac{\\partial}{\\partial W} (\\lambda \\|W\\|_{F}^{2}) = 2\\lambda W\n$$\n结合这些梯度，目标函数的总梯度是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = -2(Y - WX)X^{\\top} + 2\\lambda W\n$$\n将梯度设为零以找到最优权重矩阵 $W^{\\star}$：\n$$\n-2(Y - W^{\\star}X)X^{\\top} + 2\\lambda W^{\\star} = 0_{C \\times d}\n$$\n$$\n-(Y X^{\\top} - W^{\\star}XX^{\\top}) + \\lambda W^{\\star} = 0_{C \\times d}\n$$\n$$\n-Y X^{\\top} + W^{\\star}XX^{\\top} + \\lambda W^{\\star} = 0_{C \\times d}\n$$\n$$\nW^{\\star}(XX^{\\top} + \\lambda I_d) = YX^{\\top}\n$$\n其中 $I_d$ 是 $d \\times d$ 单位矩阵。矩阵 $XX^{\\top} \\in \\mathbb{R}^{d \\times d}$ 是半正定的。由于 $\\lambda  0$，矩阵 $(XX^{\\top} + \\lambda I_d)$ 是正定的，因此是可逆的。我们可以通过右乘其逆矩阵来求解 $W^{\\star}$：\n$$\nW^{\\star} = YX^{\\top}(XX^{\\top} + \\lambda I_d)^{-1}\n$$\n这就完成了问题的第一部分。\n\n第2部分：$k=1$ 时分类间隔的计算。\n现在我们考虑单样本学习的特殊情况，即 $k=1$。训练数据包含 $C$ 个类别中的每个类别的一个特征向量 $\\mu_c \\in \\mathbb{R}^d$。\n数据矩阵是 $X = [\\mu_1 \\,\\, \\mu_2 \\,\\, \\cdots \\,\\, \\mu_C] \\in \\mathbb{R}^{d \\times C}$。标签矩阵 $Y \\in \\mathbb{R}^{C \\times C}$ 的第 $c$ 列是标准基向量 $e_c$，这意味着 $Y$ 是 $C \\times C$ 的单位矩阵，即 $Y = I_C$。\n特征向量满足条件 $\\mu_c^{\\top}\\mu_{c'} = m^2 \\delta_{cc'}$，其中标量 $m  0$。\n\n将 $Y=I_C$ 代入 $W^{\\star}$ 的表达式中：\n$$\nW^{\\star} = I_C X^{\\top}(XX^{\\top} + \\lambda I_d)^{-1} = X^{\\top}(XX^{\\top} + \\lambda I_d)^{-1}\n$$\n让我们分析一下 $XX^{\\top}$ 这一项。\n$$\nXX^{\\top} = \\left( \\sum_{c=1}^{C} \\mu_c e_c^{\\top} \\right) \\left( \\sum_{c'=1}^{C} e_{c'} \\mu_{c'}^{\\top} \\right) = \\sum_{c=1}^{C} \\sum_{c'=1}^{C} \\mu_c (e_c^{\\top} e_{c'}) \\mu_{c'}^{\\top} = \\sum_{c=1}^{C} \\mu_c \\mu_c^{\\top}\n$$\n需要求逆的矩阵是 $A = XX^{\\top} + \\lambda I_d = \\left(\\sum_{c=1}^{C} \\mu_c \\mu_c^{\\top}\\right) + \\lambda I_d$。\n让我们将这个矩阵 $A$ 应用到其中一个原型向量上，比如 $\\mu_t$：\n$$\nA\\mu_t = \\left( \\sum_{c=1}^{C} \\mu_c \\mu_c^{\\top} + \\lambda I_d \\right) \\mu_t = \\sum_{c=1}^{C} \\mu_c(\\mu_c^{\\top} \\mu_t) + \\lambda \\mu_t\n$$\n使用给定条件 $\\mu_c^{\\top}\\mu_t = m^2 \\delta_{ct}$：\n$$\nA\\mu_t = \\sum_{c=1}^{C} \\mu_c(m^2 \\delta_{ct}) + \\lambda \\mu_t = m^2 \\mu_t + \\lambda \\mu_t = (m^2 + \\lambda)\\mu_t\n$$\n这表明每个 $\\mu_t$ 都是 $A$ 的一个特征向量，其特征值为 $m^2+\\lambda$。因此，对于逆矩阵 $A^{-1}$，我们有：\n$$\nA^{-1}\\mu_t = \\frac{1}{m^2+\\lambda}\\mu_t\n$$\n最优权重矩阵 $W^{\\star}$ 是行向量 $w_1^{\\top}, \\dots, w_C^{\\top}$ 的堆叠。第 $c$ 行是 $w_c^{\\top} = e_c^{\\top} W^{\\star}$。由 $W^{\\star} = X^{\\top}A^{-1}$，第 $c$ 个行向量由 $w_c^{\\top} = \\mu_c^{\\top}A^{-1}$ 给出。对应的列向量是 $w_c = (w_c^{\\top})^{\\top} = (A^{-1})^{\\top}\\mu_c$。由于 $A$ 是对称的，其逆矩阵 $A^{-1}$ 也是对称的。因此，$w_c = A^{-1}\\mu_c$。\n$$\nw_c = \\frac{1}{m^2+\\lambda}\\mu_c\n$$\n所以 $W^{\\star}$ 的第 $c$ 行是 $w_c^{\\top} = \\frac{1}{m^2+\\lambda}\\mu_c^{\\top}$。这意味着整个权重矩阵是：\n$$\nW^{\\star} = \\frac{1}{m^2+\\lambda} \\begin{pmatrix} \\mu_1^{\\top} \\\\ \\vdots \\\\ \\mu_C^{\\top} \\end{pmatrix} = \\frac{1}{m^2+\\lambda}X^{\\top}\n$$\n现在我们为某个真实类别 $t$ 的测试特征 $x_{\\text{test}} = \\mu_t$ 计算 logits。logit 向量为 $z = W^{\\star}x_{\\text{test}}$。\n$$\nz = \\left(\\frac{1}{m^2+\\lambda}X^{\\top}\\right) \\mu_t = \\frac{1}{m^2+\\lambda} \\begin{pmatrix} \\mu_1^{\\top} \\\\ \\vdots \\\\ \\mu_C^{\\top} \\end{pmatrix} \\mu_t = \\frac{1}{m^2+\\lambda} \\begin{pmatrix} \\mu_1^{\\top}\\mu_t \\\\ \\vdots \\\\ \\mu_C^{\\top}\\mu_t \\end{pmatrix}\n$$\n再次使用 $\\mu_c^{\\top}\\mu_t = m^2 \\delta_{ct}$，logit 向量 $z$ 的第 $c$ 个分量是：\n$$\nz_c = \\frac{m^2 \\delta_{ct}}{m^2+\\lambda}\n$$\n对于真实类别 $c=t$，logit 是 $z_t = \\frac{m^2 \\delta_{tt}}{m^2+\\lambda} = \\frac{m^2}{m^2+\\lambda}$。对于任何不正确的类别 $c \\neq t$，logit 是 $z_c = \\frac{m^2 \\delta_{ct}}{m^2+\\lambda} = 0$。\n\n分类间隔定义为真实类别的 logit 与不正确类别中最大 logit 之间的差值。\n真实类别的 Logit：$z_t = \\frac{m^2}{m^2+\\lambda}$。不正确类别中的最大 logit：$\\max_{c \\neq t} z_c = \\max_{c \\neq t} 0 = 0$。因此，间隔是：\n$$\n\\text{间隔} = z_t - \\max_{c \\neq t} z_c = \\frac{m^2}{m^2+\\lambda} - 0 = \\frac{m^2}{m^2+\\lambda}\n$$", "answer": "$$\n\\boxed{\\frac{m^{2}}{m^{2}+\\lambda}}\n$$", "id": "3125783"}]}