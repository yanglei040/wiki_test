## 引言
现代[深度学习](@entry_id:142022)的巨大成功在很大程度上建立在海量标注数据的基础之上。然而，在许多现实世界的应用场景中，获取大规模、高质量的标注数据成本高昂甚至不切实际。人类却展现出一种非凡的能力——我们能从一个或几个例子中迅速学会识别新事物。为了弥合这一差距，[小样本学习](@entry_id:636112)（Few-Shot Learning, FSL）和[零样本学习](@entry_id:635210)（Zero-Shot Learning, ZSL）应运而生，它们旨在赋予机器学习模型类似人类的快速泛化能力，解决[数据稀疏性](@entry_id:136465)这一根本挑战。

本文将系统性地引导你进入这个激动人心的前沿领域。我们将从第一性原理出发，逐步揭示模型如何“[学会学习](@entry_id:638057)”，并利用先验知识来理解在训练期间从未见过或仅见过几次的新概念。

在接下来的内容中，你将学到：
- **原理与机制**：我们将深入剖析小样本与[零样本学习](@entry_id:635210)的核心方法论。你将理解情景式训练（episodic training）的框架，掌握以原型网络为代表的[度量学习](@entry_id:636905)方法，并探索如何利用属性和语言模型（如CLIP）进行零样本的语义迁移。
- **应用与[交叉](@entry_id:147634)学科联系**：我们将展示这些理论如何在现实世界中发挥作用，涵盖它们在[计算机视觉](@entry_id:138301)、自然语言处理、[机器人学](@entry_id:150623)等领域的广泛应用，并探讨如何将领域知识融入模型以应对[Sim2Real](@entry_id:637968)等挑战。
- **动手实践**：通过一系列精心设计的编程练习，你将亲手实现并验证关键算法，将抽象的理论转化为具体可操作的代码，从而真正巩固所学知识。

学完本文，你将不仅掌握小样本与[零样本学习](@entry_id:635210)的基本概念，更能理解其背后的科学原理和实践中的关键考量，为解决现实世界中的数据稀缺问题打下坚实的基础。

## 原理与机制

本章在前一章介绍性概述的基础上，深入探讨驱动[小样本学习](@entry_id:636112)（Few-Shot Learning, FSL）和[零样本学习](@entry_id:635210)（Zero-Shot Learning, ZSL）的核心科学原理与关键机制。我们将系统性地剖析这些领域的基本方法论，从[度量学习](@entry_id:636905)的几何直觉到基于属性和语言的语义迁移，最终考察在实际应用中遇到的泛化性、领[域适应](@entry_id:637871)和评估偏差等高级挑战。

### [小样本学习](@entry_id:636112)：原理与方法

[小样本学习](@entry_id:636112)的核心目标是构建能够从极少数标注样本中学习新概念的模型。这挑战了传统[深度学习](@entry_id:142022)对大规模标注数据的依赖。其成功的关键在于“[学会学习](@entry_id:638057)”（learning to learn）的[元学习](@entry_id:635305)[范式](@entry_id:161181)。

#### N-way k-shot 任务与情景式训练

[小样本学习](@entry_id:636112)问题通常被形式化为 **$N$-way $k$-shot** [分类任务](@entry_id:635433)。在此设定中，模型会接触到一个**支持集（support set）**，其中包含 $N$ 个新类别，每个类别有 $k$ 个标注样本。模型的任务是，利用这个支持集，对一个**查询集（query set）**中的无标签样本进行分类。

为了训练模型具备这种快速[适应能力](@entry_id:194789)，研究者们引入了**情景式训练（episodic training）**的框架。其核心思想是在训练阶段模拟测试阶段的任务[分布](@entry_id:182848)。具体而言，训练过程由一系列的**情景（episodes）**组成，每个情景本身就是一个小型的 $N$-way $k$-shot [分类任务](@entry_id:635433)。一个标准的情景构建流程如下 [@problem_id:3125751]：

1.  从包含大量类别（例如 $M$ 个基类）的基准数据集中，无放回地随机抽取 $N$ 个类别。
2.  对于这 $N$ 个类别中的每一个，从其所有样本中，无放回地随机抽取 $k$ 个样本构成支持集。
3.  从剩余的样本中，再为每个类别抽取 $q$ 个样本，构成查询集。

通过最小化在大量此类情景的查询集上的平均损失，模型并非在学习区分特定的基类，而是在学习一个通用的、能从任意一个小的支持集快速泛化到对应查询集的分类*算法*。

#### 基于度量的学习：原型网络

在[小样本学习](@entry_id:636112)的诸多方法中，**基于度量的学习（metric-based learning）**因其简洁和高效而备受关注。其核心思想是学习一个优质的**嵌入函数（embedding function）** $\phi$，该函数能将输入数据映射到一个[特征空间](@entry_id:638014)，在这个空间中，同类样本距离相近，异类样本距离遥远。

**原型网络（Prototypical Networks）**是基于度量的学习的典范。对于一个 $N$-way $k$-shot 任务，原型网络首先为每个类别 $c$ 计算一个**原型（prototype）**，该原型通常是该类别支持集样本嵌入的均值：
$$
\mu_c = \frac{1}{k} \sum_{i=1}^{k} \phi(x_i^c)
$$
其中 $\{x_i^c\}_{i=1}^k$ 是类别 $c$ 的支持集样本。这个原型可以被看作是类别 $c$ 在[嵌入空间](@entry_id:637157)中的“[质心](@entry_id:265015)”或典型代表。

随后，对于一个查询样本 $x$，模型通过计算其嵌入 $\phi(x)$ 与各个类别原型 $\mu_c$ 之间的距离（或非相似度）$d(\phi(x), \mu_c)$ 来进行分类。为了得到概率化的输出，通常会使用一个 softmax 函数作用于负的缩放非相似度上 [@problem_id:3125723]：
$$
p(y=c \mid x) = \frac{\exp(-\tau \cdot d(\phi(x), \mu_c))}{\sum_{c'} \exp(-\tau \cdot d(\phi(x), \mu_{c'}))}
$$
其中 $\tau > 0$ 是一个可调的[逆温](@entry_id:140086)度参数，用于控制[概率分布](@entry_id:146404)的锐度。

#### 度量的选择：从欧氏距离到学习[马氏距离](@entry_id:269828)

原型网络以及其他[度量学习](@entry_id:636905)方法的性能在很大程度上取决于非相似度函数 $d(\cdot, \cdot)$ 的选择。这个选择隐含了对[嵌入空间](@entry_id:637157)几何结构的假设。

*   **平方欧氏距离（Squared Euclidean Distance）**: $d_{\mathrm{E}}(u,v) = \|u-v\|_2^2$。这是最简单和最常见的选择。它等价于假设类别在[嵌入空间](@entry_id:637157)中形成球状（各向同性）的高斯簇。具体来说，如果每个类别 $c$ 的嵌入遵循一个协[方差](@entry_id:200758)为 $\sigma^2 I_d$（其中 $I_d$ 是[单位矩阵](@entry_id:156724)）且具有相同类[先验概率](@entry_id:275634)的[高斯分布](@entry_id:154414) $\mathcal{N}(\mu_c^\star, \sigma^2 I_d)$，那么[贝叶斯最优分类器](@entry_id:164732)就是选择与均值 $\mu_c^\star$ 的平方欧氏距离最小的类别。因此，使用平方欧氏距离的原型网络可以看作是在这种理想假设下，用样本均值 $\mu_c$ 作为真实均值 $\mu_c^\star$ 的一个“即插即用”式估计 [@problem_id:3125723] [@problem_id:3125755]。

*   **负余弦相似度（Negative Cosine Similarity）**: $d_{\mathrm{C}}(u,v) = - \cos(u,v) = -\frac{u^\top v}{\|u\|_2 \|v\|_2}$。当类别的区分主要依赖于嵌入向量的**方向**而非**模长**时，余弦相似度是更合适的度量。这种情况在许多文本[嵌入空间](@entry_id:637157)中很常见。余弦相似度的一个重要优点是它对向量的全局缩放具有不变性。例如，如果整个[嵌入空间](@entry_id:637157)被一个情节相关的因子 $\alpha$ 缩放（$\phi'(x) = \alpha \phi(x)$），欧氏距离会按 $\alpha^2$ 比例变化，导致模型置信度随 $\alpha$ 波动，损害**校准性（calibration）**。而余弦相似度由于其内在的归一化，其值不受影响，从而能提供更稳定的[置信度](@entry_id:267904)估计 [@problem_id:3125723]。

*   **学习[马氏距离](@entry_id:269828)（Learned Mahalanobis Distance）**: 现实中，数据簇很少是完美的球状。它们可能是被拉伸或旋转的椭球状，这意味着特征维度之间存在相关性，且[方差](@entry_id:200758)各不相同。这种情况可以用一个共享的、非各向异性的协方差矩阵 $\Sigma$ 来描述。在这种更一般的高斯模型下，[贝叶斯最优分类器](@entry_id:164732)使用的不再是欧氏距离，而是**[马氏距离](@entry_id:269828)**：$d_M(u, v) = (u-v)^\top \Sigma^{-1} (u-v)$。

    我们可以不直接假设 $\Sigma$ 的形式，而是学习一个通用的二次型度量 $d_W(u,v) = (u-v)^\top W (u-v)$，其中 $W$ 是一个[对称半正定矩阵](@entry_id:163376)。这在功能上等价于学习一个[马氏距离](@entry_id:269828)，其中 $W$ 扮演了[逆协方差矩阵](@entry_id:138450)的角色。一种有效的策略是，在基类上估计一个共享的类内[协方差矩阵](@entry_id:139155) $\widehat{\Sigma}$，然后用其逆 $\widehat{\Sigma}^{-1}$ 作为度量矩阵 $W$ 在新的小样本任务上进行分类。例如，我们可以使用汇集的类内[协方差矩阵](@entry_id:139155)的[最大似然估计](@entry_id:142509)：
    $$
    \widehat{\Sigma} = \frac{1}{N_b - K_b} \sum_{k=1}^{K_b} \sum_{i: y_i = k} (\phi(x_i) - \widehat{\mu}_k)(\phi(x_i) - \widehat{\mu}_k)^\top
    $$
    其中 $N_b$ 是基类样本总数，$K_b$ 是基[类数](@entry_id:156164)量，$\widehat{\mu}_k$ 是基类 $k$ 的经验均值。在实践中，为了确保 $\widehat{\Sigma}$ 的[可逆性](@entry_id:143146)，特别是当基[类数](@entry_id:156164)据不足以可靠地估计一个高维[协方差矩阵](@entry_id:139155)时，通常会加入一个岭正则化项：$\widehat{\Sigma}_\lambda = \widehat{\Sigma} + \lambda I_d$ [@problem_id:3125756]。通过这种方式，模型从基类中学习到了特征空间普遍的[方差](@entry_id:200758)和相关性结构，并利用这些知识在仅有少量样本的新类上进行更精确的[距离度量](@entry_id:636073)，这往往能带来显著的性能提升 [@problem_id:3125756]。

#### 基于优化的学习：微调线性头

与基于度量的方法不同，**基于优化的学习（optimization-based learning）**方法将小样本分类视为一个标准的微调问题。一个常见的策略是：冻结从基类上预训练好的[特征提取器](@entry_id:637338) $\phi$，然后在仅有的 $N \times k$ 个支持集样本上训练一个新的线性分类头（例如，一个 $N \times d$ 的权重矩阵 $W$）。该线性头通过最小化支持集上的[经验风险](@entry_id:633993)（如[交叉熵损失](@entry_id:141524)）来学习 [@problem_id:3125755]：
$$
\min_{W} - \sum_{(x,y) \in \text{support}} \log \left( \frac{\exp(w_y^\top \phi(x))}{\sum_{j=1}^N \exp(w_j^\top \phi(x))} \right) + \frac{\lambda}{2} \|W\|_F^2
$$
其中 $w_c^\top$ 是权重矩阵 $W$ 的第 $c$ 行。

与原型网络相比，训练线性分类头具有更高的灵活性。在数据充足（$k$ 较大）且[协方差矩阵](@entry_id:139155) $\Sigma$ 非各向异性的情况下，[线性分类器](@entry_id:637554)能够学习到近似贝叶斯最优的决策边界（即[线性判别分析](@entry_id:178689)，[LDA](@entry_id:138982)），而欧氏距离原型网络则受限于其错误的各向同性假设。此外，通过为每个类学习一个偏置项 $b_c$，[线性分类器](@entry_id:637554)还能对不均衡的类先验概率 $\pi_c$ 进行建模，这是原型网络默认无法做到的 [@problem_id:3125755]。然而，当 $k$ 极小（例如 $k=1$）时，微调一个[线性分类器](@entry_id:637554)很容易[过拟合](@entry_id:139093)。在这种情况下，原型网络作为一种更简单、正则化程度更高的模型（它将每个类的权重向量 $w_c$ 约束为该类的原型 $\mu_c$），通常表现得更稳健。

#### [小样本学习](@entry_id:636112)中的高级主题

*   **原型收缩（Prototype Shrinkage）**: 当 $k$ 非常小时，仅凭 $k$ 个样本计算出的经验均值 $\bar{\phi}_c$ 是对真实类别中心 $\mu_c^\star$ 的一个高[方差估计](@entry_id:268607)。为了提高其鲁棒性，我们可以采用**贝叶斯收缩（Bayesian shrinkage）**的思想。假设类别均值 $\mu_c$ 本身服从一个以某个先验原型 $\mu_0$ 为中心的[高斯先验](@entry_id:749752)[分布](@entry_id:182848) $\mathcal{N}(\mu_0, \lambda^{-1}\Sigma)$。那么，给定 $k$ 个观测样本，$\mu_c$ 的[后验均值](@entry_id:173826)（即[贝叶斯估计](@entry_id:137133)）可以被推导为一个加权平均值 [@problem_id:3125776]：
    $$
    \hat{\mu}_c = \frac{k}{k+\lambda} \bar{\phi}_c + \frac{\lambda}{k+\lambda} \mu_0
    $$
    这个估计量将经验均值 $\bar{\phi}_c$ “收缩”到了先验均值 $\mu_0$ 的方向。收缩的强度由数据量 $k$ 和先验精度 $\lambda$ 的比例决定。当 $k$ 很小时，估计值更多地依赖于稳定的先验 $\mu_0$；当 $k$ 增大时，估计值则更多地信任来自数据的证据 $\bar{\phi}_c$。

*   **直推式[小样本学习](@entry_id:636112)（Transductive FSL）**: 上述方法都是**归纳式（inductive）**的，即学习一个可以对任何*单个*查询样本进行分类的函数。**直推式（transductive）**学习则利用了查询集是*批量*给出的这一事实。其核心思想是，查询集样本之间的关系也包含了关于类别结构的宝贵信息。一种实现方式是基于图的**标签传播（label propagation）**[@problem_id:3125798]。我们可以构建一个包含所有支持集和查询集样本的图，节点是样本，边的权重表示它们在[嵌入空间](@entry_id:637157)的相似度（例如，使用高斯核函数 $w_{ij} = \exp(-\|\phi(x_i)-\phi(x_j)\|_2^2/\sigma^2)$）。然后，从支持集样本的已知标签出发，通过一个迭代过程将标签“传播”到图中的查询节点。这种方法通过在整个[数据流形](@entry_id:636422)上传播信息，有效地利用了查询集的内在结构，往往能比归纳式方法取得更好的性能。

### [零样本学习](@entry_id:635210)：原理与方法

[零样本学习](@entry_id:635210)（ZSL）将学习的极限推向了极致：模型需要在没有任何标注样本的情况下，对全新类别的实例进行分类。这听起来似乎不可能，但其关键在于利用辅助的**语义信息（semantic information）**作为类别之间的桥梁。

#### 核心思想：通过语义嵌入进行迁移

ZSL 的可行性依赖于一个共享的**语义空间**。在这个空间中，每个类别（无论是见过的还是未见过的）都由一个语义描述符来表示。这个描述符可以是：

1.  一组人工定义的**属性（attributes）**，例如，描述动物类别时，可能会有“有羽毛”、“会飞”、“食肉”等二元属性。
2.  从文本语料库（如维基百科）中提取的**词向量（word vectors）**。
3.  由现代[大型语言模型](@entry_id:751149)生成的**文本嵌入（text embeddings）**。

ZSL 模型的目标是学习一个从视觉[特征空间](@entry_id:638014)到这个共享语义空间的映射，或者反之。

#### 基于属性的[零样本学习](@entry_id:635210)

在基于属性的 ZSL 中，每个类别 $c$ 都与一个已知的属性向量 $a_c \in \mathbb{R}^A$ 相关联。一个经典的建模方法是假设存在一个线性映射 $W$，它能将属性向量转换为类别在视觉特征空间中的理想原型 [@problem_id:3125728]：
$$
\mathbb{E}[\phi(x) \mid y=c] = W a_c
$$
训练阶段，模型利用基类的视觉数据和它们的属性向量来学习矩阵 $W$。在测试阶段，对于一个来自未知类别 $c_{\text{new}}$ 的查询样本 $x$，模型可以预测其类别为：
$$
\hat{c}(x) = \arg\min_{c_{\text{new}}} \| \phi(x) - W a_{c_{\text{new}}} \|_2
$$
这种方法的成功取决于几个基本条件。首先，为了能从基类数据中唯一地辨识出 $W$，基类的属性向量必须具有足够的多样性（即，由基类属性向量构成的矩阵必须是满秩的）。如果基类属性[线性相关](@entry_id:185830)，那么 $W$ 的解就不是唯一的，导致泛化能力下降。其次，如果两个不同的类别（无论可见与否）碰巧具有相同的语义表示（$W a_{c_1} = W a_{c_2}$），那么在视觉[特征空间](@entry_id:638014)中它们将是不可区分的，这构成了模型的内在混淆边界 [@problem_id:3125728]。

#### 基于语言模型的现代[零样本学习](@entry_id:635210)

随着像 CLIP（Contrastive Language-Image Pre-training）这样的大规模多模态模型的出现，ZSL 的[范式](@entry_id:161181)发生了革命性的变化。这类模型通过在海量图文对上进行[对比学习](@entry_id:635684)，联合训练一个图像编码器 $\phi(x)$ 和一个[文本编码](@entry_id:755878)器 $g(t)$，使它们将匹配的图像和文本映射到[嵌入空间](@entry_id:637157)中的相近位置。

这种预训练好的模型可以直接用于 ZSL 分类，无需任何针对特定任务的训练。流程如下 [@problem_id:3125810]：

1.  **构建分类器**: 对于每个待分类别（例如，“猫”、“狗”），创建一个或多个描述性的文本提示，如 "a photo of a cat"。
2.  **编码提示**: 使用[文本编码](@entry_id:755878)器 $g(t)$ 将这些提示转换成类别原型向量。
3.  **分类**: 给定一个查询图像 $x$，用图像编码器 $\phi(x)$ 提取其特征。然后，[计算图](@entry_id:636350)像特征与所有类别原型向量之间的余弦相似度。分类决策基于相似度最高的类别，通常通过一个带温度的 softmax 函数来计算概率：
    $$
    p(y=c \mid x) \propto \exp(\tau \cdot \cos(\phi(x), g(t_c)))
    $$
    其中 $t_c$ 是类别 $c$ 的文本提示。

这种方法的性能对几个因素非常敏感：
*   **提示工程（Prompt Engineering）**: 文本提示的措辞会显著影响其嵌入向量，从而改变分类结果。例如，"a photo of a cat" 和 "an image of a feline" 会产生不同的嵌入和不同的相似度分数。使用多个不同措辞的提示并将其嵌入进行平均（**提示集成**），通常能提高分类的鲁棒性 [@problem_id:3125810]。
*   **温度缩放（Temperature Scaling）**: 温度参数 $\tau$ 控制着输出概率的“尖锐程度”。较高的 $\tau$ 会放大相似度得分之间的微小差异，导致模型对其预测更加“自信”。这会使得概率对相似度的变化更加敏感 [@problem_id:3125810]。
*   **类别数量**: softmax 函数的分母是对所有类别的指数得分求和。因此，当类别集合改变时（例如，增加一个新的类别），即使查询图像和某个特定类别的相似度保持不变，该类别的最终预测概率也会因为归一化项的改变而改变 [@problem_id:3125810]。

### 实践挑战与高级考量

尽管小样本和[零样本学习](@entry_id:635210)取得了显著进展，但在实际应用中仍面临诸多挑战，这些挑战要求我们对模型和评估方法有更深入的理解。

#### [领域偏移](@entry_id:637840)问题

模型通常在某个源领域（例如，网络图片）的基类上训练，但可能需要应用到[分布](@entry_id:182848)不同的目标领域（例如，[医学影像](@entry_id:269649)）。这种**[领域偏移](@entry_id:637840)（domain shift）**会严重影响模型性能。一个常见的假设是**[协变](@entry_id:634097)偏移（covariate shift）**，即特征的[边际分布](@entry_id:264862)发生变化 ($p_{\text{train}}(x) \neq p_{\text{test}}(x)$)，但类别[条件概率](@entry_id:151013)保持不变 ($p(y|x)$ 不变)。

在这种情况下，直接从目标领域的少量样本计算出的经验原型会是有偏的。一种修正方法是**[重要性加权](@entry_id:636441)（importance weighting）**。如果我们能估计出密度比 $w(x) = p_{\text{test}}(x)/p_{\text{train}}(x)$，我们就可以通过加权平均来计算一个修正后的原型，该原型是目标领域真实原型的[无偏估计](@entry_id:756289) [@problem_id:3125778]：
$$
\hat{\mu}_c^{\text{test}} = \frac{\sum_{i: y_i=c} w(x_i) \phi(x_i)}{\sum_{i: y_i=c} w(x_i)}
$$
这种[自归一化](@entry_id:636594)的估计器之所以有效，是因为即使类别的[先验概率](@entry_id:275634)在训练和测试领域之间发生变化，这个变化带来的常数因子也会在分式的分子和分母中被抵消掉。然而，[重要性加权](@entry_id:636441)依赖于一个关键假设：训练数据的支撑集必须覆盖测试数据的支撑集。如果测试时出现了训练时完全不可能出现的样本，该方法就会失效。此外，当权重 $w(x)$ [方差](@entry_id:200758)很大时，估计器的[方差](@entry_id:200758)会急剧增加，导致性能不稳定 [@problem_id:3125778]。

#### 泛化与评估偏差

*   **“Way”数不匹配**: 在情景式训练中，我们通常固定一个 $N$-way 的任务结构。但是，如果测试时的任务是 $N'$-way 且 $N' \neq N$，模型的性能可能会下降。这是因为基于 softmax 的[损失函数](@entry_id:634569)，其归一化项依赖于类别的数量 $N$。在 $N$ 个类别中训练出的区分度可能不足以在更多（$N'>N$）的类别中保持竞争力。一种缓解策略是在训练时就在一个变化的 $N$ 值范围内采样情景，使模型能适应不同数量的干扰项 [@problem_id:3125751]。

*   **类别划分泄露**: 在评估[小样本学习](@entry_id:636112)时，一个微妙的问题是基类和新类之间的语义关系。如果新类在语义上与某些基类非常相似（例如，新类是“波斯猫”，而基类中包含“虎斑猫”），那么模型在新类上的表现可能会被人为地“夸大”。这是因为[特征提取器](@entry_id:637338)已经在基类上学习了区分这些细粒度特征的能力。这种“泄露”并非来自标签，而是来自基类和新类在语义上的重叠。为了量化这种重叠，我们可以设计一个同时考虑类别划分和特征空间几何的指标。例如，我们可以计算新类原型与其对应超类在基类上学习到的平均方向之间的余弦相似度。这个指标越高，表明新类从基类的层次结构中学到的知识越多，其性能也越可能被这种语义重叠所增益 [@problem_id:3125770]。

*   **[负迁移](@entry_id:634593)（Negative Transfer）**: 从基类学习到的知识并非总是有益的。当基类任务与新类任务在本质上非常不相关或存在冲突时，强行进行迁移（例如，微调）可能会损害模型在新任务上的性能，这种现象被称为**[负迁移](@entry_id:634593)**。在实际应用中，尤其是在标签极度稀缺时，我们需要一种机制来判断是否应该进行[小样本学习](@entry_id:636112)。一个启发式的方法是，在进行适配前，先计算一个源领域和目标领域之间在特征空间的**对齐分数**。例如，可以计算基[类数](@entry_id:156164)据平均嵌入与新类少量可用样本平均嵌入之间的余弦相似度。如果该分数低于某个阈值，则表明两个领域可能存在显著的[分布](@entry_id:182848)差异，此时放弃小样本适配，转而使用更保守的[零样本学习](@entry_id:635210)方法可能是更安全的选择，以避免[负迁移](@entry_id:634593)的风险 [@problem_id:3125802]。