{"hands_on_practices": [{"introduction": "神经切向核（NTK）不仅定义了一个特征空间，它还主导了网络在该空间中的学习方式。这个练习揭示了NTK动力学的一个基本原则：网络沿着NTK格拉姆矩阵（Gram matrix）最大特征值对应的特征向量方向学习目标函数的速度最快。通过在一个小而明确的问题上模拟梯度下降，你将直观地理解核的光谱特性如何转化为具体的学习速率[@problem_id:3159036]。", "problem": "给定一个小的神经切向核 (NTK) Gram 矩阵，要求你分析在训练集上相应核回归中的梯度下降动态。神经切向核 (NTK) Gram 矩阵 $K \\in \\mathbb{R}^{n \\times n}$ 是对称半正定的，它作为线性算子作用于 $n$ 个训练样本上的预测向量。任务是通过显式计算来证明，当目标向量与 $K$ 的最大特征向量对齐时，核梯度下降下的误差会在几步内迅速衰减。\n\n基本基础和设置：\n- 训练集上的核回归训练损失是平方误差 $L(f) = \\tfrac{1}{2} \\lVert f - y \\rVert_2^2$，其中 $f \\in \\mathbb{R}^n$ 是模型在 $n$ 个训练点上的预测向量，而 $y \\in \\mathbb{R}^n$ 是目标向量。\n- 学习率为 $\\eta$ 且 NTK Gram 矩阵为 $K$ 的核梯度下降（在函数空间中）执行以下更新\n$$\nf_{t+1} = f_t - \\eta \\, K \\,(f_t - y),\n$$\n从 $f_0 = 0$ 开始。这是对 $L(f)$ 应用梯度下降的结果，因为 $\\nabla_f L = f - y$，并且 NTK 在函数空间中用 $K$ 对更新进行预处理。\n- 设特征分解为 $K = U \\Lambda U^\\top$，其中 $U = [u_1, \\dots, u_n]$ 是标准正交的，$\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ 且 $\\lambda_1 \\ge \\cdots \\ge \\lambda_n \\ge 0$。任何目标 $y$ 都可以分解为 $y = \\sum_{i=1}^n c_i u_i$，其中 $c_i = u_i^\\top y$。\n\n你的程序必须：\n1. 使用固定的 NTK Gram 矩阵\n$$\nK = \\begin{bmatrix}\n2  1  0 \\\\\n1  2  1 \\\\\n0  1  2\n\\end{bmatrix}.\n$$\n2. 计算其特征分解，以获得特征值和相应的单位特征向量 $u_1, u_2, u_3$，其中 $u_1$ 与最大特征值相关联。\n3. 对于每个测试用例，构造指定的目标 $y \\in \\mathbb{R}^3$，使用学习率 $\\eta$ 从 $f_0=0$ 开始运行核梯度下降指定的步数 $T$，并计算训练损失\n$$\nL_T = \\tfrac{1}{2} \\lVert f_T - y \\rVert_2^2.\n$$\n4. 将所有测试用例的结果输出为一行，其中包含一个 Python 风格的列表，每个浮点数四舍五入到六位小数。\n\n测试套件：\n- 用例 1：$y = u_1$（单位向量），$\\eta = 0.25$，$T = 3$。\n- 用例 2：$y = u_3$（单位向量），$\\eta = 0.25$，$T = 3$。\n- 用例 3：$y = \\mathrm{normalize}(0.8\\, u_1 + 0.2\\, u_3)$ 至单位范数，$\\eta = 0.25$，$T = 3$。这里的 $0.8$ 和 $0.2$ 是实系数，$\\mathrm{normalize}(\\cdot)$ 将向量缩放至单位欧几里得范数。\n- 用例 4：$y = u_1$，$\\eta = 0.1$，$T = 3$。\n- 用例 5：$y = u_1$，$\\eta = 0.25$，$T = 1$。\n\n角度单位不适用。没有物理单位。所有输出必须是浮点数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是对应测试用例的 $L_T$ 值，四舍五入到六位小数。", "solution": "该问题是有效的，因为它在科学上基于核方法理论，定义和数据完整一致，问题阐述清晰，并且表述客观。我们将继续提供完整解答。\n\n该问题要求我们分析特定神经切向核 (NTK) Gram 矩阵 $K$ 的核梯度下降动态。我们将首先推导训练损失动态的一般解析表达式，然后将其应用于给定的矩阵和测试用例。这将展示收敛速度如何取决于目标向量与核矩阵特征向量的对齐程度。\n\n设 $f_t \\in \\mathbb{R}^n$ 是模型在第 $t$ 步时在 $n$ 个训练点上的预测向量，设 $y \\in \\mathbb{R}^n$ 是目标向量。训练损失是平方误差 $L(f) = \\frac{1}{2} \\lVert f - y \\rVert_2^2$。\n学习率为 $\\eta$ 且 NTK Gram 矩阵为 $K$ 的核梯度下降更新规则如下：\n$$\nf_{t+1} = f_t - \\eta K (f_t - y)\n$$\n我们从初始条件 $f_0 = 0$ 开始。设第 $t$ 步的误差向量为 $e_t = f_t - y$。初始误差为 $e_0 = f_0 - y = -y$。\n我们可以通过在两边减去 $y$ 来用误差向量表示更新规则：\n$$\nf_{t+1} - y = (f_t - y) - \\eta K (f_t - y)\n$$\n$$\ne_{t+1} = e_t - \\eta K e_t = (I - \\eta K) e_t\n$$\n其中 $I$ 是 $n \\times n$ 的单位矩阵。这是一个关于误差向量的线性动力学系统。通过展开递归，我们得到第 $t$ 步的误差：\n$$\ne_t = (I - \\eta K)^t e_0 = - (I - \\eta K)^t y\n$$\n第 $T$ 步的训练损失是 $L_T = \\frac{1}{2} \\lVert e_T \\rVert_2^2$。代入 $e_T$ 的表达式：\n$$\nL_T = \\frac{1}{2} \\left\\lVert - (I - \\eta K)^T y \\right\\rVert_2^2 = \\frac{1}{2} \\left\\lVert (I - \\eta K)^T y \\right\\rVert_2^2\n$$\n为了分析这个表达式，我们使用对称矩阵 $K$ 的特征分解，即 $K = U \\Lambda U^\\top$。这里，$U$ 是一个标准正交矩阵，其列是特征向量 $u_1, u_2, \\dots, u_n$，$\\Lambda$ 是相应特征值 $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$ 构成的对角矩阵。\n算子 $(I - \\eta K)$ 可以在相同的基中对角化：\n$$\nI - \\eta K = U I U^\\top - \\eta U \\Lambda U^\\top = U (I - \\eta \\Lambda) U^\\top\n$$\n将其提升到 $T$ 次方：\n$$\n(I - \\eta K)^T = U (I - \\eta \\Lambda)^T U^\\top\n$$\n将其代入 $L_T$ 的表达式中：\n$$\nL_T = \\frac{1}{2} \\left\\lVert U (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\n由于 $U$ 是一个标准正交矩阵，它保持欧几里得范数，即对于任何向量 $v$，$\\lVert Uv \\rVert_2 = \\lVert v \\rVert_2$。因此：\n$$\nL_T = \\frac{1}{2} \\left\\lVert (I - \\eta \\Lambda)^T U^\\top y \\right\\rVert_2^2\n$$\n向量 $U^\\top y$ 表示 $y$ 在特征向量基中的坐标。其第 $i$ 个分量是 $c_i = u_i^\\top y$。矩阵 $(I - \\eta \\Lambda)^T$ 是对角矩阵，其对角线上的元素为 $(1 - \\eta \\lambda_i)^T$。将此对角矩阵应用于向量 $U^\\top y$ 会将其第 $i$ 个分量缩放 $(1 - \\eta \\lambda_i)^T$ 倍。所得向量的平方范数是其各分量平方的和：\n$$\nL_T = \\frac{1}{2} \\sum_{i=1}^n \\left( c_i (1 - \\eta \\lambda_i)^T \\right)^2 = \\frac{1}{2} \\sum_{i=1}^n c_i^2 (1 - \\eta \\lambda_i)^{2T}\n$$\n这个最终表达式揭示了核心原理：目标 $y$ 的每个特征分量对损失的贡献呈指数级衰减。第 $i$ 个分量的衰减率由因子 $(1 - \\eta \\lambda_i)^2$ 决定。对于稳定的学习率，其中 $0  \\eta \\lambda_i  2$，该因子小于 1。较大的特征值 $\\lambda_i$ 会导致较小的因子 $(1 - \\eta \\lambda_i)^2$，从而使该分量收敛得更快。\n\n现在，我们对给定的问题进行显式计算。\nNTK Gram 矩阵为：\n$$\nK = \\begin{bmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{bmatrix}\n$$\n特征值是特征方程 $\\det(K - \\lambda I) = 0$ 的根，它们是 $\\lambda_1 = 2 + \\sqrt{2}$、$\\lambda_2 = 2$ 和 $\\lambda_3 = 2 - \\sqrt{2}$。相应的标准正交特征向量是：\n$$\nu_1 = \\begin{pmatrix} 1/2 \\\\ \\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}, \\quad u_2 = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\end{pmatrix}, \\quad u_3 = \\begin{pmatrix} 1/2 \\\\ -\\sqrt{2}/2 \\\\ 1/2 \\end{pmatrix}\n$$\n我们现在为每个测试用例评估损失 $L_T$。\n\n用例 1：$y = u_1$，$\\eta = 0.25$，$T = 3$。\n目标 $y$ 与第一个特征向量 $u_1$ 对齐。系数为 $c_1 = u_1^\\top y = u_1^\\top u_1 = 1$，$c_2 = 0$，$c_3 = 0$。\n损失完全由第一个特征分量决定：\n$L_3 = \\frac{1}{2} c_1^2 (1 - \\eta \\lambda_1)^{2T} = \\frac{1}{2} (1 - 0.25 (2+\\sqrt{2}))^6 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^6 \\approx 0.000004$。\n\n用例 2：$y = u_3$，$\\eta = 0.25$，$T = 3$。\n目标 $y$ 与第三个特征向量 $u_3$ 对齐。系数为 $c_1 = 0$，$c_2 = 0$，$c_3 = 1$。\n损失完全由第三个特征分量决定：\n$L_3 = \\frac{1}{2} c_3^2 (1 - \\eta \\lambda_3)^{2T} = \\frac{1}{2} (1 - 0.25 (2-\\sqrt{2}))^6 = \\frac{1}{2} (0.5 + \\frac{\\sqrt{2}}{4})^6 \\approx 0.192317$。\n与用例 1 相比，收敛速度要慢得多，因为 $\\lambda_3$ 很小。\n\n用例 3：$y = \\mathrm{normalize}(0.8 u_1 + 0.2 u_3)$，$\\eta = 0.25$，$T = 3$。\n设 $v = 0.8 u_1 + 0.2 u_3$。则 $y = v / \\lVert v \\rVert_2$。由于 $u_1$ 和 $u_3$ 是标准正交的，$\\lVert v \\rVert_2 = \\sqrt{0.8^2 + 0.2^2} = \\sqrt{0.68}$。\n系数为 $c_1 = u_1^\\top y = 0.8/\\sqrt{0.68}$ 和 $c_3 = u_3^\\top y = 0.2/\\sqrt{0.68}$，其中 $c_2 = 0$。\n所以，$c_1^2 = 0.64/0.68 = 16/17$ 且 $c_3^2 = 0.04/0.68 = 1/17$。\n损失是第一和第三分量贡献的加权和：\n$L_3 = \\frac{1}{2} [c_1^2 (1 - \\eta \\lambda_1)^6 + c_3^2 (1 - \\eta \\lambda_3)^6] = \\frac{1}{2} [ \\frac{16}{17} (0.5 - \\frac{\\sqrt{2}}{4})^6 + \\frac{1}{17} (0.5 + \\frac{\\sqrt{2}}{4})^6 ] \\approx 0.011317$。\n最终误差由与 $u_3$ 相关的缓慢衰减分量主导。\n\n用例 4：$y = u_1$，$\\eta = 0.1$，$T = 3$。\n这与用例 1 类似，但学习率更小。$c_1 = 1$，$c_2 = 0$，$c_3 = 0$。\n$L_3 = \\frac{1}{2} (1 - 0.1 \\lambda_1)^6 = \\frac{1}{2} (1 - 0.1(2+\\sqrt{2}))^6 = \\frac{1}{2} (0.8 - \\frac{\\sqrt{2}}{10})^6 \\approx 0.040727$。\n较小的学习率导致衰减因子 $(1 - \\eta \\lambda_1)$ 更接近 1，因此与用例 1 相比收敛更慢。\n\n用例 5：$y = u_1$，$\\eta = 0.25$，$T = 1$。\n这与用例 1 类似，但步数更少。$c_1 = 1$，$c_2 = 0$，$c_3 = 0$。\n$L_1 = \\frac{1}{2} (1 - \\eta \\lambda_1)^{2 \\times 1} = \\frac{1}{2} (1 - 0.25(2+\\sqrt{2}))^2 = \\frac{1}{2} (0.5 - \\frac{\\sqrt{2}}{4})^2 \\approx 0.010723$。\n仅一步，与用例 1 相比，误差衰减的时间更少。\n\n结果证实了理论分析：在核梯度下降中，与 NTK Gram 矩阵较大特征值对应的特征向量对齐的目标分量学习得快得多。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training loss for kernel gradient descent under several scenarios.\n    \"\"\"\n    # 1. Define the fixed NTK Gram matrix K.\n    K = np.array([\n        [2.0, 1.0, 0.0],\n        [1.0, 2.0, 1.0],\n        [0.0, 1.0, 2.0]\n    ])\n\n    # 2. Compute the eigendecomposition of K.\n    # np.linalg.eigh returns eigenvalues in ascending order. We sort them descending.\n    eigenvalues, eigenvectors = np.linalg.eigh(K)\n    sort_indices = np.argsort(eigenvalues)[::-1]\n    # lambdas = eigenvalues[sort_indices] # Not used directly in simulation\n    U = eigenvectors[:, sort_indices]\n\n    # Extract the unit eigenvectors u1, u2, u3.\n    u1 = U[:, 0]\n    # u2 = U[:, 1] # Not used in test cases\n    u3 = U[:, 2]\n\n    # 3. Define the test suite.\n    # Each case is a tuple: (y_description, eta, T)\n    # y_description is a tuple: (type, spec)\n    test_cases = [\n        # Case 1: y = u1, eta = 0.25, T = 3\n        (('eigenvector', u1), 0.25, 3),\n        # Case 2: y = u3, eta = 0.25, T = 3\n        (('eigenvector', u3), 0.25, 3),\n        # Case 3: y = normalize(0.8*u1 + 0.2*u3), eta = 0.25, T = 3\n        (('combo', (0.8, u1, 0.2, u3)), 0.25, 3),\n        # Case 4: y = u1, eta = 0.1, T = 3\n        (('eigenvector', u1), 0.1, 3),\n        # Case 5: y = u1, eta = 0.25, T = 1\n        (('eigenvector', u1), 0.25, 1),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        y_desc, eta, T = case\n        y_type, y_spec = y_desc\n\n        # Construct the target vector y for the current case.\n        if y_type == 'eigenvector':\n            y = y_spec\n        elif y_type == 'combo':\n            c1, v1, c2, v2 = y_spec\n            v = c1 * v1 + c2 * v2\n            y = v / np.linalg.norm(v)\n        else:\n            raise ValueError(\"Unknown y_type\")\n        \n        # Initialize predictions f_0 = 0.\n        f = np.zeros_like(y)\n\n        # Run kernel gradient descent for T steps.\n        for _ in range(T):\n            gradient_term = f - y\n            update = K @ gradient_term\n            f = f - eta * update\n        \n        # This is f_T. Now compute the final loss L_T.\n        loss = 0.5 * np.linalg.norm(f - y)**2\n        results.append(loss)\n\n    # 4. Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3159036"}, {"introduction": "与任何核一样，NTK也拥有一个揭示其内在“特征空间”的特征系统。本练习通过一个具体的例子，让你直观地理解这一点：你将使用傅里叶特征构建一个NTK，并证明其特征函数确实是熟悉的傅里叶模式。这项实践将加深你对模型架构（即其使用的特征）与其对应NTK结构之间联系的理解[@problem_id:3159099]。", "problem": "给定一个一维实数输入域，其中的点在周期性区间上采样。考虑一个由固定傅里叶特征构建的参数线性模型，及其关联的神经正切核 (NTK)。具体来说，该模型定义为\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0 \\cdot \\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big),\n$$\n其中参数为 $\\theta = \\big(a_0, \\{a_k^{(c)}, a_k^{(s)}\\}_{k=1}^M\\big)$，固定特征为\n$$\n\\phi_0(x)=1,\\quad \\phi_k^{(c)}(x)=\\cos(2\\pi k x),\\quad \\phi_k^{(s)}(x)=\\sin(2\\pi k x),\n$$\n对于整数 $k \\ge 1$。此处 $\\alpha_k \\ge 0$ 是非负标量，用于设定每个频率的相对贡献。使用神经正切核 (NTK) 的定义，\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top} \\nabla_{\\theta} f(x';\\theta_0),\n$$\n证明对于此模型，核函数为\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big),\n$$\n其中所有角度均以弧度为单位。考虑单位圆上的一组离散等距点，\n$$\nx_n \\;=\\; \\frac{n}{N}, \\quad n \\in \\{0,1,\\dots,N-1\\},\n$$\n并令 $K \\in \\mathbb{R}^{N\\times N}$ 为核矩阵，其元素为 $K_{n,m} = k(x_n,x_m)$。注意 $K$ 是实对称的，并且仅依赖于差值 $n-m$ 模 $N$，因此它是一个循环矩阵。\n\n任务：\n1) 数值计算特征分解 $K = U \\Lambda U^{\\top}$，其中 $U$ 是一个 $N\\times N$ 的标准正交矩阵，$\\Lambda$ 是一个 $N\\times N$ 的对角矩阵，其对角线上的元素为非负值。\n2) 将特征函数与傅里叶模式关联：对于循环矩阵，复数离散傅里叶变换 (DFT) 矩阵可以对角化 $K$，其特征值由 $K$ 第一行的 DFT 给出。利用这一事实，通过计算第一行的 DFT，生成特征值的解析参考。\n3) 通过将其特征值与第一行 DFT 得出的解析特征值进行比较，验证数值特征分解的正确性，并报告最大绝对差。\n\n你的程序必须为以下测试套件实现上述任务。对于每个测试用例，给定 $N$ 和一个包含非零 $\\alpha_k$ 值的字典。任何未列出的 $\\alpha_k$ 都应视为零。所有角度均以弧度为单位。\n\n测试套件：\n- 测试 1 (单个非零频率)：$N=8$, $\\{\\alpha_0=0.0,\\;\\alpha_1=2.0\\}$。\n- 测试 2 (包含奈奎斯特项的混合)：$N=16$, $\\{\\alpha_0=1.5,\\;\\alpha_1=1.0,\\;\\alpha_2=0.5,\\;\\alpha_8=0.25\\}$。\n- 测试 3 (纯常数核)：$N=10$, $\\{\\alpha_0=3.0\\}$。\n\n你的程序应：\n- 对每个测试用例，使用上面的精确核公式构造 $K$。\n- 通过对称特征求解器计算 $K$ 的数值特征值。\n- 将 $K$ 的第一行的复数 DFT 作为解析特征值进行计算。\n- 通过计算每个测试用例中排序后的数值特征值和解析特征值列表之间的最大绝对差，对它们进行比较。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个条目是对应测试用例的最大绝对差（一个浮点数），四舍五入到十位小数，并按上述测试的顺序排列。例如，如果差异在舍入误差范围内数值上为零，则形如“[0.0,0.0,0.0]”的输出是可以接受的。不应打印任何额外文本。", "solution": "我们从神经正切核 (NTK) 的定义开始。对于一个模型 $f(x;\\theta)$，在初始化 $\\theta_0$ 处的 NTK 为\n$$\nk(x,x') \\;=\\; \\nabla_{\\theta} f(x;\\theta_0)^{\\top}\\nabla_{\\theta} f(x';\\theta_0).\n$$\n当模型对其参数是线性的，且具有固定的特征映射时，关于参数的梯度与 $\\theta_0$ 无关，且等于特征向量。在当前情况下，模型是\n$$\nf(x;\\theta) \\;=\\; \\sqrt{\\alpha_0}\\,a_0\\,\\phi_0(x) \\;+\\; \\sum_{k=1}^{M} \\sqrt{\\alpha_k}\\,\\Big(a_k^{(c)} \\,\\phi_k^{(c)}(x) \\;+\\; a_k^{(s)} \\,\\phi_k^{(s)}(x)\\Big).\n$$\n关于参数的梯度是\n$$\n\\nabla_{\\theta} f(x;\\theta) \\;=\\; \\big[\\sqrt{\\alpha_0}\\,\\phi_0(x),\\;\\{\\sqrt{\\alpha_k}\\,\\phi_k^{(c)}(x),\\sqrt{\\alpha_k}\\,\\phi_k^{(s)}(x)\\}_{k=1}^M\\big]^{\\top}.\n$$\n因此，NTK 是这些梯度的内积：\n$$\n\\begin{aligned}\nk(x,x') \n&= \\alpha_0 \\,\\phi_0(x)\\phi_0(x') \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\phi_k^{(c)}(x)\\phi_k^{(c)}(x') \\;+\\; \\phi_k^{(s)}(x)\\phi_k^{(s)}(x')\\Big) \\\\\n&= \\alpha_0 \\cdot 1 \\cdot 1 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\Big(\\cos(2\\pi k x)\\cos(2\\pi k x') \\;+\\; \\sin(2\\pi k x)\\sin(2\\pi k x')\\Big).\n\\end{aligned}\n$$\n使用三角恒等式 $\\cos(A)\\cos(B)+\\sin(A)\\sin(B)=\\cos(A-B)$，我们得到\n$$\nk(x,x') \\;=\\; \\alpha_0 \\;+\\; \\sum_{k=1}^{M} \\alpha_k \\,\\cos\\!\\big(2\\pi k (x - x')\\big).\n$$\n所有角度均以弧度为单位。该核在圆上是平移不变的，仅依赖于差值 $x-x'$ 模 $1$。\n\n现在考虑离散点集 $x_n = n/N$ 对于 $n\\in\\{0,\\dots,N-1\\}$。定义核矩阵 $K \\in \\mathbb{R}^{N\\times N}$，其元素为 $K_{n,m} = k(x_n,x_m)$。由于 $k$ 仅依赖于 $x_n - x_m$ 模 $1$，并且 $x_n - x_m = (n-m)/N$ 模 $1$，所以 $K$ 仅依赖于 $(n-m)\\bmod N$。因此，$K$ 是一个实对称循环矩阵。循环矩阵的一个核心性质是它们可以被复数离散傅里叶变换 (DFT) 矩阵对角化。设 $F$ 为 $N\\times N$ 的 DFT 矩阵，其元素为 $F_{r,m} = \\exp\\!\\big(-2\\pi i r m / N\\big)$ 对于 $r,m\\in\\{0,\\dots,N-1\\}$。那么，\n$$\nK \\;=\\; \\frac{1}{N} F^{\\ast} \\,\\mathrm{diag}(\\lambda_0,\\dots,\\lambda_{N-1})\\, F,\n$$\n其中 $(\\lambda_r)_{r=0}^{N-1}$ 是 $K$ 的特征值，并由 $K$ 的第一行的 DFT 给出。如果第一行是 $(c_0,\\dots,c_{N-1})$，则\n$$\n\\lambda_r \\;=\\; \\sum_{m=0}^{N-1} c_m \\, e^{-2\\pi i r m / N}, \\quad r=0,\\dots,N-1.\n$$\n由于 $K$ 是实对称的，且 $c_m$ 是实的并满足 $c_m=c_{N-m}$，因此特征值 $\\lambda_r$ 是实数且非负。对应的特征向量是复数傅里叶模式，在实数特征分解中，它们表现为跨越相同特征空间的余弦和正弦对。\n\n算法规划：\n1) 使用给定的 $N$ 和 $\\{\\alpha_k\\}$，通过公式\n$$\nK_{n,m} \\;=\\; \\alpha_0 \\;+\\; \\sum_{k\\ge 1} \\alpha_k \\,\\cos\\!\\Big(2\\pi k \\,\\frac{n-m}{N}\\Big).\n$$\n构造核矩阵 $K$。\n2) 使用对称特征求解器计算数值特征分解以获得特征值。按升序对它们进行排序。\n3) 通过对 $K$ 的第一行进行复数 DFT 来计算解析特征值。提取其实部（数值上的虚部是由浮点舍入引起的），并按升序排序。\n4) 对于每个测试用例，报告排序后的数值特征值和排序后的解析特征值之间的最大绝对差。一个很小的差值可以验证特征分解以及特征函数与傅里叶模式之间关系的正确性。\n\n测试套件详情：\n- 测试 1：$N=8$, $\\alpha_0=0.0$, $\\alpha_1=2.0$。该核是秩为 2 的，在频率 1 和 7 处有非零特征值；每个预期值为 $N\\alpha_1/2 = 8$。\n- 测试 2：$N=16$, $\\alpha_0=1.5$, $\\alpha_1=1.0$, $\\alpha_2=0.5$, $\\alpha_8=0.25$。预期的特征值在 $r=0$ 处为 $N\\alpha_0=24$，在 $r=1,15$ 处为 $N\\alpha_1/2=8$，在 $r=2,14$ 处为 $N\\alpha_2/2=4$，在奈奎斯特频率 $r=8$ 处为 $N\\alpha_8=4$。所有其他特征值为零。\n- 测试 3：$N=10$, $\\alpha_0=3.0$。这是一个纯常数核，只有一个非零特征值 $N\\alpha_0=30$ 在 $r=0$ 处，其他地方均为零。\n\n程序实现了这些步骤，并打印单行输出，其中包含一个包含三个浮点数的列表：测试 1-3 的最大绝对特征值差异，四舍五入到十位小数。这通过循环结构和 DFT，将等距点上 NTK 的特征分解与傅里叶模式直接联系起来。", "answer": "```python\nimport numpy as np\n\ndef build_kernel_and_first_row(N: int, alpha: dict) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Build the NTK kernel matrix K and its first row c for evenly spaced points x_n = n/N on [0,1).\n    Kernel: k(x, x') = alpha_0 + sum_{k>=1} alpha_k cos(2*pi*k*(x - x')).\n    Angles are in radians.\n\n    Parameters:\n        N: number of evenly spaced points.\n        alpha: dict mapping frequency k to alpha_k (nonnegative). Unspecified k treated as 0.\n\n    Returns:\n        K: NxN kernel matrix.\n        c: length-N first row of K.\n    \"\"\"\n    n = np.arange(N)\n    # Differences tau = n - m mod N; shape (N, N)\n    tau = (n[:, None] - n[None, :]) % N\n    tau = tau.astype(float)\n\n    # Start with alpha_0 term\n    a0 = alpha.get(0, 0.0)\n    K = np.full((N, N), fill_value=a0, dtype=float)\n\n    # Add cosine terms for k >= 1\n    for k, ak in alpha.items():\n        if k == 0 or ak == 0.0:\n            continue\n        # cos(2*pi*k*(n-m)/N)\n        K += ak * np.cos(2.0 * np.pi * k * tau / N)\n\n    # First row c_m = K_{0, m}\n    c = K[0, :].copy()\n    return K, c\n\ndef numerical_sorted_eigenvalues(K: np.ndarray) -> np.ndarray:\n    \"\"\"Compute and return sorted eigenvalues of a real symmetric matrix K.\"\"\"\n    w = np.linalg.eigh(K)[0]\n    return np.sort(w)\n\ndef analytical_sorted_eigenvalues_from_first_row(c: np.ndarray) -> np.ndarray:\n    \"\"\"\n    For a circulant matrix with first row c, eigenvalues are the DFT of c.\n    Return sorted real parts of these eigenvalues.\n    \"\"\"\n    lam = np.fft.fft(c)\n    lam_real = np.real_if_close(lam, tol=1000)  # strip negligible imaginary parts\n    lam_real = lam_real.astype(float)\n    return np.sort(lam_real)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays of same shape.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef run_test_cases():\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        # Test 1: N=8, alpha_0=0.0, alpha_1=2.0\n        {\"N\": 8, \"alpha\": {0: 0.0, 1: 2.0}},\n        # Test 2: N=16, alpha_0=1.5, alpha_1=1.0, alpha_2=0.5, alpha_8=0.25\n        {\"N\": 16, \"alpha\": {0: 1.5, 1: 1.0, 2: 0.5, 8: 0.25}},\n        # Test 3: N=10, alpha_0=3.0\n        {\"N\": 10, \"alpha\": {0: 3.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        alpha = case[\"alpha\"]\n        K, c = build_kernel_and_first_row(N, alpha)\n\n        # Numerical eigenvalues (sorted)\n        w_num = numerical_sorted_eigenvalues(K)\n\n        # Analytical eigenvalues via DFT of first row (sorted)\n        w_ana = analytical_sorted_eigenvalues_from_first_row(c)\n\n        # Compute maximum absolute difference\n        diff = max_abs_diff(w_num, w_ana)\n        results.append(diff)\n\n    # Print results as a single line with specified format: list of floats rounded to 10 decimal places.\n    formatted = \"[\" + \",\".join(f\"{val:.10f}\" for val in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    run_test_cases()\n```", "id": "3159099"}, {"introduction": "NTK理论在形式上是无限宽度网络的近似。这个动手实践将让你直面这一近似，通过比较理想化的NTK训练动力学与有限宽度网络的实际训练过程。通过量化两者之间的偏差，你将探索在何种条件下（如网络宽度和学习率）NTK能够忠实地模拟训练过程，以及何时更复杂的现象（如“特征学习”）会导致真实网络的行为偏离理论预测[@problem_id:3159054]。", "problem": "你需要实现并比较一个双层修正线性单元 (ReLU) 神经网络在固定回归任务上的两种训练动态，并量化它们之间的偏差。这两种动态是：(a) 在实际的有限宽度网络上进行全批量梯度下降（允许特征改变），以及 (b) 训练通过网络在其随机初始化周围进行一阶泰勒展开得到的线性化模型（这会保持特征固定，并对应于在神经正切核 (NTK) 近似下进行训练）。你的目标是构建一个案例，其中由于特征学习，有限宽度网络的训练偏离了 NTK 的预测，并量化该偏差。\n\n定义和设置：\n- 考虑一个宽度为 $w$、具有标量输出的双层全连接 ReLU 网络，对于输入 $\\mathbf{x} \\in \\mathbb{R}^d$，其定义为\n$$\nf(\\mathbf{x}; W, \\mathbf{a}) \\;=\\; \\frac{1}{\\sqrt{w}} \\sum_{j=1}^{w} a_j \\, \\max\\{0, \\langle \\mathbf{w}_j, \\mathbf{x} \\rangle\\},\n$$\n其中 $W \\in \\mathbb{R}^{w \\times d}$ 的行向量为 $\\mathbf{w}_j \\in \\mathbb{R}^d$，且 $\\mathbf{a} \\in \\mathbb{R}^{w}$。\n- 使用固定的随机种子 $2024$，通过独立的标准正态分布采样来初始化参数：每个 $W_{jk} \\sim \\mathcal{N}(0,1)$ 和 $a_j \\sim \\mathcal{N}(0,1)$。\n- 使用平均平方损失\n$$\n\\mathcal{L}(W,\\mathbf{a}) \\;=\\; \\frac{1}{2n} \\sum_{i=1}^{n} \\big(f(\\mathbf{x}_i; W,\\mathbf{a}) - y_i \\big)^2,\n$$\n其中 $(\\mathbf{x}_i,y_i)$ 是训练样本。\n- 对全网络使用学习率为 $\\eta$ 和 $T$ 步的批量梯度下降：\n$$\n(W^{t+1}, \\mathbf{a}^{t+1}) \\;=\\; (W^{t}, \\mathbf{a}^{t}) - \\eta \\, \\nabla \\mathcal{L}(W^{t}, \\mathbf{a}^{t}),\n$$\n从初始化给定的 $(W^0,\\mathbf{a}^0)$ 开始。\n\n线性化 (NTK) 模型：\n- 令 $\\boldsymbol{\\theta}$ 表示所有参数堆叠成的单个向量，令 $\\mathbf{f}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^n$ 表示网络在 $n$ 个训练输入上的输出向量。考虑围绕初始化 $\\boldsymbol{\\theta}^0$ 的一阶泰勒展开：\n$$\n\\mathbf{f}(\\boldsymbol{\\theta}) \\;\\approx\\; \\mathbf{f}(\\boldsymbol{\\theta}^0) \\;+\\; J_0 \\, (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0),\n$$\n其中 $J_0 \\in \\mathbb{R}^{n \\times p}$ 是训练输出相对于参数在初始化时的雅可比矩阵，$p$ 是参数数量。使用相同的学习率 $\\eta$ 和步数 $T$，通过批量梯度下降训练这个线性化模型，并在训练集上使用相同的平均平方损失。对于测试输入，使用相应的初始化雅可比矩阵来获得预测\n$$\n\\mathbf{f}_{\\text{test}}(\\boldsymbol{\\theta}) \\;\\approx\\; \\mathbf{f}_{\\text{test}}(\\boldsymbol{\\theta}^0) \\;+\\; J^{\\text{test}}_0 \\, (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0).\n$$\n\n数据生成：\n- 输入维度为 $d = 2$。使用随机种子 $123$，通过从标准正态分布中独立抽取每个分量，生成 $n = 12$ 个训练输入 $\\{\\mathbf{x}_i\\}_{i=1}^{n}$。使用随机种子 $456$，按相同规则生成 $m = 12$ 个测试输入。所有用于三角函数的随机角度都应理解为弧度。\n- 定义固定向量 $\\mathbf{u} = (1.0, -0.5)$ 和 $\\mathbf{v} = (0.3, 1.1)$。通过以下方式定义目标函数\n$$\ny(\\mathbf{x}) \\;=\\; \\sin(\\langle \\mathbf{u}, \\mathbf{x} \\rangle) \\;+\\; 0.5 \\, \\cos(\\langle \\mathbf{v}, \\mathbf{x} \\rangle).\n$$\n为训练集计算 $y_i = y(\\mathbf{x}_i)$，为测试集计算 $y^{\\text{test}}_j = y(\\mathbf{z}_j)$，其中 $\\{\\mathbf{z}_j\\}_{j=1}^{m}$ 是测试输入。\n\n偏差度量：\n- 在为相同的 $(w,\\eta,T)$ 训练了全网络和线性化模型后，计算它们在 $n$ 个训练输入和 $m$ 个测试输入并集上的预测。令 $\\widehat{f}_{\\text{full}}$ 和 $\\widehat{f}_{\\text{lin}}$ 表示这些预测。通过均方偏差来量化偏差\n$$\nD \\;=\\; \\frac{1}{n+m} \\sum_{i=1}^{n+m} \\Big(\\widehat{f}_{\\text{full},i} - \\widehat{f}_{\\text{lin},i}\\Big)^2.\n$$\n\n测试套件：\n为以下参数集 $(w,\\eta,T)$ 运行实验：\n- 案例 $1$：$(w,\\eta,T) = (10, 0.1, 200)$。\n- 案例 $2$：$(w,\\eta,T) = (200, 0.1, 200)$。\n- 案例 $3$：$(w,\\eta,T) = (10, 0.5, 200)$。\n- 案例 $4$（边界条件）：$(w,\\eta,T) = (10, 0.1, 0)$。\n\n要求的程序行为：\n- 你的程序必须完全按照规定实现这两种训练过程，对全网络和线性化模型使用相同的初始化 $(W^0,\\mathbf{a}^0)$，并在每个案例中使用相同的学习率 $\\eta$ 和步数 $T$。\n- 计算每个案例的偏差 $D$。\n- 最终输出格式：打印单行，包含一个Python风格的列表，其中包含按上述案例顺序排列的四个偏差，四舍五入到 $6$ 位小数，例如\n\"[0.123456,0.000789,0.234567,0.000000]\"。", "solution": "用户的请求是深度学习理论领域中一个有效且定义明确的问题。它要求对一个双层ReLU网络的训练动态及其一阶泰勒近似（线性化或神经正切核模型）进行数值比较。该问题具有科学依据，形式上规范，并且计算上是可行的。我们将提供一个完整的解决方案。\n\n问题的核心是为一个神经网络在回归任务上实现两种不同的训练过程，并量化它们最终预测的差异。\n\n**1. 模型与设置**\n\n该网络是一个双层全连接ReLU网络，宽度为 $w$，输出为标量，对于输入 $\\mathbf{x} \\in \\mathbb{R}^d$ 定义为：\n$$\nf(\\mathbf{x}; W, \\mathbf{a}) = \\frac{1}{\\sqrt{w}} \\sum_{j=1}^{w} a_j \\, \\max\\{0, \\langle \\mathbf{w}_j, \\mathbf{x} \\rangle\\}\n$$\n其中 $W \\in \\mathbb{R}^{w \\times d}$ 和 $\\mathbf{a} \\in \\mathbb{R}^w$ 是参数。训练目标是在训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 上最小化平均平方损失：\n$$\n\\mathcal{L}(W, \\mathbf{a}) = \\frac{1}{2n} \\sum_{i=1}^{n} \\big(f(\\mathbf{x}_i; W, \\mathbf{a}) - y_i \\big)^2\n$$\n\n**2. 全网络梯度下降训练**\n\n第一种训练动态是在参数 $(W, \\mathbf{a})$ 上进行标准的批量梯度下降。参数通过学习率 $\\eta$ 迭代更新 $T$ 步：\n$$\n(W^{t+1}, \\mathbf{a}^{t+1}) = (W^{t}, \\mathbf{a}^{t}) - \\eta \\, \\nabla \\mathcal{L}(W^{t}, \\mathbf{a}^{t})\n$$\n梯度通过链式法则计算。设 $r_i = f(\\mathbf{x}_i; W, \\mathbf{a}) - y_i$ 为样本 $i$ 的残差，设 $\\sigma(z)=\\max\\{0,z\\}$ 为ReLU激活函数，其导数（次梯度）取为 $\\sigma'(z) = \\mathbb{1}_{z > 0}$。损失对参数的梯度为：\n\n- **关于输出层权重 $\\mathbf{a}$ 的梯度**：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial a_j} = \\frac{1}{n} \\sum_{i=1}^{n} r_i \\frac{\\partial f(\\mathbf{x}_i)}{\\partial a_j} = \\frac{1}{n \\sqrt{w}} \\sum_{i=1}^{n} r_i \\, \\sigma(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)\n$$\n以矩阵形式，如果 $\\mathbf{f} \\in \\mathbb{R}^n$ 是预测向量，$\\mathbf{r} = \\mathbf{f} - \\mathbf{y}$ 是残差向量，且 $A \\in \\mathbb{R}^{n \\times w}$ 是激活后值的矩阵，其中 $A_{ij} = \\sigma(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)$，则 $\\nabla_{\\mathbf{a}}\\mathcal{L} = \\frac{1}{n \\sqrt{w}} A^T \\mathbf{r}$。\n\n- **关于输入层权重 $W$ 的梯度**：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W_{jk}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i \\frac{\\partial f(\\mathbf{x}_i)}{\\partial W_{jk}} = \\frac{1}{n \\sqrt{w}} \\sum_{i=1}^{n} r_i \\, a_j \\, \\sigma'(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle) x_{ik}\n$$\n以矩阵形式，令 $S \\in \\mathbb{R}^{n \\times w}$ 为矩阵，其中 $S_{ij} = \\sigma'(\\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle)$，梯度矩阵 $\\nabla_W \\mathcal{L} \\in \\mathbb{R}^{w \\times d}$ 可以通过首先计算损失对激活前值 $H_{ij} = \\langle \\mathbf{w}_j, \\mathbf{x}_i \\rangle$ 的梯度来得到。这个中间梯度矩阵 $G_H \\in \\mathbb{R}^{n \\times w}$ 的元素为 $(G_H)_{ij} = \\frac{\\partial \\mathcal{L}}{\\partial H_{ij}} = \\frac{1}{n} r_i \\frac{a_j}{\\sqrt{w}} S_{ij}$。最终的梯度为 $\\nabla_W \\mathcal{L} = G_H^T X$，其中 $X \\in \\mathbb{R}^{n \\times d}$ 是训练数据矩阵。\n\n这些更新执行 $T$ 步以获得最终参数 $(W^T, \\mathbf{a}^T)$，用于在训练数据和测试数据的并集上进行预测 $\\widehat{f}_{\\text{full}}$。\n\n**3. 线性化 (NTK) 模型训练**\n\n第二种动态训练模型的线性化版本。网络输出函数 $\\mathbf{f}(\\boldsymbol{\\theta})$（其中 $\\boldsymbol{\\theta}$ 代表所有参数）由其围绕初始参数 $\\boldsymbol{\\theta}^0 = (W^0, \\mathbf{a}^0)$ 的一阶泰勒级数近似：\n$$\n\\mathbf{f}_{\\text{lin}}(\\boldsymbol{\\theta}) = \\mathbf{f}(\\boldsymbol{\\theta}^0) + J_0 (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^0)\n$$\n其中 $J_0 = \\nabla_{\\boldsymbol{\\theta}} \\mathbf{f}(\\boldsymbol{\\theta}^0)$ 是网络输出相对于参数在初始化时评估的雅可比矩阵。用梯度下降训练此模型会导致预测本身有一个简化的动态。训练集预测 $\\mathbf{f}_{\\text{lin,train}}^t$ 的更新为：\n$$\n\\mathbf{f}_{\\text{lin,train}}^{t+1} = \\mathbf{f}_{\\text{lin,train}}^{t} - \\frac{\\eta}{n} K_{\\text{train,train}} (\\mathbf{f}_{\\text{lin,train}}^t - \\mathbf{y}_{\\text{train}})\n$$\n其中 $K_{\\text{train,train}} = J_0^{\\text{train}} (J_0^{\\text{train}})^T$ 是神经正切核 (NTK) 矩阵。任何其他数据集（例如测试集）上的预测协同演化如下：\n$$\n\\mathbf{f}_{\\text{lin,test}}^{t+1} = \\mathbf{f}_{\\text{lin,test}}^{t} - \\frac{\\eta}{n} K_{\\text{test,train}} (\\mathbf{f}_{\\text{lin,train}}^t - \\mathbf{y}_{\\text{train}})\n$$\n其中 $(K_{\\text{test,train}})_{ij} = \\langle \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{z}_i; \\boldsymbol{\\theta}^0), \\nabla_{\\boldsymbol{\\theta}}f(\\mathbf{x}_j; \\boldsymbol{\\theta}^0) \\rangle$。\n\n我们可以直接计算任意两个输入 $\\mathbf{x}, \\mathbf{x}'$ 的核函数 $K(\\mathbf{x}, \\mathbf{x}')$，而不是构建可能非常大的雅可比矩阵：\n$$\nK(\\mathbf{x}, \\mathbf{x}') = \\underbrace{\\frac{1}{w} \\sum_{k=1}^{w} \\sigma(\\langle\\mathbf{w}_k^0,\\mathbf{x}\\rangle)\\sigma(\\langle\\mathbf{w}_k^0,\\mathbf{x}'\\rangle)}_{\\text{from } \\nabla_{\\mathbf{a}}} + \\underbrace{\\frac{\\langle\\mathbf{x},\\mathbf{x}'\\rangle}{w} \\sum_{k=1}^{w} (a_k^0)^2 \\mathbb{1}_{\\langle\\mathbf{w}_k^0,\\mathbf{x}\\rangle > 0} \\mathbb{1}_{\\langle\\mathbf{w}_k^0,\\mathbf{x}'\\rangle > 0}}_{\\text{from } \\nabla_W}\n$$\n这允许通过只跟踪预测向量来高效地模拟线性化模型的动态。\n\n**4. 偏差与实现**\n\n经过 $T$ 步后，我们获得了两个模型在 $n+m$ 个训练和测试输入的组合集上的预测，记为 $\\widehat{\\mathbf{f}}_{\\text{full}}$ 和 $\\widehat{\\mathbf{f}}_{\\text{lin}}$。偏差通过均方偏差来量化：\n$$\nD = \\frac{1}{n+m} \\sum_{i=1}^{n+m} \\left( \\widehat{f}_{\\text{full},i} - \\widehat{f}_{\\text{lin},i} \\right)^2\n$$\n对于每个测试案例 $(w, \\eta, T)$，实现将遵循以下步骤：\n1. 根据指定的分布和种子生成训练和测试数据。\n2. 使用指定的种子初始化网络参数 $(W^0, \\mathbf{a}^0)$。\n3. 对于全网络，运行梯度下降循环 $T$ 步并计算最终预测。\n4. 对于线性化模型，使用 $(W^0, \\mathbf{a}^0)$ 计算必要的核矩阵，并模拟预测演化 $T$ 步。\n5. 计算并存储偏差 $D$。\n\n$T=0$ 的情况作为一个健全性检查，此时两个模型都保持在初始状态，因此偏差必须为 $0$。其他情况探讨了网络宽度 $w$ 和学习率 $\\eta$ 如何影响 NTK 近似的有效性。我们预期对于更大的宽度（案例2 vs. 案例1），偏差会更小；对于更高的学习率（案例3 vs. 案例1），偏差会更大。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares full network training with linearized (NTK) dynamics \n    for a two-layer ReLU network.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (w, eta, T)\n        (10, 0.1, 200),\n        (200, 0.1, 200),\n        (10, 0.5, 200),\n        (10, 0.1, 0),\n    ]\n\n    results = []\n\n    # --- Data Generation ---\n    d = 2\n    n = 12\n    m = 12\n\n    # Generate training data\n    rng_train = np.random.default_rng(123)\n    X_train = rng_train.standard_normal((n, d), dtype=np.float64)\n\n    # Generate test data\n    rng_test = np.random.default_rng(456)\n    X_test = rng_test.standard_normal((m, d), dtype=np.float64)\n\n    # Define target function and compute labels\n    u = np.array([1.0, -0.5], dtype=np.float64)\n    v = np.array([0.3, 1.1], dtype=np.float64)\n\n    def target_function(x_data):\n        return np.sin(x_data @ u) + 0.5 * np.cos(x_data @ v)\n\n    y_train = target_function(X_train)\n    \n    # Combine training and test inputs for final predictions\n    X_all = np.vstack([X_train, X_test])\n\n    for w, eta, T in test_cases:\n        \n        # --- Initialization ---\n        rng_init = np.random.default_rng(2024)\n        W0 = rng_init.standard_normal((w, d), dtype=np.float64)\n        a0 = rng_init.standard_normal(w, dtype=np.float64)\n\n        # --- Full Network Training ---\n        W, a = W0.copy(), a0.copy()\n        for _ in range(T):\n            # Forward pass on training data\n            H_train = X_train @ W.T  # Shape: (n, w)\n            A_train = np.maximum(0, H_train)\n            f_pred = (1 / np.sqrt(w)) * (A_train @ a)\n            residuals = f_pred - y_train\n\n            # Gradient w.r.t. a\n            grad_a = (1 / (n * np.sqrt(w))) * (A_train.T @ residuals)\n\n            # Gradient w.r.t. W\n            S_train = (H_train > 0).astype(np.float64)\n            d_loss_d_h = (1 / (n * np.sqrt(w))) * (residuals.reshape(-1, 1) * S_train) * a.reshape(1, -1)\n            grad_W = d_loss_d_h.T @ X_train\n\n            # Update parameters\n            W -= eta * grad_W\n            a -= eta * grad_a\n\n        # Final predictions from the fully trained network\n        def predict_network(x_data, W_final, a_final):\n            H_final = x_data @ W_final.T\n            A_final = np.maximum(0, H_final)\n            return (1 / np.sqrt(w)) * (A_final @ a_final)\n\n        f_full_final = predict_network(X_all, W, a)\n\n        # --- Linearized Model (NTK) Training ---\n\n        # Kernel computation function\n        def compute_kernel(x1, x2, width, W_init, a_init):\n            # Part 1: Contribution from derivatives w.r.t. 'a'\n            act1 = np.maximum(0, x1 @ W_init.T)\n            act2 = np.maximum(0, x2 @ W_init.T)\n            K1 = (1 / width) * (act1 @ act2.T)\n\n            # Part 2: Contribution from derivatives w.r.t. 'W'\n            dot_prod_xx = x1 @ x2.T\n            S1 = (x1 @ W_init.T > 0).astype(np.float64)\n            S2 = (x2 @ W_init.T > 0).astype(np.float64)\n            a_init_sq = a_init**2\n            K2_scalar_part = (S1 * a_init_sq) @ S2.T\n            K2 = (1 / width) * dot_prod_xx * K2_scalar_part\n\n            return K1 + K2\n\n        K_train_train = compute_kernel(X_train, X_train, w, W0, a0)\n        K_all_train = compute_kernel(X_all, X_train, w, W0, a0)\n\n        # Initial predictions for the linearized model\n        f0_all = predict_network(X_all, W0, a0)\n        \n        # Evolve all predictions using the linearized dynamics\n        f_lin_all = f0_all.copy()\n        \n        # Track training predictions to compute residuals\n        f_lin_train = f0_all[:n].copy()\n        \n        for _ in range(T):\n            r_lin_train = f_lin_train - y_train\n            # Update predictions on all data points\n            f_lin_all -= (eta / n) * (K_all_train @ r_lin_train)\n            # Update local copy of training predictions for the next iteration\n            f_lin_train = f_lin_all[:n]\n\n        f_lin_final = f_lin_all\n\n        # --- Deviation Calculation ---\n        deviation = np.mean((f_full_final - f_lin_final)**2)\n        results.append(deviation)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3159054"}]}