{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本章节将通过三个精心设计的动手练习，带你从实现、评估到应用可解释性方法，全面提升你的实战能力。我们将从一个基本但功能强大的归因方法——积分梯度（Integrated Gradients, IG）——开始。IG通过在从基线到输入的路径上对梯度进行积分来计算特征贡献，但其核心的积分定义在实践中必须通过数值方法来近似。这个练习 ([@problem_id:3153194]) 将指导你从零开始实现IG，比较不同数值积分方案的优劣，并深入理解在将理论应用于实践时，准确性与计算成本之间的权衡。", "problem": "构建一个程序，用于比较两种计算沿直线路径的积分梯度 (IG) 的数值求积方案，并随着求积步数的增加评估其准确性和稳定性。请在以下纯数学设定下完成。\n\n考虑一个定义在维度为 $d$ 的向量空间上的可微标量值函数 $f(\\mathbf{x})$，其定义如下：\n$$\nf(\\mathbf{x}) = \\log\\left(1 + e^{\\mathbf{w}^{\\top}\\mathbf{x}}\\right),\n$$\n其中 $\\mathbf{x} \\in \\mathbb{R}^d$ 且 $\\mathbf{w} \\in \\mathbb{R}^d$。对于一个基线 $\\mathbf{x}_0$ 和输入 $\\mathbf{x}$，令 $\\Delta = \\mathbf{x} - \\mathbf{x}_0$。沿直线路径 $\\mathbf{x}(\\alpha) = \\mathbf{x}_0 + \\alpha \\Delta$（其中 $\\alpha \\in [0,1]$）的积分梯度 (IG) 归因向量定义为：\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = \\int_{0}^{1} \\nabla f\\big(\\mathbf{x}_0 + \\alpha \\Delta\\big) \\odot \\Delta \\ d\\alpha,\n$$\n其中 $\\odot$ 表示逐元素乘法。在整个过程中，请使用微积分中的梯度定义和标准微分法则。\n\n您必须：\n- 根据定义和基本微积分，推导出此函数 $f(\\mathbf{x})$ 的精确 IG 的闭式表达式，并用 $\\mathbf{w}$、$\\mathbf{x}_0$ 和 $\\mathbf{x}$ 表示。将此精确结果用作误差评估的基准真相。\n- 实现上述路径积分的两种数值求积近似方法：\n  1. 在 $[0,1]$ 上使用 $m$ 个子区间的梯形法则：\n     - 使用节点 $\\alpha_k = k/m$（其中 $k = 0, 1, \\dots, m$），步长为 $h = 1/m$，权重为 $w_0 = w_m = h/2$，$w_k = h$（其中 $k = 1, \\dots, m-1$）。\n  2. 在 $[0,1]$ 上的 $m$ 点 Gauss–Legendre 求积法：\n     - 使用在 $[-1,1]$ 上的 $m$ 次 Gauss–Legendre 节点 $\\{\\tilde{\\alpha}_k\\}_{k=1}^m$ 和权重 $\\{\\tilde{w}_k\\}_{k=1}^m$，通过 $\\alpha_k = (\\tilde{\\alpha}_k + 1)/2$ 仿射映射到 $[0,1]$，相应权重为 $w_k = \\tilde{w}_k/2$。\n- 对于每种求积方法，通过在各自的节点上评估向量值被积函数 $\\nabla f(\\mathbf{x}_0 + \\alpha \\Delta) \\odot \\Delta$ 并使用指定的权重求和，来近似该向量积分。\n\n使用以下测试套件，其中所有向量均已明确给出：\n- 测试用例 1（正常路径，中度非线性）：\n  - $d = 3$\n  - $\\mathbf{w} = [0.5, -1.0, 2.0]$\n  - $\\mathbf{x}_0 = [0.0, 0.0, 0.0]$\n  - $\\mathbf{x} = [1.0, -2.0, 0.5]$\n- 测试用例 2（边界情况，logit 中方向变化为零，即正交性导致被积函数为常数）：\n  - $d = 2$\n  - $\\mathbf{w} = [1.0, 2.0]$\n  - $\\mathbf{x}_0 = [1.0, -1.0]$\n  - $\\mathbf{x} = [3.0, -2.0]$\n- 测试用例 3（强饱和区域）：\n  - $d = 3$\n  - $\\mathbf{w} = [3.0, -0.5, 1.0]$\n  - $\\mathbf{x}_0 = [2.0, -4.0, 3.0]$\n  - $\\mathbf{x} = [3.0, -4.0, 4.0]$\n\n对于每个测试用例，针对步数集合\n$$\n\\mathcal{M} = [1, 2, 4, 8, 16, 32, 64].\n$$\n评估近似值。\n设准确度容差为 $\\varepsilon = 10^{-8}$。\n\n对于每个测试用例和每种求积方法，计算：\n- 在 $m \\in \\mathcal{M}$ 上，近似 IG 与精确 IG 之间的最大 $\\ell_2$ 误差。\n- 使 $\\ell_2$ 误差小于或等于 $\\varepsilon$ 的最小 $m \\in \\mathcal{M}$；如果不存在这样的 $m$，则报告 $-1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的扁平、逗号分隔的列表，顺序如下：\n  - 对于测试用例 1：[梯形法则最大误差, Gauss–Legendre 最大误差, 梯形法则最小 $m$, Gauss–Legendre 最小 $m$]\n  - 对于测试用例 2：追加相同的四个量\n  - 对于测试用例 3：追加相同的四个量\n- 具体而言，输出必须是以下形式的单行：\n$$\n[\\mathrm{E}^{(1)}_{\\mathrm{trap}}, \\mathrm{E}^{(1)}_{\\mathrm{gl}}, m^{(1)}_{\\mathrm{trap}}, m^{(1)}_{\\mathrm{gl}}, \\mathrm{E}^{(2)}_{\\mathrm{trap}}, \\mathrm{E}^{(2)}_{\\mathrm{gl}}, m^{(2)}_{\\mathrm{trap}}, m^{(2)}_{\\mathrm{gl}}, \\mathrm{E}^{(3)}_{\\mathrm{trap}}, \\mathrm{E}^{(3)}_{\\mathrm{gl}}, m^{(3)}_{\\mathrm{trap}}, m^{(3)}_{\\mathrm{gl}}].\n$$\n\n所有量均为无量纲，并且必须以基本数值类型（误差为浮点数，$m$ 为整数）报告，不带任何附加文本。", "solution": "用户希望构建一个程序，用于计算特定函数的积分梯度 (IG)，并比较两种数值求积方案的准确性。\n\n解决过程包括三个主要阶段：\n1.  推导积分梯度向量的闭式解析表达式，该表达式将作为基准真相。\n2.  实现两种数值求积方法（梯形法则和 Gauss-Legendre 求积法）来近似 IG 路径积分。\n3.  针对一组给定的测试用例和参数，评估这些数值方法相对于精确解的准确性和收敛性。\n\n### 1. 积分梯度的解析推导\n\n函数为 $f(\\mathbf{x}) = \\log\\left(1 + e^{\\mathbf{w}^{\\top}\\mathbf{x}}\\right)$。这是应用于 $\\mathbf{x}$ 的线性投影的 softplus 函数。令 $u(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x}$。则函数为 $f(\\mathbf{x}) = \\log(1+e^{u(\\mathbf{x})})$。\n\n首先，我们使用链式法则计算 $f(\\mathbf{x})$ 的梯度。外部函数 $\\log(1+e^u)$ 对其参数 $u$ 的导数是 $\\frac{e^u}{1+e^u} = \\frac{1}{1+e^{-u}}$，即逻辑 sigmoid 函数，记为 $\\sigma(u)$。内部函数 $u(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} = \\sum_j w_j x_j$ 对 $\\mathbf{x}$ 的梯度就是向量 $\\mathbf{w}$。\n\n因此，梯度 $\\nabla f(\\mathbf{x})$ 为：\n$$\n\\nabla f(\\mathbf{x}) = \\frac{d f}{d u} \\nabla u(\\mathbf{x}) = \\sigma(\\mathbf{w}^{\\top}\\mathbf{x}) \\mathbf{w}\n$$\n\n积分梯度归因由沿直线 $\\mathbf{x}(\\alpha) = \\mathbf{x}_0 + \\alpha \\Delta$ 的路径积分定义，其中 $\\Delta = \\mathbf{x} - \\mathbf{x}_0$ 且 $\\alpha \\in [0,1]$。\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = \\int_{0}^{1} \\nabla f\\big(\\mathbf{x}_0 + \\alpha \\Delta\\big) \\odot \\Delta \\ d\\alpha\n$$\n代入梯度的表达式：\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = \\int_{0}^{1} \\left( \\sigma\\left(\\mathbf{w}^{\\top}(\\mathbf{x}_0 + \\alpha \\Delta)\\right) \\mathbf{w} \\right) \\odot \\Delta \\ d\\alpha\n$$\n项 $\\mathbf{w} \\odot \\Delta$ 是一个相对于积分变量 $\\alpha$ 的常数向量。我们可以将其从积分中提出（这适用于分量级别）：\n$$\n\\mathrm{IG}(\\mathbf{x}; \\mathbf{x}_0) = (\\mathbf{w} \\odot \\Delta) \\int_{0}^{1} \\sigma\\left(\\mathbf{w}^{\\top}\\mathbf{x}_0 + \\alpha (\\mathbf{w}^{\\top}\\Delta)\\right) d\\alpha\n$$\n我们定义标量常数 $C_0 = \\mathbf{w}^{\\top}\\mathbf{x}_0$ 和 $C_1 = \\mathbf{w}^{\\top}\\Delta$。积分变为一个标量积分 $I$：\n$$\nI = \\int_{0}^{1} \\sigma(C_0 + \\alpha C_1) \\ d\\alpha\n$$\n为求解此积分，我们回想一下 $\\sigma(z)$ 的不定积分是 $\\log(1+e^z)$，这是我们原始函数 $f$ 的一个原函数。令 $v(\\alpha) = C_0 + \\alpha C_1$，则 $dv = C_1 d\\alpha$。\n\n如果 $C_1 \\neq 0$：我们进行变量替换。\n$$\nI = \\frac{1}{C_1} \\int_{v(0)}^{v(1)} \\sigma(v) \\ dv = \\frac{1}{C_1} \\left[ \\log(1+e^v) \\right]_{C_0}^{C_0+C_1} = \\frac{1}{C_1} \\left( \\log(1+e^{C_0+C_1}) - \\log(1+e^{C_0}) \\right)\n$$\n将 $C_0 = \\mathbf{w}^{\\top}\\mathbf{x}_0$ 和 $C_0+C_1 = \\mathbf{w}^{\\top}(\\mathbf{x}_0+\\Delta) = \\mathbf{w}^{\\top}\\mathbf{x}$ 代回，我们得到：\n$$\nI = \\frac{\\log(1+e^{\\mathbf{w}^{\\top}\\mathbf{x}}) - \\log(1+e^{\\mathbf{w}^{\\top}\\mathbf{x}_0})}{\\mathbf{w}^{\\top}(\\mathbf{x} - \\mathbf{x}_0)} = \\frac{f(\\mathbf{x}) - f(\\mathbf{x}_0)}{\\mathbf{w}^{\\top}\\Delta}\n$$\n所以，精确的 IG 向量是：\n$$\n\\mathrm{IG}_{\\text{exact}} = (\\mathbf{w} \\odot \\Delta) \\frac{f(\\mathbf{x}) - f(\\mathbf{x}_0)}{\\mathbf{w}^{\\top}\\Delta}\n$$\n\n如果 $C_1 = 0$：分母为零。在这种情况下，被积函数 $\\sigma(C_0 + \\alpha C_1)$ 简化为一个常数 $\\sigma(C_0)$。积分变为：\n$$\nI = \\int_{0}^{1} \\sigma(C_0) \\ d\\alpha = \\sigma(C_0) [\\alpha]_0^1 = \\sigma(C_0)\n$$\n这种特殊情况下的精确 IG 向量是：\n$$\n\\mathrm{IG}_{\\text{exact}} = (\\mathbf{w} \\odot \\Delta) \\sigma(\\mathbf{w}^{\\top}\\mathbf{x}_0)\n$$\n这种情况对应于积分路径与权重向量 $\\mathbf{w}$ 正交，导致沿路径的“预激活”值恒定。\n\n### 2. 数值求积方案\n\n待近似的积分形式为 $\\int_0^1 \\mathbf{g}(\\alpha) d\\alpha \\approx \\sum_k w_k \\mathbf{g}(\\alpha_k)$，其中 $\\mathbf{g}(\\alpha) = (\\mathbf{w} \\odot \\Delta) \\sigma(C_0 + \\alpha C_1)$。该近似可以计算为 $(\\mathbf{w} \\odot \\Delta) \\left( \\sum_k w_k \\sigma(C_0 + \\alpha_k C_1) \\right)$。\n\n1.  **梯形法则**：对于 $m$ 个子区间，步长为 $h=1/m$。节点为 $\\alpha_k = k/m$（其中 $k = 0, 1, \\dots, m$）。内点（$k=1, \\dots, m-1$）的权重为 $w_k = h$，端点（$k=0, m$）的权重为 $w_k = h/2$。近似值为：\n    $$\n    \\mathrm{IG}_{\\text{trap}}(m) = (\\mathbf{w} \\odot \\Delta) \\sum_{k=0}^{m} w_k \\sigma(C_0 + \\alpha_k C_1)\n    $$\n\n2.  **Gauss–Legendre 求积法**：对于一个 $m$ 点法则，我们使用标准区间 $[-1,1]$ 上的 $m$ 次 Gauss-Legendre 多项式的节点 $\\tilde{\\alpha}_k$ 和权重 $\\tilde{w}_k$。这些必须被仿射变换到区间 $[0,1]$。\n    - $[0,1]$ 上的节点：$\\alpha_k = (\\tilde{\\alpha}_k + 1)/2$\n    - $[0,1]$ 的权重：$w_k = \\tilde{w}_k/2$\n    近似值是这些 $m$ 个节点上的加权和：\n    $$\n    \\mathrm{IG}_{\\text{gl}}(m) = (\\mathbf{w} \\odot \\Delta) \\sum_{k=1}^{m} w_k \\sigma(C_0 + \\alpha_k C_1)\n    $$\n\n### 3. 评估程序\n\n对于每个测试用例以及 $\\mathcal{M} = [1, 2, 4, 8, 16, 32, 64]$ 中的每个 $m$ 值，我们使用两种方法 $\\mathrm{IG}_{\\text{trap}}(m)$ 和 $\\mathrm{IG}_{\\text{gl}}(m)$ 计算近似 IG 向量。然后，我们计算误差向量的 $\\ell_2$ 范数，即近似解与精确解 $\\mathrm{IG}_{\\text{exact}}$ 之间的差。\n$$\ne(m) = \\| \\mathrm{IG}_{\\text{approx}}(m) - \\mathrm{IG}_{\\text{exact}} \\|_2\n$$\n对于每个测试用例和求积方法，我们确定两个量：\n1.  在所有 $m \\in \\mathcal{M}$ 值中观察到的最大误差：$E = \\max_{m \\in \\mathcal{M}} e(m)$。\n2.  使误差 $e(m)$ 小于或等于容差 $\\varepsilon = 10^{-8}$ 的最小 $m \\in \\mathcal{M}$。如果找不到这样的 $m$，则报告 -1。\n\n此程序被系统地应用于所有三个测试用例，以生成最终的 12 元素输出向量。", "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_legendre\n\ndef solve():\n    \"\"\"\n    Computes and compares Integrated Gradients using analytical, \n    Trapezoidal, and Gauss-Legendre methods.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"id\": 1,\n            \"d\": 3,\n            \"w\": np.array([0.5, -1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0, 0.0]),\n            \"x\": np.array([1.0, -2.0, 0.5]),\n        },\n        {\n            \"id\": 2,\n            \"d\": 2,\n            \"w\": np.array([1.0, 2.0]),\n            \"x0\": np.array([1.0, -1.0]),\n            \"x\": np.array([3.0, -2.0]),\n        },\n        {\n            \"id\": 3,\n            \"d\": 3,\n            \"w\": np.array([3.0, -0.5, 1.0]),\n            \"x0\": np.array([2.0, -4.0, 3.0]),\n            \"x\": np.array([3.0, -4.0, 4.0]),\n        },\n    ]\n\n    M_values = [1, 2, 4, 8, 16, 32, 64]\n    epsilon = 1e-8\n    \n    all_results = []\n    \n    def log_1_plus_exp(z):\n        \"\"\"Numerically stable implementation of log(1 + exp(z)).\"\"\"\n        return np.logaddexp(0, z)\n\n    def sigmoid(z):\n        \"\"\"Numerically stable sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    for case in test_cases:\n        w, x0, x = case[\"w\"], case[\"x0\"], case[\"x\"]\n        delta = x - x0\n        \n        # --- 1. Compute Exact Integrated Gradients ---\n        c0 = np.dot(w, x0)\n        c1 = np.dot(w, delta)\n        v_const = w * delta\n        \n        # If c1 is very close to zero, the integrand is constant.\n        if np.abs(c1)  1e-12:\n            scalar_integral_exact = sigmoid(c0)\n        else:\n            fx = log_1_plus_exp(np.dot(w, x))\n            fx0 = log_1_plus_exp(c0)\n            scalar_integral_exact = (fx - fx0) / c1\n            \n        ig_exact = v_const * scalar_integral_exact\n\n        # --- 2. Compute Approximations and Errors ---\n        trap_errors = []\n        gl_errors = []\n        \n        for m in M_values:\n            # --- Trapezoidal Rule ---\n            h = 1.0 / m\n            nodes_trap = np.linspace(0.0, 1.0, m + 1)\n            weights_trap = np.full(m + 1, h)\n            weights_trap[0] = h / 2.0\n            weights_trap[-1] = h / 2.0\n            \n            integrand_vals_trap = sigmoid(c0 + nodes_trap * c1)\n            scalar_integral_trap = np.dot(weights_trap, integrand_vals_trap)\n            ig_trap = v_const * scalar_integral_trap\n            trap_errors.append(np.linalg.norm(ig_trap - ig_exact))\n\n            # --- Gauss-Legendre Quadrature ---\n            nodes_gl_std, weights_gl_std = roots_legendre(m)\n            nodes_gl = 0.5 * (nodes_gl_std + 1)\n            weights_gl = 0.5 * weights_gl_std\n            \n            integrand_vals_gl = sigmoid(c0 + nodes_gl * c1)\n            scalar_integral_gl = np.dot(weights_gl, integrand_vals_gl)\n            ig_gl = v_const * scalar_integral_gl\n            gl_errors.append(np.linalg.norm(ig_gl - ig_exact))\n            \n        # --- 3. Aggregate Results for the Current Case ---\n        max_trap_error = np.max(trap_errors)\n        max_gl_error = np.max(gl_errors)\n        \n        min_m_trap = -1\n        for i, err in enumerate(trap_errors):\n            if err = epsilon:\n                min_m_trap = M_values[i]\n                break\n        \n        min_m_gl = -1\n        for i, err in enumerate(gl_errors):\n            if err = epsilon:\n                min_m_gl = M_values[i]\n                break\n        \n        all_results.extend([max_trap_error, max_gl_error, min_m_trap, min_m_gl])\n\n    # --- 4. Final Output ---\n    # Using .15g to maintain precision for small floats in scientific notation\n    # and provide a clean representation for integers.\n    result_str = \",\".join(f'{r:.15g}' if isinstance(r, (float, np.floating)) else str(r) for r in all_results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3153194"}, {"introduction": "生成一个解释（例如，一张显著性图）只是可解释性工作的第一步，更关键的问题是：这个解释可信吗？一个模型可能因为学习到了虚假的“捷径”而产生误导性的解释。这个练习 ([@problem_id:3153222]) 介绍了一种强大的评估技术——删除和插入曲线（deletion and insertion curves），用于衡量解释的“忠实度”（faithfulness）。通过系统地移除或添加显著性得分最高的特征，并观察模型输出的变化，我们可以量化一个解释与模型真实决策依据的对齐程度，从而诊断出模型是否在依赖非预期的捷径特征。", "problem": "您必须编写一个完整、可运行的程序，该程序构建两个作用于图像的可微二元分类器，并使用删除和插入曲线评估其基于梯度的显著性的忠实度。该程序必须仅依赖于明确定义的数学运算和定义。无需用户输入，且所有计算都必须是确定性的。\n\n推导的基本基础必须使用：基于梯度的显著性定义为输入梯度 $\\nabla_{x} f(x)$、导数的链式法则，以及 logistic sigmoid 和双曲正切函数的标准导数。logistic sigmoid 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，其导数为 $\\sigma'(z) = \\sigma(z)\\,(1-\\sigma(z))$。双曲正切函数为 $\\tanh(u) = \\frac{e^{u} - e^{-u}}{e^{u} + e^{-u}}$，其导数为 $\\frac{d}{du}\\tanh(u) = \\operatorname{sech}^{2}(u)$，其中 $\\operatorname{sech}(u) = \\frac{1}{\\cosh(u)}$。\n\n任务规范：\n\n- 输入表示：\n  - 考虑灰度图像 $x \\in \\mathbb{R}^{H \\times W}$，其中 $H = W = 16$。\n  - 定义一个信号区域 $S = \\{(i,j) : 0 \\le i  4, 0 \\le j  4\\}$ 和一个捷径区域 $C = \\{(i,j) : 0 \\le i  4, 12 \\le j  16\\}$。令 $|S| = |C| = 16$。\n  - 对于图像 $x$，将 $\\mathrm{mean}_{S}(x)$ 定义为 $S$ 中像素的算术平均值，将 $\\mathrm{mean}_{C}(x)$ 定义为 $C$ 中像素的算术平均值。\n\n- 模型：\n  - 模型 A (线性-logistic)：\n    - Logit：$z_{\\mathrm{A}}(x) = a_{S}\\,\\mathrm{mean}_{S}(x) + a_{C}\\,\\mathrm{mean}_{C}(x)$，其中 $a_{S} = 4.0$ 且 $a_{C} = 0.2$。\n    - 概率：$f_{\\mathrm{A}}(x) = \\sigma(z_{\\mathrm{A}}(x))$。\n  - 模型 B (饱和-捷径)：\n    - Logit：$z_{\\mathrm{B}}(x) = b_{S}\\,\\mathrm{mean}_{S}(x) + b_{C}\\,\\tanh(\\alpha\\,\\mathrm{mean}_{C}(x))$，其中 $b_{S} = 1.5$，$b_{C} = 3.0$ 且 $\\alpha = 5.0$。\n    - 概率：$f_{\\mathrm{B}}(x) = \\sigma(z_{\\mathrm{B}}(x))$。\n\n- 显著性定义：\n  - 对于每个模型，显著图是根据上述定义通过链式法则计算的梯度幅值绝对值 $s(x) = |\\nabla_{x} f(x)|$。\n\n- 删除和插入曲线：\n  - 令 $N = H \\cdot W$ 为像素总数。\n  - 对于一个固定的归因 $s(x)$，通过按显著性得分降序对像素索引进行排序来定义一个排序 $\\pi$。\n  - 删除曲线：从原始图像 $x^{(0)} = x$ 开始，对于步长 $t \\in \\{0,1,\\dots,T\\}$（其中 $T=20$），通过将根据排序 $\\pi$ 的前 $k_{t} = \\left\\lfloor \\frac{t}{T} \\cdot N \\right\\rfloor$ 个像素值设为零（同时保持其他像素与 $x$ 中相同）来形成 $x^{(t)}$。记录 $y^{(t)} = f(x^{(t)})$，其中 $f$ 是模型的概率。删除曲线下面积（AUC）是 $y^{(t)}$ 相对于分数 $\\frac{t}{T}$ 的梯形积分，通过水平轴上的单位区间和垂直轴上的概率范围 $[0,1]$ 将其归一化至 $[0,1]$。\n  - 插入曲线：从零图像 $\\tilde{x}^{(0)} = 0$ 开始，对于步长 $t \\in \\{0,1,\\dots,T\\}$，通过将根据排序 $\\pi$ 的前 $k_{t}$ 个像素值设为它们在 $x$ 中的原始值（其余像素保持为零）来形成 $\\tilde{x}^{(t)}$。记录 $\\tilde{y}^{(t)} = f(\\tilde{x}^{(t)})$。插入曲线下面积（AUC）通过梯形法则进行类似定义。\n\n- 忠实度得分和决策规则：\n  - 对于每个测试图像上的每个模型，定义一个忠实度得分 $F = \\mathrm{AUC}_{\\mathrm{ins}} - \\mathrm{AUC}_{\\mathrm{del}}$。\n  - 对于每个测试用例，通过比较得分来决定哪个模型的显著性更忠实：更大的 $F$ 值表示更高的忠实度。\n\n数据集构建：\n\n- 使用由 $\\mu_{S} \\in \\{2.0, 0.8, 0.2\\}$ 和一个固定相关因子 $\\rho = 0.8$ 参数化的三个确定性测试图像。\n- 对于每个测试图像：\n  - 将 $S$ 中的所有像素设置为 $\\mu_{S}$。\n  - 将 $C$ 中的所有像素设置为 $\\mu_{C} = \\rho \\cdot \\mu_{S}$。\n  - 将所有其他像素设置为 $0$。\n  - 预期的真实标签为 $y=1$，但程序必须仅评估模型的概率和派生的显著性度量。\n- 注意：使用上述参数，两个模型在这些案例上都达到相同的准确率（每个都预测类别为 $1$），但它们依赖于不同的机制：模型 $\\mathrm{A}$ 强调信号区域 $S$，而模型 $\\mathrm{B}$ 依赖于通过 $C$ 的饱和捷径。由于饱和效应抑制了模型 $\\mathrm{B}$ 在 $C$ 区域的梯度，梯度 $\\nabla_{x} f_{\\mathrm{A}}(x)$ 和 $\\nabla_{x} f_{\\mathrm{B}}(x)$ 可能看起来相似。\n\n程序要求：\n\n- 严格按照规定实现两个模型及其梯度。\n- 对于每个测试图像，使用模型 $\\mathrm{A}$ 和模型 $\\mathrm{B}$ 各自的显著性排名计算其忠实度得分，然后输出哪个模型更忠实。\n- 最终输出格式：您的程序应生成单行输出，其中包含三个测试用例的决策，形式为方括号括起来的逗号分隔整数列表，其中 $0$ 表示模型 $\\mathrm{A}$ 更忠实，$1$ 表示模型 $\\mathrm{B}$ 更忠实。例如，形如 $[0,1,0]$ 的输出是可以接受的。\n\n测试套件：\n\n- 三个测试用例是集合 $\\{2.0, 0.8, 0.2\\}$ 中的三个 $\\mu_{S}$ 值，固定 $\\rho = 0.8$。\n- 预期输出是整数，每个测试用例一个，由上述算法确定。\n- 覆盖范围：\n  - 一个强信号案例 $\\mu_{S} = 2.0$。\n  - 一个中等信号案例 $\\mu_{S} = 0.8$。\n  - 一个近边界案例 $\\mu_{S} = 0.2$。\n\n您的程序必须遵循精确的输出格式：单行输出一个包含三个整数的列表，如 $[r_{1},r_{2},r_{3}]$，无任何额外文本。本问题不使用角度，也无任何物理单位适用。上文描述的所有数值常量必须严格按规定使用。", "solution": "我们从可微模型、梯度和归因忠实度的核心定义开始。\n\n定义和导数。logistic sigmoid 函数为 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，其导数为 $\\sigma'(z) = \\sigma(z)\\,(1-\\sigma(z))$。双曲正切函数为 $\\tanh(u) = \\frac{e^{u}-e^{-u}}{e^{u}+e^{-u}}$，其导数为 $\\frac{d}{du}\\tanh(u) = \\operatorname{sech}^{2}(u)$，其中 $\\operatorname{sech}(u) = \\frac{1}{\\cosh(u)}$。对于复合函数，将使用链式法则来组合这些导数。\n\n图像几何与均值。令 $H=W=16$，并令 $S$ 和 $C$ 为不相交的 $4\\times 4$ 区域，每个区域的基数 $|S|=|C|=16$。对于图像 $x \\in \\mathbb{R}^{H \\times W}$，定义均值 $\\mathrm{mean}_{S}(x) = \\frac{1}{|S|} \\sum_{(i,j)\\in S} x_{ij}$ 和 $\\mathrm{mean}_{C}(x) = \\frac{1}{|C|} \\sum_{(i,j)\\in C} x_{ij}$。\n\n模型。我们定义两个模型，其概率为 $f_{\\mathrm{A}}(x) = \\sigma(z_{\\mathrm{A}}(x))$ 和 $f_{\\mathrm{B}}(x) = \\sigma(z_{\\mathrm{B}}(x))$，其中\n- $z_{\\mathrm{A}}(x) = a_{S}\\,\\mathrm{mean}_{S}(x) + a_{C}\\,\\mathrm{mean}_{C}(x)$，其中 $a_{S}=4.0$ 且 $a_{C}=0.2$，\n- $z_{\\mathrm{B}}(x) = b_{S}\\,\\mathrm{mean}_{S}(x) + b_{C}\\,\\tanh(\\alpha\\,\\mathrm{mean}_{C}(x))$，其中 $b_{S}=1.5$，$b_{C}=3.0$ 且 $\\alpha=5.0$。\n\n以梯度作为显著性。显著图为 $s(x) = |\\nabla_{x} f(x)|$。使用链式法则，$\\nabla_{x} f(x) = \\sigma'(z(x)) \\cdot \\nabla_{x} z(x)$。\n\n对于模型 $\\mathrm{A}$，$z_{\\mathrm{A}}(x)$ 在均值上是线性的。$\\mathrm{mean}_{S}(x)$ 关于 $S$ 中任何像素的梯度是 $\\frac{1}{|S|}$，在 $S$ 之外为零。$C$ 区域也类似。因此，\n- 对于 $(i,j)\\in S$：$\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}} = \\frac{a_{S}}{|S|}$，\n- 对于 $(i,j)\\in C$：$\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}} = \\frac{a_{C}}{|C|}$，\n- 其他位置：$\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}} = 0$。\n因此梯度为 $\\frac{\\partial f_{\\mathrm{A}}}{\\partial x_{ij}} = \\sigma'(z_{\\mathrm{A}}(x)) \\cdot \\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}}$，其中 $\\frac{\\partial z_{\\mathrm{A}}}{\\partial x_{ij}}$ 是每个区域对应的上述分段常数。\n\n对于模型 $\\mathrm{B}$，我们再次应用链式法则，但捷径路径使用了 $\\tanh(\\cdot)$ 函数。令 $m_{C}(x) = \\mathrm{mean}_{C}(x)$。则\n$\\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}} =\n\\begin{cases}\n\\frac{b_{S}}{|S|},  (i,j)\\in S,\\\\\nb_{C}\\cdot \\frac{d}{du}\\tanh(\\alpha u)\\big|_{u=m_{C}(x)} \\cdot \\frac{\\partial m_{C}}{\\partial x_{ij}},  (i,j)\\in C,\\\\\n0,  \\text{otherwise}.\n\\end{cases}$\n我们有 $\\frac{d}{du}\\tanh(\\alpha u) = \\alpha\\,\\operatorname{sech}^{2}(\\alpha u)$ 且对于 $(i,j)\\in C$ 有 $\\frac{\\partial m_{C}}{\\partial x_{ij}} = \\frac{1}{|C|}$。因此，\n- 对于 $(i,j)\\in S$：$\\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}} = \\frac{b_{S}}{|S|}$，\n- 对于 $(i,j)\\in C$：$\\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}} = \\frac{b_{C}\\,\\alpha}{|C|}\\,\\operatorname{sech}^{2}(\\alpha\\,m_{C}(x))$，\n- 其他位置：$0$。\n因此 $\\frac{\\partial f_{\\mathrm{B}}}{\\partial x_{ij}} = \\sigma'(z_{\\mathrm{B}}(x)) \\cdot \\frac{\\partial z_{\\mathrm{B}}}{\\partial x_{ij}}$。\n\n解读。对于 $m_{C}(x) > 0$ 且 $\\alpha$ 很大的正输入，$\\operatorname{sech}^{2}(\\alpha\\,m_{C}(x))$ 会变得非常小，从而使模型 $\\mathrm{B}$ 中的捷径路径饱和。这导致即使 $C$ 区域对 logit 有很大贡献，其梯度却很小。因此，$\\nabla_{x} f_{\\mathrm{B}}(x)$ 通过强调区域 $S$ 而看起来与 $\\nabla_{x} f_{\\mathrm{A}}(x)$ 相似；然而，底层的因果依赖关系不同：模型 $\\mathrm{B}$ 通过一个饱和的非线性关系依赖于 $C$，而模型 $\\mathrm{A}$ 则因为 $a_{S} \\gg a_{C}$ 而主要依赖于 $S$。\n\n删除和插入曲线。为量化忠实度，我们评估当按照显著图 $s(x)$ 建议的顺序扰动特征时，模型输出 $f(x)$ 变化的快慢。对于删除操作，我们逐步将显著性最高的像素置零，并使用梯形法则对 $t=0,\\dots,T$（$T=20$）下概率曲线关于 $\\frac{t}{T}$ 进行积分。一个较小的删除曲线下面积（AUC）表示移除排名最高的特征会迅速降低模型的置信度，这与忠实的归因相符。对于插入操作，我们从零图像开始，逐步插入显著性最高的像素；一个较大的插入曲线下面积（AUC）表示在添加重要特征时模型置信度恢复得更快。一个组合的忠实度得分 $F = \\mathrm{AUC}_{\\mathrm{ins}} - \\mathrm{AUC}_{\\mathrm{del}}$ 总结了这两种效应；更大的 $F$ 值意味着更忠实的显著性。\n\n数据集构建和相同的准确率。我们构建了三个确定性的测试图像，其中 $\\mu_{S} \\in \\{2.0, 0.8, 0.2\\}$ 且 $\\mu_{C} = \\rho\\,\\mu_{S}$，$\\rho=0.8$。$S$ 中的所有像素被设为 $\\mu_{S}$，$C$ 中的所有像素被设为 $\\mu_{C}$，其余像素为 $0$。对于这些图像，两个模型都预测出大于 $0.5$ 的概率（类别 $1$），因此在这个测试套件上具有相同的准确率，但它们的依赖关系不同：模型 $\\mathrm{A}$ 由 $S$ 驱动，而模型 $\\mathrm{B}$ 由 $C$ 的一个饱和函数驱动。\n\n预期的诊断结果。由于模型 $\\mathrm{B}$ 的梯度低估了 $C$ 区域中捷径的贡献（由于饱和效应），其基于梯度的显著性会将 $S$ 中像素的排名排得过高，超出了它们对模型输出的真实因果影响。因此，沿着梯度排名进行删除操作不会那么快地降低 $f_{\\mathrm{B}}(x)$，而插入操作也不会那么有效地恢复它，从而产生一个较小的 $F$ 值。模型 $\\mathrm{A}$ 的梯度与其对 $S$ 的依赖关系一致，将表现出更低的删除 AUC 和更高的插入 AUC，从而得到一个更大的 $F$ 值。因此，对于每个测试用例，我们预期决策会判定模型 $\\mathrm{A}$ 更忠实，并编码为整数 $0$。\n\n程序实现的算法步骤：\n- 根据 $\\mu_{S} \\in \\{2.0, 0.8, 0.2\\}$ 和 $\\rho=0.8$ 构建三个图像。\n- 通过链式法则实现 $f_{\\mathrm{A}}$、$f_{\\mathrm{B}}$ 及其梯度。\n- 对于每个图像和模型：\n  - 计算显著性 $s(x) = |\\nabla_{x} f(x)|$。\n  - 使用显著性排名和零基线，在 $T=20$ 个步骤上构建删除和插入曲线。\n  - 使用梯形法则在 $\\frac{t}{T}$ 上计算 $\\mathrm{AUC}_{\\mathrm{del}}$ 和 $\\mathrm{AUC}_{\\mathrm{ins}}$。\n  - 计算 $F = \\mathrm{AUC}_{\\mathrm{ins}} - \\mathrm{AUC}_{\\mathrm{del}}$。\n- 对每个测试用例，如果 $F_{\\mathrm{A}} \\ge F_{\\mathrm{B}}$ 则输出 $0$，否则输出 $1$。\n- 最后一行必须是一个单独的列表 $[r_{1},r_{2},r_{3}]$。\n\n这个过程将模型的可解释性和可说明性操作化：仅凭梯度相似性可能会因饱和效应而产生误导，而删除/插入曲线则为显著性的忠实度提供了一种有原则的、基于效果的诊断方法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Sigmoid and its derivative\ndef sigmoid(z: np.ndarray) - np.ndarray:\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef dsigmoid(z: np.ndarray) - np.ndarray:\n    s = sigmoid(z)\n    return s * (1.0 - s)\n\n# Tanh and sech^2 utilities\ndef tanh(u: np.ndarray) - np.ndarray:\n    return np.tanh(u)\n\ndef sech2(u: np.ndarray) - np.ndarray:\n    # sech(u) = 1/cosh(u); sech^2(u) = 1 / cosh(u)^2\n    return 1.0 / (np.cosh(u) ** 2)\n\n# Image geometry and regions\nH, W = 16, 16\n# Define S: rows 0..3, cols 0..3\nS_rows = slice(0, 4)\nS_cols = slice(0, 4)\n# Define C: rows 0..3, cols 12..15\nC_rows = slice(0, 4)\nC_cols = slice(12, 16)\nS_size = (S_rows.stop - S_rows.start) * (S_cols.stop - S_cols.start)\nC_size = (C_rows.stop - C_rows.start) * (C_cols.stop - C_cols.start)\n\n# Model parameters\na_S, a_C = 4.0, 0.2\nb_S, b_C, alpha = 1.5, 3.0, 5.0\n\ndef mean_region(img: np.ndarray, rows: slice, cols: slice) - float:\n    region = img[rows, cols]\n    return float(np.mean(region))\n\ndef modelA_logit(img: np.ndarray) - float:\n    mS = mean_region(img, S_rows, S_cols)\n    mC = mean_region(img, C_rows, C_cols)\n    return a_S * mS + a_C * mC\n\ndef modelA_prob(img: np.ndarray) - float:\n    return float(sigmoid(modelA_logit(img)))\n\ndef modelA_grad(img: np.ndarray) - np.ndarray:\n    # Gradient of f_A wrt pixels: dsigmoid(z_A) * dz_A/dx\n    zA = modelA_logit(img)\n    g = dsigmoid(zA)\n    grad = np.zeros_like(img, dtype=float)\n    grad[S_rows, S_cols] = g * (a_S / S_size)\n    grad[C_rows, C_cols] = g * (a_C / C_size)\n    return grad\n\ndef modelB_logit(img: np.ndarray) - float:\n    mS = mean_region(img, S_rows, S_cols)\n    mC = mean_region(img, C_rows, C_cols)\n    return b_S * mS + b_C * tanh(alpha * mC)\n\ndef modelB_prob(img: np.ndarray) - float:\n    return float(sigmoid(modelB_logit(img)))\n\ndef modelB_grad(img: np.ndarray) - np.ndarray:\n    # Gradient of f_B wrt pixels: dsigmoid(z_B) * dz_B/dx\n    mC = mean_region(img, C_rows, C_cols)\n    zB = modelB_logit(img)\n    g = dsigmoid(zB)\n    grad = np.zeros_like(img, dtype=float)\n    # S contribution\n    grad[S_rows, S_cols] = g * (b_S / S_size)\n    # C contribution with tanh chain rule: b_C * alpha * sech^2(alpha * mC) / |C|\n    grad[C_rows, C_cols] = g * (b_C * alpha * float(sech2(alpha * mC)) / C_size)\n    return grad\n\ndef auc_trapezoid(y: np.ndarray) - float:\n    # y is an array of f-values at uniformly spaced x in [0,1]\n    T = len(y) - 1\n    x = np.linspace(0.0, 1.0, T + 1)\n    return float(np.trapz(y, x))\n\ndef deletion_insertion_auc(img: np.ndarray, prob_fn, grad_fn, T: int = 20) - tuple[float, float]:\n    # Compute saliency from gradient magnitude\n    grad = grad_fn(img)\n    sal = np.abs(grad).reshape(-1)  # flatten\n    order = np.argsort(-sal)  # descending saliency\n\n    N = img.size\n    # Deletion: start from original, progressively zero-out top-k pixels\n    del_probs = []\n    del_img = img.copy().reshape(-1)\n    for t in range(T + 1):\n        # Record\n        del_probs.append(prob_fn(del_img.reshape(img.shape)))\n        # Next step: zero next batch\n        if t  T:\n            k_next = int(np.floor((t + 1) * N / T))\n            k_curr = int(np.floor(t * N / T))\n            idx = order[k_curr:k_next]\n            del_img[idx] = 0.0\n    del_probs = np.array(del_probs, dtype=float)\n    del_auc = auc_trapezoid(del_probs)\n\n    # Insertion: start from zeros, progressively insert top-k original pixels\n    ins_probs = []\n    ins_img = np.zeros_like(img).reshape(-1)\n    x_flat = img.reshape(-1)\n    for t in range(T + 1):\n        ins_probs.append(prob_fn(ins_img.reshape(img.shape)))\n        if t  T:\n            k_next = int(np.floor((t + 1) * N / T))\n            k_curr = int(np.floor(t * N / T))\n            idx = order[k_curr:k_next]\n            ins_img[idx] = x_flat[idx]\n    ins_probs = np.array(ins_probs, dtype=float)\n    ins_auc = auc_trapezoid(ins_probs)\n\n    return del_auc, ins_auc\n\ndef build_image(mu_S: float, rho: float = 0.8) - np.ndarray:\n    mu_C = rho * mu_S\n    img = np.zeros((H, W), dtype=float)\n    img[S_rows, S_cols] = mu_S\n    img[C_rows, C_cols] = mu_C\n    return img\n\ndef solve():\n    # Define the test cases from the problem statement: mu_S values\n    mu_S_values = [2.0, 0.8, 0.2]\n    rho = 0.8\n\n    results = []\n    for mu_S in mu_S_values:\n        img = build_image(mu_S, rho=rho)\n\n        # Compute AUCs for model A\n        delA, insA = deletion_insertion_auc(img, modelA_prob, modelA_grad, T=20)\n        scoreA = insA - delA\n\n        # Compute AUCs for model B\n        delB, insB = deletion_insertion_auc(img, modelB_prob, modelB_grad, T=20)\n        scoreB = insB - delB\n\n        # Decide which model is more faithful: 0 for A if scoreA = scoreB, else 1 for B\n        result = 0 if scoreA = scoreB else 1\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3153222"}, {"introduction": "在掌握了归因和评估技术后，我们可以探索更前沿的因果推断方法，以更精确地定位模型内部组件的功能。激活补丁（Activation Patching）是一种基于干预的强大技术，它让我们能够探究“反事实”问题。通过将一个输入样本中某个特定组件（如注意力头）的激活替换为来自另一个反例样本的激活，我们可以直接衡量该组件对模型最终预测的因果贡献。这个练习 ([@problem_id:3153142]) 将在一个简化的Transformer模型上实践这一技术，让你亲身体验如何通过精确干预来剖析复杂模型的内部工作机制。", "problem": "给定一个简化的、完全指定的双头自注意力分类器和一个激活修补（activation patching）程序。目标是计算当一个样本的某个注意力头的注意力后激活值被来自反例的激活值替换时，该注意力头对预测概率变化的因果贡献。您的程序必须实现该模型，计算原始预测和修补后的预测，并为提供的测试套件输出预测概率的有符号差异。\n\n模型的第一性原理定义：\n- 令 $L$ 表示序列长度，$d_{\\text{model}}$ 表示模型维度，$H$ 表示注意力头的数量，$d_h$ 表示每个头的维度。对于每个输入序列，令词元嵌入矩阵为 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$。\n- 对于头 $h \\in \\{0, 1\\}$，定义查询、键和值权重矩阵 $W_Q^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_h}$、$W_K^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_h}$ 和 $W_V^{(h)} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_h}$。\n- 查询、键和值计算如下：$Q^{(h)} = X W_Q^{(h)}$，$K^{(h)} = X W_K^{(h)}$ 和 $V^{(h)} = X W_V^{(h)}$。\n- 头 $h$ 的缩放点积注意力计算如下\n$$\nS^{(h)} = \\frac{Q^{(h)} (K^{(h)})^\\top}{\\sqrt{d_h}}, \\quad A^{(h)}_{i,:} = \\operatorname{softmax}\\left(S^{(h)}_{i,:}\\right),\n$$\n其中 $\\operatorname{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$ 按行应用，得到注意力权重 $A^{(h)} \\in \\mathbb{R}^{L \\times L}$。注意力后头部输出为\n$$\nO^{(h)} = A^{(h)} V^{(h)} \\in \\mathbb{R}^{L \\times d_h}.\n$$\n- 多头注意力 (MHA) 输出是拼接 $O = \\operatorname{concat}(O^{(0)}, O^{(1)}) \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$，然后是一个输出投影 $W_O \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}}$，得到\n$$\n\\tilde{O} = O W_O^\\top \\in \\mathbb{R}^{L \\times d_{\\text{model}}}.\n$$\n- 分类器读取第一个词元（索引为 $0$，类似于分类词元）作为 $\\tilde{o}_{\\text{cls}} = \\tilde{O}_{0,:} \\in \\mathbb{R}^{d_{\\text{model}}}$，并计算一个 logit $y = w^\\top \\tilde{o}_{\\text{cls}} + b$ 用于二元预测，其概率为\n$$\np = \\sigma(y) = \\frac{1}{1 + e^{-y}}.\n$$\n\n激活修补程序：\n- 对于选定的头索引 $h^\\star \\in \\{0, 1\\}$，将一个样本的整个注意力后激活矩阵 $O^{(h^\\star)}$ 替换为从反例计算出的相应矩阵，同时保持所有其他头不变。重新计算修补后样本的预测概率，并将因果贡献定义为有符号差异\n$$\n\\Delta = p_{\\text{patched}} - p_{\\text{original}}.\n$$\n\n具体的、数值指定的模型：\n- 维度：$L = 3$，$d_{\\text{model}} = 6$，$H = 2$，$d_h = 3$。\n- 头 $0$ 的权重（选择前三个嵌入维度）：\n$$\nW_Q^{(0)} = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\n\\end{bmatrix},\\quad\nW_K^{(0)} = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\n\\end{bmatrix},\\quad\nW_V^{(0)} = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- 头 $1$ 的权重（选择后三个嵌入维度）：\n$$\nW_Q^{(1)} = \\begin{bmatrix}\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\\\\\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\n\\end{bmatrix},\\quad\nW_K^{(1)} = \\begin{bmatrix}\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\\\\\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\n\\end{bmatrix},\\quad\nW_V^{(1)} = \\begin{bmatrix}\n0  0  0\\\\\n0  0  0\\\\\n0  0  0\\\\\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\n\\end{bmatrix}.\n$$\n- 输出投影：\n$$\nW_O = I_6,\n$$\n$6 \\times 6$ 的单位矩阵。\n- 分类器参数：\n$$\nw = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix},\\quad b = 0.\n$$\n\n测试套件输入：\n- 样本 $X^{(E1)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(E1)} = \\begin{bmatrix}\n2  2  2  0.1  -0.2  0.0\\\\\n3  -1  0  0.2  0.1  -0.1\\\\\n-1  1  0.5  0.0  0.0  0.2\n\\end{bmatrix}.\n$$\n- 反例 $X^{(C1)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(C1)} = \\begin{bmatrix}\n-2  -2  -2  0.0  0.0  0.1\\\\\n-3  0.5  1  0.1  -0.2  0.3\\\\\n1  1  1  0.0  -0.1  0.0\n\\end{bmatrix}.\n$$\n- 样本 $X^{(E3)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(E3)} = \\begin{bmatrix}\n4  0  1  0.0  0.0  0.0\\\\\n0.5  3  1  0.1  0.0  -0.1\\\\\n1  -2  2  0.0  0.2  0.2\n\\end{bmatrix}.\n$$\n- 反例 $X^{(C3)} \\in \\mathbb{R}^{3 \\times 6}$：\n$$\nX^{(C3)} = \\begin{bmatrix}\n-4  0  -1  0.0  0.0  0.0\\\\\n0.5  -3  -1  0.0  0.1  0.0\\\\\n-1  2  -2  -0.1  0.0  0.1\n\\end{bmatrix}.\n$$\n\n测试套件和要求的输出：\n- 情况1：用 $X^{(C1)}$ 修补 $X^{(E1)}$ 的头 $h^\\star = 0$；输出 $\\Delta_1 \\in \\mathbb{R}$。\n- 情况2：用 $X^{(C1)}$ 修补 $X^{(E1)}$ 的头 $h^\\star = 1$；输出 $\\Delta_2 \\in \\mathbb{R}$。\n- 情况3：用其自身 $X^{(E1)}$ 修补 $X^{(E1)}$ 的头 $h^\\star = 0$；输出 $\\Delta_3 \\in \\mathbb{R}$（一个应为零的边界情况）。\n- 情况4：用 $X^{(C3)}$ 修补 $X^{(E3)}$ 的头 $h^\\star = 0$；输出 $\\Delta_4 \\in \\mathbb{R}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，具体格式为 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。不涉及物理单位或角度；所有输出都是实数。", "solution": "该问题是有效的，因为它具有科学依据、适定且完全指定。它在深度学习模型可解释性领域提供了一个清晰的、可通过计算验证的任务。我们将着手提供一个解决方案。\n\n目标是计算特定注意力头对模型预测的因果贡献 $\\Delta$。这被定义为当一个样本 $X_E$ 的该头的注意力后激活值被来自反例 $X_C$ 的激活值替换时，预测概率的变化。其公式为 $\\Delta = p_{\\text{patched}} - p_{\\text{original}}$。\n\n解决方案需要实现指定的双头注意力模型的前向传播，然后对四个给定的测试用例应用激活修补程序。\n\n**1. 模型前向传播**\n\n前向传播计算给定输入词元嵌入矩阵 $X \\in \\mathbb{R}^{L \\times d_{\\text{model}}}$ 的预测概率 $p$。维度给定为 $L=3$，$d_{\\text{model}}=6$，$H=2$，每个头的维度是 $d_h = d_{\\text{model}} / H = 3$。\n\n**步骤1.1：逐头计算**\n\n对于每个头 $h \\in \\{0, 1\\}$，我们计算查询（$Q^{(h)}$）、键（$K^{(h)}$）和值（$V^{(h)}$）矩阵。问题指定的权重矩阵 $W_Q^{(h)}$、$W_K^{(h)}$、$W_V^{(h)}$ 起到选择器的作用。\n\n对于头 $h=0$，权重选择输入嵌入 $X$ 的前 $d_h=3$ 列。因此：\n$$\nQ^{(0)} = K^{(0)} = V^{(0)} = X_{[ :, 0:3 ]} \\in \\mathbb{R}^{3 \\times 3}\n$$\n\n对于头 $h=1$，权重选择 $X$ 的后 $d_h=3$ 列：\n$$\nQ^{(1)} = K^{(1)} = V^{(1)} = X_{[ :, 3:6 ]} \\in \\mathbb{R}^{3 \\times 3}\n$$\n\n接下来，我们计算缩放点积注意力分数 $S^{(h)}$，通过按行 softmax 计算注意力权重 $A^{(h)}$，以及注意力后头部输出 $O^{(h)}$。\n$$\nS^{(h)} = \\frac{Q^{(h)} (K^{(h)})^\\top}{\\sqrt{d_h}}\n$$\n$$\nA^{(h)}_{i,:} = \\operatorname{softmax}\\left(S^{(h)}_{i,:}\\right) \\quad \\text{where } \\operatorname{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n$$\n$$\nO^{(h)} = A^{(h)} V^{(h)} \\in \\mathbb{R}^{3 \\times 3}\n$$\n\n**步骤1.2：MHA 输出和分类**\n\n两个头 $O^{(0)}$ 和 $O^{(1)}$ 的输出被拼接起来，形成多头注意力 (MHA) 的输入 $O$：\n$$\nO = \\operatorname{concat}(O^{(0)}, O^{(1)}) \\in \\mathbb{R}^{3 \\times 6}\n$$\n\n然后是一个输出投影 $\\tilde{O} = O W_O^\\top$。因为 $W_O = I_6$（$6 \\times 6$ 单位矩阵），我们有 $W_O^\\top = I_6$，因此：\n$$\n\\tilde{O} = O I_6 = O\n$$\n\n分类器使用第一个词元（索引为 $0$）的表示，记为 $\\tilde{o}_{\\text{cls}}$，即 $\\tilde{O}$ 的第一行：\n$$\n\\tilde{o}_{\\text{cls}} = \\tilde{O}_{0,:} \\in \\mathbb{R}^{6}\n$$\n\n使用权重向量 $w = [1, 1, 1, 0.1, 0.1, 0.1]^\\top$ 和偏置 $b=0$ 计算 logit $y$：\n$$\ny = w^\\top \\tilde{o}_{\\text{cls}} + b = \\sum_{i=0}^{5} w_i (\\tilde{o}_{\\text{cls}})_i\n$$\n\n最后，通过应用 sigmoid 函数 $\\sigma(\\cdot)$ 得到预测概率 $p$：\n$$\np = \\sigma(y) = \\frac{1}{1 + e^{-y}}\n$$\n\n这样就完成了单个输入 $X$ 的前向传播。\n\n**2. 激活修补程序**\n\n为了计算给定样本 $X_E$、反例 $X_C$ 和目标头 $h^\\star$ 的因果贡献 $\\Delta$，我们按以下步骤进行：\n\n**步骤2.1：计算原始概率**\n首先，我们通过对样本输入 $X_E$ 执行完整的前向传播来计算原始预测概率 $p_{\\text{original}}$。这需要从 $X_E$ 计算两个头的输出 $O^{(0)}_E$ 和 $O^{(1)}_E$。\n$$\np_{\\text{original}} = \\text{compute_prob}(X_E)\n$$\n\n**步骤2.2：计算修补后概率**\n接下来，我们计算“修补后的”概率 $p_{\\text{patched}}$。这涉及将目标头 $h^\\star$ 的激活值替换为来自反例 $X_C$ 的相应激活值，同时保持原始样本 $X_E$ 中另一个头的激活值不变。\n\n如果 $h^\\star = 0$，则修补后的 MHA 输入由 $O^{(0)}_C$（从 $X_C$ 计算）和 $O^{(1)}_E$（从 $X_E$ 计算）构成：\n$$\nO_{\\text{patched}} = \\operatorname{concat}(O^{(0)}_C, O^{(1)}_E)\n$$\n\n如果 $h^\\star = 1$，则修补后的 MHA 输入使用 $O^{(0)}_E$ 和 $O^{(1)}_C$：\n$$\nO_{\\text{patched}} = \\operatorname{concat}(O^{(0)}_E, O^{(1)}_C)\n$$\n\n从 $O_{\\text{patched}}$ 开始，我们遵循与标准前向传播中相同的分类步骤，计算修补后的 logit $y_{\\text{patched}}$ 和概率 $p_{\\text{patched}}$。\n\n**步骤2.3：计算因果贡献**\n因果贡献 $\\Delta$ 是修补后概率与原始概率之间的有符号差异：\n$$\n\\Delta = p_{\\text{patched}} - p_{\\text{original}}\n$$\n\n对于 $X_C = X_E$ 的测试用例（情况3），“修补后的”激活值 $O^{(h^\\star)}_C$ 与原始激活值 $O^{(h^\\star)}_E$ 相同。因此，$p_{\\text{patched}} = p_{\\text{original}}$，我们期望 $\\Delta_3 = 0$，这可作为对实现的合理性检查。\n\n以下 Python 程序实现了此逻辑，以计算指定测试套件的 $\\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Computes the causal contribution of attention heads via activation patching.\n    \"\"\"\n    \n    # Define model parameters\n    d_h = 3\n    w = np.array([1.0, 1.0, 1.0, 0.1, 0.1, 0.1])\n    b = 0.0\n\n    # Define test suite inputs as numpy arrays\n    X_E1 = np.array([\n        [2.0, 2.0, 2.0, 0.1, -0.2, 0.0],\n        [3.0, -1.0, 0.0, 0.2, 0.1, -0.1],\n        [-1.0, 1.0, 0.5, 0.0, 0.0, 0.2]\n    ])\n    X_C1 = np.array([\n        [-2.0, -2.0, -2.0, 0.0, 0.0, 0.1],\n        [-3.0, 0.5, 1.0, 0.1, -0.2, 0.3],\n        [1.0, 1.0, 1.0, 0.0, -0.1, 0.0]\n    ])\n    X_E3 = np.array([\n        [4.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.5, 3.0, 1.0, 0.1, 0.0, -0.1],\n        [1.0, -2.0, 2.0, 0.0, 0.2, 0.2]\n    ])\n    X_C3 = np.array([\n        [-4.0, 0.0, -1.0, 0.0, 0.0, 0.0],\n        [0.5, -3.0, -1.0, 0.0, 0.1, 0.0],\n        [-1.0, 2.0, -2.0, -0.1, 0.0, 0.1]\n    ])\n\n    # Define the test cases\n    test_cases = [\n        {'X_E': X_E1, 'X_C': X_C1, 'h_star': 0},  # Case 1\n        {'X_E': X_E1, 'X_C': X_C1, 'h_star': 1},  # Case 2\n        {'X_E': X_E1, 'X_C': X_E1, 'h_star': 0},  # Case 3\n        {'X_E': X_E3, 'X_C': X_C3, 'h_star': 0}   # Case 4\n    ]\n    \n    def sigmoid(y):\n        \"\"\"Computes the sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-y))\n\n    def compute_head_output(X, h, d_h_val):\n        \"\"\"\n        Computes the post-attention output for a single head.\n        \n        Args:\n            X (np.ndarray): Input token embeddings, shape (L, d_model).\n            h (int): Head index (0 or 1).\n            d_h_val (int): Per-head dimension.\n            \n        Returns:\n            np.ndarray: Post-attention head output, shape (L, d_h).\n        \"\"\"\n        if h == 0:\n            sub_X = X[:, :d_h_val]\n        else: # h == 1\n            sub_X = X[:, d_h_val:]\n        \n        Q, K, V = sub_X, sub_X, sub_X\n        \n        S = (Q @ K.T) / np.sqrt(d_h_val)\n        A = softmax(S, axis=1)\n        O_h = A @ V\n        \n        return O_h\n\n    def compute_prob_from_heads(O_0, O_1):\n        \"\"\"\n        Computes the final probability from the head outputs.\n        \"\"\"\n        O = np.concatenate((O_0, O_1), axis=1)\n        o_cls = O[0, :]\n        y = np.dot(w, o_cls) + b\n        p = sigmoid(y)\n        return p\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        X_E = case['X_E']\n        X_C = case['X_C']\n        h_star = case['h_star']\n\n        # 1. Compute original probability\n        O0_E = compute_head_output(X_E, 0, d_h)\n        O1_E = compute_head_output(X_E, 1, d_h)\n        p_original = compute_prob_from_heads(O0_E, O1_E)\n\n        # 2. Compute patched probability\n        if h_star == 0:\n            O0__patched = compute_head_output(X_C, 0, d_h)\n            O1_patched = O1_E\n        else: # h_star == 1\n            O0_patched = O0_E\n            O1__patched = compute_head_output(X_C, 1, d_h)\n        \n        if h_star == 0:\n             p_patched = compute_prob_from_heads(O0__patched, O1_patched)\n        else:\n             p_patched = compute_prob_from_heads(O0_patched, O1__patched)\n        \n        # 3. Compute and store the difference\n        delta = p_patched - p_original\n        results.append(delta)\n\n    # Format the output as specified\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3153142"}]}