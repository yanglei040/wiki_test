{"hands_on_practices": [{"introduction": "为了真正理解认知不确定性（epistemic uncertainty）和偶然不确定性（aleatoric uncertainty），最好从一个能够清晰、精确地分离这两种不确定性的分析模型入手。本练习将使用正态-逆伽马（Normal-Inverse-Gamma, NIG）共轭先验模型，这是一个经典的贝叶斯工具，它允许我们精确地分解预测方差。通过这个练习 [@problem_id:3197127]，你将通过探索数据量和噪声水平如何影响每种不确定性成分，建立起对这两个核心概念的深刻直觉。", "problem": "你的任务是使用正态-逆伽马（Normal–Inverse-Gamma, NIG）先验和后验，为单变量目标实现贝叶斯证据回归，并明确计算不确定性分解。目标是量化和比较在稀疏数据与密集数据情景下的认知不确定性和偶然不确定性。实现必须是自包含的，并为固定的测试套件产生数值输出。\n\n从概率论和贝叶斯统计中的以下基本基础开始：\n- 设数据生成过程为 $y_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$，其中 $\\mathcal{N}$ 表示正态分布。\n- 使用共轭先验 $(\\mu, \\sigma^2) \\sim \\text{Normal–Inverse-Gamma (NIG)}(\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)$，其中条件先验 $p(\\mu \\mid \\sigma^2) = \\mathcal{N}(\\mu_0, \\sigma^2 / \\kappa_0)$ 且边缘先验 $p(\\sigma^2) = \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$。\n- 对于观测数据 $D = \\{y_1, \\dots, y_n\\}$，其样本均值为 $\\bar{y}$，总平方偏差和为 $S = \\sum_{i=1}^n (y_i - \\bar{y})^2$，后验分布为 $(\\mu, \\sigma^2) \\mid D \\sim \\text{NIG}(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$，更新后的参数为\n$$\n\\kappa_n = \\kappa_0 + n, \\quad\n\\mu_n = \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_0 + n}, \\quad\n\\alpha_n = \\alpha_0 + \\frac{n}{2}, \\quad\n\\beta_n = \\beta_0 + \\frac{1}{2} S + \\frac{1}{2} \\cdot \\frac{\\kappa_0 n}{\\kappa_0 + n} (\\bar{y} - \\mu_0)^2.\n$$\n- 后验预测分布是位置为 $\\mu_n$ 的学生t分布（Student's $t$），其方差等于\n$$\nV_{\\text{pred}} = \\frac{\\beta_n}{\\alpha_n - 1} \\left( 1 + \\frac{1}{\\kappa_n} \\right),\n$$\n前提是 $\\alpha_n > 1$。该分解源于全方差定律：\n$$\nV_{\\text{alea}} = \\mathbb{E}[\\sigma^2 \\mid D] = \\frac{\\beta_n}{\\alpha_n - 1}, \\quad\nV_{\\text{epi}} = \\operatorname{Var}[\\mu \\mid D] = \\frac{\\mathbb{E}[\\sigma^2 \\mid D]}{\\kappa_n} = \\frac{\\beta_n}{(\\alpha_n - 1)\\kappa_n},\n$$\n因此 $V_{\\text{pred}} = V_{\\text{alea}} + V_{\\text{epi}}$。\n- 在证据回归中，均值的“证据水平”由类精度参数 $\\nu$ 量化，这里对应于 $\\nu = \\kappa_n$；较大的 $\\nu$ 表示在该区域有更多的数据支持，因此认知不确定性较低。\n\n在一个单一的 Python 程序中实现以下任务：\n1. 使用 $\\mathcal{N}(\\mu_{\\text{true}}, \\sigma_{\\text{true}}^2)$ 为每个测试案例生成数据，样本大小为 $n = N$。为保证可复现性，使用固定的随机种子 $42$。不允许外部输入。\n2. 对每个测试案例，使用上述更新方程计算后验参数 $(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$。\n3. 计算证据水平 $\\nu = \\kappa_n$、偶然方差 $V_{\\text{alea}}$、认知方差 $V_{\\text{epi}}$ 和预测方差 $V_{\\text{pred}}$。\n4. 将所有报告的数字四舍五入到六位小数。\n\n对所有测试案例使用以下固定的先验超参数：$\\mu_0 = 0$, $\\kappa_0 = 1$, $\\alpha_0 = 2$, $\\beta_0 = 1$。\n\n测试套件和覆盖范围：\n- 案例 A (密集, 低噪声): $N = 200$, $\\mu_{\\text{true}} = 0$, $\\sigma_{\\text{true}} = 0.2$。\n- 案例 B (稀疏, 低噪声): $N = 5$, $\\mu_{\\text{true}} = 0$, $\\sigma_{\\text{true}} = 0.2$。\n- 案例 C (密集, 高噪声): $N = 200$, $\\mu_{\\text{true}} = 0$, $\\sigma_{\\text{true}} = 1.0$。\n- 案例 D (极度稀疏, 中等噪声): $N = 1$, $\\mu_{\\text{true}} = 0$, $\\sigma_{\\text{true}} = 0.5$。\n- 案例 E (稀疏, 低噪声, 先验均值冲突): $N = 5$, $\\mu_{\\text{true}} = 3.0$, $\\sigma_{\\text{true}} = 0.2$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素本身是对应于一个案例的方括号列表。对于每个案例，输出四元组 $[\\nu, V_{\\text{alea}}, V_{\\text{epi}}, V_{\\text{pred}}]$，所有数值都四舍五入到六位小数。最终输出格式必须是：\n\"[[v_a,a_a,e_a,p_a],[v_b,a_b,e_b,p_b],[v_c,a_c,e_c,p_c],[v_d,a_d,e_d,p_d],[v_e,a_e,e_e,p_e]]\"\n整行中不得有空格。", "solution": "该问题被评估为有效。它在贝叶斯统计理论方面有科学依据，问题陈述适定，具有唯一且可计算的解，是客观且自包含的。所有必要的数学公式、参数和测试条件都已提供，没有歧义或矛盾。因此，我们可以着手提供一个完整的解决方案。\n\n目标是为单变量输出实现一个贝叶斯证据回归模型。该模型使用正态-逆伽马（NIG）共轭先验，以推导高斯数据生成过程的均值 $\\mu$ 和方差 $\\sigma^2$ 的后验分布。基于此后验分布，我们将预测不确定性分解为其偶然分量和认知分量，并分析这些分量如何随数据稀疏度和噪声水平的变化而变化。\n\n首先，我们形式化该概率模型。我们假设数据 $D = \\{y_1, \\dots, y_n\\}$ 是来自未知均值 $\\mu$ 和未知方差 $\\sigma^2$ 的正态分布的独立同分布样本：\n$$\ny_i \\sim \\mathcal{N}(\\mu, \\sigma^2)\n$$\n\n$(\\mu, \\sigma^2)$ 的共轭先验是正态-逆伽马分布，表示为 $\\text{NIG}(\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)$。此先验由一个关于均值的条件正态分布 $p(\\mu \\mid \\sigma^2) = \\mathcal{N}(\\mu_0, \\sigma^2 / \\kappa_0)$ 和一个关于方差的边缘逆伽马分布 $p(\\sigma^2) = \\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$ 定义。超参数 $(\\mu_0, \\kappa_0, \\alpha_0, \\beta_0)$ 编码了我们的先验证信。对于此问题，我们使用固定的先验超参数：$\\mu_0 = 0$, $\\kappa_0 = 1$, $\\alpha_0 = 2$, $\\beta_0 = 1$。\n\n给定一组 $n$ 个观测值 $D$，我们更新我们的先验证信以形成后验分布 $p(\\mu, \\sigma^2 \\mid D)$。由于 NIG 先验的共轭性，后验分布也是一个 NIG 分布，$p(\\mu, \\sigma^2 \\mid D) \\sim \\text{NIG}(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$。更新后的参数使用样本统计量计算：样本均值 $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ 和平方偏差和 $S = \\sum_{i=1}^n (y_i - \\bar{y})^2$。后验更新方程为：\n$$\n\\kappa_n = \\kappa_0 + n \\\\\n\\mu_n = \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_0 + n} \\\\\n\\alpha_n = \\alpha_0 + \\frac{n}{2} \\\\\n\\beta_n = \\beta_0 + \\frac{1}{2} S + \\frac{1}{2} \\cdot \\frac{\\kappa_0 n}{\\kappa_0 + n} (\\bar{y} - \\mu_0)^2\n$$\n参数 $\\kappa_n$ 在证据回归框架中可被解释为“证据水平” $\\nu$，量化了支持均值后验估计的数据量。较大的 $\\kappa_n$ 表示对均值位置的置信度更高。\n\n分析的核心在于总预测方差 $V_{\\text{pred}}$ 的分解。新观测值的后验预测分布是一个学生t分布。其方差 $V_{\\text{pred}}$ 由全方差定律给出：\n$$\nV_{\\text{pred}} = \\operatorname{Var}(y^* \\mid D) = \\mathbb{E}[\\operatorname{Var}(y^* \\mid \\mu, \\sigma^2)] + \\operatorname{Var}(\\mathbb{E}[y^* \\mid \\mu, \\sigma^2])\n$$\n第一项代表数据中固有的、不可约的随机性，即偶然不确定性。第二项代表我们模型参数的不确定性，它可以通过更多数据来减少，因此是认知不确定性。对于我们的模型，此分解产生：\n1.  **偶然方差 ($V_{\\text{alea}}$)**：数据方差 $\\sigma^2$ 的后验期望。这是关于 $\\sigma^2$ 的后验边缘逆伽马分布的均值。\n    $$\n    V_{\\text{alea}} = \\mathbb{E}[\\sigma^2 \\mid D] = \\frac{\\beta_n}{\\alpha_n - 1} \\quad (\\text{对于 } \\alpha_n > 1)\n    $$\n2.  **认知方差 ($V_{\\text{epi}}$)**：数据均值 $\\mu$ 的后验方差。\n    $$\n    V_{\\text{epi}} = \\operatorname{Var}[\\mu \\mid D] = \\frac{\\mathbb{E}[\\sigma^2 \\mid D]}{\\kappa_n} = \\frac{\\beta_n}{(\\alpha_n - 1)\\kappa_n}\n    $$\n总预测方差是这两个分量的和：$V_{\\text{pred}} = V_{\\text{alea}} + V_{\\text{epi}}$。\n\n对每个测试案例，实现将按以下步骤进行：\n1.  使用固定的种子 $42$ 初始化一个单一的 `numpy` 随机数生成器，以保证所有数据生成步骤的可复现性。\n2.  通过从 $\\mathcal{N}(\\mu_{\\text{true}}, \\sigma_{\\text{true}}^2)$ 抽样，生成大小为 $N$ 的数据集。\n3.  计算样本统计量 $\\bar{y}$ 和 $S$。对于 $N = 1$ 的情况，$S$ 被正确地视为 $0$。\n4.  应用更新方程计算后验超参数 $\\mu_n, \\kappa_n, \\alpha_n, \\beta_n$。\n5.  使用后验超参数计算证据水平 $\\nu = \\kappa_n$ 和方差分量 $V_{\\text{alea}}$、$V_{\\text{epi}}$ 和 $V_{\\text{pred}}$。\n6.  将所有四个得到的数值四舍五入到六位小数，并按要求进行格式化。\n\n此过程将对所有五个测试案例重复进行，这些案例旨在展示关键行为：\n-   **案例 A vs. B**：显示数据量（$N=200$ vs. $N=5$）对认知不确定性的影响。在案例 A 中，$V_{\\text{epi}}$ 将显著更小。\n-   **案例 A vs. C**：显示数据噪声（$\\sigma_{\\text{true}}=0.2$ vs. $\\sigma_{\\text{true}}=1.0$）的影响。在案例 C 中，$V_{\\text{alea}}$ 将显著更大，反映了更高的内在方差。\n-   **案例 D**：一个极端稀疏的情况（$N=1$），其中后验是对先验的轻微更新。\n-   **案例 E**：一个稀疏情况（$N=5$），其中存在先验与数据的冲突（$\\mu_{\\text{true}}=3.0$ vs. $\\mu_0=0$）。预计 $\\beta_n$ 更新中的冲突项会增加估计的偶然方差和认知方差。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Bayesian evidential regression with a Normal-Inverse-Gamma prior\n    to compute and decompose predictive uncertainty for several test cases.\n    \"\"\"\n    # Fixed prior hyperparameters\n    mu_0 = 0.0\n    kappa_0 = 1.0\n    alpha_0 = 2.0\n    beta_0 = 1.0\n    \n    # Test suite: (N, mu_true, sigma_true)\n    test_cases = [\n        (200, 0.0, 0.2),  # Case A: dense, low noise\n        (5, 0.0, 0.2),    # Case B: sparse, low noise\n        (200, 0.0, 1.0),  # Case C: dense, high noise\n        (1, 0.0, 0.5),    # Case D: extremely sparse, medium noise\n        (5, 3.0, 0.2),    # Case E: sparse, low noise, prior-mean conflict\n    ]\n    \n    # Initialize a single random number generator for reproducibility\n    rng = np.random.default_rng(42)\n    \n    all_results = []\n\n    for n, mu_true, sigma_true in test_cases:\n        # Step 1: Generate data\n        y_data = rng.normal(loc=mu_true, scale=sigma_true, size=n)\n        \n        # Calculate sample statistics\n        if n > 0:\n            y_bar = np.mean(y_data)\n        else:\n            # This case is not in the test suite but is handled for completeness\n            y_bar = 0.0\n            \n        if n > 1:\n            # Sum of squared deviations from the mean\n            s_stat = np.sum((y_data - y_bar)**2)\n        else:\n            # For n=0 or n=1, the sum of squared deviations is 0\n            s_stat = 0.0\n            \n        # Step 2: Compute posterior parameters\n        kappa_n = kappa_0 + n\n        mu_n = (kappa_0 * mu_0 + n * y_bar) / kappa_n\n        alpha_n = alpha_0 + n / 2.0\n        \n        beta_update_term = (kappa_0 * n / kappa_n) * (y_bar - mu_0)**2\n        beta_n = beta_0 + 0.5 * s_stat + 0.5 * beta_update_term\n        \n        # Step 3: Compute evidence level and uncertainty variances\n        # The condition alpha_n > 1 is always met since alpha_0=2 and n>=1\n        nu = kappa_n\n        v_alea = beta_n / (alpha_n - 1.0)\n        v_epi = v_alea / kappa_n\n        v_pred = v_alea + v_epi\n        \n        # Append the results for the current case\n        # nu is treated as a float for consistent formatting\n        all_results.append([float(nu), v_alea, v_epi, v_pred])\n\n    # Step 4: Format the final output string\n    case_strings = []\n    for case_result in all_results:\n        # Format each number to 6 decimal places and join into a string\n        formatted_numbers = [f\"{num:.6f}\" for num in case_result]\n        case_strings.append(f\"[{','.join(formatted_numbers)}]\")\n        \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Print the final result in the exact required format\n    print(final_output)\n\nsolve()\n```", "id": "3197127"}, {"introduction": "在理想的贝叶斯模型中分离不确定性之后，我们转向深度学习的现实世界，在深度学习中，精确的贝叶斯推断通常是难以处理的。蒙特卡洛 Dropout（MC Dropout）等方法为近似贝叶斯推断提供了一种实用的途径，被广泛用于估计模型（认知）不确定性。这个练习 [@problem_id:3197106] 在理论与实践之间架起了一座至关重要的桥梁：它在一个简单的线性回归问题中，将 MC Dropout 估计的认知不确定性与精确的贝叶斯后验“真实值”进行比较，从而揭示了这种近似方法的性质和潜在的偏差。", "problem": "要求您设计并实现一个程序，用于在一个一维线性回归问题中，比较不同丢弃率下通过蒙特卡洛（MC）Dropout估计的认知不确定性与解析上正确的贝叶斯后验认知不确定性。您的程序必须遵循全方差定律和使用高斯先验及高斯似然的共轭贝叶斯线性回归的科学基础。除了从这些基础推导出的公式外，不要使用任何启发式公式。\n\n假设一个带加性高斯噪声的一维线性模型：对于每个训练对 $(x_n, y_n)$，$y_n = w x_n + \\epsilon_n$，其中 $\\epsilon_n \\sim \\mathcal{N}(0,\\sigma^2)$ 独立同分布，且先验为 $w \\sim \\mathcal{N}(0, s_0^2)$。全方差定律指出，对于一个新输入 $x_\\star$，预测方差可以分解为 $\\mathrm{Var}(y_\\star \\mid x_\\star, \\mathcal{D}) = \\mathbb{E}_{w \\mid \\mathcal{D}}[\\mathrm{Var}(y_\\star \\mid x_\\star, w)] + \\mathrm{Var}_{w \\mid \\mathcal{D}}(\\mathbb{E}[y_\\star \\mid x_\\star, w])$。请用此公式分别定义偶然不确定性分量和认知不确定性分量。您必须从这些基本定义出发，推导出在测试时为一个使用倒置丢弃缩放的单权重模型计算后验认知方差和 MC Dropout 引起的方差所需的表达式。\n\n使用以下大小为 $N = 5$ 的固定数据集 $\\mathcal{D}$：\n- 输入 $x$ 和输出 $y$（每个都按相同的索引顺序）：$x = [-2, -1, 0, 1, 2]$ 和 $y = [-3.12, -1.64, 0.0, 1.65, 3.14]$。这里的每个数字都是一个实标量。\n- 已知噪声方差 $\\sigma^2 = 0.01$ 和先验方差 $s_0^2 = 1.0$。\n\n定义 $x_\\star = 3.0$。使用普通最小二乘法对给定数据进行最大似然估计，以估算单个权重参数（无截距项）。然后：\n- 基于高斯先验和高斯似然模型，计算在 $x_\\star$ 处的解析正确的贝叶斯后验认知方差。\n- 在测试时，通过将单个丢弃掩码和倒置丢弃缩放应用于单个权重来对 MC Dropout 进行建模。具体来说，在 $x_\\star$ 处进行预测之前，将一个保留概率为 $(1-p)$ 的伯努利掩码（在保留时按 $\\frac{1}{1-p}$ 进行缩放）应用于学习到的单个权重。根据缩放伯努利随机变量的期望和方差定义，推导并计算由 dropout 引起的预测方差（此处不添加偶然不确定性噪声；将此由 dropout 引起的预测方差解释为在这种单权重设置下认知方差的 MC Dropout 估计）。\n\n对于下面测试套件中的每个丢弃率 $p$，计算 MC Dropout 认知方差相对于在 $x_\\star$ 处的贝叶斯后验认知方差的有符号分数误差：\n$$\\text{fractional\\_error}(p) = \\frac{\\widehat{v}_{\\text{drop}}(p) - v_{\\text{epistemic,true}}}{v_{\\text{epistemic,true}}}.$$\n\n使用以下丢弃率测试套件：\n- $p = 0.0$（无丢弃；边界情况），\n- $p = 0.1$（轻度丢弃；典型情况），\n- $p = 0.5$（中度丢弃），\n- $p = 0.9$（重度丢弃；近退化掩码率）。\n\n您的程序必须：\n- 从提供的 $(x,y)$ 计算普通最小二乘估计 $\\hat{w}$，\n- 计算在 $x_\\star$ 处的贝叶斯后验认知方差，\n- 对于测试套件中的每个 $p$，计算在 $x_\\star$ 处、将倒置丢弃缩放应用于单个权重时的 MC Dropout 认知方差，然后计算上面定义的有符号分数误差，\n- 生成一行输出，其中包含结果，格式为方括号括起来的逗号分隔列表，顺序与上面给出的 $p$ 的顺序相同，每个值都四舍五入到恰好六位小数。\n\n此问题中没有物理单位或角度。所有输出都必须是实值标量。您的输出必须恰好为一行一个列表，例如 $[a_1,a_2,a_3,a_4]$，其中每个 $a_k$ 是对应 $p$ 的四舍五入后的有符号分数误差。", "solution": "问题陈述已经过严格验证，并被确定为具有科学依据、提法得当且内部一致。它提出了一个比较，一方是从正式的贝叶斯线性回归模型推导出的认知不确定性，另一方是通过将蒙特卡洛（MC）Dropout应用于频率学派点估计模型而获得的估计。这是不确定性量化领域一个标准且富有洞察力的练习。提供了必要的数据、常数和定义，以得到一个唯一且可验证的解。\n\n解题过程分四个阶段：\n1.  通过普通最小二乘法（OLS）估计模型权重 $\\hat{w}$。\n2.  在测试点 $x_\\star$ 处计算真实的贝叶斯后验认知方差 $v_{\\text{epistemic,true}}$。\n3.  针对不同的丢弃率 $p$，推导并计算 MC Dropout 引起的方差 $\\widehat{v}_{\\text{drop}}(p)$。\n4.  对于每个指定的丢弃率，计算有符号分数误差。\n\n**1. 普通最小二乘法（OLS）权重估计**\n\n模型是一个单参数线性方程 $y = wx$。OLS 方法最小化残差平方和 $L(w) = \\sum_{n=1}^{N} (y_n - w x_n)^2$。最小化 $L(w)$ 的权重估计 $\\hat{w}$ 可通过将关于 $w$ 的导数设为零来找到：\n$$ \\frac{dL}{dw} = \\sum_{n=1}^{N} -2x_n(y_n - w x_n) = 0 $$\n$$ \\sum_{n=1}^{N} x_n y_n = \\hat{w} \\sum_{n=1}^{N} x_n^2 $$\n$$ \\hat{w} = \\frac{\\sum_{n=1}^{N} x_n y_n}{\\sum_{n=1}^{N} x_n^2} $$\n使用提供的数据集 $\\mathcal{D}$，其中输入为 $\\mathbf{x} = [-2, -1, 0, 1, 2]^T$，输出为 $\\mathbf{y} = [-3.12, -1.64, 0.0, 1.65, 3.14]^T$：\n$$ \\sum_{n=1}^{N} x_n y_n = (-2)(-3.12) + (-1)(-1.64) + (0)(0.0) + (1)(1.65) + (2)(3.14) = 6.24 + 1.64 + 0 + 1.65 + 6.28 = 15.81 $$\n$$ \\sum_{n=1}^{N} x_n^2 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10 $$\n权重的 OLS 估计为：\n$$ \\hat{w} = \\frac{15.81}{10} = 1.581 $$\n\n**2. 真实贝叶斯后验认知方差**\n\n问题基于全方差定律为新输入 $x_\\star$ 上的新预测 $y_\\star$ 定义了认知不确定性：\n$$ \\mathrm{Var}(y_\\star \\mid x_\\star, \\mathcal{D}) = \\underbrace{\\mathbb{E}_{w \\mid \\mathcal{D}}[\\mathrm{Var}(y_\\star \\mid x_\\star, w)]}_{\\text{偶然方差}} + \\underbrace{\\mathrm{Var}_{w \\mid \\mathcal{D}}(\\mathbb{E}[y_\\star \\mid x_\\star, w])}_{\\text{认知方差}} $$\n对于模型 $y_\\star = w x_\\star + \\epsilon_\\star$ 和噪声 $\\epsilon_\\star \\sim \\mathcal{N}(0, \\sigma^2)$，各分量为：\n- $\\mathbb{E}[y_\\star \\mid x_\\star, w] = w x_\\star$\n- $\\mathrm{Var}(y_\\star \\mid x_\\star, w) = \\sigma^2$\n因此，认知方差为：\n$$ v_{\\text{epistemic,true}} = \\mathrm{Var}_{w \\mid \\mathcal{D}}(w x_\\star) = x_\\star^2 \\mathrm{Var}_{w \\mid \\mathcal{D}}(w) $$\n为了找到权重的后验方差 $\\mathrm{Var}_{w \\mid \\mathcal{D}}(w)$，我们使用贝叶斯推断。模型指定了一个高斯先验 $w \\sim \\mathcal{N}(0, s_0^2)$ 和一个高斯似然 $p(\\mathcal{D} \\mid w) = \\prod_n \\mathcal{N}(y_n \\mid w x_n, \\sigma^2)$。后验分布 $p(w \\mid \\mathcal{D})$ 也是高斯分布，$w \\mid \\mathcal{D} \\sim \\mathcal{N}(\\mu_N, s_N^2)$。后验精度 $s_N^{-2}$ 是先验精度与依赖于数据的似然精度之和：\n$$ \\frac{1}{s_N^2} = \\frac{1}{s_0^2} + \\frac{1}{\\sigma^2} \\sum_{n=1}^{N} x_n^2 $$\n使用给定值 $s_0^2 = 1.0$, $\\sigma^2 = 0.01$ 和 $\\sum_{n=1}^{N} x_n^2 = 10$：\n$$ \\frac{1}{s_N^2} = \\frac{1}{1.0} + \\frac{10}{0.01} = 1 + 1000 = 1001 $$\n权重的后验方差是 $\\mathrm{Var}_{w \\mid \\mathcal{D}}(w) = s_N^2 = \\frac{1}{1001}$。\n在测试点 $x_\\star = 3.0$ 处，真实的认知方差是：\n$$ v_{\\text{epistemic,true}} = x_\\star^2 s_N^2 = (3.0)^2 \\left(\\frac{1}{1001}\\right) = \\frac{9}{1001} $$\n\n**3. MC Dropout 认知方差估计**\n\n问题要求在测试时通过将一个随机掩码应用于 OLS 权重估计 $\\hat{w}$ 来对 MC Dropout 进行建模。丢弃率为 $p$ 时，保留概率为 $1-p$。应用一个伯努利掩码 $z \\sim \\text{Bernoulli}(1-p)$。对于倒置丢弃，随机权重 $w_{\\text{drop}}$ 为：\n$$ w_{\\text{drop}} = \\hat{w} \\frac{z}{1-p} $$\n在 $x_\\star$ 处的预测是 $y_\\star^{\\text{drop}} = w_{\\text{drop}} x_\\star$。MC Dropout 方差是这个预测在掩码 $z$ 分布上的方差：\n$$ \\widehat{v}_{\\text{drop}}(p) = \\mathrm{Var}_z(y_\\star^{\\text{drop}}) = \\mathrm{Var}_z\\left(\\hat{w} \\frac{z}{1-p} x_\\star\\right) $$\n由于 $\\hat{w}$、$x_\\star$ 和 $p$ 相对于 $z$ 是常数：\n$$ \\widehat{v}_{\\text{drop}}(p) = (\\hat{w} x_\\star)^2 \\mathrm{Var}_z\\left(\\frac{z}{1-p}\\right) = \\frac{(\\hat{w} x_\\star)^2}{(1-p)^2} \\mathrm{Var}_z(z) $$\n伯努利随机变量 $z \\sim \\text{Bernoulli}(\\theta)$ 的方差是 $\\theta(1-\\theta)$。这里，$\\theta=1-p$，所以 $\\mathrm{Var}_z(z) = (1-p)(1 - (1-p)) = p(1-p)$。\n将此代入 $\\widehat{v}_{\\text{drop}}(p)$ 的表达式中：\n$$ \\widehat{v}_{\\text{drop}}(p) = \\frac{(\\hat{w} x_\\star)^2}{(1-p)^2} [p(1-p)] = (\\hat{w} x_\\star)^2 \\frac{p}{1-p} $$\n使用 $\\hat{w} = 1.581$ 和 $x_\\star = 3.0$：\n$$ \\widehat{v}_{\\text{drop}}(p) = (1.581 \\times 3.0)^2 \\frac{p}{1-p} = (4.743)^2 \\frac{p}{1-p} = 22.496049 \\frac{p}{1-p} $$\n\n**4. 有符号分数误差计算**\n\n有符号分数误差定义为：\n$$ \\text{fractional\\_error}(p) = \\frac{\\widehat{v}_{\\text{drop}}(p) - v_{\\text{epistemic,true}}}{v_{\\text{epistemic,true}}} = \\frac{\\widehat{v}_{\\text{drop}}(p)}{v_{\\text{epistemic,true}}} - 1 $$\n我们代入 $\\widehat{v}_{\\text{drop}}(p)$ 和 $v_{\\text{epistemic,true}}$ 的表达式：\n$$ \\text{fractional\\_error}(p) = \\frac{22.496049 \\frac{p}{1-p}}{9/1001} - 1 = \\left(\\frac{22.496049 \\times 1001}{9}\\right) \\frac{p}{1-p} - 1 \\approx 2502.060561 \\frac{p}{1-p} - 1 $$\n我们为测试套件 $\\{0.0, 0.1, 0.5, 0.9\\}$ 中的每个 $p$ 计算此值：\n- 对于 $p=0.0$: $\\text{fractional\\_error}(0.0) = 2502.060561 \\times 0 - 1 = -1.0$。\n- 对于 $p=0.1$: $\\text{fractional\\_error}(0.1) = 2502.060561 \\times \\frac{0.1}{0.9} - 1 \\approx 278.006729 - 1 = 277.006729$。\n- 对于 $p=0.5$: $\\text{fractional\\_error}(0.5) = 2502.060561 \\times \\frac{0.5}{0.5} - 1 = 2502.060561 - 1 = 2501.060561$。\n- 对于 $p=0.9$: $\\text{fractional\\_error}(0.9) = 2502.060561 \\times \\frac{0.9}{0.1} - 1 = 22518.545049 - 1 = 22517.545049$。\n\n将这些值四舍五入到六位小数，得到最终结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the signed fractional error of MC Dropout epistemic variance\n    with respect to the true Bayesian posterior epistemic variance for\n    a 1D linear regression problem.\n    \"\"\"\n    # Define the dataset and constants from the problem statement.\n    x_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data = np.array([-3.12, -1.64, 0.0, 1.65, 3.14])\n    \n    sigma2 = 0.01  # Known noise variance\n    s02 = 1.0      # Prior variance\n    x_star = 3.0   # Test point\n    \n    # Test suite of dropout rates\n    p_values = [0.0, 0.1, 0.5, 0.9]\n\n    # Step 1: Compute the Ordinary Least Squares (OLS) estimate for the weight.\n    # The model is y = w*x, so the OLS estimate is w_hat = sum(x*y) / sum(x^2).\n    sum_xy = np.sum(x_data * y_data)\n    sum_x2 = np.sum(x_data**2)\n    w_hat = sum_xy / sum_x2\n\n    # Step 2: Compute the analytically correct Bayesian posterior epistemic variance.\n    # Posterior precision s_N^-2 = s_0^-2 + sum(x^2)/sigma^2\n    # Posterior variance s_N^2 = 1 / (1/s02 + sum_x2/sigma2)\n    posterior_variance_w = 1.0 / (1.0/s02 + sum_x2/sigma2)\n    # Epistemic variance at x_star is v_epistemic = x_star^2 * Var(w|D)\n    v_epistemic_true = x_star**2 * posterior_variance_w\n\n    results = []\n    for p in p_values:\n        # Step 3: Compute the MC Dropout epistemic variance estimate.\n        # This is the variance of the prediction over the dropout mask distribution.\n        # With inverted dropout, v_drop = (w_hat * x_star)^2 * p / (1-p).\n        if p == 1.0:\n            # Although not in the test suite, this is the theoretical limit.\n            v_drop = np.inf\n        else:\n            v_drop = (w_hat * x_star)**2 * p / (1.0 - p)\n\n        # Step 4: Compute the signed fractional error.\n        # fractional_error = (v_drop - v_epistemic_true) / v_epistemic_true\n        fractional_error = (v_drop / v_epistemic_true) - 1.0\n        results.append(fractional_error)\n\n    # Format the final results as a string with each value rounded to 6 decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3197106"}, {"introduction": "一旦我们能够量化认知不确定性，就可以提出更深入的问题：究竟是什么让模型感到“不确定”？这个练习 [@problem_id:3197111] 旨在探索模型的“已知的未知”，通过研究不同类型的输入扰动——是随机的“自然”噪声还是有目的的“对抗性”攻击——能最有效地增加认知不确定性。这项实践将不确定性量化（UQ）与模型鲁棒性联系起来，帮助你理解模型不确定性的几何性质，并识别出那些最挑战模型知识边界的输入方向。", "problem": "考虑具有高斯先验和高斯似然的贝叶斯线性回归（BLR）。设参数向量为 $w \\in \\mathbb{R}^d$，其先验分布为 $p(w) = \\mathcal{N}(0, \\alpha^{-1} I_d)$，其中 $\\alpha > 0$ 是先验精度， $I_d$ 是 $d \\times d$ 的单位矩阵。给定设计矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 和目标值 $y \\in \\mathbb{R}^N$，似然函数为 $p(y \\mid X, w) = \\mathcal{N}(X w, \\sigma^2 I_N)$，其中 $\\sigma^2 > 0$ 是已知的观测噪声方差。对于一个测试输入 $x_* \\in \\mathbb{R}^d$，BLR 的后验分布会导出 $y_*$ 的高斯预测分布，其均值和方差由 $w$ 的后验分布确定。预测方差可以加性分解为认知分量和偶然分量。\n\n从贝叶斯定理、高斯先验和似然以及线性回归的基本定义出发，推导 $w$ 的后验分布为何是高斯分布，确定后验协方差，并证明在 $x_*$ 处的预测方差可以写成以下形式\n$$\n\\mathrm{Var}(y_* \\mid x_*, X, y) \\;=\\; \\underbrace{x_*^\\top S_N x_*}_{\\text{认知}} \\;+\\; \\underbrace{\\sigma^2}_{\\text{偶然}},\n$$\n其中 $S_N \\in \\mathbb{R}^{d \\times d}$ 是 $w$ 的后验协方差。\n\n现在，引入受 $\\ell_2$ 范数约束 $\\|\\delta\\|_2 \\le \\epsilon$（对于给定的 $\\epsilon \\ge 0$）的输入扰动 $\\delta \\in \\mathbb{R}^d$。定义两种类型的扰动：\n- 对抗性扰动：选择 $\\delta$ 以在约束 $\\|\\delta\\|_2 \\le \\epsilon$ 下最大化认知分量 $(x_* + \\delta)^\\top S_N (x_* + \\delta)$。\n- 自然扰动：将 $\\delta$ 建模为 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$（其中 $\\eta^2 \\ge 0$），并考虑期望的认知分量 $\\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)]$。\n\n您的任务是实现一个程序，对每个提供的测试用例计算以下内容：\n1. 从 $X$、$\\alpha$ 和 $\\sigma^2$ 计算 BLR 后验协方差 $S_N$。\n2. 在 $x_*$ 处的基线预测方差及其认知和偶然部分。\n3. 最大对抗性认知方差 $(x_* + \\delta_{\\text{adv}})^\\top S_N (x_* + \\delta_{\\text{adv}})$，其中 $\\delta_{\\text{adv}}$ 是 $\\ell_2$ 约束最大化问题的解。\n4. 期望的自然认知方差 $\\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)]$，其中 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$。\n5. 一个布尔值，指示对抗性扰动所增加的总预测方差是否大于自然扰动下的期望增量。\n6. 一个布尔值，指示在两种扰动下偶然分量是否保持不变（在 $10^{-12}$ 的数值容差范围内）。\n\n使用以下参数值的测试套件，所有数值均已指定：\n\n- 测试用例 1（正常路径，中等先验强度和小扰动）：\n  - $X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$, $x_* = \\begin{bmatrix} 1.0 \\\\ 0.5 \\end{bmatrix}$。\n  - $\\alpha = 1.0$，$\\sigma^2 = 0.04$，$\\epsilon = 0.2$，$\\eta^2 = 0.01$。\n\n- 测试用例 2（边界条件，对抗性半径为零）：\n  - $X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 3.0 \\end{bmatrix}$, $x_* = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$。\n  - $\\alpha = 1.0$，$\\sigma^2 = 0.04$，$\\epsilon = 0.0$，$\\eta^2 = 0.0025$。\n\n- 测试用例 3（边缘情况，数据少且先验弱，导致较高的认知方差）：\n  - $X = \\begin{bmatrix} 1  -2 \\\\ 1  2 \\end{bmatrix}$, $y = \\begin{bmatrix} -1.0 \\\\ 1.2 \\end{bmatrix}$, $x_* = \\begin{bmatrix} 1.0 \\\\ 3.0 \\end{bmatrix}$。\n  - $\\alpha = 0.1$，$\\sigma^2 = 0.25$，$\\epsilon = 0.5$，$\\eta^2 = 0.0$。\n\n对于每个测试用例，设基线认知方差为 $E_{\\text{base}} = x_*^\\top S_N x_*$，基线偶然方差为 $A_{\\text{base}} = \\sigma^2$。设 $E_{\\text{adv}} = (x_* + \\delta_{\\text{adv}})^\\top S_N (x_* + \\delta_{\\text{adv}})$ 和 $E_{\\text{nat}} = \\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)]$。对于自然扰动 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$，使用恒等式\n$$\n\\mathbb{E}\\big[(x_* + \\delta)^\\top S_N (x_* + \\delta)\\big] \\;=\\; x_*^\\top S_N x_* \\;+\\; \\eta^2 \\operatorname{tr}(S_N).\n$$\n\n如果对抗性扰动导致的总预测方差增量至少与期望的自然增量一样大，则将布尔值 $b_1$ 定义为真，即：\n$$\nb_1 \\;=\\; \\big( (E_{\\text{adv}} - E_{\\text{base}}) \\;\\ge\\; (\\eta^2 \\operatorname{tr}(S_N)) \\big).\n$$\n如果偶然分量在两种扰动下保持不变（在 $10^{-12}$ 的容差范围内），则将布尔值 $b_2$ 定义为真，即：\n$$\nb_2 \\;=\\; \\big( |A_{\\text{base}} - \\sigma^2|  10^{-12} \\big) \\;\\wedge\\; \\big( |A_{\\text{base}} - \\sigma^2|  10^{-12} \\big),\n$$\n这会检查偶然项在两种扰动下的不变性。\n\n您的程序应生成单行输出，其中包含三个测试用例的六个布尔值，按 $[b_{1,1}, b_{2,1}, b_{1,2}, b_{2,2}, b_{1,3}, b_{2,3}]$ 的顺序排列，形式为用方括号括起来的逗号分隔列表（例如， $[{\\tt True},{\\tt False},\\dots]$）。不应打印任何其他文本。", "solution": "用户提供的问题是有效的。它在科学上基于贝叶斯统计和线性代数，问题提出得很好，有明确的目标和充足的数据，并且表述客观。我们将进行完整的解答。\n\n### 1. 理论推导\n\n#### 1.1. 权重后验分布\n\n我们从贝叶斯线性回归开始。模型参数是权重 $w \\in \\mathbb{R}^d$。\n权重的先验分布是零均值高斯分布：\n$$p(w) = \\mathcal{N}(w \\mid 0, \\alpha^{-1} I_d) \\propto \\exp\\left(-\\frac{\\alpha}{2} w^\\top w\\right)$$\n其中 $\\alpha  0$ 是先验精度， $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n给定一个包含设计矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 和目标值 $y \\in \\mathbb{R}^N$ 的训练数据集，数据的似然也是高斯分布：\n$$p(y \\mid X, w) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_N) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw)\\right)$$\n其中 $\\sigma^2  0$ 是已知的观测噪声方差。\n\n为了找到后验分布 $p(w \\mid X, y)$，我们应用贝叶斯定理：\n$$p(w \\mid X, y) \\propto p(y \\mid X, w) p(w)$$\n代入似然和先验的表达式：\n$$p(w \\mid X, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top(y - Xw) - \\frac{\\alpha}{2} w^\\top w\\right)$$\n后验分布与一个关于 $w$ 的函数的指数成正比。为了确定该分布，我们检查指数的参数，重点关注涉及 $w$ 的项：\n$$L(w) = -\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top Xw) - \\frac{\\alpha}{2} w^\\top w$$\n$$L(w) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma^2} w^\\top X^\\top Xw - \\frac{2}{\\sigma^2} y^\\top Xw + \\alpha w^\\top I_d w \\right) + \\text{const.}$$\n$$L(w) = -\\frac{1}{2} \\left( w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d\\right) w - 2 \\left(\\frac{1}{\\sigma^2} X^\\top y\\right)^\\top w \\right) + \\text{const.}$$\n这是一个关于 $w$ 的二次型。高斯分布 $\\mathcal{N}(w \\mid m, S)$ 的对数概率密度形式为 $-\\frac{1}{2}(w-m)^\\top S^{-1}(w-m) + \\text{const} = -\\frac{1}{2}(w^\\top S^{-1}w - 2m^\\top S^{-1}w) + \\text{const}$。\n通过比较 $w$ 中的二次项和线性项，我们可以确定后验分布的逆协方差（精度矩阵）$S_N^{-1}$ 和均值 $m_N$。\n\n二次项给出了后验精度矩阵：\n$$S_N^{-1} = \\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d$$\n因此，后验协方差矩阵是：\n$$S_N = \\left(\\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d\\right)^{-1}$$\n线性项给出了均值：$m_N^\\top S_N^{-1} = (\\frac{1}{\\sigma^2} X^\\top y)^\\top$，这导致 $m_N = S_N \\left(\\frac{1}{\\sigma^2} X^\\top y\\right)$。\n因此，权重的后验分布是一个高斯分布：\n$$p(w \\mid X, y) = \\mathcal{N}(w \\mid m_N, S_N)$$\n\n#### 1.2. 预测分布和方差分解\n\n对于一个新的测试输入 $x_* \\in \\mathbb{R}^d$，我们希望找到相应输出 $y_*$ 的预测分布。这可以通过对权重的后验进行边缘化得到：\n$$p(y_* \\mid x_*, X, y) = \\int p(y_* \\mid x_*, w) p(w \\mid X, y) dw$$\n这里，$p(y_* \\mid x_*, w) = \\mathcal{N}(y_* \\mid x_*^\\top w, \\sigma^2)$ 是测试点的似然，而 $p(w \\mid X, y) = \\mathcal{N}(w \\mid m_N, S_N)$ 是后验分布。这是一个两个高斯分布乘积的积分，其结果是一个高斯预测分布。\n\n我们感兴趣的是这个预测分布的方差，$\\mathrm{Var}(y_* \\mid x_*, X, y)$。我们使用全方差定律：\n$$\\mathrm{Var}(y_*) = \\mathbb{E}_{w}[\\mathrm{Var}(y_* \\mid w)] + \\mathrm{Var}_{w}[\\mathbb{E}(y_* \\mid w)]$$\n1. **偶然不确定性**：第一项是观测噪声方差在 $w$ 的后验上的期望。\n    $$\\mathbb{E}_{w}[\\mathrm{Var}(y_* \\mid x_*, w)] = \\mathbb{E}_{w}[\\sigma^2] = \\sigma^2$$\n    这是偶然不确定性。它是不可约减的，代表了数据生成过程中的内在随机性。它不依赖于数据的数量或质量。\n\n2. **认知不确定性**：第二项是由于参数 $w$ 的不确定性而导致模型预测的方差。\n    $$\\mathrm{Var}_{w}[\\mathbb{E}(y_* \\mid x_*, w)] = \\mathrm{Var}_{w}[x_*^\\top w]$$\n    由于 $w \\sim \\mathcal{N}(m_N, S_N)$，线性变换 $x_*^\\top w$ 产生一个标量随机变量。其方差由下式给出：\n    $$\\mathrm{Var}(x_*^\\top w) = x_*^\\top \\mathrm{Cov}(w) x_* = x_*^\\top S_N x_*$$\n    这是认知不确定性。它代表了模型关于参数 $w$ 的不确定性，可以通过观测更多数据来减少，因为这会使后验协方差 $S_N$ 缩小。\n\n将这两个分量相加，总预测方差为：\n$$\\mathrm{Var}(y_* \\mid x_*, X, y) = \\underbrace{x_*^\\top S_N x_*}_{\\text{认知}} + \\underbrace{\\sigma^2}_{\\text{偶然}}$$\n这与问题陈述中指定的形式相符。\n\n#### 1.3. 输入扰动\n\n我们分析输入 $x_*$ 的两种类型的扰动 $\\delta$。\n\n1. **对抗性扰动**：我们寻求找到 $\\delta_{adv}$，它在 $\\ell_2$ 范数约束 $\\|\\delta\\|_2 \\le \\epsilon$ 下最大化扰动后输入 $x_* + \\delta$ 的认知方差分量。目标是最大化 $f(\\delta) = (x_* + \\delta)^\\top S_N (x_* + \\delta)$。由于 $S_N$ 是正定的，$f(\\delta)$ 是一个凸函数。凸函数在紧凸集（$\\ell_2$ 球）上的最大值必须位于集合的边界上，因此 $\\|\\delta_{adv}\\|_2 = \\epsilon$（除非 $\\epsilon=0$）。找到精确解需要解决一个类似信赖域的问题。在对抗性样本的背景下，一个标准且计算上可行的近似方法是从 $\\delta=0$ 开始进行单步梯度上升。目标函数关于 $\\delta$ 的梯度是 $\\nabla_\\delta f(\\delta) = 2 S_N (x_* + \\delta)$。在 $\\delta = 0$ 处，梯度为 $2 S_N x_*$。因此，上升方向是 $S_N x_*$。为满足约束，我们将此方向缩放至长度为 $\\epsilon$：\n    $$\\delta_{adv} = \\epsilon \\frac{S_N x_*}{\\|S_N x_*\\|_2}$$\n    此解释用于计算。如果 $\\|S_N x_*\\|_2=0$ 或 $\\epsilon=0$，则 $\\delta_{adv}=0$。最大对抗性认知方差则为 $E_{\\text{adv}} = (x_* + \\delta_{adv})^\\top S_N (x_* + \\delta_{adv})$。\n\n2. **自然扰动**：扰动 $\\delta$ 被建模为一个随机变量 $\\delta \\sim \\mathcal{N}(0, \\eta^2 I_d)$。我们关心的是期望认知方差。如问题推导中所给出并验证的，这为：\n    $$E_{\\text{nat}} = \\mathbb{E}[(x_* + \\delta)^\\top S_N (x_* + \\delta)] = x_*^\\top S_N x_* + \\eta^2 \\operatorname{tr}(S_N)$$\n    其中 $\\operatorname{tr}(S_N)$ 是后验协方差矩阵的迹。\n\n#### 1.4. 布尔值计算\n\n问题要求为每个测试用例计算两个布尔值。\n-   $b_1 = \\big( (E_{\\text{adv}} - E_{\\text{base}}) \\ge (\\eta^2 \\operatorname{tr}(S_N)) \\big)$：此项比较由对抗性扰动引起的认知方差增量 $E_{\\text{adv}} - E_{\\text{base}}$ 与自然扰动引起的期望增量 $E_{\\text{nat}} - E_{\\text{base}} = \\eta^2 \\operatorname{tr}(S_N)$。\n-   $b_2 = \\big( |A_{\\text{base}} - \\sigma^2|  10^{-12} \\big) \\wedge \\big( |A_{\\text{base}} - \\sigma^2|  10^{-12} \\big)$：此项检查偶然分量是否保持不变。根据定义，偶然方差为 $\\sigma^2$ 且独立于输入 $x_*$。因此，$A_{\\text{base}} = \\sigma^2$，并且扰动输入不会改变该分量。该条件简化为 $| \\sigma^2 - \\sigma^2 |  10^{-12}$，即 $0  10^{-12}$。这总是成立的，因此对于所有有效的测试用例，$b_2$ 都将为真。\n\n### 2. 实现策略\n\n对于每个测试用例，程序将执行以下步骤：\n1. 定义矩阵和标量 $X, y, x_*, \\alpha, \\sigma^2, \\epsilon, \\eta^2$。\n2. 计算后验协方差矩阵 $S_N = \\left(\\frac{1}{\\sigma^2} X^\\top X + \\alpha I_d\\right)^{-1}$。\n3. 计算基线认知方差 $E_{\\text{base}} = x_*^\\top S_N x_*$ 和偶然方差 $A_{\\text{base}} = \\sigma^2$。\n4. 计算对抗性扰动 $\\delta_{adv} = \\epsilon \\frac{S_N x_*}{\\|S_N x_*\\|_2}$（处理零范数情况）以及由此产生的对抗性认知方差 $E_{\\text{adv}} = (x_* + \\delta_{adv})^\\top S_N (x_* + \\delta_{adv})$。\n5. 确定自然扰动引起的方差增量为 $\\eta^2 \\operatorname{tr}(S_N)$。\n6. 评估布尔条件 $b_1$ 和 $b_2$ 并存储结果。\n7. 将最终的布尔值集合格式化为指定的输出字符串。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian Linear Regression uncertainty quantification problem for given test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            \"X\": np.array([[1, -1], [1, 0], [1, 1], [1, 2]], dtype=float),\n            \"y\": np.array([[0.0], [1.0], [2.0], [3.0]], dtype=float),\n            \"x_star\": np.array([1.0, 0.5], dtype=float),\n            \"alpha\": 1.0,\n            \"sigma2\": 0.04,\n            \"epsilon\": 0.2,\n            \"eta2\": 0.01,\n        },\n        # Test case 2\n        {\n            \"X\": np.array([[1, -1], [1, 0], [1, 1], [1, 2]], dtype=float),\n            \"y\": np.array([[0.0], [1.0], [2.0], [3.0]], dtype=float),\n            \"x_star\": np.array([1.0, -1.0], dtype=float),\n            \"alpha\": 1.0,\n            \"sigma2\": 0.04,\n            \"epsilon\": 0.0,\n            \"eta2\": 0.0025,\n        },\n        # Test case 3\n        {\n            \"X\": np.array([[1, -2], [1, 2]], dtype=float),\n            \"y\": np.array([[-1.0], [1.2]], dtype=float),\n            \"x_star\": np.array([1.0, 3.0], dtype=float),\n            \"alpha\": 0.1,\n            \"sigma2\": 0.25,\n            \"epsilon\": 0.5,\n            \"eta2\": 0.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        x_star = case[\"x_star\"]\n        alpha = case[\"alpha\"]\n        sigma2 = case[\"sigma2\"]\n        epsilon = case[\"epsilon\"]\n        eta2 = case[\"eta2\"]\n\n        d = X.shape[1]\n        I_d = np.identity(d)\n\n        # 1. Compute the BLR posterior covariance SN\n        precision_matrix = (1 / sigma2) * (X.T @ X) + alpha * I_d\n        SN = np.linalg.inv(precision_matrix)\n\n        # 2. Compute baseline predictive variance components\n        E_base = x_star.T @ SN @ x_star\n        A_base = sigma2\n\n        # 3. Compute maximal adversarial epistemic variance\n        grad_term = SN @ x_star\n        norm_grad_term = np.linalg.norm(grad_term)\n        \n        # Handle cases where gradient is zero or epsilon is zero\n        if norm_grad_term  1e-15 or epsilon == 0.0:\n            delta_adv = np.zeros_like(x_star)\n        else:\n            delta_adv = epsilon * grad_term / norm_grad_term\n        \n        x_adv = x_star + delta_adv\n        E_adv = x_adv.T @ SN @ x_adv\n        \n        adv_increase = E_adv - E_base\n\n        # 4. Compute expected increase from natural perturbations\n        tr_SN = np.trace(SN)\n        nat_increase = eta2 * tr_SN\n\n        # 5. Compute boolean b1\n        b1 = adv_increase >= nat_increase\n\n        # 6. Compute boolean b2\n        # The aleatoric variance is sigma2 by definition and is independent of the input x*.\n        # Therefore, it remains unchanged under any perturbation to x*.\n        # The check simplifies to abs(sigma2 - sigma2)  tol, which is always true.\n        b2 = True \n\n        results.extend([b1, b2])\n\n    # Format the output as specified\n    print(f\"[{','.join(map(lambda b: 'True' if b else 'False', results))}]\")\n\nsolve()\n\n```", "id": "3197111"}]}