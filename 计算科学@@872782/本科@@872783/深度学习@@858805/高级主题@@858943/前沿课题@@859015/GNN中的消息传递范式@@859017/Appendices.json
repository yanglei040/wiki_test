{"hands_on_practices": [{"introduction": "消息传递的核心在于聚合邻居节点的信息，但一个关键问题是如何平衡来自不同度数节点的消息。直接求和会导致高度节点信号过大，而简单平均则可能削弱重要信息。本练习 [@problem_id:3189832] 将引导你从几个基本公理出发，推导出图卷积网络（GCN）中广泛使用的对称归一化因子，让你亲身体会这一重要设计背后的数学原理。", "problem": "给定一个具有节点度数和标量节点特征的无向简单图。在图神经网络（GNNs）的消息传递框架中，考虑一个线性的、度感知的归一化器，它沿着边 $u \\leftarrow v$ 对每个传入消息进行重新缩放，缩放因子仅取决于度数 $d_u$ 和 $d_v$。设未归一化的标量消息为 $m_{uv}$，归一化的消息为 $\\tilde{m}_{uv}$。节点 $u$ 处的聚合信号为 $s_u = \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}$，其中 $\\mathcal{N}(u)$ 是 $u$ 的邻居节点集合。\n\n从以下关于无向图的基本且被广泛接受的约束条件出发：\n\n- 归一化因子是仅与度数相关的标量函数 $\\gamma$，因此 $\\tilde{m}_{uv} = \\gamma(d_u, d_v)\\, m_{uv}$。\n- 无向图的对称性要求：$\\gamma(d_u, d_v) = \\gamma(d_v, d_u)$。\n- 乘法可分性：存在一个正函数 $\\alpha$，使得 $\\gamma(d_u, d_v) = \\alpha(d_u)\\,\\alpha(d_v)$。\n- 在 $d$-正则图上的常数信号保持：如果对所有节点 $x_u = c$ 且 $m_{uv} = x_v$，则对于任何 $d \\geq 1$，聚合信号对所有 $u$ 都满足 $s_u = c$。\n\n推导出满足这些约束的唯一正函数 $\\gamma(d_u, d_v)$。然后，使用该结果计算下文所述的评估值。在整个问题中，假设度数为正整数，且所有图都是无向的。\n\n对于本问题中的所有计算，使用标量特征，其中每个节点 $v$ 的特征为 $x_v = 1$。定义边上的未归一化消息为 $m_{uv} = w_{uv}\\, x_v$，其中 $w_{uv}$ 是给定的非负标量权重。在同构图中，所有边的 $w_{uv} = 1$；在异构图中，$w_{uv}$ 可能因边类型而异。沿边的衰减因子定义为比率\n$$\nA_{u \\leftarrow v} \\equiv \\frac{\\tilde{m}_{uv}}{m_{uv}},\n$$\n节点 $u$ 处的节点级总衰减定义为\n$$\nR_u \\equiv \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} m_{uv}} \\quad \\text{当分母不为零时。}\n$$\n\n测试套件和要求输出：\n\n使用您推导出的归一化方法计算以下三个量。每个要求的输出都是一个实数。\n\n1) 星形图，低度节点从中心节点接收消息时的边级效应：\n- 图：一个包含 $n = 6$ 个节点的星形图，其中一个中心节点的度为 $d_{\\text{center}} = 5$，五个叶节点的度为 $1$。\n- 同构边：所有边的 $w_{uv} = 1$。\n- 数量：在任意一条 $u$ 为叶节点、$v$ 为中心节点的边上的衰减因子 $A_{\\text{leaf} \\leftarrow \\text{center}}$。\n\n2) 星形图，对中心节点的节点级总效应：\n- 与情况1相同的星形图，具有同构边 $w_{uv} = 1$。\n- 数量：中心节点处的节点级总衰减 $R_{\\text{center}}$。\n\n3) 异构小图，节点从混合度数和类型的邻居接收消息时的节点级总效应：\n- 考虑一个度为 $d_u = 3$ 的节点 $u$，它连接到三个度分别为 $d_{v_1} = 5$，$d_{v_2} = 2$，$d_{v_3} = 2$ 的邻居 $v_1, v_2, v_3$。\n- 异构边权重： $w_{u v_1} = 2.0$, $w_{u v_2} = 0.5$, $w_{u v_3} = 1.5$。\n- 数量：节点级总衰减 $R_u$。\n\n最终输出格式：\n\n您的程序应生成一行输出，其中包含三个结果，以逗号分隔并用方括号括起来（例如 $[r_1,r_2,r_3]$），其中 $r_1$、$r_2$ 和 $r_3$ 分别对应于情况1、2和3，每个结果都四舍五入到恰好6位小数。", "solution": "该问题要求基于四个基本约束，为图神经网络中的消息传递推导出一个唯一的、正的、对称的且可分离的归一化函数 $\\gamma(d_u, d_v)$。随后，必须使用该函数计算三个指定的量。\n\n第一部分：归一化函数 $\\gamma(d_u, d_v)$ 的推导\n\n推导过程通过系统地应用给定的约束来进行。\n\n1.  **归一化定义**：归一化消息 $\\tilde{m}_{uv}$ 与未归一化消息 $m_{uv}$ 通过一个因子 $\\gamma$ 相关联，该因子取决于源节点和目标节点的度数 $d_v$ 和 $d_u$。\n    $$\n    \\tilde{m}_{uv} = \\gamma(d_u, d_v) m_{uv}\n    $$\n\n2.  **对称性**：对于无向图，归一化函数必须对其参数对称。\n    $$\n    \\gamma(d_u, d_v) = \\gamma(d_v, d_u)\n    $$\n\n3.  **乘法可分性**：函数 $\\gamma$ 可以表示为在每个度数上求值的正函数 $\\alpha$ 的乘积。\n    $$\n    \\gamma(d_u, d_v) = \\alpha(d_u) \\alpha(d_v) \\quad \\text{其中 } \\alpha(d) > 0 \\text{ 对于 } d \\ge 1\n    $$\n    这种形式自动满足对称性约束，因为 $\\alpha(d_u) \\alpha(d_v) = \\alpha(d_v) \\alpha(d_u)$。\n\n4.  **常数信号保持**：在任何 $d$-正则图（其中所有节点的度数 $d_u=d$，对于某个 $d \\ge 1$）上，如果输入特征是常数（对所有 $u$，$x_u = c$）并且未归一化消息取为源节点的特征（$m_{uv} = x_v$），则在任何节点 $u$ 处的聚合信号必须等于该常数 $c$。\n    $$\n    s_u = \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv} = c\n    $$\n    我们来展开这个条件。对于一个 $d$-正则图，任何节点 $u$ 都有 $|\\mathcal{N}(u)| = d_u = d$ 个邻居。对于任何边 $(u, v)$，两个节点的度数都为 $d$，因此 $d_u = d_v = d$。未归一化的消息是 $m_{uv} = x_v = c$。\n    将这些代入聚合公式：\n    $$\n    s_u = \\sum_{v \\in \\mathcal{N}(u)} \\gamma(d_u, d_v) m_{uv} = \\sum_{v \\in \\mathcal{N}(u)} \\gamma(d, d) c\n    $$\n    由于求和是对 $u$ 的 $d$ 个邻居进行的，并且项 $\\gamma(d, d) c$对所有邻居都是常数：\n    $$\n    s_u = d \\cdot \\gamma(d, d) \\cdot c\n    $$\n    约束要求 $s_u = c$。因此，对于任何 $c \\ne 0$：\n    $$\n    d \\cdot \\gamma(d, d) \\cdot c = c \\implies d \\cdot \\gamma(d, d) = 1\n    $$\n    这必须对任何整数度 $d \\ge 1$ 成立。\n\n现在，我们将此结果与乘法可分性约束相结合。根据约束3，我们有 $\\gamma(d, d) = \\alpha(d) \\alpha(d) = (\\alpha(d))^2$。将此代入我们推导出的方程：\n$$\nd \\cdot (\\alpha(d))^2 = 1\n$$\n求解 $\\alpha(d)$：\n$$\n(\\alpha(d))^2 = \\frac{1}{d}\n$$\n由于 $\\alpha$ 是一个正函数，我们取正平方根：\n$$\n\\alpha(d) = \\sqrt{\\frac{1}{d}} = \\frac{1}{\\sqrt{d}} = d^{-1/2}\n$$\n找到了唯一的正函数 $\\alpha(d)$ 后，我们现在可以构建唯一的归一化函数 $\\gamma(d_u, d_v)$：\n$$\n\\gamma(d_u, d_v) = \\alpha(d_u) \\alpha(d_v) = (d_u^{-1/2}) (d_v^{-1/2}) = (d_u d_v)^{-1/2} = \\frac{1}{\\sqrt{d_u d_v}}\n$$\n这就是图卷积网络（GCNs）中著名的对称归一化方法。\n\n第二部分：计算所要求的量\n\n我们使用推导出的归一化函数 $\\gamma(d_u, d_v) = 1 / \\sqrt{d_u d_v}$ 来计算这三个量。问题指定所有节点的特征为 $x_v = 1$，未归一化的消息为 $m_{uv} = w_{uv} x_v = w_{uv}$。\n\n这些量的定义是：\n-   衰减因子：$A_{u \\leftarrow v} = \\frac{\\tilde{m}_{uv}}{m_{uv}} = \\frac{\\gamma(d_u, d_v) m_{uv}}{m_{uv}} = \\gamma(d_u, d_v)$。\n-   节点级总衰减：$R_u = \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} m_{uv}} = \\frac{\\sum_{v \\in \\mathcal{N}(u)} \\gamma(d_u, d_v) w_{uv}}{\\sum_{v \\in \\mathcal{N}(u)} w_{uv}}$。\n\n**情况1：星形图，边级效应**\n-   图：一个星形图，一个中心节点和5个叶节点。\n-   度数：$d_{\\text{center}} = 5$，$d_{\\text{leaf}} = 1$。\n-   数量：衰减因子 $A_{\\text{leaf} \\leftarrow \\text{center}}$。\n-   这里，接收节点 $u$ 是一个叶节点（$d_u = 1$），发送节点 $v$ 是中心节点（$d_v = 5$）。\n-   计算过程如下：\n    $$\n    A_{\\text{leaf} \\leftarrow \\text{center}} = \\gamma(d_{\\text{leaf}}, d_{\\text{center}}) = \\gamma(1, 5) = \\frac{1}{\\sqrt{1 \\cdot 5}} = \\frac{1}{\\sqrt{5}} \\approx 0.4472136\n    $$\n\n**情况2：星形图，节点级效应**\n-   图：与情况1相同的星形图，其中 $d_{\\text{center}} = 5$ 和5个度为 $d_{\\text{leaf}} = 1$ 的邻居。\n-   边：同构边，所以所有边的 $w_{uv} = 1$。\n-   数量：中心节点处的节点级总衰减 $R_{\\text{center}}$。\n-   接收节点是中心节点，所以 $u = \\text{center}$ 且 $d_u = 5$。邻居节点 $v_i$ 是5个叶节点，所以 $d_{v_i} = 1$。\n-   $R_u$ 的公式变为：\n    $$\n    R_{\\text{center}} = \\frac{\\sum_{i=1}^5 \\gamma(d_{\\text{center}}, d_{\\text{leaf}}) \\cdot w_{uv_i}}{\\sum_{i=1}^5 w_{uv_i}} = \\frac{\\sum_{i=1}^5 \\gamma(5, 1) \\cdot 1}{\\sum_{i=1}^5 1} = \\frac{5 \\cdot \\gamma(5, 1)}{5} = \\gamma(5, 1)\n    $$\n-   计算过程如下：\n    $$\n    R_{\\text{center}} = \\frac{1}{\\sqrt{5 \\cdot 1}} = \\frac{1}{\\sqrt{5}} \\approx 0.4472136\n    $$\n\n**情况3：异构小图**\n-   图：一个度为 $d_u = 3$ 的节点 $u$ 与三个邻居 $v_1, v_2, v_3$ 相连。\n-   邻居度数：$d_{v_1} = 5$, $d_{v_2} = 2$, $d_{v_3} = 2$。\n-   边权重：$w_{uv_1} = 2.0$, $w_{uv_2} = 0.5$, $w_{uv_3} = 1.5$。\n-   数量：节点级总衰减 $R_u$。\n-   我们分别计算 $R_u$ 公式的分子和分母。\n-   分母（未归一化消息之和）：\n    $$\n    \\sum_{v \\in \\mathcal{N}(u)} m_{uv} = w_{uv_1} + w_{uv_2} + w_{uv_3} = 2.0 + 0.5 + 1.5 = 4.0\n    $$\n-   分子（归一化消息之和）：\n    $$\n    \\sum_{v \\in \\mathcal{N}(u)} \\tilde{m}_{uv} = \\gamma(d_u, d_{v_1}) w_{uv_1} + \\gamma(d_u, d_{v_2}) w_{uv_2} + \\gamma(d_u, d_{v_3}) w_{uv_3}\n    $$\n    $$\n    = \\gamma(3, 5) \\cdot 2.0 + \\gamma(3, 2) \\cdot 0.5 + \\gamma(3, 2) \\cdot 1.5\n    $$\n    $$\n    = \\frac{1}{\\sqrt{3 \\cdot 5}} \\cdot 2.0 + \\frac{1}{\\sqrt{3 \\cdot 2}} \\cdot 0.5 + \\frac{1}{\\sqrt{3 \\cdot 2}} \\cdot 1.5\n    $$\n    $$\n    = \\frac{2.0}{\\sqrt{15}} + \\frac{0.5}{\\sqrt{6}} + \\frac{1.5}{\\sqrt{6}} = \\frac{2.0}{\\sqrt{15}} + \\frac{2.0}{\\sqrt{6}}\n    $$\n-   将它们合并以求得 $R_u$：\n    $$\n    R_u = \\frac{\\frac{2.0}{\\sqrt{15}} + \\frac{2.0}{\\sqrt{6}}}{4.0} = \\frac{1}{2.0} \\left( \\frac{1}{\\sqrt{15}} + \\frac{1}{\\sqrt{6}} \\right) \\approx 0.5 \\cdot (0.2581989 + 0.4082483) \\approx 0.3332236\n    $$\n\n最终的数值结果，四舍五入到6位小数，是：\n-   情况1: $0.447214$\n-   情况2: $0.447214$\n-   情况3: $0.333224$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a GNN normalization factor to solve three test cases.\n\n    The normalization factor is derived from first principles to be\n    gamma(du, dv) = 1 / sqrt(du * dv). This function is then used\n    to compute attenuation factors as requested.\n    \"\"\"\n\n    def gamma(du, dv):\n        \"\"\"Computes the symmetric normalization factor.\"\"\"\n        if du == 0 or dv == 0:\n            raise ValueError(\"Degrees must be positive integers.\")\n        return 1.0 / np.sqrt(du * dv)\n\n    # Test Case 1: Star graph, edge-level effect\n    # Attenuation factor A_{leaf - center}\n    # u = leaf, v = center\n    d_leaf_1 = 1\n    d_center_1 = 5\n    r1 = gamma(d_leaf_1, d_center_1)\n\n    # Test Case 2: Star graph, node-level total effect\n    # Total attenuation R_center at the hub\n    # u = center, neighbors v_i are leaves\n    # Since edge weights are homogeneous (w_uv=1), the total attenuation\n    # reduces to the per-edge attenuation factor.\n    # R_center = (sum_i gamma(d_u, d_vi) * 1) / (sum_i 1)\n    #          = (N * gamma(d_u, d_vi)) / N = gamma(d_u, d_vi)\n    d_center_2 = 5\n    d_leaf_2 = 1\n    r2 = gamma(d_center_2, d_leaf_2)\n\n    # Test Case 3: Heterogeneous small graph, node-level total effect\n    # Total attenuation R_u for a node u with diverse neighbors.\n    # R_u = (sum_v tilde_m_uv) / (sum_v m_uv)\n    # m_uv = w_uv * x_v, with x_v=1\n    d_u_3 = 3\n    neighbors = [\n        {'d_v': 5, 'w_uv': 2.0},\n        {'d_v': 2, 'w_uv': 0.5},\n        {'d_v': 2, 'w_uv': 1.5},\n    ]\n\n    sum_m_uv = 0.0\n    sum_tilde_m_uv = 0.0\n\n    for neighbor in neighbors:\n        d_v = neighbor['d_v']\n        w_uv = neighbor['w_uv']\n        \n        # Unnormalized message m_uv = w_uv * x_v = w_uv (since x_v=1)\n        m_uv = w_uv\n        sum_m_uv += m_uv\n\n        # Normalized message tilde_m_uv = gamma(d_u, d_v) * m_uv\n        tilde_m_uv = gamma(d_u_3, d_v) * m_uv\n        sum_tilde_m_uv += tilde_m_uv\n\n    if sum_m_uv == 0:\n        r3 = 0.0 # As per problem statement, denominator is non-zero\n    else:\n        r3 = sum_tilde_m_uv / sum_m_uv\n\n    results = [r1, r2, r3]\n    \n    # Format the output rounded to exactly 6 decimal places.\n    # The f-string format `{r:.6f}` handles the rounding.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3189832"}, {"introduction": "在推导了对称归一化之后，一个自然的问题是：不同的归一化方案会如何影响模型的行为？此练习 [@problem_id:3189858] 通过一个星形图的特例，让你直观地比较对称归一化与行归一化（随机游走归一化）的差异。你将分析“中心节点漂移”现象，从而理解不同归一化方法如何调节图中信息流，尤其是在处理高度不均匀的节点度分布时。", "problem": "考虑一个无向简单图，其邻接矩阵为 $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，度矩阵为 $\\mathbf{D} = \\mathrm{diag}(d_{1}, \\dots, d_{n})$，其中 $d_{i} = \\sum_{j=1}^{n} \\mathbf{A}_{ij}$。在图神经网络（GNN）的消息传递范式中，一层的线性聚合器可以写成 $\\mathbf{H}^{(1)} = \\hat{\\mathbf{A}} \\, \\mathbf{H}^{(0)}$，其中 $\\mathbf{H}^{(0)} \\in \\mathbb{R}^{n \\times p}$ 是节点特征，而 $\\hat{\\mathbf{A}}$ 是一个归一化的邻接矩阵。两种常见的选择是行归一化（随机游走）矩阵 $\\hat{\\mathbf{A}} = \\mathbf{D}^{-1} \\mathbf{A}$ 和对称归一化矩阵 $\\hat{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$。沿中心节点的特征漂移（Feature drift along hubs）指的是节点（特别是低度节点）的特征在跨越消息传递层时，趋向于被拉向高度节点的特征的现象。\n\n为了分析中心节点引发的漂移（hub-induced drift），考虑一个包含 $k+1$ 个节点的星形图：一个度为 $d_{c} = k$ 的中心节点 $c$ 和 $k$ 个叶节点 $\\ell \\in \\{1, \\dots, k\\}$，每个叶节点的度为 $d_{\\ell} = 1$。假设初始特征是标量（即 $p = 1$），中心节点的特征表示为 $h^{(0)}_{c} \\in \\mathbb{R}$，叶节点 $\\ell$ 的特征表示为 $h^{(0)}_{\\ell} \\in \\mathbb{R}$。除非明确说明，否则假设不添加自环。基于线性聚合的基本原理和上述定义，下面关于两种归一化对沿中心节点的特征漂移影响的陈述中，哪些是正确的？\n\nA. 在 $\\hat{\\mathbf{A}} = \\mathbf{D}^{-1} \\mathbf{A}$ 的情况下，经过一次聚合后，每个叶节点满足 $h^{(1)}_{\\ell} = h^{(0)}_{c}$，中心节点满足 $h^{(1)}_{c} = \\frac{1}{k} \\sum_{\\ell=1}^{k} h^{(0)}_{\\ell}$。这表明 $\\mathbf{D}^{-1} \\mathbf{A}$ 比 $\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 具有更强的中心节点引发的漂移。\n\nB. 在 $\\hat{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 的情况下，经过一次聚合后，每个叶节点满足 $h^{(1)}_{\\ell} = \\frac{1}{\\sqrt{k}} \\, h^{(0)}_{c}$，中心节点满足 $h^{(1)}_{c} = \\frac{1}{\\sqrt{k}} \\sum_{\\ell=1}^{k} h^{(0)}_{\\ell}$，因此中心节点对叶节点的影响被一个 $1/\\sqrt{k}$ 的因子衰减，导致向中心节点的漂移比使用 $\\mathbf{D}^{-1} \\mathbf{A}$ 时更慢。\n\nC. 算子 $\\mathbf{D}^{-1} \\mathbf{A}$ 在无向图上具有与节点度成正比的平稳分布，因此重复聚合必然会将节点特征对齐到按度加权的全局平均值，并放大中心节点的作用；相比之下，$\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 相对于度是无偏的，并且在极限情况下不偏好高度节点。\n\nD. $\\mathbf{D}^{-1} \\mathbf{A}$ 和 $\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 都在每一步 $t$ 保持欧几里得范数 $\\lVert \\mathbf{H}^{(t)} \\rVert_{2}$，因此两种归一化都不会导致特征漂移。\n\nE. 在归一化之前通过 $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ 添加自环，总是能消除两种归一化方法中的中心节点引发的漂移，无论 $k$ 和层数如何。", "solution": "问题陈述已经过验证，被认为是具有科学依据、适定、客观和完整的。它为分析两种常见 GNN 聚合方案在特定图结构上的行为提供了一个清晰的理论框架。我们可以开始解答。\n\n该问题要求我们分析 GNN 消息传递层中邻接矩阵的两种不同归一化方案的效果，特别是在星形图上关于“沿中心节点的特征漂移”的问题。\n\n设星形图有 $n=k+1$ 个节点，包括一个由 $c$ 索引的中央中心节点和 $k$ 个由 $\\ell \\in \\{1, \\dots, k\\}$ 索引的外围叶节点。\n节点的度为：\n- 中心节点 $c$：$d_c = k$\n- 叶节点 $\\ell$：$d_\\ell = 1$\n\n度矩阵为 $\\mathbf{D} = \\mathrm{diag}(d_c, d_{\\ell_1}, \\dots, d_{\\ell_k}) = \\mathrm{diag}(k, 1, \\dots, 1)$。\n邻接矩阵 $\\mathbf{A}$ 的元素对于所有叶节点 $\\ell$ 满足 $\\mathbf{A}_{c\\ell} = \\mathbf{A}_{\\ell c} = 1$，所有其他元素均为 $0$。该图是简单图，因此对角线元素 $\\mathbf{A}_{ii}$ 为 $0$。\n初始特征是标量（$p=1$），用向量 $\\mathbf{h}^{(0)} \\in \\mathbb{R}^{k+1}$ 表示，其分量为中心节点的 $h^{(0)}_c$ 和叶节点的 $h^{(0)}_\\ell$。\n单层聚合由 $\\mathbf{h}^{(1)} = \\hat{\\mathbf{A}} \\mathbf{h}^{(0)}$ 给出。\n\n我们将分别分析这两种归一化方案。\n\n### 1. 行归一化（随机游走）聚合器：$\\hat{\\mathbf{A}} = \\mathbf{D}^{-1} \\mathbf{A}$\n\n节点 $i$ 的更新规则为 $h^{(1)}_i = \\sum_{j=1}^{n} (\\mathbf{D}^{-1}\\mathbf{A})_{ij} h^{(0)}_j$。\n由于 $(\\mathbf{D}^{-1}\\mathbf{A})_{ij} = \\frac{1}{d_i} \\mathbf{A}_{ij}$，更新规则简化为对邻居节点的特征进行平均：\n$$h^{(1)}_i = \\frac{1}{d_i} \\sum_{j \\in \\mathcal{N}(i)} h^{(0)}_j$$\n其中 $\\mathcal{N}(i)$ 是节点 $i$ 的邻居集合。\n\n- **叶节点 $\\ell$ 的更新**：\n一个叶节点只有一个邻居，即中心节点 $c$。它的度是 $d_\\ell = 1$。\n$$h^{(1)}_\\ell = \\frac{1}{d_\\ell} \\sum_{j \\in \\mathcal{N}(\\ell)} h^{(0)}_j = \\frac{1}{1} h^{(0)}_c = h^{(0)}_c$$\n经过一个聚合步骤后，每个叶节点的特征完全被中心节点的初始特征所取代。\n\n- **中心节点 $c$ 的更新**：\n中心节点连接到所有 $k$ 个叶节点。它的度是 $d_c = k$。\n$$h^{(1)}_c = \\frac{1}{d_c} \\sum_{j \\in \\mathcal{N}(c)} h^{(0)}_j = \\frac{1}{k} \\sum_{\\ell=1}^k h^{(0)}_\\ell$$\n中心节点的新特征是所有叶节点初始特征的平均值。\n\n### 2. 对称归一化聚合器：$\\hat{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$\n\n节点 $i$ 的更新规则是 $h^{(1)}_i = \\sum_{j=1}^{n} (\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2})_{ij} h^{(0)}_j$。\n矩阵元素 $(\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2})_{ij} = \\frac{1}{\\sqrt{d_i}} \\mathbf{A}_{ij} \\frac{1}{\\sqrt{d_j}} = \\frac{\\mathbf{A}_{ij}}{\\sqrt{d_i d_j}}$。更新规则是：\n$$h^{(1)}_i = \\sum_{j \\in \\mathcal{N}(i)} \\frac{1}{\\sqrt{d_i d_j}} h^{(0)}_j$$\n\n- **叶节点 $\\ell$ 的更新**：\n唯一的邻居是 $c$。我们有 $d_\\ell=1$ 和 $d_c=k$。\n$$h^{(1)}_\\ell = \\frac{1}{\\sqrt{d_\\ell d_c}} h^{(0)}_c = \\frac{1}{\\sqrt{1 \\cdot k}} h^{(0)}_c = \\frac{1}{\\sqrt{k}} h^{(0)}_c$$\n叶节点的新特征是中心节点的初始特征，但按 $1/\\sqrt{k}$ 的因子进行了缩放。\n\n- **中心节点 $c$ 的更新**：\n邻居是 $k$ 个叶节点，每个叶节点的度为 $d_\\ell=1$。中心节点的度为 $d_c=k$。\n$$h^{(1)}_c = \\sum_{\\ell=1}^k \\frac{1}{\\sqrt{d_c d_\\ell}} h^{(0)}_\\ell = \\sum_{\\ell=1}^k \\frac{1}{\\sqrt{k \\cdot 1}} h^{(0)}_\\ell = \\frac{1}{\\sqrt{k}} \\sum_{\\ell=1}^k h^{(0)}_\\ell$$\n中心节点的新特征是叶节点特征的总和，按 $1/\\sqrt{k}$ 的因子进行了缩放。\n\n### 选项评估\n\n**A. 在 $\\hat{\\mathbf{A}} = \\mathbf{D}^{-1} \\mathbf{A}$ 的情况下，经过一次聚合后，每个叶节点满足 $h^{(1)}_{\\ell} = h^{(0)}_{c}$，中心节点满足 $h^{(1)}_{c} = \\frac{1}{k} \\sum_{\\ell=1}^{k} h^{(0)}_{\\ell}$。这表明 $\\mathbf{D}^{-1} \\mathbf{A}$ 比 $\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 具有更强的中心节点引发的漂移。**\n我们对行归一化情况的推导证实了 $h^{(1)}_\\ell = h^{(0)}_c$ 和 $h^{(1)}_c = \\frac{1}{k} \\sum_{\\ell=1}^k h^{(0)}_\\ell$。计算是正确的。“中心节点引发的漂移”指的是高度节点（中心）对低度节点（叶节点）的拉动。在这种情况下，叶节点的特征完全被中心节点的特征所取代。对于对称归一化的情况，叶节点的新特征是 $h^{(1)}_\\ell = \\frac{1}{\\sqrt{k}} h^{(0)}_c$。由于 $k \\ge 1$，我们有 $1 \\ge \\frac{1}{\\sqrt{k}}$。在 $\\mathbf{D}^{-1}\\mathbf{A}$ 下的更新导致叶节点特征的变化不受中心节点度的衰减，而在 $\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$ 下的更新则被衰减了。因此，$\\mathbf{D}^{-1}\\mathbf{A}$ 的漂移更强。该陈述与我们的分析完全一致。\n**结论：正确**\n\n**B. 在 $\\hat{\\mathbf{A}} = \\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 的情况下，经过一次聚合后，每个叶节点满足 $h^{(1)}_{\\ell} = \\frac{1}{\\sqrt{k}} \\, h^{(0)}_{c}$，中心节点满足 $h^{(1)}_{c} = \\frac{1}{\\sqrt{k}} \\sum_{\\ell=1}^{k} h^{(0)}_{\\ell}$，因此中心节点对叶节点的影响被一个 $1/\\sqrt{k}$ 的因子衰减，导致向中心节点的漂移比使用 $\\mathbf{D}^{-1} \\mathbf{A}$ 时更慢。**\n我们对对称归一化情况的推导证实了 $h^{(1)}_\\ell = \\frac{1}{\\sqrt{k}} h^{(0)}_c$ 和 $h^{(1)}_c = \\frac{1}{\\sqrt{k}} \\sum_{\\ell=1}^k h^{(0)}_\\ell$。计算是正确的。结论指出，中心节点的影响被 $1/\\sqrt{k}$ 的因子衰减，与 $\\mathbf{D}^{-1}\\mathbf{A}$ 的情况（其中隐式因子为1）相比，这是正确的。这种衰减导致了更慢（或更弱）的漂移。这与选项 A 的结论相同，只是从对称归一化的角度来看。该陈述与我们的分析完全一致。\n**结论：正确**\n\n**C. 算子 $\\mathbf{D}^{-1} \\mathbf{A}$ 在无向图上具有与节点度成正比的平稳分布，因此重复聚合必然会将节点特征对齐到按度加权的全局平均值，并放大中心节点的作用；相比之下，$\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 相对于度是无偏的，并且在极限情况下不偏好高度节点。**\n第一部分是正确的。无向图上的随机游走矩阵 $\\mathbf{D}^{-1}\\mathbf{A}$ 具有平稳分布 $\\pi_i \\propto d_i$。重复应用会导致特征收敛到一个共识值，其中高度节点的初始特征权重更大。第二部分是错误的。当 $t \\to \\infty$ 时，由 $\\mathbf{h}^{(t)} = (\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2})^t \\mathbf{h}^{(0)}$ 传播的特征会与 $\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 的主导特征向量对齐。已知该特征向量为 $\\mathbf{u} = (\\sqrt{d_1}, \\sqrt{d_2}, \\dots, \\sqrt{d_n})^T$。这意味着在极限情况下，$h_i^{(\\infty)} \\propto \\sqrt{d_i}$。这仍然偏好高度节点，尽管程度弱于与 $d_i$ 成正比。声称它“相对于度是无偏的”是错误的。\n**结论：不正确**\n\n**D. $\\mathbf{D}^{-1} \\mathbf{A}$ 和 $\\mathbf{D}^{-1/2} \\mathbf{A} \\mathbf{D}^{-1/2}$ 都在每一步 $t$ 保持欧几里得范数 $\\lVert \\mathbf{H}^{(t)} \\rVert_{2}$，因此两种归一化都不会导致特征漂移。**\n如果一个算子是正交矩阵，它会保持向量的 L2 范数。通常情况下，$\\mathbf{D}^{-1}\\mathbf{A}$ 和 $\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$ 都不是正交矩阵。例如，$\\mathbf{D}^{-1}\\mathbf{A}$ 的最大特征值为 1，但其他特征值的模可能小于 1，导致范数衰减。$\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}$ 的特征值都在 $[-1, 1]$ 区间内，除非它们的模都为 1，否则范数通常不会被保持。前提是错误的。此外，结论也不合逻辑；保持范数并不排除向量方向的改变，而漂移正是指方向的改变。\n**结论：不正确**\n\n**E. 在归一化之前通过 $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$ 添加自环，总是能消除两种归一化方法中的中心节点引发的漂移，无论 $k$ 和层数如何。**\n让我们分析添加自环的效果。新的邻接矩阵是 $\\tilde{\\mathbf{A}} = \\mathbf{A} + \\mathbf{I}$。新的度是 $\\tilde{d}_i = d_i + 1$。对于星形图，$\\tilde{d}_c = k+1$ 和 $\\tilde{d}_\\ell = 2$。考虑带自环的行归一化情况：$\\hat{\\mathbf{A}} = \\tilde{\\mathbf{D}}^{-1}\\tilde{\\mathbf{A}}$。叶节点 $\\ell$ 的更新变为：$$h^{(1)}_\\ell = \\frac{1}{\\tilde{d}_\\ell} (h^{(0)}_\\ell + h^{(0)}_c) = \\frac{1}{2}h^{(0)}_\\ell + \\frac{1}{2}h^{(0)}_c$$ 新特征是叶节点旧特征和中心节点特征的平均值。来自中心节点的影响仍然存在，将叶节点的特征拉向中心节点的特征。因此，漂移并未被“消除”。它仅仅是被减缓了，因为叶节点保留了其先前特征的一半。“总是消除”的说法绝对是错误的。\n**结论：不正确**\n\n选项 A 和 B 都是从问题前提中推导出的数学上正确的陈述。它们从两个不同但一致的角度描述了同一现象。", "answer": "$$\\boxed{AB}$$", "id": "3189858"}, {"introduction": "标准的消息传递机制虽然强大，但其表达能力存在理论上限，无法区分某些具有不同结构但局部邻域相似的图。本练习 [@problem_id:3189816] 揭示了这一局限性，它将向你展示为何标准MPNNs在区分一个含三角形的图和一个不含三角形的3-正则图时会失败。通过分析这个问题，你将深入理解MPNNs与Weisfeiler-Lehman测试的联系，并探索如何通过设计基于“基序”（motif）的邻域来突破这一表达能力的瓶颈。", "problem": "你的任务是提出并分析一个合成图分类任务，该任务旨在探究图神经网络（GNNs）中消息传递范式的表达能力限制。目标是从核心定义出发进行推理，以展示标准的消息传递神经网络（MPNNs）在何种情况下可证明地无法检测到三角形基序，以及一个经过修改、能感知基序的邻域定义如何克服这一限制。\n\n考虑以下通用设置。一个简单无向图表示为 $G=(V,E)$，其中 $V$ 是节点集，$E$ 是边集。消息传递神经网络（MPNN）通过迭代更新节点状态，对于 $t=0,1,\\dots,T-1$，其更新规则为：\n$$\nh_v^{(t+1)} \\;=\\; \\phi^{(t)}\\Big(h_v^{(t)}, \\;\\square_{u \\in \\mathcal{N}(v)} \\psi^{(t)}\\big(h_v^{(t)}, h_u^{(t)}, e_{vu}\\big)\\Big),\n$$\n其中 $h_v^{(0)}=x_v$ 是初始节点特征，$\\mathcal{N}(v)$ 是 $v$ 的邻域，$\\psi^{(t)}$ 和 $\\phi^{(t)}$ 是可学习函数，$e_{vu}$ 编码存在的边特征，$\\square$ 是一个置换不变的多重集聚合器，如求和、均值或最大值。图级输出由作用于 $\\{h_v^{(T)}: v \\in V\\}$ 的置换不变读出函数产生。一个广为接受的事实是，具有此类局部、置换不变聚合的同构不变MPNN，在区分非同构图方面，其能力最多与1维Weisfeiler–Lehman（WL）测试相当。\n\n定义一个节点 $v$ 的三角形基序邻域为\n$$\n\\mathcal{N}_{\\triangle}(v) \\;=\\; \\left\\{\\, u \\in V \\;:\\; \\exists w \\in V \\;\\text{with}\\; \\{v,u\\}\\in E,\\; \\{u,w\\}\\in E,\\; \\{w,v\\}\\in E \\,\\right\\},\n$$\n也就是说，$\\mathcal{N}_{\\triangle}(v)$ 恰好收集了那些与某个 $w$ 一起通过 $v$ 闭合一个3-环的邻居 $u$。可以通过在聚合操作中用 $\\mathcal{N}_{\\triangle}(v)$ 替换 $\\mathcal{N}(v)$ 来定义一个基序感知的MPNN。\n\n现在，考虑构建一个合成的二元图分类任务“三角形存在性”：给定图 $G$，如果 $G$ 包含至少一个三角形，则预测标签 $y(G)=1$，否则预测 $y(G)=0$。所有节点都以相同的特征开始，即对于所有 $v \\in V$，有 $x_v = \\mathbf{c} \\in \\mathbb{R}^{d}$，其中 $d \\in \\mathbb{N}$ 是固定的。该MPNN是同构不变的，并使用任何标准的置换不变聚合器。你可以假设图中没有自环也没有平行边。\n\n下列哪个（或哪些）选项正确地指定了一对具体的图，并为标准MPNN在这种任务上在最坏情况下失败的原因，以及为什么切换到基于基序的邻域 $\\mathcal{N}_{\\triangle}(v)$ 可以成功的原因，提供了正确的推理？\n\nA. 使用 $G_{1}$ 为6个节点上的三棱柱图（两个不相交的三角形，其对应顶点相连，该图包含三角形），$G_{2}$ 为 $K_{3,3}$（划分大小为3的完全二分图，该图无三角形）。两者都是6个节点上的3-正则图。由于初始节点特征相同，一个具有标准1跳邻域的同构不变MPNN无法区分 $G_{1}$ 和 $G_{2}$，因为其表达能力与1维Weisfeiler–Lehman测试相当，而该测试在这些正则图上会分配统一的颜色且从不进行细化。因此，尽管 $y(G_{1})=1$ 和 $y(G_{2})=0$，模型仍会为 $G_{1}$ 和 $G_{2}$ 输出相同的图级嵌入。如果我们转而对 $\\mathcal{N}_{\\triangle}(v)$ 进行聚合，那么 $G_{1}$ 中的每个节点都有 $\\lvert \\mathcal{N}_{\\triangle}(v) \\rvert = 2$，而 $G_{2}$ 中的每个节点都有 $\\lvert \\mathcal{N}_{\\triangle}(v) \\rvert = 0$，因此单个基序感知层就可以通过 $G_{1}$ 中的非平凡消息与 $G_{2}$ 中的空消息来区分这两个图。\n\nB. 使用任何具有不同三角形数量的图对。一个具有足够层数的标准MPNN总是能够捕获三角形，因为堆叠3层可以编码3跳游走，这在聚合消息中直接计算了3-环；因此，基于基序的邻域是不必要的。\n\nC. 标准MPNN在三角形计数上的失败主要源于在使用深层网络进行训练时的梯度消失问题；引入 $\\mathcal{N}_{\\triangle}(v)$ 是通过重新缩放消息来修复优化曲面，而不是改变表达能力，所以任何具有不同三角形数量的图对在训练后都将被分开。\n\nD. 在标准MPNN中，用闭合2跳邻域 $\\mathcal{N}_{2}(v)=\\{u \\in V : \\mathrm{dist}(u,v) \\le 2\\}$ 替换 $\\mathcal{N}(v)$。这足以检测3-环，因为所有三角形的顶点都在2跳之内，所以模型可以在不使用基于基序的邻域的情况下区分 $K_{3,3}$ 和三棱柱图。\n\nE. 用节点的度来增强其初始特征，即 $x_v = [\\mathbf{c}; \\deg(v)]$。由于富含三角形的图对于任何d都不是d-正则的，标准MPNN仅凭度信息就能精确计数三角形；$\\mathcal{N}_{\\triangle}(v)$ 对于完美区分是不必要的。", "solution": "用户要求分析一个关于消息传递神经网络（MPNNs）表达能力限制的问题，特别是关于它们检测三角形基序的能力。任务是验证问题陈述，如果有效，则在提供的选项中确定正确的一项。\n\n### 问题陈述验证\n\n首先，我将验证所提供的问题陈述。\n\n**步骤1：提取已知条件**\n-   一个简单无向图为 $G=(V,E)$。\n-   标准的MPNN更新规则是 $h_v^{(t+1)} \\;=\\; \\phi^{(t)}\\Big(h_v^{(t)}, \\;\\square_{u \\in \\mathcal{N}(v)} \\psi^{(t)}\\big(h_v^{(t)}, h_u^{(t)}, e_{vu}\\big)\\Big)$。\n-   初始节点特征为 $h_v^{(0)}=x_v$。\n-   $\\mathcal{N}(v)$ 是节点 $v$ 的标准1跳邻域。\n-   $\\psi^{(t)}$ 和 $\\phi^{(t)}$ 是可学习函数（例如，神经网络）。\n-   $\\square$ 是一个置换不变的多重集聚合器（例如，求和、均值、最大值）。\n-   图级输出由一个作用于最终节点嵌入集合 $\\{h_v^{(T)}: v \\in V\\}$ 的置换不变读出函数得到。\n-   一个既定事实是，此类MPNN的表达能力最多与1维Weisfeiler–Lehman（$1$-WL）测试相当。\n-   三角形基序邻域定义为 $\\mathcal{N}_{\\triangle}(v) \\;=\\; \\left\\{\\, u \\in V \\;:\\; \\exists w \\in V \\;\\text{with}\\; \\{v,u\\}\\in E,\\; \\{u,w\\}\\in E,\\; \\{w,v\\}\\in E \\,\\right\\}$。\n-   基序感知的MPNN使用 $\\mathcal{N}_{\\triangle}(v)$ 而非 $\\mathcal{N}(v)$ 进行聚合。\n-   任务是一个二元图分类问题，“三角形存在性”，其中如果 $G$ 包含三角形则 $y(G)=1$，否则 $y(G)=0$。\n-   对于此任务，所有节点具有相同的初始特征：对于所有 $v \\in V$，有 $x_v = \\mathbf{c} \\in \\mathbb{R}^{d}$，其中 $\\mathbf{c}$ 和 $d$ 是固定的。\n-   假设图没有自环或平行边。\n-   问题是确定哪个选项为标准MPNN的失败和基序感知MPNN的成功提供了一对正确的图和正确的推理。\n\n**步骤2：使用提取的已知条件进行验证**\n-   **科学依据：** 问题陈述牢固地植根于图神经网络理论的既定原则。对MPNNs的描述、它们与$1$-WL测试的联系，以及使用合成图对来探究表达能力，都是该领域的标准方法论（参见Xu et al., 2019, \"How Powerful are Graph Neural Networks?\"）。定义是形式化且正确的。\n-   **适定性：** 问题是适定的。它建立了一个清晰的理论情景，并要求基于所提供的定义进行分析。根据这些原则，预期在选项中存在一个唯一的正确答案。\n-   **客观性：** 问题使用精确、客观、形式化的数学语言陈述。诸如“同构不变”、“置换不变聚合器”和“正则图”等术语在数学和计算机科学中具有明确的含义。\n\n问题陈述没有科学不健全、歧义和内部矛盾。它提出了一个在GNN表达能力研究中非平凡且典型的问题。\n\n**步骤3：结论与行动**\n问题陈述是**有效的**。我将继续对选项进行全面推导和分析。\n\n### 解答推导\n\n问题的核心在于，具有相同初始节点特征的同构不变MPNN的表达能力与1维Weisfeiler–Lehman（$1$-WL）测试之间的等价性。$1$-WL测试通过迭代地细化节点“颜色”（在此类似于节点嵌入 $h_v^{(t)}$）来区分图。一个节点的新颜色由其当前颜色及其邻居颜色的多重集决定。\n\n如果两个图 $G_1$ 和 $G_2$ 都是 $d$-正则图，具有相同的度 $d$ 和相同的节点数，则$1$-WL测试无法区分它们。让我们来追踪这个过程：\n1.  **初始化 ($t=0$)：** 两个图中的所有节点都以相同的颜色开始，因为它们的初始特征 $x_v=\\mathbf{c}$ 是相同的。设这个初始嵌入为 $h^{(0)}$。\n2.  **迭代 1 ($t=1$)：** 对于 $G_1$ 或 $G_2$ 中的任何节点 $v$，它有 $d$ 个邻居。所有这些邻居都具有相同的嵌入 $h^{(0)}$。邻居嵌入的多重集是 $\\{h^{(0)}, h^{(0)}, \\dots, h^{(0)}\\}$（$d$次）。聚合步骤 $\\square_{u \\in \\mathcal{N}(v)} \\psi^{(0)}(h_v^{(0)}, h_u^{(0)}, \\dots)$ 将为两个图中的每个节点 $v$ 产生一个相同的聚合消息。然后，更新步骤 $\\phi^{(0)}$ 将为 $G_1$ 和 $G_2$ 中的所有节点产生一个新的、统一的嵌入 $h^{(1)}$。\n3.  **后续迭代：** 这个过程会重复。在每一步 $t$，两个图中的所有节点都将具有相同的嵌入 $h^{(t)}$。\n\n因此，最终的节点嵌入集合 $\\{h_v^{(T)}\\}_{v \\in V(G_1)}$ 是一个由相同向量组成的多重集，$\\{h_v^{(T)}\\}_{v \\in V(G_2)}$ 也是如此。任何置换不变的读出函数（如对节点嵌入求和或求平均）都将为 $G_1$ 和 $G_2$ 产生完全相同的图级嵌入。\n\n因此，要创建一个标准MPNN会失败的任务，我们必须找到一对图 ($G_1$, $G_2$) 使得：\n-   $y(G_1) \\neq y(G_2)$（即，一个有三角形，一个没有）。\n-   $G_1$ 和 $G_2$ 是 $1$-WL测试无法区分的。一个充分条件是它们是具有相同度和相同节点数的正则图。\n\n为了让基序感知的MPNN成功，两个图之间基于三角形的邻域 $\\mathcal{N}_{\\triangle}(v)$ 必须在结构上有所不同，从而允许GNN计算出不同的节点嵌入。\n\n### 逐项分析\n\n**A. 使用 $G_{1}$ 为6个节点上的三棱柱图（两个不相交的三角形，其对应顶点相连，该图包含三角形），$G_{2}$ 为 $K_{3,3}$（划分大小为3的完全二分图，该图无三角形）。两者都是6个节点上的3-正则图。由于初始节点特征相同，一个具有标准1跳邻域的同构不变MPNN无法区分 $G_{1}$ 和 $G_{2}$，因为其表达能力与1维Weisfeiler–Lehman测试相当，而该测试在这些正则图上会分配统一的颜色且从不进行细化。因此，尽管 $y(G_{1})=1$ 和 $y(G_{2})=0$，模型仍会为 $G_{1}$ 和 $G_{2}$ 输出相同的图级嵌入。如果我们转而对 $\\mathcal{N}_{\\triangle}(v)$ 进行聚合，那么 $G_{1}$ 中的每个节点都有 $\\lvert \\mathcal{N}_{\\triangle}(v) \\rvert = 2$，而 $G_{2}$ 中的每个节点都有 $\\lvert \\mathcal{N}_{\\triangle}(v) \\rvert = 0$，因此单个基序感知层就可以通过 $G_{1}$ 中的非平凡消息与 $G_{2}$ 中的空消息来区分这两个图。**\n\n-   **图分析：**\n    -   $G_1$，三棱柱图，有6个顶点和9条边。它是3-正则的。它包含三角形（具体来说，是两个不相交的3-环）。所以，$y(G_1)=1$。\n    -   $G_2$，完全二分图 $K_{3,3}$，有6个顶点和 $3 \\times 3 = 9$ 条边。它也是3-正则的。根据定义，二分图是无三角形的。所以，$y(G_2)=0$。\n-   **标准MPNN失败：** 正如在推导中所确立的，由于 $G_1$ 和 $G_2$ 都是6个顶点上的3-正则图，一个具有相同初始特征的标准MPNN无法区分它们。选项中提供的推理是正确的。\n-   **基序感知MPNN成功：**\n    -   在 $G_1$ 中，每个顶点 $v$ 都恰好是一个三角形的一部分。该三角形的另外两个顶点是它的邻居。因此，对于任何 $v \\in V(G_1)$，其三角形基序邻域 $\\mathcal{N}_{\\triangle}(v)$ 恰好包含这两个邻居。所以，对于所有 $v \\in V(G_1)$，有 $|\\mathcal{N}_{\\triangle}(v)| = 2$。\n    -   在 $G_2$ 中，由于该图是无三角形的，没有节点 $v$ 可以是三角形的一部分。因此，对于任何 $v \\in V(G_2)$，其三角形基序邻域是空的：$\\mathcal{N}_{\\triangle}(v) = \\emptyset$。所以，对于所有 $v \\in V(G_2)$，有 $|\\mathcal{N}_{\\triangle}(v)| = 0$。\n    -   在一个基序感知的MPNN的第一层，对 $G_1$ 中节点的聚合是作用于一个大小为2的集合，而对 $G_2$ 中节点的聚合是作用于一个空集。除非可学习函数是平凡的，否则这将产生不同的聚合消息，从而为 $G_1$ 和 $G_2$ 中的节点产生不同的更新嵌入 $h_v^{(1)}$。随后的读出函数便可以区分这两个图。这个推理是合理的。\n\n-   **结论：** **正确**。\n\n**B. 使用任何具有不同三角形数量的图对。一个具有足够层数的标准MPNN总是能够捕获三角形，因为堆叠3层可以编码3跳游走，这在聚合消息中直接计算了3-环；因此，基于基序的邻域是不必要的。**\n\n-   **分析：** 这个陈述根本上是错误的。虽然一个 $k$ 层的MPNN会从 $k$-跳邻域聚合信息，但置换不变的聚合器 $\\square$ 会丢弃该邻域内的特定拓扑结构。它只对来自邻居的特征向量多重集进行操作。一个MPNN无法区分一个长度为2的路径和连接到中心节点的两条不相交的边。要计算3-环（$v \\to u \\to w \\to v$），节点 $v$ 需要知道它的哪些邻居是相互连接的（即，检测到边 $\\{u, w\\}$）。从 $u$到$v$ 的标准MPNN消息不携带此信息。选项A中的反例提供了确凿的证据，证明标准MPNN并非总能检测到三角形。\n-   **结论：** **错误**。\n\n**C. 标准MPNN在三角形计数上的失败主要源于在使用深层网络进行训练时的梯度消失问题；引入 $\\mathcal{N}_{\\triangle}(v)$ 是通过重新缩放消息来修复优化曲面，而不是改变表达能力，所以任何具有不同三角形数量的图对在训练后都将被分开。**\n\n-   **分析：** 这个陈述错误地归因了失败的原因。标准MPNN无法区分某些图（如选项A中的图对）是一个根本性的**表达能力**问题，而不是**优化**问题。无论模型训练得多好（即，即使有一个能找到损失函数全局最小值的完美优化器），它在数学上也不可能为这些图产生不同的输出。MPNN架构可以表示的函数类别根本不够强大。引入 $\\mathcal{N}_{\\triangle}(v)$ 是对模型架构的修改；它改变了计算图本身，从而从根本上增加了模型的表达能力。它允许模型计算以前无法表示的函数。问题不在于梯度消失。\n-   **结论：** **错误**。\n\n**D. 在标准MPNN中，用闭合2跳邻域 $\\mathcal{N}_{2}(v)=\\{u \\in V : \\mathrm{dist}(u,v) \\le 2\\}$ 替换 $\\mathcal{N}(v)$。这足以检测3-环，因为所有三角形的顶点都在2跳之内，所以模型可以在不使用基于基序的邻域的情况下区分 $K_{3,3}$ 和三棱柱图。**\n\n-   **分析：** 这个修改是不够的。虽然三角形的顶点确实在彼此的2跳邻域内，但标准的置换不变聚合器 $\\square$ 仍然作用于一个节点特征的多重集。它无法“看到”聚合集合中节点*之间*存在的边。让我们用选项A中的图来分析这一点。\n    -   在 $K_{3,3}$ 中，对于任何节点 $v$，其邻居距离为1，其自身划分中的另外两个节点距离为2。所以开放2跳邻域 $\\{u : 1 \\le \\mathrm{dist}(v,u) \\le 2\\}$ 的大小为 $3+2=5$。\n    -   在三棱柱图中，对于任何节点 $v$，它有3个距离为1的邻居。剩下的2个节点距离为2。所以开放2跳邻域的大小也是5。\n    -   由于所有节点都以相同的特征 $\\mathbf{c}$ 开始，一个在2跳邻域上聚合的MPNN将再次为两个图中的每个节点看到一个由5个相同特征向量组成的多重集。计算过程保持相同，图仍然无法区分。仅仅扩大邻域并不能克服置换不变聚合器的核心限制。\n-   **结论：** **错误**。\n\n**E. 用节点的度来增强其初始特征，即 $x_v = [\\mathbf{c}; \\deg(v)]$。由于富含三角形的图对于任何d都不是d-正则的，标准MPNN仅凭度信息就能精确计数三角形；$\\mathcal{N}_{\\triangle}(v)$ 对于完美区分是不必要的。**\n\n-   **分析：** 这个选项包含多个谬误。\n    -   首先，前提“富含三角形的图对于任何d都不是d-正则的”是错误的。完全图 $K_n$ 是 $(n-1)$-正则的，并且三角形密度最大。选项A中的三棱柱图是3-正则的并且包含三角形。\n    -   其次，对于具体的反例对（$G_1$: 三棱柱图, $G_2$: $K_{3,3}$），两个图都是3-正则的。用节点的度来增强其特征将导致两个图中每个节点的特征都是 $x_v = [\\mathbf{c}; 3]$。这再次导致所有节点的初始特征相同，没有提供任何新信息来区分这两个图。\n    -   第三，即使对于非正则图，度序列也并不能唯一确定三角形的数量。例如，一个6-环图和一个由两个不相交的三角形组成的图都包含6个顶点，所有顶点的度都为2。它们的度序列相同，但一个没有三角形，另一个有两个。一个带有度特征的标准MPNN将无法区分它们。\n-   **结论：** **错误**。", "answer": "$$\\boxed{A}$$", "id": "3189816"}]}