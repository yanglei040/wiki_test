{"hands_on_practices": [{"introduction": "时序差分（Temporal Difference, TD）误差是Q学习的核心驱动力。本练习将探讨TD误差的统计特性如何随着智能体的学习而演变。通过分析一个简化的场景，我们将深入理解学习过程的基本动态，特别是对比初始“预热”阶段与最终“收敛”状态下TD误差的分布差异 [@problem_id:3113150]。", "problem": "你需要分析一个简化的离策略深度Q网络（DQN）学习设置，该设置包含经验回放（ER）和一个长度为 $T_{\\text{burn}}$ 的回放缓冲区预热期。环境是一个具有两个可用动作的单状态马尔可夫决策过程（MDP）。奖励是随机的且依赖于动作，下一个状态总是同一个状态。智能体根据其当前的动作价值函数使用一个 $\\epsilon$-贪心行为策略。你的分析必须从时间差分（TD）学习和贝尔曼最优方程的基本定义出发，并且必须避免使用任何未从这些基础推导出的快捷公式。\n\n假设与定义：\n- 存在单一状态 $s$ 和两个动作 $a_1$ 和 $a_2$。如果智能体采取动作 $a_i$，它会收到一个奖励 $R_i$，其条件分布为 $R_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$，其中 $\\mu_i \\in \\mathbb{R}$ 和 $\\sigma \\ge 0$ 是已知常数，并确定性地转移回状态 $s$。\n- 折扣因子为 $\\gamma \\in [0,1)$。\n- 回放缓冲区在长度为 $T_{\\text{burn}}$ 的预热期内收集转移，在此期间不执行参数更新。\n- 动作价值函数初始化为 $Q_0(s,a_1) = 0$ 和 $Q_0(s,a_2) = 0$。\n- 行为策略是 $\\epsilon$-贪心的：以 $1-\\epsilon$ 的概率选择一个贪心动作（均匀随机地打破平局），并以 $\\epsilon$ 的概率均匀随机地选择每个动作。\n- 时间 $t$ 的时间差分（TD）误差是 $\\delta_t = r_t + \\gamma \\max_{a'} Q(s, a') - Q(s, a_t)$。\n- 理想化的预热后学习假设：在预热期之后，智能体立即通过经验回放（ER）更新 $Q$，以达到该单状态MDP的贝尔曼最优方程的不动点。也就是说，它达到满足 $Q^*(a) = \\mathbb{E}[R_a] + \\gamma \\max_{a'} Q^*(a')$ 的 $Q^*$。\n\n任务：\n1. 仅使用上述假设以及TD误差和贝尔曼最优方程的定义，推导在任何学习发生之前，即当 $Q(s,a_1) = Q(s,a_2) = 0$ 时，TD误差的分布。具体来说，推导在初始化时两个动作价值相等的情况下，根据 $\\epsilon$-贪心探索的行为策略汇总的 $\\delta$ 的期望值和方差的闭式表达式。\n2. 使用相同的基础，推导在理想化的预热后学习假设下，收敛到 $Q^*$ 后的TD误差分布。具体来说，推导在收敛时，根据 $\\epsilon$-贪心行为策略汇总的 $\\delta$ 的期望值和方差的闭式表达式。\n3. 定义一个早期学习速度的度量，即前 $N$ 个环境步骤的预期累积均方TD误差，记作 $C(N, T_{\\text{burn}})$。在理想化的预热后收敛假设下，用收敛前均方TD误差和收敛后均方TD误差来推导 $C(N, T_{\\text{burn}})$ 的闭式表达式。你的表达式必须能处理边界情况 $T_{\\text{burn}} \\ge N$。\n4. 实现一个程序，对于下面测试套件中的每个测试用例，按顺序计算并返回以下五个量的元组：\n   - 学习前的 $\\delta$ 的期望值，记作 $m_{\\text{before}}$。\n   - 学习前的 $\\delta$ 的方差，记作 $v_{\\text{before}}$。\n   - 收敛后的 $\\delta$ 的期望值，记作 $m_{\\text{after}}$。\n   - 收敛后的 $\\delta$ 的方差，记作 $v_{\\text{after}}$。\n   - 前 $N$ 步的预期累积均方TD误差，$C(N, T_{\\text{burn}})$。\n   你的实现不应进行模拟；它必须根据你推导的公式计算这些值。\n\n测试套件（每一项是一个元组 $(\\mu_1, \\mu_2, \\sigma, \\gamma, \\epsilon, T_{\\text{burn}}, N)$）：\n- 案例 A: $(1.0, 0.2, 1.0, 0.9, 0.1, 20, 100)$\n- 案例 B: $(0.5, 0.5, 0.5, 0.0, 0.5, 0, 50)$\n- 案例 C: $(2.0, -1.0, 0.0, 0.8, 0.2, 10, 10)$\n- 案例 D: $(-0.3, -0.1, 0.7, 0.95, 0.05, 100, 30)$\n- 案例 E: $(0.0, 1.0, 0.3, 0.9, 0.3, 5, 5)$\n\n最终输出格式：\n- 你的程序应生成单行输出，包含所有测试用例的结果，形式为用方括号括起来的逗号分隔列表，其中每个用例的结果本身也是相同风格的逗号分隔列表。例如：\"[[m_b,v_b,m_a,v_a,C],[...],...]\"。\n- 所有输出必须是实数；不要包含任何单位或百分号，并且除了所要求的单行文本外，不要打印任何额外文本。", "solution": "该问题陈述被评估为科学上合理、定义明确、客观且自洽。进行严谨分析所需的所有必要参数和定义均已提供。预热后瞬时收敛的理想化处理是一个有效的理论简化，它使问题在不违反强化学习基本原则的情况下变得可解析处理。因此，该问题是有效的。\n\n解决方案通过从第一性原理出发，为学习的两个指定阶段（学习前和收敛后）推导所要求的统计量，然后将它们组合起来以找到总预期误差。\n\n**任务1：学习前的TD误差分布**\n\n在任何学习更新之前，动作价值函数对两个动作都初始化为零：$Q_0(s, a_1) = 0$ 和 $Q_0(s, a_2) = 0$。\n\n在给定时间步 $t$ 的时间差分（TD）误差定义为 $\\delta_t = r_t + \\gamma \\max_{a'} Q(s, a') - Q(s, a_t)$。\n代入初始Q值，我们得到：\n$$ \\delta_t = r_t + \\gamma \\max(0, 0) - Q(s, a_t) = r_t - 0 = r_t $$\n因此，在学习之前，TD误差就是收到的奖励。\n\n行为策略是 $\\epsilon$-贪心的。在初始化时，$Q(s, a_1) = Q(s, a_2)$，所以两个动作的价值相等。策略规定均匀随机地打破平局。\n- 以 $1-\\epsilon$ 的概率，智能体贪心地行动，以各 $0.5$ 的概率选择 $a_1$ 或 $a_2$。\n- 以 $\\epsilon$ 的概率，智能体进行探索，以各 $0.5$ 的概率选择 $a_1$ 或 $a_2$。\n因此，选择动作 $a_i$（对于 $i \\in \\{1, 2\\}$）的概率是：\n$$ P(A_t = a_i) = (1-\\epsilon) \\times 0.5 + \\epsilon \\times 0.5 = 0.5 $$\nTD误差 $\\delta$ 的分布是两个奖励分布 $R_1 \\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ 和 $R_2 \\sim \\mathcal{N}(\\mu_2, \\sigma^2)$ 的混合，混合概率相等，均为 $0.5$。\n\nTD误差的期望值 $m_{\\text{before}} = \\mathbb{E}[\\delta]$，使用全期望定律计算：\n$$ \\mathbb{E}[\\delta] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[\\delta | A_t=a_i] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[R_i] $$\n$$ m_{\\text{before}} = 0.5 \\cdot \\mu_1 + 0.5 \\cdot \\mu_2 = \\frac{\\mu_1 + \\mu_2}{2} $$\nTD误差的方差 $v_{\\text{before}} = \\text{Var}(\\delta)$，使用全方差定律计算：$\\text{Var}(\\delta) = \\mathbb{E}[\\text{Var}(\\delta | A_t)] + \\text{Var}(\\mathbb{E}[\\delta | A_t])$。\n条件方差是 $\\text{Var}(\\delta | A_t=a_i) = \\text{Var}(R_i) = \\sigma^2$。\n$$ \\mathbb{E}[\\text{Var}(\\delta | A_t)] = \\sum_{i=1}^{2} P(A_t=a_i) \\text{Var}(\\delta | A_t=a_i) = 0.5 \\cdot \\sigma^2 + 0.5 \\cdot \\sigma^2 = \\sigma^2 $$\n条件期望是 $\\mathbb{E}[\\delta | A_t=a_i] = \\mu_i$。随机变量 $\\mathbb{E}[\\delta | A_t]$ 以概率 $0.5$ 取值 $\\mu_1$，以概率 $0.5$ 取值 $\\mu_2$。该变量的方差是：\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = \\mathbb{E}[(\\mathbb{E}[\\delta | A_t])^2] - (\\mathbb{E}[\\mathbb{E}[\\delta | A_t]])^2 $$\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = (0.5 \\cdot \\mu_1^2 + 0.5 \\cdot \\mu_2^2) - \\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)^2 = \\frac{2\\mu_1^2 + 2\\mu_2^2 - (\\mu_1^2 + 2\\mu_1\\mu_2 + \\mu_2^2)}{4} = \\frac{\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2}{4} = \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2 $$\n合并各项，总方差为：\n$$ v_{\\text{before}} = \\sigma^2 + \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2 $$\n\n**任务2：收敛后的TD误差分布**\n\n收敛后，动作价值函数 $Q^*$ 满足此单状态MDP的贝尔曼最优方程：$Q^*(a) = \\mathbb{E}[R_a] + \\gamma \\max_{a'} Q^*(a')$。\n设 $Q^*_i = Q^*(s, a_i)$ 和 $V^* = \\max_{a'} Q^*(s, a') = \\max(Q^*_1, Q^*_2)$。方程为：\n$$ Q^*_1 = \\mu_1 + \\gamma V^* $$\n$$ Q^*_2 = \\mu_2 + \\gamma V^* $$\n将这些代入 $V^*$ 的定义中：\n$$ V^* = \\max(\\mu_1 + \\gamma V^*, \\mu_2 + \\gamma V^*) = \\max(\\mu_1, \\mu_2) + \\gamma V^* $$\n对于 $\\gamma \\in [0,1)$，解出 $V^*$：\n$$ V^*(1-\\gamma) = \\max(\\mu_1, \\mu_2) \\implies V^* = \\frac{\\max(\\mu_1, \\mu_2)}{1-\\gamma} $$\n现在考虑TD误差 $\\delta_t = r_t + \\gamma \\max_{a'} Q^*(s, a') - Q^*(s, a_t)$。如果采取动作 $a_i$，则 $a_t=a_i$，$r_t$ 是从 $R_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2)$ 中抽取的样本。TD误差为：\n$$ \\delta_t(a_i) = R_i + \\gamma V^* - Q^*_i = R_i + \\gamma V^* - (\\mu_i + \\gamma V^*) = R_i - \\mu_i $$\n这意味着，以动作 $a_i$ 为条件的TD误差是一个服从分布 $\\mathcal{N}(0, \\sigma^2)$ 的随机变量，因为 $\\mathbb{E}[R_i - \\mu_i] = \\mu_i - \\mu_i = 0$ 且 $\\text{Var}(R_i - \\mu_i) = \\text{Var}(R_i) = \\sigma^2$。\n\nTD误差的期望值 $m_{\\text{after}} = \\mathbb{E}[\\delta]$，再次使用全期望定律计算。由于对于任何动作，条件期望都是 $0$，所以总期望也是 $0$：\n$$ m_{\\text{after}} = \\mathbb{E}[\\delta] = \\sum_{i=1}^{2} P(A_t=a_i) \\mathbb{E}[\\delta | A_t=a_i] = \\sum_{i=1}^{2} P(A_t=a_i) \\cdot 0 = 0 $$\nTD误差的方差 $v_{\\text{after}} = \\text{Var}(\\delta)$，使用全方差定律计算。条件方差是 $\\text{Var}(\\delta | A_t=a_i) = \\sigma^2$，条件期望是 $\\mathbb{E}[\\delta|A_t=a_i]=0$。\n$$ \\mathbb{E}[\\text{Var}(\\delta | A_t)] = \\sum_{i=1}^{2} P(A_t=a_i) \\cdot \\sigma^2 = \\sigma^2 \\sum_{i=1}^{2} P(A_t=a_i) = \\sigma^2 $$\n$$ \\text{Var}(\\mathbb{E}[\\delta | A_t]) = \\text{Var}(0) = 0 $$\n因此，收敛后的总方差为：\n$$ v_{\\text{after}} = \\sigma^2 + 0 = \\sigma^2 $$\n注意，这些结果与动作选择概率无关，因此与 $\\epsilon$ 和哪个动作是最优的无关。\n\n**任务3：预期累积均方TD误差**\n\n预期累积均方TD误差 $C(N, T_{\\text{burn}})$，被解释为 $N$ 步内预期TD误差平方和：$C(N, T_{\\text{burn}}) = \\sum_{t=1}^{N} \\mathbb{E}[\\delta_t^2]$。\n一个随机变量 $X$ 的均方误差（MSE）是 $\\mathbb{E}[X^2] = \\text{Var}(X) + (\\mathbb{E}[X])^2$。\n\n对于学习前阶段（从 $t=1$ 到 $t=T_{\\text{burn}}$），MSE为：\n$$ \\text{MSE}_{\\text{before}} = v_{\\text{before}} + m_{\\text{before}}^2 = \\left(\\sigma^2 + \\left(\\frac{\\mu_1 - \\mu_2}{2}\\right)^2\\right) + \\left(\\frac{\\mu_1 + \\mu_2}{2}\\right)^2 $$\n$$ \\text{MSE}_{\\text{before}} = \\sigma^2 + \\frac{\\mu_1^2 - 2\\mu_1\\mu_2 + \\mu_2^2}{4} + \\frac{\\mu_1^2 + 2\\mu_1\\mu_2 + \\mu_2^2}{4} = \\sigma^2 + \\frac{2\\mu_1^2 + 2\\mu_2^2}{4} = \\sigma^2 + \\frac{\\mu_1^2 + \\mu_2^2}{2} $$\n对于收敛后阶段（从 $t = T_{\\text{burn}}+1$ 到 $t=N$），MSE为：\n$$ \\text{MSE}_{\\text{after}} = v_{\\text{after}} + m_{\\text{after}}^2 = \\sigma^2 + 0^2 = \\sigma^2 $$\n总时长为 $N$ 步。预热期持续 $T_{\\text{burn}}$ 步。\n- 如果 $T_{\\text{burn}} \\ge N$，所有 $N$ 步都发生在学习前阶段。\n- 如果 $T_{\\text{burn}}  N$，前 $T_{\\text{burn}}$ 步是学习前阶段，剩下的 $N - T_{\\text{burn}}$ 步是收敛后阶段。\n这可以表示为一个单一公式：\n$$ C(N, T_{\\text{burn}}) = \\min(N, T_{\\text{burn}}) \\cdot \\text{MSE}_{\\text{before}} + \\max(0, N - T_{\\text{burn}}) \\cdot \\text{MSE}_{\\text{after}} $$\n代入MSE的表达式：\n$$ C(N, T_{\\text{burn}}) = \\min(N, T_{\\text{burn}}) \\left(\\sigma^2 + \\frac{\\mu_1^2 + \\mu_2^2}{2}\\right) + \\max(0, N - T_{\\text{burn}}) \\cdot \\sigma^2 $$\n\n**任务4：实现**\n\n推导出的公式在以下Python程序中实现，用于计算每个测试用例的五个量。从推导中可以确定，输入元组中的参数 $\\gamma$ 和 $\\epsilon$ 对于最终计算不是必需的。\n- $m_{\\text{before}} = (\\mu_1 + \\mu_2)/2$\n- $v_{\\text{before}} = \\sigma^2 + ((\\mu_1 - \\mu_2)/2)^2$\n- $m_{\\text{after}} = 0$\n- $v_{\\text{after}} = \\sigma^2$\n- $C(N, T_{\\text{burn}})$ 如上推导。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes statistical properties of the TD error for a simplified DQN setup.\n    \"\"\"\n    # Test suite: each item is a tuple (mu1, mu2, sigma, gamma, epsilon, T_burn, N)\n    test_cases = [\n        (1.0, 0.2, 1.0, 0.9, 0.1, 20, 100),\n        (0.5, 0.5, 0.5, 0.0, 0.5, 0, 50),\n        (2.0, -1.0, 0.0, 0.8, 0.2, 10, 10),\n        (-0.3, -0.1, 0.7, 0.95, 0.05, 100, 30),\n        (0.0, 1.0, 0.3, 0.9, 0.3, 5, 5),\n    ]\n\n    all_results_str = []\n    for case in test_cases:\n        mu1, mu2, sigma, gamma, epsilon, T_burn, N = case\n\n        # Task 1: Expected value and variance of TD error before learning\n        # m_before = (mu1 + mu2) / 2\n        m_before = 0.5 * (mu1 + mu2)\n        # v_before = sigma^2 + ((mu1 - mu2) / 2)^2\n        v_before = sigma**2 + (0.5 * (mu1 - mu2))**2\n\n        # Task 2: Expected value and variance of TD error after convergence\n        # m_after = 0\n        m_after = 0.0\n        # v_after = sigma^2\n        v_after = sigma**2\n\n        # Task 3: Expected cumulative mean squared TD error\n        # MSE_before = v_before + m_before^2, or sigma^2 + (mu1^2 + mu2^2)/2\n        mse_before = sigma**2 + 0.5 * (mu1**2 + mu2**2)\n        # MSE_after = v_after + m_after^2 = sigma^2\n        mse_after = sigma**2\n        \n        # C(N, T_burn) = min(N, T_burn) * MSE_before + max(0, N - T_burn) * MSE_after\n        num_steps_before = min(N, T_burn)\n        num_steps_after = max(0, N - T_burn)\n        \n        C = num_steps_before * mse_before + num_steps_after * mse_after\n\n        # Store results for this case\n        case_result = [m_before, v_before, m_after, v_after, C]\n        \n        # Format the result list into the required string format \" [v1,v2,...] \"\n        case_str = f\"[{','.join(map(str, case_result))}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format \"[[...],[...],...]\"\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3113150"}, {"introduction": "从经验回放池中进行均匀采样往往效率低下，因为许多转换的TD误差很小，对学习贡献不大。优先经验回放（Prioritized Experience Replay, PER）通过更频繁地采样“重要”的转换来解决这个问题，但这本身也可能引入新的问题。本动手模拟将构建一个具有双峰奖励（bimodal rewards）的场景，以生动地展示过度激进的优先级设置如何导致“模式坍塌”（mode collapse），即智能体完全忽略某一类经验，从而阻碍学习 [@problem_id:3113071]。", "problem": "构建一个最小的马尔可夫决策过程 (Markov Decision Process)，以揭示随机奖励如何引发双峰时间差分 (Temporal-Difference, TD) 误差，以及优先经验回放 (Prioritized Experience Replay) 如何使采样偏向其中一个模式，从而可能破坏采样过渡的多样性。使用以下基本原理：贝尔曼最优方程 (Bellman optimality equation) 和时间差分误差的定义，以及优先采样的标准定义。\n\n给定一个单状态、单动作的环境。假设存在一个深度Q网络 (Deep Q-Network, DQN)，但其参数固定（无学习过程），因此动作价值估计保持不变。设每个时间步的奖励独立地从一个两点混合分布中抽取：\n- 奖励为 $r_{\\mathrm{small}}$ 的概率为 $p_{\\mathrm{small}}$。\n- 奖励为 $r_{\\mathrm{big}}$ 的概率为 $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$。\n\n设折扣因子为 $\\gamma$，对于唯一的状态-动作对，固定的Q值为 $q_0$。时间差分 (TD) 误差由核心的时间差分 (TD) 学习定义给出：\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a).\n$$\n由于 $Q$ 固定为 $q_0$ 且只有一个动作，你必须使用：\n$$\n\\delta = r + \\gamma q_0 - q_0.\n$$\n当 $q_0 = 0$ 时，该式简化为 $\\delta = r$，只要 $\\lvert r_{\\mathrm{small}} \\rvert \\neq \\lvert r_{\\mathrm{big}} \\rvert$，TD误差就直接继承了两点奖励分布的双峰特性。\n\n对于优先经验回放，存储在大小为 $N$ 的缓冲区中的过渡，会根据其绝对TD误差的幂次方成比例的概率进行采样，采用的是标准且经过充分测试的公式：\n- 对于每个TD误差为 $\\delta_i$ 的过渡 $i$，为其分配一个未归一化的优先级 $w_i = \\left( \\lvert \\delta_i \\rvert + \\varepsilon \\right)^{\\alpha}$，其中给定 $\\alpha \\ge 0$ 和一个小的 $\\varepsilon \\ge 0$。\n- 采样概率为 $p_i = w_i / \\sum_j w_j$。\n- 样本是有放回地抽取的。\n\n你的任务是编写一个完整、可运行的程序，该程序：\n1. 构建所述环境，并通过从指定的混合分布中抽取奖励，用 $N$ 个独立的过渡填充一个回放缓冲区。为了可复现性，使用一个固定的随机种子 $s$。\n2. 根据生成奖励的混合模式，标记每个存储的过渡。将绝对TD误差幅度较小的模式定义为“次要模式”，其幅度为 $m_{\\mathrm{small}} = \\min\\{\\lvert r_{\\mathrm{small}} + (\\gamma - 1) q_0 \\rvert, \\lvert r_{\\mathrm{big}} + (\\gamma - 1) q_0 \\rvert\\}$，另一个幅度较大的模式定义为“主要模式”，其幅度为 $m_{\\mathrm{big}}$。\n3. 使用 $p_i \\propto (\\lvert \\delta_i \\rvert + \\varepsilon)^{\\alpha}$ 从缓冲区中执行 $M$ 次有放回的优先采样。\n4. 计算采样到的过渡中来自次要模式的观测比例 $\\hat{f}_{\\mathrm{minor}}$。\n5. 将每个测试用例的 $\\hat{f}_{\\mathrm{minor}}$ 报告为一个精确到 $6$ 位小数的浮点数。\n\n基于原理的推理目标：从上述TD误差定义和优先权重规则出发，推理预期的次要模式采样比例如何依赖于混合概率和两种模式的幅度。通过蒙特卡洛采样以编程方式估计该数量并报告结果。\n\n测试套件和参数化。在以下四个参数集上运行程序，每个参数集指定为一个元组 $(N, M, p_{\\mathrm{small}}, r_{\\mathrm{small}}, r_{\\mathrm{big}}, \\alpha, \\varepsilon, \\gamma, q_0, s)$：\n- 案例 A (边界情况，均匀回放): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.2,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 3,\\; \\alpha = \\; 0,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n- 案例 B (理想路径，中度优先化): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.5,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 5,\\; \\alpha = \\; 1,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n- 案例 C (边缘情况，导致模式坍塌的强优先化): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 0,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n- 案例 D (边缘情况，添加 $\\varepsilon$ 以缓解坍塌): $(\\; N = \\; 50000,\\; M = \\; 50000,\\; p_{\\mathrm{small}} = \\; 0.8,\\; r_{\\mathrm{small}} = \\; -1,\\; r_{\\mathrm{big}} = \\; 20,\\; \\alpha = \\; 2,\\; \\varepsilon = \\; 20,\\; \\gamma = \\; 0,\\; q_0 = \\; 0,\\; s = \\; 42 \\;)$。\n\n注意：\n- 因为在所有案例中 $q_0 = 0$ 且 $\\gamma = 0$，TD误差简化为 $\\delta = r$。\n- 案例A测试边界情况 $\\alpha = 0$，此时采样必须在缓冲区上是均匀的，与幅度无关。\n- 案例B展示了在中度优先化下，采样偏向于更大幅度模式的现象。\n- 案例C展示了“模式坍塌”，此处具体表现为当 $\\alpha$ 很大且幅度比很大时，$\\hat{f}_{\\mathrm{minor}}$ 非常小。\n- 案例D展示了使 $\\varepsilon$ 与更大幅度相当可以通过平滑化优先级来缓解坍塌。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个逗号分隔的Python风格列表，列表内容为四个四舍五入的次要模式比例 $[\\hat{f}_{\\mathrm{minor}}^{(A)}, \\hat{f}_{\\mathrm{minor}}^{(B)}, \\hat{f}_{\\mathrm{minor}}^{(C)}, \\hat{f}_{\\mathrm{minor}}^{(D)}]$，分别对应案例A、B、C和D。例如，如果计算出的值是这些，那么形如 $[0.200000,0.166700,0.010000,0.524000]$ 的输出是可以接受的。", "solution": "该问题是有效的，因为它在深度强化学习领域内提出了一个定义明确、具有科学依据的模拟任务。所有参数和定义都已提供，目标清晰明确。\n\n目标是在一个最小的马尔可夫决策过程 (MDP) 中，演示由优先经验回放 (Prioritized Experience Replay, PER) 引起的采样偏差。该MDP构建为只有一个状态和一个动作，因此动作价值函数 $Q(s, a)$ 是一个固定常数 $q_0$。每一步的奖励 $r$ 是随机的，从一个两点混合分布中抽取：以概率 $p_{\\mathrm{small}}$ 得到 $r = r_{\\mathrm{small}}$，以概率 $p_{\\mathrm{big}} = 1 - p_{\\mathrm{small}}$ 得到 $r = r_{\\mathrm{big}}$。\n\n分析的核心在于时间差分 (TD) 误差 $\\delta$，它是计算PER中优先级的依据。对于给定的一个过渡，TD误差由贝尔曼方程定义：\n$$\n\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\n$$\n在我们简化的单状态MDP中，下一个状态 $s'$ 与当前状态 $s$ 相同，且只有一个动作。Q值固定为 $q_0$。因此，方程简化为：\n$$\n\\delta = r + (\\gamma - 1)q_0\n$$\n由于奖励 $r$ 是双峰的，TD误差 $\\delta$ 也是双峰的，有两个可能的值：\n$$\n\\delta_{\\mathrm{small}} = r_{\\mathrm{small}} + (\\gamma - 1)q_0\n$$\n$$\n\\delta_{\\mathrm{big}} = r_{\\mathrm{big}} + (\\gamma - 1)q_0\n$$\n该问题根据这些TD误差的绝对幅度定义了两种模式。“次要模式”对应较小的幅度，$m_{\\mathrm{minor}} = \\min(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$，“主要模式”对应较大的幅度，$m_{\\mathrm{major}} = \\max(|\\delta_{\\mathrm{small}}|, |\\delta_{\\mathrm{big}}|)$。\n\n在PER中，从大小为 $N$ 的回放缓冲区中采样过渡，其概率与它们的优先级成正比。具有TD误差 $\\delta_i$ 的过渡 $i$ 的优先级 $w_i$ 由下式给出：\n$$\nw_i = (|\\delta_i| + \\varepsilon)^{\\alpha}\n$$\n其中 $\\alpha \\ge 0$ 是优先化指数，$\\varepsilon \\ge 0$ 是一个防止优先级为零的小常数。采样过渡 $i$ 的概率为：\n$$\np_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j}\n$$\n设回放缓冲区包含 $N_{\\mathrm{minor}}$ 个来自次要模式的过渡和 $N_{\\mathrm{major}}$ 个来自主要模式的过渡，其中 $N_{\\mathrm{minor}} + N_{\\mathrm{major}} = N$。这些模式的优先级分别为 $w_{\\mathrm{minor}} = (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$ 和 $w_{\\mathrm{major}} = (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}$。缓冲区中优先级的总和为 $W = N_{\\mathrm{minor}} w_{\\mathrm{minor}} + N_{\\mathrm{major}} w_{\\mathrm{major}}$。\n\n在单次抽样中，选取任何一个来自次要模式的过渡的理论概率是它们各自概率的总和，可以简化为：\n$$\nP(\\text{sample is minor}) = \\frac{N_{\\mathrm{minor}} \\cdot w_{\\mathrm{minor}}}{W} = \\frac{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}}{N_{\\mathrm{minor}} \\cdot (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha} + N_{\\mathrm{major}} \\cdot (m_{\\mathrm{major}} + \\varepsilon)^{\\alpha}}\n$$\n这个方程揭示了采样偏差的来源。\n- 当 $\\alpha = 0$ 时，$w_{\\mathrm{minor}} = w_{\\mathrm{major}} = 1$。采样概率变为 $P(\\text{sample is minor}) = N_{\\mathrm{minor}} / (N_{\\mathrm{minor}} + N_{\\mathrm{major}}) = N_{\\mathrm{minor}} / N$。这对应于均匀采样，其中采样比例反映了缓冲区的构成。\n- 当 $\\alpha  0$ 时，如果 $m_{\\mathrm{major}} \\gg m_{\\mathrm{minor}}$，优先级的比率 $w_{\\mathrm{major}}/w_{\\mathrm{minor}}$ 会变得非常大。这会严重地使采样偏向主要模式，即使 $N_{\\mathrm{minor}} \\gg N_{\\mathrm{major}}$。这可能导致“模式坍塌”，即来自次要模式的过渡很少被采样。\n- 一个非零的 $\\varepsilon$，特别是当它与 $m_{\\mathrm{major}}$ 相当时，会减小优先级的比率 $(m_{\\mathrm{major}} + \\varepsilon)^{\\alpha} / (m_{\\mathrm{minor}} + \\varepsilon)^{\\alpha}$，从而缓解采样偏差。\n\n为了解决这个问题，我们为每个测试用例执行一个蒙特卡洛模拟：\n1. 实例化一个包含 $N$ 个过渡的回放缓冲区。对于每个过渡，从指定的两点分布中抽取一个奖励。我们使用一个固定的随机种子 $s$ 以保证可复现性。\n2. 根据每个案例的参数，我们确定哪个奖励（$r_{\\mathrm{small}}$ 或 $r_{\\mathrm{big}}$）产生次要TD误差模式。缓冲区中的每个过渡都相应地被标记。对于所有给定的测试用例，$\\gamma = 0$ 且 $q_0 = 0$，因此TD误差与奖励相同，即 $\\delta = r$。因此，次要模式就是绝对值较小的奖励所对应的模式。\n3. 使用公式 $w_i = (|\\delta_i| + \\varepsilon)^{\\alpha}$ 计算所有 $N$ 个过渡的优先级。\n4. 将这些优先级归一化，形成一个在 $N$ 个过渡上的采样概率分布。\n5. 根据此分布，从缓冲区中有放回地抽取 $M$ 个样本。\n6. 计算这 $M$ 个样本中属于次要模式的比例 $\\hat{f}_{\\mathrm{minor}}$。这个比例作为 $P(\\text{sample is minor})$ 的数值估计。\n\n此过程被系统地应用于所有四个测试用例，以量化参数 $\\alpha$ 和 $\\varepsilon$ 对采样多样性的影响。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a Monte Carlo simulation to demonstrate the effect of Prioritized\n    Experience Replay (PER) on sampling bias in a simplified MDP.\n    \"\"\"\n    test_cases = [\n        # Case A: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.2, -1, 3, 0, 0, 0, 0, 42),\n        # Case B: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.5, -1, 5, 1, 0, 0, 0, 42),\n        # Case C: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 0, 0, 0, 42),\n        # Case D: (N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s)\n        (50000, 50000, 0.8, -1, 20, 2, 20, 0, 0, 42),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, M, p_small, r_small, r_big, alpha, epsilon, gamma, q0, s = case\n\n        # 1. Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(s)\n\n        # 2. Fill the replay buffer with N transitions.\n        # Draw N random numbers to determine the reward for each transition.\n        reward_choices = rng.random(size=N)\n        # Assign rewards based on the mixture probability p_small.\n        rewards = np.where(reward_choices  p_small, r_small, r_big)\n\n        # 3. Calculate TD errors and identify which transitions belong to the minor mode.\n        td_error_for_r_small = r_small + (gamma - 1) * q0\n        td_error_for_r_big = r_big + (gamma - 1) * q0\n\n        # An array indicating which transitions were generated by the r_small component.\n        is_from_small_reward_source = (rewards == r_small)\n\n        # Determine which reward source corresponds to the minor TD error mode.\n        if np.abs(td_error_for_r_small)  np.abs(td_error_for_r_big):\n            # The minor mode corresponds to r_small.\n            is_minor_mode = is_from_small_reward_source\n        else:\n            # The minor mode corresponds to r_big.\n            is_minor_mode = ~is_from_small_reward_source\n\n        # Calculate the TD error for each transition in the buffer.\n        td_errors = rewards + (gamma - 1) * q0\n\n        # 4. Compute sampling priorities and probabilities.\n        priorities = (np.abs(td_errors) + epsilon)**alpha\n        \n        total_priority = np.sum(priorities)\n        \n        if total_priority > 0:\n            sampling_probs = priorities / total_priority\n        else:\n            # Fallback to uniform sampling if all priorities are zero.\n            # This can happen if alpha > 0 and |delta_i| + epsilon is 0 for all i.\n            # Not expected for the given test cases.\n            sampling_probs = np.full(N, 1.0 / N)\n\n        # 5. Perform M prioritized samples with replacement.\n        sampled_indices = rng.choice(N, size=M, replace=True, p=sampling_probs)\n\n        # 6. Compute the observed fraction of sampled transitions from the minor mode.\n        num_minor_sampled = np.sum(is_minor_mode[sampled_indices])\n        f_minor_hat = num_minor_sampled / M\n        \n        results.append(f_minor_hat)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113071"}, {"introduction": "真正的强化学习应用常常需要我们超越标准算法，进行定制化设计。本练习挑战你分析一种新颖的“年龄感知”优先回放机制，该机制试图在转换的重要性（由TD误差衡量）和其新近度之间取得平衡。通过这个过程，你将学会使用学习速度、稳定性、有效样本量等关键指标来评估不同算法设计之间的权衡 [@problem_id:3113068]。", "problem": "考虑使用经验回放 (ER) 训练 Deep Q-Network (DQN) 的设定。假设一个回放缓冲区包含 $N$ 个转移，索引为 $i \\in \\{1, \\dots, N\\}$。每个转移都有一个时间差分误差 $\\delta_i$ 和一个关联的年龄 $\\text{age}_i$，该年龄以转移被存储后所经过的环境步数来衡量。我们提出一种年龄感知的优先回放方法，其中转移 $i$ 的未归一化优先级由下式给出：\n$$\ns_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i},\n$$\n其中 $\\alpha \\ge 0$ 控制优先级的程度，$\\lambda \\ge 0$ 惩罚较旧的转移。采样概率为\n$$\np_i = \\frac{s_i}{\\sum_{j=1}^{N} s_j},\n$$\n约定当 $\\sum_{j=1}^{N} s_j = 0$ 时，对所有 $i$ 都有 $p_i = \\frac{1}{N}$。\n\n为减轻非均匀采样带来的偏差，我们使用重要性采样权重。令 $\\beta \\in [0,1]$ 控制修正的强度。定义未归一化的权重为\n$$\nw_i = (N \\, p_i)^{-\\beta}\n$$\n适用于 $p_i  0$ 的情况，当 $p_i = 0$ 时 $w_i = 0$。为保证数值稳定性并避免不受控制的步长缩放，定义归一化的权重为\n$$\n\\hat{w}_i = \n\\begin{cases}\n\\frac{w_i}{\\max_{k: p_k  0} w_k}  \\text{if } \\max_{k: p_k  0} w_k  0, \\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\n\n我们分析两个核心方面：\n- 学习速度代理指标 $S$，定义为在采样分布下期望的绝对更新幅度，\n$$\nS = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, |\\delta_i|.\n$$\n- 稳定性代理指标 $V$，定义为带符号更新的方差，\n$$\n\\mu = \\sum_{i=1}^{N} p_i \\, \\hat{w}_i \\, \\delta_i, \\quad\nV = \\sum_{i=1}^{N} p_i \\left(\\hat{w}_i \\, \\delta_i - \\mu \\right)^2.\n$$\n\n此外，报告有效样本量 (ESS) 和一个异常值敏感度因子：\n$$\n\\text{ESS} = \\frac{1}{\\sum_{i=1}^{N} p_i^2}, \\quad\nO = \\frac{\\max_{i} \\hat{w}_i}{\\frac{1}{N} \\sum_{i=1}^{N} \\hat{w}_i}.\n$$\nESS 量化了采样分布的多样性，而 $O$ 则量化了最大归一化权重相对于平均归一化权重的潜在主导作用。\n\n从 DQN 使用随机梯度下降最小化均方贝尔曼误差以及重要性采样在非均匀采样下修正梯度估计期望这一基本原理出发，推导 $S$ 和 $V$ 如何与 $p_i$ 和 $\\hat{w}_i$ 相关联。实现一个程序，对下面的每个测试用例，计算并返回 $(S, V, \\text{ESS}, O)$。\n\n测试套件：\n- 用例 1 (一般正常路径)：$N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 0.5$, $\\beta = 0.4$.\n- 用例 2 (均匀基线)：$N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0$, $\\lambda = 0$, $\\beta = 0$.\n- 用例 3 (强年龄惩罚)：$N=6$, $\\delta = [\\,0.3,\\,-0.1,\\,0.05,\\,0.8,\\,-0.4,\\,0.2\\,]$, $\\text{age} = [\\,1,\\,5,\\,0,\\,2,\\,10,\\,3\\,]$, $\\alpha = 0.6$, $\\lambda = 2.0$, $\\beta = 0.4$.\n- 用例 4 (异常时间差分误差)：$N=6$, $\\delta = [\\,0.01,\\,0.02,\\,0.01,\\,5.0,\\,0.0,\\,-0.01\\,]$, $\\text{age} = [\\,0,\\,1,\\,0,\\,0,\\,50,\\,2\\,]$, $\\alpha = 1.0$, $\\lambda = 0.1$, $\\beta = 0.6$.\n- 用例 5 (退化的零优先级)：$N=4$, $\\delta = [\\,0,\\,0,\\,0,\\,0\\,]$, $\\text{age} = [\\,0,\\,10,\\,20,\\,30\\,]$, $\\alpha = 0.5$, $\\lambda = 1.0$, $\\beta = 0.5$.\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。顺序必须为 $[S_1,V_1,\\text{ESS}_1,O_1,S_2,V_2,\\text{ESS}_2,O_2,S_3,V_3,\\text{ESS}_3,O_3,S_4,V_4,\\text{ESS}_4,O_4,S_5,V_5,\\text{ESS}_5,O_5]$，其中下标表示用例索引。所有值必须是浮点数。", "solution": "所提出的问题是关于深度Q网络 (DQN) 中优先经验回放机制分析的一个定义明确且有科学依据的练习。它提供了一套完整且一致的定义和数据，从而可以得出一个唯一且可验证的解。因此，我们可以进行完整的求解。\n\n问题的核心是要求我们理解在用于优先回放的重要性采样修正的背景下，学习速度代理指标 $S$ 和稳定性代理指标 $V$ 的作用。为此，我们必须首先回顾 DQN 中学习的基本原理。\n\nDQN 的训练目标是最小化转移分布 $\\mathcal{D}$ 上的均方贝尔曼误差 (MSBE)。对于单个转移 $i$（由状态 $s_i$、动作 $a_i$、奖励 $r_i$ 和下一状态 $s'_i$ 组成），其损失函数为 $L_i(\\theta) = \\delta_i^2$，其中时间差分 (TD) 误差 $\\delta_i = (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-)) - Q(s_i, a_i; \\theta)$，$\\theta$ 是在线网络的权重，$\\theta^-$ 是目标网络的权重。\n\n训练通过随机梯度下降 (SGD) 进行。损失函数关于在线网络权重 $\\theta$ 对转移 $i$ 的梯度是：\n$$\n\\nabla_\\theta L_i(\\theta) = \\nabla_\\theta \\left( (r_i + \\gamma \\max_{a'} Q(s'_i, a'; \\theta^-) - Q(s_i, a_i; \\theta))^2 \\right) \\approx -2\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)\n$$\n在这里，我们将目标项视为一个常数，这是 Q-learning 中的一种标准做法。因此，SGD 的更新规则是 $\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)$，其中 $\\eta$ 是学习率。对于单个样本 $i$，更新量与 $\\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$ 成正比。\n\n如果我们从大小为 $N$ 的回放缓冲区中均匀采样转移，则采样到任何转移 $i$ 的概率是 $1/N$。一个小批量上的期望梯度是整个缓冲区上真实梯度的无偏估计。然而，均匀采样效率低下，因为它对所有转移都给予同等重要性，包括那些误差很小、对学习贡献甚微的转移。\n\nPrioritized Experience Replay (PER) 通过非均匀地采样转移来解决这个问题，其采样概率 $p_i$ 与 TD 误差的绝对值 $|\\delta_i|$ 单调相关。在这个问题中，采样概率 $p_i$ 是从得分 $s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$ 导出的。这引入了偏差：在此采样分布 $p_i$ 下的梯度期望不再与真实的梯度期望相匹配。\n$$\n\\mathbb{E}_{i \\sim p}[\\nabla_\\theta L_i] = \\sum_{i=1}^{N} p_i \\nabla_\\theta L_i \\neq \\frac{1}{N} \\sum_{i=1}^{N} \\nabla_\\theta L_i = \\mathbb{E}_{i \\sim U(1/N)}[\\nabla_\\theta L_i]\n$$\n为了修正这种偏差，我们采用重要性采样 (IS)。每个样本 $i$ 的更新通过目标分布（均匀分布）与采样分布的比率 $\\frac{1/N}{p_i}$ 进行加权。样本 $i$ 的损失变为 $L_i^{IS}(\\theta) = \\frac{1}{N p_i} \\delta_i^2$。然而，使用这些原始权重可能导致不稳定性。因此，有两种标准的修改方法：\n1. 将权重提升到 $\\beta \\in [0, 1]$ 次幂，得到 $w_i = (N p_i)^{-\\beta}$。此参数允许在均匀采样（$\\beta=0$，此时 $w_i=1$）和完全的重要性采样修正（$\\beta=1$）之间进行平滑插值。这引入了可控量的偏差，以换取方差的显著降低。\n2. 权重通过它们的最大值进行归一化，即 $\\hat{w}_i = w_i / \\max_k w_k$。这确保了最大权重上限为 1，防止任何单个转移主导梯度更新，从而破坏学习过程的稳定性。\n\n因此，样本 $i$ 的最终梯度更新现在与 $\\hat{w}_i \\delta_i \\nabla_\\theta Q(s_i, a_i; \\theta)$ 成正比。项 $\\hat{w}_i \\delta_i$ 可以被看作是驱动样本 $i$ 学习的、有效的、修正后的误差信号。\n\n在此基础上，我们现在可以证明代理指标 $S$ 和 $V$ 定义的合理性。\n- **学习速度代理指标 $S$**：样本 $i$ 的参数更新幅度与 $|\\hat{w}_i \\delta_i| = \\hat{w}_i |\\delta_i|$（因为 $\\hat{w}_i \\ge 0$）成正比。当以概率 $p_i$ 进行采样时，一个更新步骤的期望幅度是该数量在采样分布下的期望：\n$$\n\\mathbb{E}_{i \\sim p}[\\text{update magnitude}] \\propto \\sum_{i=1}^N p_i \\hat{w}_i |\\delta_i| = S\n$$\n因此，$S$ 表示梯度步骤的期望大小。较大的 $S$ 值表明，平均而言，网络正在对其权重进行更大的调整，这是加速学习的一个合理代理指标。\n\n- **稳定性代理指标 $V$**：SGD 的稳定性关键地依赖于梯度估计的方差。高方差可能导致学习过程振荡或发散。项 $\\hat{w}_i \\delta_i$ 是表示采样转移 $i$ 的带符号修正信号的随机变量。量 $V$ 定义为该随机变量在采样分布 $p_i$ 下的方差：\n$$\n\\mu = \\mathbb{E}[\\hat{w} \\delta] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i)\n$$\n$$\nV = \\text{Var}(\\hat{w} \\delta) = \\mathbb{E}[(\\hat{w} \\delta - \\mu)^2] = \\sum_{i=1}^N p_i (\\hat{w}_i \\delta_i - \\mu)^2\n$$\n较小的 $V$ 值表示修正信号在不同样本间更加一致，这是一个更稳定和可靠的学习过程的代理指标。\n\n另外两个指标提供了额外的诊断信息：\n- **有效样本量 (ESS)**：ESS 衡量样本的多样性。对于均匀分布 ($p_i = 1/N$)，$\\text{ESS} = N$。对于一个高度偏斜的分布，其中某个 $p_k \\approx 1$，ESS 接近于 1。低的 ESS 表示回放正在重复地采样一小部分转移，这可能导致过拟合和泛化能力差。\n- **异常值敏感度因子 $O$**：该指标量化了最大归一化权重 $\\hat{w}_i$ 与平均值相比的主导程度。高的 $O$ 值表示一个或少数几个样本对更新有不成比例的巨大影响，这可能是不稳定的一个来源。\n\n对于给定的测试用例，计算这四个指标的算法如下：\n1.  给定 $N, \\{\\delta_i\\}, \\{\\text{age}_i\\}, \\alpha, \\lambda, \\beta$。\n2.  计算每个转移 $i$ 的未归一化优先级：$s_i = \\frac{|\\delta_i|^\\alpha}{1 + \\lambda \\, \\text{age}_i}$。\n3.  计算优先级之和，$S_{tot} = \\sum_{j=1}^N s_j$。\n4.  如果 $S_{tot} = 0$，则将采样概率设为均匀分布：对所有 $i$，$p_i = 1/N$。否则，进行归一化：$p_i = s_i / S_{tot}$。\n5.  计算未归一化的重要性采样权重 $w_i$。对于每个 $p_i  0$ 的 $i$，设 $w_i = (N p_i)^{-\\beta}$。对于 $p_i = 0$ 的 $i$，设 $w_i = 0$。\n6.  计算最大权重，$w_{max} = \\max_k w_k$。\n7.  如果 $w_{max}  0$，计算归一化权重 $\\hat{w}_i = w_i / w_{max}$。否则，对所有 $i$，设 $\\hat{w}_i = 0$。\n8.  使用它们的定义计算四个指标：\n    - $S = \\sum_{i=1}^{N} p_i \\hat{w}_i |\\delta_i|$。\n    - $\\mu = \\sum_{i=1}^{N} p_i \\hat{w}_i \\delta_i$，然后 $V = \\sum_{i=1}^{N} p_i (\\hat{w}_i \\delta_i - \\mu)^2$。\n    - $\\text{ESS} = 1 / (\\sum_{i=1}^{N} p_i^2)$。\n    - $O = (\\max_i \\hat{w}_i) / (\\frac{1}{N}\\sum_{i=1}^N \\hat{w}_i)$，假设分母不为零。\n此过程是确定性的，且在计算上简单明了。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes learning and stability metrics for an age-aware prioritized experience replay scheme.\n    \"\"\"\n    test_cases = [\n        # Case 1: general happy path\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 0.5, 'beta': 0.4},\n        # Case 2: uniform baseline\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.0, 'lambda': 0.0, 'beta': 0.0},\n        # Case 3: strong age penalty\n        {'N': 6, 'delta': [0.3, -0.1, 0.05, 0.8, -0.4, 0.2], 'age': [1, 5, 0, 2, 10, 3], 'alpha': 0.6, 'lambda': 2.0, 'beta': 0.4},\n        # Case 4: outlier temporal-difference error\n        {'N': 6, 'delta': [0.01, 0.02, 0.01, 5.0, 0.0, -0.01], 'age': [0, 1, 0, 0, 50, 2], 'alpha': 1.0, 'lambda': 0.1, 'beta': 0.6},\n        # Case 5: degenerate zero priorities\n        {'N': 4, 'delta': [0.0, 0.0, 0.0, 0.0], 'age': [0, 10, 20, 30], 'alpha': 0.5, 'lambda': 1.0, 'beta': 0.5},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        delta = np.array(case['delta'], dtype=float)\n        age = np.array(case['age'], dtype=float)\n        alpha = case['alpha']\n        lambda_ = case['lambda']\n        beta = case['beta']\n\n        # 1. Calculate unnormalized priorities s_i\n        # np.power(0.0, 0.0) correctly returns 1.0, handling the alpha=0 case.\n        s = np.power(np.abs(delta), alpha) / (1.0 + lambda_ * age)\n\n        # 2. Calculate sampling probabilities p_i\n        sum_s = np.sum(s)\n        if sum_s == 0.0:\n            p = np.full(N, 1.0 / N)\n        else:\n            p = s / sum_s\n\n        # 3. Calculate unnormalized importance sampling weights w_i\n        w = np.zeros(N, dtype=float)\n        positive_p_mask = p > 0.0\n        if np.any(positive_p_mask):\n            w[positive_p_mask] = np.power(N * p[positive_p_mask], -beta)\n            \n        # 4. Calculate normalized importance sampling weights w_hat_i\n        w_hat = np.zeros(N, dtype=float)\n        max_w = np.max(w)\n        if max_w > 0.0:\n            w_hat = w / max_w\n            \n        # 5. Calculate metrics S, V, ESS, O\n        # S: Learning speed proxy\n        S = np.sum(p * w_hat * np.abs(delta))\n\n        # V: Stability proxy (Variance)\n        mu = np.sum(p * w_hat * delta)\n        V = np.sum(p * np.power(w_hat * delta - mu, 2))\n\n        # ESS: Effective Sample Size\n        ESS = 1.0 / np.sum(np.power(p, 2))\n\n        # O: Outlier-sensitivity factor\n        max_w_hat = np.max(w_hat)\n        mean_w_hat = np.mean(w_hat)\n        \n        if mean_w_hat > 0:\n            O = max_w_hat / mean_w_hat\n        else:\n            # This case implies all w_hat are 0, so max_w_hat is also 0.\n            # No single dominant weight exists. A ratio of 1.0 is a reasonable convention.\n            O = 1.0\n\n        results.extend([S, V, ESS, O])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3113068"}]}