{"hands_on_practices": [{"introduction": "在强化学习中，智能体对“状态”的定义至关重要。理想情况下，状态应包含做出最优决策所需的所有信息。然而，在许多现实场景中，智能体只能获得不完整的观测，即处于部分可观测马尔可夫决策过程 (POMDP) 中。本练习 [@problem_id:3163045] 将通过一个精巧的例子，揭示当观测信息不足时 Q 学习将如何失效，并展示如何通过引入简单的“记忆”（如上一步的动作）来增强状态表示，从而让智能体成功学习到看似复杂的循环策略。", "problem": "构建一个有限的、完全指定的马尔可夫决策过程（MDP），并分析基于表格法的动作价值迭代如何检索循环的动作选择。然后，评估在观测混淆的情况下，向智能体的可观测状态添加一个最小的循环记忆是否有助于捕捉此类循环。使用以下规范设置，并将算法解决方案实现为一个完整的程序。\n\n底层环境是一个马尔可夫决策过程（MDP），其状态空间为 $\\mathcal{S} = \\{0,1\\}$，动作空间为 $\\mathcal{A} = \\{0,1\\}$。对于任意时间步 $t$，如果当前状态为 $s_t \\in \\{0,1\\}$ 且智能体采取动作 $a_t \\in \\{0,1\\}$，则转移是确定性的：下一个状态为 $s_{t+1} = 1 - s_t$。奖励函数为：当 $a_t = s_t$ 时 $r_t = 1$，否则 $r_t = 0$。\n\n定义三种智能体侧的“状态表示”（智能体用作其动作价值函数参数的量），每种表示在底层 MDP 保持不变的情况下引入不同的可观测性水平。\n\n- 全状态表示：智能体在每一步都观测到 $x_t = s_t$。\n\n- 混淆表示：无论底层状态 $s_t$ 为何，智能体在所有时间步 $t$ 都观测到相同的符号，即 $x_t = 0$。\n\n- 循环增强（有限记忆）表示：智能体不观测底层状态 $s_t$。取而代之，它观测一个标量记忆 $m_t \\in \\{0,1\\}$，代表上一个采取的动作（其中 $m_{t+1} = a_t$）。在每回合开始时，设置 $m_0 = 0$ 并将初始底层状态设为 $s_0 = 1$。这种对齐确保了交替模式 $a_t = 1 - m_t$ 在原则上能够在此 MDP 下实现最大的长期奖励。\n\n你的程序必须实现一个表格型动作价值学习器，该学习器遵循基于动态规划原理的折扣控制的规范动作价值迭代逻辑。使用具有固定探索率的 $\\epsilon$-贪心策略进行动作选择。使用固定的常数步长进行学习。除了基于智能体状态表示的简单表格外，不要使用任何函数逼近器；不要使用回放缓冲区；不要进行批量更新。\n\n训练和评估协议：\n- 使用固定的伪随机种子，以确保结果可复现。\n- 对于每个测试用例，训练 $N$ 回合，每回合长 $L$ 步。在每回合开始时，将初始底层状态设为 $s_0 = 1$。对于循环增强表示，每回合开始时还需将 $m_0 = 0$。\n- 在训练期间，遵循从当前动作价值估计得出的 $\\epsilon$-贪心策略。\n- 训练后，从与相应表示相同的初始条件开始，评估由学习到的动作价值函数导出的贪心策略（无探索） $T$ 步。报告这 $T$ 步中每步的经验平均奖励，作为一个浮点数值。\n\n所有测试用例统一使用的超参数：\n- 学习率（步长） $\\alpha = 0.1$。\n- 探索率 $\\epsilon = 0.05$。\n- 回合数 $N = 200$。\n- 每回合长度 $L = 200$。\n- 评估时长 $T = 1000$。\n- 伪随机种子设置为你选择的固定整数，并在所有测试用例中保持不变。\n\n测试套件：\n对于下面的每一对 $(\\text{表示}, \\gamma)$，按规定进行训练和评估，并将得到的每步经验平均奖励记录为浮点数。\n\n1. $(\\text{全状态}, \\gamma = 0.0)$\n2. $(\\text{全状态}, \\gamma = 0.9)$\n3. $(\\text{全状态}, \\gamma = 0.99)$\n4. $(\\text{混淆}, \\gamma = 0.0)$\n5. $(\\text{混淆}, \\gamma = 0.9)$\n6. $(\\text{混淆}, \\gamma = 0.99)$\n7. $(\\text{循环}, \\gamma = 0.0)$\n8. $(\\text{循环}, \\gamma = 0.9)$\n9. $(\\text{循环}, \\gamma = 0.99)$\n\n注释与说明：\n- 在全状态表示中，最优平稳策略映射为 $s \\mapsto a = s$，这会随时间产生一个双状态循环，具有周期性的动作模式和最大的平均奖励。\n- 在混淆表示中，智能体在每一步都观测到相同的符号。由于隐藏状态是确定性翻转的，从此混淆观测到动作的最佳平稳映射在此 MDP 下产生的长期平均奖励为 $0.5$。\n- 在循环增强表示中，可观测记忆 $m_t$ 更新为 $m_{t+1} = a_t$。从 $m_0 = 0$ 和 $s_0 = 1$ 开始，最优动作规则 $a_t = 1 - m_t$ 能够引导正确的交替，并实现高平均奖励。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含与上述测试套件相对应的 9 个浮点数，按顺序排列，每个数四舍五入到小数点后三位，并以逗号分隔的列表形式用方括号括起来。例如，输出必须如下所示：\n\"[0.997,0.998,0.999,0.500,0.501,0.500,0.998,0.999,0.999]\"\n\n本问题中的所有数学符号和数字都以 LaTeX 格式指定，并且要求的每个数值答案都没有单位。你的程序的输出必须严格遵循指定的格式和四舍五入规则。", "solution": "该问题要求针对一个给定的确定性马尔可夫决策过程（MDP），在三种不同的状态表示方案下，实现并评估一个表格型动作价值学习智能体。底层 MDP 由状态空间 $\\mathcal{S} = \\{0,1\\}$、动作空间 $\\mathcal{A} = \\{0,1\\}$、确定性转移函数 $s_{t+1} = 1 - s_t$ 以及奖励函数（若动作与状态匹配，即 $a_t = s_t$ 时为 $r_t = 1$，否则为 $r_t = 0$）定义。我们根据智能体感知底层状态 $s_t$ 的能力来分析其性能。\n\n学习算法是动作价值迭代，通常称为 Q-learning。动作价值函数 $Q(x, a)$ 的更新规则如下，其中 $x$ 是智能体的可观测状态，$a$ 是所选动作：\n$$\nQ(x_t, a_t) \\leftarrow Q(x_t, a_t) + \\alpha [r_t + \\gamma \\max_{a'} Q(x_{t+1}, a') - Q(x_t, a_t)]\n$$\n此处，$\\alpha$ 是学习率，$\\gamma$ 是折扣因子，$r_t$ 是在时间步 $t$ 收到的奖励，$x_{t+1}$ 是下一个可观测状态。训练期间的动作通过 $\\epsilon$-贪心策略选择，该策略以概率 $\\epsilon$ 选择随机动作，以概率 $1-\\epsilon$ 选择贪心动作 $\\arg\\max_a Q(x_t, a)$。\n\n该解决方案涉及实现一个统一的框架，该框架可以容纳三种不同的智能体侧表示，每种表示都引入了不同级别的可观测性：\n\n1.  **全状态表示**：智能体的可观测状态是真实的环境状态，因此 $x_t = s_t$。智能体的状态空间是 $\\{0,1\\}$。Q-table 是一个 $2 \\times 2$ 的矩阵，对应于状态 $\\{0,1\\}$ 和动作 $\\{0,1\\}$。最优策略是 $\\pi^*(s) = s$，该策略是平稳的，并且可以在此表示下直接学习。智能体预期会学会在状态 $s=0$ 时选择动作 $a=0$，在状态 $s=1$ 时选择动作 $a=1$。该策略会产生一个状态和动作始终匹配的序列，从而获得恒为 1 的奖励。\n\n2.  **混淆表示**：无论底层状态 $s_t$ 如何，智能体始终观测到相同的符号 $x_t = 0$。智能体的状态空间由单个状态 $\\{0\\}$ 组成。Q-table 是一个 $1 \\times 2$ 的矩阵，表示值 $Q(0,0)$ 和 $Q(0,1)$。由于底层状态 $s_t$ 在每一步都会翻转（从 $s_0=1$ 开始的序列是 $1, 0, 1, 0, \\dots$），任何平稳策略 $a_t = \\text{const}$ 的正确率都恰好是 $50\\%$。例如，如果智能体总是选择 $a_t=1$，奖励序列将是 $1, 0, 1, 0, \\dots$。在这种有限观测下，智能体能做的最好的事情就是坚持一个固定的动作，从而得到 0.5 的平均奖励。\n\n3.  **循环增强**：智能体的可观测状态 $x_t$ 是其自身的上一个动作，即 $m_t = a_{t-1}$。因此，智能体的可观测状态空间为 $\\{0,1\\}$，表示对上一个动作的记忆。初始条件设置为 $s_0=1$ 和 $m_0=0$。这种有限记忆使智能体能够打破前一种情况下的感知混淆。相对于底层状态 $s_t$，最优策略是非平稳的，但相对于智能体的记忆状态 $m_t$ 则是平稳的。最优动作规则是 $a_t = 1 - m_t$。该策略创建了一个交替的动作序列（从 $a_0 = 1-m_0=1$ 开始为 $1, 0, 1, 0, \\dots$），该序列与翻转的环境状态（$1, 0, 1, 0, \\dots$）完美同步，从而在每一步都获得 1 的奖励。智能体的 $2 \\times 2$ Q-table 应学习以反映此策略：$Q(0,1)$ 和 $Q(1,0)$ 将收敛到高值，而 $Q(0,0)$ 和 $Q(1,1)$ 将收敛到低值。\n\n实现遵循指定的协议：对于九个测试用例中的每一个（将三种表示之一与折扣因子 $\\gamma \\in \\{0.0, 0.9, 0.99\\}$ 结合），我们训练一个智能体 $N=200$ 回合，每回合 $L=200$ 步。学习率为 $\\alpha=0.1$，探索率为 $\\epsilon=0.05$。训练后，对学习到的贪心策略（$\\epsilon=0$）进行 $T=1000$ 步的评估，并记录每步的经验平均奖励。在所有运行中使用固定的伪随机种子以确保可复现性。该程序系统地执行这九个测试用例，并将结果格式化为单个逗号分隔的列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a Q-learning agent for a specified MDP under three\n    different state representations and three discount factors.\n    \"\"\"\n\n    # Hyperparameters as defined in the problem statement\n    ALPHA = 0.1\n    EPSILON = 0.05\n    N_EPISODES = 200\n    L_STEPS = 200\n    T_EVAL_STEPS = 1000\n    SEED = 42  # A fixed integer for reproducibility\n\n    # MDP initial conditions\n    S_INITIAL = 1\n    M_INITIAL = 0\n\n    test_cases = [\n        ('full', 0.0),\n        ('full', 0.9),\n        ('full', 0.99),\n        ('aliased', 0.0),\n        ('aliased', 0.9),\n        ('aliased', 0.99),\n        ('recurrent', 0.0),\n        ('recurrent', 0.9),\n        ('recurrent', 0.99),\n    ]\n\n    results = []\n\n    for representation, gamma in test_cases:\n        # --- Agent and Environment Setup ---\n        if representation == 'full':\n            num_obs_states = 2  # Observable states {0, 1}\n        elif representation == 'aliased':\n            num_obs_states = 1  # Single observable state {0}\n        else:  # 'recurrent'\n            num_obs_states = 2  # Memory states {0, 1}\n        \n        num_actions = 2  # Actions {0, 1}\n\n        # Initialize Q-table with zeros\n        q_table = np.zeros((num_obs_states, num_actions))\n        rng = np.random.default_rng(SEED)\n\n        # --- Training Loop ---\n        for _ in range(N_EPISODES):\n            s_t = S_INITIAL\n            if representation == 'recurrent':\n                m_t = M_INITIAL\n\n            for _ in range(L_STEPS):\n                # 1. Determine agent's observable state\n                if representation == 'full':\n                    obs_state = s_t\n                elif representation == 'aliased':\n                    obs_state = 0\n                else:  # 'recurrent'\n                    obs_state = m_t\n                \n                # 2. Choose action using epsilon-greedy policy\n                if rng.random()  EPSILON:\n                    action = rng.integers(0, num_actions)\n                else:\n                    action = np.argmax(q_table[obs_state, :])\n\n                # 3. Get reward and next underlying state from the environment\n                reward = 1 if action == s_t else 0\n                s_t_plus_1 = 1 - s_t\n\n                # 4. Determine agent's next observable state\n                if representation == 'full':\n                    next_obs_state = s_t_plus_1\n                elif representation == 'aliased':\n                    next_obs_state = 0\n                else:  # 'recurrent'\n                    m_t_plus_1 = action\n                    next_obs_state = m_t_plus_1\n                \n                # 5. Update Q-table (action-value iteration)\n                old_value = q_table[obs_state, action]\n                next_max = np.max(q_table[next_obs_state, :])\n                target = reward + gamma * next_max\n                new_value = old_value + ALPHA * (target - old_value)\n                q_table[obs_state, action] = new_value\n\n                # 6. Update states for the next time step\n                s_t = s_t_plus_1\n                if representation == 'recurrent':\n                    m_t = m_t_plus_1\n        \n        # --- Evaluation Loop ---\n        total_eval_reward = 0\n        s_t = S_INITIAL\n        if representation == 'recurrent':\n            m_t = M_INITIAL\n\n        for _ in range(T_EVAL_STEPS):\n            # 1. Determine observable state\n            if representation == 'full':\n                obs_state = s_t\n            elif representation == 'aliased':\n                obs_state = 0\n            else:  # 'recurrent'\n                obs_state = m_t\n\n            # 2. Choose greedy action (no exploration)\n            action = np.argmax(q_table[obs_state, :])\n\n            # 3. Get reward and update total reward\n            reward = 1 if action == s_t else 0\n            total_eval_reward += reward\n            s_t_plus_1 = 1 - s_t\n\n            # 4. Update states for the next time step\n            s_t = s_t_plus_1\n            if representation == 'recurrent':\n                m_t = action\n\n        avg_reward = total_eval_reward / T_EVAL_STEPS\n        results.append(avg_reward)\n    \n    # --- Final Print Statement ---\n    # Format the results as a comma-separated list of floats rounded to 3 decimal places.\n    formatted_results = [f'{r:.3f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3163045"}, {"introduction": "将 Q 学习与函数近似（特别是深度神经网络）相结合是深度强化学习的基石，但也引入了新的挑战。当自举 (bootstrapping)、离策略 (off-policy) 学习和函数近似这“致命三元组”同时存在时，训练过程可能变得极不稳定甚至发散。本练习 [@problem_id:3163145] 将引导您构建一个经典的贝尔德反例 (Baird's counterexample) 的变体，亲手触发并观察这种不稳定性。通过这个过程，您将深刻理解为什么目标网络 (target networks) 和双 Q 学习 (Double Q-learning) 等技术对于现代深度 Q 网络 (DQN) 的成功至关重要。", "problem": "您的任务是设计并运行一个可复现的计算实验，该实验旨在探究在有限马尔可夫决策过程（MDP）中，使用函数逼近时，带自举的半梯度控制的稳定性，其中使用深度线性网络作为动作价值函数的逼近器。您的任务是实现一个类似Baird的、可能表现出发散性的离策略（off-policy）设置，然后测试两种常见的稳定化思想——目标网络和双动作价值学习——是否能改善这种不稳定性。最终答案必须是一个完整的、可运行的程序。\n\n您必须基于以下基本概念开始：\n- 马尔可夫决策过程（MDP）的定义，包括状态集 $\\mathcal{S}$、动作集 $\\mathcal{A}$、转移动态 $P(s' \\mid s,a)$、奖励函数 $R(s,a)$ 和折扣因子 $\\gamma \\in [0,1)$。\n- 针对策略 $\\pi$ 的动作价值函数 $Q^{\\pi}(s,a)$ 的定义，以及作为控制更新基础的动作价值的贝尔曼最优算子。\n- 经过充分检验的半梯度时序差分控制思想（例如深度Q网络（DQN）中的方法，其中构建一个自举目标，并且仅对预测值 $Q_{\\theta}(s,a)$ 求导），以及作为稳定化变体的、同样经过充分检验的目标网络和双Q学习（双DQN使用在线选择和目标评估）思想。\n\n需要实现的问​​题设置：\n1. 环境。使用一个有限MDP，其状态数为 $|\\mathcal{S}|=7$，索引为六个上层状态 $\\{s_0,\\dots,s_5\\}$ 和一个下层状态 $s_6$，以及两个动作 $\\mathcal{A}=\\{a_{\\mathrm{solid}}, a_{\\mathrm{dashed}}\\}$。转移动态是确定性或伪确定性的，具体如下：\n   - 对于任何上层状态 $s_i$，$i \\in \\{0,\\dots,5\\}$：\n     - 如果采取动作 $a_{\\mathrm{solid}}$，下一个状态是 $s_6$。\n     - 如果采取动作 $a_{\\mathrm{dashed}}$，下一个状态是从 $\\{s_0,\\dots,s_5\\}$ 中均匀随机抽取的上层状态。\n   - 对于下层状态 $s_6$：\n     - 无论采取哪个动作，下一个状态都是从 $\\{s_0,\\dots,s_5\\}$ 中均匀随机抽取的上层状态。\n   - 奖励恒为零，即对于所有 $(s,a)$，$R(s,a)=0$。\n   - 用于采样经验的行为策略是固定的，并且相对于贪婪控制是离策略的：在每一步，以概率 $\\beta=0.95$ 选择 $a_{\\mathrm{dashed}}$，以概率 $1-\\beta=0.05$ 选择 $a_{\\mathrm{solid}}$。\n   - 折扣因子为 $\\gamma \\in [0,1)$，具体值由每个测试用例指定。\n   所有随机选择必须由一个具有固定种子的可复现伪随机数生成器产生。\n\n2. 函数逼近。使用一个深度线性网络来参数化动作价值函数 $Q_{\\theta}(s,a)$，该网络没有非线性层，其架构为：输入维度 $d_{\\mathrm{in}}=14$（用于 $(s,a)$ 的one-hot编码），一个宽度为 $h$ 的隐藏层（选择 $h=4$），以及标量输出。设参数为 $\\theta = \\{W_1 \\in \\mathbb{R}^{14 \\times 4}, W_2 \\in \\mathbb{R}^{4 \\times 1}\\}$，并定义\n   $$Q_{\\theta}(s,a) \\equiv x(s,a)^{\\top} W_1 W_2,$$\n   其中 $x(s,a) \\in \\mathbb{R}^{14}$ 是状态-动作对的one-hot编码。将 $W_1$ 和 $W_2$ 初始化为带有小随机扰动的小正值。\n\n3. 待比较的学习规则。对于从行为策略中采样的每个转移 $(s,a,r,s')$，执行一个半梯度步骤以最小化平方时序差分误差，其中自举目标的构建方式在每个变体中都不同：\n   - 普通半梯度控制（类似于没有目标网络的基础DQN）：使用正在更新的相同参数（在线网络）来构建目标，即根据当前的 $Q_{\\theta}$ 选择贪婪动作，并用相同的 $Q_{\\theta}$ 对其进行评估。\n   - 目标网络变体：维护一组独立的目标参数 $\\bar{\\theta}$，该参数每 $C$ 步从 $\\theta$ 进行周期性硬更新（复制）。完全使用 $\\bar{\\theta}$ 来构建目标。梯度仍然只通过 $Q_{\\theta}(s,a)$ 计算。\n   - 双动作价值变体（双DQN）：通过在线参数 $\\theta$ 在 $s'$ 处选择贪婪动作，但使用目标参数 $\\bar{\\theta}$ 来评估该动作的价值，从而构建目标。\n\n4. 发散检测与轨迹总结。在 $T$ 步的训练过程中，定期记录参数范数\n   $$\\|\\theta_t\\|_2 \\equiv \\sqrt{\\|W_{1,t}\\|_F^2 + \\|W_{2,t}\\|_F^2}$$\n   并对 $\\log \\|\\theta_t\\|_2$ 与步数索引 $t$ 拟合一个单变量线性模型来估计斜率。如果最终范数与初始范数之比超过指定的增长因子，并且拟合斜率为正且大于一个小的阈值，或者出现任何数值溢出或非数值（not-a-number），则宣布该次运行为“发散”。精确的阈值由您自行选择，但它们必须是固定常数，并统一应用于所有测试用例。\n\n测试套件和要求的输出：\n实现以下参数用例。每个用例是一个元组 $(\\mathrm{algo}, \\alpha, \\gamma, C, T)$，其中 $\\mathrm{algo} \\in \\{\\text{plain}, \\text{target}, \\text{double}\\}$ 是学习规则，$\\alpha$ 是学习率，$\\gamma$ 是折扣因子， $C$ 是目标参数的硬更新周期（以步为单位，对于plain变体则忽略），$T$ 是训练步数：\n- 案例 1：$(\\text{plain}, 0.20, 0.99, 1, 20000)$\n- 案例 2：$(\\text{target}, 0.20, 0.99, 50, 20000)$\n- 案例 3：$(\\text{target}, 0.01, 0.99, 50, 20000)$\n- 案例 4：$(\\text{double}, 0.05, 0.99, 50, 20000)$\n- 案例 5（无自举传递的边界条件）：$(\\text{plain}, 0.20, 0.00, 1, 10000)$\n\n您的程序必须：\n- 对所有随机性使用固定的伪随机种子。\n- 精确地按照描述实现环境和学习规则。\n- 对于每个用例，根据您统一的标准返回一个布尔值，指示运行是否发散。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含按给定顺序排列的五个案例的结果，结果为逗号分隔的列表，并用方括号括起来。例如，格式必须与以下完全一样\n\"[result1,result2,result3,result4,result5]\"\n其中每个结果都是布尔字面量，即“True”或“False”。输出行中不允许有任何其他文本。", "solution": "问题陈述已经过验证，被认为是有效的。它提出了一个在深度强化学习领域中定义明确、有科学依据且客观的计算实验。任务是实现一个特定的马尔可夫决策过程（MDP）和几种半梯度Q学习算法，以研究使用函数逼近的离策略学习的稳定性。该设置是经典Baird反例的一个变体，旨在引发发散。使用深度线性网络作为函数逼近器是一种现代且切题的表述方式。指令是完整的、一致的，并且可以形式化为一个可运行的程序。\n\n解决方案按以下步骤进行：首先，我们形式化实验的各个组成部分——MDP、函数逼近器和学习规则。其次，我们详细说明模拟循环的实现。最后，我们明确检测发散的标准。\n\n环境是由一个元组 $(\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)$ 定义的有限MDP。\n- 状态集为 $\\mathcal{S} = \\{s_0, s_1, s_2, s_3, s_4, s_5, s_6\\}$，其中 $|\\mathcal{S}|=7$。状态 $\\{s_0, \\dots, s_5\\}$ 被指定为“上层状态”，$s_6$ 被指定为“下层状态”。\n- 动作集为 $\\mathcal{A} = \\{a_{\\text{solid}}, a_{\\text{dashed}}\\}$。\n- 转移动态 $P(s'|s,a)$ 对某些转移是确定性的，而对其他转移是随机性的：\n  - 对于一个上层状态 $s \\in \\{s_0, \\dots, s_5\\}$：\n    - 采取 $a_{\\text{solid}}$ 会导致 $s' = s_6$。\n    - 采取 $a_{\\text{dashed}}$ 会导致下一个状态 $s'$ 从 $\\{s_0, \\dots, s_5\\}$ 中均匀随机抽取。\n  - 对于下层状态 $s_6$，任何动作都会导致下一个状态 $s'$ 从 $\\{s_0, \\dots, s_5\\}$ 中均匀随机抽取。\n- 对于所有转移，奖励函数恒为零：$R(s,a,s')=0$。\n- 折扣因子是一个参数 $\\gamma \\in [0, 1)$，由每个测试用例指定。\n- 用于生成经验的行为策略 $\\pi_b$ 是固定的，以概率 $\\beta=0.95$ 选择 $a_{\\text{dashed}}$，以概率 $1-\\beta=0.05$ 选择 $a_{\\text{solid}}$。\n\n动作价值函数 $Q(s,a)$ 由一个带有参数 $\\theta = \\{W_1, W_2\\}$ 的深度线性网络逼近。该网络有一个宽度为 $h=4$ 的隐藏层。\n- 输入是一个one-hot向量 $x(s,a) \\in \\mathbb{R}^{14}$，代表 $7 \\times 2 = 14$ 个可能的状态-动作对之一。\n- 网络参数是两个矩阵：$W_1 \\in \\mathbb{R}^{14 \\times 4}$ 和 $W_2 \\in \\mathbb{R}^{4 \\times 1}$。\n- 动作价值计算为 $Q_{\\theta}(s,a) = x(s,a)^{\\top} W_1 W_2$。\n\n学习过程涉及使用基于单个采样转移 $(s_t, a_t, r_t, s_{t+1})$（其中 $r_t=0$）的半梯度方法来更新参数 $\\theta$。更新规则是针对平方TD误差的随机梯度下降步骤：\n$$ \\theta_{t+1} \\leftarrow \\theta_t - \\alpha \\cdot (Q_{\\theta_t}(s_t, a_t) - Y_t) \\cdot \\nabla_{\\theta_t} Q_{\\theta_t}(s_t, a_t) $$\n这里，$\\alpha$ 是学习率，$Y_t$ 是自举目标。算法之间的关键区别在于 $Y_t$ 的构建方式。设 $\\theta_t$ 为正在更新的在线参数，$\\bar{\\theta}_t$ 为一组独立的目标参数。\n- **普通半梯度控制：** 目标仅使用在线参数构成。\n  $$ Y_t^{\\text{plain}} = r_t + \\gamma \\max_{a'} Q_{\\theta_t}(s_{t+1}, a') $$\n- **目标网络变体：** 目标使用固定的目标网络参数 $\\bar{\\theta}_t$ 构成，这些参数每 $C$ 步更新一次以匹配在线参数 $\\theta_t$（即，如果 $t \\pmod C = 0$，则 $\\bar{\\theta}_{t} \\leftarrow \\theta_{t}$）。\n  $$ Y_t^{\\text{target}} = r_t + \\gamma \\max_{a'} Q_{\\bar{\\theta}_t}(s_{t+1}, a') $$\n- **双动作价值变体：** 使用在线网络选择贪婪动作，但其价值使用目标网络进行估计。这也使用一个每 $C$ 步更新一次的目标网络。\n  $$ Y_t^{\\text{double}} = r_t + \\gamma Q_{\\bar{\\theta}_t}(s_{t+1}, \\arg\\max_{a'} Q_{\\theta_t}(s_{t+1}, a')) $$\n\n由于 $Q_{\\theta}(s,a)$ 是其参数的线性函数（以分解形式），梯度 $\\nabla_{\\theta} Q_{\\theta}(s,a)$ 是关于 $W_1$ 和 $W_2$ 计算的。使用链式法则，我们有：\n$$ \\nabla_{W_1} Q_{\\theta}(s,a) = x(s,a) W_2^{\\top} $$\n$$ \\nabla_{W_2} Q_{\\theta}(s,a) = W_1^{\\top} x(s,a) $$\n鉴于 $x(s,a)$ 是一个在索引 $k$ 处为 $1$ 的one-hot向量，梯度更新得以简化。对 $W_1$ 的更新只影响其第 $k$ 行，而对 $W_2$ 的更新则取决于 $W_1$ 的第 $k$ 行。\n\n为了评估稳定性，我们监控参数范数 $\\|\\theta_t\\|_2 = \\sqrt{\\|W_{1,t}\\|_F^2 + \\|W_{2,t}\\|_F^2}$ 的轨迹，其中 $\\|\\cdot\\|_F$ 是弗罗贝尼乌斯范数。如果满足以下两个条件之一，则宣布一次运行发散：\n1. 在任何步骤中，参数中检测到数值溢出（`NaN` 或 `inf`）。\n2. 参数表现出持续的指数增长。这在 $T$ 步运行结束时通过检查以下两项是否都为真来评估：\n   a. 最终参数范数与初始参数范数之比超过一个增长因子阈值：$\\|\\theta_T\\|_2 / \\|\\theta_0\\|_2 > G_{\\text{thresh}}$。我们设定 $G_{\\text{thresh}} = 1000$。\n   b. 对数范数 $\\log \\|\\theta_t\\|_2$ 相对于时间步 $t$ 的线性回归产生一个大于阈值的正斜率：$m > m_{\\text{thresh}}$。我们设定 $m_{\\text{thresh}} = 10^{-5}$。为了进行此拟合，范数会定期记录。\n\n对五个指定的测试用例中的每一个都进行计算实验。为了可复现性，所有随机元素（初始权重、环境转移、行为策略选择）都源自一个用固定种子初始化的伪随机数生成器。每个测试用例都会重置种子，以确保条件可比。收集并报告每个用例的布尔结果（是否发散）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# No other libraries are imported, satisfying the constraints.\n\n# --- Problem Constants and Configuration ---\n# MDP parameters\nNUM_STATES = 7\nNUM_ACTIONS = 2\nUPPER_STATES = list(range(6))\nLOWER_STATE = 6\nACTION_SOLID = 0\nACTION_DASHED = 1\nBEHAVIOR_PROB_DASHED = 0.95\n\n# Function approximator architecture\nINPUT_DIM = 14\nHIDDEN_DIM = 4\nOUTPUT_DIM = 1\n\n# Divergence detection parameters\nGROWTH_FACTOR_THRESH = 1000.0\nSLOPE_THRESH = 1e-5\nNORM_LOG_INTERVAL = 100\n\n# Reproducibility\nSEED = 42\n\ndef step_env(state, action, rng):\n    \"\"\"Simulates one step in the MDP.\"\"\"\n    if state in UPPER_STATES:\n        if action == ACTION_SOLID:\n            next_state = LOWER_STATE\n        else:  # ACTION_DASHED\n            next_state = rng.choice(UPPER_STATES)\n    else:  # state == LOWER_STATE\n        next_state = rng.choice(UPPER_STATES)\n    return next_state\n\ndef select_behavior_action(rng):\n    \"\"\"Samples an action from the behavior policy.\"\"\"\n    if rng.random()  BEHAVIOR_PROB_DASHED:\n        return ACTION_DASHED\n    else:\n        return ACTION_SOLID\n\ndef get_q_values(state, W_eff):\n    \"\"\"Computes Q(state, a) for all actions 'a' given an effective weight matrix.\"\"\"\n    return W_eff[2 * state: 2 * state + 2, 0]\n\ndef run_experiment(algo, alpha, gamma, C, T, rng):\n    \"\"\"\n    Runs a single experiment for a given configuration.\n\n    Returns:\n        bool: True if divergence is detected, False otherwise.\n    \"\"\"\n    # 1. Initialize parameters\n    W1 = 0.1 + 0.01 * rng.standard_normal(size=(INPUT_DIM, HIDDEN_DIM))\n    W2 = 0.1 + 0.01 * rng.standard_normal(size=(HIDDEN_DIM, OUTPUT_DIM))\n    \n    W1_target = np.copy(W1)\n    W2_target = np.copy(W2)\n    \n    # 2. Initialize tracking variables\n    initial_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n    norm_history = []\n    steps_history = []\n    has_overflown = False\n    \n    # 3. Training Loop\n    state = rng.choice(UPPER_STATES)\n    for t in range(T):\n        # Sample transition from behavior policy\n        action = select_behavior_action(rng)\n        reward = 0.0\n        next_state = step_env(state, action, rng)\n\n        # Pre-compute effective weight matrices\n        W_online = W1 @ W2\n        W_target = W1_target @ W2_target\n\n        # Get Q-value for the current state-action pair\n        sa_index = 2 * state + action\n        q_sa = W_online[sa_index, 0]\n        \n        # Calculate target value Yt based on algorithm\n        q_next_online = get_q_values(next_state, W_online)\n        target_q_val = 0.0\n        \n        if algo == 'plain':\n            target_q_val = np.max(q_next_online)\n        elif algo == 'target':\n            q_next_target = get_q_values(next_state, W_target)\n            target_q_val = np.max(q_next_target)\n        elif algo == 'double':\n            q_next_target = get_q_values(next_state, W_target)\n            best_action_online = np.argmax(q_next_online)\n            target_q_val = q_next_target[best_action_online]\n\n        td_target = reward + gamma * target_q_val\n        td_error = q_sa - td_target\n\n        # Calculate gradients and perform semi-gradient update\n        grad_wrt_w1_row = W2.T\n        grad_wrt_w2_col = W1[sa_index, :].reshape(HIDDEN_DIM, 1)\n\n        W1[sa_index, :] -= alpha * td_error * grad_wrt_w1_row.flatten()\n        W2 -= alpha * td_error * grad_wrt_w2_col\n\n        # Update target network if applicable\n        if algo in ['target', 'double'] and (t + 1) % C == 0:\n            W1_target = np.copy(W1)\n            W2_target = np.copy(W2)\n        \n        # Check for numerical overflow\n        if not np.all(np.isfinite(W1)) or not np.all(np.isfinite(W2)):\n            has_overflown = True\n            break\n            \n        # Log parameter norm\n        if t % NORM_LOG_INTERVAL == 0:\n            current_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n            norm_history.append(current_norm)\n            steps_history.append(t)\n            \n        # Update state for next iteration\n        state = next_state\n\n    # 4. Divergence Analysis\n    if has_overflown:\n        return True\n\n    if not norm_history: # In case T is very small\n        return False\n\n    final_norm = np.sqrt(np.sum(W1**2) + np.sum(W2**2))\n\n    # Avoid division by zero if initial norm is zero\n    if initial_norm > 1e-9:\n        norm_ratio = final_norm / initial_norm\n    else:\n        norm_ratio = np.inf if final_norm > 1e-9 else 1.0\n\n    # Fit linear model to log(norm) vs. steps\n    log_norms = np.log(np.maximum(norm_history, 1e-9)) # avoid log(0)\n    \n    # np.polyfit returns [slope, intercept] for deg=1\n    if len(steps_history) > 1:\n        slope = np.polyfit(steps_history, log_norms, 1)[0]\n    else:\n        slope = 0.0\n\n    # Apply divergence criteria\n    if norm_ratio > GROWTH_FACTOR_THRESH and slope > SLOPE_THRESH:\n        return True\n        \n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of experiments and print results.\n    \"\"\"\n    # Parameter cases from the problem statement\n    test_cases = [\n        # (algo, alpha, gamma, C, T)\n        ('plain', 0.20, 0.99, 1, 20000),\n        ('target', 0.20, 0.99, 50, 20000),\n        ('target', 0.01, 0.99, 50, 20000),\n        ('double', 0.05, 0.99, 50, 20000),\n        ('plain', 0.20, 0.00, 1, 10000)\n    ]\n\n    results = []\n    for case in test_cases:\n        # Reset the seed for each run to ensure comparable conditions\n        rng = np.random.default_rng(SEED)\n        diverged = run_experiment(*case, rng=rng)\n        results.append(str(diverged))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3163145"}, {"introduction": "在拥有了稳定的学习算法之后，下一个关键问题是如何提高学习效率。经验回放是稳定深度 Q 学习的有效机制，但并非所有经验都具有同等的学习价值。本练习 [@problem_id:3163134] 将带您超越标准的均匀采样，探索如何通过优先回放那些“意外”或高误差的经验（即优先经验回放，PER）来加速学习。您还将设计并评估一种新颖的采样策略，旨在平衡不同误差幅度的样本，以减轻异常值对梯度更新的过度影响，从而进一步提升训练的稳定性。", "problem": "要求您为基于价值的强化学习设计并评估一种经验回放缓冲与采样策略，该策略能显式控制时序差分（TD）误差大小的分布。您的程序必须实现并比较三种用于经验回放缓冲区的采样器，该缓冲区与线性状态-动作价值函数（也称为线性$Q$函数）一同使用。同时，程序需要定量评估一个均衡的TD误差分布是否能减轻离群值导致的梯度支配问题，并稳定$Q$函数的更新过程。该比较必须在一个固定的合成数据集上进行，此数据集由一个具有两个动作和连续特征的马尔可夫决策过程（MDP）生成。\n\n您必须从以下基本概念出发：\n\n- 一个马尔可夫决策过程（MDP）包含一个状态空间 $\\mathcal{S}$、一个动作空间 $\\mathcal{A}$、一个转移核 $p(\\mathbf{s}^{\\prime}\\mid \\mathbf{s}, a)$、一个奖励函数 $r(\\mathbf{s}, a)$ 以及一个折扣因子 $\\gamma \\in (0,1)$。\n- 一个策略的动作价值函数（$Q$函数）为 $Q(\\mathbf{s}, a) = \\mathbb{E}\\big[\\sum_{t=0}^{\\infty} \\gamma^{t} r(\\mathbf{s}_{t}, a_{t}) \\,\\big|\\, \\mathbf{s}_{0}=\\mathbf{s}, a_{0}=a\\big]$。\n- 对于使用目标网络的单步$Q$学习，时序差分（TD）误差定义为 $\\delta = r + \\gamma \\max_{a^{\\prime}} Q_{\\text{tgt}}(\\mathbf{s}^{\\prime}, a^{\\prime}) - Q(\\mathbf{s}, a)$，其中 $Q_{\\text{tgt}}$ 是一个目标$Q$函数，其参数在多次更新中保持不变。\n- 在函数近似中，若使用平方TD误差损失 $\\ell = \\tfrac{1}{2}\\delta^{2}$ 并将目标值视为常数，则梯度步长与 $\\delta \\,\\nabla_{\\boldsymbol{\\theta}} Q(\\mathbf{s}, a; \\boldsymbol{\\theta})$ 成正比。\n\n您的程序必须：\n\n- 构建一个大小为 $N$ 的固定转移数据集 $(\\mathbf{s}, a, r, \\mathbf{s}^{\\prime})$，其中 $\\mathbf{s} \\in \\mathbb{R}^{d}$，包含两个动作 $a \\in \\{0,1\\}$，奖励 $r$ 是动作增广特征的线性函数加上加性噪声。该噪声必须是一种混合模型，包含一个离群值模式，以产生重尾误差情景。动作必须是均匀分布的。每个测试用例的数据集必须只生成一次，并在该测试用例的所有采样器中重复使用，以确保公平比较。为保证可复现性，请使用固定的随机种子。\n- 使用线性$Q$函数 $Q(\\mathbf{s}, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a)$，其特征 $\\boldsymbol{\\phi}(\\mathbf{s}, a) \\in \\mathbb{R}^{2d}$ 的定义方式是：根据动作 $a$ 将 $\\mathbf{s}$ 拼接到前 $d$ 个或后 $d$ 个坐标。将 $\\boldsymbol{\\theta}$ 初始化为零。\n- 使用折扣因子 $\\gamma$、学习率 $\\eta$、小批量大小 $B$、每个采样器总共 $T$ 个参数更新步数，以及目标网络更新周期 $K$ 步。目标网络参数 $\\boldsymbol{\\theta}_{\\text{tgt}}$ 必须每 $K$ 步与 $\\boldsymbol{\\theta}$ 同步一次。\n- 在经验回放缓冲区中，为每个转移维护一个绝对TD误差大小 $|\\delta|$ 的估计值，并在训练期间为所有样本周期性地重新计算此值，以保持采样统计信息的更新。\n\n实现三种在经验回放缓冲区上操作的采样策略：\n\n- 均匀采样（Uniform）：以相等的概率对每个转移进行采样。\n- 比例化优先经验回放（Proportional Prioritized Experience Replay, PER）：以与 $(|\\delta_{i}| + \\varepsilon)^{\\alpha}$ 成正比的概率对转移 $i$ 进行采样，其中 $\\alpha \\in [0,1]$ 控制优先级的强度，$\\varepsilon$ 是一个小的正数，用于避免概率为零。\n- 均衡分位数TD采样器（Balanced-Quantile TD Sampler，待评估的方法）：计算 $|\\delta|$ 分布的经验分位数边界，将其划分为 $Q$ 个等质量的区间（bin），并从每个区间中均匀抽取约 $B/Q$ 个样本（必要时可重复采样），以使小批量数据在TD误差大小上包含均衡的混合。\n\n为每个采样器在训练期间定义并计算以下定量诊断指标：\n\n- 离群值梯度支配度量：在每个更新步骤 $t$，为小批量中的 $B$ 个样本计算每个样本贡献的范数 $\\|\\delta_{i} \\,\\boldsymbol{\\phi}(\\mathbf{s}_{i}, a_{i})\\|_{2}$。令 $k=\\lceil 0.1 B \\rceil$ 为前十分位的数量。令 $G_{t}$ 为前 $k$ 个样本的范数之和除以所有 $B$ 个样本的范数之和（分母加一个微小的正稳定项以避免除以零）。一次运行的支配度量是训练步数后半段（以排除早期瞬态）$G_{t}$ 的均值。\n- $Q$更新稳定性度量：在每个步骤 $t$，计算平均小批量损失 $\\bar{\\ell}_{t} = \\tfrac{1}{2}\\tfrac{1}{B}\\sum_{i=1}^{B}\\delta_{i}^{2}$。一次运行的稳定性度量是初始预热期之后所有步骤中 $\\bar{\\ell}_{t}$ 的样本方差。\n\n对于每个测试用例，您的程序必须为三种采样器独立运行训练，并报告一组三个决策布尔值：\n\n- $b_{1}$：如果均衡分位数采样器相比PER，其梯度支配度降低超过一个小的容差，即 $\\overline{G}_{\\text{balanced}}  \\overline{G}_{\\text{PER}} - \\tau_{g}$，则为True，否则为False。\n- $b_{2}$：如果均衡分位数采样器相比PER，其损失方差降低超过一个小的容差，即 $\\mathrm{Var}(\\bar{\\ell})_{\\text{balanced}}  \\mathrm{Var}(\\bar{\\ell})_{\\text{PER}} - \\tau_{v}$，则为True，否则为False。\n- $b_{3}$：如果 $Q=1$ 的均衡分位数采样器在支配度量上退化为均匀采样（在容差范围内），即 $|\\overline{G}_{\\text{balanced}} - \\overline{G}_{\\text{uniform}}| \\le \\tau_{m}$，则为True，否则为False。对于 $Q \\ne 1$ 的情况，将 $b_{3}$ 设置为False。\n\n所有运行的通用参数设置：\n\n- 特征维度 $d = 4$。\n- 数据集大小 $N = 3000$。\n- 折扣因子 $\\gamma = 0.97$。\n- 学习率 $\\eta = 0.01$。\n- 总更新步数 $T = 400$。\n- 目标网络更新周期 $K = 20$。\n- 所有转移的TD误差表重新计算周期为每 $10$ 步一次。\n- PER参数：指数 $\\alpha = 0.95$，偏移量 $\\varepsilon = 10^{-3}$。\n- 度量计算中的稳定项可在分母中使用 $\\epsilon = 10^{-12}$。\n\n测试套件。您的程序必须执行以下三个测试用例：\n\n- 案例A（重尾噪声）：离群值概率 $p_{\\mathrm{out}} = 0.2$，离群值尺度因子 $s_{\\mathrm{out}} = 8.0$，均衡采样器区间数 $Q = 5$，批量大小 $B = 64$。\n- 案例B（无离群值）：离群值概率 $p_{\\mathrm{out}} = 0.0$，离群值尺度因子 $s_{\\mathrm{out}} = 8.0$，均衡采样器区间数 $Q = 5$，批量大小 $B = 64$。\n- 案例C（退化的均衡采样）：离群值概率 $p_{\\mathrm{out}} = 0.2$，离群值尺度因子 $s_{\\mathrm{out}} = 8.0$，均衡采样器区间数 $Q = 1$，批量大小 $B = 64$。\n\n奖励噪声模型。设基础噪声为标准差 $\\sigma_{\\mathrm{base}} = 0.1$ 的高斯噪声。以概率 $p_{\\mathrm{out}}$，将基础噪声替换为标准差 $\\sigma_{\\mathrm{out}} = s_{\\mathrm{out}} \\cdot \\sigma_{\\mathrm{base}}$ 的高斯噪声。\n\n用于决定布尔值的输出容差：\n\n- 梯度支配容差 $\\tau_{g} = 0.02$。\n- 损失方差容差 $\\tau_{v} = 10^{-5}$。\n- 退化匹配容差 $\\tau_{m} = 0.03$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个包含三个布尔值的列表，顺序为 $[b_{1}, b_{2}, b_{3}]$。例如：`[[True,False,False],[False,False,False],[True,True,True]]`。除此单行外，不得有任何多余的空格或字符。\n\n角度单位不适用。本问题中没有物理单位；所有量均为无单位实数。本问题陈述中的所有数值必须严格按规定使用。", "solution": "问题陈述是有效的。它明确定义了一个强化学习中的计算实验，该实验具有科学依据、客观性，并包含足够的信息以产生唯一、可验证的结果。关于精确的马尔可夫决策过程（MDP）动态的微小模糊之处，通过问题强调在固定的、生成的数据集上比较采样器而得以解决，这允许为这些底层函数做出合理的、固定的选择。\n\n目标是实现并比较三种经验回放采样策略——均匀采样、优先经验回放（PER）和一种提出的均衡分位数TD采样器——以评估它们对Q学习稳定性的影响。评估将基于两个度量：离群值梯度支配度和Q更新损失方差。\n\n首先，我们建立合成数据生成过程。环境是一个马尔可夫决策过程，具有连续状态空间 $\\mathcal{S} \\subseteq \\mathbb{R}^{d}$（其中 $d=4$）和离散动作空间 $\\mathcal{A} = \\{0, 1\\}$。对于每个测试用例，都会生成一个包含 $N=3000$ 个转移 $(\\mathbf{s}, a, r, \\mathbf{s}^{\\prime})$ 的固定数据集。状态向量 $\\mathbf{s}$ 独立地从标准多元正态分布中抽取，即 $\\mathbf{s} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$。动作 $a$ 从 $\\{0, 1\\}$ 中均匀选择。转移动态定义为一个简单的线性变换 $\\mathbf{s}^{\\prime} = \\mathbf{M}_a \\mathbf{s}$，其中 $\\mathbf{M}_0$ 和 $\\mathbf{M}_1$ 是每个测试用例只生成一次的固定随机矩阵，并按 $1/\\sqrt{d}$ 进行缩放，以防止状态范数爆炸。奖励 $r$ 是动作增广特征的线性函数加上噪声：$r(\\mathbf{s}, a) = \\boldsymbol{w}_r^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a) + \\epsilon_r$，其中 $\\boldsymbol{w}_r$ 是一个固定的随机权重向量。噪声 $\\epsilon_r$ 从一个混合模型中抽取：以 $1-p_{\\text{out}}$ 的概率，它来自基础高斯分布 $\\mathcal{N}(0, \\sigma_{\\text{base}}^2)$，其中 $\\sigma_{\\text{base}}=0.1$；以 $p_{\\text{out}}$ 的概率，它来自离群值分布 $\\mathcal{N}(0, \\sigma_{\\text{out}}^2)$，其中 $\\sigma_{\\text{out}} = s_{\\text{out}} \\cdot \\sigma_{\\text{base}}$。这种设置，特别是当 $p_{\\text{out}} > 0$ 时，旨在创建时序差分（TD）误差的重尾分布。在每个测试用例中，所有三种采样器都使用相同的数据集，以确保公平比较。\n\n学习智能体使用线性函数逼近器来表示动作价值函数：$Q(\\mathbf{s}, a; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^{\\top} \\boldsymbol{\\phi}(\\mathbf{s}, a)$。特征向量 $\\boldsymbol{\\phi}(\\mathbf{s}, a) \\in \\mathbb{R}^{2d}$ 的定义方式是，根据动作将状态向量 $\\mathbf{s}$ 拼接到两个位置之一：$\\boldsymbol{\\phi}(\\mathbf{s}, a=0) = [\\mathbf{s}^{\\top}, \\mathbf{0}_d^{\\top}]^{\\top}$ 和 $\\boldsymbol{\\phi}(\\mathbf{s}, a=1) = [\\mathbf{0}_d^{\\top}, \\mathbf{s}^{\\top}]^{\\top}$。参数 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2d}$ 初始化为零。学习通过对均方TD误差 $\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2}\\mathbb{E}[ (r + \\gamma \\max_{a'} Q(\\mathbf{s}', a'; \\boldsymbol{\\theta}_{\\text{tgt}}) - Q(\\mathbf{s}, a; \\boldsymbol{\\theta}))^2 ]$ 进行小批量随机梯度下降来进行。对于一个大小为 $B$ 的小批量，梯度更新为 $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\frac{\\eta}{B} \\sum_{i=1}^{B} \\delta_i \\boldsymbol{\\phi}(\\mathbf{s}_i, a_i)$，其中学习率 $\\eta=0.01$。一个带有参数 $\\boldsymbol{\\theta}_{\\text{tgt}}$ 的目标网络被用来计算TD目标 $y = r + \\gamma \\max_{a'} Q_{\\text{tgt}}(\\mathbf{s}', a')$，该网络每 $K=20$ 步与 $\\boldsymbol{\\theta}$ 同步一次，以稳定学习过程。样本 $i$ 的TD误差为 $\\delta_i = y_i - Q(\\mathbf{s}_i, a_i; \\boldsymbol{\\theta})$。\n\n研究的核心在于三种采样策略：\n1.  **均匀采样**：这是基准策略，其中经验回放缓冲区中的 $N$ 个转移中的每一个都以相等的概率 $1/N$ 被采样。它对任何给定转移的效用不做任何假设。\n2.  **比例化优先经验回放（PER）**：该方法优先考虑智能体能从中学习最多的转移，其假设是TD误差的大小 $|\\delta|$ 是“惊奇度”或学习潜力的良好代理。转移 $i$ 以概率 $P_i \\propto (|\\delta_i| + \\varepsilon)^{\\alpha}$ 被采样，其中优先级指数 $\\alpha=0.95$，小常数 $\\varepsilon=10^{-3}$ 用于确保非零概率。该策略会过采样高误差的转移。为实现此目的，所有 $N$ 个转移的绝对TD误差 $|\\delta_i|$ 每 $10$ 步重新评估一次。\n3.  **均衡分位数TD采样器**：这个提出的策略旨在减轻因PER关注高误差离群值而导致的训练不稳定性风险。它的目标是构建在TD误差大小上具有多样性的小批量数据。其操作方式是根据 $|\\delta_i|$ 的分布将整个经验回放缓冲区划分为 $Q$ 个分位数区间。首先，所有 $N$ 个转移根据其当前的 $|\\delta_i|$ 值进行排序。然后将排序后的索引列表分成 $Q$ 个连续的块，每个块包含 $N/Q$ 个索引，从而形成等质量的区间。为了形成一个大小为 $B$ 的小批量，从这 $Q$ 个区间中的每一个均匀地（可重复）抽取大约 $B/Q$ 个样本。这确保了小批量的梯度是低、中、高误差样本贡献的平均值，从而促进更稳定的更新。在 $Q=1$ 的特殊情况下，单个区间包含所有样本，该方法等价于均匀采样。\n\n为量化每个采样器的性能，在 $T=400$ 个训练步数的后半段（即在 $200$ 步的预热期之后）计算两个度量：\n-   **梯度支配度量（$\\overline{G}$）**：对于每次小批量更新，我们计算每个样本对梯度更新贡献的范数 $\\|\\delta_i \\boldsymbol{\\phi}(\\mathbf{s}_i, a_i)\\|_2$。每步度量 $G_t$ 是前十分位（前 $\\lceil 0.1 B \\rceil$ 个样本）范数之和与该批量中总范数之和的比率。高值表示少数离群样本正在支配梯度。最终度量 $\\overline{G}$ 是评估期间 $G_t$ 的均值。\n-   **Q更新稳定性度量（$\\mathrm{Var}(\\bar{\\ell})$）**：对于每个小批量，计算均方TD误差（损失）：$\\bar{\\ell}_t = \\frac{1}{2B}\\sum_{i=1}^B \\delta_i^2$。在评估期间计算该损失序列的样本方差 $\\mathrm{Var}(\\bar{\\ell}_t)$。较低的方差表明学习动态更稳定和可预测。\n\n对于每个测试用例，三种采样器独立运行，并使用所得度量根据指定的容差（$\\tau_g=0.02$, $\\tau_v=10^{-5}$, $\\tau_m=0.03$）来评估三个布尔条件，这些条件将均衡采样器与PER和均匀采样进行比较。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the reinforcement learning sampler comparison experiment.\n    \"\"\"\n\n    # Common parameters\n    D = 4\n    N = 3000\n    GAMMA = 0.97\n    ETA = 0.01\n    T = 400\n    K = 20\n    TD_RECOMPUTE_PERIOD = 10\n    ALPHA = 0.95\n    EPS_PER = 1e-3\n    EPS_METRIC = 1e-12\n    SIGMA_BASE = 0.1\n    TAU_G = 0.02\n    TAU_V = 1e-5\n    TAU_M = 0.03\n\n    def phi(s, a, d_val):\n        \"\"\"Constructs the action-augmented feature vector.\"\"\"\n        features = np.zeros(2 * d_val)\n        if a == 0:\n            features[:d_val] = s\n        else:\n            features[d_val:] = s\n        return features\n\n    def generate_dataset(n_val, d_val, p_out, s_out, rng):\n        \"\"\"Generates a fixed synthetic dataset of transitions.\"\"\"\n        w_r = rng.standard_normal(size=2 * d_val)\n        m_0 = rng.standard_normal(size=(d_val, d_val)) / np.sqrt(d_val)\n        m_1 = rng.standard_normal(size=(d_val, d_val)) / np.sqrt(d_val)\n        \n        states = rng.standard_normal(size=(n_val, d_val))\n        actions = rng.integers(0, 2, size=n_val)\n        \n        next_states = np.zeros_like(states)\n        rewards = np.zeros(n_val)\n        \n        sigma_out = s_out * SIGMA_BASE\n        \n        for i in range(n_val):\n            s, a = states[i], actions[i]\n            \n            if a == 0:\n                next_states[i] = m_0 @ s\n            else:\n                next_states[i] = m_1 @ s\n            \n            features = phi(s, a, d_val)\n            r_det = w_r @ features\n            \n            noise_std = sigma_out if rng.random()  p_out else SIGMA_BASE\n            noise = rng.normal(0, noise_std)\n            rewards[i] = r_det + noise\n            \n        return states, actions, rewards, next_states\n\n    def run_sampler_experiment(sampler_type, dataset, params, run_rng):\n        \"\"\"Runs the training loop for a given sampler and returns metrics.\"\"\"\n        b_val, q_val = params['B'], params['Q']\n        \n        theta = np.zeros(2 * D)\n        theta_tgt = np.zeros(2 * D)\n        td_errors_abs = np.ones(N)\n\n        g_values = []\n        loss_values = []\n        \n        states, actions, rewards, next_states = dataset\n\n        # Pre-compute features for efficiency\n        phis = np.array([phi(s, a, D) for s, a in zip(states, actions)])\n        phis_next_0 = np.array([phi(s_prime, 0, D) for s_prime in next_states])\n        phis_next_1 = np.array([phi(s_prime, 1, D) for s_prime in next_states])\n\n        for t in range(1, T + 1):\n            if (t - 1) % TD_RECOMPUTE_PERIOD == 0:\n                q_values = phis @ theta\n                q_tgt_next_0 = phis_next_0 @ theta_tgt\n                q_tgt_next_1 = phis_next_1 @ theta_tgt\n                max_q_tgt_next = np.maximum(q_tgt_next_0, q_tgt_next_1)\n                deltas = rewards + GAMMA * max_q_tgt_next - q_values\n                td_errors_abs = np.abs(deltas)\n\n            if sampler_type == 'uniform':\n                batch_indices = run_rng.choice(N, size=b_val, replace=True)\n            elif sampler_type == 'per':\n                probs = (td_errors_abs + EPS_PER) ** ALPHA\n                probs_sum = probs.sum()\n                if probs_sum > 0:\n                    probs /= probs_sum\n                else: # Fallback to uniform if all probabilities are zero\n                    probs = np.full(N, 1/N)\n                batch_indices = run_rng.choice(N, size=b_val, p=probs, replace=True)\n            else: # balanced\n                if q_val == 1:\n                    batch_indices = run_rng.choice(N, size=b_val, replace=True)\n                else:\n                    samples_per_bin_base = b_val // q_val\n                    rem = b_val % q_val\n                    bin_sample_counts = [samples_per_bin_base] * q_val\n                    for i in range(rem): bin_sample_counts[i] += 1\n                    \n                    sorted_indices = np.argsort(td_errors_abs)\n                    partitioned_indices = np.array_split(sorted_indices, q_val)\n                    \n                    batch_indices_list = []\n                    for i in range(q_val):\n                        bin_indices = partitioned_indices[i]\n                        if len(bin_indices) > 0:\n                            chosen = run_rng.choice(bin_indices, size=bin_sample_counts[i], replace=True)\n                            batch_indices_list.append(chosen)\n                    batch_indices = np.concatenate(batch_indices_list)\n\n            batch_phis = phis[batch_indices]\n            batch_rewards = rewards[batch_indices]\n            batch_q_tgt_next_0 = phis_next_0[batch_indices] @ theta_tgt\n            batch_q_tgt_next_1 = phis_next_1[batch_indices] @ theta_tgt\n            batch_max_q_tgt_next = np.maximum(batch_q_tgt_next_0, batch_q_tgt_next_1)\n            batch_q_values = batch_phis @ theta\n            batch_deltas = batch_rewards + GAMMA * batch_max_q_tgt_next - batch_q_values\n\n            grad_sum = (batch_deltas.reshape(-1, 1) * batch_phis).sum(axis=0)\n            theta += (ETA / b_val) * grad_sum\n\n            # Diagnostics\n            grad_contrib_norms = np.linalg.norm(batch_deltas.reshape(-1, 1) * batch_phis, axis=1)\n            k = int(np.ceil(0.1 * b_val))\n            sorted_norms = np.sort(grad_contrib_norms)[::-1]\n            sum_top_k = np.sum(sorted_norms[:k])\n            sum_all = np.sum(sorted_norms)\n            g_t = sum_top_k / (sum_all + EPS_METRIC)\n            g_values.append(g_t)\n            \n            loss_t = 0.5 * np.mean(batch_deltas**2)\n            loss_values.append(loss_t)\n\n            if t % K == 0:\n                theta_tgt = theta.copy()\n\n        burn_in_steps = T // 2\n        mean_g = np.mean(g_values[burn_in_steps:])\n        var_loss = np.var(loss_values[burn_in_steps:], ddof=1) if len(loss_values[burn_in_steps:]) > 1 else 0\n\n        return mean_g, var_loss\n\n    test_cases = [\n        {'p_out': 0.2, 's_out': 8.0, 'Q': 5, 'B': 64, 'id': 'A', 'seed': 123},\n        {'p_out': 0.0, 's_out': 8.0, 'Q': 5, 'B': 64, 'id': 'B', 'seed': 456},\n        {'p_out': 0.2, 's_out': 8.0, 'Q': 1, 'B': 64, 'id': 'C', 'seed': 789},\n    ]\n    \n    all_results = []\n    \n    for case_params in test_cases:\n        rng = np.random.default_rng(case_params['seed'])\n        dataset = generate_dataset(N, D, case_params['p_out'], case_params['s_out'], rng)\n\n        metrics = {}\n        for sampler in ['uniform', 'per', 'balanced']:\n            run_rng = np.random.default_rng(rng.integers(2**32 - 1))\n            params = {'B': case_params['B'], 'Q': case_params['Q']}\n            mean_g, var_loss = run_sampler_experiment(sampler, dataset, params, run_rng)\n            metrics[sampler] = {'g_dom': mean_g, 'loss_var': var_loss}\n\n        g_bal = metrics['balanced']['g_dom']\n        g_per = metrics['per']['g_dom']\n        g_uni = metrics['uniform']['g_dom']\n        v_bal = metrics['balanced']['loss_var']\n        v_per = metrics['per']['loss_var']\n        \n        b1 = bool(g_bal  g_per - TAU_G)\n        b2 = bool(v_bal  v_per - TAU_V)\n        \n        if case_params['Q'] == 1:\n            b3 = bool(abs(g_bal - g_uni) = TAU_M)\n        else:\n            b3 = False\n            \n        all_results.append([b1, b2, b3])\n\n    result_str = ','.join(f'[{b1},{b2},{b3}]' for b1, b2, b3 in all_results)\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3163134"}]}