{"hands_on_practices": [{"introduction": "隐式神经表示（INRs）的核心挑战之一是表征高频细节。标准的多层感知机（MLP）天然存在“频谱偏差”，倾向于学习低频信号。本练习将引导你通过一个具体的编码示例，探索位置编码（Positional Encoding）如何帮助网络克服这一限制，并亲手验证当编码能力不足时，采样理论中的混叠（aliasing）现象是如何发生的。[@problem_id:3136712]", "problem": "考虑使用带有受限位置编码 (Positional Encoding, PE) 的隐式神经表示 (Implicit Neural Representation, INR) 对一维带限信号进行建模的任务。输入坐标 $x$ 位于区间 $[0,1)$ 内，角度必须以弧度为单位处理。目标信号是由 $f_K(x) = \\cos(2\\pi K x)$ 定义的合成条纹图案，其中 $K$ 是一个整数频率，指定了单位区间内的周期数。该 INR 是在一个截断的傅里叶特征图上构建的线性模型，该特征图用作 PE。具体来说，对于选定的截止频率 $K_{\\text{cut}} \\in \\mathbb{N}$，PE 由以下特征向量定义\n$$\n\\phi_{K_{\\text{cut}}}(x) = \\left[1, \\cos(2\\pi \\cdot 1 \\cdot x), \\sin(2\\pi \\cdot 1 \\cdot x), \\ldots, \\cos(2\\pi \\cdot k \\cdot x), \\sin(2\\pi \\cdot k \\cdot x), \\ldots, \\cos(2\\pi \\cdot K_{\\text{cut}} \\cdot x), \\sin(2\\pi \\cdot K_{\\text{cut}} \\cdot x)\\right],\n$$\n并且 INR 预测为\n$$\n\\hat{f}(x) = \\mathbf{w}^{\\top} \\phi_{K_{\\text{cut}}}(x),\n$$\n其中 $\\mathbf{w}$ 是通过最小化训练集上的平方误差得到的。\n\n训练集由 $N$ 个均匀间隔的样本 $x_n = \\frac{n}{N}$（$n = 0,1,\\ldots,N-1$）及其对应的目标值 $y_n = f_K(x_n)$ 组成。模型通过对这 $N$ 个样本进行线性最小二乘法拟合。\n\n您的任务是构建一个玩具示例，以演示由于位置编码不足而导致的混叠现象，并使用此合成条纹图案来量化该现象。请使用以下经过充分检验的推理基础：$[0,1)$ 上傅里叶级数的性质、通过截断傅里叶特征实现的带限表示概念，以及关于均匀采样序列的奈奎斯特采样观点。除这些基本事实外，不要假设任何特殊的恒等式；请通过算法推导出观察到的行为。\n\n对于下方的每个测试用例，您必须：\n- 使用指定的 $K_{\\text{cut}}$ 构建训练设计矩阵，通过最小二乘法拟合 $\\mathbf{w}$，并计算训练均方误差 (Mean Squared Error, MSE)，其定义为\n$$\n\\text{MSE}_{\\text{train}} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(\\hat{f}(x_n) - f_K(x_n)\\right)^2.\n$$\n- 在一个由 $M$ 个点 $x_m = \\frac{m}{M}$（$m = 0,1,\\ldots,M-1$）组成的密集网格上评估拟合模型（其中 $M$ 选为 2 的幂），并计算评估 MSE\n$$\n\\text{MSE}_{\\text{eval}} = \\frac{1}{M} \\sum_{m=0}^{M-1} \\left(\\hat{f}(x_m) - f_K(x_m)\\right)^2.\n$$\n- 通过对 $\\{\\hat{f}(x_m)\\}_{m=0}^{M-1}$ 应用快速傅里叶变换 (Fast Fourier Transform, FFT) 并选择使单边谱幅值最大化的索引 $k^{\\star} \\in \\{1,2,\\ldots,\\lfloor M/2 \\rfloor\\}$，来估计拟合模型的主导频率。将 $k^{\\star}$ 作为整数报告。\n\n使用以下参数值测试套件，其旨在涵盖该现象的不同方面：\n1. 理想情况：$(N, K, K_{\\text{cut}}) = (64, 4, 10)$。\n2. 混叠情况，目标频率超过 PE 截止频率，但在训练网格上可以混叠到可表示的频带内：$(N, K, K_{\\text{cut}}) = (64, 60, 10)$。\n3. 边缘情况，目标频率超过 PE 截止频率且无法混叠到可表示的频带内，导致低通近似：$(N, K, K_{\\text{cut}}) = (64, 53, 10)$。\n4. 训练网格奈奎斯特极限处的边界条件：$(N, K, K_{\\text{cut}}) = (64, 32, 32)$。\n\n将密集评估网格大小设置为 $M = 2048$。对于每个测试用例，生成三个量：$\\text{MSE}_{\\text{train}}$、$\\text{MSE}_{\\text{eval}}$ 和在密集网格上检测到的整数主导频率 $k^{\\star}$。将 MSE 值表示为四舍五入到六位小数的浮点数。不涉及物理单位；角度以弧度为单位。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按测试用例和指标排序如下：\n$$\n\\left[\\text{MSE}_{\\text{train}}^{(1)}, \\text{MSE}_{\\text{eval}}^{(1)}, k^{\\star (1)}, \\text{MSE}_{\\text{train}}^{(2)}, \\text{MSE}_{\\text{eval}}^{(2)}, k^{\\star (2)}, \\text{MSE}_{\\text{train}}^{(3)}, \\text{MSE}_{\\text{eval}}^{(3)}, k^{\\star (3)}, \\text{MSE}_{\\text{train}}^{(4)}, \\text{MSE}_{\\text{eval}}^{(4)}, k^{\\star (4)}\\right].\n$$\n所有量必须由您的程序计算。不允许使用外部输入或文件。在整个过程中，角度必须以弧度处理。", "solution": "用户的请求已经过评估且有效。它提出了一个在深度学习和信号处理领域中定义明确、有科学依据的问题，没有任何不一致或含糊不清之处。我们将提供一个完整的解决方案。\n\n该问题要求分析一种基于截断傅里叶特征图（也称为位置编码 PE）的隐式神经表示 (INR)。目标是通过在不同条件下将此模型拟合到简单正弦信号 $f_K(x) = \\cos(2\\pi K x)$，来演示混叠和其他表示失真。INR 是一个线性模型 $\\hat{f}(x) = \\mathbf{w}^{\\top} \\phi_{K_{\\text{cut}}}(x)$，其中权重 $\\mathbf{w}$ 通过对一组 $N$ 个均匀样本进行线性最小二乘法来确定。\n\n分析的核心基于三个原则：\n1.  **表示能力**：PE $\\phi_{K_{\\text{cut}}}(x)$ 由最高频率为 $K_{\\text{cut}}$ 的正弦基函数组成。INR 作为这些基函数的线性组合，其固有频带被限制在 $[0, K_{\\text{cut}}]$ 范围内。它无法完美表示任何包含高于 $K_{\\text{cut}}$ 频率的信号。\n2.  **最小二乘投影**：最小二乘拟合过程通过将采样目标向量 $\\mathbf{y} = [y_0, \\ldots, y_{N-1}]^\\top$ 投影到由采样基向量（设计矩阵 $\\Phi$ 的列）张成的子空间上，来找到最优权重 $\\mathbf{w}$。当基向量在采样网格上正交时，解简化为对每个基向量的独立投影。\n3.  **采样与混叠**：根据奈奎斯特-香农采样定理，以每单位区间 $N$ 个样本的速率对连续信号进行采样，只能无歧义地捕获最高为奈奎斯特频率 $N/2$ 的频率。频率为 $K  N/2$ 的信号在采样网格上会变得与一个较低频率的混叠信号无法区分。对于在 $x_n = n/N$ 处采样的纯余弦信号 $\\cos(2\\pi K x)$，其值 $\\cos(2\\pi K n/N)$ 与 $\\cos(2\\pi K' n/N)$ 的值相同，其中 $K'$ 是与 $\\pm K$ 模 $N$ 同余的最小幅值频率，即 $K' = \\min_{j \\in \\mathbb{Z}} |K - jN|$。\n\n现在我们将应用此框架来分析每个测试用例。对于所有情况，评估网格的大小为 $M=2048$。\n\n**情况 1：$(N, K, K_{\\text{cut}}) = (64, 4, 10)$**\n- **分析**：目标信号频率为 $K=4$，PE 截止频率为 $K_{\\text{cut}}=10$。由于 $K \\le K_{\\text{cut}}$，目标信号 $f_4(x) = \\cos(2\\pi \\cdot 4 \\cdot x)$ 可以由 PE 基直接表示。最小二乘拟合将为 $\\cos(2\\pi \\cdot 4 \\cdot x)$ 基函数确定正确的非零权重，并为所有其他基函数确定接近零的权重。\n- **预测**：模型将完美重建目标信号。我们预计 $\\text{MSE}_{\\text{train}}$ 和 $\\text{MSE}_{\\text{eval}}$ 都将接近于 $0$（在机器精度范围内）。在密集网格上，拟合模型 $\\hat{f}(x)$ 的主导频率将是 $k^{\\star}=4$。\n\n**情况 2：$(N, K, K_{\\text{cut}}) = (64, 60, 10)$**\n- **分析**：目标频率 $K=60$ 远大于 PE 截止频率 $K_{\\text{cut}}=10$。然而，训练是在一个有 $N=64$ 个样本的网格上进行的。奈奎斯特频率为 $N/2 = 32$。频率 $K=60$ 将会发生混叠。混叠频率为 $K' = |60 - 64| = 4$。在训练网格上，从 $f_{60}(x)$ 采样的数据与从 $f_4(x)$ 采样的数据完全相同：\n$$y_n = \\cos\\left(2\\pi \\cdot 60 \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi \\cdot (64-4) \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi n - 2\\pi \\cdot 4 \\cdot \\frac{n}{64}\\right) = \\cos\\left(2\\pi \\cdot 4 \\cdot \\frac{n}{64}\\right)$$\n因此，最小二乘算法接收到的数据看起来像是来自一个频率为 4 的信号。由于 $K'=4 \\le K_{\\text{cut}}=10$，模型将完美拟合这个混叠信号。\n- **预测**：$\\text{MSE}_{\\text{train}}$ 将接近于 $0$。学习到的模型将是 $\\hat{f}(x) \\approx \\cos(2\\pi \\cdot 4 \\cdot x)$。当在密集网格上评估时，这个低频模型将与真实的高频信号 $f_{60}(x) = \\cos(2\\pi \\cdot 60 \\cdot x)$ 进行比较。两者非常不同，所以 $\\text{MSE}_{\\text{eval}}$ 会很大。期望值约为 $\\int_0^1 (\\cos(2\\pi \\cdot 4x) - \\cos(2\\pi \\cdot 60x))^2 dx = \\int_0^1 \\cos^2(2\\pi \\cdot 4x) dx + \\int_0^1 \\cos^2(2\\pi \\cdot 60x) dx = \\frac{1}{2} + \\frac{1}{2} = 1$。在 $\\hat{f}(x)$ 中找到的主导频率将是混叠频率，因此 $k^{\\star}=4$。\n\n**情况 3：$(N, K, K_{\\text{cut}}) = (64, 53, 10)$**\n- **分析**：目标频率为 $K=53$，其中 $N=64$ 且 $K_{\\text{cut}}=10$。在训练网格上的混叠频率为 $K' = |53-64| = 11$。因此，训练数据看起来像是来自一个频率为 11 的信号。然而，这个混叠频率 $K'=11$ *仍然大于* PE 截止频率 $K_{\\text{cut}}=10$。模型的基函数（频率为 $0, 1, \\ldots, 10$）无法表示它们正在训练的频率为 11 的信号。离散傅里叶基函数在均匀网格上是正交的。对应于频率 11 的采样目标向量与 PE 提供的所有基向量（频率从 0 到 10）正交。因此，到这个基空间上的投影是零向量。\n- **预测**：最小二乘拟合将导致 $\\mathbf{w} \\approx \\mathbf{0}$，意味着对所有 $x$ 都有 $\\hat{f}(x) \\approx 0$。训练 MSE 将是 $\\text{MSE}_{\\text{train}} = \\frac{1}{N}\\sum_{n=0}^{N-1}(0 - y_n)^2 = \\frac{1}{N}\\sum_{n=0}^{N-1}\\cos^2(2\\pi \\cdot 11 \\cdot n/N) \\approx \\frac{1}{2}$。类似地，$\\text{MSE}_{\\text{eval}} = \\frac{1}{M}\\sum_{m=0}^{M-1}(0-f_{53}(x_m))^2 \\approx \\frac{1}{2}\\int_0^1 \\cos^2(2\\pi \\cdot 53 x) dx = 0.5$。由于 $\\hat{f}(x)$ 是零函数，其 FFT 将处处为零。`argmax` 函数将返回搜索范围 $\\{1, \\ldots, \\lfloor M/2 \\rfloor\\}$ 的第一个索引，因此我们预计 $k^{\\star}=1$。\n\n**情况 4：$(N, K, K_{\\text{cut}}) = (64, 32, 32)$**\n- **分析**：目标频率 $K=32$ 正好是 $N=64$ 时的奈奎斯特频率。PE 截止频率为 $K_{\\text{cut}}=32$，因此基函数包含 $\\cos(2\\pi \\cdot 32 \\cdot x)$。在采样点 $x_n = n/64$ 处，目标值为 $y_n = \\cos(2\\pi \\cdot 32 \\cdot n/64) = \\cos(\\pi n) = (-1)^n$。这正是采样的奈奎斯特频率余弦基函数。值得注意的是，相应的正弦项 $\\sin(2\\pi \\cdot 32 \\cdot n/64) = \\sin(\\pi n)$ 对所有整数 $n$ 均为零，导致设计矩阵中出现一列零。这引入了秩亏损。然而，由于目标信号完全位于其余基向量的张成空间内，仍然可以实现完美拟合。`numpy.linalg.lstsq` 将正确找到解，将 $\\cos(2\\pi \\cdot 32 \\cdot x)$ 项的权重设置为 1，所有其他权重设置为 0。\n- **预测**：模型将完美表示目标。$\\text{MSE}_{\\text{train}}$ 和 $\\text{MSE}_{\\text{eval}}$ 都将接近于 $0$。拟合模型 $\\hat{f}(x)=\\cos(2\\pi \\cdot 32 \\cdot x)$ 的主导频率将是 $k^{\\star}=32$。\n\n这些分析引出了最终答案中提供的实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of fitting a band-limited INR to a sinusoidal signal\n    under various conditions of sampling, signal frequency, and PE cutoff.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, K, K_cut)\n        (64, 4, 10),    # Happy path\n        (64, 60, 10),   # Aliasing into representable band\n        (64, 53, 10),   # Aliasing outside representable band\n        (64, 32, 32),   # Nyquist boundary case\n    ]\n    \n    # Dense evaluation grid size\n    M = 2048\n\n    all_results = []\n\n    for N, K, K_cut in test_cases:\n        # 1. Construct the training data\n        x_train = np.arange(N) / N\n        y_train = np.cos(2 * np.pi * K * x_train)\n        \n        # 2. Construct the Positional Encoding (PE) design matrix for training\n        num_features = 1 + 2 * K_cut\n        Phi_train = np.zeros((N, num_features))\n        \n        # DC component (frequency 0)\n        Phi_train[:, 0] = 1.0\n        \n        # Sinusoidal components (frequencies 1 to K_cut)\n        freqs = np.arange(1, K_cut + 1)\n        # Use broadcasting for efficient construction\n        cos_terms = np.cos(2 * np.pi * freqs[None, :] * x_train[:, None])\n        sin_terms = np.sin(2 * np.pi * freqs[None, :] * x_train[:, None])\n        \n        # Interleave cos and sin terms as per the problem definition\n        Phi_train[:, 1::2] = cos_terms\n        Phi_train[:, 2::2] = sin_terms\n        \n        # 3. Fit the model by solving the linear least squares problem\n        # w = (Phi^T Phi)^-1 Phi^T y\n        w = np.linalg.lstsq(Phi_train, y_train, rcond=None)[0]\n        \n        # 4. Compute the training MSE\n        y_hat_train = Phi_train @ w\n        mse_train = np.mean((y_hat_train - y_train) ** 2)\n        \n        # 5. Evaluate the fitted model on a dense grid\n        x_eval = np.arange(M) / M\n        y_eval_true = np.cos(2 * np.pi * K * x_eval)\n        \n        # Construct the PE design matrix for evaluation\n        Phi_eval = np.zeros((M, num_features))\n        Phi_eval[:, 0] = 1.0\n        cos_terms_eval = np.cos(2 * np.pi * freqs[None, :] * x_eval[:, None])\n        sin_terms_eval = np.sin(2 * np.pi * freqs[None, :] * x_eval[:, None])\n        Phi_eval[:, 1::2] = cos_terms_eval\n        Phi_eval[:, 2::2] = sin_terms_eval\n        \n        y_hat_eval = Phi_eval @ w\n        \n        # 6. Compute the evaluation MSE\n        mse_eval = np.mean((y_hat_eval - y_eval_true) ** 2)\n\n        # 7. Estimate the dominant frequency of the fitted model\n        # Apply FFT on the densely sampled predictions\n        fft_vals = np.fft.fft(y_hat_eval)\n        # Get magnitudes of the one-sided spectrum\n        fft_mags = np.abs(fft_vals)\n        \n        # Find the peak frequency index k* in {1, 2, ..., floor(M/2)}\n        # This corresponds to array indices 1 through M//2\n        search_range = fft_mags[1 : M//2 + 1]\n        # argmax returns 0-based index relative to the slice; add 1 for frequency\n        k_star = int(np.argmax(search_range) + 1)\n\n        # 8. Append results for the current test case\n        all_results.append(f\"{mse_train:.6f}\")\n        all_results.append(f\"{mse_eval:.6f}\")\n        all_results.append(str(k_star))\n\n    #\n    # Final print statement in the exact required format.\n    #\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3136712"}, {"introduction": "隐式神经场将三维形状或场景表示为一个连续函数，例如占用场（occupancy field）。要将这个隐式函数可视化为显式表面（如网格），我们需要提取其等值面（level set）。本练习将让你模拟这一过程，通过调整决策阈值$\\tau$，直观地理解它如何影响最终恢复出的表面的拓扑结构（如连通组件的数量）和几何保真度。[@problem_id:3136800]", "problem": "给定一个由学习得到的占用场 $o_{\\theta}(\\mathbf{x})$ 表示的隐式二元曲面，该占用场定义在 $\\mathbb{R}^3$ 上。在阈值 $\\tau \\in (0,1)$ 下的隐式曲面是水平集 $\\{\\mathbf{x} \\in \\mathbb{R}^3 \\mid o_{\\theta}(\\mathbf{x}) = \\tau\\}$，其诱导的实体是超水平集 $\\{\\mathbf{x} \\in \\mathbb{R}^3 \\mid o_{\\theta}(\\mathbf{x}) \\ge \\tau\\}$。您将研究改变阈值 $\\tau$ 如何影响在离散网格上恢复的曲面的拓扑结构（连通分量的数量）和几何保真度（一种流形质量的代理指标）。您的程序必须从基本原理出发构建 $o_{\\theta}(\\mathbf{x})$，并在一个固定的网格上评估所要求的指标，无需读取任何输入。\n\n从以下基本概念开始：\n- 隐式占用场是一个函数 $o_{\\theta} : \\mathbb{R}^3 \\to [0,1]$；在 $\\tau$ 处进行阈值处理，可将空间二元分类为占用和空闲。\n- 符号距离函数 (SDF) $s(\\mathbf{x})$ 在真实曲面上满足 $s(\\mathbf{x}) = 0$，在内部满足 $s(\\mathbf{x})  0$，在外部满足 $s(\\mathbf{x})  0$。\n- logistic sigmoid 函数为 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$，logit 函数为 $\\operatorname{logit}(\\tau) = \\log\\!\\left(\\dfrac{\\tau}{1 - \\tau}\\right)$。它们互为反函数，即 $\\sigma(\\operatorname{logit}(\\tau)) = \\tau$ 且 $\\operatorname{logit}(\\sigma(z)) = z$。\n- 在具有 $6$-连通性的三维网格上的离散连通分量是一个占用的体素的最大集合，其中集合中的任意两个体素都可以通过一条面相邻步长的路径连接。\n\n通过将 SDF 与一个 logistic 链接复合，并加入一个小的空间变化偏置来模拟学习中的不完美之处，从而构建学习得到的占用场。具体定义如下：\n$$\no_{\\theta}(\\mathbf{x}) \\;=\\; \\sigma\\!\\Big(-k \\, s(\\mathbf{x}) + b + n(\\mathbf{x})\\Big),\n$$\n其中刚度 $k \\in \\mathbb{R}_{0}$，偏置 $b \\in \\mathbb{R}$。项 $n(\\mathbf{x})$ 通过以下方式模拟空间变化的拟合误差：\n$$\nn(\\mathbf{x}) \\;=\\; \\varepsilon \\,\\sin(\\omega x)\\,\\sin(\\omega y)\\,\\sin(\\omega z),\n$$\n其中 $\\varepsilon \\in \\mathbb{R}_{\\ge 0}$ 且 $\\omega \\in \\mathbb{R}_{0}$。\n\n为 $s(\\mathbf{x})$ 使用两种形状族：\n- 以原点为中心、半径为 $R$ 的单个球体：$s(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 - R$。\n- 两个半径为 $R$、中心分别为 $\\mathbf{c}_1 = (-d/2,0,0)$ 和 $\\mathbf{c}_2 = (d/2,0,0)$ 的相同球体的并集：$s(\\mathbf{x}) = \\min\\!\\big(\\|\\mathbf{x}-\\mathbf{c}_1\\|_2 - R, \\|\\mathbf{x}-\\mathbf{c}_2\\|_2 - R\\big)$。\n\n忽略 $n(\\mathbf{x})$，设 $o_{\\theta}(\\mathbf{x}) = \\tau$ 并反转 logistic 链接，可以推断出 SDF 水平为 $s(\\mathbf{x}) = \\delta(\\tau)$，其中偏移量 $\\delta(\\tau)$ 依赖于 $\\tau$、$k$ 和 $b$。您必须在您的解法中推导出这个关系，然后用它来定义球体的解析目标半径 $r^{\\star}(\\tau)$ 为 $r^{\\star}(\\tau) = R + \\delta(\\tau)$（对于并集情况，使用相对于最近中心的半径）。这个 $r^{\\star}(\\tau)$ 提供了一个与采样无关的几何保真度参考。\n\n将域离散化为一个 $N \\times N \\times N$ 的规则网格，其中 $N = 64$，覆盖立方体 $[-1,1]^3$。对于下面指定的每个测试案例：\n- 在网格点上评估 $o_{\\theta}(\\mathbf{x})$。\n- 使用超水平集条件在 $\\tau$ 处进行阈值处理，以获得一个二元占用体 $\\mathcal{V}_{\\tau}$。\n- 使用 $6$-连通性计算 $\\mathcal{V}_{\\tau}$ 的连通分量数量 $C(\\tau)$。\n- 将边界体素识别为至少有一个面相邻邻居是未被占用的占用体素；计算平均绝对径向偏差\n$$\nE(\\tau) \\;=\\; \\frac{1}{M}\\sum_{i=1}^{M}\\Big|\\|\\mathbf{p}_i - \\mathbf{c}(\\mathbf{p}_i)\\|_2 - r^{\\star}(\\tau)\\Big|,\n$$\n其中 $\\mathbf{p}_i$ 是第 $i$ 个边界体素中心的坐标，$\\mathbf{c}(\\mathbf{p}_i)$ 是最近的球体中心（对于单个球体，使用 $\\mathbf{0}$），$M$ 是边界体素的数量。如果 $M = 0$，则定义 $E(\\tau) = 0$。\n\n使用常数 $k = 16$、$b = 0$、$R = 0.35$、$\\omega = 12$ 以及双球体中心间距 $d = 0.8$。测试套件包含以下 $5$ 个案例，每个案例由一个元组 $(\\text{shape}, \\varepsilon, \\tau)$ 描述，其中 $\\text{shape} \\in \\{1,2\\}$ 表示单个球体 ($1$) 或两个球体的并集 ($2$)：\n- 案例 1：$(1, 0.0, 0.5)$\n- 案例 2：$(1, 0.12, 0.5)$\n- 案例 3：$(2, 0.0, 0.7)$\n- 案例 4：$(2, 0.0, 0.31)$\n- 案例 5：$(2, 0.05, 0.30)$\n\n您的程序必须：\n- 实现上述构建过程。\n- 对每个案例，计算 $C(\\tau)$（整数）和 $E(\\tau)$（浮点数）。\n- 将 $E(\\tau)$ 四舍五入到 $4$ 位小数。\n\n最终输出格式：您的程序应生成单行输出，包含一个逗号分隔的各案例结果列表，每个结果是一个双元素列表 $[C(\\tau),E(\\tau)]$，所有内容都包含在方括号中。例如，一个包含三个案例的输出应类似于 $[[1,0.0123],[2,0.0456],[1,0.0034]]$，但需使用上述五个案例的计算值。", "solution": "问题要求分析一个由学习得到的占用场 $o_{\\theta}(\\mathbf{x})$ 定义的隐式二元曲面。我们的任务是构建这个场，在离散网格上对其进行评估，然后针对几个测试案例计算两个指标：连通分量的数量 $C(\\tau)$，以及一个几何保真度度量——平均绝对径向偏差 $E(\\tau)$，它们都是占用阈值 $\\tau$ 的函数。\n\n该问题定义明确，在隐式神经表示领域有科学依据，并为得到唯一、确定性的解提供了所有必要的参数和定义。我们遵循有原则的推导和计算流程的概述来进行。\n\n**1. 理论公式**\n\n问题的核心是占用场，定义为：\n$$\no_{\\theta}(\\mathbf{x}) \\;=\\; \\sigma\\!\\Big(-k \\, s(\\mathbf{x}) + b + n(\\mathbf{x})\\Big)\n$$\n其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是 logistic sigmoid 函数，$s(\\mathbf{x})$ 是一个符号距离函数 (SDF)，$k$ 和 $b$ 是代表网络权重的标量参数，$n(\\mathbf{x})$ 是一个模拟学习不完美性的空间变化噪声项。在参数中使用 $-k \\, s(\\mathbf{x})$ 确保了物体内部的点（其中 $s(\\mathbf{x})  0$）映射到高占用值（接近 $1$），而物体外部的点（其中 $s(\\mathbf{x})  0$）映射到低占用值（接近 $0$）。\n\n问题要求推导一个用于重构曲面的解析参考半径 $r^{\\star}(\\tau)$。这可以通过分析理想的、无噪声的情况（即 $n(\\mathbf{x}) = 0$）来找到。隐式曲面是 $o_{\\theta}(\\mathbf{x}) = \\tau$ 的水平集。\n$$\n\\sigma(-k \\, s(\\mathbf{x}) + b) = \\tau\n$$\n为了求解 $s(\\mathbf{x})$，我们将 sigmoid 函数的反函数，即 logit 函数 $\\operatorname{logit}(p) = \\log(p/(1-p))$，应用于等式两边：\n$$\n\\operatorname{logit}(\\sigma(-k \\, s(\\mathbf{x}) + b)) = \\operatorname{logit}(\\tau)\n$$\n$$\n-k \\, s(\\mathbf{x}) + b = \\operatorname{logit}(\\tau)\n$$\n求解 $s(\\mathbf{x})$ 得到与占用阈值 $\\tau$ 对应的特定 SDF 水平，我们将其表示为 $\\delta(\\tau)$：\n$$\ns(\\mathbf{x}) = \\frac{b - \\operatorname{logit}(\\tau)}{k} = \\delta(\\tau)\n$$\n对于一个以原点为中心、半径为 $R$ 的单个球体，其 SDF 为 $s(\\mathbf{x}) = \\|\\mathbf{x}\\|_2 - R$。曲面方程变为：\n$$\n\\|\\mathbf{x}\\|_2 - R = \\delta(\\tau) \\quad \\implies \\quad \\|\\mathbf{x}\\|_2 = R + \\delta(\\tau)\n$$\n这描述了一个球体，其半径在 $R$ 的基础上偏移了 $\\delta(\\tau)$。这定义了解析目标半径 $r^{\\star}(\\tau)$：\n$$\nr^{\\star}(\\tau) = R + \\delta(\\tau) = R + \\frac{b - \\operatorname{logit}(\\tau)}{k}\n$$\n对于两个球体的并集，这同一个 $r^{\\star}(\\tau)$ 代表每个球形分量的目标半径，从其各自的中心测量。\n\n**2. 算法实现**\n\n该问题通过离散化连续域并对每个测试案例应用一系列定义好的操作来计算解决。\n\n**步骤 1：域离散化**\n将连续域 $[-1, 1]^3$ 离散化为一个 $N \\times N \\times N$ 的均匀网格，其中 $N=64$。每个体素 $(\\mathbf{p}_{ijk})$ 的中心被用作评估函数的采样点。\n\n**步骤 2：占用体生成**\n对于每个由形状类型、噪声幅值 $\\varepsilon$ 和阈值 $\\tau$ 定义的测试案例，我们执行以下操作：\n- 在每个网格点上评估相应的 SDF $s(\\mathbf{x})$。对于形状 $1$，这是单个球体的 SDF。对于形状 $2$，这是两个球体并集的 SDF。\n- 在每个网格点上评估噪声场 $n(\\mathbf{x}) = \\varepsilon \\sin(\\omega x)\\sin(\\omega y)\\sin(\\omega z)$。\n- 使用给定公式在网格上计算完整的占用场 $o_{\\theta}(\\mathbf{x})$。\n- 通过对场进行阈值处理来生成二元占用体 $\\mathcal{V}_{\\tau}$：如果一个体素的占用值大于或等于 $\\tau$，则标记为占用（值为 $1$），否则标记为未占用（值为 $0$）。\n\n**步骤 3：拓扑分析 ($C(\\tau)$)**\n确定二元占用体 $\\mathcal{V}_{\\tau}$ 中的连通分量数量 $C(\\tau)$。这是通过使用三维连通分量标记算法实现的。连通性定义为 $6$-连通性，意味着如果体素共享一个面，则认为它们是连通的。这通过使用 `scipy.ndimage.label` 函数和一个由 `scipy.ndimage.generate_binary_structure(3, 1)` 生成的结构元素来实现。\n\n**步骤 4：几何保真度分析 ($E(\\tau)$)**\n先验计算出的值 $r^{\\star}(\\tau)$ 作为基准真实半径。几何误差 $E(\\tau)$ 计算为边界体素的径向距离与此目标半径的平均绝对偏差。\n- **边界体素识别**：$\\mathcal{V}_{\\tau}$ 中的一个体素如果被占用且其 $6$ 个面相邻邻居中至少有一个未被占用，则被认为是边界体素。一种计算上高效的方法是取所有占用体素的集合，然后减去*内部*体素的集合。内部体素是指其自身及其所有 $6$-连通邻居都被占用的体素。这个集合可以通过使用二值腐蚀操作（`scipy.ndimage.binary_erosion`）和相同的 $6$-连通性结构元素来找到。\n- **误差计算**：对于每个识别出的边界体素中心 $\\mathbf{p}_i$：\n    1. 识别出最近的球体中心 $\\mathbf{c}(\\mathbf{p}_i)$。对于单球体情况，这始终是原点 $\\mathbf{0}$。对于双球体情况，它是 $\\mathbf{p}_i$ 在欧氏距离上更接近的那个中心。\n    2. 计算径向距离 $r_i = \\|\\mathbf{p}_i - \\mathbf{c}(\\mathbf{p}_i)\\|_2$。\n    3. 计算与目标半径的绝对偏差 $|\\,r_i - r^{\\star}(\\tau)\\,|$。\n- 最终指标 $E(\\tau)$ 是这些偏差在所有 $M$ 个边界体素上的算术平均值。如果没有边界体素（$M=0$），则 $E(\\tau)$ 定义为 $0$。\n\n对所有五个测试案例重复此过程，并将得到的结果对 $[C(\\tau), E(\\tau)]$ 收集并按规定格式化。", "answer": "```python\nimport numpy as np\nfrom scipy.ndimage import label, generate_binary_structure, binary_erosion\n\ndef solve():\n    \"\"\"\n    Computes topological and geometric metrics for an implicit surface.\n    \"\"\"\n    # Define constants from the problem statement.\n    N = 64\n    k = 16.0\n    b = 0.0\n    R = 0.35\n    omega = 12.0\n    d = 0.8\n\n    # Set up the computational grid.\n    coords_1d = np.linspace(-1.0, 1.0, N)\n    grid_x, grid_y, grid_z = np.meshgrid(coords_1d, coords_1d, coords_1d, indexing='ij')\n\n    # Define the test cases.\n    test_cases = [\n        # (shape, epsilon, tau)\n        # shape=1: single sphere, shape=2: two spheres\n        (1, 0.0, 0.5),\n        (1, 0.12, 0.5),\n        (2, 0.0, 0.7),\n        (2, 0.0, 0.31),\n        (2, 0.05, 0.30)\n    ]\n\n    results = []\n\n    for case in test_cases:\n        shape, epsilon, tau = case\n\n        # --- Step 1: Derive analytic target radius r_star ---\n        # Handle logit(tau) for tau near 0 or 1.\n        if tau == 0.0 or tau >= 1.0:\n            # According to the problem tau is in (0,1), but as a safeguard.\n            logit_tau = np.nan\n        else:\n            logit_tau = np.log(tau / (1.0 - tau))\n        \n        delta_tau = (b - logit_tau) / k\n        r_star = R + delta_tau\n\n        # --- Step 2: Evaluate the occupancy field on the grid ---\n        # SDF calculation\n        if shape == 1:\n            c1 = np.array([0.0, 0.0, 0.0])\n            centers = [c1]\n            s_xyz = np.sqrt(grid_x**2 + grid_y**2 + grid_z**2) - R\n        else:  # shape == 2\n            c1 = np.array([-d / 2, 0.0, 0.0])\n            c2 = np.array([d / 2, 0.0, 0.0])\n            centers = [c1, c2]\n            dist1 = np.sqrt((grid_x - c1[0])**2 + (grid_y - c1[1])**2 + (grid_z - c1[2])**2) - R\n            dist2 = np.sqrt((grid_x - c2[0])**2 + (grid_y - c2[1])**2 + (grid_z - c2[2])**2) - R\n            s_xyz = np.minimum(dist1, dist2)\n        \n        # Noise field\n        n_xyz = epsilon * np.sin(omega * grid_x) * np.sin(omega * grid_y) * np.sin(omega * grid_z)\n        \n        # Occupancy field (sigmoid)\n        z_arg = -k * s_xyz + b + n_xyz\n        o_xyz = 1.0 / (1.0 + np.exp(-z_arg))\n        \n        # Threshold to get the binary occupancy volume\n        occupancy_volume = o_xyz >= tau\n\n        # --- Step 3: Compute number of connected components C(tau) ---\n        if not np.any(occupancy_volume):\n            C_tau = 0\n        else:\n            # 6-connectivity for 3D\n            structure = generate_binary_structure(3, 1)\n            _, C_tau = label(occupancy_volume, structure=structure)\n\n        # --- Step 4: Compute mean absolute radial deviation E(tau) ---\n        # Identify boundary voxels using erosion\n        structure = generate_binary_structure(3, 1)\n        eroded_volume = binary_erosion(occupancy_volume, structure=structure)\n        boundary_voxels_mask = occupancy_volume  ~eroded_volume\n\n        boundary_indices = np.argwhere(boundary_voxels_mask)\n        M = len(boundary_indices)\n\n        if M == 0:\n            E_tau = 0.0\n        else:\n            # Convert voxel indices to world coordinates\n            boundary_coords = -1.0 + boundary_indices * (2.0 / (N - 1))\n            \n            total_abs_dev = 0.0\n            for p_i in boundary_coords:\n                # Find the nearest sphere center\n                if shape == 1:\n                    nearest_center = centers[0]\n                else:\n                    dist_to_c1 = np.linalg.norm(p_i - centers[0])\n                    dist_to_c2 = np.linalg.norm(p_i - centers[1])\n                    if dist_to_c1  dist_to_c2:\n                        nearest_center = centers[0]\n                    else:\n                        nearest_center = centers[1]\n                \n                # Calculate deviation for this boundary point\n                radial_dist = np.linalg.norm(p_i - nearest_center)\n                abs_dev = np.abs(radial_dist - r_star)\n                total_abs_dev += abs_dev\n            \n            E_tau = total_abs_dev / M\n        \n        # Store the results for the current case\n        results.append([C_tau, round(E_tau, 4)])\n\n    # Format the final output string.\n    # e.g., [[1,0.0123],[2,0.0456]]\n    result_str = \",\".join([f\"[{c},{e}]\" for c, e in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3136800"}, {"introduction": "在神经辐射场（NeRF）中，体渲染（volume rendering）需要在每条光线上密集采样，计算成本高昂。一种关键的优化策略是重要性采样，即在对最终颜色贡献大的区域进行更密集的采样。本练习将指导你实现一个简化的神经重要性采样器，它通过学习一个概率分布来匹配场景内容，从而让你理解如何通过学习来提升渲染效率。[@problem_id:3136700]", "problem": "给定一个一维射线参数域，您需要实现一个神经重要性采样概率密度函数，该函数能够适应一个由沿射线的可微隐式神经表示所导出的目标分布。您的目标是构建一个完整的程序，该程序使用梯度下降学习一个分段常数采样分布，以减小其与从体渲染权重派生出的目标分布之间的 Kullback–Leibler 散度 (KL 散度)，并报告一组指定测试用例的最终 KL 散度值。\n\n此问题的基本原理如下。一条射线由 $t \\in [t_{\\min}, t_{\\max}]$ 参数化。一个隐式神经表示 $f_\\theta$ 将 $t$ 映射到一个非负的体密度 $\\sigma_\\theta(t)$。沿射线上点 $t$ 的渲染权重通过透射率和不透明度定义。在连续形式下，透射率为 $T(t) = \\exp\\left(-\\int_{t_{\\min}}^{t} \\sigma_\\theta(s) \\,\\mathrm{d}s\\right)$，对于一个小步长 $\\Delta t$，无穷小不透明度为 $\\alpha(t) = 1 - \\exp(-\\sigma_\\theta(t)\\,\\Delta t)$。点 $t$ 处的渲染权重为 $w(t) = T(t)\\,\\alpha(t)$，一个重要性目标分布与 $w(t)$ 成正比。概率密度函数的积分必须为 $1$，因此目标分布 $q(t)$ 满足 $q(t) = \\frac{w(t)}{\\int_{t_{\\min}}^{t_{\\max}} w(u)\\,\\mathrm{d}u}$。\n\n您将以离散形式在均匀网格上实现以下步骤：\n- 使用 $t_{\\min} = 0$ 和 $t_{\\max} = 1$。\n- 使用 $N_s$ 个均匀样本，其中 $t_i = t_{\\min} + i\\,\\Delta t$，$i = 0,1,\\dots,N_s-1$，且 $\\Delta t = \\frac{t_{\\max} - t_{\\min}}{N_s}$。\n- 计算每个 $t_i$ 的 $\\sigma_\\theta(t_i)$，然后计算 $T_i$，其中 $T_0 = 1$ 且 $T_{i+1} = T_i \\cdot \\exp(-\\sigma_\\theta(t_i)\\,\\Delta t)$，并计算 $\\alpha_i = 1 - \\exp(-\\sigma_\\theta(t_i)\\,\\Delta t)$。设置 $w_i = T_i\\,\\alpha_i$。离散目标分布为 $q_i = \\frac{w_i}{\\sum_{j=0}^{N_s-1} w_j}$。\n\n您将通过 logits $\\phi \\in \\mathbb{R}^B$ 对一个覆盖 $B$ 个等宽区间的分段常数重要性分布进行参数化。令 $m_b = \\frac{\\exp(\\phi_b)}{\\sum_{k=0}^{B-1}\\exp(\\phi_k)}$ 为区间 $b$ 的区间质量。在区间 $b$ 内，概率质量均匀分布在其所有网格单元上，因此属于区间 $b$ 的网格索引 $i$ 处的概率质量为 $p_i = \\frac{m_b}{C_b}$，其中 $C_b$ 是区间 $b$ 中的网格样本数。目标分布与学习到的重要性分布之间的离散 Kullback–Leibler 散度为\n$$\nD_{\\mathrm{KL}}(q \\Vert p) = \\sum_{i=0}^{N_s-1} q_i \\,\\log\\left(\\frac{q_i}{p_i}\\right).\n$$\n为了学习 $\\phi$，需要最小化聚合的目标区间质量与 softmax 区间质量之间的交叉熵。令 $Q_b = \\sum_{i \\in \\text{bin } b} q_i$ 为每个区间的目标质量。交叉熵目标函数为\n$$\n\\mathcal{L}(\\phi) = -\\sum_{b=0}^{B-1} Q_b \\,\\log m_b,\n$$\n其关于 logits 的梯度为\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\phi_b} = m_b - Q_b.\n$$\n您必须使用此梯度实现对 $\\phi$ 的梯度下降更新。\n\n为了满足与 $f_\\theta$ 的耦合，将 $f_\\theta$ 定义为一个小型正弦表示网络 (Sine Representation Network，通常称为 SIREN)，它将 $t$ 映射到一个标量，然后通过一个 Softplus 函数以确保非负性：\n- 令 $h_1(t) = \\sin(w_1\\,t + b_1)$。\n- 令 $h_2(t) = \\sin(w_2\\,h_1(t) + b_2)$。\n- 令 $\\sigma_\\theta(t) = \\operatorname{softplus}(a\\,h_2(t) + c + d\\,t)$，其中 $\\operatorname{softplus}(x) = \\log(1 + e^x)$。\n\n您还必须为 $p_\\phi(t)$ 实现一个采样器，通过在区间层面使用逆变换采样，并在每个选定的区间内进行均匀采样，沿射线抽取 $K$ 个样本。尽管该采样器不影响所需的最终输出，但仍需实现。\n\n设置以下固定的数值超参数：\n- 使用 $N_s = 256$。\n- 使用 $B = 16$。\n- 使用学习率 $\\eta = 0.5$。\n- 使用 $S = 800$ 步梯度下降。\n- 使用 $K = 64$ 个样本用于采样器演示（不属于最终输出的一部分）。\n\n设计程序以运行以下测试套件，每个测试用例定义一个不同的 $\\theta = (w_1, b_1, w_2, b_2, a, c, d)$：\n- 用例 1：$(w_1, b_1, w_2, b_2, a, c, d) = (8.0, 0.3, 2.0, 0.2, 1.5, 0.1, 0.0)$。\n- 用例 2 (近似均匀的密度)：$(w_1, b_1, w_2, b_2, a, c, d) = (4.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0)$。\n- 用例 3 (高频变化)：$(w_1, b_1, w_2, b_2, a, c, d) = (30.0, 0.1, 4.0, 0.7, 0.8, 0.0, 0.0)$。\n- 用例 4 (密度随 $t$ 递减)：$(w_1, b_1, w_2, b_2, a, c, d) = (10.0, -0.5, 3.0, 0.0, 1.0, 0.2, -1.0)$。\n\n您的程序必须：\n- 对于每个用例，通过指定的离散透射率和不透明度计算由 $f_\\theta$ 导出的离散目标分布 $q_i$。\n- 通过梯度下降最小化交叉熵来训练 logits $\\phi$，共进行 $S$ 步。\n- 训练后，使用从学习到的区间质量（在每个区间内的网格单元上均匀分布）获得的 $p_i$ 来计算最终的离散 KL 散度 $D_{\\mathrm{KL}}(q \\Vert p)$。\n- 生成一行输出，其中包含四个用例的最终 KL 散度，形式为用方括号括起来的逗号分隔列表。\n\n所有计算都是无量纲的；不涉及物理单位或角度。所有输出必须是浮点数。最终输出格式必须严格为 $[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是用例 $k$ 的浮点数 KL 散度。\n\n必须遵循上述定义和公式，并实现数值稳定的操作，包括在需要时通过小的正值截断来适当处理 KL 散度计算中的除零问题，以确保科学真实性和自洽性。", "solution": "该问题是有效的。它提出了一个在深度学习和计算机图形学领域内适定且有科学依据的任务，具体涉及用于体渲染的神经重要性采样。所有必要的参数、定义和数学公式均已提供，并且该任务是可通过计算验证的。\n\n目标是学习一个分段常数概率密度函数 $p(t)$，它能近似一个复杂的目标分布 $q(t)$，该目标分布由应用于体密度神经表示的体渲染方程派生而来。近似的质量由 Kullback–Leibler 散度 $D_{\\mathrm{KL}}(q \\Vert p)$ 衡量，并通过基于梯度的优化过程将其最小化。整个过程分为三个主要阶段：目标分布的生成、重要性分布的参数化与优化，以及最终评估。\n\n**1. 来自神经体渲染的目标分布**\n\n问题的基础是一个隐式神经表示，具体是一个记为 $f_\\theta$ 的小型正弦表示网络 (SIREN)。该网络将一维射线参数 $t$ 映射到一个非负的体密度 $\\sigma_\\theta(t)$。网络参数由 $\\theta = (w_1, b_1, w_2, b_2, a, c, d)$ 给出。网络由以下操作序列定义：\n$$ h_1(t) = \\sin(w_1 t + b_1) $$\n$$ h_2(t) = \\sin(w_2 h_1(t) + b_2) $$\n$$ \\sigma_\\theta(t) = \\operatorname{softplus}(a h_2(t) + c + d t) $$\n$\\operatorname{softplus}(x) = \\log(1 + e^x)$ 函数确保输出密度 $\\sigma_\\theta(t)$ 始终为非负。\n\n连续的射线域 $t \\in [t_{\\min}, t_{\\max}] = [0, 1]$ 被离散化为 $N_s = 256$ 个均匀段。采样点为 $t_i = t_{\\min} + i \\cdot \\Delta t$，$i \\in \\{0, 1, \\dots, N_s-1\\}$，其中步长为 $\\Delta t = (t_{\\max} - t_{\\min}) / N_s$。\n\n目标概率分布 $q$ 是使用离散体渲染的原理派生的。透射率 $T_i$ 表示光线从射线起点传播到第 $i$ 段起点 $t_i$ 处而未被遮挡的概率，它是递归计算的。\n$$ T_i = \\exp\\left(-\\sum_{j=0}^{i-1} \\sigma_\\theta(t_j) \\Delta t\\right), \\quad \\text{with } T_0 = 1 $$\n第 $i$ 段的不透明度 $\\alpha_i$ 表示光线在该段内与介质相互作用的概率，由以下公式给出：\n$$ \\alpha_i = 1 - \\exp(-\\sigma_\\theta(t_i) \\Delta t) $$\n未归一化的渲染权重 $w_i$ 代表第 $i$ 段对最终渲染值的贡献，是到达该段的光线与该段不透明度的乘积：$w_i = T_i \\alpha_i$。\n最后，这些权重被归一化，形成一个有效的离散概率分布 $q$：\n$$ q_i = \\frac{w_i}{\\sum_{j=0}^{N_s-1} w_j} $$\n这个分布 $q$ 作为我们重要性采样模型的目标。\n\n**2. 重要性分布的参数化与优化**\n\n学习的重要性分布 $p(t)$ 被建模为一个在覆盖域 $[0, 1]$ 的 $B=16$ 个连续等宽区间上的分段常数函数。每个区间的概率质量 $b \\in \\{0, \\dots, B-1\\}$ 由相应的 logit $\\phi_b$ 决定。所有 logits 的集合是一个向量 $\\phi \\in \\mathbb{R}^B$。区间 $b$ 的概率质量 $m_b$ 使用 softmax 函数计算，该函数确保质量为正且总和为 1。\n$$ m_b = \\frac{\\exp(\\phi_b)}{\\sum_{k=0}^{B-1} \\exp(\\phi_k)} $$\n在每个区间内，该质量被均匀地分布到落入其中的 $C_b = N_s / B = 16$ 个离散采样点上。因此，属于区间 $b$ 的采样点 $t_i$ 的概率 $p_i$ 为：\n$$ p_i = \\frac{m_b}{C_b} $$\n目标是找到使所得分布 $p$ 尽可能接近目标 $q$ 的 logits $\\phi$。这通过最小化 $D_{\\mathrm{KL}}(q \\Vert p)$ 来实现。对于分段常数模型，这等同于最小化聚合的目标区间质量 $Q_b = \\sum_{i \\in \\text{bin } b} q_i$ 和学习到的区间质量 $m_b$ 之间的交叉熵。损失函数为：\n$$ \\mathcal{L}(\\phi) = -\\sum_{b=0}^{B-1} Q_b \\log m_b $$\n此损失函数的一个关键特性是其关于 logits 的梯度非常简单：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\phi_b} = m_b - Q_b $$\n这个优雅的结果使得使用梯度下降进行高效优化成为可能。logits 被初始化（例如，为 0），并以学习率 $\\eta=0.5$ 迭代更新 $S=800$ 步：\n$$ \\phi \\leftarrow \\phi - \\eta \\cdot (m - Q) $$\n\n**3. 实现与评估**\n\n每个测试用例的总体算法流程如下：\n首先，使用 SIREN 参数 $\\theta$ 计算离散化射线上各点的密度值 $\\sigma_\\theta(t_i)$。其次，将离散体渲染公式应用于这些密度以生成目标分布 $q$。再次，将目标分布分箱以形成聚合的目标质量 $Q_b$。最后，使用梯度下降更新规则训练 logits $\\phi$，共进行 $S=800$ 步。\n训练后，使用最终优化的 logits 来构建离散概率分布 $p$。通过计算 Kullback–Leibler 散度 $D_{\\mathrm{KL}}(q \\Vert p)$ 来评估最终性能。该实现还包括一个函数，使用逆变换采样从学习到的分布 $p_\\phi(t)$ 中抽取 $K=64$ 个样本，这涉及首先根据质量 $m_b$ 采样一个区间，然后在该区间的空间范围内进行均匀采样。这演示了在实践中如何将学习到的分布用于渲染中的重要性采样。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Main function to run the neural importance sampling simulation for all test cases.\n    \"\"\"\n    # Define fixed numerical hyperparameters\n    N_S = 256\n    B = 16\n    ETA = 0.5\n    S = 800\n    K = 64\n    T_MIN = 0.0\n    T_MAX = 1.0\n\n    def _softplus(x):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        return np.logaddexp(0, x)\n\n    def _siren(t, params):\n        \"\"\"\n        Computes density sigma from a SIREN model.\n        params = (w1, b1, w2, b2, a, c, d)\n        \"\"\"\n        w1, b1, w2, b2, a, c, d = params\n        h1 = np.sin(w1 * t + b1)\n        h2 = np.sin(w2 * h1 + b2)\n        output = a * h2 + c + d * t\n        sigma = _softplus(output)\n        return sigma\n\n    def _get_target_distribution(params):\n        \"\"\"\n        Computes the discrete target distribution q_i from the SIREN parameters.\n        \"\"\"\n        t_points = np.linspace(T_MIN, T_MAX, N_S, endpoint=False, dtype=np.float64)\n        delta_t = (T_MAX - T_MIN) / N_S\n        \n        sigma = _siren(t_points, params)\n        \n        alpha = 1.0 - np.exp(-sigma * delta_t)\n        \n        # T_i is the transmittance to the start of interval i.\n        # This is an exclusive cumulative product.\n        transmittance_contrib = np.exp(-sigma * delta_t)\n        T = np.concatenate(([1.0], np.cumprod(transmittance_contrib[:-1])))\n        \n        w = T * alpha\n        \n        w_sum = np.sum(w)\n        if w_sum  1e-9: # Robustness for near-zero weights\n            return np.full(N_S, 1.0 / N_S)\n        \n        q = w / w_sum\n        return q\n\n    def _train_importance_distribution(q):\n        \"\"\"\n        Trains the piecewise-constant distribution logits phi using gradient descent.\n        \"\"\"\n        if N_S % B != 0:\n            raise ValueError(\"N_s must be divisible by B.\")\n        C_b = N_S // B\n\n        q_reshaped = q.reshape((B, C_b))\n        Q = q_reshaped.sum(axis=1)\n\n        phi = np.zeros(B, dtype=np.float64)\n\n        for _ in range(S):\n            m = softmax(phi)\n            grad = m - Q\n            phi -= ETA * grad\n            \n        return phi\n\n    def _calculate_kl_divergence(q, trained_phi):\n        \"\"\"\n        Calculates the final KL divergence between q and the learned distribution p.\n        \"\"\"\n        C_b = N_S // B\n        \n        m_final = softmax(trained_phi)\n        \n        p_per_bin = m_final / C_b\n        p = np.repeat(p_per_bin, C_b)\n        \n        # Add a small epsilon for numerical stability in a practical setting,\n        # although with softplus, sigma, w, and q should be positive.\n        eps = 1e-10\n        kl_div = np.sum(q * (np.log(q + eps) - np.log(p + eps)))\n\n        return kl_div\n\n    def _sample_from_distribution(phi, k_samples):\n        \"\"\"\n        Draws samples from the learned piecewise-constant distribution p_phi(t).\n        This function is implemented as required but not used for the final output.\n        \"\"\"\n        m = softmax(phi)\n        \n        bin_indices = np.random.choice(np.arange(B), size=k_samples, p=m)\n        \n        bin_width = (T_MAX - T_MIN) / B\n        bin_starts = bin_indices * bin_width + T_MIN\n        offsets = np.random.uniform(low=0.0, high=bin_width, size=k_samples)\n        \n        samples = bin_starts + offsets\n        return samples\n\n    # Define the test cases\n    test_cases = [\n        (8.0, 0.3, 2.0, 0.2, 1.5, 0.1, 0.0),\n        (4.0, 1.0, 0.0, 0.0, 0.0, 0.4, 0.0),\n        (30.0, 0.1, 4.0, 0.7, 0.8, 0.0, 0.0),\n        (10.0, -0.5, 3.0, 0.0, 1.0, 0.2, -1.0),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        # Step 1: Compute target distribution q\n        q = _get_target_distribution(case_params)\n        \n        # Step 2: Train the importance distribution to get final logits phi\n        trained_phi = _train_importance_distribution(q)\n        \n        # Step 3: Compute final KL divergence\n        final_kl = _calculate_kl_divergence(q, trained_phi)\n        results.append(final_kl)\n\n        # The sampler is implemented but its output is not part of the final result.\n        _ = _sample_from_distribution(trained_phi, K)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3136700"}]}