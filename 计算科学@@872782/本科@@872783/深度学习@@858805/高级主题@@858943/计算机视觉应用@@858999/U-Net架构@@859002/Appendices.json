{"hands_on_practices": [{"introduction": "在将深度学习模型投入实际应用之前，评估其计算成本是至关重要的一步。本练习将引导你从第一性原理出发，推导一个U-Net架构的浮点运算次数（FLOPs），这是衡量模型计算复杂度的标准指标。通过这个练习，你将学会如何精确量化模型各组件的计算开销，这对于模型优化、性能预测以及硬件感知的模型设计至关重要。[@problem_id:3193868]", "problem": "考虑一个用于图像到图像映射的二维U形卷积神经网络（U-Net），其架构和约定如下。输入为任意空间分辨率的方形图像，分析以每个输入像素为基础进行，因此结果与绝对输入尺寸无关。编码器（下采样路径）有 $L$ 个分辨率级别，索引为 $l \\in \\{0,\\dots,L-1\\}$，随后是级别 $L$ 的瓶颈，然后是解码器（上采样路径），有 $L$ 个分辨率级别，索引为 $l \\in \\{L-1,\\dots,0\\}$。设级别 $l$ 的特征通道数为 $2^{l} c$，其中 $c$ 是给定的基础通道宽度。在每个编码器级别 $l \\in \\{0,\\dots,L-1\\}$，有两个卷积层，每个都是标准的二维卷积，卷积核大小为 $k \\times k$，步幅为 $1$，并使用“same”填充以保持该级别的空间分辨率；这两个卷积都接受 $2^{l} c$ 个输入通道并产生 $2^{l} c$ 个输出通道。在编码器级别之间，通过一个 $2 \\times 2$ 的非参数化下采样，在每个空间维度上将分辨率降低 $2$ 倍。级别 $L$ 的瓶颈包含两个 $k \\times k$ 的卷积，将 $2^{L} c$ 个通道映射到 $2^{L} c$ 个通道。在解码器中，于每个级别 $l \\in \\{L-1,\\dots,0\\}$，首先有一个转置卷积（也称为反卷积），其卷积核大小为 $2 \\times 2$，步幅为 $2$，在每个空间维度上进行 $2$ 倍的上采样，并将通道数从 $2^{l+1} c$ 减半至 $2^{l} c$。上采样后的特征图与来自编码器的相同分辨率的跳跃连接（具有 $2^{l} c$ 个通道）进行拼接（在通道维度上），产生 $2^{l+1} c$ 个通道。随后是两个标准的 $k \\times k$ 卷积：第一个将通道数从 $2^{l+1} c$ 减少到 $2^{l} c$，第二个将通道数保持在 $2^{l} c$。忽略池化、拼接、偏置加法和非线性激活的成本。不包括任何最终的分类层。按照惯例计算浮点运算次数：一次乘法计为 $1$ 次浮点运算，一次加法计为 $1$ 次浮点运算，因此一次乘加运算贡献 $2$ 次浮点运算。\n\n任务：\n- 仅从以下定义出发：一个标准的二维卷积，产生空间面积为 $A$、卷积核大小为 $k \\times k$、输入通道数为 $C_{\\text{in}}$、输出通道数为 $C_{\\text{out}}$ 的输出张量，其执行的浮点运算次数为 $2 A k^{2} C_{\\text{in}} C_{\\text{out}}$。推导此U-Net每个输入像素的总理论浮点运算次数的闭式表达式 $F(k,c,L)$，该表达式纯粹用 $k$、 $c$ 和 $L$ 表示。您的推导必须涵盖所有编码器块、瓶颈和所有解码器块（包括转置卷积），并且必须使用级别 $l$ 的空间面积相对于输入减小了 $4^{l}$ 倍这一事实。\n- 一个假设的性能分析器运行报告指出，对于 $k=3$、$c=64$ 和 $L=4$，所有转置卷积合计占总浮点运算次数的比例约为 $0.14$。使用您推导的 $F(k,c,L)$ 计算所有转置卷积所占的理论比例，并评论该性能分析器的报告是否与理论一致。无需四舍五入。\n- 提出一项修改，将每个 $2 \\times 2$ 的转置卷积替换为非学习型双线性上采样，然后是一个从 $2^{l+1} c$ 通道到 $2^{l} c$ 通道的 $1 \\times 1$ 卷积。使用相同的计数惯例，推导此替换所带来的每个输入像素浮点运算次数的变化，并指出它改变了 $F(k,c,L)$ 中的哪一项。无需四舍五入。\n\n您最终报告的答案必须仅为第一个任务中指定的单个闭式表达式 $F(k,c,L)$。不包括任何单位。不要四舍五入。", "solution": "该问题陈述已经过严格验证，并被确定为有效。它具有科学依据，问题设定良好、客观，并包含足够的信息以获得唯一解。其定义和架构与深度学习领域的标准实践一致。\n\n解题过程通过推导所需表达式来进行。\n\n**任务1：推导每个输入像素的总浮点运算次数 $F(k,c,L)$**\n\n我们计算U-Net架构中每个组件的浮点运算次数（FLOPs）。最终结果以每个输入像素为基础表示，通过将总FLOPs除以输入图像面积（表示为 $A_0$）进行归一化。\n\n分辨率级别 $l$ 的特征通道数由 $C_l = 2^{l} c$ 给出。级别 $l$ 的空间面积为 $A_l = \\frac{A_0}{4^{l}}$。对于一个输出面积为 $A$、卷积核大小为 $k \\times k$、输入通道数为 $C_{\\text{in}}$、输出通道数为 $C_{\\text{out}}$ 的标准卷积，其FLOPs的计算公式为 $2 A k^{2} C_{\\text{in}} C_{\\text{out}}$。\n\n**1. 编码器路径的FLOPs**\n对于每个级别 $l \\in \\{0, \\dots, L-1\\}$，有两个相同的 $k \\times k$ 卷积。对于这些卷积，输入和输出通道数均为 $C_{\\text{in}} = C_{\\text{out}} = C_l = 2^{l} c$。输出空间面积为 $A_l = \\frac{A_0}{4^{l}}$。该级别单个卷积的FLOPs为：\n$$F_{\\text{enc,conv},l} = 2 A_l k^2 C_l^2 = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (2^l c)^2 = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (4^l c^2) = 2 A_0 k^2 c^2$$\n由于在 $L$ 个编码器级别中的每个级别都有两个这样的卷积，编码器路径的总FLOPs为：\n$$F_{\\text{enc}} = \\sum_{l=0}^{L-1} 2 \\times (2 A_0 k^2 c^2) = L \\times (4 A_0 k^2 c^2) = 4 L A_0 k^2 c^2$$\n编码器的每个输入像素FLOPs为 $f_{\\text{enc}} = \\frac{F_{\\text{enc}}}{A_0} = 4 L k^2 c^2$。\n\n**2. 瓶颈部分的FLOPs**\n在级别 $L$，有两个 $k \\times k$ 卷积，其 $C_{\\text{in}} = C_{\\text{out}} = C_L = 2^{L} c$，输出面积为 $A_L = \\frac{A_0}{4^{L}}$。其中一个卷积的FLOPs为：\n$$F_{\\text{bottle,conv}} = 2 A_L k^2 C_L^2 = 2 \\left(\\frac{A_0}{4^L}\\right) k^2 (2^L c)^2 = 2 A_0 k^2 c^2$$\n瓶颈中有两个卷积，因此总的瓶颈FLOPs为 $F_{\\text{bottle}} = 2 \\times (2 A_0 k^2 c^2) = 4 A_0 k^2 c^2$。\n瓶颈的每个输入像素FLOPs为 $f_{\\text{bottle}} = \\frac{F_{\\text{bottle}}}{A_0} = 4 k^2 c^2$。\n\n**3. 解码器路径的FLOPs**\n对于每个级别 $l \\in \\{L-1, \\dots, 0\\}$（对应于 $L$ 个不同的级别），我们计算转置卷积和两个标准卷积的FLOPs。\n\n- **转置卷积：** 这是一个 $2 \\times 2$ 的操作，步幅为 $2$，将数据从级别 $l+1$ 上采样到级别 $l$。\n输入通道数：$C_{\\text{in}} = C_{l+1} = 2^{l+1} c$。\n输出通道数：$C_{\\text{out}} = C_l = 2^l c$。\n输出面积：$A_l = \\frac{A_0}{4^l}$。\n此操作的FLOPs为：\n$$F_{\\text{deconv},l} = 2 A_l (\\text{kernel\\_size})^2 C_{\\text{in}} C_{\\text{out}} = 2 \\left(\\frac{A_0}{4^l}\\right) (2^2) (2^{l+1} c)(2^l c)$$\n$$F_{\\text{deconv},l} = 8 \\left(\\frac{A_0}{4^l}\\right) (2 \\cdot 2^l c)(2^l c) = 16 A_0 c^2 \\frac{4^l}{4^l} = 16 A_0 c^2$$\n对 $L$ 个解码器级别求和，所有转置卷积的总FLOPs为 $F_{\\text{deconv}} = L \\times (16 A_0 c^2) = 16 L A_0 c^2$。每个输入像素的FLOPs为 $f_{\\text{deconv}} = 16 L c^2$。\n\n- **标准卷积：** 与跳跃连接拼接后，第一个卷积的输入通道数为 $C_l + C_l = 2 C_l = 2^{l+1} c$。\n第一个 $k \\times k$ 卷积将通道数从 $2^{l+1} c$ 减少到 $2^l c$：\n$$F_{\\text{dec,conv1},l} = 2 A_l k^2 (2^{l+1}c)(2^l c) = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (2 \\cdot 2^l c)(2^l c) = 4 A_0 k^2 c^2 \\frac{4^l}{4^l} = 4 A_0 k^2 c^2$$\n第二个 $k \\times k$ 卷积将通道数从 $2^l c$ 映射到 $2^l c$：\n$$F_{\\text{dec,conv2},l} = 2 A_l k^2 (2^l c)(2^l c) = 2 \\left(\\frac{A_0}{4^l}\\right) k^2 (2^l c)^2 = 2 A_0 k^2 c^2 \\frac{4^l}{4^l} = 2 A_0 k^2 c^2$$\n每个解码器级别 $l$ 的标准卷积总FLOPs为 $F_{\\text{dec,convs},l} = 4 A_0 k^2 c^2 + 2 A_0 k^2 c^2 = 6 A_0 k^2 c^2$。\n对 $L$ 个级别求和得到 $F_{\\text{dec,convs}} = L \\times (6 A_0 k^2 c^2) = 6 L A_0 k^2 c^2$。每个输入像素的FLOPs为 $f_{\\text{dec,convs}} = 6 L k^2 c^2$。\n\n**每个输入像素的总FLOPs**\n每个输入像素的总FLOPs $F(k,c,L)$ 是所有部分贡献的总和：\n$$F(k,c,L) = f_{\\text{enc}} + f_{\\text{bottle}} + f_{\\text{deconv}} + f_{\\text{dec,convs}}$$\n$$F(k,c,L) = 4 L k^2 c^2 + 4 k^2 c^2 + 16 L c^2 + 6 L k^2 c^2$$\n合并各项：\n$$F(k,c,L) = (4L + 4 + 6L) k^2 c^2 + 16 L c^2$$\n$$F(k,c,L) = (10L + 4) k^2 c^2 + 16 L c^2$$\n因式分解表达式得到最终的闭式形式：\n$$F(k,c,L) = 2 k^2 c^2 (5L + 2) + 16 L c^2$$\n\n**任务2：与性能分析器报告的一致性**\n由转置卷积贡献的总FLOPs比例 $\\eta$ 为：\n$$\\eta = \\frac{f_{\\text{deconv}}}{F(k,c,L)} = \\frac{16 L c^2}{2 k^2 c^2 (5L + 2) + 16 L c^2} = \\frac{16 L}{2 k^2 (5L + 2) + 16 L}$$\n代入给定值 $k=3$ 和 $L=4$：\n$$\\eta = \\frac{16 \\times 4}{2 \\times 3^2 (5 \\times 4 + 2) + 16 \\times 4} = \\frac{64}{2 \\times 9 \\times 22 + 64} = \\frac{64}{396 + 64} = \\frac{64}{460}$$\n化简并计算该分数：\n$$\\eta = \\frac{16}{115} \\approx 0.13913$$\n理论值约为 $0.139$，与性能分析器报告的约 $0.14$ 的值是一致的。\n\n**任务3：上采样路径的修改**\n所提出的修改是将每个 $2 \\times 2$ 的转置卷积替换为非学习型双线性上采样，后跟一个 $1 \\times 1$ 的卷积。非参数化上采样的成本被忽略。下面计算新的 $1 \\times 1$ 卷积的成本。\n在每个解码器级别 $l$，该卷积的卷积核大小为 $k=1$，输入通道数为 $C_{\\text{in}}=2^{l+1}c$，输出通道数为 $C_{\\text{out}}=2^l c$，输出面积为 $A_l = \\frac{A_0}{4^l}$。这个新操作的每个输入像素FLOPs $f'_{\\text{upsample},l}$ 为：\n$$f'_{\\text{upsample},l} = \\frac{2 A_l (1^2) C_{\\text{in}} C_{\\text{out}}}{A_0} = \\frac{2 (\\frac{A_0}{4^l}) (1) (2^{l+1}c)(2^l c)}{A_0} = 2 \\left(\\frac{1}{4^l}\\right) (2 \\cdot 2^l c)(2^l c) = 4 c^2$$\n这取代了原来每个级别的转置卷积成本，即 $f_{\\text{deconv},l} = 16 c^2$。每个解码器级别每像素FLOPs的变化量为 $\\Delta f_l = 4 c^2 - 16 c^2 = -12 c^2$。\n由于此修改应用于所有 $L$ 个解码器级别，每个输入像素总FLOPs的变化量为：\n$$\\Delta F = \\sum_{l=0}^{L-1} (-12 c^2) = -12 L c^2$$\n此修改改变了 $F(k,c,L)$ 中代表转置卷积总成本的项。原来的项 $16 L c^2$ 被新的上采样操作总成本 $L \\times (4c^2) = 4Lc^2$ 所取代。变化是每个输入像素的FLOPs减少了 $12 L c^2$。", "answer": "$$\\boxed{2 k^2 c^2 (5L + 2) + 16 L c^2}$$", "id": "3193868"}, {"introduction": "在深度学习训练中，除了计算资源，内存占用是另一个关键瓶颈，尤其是在处理高分辨率图像时。本练习聚焦于U-Net训练过程中的峰值激活内存，它通常是限制我们使用更大批次或更深模型的主要因素。你将学习如何为U-Net的激活内存建立数学模型，并探索梯度检查点（gradient checkpointing）这一核心优化技术，它允许我们用额外的计算换取宝贵的内存空间。[@problem_id:3193905]", "problem": "一个用于二维语义分割的 U-Net 编码器-解码器架构被构建，其包含 $L$ 个下采样步骤（编码器层级索引为 $\\ell=0,1,\\dots,L-1$，以及一个底层 $\\ell=L$）。在编码器层级 $\\ell$，空间分辨率为 $H/2^{\\ell}\\times W/2^{\\ell}$，通道数为 $c\\cdot 2^{\\ell}$。每个编码器层级包含两个保持通道数的 $3\\times 3$ 卷积，然后是两个空间维度上因子为 $2$ 的下采样（底层 $\\ell=L$ 无下采样）。解码器是编码器的镜像：在解码器层级 $\\ell=L-1,L-2,\\dots,0$，特征被上采样 $2$ 倍，与来自层级 $\\ell$ 的编码器跳跃连接（其具有 $c\\cdot 2^{\\ell}$ 个通道）拼接，然后通过两个 $3\\times 3$ 卷积，输出 $c\\cdot 2^{\\ell}$ 个通道，空间分辨率为 $H/2^{\\ell}\\times W/2^{\\ell}$。输入批次大小为 $b$。\n\n假设如下：\n- 数据类型为 $32$ 位浮点数，因此每个标量占用 $s=4$ 字节。\n- 除非使用梯度检查点，否则深度学习框架会保留每个卷积层的输出直到前向传播结束，并在不再需要时立即释放任何张量。\n- 忽略参数张量、优化器状态、非线性激活函数的缓冲区以及最后的 $1\\times 1$ 预测层。\n- 块粒度的梯度检查点意味着：在每个编码器和解码器块中，只有第二个卷积之后的输出会在前向传播结束后被存储；块内的所有中间激活值都会被丢弃并在反向传播期间重新计算。编码器块的输出必须为前向跳跃连接保留；这些张量与块级别的检查点完全相同。\n\n从张量形状、元素数量和内存使用（$\\text{bytes}=\\text{element count}\\times s$）的定义出发，完成以下任务：\n1) 推导在不使用检查点的情况下，前向传播结束时的峰值激活内存的闭式表达式 $M_{\\text{no}}(b)$（以字节为单位），作为 $H$、$W$、$c$、$L$、$b$ 和 $s$ 的函数。\n2) 推导在使用块级别检查点的情况下，峰值激活内存的闭式表达式 $M_{\\text{ckpt}}(b)$（以字节为单位），作为 $H$、$W$、$c$、$L$、$b$ 和 $s$ 的函数。\n3) 使用 $H=W=512$，$c=64$，$L=5$，$s=4$ 字节，以及图形处理器（GPU, Graphics Processing Unit）激活内存预算为 $B=6$ GiB（其中 $1$ GiB $=2^{30}$ 字节）的条件，计算在块级别检查点下可以容纳的最大整数批次大小 $b$。仅报告该 $b$ 作为你的最终答案。", "solution": "该问题要求在两种不同的内存管理方案下，推导 U-Net 架构中峰值激活内存的表达式，然后在一个给定的内存预算下，计算其中一种方案可行的最大批次大小。\n\n### 问题验证\n\n**步骤 1：提取给定信息**\n- 具有 $L$ 个下采样步骤的 U-Net 架构。\n- 编码器层级：$\\ell=0, 1, \\dots, L-1$。\n- 底层：$\\ell=L$。\n- 解码器层级：$\\ell=L-1, L-2, \\dots, 0$。\n- 在层级 $\\ell$，空间分辨率为 $H/2^{\\ell} \\times W/2^{\\ell}$。\n- 在层级 $\\ell$，通道数为 $c \\cdot 2^{\\ell}$。\n- 在层级 $\\ell$ ($0 \\le \\ell  L$) 的编码器块：两个保持通道数的 $3 \\times 3$ 卷积，然后是因子为 2 的下采样。\n- 在层级 $\\ell=L$ 的底层块：两个保持通道数的 $3 \\times 3$ 卷积，无下采样。\n- 在层级 $\\ell$ ($0 \\le \\ell  L$) 的解码器块：上采样 2 倍，与来自层级 $\\ell$ 的编码器跳跃连接拼接，然后通过两个 $3 \\times 3$ 卷积输出 $c \\cdot 2^{\\ell}$ 个通道，分辨率为 $H/2^{\\ell} \\times W/2^{\\ell}$。\n- 输入批次大小：$b$。\n- 数据类型：32 位浮点数，标量大小 $s=4$ 字节。\n- 内存模型（无检查点）：每个卷积层的输出都保留到前向传播结束。\n- 内存模型（梯度检查点）：在每个编码器和解码器块中，只有第二个卷积后的输出被存储。编码器块的输出用作跳跃连接并被保留。\n- 排除项：参数张量、优化器状态、非线性激活函数的缓冲区、最终预测层。\n- 任务 1：推导不使用检查点时的峰值激活内存 $M_{\\text{no}}(b)$。\n- 任务 2：推导使用检查点时的峰值激活内存 $M_{\\text{ckpt}}(b)$。\n- 任务 3：对于 $H=512$，$W=512$，$c=64$，$L=5$，$s=4$ 字节，内存预算 $B=6$ GiB（$1$ GiB $= 2^{30}$ 字节），找到在使用检查点时可容纳的最大整数批次大小 $b$。\n\n**步骤 2：使用提取的给定信息进行验证**\n该问题具有科学依据，因为它以简化但一致的方式描述了一种标准的深度学习架构（U-Net）和一种常见的优化技术（梯度检查点）。定义和约束都已明确，使得问题定义良好且客观。它没有违反任何基本原则，并且可以形式化为数学推导。\n\n**步骤 3：结论与行动**\n问题有效。将提供完整解答。\n\n### 解题推导\n\n单个激活张量的内存占用是其维度和其标量数据类型大小的乘积。对于一个批次大小为 $b$、通道数为 $C$、高度为 $H'$、宽度为 $W'$ 的张量，其内存（以字节为单位）为 $s \\cdot b \\cdot C \\cdot H' \\cdot W'$。\n\n让我们定义在层级 $\\ell$ 的一个张量的内存。在层级 $\\ell$ 的张量有 $c \\cdot 2^{\\ell}$ 个通道和 $H/2^{\\ell} \\times W/2^{\\ell}$ 的空间分辨率。\n元素数量为 $(c \\cdot 2^{\\ell}) \\cdot (H/2^{\\ell}) \\cdot (W/2^{\\ell}) = cHW \\frac{2^{\\ell}}{2^{2\\ell}} = cHW \\cdot 2^{-\\ell}$。\n这样一个张量的内存（对于给定的批次大小 $b$）是：\n$$ M_{\\ell}(b) = s \\cdot b \\cdot (c \\cdot 2^{\\ell}) \\cdot \\frac{H}{2^{\\ell}} \\cdot \\frac{W}{2^{\\ell}} = s \\cdot b \\cdot c \\cdot H \\cdot W \\cdot 2^{-\\ell} $$\n\n所描述的 U-Net 架构包括：\n- 一个包含 $L+1$ 个块（层级 $\\ell = 0, 1, \\dots, L$）的编码器路径。\n- 一个包含 $L$ 个块（层级 $\\ell = L-1, L-2, \\dots, 0$）的解码器路径。\n\n编码器和解码器路径中的每个块都包含两个卷积。问题要求计算前向传播结束时存储的激活值的总内存。\n\n**1) 不使用检查点时的峰值激活内存，$M_{\\text{no}}(b)$**\n\n不使用检查点的内存模型指出，*每个*卷积层的输出都会被保留。这意味着对于每个块（在编码器和解码器中），其两个卷积的输出都会存储在内存中。\n\n- **编码器路径内存**：对于每个层级 $\\ell \\in \\{0, 1, \\dots, L\\}$，会存储两个大小为 $M_{\\ell}(b)$ 的张量。\n编码器路径的总内存为：\n$$ M_{\\text{enc, no}}(b) = \\sum_{\\ell=0}^{L} 2 \\cdot M_{\\ell}(b) = 2 \\cdot s \\cdot b \\cdot c \\cdot H \\cdot W \\sum_{\\ell=0}^{L} 2^{-\\ell} $$\n- **解码器路径内存**：对于每个层级 $\\ell \\in \\{0, 1, \\dots, L-1\\}$，会存储两个大小为 $M_{\\ell}(b)$ 的张量。\n解码器路径的总内存为：\n$$ M_{\\text{dec, no}}(b) = \\sum_{\\ell=0}^{L-1} 2 \\cdot M_{\\ell}(b) = 2 \\cdot s \\cdot b \\cdot c \\cdot H \\cdot W \\sum_{\\ell=0}^{L-1} 2^{-\\ell} $$\n\n总内存是编码器和解码器路径内存的总和：\n$$ M_{\\text{no}}(b) = M_{\\text{enc, no}}(b) + M_{\\text{dec, no}}(b) = 2 \\cdot s \\cdot b \\cdot c \\cdot H \\cdot W \\left( \\sum_{\\ell=0}^{L} 2^{-\\ell} + \\sum_{\\ell=0}^{L-1} 2^{-\\ell} \\right) $$\n这些和是有限几何级数。令 $r = 1/2$。\n$\\sum_{k=0}^{n} r^{k} = \\frac{1-r^{n+1}}{1-r}$。\n$$ \\sum_{\\ell=0}^{L} \\left(\\frac{1}{2}\\right)^{\\ell} = \\frac{1 - (1/2)^{L+1}}{1 - 1/2} = 2(1 - 2^{-L-1}) = 2 - 2^{-L} $$\n$$ \\sum_{\\ell=0}^{L-1} \\left(\\frac{1}{2}\\right)^{\\ell} = \\frac{1 - (1/2)^{L}}{1 - 1/2} = 2(1 - 2^{-L}) = 2 - 2^{-L+1} $$\n将这些代入 $M_{\\text{no}}(b)$ 的表达式中：\n$$ M_{\\text{no}}(b) = 2 s b c H W \\left( (2 - 2^{-L}) + (2 - 2^{-L+1}) \\right) $$\n$$ M_{\\text{no}}(b) = 2 s b c H W \\left( 4 - 2^{-L} - 2 \\cdot 2^{-L} \\right) = 2 s b c H W \\left( 4 - 3 \\cdot 2^{-L} \\right) $$\n$$ M_{\\text{no}}(b) = s \\cdot b \\cdot c \\cdot H \\cdot W \\left( 8 - 3 \\cdot 2^{-L+1} \\right) $$\n\n**2) 使用检查点时的峰值激活内存，$M_{\\text{ckpt}}(b)$**\n\n使用块级别检查点时，只存储每个块的最终输出（第二个卷积的输出）。\n\n- **编码器路径内存**：对于每个层级 $\\ell \\in \\{0, 1, \\dots, L\\}$，会存储一个大小为 $M_{\\ell}(b)$ 的张量。\n$$ M_{\\text{enc, ckpt}}(b) = \\sum_{\\ell=0}^{L} M_{\\ell}(b) = s \\cdot b \\cdot c \\cdot H \\cdot W \\sum_{\\ell=0}^{L} 2^{-\\ell} $$\n- **解码器路径内存**：对于每个层级 $\\ell \\in \\{0, 1, \\dots, L-1\\}$，会存储一个大小为 $M_{\\ell}(b)$ 的张量。\n$$ M_{\\text{dec, ckpt}}(b) = \\sum_{\\ell=0}^{L-1} M_{\\ell}(b) = s \\cdot b \\cdot c \\cdot H \\cdot W \\sum_{\\ell=0}^{L-1} 2^{-\\ell} $$\n\n总内存是其和：\n$$ M_{\\text{ckpt}}(b) = M_{\\text{enc, ckpt}}(b) + M_{\\text{dec, ckpt}}(b) = s \\cdot b \\cdot c \\cdot H \\cdot W \\left( \\sum_{\\ell=0}^{L} 2^{-\\ell} + \\sum_{\\ell=0}^{L-1} 2^{-\\ell} \\right) $$\n使用前面的几何级数结果：\n$$ M_{\\text{ckpt}}(b) = s b c H W \\left( (2 - 2^{-L}) + (2 - 2^{-L+1}) \\right) $$\n$$ M_{\\text{ckpt}}(b) = s \\cdot b \\cdot c \\cdot H \\cdot W \\left( 4 - 3 \\cdot 2^{-L} \\right) $$\n正如预期的那样，$M_{\\text{ckpt}}(b) = \\frac{1}{2} M_{\\text{no}}(b)$。\n\n**3) 最大批次大小计算**\n\n我们需要找到满足 $M_{\\text{ckpt}}(b) \\le B$ 的最大整数 $b$。\n给定值：\n$H = 512 = 2^9$\n$W = 512 = 2^9$\n$c = 64 = 2^6$\n$L = 5$\n$s = 4$ 字节 $= 2^2$ 字节\n$B = 6$ GiB $= 6 \\times 2^{30}$ 字节\n\n条件是：\n$$ b \\cdot s \\cdot c \\cdot H \\cdot W \\left( 4 - 3 \\cdot 2^{-L} \\right) \\le B $$\n首先，让我们计算每个批次项目的内存，$M_{\\text{ckpt}}(1)$：\n$$ M_{\\text{ckpt}}(1) = s \\cdot c \\cdot H \\cdot W \\left( 4 - 3 \\cdot 2^{-L} \\right) $$\n常数因子是：\n$$ s \\cdot c \\cdot H \\cdot W = 4 \\cdot 64 \\cdot 512 \\cdot 512 = 2^2 \\cdot 2^6 \\cdot 2^9 \\cdot 2^9 = 2^{26} \\text{ 字节} $$\n依赖于 $L$ 的项是：\n$$ 4 - 3 \\cdot 2^{-L} = 4 - 3 \\cdot 2^{-5} = 4 - \\frac{3}{32} = \\frac{128 - 3}{32} = \\frac{125}{32} = \\frac{125}{2^5} $$\n将这些代回 $M_{\\text{ckpt}}(1)$ 的表达式中：\n$$ M_{\\text{ckpt}}(1) = 2^{26} \\cdot \\frac{125}{2^5} = 125 \\cdot 2^{21} \\text{ 字节} $$\n现在，我们求解 $b$：\n$$ b \\cdot (125 \\cdot 2^{21}) \\le 6 \\cdot 2^{30} $$\n$$ b \\le \\frac{6 \\cdot 2^{30}}{125 \\cdot 2^{21}} $$\n$$ b \\le \\frac{6 \\cdot 2^{30-21}}{125} = \\frac{6 \\cdot 2^9}{125} $$\n由于 $2^9 = 512$：\n$$ b \\le \\frac{6 \\cdot 512}{125} = \\frac{3072}{125} $$\n进行除法运算：\n$$ b \\le 24.576 $$\n由于批次大小 $b$ 必须是整数，因此 $b$ 的最大可能值是此结果的向下取整。\n$$ b_{\\text{max}} = \\lfloor 24.576 \\rfloor = 24 $$", "answer": "$$\\boxed{24}$$", "id": "3193905"}, {"introduction": "掌握了模型的计算和内存分析之后，我们转向一个更高级但同样关键的主题：训练动态。在许多现实世界的分割任务中，尤其是U-Net的主场——医学影像分析，类别不平衡问题普遍存在。本练习将挑战你深入思考不同损失函数（如二元交叉熵与Dice损失）在数据不平衡下的行为差异，并从原理上设计出一种能够动态适应数据特性的混合损失函数，以实现更稳健、更精确的分割性能。[@problem_id:3193860]", "problem": "一个二元语义分割任务使用 U-Net 架构来解决，该架构包含一个压缩空间分辨率的编码器和一个使用跳跃连接来桥接相应编码器和解码器阶段以扩展分辨率的解码器。对于每个像素索引 $i$，U-Net 通过对解码器 logits 进行 logistic sigmoid 变换，输出一个概率 $\\hat{y}_i \\in (0,1)$。考虑使用逐像素损失进行训练，其中真实标签为 $y_i \\in \\{0,1\\}$，在给定数据集中前景（正）类的流行率为 $p = \\mathbb{P}(y=1)$。假设批次足够大，以至于经验比例能够紧密地追踪 $p$。两种常见的损失族是二元交叉熵（BCE）损失和基于重叠的损失（例如，Soft Dice 损失），后者通过预测值和标签的总和进行归一化，以强调集合层面的重叠。假设使用标准的、未加权的 BCE 和标准的、无类别权重的 Soft Dice 损失。我们考虑的混合损失是 $L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$，其中混合参数为 $\\lambda \\in [0,1]$，在多类别情况下逐通道应用。对于评估，重点关注与 Dice 系数密切相关的交并比（IoU）。\n\n从基本原理出发：\n- BCE 在像素级别上是一个恰当评分规则，其产生的逐像素梯度与预测概率和目标标签之间的差异成正比，因此它在所有像素上进行聚合，没有显式的类别大小归一化。\n- Soft Dice 损失可微地近似 Dice 重叠度，并包含通过预测值和标签的总和进行的归一化，这倾向于平衡来自前景和背景的贡献，而不管 $p$ 的值如何。\n\n基于这些基本原理，比较未加权的 sigmoid + BCE 与 Dice + BCE 混合损失在不同 $p$ 值下的表现。特别地，推理在初始化附近（通常 $\\hat{y} \\approx 0.5$）时，BCE 下负样本与正样本的预期聚合梯度贡献，以及由于归一化导致的 Dice 损失近似的 $p$-不变性。利用这一点来描绘 IoU 的优势区域：对于哪些 $p$ 的范围，你期望混合损失优于纯 BCE，反之亦然？然后，为每个类别通道提出一个有原则的、特定于类别的规则来选择作为 $p$ 的函数的 $\\lambda$，旨在将聚合梯度不平衡保持在选定的容忍度内，并解释其基本原理。\n\n选择与上述第一性原理推理最一致的陈述。\n\n选项：\nA. 当前景流行率 $p$ 很小（$p \\ll 0.5$）时，一个混合损失 $L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$，其中 $\\lambda$ 随 $\\lvert p - 0.5 \\rvert$ 增大而增大，通常会产生比纯 $L_{\\text{BCE}}$ 更高的 IoU。一个方便的特定于类别的规则是 $\\lambda(p) = 1 - 2 \\lvert p - 0.5 \\rvert$ 并裁剪到 $[0,1]$ 区间，应用于每个类别通道；当 $p \\approx 1$ 时，倾向于使用小的 $\\lambda$。\n\nB. 纯 $L_{\\text{BCE}}$ 直接优化 IoU，并且对任何 $p$ 值的类别不平衡都不敏感，因此它对所有 $p$ 值都是最优的；基于重叠的归一化损失会增加不必要的偏差。\n\nC. 如果 $p$ 非常小，应始终设置 $\\lambda = 1$（纯 $L_{\\text{Dice}}$），如果 $p$ 非常大，则始终设置 $\\lambda = 0$（纯 $L_{\\text{BCE}}$）；混合从无助于校准或稳定性。\n\nD. 一个有原则的混合是 $\\lambda(p) = p$：当前景变得普遍（$p$ 大）时，更多地依赖 $L_{\\text{Dice}}$；当 $p$ 小时，更多地依赖 $L_{\\text{BCE}}$。\n\nE. 在初始化附近，未加权 $L_{\\text{BCE}}$ 下负样本与正样本的聚合梯度量级的预期比率约为 $(1-p)/p$，而 $L_{\\text{Dice}}$ 由于其归一化而对 $p$ 近似不变。因此，选择一个可接受的不平衡容忍度 $k  1$，并设置混合比例，使得有效比率不超过 $k$：当 $\\max\\{(1-p)/p,\\, p/(1-p)\\}  k$ 时，要求 $\\lambda \\ge \\dfrac{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - k}{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - 1}$；否则使用一个小的 $\\lambda$（例如 $\\lambda \\le 0.5$）。这将优势区域映射如下：当 $p$ 极端（接近 $0$ 或接近 $1$）时，Dice + BCE 混合损失在 IoU 上占主导地位，而当 $p \\approx 0.5$ 时，纯 BCE 或低 $\\lambda$ 的混合损失就足够了。通过估计每个类别的流行率 $p_c$，将相同的逐类 $\\lambda(p)$ 规则应用于多类别 U-Net。", "solution": "用户需要对问题陈述进行批判性验证，然后进行第一性原理推导并评估给定选项。\n\n### 问题验证\n\n#### 步骤 1：提取已知信息\n\n*   **任务**：二元语义分割。\n*   **架构**：U-Net。\n*   **输出**：通过 logistic sigmoid 变换得到的逐像素概率 $\\hat{y}_i \\in (0,1)$。\n*   **真实值**：逐像素标签 $y_i \\in \\{0,1\\}$。\n*   **类别流行率**：前景（正）类的流行率为 $p = \\mathbb{P}(y=1)$。\n*   **假设**：批次足够大，以至于经验比例能够近似 $p$。\n*   **损失函数**：\n    *   $L_{\\text{BCE}}$: 未加权的二元交叉熵。\n    *   $L_{\\text{Dice}}$: 标准的 Soft Dice 损失，一种基于重叠的损失。\n*   **混合损失**：$L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$，其中 $\\lambda \\in [0,1]$。\n*   **多类别扩展**：损失逐通道应用。\n*   **评估指标**：交并比（IoU）。\n*   **前提 1 (BCE)**：$L_{\\text{BCE}}$ 是一个恰当评分规则；像素级梯度与 $\\hat{y}_i - y_i$ 成正比；聚合时没有类别大小归一化。\n*   **前提 2 (Dice)**：$L_{\\text{Dice}}$ 近似 Dice 重叠度；包含通过预测值和标签的总和进行的归一化；倾向于平衡前景和背景的贡献，而不管 $p$ 的值。\n*   **推理假设**：分析在初始化附近进行，此时通常 $\\hat{y} \\approx 0.5$。\n\n#### 步骤 2：使用提取的已知信息进行验证\n\n*   **科学依据**：该问题牢固地植根于成熟的深度学习计算机视觉领域。U-Net、BCE 损失、Dice 损失、IoU 以及类别不平衡的挑战都是标准和基本的概念。对 BCE 和 Dice 损失行为的描述是其数学性质的准确提炼。\n*   **问题良定性**：问题是良定的。它要求在不同条件（$p$）下对损失函数行为进行合理的比较，并选择与此推理最一致的选项。提供的前提足以推导出逻辑结论。\n*   **客观性**：问题以精确、客观和技术性的语言陈述，没有主观性或歧义。\n\n问题陈述通过了所有验证标准。它没有表现出任何列出的缺陷（例如，科学上不健全、不完整、有歧义）。\n\n#### 步骤 3：结论与行动\n\n*   **结论**：问题是**有效的**。\n*   **行动**：进行解决方案推导。\n\n### 解决方案推导\n\n问题的核心是理解类别不平衡（由前景流行率 $p$ 量化）如何影响 U-Net 在不同损失函数下的训练动态，并因此影响以 IoU 衡量的最终性能。\n\n#### 1. $L_{\\text{BCE}}$ 与类别不平衡分析\n\n单个像素 $i$ 的二元交叉熵损失由下式给出：\n$$L_{\\text{BCE}, i} = -[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$\n该损失相对于 logit $z_i$（其中 $\\hat{y}_i = \\sigma(z_i) = 1/(1+e^{-z_i})$）的梯度为：\n$$\\frac{\\partial L_{\\text{BCE}, i}}{\\partial z_i} = \\hat{y}_i - y_i$$\n总损失是批次中所有 $N$ 个像素的平均值，$L_{\\text{BCE}} = \\frac{1}{N} \\sum_{i=1}^N L_{\\text{BCE}, i}$。相对于任何网络参数的总梯度是这些逐像素贡献的总和。\n\n根据问题的前提，我们分析初始化附近的情况，此时所有 $i$ 的 $\\hat{y}_i \\approx 0.5$。\n正像素（前景，$y_i=1$）的预期数量为 $N \\cdot p$。\n负像素（背景，$y_i=0$）的预期数量为 $N \\cdot (1-p)$。\n\n来自正像素的聚合梯度贡献与以下成正比：\n$$\\sum_{i \\text{ s.t. } y_i=1} (\\hat{y}_i - y_i) \\approx \\sum_{i \\text{ s.t. } y_i=1} (0.5 - 1) = -0.5 \\cdot (N \\cdot p)$$\n来自负像素的聚合梯度贡献与以下成正比：\n$$\\sum_{i \\text{ s.t. } y_i=0} (\\hat{y}_i - y_i) \\approx \\sum_{i \\text{ s.t. } y_i=0} (0.5 - 0) = 0.5 \\cdot (N \\cdot (1-p))$$\n\n聚合负梯度量级与聚合正梯度量级的比率为：\n$$\\frac{|0.5 \\cdot N \\cdot (1-p)|}{|-0.5 \\cdot N \\cdot p|} = \\frac{1-p}{p}$$\n如果 $p$ 很小（例如 $p=0.01$），这个比率是 $\\frac{0.99}{0.01} = 99$。来自背景像素的梯度以 99 倍的优势压倒了来自前景像素的梯度。因此，网络被激励在任何地方都预测背景，导致前景类的 IoU 非常低。相反，如果 $p$ 很大（例如 $p=0.99$），比率是 $\\frac{0.01}{0.99} \\approx 0.01$，前景梯度占主导。\n因此，未加权的 $L_{\\text{BCE}}$ 对类别不平衡高度敏感。当 $p$ 远非 $0.5$ 时，其在像 IoU 这样的重叠指标上的性能会显著下降。\n\n#### 2. $L_{\\text{Dice}}$ 与类别不平衡分析\n\nSoft Dice 损失通常表示为 $L_{\\text{Dice}} = 1 - D_s$，其中 Soft Dice 系数 $D_s$ 为：\n$$D_s = \\frac{2 \\sum_i y_i \\hat{y}_i + \\epsilon}{\\sum_i y_i^2 + \\sum_i \\hat{y}_i^2 + \\epsilon} \\quad \\text{或类似变体，如} \\quad D_s = \\frac{2 \\sum_i y_i \\hat{y}_i + \\epsilon}{\\sum_i y_i + \\sum_i \\hat{y}_i + \\epsilon}$$\n如问题陈述中所指出的，关键特征是分母，它涉及对所有预测和标签的求和。这个项作为一个归一化因子。损失是根据整个批次/图像的聚合统计数据计算的，而不是简单平均的逐像素基础。这种结构使得损失关注于预测集和真实集之间的重叠，这也是 IoU 所度量的。通过按预测集和真实集的大小进行归一化，它内在平衡了前景和背景的重要性。如果一个小的前景对象被漏掉，它会对损失产生显著影响，无论有多少背景像素被正确分类。因此，$L_{\\text{Dice}}$ 相对不受类别流行率 $p$ 的影响。\n\n#### 3. 混合损失与最优混合\n\n根据以上性质：\n*   当 $p \\approx 0.5$（类别平衡）时，$L_{\\text{BCE}}$ 是有效的。其梯度表现良好，并且作为恰当评分规则，它鼓励良好校准的概率。\n*   当 $p \\to 0$ 或 $p \\to 1$（极端不平衡）时，$L_{\\text{BCE}}$ 被多数类主导，导致 IoU 差。在这些情况下，$L_{\\text{Dice}}$ 因其固有的平衡性而表现更优。\n\n混合损失 $L = \\lambda L_{\\text{Dice}} + (1 - \\lambda) L_{\\text{BCE}}$ 允许进行权衡。混合参数 $\\lambda$ 应根据不平衡程度来选择。\n*   对于低不平衡（$p \\approx 0.5$），$\\lambda$ 应该小。\n*   对于高不平衡（$p \\to 0$ 或 $p \\to 1$），$\\lambda$ 应该大。\n\n不平衡程度可以通过 $\\max\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\}$ 来量化。选择 $\\lambda$ 的一个有原则的规则应该使 $\\lambda$ 成为这个不平衡因子的增函数。\n\n### 逐项分析\n\n*   **A. 当前景流行率 $p$ 很小（$p \\ll 0.5$）时，一个混合...其中 $\\lambda$ 随 $\\lvert p - 0.5 \\rvert$ 增大而增大，通常会产生更高的 IoU... 一个方便的特定于类别的规则是 $\\lambda(p) = 1 - 2 \\lvert p - 0.5 \\rvert$ 并裁剪到 $[0,1]$ 区间...**\n    *   陈述的第一部分，即 $\\lambda$ 随不平衡度（由 $\\lvert p - 0.5 \\rvert$ 衡量）增加的混合损失更优，是正确的。然而，提出的规则 $\\lambda(p) = 1 - 2 \\lvert p - 0.5 \\rvert$ 存在根本性缺陷。对于极端不平衡（$p=0$ 或 $p=1$），此规则给出 $\\lambda = 1 - 2(0.5) = 0$，推荐纯 BCE。对于完美平衡（$p=0.5$），它给出 $\\lambda = 1 - 2(0) = 1$，推荐纯 Dice。这与期望的行为正好相反。\n    *   **结论**：不正确。\n\n*   **B. 纯 $L_{\\text{BCE}}$ 直接优化 IoU，并且对任何 $p$ 值的类别不平衡都不敏感，因此它对所有 $p$ 值都是最优的；基于重叠的归一化损失会增加不必要的偏差。**\n    *   这个陈述包含几个错误的论断。$L_{\\text{BCE}}$ 并不直接优化 IoU；它优化的是逐像素的对数似然。如上所述，它对类别不平衡高度敏感。基于重叠的归一化损失是*对抗*由不平衡引入的偏差的主要工具，而不是增加偏差。\n    *   **结论**：不正确。\n\n*   **C. 如果 $p$ 非常小，应始终设置 $\\lambda = 1$（纯 $L_{\\text{Dice}}$），如果 $p$ 非常大，则始终设置 $\\lambda = 0$（纯 $L_{\\text{BCE}}$）；混合从无助于校准或稳定性。**\n    *   对小 $p$ 的建议是合理的。然而，对大 $p$（例如 $p=0.99$）的建议是有缺陷的。这也是一个极端不平衡的情况，其中背景类是罕见的，同样的逻辑也适用：$L_{\\text{BCE}}$ 会有偏差，而 $L_{\\text{Dice}}$ 会有帮助。声称混合“从无助于”是一个过于强烈且错误的概括；混合损失被广泛使用，正是因为它们通常能改善性能和训练稳定性。\n    *   **结论**：不正确。\n\n*   **D. 一个有原则的混合是 $\\lambda(p) = p$：当前景变得普遍（$p$ 大）时，更多地依赖 $L_{\\text{Dice}}$；当 $p$ 小时，更多地依赖 $L_{\\text{BCE}}$。**\n    *   这个规则建议对于罕见的前景类（$p \\to 0$），应该使用 $\\lambda \\to 0$，即纯 $L_{\\text{BCE}}$。这恰恰是 $L_{\\text{BCE}}$ 因不平衡而表现不佳的场景。这个规则是反直觉且有缺陷的。\n    *   **结论**：不正确。\n\n*   **E. 在初始化附近，未加权 $L_{\\text{BCE}}$ 下负样本与正样本的聚合梯度量级的预期比率约为 $(1-p)/p$，而 $L_{\\text{Dice}}$ 由于其归一化而对 $p$ 近似不变。因此，选择一个可接受的不平衡容忍度 $k  1$，并设置混合比例，使得有效比率不超过 $k$：当 $\\max\\{(1-p)/p,\\, p/(1-p)\\}  k$ 时，要求 $\\lambda \\ge \\dfrac{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - k}{\\max\\left\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\right\\} - 1}$；否则使用一个小的 $\\lambda$（例如 $\\lambda \\le 0.5$）。这将优势区域映射如下：当 $p$ 极端（接近 $0$ 或接近 $1$）时，Dice + BCE 混合损失在 IoU 上占主导地位，而当 $p \\approx 0.5$ 时，纯 BCE 或低 $\\lambda$ 的混合损失就足够了。通过估计每个类别的流行率 $p_c$，将相同的逐类 $\\lambda(p)$ 规则应用于多类别 U-Net。**\n    *   该陈述正确地指出了 BCE 梯度不平衡比为 $\\frac{1-p}{p}$（或其倒数）。它正确地指出 $L_{\\text{Dice}}$ 近似不变。然后，它提出了一个有原则的方法来控制整体梯度不平衡。设不平衡因子为 $I(p) = \\max\\{\\frac{1-p}{p}, \\frac{p}{1-p}\\}$。混合损失的有效不平衡可以近似为 $(1-\\lambda)I(p) + \\lambda$。将其设置为小于或等于容忍度 $k$ 得到 $(1-\\lambda)I(p) + \\lambda \\le k$。当 $I(p)  k$ 时解出 $\\lambda$ 得到 $\\lambda \\ge \\frac{I(p) - k}{I(p) - 1}$。所提出的公式是正确的。由此得出的优势区域映射（极端 $p$ 用混合损失， $p \\approx 0.5$ 用 BCE 主导的损失）与我们的第一性原理分析一致。对多类别情况的扩展是合乎逻辑的。该选项是问题所要求的推理的完整和正确的概括。\n    *   **结论**：正确。", "answer": "$$\\boxed{E}$$", "id": "3193860"}]}