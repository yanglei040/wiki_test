{"hands_on_practices": [{"introduction": "快速神经风格迁移的关键创新之一是自适应实例归一化 (AdaIN)。为了真正理解这类模型的工作原理，我们必须掌握其核心数学机制，即反向传播。本练习将引导你从基本原理出发，推导 AdaIN 层的梯度，这是理解和构建自定义神经网络层的一项基本技能。[@problem_id:3158636]", "problem": "自适应实例归一化 (AdaIN) 是一种用于神经风格迁移的变换，它将内容特征的均值和方差与风格特征的均值和方差对齐。考虑一个单通道的内容特征，由向量 $x \\in \\mathbb{R}^{M}$ 表示，其中包含 $M$ 个空间元素。设内容均值和标准差定义如下\n$$\\mu_{c}(x) = \\frac{1}{M} \\sum_{j=1}^{M} x_{j}, \\quad v(x) = \\frac{1}{M} \\sum_{j=1}^{M} \\left(x_{j} - \\mu_{c}(x)\\right)^{2}, \\quad \\sigma_{c}(x) = \\sqrt{v(x) + \\epsilon},$$\n其中 $\\epsilon > 0$ 是为了数值稳定性而添加的一个很小的常数。设 $\\mu_{s} \\in \\mathbb{R}$ 和 $\\sigma_{s} \\in \\mathbb{R}_{>0}$ 是从风格特征计算出的逐通道风格统计量。AdaIN 的输出按元素定义为\n$$y_{i} = \\sigma_{s} \\frac{x_{i} - \\mu_{c}(x)}{\\sigma_{c}(x)} + \\mu_{s}, \\quad i = 1, \\dots, M.$$\n设 $L(y)$ 是一个依赖于 $y$ 的可微标量损失。用 $g \\in \\mathbb{R}^{M}$ 表示上游梯度，其中 $g_{i} = \\frac{\\partial L}{\\partial y_{i}}$。定义中心化的内容特征 $\\bar{x}_{i} = x_{i} - \\mu_{c}(x)$ 和全1向量 $\\mathbf{1} \\in \\mathbb{R}^{M}$。\n\n从均值、方差和微分链式法则的标准定义出发，推导梯度 $\\frac{\\partial L}{\\partial \\mu_{s}}$、$\\frac{\\partial L}{\\partial \\sigma_{s}}$ 以及向量梯度 $\\nabla_{x} L = \\left(\\frac{\\partial L}{\\partial x_{1}}, \\dots, \\frac{\\partial L}{\\partial x_{M}}\\right)^{\\top}$ 的闭式表达式，这些表达式应以 $g$、$x$、$\\mu_{c}(x)$、$\\sigma_{c}(x)$、$\\mu_{s}$、$\\sigma_{s}$、$M$ 和 $\\epsilon$ 表示。您的最终表达式必须是精确的，并使用定义所蕴含的恒等式进行简化。将最终答案表示为一个包含这三个梯度表达式的单行矩阵，其中第三项应写成向量表达式。无需进行数值近似。", "solution": "## 问题验证\n\n### 步骤 1：提取已知条件\n问题提供了以下定义和变量：\n- 内容特征：$x \\in \\mathbb{R}^{M}$\n- 内容均值：$\\mu_{c}(x) = \\frac{1}{M} \\sum_{j=1}^{M} x_{j}$\n- 内容方差：$v(x) = \\frac{1}{M} \\sum_{j=1}^{M} \\left(x_{j} - \\mu_{c}(x)\\right)^{2}$\n- 内容标准差：$\\sigma_{c}(x) = \\sqrt{v(x) + \\epsilon}$，其中 $\\epsilon > 0$\n- 风格统计量：$\\mu_{s} \\in \\mathbb{R}$ 和 $\\sigma_{s} \\in \\mathbb{R}_{>0}$\n- AdaIN 输出（逐元素）：$y_{i} = \\sigma_{s} \\frac{x_{i} - \\mu_{c}(x)}{\\sigma_{c}(x)} + \\mu_{s}$，对于 $i = 1, \\dots, M$\n- 可微标量损失：$L(y)$\n- 上游梯度向量：$g \\in \\mathbb{R}^{M}$，其中 $g_{i} = \\frac{\\partial L}{\\partial y_{i}}$\n- 中心化内容特征：$\\bar{x}_{i} = x_{i} - \\mu_{c}(x)$\n- 全1向量：$\\mathbf{1} \\in \\mathbb{R}^{M}$\n\n目标是推导 $\\frac{\\partial L}{\\partial \\mu_{s}}$、$\\frac{\\partial L}{\\partial \\sigma_{s}}$ 和 $\\nabla_{x} L$ 的闭式表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题依据以下验证标准进行检验：\n- **科学依据：** 这是一个标准的微积分练习，具体而言，是将微分链式法则（反向传播）应用于一个已定义的数学变换（AdaIN）。AdaIN 是深度学习中用于神经风格迁移的一种成熟技术。其公式基于均值和方差的标准统计定义。所有方面都在数学和计算机科学中有坚实的基础。\n- **适定性：** 问题要求推导一个标量函数关于其参数和输入的梯度。鉴于 AdaIN 变换由可微的基本运算（加法、乘法、除法、平方根）组成（当 $\\sigma_c > 0$ 时），并且损失函数 $L$ 被声明为可微的，因此梯度是良定义的。该问题的结构保证了其具有唯一的解析解。\n- **客观性：** 问题使用精确的数学定义和符号进行陈述。没有主观、模糊或基于观点的陈述。\n- **完整性和一致性：** 提供了所有必要的定义和变量。没有内部矛盾。\n\n### 步骤 3：结论与行动\n该问题是有效的。这是深度学习领域内一个定义明确的数学推导任务。将提供完整的解法。\n\n## 梯度推导\n\n推导过程将应用微积分的链式法则。损失 $L$ 是输出 $y = (y_1, \\dots, y_M)$ 的函数，而输出 $y$ 又是风格统计量 $\\mu_s, \\sigma_s$ 和内容特征 $x$ 的函数。\n\n### 1. 关于 $\\mu_{s}$ 的梯度\n损失 $L$ 关于风格均值 $\\mu_{s}$ 的梯度是通过将 $\\mu_{s}$ 对每个输出元素 $y_i$ 的影响求和得到的。\n$$ \\frac{\\partial L}{\\partial \\mu_{s}} = \\sum_{i=1}^{M} \\frac{\\partial L}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial \\mu_{s}} $$\n根据 $y_i$ 的定义，我们有：\n$$ \\frac{\\partial y_{i}}{\\partial \\mu_{s}} = \\frac{\\partial}{\\partial \\mu_{s}} \\left( \\sigma_{s} \\frac{x_{i} - \\mu_{c}(x)}{\\sigma_{c}(x)} + \\mu_{s} \\right) = 1 $$\n将此代入链式法则表达式，并使用给定的定义 $g_i = \\frac{\\partial L}{\\partial y_{i}}$：\n$$ \\frac{\\partial L}{\\partial \\mu_{s}} = \\sum_{i=1}^{M} g_{i} \\cdot 1 = \\sum_{i=1}^{M} g_{i} $$\n\n### 2. 关于 $\\sigma_{s}$ 的梯度\n类似地，关于风格标准差 $\\sigma_{s}$ 的梯度为：\n$$ \\frac{\\partial L}{\\partial \\sigma_{s}} = \\sum_{i=1}^{M} \\frac{\\partial L}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial \\sigma_{s}} $$\n$y_i$ 关于 $\\sigma_s$ 的偏导数为：\n$$ \\frac{\\partial y_{i}}{\\partial \\sigma_{s}} = \\frac{\\partial}{\\partial \\sigma_{s}} \\left( \\sigma_{s} \\frac{x_{i} - \\mu_{c}(x)}{\\sigma_{c}(x)} + \\mu_{s} \\right) = \\frac{x_{i} - \\mu_{c}(x)}{\\sigma_{c}(x)} $$\n代入可得：\n$$ \\frac{\\partial L}{\\partial \\sigma_{s}} = \\sum_{i=1}^{M} g_{i} \\frac{x_{i} - \\mu_{c}(x)}{\\sigma_{c}(x)} $$\n\n### 3. 关于 $x$ 的梯度\n$L$ 关于输入特征 $x_k$ 的梯度更为复杂，因为除了对 $y_k$ 的直接影响外，$x_k$ 还通过共享的统计量 $\\mu_c(x)$ 和 $\\sigma_c(x)$ 影响所有输出元素 $y_i$。我们使用多元链式法则，考虑从 $x_k$ 到 $L$ 的所有影响路径。\n$L$ 关于 $x_k$ 的全导数为：\n$$ \\frac{\\partial L}{\\partial x_k} = \\sum_{i=1}^{M} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial x_k} = \\sum_{i=1}^{M} g_i \\frac{\\partial y_i}{\\partial x_k} $$\n项 $\\frac{\\partial y_i}{\\partial x_k}$ 可以展开为：\n$$ \\frac{\\partial y_i}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\sigma_s \\frac{x_i - \\mu_c(x)}{\\sigma_c(x)} + \\mu_s \\right) = \\sigma_s \\frac{\\partial}{\\partial x_k} \\left( \\frac{x_i - \\mu_c(x)}{\\sigma_c(x)} \\right) $$\n使用商法则 $\\frac{d}{dt} \\frac{u}{v} = \\frac{u'v - uv'}{v^2}$：\n$$ \\frac{\\partial y_i}{\\partial x_k} = \\sigma_s \\frac{ \\frac{\\partial}{\\partial x_k}(x_i - \\mu_c(x)) \\sigma_c(x) - (x_i - \\mu_c(x)) \\frac{\\partial \\sigma_c(x)}{\\partial x_k} }{ \\sigma_c(x)^2 } $$\n我们需要计算中间项的导数：\na) $\\frac{\\partial \\mu_c(x)}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{M} \\sum_{j=1}^{M} x_j \\right) = \\frac{1}{M}$\nb) $\\frac{\\partial}{\\partial x_k}(x_i - \\mu_c(x)) = \\frac{\\partial x_i}{\\partial x_k} - \\frac{\\partial \\mu_c(x)}{\\partial x_k} = \\delta_{ik} - \\frac{1}{M}$，其中 $\\delta_{ik}$ 是克罗内克 δ 函数。\nc) 为了求出 $\\frac{\\partial \\sigma_c(x)}{\\partial x_k}$，我们首先求出 $\\frac{\\partial v(x)}{\\partial x_k}$。最简单的方法是使用方差的另一种形式：$v(x) = \\frac{1}{M}\\sum_{j=1}^{M} x_j^2 - (\\mu_c(x))^2$。\n$$ \\frac{\\partial v(x)}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{M}\\sum_{j=1}^{M} x_j^2 - (\\mu_c(x))^2 \\right) = \\frac{1}{M}(2x_k) - 2\\mu_c(x)\\frac{\\partial \\mu_c(x)}{\\partial x_k} = \\frac{2x_k}{M} - 2\\mu_c(x)\\frac{1}{M} = \\frac{2}{M}(x_k - \\mu_c(x)) $$\n现在我们可以求出 $\\sigma_c(x) = \\sqrt{v(x) + \\epsilon}$ 的导数：\n$$ \\frac{\\partial \\sigma_c(x)}{\\partial x_k} = \\frac{1}{2\\sqrt{v(x) + \\epsilon}} \\frac{\\partial v(x)}{\\partial x_k} = \\frac{1}{2\\sigma_c(x)} \\frac{2}{M}(x_k - \\mu_c(x)) = \\frac{x_k - \\mu_c(x)}{M \\sigma_c(x)} $$\n将这些导数代回到 $\\frac{\\partial y_i}{\\partial x_k}$ 的表达式中：\n$$ \\frac{\\partial y_i}{\\partial x_k} = \\frac{\\sigma_s}{\\sigma_c(x)^2} \\left[ \\left(\\delta_{ik} - \\frac{1}{M}\\right)\\sigma_c(x) - (x_i - \\mu_c(x))\\frac{x_k - \\mu_c(x)}{M \\sigma_c(x)} \\right] $$\n$$ \\frac{\\partial y_i}{\\partial x_k} = \\frac{\\sigma_s}{\\sigma_c(x)} \\left[ \\delta_{ik} - \\frac{1}{M} - \\frac{(x_i - \\mu_c(x))(x_k - \\mu_c(x))}{M \\sigma_c(x)^2} \\right] $$\n最后，我们通过对 $i$ 求和来计算 $\\frac{\\partial L}{\\partial x_k}$：\n$$ \\frac{\\partial L}{\\partial x_k} = \\sum_{i=1}^{M} g_i \\frac{\\partial y_i}{\\partial x_k} = \\frac{\\sigma_s}{\\sigma_c(x)} \\sum_{i=1}^{M} g_i \\left[ \\delta_{ik} - \\frac{1}{M} - \\frac{(x_i - \\mu_c(x))(x_k - \\mu_c(x))}{M \\sigma_c(x)^2} \\right] $$\n分配求和：\n$$ \\frac{\\partial L}{\\partial x_k} = \\frac{\\sigma_s}{\\sigma_c(x)} \\left[ \\sum_{i=1}^{M} g_i \\delta_{ik} - \\frac{1}{M} \\sum_{i=1}^{M} g_i - \\frac{x_k - \\mu_c(x)}{M \\sigma_c(x)^2} \\sum_{i=1}^{M} g_i (x_i - \\mu_c(x)) \\right] $$\n第一项简化为 $\\sum_{i=1}^{M} g_i \\delta_{ik} = g_k$。这给出了梯度的分量表达式：\n$$ \\frac{\\partial L}{\\partial x_k} = \\frac{\\sigma_s}{\\sigma_c(x)} \\left[ g_k - \\frac{1}{M}\\sum_{j=1}^{M} g_j - \\frac{x_k - \\mu_c(x)}{M \\sigma_c(x)^2} \\sum_{j=1}^{M} g_j (x_j - \\mu_c(x)) \\right] $$\n这个表达式可以写成关于 $\\nabla_x L$ 的单个向量方程。设 $g$ 为上游梯度向量，$x$ 为输入向量，$\\mathbf{1}$ 为全1向量。使用已定义的 $\\bar{x} = x - \\mu_c(x)\\mathbf{1}$：\n$$ \\nabla_{x} L = \\frac{\\sigma_s}{\\sigma_c(x)} \\left[ g - \\frac{1}{M}\\left(\\sum_{j=1}^{M} g_j\\right)\\mathbf{1} - \\frac{1}{M \\sigma_c(x)^2} \\left(\\sum_{j=1}^{M} g_j (x_j - \\mu_c(x))\\right) (x - \\mu_c(x)\\mathbf{1}) \\right] $$\n\n推导出的三个梯度是：\n1. $\\frac{\\partial L}{\\partial \\mu_{s}} = \\sum_{j=1}^{M} g_j$\n2. $\\frac{\\partial L}{\\partial \\sigma_{s}} = \\sum_{j=1}^{M} g_j \\frac{x_j - \\mu_c(x)}{\\sigma_c(x)}$\n3. $\\nabla_{x} L = \\frac{\\sigma_s}{\\sigma_c(x)} \\left[ g - \\frac{1}{M}\\left(\\sum_{j=1}^{M} g_j\\right)\\mathbf{1} - \\frac{1}{M \\sigma_c(x)^2} \\left(\\sum_{j=1}^{M} g_j (x_j - \\mu_c(x))\\right) (x - \\mu_c(x)\\mathbf{1}) \\right]$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\sum_{j=1}^{M} g_j & \\sum_{j=1}^{M} g_j \\frac{x_j - \\mu_c(x)}{\\sigma_c(x)} & \\frac{\\sigma_s}{\\sigma_c(x)} \\left[ g - \\frac{1}{M}\\left(\\sum_{j=1}^{M} g_j\\right)\\mathbf{1} - \\frac{1}{M \\sigma_c(x)^2} \\left(\\sum_{j=1}^{M} g_j (x_j - \\mu_c(x))\\right) (x - \\mu_c(x)\\mathbf{1}) \\right] \\end{pmatrix} } $$", "id": "3158636"}, {"introduction": "理论学习之后，我们需要通过编码实践来加深理解。本练习将探讨经典的基于格拉姆矩阵的风格损失，并研究一个有趣的问题：当风格图像缺少内容图像中的某些纹理时，会发生什么？通过这个简化的模型实验，你将直观地理解风格迁移中特征统计匹配的过程及其可能出现的“内容坍塌”现象。[@problem_id:3158635]", "problem": "您将研究一个简化的、纯数学的神经风格迁移模型，在特征空间中探讨当风格信号缺少内容中存在的关键纹理时会发生什么。考虑一个单层，其通道和空间维度由矩阵表示。令内容特征图表示为 $\\phi(c) \\in \\mathbb{R}^{C \\times N}$，风格特征图表示为 $\\phi(s) \\in \\mathbb{R}^{C \\times N}$，其中 $C$ 是通道数，$N$ 是空间位置数（例如，$N$ 可以是特征图中所有空间位置的像素总数）。待优化的风格化特征为 $\\Phi \\in \\mathbb{R}^{C \\times N}$。风格格拉姆矩阵定义为 $G_{s} = \\frac{1}{N} \\phi(s)\\phi(s)^{\\top} \\in \\mathbb{R}^{C \\times C}$。目标是最小化以下能量：\n$$\n\\mathcal{L}(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2} + \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2},\n$$\n其中 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 是权重，$\\|\\cdot\\|_{F}$ 表示弗罗贝尼乌斯范数。您将直接在特征空间中进行操作，通过梯度下降法优化 $\\Phi$，从 $\\Phi^{(0)} = \\phi(c)$ 开始，使用固定的步长 $\\eta > 0$ 和固定的步数 $T$。\n\n推导和实现的基本依据：\n- 使用弗罗贝尼乌斯范数的定义及其标准变分性质。\n- 对于特征矩阵 $X$，使用格拉姆矩阵的定义 $G = \\frac{1}{N}XX^{\\top}$。\n- 使用矩阵微积分的基本法则以及迹和内积的恒等式。\n\n您的任务：\n- 从目标定义和矩阵微积分的标准法则出发，推导 $\\mathcal{L}(\\Phi)$ 相对于 $\\Phi$ 的梯度，并设计一个梯度下降算法来计算近似最小化器 $\\Phi^{*}$。\n- 通过将 $\\phi(s)$ 中相应通道置零，构造一个缺少内容中存在的关键纹理的风格，并测试优化后的 $\\Phi^{*}$ 是否相对于 $\\phi(c)$ 压缩了该通道的能量。将任意通道索引 $i$ 的能量比定义为 $r_{i} = \\frac{\\|\\Phi^{*}_{i,:}\\|_{2}}{\\|\\phi(c)_{i,:}\\|_{2}}$，其中 $\\|\\cdot\\|_{2}$ 是应用于行向量的欧几里得范数。对于一组通道索引 $S$，将平均比率 $R(S)$ 定义为 $r_{i}$ 在 $i \\in S$ 上的算术平均值。\n- 令“缺失”集合为 $M = \\{ i \\mid \\|\\phi(s)_{i,:}\\|_{2} = 0 \\}$，“存在”集合为 $P = \\{0,\\ldots,C-1\\}\\setminus M$。按照惯例，如果 $M$ 为空，则设 $R(M) = 1.0$；如果 $P$ 为空，则设 $R(P) = 1.0$。\n- 使用坍塌检测阈值 $\\tau = 0.6$：如果 $R(M) \\le \\tau$，则声明发生坍塌。除了比率之外，还需报告最终的目标值 $\\mathcal{L}(\\Phi^{*})$。\n\n您必须遵循的实现细节：\n- 使用梯度下降法，固定步长 $\\eta$ 和固定步数 $T$，从 $\\Phi^{(0)} = \\phi(c)$ 开始。\n- 将 $\\phi(c)$ 和 $\\phi(s)$ 构建为具有恒定列的秩-1矩阵：令 $v_{c} \\in \\mathbb{R}^{C}$ 和 $v_{s} \\in \\mathbb{R}^{C}$ 为通道能量向量，并设置 $\\phi(c) = v_{c}\\mathbf{1}^{\\top}$ 和 $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$，其中 $\\mathbf{1} \\in \\mathbb{R}^{N}$ 是全1向量。\n\n测试套件：\n- 所有测试用例均使用 $C = 3$ 和 $N = 4$，因此 $\\phi(c), \\phi(s) \\in \\mathbb{R}^{3 \\times 4}$。令 $\\mathbf{1} \\in \\mathbb{R}^{4}$。\n- 在以下所有情况中，根据指定的 $v_{c}$ 和 $v_{s}$ 构建 $\\phi(c) = v_{c}\\mathbf{1}^{\\top}$ 和 $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$。\n- 使用以下四种情况，涵盖正常路径、缺失通道的风格以及 $\\alpha$ 和 $\\beta$ 的边界条件：\n  - 情况 A（正常路径，匹配的风格）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [2.0, 1.0, 0.5]$，$\\alpha = 1.0$，$\\beta = 3.0$，$\\eta = 0.05$，$T = 400$。\n  - 情况 B（风格在通道 $0$ 缺少关键纹理）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [0.0, 1.0, 0.5]$，$\\alpha = 1.0$，$\\beta = 3.0$，$\\eta = 0.05$，$T = 400$。\n  - 情况 C（无风格项）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [0.0, 1.0, 0.5]$，$\\alpha = 1.0$，$\\beta = 0.0$，$\\eta = 0.05$，$T = 400$。\n  - 情况 D（仅风格，内容权重为零，风格缺少通道 $0$）：$v_{c} = [2.0, 1.0, 0.5]$，$v_{s} = [0.0, 1.0, 0.5]$，$\\alpha = 0.0$，$\\beta = 3.0$，$\\eta = 0.05$，$T = 400$。\n\n对于每种情况，计算：\n- $R(M)$ 作为缺失通道的平均比率（如果 $M$ 为空，则为 $1.0$）。\n- $R(P)$ 作为存在通道的平均比率（如果 $P$ 为空，则为 $1.0$）。\n- 最终值 $\\mathcal{L}(\\Phi^{*})$。\n- 一个布尔值“检测到坍塌”，由 $R(M) \\le \\tau$ 给出，其中 $\\tau = 0.6$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。\n- 每个测试用例的结果必须是 $[R(M), R(P), \\mathcal{L}(\\Phi^{*}), \\text{collapse}]$ 形式的列表，其中 $R(M)$ 和 $R(P)$ 四舍五入到 $6$ 位小数，$\\mathcal{L}(\\Phi^{*})$ 四舍五入到 $6$ 位小数，$\\text{collapse}$ 是一个布尔值。\n- 因此，最终打印的行具有 $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$ 的结构，没有额外的空格或文本。", "solution": "该问题是有效的，因为它具有科学依据、是良定的且客观的。它为研究简化的神经风格迁移模型提供了一个自洽的、数学上精确的框架。所有参数和条件都已指定，从而能够得到一个唯一且可验证的计算解。\n\n求解过程包括两个主要阶段：首先，推导目标函数 $\\mathcal{L}(\\Phi)$ 的梯度；其次，实现一个梯度下降算法，为每个指定的测试用例找到优化后的特征图 $\\Phi^{*}$。\n\n目标函数由下式给出：\n$$\n\\mathcal{L}(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2} + \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2}\n$$\n这可以写成内容损失 $\\mathcal{L}_c(\\Phi)$ 和风格损失 $\\mathcal{L}_s(\\Phi)$ 的和：\n$$\n\\mathcal{L}_c(\\Phi) = \\alpha \\left\\|\\Phi - \\phi(c)\\right\\|_{F}^{2}\n$$\n$$\n\\mathcal{L}_s(\\Phi) = \\beta \\left\\| \\frac{1}{N}\\Phi\\Phi^{\\top} - G_{s} \\right\\|_{F}^{2}\n$$\n$\\mathcal{L}(\\Phi)$ 相对于 $\\Phi$ 的梯度是其各组成部分梯度的总和：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Phi} = \\frac{\\partial \\mathcal{L}_c}{\\partial \\Phi} + \\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi}\n$$\n\n我们现在将使用弗罗贝尼乌斯范数的定义 $\\|\\mathbf{A}\\|_{F}^2 = \\operatorname{tr}(\\mathbf{A}^{\\top}\\mathbf{A})$ 和矩阵微积分的标准法则来推导每个梯度项。\n\n**1. 内容损失($\\mathcal{L}_c$)的梯度**\n\n内容损失为 $\\mathcal{L}_c(\\Phi) = \\alpha \\operatorname{tr}\\left((\\Phi - \\phi(c))^{\\top}(\\Phi - \\phi(c))\\right)$。\n使用恒等式 $\\frac{\\partial}{\\partial \\mathbf{X}} \\operatorname{tr}((\\mathbf{X}-\\mathbf{A})^{\\top}(\\mathbf{X}-\\mathbf{A})) = 2(\\mathbf{X}-\\mathbf{A})$，我们可以直接求出梯度。\n令 $\\mathbf{X} = \\Phi$ 和 $\\mathbf{A} = \\phi(c)$。那么，内容损失相对于 $\\Phi$ 的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}_c}{\\partial \\Phi} = 2\\alpha(\\Phi - \\phi(c))\n$$\n\n**2. 风格损失($\\mathcal{L}_s$)的梯度**\n\n风格损失为 $\\mathcal{L}_s(\\Phi) = \\beta \\left\\| \\mathbf{G}_{\\Phi} - G_{s} \\right\\|_{F}^{2}$，其中 $\\mathbf{G}_{\\Phi} = \\frac{1}{N}\\Phi\\Phi^{\\top}$ 是特征图 $\\Phi$ 的格拉姆矩阵。\n我们可以将其写作 $\\mathcal{L}_s(\\Phi) = \\beta \\operatorname{tr}\\left((\\mathbf{G}_{\\Phi} - G_s)^{\\top}(\\mathbf{G}_{\\Phi} - G_s)\\right)$。\n由于 $\\mathbf{G}_{\\Phi}$ 和 $G_s$ 是对称的，$(\\mathbf{G}_{\\Phi} - G_s)^{\\top} = \\mathbf{G}_{\\Phi} - G_s$。因此，$\\mathcal{L}_s(\\Phi) = \\beta \\operatorname{tr}\\left((\\mathbf{G}_{\\Phi} - G_s)^2\\right)$。\n\n为了求梯度，我们使用微分。令 $\\mathbf{E} = \\mathbf{G}_{\\Phi} - G_s$。\n$\\mathcal{L}_s$ 的微分是 $d\\mathcal{L}_s = \\beta \\cdot d(\\operatorname{tr}(\\mathbf{E}^{\\top}\\mathbf{E})) = \\beta \\cdot \\operatorname{tr}(d(\\mathbf{E}^{\\top})\\mathbf{E} + \\mathbf{E}^{\\top}d\\mathbf{E})$。由于 $\\mathbf{E}$ 是对称的，这可以简化为 $d\\mathcal{L}_s = 2\\beta \\operatorname{tr}(\\mathbf{E}^{\\top}d\\mathbf{E})$。\n$\\mathbf{E}$ 的微分是 $d\\mathbf{E} = d\\mathbf{G}_{\\Phi} = d\\left(\\frac{1}{N}\\Phi\\Phi^{\\top}\\right) = \\frac{1}{N}(d\\Phi \\cdot \\Phi^{\\top} + \\Phi \\cdot d\\Phi^{\\top})$。\n将 $d\\mathbf{E}$ 代入 $d\\mathcal{L}_s$ 的表达式中：\n$$\nd\\mathcal{L}_s = 2\\beta \\operatorname{tr}\\left( \\mathbf{E}^{\\top} \\frac{1}{N}(d\\Phi \\cdot \\Phi^{\\top} + \\Phi \\cdot d\\Phi^{\\top}) \\right) = \\frac{2\\beta}{N} \\left( \\operatorname{tr}(\\mathbf{E}^{\\top} d\\Phi \\Phi^{\\top}) + \\operatorname{tr}(\\mathbf{E}^{\\top} \\Phi d\\Phi^{\\top}) \\right)\n$$\n使用迹的循环性质 $\\operatorname{tr}(\\mathbf{A}\\mathbf{B}\\mathbf{C}) = \\operatorname{tr}(\\mathbf{C}\\mathbf{A}\\mathbf{B})$ 和性质 $\\operatorname{tr}(\\mathbf{A}) = \\operatorname{tr}(\\mathbf{A}^{\\top})$：\n第一项是 $\\operatorname{tr}(\\mathbf{E}^{\\top} d\\Phi \\Phi^{\\top}) = \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}^{\\top}d\\Phi)$。\n第二项是 $\\operatorname{tr}(\\mathbf{E}^{\\top} \\Phi d\\Phi^{\\top}) = \\operatorname{tr}(d\\Phi^{\\top}\\mathbf{E}^{\\top}\\Phi) = \\operatorname{tr}((\\mathbf{E}^{\\top}\\Phi)^{\\top}d\\Phi) = \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi)$。\n由于 $\\mathbf{E}$ 是对称的，$\\mathbf{E}^{\\top}=\\mathbf{E}$。两项是相同的。\n$$\nd\\mathcal{L}_s = \\frac{2\\beta}{N} \\left( \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi) + \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi) \\right) = \\frac{4\\beta}{N} \\operatorname{tr}(\\Phi^{\\top}\\mathbf{E}d\\Phi)\n$$\n根据恒等式 $df = \\operatorname{tr}(\\mathbf{A}^{\\top}d\\mathbf{X}) \\implies \\frac{\\partial f}{\\partial \\mathbf{X}} = \\mathbf{A}$，我们确定梯度为：\n$$\n\\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi} = \\left(\\frac{4\\beta}{N} \\Phi^{\\top}\\mathbf{E}\\right)^{\\top} = \\frac{4\\beta}{N} \\mathbf{E}^{\\top}\\Phi = \\frac{4\\beta}{N} \\mathbf{E}\\Phi\n$$\n代入 $\\mathbf{E} = \\frac{1}{N}\\Phi\\Phi^{\\top} - G_s$，我们得到：\n$$\n\\frac{\\partial \\mathcal{L}_s}{\\partial \\Phi} = \\frac{4\\beta}{N} \\left(\\frac{1}{N}\\Phi\\Phi^{\\top} - G_s\\right) \\Phi\n$$\n\n**3. 总梯度与梯度下降算法**\n\n结合内容损失和风格损失的梯度，$\\mathcal{L}(\\Phi)$ 的总梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Phi} = 2\\alpha (\\Phi - \\phi(c)) + \\frac{4\\beta}{N} \\left( \\frac{1}{N}\\Phi\\Phi^{\\top} - G_s \\right) \\Phi\n$$\n梯度下降算法通过迭代更新 $\\Phi$ 来最小化 $\\mathcal{L}(\\Phi)$。从 $\\Phi^{(0)} = \\phi(c)$ 开始，每一步 $t$ 的更新规则是：\n$$\n\\Phi^{(t+1)} = \\Phi^{(t)} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\Phi}\\bigg|_{\\Phi=\\Phi^{(t)}}\n$$\n这个过程重复固定的步数 $T$。最终优化后的特征图是 $\\Phi^{*} = \\Phi^{(T)}$。\n\n对于 $\\phi(s) = v_{s}\\mathbf{1}^{\\top}$ 的具体实现，风格格拉姆矩阵简化为一个秩-1矩阵：\n$$\nG_s = \\frac{1}{N}\\phi(s)\\phi(s)^{\\top} = \\frac{1}{N}(v_{s}\\mathbf{1}^{\\top})(\\mathbf{1}v_{s}^{\\top}) = \\frac{1}{N}v_{s}(\\mathbf{1}^{\\top}\\mathbf{1})v_{s}^{\\top} = \\frac{1}{N}v_{s}(N)v_{s}^{\\top} = v_{s}v_{s}^{\\top}\n$$\n这个简化在实现中被使用。经过 $T$ 次迭代后，我们计算所需的度量指标：$R(M)$、$R(P)$、$\\mathcal{L}(\\Phi^{*})$ 以及坍塌检测布尔值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Happy path, matching style\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([2.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n        # Case B: Style lacks key texture in channel 0\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n        # Case C: No style term\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 1.0, 'beta': 0.0, 'eta': 0.05, 'T': 400},\n        # Case D: Style only, content weight zero, style lacks channel 0\n        {'vc': np.array([2.0, 1.0, 0.5]), 'vs': np.array([0.0, 1.0, 0.5]), \n         'alpha': 0.0, 'beta': 3.0, 'eta': 0.05, 'T': 400},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    # Format the final output string as per problem specification.\n    # [[R(M),R(P),L,collapse],[...],...]\n    formatted_results = []\n    for r in results:\n        rm, rp, l_val, c_val = r\n        s = f\"[{rm:.6f},{rp:.6f},{l_val:.6f},{str(c_val).lower()}]\"\n        formatted_results.append(s)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_simulation(vc, vs, alpha, beta, eta, T):\n    \"\"\"\n    Performs the gradient descent optimization and calculates final metrics for one test case.\n    \"\"\"\n    C, N = 3, 4\n    tau = 0.6\n    \n    # Construct feature maps and style Gram matrix\n    ones_N = np.ones((1, N))\n    phi_c = vc.reshape(C, 1) @ ones_N\n    \n    # G_s simplifies to the outer product v_s * v_s^T\n    G_s = vs.reshape(C, 1) @ vs.reshape(1, C)\n\n    # Initialize Phi at the content feature map\n    Phi = phi_c.copy()\n    \n    # Gradient Descent Loop\n    for _ in range(T):\n        # Calculate Gram matrix of the current Phi\n        G_Phi = (1/N) * (Phi @ Phi.T)\n        \n        # Calculate content loss gradient\n        grad_content = 2 * alpha * (Phi - phi_c)\n        \n        # Calculate style loss gradient\n        grad_style = (4 * beta / N) * ((G_Phi - G_s) @ Phi)\n        \n        # Total gradient\n        grad_L = grad_content + grad_style\n        \n        # Update Phi\n        Phi -= eta * grad_L\n\n    Phi_star = Phi\n\n    # Calculate final objective value L(Phi*)\n    G_Phi_star = (1/N) * (Phi_star @ Phi_star.T)\n    loss_content_final = alpha * np.linalg.norm(Phi_star - phi_c, 'fro')**2\n    loss_style_final = beta * np.linalg.norm(G_Phi_star - G_s, 'fro')**2\n    L_final = loss_content_final + loss_style_final\n    \n    # Identify missing (M) and present (P) channel sets\n    M_indices = np.where(vs == 0)[0]\n    P_indices = np.where(vs != 0)[0]\n    \n    # Calculate energy ratios r_i\n    phi_c_row_norms = np.linalg.norm(phi_c, axis=1)\n    phi_star_row_norms = np.linalg.norm(Phi_star, axis=1)\n    \n    # Avoid division by zero, although not an issue with the given vc.\n    ratios = np.divide(phi_star_row_norms, phi_c_row_norms, \n                       out=np.zeros_like(phi_c_row_norms, dtype=float), \n                       where=phi_c_row_norms!=0)\n\n    # Calculate average ratio R(M)\n    if len(M_indices) == 0:\n        R_M = 1.0\n    else:\n        R_M = np.mean(ratios[M_indices])\n        \n    # Calculate average ratio R(P)\n    if len(P_indices) == 0:\n        R_P = 1.0\n    else:\n        R_P = np.mean(ratios[P_indices])\n        \n    # Detect collapse\n    collapse_detected = R_M = tau\n\n    return [R_M, R_P, L_final, collapse_detected]\n\nsolve()\n```", "id": "3158635"}, {"introduction": "在理解了格拉姆矩阵如何用于风格表示之后，让我们更深入地探究其内在属性。这个练习将通过一个合成实验，检验格拉姆矩阵对旋转和裁剪等常见图像变换的不变性程度。通过量化这些变换对风格表示的影响，你将揭示经典神经风格迁移方法的一个重要局限性。[@problem_id:3158617]", "problem": "考虑一个合成实验，用于评估在神经风格迁移 (NST) 中使用的基于格拉姆矩阵的风格表示的不变性。假设风格图像是一个关于空间坐标 $(x, y)$ 的实值函数 $s: \\{0, \\dots, H-1\\} \\times \\{0, \\dots, W-1\\} \\rightarrow \\mathbb{R}$，其中 $H$ 和 $W$ 是指定的。使用卷积神经网络 (CNN) 中的空间卷积定义一个单层特征提取器：对于一个包含 $C$ 个滤波器 $\\{k_c\\}_{c=1}^C$ 的滤波器组，每个特征图 $f_c$ 通过二维卷积 $f_c = s \\star k_c$ 计算，然后应用一个整流线性单元 (ReLU) 非线性，得到 $a_c(x, y) = \\max(0, f_c(x, y))$。将这些激活值 $a_c$ 堆叠成一个张量 $A \\in \\mathbb{R}^{C \\times H \\times W}$。\n\n将第 $l$ 层的格拉姆矩阵 $G_l \\in \\mathbb{R}^{C \\times C}$ 定义为扁平化激活值的归一化通道间内积，\n$$\nG_l = \\frac{1}{N} F F^\\top,\n$$\n其中 $F \\in \\mathbb{R}^{C \\times N}$ 是通过重塑 $A$ 得到的，使得每一行对应一个在空间位置上被扁平化的通道，且 $N = H \\cdot W$。格拉姆矩阵 $G_l$ 是神经风格迁移中用于捕捉跨空间位置的特征激活的二阶统计量的核心组件。\n\n对于应用于风格图像 $s$ 的给定增广 $\\mathcal{T}$，定义增广后的图像 $s_{\\mathcal{T}}$，使用相同的特征提取器计算其对应的格拉姆矩阵 $G_l^{(\\mathcal{T})}$，并通过归一化的弗罗贝尼乌斯差来量化基于格拉姆矩阵的风格捕捉的变化：\n$$\n\\Delta G_l(\\mathcal{T}) = \\frac{\\left\\|G_l^{(\\mathcal{T})} - G_l\\right\\|_F}{\\left\\|G_l\\right\\|_F},\n$$\n其中 $\\|\\cdot\\|_F$ 表示弗罗贝尼乌斯范数。旋转的角度单位必须是度。\n\n你的任务是编写一个完整、可运行的程序，该程序：\n- 构建一个大小为 $H=128$ 和 $W=128$ 的确定性合成风格图像 $s$，该图像由正弦模式的混合定义：\n$$\ns(x,y) = \\sin\\!\\left(2\\pi f_1 \\frac{x}{H}\\right) + \\sin\\!\\left(2\\pi f_2 \\frac{y}{W}\\right) + \\sin\\!\\left(2\\pi f_3 \\left(\\frac{x}{H}\\cos\\alpha + \\frac{y}{W}\\sin\\alpha\\right)\\right),\n$$\n其中 $f_1 = 4$，$f_2 = 6$，$f_3 = 5$，且 $\\alpha = \\frac{\\pi}{4}$。\n- 构建一个包含 $C = 6$ 个滤波器的滤波器组，其中包括：$4$ 个定向 Gabor 滤波器，其方向 $\\theta \\in \\{0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ\\}$，参数为 $\\sigma = 2.5$，$\\gamma = 0.5$，波长 $\\lambda = 4.0$；一个高斯拉普拉斯 (LoG) 滤波器，其标准差为 $\\sigma_{\\text{LoG}} = 2.0$；以及一个高斯模糊滤波器，其标准差为 $\\sigma_{\\text{G}} = 1.5$。每个滤波器都应实现为一个实值 $K \\times K$ 的核（$K = 15$），并如上所述与二维卷积和 ReLU 一起使用。\n- 计算原始图像 $s$ 的 $G_l$，并对下面测试套件中的每个增广 $\\mathcal{T}$，计算 $G_l^{(\\mathcal{T})}$ 和 $\\Delta G_l(\\mathcal{T})$。\n\n待评估的增广 $\\mathcal{T}$（测试套件；角度单位为度，裁剪尺寸单位为像素）：\n1. 旋转 $\\theta = 0^\\circ$；不裁剪。\n2. 旋转 $\\theta = 90^\\circ$；不裁剪。\n3. 旋转 $\\theta = 180^\\circ$；不裁剪。\n4. 旋转 $\\theta = 0^\\circ$；中心裁剪，尺寸为 $64 \\times 64$。\n5. 旋转 $\\theta = 0^\\circ$；左上角裁剪，尺寸为 $64 \\times 64$。\n6. 旋转 $\\theta = 90^\\circ$；中心裁剪，尺寸为 $64 \\times 64$。\n\n裁剪定义为从旋转后的图像中选择一个连续的、轴对齐的子数组：对于中心裁剪，裁剪窗口的起始位置为 $(\\lfloor (H - h)/2 \\rfloor, \\lfloor (W - w)/2 \\rfloor)$；对于左上角裁剪，起始位置为 $(0, 0)$，其中 $(h, w)$ 是裁剪尺寸。\n\n将问题建立在第一性原理之上的科学依据：\n- 空间卷积 $f_c = s \\star k_c$ 定义为 $s$ 与带有空间位移的 $k_c$ 的空间乘积之和，这是 CNN 中的一个基本操作。\n- 格拉姆矩阵 $G_l$ 聚合了跨空间位置的二阶通道统计量，并且通过 $N$ 进行归一化后，对位置的空间置换具有不变性。裁剪改变了位置的集合，因此会根据特征统计量的平稳性而改变 $G_l$。\n- 以 $90^\\circ$ 的倍数旋转会置换空间位置，但不会置换通道；对 $G_l$ 的影响取决于定向滤波器如何响应旋转后的内容。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,\\dots,r_6]$），其中每个 $r_i$ 是按上述顺序列出的第 $i$ 个测试用例的 $\\Delta G_l(\\mathcal{T}_i)$ 的浮点值。$\\Delta G_l(\\mathcal{T})$ 值不需要单位。程序不得读取任何输入，也不得访问外部文件或网络。要使用的角度单位是度，所有裁剪尺寸均以像素为单位。", "solution": "该问题要求探究神经风格迁移 (NST) 中使用的基于格拉姆矩阵的风格表示的不变性。这通过一个合成实验来完成，该实验涉及一个程序生成的图像、一个固定的滤波器组和一组几何增广。风格表示的变化由格拉姆矩阵捕捉，并使用归一化的弗罗贝尼乌斯距离进行量化。\n\n解决方案以结构化的方式进行：首先，我们定义并实现所有必要的组件，包括合成图像、滤波器组以及计算格拉姆矩阵的过程。其次，我们将指定的增广应用于图像，并计算格拉姆矩阵中产生的偏差。\n\n### 1. 合成风格图像的生成\n风格图像 $s(x,y)$ 定义在大小为 $H \\times W$ 的离散网格上，其中 $H=128$ 且 $W=128$。其值由三个正弦波的叠加确定：\n$$\ns(x,y) = \\sin\\!\\left(2\\pi f_1 \\frac{x}{H}\\right) + \\sin\\!\\left(2\\pi f_2 \\frac{y}{W}\\right) + \\sin\\!\\left(2\\pi f_3 \\left(\\frac{x}{H}\\cos\\alpha + \\frac{y}{W}\\sin\\alpha\\right)\\right)\n$$\n给定参数为 $f_1 = 4$，$f_2 = 6$，$f_3 = 5$ 以及 $\\alpha = \\frac{\\pi}{4}$。前两项代表垂直和水平的正弦模式，而第三项代表一个以角度 $\\alpha$ 定向的模式。这种构造提供了一个具有已知频率和方向内容的确定性且结构化的输入。我们通过为 $x \\in \\{0, \\dots, W-1\\}$ 和 $y \\in \\{0, \\dots, H-1\\}$ 创建二维坐标网格，并在每个点上评估该函数来生成此图像。\n\n### 2. 特征提取器和滤波器组\n特征提取器被建模为单个卷积层，后跟一个整流线性单元 (ReLU) 非线性。该层由一个包含 $C=6$ 个滤波器的滤波器组组成，每个滤波器的大小为 $K \\times K$，$K=15$。这些滤波器被设计为对不同类型的局部图像结构敏感。\n\n- **Gabor 滤波器（4个）**：这些是受生物学启发的滤波器，用于模拟视觉皮层中简单细胞的感受野。它们能有效地检测特定方向的边缘和纹理。一个实值二维 Gabor 滤波器核定义为：\n$$\ng(x,y) = \\exp\\left(-\\frac{x'^2 + \\gamma^2 y'^2}{2\\sigma^2}\\right) \\cos\\left(2\\pi \\frac{x'}{\\lambda}\\right)\n$$\n其中 $x' = x \\cos\\theta + y \\sin\\theta$ 且 $y' = -x \\sin\\theta + y \\cos\\theta$。提供的参数为：方向 $\\theta \\in \\{0^\\circ, 45^\\circ, 90^\\circ, 135^\\circ\\}$，波长 $\\lambda=4.0$，标准差 $\\sigma=2.5$，以及纵横比 $\\gamma=0.5$。为确保这些滤波器作为带通滤波器工作而不影响平均亮度，它们的核被归一化为零均值。\n\n- **高斯拉普拉斯 (LoG) 滤波器（1个）**：该滤波器是一种有效的斑点检测器，能找到被不同强度包围的均匀强度区域。它通过将拉普拉斯算子应用于高斯函数来计算：\n$$\n\\text{LoG}(x,y) = -\\frac{1}{\\pi\\sigma^4}\\left(1 - \\frac{x^2+y^2}{2\\sigma^2}\\right)e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n$$\n其中 $\\sigma_{\\text{LoG}} = 2.0$。该核天然是零和的。\n\n- **高斯模糊滤波器（1个）**：这是一个平滑图像的低通滤波器。其核为：\n$$\n\\text{G}(x,y) = \\frac{1}{2\\pi\\sigma^2} e^{-\\frac{x^2+y^2}{2\\sigma^2}}\n$$\n其中 $\\sigma_{\\text{G}} = 1.5$。为了保持图像亮度，该核被归一化为和为 1。\n\n### 3. 特征图和格拉姆矩阵的计算\n对于滤波器组中的每个滤波器 $k_c$，通过以下步骤生成一个特征图 $a_c$：\n1.  **卷积**：$f_c = s \\star k_c$。这是一个二维卷积，使用 `same` 填充以确保输出特征图与输入图像具有相同的空间维度。使用对称边界条件来处理边界效应。\n2.  **ReLU 激活**：$a_c(x,y) = \\max(0, f_c(x,y))$。这引入了非线性，这是深度神经网络的一个关键组成部分。\n\n得到的激活图 $\\{a_c\\}_{c=1}^C$ 形成一个张量 $A \\in \\mathbb{R}^{C \\times H' \\times W'}$，其中 $(H', W')$ 是这些图的空间维度。这些图被扁平化为一个矩阵 $F \\in \\mathbb{R}^{C \\times N}$，其中 $N = H' \\cdot W'$。$F$ 的每一行对应于单个通道的扁平化激活值。\n\n于是，格拉姆矩阵 $G_l \\in \\mathbb{R}^{C \\times C}$ 被计算为此矩阵与其自身的归一化外积：\n$$\nG_l = \\frac{1}{N} F F^\\top\n$$\n格拉姆矩阵的每个元素 $(G_l)_{ij}$ 代表通道 $i$ 和 $j$ 的特征响应之间的相关性，该相关性在所有空间位置上进行了平均。它可作为图像风格的统计摘要。\n\n### 4. 增广和差异计算\n实验的核心是评估当输入图像经受几何增广 $\\mathcal{T}$ 时，格拉姆矩阵如何变化。对于每个增广，都会创建一个增广后的图像 $s_{\\mathcal{T}}$，并计算其对应的格拉姆矩阵 $G_l^{(\\mathcal{T})}$。这些增广包括：\n- **旋转**：图像按指定角度 $\\theta \\in \\{0^\\circ, 90^\\circ, 180^\\circ\\}$ 进行旋转。旋转时保持图像尺寸不变，原始图像边界之外的区域用零填充。\n- **裁剪**：选择图像的一个子数组。中心裁剪从中间取一个图片块，而左上角裁剪从角落取一个图片块。裁剪会改变空间域，从而改变归一化因子 $N = H' \\cdot W'$。\n\n风格表示的变化由归一化的弗罗贝尼乌斯差量化：\n$$\n\\Delta G_l(\\mathcal{T}) = \\frac{\\left\\|G_l^{(\\mathcal{T})} - G_l\\right\\|_F}{\\left\\|G_l\\right\\|_F}\n$$\n其中 $\\|M\\|_F = \\sqrt{\\sum_{i,j} M_{ij}^2}$ 是弗罗贝尼乌斯范数。该度量给出了原始格拉姆矩阵和增广后格拉姆矩阵之间相对差异的标量度量。\n\n### 5. 测试套件的执行\n程序系统地评估指定的六个测试用例。对于每个用例，它应用相应的旋转和/或裁剪以获得 $s_{\\mathcal{T}}$，计算 $G_l^{(\\mathcal{T})}$，然后计算相对于原始未增广图像的格拉姆矩阵 $G_l$ 的 $\\Delta G_l(\\mathcal{T})$。\n\n- 测试用例 1（旋转 $0^\\circ$，不裁剪）作为对照组，因为 $s_{\\mathcal{T}} = s$，所以 $\\Delta G_l$ 必须为 $0$。\n- 旋转 $90^\\circ$ 和 $180^\\circ$ 预期会产生非零差异，因为定向 Gabor 滤波器会对旋转后的模式产生不同的响应。\n- 裁剪预期会产生非零差异，因为特征的统计样本发生了变化，并且合成图像的内容不是空间平稳的。\n\n最终输出是为每个测试用例计算出的 $\\Delta G_l(\\mathcal{T})$ 值的列表，并按要求格式化。", "answer": "```python\nimport numpy as np\nfrom scipy import signal, ndimage\n\ndef create_style_image(H, W, f1, f2, f3, alpha):\n    \"\"\"\n    Constructs the synthetic style image from a mixture of sinusoidal patterns.\n    \"\"\"\n    y_coords, x_coords = np.mgrid[0:H, 0:W]\n    term1 = np.sin(2 * np.pi * f1 * x_coords / W)\n    term2 = np.sin(2 * np.pi * f2 * y_coords / H)\n    term3 = np.sin(2 * np.pi * f3 * (x_coords / W * np.cos(alpha) + y_coords / H * np.sin(alpha)))\n    return term1 + term2 + term3\n\ndef create_gabor_filter(K, sigma, theta_deg, lam, gamma):\n    \"\"\"\n    Creates a real-valued, zero-mean Gabor filter kernel.\n    \"\"\"\n    theta_rad = np.deg2rad(theta_deg)\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    x_theta = x * np.cos(theta_rad) + y * np.sin(theta_rad)\n    y_theta = -x * np.sin(theta_rad) + y * np.cos(theta_rad)\n    \n    gb = np.exp(-(x_theta**2 + gamma**2 * y_theta**2) / (2 * sigma**2)) * np.cos(2 * np.pi * x_theta / lam)\n    \n    # Make the filter zero-mean\n    return gb - np.mean(gb)\n\ndef create_log_filter(K, sigma):\n    \"\"\"\n    Creates a Laplacian-of-Gaussian (LoG) filter kernel.\n    \"\"\"\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    # Unnormalized LoG\n    norm_sq = x**2 + y**2\n    factor1 = (norm_sq - 2 * sigma**2) / (sigma**4)\n    factor2 = np.exp(-norm_sq / (2 * sigma**2))\n    log_kernel = factor1 * factor2\n    \n    # Constant scaling factor is not critical as Gram matrix is correlation-based\n    # and we normalize the final difference. \n    # Let's make it zero-mean for numerical stability.\n    return log_kernel - log_kernel.mean()\n\ndef create_gaussian_filter(K, sigma):\n    \"\"\"\n    Creates a Gaussian blur filter kernel, normalized to sum to 1.\n    \"\"\"\n    center = K // 2\n    y, x = np.mgrid[-center:center+1, -center:center+1]\n    \n    g = np.exp(-(x**2 + y**2) / (2 * sigma**2))\n    return g / np.sum(g)\n\ndef get_gram_matrix(image, filters):\n    \"\"\"\n    Computes the Gram matrix for an image given a bank of filters.\n    \"\"\"\n    activations = []\n    for k in filters:\n        # Convolve using 'same' padding to maintain dimensions\n        conv_result = signal.convolve2d(image, k, mode='same', boundary='symm')\n        # Apply ReLU nonlinearity\n        relu_result = np.maximum(0, conv_result)\n        activations.append(relu_result)\n    \n    A = np.array(activations)\n    C, H_prime, W_prime = A.shape\n    N = H_prime * W_prime\n    \n    if N == 0:\n        return np.zeros((C,C))\n\n    F = A.reshape(C, N)\n    \n    G = (1 / N) * (F @ F.T)\n    return G\n\ndef solve():\n    # Problem parameters\n    H, W = 128, 128\n    F1, F2, F3 = 4, 6, 5\n    ALPHA = np.pi / 4\n    K = 15\n    \n    GABOR_SIGMA, GABOR_GAMMA, GABOR_LAM = 2.5, 0.5, 4.0\n    GABOR_THETAS = [0, 45, 90, 135]\n    LOG_SIGMA = 2.0\n    GAUSS_SIGMA = 1.5\n    \n    # Test suite definition\n    test_cases = [\n        {'rot': 0, 'crop': None, 'crop_size': None},\n        {'rot': 90, 'crop': None, 'crop_size': None},\n        {'rot': 180, 'crop': None, 'crop_size': None},\n        {'rot': 0, 'crop': 'center', 'crop_size': (64, 64)},\n        {'rot': 0, 'crop': 'top_left', 'crop_size': (64, 64)},\n        {'rot': 90, 'crop': 'center', 'crop_size': (64, 64)},\n    ]\n\n    # 1. Generate original style image\n    s_orig = create_style_image(H, W, F1, F2, F3, ALPHA)\n    \n    # 2. Build filter bank\n    filters = []\n    for theta in GABOR_THETAS:\n        filters.append(create_gabor_filter(K, GABOR_SIGMA, theta, GABOR_LAM, GABOR_GAMMA))\n    filters.append(create_log_filter(K, LOG_SIGMA))\n    filters.append(create_gaussian_filter(K, GAUSS_SIGMA))\n    \n    # 3. Compute Gram matrix for the original image\n    G_orig = get_gram_matrix(s_orig, filters)\n    norm_G_orig = np.linalg.norm(G_orig, 'fro')\n    \n    results = []\n    for case in test_cases:\n        s_aug = s_orig.copy()\n        \n        # Apply rotation\n        if case['rot'] != 0:\n            s_aug = ndimage.rotate(s_aug, case['rot'], reshape=False, mode='constant', cval=0.0)\n            \n        # Apply cropping\n        if case['crop'] is not None:\n            h_crop, w_crop = case['crop_size']\n            h_img, w_img = s_aug.shape\n            \n            if case['crop'] == 'center':\n                y_start = (h_img - h_crop) // 2\n                x_start = (w_img - w_crop) // 2\n                s_aug = s_aug[y_start : y_start + h_crop, x_start : x_start + w_crop]\n            elif case['crop'] == 'top_left':\n                s_aug = s_aug[0:h_crop, 0:w_crop]\n\n        # Compute Gram matrix for the augmented image\n        G_aug = get_gram_matrix(s_aug, filters)\n        \n        # Compute normalized Frobenius difference\n        diff_norm = np.linalg.norm(G_aug - G_orig, 'fro')\n        \n        if norm_G_orig == 0:\n            delta_G = 0.0 if diff_norm == 0.0 else np.inf\n        else:\n            delta_G = diff_norm / norm_G_orig\n            \n        results.append(delta_G)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158617"}]}