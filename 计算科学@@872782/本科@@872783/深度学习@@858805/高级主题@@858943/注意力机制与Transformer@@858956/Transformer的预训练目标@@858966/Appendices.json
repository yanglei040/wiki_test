{"hands_on_practices": [{"introduction": "在像 BERT 这样的大型语言模型中，掩码语言建模（Masked Language Modeling, MLM）的实现并非一成不变。为了防止模型仅仅记住特定掩码位置的上下文，掩码在每个训练周期（epoch）中都会被动态地重新生成。这个动手实践将帮助我们从概率论的角度来理解这一动态过程。通过这个练习，我们将推导出在整个训练过程中，一个普通词元预计被掩码并用于模型学习的次数，以及它至少被掩码一次的概率，从而深化对预训练动态性的理解 [@problem_id:3164748]。", "problem": "一个 Transformer 编码器使用掩码语言建模（MLM）进行预训练。在每次数据遍历中，每个词元位置都以概率 $p$ 独立地成为一个预测目标，无论它是否被替换为特殊的掩码符号、随机词元或保持不变。训练语料库包含一组固定的词元位置，每个轮次（epoch）精确地遍历每个词元位置一次。掩码在每个轮次动态地重新采样，因此在一个轮次中对给定词元位置的选择决策独立于其在任何其他轮次中的选择。训练进行 $E$ 个轮次。虽然使用随机梯度下降（SGD），但对于此问题，我们只关注每个词元的贡献：每次一个词元位置被选为预测目标时，它贡献一个单一的损失项，因此在该轮次中为该词元位置贡献一个梯度贡献事件。\n\n对于一个固定的词元位置 $i$，定义随机变量 $S$ 为该位置被选为预测目标的总轮次数。仅使用独立伯努利试验的概率公理和期望的线性性作为基本依据，以闭合形式推导以下用 $E$ 和 $p$ 表示的量：\n- 每个词元位置的梯度贡献事件的期望数，$\\mathbb{E}[S]$。\n- 在 $E$ 个轮次中该词元位置至少被选择一次的覆盖概率，$\\Pr(S \\geq 1)$。\n\n将您的最终答案表示为单个行向量 $\\begin{pmatrix} \\mathbb{E}[S] & \\Pr(S \\geq 1) \\end{pmatrix}$。无需四舍五入，也不涉及物理单位。", "solution": "在尝试解答之前，首先评估问题陈述的有效性。\n\n### 步骤 1：提取已知条件\n- 一个 Transformer 编码器使用掩码语言建模（MLM）进行预训练。\n- 在每次遍历（轮次）中，每个词元位置独立地以概率 $p$ 成为一个预测目标。\n- 在一个轮次中对给定词元位置的选择决策独立于其在任何其他轮次中的选择。\n- 训练的总轮次数为 $E$。\n- 每次一个词元位置被选为预测目标时，它贡献一个单一的梯度贡献事件。\n- 对于一个固定的词元位置 $i$，随机变量 $S$ 被定义为该位置被选为预测目标的总轮次数。\n- 推导必须仅使用独立伯努利试验的概率公理和期望的线性性。\n- 需要推导的量是每个词元位置的梯度贡献事件的期望数 $\\mathbb{E}[S]$，以及该词元位置至少被选择一次的覆盖概率 $\\Pr(S \\geq 1)$。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学基础：** 该问题描述了掩码语言建模（MLM）预训练目标的一个简化但基本正确的概率模型，该目标是像 BERT 这样的现代自然语言处理模型的基石。使用伯努利试验来模拟独立的掩码事件是标准的且在数学上是合理的。\n- **适定性：** 问题定义清晰，包含了所有必要的参数（$E$，$p$），并明确定义了随机变量 $S$ 和目标量（$\\mathbb{E}[S]$，$\\Pr(S \\geq 1)$）。这些条件可以导出一个唯一且有意义的解。\n- **客观性：** 语言形式化、精确，没有任何主观或模棱两可的术语。\n- **缺陷清单：**\n    1.  **科学或事实不准确性：** 无。该设置是真实世界机器学习过程的有效抽象。\n    2.  **不可形式化或不相关：** 该问题完全可以在概率论中形式化，并且与深度学习领域直接相关。\n    3.  **不完整或矛盾的设置：** 问题是自洽且一致的。明确说明了各轮次事件之间的独立性。\n    4.  **不切实际或不可行：** 该场景是用于分析此类算法的标准理论模型。它并非不切实际。\n    5.  **不适定或结构不良：** 问题结构良好，并能导出一个唯一的解。\n    6.  **伪深刻、琐碎或同义反复：** 该问题需要正确应用概率论的基本原理（期望的线性性、独立事件的性质），是一个非平凡的形式推导练习。\n    7.  **超出科学可验证性：** 推导过程在数学上是可验证的。\n\n### 步骤 3：结论与行动\n问题是**有效**且适定的。将进行解答推导。\n\n### 推导\n\n让我们考虑一个单一、固定的词元位置。训练过程进行 $E$ 个轮次。\n\n首先，我们为每个轮次 $j \\in \\{1, 2, \\dots, E\\}$ 定义一组指示随机变量 $X_j$。\n令 $X_j = 1$ 表示在轮次 $j$ 中该词元位置被选为预测目标，否则 $X_j = 0$。\n\n根据问题陈述，在任何给定轮次中的选择都是一个独立事件，概率为 $p$。因此，每个 $X_j$ 都是一个参数为 $p$ 的独立伯努利随机变量。每个 $X_j$ 的概率质量函数为：\n$$ \\Pr(X_j = 1) = p $$\n$$ \\Pr(X_j = 0) = 1 - p $$\n\n随机变量 $S$ 表示该位置被选中的总轮次数。这是所有轮次指示变量的总和：\n$$ S = \\sum_{j=1}^{E} X_j $$\n\n**1. 期望值 $\\mathbb{E}[S]$ 的推导**\n\n我们需要找到梯度贡献事件的期望总数，即 $\\mathbb{E}[S]$。问题要求使用期望的线性性原理。\n\n单个伯努利随机变量 $X_j$ 的期望为：\n$$ \\mathbb{E}[X_j] = 1 \\cdot \\Pr(X_j=1) + 0 \\cdot \\Pr(X_j=0) = 1 \\cdot p + 0 \\cdot (1-p) = p $$\n\n使用期望的线性性，即随机变量之和的期望等于它们各自期望之和，我们有：\n$$ \\mathbb{E}[S] = \\mathbb{E}\\left[\\sum_{j=1}^{E} X_j\\right] = \\sum_{j=1}^{E} \\mathbb{E}[X_j] $$\n\n由于对于每个轮次 $j$，$\\mathbb{E}[X_j] = p$，所以总和变为：\n$$ \\mathbb{E}[S] = \\sum_{j=1}^{E} p = Ep $$\n\n**2. 覆盖概率 $\\Pr(S \\geq 1)$ 的推导**\n\n我们需要找到在 $E$ 个轮次中该词元位置至少被选择一次的概率。这个概率是 $\\Pr(S \\geq 1)$。\n\n使用补集法则来计算这个概率更直接。事件“$S \\geq 1$”（词元至少被选择一次）是事件“$S = 0$”（词元从未被选择）的补集。\n$$ \\Pr(S \\geq 1) = 1 - \\Pr(S = 0) $$\n\n事件“$S=0$”发生当且仅当该词元在 $E$ 个轮次中都未被选择。这意味着 $X_1=0$ 且 $X_2=0$ 且 ... 且 $X_E=0$。\n$$ \\Pr(S=0) = \\Pr(X_1=0, X_2=0, \\dots, X_E=0) $$\n\n问题陈述指出，各轮次的选择决策是独立的。因此，联合事件的概率是各个独立事件概率的乘积：\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} \\Pr(X_j=0) $$\n\n对于每个轮次 $j$，词元未被选择的概率是 $\\Pr(X_j=0) = 1-p$。\n将此代入乘积中，我们得到：\n$$ \\Pr(S=0) = \\prod_{j=1}^{E} (1-p) = (1-p)^E $$\n\n最后，将此结果代回补集法则方程，得到覆盖概率：\n$$ \\Pr(S \\geq 1) = 1 - (1-p)^E $$\n\n推导出的量为 $\\mathbb{E}[S] = Ep$ 和 $\\Pr(S \\geq 1) = 1 - (1-p)^E$。这些量按照要求以 $E$ 和 $p$ 的闭合形式表示。", "answer": "$$\\boxed{\\begin{pmatrix} Ep & 1 - (1-p)^{E} \\end{pmatrix}}$$", "id": "3164748"}, {"introduction": "当一个词元被掩码后，模型会尝试预测它，并根据预测的准确性计算出一个损失值。这个损失信号随后通过反向传播算法，用于更新模型的每一个参数，从而实现学习。这个练习将带我们深入探索学习过程的核心机制。我们将亲手推导 MLM 损失函数相对于输入层掩码嵌入（mask embeddings）的梯度，这不仅能揭示梯度是如何从输出层流回输入层的，还能帮助我们理解不同参数共享策略（例如，全局共享掩码与跨度级独立掩码）对学习过程的细微影响 [@problem_id:3164800]。", "problem": "考虑一个使用掩码语言建模（MLM）训练的基于 Transformer 的序列模型，其中掩码语言建模（MLM）定义为最小化被掩码位置上真实词元的负对数似然。设词汇表大小为 $V$，嵌入维度为 $d$，输出分类层是一个线性映射 $U \\in \\mathbb{R}^{V \\times d}$，后接一个 softmax 函数。对于一个给定的训练样本，假设有 $K$ 个不相交的掩码片段，由 $k \\in \\{1,\\dots,K\\}$ 索引，片段 $k$ 中的掩码位置集合表示为 $S^{(k)}$。\n\n对于每个掩码位置 $i \\in S^{(k)}$，Transformer 生成一个隐藏状态 $h_{i} \\in \\mathbb{R}^{d}$，该状态被送入输出头以生成 logits $z_{i} = U h_{i} \\in \\mathbb{R}^{V}$ 和概率 $p_{i} = \\mathrm{softmax}(z_{i}) \\in \\mathbb{R}^{V}$。设位置 $i$ 的真实标签索引为 $y_{i} \\in \\{1,\\dots,V\\}$，并令 $e_{y_{i}} \\in \\mathbb{R}^{V}$ 为 $y_{i}$ 的独热向量。定义每个样本的 MLM 损失为\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k=1}^{K} \\sum_{i \\in S^{(k)}} \\left( - \\ln p_{i}[y_{i}] \\right).\n$$\n\n假设 Transformer 在掩码输入周围具有以下一阶局部线性化：\n- 在共享掩码词元设置中，使用一个单一的全局参数向量 $m \\in \\mathbb{R}^{d}$作为每个片段中每个掩码词元的输入嵌入，并且隐藏状态满足\n$$\nh_{i} = A_{i} m + b_{i},\n$$\n对于每个掩码位置 $i$，其中 $A_{i} \\in \\mathbb{R}^{d \\times d}$ 和 $b_{i} \\in \\mathbb{R}^{d}$ 不依赖于 $m$。\n\n- 在学习的片段级掩码嵌入设置中，每个片段 $k$ 都有其自己的参数向量 $s^{(k)} \\in \\mathbb{R}^{d}$，用作该片段中每个掩码词元的输入嵌入，并且隐藏状态满足\n$$\nh_{i} = A_{i} s^{(k)} + b_{i},\n$$\n对于每个 $i \\in S^{(k)}$，其中 $A_i$ 和 $b_i$ 与上述相同，不依赖于 $s^{(k)}$。\n\n从 softmax 函数和交叉熵的核心定义出发，并仅使用标准微积分（包括链式法则），推导 $\\mathcal{L}_{\\mathrm{MLM}}$ 关于以下参数的梯度闭式解析表达式：\n- 共享掩码词元 $m$；\n- 对于任意固定的片段索引 $k$ 的学习的片段级掩码嵌入 $s^{(k)}$。\n\n将你的最终答案表示为一个单行矩阵，其第一个元素是 $\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$，第二个元素是 $\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$。不需要数值近似或四舍五入，也不涉及物理单位。你的推导过程必须是自洽的，并且不得引用超出给定定义之外的任何快捷公式。", "solution": "问题要求推导掩码语言建模（MLM）损失 $\\mathcal{L}_{\\mathrm{MLM}}$ 相对于两组参数的梯度：一组是共享的掩码词元嵌入 $m$，另一组是片段级的掩码嵌入 $s^{(k)}$。推导将基于第一性原理，使用微积分的链式法则。\n\nMLM 损失定义为所有掩码位置上负对数似然的总和：\n$$\n\\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i\n$$\n其中 $\\mathcal{L}_i = - \\ln p_{i}[y_{i}]$ 是位置 $i$ 处单个词元的损失。索引 $k'$ 用作片段的哑求和变量，以区别于问题第二部分中的固定索引 $k$。\n\n单个位置 $i$ 的计算序列如下：\n1. 输入嵌入（$m$ 或 $s^{(k)}$）用于计算隐藏状态 $h_i$。\n2. 隐藏状态 $h_i \\in \\mathbb{R}^d$ 用于通过 $z_i = U h_i$ 计算 logits $z_i \\in \\mathbb{R}^V$。\n3. logits $z_i$ 通过 $p_i = \\mathrm{softmax}(z_i)$ 转换为概率 $p_i \\in \\mathbb{R}^V$。\n4. 损失 $\\mathcal{L}_i$ 根据 $p_i$ 和真实标签索引 $y_i$ 计算得出。\n\n我们将通过这个计算图反向应用链式法则来计算梯度。\n\n**步骤 1：$\\mathcal{L}_i$ 相对于 logits $z_i$ 的梯度**\n\n位置 $i$ 的损失是 $\\mathcal{L}_i = -\\ln p_i[y_i]$。对于任何类别索引 $j \\in \\{1, \\dots, V\\}$，其概率 $p_i[j]$ 由 softmax 函数给出：\n$$\np_i[j] = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])}\n$$\n其中 $z_i[j]$ 是 logit 向量 $z_i$ 的第 $j$ 个分量。\n\n将其代入真实类别 $y_i$ 的损失函数中：\n$$\n\\mathcal{L}_i = - \\ln \\left( \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\right) = -z_i[y_i] + \\ln\\left(\\sum_{l=1}^{V} \\exp(z_i[l])\\right)\n$$\n我们现在计算 $\\mathcal{L}_i$ 相对于 logit 向量 $z_i$ 的任意分量 $z_i[j]$ 的偏导数。\n\n如果 $j = y_i$：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[y_i]} = -1 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[y_i]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = -1 + \\frac{\\exp(z_i[y_i])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[y_i] - 1\n$$\n如果 $j \\neq y_i$：\n$$\n\\frac{\\partial \\mathcal{L}_i}{\\partial z_i[j]} = 0 + \\frac{1}{\\sum_{l=1}^{V} \\exp(z_i[l])} \\cdot \\frac{\\partial}{\\partial z_i[j]} \\left( \\sum_{l=1}^{V} \\exp(z_i[l]) \\right) = \\frac{\\exp(z_i[j])}{\\sum_{l=1}^{V} \\exp(z_i[l])} = p_i[j]\n$$\n这两种情况可以合并成一个单一的向量表达式。设 $e_{y_i}$ 是独热向量，其中对应于索引 $y_i$ 的分量为 1，所有其他分量为 0。标量 $\\mathcal{L}_i$ 相对于向量 $z_i$ 的梯度是：\n$$\n\\nabla_{z_i} \\mathcal{L}_i = p_i - e_{y_i}\n$$\n这个向量 $\\nabla_{z_i} \\mathcal{L}_i \\in \\mathbb{R}^V$ 代表了 logit 级别的误差信号。\n\n**步骤 2：$\\mathcal{L}_i$ 相对于隐藏状态 $h_i$ 的梯度**\n\nLogits 是隐藏状态的线性函数：$z_i = U h_i$，其中 $U \\in \\mathbb{R}^{V \\times d}$ 且 $h_i \\in \\mathbb{R}^d$。我们使用向量值函数的链式法则。标量 $\\mathcal{L}_i$ 相对于向量 $h_i$ 的梯度是：\n$$\n\\nabla_{h_i} \\mathcal{L}_i = \\left(\\frac{\\partial z_i}{\\partial h_i}\\right)^T (\\nabla_{z_i} \\mathcal{L}_i)\n$$\n项 $\\frac{\\partial z_i}{\\partial h_i}$ 是函数 $z_i(h_i)$ 的雅可比矩阵。由于 $z_i = U h_i$，这个雅可比矩阵就是矩阵 $U$。因此：\n$$\n\\nabla_{h_i} \\mathcal{L}_i = U^T (\\nabla_{z_i} \\mathcal{L}_i) = U^T (p_i - e_{y_i})\n$$\n这个梯度，我们表示为 $g_i = \\nabla_{h_i} \\mathcal{L}_i$，是一个在 $\\mathbb{R}^d$ 中的向量。\n\n**步骤 3：$\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}}$ 的推导**\n\n在共享掩码词元设置中，每个掩码位置 $i$ 的隐藏状态 $h_i$ 是单个共享参数向量 $m \\in \\mathbb{R}^d$ 的线性函数：\n$$\nh_i = A_i m + b_i\n$$\n总损失 $\\mathcal{L}_{\\mathrm{MLM}}$ 是 $m$ 的函数，通过其对每个 $h_i$ 的依赖关系实现。根据梯度算子的线性和链式法则：\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{m} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i(h_i(m)) \\right) = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\nabla_{m} \\mathcal{L}_i\n$$\n对于每一项 $\\mathcal{L}_i$，我们再次应用链式法则：\n$$\n\\nabla_{m} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial m}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i)\n$$\n函数 $h_i(m)$ 的雅可比矩阵是矩阵 $A_i \\in \\mathbb{R}^{d \\times d}$。代入此式以及 $\\nabla_{h_i} \\mathcal{L}_i = g_i$ 的表达式：\n$$\n\\nabla_{m} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\n对所有掩码位置求和，得到关于 $m$ 的最终梯度：\n$$\n\\nabla_{m} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i})\n$$\n\n**步骤 4：$\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}}$ 的推导**\n\n在片段级掩码嵌入设置中，对于一个固定的片段索引 $k$，参数向量 $s^{(k)} \\in \\mathbb{R}^d$ 只影响那些 $i \\in S^{(k)}$ 的隐藏状态 $h_i$。对于这些位置，关系如下：\n$$\nh_i = A_i s^{(k)} + b_i \\quad \\text{for } i \\in S^{(k)}\n$$\n对于不同片段中的任何位置 $j$，$j \\in S^{(k')}$ 且 $k' \\neq k$，隐藏状态 $h_j$ 依赖于 $s^{(k')}$ 而不是 $s^{(k)}$。因此，对于 $j \\notin S^{(k)}$，有 $\\nabla_{s^{(k)}} \\mathcal{L}_j = 0$。\n\n总损失关于 $s^{(k)}$ 的梯度是：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\nabla_{s^{(k)}} \\left( \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} \\mathcal{L}_i \\right) = \\sum_{i \\in S^{(k)}} \\nabla_{s^{(k)}} \\mathcal{L}_i\n$$\n求和仅针对片段 $k$ 内的位置。对这个和中的每一项，我们应用链式法则：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = \\left(\\frac{\\partial h_i}{\\partial s^{(k)}}\\right)^T (\\nabla_{h_i} \\mathcal{L}_i) \\quad \\text{for } i \\in S^{(k)}\n$$\n雅可比矩阵 $\\frac{\\partial h_i}{\\partial s^{(k)}}$ 是矩阵 $A_i$。代入此式以及 $g_i = \\nabla_{h_i} \\mathcal{L}_i$ 的表达式：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_i = A_i^T g_i = A_i^T U^T (p_i - e_{y_i})\n$$\n对片段 $k$ 内的所有位置求和，得到关于 $s^{(k)}$ 的最终梯度：\n$$\n\\nabla_{s^{(k)}} \\mathcal{L}_{\\mathrm{MLM}} = \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i})\n$$\n结果即为所求梯度的闭式解析表达式。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{k'=1}^{K} \\sum_{i \\in S^{(k')}} A_i^T U^T (p_i - e_{y_i}) & \\sum_{i \\in S^{(k)}} A_i^T U^T (p_i - e_{y_i}) \\end{pmatrix}}\n$$", "id": "3164800"}, {"introduction": "我们已经了解了 MLM 的工作机制，但能否设计出更高效的掩码策略呢？这个实践从信息论的视角为我们提供了一个解答思路。通过分析一个假设情景——为不同词性的词元使用专门的掩码标记——我们可以精确地量化这种策略能为模型提供多少额外信息。这个练习将引导我们推导出，提供更丰富的掩码信息能够直接降低模型的预测不确定性（即最优负对数似然），其降低的幅度恰好等于所提供信息的熵，从而让我们从根本上理解预训练目标设计的原则 [@problem_id:3164769]。", "problem": "考虑一个使用掩码语言模型（MLM）目标训练的 Transformer。令上下文为随机变量 $C$，被掩码的词为随机变量 $W$，词性为随机变量 $P$，其取值于有限集合 $\\{p_{1}, p_{2}, \\dots, p_{k}\\}$。MLM 训练旨在最小化期望负对数似然（NLL），在一个具有足够容量和数据的最优模型下，该值等于真实数据生成分布与模型分布之间的交叉熵。假设在掩码位置处的数据生成过程如下：\n- 给定 $C$，首先根据 $p(P \\mid C)$ 采样 $P$。\n- 给定 $P$，从大小为 $V_{P}$ 的不相交词汇子集 $\\mathcal{V}_{P}$ 中均匀采样 $W$。\n\n考虑两种掩码方案：\n1. 单一通用掩码嵌入 $m$，除了可从 $C$ 推断出的信息外，不传达关于 $P$ 的任何显式信息。\n2. 每种词性对应一个专门的掩码嵌入 $m_{p}$，它确定性地向模型揭示掩码位置的真实 $P$。\n\n仅从交叉熵和条件熵的核心定义出发，推导在固定上下文 $C$ 的情况下，通用掩码方案与专门掩码方案之间最优期望 NLL 的差值的封闭形式表达式。然后，对于一个特定的上下文，其中词性分布为 $p(P=\\text{名词} \\mid C)=0.6$，$p(P=\\text{动词} \\mid C)=0.3$ 和 $p(P=\\text{形容词} \\mid C)=0.1$，计算专门掩码相对于通用掩码所实现的最优期望 NLL 的降低数值。使用自然对数，并以“奈特”（nats）为单位表示最终的数值降低量。将您的答案四舍五入到四位有效数字。", "solution": "所提供的问题经过了严格的验证。\n\n**步骤 1：提取已知条件**\n- 令 $C$ 为上下文的随机变量。\n- 令 $W$ 为被掩码的词的随机变量。\n- 令 $P$ 为词性的随机变量，其状态空间为有限集合 $\\{p_{1}, p_{2}, \\dots, p_{k}\\}$。\n- 训练目标是最小化期望负对数似然（NLL）。\n- 对于最优模型，期望 NLL 等于交叉熵，它简化为真实数据生成分布的熵。\n- 数据生成过程建模如下：\n    1. 给定上下文 $C$，根据条件概率分布 $p(P \\mid C)$ 采样词性 $P$。\n    2. 给定词性 $P$，从大小为 $V_{P}$ 的不相交词汇子集 $\\mathcal{V}_{P}$ 中均匀采样词 $W$。这意味着马尔可夫链 $C \\rightarrow P \\rightarrow W$。\n- 掩码方案 1（通用）：使用单一掩码嵌入 $m$。模型输入为上下文 $C$。\n- 掩码方案 2（专门）：对每种词性 $p$ 使用特定的掩码嵌入 $m_p$，确定性地揭示被掩码词的真实词性 $P$。模型输入为上下文 $C$ 和词性 $P$。\n- 任务是推导在固定上下文 $C$ 下两种方案之间最优期望 NLL 的差值，然后计算一个特定情况下的数值。\n- 对于数值计算，具体的条件分布为 $p(P=\\text{名词} \\mid C)=0.6$，$p(P=\\text{动词} \\mid C)=0.3$ 和 $p(P=\\text{形容词} \\mid C)=0.1$。\n- 使用自然对数（底为 $e$），结果以奈特（nats）为单位。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题在科学上基于信息论及其在深度学习中的应用，特别是基于 Transformer 的语言模型。该问题是适定 (well-posed) 的，提供了一个清晰的数据生成模型和足够的约束，可以得出一个唯一的、有意义的解。术语是精确和客观的。该问题不违反任何无效性标准。对于数值计算部分，没有提供词汇子集大小 $V_P$ 并非一个缺陷，因为一个完整的推导表明它们会在 NLL *差值* 的最终表达式中被抵消掉。\n\n**步骤 3：结论与行动**\n该问题被判定为**有效**。将提供一个完整的解答。\n\n**封闭形式表达式的推导**\n\n一个预测模型的最优期望负对数似然（NLL）是目标变量的熵，以模型可用的信息为条件。我们分析固定上下文 $C$ 下的 NLL。\n\n令 $NLL_{generic}$ 为通用掩码方案下的最优期望 NLL。在这种情况下，模型必须仅在给定上下文 $C$ 的条件下预测词 $W$。因此，NLL 是 $W$ 在给定 $C$ 下的条件熵：\n$$NLL_{generic} = H(W \\mid C)$$\n\n令 $NLL_{specialized}$ 为专门掩码方案下的最优期望 NLL。在这里，模型被给予了上下文 $C$ 和被掩码词的真实词性 $P$。任务仍然是预测 $W$。因此，NLL 是 $W$ 在给定 $C$ 和 $P$ 下的条件熵：\n$$NLL_{specialized} = H(W \\mid C, P)$$\n\n问题要求计算这些量的差值，我们记为 $\\Delta NLL$：\n$$\\Delta NLL = NLL_{generic} - NLL_{specialized} = H(W \\mid C) - H(W \\mid C, P)$$\n根据熵的链式法则，$H(X, Y | Z) = H(X | Y, Z) + H(Y | Z)$。将此应用于我们的变量，我们得到：\n$$H(W, P \\mid C) = H(W \\mid P, C) + H(P \\mid C)$$\n重新整理这个方程得到：\n$$H(W \\mid C) - H(W \\mid C, P) = H(P \\mid C) - H(P \\mid W, C)$$\n这个表达式，$I(P; W \\mid C)$，是条件互信息。然而，基于问题前提的更直接的推导更具启发性。\n\n我们首先计算 $NLL_{specialized} = H(W \\mid C, P)$。根据条件熵的定义：\n$$H(W \\mid C, P) = \\sum_{p \\in P} p(p \\mid C) H(W \\mid C, P=p)$$\n问题陈述了一个马尔可夫链 $C \\rightarrow P \\rightarrow W$，这意味着在给定 $P$ 的条件下，$W$ 与 $C$ 条件独立。因此，$p(W \\mid C, P=p) = p(W \\mid P=p)$，从而 $H(W \\mid C, P=p) = H(W \\mid P=p)$。\n问题还规定，给定 $P=p$，词 $W$ 从大小为 $V_p$ 的词汇子集 $\\mathcal{V}_p$ 上的均匀分布中抽取。一个在 $V_p$ 个结果上的均匀分布的熵是 $\\ln(V_p)$。因此，$H(W \\mid P=p) = \\ln(V_p)$。\n将其代回，我们得到：\n$$NLL_{specialized} = \\sum_{p} p(p \\mid C) \\ln(V_p)$$\n\n现在，我们计算 $NLL_{generic} = H(W \\mid C)$。根据定义：\n$$H(W \\mid C) = - \\sum_{w \\in \\mathcal{V}} p(w \\mid C) \\ln p(w \\mid C)$$\n为了计算这个值，我们首先需要分布 $p(w \\mid C)$。使用全概率定律和马尔可夫假设：\n$$p(w \\mid C) = \\sum_{p} p(w, p \\mid C) = \\sum_{p} p(p \\mid C) p(w \\mid p, C) = \\sum_{p} p(p \\mid C) p(w \\mid p)$$\n问题陈述词汇子集 $\\mathcal{V}_p$ 是不相交的。这意味着对于任何词 $w$，它最多只能属于一个子集，我们可以表示为 $\\mathcal{V}_{p_w}$。对于这个特定的 $p_w$，$p(w \\mid p_w) = 1/V_{p_w}$，而对于任何其他 $p \\neq p_w$，$p(w \\mid p) = 0$。因此，对 $p$ 的求和坍缩为一项：\n$$p(w \\mid C) = p(p_w \\mid C) \\frac{1}{V_{p_w}} \\quad \\text{for } w \\in \\mathcal{V}_{p_w}$$\n我们现在可以将此代入 $H(W \\mid C)$ 的熵公式中。我们将对所有词 $w$ 的求和分组为对不相交子集 $\\mathcal{V}_p$ 的求和：\n$$H(W \\mid C) = - \\sum_{p} \\sum_{w \\in \\mathcal{V}_p} p(w \\mid C) \\ln p(w \\mid C)$$\n对于任何 $w \\in \\mathcal{V}_p$，项 $p(w \\mid C)$ 是常数，等于 $\\frac{p(p \\mid C)}{V_p}$。内层求和由 $V_p$ 个相同的项组成：\n$$H(W \\mid C) = - \\sum_{p} V_p \\left( \\frac{p(p \\mid C)}{V_p} \\ln\\left(\\frac{p(p \\mid C)}{V_p}\\right) \\right)$$\n$$H(W \\mid C) = - \\sum_{p} p(p \\mid C) \\left( \\ln(p(p \\mid C)) - \\ln(V_p) \\right)$$\n$$H(W \\mid C) = - \\sum_{p} p(p \\mid C) \\ln(p(p \\mid C)) + \\sum_{p} p(p \\mid C) \\ln(V_p)$$\n第一项是给定 $C$ 的 $P$ 的条件熵的定义，记作 $H(P \\mid C)$。第二项正是我们之前推导出的 $NLL_{specialized}$ 的表达式。\n$$H(W \\mid C) = H(P \\mid C) + NLL_{specialized}$$\n因此，最优 NLL 的差值为：\n$$\\Delta NLL = NLL_{generic} - NLL_{specialized} = H(W \\mid C) - NLL_{specialized} = H(P \\mid C)$$\nNLL 降低量的封闭形式表达式就是给定上下文的词性分布的条件熵 $H(P \\mid C)$。这个结果很直观：专门掩码的优势在于它消除了关于词性的所有不确定性，而这种不确定性的减少量恰好由 $H(P \\mid C)$ 来量化。\n\n**数值计算**\n\n我们被要求计算在给定词性分布的特定上下文 $C$ 下 NLL 降低的值。该降低量为 $\\Delta NLL = H(P \\mid C)$。\n该熵的公式为：\n$$H(P \\mid C) = - \\sum_{i} p(P=p_i \\mid C) \\ln (p(P=p_i \\mid C))$$\n给定的概率是：\n$p(P=\\text{名词} \\mid C) = 0.6$\n$p(P=\\text{动词} \\mid C) = 0.3$\n$p(P=\\text{形容词} \\mid C) = 0.1$\n\n将这些值代入熵公式：\n$$H(P \\mid C) = - \\left( 0.6 \\ln(0.6) + 0.3 \\ln(0.3) + 0.1 \\ln(0.1) \\right)$$\n我们分别计算各项：\n$0.6 \\ln(0.6) \\approx 0.6 \\times (-0.5108256) \\approx -0.30649536$\n$0.3 \\ln(0.3) \\approx 0.3 \\times (-1.2039728) \\approx -0.36119184$\n$0.1 \\ln(0.1) \\approx 0.1 \\times (-2.3025851) \\approx -0.23025851$\n\n将这些项相加：\n$$-0.30649536 - 0.36119184 - 0.23025851 = -0.89794571$$\n熵是这个总和的负数：\n$$H(P \\mid C) = -(-0.89794571) = 0.89794571 \\text{ 奈特}$$\n四舍五入到四位有效数字，我们得到 $0.8979$。", "answer": "$$\\boxed{0.8979}$$", "id": "3164769"}]}