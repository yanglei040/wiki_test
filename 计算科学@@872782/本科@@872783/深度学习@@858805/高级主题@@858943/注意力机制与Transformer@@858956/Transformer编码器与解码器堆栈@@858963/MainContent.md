## 引言
Transformer 架构已成为现代人工智能领域的基石，尤其在处理语言、语音和时间序列等序列数据方面展现出无与伦比的能力。其强大的性能源于其独特的[编码器-解码器堆栈](@entry_id:637728)结构。然而，许多学习者对 Transformer 的理解往往停留在宏观层面，对其内部组件如何协同工作、为何采用特定设计以及这些设计选择背后的深刻原理缺乏系统性的认识。本文旨在填补这一知识鸿沟，带领读者从第一性原理出发，深入探索 Transformer 的核心。

在接下来的内容中，我们将踏上一段从理论到实践的[深度学习](@entry_id:142022)之旅。首先，在“**原理与机制**”一章中，我们将逐一解构编码器和解码器堆栈，剖析[自注意力](@entry_id:635960)、多头机制、[残差连接](@entry_id:637548)等核心模块的工作原理，并揭示它们如何共同赋予模型强大的序列处理能力。接着，在“**应用与跨学科连接**”一章中，我们将视野扩展到自然语言处理之外，探讨这些核心原理如何应用于[计算机视觉](@entry_id:138301)、语音识别、图论乃至理论物理等不同领域，展示其惊人的通用性。最后，通过“**动手实践**”环节，您将有机会通过具体的编程练习，将理论知识转化为解决实际问题的能力，从而真正内化对 Transformer 编码器与解码器堆栈的理解。

## 原理与机制

继前一章对 Transformer 架构的宏观介绍之后，本章将深入剖析其内部工作原理与核心机制。我们将逐一拆解构成编码器和解码器堆栈的关键组件，阐明它们的设计哲学，并从第一性原理出发，探讨这些机制如何赋予 Transformer 模型强大的序列处理能力。本章旨在为您构建一个关于 Transformer 运行机理的严谨、系统且深入的知识框架。

### 解构 Transformer：核心构建模块

Transformer 的强大能力源于其精心设计的模块化结构。每个模块都扮演着独特的角色，并通过巧妙的组合，共同实现了对[序列数据](@entry_id:636380)的深度理解和生成。

#### 注意力机制：核心引擎

[自注意力](@entry_id:635960)（Self-Attention）机制是 Transformer 的灵魂。它允许模型在处理序列中的每个元素时，动态地权衡序列中所有其他元素的重要性。

**掩码的力量：控制信息流**

在实践中，注意力并非总是“全局”的；我们需要通过**掩码（masking）**来精确控制信息流。例如，在处理一批长度不同的句子时，需要忽略“填充”（padding）符号的影响；在解码器中生成文本时，必须阻止当前位置“看到”未来的信息。

Transformer 通过在 softmax 操作之前，向注意力对数（logits）添加一个掩码矩阵 $M$ 来实现这一点。假设原始的注意力对数是 $s_i$，那么应用掩码后的有效对数是 $z_i = s_i + M_i$。为了禁止对位置 $j$ 的注意力，我们只需设置 $M_j = -\infty$。从 softmax 的定义 $p_j = \exp(z_j) / \sum_k \exp(z_k)$ 出发，当 $M_j \to -\infty$ 时，$z_j \to -\infty$，于是 $\exp(z_j) \to 0$，从而使得该位置的注意力权重 $p_j$ 趋近于零。

在有限精度的计算机中，我们用一个非常大的负数（例如 $-10^9$）来近似 $-\infty$。考虑一个具体的例子：假设一组原始对数为 $s = [2.3, 1.1, -0.4, 3.7, 0.0, 2.0]$，我们只允许关注索引为 $\{1, 4, 6\}$ 的位置。通过为禁用的位置 $\{2, 3, 5\}$ 加上一个大的负常数 $-C$，我们可以将它们的概率质量压缩到任意小的范围。例如，可以精确计算出，要使所有禁用位置的总概率质量不超过 $10^{-7}$，所需的 $C$ 值约为 $13.60$ [@problem_id:3195557]。这种加性掩码机制是实现**填充掩码（padding mask）**和**因果掩码（causal mask）**的通用且高效的方法。

**注意力作为图：拓扑视角下的信息混合**

我们可以将注意力机制理解为在序列的词元（tokens）之间构建一个动态的、加权的、有向的图。每个词元是一个节点，注意力权重则代表了节点之间的连接强度。而注意力掩码，则相当于在这个图上施加了一个固定的拓扑结构，预先定义了哪些连接是允许的。

我们可以通过[图论](@entry_id:140799)的工具来分析不同掩码策略下的信息流动特性。将词元视为图的顶点，如果两个词元之间（在任一方向上）存在合法的注意力连接，我们就在它们之间定义一条无向边。这样得到的[无向图](@entry_id:270905)反映了单层注意力网络中的有效信息混合拓扑。该图的**[图拉普拉斯矩阵](@entry_id:275190)（Graph Laplacian）** $L = D - A$（其中 $A$ 是邻接矩阵，$D$ 是度矩阵）的谱性质，特别是其**[代数连通度](@entry_id:152762)**（即第二小[特征值](@entry_id:154894) $\lambda_2(L)$），可以量化图的连通性，从而衡量信息在整个序列中传递的效率。

考虑一个包含 $n=6$ 个词元的序列，我们可以分析三种典型的掩码模式 [@problem_id:3195504]：
1.  **双向编码器掩码（Bidirectional Encoder Mask）**：任何词元都可以关注任何其他词元。这对应于一个**完全图（complete graph）$K_6$**，其[代数连通度](@entry_id:152762) $\lambda_2(L_{\text{enc}}) = n = 6$。这表示信息可以在一步之内在所有词元间无阻碍地流动，实现了最大化的信息混合。
2.  **因果解码器掩码（Causal Decoder Mask）**：词元 $i$ 只能关注位置 $j \le i$ 的词元。虽然底层的注意力是单向的，但根据上述对称化规则（只要一个方向存在连接就视为有边），其等效的[无向图](@entry_id:270905)同样是**完全图 $K_6$**，因此 $\lambda_2(L_{\text{dec}}) = 6$。这揭示了一个深刻的观点：尽管有因果限制，从信息混合的拓扑角度看，解码器在已生成部分仍然保持了极高的连通性。
3.  **块状掩码（Block Mask）**：假设词元被分为两个独立的块 $\{1,2,3\}$ 和 $\{4,5,6\}$，注意力只在块内发生。这将产生一个**[不连通图](@entry_id:275570)**，由两个独立的 $K_3$ [子图](@entry_id:273342)构成。该图的[代数连通度](@entry_id:152762) $\lambda_2(L_{\text{blk}}) = 0$，表明信息被完全限制在各自的块内，块与块之间存在绝对的[信息瓶颈](@entry_id:263638)。

通过比较这些值，例如计算 $S = \lambda_{2}(L_{\text{enc}}) + \lambda_{2}(L_{\text{dec}}) - \lambda_{2}(L_{\text{blk}}) = 6 + 6 - 0 = 12$，我们可以清晰地看到不同掩码策略如何塑造出截然不同的信息流拓扑，从而服务于不同的架构目标（如编码器的深度上下文理解、解码器的自回归生成、稀疏注意力的效率提升）。

#### [多头注意力](@entry_id:634192)：并行信息处理

[多头注意力](@entry_id:634192)（Multi-Head Attention, MHA）并非简单地重复执行单次注意力操作，而是将模型的表示空间 ($d_{\text{model}}$) 分割成多个[子空间](@entry_id:150286)，让每个“头”在各自的[子空间](@entry_id:150286)中独立地学习和关注信息。

具体来说，输入 $X \in \mathbb{R}^{n \times d}$ 会通过 $h$ 组不同的、可学习的线性[投影矩阵](@entry_id:154479) $W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}$ ($i=1, \dots, h$)，被映射到 $h$ 个不同的查询（Query）、键（Key）和值（Value）三元组。每个头在低维表示上执行[缩放点积注意力](@entry_id:636814)，然后将所有头的输出拼接起来，再通过一个最终的线性变换 $W_O$ 融合信息。

这种设计的核心优势在于，它允许模型**并行地从不同的表示[子空间](@entry_id:150286)中提取信息**。我们可以将每个[注意力头](@entry_id:637186) $i$ 视为一个作用于其特定**值[子空间](@entry_id:150286)（value subspace）** $\mathcal{S}_i = \text{col}(W_V^{(i)})$ 上的内容自适应线性滤波器。$\text{col}(W_V^{(i)})$ 表示由值[投影矩阵](@entry_id:154479) $W_V^{(i)} \in \mathbb{R}^{d \times d_v}$ 的列向量张成的[子空间](@entry_id:150286)。每个头可以学会关注不同类型的信息，例如一个头可能关注句法关系，另一个头关注长距离依赖，还有一个头关注局部搭配。

为了使这些头尽可能地关注不同的信息，一个理想的情况是它们的值[子空间](@entry_id:150286)是**正交（orthogonal）**的。如果每个头的[投影矩阵](@entry_id:154479) $W_V^{(i)}$ 的列向量都是标准正交的（即 $(W_V^{(i)})^\top W_V^{(i)} = I_{d_v}$），那么要使任意两个头 $i$ 和 $j$ 的值[子空间](@entry_id:150286) $\mathcal{S}_i$ 和 $\mathcal{S}_j$ 相互正交，其充要条件是 $(W_V^{(i)})^\top W_V^{(j)} = \mathbf{0}$。综合起来，所有 $h$ 个头的值[子空间](@entry_id:150286)两两正交的条件可以表示为：
$$
(W_{V}^{(i)})^{\top} W_{V}^{(j)} = \delta_{ij} I_{d_v}
$$
其中 $\delta_{ij}$ 是克罗内克 δ 函数。

由于所有这些正交[子空间](@entry_id:150286)都存在于同一个 $d$ 维的[嵌入空间](@entry_id:637157)中，它们的维度之和不能超过 $d$。即 $h \cdot d_v \le d$。由此，我们可以推导出在[嵌入维度](@entry_id:268956) $d$ 和头的值维度 $d_v$ 给定的情况下，理论上最多可以容纳的成对正交头的数量为：
$$
h_{\max} = \left\lfloor \frac{d}{d_v} \right\rfloor
$$
这个结果从根本上解释了多头机制的内在逻辑：通过在正交[子空间](@entry_id:150286)中[并行处理](@entry_id:753134)信息，模型能够以一种分而治之的方式捕捉输入序列中丰富多样的[特征和](@entry_id:189446)关系 [@problem_id:3195523]。

#### 逐位置前馈网络

在每个[自注意力](@entry_id:635960)子层之后，编码器和解码器都包含一个**逐位置前馈网络（Position-wise Feed-Forward Network, FFN）**。这个模块结构非常简单，通常由两个[线性变换](@entry_id:149133)和一个[非线性激活函数](@entry_id:635291)（如 ReLU）组成：
$$
\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
$$
重要的是，这个网络是“逐位置”应用的，意味着它独立地作用于序列中的每一个词元表示，并且在所有位置上共享同一组参数（$W_1, b_1, W_2, b_2$）。FFN 的主要作用是为模型增加[非线性](@entry_id:637147)度和建模能力，允许模型对[自注意力](@entry_id:635960)层聚合来的信息进行更复杂的变换和提炼。

### 组装堆栈：编码器与解码器架构

将上述构建模块组合起来，我们便得到了 Transformer 的两大核心——编码器堆栈和解码器堆栈。

#### 编码器堆栈

编码器由 $L$ 个相同的层堆叠而成，每层包含一个多头[自注意力](@entry_id:635960)模块和一个前馈网络模块。

**基本属性：[置换](@entry_id:136432)[等变性](@entry_id:636671)**

理解编码器的一个关键点是，在不引入任何位置信息的情况下，它本质上是一个**集合处理器（set processor）**。这意味着它的操作是**[置换](@entry_id:136432)等变的（permutation-equivariant）**。具体来说，如果我们将输入序列 $\mathbf{x} = (x_1, x_2, \dots, x_n)$ 的顺序打乱，得到一个新的序列 $\mathbf{x}' = \pi \cdot \mathbf{x}$（其中 $\pi$ 是一个[置换](@entry_id:136432)），那么编码器的输出 $\mathbf{h}' = E(\mathbf{x}')$ 将会是原始输出 $\mathbf{h} = E(\mathbf{x})$ 的同样[置换](@entry_id:136432)版本，即 $\mathbf{h}' = \pi \cdot \mathbf{h}$。

这一特性导致了一个重要的结论：没有位置信息的 Transformer 编码器无法解决任何对顺序敏感的任务。我们可以通过一个[最小反例](@entry_id:160710)来证明这一点 [@problem_id:3195584]。考虑一个[二元分类](@entry_id:142257)任务，标签依赖于输入序列的顺序，例如，对于序列 $(x_1, x_2)$，当且仅当 $u^\top x_1 > u^\top x_2$ 时标签为 1。现在给定两个序列 $\mathbf{x} = (a, b)$ 和 $\mathbf{x}' = (b, a)$。假设 $u^\top a > u^\top b$，那么 $\mathbf{x}$ 的标签应为 1，而 $\mathbf{x}'$ 的标签应为 0。然而，由于编码器是[置换](@entry_id:136432)等变的，它对 $(a, b)$ 和 $(b, a)$ 的输出表示仅仅是位置上的交换。如果后续的分类器（如[平均池化](@entry_id:635263)）是[置换](@entry_id:136432)不变的，那么它将无法区分这两个序列，从而对它们给出完全相同的预测。这个简单的思想实验有力地证明了**位置编码（Positional Encoding）**对于 Transformer 捕捉序列顺序信息的不可或缺性。

**信息流**

编码器中使用的双向[注意力机制](@entry_id:636429)，允许每个词元在每一层都与序列中的所有其他词元进行交互。这种全局性的信息交换使得编码器能够构建出深度情境化的表示。对于一些需要依赖未来信息的任务，例如一个判断转换 $(t_i \to t_{i+1})$ 是否合法的规则取决于序列后缀 $(t_{i+2}, \dots, t_n)$ 的多数票 [@problem_id:3195520]，一个双向的编码器架构原则上是能够解决的，因为它在处理任何位置时都能访问整个序列。

#### 解码器堆栈

解码器同样由 $L$ 个相同的层堆叠而成，但其结构更为复杂，每层包含三个子模块：一个带掩码的多头[自注意力](@entry_id:635960)模块，一个多头[交叉注意力](@entry_id:634444)模块，以及一个前馈网络。

**两种注意力机制**

解码器中的两种注意力机制各自承担着不同的关键任务：
1.  **带掩码的[自注意力](@entry_id:635960)（Masked Self-Attention）**：此模块的功能与编码器中的[自注意力](@entry_id:635960)类似，但增加了一个**因果掩码**。这个掩码确保了在计算位置 $t$ 的表示时，只能关注到位置 $j \le t$ 的词元。这种严格的自回归（autoregressive）属性是解码器能够逐个生成序列的基础，因为它保证了在预测第 $t$ 个词元时，模型不会“偷看”到第 $t$ 个及之后的词元。

2.  **[交叉注意力](@entry_id:634444)（Cross-Attention）**：这是连接解码器与编码器的桥梁。在该模块中，Query 来自于前一个解码器子层（即带掩码的[自注意力](@entry_id:635960)的输出），而 Key 和 Value 则来自于**编码器堆栈的最终输出**。这使得解码器在生成每个词元时，能够“回顾”和“参考”源序列的完整情境化表示，从而决定当前最应该生成什么内容。

#### [编码器-解码器](@entry_id:637839)交互

编码器和解码器之间的交互主要通过[交叉注意力](@entry_id:634444)机制实现，这一机制在结构和功能上与[自注意力](@entry_id:635960)有着显著区别。

**[自注意力](@entry_id:635960) vs. [交叉注意力](@entry_id:634444)**

让我们从张量形状的角度来对比这两种机制 [@problem_id:3192568]。在一个典型的 Transformer 中，假设[批大小](@entry_id:174288)为 $B$，编码器序列长度为 $L_e$，解码器序列长度为 $L_d$，模型维度为 $d_{\text{model}}$，[注意力头](@entry_id:637186)数为 $h$。
*   在**编码器[自注意力](@entry_id:635960)**中，Q, K, V 均来自编码器自身，其序列长度均为 $L_e$。注意力分数矩阵的形状为 $\mathbb{R}^{B \times h \times L_e \times L_e}$。
*   在**解码器[自注意力](@entry_id:635960)**中，Q, K, V 均来自解码器自身，其序列长度均为 $L_d$。注意力分数矩阵的形状为 $\mathbb{R}^{B \times h \times L_d \times L_d}$。
*   在**[交叉注意力](@entry_id:634444)**中，Q 来自解码器（长度 $L_d$），而 K, V 来自编码器（长度 $L_e$）。因此，注意力分数矩阵的形状为 $\mathbb{R}^{B \times h \times L_d \times L_e}$，它捕捉了目标序列中每个位置对源序列中每个位置的关注程度。

**自回归生成与 KV 缓存**

在推理阶段，解码器以自回归的方式逐个生成词元。如果在每个时间步都从头计算所有注意力，效率将非常低下。**KV 缓存（KV Caching）**是一种至关重要的优化手段。
*   对于**[交叉注意力](@entry_id:634444)**，编码器的输出是固定不变的。因此，编码器产生的 Key ($K_e$) 和 Value ($V_e$) 只需要计算**一次**，然后在解码的每一步中重复使用。这极大地节省了计算资源。
*   对于**解码器[自注意力](@entry_id:635960)**，在生成第 $t$ 个词元时，它需要关注前面已经生成的 $t-1$ 个词元。我们可以缓存这 $t-1$ 个词元的 Key 和 Value。在下一步生成第 $t+1$ 个词元时，只需计算第 $t$ 个词元的 K, V 并将其追加到缓存中，而无需重新计算前面所有词元的 K, V。

因此，[交叉注意力](@entry_id:634444)的 KV 缓存大小与 $L_e$ 成正比，在解码过程中保持不变；而解码器[自注意力](@entry_id:635960)的 KV 缓存大小与当前生成的长度 $L_d$ 成正比，随着生成的进行而增长 [@problem_id:3192568]。

**关于[信息泄露](@entry_id:155485)的微妙之处**

一个值得注意的细节是，标准的[编码器-解码器](@entry_id:637839)架构可能存在微妙的**[信息泄露](@entry_id:155485)**问题。由于编码器是双向的，它在计算源序列的表示时，可以访问整个序列。如果训练数据中源序列和目标序列存在某种关联（例如，在机器翻译中，源序列是目标序列的直接翻译），那么编码器的输出 $E_j$ 可能已经间接编码了关于未来目标词元 $y_k$ (其中 $k>t$) 的信息。当解码器在时间步 $t$ 通过[交叉注意力](@entry_id:634444)查询 $E_j$ 时，这个未来信息就可能“泄露”到当前的预测 $z_t$ 中。

我们可以通过一个线性化模型来精确分析这种泄露 [@problem_id:3195596]。假设编码器输出 $E_j = \sum_k B_{jk} y_k$，解码器在时间步 $t$ 的预测 $z_t$ 依赖于[交叉注意力](@entry_id:634444)上下文 $c_t = \sum_j a_{t,j} E_j$。那么，预测 $z_t$ 对未来目标词元 $y_k$ ($k>t$) 的敏感度（即[偏导数](@entry_id:146280)）为：
$$
\frac{\partial z_t}{\partial y_k} = r \sum_{j=1}^{n} a_{t,j} B_{j,k}
$$
要完全杜绝[信息泄露](@entry_id:155485)，必须满足 $\sum_{j} a_{t,j} B_{j,k} = 0$ 对于所有 $k>t$。然而在实际模型中，这个条件通常难以满足。在一个具体数值例子中，我们可以计算出泄露敏感度 $\frac{\partial z_2}{\partial y_3} = 1.46$，这表明未来词元 $y_3$ 的值的变化会直接影响到当前词元 $z_2$ 的预测，证实了泄露的存在。

### 稳定与性能的核心机制

除了上述基本架构，Transformer 的成功还得益于几个保证其深度和性能的关键机制。

#### 深度带来的挑战：[残差连接](@entry_id:637548)

与所有深度神经网络一样，堆叠非常多的 Transformer 层会面临梯度消失或爆炸的问题。**[残差连接](@entry_id:637548)（Residual Connections）**是解决这个问题的关键。在每个子层（如 MHA 或 FFN）周围，都包裹着一个[残差连接](@entry_id:637548)，其更新规则为：
$$
x_{l+1} = x_l + \text{Sublayer}(x_l)
$$
其中 $x_l$ 是第 $l$ 层的输入，$\text{Sublayer}(\cdot)$ 是子层函数。

[残差连接](@entry_id:637548)通过在网络中创建一条直接的“信息高速公路”，极大地缓解了[梯度消失问题](@entry_id:144098)。我们可以通过分析梯度在反向传播过程中的范数来理解这一点 [@problem_id:3195511]。第 $l$ 层的[雅可比矩阵](@entry_id:264467)为 $J_l = I + \nabla F_l(x_l)$，其中 $F_l$ 代表子层函数。[反向传播](@entry_id:199535)时，梯度从 $l+1$ 层传递到 $l$ 层遵循 $g_l = J_l^\top g_{l+1}$。经过 $L$ 层的堆叠，输入端的梯度 $g_0$ 与输出端的梯度 $g_L$ 之间的关系由一长串[雅可比矩阵](@entry_id:264467)的乘积决定。

由于 $J_l$ 中包含了单位矩阵 $I$，整个乘积不容易变为零。更严格地说，如果我们假设子层雅可比矩阵的[谱范数](@entry_id:143091)有界，即 $\|\nabla F_l(x_l)\|_2 \le \rho  1$，那么可以证明 $J_l$ 的最小[奇异值](@entry_id:152907)有下界 $1-\rho$。这确保了梯度范数在反向传播时，其衰减速度是指数级的，但基数是 $(1-\rho)$ 而非可能远小于 1 的 $\rho$。最终我们得到一个下界：
$$
\|g_0\|_2 \ge (1-\rho)^L \|g_L\|_2
$$
例如，对于一个 $L=24$ 层的网络，如果 $\rho=0.1$，那么梯度范数至少能保持其初始值的 $(0.9)^{24} \approx 0.07977$。相比之下，没有[残差连接](@entry_id:637548)的普通网络，其梯度范数可能以 $\rho^L = (0.1)^{24}$ 的速度衰减，这是一个天文数字般的差异。

#### 归一化策略：Pre-LN vs. Post-LN

[层归一化](@entry_id:636412)（Layer Normalization, LN）是另一个稳定 Transformer 训练的关键组件。它在应用的位置上产生了两种主流变体：
*   **Post-LN**：这是原始 Transformer 论文中提出的结构，归一化在[残差连接](@entry_id:637548)**之后**进行，即 $x_{l+1} = \text{LN}(x_l + F(x_l))$。
*   **Pre-LN**：这种变体将归一化移到了[残差连接](@entry_id:637548)**之前**，即 $x_{l+1} = x_l + F(\text{LN}(x_l))$。

尽管看似微小的改动，却对训练稳定性，尤其是对于非常深的模型，有着巨大影响。经验表明，Pre-LN 通常比 Post-LN 更为稳定。我们可以通过分析[梯度爆炸](@entry_id:635825)风险来从理论上解释这一现象 [@problem_id:3195586]。

在 Post-LN 架构中，LN 操作位于主干信息路径上。其单层梯度[放大系数](@entry_id:144315)的上限可以被推导为 $C = \kappa (1 + \alpha \|J_l\|_2)$，其中 $\kappa$ 是 LN [雅可比](@entry_id:264467)范数的[上界](@entry_id:274738)，$\alpha$ 是残差缩放因子。在训练初期，子层雅可比矩阵 $J_l$ 的范数可能较大，导致 $C$ 值大于 1。经过 $L$ 层网络，总的梯度放大因子[上界](@entry_id:274738)为 $C^L$，这意味着梯度范数可能随深度呈指数级增长，导致训练不稳定。

相比之下，在 Pre-LN 架构中，LN 操作位于残差分支内部，主干路径是一条“干净”的恒等连接 $x_{l+1} = x_l + \dots$。这使得梯度能够更平滑地在层间传递，避免了 Post-LN 中可能出现的指数级放大效应，从而保证了更深的网络的训练稳定性。

#### 计算复杂度与效率

[自注意力机制](@entry_id:638063)虽然强大，但其计算成本是其应用于长序列时的主要瓶颈。标准（密集）[自注意力](@entry_id:635960)的核心计算是 $Q K^\top$ [矩阵乘法](@entry_id:156035)。对于一个长度为 $n$ 的序列，Q 和 K 矩阵的形状为 $n \times d_k$。因此，$Q K^\top$ 的计算复杂度为 $O(n^2 d_k)$。后续与 V 矩阵的乘法复杂度为 $O(n^2 d_v)$。因此，单个[注意力头](@entry_id:637186)的总计算复杂度为 $O(n^2 d)$（其中 $d$ 是模型维度），而存储注意力分数矩阵 $A$ 的内存占用为 $O(n^2)$ [@problem_id:3195507]。

这种与序列长度 $n$ 的**二次方依赖关系**意味着，当 $n$ 很大时（例如，处理长文档或高分辨率图像），计算量和内存需求会急剧增长，变得难以承受。

为了解决这个问题，研究者们提出了多种**稀疏注意力（Sparse Attention）**方案。一个简单的例子是**块状稀疏注意力（Block-sparse Attention）**。它将序列划分为 $n/b$ 个大小为 $b$ 的块，并限制注意力只在块内进行。在这种情况下，计算复杂度变成了在 $n/b$ 个块上分别执行密集注意力的总和，即 $(n/b) \times O(b^2 d) = O(nbd)$。与密集注意力的 $O(n^2 d)$ 相比，计算量减少了一个因子 $n/b$。例如，如果序列长度为 $n=4096$，块大小为 $b=64$，那么计算量可以减少到原来的 $1/64$。这种从二次复杂度到[线性复杂度](@entry_id:144405)的转变，是使 Transformer 能够高效处理长序列的关键一步。