{"hands_on_practices": [{"introduction": "理解任何深度学习模型的第一步是掌握其基本架构和规模。这个练习将引导你深入BERT编码器的内部，通过从头推导其可学习参数总数，来量化模型的复杂性。通过这个计算过程 [@problem_id:3102535]，你将清晰地看到隐藏层大小$d$、前馈网络维度$d_{ff}$等超参数是如何共同决定一个BERT模型的大小的。", "problem": "考虑 BERT (Bidirectional Encoder Representations from Transformers) 中的单个编码器块。该块包含一个多头自注意力（Multi-Head Self-Attention, MHSA）模块，其后是一个双层逐点前馈网络（position-wise feed-forward network），并带有残差连接（residual connections）和两个层归一化（Layer Normalization, LN）模块。假设采用以下与原始架构一致的经典设计选择：\n- 隐藏层大小为$d$，注意力头的数量为$h$，前馈网络内部维度为$d_{ff}$。\n- 查询（Query）、键（Key）和值（Value）的投影均实现为从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d}$的可学习仿射映射（affine map），每个映射都有一个$\\mathbb{R}^{d \\times d}$的权重矩阵和一个$\\mathbb{R}^{d}$的偏置。注意力输出投影同样是一个可学习的仿射映射，其权重在$\\mathbb{R}^{d \\times d}$中，偏置在$\\mathbb{R}^{d}$中。\n- 前馈网络由两个可学习的仿射映射组成：第一个是从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d_{ff}}$，第二个是从$\\mathbb{R}^{d_{ff}}$回到$\\mathbb{R}^{d}$，每个映射都有一个权重矩阵和一个偏置向量。两者之间的非线性函数是高斯误差线性单元（Gaussian Error Linear Unit, GELU），它没有可学习的参数。\n- 每个块中应用两次层归一化，每个 LN 都有一个可学习的缩放（scale）$\\gamma \\in \\mathbb{R}^{d}$和位移（shift）$\\beta \\in \\mathbb{R}^{d}$。该块中没有其他可学习的参数。\n- 词元嵌入（Token embeddings）和位置编码（positional encodings）是块外部嵌入层的一部分，此处不得计算在内。\n\n仅从核心定义出发——即一个从$\\mathbb{R}^{m}$到$\\mathbb{R}^{n}$的可学习仿射映射有$n \\times m$个权重参数和$n$个偏置参数，并且每次应用层归一化会贡献$2 \\times d$个参数（缩放和位移）——推导出一个关于$d$、$d_{ff}$和$h$的该编码器块总可学习参数数量的封闭形式表达式。然后，提出一个变体，将头的数量从$h$增加到$\\alpha h$，同时在给定隐藏层大小$d$和序列长度的情况下，保持 MHSA 投影和注意力分数乘法的总计算量不变（你可以假设每个头的维度是$d/h$，并且保持$d$不变可以固定主要的 MHSA 计算量）。对于$\\alpha = 2$的这个变体，给出最终的每个头的维度。\n\n最后，对 BERT-Base 配置$d = 768$，$d_{ff} = 3072$，$h = 12$和$\\alpha = 2$进行数值评估，计算你的表达式。以包含以下两个条目的行矩阵形式提供你的最终答案：\n- 编码器块的总参数数量，以及\n- 将头的数量增加因子$\\alpha$后的新每个头的维度。\n\n无需四舍五入；报告精确整数。将你的最终答案表示为$\\begin{pmatrix} \\text{param\\_count}  \\text{new\\_head\\_dim} \\end{pmatrix}$形式的行矩阵。", "solution": "解决方案分三部分进行：首先，推导 BERT 编码器块中可学习参数数量的一般符号表达式；其次，分析所提出的架构变体；第三，对给定配置的推导表达式进行数值评估。\n\n**第一部分：总可学习参数数量的推导**\n\n总可学习参数数量$P_{\\text{total}}$是其组成部分参数的总和：多头自注意力（MHSA）模块、逐点前馈网络（FFN）以及两个层归一化（LN）模块。我们将根据提供的定义计算每个组件的参数。\n\n一个从$\\mathbb{R}^{m}$到$\\mathbb{R}^{n}$的可学习仿射映射有一个大小为$n \\times m$的权重矩阵和一个大小为$n$的偏置向量，总共有$n \\times m + n$个参数。\n\n1.  **多头自注意力（MHSA）参数 ($P_{\\text{MHSA}}$)**\n    MHSA 机制由问题陈述中定义的四个可学习仿射投影组成。\n    *   **查询投影（Q）：** 从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d}$的仿射映射。\n        - 权重矩阵$W_Q \\in \\mathbb{R}^{d \\times d}$：$d \\times d = d^2$个参数。\n        - 偏置向量$b_Q \\in \\mathbb{R}^{d}$：$d$个参数。\n        - Q 的总参数：$d^2 + d$。\n    *   **键投影（K）：** 从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d}$的仿射映射。\n        - 权重矩阵$W_K \\in \\mathbb{R}^{d \\times d}$：$d^2$个参数。\n        - 偏置向量$b_K \\in \\mathbb{R}^{d}$：$d$个参数。\n        - K 的总参数：$d^2 + d$。\n    *   **值投影（V）：** 从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d}$的仿射映射。\n        - 权重矩阵$W_V \\in \\mathbb{R}^{d \\times d}$：$d^2$个参数。\n        - 偏置向量$b_V \\in \\mathbb{R}^{d}$：$d$个参数。\n        - V 的总参数：$d^2 + d$。\n    *   **输出投影（O）：** 从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d}$的仿射映射。\n        - 权重矩阵$W_O \\in \\mathbb{R}^{d \\times d}$：$d^2$个参数。\n        - 偏置向量$b_O \\in \\mathbb{R}^{d}$：$d$个参数。\n        - O 的总参数：$d^2 + d$。\n    MHSA 模块中的总参数数量是这四个组件的总和。请注意，头的数量$h$没有明确出现在这个公式中，因为投影被定义为对维度为$d$的整个隐藏状态进行的单个变换。\n    $$P_{\\text{MHSA}} = (d^2 + d) + (d^2 + d) + (d^2 + d) + (d^2 + d) = 4(d^2 + d) = 4d^2 + 4d$$\n\n2.  **逐点前馈网络（FFN）参数 ($P_{\\text{FFN}}$)**\n    FFN 由两个可学习的仿射映射和一个 GELU 非线性函数组成，该非线性函数没有参数。\n    *   **第一层：** 从$\\mathbb{R}^{d}$到$\\mathbb{R}^{d_{ff}}$的仿射映射。\n        - 权重矩阵$W_1 \\in \\mathbb{R}^{d_{ff} \\times d}$：$d_{ff} \\times d$个参数。\n        - 偏置向量$b_1 \\in \\mathbb{R}^{d_{ff}}$：$d_{ff}$个参数。\n        - 第一层的总参数：$d \\cdot d_{ff} + d_{ff}$。\n    *   **第二层：** 从$\\mathbb{R}^{d_{ff}}$到$\\mathbb{R}^{d}$的仿射映射。\n        - 权重矩阵$W_2 \\in \\mathbb{R}^{d \\times d_{ff}}$：$d \\times d_{ff}$个参数。\n        - 偏置向量$b_2 \\in \\mathbb{R}^{d}$：$d$个参数。\n        - 第二层的总参数：$d \\cdot d_{ff} + d$。\n    FFN 中的总参数数量是这两层的总和。\n    $$P_{\\text{FFN}} = (d \\cdot d_{ff} + d_{ff}) + (d \\cdot d_{ff} + d) = 2d \\cdot d_{ff} + d_{ff} + d$$\n\n3.  **层归一化（LN）参数 ($P_{\\text{LN}}$)**\n    该块包含两个层归一化模块。每个模块都有一个可学习的缩放向量$\\gamma \\in \\mathbb{R}^{d}$和一个可学习的位移向量$\\beta \\in \\mathbb{R}^{d}$。\n    - 每个 LN 模块的参数：$d + d = 2d$。\n    - 两个 LN 模块的总参数：\n    $$P_{\\text{LN}} = 2 \\times (2d) = 4d$$\n\n**总参数 ($P_{\\text{total}}$)**\n编码器块中的总可学习参数数量是所有组件参数的总和。\n$$P_{\\text{total}} = P_{\\text{MHSA}} + P_{\\text{FFN}} + P_{\\text{LN}}$$\n$$P_{\\text{total}} = (4d^2 + 4d) + (2d \\cdot d_{ff} + d_{ff} + d) + (4d)$$\n$$P_{\\text{total}} = 4d^2 + (4d + d + 4d) + 2d \\cdot d_{ff} + d_{ff}$$\n$$P_{\\text{total}} = 4d^2 + 9d + d_{ff}(2d + 1)$$\n这就是总参数数量的封闭形式表达式。\n\n**第二部分：架构变体与新的每头维度**\n\n问题提出了一个变体，其中注意力头的数量从$h$变为$h' = \\alpha h$，而模型的隐藏层大小$d$保持不变。在标准的 Transformer 架构中，所有头的总维度等于模型的隐藏层大小。也就是说，如果$d_k$是每个头的查询、键和值向量的维度，那么$h \\cdot d_k = d$。\n\n原始的每个头的维度是$d_k = \\frac{d}{h}$。\n新的头的数量是$h' = \\alpha h$。\n为了保持关系$h' \\cdot d'_k = d$，其中$d'_k$是新的每个头的维度，我们可以求解$d'_k$：\n$$d'_k = \\frac{d}{h'} = \\frac{d}{\\alpha h} = \\frac{1}{\\alpha} \\left(\\frac{d}{h}\\right)$$\n新的每个头的维度是原始每个头的维度乘以$1/\\alpha$。\n\n**第三部分：数值评估**\n\n我们给出了 BERT-Base 配置：$d = 768$，$d_{ff} = 3072$，$h = 12$，以及修改因子$\\alpha = 2$。\n\n*   **总参数数量：**\n    我们将给定值代入推导出的$P_{\\text{total}}$公式中。\n    $$P_{\\text{total}} = 4(768)^2 + 9(768) + 3072(2 \\cdot 768 + 1)$$\n    $$P_{\\text{total}} = 4(589824) + 6912 + 3072(1536 + 1)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 3072(1537)$$\n    $$P_{\\text{total}} = 2359296 + 6912 + 4721664$$\n    $$P_{\\text{total}} = 7087872$$\n\n*   **新的每头维度：**\n    原始的头的数量是$h=12$。新的头的数量是$h' = \\alpha h = 2 \\times 12 = 24$。\n    隐藏层大小为$d = 768$。新的每个头的维度$d'_k$是：\n    $$d'_k = \\frac{d}{h'} = \\frac{768}{24}$$\n    为了计算这个值，我们可以先除以$12$再除以$2$：\n    $$d'_k = \\frac{768/12}{2} = \\frac{64}{2} = 32$$\n    新的每个头的维度是$32$。\n\n最终答案要求以一个包含总参数数量和新的每头维度的行矩阵形式给出。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 7087872  32 \\end{pmatrix}\n}\n$$", "id": "3102535"}, {"introduction": "虽然像BERT这样的大型模型常被视为“黑箱”，但我们可以运用一些技巧来解读其内部决策过程。本练习 [@problem_id:3102501] 聚焦于自注意力机制的核心功能，通过分析注意力权重来追踪模型如何解决代词指代消解这一关键语言现象。这是一个绝佳的动手实践，可以让你直观地量化并理解模型学到的词元之间的依赖关系。", "problem": "给定一个简化的单层多头自注意力设置，其灵感来自 Transformers 的双向编码器表示 (BERT)。目标是在三个共指示例中，计算从一个代词词元到其候选先行词的基于注意力的归因，并识别哪些注意力头专用于处理长距离依赖。\n\n基本原理：\n- 在缩放点积自注意力中，会为每个词元计算查询、键和值。从词元$i$到词元$j$的注意力权重为$a_{ij} = \\mathrm{softmax}\\left(\\frac{q_i \\cdot k_j}{\\sqrt{d_k}}\\right)$，由于 softmax 函数的作用，该权重是行随机的，即对于每一行$i$，都有$\\sum_{j=0}^{N-1} a_{ij} = 1$。\n- 在具有$H$个头的多头注意力中，每个头$h$都会产生一个行随机矩阵$A^{(h)} \\in \\mathbb{R}^{N \\times N}$。对所有头进行简单的均匀聚合会得到$A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)}$，该矩阵也是行随机的。\n- 在注意力归因层面，残差连接可以通过凸组合$\\tilde{A} = \\lambda I + (1 - \\lambda) A$来近似，其中$I$是单位矩阵，$\\lambda \\in [0,1]$控制保留多少词元原始表示的比例。\n- 从源索引$s$到所有词元的注意力归因计算为行向量$e_s^\\top \\tilde{A}$，其中$e_s$是在位置$s$处为 1、其他位置为 0 的 one-hot 行向量。对于候选先行词索引$k$的归因分数是$e_s^\\top \\tilde{A}$的第$k$个元素。\n\n长距离专职化定义：\n- 固定一个距离阈值$D \\in \\mathbb{N}$和一个专职化阈值$T \\in [0,1]$。对于头$h$，在源索引$s$处的长距离质量为$r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k}$。如果$r^{(h)}(s) \\ge T$，则认为头$h$在$s$处是“长距离专职化”的。\n\n完整注意力矩阵的假设与构建：\n- 对于每个测试用例，每个头的代词行（代词索引处的查询）被明确指定为在所有词元索引上的概率分布。对于所有非代词行，假设一个简单的、科学上合理的局部上下文分布：设$w_{\\text{self}} = 0.7$，并将剩余的质量$1 - w_{\\text{self}} = 0.3$平均分配给存在的直接邻居（左和右），并根据需要进行边界调整，以使每个非代词行的总和为$1$。\n\n索引规则：\n- 使用从零开始的索引。词元索引从$0$到$N-1$。\n- 每个测试用例都提供了候选先行词的索引。如果两个候选者获得相等的最大归因（在机器精度范围内），则通过选择最小的索引来打破平局。\n\n全局参数：\n- 层数$L = 1$。\n- 头的数量$H = 3$。\n- 残差混合参数$\\lambda = 0.2$。\n- 长距离阈值$D = 2$。\n- 专职化阈值$T = 0.6$。\n\n测试套件：\n- 测试用例 1（短距离与中等距离共指）：\n  - 序列长度$N = 7$。\n  - 代词索引$s = 5$。\n  - 候选先行词$\\{1, 3\\}$。\n  - 每个头的代词行分布：\n    - 头 0: [0.05, 0.30, 0.05, 0.45, 0.05, 0.00, 0.10]。\n    - 头 1: [0.10, 0.20, 0.10, 0.40, 0.10, 0.00, 0.10]。\n    - 头 2: [0.05, 0.20, 0.05, 0.55, 0.05, 0.00, 0.10]。\n- 测试用例 2（更远的长距离先行词）：\n  - 序列长度$N = 12$。\n  - 代词索引$s = 11$。\n  - 候选先行词$\\{6, 9\\}$。\n  - 每个头的代词行分布：\n    - 头 0: [0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.55, 0.05, 0.15, 0.05, 0.03, 0.02]。\n    - 头 1: [0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.30, 0.05, 0.10, 0.35, 0.03, 0.02]。\n    - 头 2: [0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.45, 0.05, 0.20, 0.10, 0.03, 0.02]。\n- 测试用例 3（两个候选者平局）：\n  - 序列长度$N = 6$。\n  - 代词索引$s = 5$。\n  - 候选先行词$\\{1, 4\\}$。\n  - 每个头的代词行分布：\n    - 头 0: [0.05, 0.40, 0.05, 0.15, 0.30, 0.05]。\n    - 头 1: [0.05, 0.20, 0.05, 0.30, 0.40, 0.00]。\n    - 头 2: [0.05, 0.30, 0.05, 0.20, 0.20, 0.20]。\n\n每个测试用例所需的计算：\n1. 构建每个头的完整注意力矩阵$A^{(h)} \\in \\mathbb{R}^{N \\times N}$，方法是将给定的代词行放在索引$s$处，并使用上述局部上下文规则填充所有其他行。验证每行的总和为$1$。\n2. 计算均匀的头聚合$A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)}$，然后计算残差混合矩阵$\\tilde{A} = \\lambda I + (1 - \\lambda) A$。\n3. 计算归因向量$u = e_s^\\top \\tilde{A}$，并提取候选索引处的归因分数。选择具有最大归因的先行词索引$k^\\star$；通过选择最小索引来打破平局。\n4. 对每个头$h$，计算长距离质量$r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k}$。报告满足$r^{(h)}(s) \\ge T$的头索引列表。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个列表，每个测试用例对应一个条目。每个条目本身必须是一个包含两个元素的列表：所选的先行词索引$k^\\star$（一个整数）和长距离专职化头索引列表（一个整数列表）。整个输出必须打印为单个以逗号分隔的列表，用方括号括起来，不含空格，例如$[\\,[k_1^\\star,[h_{1,1},\\dots]],[k_2^\\star,[\\dots]],\\dots\\,]$。", "solution": "该问题要求分析一个简化的单层多头自注意力模型在三个测试用例中的表现。对于每个用例，我们必须从一组候选中确定代词最可能的先行词，并识别哪些注意力头专用于处理长距离依赖。\n\n分析基于以下全局参数：\n- 注意力头数量：$H = 3$\n- 残差混合参数：$\\lambda = 0.2$\n- 长距离阈值：$D = 2$\n- 长距离专职化阈值：$T = 0.6$\n- 非代词行的自注意力权重：$w_{\\text{self}} = 0.7$\n\n每个测试用例的解决方案都涉及四个连续的计算步骤。\n\n**步骤 1：构建各注意力头的注意力矩阵 ($A^{(h)}$)**\n对于每个头$h \\in \\{0, 1, 2\\}$，我们需要构建一个完整的$N \\times N$注意力矩阵$A^{(h)}$。对应于索引$s$处代词的行，表示为$A^{(h)}_{s,:}$，被明确指定为一个概率分布。所有其他行$i \\neq s$则根据局部上下文启发式规则填充：自注意力权重为$A^{(h)}_{i,i} = w_{\\text{self}} = 0.7$。剩余的概率$1 - w_{\\text{self}} = 0.3$被平均分配给其直接邻居。这导致了对$i \\neq s$的以下赋值：\n- 对于内部词元$i \\in \\{1, \\dots, N-2\\}$：$A^{(h)}_{i, i-1} = 0.15$且$A^{(h)}_{i, i+1} = 0.15$。\n- 对于第一个词元$i=0$：$A^{(h)}_{0, 1} = 0.3$。\n- 对于最后一个词元$i=N-1$：$A^{(h)}_{N-1, N-2} = 0.3$。\n行$i \\neq s$中的所有其他条目均为零。这种构造确保每个矩阵$A^{(h)}$都是行随机的。\n\n**步骤 2：头聚合与残差混合**\n将各个头的矩阵平均，形成一个单一的聚合注意力矩阵$A$：\n$$ A = \\frac{1}{H} \\sum_{h=0}^{H-1} A^{(h)} $$\n接下来，通过对$A$和单位矩阵$I$进行凸组合来建模残差连接：\n$$ \\tilde{A} = \\lambda I + (1 - \\lambda) A $$\n当$\\lambda = 0.2$时，公式变为$\\tilde{A} = 0.2 I + 0.8 A$。\n\n**步骤 3：通过注意力归因进行先行词识别**\n从源$s$处的代词到所有其他词元的归因由$\\tilde{A}$的第$s$行给出，我们将其表示为向量$u = e_s^\\top \\tilde{A}$。对于索引为$k$的候选先行词，其分数为该向量的第$k$个元素，即$u_k = (\\tilde{A})_{s,k}$。\n通过找到具有最大归因分数的候选者来从给定集合中选择先行词$k^\\star$：\n$$ k^\\star = \\arg\\max_{k \\in \\text{Candidates}} u_k $$\n如果出现平局，则选择索引最小的候选者。值得注意的是，对于任何候选者$k \\neq s$，其分数为$u_k = (1-\\lambda)A_{s,k}$。由于$(1-\\lambda)$是一个正常数，最大化$u_k$等同于最大化$A_{s,k}$。\n\n**步骤 4：识别长距离专职化头**\n如果一个头$h$对距离大于等于$D$的词元所付出的总注意力达到或超过阈值$T$，则该头在源索引$s$处被视为“长距离专职化”。长距离质量计算如下：\n$$ r^{(h)}(s) = \\sum_{k \\,:\\, |k-s| \\ge D} A^{(h)}_{s,k} $$\n如果$r^{(h)}(s) \\ge T$，则该头是专职化的。\n\n一个关键的观察是，步骤 3（先行词识别）和步骤 4（专职化）的结果仅取决于代词行$A^{(h)}_{s,:}$。因此，构建非代词行对于最终所需的输出是无关紧要的。然而，为了严格遵守问题陈述，我们仍按规定执行该程序。\n\n**各测试用例的计算与结果：**\n\n**测试用例 1：**\n- 参数：$N=7$，$s=5$，候选者$=\\{1, 3\\}$。\n- **先行词识别**：我们比较候选者$k=1$和$k=3$的聚合注意力分数。\n  - $A_{5,1} = \\frac{1}{3} (A^{(0)}_{5,1} + A^{(1)}_{5,1} + A^{(2)}_{5,1}) = \\frac{1}{3} (0.30 + 0.20 + 0.20) = \\frac{0.70}{3} \\approx 0.2333$\n  - $A_{5,3} = \\frac{1}{3} (A^{(0)}_{5,3} + A^{(1)}_{5,3} + A^{(2)}_{5,3}) = \\frac{1}{3} (0.45 + 0.40 + 0.55) = \\frac{1.40}{3} \\approx 0.4667$\n  由于$A_{5,3} > A_{5,1}$，所选的先行词为$k^\\star = 3$。\n- **长距离专职化**：对于$s=5$和$D=2$，长距离索引为$k \\in \\{0, 1, 2, 3\\}$。\n  - $r^{(0)}(5) = A^{(0)}_{5,0}+A^{(0)}_{5,1}+A^{(0)}_{5,2}+A^{(0)}_{5,3} = 0.05 + 0.30 + 0.05 + 0.45 = 0.85 \\ge 0.6$。头 0 是专职化的。\n  - $r^{(1)}(5) = A^{(1)}_{5,0}+A^{(1)}_{5,1}+A^{(1)}_{5,2}+A^{(1)}_{5,3} = 0.10 + 0.20 + 0.10 + 0.40 = 0.80 \\ge 0.6$。头 1 是专职化的。\n  - $r^{(2)}(5) = A^{(2)}_{5,0}+A^{(2)}_{5,1}+A^{(2)}_{5,2}+A^{(2)}_{5,3} = 0.05 + 0.20 + 0.05 + 0.55 = 0.85 \\ge 0.6$。头 2 是专职化的。\n- 结果：$k^\\star=3$；专职化头：$[0, 1, 2]$。\n\n**测试用例 2：**\n- 参数：$N=12$，$s=11$，候选者$=\\{6, 9\\}$。\n- **先行词识别**：\n  - $A_{11,6} = \\frac{1}{3}(0.55 + 0.30 + 0.45) = \\frac{1.30}{3} \\approx 0.4333$\n  - $A_{11,9} = \\frac{1}{3}(0.15 + 0.35 + 0.10) = \\frac{0.60}{3} = 0.2000$\n  由于$A_{11,6} > A_{11,9}$，所选的先行词为$k^\\star = 6$。\n- **长距离专职化**：对于$s=11$和$D=2$，长距离索引为$k \\in \\{0, \\dots, 9\\}$。长距离质量可以计算为$r^{(h)}(11) = 1 - (A^{(h)}_{11,10} + A^{(h)}_{11,11})$。\n  - 对于所有头$h \\in \\{0,1,2\\}$，我们有$A^{(h)}_{11,10}=0.03$和$A^{(h)}_{11,11}=0.02$。\n  - 因此，对于每个头，$r^{(h)}(11) = 1 - (0.03 + 0.02) = 0.95 \\ge 0.6$。所有三个头都是专职化的。\n- 结果：$k^\\star=6$；专职化头：$[0, 1, 2]$。\n\n**测试用例 3：**\n- 参数：$N=6$，$s=5$，候选者$=\\{1, 4\\}$。\n- **先行词识别**：\n  - $A_{5,1} = \\frac{1}{3}(0.40 + 0.20 + 0.30) = \\frac{0.90}{3} = 0.3000$\n  - $A_{5,4} = \\frac{1}{3}(0.30 + 0.40 + 0.20) = \\frac{0.90}{3} = 0.3000$\n  分数相同。平局打破规则要求选择最小的索引，因此$k^\\star = 1$。\n- **长距离专职化**：对于$s=5$和$D=2$，长距离索引为$k \\in \\{0, 1, 2, 3\\}$。\n  - $r^{(0)}(5) = 0.05 + 0.40 + 0.05 + 0.15 = 0.65 \\ge 0.6$。头 0 是专职化的。\n  - $r^{(1)}(5) = 0.05 + 0.20 + 0.05 + 0.30 = 0.60 \\ge 0.6$。头 1 是专职化的。\n  - $r^{(2)}(5) = 0.05 + 0.30 + 0.05 + 0.20 = 0.60 \\ge 0.6$。头 2 是专职化的。\n- 结果：$k^\\star=1$；专职化头：$[0, 1, 2]$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the self-attention attribution problem for the given test cases.\n    \"\"\"\n\n    # Global parameters\n    H = 3      # Number of heads\n    LAMBDA = 0.2  # Residual mixing parameter\n    D = 2        # Long-range distance threshold\n    T = 0.6      # Specialization threshold\n    W_SELF = 0.7 # Self-attention weight for local context\n\n    test_cases = [\n        {\n            \"N\": 7,\n            \"s\": 5,\n            \"candidates\": {1, 3},\n            \"pronoun_rows\": [\n                np.array([0.05, 0.30, 0.05, 0.45, 0.05, 0.00, 0.10]),\n                np.array([0.10, 0.20, 0.10, 0.40, 0.10, 0.00, 0.10]),\n                np.array([0.05, 0.20, 0.05, 0.55, 0.05, 0.00, 0.10]),\n            ],\n        },\n        {\n            \"N\": 12,\n            \"s\": 11,\n            \"candidates\": {6, 9},\n            \"pronoun_rows\": [\n                np.array([0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.55, 0.05, 0.15, 0.05, 0.03, 0.02]),\n                np.array([0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.30, 0.05, 0.10, 0.35, 0.03, 0.02]),\n                np.array([0.02, 0.02, 0.02, 0.02, 0.02, 0.05, 0.45, 0.05, 0.20, 0.10, 0.03, 0.02]),\n            ],\n        },\n        {\n            \"N\": 6,\n            \"s\": 5,\n            \"candidates\": {1, 4},\n            \"pronoun_rows\": [\n                np.array([0.05, 0.40, 0.05, 0.15, 0.30, 0.05]),\n                np.array([0.05, 0.20, 0.05, 0.30, 0.40, 0.00]),\n                np.array([0.05, 0.30, 0.05, 0.20, 0.20, 0.20]),\n            ],\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        s = case[\"s\"]\n        candidates = case[\"candidates\"]\n        pronoun_rows = case[\"pronoun_rows\"]\n\n        # 1. Construct each head's full attention matrix A_h\n        all_A_h = []\n        for h in range(H):\n            A_h = np.zeros((N, N))\n            \n            # Place the given pronoun row\n            A_h[s, :] = pronoun_rows[h]\n\n            # Fill non-pronoun rows with the local-context rule\n            for i in range(N):\n                if i == s:\n                    continue\n                A_h[i, i] = W_SELF\n                remaining_mass = 1.0 - W_SELF\n                if i == 0:\n                    if N > 1: A_h[i, i + 1] = remaining_mass\n                elif i == N - 1:\n                    A_h[i, i - 1] = remaining_mass\n                else:\n                    A_h[i, i - 1] = remaining_mass / 2.0\n                    A_h[i, i + 1] = remaining_mass / 2.0\n            all_A_h.append(A_h)\n\n        # 2. Compute uniform head aggregation A and residual-mixed matrix A_tilde\n        A = np.mean(all_A_h, axis=0)\n        A_tilde = LAMBDA * np.identity(N) + (1 - LAMBDA) * A\n        \n        # 3. Compute attribution vector u and find best candidate k_star\n        u = A_tilde[s, :]\n        \n        candidate_scores = {k: u[k] for k in candidates}\n        \n        max_score = -1.0\n        if candidate_scores:\n            max_score = max(candidate_scores.values())\n\n        # Find all candidates with the maximal score (within a small tolerance)\n        tied_candidates = sorted([k for k, score in candidate_scores.items() if np.isclose(score, max_score)])\n        \n        # Break ties by choosing smallest index\n        k_star = tied_candidates[0] if tied_candidates else -1\n\n        # 4. Identify long-range specialized heads\n        specialized_heads = []\n        for h in range(H):\n            pronoun_row = pronoun_rows[h]\n            long_range_mass = 0.0\n            for k in range(N):\n                if abs(k - s) >= D:\n                    long_range_mass += pronoun_row[k]\n            \n            if long_range_mass >= T or np.isclose(long_range_mass, T):\n                specialized_heads.append(h)\n        \n        results.append([k_star, specialized_heads])\n    \n    # Format the final output string exactly as specified\n    output_parts = []\n    for k_star, head_list in results:\n        head_list_str = '[' + ','.join(map(str, head_list)) + ']'\n        output_parts.append(f'[{k_star},{head_list_str}]')\n    \n    final_output_str = '[' + ','.join(output_parts) + ']'\n    print(final_output_str)\n\nsolve()\n```", "id": "3102501"}, {"introduction": "一个强大的模型不应被输入中的微小、非语义的改动所欺骗。本练习 [@problem_id:3102527] 将带你进入对抗性攻击这一前沿领域，通过实现HotFlip算法来主动寻找模型的“盲点”。你将通过有策略地替换词元来翻转模型的预测结果，从而亲身体验评估模型鲁棒性的过程，并理解其在现实世界应用中的重要性。", "problem": "给定一个经过微调的 BERT (Bidirectional Encoder Representations from Transformers) 情感分类器的一阶代理模型。该代理模型模拟了作用于聚合词元表示的最终分类头。设词汇表大小为$V$，嵌入维度为$d$，长度为$L$的序列由词元索引$\\{t_1, t_2, \\dots, t_L\\}$表示，其中每个$t_i \\in \\{0,1,\\dots,V-1\\}$。嵌入矩阵为$E \\in \\mathbb{R}^{V \\times d}$，其中行$E_k \\in \\mathbb{R}^d$对应词元索引$k$。序列表示是其词元嵌入的平均值，\n$$\nh = \\frac{1}{L} \\sum_{i=1}^L E_{t_i}.\n$$\n一个带有权重$W \\in \\mathbb{R}^d$和偏置$b \\in \\mathbb{R}$的逻辑回归头计算 logit\n$$\nz = W^\\top h + b,\n$$\n正类的概率\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}},\n$$\n预测标签$\\hat{y}$由阈值规则定义：如果$p \\ge 0.5$，则$\\hat{y} = 1$；否则$\\hat{y} = 0$。对于标签$y \\in \\{0,1\\}$，二元交叉熵损失为\n$$\n\\mathcal{L}(y,p) = -\\left(y\\log(p) + (1-y)\\log(1-p)\\right).\n$$\n\nHotFlip 攻击通过翻转词元来构建对抗性样本，其目标是在最小化输入改变的同时最大化损失的增量。使用一阶泰勒近似，将位置$j$上的单个词元从$a$翻转为$b$，损失的变化量近似为\n$$\n\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\nabla_{E_{t_j}} \\mathcal{L}(y,p)^\\top \\left(E_b - E_a\\right).\n$$\n在上述代理模型下，根据链式法则可得\n$$\n\\nabla_{E_{t_j}} \\mathcal{L}(y,p) = \\frac{1}{L} \\left(p - y\\right) W,\n$$\n因此\n$$\n\\Delta \\mathcal{L}_{j,a \\rightarrow b} \\approx \\frac{p - y}{L} \\, W^\\top \\left(E_b - E_a\\right).\n$$\n\n您的任务是实现一个贪心 HotFlip 攻击。对于每个输入序列，该攻击会迭代地应用使$\\Delta \\mathcal{L}$最大化的单个翻转$(j,a \\rightarrow b)$，其中$y$固定为当前预测的标签$\\hat{y}$（也就是说，您总是最大化当前预测类别的损失）。每次翻转后，重新计算$p$和$\\hat{y}$，并持续此过程，直到预测标签翻转到相反的类别。当多个翻转产生相同的$\\Delta \\mathcal{L}$时，首先选择最小的词元索引$b$来打破平局，然后选择最小的位置索引$j$。如果无法进行翻转（本测试套件中不会出现此情况），则序列保持不变。目标是在最小化词元更改的情况下改变情感，因此当预测翻转到相反类别时，贪心过程必须立即停止。\n\n使用以下固定参数，这些参数定义了 BERT 分类头及其词汇表的代理模型：\n\n- 词汇表大小$V = 6$，嵌入维度$d = 2$。\n- 嵌入矩阵$E \\in \\mathbb{R}^{6 \\times 2}$按行定义如下\n  - $E_0 = [0.0, 0.0]$,\n  - $E_1 = [2.0, 2.0]$,\n  - $E_2 = [-2.0, -2.0]$,\n  - $E_3 = [1.0, 0.5]$,\n  - $E_4 = [-1.0, -0.5]$,\n  - $E_5 = [-1.0, -0.5]$。\n- 分类器权重$W = [1.0, 1.0]$，偏置$b = 0.0$。\n\n设计并实现一个算法来执行上述的贪心 HotFlip 攻击。您必须将其应用于以下序列测试套件（每个序列是一个词元索引列表）：\n\n- 测试用例 1 (一般情况)： $[3, 1, 0, 3]$。\n- 测试用例 2 (需要多次翻转)： $[4, 4, 4, 4, 4, 4, 4, 4]$。\n- 测试用例 3 (候选词元间的平局打破)： $[3, 3]$。\n\n对于每个测试用例，当预测标签翻转到相反类别时，程序必须输出通过贪心 HotFlip 过程获得的最终对抗性词元索引序列。每个测试用例的答案必须是一个整数列表。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个内部列表的格式不含空格。例如，格式必须为\n$$\n[\\,[a_1,a_2,\\dots],\\,[b_1,b_2,\\dots],\\,[c_1,c_2,\\dots]\\,].\n$$", "solution": "该解决方案将实现指定的贪心 HotFlip 对抗性攻击。\n\n### 算法设计与原则\n\n任务的核心是实现指定的贪心迭代攻击。对于每个测试用例，我们从初始序列及其预测开始。然后，我们进行循环，在每次迭代中找到并应用最佳的单个词元翻转，直到预测被逆转。\n\n**1. 简化目标函数**\n目标是最大化$\\Delta \\mathcal{L} \\approx \\frac{p - \\hat{y}}{L} W^\\top(E_b - E_a)$。我们来分析一下$\\frac{p - \\hat{y}}{L}$这一项：\n- 如果当前预测为$\\hat{y} = 1$，那么$p \\ge 0.5$，所以$p-1 \\le 0$。最大化$\\Delta \\mathcal{L}$需要最小化$S_{j,b} = W^\\top(E_b - E_a)$这一项。\n- 如果当前预测为$\\hat{y} = 0$，那么$p  0.5$，所以$p-0 > 0$。最大化$\\Delta \\mathcal{L}$需要最大化$S_{j,b} = W^\\top(E_b - E_a)$这一项。\n\n对于每个词元$k$，$W^\\top E_k$这一项在整个过程中是恒定的。我们可以预先计算这些值，以加快寻找最佳翻转的速度。我们定义一个向量 `wte_scores`，其中 `wte_scores[k]` $= W^\\top E_k$。\n\n**2. 贪心搜索策略**\n在攻击的每一步，我们必须根据目标和平局打破规则找到最优的$(j, b)$对。一个稳健的实现方法是分两步进行：\n1.  **找到最佳分数：**遍历所有可能的翻转（所有位置$j$和所有可能的新词元$b \\neq t_j$），找到最佳可能的分数（$S_{j,b}$的最小值或最大值）。\n2.  **收集与选择：**收集所有达到此最佳分数的翻转$(j, b)$。从这个最优候选列表中，应用平局打破规则：首先按新词元索引$b$排序，然后按位置索引$j$排序，并选择第一个。\n\n**3. 高效更新**\n每次翻转后都从头计算序列表示$h = \\frac{1}{L} \\sum E_{t_i}$效率低下。一个更高效的方法是更新嵌入的总和。如果我们将位置$j$上的词元$a$翻转为词元$b$，新的嵌入总和是：\n$$ \\sum_{\\text{new}} = \\sum_{\\text{old}} - E_a + E_b $$\n我们可以维护这个总和，并且仅在需要计算预测时才通过除以$L$来计算$h$。\n\n**4. 算法**\n对于每个测试用例：\n1.  用输入序列初始化 `tokens`。\n2.  计算初始嵌入总和$\\sum E = \\sum_{i=1}^L E_{t_i}$。\n3.  从$\\sum E$计算初始预测$\\hat{y}_{\\text{initial}}$。\n4.  进入一个 `while` 循环，直到预测翻转。\n    a. 在循环内部，确定当前预测$\\hat{y}_{\\text{current}}$。\n    b. 如果$\\hat{y}_{\\text{current}} \\neq \\hat{y}_{\\text{initial}}$，则跳出循环。\n    c. 找到最佳候选翻转集合：\n        i. 初始化一个空列表 `candidate_flips` 和 `best_score` 为$+\\infty$或$-\\infty$。\n        ii. 遍历每个位置$j \\in [0, L-1]$和每个可能的新词元$b \\in [0, V-1]$。\n        iii. 如果$b$与位置$j$的当前词元相同，则跳过。\n        iv. 计算分数$S_{j,b} = \\text{wte\\_scores}[b] - \\text{wte\\_scores}[t_j]$。\n        v. 将 `score` 与 `best_score` 比较。如果更优，则将 `candidate_flips` 重置为$[(j, b)]$并更新 `best_score`。如果相等，则将$(j, b)$添加到 `candidate_flips`。\n    d. 应用平局打破规则：按$b$再按$j$对 `candidate_flips` 排序。取第一个元素作为 `(best_j, best_b)`。\n    e. 更新状态：\n        i. 令$a = tokens[\\text{best\\_j}]$。\n        ii. 更新 `tokens[best_j] = best_b`。\n        iii. 更新$\\sum E = \\sum E - E_a + E_b$。\n5.  将最终的 `tokens` 列表添加到结果中。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a greedy HotFlip attack on a surrogate BERT sentiment classifier.\n    The attack iteratively flips a single token to maximally increase the loss,\n    until the predicted class label is reversed.\n    \"\"\"\n    \n    # Define fixed parameters for the surrogate model\n    V = 6\n    E = np.array([\n        [0.0, 0.0],    # E_0\n        [2.0, 2.0],    # E_1\n        [-2.0, -2.0],  # E_2\n        [1.0, 0.5],    # E_3\n        [-1.0, -0.5],  # E_4\n        [-1.0, -0.5],  # E_5\n    ], dtype=np.float64)\n    \n    W = np.array([1.0, 1.0], dtype=np.float64)\n    b = 0.0\n\n    # Define the test suite of input sequences\n    test_cases = [\n        [3, 1, 0, 3],\n        [4, 4, 4, 4, 4, 4, 4, 4],\n        [3, 3]\n    ]\n\n    # Pre-compute W.T @ E_k for all tokens k, as this is used repeatedly in the score calculation\n    wte_scores = E @ W\n\n    final_results = []\n\n    for initial_sequence in test_cases:\n        tokens = list(initial_sequence)\n        L = len(tokens)\n\n        # To avoid recomputing the sum of embeddings at each step, we maintain a running sum.\n        sum_E = np.sum([E[t] for t in tokens], axis=0)\n\n        # -- Calculate initial prediction --\n        h_initial = sum_E / L\n        z_initial = W.T @ h_initial + b\n        y_hat_initial = 1 if z_initial = 0 else 0\n\n        # -- Start the iterative attack --\n        while True:\n            # -- Calculate current prediction --\n            h_current = sum_E / L\n            z_current = W.T @ h_current + b\n            y_hat_current = 1 if z_current = 0 else 0\n\n            # -- Check for termination condition: prediction has flipped --\n            if y_hat_current != y_hat_initial:\n                final_results.append(tokens)\n                break\n\n            best_score = 0\n            candidate_flips = []\n\n            if y_hat_current == 1:\n                # Goal: make z negative. This requires minimizing W.T @ (E_b - E_a).\n                best_score = float('inf')\n                for j in range(L):\n                    a = tokens[j]\n                    for b_candidate in range(V):\n                        if b_candidate == a:\n                            continue\n                        \n                        score = wte_scores[b_candidate] - wte_scores[a]\n                        \n                        if score  best_score:\n                            best_score = score\n                            candidate_flips = [(j, b_candidate)]\n                        elif score == best_score:\n                            candidate_flips.append((j, b_candidate))\n            else:  # y_hat_current == 0\n                # Goal: make z positive. This requires maximizing W.T @ (E_b - E_a).\n                best_score = float('-inf')\n                for j in range(L):\n                    a = tokens[j]\n                    for b_candidate in range(V):\n                        if b_candidate == a:\n                            continue\n                            \n                        score = wte_scores[b_candidate] - wte_scores[a]\n                        \n                        if score  best_score:\n                            best_score = score\n                            candidate_flips = [(j, b_candidate)]\n                        elif score == best_score:\n                            candidate_flips.append((j, b_candidate))\n\n            # Apply tie-breaking rules to the list of best-scoring flips.\n            # 1. Sort by new token index `b` (ascending).\n            # 2. Sort by position index `j` (ascending).\n            candidate_flips.sort(key=lambda x: (x[1], x[0]))\n            best_j, best_b = candidate_flips[0]\n\n            # Apply the chosen flip and update the running sum of embeddings.\n            original_token = tokens[best_j]\n            tokens[best_j] = best_b\n            sum_E += (E[best_b] - E[original_token])\n\n    # Format the final output according to the problem specification.\n    output_parts = [f\"[{','.join(map(str, res))}]\" for res in final_results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3102527"}]}