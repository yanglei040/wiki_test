{"hands_on_practices": [{"introduction": "要真正理解一个机制，从最简单的特例入手往往很有帮助。这项练习将缩放点积注意力简化到一维场景，此时向量点积退化为简单的标量乘法。通过手动计算相似度分数、注意力权重和最终输出，你将对注意力机制的核心组成部分——计算相似度、归一化权重、加权求和——有一个具体而清晰的认识。[@problem_id:3172468]", "problem": "一个系统使用通常称为注意力的机制，根据一组标量值与一个标量查询的相似度来组合它们。在键维度等于1的边缘情况下，查询和键之间的相似度简化为它们的标量积。设查询为$q$，键为$k_1, k_2, k_3$，值为$v_1, v_2, v_3$，它们都是实数。假设在通过归一化指数映射转换相似度得分之前，对其应用了常规的按键维度的平方根进行的缩放。从标量的点积定义和归一化指数映射的定义出发，推导分配给每个值的权重，并使用它们计算以下数据的注意力操作的单个标量输出：\n$q = 2$, $k_1 = 0.1$, $k_2 = 0.2$, $k_3 = 0.5$, $v_1 = 1$, $v_2 = -1$, $v_3 = 3$，键维度$d_k = 1$。\n将最终数值结果四舍五入到四位有效数字，并表示为一个无单位的纯数。", "solution": "目标是在键维度等于1的情况下，从基本原理出发构建注意力输出。我们使用的基本依据是点积的定义和归一化指数映射的定义，后者通常被称为$\\mathrm{softmax}$函数。\n1. 来自标量点积和常规缩放的相容性得分。当键维度$d_k = 1$时，平方根缩放因子为$\\sqrt{d_k} = \\sqrt{1} = 1$。对于标量查询$q$和标量键$k_i$，相似度（相容性）得分为\n$$\ns_i = \\frac{q \\cdot k_i}{\\sqrt{d_k}} = q\\,k_i.\n$$\n2. 归一化指数映射（$\\mathrm{softmax}$）。给定一组得分$(s_1, s_2, s_3)$，通过对得分取指数并进行归一化，使得它们的和为1，从而获得注意力权重：\n$$\n\\alpha_i = \\frac{\\exp(s_i)}{\\sum_{j=1}^{3} \\exp(s_j)}.\n$$\n3. 作为值的凸组合的注意力输出。注意力输出$y$是值的加权和：\n$$\ny = \\sum_{i=1}^{3} \\alpha_i v_i.\n$$\n我们现在将这些步骤应用于给定的数值数据：$q = 2$，$k_1 = 0.1$，$k_2 = 0.2$，$k_3 = 0.5$，$v_1 = 1$，$v_2 = -1$，$v_3 = 3$以及$d_k = 1$。\n- 使用$s_i = q k_i$计算得分：\n$$\ns_1 = 2 \\cdot 0.1 = 0.2,\\quad s_2 = 2 \\cdot 0.2 = 0.4,\\quad s_3 = 2 \\cdot 0.5 = 1.0.\n$$\n- 对得分取指数：\n$$\n\\exp(s_1) = \\exp(0.2),\\quad \\exp(s_2) = \\exp(0.4),\\quad \\exp(s_3) = \\exp(1.0).\n$$\n在最后的数值计算步骤之前，保持这些符号形式。归一化常数为\n$$\nZ = \\exp(0.2) + \\exp(0.4) + \\exp(1.0).\n$$\n- 权重为\n$$\n\\alpha_1 = \\frac{\\exp(0.2)}{Z},\\quad \\alpha_2 = \\frac{\\exp(0.4)}{Z},\\quad \\alpha_3 = \\frac{\\exp(1.0)}{Z}.\n$$\n- 输出为\n$$\ny = \\alpha_1 \\cdot 1 + \\alpha_2 \\cdot (-1) + \\alpha_3 \\cdot 3 = \\frac{\\exp(0.2) - \\exp(0.4) + 3\\exp(1.0)}{Z}.\n$$\n代入指数的数值计算结果以产生最终的数字：\n$$\n\\exp(0.2) \\approx 1.221402758160170,\\quad \\exp(0.4) \\approx 1.491824697641270,\\quad \\exp(1.0) \\approx 2.718281828459045.\n$$\n那么\n$$\nZ \\approx 1.221402758160170 + 1.491824697641270 + 2.718281828459045 \\approx 5.431509284260485,\n$$\n以及\n$$\n\\text{numerator} \\approx 1.221402758160170 - 1.491824697641270 + 3 \\cdot 2.718281828459045 \\approx 7.884423545896035.\n$$\n因此\n$$\ny \\approx \\frac{7.884423545896035}{5.431509284260485} \\approx 1.45160822\\ldots\n$$\n四舍五入到四位有效数字，得到$1.452$。\n与标量相关性模型相关的解释：当$d_k = 1$时，相容性$s_i = q k_i$就是两个标量的乘积。对于一个正的查询$q$，更大（更正）的键$k_i$会产生更大的$s_i$，因此在经过归一化指数映射后会产生更大的权重。这种行为反映了一个简单的标量相关性模型，其中对齐由两个标量信号的乘积来捕捉，而$\\mathrm{softmax}$提供了一个归一化的正权重，它强调较高的相关性，同时保持了值的凸组合。", "answer": "$$\\boxed{1.452}$$", "id": "3172468"}, {"introduction": "理论的掌握固然重要，但在实际应用中我们常常需要做出权衡，尤其是在模型性能和计算成本之间。这项练习将介绍一种“基于置信度的剪枝”启发式策略，当模型对某个输出足够“自信”时，可以提前终止部分计算以提升效率。你将亲手实现这一启发式算法，并量化分析它对计算速度和模型精度的影响，从而对部署注意力模型时所面临的工程挑战建立直观感受。[@problem_id:3172461]", "problem": "考虑一个单头缩放点积注意力（SDPA）机制，该机制作用于维度为 $d$ 的实向量空间中的一组查询和键，以及相应的值向量。假设有 $n_q$ 个查询向量 $\\{q_i\\}_{i=1}^{n_q}$，$n_k$ 个键向量 $\\{k_j\\}_{j=1}^{n_k}$，以及 $n_k$ 个值向量 $\\{v_j\\}_{j=1}^{n_k}$，它们都属于 $\\mathbb{R}^d$。假设键是单位范数的。使用欧几里得内积和 softmax 函数的基本定义，SDPA 将每个查询 $q_i$ 映射到键上的注意力分布，然后映射到由值向量构建的输出向量。定义基于置信度的剪枝规则：如果查询 $q_i$ 的最大注意力概率超过阈值 $\\tau$（即，如果 $\\max_j \\alpha^{(i)}_j > \\tau$），则将该查询的 SDPA 输出近似为与最大化键相关联的单个值向量，并跳过下游的逐令牌（per-token）计算。否则，计算完整的注意力输出并保留下游计算。\n\n您必须编写一个程序，该程序：\n- 计算每个 $q_i$ 在不进行剪枝时的基线 SDPA 输出。\n- 对给定的 $\\tau$ 值，逐令牌应用基于置信度的剪枝规则，当规则触发时，用最大化的值向量替换输出，否则使用基线 SDPA 输出。\n- 将准确率损失量化为所有查询的剪枝输出与基线输出之间的平均欧几里得距离平方，表示为十进制数（无百分号）。\n- 根据以下成本模型，将加速比量化为基线总操作数与剪枝后总操作数的比率：\n    - 为一个查询计算所有缩放内积 $s_j$ 的成本为 $n_k \\cdot d$。\n    - 为一个查询应用 softmax 的成本为 $2 \\cdot n_k$。\n    - 为一个查询计算值的完整加权和的成本为 $n_k \\cdot d$。\n    - 剪枝时复制单个值向量的成本为 $d$。\n    - 当不发生剪枝时，会产生一个下游逐令牌成本 $C_{\\mathrm{tail}}$，而发生剪枝时则完全跳过此成本。\n  在此模型下，对于每个查询，基线成本为 $(n_k \\cdot d) + (2 \\cdot n_k) + (n_k \\cdot d) + C_{\\mathrm{tail}}$，而剪枝路径成本为 $(n_k \\cdot d) + (2 \\cdot n_k) + d$。\n\n对所有测试用例使用以下固定数据：\n- 维度 $d = 3$。\n- 查询数量 $n_q = 3$，键数量 $n_k = 3$。\n- 键（已是单位范数）：$k_1 = [1, 0, 0]$, $k_2 = [0, 1, 0]$, $k_3 = [0, 0, 1]$。\n- 值：$v_1 = [1, 0, 0]$, $v_2 = [0, 1, 0]$, $v_3 = [0, 0, 1]$。\n- 查询：$q_1 = [2.0, -2.0, -2.0]$, $q_2 = [0.0, 1.8, -1.8]$, $q_3 = [-1.8, 0.0, 1.8]$。\n\n为了 softmax 的稳定性，在进行指数运算之前，从 logits 中减去 $\\max_j s_j$；这可以在不改变概率的情况下稳定计算。\n\n测试套件：\n- 情况 1：阈值 $\\tau = 0.7$，下游成本 $C_{\\mathrm{tail}} = 500$。\n- 情况 2：阈值 $\\tau = 0.95$，下游成本 $C_{\\mathrm{tail}} = 500$。\n- 情况 3：阈值 $\\tau = 0.0$，下游成本 $C_{\\mathrm{tail}} = 500$。\n- 情况 4：阈值 $\\tau = 1.0$，下游成本 $C_{\\mathrm{tail}} = 500$。\n- 情况 5：阈值 $\\tau = 0.6$，下游成本 $C_{\\mathrm{tail}} = 50$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身是一个双元素列表 $[S, L]$，其中 $S$ 是加速比（十进制浮点数），$L$ 是准确率损失（十进制浮点数）。例如：$[[S_1, L_1],[S_2, L_2],[S_3, L_3],[S_4, L_4],[S_5, L_5]]$。", "solution": "该问题要求对缩放点积注意力（SDPA）机制的一种基于置信度的剪枝启发式方法进行分析。我们将首先建立 SDPA 的基线计算，然后应用指定的剪枝规则，最后对几个测试用例的计算加速和准确率损失之间的权衡进行量化。\n\n问题提供了以下数据：\n- 向量空间的维度是 $d=3$。\n- 查询向量的数量是 $n_q=3$。\n- 键向量的数量是 $n_k=3$。\n- 查询矩阵 $Q \\in \\mathbb{R}^{n_q \\times d}$ 由下式给出：\n$$\nQ = \\begin{pmatrix} 2.0 & -2.0 & -2.0 \\\\ 0.0 & 1.8 & -1.8 \\\\ -1.8 & 0.0 & 1.8 \\end{pmatrix}\n$$\n- 键矩阵 $K \\in \\mathbb{R}^{n_k \\times d}$ 由下式给出：\n$$\nK = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n- 值矩阵 $V \\in \\mathbb{R}^{n_k \\times d}$ 由下式给出：\n$$\nV = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\n需要注意的是，键向量（$K$ 的行）是单位范数的。\n\nSDPA 的核心操作定义为：\n$$\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\n$$\n\n让我们对单个查询向量 $q_i$（$Q$ 的第 $i$ 行）的计算过程进行剖析。\n\n1.  **缩放点积得分**：对于每个查询 $q_i$，我们计算它与每个键向量 $k_j$（$K$ 的第 $j$ 行）的点积，并按维度 $d$ 的平方根的倒数进行缩放。这些是注意力得分或 logits，$s^{(i)}_j$。\n    $$\n    s^{(i)}_j = \\frac{q_i \\cdot k_j}{\\sqrt{d}}\n    $$\n    在矩阵形式中，我们计算得分矩阵 $M \\in \\mathbb{R}^{n_q \\times n_k}$ 为 $M = \\frac{QK^T}{\\sqrt{d}}$。当 $d=3$ 时，我们有：\n    $$\n    M = \\frac{1}{\\sqrt{3}} QK^T = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 2.0 & -2.0 & -2.0 \\\\ 0.0 & 1.8 & -1.8 \\\\ -1.8 & 0.0 & 1.8 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}^T \\approx \\begin{pmatrix} 1.1547 & -1.1547 & -1.1547 \\\\ 0.0 & 1.0392 & -1.0392 \\\\ -1.0392 & 0.0 & 1.0392 \\end{pmatrix}\n    $$\n\n2.  **注意力概率**：使用 softmax 函数将得分转换为概率 $\\alpha^{(i)}_j$，该函数逐行应用于矩阵 $M$。为了数值稳定性，我们在进行指数运算前，从每行的得分中减去该行的最大值。对于第 $i$ 个查询：\n    $$\n    \\alpha^{(i)}_j = \\frac{\\exp(s^{(i)}_j - \\max_l s^{(i)}_l)}{\\sum_{l=1}^{n_k} \\exp(s^{(i)}_l - \\max_l s^{(i)}_l)}\n    $$\n    令 $A \\in \\mathbb{R}^{n_q \\times n_k}$ 为注意力概率矩阵。其行 $A_{i,:}$ 是概率分布 $\\{\\alpha^{(i)}_j\\}_{j=1}^{n_k}$。\n    将此应用于 $M$，我们得到注意力矩阵 $A$：\n    $$\n    A \\approx \\begin{pmatrix} 0.8343 & 0.0828 & 0.0828 \\\\ 0.2392 & 0.6762 & 0.0846 \\\\ 0.0846 & 0.2392 & 0.6762 \\end{pmatrix}\n    $$\n\n3.  **基线输出向量**：查询 $q_i$ 的输出向量 $o_i$ 是值向量的加权和，使用注意力概率作为权重。\n    $$\n    o_i = \\sum_{j=1}^{n_k} \\alpha^{(i)}_j v_j\n    $$\n    在矩阵形式中，基线输出矩阵 $O \\in \\mathbb{R}^{n_q \\times d}$ 是 $O = AV$。由于在此问题中 $V$ 是单位矩阵，基线输出矩阵就是注意力矩阵，$O = A$。\n\n4.  **基于置信度的剪枝**：剪枝规则应用于每个查询 $q_i$。\n    - 令 $j^* = \\arg\\max_j \\alpha^{(i)}_j$ 为具有最高注意力概率的键的索引。\n    - 如果 $\\alpha^{(i)}_{j^*} > \\tau$，则触发规则。剪枝后的输出 $o'_i$ 近似为与最受关注的键对应的单个值向量：$o'_i = v_{j^*}$。\n    - 否则，使用完整的注意力输出：$o'_i = o_i$。\n\n5.  **指标计算**：\n    - **准确率损失 ($L$)**：准确率损失是基线输出与剪枝后输出之间平均欧几里得距离的平方：\n      $$\n      L = \\frac{1}{n_q} \\sum_{i=1}^{n_q} \\|o_i - o'_i\\|^2\n      $$\n    - **加速比 ($S$)**：加速比是总基线成本与总剪枝路径成本的比率。每个查询的成本定义如下：\n      - 基线成本：$C_{\\text{base}} = 2n_k d + 2n_k + C_{\\text{tail}} = 2(3)(3) + 2(3) + C_{\\text{tail}} = 24 + C_{\\text{tail}}$。\n      - 剪枝路径成本：$C_{\\text{pruned}} = n_k d + 2n_k + d = (3)(3) + 2(3) + 3 = 18$。\n      - 令 $P$ 为被剪枝查询的索引集合。总剪枝成本为 $\\sum_{i \\in P} C_{\\text{pruned}} + \\sum_{i \\notin P} C_{\\text{base}}$。\n      - 加速比为 $S = \\frac{n_q \\cdot C_{\\text{base}}}{\\sum_{i \\in P} C_{\\text{pruned}} + \\sum_{i \\notin P} C_{\\text{base}}}$。\n\n我们现在将此过程应用于每个测试用例。\n\n**通用计算：**\n每个查询的最大注意力概率为：\n- 对于 $q_1$：$\\max_j \\alpha^{(1)}_j \\approx 0.8343$。\n- 对于 $q_2$：$\\max_j \\alpha^{(2)}_j \\approx 0.6762$。\n- 对于 $q_3$：$\\max_j \\alpha^{(3)}_j \\approx 0.6762$。\n\n**情况 1：$\\tau = 0.7$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 24 + 500 = 524$。$C_{\\text{pruned}} = 18$。\n- 对于 $q_1$：$0.8343 > 0.7$，因此我们进行剪枝。$j^*=1$。$o'_1 = v_1 = [1,0,0]$。成本为 $18$。\n- 对于 $q_2$：$0.6762 \\le 0.7$，不剪枝。$o'_2 = o_2$。成本为 $524$。\n- 对于 $q_3$：$0.6762 \\le 0.7$，不剪枝。$o'_3 = o_3$。成本为 $524$。\n- 总剪枝成本 = $18 + 524 + 524 = 1066$。\n- 总基线成本 = $3 \\times 524 = 1572$。\n- $S_1 = 1572 / 1066 \\approx 1.47467$。\n- 损失：$L_1 = \\frac{1}{3} (\\|o_1 - v_1\\|^2 + \\|o_2 - o_2\\|^2 + \\|o_3 - o_3\\|^2) = \\frac{1}{3} \\|o_1 - v_1\\|^2 \\approx \\frac{1}{3} (0.04118) \\approx 0.01373$。\n\n**情况 2：$\\tau = 0.95$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$。\n- 对于所有查询，$\\max_j \\alpha^{(i)}_j \\le 0.95$。没有查询被剪枝。\n- 总剪枝成本 = $3 \\times 524 = 1572$。\n- 总基线成本 = $1572$。\n- $S_2 = 1572 / 1572 = 1.0$。\n- 损失：没有发生剪枝，因此对所有 $i$ 都有 $o'_i = o_i$。$L_2 = 0.0$。\n\n**情况 3：$\\tau = 0.0$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$，$C_{\\text{pruned}} = 18$。\n- 对于所有查询，$\\max_j \\alpha^{(i)}_j > 0.0$。所有查询都被剪枝。\n- $q_1$ 的 $j^*$ 是 $1$；$q_2$ 的 $j^*$ 是 $2$；$q_3$ 的 $j^*$ 是 $3$。$o'_1=v_1, o'_2=v_2, o'_3=v_3$。\n- 总剪枝成本 = $3 \\times 18 = 54$。\n- 总基线成本 = $1572$。\n- $S_3 = 1572 / 54 \\approx 29.11111$。\n- 损失：$L_3 = \\frac{1}{3} (\\|o_1 - v_1\\|^2 + \\|o_2 - v_2\\|^2 + \\|o_3 - v_3\\|^2) \\approx \\frac{1}{3}(0.04118 + 0.16922 + 0.16922) \\approx 0.12654$。\n\n**情况 4：$\\tau = 1.0$, $C_{\\text{tail}} = 500$**\n- $C_{\\text{base}} = 524$。\n- 因为 $\\sum_j \\alpha^{(i)}_j = 1$，所以 $\\max_j \\alpha^{(i)}_j \\le 1$。规则 $\\max > 1.0$ 永远不会触发。没有查询被剪枝。\n- 结果与情况 2 相同：$S_4 = 1.0$，$L_4 = 0.0$。\n\n**情况 5：$\\tau = 0.6$, $C_{\\text{tail}} = 50$**\n- $C_{\\text{base}} = 24 + 50 = 74$。$C_{\\text{pruned}} = 18$。\n- 对于所有查询，$\\max_j \\alpha^{(i)}_j > 0.6$。所有查询都被剪枝。\n- 总剪枝成本 = $3 \\times 18 = 54$。\n- 总基线成本 = $3 \\times 74 = 222$。\n- $S_5 = 222 / 54 \\approx 4.11111$。\n- 损失：损失计算与 $C_{\\text{tail}}$ 无关。所有查询都被剪枝，类似于情况 3。$L_5 = L_3 \\approx 0.12654$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes speedup and accuracy loss for a certainty-based pruning heuristic\n    in Scaled Dot-Product Attention.\n    \"\"\"\n    \n    # Define fixed data from the problem statement.\n    d = 3.0\n    nq = 3\n    nk = 3\n    \n    Q = np.array([\n        [2.0, -2.0, -2.0],\n        [0.0, 1.8, -1.8],\n        [-1.8, 0.0, 1.8]\n    ])\n    \n    K = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n    \n    V = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    # Step 1: Compute baseline SDPA output.\n    # Scaled dot-product scores\n    sqrt_d = np.sqrt(d)\n    scaled_scores = (Q @ K.T) / sqrt_d\n    \n    # Numerically stable softmax to get attention probabilities\n    max_scores = scaled_scores.max(axis=1, keepdims=True)\n    exp_scores = np.exp(scaled_scores - max_scores)\n    attention_probs = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n    \n    # Baseline output vectors\n    baseline_outputs = attention_probs @ V\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.7, 500),  # Case 1\n        (0.95, 500), # Case 2\n        (0.0, 500),  # Case 3\n        (1.0, 500),  # Case 4\n        (0.6, 50),   # Case 5\n    ]\n    \n    results = []\n    \n    for tau, C_tail in test_cases:\n        # Define costs based on the model\n        cost_base = 2 * nk * d + 2 * nk + C_tail\n        cost_pruned = nk * d + 2 * nk + d\n        \n        total_pruned_cost = 0.0\n        total_squared_error = 0.0\n        \n        pruned_outputs = np.zeros_like(baseline_outputs)\n        \n        for i in range(nq):\n            probs_i = attention_probs[i]\n            max_prob = np.max(probs_i)\n            \n            is_pruned = max_prob > tau\n            \n            baseline_output_i = baseline_outputs[i]\n            \n            if is_pruned:\n                # Pruning rule triggers\n                max_idx = np.argmax(probs_i)\n                pruned_output_i = V[max_idx]\n                total_pruned_cost += cost_pruned\n            else:\n                # Full computation\n                pruned_output_i = baseline_output_i\n                total_pruned_cost += cost_base\n                \n            # Calculate squared Euclidean distance for this query\n            squared_error_i = np.sum((baseline_output_i - pruned_output_i)**2)\n            total_squared_error += squared_error_i\n            \n        # Calculate final metrics for the test case\n        # Accuracy Loss\n        accuracy_loss = total_squared_error / nq\n        \n        # Speedup\n        total_baseline_cost = nq * cost_base\n        speedup = total_baseline_cost / total_pruned_cost if total_pruned_cost > 0 else float('inf')\n        \n        results.append([speedup, accuracy_loss])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{s},{l}]\" for s, l in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3172461"}, {"introduction": "注意力机制作为神经网络的一部分，必须是可训练的，这意味着我们需要理解梯度是如何通过其内部运算进行反向传播的。这项高级练习将引导你从第一性原理出发，推导并实现注意力机制关于其键（key）和值（value）输入的梯度。通过手动编写反向传播的代码，你将深刻理解注意力机制如何从数据中学习，这对于构建和调试深度学习模型至关重要。[@problem_id:3172478]", "problem": "给定一个在因果掩码下的单头缩放点积注意力机制，要求你分析在长序列中，流向远处词元的梯度作为位置的函数。请从以下定义出发，并根据第一性原理计算所有派生量。\n\n设序列长度为 $L$，嵌入维度为 $d$。设查询矩阵为 $Q \\in \\mathbb{R}^{L \\times d}$，键矩阵为 $K \\in \\mathbb{R}^{L \\times d}$，值矩阵为 $V \\in \\mathbb{R}^{L \\times d}$。对于位置索引 $t \\in \\{0,1,\\dots,L-1\\}$，因果掩码只允许满足 $j \\le t$ 的索引 $j$。定义分数向量 $s \\in \\mathbb{R}^{t+1}$ 为 $s_j = \\frac{1}{\\sqrt{d}} \\, Q_t^\\top K_j$，并对允许的分数应用Softmax函数（SMF）来定义注意力权重向量 $a \\in \\mathbb{R}^{t+1}$：$a = \\operatorname{softmax}(s)$，其中 $\\operatorname{softmax}(s)_j = \\frac{e^{s_j}}{\\sum_{m=0}^{t} e^{s_m}}$。位置 $t$ 的注意力输出为 $o_t \\in \\mathbb{R}^{d}$，由 $o_t = \\sum_{j=0}^{t} a_j V_j$ 给出。\n\n考虑位置 $t$ 的标量损失 $L_{\\text{loss}}$，定义为 $L_{\\text{loss}} = \\frac{1}{2} \\lVert o_t - y_t \\rVert_2^2$，其中 $y_t \\in \\mathbb{R}^{d}$ 是一个给定的目标向量。仅使用上述定义和标准的多元微积分（链式法则和向量-雅可比积），推导并实现对于所有允许的索引 $j \\le t$ 处的值和键的梯度的表达式。具体来说，计算 $g^{(V)}_j = \\left\\lVert \\frac{\\partial L_{\\text{loss}}}{\\partial V_j} \\right\\rVert_2$ 和 $g^{(K)}_j = \\left\\lVert \\frac{\\partial L_{\\text{loss}}}{\\partial K_j} \\right\\rVert_2$ 对于所有 $j \\in \\{0,1,\\dots,t\\}$，其中 $\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。\n\n你的程序必须为以下测试套件计算这些梯度范数，使用指定的精确随机种子和参数，并且必须通过在指数化之前减去最大分数来实现数值稳定的Softmax。对于每个测试用例，$V$ 和 $y_t$ 应具有相同的维度 $d$。\n\n测试套件：\n- 案例1：$L=8, d=4$，随机正态分布的 $Q, K, V$ (种子为 $42$)，位置 $t=6$，目标 $y_t=\\mathbf{0} \\in \\mathbb{R}^4$。\n- 案例2：$L=8, d=4$，随机正态分布的 $Q, K, V$ (种子为 $7$)，位置 $t=0$，目标 $y_t=\\mathbf{0} \\in \\mathbb{R}^4$。\n- 案例3：$L=10, d=5$，$Q=\\mathbf{0}$ (所有条目为零)，随机正态分布的 $K, V$ (种子为 $0$)，位置 $t=9$，目标 $y_t=\\mathbf{0} \\in \\mathbb{R}^5$。\n- 案例4：$L=16, d=64$，随机正态分布的 $Q, K, V$ (种子为 $123$)，位置 $t=15$，目标 $y_t=\\mathbf{0} \\in \\mathbb{R}^{64}$。\n\n实现要求：\n- 严格使用因果掩码，即只有索引 $j \\le t$ 对 $o_t$ 有贡献并接收非零注意力权重。\n- 使用数值稳定的Softmax：给定一个分数向量 $s$，计算 $\\tilde{s}_j = s_j - \\max_{m \\le t} s_m$，然后 $a_j = \\frac{e^{\\tilde{s}_j}}{\\sum_{m \\le t} e^{\\tilde{s}_m}}$。\n- 使用链式法则从上述定义中推导梯度，不调用自动微分或外部深度学习库。\n\n最终输出格式：\n- 对于每个测试用例，输出一个列表对 $[G_V,G_K]$，其中 $G_V$ 是列表 $[g^{(V)}_0,g^{(V)}_1,\\dots,g^{(V)}_t]$，$G_K$ 是列表 $[g^{(K)}_0,g^{(K)}_1,\\dots,g^{(K)}_t]$。\n- 按案例顺序将四个案例的结果聚合成一个单一列表，形成一个形如 $[[G_V^{(1)},G_K^{(1)}],[G_V^{(2)},G_K^{(2)}],[G_V^{(3)},G_K^{(3)}],[G_V^{(4)},G_K^{(4)}]]$ 的嵌套列表。\n- 你的程序应生成一行输出，其中包含此嵌套列表，作为不含任何空格的逗号分隔结构（例如，`[[1.0,2.0],[3.0,4.0]]` 是可接受的，且必须不含空格）。", "solution": "该问题要求在单头因果缩放点积注意力机制中，推导并实现值向量和键向量的梯度范数。我们将使用多元微积分的链式法则从第一性原理出发推导梯度。\n\n设序列长度为 $L$，嵌入维度为 $d$，感兴趣的位置为 $t \\in \\{0, 1, \\dots, L-1\\}$。相关矩阵为查询矩阵 $Q \\in \\mathbb{R}^{L \\times d}$、键矩阵 $K \\in \\mathbb{R}^{L \\times d}$ 和值矩阵 $V \\in \\mathbb{R}^{L \\times d}$。位置 $t$ 的因果注意力机制只考虑来自位置 $j \\le t$ 的输入。\n\n定义如下：\n- 分数向量 $s \\in \\mathbb{R}^{t+1}$：对于 $j \\in \\{0, \\dots, t\\}$，$s_j = \\frac{1}{\\sqrt{d}} Q_t^\\top K_j$。\n- 注意力权重向量 $a \\in \\mathbb{R}^{t+1}$：$a = \\operatorname{softmax}(s)$。\n- 注意力输出向量 $o_t \\in \\mathbb{R}^{d}$：$o_t = \\sum_{j=0}^{t} a_j V_j$。\n- 标量损失 $L_{\\text{loss}}$：$L_{\\text{loss}} = \\frac{1}{2} \\lVert o_t - y_t \\rVert_2^2$，其中 $y_t \\in \\mathbb{R}^{d}$ 是一个目标向量。\n\n我们的目标是找到对于所有 $j \\in \\{0, \\dots, t\\}$，关于 $V_j$ 和 $K_j$ 的梯度范数的表达式。这些是 $g^{(V)}_j = \\left\\lVert \\frac{\\partial L_{\\text{loss}}}{\\partial V_j} \\right\\rVert_2$ 和 $g^{(K)}_j = \\left\\lVert \\frac{\\partial L_{\\text{loss}}}{\\partial K_j} \\right\\rVert_2$。\n\n首先，我们定义损失关于输出向量 $o_t$ 的梯度。对二次型使用向量微积分的标准法则：\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial o_t} = (o_t - y_t)\n$$\n为简洁起见，我们将此梯度向量表示为 $\\delta_o = o_t - y_t \\in \\mathbb{R}^d$。\n\n**1. 关于值向量的梯度 ($\\frac{\\partial L_{\\text{loss}}}{\\partial V_j}$)**\n\n输出 $o_t$ 是值向量 $V_j$（对于 $j \\in \\{0, \\dots, t\\}$）的线性组合。梯度 $\\frac{\\partial L_{\\text{loss}}}{\\partial V_j}$ 可以使用链式法则找到：\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial V_j} = \\left(\\frac{\\partial o_t}{\\partial V_j}\\right)^\\top \\frac{\\partial L_{\\text{loss}}}{\\partial o_t}\n$$\n需要雅可比矩阵 $\\frac{\\partial o_t}{\\partial V_j}$。注意力权重 $a_k$ 不依赖于任何 $V_j$。\n$$\no_t = \\sum_{k=0}^{t} a_k V_k\n$$\n$o_t$ 的第 $i$ 个分量对 $V_j$ 的第 $l$ 个分量的导数是：\n$$\n\\frac{\\partial (o_t)_i}{\\partial (V_j)_l} = \\frac{\\partial}{\\partial (V_j)_l} \\left( \\sum_{k=0}^{t} a_k (V_k)_i \\right) = a_j \\delta_{ij} \\delta_{il}\n$$\n这个表达式可以简化。正确地说，$o_t$ 向量对 $V_j$ 向量的导数是一个矩阵，其 $(i,l)$ 项为 $\\frac{\\partial (o_t)_i}{\\partial (V_j)_l} = a_j \\delta_{il}$。这意味着雅可比矩阵是 $\\frac{\\partial o_t}{\\partial V_j} = a_j I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n将此代入链式法则表达式：\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial V_j} = (a_j I_d)^\\top \\delta_o = a_j \\delta_o = a_j (o_t - y_t)\n$$\n关于 $V_j$ 的梯度是由注意力权重 $a_j$ 缩放的上游梯度 $\\delta_o$。其范数为：\n$$\ng^{(V)}_j = \\left\\lVert a_j (o_t - y_t) \\right\\rVert_2 = |a_j| \\lVert o_t - y_t \\rVert_2\n$$\n由于 $a_j \\ge 0$，这简化为 $g^{(V)}_j = a_j \\lVert o_t - y_t \\rVert_2$。\n\n**2. 关于键向量的梯度 ($\\frac{\\partial L_{\\text{loss}}}{\\partial K_j}$)**\n\n这个推导更复杂，因为 $K_j$ 通过 softmax 函数影响所有的注意力权重 $a_k$。依赖链是 $L_{\\text{loss}} \\rightarrow o_t \\rightarrow a \\rightarrow s \\rightarrow K$。\n\n首先，我们求损失关于注意力权重 $a_k$ 的梯度。\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial a_k} = \\left(\\frac{\\partial o_t}{\\partial a_k}\\right)^\\top \\frac{\\partial L_{\\text{loss}}}{\\partial o_t} = V_k^\\top \\delta_o\n$$\n这是一个标量值，表示 $V_k$ 和 $\\delta_o$ 的点积。\n\n接下来，我们求关于分数 $s_m$ 的梯度。这涉及到softmax函数的雅可比矩阵 $\\frac{\\partial a_k}{\\partial s_m}$。\n$$\n\\frac{\\partial a_k}{\\partial s_m} = a_k (\\delta_{km} - a_m)\n$$\n其中 $\\delta_{km}$ 是克罗内克δ。\n\n使用链式法则，关于分数 $s_m$ 的梯度是：\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial s_m} = \\sum_{k=0}^{t} \\frac{\\partial L_{\\text{loss}}}{\\partial a_k} \\frac{\\partial a_k}{\\partial s_m} = \\sum_{k=0}^{t} (V_k^\\top \\delta_o) \\cdot (a_k (\\delta_{km} - a_m))\n$$\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial s_m} = (V_m^\\top \\delta_o) a_m(1-a_m) + \\sum_{k \\neq m} (V_k^\\top \\delta_o)(-a_k a_m)\n$$\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial s_m} = a_m (V_m^\\top \\delta_o) - a_m \\sum_{k=0}^{t} a_k (V_k^\\top \\delta_o)\n$$\n这个和可以表示为 $\\left(\\sum_{k=0}^{t} a_k V_k\\right)^\\top \\delta_o = o_t^\\top \\delta_o$。\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial s_m} = a_m (V_m^\\top \\delta_o - o_t^\\top \\delta_o) = a_m (V_m - o_t)^\\top \\delta_o\n$$\n我们将这个标量梯度表示为 $\\delta_{s_m}$。\n\n最后，我们将梯度反向传播到键向量 $K_j$。一个键向量 $K_j$ 只影响其对应的分数 $s_j$。\n$$\ns_j = \\frac{1}{\\sqrt{d}} Q_t^\\top K_j\n$$\n标量 $s_j$ 关于向量 $K_j$ 的梯度是：\n$$\n\\frac{\\partial s_j}{\\partial K_j} = \\frac{1}{\\sqrt{d}} Q_t\n$$\n对最后一步应用链式法则：\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial K_j} = \\frac{\\partial L_{\\text{loss}}}{\\partial s_j} \\frac{\\partial s_j}{\\partial K_j} = \\delta_{s_j} \\cdot \\frac{1}{\\sqrt{d}} Q_t\n$$\n代入 $\\delta_{s_j}$ 的表达式：\n$$\n\\frac{\\partial L_{\\text{loss}}}{\\partial K_j} = \\left[ a_j (V_j - o_t)^\\top (o_t - y_t) \\right] \\frac{1}{\\sqrt{d}} Q_t\n$$\n方括号中的项是一个标量。我们称之为 $c_j$。梯度是 $\\mathbb{R}^d$ 中的一个向量，与 $Q_t$ 成正比。其范数为：\n$$\ng^{(K)}_j = \\left\\lVert \\frac{c_j}{\\sqrt{d}} Q_t \\right\\rVert_2 = \\frac{|c_j|}{\\sqrt{d}} \\lVert Q_t \\rVert_2\n$$\n\n**算法摘要**\n\n1.  **前向传播**：对于给定的位置 $t$，计算分数 $s_j$、注意力权重 $a_j$（使用数值稳定的softmax）和输出向量 $o_t$。\n2.  **反向传播**：\n    a. 计算初始梯度 $\\delta_o = o_t - y_t$。\n    b. 对于 $j = 0, \\dots, t$，计算值梯度范数：$g^{(V)}_j = a_j \\lVert \\delta_o \\rVert_2$。\n    c. 对于 $j = 0, \\dots, t$，计算键梯度范数：\n        i. 计算标量 $c_j = a_j ((V_j - o_t)^\\top \\delta_o)$。\n        ii. 计算 $g^{(K)}_j = \\frac{|c_j|}{\\sqrt{d}} \\lVert Q_t \\rVert_2$。\n3.  **收集和格式化**：按每个测试用例的要求，收集并格式化梯度范数列表。", "answer": "```python\nimport numpy as np\n\ndef calculate_gradients(Q_mat, K_mat, V_mat, y_t, t, d):\n    \"\"\"\n    Computes the gradients for scaled dot-product attention from first principles.\n\n    Args:\n        Q_mat (np.ndarray): Query matrix of shape (L, d).\n        K_mat (np.ndarray): Key matrix of shape (L, d).\n        V_mat (np.ndarray): Value matrix of shape (L, d).\n        y_t (np.ndarray): Target vector of shape (d,).\n        t (int): The current position in the sequence.\n        d (int): The embedding dimension.\n\n    Returns:\n        list: A list containing two lists: [G_V, G_K].\n              G_V is the list of value gradient norms.\n              G_K is the list of key gradient norms.\n    \"\"\"\n    # Extract query vector for the current time step\n    q_t = Q_mat[t, :]\n    \n    # Select relevant keys and values up to position t (causal mask)\n    K_le_t = K_mat[:t + 1, :]\n    V_le_t = V_mat[:t + 1, :]\n\n    # === Forward Pass ===\n    \n    # 1. Compute scores\n    scores = (K_le_t @ q_t) / np.sqrt(d)\n\n    # 2. Numerically stable softmax to get attention weights\n    if scores.size > 0:\n        stable_scores = scores - np.max(scores)\n        exp_scores = np.exp(stable_scores)\n        sum_exp_scores = np.sum(exp_scores)\n        attention_weights = exp_scores / sum_exp_scores if sum_exp_scores > 0 else np.zeros_like(exp_scores)\n    else: # Should not be hit given constraints t >= 0\n        attention_weights = np.array([])\n    \n    # 3. Compute attention output vector o_t\n    o_t = np.sum(attention_weights[:, np.newaxis] * V_le_t, axis=0)\n\n    # === Backward Pass ===\n\n    # 1. Gradient of the loss with respect to the output o_t\n    delta_o = o_t - y_t\n    norm_delta_o = np.linalg.norm(delta_o)\n\n    # 2. Compute gradient norms for Value vectors (G_V)\n    # g_v_j = || a_j * delta_o || = a_j * ||delta_o||\n    G_V = (attention_weights * norm_delta_o).tolist()\n\n    # 3. Compute gradient norms for Key vectors (G_K)\n    G_K = []\n    norm_q_t = np.linalg.norm(q_t)\n    o_t_dot_delta_o = o_t @ delta_o\n\n    for j in range(t + 1):\n        V_j = V_le_t[j, :]\n        v_j_dot_delta_o = V_j @ delta_o\n        \n        # Scalar coefficient c_j = a_j * ((V_j - o_t)^T * delta_o)\n        c_j = attention_weights[j] * (v_j_dot_delta_o - o_t_dot_delta_o)\n        \n        # g_k_j = (|c_j| / sqrt(d)) * ||q_t||\n        g_k_j = (np.abs(c_j) / np.sqrt(d)) * norm_q_t\n        G_K.append(g_k_j)\n\n    return [G_V, G_K]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {'L': 8, 'd': 4, 'seed': 42, 't': 6, 'Q_is_zero': False},\n        {'L': 8, 'd': 4, 'seed': 7, 't': 0, 'Q_is_zero': False},\n        {'L': 10, 'd': 5, 'seed': 0, 't': 9, 'Q_is_zero': True},\n        {'L': 16, 'd': 64, 'seed': 123, 't': 15, 'Q_is_zero': False},\n    ]\n\n    results = []\n    for params in test_cases:\n        L, d, seed, t, Q_is_zero = params['L'], params['d'], params['seed'], params['t'], params['Q_is_zero']\n        \n        rng = np.random.default_rng(seed)\n        \n        if Q_is_zero:\n            Q_mat = np.zeros((L, d), dtype=np.float64)\n        else:\n            Q_mat = rng.standard_normal((L, d), dtype=np.float64)\n        \n        K_mat = rng.standard_normal((L, d), dtype=np.float64)\n        V_mat = rng.standard_normal((L, d), dtype=np.float64)\n        y_t = np.zeros(d, dtype=np.float64)\n        \n        case_result = calculate_gradients(Q_mat, K_mat, V_mat, y_t, t, d)\n        results.append(case_result)\n\n    # Format the final output string to remove all whitespace\n    # Example: [[1,2],[3,4]] becomes \"[[1,2],[3,4]]\"\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n\n```", "id": "3172478"}]}