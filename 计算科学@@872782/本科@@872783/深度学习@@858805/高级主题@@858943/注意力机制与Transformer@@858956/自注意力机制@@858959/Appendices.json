{"hands_on_practices": [{"introduction": "自注意力机制的核心是 `softmax` 函数，它将注意力得分转换为概率分布。然而，直接根据其数学定义进行计算很容易遇到数值陷阱，例如因数值范围限制导致的上溢（overflow）和下溢（underflow）。本练习将指导你实现“log-sum-exp”技巧，这是一种确保 `softmax` 计算稳定性的标准技术 [@problem_id:3192585]。通过比较不同浮点精度下的数值误差，你将深刻理解为何在构建稳健的深度学习模型时，数值稳定性是一个必须优先考虑的关键问题。", "problem": "考虑一个自注意力评分设置，其中为由 $i$ 索引的查询（queries）和由 $j$ 索引的键（keys）计算一组能量值 $e_{ij}$，并通过对能量值逐行应用 softmax 函数来获得注意力权重。对于行向量 $\\mathbf{e}_i$，softmax 函数的基本定义为：\n$$ \\mathrm{softmax}(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})} $$\n其中 $\\exp$ 表示指数函数，$\\sum$ 表示有限求和。$\\log \\sum_{j} \\exp(e_{ij})$ 的计算是 softmax 数值分析的核心，因为 $\\log$ 是 $\\exp$ 的反函数，并将乘积转换为和。电气和电子工程师协会 (IEEE) 754 标准的浮点格式 `fp16` (binary 16) 和 `fp32` (binary 32) 在精度和动态范围上有所不同，这在计算 $\\exp$、$\\log$ 和 `softmax` 时会影响数值稳定性和舍入误差。\n\n仅从指数函数 $\\exp$、自然对数 $\\log$ 和 softmax 函数 `softmax` 的定义出发，为每一行 $i$ 推导一个计算 $\\log \\sum_{j} \\exp(e_{ij})$ 的数值稳定变换，并解释为什么该稳定变换在不改变数学值的情况下能减少上溢和下溢。然后，利用该稳定变换，设计一个算法，以指定的浮点精度逐行计算 $\\mathrm{softmax}(\\mathbf{e}_i)$，同时减轻灾难性的数值问题。\n\n实现该算法，通过将 `fp16` 和 `fp32` 稳定化 softmax 的输出与高精度 `fp64` (binary 64) 稳定化参考值进行比较，来量化不同浮点精度下的数值误差。\n\n对于下述每个测试用例，生成以下四个量：\n- `fp16` 稳定化 softmax 与 `fp64` 稳定化参考值在测试矩阵所有条目上的最大绝对差（一个实数）。\n- `fp32` 稳定化 softmax 与 `fp64` 稳定化参考值在测试矩阵所有条目上的最大绝对差（一个实数）。\n- 在 `fp64` 中，朴素直接计算 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ 与相同数量的稳定化计算之间的逐行最大绝对差（一个实数；如果某行的朴素值为非有限值，则其对该最大值的贡献视为 `nan`）。\n- 一个布尔值，指示是否有任何行的朴素计算 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ 在 `fp64` 中为非有限值（即，上溢到 $+\\infty$ 或传播了 `nan`）。\n\n使用以下能量矩阵测试套件，每个矩阵都被理解为 softmax 的逐行输入 $\\mathbf{e}_i$。所有条目均为无单位实数值。在计算中，您必须独立处理每个矩阵行。这些矩阵是：\n- 情况 $1$ (中等值): $$E^{(1)} = \\begin{bmatrix} -1  & 0  & 1  & 2 \\\\ 0.5  & -0.5  & 3  & -3 \\end{bmatrix}.$$\n- 情况 $2$ (极端范围): $$E^{(2)} = \\begin{bmatrix} 1000  & -1000  & 0  & 1 \\\\ 88  & 87  & 86  & 85 \\end{bmatrix}.$$\n- 情况 $3$ (均匀输入): $$E^{(3)} = \\begin{bmatrix} 0  & 0  & 0  & 0 \\\\ 0  & 0  & 0  & 0 \\end{bmatrix}.$$\n- 情况 $4$ (极大负值): $$E^{(4)} = \\begin{bmatrix} -1000  & -1001  & -999  & -1200 \\\\ -50  & -60  & -70  & -80 \\end{bmatrix}.$$\n\n算法要求：\n- 对每个矩阵 $E^{(k)}$，根据 `softmax` 的定义和您推导出的稳定变换，逐行计算一个稳定化的 `fp64` 参考 softmax。\n- 对每个矩阵 $E^{(k)}$，使用相同的算法在 `fp16` 和 `fp32` 中逐行计算稳定化 `softmax`，但需在指定的精度下执行算术运算。\n- 对每个矩阵 $E^{(k)}$，直接根据定义逐行计算朴素的 `fp64` 值 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$，并与相同数量的稳定化 `fp64` 值进行比较。\n\n答案格式规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中第 $k$ 个元素是与上述顺序中的情况 $k$ 对应的四个项目的列表。例如，最终输出应如下所示：$$[ [a_1,b_1,c_1,d_1], [a_2,b_2,c_2,d_2], [a_3,b_3,c_3,d_3], [a_4,b_4,c_4,d_4] ],$$ 除用于分隔数字和布尔值的必要空格外，不插入其他空格。\n\n不需要外部输入。所有计算都是无单位实数。不涉及角度。将所有差值量表示为实数。最后一行必须是唯一的输出。", "solution": "目标是通过对能量值 $e_{ij}$ 应用 softmax 函数来逐行计算注意力权重，同时保持数值稳定性，然后测量依赖于精度的数值误差。我们从核心定义和属性开始：\n\n1. 行向量 $\\mathbf{e}_i$ 的 softmax 定义：\n$$ \\mathrm{softmax}(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij})}{\\sum_{k} \\exp(e_{ik})} $$\nSoftmax 将实数值分数 $e_{ij}$ 映射到总和为 $1$ 的非负权重，这是自注意力中注意力分布的要求。\n\n2. 指数和对数的基本性质：\n指数函数 $\\exp$ 严格递增，并将 $\\mathbb{R}$ 映射到 $(0,\\infty)$。自然对数 $\\log$ 是其在 $(0,\\infty)$ 上的反函数，且对于 $a>0$ 和 $b>0$ 满足 $\\log(ab) = \\log(a) + \\log(b)$。这些性质允许对指数和进行受控的操作。\n\n3. 数值问题：\n直接计算 $\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right)$ 可能会出现上溢（如果任何 $e_{ij}$ 非常大，导致 $\\exp(e_{ij})$ 超出可表示范围）或下溢（如果 $e_{ij}$ 是非常大的负数，导致 $\\exp(e_{ij})$ 舍入为 $0$）。这种上溢或下溢会损害 softmax 分母的正确性，从而影响 $\\mathrm{softmax}(\\mathbf{e}_i)$ 的正确性。\n\n为了推导 $\\log \\sum_{j} \\exp(e_{ij})$ 的数值稳定变换，我们使用基于对数-指数关系的因式分解。设 $m_i$ 为行向量 $\\mathbf{e}_i$ 的最大元素，即 $m_i = \\max_j e_{ij}$。那么：\n$$\n\\sum_{j} \\exp(e_{ij}) = \\sum_{j} \\exp\\left((e_{ij} - m_i) + m_i\\right) = \\sum_{j} \\left[\\exp(e_{ij} - m_i)\\cdot \\exp(m_i)\\right] = \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i)\n$$\n应用 $\\log$ 和性质 $\\log(ab) = \\log(a) + \\log(b)$ 可得\n$$\n\\log \\left( \\sum_{j} \\exp(e_{ij}) \\right) = \\log\\left( \\exp(m_i)\\cdot \\sum_{j} \\exp(e_{ij} - m_i) \\right) = m_i + \\log \\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right)\n$$\n该变换是数值稳定的，因为：\n- 对所有 $j$，项 $e_{ij} - m_i \\le 0$，因此移位后的最大值为 $0$，其指数恰好为 $1$，即使 $m_i$ 很大也能避免上溢。\n- $e_{ij}$ 的极大负值会产生 $e_{ij} - m_i \\ll 0$，其指数非常小；将这些贡献下溢为 $0$ 对总和的影响很小，因为相对于最大项，它们已经可以忽略不计。\n- 整体数学值通过代数恒等式得以保持；该变换不改变 $\\log \\sum_{j} \\exp(e_{ij})$ 的值。\n\n对于稳定化的 softmax，我们使用相同的移位。对于行向量 $\\mathbf{e}_i$，定义 $m_i = \\max_j e_{ij}$ 并计算\n$$\n\\mathrm{softmax}(\\mathbf{e}_i)_j = \\frac{\\exp(e_{ij} - m_i)}{\\sum_{k} \\exp(e_{ik} - m_i)}\n$$\n这在数学上不改变 softmax 的值，因为公共乘法因子 $\\exp(m_i)$ 在分子和分母中被消去，但它通过防止分子和分母中的上溢，极大地提高了数值稳定性。\n\n浮点数考量：\n- 根据电气和电子工程师协会 (IEEE) 754 标准，`fp16` (binary 16) 的动态范围和尾数位数都比 `fp32` (binary 32) 小，而 `fp32` 的精度又低于 `fp64` (binary 64)。较低的精度会增加舍入误差，并增加在未移位的情况下，极小指数发生下溢和极大指数发生上溢的可能性。\n- 通过使用稳定算法计算 softmax 并在 `fp16`、`fp32` 和 `fp64` 中执行算术运算，我们可以通过测量与高精度 `fp64` 参考值的最大绝对差来量化精度对最终注意力权重的影响。\n\n算法设计：\n- 对每个测试矩阵 $E^{(k)}$，使用上述的最大值移位技术，逐行计算一个稳定化的 `fp64` 参考 softmax。\n- 对相同的输入，通过在指定 dtype 的数组上执行相同的操作，在 `fp16` 和 `fp32` 中计算稳定化 softmax。将输出转换为 `fp64` 以进行比较。\n- 单独地，对每一行，直接计算朴素的 `fp64` 值 $\\log\\left( \\sum_{j} \\exp(e_{ij}) \\right)$。当 $\\exp(e_{ij})$ 发生上溢时（特别是对于非常大的正 $e_{ij}$），这可能产生非有限值（如 $+\\infty$）。将此朴素值与稳定化的 `fp64` 值 $m_i + \\log\\left( \\sum_{j} \\exp(e_{ij} - m_i) \\right)$ 进行比较，并报告各行之间的最大绝对差。如果某行的朴素计算不是有限值，则该行对最大值的贡献为 `nan`，并且如果出现任何此类非有限值，则布尔上溢指示符设置为 true。\n\n测试套件覆盖范围的基本原理：\n- 情况 $1$ 使用中等值，代表了朴素计算安全且舍入误差较小的典型情况。\n- 情况 $2$ 包含一个具有极大正值 $1000$、极大负值 $-1000$ 以及中等范围 $0$ 到 $1$ 的行，这会在朴素的 $\\exp$ 计算中导致上溢，并测试了稳定化的必要性；第二行使用 $88$ 到 $85$ 的值，这些值很大但在 `fp64` 中是有限的。\n- 情况 $3$ 使用均匀的零，产生完全均匀的 softmax 权重，并测试精度对对称性的影响。\n- 情况 $4$ 使用极大负值来研究较低精度格式中的下溢行为以及 softmax 对微小指数的敏感性。\n\n输出规范：\n- 对每个矩阵 $E^{(k)}$，输出一个列表 $[a_k, b_k, c_k, d_k]$，其中 $a_k$ 是 `fp16` 稳定化 softmax 相对于 `fp64` 参考值的最大绝对差，$b_k$ 是 `fp32` 对应的量，$c_k$ 是在 `fp64` 中，朴素计算和稳定化计算 $\\log \\sum_{j} \\exp(e_{ij})$ 之间在各行上聚合的最大绝对差（非有限的朴素值贡献为 `nan`），$d_k$ 是一个布尔值，指示是否有任何行的朴素计算不是有限值。将四个情况的结果聚合成一个用方括号括起来的单一逗号分隔列表。\n\n这种有原则的设计将 $\\exp$ 和 $\\log$ 的数学恒等式与自注意力 softmax 计算中使用的算法选择联系起来，展示了其理论上的稳定性以及依赖于精度的经验误差特性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logsumexp_naive(row: np.ndarray) -> float:\n    \"\"\"Compute naive log(sum(exp(row))) in float64.\"\"\"\n    row64 = row.astype(np.float64)\n    with np.errstate(over='ignore'):\n        s = np.sum(np.exp(row64))\n    return float(np.log(s))\n\ndef logsumexp_stable(row: np.ndarray) -> float:\n    \"\"\"Compute stable logsumexp in float64: m + log(sum(exp(row - m))).\"\"\"\n    row64 = row.astype(np.float64)\n    m = np.max(row64)\n    if not np.isfinite(m):\n        return m\n    shifted = row64 - m\n    s = np.sum(np.exp(shifted))\n    return float(m + np.log(s))\n\ndef softmax_stable(matrix: np.ndarray, dtype: np.dtype) -> np.ndarray:\n    \"\"\"\n    Compute row-wise stabilized softmax in the specified dtype.\n    Returns results as float64 for comparison.\n    \"\"\"\n    x = matrix.astype(dtype)\n    # Row-wise max\n    m = np.max(x, axis=1, keepdims=True)\n    # Shift and exponentiate in dtype; cast back to dtype explicitly\n    shifted = (x - m).astype(dtype)\n    exps = np.exp(shifted).astype(dtype)\n    sums = np.sum(exps, axis=1, keepdims=True).astype(dtype)\n    # Avoid division by zero: if sum is zero (underflow), result remains zero\n    with np.errstate(divide='ignore', invalid='ignore'):\n        soft = exps / sums\n    soft[sums == 0] = 0.0 # Handle cases where all exps underflow to 0\n    return soft.astype(np.float64)\n\ndef max_abs_diff(a: np.ndarray, b: np.ndarray) -> float:\n    \"\"\"Maximum absolute difference between two arrays.\"\"\"\n    return float(np.max(np.abs(a - b)))\n\ndef format_item(item):\n    \"\"\"Format item without spaces, recursively for lists.\"\"\"\n    if isinstance(item, list):\n        return \"[\" + \",\".join(format_item(x) for x in item) + \"]\"\n    elif isinstance(item, float):\n        # Ensure Python's default float string is used (includes 'nan' or 'inf' if present)\n        return str(item)\n    elif isinstance(item, (int, np.integer)):\n        return str(int(item))\n    elif isinstance(item, (bool, np.bool_)):\n        return \"True\" if bool(item) else \"False\"\n    else:\n        return str(item)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([[-1.0, 0.0, 1.0, 2.0],\n                  [0.5, -0.5, 3.0, -3.0]], dtype=np.float64),\n        np.array([[1000.0, -1000.0, 0.0, 1.0],\n                  [88.0, 87.0, 86.0, 85.0]], dtype=np.float64),\n        np.array([[0.0, 0.0, 0.0, 0.0],\n                  [0.0, 0.0, 0.0, 0.0]], dtype=np.float64),\n        np.array([[-1000.0, -1001.0, -999.0, -1200.0],\n                  [-50.0, -60.0, -70.0, -80.0]], dtype=np.float64),\n    ]\n\n    results = []\n    for E in test_cases:\n        # Reference stabilized softmax in float64\n        soft_ref = softmax_stable(E, np.float64)\n\n        # Stabilized softmax in fp16 and fp32\n        soft16 = softmax_stable(E, np.float16)\n        soft32 = softmax_stable(E, np.float32)\n\n        # Error metrics (max absolute difference w.r.t. fp64 reference)\n        err16 = max_abs_diff(soft16, soft_ref)\n        err32 = max_abs_diff(soft32, soft_ref)\n\n        # Naive vs stabilized logsumexp in float64, row-wise\n        diffs = []\n        nonfinite_flag = False\n        for i in range(E.shape[0]):\n            row = E[i, :]\n            naive = logsumexp_naive(row)\n            stable = logsumexp_stable(row)\n            if not np.isfinite(naive):\n                nonfinite_flag = True\n                diffs.append(np.nan)\n            else:\n                diffs.append(abs(naive - stable))\n\n        diffs_arr = np.array(diffs, dtype=np.float64)\n        if np.all(np.isnan(diffs_arr)):\n            max_logsumexp_diff = float('nan')\n        else:\n            max_logsumexp_diff = float(np.nanmax(diffs_arr))\n\n        results.append([err16, err32, max_logsumexp_diff, nonfinite_flag])\n\n    # Final print statement in the exact required format: nested list without extra spaces.\n    print(format_item(results))\n\nsolve()\n```", "id": "3192585"}, {"introduction": "在保证计算正确性的基础上，效率是另一个关键考量。当处理长序列时，自注意力机制的计算成本可能成为性能瓶颈。本练习探讨了一种根本性的优化方法，即通过重新排列矩阵乘法的顺序，将计算复杂度从与序列长度的二次方相关转变为线性相关 [@problem_id:3192615]。你将使用一个简化的屋顶线（roofline）模型来分析浮点运算次数（FLOPs）和内存访问量之间的权衡，这是理解诸如FlashAttention等现代高效注意力变体背后核心思想的关键一步。", "problem": "在一个单头缩放点积自注意力模块中，在任何非线性激活函数之前，其核心的线性代数链可以写成三个维度兼容的矩阵的乘积。考虑一个批处理、多头的设定，其中查询、键和值以张量的形式存储，形状为 $B \\times H \\times n \\times d_h$，其中 $B$ 是批量大小，$H$ 是头的数量，$n$ 是序列长度，$d_h$ 是每个头的嵌入维度。对于每个批次-头对，定义矩阵 $Q, K, V \\in \\mathbb{R}^{n \\times d_h}$。忽略任何缩放因子和非线性激活函数，并仅关注这三个矩阵的结合律乘积。\n\n您的任务是分析该批处理乘积的两种数学上等价的计算顺序：\n- 基线顺序：$\\left(QK^{\\top}\\right)V$。\n- 重排（结合律）顺序：$Q\\left(K^{\\top}V\\right)$。\n\n假设以下基本事实：\n- 一个形状为 $(m \\times k)$ 乘以 $(k \\times n)$ 的通用矩阵乘法（GEMM）执行 $2mkn$ 次浮点运算。\n- 对于每次 GEMM，每个输入元素被精确读取一次，每个输出元素被精确写入一次。由一次 GEMM 产生并由另一次 GEMM 消耗的任何中间结果，都必须精确地写入主存一次，然后再从主存读取一次。不假设任何进一步的缓存或融合。\n- 计算的屋顶线（roofline）执行时间是计算时间和内存时间中的较大者，其中计算时间等于总浮点运算次数除以峰值浮点速率，内存时间等于总传输字节数除以持续内存带宽。\n\n考虑半精度浮点（FP16）张量，其元素大小为 $s = 2$ 字节。假设硬件为图形处理单元（GPU），其峰值 FP16 吞吐量为 $P = 1.0 \\times 10^{14}$ 次浮点运算/秒，持续内存带宽为 $W = 1.0 \\times 10^{12}$ 字节/秒。取 $B = 2$，$H = 8$，$n = 4096$，以及 $d_h = 64$。\n\n仅使用上述原理和标准矩阵维度，对所有 $B \\times H$ 个头执行以下操作：\n- 推导每种顺序的总浮点运算次数。\n- 推导每种顺序的总内存流量（以字节为单位）。\n- 计算每种顺序的屋顶线预测执行时间，即其计算密集型时间和内存密集型时间中的最大值。\n- 最后，计算加速比 $S$，定义为基线时间除以重排时间，结果为一个纯数。\n\n将您的最终答案 $S$ 四舍五入到四位有效数字。将最终答案表示为一个无单位的纯数。", "solution": "问题陈述已经过验证，被认为是科学上可靠、定义明确且客观的。它提出了对自注意力机制的两种计算策略的标准屋顶线模型分析，使用了明确定义的参数以及清晰的计算和内存访问模型。该问题是有效的，将推导出一个解决方案。\n\n核心任务是比较批处理、多头自注意力计算的两种计算顺序的性能，具体来说是 $(QK^{\\top})V$（基线）和 $Q(K^{\\top}V)$（重排）。我们将使用提供的屋顶线模型，该模型将执行时间定义为计算时间和内存时间的最大值。\n\n首先，我们用符号定义给定的参数：\n- 批量大小：$B = 2$\n- 头的数量：$H = 8$\n- 序列长度：$n = 4096$\n- 每个头的嵌入维度：$d_h = 64$\n- FP16 的元素大小：$s = 2$ 字节\n- 峰值 FP16 吞吐量：$P = 1.0 \\times 10^{14}$ FLOPS\n- 持续内存带宽：$W = 1.0 \\times 10^{12}$ 字节/秒\n\n独立注意力计算的总数为 $N_{\\text{ops}} = B \\times H = 2 \\times 8 = 16$。\n对于每次计算，矩阵 $Q, K, V$ 的维度为 $\\mathbb{R}^{n \\times d_h}$。\n一个形状为 $(m \\times k)$ 的矩阵与一个形状为 $(k \\times p)$ 的矩阵进行通用矩阵乘法（GEMM），需要 $2mkp$ 次浮点运算（FLOPs）。此类操作的总内存流量，计入读取两个输入矩阵和写入一个输出矩阵，为 $s(mk + kp + mp)$ 字节。\n\n### 基线顺序分析：$(QK^{\\top})V$\n\n此评估在所有 $N_{\\text{ops}}$ 个头上分两步进行。\n\n**步骤 1：计算注意力矩阵 $A = QK^{\\top}$**\n对于每个头，这是 $Q \\in \\mathbb{R}^{n \\times d_h}$ 和 $K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$ 的乘积。结果矩阵 $A$ 的形状为 $(n \\times n)$。\n- 每个头的浮点运算次数：当 $m=n, k=d_h, p=n$ 时，FLOPs 为 $2 \\times n \\times d_h \\times n = 2n^2d_h$。\n- 每个头的内存流量：流量为 $s(nd_h + d_h n + n^2) = s(2nd_h + n^2)$ 字节。\n\n**步骤 2：计算输出 $C = AV$**\n对于每个头，这是 $A \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{n \\times d_h}$ 的乘积。结果矩阵 $C$ 的形状为 $(n \\times d_h)$。\n- 每个头的浮点运算次数：当 $m=n, k=n, p=d_h$ 时，FLOPs 为 $2 \\times n \\times n \\times d_h = 2n^2d_h$。\n- 每个头的内存流量：流量为 $s(n^2 + nd_h + nd_h) = s(n^2 + 2nd_h)$ 字节。\n\n**基线顺序总计**\n总 FLOPs 和内存流量是这两个步骤的总和，再乘以头的数量 $N_{\\text{ops}}$。\n- 总 FLOPs, $F_{\\text{base}}$：\n$$F_{\\text{base}} = N_{\\text{ops}} (2n^2d_h + 2n^2d_h) = 4BHn^2d_h$$\n$$F_{\\text{base}} = 4 \\times 2 \\times 8 \\times (4096)^2 \\times 64 = 64 \\times (2^{12})^2 \\times 2^6 = 2^6 \\times 2^{24} \\times 2^6 = 2^{36} \\approx 6.872 \\times 10^{10} \\text{ 次浮点运算}$$\n- 总内存流量, $M_{\\text{base}}$：\n$$M_{\\text{base}} = N_{\\text{ops}} [s(2nd_h + n^2) + s(n^2 + 2nd_h)] = BHs(4nd_h + 2n^2)$$\n$$M_{\\text{base}} = 16 \\times 2 \\times [4(4096)(64) + 2(4096)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^{12})^2]$$\n$$M_{\\text{base}} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{24}] = 2^5 [2^{20} + 2^{25}] = 2^{25}(1+2^5) = 33 \\times 2^{25} \\approx 1.107 \\times 10^9 \\text{ 字节}$$\n\n**基线屋顶线时间, $T_{\\text{base}}$**\n- 计算时间：$T_{\\text{compute, base}} = F_{\\text{base}} / P = (2^{36}) / (10^{14}) \\approx 6.872 \\times 10^{-4}$ 秒。\n- 内存时间：$T_{\\text{memory, base}} = M_{\\text{base}} / W = (33 \\times 2^{25}) / (10^{12}) \\approx 1.107 \\times 10^{-3}$ 秒。\n- 执行时间：$T_{\\text{base}} = \\max(T_{\\text{compute, base}}, T_{\\text{memory, base}}) = T_{\\text{memory, base}} \\approx 1.107 \\times 10^{-3}$ 秒。\n\n### 重排顺序分析：$Q(K^{\\top}V)$\n\n此评估同样在所有 $N_{\\text{ops}}$ 个头上分两步进行。\n\n**步骤 1：计算上下文矩阵 $B = K^{\\top}V$**\n对于每个头，这是 $K^{\\top} \\in \\mathbb{R}^{d_h \\times n}$ 和 $V \\in \\mathbb{R}^{n \\times d_h}$ 的乘积。结果矩阵 $B$ 的形状为 $(d_h \\times d_h)$。\n- 每个头的浮点运算次数：当 $m=d_h, k=n, p=d_h$ 时，FLOPs 为 $2 \\times d_h \\times n \\times d_h = 2nd_h^2$。\n- 每个头的内存流量：流量为 $s(d_h n + nd_h + d_h^2) = s(2nd_h + d_h^2)$ 字节。\n\n**步骤 2：计算输出 $C = QB$**\n对于每个头，这是 $Q \\in \\mathbb{R}^{n \\times d_h}$ 和 $B \\in \\mathbb{R}^{d_h \\times d_h}$ 的乘积。结果矩阵 $C$ 的形状为 $(n \\times d_h)$。\n- 每个头的浮点运算次数：当 $m=n, k=d_h, p=d_h$ 时，FLOPs 为 $2 \\times n \\times d_h \\times d_h = 2nd_h^2$。\n- 每个头的内存流量：流量为 $s(nd_h + d_h^2 + nd_h) = s(2nd_h + d_h^2)$ 字节。\n\n**重排顺序总计**\n- 总 FLOPs, $F_{\\text{reord}}$：\n$$F_{\\text{reord}} = N_{\\text{ops}}(2nd_h^2 + 2nd_h^2) = 4BHnd_h^2$$\n$$F_{\\text{reord}} = 4 \\times 2 \\times 8 \\times 4096 \\times (64)^2 = 64 \\times 2^{12} \\times (2^6)^2 = 2^6 \\times 2^{12} \\times 2^{12} = 2^{30} \\approx 1.074 \\times 10^9 \\text{ 次浮点运算}$$\n- 总内存流量, $M_{\\text{reord}}$：\n$$M_{\\text{reord}} = N_{\\text{ops}} [s(2nd_h + d_h^2) + s(2nd_h + d_h^2)] = BHs(4nd_h + 2d_h^2)$$\n$$M_{\\text{reord}} = 16 \\times 2 \\times [4(4096)(64) + 2(64)^2] = 32 [4 \\cdot 2^{12} \\cdot 2^6 + 2 \\cdot (2^6)^2]$$\n$$M_{\\text{reord}} = 2^5 [2^2 \\cdot 2^{18} + 2 \\cdot 2^{12}] = 2^5 [2^{20} + 2^{13}] = 2^{18}(2^7+1) = 129 \\times 2^{18} \\approx 3.382 \\times 10^7 \\text{ 字节}$$\n\n**重排屋顶线时间, $T_{\\text{reord}}$**\n- 计算时间：$T_{\\text{compute, reord}} = F_{\\text{reord}} / P = (2^{30}) / (10^{14}) \\approx 1.074 \\times 10^{-5}$ 秒。\n- 内存时间：$T_{\\text{memory, reord}} = M_{\\text{reord}} / W = (129 \\times 2^{18}) / (10^{12}) \\approx 3.382 \\times 10^{-5}$ 秒。\n- 执行时间：$T_{\\text{reord}} = \\max(T_{\\text{compute, reord}}, T_{\\text{memory, reord}}) = T_{\\text{memory, reord}} \\approx 3.382 \\times 10^{-5}$ 秒。\n\n### 加速比 $S$\n\n两种评估顺序都是内存密集型（memory-bound），因为在这两种情况下都有 $T_{\\text{memory}} > T_{\\text{compute}}$。加速比 $S$ 是它们执行时间的比率。\n$$S = \\frac{T_{\\text{base}}}{T_{\\text{reord}}} = \\frac{T_{\\text{memory, base}}}{T_{\\text{memory, reord}}} = \\frac{M_{\\text{base}} / W}{M_{\\text{reord}} / W} = \\frac{M_{\\text{base}}}{M_{\\text{reord}}}$$\n代入内存流量的符号表达式：\n$$S = \\frac{BHs(4nd_h + 2n^2)}{BHs(4nd_h + 2d_h^2)} = \\frac{2n(2d_h + n)}{2d_h(2n + d_h)}$$\n使用计算出的值：\n$$S = \\frac{33 \\times 2^{25}}{129 \\times 2^{18}} = \\frac{33}{129} \\times 2^{25-18} = \\frac{3 \\times 11}{3 \\times 43} \\times 2^7 = \\frac{11}{43} \\times 128 = \\frac{1408}{43}$$\n$$S \\approx 32.744186...$$\n四舍五入到四位有效数字，加速比为 $32.74$。", "answer": "$$\\boxed{32.74}$$", "id": "3192615"}, {"introduction": "自注意力机制远非一个简单的“黑箱”，它的各个组件可以通过精心设计来解决特定的算法问题。在本练习中，你将从零开始构建一个注意力头，用它来解决奇偶校验（parity）任务 [@problem_id:3192596]。通过巧妙地构造查询（query）、键（key）和值（value）的投影矩阵，并利用正弦位置编码，你将亲眼见证注意力机制如何被“编程”以创建特定的、可解释的信息流模式，从而展示其非凡的表达能力。", "problem": "要求您设计并实现一个完整的、可运行的程序，该程序构建一个玩具数据集，其中一个单头缩放点积自注意力头使用正弦位置编码在不同位置上交替其焦点，然后针对一个奇偶校验任务，定量分析由此产生的注意力模式。\n\n您必须使用的基本依据是：\n\n- 单头缩放点积自注意力机制的定义。对于一个长度为 $T$ 的序列，给定查询 $Q \\in \\mathbb{R}^{T \\times d_k}$、键 $K \\in \\mathbb{R}^{T \\times d_k}$ 和值 $V \\in \\mathbb{R}^{T \\times d_v}$，在查询索引 $t$ 处的注意力输出为\n$$\n\\mathrm{Attn}(t) = \\sum_{s=0}^{T-1} \\alpha_{t,s} V_s,\n\\quad\n\\alpha_{t,s} = \\frac{\\exp\\left(z_{t,s}\\right)}{\\sum_{u=0}^{T-1} \\exp\\left(z_{t,u}\\right)},\n\\quad\nz_{t,s} = \\frac{\\langle Q_t, K_s \\rangle}{\\sqrt{d_k}},\n$$\n其中 $\\langle \\cdot , \\cdot \\rangle$ 表示标准欧几里得点积。\n\n- 由一个以弧度为单位的角度 $\\theta_p$ 参数化的正弦位置编码。对于位置索引 $p \\in \\{0,1,\\dots,T-1\\}$，您必须使用 $\\theta_p = \\pi p$。位置编码是二维向量 $[\\sin(\\theta_p), \\cos(\\theta_p)]$。\n\n- 对于所有实数角度 $a$ 和 $b$ 的三角恒等式：\n$$\n\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b).\n$$\n\n您的构建必须满足以下规范：\n\n- 输入是二元序列 $x \\in \\{0,1\\}^T$。对于每个位置 $p$，通过拼接词元值及其位置编码来定义一个嵌入向量 $e_p \\in \\mathbb{R}^3$：\n$$\ne_p = \\big[x_p,\\, \\sin(\\theta_p),\\, \\cos(\\theta_p)\\big].\n$$\n所有正弦和余弦都应以弧度计算。\n\n- 您必须为一个键-查询维度 $d_k = 2$ 和值维度 $d_v = 1$ 的单注意力头构建线性投影 $W_Q \\in \\mathbb{R}^{3 \\times d_k}$、$W_K \\in \\mathbb{R}^{3 \\times d_k}$ 和 $W_V \\in \\mathbb{R}^{3 \\times 1}$，使得：\n  - 查询 $Q_t \\in \\mathbb{R}^{2}$ 和键 $K_s \\in \\mathbb{R}^{2}$ 仅依赖于 $e_t$ 和 $e_s$ 的位置分量。\n  - 值 $V_s \\in \\mathbb{R}$ 仅依赖于 $e_s$ 的词元分量 $x_s$。\n  - 因此，未归一化的注意力分数 $z_{t,s}$ 仅依赖于 $t$ 和 $s$ 之间的位置关系。\n\n- 引入一个非负标量温度 $\\beta \\in \\mathbb{R}_{\\ge 0}$，在 softmax 之前乘以分数 $z_{t,s}$，即，使用分数 $z_{t,s}^{(\\beta)} = \\beta \\cdot \\langle Q_t, K_s \\rangle$ 代替 $z_{t,s}$，并且不要在其他地方改变 $d_k$。这将点积的大小效应与 softmax 的尖锐度分离开来。\n\n- 将您的注意力分析集中在查询位置 $t = 0$ 上。在 $\\theta_p = \\pi p$ 的情况下，偶数索引和奇数索引的位置在 $\\cos(\\theta_p)$ 中具有相反的相位，这导致当 $z_{t,s}^{(\\beta)}$ 依赖于 $\\theta_t - \\theta_s$ 时，注意力模式在不同位置上交替出现。通过保真度度量来量化这种交替\n$$\nF = \\sum_{\\substack{s \\in \\{0,\\dots,T-1\\}\\\\ s \\text{ even}}} \\alpha_{0,s} \\;-\\; \\sum_{\\substack{s \\in \\{0,\\dots,T-1\\}\\\\ s \\text{ odd}}} \\alpha_{0,s}.\n$$\n\n- 将每个序列 $x$ 的目标标签定义为奇偶校验\n$$\ny(x) = \\left(\\sum_{p=0}^{T-1} x_p\\right) \\bmod 2.\n$$\n\n- 仅使用上面描述的单个注意力头和查询 $t=0$ 的注意力权重，按如下方式恢复精确的奇偶性。因为您的设计必须使 $\\alpha_{0,s}$ 在两个奇偶性组 $\\{s \\,:\\, s \\text{ even}\\}$ 和 $\\{s \\,:\\, s \\text{ odd}\\}$ 内保持恒定，所以您可以计算词元值的组内加权平均值，然后按组的大小重新缩放以获得精确的组总和：\n  - 令 $E = \\{s \\in \\{0,\\dots,T-1\\} \\,:\\, s \\text{ even}\\}$ 和 $O = \\{s \\in \\{0,\\dots,T-1\\} \\,:\\, s \\text{ odd}\\}$。定义\n  $$\n  \\mu_E(x) = \\frac{\\sum_{s \\in E} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in E} \\alpha_{0,s}}, \\quad\n  \\mu_O(x) = \\frac{\\sum_{s \\in O} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in O} \\alpha_{0,s}},\n  $$\n  遵循一个约定：如果分母因为对应的集合为空而为零，则对应的平均值定义为 $0$，并且组大小视为 $0$。令 $k_E = |E|$ 和 $k_O = |O|$。形成预测的总和\n  $$\n  \\widehat{S}(x) = k_E \\cdot \\mu_E(x) + k_O \\cdot \\mu_O(x),\n  $$\n  和预测的奇偶性\n  $$\n  \\widehat{y}(x) = \\big\\lfloor \\widehat{S}(x) + \\tfrac{1}{2} \\big\\rfloor \\bmod 2.\n  $$\n  该过程必须以数值方式实现；在推理过程中，除了计算准确率外，不得使用真实标签。\n\n- 对于下面的每个测试用例，在长度为 $T$ 的所有二元序列的穷举数据集上，评估交替保真度 $F$ 和分类准确率\n$$\n\\mathrm{Acc} = \\frac{1}{2^T} \\sum_{x \\in \\{0,1\\}^T} \\mathbf{1}\\{\\widehat{y}(x) = y(x)\\},\n$$\n\n角度单位要求：所有三角函数必须使用弧度。\n\n测试套件和程序输出：\n\n- 使用以下测试用例，每个用例指定为一个对 $(T,\\beta)$：\n  - 用例 1：$(T,\\beta) = (8, 2.0)$。\n  - 用例 2：$(T,\\beta) = (1, 3.0)$。\n  - 用例 3：$(T,\\beta) = (7, 0.0)$。\n  - 用例 4：$(T,\\beta) = (10, 5.0)$。\n\n- 对于每个用例，计算两个浮点数：查询 $t=0$ 的交替保真度 $F$，以及准确率 $\\mathrm{Acc}$（一个在 $[0,1]$ 区间内的小数）。在最终输出中将这两个数字都四舍五入到 $6$ 位小数。\n\n- 您的程序应生成单行输出，其中包含所有按顺序排列的结果，格式为方括号括起来的逗号分隔列表，即，\n$[\\;F_1,\\mathrm{Acc}_1,F_2,\\mathrm{Acc}_2,F_3,\\mathrm{Acc}_3,F_4,\\mathrm{Acc}_4\\;]$,\n每个浮点数四舍五入到 $6$ 位小数，并且行内任何地方都没有空格。\n\n程序必须是自包含的，不需要任何输入，并且必须仅使用上述定义和指定的测试用例来确定性地计算所需的输出。不允许使用随机化。所有三角计算都必须使用弧度。结果类型必须是指定的浮点数。", "solution": "用户提供的问题陈述已经过分析，并被验证为科学上合理、问题定义明确且内部一致。\n\n该问题要求设计一个专门的单头自注意力机制来解决二元序列上的奇偶校验任务。该设计必须利用正弦位置编码来创建一个交替的注意力模式。我们将首先构建所需的投影矩阵，然后推导注意力权重和保真度度量 $F$ 的解析形式，最后分析奇偶性预测机制的准确性。\n\n### 步骤 1：构建投影矩阵\n\n在位置 $p$ 处的词元 $x_p \\in \\{0, 1\\}$ 的输入嵌入由 $e_p = [x_p, \\sin(\\theta_p), \\cos(\\theta_p)] \\in \\mathbb{R}^3$ 给出，其中位置角度为 $\\theta_p = \\pi p$ 弧度。我们需要构建权重矩阵 $W_Q \\in \\mathbb{R}^{3 \\times 2}$、$W_K \\in \\mathbb{R}^{3 \\times 2}$ 和 $W_V \\in \\mathbb{R}^{3 \\times 1}$（因为 $d_k=2, d_v=1$），并使其满足几个约束条件。\n\n1.  查询 $Q_t = e_t W_Q$ 和键 $K_s = e_s W_K$ 必须仅依赖于嵌入的位置分量。这意味着 $W_Q$ 和 $W_K$ 的第一行必须都为零。\n2.  值 $V_s = e_s W_V$ 必须仅依赖于词元分量 $x_s$。这意味着 $W_V$ 的第二和第三行必须都为零。\n3.  未归一化的注意力分数是 $\\langle Q_t, K_s \\rangle$ 的函数，它应该依赖于位置差 $\\theta_t - \\theta_s$。提供的三角恒等式 $\\cos(a-b) = \\cos(a)\\cos(b) + \\sin(a)\\sin(b)$ 指明了一条路径。\n\n令嵌入的位置部分为向量 $\\vec{p}_p = [\\sin(\\theta_p), \\cos(\\theta_p)]$。点积 $\\langle\\vec{p}_t, \\vec{p}_s\\rangle$ 恰好是 $\\cos(\\theta_t - \\theta_s)$。如果查询和键就是嵌入的位置部分，我们就可以实现这一点。我们选择作用于位置分量的子矩阵为单位矩阵。一个满足所有约束的权重矩阵的简单选择是：\n$$\nW_Q = \\begin{pmatrix} 0  & 0 \\\\ 1  & 0 \\\\ 0  & 1 \\end{pmatrix}, \\quad W_K = \\begin{pmatrix} 0  & 0 \\\\ 1  & 0 \\\\ 0  & 1 \\end{pmatrix}, \\quad W_V = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n使用这些矩阵，我们得到：\n-   t 位置的查询：$Q_t = e_t W_Q = [\\sin(\\theta_t), \\cos(\\theta_t)]$。\n-   s 位置的键：$K_s = e_s W_K = [\\sin(\\theta_s), \\cos(\\theta_s)]$。\n-   s 位置的值：$V_s = e_s W_V = [x_s]$。\n\n查询和键的点积是 $\\langle Q_t, K_s \\rangle = \\sin(\\theta_t)\\sin(\\theta_s) + \\cos(\\theta_t)\\cos(\\theta_s) = \\cos(\\theta_t - \\theta_s)$。当 $\\theta_p=\\pi p$ 时，这变成 $\\langle Q_t, K_s \\rangle = \\cos(\\pi(t-s))$。\n\n### 步骤 2：推导注意力权重和保真度\n\n问题指定在 softmax 函数中使用经温度缩放的分数 $z_{t,s}^{(\\beta)} = \\beta \\cdot \\langle Q_t, K_s \\rangle$，以取代标准的缩放点积公式。因此，未归一化的分数为：\n$$\nz_{t,s}^{(\\beta)} = \\beta \\cos(\\pi(t-s))\n$$\n注意力权重 $\\alpha_{t,s}$ 是将 softmax 函数应用于这些分数得到的。我们关心的是位置 $t=0$ 处的查询：\n$$\n\\alpha_{0,s} = \\frac{\\exp\\left(z_{0,s}^{(\\beta)}\\right)}{\\sum_{u=0}^{T-1} \\exp\\left(z_{0,u}^{(\\beta)}\\right)}\n$$\n对于 $t=0$，分数简化为：$z_{0,s}^{(\\beta)} = \\beta \\cos(-\\pi s) = \\beta \\cos(\\pi s)$。如果 $s$ 是偶数，$\\cos(\\pi s)$ 的值为 $1$；如果 $s$ 是奇数，则为 $-1$。\n$$\nz_{0,s}^{(\\beta)} = \\begin{cases} \\beta  & \\text{if } s \\text{ is even} \\\\ -\\beta & \\text{if } s \\text{ is odd} \\end{cases}\n$$\n令 $E = \\{s \\in \\{0, \\dots, T-1\\} : s \\text{ is even}\\}$ 和 $O = \\{s \\in \\{0, \\dots, T-1\\} : s \\text{ is odd}\\}$。令它们的大小为 $k_E = |E| = \\lceil T/2 \\rceil$ 和 $k_O = |O| = \\lfloor T/2 \\rfloor$。注意力权重 $\\alpha_{0,s}$ 将取两个值之一：\n-   对于 $s \\in E$： $\\alpha_{0,s} = \\alpha_{\\text{even}} = \\frac{e^{\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}$\n-   对于 $s \\in O$： $\\alpha_{0,s} = \\alpha_{\\text{odd}} = \\frac{e^{-\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}$\n\n在 $t=0$ 处的交替保真度度量 $F$ 定义为 $F = \\sum_{s \\in E} \\alpha_{0,s} - \\sum_{s \\in O} \\alpha_{0,s}$。代入常数权重：\n$$\nF = k_E \\alpha_{\\text{even}} - k_O \\alpha_{\\text{odd}} = \\frac{k_E e^{\\beta} - k_O e^{-\\beta}}{k_E e^{\\beta} + k_O e^{-\\beta}}\n$$\n对于每个测试用例 $(T, \\beta)$，都可以直接计算此公式。\n\n### 步骤 3：分析奇偶性预测和准确率\n\n问题定义了一个预测序列 $x \\in \\{0,1\\}^T$ 的奇偶性的过程。真实奇偶性是 $y(x) = (\\sum_{p=0}^{T-1} x_p) \\pmod 2$。预测机制使用偶数和奇数位置上词元值的加权平均值。\n$$\n\\mu_E(x) = \\frac{\\sum_{s \\in E} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in E} \\alpha_{0,s}}, \\quad \\mu_O(x) = \\frac{\\sum_{s \\in O} \\alpha_{0,s} \\, x_s}{\\sum_{s \\in O} \\alpha_{0,s}}\n$$\n由于 $\\alpha_{0,s}$ 在每个组（偶数/奇数）内是常数，我们可以简化这些表达式。对于 $T>0$，$k_E > 0$ 且 $\\alpha_{\\text{even}} > 0$，所以 $\\mu_E(x)$ 的分母不为零。\n$$\n\\mu_E(x) = \\frac{\\alpha_{\\text{even}} \\sum_{s \\in E} x_s}{k_E \\alpha_{\\text{even}}} = \\frac{1}{k_E} \\sum_{s \\in E} x_s\n$$\n类似地，如果 $k_O > 0$：\n$$\n\\mu_O(x) = \\frac{\\alpha_{\\text{odd}} \\sum_{s \\in O} x_s}{k_O \\alpha_{\\text{odd}}} = \\frac{1}{k_O} \\sum_{s \\in O} x_s\n$$\n如果 $k_O = 0$（当 $T=1$ 时发生），问题约定设置 $\\mu_O(x) = 0$。\n\n预测的总和是 $\\widehat{S}(x) = k_E \\mu_E(x) + k_O \\mu_O(x)$。代入平均值的表达式：\n-   如果 $k_E > 0$ 且 $k_O > 0$：\n    $$ \\widehat{S}(x) = k_E \\left(\\frac{1}{k_E} \\sum_{s \\in E} x_s\\right) + k_O \\left(\\frac{1}{k_O} \\sum_{s \\in O} x_s\\right) = \\sum_{s \\in E} x_s + \\sum_{s \\in O} x_s = \\sum_{p=0}^{T-1} x_p $$\n-   如果 $k_O = 0$（即 $T=1$）：\n    $$ \\widehat{S}(x) = k_E \\mu_E(x) + 0 \\cdot \\mu_O(x) = 1 \\cdot \\left(\\frac{1}{1} \\sum_{s \\in E} x_s\\right) + 0 = x_0 = \\sum_{p=0}^{0} x_p $$\n在所有 $T>0$ 的情况下，预测总和 $\\widehat{S}(x)$ 精确等于词元的真实总和 $S(x) = \\sum_{p=0}^{T-1} x_p$。\n\n预测的奇偶性是 $\\widehat{y}(x) = \\lfloor \\widehat{S}(x) + 0.5 \\rfloor \\pmod 2$。由于 $S(x)$ 始终是整数（因为它是二元值的总和），我们有 $\\lfloor S(x) + 0.5 \\rfloor = S(x)$。因此：\n$$\n\\widehat{y}(x) = S(x) \\pmod 2 = y(x)\n$$\n对于每个序列 $x$，预测的奇偶性都与真实的奇偶性相同。这在序列长度 $T>0$ 和温度 $\\beta$ 无关的情况下都成立。因此，在整个数据集 $\\{0,1\\}^T$ 上的准确率 $\\mathrm{Acc}$ 必须精确为 $1.0$。\n\n问题要求对此过程进行数值实现。由于精确的代数抵消，数值浮点误差是唯一的潜在偏差来源。然而，对于标准的双精度算术，这些误差可以忽略不计，并且四舍五入操作 $\\lfloor \\cdot + 0.5 \\rfloor$ 确保了鲁棒性，从而导致计算出的准确率为 $1.0$。\n\n### 步骤 4：数值评估\n\n我们现在将这些公式应用于指定的测试用例。\n\n-   **用例 1：** $(T, \\beta) = (8, 2.0)$。$k_E=4$, $k_O=4$。\n    $F = \\frac{4e^2 - 4e^{-2}}{4e^2 + 4e^{-2}} = \\frac{e^2 - e^{-2}}{e^2 + e^{-2}} = \\tanh(2.0) \\approx 0.964028$。$\\mathrm{Acc} = 1.0$。\n-   **用例 2：** $(T, \\beta) = (1, 3.0)$。$k_E=1$, $k_O=0$。\n    $F = \\frac{1e^3 - 0}{1e^3 + 0} = 1.0$。$\\mathrm{Acc} = 1.0$。\n-   **用例 3：** $(T, \\beta) = (7, 0.0)$。$k_E=4$, $k_O=3$。\n    $F = \\frac{4e^0 - 3e^{0}}{4e^0 + 3e^{0}} = \\frac{4-3}{4+3} = \\frac{1}{7} \\approx 0.142857$。$\\mathrm{Acc} = 1.0$。\n-   **用例 4：** $(T, \\beta) = (10, 5.0)$。$k_E=5$, $k_O=5$。\n    $F = \\frac{5e^5 - 5e^{-5}}{5e^5 + 5e^{-5}} = \\frac{e^5 - e^{-5}}{e^5 + e^{-5}} = \\tanh(5.0) \\approx 0.999909$。$\\mathrm{Acc} = 1.0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating alternation fidelity and parity prediction accuracy\n    for a series of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement as (T, beta) pairs.\n    test_cases = [\n        (8, 2.0),\n        (1, 3.0),\n        (7, 0.0),\n        (10, 5.0),\n    ]\n\n    results = []\n    \n    for T, beta in test_cases:\n        # Step 1: Calculate alternation fidelity F\n        \n        # Determine the number of even and odd indices in the sequence of length T.\n        # k_E is the size of the set E = {s in {0,...,T-1} : s is even}\n        # k_O is the size of the set O = {s in {0,...,T-1} : s is odd}\n        k_E = int(np.ceil(T / 2.0))\n        k_O = int(np.floor(T / 2.0))\n        \n        # Calculate the numerator and denominator for the fidelity metric F.\n        # F = (k_E * exp(beta) - k_O * exp(-beta)) / (k_E * exp(beta) + k_O * exp(-beta))\n        # This formula is robust. If T=1, k_O=0, denominator is exp(beta) != 0.\n        # If beta=0, denominator is k_E + k_O = T != 0 for T>=1.\n        numerator_F = k_E * np.exp(beta) - k_O * np.exp(-beta)\n        denominator_F = k_E * np.exp(beta) + k_O * np.exp(-beta)\n        \n        fidelity = 0.0\n        if denominator_F != 0:\n            fidelity = numerator_F / denominator_F\n\n        results.append(f\"{fidelity:.6f}\")\n        \n        # Step 2: Calculate classification accuracy Acc\n        \n        # The problem requires a numerical implementation of the accuracy calculation.\n        # We iterate through all 2^T binary sequences.\n        if T > 20: # To prevent extremely long computation times for large T\n             # Based on analytical solution, Acc is always 1.0 for T>0\n            accuracy = 1.0\n        else:\n            num_sequences = 2**T\n            correct_predictions = 0\n            \n            # Generate indices for even and odd positions\n            even_indices = [i for i in range(T) if i % 2 == 0]\n            odd_indices = [i for i in range(T) if i % 2 != 0]\n\n            for i in range(num_sequences):\n                # Generate the binary sequence x of length T from integer i\n                x = np.array([(i >> p)  1 for p in range(T)])\n                \n                # True parity y(x)\n                true_sum = np.sum(x)\n                true_parity = true_sum % 2\n                \n                # Predicted parity y_hat(x)\n                # The calculation simplifies as shown in the analytical solution.\n                # Here we implement the full procedure as requested.\n                \n                # Calculate means mu_E and mu_O.\n                # alpha_{0,s} are constant within even/odd groups, so they cancel.\n                sum_x_even = np.sum(x[even_indices]) if len(even_indices) > 0 else 0\n                sum_x_odd = np.sum(x[odd_indices]) if len(odd_indices) > 0 else 0\n                \n                mu_E = sum_x_even / k_E if k_E > 0 else 0.0\n                mu_O = sum_x_odd / k_O if k_O > 0 else 0.0\n                \n                # Calculate predicted total sum S_hat(x)\n                pred_sum = k_E * mu_E + k_O * mu_O\n                \n                # Calculate predicted parity y_hat(x)\n                # Use floor(val + 0.5) for rounding to nearest integer.\n                pred_parity = int(np.floor(pred_sum + 0.5)) % 2\n                \n                if pred_parity == true_parity:\n                    correct_predictions += 1\n                    \n            accuracy = correct_predictions / num_sequences\n        results.append(f\"{accuracy:.6f}\")\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3192596"}]}