{"hands_on_practices": [{"introduction": "本练习将通过一个具体案例，揭示加性注意力和乘性注意力的一个根本行为差异。我们将探索加性注意力如何因其 $\\tanh$ 非线性而在输入值较大时出现饱和，从而对输入的大小失去敏感性；相比之下，乘性注意力的得分会随着输入大小线性缩放。通过这个练习，你将对不同注意力机制如何响应输入尺度建立起量化的直观理解。[@problem_id:3097423]", "problem": "考虑一个对编码器状态施加注意力机制的序列到序列模型，重点关注单个标量维度，以分离解码器状态中幅度的作用。设编码器隐藏状态为三个标量 $h_{1}$、$h_{2}$ 和 $h_{3}$，每个都属于 $\\mathbb{R}$，并设时间步 $t$ 的解码器状态为一个标量 $s_{t} \\in \\mathbb{R}$。$s_{t}$ 的幅度编码了重要性：较大的 $|s_{t}|$ 应导致对与 $s_{t}$ 符号一致的输入给予更具选择性的注意力。我们比较两种注意力得分的构造方法：加性注意力（Bahdanau）和乘性注意力（Luong）。\n\n使用以下基本定义和参数化设置：\n\n- 对于一个实值能量向量 $e \\in \\mathbb{R}^{n}$，带有温度参数 $\\tau$ 的 Softmax 函数定义为\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}.\n$$\n在所有计算中，设置 $\\tau = 1$。\n\n- 加性注意力的能量使用带有双曲正切非线性的单隐藏层前馈结构：\n$$\ne_{i}^{\\mathrm{add}} = v \\cdot \\tanh\\left(w_{h} h_{i} + w_{s} s_{t} + b\\right),\n$$\n固定标量参数为 $v = 1$，$w_{h} = 1$，$w_{s} = 10$ 和 $b = 0$。\n\n- 乘性注意力的能量使用双线性（缩放点积）形式：\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i},\n$$\n固定标量参数为 $W = 1$。\n\n设编码器状态为\n$$\nh_{1} = 1, \\quad h_{2} = -1, \\quad h_{3} = 0.\n$$\n将 $h_{1}$ 解释为与正值 $s_{t}$ 对齐的状态，因为它们的符号一致。\n\n对于每种能量构造方法，使用上述 Softmax 定义将能量 $\\{e_{i}\\}$ 转换为注意力权重 $\\{\\alpha_{i}\\}$。在给定构造方法下，分配给 $h_{1}$ 的注意力权重是相应的 $\\alpha_{1}$。\n\n测试套件规范（每个测试用例指定一个 $s_{t}$ 的值）：\n- 情况 1：$s_{t} = 0$（无幅度的边界情况）。\n- 情况 2：$s_{t} = 0.1$（小幅度）。\n- 情况 3：$s_{t} = 1$（中等幅度）。\n- 情况 4：$s_{t} = 5$（大幅度）。\n- 情况 5：$s_{t} = 50$（极大幅度）。\n\n对于每种情况，计算：\n- $h_{1}$ 上的乘性注意力权重，记为 $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$。\n- $h_{1}$ 上的加性注意力权重，记为 $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$。\n\n然后，在整个测试套件上评估以下布尔属性：\n- $B_{\\mathrm{mul}}$：当 $s_{t}$ 在测试套件中递增时，$w^{\\mathrm{mul}}(s_{t})$ 是非递减的。形式上，对于有序集合 $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$，检查 $w^{\\mathrm{mul}}(0) \\le w^{\\mathrm{mul}}(0.1) \\le w^{\\mathrm{mul}}(1) \\le w^{\\mathrm{mul}}(5) \\le w^{\\mathrm{mul}}(50)$。\n- $B_{\\mathrm{add}}$：加性注意力在 $|s_{t}|$ 较大时会饱和并丢失幅度信息，这可以通过权重在大幅度值下近似恒定来量化。检查大幅度和极大幅度情况之间的变化是否可以忽略不计：\n$$\n\\left| w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5) \\right| \\le \\epsilon,\n$$\n其中 $\\epsilon = 10^{-12}$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含：\n- $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$ 的五个 $w^{\\mathrm{mul}}(s_{t})$ 值。\n- $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$ 的五个 $w^{\\mathrm{add}}(s_{t})$ 值。\n- 布尔值 $B_{\\mathrm{mul}}$。\n- 布尔值 $B_{\\mathrm{add}}$。\n\n例如，一个输出行的形式为\n$$\n[\\underbrace{w^{\\mathrm{mul}}(0), w^{\\mathrm{mul}}(0.1), w^{\\mathrm{mul}}(1), w^{\\mathrm{mul}}(5), w^{\\mathrm{mul}}(50)}_{\\text{五个乘性权重}}, \\underbrace{w^{\\mathrm{add}}(0), w^{\\mathrm{add}}(0.1), w^{\\mathrm{add}}(1), w^{\\mathrm{add}}(5), w^{\\mathrm{add}}(50)}_{\\text{五个加性权重}}, B_{\\mathrm{mul}}, B_{\\mathrm{add}} ].\n$$\n所有值都是无单位的，不涉及任何物理单位。答案中的权重是浮点数，最后两个属性是布尔值。", "solution": "经评估，用户提供的问题是有效的。它在科学上基于神经注意力机制的原理，问题陈述清晰，提供了所有必要的定义和参数，并且表述客观。该问题要求对加性（Bahdanau 风格）和乘性（Luong 风格）注意力进行定量比较，重点关注每种机制如何响应解码器状态幅度的变化。我们现在将给出一个完整且论证充分的解决方案。\n\n问题的核心在于计算注意力权重，这些权重是通过 Softmax 函数从一组能量得分中导出的。对于一组能量 $\\{e_{1}, e_{2}, \\dots, e_{n}\\}$，对应的注意力权重 $\\{\\alpha_{1}, \\alpha_{2}, \\dots, \\alpha_{n}\\}$ 由以下公式给出：\n$$\n\\alpha_{i} = \\frac{\\exp\\left(\\frac{e_{i}}{\\tau}\\right)}{\\sum_{j=1}^{n} \\exp\\left(\\frac{e_{j}}{\\tau}\\right)}\n$$\n问题指定了温度 $\\tau = 1$。编码器隐藏状态是标量 $h_{1} = 1$、$h_{2} = -1$ 和 $h_{3} = 0$。我们将为测试套件 $\\{0, 0.1, 1, 5, 50\\}$ 中解码器状态 $s_{t}$ 的每个指定值计算注意力权重 $\\alpha_{1}$。\n\n**1. 乘性注意力分析**\n\n乘性注意力的能量由双线性形式 $e_{i}^{\\mathrm{mul}} = s_{t} \\cdot W \\cdot h_{i}$ 定义。当标量参数 $W = 1$ 时，该式简化为：\n$$\ne_{i}^{\\mathrm{mul}} = s_{t} h_{i}\n$$\n对于给定的编码器状态，能量为：\n- $e_{1}^{\\mathrm{mul}} = s_{t} \\cdot (1) = s_{t}$\n- $e_{2}^{\\mathrm{mul}} = s_{t} \\cdot (-1) = -s_{t}$\n- $e_{3}^{\\mathrm{mul}} = s_{t} \\cdot (0) = 0$\n\n第一个编码器状态上的注意力权重 $w^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}}$ 使用 Softmax 函数计算如下：\n$$\nw^{\\mathrm{mul}}(s_{t}) = \\alpha_{1}^{\\mathrm{mul}} = \\frac{\\exp(e_{1}^{\\mathrm{mul}})}{\\exp(e_{1}^{\\mathrm{mul}}) + \\exp(e_{2}^{\\mathrm{mul}}) + \\exp(e_{3}^{\\mathrm{mul}})} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + \\exp(0)} = \\frac{\\exp(s_{t})}{\\exp(s_{t}) + \\exp(-s_{t}) + 1}\n$$\n我们为测试套件中的每个 $s_t$ 计算该值：\n- 对于 $s_{t} = 0$：$w^{\\mathrm{mul}}(0) = \\frac{\\exp(0)}{\\exp(0) + \\exp(0) + 1} = \\frac{1}{1 + 1 + 1} = \\frac{1}{3} \\approx 0.3333333333333333$\n- 对于 $s_{t} = 0.1$：$w^{\\mathrm{mul}}(0.1) = \\frac{\\exp(0.1)}{\\exp(0.1) + \\exp(-0.1) + 1} \\approx 0.3670997184200147$\n- 对于 $s_{t} = 1$：$w^{\\mathrm{mul}}(1) = \\frac{\\exp(1)}{\\exp(1) + \\exp(-1) + 1} \\approx 0.6652409557224216$\n- 对于 $s_{t} = 5$：$w^{\\mathrm{mul}}(5) = \\frac{\\exp(5)}{\\exp(5) + \\exp(-5) + 1} \\approx 0.9932620531633535$\n- 对于 $s_{t} = 50$：$w^{\\mathrm{mul}}(50) = \\frac{\\exp(50)}{\\exp(50) + \\exp(-50) + 1} \\approx 0.9999999999999999$\n\n**2. 加性注意力分析**\n\n加性注意力的能量由 $e_{i}^{\\mathrm{add}} = v \\cdot \\tanh(w_{h} h_{i} + w_{s} s_{t} + b)$ 定义。当参数为 $v = 1$、$w_{h} = 1$、$w_{s} = 10$ 和 $b = 0$ 时，该式变为：\n$$\ne_{i}^{\\mathrm{add}} = \\tanh(h_{i} + 10s_{t})\n$$\n对于给定的编码器状态，能量为：\n- $e_{1}^{\\mathrm{add}} = \\tanh(1 + 10s_{t})$\n- $e_{2}^{\\mathrm{add}} = \\tanh(-1 + 10s_{t})$\n- $e_{3}^{\\mathrm{add}} = \\tanh(0 + 10s_{t}) = \\tanh(10s_{t})$\n\n注意力权重 $w^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}}$ 为：\n$$\nw^{\\mathrm{add}}(s_{t}) = \\alpha_{1}^{\\mathrm{add}} = \\frac{\\exp(\\tanh(1 + 10s_{t}))}{\\exp(\\tanh(1 + 10s_{t})) + \\exp(\\tanh(-1 + 10s_{t})) + \\exp(\\tanh(10s_{t}))}\n$$\n我们为每个 $s_{t}$ 计算该值：\n- 对于 $s_{t} = 0$：$e_{1} = \\tanh(1)$，$e_{2} = \\tanh(-1)$，$e_{3} = \\tanh(0) = 0$。\n  $w^{\\mathrm{add}}(0) = \\frac{\\exp(\\tanh(1))}{\\exp(\\tanh(1)) + \\exp(-\\tanh(1)) + 1} \\approx 0.5052857441183307$\n- 对于 $s_{t} = 0.1$：$e_{1} = \\tanh(2)$，$e_{2} = \\tanh(0) = 0$，$e_{3} = \\tanh(1)$。\n  $w^{\\mathrm{add}}(0.1) = \\frac{\\exp(\\tanh(2))}{\\exp(\\tanh(2)) + \\exp(0) + \\exp(\\tanh(1))} \\approx 0.44917951253013896$\n- 对于 $s_{t} = 1$：$e_{1} = \\tanh(11)$，$e_{2} = \\tanh(9)$，$e_{3} = \\tanh(10)$。\n  $w^{\\mathrm{add}}(1) = \\frac{\\exp(\\tanh(11))}{\\exp(\\tanh(11)) + \\exp(\\tanh(9)) + \\exp(\\tanh(10))} \\approx 0.3333333333333333$\n- 对于 $s_{t} = 5$：$e_{1} = \\tanh(51)$，$e_{2} = \\tanh(49)$，$e_{3} = \\tanh(50)$。\n  $w^{\\mathrm{add}}(5) \\approx 0.3333333333333333$\n- 对于 $s_{t} = 50$：$e_{1} = \\tanh(501)$，$e_{2} = \\tanh(499)$，$e_{3} = \\tanh(500)$。\n  $w^{\\mathrm{add}}(50) \\approx 0.3333333333333333$\n\n对于较大的 $s_{t}$，其行为可以用双曲正切函数的饱和特性来解释。对于一个大的正自变量 $x$，$\\tanh(x) \\approx 1$。当 $s_{t} = 5$ 和 $s_{t} = 50$ 时，$\\tanh$ 函数的自变量是大的正数（分别为 49、50、51 和 499、500、501）。因此，所有三个能量 $e_{1}^{\\mathrm{add}}$、$e_{2}^{\\mathrm{add}}$ 和 $e_{3}^{\\mathrm{add}}$ 都极其接近 1。当所有能量几乎相同时，Softmax 函数会近乎均匀地分配概率质量，导致每个权重都趋近于 $\\frac{1}{3}$。这证明了加性注意力在大尺度下对输入幅度丧失敏感性，这是饱和非线性函数的直接结果。\n\n**3. 布尔属性评估**\n\n- **$B_{\\mathrm{mul}}$**：此属性检查当 $s_{t} \\in \\{0, 0.1, 1, 5, 50\\}$ 时，$w^{\\mathrm{mul}}(s_{t})$ 是否为非递减的。函数 $f(x) = \\frac{\\exp(x)}{\\exp(x) + \\exp(-x) + 1}$ 的导数为 $f'(x) = \\frac{\\exp(x) + 2}{(\\exp(x) + \\exp(-x) + 1)^2}$，对于所有实数 $x$，该导数都严格为正。因此，$w^{\\mathrm{mul}}(s_{t})$ 是 $s_{t}$ 的一个严格递增函数，满足非递减条件。计算出的值证实了这一点：$0.333... \\le 0.367... \\le 0.665... \\le 0.993... \\le 0.999...$。因此，$B_{\\mathrm{mul}}$ 为真。\n\n- **$B_{\\mathrm{add}}$**：此属性检查 $|w^{\\mathrm{add}}(50) - w^{\\mathrm{add}}(5)| \\le \\epsilon$ 是否成立，其中 $\\epsilon = 10^{-12}$。如前所述，当 $s_t=5$ 和 $s_t=50$ 时，$\\tanh$ 函数的所有自变量都很大，导致能量饱和到极其接近 1 的值。这些自变量的微小差异（例如 $\\tanh(49)$ 和 $\\tanh(499)$ 之间）导致的能量差异是指数级小的。随后应用 $\\exp$ 函数和 Softmax 归一化，得到的 $s_{t}=5$ 和 $s_{t}=50$ 时的注意力权重实际上是相同的。数值计算证实，它们的差远小于阈值 $\\epsilon = 10^{-12}$。因此，$B_{\\mathrm{add}}$ 为真。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and verifies properties of additive and multiplicative attention\n    for a given set of encoder and decoder states.\n    \"\"\"\n    # Define givens from the problem statement\n    h = np.array([1, -1, 0])\n    s_t_values = [0, 0.1, 1, 5, 50]\n    tau = 1.0\n\n    # Parameters for additive attention\n    v_add = 1.0\n    w_h_add = 1.0\n    w_s_add = 10.0\n    b_add = 0.0\n\n    # Parameter for multiplicative attention\n    W_mul = 1.0\n    \n    epsilon = 1e-12\n\n    # Lists to store results\n    w_mul_results = []\n    w_add_results = []\n\n    def softmax(energies, temperature):\n        \"\"\"Computes softmax probabilities for a vector of energies.\"\"\"\n        # The temperature is given as 1, so division is nominal.\n        # Stabilize by subtracting the max energy.\n        e_stable = energies / temperature\n        e_stable = e_stable - np.max(e_stable)\n        exps = np.exp(e_stable)\n        return exps / np.sum(exps)\n\n    # Loop through each test case for s_t\n    for s_t in s_t_values:\n        # 1. Multiplicative Attention Calculation\n        energies_mul = s_t * W_mul * h\n        alphas_mul = softmax(energies_mul, tau)\n        w_mul = alphas_mul[0]\n        w_mul_results.append(w_mul)\n\n        # 2. Additive Attention Calculation\n        args_add = w_h_add * h + w_s_add * s_t + b_add\n        energies_add = v_add * np.tanh(args_add)\n        alphas_add = softmax(energies_add, tau)\n        w_add = alphas_add[0]\n        w_add_results.append(w_add)\n\n    # 3. Boolean Property Evaluation\n    # B_mul: Check if w_mul is nondecreasing\n    B_mul = all(w_mul_results[i] = w_mul_results[i+1] for i in range(len(w_mul_results) - 1))\n\n    # B_add: Check for saturation at large magnitudes\n    B_add = abs(w_add_results[4] - w_add_results[3]) = epsilon\n    \n    # Combine all results into a single list\n    final_results = w_mul_results + w_add_results + [B_mul, B_add]\n\n    # Format the output as specified\n    formatted_results = []\n    for r in final_results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r))\n        else:\n            # Format floats to a high precision to show stability\n            formatted_results.append(f\"{r:.16f}\")\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3097423"}, {"introduction": "在上一个练习的基础上，本练习将深入探讨两种机制的表达能力。我们将构建一个包含重复、模糊输入的场景，来观察加性注意力中的非线性如何帮助它创建更复杂的决策边界以区分相似的输入。你将发现，这是简单的双线性形式的乘性注意力可能无法完成的任务，从而理解加性注意力在某些情况下为何是更强大的选择。[@problem_id:3097330]", "problem": "给定一个纯粹以数学术语表述的序列到序列注意力对齐任务。考虑一个编码器生成隐藏状态向量的有限序列，以及一个解码器生成查询向量。注意力机制将查询向量和每个编码器隐藏状态向量映射到一个标量分数，然后使用softmax函数对这些分数进行归一化，以获得序列位置上的注意力权重。必须实现两种不同的注意力评分机制：一种是带有逐点双曲正切的单隐藏层前馈评分函数，另一种是双线性评分函数。您的目标是构建一个包含频繁标记重复的数据集，计算两种注意力权重分布，并确定该机制是否能正确判别哪个位置是预期的对齐目标。您还必须根据一个明确定义的标准，判断加性评分中的非线性是否存在是否能改善消歧效果。\n\n基本原理和定义：\n- 设编码器生成一个由$n$个隐藏状态向量组成的序列 $\\mathbf{h}_1, \\dots, \\mathbf{h}_n \\in \\mathbb{R}^d$，解码器生成一个查询（当前状态）向量 $\\mathbf{s} \\in \\mathbb{R}^d$。注意力评分函数将 $(\\mathbf{s}, \\mathbf{h}_i)$ 映射到一个标量分数 $a_i$。归一化后的注意力权重通过softmax函数计算：对于分数 $a_1, \\dots, a_n$，位置 $i$ 的注意力权重为 $\\alpha_i = \\exp(a_i) \\big/ \\sum_{j=1}^n \\exp(a_j)$。\n- 加性评分必须实现为单隐藏层前馈映射，使用对 $\\mathbf{h}_i$ 的线性变换和对 $\\mathbf{s}$ 的线性变换，与一个偏置相加，通过双曲正切非线性函数，最后由一个读出向量进行线性组合以产生一个标量。具体来说，这需要维度兼容的参数矩阵和向量，并且双曲正切函数是按分量作用的。\n- 乘性评分必须通过一个维度兼容的矩阵，实现为 $\\mathbf{s}$ 和 $\\mathbf{h}_i$ 之间的双线性形式。\n\n数据集构建和参数：\n- 所有向量的维度使用 $d = 4$。对于加性评分机制，隐藏层宽度使用 $p = 3$。\n- 定义两个频繁重复的原型标记向量：$\\mathbf{v}_A = \\left[1, 0, 0, 0\\right]$ 和 $\\mathbf{v}_B = \\left[0, 1, 0, 0\\right]$。定义一个小的扰动向量 $\\boldsymbol{\\delta} = \\left[0, 0.2, 0, 0\\right]$。\n- 对于加性机制，将参数矩阵和向量固定如下：\n  - 令 $\\mathbf{M} \\in \\mathbb{R}^{p \\times d}$ 为 $\\mathbf{M} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  1  0  0 \\end{bmatrix}$。\n  - 令偏置向量 $\\mathbf{b} \\in \\mathbb{R}^p$ 为 $\\mathbf{b} = \\left[0, 0, 0\\right]$。\n  - 令读出向量 $\\mathbf{v} \\in \\mathbb{R}^p$ 为 $\\mathbf{v} = \\left[0.0, 3.0, 3.0\\right]$。\n- 对于乘性机制，双线性矩阵 $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$ 将根据每个测试案例指定。\n\n注意力权重计算：\n- 对于每个位置 $i \\in \\{1, \\dots, n\\}$，计算两种机制的标量分数：\n  - 加性分数是通过首先形成隐藏层预激活 $\\mathbf{z}_i = \\mathbf{M}\\mathbf{h}_i + \\mathbf{M}\\mathbf{s} + \\mathbf{b} \\in \\mathbb{R}^p$，然后对其逐分量应用双曲正切函数得到 $\\tanh(\\mathbf{z}_i)$，最后与 $\\mathbf{v}$ 进行点积得到的 $a^{\\text{add}}_i$。\n  - 乘性分数为双线性形式 $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{W}\\mathbf{h}_i$。\n- 使用softmax对每组分数 $\\{a^{\\text{add}}_i\\}$ 和 $\\{a^{\\text{mult}}_i\\}$ 进行归一化，以分别获得注意力权重 $\\{\\alpha^{\\text{add}}_i\\}$ 和 $\\{\\alpha^{\\text{mult}}_i\\}$。\n\n判别和改进标准：\n- 如果指定正确索引 $i^\\star$ 处的注意力权重严格大于所有其他位置的注意力权重，则称该案例对于某一机制是可判别的。形式上，如果 $\\alpha_{i^\\star}  \\max_{j \\neq i^\\star} \\alpha_j$，则机制可判别。\n- 将一个机制的置信度边际定义为 $\\Delta = \\alpha_{i^\\star} - \\max_{j \\neq i^\\star} \\alpha_j$。\n- 如果满足以下任一条件，则认为加性机制中非线性的存在改善了消歧效果：\n  1. 加性机制能够判别该案例而乘性机制不能，或者\n  2. 两种机制都能判别该案例，但加性机制的置信度边际严格大于乘性机制的置信度边际。\n\n测试套件：\n实现以下四个测试案例，以覆盖一般情况、边界条件和边缘条件。对每一个案例，构建 $\\{\\mathbf{h}_i\\}$、$\\mathbf{s}$，指定 $\\mathbf{W}$，并给出 $i^\\star$。\n1. 案例1（一般情况，预计两种机制都能判别）：$n = 7$。编码器序列：位置 $i \\in \\{1, \\dots, 7\\}$ 处 $\\mathbf{h}_i = \\mathbf{v}_A$，但位置 $i^\\star = 4$ 处 $\\mathbf{h}_{4} = \\mathbf{v}_B$。查询向量 $\\mathbf{s} = \\mathbf{v}_B$。乘性矩阵 $\\mathbf{W} = \\mathbf{I}_4$（$4 \\times 4$ 单位矩阵）。\n2. 案例2（边界情况，乘性机制退化）：$n = 5$。编码器序列：对于所有 $i \\neq i^\\star$，$\\mathbf{h}_i = \\mathbf{v}_A$，且 $\\mathbf{h}_{3} = \\mathbf{v}_B$，其中 $i^\\star = 3$。查询向量 $\\mathbf{s} = \\mathbf{v}_B$。乘性矩阵 $\\mathbf{W} = \\mathbf{0}_{4 \\times 4}$（$4 \\times 4$ 零矩阵）。\n3. 案例3（边缘情况，所有标记相同）：$n = 6$。编码器序列：对于所有 $i$，$\\mathbf{h}_i = \\mathbf{v}_A$。查询向量 $\\mathbf{s} = \\mathbf{v}_A$。指定的正确索引 $i^\\star = 5$。乘性矩阵 $\\mathbf{W} = \\mathbf{I}_4$。\n4. 案例4（带有小扰动的近似重复，乘性机制对被掩盖的维度不敏感）：$n = 8$。编码器序列：对于所有 $i \\neq i^\\star$，$\\mathbf{h}_i = \\mathbf{v}_A$，且 $\\mathbf{h}_{6} = \\mathbf{v}_A + \\boldsymbol{\\delta}$，其中 $i^\\star = 6$。查询向量 $\\mathbf{s} = \\mathbf{v}_A + \\boldsymbol{\\delta}$。乘性矩阵 $\\mathbf{W} = \\operatorname{diag}\\left(1, 0, 1, 1\\right)$。\n\n程序要求：\n- 根据上述定义，使用指定的参数，实现计算加性与乘性分数及注意力权重的函数。\n- 对于每个测试案例，计算：\n  - 一个布尔值，指示加性机制是否能判别正确索引。\n  - 一个布尔值，指示乘性机制是否能判别正确索引。\n  - 一个布尔值，指示加性机制是否根据改进标准改善了消歧效果。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。每个测试案例的结果本身必须是按上述顺序排列的三个布尔值的列表。例如，输出应类似于 $\\left[\\left[\\text{True},\\text{False},\\text{True}\\right],\\dots\\right]$，除了是有效的Python布尔值打印格式外，没有其他与空白相关的约束。", "solution": "经评估，用户提供的问题是**有效的**。这是一个在深度学习领域，特别是关于注意力机制方面，提法得当、有科学依据且客观的问题。所有参数、定义和标准都已提供，从而可以得出一个唯一且可验证的解。\n\n### 1. 数学公式\n\n该问题要求实现和比较两种注意力机制：加性（Bahdanau风格）和乘性（Luong风格）。\n\n设$n$个编码器隐藏状态的序列为 $\\{\\mathbf{h}_i\\}_{i=1}^n$，其中每个 $\\mathbf{h}_i \\in \\mathbb{R}^d$。设解码器查询向量为 $\\mathbf{s} \\in \\mathbb{R}^d$。第$i$个位置的注意力分数是一个标量 $a_i$，相应的注意力权重 $\\alpha_i$ 通过softmax函数计算：\n$$ \\alpha_i = \\frac{\\exp(a_i)}{\\sum_{j=1}^n \\exp(a_j)} $$\n\n**加性注意力分数：**\n加性分数由一个单隐藏层前馈网络定义。给定参数矩阵 $\\mathbf{M} \\in \\mathbb{R}^{p \\times d}$、偏置向量 $\\mathbf{b} \\in \\mathbb{R}^p$ 和读出向量 $\\mathbf{v} \\in \\mathbb{R}^p$，分数 $a^{\\text{add}}_i$ 为：\n$$ a^{\\text{add}}_i = \\mathbf{v}^\\top \\tanh(\\mathbf{M}\\mathbf{h}_i + \\mathbf{M}\\mathbf{s} + \\mathbf{b}) $$\n其中 $\\tanh$ 是逐元素应用的。\n\n**乘性注意力分数：**\n乘性分数由双线性形式定义。给定参数矩阵 $\\mathbf{W} \\in \\mathbb{R}^{d \\times d}$，分数 $a^{\\text{mult}}_i$ 为：\n$$ a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{W} \\mathbf{h}_i $$\n\n### 2. 参数设定与简化\n\n问题指定了以下参数：\n- 向量维度: $d = 4$\n- 加性隐藏层维度: $p = 3$\n- 原型向量: $\\mathbf{v}_A = [1, 0, 0, 0]^\\top$ 和 $\\mathbf{v}_B = [0, 1, 0, 0]^\\top$\n- 扰动向量: $\\boldsymbol{\\delta} = [0, 0.2, 0, 0]^\\top$\n- 加性机制参数:\n  - $\\mathbf{M} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  1  0  0 \\end{bmatrix}$\n  - $\\mathbf{b} = [0, 0, 0]^\\top$\n  - $\\mathbf{v} = [0, 3, 3]^\\top$\n\n对于加性分数，这些特定参数带来了一个显著的简化。对于任意向量 $\\mathbf{x} = [x_1, x_2, x_3, x_4]^\\top \\in \\mathbb{R}^4$，乘积 $\\mathbf{M}\\mathbf{x}$ 为：\n$$ \\mathbf{M}\\mathbf{x} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  1  0  0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_2 \\end{bmatrix} $$\n加性分数的预激活为 $\\mathbf{z}_i = \\mathbf{M}(\\mathbf{h}_i + \\mathbf{s}) + \\mathbf{b}$。令 $\\mathbf{u}_i = \\mathbf{h}_i + \\mathbf{s} = [u_{i1}, u_{i2}, u_{i3}, u_{i4}]^\\top$。由于 $\\mathbf{b} = \\mathbf{0}$，我们有 $\\mathbf{z}_i = [u_{i1}, u_{i2}, u_{i2}]^\\top$。\n最终分数为：\n$$ a^{\\text{add}}_i = \\mathbf{v}^\\top \\tanh(\\mathbf{z}_i) = \\begin{bmatrix} 0 \\\\ 3 \\\\ 3 \\end{bmatrix}^\\top \\begin{bmatrix} \\tanh(u_{i1}) \\\\ \\tanh(u_{i2}) \\\\ \\tanh(u_{i2}) \\end{bmatrix} = 6 \\tanh(u_{i2}) $$\n因此，加性分数简化为 $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$，其中 $h_{i2}$ 和 $s_2$ 分别是 $\\mathbf{h}_i$ 和 $\\mathbf{s}$ 的第二个分量。\n\n### 3. 评估标准\n- **判别**：如果目标索引处的注意力权重 $\\alpha_{i^\\star}$ 严格大于任何其他索引处的最大注意力权重，则一个机制是可判别的：$\\alpha_{i^\\star}  \\max_{j \\neq i^\\star} \\alpha_j$。\n- **置信度边际**：边际为 $\\Delta = \\alpha_{i^\\star} - \\max_{j \\neq i^\\star} \\alpha_j$。\n- **改进**：如果 (1) 加性机制可判别而乘性机制不可，或者 (2) 两者都可判别但 $\\Delta^{\\text{add}}  \\Delta^{\\text{mult}}$，则加性机制改善了消歧效果。\n\n### 4. 按案例分析\n\n**案例 1:**\n- 参数: $n=7$, $i^\\star=4$, $\\mathbf{h}_{i \\ne 4} = \\mathbf{v}_A$, $\\mathbf{h}_4 = \\mathbf{v}_B$, $\\mathbf{s} = \\mathbf{v}_B$, $\\mathbf{W} = \\mathbf{I}_4$。\n- **乘性:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{h}_i$。\n    - 对于 $i \\ne 4$: $a^{\\text{mult}}_i = \\mathbf{v}_B^\\top \\mathbf{v}_A = 0$。\n    - 对于 $i = 4$: $a^{\\text{mult}}_4 = \\mathbf{v}_B^\\top \\mathbf{v}_B = 1$。\n    - 分数: $\\{0,0,0,1,0,0,0\\}$。由于 $1  0$，最大分数在 $i=4$ 处。因此，根据softmax的单调性，该机制可判别。**判别：是**。\n    - 边际: $\\Delta^{\\text{mult}} = \\frac{e^1}{6e^0+e^1} - \\frac{e^0}{6e^0+e^1} = \\frac{e-1}{6+e} \\approx 0.197$。\n- **加性:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$。$s_2 = (\\mathbf{v}_B)_2 = 1$。\n    - 对于 $i \\ne 4$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$。$a^{\\text{add}}_i = 6\\tanh(0+1) = 6\\tanh(1) \\approx 4.570$。\n    - 对于 $i = 4$: $h_{4,2} = (\\mathbf{v}_B)_2 = 1$。$a^{\\text{add}}_4 = 6\\tanh(1+1) = 6\\tanh(2) \\approx 5.784$。\n    - 由于 $6\\tanh(2)  6\\tanh(1)$，该机制可判别。**判别：是**。\n    - 边际: $\\Delta^{\\text{add}} = \\frac{e^{6\\tanh(2)}}{6e^{6\\tanh(1)}+e^{6\\tanh(2)}} - \\frac{e^{6\\tanh(1)}}{6e^{6\\tanh(1)}+e^{6\\tanh(2)}} \\approx 0.253$。\n- **改进:** 两者都可判别，且 $\\Delta^{\\text{add}} \\approx 0.253  \\Delta^{\\text{mult}} \\approx 0.197$。**改进：是**。\n- 结果: `[True, True, True]`\n\n**案例 2:**\n- 参数: $n=5$, $i^\\star=3$, $\\mathbf{h}_{i \\ne 3} = \\mathbf{v}_A$, $\\mathbf{h}_3 = \\mathbf{v}_B$, $\\mathbf{s} = \\mathbf{v}_B$, $\\mathbf{W} = \\mathbf{0}_{4 \\times 4}$。\n- **乘性:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{0}_{4 \\times 4} \\mathbf{h}_i = 0$ 对所有 $i$ 成立。\n    - 所有分数都相等。注意力权重将是均匀的，$\\alpha_i = 1/5$。因此，$\\alpha_3 = \\max_{j \\neq 3} \\alpha_j$，所以该机制不可判别。**判别：否**。\n- **加性:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$。$s_2 = (\\mathbf{v}_B)_2 = 1$。\n    - 对于 $i \\ne 3$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$。$a^{\\text{add}}_i = 6\\tanh(1)$。\n    - 对于 $i = 3$: $h_{3,2} = (\\mathbf{v}_B)_2 = 1$。$a^{\\text{add}}_3 = 6\\tanh(2)$。\n    - 由于 $6\\tanh(2)  6\\tanh(1)$，该机制可判别。**判别：是**。\n- **改进:** 加性机制可判别而乘性机制不可。**改进：是**。\n- 结果: `[True, False, True]`\n\n**案例 3:**\n- 参数: $n=6$, $i^\\star=5$, 对所有 $i$, $\\mathbf{h}_i = \\mathbf{v}_A$, $\\mathbf{s} = \\mathbf{v}_A$, $\\mathbf{W} = \\mathbf{I}_4$。\n- **乘性:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{h}_i = \\mathbf{v}_A^\\top \\mathbf{v}_A = 1$ 对所有 $i$ 成立。\n    - 所有分数都相等。该机制不可判别。**判别：否**。\n- **加性:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$。$s_2 = (\\mathbf{v}_A)_2 = 0$。\n    - 对于所有 $i$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$。$a^{\\text{add}}_i = 6\\tanh(0+0) = 0$。\n    - 所有分数都相等。该机制不可判别。**判别：否**。\n- **改进:** 加性机制不可判别。**改进：否**。\n- 结果: `[False, False, False]`\n\n**案例 4:**\n- 参数: $n=8$, $i^\\star=6$, $\\mathbf{h}_{i \\ne 6}=\\mathbf{v}_A$, $\\mathbf{h}_6=\\mathbf{v}_A+\\boldsymbol{\\delta}$, $\\mathbf{s}=\\mathbf{v}_A+\\boldsymbol{\\delta}$, $\\mathbf{W}=\\operatorname{diag}(1,0,1,1)$。\n- 注意: $\\mathbf{v}_A+\\boldsymbol{\\delta} = [1, 0.2, 0, 0]^\\top$。所以 $\\mathbf{s}=[1, 0.2, 0, 0]^\\top$。\n- **乘性:** $a^{\\text{mult}}_i = \\mathbf{s}^\\top \\mathbf{W} \\mathbf{h}_i$。\n    - 对于 $i \\ne 6$: $\\mathbf{h}_i = \\mathbf{v}_A = [1,0,0,0]^\\top$。$\\mathbf{W}\\mathbf{h}_i = [1,0,0,0]^\\top$。$a^{\\text{mult}}_i = [1,0.2,0,0] \\cdot [1,0,0,0]^\\top = 1$。\n    - 对于 $i = 6$: $\\mathbf{h}_6 = [1,0.2,0,0]^\\top$。$\\mathbf{W}\\mathbf{h}_6 = [1,0,0,0]^\\top$。$a^{\\text{mult}}_6 = [1,0.2,0,0] \\cdot [1,0,0,0]^\\top = 1$。\n    - 扰动在第二个维度上，该维度被 $\\mathbf{W}$ 置零。所有分数都相等。该机制不可判别。**判别：否**。\n- **加性:** $a^{\\text{add}}_i = 6 \\tanh(h_{i2} + s_2)$。$s_2 = (\\mathbf{v}_A+\\boldsymbol{\\delta})_2 = 0.2$。\n    - 对于 $i \\ne 6$: $h_{i2} = (\\mathbf{v}_A)_2 = 0$。$a^{\\text{add}}_i = 6\\tanh(0+0.2) = 6\\tanh(0.2)$。\n    - 对于 $i = 6$: $h_{6,2} = (\\mathbf{v}_A+\\boldsymbol{\\delta})_2 = 0.2$。$a^{\\text{add}}_6 = 6\\tanh(0.2+0.2) = 6\\tanh(0.4)$。\n    - 由于当 $x0$ 时 $\\tanh(x)$ 是严格递增的，所以 $\\tanh(0.4)  \\tanh(0.2)$，因此 $a^{\\text{add}}_6  a^{\\text{add}}_{i \\neq 6}$。该机制可判别。**判别：是**。\n- **改进:** 加性机制可判别而乘性机制不可。**改进：是**。\n- 结果: `[True, False, True]`", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the attention mechanism comparison problem.\n    \"\"\"\n    \n    # Define global parameters\n    d = 4\n    p = 3\n    v_A = np.array([1.0, 0.0, 0.0, 0.0])\n    v_B = np.array([0.0, 1.0, 0.0, 0.0])\n    delta = np.array([0.0, 0.2, 0.0, 0.0])\n\n    M = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0]], dtype=float)\n    b = np.array([0, 0, 0], dtype=float)\n    v = np.array([0.0, 3.0, 3.0])\n\n    def compute_additive_scores(h_states, s):\n        scores = []\n        for h_i in h_states:\n            # As derived in the solution, a_i = 6 * tanh(h_{i2} + s_2)\n            # This is significantly more efficient than full matrix multiplication.\n            u_i2 = h_i[1] + s[1]\n            score = 6 * np.tanh(u_i2)\n            scores.append(score)\n        return np.array(scores)\n\n    def compute_multiplicative_scores(h_states, s, W):\n        scores = []\n        for h_i in h_states:\n            score = s.T @ W @ h_i\n            scores.append(score)\n        return np.array(scores)\n    \n    def softmax(scores):\n        # Numerically stable softmax\n        if scores.size == 0:\n            return np.array([])\n        scores_stable = scores - np.max(scores)\n        exps = np.exp(scores_stable)\n        return exps / np.sum(exps)\n\n    def analyze_attention(alphas, i_star_0based):\n        n = len(alphas)\n        if n == 0:\n            return False, 0.0\n\n        alpha_star = alphas[i_star_0based]\n        \n        other_indices = [j for j in range(n) if j != i_star_0based]\n        if not other_indices:\n            # Case where n=1, resolution is trivially true.\n            return True, alpha_star \n            \n        max_other_alpha = np.max(alphas[other_indices])\n        \n        resolves = alpha_star > max_other_alpha\n        margin = alpha_star - max_other_alpha\n        \n        return resolves, margin\n\n    # Define test cases\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"n\": 7,\n            \"h_seq_builder\": lambda n, i_star_0: [v_B if i == i_star_0 else v_A for i in range(n)],\n            \"s\": v_B,\n            \"i_star\": 4, # 1-based index\n            \"W\": np.identity(d)\n        },\n        {\n            \"name\": \"Case 2\",\n            \"n\": 5,\n            \"h_seq_builder\": lambda n, i_star_0: [v_B if i == i_star_0 else v_A for i in range(n)],\n            \"s\": v_B,\n            \"i_star\": 3,\n            \"W\": np.zeros((d, d))\n        },\n        {\n            \"name\": \"Case 3\",\n            \"n\": 6,\n            \"h_seq_builder\": lambda n, i_star_0: [v_A for _ in range(n)],\n            \"s\": v_A,\n            \"i_star\": 5,\n            \"W\": np.identity(d)\n        },\n        {\n            \"name\": \"Case 4\",\n            \"n\": 8,\n            \"h_seq_builder\": lambda n, i_star_0: [v_A + delta if i == i_star_0 else v_A for i in range(n)],\n            \"s\": v_A + delta,\n            \"i_star\": 6,\n            \"W\": np.diag([1.0, 0.0, 1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        i_star_0based = case[\"i_star\"] - 1\n        h_states = case[\"h_seq_builder\"](n, i_star_0based)\n        s = case[\"s\"]\n        W = case[\"W\"]\n        \n        # Additive mechanism\n        scores_add = compute_additive_scores(h_states, s)\n        alphas_add = softmax(scores_add)\n        add_resolves, add_margin = analyze_attention(alphas_add, i_star_0based)\n        \n        # Multiplicative mechanism\n        scores_mult = compute_multiplicative_scores(h_states, s, W)\n        alphas_mult = softmax(scores_mult)\n        mult_resolves, mult_margin = analyze_attention(alphas_mult, i_star_0based)\n        \n        # Improvement criterion\n        improves = False\n        if add_resolves and not mult_resolves:\n            improves = True\n        elif add_resolves and mult_resolves and add_margin > mult_margin:\n            improves = True\n            \n        results.append([add_resolves, mult_resolves, improves])\n\n    # Format the final output string precisely as requested\n    case_results_str = []\n    for res_list in results:\n        # e.g., \"[True,False,True]\"\n        case_str = f\"[{','.join(str(b) for b in res_list)}]\"\n        case_results_str.append(case_str)\n    \n    final_output = f\"[{','.join(case_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3097330"}, {"introduction": "最后的这个练习将从具体的数值示例转向其背后的理论支撑。我们将对注意力得分在初始化时的统计特性进行数学分析，通过推导乘性注意力得分的方差，揭示一个关键问题：其方差会随着向量维度的增加而增长，这可能导致训练过程不稳定。这个理论练习解释了“缩放点积注意力”（Scaled Dot-Product Attention）背后的动机，将稳定的梯度需求与严格的数学分析联系起来。[@problem_id:3097429]", "problem": "考虑一个带注意力机制的序列到序列模型，其中时间 $t$ 的解码器状态是一个向量 $s_t \\in \\mathbb{R}^{d_h}$，位置 $i$ 的编码器隐藏状态是一个向量 $h_i \\in \\mathbb{R}^{d_h}$。乘法（Luong）能量定义为 $e_i^{\\text{mult}} = s_t^{\\top} W h_i$，其中权重矩阵 $W \\in \\mathbb{R}^{d_h \\times d_h}$。加法（Bahdanau）能量定义为 $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$，其中权重向量 $v \\in \\mathbb{R}^{d_h}$，权重矩阵 $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$。假设存在以下随机初始化和分布假设：\n- $s_t$ 的条目是独立同分布 (i.i.d.) 的，对于所有 $a \\in \\{1, \\dots, d_h\\}$，有 $s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$。\n- $h_i$ 的条目是独立同分布的，对于所有 $b \\in \\{1, \\dots, d_h\\}$，有 $h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$。\n- $W$ 的条目是独立同分布的，有 $W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$，并且与 $s_t$ 和 $h_i$ 独立。\n- $v$ 的条目是独立同分布的，有 $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$，并且与所有其他变量独立。\n- $W_s$ 和 $W_h$ 的条目是独立同分布的，有 $W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ 和 $W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$，并且与 $s_t$、$h_i$ 和 $v$ 独立。\n- 提到的所有随机变量在不同索引和不同参数组之间都是相互独立的，双曲正切函数 $\\tanh(\\cdot)$ 是逐元素应用的。\n从期望和方差的基本定义、独立性性质以及零均值高斯分布的对称性出发，推导出乘法能量的期望 $\\mathbb{E}[e_i^{\\text{mult}}]$ 和方差 $\\operatorname{Var}(e_i^{\\text{mult}})$。然后，将你的方差表达式特化到 $W$ 使用 Xavier 正态初始化的情况，其中 $\\sigma_W^2 = 1/d_h$。接下来，在相同的独立性和对称性假设下，推导出加法能量的期望 $\\mathbb{E}[e_i^{\\text{add}}]$。在 Xavier 正态选择下，明确量化乘法能量的方差如何依赖于关键维度 $d_h$。你的最终答案必须是一个单一的解析表达式，按顺序收集 $\\mathbb{E}[e_i^{\\text{mult}}]$、$\\sigma_W^2 = 1/d_h$ 下的 $\\operatorname{Var}(e_i^{\\text{mult}})$ 和 $\\mathbb{E}[e_i^{\\text{add}}]$ 这三个量。不需要进行数值计算。", "solution": "首先对问题陈述进行严格的验证程序。\n\n### 第 1 步：提取已知条件\n- **解码器状态：** $s_t \\in \\mathbb{R}^{d_h}$\n- **编码器状态：** $h_i \\in \\mathbb{R}^{d_h}$\n- **乘法能量：** $e_i^{\\text{mult}} = s_t^{\\top} W h_i$，其中 $W \\in \\mathbb{R}^{d_h \\times d_h}$\n- **加法能量：** $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$，其中 $v \\in \\mathbb{R}^{d_h}$ 且 $W_s, W_h \\in \\mathbb{R}^{d_h \\times d_h}$\n- **分布假设：**\n  - $s_t$ 的条目：$s_{t,a} \\sim \\mathcal{N}(0, \\sigma_s^2)$ 是独立同分布 (i.i.d.) 的。\n  - $h_i$ 的条目：$h_{i,b} \\sim \\mathcal{N}(0, \\sigma_h^2)$ 是独立同分布的。\n  - $W$ 的条目：$W_{ab} \\sim \\mathcal{N}(0, \\sigma_W^2)$ 是独立同分布的。\n  - $v$ 的条目：$v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$ 是独立同分布的。\n  - $W_s$ 的条目：$W_{s,ka} \\sim \\mathcal{N}(0, \\sigma_{W_s}^2)$ 是独立同分布的。\n  - $W_h$ 的条目：$W_{h,kb} \\sim \\mathcal{N}(0, \\sigma_{W_h}^2)$ 是独立同分布的。\n- **独立性：** 所有指定的随机变量都是相互独立的。\n- **特殊条件：** 对于分析的一部分，假设 $W$ 使用 Xavier 正态初始化，即 $\\sigma_W^2 = 1/d_h$。\n- **目标：** 推导 $\\mathbb{E}[e_i^{\\text{mult}}]$、$\\operatorname{Var}(e_i^{\\text{mult}})$ (及其在 Xavier 初始化下的特化形式)，以及 $\\mathbb{E}[e_i^{\\text{add}}]$。\n\n### 第 2 步：使用提取的已知条件进行验证\n对问题的有效性进行评估：\n- **科学依据：** 该问题牢固地建立在神经网络的理论分析框架内，特别是注意力机制。乘法（Luong）和加法（Bahdanau）注意力的定义是标准的。对权重和状态使用零均值高斯先验的假设，在分析网络初始化行为时很常见。该问题是将概率论和数理统计应用于一个成熟的机器学习概念的标准练习。\n- **适定性：** 该问题提供了一套完整的定义、分布假设和独立性标准，这些都是推导所要求的量（期望和方差）所必需的。目标清晰且在数学上没有歧义，保证了唯一且有意义的解。\n- **客观性：** 问题使用形式化的数学语言陈述，没有任何主观或有偏见的措辞。\n\n该问题没有表现出任何列出的缺陷（例如，科学上不健全、不完整、有歧义）。所有术语都定义明确，前提条件相互一致。\n\n### 第 3 步：结论与行动\n该问题是 **有效的**。现在将提供一个完整、合理的解答。\n\n### 解题推导\n\n**第 1 部分：乘法能量的期望 $\\mathbb{E}[e_i^{\\text{mult}}]$**\n\n乘法能量定义为 $e_i^{\\text{mult}} = s_t^{\\top} W h_i$。这可以写成求和形式：\n$$e_i^{\\text{mult}} = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}$$\n为了求其期望，我们应用期望算子并利用其线性性质：\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\mathbb{E}\\left[\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a} W_{ab} h_{i,b}]$$\n问题陈述所有随机变量 $s_{t,a}$、$W_{ab}$ 和 $h_{i,b}$ 都是相互独立的。因此，它们乘积的期望等于它们各自期望的乘积：\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = \\mathbb{E}[s_{t,a}] \\mathbb{E}[W_{ab}] \\mathbb{E}[h_{i,b}]$$\n根据给定的分布假设，所有这些变量都来自零均值高斯分布：\n- $\\mathbb{E}[s_{t,a}] = 0$\n- $\\mathbb{E}[W_{ab}] = 0$\n- $\\mathbb{E}[h_{i,b}] = 0$\n因此，对于每一组索引 $(a,b)$，我们有：\n$$\\mathbb{E}[s_{t,a} W_{ab} h_{i,b}] = 0 \\cdot 0 \\cdot 0 = 0$$\n将此结果代回求和式，我们得到总期望：\n$$\\mathbb{E}[e_i^{\\text{mult}}] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} 0 = 0$$\n\n**第 2 部分：乘法能量的方差 $\\operatorname{Var}(e_i^{\\text{mult}}]$**\n\n随机变量 $X$ 的方差定义为 $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$。由于我们已经证明 $\\mathbb{E}[e_i^{\\text{mult}}] = 0$，方差简化为平方的期望：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\mathbb{E}\\left[ (e_i^{\\text{mult}})^2 \\right] = \\mathbb{E}\\left[ \\left(\\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} s_{t,a} W_{ab} h_{i,b}\\right)^2 \\right]$$\n我们展开求和的平方：\n$$\\left(\\sum_{a,b} s_{t,a} W_{ab} h_{i,b}\\right)^2 = \\sum_{a,b} \\sum_{c,d} (s_{t,a} W_{ab} h_{i,b}) (s_{t,c} W_{cd} h_{i,d})$$\n其中关于 $a, b, c, d$ 的求和都是从 $1$ 到 $d_h$。取期望：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a,b,c,d} \\mathbb{E}[s_{t,a} s_{t,c} W_{ab} W_{cd} h_{i,b} h_{i,d}]$$\n由于独立性，乘积的期望可以分开。如果任何一个组成随机变量的指数为 1，那么该项的期望将为零，因为所有变量都是零均值的。为了使期望不为零，乘积中的每个随机变量都必须与自身配对。这只在索引匹配时发生：\n- $s_{t,a}$ 必须与 $s_{t,c}$ 配对，这要求 $a=c$。\n- $W_{ab}$ 必须与 $W_{cd}$ 配对，这要求 $(a,b)=(c,d)$。\n- $h_{i,b}$ 必须与 $h_{i,d}$ 配对，这要求 $b=d$。\n条件 $(a,b)=(c,d)$ 包含了另外两个条件。因此，当 $(a,b) \\neq (c,d)$ 时，交叉项的期望为零。我们只需要考虑 $a=c$ 且 $b=d$ 的项：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[(s_{t,a} W_{ab} h_{i,b})^2] = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2 W_{ab}^2 h_{i,b}^2]$$\n根据独立性，这变为：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} \\mathbb{E}[s_{t,a}^2] \\mathbb{E}[W_{ab}^2] \\mathbb{E}[h_{i,b}^2]$$\n对于任何方差为 $\\sigma^2$ 的零均值随机变量 $Z$，我们有 $\\mathbb{E}[Z^2] = \\operatorname{Var}(Z) + (\\mathbb{E}[Z])^2 = \\sigma^2 + 0^2 = \\sigma^2$。应用此公式：\n- $\\mathbb{E}[s_{t,a}^2] = \\sigma_s^2$\n- $\\mathbb{E}[W_{ab}^2] = \\sigma_W^2$\n- $\\mathbb{E}[h_{i,b}^2] = \\sigma_h^2$\n将这些代入求和式：\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = \\sum_{a=1}^{d_h} \\sum_{b=1}^{d_h} (\\sigma_s^2 \\sigma_W^2 \\sigma_h^2)$$\n求和内的项相对于索引 $a$ 和 $b$ 是常数。该求和有 $d_h \\times d_h = d_h^2$ 个相同的项。\n$$\\operatorname{Var}(e_i^{\\text{mult}}) = d_h^2 \\sigma_s^2 \\sigma_W^2 \\sigma_h^2$$\n\n**第 3 部分：针对 Xavier 初始化的特化以及对 $d_h$ 的依赖性**\n\n问题要求将此结果特化到矩阵 $W$ 使用 Xavier 正态初始化的情况，其中 $\\sigma_W^2 = 1/d_h$。将此代入我们的方差表达式：\n$$\\operatorname{Var}(e_i^{\\text{mult}})_{\\text{Xavier}} = d_h^2 \\sigma_s^2 \\left(\\frac{1}{d_h}\\right) \\sigma_h^2 = d_h \\sigma_s^2 \\sigma_h^2$$\n这个结果明确地量化了能量方差对隐藏维度 $d_h$ 的依赖性。在 Xavier 初始化下，乘法注意力能量的方差随 $d_h$ 线性增长。这种缩放行为是“缩放点积注意力”机制的一个主要动机，该机制将能量除以 $\\sqrt{d_k}$（其中 $d_k$ 是键的维度，此处等价于 $d_h$）以抵消这种增长并在训练期间稳定梯度。\n\n**第 4 部分：加法能量的期望 $\\mathbb{E}[e_i^{\\text{add}}]$**\n\n加法能量定义为 $e_i^{\\text{add}} = v^{\\top} \\tanh(W_s s_t + W_h h_i)$。以求和形式表示：\n$$e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k \\tanh((W_s s_t + W_h h_i)_k)$$\n其中下标 $k$ 表示结果向量的第 $k$ 个元素。令 $z_k = \\tanh((W_s s_t + W_h h_i)_k)$。表达式变为 $e_i^{\\text{add}} = \\sum_{k=1}^{d_h} v_k z_k$。\n为了求期望，我们再次使用线性性质：\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\mathbb{E}\\left[\\sum_{k=1}^{d_h} v_k z_k\\right] = \\sum_{k=1}^{d_h} \\mathbb{E}[v_k z_k]$$\n项 $z_k$ 是 $s_t$、$h_i$、$W_s$ 和 $W_h$ 中随机变量的函数。问题陈述向量 $v$ 的条目与所有其他变量独立。因此，对于每个 $k$，$v_k$ 与 $z_k$ 独立。这使我们可以分离期望：\n$$\\mathbb{E}[v_k z_k] = \\mathbb{E}[v_k] \\mathbb{E}[z_k]$$\n我们已知 $v_k \\sim \\mathcal{N}(0, \\sigma_v^2)$，这意味着 $\\mathbb{E}[v_k] = 0$。\n因此，对于每个 $k$：\n$$\\mathbb{E}[v_k z_k] = 0 \\cdot \\mathbb{E}[z_k] = 0$$\n无论 $\\mathbb{E}[z_k]$ 的值是多少，这个结论都成立。总期望为：\n$$\\mathbb{E}[e_i^{\\text{add}}] = \\sum_{k=1}^{d_h} 0 = 0$$\n值得注意的是，$\\mathbb{E}[z_k]$ 也为零。$\\tanh$ 函数的参数，我们称之为 $u_k = (W_s s_t + W_h h_i)_k$，是独立的零均值随机变量乘积之和。因此，其分布关于零对称。由于 $\\tanh(x)$ 是一个奇函数（$\\tanh(-x) = -\\tanh(x)$），在一个对称分布上 $\\mathbb{E}[\\tanh(u_k)]$ 的期望必然为零。然而，利用 $v$ 的独立性及其零均值是得出结果的最直接途径。\n\n### 结果总结\n1.  **乘法能量的期望：** $\\mathbb{E}[e_i^{\\text{mult}}] = 0$\n2.  **乘法能量的方差 (Xavier 初始化)：** $\\operatorname{Var}(e_i^{\\text{mult}}) = d_h \\sigma_s^2 \\sigma_h^2$\n3.  **加法能量的期望：** $\\mathbb{E}[e_i^{\\text{add}}] = 0$\n将这三个量收集到一个行矩阵中作为最终答案。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  d_h \\sigma_s^2 \\sigma_h^2  0\n\\end{pmatrix}\n}\n$$", "id": "3097429"}]}