{"hands_on_practices": [{"introduction": "在自回归模型（如GPT）中，一个核心要求是模型在预测下一个词元时，只能利用过去的信息。这是通过在自注意力机制中应用因果掩码（causal mask）来实现的。本练习将通过第一性原理推导和代码实践，帮助你理解掩码在数学上是如何实现的，以及不正确的掩码为何会导致灾难性的“信息泄漏”，从而让你掌握构建解码器时的一个基本且关键的技巧 [@problem_id:3193602]。", "problem": "你需要从第一性原理出发，对Transformer架构中的掩码注意力进行推理。仅可使用以下基础：SoftMax函数的定义、基本矩阵乘法、指数和对数函数的性质，以及缩放点积注意力的定义。你的任务是推导并凭经验验证，在SoftMax之前应用二元掩码等同于在指数域中与该掩码进行按元素乘法，并演示不正确的掩码如何导致未来词元(token)向过去输出的信息泄露。\n\n用作起点的定义：\n- 对于一个向量 $x \\in \\mathbb{R}^n$ 的SoftMax函数为 $$\n\\operatorname{SoftMax}(x)_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}} \\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\n- 对于查询 $Q \\in \\mathbb{R}^{L \\times d_k}$ 和键 $K \\in \\mathbb{R}^{L \\times d_k}$ 的缩放点积注意力（SDPA）logits为 $$\nZ = \\frac{QK^\\top}{\\sqrt{d_k}} \\in \\mathbb{R}^{L \\times L}.\n$$\n- 一个二元掩码 $M \\in \\{0,1\\}^{L \\times L}$ 用 $0$ 表示不允许的位置，用 $1$ 表示允许的位置。哈达玛（按元素）积表示为 $Z \\odot M$。\n\n你的程序必须：\n- 根据上述定义（在你自己的推理中，而非在程序内部）推导出，在SoftMax之前将一个加性掩码 $A$（其条目为 $A_{ij} = \\log M_{ij}$）加到logits $Z$ 上，等同于在归一化之前，在指数域中与 $M$ 进行按元素乘法。即，掩码后的SoftMax等于 $$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} e^{Z_{:,j}} \\odot M_{:,j}} \\quad \\text{row-wise}.\n$$\n- 解释为什么对被掩码的条目使用一个有限大的负常数 $-C$ 代替 $-\\infty$ 会导致在被掩码的位置上产生微小但非零的概率质量，如果掩码不正确，这可能导致信息泄露。\n\n然后实现一个单一的程序，使用下面给出的确切数值参数计算以下四个测试用例，并按规定格式输出其结果。\n\n给定的常数和矩阵：\n- 使用序列长度 $L = 4$，键维度 $d_k = 3$，以及值维度 $d_v = 2$。\n- 使用\n$$\nQ = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n1  1  1\n\\end{bmatrix}, \\quad\nK = \\begin{bmatrix}\n1  0  0\\\\\n0  1  0\\\\\n0  0  1\\\\\n1  1  1\n\\end{bmatrix}, \\quad\nV = \\begin{bmatrix}\n1  2\\\\\n3  4\\\\\n5  6\\\\\n7  8\n\\end{bmatrix}.\n$$\n- 定义正确的因果二元掩码 $M^{\\mathrm{causal}} \\in \\{0,1\\}^{4 \\times 4}$ 为：如果 $j \\le i$，则 $M^{\\mathrm{causal}}_{ij} = 1$，否则 $M^{\\mathrm{causal}}_{ij} = 0$。\n- 定义不正确的差一二元掩码 $M^{\\mathrm{off}} \\in \\{0,1\\}^{4 \\times 4}$ 为：如果 $j \\le i+1$，则 $M^{\\mathrm{off}}_{ij} = 1$，否则 $M^{\\mathrm{off}}_{ij} = 0$。\n- 为了进行演示性修改，定义 $V^{\\mathrm{mod}}$ 与 $V$ 相等，但其最后一行更改为 $[700, 800]$。\n\n此外，对于SoftMax掩码等价性测试，使用以下较小的logits和掩码：\n$$\nZ^{(a)} = \\begin{bmatrix}\n0.2  -0.1  0.4\\\\\n1.0  -1.0  0.0\n\\end{bmatrix}, \\quad\nM^{(a)} = \\begin{bmatrix}\n1  0  1\\\\\n0  1  1\n\\end{bmatrix}.\n$$\n\n需要实现和计算的测试套件：\n- 案例1（SoftMax前作为乘法的掩码等价性）：用两种方式在带掩码 $M^{(a)}$ 的 $Z^{(a)}$ 上按行计算SoftMax：\n  - 理想乘法形式：分子为 $e^{Z^{(a)}} \\odot M^{(a)}$，分母为该分子的按行总和。\n  - 加法近似：对于被掩码的位置，在logits中使用一个大的负常数 $-C$，$C = 10^9$。\n  将两个结果概率矩阵之间的最大绝对差报告为一个浮点数。\n- 案例2（使用正确因果掩码无信息泄露）：使用 $Q, K, V$ 和 $M^{\\mathrm{causal}}$ 计算注意力输出 $Y$。然后使用 $Q, K, V^{\\mathrm{mod}}$ 和 $M^{\\mathrm{causal}}$ 重新计算。检查输出的前3行（位置0、1和2）在 $10^{-12}$ 的绝对容忍度内是否保持不变。如果不变则报告布尔值true，否则报告false。\n- 案例3（使用不正确掩码导致信息泄露）：使用 $Q, K, V$ 和 $M^{\\mathrm{off}}$ 计算注意力输出，然后使用 $V^{\\mathrm{mod}}$ 和 $M^{\\mathrm{off}}$ 再次计算。检查前3行中是否有任何一行的变化超过 $10^{-6}$ 的绝对容忍度。如果检测到泄露则报告布尔值true，否则报告false。\n- 案例4（有限掩码边界效应）：重复案例1，但在加法近似中使用 $C = 50$ 代替 $C = 10^9$。将最大绝对差报告为一个浮点数。\n\n不涉及角度单位。不涉及物理单位。所有报告的数值答案必须是指定的布尔值或浮点数。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\text{案例1_浮点数}, \\text{案例2_布尔值}, \\text{案例3_布尔值}, \\text{案例4_浮点数}]$。例如，一个语法上有效的格式是 $[0.0,True,False,1e-12]$。", "solution": "该问题陈述被评估为有效。它在科学上基于Transformer架构的数学原理，其定义和参数完整且一致，问题提出得当，表述客观。这些任务要求从第一性原理进行严格推导并进行经验验证，这两者都是科学和工程学科的核心。满足有效问题的所有条件。\n\n问题的核心在于理解在缩放点积注意力机制中，掩码是如何在SoftMax函数中实现的，以及不完美实现所带来的后果。\n\n**第1部分：掩码等价性推导**\n\n我们的任务是推导：在应用SoftMax函数之前，将一个加性掩码 $A$（其中 $A_{ij} = \\log M_{ij}$）加到logits $Z$ 上，等同于在指数域中与二元掩码 $M$ 进行按元素乘法。\n\n设 $Z \\in \\mathbb{R}^{L \\times L}$ 为注意力logits矩阵，设 $M \\in \\{0, 1\\}^{L \\times L}$ 为一个二元掩码，其中 $M_{ij}=1$ 表示允许的注意力连接，$M_{ij}=0$ 表示不允许的连接。我们定义一个加性掩码矩阵 $A \\in (\\mathbb{R} \\cup \\{-\\infty\\})^{L \\times L}$，其条目为 $A_{ij} = \\log M_{ij}$。\n\nSoftMax函数按行应用于logits矩阵。对于给定的行 $i$，到列 $j$ 的连接的掩码SoftMax概率是基于加性掩码后的logits $Z'_{ij} = Z_{ij} + A_{ij}$ 计算的。\n\n根据提供的SoftMax函数定义，对于行 $i$：\n$$\n\\operatorname{SoftMax}(Z'_i)_j = \\frac{e^{Z'_{ij}}}{\\sum_{k=1}^L e^{Z'_{ik}}}\n$$\n代入 $Z'_{ij} = Z_{ij} + A_{ij} = Z_{ij} + \\log M_{ij}$：\n$$\n\\operatorname{SoftMax}(Z_i + \\log M_i)_j = \\frac{e^{Z_{ij} + \\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik} + \\log M_{ik}}}\n$$\n利用指数函数的性质 $e^{a+b} = e^a e^b$，我们可以将表达式重写为：\n$$\n\\frac{e^{Z_{ij}} e^{\\log M_{ij}}}{\\sum_{k=1}^L e^{Z_{ik}} e^{\\log M_{ik}}}\n$$\n现在，我们分析 $e^{\\log M_{ij}}$ 这一项。二元掩码 $M$ 的条目为 $1$ 或 $0$。\n- 如果 $M_{ij} = 1$，那么 $\\log M_{ij} = \\log 1 = 0$。因此，$e^{\\log M_{ij}} = e^0 = 1$。所以，$e^{\\log M_{ij}} = M_{ij}$。\n- 如果 $M_{ij} = 0$，那么 $\\log M_{ij} = \\log 0$。在此上下文所需的极限意义下，$\\log 0 \\to -\\infty$。因此，$e^{\\log M_{ij}} \\to e^{-\\infty} = 0$。所以，$e^{\\log M_{ij}} = M_{ij}$。\n\n在这两种情况下，都有 $e^{\\log M_{ij}} = M_{ij}$。将此恒等式代回主表达式：\n$$\n\\frac{e^{Z_{ij}} M_{ij}}{\\sum_{k=1}^L e^{Z_{ik}} M_{ik}}\n$$\n这个表达式可以用哈达玛（按元素）积表示法来写。设 $E$ 是一个矩阵，其条目为 $E_{ij}=e^{Z_{ij}}$。那么分子是 $(E \\odot M)_{ij}$。分母是矩阵 $E \\odot M$ 的第 $i$ 行的总和。这证明了等价性：\n$$\n\\operatorname{SoftMax}(Z + \\log M) = \\frac{e^{Z} \\odot M}{\\sum_{j} (e^{Z} \\odot M)_{:,j}} \\quad \\text{(按行)}\n$$\n这个推导正式表明，将掩码的对数加到logits上，在数学上等同于在指数化之后、SoftMax归一化步骤之前乘以掩码。\n\n**第2部分：使用不正确或有限掩码导致的信息泄露**\n\n在实际的浮点实现中，我们无法表示 $-\\infty$。取而代之的是，我们用一个大数量级的负数 $-C$ 来近似 $\\log 0$，其中 $C$ 是一个大的正常数（例如，$10^9$）。\n\n加性掩码的应用如下：对于一个不允许的连接（$M_{ij}=0$），我们将 $-C$ 加到logit $Z_{ij}$ 上。SoftMax计算中的相应项变为：\n$$\ne^{Z_{ij} - C} = e^{Z_{ij}} e^{-C}\n$$\n对于一个大的 $C$，$e^{-C}$ 的值极小，但关键在于它非零。这意味着注意力矩阵中不允许的位置将被分配一个微小但非零的概率质量，而理想的数学公式会给它们分配精确为零的概率。\n\n当这个非零概率与一个**不正确的掩码**结合时，就会发生信息泄露。对于解码器，一个正确的因果掩码 $M^{\\mathrm{causal}}$ 确保对于任何查询位置 $i$，对所有键位置 $j > i$ 的注意力权重 $P_{ij}$ 都为零。这可以防止一个词元（token）“看到”未来的词元。\n\n给定的不正确掩码 $M^{\\mathrm{off}}$ 定义为 $j \\le i+1$ 时 $M^{\\mathrm{off}}_{ij} = 1$。这允许位置 $i$ 的查询关注到位置 $i+1$ 的键/值，即未来一个步骤。例如，对于 $i=2$，它可以关注到 $j=0, 1, 2, 3$。由于它可以关注到位置 $j=3$，如果值向量 $V_3$ 发生变化，位置 $Y_2$ 的输出也将发生变化。\n\n查询 $i$ 的注意力输出是一个加权和：\n$$\nY_i = \\sum_{j=1}^L P_{ij} V_j\n$$\n如果使用 $M^{\\mathrm{off}}$，注意力概率 $P_{2,3}$ 将为非零。让我们比较使用原始值 $V$ 的输出 $Y_2$ 和使用修改后值 $V^{\\mathrm{mod}}$ 的输出 $Y'_2$。位置 $2$ 的输出变化为：\n$$\nY'_2 - Y_2 = \\sum_{j=1}^L P_{2,j} V^{\\mathrm{mod}}_j - \\sum_{j=1}^L P_{2,j} V_j = \\sum_{j=1}^L P_{2,j} (V^{\\mathrm{mod}}_j - V_j)\n$$\n由于 $V^{\\mathrm{mod}}$ 仅在最后一行（索引3）与 $V$ 不同，这可以简化为：\n$$\nY'_2 - Y_2 = P_{2,3} (V^{\\mathrm{mod}}_3 - V_3)\n$$\n由于 $M^{\\mathrm{off}}$ 允许从 $i=2$ 到 $j=3$ 的注意力，$P_{2,3}$ 是非零的。问题中定义了 $V_3$ 的一个非常大的变化，所以 $(V^{\\mathrm{mod}}_3 - V_3)$ 很大。这个乘积代表了一个“过去”词元的输出变化，它变得很显著。这直接证明了信息从未来（步骤3的值）泄漏到了过去（步骤2的输出）。\n\n使用正确的因果掩码 $M^{\\mathrm{causal}}$，权重 $P_{2,3}$ 将精确为零，因此 $Y_2$ 将完全不受 $V_3$ 任何变化的影响。\n\n在有限近似中，常数 $C$ 的大小决定了被掩码的概率与零的接近程度。一个较小的 $C$（如 $50$）会导致 $e^{-50}$ 比 $e^{-10^9}$ 大，从而导致与理想零概率情况的偏差更大，以及更显著的“泄露”或数值误差。这就是案例4所演示的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs all calculations for the four test cases regarding\n    masked attention in Transformers.\n    \"\"\"\n\n    # --- Given Constants and Matrices ---\n    L = 4\n    dk = 3\n    dv = 2\n\n    Q = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    K = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 1]\n    ], dtype=np.float64)\n\n    V = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]\n    ], dtype=np.float64)\n\n    V_mod = np.array([\n        [1, 2],\n        [3, 4],\n        [5, 6],\n        [700, 800]\n    ], dtype=np.float64)\n\n    # M_causal: j = i\n    M_causal = np.tril(np.ones((L, L), dtype=np.float64))\n\n    # M_off: j = i + 1\n    i_indices = np.arange(L).reshape(-1, 1)\n    j_indices = np.arange(L).reshape(1, -1)\n    M_off = (j_indices = i_indices + 1).astype(np.float64)\n\n    # Matrices for smaller test cases\n    Z_a = np.array([\n        [0.2, -0.1, 0.4],\n        [1.0, -1.0, 0.0]\n    ], dtype=np.float64)\n\n    M_a = np.array([\n        [1, 0, 1],\n        [0, 1, 1]\n    ], dtype=np.float64)\n\n    # --- Helper Function for Attention Calculation ---\n    def compute_attention(q_mat, k_mat, v_mat, mask, C=1e9):\n        \"\"\"\n        Computes Scaled Dot-Product Attention with an additive mask.\n        Uses a numerically stable softmax.\n        \"\"\"\n        dk_val = q_mat.shape[1]\n        logits = (q_mat @ k_mat.T) / np.sqrt(dk_val)\n        \n        # Apply the additive mask\n        additive_mask = (mask - 1) * C\n        masked_logits = logits + additive_mask\n        \n        # Numerically stable softmax\n        shifted_logits = masked_logits - np.max(masked_logits, axis=1, keepdims=True)\n        attention_weights = np.exp(shifted_logits)\n        attention_weights /= np.sum(attention_weights, axis=1, keepdims=True)\n        \n        output = attention_weights @ v_mat\n        return output\n\n    # --- Test Cases ---\n    results = []\n\n    # Case 1: Equivalence of masking with C = 1e9\n    # Ideal multiplicative form\n    exp_Z_a = np.exp(Z_a)\n    numerator_ideal = exp_Z_a * M_a\n    denominator_ideal = np.sum(numerator_ideal, axis=1, keepdims=True)\n    # Handle cases where a whole row is masked, to avoid division by zero\n    denominator_ideal[denominator_ideal == 0] = 1.0 \n    P_ideal = numerator_ideal / denominator_ideal\n    \n    # Additive approximation with C = 1e9\n    C1 = 1e9\n    additive_mask_1 = (M_a - 1) * C1\n    Z_masked_1 = Z_a + additive_mask_1\n    # Stable softmax for additive form\n    shifted_Z_masked_1 = Z_masked_1 - np.max(Z_masked_1, axis=1, keepdims=True)\n    exp_Z_masked_1 = np.exp(shifted_Z_masked_1)\n    P_approx_1 = exp_Z_masked_1 / np.sum(exp_Z_masked_1, axis=1, keepdims=True)\n    \n    case1_diff = np.max(np.abs(P_ideal - P_approx_1))\n    results.append(case1_diff)\n\n    # Case 2: No leakage with correct causal mask\n    Y1 = compute_attention(Q, K, V, M_causal)\n    Y2 = compute_attention(Q, K, V_mod, M_causal)\n    case2_unchanged = bool(np.allclose(Y1[:3, :], Y2[:3, :], atol=1e-12, rtol=0))\n    results.append(case2_unchanged)\n\n    # Case 3: Leakage with incorrect mask\n    Y3 = compute_attention(Q, K, V, M_off)\n    Y4 = compute_attention(Q, K, V_mod, M_off)\n    diff = np.abs(Y3[:3, :] - Y4[:3, :])\n    case3_leakage_detected = bool(np.any(diff > 1e-6))\n    results.append(case3_leakage_detected)\n\n    # Case 4: Finite-mask boundary effect with C = 50\n    C4 = 50.0\n    additive_mask_4 = (M_a - 1) * C4\n    Z_masked_4 = Z_a + additive_mask_4\n    # Stable softmax\n    shifted_Z_masked_4 = Z_masked_4 - np.max(Z_masked_4, axis=1, keepdims=True)\n    exp_Z_masked_4 = np.exp(shifted_Z_masked_4)\n    P_approx_4 = exp_Z_masked_4 / np.sum(exp_Z_masked_4, axis=1, keepdims=True)\n    \n    case4_diff = np.max(np.abs(P_ideal - P_approx_4))\n    results.append(case4_diff)\n    \n    # Final print statement\n    print(f\"[{results[0]},{str(results[1])},{str(results[2])},{results[3]}]\")\n\nsolve()\n```", "id": "3193602"}, {"introduction": "尽管点积注意力机制功能强大，但它并非没有弱点。本练习将向你展示一种称为“注意力劫持”的对抗性攻击：攻击者可以通过注入一个具有超大范数的键向量，使其在softmax计算中占据主导地位，从而迫使模型关注无关的恶意输入。通过亲手实现这种攻击以及两种常见的防御策略——范数裁剪（norm clipping）和余弦相似度归一化（cosine similarity normalization），你将深刻理解注意力机制的鲁棒性，并学会如何加固你的模型 [@problem_id:3193536]。", "problem": "请考虑 Transformer 架构中使用的缩放点积注意力 (SDPA) 机制。给定一组查询 (Q)、键 (K) 和值 (V) 向量，通过对一组注意力 logit 应用 softmax 函数来形成注意力权重。注意力 logit 来自查询向量与每个键向量之间的内积，并按键维度平方根的倒数进行缩放。softmax 函数将任意实值分数转换为键上的概率分布。假设以下经过充分测试的定义：向量之间的内积是逐元素乘积的和，欧几里得范数是元素平方和的平方根，softmax 函数通过对每个元素进行指数运算并除以指数之和进行归一化，将实数向量映射到非负实数向量，且其和为一。\n\n当攻击者添加一个具有异常大范数的新键 token 时，可能会发生对抗性“注意力劫持”，从而与查询产生巨大的内积并主导 softmax 分布。本问题要求您通过计算来演示这种攻击如何主导注意力，并实现两种缓解策略：键的范数裁剪和 logit 的余弦相似度归一化。\n\n您必须从第一性原理实现以下内容：\n- 对于单个查询 $Q$ 和一组维度为 $d_k$ 的键 $\\{K_i\\}$，计算缩放点积注意力 logit，将每个 logit 按 $1/\\sqrt{d_k}$ 缩放，并应用 softmax 函数以获得注意力权重。\n- 通过附加一个方向已指定且范数设置为一个较大值的攻击键 $K_{\\text{attack}}$ 来演示对抗性劫持。\n- 实现两种缓解措施：\n    1. 范数裁剪：对于裁剪上限 $c$，将每个键 $K_i$ 替换为 $\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i$，应用于所有键，包括 $K_{\\text{attack}}$。\n    2. 余弦相似度归一化：在应用 softmax 函数之前，将每个 logit 替换为 $\\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$，其中 $\\varepsilon > 0$ 是一个小数。\n\n对于下面的每个测试用例，计算四个量：\n- 基线情况下（攻击 token 范数设置为 $1$）攻击 token 的注意力权重。\n- 对抗条件下（攻击 token 范数设置为一个较大值）攻击 token 的注意力权重。\n- 对抗性键应用范数裁剪后（裁剪上限为 $c$）攻击 token 的注意力权重。\n- 对抗性键应用余弦相似度归一化后（使用一个小的 $\\varepsilon$ 以避免除以零）攻击 token 的注意力权重。\n\n您的程序必须使用以下测试套件。在每种情况下，攻击 token 都作为最后一个键附加。请完全按照给定的向量使用。\n\n测试用例 $1$ (正常情况：查询与攻击方向对齐):\n- $d_k = 4$\n- $Q = [0.9, 0.1, 0.0, 0.0]$\n- 正常键: $K_1 = [1, 0, 0, 0]$, $K_2 = [0, 1, 0, 0]$, $K_3 = [0, 0, 1, 0]$\n- 攻击方向: $K_{\\text{dir}} = [1, 0, 0, 0]$\n- 基线攻击范数: $1$\n- 对抗攻击范数: $100$\n- 裁剪上限: $2$\n\n测试用例 $2$ (边界情况：零查询向量):\n- $d_k = 4$\n- $Q = [0.0, 0.0, 0.0, 0.0]$\n- 正常键: $K_1 = [1, 0, 0, 0]$, $K_2 = [0, 1, 0, 0]$, $K_3 = [0, 0, 1, 0]$\n- 攻击方向: $K_{\\text{dir}} = [0, 0, 0, 1]$\n- 基线攻击范数: $1$\n- 对抗攻击范数: $100$\n- 裁剪上限: $2$\n\n测试用例 $3$ (查询与攻击方向正交):\n- $d_k = 4$\n- $Q = [0.0, 1.0, 0.0, 0.0]$\n- 正常键: $K_1 = [0, 1, 0, 0]$, $K_2 = [0, 0, 1, 0]$, $K_3 = [0, 0, 0, 1]$\n- 攻击方向: $K_{\\text{dir}} = [1, 0, 0, 0]$\n- 基线攻击范数: $1$\n- 对抗攻击范数: $100$\n- 裁剪上限: $2$\n\n测试用例 $4$ (极端对抗范数):\n- $d_k = 8$\n- $Q = [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0]$\n- 正常键: $K_1 = [0, 1, 0, 0, 0, 0, 0, 0]$, $K_2 = [0, 0, 1, 0, 0, 0, 0, 0]$, $K_3 = [0, 0, 0, 1, 0, 0, 0, 0]$\n- 攻击方向: $K_{\\text{dir}} = [1, 0, 0, 0, 0, 0, 0, 0]$\n- 基线攻击范数: $1$\n- 对抗攻击范数: $10^6$\n- 裁剪上限: $10$\n\n对于每个测试用例，您的程序必须按上述顺序计算一个包含四个浮点数的列表。您的程序应生成一行输出，其中包含一个逗号分隔的列表，用方括号括起来，每个元素是对应测试用例的四元数列表。例如：$[[w_{1,\\text{base}}, w_{1,\\text{adv}}, w_{1,\\text{clip}}, w_{1,\\text{cos}}],[w_{2,\\text{base}}, w_{2,\\text{adv}}, w_{2,\\text{clip}}, w_{2,\\text{cos}}],\\ldots]$。", "solution": "该问题要求实现并演示对缩放点积注意力 (SDPA) 机制的对抗性攻击，以及两种缓解策略。我将首先将操作形式化，然后概述问题每个部分的计算步骤。\n\n### 1. 基本概念\n\n**缩放点积注意力：**\n注意力机制根据查询向量 $Q$ 计算一组键值对上的权重分布 $\\alpha_i$。对于单个查询 $Q \\in \\mathbb{R}^{d_k}$ 和一组键 $\\{K_1, K_2, \\ldots, K_N\\}$，其中每个 $K_i \\in \\mathbb{R}^{d_k}$，注意力 logit $e_i$ 的计算方式如下：\n$$\ne_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}\n$$\n这里，$d_k$ 是键向量的维度。然后通过对 logit 向量应用 softmax 函数获得注意力权重 $\\alpha_i$：\n$$\n\\alpha_i = \\text{softmax}(e)_i = \\frac{\\exp(e_i)}{\\sum_{j=1}^{N} \\exp(e_j)}\n$$\n注意力层的输出是值向量的加权和 $\\sum_i \\alpha_i V_i$，但这不属于本问题的范畴。\n\n**对抗性劫持：**\n内积 $Q^\\top K_i$ 与键向量的范数 $\\|K_i\\|$ 成正比。攻击者可以利用这一点，引入一个具有极大范数的对抗性键 $K_{\\text{attack}}$。如果 $K_{\\text{attack}}$ 与 $Q$ 不正交，那么产生的点积 $Q^\\top K_{\\text{attack}}$ 将会很大，导致 logit $e_{\\text{attack}}$ 很大。因此，在指数化之后，$\\exp(e_{\\text{attack}})$ 将在 softmax 分母的和中占主导地位，导致注意力权重 $\\alpha_{\\text{attack}}$ 趋近于 $1$。这实际上迫使模型几乎只关注对抗性 token，“劫持”了注意力机制。\n\n### 2. 缓解策略\n\n需要实现两种缓解策略。\n\n**缓解策略 1：范数裁剪**\n此方法对所有键向量强制施加一个最大范数 $c$。每个键 $K_i$ 被替换为裁剪后的版本 $\\tilde{K}_i$：\n$$\n\\tilde{K}_i = \\min\\left(1, \\frac{c}{\\|K_i\\|}\\right) K_i\n$$\n如果一个键的范数 $\\|K_i\\|$ 已经小于或等于裁剪上限 $c$，它将保持不变。如果 $\\|K_i\\| > c$，它将被缩小，使其新范数恰好为 $c$。这可以防止任何单个键具有任意大的范数，从而对 logit 产生过大的影响。对于此计算，我们注意到对于零范数的键可能存在除以零的情况，但这在测试数据中不存在。\n\n**缓解策略 2：余弦相似度归一化**\n该策略用查询向量和键向量之间的余弦相似度替换缩放点积 logit。logit $e_i$ 被重新定义为：\n$$\ne'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}\n$$\n余弦相似度在本质上已通过向量的模长进行归一化，使其对范数不敏感。所得 logit 的值被限制在 $[-1, 1]$ 区间内。添加小的常数 $\\varepsilon > 0$ 是为了数值稳定性，以防止在 $Q$ 或 $K_i$ 是零向量时出现除以零的情况。在此实现中，将使用标准值 $\\varepsilon = 10^{-8}$。请注意，此公式替换了整个缩放点积，包括 $1/\\sqrt{d_k}$ 缩放因子。\n\n### 3. 计算流程\n\n对于每个测试用例，我们必须在四种不同场景下计算攻击 token 的注意力权重。攻击键 $K_{\\text{attack}}$ 是通过将给定的方向向量 $K_{\\text{dir}}$ 缩放到指定范数来构建的。如果 $\\|K_{\\text{dir}}\\| \\neq 0$，则 $K_{\\text{attack}} = (\\text{norm} / \\|K_{\\text{dir}}\\| ) \\cdot K_{\\text{dir}}$。攻击键始终作为序列中的最后一个键附加。\n\n设正常键集为 $\\{K_1, \\ldots, K_{N-1}\\}$，攻击键为 $K_N = K_{\\text{attack}}$。\n\n**A. 基线注意力：**\n1. 使用指定的 `baseline_attack_norm` 构建 $K_{\\text{attack}}$。\n2. 形成完整的键集 $\\{K_1, \\ldots, K_{N-1}, K_{\\text{attack}}\\}$。\n3. 对所有 $i=1, \\ldots, N$ 计算缩放点积 logit $e_i = \\frac{Q^\\top K_i}{\\sqrt{d_k}}$。\n4. 应用 softmax 函数以获得权重 $\\{\\alpha_1, \\ldots, \\alpha_N\\}$。\n5. 结果是 $\\alpha_N$。\n\n**B. 对抗性注意力：**\n1. 使用 `adversarial_attack_norm` 构建 $K_{\\text{attack}}$。\n2. 使用这个新的对抗性键形成完整的键集。\n3. 重复基线流程中的步骤 3-5。\n4. 结果是新的 $\\alpha_N$。\n\n**C. 范数裁剪缓解：**\n1. 使用对抗场景中的键集，包括高范数的 $K_{\\text{attack}}$。\n2. 使用指定的裁剪上限 $c$，对每个键 $K_i$ 应用范数裁剪公式，生成一组裁剪后的键 $\\{\\tilde{K}_1, \\ldots, \\tilde{K}_N\\}$。\n3. 使用这些裁剪后的键计算缩放点积 logit：$\\tilde{e}_i = \\frac{Q^\\top \\tilde{K}_i}{\\sqrt{d_k}}$。\n4. 对其应用 softmax 函数以获得权重 $\\{\\tilde{\\alpha}_1, \\ldots, \\tilde{\\alpha}_N\\}$。\n5. 结果是 $\\tilde{\\alpha}_N$。\n\n**D. 余弦相似度缓解：**\n1. 使用对抗场景中的键集。\n2. 对每个键使用余弦相似度公式计算 logit：$e'_i = \\frac{Q^\\top K_i}{\\|Q\\| \\|K_i\\| + \\varepsilon}$。\n3. 对这些新的 logit $\\{e'_1, \\ldots, e'_N\\}$ 应用 softmax 函数以获得权重 $\\{\\alpha'_1, \\ldots, \\alpha'_N\\}$。\n4. 结果是 $\\alpha'_N$。\n\n按顺序计算这四个值，构成单个测试用例的解。最终输出按规定格式汇总所有测试用例的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes attentional hijacking and mitigation effects for SDPA.\n    \"\"\"\n    # A small constant for numerical stability in cosine similarity.\n    EPSILON = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d_k\": 4,\n            \"Q\": [0.9, 0.1, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]],\n            \"attack_direction\": [0, 0, 0, 1],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 4,\n            \"Q\": [0.0, 1.0, 0.0, 0.0],\n            \"normal_keys\": [[0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]],\n            \"attack_direction\": [1, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 100,\n            \"clip_cap\": 2,\n        },\n        {\n            \"d_k\": 8,\n            \"Q\": [1.0, -0.5, 0.3, -0.2, 0.0, 0.0, 0.0, 0.0],\n            \"normal_keys\": [\n                [0, 1, 0, 0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0, 0, 0],\n            ],\n            \"attack_direction\": [1, 0, 0, 0, 0, 0, 0, 0],\n            \"baseline_attack_norm\": 1,\n            \"adversarial_attack_norm\": 1e6,\n            \"clip_cap\": 10,\n        },\n    ]\n\n    def softmax(x):\n        \"\"\"Computes softmax of vector x for numerical stability.\"\"\"\n        if x.size == 0:\n            return np.array([])\n        e_x = np.exp(x - np.max(x))\n        return e_x / np.sum(e_x)\n\n    def create_attack_key(direction, norm):\n        \"\"\"Creates a key vector from a direction and a norm.\"\"\"\n        direction_vec = np.array(direction, dtype=float)\n        dir_norm = np.linalg.norm(direction_vec)\n        if dir_norm == 0:\n            return np.zeros_like(direction_vec)\n        return norm * (direction_vec / dir_norm)\n\n    def calculate_sdpa_weights(Q, keys, d_k):\n        \"\"\"Computes weights using scaled dot-product attention.\"\"\"\n        logits = np.array([np.dot(Q, k) for k in keys]) / np.sqrt(d_k)\n        return softmax(logits)\n\n    def calculate_cosine_sim_weights(Q, keys):\n        \"\"\"Computes weights using cosine similarity logits.\"\"\"\n        Q_norm = np.linalg.norm(Q)\n        logits = []\n        for k in keys:\n            k_norm = np.linalg.norm(k)\n            dot_product = np.dot(Q, k)\n            logit = dot_product / (Q_norm * k_norm + EPSILON)\n            logits.append(logit)\n        return softmax(np.array(logits))\n\n    results = []\n    for case in test_cases:\n        Q_vec = np.array(case[\"Q\"], dtype=float)\n        normal_keys_vecs = [np.array(k, dtype=float) for k in case[\"normal_keys\"]]\n        d_k = case[\"d_k\"]\n        clip_cap = case[\"clip_cap\"]\n\n        # 1. Baseline calculation\n        K_attack_base = create_attack_key(case[\"attack_direction\"], case[\"baseline_attack_norm\"])\n        all_keys_base = normal_keys_vecs + [K_attack_base]\n        weights_base = calculate_sdpa_weights(Q_vec, all_keys_base, d_k)\n        w_base = weights_base[-1]\n\n        # 2. Adversarial calculation\n        K_attack_adv = create_attack_key(case[\"attack_direction\"], case[\"adversarial_attack_norm\"])\n        all_keys_adv = normal_keys_vecs + [K_attack_adv]\n        weights_adv = calculate_sdpa_weights(Q_vec, all_keys_adv, d_k)\n        w_adv = weights_adv[-1]\n\n        # 3. Norm clipping mitigation\n        all_keys_clipped = []\n        for k in all_keys_adv:\n            k_norm = np.linalg.norm(k)\n            # Use min(1, c/||K||) * K formulation\n            scale_factor = min(1.0, clip_cap / k_norm if k_norm > 0 else 1.0)\n            all_keys_clipped.append(k * scale_factor)\n        weights_clip = calculate_sdpa_weights(Q_vec, all_keys_clipped, d_k)\n        w_clip = weights_clip[-1]\n\n        # 4. Cosine similarity mitigation\n        weights_cos = calculate_cosine_sim_weights(Q_vec, all_keys_adv)\n        w_cos = weights_cos[-1]\n\n        results.append([w_base, w_adv, w_clip, w_cos])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3193536"}, {"introduction": "标准注意力机制的内存复杂度为 $O(L^2)$（$L$ 为序列长度），这在处理长序列时会成为一个主要的性能瓶颈。像FlashAttention这样的高级算法通过重新排序计算来解决这个问题，避免了实例化完整的 $L \\times L$ 注意力分数矩阵。本练习将揭示这一关键优化的奥秘：你将推导分块计算和在线归一化（online normalization）背后的数学原理，并实现一个简化版算法来验证其与标准注意力的数值等价性。这将让你深入理解使现代大语言模型能够高效处理长序列的前沿算法思想 [@problem_id:3193562]。", "problem": "考虑 Transformer 架构中的缩放点积注意力 (Scaled Dot-Product Attention, SDPA)，其中，对于查询矩阵 $Q \\in \\mathbb{R}^{N \\times d}$、键矩阵 $K \\in \\mathbb{R}^{M \\times d}$ 和值矩阵 $V \\in \\mathbb{R}^{M \\times d_v}$，注意力分数矩阵为 $S = \\frac{QK^\\top}{\\sqrt{d}}$。注意力概率通过对 $S$ 进行逐行 softmax 计算得到，输出是矩阵 $O = \\operatorname{softmax}(S)V$。FlashAttention 是一种算法技术，它通过分块计算 $QK^\\top$ 并执行在线归一化 (online normalization) 来重排计算顺序，以减少内存占用，从而无需实例化完整的注意力概率矩阵。在本问题中，您将严格推导为何分块 softmax 累加能保持数值等价性，并实现一个中央处理器 (CPU) 的“玩具”版本，以展示分块计算 $QK^\\top$ 和在线归一化。\n\n从仅适用于中级本科深度学习水平的基础知识出发：\n- 逐行应用 softmax 的定义：对于行向量 $x \\in \\mathbb{R}^{M}$，$\\operatorname{softmax}(x)_j = \\frac{e^{x_j}}{\\sum_{k=1}^{M} e^{x_k}}$，其中 $j \\in \\{1,\\dots,M\\}$。\n- 实数上的加法和乘法的分配律和结合律，以及对于所有实数 $a$ 和 $b$，规则 $e^{a+b} = e^{a} e^{b}$ 成立。\n- 对于任意实数 $m$，恒等式 $e^{x} = e^{x-m} e^{m}$ 成立。\n\n您必须：\n1. 从第一性原理出发，推导为什么沿键维度分块处理注意力分数，并使用一个逐行运行的最大值和运行的总和，能够在假定精确实数运算的情况下，得到与完整 softmax 完全相同的结果。您的推导必须仅使用上述核心事实和定义，并且必须解释如何在不存储完整注意力矩阵的情况下执行在线归一化。\n2. 提供一个可靠的方案来处理掩码（masking），包括因果掩码（causal masking），其中不允许的位置的分数被视为 $-\\infty$，从而使其 softmax 贡献恰好为 $0$。\n3. 实现一个完整且可运行的程序，该程序：\n   - 通过构建完整的分数矩阵 $S$，应用带掩码的数值稳定的逐行 softmax，然后乘以 $V$ 来计算参考 SDPA。\n   - 通过迭代大小为 $c$ 的连续键值块来计算分块 SDPA，更新每行的运行最大值、每行指数化分数的运行总和（经当前最大值稳定化后）以及加权值的每行运行累加，最后进行归一化。此实现必须仅使用 CPU，并且在任何时候都不能实例化完整的注意力概率矩阵。\n   - 通过为每个测试用例生成参考输出和分块输出之间的最大绝对差（以浮点数形式）来证明等价性。\n\n使用以下参数值测试套件。所有矩阵必须为每个测试用例使用固定的随机种子生成，采用独立的标准正态分布条目，然后完全按照描述进行调整。角度单位不适用。没有需要报告的物理单位。\n\n- 测试用例 1 (通用“理想路径”)：\n  - $N = 5$, $M = 7$, $d = 4$, $d_v = 3$, 块大小 $c = 3$, 无掩码。\n- 测试用例 2 (边界条件：单个块等于完整计算)：\n  - $N = 5$, $M = 7$, $d = 4$, $d_v = 3$, 块大小 $c = 7$, 无掩码。\n- 测试用例 3 (边缘情况：带因果掩码的逐元素分块)：\n  - $N = 4$, $M = 4$, $d = 8$, $d_v = 2$, 块大小 $c = 1$, 因果掩码，其中对于查询行索引 $i$，只允许键索引 $j \\le i$。\n- 测试用例 4 (数值稳定性压力测试：大分数量级)：\n  - $N = 3$, $M = 6$, $d = 16$, $d_v = 5$, 块大小 $c = 2$, 无掩码。在计算注意力之前，将查询矩阵 $Q$ 乘以标量 $30$ 以放大数据点积。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例对应一个浮点数，表示参考输出和分块输出之间的最大绝对差（例如，“[$\\text{result}_1$, $\\text{result}_2$, $\\text{result}_3$, $\\text{result}_4$]”）。实际打印的字符串必须遵循“[result1,result2,result3,result4]”的模式。\n\n您的推导和实现必须确保科学真实性和数值稳健性。不要在问题陈述中提供快捷公式；相反，应从列出的基础定义和属性构建论证。最终的数值答案必须是浮点数，并且您的代码必须是自包含的，不需要用户输入或外部文件。", "solution": "该问题要求推导分块 softmax 计算的在线更新规则，制定一个整合掩码的方案，并实现一个程序来证明其与标准缩放点积注意力 (SDPA) 在数值上的等价性。\n\n### 1. 带在线归一化的分块 Softmax 推导\n\n核心任务是在不实例化完整的 $N \\times M$ 注意力分数矩阵 $S$ 的情况下，计算注意力层的输出。我们关注输出矩阵 $O$ 的单行，它对应于单个查询向量。对于一个查询 $q_i$（$Q$ 的第 $i$ 行），其与所有键向量对应的分数由行向量 $s_i \\in \\mathbb{R}^M$ 给出，其中第 $j$ 个元素是 $s_{ij} = \\frac{q_i \\cdot k_j}{\\sqrt{d}}$。输出矩阵 $O$ 的第 $i$ 行，记为 $o_i \\in \\mathbb{R}^{d_v}$，是值向量 $v_j \\in \\mathbb{R}^{d_v}$（$V$ 的行）的加权和。\n\n从提供的逐行 softmax 定义出发，第 $i$ 行的输出为：\n$$ o_i = \\sum_{j=1}^{M} \\operatorname{softmax}(s_i)_j v_j = \\sum_{j=1}^{M} \\frac{\\exp(s_{ij})}{\\sum_{k=1}^{M} \\exp(s_{ik})} v_j $$\n使用分配律，我们可以将其重写为：\n$$ o_i = \\frac{\\sum_{j=1}^{M} \\exp(s_{ij}) v_j}{\\sum_{k=1}^{M} \\exp(s_{ik})} $$\n\n为防止 $s_{ij}$ 的较大正值导致数值溢出，我们使用提供的恒等式 $e^x = e^{x-m}e^m$。令 $m_i = \\max_{j=1, \\dots, M} s_{ij}$。将分子和分母同乘以 $e^{-m_i}$ 可得：\n$$ o_i = \\frac{\\sum_{j=1}^{M} \\exp(s_{ij} - m_i) v_j}{\\sum_{k=1}^{M} \\exp(s_{ik} - m_i)} $$\n在这种形式下，$\\exp(\\cdot)$ 的参数小于或等于 $0$，从而防止了溢出并提高了数值稳定性。\n\nFlashAttention 的核心思想是在不存储完整向量 $s_i$ 的情况下分块计算此表达式。我们将键/值索引 $\\{1, \\dots, M\\}$ 划分为 $T$ 个连续的块 $B_1, B_2, \\dots, B_T$。使用加法的结合律，分子和分母可以写成在这些块上的和：\n$$ o_i = \\frac{\\sum_{t=1}^{T} \\left( \\sum_{j \\in B_t} \\exp(s_{ij}) v_j \\right)}{\\sum_{t=1}^{T} \\left( \\sum_{k \\in B_t} \\exp(s_{ik}) \\right)} $$\n\n我们可以迭代地计算这个表达式。让我们定义处理完前 $t$ 个块后，查询 $i$ 的计算状态。该状态包含三个量：\n1.  $m_i^{(t)}$：到目前为止遇到的分数的运行最大值，即 $m_i^{(t)} = \\max_{j \\in B_1 \\cup \\dots \\cup B_t} s_{ij}$。\n2.  $\\mathcal{O}_i^{(t)}$：运行（稳定化后的）分子，$\\mathcal{O}_i^{(t)} = \\sum_{j \\in B_1 \\cup \\dots \\cup B_t} \\exp(s_{ij} - m_i^{(t)}) v_j$。\n3.  $l_i^{(t)}$：运行（稳定化后的）分母，$l_i^{(t)} = \\sum_{j \\in B_1 \\cup \\dots \\cup B_t} \\exp(s_{ij} - m_i^{(t)})$。\n\n在处理完所有 $T$ 个块后，查询 $i$ 的最终输出将是 $o_i = \\mathcal{O}_i^{(T)} / l_i^{(T)}$。\n\n让我们推导从状态 $(t-1)$ 转换到状态 $(t)$ 的更新规则。假设我们已经为前 $t-1$ 个块计算了 $m_i^{(t-1)}$、$\\mathcal{O}_i^{(t-1)}$ 和 $l_i^{(t-1)}$。现在，我们处理块 $B_t$。\n\n首先，我们计算当前块的分数，$s_{ij}$ 对于所有 $j \\in B_t$。令 $m_{i,t}^{\\text{block}} = \\max_{j \\in B_t} s_{ij}$。\n新的运行最大值是 $m_i^{(t)} = \\max(m_i^{(t-1)}, m_{i,t}^{\\text{block}})$。\n\n接下来，我们计算当前块 $B_t$ 对分子和分母的贡献，并用新的最大值 $m_i^{(t)}$ 对其进行稳定化：\n- 块分子：$\\mathcal{O}_{i,t}^{\\text{block}} = \\sum_{j \\in B_t} \\exp(s_{ij} - m_i^{(t)}) v_j$。\n- 块分母：$l_{i,t}^{\\text{block}} = \\sum_{j \\in B_t} \\exp(s_{ij} - m_i^{(t)})$。\n\n新的总稳定化分子 $\\mathcal{O}_i^{(t)}$ 是前 $t-1$ 个块的贡献与新块 $t$ 的贡献之和。先前的运行分子 $\\mathcal{O}_i^{(t-1)}$ 是用旧的最大值 $m_i^{(t-1)}$ 稳定化的，因此必须根据新的最大值 $m_i^{(t)}$ 对其进行重新缩放。\n\n使用恒等式 $e^{a+b} = e^a e^b$ 和结合律：\n$$ \\mathcal{O}_i^{(t-1)} = \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t-1)}) v_j $$\n为了重新缩放这个和，我们乘以一个因子 $1 = \\exp(m_i^{(t-1)} - m_i^{(t)}) \\exp(m_i^{(t)} - m_i^{(t-1)})$：\n$$ \\mathcal{O}_i^{(t-1)} \\cdot \\exp(m_i^{(t-1)} - m_i^{(t)}) = \\left( \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t-1)}) v_j \\right) \\exp(m_i^{(t-1)} - m_i^{(t)}) $$\n$$ = \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t-1)} + m_i^{(t-1)} - m_i^{(t)}) v_j = \\sum_{j \\in B_1 \\cup \\dots \\cup B_{t-1}} \\exp(s_{ij} - m_i^{(t)}) v_j $$\n这就是先前的分子，已通过新的最大值 $m_i^{(t)}$ 正确地稳定化。\n\n因此，运行分子的更新规则是：\n$$ \\mathcal{O}_i^{(t)} = \\mathcal{O}_i^{(t-1)} \\cdot \\exp(m_i^{(t-1)} - m_i^{(t)}) + \\mathcal{O}_{i,t}^{\\text{block}} $$\n类似地，运行分母的更新规则是：\n$$ l_i^{(t)} = l_i^{(t-1)} \\cdot \\exp(m_i^{(t-1)} - m_i^{(t)}) + l_{i,t}^{\\text{block}} $$\n\n这些更新规则允许对键值对进行单遍处理。我们用 $m_i^{(0)} = -\\infty$、$\\mathcal{O}_i^{(0)} = \\vec{0}$ 和 $l_i^{(0)} = 0$ 进行初始化。在迭代完所有块 $t=1, \\dots, T$ 后，查询 $i$ 的最终输出计算为 $o_i = \\mathcal{O}_i^{(T)} / l_i^{(T)}$。这个过程（在假定精确运算下）计算出与完整 softmax 完全相同的结果，而无需存储完整的 $N \\times M$ 分数矩阵。\n\n### 2. 处理掩码的方案\n\n掩码用于防止对某些键位置的注意力。这通常通过将被禁止位置的分数设置为 $-\\infty$ 来实现。由于 $\\lim_{x \\to -\\infty} \\exp(x) = 0$，这些位置在 softmax 计算中的权重为零，因此对输出没有贡献。\n\n这个原理可以无缝地集成到分块算法中。当计算块 $B_t$ 的分数时，对于任何不允许的查询-键对 $(i,j)$，我们只需将 $s_{ij}$ 设置为 $-\\infty$。\n\n- **通用掩码**：一个布尔掩码可以指定哪些 $(i,j)$ 对是不允许的。当为块 $B_t$ 中的键计算 `S_block` 时，应用相应列的掩码，将适当的分数设置为 $-\\infty$。\n- **因果掩码**：在因果注意力中（例如，对于仅解码器的 Transformer），位置 $i$ 的查询只能关注位置 $j \\le i$ 的键。当为查询 $i$ 计算一个键索引从 $j_{\\text{start}}$ 到 $j_{\\text{end}}$ 的块的注意力时，我们对块中所有满足 $j > i$ 的 $j$ 设置 $s_{ij} = -\\infty$。\n\n这些被掩码的分数随后由标准的在线更新机制处理。由于 $s_{ij} = -\\infty$ 永远不会是最大分数（除非块中所有分数都是 $-\\infty$），它不会不适当地影响运行最大值 $m_i^{(t)}$。它对运行总和 $\\mathcal{O}_i^{(t)}$ 和 $l_i^{(t)}$ 的贡献将是 $\\exp(-\\infty) = 0$，从而正确地将其从最终平均值中排除。", "answer": "```python\nimport numpy as np\n\ndef sdpa_reference(Q, K, V, mask_type):\n    \"\"\"\n    Computes standard Scaled Dot-Product Attention with full matrix materialization.\n    \"\"\"\n    N, d = Q.shape\n    M, _ = K.shape\n    \n    # 1. Compute scores\n    scores = (Q @ K.T) / np.sqrt(d)\n    \n    # 2. Apply mask\n    if mask_type == \"causal\":\n        # Mask where j > i\n        rows = np.arange(N)[:, np.newaxis]\n        cols = np.arange(M)[np.newaxis, :]\n        mask = cols > rows\n        scores[mask] = -np.inf\n\n    # 3. Numerically stable softmax\n    # Subtract max for stability\n    scores_max = np.max(scores, axis=1, keepdims=True)\n    # When a row is all -inf, max is -inf. exp(-inf - (-inf)) = exp(0) = 1.\n    # We should avoid this by replacing -inf max with 0.\n    scores_max = np.where(np.isneginf(scores_max), 0, scores_max)\n    \n    exp_scores = np.exp(scores - scores_max)\n    \n    # Handle fully masked rows where sum would be 0\n    # exp_scores will be all 0s for such rows.\n    sum_exp_scores = np.sum(exp_scores, axis=1, keepdims=True)\n    \n    attention_probs = np.divide(exp_scores, sum_exp_scores, where=sum_exp_scores!=0)\n    \n    # 4. Compute output\n    output = attention_probs @ V\n    return output\n\ndef sdpa_blockwise(Q, K, V, c, mask_type):\n    \"\"\"\n    Computes Scaled Dot-Product Attention in blocks without materializing full score matrix.\n    \"\"\"\n    N, d = Q.shape\n    M, dv = V.shape\n    \n    # Initialize running statistics\n    output_acc = np.zeros((N, dv), dtype=np.float64)\n    log_sum_exp_acc = np.zeros(N, dtype=np.float64)\n    max_score_acc = np.full(N, -np.inf, dtype=np.float64)\n    \n    # Iterate over key/value blocks\n    for j_start in range(0, M, c):\n        j_end = min(j_start + c, M)\n        \n        # Get blocks\n        K_block = K[j_start:j_end, :]\n        V_block = V[j_start:j_end, :]\n        \n        # 1. Compute block scores\n        scores_block = (Q @ K_block.T) / np.sqrt(d)\n\n        # 2. Apply mask for the current block\n        if mask_type == \"causal\":\n            key_indices = np.arange(j_start, j_end)\n            query_indices = np.arange(N)[:, np.newaxis]\n            causal_mask = key_indices > query_indices\n            scores_block[causal_mask] = -np.inf\n        \n        # 3. Compute new running maximum\n        block_max_scores = np.max(scores_block, axis=1)\n        new_max_scores = np.maximum(max_score_acc, block_max_scores)\n        \n        # Replace -inf with 0 to prevent issues in exp(x - m) if all scores are -inf\n        new_max_scores_safe = np.where(np.isneginf(new_max_scores), 0, new_max_scores)\n\n        # 4. Rescale previous accumulated values\n        rescale_factor = np.exp(max_score_acc - new_max_scores_safe)\n        output_acc *= rescale_factor[:, np.newaxis]\n        log_sum_exp_acc *= rescale_factor\n        \n        # 5. Compute and accumulate current block's values\n        exp_scores_block = np.exp(scores_block - new_max_scores_safe[:, np.newaxis])\n        \n        # We must ensure that exp_scores_block is float64 for precision in accumulation\n        exp_scores_block = exp_scores_block.astype(np.float64)\n\n        block_log_sum_exp = np.sum(exp_scores_block, axis=1)\n        block_output = exp_scores_block @ V_block.astype(np.float64)\n        \n        log_sum_exp_acc += block_log_sum_exp\n        output_acc += block_output\n        \n        # 6. Update running max\n        max_score_acc = new_max_scores\n\n    # Final normalization\n    # Handle rows that are fully masked (log_sum_exp_acc will be 0)\n    final_output = np.divide(\n        output_acc, \n        log_sum_exp_acc[:, np.newaxis], \n        out=np.zeros_like(output_acc), \n        where=(log_sum_exp_acc[:, np.newaxis] != 0)\n    )\n\n    return final_output\n\ndef run_test_case(N, M, d, dv, c, mask_type, scale_Q, seed):\n    \"\"\"\n    Runs a single test case, comparing reference and blockwise implementations.\n    \"\"\"\n    np.random.seed(seed)\n    \n    Q = np.random.standard_normal((N, d)).astype(np.float64)\n    K = np.random.standard_normal((M, d)).astype(np.float64)\n    V = np.random.standard_normal((M, dv)).astype(np.float64)\n\n    if scale_Q != 1.0:\n        Q *= scale_Q\n    \n    # Compute using reference implementation\n    output_ref = sdpa_reference(Q, K, V, mask_type)\n    \n    # Compute using blockwise implementation\n    output_blockwise = sdpa_blockwise(Q, K, V, c, mask_type)\n    \n    # Calculate maximum absolute difference\n    max_abs_diff = np.max(np.abs(output_ref - output_blockwise))\n    \n    return max_abs_diff\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (N, M, d, dv, c, mask_type, scale_Q)\n        (5, 7, 4, 3, 3, None, 1.0),\n        (5, 7, 4, 3, 7, None, 1.0),\n        (4, 4, 8, 2, 1, \"causal\", 1.0),\n        (3, 6, 16, 5, 2, None, 30.0),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        N, M, d, dv, c, mask_type, scale_Q = params\n        diff = run_test_case(N, M, d, dv, c, mask_type, scale_Q, seed=i)\n        results.append(diff)\n    \n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3193562"}]}