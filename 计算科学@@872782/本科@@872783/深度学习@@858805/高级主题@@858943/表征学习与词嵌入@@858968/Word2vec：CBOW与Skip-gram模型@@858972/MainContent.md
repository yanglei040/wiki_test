## 引言
作为自然语言处理（NLP）领域的基石，[词嵌入](@entry_id:633879)（word embeddings）彻底改变了我们表示和处理文本数据的方式。在众多[词嵌入](@entry_id:633879)技术中，由 Tomas Mikolov 及其团队于2013年提出的 Word2vec 框架无疑是影响最为深远的之一。它通过简单而高效的模型，证明了可以从海量无标注文本中学习到捕捉丰富语义和句法关系的词向量。然而，对于初学者而言，Word2vec 模型（如 CBOW 和 Skip-gram）的内部工作原理、其令人惊艳的性能背后的数学基础，以及它如何超越语言本身，常常是一个知识上的“黑箱”。本文旨在打开这个黑箱，为读者提供一个全面而深入的理解。

在接下来的内容中，我们将分三步深入探索 Word2vec 的世界。首先，在**“原理与机制”**一章中，我们将详细剖析 CBOW 和 Skip-gram 两种核心架构，阐明它们如何通过上下文预测来学习，并揭示分层 [Softmax](@entry_id:636766) 和[负采样](@entry_id:634675)等关键[优化技术](@entry_id:635438)的运作方式及其与[矩阵分解](@entry_id:139760)的深刻理论联系。接着，在**“应用与跨学科联系”**一章中，我们将展示 Word2vec 的思想如何超越 NLP，在[计算生物学](@entry_id:146988)、软件工程和推荐系统等多个领域中催生创新应用，并讨论其模型的局限与扩展。最后，在**“动手实践”**部分，你将有机会通过具体的计算练习，亲手执行模型的梯度更新，从而在实践中巩固理论知识。通过这一系列的学习，你将不仅掌握 Word2vec 的“如何做”，更能深刻理解其“为什么”以及“还能做什么”。

## 原理与机制

在上一章介绍[词嵌入](@entry_id:633879)的基本概念之后，本章将深入探讨 [Word2Vec](@entry_id:634267) 框架的核心原理与学习机制。我们将详细剖析其两种主要模型架构——连续[词袋模型](@entry_id:635726)（CBOW）和 Skip-gram 模型，并阐明它们背后的优化策略，包括分层 [Softmax](@entry_id:636766) 和[负采样](@entry_id:634675)。此外，我们还将分析一些关键的训练启发式方法，并揭示这些模型与[矩阵分解](@entry_id:139760)之间深刻的理论联系。

### 核心架构：通过上下文预测词语（反之亦然）

[Word2Vec](@entry_id:634267) 的核心思想是“一个词的含义由其相伴的词语所决定”（a word is characterized by the company it keeps）。这一思想通过两种互补的模型架构得以实现：连续[词袋模型](@entry_id:635726)（CBOW）和 Skip-gram 模型。

#### 连续[词袋模型](@entry_id:635726) (CBOW)

**连续[词袋模型](@entry_id:635726)（Continuous Bag-of-Words, CBOW）** 的任务是根据一个词的上下文（其周围的词语）来预测该词本身。想象一下在句子“猫 _ 在垫子上”中填空，CBOW 模型的目标就是利用上下文“猫”、“在”、“垫子上”来预测中心词“坐”。

为了实现这一目标，CBOW 首先将上下文中的每个词的输入嵌入向量 $v_c$ 聚合起来，形成一个综合的上下文表示 $h$。最常见的聚合方式是取平均值：
$$
h = \frac{1}{|C|} \sum_{c \in C} v_c
$$
其中 $C$ 是上下文词语的集合，$|C|$ 是上下文窗口的大小。这个平均向量 $h$ 随后被用作一个特征，通过一个输出层来预测中心词。

选择**平均**而非简单的求和作为聚合策略，背后有其精妙的考量。平均操作使得上下文表示 $h$ 的尺度对于上下文窗口的大小 $|C|$ 不敏感。在一个思想实验中，假设所有上下文词向量 $v_c$ 与目标词的输出向量 $u_w$ 的[点积](@entry_id:149019)均为一个常数 $\alpha$，即 $u_w^\top v_c = \alpha$。如果使用求和表示 $h_{\text{sum}} = \sum v_c$，那么得分 $u_w^\top h_{\text{sum}}$ 将会是 $|C|\alpha$，它会随着窗口大小的增加而[线性增长](@entry_id:157553)，这可能导致逻辑斯蒂函数饱和，使得梯度消失，从而影响模型训练的稳定性。相反，如果使用平均表示 $h_{\text{mean}} = \frac{1}{|C|} \sum v_c$，得分将是 $u_w^\top h_{\text{mean}} = \alpha$，与窗口大小无关，从而获得更稳健的训练动态 [@problem_id:3200053]。

从统计学的角度看，这个上下文向量 $h$ 可以被视为一个**估计量** [@problem_id:3200030]。假设上下文中的词是根据某个以中心词为条件的[概率分布](@entry_id:146404) $p(w|c)$ [独立同分布](@entry_id:169067)地采样的，那么 $h$ 就是这些样本向量的均值。因此，$h$ 是对该[分布](@entry_id:182848)下词向量的[期望值](@entry_id:153208) $\mathbb{E}[v_c | C] = \sum_{w \in \mathcal{V}} p(w|c) v_w$ 的一个[无偏估计](@entry_id:756289)。随着上下文窗口大小 $n$ 的增加，根据多元[中心极限定理](@entry_id:143108)（Multivariate Central Limit Theorem），$h$ 的[分布](@entry_id:182848)会趋向于一个均值为 $\mathbb{E}[v_c|C]$、协[方差](@entry_id:200758)按 $\frac{1}{n}$ 缩放的[多元正态分布](@entry_id:175229)。这为 CBOW 模型的聚合步骤提供了坚实的统计学基础。

#### Skip-gram 模型

**Skip-gram 模型**将 CBOW 的任务反转：它使用一个中心词来预测其上下文中的多个词。例如，给定中心词“坐”，Skip-gram 模型会尝试生成“猫”、“在”、“垫子上”等上下文词。尽管这在直觉上可能不如 CBOW 直接，但 Skip-gram 在实践中表现出强大的性能，尤其是在处理罕见词方面。

对于中心词 $w$ 的每一次出现，Skip-gram 都会生成多个训练样本，每个样本对应一个上下文词 $c$。模型的目标是最大化在给定 $w$ 的情况下，观察到其真实上下文的概率。

#### 架构比较：句法 vs. 语义

CBOW 和 Skip-gram 虽然目标不同，但它们在学习不同类型的语言规律方面各有侧重 [@problem_id:3200063]。

- **Skip-gram 更擅长学习罕见词的表示**：一个罕见词即使在语料中只出现几次，每次出现时，Skip-gram 都会用它来预测多个上下文词。这意味着该罕见词的输入向量 $v_w$ 会在单次出现时接收到来自其所有邻居的梯度更新。这些累积的更新信号比 CBOW 中的信号要强得多。在 CBOW 中，罕见词作为上下文的一部[分时](@entry_id:274419)，其贡献会被平均操作所“稀释”。由于丰富的语义信息通常由内容词（名词、动词等）携带，而这些词往往比功能词（如“的”、“在”）更罕见，因此 Skip-gram 通常在衡量语义关系的基准测试（如“国王-男性+女性 ≈ 女王”）上表现更优。

- **CBOW 在句法任务上通常表现更佳且训练更快**：CBOW 的平均操作是一种平滑处理，它降低了上下文表示的[方差](@entry_id:200758)。这使得模型对上下文中的个别词语不那么敏感，反而更能捕捉到由高频功能词主导的、稳定的局部共现模式。这些模式往往对应于句法规律（如词形变化，$go - going + walk \approx walking$）。因此，CBOW 在句法类比任务上常常略胜一筹。此外，由于每个训练步骤只进行一次预测，CBOW 的[计算效率](@entry_id:270255)通常高于 Skip-gram。

### 学习嵌入：优化策略

定义了模型架构和预测任务后，下一步是确定如何学习词向量。这通常通过定义一个[损失函数](@entry_id:634569)并使用梯度下降法（或其变体）来最小化损失（或最大化[对数似然](@entry_id:273783)）来实现。[Word2Vec](@entry_id:634267) 提供了两种主要的优化策略来处理计算瓶颈——巨大的词汇量 $|V|$。

#### 分层 [Softmax](@entry_id:636766) (Hierarchical [Softmax](@entry_id:636766))

传统的 [Softmax](@entry_id:636766) 函数需要在每个训练步骤中对整个词汇表进行归一化，其计算成本与词汇量 $|V|$ 成正比，这对于拥有数万甚至数百万词的词汇表来说是不可接受的。

**分层 [Softmax](@entry_id:636766) (Hierarchical [Softmax](@entry_id:636766), HS)** 是一种计算上更高效的替代方案。它将预测特定单词的问题转化为在一棵[二叉树](@entry_id:270401)（通常是[霍夫曼树](@entry_id:272425)）中的一系列二元决策问题。在这棵树中，每个叶节点代表词汇表中的一个单词，每个内部节点都是一个二元逻辑斯蒂分类器。从根节点到代表目标词的叶节点存在一条唯一的路径。预测目标词的概率就等于沿途所有分支的决策概率之积。

假设从根到目标词的路径上有 $L$ 个内部节点 $\{n_1, \dots, n_L\}$，每个节点 $n_j$ 有一个参数向量 $u_{n_j}$。在每个节点，模型需要做一个向左或向右的决策。设 $y_j \in \{0, 1\}$ 为正确的决策（例如，1 代表向右）。该路径的对数似然 $J$ 可以表示为沿途所有[二元分类](@entry_id:142257)对数似然之和。对聚合的上下文向量 $h$ 求导，可以得到一个极其简洁和直观的梯度 [@problem_id:3200002]：
$$
\frac{\partial J}{\partial h} = \sum_{j=1}^{L} \big(y_j - \sigma(u_{n_j}^{\top} h)\big) u_{n_j}
$$
其中 $\sigma(\cdot)$ 是 sigmoid 函数。这个表达式的每一项 $(y_j - \sigma(u_{n_j}^{\top} h))$ 都代表了在节点 $n_j$ 的**预测误差**：真实决策 $y_j$ 与模型预测概率 $\sigma(u_{n_j}^{\top} h)$ 之间的差值。梯度更新的方向和大小直接由这个误差决定，它将 $h$ 和 $u_{n_j}$ 推向能使预测更准确的方向。

分层 [Softmax](@entry_id:636766) 的计算复杂度与路径长度成正比。对于一棵平衡[二叉树](@entry_id:270401)，[平均路径长度](@entry_id:141072)约为 $\log_2|V|$。因此，其计算成本为 $O(\log_2|V|)$ [@problem_id:3199987]，远低于传统 [Softmax](@entry_id:636766) 的 $O(|V|)$。

#### [负采样](@entry_id:634675) (Negative Sampling)

**[负采样](@entry_id:634675) (Negative Sampling, NEG)** 是另一种高效的优化策略，它将多[分类问题](@entry_id:637153)简化为一系列[二元分类](@entry_id:142257)问题。其思想是，对于一个真实的训练样本（例如，中心词 $w$ 和上下文词 $c$），我们将其标记为正样本（label=1）。然后，我们从一个噪声[分布](@entry_id:182848)中随机抽取 $k$ 个“负样本”词 $n_i$，将它们与中心词 $w$ 配对，并标记为负样本（label=0）。模型的目标就是区分正样本和负样本。

对于 Skip-gram 中的一个正样本对 $(w, c)$ 和 $k$ 个负样本 $\{n_i\}_{i=1}^k$，目标函数 $J$ 旨在最大化正样本的概率并最小化负样本的概率。其梯度更新的动态非常直观 [@problem_id:3200018]：
$$
\frac{\partial J}{\partial v_{w}} = \underbrace{\big(1 - \sigma(u_{c}^{\top} v_{w})\big) u_{c}}_{\text{将 } v_w \text{ “拉向” } u_c} - \underbrace{\sum_{i=1}^{k} \sigma(u_{n_{i}}^{\top} v_{w}) u_{n_{i}}}_{\text{将 } v_w \text{ “推离” } u_{n_i}}
$$
梯度由两部分组成：一个“拉力”和一个“推力”。“拉力”使中心词向量 $v_w$ 向其真实上下文词向量 $u_c$ 靠拢。这个力的强度与模型当前的错误程度成正比：如果 $v_w$ 和 $u_c$ 已经很相似（[点积](@entry_id:149019)很大），则系数 $(1 - \sigma)$ 接近 0，更新很小；反之，更新很大。“推力”则使 $v_w$ 远离所有负样本向量 $u_{n_i}$。同样，推力的强度取决于 $v_w$ 与 $u_{n_i}$ 的相似度，如果模型错误地认为它们相似，则推力很强。

对于 CBOW 模型，梯度更新的逻辑类似 [@problem_id:3200087]。损失函数关于隐藏层表示 $h$ 的梯度 $\frac{\partial J}{\partial h}$ 包含了来自目标词和负样本的拉力和推力。然后，这个梯度通过链式法则被“广播”回每个上下文词的输入嵌入 $v_c$：
$$
\frac{\partial J}{\partial v_{c}} = \frac{1}{\lvert C \rvert} \frac{\partial J}{\partial h}
$$
因子 $\frac{1}{|C|}$ 表明，总的误差信号被平均分配给所有构成上下文的词向量。这再次体现了 CBOW 的平均/平滑特性。

[负采样](@entry_id:634675)的计算复杂度与负样本数量 $k$ 直接相关，为 $O(k+1)$ [@problem_id:3199987]。这个成本不依赖于词汇表大小 $|V|$，这使得它在拥有巨大词汇表的非常大的语料库上特别有吸[引力](@entry_id:175476)。

#### 优化策略对比

分层 [Softmax](@entry_id:636766) 和[负采样](@entry_id:634675)之间的选择是一个典型的权衡。当词汇量 $|V|$ 很大时，$\log_2|V|$ 可能大于一个小的 $k$（通常为 5-20）。例如，当 $d=300$ 时，对于一个包含 50,000 个词的词汇表，HS 和 NEG 的计算成本将在 $k \approx 15$ 时[达到平衡](@entry_id:170346)点 [@problem_id:3199987]。对于更小的词汇表或当需要更精确的概率估计时，HS 可能是更好的选择。而对于极大的语料库和词汇表，NEG 通常更快，并且经验表明它能学习到同样高质量的词向量。

### 实践启发式：高频词的二次采样

在自然语言中，词频遵循齐夫定律（Zipf's law），即少数词（如“的”、“是”、“在”）的出现频率极高，而大量词语非常罕见。这些高频词通常携带较少的语义信息，但它们在训练数据中占据了不成比例的份额，这不仅会减慢训练速度，还可能影响罕见词的表示质量。

为了解决这个问题，[Word2Vec](@entry_id:634267) 引入了一种简单的**启发式二次采样（subsampling）**方法。对于语料库中的每个词 $w$，它以一定的概率 $P_{\text{keep}}(w)$ 被保留下来，该概率与词频 $f(w)$ 负相关：
$$
P_{\text{keep}}(w) = \min\left(1, \sqrt{\frac{t}{f(w)}} + \frac{t}{f(w)}\right)
$$
其中 $t$ 是一个超参数阈值，通常在 $10^{-5}$ 左右。这个公式的设计使得频率远高于 $t$ 的词被丢弃的概率很高，而频率低于 $t$ 的词则几乎总被保留。

二次采样带来了几个重要影响 [@problem_id:3200047]：
1.  **加速训练**：通过丢弃大量高频词的出现，显著减少了训练样本的总数。
2.  **改善罕见词表示**：减少了高频词的“噪声”，使得模型有更多机会从信息量更丰富的罕见词共现中学习。这会导致罕见词的嵌入范数（norm）相对较大，而高频词的范数则被抑制。
3.  **对隐式目标的系统性影响**：理论分析表明，这种二次[采样方法](@entry_id:141232)对模型隐式优化的逐点互信息（PMI）目标产生了一个系统性的、近似恒定的加性偏移。具体来说，在二次采样后的语料上计算的 PMI' 约等于原始的 PMI 加上一个不依赖于具体词对的常数项 $\log(M'/M)$，其中 $M$ 和 $M'$ 分别是采样前后的总共现对数。

### 理论基础：[Word2Vec](@entry_id:634267) 到底在学习什么？

虽然 [Word2Vec](@entry_id:634267) 的模型架构和学习算法在实践中非常有效，但一个更深层次的问题是：它们究竟在学习什么？后续的研究揭示，[Word2Vec](@entry_id:634267) 模型可以被理解为在**隐式地进行[矩阵分解](@entry_id:139760)**。

#### Skip-gram 与逐点[互信息](@entry_id:138718) (PMI)

研究表明，Skip-gram 结合[负采样](@entry_id:634675)（SGNS）的优化过程，在理想条件下，等价于对一个特定的**逐点互信息（Pointwise Mutual Information, PMI）**矩阵进行因式分解 [@problem_id:3200029]。

PMI 衡量的是两个事件（在这里是两个词 $w$ 和 $c$）同时发生的概率与它们各自独立发生概率的乘积之间的比率，它捕捉了词语之间的“关联度”：
$$
\operatorname{PMI}(w,c) = \log\frac{P(w,c)}{P(w)P(c)}
$$
当 SGNS 模型训练收敛时，其学到的词向量之间的[点积](@entry_id:149019) $v_w^\top u_c$ 近似等于一个移位的 PMI 值：
$$
v_w^\top u_c \approx \operatorname{PMI}(w,c) - \log k
$$
这个深刻的联系意味着，SGNS 算法通过其简单的“拉近-推远”的局部学习规则，最终实现了对整个语料库共现统计量的全局信息的分解。这个理论结果成立需要一些前提条件，包括语料库过程的平稳性（即词语概率不随时间变化），以及[负采样](@entry_id:634675)的噪声[分布](@entry_id:182848) $P_n(c)$ 恰好等于语料库中上下文词的[边际分布](@entry_id:264862) $P(c)$ [@problem_id:3200029]。诸如窗口大小 [@problem_id:3200014] 和二次采样 [@problem_id:3200030] 等超参数和[启发式方法](@entry_id:637904)，会改变这个隐式目标矩阵的具体形式，但其作为[矩阵分解](@entry_id:139760)的核心思想保持不变。

#### CBOW 与[矩阵补全](@entry_id:172040)

从另一个角度看，像 CBOW 这样的模型也可以被视为一个**低秩[矩阵补全](@entry_id:172040)（low-rank matrix completion）**问题 [@problem_id:3200033]。想象一个巨大的矩阵 $\tilde{M}$，其行代表词汇表中的所有词，列代表所有可能的上下文。矩阵的每个单元格 $\tilde{M}_{w,c}$ 存储了词 $w$ 与上下文 $c$ 之间某种理想的关联分数（如 PMI）。然而，在真实的语料库中，我们只能观察到这个矩阵中非常稀疏的一部分条目。

[Word2Vec](@entry_id:634267) 的任务可以看作是找到一个低秩矩阵 $X = UV^\top$ 来逼近这个稀疏观测到的 $\tilde{M}$。这里的 $U$ 和 $V$ 分别是输入和输出嵌入矩阵，[嵌入维度](@entry_id:268956) $d$ 强制了 $X$ 的秩不能超过 $d$。
- **低秩约束**是泛化的关键。一个全秩矩阵有 $|W| \times |C|$ 个自由度，在[稀疏数据](@entry_id:636194)上很容易[过拟合](@entry_id:139093)。而一个秩为 $d$ 的矩阵只有约 $(|W|+|C|)d$ 个自由度，这极大地降低了[模型复杂度](@entry_id:145563)，迫使模型学习更具普遍性的潜在结构 [@problem_id:3200033]。
- 在这个框架下，对嵌入矩阵 $U$ 和 $V$ 施加的 $\ell_2$ 正则化，可以被证明等价于对最终的得分矩阵 $X$ 施加**[核范数](@entry_id:195543)（nuclear norm）**惩罚。[核范数](@entry_id:195543)是秩函数的最紧[凸松弛](@entry_id:636024)，因此这种正则化方式直接鼓励模型学习一个低秩的解，从而增强了泛化能力 [@problem_id:3200033]。

综上所述，[Word2Vec](@entry_id:634267) 不仅仅是一套有效的工程技巧，其背后蕴含着深刻的数学原理。无论是 CBOW 还是 Skip-gram，它们都通过简单的局部学习规则，隐式地实现了对全局词语共现[统计矩](@entry_id:268545)阵的低秩分解，从而在低维[向量空间](@entry_id:151108)中捕获了丰富的语义和句法关系。