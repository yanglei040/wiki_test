{"hands_on_practices": [{"introduction": "在嵌入向量的世界里，余弦相似度是首选的度量标准，我们常常直观地将 $1 - \\cos(\\theta)$ 视为一种“距离”。但这种直觉是否可靠？本练习将深入探讨嵌入空间的基本几何属性，特别是单位超球面上的特性，通过检验余弦相异性是否满足三角不等式这一核心度量公理，来挑战一个常见的假设。通过构建一个具体的反例 [@problem_id:3114488]，你将对高维空间的独特几何学以及审慎选择度量标准的重要性有更深刻的理解。", "problem": "深度学习中使用的密集向量嵌入是 $\\mathbb{R}^{d}$ 中的实值向量，为了在相似性搜索中保证稳定性和可解释性，通常将其归一化为单位欧几里得范数。两个嵌入 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{d}$ 之间的余弦相似度定义为 $s(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}$。考虑当使用余弦相异度 $d_{c}(\\mathbf{x}, \\mathbf{y}) = 1 - s(\\mathbf{x}, \\mathbf{y})$ 来度量嵌入之间的分离度时，在单位球面 $\\mathbb{S}^{d-1} = \\{\\mathbf{u} \\in \\mathbb{R}^{d} : \\|\\mathbf{u}\\| = 1\\}$ 上所诱导的几何结构。\n\n仅从欧几里得内积和范数的基本定义出发，分析 $s(\\mathbf{x}, \\mathbf{y})$ 与单位向量之间夹角的关系，并利用此关系从概念上解释为什么三角不等式对于 $\\mathbb{S}^{d-1}$ 上的 $d_{c}$ 可能会不成立。然后，构造一个由三个单位归一化嵌入 $\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathbb{S}^{1} \\subset \\mathbb{S}^{d-1}$ 构成的显式反例（为具体起见，取 $d = 2$），具体如下\n$$\n\\mathbf{x} = (\\cos 0^{\\circ}, \\sin 0^{\\circ}), \\quad\n\\mathbf{y} = (\\cos 60^{\\circ}, \\sin 60^{\\circ}), \\quad\n\\mathbf{z} = (\\cos 120^{\\circ}, \\sin 120^{\\circ}),\n$$\n其中角度以度为单位。计算量\n$$\nT = d_{c}(\\mathbf{x}, \\mathbf{z}) - d_{c}(\\mathbf{x}, \\mathbf{y}) - d_{c}(\\mathbf{y}, \\mathbf{z}),\n$$\n并给出其精确值。无需四舍五入。最终答案必须是一个不带单位的实数。", "solution": "$\\mathbb{R}^{d}$ 中 $\\mathbf{x}$ 和 $\\mathbf{y}$ 之间的欧几里得内积定义为 $\\mathbf{x} \\cdot \\mathbf{y} = \\sum_{i=1}^{d} x_{i} y_{i}$，范数是 $\\|\\mathbf{x}\\| = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}}$。一个关联内积和范数的基础几何事实是，对于任何非零的 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{d}$，\n$$\n\\mathbf{x} \\cdot \\mathbf{y} = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos \\theta,\n$$\n其中 $\\theta$ 是 $\\mathbf{x}$ 和 $\\mathbf{y}$ 之间的夹角。当 $\\mathbf{x}$ 和 $\\mathbf{y}$ 是单位向量时，$\\|\\mathbf{x}\\| = \\|\\mathbf{y}\\| = 1$，因此余弦相似度简化为\n$$\ns(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} = \\mathbf{x} \\cdot \\mathbf{y} = \\cos \\theta.\n$$\n因此，在单位球面 $\\mathbb{S}^{d-1}$ 上，余弦相似度等于向量之间中心角的余弦值。余弦相异度为\n$$\nd_{c}(\\mathbf{x}, \\mathbf{y}) = 1 - s(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos \\theta.\n$$\n\n为讨论三角不等式，回顾度量公理：一个函数 $d$ 是一个度量，如果它对所有点都满足非负性、不可区分者的同一性、对称性和三角不等式，\n$$\nd(\\mathbf{x}, \\mathbf{z}) \\leq d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z}).\n$$\n在 $\\mathbb{S}^{d-1}$ 上，角距离 $\\theta$ 本身（作为测地角测量）遵循三角不等式，因为它源于球面上的测地度量。然而，映射 $\\theta \\mapsto 1 - \\cos \\theta$ 非线性地扭曲了角度：$\\cos \\theta$ 不是 $\\theta$ 的次可加函数，并且当角度相加时（例如，当两个较小的角组合成一个较大的钝角时），$1 - \\cos \\theta$ 的增长速度可能快于线性。因此，不能保证 $d_{c}$ 满足三角不等式。\n\n我们现在构造一个嵌入在 $\\mathbb{R}^{2}$ 中的 $\\mathbb{S}^{1}$ 上的显式反例：\n$$\n\\mathbf{x} = (\\cos 0^{\\circ}, \\sin 0^{\\circ}) = (1, 0), \\quad\n\\mathbf{y} = (\\cos 60^{\\circ}, \\sin 60^{\\circ}) = \\left(\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right), \\quad\n\\mathbf{z} = (\\cos 120^{\\circ}, \\sin 120^{\\circ}) = \\left(-\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right).\n$$\n各向量对之间的夹角为：\n- $\\mathbf{x}$ 和 $\\mathbf{y}$ 之间：$\\theta_{xy} = 60^{\\circ}$。\n- $\\mathbf{y}$ 和 $\\mathbf{z}$ 之间：$\\theta_{yz} = 60^{\\circ}$。\n- $\\mathbf{x}$ 和 $\\mathbf{z}$ 之间：$\\theta_{xz} = 120^{\\circ}$。\n\n对单位向量使用 $s(\\mathbf{u}, \\mathbf{v}) = \\cos \\theta$ 和 $d_{c} = 1 - s$，我们计算\n$$\ns(\\mathbf{x}, \\mathbf{y}) = \\cos 60^{\\circ} = \\frac{1}{2}, \\quad d_{c}(\\mathbf{x}, \\mathbf{y}) = 1 - \\frac{1}{2} = \\frac{1}{2},\n$$\n$$\ns(\\mathbf{y}, \\mathbf{z}) = \\cos 60^{\\circ} = \\frac{1}{2}, \\quad d_{c}(\\mathbf{y}, \\mathbf{z}) = 1 - \\frac{1}{2} = \\frac{1}{2},\n$$\n$$\ns(\\mathbf{x}, \\mathbf{z}) = \\cos 120^{\\circ} = -\\frac{1}{2}, \\quad d_{c}(\\mathbf{x}, \\mathbf{z}) = 1 - \\left(-\\frac{1}{2}\\right) = \\frac{3}{2}.\n$$\n因此，\n$$\nT = d_{c}(\\mathbf{x}, \\mathbf{z}) - d_{c}(\\mathbf{x}, \\mathbf{y}) - d_{c}(\\mathbf{y}, \\mathbf{z}) = \\frac{3}{2} - \\frac{1}{2} - \\frac{1}{2} = \\frac{1}{2}.\n$$\n由于 $T > 0$，三角不等式 $d_{c}(\\mathbf{x}, \\mathbf{z}) \\leq d_{c}(\\mathbf{x}, \\mathbf{y}) + d_{c}(\\mathbf{y}, \\mathbf{z})$ 在此例中不成立，这证实了尽管底层的角距离是度量，但 $d_{c}$ 在 $\\mathbb{S}^{d-1}$ 上不是一个度量。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3114488"}, {"introduction": "理解了嵌入空间的几何特性后，我们如何在该空间中有效地组织向量呢？以InfoNCE等损失函数为代表的对比学习是实现这一目标的核心方法。本练习将带你深入对比学习的心脏地带，推导InfoNCE损失函数相对于锚点（anchor）嵌入的梯度。这个梯度正是指导优化的关键信号，它将锚点拉向其正样本，同时推开负样本。从第一性原理出发推导此梯度 [@problem_id:3114472]，你将揭示自监督训练中“拉近-推远”动态的神秘面纱，并理解嵌入向量的方向和模长（norm）之间微妙的相互作用。", "problem": "考虑一个深度学习中的密集向量嵌入场景，其中一个锚点嵌入 $x \\in \\mathbb{R}^{d}$ 和一组候选嵌入 $\\{y_{j}\\}_{j=1}^{n} \\subset \\mathbb{R}^{d}$ 使用信息噪声对比估计（Information Noise-Contrastive Estimation, InfoNCE）损失进行比较。令温度为一个固定的标量 $\\tau > 0$。对于 $\\{y_{j}\\}$ 中的一个指定正样本 $y$，其损失为\n$$\nL(x) \\;=\\; -\\ln\\!\\left(\\frac{\\exp\\!\\big(\\hat{x}\\cdot y/\\tau\\big)}{\\sum_{j=1}^{n}\\exp\\!\\big(\\hat{x}\\cdot y_{j}/\\tau\\big)}\\right),\n$$\n其中 $\\hat{x} = x/\\|x\\|$ 表示 $x$ 的欧几里得二范数（L2）归一化，点号表示 $\\mathbb{R}^{d}$ 上的标准内积。假设所有 $y_{j}$ 相对于 $x$ 都是固定的。\n\n仅从 softmax、log-sum-exp 和 L2 归一化映射的核心定义出发，推导关于未归一化锚点 $x$ 的梯度 $\\nabla_{x}L(x)$ 的闭式表达式，并确保该表达式正确地考虑了 L2 归一化。然后，用文字简要说明该梯度如何编码了作用于 $x$ 的范数和方向上的竞争作用力。\n\n你的最终答案必须是 $\\nabla_{x}L(x)$ 的一个闭式解析表达式，用 $x$, $\\{y_{j}\\}$, $y$ 和 $\\tau$ 表示。无需进行数值计算。", "solution": "该问题要求计算信息噪声对比估计（Information Noise-Contrastive Estimation, InfoNCE）损失函数关于未归一化锚点嵌入 $x$ 的梯度。\n\n损失函数由下式给出：\n$$\nL(x) = -\\ln\\left(\\frac{\\exp(\\hat{x}\\cdot y/\\tau)}{\\sum_{j=1}^{n}\\exp(\\hat{x}\\cdot y_{j}/\\tau)}\\right)\n$$\n其中 $\\hat{x} = x/\\|x\\|$ 是 L2 归一化的锚点嵌入，$y$ 是正候选嵌入，$\\{y_j\\}_{j=1}^n$ 是所有候选嵌入的集合（包括 $y$），$\\tau > 0$ 是温度。\n\n首先，我们可以利用对数的性质简化损失 $L(x)$ 的表达式：\n$$\nL(x) = -\\left(\\frac{\\hat{x}\\cdot y}{\\tau}\\right) + \\ln\\left(\\sum_{j=1}^{n}\\exp\\left(\\frac{\\hat{x}\\cdot y_{j}}{\\tau}\\right)\\right)\n$$\n这个函数是一个复合函数。我们将损失定义为归一化向量 $u = \\hat{x}$ 的函数，记作 $L(u)$，并将 $u$ 定义为 $x$ 的函数，记作 $u(x) = x/\\|x\\|$。\n$$\nL(u) = -\\frac{u \\cdot y}{\\tau} + \\ln\\left(\\sum_{j=1}^{n}\\exp\\left(\\frac{u \\cdot y_{j}}{\\tau}\\right)\\right)\n$$\n为了求梯度 $\\nabla_{x}L(x)$，我们应用多元链式法则：\n$$\n\\nabla_{x}L(x) = (\\nabla_{x} u(x))^T \\nabla_{u}L(u)\n$$\n其中 $\\nabla_{x} u(x)$ 是函数 $u(x)$ 的雅可比矩阵，$\\nabla_{u}L(u)$ 是 $L$ 关于 $u$ 的梯度。\n\n让我们分别计算这两个部分。\n\n1.  **关于归一化向量 $u = \\hat{x}$ 的梯度**：\n    我们计算梯度 $\\nabla_{u}L(u)$。\n    $$\n    \\nabla_{u}L(u) = \\nabla_{u}\\left(-\\frac{u \\cdot y}{\\tau}\\right) + \\nabla_{u}\\left(\\ln\\left(\\sum_{j=1}^{n}\\exp\\left(\\frac{u \\cdot y_{j}}{\\tau}\\right)\\right)\\right)\n    $$\n    第一项的梯度很简单：\n    $$\n    \\nabla_{u}\\left(-\\frac{u \\cdot y}{\\tau}\\right) = -\\frac{1}{\\tau}y\n    $$\n    对于第二项，我们再次使用链式法则。令 $Z(u) = \\sum_{j=1}^{n}\\exp((u \\cdot y_{j})/\\tau)$。\n    $$\n    \\nabla_{u}\\ln(Z(u)) = \\frac{1}{Z(u)}\\nabla_{u}Z(u) = \\frac{1}{Z(u)}\\sum_{k=1}^{n} \\nabla_{u}\\left(\\exp\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right)\\right)\n    $$\n    $$\n    = \\frac{1}{Z(u)}\\sum_{k=1}^{n} \\exp\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right) \\nabla_{u}\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right) = \\frac{1}{Z(u)}\\sum_{k=1}^{n} \\exp\\left(\\frac{u \\cdot y_{k}}{\\tau}\\right) \\frac{y_{k}}{\\tau}\n    $$\n    让我们定义 softmax 概率 $P_{j}(u) = \\frac{\\exp((u \\cdot y_{j})/\\tau)}{Z(u)}$。表达式简化为：\n    $$\n    \\nabla_{u}\\ln(Z(u)) = \\frac{1}{\\tau}\\sum_{k=1}^{n} P_{k}(u) y_{k}\n    $$\n    合并 $\\nabla_{u}L(u)$ 的各项：\n    $$\n    \\nabla_{u}L(u) = \\frac{1}{\\tau}\\left(\\left(\\sum_{j=1}^{n} P_{j}(u)y_{j}\\right) - y\\right)\n    $$\n\n2.  **L2 归一化映射的雅可比矩阵**：\n    接下来，我们计算 $u(x) = x/\\|x\\|$ 的雅可比矩阵。该矩阵的第 $(i,k)$ 个元素是 $\\frac{\\partial u_i}{\\partial x_k}$。\n    $$\n    \\frac{\\partial u_i}{\\partial x_k} = \\frac{\\partial}{\\partial x_k}\\left(\\frac{x_i}{\\|x\\|}\\right) = \\frac{\\delta_{ik}\\|x\\| - x_i \\frac{\\partial\\|x\\|}{\\partial x_k}}{\\|x\\|^2}\n    $$\n    由于 $\\|x\\| = (x \\cdot x)^{1/2}$，其偏导数为 $\\frac{\\partial\\|x\\|}{\\partial x_k} = \\frac{x_k}{\\|x\\|}$。\n    $$\n    \\frac{\\partial u_i}{\\partial x_k} = \\frac{\\delta_{ik}\\|x\\| - x_i \\frac{x_k}{\\|x\\|}}{\\|x\\|^2} = \\frac{\\delta_{ik}}{\\|x\\|} - \\frac{x_i x_k}{\\|x\\|^3}\n    $$\n    以矩阵形式，雅可比矩阵为：\n    $$\n    \\nabla_{x} u(x) = \\frac{1}{\\|x\\|}I - \\frac{1}{\\|x\\|^3}xx^T = \\frac{1}{\\|x\\|}\\left(I - \\frac{x}{\\|x\\|}\\frac{x^T}{\\|x\\|}\\right) = \\frac{1}{\\|x\\|}(I - \\hat{x}\\hat{x}^T)\n    $$\n    这个矩阵是对称的，所以 $(\\nabla_{x} u(x))^T = \\nabla_{x} u(x)$。\n\n3.  **组合以求得最终梯度 $\\nabla_x L(x)$**：\n    将 $u=\\hat{x}$ 和这两个分量的表达式代回链式法则公式中：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\|x\\|}(I - \\hat{x}\\hat{x}^T) \\left[ \\frac{1}{\\tau}\\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) \\right]\n    $$\n    让我们展开这个乘积：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|} \\left[ \\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) - \\hat{x}\\hat{x}^T \\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) \\right]\n    $$\n    第二部分使用 $\\hat{x}^T v = \\hat{x} \\cdot v$ 进行简化：\n    $$\n    \\hat{x}\\hat{x}^T \\left( \\dots \\right) = \\hat{x} \\left( \\hat{x} \\cdot \\left[ \\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y \\right] \\right) = \\left( \\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})(\\hat{x}\\cdot y_{j})\\right) - (\\hat{x}\\cdot y) \\right) \\hat{x}\n    $$\n    所以完整的梯度是：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|} \\left[ \\left(\\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})y_{j}\\right) - y\\right) - \\left( \\left(\\sum_{j=1}^{n} P_{j}(\\hat{x})(\\hat{x}\\cdot y_{j})\\right) - (\\hat{x}\\cdot y) \\right) \\hat{x} \\right]\n    $$\n    为了以 $x$, $\\{y_j\\}$, $y$ 和 $\\tau$ 的形式纯粹地表示最终答案，我们代入 $\\hat{x}=x/\\|x\\|$ 并展开各项：\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|}\\left( \\left(\\sum_{j=1}^{n} P_{j}y_{j}\\right) - y \\right) - \\frac{1}{\\tau\\|x\\|^2}\\left( \\left(\\sum_{j=1}^{n} P_{j}\\frac{x\\cdot y_j}{\\|x\\|}\\right) - \\frac{x\\cdot y}{\\|x\\|} \\right)x\n    $$\n    $$\n    \\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|}\\left( \\left(\\sum_{j=1}^{n} P_{j}y_{j}\\right) - y \\right) - \\frac{1}{\\tau\\|x\\|^3}\\left( \\left(\\sum_{j=1}^{n} P_{j}(x\\cdot y_j)\\right) - (x\\cdot y) \\right)x\n    $$\n    其中概率 $P_j$ 本身是 $x$ 的函数：\n    $$\n    P_{j} = P_j(x) = \\frac{\\exp((x \\cdot y_{j})/(\\tau\\|x\\|))}{\\sum_{k=1}^{n}\\exp((x \\cdot y_{k})/(\\tau\\|x\\|))}\n    $$\n    这就是梯度的最终闭式表达式。\n\n**对竞争作用力的说明**\n\n深度学习中的梯度更新规则是 $x_{\\text{new}} = x - \\eta \\nabla_{x}L(x)$，其中学习率 $\\eta > 0$。更新方向 $-\\nabla_x L(x)$ 揭示了作用于 $x$ 的力。\n$$\n-\\nabla_{x}L(x) = \\frac{1}{\\tau\\|x\\|} \\left[ (y - \\bar{y}) + (\\bar{c} - c_y)\\hat{x} \\right]\n$$\n其中 $\\bar{y} = \\sum_{j}P_j y_j$ 是期望候选向量，$c_y = \\hat{x}\\cdot y$ 是与正样本的余弦相似度，$\\bar{c} = \\sum_j P_j(\\hat{x}\\cdot y_j)$ 是在所有候选样本上的期望余弦相似度。\n\n1.  **作用于 $x$ 方向的力**：项 $(y - \\bar{y})$ 决定了方向的变化。更新试图将 $x$ 移动到一个“更像 $y$”且“更不像 $\\bar{y}$”的新方向。由于 $\\bar{y}$ 是所有候选向量的加权平均，其中对于与 $x$ 相似到足以混淆的候选向量，其权重 $P_j$ 会很高，因此该项有效地推动 $x$ 与正嵌入 $y$ 更好地对齐，同时远离负“干扰”嵌入的平均值。这个力主要作用是旋转向量 $x$。\n\n2.  **作用于 $x$ 范数的力**：项 $(\\bar{c} - c_y)\\hat{x}$ 是一个与 $x$ 本身平行的向量，因此其主要作用是改变范数 $\\|x\\|$。\n    - 如果锚点对齐良好，意味着它与正样本的相似度 $c_y$ 大于与所有候选样本的平均相似度 $\\bar{c}$（即 $c_y > \\bar{c}$），则标量系数 $(\\bar{c} - c_y)$ 为负。因此，更新方向 $-\\nabla_x L(x)$ 有一个沿着 $\\hat{x}$ 方向的分量，这会*增加*范数 $\\|x\\|$。更大的范数会使 softmax 分布更尖锐，从而有效地使模型对其正确的对齐“更有信心”。\n    - 相反，如果锚点对齐不佳（$c_y < \\bar{c}$），则标量系数为正。更新方向有一个与 $\\hat{x}$ 方向相反的分量，这会*减小*范数 $\\|x\\|$。较小的范数使 softmax 分布更“平滑”或不那么尖锐。这使得模型不那么固执于当前错误的方向，并增加了旋转梯度分量的相对大小，从而鼓励进行更大的方向校正。\n\n总而言之，梯度编码了两种相互竞争但又协同的作用力：一种是旋转力，它将 $x$ 的方向引向正样本并远离干扰项；另一种是缩放力，它根据模型当前表现调整 $x$ 的范数，以调节模型的置信度。", "answer": "$$\n\\boxed{\\frac{1}{\\tau\\|x\\|}\\left( \\left(\\sum_{j=1}^{n} P_{j}y_{j}\\right) - y \\right) - \\frac{1}{\\tau\\|x\\|^3}\\left( \\left(\\sum_{j=1}^{n} P_{j}(x\\cdot y_j)\\right) - (x\\cdot y) \\right)x, \\quad \\text{其中 } P_{j} = \\frac{\\exp((x \\cdot y_{j})/(\\tau\\|x\\|))}{\\sum_{k=1}^{n}\\exp((x \\cdot y_{k})/(\\tau\\|x\\|))}}\n$$", "id": "3114472"}, {"introduction": "我们已经探讨了嵌入空间的几何学和核心学习机制，现在让我们将这些强大的嵌入向量应用于一个富有挑战性的前沿场景：少样本学习（few-shot learning）。本练习将指导你实现一个半监督算法，其中仅由极少数样本构成的初始类原型（prototype）将利用未标记的数据进行迭代优化。你将采用期望最大化（EM）的框架，其中查询点（query point）会基于余弦相似度被“软分配”到各个类别。完成这个编码任务 [@problem_id:3114433]，你不仅能掌握一种精密的算法，还将获得关于密集嵌入如何赋能少样本分类和直推式学习等强大应用的实践洞见。", "problem": "您必须实现并分析一种用于密集向量嵌入的原型优化算法，该算法基于余弦相似度使用期望最大化（EM）方法。您将获得几个少样本（few-shot）学习情景（episode），每个情景都包含在一个共同欧几里得空间中的带标签的支持集嵌入和带标签的查询集嵌入。目标是比较一个基线分类器和一个经过 EM 优化的分类器。基线分类器使用从支持集计算出的类原型，而 EM 优化后的分类器则通过基于余弦相似度的软责任（soft responsibilities）来整合未标记的查询。\n\n从以下基本概念开始：\n- 两个非零向量 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ 之间的余弦相似度定义为 $\\cos(\\theta) = \\dfrac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\|_2 \\, \\|\\mathbf{y}\\|_2}$，其中 $\\|\\cdot\\|_2$ 表示欧几里得范数。\n- 向量 $\\mathbf{z} \\in \\mathbb{R}^K$ 的 softmax 函数为 $\\operatorname{softmax}(\\mathbf{z})_k = \\dfrac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)}$，其中 $k \\in \\{1,\\dots,K\\}$。\n- 在一个混合模型中，对于数据点 $i$ 和成分 $k$，其成分分数为 $s_{ik}$，后验责任为 $r_{ik} = \\dfrac{\\exp(s_{ik})}{\\sum_{j=1}^K \\exp(s_{ij})}$。期望最大化（EM）算法交替进行两个步骤：计算责任（E 步）和针对参数最大化期望完全数据对数似然（M 步），在标准正则性条件下，这会产生观测数据对数似然的单调改进。\n\n您的算法任务：\n1. 将所有嵌入向量归一化，使其欧几里得范数为 1。对于任意向量 $\\mathbf{x} \\neq \\mathbf{0}$，定义 $\\widehat{\\mathbf{x}} = \\dfrac{\\mathbf{x}}{\\|\\mathbf{x}\\|_2}$。\n2. 基线原型：对于每个类别 $k \\in \\{1,\\dots,K\\}$，计算初始原型，即类别 $k$ 的归一化支持向量的均值，然后重新归一化至单位范数。也就是说，如果 $\\mathcal{S}_k$ 是类别 $k$ 的支持集嵌入集合，定义\n   $$\\mathbf{m}_k^{(0)} = \\operatorname{norm}\\left(\\frac{1}{|\\mathcal{S}_k|} \\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\right),$$\n   其中对于 $\\mathbf{v} \\neq \\mathbf{0}$，$\\operatorname{norm}(\\mathbf{v}) = \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|_2}$，而 $\\operatorname{norm}(\\mathbf{0})$ 未定义，因为它不会在所提供的数据中出现。\n3. 查询的基线分类：对于每个查询 $\\widehat{\\mathbf{q}}$，通过最大化与基线原型的余弦相似度来分配预测标签，\n   $$\\widehat{y}(\\widehat{\\mathbf{q}}) = \\arg\\max_{k \\in \\{1,\\dots,K\\}} \\widehat{\\mathbf{q}}^\\top \\mathbf{m}_k^{(0)}.$$\n   基线准确率是在该情景中正确分类的查询所占的比例，表示为小数。\n4. 使用余弦责任的 EM 优化：固定温度参数 $\\tau > 0$ 和查询权重 $\\lambda \\ge 0$。对于 $t = 0,1,\\dots,T-1$，重复以下步骤：\n   - 对查询执行 E 步：对于每个查询 $\\widehat{\\mathbf{q}}_i$，计算其对各个类别的责任\n     $$r_{ik}^{(t)} = \\frac{\\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_k^{(t)}\\right)}{\\sum_{j=1}^K \\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_j^{(t)}\\right)}.$$\n   - 对原型执行 M 步：对于每个类别 $k$，通过组合带标签的支持向量（作为对其类别的硬分配）和按责任加权的未标记查询来更新原型，然后重新归一化，\n     $$\\mathbf{m}_k^{(t+1)} = \\operatorname{norm}\\left(\\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\;+\\; \\lambda \\sum_{i} r_{ik}^{(t)} \\widehat{\\mathbf{q}}_i \\right).$$\n5. 查询的优化分类：经过 $T$ 次迭代后，计算优化后的原型 $\\mathbf{m}_k^{(T)}$，并像步骤 3 那样通过最近余弦相似度对查询进行分类，以获得优化后的准确率。\n6. 增益：对于每个情景，计算增益，即优化后的准确率减去基线准确率。增益必须以浮点数形式报告，并四舍五入到三位小数。\n\n角度单位：如果在内部计算角度，请使用弧度。要求的输出是无单位的小数。\n\n您必须实现此算法，并在以下情景测试套件上进行评估。每个情景由嵌入维度 $d$、类别数 $K$、温度 $\\tau$、查询权重 $\\lambda$、EM 迭代次数 $T$、每个类别的支持集嵌入以及带标签的查询集嵌入指定。所有向量必须被视为 $\\mathbb{R}^d$ 的元素，并按规定进行归一化。\n\n测试套件：\n- 情景 $1$：\n  - $d = 3$, $K = 3$, $\\tau = 10$, $\\lambda = 1$, $T = 5$。\n  - 支持集：\n    - 类别 $0$: $\\{[\\,1.0,\\,0.0,\\,0.0\\,],\\;[\\,0.8,\\,0.2,\\,0.0\\,]\\}$。\n    - 类别 $1$: $\\{[\\,0.0,\\,1.0,\\,0.0\\,],\\;[\\,0.1,\\,0.9,\\,0.0\\,]\\}$。\n    - 类别 $2$: $\\{[\\,0.0,\\,0.0,\\,1.0\\,],\\;[\\,0.1,\\,0.0,\\,0.9\\,]\\}$。\n  - 查询集（嵌入向量，标签）：\n    - $[\\,0.85,\\,0.15,\\,0.0\\,]$，标签 $0$，\n    - $[\\,0.15,\\,0.85,\\,0.0\\,]$，标签 $1$，\n    - $[\\,0.10,\\,0.10,\\,0.98\\,]$，标签 $2$，\n    - $[\\,0.65,\\,0.35,\\,0.0\\,]$，标签 $0$，\n    - $[\\,0.35,\\,0.65,\\,0.0\\,]$，标签 $1$。\n- 情景 $2$：\n  - $d = 2$, $K = 2$, $\\tau = 5$, $\\lambda = 2$, $T = 5$。\n  - 支持集：\n    - 类别 $0$: $\\{[\\,1.0,\\,0.0\\,],\\;[\\,0.9,\\,0.2\\,]\\}$。\n    - 类别 $1$: $\\{[\\,0.2,\\,0.98\\,],\\;[\\,0.0,\\,1.0\\,]\\}$。\n  - 查询集（嵌入向量，标签）：\n    - $[\\,0.70,\\,0.30\\,]$，标签 $0$，\n    - $[\\,0.60,\\,0.40\\,]$，标签 $0$，\n    - $[\\,0.40,\\,0.60\\,]$，标签 $1$，\n    - $[\\,0.30,\\,0.70\\,]$，标签 $1$。\n- 情景 $3$（具有重复支持向量和最小化优化的边缘案例）：\n  - $d = 2$, $K = 2$, $\\tau = 1$, $\\lambda = 0.5$, $T = 1$。\n  - 支持集：\n    - 类别 $0$: $\\{[\\,1.0,\\,0.0\\,],\\;[\\,1.0,\\,0.0\\,]\\}$。\n    - 类别 $1$: $\\{[\\,0.0,\\,1.0\\,],\\;[\\,0.0,\\,1.0\\,]\\}$。\n  - 查询集（嵌入向量，标签）：\n    - $[\\,1.0,\\,0.0\\,]$，标签 $0$，\n    - $[\\,0.0,\\,1.0\\,]$，标签 $1$。\n\n实现要求：\n- 您的程序必须严格按照上述算法实现，并且只能使用提供的测试套件。\n- 在进行任何余弦或平均运算之前，所有向量都必须归一化为单位范数；原型在每次更新时都必须重新归一化。\n- 基线准确率和优化后的准确率必须计算为正确分类的查询所占的比例，以 $[\\,0,\\,1\\,]$ 范围的小数表示。\n- 最终输出格式必须是单行，包含一个由方括号括起来的、以逗号分隔的列表，其中包含每个情景的增益，四舍五入到三位小数。例如，输出行必须类似于 $[\\,g_1, g_2, g_3\\,]$，其中每个 $g_i$ 是一个四舍五入到三位小数的浮点数。\n\n不提供外部输入；所有数据都嵌入在程序中。此问题中没有物理单位。如果在内部使用角度，则必须使用弧度。期望的输出是指定格式的无单位小数。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表（例如，“[result1,result2,result3]”）。", "solution": "该问题要求实现并分析一个用于密集向量嵌入的原型优化算法。该算法是期望最大化（EM）在少样本学习上下文中的一种变体。任务是计算对于几个测试情景，通过此优化过程相比于基线方法所获得的分类准确率增益。\n\n首先，需要对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **向量归一化：** 所有嵌入向量 $\\mathbf{x}$ 都被归一化为单位欧几里得范数：$\\widehat{\\mathbf{x}} = \\mathbf{x} / \\|\\mathbf{x}\\|_2$。我们将其表示为函数 $\\operatorname{norm}(\\mathbf{x})$。\n- **基线原型（$\\mathbf{m}_k^{(0)}$）：** 对于每个类别 $k$，初始原型是其归一化支持向量的归一化均值。给定类别 $k$ 的支持向量集合为 $\\mathcal{S}_k$，原型为 $\\mathbf{m}_k^{(0)} = \\operatorname{norm}\\left(\\frac{1}{|\\mathcal{S}_k|} \\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\right)$。\n- **基线分类：** 通过找到具有最高余弦相似度的原型来对查询 $\\widehat{\\mathbf{q}}$ 进行分类：$\\widehat{y}(\\widehat{\\mathbf{q}}) = \\arg\\max_{k} \\widehat{\\mathbf{q}}^\\top \\mathbf{m}_k^{(0)}$。\n- **EM 优化：** 该算法迭代 $T$ 步，使用温度参数 $\\tau > 0$ 和查询权重 $\\lambda \\ge 0$ 来更新原型 $\\mathbf{m}_k^{(t)}$。\n- **E 步（责任）：** 对于每个查询 $\\widehat{\\mathbf{q}}_i$，类别 $k$ 对此查询的责任是使用余弦相似度的 softmax 函数计算得出的：\n  $$r_{ik}^{(t)} = \\frac{\\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_k^{(t)}\\right)}{\\sum_{j=1}^K \\exp\\left(\\tau \\, \\widehat{\\mathbf{q}}_i^\\top \\mathbf{m}_j^{(t)}\\right)}$$\n- **M 步（原型更新）：** 类别 $k$ 的原型通过对其支持向量和所有查询向量的加权求和然后进行归一化来更新。支持向量作为硬分配（权重为 $1$），而查询则通过责任进行软分配：\n  $$\\mathbf{m}_k^{(t+1)} = \\operatorname{norm}\\left(\\sum_{\\mathbf{s}\\in \\mathcal{S}_k} \\widehat{\\mathbf{s}} \\;+\\; \\lambda \\sum_{i} r_{ik}^{(t)} \\widehat{\\mathbf{q}}_i \\right)$$\n- **优化分类：** 经过 $T$ 次迭代后，使用最终原型 $\\mathbf{m}_k^{(T)}$ 对查询进行分类。\n- **性能指标（增益）：** 增益 = 优化后准确率 - 基线准确率。增益必须四舍五入到三位小数。\n- **测试套件：** 提供了三个具体的情景，并定义了所有参数（$d, K, \\tau, \\lambda, T$）和数据（支持集和查询集嵌入）。\n\n### 步骤 2：使用提取的已知条件进行验证\n对问题进行严格验证。\n1.  **科学依据：** 该问题在机器学习领域，特别是在少样本学习和半监督学习的向量嵌入方面，有充分的理论基础。EM、余弦相似度和基于原型的分类都是标准概念。数学公式是正确的，并与该领域的文献一致。\n2.  **良态性（Well-Posed）：** 算法步骤是确定性定义的。给定每个情景的输入数据和参数，该过程会产生一个唯一的、可计算的结果。\n3.  **客观性：** 问题使用精确、客观的数学语言表述，没有任何主观性。\n4.  **不完整或矛盾的设置：** 为每个测试用例提供了所有必要的参数和数据。定义是自洽的。例如，在所有步骤中都一致地指定了使用归一化向量（$\\widehat{\\mathbf{s}}$，$\\widehat{\\mathbf{q}}$）。M 步的公式，尽管其加权方案（支持向量的总和 vs. 查询的加权和）略显不寻常，但其定义明确无歧义，代表了一种有效的设计选择。\n5.  **不切实际或不可行：** 该问题使用低维向量以保证计算的可行性，这对于一个测试问题是合适的。所有提供的向量都是非零的，确保了归一化总是有定义的。\n6.  **所有其他标准：** 该问题是形式化的、相关的、非病态的、非平凡的且计算上可验证的。\n\n### 步骤 3：结论与行动\n该问题被认为是**有效的**，因为它科学上合理，自洽且良态。我们可以着手开发解决方案。\n\n### 算法解决方案\n将按照每个情景的指定步骤来实现解决方案。\n\n1.  **数据准备：** 所有支持向量和查询向量首先被归一化为单位欧几里得范数。这是一个关键的预处理步骤，因为算法始终在单位超球面上操作。\n2.  **基线性能：**\n    - 对于每个类别，通过首先平均该类别的归一化支持向量，然后重新归一化得到的均值向量来计算基线原型。\n    - 然后我们通过找到产生最高余弦相似度（对于单位向量，这等同于最高点积）的基线原型来对每个归一化的查询向量进行分类。\n    - 基线准确率是正确分类的查询所占的比例。\n3.  **基于 EM 的优化：**\n    - 原型用基线原型进行初始化。\n    - 我们通过 E 步和 M 步迭代 $T$ 次。\n    - **E 步：** 对于每个查询，我们计算其与当前原型集的相似度分数。这些分数由温度 $\\tau$ 缩放，然后使用 softmax 函数转换为关于类别的概率分布（责任）。使用数值稳定的 softmax 实现来防止由于大参数引起的上溢/下溢问题。\n    - **M 步：** 计算一组新的原型。对于每个类别 $k$，新的原型是两个分量之和的归一化结果：(i) 其所有归一化支持向量的总和，以及 (ii) 所有归一化查询向量的加权和，其中每个查询的贡献由其对类别 $k$ 的责任和全局查询权重 $\\lambda$ 缩放。\n4.  **优化后性能：**\n    - 经过 $T$ 次迭代后，使用最终的优化原型对查询向量进行分类。\n    - 优化后的准确率是使用这些最终原型正确分类的查询所占的比例。\n5.  **增益计算：**\n    - 对于每个情景，增益计算为差异值：`优化后准确率 - 基线准确率`。\n    - 然后根据要求将结果四舍五入到三位小数。\n\n实现将处理三个测试情景中的每一个，并计算相应的增益。然后将这些增益编译成一个列表以供输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithm, and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d\": 3, \"K\": 3, \"tau\": 10, \"lamb\": 1, \"T\": 5,\n            \"supports\": {\n                0: np.array([[1.0, 0.0, 0.0], [0.8, 0.2, 0.0]]),\n                1: np.array([[0.0, 1.0, 0.0], [0.1, 0.9, 0.0]]),\n                2: np.array([[0.0, 0.0, 1.0], [0.1, 0.0, 0.9]])\n            },\n            \"queries\": [\n                (np.array([0.85, 0.15, 0.0]), 0),\n                (np.array([0.15, 0.85, 0.0]), 1),\n                (np.array([0.10, 0.10, 0.98]), 2),\n                (np.array([0.65, 0.35, 0.0]), 0),\n                (np.array([0.35, 0.65, 0.0]), 1)\n            ]\n        },\n        {\n            \"d\": 2, \"K\": 2, \"tau\": 5, \"lamb\": 2, \"T\": 5,\n            \"supports\": {\n                0: np.array([[1.0, 0.0], [0.9, 0.2]]),\n                1: np.array([[0.2, 0.98], [0.0, 1.0]])\n            },\n            \"queries\": [\n                (np.array([0.70, 0.30]), 0),\n                (np.array([0.60, 0.40]), 0),\n                (np.array([0.40, 0.60]), 1),\n                (np.array([0.30, 0.70]), 1)\n            ]\n        },\n        {\n            \"d\": 2, \"K\": 2, \"tau\": 1, \"lamb\": 0.5, \"T\": 1,\n            \"supports\": {\n                0: np.array([[1.0, 0.0], [1.0, 0.0]]),\n                1: np.array([[0.0, 1.0], [0.0, 1.0]])\n            },\n            \"queries\": [\n                (np.array([1.0, 0.0]), 0),\n                (np.array([0.0, 1.0]), 1)\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        gain = run_episode(case['d'], case['K'], case['tau'], case['lamb'], case['T'],\n                           case['supports'], case['queries'])\n        results.append(gain)\n\n    # Format the final output string as [gain1,gain2,gain3] with three decimal places.\n    formatted_results = [f\"{r:.3f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef run_episode(d, K, tau, lamb, T, supports, queries_with_labels):\n    \"\"\"\n    Implements the prototype refinement algorithm for a single few-shot episode.\n    \"\"\"\n    \n    # Helper to normalize a single vector or a 2D array of vectors.\n    def normalize(v):\n        if v.ndim == 1:\n            norm = np.linalg.norm(v)\n            return v / norm if norm > 0 else v\n        # For 2D array of vectors\n        norms = np.linalg.norm(v, axis=1, keepdims=True)\n        # Handle potential division by zero for zero vectors.\n        return np.divide(v, norms, out=np.zeros_like(v, dtype=float), where=norms!=0)\n\n    # Helper for numerically stable softmax.\n    def stable_softmax(x, axis=-1):\n        x = x - np.max(x, axis=axis, keepdims=True)\n        exps = np.exp(x)\n        return exps / np.sum(exps, axis=axis, keepdims=True)\n\n    # --- Step 1: Data Preparation ---\n    queries = np.array([q[0] for q in queries_with_labels])\n    query_labels = np.array([q[1] for q in queries_with_labels])\n    \n    # Normalize all embeddings to unit norm.\n    hat_supports_per_class = {k: normalize(supports[k]) for k in range(K)}\n    hat_queries = normalize(queries)\n\n    # --- Step 2: Baseline Prototypes ---\n    baseline_prototypes = np.zeros((K, d))\n    for k in range(K):\n        mean_s = np.mean(hat_supports_per_class[k], axis=0)\n        baseline_prototypes[k] = normalize(mean_s)\n\n    # --- Step 3: Baseline Classification ---\n    # For unit vectors, cosine similarity is the dot product.\n    baseline_scores = hat_queries @ baseline_prototypes.T\n    baseline_preds = np.argmax(baseline_scores, axis=1)\n    baseline_accuracy = np.mean(baseline_preds == query_labels)\n\n    # --- Step 4: EM Refinement ---\n    refined_prototypes = np.copy(baseline_prototypes)\n    \n    # Pre-calculate sum of normalized support vectors (constant through iterations).\n    sum_hat_supports = np.array([np.sum(hat_supports_per_class[k], axis=0) for k in range(K)])\n\n    for _ in range(T):\n        # E-step: Compute responsibilities for each query.\n        em_scores = tau * (hat_queries @ refined_prototypes.T)\n        responsibilities = stable_softmax(em_scores, axis=1)\n        \n        # M-step: Update prototypes.\n        weighted_sum_queries = responsibilities.T @ hat_queries\n        new_prototypes_unnorm = sum_hat_supports + lamb * weighted_sum_queries\n        refined_prototypes = normalize(new_prototypes_unnorm)\n\n    # --- Step 5: Refined Classification ---\n    refined_scores = hat_queries @ refined_prototypes.T\n    refined_preds = np.argmax(refined_scores, axis=1)\n    refined_accuracy = np.mean(refined_preds == query_labels)\n\n    # --- Step 6: Gain Calculation ---\n    gain = refined_accuracy - baseline_accuracy\n    return gain\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3114433"}]}