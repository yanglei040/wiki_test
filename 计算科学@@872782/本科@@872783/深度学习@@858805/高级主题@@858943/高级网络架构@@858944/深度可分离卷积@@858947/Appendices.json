{"hands_on_practices": [{"introduction": "深度可分离卷积的一个核心优势在于其计算效率。本练习将引导你完成一个基本但至关重要的任务：推导卷积操作的输出维度公式，并用它来精确计算一个深度可分离卷积块所需的乘加运算总数。通过这个实践 [@problem_id:3115192]，你将亲手验证深度可分离卷积为何能成为标准卷积的轻量级替代方案，从而巩固对其工作原理和效率优势的理解。", "problem": "深度可分离卷积（Depthwise Separable Convolution, DSC）由一个深度卷积（depthwise convolution）后接一个逐点卷积（pointwise convolution）组成。深度卷积为每个输入通道应用一个空间滤波器，而逐点卷积使用空间尺寸为 $1 \\times 1$ 的滤波器来混合通道。考虑一个二维离散卷积，并使用以下基本原理：\n\n- 单通道输入 $x$ 与核 $w$ 在空间位置 $(u,v)$ 的二维离散卷积计算了 $x$ 的一个局部感受野上的加权和。\n- 大小为 $p$ 的零填充（zero-padding）在每个空间边界上将输入扩展 $p$ 个零，步长 $s$ 则使卷积窗口沿每个空间轴前进 $s$ 个像素。\n\n基于这些原理，推导出一个计数论证，以确定当深度卷积应用于空间尺寸为 $(H,W)$ 的输入，使用大小为 $k \\times k$ 的方形核、步长为 $s$、对称零填充为 $p$ 时，沿高度和宽度可以产生多少个有效的空间位置。\n\n然后，将您的推导应用于以下DSC模块：\n\n- 输入张量的高度 $H = 128$，宽度 $W = 96$，通道数 $C_{\\text{in}} = 32$。\n- 深度卷积使用大小为 $k = 5$ 的方形核，步长 $s = 2$，在高度和宽度上均使用对称零填充 $p = 1$。\n- 逐点卷积使用 $C_{\\text{out}} = 64$ 个输出通道，步长为 $1$，零填充为 $0$。\n\n假设没有扩张（dilation），并且逐点卷积保留了深度卷积产生的空间维度。将一次乘加运算（multiply-add operation）定义为一次乘法紧随一次加法，对每个贡献于单个输出值的权重-激活对计数一次。偏置项不计入。\n\n计算处理一个输入图像通过上述整个DSC模块（深度阶段加逐点阶段）所需的乘加运算总数。将最终答案表示为一个没有单位且不进行四舍五入的整数。只提供这个总数作为您的最终答案。", "solution": "问题要求完成两个任务：首先，推导二维卷积的空间输出维度公式；其次，应用此公式计算一个特定的深度可分离卷积（DSC）模块的总乘加运算次数。\n\n首先，我们推导输出维度的公式。考虑输入张量的一个空间维度，其大小为 $D_{in}$。当应用大小为 $p$ 的对称零填充时，该维度的两端各增加 $p$ 个零。填充后维度的有效大小变为 $D_{padded} = D_{in} + 2p$。一个大小为 $k$ 的核以步长 $s$ 在这个填充后的维度上进行卷积。\n\n设核的位置由索引 $i$ 表示，从 $i=0$ 开始。在位置 $i$ 处，核所覆盖的感受野从索引 $i \\times s$ 开始，到索引 $i \\times s + k - 1$ 结束（使用基于0的索引）。为了使核处于有效位置，其整个感受野必须位于填充后的维度内。因此，所覆盖的最后一个索引必须小于填充后维度的大小：\n$$i \\times s + k - 1  D_{padded}$$\n$$i \\times s \\le D_{padded} - k$$\n$$i \\le \\frac{D_{padded} - k}{s}$$\n由于 $i$ 必须是整数，索引 $i$ 的最大值为 $\\lfloor \\frac{D_{padded} - k}{s} \\rfloor$。有效位置的数量是 $i$ 可能的整数值的个数，范围从 $0$ 到 $\\lfloor \\frac{D_{padded} - k}{s} \\rfloor$。这样的位置总数，即对应于输出维度 $D_{out}$，是：\n$$D_{out} = \\left\\lfloor \\frac{D_{padded} - k}{s} \\right\\rfloor + 1$$\n代入 $D_{padded} = D_{in} + 2p$，我们得到输出空间维度的一般公式：\n$$D_{out} = \\left\\lfloor \\frac{D_{in} + 2p - k}{s} \\right\\rfloor + 1$$\n将此公式应用于高度 ($H$) 和宽度 ($W$) 维度，我们得到：\n$$H_{out} = \\left\\lfloor \\frac{H + 2p - k}{s} \\right\\rfloor + 1$$\n$$W_{out} = \\left\\lfloor \\frac{W + 2p - k}{s} \\right\\rfloor + 1$$\n这完成了问题的第一部分。\n\n接下来，我们计算给定DSC模块的乘加运算总数。总成本 $\\text{Ops}_{\\text{total}}$ 是深度卷积阶段成本 $\\text{Ops}_{\\text{dw}}$ 和逐点卷积阶段成本 $\\text{Ops}_{\\text{pw}}$ 的总和。\n\n**1. 深度卷积阶段**\n输入张量的维度为 $H = 128$，$W = 96$，$C_{\\text{in}} = 32$。\n深度卷积的参数为：核大小 $k = 5$，步长 $s = 2$，填充 $p = 1$。\n首先，我们计算此阶段输出特征图的空间维度，记为 $(H_{\\text{dw}}, W_{\\text{dw}})$：\n$$H_{\\text{dw}} = \\left\\lfloor \\frac{128 + 2(1) - 5}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{125}{2} \\right\\rfloor + 1 = 62 + 1 = 63$$\n$$W_{\\text{dw}} = \\left\\lfloor \\frac{96 + 2(1) - 5}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{93}{2} \\right\\rfloor + 1 = 46 + 1 = 47$$\n深度卷积对 $C_{\\text{in}}$ 个输入通道中的每一个应用一个独立的 $k \\times k$ 滤波器。计算一个输出通道特征图中的单个值需要 $k \\times k$ 次乘加运算。这个过程对所有 $H_{\\text{dw}} \\times W_{\\text{dw}}$ 个空间位置和所有 $C_{\\text{in}}$ 个通道重复进行。\n深度卷积阶段的总运算次数为：\n$$\\text{Ops}_{\\text{dw}} = C_{\\text{in}} \\times k \\times k \\times H_{\\text{dw}} \\times W_{\\text{dw}}$$\n代入给定值：\n$$\\text{Ops}_{\\text{dw}} = 32 \\times 5 \\times 5 \\times 63 \\times 47$$\n$$\\text{Ops}_{\\text{dw}} = 32 \\times 25 \\times 2961$$\n$$\\text{Ops}_{\\text{dw}} = 800 \\times 2961 = 2,368,800$$\n\n**2. 逐点卷积阶段**\n此阶段的输入是深度卷积阶段的输出，其维度为 $(H_{\\text{in}}, W_{\\text{in}}, C_{\\text{in}}) = (63, 47, 32)$。\n逐点卷积是使用 $1 \\times 1$ 滤波器的标准卷积。其参数为：核大小 $k_{pw} = 1$，步长 $s_{pw} = 1$，填充 $p_{pw} = 0$，输出通道数 $C_{\\text{out}} = 64$。空间维度被保留，这与这些参数是一致的。\n要在 $C_{\\text{out}}$ 个输出通道之一的单个空间位置上计算一个值，需要在 $C_{\\text{in}}$ 个输入通道上执行点积。这需要 $1 \\times 1 \\times C_{\\text{in}} = C_{\\text{in}}$ 次乘加运算。此计算对输出图中的所有空间位置 ($H_{\\text{dw}} \\times W_{\\text{dw}}$) 和每个输出通道 ($C_{\\text{out}}$) 重复进行。\n逐点卷积阶段的总运算次数为：\n$$\\text{Ops}_{\\text{pw}} = C_{\\text{in}} \\times C_{\\text{out}} \\times H_{\\text{dw}} \\times W_{\\text{dw}}$$\n代入数值：\n$$\\text{Ops}_{\\text{pw}} = 32 \\times 64 \\times 63 \\times 47$$\n$$\\text{Ops}_{\\text{pw}} = 2048 \\times 2961 = 6,064,128$$\n\n**总运算次数**\n整个DSC模块的乘加运算总数是两个阶段运算次数的总和：\n$$\\text{Ops}_{\\text{total}} = \\text{Ops}_{\\text{dw}} + \\text{Ops}_{\\text{pw}} = 2,368,800 + 6,064,128 = 8,432,928$$", "answer": "$$\\boxed{8432928}$$", "id": "3115192"}, {"introduction": "理解了深度可分离卷积的内部机制后，将其与我们熟悉的标准卷积联系起来至关重要。本练习 [@problem_id:3185403] 设计了一个巧妙的思维实验，通过具体的数值计算，揭示了深度可分离卷积在特定条件下等价于一个结构化的标准卷积。完成这个练习将帮助你从根本上理解这两种卷积操作之间的关系，即深度可分离卷积是标准卷积的一种因子分解形式。", "problem": "一个卷积神经网络（CNN）层通过应用由卷积核定义的空间聚合来执行前向传播。考虑一个具有 $C = 2$ 个通道和空间维度 $H = W = 3$ 的合成输入张量，其中每个通道在 Frobenius 内积下携带一个正交模式。输入通道为\n$$\nX^{(1)} = \\begin{pmatrix}\n1  0  -1 \\\\\n1  0  -1 \\\\\n1  0  -1\n\\end{pmatrix},\n\\qquad\nX^{(2)} = \\begin{pmatrix}\n1  1  1 \\\\\n0  0  0 \\\\\n-1  -1  -1\n\\end{pmatrix}.\n$$\n你将计算一个深度可分离卷积的前向传播过程，并将其与一个全卷积进行比较，两者都使用相同的步幅和填充约定。\n\n此处深度可分离卷积定义如下：\n- 深度步骤：对于每个通道 $c \\in \\{1,2\\}$，使用步幅为 $1$ 和有效填充（valid padding）将 $X^{(c)}$ 与一个通道特定的空间核 $K^{(c)}$ 进行卷积，产生一个标量 $d^{(c)} \\in \\mathbb{R}$，因为核与输入具有相同的空间大小。\n- 逐点步骤：将深度步骤的输出与一个 $1 \\times 1$ 的逐点权重向量 $p = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$ 进行线性混合，以获得一个单一输出的标量 $y_{\\mathrm{dw}} = \\alpha \\, d^{(1)} + \\beta \\, d^{(2)}$。\n\n在此设置中，全卷积被定义为一种单一输出的卷积，其核 $W$ 对每个输入通道都有一个空间切片，应用步幅为 $1$ 和有效填充（valid padding）以产生一个标量 $y_{\\mathrm{full}}$。\n\n使用以下具体的、科学上一致的选择：\n- 深度核等于通道模式：$K^{(1)} = X^{(1)}$ 和 $K^{(2)} = X^{(2)}$。\n- 逐点混合权重为 $\\alpha = 2$ 和 $\\beta = -3$。\n- 全卷积核 $W$ 被选择为具有通道切片 $W^{(1)} = \\alpha \\, K^{(1)}$ 和 $W^{(2)} = \\beta \\, K^{(2)}$。\n\n从前向传播作为通过卷积进行的线性聚合的核心定义出发，计算两个输出 $y_{\\mathrm{dw}}$ 和 $y_{\\mathrm{full}}$，然后确定其绝对差的平方\n$$\n\\Delta = \\left| y_{\\mathrm{dw}} - y_{\\mathrm{full}} \\right|^{2}.\n$$\n将你的最终答案表示为一个实数。不需要四舍五入。", "solution": "该问题要求计算两个量，$y_{\\mathrm{dw}}$ 和 $y_{\\mathrm{full}}$，它们分别由深度可分离卷积和全卷积产生，然后求出它们之间的差的平方。\n\n首先，我们必须如所述形式化卷积操作。输入通道 $X^{(c)}$ 和空间核 $K^{(c)}$ 的维度均为 $3 \\times 3$。卷积使用步幅为 $1$ 和“有效”（valid）填充进行。对于大小为 $N \\times N$ 的输入和大小为 $F \\times F$ 的核，步幅为 $S$，填充为 $P$，输出空间维度由 $\\lfloor \\frac{N - F + 2P}{S} \\rfloor + 1$ 给出。当 $N=3, F=3, S=1, P=0$（对于有效填充）时，输出维度为 $\\lfloor \\frac{3 - 3 + 0}{1} \\rfloor + 1 = 1$。这意味着每个通道的空间卷积输出是一个单一标量。该标量的值是核与输入矩阵逐元素乘积之和，这正是 Frobenius 内积的定义，$\\langle A, B \\rangle_F = \\sum_{i,j} A_{ij} B_{ij}$。\n\n让我们首先计算深度可分离卷积的输出 $y_{\\mathrm{dw}}$。这个过程包括两个步骤：深度步骤和逐点步骤。\n\n**1. 深度步骤：**\n对于每个输入通道 $c$，我们将输入 $X^{(c)}$ 与其对应的深度核 $K^{(c)}$ 进行卷积。输出是一个标量 $d^{(c)}$。\n$$\nd^{(c)} = \\text{conv}(X^{(c)}, K^{(c)}) = \\langle X^{(c)}, K^{(c)} \\rangle_F\n$$\n问题指定深度核等于输入通道模式本身：$K^{(1)} = X^{(1)}$ 和 $K^{(2)} = X^{(2)}$。\n\n对于第一个通道 ($c=1$)：\n$$\nd^{(1)} = \\langle X^{(1)}, K^{(1)} \\rangle_F = \\langle X^{(1)}, X^{(1)} \\rangle_F = \\|X^{(1)}\\|_F^2\n$$\n给定 $X^{(1)}$ 的矩阵：\n$$\nX^{(1)} = \\begin{pmatrix}\n1  0  -1 \\\\\n1  0  -1 \\\\\n1  0  -1\n\\end{pmatrix}\n$$\nFrobenius 范数的平方是其元素平方的总和：\n$$\nd^{(1)} = 1^2 + 0^2 + (-1)^2 + 1^2 + 0^2 + (-1)^2 + 1^2 + 0^2 + (-1)^2 = 1 + 0 + 1 + 1 + 0 + 1 + 1 + 0 + 1 = 6\n$$\n\n对于第二个通道 ($c=2$)：\n$$\nd^{(2)} = \\langle X^{(2)}, K^{(2)} \\rangle_F = \\langle X^{(2)}, X^{(2)} \\rangle_F = \\|X^{(2)}\\|_F^2\n$$\n给定 $X^{(2)}$ 的矩阵：\n$$\nX^{(2)} = \\begin{pmatrix}\n1  1  1 \\\\\n0  0  0 \\\\\n-1  -1  -1\n\\end{pmatrix}\n$$\nFrobenius 范数的平方是：\n$$\nd^{(2)} = 1^2 + 1^2 + 1^2 + 0^2 + 0^2 + 0^2 + (-1)^2 + (-1)^2 + (-1)^2 = 1 + 1 + 1 + 0 + 0 + 0 + 1 + 1 + 1 = 6\n$$\n\n**2. 逐点步骤：**\n来自深度步骤的标量 $d^{(1)}=6$ 和 $d^{(2)}=6$，使用逐点权重向量 $p = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$ 进行线性组合。混合权重给定为 $\\alpha = 2$ 和 $\\beta = -3$。\n$$\ny_{\\mathrm{dw}} = \\alpha \\, d^{(1)} + \\beta \\, d^{(2)}\n$$\n代入数值：\n$$\ny_{\\mathrm{dw}} = (2)(6) + (-3)(6) = 12 - 18 = -6\n$$\n\n接下来，我们计算全卷积的输出 $y_{\\mathrm{full}}$。\n全卷积使用一个 3D 核 $W$ 对整个输入体（所有通道）进行操作，该核对每个输入通道 $c$ 都有空间切片 $W^{(c)}$。由于输出是一个单一标量，该操作是所有通道上相应输入切片和核切片之间 Frobenius 内积的总和。\n$$\ny_{\\mathrm{full}} = \\sum_{c=1}^{2} \\langle X^{(c)}, W^{(c)} \\rangle_F = \\langle X^{(1)}, W^{(1)} \\rangle_F + \\langle X^{(2)}, W^{(2)} \\rangle_F\n$$\n问题将全卷积的核切片指定为 $W^{(1)} = \\alpha \\, K^{(1)}$ 和 $W^{(2)} = \\beta \\, K^{(2)}$。我们还知道 $K^{(1)} = X^{(1)}$ 和 $K^{(2)} = X^{(2)}$。因此，我们可以将核切片写为：\n$$\nW^{(1)} = \\alpha \\, X^{(1)}\n\\quad \\text{和} \\quad\nW^{(2)} = \\beta \\, X^{(2)}\n$$\n将这些代入 $y_{\\mathrm{full}}$ 的表达式中：\n$$\ny_{\\mathrm{full}} = \\langle X^{(1)}, \\alpha X^{(1)} \\rangle_F + \\langle X^{(2)}, \\beta X^{(2)} \\rangle_F\n$$\n使用内积的线性性质，即对于标量 $k$，有 $\\langle A, k B \\rangle = k \\langle A, B \\rangle$：\n$$\ny_{\\mathrm{full}} = \\alpha \\langle X^{(1)}, X^{(1)} \\rangle_F + \\beta \\langle X^{(2)}, X^{(2)} \\rangle_F\n$$\n这个表达式等价于：\n$$\ny_{\\mathrm{full}} = \\alpha \\|X^{(1)}\\|_F^2 + \\beta \\|X^{(2)}\\|_F^2\n$$\n我们认识到 $\\|X^{(1)}\\|_F^2 = d^{(1)}$ 并且 $\\|X^{(2)}\\|_F^2 = d^{(2)}$。因此，$y_{\\mathrm{full}}$ 的表达式与 $y_{\\mathrm{dw}}$ 的表达式是相同的：\n$$\ny_{\\mathrm{full}} = \\alpha \\, d^{(1)} + \\beta \\, d^{(2)} = y_{\\mathrm{dw}}\n$$\n这表明，对于本问题中核的特定构造，全卷积在数学上等价于深度可分离卷积。\n在数值上，我们有：\n$$\ny_{\\mathrm{full}} = -6\n$$\n\n最后，我们计算所求的量 $\\Delta$，即 $y_{\\mathrm{dw}}$ 和 $y_{\\mathrm{full}}$ 之间的绝对差的平方。\n$$\n\\Delta = | y_{\\mathrm{dw}} - y_{\\mathrm{full}} |^2\n$$\n由于 $y_{\\mathrm{dw}} = y_{\\mathrm{full}} = -6$：\n$$\n\\Delta = | -6 - (-6) |^2 = |0|^2 = 0\n$$\n绝对差的平方为 $0$。输入通道 $X^{(1)}$ 和 $X^{(2)}$ 的正交性是所提供数据的一个属性，但在本问题特定的核定义下，建立 $y_{\\mathrm{dw}}$ 和 $y_{\\mathrm{full}}$ 相等的过程中并不直接需要它。这种相等关系的出现是因为全卷积核 $W$ 的构造方式使其“可分离”，这与深度可分离卷积的定义方式完全相同。", "answer": "$$\\boxed{0}$$", "id": "3185403"}, {"introduction": "神经网络的强大能力不仅源于卷积等线性操作，更来自于与之交织的非线性激活函数。本练习 [@problem_id:3115159] 将探讨一个精细但至关重要的设计选择：在深度可分离卷积块中放置非线性激活函数（如ReLU）的位置。通过理论推导和编程实验，你将发现不同的放置策略会构建出表达能力截然不同的模型，从而学会超越简单地堆叠网络层，更深入地思考层级结构的设计细节。", "problem": "考虑一维深度可分离卷积在以下简化但科学合理的情形下的应用：空间核大小为 $1$，深度滤波器在每个通道上为恒等变换，且该操作简化为逐通道偏置后进行跨通道混合。从核心定义出发：卷积是线性移不变映射，深度可分离结构是逐通道卷积后接一个 $1 \\times 1$ 的跨通道逐点混合，整流线性单元（ReLU）非线性定义为 $\\,\\mathrm{ReLU}(t)=\\max\\{0,t\\}\\,$。在这些约束下，有两种非线性放置顺序需要比较：(A) ReLU 在深度阶段之后、逐点阶段之前，以及 (B) ReLU 在逐点阶段之后。您的任务是推导每种顺序所代表的函数类别，并在一个合成可分性基准上经验性地测试它们的表征能力所带来的影响。请使用纯数学术语，处理一个在单一空间位置上具有 $C=2$ 个通道的输入，记为 $x=(x_1,x_2) \\in \\mathbb{R}^2$。\n\n推导任务：\n- 根据上述核心定义，推导当深度滤波器为恒等变换且核大小为 $1$ 时，顺序 (A) 和顺序 (B) 的函数形式。仔细识别线性映射和 ReLU 的位置。不要假设任何快捷公式；从逐通道线性变换加偏置开始，然后是跨通道线性混合，并根据每种顺序放置 ReLU。\n\n基准测试任务：\n- 定义以 $0$ 为阈值的分类：如果一个模型的标量输出大于或等于 $0$，则预测类别为 $1$，否则为类别 $0$。使用三个合成测试用例来探究非线性放置顺序的作用：\n    1. 用例 1（需要混合的半空间）：输入为网格 $S=\\{-1.0,-0.5,0.0,0.5,1.0\\}$ 上的所有点对 $(x_1,x_2)$，共产生 $25$ 个点。如果 $x_1 - x_2 \\ge 0$，则目标类别为 $1$，否则为 $0$。这个目标要求在阈值化之前进行混合。\n    2. 用例 2（正部之和）：输入为来自网格 $S$ 的相同 $25$ 个点。如果 $\\mathrm{ReLU}(x_1) + 2\\,\\mathrm{ReLU}(x_2) - 1.0 \\ge 0$，则目标类别为 $1$，否则为 $0$。这个目标要求在混合之前进行逐通道整流。\n    3. 用例 3（非负边界等价性）：输入为网格 $S_+=\\{0.0,0.5,1.0\\}$ 上的所有点对 $(x_1,x_2)$，共产生 $9$ 个点。如果 $x_1 + 2\\,x_2 - 1.0 \\ge 0$，则目标类别为 $1$，否则为 $0$。对于非负输入，逐通道整流没有效果，因此对于这个半空间，两种顺序在表征能力上应该是等价的。\n\n评估协议：\n- 对于每种用例和每种顺序，在整数参数网格上进行搜索，以找到在上述分类规则下对给定输入能达到的最佳分类准确率。参数化如下：\n    - 深度偏置 $b_{d,1}, b_{d,2} \\in \\{-1,0,1\\}$。\n    - 逐点混合权重 $w_1, w_2 \\in \\{-2,-1,0,1,2\\}$。\n    - 输出偏置 $b_p \\in \\{-2,-1,0,1,2\\}$。\n- 对于顺序 (A)，标量输出是对经过逐通道整流和偏置的输入应用跨通道线性混合，再加上输出偏置，没有进一步的非线性。如果输出 $\\ge 0$，则预测为类别 $1$，否则为类别 $0$。\n- 对于顺序 (B)，标量输出是对偏置后的输入的跨通道线性混合加上输出偏置的结果进行整流（通过 ReLU）。如果输出 $\\ge 0$，则预测为类别 $1$，否则为类别 $0$，这等价于检查 ReLU 之前的总和是否 $\\ge 0$。\n\n测试套件与最终输出规范：\n- 实现两种顺序，并为三个用例中的每一个评估在指定参数网格上能达到的最佳分类准确率。\n- 您的程序应生成一行输出，其中包含六个准确率，顺序如下：$[\\text{A1},\\text{B1},\\text{A2},\\text{B2},\\text{A3},\\text{B3}]$，其中 $\\text{A}k$ 是顺序 (A) 在用例 $k$ 上的最佳准确率，$\\text{B}k$ 是顺序 (B) 在用例 $k$ 上的最佳准确率。\n- 最终输出是 $[0,1]$ 范围内的浮点数，每个值等于在该顺序和用例下找到的最佳参数所能正确分类的输入所占的比例。不涉及物理单位或角度单位。\n\n您的解决方案必须从卷积、线性混合和整流线性单元（ReLU）的基本定义出发，推导两种顺序的函数类别，并实现所述的基准测试和搜索协议以产生指定的输出。", "solution": "该问题要求推导一个简化的深度可分离卷积层的两种变体的函数形式，然后在一系列合成的分类任务上进行经验性评估。验证表明，该问题是科学合理的、良定的，并且提供了获得完整解决方案所需的所有信息。\n\n### 函数形式的推导\n\n背景设定为一个空间核大小为 $1$ 的深度可分离卷积，作用于一个单一空间位置上，具有 $C=2$ 个通道的输入，该输入由向量 $x = (x_1, x_2)^T \\in \\mathbb{R}^2$ 表示。该操作由一个深度阶段和一个逐点阶段组成。\n\n**1. 阶段定义**\n\n*   **深度阶段**：问题指定深度滤波器为恒等变换，核大小为 $1$。这将逐通道卷积简化为一个对每个通道独立应用偏置的函数。设深度偏置为 $b_d = (b_{d,1}, b_{d,2})^T$。此阶段的输出 $x'$ 由下式给出：\n    $$\n    x' = \\begin{pmatrix} x'_1 \\\\ x'_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot x_1 + b_{d,1} \\\\ 1 \\cdot x_2 + b_{d,2} \\end{pmatrix} = \\begin{pmatrix} x_1 + b_{d,1} \\\\ x_2 + b_{d,2} \\end{pmatrix}\n    $$\n\n*   **逐点阶段**：此阶段执行 $1 \\times 1$ 卷积，这简化为输入通道的线性组合，以产生单个标量输出。设此阶段的输入为向量 $z = (z_1, z_2)^T$。逐点权重为 $w = (w_1, w_2)$，并有一个单一的输出偏置 $b_p$。输出为：\n    $$\n    y_{\\text{out}} = w_1 z_1 + w_2 z_2 + b_p\n    $$\n\n*   **非线性**：整流线性单元（ReLU）定义为 $\\mathrm{ReLU}(t) = \\max\\{0, t\\}$。\n\n我们现在推导两种指定的非线性放置顺序的函数形式。\n\n**2. 顺序 (A)：ReLU 在深度阶段之后，逐点阶段之前**\n\n在此配置中，ReLU 激活函数应用于深度阶段的输出，其结果再被送入逐点阶段。\n\n1.  输入 $x=(x_1, x_2)^T$ 通过深度阶段，得到 $x'=(x_1+b_{d,1}, x_2+b_{d,2})^T$。\n2.  ReLU 函数逐元素地应用于 $x'$：\n    $$\n    x'' = \\begin{pmatrix} \\mathrm{ReLU}(x_1 + b_{d,1}) \\\\ \\mathrm{ReLU}(x_2 + b_{d,2}) \\end{pmatrix}\n    $$\n3.  向量 $x''$ 是逐点阶段的输入（$z=x''$）。最终的标量输出，我们记为 $f_A(x)$，是：\n    $$\n    f_A(x; b_d, w, b_p) = w_1 \\mathrm{ReLU}(x_1 + b_{d,1}) + w_2 \\mathrm{ReLU}(x_2 + b_{d,2}) + b_p\n    $$\n顺序 (A) 代表的函数类别是经过移位和整流的输入的线性组合。这是一个简单的两层神经网络的形式，它有一个包含两个 ReLU 神经元的隐藏层和一个线性输出单元。由 $f_A(x) = 0$ 定义的决策边界是分段线性的，这使其能够逼近非线性和非凸的决策区域。\n\n**3. 顺序 (B)：ReLU 在逐点阶段之后**\n\n在此配置中，深度阶段和逐点阶段首先进行线性组合，然后 ReLU 激活函数仅应用于最终的标量输出。\n\n1.  输入 $x=(x_1, x_2)^T$ 通过深度阶段，得到 $x'=(x_1+b_{d,1}, x_2+b_{d,2})^T$。\n2.  向量 $x'$ 是逐点阶段的输入（$z=x'$）。激活前的输出是：\n    $$\n    y_{\\text{pre}} = w_1(x_1 + b_{d,1}) + w_2(x_2 + b_{d,2}) + b_p\n    $$\n    我们可以将其重排为关于 $x_1$ 和 $x_2$ 的标准线性函数形式：\n    $$\n    y_{\\text{pre}} = w_1 x_1 + w_2 x_2 + (w_1 b_{d,1} + w_2 b_{d,2} + b_p)\n    $$\n3.  ReLU 函数应用于此标量值。最终输出 $f_B(x)$ 是：\n    $$\n    f_B(x; b_d, w, b_p) = \\mathrm{ReLU}(y_{\\text{pre}}) = \\mathrm{ReLU}(w_1 x_1 + w_2 x_2 + (w_1 b_{d,1} + w_2 b_{d,2} + b_p))\n    $$\n分类规则是如果 $f_B(x) \\ge 0$ 则预测为类别 $1$。由于 $\\mathrm{ReLU}(t) \\ge 0$ 当且仅当 $t \\ge 0$，这等价于检查是否 $y_{\\text{pre}} \\ge 0$。因此，决策边界是 $y_{\\text{pre}}=0$，这是 $(x_1, x_2)$ 平面中的一条直线方程。因此，对于任何参数选择，顺序 (B) 都充当一个线性分类器，只能学习超平面决策边界。\n\n### 基准测试任务分析与实现策略\n\n这些基准测试任务旨在探究这两种函数形式不同的表征能力。\n\n*   **用例 1（需要混合的半空间）：** 目标是 $x_1 - x_2 \\ge 0$。这是一个线性决策边界。如前所推导，顺序 (B) 内在地是一个线性分类器，应该能够完美解决此问题。顺序 (A) 也可以表示这个线性函数，例如，通过选择足够大的深度偏置 $b_{d,1}, b_{d,2}$，使得对于网格上的所有输入，ReLU 的参数总是正的。在这种情况下，$\\mathrm{ReLU}(x_i+b_{d,i}) = x_i+b_{d,i}$，模型就变成了线性的。预计两种模型都能达到完美的准确率。\n\n*   **用例 2（正部之和）：** 目标是 $\\mathrm{ReLU}(x_1) + 2\\mathrm{ReLU}(x_2) - 1.0 \\ge 0$。这个目标函数的形式与顺序 (A) 的结构精确匹配。通过选择参数 $b_{d,1}=0, b_{d,2}=0, w_1=1, w_2=2, b_p=-1$（所有这些参数都在指定的搜索网格内），顺序 (A) 可以完美地表示目标函数。相比之下，这个决策边界是非线性的，所以由顺序 (B) 代表的线性分类器将无法达到完美的准确率。\n\n*   **用例 3（非负边界等价性）：** 目标是 $x_1 + 2x_2 - 1.0 \\ge 0$，并且所有输入 $(x_1, x_2)$ 都满足 $x_i \\ge 0$。对于顺序 (A)，如果我们选择 $b_{d,1}=0$ 和 $b_{d,2}=0$，函数变为 $f_A(x) = w_1\\mathrm{ReLU}(x_1) + w_2\\mathrm{ReLU}(x_2) + b_p$。由于 $x_i \\ge 0$，$\\mathrm{ReLU}(x_i) = x_i$，所以函数简化为 $f_A(x) = w_1 x_1 + w_2 x_2 + b_p$。这与顺序 (B) 的激活前函数（在 $b_{d,1}=b_{d,2}=0$ 的情况下）是相同的。由于顺序 (A) 的参数搜索空间包含了使其等价于顺序 (B) 的参数，并且目标是一个线性分隔器，两种模型都有能力完美解决此任务。\n\n实现将通过对六种场景（3 个用例 $\\times$ 2 种顺序）中的每一种，穷举搜索离散参数网格来完成。对于每种参数组合，在相应的数据集上计算模型的准确率。报告每种场景下在所有参数组合中找到的最大准确率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Derives and empirically tests two orderings of nonlinearity placement\n    in a simplified depthwise separable convolution.\n    \"\"\"\n\n    # Define parameter grids\n    b_d_vals = [-1, 0, 1]\n    w_vals = [-2, -1, 0, 1, 2]\n    b_p_vals = [-2, -1, 0, 1, 2]\n    \n    b_d_pairs = list(itertools.product(b_d_vals, repeat=2))\n    w_pairs = list(itertools.product(w_vals, repeat=2))\n\n    # Define input grids for test cases\n    S = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    S_plus = np.array([0.0, 0.5, 1.0])\n\n    inputs_1_2 = np.array(list(itertools.product(S, repeat=2)))\n    inputs_3 = np.array(list(itertools.product(S_plus, repeat=2)))\n    \n    # Define ground truth labels for test cases\n    # Case 1: mixing-needed halfspace\n    x1, x2 = inputs_1_2[:, 0], inputs_1_2[:, 1]\n    y_true_1 = (x1 - x2 = 0).astype(int)\n\n    # Case 2: sum of positive parts\n    y_true_2 = (np.maximum(0, x1) + 2 * np.maximum(0, x2) - 1.0 = 0).astype(int)\n\n    # Case 3: nonnegative boundary equivalence\n    x1_3, x2_3 = inputs_3[:, 0], inputs_3[:, 1]\n    y_true_3 = (x1_3 + 2 * x2_3 - 1.0 = 0).astype(int)\n\n    test_cases = [\n        (inputs_1_2, y_true_1),\n        (inputs_1_2, y_true_2),\n        (inputs_3, y_true_3)\n    ]\n\n    final_results = []\n    \n    # ReLU function\n    def relu(t):\n        return np.maximum(0, t)\n\n    for case_inputs, y_true in test_cases:\n        max_acc_A = 0.0\n        max_acc_B = 0.0\n        \n        num_points = len(case_inputs)\n        x1_data, x2_data = case_inputs[:, 0], case_inputs[:, 1]\n\n        # Exhaustive search over the parameter grid\n        for bd1, bd2 in b_d_pairs:\n            for w1, w2 in w_pairs:\n                for a_bp in b_p_vals: # 'a_bp' to distinguish from the var name in the loop\n                    bp = a_bp\n                    \n                    # Ordering (A): ReLU after depthwise, before pointwise\n                    output_A = w1 * relu(x1_data + bd1) + w2 * relu(x2_data + bd2) + bp\n                    preds_A = (output_A = 0).astype(int)\n                    acc_A = np.sum(preds_A == y_true) / num_points\n                    if acc_A  max_acc_A:\n                        max_acc_A = acc_A\n\n                    # Ordering (B): ReLU after pointwise\n                    # Classification is based on the pre-ReLU value being = 0\n                    output_B_pre = w1 * (x1_data + bd1) + w2 * (x2_data + bd2) + bp\n                    preds_B = (output_B_pre = 0).astype(int)\n                    acc_B = np.sum(preds_B == y_true) / num_points\n                    if acc_B  max_acc_B:\n                        max_acc_B = acc_B\n        \n        final_results.append(max_acc_A)\n        final_results.append(max_acc_B)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3115159"}]}