{"hands_on_practices": [{"introduction": "理论联系实际是掌握任何概念的关键。要真正理解 Squeeze-and-Excitation (SE) 网络的内部工作原理，最好的方法莫过于亲手从零开始构建它。这个练习 [@problem_id:3139403] 将引导你基于其核心数学定义——“挤压”（Squeeze）、“激励”（Excitation）和“缩放”（Scale）——来实现一个完整的 SE 模块，并计算其引入的参数开销。", "problem": "要求您实现一个通道级“压缩-激励”（Squeeze-and-Excitation, SE）模块，并从第一性原理量化其参数开销。SE模块作用于一个具有通道、高度和宽度轴的三维输入张量。您必须仅使用线性映射和逐点非线性的核心定义来实现下述操作。\n\n给定一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，压缩操作定义为通道级全局平均：\n$$\ns_c \\;=\\; \\frac{1}{H\\,W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}, \\quad \\text{for } c \\in \\{0,\\dots,C-1\\}.\n$$\n将 $s_c$ 堆叠起来，得到描述子向量 $s \\in \\mathbb{R}^{C}$。激励操作定义为一个具有压缩率 $r$ 的两层多层感知机（MLP），其中 $r$ 是一个可以整除 $C$ 的正整数。设 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$ 表示修正线性单元，$\\sigma(u)=\\frac{1}{1+e^{-u}}$ 表示 logistic sigmoid 函数。通过学习到的参数 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$、$b_1 \\in \\mathbb{R}^{C/r}$、$W_2 \\in \\mathbb{R}^{C \\times (C/r)}$ 和 $b_2 \\in \\mathbb{R}^{C}$，激励输出 $z \\in \\mathbb{R}^{C}$ 为：\n$$\nt \\;=\\; \\mathrm{ReLU}(W_1 s + b_1), \\qquad z \\;=\\; \\sigma(W_2 t + b_2).\n$$\n最后，通过 $z$ 对输入张量进行通道级缩放：\n$$\ny_{cij} \\;=\\; x_{cij}\\, z_c, \\quad \\text{for all } c,i,j,\n$$\n以获得经SE增强的张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$。\n\n参数开销定义为激励MLP相对于没有SE模块的基线所引入的额外参数数量。对于两个非共享权重的线性层，权重数量为：\n$$\n\\#\\text{weights}_{\\text{untied}} \\;=\\; C \\cdot \\frac{C}{r} \\;+\\; \\frac{C}{r} \\cdot C \\;=\\; \\frac{2C^2}{r},\n$$\n偏置数量为：\n$$\n\\#\\text{biases} \\;=\\; \\frac{C}{r} \\;+\\; C.\n$$\n在一个权重共享的变体中，即 $W_2 = W_1^{\\top}$，独立权重数量减少为：\n$$\n\\#\\text{weights}_{\\text{tied}} \\;=\\; C \\cdot \\frac{C}{r} \\;=\\; \\frac{C^2}{r}.\n$$\n\n请严格按照规定实现SE模块，并为每个测试用例计算以下输出：\n- 压缩描述子之和，$\\sum_{c=0}^{C-1} s_c$。\n- 缩放后输出的所有元素之和，$\\sum_{c,i,j} y_{cij}$，四舍五入到六位小数。\n- 非共享权重的开销 $\\#\\text{weights}_{\\text{untied}}$。\n- 共享权重的开销 $\\#\\text{weights}_{\\text{tied}}$。\n- 非共享权重加偏置的开销 $\\#\\text{weights}_{\\text{untied}} + \\#\\text{biases}$。\n\n您的程序必须使用以下测试套件。在所有情况下，$C$ 都能被 $r$ 整除，并且所有以下定义都必须严格实现。\n\n测试用例 A（正常路径）：\n- $C = 4$, $H = 2$, $W = 2$, $r = 2$。\n- 输入由 $x_{cij} = c - i + j$ 定义，其中 $c \\in \\{0,1,2,3\\}$，$i \\in \\{0,1\\}$，$j \\in \\{0,1\\}$。\n- 参数：\n$$\nW_1 \\;=\\; \\begin{bmatrix}\n1  -1  0.5  0 \\\\\n0.25  0.5  -0.5  1\n\\end{bmatrix}, \\quad\nb_1 \\;=\\; \\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix},\n$$\n$$\nW_2 \\;=\\; \\begin{bmatrix}\n1  0.5 \\\\\n-0.5  0.25 \\\\\n0  1 \\\\\n0.75  -1\n\\end{bmatrix}, \\quad\nb_2 \\;=\\; \\begin{bmatrix} 0 \\\\ 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}.\n$$\n\n测试用例 B（空间维度的边界情况，$H W = 1$）：\n- $C = 8$, $H = 1$, $W = 1$, $r = 2$。\n- 输入由 $x_{c00} = c + 1$ 定义，其中 $c \\in \\{0,1,\\dots,7\\}$。\n- 参数由基于索引的公式逐元素定义。设 $a$ 索引行，$b$ 索引列，均为零基索引：\n$$\nW_1[a,b] \\;=\\; \\frac{a\\cdot 8 + b - 12}{16}, \\quad \\text{for } a \\in \\{0,1,2,3\\}, \\; b \\in \\{0,1,\\dots,7\\},\n$$\n$$\nb_1 \\;=\\; \\begin{bmatrix} -0.25 \\\\ 0 \\\\ 0.1 \\\\ -0.1 \\end{bmatrix}, \\quad\nW_2[a,b] \\;=\\; \\frac{a\\cdot 4 + b - 8}{8}, \\quad \\text{for } a \\in \\{0,1,\\dots,7\\}, \\; b \\in \\{0,1,2,3\\},\n$$\n$$\nb_2 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{8}.\n$$\n\n测试用例 C（边缘情况 $r = 1$，带有符号交替的通道）：\n- $C = 6$, $H = 3$, $W = 1$, $r = 1$。\n- 输入由 $x_{ci0} = (-1)^c \\cdot (i+1)$ 定义，其中 $c \\in \\{0,1,\\dots,5\\}$ 和 $i \\in \\{0,1,2\\}$。\n- 参数：\n$$\nW_1 \\;=\\; 0.5\\, I_6 \\;-\\; 0.25\\,(\\mathbf{1}_6 \\mathbf{1}_6^{\\top} - I_6), \\quad b_1 \\;=\\; \\mathbf{0} \\in \\mathbb{R}^{6},\n$$\n$$\nW_2 \\;=\\; 0.3\\, I_6 \\;-\\; 0.05\\, \\mathbf{1}_6 \\mathbf{1}_6^{\\top}, \\quad b_2 \\;=\\; \\begin{bmatrix} -0.1 \\\\ -0.05 \\\\ 0 \\\\ 0.05 \\\\ 0.1 \\\\ 0.15 \\end{bmatrix},\n$$\n其中 $I_6$ 是 $6 \\times 6$ 的单位矩阵，$\\mathbf{1}_6$ 是长度为6的全1向量。\n\n角度单位和物理单位在此不适用。所有数值输出都应为标准十进制形式的实数。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含所有三个测试用例的结果，形式为列表的列表。每个内部列表必须按以下顺序排列：\n$$\n\\left[ \\sum_{c} s_c,\\; \\mathrm{round}\\!\\left(\\sum_{c,i,j} y_{cij},\\,6\\right),\\; \\#\\text{weights}_{\\text{untied}},\\; \\#\\text{weights}_{\\text{tied}},\\; \\#\\text{weights}_{\\text{untied}} + \\#\\text{biases} \\right].\n$$\n例如，打印的结构必须如下所示：\n$$\n\\big[\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,\\big].\n$$", "solution": "该问题要求根据Squeeze-and-Excitation (SE) 模块的基本数学定义来实现和分析它，这是一个深度学习中常见的架构组件。解决方案包括对三个不同的测试用例，逐步执行所定义的操作。其核心原理是通过数据驱动的机制进行通道级特征的重新校准。\n\nSE模块作用于一个输入张量 $x \\in \\mathbb{R}^{C \\times H \\times W}$，其中 $C$ 是通道数，$H$ 和 $W$ 分别是空间高度和宽度。该过程包括三个主要阶段：挤压（Squeeze）、激励（Excitation）和缩放（Scale）。\n\n**1. 挤压：全局信息嵌入**\n第一步，“挤压”，将其特征图在空间维度（$H \\times W$）上进行聚合，以产生一个通道描述子。这是通过全局平均池化实现的，它计算每个通道的平均值。得到的向量 $s \\in \\mathbb{R}^{C}$ 为每个通道嵌入了全局感受野。$s$ 的第 $c$ 个元素的公式为：\n$$\ns_c = \\frac{1}{H \\cdot W}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x_{cij}\n$$\n在实现中，这对应于沿着输入张量 $x$ 的空间轴（轴1和轴2）取平均值。每个测试用例的第一个要求输出是这个挤压向量的元素之和，$\\sum_{c=0}^{C-1} s_c$。\n\n**2. 激励：自适应重新校准**\n“激励”阶段从挤压描述子 $s$ 中生成一组逐通道的调制权重，或称“激活值”。这是通过一个小型神经网络，具体来说是一个两层的多层感知机（MLP）来完成的。MLP的结构设计为先降低通道维度然后再恢复它，从而创建一个计算瓶颈，有助于捕捉非线性的通道间相互依赖关系。\n\nMLP包括：\n- 一个降维线性层，其权重为 $W_1 \\in \\mathbb{R}^{(C/r)\\times C}$，偏置为 $b_1 \\in \\mathbb{R}^{C/r}$，其中 $r$ 是压缩率。\n- 一个修正线性单元（$\\mathrm{ReLU}$）激活函数，定义为 $\\mathrm{ReLU}(u) = \\max\\{0,u\\}$。\n- 一个升维线性层，其权重为 $W_2 \\in \\mathbb{R}^{C \\times (C/r)}$，偏置为 $b_2 \\in \\mathbb{R}^{C}$。\n- 一个 logistic sigmoid 激活函数，$\\sigma(u)=\\frac{1}{1+e^{-u}}$，它将输出归一化到 $(0, 1)$ 范围内。\n\n计算过程如下：\n$$\nt = \\mathrm{ReLU}(W_1 s + b_1)\n$$\n$$\nz = \\sigma(W_2 t + b_2)\n$$\n得到的向量 $z \\in \\mathbb{R}^{C}$ 包含了通道级的缩放因子。\n\n**3. 缩放：特征重新校准**\n最后阶段将激励步骤中学习到的缩放因子应用于原始输入张量 $x$。输入张量的每个通道都乘以其来自向量 $z$ 的相应缩放因子。这重新校准了特征图，放大了信息丰富的通道，并抑制了不太有用的通道。输出张量 $y \\in \\mathbb{R}^{C \\times H \\times W}$ 由下式给出：\n$$\ny_{cij} = x_{cij} \\cdot z_c\n$$\n在实现中，这是通过将缩放向量 $z$（重塑为 $C \\times 1 \\times 1$ 维度）广播到输入张量 $x$ 上来实现的。第二个要求的输出是最终缩放张量中所有元素的总和，$\\sum_{c,i,j} y_{cij}$，其计算方式为 $\\sum_c (z_c \\cdot \\sum_{i,j} x_{cij}) = H \\cdot W \\cdot \\sum_c (z_c \\cdot s_c)$。\n\n**4. 参数开销计算**\n问题还要求量化SE模块的MLP引入的额外可学习参数的数量。这些参数包括两个线性层的权重和偏置。\n- **非共享权重：** 当 $W_1$ 和 $W_2$ 相互独立时，总权重数量是两个矩阵中元素的总和：$(\\frac{C}{r} \\times C) + (C \\times \\frac{C}{r}) = \\frac{2C^2}{r}$。\n- **共享权重：** 在 $W_2 = W_1^{\\top}$ 的变体中，独立权重的数量减少为 $W_1$ 的大小，即 $\\frac{C^2}{r}$。\n- **偏置：** 偏置参数的数量是 $b_1$ 和 $b_2$ 中元素的总和，即 $\\frac{C}{r} + C$。\n- **总非共享开销：** 非共享权重情况下的总参数数量是未绑定权重和偏置的总和：$\\frac{2C^2}{r} + \\frac{C}{r} + C$。\n\n实现将针对提供的三个测试用例，使用指定的输入张量和MLP参数，计算这五个量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format results for all test cases.\n    \"\"\"\n\n    def se_block_and_overhead(C, H, W, r, x, W1, b1, W2, b2):\n        \"\"\"\n        Implements the SE block operations and calculates parameter overhead.\n\n        Args:\n            C (int): Number of channels.\n            H (int): Height of the input tensor.\n            W (int): Width of the input tensor.\n            r (int): Reduction ratio.\n            x (np.ndarray): Input tensor of shape (C, H, W).\n            W1 (np.ndarray): Weights of the first linear layer.\n            b1 (np.ndarray): Biases of the first linear layer.\n            W2 (np.ndarray): Weights of the second linear layer.\n            b2 (np.ndarray): Biases of the second linear layer.\n\n        Returns:\n            list: A list containing the five required output values.\n        \"\"\"\n        # 1. Squeeze Operation: Global Average Pooling\n        # s_c = (1/(H*W)) * sum(x_cij for i,j)\n        s = np.mean(x, axis=(1, 2))\n        sum_s = np.sum(s)\n\n        # 2. Excitation Operation: MLP\n        # t = ReLU(W1*s + b1)\n        t = np.maximum(0, W1 @ s + b1)\n        \n        # z = sigmoid(W2*t + b2)\n        z_raw = W2 @ t + b2\n        z = 1 / (1 + np.exp(-z_raw))\n\n        # 3. Scale Operation\n        # y_cij = x_cij * z_c\n        # Reshape z to (C, 1, 1) for broadcasting\n        y = x * z.reshape((C, 1, 1))\n        sum_y = np.sum(y)\n\n        # 4. Parameter Overhead Calculation\n        weights_untied = (2 * C**2) // r\n        weights_tied = (C**2) // r\n        biases = (C // r) + C\n        total_untied_with_biases = weights_untied + biases\n        \n        return [\n            float(sum_s),\n            round(sum_y, 6),\n            int(weights_untied),\n            int(weights_tied),\n            int(total_untied_with_biases)\n        ]\n\n    results = []\n\n    # --- Test Case A ---\n    C_A, H_A, W_A, r_A = 4, 2, 2, 2\n    x_A = np.fromfunction(lambda c, i, j: c - i + j, (C_A, H_A, W_A), dtype=int)\n    W1_A = np.array([\n        [1, -1, 0.5, 0],\n        [0.25, 0.5, -0.5, 1]\n    ])\n    b1_A = np.array([-0.5, 0])\n    W2_A = np.array([\n        [1, 0.5],\n        [-0.5, 0.25],\n        [0, 1],\n        [0.75, -1]\n    ])\n    b2_A = np.array([0, 0.1, -0.2, 0.3])\n    results.append(se_block_and_overhead(C_A, H_A, W_A, r_A, x_A, W1_A, b1_A, W2_A, b2_A))\n\n    # --- Test Case B ---\n    C_B, H_B, W_B, r_B = 8, 1, 1, 2\n    x_B = (np.arange(C_B) + 1).reshape(C_B, H_B, W_B)\n    W1_B = np.fromfunction(lambda a, b: (a * 8 + b - 12) / 16, (C_B // r_B, C_B))\n    b1_B = np.array([-0.25, 0, 0.1, -0.1])\n    W2_B = np.fromfunction(lambda a, b: (a * 4 + b - 8) / 8, (C_B, C_B // r_B))\n    b2_B = np.zeros(C_B)\n    results.append(se_block_and_overhead(C_B, H_B, W_B, r_B, x_B, W1_B, b1_B, W2_B, b2_B))\n\n    # --- Test Case C ---\n    C_C, H_C, W_C, r_C = 6, 3, 1, 1\n    c_vec_C = (-1)**np.arange(C_C)\n    i_vec_C = np.arange(1, H_C + 1)\n    x_C = (c_vec_C[:, np.newaxis, np.newaxis]\n           * i_vec_C[np.newaxis, :, np.newaxis])\n    \n    I6 = np.identity(C_C)\n    J6 = np.ones((C_C, C_C))\n    \n    W1_C = 0.5 * I6 - 0.25 * (J6 - I6)\n    b1_C = np.zeros(C_C)\n    W2_C = 0.3 * I6 - 0.05 * J6\n    b2_C = np.array([-0.1, -0.05, 0, 0.05, 0.1, 0.15])\n    results.append(se_block_and_overhead(C_C, H_C, W_C, r_C, x_C, W1_C, b1_C, W2_C, b2_C))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3139403"}, {"introduction": "理解了 SE 模块的构造之后，我们自然会问：它究竟是如何提升网络性能的？此练习 [@problem_id:3175797] 通过设计一个包含“信号”通道和“对抗性噪声”通道的合成数据集，让你亲眼见证 SE 模块的核心功能。你将训练一个带有谱范数正则化的 SE 模块，并观察它是否能学会有选择地抑制那些无关紧要的噪声通道，从而将注意力集中在有用的信号上。", "problem": "要求您在对抗性构建的杂波通道上，实现并研究一个在谱范数正则化下的简化版压缩-激励（Squeeze-and-Excitation, SE）机制。整个工作应以向量形式处理逐通道特征，并假设进行二元分类。\n\n使用的基本原理：\n- 采用交叉熵损失的二元逻辑斯谛模型：对于一个 logit $a$，预测值 $\\hat{y} = \\sigma(a)$，其中 $\\sigma(\\cdot)$ 是逻辑斯谛S型函数，损失 $L = -y \\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})$。损失对 logit 的导数为 $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$。\n- 通过一个带有修正线性单元（ReLU）和 sigmoid 激活函数的两层瓶颈结构来计算逐通道的 SE 门控。\n- 矩阵 $W$ 的谱范数定义为 $\\|W\\|_{2} = \\max_{\\|v\\|=1} \\|W v\\|$，其值等于最大的奇异值。最大奇异值相对于 $W$ 的梯度由顶层左奇异向量和右奇异向量的外积给出。\n\n设置：\n- 共有 $C$ 个通道，其中 $C = 8$。一个输入样本是一个向量 $x \\in \\mathbb{R}^{C}$，表示通道的激活值（空间维度被压缩，因此“压缩”操作是恒等变换）。\n- SE 模块使用两个学习到的矩阵 $W_1 \\in \\mathbb{R}^{C/r \\times C}$ 和 $W_2 \\in \\mathbb{R}^{C \\times C/r}$ 从 $x$ 计算出门控向量 $g \\in \\mathbb{R}^{C}$，缩减率 $r \\in \\{2,4\\}$。计算 $z_1 = W_1 x$，$a_1 = \\max(0, z_1)$（逐元素），$z_2 = W_2 a_1$，以及 $g = \\sigma(z_2)$（逐元素）。\n- 调制后的特征为 $x^{\\mathrm{tilde}} = g \\odot x$，其中 $\\odot$ 表示逐元素乘积。分类器是固定的，作为一个均匀聚合器 $a = \\sum_{i=1}^{C} x^{\\mathrm{tilde}}_i$，且 $\\hat{y} = \\sigma(a)$。\n\n训练目标：\n- 最小化数据集上的平均交叉熵，外加对 $W_1$ 和 $W_2$ 的谱范数正则化惩罚项。总损失为\n$$\n\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} \\left( -y_n \\log(\\hat{y}_n) - (1 - y_n)\\log(1 - \\hat{y}_n) \\right) + \\lambda\\left(\\|W_1\\|_2 + \\|W_2\\|_2\\right),\n$$\n其中 $N$ 是样本数量，$y_n \\in \\{0,1\\}$，$\\lambda > 0$ 是谱正则化强度。\n\n含对抗性杂波的数据集设计：\n- 设信号通道为索引 $\\{0,1\\}$，杂波通道为索引 $\\{2,3,4,5,6,7\\}$。对于每个样本 $n$：\n  1. 均匀抽取标签 $y_n \\in \\{0,1\\}$。\n  2. 对于 $i \\in \\{0,1\\}$，将信号通道设置为 $x_{n,i} = s \\cdot \\mu + \\varepsilon_{n,i}$，其中如果 $y_n = 1$ 则 $s = +1$，如果 $y_n = 0$ 则 $s = -1$；$\\mu \\ge 0$ 是信号幅度，$\\varepsilon_{n,i} \\sim \\mathcal{N}(0, \\sigma^2)$ 是标准差为 $\\sigma$ 的高斯噪声。\n  3. 计算 $S_n = \\sum_{i \\in \\{0,1\\}} x_{n,i}$ 并定义一个对抗性符号 $c_n = -\\operatorname{sign}(S_n)$（如果 $S_n \\neq 0$）；如果 $\\mu = 0$ 或 $S_n = 0$，则将 $c_n$ 设置为与 $y_n$ 无关的随机 $\\pm 1$。\n  4. 对于每个杂波通道 $j \\in \\{2,3,4,5,6,7\\}$，设置 $x_{n,j} = \\gamma \\cdot c_n \\cdot (\\mu + |\\eta_{n,j}|)$，其中 $\\eta_{n,j} \\sim \\mathcal{N}(0, \\sigma^2)$，$\\gamma > 0$ 控制杂波幅度。这样构建的杂波在符号上与信号总和相反，且具有较大的幅度。\n\n训练方法：\n- 仅对 $W_1$ 和 $W_2$ 使用批量梯度下降法；分类器按前述方式固定。通过链式法则推导梯度：从交叉熵导数 $\\frac{\\partial L}{\\partial a} = \\hat{y} - y$ 开始，依次通过 $x^{\\mathrm{tilde}} = g \\odot x$，$g = \\sigma(z_2)$，$z_2 = W_2 a_1$，$a_1 = \\max(0, z_1)$ 和 $z_1 = W_1 x$ 进行反向传播。对于谱惩罚项，通过幂迭代法近似计算顶层奇异向量，并将 $\\lambda \\, u_1 v_1^{\\top}$ 加到 $\\frac{\\partial \\mathcal{L}}{\\partial W_1}$ 上，将 $\\lambda \\, u_2 v_2^{\\top}$ 加到 $\\frac{\\partial \\mathcal{L}}{\\partial W_2}$ 上，其中 $u_k, v_k$ 是 $W_k$ 的顶层左、右奇异向量。\n- 用小的随机值初始化 $W_1$ 和 $W_2$。使用恒定的学习率和固定数量的训练轮次。\n\n评估与决策规则：\n- 训练后，计算每个通道的平均门控值 $\\bar{g}_i = \\frac{1}{N} \\sum_{n=1}^{N} g_{n,i}$。令 $\\bar{g}_{\\mathrm{sig}}$ 为信号通道上 $\\bar{g}_i$ 的均值，$\\bar{g}_{\\mathrm{clutter}}$ 为杂波通道上 $\\bar{g}_i$ 的均值。\n- 如果 $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}} > \\delta$（对于容差 $\\delta = 0.1$，无量纲门控值），则声明杂波已被抑制。\n\n不涉及角度和物理单位；所有量均为无量纲量。\n\n测试套件：\n在以下参数集上运行您的程序，每个参数集根据上述决策规则产生一个布尔值：\n- 案例 1：$C = 8$, $r = 2$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.3$, $\\gamma = 3.0$, $\\lambda = 0.1$, 随机种子 $0$。\n- 案例 2：$C = 8$, $r = 2$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.3$, $\\gamma = 10.0$, $\\lambda = 0.2$, 随机种子 $1$。\n- 案例 3：$C = 8$, $r = 4$, $N = 240$, $\\mu = 1.0$, $\\sigma = 0.5$, $\\gamma = 15.0$, $\\lambda = 0.5$, 随机种子 $2$。\n- 案例 4（边界情况）：$C = 8$, $r = 2$, $N = 240$, $\\mu = 0.0$, $\\sigma = 0.5$, $\\gamma = 10.0$, $\\lambda = 0.2$, 随机种子 $3$。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，`[True,True,True,False]`），其中每个结果是对应测试案例的布尔值，指示根据规则 $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}} > \\delta$，SE 模块在谱范数正则化下是否抑制了杂波。", "solution": "该问题要求实现一个特定的机器学习模型和训练过程，以研究 Squeeze-and-Excitation (SE) 模块在对抗性条件和谱范数正则化下的行为。所有提供的术语、方程和过程都与该领域的既定原则一致。解决方案首先按描述通过算法生成数据集，然后构建指定的两层 SE 网络，最后使用批量梯度下降法进行训练。推导并实现了交叉熵损失和谱范数惩罚项的梯度。谱范数梯度按照规定使用幂迭代法进行近似。训练后，使用指定的度量标准对模型进行评估，以确定它是否成功学会抑制对抗性杂波。\n\n### 1. 模型与前向传播\n\n该模型由一个 SE 模块和一个固定的分类器组成。设一个输入样本为向量 $x \\in \\mathbb{R}^{C}$，其中 $C=8$。\n\n1.  **压缩-激励（Squeeze-and-Excitation）模块**：输入 $x$ 通过一个两层全连接网络来计算门控向量 $g$。\n    - 第一层使用权重矩阵 $W_1 \\in \\mathbb{R}^{C/r \\times C}$，以比例 $r$ 降低通道维度。\n      $$ z_1 = W_1 x $$\n    - 逐元素应用 ReLU 激活函数。\n      $$ a_1 = \\text{ReLU}(z_1) = \\max(0, z_1) $$\n    - 第二层使用权重矩阵 $W_2 \\in \\mathbb{R}^{C \\times C/r}$，将向量投影回原始通道维度。\n      $$ z_2 = W_2 a_1 $$\n    - 逐元素应用 sigmoid 激活函数 $\\sigma(u) = 1/(1+e^{-u})$，生成最终的门控向量 $g \\in \\mathbb{R}^{C}$，其中每个元素 $g_i \\in (0, 1)$。\n      $$ g = \\sigma(z_2) $$\n2.  **调制**：原始输入 $x$ 通过与门控向量 $g$ 进行逐元素乘积（$\\odot$）而被重新缩放。\n    $$ x^{\\mathrm{tilde}} = g \\odot x $$\n3.  **分类器**：logit $a$ 由一个固定的、均匀的聚合层计算得出，该层对调制后的特征向量的各分量求和。\n    $$ a = \\sum_{i=1}^{C} x^{\\mathrm{tilde}}_i $$\n    最终预测值 $\\hat{y}$ 通过应用 sigmoid 函数获得。\n    $$ \\hat{y} = \\sigma(a) $$\n\n### 2. 损失函数与梯度推导\n\n总损失 $\\mathcal{L}$ 是一个包含 $N$ 个样本的数据集上的平均二元交叉熵损失与 $W_1$ 和 $W_2$ 的谱范数正则化项之和。\n\n$$ \\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^{N} L_n + \\lambda \\left( \\|W_1\\|_2 + \\|W_2\\|_2 \\right) $$\n其中 $L_n = -y_n \\log(\\hat{y}_n) - (1-y_n)\\log(1-\\hat{y}_n)$，$\\lambda$ 是正则化强度。\n\n梯度通过链式法则（反向传播）计算。对于单个样本 $(x, y)$：\n\n-   **关于 Logit ($a$) 的梯度**：交叉熵损失对其输入的导数已给出。\n    $$ \\frac{\\partial L}{\\partial a} = \\hat{y} - y $$\n-   **关于调制后特征 ($x^{\\mathrm{tilde}}$) 的梯度**：由于 $a = \\mathbf{1}^\\top x^{\\mathrm{tilde}}$，梯度在所有通道上是均匀的。\n    $$ \\frac{\\partial L}{\\partial x^{\\mathrm{tilde}}_i} = \\frac{\\partial L}{\\partial a} \\frac{\\partial a}{\\partial x^{\\mathrm{tilde}}_i} = (\\hat{y} - y) \\cdot 1 $$\n    $$ \\nabla_{x^{\\mathrm{tilde}}} L = (\\hat{y} - y) \\mathbf{1} $$\n-   **关于门控值 ($g$) 的梯度**：在 $x^{\\mathrm{tilde}}_i = g_i x_i$ 上使用链式法则。\n    $$ \\frac{\\partial L}{\\partial g_i} = \\frac{\\partial L}{\\partial x^{\\mathrm{tilde}}_i} x_i = (\\hat{y} - y) x_i \\implies \\nabla_g L = (\\hat{y} - y) x $$\n-   **关于 $z_2$ 的梯度**：在 $g_i = \\sigma(z_{2,i})$ 上使用链式法则，其中 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$。\n    $$ \\frac{\\partial L}{\\partial z_{2,i}} = \\frac{\\partial L}{\\partial g_i} g_i (1-g_i) \\implies \\nabla_{z_2} L = \\nabla_g L \\odot g \\odot (1-g) $$\n-   **关于 $W_2$ 和 $a_1$ 的梯度**：在 $z_2 = W_2 a_1$ 上使用链式法则。\n    $$ \\frac{\\partial L}{\\partial W_2} = (\\nabla_{z_2} L) a_1^\\top \\quad \\text{(外积)} $$\n    $$ \\nabla_{a_1} L = W_2^\\top (\\nabla_{z_2} L) $$\n-   **关于 $z_1$ 的梯度**：在 $a_1 = \\text{ReLU}(z_1)$ 上使用链式法则，其中 ReLU 的导数对于正输入为 $1$，否则为 $0$。令 $\\mathbb{I}(z_1 > 0)$ 为逐元素指示函数。\n    $$ \\nabla_{z_1} L = (\\nabla_{a_1} L) \\odot \\mathbb{I}(z_1 > 0) $$\n-   **关于 $W_1$ 的梯度**：在 $z_1 = W_1 x$ 上使用链式法则。\n    $$ \\frac{\\partial L}{\\partial W_1} = (\\nabla_{z_1} L) x^\\top \\quad \\text{(外积)} $$\n\n交叉熵项的总梯度是批次中这些样本梯度的平均值。\n\n### 3. 谱范数正则化梯度\n\n谱范数 $\\|W\\|_2$ 是 $W$ 的最大奇异值。其梯度由 $\\frac{\\partial \\|W\\|_2}{\\partial W} = u v^\\top$ 给出，其中 $u$ 和 $v$ 是对应于最大奇异值的左、右奇异向量。按照规定，这些向量使用幂迭代法进行近似。对于一个矩阵 $W$，从一个随机向量 $v_0$ 开始：\n$$ v_{k+1} = \\frac{W^\\top (W v_k)}{\\|W^\\top (W v_k)\\|_2} $$\n向量收敛于最大右奇异向量 $v$。然后可以找到最大左奇异向量为 $u = \\frac{Wv}{\\|Wv\\|_2}$。梯度的正则化分量是：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_1} \\mathrel{+}= \\lambda u_1 v_1^\\top \\quad \\text{和} \\quad \\frac{\\partial \\mathcal{L}}{\\partial W_2} \\mathrel{+}= \\lambda u_2 v_2^\\top $$\n\n### 4. 训练与评估\n\n使用批量梯度下降来更新权重 $W_1$ 和 $W_2$。固定的超参数选择如下：学习率 $\\eta = 0.01$，训练轮次数 $= 100$，幂迭代次数 $= 20$。\n$$ W_k \\leftarrow W_k - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W_k} \\quad \\text{for } k \\in \\{1, 2\\} $$\n训练结束后，计算整个数据集上每个通道的平均门控值：\n$$ \\bar{g}_i = \\frac{1}{N} \\sum_{n=1}^{N} g_{n,i} $$\n对信号通道（$\\bar{g}_{\\mathrm{sig}}$，对于 $i \\in \\{0, 1\\}$）和杂波通道（$\\bar{g}_{\\mathrm{clutter}}$，对于 $i \\in \\{2, \\ldots, 7\\}$）的平均门控值进行比较。如果 $\\bar{g}_{\\mathrm{sig}} - \\bar{g}_{\\mathrm{clutter}} > \\delta$，则认为杂波已被抑制，容差 $\\delta = 0.1$。\n\n### 5. 实现策略\n\n该解决方案使用 Python 的 `numpy` 库实现。\n1.  **数据生成**：一个函数根据所提供的对抗性设计生成数据集 $(X, y)$，使用 `numpy.random.Generator` 以确保可复现性。\n2.  **训练**：为了效率，使用了前向和后向传播的向量化实现。这样可以同时计算整个 $N$ 个样本批次的梯度。幂迭代法在一个辅助函数中实现，用于找到正则化梯度的顶层奇异向量。\n3.  **主循环**：一个主函数遍历所有测试案例，为每次运行设置参数。它调用训练和评估逻辑，并根据决策规则存储布尔结果。最终的布尔值列表以要求的格式打印出来。", "answer": "```python\nimport numpy as np\n\ndef sigmoid(z):\n    \"\"\"Computes the sigmoid function element-wise.\"\"\"\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef relu(z):\n    \"\"\"Computes the Rectified Linear Unit function element-wise.\"\"\"\n    return np.maximum(0, z)\n\ndef power_iteration(W, rng, num_iterations=20):\n    \"\"\"\n    Approximates top left and right singular vectors of a matrix W via power iteration.\n    \n    Args:\n        W (np.ndarray): The matrix.\n        rng (np.random.Generator): Random number generator for initialization.\n        num_iterations (int): Number of iterations to run.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The top left singular vector u and top right singular vector v.\n    \"\"\"\n    m, n = W.shape\n    v = rng.normal(size=n)\n    if np.linalg.norm(v) == 0: v = np.ones(n) # Handle zero vector case\n    v = v / np.linalg.norm(v)\n\n    for _ in range(num_iterations):\n        u_un = W @ v\n        norm_u = np.linalg.norm(u_un)\n        if norm_u == 0: break # Matrix is likely zero\n        u = u_un / norm_u\n        \n        v_un = W.T @ u\n        norm_v = np.linalg.norm(v_un)\n        if norm_v == 0: break # Matrix is likely zero\n        v = v_un / norm_v\n    \n    u = W @ v\n    norm_u_final = np.linalg.norm(u)\n    if norm_u_final > 0:\n        u = u / norm_u_final\n    \n    return u, v\n\ndef generate_data(N, C, mu, sigma, gamma, rng):\n    \"\"\"\n    Generates the dataset with adversarial clutter.\n    \"\"\"\n    X = np.zeros((N, C))\n    y = rng.integers(0, 2, size=N)\n\n    for n in range(N):\n        s = 1.0 if y[n] == 1 else -1.0\n        \n        # Signal channels\n        eps = rng.normal(0, sigma, size=2)\n        X[n, 0:2] = s * mu + eps\n        \n        S_n = np.sum(X[n, 0:2])\n        \n        # Adversarial sign\n        if mu == 0 or S_n == 0:\n            c_n = rng.choice([-1.0, 1.0])\n        else:\n            c_n = -np.sign(S_n)\n            \n        # Clutter channels\n        eta = rng.normal(0, sigma, size=C - 2)\n        X[n, 2:] = gamma * c_n * (mu + np.abs(eta))\n        \n    return X, y\n\ndef run_case(C, r, N, mu, sigma, gamma, lambda_reg, seed):\n    \"\"\"\n    Runs a single test case from the problem statement.\n    \"\"\"\n    # Hyperparameters\n    learning_rate = 0.01\n    num_epochs = 100\n    delta = 0.1\n    init_weight_scale = 0.1\n\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate Data\n    X, y = generate_data(N, C, mu, sigma, gamma, rng)\n\n    # 2. Initialize Weights\n    hidden_dim = C // r\n    W1 = rng.uniform(-init_weight_scale, init_weight_scale, size=(hidden_dim, C))\n    W2 = rng.uniform(-init_weight_scale, init_weight_scale, size=(C, hidden_dim))\n\n    # 3. Training Loop (Batch Gradient Descent)\n    y_reshaped = y.reshape(-1, 1)\n\n    for epoch in range(num_epochs):\n        # -- Forward Pass --\n        z1 = X @ W1.T\n        a1 = relu(z1)\n        z2 = a1 @ W2.T\n        g = sigmoid(z2)\n        x_tilde = g * X\n        a = np.sum(x_tilde, axis=1, keepdims=True)\n        y_hat = sigmoid(a)\n        \n        # -- Backward Pass --\n        # Gradient of cross-entropy loss\n        dL_da = (y_hat - y_reshaped) / N\n        dL_dx_tilde = dL_da\n        dL_dg = dL_dx_tilde * X\n        dL_dz2 = dL_dg * (g * (1 - g))\n        \n        grad_L_W2 = dL_dz2.T @ a1\n        \n        dL_da1 = dL_dz2 @ W2\n        d_relu_dz1 = (z1 > 0).astype(float)\n        dL_dz1 = dL_da1 * d_relu_dz1\n        \n        grad_L_W1 = dL_dz1.T @ X\n        \n        # Gradient of spectral norm regularization\n        u1, v1 = power_iteration(W1, rng)\n        grad_reg_W1 = lambda_reg * np.outer(u1, v1)\n        \n        u2, v2 = power_iteration(W2, rng)\n        grad_reg_W2 = lambda_reg * np.outer(u2, v2)\n\n        # Total gradient\n        grad_W1 = grad_L_W1 + grad_reg_W1\n        grad_W2 = grad_L_W2 + grad_reg_W2\n        \n        # -- Weight Update --\n        W1 -= learning_rate * grad_W1\n        W2 -= learning_rate * grad_W2\n\n    # 4. Evaluation\n    z1_final = X @ W1.T\n    a1_final = relu(z1_final)\n    z2_final = a1_final @ W2.T\n    g_final = sigmoid(z2_final)\n\n    g_bar = np.mean(g_final, axis=0)\n    g_bar_sig = np.mean(g_bar[0:2])\n    g_bar_clutter = np.mean(g_bar[2:])\n    \n    return g_bar_sig - g_bar_clutter > delta\n\ndef solve():\n    \"\"\"\n    Main function to define and run all test cases.\n    \"\"\"\n    test_cases = [\n        # C, r, N, mu, sigma, gamma, lambda, seed\n        (8, 2, 240, 1.0, 0.3, 3.0, 0.1, 0),\n        (8, 2, 240, 1.0, 0.3, 10.0, 0.2, 1),\n        (8, 4, 240, 1.0, 0.5, 15.0, 0.5, 2),\n        (8, 2, 240, 0.0, 0.5, 10.0, 0.2, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        C, r, N, mu, sigma, gamma, lambda_reg, seed = case\n        result = run_case(C, r, N, mu, sigma, gamma, lambda_reg, seed)\n        results.append(str(result).lower()) # Using .lower() for \"true\"/\"false\"\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3175797"}, {"introduction": "在设计 SE 模块时，一个关键的超参数是缩减率 $r$。这个参数直接控制了模块的容量和复杂性，决定了模型在近似能力和泛化能力之间的权衡。此练习 [@problem_id:3175715] 提供了一种基于结构风险最小化原理的方法，让你能够以一种有理论依据的方式来选择最优的 $r$ 值，而不是仅仅依赖于试错。", "problem": "您将实现一个程序，用于评估 Squeeze-and-Excitation (SE) 网络中缩减率对期望风险简化代理指标的影响，并为多个测试用例输出最优缩减率。您的分析必须严格遵循基于原则的推导，该推导植根于经验风险最小化 (ERM)、通过伪维度进行的复杂度控制以及线性代数中最佳低秩近似的核心定义。\n\n背景与定义：\n- 在一个具有 $C$ 个通道的 Squeeze-and-Excitation (SE) 模块中，“挤压”（squeeze）操作通过全局平均池化计算出一个通道描述符 $z \\in \\mathbb{R}^{C}$，“激励”（excitation）操作则应用一个带有瓶颈结构的双层多层感知机 (MLP)，其缩减率为 $r \\in \\mathbb{N}$，该 MLP 将 $z$ 映射到一个门控向量 $g(z) \\in (0,1)^{C}$，用于缩放各个通道。当 $r$ 能整除 $C$ 时，瓶颈维度为 $m(r) = C / r$。\n- 激励 MLP 中的总参数数量（包括权重和偏置）被建模为\n$$\nP(r) = \\frac{2 C^{2}}{r} + \\frac{C}{r} + C,\n$$\n因为两个权重矩阵的形状分别为 $\\left(\\frac{C}{r} \\times C\\right)$ 和 $\\left(C \\times \\frac{C}{r}\\right)$，并且存在大小为 $\\frac{C}{r}$ 和 $C$ 的偏置。\n- 我们通过线性化激励映射来研究门控映射的一个回归代理。设真实线性算子为 $A \\in \\mathbb{R}^{C \\times C}$，并假设输入是各向同性的 $z \\sim \\mathcal{N}(0, I_{C})$。训练使用带平方损失的经验风险最小化。模型类别受到瓶颈的限制，只能实现秩至多为 $m(r)$ 的矩阵，因为在线性情况下，可以将激励建模为两个具有中间维度 $m(r)$ 的线性层的乘积。\n- 在期望平方误差下，使用秩为 $m(r)$ 的矩阵对 $A$ 进行近似所能达到的最佳近似误差，等于最佳秩-$m(r)$ 近似的平方弗罗贝尼乌斯范数残差：\n$$\n\\mathcal{E}_{\\text{approx}}(r; A) = \\sum_{i=m(r)+1}^{C} \\sigma_{i}(A)^{2},\n$$\n其中 $\\{\\sigma_{i}(A)\\}_{i=1}^{C}$ 是 $A$ 的按非增序排列的奇异值。这可由 Eckart–Young–Mirsky 定理以及当 $z \\sim \\mathcal{N}(0, I_{C})$ 时 $\\mathbb{E}\\left[\\lVert (A - \\hat{A}) z \\rVert_{2}^{2}\\right] = \\lVert A - \\hat{A} \\rVert_{F}^{2}$ 这一事实推导得出。\n- 为控制过拟合，我们采用一种基于统计学习理论的经典结构风险视角。对于具有分段线性或 sigmoid 激活函数的实值神经网络，其伪维度的数量级与 $P \\log P$ 相当。我们使用代理\n$$\nd(r) = P(r) \\cdot \\ln\\big(\\max\\{P(r), e\\}\\big),\n$$\n以及泛化惩罚项\n$$\n\\mathcal{C}(r; N) = \\sqrt{\\frac{d(r) \\cdot \\ln\\!\\left(\\max\\left\\{\\frac{eN}{d(r)}, 1\\right\\}\\right)}{N}},\n$$\n这反映了标准的泛化差距界，其中复杂度随伪维度增长而随样本量 $N$ 减小。所有对数均为自然对数。\n- 定义结构风险准则\n$$\nJ(r; N, A) = \\mathcal{E}_{\\text{approx}}(r; A) + \\mathcal{C}(r; N).\n$$\n对于一个固定的问题 $(N, A)$ 和一个由能整除 $C$ 的有效缩减率组成的候选集 $\\mathcal{R}$，首选的缩减率是最小化子\n$$\nr^{\\star} = \\arg\\min_{r \\in \\mathcal{R}} J(r; N, A).\n$$\n如果出现平局，选择最小化子中最大的 $r$（即最小的容量），以对过拟合保持保守。\n\n您的程序必须：\n- 精确实现上述定义。\n- 对每个测试用例，在候选集 $\\mathcal{R} = \\{1, 2, 4, 8, 16\\}$（其中 $C=16$）上计算 $r^{\\star}$。\n- 将 $\\{\\sigma_{i}(A)\\}_{i=1}^{C}$ 视为给定值；您不需要显式构造 $A$，因为 $\\mathcal{E}_{\\text{approx}}$ 仅依赖于奇异值。\n\n测试套件：\n- 所有情况均使用 $C = 16$ 和候选集 $\\mathcal{R} = \\{1, 2, 4, 8, 16\\}$。对于前三个测试用例，使用相同的真实奇异值，仅改变样本量 $N$ 来探究容量-过拟合权衡以及“先小后大增加容量”的课程学习思想。\n- 情况 1（小样本）：$N = 100$，奇异值为\n$$\n\\sigma(A) = \\left[3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1\\right].\n$$\n- 情况 2（中等样本）：$N = 500$，$\\sigma(A)$ 与情况 1 相同。\n- 情况 3（大样本）：$N = 5000$，$\\sigma(A)$ 与情况 1 相同。\n- 情况 4（低秩目标）：$N = 100$，真实值为低秩，由以下奇异值给出\n$$\n\\sigma(A) = \\left[3.0, 2.5, 2.0, 1.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\\right].\n$$\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含对应于情况 1 至 4 的四个所选缩减率 $[r^{\\star}_{1}, r^{\\star}_{2}, r^{\\star}_{3}, r^{\\star}_{4}]$，格式为用方括号括起来的逗号分隔列表。例如，一个有效的输出看起来像 `[8,4,2,4]`。\n\n注意：\n- 精确使用 $m(r) = C / r$（$\\mathcal{R}$ 中所有的 $r$ 都能整除 $C$）。\n- 在所有出现的 $\\ln(\\cdot)$ 中均使用自然对数。\n- 如果 $d(r) = 0$，为了数值稳定性，将 $\\ln\\!\\left(\\max\\left\\{\\frac{eN}{d(r)}, 1\\right\\}\\right)$ 视为 $\\ln(eN)$。如果 $\\frac{eN}{d(r)}  1$，则将对数的参数钳位到 $1$（使对数对惩罚项的贡献为 $0$），如上所述。\n- 对结果的预期解释与缩减率的课程学习有关：随着 $N$ 在情况 $1 \\rightarrow 2 \\rightarrow 3$ 中增长，最优的 $r^{\\star}$ 不应在所有情况下都严格减小，但预期会增加偏好更小的 $r$（更大的容量）的趋势，因为复杂度惩罚随 $N$ 减小，这反映了近似与过拟合之间的原则性权衡。", "solution": "该问题要求实现一个程序，通过最小化一个已定义的结构风险准则 $J(r; N, A)$，来确定 Squeeze-and-Excitation (SE) 网络块的最佳缩减率 $r^{\\star}$。该解决方案严格遵循所提供的理论框架推导得出，该框架整合了统计学习理论和线性代数的原理。\n\n总体目标是，对于给定的通道数 $C=16$、候选缩减率集合 $\\mathcal{R} = \\{1, 2, 4, 8, 16\\}$ 以及由样本量 $N$ 和真实线性算子 $A$ 的奇异值谱 $\\sigma(A)$ 定义的特定测试用例，找到 $r^{\\star} = \\arg\\min_{r \\in \\mathcal{R}} J(r; N, A)$。结构风险 $J(r; N, A)$ 包含两个相互竞争的项：近似误差 $\\mathcal{E}_{\\text{approx}}(r; A)$ 和复杂度惩罚项 $\\mathcal{C}(r; N)$。\n\n该解决方案通过实现结构风险准则的每个组成部分来进行。\n\n1.  **模型复杂度量化**：与缩减率 $r$ 相关联的模型的复杂度通过一个层次化的定义体系来捕捉。\n    -   SE 模块内部 MLP 的瓶颈维度是 $m(r) = C/r$。对于 $C=16$ 和 $r \\in \\mathcal{R}$，$m(r)$ 的取值范围是 $\\{16, 8, 4, 2, 1\\}$。\n    -   激励 MLP 中的总参数数量（权重和偏置）由 $P(r) = \\frac{2 C^{2}}{r} + \\frac{C}{r} + C$ 给出。较小的 $r$ 对应于较大的瓶颈维度 $m(r)$，从而有更高的参数数量 $P(r)$，代表一个更复杂的模型。\n    -   模型的复杂度通过伪维度的代理指标 $d(r) = P(r) \\cdot \\ln\\big(\\max\\{P(r), e\\}\\big)$ 来正式度量，其中 $e$ 是欧拉数，$\\ln$ 是自然对数。该函数随 $P(r)$ 增长，将参数数量映射到泛化界中使用的复杂度度量。\n\n2.  **近似误差计算**：近似误差 $\\mathcal{E}_{\\text{approx}}(r; A)$ 量化了容量受 $r$ 限制的模型可能达到的最佳性能。\n    -   根据 Eckart–Young–Mirsky 定理，在弗罗贝尼乌斯范数下，对矩阵 $A$ 的最佳秩-k 近似是通过截断其奇异值分解得到的。对于具有各向同性输入的指定回归任务，这转化为一个期望平方误差。\n    -   因此，近似误差是因秩-$m(r)$ 近似而被丢弃的奇异值的平方和：\n        $$\n        \\mathcal{E}_{\\text{approx}}(r; A) = \\sum_{i=m(r)+1}^{C} \\sigma_{i}(A)^{2}\n        $$\n    -   该项代表了由于模型容量有限而产生的不可约误差。较大的 $r$（较小的容量，较小的 $m(r)$）会导致较大的近似误差，因为有更多的奇异值对总和有贡献。相反，对于 $r=1$，$m(1)=C=16$，求和为空，$\\mathcal{E}_{\\text{approx}}(1; A)=0$，因为一个全秩模型可以完美地表示任何矩阵 $A$。\n\n3.  **泛化惩罚项计算**：复杂度惩罚项 $\\mathcal{C}(r; N)$ 用于解释过拟合的风险。\n    -   它由统计学习理论中的一个标准形式定义：\n        $$\n        \\mathcal{C}(r; N) = \\sqrt{\\frac{d(r) \\cdot \\ln\\!\\left(\\max\\left\\{\\frac{eN}{d(r)}, 1\\right\\}\\right)}{N}}\n        $$\n    -   该项随模型复杂度 $d(r)$ 的增加而增加，随样本量 $N$ 的增加而减少。对数因子反映了泛化差距如何随着数据增多而缩小。\n    -   该公式的一个关键特征是将对数的参数钳位在 $1$。如果 $\\frac{eN}{d(r)}  1$（即，如果复杂度 $d(r)$ 相对于样本量 $N$ 较大），对数将变为 $\\ln(1)=0$，惩罚项 $\\mathcal{C}(r;N)$ 也会消失。这模拟了一种情况，即当数据稀少时，泛化界是空泛的，无法惩罚过于复杂的模型。\n\n4.  **优化与选择**：对每个测试用例，通过执行以下过程找到最佳缩减率 $r^{\\star}$：\n    -   对于每个候选 $r \\in \\{1, 2, 4, 8, 16\\}$，计算总风险 $J(r) = \\mathcal{E}_{\\text{approx}}(r; A) + \\mathcal{C}(r; N)$。\n    -   识别出最小风险值 $J_{\\min} = \\min_{r \\in \\mathcal{R}} J(r)$。\n    -   确定最小化子集合 $\\{r | J(r) = J_{\\min}\\}$。使用一个小的数值容差（例如 $10^{-9}$）来稳健地比较浮点风险值。\n    -   最终的最佳比率 $r^{\\star}$ 被选为最小化子集合中的最大值。这个平局打破规则强制选择达到最小风险的最简单模型（最大的 $r$，即最小的容量），这体现了奥卡姆剃刀原理。\n\n这个结构化的、基于原则的过程被应用于四个测试用例中的每一个，以得出最终的最佳缩减率列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal reduction ratio for Squeeze-and-Excitation networks\n    based on a structural risk minimization framework for several test cases.\n    \"\"\"\n    # Define constants and test cases as per the problem statement.\n    C = 16\n    R_CANDIDATES = [1, 2, 4, 8, 16]\n    \n    test_cases = [\n        {\n            \"N\": 100,\n            \"sigmas\": np.array([3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n        },\n        {\n            \"N\": 500,\n            \"sigmas\": np.array([3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n        },\n        {\n            \"N\": 5000,\n            \"sigmas\": np.array([3.0, 2.7, 2.4, 2.1, 1.9, 1.7, 1.5, 1.3, 1.1, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n        },\n        {\n            \"N\": 100,\n            \"sigmas\": np.array([3.0, 2.5, 2.0, 1.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n        }\n    ]\n\n    # Helper functions implementing the provided mathematical definitions.\n    def calculate_P(r, C_val):\n        \"\"\"Calculates the parameter count P(r).\"\"\"\n        return (2 * C_val**2 / r) + (C_val / r) + C_val\n\n    def calculate_d(r, C_val):\n        \"\"\"Calculates the pseudo-dimension surrogate d(r).\"\"\"\n        p_val = calculate_P(r, C_val)\n        return p_val * np.log(max(p_val, np.e))\n\n    def calculate_E_approx(r, C_val, sigmas_val):\n        \"\"\"Calculates the approximation error E_approx(r).\"\"\"\n        m_r = C_val // r\n        # The sum is over singular values with index > m(r).\n        # In 0-based indexing, this corresponds to slicing from index m_r.\n        if m_r >= len(sigmas_val):\n            return 0.0\n        return np.sum(np.square(sigmas_val[m_r:]))\n\n    def calculate_C_penalty(r, C_val, N_val):\n        \"\"\"Calculates the generalization penalty C(r, N).\"\"\"\n        d_val = calculate_d(r, C_val)\n        if d_val == 0: # Should not happen with given P(r) but good practice\n             log_arg = np.e * N_val\n        else:\n             # As per the problem, clamp the argument of the logarithm to be at least 1.\n             log_arg = max((np.e * N_val) / d_val, 1.0)\n        \n        log_term = np.log(log_arg)\n        # The penalty is sqrt( (d * log_term) / N ).\n        penalty = np.sqrt((d_val * log_term) / N_val)\n        return penalty\n\n    optimal_ratios = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        sigmas = case[\"sigmas\"]\n        \n        risk_results = []\n        for r in R_CANDIDATES:\n            # Calculate the two components of the structural risk.\n            e_approx = calculate_E_approx(r, C, sigmas)\n            c_penalty = calculate_C_penalty(r, C, N)\n            \n            # The total risk J(r).\n            j_risk = e_approx + c_penalty\n            risk_results.append((j_risk, r))\n            \n        # Find the minimum risk value.\n        min_risk = min(res[0] for res in risk_results)\n        \n        # Identify all reduction ratios that achieve this minimum risk (using a tolerance for float comparison).\n        minimizers = [r for risk, r in risk_results if np.isclose(risk, min_risk, atol=1e-9)]\n        \n        # Apply the tie-breaking rule: choose the largest r among the minimizers.\n        optimal_r = max(minimizers)\n        optimal_ratios.append(optimal_r)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, optimal_ratios))}]\")\n\nsolve()\n```", "id": "3175715"}]}