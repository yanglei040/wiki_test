{"hands_on_practices": [{"introduction": "要想真正掌握神经网络层是如何学习的，我们必须超越高级库的调用，深入理解其底层的微积分原理。本练习将引导您从零开始，手动推导1x1卷积层关于权重矩阵 $W$ 的梯度 $\\nabla_W L$，从而揭开其反向传播过程的神秘面纱。通过将您的解析结果与数值估算进行验证，您将为深刻理解深度网络中的学习机制打下坚实的基础。[@problem_id:3094339]", "problem": "一个 $1 \\times 1$ 卷积在各个位置上对通道应用相同的线性映射。考虑一个输入张量 $X \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{in}}}$，权重 $W \\in \\mathbb{R}^{C_{\\text{out}} \\times C_{\\text{in}}}$，以及偏置 $b \\in \\mathbb{R}^{C_{\\text{out}}}$。$1 \\times 1$ 卷积的输出 $Y \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{out}}}$ 按分量定义为\n$$\nY[n,h,w,c] \\;=\\; \\sum_{k=1}^{C_{\\text{in}}} W[c,k]\\,X[n,h,w,k] \\;+\\; b[c],\n$$\n对于所有的 $n \\in \\{1,\\dots,N\\}$，$h \\in \\{1,\\dots,H\\}$，$w \\in \\{1,\\dots,W\\}$，以及 $c \\in \\{1,\\dots,C_{\\text{out}}\\}$。给定一个目标张量 $T \\in \\mathbb{R}^{N \\times H \\times W \\times C_{\\text{out}}}$，定义均方误差损失为\n$$\nL \\;=\\; \\frac{1}{2 N H W} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\sum_{c=1}^{C_{\\text{out}}} \\big(Y[n,h,w,c] - T[n,h,w,c]\\big)^{2}.\n$$\n从多元微积分的基本原理（链式法则和线性性质）以及上述 $1 \\times 1$ 卷积和损失的定义出发。不要假设任何预先推导出的梯度公式。\n\n考虑一个具体设置，其中 $N=1$，$H=1$，$W=2$，$C_{\\text{in}}=2$，$C_{\\text{out}}=2$。一个简单的伪随机生成器产生了以下具体的张量（在本练习中将它们视为固定值）：\n- 输入 $X$ 有两个空间位置 $(h,w)=(1,1)$ 和 $(h,w)=(1,2)$，其通道向量为\n$$\nx_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad\nx_{2} \\;=\\; \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix}.\n$$\n- 权重和偏置为\n$$\nW \\;=\\; \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix}, \\qquad\nb \\;=\\; \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix}.\n$$\n- 两个空间位置的目标为\n$$\nt_{1} \\;=\\; \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad\nt_{2} \\;=\\; \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix}.\n$$\n\n任务：\n1. 从上述定义出发，为通用的 $X$、$W$、$b$ 和 $T$ 推导梯度 $\\nabla_{W} L$、$\\nabla_{b} L$ 和 $\\nabla_{X} L$ 的符号公式。\n2. 对给定的具体数值张量，评估你的解析表达式，以获得 $\\nabla_{W} L$ 的具体数值矩阵。\n3. 概念上实现一个关于 $W$ 的中心差分数值梯度，方法是为每个索引对 $(i,j)$ 定义扰动矩阵 $E_{ij}$，其中如果 $p=i,q=j$ 则 $(E_{ij})_{pq} = 1$，否则为 $0$，并计算\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}}_{ij} \\;=\\; \\frac{L\\big(W + \\epsilon E_{ij}\\big) - L\\big(W - \\epsilon E_{ij}\\big)}{2 \\epsilon},\n$$\n其中 $\\epsilon = 10^{-3}$，并保持 $X$、$b$ 和 $T$ 固定。\n4. 使用你的结果，计算关于 $W$ 的解析梯度和数值梯度之差的 Frobenius 范数，\n$$\n\\Delta \\;=\\; \\left\\| \\nabla_{W} L \\;-\\; \\big(\\nabla_{W} L\\big)^{\\text{num}} \\right\\|_{F}.\n$$\n\n报告单一的实数值 $\\Delta$。无需四舍五入。以不带单位的纯数字形式表示答案。", "solution": "该问题定义明确，有科学依据，并包含唯一解所需的所有信息。我们按照概述的任务进行。\n\n该问题要求一个包含四个部分的解答，涉及推导和计算一个 $1 \\times 1$ 卷积层的梯度。我们将依次处理每个任务。\n\n**任务1：符号梯度推导**\n\n给定均方误差损失函数：\n$$\nL = \\frac{1}{2NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} \\sum_{c=1}^{C_{\\text{out}}} \\left( Y[n,h,w,c] - T[n,h,w,c] \\right)^{2}\n$$\n以及 $1 \\times 1$ 卷积操作：\n$$\nY[n,h,w,c] = \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c]\n$$\n\n我们将使用多元微积分的链式法则。我们用 $E[n,h,w,c] = Y[n,h,w,c] - T[n,h,w,c]$ 表示误差项。损失为 $L = \\frac{1}{2NHW} \\sum_{n,h,w,c} E[n,h,w,c]^2$。\n\n损失函数关于任意参数 $\\theta$ 的导数为：\n$$\n\\frac{\\partial L}{\\partial \\theta} = \\frac{1}{2NHW} \\sum_{n,h,w,c} 2 E[n,h,w,c] \\frac{\\partial E[n,h,w,c]}{\\partial \\theta} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\frac{\\partial Y[n,h,w,c]}{\\partial \\theta}\n$$\n因为 $T$ 是一个常数张量。\n\n1.  **关于权重的梯度, $\\nabla_W L$**：\n    我们计算关于单个权重元素 $W[i,j]$ 的偏导数，其中 $i \\in \\{1,\\dots,C_{\\text{out}}\\}$ 且 $j \\in \\{1,\\dots,C_{\\text{in}}\\}$。\n    首先，我们求输出 $Y$ 关于 $W[i,j]$ 的导数：\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial W[i,j]} = \\frac{\\partial}{\\partial W[i,j]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c] \\right)\n    $$\n    该导数仅在 $c=i$ 且 $k=j$ 时非零。使用克罗内克（Kronecker）δ函数，可以写成：\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial W[i,j]} = \\delta_{ci} X[n,h,w,j]\n    $$\n    将此代入通用梯度公式：\n    $$\n    (\\nabla_W L)_{ij} = \\frac{\\partial L}{\\partial W[i,j]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\left( \\delta_{ci} X[n,h,w,j] \\right)\n    $$\n    由于δ函数，对 $c$ 的求和会收缩，只剩下 $c=i$ 的项：\n    $$\n    (\\nabla_W L)_{ij} = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} E[n,h,w,i] X[n,h,w,j]\n    $$\n    用其定义替换 $E[n,h,w,i]$，我们得到关于 $W$ 的梯度的最终符号公式：\n    $$\n    (\\nabla_W L)_{ij} = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (Y[n,h,w,i] - T[n,h,w,i]) X[n,h,w,j]\n    $$\n\n2.  **关于偏置的梯度, $\\nabla_b L$**：\n    我们计算关于单个偏置元素 $b[i]$ 的偏导数，其中 $i \\in \\{1,\\dots,C_{\\text{out}}\\}$。\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial b[i]} = \\frac{\\partial}{\\partial b[i]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] + b[c] \\right) = \\delta_{ci}\n    $$\n    将此代入通用梯度公式：\n    $$\n    (\\nabla_b L)_i = \\frac{\\partial L}{\\partial b[i]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] \\delta_{ci} = \\frac{1}{NHW} \\sum_{n,h,w} E[n,h,w,i]\n    $$\n    $$\n    (\\nabla_b L)_i = \\frac{1}{NHW} \\sum_{n=1}^{N} \\sum_{h=1}^{H} \\sum_{w=1}^{W} (Y[n,h,w,i] - T[n,h,w,i])\n    $$\n\n3.  **关于输入的梯度, $\\nabla_X L$**：\n    我们计算关于输入元素 $X[n',h',w',j]$ 的偏导数，其中 $(n',h',w')$ 是一个特定位置，且 $j \\in \\{1,\\dots,C_{\\text{in}}\\}$。\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial X[n',h',w',j]} = \\frac{\\partial}{\\partial X[n',h',w',j]} \\left( \\sum_{k=1}^{C_{\\text{in}}} W[c,k] X[n,h,w,k] \\right)\n    $$\n    这仅在 $n=n'$, $h=h'$, $w=w'$, 和 $k=j$ 时非零。\n    $$\n    \\frac{\\partial Y[n,h,w,c]}{\\partial X[n',h',w',j]} = \\delta_{nn'} \\delta_{hh'} \\delta_{ww'} W[c,j]\n    $$\n    代入通用梯度公式：\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{\\partial L}{\\partial X[n',h',w',j]} = \\frac{1}{NHW} \\sum_{n,h,w,c} E[n,h,w,c] (\\delta_{nn'} \\delta_{hh'} \\delta_{ww'} W[c,j])\n    $$\n    对 $n,h,w$ 的求和会收缩，剩下：\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{1}{NHW} \\sum_{c=1}^{C_{\\text{out}}} E[n',h',w',c] W[c,j]\n    $$\n    $$\n    (\\nabla_X L)_{n'h'w'j} = \\frac{1}{NHW} \\sum_{c=1}^{C_{\\text{out}}} (Y[n',h',w',c] - T[n',h',w',c]) W[c,j]\n    $$\n\n**任务2：解析梯度 $\\nabla_W L$ 的数值评估**\n\n维度为 $N=1$, $H=1$, $W=2$, $C_{\\text{in}}=2$, $C_{\\text{out}}=2$。归一化因子是 $\\frac{1}{NHW} = \\frac{1}{2}$。给定两个空间位置，我们用 $w=1$ 和 $w=2$ 进行索引。\n\n首先，使用 $y_w = W x_w + b$ 计算每个位置的输出向量 $y_1, y_2$：\n$$\nx_1 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad x_2 = \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix}, \\quad W = \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix}\n$$\n$$\ny_1 = \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 2 \\cdot (-1) \\\\ -1 \\cdot 1 + 0.5 \\cdot (-1) \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix}\n$$\n$$\ny_2 = \\begin{pmatrix} 1  2 \\\\ -1  0.5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 2 + 2 \\cdot 0.5 \\\\ -1 \\cdot 2 + 0.5 \\cdot 0.5 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -1.75 \\end{pmatrix} + \\begin{pmatrix} 0.5 \\\\ -0.5 \\end{pmatrix} = \\begin{pmatrix} 3.5 \\\\ -2.25 \\end{pmatrix}\n$$\n接下来，计算误差向量 $E_1 = y_1-t_1$ 和 $E_2 = y_2-t_2$：\n$$\nt_1 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}, \\quad t_2 = \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix}\n$$\n$$\nE_1 = y_1 - t_1 = \\begin{pmatrix} -0.5 \\\\ -2 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -0.5 \\\\ -1 \\end{pmatrix}\n$$\n$$\nE_2 = y_2 - t_2 = \\begin{pmatrix} 3.5 \\\\ -2.25 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ -2.5 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0.25 \\end{pmatrix}\n$$\n现在，应用 $\\nabla_W L$ 的符号公式。对于单个批次和高度，这简化为对宽度维度求和：\n$$\n\\nabla_W L = \\frac{1}{2} \\sum_{w=1}^{2} E_w x_w^T = \\frac{1}{2} (E_1 x_1^T + E_2 x_2^T)\n$$\n其中 $E_w x_w^T$ 是一个外积。\n$$\nE_1 x_1^T = \\begin{pmatrix} -0.5 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\end{pmatrix} = \\begin{pmatrix} -0.5  0.5 \\\\ -1  1 \\end{pmatrix}\n$$\n$$\nE_2 x_2^T = \\begin{pmatrix} 0.5 \\\\ 0.25 \\end{pmatrix} \\begin{pmatrix} 2  0.5 \\end{pmatrix} = \\begin{pmatrix} 1  0.25 \\\\ 0.5  0.125 \\end{pmatrix}\n$$\n将这些相加并乘以归一化因子：\n$$\n\\nabla_W L = \\frac{1}{2} \\left( \\begin{pmatrix} -0.5  0.5 \\\\ -1  1 \\end{pmatrix} + \\begin{pmatrix} 1  0.25 \\\\ 0.5  0.125 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 0.5  0.75 \\\\ -0.5  1.125 \\end{pmatrix}\n$$\n$$\n\\nabla_W L = \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix}\n$$\n\n**任务3：数值梯度的概念性实现**\n\n分量 $W_{ij}$ 的数值梯度由中心差分公式给出：\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}}_{ij} = \\frac{L(W + \\epsilon E_{ij}) - L(W - \\epsilon E_{ij})}{2 \\epsilon}\n$$\n其中 $\\epsilon = 10^{-3}$ 且 $E_{ij}$ 是标准基矩阵。\n损失 $L$ 是输出 $Y$ 的二次函数。输出 $Y$ 是权重 $W$ 的线性函数。因此，$L$ 是 $W$ 各分量的二次多项式。\n对于任何一维二次多项式 $P(w) = aw^2 + bw + c$，其导数的中心差分公式是精确的：\n$$\n\\frac{P(w + \\epsilon) - P(w - \\epsilon)}{2\\epsilon} = \\frac{[a(w+\\epsilon)^2 + b(w+\\epsilon) + c] - [a(w-\\epsilon)^2 + b(w-\\epsilon) + c]}{2\\epsilon}\n$$\n$$\n= \\frac{a(w^2+2w\\epsilon+\\epsilon^2) - a(w^2-2w\\epsilon+\\epsilon^2) + b(w+\\epsilon) - b(w-\\epsilon)}{2\\epsilon} = \\frac{4aw\\epsilon + 2b\\epsilon}{2\\epsilon} = 2aw+b = P'(w)\n$$\n在计算偏导数时，此结果对任何多元二次函数都成立。由于 $L(W)$ 是 $W$ 各项的二次函数，中心差分近似是精确的，并且不依赖于 $\\epsilon$ 的具体值（对于 $\\epsilon \\neq 0$）。\n因此，数值梯度必须与解析梯度相同。\n$$\n\\big(\\nabla_{W} L\\big)^{\\text{num}} = \\nabla_{W} L = \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix}\n$$\n\n**任务4：差的 Frobenius 范数**\n\n我们被要求计算 $\\Delta = \\left\\| \\nabla_{W} L - \\big(\\nabla_{W} L\\big)^{\\text{num}} \\right\\|_{F}$。\n根据任务3的分析，解析梯度和数值梯度之间的差是零矩阵：\n$$\n\\nabla_{W} L - \\big(\\nabla_{W} L\\big)^{\\text{num}} = \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix} - \\begin{pmatrix} 0.25  0.375 \\\\ -0.25  0.5625 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}\n$$\n矩阵 $A$ 的 Frobenius 范数是 $\\|A\\|_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$。对于零矩阵，这是：\n$$\n\\Delta = \\left\\| \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix} \\right\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0\n$$", "answer": "$$\\boxed{0}$$", "id": "3094339"}, {"introduction": "1x1卷积的威力远不止于简单的线性投影。正如开创性的“Network in Network”论文所提出的，通过堆叠带有非线性激活函数的1x1卷积层，可以创建一个小型的多层感知机（MLP），独立地处理每个像素的通道向量。本实践将指导您实现这个强大的“MLPConv”模块，展示它如何在不增加空间感受野的情况下，学习通道之间复杂的非线性关系。[@problem_id:3094438]", "problem": "要求您仅使用空间尺寸为 $1\\times 1$ 的逐点卷积来实现和分析 Network in Network (NiN) 中的逐像素多层感知器卷积 (MLPConv) 思想，并在合成数据集上测试分类准确率。实现必须是一个完整的、可运行的程序。从以下基本概念开始：\n\n- 离散卷积定义：对于输入张量 $X\\in\\mathbb{R}^{H\\times W\\times C}$ 和卷积核 $K\\in\\mathbb{R}^{k_h\\times k_w\\times C\\times F}$，在通道 $f$ 的空间位置 $(i,j)$ 处的输出为\n$$\nY_{i,j,f}=\\sum_{u=1}^{k_h}\\sum_{v=1}^{k_w}\\sum_{c=1}^{C}K_{u,v,c,f}\\,X_{i+u',j+v',c}\n$$\n其中 $(u',v')$ 索引适当的空间偏移量。空间尺寸为 $1\\times 1$ 的卷积核将其简化为逐像素、跨通道的线性投影：\n$$\nY_{i,j,f}=\\sum_{c=1}^{C}W_{c,f}\\,X_{i,j,c}+b_f\n$$\n其中权重 $W\\in\\mathbb{R}^{C\\times F}$ 和偏置 $b\\in\\mathbb{R}^{F}$ 在所有 $(i,j)$ 位置共享。\n\n- 修正线性单元 (ReLU) 激活函数：对于标量 $z$，定义 $\\operatorname{ReLU}(z)=\\max(0,z)$，并逐分量应用于向量。\n\n- Softmax 函数：对于 logits $z\\in\\mathbb{R}^{K}$，定义 $\\operatorname{softmax}(z)_k=\\exp(z_k)\\big/\\sum_{t=1}^{K}\\exp(z_t)$。\n\n- 多类别分类的交叉熵损失函数：给定一个独热标签向量 $y\\in\\{0,1\\}^{K}$，损失为\n$$\n\\ell(z,y)=-\\sum_{k=1}^{K}y_k\\log\\left(\\operatorname{softmax}(z)_k\\right).\n$$\n\n您的任务是：\n\n- 仅使用 $1\\times 1$ 卷积构建一个三层的逐像素 MLP。对于每个像素 $(i,j)$，网络计算\n$$\nz^{(1)}_{i,j} = W_1\\,x_{i,j} + b_1,\\quad a^{(1)}_{i,j}=\\operatorname{ReLU}\\!\\left(z^{(1)}_{i,j}\\right),\n$$\n$$\nz^{(2)}_{i,j} = W_2\\,a^{(1)}_{i,j} + b_2,\\quad a^{(2)}_{i,j}=\\operatorname{ReLU}\\!\\left(z^{(2)}_{i,j}\\right),\n$$\n$$\nz^{(3)}_{i,j} = W_3\\,a^{(2)}_{i,j} + b_3,\\quad p_{i,j}=\\operatorname{softmax}\\!\\left(z^{(3)}_{i,j}\\right),\n$$\n其中 $x_{i,j}\\in\\mathbb{R}^{C}$ 是像素 $(i,j)$ 处的输入，$p_{i,j}\\in\\mathbb{R}^{K}$ 是预测的类别概率。不允许使用尺寸 $k>1$ 的空间卷积核；只允许使用 $1\\times 1$ 卷积核。\n\n- 从随机初始化开始，使用全批量梯度下降从头开始训练该网络，并在所有像素间共享权重（卷积权重共享）。使用在所有像素上平均的交叉熵损失：\n$$\n\\mathcal{L}=\\frac{1}{N}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\ell\\!\\left(z^{(3)}_{i,j},y_{i,j}\\right),\n$$\n其中 $N=H\\cdot W$ 且 $y_{i,j}\\in\\{0,\\dots,K-1\\}$ 是真实标签。计算的任何部分都不得出现 $k>1$ 的卷积核。\n\n- 训练后评估分类准确率，其定义为\n$$\n\\operatorname{acc}=\\frac{1}{N}\\sum_{i=1}^{H}\\sum_{j=1}^{W}\\mathbf{1}\\!\\left[\\arg\\max_{k}p_{i,j,k}=y_{i,j}\\right],\n$$\n其中 $\\mathbf{1}[\\cdot]$ 表示指示函数。\n\n实现一个单一程序，在以下合成测试套件上进行训练和评估。所有随机性必须由指定的种子值控制以确保可复现性。不允许使用外部数据或用户输入。不涉及任何物理单位。\n\n测试套件（每个案例指定 $H$、$W$、$C$、$K$、隐藏层宽度 $F_1$、$F_2$、优化步数、学习率 $\\eta$ 和种子；以及一个确定性的数据生成规则）：\n\n- 案例 1（一般情况）：\n  - $H=4$, $W=4$, $C=3$, $K=2$, $F_1=5$, $F_2=3$, $\\text{steps}=800$, $\\eta=0.2$, $\\text{seed}=123$。\n  - 特征数据生成：对于像素坐标 $(i,j)$，其中 $i\\in\\{0,\\dots,H-1\\}$ 且 $j\\in\\{0,\\dots,W-1\\}$，\n    $x_{i,j,0}=\\frac{i+1}{H}$，$x_{i,j,1}=\\frac{j+1}{W}$，$x_{i,j,2}=\\frac{(i+1)+(j+1)}{H+W}$。\n    标签：定义一个分数 $s=0.7\\,x_{i,j,0}-0.5\\,x_{i,j,1}+0.4\\,x_{i,j,2}-0.3$；如果 $s>0$ 则设置 $y_{i,j}=1$，否则 $y_{i,j}=0$。\n\n- 案例 2（多类别情况）：\n  - $H=3$, $W=3$, $C=2$, $K=3$, $F_1=4$, $F_2=3$, $\\text{steps}=1500$, $\\eta=0.2$, $\\text{seed}=456$。\n  - 特征数据生成：$x_{i,j,0}=\\frac{i+1}{H}$，$x_{i,j,1}=\\frac{j+1}{W}-0.5$。\n    真实线性分类器：$W^{\\star}\\in\\mathbb{R}^{3\\times 2}$，其行为 $\\left[1.0,-1.2\\right]$、$\\left[-0.6,0.8\\right]$、$\\left[0.2,0.1\\right]$，偏置为 $b^{\\star}=\\left[0.0,0.1,-0.2\\right]$。标签：$y_{i,j}=\\arg\\max_{k}\\left((W^{\\star}x_{i,j}+b^{\\star})_k\\right)$。\n\n- 案例 3（单像素边界）：\n  - $H=1$, $W=1$, $C=4$, $K=2$, $F_1=3$, $F_2=2$, $\\text{steps}=500$, $\\eta=0.2$, $\\text{seed}=789$。\n  - 数据生成：$x_{0,0}=\\left[0.2,-0.4,0.6,-0.8\\right]$。通过阈值生成标签：设 $v=\\left[1.0,1.0,-0.5,-0.5\\right]$ 和 $b=-0.1$，如果 $v^{\\top}x_{0,0}+b>0$ 则设置 $y_{0,0}=1$，否则 $y_{0,0}=0$。\n\n- 案例 4（退化的单类别边缘情况）：\n  - $H=2$, $W=2$, $C=1$, $K=1$, $F_1=2$, $F_2=2$, $\\text{steps}=0$, $\\eta=0.1$, $\\text{seed}=42$。\n  - 数据生成：所有特征为零，$x_{i,j,0}=0$，所有标签 $y_{i,j}=0$。\n\n您的程序必须根据上述架构和约束，在 $\\mathcal{L}$ 上实现全批量梯度下降训练，评估每个案例的分类准确率，并生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，每个准确率表示为四舍五入到四位小数的浮点数（例如，$\\left[\\text{acc}_1,\\text{acc}_2,\\text{acc}_3,\\text{acc}_4\\right]$ 打印为 $\\left[0.9375,1.0000,1.0000,1.0000\\right]$）。", "solution": "用户的问题陈述被确定为有效。它具有科学依据、定义明确、客观且自成体系。该问题要求使用仅 $1\\times 1$ 卷积来实现一个特定的神经网络架构，称为逐像素多层感知器 (MLP) 或 Network-in-Network (NiN) 块。训练需使用全批量梯度下降进行。所有必需的数学定义、架构规范、训练参数和可复现的数据生成规则都已提供。该问题是计算机器学习中一个标准的、尽管详细的实现任务。\n\n解决方案首先实现指定的网络架构及相关的训练算法。核心思想是，$1\\times 1$ 卷积对每个像素的特征向量独立操作，但在所有空间位置上共享权重，这等同于对每个像素的特征向量应用一个标准的全连接层。\n\n遵循常见的机器学习库惯例，我们将输入张量 $X \\in \\mathbb{R}^{H \\times W \\times C}$ 重塑为一个“展平”的像素向量矩阵 $X_{\\text{flat}} \\in \\mathbb{R}^{N \\times C}$，其中 $N=H \\cdot W$ 是像素总数。该矩阵的每一行对应一个像素的特征向量 $x_{i,j} \\in \\mathbb{R}^{C}$。\n\n三层逐像素 MLP 由以下前向传播方程定义：\n第一层计算预激活值 $Z^{(1)} \\in \\mathbb{R}^{N \\times F_1}$ 和激活值 $A^{(1)} \\in \\mathbb{R}^{N \\times F_1}$：\n$$ Z^{(1)} = X_{\\text{flat}} W_1 + b_1 $$\n$$ A^{(1)} = \\operatorname{ReLU}(Z^{(1)}) $$\n这里，$W_1 \\in \\mathbb{R}^{C \\times F_1}$ 和 $b_1 \\in \\mathbb{R}^{F_1}$ 是第一层的权重和偏置，$\\operatorname{ReLU}$ 函数被逐元素应用。\n\n第二层类似地计算预激活值 $Z^{(2)} \\in \\mathbb{R}^{N \\times F_2}$ 和激活值 $A^{(2)} \\in \\mathbb{R}^{N \\times F_2}$：\n$$ Z^{(2)} = A^{(1)} W_2 + b_2 $$\n$$ A^{(2)} = \\operatorname{ReLU}(Z^{(2)}) $$\n其权重为 $W_2 \\in \\mathbb{R}^{F_1 \\times F_2}$，偏置为 $b_2 \\in \\mathbb{R}^{F_2}$。\n\n最后的输出层为 $K$ 个类别生成 logits $Z^{(3)} \\in \\mathbb{R}^{N \\times K}$：\n$$ Z^{(3)} = A^{(2)} W_3 + b_3 $$\n其权重为 $W_3 \\in \\mathbb{R}^{F_2 \\times K}$，偏置为 $b_3 \\in \\mathbb{R}^{K}$。\n\n类别概率 $P \\in \\mathbb{R}^{N \\times K}$ 是通过对每个像素的 logits 应用 $\\operatorname{softmax}$ 函数得到的：\n$$ P_{i,k} = \\frac{\\exp((Z^{(3)})_{i,k})}{\\sum_{t=1}^{K}\\exp((Z^{(3)})_{i,t})} $$\n为保证数值稳定性，其实现为 $\\operatorname{softmax}(z) = \\operatorname{softmax}(z - \\max(z))$。\n\n模型通过最小化所有 $N$ 个像素上的平均交叉熵损失 $\\mathcal{L}$ 来进行训练：\n$$ \\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{k=1}^{K}(Y_{\\text{oh}})_{i,k} \\log(P_{i,k}) $$\n其中 $Y_{\\text{oh}} \\in \\{0,1\\}^{N \\times K}$ 是真实标签的独热编码矩阵。\n\n训练使用全批量梯度下降进行，这需要计算损失 $\\mathcal{L}$ 相对于所有模型参数（$W_1, b_1, W_2, b_2, W_3, b_3$）的梯度。这通过反向传播算法实现，该算法递归地应用链式法则。\n\n损失函数关于输出 logits $Z^{(3)}$ 的梯度，记作误差信号 $\\delta^{(3)}$，是反向传播的起点：\n$$ \\delta^{(3)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(3)}} = \\frac{1}{N}(P - Y_{\\text{oh}}) $$\n此误差随后通过网络向后传播。\n\n第三层参数的梯度为：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_3} = (A^{(2)})^T \\delta^{(3)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_3} = \\sum_{i=1}^{N} (\\delta^{(3)})_i $$\n\n误差传播到第二层的激活值，$\\frac{\\partial \\mathcal{L}}{\\partial A^{(2)}} = \\delta^{(3)} W_3^T$，然后通过 ReLU 非线性函数，其导数为 $\\operatorname{ReLU}'(z) = \\mathbf{1}[z>0]$。第二层预激活值的误差信号为：\n$$ \\delta^{(2)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(2)}} = (\\delta^{(3)} W_3^T) \\odot \\operatorname{ReLU}'(Z^{(2)}) $$\n其中 $\\odot$ 表示逐元素乘法。\n\n第二层参数的梯度为：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_2} = (A^{(1)})^T \\delta^{(2)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_2} = \\sum_{i=1}^{N} (\\delta^{(2)})_i $$\n\n类似地，第一层预激活值的误差信号为：\n$$ \\delta^{(1)} = \\frac{\\partial \\mathcal{L}}{\\partial Z^{(1)}} = (\\delta^{(2)} W_2^T) \\odot \\operatorname{ReLU}'(Z^{(1)}) $$\n\n第一层参数的梯度为：\n$$ \\frac{\\partial \\mathcal{L}}{\\partial W_1} = (X_{\\text{flat}})^T \\delta^{(1)} $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial b_1} = \\sum_{i=1}^{N} (\\delta^{(1)})_i $$\n\n最后，在梯度下降算法的每一步中，使用学习率 $\\eta$ 更新参数：\n$$ W \\leftarrow W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}, \\quad b \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b} $$\n对每一层都进行此操作。在指定的训练步数之后，通过比较每个像素的预测类别（最大 logit 的索引）和真实类别，在相同的数据上评估模型的准确率。\n\n整个过程在以下程序中实现，并应用于问题陈述中定义的四个测试案例。", "answer": "```python\nimport numpy as np\n\ndef train_and_evaluate(X, Y, K, F1, F2, steps, eta, seed):\n    \"\"\"\n    Trains and evaluates the pixel-wise MLP model.\n\n    This function encapsulates the entire process:\n    1. Data preparation.\n    2. Weight initialization.\n    3. The training loop with forward and backward passes.\n    4. Final accuracy evaluation.\n    \"\"\"\n    H, W, C = X.shape\n    N = H * W\n    \n    rng = np.random.default_rng(seed)\n\n    # Reshape data for matrix operations\n    X_flat = X.reshape(N, C)\n    Y_flat = Y.flatten()\n    \n    if K > 1:\n        Y_oh = np.zeros((N, K))\n        Y_oh[np.arange(N), Y_flat] = 1\n    else: # K=1\n        # For K=1, cross-entropy is not well-defined in the multi-class sense.\n        # Accuracy is the only meaningful metric.\n        # Since predictions will be argmax of a single logit, it's always 0.\n        # If all true labels are 0, accuracy will be 100%.\n        if steps == 0:\n            return 1.0\n        Y_oh = np.ones((N, 1))\n\n    # Initialize weights and biases\n    W1 = rng.standard_normal((C, F1), dtype=np.float64) * 0.1\n    b1 = np.zeros(F1, dtype=np.float64)\n    W2 = rng.standard_normal((F1, F2), dtype=np.float64) * 0.1\n    b2 = np.zeros(F2, dtype=np.float64)\n    W3 = rng.standard_normal((F2, K), dtype=np.float64) * 0.1\n    b3 = np.zeros(K, dtype=np.float64)\n\n    # ReLU activation and its derivative\n    def relu(z):\n        return np.maximum(0, z)\n\n    def relu_derivative(z):\n        return (z > 0).astype(z.dtype)\n\n    # Softmax function (numerically stable)\n    def softmax(z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    # Training loop\n    for _ in range(steps):\n        # --- Forward Pass ---\n        Z1 = X_flat @ W1 + b1\n        A1 = relu(Z1)\n        Z2 = A1 @ W2 + b2\n        A2 = relu(Z2)\n        Z3 = A2 @ W3 + b3\n\n        if K > 1:\n            P = softmax(Z3)\n        else: # K=1, softmax is just 1\n            P = np.ones_like(Z3)\n        \n        # --- Backward Pass ---\n        # Gradient of loss w.r.t. Z3\n        delta3 = (P - Y_oh) / N\n\n        # Gradients for layer 3\n        grad_W3 = A2.T @ delta3\n        grad_b3 = np.sum(delta3, axis=0)\n\n        # Propagate error to A2\n        delta_A2 = delta3 @ W3.T\n        # Propagate through ReLU\n        delta2 = delta_A2 * relu_derivative(Z2)\n\n        # Gradients for layer 2\n        grad_W2 = A1.T @ delta2\n        grad_b2 = np.sum(delta2, axis=0)\n\n        # Propagate error to A1\n        delta_A1 = delta2 @ W2.T\n        # Propagate through ReLU\n        delta1 = delta_A1 * relu_derivative(Z1)\n\n        # Gradients for layer 1\n        grad_W1 = X_flat.T @ delta1\n        grad_b1 = np.sum(delta1, axis=0)\n\n        # --- Gradient Descent Update ---\n        W1 -= eta * grad_W1\n        b1 -= eta * grad_b1\n        W2 -= eta * grad_W2\n        b2 -= eta * grad_b2\n        W3 -= eta * grad_W3\n        b3 -= eta * grad_b3\n\n    # --- Final Evaluation ---\n    # Forward pass with trained weights\n    Z1 = X_flat @ W1 + b1\n    A1 = relu(Z1)\n    Z2 = A1 @ W2 + b2\n    A2 = relu(Z2)\n    Z3 = A2 @ W3 + b3\n    \n    # Predictions are the argmax of the logits\n    if K > 1:\n        predictions = np.argmax(Z3, axis=1)\n    else: # K=1\n        predictions = np.zeros_like(Y_flat)\n    \n    # Calculate accuracy\n    accuracy = np.mean(predictions == Y_flat)\n    \n    return accuracy\n\n\ndef solve():\n    test_cases = [\n        {'id': 1, 'H': 4, 'W': 4, 'C': 3, 'K': 2, 'F1': 5, 'F2': 3, 'steps': 800, 'eta': 0.2, 'seed': 123},\n        {'id': 2, 'H': 3, 'W': 3, 'C': 2, 'K': 3, 'F1': 4, 'F2': 3, 'steps': 1500, 'eta': 0.2, 'seed': 456},\n        {'id': 3, 'H': 1, 'W': 1, 'C': 4, 'K': 2, 'F1': 3, 'F2': 2, 'steps': 500, 'eta': 0.2, 'seed': 789},\n        {'id': 4, 'H': 2, 'W': 2, 'C': 1, 'K': 1, 'F1': 2, 'F2': 2, 'steps': 0, 'eta': 0.1, 'seed': 42},\n    ]\n\n    results = []\n    for params in test_cases:\n        H, W, C = params['H'], params['W'], params['C']\n        X = np.zeros((H, W, C), dtype=np.float64)\n        Y = np.zeros((H, W), dtype=int)\n\n        if params['id'] == 1:\n            for i in range(H):\n                for j in range(W):\n                    x0 = (i + 1) / H\n                    x1 = (j + 1) / W\n                    x2 = ((i + 1) + (j + 1)) / (H + W)\n                    X[i, j, :] = [x0, x1, x2]\n                    s = 0.7 * x0 - 0.5 * x1 + 0.4 * x2 - 0.3\n                    Y[i, j] = 1 if s > 0 else 0\n        \n        elif params['id'] == 2:\n            W_star = np.array([[1.0, -1.2], [-0.6, 0.8], [0.2, 0.1]], dtype=np.float64)\n            b_star = np.array([0.0, 0.1, -0.2], dtype=np.float64)\n            for i in range(H):\n                for j in range(W):\n                    x0 = (i + 1) / H\n                    x1 = (j + 1) / W - 0.5\n                    x_ij = np.array([x0, x1], dtype=np.float64)\n                    X[i, j, :] = x_ij\n                    logits = W_star @ x_ij + b_star\n                    Y[i, j] = np.argmax(logits)\n\n        elif params['id'] == 3:\n            x_00 = np.array([0.2, -0.4, 0.6, -0.8], dtype=np.float64)\n            v = np.array([1.0, 1.0, -0.5, -0.5], dtype=np.float64)\n            b = -0.1\n            X[0, 0, :] = x_00\n            Y[0, 0] = 1 if v @ x_00 + b > 0 else 0\n            \n        elif params['id'] == 4:\n            # X and Y are already initialized to zeros.\n            pass\n\n        accuracy = train_and_evaluate(X, Y, params['K'], params['F1'], params['F2'], \n                                      params['steps'], params['eta'], params['seed'])\n        \n        results.append(f\"{accuracy:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3094438"}, {"introduction": "1x1卷积最具影响力的应用之一是构建“瓶颈层”(bottleneck layers)，这是现代高效架构（如 ResNet）的核心。这个动手问题将挑战您像网络架构师一样思考，通过建立并解决一个优化问题来进行设计。您将学习如何为瓶颈模块的计算成本（以浮点运算次数 FLOPs 衡量）建模，并确定最优的瓶颈宽度以达到预期的性能目标，从而掌握在模型复杂性与效率之间进行权衡的关键技能。[@problem_id:3094430]", "problem": "您正在分析一个受深度学习中 Network in Network 概念启发的瓶颈模块：它由三个卷积层序列组成，包括一个将通道数从 $C_{\\text{in}}$ 减少到 $C_{\\text{mid}}$ 的 $1 \\times 1$ 卷积，然后是一个将 $C_{\\text{mid}}$ 映射到 $C_{\\text{mid}}$ 的空间 $k \\times k$ 卷积，最后是一个将通道数从 $C_{\\text{mid}}$ 扩展到 $C_{\\text{out}}$ 的 $1 \\times 1$ 卷积。输入特征图的空间维度为 $H \\times W$，有 $C_{\\text{in}}$ 个通道，输出特征图有 $C_{\\text{out}}$ 个通道。所有量 $H$、$W$、$C_{\\text{in}}$、$C_{\\text{out}}$、$k$ 均为正整数。\n\n您需要选择瓶颈宽度 $C_{\\text{mid}}$ 来平衡计算成本和准确率。计算成本以浮点运算次数（FLOPs）衡量，其中浮点运算次数（FLOPs）指模块单次前向传播的乘加计算次数。该模块的验证准确率贡献由一个严格递增且收益递减的凹函数 $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$ 建模，其中 $A_{0}$ 和 $\\alpha$ 是通过在开发集上进行经验校准得到的正常数。您需要达到一个目标验证准确率 $A_{\\text{t}}$，且 $A_{\\text{t}} > A_{0}$。\n\n从离散网格上的卷积计算定义以及在所有空间位置和通道上对 $1 \\times 1$ 和 $k \\times k$ 卷积的乘法计数出发，推导三层瓶颈模块的总 FLOPs 作为 $C_{\\text{mid}}$ 的函数，并确定对于正的 $H$、$W$、$C_{\\text{in}}$、$C_{\\text{out}}$、$k$，该函数关于 $C_{\\text{mid}}$ 是否单调。然后，建立并求解约束优化问题，在准确率约束 $A(C_{\\text{mid}}) \\geq A_{\\text{t}}$ 下最小化总 FLOPs。以单一解析表达式的闭式解形式给出最优的 $C_{\\text{mid}}$。如果需要任何近似，请陈述并证明其合理性。无需取整。", "solution": "该问题被评估为有效。它在科学上基于深度学习的原理，特别是关于卷积神经网络架构和计算成本分析。作为一个约束优化问题，它具有明确定义的目标函数和约束函数，因此是适定的。其语言客观，设定自洽且一致，并且隐含了通过填充来保持空间维度的假设，这是该领域中的标准惯例。\n\n第一步是推导三层瓶颈模块的总计算成本，以浮点运算次数（FLOPs）衡量。单次乘加运算计为一个 FLOP。假设特征图的空间维度 $H \\times W$ 在整个模块中保持不变，这是通过在卷积层中使用适当填充实现的常规做法。\n\n对于一个从尺寸为 $H_{\\text{in}} \\times W_{\\text{in}} \\times C_{\\text{in}}$ 的输入图，使用尺寸为 $k \\times k$ 的卷积核，生成尺寸为 $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}}$ 的输出图的单个卷积层，其计算成本由输出位置数、输出通道数以及每个输出值的操作数三者之积给出。每个输出值是尺寸为 $k \\times k \\times C_{\\text{in}}$ 的点积的结果。因此，FLOPs 为 $H_{\\text{out}} \\times W_{\\text{out}} \\times C_{\\text{out}} \\times C_{\\text{in}} \\times k^2$。\n\n我们将此公式应用于瓶颈模块中的每一层：\n\n1.  **第一层：** 一个将通道数从 $C_{\\text{in}}$ 减少到 $C_{\\text{mid}}$ 的 $1 \\times 1$ 卷积。\n    *   输入通道数：$C_{\\text{in}}$\n    *   输出通道数：$C_{\\text{mid}}$\n    *   卷积核大小：$1 \\times 1$（即 $k=1$）\n    *   输出图大小：$H \\times W$\n    *   第 1 层的 FLOPs, $F_1$：$H \\times W \\times C_{\\text{mid}} \\times C_{\\text{in}} \\times 1^2 = HW C_{\\text{in}} C_{\\text{mid}}$。\n\n2.  **第二层：** 一个 $k \\times k$ 空间卷积。\n    *   输入通道数：$C_{\\text{mid}}$\n    *   输出通道数：$C_{\\text{mid}}$\n    *   卷积核大小：$k \\times k$\n    *   输出图大小：$H \\times W$\n    *   第 2 层的 FLOPs, $F_2$：$H \\times W \\times C_{\\text{mid}} \\times C_{\\text{mid}} \\times k^2 = HW k^2 C_{\\text{mid}}^2$。\n\n3.  **第三层：** 一个将通道数从 $C_{\\text{mid}}$ 扩展到 $C_{\\text{out}}$ 的 $1 \\times 1$ 卷积。\n    *   输入通道数：$C_{\\text{mid}}$\n    *   输出通道数：$C_{\\text{out}}$\n    *   卷积核大小：$1 \\times 1$\n    *   输出图大小：$H \\times W$\n    *   第 3 层的 FLOPs, $F_3$：$H \\times W \\times C_{\\text{out}} \\times C_{\\text{mid}} \\times 1^2 = HW C_{\\text{out}} C_{\\text{mid}}$。\n\n总 FLOPs $F(C_{\\text{mid}})$ 是这三层 FLOPs 的总和：\n$$ F(C_{\\text{mid}}) = F_1 + F_2 + F_3 = HW C_{\\text{in}} C_{\\text{mid}} + HW k^2 C_{\\text{mid}}^2 + HW C_{\\text{out}} C_{\\text{mid}} $$\n提取公因式，我们得到总 FLOPs 作为 $C_{\\text{mid}}$ 的函数：\n$$ F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\n这是一个关于 $C_{\\text{mid}}$ 的二次函数。\n\n为了确定该函数关于 $C_{\\text{mid}}$ 是否单调，我们考察其导数。为便于分析，将 $C_{\\text{mid}}$ 视为一个连续的正变量：\n$$ \\frac{dF}{dC_{\\text{mid}}} = HW \\left( 2k^2 C_{\\text{mid}} + (C_{\\text{in}} + C_{\\text{out}}) \\right) $$\n根据问题陈述，$H, W, k, C_{\\text{in}}, C_{\\text{out}}$ 均为正整数。瓶颈宽度 $C_{\\text{mid}}$ 也必须是一个正量。因此，导数表达式中的每一项都是正的：$H>0$，$W>0$，$2k^2>0$，$C_{\\text{mid}}>0$ 且 $(C_{\\text{in}}+C_{\\text{out}})>0$。因此，对于所有 $C_{\\text{mid}} > 0$，有 $\\frac{dF}{dC_{\\text{mid}}} > 0$。这证明了总 FLOPs 函数 $F(C_{\\text{mid}})$ 对于正的 $C_{\\text{mid}}$ 是严格递增的。\n\n接下来，我们建立并求解该约束优化问题。我们希望在满足验证准确率约束的条件下，最小化计算成本 $F(C_{\\text{mid}})$。\n优化问题为：\n$$ \\min_{C_{\\text{mid}} > 0} F(C_{\\text{mid}}) = HW \\left( k^2 C_{\\text{mid}}^2 + (C_{\\text{in}} + C_{\\text{out}})C_{\\text{mid}} \\right) $$\n约束条件为：\n$$ A(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\n其中 $A(C_{\\text{mid}}) = A_{0} + \\alpha \\ln(C_{\\text{mid}})$。\n\n我们首先求解关于 $C_{\\text{mid}}$ 的约束。\n$$ A_{0} + \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} $$\n已知 $\\alpha > 0$ 且 $A_{\\text{t}} > A_{0}$：\n$$ \\alpha \\ln(C_{\\text{mid}}) \\geq A_{\\text{t}} - A_{0} $$\n$$ \\ln(C_{\\text{mid}}) \\geq \\frac{A_{\\text{t}} - A_{0}}{\\alpha} $$\n由于指数函数是严格递增的，我们可以对不等式两边取指数而不改变其方向：\n$$ C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\n这个不等式定义了 $C_{\\text{mid}}$ 的可行域。\n\n我们已经确定目标函数 $F(C_{\\text{mid}})$ 对于 $C_{\\text{mid}} > 0$ 是严格递增的。要最小化一个严格递增的函数，必须从可行集中选择其自变量的最小值。因此，$F(C_{\\text{mid}})$ 的最小值将在满足约束的 $C_{\\text{mid}}$ 的最小值处取得。\n\n满足 $C_{\\text{mid}} \\geq \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)$ 的 $C_{\\text{mid}}$ 的最小值恰好在可行域的边界上。尽管在实际实现中 $C_{\\text{mid}}$ 必须是整数，但问题要求提供一个闭式解析表达式，并且无需取整。这意味着我们应该在连续域中求解该问题，这是对此类问题的标准松弛方法。因此，最优值为：\n$$ C_{\\text{mid, opt}} = \\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right) $$\n此表达式给出了达到目标准确率 $A_{\\text{t}}$ 所需的最小瓶颈宽度，并且由于成本函数是单调的，这个 $C_{\\text{mid}}$ 的选择也最小化了计算成本。", "answer": "$$\\boxed{\\exp\\left(\\frac{A_{\\text{t}} - A_{0}}{\\alpha}\\right)}$$", "id": "3094430"}]}