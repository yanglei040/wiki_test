{"hands_on_practices": [{"introduction": "要真正掌握复合缩放，最好的方式莫过于亲手实现它。这个练习将指导你从头开始构建 EfficientNet 的核心计算模型，包括其基本的 MBConv 构建块和关键的复合缩放法则 [@problem_id:3119662]。通过动手计算模型的计算成本（MACs）并分析其与理想性能目标的偏差，你将深刻理解网络维度（深度、宽度、分辨率）如何共同影响计算资源，以及现实因素（如通道数取整和训练超参数）如何影响最终效果。", "problem": "您的任务是为一个高效卷积神经网络架构系列实现一个规范的成本模型和缩放流程，重点关注 EfficientNet 的复合缩放思想以及带有压缩和激励 (Squeeze-and-Excitation, SE) 模块的移动倒置瓶颈卷积 (Mobile Inverted Bottleneck Convolution, MBConv)。您的程序必须完全自包含，并能为预定义的测试套件产生可量化的输出。目标是评估从 EfficientNet-B0 到 EfficientNet-B3 的类似模型在复合缩放下的准确率恢复情况，并量化由训练超参数引起的偏差。\n\n您必须使用的基础理论：\n\n- 对于一个核大小为 $k \\times k$、输入通道数为 $C_{\\text{in}}$、输出通道数为 $C_{\\text{out}}$、空间分辨率为 $H \\times W$ 的二维卷积，其乘加运算次数 (MAC) 为 $H W C_{\\text{in}} k^2 C_{\\text{out}}$；参数数量为 $C_{\\text{in}} k^2 C_{\\text{out}}$。\n- 对于一个在 $C$ 个通道上、核大小为 $k \\times k$、空间分辨率为 $H \\times W$ 的深度可分离卷积，其 MAC 为 $H W C k^2$；参数数量为 $C k^2$。\n- 一个从 $C_{\\text{in}}$ 到 $C_{\\text{out}}$、分辨率为 $H \\times W$ 的逐点 ($1 \\times 1$) 卷积，其 MAC 为 $H W C_{\\text{in}} C_{\\text{out}}$，参数数量为 $C_{\\text{in}} C_{\\text{out}}$。\n- 移动倒置瓶颈卷积 (MBConv) 块，其扩展因子为 $t$，核大小为 $k$，包含以下部分：一个从 $C_{\\text{in}}$ 到 $t C_{\\text{in}}$ 的扩展逐点卷积，一个在 $t C_{\\text{in}}$ 个通道上、核大小为 $k \\times k$ 的深度可分离卷积，一个将通道数按比例 $\\rho_{\\text{se}}$ 减少然后重新扩展的 SE 模块，以及一个投影回 $C_{\\text{out}}$ 的逐点卷积。\n- 压缩和激励 (SE) 模块使用两个全连接层或 $1 \\times 1$ 卷积变换：一个从 $t C_{\\text{in}}$ 到 $t C_{\\text{in}} \\rho_{\\text{se}}$，另一个从 $t C_{\\text{in}} \\rho_{\\text{se}}$ 回到 $t C_{\\text{in}}$。其参数数量和用于这些线性变换的 MACs 为 $t C_{\\text{in}} \\cdot (t C_{\\text{in}} \\rho_{\\text{se}}) + (t C_{\\text{in}} \\rho_{\\text{se}}) \\cdot t C_{\\text{in}}$。在计算 MACs 时，忽略全局平均池化和逐元素非线性操作。\n- 舍入规则：任何通道数都必须舍入到最接近的、能被 $8$ 整除的整数。\n- 复合缩放原则：一个宽度为 $w$、深度重复次数为 $d$、方形分辨率为 $s \\times s$ 的阶段，其总计算量大约与 $s^2 w^2 d$ 乘以块结构中的常数的乘积成比例。设定一个逐级递增的缩放系数 $\\phi$ 和分别作用于 $(d, w, s)$ 的固定乘数 $(\\alpha, \\beta, \\gamma)$，计算量加倍约束要求将 $\\phi$ 增加 $1$ 时，计算量大约增加一倍，这产生了一个形式为 $\\alpha \\beta^2 \\gamma^2 \\approx 2$ 的约束。\n\n您的程序必须：\n\n1. 实现一个带有 SE 模块的 MBConv 成本模型：\n   - 输入：$C_{\\text{in}}$, $C_{\\text{out}}$, $t$, $k$, $s$, $\\rho_{\\text{se}}$。\n   - 输出：使用上述基础公式计算的每个块的 MACs 和参数数量。\n   - 对于一个有 $d$ 次重复的阶段，假设所有重复的 $C_{\\text{in}} = C_{\\text{out}} = w$，并将阶段 MACs 计算为单个块 MACs 的 $d$ 倍，同时对所有通道数应用舍入规则，使其能被 $8$ 整除。\n\n2. 实现复合缩放：\n   - 输入：基线 $(w_0, d_0, s_0)$，缩放乘数 $(\\alpha, \\beta, \\gamma)$，以及缩放系数 $\\phi$。\n   - 根据所述的计算量加倍约束，使用给定的 $\\phi$ 和各自的乘数对每个维度进行指数缩放，计算出缩放后的 $(w, d, s)$。将 $w$ 和 $t w$ 舍入到能被 $8$ 整除，将 $d$ 舍入到最接近的正整数，并将 $s$ 舍入到最接近的整数。\n\n3. 定义一个基线阶段 (EfficientNet-B0 的类似模型) 并使用 MBConv 设置计算其阶段 MACs $N_0$：\n   - 基线：$w_0 = 32$, $d_0 = 4$, $s_0 = 224$, $t = 6$, $k = 3$, $\\rho_{\\text{se}} = 0.25$，通道除数 $8$。\n\n4. 对于系数为 $\\phi$ 的缩放阶段，计算：\n   - 使用步骤 2 中缩放后的 $(w, d, s)$ 和步骤 3 中的 MBConv 设置（除非测试用例中另有规定），计算缩放后阶段的 MACs $N$。\n   - 目标计算量加倍值 $N_{\\text{target}} = N_0 \\cdot 2^{\\phi}$。\n\n5. 根据广泛观察到的神经缩放行为定义一个准确率代理模型：\n   - 使用归一化计算量 $x = N / N_0$。\n   - 一个代理损失 $L(x) = c_0 x^{-p} + c_1$，其中 $c_0$, $p$ 和 $c_1$ 是正常数，代理准确率 $A(x) = 1 - L(x)$。\n   - 训练超参数引入一个有效利用率因子 $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$，其中 $(\\eta, B, E)$ 分别是学习率、批量大小和训练轮数，而 $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ 是它们各自的最优值。\n   - 实现的准确率为 $A_{\\text{eff}} = A(x) \\cdot u$。在系数为 $\\phi$ 时的理想目标准确率为 $A_{\\text{target}} = 1 - \\left(c_0 \\left(2^{\\phi}\\right)^{-p} + c_1\\right)$。\n   - 定义偏差 $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$。\n\n6. 使用以下固定常数和缩放乘数：\n   - 缩放乘数：$\\alpha = 1.2$, $\\beta = 1.1$, $\\gamma = 1.15$。\n   - 准确率代理常数：$c_0 = 0.4$, $p = 0.2$, $c_1 = 0.1$。\n   - 最优超参数：$\\eta^{\\ast} = 0.2$, $B^{\\ast} = 128$, $E^{\\ast} = 350$。\n\n7. 测试套件：\n   - 用例 1：$\\phi = 0$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.2$, $E = 350$。\n   - 用例 2：$\\phi = 1$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.2$, $E = 350$。\n   - 用例 3：$\\phi = 2$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 128$, $\\eta = 0.05$, $E = 350$。\n   - 用例 4：$\\phi = 3$, $t = 6$, $\\rho_{\\text{se}} = 0.25$, $B = 32$, $\\eta = 0.2$, $E = 200$。\n   - 用例 5：$\\phi = 1$, $t = 6$, $\\rho_{\\text{se}} = 0.5$, $B = 128$, $\\eta = 0.2$, $E = 350$。\n\n您的程序应生成单行输出，其中包含按上述顺序排列的各个用例的偏差 $\\Delta$，格式为逗号分隔的列表并用方括号括起，例如 $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$。不应打印任何其他文本。[@problem_id:1018]", "solution": "该问题要求基于为 EfficientNet 系列模型阐明的原则，实现一个计算模型来分析卷积神经网络（CNN）阶段的复合缩放。该分析涉及计算一个阶段的计算成本（以乘加运算次数或 MACs 为单位），缩放其维度，并使用一个已定义的准确率代理模型，将其所得性能与一个理想化目标进行评估。此过程可以量化因架构上的舍入和次优的训练超参数而导致的与理想缩放的偏差。\n\n解决方案构建在三大基础支柱之上：移动倒置瓶颈卷积（MBConv）块的成本模型、网络维度的复合缩放定律，以及一个关于准确率及其与理想目标偏差的显式模型。\n\n首先，我们为一个阶段内的单个 MBConv 块建立成本模型。一个阶段由其宽度（通道数，$w$）、深度（块重复次数，$d$）和边长为 $s$ 的方形空间分辨率定义。对于一个输入和输出通道数均为 $w$、扩展因子为 $t$、深度可分离卷积核大小为 $k \\times k$、并带有压缩和激励（SE）模块（其缩减比率为 $\\rho_{\\text{se}}$）的块，其 MAC 计数是其构成操作的总和。问题规定了一个舍入规则，即所有通道数必须舍入到最接近的、能被一个除数（给定为 8）整除的整数。设 $\\text{round}_m(x)$ 表示将 $x$ 舍入到最接近的 $m$ 的倍数的函数。计算中使用的通道数为：\n- 扩展通道数：$C_{\\text{exp}} = \\text{round}_8(t \\cdot w)$\n- SE 缩减通道数：$C_{\\text{se}} = \\text{round}_8(C_{\\text{exp}} \\cdot \\rho_{\\text{se}})$\n\n一个 MBConv 块的总 MACs 是每个组件 MACs 的总和：\n1.  扩展（$1 \\times 1$ 卷积）：$s^2 \\cdot w \\cdot C_{\\text{exp}}$\n2.  深度可分离（$k \\times k$ 卷积）：$s^2 \\cdot C_{\\text{exp}} \\cdot k^2$\n3.  压缩和激励（在池化特征上的两个线性层）：$2 \\cdot C_{\\text{exp}} \\cdot C_{\\text{se}}$\n4.  投影（$1 \\times 1$ 卷积）：$s^2 \\cdot C_{\\text{exp}} \\cdot w$\n\n单个块的总 MACs，即 $\\text{MACs}_{\\text{block}}$，是这四项之和。该阶段的总 MACs，即 $N$，则为 $d \\cdot \\text{MACs}_{\\text{block}}$。\n\n其次，我们实现复合缩放原则。给定一个具有深度 $d_0$、宽度 $w_0$ 和分辨率 $s_0$ 的基线架构，其维度通过一个系数 $\\phi$ 以及分别针对深度、宽度和分辨率的特定乘数 $\\alpha$、$\\beta$ 和 $\\gamma$ 进行缩放。缩放后的维度由以下公式给出：\n- $d(\\phi) = d_0 \\cdot \\alpha^{\\phi}$\n- $w(\\phi) = w_0 \\cdot \\beta^{\\phi}$\n- $s(\\phi) = s_0 \\cdot \\gamma^{\\phi}$\n\n然后根据问题的规则对这些原始缩放值进行舍入：$d$ 舍入到最接近的正整数，$w$ 舍入到最接近的、能被 8 整除的整数，$s$ 舍入到最接近的整数。约束条件 $\\alpha \\beta^2 \\gamma^2 \\approx 2$ 确保将 $\\phi$ 增加 1 会使总计算成本（与 $d \\cdot w^2 \\cdot s^2$ 成正比）大约增加一倍。\n\n第三，我们定义准确率和偏差模型。模型的性能通过幂律关系与其计算预算相关联。代理准确率 $A(x)$ 定义为 $A(x) = 1 - L(x)$，其中 $L(x) = c_0 x^{-p} + c_1$ 是代理损失。这里，$x = N/N_0$ 是缩放后模型的计算预算相对于基线模型预算 $N_0$ 的归一化值。常数 $c_0$、$c_1$ 和 $p$ 是给定的。对于一个缩放级别 $\\phi$ 的理想或目标准确率 $A_{\\text{target}}$，假设计算量完美缩放，即 $N = N_0 \\cdot 2^\\phi$。因此，$A_{\\text{target}}(\\phi) = 1 - (c_0 (2^\\phi)^{-p} + c_1)$。\n实现的准确率 $A_{\\text{eff}}$ 受训练超参数的影响。这通过一个利用率因子 $u$ 来建模，定义为 $u = \\min(\\eta / \\eta^{\\ast}, 1) \\cdot \\min(B / B^{\\ast}, 1) \\cdot \\min(E / E^{\\ast}, 1)$，其中 $(\\eta, B, E)$ 分别是学习率、批量大小和训练轮数，而 $(\\eta^{\\ast}, B^{\\ast}, E^{\\ast})$ 是它们的最优值。有效准确率则为 $A_{\\text{eff}} = A(x) \\cdot u$。最终我们关心的量是偏差 $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$，它衡量了模型的实现准确率与其理想目标之间的差异。\n\n每个测试用例的计算过程如下：\n1.  使用给定的基线参数（$\\phi=0$）计算基线 MAC 计数 $N_0$。\n2.  对于每个具有特定集合 $(\\phi, t, \\rho_{\\text{se}}, B, \\eta, E)$ 的测试用例：\n    a. 使用 $\\phi$ 和指定的舍入规则计算缩放后的维度 $(w, d, s)$。\n    b. 使用这些缩放后的维度和给定的 MBConv 参数 $(t, k, \\rho_{\\text{se}})$ 计算阶段 MACs $N$。\n    c. 计算归一化计算量 $x = N/N_0$。\n    d. 根据训练超参数计算利用率因子 $u$。\n    e. 计算有效准确率 $A_{\\text{eff}} = (1 - (c_0 x^{-p} + c_1)) \\cdot u$。\n    f. 计算目标准确率 $A_{\\text{target}} = 1 - (c_0 (2^\\phi)^{-p} + c_1)$。\n    g. 计算并记录偏差 $\\Delta = A_{\\text{eff}} - A_{\\text{target}}$。\n\n以下程序实现了这一逻辑，以计算所提供测试套件的偏差。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a cost and accuracy model for a CNN stage based on EfficientNet principles,\n    calculates performance deviations for a set of test cases.\n    \"\"\"\n\n    # --- Constants and Baseline Definitions ---\n\n    # Scaling multipliers\n    ALPHA = 1.2\n    BETA = 1.1\n    GAMMA = 1.15\n\n    # Accuracy proxy constants\n    C0 = 0.4\n    P = 0.2\n    C1 = 0.1\n\n    # Optimal hyperparameters\n    ETA_STAR = 0.2\n    B_STAR = 128\n    E_STAR = 350\n\n    # Baseline architecture and MBConv settings\n    W0 = 32\n    D0 = 4\n    S0 = 224\n    K_BASE = 3\n    CHANNEL_DIVISOR = 8\n\n    # Test suite\n    test_cases = [\n        # (phi, t, rho_se, B, eta, E)\n        (0, 6, 0.25, 128, 0.2, 350),\n        (1, 6, 0.25, 128, 0.2, 350),\n        (2, 6, 0.25, 128, 0.05, 350),\n        (3, 6, 0.25, 32, 0.2, 200),\n        (1, 6, 0.5, 128, 0.2, 350),\n    ]\n\n    # --- Helper Functions ---\n\n    def round_divisible(n, divisor):\n        \"\"\"Rounds n to the nearest integer divisible by divisor.\"\"\"\n        if divisor == 0:\n            return int(n)\n        return int(np.round(n / divisor) * divisor)\n\n    def calculate_stage_macs(w, d, s, t, k, rho_se, divisor):\n        \"\"\"Calculates total MACs for an MBConv stage.\"\"\"\n        \n        # Apply rounding to all channel counts\n        c_in = w # w is assumed pre-rounded\n        c_expand = round_divisible(t * c_in, divisor)\n        c_se = round_divisible(c_expand * rho_se, divisor)\n        # Ensure c_se is at least divisor if c_expand is not zero to avoid div by zero in some real cases,\n        # but problem says \"round\" not \"round up\". For n  divisor/2, this can become 0.\n        if c_se == 0 and c_expand > 0:\n             # As per strict problem spec, a 0 is possible. In a real net this would be an issue.\n             # e.g., if t*w*rho_se  4. We will follow spec.\n             pass\n        c_out = w\n\n        s_squared = s * s\n        \n        # MACs for each part of a single MBConv block\n        macs_expansion = s_squared * c_in * c_expand\n        macs_depthwise = s_squared * c_expand * (k * k)\n        macs_se = 2 * c_expand * c_se\n        macs_projection = s_squared * c_expand * c_out\n        \n        macs_per_block = macs_expansion + macs_depthwise + macs_se + macs_projection\n        \n        total_macs = d * macs_per_block\n        return total_macs\n\n    def scale_dimensions(phi, w0, d0, s0, alpha, beta, gamma, divisor):\n        \"\"\"Scales network dimensions based on phi and rounds them.\"\"\"\n        d_scaled = d0 * (alpha ** phi)\n        w_scaled = w0 * (beta ** phi)\n        s_scaled = s0 * (gamma ** phi)\n        \n        # Rounding as per problem specification\n        d = max(1, int(np.round(d_scaled))) # nearest positive integer\n        w = round_divisible(w_scaled, divisor)\n        s = int(np.round(s_scaled))\n        \n        return w, d, s\n\n    # --- Main Calculation Logic ---\n\n    results = []\n\n    # 1. Calculate baseline MACs (N0)\n    w_base, d_base, s_base = scale_dimensions(0, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n    base_case = test_cases[0]\n    n0 = calculate_stage_macs(\n        w=w_base, \n        d=d_base, \n        s=s_base, \n        t=base_case[1], \n        k=K_BASE, \n        rho_se=base_case[2], \n        divisor=CHANNEL_DIVISOR\n    )\n\n    if n0 == 0:\n        raise ValueError(\"Baseline MAC count (N0) is zero, preventing normalization.\")\n\n    # 2. Iterate through test cases\n    for case in test_cases:\n        phi, t, rho_se, b, eta, e = case\n        \n        # a. Compute scaled dimensions\n        w, d, s = scale_dimensions(phi, W0, D0, S0, ALPHA, BETA, GAMMA, CHANNEL_DIVISOR)\n        \n        # b. Calculate actual stage MACs (N)\n        n = calculate_stage_macs(w, d, s, t, K_BASE, rho_se, CHANNEL_DIVISOR)\n        \n        # c. Compute normalized compute (x)\n        x = n / n0\n        \n        # d. Calculate utilization factor (u)\n        u = min(eta / ETA_STAR, 1) * min(b / B_STAR, 1) * min(e / E_STAR, 1)\n        \n        # e. Compute effective accuracy (A_eff)\n        loss_x = C0 * (x ** -P) + C1\n        a_x = 1 - loss_x\n        a_eff = a_x * u\n        \n        # f. Compute target accuracy (A_target)\n        loss_target = C0 * ((2 ** phi) ** -P) + C1\n        a_target = 1 - loss_target\n        \n        # g. Calculate deviation (Delta)\n        delta = a_eff - a_target\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3119662"}, {"introduction": "在掌握了标准的同向性（方形）输入缩放后，我们将挑战一个更贴近现实的场景：处理非方形的图像输入 [@problem_id:3119513]。此练习要求你将计算成本的缩放定律推广到各向异性分辨率，并通过求解一个约束优化问题来调整网络深度和宽度，以匹配同等计算预算下的标称模型。这项实践不仅能加深你对缩放定律背后基本原理的理解，还能锻炼你将设计原则应用于非标准条件的能力。", "problem": "你的任务是形式化并计算各向异性输入分辨率对遵循 EfficientNet 普及的复合缩放原则的卷积神经网络计算预算的影响。目标是从关于卷积中乘加运算数量的基本定义出发进行推理，并确定当输入分辨率进行各向异性缩放时，如何最小化地调整宽度和深度乘数。\n\n假设与基本定义：\n- 一个二维卷积，其卷积核大小为 $k_x \\times k_y$，输入通道数为 $c_{\\mathrm{in}}$，输出通道数为 $c_{\\mathrm{out}}$，输出空间尺寸为 $h \\times w$，其所需的乘加运算次数与 $k_x k_y \\, c_{\\mathrm{in}} \\, c_{\\mathrm{out}} \\, h \\, w$ 成正比。\n- 考虑一个由这类卷积堆叠而成的网络，其中，在宽度乘数 $w$ 的作用下，所有层的通道维度均与 $w$ 成比例缩放；在深度乘数 $d$ 的作用下，总层数与 $d$ 成比例缩放。\n- 网络输入的空间分辨率为 $(r_x, r_y)$，卷积层内的空间分辨率与 $(r_x, r_y)$ 成比例缩放，比例因子为不依赖于 $w$ 或 $d$ 的常数。\n\n基于这些基本事实，推导出网络总乘加运算次数作为宽度乘数 $w$、深度乘数 $d$ 和输入空间维度 $(r_x, r_y)$ 函数的缩放定律。不要引入任何无法从所述基本事实直接推导出的公式。\n\n定义一个基线分辨率 $r_0 = 224$，并为标称（各向同性）系列采用以下复合缩放参数化：对于一个复合指数 $\\phi \\ge 0$ 和系数 $(\\alpha,\\beta,\\gamma)$，\n- 宽度乘数 $w(\\phi) = \\beta^{\\phi}$，\n- 深度乘数 $d(\\phi) = \\alpha^{\\phi}$，\n- 目标各向同性分辨率 $r(\\phi) = r_0 \\, \\gamma^{\\phi}$。\n\n使用具体系数 $(\\alpha,\\beta,\\gamma) = (1.2, 1.1, 1.15)$。\n\n任务：\n- 对于给定的 $\\phi$ 和一个各向异性输入分辨率 $(R_x, R_y)$，确定调整后的乘数 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$，使得总乘加运算次数与相同 $\\phi$ 下的标称各向同性计算量相匹配。\n- 在所有满足计算量相等约束的 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$ 中，选择使与标称乘数的平方对数偏差最小化的那一对：\n  最小化 $(\\log w_{\\mathrm{adj}} - \\log w(\\phi))^2 + (\\log d_{\\mathrm{adj}} - \\log d(\\phi))^2$，约束条件为计算量相等。\n- 你的程序应为每个测试用例输出相对调整因子：\n  $$\n  \\frac{w_{\\mathrm{adj}}}{w(\\phi)} \\quad \\text{和} \\quad \\frac{d_{\\mathrm{adj}}}{d(\\phi)} \\, .\n  $$\n\n测试套件：\n使用 $r_0 = 224$，$(\\alpha,\\beta,\\gamma) = (1.2, 1.1, 1.15)$，以及以下五个情况。在每种情况下，程序必须计算出对 $\\left(\\frac{w_{\\mathrm{adj}}}{w(\\phi)}, \\frac{d_{\\mathrm{adj}}}{d(\\phi)}\\right)$。\n\n- 情况 1：$\\phi = 1$, $R_x = 224 \\cdot 1.15$, $R_y = 224 \\cdot 1.15$。\n- 情况 2：$\\phi = 2$, $R_x = 224 \\cdot 1.15^{2} \\cdot 3$, $R_y = 224 \\cdot 1.15^{2} / 3$。\n- 情况 3：$\\phi = 1$, $R_x = 224 \\cdot 1.15$, $R_y = 224 \\cdot 1.15 / 2$。\n- 情况 4：$\\phi = 1$, $R_x = 224 \\cdot 1.15$, $R_y = 224 \\cdot 1.15 \\cdot 2$。\n- 情况 5：$\\phi = 0$, $R_x = 336$, $R_y = 112$。\n\n最终输出格式：\n- 你的程序应生成单行输出，包含一个逗号分隔的列表的列表形式的结果，其中每个内部列表按给定顺序对应一个测试用例，并包含两个浮点数 $\\left[\\frac{w_{\\mathrm{adj}}}{w(\\phi)}, \\frac{d_{\\mathrm{adj}}}{d(\\phi)}\\right]$，四舍五入到 $6$ 位小数。\n- 例如，格式必须完全如下：\n  $$\n  [[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4],[a_5,b_5]]\n  $$\n  其中每个 $a_i$ 和 $b_i$ 是一个小数点后最多有六位的十进制数。", "solution": "用户提供了一个提法明确且有科学依据的问题，要求推导神经网络计算成本的缩放定律，并求解一个约束优化问题。该问题已验证为合理且自洽。\n\n### 1. 计算缩放定律的推导\n总乘加运算（MACs）次数是计算成本的一种度量，通常称为FLOPs，需要被确定为网络缩放参数的函数。\n\n单个二维卷积的MACs被给定为与卷积核维度（$k_x, k_y$）、输入通道数（$c_{\\mathrm{in}}$）、输出通道数（$c_{\\mathrm{out}}$）以及输出空间维度（$h, w$）的乘积成正比。\n$$\n\\text{MACs}_{\\text{layer}} \\propto k_x k_y c_{\\mathrm{in}} c_{\\mathrm{out}} h w\n$$\n问题陈述了网络架构如何随宽度乘数 $w$、深度乘数 $d$ 和输入分辨率 $(r_x, r_y)$ 缩放。\n\n- **宽度缩放**：所有通道维度（$c_{\\mathrm{in}}$, $c_{\\mathrm{out}}$）都与宽度乘数 $w$ 成比例缩放。因此，乘积 $c_{\\mathrm{in}} c_{\\mathrm{out}}$ 按 $w^2$ 缩放。\n- **分辨率缩放**：特征图的空间维度（$h, w$）与输入分辨率 $(r_x, r_y)$ 成比例缩放。这意味着 $h \\propto r_y$ 且 $w \\propto r_x$。因此，乘积 $h w$ 按 $r_x r_y$ 缩放。\n- **深度缩放**：网络中的总层数与深度乘数 $d$ 成比例缩放。\n\n结合这些因素，缩放后网络中任何单层的MACs都与 $w^2 r_x r_y$ 成正比。整个网络的总MACs是所有层的MACs之和，与每层的MACs乘以层数成正比。\n$$\n\\text{总 MACs} \\propto (\\text{层数}) \\times (\\text{每层的 MACs})\n$$\n因此，我们表示为函数 $F(w, d, r_x, r_y)$ 的总MACs，其缩放关系如下：\n$$\nF(w, d, r_x, r_y) \\propto d \\cdot w^2 \\cdot r_x r_y\n$$\n我们可以用一个比例常数 $C$ 将其表示为等式，该常数取决于基础网络的具体架构，但不依赖于缩放因子：\n$$\nF(w, d, r_x, r_y) = C \\cdot d \\cdot w^2 \\cdot r_x r_y\n$$\n这个推导出的公式即为网络计算成本的缩放定律。\n\n### 2. 优化问题的公式化\n给定一个由复合指数 $\\phi \\ge 0$ 和系数 $(\\alpha, \\beta, \\gamma) = (1.2, 1.1, 1.15)$ 定义的标称（各向同性）缩放配置：\n- 宽度乘数：$w(\\phi) = \\beta^\\phi = 1.1^\\phi$\n- 深度乘数：$d(\\phi) = \\alpha^\\phi = 1.2^\\phi$\n- 各向同性分辨率：$r(\\phi) = r_0 \\gamma^\\phi = 224 \\cdot 1.15^\\phi$\n\n此标称配置的计算成本为：\n$$\nF_{\\mathrm{nom}} = F(w(\\phi), d(\\phi), r(\\phi), r(\\phi)) = C \\cdot d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2\n$$\n对于给定的 $\\phi$ 和各向异性输入分辨率 $(R_x, R_y)$，我们需要找到调整后的乘数 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$，使得计算预算相同。这给了我们约束条件：\n$$\nF(w_{\\mathrm{adj}}, d_{\\mathrm{adj}}, R_x, R_y) = F_{\\mathrm{nom}}\n$$\n$$\nC \\cdot d_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2 \\cdot R_x R_y = C \\cdot d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2\n$$\n常数 $C$ 可以消掉，得到约束方程：\n$$\nd_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2 = \\frac{d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2}{R_x R_y}\n$$\n目标是找到满足此约束的对 $(w_{\\mathrm{adj}}, d_{\\mathrm{adj}})$，同时最小化与标称乘数的平方对数偏差：\n$$\n\\text{最小化 } J(w_{\\mathrm{adj}}, d_{\\mathrm{adj}}) = (\\log w_{\\mathrm{adj}} - \\log w(\\phi))^2 + (\\log d_{\\mathrm{adj}} - \\log d(\\phi))^2\n$$\n\n### 3. 求解优化问题\n为简化问题，我们在乘数的对数空间中进行操作。令：\n- $W = \\log w_{\\mathrm{adj}}$，$D = \\log d_{\\mathrm{adj}}$\n- $W_0 = \\log w(\\phi)$，$D_0 = \\log d(\\phi)$\n\n目标函数变为最小化 $(W, D)$ 和 $(W_0, D_0)$ 之间的平方欧几里得距离：\n$$\n\\text{最小化 } (W - W_0)^2 + (D - D_0)^2\n$$\n对约束方程取对数：\n$$\n\\log(d_{\\mathrm{adj}} \\cdot w_{\\mathrm{adj}}^2) = \\log\\left(\\frac{d(\\phi) \\cdot [w(\\phi)]^2 \\cdot [r(\\phi)]^2}{R_x R_y}\\right)\n$$\n$$\nD + 2W = D_0 + 2W_0 + \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n这是一个关于 $W$ 和 $D$ 的线性约束。问题是找到这条线上离点 $(W_0, D_0)$ 最近的点。我们使用拉格朗日乘数法。设拉格朗日量为 $\\mathcal{L}(W, D, \\lambda)$:\n$$\n\\mathcal{L} = (W - W_0)^2 + (D - D_0)^2 - \\lambda \\left( D + 2W - C' \\right)\n$$\n其中 $C'$ 是对数约束方程右侧的常数。求偏导数并设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = 2(W - W_0) - 2\\lambda = 0 \\implies W - W_0 = \\lambda\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial D} = 2(D - D_0) - \\lambda = 0 \\implies D - D_0 = \\frac{\\lambda}{2}\n$$\n将 $W = W_0 + \\lambda$ 和 $D = D_0 + \\lambda/2$ 代入约束 $D + 2W = C'$:\n$$\n(D_0 + \\lambda/2) + 2(W_0 + \\lambda) = C' \\implies D_0 + 2W_0 + \\frac{5}{2}\\lambda = C'\n$$\n解出 $\\lambda$:\n$$\n\\lambda = \\frac{2}{5} (C' - (D_0 + 2W_0)) = \\frac{2}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n现在我们找到对数空间中的最优调整量：\n$$\n\\log w_{\\mathrm{adj}} = W = W_0 + \\lambda = \\log w(\\phi) + \\frac{2}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n$$\n\\log d_{\\mathrm{adj}} = D = D_0 + \\frac{\\lambda}{2} = \\log d(\\phi) + \\frac{1}{5} \\log\\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)\n$$\n对两边取指数以求得 $w_{\\mathrm{adj}}$ 和 $d_{\\mathrm{adj}}$:\n$$\nw_{\\mathrm{adj}} = w(\\phi) \\cdot \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{2/5}\n$$\n$$\nd_{\\mathrm{adj}} = d(\\phi) \\cdot \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{1/5}\n$$\n问题要求的是相对调整因子，即：\n$$\n\\frac{w_{\\mathrm{adj}}}{w(\\phi)} = \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{2/5}\n$$\n$$\n\\frac{d_{\\mathrm{adj}}}{d(\\phi)} = \\left(\\frac{[r(\\phi)]^2}{R_x R_y}\\right)^{1/5}\n$$\n这些公式提供了待实现的解。项 $\\frac{[r(\\phi)]^2}{R_x R_y}$ 量化了目标各向同性分辨率与给定各向异性分辨率之间总像素数的不平衡。指数 $2/5$ 和 $1/5$ 源于宽度 ($w^2$) 和深度 ($d^1$) 对计算预算的相对影响以及最小化准则。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the adjusted width and depth multipliers for a CNN under\n    anisotropic input resolution to maintain a constant compute budget,\n    based on the principles of compound scaling.\n    \"\"\"\n    \n    # Define constants from the problem statement.\n    r0 = 224.0\n    alpha = 1.2\n    beta = 1.1\n    gamma = 1.15\n\n    # Define the test cases.\n    test_cases = [\n        # Case 1: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0)),\n        # Case 2: phi = 2, Rx = 224 * 1.15^2 * 3, Ry = 224 * 1.15^2 / 3\n        (2.0, r0 * np.power(gamma, 2.0) * 3.0, r0 * np.power(gamma, 2.0) / 3.0),\n        # Case 3: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15 / 2\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0) / 2.0),\n        # Case 4: phi = 1, Rx = 224 * 1.15, Ry = 224 * 1.15 * 2\n        (1.0, r0 * np.power(gamma, 1.0), r0 * np.power(gamma, 1.0) * 2.0),\n        # Case 5: phi = 0, Rx = 336, Ry = 112\n        (0.0, 336.0, 112.0)\n    ]\n\n    results = []\n    for phi, Rx, Ry in test_cases:\n        # Calculate the nominal isotropic resolution for the given phi.\n        r_phi = r0 * np.power(gamma, phi)\n        \n        # Calculate the ratio of nominal squared resolution to the product of\n        # the anisotropic resolutions. This is the core scaling factor.\n        resolution_ratio = np.power(r_phi, 2) / (Rx * Ry)\n        \n        # Calculate the relative adjustment factors.\n        # The exponents 2/5 and 1/5 come from the constrained optimization.\n        # w_adj/w(phi) = (resolution_ratio)^(2/5)\n        # d_adj/d(phi) = (resolution_ratio)^(1/5)\n        width_adj_ratio = np.power(resolution_ratio, 0.4)\n        depth_adj_ratio = np.power(resolution_ratio, 0.2)\n        \n        # Round the results to 6 decimal places as required.\n        w_adj_rounded = round(width_adj_ratio, 6)\n        d_adj_rounded = round(depth_adj_ratio, 6)\n        \n        results.append([w_adj_rounded, d_adj_rounded])\n\n    # Format the final output string to exactly match the problem specification:\n    # \"[[a_1,b_1],[a_2,b_2],...]\"\n    # Using str() on a list of lists and removing spaces is a robust way to achieve this.\n    output_str = str(results).replace(\" \", \"\")\n    \n    print(output_str)\n\nsolve()\n```", "id": "3119513"}, {"introduction": "之前的练习中，我们使用了给定的缩放系数 $(\\alpha, \\beta, \\gamma)$，但在实际应用中，如何确定这些系数本身就是关键问题。这个终极挑战将模型设计构建为一个多目标优化问题，你需要在模型精度与特定硬件的延迟、内存占用等成本之间做出权衡 [@problem_id:3119675]。通过求解最优的缩放系数，你将学会如何为特定部署目标自动化地搜索高效网络架构，从而完成从模型分析者到设计者的转变。", "problem": "你的任务是实现一个自包含程序，为 EfficientNet 风格的卷积神经网络 (CNN) 计算最优复合缩放系数。该优化必须在现实的硬件预算约束下，平衡模型准确率与延迟和内存成本。你的程序必须通过标量化来解决一个多目标优化问题。\n\n从卷积架构中以下基于经验且广泛使用的缩放关系开始：\n\n1. 对于 CNN 中的一堆卷积层，所需的浮点计算量与深度、宽度的平方以及输入分辨率的平方的乘积成正比。设深度、宽度和分辨率的缩放乘数分别表示为 $\\alpha$、$\\beta$ 和 $\\gamma$。那么，一个理想化的延迟度量 $L$（以归一化的无量纲单位表示）被建模为\n$$\nL(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2,\n$$\n其中 $k_L$ 是一个正常数，用于捕捉基线计算到延迟的比例关系。\n\n2. 总内存占用 $M$（以归一化的无量纲单位表示），结合了参数内存和激活内存，被建模为\n$$\nM(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2,\n$$\n其中 $k_P$ 表示参数内存的比例系数，$k_A$ 表示激活内存的比例系数。这些形式源于参数大致与深度和宽度的平方的乘积成比例，而激活值与深度、宽度和分辨率的平方成比例。\n\n3. 准确率 $A$（以一个无量綱的抽象准确率得分单位表示）随着规模的增加表现出收益递减的特性。一个与经验缩放定律一致的凹代理模型是\n$$\nA(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma),\n$$\n其中 $s_d$、$s_w$ 和 $s_r$ 是正系数，编码了准确率对深度、宽度和分辨率的相对敏感度。\n\n多目标优化被标量化为\n$$\nJ(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma),\n$$\n其中 $\\lambda_1$ 和 $\\lambda_2$ 是非负的硬件特定权重，分别对延迟和内存进行惩罚，两者都采用无量纲的归一化单位。\n\n优化约束：\n- 复合计算预算：\n$$\n\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B,\n$$\n其中 $B \\ge 1$ 是一个无量纲的预算。这个不等式反映了如果惩罚很高，可用计算资源可能会未被充分利用；但它绝不能被超过。\n- 下界：\n$$\n\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1.\n$$\n这些下界强制执行非收缩缩放（不低于基线进行缩减）。\n- 上界：\n$$\n\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U,\n$$\n其中 $U$ 是一个固定的上限，以确保问题的适定性，并反映缩放的实际界限。\n\n在所有计算中使用的常量：\n- $k_L = 1.0$, $k_P = 1.0$, $k_A = 0.5$,\n- $s_d = 0.20$, $s_w = 0.30$, $s_r = 0.50$,\n- $U = 5.0$.\n\n你的程序必须在约束条件下数值最大化 $J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2)$。你可以使用任何正确的数值方法来解决这个问题；一个用于可微目标函数的约束优化器是可以接受的。所有量 $L$、$M$、$A$、$J$、$\\alpha$、$\\beta$、$\\gamma$、$\\lambda_1$、$\\lambda_2$、$B$、$k_L$、$k_P$、$k_A$、$s_d$、$s_w$、$s_r$ 和 $U$ 都是无量纲的，并且必须如此处理。\n\n测试套件：\n在以下硬件特定的惩罚权重和预算 $(\\lambda_1,\\lambda_2,B)$ 上运行你的求解器：\n- 案例 1（均衡，理想情况）：$(0.02,\\, 0.01,\\, 2.0)$\n- 案例 2（内存主导的惩罚）：$(0.0,\\, 0.03,\\, 3.0)$\n- 案例 3（延迟主导的惩罚）：$(0.05,\\, 0.0,\\, 3.0)$\n- 案例 4（非常高的惩罚，可能导致最小化缩放）：$(0.20,\\, 0.10,\\, 2.0)$\n- 案例 5（边界预算）：$(0.01,\\, 0.005,\\, 1.0)$\n\n要求的最终输出：\n- 对于每个测试用例，计算最优三元组 $(\\alpha^\\star,\\beta^\\star,\\gamma^\\star)$。\n- 将三元组中的每个标量四舍五入到4位小数。\n- 你的程序应生成单行输出，其中包含所有案例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素是四舍五入后的Python风格列表形式的三元组。例如，输出应如下所示：\n$$\n[\\,[\\alpha_1,\\beta_1,\\gamma_1],\\,[\\alpha_2,\\beta_2,\\gamma_2],\\,\\dots\\,]\n$$\n其中每个 $\\alpha_i$、$\\beta_i$ 和 $\\gamma_i$ 都四舍五入到4位小数。不应打印任何额外的文本。", "solution": "用户提供了一个定义明确的多目标优化问题，我将首先对其进行验证，然后求解。\n\n### 步骤1：提取已知条件\n\n**模型：**\n- 延迟：$L(\\alpha,\\beta,\\gamma) = k_L \\, \\alpha \\, \\beta^2 \\, \\gamma^2$\n- 内存：$M(\\alpha,\\beta,\\gamma) = k_P \\, \\alpha \\, \\beta^2 \\;+\\; k_A \\, \\alpha \\, \\beta \\, \\gamma^2$\n- 准确率：$A(\\alpha,\\beta,\\gamma) = s_d \\, \\ln(\\alpha) \\;+\\; s_w \\, \\ln(\\beta) \\;+\\; s_r \\, \\ln(\\gamma)$\n\n**目标函数：**\n- 要最大化的标量化目标：$J(\\alpha,\\beta,\\gamma;\\lambda_1,\\lambda_2) \\;=\\; A(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_1 \\, L(\\alpha,\\beta,\\gamma) \\;-\\; \\lambda_2 \\, M(\\alpha,\\beta,\\gamma)$\n\n**约束条件：**\n- 计算预算：$\\alpha \\, \\beta^2 \\, \\gamma^2 \\;\\le\\; B$，其中 $B \\ge 1$\n- 下界：$\\alpha \\ge 1, \\quad \\beta \\ge 1, \\quad \\gamma \\ge 1$\n- 上界：$\\alpha \\le U, \\quad \\beta \\le U, \\quad \\gamma \\le U$\n\n**常量：**\n- $k_L = 1.0$\n- $k_P = 1.0$\n- $k_A = 0.5$\n- $s_d = 0.20$\n- $s_w = 0.30$\n- $s_r = 0.50$\n- $U = 5.0$\n\n**测试套件：**\n- 案例 1：$(\\lambda_1, \\lambda_2, B) = (0.02, 0.01, 2.0)$\n- 案例 2：$(\\lambda_1, \\lambda_2, B) = (0.0, 0.03, 3.0)$\n- 案例 3：$(\\lambda_1, \\lambda_2, B) = (0.05, 0.0, 3.0)$\n- 案例 4：$(\\lambda_1, \\lambda_2, B) = (0.20, 0.10, 2.0)$\n- 案例 5：$(\\lambda_1, \\lambda_2, B) = (0.01, 0.005, 1.0)$\n\n### 步骤2：使用提取的已知条件进行验证\n\n1.  **科学依据：** 该问题植根于神经网络架构设计的原则，特别是与 EfficientNet 普及的复合缩放相关的原则。延迟（$L$）、内存（$M$）和准确率（$A$）的函数形式是简化但合理的模型。计算量随深度（$\\alpha$）、宽度平方（$\\beta^2$）和分辨率平方（$\\gamma^2$）的缩放是标准做法。内存模型正确地区分了参数（随 $\\alpha \\beta^2$ 缩放）和激活（随 $\\alpha \\beta \\gamma^2$ 缩放）。对数准确率模型反映了收益递减的原则。该公式是对深度学习中一个真实世界工程问题的有效抽象。\n\n2.  **适定性：** 该问题是在 $\\mathbb{R}^3$ 的一个紧集（闭合且有界）上最大化一个连续、可微的函数 $J$。可行域由不等式 $1 \\le \\alpha \\le U$、$1 \\le \\beta \\le U$、$1 \\le \\gamma \\le U$ 和 $\\alpha \\beta^2 \\gamma^2 \\le B$ 定义。根据极值定理，紧集上的连续函数必能达到最大值。因此，解的存在性得到保证。该问题是适定的。\n\n3.  **客观性：** 该问题使用精确的数学定义和客观、正式的语言进行陈述。所有常量和变量都有明确的定义。没有歧义。\n\n### 步骤3：结论与行动\n\n该问题是有效的。它是一个适定的、有科学依据的、客观的约束非线性优化问题。我将继续进行求解。\n\n### 求解推导\n\n任务是找到 $(\\alpha, \\beta, \\gamma)$ 的值，使目标函数 $J$ 在给定约束条件下最大化。这是一个约束非线性规划问题。由于数值优化库通常设计用于寻找最小值，我们将等价地最小化目标函数的负值，即 $-J$。\n\n设变量向量为 $\\mathbf{x} = [\\alpha, \\beta, \\gamma]$。要最小化的目标函数是：\n$$\n-J(\\mathbf{x}; \\lambda_1, \\lambda_2) = -A(\\mathbf{x}) + \\lambda_1 L(\\mathbf{x}) + \\lambda_2 M(\\mathbf{x})\n$$\n代入给定的表达式和常量：\n$$\n-J(\\alpha, \\beta, \\gamma) = -[s_d \\ln(\\alpha) + s_w \\ln(\\beta) + s_r \\ln(\\gamma)] + \\lambda_1 [k_L \\alpha \\beta^2 \\gamma^2] + \\lambda_2 [k_P \\alpha \\beta^2 + k_A \\alpha \\beta \\gamma^2]\n$$\n按变量 $\\alpha, \\beta, \\gamma$ 对项进行分组：\n$$\n-J(\\alpha, \\beta, \\gamma) = -s_d \\ln(\\alpha) - s_w \\ln(\\beta) - s_r \\ln(\\gamma) + (\\lambda_1 k_L + \\lambda_2 k_A) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 k_P \\alpha \\beta^2\n$$\n代入已知的常量值 ($k_L=1.0, k_P=1.0, k_A=0.5, s_d=0.2, s_w=0.3, s_r=0.5$)：\n$$\n-J(\\alpha, \\beta, \\gamma) = -0.2 \\ln(\\alpha) - 0.3 \\ln(\\beta) - 0.5 \\ln(\\gamma) + (\\lambda_1 + 0.5 \\lambda_2) \\alpha \\beta^2 \\gamma^2 + \\lambda_2 \\alpha \\beta^2\n$$\n这是我们将提供给数值优化器的函数。\n\n约束条件是：\n1.  **不等式约束：** $\\alpha \\beta^2 \\gamma^2 \\le B$。对于要求约束形式为 $g(\\mathbf{x}) \\ge 0$ 的标准求解器，这可以写成 $B - \\alpha \\beta^2 \\gamma^2 \\ge 0$。\n2.  **边界约束：**\n    $1 \\le \\alpha \\le U$\n    $1 \\le \\beta \\le U$\n    $1 \\le \\gamma \\le U$\n    其中 $U=5.0$。\n\n我们将使用 `scipy.optimize` 库中的序列最小二乘规划 (`SLSQP`) 方法，因为它非常适合处理同时具有边界和不等式约束的非线性优化问题。\n\n我们将遍历每个测试用例 $(\\lambda_1, \\lambda_2, B)$，相应地定义目标函数和约束。一个稳健的优化器初始猜测是基线模型 $(\\alpha, \\beta, \\gamma) = (1, 1, 1)$，对于任何 $B \\ge 1$，它都保证在可行域内。\n\n对于 $B=1.0$ 的特殊情况（测试用例 5），约束 $\\alpha \\ge 1, \\beta \\ge 1, \\gamma \\ge 1$ 和 $\\alpha \\beta^2 \\gamma^2 \\le 1$ 共同迫使唯一的可能解为 $\\alpha=1, \\beta=1, \\gamma=1$。优化器应收敛到此点。\n\n每个测试用例的最终结果将是最优三元组 $(\\alpha^\\star, \\beta^\\star, \\gamma^\\star)$，每个分量都四舍五入到4位小数，并按指定格式输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes optimal compound scaling coefficients for a CNN model by solving\n    a constrained multi-objective optimization problem.\n    \"\"\"\n    # Define constants as specified in the problem statement.\n    # All quantities are dimensionless.\n    k_L = 1.0  # Latency proportionality\n    k_P = 1.0  # Parameter memory proportionality\n    k_A = 0.5  # Activation memory proportionality\n    s_d = 0.20 # Accuracy sensitivity to depth\n    s_w = 0.30 # Accuracy sensitivity to width\n    s_r = 0.50 # Accuracy sensitivity to resolution\n    U = 5.0    # Upper bound for scaling factors\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (lambda1, lambda2, B)\n    test_cases = [\n        (0.02, 0.01, 2.0),   # Case 1: balanced, happy path\n        (0.0, 0.03, 3.0),    # Case 2: memory-dominated penalty\n        (0.05, 0.0, 3.0),    # Case 3: latency-dominated penalty\n        (0.20, 0.10, 2.0),   # Case 4: very high penalties\n        (0.01, 0.005, 1.0),  # Case 5: boundary budget\n    ]\n\n    results = []\n\n    # Define the objective function to be minimized (-J)\n    def objective_function(x, lambda1, lambda2):\n        \"\"\"\n        Calculates the negative of the scalarized objective function J.\n        We minimize -J to maximize J.\n        \n        x: numpy array [alpha, beta, gamma]\n        lambda1: latency penalty weight\n        lambda2: memory penalty weight\n        \"\"\"\n        alpha, beta, gamma = x[0], x[1], x[2]\n        \n        # Guard against log(x) for x=0, though bounds should prevent this.\n        if alpha = 0 or beta = 0 or gamma = 0:\n            return np.inf\n\n        accuracy_term = s_d * np.log(alpha) + s_w * np.log(beta) + s_r * np.log(gamma)\n        \n        latency_term = k_L * alpha * beta**2 * gamma**2\n        \n        memory_term = k_P * alpha * beta**2 + k_A * alpha * beta * gamma**2\n        \n        penalty = lambda1 * latency_term + lambda2 * memory_term\n        \n        # Return negative J for minimization\n        return -(accuracy_term - penalty)\n\n    for case in test_cases:\n        lambda1, lambda2, B = case\n\n        # Define the inequality constraint: alpha * beta^2 * gamma^2 = B\n        # Scipy's SLSQP expects constraints in the form g(x) >= 0\n        constraint = {\n            'type': 'ineq',\n            'fun': lambda x: B - x[0] * x[1]**2 * x[2]**2\n        }\n\n        # Define the bounds for alpha, beta, gamma: 1 = x_i = U\n        bounds = ((1.0, U), (1.0, U), (1.0, U))\n\n        # Initial guess: the baseline model, which is always feasible.\n        x0 = np.array([1.0, 1.0, 1.0])\n\n        # Perform the constrained optimization\n        opt_result = minimize(\n            fun=objective_function,\n            x0=x0,\n            args=(lambda1, lambda2),\n            method='SLSQP',\n            bounds=bounds,\n            constraints=[constraint],\n            tol=1e-9\n        )\n\n        # Extract the optimal scaling factors\n        optimal_coeffs = opt_result.x\n\n        # Round the results to 4 decimal places\n        rounded_coeffs = np.round(optimal_coeffs, 4).tolist()\n        results.append(rounded_coeffs)\n    \n    # Format the final output string exactly as required\n    # e.g., [[1.0,1.0,1.0],[...]]\n    # Using str() on list elements is a clean way to achieve the desired formatting.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "3119675"}]}