{"hands_on_practices": [{"introduction": "训练生成对抗网络（GAN）的核心是一个在生成器和判别器之间的精妙“博弈”。一个常见的实践挑战是，其中一方可能会压倒另一方，导致训练不稳定或模式坍塌。这个练习 [@problem_id:3128933] 让你通过实验研究控制这种平衡最基本的参数之一：在生成器的每一步更新中，判别器所执行的更新次数 $k$，从而帮助你建立对 GAN 训练动态的直观理解。", "problem": "给定一个简化的生成对抗网络（GAN）实验，旨在分析每个生成器步骤交替进行 $k$ 次判别器步骤如何影响在一维高斯数据集上的训练稳定性。该实验基于第一性原理：生成器 $G$ 将标准正态噪声 $z \\sim \\mathcal{N}(0,1)$ 转换为 $x_g = a z + b$，判别器 $D$ 通过 $D(x) = \\sigma(w x + c)$ 估计输入为真实的概率，其中 $\\sigma$ 是 logistic S型函数。真实数据分布为 $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$。判别器最大化正确分类真实样本和伪造样本的期望对数似然，而生成器则基于判别器的输出最小化非饱和损失。\n\n从深度学习和概率论中的以下基本依据和核心定义出发：\n- 生成对抗网络（GAN）的目标是生成器和判别器之间的双人极小极大博弈，该博弈由数据和噪声分布上的期望值驱动。\n- 基于梯度的学习动态使用蒙特卡洛样本来近似这些期望，并应用梯度上升进行最大化和梯度下降进行最小化。\n- 对于线性生成器 $G(z) = a z + b$，其中 $z \\sim \\mathcal{N}(0,1)$，生成器导出的分布是均值为 $b$、标准差为 $|a|$ 的高斯分布。\n- logistic S型函数为 $\\sigma(s) = \\frac{1}{1 + e^{-s}}$，计算梯度所需的导数恒等式在统计学和机器学习中得到了充分检验。\n\n你的任务：\n1.  公式化交替梯度动态，其中判别器参数 $(w,c)$ 在其目标函数上通过梯度上升更新 $k$ 次，对应生成器参数 $(a,b)$ 在其非饱和目标上进行一次梯度下降更新。对两个参与者使用具有固定批量大小和固定学习率的蒙特卡洛估计。通过控制输入域来确保 $\\sigma$ 的数值稳定性。\n2.  定义训练稳定性的经验性概念。使用以下复合准则：令 $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$，其中 $\\mu_g(t) = b(t)$ 和 $\\sigma_g(t) = |a(t)|$ 分别是训练步骤 $t$ 时生成器的均值和标准差。对于一个很小的 $\\epsilon$，定义改进分数 $I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)}$，并定义振荡指数 $O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon}$，其中 $\\theta_g(t) = [a(t), b(t)]^\\top$。如果参数保持有限和有界，$I$ 超过一个最小阈值，且 $O$ 低于一个最大阈值，则将该次运行分类为稳定。\n3.  通过在一个简单数据集上使用多个 $k$ 值运行训练，经验性地推导出一个稳定性关于 $k$ 的相图，并报告每次运行是稳定还是不稳定，稳定编码为 $1$，不稳定编码为 $0$。\n\n使用以下数据集、训练和评估设置：\n- 真实数据参数 $(\\mu_r, \\sigma_r)$ 和调度参数 $k$ 因测试案例而异。\n- 噪声分布为 $z \\sim \\mathcal{N}(0,1)$。\n- 生成器为 $G(z) = a z + b$，初始化为 $a = 0.2$ 和 $b = 0.0$。\n- 判别器为 $D(x) = \\sigma(w x + c)$，其中 $(w,c)$ 初始化为 $(0.0, 0.0)$。\n- 判别器目标是真实样本上的 $\\log D(x)$ 和伪造样本上的 $\\log(1 - D(G(z)))$ 的期望和；判别器使用梯度上升。\n- 生成器目标是 $-\\log D(G(z))$ 的期望值（非饱和）；生成器使用梯度下降。\n- 使用学习率 $\\alpha_D = 0.05$ 和 $\\alpha_G = 0.02$，批量大小 $B = 1024$，以及 $T = 200$ 个生成器步骤。\n- 为保证数值稳定性，在应用 $\\sigma(s)$ 之前，将 sigmoid 输入 $s$ 裁剪到区间 $[-50, 50]$。\n\n定义稳定性分类阈值和保障措施：\n- 使用 $\\epsilon = 10^{-8}$ 进行分母稳定化。\n- 如果在任何时候任何参数的幅值超过 $100$ 或任何参数变为非数值 (not-a-number)，则声明为发散。\n- 使用阈值 $I_{\\min} = 0.25$ 和 $O_{\\max} = 2.5$。\n- 一次运行是稳定的，如果它不发散，$I \\ge I_{\\min}$，且 $O \\le O_{\\max}$。\n\n测试套件：\n- 案例 1: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 0, 42)$。\n- 案例 2: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 1, 42)$。\n- 案例 3: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (0.0, 1.0, 5, 42)$。\n- 案例 4: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (2.0, 0.5, 2, 123)$。\n- 案例 5: $(\\mu_r, \\sigma_r, k, \\text{seed}) = (-1.0, 1.5, 10, 7)$。\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是一个整数，$1$ 表示稳定运行，$0$ 表示不稳定运行。", "solution": "该问题要求对一个简化的生成对抗网络（GAN）的训练稳定性进行实证研究，将其作为每个生成器更新所对应的判别器更新次数 $k$ 的函数。这包括推导基于梯度的学习动态、实现模拟，并根据一组指定的稳定性准则评估结果。\n\n首先，我们对模型的各个组件进行形式化。真实数据分布是一个一维高斯分布，$x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$。生成器 $G$ 通过一个线性变换将标准正态噪声变量 $z \\sim \\mathcal{N}(0,1)$ 映射到一个样本 $x_g$：\n$$G(z) = a z + b$$\n生成器的参数为 $\\theta_G = (a, b)$。因此，由生成器导出的分布也是高斯分布，其均值为 $\\mu_g = b$，标准差为 $\\sigma_g = |a|$。判别器 $D$ 是一个逻辑回归器，它对输入 $x$ 来自真实数据分布的概率进行建模：\n$$D(x) = \\sigma(w x + c)$$\n其中 $\\sigma(s) = (1 + e^{-s})^{-1}$ 是 logistic S型函数。判别器的参数为 $\\theta_D = (w,c)$。\n\n训练过程是一个极小极大博弈。判别器的训练目标是最大化目标函数 $V_D$，即正确分类真实样本和伪造样本的对数似然之和：\n$$V_D(\\theta_D, \\theta_G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n生成器的训练目标是最小化非饱和目标函数 $V_G$，该函数旨在生成被判别器分类为真实的样本：\n$$V_G(\\theta_G) = -\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$$\n\n为了实现学习动态，我们推导这些目标函数相对于模型参数的梯度。期望值通过对一个大小为 $B$ 的批量进行蒙特卡洛估计来近似。\n\n判别器目标 $V_D$ 相对于其参数 $w$ 和 $c$ 的梯度是使用链式法则和 S型函数的导数性质（具体来说是 $\\frac{d}{ds}\\log \\sigma(s) = 1 - \\sigma(s)$ 和 $\\frac{d}{ds}\\log(1-\\sigma(s)) = -\\sigma(s)$）找到的。对于一批真实数据 $\\{x_i\\}_{i=1}^B$ 和生成数据 $\\{x_{g,i}\\}_{i=1}^B$，梯度为：\n$$ \\nabla_w \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i))x_i - D(x_{g,i})x_{g,i} \\right) $$\n$$ \\nabla_c \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i)) - D(x_{g,i}) \\right) $$\n判别器参数通过梯度上升进行更新：\n$$ w \\leftarrow w + \\alpha_D \\nabla_w \\hat{V}_D $$\n$$ c \\leftarrow c + \\alpha_D \\nabla_c \\hat{V}_D $$\n\n生成器的非饱和目标 $V_G$ 相对于其参数 $a$ 和 $b$ 的梯度为：\n$$ \\nabla_a V_G = \\frac{\\partial V_G}{\\partial a} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial a} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial a} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w z \\right] $$\n$$ \\nabla_b V_G = \\frac{\\partial V_G}{\\partial b} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial b} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial b} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w \\right] $$\n使用一批噪声 $\\{z_i\\}_{i=1}^B$ 进行近似，生成器参数通过梯度下降进行更新：\n$$ a \\leftarrow a - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i \\right) = a + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i $$\n$$ b \\leftarrow b - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) \\right) = b + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) $$\n\n完整的训练算法进行 $T$ 个生成器步骤。在每个步骤 $t \\in \\{1, \\dots, T\\}$ 中：\n1.  判别器更新 $k$ 次。对于每次判别器更新，都抽取新的一批真实数据 $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$ 和噪声 $z \\sim \\mathcal{N}(0,1)$。参数 $(w, c)$ 使用学习率 $\\alpha_D$ 通过梯度上升进行更新。\n2.  生成器更新一次。抽取新的一批噪声 $z \\sim \\mathcal{N}(0,1)$。参数 $(a, b)$ 使用学习率 $\\alpha_G$ 通过对非饱和损失进行梯度下降来更新。\n在整个模拟过程中，参数值受到监控。如果任何参数的幅值超过 $100$ 或变为非有限值（NaN），则运行终止并被归类为发散。S型函数的输入被裁剪到 $[-50, 50]$ 以防止数值溢出。\n\n完成 $T$ 步后，对该次训练运行进行稳定性评估。生成器的分布参数 $(\\mu_g(t)=b(t), \\sigma_g(t)=|a(t)|)$ 与目标真实数据参数 $(\\mu_r, \\sigma_r)$ 之间的距离定义为 $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$。计算两个指标：\n1.  改进分数 $I$，衡量从初始状态到最终状态的距离相对减少量：\n    $$ I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)} $$\n    其中 $d_0$ 和 $d_T$ 是步骤 $0$ 和 $T$ 时的距离，$\\epsilon = 10^{-8}$ 是一个用于数值稳定性的小常数。一次运行必须达到 $I \\ge I_{\\min} = 0.25$。\n2.  振荡指数 $O$，衡量生成器参数 $\\theta_g(t)=[a(t), b(t)]^\\top$ 的总路径长度与净位移的比率：\n    $$ O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon} $$\n    一次运行必须满足 $O \\le O_{\\max} = 2.5$。\n\n当且仅当一次运行不发散，达到 $I \\ge I_{\\min}$，且满足 $O \\le O_{\\max}$ 时，它被分类为稳定（输出 $1$）。否则，它是不稳定的（输出 $0$）。提供的实现为每个测试案例执行这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN stability analysis for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # (mu_r, sigma_r, k, seed)\n        (0.0, 1.0, 0, 42),\n        (0.0, 1.0, 1, 42),\n        (0.0, 1.0, 5, 42),\n        (2.0, 0.5, 2, 123),\n        (-1.0, 1.5, 10, 7)\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_r, sigma_r, k, seed = case\n        result = run_training_and_evaluate(mu_r, sigma_r, k, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef sigmoid(s):\n    \"\"\"\n    Computes the logistic sigmoid function with input clipping for numerical stability.\n    \"\"\"\n    s_clipped = np.clip(s, -50.0, 50.0)\n    return 1.0 / (1.0 + np.exp(-s_clipped))\n\ndef run_training_and_evaluate(mu_r, sigma_r, k, seed,\n                              a_init=0.2, b_init=0.0,\n                              w_init=0.0, c_init=0.0,\n                              alpha_D=0.05, alpha_G=0.02,\n                              B=1024, T=200,\n                              epsilon=1e-8, divergence_threshold=100.0,\n                              I_min=0.25, O_max=2.5):\n    \"\"\"\n    Simulates the GAN training for one parameter set and evaluates its stability.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Initialize parameters\n    a, b = a_init, b_init\n    w, c = w_init, c_init\n\n    # History for stability metrics\n    theta_g_history = [np.array([a, b])]\n    diverged = False\n\n    # Calculate initial distance to target distribution parameters\n    d_0 = np.sqrt((b - mu_r)**2 + (np.abs(a) - sigma_r)**2)\n\n    # Main training loop (T generator steps)\n    for _ in range(T):\n        # --- Discriminator updates (k steps) ---\n        if k > 0:\n            for _ in range(k):\n                # Sample data\n                x_r = np.random.normal(loc=mu_r, scale=sigma_r, size=B)\n                z = np.random.normal(loc=0.0, scale=1.0, size=B)\n                x_g = a * z + b\n\n                # Discriminator predictions\n                d_real = sigmoid(w * x_r + c)\n                d_fake = sigmoid(w * x_g + c)\n                \n                # Discriminator gradients (for maximizing log-likelihood)\n                grad_w = np.mean((1.0 - d_real) * x_r - d_fake * x_g)\n                grad_c = np.mean((1.0 - d_real) - d_fake)\n\n                # Update D parameters via gradient ascent\n                w += alpha_D * grad_w\n                c += alpha_D * grad_c\n\n                # Check for D divergence\n                if not (np.isfinite(w) and np.isfinite(c) and\n                        abs(w)  divergence_threshold and abs(c)  divergence_threshold):\n                    diverged = True\n                    break\n            if diverged:\n                break\n\n        # --- Generator update (1 step) ---\n        z_g = np.random.normal(loc=0.0, scale=1.0, size=B)\n        x_g_g = a * z_g + b\n        d_fake_for_g = sigmoid(w * x_g_g + c)\n\n        # Generator gradients (for minimizing non-saturating loss -log(D(G(z))))\n        grad_V_G_a = -np.mean((1.0 - d_fake_for_g) * w * z_g)\n        grad_V_G_b = -np.mean((1.0 - d_fake_for_g) * w)\n        \n        # Update G parameters via gradient descent\n        a -= alpha_G * grad_V_G_a\n        b -= alpha_G * grad_V_G_b\n\n        # Check for G divergence\n        if not (np.isfinite(a) and np.isfinite(b) and\n                abs(a)  divergence_threshold and abs(b)  divergence_threshold):\n            diverged = True\n            break\n            \n        theta_g_history.append(np.array([a, b]))\n\n    # --- Stability Evaluation ---\n    if diverged:\n        return 0\n\n    a_T, b_T = theta_g_history[-1]\n    \n    # Final distance\n    d_T = np.sqrt((b_T - mu_r)**2 + (np.abs(a_T) - sigma_r)**2)\n    \n    # Improvement Fraction (I)\n    I = (d_0 - d_T) / max(d_0, epsilon)\n\n    # Oscillation Index (O)\n    path_length = np.sum([np.linalg.norm(theta_g_history[t] - theta_g_history[t-1]) for t in range(1, T + 1)])\n    net_displacement = np.linalg.norm(theta_g_history[-1] - theta_g_history[0])\n    O = path_length / (net_displacement + epsilon)\n\n    # Classify as stable (1) or unstable (0)\n    if I >= I_min and O = O_max:\n        return 1\n    else:\n        return 0\n\nsolve()\n```", "id": "3128933"}, {"introduction": "虽然调整训练比率有所帮助，但通常需要更稳健的方法来确保 GAN 的稳定训练。Wasserstein GAN（WGAN）框架通过要求其判别器（或评论家）满足 1-Lipschitz 约束，为稳定性提供了理论基础。这项练习 [@problem_id:3124549] 邀请你分析和比较三种关键技术——权重裁剪、谱归一化和梯度惩罚——以理解它们在实践中如何强制执行这一关键数学属性，并影响整个网络的行为。", "problem": "考虑一个在瓦瑟斯坦生成对抗网络（WGAN）框架中训练的生成对抗网络（GAN）中的判别器（也称为评论家）$D$，其中要求 $D$ 满足 $1$-Lipschitz 约束。该网络是一个带有整流线性单元（ReLU）激活函数的三层前馈映射，\n$$\nD(x) \\;=\\; W_{3}\\,\\sigma\\!\\left(W_{2}\\,\\sigma\\!\\left(W_{1}\\,x\\right)\\right),\n$$\n其中 $\\sigma$ 表示 ReLU 激活函数，权重矩阵的形状分别为 $W_{1} \\in \\mathbb{R}^{128 \\times 64}$，$W_{2} \\in \\mathbb{R}^{64 \\times 128}$ 和 $W_{3} \\in \\mathbb{R}^{1 \\times 64}$。仅使用以下来自分析学和线性代数的基本事实：\n- 对于赋范空间之间的一個函数 $f$，如果对所有的 $x, y$ 都满足 $|f(x)-f(y)| \\leq L\\,\\|x-y\\|$，则称 $f$ 是 $L$-Lipschitz 的，而满足此条件的最小 $L$ 值就是 $f$ 的 Lipschitz 常数。\n- 如果 $h = g \\circ f$ 是两个分别为 $L_{f}$-Lipschitz 和 $L_{g}$-Lipschitz 的映射的复合，那么 $h$ 是 $L_{g}\\,L_{f}$-Lipschitz 的。\n- 对于欧几里得范数，由矩阵 $W$ 表示的线性映射的 Lipschitz 常数等于其算子范数（最大奇异值）$\\|W\\|_{2}$。\n- 整流线性单元（ReLU）激活函数 $\\sigma(z) = \\max\\{0,z\\}$ 是 $1$-Lipschitz 的。\n- 对于任意矩阵 $W \\in \\mathbb{R}^{m \\times n}$，若其元素满足 $|W_{ij}| \\leq c$，则其算子范数满足 $\\|W\\|_{2} \\leq \\|W\\|_{F} \\leq c\\,\\sqrt{mn}$，其中 $\\|W\\|_{F}$ 是弗罗贝尼乌斯范数（Frobenius norm）。\n\n你将分析三种用于实施 $1$-Lipschitz 约束的机制：\n\n(1) 权重裁剪：每次更新后，将每个 $W_{i}$ 的所有元素裁剪到 $[-c,c]$ 区间内，其中 $c = 0.01$。仅使用给定事实，构建 $D$ 在裁剪条件下的 Lipschitz 常数的上界。\n\n(2) 谱归一化：假设使用谱归一化，训练后的权重矩阵的算子范数（最大奇异值）测量结果为 $(\\|W_{1}\\|_{2}, \\|W_{2}\\|_{2}, \\|W_{3}\\|_{2}) = (0.98,\\,1.03,\\,0.99)$。仅使用给定事实，构建 $D$ 在谱归一化条件下的 Lipschitz 常数的上界。\n\n(3) 梯度惩罚：在带梯度惩罚的WGAN（WGAN-GP）中，惩罚项促使在真实样本和生成样本之间的连线上的点的梯度范数 $\\|\\nabla_{x} D(x)\\|_{2} \\approx 1$。在一批插值点 $\\{x_{t}\\}$ 上，观测到以下梯度范数：$\\|\\nabla_{x} D(x_{t})\\|_{2} \\in \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\}$。仅使用给定事实，用该批次中观测到的最大梯度范数来近似 Lipschitz 常数。\n\n按权重裁剪、谱归一化、梯度惩罚的顺序计算这三个得到的近似 Lipschitz 常数。将所有数值结果四舍五入到四位有效数字。将最终答案表示为一个包含这三个值的单行矩阵。", "solution": "该问题要求在三种不同的约束实施机制下，计算判别器网络 $D(x)$ 的 Lipschitz 常数的估计值。判别器是一个三层前馈网络，由下式给出：\n$$D(x) \\;=\\; W_{3}\\,\\sigma\\!\\left(W_{2}\\,\\sigma\\!\\left(W_{1}\\,x\\right)\\right)$$\n其中 $W_{1} \\in \\mathbb{R}^{128 \\times 64}$，$W_{2} \\in \\mathbb{R}^{64 \\times 128}$ 和 $W_{3} \\in \\mathbb{R}^{1 \\times 64}$ 是权重矩阵，$\\sigma$ 是整流线性单元（ReLU）激活函数。\n\n为了确定 $D(x)$ 的 Lipschitz 常数，我们将其建模为函数的复合。令 $f_1(x) = W_1 x$, $g_1(y_1) = \\sigma(y_1)$, $f_2(z_1) = W_2 z_1$, $g_2(y_2) = \\sigma(y_2)$ 以及 $f_3(z_2) = W_3 z_2$。那么判别器可以写成复合函数 $D = f_3 \\circ g_2 \\circ f_2 \\circ g_1 \\circ f_1$。\n\n根据所提供的事实：\n- 一个 $L_f$-Lipschitz 函数 $f$ 和一个 $L_g$-Lipschitz 函数 $g$ 的复合是 $(L_g L_f)$-Lipschitz 的。由此推广，$D$ 的 Lipschitz 常数（记为 $L_D$）的上界是其各组成函数的 Lipschitz 常数的乘积：\n$$L_D \\leq L_{f_3} \\cdot L_{g_2} \\cdot L_{f_2} \\cdot L_{g_1} \\cdot L_{f_1}$$\n- 函数 $f_i$ 是由矩阵 $W_i$ 表示的线性映射。它们关于欧几里得范数的 Lipschitz 常数是它们的算子范数，$L_{f_i} = \\|W_i\\|_2$。\n- 函数 $g_i$ 是 ReLU 激活函数 $\\sigma$。问题中指出 ReLU 是 $1$-Lipschitz 的，所以 $L_{g_1} = L_{g_2} = 1$。\n\n将这些代入不等式，我们得到判别器 Lipschitz 常数的一个上界：\n$$L_D \\leq \\|W_3\\|_2 \\cdot 1 \\cdot \\|W_2\\|_2 \\cdot 1 \\cdot \\|W_1\\|_2 = \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2$$\n这个不等式构成了分析前两种情况的基础。\n\n(1) **权重裁剪：**\n在这种情况下，每个权重矩阵 $W_i$ 的所有元素都被裁剪，使得 $|(W_i)_{jk}| \\leq c$，其中 $c = 0.01$。根据给定事实，对于矩阵 $W \\in \\mathbb{R}^{m \\times n}$，其算子范数的上界为 $\\|W\\|_2 \\leq c \\sqrt{mn}$。我们将这个界应用于每个权重矩阵。\n\n- 对于 $W_1 \\in \\mathbb{R}^{128 \\times 64}$：\n维度为 $m=128$ 和 $n=64$。其算子范数的上界为：\n$$\\|W_1\\|_2 \\leq c \\sqrt{128 \\times 64} = 0.01 \\sqrt{8192} = 0.01 \\sqrt{64^2 \\times 2} = 0.01 \\times 64\\sqrt{2} = 0.64\\sqrt{2}$$\n\n- 对于 $W_2 \\in \\mathbb{R}^{64 \\times 128}$：\n维度为 $m=64$ 和 $n=128$。其算子范数的上界为：\n$$\\|W_2\\|_2 \\leq c \\sqrt{64 \\times 128} = 0.01 \\sqrt{8192} = 0.64\\sqrt{2}$$\n\n- 对于 $W_3 \\in \\mathbb{R}^{1 \\times 64}$：\n维度为 $m=1$ 和 $n=64$。其算子范数的上界为：\n$$\\|W_3\\|_2 \\leq c \\sqrt{1 \\times 64} = 0.01 \\sqrt{64} = 0.01 \\times 8 = 0.08$$\n\nLipschitz 常数 $L_D$ 的上界是这些单个上界的乘积：\n$$L_D \\leq (0.64\\sqrt{2}) \\cdot (0.64\\sqrt{2}) \\cdot (0.08) = (0.64)^2 \\cdot (\\sqrt{2})^2 \\cdot 0.08 = 0.4096 \\cdot 2 \\cdot 0.08 = 0.8192 \\cdot 0.08 = 0.065536$$\n四舍五入到四位有效数字，近似的 Lipschitz 常数为 $0.06554$。\n\n(2) **谱归一化：**\n在这里，直接给出了训练后权重矩阵的算子范数：$\\|W_{1}\\|_{2} = 0.98$，$\\|W_{2}\\|_{2} = 1.03$ 和 $\\|W_{3}\\|_{2} = 0.99$。我们将这些值代入我们推导出的 $L_D$ 的不等式中：\n$$L_D \\leq \\|W_1\\|_2 \\|W_2\\|_2 \\|W_3\\|_2 = 0.98 \\times 1.03 \\times 0.99$$\n计算乘积：\n$$L_D \\leq 1.0094 \\times 0.99 = 0.999306$$\n四舍五入到四位有效数字，近似的 Lipschitz 常数为 $0.9993$。\n\n(3) **梯度惩罚：**\n对于像判别器 $D(x)$ 这样的可微函数，其 Lipschitz 常数 $L_D$ 等于其梯度范数在整个输入域上的上确界：$L_D = \\sup_x \\|\\nabla_x D(x)\\|_2$。梯度惩罚方法促使这个梯度范数接近 $1$。问题提供了一批点上观测到的梯度范数值：$\\|\\nabla_{x} D(x_{t})\\|_{2} \\in \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\}$。\n根据指示，我们用这个集合中观测到的最大梯度范数来近似 Lipschitz 常数。\n$$L_D \\approx \\max \\{0.92,\\,1.07,\\,1.01,\\,0.95,\\,1.10\\} = 1.10$$\n为了满足四位有效数字的要求，这表示为 $1.100$。\n\n三个得到的近似 Lipschitz 常数分别为 $0.06554$（权重裁剪）、$0.9993$（谱归一化）和 $1.100$（梯度惩罚）。", "answer": "$$\\boxed{\\begin{pmatrix} 0.06554  0.9993  1.100 \\end{pmatrix}}$$", "id": "3124549"}, {"introduction": "成功训练一个生成器后，我们如何定量地衡量其输出的质量和多样性？Fréchet Inception Distance (FID) 是一个为此目的被广泛采用的度量标准，它在一个深度特征空间中比较真实样本和生成样本的统计数据。这项练习 [@problem_id:3128911] 不仅引导你了解 FID 背后的数学基础，更重要的是，它挑战你去发现一个 FID 可能产生误导的场景，从而磨练你对评估指标的批判性思维。", "problem": "在训练生成对抗网络 (GAN) 时，一个广泛使用的评估指标是弗雷歇初始距离 (Fréchet Inception Distance, FID)，它在一个学习到的特征空间中比较真实数据和生成数据。设特征提取器是一个映射 $\\phi:\\mathcal{X}\\to\\mathbb{R}^d$，其中 $d\\in\\mathbb{N}$，并假设真实图像和生成图像的特征分布分别由多元正态分布近似，参数分别为 $(\\mu_r,\\Sigma_r)$ 和 $(\\mu_g,\\Sigma_g)$，其中 $\\mu_r,\\mu_g\\in\\mathbb{R}^d$，而 $\\Sigma_r,\\Sigma_g\\in\\mathbb{R}^{d\\times d}$ 是对称半正定矩阵。弗雷歇初始距离定义为这些高斯近似之间的平方2-瓦瑟斯坦距离，其中平方2-瓦瑟斯坦距离的定义为\n$$\nW_2^2(P,Q)\\;=\\;\\inf_{\\gamma\\in\\Pi(P,Q)}\\;\\mathbb{E}_{(X,Y)\\sim\\gamma}\\big[\\|X-Y\\|_2^2\\big],\n$$\n此处 $\\Pi(P,Q)$ 是所有在 $\\mathbb{R}^d$ 上以 $P$ 和 $Q$ 为边际分布的耦合的集合。\n\n任务 (a)：仅使用上面 $W_2^2$ 的定义、多元正态分布的性质以及关于对称半正定矩阵（包括主平方根）的基础线性代数知识，推导 $W_2^2\\big(\\mathcal{N}(\\mu_r,\\Sigma_r),\\mathcal{N}(\\mu_g,\\Sigma_g)\\big)$ 关于 $\\mu_r,\\mu_g,\\Sigma_r,\\Sigma_g$ 的闭式表达式。\n\n任务 (b)：构建一个简单的特征空间，在该空间中弗雷歇初始距离可能具有误导性。考虑一维情况 $d=1$，图像空间为 $\\mathcal{X}=\\{-1,1\\}$，特征提取器 $\\phi:\\mathcal{X}\\to\\mathbb{R}$ 定义为 $\\phi(x)=|x|$。设真实数据分布在 $-1$ 和 $1$ 上的概率各为 $1/2$，并设生成器以概率 $1$ 确定性地输出 $1$。计算特征空间中得到的高斯参数 $(\\mu_r,\\Sigma_r)$ 和 $(\\mu_g,\\Sigma_g)$，然后对这些参数评估你在(a)部分推导出的平方弗雷歇初始距离。将最终值报告为单个精确实数。无需四舍五入。", "solution": "该问题包含两部分。部分(a)要求推导两个多元正态分布之间平方2-瓦瑟斯坦距离的闭式表达式。部分(b)要求将此公式应用于一个特定的一维示例，以展示弗雷歇初始距离 (FID) 的一个潜在缺陷。\n\n**部分(a)：两个高斯分布之间平方2-瓦瑟斯坦距离的推导**\n\n设两个多元正态分布为 $P = \\mathcal{N}(\\mu_r, \\Sigma_r)$ 和 $Q = \\mathcal{N}(\\mu_g, \\Sigma_g)$，其中 $\\mu_r, \\mu_g \\in \\mathbb{R}^d$ 是均值，$\\Sigma_r, \\Sigma_g \\in \\mathbb{R}^{d \\times d}$ 是对称半正定协方差矩阵。设 $X \\sim P$ 和 $Y \\sim Q$。\n\n平方2-瓦瑟斯坦距离定义为：\n$$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\mathbb{E}_{(X,Y) \\sim \\gamma} \\left[ \\|X-Y\\|_2^2 \\right] $$\n其中 $\\Pi(P, Q)$ 是在 $\\mathbb{R}^d \\times \\mathbb{R}^d$ 上所有以 $P$ 和 $Q$ 为边际分布的联合分布（耦合）$\\gamma$ 的集合。\n\n让我们将随机向量分解为其均值和零均值分量：$X = \\mu_r + X_0$ 和 $Y = \\mu_g + Y_0$，其中 $X_0 \\sim \\mathcal{N}(0, \\Sigma_r)$ 和 $Y_0 \\sim \\mathcal{N}(0, \\Sigma_g)$。期望项可以展开为：\n$$ \\|X-Y\\|_2^2 = \\|(\\mu_r - \\mu_g) + (X_0 - Y_0)\\|_2^2 = \\|\\mu_r - \\mu_g\\|_2^2 + \\|X_0 - Y_0\\|_2^2 + 2(\\mu_r - \\mu_g)^T(X_0 - Y_0) $$\n对任意耦合 $\\gamma$ 取期望：\n$$ \\mathbb{E}_{(X,Y)\\sim\\gamma}\\left[\\|X-Y\\|_2^2\\right] = \\|\\mu_r - \\mu_g\\|_2^2 + \\mathbb{E}_{(X_0,Y_0)\\sim\\gamma'}\\left[\\|X_0 - Y_0\\|_2^2\\right] + 2(\\mu_r - \\mu_g)^T(\\mathbb{E}[X_0] - \\mathbb{E}[Y_0]) $$\n由于 $X_0$ 和 $Y_0$ 是零均值的，第三项为 $2(\\mu_r - \\mu_g)^T(0 - 0) = 0$。$(X,Y)$ 的耦合为零均值分量 $(X_0, Y_0)$ 引入了一个耦合 $\\gamma'$，其边际分布为 $\\mathcal{N}(0, \\Sigma_r)$ 和 $\\mathcal{N}(0, \\Sigma_g)$。项 $\\|\\mu_r - \\mu_g\\|_2^2$ 相对于耦合的选择是恒定的。因此，下确界仅作用于第二项：\n$$ W_2^2(P,Q) = \\|\\mu_r - \\mu_g\\|_2^2 + \\inf_{\\gamma'} \\mathbb{E}_{(X_0,Y_0)\\sim\\gamma'}\\left[\\|X_0 - Y_0\\|_2^2\\right] $$\n让我们分析下确界内的项：\n$$ \\mathbb{E}\\left[\\|X_0 - Y_0\\|_2^2\\right] = \\mathbb{E}[X_0^T X_0] - 2\\mathbb{E}[X_0^T Y_0] + \\mathbb{E}[Y_0^T Y_0] $$\n对于一个协方差为 $\\Sigma$ 的零均值随机向量 $Z$，我们有 $\\mathbb{E}[Z^T Z] = \\mathbb{E}[\\text{Tr}(ZZ^T)] = \\text{Tr}(\\mathbb{E}[ZZ^T]) = \\text{Tr}(\\Sigma)$。\n所以，$\\mathbb{E}[X_0^T X_0] = \\text{Tr}(\\Sigma_r)$ 且 $\\mathbb{E}[Y_0^T Y_0] = \\text{Tr}(\\Sigma_g)$。这些项与耦合无关。问题简化为：\n$$ \\inf_{\\gamma'} \\mathbb{E}\\left[\\|X_0 - Y_0\\|_2^2\\right] = \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\sup_{\\gamma'} \\mathbb{E}[X_0^T Y_0] $$\n对于高斯测度，最优传输方案（即达到上确界的耦合）是由一个线性变换所导出的。我们寻找一个矩阵 $A$，使得如果 $X_0 \\sim \\mathcal{N}(0, \\Sigma_r)$，那么设置 $Y_0 = AX_0$ 会导致 $Y_0$ 具有正确的边际分布 $\\mathcal{N}(0, \\Sigma_g)$。$Y_0$ 的协方差是 $\\text{Cov}(AX_0) = A \\text{Cov}(X_0) A^T = A\\Sigma_rA^T$。因此我们需要 $A\\Sigma_rA^T = \\Sigma_g$。\n需要最大化的项是 $\\mathbb{E}[X_0^T Y_0] = \\mathbb{E}[X_0^T A X_0]$。使用零均值向量 $Z$ 的迹性质 $\\mathbb{E}[Z^T M Z] = \\text{Tr}(M \\text{Cov}(Z))$，我们得到：\n$$ \\mathbb{E}[X_0^T A X_0] = \\text{Tr}(A \\Sigma_r) $$\n所以，我们必须求解 $\\sup_{A: A\\Sigma_rA^T=\\Sigma_g} \\text{Tr}(A\\Sigma_r)$。\n暂时假设 $\\Sigma_r$ 是可逆的。那么 $\\Sigma_r^{1/2}$ 是其唯一的对称正定平方根。设 $B = A \\Sigma_r^{1/2}$，所以 $A = B \\Sigma_r^{-1/2}$。约束条件变为 $B \\Sigma_r^{-1/2} \\Sigma_r (\\Sigma_r^{-1/2})^T B^T = BB^T = \\Sigma_g$。我们需要最大化 $\\text{Tr}(B\\Sigma_r^{-1/2}\\Sigma_r) = \\text{Tr}(B\\Sigma_r^{1/2})$。\n任何满足 $BB^T = \\Sigma_g$ 的矩阵 $B$ 都可以写成 $B = \\Sigma_g^{1/2}O$ 的形式，其中 $O$ 是一个正交矩阵。我们想要最大化 $\\text{Tr}(\\Sigma_g^{1/2} O \\Sigma_r^{1/2}) = \\text{Tr}(O \\Sigma_r^{1/2} \\Sigma_g^{1/2})$。\n设 $M = \\Sigma_r^{1/2} \\Sigma_g^{1/2}$。设其奇异值分解为 $M = USV^T$。我们想要最大化 $\\text{Tr}(O U S V^T) = \\text{Tr}(V^T O U S)$。矩阵 $O' = V^T O U$ 是正交的。当 $O'=I$ 时，表达式取最大值，最大值为 $\\text{Tr}(S)$。$S$ 中的奇异值是 $M^T M = (\\Sigma_g^{1/2}\\Sigma_r^{1/2})(\\Sigma_r^{1/2}\\Sigma_g^{1/2}) = \\Sigma_g^{1/2} \\Sigma_r \\Sigma_g^{1/2}$ 的特征值的平方根。或者说，它们是 $MM^T = \\Sigma_r^{1/2}\\Sigma_g\\Sigma_r^{1/2}$ 的特征值的平方根。这些奇异值组成的对角矩阵的迹是 $\\text{Tr}(S) = \\text{Tr}\\left(\\left( \\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} \\right)^{1/2}\\right)$。通过连续性论证，这个结果即使对于奇异的 $\\Sigma_r$ 也成立。\n\n综合所有部分：\n$$ \\sup_{\\gamma'} \\mathbb{E}[X_0^T Y_0] = \\text{Tr}\\left(\\left( \\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} \\right)^{1/2}\\right) $$\n所以，平方2-瓦瑟斯坦距离的完整表达式是：\n$$ W_2^2(\\mathcal{N}(\\mu_r,\\Sigma_r), \\mathcal{N}(\\mu_g,\\Sigma_g)) = \\|\\mu_r - \\mu_g\\|_2^2 + \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\text{Tr}\\left( \\left(\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2}\\right)^{1/2} \\right) $$\n\n**部分(b)：特定示例的计算**\n\n给定一个一维情况 ($d=1$)：\n- 图像空间 $\\mathcal{X} = \\{-1, 1\\}$。\n- 特征提取器 $\\phi: \\mathcal{X} \\to \\mathbb{R}$ 定义为 $\\phi(x) = |x|$。\n- 真实数据分布 $P_r$：$P_r(X=-1) = \\frac{1}{2}$，$P_r(X=1) = \\frac{1}{2}$。\n- 生成的数据分布 $P_g$：确定性地输出 $1$。\n\n首先，我们计算特征分布及其参数 $(\\mu, \\Sigma)$。由于 $d=1$，$\\Sigma$ 是一个表示方差的 $1 \\times 1$ 矩阵，即 $\\Sigma = [\\sigma^2]$。\n\n**真实数据特征：**\n真实数据点 $x_r$ 可以是 $-1$ 或 $1$。对应的特征值是 $z_r = \\phi(x_r) = |x_r|$。\n- 如果 $x_r = -1$，$z_r = |-1| = 1$。\n- 如果 $x_r = 1$，$z_r = |1| = 1$。\n真实特征的分布是确定性的：它总是 $1$。\n真实特征的均值是 $\\mu_r = \\mathbb{E}[Z_r] = \\frac{1}{2} \\cdot \\phi(-1) + \\frac{1}{2} \\cdot \\phi(1) = \\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot 1 = 1$。\n真实特征的方差是 $\\sigma_r^2 = \\mathbb{E}[(Z_r - \\mu_r)^2] = \\mathbb{E}[(1-1)^2] = 0$。\n所以，$\\mu_r = 1$ 且 $\\Sigma_r = [0]$。\n\n**生成数据特征：**\n生成器确定性地输出 $x_g=1$。对应的特征是 $z_g = \\phi(1) = |1| = 1$。\n生成特征的分布也是确定性的：它总是 $1$。\n生成特征的均值是 $\\mu_g = \\mathbb{E}[Z_g] = 1$。\n生成特征的方差是 $\\sigma_g^2 = \\mathbb{E}[(Z_g - \\mu_g)^2] = \\mathbb{E}[(1-1)^2] = 0$。\n所以，$\\mu_g = 1$ 且 $\\Sigma_g = [0]$。\n\n现在，我们使用(a)部分的公式计算平方FID。\n$$ \\text{FID}^2 = W_2^2 = \\|\\mu_r - \\mu_g\\|_2^2 + \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\text{Tr}\\left( \\left(\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2}\\right)^{1/2} \\right) $$\n代入参数：\n- $\\mu_r = 1$, $\\mu_g = 1$。\n- $\\Sigma_r = [0]$, $\\Sigma_g = [0]$。\n\n各项分别为：\n1. 均值之差：$\\|\\mu_r - \\mu_g\\|_2^2 = (1-1)^2 = 0$。\n2. $\\Sigma_r$ 的迹：$\\text{Tr}(\\Sigma_r) = \\text{Tr}([0]) = 0$。\n3. $\\Sigma_g$ 的迹：$\\text{Tr}(\\Sigma_g) = \\text{Tr}([0]) = 0$。\n4. 协方差交叉项：\n   - $\\Sigma_r^{1/2} = [0]^{1/2} = [0]$。\n   - 括号内的乘积是 $\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} = [0][0][0] = [0]$。\n   - 它的主平方根是 $([0])^{1/2} = [0]$。\n   - 迹是 $\\text{Tr}([0]) = 0$。\n   - 整个项是 $-2 \\times 0 = 0$。\n\n各项求和：\n$$ \\text{FID}^2 = 0 + 0 + 0 - 0 = 0 $$\n得到的平方弗雷歇初始距离为 $0$。这个结果是“有误导性的”，因为真实数据分布覆盖了两种模式（$-1$ 和 $1$），而生成器坍缩到单一模式（$1$）。非单射的特征映射 $\\phi(x)=|x|$ 将真实数据的两种模式映射到相同的特征表示，使得真实和生成的特征分布完全相同。因此，尽管生成器存在严重的模式坍塌，FID 仍然报告了一个为 $0$ 的完美分数。", "answer": "$$\\boxed{0}$$", "id": "3128911"}]}