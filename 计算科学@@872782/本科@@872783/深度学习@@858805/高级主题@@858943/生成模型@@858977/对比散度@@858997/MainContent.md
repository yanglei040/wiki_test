## 引言
[受限玻尔兹曼机](@entry_id:636627)（RBM）等[基于能量的模型](@entry_id:636419)（EBM）因其强大的[表达能力](@entry_id:149863)在机器学习中备受关注，但其训练过程却面临一个核心挑战：难以计算的[配分函数](@entry_id:193625)（partition function），这使得精确的最大似然学习在计算上变得不可行。那么，我们如何才能有效地训练这些模型，释放它们的潜力呢？对比散度（Contrastive Divergence, CD）算法正是为解决这一难题而提出的优雅且实用的近似方法。本文将系统地剖析对比散度算法，带领读者从理论基础走向实践应用。在接下来的内容中，我们将首先在“原理与机制”一章中深入探讨CD的理论核心，揭示其梯度更新的来源、雕刻能量[曲面](@entry_id:267450)的几何直觉，并分析其作为有偏估计的微妙之处。随后，在“应用与跨学科联系”一章中，我们将视野拓宽至实际应用，考察CD如何在推荐系统和序列建模中发挥作用，并探索其与计算物理、[强化学习](@entry_id:141144)等领域的深刻理论类比。最后，通过“动手实践”环节，读者将有机会通过具体的编程练习将所学知识付诸实践，从而真正掌握这一强大的训练算法。

## 原理与机制

在上一章中，我们介绍了[受限玻尔兹曼机](@entry_id:636627)（RBM）作为一种[基于能量的模型](@entry_id:636419)的基本结构。本章将深入探讨其核心的学习算法——对比散度（Contrastive Divergence, CD）的原理与机制。我们将从其理论基础出发，逐步揭示其几何直觉、近似的本质，并探讨其作为一种有偏估计的微妙之处及其对模型训练的实际影响。

### 从[对数似然](@entry_id:273783)梯度说起

训练一个像RBM这样的能量模型，其根本目标是调整模型参数 $\theta = \{W, b, c\}$，以最大化模型赋予训练数据的（对数）似然。对于单个数据样本 $\mathbf{v}$，其对数似然为 $\ln p(\mathbf{v})$。根据能量模型的定义，$p(\mathbf{v}) = \frac{1}{Z} \sum_{\mathbf{h}} \exp(-E(\mathbf{v}, \mathbf{h}))$，其中 $Z$ 是[配分函数](@entry_id:193625)。[对数似然](@entry_id:273783)的梯度是执行梯度上升以优化参数的关键：

$$
\frac{\partial \ln p(\mathbf{v})}{\partial \theta} = -\frac{\partial F(\mathbf{v})}{\partial \theta} - \frac{\partial \ln Z}{\partial \theta}
$$

这里，$F(\mathbf{v}) = -\ln \sum_{\mathbf{h}} \exp(-E(\mathbf{v}, \mathbf{h}))$ 是可见配置 $\mathbf{v}$ 的**自由能 (free energy)**。经过一系列推导，可以证明对数似然关于权重矩阵 $W$ 的梯度具有一种优雅而深刻的结构，它表现为两个[期望值](@entry_id:153208)之差 [@problem_id:3109775]：

$$
\frac{\partial \ln p(\mathbf{v})}{\partial W_{ij}} = \mathbb{E}_{p(\mathbf{h}|\mathbf{v})}[v_i h_j] - \mathbb{E}_{p(\mathbf{v}, \mathbf{h})}[v_i h_j]
$$

在对整个数据集进行平均后，权重更新的规则 $\Delta W$ 正比于：

$$
\Delta W \propto \langle \mathbf{v}\mathbf{h}^\top \rangle_{\text{data}} - \langle \mathbf{v}\mathbf{h}^\top \rangle_{\text{model}}
$$

这个梯度结构包含两个关键部分：

1.  **正向阶段 (positive phase)**：由项 $\langle \mathbf{v}\mathbf{h}^\top \rangle_{\text{data}}$ 表示。此项是在数据[分布](@entry_id:182848) $p_{\text{data}}$ 和给定数据时[模型推断](@entry_id:636556)的[隐变量](@entry_id:150146)条件分布 $p(\mathbf{h}|\mathbf{v})$ 下计算的期望。具体来说，当一个可见单元 $v_i$ 和一个隐单元 $h_j$ 在给定一个真实数据样本 $\mathbf{v}$ 时倾向于同时激活，该项会促使它们之间的权重 $W_{ij}$ 增加。这是一种经典的 **赫布式学习 (Hebbian learning)** 形式，即“一起发放的神经元连接在一起”（neurons that fire together, wire together）。它的作用是让模型更好地捕捉和编码数据中存在的[统计相关性](@entry_id:267552)。

2.  **负向阶段 (negative phase)**：由项 $\langle \mathbf{v}\mathbf{h}^\top \rangle_{\text{model}}$ 表示。此项是在模型自身的联合分布 $p(\mathbf{v}, \mathbf{h})$ 下计算的期望。它代表了模型在没有任何外部数据“钳制”的情况下，其内部动态自由演化时产生的相关性。学习规则从此项中减去这部分，意味着当模型自身倾向于产生某种 $v_i$ 和 $h_j$ 的共激活模式时，相应的权重 $W_{ij}$ 会被减小。这可以被看作是一种 **反赫布式学习 (anti-Hebbian learning)** 或[突触抑制](@entry_id:194987) [@problem_id:3109775]。其作用是防止模型陷入自激的“幻想”状态，即不断增强那些仅仅由模型内部结构而非真实数据驱动的相关性。这在特征之间引入了一种竞争机制，迫使模型将有限的建模能力用于解释真实数据中独特的、外部驱动的结构。

### 几何直觉：雕刻能量[曲面](@entry_id:267450)

为了更直观地理解学习过程，我们可以引入RBM的自由能 $F(\mathbf{v})$。对于一个给定的可见向量 $\mathbf{v}$，其自由能可以通过对所有可能的隐单元配置求和来解析地计算 [@problem_id:3109703]：

$$
F(\mathbf{v}) = - \mathbf{b}^\top \mathbf{v} - \sum_{j=1}^{n_h} \ln\left(1 + \exp(c_j + \mathbf{v}^\top \mathbf{W}_j)\right)
$$

其中 $\mathbf{W}_j$ 是权重矩阵 $W$ 的第 $j$ 列。由于 $p(\mathbf{v}) \propto \exp(-F(\mathbf{v}))$，自由能 $F(\mathbf{v})$ 在整个可见[状态空间](@entry_id:177074)上定义了一个**能量[曲面](@entry_id:267450) (energy landscape)**。能量低的地方对应概率高的区域。因此，最大化数据[似然](@entry_id:167119)的目标等价于降低训练数据点所在位置的自由能，同时确保[配分函数](@entry_id:193625) $Z$（即所有状态的 $\exp(-F(\mathbf{v}))$ 的总和）不会无限增长。

从这个几何视角看，学习过程就像是在“雕刻”这个能量[曲面](@entry_id:267450)：

-   **正向阶段** 的作用是在每个数据点 $\mathbf{v}_{\text{data}}$ 的位置“向下挖掘”，降低其自由能，从而提高其概率。

-   **负向阶段** 则在模型自身倾向于生成样本（即能量较低的区域）的位置“向上推起”，提高这些区域的能量。这可以防止能量[曲面](@entry_id:267450)变得过于平坦或在数据点周围形成过于尖锐的“陷阱”，而忽略了空间的其他部分，这会导致[配分函数](@entry_id:193625) $Z$ 爆炸，使[模型泛化](@entry_id:174365)能力差。

这个“雕刻”过程的净效应是，模型试图塑造一个能量[曲面](@entry_id:267450)，使其在数据点所在区域[形成能](@entry_id:142642)量低谷（高概率模式），而在其他区域保持较高的能量（低概率）。一个具体的计算实验可以验证这一直觉 [@problem_id:3109724]。在一个简单的RBM上执行单步CD更新后，可以精确计算所有可见状态在更新前后的自由能变化 $\Delta F(\mathbf{v})$。实验结果清晰地表明，对于训练数据点集合 $\mathcal{D}$，其平均自由能变化 $\langle \Delta F \rangle_{\mathcal{D}}$ 是负的；而对于非数据点集合 $\mathcal{D}^c$，其平均自由能变化 $\langle \Delta F \rangle_{\mathcal{D}^c}$ 则是正的。这精确地展示了CD算法如何通过一次更新就在数据点处压低能量，同时在其他地方抬高能量。

### 对比散度的核心近似

理论上，计算负向阶段需要从模型的联合分布 $p(\mathbf{v}, \mathbf{h})$ 中采样，这是一个极具挑战性的任务。对于大多数非平凡的模型，由于[状态空间](@entry_id:177074)巨大，精确计算[配分函数](@entry_id:193625) $Z$ 和从中采样都是不可行的。这正是**对比散度 (Contrastive Divergence, CD)** 算法发挥作用的地方。

CD算法的核心思想是对负向阶段进行一个巧妙的近似。它没有试图从已经[达到平衡](@entry_id:170346)态的模型[分布](@entry_id:182848) $p(\mathbf{v})$ 中采样，而是：

1.  从一个训练数据点 $\mathbf{v}^{(0)}$ 开始。
2.  运行一个短时长的马尔可夫链蒙特卡洛（MCMC）过程，通常是**块[吉布斯采样](@entry_id:139152) (block Gibbs sampling)**，持续 $k$ 个完整步骤。一个完整步骤包括从 $p(\mathbf{h}|\mathbf{v}^{(t)})$ 中采样得到 $\mathbf{h}^{(t)}$，再从 $p(\mathbf{v}|\mathbf{h}^{(t)})$ 中采样得到 $\mathbf{v}^{(t+1)}$。
3.  在 $k$ 步之后，得到一个样本 $\mathbf{v}^{(k)}$（有时称为“负粒子”或“幻想粒子”）。
4.  使用这个（或这些）样本来估计负向阶段的[期望值](@entry_id:153208)。最常见的算法是 CD-$k$，其中 $k$ 通常是一个很小的整数，比如1。

这个过程用从数据点开始的短时运行链的末端样本，替代了从完全混合的、处于平衡态的链中获取的样本。直观上，如果 $k$ 足够大，$\mathbf{v}^{(k)}$ 的[分布](@entry_id:182848)将接近模型[分布](@entry_id:182848) $p(\mathbf{v})$。但CD的惊人之处在于，即使 $k$ 非常小（例如 $k=1$），它在实践中也常常有效。

### 对比散度中的偏差

CD算法的有效性背后隐藏着一个重要的理论事实：它并非真实[对数似然](@entry_id:273783)梯度的[无偏估计](@entry_id:756289)，而是一个**有偏估计 (biased estimator)**。理解这种偏差的来源和性质，对于深刻掌握CD至关重要。

#### 偏差的来源：非[平衡态](@entry_id:168134)采样

偏差的根源在于CD-$k$使用的样本并非来自模型的目标平稳分布 $p_{\text{model}}$，而是来自一个从数据[分布](@entry_id:182848) $p_{\text{data}}$ 开始、经过 $k$ 步马尔可夫链演化后的非[平衡分布](@entry_id:263943) $q_k$。当 $k$ 是一个小的有限值时，$q_k$ 与 $p_{\text{model}}$ 之间存在差异，这导致了梯度的偏差。

我们可以通过一个“详细平衡违背度”的度量来量化这个$k$步链离[平衡态](@entry_id:168134)有多远 [@problem_id:3109737]。对于一个马尔可夫核 $K$ 和一个[分布](@entry_id:182848) $\nu$，其违背度 $\mathcal{V}(\nu, K) = \frac{1}{2} \sum_{x,y} |\nu(x)K(x \to y) - \nu(y)K(y \to x)|$ 衡量了从[分布](@entry_id:182848) $\nu$ 出发的概率流的不对称性。只有当 $\nu$ 是 $K$ 的平稳可逆[分布](@entry_id:182848)时，该值为零。在CD-$k$中，$\nu_k = q_k$ 通常不是平稳分布，因此 $\mathcal{V}(q_k, K) > 0$，这直接导致了对负向统计量的估计偏差。

#### 偏差的数学形式

从更形式化的角度看，CD算法可以被理解为在优化一个替代的[目标函数](@entry_id:267263) [@problem_id:3109730]。这个替代目标是两个[KL散度](@entry_id:140001)之差：

$$
J_k(\theta) = \mathrm{KL}(p_{\text{data}} \| p_{\theta}) - \mathrm{KL}(q_{k,\theta} \| p_{\theta})
$$

其中 $q_{k,\theta}$ 是从数据[分布](@entry_id:182848)开始、通过参数为 $\theta$ 的[马尔可夫链](@entry_id:150828)演化 $k$ 步后得到的[分布](@entry_id:182848)。CD-$k$ 的更新步骤恰好是 $J_k(\theta)$ 梯度的一部分，但它忽略了 $q_{k,\theta}$ 对参数 $\theta$ 的依赖性（即忽略了对马尔可夫转移算子本身求导的项）。正是这个被忽略的项构成了偏差。

为了让这个抽象的偏差变得具体，我们可以分析一个极简的双状态玩具模型 [@problem_id:3109761]。在这个模型中，我们可以解析地推导出CD-1[梯度估计](@entry_id:164549)的偏差 $b(\theta)$ 的精确表达式。例如，对于一个能量函数为 $E_{\theta}(1)=\theta, E_{\theta}(0)=0$ 的模型，其偏差为：

$$
b(\theta) = \frac{\rho + (\rho-1)\exp(-\theta)}{1+\exp(\theta)}
$$

其中 $\rho$ 是数据[分布](@entry_id:182848)中状态1的概率。这个公式明确显示了偏差是如何依赖于模型参数 $\theta$ 和数据[分布](@entry_id:182848) $\rho$ 的。它揭示了CD梯度与真实梯度之间的系统性差异。此外，对偏差来源的分析表明，初始化[分布](@entry_id:182848)的选择（例如，从数据点开始 vs. 从随机点开始）会影响偏差的大小，而调节采样过程的温度（tempering）也是一种可以改变偏差特性的技术 [@problem_id:3109768]。

### [混合时间](@entry_id:262374)、谱隙与 $k$ 值的选择

尽管CD是一个有偏估计，但它在实践中往往能成功训练出强大的模型。这引出了一个关键问题：这种偏差的影响有多大？以及我们应该如何选择步数 $k$？

答案与[马尔可夫链](@entry_id:150828)的**[混合时间](@entry_id:262374) (mixing time)** 密切相关。[混合时间](@entry_id:262374)衡量了马尔可夫链从任意初始[分布](@entry_id:182848)收敛到其[平稳分布](@entry_id:194199)的速度。混合慢的链需要更长的时间来“忘记”其起点（即数据点），从而需要更大的 $k$ 才能产生接近模型真实[分布](@entry_id:182848)的样本。

马尔可夫链混合速度的一个严格度量是其[转移矩阵](@entry_id:145510) $P$ 的**[谱隙](@entry_id:144877) (spectral gap)** $\gamma$ [@problem_id:3109707]。[谱隙](@entry_id:144877)定义为 $1 - |\lambda_2|$，其中 $\lambda_2$ 是 $P$ 的第二大[特征值](@entry_id:154894)的模。谱隙越大，链的混合速度越快。通过在一个小型RBM上进行精确的数值实验，可以发现CD-$k$估计的偏差大小与[谱隙](@entry_id:144877)之间存在很强的负相关关系。也就是说，对于混合得更快的模型（谱隙更大），在固定的 $k$ 步之后，其CD估计的偏差会更小。

模型的[混合时间](@entry_id:262374)（以及[谱隙](@entry_id:144877)）很大程度上取决于其能量[曲面](@entry_id:267450)的形状 [@problem_id:3109723]。
-   如果能量[曲面](@entry_id:267450)相对**平坦且只有一个浅的能量盆地**，[吉布斯采样](@entry_id:139152)链可以快速地探索整个[状态空间](@entry_id:177074)。在这种情况下，[混合时间](@entry_id:262374)很短，即使是 CD-1 ($k=1$) 也可能提供一个足够好的梯度近似。
-   相反，如果能量[曲面](@entry_id:267450)具有**多个深的、被高能量壁垒隔开的能量盆地**（即多模态[分布](@entry_id:182848)），链可能会被困在某个模式中很长时间。从一个数据点（可能位于一个模式中）开始的短时链将很难生成代表其他模式的样本。在这种情况下，[混合时间](@entry_id:262374)很长，需要更大的 $k$ 值（如CD-5, CD-10或更高）才能让链有机会跨越能量壁垒，从而获得对整个模型[分布](@entry_id:182848)更具[代表性](@entry_id:204613)的负样本，减小[梯度估计](@entry_id:164549)的偏差。

因此，选择 $k$ 值涉及在一个权衡中做出抉择：较小的 $k$ 计算成本低但可能带来较大的偏差，尤其是在模型学习到多模态[分布](@entry_id:182848)时；较大的 $k$ 偏差较小但计算成本更高。在实践中，CD-1 因其高效性而被广泛使用，并被证明足以训练出能够捕捉数据显著特征的RBM模型。然而，当需要模型生成高质量、多样化的样本，或者当数据[分布](@entry_id:182848)本身高度复杂时，增加 $k$ 的值通常是必要的。