{"hands_on_practices": [{"introduction": "扩散模型的核心思想是逆转一个逐渐增加噪声的过程。这个练习提供了一个宝贵的机会，让我们在一个简化的场景中，通过解析方法精确计算出真实的逆向步骤。通过将这个真实的、非线性的结果与最佳的线性近似进行比较，我们将深刻理解为何需要像神经网络这样强大的非线性函数逼近器来构建高质量的扩散模型。[@problem_id:3116015]", "problem": "考虑一个一维扩散概率模型 (DPM)，其单步前向过程由随机过程 $x_{1} = \\sqrt{\\alpha}\\, x_{0} + \\epsilon$ 定义，其中 $\\epsilon \\sim \\mathcal{N}(0, \\beta)$ 独立于 $x_{0}$，且 $\\alpha = 1 - \\beta$。数据分布 $p_{\\text{data}}$ 是一个具有相等权重的对称双分量高斯混合分布：$x_{0} \\sim \\frac{1}{2}\\,\\mathcal{N}(-m, \\sigma_{0}^{2}) + \\frac{1}{2}\\,\\mathcal{N}(m, \\sigma_{0}^{2})$。在这个最小示例中，取 $m = 1$，$\\sigma_{0}^{2} = 1$ 以及 $\\beta = \\frac{1}{2}$ (因此 $\\alpha = \\frac{1}{2}$)。\n仅使用贝叶斯法则、线性高斯共轭性和混合分布的性质：\n1) 在 $p_{\\text{data}}$ 下，推导真实逆向步骤后验均值 $\\mathbb{E}[x_{0} \\mid x_{1}]$ 的解析表达式。\n2) 假设一个线性预测器 $\\hat{x}_{0} = a\\, x_{1}$ 经过训练，以最小化由前向过程和 $p_{\\text{data}}$ 诱导的联合分布下的均方误差 $\\mathbb{E}\\big[(x_{0} - a x_{1})^{2}\\big]$。从最小二乘回归的基本原理出发，推导最优 $a$ 的闭式表达式。\n3) 在 $x_{1} = 1$ 处，评估解析后验均值与学习到的线性估计值之间的差异，并给出精确的简化表达式。将最终答案表示为单个闭式解析表达式。不要四舍五入。", "solution": "该问题被验证为是适定的、有科学依据且自洽的。解答按要求分三部分推导。\n\n### 第一部分：真实后验均值 $\\mathbb{E}[x_{0} \\mid x_{1}]$ 的推导\n\n$x_0$ 的数据分布是一个对称高斯混合模型 (GMM)：\n$$ p(x_0) = \\frac{1}{2}\\,\\mathcal{N}(x_0; -m, \\sigma_0^2) + \\frac{1}{2}\\,\\mathcal{N}(x_0; m, \\sigma_0^2) $$\n这可以通过引入一个潜在的拉德马赫随机变量 $z \\in \\{-1, 1\\}$ 来建模，其中 $p(z=1) = p(z=-1) = \\frac{1}{2}$。在给定 $z$ 的条件下，$x_0$ 的分布是单个高斯分布：\n$$ p(x_0 \\mid z) = \\mathcal{N}(x_0; z \\cdot m, \\sigma_0^2) $$\n前向过程定义为 $x_1 = \\sqrt{\\alpha}\\, x_0 + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\beta)$ 独立于 $x_0$。这定义了条件似然：\n$$ p(x_1 \\mid x_0) = \\mathcal{N}(x_1; \\sqrt{\\alpha}\\, x_0, \\beta) $$\n我们寻求后验均值 $\\mathbb{E}[x_0 \\mid x_1]$。根据全期望定律：\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\mathbb{E}_{z \\mid x_1} \\left[ \\mathbb{E}[x_0 \\mid x_1, z] \\right] = \\sum_{z \\in \\{-1, 1\\}} \\mathbb{E}[x_0 \\mid x_1, z] \\, p(z \\mid x_1) $$\n首先，我们求条件均值 $\\mathbb{E}[x_0 \\mid x_1, z]$。对于一个固定的 $z$，我们有一个线性高斯系统：\n- 先验：$x_0 \\mid z \\sim \\mathcal{N}(z \\cdot m, \\sigma_0^2)$\n- 似然：$x_1 \\mid x_0 \\sim \\mathcal{N}(\\sqrt{\\alpha}\\, x_0, \\beta)$\n\n联合分布 $p(x_0, x_1 \\mid z)$ 是一个二元高斯分布。后验分布 $p(x_0 \\mid x_1, z)$ 也是高斯分布。使用线性高斯模型的标准贝叶斯共轭公式，其均值为：\n$$ \\mathbb{E}[x_0 \\mid x_1, z] = \\mu_{x_0|z} + \\frac{\\text{Cov}(x_0, x_1 \\mid z)}{\\text{Var}(x_1 \\mid z)}(x_1 - \\mu_{x_1|z}) $$\n给定 $z$ 的条件下，所需的矩为：\n- $\\mu_{x_0|z} = \\mathbb{E}[x_0 \\mid z] = z \\cdot m$\n- $\\mu_{x_1|z} = \\mathbb{E}[x_1 \\mid z] = \\mathbb{E}[\\sqrt{\\alpha}\\, x_0 + \\epsilon \\mid z] = \\sqrt{\\alpha}\\, \\mathbb{E}[x_0 \\mid z] = \\sqrt{\\alpha} \\, z \\cdot m$\n- $\\text{Cov}(x_0, x_1 \\mid z) = \\text{Cov}(x_0, \\sqrt{\\alpha}\\, x_0 + \\epsilon \\mid z) = \\sqrt{\\alpha}\\, \\text{Var}(x_0 \\mid z) = \\sqrt{\\alpha}\\, \\sigma_0^2$\n- $\\text{Var}(x_1 \\mid z) = \\text{Var}(\\sqrt{\\alpha}\\, x_0 + \\epsilon \\mid z) = \\alpha\\, \\text{Var}(x_0 \\mid z) + \\text{Var}(\\epsilon) = \\alpha\\, \\sigma_0^2 + \\beta$\n\n令 $\\sigma_1^2 = \\alpha\\, \\sigma_0^2 + \\beta$。分量后验的均值为：\n$$ \\mu_z(x_1) \\equiv \\mathbb{E}[x_0 \\mid x_1, z] = z \\cdot m + \\frac{\\sqrt{\\alpha}\\, \\sigma_0^2}{\\alpha\\, \\sigma_0^2 + \\beta} (x_1 - \\sqrt{\\alpha}\\, z \\cdot m) $$\n接下来，我们使用贝叶斯法则确定后验概率 $p(z \\mid x_1)$：\n$$ p(z \\mid x_1) = \\frac{p(x_1 \\mid z) p(z)}{p(x_1)} \\propto p(x_1 \\mid z) $$\n分布 $p(x_1 \\mid z)$ 是联合分布 $p(x_0, x_1 \\mid z)$ 对 $x_0$ 的边际分布，它是一个均值为 $\\mu_{x_1|z}$、方差为 $\\sigma_1^2$ 的高斯分布：\n$$ p(x_1 \\mid z) = \\mathcal{N}(x_1; \\sqrt{\\alpha}\\, z \\cdot m, \\sigma_1^2) $$\n$z=1$ 和 $z=-1$ 的后验概率为：\n$$ p(z=1 \\mid x_1) \\propto \\exp\\left(-\\frac{(x_1 - \\sqrt{\\alpha}\\, m)^2}{2\\sigma_1^2}\\right) $$\n$$ p(z=-1 \\mid x_1) \\propto \\exp\\left(-\\frac{(x_1 + \\sqrt{\\alpha}\\, m)^2}{2\\sigma_1^2}\\right) $$\n将它们归一化得到：\n$$ p(z=1 \\mid x_1) = \\frac{\\exp(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2})}{\\exp(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2}) + \\exp(-\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2})} = \\frac{1}{2} \\left[ 1 + \\tanh\\left(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2}\\right) \\right] $$\n$$ p(z=-1 \\mid x_1) = 1 - p(z=1 \\mid x_1) = \\frac{1}{2} \\left[ 1 - \\tanh\\left(\\frac{\\sqrt{\\alpha}\\,m\\,x_1}{\\sigma_1^2}\\right) \\right] $$\n令 $\\theta = \\frac{\\sqrt{\\alpha}\\,m}{\\sigma_1^2}$。完整的后验均值为：\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\mu_1(x_1) p(z=1|x_1) + \\mu_{-1}(x_1) p(z=-1|x_1) $$\n$$ = \\frac{1}{2}(\\mu_1(x_1) + \\mu_{-1}(x_1)) + \\frac{1}{2}(\\mu_1(x_1) - \\mu_{-1}(x_1)) \\tanh(\\theta x_1) $$\n我们计算各项：\n$$ \\mu_1(x_1) + \\mu_{-1}(x_1) = 2 \\frac{\\sqrt{\\alpha}\\, \\sigma_0^2}{\\sigma_1^2} x_1 $$\n$$ \\mu_1(x_1) - \\mu_{-1}(x_1) = 2m \\left( 1 - \\frac{\\alpha\\, \\sigma_0^2}{\\sigma_1^2} \\right) = 2m \\frac{\\sigma_1^2 - \\alpha\\,\\sigma_0^2}{\\sigma_1^2} = 2m \\frac{\\beta}{\\sigma_1^2} $$\n将这些代入 $\\mathbb{E}[x_0 \\mid x_1]$ 的表达式中：\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\frac{\\sqrt{\\alpha}\\, \\sigma_0^2}{\\sigma_1^2} x_1 + \\frac{m \\beta}{\\sigma_1^2} \\tanh\\left(\\frac{\\sqrt{\\alpha}\\,m}{\\sigma_1^2} x_1\\right) $$\n使用给定值：$m=1$，$\\sigma_0^2=1$，$\\beta=\\frac{1}{2}$，以及 $\\alpha=1-\\beta=\\frac{1}{2}$。\n- $\\sqrt{\\alpha} = \\frac{1}{\\sqrt{2}}$\n- $\\sigma_1^2 = \\alpha\\, \\sigma_0^2 + \\beta = \\frac{1}{2}(1) + \\frac{1}{2} = 1$\n- $x_1$ 的系数是 $\\frac{(1/\\sqrt{2})(1)}{1} = \\frac{1}{\\sqrt{2}}$。\n- tanh 项的系数是 $\\frac{(1)(1/2)}{1} = \\frac{1}{2}$。\n- tanh 的参数是 $\\frac{(1/\\sqrt{2})(1)}{1} x_1 = \\frac{x_1}{\\sqrt{2}}$。\n因此，后验均值为：\n$$ \\mathbb{E}[x_0 \\mid x_1] = \\frac{1}{\\sqrt{2}} x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) $$\n\n### 第二部分：最优线性预测器系数 $a$ 的推导\n\n我们希望找到使均方误差 $L(a) = \\mathbb{E}[(x_0 - a x_1)^2]$ 最小化的系数 $a$。期望是关于联合分布 $p(x_0, x_1)$ 计算的。通过将 $L(a)$ 的导数设为零来找到最优的 $a$：\n$$ \\frac{d L}{d a} = \\mathbb{E}[-2 x_1 (x_0 - a x_1)] = -2 (\\mathbb{E}[x_0 x_1] - a \\mathbb{E}[x_1^2]) = 0 $$\n求解 $a$ 得到经典的最小二乘解：\n$$ a = \\frac{\\mathbb{E}[x_0 x_1]}{\\mathbb{E}[x_1^2]} $$\n我们需要计算矩 $\\mathbb{E}[x_0 x_1]$ 和 $\\mathbb{E}[x_1^2]$。\n$x_0$ 的矩为：\n- $\\mathbb{E}[x_0] = \\frac{1}{2}(-m) + \\frac{1}{2}(m) = 0$\n- $\\mathbb{E}[x_0^2] = \\text{Var}(x_0) + (\\mathbb{E}[x_0])^2 = \\text{Var}(x_0)$。根据全方差定律，$\\text{Var}(x_0) = \\mathbb{E}[\\text{Var}(x_0|z)] + \\text{Var}(\\mathbb{E}[x_0|z])$。\n- $\\text{Var}(x_0|z) = \\sigma_0^2$，所以 $\\mathbb{E}[\\text{Var}(x_0|z)] = \\sigma_0^2$。\n- $\\mathbb{E}[x_0|z] = z \\cdot m$，所以 $\\text{Var}(\\mathbb{E}[x_0|z]) = \\text{Var}(z \\cdot m) = m^2 \\text{Var}(z) = m^2(1) = m^2$。\n- $\\mathbb{E}[x_0^2] = \\sigma_0^2 + m^2$。\n\n现在我们求所需的交叉矩和二阶矩：\n- $\\mathbb{E}[x_0 x_1] = \\mathbb{E}[x_0(\\sqrt{\\alpha}\\, x_0 + \\epsilon)] = \\mathbb{E}[\\sqrt{\\alpha}\\, x_0^2 + x_0 \\epsilon] = \\sqrt{\\alpha}\\, \\mathbb{E}[x_0^2] + \\mathbb{E}[x_0]\\mathbb{E}[\\epsilon] = \\sqrt{\\alpha}(\\sigma_0^2 + m^2)$。\n- $\\mathbb{E}[x_1^2] = \\text{Var}(x_1) + (\\mathbb{E}[x_1])^2$。由于 $\\mathbb{E}[x_1] = \\mathbb{E}[\\sqrt{\\alpha}\\, x_0 + \\epsilon] = 0$，我们有 $\\mathbb{E}[x_1^2]=\\text{Var}(x_1)$。\n- $\\text{Var}(x_1) = \\text{Var}(\\sqrt{\\alpha}\\, x_0 + \\epsilon) = \\alpha \\text{Var}(x_0) + \\text{Var}(\\epsilon) = \\alpha(\\sigma_0^2 + m^2) + \\beta$。\n\n将这些代入 $a$ 的表达式中：\n$$ a = \\frac{\\sqrt{\\alpha}(\\sigma_0^2 + m^2)}{\\alpha(\\sigma_0^2 + m^2) + \\beta} $$\n使用给定值：$m=1$，$\\sigma_0^2=1$，$\\beta=\\frac{1}{2}$，$\\alpha=\\frac{1}{2}$：\n- $\\sigma_0^2 + m^2 = 1^2 + 1 = 2$。\n- 分子：$\\sqrt{\\frac{1}{2}}(2) = \\frac{1}{\\sqrt{2}}(2) = \\sqrt{2}$。\n- 分母：$\\frac{1}{2}(2) + \\frac{1}{2} = 1 + \\frac{1}{2} = \\frac{3}{2}$。\n$$ a = \\frac{\\sqrt{2}}{3/2} = \\frac{2\\sqrt{2}}{3} $$\n\n### 第三部分：差异的评估\n\n我们需要在点 $x_1=1$ 处评估真实后验均值（最优估计器）和学习到的线性估计（最优线性估计器）之间的差异。\n差异由以下公式给出：\n$$ D(x_1) = \\mathbb{E}[x_0 \\mid x_1] - a x_1 $$\n代入第一部分和第二部分的表达式：\n$$ D(x_1) = \\left[ \\frac{1}{\\sqrt{2}} x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) \\right] - \\frac{2\\sqrt{2}}{3} x_1 $$\n$$ D(x_1) = \\left(\\frac{1}{\\sqrt{2}} - \\frac{2\\sqrt{2}}{3}\\right)x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) $$\n$x_1$ 的系数简化为：\n$$ \\frac{1}{\\sqrt{2}} - \\frac{2\\sqrt{2}}{3} = \\frac{\\sqrt{2}}{2} - \\frac{2\\sqrt{2}}{3} = \\left(\\frac{3-4}{6}\\right)\\sqrt{2} = -\\frac{\\sqrt{2}}{6} $$\n所以差异作为 $x_1$ 的函数是：\n$$ D(x_1) = -\\frac{\\sqrt{2}}{6} x_1 + \\frac{1}{2} \\tanh\\left(\\frac{x_1}{\\sqrt{2}}\\right) $$\n题目要求我们计算其在 $x_1=1$ 处的值：\n$$ D(1) = -\\frac{\\sqrt{2}}{6}(1) + \\frac{1}{2} \\tanh\\left(\\frac{1}{\\sqrt{2}}\\right) $$\n最终的简化表达式是：\n$$ D(1) = \\frac{1}{2} \\tanh\\left(\\frac{1}{\\sqrt{2}}\\right) - \\frac{\\sqrt{2}}{6} $$\n此表达式是精确的简化差异，即最终答案。", "answer": "$$\n\\boxed{\\frac{1}{2} \\tanh\\left(\\frac{1}{\\sqrt{2}}\\right) - \\frac{\\sqrt{2}}{6}}\n$$", "id": "3116015"}, {"introduction": "在理解了理论之后，将其付诸实践至关重要。本练习将指导你构建并训练一个极简的扩散模型，以重现一个常见的失败模式：过拟合与样本多样性崩溃。通过追踪训练损失和生成样本的熵，你将获得诊断模型行为和理解数据集大小、采样随机性重要性的实践经验。[@problem_id:3115973]", "problem": "您的任务是构建一个最小但科学上可信的实验，用以展示扩散概率模型中的一种过拟合失败模式。您将实现并分析一个一维去噪扩散模型，该模型在每个扩散时间步使用简单的线性去噪器，并在一个高斯混合数据集上进行训练。您的目标是展示在训练过程中，训练集上的平均去噪损失下降，而生成样本的多样性却发生崩塌。多样性通过对训练过程中生成样本的直方图计算离散熵来衡量。\n\n基本原理和假设：\n- 考虑一个具有有限数量扩散步骤的前向加噪过程，由整数 $t \\in \\{1,\\dots,T\\}$ 索引。设 $\\beta_t \\in (0,1)$ 为预定义的方差调度，$\\alpha_t = 1 - \\beta_t$，且 $\\overline{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$。前向过程由 $x_t = \\sqrt{\\overline{\\alpha}_t}\\,x_0 + \\sqrt{1 - \\overline{\\alpha}_t}\\,\\epsilon$ 给出，其中 $\\epsilon \\sim \\mathcal{N}(0,1)$ 是一维的。\n- 训练目标是去噪分数匹配，形式为真实噪声 $\\epsilon$ 与参数化预测 $\\epsilon_\\theta(x_t, t)$ 之间的均方误差，即使用训练集对 $\\mathbb{E}\\left[ \\|\\epsilon - \\epsilon_\\theta(x_t,t)\\|^2 \\right]$ 进行经验估计。\n- 您将在每个时间步使用一个线性去噪器：$\\epsilon_\\theta(x_t, t) = a_t x_t + b_t$，其中 $a_t$ 和 $b_t$ 是特定于时间 $t$ 的标量参数。\n- 祖先逆时采样步骤使用与原始去噪扩散概率模型一致的标准条件均值参数化。对于 $t \\ge 1$，令 $x_t$ 表示当前状态，模型计算预测噪声 $\\widehat{\\epsilon}_t = a_t x_t + b_t$ 并更新\n$$\n\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha}_t}} \\widehat{\\epsilon}_t \\right),\n$$\n并在 $t>1$ 时抽取 $x_{t-1} \\sim \\mathcal{N}\\!\\left(\\mu_\\theta(x_t,t),\\, \\gamma^2 \\beta_t \\right)$，在 $t=1$ 时设置 $x_0 = \\mu_\\theta(x_1,1)$。这里 $\\gamma \\in [0,1]$ 是一个采样随机性参数：$\\gamma = 1$ 是完全随机的祖先采样，而 $\\gamma = 0$ 是确定性采样。\n\n数据分布：\n- 数据分布是一个均衡的一维高斯混合分布，具有两个对称的众数：$x_0 \\sim \\tfrac{1}{2}\\,\\mathcal{N}(-\\mu, \\sigma_{\\text{data}}^2) + \\tfrac{1}{2}\\,\\mathcal{N}(+\\mu, \\sigma_{\\text{data}}^2)$，其中参数 $\\mu > 0$ 和 $\\sigma_{\\text{data}} > 0$ 已给定。\n\n训练协议：\n- 对所有 $t \\in \\{1,\\dots,T\\}$，将 $a_t$ 和 $b_t$ 初始化为零。\n- 在每个轮次中，为每个训练样本均匀抽取一个时间步索引 $t$，使用新的 $\\epsilon \\sim \\mathcal{N}(0,1)$ 通过前向过程生成 $x_t$，计算单样本损失 $\\ell = \\left(\\epsilon - (a_t x_t + b_t)\\right)^2$，并使用全批量梯度下降法，以学习率 $\\eta$ 对批次中的平均损失，就 $a_t$ 和 $b_t$ 进行一步梯度更新。如果需要，您可以使用梯度裁剪来保持数值稳定性，前提是它在所有测试用例中得到一致的应用。\n\n生成多样性追踪：\n- 在指定的轮次，通过从 $x_T \\sim \\mathcal{N}(0,1)$ 到 $x_0$ 运行逆向过程，使用当前的参数 $\\{a_t, b_t\\}$ 生成 $K$ 个样本。使用一个具有 $B$ 个箱（bin）且范围固定的 $[-R, R]$ 的直方图，计算在 $x_0$ 处生成的边际分布的离散熵。设直方图箱的概率为 $\\{p_i\\}_{i=1}^B$；计算 $H = -\\sum_{i=1}^B p_i \\log(p_i)$，单位为奈特（nats），并约定 $0 \\log 0 = 0$。追踪训练开始时和训练结束时的熵。\n\n每个测试用例的任务：\n- 实现上述的训练和采样过程。\n- 测量初始平均训练损失（在任何参数更新之前）和最终平均训练损失（在训练之后），两者都通过在训练集上计算 $\\epsilon$ 和 $\\epsilon_\\theta(x_t,t)$ 之间的经验均方误差来得到，其中时间步和噪声是新采样的。\n- 使用上面定义的基于直方图的估计器，测量训练开始时和结束时的生成样本熵。\n- 返回一个布尔值，指示以下两项是否同时成立：\n  1. 最终训练损失严格小于初始训练损失，且差值至少为指定的容差 $\\delta_{\\text{loss}}$。\n  2. 最终熵严格小于初始熵，且差值至少为指定的容差 $\\delta_{\\text{ent}}$。\n\n数值和实现要求：\n- 对所有数据和参数使用一维数组。\n- 使用一个在小的正最小值和一个适中的最大值之间（含两端）的线性 $\\beta_t$ 调度。\n- 对每个测试用例使用带有给定种子的确定性伪随机数生成，以确保可复现性。\n- 所有计算出的熵必须以奈特（nats）为单位表示。不涉及其他物理单位。\n\n要实现和评估的测试套件：\n- 固定的全局超参数：\n  - 扩散步数 $T = 20$。\n  - 在 $T$ 步内，从 $\\beta_{\\min} = 10^{-4}$ 到 $\\beta_{\\max} = 2\\times 10^{-2}$ 的线性调度 $\\beta_t$（含两端）。\n  - 混合分布参数 $\\mu = 2.0$ 和 $\\sigma_{\\text{data}} = 0.3$。\n  - 学习率 $\\eta = 0.1$。\n  - 直方图箱数 $B = 50$ 和范围 $[-R,R]$，其中 $R = 6.0$。\n  - 每次评估生成的样本数 $K = 1000$。\n  - 容差 $\\delta_{\\text{loss}} = 10^{-2}$ 和 $\\delta_{\\text{ent}} = 5\\times 10^{-2}$。\n- 三个测试用例，每个都指定为一个元组 $(\\text{seed}, N, \\text{epochs}, \\gamma)$：\n  1. $(42, 8, 600, 0.0)$: 小数据集，长时训练，确定性采样；旨在展示过拟合和多样性崩塌。\n  2. $(123, 512, 200, 1.0)$: 较大数据集，中等训练，随机采样；旨在避免多样性崩塌，同时仍降低损失。\n  3. $(7, 8, 5, 1.0)$: 小数据集，极短训练，随机采样；旨在作为一个没有明显崩塌的边界情况。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个对应于上述三个测试用例的布尔值列表，按顺序列出，格式为 Python 风格的列表，不含空格，例如 `[True,False,False]`。", "solution": "我们概述了一个有原则的推导和算法方案，它将扩散概率模型的基本原理与一个具体的、可测试的实现联系起来，以揭示由过拟合驱动的多样性崩塌。\n\n从前向扩散过程开始。对于固定的调度 $\\{\\beta_t\\}_{t=1}^T$ 且 $\\beta_t \\in (0,1)$，定义 $\\alpha_t = 1 - \\beta_t$ 和累积乘积 $\\overline{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$。标准的前向损坏过程是\n$$\nx_t = \\sqrt{\\overline{\\alpha}_t}\\,x_0 + \\sqrt{1 - \\overline{\\alpha}_t}\\,\\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,1),\n$$\n这是通过递归地复合方差为 $\\beta_t$ 的高斯扰动，并注意到在给定 $x_0$ 的条件下，时间 $t$ 的边际分布是高斯分布而得出的。这是源自原始去噪扩散公式的一个经过充分检验的事实。\n\n通过去噪分数匹配设定训练目标。在每个时间 $t$，我们寻求使用参数化模型 $\\epsilon_\\theta(x_t,t)$ 来近似条件分数。一个广泛使用且理由充分的代理损失是期望均方误差\n$$\n\\mathcal{L}(\\theta) = \\mathbb{E}_{t \\sim \\text{Unif}(\\{1,\\dots,T\\}),\\,x_0 \\sim p_{\\text{data}},\\,\\epsilon \\sim \\mathcal{N}(0,1)}\n\\left[ \\left\\| \\epsilon - \\epsilon_\\theta(x_t,t) \\right\\|^2 \\right],\n$$\n其中 $x_t$ 如上所述。在我们的的一维设定中，我们为每个时间步 $t$ 采用一个线性去噪器：$\\epsilon_\\theta(x_t,t) = a_t x_t + b_t$，其中 $a_t$ 和 $b_t$ 是标量。线性模型提供了一个可解析处理且计算高效的基线，从而可以清晰地展示这一现象。\n\n经验风险和梯度。考虑在数据集 $\\{x_0^{(i)}\\}_{i=1}^N$ 上 $\\mathcal{L}(\\theta)$ 的经验对应项：\n$$\n\\widehat{\\mathcal{L}}(\\{a_t,b_t\\}) = \\frac{1}{N} \\sum_{i=1}^N \\left( \\epsilon^{(i)} - \\left[a_{t^{(i)}} x_{t^{(i)}}^{(i)} + b_{t^{(i)}}\\right] \\right)^2,\n$$\n其中对于每个 $i$，我们从 $\\{1,\\dots,T\\}$ 中均匀抽取 $t^{(i)}$ 并抽取 $\\epsilon^{(i)} \\sim \\mathcal{N}(0,1)$，然后构成 $x_{t^{(i)}}^{(i)} = \\sqrt{\\overline{\\alpha}_{t^{(i)}}}\\,x_0^{(i)} + \\sqrt{1 - \\overline{\\alpha}_{t^{(i)}}}\\,\\epsilon^{(i)}$。对平方损失关于线性参数求导，对于每个时间步 $t$，得到\n$$\n\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial a_t} = -\\frac{2}{N} \\sum_{i: t^{(i)}=t} \\left(\\epsilon^{(i)} - (a_t x_{t}^{(i)} + b_t)\\right) x_t^{(i)},\n\\quad\n\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial b_t} = -\\frac{2}{N} \\sum_{i: t^{(i)}=t} \\left(\\epsilon^{(i)} - (a_t x_{t}^{(i)} + b_t)\\right).\n$$\n使用学习率 $\\eta$ 的全批量梯度下降更新应用如下\n$$\na_t \\leftarrow a_t - \\eta \\,\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial a_t}, \\quad\nb_t \\leftarrow b_t - \\eta \\,\\frac{\\partial \\widehat{\\mathcal{L}}}{\\partial b_t}.\n$$\n对所有 $t$ 将 $a_t=0$ 和 $b_t=0$ 进行初始化，对应一个接近零的去噪器，并为观察训练动态提供了一个中性基线。\n\n逆时采样。给定预测的噪声 $\\widehat{\\epsilon}_t = a_t x_t + b_t$，原始去噪扩散概率模型中的非参数逆时条件均值为\n$$\n\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{\\beta_t}{\\sqrt{1-\\overline{\\alpha}_t}} \\widehat{\\epsilon}_t \\right).\n$$\n当 $t>1$ 时，从均值为 $\\mu_\\theta(x_t,t)$、方差为 $\\gamma^2 \\beta_t$ 的高斯分布中采样 $x_{t-1}$，这在 $\\gamma=1$ 时恢复了随机祖先采样器。设置 $\\gamma=0$ 会移除采样噪声，产生一个与 Denoising Diffusion Implicit Models 极限一致的确定性采样器；当模型过拟合时，这会加剧多样性崩塌。\n\n通过离散熵衡量多样性。为量化样本多样性，从当前模型中抽取 $K$ 个样本 $\\{x_0^{(k)}\\}_{k=1}^K$，并将它们离散化到固定范围 $[-R,R]$ 上的 $B$ 个箱中。得到的归一化计数定义了满足 $\\sum_i p_i = 1$ 的概率 $\\{p_i\\}_{i=1}^B$。以奈特（nats）为单位的离散熵为\n$$\nH = -\\sum_{i=1}^B p_i \\log(p_i),\n$$\n约定 $p_i = 0$ 的项贡献为 $0$。较低的熵表示较低的多样性。\n\n实验设计逻辑。通过使用非常小的训练集并进行多轮次训练，可以诱发过拟合。特别是当与确定性采样 $\\gamma=0$ 结合时，学到的去噪器可能会实现一种收缩性逆向动力学，它将许多初始条件 $x_T$ 映射到一个更窄的输出集合中，从而降低直方图熵。相比之下，较大的数据集和使用随机采样 $\\gamma=1$ 的适度训练应该既能降低训练损失，又能避免熵的大幅下降。一个使用微小数据和极少训练轮次的边界情况，应该不会显示出有意义的损失下降或熵崩塌。\n\n要实现的算法步骤：\n1. 固定 $T$、线性调度端点 $\\beta_{\\min}$ 和 $\\beta_{\\max}$，并计算 $\\{\\beta_t\\}$, $\\{\\alpha_t\\}$ 和 $\\{\\overline{\\alpha}_t\\}$。\n2. 对于每个测试用例：\n   - 设置伪随机种子，从参数为 $(\\mu, \\sigma_{\\text{data}})$ 的混合分布中采样 $N$ 个数据点，并将 $\\{a_t,b_t\\}$ 初始化为零。\n   - 通过采样一批时间步和噪声，并平均 $\\epsilon$ 与 $\\epsilon_\\theta(x_t,t)$ 之间的平方误差，来计算初始经验损失。\n   - 通过使用给定的 $\\gamma$ 从逆向过程中生成 $K$ 个样本，并计算基于直方图的熵，来估计初始熵 $H_{\\text{init}}$。\n   - 使用全批量梯度下降法对经验损失进行指定轮次的训练，可选地使用梯度裁剪以保持稳定性。\n   - 使用与初始化时相同的程序（但损失计算使用新的时间步和噪声）计算最终经验损失和最终熵 $H_{\\text{final}}$。\n   - 通过检查损失的减少量是否至少为 $\\delta_{\\text{loss}}$ 且熵的减少量是否至少为 $\\delta_{\\text{ent}}$，来决定一个布尔结果。\n3. 输出一个包含三个布尔值的列表，每个布尔值对应一个测试用例，按规定顺序排列，无空格。\n\n具体的测试套件参数：\n- $T = 20$, $\\beta_{\\min} = 10^{-4}$, $\\beta_{\\max} = 2 \\times 10^{-2}$, $\\eta = 0.1$, $\\mu = 2.0$, $\\sigma_{\\text{data}} = 0.3$, $B = 50$, $R = 6.0$, $K = 1000$, $\\delta_{\\text{loss}} = 10^{-2}$ 和 $\\delta_{\\text{ent}} = 5 \\times 10^{-2}$。\n- 测试用例元组 $(\\text{seed}, N, \\text{epochs}, \\gamma)$：\n  1. $(42, 8, 600, 0.0)$。\n  2. $(123, 512, 200, 1.0)$。\n  3. $(7, 8, 5, 1.0)$。\n\n这些步骤和参数直接源于标准的扩散建模框架，构成了一个科学上现实且计算上可行的实验。最终输出必须是单行格式化的 Python 风格布尔值列表，例如 `[True,False,False]`。", "answer": "```python\nimport numpy as np\n\ndef make_beta_schedule(T, beta_min, beta_max):\n    # Linear schedule inclusive\n    return np.linspace(beta_min, beta_max, T, dtype=np.float64)\n\ndef precompute_alphas(betas):\n    alphas = 1.0 - betas\n    alpha_bars = np.cumprod(alphas)\n    return alphas, alpha_bars\n\ndef sample_mog_1d(rng, N, mu, sigma):\n    signs = rng.choice([-1.0, 1.0], size=N)\n    return signs * mu + sigma * rng.standard_normal(N)\n\ndef forward_sample_xt(rng, x0, t_idx, sqrt_alpha_bar, sqrt_one_minus_alpha_bar):\n    # t_idx is integer in [0, T-1]\n    eps = rng.standard_normal(x0.shape[0])\n    xt = sqrt_alpha_bar[t_idx] * x0 + sqrt_one_minus_alpha_bar[t_idx] * eps\n    return xt, eps\n\ndef compute_loss_and_grads(rng, x0, a, b, alpha_bar, batch_timesteps=None):\n    # Full-batch loss and grads for linear epsilon model per time step\n    N = x0.shape[0]\n    T = a.shape[0]\n    if batch_timesteps is None:\n        t_idx = rng.integers(low=0, high=T, size=N)\n    else:\n        t_idx = batch_timesteps\n    sqrt_ab = np.sqrt(alpha_bar)\n    sqrt_1mab = np.sqrt(1.0 - alpha_bar)\n    eps = rng.standard_normal(N)\n    xt = sqrt_ab[t_idx] * x0 + sqrt_1mab[t_idx] * eps\n    pred = a[t_idx] * xt + b[t_idx]\n    diff = eps - pred\n    loss = np.mean(diff**2)\n\n    # grads\n    # dL/da_t = -2/N sum_{i: t_i=t} diff_i * x_t_i\n    # dL/db_t = -2/N sum_{i: t_i=t} diff_i\n    grad_a = np.zeros(T, dtype=np.float64)\n    grad_b = np.zeros(T, dtype=np.float64)\n    scale = -2.0 / N\n    # Use numpy.add.at to accumulate\n    np.add.at(grad_a, t_idx, scale * diff * xt)\n    np.add.at(grad_b, t_idx, scale * diff)\n    return loss, grad_a, grad_b\n\ndef reverse_sample(rng, a, b, betas, alphas, alpha_bar, K, gamma):\n    # Start from x_T ~ N(0,1), go down to x_0\n    x = rng.standard_normal(K).astype(np.float64)\n    T = a.shape[0]\n    for i in range(T-1, -1, -1):\n        # predict epsilon\n        eps_hat = a[i] * x + b[i]\n        # mu = 1/sqrt(alpha) * (x - beta / sqrt(1-alpha_bar) * eps_hat)\n        denom = np.sqrt(1.0 - alpha_bar[i])\n        mu = (x - betas[i] * (eps_hat / denom)) / np.sqrt(alphas[i])\n        if i > 0:\n            noise = rng.standard_normal(K) * (np.sqrt(betas[i]) * gamma)\n            x = mu + noise\n        else:\n            x = mu\n    return x\n\ndef discrete_entropy(samples, bins, rng, R=6.0):\n    # Histogram over fixed range [-R, R], convert to probabilities\n    hist, edges = np.histogram(samples, bins=bins, range=(-R, R), density=False)\n    total = np.sum(hist)\n    if total == 0:\n        return 0.0\n    p = hist.astype(np.float64) / total\n    # compute -sum p log p; ignore zeros\n    mask = p > 0\n    H = -np.sum(p[mask] * np.log(p[mask]))\n    return float(H)\n\ndef train_and_evaluate_case(seed, N, epochs, gamma,\n                            T=20, beta_min=1e-4, beta_max=2e-2,\n                            lr=0.1, mu=2.0, sigma_data=0.3,\n                            bins=50, R=6.0, K=1000,\n                            loss_tol=1e-2, ent_tol=5e-2,\n                            clip_value=1.0):\n    rng = np.random.default_rng(seed)\n    betas = make_beta_schedule(T, beta_min, beta_max)\n    alphas, alpha_bar = precompute_alphas(betas)\n    sqrt_alpha_bar = np.sqrt(alpha_bar)\n    sqrt_one_minus_alpha_bar = np.sqrt(1.0 - alpha_bar)\n\n    # Data\n    x0 = sample_mog_1d(rng, N, mu, sigma_data)\n\n    # Initialize parameters\n    a = np.zeros(T, dtype=np.float64)\n    b = np.zeros(T, dtype=np.float64)\n\n    # Initial loss\n    init_loss, _, _ = compute_loss_and_grads(rng, x0, a, b, alpha_bar)\n\n    # Initial entropy\n    init_samples = reverse_sample(rng, a, b, betas, alphas, alpha_bar, K, gamma)\n    init_entropy = discrete_entropy(init_samples, bins=bins, rng=rng, R=R)\n\n    # Training loop\n    for ep in range(epochs):\n        loss, grad_a, grad_b = compute_loss_and_grads(rng, x0, a, b, alpha_bar)\n        # Optional gradient clipping for stability\n        ga_norm = np.linalg.norm(grad_a)\n        gb_norm = np.linalg.norm(grad_b)\n        if ga_norm > clip_value and ga_norm > 0:\n            grad_a = grad_a * (clip_value / ga_norm)\n        if gb_norm > clip_value and gb_norm > 0:\n            grad_b = grad_b * (clip_value / gb_norm)\n        a -= lr * grad_a\n        b -= lr * grad_b\n\n    # Final loss (fresh batch of t and eps)\n    final_loss, _, _ = compute_loss_and_grads(rng, x0, a, b, alpha_bar)\n\n    # Final entropy\n    final_samples = reverse_sample(rng, a, b, betas, alphas, alpha_bar, K, gamma)\n    final_entropy = discrete_entropy(final_samples, bins=bins, rng=rng, R=R)\n\n    loss_decrease = init_loss - final_loss\n    ent_decrease = init_entropy - final_entropy\n\n    outcome = (loss_decrease >= loss_tol) and (ent_decrease >= ent_tol)\n    return outcome, (init_loss, final_loss, init_entropy, final_entropy)\n\ndef solve():\n    # Define the test cases as specified in the problem statement:\n    # (seed, N, epochs, gamma)\n    test_cases = [\n        (42, 8, 600, 0.0),\n        (123, 512, 200, 1.0),\n        (7, 8, 5, 1.0),\n    ]\n    # Fixed hyperparameters per problem statement\n    T = 20\n    beta_min = 1e-4\n    beta_max = 2e-2\n    lr = 0.1\n    mu = 2.0\n    sigma_data = 0.3\n    bins = 50\n    R = 6.0\n    K = 1000\n    loss_tol = 1e-2\n    ent_tol = 5e-2\n    clip_value = 1.0\n\n    results = []\n    # Optionally, collect diagnostics if needed for debugging (not printed)\n    for seed, N, epochs, gamma in test_cases:\n        outcome, _ = train_and_evaluate_case(\n            seed=seed, N=N, epochs=epochs, gamma=gamma,\n            T=T, beta_min=beta_min, beta_max=beta_max,\n            lr=lr, mu=mu, sigma_data=sigma_data,\n            bins=bins, R=R, K=K,\n            loss_tol=loss_tol, ent_tol=ent_tol,\n            clip_value=clip_value\n        )\n        results.append(outcome)\n\n    # Final print statement in the exact required format (no spaces).\n    print(f\"[{','.join('True' if r else 'False' for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3115973"}, {"introduction": "Denoising Diffusion Implicit Models (DDIM) 是标准DDPM采样器的一种更快的替代方案，但其加速是以引入新的权衡为代价的。本练习探讨了DDIM的一个关键方面：由噪声缩放参数 $\\eta$ 控制的采样稳定性。通过推导并计算防止样本方差崩溃的临界值 $\\eta_c$，你将对加速扩散采样过程中的动态特性和潜在陷阱建立起定量的理解。[@problem_id:3116034]", "problem": "考虑一个一维的去噪扩散隐式模型（DDIM）采样器，应用于一个方差已知的数据集。\n\n来自去噪扩散概率模型（DDPM）的基础前向过程定义如下：对于离散时间索引 $t \\in \\{1,2,\\dots,T\\}$，前向扩散由条件分布 $q(x_t \\mid x_{t-1})$ 给出，该分布是一个高斯分布，其均值为 $\\sqrt{1 - \\beta_t}\\,x_{t-1}$，方差为 $\\beta_t$，即：\n$$\nq(x_t \\mid x_{t-1}) = \\mathcal{N}\\!\\left(\\sqrt{1 - \\beta_t}\\,x_{t-1},\\,\\beta_t\\right),\n$$\n其中 $\\beta_t \\in (0,1)$ 是一个小的正数。\n\n假设一个简化的线性化 DDIM 风格的采样器，其中学习到的去噪器和确定性映射在每一步共同产生一个有效的收缩系数 $c_t \\in (0,1)$，并且随机性项使用方差 $\\sigma_t$，该方差被一个标量噪声缩放参数 $\\eta \\ge 0$ 替换为 $\\eta\\,\\sigma_t$。由此得到的一维采样递归模型为\n$$\nx_{t-1} = c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t,\n$$\n其中 $z_t \\sim \\mathcal{N}(0,1)$ 在不同时间步 $t$ 之间相互独立，初始条件 $x_T \\sim \\mathcal{N}(0,1)$ 是标准正态分布。这捕捉了当学习到的去噪器在轨迹周围被线性化并且方差项被缩放时，DDIM 更新的累积效应。\n\n定义“坍缩”判据如下：在最终时刻 $t=0$ 的样本方差，记作 $\\mathrm{Var}[x_0]$，如果满足以下条件，则称之为“坍缩”：\n$$\n\\mathrm{Var}[x_0]  r_c \\cdot v_{\\text{data}},\n$$\n其中 $v_{\\text{data}}$ 是固定数据集的已知方差，$r_c \\in (0,1)$ 是一个以小数表示的指定方差比率目标。\n\n您的任务是确定临界噪声缩放值 $\\eta_c \\ge 0$，在该值下采样器从坍缩状态过渡到非坍缩状态。该值定义为不触发坍缩判据的最小 $\\eta$：\n$$\n\\eta_c = \\inf\\left\\{ \\eta \\ge 0 \\,\\big|\\, \\mathrm{Var}[x_0] \\ge r_c \\cdot v_{\\text{data}} \\right\\}.\n$$\n\n从基本定义和经过充分检验的公式出发：从 DDPM 前向过程和给定的线性采样递归开始，在独立性假设下推导方差在时间步 $t$ 间的传播。为此问题使用以下科学上一致的建模选择：\n- 使用与前向过程对齐的系数 $c_t = \\sqrt{1 - \\beta_t}$ 和 $\\sigma_t = \\sqrt{\\beta_t}$。\n- 对 $\\beta_t$ 使用线性间隔的时间表：对于 $t \\in \\{1,2,\\dots,T\\}$，\n$$\n\\beta_t = \\beta_{\\min} + \\frac{t-1}{T-1}\\left(\\beta_{\\max} - \\beta_{\\min}\\right),\n$$\n其中 $\\beta_{\\min}  0$ 和 $\\beta_{\\max}  0$ 足够小以确保数值稳定性，且 $T \\ge 1$ 为整数。\n- 初始方差为 $\\mathrm{Var}[x_T] = 1$。\n\n实现一个程序来为一小组测试套件计算 $\\eta_c$。对于每个测试用例，计算由方差递归和坍缩判据所隐含的精确 $\\eta_c$。如果采样器在 $\\eta = 0$ 时已经是非坍缩的（即，不等式在 $\\eta = 0$ 时成立），则报告该情况下的 $\\eta_c = 0$。如果所需的 $\\eta_c$ 因零方差注入而未定义（在给定的时间表下不会发生），则将其平稳地处理为 $\\eta_c = 0$。\n\n测试套件参数：\n- 情况 1：$T = 100$, $\\beta_{\\min} = 0.0001$, $\\beta_{\\max} = 0.02$, $v_{\\text{data}} = 1.5$, $r_c = 0.3$。\n- 情况 2：$T = 50$, $\\beta_{\\min} = 0.0005$, $\\beta_{\\max} = 0.01$, $v_{\\text{data}} = 4.0$, $r_c = 0.25$。\n- 情况 3：$T = 100$, $\\beta_{\\min} = 0.001$, $\\beta_{\\max} = 0.005$, $v_{\\text{data}} = 0.25$, $r_c = 0.5$。\n\n答案格式：\n- 将所有情况的临界值 $\\eta_c$ 计算为实数，并报告四舍五入到 $6$ 位小数的结果。\n- 您的程序应产生单行输出，其中包含一个由方括号括起来的逗号分隔列表（例如，$[0.123456,0.000000,1.234567]$）。", "solution": "该问题是有效的。它在深度学习理论领域内，具体涉及扩散模型采样器的稳定性，提出了一个自洽、有科学依据且定义明确的任务。所有参数和条件都已明确定义，没有矛盾或歧义之处。\n\n解决方案首先推导简化的去噪扩散隐式模型（DDIM）采样器的方差传播动力学，然后利用此结果确定最终样本方差，最后求解标志着“坍缩”区域边界的临界噪声缩放参数 $\\eta_c$。\n\n### 步骤1：方差传播定律的推导\n\n问题将一维线性化 DDIM 风格的采样递归定义为：\n$$x_{t-1} = c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t$$\n其中 $z_t \\sim \\mathcal{N}(0,1)$ 是相互独立的标准正态随机变量，$\\eta \\ge 0$ 是一个标量噪声缩放参数。初始条件是 $x_T \\sim \\mathcal{N}(0,1)$。\n\n我们的目标是找到方差 $\\mathrm{Var}[x_t]$ 的一个递归关系。令 $V_t = \\mathrm{Var}[x_t]$。首先，我们确定均值 $\\mathbb{E}[x_t]$。\n初始均值为 $\\mathbb{E}[x_T] = 0$。对于任意后续步骤 $t \\in \\{1, 2, \\dots, T\\}$，均值为：\n$$\\mathbb{E}[x_{t-1}] = \\mathbb{E}[c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t]$$\n根据期望的线性性质，\n$$\\mathbb{E}[x_{t-1}] = c_t\\,\\mathbb{E}[x_t] + \\eta\\,\\sigma_t\\,\\mathbb{E}[z_t]$$\n由于 $\\mathbb{E}[z_t] = 0$，如果我们假设 $\\mathbb{E}[x_t] = 0$，则可推断出 $\\mathbb{E}[x_{t-1}] = 0$。通过从 $t=T$ 开始的反向归纳，我们得出结论：对于所有 $t \\in \\{0, 1, \\dots, T\\}$，$\\mathbb{E}[x_t] = 0$。\n\n因此，方差 $V_{t-1} = \\mathrm{Var}[x_{t-1}]$ 等于二阶矩 $\\mathbb{E}[x_{t-1}^2]$。\n$$V_{t-1} = \\mathbb{E}[(c_t\\,x_t + \\eta\\,\\sigma_t\\,z_t)^2] = \\mathbb{E}[c_t^2\\,x_t^2 + 2\\,c_t\\,\\eta\\,\\sigma_t\\,x_t\\,z_t + \\eta^2\\,\\sigma_t^2\\,z_t^2]$$\n根据期望的线性性质：\n$$V_{t-1} = c_t^2\\,\\mathbb{E}[x_t^2] + 2\\,c_t\\,\\eta\\,\\sigma_t\\,\\mathbb{E}[x_t\\,z_t] + \\eta^2\\,\\sigma_t^2\\,\\mathbb{E}[z_t^2]$$\n状态 $x_t$ 是初始状态 $x_T$ 和噪声项 $\\{z_k\\}_{k=t+1}^T$ 的函数。噪声 $z_t$ 独立于 $x_T$ 和所有 $\\{z_k\\}_{k=t+1}^T$，因此 $z_t$ 独立于 $x_t$。这意味着 $\\mathbb{E}[x_t\\,z_t] = \\mathbb{E}[x_t]\\,\\mathbb{E}[z_t] = 0 \\cdot 0 = 0$。\n同时，我们有 $\\mathbb{E}[x_t^2] = V_t$ 和 $\\mathbb{E}[z_t^2] = \\mathrm{Var}[z_t] = 1$。将这些代入方程，得到方差传播定律：\n$$V_{t-1} = c_t^2\\,V_t + \\eta^2\\,\\sigma_t^2$$\n\n### 步骤2：最终方差 $\\mathrm{Var}[x_0]$ 的推导\n\n我们可以从 $V_T = \\mathrm{Var}[x_T] = 1$ 开始展开递归公式：\n$$V_{T-1} = c_T^2\\,V_T + \\eta^2\\,\\sigma_T^2 = c_T^2 + \\eta^2\\,\\sigma_T^2$$\n$$V_{T-2} = c_{T-1}^2\\,V_{T-1} + \\eta^2\\,\\sigma_{T-1}^2 = c_{T-1}^2\\,(c_T^2 + \\eta^2\\,\\sigma_T^2) + \\eta^2\\,\\sigma_{T-1}^2 = c_{T-1}^2\\,c_T^2 + \\eta^2\\,(c_{T-1}^2\\,\\sigma_T^2 + \\sigma_{T-1}^2)$$\n将此过程继续到 $t=0$，我们得到最终步骤的方差 $V_0 = \\mathrm{Var}[x_0]$：\n$$V_0 = \\left(\\prod_{k=1}^T c_k^2\\right) V_T + \\eta^2 \\sum_{j=1}^T \\left( \\sigma_j^2 \\prod_{k=1}^{j-1} c_k^2 \\right)$$\n其中 $j=1$ 的空积定义为 $1$。给定 $V_T=1$，表达式简化为：\n$$V_0 = \\prod_{k=1}^T c_k^2 + \\eta^2 \\sum_{j=1}^T \\left( \\sigma_j^2 \\prod_{k=1}^{j-1} c_k^2 \\right)$$\n为清晰起见，我们定义两个系数：\n$$A = \\prod_{k=1}^T c_k^2$$\n$$B = \\sum_{j=1}^T \\left( \\sigma_j^2 \\prod_{k=1}^{j-1} c_k^2 \\right)$$\n最终方差可以写成关于 $\\eta$ 的函数：\n$$\\mathrm{Var}[x_0] = A + B\\,\\eta^2$$\n问题指定了与前向过程对齐的系数，$c_t = \\sqrt{1 - \\beta_t}$ 和 $\\sigma_t = \\sqrt{\\beta_t}$，因此 $c_t^2 = 1 - \\beta_t$ 且 $\\sigma_t^2 = \\beta_t$。$\\beta_t$ 的时间表使用 $\\beta_{\\min}0$，确保对所有 $t$ 都有 $\\beta_t  0$，从而 $c_t \\in (0,1)$ 且 $\\sigma_t  0$。因此，$A  0$ 且 $B  0$。\n\n### 步骤3：临界噪声值 $\\eta_c$ 的公式化\n\n“坍缩”判据为 $\\mathrm{Var}[x_0]  r_c \\cdot v_{\\text{data}}$。临界噪声缩放值 $\\eta_c$ 定义为采样器不处于坍缩状态的最小 $\\eta \\ge 0$：\n$$\\eta_c = \\inf\\left\\{ \\eta \\ge 0 \\,\\big|\\, \\mathrm{Var}[x_0] \\ge r_c \\cdot v_{\\text{data}} \\right\\}$$\n代入我们推导出的 $\\mathrm{Var}[x_0]$ 表达式：\n$$A + B\\,\\eta^2 \\ge r_c \\cdot v_{\\text{data}}$$\n我们解出 $\\eta$：\n$$B\\,\\eta^2 \\ge r_c \\cdot v_{\\text{data}} - A$$\n由于 $B0$：\n$$\\eta^2 \\ge \\frac{r_c \\cdot v_{\\text{data}} - A}{B}$$\n令 $V_{\\text{target}} = r_c \\cdot v_{\\text{data}}$。\n出现两种情况：\n1.  如果 $A \\ge V_{\\text{target}}$，方差的确定性部分（对应于 $\\eta=0$）已足以避免坍缩。不等式 $\\eta^2 \\ge (V_{\\text{target}} - A)/B$ 对所有 $\\eta \\ge 0$ 都成立，因为右侧为非正数。有效 $\\eta$ 的集合是 $[0, \\infty)$，其下确界为 $\\eta_c = 0$。\n2.  如果 $A  V_{\\text{target}}$，系统在 $\\eta=0$ 时坍缩。为避免坍缩，$\\eta$ 必须足够大。由于 $\\eta \\ge 0$，不等式变为 $\\eta \\ge \\sqrt{\\frac{V_{\\text{target}} - A}{B}}$。有效 $\\eta$ 的集合是 $[\\sqrt{(V_{\\text{target}} - A)/B}, \\infty)$，其下确界为 $\\eta_c = \\sqrt{\\frac{V_{\\text{target}} - A}{B}}$。\n\n结合两种情况，$\\eta_c$ 的公式为：\n$$\\eta_c = \\sqrt{\\max\\left(0, \\frac{r_c \\cdot v_{\\text{data}} - A}{B}\\right)}$$\n\n### 步骤4：计算算法\n\n每个测试用例的 $\\eta_c$ 计算过程如下：\n1.  使用从 $\\beta_{\\min}$ 到 $\\beta_{\\max}$ 的线性间隔，在 $T$ 个步骤上生成 $\\beta_t$ 值的时间表。对于 $t \\in \\{1, \\dots, T\\}$，$\\beta_t = \\beta_{\\min} + \\frac{t-1}{T-1}(\\beta_{\\max} - \\beta_{\\min})$。这可以使用 `numpy.linspace(beta_min, beta_max, T)` 高效实现。\n2.  迭代计算系数 $A$ 和 $B$。初始化一个累积乘积 `prod_c_sq = 1.0` 和一个和 `B_sum = 0.0`。从 $t=1$ 到 $T$ 循环：\n    a. 获取当前的 $\\beta_t$。计算 $\\sigma_t^2 = \\beta_t$ 和 $c_t^2 = 1 - \\beta_t$。\n    b. 为求和 $B$ 添加项：`B_sum += sigma_t_sq * prod_c_sq`。\n    c. 更新下一次迭代的累积乘积：`prod_c_sq *= c_t_sq`。\n3.  循环结束后，`prod_c_sq` 的最终值是系数 $A$，`B_sum` 是系数 $B$。\n4.  计算目标方差 $V_{\\text{target}} = r_c \\cdot v_{\\text{data}}$。\n5.  计算分数的分子 `num = V_target - A`。\n6.  如果 `num = 0`，则设置 $\\eta_c = 0$。\n7.  否则，计算 $\\eta_c = \\sqrt{\\text{num} / B}$。\n8.  然后将结果格式化为 $6$ 位小数。\n\n该算法直接实现了推导出的公式，并应用于问题中指定的每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the critical noise-scaling value eta_c for a simplified DDIM sampler.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, beta_min, beta_max, v_data, r_c)\n        (100, 0.0001, 0.02, 1.5, 0.3),\n        (50, 0.0005, 0.01, 4.0, 0.25),\n        (100, 0.001, 0.005, 0.25, 0.5),\n    ]\n\n    results = []\n    for T, beta_min, beta_max, v_data, r_c in test_cases:\n        # Step 1: Generate the beta schedule.\n        # Per the problem description, for t in {1, ..., T},\n        # beta_t = beta_min + ((t-1)/(T-1))*(beta_max - beta_min).\n        # np.linspace is a robust way to implement this, handling T=1 correctly.\n        if T == 1:\n            # For T=1, t-1=0 and T-1=0. A reasonable interpretation,\n            # consistent with np.linspace(..., num=1), is beta_1 = beta_min.\n            betas = np.array([beta_min], dtype=np.float64)\n        else:\n            betas = np.linspace(beta_min, beta_max, T, dtype=np.float64)\n\n        # Step 2: Iteratively compute coefficients A and B.\n        # A = product_{k=1 to T} c_k^2\n        # B = sum_{j=1 to T} (sigma_j^2 * product_{k=1 to j-1} c_k^2)\n        # We can compute both in a single loop.\n        \n        # 'A_running_prod' will hold the cumulative product of c_k^2.\n        # At the start of iteration j, it holds product_{k=1 to j-1} c_k^2.\n        A_running_prod = 1.0\n        B_sum = 0.0\n        \n        # The loop implicitly handles indices from t=1 to T.\n        for beta_val in betas:\n            # Per problem, c_t^2 = 1 - beta_t and sigma_t^2 = beta_t\n            sigma_sq = beta_val\n            c_sq = 1.0 - beta_val\n            \n            # Add the term for the sum B\n            B_sum += sigma_sq * A_running_prod\n            \n            # Update the cumulative product for the next iteration\n            A_running_prod *= c_sq\n        \n        A = A_running_prod\n        B = B_sum\n        \n        # Step 3: Calculate the target variance.\n        target_var = r_c * v_data\n        \n        # Step 4: Calculate the numerator for the eta_c formula.\n        numerator = target_var - A\n        \n        # Step 5: Determine eta_c based on the sign of the numerator.\n        if numerator = 0:\n            # If A >= target_var, the sampler is non-collapsing even at eta=0.\n            # The infimum of the set of valid eta values is 0.\n            eta_c = 0.0\n        else:\n            # If B=0, eta_c would be undefined, but problem constraints (beta_min > 0)\n            # ensure B > 0.\n            eta_c = np.sqrt(numerator / B)\n            \n        results.append(f\"{eta_c:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3116034"}]}