{"hands_on_practices": [{"introduction": "要真正掌握生成对抗网络（GAN），第一步是理解其核心的极小化极大博弈（minimax game）的理论基础。这个练习将剥离复杂神经网络的外衣，让我们在一个简化的、可解析的情境下，从第一性原理出发推导GAN的均衡点。通过为一个一维高斯数据分布设计一个简单的线性生成器，你将亲手计算出最优判别器，并将GAN的目标函数与信息论中的詹森-香农散度（Jensen-Shannon Divergence）联系起来，从而揭示GAN训练的最终目标：使生成分布与真实数据分布完全匹配。[@problem_id:3185804]", "problem": "考虑标准的生成对抗网络 (GAN) 的极小极大博弈，其价值函数为\n$$\nV(D,G) \\;=\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}\\big[\\ln D(x)\\big] \\;+\\; \\mathbb{E}_{z\\sim p_{z}}\\big[\\ln\\big(1 - D(G(z))\\big)\\big],\n$$\n其中 $p_{\\text{data}}=\\mathcal{N}(\\mu,\\sigma^{2})$ 是一个一维高斯数据分布，其参数 $\\mu\\in\\mathbb{R}$ 和 $\\sigma>0$ 未知；潜变量分布为 $p_{z}=\\mathcal{N}(0,1)$；生成器是一个仿射映射 $G(z)=a z + b$，其参数为 $a\\in\\mathbb{R}$ 和 $b\\in\\mathbb{R}$；判别器 $D:\\mathbb{R}\\to(0,1)$ 是一个任意的可测函数。假设参数化强制执行可辨识性约束 $a\\ge 0$。\n\n从上述极小极大博弈的定义出发，并仅使用概率论、优化和信息度量的第一性原理，完成以下任务：\n- 对于一个固定的生成器 $G$（等价于一个固定的模型分布），通过关于 $D$ 最大化价值函数来推导最优判别器 $D^{\\star}$。\n- 将此最优判别器代回价值函数中，以获得生成器的导出目标函数。利用著名信息度量的基本性质，论证该导出目标函数在何处最小化。\n- 由此，确定均衡状态下的生成器参数 $(a^{\\star}, b^{\\star})$，将其表示为 $\\mu$ 和 $\\sigma$ 的函数。\n\n请将你的最终答案以行矩阵 $\\big(a^{\\star}\\;\\; b^{\\star}\\big)$ 的形式给出。无需进行数值取整。", "solution": "本题旨在寻找一个标准生成对抗网络（GAN）设置中生成模型的均衡参数 $(a^{\\star}, b^{\\star})$。该过程包括三个主要步骤：为固定生成器寻找最优判别器，推导生成器的目标函数，以及最小化此目标以找到最优生成器参数。\n\n首先，我们来确定所涉及的分布。数据分布为 $p_{\\text{data}} = \\mathcal{N}(\\mu, \\sigma^2)$。生成器是一个仿射变换 $G(z) = az + b$，它作用于从标准正态分布 $p_z = \\mathcal{N}(0,1)$ 中抽取的潜变量 $z$。因此，生成数据的分布 $p_g$ 也是一个正态分布。生成数据的均值为 $\\mathbb{E}[G(z)] = \\mathbb{E}[az+b] = a\\mathbb{E}[z]+b = a \\cdot 0 + b = b$。方差为 $\\text{Var}(G(z)) = \\text{Var}(az+b) = a^2\\text{Var}(z) = a^2 \\cdot 1^2 = a^2$。因此，生成数据的分布为 $p_g = \\mathcal{N}(b, a^2)$。可辨识性约束为 $a \\ge 0$。\n\n极小极大博弈由价值函数 $V(D, G)$ 定义：\n$$V(D,G) \\;=\\; \\mathbb{E}_{x\\sim p_{\\text{data}}}\\big[\\ln D(x)\\big] \\;+\\; \\mathbb{E}_{z\\sim p_{z}}\\big[\\ln\\big(1 - D(G(z))\\big)\\big]$$\n通过变量代换，第二个期望可以基于生成数据的分布 $p_g$ 来计算。这使我们能够将价值函数写成在数据空间 $\\mathbb{R}$ 上的积分：\n$$V(D,G) \\;=\\; \\int_{x \\in \\mathbb{R}} p_{\\text{data}}(x) \\ln D(x) \\,dx \\;+\\; \\int_{x \\in \\mathbb{R}} p_g(x) \\ln(1 - D(x)) \\,dx$$\n$$V(D,G) \\;=\\; \\int_{x \\in \\mathbb{R}} \\big[ p_{\\text{data}}(x) \\ln D(x) + p_g(x) \\ln(1 - D(x)) \\big] \\,dx$$\n对于一个固定的生成器 $G$（因此 $p_g$ 也固定），我们通过对函数 $D(x)$ 最大化 $V(D,G)$ 来找到最优判别器 $D^{\\star}(x)$。由于当被积函数在每个 $x$ 值上都最大化时，积分也达到最大值，我们考虑函数 $f(y) = c_1 \\ln y + c_2 \\ln(1 - y)$，其中 $x$ 是固定的，$y = D(x)$, $c_1 = p_{\\text{data}}(x)$，$c_2 = p_g(x)$。$y$ 的定义域是 $(0,1)$。为了找到最大值，我们计算其关于 $y$ 的导数并令其为零：\n$$\\frac{df}{dy} = \\frac{c_1}{y} - \\frac{c_2}{1-y} = 0$$\n$$\\frac{p_{\\text{data}}(x)}{D(x)} - \\frac{p_g(x)}{1 - D(x)} = 0$$\n$$p_{\\text{data}}(x)(1 - D(x)) = p_g(x)D(x)$$\n$$p_{\\text{data}}(x) = D(x) \\big(p_{\\text{data}}(x) + p_g(x)\\big)$$\n求解 $D(x)$ 得到最优判别器 $D^{\\star}$:\n$$D^{\\star}(x) = \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n\n接下来，我们将此最优判别器 $D^{\\star}(x)$ 代回价值函数 $V(D,G)$ 中，以找到生成器的目标函数 $C(G) = \\max_D V(D,G) = V(D^{\\star}, G)$。\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right] + \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right]$$\n第二个对数中的项可以简化为：\n$$1 - \\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{p_{\\text{data}}(x) + p_g(x) - p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n所以生成器的目标函数变为：\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right] + \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right)\\right]$$\n我们可以将这个表达式与著名的信息度量联系起来。通过在每个对数内加减 $\\ln 2$，我们可以用Kullback-Leibler (KL) 散度来表示 $C(G)$。两个分布 $P$ 和 $Q$ 之间的KL散度是 $D_{KL}(P \\| Q) = \\mathbb{E}_{x \\sim P}[\\ln(P(x)/Q(x))]$。\n我们重写 $C(G)$ 中的各项：\n$$ \\ln\\left(\\frac{p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) = \\ln\\left(\\frac{p_{\\text{data}}(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right) - \\ln(2) $$\n$$ \\ln\\left(\\frac{p_g(x)}{p_{\\text{data}}(x) + p_g(x)}\\right) = \\ln\\left(\\frac{p_g(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right) - \\ln(2) $$\n将这些代回 $C(G)$ 的表达式中：\n$$C(G) = \\mathbb{E}_{x\\sim p_{\\text{data}}}\\left[\\ln\\left(\\frac{p_{\\text{data}}(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right)\\right] - \\ln(2) \\;+\\; \\mathbb{E}_{x\\sim p_g}\\left[\\ln\\left(\\frac{p_g(x)}{\\frac{p_{\\text{data}}(x) + p_g(x)}{2}}\\right)\\right] - \\ln(2)$$\n识别出KL散度项，我们有：\n$$C(G) = D_{KL}\\left(p_{\\text{data}} \\left\\| \\frac{p_{\\text{data}} + p_g}{2}\\right.\\right) + D_{KL}\\left(p_g \\left\\| \\frac{p_{\\text{data}} + p_g}{2}\\right.\\right) - 2\\ln(2)$$\n这两个KL散度之和是 $p_{\\text{data}}$ 和 $p_g$ 之间的Jensen-Shannon散度 (JSD) 的两倍，其中 $JSD(P \\| Q) = \\frac{1}{2} D_{KL}(P \\| M) + \\frac{1}{2} D_{KL}(Q \\| M)$，且 $M = \\frac{P+Q}{2}$。\n因此，生成器的目标函数是：\n$$C(G) = 2 \\cdot JSD(p_{\\text{data}} \\| p_g) - 2\\ln(2)$$\n生成器的目标是最小化 $C(G)$。由于 $2$ 和 $-2\\ln(2)$ 是常数，这等价于最小化Jensen-Shannon散度 $JSD(p_{\\text{data}} \\| p_g)$。JSD的一个基本性质是它总是非负的，并且 $JSD(P \\| Q) = 0$ 当且仅当 $P=Q$。因此，当生成分布 $p_g$ 与数据分布 $p_{\\text{data}}$ 相同时，$C(G)$ 达到最小值。\n\n最后，我们通过令 $p_g = p_{\\text{data}}$ 来确定最优生成器参数 $(a^{\\star}, b^{\\star})$。\n我们有数据分布 $p_{\\text{data}} = \\mathcal{N}(\\mu, \\sigma^2)$ 和生成分布 $p_g = \\mathcal{N}(b, a^2)$。要使这两个正态分布相同，它们的参数（均值和方差）必须相等。\n令均值相等：\n$$b = \\mu$$\n令方差相等：\n$$a^2 = \\sigma^2$$\n这意味着 $a = \\pm \\sigma$。题目规定了可辨识性约束 $a \\ge 0$。由于标准差 $\\sigma$ 被设定为正数（$\\sigma > 0$），满足该约束的唯一解是 $a = \\sigma$。\n因此，均衡生成器参数为 $a^{\\star} = \\sigma$ 和 $b^{\\star} = \\mu$。\n最终答案是行矩阵 $(a^{\\star}, b^{\\star})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sigma & \\mu\n\\end{pmatrix}\n}\n$$", "id": "3185804"}, {"introduction": "理论上的均衡点优雅而简洁，但在实践中，训练GAN却充满了挑战，其中最主要的问题就是训练过程的不稳定性。这个练习通过一个极简的双线性鞍点问题 $V(x,y)=xy$ 来模拟GAN训练中的局部动态。你将通过编程实现并比较两种优化算法：朴素的同步梯度下降-上升（GDA）和更先进的额外梯度法（Extragradient）。通过分析迭代过程中的范数变化，你将直观地看到为何GDA会导致发散，而额外梯度法却能在特定条件下收敛，从而深刻理解GAN训练不稳定的根源以及稳定训练算法的重要性。[@problem_id:3185794]", "problem": "考虑生成对抗网络 (GAN) 的极小化极大博弈，该博弈通过求解 $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} V(\\mathbf{x},\\mathbf{y})$ 来寻找价值函数 $V(\\mathbf{x},\\mathbf{y})$ 的鞍点。为了研究 GAN 训练中常见的局部动态，请分析简化的二维双线性鞍点问题 $V(x,y) = x y$。在对抗性学习中，这种双线性形式是描述参数在平衡点附近相互作用的规范局部模型。\n\n从核心定义开始：\n- 对于极小化极大问题 $\\min_x \\max_y V(x,y)$，梯度下降-上升 (GDA) 更新会计算一个同步步骤，使 $V$ 相对于 $x$ 减小，相对于 $y$ 增大。\n- 外梯度法执行一个两阶段步骤：一个预测步骤，使用当前迭代点的梯度形成一个中间点；然后是一个校正步骤，使用在中间点评估的梯度来更新迭代点。\n\n你的任务是针对特定的双线性价值函数 $V(x,y) = x y$ 实现以下内容：\n- 一次由 $V$ 的梯度驱动的单步 GDA 更新，其中 $x$ 执行下降步， $y$ 执行上升步。\n- 一次单步外梯度更新，该更新首先使用当前点的梯度执行预测步骤，然后使用在预测点计算出的梯度执行校正步骤。\n\n通过测量在这些单步映射下欧几里得范数如何变化，来经验性地展示收缩和扩张区域。对于一个非零点 $(x,y)$ 和步长 $\\eta > 0$，将单步映射 $\\Phi$ 的范数比定义为\n$$\nr(x,y;\\eta) = \\frac{\\|\\Phi(x,y;\\eta)\\|_2}{\\|(x,y)\\|_2}.\n$$\n如果 $r(x,y;\\eta) < 1$，则区域被分类为收缩区域；如果 $r(x,y;\\eta) = 1$，则为中性区域；如果 $r(x,y;\\eta) > 1$，则为扩张区域。\n\n实现一个程序，该程序：\n- 使用 $V(x,y) = x y$ 的精确双线性梯度来构建定义的更新。\n- 在一个固定的初始点测试集上评估平均范数比：\n  - 测试集点是列表 $[(1,0), (0,1), (1,1), (-1,1), (0.5,-0.7)]$。\n  - 对于测试集中的每个点 $(x,y)$ 以及给定的方法和步长 $\\eta$，计算 $r(x,y;\\eta)$ 并对这些值求平均，为该案例生成一个单一的浮点数结果。\n\n测试套件：\n- 案例1（理想路径，外梯度收缩）：方法为外梯度法，步长 $\\eta = 0.5$。\n- 案例2（边界，外梯度中性）：方法为外梯度法，步长 $\\eta = 1.0$。\n- 案例3（扩张，外梯度）：方法为外梯度法，步长 $\\eta = 1.2$。\n- 案例4（与 GAN 朴素更新的比较，梯度下降-上升）：方法为梯度下降-上升法，步长 $\\eta = 0.5$。\n- 案例5（小步长比较，梯度下降-上升）：方法为梯度下降-上升法，步长 $\\eta = 0.1$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的结果按上述测试案例的顺序列出。每个条目必须是对应案例的平均范数比，以浮点数（无单位）表示。例如，输出格式为 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是在给定方法和步长下，对指定测试集计算的 $r(x,y;\\eta)$ 的平均值。", "solution": "所提出的问题是分析两种常见的优化算法——梯度下降-上升 (GDA) 和外梯度 (EG) 方法——在由价值函数 $V(x,y) = xy$ 给出的规范双线性鞍点问题上的局部动态。该分析将通过检查迭代向量 $(x,y)$ 在单次更新步骤后其欧几里得范数的变化来进行。\n\n该问题在科学和数学上是合理的。它研究了优化文献中一个与生成对抗网络 (GAN) 训练相关的明确定义的问题，使用了标准算法和一个简化但规范的模型。所有参数和条件都已指定，使得该问题是适定的，并允许一个唯一且可验证的解。\n\n极小化极大问题的核心是通过求解 $\\min_{\\mathbf{x}} \\max_{\\mathbf{y}} V(\\mathbf{x}, \\mathbf{y})$ 来找到价值函数 $V(\\mathbf{x}, \\mathbf{y})$ 的一个鞍点 $(\\mathbf{x}^*, \\mathbf{y}^*)$。这可以被构建为寻找梯度场 $F(\\mathbf{z}) = (\\nabla_{\\mathbf{x}}V, -\\nabla_{\\mathbf{y}}V)^T$ 的一个零点，其中 $\\mathbf{z} = (\\mathbf{x}, \\mathbf{y})^T$。对于这个特定的二维双线性问题，我们有 $V(x,y) = xy$。\n\n首先，我们计算 $V(x,y)$ 的偏导数：\n$$\n\\nabla_x V(x,y) = \\frac{\\partial}{\\partial x}(xy) = y\n$$\n$$\n\\nabla_y V(x,y) = \\frac{\\partial}{\\partial y}(xy) = x\n$$\n令系统的状态由向量 $\\mathbf{z} = (x, y)^T$ 表示。GDA 更新对 $x$ 执行梯度下降步，对 $y$ 执行梯度上升步，而 EG 方法使用一个两步的预测-校正机制。我们现在为每种方法推导单步更新映射 $\\Phi(\\mathbf{z};\\eta)$。\n\n**1. 梯度下降-上升 (GDA) 方法**\n\n对于一个迭代点 $\\mathbf{z}_k = (x_k, y_k)^T$ 和步长 $\\eta > 0$，GDA 更新定义为：\n$$\nx_{k+1} = x_k - \\eta \\nabla_x V(x_k, y_k)\n$$\n$$\ny_{k+1} = y_k + \\eta \\nabla_y V(x_k, y_k)\n$$\n代入 $V(x,y) = xy$ 的梯度：\n$$\nx_{k+1} = x_k - \\eta y_k\n$$\n$$\ny_{k+1} = y_k + \\eta x_k\n$$\n这构成了单步 GDA 映射 $\\Phi_{GDA}(x_k, y_k; \\eta) = (x_{k+1}, y_{k+1})$。为了分析其对范数的影响，我们计算新迭代点 $\\mathbf{z}_{k+1}$ 的欧几里得范数的平方：\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = x_{k+1}^2 + y_{k+1}^2 = (x_k - \\eta y_k)^2 + (y_k + \\eta x_k)^2\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = (x_k^2 - 2\\eta x_k y_k + \\eta^2 y_k^2) + (y_k^2 + 2\\eta x_k y_k + \\eta^2 x_k^2)\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = x_k^2 + y_k^2 + \\eta^2 x_k^2 + \\eta^2 y_k^2 = (1 + \\eta^2)(x_k^2 + y_k^2) = (1 + \\eta^2)\\|\\mathbf{z}_k\\|_2^2\n$$\n因此，范数比 $r_{GDA}$ 为：\n$$\nr_{GDA}(x,y;\\eta) = \\frac{\\|\\mathbf{z}_{k+1}\\|_2}{\\|\\mathbf{z}_k\\|_2} = \\sqrt{1 + \\eta^2}\n$$\n该比率与点 $(x,y)$ 无关，并且对于任何 $\\eta > 0$ 都严格大于 $1$。这表明 GDA 会持续扩张迭代点的范数，导致在这个简单的双线性博弈上发散。\n\n**2. 外梯度 (EG) 方法**\n\nEG 方法执行一个两阶段更新。设当前迭代点为 $\\mathbf{z}_k = (x_k, y_k)^T$。\n\n**阶段1：预测。** 使用 GDA 步骤计算一个中间点 $\\mathbf{z}_{k+1/2} = (x_{k+1/2}, y_{k+1/2})^T$：\n$$\nx_{k+1/2} = x_k - \\eta \\nabla_x V(x_k, y_k) = x_k - \\eta y_k\n$$\n$$\ny_{k+1/2} = y_k + \\eta \\nabla_y V(x_k, y_k) = y_k + \\eta x_k\n$$\n\n**阶段2：校正。** 使用在中间点 $\\mathbf{z}_{k+1/2}$ 处评估的梯度，从原始点 $\\mathbf{z}_k$ 计算最终更新：\n$$\nx_{k+1} = x_k - \\eta \\nabla_x V(x_{k+1/2}, y_{k+1/2})\n$$\n$$\ny_{k+1} = y_k + \\eta \\nabla_y V(x_{k+1/2}, y_{k+1/2})\n$$\n中间点的梯度为 $\\nabla_x V(x_{k+1/2}, y_{k+1/2}) = y_{k+1/2}$ 和 $\\nabla_y V(x_{k+1/2}, y_{k+1/2}) = x_{k+1/2}$。代入这些可得：\n$$\nx_{k+1} = x_k - \\eta (y_k + \\eta x_k) = (1 - \\eta^2)x_k - \\eta y_k\n$$\n$$\ny_{k+1} = y_k + \\eta (x_k - \\eta y_k) = \\eta x_k + (1 - \\eta^2)y_k\n$$\n这定义了单步 EG 映射 $\\Phi_{EG}(x_k, y_k; \\eta) = (x_{k+1}, y_{k+1})$。我们现在计算新迭代点 $\\mathbf{z}_{k+1}$ 的范数平方：\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = ((1 - \\eta^2)x_k - \\eta y_k)^2 + (\\eta x_k + (1 - \\eta^2)y_k)^2\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = ((1-\\eta^2)^2 x_k^2 - 2\\eta(1-\\eta^2)x_ky_k + \\eta^2 y_k^2) + (\\eta^2 x_k^2 + 2\\eta(1-\\eta^2)x_ky_k + (1-\\eta^2)^2 y_k^2)\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = ((1-\\eta^2)^2 + \\eta^2)x_k^2 + (\\eta^2 + (1-\\eta^2)^2)y_k^2 = ((1 - 2\\eta^2 + \\eta^4) + \\eta^2)(x_k^2 + y_k^2)\n$$\n$$\n\\|\\mathbf{z}_{k+1}\\|_2^2 = (1 - \\eta^2 + \\eta^4)(x_k^2 + y_k^2) = (1 - \\eta^2 + \\eta^4)\\|\\mathbf{z}_k\\|_2^2\n$$\n范数比 $r_{EG}$ 为：\n$$\nr_{EG}(x,y;\\eta) = \\frac{\\|\\mathbf{z}_{k+1}\\|_2}{\\|\\mathbf{z}_k\\|_2} = \\sqrt{1 - \\eta^2 + \\eta^4}\n$$\n与 GDA 类似，这个比率也与点 $(x,y)$ 无关。然而，其值关键地取决于步长 $\\eta$：\n- 如果 $0 < \\eta < 1$，则 $\\eta^4 < \\eta^2$，因此 $1-\\eta^2+\\eta^4 < 1$。该方法是**收缩的**。\n- 如果 $\\eta = 1$，则 $1 - 1 + 1 = 1$。该方法是**中性的**。\n- 如果 $\\eta > 1$，则 $\\eta^4 > \\eta^2$，因此 $1-\\eta^2+\\eta^4 > 1$。该方法是**扩张的**。\n\n该分析证实了问题描述中提到的区域。实现将为每个案例计算这些理论比率。由于对于所有非零点，该比率是恒定的，因此在测试集上的平均比率就是比率本身。尽管如此，代码仍将按照要求执行显式的平均循环。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the one-step dynamics of GDA and Extragradient methods on the\n    bilinear saddle problem V(x,y) = xy. It computes the mean norm ratio\n    for several test cases.\n    \"\"\"\n\n    # The test set of initial points.\n    test_points = [\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (-1.0, 1.0),\n        (0.5, -0.7)\n    ]\n\n    # The test suite of methods and step sizes.\n    test_cases = [\n        {'method': 'extragradient', 'eta': 0.5},\n        {'method': 'extragradient', 'eta': 1.0},\n        {'method': 'extragradient', 'eta': 1.2},\n        {'method': 'gda', 'eta': 0.5},\n        {'method': 'gda', 'eta': 0.1},\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        method = case['method']\n        eta = case['eta']\n        \n        ratios_for_case = []\n        for x, y in test_points:\n            # Calculate the Euclidean norm of the initial point.\n            norm_initial = np.sqrt(x**2 + y**2)\n\n            # A non-zero point is assumed as per the problem statement.\n            # If norm is zero, the ratio is undefined, but this is handled by the test set.\n            if norm_initial == 0:\n                continue\n\n            x_next, y_next = 0.0, 0.0\n\n            if method == 'gda':\n                # Apply the one-step Gradient Descent-Ascent update.\n                # x_k+1 = x_k - eta * grad_x(V) = x_k - eta * y_k\n                # y_k+1 = y_k + eta * grad_y(V) = y_k + eta * x_k\n                x_next = x - eta * y\n                y_next = y + eta * x\n            \n            elif method == 'extragradient':\n                # Apply the one-step Extragradient update.\n                # This uses the pre-derived simplified update equations.\n                # x_k+1 = (1 - eta^2) * x_k - eta * y_k\n                # y_k+1 = eta * x_k + (1 - eta^2) * y_k\n                eta_sq = eta**2\n                x_next = (1 - eta_sq) * x - eta * y\n                y_next = eta * x + (1 - eta_sq) * y\n\n            # Calculate the Euclidean norm of the updated point.\n            norm_next = np.sqrt(x_next**2 + y_next**2)\n            \n            # Compute the norm ratio for this point.\n            ratio = norm_next / norm_initial\n            ratios_for_case.append(ratio)\n        \n        # Calculate the mean norm ratio for the current case.\n        # As derived, this ratio is constant for all non-zero points,\n        # so the mean is just the ratio itself.\n        mean_ratio = np.mean(ratios_for_case)\n        final_results.append(mean_ratio)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3185794"}, {"introduction": "在理解了GAN的理论目标和训练动态的挑战之后，是时候将所有知识融会贯通，构建一个完整的GAN模型了。这个动手编程练习将指导你从零开始实现一个简单的GAN，用于研究一个关键的实际问题：模式覆盖（mode coverage）。你将探索如何通过设计多模态的潜在空间分布 $p_z$，来影响生成器捕捉真实数据中多个模式的能力，这是解决GAN常见问题——模式崩溃（mode collapse）的关键一步。[@problem_id:3185781]", "problem": "您的任务是设计并实现一个完整的、可运行的程序，该程序模拟生成对抗网络（GAN）的最小最大训练动态，并利用此模拟研究多模态潜先验分布如何与博弈相互作用，以促进或抑制生成器分布中的模式覆盖。\n\n您必须使用的定义和设置：\n- 生成对抗网络（GAN）由一个生成器和一个判别器组成。生成器是一个函数 $g_{\\theta}$，其参数为 $\\theta$，将潜变量 $z$ 映射到数据空间 $x$。判别器是一个分类器 $D_{\\phi}$，其参数为 $\\phi$，输出输入 $x$ 是真实数据而非生成数据的概率。\n- 最小最大值函数为\n$$\nV(D_{\\phi}, G_{\\theta}) \\;=\\; \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D_{\\phi}(x)] \\;+\\; \\mathbb{E}_{z \\sim p_{z}}[\\log(1 - D_{\\phi}(g_{\\theta}(z)))].\n$$\n- 在此任务中，您必须使用一维数据域和参数族：\n  - 生成器：$g_{\\theta}(z) \\;=\\; a z + b$，其中 $\\theta = (a,b)$ 且 $a,b \\in \\mathbb{R}$。\n  - 判别器：$D_{\\phi}(x) \\;=\\; \\sigma(w x + c)$，其中 $\\phi = (w,c)$，$w,c \\in \\mathbb{R}$，且 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ 是 logistic sigmoid 函数。\n\n需要使用的数据分布和潜先验：\n- 目标数据分布 $p_{\\text{data}}$ 是实线上的两个高斯分布的均衡混合：\n$$\np_{\\text{data}} \\;=\\; \\tfrac{1}{2}\\,\\mathcal{N}(-\\mu,\\sigma_{\\text{data}}^{2}) \\;+\\; \\tfrac{1}{2}\\,\\mathcal{N}(\\mu,\\sigma_{\\text{data}}^{2}),\n$$\n其中 $\\mu = 2.0$ 且 $\\sigma_{\\text{data}} = 0.2$。\n- 潜先验 $p_{z}$ 将是两个高斯分布的混合：\n$$\np_{z} \\;=\\; \\alpha\\,\\mathcal{N}(m_{1},\\sigma_{z}^{2}) \\;+\\; (1-\\alpha)\\,\\mathcal{N}(m_{2},\\sigma_{z}^{2}),\n$$\n其参数 $(m_{1},m_{2},\\alpha,\\sigma_{z})$ 由下面的每个测试用例指定。\n\n您必须实现的训练协议：\n- 使用如上所述的最小最大目标函数 $V(D_{\\phi}, G_{\\theta})$。\n- 在每一步中使用蒙特卡洛方法，通过从 $p_{\\text{data}}$ 和 $p_{z}$ 进行独立同分布采样来近似期望。\n- 通过计算 $V(D_{\\phi}, G_{\\theta})$ 对判别器的 $(w,c)$ 和生成器的 $(a,b)$ 的梯度，从第一性原理推导出参数更新规则。对 $(w,c)$ 使用梯度上升，对 $(a,b)$ 使用梯度下降。\n- 严格使用以下超参数：\n  - 训练迭代次数 $T = 2000$。\n  - 在每次迭代中，执行 $1$ 次判别器更新，然后执行 $1$ 次生成器更新。\n  - 每次更新的真实样本和潜样本的批大小均为 $B = 512$。\n  - 判别器的学习率 $\\eta_{D} = 0.01$。\n  - 生成器的学习率 $\\eta_{G} = 0.05$。\n  - 初始化：$a_{0} = 0.1$，$b_{0} = 0.0$，$w_{0} = 0.0$，$c_{0} = 0.0$。\n  - 将随机种子设置为固定值以确保可复现性。\n\n模式覆盖的实验性度量：\n- 训练后，计算生成器对潜分量均值的映射：$m'_{1} = a\\,m_{1} + b$ 和 $m'_{2} = a\\,m_{2} + b$。\n- 定义数据均值集合 $\\{-\\mu, \\mu\\}$ 和生成均值集合 $\\{m'_{1}, m'_{2}\\}$。\n- 如果 $\\min\\{|m'_{1} - \\mu_{d}|, |m'_{2} - \\mu_{d}|\\} \\le \\tau$，则认为均值为 $\\mu_{d} \\in \\{-\\mu, \\mu\\}$ 的数据模式被“覆盖”，其中覆盖容差为 $\\tau = 0.6$。\n- 覆盖分数是 $\\{0,1,2\\}$ 中的整数计数，等于生成器在这种意义上覆盖的数据模式数量。\n\n您必须实现并运行的测试套件：\n- 对 $(m_{1},m_{2},\\alpha,\\sigma_{z})$ 使用以下五种情况：\n  - 情况 1：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-1.0,\\,1.0,\\,0.5,\\,0.3)$。\n  - 情况 2：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (0.0,\\,0.0,\\,0.5,\\,0.3)$。\n  - 情况 3：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-1.0,\\,1.0,\\,0.95,\\,0.3)$。\n  - 情况 4：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-0.2,\\,0.2,\\,0.5,\\,0.3)$。\n  - 情况 5：$(m_{1},m_{2},\\alpha,\\sigma_{z}) = (-3.0,\\,3.0,\\,0.5,\\,0.3)$。\n\n输出内容：\n- 您的程序必须运行测试套件中的所有五个案例，按规定为每个案例训练 GAN，计算每个案例的最终覆盖分数，并将五个整数结果汇总成一个列表，单行打印。\n- 最终输出格式必须是仅包含结果列表的一行，格式为逗号分隔并用方括号括起，例如：$[r_{1},r_{2},r_{3},r_{4},r_{5}]$，其中每个 $r_{i}$ 是 $\\{0,1,2\\}$ 中的整数，无额外文本。\n\n注意：\n- 所有量都是无量纲的；不涉及物理单位。\n- 不使用角度；不需要角度单位。\n- 百分比（例如 $\\alpha$）是 $[0,1]$ 区间内的参数，必须作为小数处理，而不是百分数。", "solution": "该问题要求设计并实现一个简单生成对抗网络（GAN）训练动态的模拟。其目标是研究多模态潜先验分布 $p_z$ 的结构如何影响生成器捕捉双峰数据分布 $p_{\\text{data}}$ 模式的能力。我们将首先推导必要的数学框架，然后基于这些原理指定算法实现。\n\nGAN 框架包含一个生成器 $G_{\\theta}$（参数为 $\\theta=(a,b)$）和一个判别器 $D_{\\phi}$（参数为 $\\phi=(w,c)$）。生成器是一个线性函数 $g_{\\theta}(z) = az + b$。判别器是一个 logistic 分类器 $D_{\\phi}(x) = \\sigma(wx + c)$，其中 $\\sigma(u)=(1+e^{-u})^{-1}$ 是 logistic sigmoid 函数。训练由值函数 $V(D,G)$ 上的最小最大博弈控制：\n$$\nV(D_{\\phi}, G_{\\theta}) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D_{\\phi}(x)] + \\mathbb{E}_{z \\sim p_{z}}[\\log(1 - D_{\\phi}(g_{\\theta}(z)))]\n$$\n判别器旨在最大化此值，而生成器旨在最小化它。\n\n### 参数更新的梯度推导\n\n为了实现训练，我们使用基于梯度的优化。判别器参数 $\\phi=(w,c)$ 使用梯度上升进行更新，生成器参数 $\\theta=(a,b)$ 使用梯度下降进行更新。因此，我们必须推导 $V$ 关于这些参数的梯度。\n\n**1. 判别器梯度 ($\\nabla_{\\phi} V$)**\n\n判别器的更新规则是 $\\phi \\leftarrow \\phi + \\eta_D \\nabla_{\\phi} V$。我们计算 $V$ 关于 $w$ 和 $c$ 的偏导数。使用链式法则和属性 $\\frac{d}{du}\\log\\sigma(u) = 1-\\sigma(u)$：\n$$\n\\frac{\\partial}{\\partial w} \\log D_{\\phi}(x) = \\frac{\\partial}{\\partial w} \\log \\sigma(wx+c) = (1 - \\sigma(wx+c)) \\cdot x = (1 - D_{\\phi}(x))x\n$$\n$$\n\\frac{\\partial}{\\partial c} \\log D_{\\phi}(x) = \\frac{\\partial}{\\partial c} \\log \\sigma(wx+c) = (1 - \\sigma(wx+c)) \\cdot 1 = 1 - D_{\\phi}(x)\n$$\n同样，使用属性 $\\frac{d}{du}\\log(1-\\sigma(u)) = -\\sigma(u)$：\n$$\n\\frac{\\partial}{\\partial w} \\log(1 - D_{\\phi}(g_{\\theta}(z))) = -\\sigma(wg_{\\theta}(z)+c) \\cdot g_{\\theta}(z) = -D_{\\phi}(g_{\\theta}(z))g_{\\theta}(z)\n$$\n$$\n\\frac{\\partial}{\\partial c} \\log(1 - D_{\\phi}(g_{\\theta}(z))) = -\\sigma(wg_{\\theta}(z)+c) \\cdot 1 = -D_{\\phi}(g_{\\theta}(z))\n$$\n对相应分布取期望，我们得到梯度：\n$$\n\\frac{\\partial V}{\\partial w} = \\mathbb{E}_{x \\sim p_{\\text{data}}}[(1 - D_{\\phi}(x))x] + \\mathbb{E}_{z \\sim p_{z}}[-D_{\\phi}(g_{\\theta}(z))g_{\\theta}(z)]\n$$\n$$\n\\frac{\\partial V}{\\partial c} = \\mathbb{E}_{x \\sim p_{\\text{data}}}[1 - D_{\\phi}(x)] + \\mathbb{E}_{z \\sim p_{z}}[-D_{\\phi}(g_{\\theta}(z))]\n$$\n\n**2. 生成器梯度 ($\\nabla_{\\theta} V$)**\n\n生成器的更新规则是 $\\theta \\leftarrow \\theta - \\eta_G \\nabla_{\\theta} V$。$V$ 中的第一项与 $\\theta$ 无关。我们计算第二项关于 $a$ 和 $b$ 的偏导数：\n$$\n\\nabla_{\\theta} V = \\mathbb{E}_{z \\sim p_{z}}[\\nabla_{\\theta} \\log(1 - D_{\\phi}(g_{\\theta}(z)))]\n$$\n使用链式法则，$\\nabla_{\\theta}[\\cdot] = \\frac{\\partial[\\cdot]}{\\partial g_{\\theta}} \\nabla_{\\theta}g_{\\theta}(z)$。关于生成器输出 $g$ 的梯度是：\n$$\n\\frac{\\partial}{\\partial g} \\log(1 - D_{\\phi}(g)) = \\frac{\\partial}{\\partial g} \\log(1 - \\sigma(wg+c)) = -\\sigma(wg+c) \\cdot w = -w D_{\\phi}(g)\n$$\n生成器函数 $g_{\\theta}(z) = az+b$ 的梯度是 $\\frac{\\partial g_{\\theta}}{\\partial a} = z$ 和 $\\frac{\\partial g_{\\theta}}{\\partial b} = 1$。结合这些，得到：\n$$\n\\frac{\\partial V}{\\partial a} = \\mathbb{E}_{z \\sim p_{z}} \\left[ (-w D_{\\phi}(g_{\\theta}(z))) \\cdot z \\right] = -\\mathbb{E}_{z \\sim p_{z}} [w D_{\\phi}(g_{\\theta}(z)) z]\n$$\n$$\n\\frac{\\partial V}{\\partial b} = \\mathbb{E}_{z \\sim p_{z}} \\left[ (-w D_{\\phi}(g_{\\theta}(z))) \\cdot 1 \\right] = -\\mathbb{E}_{z \\sim p_{z}} [w D_{\\phi}(g_{\\theta}(z))]\n$$\n\n### 算法实现\n\n训练过程模拟进行 $T=2000$ 次迭代。在每次迭代中，我们执行一次判别器更新，然后执行一次生成器更新。期望值使用批大小为 $B=512$ 的蒙特卡洛估计来近似。\n\n**1. 采样**\n-   **数据分布 $p_{\\text{data}}$**：要从 $p_{\\text{data}} = \\frac{1}{2}\\,\\mathcal{N}(-\\mu,\\sigma_{\\text{data}}^{2}) + \\frac{1}{2}\\,\\mathcal{N}(\\mu,\\sigma_{\\text{data}}^{2})$ 中抽取一个样本，我们首先以等概率（$0.5$）选择两个高斯分量中的一个，然后从所选分量中抽取一个样本。对于大小为 $B$ 的批次，大约有 $B/2$ 个样本将从每个分量中抽取。\n-   **潜分布 $p_z$**：要从 $p_{z} = \\alpha\\,\\mathcal{N}(m_{1},\\sigma_{z}^{2}) + (1-\\alpha)\\,\\mathcal{N}(m_{2},\\sigma_{z}^{2})$ 中抽取一个样本，我们以概率 $\\alpha$ 选择第一个分量，以概率 $1-\\alpha$ 选择第二个分量，然后从选定的高斯分布中采样。\n\n**2. 训练循环**\n所有参数均按规定初始化：$a_0=0.1, b_0=0.0, w_0=0.0, c_0=0.0$。对于每次迭代 $t=0, \\dots, T-1$：\n\n-   **判别器更新**：\n    1.  从 $p_{\\text{data}}$ 抽取一批 $B$ 个真实样本 $\\{x_i\\}_{i=1}^B$。\n    2.  从 $p_{z}$ 抽取一批 $B$ 个潜样本 $\\{z_i\\}_{i=1}^B$。\n    3.  生成一批伪样本 $\\{x_{g,i}\\}_{i=1}^B$，其中 $x_{g,i} = g_{\\theta_t}(z_i) = a_t z_i + b_t$。\n    4.  使用当前参数 $\\phi_t, \\theta_t$ 和批次数据计算梯度 $\\nabla_{w} V_t$ 和 $\\nabla_{c} V_t$：\n        $$\n        \\nabla_{w} V_t \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ (1 - D_{\\phi_t}(x_i))x_i - D_{\\phi_t}(x_{g,i})x_{g,i} \\right] \\\\\n        \\nabla_{c} V_t \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ (1 - D_{\\phi_t}(x_i)) - D_{\\phi_t}(x_{g,i}) \\right]\n        $$\n    5.  更新判别器参数：$w_{t+1} = w_t + \\eta_D \\nabla_{w} V_t$, $c_{t+1} = c_t + \\eta_D \\nabla_{c} V_t$。\n\n-   **生成器更新**：\n    1.  从 $p_{z}$ 抽取新的一批 $B$ 个潜样本 $\\{z'_i\\}_{i=1}^B$。\n    2.  使用旧的生成器参数生成新的一批伪样本 $\\{x'_{g,i}\\}_{i=1}^B$：$x'_{g,i} = g_{\\theta_t}(z'_i) = a_t z'_i + b_t$。\n    3.  使用旧的生成器参数 $\\theta_t$ 和*新更新的*判别器参数 $\\phi_{t+1}$ 计算梯度 $\\nabla_{a} V_t$ 和 $\\nabla_{b} V_t$：\n        $$\n        \\nabla_{a} V_t \\approx -\\frac{1}{B}\\sum_{i=1}^B w_{t+1} D_{\\phi_{t+1}}(x'_{g,i}) z'_i \\\\\n        \\nabla_{b} V_t \\approx -\\frac{1}{B}\\sum_{i=1}^B w_{t+1} D_{\\phi_{t+1}}(x'_{g,i})\n        $$\n    4.  更新生成器参数：$a_{t+1} = a_t - \\eta_G \\nabla_{a} V_t$, $b_{t+1} = b_t - \\eta_G \\nabla_{b} V_t$。\n\n### 模式覆盖评估\n\n经过 $T$ 次训练迭代后，使用最终的生成器参数 $(a_T, b_T)$ 来评估模式覆盖。潜先验的分量均值为 $m_1$ 和 $m_2$。生成器将它们映射到 $m'_1 = a_T m_1 + b_T$ 和 $m'_2 = a_T m_2 + b_T$。目标数据分布的均值位于 $-\\mu$ 和 $\\mu$（其中 $\\mu=2.0$）。\n\n如果至少有一个生成均值接近数据模式的均值 $\\mu_d \\in \\{-\\mu, \\mu\\}$，即 $\\min\\{|m'_{1} - \\mu_{d}|, |m'_{2} - \\mu_{d}|\\} \\le \\tau$（其中容差为 $\\tau=0.6$），则认为该数据模式被“覆盖”。给定测试用例的最终分数是覆盖的数据模式总数，可以是 $0$、$1$ 或 $2$。对问题陈述中定义的五个测试用例中的每一个重复此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN simulation for all test cases and print results.\n    \"\"\"\n\n    # --- Fixed Parameters and Hyperparameters ---\n    fixed_params = {\n        \"mu\": 2.0,\n        \"sigma_data\": 0.2,\n    }\n    hyperparams = {\n        \"T\": 2000,\n        \"B\": 512,\n        \"eta_D\": 0.01,\n        \"eta_G\": 0.05,\n        \"a0\": 0.1,\n        \"b0\": 0.0,\n        \"w0\": 0.0,\n        \"c0\": 0.0,\n        \"seed\": 1234, # Fixed seed for reproducibility\n        \"tau\": 0.6,\n    }\n\n    # --- Test Suite ---\n    test_cases = [\n        # (m1, m2, alpha, sigma_z)\n        (-1.0, 1.0, 0.5, 0.3),   # Case 1\n        (0.0, 0.0, 0.5, 0.3),    # Case 2\n        (-1.0, 1.0, 0.95, 0.3),  # Case 3\n        (-0.2, 0.2, 0.5, 0.3),   # Case 4\n        (-3.0, 3.0, 0.5, 0.3),   # Case 5\n    ]\n    \n    results = []\n    for case_params_tuple in test_cases:\n        case_params = {\n            \"m1\": case_params_tuple[0],\n            \"m2\": case_params_tuple[1],\n            \"alpha\": case_params_tuple[2],\n            \"sigma_z\": case_params_tuple[3],\n        }\n        score = _run_simulation_for_case(case_params, fixed_params, hyperparams)\n        results.append(score)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef _sample_p_data(rng, B, mu, sigma_data):\n    \"\"\"Samples a batch from the real data distribution p_data.\"\"\"\n    # Choose which Gaussian component to sample from for each point\n    # 0 for N(-mu, sigma_data^2), 1 for N(mu, sigma_data^2)\n    choices = rng.binomial(1, 0.5, size=B)\n    \n    # Sample from N(-mu, sigma_data^2)\n    n_neg = B - np.sum(choices)\n    samples_neg = rng.normal(loc=-mu, scale=sigma_data, size=n_neg)\n    \n    # Sample from N(mu, sigma_data^2)\n    n_pos = np.sum(choices)\n    samples_pos = rng.normal(loc=mu, scale=sigma_data, size=n_pos)\n    \n    return np.concatenate((samples_neg, samples_pos))\n\ndef _sample_p_z(rng, B, m1, m2, alpha, sigma_z):\n    \"\"\"Samples a batch from the latent distribution p_z.\"\"\"\n    # Choose which Gaussian component to sample from for each point\n    # 0 for N(m1, sigma_z^2), 1 for N(m2, sigma_z^2)\n    choices = rng.binomial(1, 1 - alpha, size=B)\n\n    # Sample from N(m1, sigma_z^2)\n    n_1 = B - np.sum(choices)\n    samples_1 = rng.normal(loc=m1, scale=sigma_z, size=n_1)\n\n    # Sample from N(m2, sigma_z^2)\n    n_2 = np.sum(choices)\n    samples_2 = rng.normal(loc=m2, scale=sigma_z, size=n_2)\n\n    return np.concatenate((samples_1, samples_2))\n    \n\ndef _run_simulation_for_case(case_params, fixed_params, hyperparams):\n    \"\"\"\n    Trains a GAN for a single case and computes the coverage score.\n    \"\"\"\n    # Unpack parameters for convenience\n    mu, sigma_data = fixed_params[\"mu\"], fixed_params[\"sigma_data\"]\n    m1, m2, alpha, sigma_z = case_params[\"m1\"], case_params[\"m2\"], case_params[\"alpha\"], case_params[\"sigma_z\"]\n    T, B, eta_D, eta_G, tau = hyperparams[\"T\"], hyperparams[\"B\"], hyperparams[\"eta_D\"], hyperparams[\"eta_G\"], hyperparams[\"tau\"]\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(hyperparams[\"seed\"])\n\n    # Initialize parameters\n    a, b = hyperparams[\"a0\"], hyperparams[\"b0\"]\n    w, c = hyperparams[\"w0\"], hyperparams[\"c0\"]\n\n    # Training loop\n    for _ in range(T):\n        # --- Discriminator Update ---\n        x_real = _sample_p_data(rng, B, mu, sigma_data)\n        z_d = _sample_p_z(rng, B, m1, m2, alpha, sigma_z)\n        x_fake = a * z_d + b\n\n        D_real = sigmoid(w * x_real + c)\n        D_fake = sigmoid(w * x_fake + c)\n\n        grad_w_V = np.mean((1 - D_real) * x_real) - np.mean(D_fake * x_fake)\n        grad_c_V = np.mean(1 - D_real) - np.mean(D_fake)\n\n        w += eta_D * grad_w_V\n        c += eta_D * grad_c_V\n\n        # --- Generator Update ---\n        z_g = _sample_p_z(rng, B, m1, m2, alpha, sigma_z)\n        x_fake_g = a * z_g + b\n\n        # D output for fake samples using updated D parameters\n        D_fake_g = sigmoid(w * x_fake_g + c)\n        \n        # Gradients of V w.r.t a and b\n        grad_a_V = -np.mean(w * D_fake_g * z_g)\n        grad_b_V = -np.mean(w * D_fake_g)\n\n        # Gradient descent update\n        a -= eta_G * grad_a_V\n        b -= eta_G * grad_b_V\n\n    # --- Evaluation ---\n    m_prime_1 = a * m1 + b\n    m_prime_2 = a * m2 + b\n    \n    data_means = [-mu, mu]\n    gen_means = [m_prime_1, m_prime_2]\n    coverage_score = 0\n    \n    covered_data_means = set()\n    for data_mean in data_means:\n        min_dist = min(abs(gm - data_mean) for gm in gen_means)\n        if min_dist = tau:\n            covered_data_means.add(data_mean)\n            \n    coverage_score = len(covered_data_means)\n            \n    return coverage_score\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3185781"}]}