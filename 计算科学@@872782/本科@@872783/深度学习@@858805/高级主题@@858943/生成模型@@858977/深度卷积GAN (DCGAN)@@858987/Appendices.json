{"hands_on_practices": [{"introduction": "在开始训练深度卷积生成对抗网络（DCGAN）之前，理解其架构的基本属性至关重要。一个设计良好的网络应该能够灵活地适应不同分辨率的输入，而判别器的感受野也应足以覆盖整个图像。本练习 [@problem_id:3112718] 将引导你通过计算来分析网络的设计选择（如卷积核大小和层数）如何影响判别器的最终感受野及其对输入图像的覆盖范围，以及生成器如何与不同分辨率的训练任务兼容，这些都是设计高效、灵活的生成模型的关键技能。", "problem": "您被要求形式化并分析一个深度卷积生成对抗网络（DCGAN）的课程训练协议。在该协议中，训练从 $16\\times 16$ 大小的低分辨率图像开始，并逐步增加到 $64\\times 64$，而不改变网络架构。您的任务是推导、实现并计算形状和感受野属性，以证明判别器和生成器是否仅凭卷积运算就能处理这种尺寸缩放，假设这是一个全卷积DCGAN设计。\n\n请使用以下背景和约束，您必须纯粹从数学和算法的角度来解释它们，不假设能访问任何深度学习框架：\n\n- 该模型遵循深度卷积生成对抗网络（DCGAN）模式，被解释为全卷积网络（无线性层）。其中，判别器使用一堆带步幅的卷积和一个最终的全局空间平均来生成一个标量，而生成器使用一堆转置卷积从一个小潜空间网格上采样到一张图像。课程训练通过目标分辨率 $H \\in \\{16, 32, 64\\}$（均为方形图像）进行，并且架构在各个阶段必须保持不变。\n- 在每个判别器块中，有一个在两个空间维度上卷积核大小为 $k$、步幅为 $s$、填充为 $p$ 的卷积。在一次这样的卷积之后，输出的高度（和宽度）将使用离散卷积输出尺寸规则来计算。对于由 $n_d$ 个此类块组成的堆栈，应迭代地应用此规则。\n- 在给定层中，单个输出激活值相对于输入的感受野是仅使用卷积核大小和步幅递归定义的。您必须计算最后一个判别器块之后的感受野。填充不改变感受野大小。\n- 假设生成器产生一个固定的“原生”分辨率 $H_{\\text{native}}$，该分辨率由一个大小为 $H_0$ 的基础潜空间网格和 $n_g$ 个转置卷积块决定，每个块的步幅为2，使空间尺寸加倍。对于目标分辨率为 $H_{\\text{target}} \\in \\{16, 32, 64\\}$ 的课程阶段，如果比率 $H_{\\text{native}}/H_{\\text{target}}$ 是2的整数次幂，我们则认为生成器是“无需改变架构”即兼容的（这样，在训练期间就可以使用一个固定的、以2的幂为因子的外部下采样器，而无需修改网络）。\n- 您必须仅使用标准的离散卷积输出尺寸规则和标准的感受野递归公式（不使用大于1的扩张率）。不允许使用任何其他捷径。\n\n对于下面定义的每个测试用例，以及每个目标分辨率 $H \\in \\{16, 32, 64\\}$，您的程序必须计算并报告：\n\n1) 判别器在经过 $n_d$ 个块后的最终特征图空间尺寸 $H_{\\text{feat}} \\times H_{\\text{feat}}$。\n2) 判别器最后一层的感受野 $R$（由于对称性，高度等于宽度）以及比率 $R/H$（作为浮点数）。\n3) 一个布尔值，指示单个最后一层激活值的感受野是否覆盖整个输入，即 $R \\ge H$ 是否成立。\n4) 一个布尔值，指示生成器是否在不改变架构的情况下与课程分辨率兼容，即 $H_{\\text{native}}/H$ 是否为一个2的整数次幂的整数。\n5) 在最终平均化之前，对判别器的全局决策有贡献的空间位置数量，$N_{\\text{pos}} = H_{\\text{feat}}^2$。\n\n您的程序必须将给定测试用例的所有目标分辨率的结果汇总到一个列表中：\n[ [H_feat(16),H_feat(32),H_feat(64)],\n  [R_over_H(16),R_over_H(32),R_over_H(64)],\n  [D_global_coverage(16),D_global_coverage(32),D_global_coverage(64)],\n  [G_compatible(16),G_compatible(32),G_compatible(64)],\n  [N_pos(16),N_pos(32),N_pos(64)] ]\n\n最后，将三个测试用例的结果汇总到一个顶层列表中：\n[case1_results,case2_results,case3_results]\n\n所有数值计算必须严格按照给定的规则进行，任何浮点输出都应保留为标准十进制形式。如果您选择四舍五入，请将比率 $R/H$ 四舍五入到小数点后6位。不涉及任何物理单位。\n\n测试套件：\n\n- 用例 $1$：$n_g = 4$，$n_d = 4$，$k = 4$，$s = 2$，$p = 1$，$H_0 = 4$，$H \\in \\{16,32,64\\}$。\n- 用例 $2$：$n_g = 4$，$n_d = 3$，$k = 4$，$s = 2$，$p = 1$，$H_0 = 4$，$H \\in \\{16,32,64\\}$。\n- 用例 $3$：$n_g = 4$，$n_d = 4$，$k = 3$，$s = 2$，$p = 1$，$H_0 = 4$，$H \\in \\{16,32,64\\}$。\n\n最终输出格式：\n\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，格式与 [case1_results,case2_results,case3_results] 完全一致。", "solution": "该问题要求对深度卷积生成对抗网络（DCGAN）架构与课程学习协议的兼容性进行形式化分析。该协议涉及在逐步增加分辨率的图像（具体为 $H \\in \\{16, 32, 64\\}$）上进行训练，同时保持生成器和判别器的网络架构固定。此分析将纯粹通过卷积层和转置卷积层的数学属性进行，无需进行经验性训练。我们必须为三种特定的架构配置（测试用例）计算判别器和生成器的几个关键指标。\n\n### 基于原理的设计与推导\n\n分析依赖于卷积运算和感受野计算的基本原理。\n\n**1. 判别器模型与特征图尺寸确定**\n\n判别器是一个全卷积网络，由 $n_d$ 个相同的卷积块堆叠而成。每个块应用一个卷积核大小为 $k$、步幅为 $s$、填充为 $p$ 的卷积。问题指定了从输入维度 $H_{in}$ 计算输出空间维度 $H_{out}$ 的公式：\n$$H_{out} = \\left\\lfloor \\frac{H_{in} - k + 2p}{s} \\right\\rfloor + 1$$\n对于具有 $n_d$ 个块的判别器，此公式被迭代地应用。从大小为 $H \\times H$ 的输入图像开始，我们将第 $i$ 个块之后的特征图尺寸表示为 $H_i \\times H_i$。尺寸序列由以下公式给出：\n$$H_i = \\left\\lfloor \\frac{H_{i-1} - k + 2p}{s} \\right\\rfloor + 1 \\quad \\text{for } i = 1, \\dots, n_d$$\n其中 $H_0 = H$ 是输入图像尺寸。最终的特征图尺寸，我们表示为 $H_{\\text{feat}}$，即为 $H_{n_d}$。对最终的全局平均池化有贡献的空间位置数量则为 $N_{\\text{pos}} = H_{\\text{feat}}^2$。\n\n**2. 判别器感受野**\n\n最后一层中一个神经元的感受野（$R$）是指输入图像中影响其激活值的区域的大小。问题递归地定义了其计算方法。对于一堆卷积，第 $i$ 层之后的感受野 $R_i$ 是基于前一层的感受野 $R_{i-1}$、当前层的卷积核大小 $k_i$ 以及到前一层为止的累积步幅乘积 $J_{i-1}$ 来计算的。\n递归关系为：\n$$R_i = R_{i-1} + (k_i - 1) \\cdot J_{i-1}$$\n$$J_i = J_{i-1} \\cdot s_i$$\n基准情况为 $R_0 = 1$（输入层的一个像素的感受野大小为1）和 $J_0 = 1$。由于所有判别器块都是相同的，我们有对所有 $i$ 都有 $k_i = k$ 和 $s_i = s$。对 $i = 1, \\dots, n_d$ 执行此计算，以找到最终的感受野 $R = R_{n_d}$。一个关键指标是感受野是否覆盖整个输入图像，即 $R \\ge H$ 是否成立。\n\n**3. 生成器模型与兼容性**\n\n生成器是一个全卷积网络，它使用 $n_g$ 个转置卷积块将一个大小为 $H_0 \\times H_0$ 的潜空间网格上采样为一张完整尺寸的图像。每个块被指定为步幅 $s_g=2$ 并将空间维度加倍。生成器的“原生”分辨率 $H_{\\text{native}}$ 是所有 $n_g$ 个块之后的最终输出尺寸。它可以计算为：\n$$H_{\\text{native}} = H_0 \\cdot (s_g)^{n_g} = H_0 \\cdot 2^{n_g}$$\n为使生成器被认为与目标课程分辨率 $H$ “无需改变架构”即兼容，问题陈述比率 $H_{\\text{native}} / H$ 必须是一个2的整数次幂的整数。这意味着：\n$$\\frac{H_{\\text{native}}}{H} = 2^m \\quad \\text{for some integer } m \\ge 0$$\n这个条件确保了一个简单的、固定的下采样器（例如，步幅为2的幂的平均池化）可以弥合生成器的固定输出分辨率与课程早期阶段所需的较小目标分辨率之间的差距。\n\n### 分析步骤\n\n对于每个由一组参数 $\\{n_g, n_d, k, s, p, H_0\\}$ 定义的测试用例，我们执行以下计算。\n\n首先，我们计算对于给定架构而言恒定的属性，这些属性与输入分辨率 $H$ 无关：\n- 生成器的原生分辨率 $H_{\\text{native}}$。\n- 判别器的最终感受野 $R$。\n\n接下来，对于每个目标分辨率 $H \\in \\{16, 32, 64\\}$，我们计算五个所需的指标：\n1.  **$H_{\\text{feat}}$**：从 $H_0 = H$ 开始，迭代应用卷积输出尺寸公式 $n_d$ 次。\n2.  **$R/H$**：计算感受野与输入尺寸的比率。此值将四舍五入到小数点后6位。\n3.  **$D_{\\text{global\\_coverage}}$**：评估布尔条件 $R \\ge H$。\n4.  **$G_{\\text{compatible}}$**：评估布尔条件，即 $H_{\\text{native}} / H$ 是一个2的整数次幂的整数。\n5.  **$N_{\\text{pos}}$**：计算最终特征图中的位置数量，$H_{\\text{feat}}^2$。\n\n每个测试用例的结果被汇总到一个列表的列表中，然后这些列表被收集到一个顶层列表中作为最终输出。下面的 Python 实现执行此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes shape and receptive-field properties for a DCGAN under a curriculum\n    training protocol.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_g, n_d, k, s, p, H_0)\n        # Case 1\n        (4, 4, 4, 2, 1, 4),\n        # Case 2\n        (4, 3, 4, 2, 1, 4),\n        # Case 3\n        (4, 4, 3, 2, 1, 4),\n    ]\n\n    target_resolutions = [16, 32, 64]\n\n    def calculate_h_feat(h_in, n_d, k, s, p):\n        \"\"\"Calculates the discriminator's final feature map size.\"\"\"\n        h_current = h_in\n        for _ in range(n_d):\n            h_current = np.floor((h_current - k + 2 * p) / s) + 1\n        return int(h_current)\n\n    def calculate_receptive_field(n_d, k, s):\n        \"\"\"Calculates the discriminator's final receptive field size.\"\"\"\n        r = 1  # R_0\n        j = 1  # J_0\n        for _ in range(n_d):\n            r = r + (k - 1) * j\n            j = j * s\n        return r\n\n    def calculate_h_native(h_0, n_g):\n        \"\"\"Calculates the generator's native output resolution.\"\"\"\n        # Each block has stride 2 and doubles the size.\n        return h_0 * (2 ** n_g)\n\n    def is_power_of_two(n):\n        \"\"\"Checks if a positive integer is a power of two.\"\"\"\n        if n == 0:\n            return False\n        return (n  (n - 1)) == 0\n\n    all_cases_results = []\n    for case in test_cases:\n        n_g, n_d, k, s, p, h_0 = case\n\n        h_feat_list = []\n        r_over_h_list = []\n        d_global_coverage_list = []\n        g_compatible_list = []\n        n_pos_list = []\n        \n        # Calculate case-constant values\n        r = calculate_receptive_field(n_d, k, s)\n        h_native = calculate_h_native(h_0, n_g)\n\n        for h_target in target_resolutions:\n            # 1. Discriminator's final feature-map spatial size\n            h_feat = calculate_h_feat(h_target, n_d, k, s, p)\n            h_feat_list.append(h_feat)\n\n            # 2. Discriminator's last-layer receptive field ratio\n            r_over_h = round(r / h_target, 6)\n            r_over_h_list.append(r_over_h)\n\n            # 3. Boolean indicating if receptive field covers the entire input\n            d_global_coverage = r >= h_target\n            d_global_coverage_list.append(d_global_coverage)\n\n            # 4. Boolean indicating generator compatibility\n            ratio = h_native / h_target\n            g_compatible = ratio.is_integer() and is_power_of_two(int(ratio))\n            g_compatible_list.append(g_compatible)\n\n            # 5. Number of spatial positions contributing to the discriminator's decision\n            n_pos = h_feat ** 2\n            n_pos_list.append(n_pos)\n\n        case_results = [\n            h_feat_list,\n            r_over_h_list,\n            d_global_coverage_list,\n            g_compatible_list,\n            n_pos_list,\n        ]\n        all_cases_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert the nested list to a string and remove spaces for compact output.\n    final_output_string = str(all_cases_results).replace(\" \", \"\")\n    print(final_output_string)\n\nsolve()\n```", "id": "3112718"}, {"introduction": "一个表现良好的生成器应能将潜空间平滑、可预测地映射到图像空间，这通常被称为“解耦”特性。本练习 [@problem_id:3112803] 通过在潜空间中进行线性插值，并观察其在图像空间中生成的路径，来实践这一核心概念。通过量化这条路径的平滑度，我们可以具体地洞察生成器是学会了有用的解耦表征，还是产生了一个复杂的“纠缠”流形。", "problem": "考虑深度卷积生成对抗网络（DCGANs），其中生成器通过卷积计算和逐点非线性将潜向量映射到图像。设潜空间为维度为 $d$ 的实向量空间，潜向量表示为 $\\mathbf{z} \\in \\mathbb{R}^d$。生成器是一个函数 $G: \\mathbb{R}^d \\to \\mathbb{R}^{S \\times S}$，它从一个潜向量生成一个空间分辨率为 $S \\times S$ 的图像。目标是通过测量路径上连续生成的图像之间的 $L_2$ 差异来评估输出路径的平滑度，从而判断生成器在潜空间的线性插值上是否表现出线性解耦，或者生成器是否诱导了一个纠缠流形。\n\n基本基础与定义：\n- 生成对抗网络（GANs）定义了一个生成器 $G$，它将潜向量 $\\mathbf{z}$ 转换为数据样本；深度卷积生成对抗网络（DCGANs）通过卷积层来特化 $G$。\n- 潜空间中的线性插值路径为 $ \\mathbf{z}(t) = (1-t)\\,\\mathbf{z}_1 + t\\,\\mathbf{z}_2$，其中 $t \\in [0,1]$。\n- 向量 $\\mathbf{x} \\in \\mathbb{R}^n$ 的 $L_2$ 范数为 $ \\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}$。\n- 沿离散化插值 $t_i$ 的路径平滑度通过 $L_2$ 差异序列 $d_i = \\|G(\\mathbf{z}(t_{i+1})) - G(\\mathbf{z}(t_i))\\|_2$ 来评估。\n- 三角函数中的角度应解释为弧度。\n\n您将实现三个生成器原型，每个原型都由基于原则的卷积操作和上采样构成，并使用固定的、确定性的卷积核和基映射。所有构造必须仅使用线性卷积和逐点非线性；卷积必须是二维的，并按照指定的方式逐通道应用或在通道混合后应用。在整个过程中使用以下常量：\n- 潜空间维度 $d = 8$。\n- 基础特征网格大小 $s = 4$ 和上采样后的输出空间大小 $S = 8$。\n- 两个中间通道，索引为 $c \\in \\{0,1\\}$。\n\n定义用于从潜向量 $\\mathbf{z} = (z_1,\\dots,z_d)$ 构造通道特征图的基函数如下。对于每个通道 $c \\in \\{0,1\\}$ 和每个空间索引 $(i,j)$，其中 $i \\in \\{0,\\dots,s-1\\}$ 且 $j \\in \\{0,\\dots,s-1\\}$：\n- 对于通道 $c=0$，定义\n$$\na_k^{(0)}(i,j) = \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) + \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right).\n$$\n- 对于通道 $c=1$，定义\n$$\na_k^{(1)}(i,j) = \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) - \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right).\n$$\n通过以下方式构造基础特征图 $F_c \\in \\mathbb{R}^{s \\times s}$\n$$\nF_c(i,j) = \\frac{1}{d}\\sum_{k=1}^{d} z_k \\, a_k^{(c)}(i,j).\n$$\n通过最近邻复制将每个 $F_c$ 上采样为 $U_c \\in \\mathbb{R}^{S \\times S}$，即 $F_c$ 的每个元素被复制成一个 $2 \\times 2$ 的块。\n\n定义两个大小为 $3 \\times 3$ 的固定卷积核，索引为 $p,q \\in \\{0,1,2\\}$：\n$$\nK_0(p,q) = \\frac{1}{1 + p + q}, \\quad K_1(p,q) = \\frac{(-1)^{p+q}}{1 + p + q}.\n$$\n卷积以步幅为 $1$ 的二维卷积方式执行，填充被对称处理，输出形状与输入形状相同（标准的“same”卷积）。必须实现以下三个生成器原型：\n1. 线性生成器 $G_{\\mathrm{lin}}$：计算\n$$\nH_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1), \\quad G_{\\mathrm{lin}}(\\mathbf{z}) = H_0 + H_1.\n$$\n2. 非线性门控生成器 $G_{\\mathrm{gate}}$：计算\n$$\nH_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1),\n$$\n然后应用逐点非线性和乘法交互，\n$$\nG_{\\mathrm{gate}}(\\mathbf{z}) = \\tanh(H_0) \\odot \\max(H_1, 0) + \\frac{1}{2}\\,\\mathrm{conv2d}(\\tanh(H_0), K_0),\n$$\n其中 $\\odot$ 表示逐元素乘法。\n3. 饱和 tanh 生成器 $G_{\\tanh}$：计算\n$$\nG_{\\tanh}(\\mathbf{z}) = \\tanh\\!\\big(\\mathrm{conv2d}(U_0 + U_1, K_0)\\big).\n$$\n\n对于给定的生成器 $G$、潜空间端点 $\\mathbf{z}_1$ 和 $\\mathbf{z}_2$、离散步数 $n \\in \\mathbb{N}$ 以及阈值 $\\varepsilon  0$，定义一个均匀离散化 $t_i = \\frac{i}{n}$（其中 $i \\in \\{0,1,\\dots,n\\}$），并计算差异\n$$\nd_i = \\left\\| \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_{i+1}))\\big) - \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_i))\\big) \\right\\|_2,\\quad i \\in \\{0,1,\\dots,n-1\\},\n$$\n其中 $\\mathrm{vec}(\\cdot)$ 表示将 $S \\times S$ 图像向量化为 $\\mathbb{R}^{S^2}$。设 $\\{d_i\\}_{i=0}^{n-1}$ 的均值和标准差分别为 $m$ 和 $s$，并定义变异系数\n$$\n\\mathrm{CV} = \\begin{cases}\n\\frac{s}{m},  \\text{if } m > 0,\\\\\n0,  \\text{if } m = 0.\n\\end{cases}\n$$\n如果 $\\mathrm{CV} \\le \\varepsilon$，则将生成器行为分类为“线性解耦”，否则分类为“纠缠流形”。为每个测试用例返回一个布尔值，其中 $\\mathrm{True}$ 表示“线性解耦”，$\\mathrm{False}$ 表示“纠缠流形”。\n\n您的程序必须实现上述定义，并为以下每个测试用例计算分类（三角函数中的所有角度均为弧度）：\n\n- 测试用例 1（理想情况，线性生成器）：\n  - 生成器：$G_{\\mathrm{lin}}$。\n  - $\\mathbf{z}_1 = [0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8]$。\n  - $\\mathbf{z}_2 = [-0.5, 0.4, -0.3, 0.2, -0.1, 0.0, 0.1, 0.2]$。\n  - 步数：$n = 20$。\n  - 阈值：$\\varepsilon = 10^{-6}$。\n\n- 测试用例 2（非线性门控，预期纠缠）：\n  - 生成器：$G_{\\mathrm{gate}}$。\n  - $\\mathbf{z}_1 = [-1.5, 0.0, 0.5, -0.2, 1.0, -1.2, 0.3, 0.7]$。\n  - $\\mathbf{z}_2 = [2.0, -0.5, 0.8, -1.0, 1.5, 0.0, -0.3, 0.1]$。\n  - 步数：$n = 20$。\n  - 阈值：$\\varepsilon = 0.02$。\n\n- 测试用例 3（边界条件，相同端点）：\n  - 生成器：$G_{\\tanh}$。\n  - $\\mathbf{z}_1 = [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2]$。\n  - $\\mathbf{z}_2 = [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2]$。\n  - 步数：$n = 10$。\n  - 阈值：$\\varepsilon = 10^{-6}$。\n\n- 测试用例 4（饱和边界情况）：\n  - 生成器：$G_{\\tanh}$。\n  - $\\mathbf{z}_1 = [3.0, -2.5, 2.2, -1.8, 1.6, -1.4, 1.2, -1.0]$。\n  - $\\mathbf{z}_2 = [-3.0, 2.4, -2.1, 1.9, -1.7, 1.5, -1.3, 1.1]$。\n  - 步数：$n = 20$。\n  - 阈值：$\\varepsilon = 0.02$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[result_1,result_2,result_3,result_4]$），其中每个 $result_i$ 是如上定义的布尔值。", "solution": "用户的问题陈述已经过分析和验证。它具有科学依据、良构、客观且内部一致。所有定义和参数都已提供，可以进行直接且无歧义的实现。该问题是对一个简化的深度卷积生成对抗网络（DCGAN）生成器中特征纠缠的风格化但概念上合理的探索，这是深度学习中的一个相关主题。\n\n解决方案将通过逐步实现指定组件来构建，与所提供的数学定义相对应。\n\n### 1. 准备工作和常量\n问题定义了几个常量：潜空间维度 $d=8$，基础网格大小 $s=4$，以及输出网格大小 $S=8$。分析涉及两个通道，索引为 $c \\in \\{0,1\\}$。所有三角函数中的角度都指定为弧度。\n\n### 2. 卷积核\n定义了两个固定的 $3 \\times 3$ 卷积核 $K_0$ 和 $K_1$。对于索引 $p,q \\in \\{0,1,2\\}$：\n$$\nK_0(p,q) = \\frac{1}{1 + p + q}\n$$\n$$\nK_1(p,q) = \\frac{(-1)^{p+q}}{1 + p + q}\n$$\n这些卷积核将作为 $3 \\times 3$ 矩阵进行预计算。卷积操作是标准的二维卷积，步幅为 $1$，采用对称填充，使得输出维度与输入维度相匹配（'same' 卷积）。对于一个 $3 \\times 3$ 的核，这需要在输入特征图的所有侧面进行宽度为 $1$ 的填充。\n\n### 3. 基映射构造\n初始特征图是使用一组基函数从潜向量 $\\mathbf{z} \\in \\mathbb{R}^d$ 构造的。对于每个通道 $c \\in \\{0,1\\}$，定义了一组 $d=8$ 个基映射 $\\{ a_k^{(c)} \\}_{k=1}^d$。对于空间索引 $i \\in \\{0, \\dots, s-1\\}$ 和 $j \\in \\{0, \\dots, s-1\\}$，以及基索引 $k \\in \\{1, \\dots, d\\}$：\n- 通道 $c=0$：$a_k^{(0)}(i,j) = \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) + \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right)$\n- 通道 $c=1$：$a_k^{(1)}(i,j) = \\cos\\!\\left(\\frac{\\pi\\,(k+1)\\,(i+1)}{s}\\right) - \\sin\\!\\left(\\frac{\\pi\\,(k+1)\\,(j+1)}{s}\\right)$\n\n这些大小为 $s \\times s$ 的基映射是固定的，可以被预先计算。\n\n### 4. 基础特征图生成\n给定一个潜向量 $\\mathbf{z} = (z_1, \\dots, z_d)$，基础特征图 $F_c \\in \\mathbb{R}^{s \\times s}$ 作为基映射的线性组合计算得出：\n$$\nF_c(i,j) = \\frac{1}{d}\\sum_{k=1}^{d} z_k \\, a_k^{(c)}(i,j)\n$$\n此操作相对于输入潜向量 $\\mathbf{z}$ 是线性的。\n\n### 5. 上采样\n大小为 $s \\times s = 4 \\times 4$ 的基础特征图 $F_c$ 被上采样为大小为 $S \\times S = 8 \\times 8$ 的 $U_c$。指定的方法是最近邻复制，其中 $F_c$ 中的每个值被复制以形成 $U_c$ 中的一个 $2 \\times 2$ 块。此操作也是线性的。\n\n### 6. 生成器架构\n实现了三个不同的生成器函数，$G_{\\mathrm{lin}}$、$G_{\\mathrm{gate}}$ 和 $G_{\\tanh}$。每个函数都将一个潜向量 $\\mathbf{z}$ 映射到一个 $S \\times S$ 的输出图像。\n\n1.  **线性生成器 $G_{\\mathrm{lin}}$**：此生成器是一个纯粹的线性变换。\n    $$\n    H_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1)\n    $$\n    $$\n    G_{\\mathrm{lin}}(\\mathbf{z}) = H_0 + H_1\n    $$\n    由于所有组成操作（基映射组合、上采样、卷积、加法）都是线性的，因此从 $\\mathbf{z}$ 到 $G_{\\mathrm{lin}}(\\mathbf{z})$ 的整个映射是线性的。\n\n2.  **非线性门控生成器 $G_{\\mathrm{gate}}$**：此生成器引入了非线性和通道交互。\n    $$\n    H_0 = \\mathrm{conv2d}(U_0, K_0), \\quad H_1 = \\mathrm{conv2d}(U_1, K_1)\n    $$\n    $$\n    G_{\\mathrm{gate}}(\\mathbf{z}) = \\tanh(H_0) \\odot \\max(H_1, 0) + \\frac{1}{2}\\,\\mathrm{conv2d}(\\tanh(H_0), K_0)\n    $$\n    逐点双曲正切（$\\tanh$）、类ReLU门控（$\\max(H_1, 0)$）和逐元素乘法（$\\odot$）使其成为一个关于 $\\mathbf{z}$ 的高度非线性的函数。\n\n3.  **饱和 tanh 生成器 $G_{\\tanh}$**：此生成器应用了最终的饱和非线性。\n    $$\n    G_{\\tanh}(\\mathbf{z}) = \\tanh\\!\\big(\\mathrm{conv2d}(U_0 + U_1, K_0)\\big)\n    $$\n    在这里，通道在卷积前通过加法混合。最终的 $\\tanh$ 函数将对大的输入值产生饱和，这是 GANs 中的一个常见特征。\n\n### 7. 路径平滑度分析\n问题的核心是分析由潜空间中的线性插值生成的输出路径的平滑度。\n- 潜空间路径为 $\\mathbf{z}(t) = (1-t)\\,\\mathbf{z}_1 + t\\,\\mathbf{z}_2$，其中 $t \\in [0,1]$。\n- 该路径使用 $t_i = \\frac{i}{n}$（其中 $i \\in \\{0, 1, \\dots, n\\}$）进行离散化。\n- 对于路径的每个分段，计算连续生成的图像之间的 $L_2$ 距离：\n$$\nd_i = \\left\\| \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_{i+1}))\\big) - \\mathrm{vec}\\!\\big(G(\\mathbf{z}(t_i))\\big) \\right\\|_2, \\quad \\text{其中 } i \\in \\{0, 1, \\dots, n-1\\}\n$$\n这得到一个包含 $n$ 个距离值的序列 $\\{d_i\\}_{i=0}^{n-1}$。\n\n### 8. 分类\n生成器的行为根据这些步长的一致性进行分类。\n- 计算序列 $\\{d_i\\}$ 的均值 $m$ 和标准差 $s$。\n- 计算变异系数 $\\mathrm{CV} = s/m$（如果 $m=0$ 则为 $0$）。低的 $\\mathrm{CV}$ 表示步长 $d_i$ 几乎恒定，表明沿路径的映射是线性或近线性的。高的 $\\mathrm{CV}$ 表示步长变化大，是弯曲、纠缠流形的特征。\n- 如果 $\\mathrm{CV} \\le \\varepsilon$，生成器的行为被分类为“线性解耦”（返回 $\\mathrm{True}$），否则为“纠缠流形”（返回 $\\mathrm{False}$）。\n\n### 9. 测试用例执行\n将实现的函数应用于提供的四个测试用例。\n- 对于 $G_{\\mathrm{lin}}$，我们预期所有的 $d_i$ 值几乎相同，因为生成器是线性变换。这将导致 $s \\approx 0$ 和 $\\mathrm{CV} \\approx 0$，从而得到 `True` 分类。\n- 对于 $\\mathbf{z}_1 = \\mathbf{z}_2$ 的情况，路径是静态的。因此，所有 $d_i=0$，导致 $m=0$、$s=0$ 和 $\\mathrm{CV}=0$，结果为 `True` 分类。\n- 对于应用于不同端点的非线性生成器 $G_{\\mathrm{gate}}$ 和 $G_{\\tanh}$，预期非线性会将潜空间路径扭曲成输出空间中的一条曲线。沿此曲线的行进速率不会是恒定的，导致步长 $d_i$ 有显著的标准差 $s$，一个不可忽略的 $\\mathrm{CV}$，从而得到 `False` 分类。\n最终程序计算每个测试用例的布尔分类，并以指定格式打印它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the DCGAN generator prototypes and\n    analyzing their path smoothness.\n    \"\"\"\n\n    # Define constants from the problem statement\n    D_LATENT = 8\n    S_BASE = 4\n    S_OUTPUT = 8\n\n    # Pre-compute convolution kernels\n    p, q = np.mgrid[0:3, 0:3]\n    K0 = 1 / (1 + p + q)\n    K1 = ((-1)**(p + q)) / (1 + p + q)\n\n    # Pre-compute basis maps\n    k = np.arange(1, D_LATENT + 1)\n    i = np.arange(S_BASE)\n    j = np.arange(S_BASE)\n    \n    arg_i = np.pi * (k[:, None, None] + 1) * (i[None, :, None] + 1) / S_BASE\n    arg_j = np.pi * (k[:, None, None] + 1) * (j[None, None, :] + 1) / S_BASE\n    \n    BASIS_MAPS_0 = np.sin(arg_i) + np.cos(arg_j)\n    BASIS_MAPS_1 = np.cos(arg_i) - np.sin(arg_j)\n\n    def generate_base_maps(z: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Generates the base feature maps F_0 and F_1 from a latent vector z.\n        \"\"\"\n        z_reshaped = z[:, np.newaxis, np.newaxis]\n        \n        weighted_maps_0 = z_reshaped * BASIS_MAPS_0\n        F0 = (1 / D_LATENT) * np.sum(weighted_maps_0, axis=0)\n        \n        weighted_maps_1 = z_reshaped * BASIS_MAPS_1\n        F1 = (1 / D_LATENT) * np.sum(weighted_maps_1, axis=0)\n        \n        return F0, F1\n\n    def upsample(F: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Upsamples a feature map using nearest-neighbor replication.\n        \"\"\"\n        return np.kron(F, np.ones((2, 2)))\n\n    # Define Generator Prototypes\n    def g_lin(z: np.ndarray) -> np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        H0 = convolve2d(U0, K0, mode='same', boundary='fill', fillvalue=0)\n        H1 = convolve2d(U1, K1, mode='same', boundary='fill', fillvalue=0)\n        return H0 + H1\n\n    def g_gate(z: np.ndarray) -> np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        H0 = convolve2d(U0, K0, mode='same', boundary='fill', fillvalue=0)\n        H1 = convolve2d(U1, K1, mode='same', boundary='fill', fillvalue=0)\n        tanh_H0 = np.tanh(H0)\n        relu_H1 = np.maximum(H1, 0)\n        term1 = tanh_H0 * relu_H1\n        term2 = 0.5 * convolve2d(tanh_H0, K0, mode='same', boundary='fill', fillvalue=0)\n        return term1 + term2\n\n    def g_tanh(z: np.ndarray) -> np.ndarray:\n        F0, F1 = generate_base_maps(z)\n        U0, U1 = upsample(F0), upsample(F1)\n        U_sum = U0 + U1\n        H = convolve2d(U_sum, K0, mode='same', boundary='fill', fillvalue=0)\n        return np.tanh(H)\n\n    generator_map = {\n        \"G_lin\": g_lin,\n        \"G_gate\": g_gate,\n        \"G_tanh\": g_tanh,\n    }\n\n    def analyze_path(gen_name, z1, z2, n, epsilon):\n        \"\"\"\n        Performs the path analysis and returns the classification.\n        \"\"\"\n        gen_func = generator_map[gen_name]\n        z1_np = np.array(z1)\n        z2_np = np.array(z2)\n        \n        d_values = []\n        for i in range(n):\n            t_curr = i / n\n            t_next = (i + 1) / n\n            \n            z_curr = (1 - t_curr) * z1_np + t_curr * z2_np\n            z_next = (1 - t_next) * z1_np + t_next * z2_np\n            \n            img_curr = gen_func(z_curr)\n            img_next = gen_func(z_next)\n            \n            diff = np.linalg.norm(img_curr.flatten() - img_next.flatten())\n            d_values.append(diff)\n            \n        d_values_np = np.array(d_values)\n        \n        mean_d = np.mean(d_values_np)\n        std_d = np.std(d_values_np)\n        \n        if mean_d > 0:\n            cv = std_d / mean_d\n        else:\n            cv = 0.0\n            \n        return cv = epsilon\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"G_lin\", [0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8], [-0.5, 0.4, -0.3, 0.2, -0.1, 0.0, 0.1, 0.2], 20, 1e-6),\n        (\"G_gate\", [-1.5, 0.0, 0.5, -0.2, 1.0, -1.2, 0.3, 0.7], [2.0, -0.5, 0.8, -1.0, 1.5, 0.0, -0.3, 0.1], 20, 0.02),\n        (\"G_tanh\", [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2], [0.3, -0.1, 0.05, 0.0, -0.2, 0.4, -0.3, 0.2], 10, 1e-6),\n        (\"G_tanh\", [3.0, -2.5, 2.2, -1.8, 1.6, -1.4, 1.2, -1.0], [-3.0, 2.4, -2.1, 1.9, -1.7, 1.5, -1.3, 1.1], 20, 0.02)\n    ]\n\n    results = []\n    for case in test_cases:\n        gen_name, z1, z2, n, epsilon = case\n        result = analyze_path(gen_name, z1, z2, n, epsilon)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112803"}, {"introduction": "为了对生成器进行更深入的定量分析，我们可以运用微积分工具来考察其局部行为。这项高级实践 [@problem_id:3112792] 将引导你计算生成器的雅可比矩阵 $J(z) = \\frac{\\partial G(z)}{\\partial z}$，它描述了潜向量的无穷小变化如何影响输出图像。通过分析雅可比矩阵的奇异值，我们可以衡量生成器对不同潜空间方向的敏感度，并识别出哪些维度在局部最具有影响力，从而揭示出模型所学习到的数据流形的精细结构。", "problem": "考虑一个深度卷积生成对抗网络 (DCGAN)，其中生成器 $G$ 通过一系列在卷积神经网络中标准的操作，将一个潜向量 $z \\in \\mathbb{R}^d$ 映射到一个空间排列的输出：一个全连接仿射变换、一个修正线性单元 (ReLU) 非线性、一个二维转置卷积和一个双曲正切 ($\\tanh$) 输出非线性。目标是从第一性原理出发，推导雅可比矩阵 $J = \\partial G(z)/\\partial z$，解释其奇异值谱以评估空间特征对潜坐标的局部敏感度，并评估潜维度的冗余性。\n\n从以下基本基础开始：\n- 复合函数的雅可比矩阵链式法则，该法则指出，对于函数复合 $G = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1$，其雅可比矩阵为 $J_G(z) = J_{f_L}(f_{L-1}(\\cdots)) \\cdots J_{f_1}(z)$。\n- 线性映射 $x \\mapsto Ax$ 的雅可比矩阵是矩阵 $A$。\n- 逐元素应用的修正线性单元 (ReLU) 的导数，当激活前的值严格为正时为 $1$，否则为 $0$。\n- 双曲正切的导数是 $\\mathrm{sech}^2(y) = 1 - \\tanh^2(y)$，在 $\\tanh$ 前的激活值 $y$ 处逐元素应用。\n- 二维转置卷积是一个从输入特征图到输出特征图的线性算子，可以表示为一个作用于扁平化输入上的矩阵 $T$。\n\n在下面的所有测试用例中，生成器由以下通用组合定义：\n1. 一个全连接线性映射 $a_0 = W z + b$，其中 $W \\in \\mathbb{R}^{N_{\\text{in}} \\times d}$ 且 $b \\in \\mathbb{R}^{N_{\\text{in}}}$，产生 $N_{\\text{in}}$ 个激活前的特征值。\n2. 一个逐元素的修正线性单元 $h_0 = \\max(a_0, 0)$，被重塑为 $C_{\\text{in}}$ 个输入通道，空间尺寸为 $H_0 \\times W_0$，按通道优先、行优先的顺序排列。\n3. 一个步长 $s = 1$ 且无填充的二维转置卷积，使用卷积核 $K$ 产生一个空间尺寸为 $H_{\\text{out}} \\times W_{\\text{out}}$ 的单一输出通道，其中对于卷积核高度 $k_h$ 和宽度 $k_w$，有 $H_{\\text{out}} = H_0 + k_h - 1$ 和 $W_{\\text{out}} = W_0 + k_w - 1$。转置卷积定义为\n$$\ny[u, v] = \\sum_{c=0}^{C_{\\text{in}}-1} \\sum_{i=0}^{H_0-1} \\sum_{j=0}^{W_0-1} h_0[c, i, j] \\cdot K[c, u-i, v-j],\n$$\n其中索引范围有效，使得 $0 \\le u-i  k_h$ 且 $0 \\le v-j  k_w$；$y$ 是 $\\tanh$ 前的输出。\n4. 一个逐元素的双曲正切 $o = \\tanh(y)$。\n\n在此模型下，关于 $z$ 在给定点 $z$ 的雅可比矩阵是\n$$\nJ(z) = D_{\\tanh}(y) \\, T \\, D_{\\mathrm{ReLU}}(a_0) \\, W,\n$$\n其中 $D_{\\mathrm{ReLU}}(a_0)$ 是一个对角矩阵，其对角线元素在对应的 $a_0$ 分量严格为正时为 $1$，否则为 $0$；$T$ 是转置卷积的矩阵表示，它将扁平化的 $h_0$ 映射到扁平化的 $y$；$D_{\\tanh}(y)$ 是一个对角矩阵，其对角线元素是在扁平化的 $y$ 处计算的 $1 - \\tanh^2(y)$。\n\n您必须实现此映射，并使用奇异值分解 (SVD) 计算 $J(z)$ 的奇异值。将大的奇异值解释为空间特征对某些潜方向的高敏感度，将非常小的奇异值（相对于一个数值阈值）解释为局部冗余或被抑制的潜方向。为了确定数值秩，使用一个阈值 $\\epsilon$，其中如果奇异值 $s \\ge \\epsilon$，则该奇异值对秩有贡献。\n\n您的程序必须完全按照每个测试用例的规定实现生成器，并且必须为每个测试用例计算以下输出：\n- $J(z)$ 的最大奇异值，记为 $s_{\\max}$。\n- SVD 返回的所有奇异值中 $J(z)$ 的最小奇异值（如果 $J(z)$ 是秩亏的，则等于 $0$），记为 $s_{\\min}$。\n- 有效激活的潜维度比例，定义为数值秩除以 $d$，即\n$$\n\\text{秩比例} = \\frac{\\#\\{s_i \\mid s_i \\ge \\epsilon\\}}{d}。\n$$\n将这三个量都表示为十进制数。\n\n最终输出格式必须是包含每个测试用例结果列表的单行。每个测试用例结果是按 $[s_{\\max}, s_{\\min}, \\text{秩比例}]$ 顺序排列的三个十进制数的列表。总输出必须是这些测试用例列表的单个列表，打印为一行，不含额外文本，例如，\n$[\\,[s_{\\max}^{(1)}, s_{\\min}^{(1)}, r^{(1)}],\\, [s_{\\max}^{(2)}, s_{\\min}^{(2)}, r^{(2)}],\\, \\ldots\\,]$。\n\n测试套件规范：\n- 测试用例 1 (正常路径)：\n    - 潜维度 $d = 4$。\n    - 输入通道 $C_{\\text{in}} = 1$，输出通道 $C_{\\text{out}} = 1$。\n    - 空间尺寸 $H_0 = 2$, $W_0 = 2$。\n    - 全连接权重 $W \\in \\mathbb{R}^{4 \\times 4}$ 等于 $0.5 I_4$ (其中 $I_4$ 是 $4 \\times 4$ 单位矩阵)，偏置 $b = 0$。\n    - ReLU 逐元素应用。\n    - 转置卷积核 $K \\in \\mathbb{R}^{1 \\times 1 \\times 2 \\times 2}$，其单一输入通道的卷积核为\n    $$\n    K^{(0)} = \\begin{bmatrix} 0.5  -0.25 \\\\ 0.75  0.25 \\end{bmatrix}.\n    $$\n    - 输出非线性是 $\\tanh$ 逐元素应用。\n    - 潜向量\n    $$\n    z^{(1)} = \\begin{bmatrix} 0.2 \\\\ -0.4 \\\\ 0.1 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    - 数值阈值 $\\epsilon = 10^{-9}$。\n\n- 测试用例 2 (边界门控情况)：\n    - 架构和参数与测试用例 1 相同。\n    - 潜向量\n    $$\n    z^{(2)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n    $$\n    - 数值阈值 $\\epsilon = 10^{-9}$。\n    - 注意：根据 ReLU 在 $0$ 处的导数为 $0$ 的约定，在这种情况下，雅可比矩阵应坍缩为零矩阵。\n\n- 测试用例 3 (多通道边缘情况)：\n    - 潜维度 $d = 4$。\n    - 输入通道 $C_{\\text{in}} = 2$，输出通道 $C_{\\text{out}} = 1$。\n    - 空间尺寸 $H_0 = 2$, $W_0 = 2$。\n    - 全连接权重 $W \\in \\mathbb{R}^{8 \\times 4}$，其行按通道优先、行优先的顺序为 2 个输入通道指定，每个通道尺寸为 $2 \\times 2$。明确地，\n    $$\n    W = \\begin{bmatrix}\n    0.4  -0.3  0.1  0.0 \\\\\n    0.0  0.2  -0.1  0.5 \\\\\n    -0.2  0.1  0.3  -0.4 \\\\\n    0.3  0.0  0.2  0.1 \\\\\n    -0.1  0.4  0.0  -0.2 \\\\\n    0.2  -0.5  0.3  0.0 \\\\\n    0.0  0.1  -0.3  0.4 \\\\\n    0.5  0.2  0.1  -0.1\n    \\end{bmatrix}, \\quad b = 0.\n    $$\n    - ReLU 逐元素应用。\n    - 转置卷积核 $K \\in \\mathbb{R}^{2 \\times 1 \\times 2 \\times 2}$，每个输入通道一个（聚合到单个输出通道），由下式给出\n    $$\n    K^{(0)} = \\begin{bmatrix} 0.3  -0.1 \\\\ 0.2  0.4 \\end{bmatrix}, \\quad\n    K^{(1)} = \\begin{bmatrix} -0.2  0.5 \\\\ 0.1  -0.3 \\end{bmatrix}.\n    $$\n    - 输出非线性是 $\\tanh$ 逐元素应用。\n    - 潜向量\n    $$\n    z^{(3)} = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.2 \\end{bmatrix}.\n    $$\n    - 数值阈值 $\\epsilon = 10^{-9}$。\n\n实现与输出要求：\n- 在数学上和算法上实现上述生成器，使用链式法则将 $J(z)$ 构建为对角雅可比矩阵和线性算子的乘积，其中 $T$ 通过其对基输入的作​​用显式构建。\n- 通过奇异值分解计算 $J(z)$ 的奇异值。\n- 对于每个测试用例，输出一个列表 $[s_{\\max}, s_{\\min}, \\text{秩比例}]$ 作为十进制数。\n- 程序必须打印包含每个测试用例列表的单个列表的单行，例如，\n$[\\,[s_{\\max}^{(1)}, s_{\\min}^{(1)}, r^{(1)}],\\, [s_{\\max}^{(2)}, s_{\\min}^{(2)}, r^{(2)}],\\, [s_{\\max}^{(3)}, s_{\\min}^{(3)}, r^{(3)}]\\,]$。", "solution": "用户提供的问题是有效的。这是一个定义明确且具有科学依据的练习，旨在应用微分学来分析深度卷积生成对抗网络 (DCGAN) 生成器的局部属性。所有参数和架构细节都已指定，从而能够得到一个唯一且可验证的解。\n\n问题的核心是为给定的生成器模型 $G$ 和潜向量 $z$ 计算雅可比矩阵 $J(z) = \\frac{\\partial G(z)}{\\partial z}$，然后使用其奇异值分解 (SVD) 来分析这个雅可比矩阵。生成器 $G$ 是四个标准神经网络层的复合：\n1. 一个全连接（仿射）层：$a_0 = W z + b$\n2. 一个逐元素的修正线性单元 (ReLU) 激活：$h_0 = \\max(a_0, 0)$\n3. 一个二维转置卷积，这是一个可以表示为矩阵 $T$ 的线性算子：$y = T h_{0,\\text{flat}}$\n4. 一个逐元素的双曲正切 ($\\tanh$) 激活：$o = \\tanh(y)$\n\n因此，生成器是函数复合 $G(z) = \\tanh(T \\cdot \\mathrm{ReLU}(Wz+b))$。根据向量函数求导的链式法则，$G$ 的雅可比矩阵是其各组成层雅可比矩阵的乘积，在适当的中间值处进行评估。问题提供了最终的公式：\n$$\nJ(z) = D_{\\tanh}(y) \\, T \\, D_{\\mathrm{ReLU}}(a_0) \\, W\n$$\n在这里，$D_{\\mathrm{ReLU}}(a_0)$ 和 $D_{\\tanh}(y)$ 是分别表示 ReLU 和 $\\tanh$ 函数逐元素导数的对角矩阵。矩阵 $W$ 和 $T$ 表示全连接层和转置卷积层的线性变换。\n\n为实现该解决方案，我们对每个测试用例遵循以下步骤：\n\n**步骤 1：前向传播**\n首先，我们必须使用给定的潜向量 $z$ 对生成器网络进行一次前向传播，以计算雅可比矩阵所需的中间激活前的值。\n- ReLU 前的激活值计算为 $a_0 = W z + b$。\n- ReLU 后的激活值是 $h_0 = \\max(a_0, 0)$。此向量随后被重塑为形状为 $(C_{\\text{in}}, H_0, W_0)$ 的张量。\n- $\\tanh$ 前的激活值 $y$ 是通过对 $h_0$ 张量应用转置卷积计算得出的。\n- 最终输出是 $o=\\tanh(y)$，尽管这对于雅可比矩阵的计算本身并非严格必需。\n\n**步骤 2：雅可比矩阵构建**\n利用前向传播得到的中间值，我们构建雅可比矩阵乘积中的每个矩阵：\n- $W$：初始全连接层的权重矩阵在每个测试用例中直接给出。\n- $D_{\\mathrm{ReLU}}(a_0)$：这是一个大小为 $N_{\\text{in}} \\times N_{\\text{in}}$ 的对角矩阵，其中 $N_{\\text{in}} = C_{\\text{in}} \\times H_0 \\times W_0$。第 $i$ 个对角元素如果对应的激活前值 $a_{0,i}  0$ 则为 $1$，否则为 $0$，这符合问题为 ReLU 指定的导数。\n- $T$：该矩阵表示转置卷积的线性运算。其维度为 $N_{\\text{out}} \\times N_{\\text{in}}$，其中 $N_{\\text{out}} = H_{\\text{out}} \\times W_{\\text{out}}$。该矩阵的元素 $T_{pq}$ 对应于第 $q$ 个输入单元（在扁平化的 $h_0$ 中）对第 $p$ 个输出单元（在扁平化的 $y$ 中）的影响。这可以通过考虑转置卷积公式来确定。$T$ 在行索引 `out_idx`（对应输出中的空间位置 $(u,v)$）和列索引 `in_idx`（对应输入通道 $c$ 和位置 $(i,j)$）处的元素由连接它们的核的值给出：$K[c, u-i, v-j]$，前提是核索引有效，否则为 $0$。\n- $D_{\\tanh}(y)$：这是一个大小为 $N_{\\text{out}} \\times N_{\\text{out}}$ 的对角矩阵。第 $j$ 个对角元素由双曲正切的导数 $1 - \\tanh^2(y_j)$ 给出，在激活前的值 $y_j$ 处计算。\n\n**步骤 3：雅可比矩阵计算与SVD**\n完整的雅可比矩阵 $J(z)$ 是通过这四个分量的矩阵乘积计算得到的。一旦获得 $J(z)$，我们进行奇异值分解 (SVD)，它将 $J(z)$ 分解为 $U S V^T$，其中 $S$ 是由非负奇异值 $s_i$ 组成的对角矩阵。这些奇异值量化了映射 $G$ 在输入和输出空间的不同正交方向上对空间的拉伸程度。\n\n**步骤 4：奇异值分析**\n计算出的奇异值用于确定所需的度量指标：\n- 最大奇异值 $s_{\\max} = \\max_i s_i$ 表示输出对潜向量变化的最大局部敏感度。\n- 最小奇异值 $s_{\\min} = \\min_i s_i$ 表示最小局部敏感度。如果 $s_{\\min}$ 为零或接近零，则某些潜方向在局部被“坍缩”或对输出没有影响。\n- 秩比例计算为 $J(z)$ 的数值秩除以潜维度 $d$。数值秩是大于或等于小阈值 $\\epsilon = 10^{-9}$ 的奇异值 $s_i$ 的数量。该比例衡量了局部活跃或非冗余的潜维度的比例。\n\n这个完整的流程被封装在提供的 Python 代码中，该代码系统地构建矩阵、计算雅可比矩阵、执行 SVD，并为每个测试用例提取指定的度量指标。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for a suite of test cases.\n    It orchestrates the setup of each case and the final printing of results.\n    \"\"\"\n\n    def _compute_analysis(z, d, W, b, K, C_in, H0, W0, k_h, k_w, epsilon):\n        \"\"\"\n        Helper function to compute the Jacobian and its analysis for a single test case.\n\n        This function performs the forward pass to get intermediate activations,\n        constructs the component matrices of the Jacobian, computes the full\n        Jacobian via matrix multiplication, and finally analyzes its singular\n        value spectrum.\n        \"\"\"\n        # --- 1. Forward Pass to compute intermediate activations ---\n        N_in = C_in * H0 * W0\n\n        # Layer 1: Fully connected\n        a0 = W @ z + b\n\n        # Layer 2: ReLU\n        h0 = np.maximum(a0, 0)\n        h0_tensor = h0.reshape((C_in, H0, W0))\n\n        # Layer 3: Transposed Convolution\n        H_out = H0 + k_h - 1\n        W_out = W0 + k_w - 1\n        N_out = H_out * W_out\n        \n        # Pre-tanh output y\n        y = np.zeros((H_out, W_out))\n        for c_idx in range(C_in):\n            if np.any(h0_tensor[c_idx, :, :]):\n                for i in range(H0):\n                    for j in range(W0):\n                        if h0_tensor[c_idx, i, j] > 0:\n                            for u in range(H_out):\n                                for v in range(W_out):\n                                    ki = u - i\n                                    kj = v - j\n                                    if 0 = ki  k_h and 0 = kj  k_w:\n                                        y[u, v] += h0_tensor[c_idx, i, j] * K[c_idx, 0, ki, kj]\n        \n        y_flat = y.flatten()\n\n        # --- 2. Jacobian Matrix Construction ---\n        \n        # W is given.\n\n        # Diagonal Jacobian of ReLU, D_relu\n        d_relu_diag = (a0 > 0).astype(float)\n        D_relu = np.diag(d_relu_diag)\n\n        # Matrix for Transposed Convolution, T\n        T = np.zeros((N_out, N_in))\n        for c_idx in range(C_in):\n            for i in range(H0):\n                for j in range(W0):\n                    in_idx = c_idx * (H0 * W0) + i * W0 + j\n                    for u in range(H_out):\n                        for v in range(W_out):\n                            out_idx = u * W_out + v\n                            ki = u - i\n                            kj = v - j\n                            if 0 = ki  k_h and 0 = kj  k_w:\n                                T[out_idx, in_idx] = K[c_idx, 0, ki, kj]\n\n        # Diagonal Jacobian of tanh, D_tanh\n        d_tanh_diag = 1 - np.tanh(y_flat)**2\n        D_tanh = np.diag(d_tanh_diag)\n        \n        # Full Jacobian J(z) using the chain rule: J = D_tanh @ T @ D_relu @ W\n        # Dimensions: (N_out x N_out) @ (N_out x N_in) @ (N_in x N_in) @ (N_in x d) -> (N_out x d)\n        J = D_tanh @ T @ D_relu @ W\n\n        # --- 3. Singular Value Decomposition (SVD) and Analysis ---\n        if J.size == 0:\n            s = np.array([])\n        else:\n            s = np.linalg.svd(J, compute_uv=False)\n\n        s_max = np.max(s) if s.size > 0 else 0.0\n        \n        # The number of singular values is min(N_out, d), which is d=4 in all cases.\n        s_min = np.min(s) if s.size > 0 else 0.0\n        \n        numerical_rank = np.sum(s >= epsilon)\n        rank_fraction = numerical_rank / d\n        \n        return [float(s_max), float(s_min), float(rank_fraction)]\n\n    # --- Test Suite Specification ---\n\n    # Test Case 1: Happy path\n    d1 = 4\n    z1 = np.array([0.2, -0.4, 0.1, 0.3])\n    W1 = 0.5 * np.identity(4)\n    b1 = np.zeros(4)\n    K1_mat = np.array([[0.5, -0.25], [0.75, 0.25]])\n    K1 = K1_mat.reshape(1, 1, 2, 2)\n    case1_params = {\n        \"z\": z1, \"d\": d1, \"W\": W1, \"b\": b1, \"K\": K1,\n        \"C_in\": 1, \"H0\": 2, \"W0\": 2, \"k_h\": 2, \"k_w\": 2, \"epsilon\": 1e-9\n    }\n    result1 = _compute_analysis(**case1_params)\n\n    # Test Case 2: Boundary gating case (z=0)\n    z2 = np.zeros(4)\n    case2_params = {**case1_params, \"z\": z2}\n    result2 = _compute_analysis(**case2_params)\n\n    # Test Case 3: Multi-channel edge case\n    d3 = 4\n    z3 = np.array([0.1, -0.2, 0.05, 0.2])\n    W3 = np.array([\n        [0.4, -0.3, 0.1, 0.0], [0.0, 0.2, -0.1, 0.5],\n        [-0.2, 0.1, 0.3, -0.4], [0.3, 0.0, 0.2, 0.1],\n        [-0.1, 0.4, 0.0, -0.2], [0.2, -0.5, 0.3, 0.0],\n        [0.0, 0.1, -0.3, 0.4], [0.5, 0.2, 0.1, -0.1]\n    ])\n    b3 = np.zeros(8)\n    K3_c0 = np.array([[0.3, -0.1], [0.2, 0.4]])\n    K3_c1 = np.array([[-0.2, 0.5], [0.1, -0.3]])\n    K3 = np.stack([K3_c0, K3_c1]).reshape(2, 1, 2, 2)\n    case3_params = {\n        \"z\": z3, \"d\": d3, \"W\": W3, \"b\": b3, \"K\": K3,\n        \"C_in\": 2, \"H0\": 2, \"W0\": 2, \"k_h\": 2, \"k_w\": 2, \"epsilon\": 1e-9\n    }\n    result3 = _compute_analysis(**case3_params)\n\n    results = [result1, result2, result3]\n    \n    # Final print statement in the exact required format.\n    # str() on a list of floats includes spaces, and join adds commas.\n    # The final string is a valid representation of a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112792"}]}