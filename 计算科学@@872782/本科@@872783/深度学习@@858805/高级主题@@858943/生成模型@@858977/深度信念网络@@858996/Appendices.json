{"hands_on_practices": [{"introduction": "深度信念网络（DBN）由多个堆叠的受限玻尔兹曼机（RBM）组成，其规模直接影响训练所需的计算资源。在设计和实现任何深度学习模型之前，估算其参数数量和计算复杂性是至关重要的一步，这有助于我们做出关于模型架构和可行性的明智决策。通过这个练习[@problem_id:3112338]，你将学习如何根据网络的基本维度（如层数、可见单元数和隐藏单元数）来量化一个DBN的“成本”，并探索稀疏连接作为一种降低计算开销的有效策略。", "problem": "考虑一个由 $L$ 个受限玻尔兹曼机 (RBM) 堆叠而成的深度信念网络 (DBN)，其中可见层的大小为 $n_{v}$，每个隐藏层的大小为 $n_{h}$。每个 RBM 是一个二分图模型，其能量函数为 $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$，其中 $v \\in \\{0,1\\}^{n_{v}}$ 或 $\\{0,1\\}^{n_{h}}$（取决于层），$h \\in \\{0,1\\}^{n_{h}}$，$W$ 是连接两层的权重矩阵，$b$ 和 $c$ 分别是可见偏置和隐藏偏置。预训练是使用对比散度 (CD) 逐层进行的，具体来说，每层每个数据样本使用 $k$ 步对比散度 (CD-$k$) 。假设数据集大小为 $N$ 个样本，每个 epoch 每层进行一次参数更新。\n\n从这些定义和 RBM 的二分结构出发，并且仅使用关于矩阵-向量乘法成本的公认事实，执行以下操作：\n\n1. 推导稠密 DBN 的可训练参数总数，记为 $P_{\\text{dense}}$，计算所有 $L$ 个 RBM 中的所有权重和偏置。\n2. 在以下科学合理的假设下，推导稠密 DBN 的 CD-$k$ 预训练每轮 (epoch) 的浮点运算次数的主导阶表达式，记为 $C_{\\text{dense}}$：\n   - 每个 RBM、每个数据样本、每个 CD 步骤中的主要计算包括两次矩阵-向量乘法（上行传播 $W^{\\top}v$ 和下行传播 $W h$），且成本与 $W$ 中非零项的数量成线性关系。\n   - 非矩阵运算（加法、激活函数求值）贡献一个常数乘法因子，该因子可以被吸收到一个抽象常数中。\n   引入一个正常数 $\\alpha$，它捕捉了每个数据样本、每个 CD 步骤中每条边的计算成本，并用 $n_{v}$、$n_{h}$、$L$、$N$、$k$ 和 $\\alpha$ 来表示 $C_{\\text{dense}}$。\n3. 提出一种稀疏连接方案，其中在每个 RBM 中，每个潜在的可见-隐藏边以概率 $p \\in (0,1)$ 独立存在。在此模型下，推导可训练参数的总期望数 $P_{\\text{sparse}}$ 和每轮浮点运算次数的期望值 $C_{\\text{sparse}}$。\n4. 最后，计算比率 $R=C_{\\text{sparse}}/C_{\\text{dense}}$ 的闭式表达式。\n\n你的最终答案必须是 $R$ 的单一符号表达式。不需要数值近似或四舍五入。", "solution": "我们从受限玻尔兹曼机 (RBM) 和深度信念网络 (DBN) 的结构定义开始。RBM 是二分的：所有可见单元连接到所有隐藏单元，并且层内没有横向连接。能量函数 $E(v,h)=-v^{\\top}Wh-b^{\\top}v-c^{\\top}h$ 意味着参数集由权重矩阵 $W$ 和偏置向量 $b$ 和 $c$ 组成。$W$ 的大小由所连接层的大小决定。\n\n一个 DBN 预训练过程堆叠了 $L$ 个 RBM：第一个 RBM 连接大小为 $n_{v}$ 的可见层和大小为 $n_{h}$ 的第一个隐藏层，随后的每个 RBM 连接一个大小为 $n_{h}$ 的隐藏层到下一个大小为 $n_{h}$ 的隐藏层。因此，总共有 $L$ 个 RBM：一个维度为 $n_{v} \\times n_{h}$，以及 $L-1$ 个维度为 $n_{h} \\times n_{h}$。\n\n步骤 1：参数计数 $P_{\\text{dense}}$。\n\n对于第一个 RBM：\n- 权重：$n_{v} \\times n_{h}$ 个条目。\n- 偏置：$n_{v}$ 个可见偏置和 $n_{h}$ 个隐藏偏置。\n\n对于剩下的 $L-1$ 个 RBM 中的每一个：\n- 权重：$n_{h} \\times n_{h}$ 个条目。\n- 偏置：每个 RBM 有 $n_{h}$ 个可见偏置（此时下方的隐藏层充当“可见”层）和 $n_{h}$ 个隐藏偏置。然而，在堆叠 RBM 的预训练设置中，每一层都有自己的偏置向量。在整个堆栈中，总偏置是每个 RBM 的所有可见层偏置和所有隐藏层偏置的总和。在各层之间进行唯一聚合，每一层贡献一个偏置向量。有 1 个可见层（大小为 $n_{v}$）和 $L$ 个隐藏层（每个大小为 $n_{h}$），因此总偏置数为 $n_{v} + L n_{h}$。\n\n因此，在稠密连接下，参数总数为\n$$\nP_{\\text{dense}} = \\underbrace{n_{v} n_{h} + (L-1) n_{h}^{2}}_{\\text{权重}} + \\underbrace{n_{v} + L n_{h}}_{\\text{偏置}}。\n$$\n\n步骤 2：每轮复杂度 $C_{\\text{dense}}$。\n\n在使用大小为 $N$ 的数据集进行对比散度 (CD-$k$) 训练的每一轮中，主要的计算是每个 RBM、每个数据样本、每个 CD 步骤中的两次矩阵-向量乘积：上行传播 $W^{\\top} v$ 和下行传播 $W h$。每次矩阵-向量乘积的成本与 $W$ 中非零项的数量成比例。令 $\\alpha>0$ 表示比例常数，捕捉每个数据样本、每个 CD 步骤中每条边的累积成本，包括上行和下行传播。\n\n对于第一个 RBM，边的数量等于权重的数量 $n_{v} n_{h}$。对于剩下的 $L-1$ 个 RBM 中的每一个，边的数量是 $n_{h}^{2}$。将整个堆栈的边相加，得到 $n_{v} n_{h} + (L-1) n_{h}^{2}$。\n\n因此，稠密连接的每轮主导阶复杂度为\n$$\nC_{\\text{dense}} = \\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\n步骤 3：边概率为 $p$ 的稀疏连接。\n\n在所提出的稀疏连接方案中，RBM 中的每个潜在可见-隐藏边以概率 $p \\in (0,1)$ 独立存在。边的期望数量（因此也是权重的期望数量）变为稠密计数的 $p$ 倍，因为独立伯努利指示变量之和的期望等于它们的期望之和。\n\n因此，整个堆栈的期望权重计数为\n$$\n\\mathbb{E}[\\text{权重}] = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n偏置保持不变，因为在此方案中它们不被剪枝。因此，\n$$\nP_{\\text{sparse}} = p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right) + n_{v} + L n_{h}.\n$$\n\n对于复杂度，因为每轮的主导成本与边的数量成线性关系，所以期望的每轮成本是\n$$\nC_{\\text{sparse}} = \\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right).\n$$\n\n步骤 4：比率 $R=C_{\\text{sparse}}/C_{\\text{dense}}$。\n\n使用上面的表达式，\n$$\nR = \\frac{C_{\\text{sparse}}}{C_{\\text{dense}}} = \\frac{\\alpha \\, N \\, k \\, p \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)}{\\alpha \\, N \\, k \\left(n_{v} n_{h} + (L-1) n_{h}^{2}\\right)} = p.\n$$\n\n因此，在所提出的稀疏连接下，计算缩减因子的闭式表达式为 $p$。", "answer": "$$\\boxed{p}$$", "id": "3112338"}, {"introduction": "深度信念网络的一个核心特性是其双重角色：它既能通过“识别”向上传递来学习数据的抽象表示，又能通过“生成”向​​下传递来重构原始数据。这两个过程之间的对称性至关重要，通常通过“权重绑定”（tied weights）来实现，即生成模型的权重是识别模型权重的转置。在这个动手实践[@problem_id:3112369]中，你将通过编码实现这两个过程，并亲眼见证权重绑定被破坏时（从轻微扰动到严重不匹配），模型的生成一致性会如何受到影响，从而深刻理解这一关键设计原则。", "problem": "考虑一个两层深度信念网络 (DBN)，其所有层都使用二元单元，该网络通过堆叠两个受限玻尔兹曼机 (RBM) 构建。每个 RBM 对可见单元和隐藏单元使用伯努利变量，并使用 logistic 激活函数。其基础是 RBM 的基于能量的模型：对于可见向量 $\\mathbf{v} \\in \\{0,1\\}^d$ 和隐藏向量 $\\mathbf{h} \\in \\{0,1\\}^m$，能量由下式给出\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h},\n$$\n并且由于其二分结构，条件分布可以分解为：\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right),\\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\n其中 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ 是 logistic 函数。\n\n在一个具有可见层 $\\mathbf{v} \\in \\{0,1\\}^{d}$、第一隐藏层 $\\mathbf{h}^{(1)} \\in \\{0,1\\}^{m}$ 和第二隐藏层 $\\mathbf{h}^{(2)} \\in \\{0,1\\}^{n}$ 的两层 DBN 中，考虑一个确定性平均场上行通路（识别模型），随后是一个确定性下行通路（生成模型）。上行通路使用识别权重 $W_{1r} \\in \\mathbb{R}^{d \\times m}$ 和 $W_{2r} \\in \\mathbb{R}^{m \\times n}$ 以及偏置 $\\mathbf{b}_1 \\in \\mathbb{R}^{m}$ 和 $\\mathbf{b}_2 \\in \\mathbb{R}^{n}$：\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right), \\quad\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\n下行通路使用生成权重 $W_{2g} \\in \\mathbb{R}^{n \\times m}$ 和 $W_{1g} \\in \\mathbb{R}^{m \\times d}$ 以及偏置 $\\mathbf{c}_1 \\in \\mathbb{R}^{m}$ 和 $\\mathbf{c}_0 \\in \\mathbb{R}^{d}$：\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right), \\quad\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\n我们为数据集 $\\mathcal{D} = \\{\\mathbf{v}^{(k)}\\}_{k=1}^{K}$ 定义生成一致性得分，即原始可见向量与其经过上下行通路后的确定性重构 $\\tilde{\\mathbf{v}}^{(k)}$ 之间的逐分量平均二元交叉熵：\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\n为确保数值稳定性，在计算对数之前，使用 $\\epsilon = 10^{-12}$ 并将每个 $\\tilde{v}^{(k)}_i$ 替换为 $\\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$。\n\n您的任务是编写一个程序，为四个测试用例计算 $J$。这些用例探讨了上行和下行通路之间权重绑定与不绑定的情况，以衡量其对生成一致性的影响。使用以下固定的数据集，其中 $K = 3$，$d = 4$：\n$$\n\\mathbf{v}^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n\\mathbf{v}^{(2)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n\\mathbf{v}^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\n设 $m = 3$，$n = 2$。对于所有情况，上行（识别）参数固定如下：\n$$\nW_{1r} = \\begin{bmatrix}\n0.8  -0.4  0.3 \\\\\n0.1  0.5  -0.6 \\\\\n0.7  -0.2  0.2 \\\\\n-0.5  0.3  0.4\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.6  -0.3 \\\\\n0.4  0.2 \\\\\n-0.7  0.5\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}.\n$$\n评估以下四种关于下行（生成）参数的情况：\n\n情况 1（权重绑定，中等幅度）：\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.6  0.4  -0.7 \\\\ -0.3  0.2  0.5 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} -0.05 \\\\ 0.1 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.8  0.1  0.7  -0.5 \\\\\n-0.4  0.5  -0.2  0.3 \\\\\n0.3  -0.6  0.2  0.4\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n情况 2（权重不绑定，小扰动）：\n$$\nW_{2g} = W_{2r}^\\top + \\Delta_2,\\quad\n\\Delta_2 = \\begin{bmatrix} 0.01  -0.02  0.02 \\\\ -0.02  0.03  -0.01 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} -0.02 \\\\ 0.05 \\\\ -0.01 \\end{bmatrix},\\quad\nW_{1g} = W_{1r}^\\top + \\Delta_1,\n$$\n$$\n\\Delta_1 = \\begin{bmatrix}\n0.02  -0.01  0.03  -0.02 \\\\\n-0.03  0.02  0.01  -0.01 \\\\\n0.01  0.00  -0.02  0.03\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.0 \\\\ -0.02 \\end{bmatrix}.\n$$\n\n情况 3（权重不绑定，严重不匹配）：\n$$\nW_{2g} = - W_{2r}^\\top + M_2,\\quad M_2 = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.2  -0.2  0.2 \\end{bmatrix},\n$$\n$$\n\\mathbf{c}_1 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\end{bmatrix},\\quad\nW_{1g} = - W_{1r}^\\top + M_1,\n$$\n$$\nM_1 = \\begin{bmatrix}\n0.0  0.0  0.0  0.0 \\\\\n0.1  -0.1  0.1  -0.1 \\\\\n-0.2  0.2  -0.2  0.2\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} -0.3 \\\\ 0.3 \\\\ -0.3 \\\\ 0.3 \\end{bmatrix}.\n$$\n\n情况 4（边界处的权重绑定，全零）：\n$$\nW_{2g} = W_{2r}^\\top = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 0.0  0.0  0.0 \\end{bmatrix},\\quad\n\\mathbf{c}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{1g} = W_{1r}^\\top = \\begin{bmatrix}\n0.0  0.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.0 \\\\\n0.0  0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{c}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n同时上行（识别）参数设置为\n$$\nW_{1r} = \\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0 \\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\n$$\n$$\nW_{2r} = \\begin{bmatrix}\n0.0  0.0 \\\\\n0.0  0.0 \\\\\n0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{b}_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\n按照上述规定，实现确定性平均场的上行和下行映射，在交叉熵计算中应用数值稳定器 $\\epsilon$，并计算每种情况下的生成一致性得分 $J$。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$）。每个结果都必须是浮点数。在最终输出中将每个结果四舍五入到 6 位小数。", "solution": "我们从受限玻尔兹曼机 (RBM) 的基本定义开始。对于伯努利可见单元 $\\mathbf{v} \\in \\{0,1\\}^d$ 和伯努利隐藏单元 $\\mathbf{h} \\in \\{0,1\\}^m$，RBM 的能量函数为\n$$\nE(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{c}^\\top \\mathbf{v} - \\mathbf{b}^\\top \\mathbf{h} - \\mathbf{v}^\\top W \\mathbf{h}.\n$$\n由于 RBM 图是二分的，条件分布可以按单元分解。通过边缘化并使用玻尔兹曼分布 $p(\\mathbf{x}) \\propto \\exp(-E(\\mathbf{x}))$，可以得到 sigmoidal 条件分布：\n$$\np(h_j = 1 \\mid \\mathbf{v}) = \\sigma\\!\\left(b_j + \\sum_{i=1}^{d} W_{ij} v_i \\right), \\quad\np(v_i = 1 \\mid \\mathbf{h}) = \\sigma\\!\\left(c_i + \\sum_{j=1}^{m} W_{ij} h_j \\right),\n$$\n其中 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$。这是因为对于一个伯努利单元，其对数几率是由相应偏置加上来自连接单元的加权和给出的仿射输入。\n\n深度信念网络 (DBN) 是由 RBM 堆叠而成。在一个两层 DBN 中，上行通路（识别模型）使用平均场期望（sigmoid 概率）将可见层确定性地映射到顶层隐藏层。使用识别权重 $W_{1r}$ 和 $W_{2r}$ 以及上行偏置 $\\mathbf{b}_1$ 和 $\\mathbf{b}_2$，上行变换为：\n$$\n\\mathbf{h}^{(1)} = \\sigma\\!\\left(\\mathbf{v}^\\top W_{1r} + \\mathbf{b}_1\\right),\n$$\n$$\n\\mathbf{h}^{(2)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(1)}\\right)^\\top W_{2r} + \\mathbf{b}_2\\right).\n$$\n下行通路（生成模型）通过生成权重 $W_{2g}$ 和 $W_{1g}$ 以及下行偏置 $\\mathbf{c}_1$ 和 $\\mathbf{c}_0$ 将顶层隐藏层映射回可见层：\n$$\n\\tilde{\\mathbf{h}}^{(1)} = \\sigma\\!\\left(\\left(\\mathbf{h}^{(2)}\\right)^\\top W_{2g} + \\mathbf{c}_1\\right),\n$$\n$$\n\\tilde{\\mathbf{v}} = \\sigma\\!\\left(\\left(\\tilde{\\mathbf{h}}^{(1)}\\right)^\\top W_{1g} + \\mathbf{c}_0\\right).\n$$\n权重绑定对应于 $W_{1g} = W_{1r}^\\top$ 和 $W_{2g} = W_{2r}^\\top$，这在识别和生成映射之间强制实现对称性。权重不绑定则打破了这种对称性，允许 $W_{1g}$ 和 $W_{2g}$ 偏离转置矩阵。\n\n为了量化生成一致性，我们使用原始可见向量与其经过上下行通路后的重构之间的逐分量平均二元交叉熵：\n$$\nJ = \\frac{1}{K d} \\sum_{k=1}^{K} \\sum_{i=1}^{d} \\left[ - v^{(k)}_i \\log\\!\\left(\\tilde{v}^{(k)}_i\\right) - \\left(1 - v^{(k)}_i\\right) \\log\\!\\left(1 - \\tilde{v}^{(k)}_i\\right) \\right].\n$$\n该表达式源于目标 $v^{(k)}_i$ 的独立伯努利输出 $\\tilde{v}^{(k)}_i$ 的负对数似然，并在样本和维度上进行平均。为了数值稳定性，在计算 $\\log(\\cdot)$ 之前，我们通过 $\\tilde{v}^{(k)}_i \\leftarrow \\min\\!\\left(\\max\\!\\left(\\tilde{v}^{(k)}_i, \\epsilon\\right), 1 - \\epsilon\\right)$（其中 $\\epsilon = 10^{-12}$）来限制概率值，确保参数位于 $(0,1)$ 区间内。\n\n从算法上讲，对于每种情况：\n$1.$ 对于每个数据向量 $\\mathbf{v}^{(k)}$，使用 $W_{1r}$ 和 $\\mathbf{b}_1$ 计算 $\\mathbf{h}^{(1)}$。\n$2.$ 使用 $W_{2r}$ 和 $\\mathbf{b}_2$ 计算 $\\mathbf{h}^{(2)}$。\n$3.$ 使用 $W_{2g}$ 和 $\\mathbf{c}_1$ 计算 $\\tilde{\\mathbf{h}}^{(1)}$。\n$4.$ 使用 $W_{1g}$ 和 $\\mathbf{c}_0$ 计算 $\\tilde{\\mathbf{v}}$。\n$5.$ 将 $\\tilde{\\mathbf{v}}$ 的每个分量裁剪到 $[\\epsilon, 1 - \\epsilon]$ 范围内。\n$6.$ 累加该 $\\mathbf{v}^{(k)}$ 相对于 $\\tilde{\\mathbf{v}}$ 的交叉熵，然后对 $K$ 取平均值并除以 $d$，得到 $J$。\n\n从原理上分析，权重绑定能提高一致性，因为上行映射近似于在 RBM 条件下的充分统计量推断，而使用转置权重的下行映射则强制执行一个匹配的线性变换，以回到可见空间。在具有中等权重和平衡偏置的平均场设置中，这倾向于减少 $\\mathbf{v}$ 和 $\\tilde{\\mathbf{v}}$ 之间的失真。不绑定权重中的小扰动会引入不对称性，从而轻微降低 $J$ 的值（译注：此处原文为 degrade J，应理解为使 J 变差，即交叉熵增大）。严重的失配会反转或扭曲映射，导致 $\\tilde{\\mathbf{v}}$ 反映一个不同的生成流形，从而显著增加交叉熵。全零权重（和零偏置）的边界情况会在所有单元产生 $\\sigma(0) = 0.5$ 的输出，使得重构不包含任何信息，并导致一个固定的交叉熵，该交叉熵完全由伯努利目标相对于 0.5 预测值的熵决定。\n\n该程序直接实现了这些步骤，对每种情况使用所提供的矩阵和偏置，为固定数据集计算 $J$，并将四个结果以单个方括号括起来的、逗号分隔的列表形式打印出来，每个值都四舍五入到 6 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigmoid(x):\n    # Numerically stable sigmoid using float64\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef clamp_probs(p, eps=1e-12):\n    # Clamp probabilities to (eps, 1-eps) for numerical stability in logs\n    return np.clip(p, eps, 1.0 - eps)\n\ndef generative_consistency_score(V, W1r, b1, W2r, b2, W2g, c1, W1g, c0, eps=1e-12):\n    \"\"\"\n    Compute mean component-wise binary cross-entropy between original visible vectors V\n    and reconstructions after upward-downward deterministic mean-field pass.\n    \"\"\"\n    K, d = V.shape\n    total_ce = 0.0\n    for k in range(K):\n        v = V[k]  # shape (d,)\n        # Upward pass: v -> h1 -> h2\n        h1_input = v @ W1r + b1  # shape (m,)\n        h1 = sigmoid(h1_input)\n        h2_input = h1 @ W2r + b2  # shape (n,)\n        h2 = sigmoid(h2_input)\n        # Downward pass: h2 -> h1_down -> v_hat\n        h1_down_input = h2 @ W2g + c1  # shape (m,)\n        h1_down = sigmoid(h1_down_input)\n        v_hat_input = h1_down @ W1g + c0  # shape (d,)\n        v_hat = sigmoid(v_hat_input)\n        v_hat = clamp_probs(v_hat, eps=eps)\n        # Binary cross-entropy per component\n        ce_vec = -(v * np.log(v_hat) + (1.0 - v) * np.log(1.0 - v_hat))\n        total_ce += np.sum(ce_vec)\n    # Mean across samples and dimensions\n    J = total_ce / (K * d)\n    return J\n\ndef solve():\n    # Fixed dataset V (K=3, d=4)\n    V = np.array([\n        [1.0, 0.0, 1.0, 0.0],\n        [0.0, 1.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0, 0.0]\n    ], dtype=np.float64)\n\n    # Recognition (upward) parameters for cases 1-3\n    W1r_base = np.array([\n        [0.8, -0.4, 0.3],\n        [0.1,  0.5, -0.6],\n        [0.7, -0.2, 0.2],\n        [-0.5, 0.3, 0.4]\n    ], dtype=np.float64)\n    b1_base = np.array([0.1, -0.2, 0.05], dtype=np.float64)\n\n    W2r_base = np.array([\n        [0.6, -0.3],\n        [0.4,  0.2],\n        [-0.7, 0.5]\n    ], dtype=np.float64)\n    b2_base = np.array([0.0, 0.1], dtype=np.float64)\n\n    # Case 1: tied weights\n    W2g_1 = W2r_base.T.copy()\n    c1_1 = np.array([-0.05, 0.1, 0.0], dtype=np.float64)\n    W1g_1 = W1r_base.T.copy()\n    c0_1 = np.array([0.0, 0.0, 0.0, 0.0], dtype=np.float64)\n\n    # Case 2: untied (small perturbations)\n    Delta2 = np.array([\n        [0.01, -0.02, 0.02],\n        [-0.02, 0.03, -0.01]\n    ], dtype=np.float64)\n    W2g_2 = W2r_base.T + Delta2\n    c1_2 = np.array([-0.02, 0.05, -0.01], dtype=np.float64)\n    Delta1 = np.array([\n        [0.02, -0.01, 0.03, -0.02],\n        [-0.03,  0.02, 0.01, -0.01],\n        [0.01,  0.00, -0.02, 0.03]\n    ], dtype=np.float64)\n    W1g_2 = W1r_base.T + Delta1\n    c0_2 = np.array([0.02, -0.01, 0.0, -0.02], dtype=np.float64)\n\n    # Case 3: untied (severe mismatch)\n    M2 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W2g_3 = -W2r_base.T + M2\n    c1_3 = np.array([0.5, -0.5, 0.5], dtype=np.float64)\n    M1 = np.array([\n        [0.0,  0.0,  0.0,  0.0],\n        [0.1, -0.1,  0.1, -0.1],\n        [-0.2, 0.2, -0.2, 0.2]\n    ], dtype=np.float64)\n    W1g_3 = -W1r_base.T + M1\n    c0_3 = np.array([-0.3, 0.3, -0.3, 0.3], dtype=np.float64)\n\n    # Case 4: boundary all zeros (tied zeros)\n    W1r_4 = np.zeros((4, 3), dtype=np.float64)\n    b1_4 = np.zeros(3, dtype=np.float64)\n    W2r_4 = np.zeros((3, 2), dtype=np.float64)\n    b2_4 = np.zeros(2, dtype=np.float64)\n    W2g_4 = W2r_4.T.copy()  # zeros\n    c1_4 = np.zeros(3, dtype=np.float64)\n    W1g_4 = W1r_4.T.copy()  # zeros\n    c0_4 = np.zeros(4, dtype=np.float64)\n\n    test_cases = [\n        # Case 1: tied weights moderate\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_1, \"c1\": c1_1, \"W1g\": W1g_1, \"c0\": c0_1\n        },\n        # Case 2: untied small perturbations\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_2, \"c1\": c1_2, \"W1g\": W1g_2, \"c0\": c0_2\n        },\n        # Case 3: untied severe mismatch\n        {\n            \"W1r\": W1r_base, \"b1\": b1_base, \"W2r\": W2r_base, \"b2\": b2_base,\n            \"W2g\": W2g_3, \"c1\": c1_3, \"W1g\": W1g_3, \"c0\": c0_3\n        },\n        # Case 4: boundary all zeros (tied zeros)\n        {\n            \"W1r\": W1r_4, \"b1\": b1_4, \"W2r\": W2r_4, \"b2\": b2_4,\n            \"W2g\": W2g_4, \"c1\": c1_4, \"W1g\": W1g_4, \"c0\": c0_4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        J = generative_consistency_score(\n            V,\n            case[\"W1r\"], case[\"b1\"],\n            case[\"W2r\"], case[\"b2\"],\n            case[\"W2g\"], case[\"c1\"],\n            case[\"W1g\"], case[\"c0\"],\n            eps=1e-12\n        )\n        results.append(f\"{J:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3112369"}, {"introduction": "训练一个深度学习模型只是第一步，更深入的挑战在于解释模型学到了什么。一个训练有素的DBN的权重矩阵中编码了关于数据内在结构的丰富信息。在这个富有启发性的练习[@problem_id:3112287]中，我们将跳出传统的图像或文本应用，探索一个假设场景：使用RBM来为学生知识建模。你将实现一个算法，通过分析RBM权重来推断不同概念之间的先修关系，并将你的发现与“地面真实”的课程大纲进行验证，这让你能体验到如何从模型参数中挖掘出有意义的洞察。", "problem": "学生知识模型被实现为深度信念网络 (Deep Belief Network, DBN) 的底层，该网络是受限玻尔兹曼机 (Restricted Boltzmann Machines, RBMs) 的堆叠。考虑一个具有二元可见单元和二元隐藏单元的单一受限玻尔兹曼机 (RBM)。RBM 的能量函数定义为 $$E(\\mathbf{v},\\mathbf{h})=-\\mathbf{v}^{\\top}\\mathbf{W}\\mathbf{h}-\\mathbf{b}_v^{\\top}\\mathbf{v}-\\mathbf{b}_h^{\\top}\\mathbf{h},$$ 其中 $\\mathbf{v}\\in\\{0,1\\}^{n_v}$ 是可见单元，表示对 $n_v$ 个概念的掌握程度；$\\mathbf{h}\\in\\{0,1\\}^{n_h}$ 是隐藏单元，表示 $n_h$ 种潜在技能；$\\mathbf{W}\\in\\mathbb{R}^{n_v\\times n_h}$ 是权重矩阵；$\\mathbf{b}_v\\in\\mathbb{R}^{n_v}$ 和 $\\mathbf{b}_h\\in\\mathbb{R}^{n_h}$ 是偏置。在给定隐藏配置 $\\mathbf{h}$ 的条件下，可见单元 $i$ 的条件分布为 $$\\mathbb{P}(v_i=1\\mid\\mathbf{h})=\\sigma\\left(b_{v,i}+\\sum_{j=1}^{n_h}W_{ij}h_j\\right),$$ 其中 $\\sigma(x)=\\frac{1}{1+\\exp(-x)}$ 是逻辑 sigmoid 函数。正权重 $W_{ij}>0$ 表示在给定 $\\mathbf{h}$ 的条件下，隐藏单元 $j$ 的存在会增加掌握概念 $i$ 的概率，而负权重则表示抑制。\n\n概念的先决条件关系将根据 RBM 权重模式按如下方式推断。对于每个概念索引 $i\\in\\{0,\\dots,n_v-1\\}$，定义其正支持集 $$P_i=\\{j\\in\\{0,\\dots,n_h-1\\}\\,:\\,W_{ij}\\ge \\tau\\},$$ 其中 $\\tau>0$ 是一个阈值，指定了一项技能被认为对一个概念具有支持作用所需的最小正影响。推断出的有向先决条件关系 $k\\to i$ 必须遵循以下约束：概念 $k$ 的几乎所有支持技能也必须支持概念 $i$，并且概念 $i$ 必须严格依赖于概念 $k$ 之外的额外支持技能。使用两个参数来实现所得规则：一个覆盖率阈值 $\\alpha\\in(0,1]$，用于强制概念 $k$ 的足够比例的支持技能与概念 $i$ 共享；以及一个严格的大小余量 $\\beta\\in\\mathbb{N}$，用于强制概念 $i$ 比概念 $k$ 至少多依赖 $\\beta$ 个支持技能。\n\n验证将与一个表示为有向边集 $C\\subseteq\\{0,\\dots,n_v-1\\}\\times\\{0,\\dots,n_v-1\\}$ 的基准课程进行对比。对于给定的预测边集 $\\widehat{C}$，计算真正例 $\\mathrm{TP}=|\\widehat{C}\\cap C|$、假正例 $\\mathrm{FP}=|\\widehat{C}\\setminus C|$ 和假反例 $\\mathrm{FN}=|C\\setminus \\widehat{C}|$ 的数量。将精确率定义为 $$\\mathrm{Prec}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}},$$ 召回率定义为 $$\\mathrm{Rec}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}},$$ $F_1$ 分数定义为 $$F_1=\\frac{2\\cdot \\mathrm{Prec}\\cdot \\mathrm{Rec}}{\\mathrm{Prec}+\\mathrm{Rec}}.$$ 按照惯例，如果 $C$ 和 $\\widehat{C}$ 均为空，则定义 $F_1=1.0$；如果其中只有一个为空，则定义 $F_1=0.0$。\n\n您的任务是编写一个完整的程序，该程序：\n- 使用阈值 $\\tau$ 从给定的权重矩阵 $\\mathbf{W}$ 构建正支持集 $P_i$。\n- 当 $P_i$ 对 $P_k$ 的覆盖率满足覆盖率阈值 $\\alpha$ 并且与 $\\beta$ 相关的严格大小余量条件成立时，推断有向先决条件 $k\\to i$，不包括 $|P_k|=0$ 的情况。\n- 将推断出的边集 $\\widehat{C}$ 与课程 $C$ 进行比较，并计算 $F_1$ 分数，结果为四舍五入到四位小数的浮点数。\n\n使用以下参数值测试套件。每个测试用例提供 $(\\mathbf{W},\\tau,\\alpha,\\beta,C)$：\n\n- 测试用例 1（一般情况，清晰的超集）：$$\\mathbf{W}=\\begin{bmatrix}0.9  0.0  0.1\\\\0.8  0.7  0.0\\\\0.0  0.0  0.9\\end{bmatrix},\\quad \\tau=0.5,\\quad \\alpha=1.0,\\quad \\beta=1,\\quad C=\\{(0,1)\\}.$$\n- 测试用例 2（边界相等，无边）：$$\\mathbf{W}=\\begin{bmatrix}0.9  0.7  0.0\\\\0.9  0.7  0.0\\\\0.0  0.0  0.0\\end{bmatrix},\\quad \\tau=0.5,\\quad \\alpha=1.0,\\quad \\beta=1,\\quad C=\\varnothing.$$\n- 测试用例 3（含噪声的权重，多个先决条件）：$$\\mathbf{W}=\\begin{bmatrix}0.6  -0.2  0.0\\\\0.61  0.55  -0.3\\\\0.0  0.51  0.0\\end{bmatrix},\\quad \\tau=0.5,\\quad \\alpha=1.0,\\quad \\beta=1,\\quad C=\\{(0,1),(2,1)\\}.$$\n- 测试用例 4（有重叠但非子集，无边）：$$\\mathbf{W}=\\begin{bmatrix}0.6  0.0  0.6\\\\0.6  0.6  0.0\\\\0.0  0.6  0.6\\end{bmatrix},\\quad \\tau=0.5,\\quad \\alpha=1.0,\\quad \\beta=1,\\quad C=\\varnothing.$$\n- 测试用例 5（近似覆盖允许部分推断，一个额外的预测边）：$$\\mathbf{W}=\\begin{bmatrix}0.6  0.6  0.0\\\\0.0  0.6  0.0\\\\0.6  0.0  0.0\\end{bmatrix},\\quad \\tau=0.5,\\quad \\alpha=0.5,\\quad \\beta=1,\\quad C=\\{(2,0)\\}.$$\n\n您的程序应生成单行输出，其中包含五个测试用例的 $F_1$ 分数，格式为方括号内以逗号分隔的列表（例如，$\\texttt{[0.9500,0.8750,0.6667,1.0000,0.5000]}$）。所有数值结果必须是四舍五入到四位小数的浮点数。不涉及任何物理单位或角度单位，所有量都应视为无量纲。必须严格遵守最终的输出格式。", "solution": "该问题要求实现一个特定算法，以推断由受限玻尔兹曼机 (RBM) 建模的概念之间的先决条件关系，并使用 $F_1$ 分数对照基准课程评估此推断的性能。\n\n此过程的第一步是验证问题陈述本身的科学合理性、逻辑一致性和完整性。\n\n### 问题验证\n\n**步骤 1：提取给定信息**\n问题提供了以下定义、参数和条件：\n- 一个 RBM 能量函数：$E(\\mathbf{v},\\mathbf{h})=-\\mathbf{v}^{\\top}\\mathbf{W}\\mathbf{h}-\\mathbf{b}_v^{\\top}\\mathbf{v}-\\mathbf{b}_h^{\\top}\\mathbf{h}$。\n- 可见单元 $\\mathbf{v}\\in\\{0,1\\}^{n_v}$ 表示概念掌握程度。\n- 隐藏单元 $\\mathbf{h}\\in\\{0,1\\}^{n_h}$ 表示潜在技能。\n- 一个权重矩阵 $\\mathbf{W}\\in\\mathbb{R}^{n_v\\times n_h}$。\n- 偏置向量 $\\mathbf{b}_v\\in\\mathbb{R}^{n_v}$ 和 $\\mathbf{b}_h\\in\\mathbb{R}^{n_h}$。\n- 可见单元 $v_i$ 的条件概率：$\\mathbb{P}(v_i=1\\mid\\mathbf{h})=\\sigma\\left(b_{v,i}+\\sum_{j=1}^{n_h}W_{ij}h_j\\right)$，其中 $\\sigma(x)$ 是逻辑 sigmoid 函数。\n- 概念 $i$ 的正支持集：$P_i=\\{j\\in\\{0,\\dots,n_h-1\\}\\,:\\,W_{ij}\\ge \\tau\\}$，其中 $\\tau0$ 是一个阈值。\n- 推断先决条件关系 $k\\to i$ 的规则：\n    1. 排除 $|P_k|=0$ 的情况。\n    2. 覆盖率约束：$\\frac{|P_k \\cap P_i|}{|P_k|} \\ge \\alpha$，其中 $\\alpha\\in(0,1]$ 是覆盖率阈值。\n    3. 严格大小余量：$|P_i| - |P_k| \\ge \\beta$，其中 $\\beta\\in\\mathbb{N}$ 是严格大小余量。\n- 基准课程是一个边集 $C$。推断的集合是 $\\widehat{C}$。\n- 性能指标：\n    - 真正例：$\\mathrm{TP}=|\\widehat{C}\\cap C|$。\n    - 假正例：$\\mathrm{FP}=|\\widehat{C}\\setminus C|$。\n    - 假反例：$\\mathrm{FN}=|C\\setminus \\widehat{C}|$。\n    - 精确率：$\\mathrm{Prec}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$。\n    - 召回率：$\\mathrm{Rec}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}$。\n    - $F_1$ 分数：$F_1=\\frac{2\\cdot \\mathrm{Prec}\\cdot \\mathrm{Rec}}{\\mathrm{Prec}+\\mathrm{Rec}}$。\n- $F_1$ 的特殊条件：\n    - 如果 $C$ 和 $\\widehat{C}$ 都为空，则 $F_1=1.0$。\n    - 如果 $C$ 或 $\\widehat{C}$ 中只有一个为空，则 $F_1=0.0$。\n- 五个具体的测试用例，每个提供 $(\\mathbf{W}, \\tau, \\alpha, \\beta, C)$。\n- 输出必须是单行，包含五个测试用例的 $F_1$ 分数，以逗号分隔并四舍五入到四位小数。\n\n**步骤 2：使用提取的给定信息进行验证**\n- **科学依据**：该问题在机器学习理论中有充分的依据。RBM 是深度信念网络的基本组成部分。解释权重矩阵模式以推断结构关系是模型可解释性中一个有效且常见的研究领域。指定的能量函数和条件概率是二元-二元 RBM 的标准形式。\n- **适定性**：该问题是适定的。构建正支持集和推断先决条件边的规则是确定性且无歧义的。为每个测试用例提供了所有必要的参数和数据。评估指标，包括其针对空集的特殊情况，都有明确定义，确保每个测试用例都存在唯一解。\n- **客观性**：问题以精确、客观和数学化的语言陈述，没有主观或含糊的术语。\n\n问题陈述没有表现出验证标准中列出的任何缺陷。它是科学合理的、形式上指定的、自洽的并且客观的。\n\n**步骤 3：结论与行动**\n问题是**有效的**。将提供一个合理的解决方案。\n\n### 解决方案\n\n解决方案包含针对每个测试用例的三步算法过程，然后计算最终输出。\n\n**步骤 1：构建正支持集**\n对于每个概念 $i \\in \\{0, \\dots, n_v-1\\}$，我们识别其正支持集 $P_i$。该集合包含所有对概念的掌握概率具有超过给定阈值 $\\tau$ 的兴奋性影响的隐藏单元（潜在技能）的索引。形式上，对于权重矩阵 $\\mathbf{W}$ 的每一行 $i$，我们构建集合：\n$$P_i = \\{j \\in \\{0, \\dots, n_h-1\\} \\,:\\, W_{ij} \\ge \\tau\\}$$\n这会为每个概念生成一个支持集，共计 $n_v$ 个。\n\n**步骤 2：推断先决条件边**\n当且仅当对于一个有序的不同概念对 $(k, i)$ 满足两个条件时，才会推断出一条有向边 $k \\to i$，表示概念 $k$ 是概念 $i$ 的先决条件。我们遍历所有这样的对，其中 $k, i \\in \\{0, \\dots, n_v-1\\}$ 且 $k \\neq i$。\n首先，潜在的先决概念 $k$ 必须有一个非空的支持集，即 $|P_k|0$。如果此条件成立，我们再测试以下两个约束：\n1.  **覆盖率约束**：支持概念 $k$ 的技能中必须有相当一部分也支持概念 $i$。这通过要求它们的 支持集的交集大小 与 $k$ 的支持集大小之比 达到或超过阈值 $\\alpha$ 来量化。\n    $$\\frac{|P_k \\cap P_i|}{|P_k|} \\ge \\alpha$$\n2.  **严格大小余量**：概念 $i$ 必须严格更复杂，即它比概念 $k$ 依赖更多的支持技能。这通过要求 $P_i$ 中的技能数量比 $P_k$ 中的技能数量至少多 $\\beta$ 个来强制执行。\n    $$|P_i| - |P_k| \\ge \\beta$$\n如果对 $(k, i)$ 满足这两个条件，则将有向边 $(k, i)$ 添加到推断的先决条件集 $\\widehat{C}$ 中。\n\n**步骤 3：计算 $F_1$ 分数**\n将推断的集合 $\\widehat{C}$ 与基准课程 $C$ 进行比较。计算作为精确率和召回率的调和平均数的 $F_1$ 分数，以衡量推断的准确性。\n\n首先，我们处理定义的特殊情况：\n- 如果 $|\\widehat{C}| = 0$ 且 $|C| = 0$，则 $F_1 = 1.0$。\n- 如果 ($|\\widehat{C}|  0$ 且 $|C| = 0$) 或 ($|\\widehat{C}| = 0$ 且 $|C|  0$)，则 $F_1 = 0.0$。\n\n对于两个集合都非空的所有其他情况，我们采用标准定义进行处理：\n- 真正例：$\\mathrm{TP} = |\\widehat{C} \\cap C|$。这些是正确推断的先决条件边。\n- 假正例：$\\mathrm{FP} = |\\widehat{C} \\setminus C|$。这些是错误推断的先决条件边。\n- 假反例：$\\mathrm{FN} = |C \\setminus \\widehat{C}|$。这些是未被推断出的基准边。\n\n使用这些计数，计算精确率和召回率：\n- $\\mathrm{Prec} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}} = \\frac{|\\widehat{C} \\cap C|}{|\\widehat{C}|}$\n- $\\mathrm{Rec} = \\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}} = \\frac{|\\widehat{C} \\cap C|}{|C|}$\n\n如果 $\\mathrm{Prec} + \\mathrm{Rec} = 0$（当且仅当 $\\mathrm{TP}=0$ 时发生），则 $F_1$ 分数定义为 $0$。否则，计算公式为：\n$$F_1 = \\frac{2 \\cdot \\mathrm{Prec} \\cdot \\mathrm{Rec}}{\\mathrm{Prec} + \\mathrm{Rec}}$$\n每个测试用例的最终分数四舍五入到四位小数。实施整个程序以处理所提供的测试套件。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by iterating through test cases, inferring prerequisite relations,\n    and calculating the F1 score for each.\n    \"\"\"\n    test_cases = [\n        (np.array([[0.9, 0.0, 0.1], [0.8, 0.7, 0.0], [0.0, 0.0, 0.9]]), 0.5, 1.0, 1, {(0, 1)}),\n        (np.array([[0.9, 0.7, 0.0], [0.9, 0.7, 0.0], [0.0, 0.0, 0.0]]), 0.5, 1.0, 1, set()),\n        (np.array([[0.6, -0.2, 0.0], [0.61, 0.55, -0.3], [0.0, 0.51, 0.0]]), 0.5, 1.0, 1, {(0, 1), (2, 1)}),\n        (np.array([[0.6, 0.0, 0.6], [0.6, 0.6, 0.0], [0.0, 0.6, 0.6]]), 0.5, 1.0, 1, set()),\n        (np.array([[0.6, 0.6, 0.0], [0.0, 0.6, 0.0], [0.6, 0.0, 0.0]]), 0.5, 0.5, 1, {(2, 0)}),\n    ]\n\n    results = []\n    for W, tau, alpha, beta, C in test_cases:\n        f1_score = calculate_f1_score(W, tau, alpha, beta, C)\n        results.append(f\"{f1_score:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_f1_score(W, tau, alpha, beta, C):\n    \"\"\"\n    Calculates the F1 score for a single test case.\n\n    Args:\n        W (np.ndarray): The weight matrix.\n        tau (float): The support threshold.\n        alpha (float): The coverage threshold.\n        beta (int): The strict size margin.\n        C (set): The ground-truth curriculum edge set.\n\n    Returns:\n        float: The calculated F1 score.\n    \"\"\"\n    n_v, n_h = W.shape\n\n    # Step 1: Construct Positive Support Sets\n    support_sets = []\n    for i in range(n_v):\n        P_i = {j for j in range(n_h) if W[i, j] >= tau}\n        support_sets.append(P_i)\n\n    # Step 2: Infer Prerequisite Edges\n    C_hat = set()\n    for k in range(n_v):\n        P_k = support_sets[k]\n        if not P_k:  # Exclude cases where |P_k| = 0\n            continue\n        \n        for i in range(n_v):\n            if i == k:\n                continue\n            \n            P_i = support_sets[i]\n            \n            # Condition 1: Coverage\n            coverage = len(P_k.intersection(P_i)) / len(P_k)\n            if coverage  alpha:\n                continue\n            \n            # Condition 2: Strict size margin\n            margin = len(P_i) - len(P_k)\n            if margin  beta:\n                continue\n            \n            # If both conditions are met, add the edge\n            C_hat.add((k, i))\n\n    # Step 3: Compute the F1 Score\n    # Handle special cases for empty sets\n    if not C_hat and not C:\n        return 1.0\n    if not C_hat or not C:\n        return 0.0\n\n    # Calculate TP, FP, FN\n    tp = len(C_hat.intersection(C))\n    fp = len(C_hat.difference(C))\n    fn = len(C.difference(C_hat))\n\n    # Calculate Precision and Recall\n    # These denominators will not be zero due to the special case handling above\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # Calculate F1 score\n    if precision + recall == 0:\n        return 0.0\n    else:\n        f1 = 2 * (precision * recall) / (precision + recall)\n        return f1\n\nsolve()\n```", "id": "3112287"}]}