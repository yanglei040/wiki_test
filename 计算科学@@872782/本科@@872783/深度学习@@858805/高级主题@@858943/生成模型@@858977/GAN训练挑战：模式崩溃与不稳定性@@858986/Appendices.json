{"hands_on_practices": [{"introduction": "在训练生成对抗网络（GAN）时，判别器（Discriminator）有时会变得过于自信，对真实样本的预测概率迅速趋近于1，对生成样本的预测概率迅速趋近于0。这种情况下，生成器（Generator）会面临梯度消失的问题，学习信号变得微弱，导致训练停滞。本练习将通过一个经典的正则化技巧——标签平滑（Label Smoothing）来解决这个问题。你将从数学上推导出，通过将真实样本的标签从1“软化”为$1-\\alpha$，我们如何改变判别器的最优解，从而为生成器提供更稳定、更有意义的梯度信号 [@problem_id:3127248]。", "problem": "考虑标准的生成对抗网络 (GAN) 框架，其中生成器生成遵循密度 $p_{g}(x)$ 分布的样本，而真实数据遵循密度 $p_{r}(x)$。判别器输出一个标量 $D(x) \\in (0,1)$，该标量被解释为 $x$ 是真实样本的预测概率。判别器通过在真实样本和伪造样本上最大化二元交叉熵 (BCE) 目标函数来进行训练。\n\n假设我们在判别器训练期间对真实类别应用标签平滑：真实样本的目标标签从 $1$ 替换为 $1-\\alpha$（其中 $0 < \\alpha < 1$），而伪造样本的目标标签保持为 $0$。判别器的期望目标函数变为\n$$\nJ(D) \\;=\\; \\mathbb{E}_{x \\sim p_{r}}\\!\\left[(1-\\alpha)\\,\\ln D(x) \\;+\\; \\alpha\\,\\ln\\!\\big(1-D(x)\\big)\\right] \\;+\\; \\mathbb{E}_{x \\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right].\n$$\n假设 $D$ 在 $x$ 上进行逐点优化，并将 $p_{r}(x)$ 和 $p_{g}(x)$ 视为固定的非负密度。\n\n你的任务是：\n1. 从 BCE 形式和对 $J(D)$ 的逐点最大化出发，推导出最优判别器 $D^{\\star}(x)$ 作为 $p_{r}(x)$、$p_{g}(x)$ 和 $\\alpha$ 的函数的闭式解。\n2. 考虑非饱和生成器目标 $L_{G}^{\\mathrm{ns}}(G) = -\\,\\mathbb{E}_{x \\sim p_{g}}[\\ln D(x)]$。将 $D$ 固定为最优值 $D^{\\star}$，使用链式法则表示在生成样本 $x$ 处从判别器流向生成器的梯度，并分离出缩放反向传播的判别器梯度的标量乘数。计算在有标签平滑（$\\alpha > 0$）的情况下此标量乘数与无平滑（$\\alpha = 0$）情况下的值的比率，并将此比率简化为仅依赖于 $\\alpha$ 的闭式表达式。\n\n将你的最终答案以一个单行矩阵的形式给出，该矩阵的第一个条目是 $D^{\\star}(x)$，第二个条目是第2部分得到的简化比率。不需要进行数值近似。", "solution": "该问题要求进行两项与带有单侧标签平滑的生成对抗网络 (GAN) 相关的推导。首先，我们必须推导出最优判别器 $D^{\\star}(x)$。其次，我们必须分析这对生成器梯度的影响。\n\n**第1部分：最优判别器 $D^{\\star}(x)$ 的推导**\n\n判别器的目标函数 $J(D)$ 由下式给出：\n$$J(D) \\;=\\; \\mathbb{E}_{x \\sim p_{r}}\\!\\left[(1-\\alpha)\\,\\ln D(x) \\;+\\; \\alpha\\,\\ln\\!\\big(1-D(x)\\big)\\right] \\;+\\; \\mathbb{E}_{x \\sim p_{g}}\\!\\left[\\ln\\!\\big(1-D(x)\\big)\\right]$$\n其中 $p_r(x)$ 是真实数据密度，$p_g(x)$ 是生成器密度，$D(x)$ 是判别器输出，$\\alpha$ 是标签平滑参数，且 $0 < \\alpha < 1$。\n\n我们可以将期望表示为在数据空间上的积分：\n$$J(D) = \\int p_{r}(x)\\left[(1-\\alpha)\\ln D(x) + \\alpha\\ln(1-D(x))\\right] dx + \\int p_{g}(x)\\ln(1-D(x)) dx$$\n为了找到最优判别器 $D^{\\star}(x)$，我们可以相对于函数 $D(x)$ 最大化这个泛函。由于积分是所有 $x$ 值的总和，我们可以对每个 $x$ 逐点最大化被积函数。让我们将对于一个固定的 $x$ 的被积函数定义为函数 $f(y)$，其中 $y = D(x)$：\n$$f(y) = p_{r}(x)\\left[(1-\\alpha)\\ln(y) + \\alpha\\ln(1-y)\\right] + p_{g}(x)\\ln(1-y)$$\n我们可以收集包含 $\\ln(y)$ 和 $\\ln(1-y)$ 的项：\n$$f(y) = p_{r}(x)(1-\\alpha)\\ln(y) + \\left[p_{r}(x)\\alpha + p_{g}(x)\\right]\\ln(1-y)$$\n为了找到使 $f(y)$ 最大化的 $y$ 值，我们计算它关于 $y$ 的导数并将其设为零。\n$$\\frac{df}{dy} = \\frac{p_{r}(x)(1-\\alpha)}{y} - \\frac{p_{r}(x)\\alpha + p_{g}(x)}{1-y}$$\n将导数设为零，得到最优 $D^{\\star}(x)$ 的条件：\n$$\\frac{p_{r}(x)(1-\\alpha)}{D^{\\star}(x)} = \\frac{p_{r}(x)\\alpha + p_{g}(x)}{1-D^{\\star}(x)}$$\n现在，我们求解 $D^{\\star}(x)$：\n$$p_{r}(x)(1-\\alpha)(1-D^{\\star}(x)) = D^{\\star}(x)\\left[p_{r}(x)\\alpha + p_{g}(x)\\right]$$\n$$p_{r}(x)(1-\\alpha) - p_{r}(x)(1-\\alpha)D^{\\star}(x) = p_{r}(x)\\alpha D^{\\star}(x) + p_{g}(x)D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x)(1-\\alpha) + p_{r}(x)\\alpha + p_{g}(x)\\right]D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x) - p_{r}(x)\\alpha + p_{r}(x)\\alpha + p_{g}(x)\\right]D^{\\star}(x)$$\n$$p_{r}(x)(1-\\alpha) = \\left[p_{r}(x) + p_{g}(x)\\right]D^{\\star}(x)$$\n分离出 $D^{\\star}(x)$，我们得到最优判别器的闭式表达式：\n$$D^{\\star}(x) = \\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}$$\n这是问题第一部分的解。\n\n**第2部分：生成器梯度标量乘数的比率**\n\n生成器通过最小化非饱和目标进行训练：\n$$L_{G}^{\\mathrm{ns}}(G) = -\\,\\mathbb{E}_{x \\sim p_{g}}[\\ln D(x)]$$\n我们考虑单个生成样本 $x$，其损失贡献为 $L_{G}(x) = -\\ln D(x)$。我们需要找到该损失关于生成样本 $x$ 的梯度，然后该梯度会反向传播到生成器的参数。使用链式法则，梯度为：\n$$\\nabla_x L_{G}(x) = \\nabla_x (-\\ln D(x)) = -\\frac{1}{D(x)} \\nabla_x D(x)$$\n项 $\\nabla_x D(x)$ 是判别器输出关于其输入的梯度，由深度学习框架计算并反向传播。问题要求的是缩放此梯度的标量乘数。从上面的表达式中，这个标量乘数是：\n$$M(x) = -\\frac{1}{D(x)}$$\n我们被要求在判别器最优时，即 $D(x) = D^{\\star}(x)$ 时，计算这个乘数。所以该乘数变为：\n$$M(x, \\alpha) = -\\frac{1}{D^{\\star}(x)} = -\\frac{1}{\\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}} = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-\\alpha)}$$\n这是有标签平滑（$\\alpha > 0$）时的乘数。为了找到没有标签平滑时的乘数，我们设置 $\\alpha=0$：\n$$M(x, 0) = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-0)} = -\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)}$$\n问题要求的是有标签平滑情况下的标量乘数与无平滑情况下的值的比率。\n$$\\text{Ratio} = \\frac{M(x, \\alpha)}{M(x, 0)} = \\frac{-\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)(1-\\alpha)}}{-\\frac{p_{r}(x) + p_{g}(x)}{p_{r}(x)}}$$\n分子中的公共项 $-\\left(p_{r}(x) + p_{g}(x)\\right)$ 和分母中的 $p_r(x)$ 可以消去：\n$$\\text{Ratio} = \\frac{\\frac{1}{1-\\alpha}}{1} = \\frac{1}{1-\\alpha}$$\n这个简化表达式仅依赖于 $\\alpha$，并表示由于单侧标签平滑导致的生成器梯度大小的缩放因子。\n\n最终答案的两个结果是：\n1.  $D^{\\star}(x) = \\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}$\n2.  比率是 $\\frac{1}{1-\\alpha}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{p_{r}(x)(1-\\alpha)}{p_{r}(x) + p_{g}(x)}  \\frac{1}{1-\\alpha}\n\\end{pmatrix}\n}\n$$", "id": "3127248"}, {"introduction": "模式崩溃（Mode Collapse）是GAN训练中最臭名昭著的挑战之一，即生成器只学会了产生数据分布中有限的几种样本，丧失了多样性。为了直接解决这个问题，研究人员提出了多种方法，其中之一便是“小批量判别”（Minibatch Discrimination）。这个练习将引导你实现这一技术，通过在判别器的输入中加入一个衡量小批量内部样本相似度的项，来明确地鼓励生成样本的多样性。你将推导出相应的梯度，并亲眼见证该机制是如何在样本之间产生“排斥力”，从而迫使生成器探索更广泛的数据模式 [@problem_id:3127206]。", "problem": "考虑一个生成对抗网络（GAN），它被定义为一个生成器和一个判别器之间的双人博弈。其中，生成器 $G$ 将一个潜变量 $z$ 映射到一个数据点 $x$，而判别器 $D$ 估计一个输入来自真实数据分布而非来自 $G$ 的概率。设生成器为标量线性映射 $G_{w}(z) = w z$，其参数为 $w \\in \\mathbb{R}$ 且 $z \\in \\mathbb{R}$。设判别器的输出为 $D(x, s) = \\sigma(h(x, s))$，其中 $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$ 是 logistic sigmoid 函数，判别器的 logit 为 $h(x, s) = a x + b - \\lambda s$，其中 $a, b, \\lambda \\in \\mathbb{R}$。为了鼓励一个小批量（minibatch）内生成器输出的多样性，我们使用一个小批量判别相似度项来增强判别器\n$$\nc(x, x') = \\exp\\!\\big(-\\beta (x - x')^{2}\\big),\n$$\n其中 $\\beta > 0$。对于一个小批量 $\\{z_{i}\\}_{i=1}^{n}$，定义每个样本的相似度摘要为\n$$\ns_{i} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j}), \\quad x_{i} = G_{w}(z_{i}).\n$$\n假设生成器使用非饱和目标\n$$\nL_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big), \\quad \\ell(p) = -\\ln(p).\n$$\n任务：\n1. 仅从这些定义出发，用 $a$、$b$、$\\lambda$、$\\beta$、$w$ 和 $\\{z_{i}\\}_{i=1}^{n}$ 推导 $\\frac{d L_{G}}{d w}$ 的闭式表达式。\n2. 对于小批量大小 $n = 3$，其中 $z_{1} = -1$, $z_{2} = 0$, $z_{3} = 1$，参数 $a = 1.5$, $b = 0$, $\\lambda = 2$, $\\beta = 0.5$，在 $w = 1$ 时，数值计算該导数。将最终数值答案四舍五入到四位有效数字。将最终结果表示为无量纲标量。\n3. 根据你推导出的梯度，定性解释小批量判别贡献如何鼓励小批量内的输出多样性，并讨论当小批量大小 $n$ 相对于数据分布中不同模式的数量较小时可能出现的缺陷。此部分无需额外计算。", "solution": "该问题被评估为有效，因为它基于机器学习和微积分的科学原理，提法明确，目标清晰，并提供了一套完整且一致的定义。因此，我们可以着手进行解答。\n\n该问题分为三个任务：1) 推导生成器损失函数的梯度，2) 对特定情况数值计算此梯度，以及 3) 定性解释小批量判别项的作用。\n\n### 任务 1：梯度 $\\frac{d L_{G}}{d w}$ 的推导\n\n生成器的损失函数由下式给出：\n$$L_{G}(w) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell\\big(D(x_{i}, s_{i})\\big)$$\n其中 $\\ell(p) = -\\ln(p)$，$x_i = G_w(z_i) = w z_i$，以及 $D(x, s) = \\sigma(h(x,s))$ 是判别器输出。\n损失关于生成器参数 $w$ 的导数是：\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{d}{dw} \\left[ -\\ln\\left(D(x_{i}, s_{i})\\right) \\right]$$\n我们来分析求和中的单项。使用链式法则，我们有：\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\frac{dD_i}{dw}$$\n这里我们使用简写 $D_i = D(x_i, s_i)$。\n\n判别器输出为 $D_i = \\sigma(h_i)$，其中 $h_i = h(x_i, s_i) = a x_i + b - \\lambda s_i$。sigmoid 函数 $\\sigma(u)$ 的导数是 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$。再次应用链式法则：\n$$\\frac{dD_i}{dw} = \\frac{d}{dw} \\sigma(h_i) = \\sigma'(h_i) \\frac{dh_i}{dw} = \\sigma(h_i)(1-\\sigma(h_i)) \\frac{dh_i}{dw} = D_i(1 - D_i) \\frac{dh_i}{dw}$$\n将此代回到单个损失项的导数中：\n$$\\frac{d}{dw} \\left[ -\\ln(D_i) \\right] = -\\frac{1}{D_i} \\left[ D_i(1 - D_i) \\frac{dh_i}{dw} \\right] = -(1 - D_i) \\frac{dh_i}{dw}$$\n这是非饱和生成器损失的一个标准结果。\n\n接下来，我们需要求判别器 logit $h_i$ 关于 $w$ 的导数：\n$$h_i = a x_i + b - \\lambda s_i$$\n$$\\frac{dh_i}{dw} = \\frac{d}{dw} (a x_i + b - \\lambda s_i) = a \\frac{dx_i}{dw} - \\lambda \\frac{ds_i}{dw}$$\n已知 $x_i = w z_i$，其导数为 $\\frac{dx_i}{dw} = z_i$。\n\n现在，我们计算相似度摘要项 $s_i$ 的导数：\n$$s_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} c(x_{i}, x_{j})$$\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\frac{d}{dw} c(x_i, x_j)$$\n相似度函数为 $c(x, x') = \\exp(-\\beta (x - x')^2)$。其关于 $w$ 的导数可以通过链式法则求得：\n$$\\frac{d}{dw} c(x_i, x_j) = \\exp(-\\beta (x_i - x_j)^2) \\cdot \\frac{d}{dw} (-\\beta (x_i - x_j)^2)$$\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta (x_i - x_j)) \\cdot \\frac{d}{dw}(x_i - x_j)$$\n由于 $x_i = w z_i$ 和 $x_j = w z_j$，我们有 $x_i - x_j = w(z_i - z_j)$，因此 $\\frac{d}{dw}(x_i - x_j) = z_i - z_j$。\n将其代入，我们得到：\n$$\\frac{d}{dw} c(x_i, x_j) = c(x_i, x_j) \\cdot (-2\\beta w(z_i - z_j)) \\cdot (z_i - z_j) = -2\\beta w (z_i - z_j)^2 c(x_i, x_j)$$\n因此，$s_i$ 的导数为：\n$$\\frac{ds_i}{dw} = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} \\left[ -2\\beta w (z_i - z_j)^2 c(x_i, x_j) \\right] = -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\n现在我们将此代入 $\\frac{dh_i}{dw}$ 的表达式中：\n$$\\frac{dh_i}{dw} = a z_i - \\lambda \\left[ -2\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right] = a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j)$$\n最后，我们组合出 $\\frac{d L_{G}}{d w}$ 的完整表达式：\n$$\\frac{d L_{G}}{d w} = \\frac{1}{n} \\sum_{i=1}^{n} -(1 - D(x_i, s_i)) \\left[ a z_i + 2\\lambda\\beta w \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{n} (z_i - z_j)^2 c(x_i, x_j) \\right]$$\n其中 $x_i = w z_i$，$s_i = \\sum_{j \\neq i} c(x_i, x_j)$，$c(x_i, x_j) = \\exp(-\\beta(x_i-x_j)^2)$，以及 $D(x_i, s_i) = \\sigma(a x_i + b - \\lambda s_i)$。这就是所求的闭式表达式。\n\n### 任务 2：梯度的数值计算\n\n给定以下值：\n$n = 3$, $z_1 = -1$, $z_2 = 0$, $z_3 = 1$。\n$a = 1.5$, $b = 0$, $\\lambda = 2$, $\\beta = 0.5$，我们在 $w = 1$ 处进行计算。\n\n当 $w = 1$ 时，生成的样本为 $x_i = 1 \\cdot z_i = z_i$，所以 $x_1 = -1$, $x_2 = 0$, $x_3 = 1$。\n\n首先，计算相似度项 $c(x_i, x_j) = \\exp(-0.5(x_i - x_j)^2)$：\n$c(x_1, x_2) = c(x_2, x_1) = \\exp(-0.5(-1 - 0)^2) = \\exp(-0.5)$。\n$c(x_1, x_3) = c(x_3, x_1) = \\exp(-0.5(-1 - 1)^2) = \\exp(-0.5(-2)^2) = \\exp(-2)$。\n$c(x_2, x_3) = c(x_3, x_2) = \\exp(-0.5(0 - 1)^2) = \\exp(-0.5)$。\n\n接下来，计算每个样本的相似度摘要 $s_i = \\sum_{j \\neq i} c(x_i, x_j)$：\n$s_1 = c(x_1, x_2) + c(x_1, x_3) = \\exp(-0.5) + \\exp(-2)$。\n$s_2 = c(x_2, x_1) + c(x_2, x_3) = \\exp(-0.5) + \\exp(-0.5) = 2\\exp(-0.5)$。\n$s_3 = c(x_3, x_1) + c(x_3, x_2) = \\exp(-2) + \\exp(-0.5) = s_1$。\n\n现在计算判别器 logits $h_i = a x_i + b - \\lambda s_i = 1.5 x_i - 2 s_i$：\n$h_1 = 1.5(-1) - 2(\\exp(-0.5) + \\exp(-2))$。\n$h_2 = 1.5(0) - 2(2\\exp(-0.5)) = -4\\exp(-0.5)$。\n$h_3 = 1.5(1) - 2(\\exp(-0.5) + \\exp(-2))$。\n\n我们还需要任务1结果中方括号内的项，称之为 $K_i$：\n$K_i = a z_i + 2\\lambda\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2(2)(0.5)(1) \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j) = 1.5 z_i + 2 \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$。\n$K_1 = 1.5(-1) + 2\\left[(z_1-z_2)^2 c(x_1,x_2) + (z_1-z_3)^2 c(x_1,x_3)\\right] = -1.5 + 2\\left[(-1)^2 \\exp(-0.5) + (-2)^2 \\exp(-2)\\right] = -1.5 + 2\\exp(-0.5) + 8\\exp(-2)$。\n$K_2 = 1.5(0) + 2\\left[(z_2-z_1)^2 c(x_2,x_1) + (z_2-z_3)^2 c(x_2,x_3)\\right] = 2\\left[(1)^2 \\exp(-0.5) + (-1)^2 \\exp(-0.5)\\right] = 4\\exp(-0.5)$。\n$K_3 = 1.5(1) + 2\\left[(z_3-z_1)^2 c(x_3,x_1) + (z_3-z_2)^2 c(x_3,x_2)\\right] = 1.5 + 2\\left[(2)^2 \\exp(-2) + (1)^2 \\exp(-0.5)\\right] = 1.5 + 8\\exp(-2) + 2\\exp(-0.5)$。\n\n现在，我们进行数值计算：\n$\\exp(-0.5) \\approx 0.606531$\n$\\exp(-2) \\approx 0.135335$\n\n$s_1 = s_3 \\approx 0.606531 + 0.135335 = 0.741866$。\n$s_2 \\approx 2 \\times 0.606531 = 1.213062$。\n\n$h_1 \\approx -1.5 - 2(0.741866) = -2.983732$。\n$h_2 \\approx -4(0.606531) = -2.426124$。\n$h_3 \\approx 1.5 - 2(0.741866) = 0.016268$。\n\n$K_1 \\approx -1.5 + 2(0.606531) + 8(0.135335) = -1.5 + 1.213062 + 1.08268 = 0.795742$。\n$K_2 \\approx 4(0.606531) = 2.426124$。\n$K_3 \\approx 1.5 + 8(0.135335) + 2(0.606531) = 1.5 + 1.08268 + 1.213062 = 3.795742$。\n\n梯度为 $\\frac{d L_G}{d w} = \\frac{1}{3} \\sum_{i=1}^3 -(1 - \\sigma(h_i)) K_i = \\frac{1}{3} \\sum_{i=1}^3 -\\sigma(-h_i) K_i$。\n$\\sigma(-h_1) = \\sigma(2.983732) = \\frac{1}{1 + \\exp(-2.983732)} \\approx 0.951825$。\n$\\sigma(-h_2) = \\sigma(2.426124) = \\frac{1}{1 + \\exp(-2.426124)} \\approx 0.918790$。\n$\\sigma(-h_3) = \\sigma(-0.016268) = \\frac{1}{1 + \\exp(0.016268)} \\approx 0.495935$。\n\n求和中的各项为：\n第 1 项： $-\\sigma(-h_1) K_1 \\approx -0.951825 \\times 0.795742 \\approx -0.75747$。\n第 2 项： $-\\sigma(-h_2) K_2 \\approx -0.918790 \\times 2.426124 \\approx -2.22940$。\n第 3 项： $-\\sigma(-h_3) K_3 \\approx -0.495935 \\times 3.795742 \\approx -1.88251$。\n\n和 $= -0.75747 - 2.22940 - 1.88251 = -4.86938$。\n$\\frac{d L_G}{d w} = \\frac{-4.86938}{3} \\approx -1.623127$。\n\n四舍五入到四位有效数字，结果为 $-1.623$。\n\n### 任务 3：定性解释\n\n生成器的目标是生成能被判别器分类为真实的样本。对于非饱和损失，这意味着生成器旨在最大化判别器的输出概率 $D(x_i, s_i) = \\sigma(h_i)$，这等价于最大化 logit $h_i = a x_i + b - \\lambda s_i$。\n\n小批量判别项以 $-\\lambda s_i$ 的形式进入 logit，其中 $s_i = \\sum_{j \\neq i} \\exp(-\\beta (x_i-x_j)^2)$。项 $s_i$ 是衡量样本 $x_i$ 与小批量中其他样本相似度的指标。如果生成器生成的样本彼此非常接近（多样性低，可能发生模式坍塌），距离 $|x_i-x_j|$ 将会很小，使得指数项 $\\exp(-\\beta(x_i-x_j)^2)$ 接近于 $1$。这会导致一个较大的 $s_i$，由于 $-\\lambda$ 系数（其中 $\\lambda > 0$），这将显著惩罚 logit $h_i$。较低的 logit 意味着判别器更可能将样本分类为假的。为了对抗这一点并欺骗判别器，生成器被迫生成彼此远离的样本，即增加距离 $|x_i-x_j|$。这直接鼓励了小批量内的多样性。\n\n任务1中推导的梯度反映了这一点。$w$ 的梯度更新由诸如 $(1 - D_i) \\lambda (-\\frac{ds_i}{dw})$ 的项驱动。由于 $\\frac{ds_i}{dw} = -2\\beta w \\sum_{j \\neq i} (z_i - z_j)^2 c(x_i, x_j)$ 通常为负（对于 $w>0$），总梯度的这部分是负的。在梯度下降中（$w \\leftarrow w - \\eta \\frac{dL_G}{dw}$），负梯度会导致 $w$ 增加。增加 $w$ 会放大所有距离 $|x_i-x_j|=|w(z_i-z_j)|$，有效地将样本推开，从而促进多样性。\n\n当小批量大小 $n$ 相对于真实数据分布中不同模式的数量较小时，会出现一个显著的缺陷。小批量判别在每个批次内部 *局部地* 强制实现多样性。如果 $n$ 太小，单个批次可能只包含来自一个或少数几个真实模式的样本。该机制仍会试图将这些样本推开，这可能是有害的。它可能会阻止生成器学习单个模式内部的真实（通常很小）方差，从而有效地扭曲学习到的分布。此外，它不能防止批次间的模式坍塌。生成器可能学会在一个批次中为一个模式生成多样化的样本，然后在下一个批次中为 *相同* 的模式生成多样化的样本，而永远不会发现数据分布的其他模式。因此，虽然它能有效防止完全坍塌到单个点，但其确保覆盖所有数据模式的能力受到小批量大小的限制。", "answer": "$$\\boxed{-1.623}$$", "id": "3127206"}, {"introduction": "为什么GAN的训练过程常常如此不稳定，甚至在理论上看似简单的游戏中也会出现振荡和发散？本练习将带你深入探索GAN训练不稳定性的数学根源，将其视为一个动力系统。我们将分析一个简化的双线性博弈模型，并使用线性化和谱半径分析来比较两种不同的更新策略：同步更新和双时间尺度更新规则（TTUR）。通过这个练习，你将理解为什么同步梯度下降-上升在本质上可能是不稳定的，以及像TTUR这样的交替更新方案如何能够从根本上改善训练的收敛性 [@problem_id:3127265]。", "problem": "考虑一个零和生成对抗网络（GAN）博弈，其双线性价值函数为 $$V(\\theta,\\phi)=\\theta^{\\top}A\\phi,$$ 其中 $\\theta\\in\\mathbb{R}^{n}$，$\\phi\\in\\mathbb{R}^{m}$，且 $A\\in\\mathbb{R}^{n\\times m}$。生成器最小化 $V$，判别器最大化 $V$。设生成器的学习率为 $\\eta_{G}>0$，判别器的学习率为 $\\eta_{D}>0$，并定义比率 $$r=\\frac{\\eta_{D}}{\\eta_{G}}>0.$$\n\n分析在鞍点 $(\\theta,\\phi)=(0,0)$ 附近线性化的两种离散时间训练方案：\n1. 单时间尺度的同步梯度下降-上升：\n$$\\theta_{k+1}=\\theta_{k}-\\eta_{G}\\nabla_{\\theta}V(\\theta_{k},\\phi_{k})=\\theta_{k}-\\eta_{G}A\\phi_{k},\\quad \\phi_{k+1}=\\phi_{k}+\\eta_{D}\\nabla_{\\phi}V(\\theta_{k},\\phi_{k})=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k}.$$\n2. 双时间尺度更新规则（TTUR；交替更新）：首先更新判别器，然后更新生成器，\n$$\\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k},\\quad \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1}.$$\n\n从梯度下降-上升和线性算子谱半径的定义出发，使用 $A$ 的奇异值分解来分析沿 $A$ 的左右奇异向量对的线性化动力学。推导两种方案的迭代矩阵，并根据 $A$ 的奇异值和步长求出谱半径。从第一性原理出发，确定是否存在任何正比率 $r$，能使交替 TTUR 方案在非扩张线性动力学（谱半径小于或等于 $1$）的意义上实现局部稳定，并将其与同步方案进行比较。将交替 TTUR 方案可实现的最小谱半径（在所有 $r>0$ 的情况下）报告为一个实数。无需四舍五入。", "solution": "该问题要求分析一个具有双线性价值函数 $V(\\theta,\\phi)=\\theta^{\\top}A\\phi$ 的生成对抗网络（GAN）的两种离散时间训练方案的局部稳定性。稳定性由围绕鞍点 $(\\theta,\\phi)=(0,0)$ 线性化的动力学的迭代矩阵的谱半径决定。\n\n设系统在步骤 $k$ 的状态为连接向量 $z_k = \\begin{pmatrix} \\theta_k \\\\ \\phi_k \\end{pmatrix}$。更新是线性的，因此可以表示为 $z_{k+1} = M z_k$ 的形式，其中 $M$ 是一个适当的迭代矩阵。如果一个线性系统的迭代矩阵的谱半径 $\\rho(M)$ 小于或等于 $1$，则该系统是非扩张的（一种稳定形式）。\n\n为了分析这些矩阵，我们使用矩阵 $A$ 的奇异值分解（SVD），即 $A = U\\Sigma V^{\\top}$。这里，$U \\in \\mathbb{R}^{n\\times n}$ 和 $V \\in \\mathbb{R}^{m\\times m}$ 是正交矩阵，其列分别是 $A$ 的左奇异向量（$u_i$）和右奇异向量（$v_i$）。$\\Sigma \\in \\mathbb{R}^{n\\times m}$ 是一个包含非负奇异值 $\\sigma_i$ 的矩形对角矩阵。奇异向量满足 $Av_i = \\sigma_i u_i$ 和 $A^{\\top}u_i = \\sigma_i v_i$。通过将动力学投影到奇异向量基上，系统解耦为一组独立的 $2 \\times 2$ 系统，每个奇异值 $\\sigma_i$ 对应一个。我们分析 $\\theta_k$ 和 $\\phi_k$ 在这些基中的系数的动力学。\n\n**1. 同步梯度下降-上升（SGDA）**\n\n更新规则为：\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k} $$\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\n以矩阵形式表示，即 $z_{k+1} = M_{\\text{sim}} z_k$，迭代矩阵为：\n$$ M_{\\text{sim}} = \\begin{pmatrix} I  -\\eta_{G}A \\\\ \\eta_{D}A^{\\top}  I \\end{pmatrix} $$\n我们来分析对应于单个奇异值 $\\sigma_i$ 的单一模式的动力学。我们表示 $\\theta_k = c_k u_i$ 和 $\\phi_k = d_k v_i$。系数 $(c_k, d_k)$ 的更新变为：\n$$ c_{k+1}u_i = c_k u_i - \\eta_{G} A (d_k v_i) = c_k u_i - \\eta_{G} d_k (\\sigma_i u_i) \\implies c_{k+1} = c_k - \\eta_{G}\\sigma_i d_k $$\n$$ d_{k+1}v_i = d_k v_i + \\eta_{D} A^{\\top} (c_k u_i) = d_k v_i + \\eta_{D} c_k (\\sigma_i v_i) \\implies d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\n这为每种模式产生一个 $2 \\times 2$ 系统：\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1  -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i  1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\n这个 $2 \\times 2$ 矩阵的特征值 $\\lambda$ 由特征方程 $(\\lambda-1)^2 - (-\\eta_{G}\\sigma_i)(\\eta_{D}\\sigma_i) = 0$ 给出，该方程可简化为 $(\\lambda-1)^2 = -\\eta_{G}\\eta_{D}\\sigma_i^2$。特征值为 $\\lambda = 1 \\pm i\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}}$。\n这些特征值的模为 $|\\lambda| = \\sqrt{1^2 + (\\sigma_i\\sqrt{\\eta_{G}\\eta_{D}})^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2}$。\n对于任何非零奇异值 $\\sigma_i > 0$ 和正学习率 $\\eta_G, \\eta_D > 0$，我们有 $|\\lambda| > 1$。完整系统矩阵 $M_{\\text{sim}}$ 的谱半径是所有奇异值对应的模的最大值：\n$$ \\rho(M_{\\text{sim}}) = \\max_i \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_i^2} = \\sqrt{1 + \\eta_{G}\\eta_{D}\\sigma_{\\max}^2} $$\n其中 $\\sigma_{\\max}$ 是 $A$ 的最大奇异值。由于对于任何正学习率的选择，都有 $\\rho(M_{\\text{sim}}) > 1$，因此同步 SGDA 方案总是局部扩张的，因而是不稳定的。\n\n**2. 双时间尺度更新规则（TTUR，交替）**\n\n更新规则为：\n$$ \\phi_{k+1}=\\phi_{k}+\\eta_{D}A^{\\top}\\theta_{k} $$\n$$ \\theta_{k+1}=\\theta_{k}-\\eta_{G}A\\phi_{k+1} $$\n将第一个方程代入第二个方程，得到 $\\theta_k$ 的完整更新：\n$$ \\theta_{k+1} = \\theta_k - \\eta_G A (\\phi_k + \\eta_D A^{\\top}\\theta_k) = (I - \\eta_G \\eta_D A A^{\\top})\\theta_k - \\eta_G A \\phi_k $$\n迭代矩阵 $M_{\\text{alt}}$ 为：\n$$ M_{\\text{alt}} = \\begin{pmatrix} I - \\eta_{G}\\eta_{D}AA^{\\top}  -\\eta_{G}A \\\\ \\eta_{D}A^{\\top}  I \\end{pmatrix} $$\n同样，我们在奇异向量基中分析动力学。设 $\\theta_k = c_k u_i$ 和 $\\phi_k = d_k v_i$。系数更新为：\n$$ d_{k+1} = d_k + \\eta_{D}\\sigma_i c_k $$\n$$ c_{k+1} = c_k - \\eta_G \\sigma_i d_{k+1} = c_k - \\eta_G \\sigma_i (d_k + \\eta_D \\sigma_i c_k) = (1 - \\eta_G \\eta_D \\sigma_i^2)c_k - \\eta_G \\sigma_i d_k $$\n系数的 $2 \\times 2$ 系统为：\n$$ \\begin{pmatrix} c_{k+1} \\\\ d_{k+1} \\end{pmatrix} = \\begin{pmatrix} 1-\\eta_{G}\\eta_{D}\\sigma_i^2  -\\eta_{G}\\sigma_i \\\\ \\eta_{D}\\sigma_i  1 \\end{pmatrix} \\begin{pmatrix} c_k \\\\ d_k \\end{pmatrix} $$\n设 $x_i = \\eta_G \\eta_D \\sigma_i^2$。特征值 $\\lambda$ 的特征方程为 $(1-x_i-\\lambda)(1-\\lambda) - (-\\eta_G \\sigma_i)(\\eta_D \\sigma_i) = 0$，可简化为 $\\lambda^2 - (2-x_i)\\lambda + 1 = 0$。\n特征值为 $\\lambda = \\frac{(2-x_i) \\pm \\sqrt{(2-x_i)^2 - 4}}{2} = 1 - \\frac{x_i}{2} \\pm \\frac{\\sqrt{x_i^2 - 4x_i}}{2}$。\n\n我们根据 $x_i$ 的值分析模 $|\\lambda|$：\n- **情况 $0 \\le x_i \\le 4$**：平方根下的项为非正数，因此特征值为一对共轭复数：$\\lambda = 1 - \\frac{x_i}{2} \\pm i\\frac{\\sqrt{4x_i - x_i^2}}{2}$。\n模的平方为 $|\\lambda|^2 = (1-\\frac{x_i}{2})^2 + (\\frac{\\sqrt{4x_i - x_i^2}}{2})^2 = 1 - x_i + \\frac{x_i^2}{4} + \\frac{4x_i - x_i^2}{4} = 1$。因此，$|\\lambda|=1$。\n- **情况 $x_i > 4$**：特征值为实数。根据韦达定理，它们的乘积为 $1$。这意味着如果一个的模大于 $1$，另一个的模必须小于 $1$。较大的模为 $|\\lambda| = |\\frac{x_i}{2}-1 + \\frac{\\sqrt{x_i^2-4x_i}}{2}|$。由于当 $x_i>4$ 时，$\\frac{x_i}{2}-1>1$，所以这个模严格大于 $1$。\n\n要使交替 TTUR 方案是非扩张的，其谱半径 $\\rho(M_{\\text{alt}})$ 必须小于或等于 $1$。这要求所有模式的 $|\\lambda| \\le 1$，进而要求对所有 $i$ 都有 $x_i \\le 4$。这个条件必须对最大的奇异值 $\\sigma_{\\max}$ 成立。\n因此，非扩张动力学的条件是 $\\eta_{G}\\eta_{D}\\sigma_{\\max}^2 \\le 4$。\n使用 $r = \\eta_D/\\eta_G$，这变为 $r\\eta_G^2\\sigma_{\\max}^2 \\le 4$。对于任何正比率 $r>0$，总可以选择足够小的学习率 $\\eta_G$，例如 $\\eta_G \\le \\frac{2}{\\sigma_{\\max}\\sqrt{r}}$，来满足这个条件。因此，与同步方案不同，交替 TTUR 方案对于任何正比率 $r$ 都可以稳定。\n\n问题要求在所有 $r>0$ 的情况下可实现的最小谱半径。这等价于找到 $\\inf_{\\eta_G>0, \\eta_D>0} \\rho(M_{\\text{alt}})$。\n任何模式的特征值的模为 $|\\lambda_i| \\ge 1$。当 $0 \\le x_i \\le 4$ 时，可以达到 $|\\lambda_i|=1$ 的最小值。为了实现全系统谱半径为 $1$，我们需要对所有 $i$ 都有 $x_i \\le 4$，如果 $x_{\\max} = \\eta_G \\eta_D \\sigma_{\\max}^2 \\le 4$，这个条件就能得到保证。如前所示，通过选择足够小的学习率，这个条件对于任何 $r>0$ 都是可以实现的。当满足此条件时，每个模式的特征值的模都为 $1$，因此整个系统的谱半径为 $\\rho(M_{\\text{alt}}) = \\max_i|\\lambda_i|=1$。由于模永远不能小于 $1$，因此可实现的最小谱半径是 $1$。", "answer": "$$\\boxed{1}$$", "id": "3127265"}]}