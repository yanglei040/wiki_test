{"hands_on_practices": [{"introduction": "训练自编码器的首要关键技能是诊断模型性能。本练习提供了一个常见场景，你必须通过分析不同模型容量（瓶颈层大小）下的重构误差来识别欠拟合与过拟合，从而指导你选择最优模型。通过这项练习，你将学会在实践中解读经典的“U”型验证误差曲线，并理解偏见-方差权衡。[@problem_id:3135723]", "problem": "你在一个归一化到 $[0,1]$ 的灰度图像数据集 $x \\in \\mathbb{R}^{28 \\times 28}$ 上训练一个自编码器。编码器将 $x$ 映射到一个潜层瓶颈 $z \\in \\mathbb{R}^{d_z}$，解码器则重构出 $\\hat{x}$。你改变瓶颈大小 $d_z \\in \\{2,8,32,128\\}$ 并训练每个模型，使其在仅包含数字 $\\{0,1,2,3,4,5,6,7\\}$ 的训练集上最小化均方误差 (MSE)。然后，你在一个包含相同数字类别的预留验证集上评估 MSE，同时也在一个来自相同图像域但未在训练中出现的未见过数字 $\\{8,9\\}$ 的图像集上进行评估。\n\n设每个 $d_z$ 测得的重构 MSE 为：\n- $d_z=2$：$D_{\\text{train}}=0.060$， $D_{\\text{val}}=0.062$， $D_{\\text{unseen}}=0.080$。\n- $d_z=8$：$D_{\\text{train}}=0.030$， $D_{\\text{val}}=0.033$， $D_{\\text{unseen}}=0.060$。\n- $d_z=32$：$D_{\\text{train}}=0.015$， $D_{\\text{val}}=0.020$， $D_{\\text{unseen}}=0.045$。\n- $d_z=128$：$D_{\\text{train}}=0.005$， $D_{\\text{val}}=0.030$， $D_{\\text{unseen}}=0.090$。\n\n假设遵循以下基本原则：\n- 经验风险最小化 (ERM)：训练损失近似于经验分布下的损失期望，而验证损失则估计了在来自同一分布的新样本上的期望损失。\n- 欠拟合对应于高偏差，通常表现为在给定的假设类别下，训练误差和验证误差同时较大，并且随着训练没有得到充分改善。\n- 过拟合对应于高方差，通常表现为非常低的训练误差，而验证误差和训练误差之间存在相对较大的泛化差距。\n- 在压缩领域，率失真原理指出，对于固定的数据分布和解码器族，可达到的失真 $D$ 随着表示率 $R$ 的增加而呈非递增。这里将 $d_z$ 视为 $R$ 的一个单调代理。\n\n基于这些原则和测量结果，哪个选项最准确地诊断了不同 $d_z$ 下的欠拟合与过拟合，并同时根据率失真行为和对未见类别的重构泛化能力，确定了一个有原则的 $d_z$ 选择？\n\nA. $d_z=2$ 存在欠拟合，$d_z=128$ 存在过拟合；率失真曲线显示，到 $d_z=32$ 时出现收益递减，此时验证误差最小，并且 $d_z=128$ 时 $D_{\\text{unseen}}$ 相对于 $d_z=32$ 的恶化情况也支持选择 $d_z=32$。\n\nB. $d_z=2$ 存在过拟合，因为它丢弃了信息，而 $d_z=128$ 存在欠拟合，因为它记住了训练集；因此，率失真曲线倾向于选择最大的 $d_z=128$。\n\nC. 所有模型都存在欠拟合，因为在所有情况下 $D_{\\text{val}}$ 都非零；为了减少失真，必须总是增加 $d_z$，所以 $d_z=128$ 是首选。\n\nD. $d_z=8$ 存在过拟合，因为 $D_{\\text{unseen}}$ 超过了 $D_{\\text{val}}$；率失真曲线建议选择最小的 $d_z=2$ 以避免方差。", "solution": "问题要求通过改变潜层瓶颈维度 $d_z$ 来分析自编码器模型的欠拟合和过拟合情况，并根据训练集、验证集和未见数据集上的重构误差来选择最佳模型。\n\n### 步骤1：问题验证\n\n**1.1. 提取已知条件**\n- **模型：** 自编码器，包含编码器 $E: x \\mapsto z$ 和解码器 $D: z \\mapsto \\hat{x}$。\n- **输入数据：** 灰度图像 $x \\in \\mathbb{R}^{28 \\times 28}$，归一化到 $[0,1]$。\n- **潜层空间：** $z \\in \\mathbb{R}^{d_z}$。\n- **瓶颈大小：** $d_z \\in \\{2, 8, 32, 128\\}$。\n- **训练目标：** 在训练集上最小化均方误差 (MSE)。\n- **数据集：**\n    - 训练集：数字 $\\{0,1,2,3,4,5,6,7\\}$。\n    - 验证集：从数字 $\\{0,1,2,3,4,5,6,7\\}$ 中预留的分割。\n    - 未见数据集：来自相同图像域的未见过数字 $\\{8,9\\}$ 的图像。\n- **测得的 MSE：**\n    - 对于 $d_z=2$：$D_{\\text{train}}=0.060$, $D_{\\text{val}}=0.062$, $D_{\\text{unseen}}=0.080$。\n    - 对于 $d_z=8$：$D_{\\text{train}}=0.030$, $D_{\\text{val}}=0.033$, $D_{\\text{unseen}}=0.060$。\n    - 对于 $d_z=32$：$D_{\\text{train}}=0.015$, $D_{\\text{val}}=0.020$, $D_{\\text{unseen}}=0.045$。\n    - 对于 $d_z=128$：$D_{\\text{train}}=0.005$, $D_{\\text{val}}=0.030$, $D_{\\text{unseen}}=0.090$。\n- **基本原则：**\n    1. 经验风险最小化 (ERM)。\n    2. 欠拟合（高偏差）和过拟合（高方差，大泛化差距）的定义。\n    3. 率失真原理：失真 $D$ 随着率 $R$（由 $d_z$ 代理）的增加而非递增。\n\n**1.2. 使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地建立在标准的机器学习和深度学习概念之上。自编码器、MSE 损失、训练/验证集划分、在分布外 (OOD) 数据上的泛化测试、偏差-方差权衡（体现为欠拟合/过拟合）以及率失真权衡，都是基本且公认的原则。所提供的数值数据对于此类实验是合理的。\n- **良定性：** 该问题提供了足够的数据（各种条件下的 MSE）和指导原则，足以得出一个关于模型性能和选择的唯一的、合理的结论。\n- **客观性：** 该问题使用精确、客观的术语和数值数据进行陈述，没有为主观解释留下空间。\n- **结论：** 问题陈述是有效的、科学上合理的、良定的且内部一致的。\n\n### 步骤2：求解推导\n\n我们将根据所提供的原则和数据，分析自编码器在每个瓶颈大小 $d_z$ 下的性能。瓶颈维度 $d_z$ 可作为模型复杂度或容量的代理。增加 $d_z$ 允许模型存储更多关于训练数据的信息。\n\n**对不同 $d_z$ 下的 MSE 进行分析：**\n1.  **训练误差 ($D_{\\text{train}}$)：** 训练误差分别为 $0.060, 0.030, 0.015, 0.005$。随着 $d_z$ 的增加，$D_{\\text{train}}$ 单调递减。这与应用于经验训练分布的率失真原理一致：一个容量更高的模型（更大的 $d_z$）可以在其训练数据上达到更低的失真（重构误差）。\n\n2.  **验证误差 ($D_{\\text{val}}$)：** 验证误差分别为 $0.062, 0.033, 0.020, 0.030$。该误差估计了模型在来自同一数据分布的新样本上的性能。当 $d_z$ 从 $2$ 增加到 $32$ 时，误差初始下降，但当 $d_z$ 从 $32$ 增加到 $128$ 时，误差又开始上升。这条验证误差的“U形”曲线是偏差-方差权衡的经典标志。在 $d_z=32$ 时达到最小验证误差。\n\n3.  **泛化差距 ($D_{\\text{val}} - D_{\\text{train}}$)：** 这个差距表明模型在未见数据上的表现比在训练数据上差多少。\n    - $d_z=2$: $0.062 - 0.060 = 0.002$\n    - $d_z=8$: $0.033 - 0.030 = 0.003$\n    - $d_z=32$: $0.020 - 0.015 = 0.005$\n    - $d_z=128$: $0.030 - 0.005 = 0.025$\n    对于较小的 $d_z$，差距非常小，而对于 $d_z=128$，差距急剧增大。\n\n**诊断欠拟合与过拟合：**\n- **$d_z=2$：** 该模型具有高训练误差 ($D_{\\text{train}}=0.060$) 和高验证误差 ($D_{\\text{val}}=0.062$)。模型受到过多限制，无法捕捉数字的本质特征。这是一个典型的**欠拟合**（高偏差）案例。\n- **$d_z=128$：** 该模型具有极低的训练误差 ($D_{\\text{train}}=0.005$)，但验证误差显著更高 ($D_{\\text{val}}=0.030$)，且比 $d_z=32$ 时的值有所增加。泛化差距非常大 ($0.025$)。这表明模型利用其高容量记住了训练集的噪声和特质，未能泛化到验证集。这是一个典型的**过拟合**（高方差）案例。\n\n**$d_z$ 的原则性选择：**\n一个好的模型应该泛化得好，这意味着它在新的、未见过的数据上应该表现良好。验证集误差 $D_{\\text{val}}$ 是我们衡量这一点的主要指标。\n- 验证误差在 $d_z=32$ 时最小，为 $D_{\\text{val}}=0.020$。这表明对于在相同数据分布上的泛化能力而言，$d_z=32$ 是最佳选择。\n- 当为验证集绘制率失真曲线时，曲线显示增加率（代理变量 $d_z$）在失真减少方面，直到 $d_z=32$ 时收益递减，此后收益为负。因此，$d_z=32$ 代表了这条权衡曲线上的最优点。\n\n**对未见类别的泛化能力 ($D_{\\text{unseen}}$)：**\n该指标测试模型对分布外数据的泛化能力。一个学习了“什么是数字”的基本特征的模型，应该仍然能够相当好地重构未见过的数字类别。\n- 误差分别为 $0.080, 0.060, 0.045, 0.090$。\n- 未见类别上的误差也是在 $d_z=32$ 时最小。\n- 在 $d_z=128$ 时的性能下降尤其严重 ($D_{\\text{unseen}}=0.090$)，甚至远差于 $d_z=2$ 的欠拟合模型。这证实了过拟合模型学习到的表示不仅特定于训练集实例，还特定于训练集中出现的*类别* $\\{0-7\\}$，使其对于新类别 $\\{8,9\\}$ 变得脆弱且无效。\n\n**结论：** 分析指出 $d_z=2$ 是一个欠拟合模型，$d_z=128$ 是一个过拟合模型，而 $d_z=32$ 是最佳选择，因为它最小化了验证集和未见类别集上的误差，代表了偏差和方差的最佳平衡。\n\n### 步骤3：逐项分析选项\n\n**A. $d_z=2$ 存在欠拟合，$d_z=128$ 存在过拟合；率失真曲线显示，到 $d_z=32$ 时出现收益递减，此时验证误差最小，并且 $d_z=128$ 时 $D_{\\text{unseen}}$ 相对于 $d_z=32$ 的恶化情况也支持选择 $d_z=32$。**\n- 诊断 $d_z=2$ 欠拟合（高 $D_{\\text{train}}$）和 $d_z=128$ 过拟合（低 $D_{\\text{train}}$，大泛化差距）是正确的。\n- 率失真曲线（针对验证性能）显示收益递减并在 $d_z=32$（最小 $D_{\\text{val}}$）处找到最优点的说法是正确的。\n- 使用 $D_{\\text{unseen}}$ 数据来确认选择 $d_z=32$ 也是正确的，因为在 $d_z=128$ 的过拟合模型对新类别的泛化能力非常差。\n该陈述与我们的分析完全一致。\n**结论：正确。**\n\n**B. $d_z=2$ 存在过拟合，因为它丢弃了信息，而 $d_z=128$ 存在欠拟合，因为它记住了训练集；因此，率失真曲线倾向于选择最大的 $d_z=128$。**\n- 术语用反了。一个“丢弃信息”且在训练集上有高误差的模型是**欠拟合**（高偏差），而不是过拟合。一个“记住训练集”的模型是**过拟合**（高方差），而不是欠拟合。整个诊断的前提都是错误的。\n**结论：错误。**\n\n**C. 所有模型都存在欠拟合，因为在所有情况下 $D_{\\text{val}}$ 都非零；为了减少失真，必须总是增加 $d_z$，所以 $d_z=128$ 是首选。**\n- 一个非零的验证误差意味着欠拟合的说法是错误的。大多数现实世界问题都有一个不可约减的非零误差。过拟合是由训练误差和验证误差之间的*差距*来定义的，而不是验证误差的绝对大小。\n- “必须总是增加 $d_z$”以减少失真的说法仅对训练集误差 ($D_{\\text{train}}$) 成立。正如 $D_{\\text{val}}$ 和 $D_{\\text{unseen}}$ 的数据清楚显示的那样，将 $d_z$ 增加到超过某一点会因过拟合而*增加*未见数据上的失真。这个建议是错误的。\n**结论：错误。**\n\n**D. $d_z=8$ 存在过拟合，因为 $D_{\\text{unseen}}$ 超过了 $D_{\\text{val}}$；率失真曲线建议选择最小的 $d_z=2$ 以避免方差。**\n- $D_{\\text{unseen}}  D_{\\text{val}}$（即 $0.060  0.033$）是预料之中的，因为未见数据集是分布外的。这个事实本身并不能诊断为过拟合。关键指标，即泛化差距 $D_{\\text{val}} - D_{\\text{train}}$，对于 $d_z=8$ 来说非常小（$0.003$），表明这是一个均衡良好的模型，而不是一个过拟合的模型。\n- 选择 $d_z=2$ 以“避免方差”的建议是一个糟糕的策略。虽然这个模型方差低，但它遭受了极高的偏差（欠拟合），使其整体上成为一个差的模型。目标是平衡偏差和方差，而不是以产生巨大偏差为代价来最小化方差。\n**结论：错误。**", "answer": "$$\\boxed{A}$$", "id": "3135723"}, {"introduction": "重构损失的选择并非随意的；它内含了对数据噪声特性的深刻假设。本练习挑战你将常见的损失函数（如 $L_1$ 和 $L_2$ 范数）与其在最大似然估计（MLE）中分别对应拉普拉斯分布和高斯分布的概率起源联系起来。理解这种联系是设计鲁棒自编码器的关键，使其能够很好地适应你的数据特性，尤其是在存在异常值的情况下。[@problem_id:3099270]", "problem": "考虑一个自动编码器，其编码器 $e_{\\phi}$ 将数据向量 $x \\in \\mathbb{R}^d$ 映射到潜层表示 $z \\in \\mathbb{R}^k$，其解码器通过位置参数 $\\mu_{\\theta}(z) \\in \\mathbb{R}^d$ 和一个固定的尺度参数来参数化条件分布 $p_{\\theta}(x \\mid z)$。训练过程通过最小化条件模型 $p_{\\theta}(x \\mid z)$ 下的经验负对数似然来进行，其中 $z = e_{\\phi}(x)$。假设数据是独立同分布（i.i.d.）的，并且给定 $z$ 时坐标是条件独立的。假设观测数据由 $x = x^{\\star} + \\epsilon$ 生成，其中 $x^{\\star}$ 是一个干净信号，而 $\\epsilon$ 是对称的、中位数为零且独立于 $x^{\\star}$ 的重尾噪声。你用两种方式对 $p_{\\theta}(x \\mid z)$ 进行建模：一种是固定 $\\sigma^2 > 0$ 的高斯分布 $\\mathcal{N}(\\mu_{\\theta}(z), \\sigma^2 I)$，另一种是固定尺度 $b > 0$ 的拉普拉斯分布 $\\mathrm{Laplace}(\\mu_{\\theta}(z), b)$。在最大似然估计（MLE）下，训练目标是最小化关于 $\\phi$ 和 $\\theta$ 的经验负对数似然。\n\n基于最大似然估计（MLE）的基本原理和标准概率密度函数，关于在重尾噪声下选择高斯解码器与拉普拉斯解码器对学习到的潜层 $z$ 和重构行为的影响，以下哪些陈述是正确的？\n\nA. 在使用固定尺度 $b$ 的拉普拉斯解码器下，MLE 目标使 $\\mu_{\\theta}(z)$ 的每个坐标与给定 $z$ 时 $x$ 的条件中位数对齐，从而引出比使用高斯解码器获得的潜层表示 $z$ 对异常值更不敏感。\n\nB. 在使用固定方差 $\\sigma^2$ 的高斯解码器下，重构项关于 $\\mu_{\\theta}(z)$ 在每个坐标上的梯度与残差成正比，并且其大小随残差大小线性增长，因此大的异常值会产生大的梯度，这些梯度可能会主导对 $z$ 的更新。\n\nC. 在重尾噪声设置中，由高斯解码器引出的 $L_2$ 重构会减小极端残差的影响，因为对于大的残差大小，其导数是有界的，这有助于防止表示崩溃。\n\nD. 从高斯似然切换到拉普拉斯似然，倾向于使学习到的潜层 $z$ 编码更少关于异常值污染的信息，而更多地编码关于 $x$ 的中心结构的信息，因为其引出的估计器目标是条件中位数而不是均值。\n\nE. 对于任何重尾噪声分布和任何数据集，当高斯解码器和拉普拉斯解码器具有相同的解码器容量和优化条件时，拉普拉斯解码器保证其期望重构误差严格低于高斯解码器。\n\n选择所有适用项。", "solution": "用户希望对选择高斯与拉普拉斯似然作为自动编码器解码器的影响进行批判性分析，特别是在存在重尾噪声的情况下。该分析必须基于最大似然估计（MLE）的原理。\n\n**问题陈述验证**\n\n*   **步骤1：提取已知条件**\n    *   编码器：$e_{\\phi}: \\mathbb{R}^d \\to \\mathbb{R}^k$，将数据向量 $x$ 映射到潜层向量 $z = e_{\\phi}(x)$。\n    *   解码器：通过位置参数 $\\mu_{\\theta}(z) \\in \\mathbb{R}^d$ 和一个固定的尺度参数来参数化条件分布 $p_{\\theta}(x \\mid z)$。\n    *   训练目标：最小化经验负对数似然，$\\sum_i -\\log p_{\\theta}(x_i \\mid e_{\\phi}(x_i))$。\n    *   数据生成过程：$x = x^{\\star} + \\epsilon$，其中 $x^{\\star}$ 是一个干净信号，而 $\\epsilon$ 是对称的、中位数为零且独立于 $x^{\\star}$ 的重尾噪声。\n    *   数据假设：数据点独立同分布（i.i.d.），并且给定 $z$ 时 $x$ 的坐标条件独立。\n    *   解码器模型：\n        1.  高斯分布：$p_{\\theta}(x \\mid z) = \\mathcal{N}(\\mu_{\\theta}(z), \\sigma^2 I)$，具有固定的方差 $\\sigma^2 > 0$。\n        2.  拉普拉斯分布：$p_{\\theta}(x \\mid z) = \\mathrm{Laplace}(\\mu_{\\theta}(z), b)$，具有固定的尺度 $b > 0$。\n\n*   **步骤2：使用提取的已知条件进行验证**\n    *   **科学基础**：该问题在概率机器学习、统计估计理论和深度学习的原理方面有很好的基础。似然函数（高斯、拉普拉斯）和损失函数（$L_2$、$L_1$）之间的联系是一个基本概念。对重尾噪声的鲁棒性研究是统计学中的一个经典课题，也是机器学习中的一个实际问题。\n    *   **良置性**：该问题是良置的。它要求在特定条件下对两种不同建模选择的行为进行定性分析。所提供的信息足以推断出由此产生的训练目标的属性及其对学习表示的影响。\n    *   **客观性**：该问题使用来自数学和机器学习的精确、客观和标准的术语来陈述。\n\n*   **步骤3：结论与行动**\n    *   问题陈述是有效的。它在科学上是合理的、良置的且客观的。我将进行完整的推导和分析。\n\n**从第一性原理推导**\n\n训练目标是最小化给定模型下数据的负对数似然（NLL）。条件分布 $p_{\\theta}(x \\mid z)$ 的选择决定了重构损失函数。\n\n**情况1：高斯解码器**\n似然函数为 $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; \\mu_{\\theta}(z), \\sigma^2 I)$。由于坐标的条件独立性，概率密度函数为：\n$$ p_{\\theta}(x \\mid z) = \\prod_{j=1}^{d} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_j - \\mu_{\\theta,j}(z))^2}{2\\sigma^2}\\right) $$\n单个数据点 $x$ 的负对数似然是：\n$$ -\\log p_{\\theta}(x \\mid z) = - \\sum_{j=1}^{d} \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(x_j - \\mu_{\\theta,j}(z))^2}{2\\sigma^2} \\right] $$\n$$ = \\frac{d}{2}\\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{j=1}^{d} (x_j - \\mu_{\\theta,j}(z))^2 $$\n由于 $\\sigma^2$ 是固定的正常数，最小化此式等价于最小化平方误差和。这对应于 $L_2$ 损失：\n$$ L_{\\text{Gaussian}} \\propto \\|x - \\mu_{\\theta}(z)\\|_2^2 $$\n统计学中的一个标准结论是，最小化期望平方误差的估计器是条件均值。因此，该目标鼓励解码器输出 $\\mu_{\\theta}(z)$ 来逼近条件均值 $\\mathbb{E}[x \\mid z]$。众所周知，均值对异常值高度敏感，而异常值是重尾噪声的特征。\n\n**情况2：拉普拉斯解码器**\n似然函数为 $p_{\\theta}(x \\mid z) = \\mathrm{Laplace}(x; \\mu_{\\theta}(z), b)$。概率密度函数为：\n$$ p_{\\theta}(x \\mid z) = \\prod_{j=1}^{d} \\frac{1}{2b} \\exp\\left(-\\frac{|x_j - \\mu_{\\theta,j}(z)|}{b}\\right) $$\n单个数据点 $x$ 的负对数似然是：\n$$ -\\log p_{\\theta}(x \\mid z) = - \\sum_{j=1}^{d} \\left[ -\\log(2b) - \\frac{|x_j - \\mu_{\\theta,j}(z)|}{b} \\right] $$\n$$ = d\\log(2b) + \\frac{1}{b} \\sum_{j=1}^{d} |x_j - \\mu_{\\theta,j}(z)| $$\n由于 $b$ 是固定的正常数，最小化此NLL等价于最小化绝对误差和。这对应于 $L_1$ 损失：\n$$ L_{\\text{Laplace}} \\propto \\|x - \\mu_{\\theta}(z)\\|_1 $$\n最小化期望绝对误差的估计器是条件中位数。因此，该目标鼓励 $\\mu_{\\theta}(z)$ 来逼近给定 $z$ 时 $x$ 的条件中位数。众所周知，中位数对异常值是鲁棒的。\n\n**逐项分析**\n\n**A. 在使用固定尺度 $b$ 的拉普拉斯解码器下，MLE 目标使 $\\mu_{\\theta}(z)$ 的每个坐标与给定 $z$ 时 $x$ 的条件中位数对齐，从而引出比使用高斯解码器获得的潜层表示 $z$ 对异常值更不敏感。**\n如上所述，最小化拉普拉斯似然的NLL等价于最小化 $L_1$ 损失，其目的是寻找条件中位数。中位数是一种鲁棒的统计量，意味着它不受异常值的严重影响。因为重构目标是鲁棒的，反向传播到编码器的梯度也将更少地受到异常值大小的影响，从而引导编码器学习能够捕捉数据中心趋势的表示 $z=e_{\\phi}(x)$，使其对噪声不那么敏感。该陈述是对统计学原理在这种深度学习背景下的正确应用。\n**结论：正确**\n\n**B. 在使用固定方差 $\\sigma^2$ 的高斯解码器下，重构项关于 $\\mu_{\\theta}(z)$ 在每个坐标上的梯度与残差成正比，并且其大小随残差大小线性增长，因此大的异常值会产生大的梯度，这些梯度可能会主导对 $z$ 的更新。**\n第 $j$ 个坐标的重构损失为 $L_j = C + \\frac{1}{2\\sigma^2}(x_j - \\mu_{\\theta,j}(z))^2$。损失关于解码器输出 $\\mu_{\\theta,j}(z)$ 的梯度是：\n$$ \\frac{\\partial L_j}{\\partial \\mu_{\\theta,j}(z)} = \\frac{\\partial}{\\partial \\mu_{\\theta,j}(z)} \\left( \\frac{(x_j - \\mu_{\\theta,j}(z))^2}{2\\sigma^2} \\right) = \\frac{1}{\\sigma^2} (x_j - \\mu_{\\theta,j}(z)) \\cdot (-1) = -\\frac{1}{\\sigma^2}(x_j - \\mu_{\\theta,j}(z)) $$\n梯度与残差 $x_j - \\mu_{\\theta,j}(z)$ 成正比。其大小 $|-\\frac{1}{\\sigma^2}(x_j - \\mu_{\\theta,j}(z))|$ 随残差的大小线性增长。一个大的异常值会产生一个大的残差，进而产生一个大的梯度。这个大梯度随后通过解码器和编码器反向传播，可能导致模型参数（包括定义潜层表示 $z$ 的参数）发生大的、不稳定的更新。该陈述完全准确。\n**结论：正确**\n\n**C. 在重尾噪声设置中，由高斯解码器引出的 $L_2$ 重构会减小极端残差的影响，因为对于大的残差大小，其导数是有界的，这有助于防止表示崩溃。**\n该陈述在事实上是不正确的。如选项 B 的分析所示，$L_2$ 损失关于模型输出的导数与残差是*线性*关系，而不是有界的。具有有界导数的是 $L_1$ 损失（来自拉普拉斯解码器）。$|r_j|/b$ 关于重构（影响 $r_j$）的导数是 $-\\mathrm{sgn}(r_j)/b$，对于任何非零残差 $r_j$，其大小恒为 $1/b$。该陈述错误地将 $L_1$ 损失的鲁棒性属性归因于 $L_2$ 损失。\n**结论：不正确**\n\n**D. 从高斯似然切换到拉普拉斯似然，倾向于使学习到的潜层 $z$ 编码更少关于异常值污染的信息，而更多地编码关于 $x$ 的中心结构的信息，因为其引出的估计器目标是条件中位数而不是均值。**\n该陈述正确地综合了模型选择的后果。从高斯似然切换到拉普拉斯似然意味着将优化目标从 $L_2$ 损失切换到 $L_1$ 损失。如前所述，这将估计目标从条件均值改变为条件中位数。中位数对异常值（“异常值污染”）是鲁棒的，而均值对它们敏感。因此，使用拉普拉斯解码器训练的自动编码器因未能完美重构异常值而受到的惩罚较小，并将更多能力集中在建模数据的“中心结构”上，这对应于干净信号 $x^{\\star}$。因此，得到的潜层编码 $z$ 将更好地反映这种底层结构，而不是被噪声扭曲。此推理是合理的。\n**结论：正确**\n\n**E. 对于任何重尾噪声分布和任何数据集，当高斯解码器和拉普拉斯解码器具有相同的解码器容量和优化条件时，拉普拉斯解码器保证其期望重构误差严格低于高斯解码器。**\n该陈述做出了一个普适性声明，在机器学习的背景下过于强烈以至于不成立。\n1.  **随机性**：训练好的神经网络的最终性能取决于特定的随机初始化、优化算法的随机性（例如 SGD）以及有限数据集的具体实现。在某次特定的训练运行中，高斯模型有可能找到比拉普拉斯模型更好的局部最小值。\n2.  **缺乏绝对规则**：没有数学定理能保证在每一种可能的情况下都会有这个结果。人们可以构建一些有限的数据集，即使是从重尾分布中抽取的，其中样本均值恰好比样本中位数更能估计真实的中心趋势。\n3.  **优化问题**：$L_1$ 损失函数在零点处不可微，这有时会比平滑的 $L_2$ 损失带来一些轻微的优化挑战，尽管这在实践中已得到很好的处理。\n该陈述使用“任何”和“保证”使其过于绝对，因此不正确。\n**结论：不正确**", "answer": "$$\\boxed{ABD}$$", "id": "3099270"}, {"introduction": "一个好的表示不仅仅意味着低的重构误差，它还应该以有意义的方式组织数据。这个编码练习提供了一种直接的方法来评估所学潜在空间的几何结构。你将实现一个最近原型分类器，以测试自编码器是否成功地将来自同一类的数据点聚集在一起，从而为潜在空间捕获类别区分信息的能力提供一个量化度量。[@problem_id:3099275]", "problem": "您将要完成一个表示学习评估任务，该任务基于自编码器嵌入的概念。考虑一个自编码器，其编码器被建模为从输入空间到潜空间的仿射映射。形式上，对于一个输入向量 $x \\in \\mathbb{R}^d$，其潜表示为 $z = W x + b$，其中 $W \\in \\mathbb{R}^{k \\times d}$ 和 $b \\in \\mathbb{R}^k$ 是固定参数。为评估潜空间 $z$ 是否与类别结构对齐，您需要计算类别原型，并在预留的测试数据上评估最近原型分类。您的程序必须从基本原理开始实现以下内容。\n\n要使用的基本依据：\n- 将自编码器的编码器定义为一个确定性函数 $f_{\\text{enc}}: \\mathbb{R}^d \\to \\mathbb{R}^k$，由 $z = W x + b$ 给出。\n- 经验类别原型定义为一个类别的潜编码的样本均值。\n- $\\mathbb{R}^k$ 中的欧几里得距离定义为 $\\|u - v\\|_2$。\n- 最近原型分类规则：将一个测试点分配给其原型在欧几里得距离上最近的类别。如果出现距离完全相等的情况（最小距离相同），则选择类别标签值最小的类别来打破平局。\n\n任务：\n- 对于下方的每个测试用例，计算所有训练输入的潜编码 $z$，计算潜空间中每个类别的原型（经验均值），然后根据最近原型（欧几里得距离）和指定的打破平局规则对每个测试输入进行分类，并输出分类准确率（即正确分类的测试样本所占的比例）。报告每个准确率时，四舍五入到小数点后$3$位。\n\n重要实现细节：\n- 类别由整数（如 $0$、$1$、$2$ 等）索引。\n- 准确率必须计算为 $(\\text{正确预测数}) / (\\text{测试样本数})$，并以小数形式表示（而非百分比）。\n\n测试套件（四个用例），涵盖分离、重叠、坍缩和带平局的多类别情况：\n- 用例1（良好分离的类别，完美对齐）：\n  - 维度：$d = 2$，$k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[-1, 0]$，$[0, 0]$，$[1, 0]$。\n    - 类别 $1$：$[4, 0]$，$[5, 0]$，$[6, 0]$。\n  - 测试数据和标签：\n    - 输入：$[-0.5, 0]$，$[0.5, 0]$，$[4.5, 0]$，$[5.5, 0]$。\n    - 标签：$[0, 0, 1, 1]$。\n- 用例2（导致一些错误的重叠类别）：\n  - 维度：$d = 2$，$k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[0, 0]$，$[0, 0]$。\n    - 类别 $1$：$[1, 0]$，$[1, 0]$。\n  - 测试数据和标签：\n    - 输入：$[0.1, 0]$，$[0.9, 0]$，$[0.49, 0]$，$[0.51, 0]$。\n    - 标签：$[0, 1, 1, 1]$。\n- 用例3（坍缩的编码器；相同的原型；测试打破平局规则和随机水平的准确率）：\n  - 维度：$d = 2$，$k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[1, 2]$，$[3, 4]$。\n    - 类别 $1$：$[-1, -2]$，$[-3, -4]$。\n  - 测试数据和标签：\n    - 输入：$[10, 10]$，$[-10, -10]$，$[7, -7]$，$[-7, 7]$。\n    - 标签：$[0, 1, 1, 0]$。\n- 用例4（使用各向异性编码器并出现明确平局的多类别情况）：\n  - 维度：$d = 2$，$k = 2$。\n  - 编码器：$W = \\begin{bmatrix} 2  0 \\\\ 0  0.5 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$。\n  - 训练数据：\n    - 类别 $0$：$[-0.5, 2]$，$[-0.5, 2]$。\n    - 类别 $1$：$[1.5, 2]$，$[1.5, 2]$。\n    - 类别 $2$：$[-0.5, 8]$，$[-0.5, 8]$。\n  - 测试数据和标签：\n    - 输入：$[-0.5, 2.2]$，$[1.5, 1.8]$，$[-0.5, 5]$，$[-0.5, 8.2]$。\n    - 标签：$[0, 1, 2, 2]$。\n\n程序要求：\n- 仅使用指定的编码器模型、欧几里得距离、潜空间中的经验类别均值以及指定的打破平局规则，实现所述的完整流程。\n- 您的程序应生成单行输出，其中包含四个准确率，每个准确率四舍五入到小数点后3位，形式为用方括号括起来的逗号分隔列表（例如，$[0.875,0.500,1.000,0.750]$）。", "solution": "该问题陈述是一个有效的计算任务，它基于线性代数和基础统计学的原理，常用于评估表示学习方法。这是一个适定问题，所有必要的参数、数据和程序规则都已明确定义，确保每个测试用例都有唯一且可验证的解。我们现在将逐步进行求解。\n\n任务的核心是在由仿射变换定义的潜空间中实现并评估一个最近原型分类器。每个测试用例的处理过程包括三个主要阶段：\n1.  **潜空间变换**：使用给定的编码器函数，将所有训练和测试数据点从输入空间 $\\mathbb{R}^d$ 映射到潜空间 $\\mathbb{R}^k$。\n2.  **原型计算**：对于每个类别，通过计算其训练成员的潜编码的经验均值来计算一个单一的代表向量，即“原型”。\n3.  **分类与评估**：对于每个测试点，通过找到欧几里得距离最近的原型来预测其类别。最终的准确率是正确分类的测试点所占的比例。\n\n让我们将这些步骤形式化。编码器是一个仿射映射 $f_{\\text{enc}}: \\mathbb{R}^d \\to \\mathbb{R}^k$，定义为：\n$$z = f_{\\text{enc}}(x) = W x + b$$\n其中 $x \\in \\mathbb{R}^d$ 是输入向量，$W \\in \\mathbb{R}^{k \\times d}$ 是权重矩阵，$b \\in \\mathbb{R}^k$ 是偏置向量。\n\n对于给定的类别 $C_j$ 及其训练样本集 $S_j = \\{x_1, x_2, \\ldots, x_{N_j}\\}$，其对应的潜编码为 $Z_j = \\{z_1, z_2, \\ldots, z_{N_j}\\}$，其中 $z_i = f_{\\text{enc}}(x_i)$。类别原型 $P_j \\in \\mathbb{R}^k$ 是这些潜编码的样本均值：\n$$P_j = \\frac{1}{N_j} \\sum_{i=1}^{N_j} z_i$$\n对于一个新的测试输入 $x_{\\text{test}}$，其潜编码为 $z_{\\text{test}} = f_{\\text{enc}}(x_{\\text{test}})$。预测的类别标签 $\\hat{y}$ 由最近原型规则确定：\n$$\\hat{y} = \\underset{j}{\\arg\\min} \\, \\| z_{\\text{test}} - P_j \\|_2$$\n其中 $\\| \\cdot \\|_2$ 表示欧几里得距离。如果多个类别都达到最小距离，则选择整数标签最小的类别来打破平局。\n\n最后，准确率计算为正确分类的测试样本数与测试样本总数的比率。我们现在将此过程应用于四个测试用例中的每一个。\n\n**用例1：良好分离的类别**\n- 维度：$d=2$，$k=2$。\n- 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。该变换为恒等变换，$z = x$。\n- 类别 $0$ 的训练数据：$\\left\\{ \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\}$。\n- 类别 $0$ 的原型：$P_0 = \\frac{1}{3} \\left( \\begin{bmatrix} -1 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别 $1$ 的训练数据：$\\left\\{ \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} \\right\\}$。\n- 类别 $1$ 的原型：$P_1 = \\frac{1}{3} \\left( \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 6 \\\\ 0 \\end{bmatrix} \\right) = \\begin{bmatrix} 5 \\\\ 0 \\end{bmatrix}$。\n- 测试数据：$z_{\\text{test},1}=\\begin{bmatrix} -0.5 \\\\ 0 \\end{bmatrix}$，$z_{\\text{test},2}=\\begin{bmatrix} 0.5 \\\\ 0 \\end{bmatrix}$，$z_{\\text{test},3}=\\begin{bmatrix} 4.5 \\\\ 0 \\end{bmatrix}$，$z_{\\text{test},4}=\\begin{bmatrix} 5.5 \\\\ 0 \\end{bmatrix}$。真实标签：$[0, 0, 1, 1]$。\n- 预测：\n  - 对于 $z_{\\text{test},1}$：$\\|z_{\\text{test},1} - P_0\\|_2=0.5$，$\\|z_{\\text{test},1} - P_1\\|_2=5.5$。预测为$0$。正确。\n  - 对于 $z_{\\text{test},2}$：$\\|z_{\\text{test},2} - P_0\\|_2=0.5$，$\\|z_{\\text{test},2} - P_1\\|_2=4.5$。预测为$0$。正确。\n  - 对于 $z_{\\text{test},3}$：$\\|z_{\\text{test},3} - P_0\\|_2=4.5$，$\\|z_{\\text{test},3} - P_1\\|_2=0.5$。预测为$1$。正确。\n  - 对于 $z_{\\text{test},4}$：$\\|z_{\\text{test},4} - P_0\\|_2=5.5$，$\\|z_{\\text{test},4} - P_1\\|_2=0.5$。预测为$1$。正确。\n- 准确率：$4/4 = 1.0$。\n\n**用例2：重叠的类别**\n- 维度：$d=2$，$k=2$。\n- 编码器：$W = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。该变换为恒等变换，$z = x$。\n- 类别 $0$ 的训练数据：$\\left\\{ \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\right\\}$。原型 $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别 $1$ 的训练数据：$\\left\\{ \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\right\\}$。原型 $P_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n- 决策边界是连接 $P_0$ 和 $P_1$ 的线段的垂直平分线，即直线 $x=0.5$。\n- 测试数据：$z_{\\text{test},1}=\\begin{bmatrix} 0.1 \\\\ 0 \\end{bmatrix}$，$z_{\\text{test},2}=\\begin{bmatrix} 0.9 \\\\ 0 \\end{bmatrix}$，$z_{\\text{test},3}=\\begin{bmatrix} 0.49 \\\\ 0 \\end{bmatrix}$，$z_{\\text{test},4}=\\begin{bmatrix} 0.51 \\\\ 0 \\end{bmatrix}$。真实标签：$[0, 1, 1, 1]$。\n- 预测：\n  - 对于 $z_{\\text{test},1}$：$\\|z_{\\text{test},1} - P_0\\|_2=0.1$，$\\|z_{\\text{test},1} - P_1\\|_2=0.9$。预测为$0$。正确。\n  - 对于 $z_{\\text{test},2}$：$\\|z_{\\text{test},2} - P_0\\|_2=0.9$，$\\|z_{\\text{test},2} - P_1\\|_2=0.1$。预测为$1$。正确。\n  - 对于 $z_{\\text{test},3}$：$\\|z_{\\text{test},3} - P_0\\|_2=0.49$，$\\|z_{\\text{test},3} - P_1\\|_2=0.51$。预测为$0$。不正确（真实标签为$1$）。\n  - 对于 $z_{\\text{test},4}$：$\\|z_{\\text{test},4} - P_0\\|_2=0.51$，$\\|z_{\\text{test},4} - P_1\\|_2=0.49$。预测为$1$。正确。\n- 准确率：$3/4 = 0.75$。\n\n**用例3：坍缩的编码器**\n- 维度：$d=2$，$k=2$。\n- 编码器：$W = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。所有输入都映射到原点：$z = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别 $0$ 的训练数据：$\\left\\{ \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 3 \\\\ 4 \\end{bmatrix} \\right\\}$。两者都映射到 $z=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。原型 $P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 类别 $1$ 的训练数据：$\\left\\{ \\begin{bmatrix} -1 \\\\ -2 \\end{bmatrix}, \\begin{bmatrix} -3 \\\\ -4 \\end{bmatrix} \\right\\}$。两者都映射到 $z=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。原型 $P_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 两个类别的原型相同。\n- 测试数据：任何测试输入 $x_{\\text{test}}$ 都映射到 $z_{\\text{test}} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 预测：对于任何 $z_{\\text{test}}$，到两个原型的距离都为 $0$：$\\|z_{\\text{test}} - P_0\\|_2 = 0$ 和 $\\|z_{\\text{test}} - P_1\\|_2 = 0$。这是一个完全相等的情况。根据打破平局的规则，我们选择最小的类别标签，即$0$。因此，每个测试样本都被预测为类别$0$。\n- 预测标签：$[0, 0, 0, 0]$。真实标签：$[0, 1, 1, 0]$。\n- 第一个和第四个样本预测正确。\n- 准确率：$2/4 = 0.5$。\n\n**用例4：带各向异性编码器和出现平局的多类别情况**\n- 维度：$d=2$，$k=2$。\n- 编码器：$W = \\begin{bmatrix} 2  0 \\\\ 0  0.5 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$。映射为 $z = \\begin{bmatrix} 2x_1+1 \\\\ 0.5x_2-1 \\end{bmatrix}$。\n- 原型：\n  - 类别 $0$（训练数据：$\\begin{bmatrix} -0.5 \\\\ 2 \\end{bmatrix}$）：$z = \\begin{bmatrix} 2(-0.5)+1 \\\\ 0.5(2)-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。所以，$P_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 类别 $1$（训练数据：$\\begin{bmatrix} 1.5 \\\\ 2 \\end{bmatrix}$）：$z = \\begin{bmatrix} 2(1.5)+1 \\\\ 0.5(2)-1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}$。所以，$P_1 = \\begin{bmatrix} 4 \\\\ 0 \\end{bmatrix}$。\n  - 类别 $2$（训练数据：$\\begin{bmatrix} -0.5 \\\\ 8 \\end{bmatrix}$）：$z = \\begin{bmatrix} 2(-0.5)+1 \\\\ 0.5(8)-1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$。所以，$P_2 = \\begin{bmatrix} 0 \\\\ 3 \\end{bmatrix}$。\n- 测试数据变换与分类。真实标签：$[0, 1, 2, 2]$。\n  - $x_{\\text{test},1} = \\begin{bmatrix} -0.5 \\\\ 2.2 \\end{bmatrix} \\implies z_1 = \\begin{bmatrix} 0 \\\\ 0.1 \\end{bmatrix}$。\n    - $\\|z_1 - P_0\\|_2 = 0.1$，$\\|z_1 - P_1\\|_2 = \\sqrt{4^2+0.1^2} \\approx 4.001$，$\\|z_1 - P_2\\|_2 = 2.9$。预测为$0$。正确。\n  - $x_{\\text{test},2} = \\begin{bmatrix} 1.5 \\\\ 1.8 \\end{bmatrix} \\implies z_2 = \\begin{bmatrix} 4 \\\\ -0.1 \\end{bmatrix}$。\n    - $\\|z_2 - P_0\\|_2 \\approx 4.001$，$\\|z_2 - P_1\\|_2 = 0.1$，$\\|z_2 - P_2\\|_2 = \\sqrt{4^2+(-3.1)^2} \\approx 5.06$。预测为$1$。正确。\n  - $x_{\\text{test},3} = \\begin{bmatrix} -0.5 \\\\ 5 \\end{bmatrix} \\implies z_3 = \\begin{bmatrix} 0 \\\\ 1.5 \\end{bmatrix}$。\n    - $\\|z_3 - P_0\\|_2 = 1.5$，$\\|z_3 - P_1\\|_2 = \\sqrt{4^2+1.5^2} \\approx 4.27$，$\\|z_3 - P_2\\|_2 = 1.5$。\n    - 类别$0$和类别$2$之间出现平局。最小标签是$0$。预测为$0$。不正确（真实标签为$2$）。\n  - $x_{\\text{test},4} = \\begin{bmatrix} -0.5 \\\\ 8.2 \\end{bmatrix} \\implies z_4 = \\begin{bmatrix} 0 \\\\ 3.1 \\end{bmatrix}$。\n    - $\\|z_4 - P_0\\|_2 = 3.1$，$\\|z_4 - P_1\\|_2 \\approx 5.06$，$\\|z_4 - P_2\\|_2 = 0.1$。预测为$2$。正确。\n- 准确率：$3/4 = 0.75$。\n\n计算出的准确率分别为 $1.0$、$0.75$、$0.5$ 和 $0.75$。在最终输出时，这些值将四舍五入到小数点后3位。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes classification accuracy for four test cases using a nearest-prototype\n    classifier in an autoencoder's latent space.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-separated classes, perfect alignment)\n        {\n            'W': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([-1.0, 0.0]), np.array([0.0, 0.0]), np.array([1.0, 0.0])],\n                1: [np.array([4.0, 0.0]), np.array([5.0, 0.0]), np.array([6.0, 0.0])]\n            },\n            'test_inputs': [\n                np.array([-0.5, 0.0]), np.array([0.5, 0.0]),\n                np.array([4.5, 0.0]), np.array([5.5, 0.0])\n            ],\n            'test_labels': [0, 0, 1, 1]\n        },\n        # Case 2 (overlapping classes causing some errors)\n        {\n            'W': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([0.0, 0.0]), np.array([0.0, 0.0])],\n                1: [np.array([1.0, 0.0]), np.array([1.0, 0.0])]\n            },\n            'test_inputs': [\n                np.array([0.1, 0.0]), np.array([0.9, 0.0]),\n                np.array([0.49, 0.0]), np.array([0.51, 0.0])\n            ],\n            'test_labels': [0, 1, 1, 1]\n        },\n        # Case 3 (collapsed encoder; tests tie-breaking)\n        {\n            'W': np.array([[0.0, 0.0], [0.0, 0.0]]),\n            'b': np.array([0.0, 0.0]),\n            'train_data': {\n                0: [np.array([1.0, 2.0]), np.array([3.0, 4.0])],\n                1: [np.array([-1.0, -2.0]), np.array([-3.0, -4.0])]\n            },\n            'test_inputs': [\n                np.array([10.0, 10.0]), np.array([-10.0, -10.0]),\n                np.array([7.0, -7.0]), np.array([-7.0, 7.0])\n            ],\n            'test_labels': [0, 1, 1, 0]\n        },\n        # Case 4 (multi-class with anisotropic encoder and an explicit tie)\n        {\n            'W': np.array([[2.0, 0.0], [0.0, 0.5]]),\n            'b': np.array([1.0, -1.0]),\n            'train_data': {\n                0: [np.array([-0.5, 2.0]), np.array([-0.5, 2.0])],\n                1: [np.array([1.5, 2.0]), np.array([1.5, 2.0])],\n                2: [np.array([-0.5, 8.0]), np.array([-0.5, 8.0])]\n            },\n            'test_inputs': [\n                np.array([-0.5, 2.2]), np.array([1.5, 1.8]),\n                np.array([-0.5, 5.0]), np.array([-0.5, 8.2])\n            ],\n            'test_labels': [0, 1, 2, 2]\n        }\n    ]\n\n    accuracies = []\n    \n    for case in test_cases:\n        W = case['W']\n        b = case['b']\n        train_data = case['train_data']\n        test_inputs = case['test_inputs']\n        test_labels = case['test_labels']\n        \n        prototypes = {}\n        # Ensure class labels are sorted for consistent processing\n        class_labels = sorted(train_data.keys())\n        \n        # 1. Compute class prototypes in latent space\n        for label in class_labels:\n            points = train_data[label]\n            # Encode each training point x to latent code z = Wx + b\n            latent_codes = [W @ x + b for x in points]\n            # Prototype is the mean of the latent codes\n            prototypes[label] = np.mean(latent_codes, axis=0)\n            \n        predictions = []\n        # 2. Classify each test input\n        for x_test in test_inputs:\n            # Encode the test point\n            z_test = W @ x_test + b\n            \n            # Calculate Euclidean distance to each prototype\n            distances = []\n            for label in class_labels:\n                p = prototypes[label]\n                dist = np.linalg.norm(z_test - p)\n                distances.append((dist, label))\n            \n            # Find the minimum distance\n            min_dist = min(d for d, _ in distances)\n            \n            # Find all class labels corresponding to the minimum distance (handles ties)\n            # Use np.isclose for robust floating-point comparison\n            tied_labels = [label for dist, label in distances if np.isclose(dist, min_dist)]\n            \n            # Apply the tie-breaking rule: choose the smallest class label\n            predicted_label = min(tied_labels)\n            predictions.append(predicted_label)\n            \n        # 3. Calculate accuracy for the current case\n        correct_predictions = sum(1 for pred, true in zip(predictions, test_labels) if pred == true)\n        accuracy = correct_predictions / len(test_labels)\n        accuracies.append(accuracy)\n\n    # Format the results as specified: a list of floats rounded to 3 decimal places\n    formatted_accuracies = [f\"{acc:.3f}\" for acc in accuracies]\n    print(f\"[{','.join(formatted_accuracies)}]\")\n\nsolve()\n```", "id": "3099275"}]}