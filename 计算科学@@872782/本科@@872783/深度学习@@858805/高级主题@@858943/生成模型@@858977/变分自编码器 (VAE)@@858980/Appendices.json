{"hands_on_practices": [{"introduction": "变分自编码器（VAE）的目标函数包含两部分：重构损失和KL散度。KL散度项负责对齐编码器的后验分布与先验分布，从而对潜空间施加正则化。虽然在常见的设置（高斯分布）中KL散度有解析解，但在训练中我们常常需要通过蒙特卡洛采样来估计它，理解这种估计的性质对于诊断训练过程中的不稳定性至关重要。本练习 [@problem_id:3197900] 将指导你直接比较KL散度的精确解析值和其蒙特卡洛估计值，并分析样本数量如何影响梯度估计的信噪比，从而加深对VAE训练动态的理解。", "problem": "在给定的变分自编码器 (VAE) 背景下，编码器定义了一个对角多元正态分布 $q_{\\phi}(z \\mid x)$，其参数为 $\\mu \\in \\mathbb{R}^k$ 和 $\\sigma^2 \\in \\mathbb{R}^k_{>0}$，先验分布为标准正态分布 $p(z) = \\mathcal{N}(0, I_k)$。Kullback–Leibler (KL) 散度定义为 $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{q}[\\log q(z \\mid x) - \\log p(z)]$。您的任务是比较通过解析计算的Kullback–Leibler (KL) 散度与蒙特卡洛 (MC) 估计值，并通过梯度信噪比分析估计器噪声如何影响一个简单的训练稳定性代理指标。\n\n使用的基本原理：\n- Kullback–Leibler (KL) 散度的定义：$\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{q}[\\log q(z) - \\log p(z)]$。\n- 重参数化：$z = \\mu + \\sigma \\odot \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, I_k)$，$\\odot$ 表示逐元素乘积。\n- 对角多元正态分布的对数密度：对于 $z \\in \\mathbb{R}^k$，$\\log \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2)) = -\\tfrac{1}{2}\\left(\\sum_{i=1}^k \\tfrac{(z_i - \\mu_i)^2}{\\sigma_i^2} + \\sum_{i=1}^k \\log(2\\pi \\sigma_i^2)\\right)$。\n- 标准正态分布的对数密度：$\\log \\mathcal{N}(z \\mid 0, I_k) = -\\tfrac{1}{2}\\left(\\|z\\|_2^2 + k \\log(2\\pi)\\right)$。\n\n要求：\n1. 基于上述基本原理，为对角多元正态分布 $q_{\\phi}$ 和标准正态分布 $p$ 推导出 $\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z))$ 的解析表达式。使用此解析表达式为给定参数计算精确的 $\\mathrm{KL}$ 值。\n2. 通过使用重参数化 $z^{(s)} = \\mu + \\sigma \\odot \\epsilon^{(s)}$（其中 $\\epsilon^{(s)} \\sim \\mathcal{N}(0, I_k)$）从 $q_{\\phi}(z \\mid x)$ 中抽取 $S$ 个独立样本 $z^{(s)}$，并对 $s = 1, \\dots, S$ 的 $\\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)})$ 进行平均，从而构建 $\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z))$ 的蒙特卡洛估计器。\n3. 使用路径导数定义 KL 项关于 $\\mu$ 的梯度估计器。令 $g^{(s)} = \\nabla_{z}\\left[\\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)})\\right]$ 并注意 $\\nabla_{\\mu} z^{(s)} = I_k$。使用 $g^{(s)}$ 作为关于 $\\mu$ 的单样本梯度估计。对于给定的样本量 $S$，计算 $S$ 个样本的经验均值向量 $\\bar{g}$ 和经验每坐标标准差向量 $\\operatorname{std}(g)$。将信噪比定义为 $\\mathrm{SNR} = \\frac{\\|\\bar{g}\\|_2}{\\|\\operatorname{std}(g)\\|_2}$。如果分母为 $0$，则将该比率定义为 $0$。\n4. 对于下方的每个测试用例，计算以下四个量：解析 KL 值、使用 $S$ 个样本的 KL 蒙特卡洛估计值、绝对估计误差 $|\\text{MC} - \\text{解析值}|$，以及使用相同的 $S$ 个样本的关于 $\\mu$ 的梯度估计器的信噪比 $\\mathrm{SNR}$。\n\n您必须实现一个完整的、可运行的程序，完成以下操作：\n- 使用固定的随机种子 $42$ 以确保可复现性。\n- 为每个测试用例计算上述指定的四个量。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序排列结果。对于每个测试用例 $i$，按顺序附加四个值 $K_i$、$M_i$、$E_i$、$R_i$。因此，对于 $N$ 个测试用例，最终输出将有 $4 \\times N$ 个条目。\n\n测试套件：\n- 用例 $1$ (正常情况，小样本): $k = 2$, $\\mu = [0.5, -0.5]$, $\\log \\sigma^2 = [-0.2, 0.3]$, $S = 1$。\n- 用例 $2$ (正常情况，中等样本): $k = 2$, $\\mu = [0.5, -0.5]$, $\\log \\sigma^2 = [-0.2, 0.3]$, $S = 10$。\n- 用例 $3$ (正常情况，大样本): $k = 2$, $\\mu = [0.5, -0.5]$, $\\log \\sigma^2 = [-0.2, 0.3]$, $S = 100$。\n- 用例 $4$ (边界情况，零均值和单位方差): $k = 2$, $\\mu = [0.0, 0.0]$, $\\log \\sigma^2 = [0.0, 0.0]$, $S = 100$。\n- 用例 $5$ (边缘情况，近确定性编码器维度): $k = 1$, $\\mu = [0.1]$, $\\log \\sigma^2 = [\\log(10^{-4})]$, $S = 50$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\text{result1},\\text{result2},\\text{result3}]$）。\n- 条目必须是按确切顺序排列的数值：对于用例 $1$，为 $K_1, M_1, E_1, R_1$；对于用例 $2$，为 $K_2, M_2, E_2, R_2$；依此类推，直到用例 $5$。", "solution": "问题陈述已经过仔细审查，并被确定为有效。它在科学上基于成熟的变分自编码器理论，问题提法清晰，提供了所有必要信息，并以客观、正式的语言表述。因此，我们可以着手提供完整的解决方案。\n\n该问题要求我们对变分自编码器 (VAE) 的解析Kullback-Leibler (KL) 散度和蒙特卡洛 (MC) 估计的KL散度进行比较分析。编码器的近似后验是一个具有对角协方差矩阵的多元正态分布，$q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2))$，先验是一个标准多元正态分布，$p(z) = \\mathcal{N}(z \\mid 0, I_k)$。我们还被要求分析KL散度相对于均值参数 $\\mu$ 的梯度的信噪比 (SNR)。\n\n**1. KL散度的解析表达式**\n\n两个分布 $q(z)$ 和 $p(z)$ 之间的Kullback-Leibler散度定义为 $\\mathrm{KL}(q \\,\\|\\, p) = \\mathbb{E}_{z \\sim q(z)}[\\log q(z) - \\log p(z)]$。由于 $q_{\\phi}(z \\mid x)$ 和 $p(z)$ 都是可以在各维度上分解的多元正态分布，总KL散度是每个维度的KL散度之和：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)) = \\sum_{i=1}^k \\mathrm{KL}(\\mathcal{N}(z_i \\mid \\mu_i, \\sigma_i^2) \\,\\|\\, \\mathcal{N}(z_i \\mid 0, 1))\n$$\n对于单个维度 $i$，两个单变量正态分布 $\\mathcal{N}(\\mu_a, \\sigma_a^2)$ 和 $\\mathcal{N}(\\mu_b, \\sigma_b^2)$ 之间的KL散度由以下公式给出：\n$$\n\\mathrm{KL}(\\mathcal{N}_a \\,\\|\\, \\mathcal{N}_b) = \\log\\frac{\\sigma_b}{\\sigma_a} + \\frac{\\sigma_a^2 + (\\mu_a-\\mu_b)^2}{2\\sigma_b^2} - \\frac{1}{2}\n$$\n在我们的情况下，对于每个维度 $i$，我们有 $\\mu_a = \\mu_i$, $\\sigma_a^2 = \\sigma_i^2$, $\\mu_b = 0$, 和 $\\sigma_b^2 = 1$。将这些代入公式可得：\n$$\n\\mathrm{KL}(\\mathcal{N}(z_i \\mid \\mu_i, \\sigma_i^2) \\,\\|\\, \\mathcal{N}(z_i \\mid 0, 1)) = \\log\\frac{1}{\\sigma_i} + \\frac{\\sigma_i^2 + (\\mu_i-0)^2}{2 \\cdot 1} - \\frac{1}{2}\n$$\n$$\n= -\\log\\sigma_i + \\frac{\\mu_i^2 + \\sigma_i^2}{2} - \\frac{1}{2}\n$$\n由于 $\\log\\sigma_i = \\frac{1}{2}\\log\\sigma_i^2$，我们可以将其重写为：\n$$\n= \\frac{1}{2}(\\mu_i^2 + \\sigma_i^2 - \\log\\sigma_i^2 - 1)\n$$\n对所有 $k$ 个维度求和，我们得到KL散度的最终解析表达式：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)) = \\frac{1}{2} \\sum_{i=1}^k \\left( \\mu_i^2 + \\sigma_i^2 - \\log(\\sigma_i^2) - 1 \\right)\n$$\n该表达式允许在给定参数 $\\mu$ 和 $\\sigma^2$ 的情况下精确计算KL散度。\n\n**2. KL散度的蒙特卡洛估计器**\n\nKL散度可以使用蒙特卡洛积分进行估计。根据大数定律，期望 $\\mathbb{E}_{z \\sim q}[f(z)]$ 可以通过对从 $q(z)$ 中抽取的 $S$ 个样本的 $f(z)$ 值求平均来近似。在这里，函数是 $f(z) = \\log q_{\\phi}(z \\mid x) - \\log p(z)$。\n\n为了从 $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu, \\operatorname{diag}(\\sigma^2))$ 中抽取样本 $z^{(s)}$，我们使用重参数化技巧。我们首先从标准正态分布中抽取 $S$ 个样本 $\\epsilon^{(s)}$，即 $\\epsilon^{(s)} \\sim \\mathcal{N}(0, I_k)$，然后如下变换它们：\n$$\nz^{(s)} = \\mu + \\sigma \\odot \\epsilon^{(s)}\n$$\n其中 $\\sigma = (\\sigma_1, \\dots, \\sigma_k)$ 是标准差向量，$\\odot$ 表示逐元素乘积。\n\nKL散度的蒙特卡洛估计器 $\\hat{K}_S$ 于是为：\n$$\n\\hat{K}_S = \\frac{1}{S} \\sum_{s=1}^S \\left[ \\log q_{\\phi}(z^{(s)} \\mid x) - \\log p(z^{(s)}) \\right]\n$$\n所需的对数密度为：\n$$\n\\log q_{\\phi}(z^{(s)} \\mid x) = -\\frac{1}{2} \\left( \\sum_{i=1}^k \\frac{(z_i^{(s)} - \\mu_i)^2}{\\sigma_i^2} + \\sum_{i=1}^k \\log(2\\pi \\sigma_i^2) \\right)\n$$\n$$\n\\log p(z^{(s)}) = -\\frac{1}{2} \\left( \\sum_{i=1}^k (z_i^{(s)})^2 + k \\log(2\\pi) \\right)\n$$\n绝对估计误差计算为 $|\\hat{K}_S - \\mathrm{KL}_{\\text{analytic}}|$。\n\n**3. 梯度估计器和信噪比**\n\n我们的任务是构建KL项相对于 $\\mu$ 的梯度估计器，即 $\\nabla_{\\mu} \\mathrm{KL}(q_{\\phi} \\,\\|\\, p)$。重参数化技巧在这里至关重要，因为它允许将梯度移到期望内部。\n$$\n\\nabla_{\\mu} \\mathrm{KL} = \\nabla_{\\mu} \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[\\log q(\\mu+\\sigma\\epsilon) - \\log p(\\mu+\\sigma\\epsilon)] = \\mathbb{E}_{\\epsilon \\sim \\mathcal{N}(0,I)}[\\nabla_{\\mu}(\\log q(z) - \\log p(z))]\n$$\n其中 $z = \\mu + \\sigma\\epsilon$。根据链式法则，$\\nabla_{\\mu}f(z) = (\\nabla_z f(z))^T \\nabla_{\\mu}z$。由于 $z_i = \\mu_i + \\sigma_i \\epsilon_i$，雅可比矩阵 $\\nabla_{\\mu}z$ 是单位矩阵 $I_k$。因此，$\\nabla_{\\mu}f(z) = \\nabla_z f(z)$。梯度为：\n$$\n\\nabla_{\\mu} \\mathrm{KL} = \\mathbb{E}_{z \\sim q}[\\nabla_z(\\log q(z) - \\log p(z))]\n$$\n问题将单样本梯度估计定义为 $g^{(s)} = \\nabla_{z}(\\log q(z^{(s)}) - \\log p(z^{(s)}))$。让我们推导其分量 $g_i(z) = \\frac{\\partial}{\\partial z_i}(\\log q(z) - \\log p(z))$。\n$$\n\\frac{\\partial}{\\partial z_i} \\log q(z) = \\frac{\\partial}{\\partial z_i} \\left[ -\\frac{1}{2} \\sum_{j=1}^k \\left( \\frac{(z_j - \\mu_j)^2}{\\sigma_j^2} + \\log(2\\pi\\sigma_j^2) \\right) \\right] = -\\frac{z_i - \\mu_i}{\\sigma_i^2}\n$$\n$$\n\\frac{\\partial}{\\partial z_i} \\log p(z) = \\frac{\\partial}{\\partial z_i} \\left[ -\\frac{1}{2} \\sum_{j=1}^k (z_j^2 + \\log(2\\pi)) \\right] = -z_i\n$$\n组合这些项，得到单样本梯度分量的表达式：\n$$\ng_i(z) = -\\frac{z_i - \\mu_i}{\\sigma_i^2} - (-z_i) = z_i - \\frac{z_i - \\mu_i}{\\sigma_i^2}\n$$\n使用重参数化 $z_i-\\mu_i = \\sigma_i \\epsilon_i$，我们可以将其简化为 $g_i(z) = z_i - \\epsilon_i / \\sigma_i$。\n\n梯度的蒙特卡洛估计是这些单样本梯度的经验均值，$\\bar{g} = \\frac{1}{S} \\sum_{s=1}^S g(z^{(s)})$。经验每坐标标准差向量 $\\operatorname{std}(g)$ 是在 $S$ 个样本上为梯度的每个维度计算的。然后，信噪比定义为：\n$$\n\\mathrm{SNR} = \\frac{\\|\\bar{g}\\|_2}{\\|\\operatorname{std}(g)\\|_2}\n$$\n如果分母 $\\|\\operatorname{std}(g)\\|_2$ 为零（当 $S=1$ 或梯度样本方差为零时发生），则SNR定义为 $0$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analytical KL divergence, a Monte Carlo estimate, the absolute error,\n    and a gradient signal-to-noise ratio for a VAE setting for several test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (k, mu, log_sigma_sq, S)\n        (2, [0.5, -0.5], [-0.2, 0.3], 1),\n        (2, [0.5, -0.5], [-0.2, 0.3], 10),\n        (2, [0.5, -0.5], [-0.2, 0.3], 100),\n        (2, [0.0, 0.0], [0.0, 0.0], 100),\n        (1, [0.1], [np.log(1e-4)], 50),\n    ]\n\n    # Use a fixed random seed for reproducibility.\n    rng = np.random.default_rng(42)\n    \n    results = []\n\n    for k, mu_list, log_sigma_sq_list, S in test_cases:\n        mu = np.array(mu_list, dtype=np.float64)\n        log_sigma_sq = np.array(log_sigma_sq_list, dtype=np.float64)\n        \n        sigma_sq = np.exp(log_sigma_sq)\n        sigma = np.sqrt(sigma_sq)\n        \n        # 1. Analytical KL divergence (K_i)\n        kl_analytic = 0.5 * np.sum(mu**2 + sigma_sq - log_sigma_sq - 1)\n        \n        # Draw S samples for epsilon from a standard normal distribution\n        epsilons = rng.normal(size=(S, k))\n        \n        # Apply reparameterization trick to get samples for z\n        z_samples = mu + sigma * epsilons\n        \n        # 2. Monte Carlo estimate of KL divergence (M_i)\n        # log q(z|x) term for each sample\n        log_q_z = -0.5 * np.sum(epsilons**2 + log_sigma_sq + np.log(2 * np.pi), axis=1)\n        \n        # log p(z) term for each sample\n        log_p_z = -0.5 * np.sum(z_samples**2 + np.log(2 * np.pi), axis=1)\n        \n        # KL term for each sample and the final MC estimate\n        kl_terms = log_q_z - log_p_z\n        kl_mc = np.mean(kl_terms)\n        \n        # 3. Absolute estimation error |MC - Analytic| (E_i)\n        abs_error = np.abs(kl_mc - kl_analytic)\n        \n        # 4. Signal-to-noise ratio of the gradient estimator (R_i)\n        # Per-sample gradient estimates g(z^(s))\n        g_samples = z_samples - epsilons / sigma\n        \n        # Empirical mean of the gradient\n        g_bar = np.mean(g_samples, axis=0)\n        \n        # Empirical per-coordinate standard deviation of the gradient (using ddof=0 for population std of the sample)\n        # For S=1, this will correctly result in a vector of zeros.\n        g_std = np.std(g_samples, axis=0, ddof=0)\n        \n        # L2-norms of the mean and std vectors\n        norm_g_bar = np.linalg.norm(g_bar)\n        norm_g_std = np.linalg.norm(g_std)\n        \n        # SNR calculation, handling the case of zero standard deviation\n        snr = norm_g_bar / norm_g_std if norm_g_std > 0 else 0.0\n        \n        results.extend([kl_analytic, kl_mc, abs_error, snr])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197900"}, {"introduction": "VAE的精髓在于其目标函数——证据下界（ELBO）——在重构保真度与潜空间正则化之间取得的精妙平衡。为了深入理解这一平衡机制，我们可以探索一个极限情景：当编码器变得异常“自信”，即其输出的方差趋近于零时，会发生什么？本练习 [@problem_id:3197922] 通过一个简化的分析模型，让你亲手计算并观察ELBO在这种边界条件下的行为，揭示KL散度项如何作为关键的正则化器，防止VAE退化为普通的自编码器。", "problem": "考虑一个具有以下组件的一维变分自编码器（VAE）。潜变量为 $z \\in \\mathbb{R}$，观测值为 $x \\in \\mathbb{R}$，且所有分布均为单变量高斯分布。先验分布为 $p(z) = \\mathcal{N}(0,1)$。编码器（近似后验）为 $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu_{\\phi}(x), \\sigma_{\\phi}(x)^2)$，其中编码器均值为 $\\mu_{\\phi}(x) \\in \\mathbb{R}$，编码器标准差为 $\\sigma_{\\phi}(x) \\in \\mathbb{R}_{>0}$。解码器（似然）为 $p_{\\theta}(x \\mid z) = \\mathcal{N}(w z, \\sigma_x^2)$，其中标量权重 $w \\in \\mathbb{R}$ 和观测噪声标准差 $\\sigma_x \\in \\mathbb{R}_{>0}$ 为固定值。将带有库尔贝克-莱布勒（KL）权重 $\\beta \\in \\mathbb{R}_{\\ge 0}$ 的证据下界（ELBO）定义为\n$$\n\\mathcal{L}_{\\beta}(x) \\;=\\; \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right] \\;-\\; \\beta \\, \\mathrm{KL}\\!\\left( q_{\\phi}(z \\mid x) \\parallel p(z) \\right).\n$$\n您将分析当编码器噪声趋于零时，即 $\\sigma_{\\phi}(x) \\to 0$ 时，$\\mathcal{L}_{\\beta}(x)$ 的行为。请仅使用高斯对数似然、期望和库尔贝克-莱布勒散度的基本定义作为起点，并从这些定义中推导出您需要的任何闭式解。您不得依赖任何未说明的捷径。\n\n任务。对于下方的每个给定测试用例，假设编码器标准差 $\\sigma_{\\phi}(x) = s$ 为固定值，并在保持 $s$、$w$ 和 $\\sigma_x$ 不变的情况下，对编码器均值 $\\mu_{\\phi}(x)$ 优化 ELBO。即，对于每组参数 $(x, w, \\sigma_x, s, \\beta)$：\n- 在给定 $s$ 为固定值的情况下，计算使 $\\mathcal{L}_{\\beta}(x)$ 相对于 $\\mu_{\\phi}(x)$ 最大化的编码器均值 $\\mu^{\\star}$。\n- 使用此 $\\mu^{\\star}$ 和给定参数，利用高斯对数似然的定义和单变量高斯分布之间库尔贝克-莱布勒散度的定义，精确计算 $\\mathcal{L}_{\\beta}(x)$ 的相应值。\n- 计算必须是解析的（即，不使用蒙特卡洛采样）。\n- 对于每个测试用例，您的程序应输出浮点数对 $[\\mathcal{L}_{\\beta}(x), \\mu^{\\star}]$。\n\n解释目标。通过比较 $s$ 非常小且 $\\beta$ 变化的测试用例结果，您的输出应揭示当 $s \\to 0$ 时 $\\mathcal{L}_{\\beta}(x)$ 的行为，以及当 $\\beta$ 很小或为零时，小的编码器噪声如何导致过拟合和 $z$ 中缺乏正则化。\n\n测试套件。请精确使用以下五个测试用例，每个用例指定为 $(x, w, \\sigma_x, s, \\beta)$：\n- 用例 $1$：$(x, w, \\sigma_x, s, \\beta) = (\\,1.0,\\, 1.0,\\, 0.5,\\, 10^{-6},\\, 1.0\\,)$。\n- 用例 $2$：$(x, w, \\sigma_x, s, \\beta) = (\\,1.0,\\, 1.0,\\, 0.5,\\, 10^{-12},\\, 1.0\\,)$。\n- 用例 $3$：$(x, w, \\sigma_x, s, \\beta) = (\\,1.0,\\, 1.0,\\, 0.5,\\, 10^{-6},\\, 0.1\\,)$。\n- 用例 $4$：$(x, w, \\sigma_x, s, \\beta) = (\\,3.0,\\, 0.1,\\, 0.5,\\, 10^{-6},\\, 0.0\\,)$。\n- 用例 $5$：$(x, w, \\sigma_x, s, \\beta) = (\\,2.0,\\, 0.0,\\, 0.5,\\, 10^{-6},\\, 1.0\\,)$。\n\n覆盖理据：\n- 用例 1 是一个具有适中参数的通用基线。\n- 用例 2 将 $s$ 进一步减小至零，以揭示在正则化下当 $\\sigma_{\\phi}(x) \\to 0$ 时 $\\mathcal{L}_{\\beta}(x)$ 的行为。\n- 用例 3 减小 $\\beta$ 以显示较弱的正则化效果。\n- 用例 4 使用 $\\beta = 0$ 移除正则化，并使用一个较小的 $w$ 来突显 $\\mu^{\\star}$ 的量级可以变得很大，而目标函数并不会惩罚 $z$。\n- 用例 5 设置 $w = 0$ 以使解码器忽略 $z$，测试潜变量未使用且 KL 项主导 $\\sigma_{\\phi}(x) \\to 0$ 行为的边界情况。\n\n最终输出格式。您的程序应生成单行结果，包含五个测试用例的结果，按顺序排列，格式为逗号分隔的数对列表，无任何附加文本，例如：\n$[[r_{1,1},r_{1,2}],[r_{2,1},r_{2,2}],[r_{3,1},r_{3,2}],[r_{4,1},r_{4,2}],[r_{5,1},r_{5,2}]]$\n其中 $r_{i,1}$ 是用例 $i$ 的 $\\mathcal{L}_{\\beta}(x)$，$r_{i,2}$ 是用例 $i$ 的 $\\mu^{\\star}$。所有 $r_{i,j}$ 必须是浮点数。", "solution": "该问题要求分析一个一维变分自编码器（VAE）。我们已知模型的组件和目标函数，即证据下界（ELBO），我们必须针对编码器的均值参数最大化此函数。\n\n我们首先陈述给定的量。\n观测值为 $x \\in \\mathbb{R}$，潜变量为 $z \\in \\mathbb{R}$。\n潜变量的先验分布是一个标准正态分布：\n$$p(z) = \\mathcal{N}(z \\mid 0, 1)$$\n编码器，或称近似后验，是一个高斯分布，其参数是输入 $x$ 的函数：\n$$q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{\\phi}(x), \\sigma_{\\phi}(x)^2)$$\n对于本问题，编码器的标准差固定为一个常数 $s \\in \\mathbb{R}_{>0}$，因此 $\\sigma_{\\phi}(x) = s$。为简化起见，我们将均值表示为 $\\mu \\equiv \\mu_{\\phi}(x)$，因为它是我们将要优化的变量。\n$$q(z \\mid x) = \\mathcal{N}(z \\mid \\mu, s^2)$$\n解码器，或称似然，也是一个高斯分布：\n$$p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid wz, \\sigma_x^2)$$\n在这里，权重 $w \\in \\mathbb{R}$ 和观测噪声标准差 $\\sigma_x \\in \\mathbb{R}_{>0}$ 是固定参数。\n\n$\\beta$-VAE 目标函数，或称 ELBO，定义为：\n$$\n\\mathcal{L}_{\\beta}(x) \\;=\\; \\mathbb{E}_{q(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right] \\;-\\; \\beta \\, \\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right)\n$$\n其中 $\\beta \\in \\mathbb{R}_{\\ge 0}$ 是库尔贝克-莱布勒（KL）散度项的权重。我们的任务是对于给定的参数集 $(x, w, \\sigma_x, s, \\beta)$，找到最大化此目标的最优均值 $\\mu^{\\star}$，然后计算在此最优点上 $\\mathcal{L}_{\\beta}(x)$ 的值。\n\n我们将为 ELBO 中的两项分别推导闭式表达式。\n\n**1. 重构项：$\\mathbb{E}_{q(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right]$**\n\n解码器的对数似然为：\n$$\n\\log p_{\\theta}(x \\mid z) = \\log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma_x^2}} \\exp\\left(-\\frac{(x - wz)^2}{2\\sigma_x^2}\\right) \\right) = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{(x - wz)^2}{2\\sigma_x^2}\n$$\n我们需要计算该量相对于编码器分布 $q(z \\mid x) = \\mathcal{N}(z \\mid \\mu, s^2)$ 的期望。令 $\\mathbb{E}_{q}[\\cdot]$ 表示此期望。\n$$\n\\mathbb{E}_{q}\\left[ \\log p_{\\theta}(x \\mid z) \\right] = \\mathbb{E}_{q}\\left[ -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{(x - wz)^2}{2\\sigma_x^2} \\right]\n$$\n利用期望的线性性质：\n$$\n\\mathbb{E}_{q}\\left[ \\log p_{\\theta}(x \\mid z) \\right] = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2}\\mathbb{E}_{q}\\left[ (x - wz)^2 \\right]\n$$\n我们展开期望内的平方项：\n$$\n\\mathbb{E}_{q}\\left[ (x - wz)^2 \\right] = \\mathbb{E}_{q}\\left[ x^2 - 2xwz + w^2z^2 \\right] = x^2 - 2xw\\mathbb{E}_{q}[z] + w^2\\mathbb{E}_{q}[z^2]\n$$\n对于一个随机变量 $z \\sim \\mathcal{N}(\\mu, s^2)$，我们有 $\\mathbb{E}_{q}[z] = \\mu$ 和 $\\mathrm{Var}_{q}(z) = s^2$。其二阶矩为 $\\mathbb{E}_{q}[z^2] = \\mathrm{Var}_{q}(z) + (\\mathbb{E}_{q}[z])^2 = s^2 + \\mu^2$。\n将其代回：\n$$\n\\mathbb{E}_{q}\\left[ (x - wz)^2 \\right] = x^2 - 2xw\\mu + w^2(s^2 + \\mu^2)\n$$\n因此，完整的重构项为：\n$$\n\\mathbb{E}_{q(z \\mid x)}\\left[ \\log p_{\\theta}(x \\mid z) \\right] = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\left[ x^2 - 2xw\\mu + w^2(s^2 + \\mu^2) \\right]\n$$\n\n**2. KL 散度项：$\\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right)$**\n\n我们需要计算两个单变量高斯分布 $q(z \\mid x) = \\mathcal{N}(\\mu, s^2)$ 和 $p(z) = \\mathcal{N}(0, 1)$ 之间的 KL 散度。其通用定义为 $\\mathrm{KL}(q \\parallel p) = \\int q(z) \\log \\frac{q(z)}{p(z)} dz$。闭式解为：\n$$\n\\mathrm{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\parallel \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\n代入我们的参数（$\\mu_1 = \\mu, \\sigma_1 = s, \\mu_2 = 0, \\sigma_2 = 1$）：\n$$\n\\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right) = \\log\\frac{1}{s} + \\frac{s^2 + (\\mu-0)^2}{2 \\cdot 1^2} - \\frac{1}{2} = -\\log s + \\frac{s^2 + \\mu^2}{2} - \\frac{1}{2}\n$$\n这可以使用 $\\log s = \\frac{1}{2}\\log s^2$ 重写为：\n$$\n\\mathrm{KL}\\!\\left( q(z \\mid x) \\parallel p(z) \\right) = \\frac{1}{2} \\left( \\mu^2 + s^2 - \\log s^2 - 1 \\right)\n$$\n\n**3. 最大化 ELBO**\n\n现在我们将完整的 ELBO 表达式组装为 $\\mu$ 的函数：\n$$\n\\mathcal{L}_{\\beta}(\\mu) = \\left( -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2} [x^2 - 2xw\\mu + w^2(s^2 + \\mu^2)] \\right) - \\frac{\\beta}{2} \\left( \\mu^2 + s^2 - \\log s^2 - 1 \\right)\n$$\n为了找到最大化 $\\mathcal{L}_{\\beta}(\\mu)$ 的最优均值 $\\mu^{\\star}$，我们对 $\\mu$ 求导并令其为零。\n$$\n\\frac{\\partial\\mathcal{L}_{\\beta}}{\\partial\\mu} = \\frac{\\partial}{\\partial\\mu} \\left( -\\frac{1}{2\\sigma_x^2} [-2xw\\mu + w^2\\mu^2] - \\frac{\\beta}{2}\\mu^2 \\right)\n$$\n$$\n\\frac{\\partial\\mathcal{L}_{\\beta}}{\\partial\\mu} = -\\frac{1}{2\\sigma_x^2}(-2xw + 2w^2\\mu) - \\beta\\mu = \\frac{xw}{\\sigma_x^2} - \\frac{w^2}{\\sigma_x^2}\\mu - \\beta\\mu\n$$\n将导数设为零以求得最优的 $\\mu^{\\star}$：\n$$\n\\frac{xw}{\\sigma_x^2} - \\left(\\frac{w^2}{\\sigma_x^2} + \\beta\\right)\\mu^{\\star} = 0\n$$\n求解 $\\mu^{\\star}$：\n$$\n\\mu^{\\star} = \\frac{xw/\\sigma_x^2}{w^2/\\sigma_x^2 + \\beta} = \\frac{xw}{w^2 + \\beta\\sigma_x^2}\n$$\n二阶导数 $\\frac{\\partial^2\\mathcal{L}_{\\beta}}{\\partial\\mu^2} = -\\frac{w^2}{\\sigma_x^2} - \\beta$ 是非正的，并且对于给定的测试用例是严格为负的（因为 $w$ 和 $\\beta$ 不会同时为零），这证实了 $\\mu^{\\star}$ 对应于一个最大值。\n\n**4. 最终计算**\n\n对于每个测试用例 $(x, w, \\sigma_x, s, \\beta)$，我们执行以下两个步骤：\n1.  计算最优编码器均值：$\\mu^{\\star} = \\frac{xw}{w^2 + \\beta\\sigma_x^2}$。\n2.  将 $\\mu = \\mu^{\\star}$ 代入完整的 ELBO 表达式以获得最大值：\n    $$\n    \\mathcal{L}_{\\beta}(x) = -\\frac{1}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2\\sigma_x^2} \\left[ x^2 - 2xw\\mu^{\\star} + w^2(s^2 + (\\mu^{\\star})^2) \\right] - \\frac{\\beta}{2} \\left( (\\mu^{\\star})^2 + s^2 - 2\\log s - 1 \\right)\n    $$\n这些计算将在提供的 Python 脚本中实现。\n当 $s \\to 0$ 时的行为值得一提。对于 $\\beta > 0$，项 $-\\frac{\\beta}{2}(-2\\log s)$ 占主导地位，导致 $\\mathcal{L}_{\\beta}(x) \\to -\\infty$。这惩罚了编码器变得过于自信（即方差接近于零）的行为。对于 $\\beta=0$，此惩罚消失，$\\mathcal{L}_{\\beta}(x)$ 收敛到一个有限值。在这种情况下，$\\mu^{\\star}=x/w$（如果 $w \\ne 0$），这对应于确定性地反转解码器的均值函数，是潜空间中过拟合的一种形式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal encoder mean and the corresponding ELBO for a 1D VAE\n    for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x, w, sigma_x, s, beta)\n    test_cases = [\n        (1.0, 1.0, 0.5, 1e-6, 1.0),\n        (1.0, 1.0, 0.5, 1e-12, 1.0),\n        (1.0, 1.0, 0.5, 1e-6, 0.1),\n        (3.0, 0.1, 0.5, 1e-6, 0.0),\n        (2.0, 0.0, 0.5, 1e-6, 1.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        x, w, sigma_x, s, beta = case\n        \n        # Calculate the optimal encoder mean mu_star\n        # mu_star = (x * w) / (w^2 + beta * sigma_x^2)\n        sigma_x_sq = sigma_x**2\n        w_sq = w**2\n        mu_star_numerator = x * w\n        mu_star_denominator = w_sq + beta * sigma_x_sq\n        \n        # Handle the case where the denominator could be zero, although not in the test cases\n        if mu_star_denominator == 0:\n            # If w=0 and beta=0, the ELBO is independent of mu. Any mu is optimal.\n            # We can choose mu_star = 0 for convention.\n            mu_star = 0.0\n        else:\n            mu_star = mu_star_numerator / mu_star_denominator\n            \n        # Calculate the two components of the ELBO\n        \n        # 1. Reconstruction Term: E_q[log p(x|z)]\n        # = -0.5*log(2*pi*sigma_x^2) - (1/(2*sigma_x^2)) * [x^2 - 2*x*w*mu + w^2*(s^2 + mu^2)]\n        s_sq = s**2\n        mu_star_sq = mu_star**2\n        \n        # The term inside the expectation's main part\n        recon_exp_term = x**2 - 2 * x * w * mu_star + w_sq * (s_sq + mu_star_sq)\n        \n        # The full reconstruction term\n        recon_term = -0.5 * np.log(2 * np.pi * sigma_x_sq) - (1 / (2 * sigma_x_sq)) * recon_exp_term\n        \n        # 2. KL Divergence Term: KL(q(z|x) || p(z))\n        # = 0.5 * (mu^2 + s^2 - log(s^2) - 1)\n        # log(s^2) = 2 * log(s)\n        kl_divergence = 0.5 * (mu_star_sq + s_sq - 2 * np.log(s) - 1)\n        \n        # Combine to get the beta-ELBO\n        elbo = recon_term - beta * kl_divergence\n        \n        results.append([elbo, mu_star])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3197922"}, {"introduction": "在设计更强大的VAE模型时，一个常见且棘手的问题是“后验坍塌”（posterior collapse）。当解码器能力过强时（例如，使用自回归结构或跳跃连接），它可能学会“抄近道”，直接利用输入信息进行重构，从而完全忽略潜变量 $z$ 提供的信息。本练习 [@problem_id:3197907] 将这个抽象问题具体化为一个可解析的线性高斯模型，让你能够通过计算互信息来量化潜变量的使用程度，并动手实践几种主流的缓解后验坍塌的策略，如调整KL权重和应用“自由比特”（free-bits）技巧。", "problem": "构建一个小型、完全指定的线性高斯研究，以检验从输入 $x$ 到解码器的跳跃连接如何在变分自编码器 (VAE) 中绕过潜变量 $z$，从而可能导致后验坍塌，并评估缓解策略。在标量设定下进行，遵循以下文献中的标准假设作为基础：\n\n- 变分自编码器 (VAE) 的证据下界 (ELBO) 定义为 $\\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\mathrm{KL}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z))$。\n- 对于单变量高斯分布 $q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^{2})$ 和 $p(z) = \\mathcal{N}(0, 1)$，其 Kullback–Leibler 散度为 $\\frac{1}{2}\\left(\\sigma^{2} + \\mu^{2} - 1 - \\log \\sigma^{2}\\right)$。\n- 对于方差为 $\\sigma_{\\epsilon}^{2}$ 的高斯似然，负对数似然对应于一个缩放的平方误差，且期望在线性二次型上是可分配的。\n\n模型和目标：\n\n- 先验：$p(z) = \\mathcal{N}(0, 1)$。\n- 数据：$x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，其中 $\\sigma_{x}^{2} = 1$。\n- 编码器：$q_{\\phi}(z \\mid x) = \\mathcal{N}(a x, v)$，其中标量参数 $a \\in \\mathbb{R}$ 和 $v \\in \\mathbb{R}_{>0}$ 需要被优化。\n- 带跳跃连接的解码器：$p_{\\theta}(x \\mid z, x) = \\mathcal{N}(W z + s x, \\sigma_{\\epsilon}^{2})$，其中跳跃系数 $s \\in \\mathbb{R}$ 和解码器增益 $W \\in \\mathbb{R}$ 在本研究中被视为固定的超参数。\n- 训练目标 (beta-ELBO 的负值)：对 $a$ 和 $v$ 进行最小化，目标为期望损失\n$$\n\\mathcal{L}_{\\beta}(a, v) \\;=\\; \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right] \\;+\\; \\beta \\, \\mathbb{E}_{x}\\!\\left[ \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\\right],\n$$\n其中 $\\beta \\in \\mathbb{R}_{>0}$ 是 Kullback–Leibler 散度的权重 (beta-VAE)。\n\n任务：\n\n- 仅从上述定义出发，推导闭式最优编码器参数 $a^{\\star}$ 和 $v^{\\star}$，使得在固定的 $W$、$s$、$\\sigma_{\\epsilon}^{2}$ 和 $\\beta$ 下，$\\mathcal{L}_{\\beta}(a, v)$ 最小。\n- 使用最优的 $a^{\\star}$ 和 $v^{\\star}$，计算编码器下 $x$ 和 $z$ 之间的互信息，\n$$\nI(x; z) \\;=\\; \\frac{1}{2}\\log\\!\\left(1 + \\frac{a^{2}\\,\\sigma_{x}^{2}}{v}\\right),\n$$\n使用自然对数，以量化潜变量的使用情况。使用 $\\sigma_{x}^{2} = 1$。\n- 解析地证明跳跃参数 $s$ 如何影响 $a^{\\star}$，并解释为何当 $s$ 接近 $1$ 时会导致 $a^{\\star}$ 接近 $0$，这表明由于绕过 $z$ 可能导致后验坍塌。\n- 以编程方式实现并评估缓解策略：\n  - 减小 Kullback–Leibler 权重 $\\beta$。\n  - 减小跳跃强度 $s$。\n  - 应用 free-bits 启发式方法：将 Kullback–Leibler 项替换为 $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$，其中阈值 $\\tau \\in \\mathbb{R}_{\\ge 0}$。按如下方式解释解：如果无约束的 beta-VAE 最优解的 $\\mathrm{KL} \\ge \\tau$，则保持该解；否则，在 $\\mathrm{KL} = \\tau$ 的约束下最小化重构项，并计算得到的 $a$ 和 $v$。全程使用自然对数。\n\n你的程序必须：\n\n- 针对以下每个测试用例，使用 beta-VAE 最优解或完全按照指定的 free-bits 规则计算 $I(x; z)$。在所有情况下，使用 $\\sigma_{x}^{2} = 1$ 和自然对数。元组顺序为 $(s, W, \\sigma_{\\epsilon}^{2}, \\beta, \\tau, \\text{regime})$，其中 $\\text{regime}$ 是字面字符串 \"beta\" 或 \"freebits\"。\n  - 情况 A: $(s = 0,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 B: $(s = 1,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 C: $(s = 0.9,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 0.1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 D: $(s = 0.5,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 E: $(s = 0.5,\\, W = 0,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 0,\\, \\text{regime}=\\text{\"beta\"})$。\n  - 情况 F: $(s = 0.9,\\, W = 1,\\, \\sigma_{\\epsilon}^{2} = 0.1,\\, \\beta = 1,\\, \\tau = 1,\\, \\text{regime}=\\text{\"freebits\"})$。\n- 对于 \"beta\" 模式，使用你为 $\\mathcal{L}_{\\beta}(a, v)$ 推导的闭式最优解来计算 $a^{\\star}$、$v^{\\star}$，然后计算 $I(x; z)$。\n- 对于 \"freebits\" 模式，如果 beta-VAE 的闭式最优解得出 $\\mathrm{KL} \\ge \\tau$，则直接使用该解；否则，在 $\\mathrm{KL} = \\tau$ 的约束下最小化重构项，并计算 $I(x; z)$。如果 $W = 0$，请仔细处理退化情况并返回 $I(x; z) = 0$。\n\n最终输出要求：\n\n- 你的程序应生成单行输出，其中包含情况 A 到 F 的六个互信息值，形式为方括号内以逗号分隔的列表，每个值四舍五入到六位小数，并以奈特 (nats) 为单位表示。例如，输出行的格式为 $[\\text{r}_{A},\\text{r}_{B},\\text{r}_{C},\\text{r}_{D},\\text{r}_{E},\\text{r}_{F}]$。", "solution": "用户提供了一个在变分自编码器 (VAE) 背景下定义明确的理论问题。我将首先验证问题陈述，然后进行完整的推导和求解。\n\n### 步骤 1：提取给定条件\n- **先验分布**: $p(z) = \\mathcal{N}(0, 1)$。\n- **数据分布**: $x \\sim \\mathcal{N}(0, \\sigma_{x}^{2})$，其中 $\\sigma_{x}^{2} = 1$。\n- **编码器 (近似后验)**: $q_{\\phi}(z \\mid x) = \\mathcal{N}(a x, v)$，待优化参数为 $a \\in \\mathbb{R}$，$v \\in \\mathbb{R}_{>0}$。\n- **解码器 (似然)**: $p_{\\theta}(x \\mid z, x) = \\mathcal{N}(W z + s x, \\sigma_{\\epsilon}^{2})$，固定超参数为 $W \\in \\mathbb{R}$，$s \\in \\mathbb{R}$。\n- **目标函数 (最小化)**:\n$$\n\\mathcal{L}_{\\beta}(a, v) \\;=\\; \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right] \\;+\\; \\beta \\, \\mathbb{E}_{x}\\!\\left[ \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\\right]\n$$\n- **KL 散度公式**: 对于 $q(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^{2})$ 和 $p(z) = \\mathcal{N}(0, 1)$，$\\mathrm{KL}(q\\,\\|\\,p) = \\frac{1}{2}\\left(\\sigma^{2} + \\mu^{2} - 1 - \\log \\sigma^{2}\\right)$。\n- **互信息公式**: $I(x; z) \\;=\\; \\frac{1}{2}\\log\\!\\left(1 + \\frac{a^{2}\\,\\sigma_{x}^{2}}{v}\\right)$，使用自然对数。\n- **Free-Bits 启发式方法**: KL 项被替换为 $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$。\n\n### 步骤 2：使用提取的给定条件进行验证\n- **科学基础**: 该问题是关于 VAE 的标准理论分析，使用了如 ELBO、KL 散度和线性高斯模型等已建立的概念。这是机器学习研究中为便于解析分析而常用的简化方法。\n- **适定的**: 该问题要求最小化一个凸函数（就我们优化的变量而言），这保证了唯一最小值的存在。任务定义清晰，能够导出一组唯一的结果。\n- **客观性**: 语言精确，所有模型和目标都经过了形式化规定。没有主观性。\n- **结论**: 问题有效。它在科学上是合理的、适定的和客观的。\n\n### 步骤 3：判断与行动\n问题有效。我将进行完整的求解。\n\n### 第 1 部分：最优编码器参数 $a^{\\star}$ 和 $v^{\\star}$ 的推导\n\n目标函数 $\\mathcal{L}_{\\beta}(a, v)$ 包括两项：期望重构误差和期望 KL 散度。\n\n**第 1 项：重构误差**\n设 $R$ 为重构项。\n$$\nR = \\mathbb{E}_{x}\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[ \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left(x - (W z + s x)\\right)^{2} \\right]\n$$\n期望内的表达式可以重写为 $\\left((1-s)x - Wz\\right)^{2}$。我们首先对 $z \\sim q_{\\phi}(z \\mid x) = \\mathcal{N}(ax, v)$ 求期望。\n$$\n\\mathbb{E}_{q_z}[((1-s)x - Wz)^2] = \\mathbb{E}_{q_z}[(1-s)^2x^2 - 2W(1-s)xz + W^2z^2]\n$$\n使用 $\\mathbb{E}_{q_z}[z] = ax$ 和 $\\mathbb{E}_{q_z}[z^2] = \\text{Var}_{q_z}[z] + (\\mathbb{E}_{q_z}[z])^2 = v + (ax)^2$：\n$$\n= (1-s)^2x^2 - 2W(1-s)x(ax) + W^2(v + a^2x^2)\n$$\n$$\n= x^2[(1-s)^2 - 2aW(1-s) + a^2W^2] + W^2v\n$$\n这可以简化为平方项 $x^2(1-s-aW)^2 + W^2v$。\n现在，我们对 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ 求期望。我们使用 $\\mathbb{E}_x[x^2] = \\sigma_x^2$。\n$$\n\\mathbb{E}_{x}[x^2(1-s-aW)^2 + W^2v] = \\sigma_x^2(1-s-aW)^2 + W^2v\n$$\n所以，重构项为：\n$$\nR = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( \\sigma_x^2(1-s-aW)^2 + W^2v \\right)\n$$\n\n**第 2 项：KL 散度**\n设 $K$ 为 KL 散度项。$q_{\\phi}(z \\mid x) = \\mathcal{N}(ax, v)$ 和 $p(z)=\\mathcal{N}(0, 1)$ 之间的 KL 散度为：\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)) = \\frac{1}{2}\\left(v + (ax)^2 - 1 - \\log v\\right)\n$$\n对 $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ 求期望：\n$$\n\\mathbb{E}_x[\\mathrm{KL}] = \\mathbb{E}_x\\left[\\frac{1}{2}(v + a^2x^2 - 1 - \\log v)\\right] = \\frac{1}{2}(v + a^2\\mathbb{E}_x[x^2] - 1 - \\log v) = \\frac{1}{2}(v + a^2\\sigma_x^2 - 1 - \\log v)\n$$\n所以，完整的 KL 项为：\n$$\nK = \\frac{\\beta}{2} \\left(v + a^2\\sigma_x^2 - 1 - \\log v \\right)\n$$\n\n**完整目标及优化**\n总损失为 $\\mathcal{L}_{\\beta}(a, v) = R + K$。为了找到最优参数 $a^{\\star}$ 和 $v^{\\star}$，我们将偏导数设为零。对于本问题，$\\sigma_x^2=1$。\n$$\n\\mathcal{L}_{\\beta}(a, v) = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( (1-s-aW)^2 + W^2v \\right) + \\frac{\\beta}{2} \\left( v + a^2 - 1 - \\log v \\right)\n$$\n关于 $a$ 的偏导数：\n$$\n\\frac{\\partial \\mathcal{L}_{\\beta}}{\\partial a} = \\frac{1}{2\\sigma_{\\epsilon}^{2}} \\left( 2(1-s-aW)(-W) \\right) + \\frac{\\beta}{2} (2a) = -\\frac{W(1-s-aW)}{\\sigma_{\\epsilon}^{2}} + \\beta a = 0\n$$\n$$\n-W(1-s) + aW^2 + \\beta a \\sigma_{\\epsilon}^{2} = 0 \\implies a(W^2 + \\beta \\sigma_{\\epsilon}^{2}) = W(1-s)\n$$\n$$\na^{\\star} = \\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}\n$$\n关于 $v$ 的偏导数：\n$$\n\\frac{\\partial \\mathcal{L}_{\\beta}}{\\partial v} = \\frac{W^2}{2\\sigma_{\\epsilon}^{2}} + \\frac{\\beta}{2} \\left(1 - \\frac{1}{v}\\right) = 0\n$$\n$$\n\\frac{W^2}{\\sigma_{\\epsilon}^{2}} + \\beta\\left(1 - \\frac{1}{v}\\right) = 0 \\implies \\beta\\left(\\frac{1}{v}-1\\right) = \\frac{W^2}{\\sigma_{\\epsilon}^{2}} \\implies \\frac{1}{v} = 1 + \\frac{W^2}{\\beta \\sigma_{\\epsilon}^{2}}\n$$\n$$\n\\frac{1}{v} = \\frac{\\beta \\sigma_{\\epsilon}^{2} + W^2}{\\beta \\sigma_{\\epsilon}^{2}} \\implies v^{\\star} = \\frac{\\beta \\sigma_{\\epsilon}^{2}}{\\beta \\sigma_{\\epsilon}^{2} + W^2}\n$$\n\n### 第 2 部分：跳跃连接参数 s 的分析\n最优编码器参数 $a^{\\star}$ 由 $a^{\\star} = \\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}$ 给出。分母严格为正。因此，$a^{\\star}$ 是 $s$ 的线性函数。\n随着跳跃连接强度 $s$ 接近 $1$，项 $(1-s)$ 接近 $0$。因此，$a^{\\star}$ 接近 $0$。\n当 $a^{\\star}=0$ 时，编码器变为 $q_{\\phi}(z \\mid x) = \\mathcal{N}(0, v^{\\star})$。此分布独立于输入 $x$。这意味着关于 $x$ 的信息没有被编码到潜变量 $z$ 中。解码器的跳跃连接项 $sx$ 即使没有来自 $z$ 的信息也能准确重构 $x$，有效地绕过了潜变量瓶颈。这种后验 $q_{\\phi}(z \\mid x)$ 坍塌到先验 $p(z)$（或变得与 $x$ 无关）的现象被称为后验坍塌。量化 $z$ 所携带的关于 $x$ 的信息的互信息 $I(x;z)$ 变为零，因为 $I(x; z) = \\frac{1}{2}\\log(1 + (a^{\\star})^2/v^{\\star}) = \\frac{1}{2}\\log(1) = 0$。\n\n### 第 3 部分：缓解策略和 Free-Bits 启发式方法\n测试用例旨在评估三种缓解策略：\n1. **减小 KL 权重 $\\beta$**：较小的 $\\beta$ 会减少偏离先验的惩罚，鼓励模型使用潜变量。情况 C ($s=0.9, \\beta=0.1$) 展示了这一点。\n2. **减小跳跃强度 $s$**：较小的 $s$（离 $1$ 更远）迫使模型更多地依赖 $Wz$ 项进行重构，从而增加 $|a^{\\star}|$。情况 D ($s=0.5$) 展示了这一点。\n3. **Free-bits 启发式方法**：该方法通过强制施加一个最小容量 $\\tau$ 来防止 KL 散度变得过小。KL 项的目标变为 $\\beta \\cdot \\max(\\mathrm{KL} - \\tau, 0)$。\n    - 如果无约束最优解 $(a^{\\star}, v^{\\star})$ 产生的 KL 散度 $\\mathrm{KL}^{\\star} \\ge \\tau$，则目标函数（在一个常数范围内）保持不变，并使用此解。\n    - 如果 $\\mathrm{KL}^{\\star}  \\tau$，我们必须在边界 $\\mathrm{KL} = \\tau$ 上找到一个新的最优解。这可以通过找到一个有效的 KL 权重 $\\beta_{\\text{eff}} > \\beta$ 来实现，使得得到的无约束最优解满足 $\\mathrm{KL}(a^{\\star}(\\beta_{\\text{eff}}), v^{\\star}(\\beta_{\\text{eff}})) = \\tau$。由于 KL 散度是 $\\beta$ 的单调递减函数，这个 $\\beta_{\\text{eff}}$ 是唯一的，并且可以通过数值求根算法找到。所提供代码中的计算对情况 F 使用了这种方法。\n\n### 第 4 部分：计算互信息 I(x;z)\n使用推导出的 $a^{\\star}$、$v^{\\star}$ 和给定的公式，其中 $\\sigma_x^2=1$：\n$$ I(x; z) = \\frac{1}{2}\\log\\left(1 + \\frac{(a^{\\star})^2}{v^{\\star}}\\right)$$\n我们代入 $a^{\\star}$ 和 $v^{\\star}$ 的表达式：\n$$\n\\frac{(a^{\\star})^2}{v^{\\star}} = \\frac{\\left(\\frac{W(1-s)}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}\\right)^2}{\\frac{\\beta \\sigma_{\\epsilon}^{2}}{W^2 + \\beta \\sigma_{\\epsilon}^{2}}} = \\frac{W^2(1-s)^2}{(W^2 + \\beta \\sigma_{\\epsilon}^{2})^2} \\frac{W^2 + \\beta \\sigma_{\\epsilon}^{2}}{\\beta \\sigma_{\\epsilon}^{2}} = \\frac{W^2(1-s)^2}{\\beta \\sigma_{\\epsilon}^{2}(W^2 + \\beta \\sigma_{\\epsilon}^{2})}\n$$\n因此，互信息为：\n$$\nI(x; z) = \\frac{1}{2}\\log\\left(1 + \\frac{W^2(1-s)^2}{\\beta \\sigma_{\\epsilon}^{2}(W^2 + \\beta \\sigma_{\\epsilon}^{2})}\\right)\n$$\n此公式用于所有 \"beta\" 模式的计算。对于 \"freebits\" 模式，适用相同的逻辑，但可能需要使用数值方法找到的有效 $\\beta$。\n特殊情况 $W=0$ 导致 $a^{\\star}=0$，因此 $I(x;z)=0$。该公式是稳健的，但已注意到问题指令中关于处理此退化情况的要求。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mutual information I(x; z) for a linear-Gaussian VAE\n    under different hyperparameter settings to study posterior collapse.\n    \"\"\"\n\n    test_cases = [\n        # (s, W, sigma_eps_sq, beta, tau, regime)\n        (0.0, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # A: Baseline, no skip\n        (1.0, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # B: Full skip, expect collapse\n        (0.9, 1.0, 0.1, 0.1, 0.0, \"beta\"),      # C: Mitigation by low beta\n        (0.5, 1.0, 0.1, 1.0, 0.0, \"beta\"),      # D: Mitigation by moderate s\n        (0.5, 0.0, 0.1, 1.0, 0.0, \"beta\"),      # E: Degenerate case W=0\n        (0.9, 1.0, 0.1, 1.0, 1.0, \"freebits\"), # F: Mitigation by free-bits\n    ]\n\n    results = []\n    \n    # Set sigma_x^2 = 1 as per problem\n    sigma_x_sq = 1.0\n\n    for case in test_cases:\n        s, W, sigma_eps_sq, beta, tau, regime = case\n\n        # Handle degeneracy for W=0 as specified.\n        # This applies to both beta and freebits regimes.\n        if W == 0:\n            results.append(0.0)\n            continue\n        \n        # --- Beta-VAE optimum calculation ---\n        def get_beta_optimum(b_eff):\n            # Calculate optimal a* and v* for a given beta_eff\n            denominator = W**2 + b_eff * sigma_eps_sq\n            a_star = (W * (1 - s)) / denominator\n            v_star = (b_eff * sigma_eps_sq) / denominator\n            return a_star, v_star\n\n        # --- KL divergence calculation ---\n        def get_kl(a, v):\n            # Calculate expected KL divergence for given a, v\n            if v = 0:\n                return float('inf')\n            # Using sigma_x^2 = 1\n            return 0.5 * (v + a**2 * sigma_x_sq - 1 - np.log(v))\n\n        # --- Mutual information calculation ---\n        def get_mi(a, v):\n            # Calculate mutual information I(x;z)\n            if v = 0:\n                return 0.0 # Or handle error appropriately\n            # Using sigma_x^2 = 1\n            return 0.5 * np.log(1 + (a**2 * sigma_x_sq) / v)\n            \n        a_final, v_final = 0, 1 # Default values\n\n        if regime == \"beta\":\n            a_final, v_final = get_beta_optimum(beta)\n\n        elif regime == \"freebits\":\n            # 1. Calculate the unconstrained beta-VAE optimum\n            a_beta, v_beta = get_beta_optimum(beta)\n            kl_beta = get_kl(a_beta, v_beta)\n\n            # 2. Check if the KL is above the threshold tau\n            if kl_beta >= tau:\n                # If so, the optimum is the same as the unconstrained one\n                a_final, v_final = a_beta, v_beta\n            else:\n                # 3. If not, find effective beta that makes KL = tau\n                # We need to find `b_eff` where `get_kl(get_beta_optimum(b_eff)) - tau = 0`.\n                # Let gamma = b_eff * sigma_eps_sq.\n                # function to find root of: kl(gamma) - tau = 0\n                def kl_of_gamma_minus_tau(gamma):\n                    if gamma = 0: return float('inf')\n                    denom_g = W**2 + gamma\n                    a_g = (W * (1 - s)) / denom_g\n                    v_g = gamma / denom_g\n                    return get_kl(a_g, v_g) - tau\n\n                # Bisection method to find gamma_eff\n                gamma_orig = beta * sigma_eps_sq\n                low_gamma = 1e-12  # Avoid log(0)\n                high_gamma = gamma_orig\n\n                # We know kl(gamma_orig)  tau, and kl(gamma->0) -> inf.\n                # So a root exists in (0, gamma_orig). We want to find b_eff > beta,\n                # so we search in (gamma_orig, inf), but KL is decreasing in beta.\n                # Let's re-think. If kl_beta  tau, we need a solution with KL=tau.\n                # KL is a decreasing function of beta. To increase KL from kl_beta to tau,\n                # we need to find a beta_eff  beta.\n                \n                # So the search range for beta_eff is (0, beta).\n                # The search range for gamma_eff is (0, gamma_orig).\n                for _ in range(100): # 100 iterations for high precision\n                    mid_gamma = (low_gamma + high_gamma) / 2\n                    if kl_of_gamma_minus_tau(mid_gamma) > 0:\n                        low_gamma = mid_gamma\n                    else:\n                        high_gamma = mid_gamma\n                \n                gamma_eff = high_gamma\n                beta_eff = gamma_eff / sigma_eps_sq\n                \n                a_final, v_final = get_beta_optimum(beta_eff)\n\n        # Compute the final mutual information for the case\n        mi = get_mi(a_final, v_final)\n        results.append(mi)\n\n    # Format output to 6 decimal places per value\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3197907"}]}