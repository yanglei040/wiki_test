{"hands_on_practices": [{"introduction": "在构建复杂的深度学习模型之前，理解其基本特性至关重要。感受野（Receptive Field）是其中一个核心概念，它描述了网络中单个神经元能够“看到”的输入区域的大小。本练习旨在从第一性原理出发，推导密集连接块（Dense Block）中感受野的增长规律，并将其与经典的残差网络（ResNet）进行比较。通过这个推导过程，你将对信息如何在密集连接架构中传播建立起深刻的直觉，并理解其与其它架构的关键区别[@problem_id:3114064]。", "problem": "考虑一个卷积神经网络（CNN）中的密集块（dense block），如 Densely Connected Convolutional Networks (DenseNet) 中所使用的。该块由 $L$ 个连续的二维卷积层组成，每个卷积层使用 $3 \\times 3$ 的卷积核，步长为 $1$，所有边进行 $1$ 的零填充（因此空间分辨率得以保持），没有池化层，没有扩张卷积（dilation），并且逐点非线性操作不改变空间依赖性。每个层的输入是该块的原始输入与该块中所有先前层输出的通道级联（channelwise concatenation）。该块的输出是所有层输出的通道级联（忽略块之后的任何压缩层或过渡层）。对于任何层的输出，将其感受野边长定义为：能影响该层单个输出像素的、沿着原始块输入的单个空间轴的输入像素数量。将块感受野边长定义为其所有输出通道中最大的感受野边长。\n\n从离散卷积和感受野传播的定义出发，推导该密集块末端的块感受野边长关于 $L$ 的闭式表达式。然后，考虑一个“等效深度”的残差堆栈，它由 $L$ 个连续的 $3 \\times 3$ 步长为 $1$ 的卷积层组成，同样使用 $1$ 的零填充，没有池化层，并安排有恒等跳跃连接以形成标准的残差通路，但不改变空间维度。使用相同的第一性原理推导，得出该残差堆栈末端的块感受野边长，并将其与密集块的感受野边长进行比较。\n\n仅报告块输出（沿一个空间轴测量）的感受野边长的唯一通用闭式表达式，该表达式以 $L$ 为函数。你的最终答案必须是单个解析表达式。无需四舍五入。", "solution": "问题要求为密集块和残差堆栈（每个都由 $L$ 层组成）的感受野边长提供一个唯一的通用闭式表达式。我们将基于第一性原理，通过分析每种架构中的感受野传播来推导这个表达式。\n\n当将一个卷积层应用于输入特征图时，感受野边长 $RF$ 的基本递推关系由下式给出：\n$$RF_{out} = RF_{in} + (k - 1) \\times S_{in}$$\n其中，$RF_{in}$ 是输入特征图相对于原始网络输入的感受野边长，$k$ 是当前层的卷积核边长，$S_{in}$ 是从原始网络输入到当前层输入之间所有先前层的步长之积。原始输入中单个像素的感受野定义为 $RF_{original} = 1$。\n\n在本问题中，对于每个卷积层，卷积核大小为 $k=3$，步长为 $s=1$。由于所有步长都为 $1$，因此步长之积 $S_{in}$ 始终为 $1$。因此，单步卷积的传播公式简化为：\n$$RF_{out} = RF_{in} + (3 - 1) \\times 1 = RF_{in} + 2$$\n这个公式表明，每个步长为 $1$ 的 $3 \\times 3$ 卷积会使感受野边长增加 $2$。\n\n现在我们来分析每种架构。我们将块的原始输入表示为 $x_0$。\n\n**1. 密集块（DenseNet）分析**\n\n在密集块中，第 $\\ell$ 层（其中 $\\ell \\in \\{1, 2, \\dots, L\\}$）的输入是原始块输入 $x_0$ 与所有先前层输出 $y_1, y_2, \\dots, y_{\\ell-1}$ 的通道级联。第 $\\ell$ 层的输出是 $y_\\ell$。\n令 $RF(Z)$ 表示张量 $Z$ 相对于原始输入 $x_0$ 的感受野边长。我们定义 $RF_\\ell \\equiv RF(y_\\ell)$。$y_\\ell$ 中的单个输出像素受到其复合输入 $\\text{concat}(x_0, y_1, \\dots, y_{\\ell-1})$ 的一个 $3 \\times 3$ 区域的影响。总感受野 $RF_\\ell$ 是来自所有输入路径的感受野的并集。该并集的大小由产生最大感受野的路径决定。\n\n对于第 $\\ell=1$ 层：\n输入为 $x_0$。输入像素的感受野为 $RF(x_0) = 1$。\n$$RF_1 = RF(x_0) + (3-1) = 1 + 2 = 3$$\n\n对于第 $\\ell=2$ 层：\n输入为 $\\text{concat}(x_0, y_1)$。$y_2$ 中的一个像素受到该输入的一个 $3 \\times 3$ 区域的影响。\n- 从 $x_0$ 经过第 2 层的卷积路径产生的感受野大小为 $k=3$。\n- 从 $y_1$ 经过第 2 层的卷积路径产生的感受野大小为 $RF_1 + (3-1) = 3 + 2 = 5$。\n$y_2$ 的感受野是这些值中的最大值：\n$$RF_2 = \\max(3, 5) = 5$$\n\n对于第 $\\ell=3$ 层：\n输入为 $\\text{concat}(x_0, y_1, y_2)$。感受野 $RF_3$ 是所有路径产生的感受野中的最大值：\n- 经过 $x_0$ 的路径：$RF=3$。\n- 经过 $y_1$ 的路径：$RF = RF_1 + 2 = 3 + 2 = 5$。\n- 经过 $y_2$ 的路径：$RF = RF_2 + 2 = 5 + 2 = 7$。\n$$RF_3 = \\max(3, 5, 7) = 7$$\n\n通过归纳法，对于任意第 $\\ell$ 层，其感受野为：\n$$RF_\\ell = \\max(RF(\\text{path via } x_0), RF(\\text{path via } y_1), \\dots, RF(\\text{path via } y_{\\ell-1}))$$\n$$RF_\\ell = \\max(3, RF_1+2, RF_2+2, \\dots, RF_{\\ell-1}+2)$$\n由于 $RF_j$ 序列随 $j$ 严格递增，因此序列 $RF_j+2$ 也随 $j$ 严格递增。因此，最大值由通过最近一层 $y_{\\ell-1}$ 的路径决定。\n$$RF_\\ell = RF_{\\ell-1} + 2$$\n这是一个简单的等差数列。以 $RF_1 = 3$ 为基础，第 $\\ell$ 层感受野的闭式解为：\n$$RF_\\ell = RF_1 + (\\ell-1) \\times 2 = 3 + 2\\ell - 2 = 2\\ell + 1$$\n\n该块的输出是所有层输出的级联：$[y_1, y_2, \\dots, y_L]$。“块感受野边长”被定义为其所有输出通道中最大的感受野边长，即 $\\max(RF_1, RF_2, \\dots, RF_L)$。由于 $RF_\\ell = 2\\ell+1$ 是关于 $\\ell$ 的严格递增函数，最大值在 $\\ell=L$ 时取得。\n$$RF_{\\text{dense block}} = RF_L = 2L + 1$$\n\n**2. 残差堆栈（ResNet）分析**\n\n在标准的残差堆栈中，第 $\\ell$ 个块的输出（我们表示为 $z_\\ell$）是该块的输入 $z_{\\ell-1}$ 与该输入的卷积变换输出 $\\text{Conv}_\\ell(z_{\\ell-1})$ 的和。（问题陈述逐点非线性操作不改变空间依赖性，因此它们不影响感受野的计算）。\n$$z_\\ell = \\text{Conv}_\\ell(z_{\\ell-1}) + z_{\\ell-1}$$\n令 $RF_\\ell'$ 为 $z_\\ell$ 相对于初始输入 $z_0 = x_0$ 的感受野边长。初始输入的感受野为 $RF_0' = 1$。\n\n对于第 $\\ell$ 层，$z_\\ell$ 中的一个输出像素受到源自 $z_{\\ell-1}$ 的两条路径的影响：\n- 卷积路径：$\\text{Conv}_\\ell(z_{\\ell-1})$。此路径贡献的感受野为 $RF_{\\ell-1}' + (3-1) = RF_{\\ell-1}' + 2$。\n- 恒等跳跃连接路径：$z_{\\ell-1}$。此路径贡献的感受野即为 $RF_{\\ell-1}'$。\n\n$z_\\ell$ 的总感受野是这两条路径感受野的并集。并集的大小是各独立感受野大小的最大值。\n$$RF_\\ell' = \\max(RF_{\\ell-1}' + 2, RF_{\\ell-1}') = RF_{\\ell-1}' + 2$$\n这建立了一个与简单卷积层堆栈相同的递推关系。我们有一个从 $RF_0' = 1$ 开始的等差数列。经过 $L$ 层后，感受野为：\n$$RF_L' = RF_0' + \\sum_{i=1}^{L} 2 = 1 + 2L$$\n\n残差堆栈的最终输出是 $z_L$，因此该块的感受野是 $RF_L'$。\n$$RF_{\\text{residual stack}} = 2L + 1$$\n\n**结论**\n\n在指定条件下（$3 \\times 3$ 卷积核，步长 $1$），密集块和残差堆栈的感受野大小都呈现出相同的线性增长。在这两种架构中，感受野扩张的主导路径是卷积的顺序链。由密集连接或残差连接创建的较短路径具有较小的感受野，这些感受野被包含在最长路径的感受野之内。\n\n两种架构的块感受野边长的唯一通用闭式表达式均为 $2L+1$。", "answer": "$$\\boxed{2L+1}$$", "id": "3114064"}, {"introduction": "理论分析是基础，但动手实践能将知识内化。在深度网络设计中，非线性激活函数的选择对模型的训练动态和最终性能有着决定性的影响。本练习将指导你从零开始构建一个简化的密集连接块，并在一个合成的数据集上进行训练，以实证的方式比较几种现代激活函数（如 ReLU、GELU 和 SiLU）的效果[@problem_id:3113972]。你将获得从数据生成、模型实现（包括前向与反向传播）、到模型训练与评估的完整实践经验，从而具体地理解不同的架构选择如何转化为可观察的性能差异。", "problem": "您将实现并比较仅激活函数不同的稠密块神经网络，以研究选择修正线性单元(ReLU)、高斯误差线性单元(GELU)和Sigmoid线性单元(SiLU)如何影响优化动态和最终准确率。您必须编写一个完整的程序，该程序构建一个合成的多类别分类任务，建立一个带有拼接连接的简化稠密块，使用全批量梯度下降进行训练，并为指定的测试套件报告量化指标。\n\n基本原理：\n- 稠密块是一系列层的序列，其中每个新层接收所有先前特征图的拼接作为输入。具体来说，对于输入向量 $x \\in \\mathbb{R}^{d_0}$、增长率 $k \\in \\mathbb{N}$ 和 $L \\in \\mathbb{N}$ 个层，第 $\\ell \\in \\{1,\\dots,L\\}$ 层计算 $z_\\ell = W_\\ell \\, \\mathrm{concat}(x, h_1, \\dots, h_{\\ell-1}) + b_\\ell$，然后 $h_\\ell = \\phi(z_\\ell)$，其中 $\\phi$ 是激活函数。最终表示为 $H = \\mathrm{concat}(x, h_1, \\dots, h_L) \\in \\mathbb{R}^{d_0 + kL}$，它被传递给一个线性分类器，为 $C$ 个类别生成 logits $o = H W_c + b_c \\in \\mathbb{R}^{C}$。\n- 对于多类别分类，使用标准的 softmax 和交叉熵：给定样本 $i$ 的 logits $o_i \\in \\mathbb{R}^{C}$ 和真实类别索引 $y_i \\in \\{0,\\dots,C-1\\}$，定义概率 $p_{i,c} = \\exp(o_{i,c}) / \\sum_{j=1}^{C} \\exp(o_{i,j})$ 和损失 $\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( - \\log p_{i, y_i} \\right)$，其中 $N$ 为样本数。\n- 通过全批量梯度下降进行训练，使用固定的学习率 $\\eta  0$ 和固定的周期数 $E \\in \\mathbb{N}$。损失函数关于参数的梯度通过微积分的链式法则计算。分类准确率是正确预测的比例 $\\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}[\\arg\\max_c p_{i,c} = y_i]$。\n\n待比较的激活函数：\n- 修正线性单元 (ReLU): $\\phi(x) = \\max(0, x)$。\n- 高斯误差线性单元 (GELU)，使用广泛应用的 tanh 近似：$\\phi(x) \\approx \\frac{1}{2} x \\left( 1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} (x + 0.044715 x^3)\\right) \\right)$。\n- Sigmoid线性单元 (SiLU)，也称为 Swish：$\\phi(x) = x \\cdot \\sigma(x)$，其中 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$。\n\n数据：\n- 通过从以半径为 $r$ 的圆上均匀间隔角度为中心的各向同性高斯分布中为每个类别抽取 $N_c$ 个样本，来合成一个在 $\\mathbb{R}^{2}$ 中的 $C$ 类别数据集。对于类别 $c \\in \\{0,\\dots,C-1\\}$，均值为 $\\mu_c = r [\\cos(2\\pi c / C), \\sin(2\\pi c / C)]$，每个样本为 $\\mu_c + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_2)$。使用 $C = 3$, $N_c = 100$, $r = 3$ 和 $\\sigma = 0.5$。总共有 $N = 300$ 个样本。\n\n训练：\n- 权重使用 He 风格初始化：对于一个 fan-in 为 $f$ 的层，将其条目初始化为来自 $\\mathcal{N}(0, \\sqrt{2/f})$ 的独立样本。将偏置初始化为零。在所有可比较的情况下使用相同的随机种子，以隔离激活函数的影响。\n- 使用全批量梯度下降，不带动量或权重衰减。\n- 每次运行计算以下指标：\n  1. $E$ 个周期后的最终平均交叉熵损失，报告为四舍五入到四位小数的实数。\n  2. 最终分类准确率，报告为在区间 $[0,1]$ 内四舍五入到三位小数的实数。\n  3. 平均损失首次严格低于阈值 $\\tau$ 的周期索引（从1开始），如果在 $E$ 个周期内从未发生，则为 $-1$。使用 $\\tau = 0.6$。\n\n测试套件：\n- 固定上述指定的数据集，并在所有测试中重复使用。为保证可比性，在适用的情况下对权重初始化使用相同的随机种子。\n- 评估以下五个案例（每个案例是一个元组，包含激活函数、层数 $L$、增长率 $k$、学习率 $\\eta$、周期数 $E$ 和初始化种子 $s$）：\n  1. 案例 A (正常路径): 激活函数 ReLU, $L = 3$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$。\n  2. 案例 B (正常路径): 激活函数 GELU, $L = 3$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$。\n  3. 案例 C (正常路径): 激活函数 SiLU, $L = 3$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$。\n  4. 案例 D (连接深度边界条件): 激活函数 ReLU, $L = 1$, $k = 12$, $\\eta = 0.05$, $E = 200$, $s = 123$。\n  5. 案例 E (优化速度边缘案例): 激活函数 GELU, $L = 3$, $k = 12$, $\\eta = 0.005$, $E = 200$, $s = 123$。\n\n角度单位：\n- 三角函数中使用的所有角度均为弧度。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果。对于每个案例，按 [final_loss, final_accuracy, first_epoch_index_or_-1] 的顺序输出一个包含三个值的列表。因此，总输出应该是一个包含五个列表的列表，不带空格。例如：[[0.1234,0.987,42],[...],...]。", "solution": "用户提供的问题已经过严格验证，被确定为一个有效、适定的科学问题。构建合成数据集、建立稠密块神经网络并使用全批量梯度下降进行训练所需的所有必要参数和定义均已提供。该问题在科学上基于既有的深度学习原理，是客观的，并且具有唯一、可验证的解。\n\n解决方案通过基于原理、循序渐进的方式实现指定的组件。\n\n### 1. 合成数据生成\n根据问题规范创建一个合成数据集。它包含在 $d_0=2$ 维空间中的 $N=300$ 个样本，分布在 $C=3$ 个类别中，每个类别有 $N_c=100$ 个样本。每个类别 $c \\in \\{0, 1, 2\\}$ 的中心或均值 $\\mu_c$ 位于半径为 $r=3$ 的圆上。坐标由极坐标到笛卡尔坐标转换的原理给出：$\\mu_c = r [\\cos(2\\pi c / C), \\sin(2\\pi c / C)]$。每个数据点通过从标准差为 $\\sigma=0.5$ 的各向同性高斯分布 $\\mathcal{N}(\\mu_c, \\sigma^2 I_2)$ 中采样生成。数据生成使用单个固定的随机种子，以确保所有模型都在完全相同的数据集上进行训练和评估，从而隔离架构和超参数变化的影响。\n\n### 2. 稠密块网络架构\n模型的核心是一个稠密块，实现为 $L$ 个层的序列。稠密块的定义性原理是其连接模式：每个层接收所有先前层的特征图与原始输入拼接后的结果作为输入。\n\n- **初始化**：网络参数（权重和偏置）在训练前进行初始化。遵循 He 初始化原理，对于一个 fan-in 为 $f$ 的层，其权重从正态分布 $\\mathcal{N}(0, \\sigma_W^2)$ 中抽取，其中标准差为 $\\sigma_W = \\sqrt{2/f}$。这有助于缓解梯度消失/爆炸问题。所有偏置都初始化为零。在每个测试案例中，初始化都使用特定的随机种子 $s$ 以确保可复现性。\n- **前向传播**：前向传播为给定的输入批次 $X$ 计算网络输出 (logits)。对于每个层 $\\ell \\in \\{1, \\dots, L\\}$：\n    1. 该层的输入 $H_{\\ell-1}$ 是通过将原始输入 $X$ 与所有先前层的输出拼接而成的：$H_{\\ell-1} = \\mathrm{concat}(X, h_1, \\dots, h_{\\ell-1})$。此输入的维度是 $d_0 + (\\ell-1)k$，它作为权重初始化的 fan-in。\n    2. 应用线性变换：$z_\\ell = H_{\\ell-1} W_\\ell + b_\\ell$，其中 $W_\\ell \\in \\mathbb{R}^{(d_0+(\\ell-1)k) \\times k}$ 且 $b_\\ell \\in \\mathbb{R}^k$。\n    3. 逐元素应用非线性激活函数 $\\phi$ 以产生该层的输出：$h_\\ell = \\phi(z_\\ell) \\in \\mathbb{R}^k$。\n- 在最后一个块层之后，所有中间输出都与原始输入拼接，形成最终表示 $H = \\mathrm{concat}(X, h_1, \\dots, h_L)$。\n- 最后的线性分类器层将此表示映射为 $C$ 个类别的 logits：$o = H W_c + b_c$。\n- **激活函数**：按规定实现了三种激活函数：ReLU ($\\phi(x) = \\max(0, x)$)、GELU 的 tanh 近似 ($\\phi(x) \\approx \\frac{1}{2} x (1 + \\tanh(\\sqrt{2/\\pi} (x + 0.044715 x^3)))$) 和 SiLU ($\\phi(x) = x \\cdot (1 + e^{-x})^{-1}$)。\n\n### 3. 损失计算与优化\n该模型被训练用于执行多类别分类。\n- **Softmax 和交叉熵损失**：输出的 logits $o$ 使用 softmax 函数转换为概率 $p$，$p_c = \\exp(o_c) / \\sum_j \\exp(o_j)$。使用 log-sum-exp 技巧 ($o_c \\leftarrow o_c - \\max(o)$) 来确保数值稳定性。训练目标是最小化批次上的平均交叉熵损失，$\\mathcal{L} = -\\frac{1}{N}\\sum_i \\log p_{i, y_i}$，其中 $y_i$ 是样本 $i$ 的真实类别。\n- **反向传播**：损失函数关于所有模型参数 ($W_\\ell, b_\\ell, W_c, b_c$) 的梯度是通过链式法则计算的，这个过程称为反向传播。交叉熵损失关于 logits 的导数是 $\\frac{\\partial \\mathcal{L}}{\\partial o_i} = (p_i - y_{i, \\text{one-hot}}) / N$。然后，这个梯度会通过分类器和稠密块向后传播。对于稠密块，给定隐藏输出 $h_\\ell$ 的梯度是其所贡献的所有路径梯度的总和：它与最终分类器的直接连接，以及它作为所有后续层 $j > \\ell$ 输入的使用。我们的实现会为每个层的参数仔细累积这些梯度。每个激活函数的导数也是必需的，并已相应实现。\n- **梯度下降**：参数使用全批量梯度下降规则进行更新：$\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}$，其中 $\\theta$ 代表任何参数，$\\eta$ 是学习率，$\\nabla_\\theta \\mathcal{L}$ 是计算出的梯度。\n\n### 4. 评估与报告\n模型按固定的周期数 $E$ 进行训练。对于每个测试案例，计算三个指标：\n1.  **最终损失**：在 $E$ 个周期后，在整个数据集上的平均交叉熵损失。\n2.  **最终准确率**：在 $E$ 个周期后，正确分类样本的比例。\n3.  **收敛速度**：训练损失首次严格低于阈值 $\\tau = 0.6$ 的周期索引（从1开始）。如果损失从未低于此阈值，则报告为 $-1$。\n\n该实现将此逻辑封装到一个类中，然后为五个指定的测试案例分别实例化和训练该类。收集结果并格式化为所需的单行输出。", "answer": "```python\nimport numpy as np\n\n# This program implements a dense-block neural network and compares the performance\n# of different activation functions (ReLU, GELU, SiLU) on a synthetic classification task.\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef relu_grad(x):\n    \"\"\"Gradient of the ReLU function.\"\"\"\n    return (x > 0).astype(float)\n\ndef gelu_approx(x):\n    \"\"\"Gaussian Error Linear Unit (GELU) approximation.\"\"\"\n    a = np.sqrt(2 / np.pi)\n    b = 0.044715\n    return 0.5 * x * (1 + np.tanh(a * (x + b * x**3)))\n\ndef gelu_approx_grad(x):\n    \"\"\"Gradient of the GELU approximation.\"\"\"\n    a = np.sqrt(2 / np.pi)\n    b = 0.044715\n    x_cubed = x**3\n    inner_term = x + b * x_cubed\n    g_x = a * inner_term\n    tanh_g_x = np.tanh(g_x)\n    g_prime_x = a * (1 + 3 * b * x**2)\n    sech_g_x_sq = 1 - tanh_g_x**2\n    return 0.5 * (1 + tanh_g_x) + 0.5 * x * sech_g_x_sq * g_prime_x\n\ndef silu(x):\n    \"\"\"Sigmoid Linear Unit (SiLU) activation function.\"\"\"\n    # sigma(x) = 1 / (1 + exp(-x))\n    return x / (1 + np.exp(-x))\n\ndef silu_grad(x):\n    \"\"\"Gradient of the SiLU function.\"\"\"\n    sigma = 1 / (1 + np.exp(-x))\n    return sigma * (1 + x * (1 - sigma))\n\ndef generate_data(C, N_c, r, sigma, seed=42):\n    \"\"\"Generates a synthetic C-class dataset.\"\"\"\n    rng = np.random.default_rng(seed)\n    N = C * N_c\n    X = np.zeros((N, 2))\n    y = np.zeros(N, dtype=int)\n    for c in range(C):\n        angle = 2 * np.pi * c / C\n        mu = r * np.array([np.cos(angle), np.sin(angle)])\n        start_idx, end_idx = c * N_c, (c + 1) * N_c\n        X[start_idx:end_idx, :] = mu + rng.normal(0, sigma, size=(N_c, 2))\n        y[start_idx:end_idx] = c\n    return X, y\n\nclass DenseBlockNet:\n    \"\"\"A neural network with a simplified dense block architecture.\"\"\"\n    def __init__(self, input_dim, num_classes, L, k, activation, seed):\n        self.input_dim = input_dim\n        self.num_classes = num_classes\n        self.L = L\n        self.k = k\n        self.rng = np.random.default_rng(seed)\n        self.activation_fn, self.activation_grad = self._get_activation(activation)\n\n        self.weights = []\n        self.biases = []\n\n        # Initialize dense block layer parameters\n        current_dim = self.input_dim\n        for _ in range(L):\n            fan_in = current_dim\n            std_dev = np.sqrt(2.0 / fan_in)\n            self.weights.append(self.rng.normal(0, std_dev, (fan_in, k)))\n            self.biases.append(np.zeros(k))\n            current_dim += k\n        \n        # Initialize classifier layer parameters\n        fan_in = current_dim\n        std_dev = np.sqrt(2.0 / fan_in)\n        self.weights.append(self.rng.normal(0, std_dev, (fan_in, num_classes)))\n        self.biases.append(np.zeros(num_classes))\n\n    def _get_activation(self, name):\n        if name == 'ReLU': return relu, relu_grad\n        if name == 'GELU': return gelu_approx, gelu_approx_grad\n        if name == 'SiLU': return silu, silu_grad\n        raise ValueError(f\"Unknown activation function: {name}\")\n\n    def forward(self, X):\n        cache = {'h_outputs': [], 'z_outputs': [], 'layer_inputs': []}\n        layer_input = X\n        \n        for i in range(self.L):\n            cache['layer_inputs'].append(layer_input)\n            W, b = self.weights[i], self.biases[i]\n            z = layer_input @ W + b\n            h = self.activation_fn(z)\n            cache['z_outputs'].append(z)\n            cache['h_outputs'].append(h)\n            layer_input = np.concatenate([X] + cache['h_outputs'], axis=1)\n\n        final_H = layer_input\n        cache['final_H'] = final_H\n        \n        W_c, b_c = self.weights[-1], self.biases[-1]\n        logits = final_H @ W_c + b_c\n\n        stable_logits = logits - np.max(logits, axis=1, keepdims=True)\n        exp_logits = np.exp(stable_logits)\n        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n        cache['probs'] = probs\n        return probs, cache\n\n    def backward(self, X, Y_one_hot, cache):\n        N = X.shape[0]\n        grads = {'weights': [None]*(self.L + 1), 'biases': [None]*(self.L + 1)}\n\n        d_logits = (cache['probs'] - Y_one_hot) / N\n        H_final = cache['final_H']\n        W_c = self.weights[-1]\n        \n        grads['weights'][-1] = H_final.T @ d_logits\n        grads['biases'][-1] = np.sum(d_logits, axis=0)\n        d_H_final = d_logits @ W_c.T\n\n        d_h_accum = {}\n        current_dim = self.input_dim\n        for i in range(self.L):\n            d_h_accum[i] = d_H_final[:, current_dim : current_dim + self.k]\n            current_dim += self.k\n\n        for i in range(self.L - 1, -1, -1):\n            d_h = d_h_accum[i]\n            z = cache['z_outputs'][i]\n            d_z = d_h * self.activation_grad(z)\n            \n            layer_input = cache['layer_inputs'][i]\n            W = self.weights[i]\n            grads['weights'][i] = layer_input.T @ d_z\n            grads['biases'][i] = np.sum(d_z, axis=0)\n            \n            d_layer_input = d_z @ W.T\n            current_dim = self.input_dim\n            for j in range(i):\n                grad_for_hj = d_layer_input[:, current_dim : current_dim + self.k]\n                d_h_accum[j] += grad_for_hj\n                current_dim += self.k\n        return grads\n\n    def train(self, X, y, epochs, learning_rate, loss_threshold):\n        N, C = X.shape[0], self.num_classes\n        y_one_hot = np.zeros((N, C)); y_one_hot[np.arange(N), y] = 1\n        first_epoch_below_threshold = -1\n\n        for epoch in range(1, epochs + 1):\n            probs, cache = self.forward(X)\n            loss = -np.sum(np.log(probs[np.arange(N), y] + 1e-9)) / N\n\n            if loss  loss_threshold and first_epoch_below_threshold == -1:\n                first_epoch_below_threshold = epoch\n            \n            grads = self.backward(X, y_one_hot, cache)\n            for i in range(self.L + 1):\n                self.weights[i] -= learning_rate * grads['weights'][i]\n                self.biases[i] -= learning_rate * grads['biases'][i]\n        \n        final_probs, _ = self.forward(X)\n        final_loss = -np.sum(np.log(final_probs[np.arange(N), y] + 1e-9)) / N\n        predictions = np.argmax(final_probs, axis=1)\n        final_accuracy = np.mean(predictions == y)\n        \n        return final_loss, final_accuracy, first_epoch_below_threshold\n\ndef solve():\n    test_cases = [\n        ('ReLU', 3, 12, 0.05, 200, 123),\n        ('GELU', 3, 12, 0.05, 200, 123),\n        ('SiLU', 3, 12, 0.05, 200, 123),\n        ('ReLU', 1, 12, 0.05, 200, 123),\n        ('GELU', 3, 12, 0.005, 200, 123),\n    ]\n\n    X_data, y_data = generate_data(C=3, N_c=100, r=3, sigma=0.5, seed=0)\n    \n    results = []\n    for activation, L, k, eta, E, s in test_cases:\n        model = DenseBlockNet(\n            input_dim=2, num_classes=3, L=L, k=k, activation=activation, seed=s)\n        \n        loss, acc, epoch_idx = model.train(\n            X_data, y_data, epochs=E, learning_rate=eta, loss_threshold=0.6)\n        \n        results.append([round(loss, 4), round(acc, 3), epoch_idx])\n\n    outer_parts = []\n    for res in results:\n        outer_parts.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    print(f\"[{','.join(outer_parts)}]\")\n\nsolve()\n```", "id": "3113972"}, {"introduction": "密集连接的核心在于其独特的特征图拼接（concatenation）操作，这也带来了一些微妙但关键的设计挑战，其中最重要的就是如何有效地应用归一化。本练习将深入探讨一个具体的设计决策：将批量归一化（Batch Normalization, BN）层放置在特征拼接之前与之后的区别。通过精确计算和比较两种策略下输出特征图的逐通道统计量，你将理解为何归一化层的位置并非随意，以及它如何影响后续层处理的特征的均匀性——这是保证模型稳定训练的关键因素[@problem_id:3114019]。", "problem": "考虑一个密集块，它沿着通道维度拼接特征图，这是密集卷积网络（DenseNet）风格架构的一种典型模式。设来自前面层的一组通道表示为 $A$，其包含 $c_A$ 个通道；新生成的一组通道表示为 $B$，其包含 $c_B$ 个通道。对于每个通道 $j$，将其归一化前的激活值建模为一个随机变量 $x_j$，其均值为 $\\mu_j$，方差为 $\\sigma_j^2$。假设各通道是独立的，并且统计量在整个批次上是平稳的。我们研究两种关于批归一化（BN）与拼接操作相对位置的策略：\n- 策略 $S_1$（拼接前BN）：仅对新集合 $B$ 应用BN，然后将 $A$ 与归一化后的 $B$ 进行拼接。\n- 策略 $S_2$（拼接后BN）：首先拼接 $A$ 和 $B$，然后对所有拼接后的通道应用单个BN层。\n\n使用以下基本定义：\n- 对单个通道 $x$ 进行批归一化（BN），其参数为 $\\gamma$ 和 $\\beta$，以及一个小的数值稳定器 $\\varepsilon0$，计算过程如下：\n$$\n\\mathrm{BN}(x) \\;=\\; \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta,\n$$\n其中 $\\mu$ 和 $\\sigma^2$ 是该通道的批估计均值和方差。在本问题中，假设 $\\gamma=1$ 和 $\\beta=0$，因此输出的中心化和缩放完全由批统计量和 $\\varepsilon$ 决定。\n- 沿通道的拼接是一个纯粹的结构性操作，它本身不改变每个通道的统计数据；它只是将来自 $A$ 和 $B$ 的通道聚合到一个张量中。\n\n任务：\n1. 根据BN的定义和独立性假设，推导出在策略 $S_1$ 和 $S_2$ 下，每个通道归一化后的均值和方差，用给定的归一化前均值 $\\mu_j$、方差 $\\sigma_j^2$ 和 $\\varepsilon$ 表示。你的推导必须只使用上述基本定义以及关于拼接和逐通道BN如何相互作用的逻辑。\n2. 定义以下两个标量度量，用于概括归一化步骤后的跨通道异质性：\n   - 跨通道最大绝对均值\n   $$\n   M_{\\max} \\;=\\; \\max_{j} \\left| \\mathbb{E}[y_j] \\right|,\n   $$\n   其中 $y_j$ 是通道 $j$ 归一化后的激活值。\n   - 跨通道方差均匀性比率\n   $$\n   R_{\\mathrm{var}} \\;=\\; \\frac{\\max_{j} \\mathrm{Var}(y_j)}{\\min_{j} \\mathrm{Var}(y_j)}.\n   $$\n3. 实现一个程序，对于下面的每个测试用例，计算并输出由四个浮点数组成的元组：\n$$\n\\left[ M_{\\max}^{(S_1)}, \\; R_{\\mathrm{var}}^{(S_1)}, \\; M_{\\max}^{(S_2)}, \\; R_{\\mathrm{var}}^{(S_2)} \\right],\n$$\n其中 $M_{\\max}^{(S_k)}$ 和 $R_{\\mathrm{var}}^{(S_k)}$ 表示策略 $S_k$ 下的度量。\n\n重要的实现细节：\n- 将BN统计量视为与为每个通道提供的 $\\mu_j$ 和 $\\sigma_j^2$ 完全相等（无估计误差），并在给定的测试用例中对所有通道使用提供的 $\\varepsilon$。\n- 在策略 $S_1$ 下，仅对 $B$ 中的通道应用BN；$A$ 中的通道保持不变。在策略 $S_2$ 下，在拼接后对 $A$ 和 $B$ 中的所有通道应用BN。\n- 你的程序必须生成单行输出，其中包含所有测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个测试用例的结果本身是如上所述的包含四个浮点数的列表。例如，如果有 $T$ 个测试用例，第 $t$ 个结果是列表 $r_t$，则最终输出格式必须是\n$$\n[ r_1, r_2, \\dots, r_T ].\n$$\n\n测试套件：\n严格使用以下四个测试用例，每个用例由 $(\\mu_A, \\sigma_A^2, \\mu_B, \\sigma_B^2, \\varepsilon)$ 定义，其中 $\\mu_A$ 和 $\\sigma_A^2$ 是长度为 $c_A$ 的列表，$\\mu_B$ 和 $\\sigma_B^2$ 是长度为 $c_B$ 的列表。提供的每个数字都必须按原样使用。\n\n- 测试用例1（理想情况，A已归一化，B未归一化）：\n  - $\\mu_A = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 1.0, \\; 1.0, \\; 1.0 \\,]$\n  - $\\mu_B = [\\, 3.0, \\; -2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 4.0, \\; 0.5 \\,]$\n  - $\\varepsilon = 10^{-5}$\n\n- 测试用例2（异质的A和B）：\n  - $\\mu_A = [\\, 1.5, \\; -0.5 \\,]$\n  - $\\sigma_A^2 = [\\, 2.0, \\; 0.2 \\,]$\n  - $\\mu_B = [\\, 0.0, \\; 0.0, \\; 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 1.0, \\; 5.0, \\; 0.01 \\,]$\n  - $\\varepsilon = 10^{-3}$\n\n- 测试用例3（极小方差的边缘情况）：\n  - $\\mu_A = [\\, 0.0 \\,]$\n  - $\\sigma_A^2 = [\\, 10^{-6} \\,]$\n  - $\\mu_B = [\\, 0.0 \\,]$\n  - $\\sigma_B^2 = [\\, 10^{-6} \\,]$\n  - $\\varepsilon = 10^{-6}$\n\n- 测试用例4（大的方差不匹配）：\n  - $\\mu_A = [\\, 5.0, \\; -3.0, \\; 1.0 \\,]$\n  - $\\sigma_A^2 = [\\, 100.0, \\; 0.1, \\; 10.0 \\,]$\n  - $\\mu_B = [\\, 2.0 \\,]$\n  - $\\sigma_B^2 = [\\, 50.0 \\,]$\n  - $\\varepsilon = 10^{-4}$\n\n你的程序必须为每个测试用例计算这四个度量，并以如下确切格式打印单行结果\n$$\n[ [M_{\\max}^{(S_1)}, R_{\\mathrm{var}}^{(S_1)}, M_{\\max}^{(S_2)}, R_{\\mathrm{var}}^{(S_2)}], \\; \\dots ]\n$$\n每个测试用例对应一个方括号列表，顺序与上面提供的一致。", "solution": "该问题被认为是有效的。它在科学上基于深度学习和统计学的原理，是适定的，每个测试用例都有唯一解，并且表述客观。解答所需的所有数据和定义均已提供且一致。我们可以开始解答。\n\n### 1. 归一化后统计量的推导\n\n该分析依赖于期望和方差的基本性质。对于一个随机变量 $X$ 和常数 $a$、 $b$：\n- 期望的线性性质：$\\mathbb{E}[aX + b] = a\\mathbb{E}[X] + b$\n- 方差的性质：$\\mathrm{Var}(aX + b) = a^2 \\mathrm{Var}(X)$\n\n问题中给出的单个通道激活值 $x$ 的批归一化（BN）变换如下：\n$$\n\\mathrm{BN}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} + \\beta\n$$\n在 $\\gamma=1$ 和 $\\beta=0$ 的约束下，上式简化为：\n$$\ny = \\mathrm{BN}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n$$\n其中 $y$ 是归一化后的激活值，$\\mu = \\mathbb{E}[x]$ 和 $\\sigma^2 = \\mathrm{Var}(x)$ 分别是该通道归一化前的均值和方差。\n\n我们基于此定义分析两种策略 $S_1$ 和 $S_2$。\n\n#### 策略 $S_1$：拼接前BN\n\n在此策略中，只有集合 $B$ 中的新通道被归一化。来自集合 $A$ 的通道未经修改直接进行拼接。\n\n- **对于集合 $A$ 中的通道 $j$**：\n激活值不变。设 $x_j$ 为归一化前激活值，$y_j$ 为归一化后激活值。\n$$\ny_j = x_j\n$$\n因此，归一化后的统计量与归一化前的统计量相同：\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}[x_j] = \\mu_j\n$$\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}(x_j) = \\sigma_j^2\n$$\n\n- **对于集合 $B$ 中的通道 $j$**：\n激活值通过BN进行变换。设 $x_j$ 是通道 $j \\in B$ 的归一化前激活值，其均值为 $\\mu_j$，方差为 $\\sigma_j^2$。归一化后的激活值为：\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\n我们推导 $y_j$ 的均值和方差：\n均值为：\n$$\n\\mathbb{E}[y_j] = \\mathbb{E}\\left[\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j]\n$$\n根据期望的线性性质，$\\mathbb{E}[x_j - \\mu_j] = \\mathbb{E}[x_j] - \\mu_j = \\mu_j - \\mu_j = 0$。\n$$\n\\mathbb{E}[y_j] = 0\n$$\n方差为：\n$$\n\\mathrm{Var}(y_j) = \\mathrm{Var}\\left(\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j - \\mu_j)\n$$\n因为对于任意常数 $c$，$\\mathrm{Var}(X-c) = \\mathrm{Var}(X)$，所以我们有 $\\mathrm{Var}(x_j - \\mu_j) = \\mathrm{Var}(x_j) = \\sigma_j^2$。\n$$\n\\mathrm{Var}(y_j) = \\frac{1}{\\sigma_j^2 + \\varepsilon} \\cdot \\sigma_j^2 = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\n\n#### 策略 $S_2$：拼接后BN\n\n在此策略中，首先拼接集合 $A$ 和 $B$ 的通道。然后，对所有通道应用一个BN层。问题指明BN是逐通道操作，意味着拼接后的集合 $A \\cup B$ 中的每个通道 $j$ 都使用其自身的统计量 $\\mu_j$ 和 $\\sigma_j^2$ 进行归一化。\n\n- **对于拼接后集合 $A \\cup B$ 中的任意通道 $j$**：\n归一化后的激活值 $y_j$ 由下式给出：\n$$\ny_j = \\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\n$$\n$y_j$ 的均值和方差的推导过程与策略 $S_1$ 下集合 $B$ 中通道的推导过程相同。\n均值为：\n$$\n\\mathbb{E}[y_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} \\mathbb{E}[x_j - \\mu_j] = \\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}} (\\mu_j - \\mu_j) = 0\n$$\n方差为：\n$$\n\\mathrm{Var}(y_j) = \\left(\\frac{1}{\\sqrt{\\sigma_j^2 + \\varepsilon}}\\right)^2 \\mathrm{Var}(x_j) = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\n$$\n这适用于所有通道，无论它们是来自集合 $A$ 还是集合 $B$。\n\n### 2. 跨通道度量的公式\n\n使用推导出的统计量，我们可以为度量 $M_{\\max}$ 和 $R_{\\mathrm{var}}$ 构建表达式。\n\n#### 策略 $S_1$ 的度量\n\n归一化后均值的集合是 $\\{\\mu_j\\}_{j \\in A} \\cup \\{0\\}_{j \\in B}$。\n最大绝对均值 $M_{\\max}^{(S_1)}$ 为：\n$$\nM_{\\max}^{(S_1)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max(\\max_{j \\in A} |\\mu_j|, \\max_{j \\in B} |0|) = \\max_{j \\in A} |\\mu_j|\n$$\n如果集合 $A$ 为空，则 $M_{\\max}^{(S_1)} = 0$。\n\n归一化后方差的集合是 $\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B}$。\n方差均匀性比率 $R_{\\mathrm{var}}^{(S_1)}$ 为：\n$$\nR_{\\mathrm{var}}^{(S_1)} = \\frac{\\max(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}{\\min(\\{\\sigma_j^2\\}_{j \\in A} \\cup \\{\\frac{\\sigma_k^2}{\\sigma_k^2 + \\varepsilon}\\}_{k \\in B})}\n$$\n\n#### 策略 $S_2$ 的度量\n\n对于每个通道 $j \\in A \\cup B$，其归一化后的均值为 $0$。\n最大绝对均值 $M_{\\max}^{(S_2)}$ 为：\n$$\nM_{\\max}^{(S_2)} = \\max_{j \\in A \\cup B} |\\mathbb{E}[y_j]| = \\max_{j \\in A \\cup B} |0| = 0\n$$\n\n归一化后方差的集合是 $\\{\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\}_{j \\in A \\cup B}$。\n方差均匀性比率 $R_{\\mathrm{var}}^{(S_2)}$ 为：\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\max_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}{\\min_{j \\in A \\cup B} \\left(\\frac{\\sigma_j^2}{\\sigma_j^2 + \\varepsilon}\\right)}\n$$\n函数 $f(v) = \\frac{v}{v + \\varepsilon}$ 在 $v \\ge 0$ 时是单调递增的。因此，变换后方差的最大值和最小值对应于原始方差 $\\sigma_j^2$ 的最大值和最小值。\n$$\nR_{\\mathrm{var}}^{(S_2)} = \\frac{\\frac{\\max_{j \\in A \\cup B} \\sigma_j^2}{\\max_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}{\\frac{\\min_{j \\in A \\cup B} \\sigma_j^2}{\\min_{j \\in A \\cup B} \\sigma_j^2 + \\varepsilon}}\n$$\n现在这些公式可以用于实现了。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the metrics for cross-channel heterogeneity\n    under two different batch normalization strategies in a dense block.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        (\n            [0.0, 0.0, 0.0],  # mu_A\n            [1.0, 1.0, 1.0],  # sigma_A^2\n            [3.0, -2.0],      # mu_B\n            [4.0, 0.5],       # sigma_B^2\n            1e-5              # epsilon\n        ),\n        # Test case 2\n        (\n            [1.5, -0.5],      # mu_A\n            [2.0, 0.2],       # sigma_A^2\n            [0.0, 0.0, 0.0],  # mu_B\n            [1.0, 5.0, 0.01], # sigma_B^2\n            1e-3              # epsilon\n        ),\n        # Test case 3\n        (\n            [0.0],            # mu_A\n            [1e-6],           # sigma_A^2\n            [0.0],            # mu_B\n            [1e-6],           # sigma_B^2\n            1e-6              # epsilon\n        ),\n        # Test case 4\n        (\n            [5.0, -3.0, 1.0], # mu_A\n            [100.0, 0.1, 10.0],# sigma_A^2\n            [2.0],            # mu_B\n            [50.0],           # sigma_B^2\n            1e-4              # epsilon\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_A, sigma_A_sq, mu_B, sigma_B_sq, epsilon = case\n\n        # --- Strategy S1: BN-before-concatenation ---\n        \n        # Means: Channels from A are unchanged, channels from B are centered to 0.\n        # M_max is the max absolute mean from the un-normalized channels in A.\n        if mu_A:\n            M_max_s1 = np.max(np.abs(np.array(mu_A)))\n        else:\n            M_max_s1 = 0.0\n            \n        # Variances: Channels from A are unchanged, channels from B are scaled.\n        post_vars_A_s1 = np.array(sigma_A_sq)\n        post_vars_B_s1 = np.array([s2 / (s2 + epsilon) for s2 in sigma_B_sq])\n        \n        all_post_vars_s1 = np.concatenate((post_vars_A_s1, post_vars_B_s1))\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s1.size > 0:\n            min_var_s1 = np.min(all_post_vars_s1)\n            max_var_s1 = np.max(all_post_vars_s1)\n            # Avoid division by zero, though problem constraints ensure min_var > 0.\n            R_var_s1 = max_var_s1 / min_var_s1 if min_var_s1 > 0 else float('inf')\n        else:\n            R_var_s1 = 1.0 # No variance heterogeneity if there are no channels.\n\n        # --- Strategy S2: BN-after-concatenation ---\n\n        # Means: All channels are normalized, so all means are 0.\n        M_max_s2 = 0.0\n        \n        # Variances: All channels are normalized.\n        all_pre_vars_s2 = np.array(sigma_A_sq + sigma_B_sq)\n        \n        all_post_vars_s2 = np.array([s2 / (s2 + epsilon) for s2 in all_pre_vars_s2])\n        \n        # R_var is the ratio of max to min variance across all channels.\n        if all_post_vars_s2.size > 0:\n            min_var_s2 = np.min(all_post_vars_s2)\n            max_var_s2 = np.max(all_post_vars_s2)\n            R_var_s2 = max_var_s2 / min_var_s2 if min_var_s2 > 0 else float('inf')\n        else:\n            R_var_s2 = 1.0\n            \n        result = [M_max_s1, R_var_s1, M_max_s2, R_var_s2]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() for a list already includes spaces, e.g., '[1.0, 2.0]'.\n    # Joining these with a comma results in '...,[...', which when wrapped\n    # in brackets becomes '[[...],[...]]', matching the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3114019"}]}