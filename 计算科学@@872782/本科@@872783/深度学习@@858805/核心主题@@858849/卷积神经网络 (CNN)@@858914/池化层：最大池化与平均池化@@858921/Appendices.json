{"hands_on_practices": [{"introduction": "在卷积神经网络中，填充（padding）是处理特征图边界的关键技术，但其选择并非无关紧要。本练习将通过一个精心设计的例子，让您亲手计算并比较“零填充”和“反射填充”对最大池化与平均池化结果的显著影响。通过这个过程，您将深化对池化层具体运算机制的理解。[@problem_id:3163871]", "problem": "给定一个一维（1D）特征图，它被特意构造成用于利用不同填充方案下的边界效应。考虑长度为 $n = 4$ 的输入向量 $x = [-100, -50, 3, 2]$。应用一个池化层，其核大小 $k = 3$，步幅 $s = 2$，并在两侧进行对称填充 $p = 1$。比较两种填充方案：\n\n- 零填充：对于任何在有效范围 $\\{0, 1, \\dots, n-1\\}$ 之外的索引 $j$，填充后的信号值定义为 $\\tilde{x}_j = 0$。\n- 反射填充（无边缘复制）：对于任何在有效范围之外的索引 $j$，并且给定 $p=1$，池化窗口中唯一可能出现的超出范围的索引是左侧的 $j=-1$ 和右侧的 $j=n$。定义反射值为 $\\tilde{x}_{-1} = x_1$ 和 $\\tilde{x}_{n} = x_{n-2}$。对于范围内的索引，$\\tilde{x}_j = x_j$。\n\n仅使用最大池化和平均池化的核心定义，在填充后的信号上应用大小为 $k$、步幅为 $s$ 的滑动窗口，以确定在零填充和反射填充下的池化输出。然后，判断对于这个对抗性输入，以下关于输出的陈述哪些是正确的：\n\nA. 在最大池化下，零填充的第一个输出大于反射填充的第一个输出。\n\nB. 在平均池化下，反射填充的第一个输出大于零填充的第一个输出。\n\nC. 对于两种池化类型，第二个输出在零填充和反射填充下是相同的。\n\nD. 第一个输出（零填充减去反射填充）的差值绝对值，对于平均池化比对于最大池化更大。\n\n选择所有适用的选项。在您的选择中不提供计算过程；根据池化和填充定义的原则性推理来做出您的选择。", "solution": "### 问题验证\n\n**步骤 1：提取已知条件**\n\n问题陈述提供了以下信息：\n- 输入向量：$x = [-100, -50, 3, 2]$\n- 输入向量长度：$n = 4$\n- 核大小：$k = 3$\n- 步幅：$s = 2$\n- 对称填充：$p = 1$\n- 零填充定义：对于任何在有效范围 $\\{0, 1, \\dots, n-1\\}$ 之外的索引 $j$，填充后的信号值定义为 $\\tilde{x}_j = 0$。\n- 反射填充定义：对于任何在有效范围之外的索引 $j$，且 $p=1$ 时，填充后的值定义为 $\\tilde{x}_{-1} = x_1$ 和 $\\tilde{x}_{n} = x_{n-2}$。对于范围内的索引，$\\tilde{x}_j = x_j$。\n- 问题：评估关于两种填充方案下池化输出比较的四个陈述（A, B, C, D）的正确性。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n根据既定标准对问题进行验证。\n- **科学依据：** 该问题在深度学习领域有充分的依据，特别是关于卷积神经网络的机制。最大池化、平均池化和填充是标准的、基本的操作。所描述的“反射填充”是一种特定的、非标准的变体，但其定义在数学上是精确和无歧义的（$\\tilde{x}_{-1} = x_1$, $\\tilde{x}_{n} = x_{n-2}$），这使其在问题范围内成为一个有效的构造。它不违反任何数学或科学定律。\n- **适定性：** 计算输出所需的所有参数都已提供：输入向量 $x$、其长度 $n$、核大小 $k$、步幅 $s$ 和填充大小 $p$。两种填充方案的规则都已明确定义。这种设置确保了可以通过直接计算确定唯一且稳定的解。\n- **客观性：** 该问题以客观的数学语言表述。它要求基于固定的定义来计算和比较量值。术语“对抗性输入”描述了输入的性质，但并未在所需的计算中引入主观性。\n\n**步骤 3：结论与行动**\n\n问题陈述是有效的。它是自洽的、科学合理的且适定的。可以进行解题推导。\n\n### 解题推导\n\n池化操作的输出大小由公式 $L_{out} = \\lfloor \\frac{n + 2p - k}{s} \\rfloor + 1$ 给出。\n代入给定值：\n$$L_{out} = \\left\\lfloor \\frac{4 + 2(1) - 3}{2} \\right\\rfloor + 1 = \\left\\lfloor \\frac{3}{2} \\right\\rfloor + 1 = 1 + 1 = 2$$\n因此，每种情况下的输出向量长度都为 $2$。将两个输出值表示为 $y_0$ 和 $y_1$。\n\n池化操作将一个大小为 $k=3$、步幅为 $s=2$ 的滑动窗口应用于填充后的输入。在填充为 $p=1$ 的情况下，填充后的信号有效索引范围从 $-1$ 到 $n-1+p = 4$。\n- 第一个窗口（用于 $y_0$）从索引 $-p = -1$ 开始，覆盖索引 $\\{-1, 0, 1\\}$。\n- 第二个窗口（用于 $y_1$）从索引 $-p+s = -1+2 = 1$ 开始，覆盖索引 $\\{1, 2, 3\\}$。\n\n输入向量为 $x = [x_0, x_1, x_2, x_3] = [-100, -50, 3, 2]$。\n\n#### 1. 零填充\n\n在零填充下，任何对原始输入范围 $\\{0, 1, 2, 3\\}$ 之外的访问都会得到值 $0$。\n\n- **第一个窗口（用于 $y_0$）：**\n该窗口覆盖填充后的值 $\\{\\tilde{x}_{-1}, \\tilde{x}_0, \\tilde{x}_1\\}$。使用零填充，这些值为 $\\{0, x_0, x_1\\} = \\{0, -100, -50\\}$。\n- 最大池化：$y_{0, \\text{zero}}^{\\text{max}} = \\max(0, -100, -50) = 0$。\n- 平均池化：$y_{0, \\text{zero}}^{\\text{avg}} = \\frac{0 + (-100) + (-50)}{3} = \\frac{-150}{3} = -50$。\n\n- **第二个窗口（用于 $y_1$）：**\n该窗口覆盖填充后的值 $\\{\\tilde{x}_1, \\tilde{x}_2, \\tilde{x}_3\\}$。这些索引都在原始输入的范围内，因此这些值为 $\\{x_1, x_2, x_3\\} = \\{-50, 3, 2\\}$。\n- 最大池化：$y_{1, \\text{zero}}^{\\text{max}} = \\max(-50, 3, 2) = 3$。\n- 平均池化：$y_{1, \\text{zero}}^{\\text{avg}} = \\frac{-50 + 3 + 2}{3} = \\frac{-45}{3} = -15$。\n\n#### 2. 反射填充\n\n在指定的反射填充下，$\\tilde{x}_{-1} = x_1$ 且 $\\tilde{x}_4 = x_2$。\n\n- **第一个窗口（用于 $y_0$）：**\n该窗口覆盖填充后的值 $\\{\\tilde{x}_{-1}, \\tilde{x}_0, \\tilde{x}_1\\}$。使用反射填充，这些值为 $\\{x_1, x_0, x_1\\} = \\{-50, -100, -50\\}$。\n- 最大池化：$y_{0, \\text{reflect}}^{\\text{max}} = \\max(-50, -100, -50) = -50$。\n- 平均池化：$y_{0, \\text{reflect}}^{\\text{avg}} = \\frac{-50 + (-100) + (-50)}{3} = \\frac{-200}{3}$。\n\n- **第二个窗口（用于 $y_1$）：**\n该窗口覆盖值 $\\{\\tilde{x}_1, \\tilde{x}_2, \\tilde{x}_3\\}$。这些索引都在原始输入的范围内，因此这些值为 $\\{x_1, x_2, x_3\\} = \\{-50, 3, 2\\}$。填充方案对这个窗口没有影响。\n- 最大池化：$y_{1, \\text{reflect}}^{\\text{max}} = \\max(-50, 3, 2) = 3$。\n- 平均池化：$y_{1, \\text{reflect}}^{\\text{avg}} = \\frac{-50 + 3 + 2}{3} = \\frac{-45}{3} = -15$。\n\n### 逐项分析\n\n**A. 在最大池化下，零填充的第一个输出大于反射填充的第一个输出。**\n- 根据我们的计算，零填充下的第一个最大池化输出为 $y_{0, \\text{zero}}^{\\text{max}} = 0$。\n- 反射填充下的第一个最大池化输出为 $y_{0, \\text{reflect}}^{\\text{max}} = -50$。\n- 比较两个值：$0 > -50$。\n- 该陈述是**正确的**。\n\n**B. 在平均池化下，反射填充的第一个输出大于零填充的第一个输出。**\n- 根据我们的计算，零填充下的第一个平均池化输出为 $y_{0, \\text{zero}}^{\\text{avg}} = -50$。\n- 反射填充下的第一个平均池化输出为 $y_{0, \\text{reflect}}^{\\text{avg}} = -\\frac{200}{3} \\approx -66.67$。\n- 比较两个值：$-50 > -\\frac{200}{3}$。因此，零填充下的输出更大。\n- 该陈述声称的恰恰相反。\n- 该陈述是**不正确的**。\n\n**C. 对于两种池化类型，第二个输出在零填充和反射填充下是相同的。**\n- 第二个池化窗口覆盖索引 $\\{1, 2, 3\\}$。这些索引对应于原始输入向量的元素 $x_1, x_2, x_3$。\n- 由于这个窗口不包含任何填充元素，其内容与填充方案无关。\n- 对于零填充和反射填充，该窗口的值均为 $\\{x_1, x_2, x_3\\} = \\{-50, 3, 2\\}$。\n- 因此，对这组相同的值应用任何确定性函数（如最大或平均池化），都必须产生相同的结果。\n- 我们的计算证实了这一点：\n  - $y_{1, \\text{zero}}^{\\text{max}} = 3$ 且 $y_{1, \\text{reflect}}^{\\text{max}} = 3$。\n  - $y_{1, \\text{zero}}^{\\text{avg}} = -15$ 且 $y_{1, \\text{reflect}}^{\\text{avg}} = -15$。\n- 该陈述是**正确的**。\n\n**D. 第一个输出（零填充减去反射填充）的差值绝对值，对于平均池化比对于最大池化更大。**\n- 设最大池化的差值为 $\\Delta_{\\text{max}} = y_{0, \\text{zero}}^{\\text{max}} - y_{0, \\text{reflect}}^{\\text{max}} = 0 - (-50) = 50$。其绝对值为 $|\\Delta_{\\text{max}}| = 50$。\n- 设平均池化的差值为 $\\Delta_{\\text{avg}} = y_{0, \\text{zero}}^{\\text{avg}} - y_{0, \\text{reflect}}^{\\text{avg}} = -50 - (-\\frac{200}{3}) = -50 + \\frac{200}{3} = \\frac{-150 + 200}{3} = \\frac{50}{3}$。其绝对值为 $|\\Delta_{\\text{avg}}| = \\frac{50}{3}$。\n- 比较绝对值：$|\\Delta_{\\text{max}}| = 50$ 且 $|\\Delta_{\\text{avg}}| = \\frac{50}{3} \\approx 16.67$。\n- 显然，$50 > \\frac{50}{3}$。差值的绝对值对于最大池化更大。\n- 该陈述声称的恰恰相反。\n- 该陈述是**不正确的**。", "answer": "$$\\boxed{AC}$$", "id": "3163871"}, {"introduction": "模型对输入的微小扰动（或“对抗性攻击”）的稳健性是深度学习中的一个核心议题。最大池化和平均池化哪一个更“稳定”？这个问题的答案可能比直觉更微妙。本练习引导您在数学上严格推导，在特定扰动约束（$\\ell_{\\infty}$范数）下，这两种池化操作的输出在最坏情况下的变化，从而精确地量化它们的局部敏感性。[@problem_id:3163869]", "problem": "考虑一个由向量 $x \\in \\mathbb{R}^{n}$ 表示的单一池化窗口，其中 $n \\in \\mathbb{N}$。将此窗口上的最大池化算子定义为 $f_{\\max}(x) = \\max_{1 \\leq i \\leq n} x_{i}$，平均池化算子定义为 $f_{\\mathrm{avg}}(x) = \\frac{1}{n}\\sum_{i=1}^{n} x_{i}$。允许一个对抗方对该窗口添加扰动 $\\eta \\in \\mathbb{R}^{n}$，从而产生 $x + \\eta$，该扰动受预算约束 $\\|\\eta\\|_{\\infty} \\leq \\epsilon$ 的限制，其中 $\\epsilon > 0$ 且 $\\|\\eta\\|_{\\infty} = \\max_{1 \\leq i \\leq n} | \\eta_{i} |$。仅从这些定义出发，推导出每个算子的池化值的确切最坏情况绝对变化，即计算\n$$\\Delta_{\\max} = \\sup_{\\|\\eta\\|_{\\infty} \\leq \\epsilon} \\left| f_{\\max}(x + \\eta) - f_{\\max}(x) \\right| \\quad \\text{和} \\quad \\Delta_{\\mathrm{avg}} = \\sup_{\\|\\eta\\|_{\\infty} \\leq \\epsilon} \\left| f_{\\mathrm{avg}}(x + \\eta) - f_{\\mathrm{avg}}(x) \\right|.$$\n将您的最终答案表示为一个包含 $\\Delta_{\\max}$ 和 $\\Delta_{\\mathrm{avg}}$ 的单行向量，并以 $\\epsilon$ 和 $n$ 的闭式解函数形式表示。无需进行数值四舍五入。最终答案中不包含单位。", "solution": "我们从池化算子的定义和 $\\ell_{\\infty}$ 范数约束开始。目标是对于一个固定但任意的 $x \\in \\mathbb{R}^{n}$，计算\n$$\\Delta_{\\max} = \\sup_{\\|\\eta\\|_{\\infty} \\leq \\epsilon} \\left| \\max_{1 \\leq i \\leq n} (x_{i} + \\eta_{i}) - \\max_{1 \\leq i \\leq n} x_{i} \\right|,$$\n和\n$$\\Delta_{\\mathrm{avg}} = \\sup_{\\|\\eta\\|_{\\infty} \\leq \\epsilon} \\left| \\frac{1}{n} \\sum_{i=1}^{n} (x_{i} + \\eta_{i}) - \\frac{1}{n} \\sum_{i=1}^{n} x_{i} \\right| = \\sup_{\\|\\eta\\|_{\\infty} \\leq \\epsilon} \\left| \\frac{1}{n} \\sum_{i=1}^{n} \\eta_{i} \\right|.$$\n\n最大池化情况。令 $m = \\max_{1 \\leq i \\leq n} x_{i}$。对于任何满足 $\\|\\eta\\|_{\\infty} \\leq \\epsilon$ 的 $\\eta$，我们对每个 $i$ 都有 $x_{i} + \\eta_{i} \\leq x_{i} + \\epsilon$。因此，\n$$\\max_{1 \\leq i \\leq n} (x_{i} + \\eta_{i}) \\leq \\max_{1 \\leq i \\leq n} (x_{i} + \\epsilon) = \\left( \\max_{1 \\leq i \\leq n} x_{i} \\right) + \\epsilon = m + \\epsilon.$$\n类似地，由于对每个 $i$ 都有 $x_{i} + \\eta_{i} \\geq x_{i} - \\epsilon$，\n$$\\max_{1 \\leq i \\leq n} (x_{i} + \\eta_{i}) \\geq \\max_{1 \\leq i \\leq n} (x_{i} - \\epsilon) = m - \\epsilon.$$\n结合这些界限可得\n$$| \\max_{1 \\leq i \\leq n} (x_{i} + \\eta_{i}) - m | \\leq \\epsilon.$$\n因此 $\\Delta_{\\max} \\leq \\epsilon$。为了证明其紧致性，我们构造 $\\eta$，令所有 $i \\in \\{1,\\dots,n\\}$ 都有 $\\eta_{i} = \\epsilon$。那么 $\\|\\eta\\|_{\\infty} = \\epsilon$ 且\n$$\\max_{1 \\leq i \\leq n} (x_{i} + \\eta_{i}) = \\max_{1 \\leq i \\leq n} (x_{i} + \\epsilon) = m + \\epsilon,$$\n这就实现了一个为 $\\epsilon$ 的绝对变化。同样地，对所有 $i$ 令 $\\eta_{i} = -\\epsilon$ 会实现一个 $\\epsilon$ 的减少。因此，\n$$\\Delta_{\\max} = \\epsilon.$$\n\n平均池化情况。我们有\n$$\\Delta_{\\mathrm{avg}} = \\sup_{\\|\\eta\\|_{\\infty} \\leq \\epsilon} \\left| \\frac{1}{n} \\sum_{i=1}^{n} \\eta_{i} \\right|.$$\n根据三角不等式以及对所有 $i$ 的约束 $|\\eta_{i}| \\leq \\epsilon$，\n$$\\left| \\frac{1}{n} \\sum_{i=1}^{n} \\eta_{i} \\right| \\leq \\frac{1}{n} \\sum_{i=1}^{n} |\\eta_{i}| \\leq \\frac{1}{n} \\sum_{i=1}^{n} \\epsilon = \\epsilon.$$\n因此 $\\Delta_{\\mathrm{avg}} \\leq \\epsilon$。这个界是紧致的：选择对所有 $i$ 都有 $\\eta_{i} = \\epsilon$，这满足 $\\|\\eta\\|_{\\infty} = \\epsilon$ 并得出\n$$\\left| \\frac{1}{n} \\sum_{i=1}^{n} \\eta_{i} \\right| = \\left| \\frac{1}{n} \\cdot n \\epsilon \\right| = \\epsilon.$$\n类似地，选择对所有 $i$ 都有 $\\eta_{i} = -\\epsilon$ 会得到一个 $\\epsilon$ 的减少。因此，\n$$\\Delta_{\\mathrm{avg}} = \\epsilon.$$\n\n结论。对于任何 $x \\in \\mathbb{R}^{n}$ 和任何 $\\epsilon > 0$，在半径为 $\\epsilon$ 的 $\\ell_{\\infty}$ 有界扰动下，最大池化和平均池化特征的最坏情况绝对变化都等于 $\\epsilon$。因此，所要求的行向量是\n$$\\begin{pmatrix} \\Delta_{\\max}  \\Delta_{\\mathrm{avg}} \\end{pmatrix} = \\begin{pmatrix} \\epsilon  \\epsilon \\end{pmatrix}.$$", "answer": "$$\\boxed{\\begin{pmatrix}\\epsilon  \\epsilon\\end{pmatrix}}$$", "id": "3163869"}, {"introduction": "最大池化和平均池化看似是两种截然不同的操作，但它们实际上可以被一个更通用的数学框架所统一。本练习将介绍 Log-Sum-Exp (LSE) 函数，它是一个平滑、可微的函数，可以视为最大值函数的一种近似。通过分析它在不同“温度”参数 $\\tau$ 下的极限行为，我们将揭示最大池化和平均池化是如何作为其特例而出现的，从而为您提供一个更深刻、更统一的理论视角。[@problem_id:3163845]", "problem": "考虑一个分量为 $\\{x_{i}\\}_{i=1}^{n}$ 的向量 $x \\in \\mathbb{R}^{n}$，定义 Log-Sum-Exp (LSE) 池化为\n$$\nP_{\\tau}(x) \\;=\\; \\tau \\,\\log\\!\\left(\\sum_{i=1}^{n} \\exp\\!\\left(\\frac{x_{i}}{\\tau}\\right)\\right),\n$$\n对于任意温度参数 $\\tau > 0$。最大池化输出 $\\max_{i} x_{i}$，平均池化输出 $\\frac{1}{n}\\sum_{i=1}^{n} x_{i}$。仅使用指数函数和对数函数的定义、它们的基本代数性质（例如 $\\log(ab)=\\log a + \\log b$）以及针对大自变量和小自变量的标准一阶渐近（例如，当 $u \\to 0$ 时 $\\exp(u) = 1 + u + o(u)$ 且当 $u \\to 0$ 时 $\\log(1+u) = u + o(u)$），完成以下任务：\n\n1. 从 $P_{\\tau}(x)$ 的定义出发，推导当 $\\tau \\to 0$ 时 $P_{\\tau}(x)$ 的极限，并解释为什么它与最大池化一致。你的推导必须合理，不能引用任何未经证明的快捷恒等式。\n\n2. 通过计算当 $\\tau \\to \\infty$ 时 $P_{\\tau}(x) - \\tau \\log n$ 的极限，证明当 $\\tau \\to \\infty$ 时，$P_{\\tau}(x)$ 在相差一个加性缩放项的意义上趋近于平均池化。你的论证必须从给定的定义和经过检验的渐近展开开始。\n\n3. 对于固定的索引 $j \\in \\{1,\\dots,n\\}$，从 $P_{\\tau}(x)$ 的给定定义和链式法则出发，推导 $P_{\\tau}(x)$ 关于单个分量 $x_{j}$ 的梯度。将结果以 $x$ 和 $\\tau$ 的闭式函数形式表示。\n\n以解析表达式的形式报告这三个所要求的结果。无需四舍五入。最终答案必须只包含这三个表达式。", "solution": "问题陈述经确认为科学上合理、定义明确且客观。所有必要的定义和条件均已提供，不存在矛盾或歧义。这些任务是微积分和极限理论中的标准练习，应用于机器学习中的一个相关函数。我们将开始求解。\n\nLog-Sum-Exp 函数定义为 $P_{\\tau}(x) = \\tau \\log\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)$。在微积分中，函数 $\\log$ 被解释为自然对数，我们记为 $\\ln$。\n\n1. 当 $\\tau \\to 0^+$ 时的极限推导（最大池化）\n\n令 $x_{\\max} = \\max_{i \\in \\{1, \\dots, n\\}} x_i$。我们分析当 $\\tau \\to 0^+$ 时 $P_{\\tau}(x)$ 的极限。为了处理自变量趋于无穷大的指数项，我们提出对应于最大值的项 $\\exp(x_{\\max}/\\tau)$。\n\n$$\nP_{\\tau}(x) = \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\n\n我们通过提出 $\\exp(x_{\\max}/\\tau)$ 来重写求和式：\n$$\nP_{\\tau}(x) = \\tau \\ln\\left( \\exp\\left(\\frac{x_{\\max}}{\\tau}\\right) \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right) \\right)\n$$\n\n使用性质 $\\ln(ab) = \\ln(a) + \\ln(b)$，我们得到：\n$$\nP_{\\tau}(x) = \\tau \\left[ \\ln\\left(\\exp\\left(\\frac{x_{\\max}}{\\tau}\\right)\\right) + \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)\\right) \\right]\n$$\n\n由于 $\\ln(\\exp(u)) = u$，上式简化为：\n$$\nP_{\\tau}(x) = \\tau \\left[ \\frac{x_{\\max}}{\\tau} + \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)\\right) \\right]\n$$\n$$\nP_{\\tau}(x) = x_{\\max} + \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)\\right)\n$$\n\n现在，我们考察当 $\\tau \\to 0^+$ 时对数函数自变量中的求和项。对于每一项 $\\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right)$，其指数的分子 $(x_i - x_{\\max})$ 是非正的。\n- 如果 $x_i  x_{\\max}$，则 $x_i - x_{\\max}  0$。当 $\\tau \\to 0^+$ 时，自变量 $\\frac{x_i - x_{\\max}}{\\tau} \\to -\\infty$，因此 $\\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right) \\to 0$。\n- 如果 $x_i = x_{\\max}$，则 $x_i - x_{\\max} = 0$。自变量为 $\\frac{0}{\\tau} = 0$，且 $\\exp(0) = 1$。\n\n令 $k$ 为满足 $x_i = x_{\\max}$ 的索引 $i$ 的数量。那么，当 $\\tau \\to 0^+$ 时，该求和项趋近于这些最大元素的数量：\n$$\n\\lim_{\\tau \\to 0^+} \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i} - x_{\\max}}{\\tau}\\right) = k\n$$\n其中 $k \\ge 1$。将此代入我们关于 $P_{\\tau}(x)$ 的表达式：\n$$\n\\lim_{\\tau \\to 0^+} P_{\\tau}(x) = \\lim_{\\tau \\to 0^+} \\left[ x_{\\max} + \\tau \\ln(k) \\right]\n$$\n当 $\\tau \\to 0^+$ 时，项 $\\tau \\ln(k)$ 趋近于 $0$，因为 $\\ln(k)$ 是一个常数。\n$$\n\\lim_{\\tau \\to 0^+} P_{\\tau}(x) = x_{\\max} + 0 = x_{\\max}\n$$\n这个结果 $x_{\\max} = \\max_{i} x_i$ 是最大池化的定义。\n\n2. 当 $\\tau \\to \\infty$ 时的极限推导（平均池化）\n\n我们被要求计算当 $\\tau \\to \\infty$ 时 $P_{\\tau}(x) - \\tau \\ln n$ 的极限。\n$$\nP_{\\tau}(x) - \\tau \\ln n = \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right) - \\tau \\ln n\n$$\n使用性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们合并这些项：\n$$\nP_{\\tau}(x) - \\tau \\ln n = \\tau \\ln\\left(\\frac{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)}{n}\\right) = \\tau \\ln\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\n当 $\\tau \\to \\infty$ 时，每个指数函数的自变量 $u_i = x_i/\\tau$ 趋近于 $0$。因此，我们可以使用指定的当 $u \\to 0$ 时的一阶渐近展开 $\\exp(u) = 1 + u + o(u)$。\n$$\n\\exp\\left(\\frac{x_{i}}{\\tau}\\right) = 1 + \\frac{x_i}{\\tau} + o\\left(\\frac{1}{\\tau}\\right)\n$$\n将此代入对数函数内的求和式：\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} \\left(1 + \\frac{x_i}{\\tau} + o\\left(\\frac{1}{\\tau}\\right)\\right) = \\frac{1}{n} \\left(\\sum_{i=1}^{n} 1 + \\frac{1}{\\tau}\\sum_{i=1}^{n} x_i + \\sum_{i=1}^{n} o\\left(\\frac{1}{\\tau}\\right)\\right)\n$$\n$$\n= \\frac{1}{n} \\left(n + \\frac{1}{\\tau}\\sum_{i=1}^{n} x_i + n \\cdot o\\left(\\frac{1}{\\tau}\\right)\\right) = 1 + \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\n$$\n现在，将此代回完整的表达式。令 $v = \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)$。当 $\\tau \\to \\infty$ 时，$v \\to 0$。我们可以使用展开式 $\\ln(1+v) = v + o(v)$。\n$$\n\\lim_{\\tau \\to \\infty} \\left[ P_{\\tau}(x) - \\tau \\ln n \\right] = \\lim_{\\tau \\to \\infty} \\tau \\ln\\left(1 + \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\\right)\n$$\n$$\n= \\lim_{\\tau \\to \\infty} \\tau \\left[ \\left(\\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\\right) + o\\left(\\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)\\right) \\right]\n$$\n方括号内的项简化为 $\\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right)$。\n$$\n= \\lim_{\\tau \\to \\infty} \\tau \\left[ \\frac{1}{n\\tau}\\sum_{i=1}^{n} x_i + o\\left(\\frac{1}{\\tau}\\right) \\right] = \\lim_{\\tau \\to \\infty} \\left[ \\frac{1}{n}\\sum_{i=1}^{n} x_i + \\tau \\cdot o\\left(\\frac{1}{\\tau}\\right) \\right]\n$$\n项 $\\tau \\cdot o(1/\\tau)$ 表示一个比 $1/\\tau$ 更快趋于零的量，因此乘以 $\\tau$ 仍然得到一个当 $\\tau \\to \\infty$ 时消失的项。例如，如果 $o(1/\\tau)$ 是 $1/\\tau^2$ 的阶，则 $\\tau(1/\\tau^2) = 1/\\tau \\to 0$。因此，$\\lim_{\\tau \\to \\infty} \\tau \\cdot o(1/\\tau) = 0$。\n因此，极限为：\n$$\n\\lim_{\\tau \\to \\infty} \\left[ P_{\\tau}(x) - \\tau \\ln n \\right] = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n$$\n这是平均池化的定义。因此，对于大的 $\\tau$，$P_{\\tau}(x)$ 近似于平均池化，两者相差一个加性项 $\\tau \\ln n$。\n\n3. 梯度 $\\frac{\\partial P_{\\tau}(x)}{\\partial x_j}$ 的推导\n\n我们计算 $P_{\\tau}(x)$ 关于分量 $x_j$ 的偏导数，其中 $j$ 是一个固定的索引， $j \\in \\{1, \\dots, n\\}$。我们使用链式法则。\n$$\nP_{\\tau}(x) = \\tau \\ln\\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\n令 $u(x) = \\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)$。那么 $P_{\\tau}(x) = \\tau \\ln(u(x))$。\n根据链式法则，$\\frac{\\partial P_{\\tau}(x)}{\\partial x_j} = \\frac{d}{du}(\\tau \\ln u) \\cdot \\frac{\\partial u}{\\partial x_j}$。\n\n首先，我们求外函数关于其自变量 $u$ 的导数：\n$$\n\\frac{d}{du}(\\tau \\ln u) = \\frac{\\tau}{u} = \\frac{\\tau}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)}\n$$\n接下来，我们求内函数 $u(x)$ 关于 $x_j$ 的偏导数：\n$$\n\\frac{\\partial u}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left(\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)\\right)\n$$\n和的导数是导数的和。对于任何索引 $i \\neq j$ 的项，$\\frac{\\partial}{\\partial x_j}\\exp\\left(\\frac{x_{i}}{\\tau}\\right) = 0$。求和式中唯一的非零项是当 $i=j$ 时。\n$$\n\\frac{\\partial u}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\exp\\left(\\frac{x_{j}}{\\tau}\\right) = \\exp\\left(\\frac{x_{j}}{\\tau}\\right) \\cdot \\frac{1}{\\tau}\n$$\n结合这些结果：\n$$\n\\frac{\\partial P_{\\tau}(x)}{\\partial x_j} = \\left( \\frac{\\tau}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)} \\right) \\cdot \\left( \\exp\\left(\\frac{x_{j}}{\\tau}\\right) \\cdot \\frac{1}{\\tau} \\right)\n$$\n分子中的因子 $\\tau$ 与因子 $1/\\tau$ 相抵消。\n$$\n\\frac{\\partial P_{\\tau}(x)}{\\partial x_j} = \\frac{\\exp\\left(\\frac{x_{j}}{\\tau}\\right)}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_{i}}{\\tau}\\right)}\n$$\n这个表达式是应用于向量 $x/\\tau$ 的 softmax 函数。\n所要求的三个结果是：\n1. $\\max_{i} x_i$\n2. $\\frac{1}{n} \\sum_{i=1}^{n} x_i$\n3. $\\frac{\\exp(x_j/\\tau)}{\\sum_{i=1}^{n} \\exp(x_i/\\tau)}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\max_{i} x_i \\\\\n\\\\\n\\frac{1}{n}\\sum_{i=1}^{n} x_{i} \\\\\n\\\\\n\\frac{\\exp\\left(\\frac{x_j}{\\tau}\\right)}{\\sum_{i=1}^{n} \\exp\\left(\\frac{x_i}{\\tau}\\right)}\n\\end{pmatrix}\n}\n$$", "id": "3163845"}]}