{"hands_on_practices": [{"introduction": "Inception 模块的核心思想是同时在多个空间尺度上处理信息，但为什么要这样做呢？本练习通过一个信号处理的类比来探讨这种设计的根本优势。我们将构建一个低光照图像的假设场景，量化不同尺寸的卷积核对噪声的抑制效果，从而为多分支设计捕捉多尺度特征的有效性提供一个具体而直观的理解 [@problem_id:3130750]。", "problem": "您将实现并分析一个简化的类 Inception 多分支模块，以研究在白噪声模型下，仅使用第一性原理推导，空间核尺寸如何通过固有平均影响去噪效果。考虑一个单通道图像和一个类 Inception 模块，该模块具有多个并行分支，每个分支使用大小为 $k \\times k$、步长为 $1$ 的方形核进行单次卷积。每个分支使用一个归一化为单位和的均匀平均核，该核模拟了 Inception 分支中较大感受野的空间聚合行为。请仅使用以下基础原理：卷积的线性性质、卷积作为加权和的定义、信噪比 (SNR) 定义为信号功率除以噪声功率，以及独立同分布的零均值高斯噪声的性质。\n\n任务。给定一张干净图像 $S$ 和加性噪声 $N$，其中 $N$ 是独立同分布的零均值高斯噪声，方差为 $\\sigma^2$。在指定的空间支持域 $\\Omega$ 上，输入信噪比定义为\n$$\n\\mathrm{SNR_{in}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}.\n$$\n对于一个核为 $h_k$（大小为 $k \\times k$）的分支，将输出信号定义为 $S_{\\mathrm{out}} = h_k * S$，输出噪声定义为 $N_{\\mathrm{out}} = h_k * N$。输出信噪比为\n$$\n\\mathrm{SNR_{out}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}.\n$$\n这里 $*$ 表示卷积。为避免边缘效应并确保在每个输出位置应用相同的滤波器，您必须使用有效卷积（无填充），使得输出支持域 $\\Omega_k$ 是所有 $k \\times k$ 核完全包含在输入中的位置集合。对于每个分支，信噪比改善因子定义为比率\n$$\nG_k = \\frac{\\mathrm{SNR_{out}}}{\\mathrm{SNR_{in}}}.\n$$\n您必须通过在独立同分布高斯假设下，从第一性原理推导滤波后的噪声功率，来计算每个测试用例中每个核尺寸的 $G_k$。不要使用对多个噪声实现的经验平均；相反，应使用独立变量线性组合的性质来获得滤波后的噪声功率。所有卷积必须以步长为 $1$ 和有效模式实现。\n\n低光数据集模型。根据测试用例描述确定性地构建干净图像 $S$。通过选择较小的正振幅 $A$ 和相对较大的噪声标准差 $\\sigma$ 来模拟低光照。噪声分布仅用于根据推导要求符号化地确定噪声功率；您不进行噪声采样。\n\n滤波器。对于给定的核尺寸 $k$，分支滤波器是一个均匀平均核，其系数在 $k \\times k$ 窗口内对所有 $u,v$ 均为 $h(u,v)=1/k^2$。\n\n要求输出。对于每个测试用例，报告一个信噪比改善因子列表 $G_k$，针对指定的核尺寸 $k$，顺序与测试用例中列出的顺序相同。将每个报告的因子四舍五入到 $6$ 位小数。\n\n测试套件。精确实现以下测试用例：\n- 测试用例 1：图像尺寸 $H=W=32$，干净信号类型：常数，振幅 $A=0.1$，噪声标准差 $\\sigma=0.2$，核尺寸 $k \\in \\{1,3,5\\}$。\n- 测试用例 2：图像尺寸 $H=W=32$，干净信号类型：棋盘格，像素值在 $\\{0,A\\}$ 中交替，振幅 $A=0.1$，噪声标准差 $\\sigma=0.2$，核尺寸 $k \\in \\{1,3,5\\}$。\n- 测试用例 3：图像尺寸 $H=W=32$，干净信号类型：常数，振幅 $A=0.05$，噪声标准差 $\\sigma=0.3$，核尺寸 $k \\in \\{1,5,7\\}$。\n- 测试用例 4：图像尺寸 $H=W=7$，干净信号类型：常数，振幅 $A=0.1$，噪声标准差 $\\sigma=0.2$，核尺寸 $k \\in \\{1,7\\}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含所有测试用例的结果，格式为列表的列表。每个内部列表必须包含该测试用例的信噪比改善因子 $G_k$，顺序与为该测试用例给出的核尺寸顺序一致。所有数字必须四舍五入到 $6$ 位小数。该行必须格式化为类似 JSON 的无空格列表的列表，例如使用符号占位符：$[[g_{1,1},g_{1,2},\\dots],[g_{2,1},\\dots],\\dots]$。", "solution": "用户问题被判定为有效，因为它具有科学依据、问题明确、客观，并包含唯一解所需的所有必要信息。任务是为一个简化的类 Inception 模块分支推导并计算信噪比 (SNR) 改善因子 $G_k$，该分支使用一个大小为 $k \\times k$ 的均匀平均核。\n\n推导过程按规定从第一性原理出发。信噪比改善因子 $G_k$ 定义为输出信噪比与输入信噪比之比：\n$$\nG_k = \\frac{\\mathrm{SNR_{out}}}{\\mathrm{SNR_{in}}}\n$$\n输入和输出信噪比定义为信号功率与噪声功率之比。\n$$\n\\mathrm{SNR_{in}} = \\frac{P_{\\mathrm{S, in}}}{P_{\\mathrm{N, in}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right]}{\\mathbb{E}\\left[N(i,j)^2\\right]}\n$$\n$$\n\\mathrm{SNR_{out}} = \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{N, out}}} = \\frac{\\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right]}{\\mathbb{E}\\left[N_{\\mathrm{out}}(i,j)^2\\right]}\n$$\n此处，$S$ 和 $N$ 是输入信号和噪声，而 $S_{\\mathrm{out}}$ 和 $N_{\\mathrm{out}}$ 是与核 $h_k$ 卷积后的信号和噪声。符号 $\\mathbb{E}_{(i,j)\\in\\Omega}[\\cdot]$ 表示在支持域 $\\Omega$ 上的空间平均，而 $\\mathbb{E}[\\cdot]$ 表示对噪声分布的统计期望。\n\n将这些定义代入 $G_k$ 的表达式中，得到：\n$$\nG_k = \\frac{P_{\\mathrm{S, out}} / P_{\\mathrm{N, out}}}{P_{\\mathrm{S, in}} / P_{\\mathrm{N, in}}} = \\left(\\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\\right) \\left(\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}}\\right)\n$$\n我们分别分析噪声功率比和信号功率比。\n\n首先，我们分析噪声功率比。输入噪声 $N(i,j)$ 是独立同分布 (i.i.d.) 的，均值为零，方差为 $\\sigma^2$。因此，任何像素点的输入噪声功率为：\n$$\nP_{\\mathrm{N, in}} = \\mathbb{E}[N(i,j)^2] = \\mathrm{Var}(N(i,j)) + (\\mathbb{E}[N(i,j)])^2 = \\sigma^2 + 0^2 = \\sigma^2\n$$\n输出噪声 $N_{\\mathrm{out}}$ 是输入噪声 $N$ 与均匀平均核 $h_k$（其中 $h_k(u,v) = 1/k^2$）卷积的结果。在输出位置 $(i,j)$，输出噪声是输入噪声样本的线性组合：\n$$\nN_{\\mathrm{out}}(i,j) = (h_k * N)(i,j) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\n$$\n由于期望的线性性质和 $\\mathbb{E}[N]=0$，输出噪声的期望为零：\n$$\n\\mathbb{E}[N_{\\mathrm{out}}(i,j)] = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} \\mathbb{E}[N(i+u, j+v)] = 0\n$$\n输出噪声功率是其方差。由于输入噪声样本 $N(i,j)$ 是独立的，它们的加权和的方差是各项方差之和：\n$$\nP_{\\mathrm{N, out}} = \\mathbb{E}[N_{\\mathrm{out}}(i,j)^2] = \\mathrm{Var}(N_{\\mathrm{out}}(i,j)) = \\mathrm{Var}\\left(\\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\frac{1}{k^2} N(i+u, j+v)\\right)\n$$\n$$\nP_{\\mathrm{N, out}} = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\mathrm{Var}\\left(\\frac{1}{k^2} N(i+u, j+v)\\right) = \\sum_{u=0}^{k-1} \\sum_{v=0}^{k-1} \\left(\\frac{1}{k^2}\\right)^2 \\mathrm{Var}(N(i+u, j+v))\n$$\n由于 $k^2$ 个噪声样本中的每一个都具有方差 $\\sigma^2$：\n$$\nP_{\\mathrm{N, out}} = k^2 \\cdot \\frac{1}{k^4} \\sigma^2 = \\frac{\\sigma^2}{k^2}\n$$\n因此，噪声功率比为：\n$$\n\\frac{P_{\\mathrm{N, in}}}{P_{\\mathrm{N, out}}} = \\frac{\\sigma^2}{\\sigma^2 / k^2} = k^2\n$$\n这证明了通过平均来降低噪声的基本原理：对 $k^2$ 个独立同分布的噪声样本进行平均，可将噪声功率降低 $k^2$ 倍。\n\n现在，我们考虑信号功率比。这是通过在各自的域上进行空间平均来计算的。对于一个大小为 $H \\times W$ 的图像，输入信号功率为：\n$$\nP_{\\mathrm{S, in}} = \\mathbb{E}_{(i,j)\\in \\Omega}\\left[S(i,j)^2\\right] = \\frac{1}{HW} \\sum_{i=0}^{H-1} \\sum_{j=0}^{W-1} S(i,j)^2\n$$\n输出信号是 $S_{\\mathrm{out}} = h_k * S$。使用“有效”卷积，输出图像的维度为 $(H-k+1) \\times (W-k+1)$。输出信号功率为：\n$$\nP_{\\mathrm{S, out}} = \\mathbb{E}_{(i,j)\\in \\Omega_k}\\left[S_{\\mathrm{out}}(i,j)^2\\right] = \\frac{1}{(H-k+1)(W-k+1)} \\sum_{i=0}^{H-k} \\sum_{j=0}^{W-k} S_{\\mathrm{out}}(i,j)^2\n$$\n结合噪声和信号功率比，信噪比改善因子 $G_k$ 的最终表达式为：\n$$\nG_k = k^2 \\frac{P_{\\mathrm{S, out}}}{P_{\\mathrm{S, in}}}\n$$\n该公式被实现用于求解每个测试用例的 $G_k$。\n对于一个常数信号 $S(i,j)=A$，平均核产生一个常数输出 $S_{\\mathrm{out}}(i,j) = A$。因此，$P_{\\mathrm{S, out}} = A^2$ 且 $P_{\\mathrm{S, in}} = A^2$。信号功率之比为 $1$，所以 $G_k=k^2$。\n对于像棋盘格这样的高频信号，平均操作起到低通滤波器的作用，会衰减信号。这导致 $P_{\\mathrm{S, out}}  P_{\\mathrm{S, in}}$，因此 $G_k  k^2$。实现通过数值计算卷积和随后的空间平均来确定每种情况下的信号功率比。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_g_factors(H, W, signal_type, A, k_list):\n    \"\"\"\n    Computes the SNR improvement factor G_k for a list of kernel sizes.\n\n    Args:\n        H (int): Image height.\n        W (int): Image width.\n        signal_type (str): Type of clean signal ('constant' or 'checkerboard').\n        A (float): Amplitude of the signal.\n        k_list (list): List of kernel sizes (k).\n\n    Returns:\n        list: A list of SNR improvement factors G_k, rounded to 6 decimal places.\n    \"\"\"\n    # 1. Construct the clean signal S\n    if signal_type == 'constant':\n        S = np.full((H, W), A, dtype=np.float64)\n    elif signal_type == 'checkerboard':\n        S = np.zeros((H, W), dtype=np.float64)\n        # Use broadcasting for efficient checkerboard generation\n        i_coords = np.arange(H).reshape(-1, 1)\n        j_coords = np.arange(W).reshape(1, -1)\n        mask = (i_coords + j_coords) % 2 == 0\n        S[mask] = A\n    else:\n        raise ValueError(\"Unknown signal type\")\n\n    # 2. Calculate input signal power P_s_in\n    # This is the mean of the squared signal values\n    P_s_in = np.mean(S**2)\n\n    g_factors = []\n    \n    # Check for zero-signal case to prevent division by zero, though not in test cases\n    if P_s_in == 0:\n        # If signal is zero, SNR is zero both in and out. The ratio is ill-defined.\n        # Based on the formula, P_s_out will also be 0, leading to 0/0.\n        # We can return a placeholder or handle as an error. For this problem, A  0.\n        # Arbitrarily returning k^2 as if signal power ratio is 1.\n        return [float(k**2) for k in k_list]\n\n    for k in k_list:\n        # 3. Calculate output signal S_out by 'valid' convolution\n        H_out = H - k + 1\n        W_out = W - k + 1\n        S_out = np.zeros((H_out, W_out), dtype=np.float64)\n        \n        # Explicitly implement convolution with a uniform averaging kernel\n        for i in range(H_out):\n            for j in range(W_out):\n                patch = S[i:i+k, j:j+k]\n                S_out[i, j] = np.mean(patch)\n\n        # 4. Calculate output signal power P_s_out\n        P_s_out = np.mean(S_out**2) if S_out.size  0 else 0.0\n\n        # 5. Calculate G_k using the derived formula\n        g_k = (k**2) * P_s_out / P_s_in\n        g_factors.append(g_k)\n\n    return [round(g, 6) for g in g_factors]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'checkerboard', 'A': 0.1, 'k_list': [1, 3, 5]},\n        {'H': 32, 'W': 32, 'signal_type': 'constant', 'A': 0.05, 'k_list': [1, 5, 7]},\n        {'H': 7, 'W': 7, 'signal_type': 'constant', 'A': 0.1, 'k_list': [1, 7]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = calculate_g_factors(case['H'], case['W'], case['signal_type'], case['A'], case['k_list'])\n        all_results.append(results)\n\n    # Format the final output as a single-line JSON-like list of lists\n    result_str = '[' + ','.join(\n        '[' + ','.join([f'{val:.6f}' for val in res]) + ']' for res in all_results\n    ) + ']'\n    print(result_str)\n\nsolve()\n\n```", "id": "3130750"}, {"introduction": "虽然在多个尺度上进行处理是有效的，但使用大尺寸卷积核（如 $5 \\times 5$ 或 $7 \\times 7$）会带来巨大的计算成本。本练习将深入探讨后期 Inception 版本中引入的一项关键优化技术：将大的方形卷积分解为更小的非对称卷积。我们将从第一性原理出发，推导其带来的计算量节省，并在一个假设的数据集上分析效率与模型准确性之间的权衡，这对于设计高效的深度网络至关重要 [@problem_id:3130734]。", "problem": "您正在研究 GoogLeNet (Inception) 架构中的计算效率技术，特别是将方形卷积分解为两个可分离的矩形卷积的方法。请使用深度学习中卷积成本的标准定义，从第一性原理推导出单个二维卷积以及分解后的两个卷积序列的精确浮点运算次数 (FLOPs)。然后，量化当用一个 $1 \\times K$ 卷积后跟一个 $K \\times 1$ 卷积来替换单个 $K \\times K$ 卷积时的计算节省量。最后，在一个抽象的 ImageNet-100 子集上，使用提供的标签和预测类别计算 top-1 准确率，以评估分解是否保持了准确率。\n\n使用的基础和定义：\n- 一个输入通道数为 $C_{\\text{in}}$、输出通道数为 $C_{\\text{out}}$、卷积核大小为 $K \\times K$、输出空间维度为 $H_{\\text{out}} \\times W_{\\text{out}}$ 的二维卷积，按照惯例，每次乘加运算（一次乘法和一次加法）执行 $2$ 次浮点运算。在此惯例下，单个 $K \\times K$ 卷积的 FLOPs 为\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2.\n$$\n- 对于一个由 $C_{\\text{in}} \\to C_{\\text{mid}}$ 的 $1 \\times K$ 卷积和其后跟随的 $C_{\\text{mid}} \\to C_{\\text{out}}$ 的 $K \\times 1$ 卷积组成的分解卷积对，其 FLOPs 为\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right).\n$$\n- FLOPs 节省分数定义为\n$$\nS \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}.\n$$\n- Top-1 准确率定义为预测的类别索引与真实标签相等的样本所占的比例，即：\n$$\n\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\},\n$$\n其中 $N$ 是样本数量，$y_i \\in \\{0,\\dots,C-1\\}$ 是真实标签，$\\hat{y}_i$ 是预测的类别索引。\n\n任务：\n1) 用代数方法证明，如果 $C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$ 且 $K = 7$，则分解后的卷积对所使用的 FLOPs 恰好是原始 $7 \\times 7$ 卷积的 $2/7$，因此 FLOPs 减少到原始值的 $2/7$。报告精确的节省分数 $S$。\n2) 针对以下三个测试用例，计算精确的 FLOPs 节省分数 $S$，假设步长为 $1$ 且输出空间维度如给定值。根据上述惯例计算 FLOPs，并将节省量表示为四舍五入到六位小数的小数。\n   - 测试 A (正常路径)：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$。\n   - 测试 B (边界情况)：$H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$。\n   - 测试 C (瓶颈分解)：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$。\n3) 使用一个包含 $N = 12$ 个样本和 $C = 100$ 个类别的抽象 ImageNet-100 子集，使用下面提供的标签和预测值，计算基线模型（单个 $K \\times K$ 卷积）和分解模型（一个 $1 \\times K$ 卷积后跟一个 $K \\times 1$ 卷积）的 top-1 准确率，并报告两种准确率及其差值（分解模型减去基线模型）。标签和预测的类别索引如下：\n   - 真实标签 $y$：$[5,12,23,23,11,65,99,0,3,42,77,13]$。\n   - 基线模型预测类别 $\\hat{y}^{\\text{base}}$：$[5,11,23,3,11,65,13,0,2,42,8,13]$。\n   - 分解模型预测类别 $\\hat{y}^{\\text{fact}}$：$[5,12,23,23,9,65,99,0,3,1,77,88]$。\n\n角度或物理单位不适用。所有要求的数值输出必须是四舍五入到六位小数的十进制数。\n\n程序要求：\n- 实现一个程序，计算：\n  - 测试 A、测试 B 和测试 C 的 FLOPs 节省分数 $S$。\n  - 在提供的 ImageNet-100 子集上，基线预测和分解预测的 top-1 准确率及其差值。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果按以下顺序排列：\n  $[S_{\\text{A}}, S_{\\text{B}}, S_{\\text{C}}, \\text{acc}_{\\text{base}}, \\text{acc}_{\\text{fact}}, \\Delta \\text{acc}]$,\n  其中 $\\Delta \\text{acc} = \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}}$。\n- 所有六个数字都必须四舍五入到六位小数。", "solution": "该问题要求分析将方形卷积分解为两个可分离的矩形卷积所带来的计算节省量和性能影响，这是 GoogLeNet (Inception) 架构中一项杰出的技术。我将首先验证问题陈述的有效性。\n\n### 问题验证\n\n**步骤 1：提取给定信息**\n- 原始卷积的 FLOPs：$F_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2$\n- 分解卷积的 FLOPs：$F_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)$\n- FLOPs 节省分数：$S \\;=\\; \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}}$\n- Top-1 准确率：$\\text{acc} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{1}\\{ \\hat{y}_i = y_i \\}$\n- 任务 1 条件：$C_{\\text{mid}} = C_{\\text{out}} = C_{\\text{in}}$, $K = 7$\n- 测试 A 参数：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$\n- 测试 B 参数：$H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$\n- 测试 C 参数：$H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$\n- 任务 3 数据：$N = 12$, $C = 100$\n    - $y$: $[5,12,23,23,11,65,99,0,3,42,77,13]$\n    - $\\hat{y}^{\\text{base}}$: $[5,11,23,3,11,65,13,0,2,42,8,13]$\n    - $\\hat{y}^{\\text{fact}}$: $[5,12,23,23,9,65,99,0,3,1,77,88]$\n\n**步骤 2：使用提取的给定信息进行验证**\n该问题在科学上基于深度学习原理和神经网络运算的计算复杂性分析。所有的定义和公式都是该领域的标准。问题提法明确，为每个任务提供了所有必要的数据和约束，以得出唯一的数值解。语言客观且正式。给定的参数对于卷积神经网络架构是切合实际的。该问题不违反任何无效性标准。\n\n**步骤 3：结论与行动**\n问题有效。将提供完整的解决方案。\n\n### 解题推导\n\n解题过程按顺序处理三个任务中的每一个。\n\n**任务 1：代数证明与节省分数**\n\n我们被要求证明，在 $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$ 且 $K=7$ 的特定情况下，分解卷积使用的 FLOPs 是原始卷积的 $2/7$，并求出节省分数 $S$。\n\n设 $C_{\\text{in}} = C_{\\text{mid}} = C_{\\text{out}} = C$。FLOPs 表达式变为：\n$$\nF_{\\text{orig}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C \\cdot C \\cdot K^2 \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2\n$$\n$$\nF_{\\text{fact}} \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C \\cdot C \\cdot K \\;+\\; C \\cdot C \\cdot K\\right) \\;=\\; 2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(2 \\cdot C^2 \\cdot K\\right)\n$$\n为求分解后 FLOPs 与原始 FLOPs 的比率，我们计算 $\\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$：\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot 2 \\cdot C^2 \\cdot K}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2 \\cdot K^2} \\;=\\; \\frac{2K}{K^2} \\;=\\; \\frac{2}{K}\n$$\n公共因子 $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C^2$ 被约去，这表明该比率与空间维度和通道数无关，只要它们相等即可。\n对于卷积核大小 $K=7$，该比率为：\n$$\n\\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; \\frac{2}{7}\n$$\n这证明了在这些条件下，分解后的卷积对使用的 FLOPs 恰好是原始 $7 \\times 7$ 卷积的 $2/7$。\n\n节省分数 $S$ 定义为 $S = \\frac{F_{\\text{orig}} - F_{\\text{fact}}}{F_{\\text{orig}}} = 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}}$。\n代入推导出的比率：\n$$\nS \\;=\\; 1 - \\frac{2}{K}\n$$\n对于 $K=7$，节省分数为：\n$$\nS \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7}\n$$\n精确的节省分数为 $5/7$。\n\n**任务 2：测试用例的 FLOPs 节省分数**\n\n为了计算给定测试用例的节省分数，我们首先推导 $S$ 的通用简化表达式。\n$$\nS \\;=\\; 1 - \\frac{F_{\\text{fact}}}{F_{\\text{orig}}} \\;=\\; 1 - \\frac{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot \\left(C_{\\text{in}} \\cdot C_{\\text{mid}} \\cdot K \\;+\\; C_{\\text{mid}} \\cdot C_{\\text{out}} \\cdot K\\right)}{2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}} \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2}\n$$\n项 $2 \\cdot H_{\\text{out}} \\cdot W_{\\text{out}}$ 被约去。我们还可以从分子中提出因子 $C_{\\text{mid}} \\cdot K$：\n$$\nS \\;=\\; 1 - \\frac{C_{\\text{mid}} \\cdot K \\cdot (C_{\\text{in}} + C_{\\text{out}})}{C_{\\text{in}} \\cdot C_{\\text{out}} \\cdot K^2} \\;=\\; 1 - \\frac{C_{\\text{mid}} (C_{\\text{in}} + C_{\\text{out}})}{K \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}}\n$$\n使用这个简化公式，我们为每个测试用例计算 $S$。\n\n- **测试 A：** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 64$, $C_{\\text{mid}} = 64$, $K = 7$。\n此情况与任务 1 的条件相符。\n$$\nS_{\\text{A}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 64)}{7 \\cdot 64 \\cdot 64} \\;=\\; 1 - \\frac{64 \\cdot 128}{7 \\cdot 4096} \\;=\\; 1 - \\frac{128}{7 \\cdot 64} \\;=\\; 1 - \\frac{2}{7} \\;=\\; \\frac{5}{7} \\approx 0.714286\n$$\n\n- **测试 B：** $H_{\\text{out}} = 28$, $W_{\\text{out}} = 28$, $C_{\\text{in}} = 32$, $C_{\\text{out}} = 32$, $C_{\\text{mid}} = 32$, $K = 1$。\n这个边界情况涉及一个 $1 \\times 1$ 卷积核，在这种情况下，分解通常没有好处。\n$$\nS_{\\text{B}} \\;=\\; 1 - \\frac{32 \\cdot (32 + 32)}{1 \\cdot 32 \\cdot 32} \\;=\\; 1 - \\frac{32 \\cdot 64}{1024} \\;=\\; 1 - \\frac{2048}{1024} \\;=\\; 1 - 2 \\;=\\; -1\n$$\n$-1$ 的节省量表明 FLOPs 翻了一番，这符合用两个连续的 $1 \\times 1$ 卷积替换一个 $1 \\times 1$ 卷积时的预期。\n\n- **测试 C：** $H_{\\text{out}} = 56$, $W_{\\text{out}} = 56$, $C_{\\text{in}} = 64$, $C_{\\text{out}} = 128$, $C_{\\text{mid}} = 64$, $K = 7$。\n这代表了一种瓶颈设计，其中 $C_{\\text{mid}}$ 小于 $C_{\\text{out}}$。\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{64 \\cdot (64 + 128)}{7 \\cdot 64 \\cdot 128} \\;=\\; 1 - \\frac{64 \\cdot 192}{7 \\cdot 8192} \\;=\\; 1 - \\frac{192}{7 \\cdot 128} \\;=\\; 1 - \\frac{192}{896}\n$$\n将分子和分母同除以它们的最大公约数 $64$ 来简化分数 $\\frac{192}{896}$：$\\frac{192 \\div 64}{896 \\div 64} = \\frac{3}{14}$。\n$$\nS_{\\text{C}} \\;=\\; 1 - \\frac{3}{14} \\;=\\; \\frac{11}{14} \\approx 0.785714\n$$\n\n**任务 3：Top-1 准确率计算**\n\n我们在提供的包含 $N=12$ 个样本的抽象数据集上计算基线模型和分解模型的 top-1 准确率。准确率是正确预测的数量除以样本总数。\n\n- **真实标签 $y$**：$[5,12,23,23,11,65,99,0,3,42,77,13]$\n\n- **基线模型预测类别 $\\hat{y}^{\\text{base}}$**：$[5,11,23,3,11,65,13,0,2,42,8,13]$\n我们逐元素比较 $y$ 和 $\\hat{y}^{\\text{base}}$ 以查找匹配项：\n- 索引 0：$5 = 5$ (匹配)\n- 索引 1：$12 \\neq 11$\n- 索引 2：$23 = 23$ (匹配)\n- 索引 3：$23 \\neq 3$\n- 索引 4：$11 = 11$ (匹配)\n- 索引 5：$65 = 65$ (匹配)\n- 索引 6：$99 \\neq 13$\n- 索引 7：$0 = 0$ (匹配)\n- 索引 8：$3 \\neq 2$\n- 索引 9：$42 = 42$ (匹配)\n- 索引 10：$77 \\neq 8$\n- 索引 11：$13 = 13$ (匹配)\n共有 $7$ 个正确预测。\n$$\n\\text{acc}_{\\text{base}} \\;=\\; \\frac{7}{12} \\approx 0.583333\n$$\n\n- **分解模型预测类别 $\\hat{y}^{\\text{fact}}$**：$[5,12,23,23,9,65,99,0,3,1,77,88]$\n我们逐元素比较 $y$ 和 $\\hat{y}^{\\text{fact}}$ 以查找匹配项：\n- 索引 0：$5 = 5$ (匹配)\n- 索引 1：$12 = 12$ (匹配)\n- 索引 2：$23 = 23$ (匹配)\n- 索引 3：$23 = 23$ (匹配)\n- 索引 4：$11 \\neq 9$\n- 索引 5：$65 = 65$ (匹配)\n- 索引 6：$99 = 99$ (匹配)\n- 索引 7：$0 = 0$ (匹配)\n- 索引 8：$3 = 3$ (匹配)\n- 索引 9：$42 \\neq 1$\n- 索引 10：$77 = 77$ (匹配)\n- 索引 11：$13 \\neq 88$\n共有 $9$ 个正确预测。\n$$\n\\text{acc}_{\\text{fact}} \\;=\\; \\frac{9}{12} \\;=\\; \\frac{3}{4} \\;=\\; 0.75\n$$\n\n- **准确率差异 $\\Delta \\text{acc}$**：\n$$\n\\Delta \\text{acc} \\;=\\; \\text{acc}_{\\text{fact}} - \\text{acc}_{\\text{base}} \\;=\\; \\frac{9}{12} - \\frac{7}{12} \\;=\\; \\frac{2}{12} \\;=\\; \\frac{1}{6} \\approx 0.166667\n$$\n结果将在程序中编译为所需的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the three tasks defined in the problem:\n    1. Computes FLOP savings fractions S for three test cases.\n    2. Computes top-1 accuracies for baseline and factorized models.\n    3. Computes the difference in accuracies.\n    \"\"\"\n\n    # --- Task 2: Compute FLOP Savings Fraction S ---\n\n    def calculate_savings_fraction(C_in, C_out, C_mid, K):\n        \"\"\"\n        Computes the FLOP savings fraction S using the derived formula:\n        S = 1 - (C_mid * (C_in + C_out)) / (K * C_in * C_out)\n        \n        A kernel size K=0 would lead to division by zero, but is physically\n        meaningless and not present in the test cases.\n        \"\"\"\n        if K == 0 or C_in == 0 or C_out == 0:\n            # Handle edge cases to prevent division by zero, though not strictly\n            # needed for the given problem data. An invalid configuration would\n            # result in undefined savings.\n            return np.nan\n\n        # Simplified formula derived in the solution text\n        numerator = C_mid * (C_in + C_out)\n        denominator = K * C_in * C_out\n        \n        # When K=1, the original convolution cost formula for F_orig can be zero if K=0.\n        # However, the problem states KxK conv, so K=1. For K=1, F_orig has K^2=1.\n        # F_fact has K=1. The formula is arithmetically sound.\n        \n        ratio = numerator / denominator\n        S = 1 - ratio\n        return S\n\n    # Test Case A\n    params_A = {'C_in': 64, 'C_out': 64, 'C_mid': 64, 'K': 7}\n    S_A = calculate_savings_fraction(**params_A)\n\n    # Test Case B\n    params_B = {'C_in': 32, 'C_out': 32, 'C_mid': 32, 'K': 1}\n    S_B = calculate_savings_fraction(**params_B)\n\n    # Test Case C\n    params_C = {'C_in': 64, 'C_out': 128, 'C_mid': 64, 'K': 7}\n    S_C = calculate_savings_fraction(**params_C)\n\n    # --- Task 3: Compute Top-1 Accuracies ---\n\n    # Provided data for the abstract ImageNet-100 subset\n    y_true = np.array([5, 12, 23, 23, 11, 65, 99, 0, 3, 42, 77, 13])\n    y_pred_base = np.array([5, 11, 23, 3, 11, 65, 13, 0, 2, 42, 8, 13])\n    y_pred_fact = np.array([5, 12, 23, 23, 9, 65, 99, 0, 3, 1, 77, 88])\n\n    # Top-1 accuracy is the mean of correct predictions\n    acc_base = np.mean(y_true == y_pred_base)\n    acc_fact = np.mean(y_true == y_pred_fact)\n\n    # Difference in accuracies\n    delta_acc = acc_fact - acc_base\n\n    # --- Final Output Formatting ---\n\n    results = [S_A, S_B, S_C, acc_base, acc_fact, delta_acc]\n    \n    # Format each result to six decimal places and join into the required string format\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n\n    print(output_string)\n\n# This block ensures the solve function is called when the script is executed.\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3130734"}, {"introduction": "在 Inception 模块中，各分支的输出在进入下一层之前会沿着通道维度进行串联。本练习探究了一个微妙但重要的设计细节：批量归一化（Batch Normalization, BN）的放置位置。通过分析在分支串联之前或之后应用 BN 对特征统计分布的影响，我们可以更好地理解网络内部不同组件的相互作用，以及这些设计决策如何影响整个模型的行为 [@problem_id:3130685]。", "problem": "在一个 GoogLeNet (Inception) 模块中，多个分支生成特征图，这些特征图在送入下一个卷积层之前沿通道维度进行拼接。考虑一个 Inception v2 风格的块中的两个分支，标记为 $A$ 和 $B$。\n\n每个分支输出一组通道，其激活前统计量（在批次和空间位置上）可以为每个通道建模为独立的髙斯随机变量，每个通道的均值为 $\\mu$，方差为 $\\sigma^{2}$。拼接操作通过堆叠来自 A 分支和 B 分支的所有通道来形成一个单一的张量。\n\n批量归一化 (Batch Normalization, BN) 遵循标准定义：对于一个具有随机变量 $X$ 的输入通道，BN 变换为 $Y = \\gamma \\frac{X - \\hat{\\mu}}{\\sqrt{\\hat{\\sigma}^{2} + \\varepsilon}} + \\beta$，其中 $\\hat{\\mu}$ 和 $\\hat{\\sigma}^{2}$ 是批次均值和批次方差的估计值，$\\gamma$ 和 $\\beta$ 是可学习的缩放和平移参数，$\\varepsilon$ 是一个用于数值稳定性的小常数。在初始化时，我们取 $\\gamma = 1$，$\\beta = 0$，并假设 $\\varepsilon$ 可以忽略不计，因此 BN 理想情况下会将每个通道归一化为零均值和单位方差。\n\n将拼接边界处的分支级别错配度量定义为\n$$\nD = \\left(m_{A} - m_{B}\\right)^{2} + \\left(v_{A} - v_{B}\\right)^{2},\n$$\n其中 $m_{A}$ 和 $m_{B}$ 分别是分支 $A$ 和 $B$ 内部各通道均值的平均值，$v_{A}$ 和 $v_{B}$ 分别是分支 $A$ 和 $B$ 内部各通道方差的平均值。这个 $D$ 量化了就在拼接前分支间的均值/方差错配。\n\n你将比较两种 BN 放置位置：\n- 先 BN (BN-before)：在拼接之前，对每个分支内的每个通道独立应用批量归一化。\n- 后 BN (BN-after)：省略分支内的 BN，在拼接后应用一个单一的批量归一化层（根据上述 BN 定义，该层独立地归一化每个拼接后的通道）。\n\n假设分支 $A$ 有 3 个通道，其 $(\\mu,\\sigma^{2})$ 值如下\n$$\n(\\mu_{A,1},\\sigma^{2}_{A,1}) = (0.5,\\,1.6),\\quad (\\mu_{A,2},\\sigma^{2}_{A,2}) = (-0.2,\\,0.9),\\quad (\\mu_{A,3},\\sigma^{2}_{A,3}) = (0.3,\\,1.1),\n$$\n分支 $B$ 有 2 个通道，其值为\n$$\n(\\mu_{B,1},\\sigma^{2}_{B,1}) = (-0.4,\\,0.8),\\quad (\\mu_{B,2},\\sigma^{2}_{B,2}) = (0.1,\\,1.4).\n$$\n\n从上面提供的拼接、期望、方差和批量归一化的定义出发（并且除了这些基本原理外不使用任何快捷公式），推导出在拼接前应用 BN 时的错配度量 $D_{\\text{before}}$ 和在拼接后应用 BN 时的错配度量 $D_{\\text{after}}$。然后计算差值\n$$\n\\Delta D = D_{\\text{after}} - D_{\\text{before}}.\n$$\n将 $\\Delta D$ 报告为单个实数。除了精确算术外，不需要进行四舍五入，也不涉及单位。确保你的推导解释了每种 BN 放置方式如何影响在 $D$ 中使用的 $m_{A}$、$m_{B}$、$v_{A}$ 和 $v_{B}$。", "solution": "用户提供的问题已经过验证，被确定为深度学习领域中的一个有效的、良态的问题。它具有科学依据，是自洽且客观的。我们现在可以开始解答。\n\n目标是计算两种批量归一化 (BN) 放置策略之间的分支级别错配度量差异 $\\Delta D = D_{\\text{after}} - D_{\\text{before}}$。错配度量定义为 $D = (m_{A} - m_{B})^{2} + (v_{A} - v_{B})^{2}$，其中 $m$ 和 $v$ 分别表示给定分支在拼接前各通道均值和方差的平均值。\n\n首先，我们确定分支 $A$ 和分支 $B$ 的初始统计数据。\n分支 $A$ 有 $N_A = 3$ 个通道，其均值和方差对 $(\\mu, \\sigma^2)$ 如下：\n$(\\mu_{A,1}, \\sigma^{2}_{A,1}) = (0.5, 1.6)$\n$(\\mu_{A,2}, \\sigma^{2}_{A,2}) = (-0.2, 0.9)$\n$(\\mu_{A,3}, \\sigma^{2}_{A,3}) = (0.3, 1.1)$\n\n分支 $B$ 有 $N_B = 2$ 个通道，其统计数据如下：\n$(\\mu_{B,1}, \\sigma^{2}_{B,1}) = (-0.4, 0.8)$\n$(\\mu_{B,2}, \\sigma^{2}_{B,2}) = (0.1, 1.4)$\n\n问题的核心在于确定在“先 BN”和“后 BN”两种情景下 $m_A, m_B, v_A, v_B$ 的值。度量 $D$ 是基于特征图在拼接*之前*的统计数据来计算的。\n\n**情况 1：“先 BN” (BN-before) 放置**\n在此情景下，批量归一化在拼接发生*之前*独立应用于每个分支内的每个通道。问题陈述，在给定的理想条件下 ($\\gamma=1, \\beta=0, \\varepsilon \\approx 0$)，BN “理想情况下将每个通道归一化为零均值和单位方差”。\n\n设 $\\mu'_{A,i}$ 和 $\\sigma'^2_{A,i}$ 为分支 $A$ 中通道 $i$ 归一化后的均值和方差。根据问题对理想 BN 的定义，对于分支 $A$ 中的每个通道 $i$：\n$$\n\\mu'_{A,i} = 0 \\\\\n\\sigma'^2_{A,i} = 1\n$$\n类似地，对于分支 $B$ 中的每个通道 $j$，BN 后的统计量是：\n$$\n\\mu'_{B,j} = 0 \\\\\n\\sigma'^2_{B,j} = 1\n$$\n这些是拼接点上各通道的统计量。我们现在计算每个分支的平均统计量以求得 $D_{\\text{before}}$。\n\n分支 $A$ 的各通道均值的平均值，记作 $m_{A, \\text{before}}$，为：\n$$\nm_{A, \\text{before}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\mu'_{A,i} = \\frac{1}{3} (0 + 0 + 0) = 0\n$$\n分支 $A$ 的各通道方差的平均值，记作 $v_{A, \\text{before}}$，为：\n$$\nv_{A, \\text{before}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\sigma'^2_{A,i} = \\frac{1}{3} (1 + 1 + 1) = 1\n$$\n类似地，对于分支 $B$：\n各通道均值的平均值，$m_{B, \\text{before}}$：\n$$\nm_{B, \\text{before}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\mu'_{B,j} = \\frac{1}{2} (0 + 0) = 0\n$$\n各通道方差的平均值，$v_{B, \\text{before}}$：\n$$\nv_{B, \\text{before}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\sigma'^2_{B,j} = \\frac{1}{2} (1 + 1) = 1\n$$\n现在我们可以计算错配度量 $D_{\\text{before}}$：\n$$\nD_{\\text{before}} = (m_{A, \\text{before}} - m_{B, \\text{before}})^{2} + (v_{A, \\text{before}} - v_{B, \\text{before}})^{2} \\\\\nD_{\\text{before}} = (0 - 0)^{2} + (1 - 1)^{2} = 0^2 + 0^2 = 0\n$$\n\n**情况 2：“后 BN” (BN-after) 放置**\n在此情景下，分支内省略了 BN。先进行拼接，然后对拼接后的张量应用一个单一的 BN 层。错配度量 $D$ 是基于拼接*之前*的统计数据进行评估的。因此，我们必须使用各通道原始的、未归一化的统计量。\n\n分支 $A$ 的各通道均值的平均值，记作 $m_{A, \\text{after}}$，为：\n$$\nm_{A, \\text{after}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\mu_{A,i} = \\frac{1}{3} (0.5 + (-0.2) + 0.3) = \\frac{1}{3}(0.6) = 0.2\n$$\n分支 $A$ 的各通道方差的平均值，记作 $v_{A, \\text{after}}$，为：\n$$\nv_{A, \\text{after}} = \\frac{1}{N_A} \\sum_{i=1}^{3} \\sigma^{2}_{A,i} = \\frac{1}{3} (1.6 + 0.9 + 1.1) = \\frac{1}{3}(3.6) = 1.2\n$$\n对于分支 $B$，相应的平均值为：\n各通道均值的平均值，$m_{B, \\text{after}}$：\n$$\nm_{B, \\text{after}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\mu_{B,j} = \\frac{1}{2} (-0.4 + 0.1) = \\frac{1}{2}(-0.3) = -0.15\n$$\n各通道方差的平均值，$v_{B, \\text{after}}$：\n$$\nv_{B, \\text{after}} = \\frac{1}{N_B} \\sum_{j=1}^{2} \\sigma^{2}_{B,j} = \\frac{1}{2} (0.8 + 1.4) = \\frac{1}{2}(2.2) = 1.1\n$$\n现在我们使用这些值计算错配度量 $D_{\\text{after}}$：\n$$\nD_{\\text{after}} = (m_{A, \\text{after}} - m_{B, \\text{after}})^{2} + (v_{A, \\text{after}} - v_{B, \\text{after}})^{2} \\\\\nD_{\\text{after}} = (0.2 - (-0.15))^{2} + (1.2 - 1.1)^{2} \\\\\nD_{\\text{after}} = (0.35)^{2} + (0.1)^{2} \\\\\nD_{\\text{after}} = 0.1225 + 0.01 = 0.1325\n$$\n\n**最终计算：差值 $\\Delta D$**\n最后，我们计算差值 $\\Delta D = D_{\\text{after}} - D_{\\text{before}}$。\n$$\n\\Delta D = 0.1325 - 0 = 0.1325\n$$\n这个结果量化了当从“先 BN”设计迁移到“后 BN”设计时，拼接边界处统计错配的增加。“先 BN”策略通过预先归一化每个分支来完全消除错配，而“后 BN”策略则允许分支间的原始统计差异一直持续到拼接点。", "answer": "$$\\boxed{0.1325}$$", "id": "3130685"}]}