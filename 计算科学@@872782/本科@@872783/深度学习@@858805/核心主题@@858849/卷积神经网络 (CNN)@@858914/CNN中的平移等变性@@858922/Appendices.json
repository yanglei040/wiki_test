{"hands_on_practices": [{"introduction": "这个练习为我们理解平移等变性奠定了基础。我们将从简单的一维信号入手，通过编程实现来验证一个核心概念：对于周期性信号（如角度或循环数据流），循环填充（circular padding）可以完美地保持卷积操作的平移等变性，而零填充（zero padding）则会在边界处破坏这一特性。通过这个练习，你将亲手量化这两种填充方式在边界效应上的差异，从而深刻理解填充策略对网络行为的基础性影响。[@problem_id:3196029]", "problem": "您的任务是评估在一维离散互相关中的平移等变性，该互相关用于卷积神经网络（CNN）。请严格处理周期信号，并形式化两种填充方案：循环填充和零填充。您的工作是实现这两种方案，并量化当输入被循环移位时产生的等变性误差。评估应纯粹是数学和算法层面的，不得借助外部文件或用户输入。\n\n使用的基本定义：\n- 设输入信号为 $x \\in \\mathbb{R}^N$，其索引 $n \\in \\{0,1,\\dots,N-1\\}$，核为 $w \\in \\mathbb{R}^M$，$M$ 为奇数。令 $c = \\frac{M-1}{2}$ 表示核中心索引。\n- 对于整数移位 $s$，定义循环移位算子 $T_s$ 如下\n$$\n\\left(T_s x\\right)[n] = x\\left[(n - s) \\bmod N\\right].\n$$\n- 定义一维循环填充互相关（输出长度为 $N$）如下\n$$\n\\left(\\mathrm{Corr}_{\\mathrm{circ}}(x,w)\\right)[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(n + m - c) \\bmod N\\right].\n$$\n- 定义一维零填充互相关（输出长度为 $N$）如下\n$$\n\\left(\\mathrm{Corr}_{0}(x,w)\\right)[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[n + m - c\\right]\\mathbf{1}\\left(0 \\le n + m - c \\le N-1\\right),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数，因此在 $\\{0,1,\\dots,N-1\\}$ 之外的索引对应的值被视为 $0$。\n\n任务：\n- 对每个给定的测试用例，计算两个非负实数：\n    - 在循环填充下，与平移等变性的最大绝对偏差，\n    $$\n    \\varepsilon_{\\mathrm{circ}} = \\max_{0 \\le n \\le N-1} \\left| \\left(\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w)\\right)[n] - \\left(T_s\\,\\mathrm{Corr}_{\\mathrm{circ}}(x,w)\\right)[n] \\right|.\n    $$\n    - 在零填充下，与平移等变性的最大绝对偏差，\n    $$\n    \\varepsilon_{0} = \\max_{0 \\le n \\le N-1} \\left| \\left(\\mathrm{Corr}_{0}(T_s x, w)\\right)[n] - \\left(T_s\\,\\mathrm{Corr}_{0}(x,w)\\right)[n] \\right|.\n    $$\n- 使用上述定义的模索引实现循环移位算子 $T_s$。\n- 使用环绕索引实现 $\\mathrm{Corr}_{\\mathrm{circ}}$，并使用上述指示函数实现 $\\mathrm{Corr}_{0}$。\n- 使用浮点运算。不涉及物理单位。提及角度仅作为周期信号的一个启发性例子；所有计算都是纯数值的。\n\n测试套件：\n为以下六个参数集 $(x,w,s)$ 提供结果，每个参数集都有指定的 $N$ 和奇数 $M$：\n- 情况 1：$N = 8$, $x = [0,1,2,3,4,5,6,7]$, $w = [1,0,-1]$, $s = 2$。\n- 情况 2：$N = 8$, $x[n] = \\sin\\left(2\\pi n / 8\\right)$，对于 $n \\in \\{0,1,2,3,4,5,6,7\\}$, $w = [1,2,3,2,1]/9$, $s = 3$。\n- 情况 3：$N = 5$, $x = [1,0,0,0,0]$, $w = [-1,0,2]$, $s = 4$。\n- 情况 4：$N = 7$, $x = [0.2,-0.1,0.5,-0.3,0.7,-0.2,0.0]$, $w = [0.25,0.5,0.25]$, $s = 0$。\n- 情况 5：$N = 9$, $x[n] = \\cos\\left(2\\pi n / 9\\right)$，对于 $n \\in \\{0,1,2,3,4,5,6,7,8\\}$, $w = [1,-1,0,1,-1]$, $s = 4$。\n- 情况 6：$N = 8$, $x = [1,-1,1,-1,1,-1,1,-1]$, $w = [2,-1,0,1,-2,1,0]$, $s = 1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。\n- 该列表必须按顺序包含 $12$ 个浮点值\n$$\n[\\varepsilon_{\\mathrm{circ}}^{(1)}, \\varepsilon_{0}^{(1)}, \\varepsilon_{\\mathrm{circ}}^{(2)}, \\varepsilon_{0}^{(2)}, \\dots, \\varepsilon_{\\mathrm{circ}}^{(6)}, \\varepsilon_{0}^{(6)}],\n$$\n其中上标表示情况编号。将每个值表示为十进制数。例如，一个语法正确的行应类似于 $[0.0,0.5,0.0,1.25,\\dots]$。", "solution": "该问题要求评估一维离散互相关在两种不同填充方案（循环填充和零填充）下的平移等变性。平移等变性是信号处理和卷积神经网络（CNNs）中的一个基本概念。如果在一个算子 $F$ 应用之前或之后应用一个变换 $T$ 会产生相同的结果，即对于任何输入 $x$ 都有 $F(T(x)) = T(F(x))$，则称算子 $F$ 对变换 $T$ 是等变的。在此背景下，算子 $F$ 是互相关（$\\mathrm{Corr}_{\\mathrm{circ}}$ 或 $\\mathrm{Corr}_{0}$），变换 $T$ 是循环移位算子 $T_s$。我们将分析每种填充方案对该属性的遵守情况，然后提出一种算法来计算指定的偏差度量 $\\varepsilon_{\\mathrm{circ}}$ 和 $\\varepsilon_{0}$。\n\n首先，让我们形式化待比较的量。对于给定的填充方案，我们计算移位后输入的互相关输出，记为 $y'_{\\mathrm{pad}} = \\mathrm{Corr}_{\\mathrm{pad}}(T_s x, w)$。我们还计算原始输入的互相关输出 $y_{\\mathrm{pad}} = \\mathrm{Corr}_{\\mathrm{pad}}(x, w)$，然后对该输出进行移位，得到 $T_s y_{\\mathrm{pad}}$。等变性误差是这两个结果信号之间逐元素的最大绝对差。\n\n**循环填充与等变性分析**\n\n根据定义，循环互相关算子对于循环移位算子 $T_s$ 是完全等变的。我们可以用数学方法证明这一点。设 $y_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$。我们旨在证明 $\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w) = T_s y_{\\mathrm{circ}}$。\n\n让我们在任意索引 $n \\in \\{0, 1, \\dots, N-1\\}$ 处展开左侧（LHS）：\n$$\n\\text{LHS}[n] = \\left(\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w)\\right)[n] = \\sum_{m=0}^{M-1} w[m]\\; (T_s x)\\left[(n + m - c) \\bmod N\\right]\n$$\n根据移位算子的定义，$(T_s x)[k] = x[(k - s) \\bmod N]$。代入 $k = (n + m - c) \\bmod N$：\n$$\n\\text{LHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[\\left( ((n + m - c) \\bmod N) - s \\right) \\bmod N\\right]\n$$\n利用模运算的性质 $((a \\bmod N) - b) \\bmod N = (a-b) \\bmod N$，我们可以简化索引：\n$$\n\\text{LHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(n - s + m - c) \\bmod N\\right]\n$$\n现在我们来考察右侧（RHS），$T_s y_{\\mathrm{circ}}$。\n$$\n\\text{RHS}[n] = (T_s y_{\\mathrm{circ}})[n] = y_{\\mathrm{circ}}[(n-s) \\bmod N]\n$$\n根据 $y_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$ 的定义，我们有：\n$$\n\\text{RHS}[n] = \\left(\\mathrm{Corr}_{\\mathrm{circ}}(x, w)\\right)[(n-s) \\bmod N] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(((n-s)\\bmod N) + m - c) \\bmod N\\right]\n$$\n再次利用模运算的性质，这可以简化为：\n$$\n\\text{RHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x\\left[(n - s + m - c) \\bmod N\\right]\n$$\n由于对于所有 $n \\in \\{0, 1, \\dots, N-1\\}$ 都有 $\\text{LHS}[n] = \\text{RHS}[n]$，因此等式 $\\mathrm{Corr}_{\\mathrm{circ}}(T_s x, w) = T_s \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$ 成立。因此，等变性误差的理论值为 $\\varepsilon_{\\mathrm{circ}} = 0$。数值计算中任何非零结果都可归因于浮点精度误差。一个例外是 $s=0$ 的情况，此时移位是恒等操作，使得任何算子的等变性都变得微不足道。\n\n**零填充与非等变性分析**\n\n相比之下，零填充的互相关通常对循环移位不是等变的。问题的核心在于循环移位 $T_s$ 的循环性质与零填充互相关 $\\mathrm{Corr}_{0}$ 的非循环、有界性质之间的不兼容性。算子 $T_s$ 将信号的一端的值环绕到另一端。然而，算子 $\\mathrm{Corr}_{0}$ 将信号边界视为绝对边界，在域 $\\{0, 1, \\dots, N-1\\}$ 之外填充零。\n\n让我们分析零填充情况下等变性方程的两侧。\n左侧是：\n$$\n\\text{LHS}[n] = (\\mathrm{Corr}_{0}(T_s x, w))[n] = \\sum_{m=0}^{M-1} w[m]\\; (T_s x)[n + m - c]\\; \\mathbf{1}(0 \\le n + m - c \\le N-1)\n$$\n代入 $(T_s x)[k] = x[(k-s) \\bmod N]$：\n$$\n\\text{LHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x[((n + m - c) - s) \\bmod N]\\; \\mathbf{1}(0 \\le n + m - c \\le N-1)\n$$\n右侧是：\n$$\n\\text{RHS}[n] = (T_s \\mathrm{Corr}_{0}(x,w))[n] = (\\mathrm{Corr}_{0}(x,w))[(n-s) \\bmod N]\n$$\n令 $n' = (n-s) \\bmod N$。那么：\n$$\n\\text{RHS}[n] = \\sum_{m=0}^{M-1} w[m]\\; x[n' + m - c]\\; \\mathbf{1}(0 \\le n' + m - c \\le N-1)\n$$\n$\\text{LHS}[n]$ 和 $\\mathrm{RHS}[n]$ 的表达式不等价。$\\mathrm{LHS}$ 将指示函数应用于索引 $k = n + m - c$，然后使用环绕后的值 $x[(k-s) \\bmod N]$。而 $\\mathrm{RHS}$ 使用移位后的输出索引 $n'$，并将指示函数应用于基于 $n'$ 的索引，使用未环绕的值 $x[n' + m - c]$。这种差异在边界处最为明显。例如，当 $\\mathrm{RHS}$ 计算中边界附近的核支撑需要读取 $[0, N-1]$ 之外的值（从而得到零）时，$\\mathrm{LHS}$ 中对应的核可能会读取到一个被 $T_s$ 环绕过来的非零值。这导致了非零的等变性误差，即 $\\varepsilon_{0} > 0$，除非在平凡情况下（如 $s=0$）。\n\n**算法流程**\n\n对每个测试用例 $(x, w, s)$ 的验证将按以下步骤进行：\n1. 确定信号长度 $N = \\mathrm{len}(x)$ 和核长度 $M = \\mathrm{len}(w)$。计算核中心索引 $c = (M-1)/2$。\n2. 根据定义 $(T_s x)[n] = x[(n - s) \\bmod N]$ 实现循环移位算子 $T_s$。\n3. 通过对核进行求和并对输入信号索引使用模运算 $(n+m-c)\\bmod N$ 来实现循环互相关 $\\mathrm{Corr}_{\\mathrm{circ}}(x, w)$。\n4. 通过对核进行求和并使用指示函数（或条件逻辑）来实现零填充互相关 $\\mathrm{Corr}_0(x, w)$，即当 $k = n+m-c$ 且 $0 \\le k \\le N-1$ 时使用 $x[k]$，否则使用 $0$。\n5. 对于循环情况：\n    a. 计算移位后的输入信号：$x' = T_s x$。\n    b. 计算移位后输入的互相关：$y'_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x', w)$。\n    c. 计算原始输入的互相关：$y_{\\mathrm{circ}} = \\mathrm{Corr}_{\\mathrm{circ}}(x, w)$。\n    d. 移位产生的输出：$y_{\\mathrm{circ\\_shifted}} = T_s y_{\\mathrm{circ}}$。\n    e. 计算误差：$\\varepsilon_{\\mathrm{circ}} = \\max |y'_{\\mathrm{circ}} - y_{\\mathrm{circ\\_shifted}}|$。\n6. 对于零填充情况：\n    a. 计算移位后的输入信号：$x' = T_s x$。\n    b. 计算移位后输入的互相关：$y'_{0} = \\mathrm{Corr}_{0}(x', w)$。\n    c. 计算原始输入的互相关：$y_{0} = \\mathrm{Corr}_{0}(x, w)$。\n    d. 移位产生的输出：$y_{0\\_\\mathrm{shifted}} = T_s y_{0}$。\n    e. 计算误差：$\\varepsilon_{0} = \\max |y'_{0} - y_{0\\_\\mathrm{shifted}}|$。\n7. 该测试用例的最终结果是序对 $(\\varepsilon_{\\mathrm{circ}}, \\varepsilon_{0})$。对所有指定的测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n#\n# Helper functions for the specified operations\n#\n\ndef circular_shift(x: np.ndarray, s: int) -> np.ndarray:\n    \"\"\"Implements the circular shift operator T_s.\n    (T_s x)[n] = x[(n - s) mod N]\n    This corresponds to a right shift by s.\n    \"\"\"\n    return np.roll(x, s)\n\ndef corr_circ(x: np.ndarray, w: np.ndarray) -> np.ndarray:\n    \"\"\"Implements 1D circularly padded cross-correlation.\n    (Corr_circ(x,w))[n] = sum_{m=0}^{M-1} w[m] x[(n + m - c) mod N]\n    \"\"\"\n    N = len(x)\n    M = len(w)\n    c = (M - 1) // 2\n    y = np.zeros(N, dtype=float)\n    for n in range(N):\n        val = 0.0\n        for m in range(M):\n            idx = (n + m - c) % N\n            val += w[m] * x[idx]\n        y[n] = val\n    return y\n\ndef corr_zero(x: np.ndarray, w: np.ndarray) -> np.ndarray:\n    \"\"\"Implements 1D zero-padded cross-correlation.\n    (Corr_0(x,w))[n] = sum_{m=0}^{M-1} w[m] x[n + m - c] * 1(0 = n+m-c = N-1)\n    \"\"\"\n    N = len(x)\n    M = len(w)\n    c = (M - 1) // 2\n    y = np.zeros(N, dtype=float)\n    for n in range(N):\n        val = 0.0\n        for m in range(M):\n            idx = n + m - c\n            if 0 = idx  N:\n                val += w[m] * x[idx]\n        y[n] = val\n    return y\n\ndef calculate_equivariance_error(x, w, s):\n    \"\"\"\n    Calculates the equivariance errors ε_circ and ε_0 for a given\n    input signal x, kernel w, and shift s.\n    \"\"\"\n    # Circular Padding Case\n    x_shifted = circular_shift(x, s)\n    y_corr_of_shifted = corr_circ(x_shifted, w)\n    \n    y = corr_circ(x, w)\n    y_shifted_of_corr = circular_shift(y, s)\n    \n    eps_circ = np.max(np.abs(y_corr_of_shifted - y_shifted_of_corr))\n\n    # Zero Padding Case\n    # x_shifted is the same\n    y_corr_of_shifted_zero = corr_zero(x_shifted, w)\n\n    y_zero = corr_zero(x, w)\n    y_shifted_of_corr_zero = circular_shift(y_zero, s)\n    \n    eps_zero = np.max(np.abs(y_corr_of_shifted_zero - y_shifted_of_corr_zero))\n    \n    return eps_circ, eps_zero\n\ndef solve():\n    \"\"\"\n    Defines the test cases, computes the equivariance errors for each,\n    and prints the results in the specified format.\n    \"\"\"\n    \n    # Define test cases\n    case1 = (np.array([0,1,2,3,4,5,6,7], dtype=float), np.array([1,0,-1], dtype=float), 2)\n    \n    N2 = 8\n    n2 = np.arange(N2)\n    x2 = np.sin(2 * np.pi * n2 / N2)\n    w2 = np.array([1,2,3,2,1], dtype=float) / 9.0\n    case2 = (x2, w2, 3)\n\n    case3 = (np.array([1,0,0,0,0], dtype=float), np.array([-1,0,2], dtype=float), 4)\n\n    case4 = (np.array([0.2,-0.1,0.5,-0.3,0.7,-0.2,0.0], dtype=float), np.array([0.25,0.5,0.25], dtype=float), 0)\n\n    N5 = 9\n    n5 = np.arange(N5)\n    x5 = np.cos(2 * np.pi * n5 / N5)\n    w5 = np.array([1,-1,0,1,-1], dtype=float)\n    case5 = (x5, w5, 4)\n\n    case6 = (np.array([1,-1,1,-1,1,-1,1,-1], dtype=float), np.array([2,-1,0,1,-2,1,0], dtype=float), 1)\n\n    test_cases = [case1, case2, case3, case4, case5, case6]\n    \n    results = []\n    for x, w, s in test_cases:\n        eps_circ, eps_zero = calculate_equivariance_error(x, w, s)\n        results.append(eps_circ)\n        results.append(eps_zero)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3196029"}, {"introduction": "在掌握了填充对等变性的影响后，我们将进入更复杂的二维图像领域，并探究一个在现代CNN中普遍存在且至关重要的环节：最大池化（max pooling）。这个练习将向你展示，即使卷积层本身是等变的，后续的最大池化操作也会打破这一特性，并且这种破坏的程度与输入特征相对于池化网格的“相位”（即对齐方式）密切相关。你将使用一个高频的棋盘格图案作为“压力测试”的输入，系统地量化这种由池化引起的等变性失效。[@problem_id:3196094]", "problem": "您的任务是使用合成棋盘格输入，量化卷积神经网络（CNN）管道中由最大池化引起的平移等变性破坏。评估必须通过计算一个对易子模长来完成，该模长比较了在管道之前应用空间平移与在管道之后应用相应的下采样平移。该管道由循环卷积、其后的整流线性单元（ReLU）以及最后的非重叠最大池化组成。您将系统地改变棋盘格相对于池化网格的输入相位，并聚合对易子的模长。\n\n定义与设置：\n- 设输入为一个大小为 $N \\times N$ 的方形图像，具有周期性边界条件（循环包裹）。\n- 对图像 $x$ 定义循环平移算子 $T_{(\\Delta u,\\Delta v)}$ 如下：\n$$\n(T_{(\\Delta u,\\Delta v)} x)[u,v] \\;=\\; x[(u-\\Delta u) \\bmod N,\\; (v-\\Delta v) \\bmod N].\n$$\n- 使用核 $K$ 定义循环卷积算子 $C_K$ 如下：\n$$\n(C_K x)[u,v] \\;=\\; \\sum_{m}\\sum_{n} K[m,n]\\; x[(u-m) \\bmod N,\\; (v-n) \\bmod N].\n$$\n- 定义逐点整流线性单元（ReLU）$\\phi$ 如下：\n$$\n\\phi(z) \\;=\\; \\max(0,z).\n$$\n- 在一个 $N \\times N$ 的输入上（假设 $N$ 可被 $s$ 整除），定义窗口大小为 $s \\times s$、步幅为 $s$ 的非重叠最大池化为算子 $P_s$，其产生一个大小为 $(N/s) \\times (N/s)$ 的输出：\n$$\n(P_s x)[i,j] \\;=\\; \\max_{0 \\le a  s,\\; 0 \\le b  s} \\; x[i s + a,\\; j s + b],\n$$\n其中索引以常规数组方式计算，并依赖于 $N$ 可被 $s$ 整除的事实（每个池化窗口内不需要边界包裹）。\n- 定义作用于大小为 $(N/s)\\times(N/s)$ 的池化输出的下采样平移算子 $U_{(\\delta_u,\\delta_v)}$ 如下：\n$$\n(U_{(\\delta_u,\\delta_v)} y)[i,j] \\;=\\; y[(i-\\delta_u) \\bmod (N/s), \\; (j-\\delta_v) \\bmod (N/s)].\n$$\n- 定义复合CNN算子 $F$ 如下：\n$$\nF \\;=\\; P_s \\circ \\phi \\circ C_K.\n$$\n- 对于任何输入 $x$ 和整数输入空间平移 $(\\Delta u,\\Delta v)$，定义相应的输出空间平移：\n$$\n(\\delta_u,\\delta_v) \\;=\\; \\left(\\left\\lfloor \\frac{\\Delta u}{s} \\right\\rfloor, \\; \\left\\lfloor \\frac{\\Delta v}{s} \\right\\rfloor \\right).\n$$\n- 定义对易子模长（归一化）如下：\n$$\nM(x; \\Delta u,\\Delta v) \\;=\\; \\frac{\\left\\| F\\left(T_{(\\Delta u,\\Delta v)} x\\right) \\;-\\; U_{(\\delta_u,\\delta_v)}\\left(F(x)\\right) \\right\\|_2}{\\left\\| F(x) \\right\\|_2 + \\varepsilon},\n$$\n其中 $\\|\\cdot\\|_2$ 是池化输出上的欧几里得范数，$\\varepsilon$ 是一个小的正数常量，用于避免除以零。\n\n输入生成与相位扫描：\n- 使用周期为 $2$ 的二元棋盘格模式：\n$$\nB[u,v] \\;=\\; \\mathbf{1}\\left\\{ \\left((u+v) \\bmod 2\\right) = 0 \\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 定义相对于池化网格的相位平移输入如下：\n$$\nB_{p,q}[u,v] \\;=\\; B[(u+p) \\bmod N,\\; (v+q) \\bmod N],\n$$\n适用于所有整数相位 $p \\in \\{0,1,\\dots,s-1\\}$ 和 $q \\in \\{0,1,\\dots,s-1\\}$。\n\n核：\n- 使用一个固定的 $3 \\times 3$ 水平方向 Sobel 核，\n$$\nK \\;=\\; \\begin{bmatrix}\n-1  0  1 \\\\\n-2  0  2 \\\\\n-1  0  1\n\\end{bmatrix}.\n$$\n\n任务：\n1. 实现如上定义的 $T_{(\\Delta u,\\Delta v)}$、$C_K$（带循环边界条件）、$\\phi$、$P_s$ 和 $U_{(\\delta_u,\\delta_v)}$。\n2. 对于下方的每个测试用例，生成所有 $s \\times s$ 个相位平移输入 $B_{p,q}$，为每个 $(p,q)$ 计算 $M(B_{p,q}; \\Delta u,\\Delta v)$，并通过计算以下值进行聚合：\n   - 所有相位的均值，\n   - 所有相位的最大值。\n3. 为了数值稳定性，在 $M$ 的定义中使用 $\\varepsilon = 10^{-12}$。\n4. 将每个聚合的浮点数四舍五入到 $6$ 位小数。\n\n测试套件：\n- 所有情况均使用上述 Sobel 核 $K$。对于每种情况，$N$ 是输入尺寸，$s$ 是池化步幅/窗口大小，$(\\Delta u,\\Delta v)$ 是输入空间平移。\n  1. 情况A：$N=24$, $s=2$, $(\\Delta u,\\Delta v) = (1,0)$。\n  2. 情况B：$N=24$, $s=2$, $(\\Delta u,\\Delta v) = (2,0)$。\n  3. 情况C：$N=24$, $s=3$, $(\\Delta u,\\Delta v) = (1,1)$。\n  4. 情况D：$N=24$, $s=3$, $(\\Delta u,\\Delta v) = (3,0)$。\n  5. 情况E：$N=24$, $s=3$, $(\\Delta u,\\Delta v) = (0,0)$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含一个逗号分隔的列表形式的结果，并用方括号括起来，每个测试用例的结果是一个双元素列表 $[\\text{mean}, \\text{max}]$，顺序与上述情况一致。例如，输出必须如下所示：\n$$\n\\big[ [m_A, M_A], [m_B, M_B], [m_C, M_C], [m_D, M_D], [m_E, M_E] \\big],\n$$\n每个浮点数四舍五入到 $6$ 位小数。", "solution": "用户提供的问题是有效的。它在科学上基于深度学习的原理，特别是关于卷积神经网络（CNN）中平移等变性的属性。所有定义、参数和任务在数学上都是精确、自洽且逻辑一致的。该问题是适定的，会得出一个唯一的、可验证的数值解。我现在将提供完整的解决方案。\n\n### 原理与方法\n\n这个问题的核心是量化一个简单的CNN管道在多大程度上不满足平移等变性。如果对输入的平移会导致输出发生相应的平移，则称算子$F$是平移等变的。形式上，对于一个平移算子 $T_\\Delta$，必须存在一个相应的输出空间平移算子 $T'_\\delta$，使得对于任何输入$x$，都有 $F(T_\\Delta x) = T'_\\delta(F(x))$。如果这个等式不成立，那么该算子在某种程度上只是平移*不变*的，但不是严格等变的。\n\n非等变性的程度可以通过管道算子 $F$ 和平移算子 $T$ 之间的对易子模长来衡量。对易子 $[F, T]$ 定义为 $F \\circ T - T' \\circ F$。如果算子完全可交换（即系统是完全等变的），将此对易子应用于输入 $x$ 的结果将为零。非零结果表示等变性的破坏。该问题定义了此对易子的归一化模长：\n$$\nM(x; \\Delta u,\\Delta v) = \\frac{\\left\\| F\\left(T_{(\\Delta u,\\Delta v)} x\\right) - U_{(\\delta_u,\\delta_v)}\\left(F(x)\\right) \\right\\|_2}{\\left\\| F(x) \\right\\|_2 + \\varepsilon}\n$$\n在这里，$F(T_{(\\Delta u,\\Delta v)} x)$ 代表“先平移后处理”路径，而 $U_{(\\delta_u,\\delta_v)}(F(x))$ 代表“先处理后平移”路径。这两条路径之间的差异就是等变性的破坏。\n\nCNN管道 $F = P_s \\circ \\phi \\circ C_K$ 包括三个阶段：\n1.  **循环卷积 ($C_K$)**：此操作是完全平移等变的。对一个平移后的信号进行卷积等同于对卷积后的信号进行平移。\n2.  **ReLU ($\\phi$)**：这个逐点激活函数也是完全平移等变的。在平移之前或之后应用它会产生相同的结果。\n3.  **最大池化 ($P_s$)**：这是等变性破坏的来源。最大池化仅对步长 $s$ 的整数倍平移是等变的。对于任何其他的平移值（“子步幅”平移），输出会以一种非线性的方式改变，这种改变无法通过对未平移输出进行简单平移来描述。这些算子的复合导致管道 $F$ 在一般情况下不是平移等变的。\n\n输入信号是一个合成的棋盘格模式。通过系统地改变其相对于固定池化网格的相位 $(p, q)$，我们可以评估在所有可能的输入对齐方式下等变性的破坏情况。这一点至关重要，因为破坏的程度高度依赖于信号的高频分量如何与池化窗口的边界对齐。将所有相位的对易子模长取平均值，为给定输入平移 $(\\Delta u, \\Delta v)$ 下管道的非等变性提供了一个鲁棒的聚合度量。\n\n### 算法实现\n\n解决方案通过为每个测试用例执行以下步骤来实现：\n\n1.  **定义算子**：为每个指定的数学算子创建函数：\n    *   `circular_translate` ($T$ 和 $U$)：使用 `numpy.roll` 实现，它在给定轴上执行所需的循环平移。\n    *   `circular_convolve` ($C_K$)：使用 `scipy.signal.convolve2d` 实现，并设置 `mode='same'` 和 `boundary='wrap'` 以匹配循环卷积的定义。\n    *   `relu` ($\\phi$)：使用 `numpy.maximum` 实现，它执行逐元素操作 $\\max(0,z)$。\n    *   `max_pool` ($P_s$)：通过将 $N \\times N$ 输入数组重塑为 $(N/s, N/s, s, s)$ 块，然后取每个 $s \\times s$ 块内的最大值来实现。\n\n2.  **定义CNN管道 ($F$)**：一个 `cnn_pipeline` 函数按照指定顺序组合上述算子：$F(x) = P_s(\\phi(C_K(x)))$。\n\n3.  **相位扫描与对易子计算**：\n    *   生成一个大小为 $N \\times N$ 的基础棋盘格模式 $B$。\n    *   一个嵌套循环遍历所有可能的输入相位 $p, q \\in \\{0, 1, \\dots, s-1\\}$。\n    *   在每次迭代中，通过对基础棋盘格应用 `circular_translate` 来创建一个相位平移的输入 $x = B_{p,q}$。\n    *   计算两条路径：\n        *   路径1：`F_Tx = cnn_pipeline(circular_translate(x, du, dv), ...)`\n        *   路径2：`Fx = cnn_pipeline(x, ...)`，`delta_u = du // s`，`delta_v = dv // s`，`U_Fx = downsampled_shift(Fx, delta_u, delta_v)`\n    *   使用 `numpy.linalg.norm` 计算差值 `F_Tx - U_Fx` 和基线 `Fx` 的L2范数。\n    *   根据公式计算对易子模长 $M$，使用给定的 $\\varepsilon = 10^{-12}$。存储所有相位计算出的模长。\n\n4.  **聚合与输出**：\n    *   在一个测试用例的所有相位迭代完成后，使用 `numpy.mean` 和 `numpy.max` 计算收集到的对易子模长的均值和最大值。\n    *   将这两个聚合值格式化为 $6$ 位小数并存储。\n    *   最后，将所有测试用例的结果格式化为指定的字符串格式 `[[mean_A, max_A], [mean_B, max_B], ...]` 并打印到标准输出。\n\n这个设计将问题的数学公式直接转化为一个可验证的数值实验。合理性检查，例如当输入平移 $(\\Delta u, \\Delta v)$ 是池化步长 $s$ 的整数倍或零时，应该产生 $0.0$ 的对易子模长，这证实了在此背景下对等变性的理论理解。", "answer": "```python\nimport numpy as np\nfrom scipy.signal import convolve2d\n\ndef solve():\n    \"\"\"\n    Computes the mean and maximum commutator magnitudes to quantify translation\n    equivariance breaks in a simple CNN pipeline for a suite of test cases.\n    \"\"\"\n\n    def circular_translate(x: np.ndarray, du: int, dv: int) - np.ndarray:\n        \"\"\"\n        Applies a circular translation T_{(\\Delta u, \\Delta v)} to an image x.\n        (T_(\\Delta u,\\Delta v) x)[u,v] = x[(u-\\Delta u) mod N, (v-\\Delta v) mod N]\n        \"\"\"\n        return np.roll(x, shift=(du, dv), axis=(0, 1))\n\n    def circular_convolve(x: np.ndarray, K: np.ndarray) - np.ndarray:\n        \"\"\"\n        Applies circular convolution C_K with kernel K to an image x.\n        \"\"\"\n        return convolve2d(x, K, mode='same', boundary='wrap')\n\n    def relu(x: np.ndarray) - np.ndarray:\n        \"\"\"\n        Applies the pointwise Rectified Linear Unit (ReLU) function.\n        \"\"\"\n        return np.maximum(0, x)\n\n    def max_pool(x: np.ndarray, s: int) - np.ndarray:\n        \"\"\"\n        Applies non-overlapping max pooling P_s with window size s x s.\n        \"\"\"\n        N = x.shape[0]\n        N_out = N // s\n        reshaped = x.reshape(N_out, s, N_out, s)\n        # Swap axes to group pooling windows together: (N_out, N_out, s, s)\n        pooled = reshaped.swapaxes(1, 2)\n        return pooled.max(axis=(2, 3))\n\n    def downsampled_shift(y: np.ndarray, du: int, dv: int) - np.ndarray:\n        \"\"\"\n        Applies a circular translation U_{(\\delta_u, \\delta_v)} on a pooled output.\n        \"\"\"\n        return np.roll(y, shift=(du, dv), axis=(0, 1))\n\n    def cnn_pipeline(x: np.ndarray, K: np.ndarray, s: int) - np.ndarray:\n        \"\"\"\n        Computes the full pipeline F = P_s o phi o C_K.\n        \"\"\"\n        convolved = circular_convolve(x, K)\n        activated = relu(convolved)\n        pooled = max_pool(activated, s)\n        return pooled\n\n    def generate_checkerboard(N: int) - np.ndarray:\n        \"\"\"\n        Generates a binary checkerboard pattern of size N x N.\n        \"\"\"\n        u, v = np.meshgrid(np.arange(N), np.arange(N), indexing='ij')\n        board = ((u + v) % 2) == 0\n        return board.astype(float)\n\n    test_cases = [\n        # (N, s, (du, dv))\n        (24, 2, (1, 0)),  # Case A\n        (24, 2, (2, 0)),  # Case B\n        (24, 3, (1, 1)),  # Case C\n        (24, 3, (3, 0)),  # Case D\n        (24, 3, (0, 0)),  # Case E\n    ]\n    \n    K = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=float)\n    epsilon = 1e-12\n    \n    final_results = []\n    \n    for case_params in test_cases:\n        N, s, (du, dv) = case_params\n        \n        commutator_magnitudes = []\n        base_board = generate_checkerboard(N)\n        \n        for p in range(s):\n            for q in range(s):\n                # 1. Generate phase-shifted input x = B_{p,q}\n                x = circular_translate(base_board, p, q)\n                \n                # 2. Compute \"shift-then-process\" path: F(T(x))\n                shifted_x = circular_translate(x, du, dv)\n                F_Tx = cnn_pipeline(shifted_x, K, s)\n                \n                # 3. Compute \"process-then-shift\" path: U(F(x))\n                Fx = cnn_pipeline(x, K, s)\n                delta_u = du // s\n                delta_v = dv // s\n                U_Fx = downsampled_shift(Fx, delta_u, delta_v)\n                \n                # 4. Calculate the commutator magnitude M(x; du, dv)\n                norm_diff = np.linalg.norm(F_Tx - U_Fx)\n                norm_Fx = np.linalg.norm(Fx)\n                \n                M = norm_diff / (norm_Fx + epsilon)\n                commutator_magnitudes.append(M)\n        \n        # 5. Aggregate results (mean and max) for the test case\n        mean_M = np.mean(commutator_magnitudes)\n        max_M = np.max(commutator_magnitudes)\n        \n        final_results.append([mean_M, max_M])\n        \n    # 6. Format and print the final output\n    formatted_pairs = []\n    for mean_val, max_val in final_results:\n        formatted_pairs.append(f\"[{mean_val:.6f}, {max_val:.6f}]\")\n    \n    print(f\"[{', '.join(formatted_pairs)}]\")\n\nsolve()\n```", "id": "3196094"}, {"introduction": "前面的练习揭示了等变性破坏的机制，但这些理论上的偏差在实践中究竟意味着什么？本练习将理论与实际应用联系起来，通过构建一个简单的图像分类任务来回答这个问题。你将发现，仅仅因为一个特征在图像中的位置不同（例如，在中心或在边缘），不同的填充策略（如“valid”、“zero-padding”或“reflection-padding”）就可能导致CNN对其做出完全不同的分类判断。这个“对抗性”实验直观地展示了为何我们必须关注这些看似微小的实现细节，因为它们直接关系到模型预测的稳定性和可靠性。[@problem_id:3126196]", "problem": "您的指令是构建利用卷积神经网络 (CNN) 中边界条件的对抗性输入。请从二维离散卷积、平移和感受野的基本定义开始，然后推断边界条件如何影响图像边缘附近的平移等变性和有效感受野。\n\n基础知识：\n- 将有限图像信号 $x \\in \\mathbb{R}^{H \\times W}$ 与有限核 $k \\in \\mathbb{R}^{m \\times n}$ 在无限网格上的二维离散卷积定义为\n$$\n(y \\ast k)[i,j] \\triangleq \\sum_{u=0}^{m-1}\\sum_{v=0}^{n-1} k[u,v]\\; y[i+u-\\lfloor m/2 \\rfloor, j+v-\\lfloor n/2 \\rfloor],\n$$\n其中 $y$ 是通过边界条件对 $x$ 进行的扩展。扩展 $y$ 由一种填充方案定义，例如零填充或反射填充。无填充的情况仅在索引有效时进行计算。输出位置的感受野是输入中通过卷积和影响该输出的子集。平移算子 $T_{\\Delta i, \\Delta j}$ 作用于 $x$ 的方式为 $(T_{\\Delta i, \\Delta j}x)[i,j] = x[i-\\Delta i,j-\\Delta j]$。在没有边界截断的理想无限网格情况下，卷积是平移等变的，即 $(T_{\\Delta i, \\Delta j}(x) \\ast k) = T_{\\Delta i, \\Delta j}(x \\ast k)$。\n\n任务：\n- 实现一个单层卷积神经网络 (CNN; Convolutional Neural Network)，该网络包含一个所有条目均为 $1$ 的 $3 \\times 3$ 卷积核 $K$、一个恒等非线性函数和一个全局平均池化层，最终产生一个标量分数 $s$。具体来说，对于一个输出特征图 $z$，将池化分数定义为\n$$\ns \\triangleq \\frac{1}{|z|}\\sum_{i,j} z[i,j],\n$$\n其中 $|z|$ 是特征图中的元素数量。使用一个二元分类器，如果 $s \\ge \\tau$ 则输出标签 $1$，否则输出 $0$，阈值固定为 $\\tau = 1.13$。\n\n- 构建输入图像 $x \\in \\mathbb{R}^{8 \\times 8}$，其背景强度为 $0$，并在指定坐标 $(i_0,j_0)$ 处放置一个强度为 $1$、大小为 $3 \\times 3$ 的明亮方形特征，即\n$$\nx[i,j] = \\begin{cases}\n1  \\text{if } i_0 \\le i \\le i_0+2 \\text{ and } j_0 \\le j \\le j_0+2,\\\\\n0  \\text{otherwise.}\n\\end{cases}\n$$\n\n- 在三种边界条件下评估该网络：\n    1. 使用零填充的相同卷积 (\"same_zero\")，即在卷积前用一个 $1$ 像素宽的零边界填充图像，输出尺寸为 $8 \\times 8$。\n    2. 使用反射填充的相同卷积 (\"same_reflect\")，即在卷积前用一个 $1$ 像素宽的反射边界填充图像，输出尺寸为 $8 \\times 8$。\n    3. 有效卷积 (\"valid\")，即不应用任何填充，仅在 $3 \\times 3$ 的核完全适合图像内部的位置进行卷积，从而得到一个 $6 \\times 6$ 大小的输出。\n\n- 对于每个测试用例，使用两个指定的边界条件 $A$ 和 $B$ 计算两个分数 $s_A$ 和 $s_B$，用阈值 $\\tau$进行分类，并返回：\n    - 一个布尔值，指示在 $A$ 下的预测类别是否与在 $B$ 下的预测类别不同。\n    - 边际差异 $s_B - s_A$，作为一个浮点数。\n\n测试套件：\n- 使用以下参数值，每个测试用例编码为 $(H, k, i_0, j_0, A, B)$:\n    1. $(8, 3, 2, 2, \\text{same\\_zero}, \\text{same\\_reflect})$ — 特征靠近中心（理想情况）。\n    2. $(8, 3, 2, 5, \\text{same\\_zero}, \\text{same\\_reflect})$ — 特征接触右边缘。\n    3. $(8, 3, 0, 0, \\text{valid}, \\text{same\\_reflect})$ — 特征位于左上角，比较有效卷积与反射填充。\n    4. $(8, 3, 5, 2, \\text{valid}, \\text{same\\_zero})$ — 特征接触底边缘，比较有效卷积与零填充。\n\n输出规范：\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的列表的列表形式的结果，每个内部列表的形式为 $[\\text{flip}, \\text{margin}]$。例如，输出必须看起来像\n$$\n[[\\text{bool}_1,\\text{float}_1],[\\text{bool}_2,\\text{float}_2],[\\text{bool}_3,\\text{float}_3],[\\text{bool}_4,\\text{float}_4]].\n$$\n不涉及物理单位或角度。所有数值输出必须是普通的布尔值和浮点数。程序必须是自包含的，并且不需要任何输入。", "solution": "该问题陈述经评估是有效的。它在科学上基于卷积神经网络的原理，问题提出得当，所有必要参数均已定义，且表述客观。它提出了一个清晰的计算任务，旨在探讨不同边界条件对一个简单CNN输出的影响，这是深度学习研究中的一个标准课题。\n\n解决方案将通过分步实现指定的单层CNN来构建。该网络包括三个阶段：带指定填充方案的卷积、全局平均池化和基于阈值的分类。\n\n首先，我们建立核心计算组件。\n\n**1. 输入图像生成**\n对于每个测试用例，生成一个输入图像 $x \\in \\mathbb{R}^{8 \\times 8}$。该图像由强度为 $0$ 的恒定背景和一个强度为 $1$ 的 $3 \\times 3$ 方形特征组成。该特征的左上角位于坐标 $(i_0, j_0)$。\n图像 $x$ 定义为：\n$$\nx[i,j] = \\begin{cases}\n1  \\text{if } i_0 \\le i \\le i_0+2 \\text{ and } j_0 \\le j \\le j_0+2 \\\\\n0  \\text{otherwise}\n\\end{cases}\n$$\n对于 $i, j \\in \\{0, 1, \\dots, 7\\}$。\n\n**2. 带填充的卷积层**\n卷积层使用一个 $3 \\times 3$ 的核 $K$，其中所有条目均为 $1$。执行的操作是二维离散互相关，在此处等同于卷积，因为核是对称的。问题中的卷积定义为：\n$$\nz[i,j] = (y \\ast k)[i,j] = \\sum_{u=0}^{2}\\sum_{v=0}^{2} K[u,v]\\; y[i+u-1, j+v-1]\n$$\n其中 $y$ 是填充后的输入图像。此操作可以使用标准库函数如 `scipy.signal.correlate2d` 来实现，该函数计算 $z[i,j] = \\sum_{u,v} K[u,v] y[i+u, j+v]$。其差异仅在于输出坐标的简单平移，这不影响输出特征图 $z$ 中的值的集合。由于后续步骤是全局平均池化，此平移对最终分数没有影响。\n\n问题指定了三种填充方案（`A`, `B`）：\n\n- **`same_zero`**：将 $8 \\times 8$ 的输入图像 $x$ 用一个 $1$ 像素宽的零边界进行填充，生成一个 $10 \\times 10$ 的图像 $y$。然后对 $y$ 执行'有效'相关运算，得到一个大小为 $(10-3+1) \\times (10-3+1) = 8 \\times 8$ 的输出特征图 $z$。\n- **`same_reflect`**：输入图像 $x$ 用一个 $1$ 像素宽的边缘值反射边界进行填充。这也产生一个 $10 \\times 10$ 的图像 $y$。对 $y$ 执行'有效'相关运算，得到一个 $8 \\times 8$ 的特征图 $z$。\n- **`valid`**：不应用任何填充。直接对 $8 \\times 8$ 的输入图像 $x$ 执行相关运算。得到的特征图 $z$ 的大小为 $(8-3+1) \\times (8-3+1) = 6 \\times 6$。\n\n**3. 全局平均池化**\n标量分数 $s$ 通过计算输出特征图 $z$ 中所有元素的算术平均值得到：\n$$\ns = \\frac{1}{|z|}\\sum_{i,j} z[i,j]\n$$\n此处， $|z|$ 表示 $z$ 中的元素总数。关键是，对于 `same_zero` 和 `same_reflect` 填充， $|z|$ 是 $8 \\times 8 = 64$；但对于 `valid` 填充，则是 $6 \\times 6 = 36$。分母的这种差异是计算的一个关键部分。\n\n**4. 分类与比较**\n对于每个测试用例，我们使用两种指定的填充方案 $A$ 和 $B$ 计算两个分数 $s_A$ 和 $s_B$。然后使用这些分数来确定预测的类别：\n$$\n\\text{class}_A = \\begin{cases} 1  \\text{if } s_A \\ge \\tau \\\\ 0  \\text{if } s_A  \\tau \\end{cases} \\quad \\text{and} \\quad \\text{class}_B = \\begin{cases} 1  \\text{if } s_B \\ge \\tau \\\\ 0  \\text{if } s_B  \\tau \\end{cases}\n$$\n给定的阈值为 $\\tau = 1.13$。\n\n每个测试用例的最终输出是：\n- 一个布尔值，指示分类结果是否不同：$\\text{flip} = (\\text{class}_A \\neq \\text{class}_B)$。\n- 分数的浮点差异：$\\text{margin} = s_B - s_A$。\n\n该过程被系统地应用于问题陈述中提供的四个测试用例中的每一个。结果被收集并格式化为所需的输出字符串。例如，让我们分析测试用例3：$(i_0=0, j_0=0, A=\\text{valid}, B=\\text{same\\_reflect})$。\n\n- **输入 `x`**：一个 $8 \\times 8$ 零矩阵的左上角有一个 $3 \\times 3$ 的全1块。\n- **路径 A (`valid`)**：\n    - 无填充。对 $x$ 执行相关运算。\n    - 输出 $z_A$ 为 $6 \\times 6$。其元素总和为 $\\sum z_A[i,j] = 81$。\n    - 分数为 $s_A = \\frac{1}{|z_A|} \\sum z_A = \\frac{81}{36} = 2.25$。\n    - 由于 $2.25 \\ge 1.13$，$\\text{class}_A = 1$。\n- **路径 B (`same_reflect`)**：\n    - 输入 `x` 进行反射填充。角上的 $3 \\times 3$ 特征与填充相互作用，在填充后的图像 `y_B` 中创建一个 $4 \\times 4$ 的全1块。\n    - 对 `y_B` 进行相关运算，生成一个 $8 \\times 8$ 的特征图 $z_B$。\n    - `y_B` 中对输出有贡献的元素之和是 $16$。核权重之和是 $9$。相关运算的一个性质是，输出特征图的总和是有效输入区域权重总和与核权重总和的乘积。在这种情况下，$\\sum z_B[i,j] = 16 \\times 9 = 144$。\n    - 分数为 $s_B = \\frac{1}{|z_B|} \\sum z_B = \\frac{144}{64} = 2.25$。\n    - 由于 $2.25 \\ge 1.13$，$\\text{class}_B = 1$。\n- **用例3的结果**：\n    - $\\text{flip} = (\\text{class}_A \\neq \\text{class}_B) = (1 \\neq 1) = \\text{False}$。\n    - $\\text{margin} = s_B - s_A = 2.25 - 2.25 = 0.0$。\n要返回的对是 $[\\text{False}, 0.0]$。\n\n同样地逻辑被应用于所提供的Python代码中的所有测试用例。", "answer": "```python\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef cnn_forward_pass(x, padding_mode, kernel):\n    \"\"\"\n    Performs a single forward pass of the simple CNN for a given padding mode.\n\n    Args:\n        x (np.ndarray): The input image.\n        padding_mode (str): The padding mode ('same_zero', 'same_reflect', 'valid').\n        kernel (np.ndarray): The convolutional kernel.\n\n    Returns:\n        float: The scalar score 's'.\n    \"\"\"\n    if padding_mode == 'same_zero':\n        # Pad with a 1-pixel border of zeros. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='constant', constant_values=0)\n        # Correlate to get 8x8 output.\n        # (10x10 input corr 3x3) -> (10-3+1)x(10-3+1) = 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'same_reflect':\n        # Pad with a 1-pixel reflection. Output size will be 8x8.\n        padded_x = np.pad(x, pad_width=1, mode='reflect')\n        # Correlate to get 8x8 output.\n        z = correlate2d(padded_x, kernel, mode='valid')\n    elif padding_mode == 'valid':\n        # No padding. Output size will be 6x6.\n        # (8x8 input corr 3x3) -> (8-3+1)x(8-3+1) = 6x6 output.\n        z = correlate2d(x, kernel, mode='valid')\n    else:\n        raise ValueError(f\"Unknown padding mode: {padding_mode}\")\n\n    # Global Average Pooling\n    score = np.mean(z)\n    return score\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and generate the final output.\n    \"\"\"\n    # Problem constants\n    H, W = 8, 8\n    kernel_size = 3\n    feature_size = 3\n    tau = 1.13\n    kernel = np.ones((kernel_size, kernel_size))\n\n    # Test suite: (i0, j0, mode_A, mode_B)\n    test_cases = [\n        (2, 2, 'same_zero', 'same_reflect'),\n        (2, 5, 'same_zero', 'same_reflect'),\n        (0, 0, 'valid', 'same_reflect'),\n        (5, 2, 'valid', 'same_zero')\n    ]\n\n    results = []\n    for i0, j0, mode_A, mode_B in test_cases:\n        # 1. Construct input image x\n        x = np.zeros((H, W), dtype=np.float64)\n        x[i0 : i0 + feature_size, j0 : j0 + feature_size] = 1.0\n\n        # 2. Evaluate for condition A\n        s_A = cnn_forward_pass(x, mode_A, kernel)\n        class_A = 1 if s_A = tau else 0\n\n        # 3. Evaluate for condition B\n        s_B = cnn_forward_pass(x, mode_B, kernel)\n        class_B = 1 if s_B = tau else 0\n\n        # 4. Compare results\n        classification_flipped = (class_A != class_B)\n        margin_difference = s_B - s_A\n\n        results.append([classification_flipped, margin_difference])\n\n    # Format the final output string exactly as specified.\n    # [ [bool_1,float_1], [bool_2,float_2], ... ]\n    # We build the string manually to avoid spaces introduced by str(list).\n    inner_strings = []\n    for flip, margin in results:\n        # Python's str(bool) is 'True'/'False'. The problem states \"plain booleans\".\n        # This is the most direct interpretation.\n        inner_strings.append(f\"[{str(flip)},{margin}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3126196"}]}