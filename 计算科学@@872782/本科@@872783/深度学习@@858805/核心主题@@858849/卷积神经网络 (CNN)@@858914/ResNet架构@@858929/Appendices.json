{"hands_on_practices": [{"introduction": "残差网络的核心思想是学习对恒等映射（identity mapping）的修正，即学习残差函数 $F(\\mathbf{x})$ 而非直接学习目标函数 $H(\\mathbf{x})$。这个练习 [@problem_id:3169949] 旨在通过一个精心设计的分类任务，直观地展示残差连接的威力。你将通过编程模拟一个场景：仅靠恒等变换不足以做出正确决策，而一个微小的残差修正却能显著提升分类准确率，从而深刻理解残差学习的本质。", "problem": "考虑一个为模拟残差网络 (ResNet) 中单个残差块而构建的二元分类任务。其中，残差块计算一个形式为 $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$ 的映射，$I(\\mathbf{x})$ 是一个已有的输入表示（即恒等映射），而 $F(\\mathbf{x})$ 是一个学习到的修正项。请使用以下基本设定：分类决策是通过将一个标量 logit 在 $0$ 处进行阈值化处理来做出的，分类准确率是所有样本中正确预测的比例。真实的决策边界由一个带有加性噪声的线性函数的符号定义。您的任务是形式化地描述输入接近决策边界、使得残差修正起决定性作用的情形，并量化在恒等基线 $I$ 的基础上增加残差 $F$ 所带来的准确率提升。\n\n数据集与决策定义：\n- 令 $\\mathbf{x} = (x_1, x_2) \\in \\mathbb{R}^2$ 为输入。对于一个大小为 $N$ 的数据集，独立地从 $x_1 \\sim \\mathrm{Uniform}(-\\delta, \\delta)$ 和 $x_2 \\sim \\mathrm{Uniform}(-1, 1)$ 中采样，其中 $\\delta  0$ 很小，从而使得输入接近由 $x_1 = 0$ 定义的决策边界。\n- 令 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 为独立的加性噪声。\n- 真实标签定义为 $y = \\mathbb{I}\\{x_1 + \\alpha\\, x_2 + \\varepsilon \\ge 0\\}$，其中 $\\alpha \\in \\mathbb{R}$ 控制与 $x_2$ 相关联的残差分量的强度，$\\mathbb{I}\\{\\cdot\\}$ 表示指示函数，当条件为真时返回 $1$，否则返回 $0$。\n- 恒等基线分类器使用 logit $I(\\mathbf{x}) = x_1$ 并预测 $\\hat{y}_I = \\mathbb{I}\\{I(\\mathbf{x}) \\ge 0\\}$。\n- 残差化分类器使用 logit $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$，其中 $F(\\mathbf{x}) = \\alpha\\, x_2$，并预测 $\\hat{y}_R = \\mathbb{I}\\{H(\\mathbf{x}) \\ge 0\\}$。\n\n对于每个数据集，计算恒等基线准确率\n$$\\mathrm{Acc}_I = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_I^{(i)} = y^{(i)}\\},$$\n残差化准确率\n$$\\mathrm{Acc}_R = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_R^{(i)} = y^{(i)}\\},$$\n以及准确率提升\n$$\\Delta \\mathrm{Acc} = \\mathrm{Acc}_R - \\mathrm{Acc}_I.$$\n\n您的程序必须使用带有指定种子的伪随机数生成器来确定性地生成数据集，并为以下每个测试用例生成准确率提升 $\\Delta \\mathrm{Acc}$：\n- 测试用例 1：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.05, 0.5, 0.0, 42)$。\n- 测试用例 2：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.05, 0.0, 0.0, 123)$。\n- 测试用例 3：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 10^{-6}, 0.5, 0.0, 7)$。\n- 测试用例 4：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.5, 0.5, 0.0, 99)$。\n- 测试用例 5：$(N, \\delta, \\alpha, \\sigma, \\text{seed}) = (5000, 0.05, 0.5, 0.3, 2023)$。\n\n覆盖性设计：\n- 第一个用例是典型的近边界场景，在此场景中残差修正应该有所帮助。\n- 第二个用例设置 $\\alpha = 0$，因此残差不增加任何信息，可作为基线边缘情况。\n- 第三个用例通过设置 $\\delta = 10^{-6}$ 使输入极其接近决策边界。\n- 第四个用例通过增加 $\\delta$ 放宽了近边界条件，以测试对残差修正的依赖性降低的情况。\n- 第五个用例通过 $\\sigma  0$ 引入噪声，以测试当标签被破坏时的鲁棒性。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目是相应测试用例计算出的 $\\Delta \\mathrm{Acc}$，四舍五入到 $6$ 位小数（例如，$[0.123456,0.000000,0.250000,0.050000,0.010000]$）。不应打印任何其他文本。", "solution": "该问题要求我们在一个简化的二元分类设置中，量化由单个残差连接带来的性能提升。该场景旨在模拟一个 ResNet 残差块，其中恒等映射 $I(\\mathbf{x})$ 被一个残差函数 $F(\\mathbf{x})$ 增强，以形成一个新的映射 $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$。我们将模拟这个过程，计算基线模型（仅恒等映射）和残差化模型的分类准确率，然后确定准确率的提升 $\\Delta \\mathrm{Acc}$。整个过程将通过使用带种子的伪随机数生成器对多个测试用例进行确定性地执行。\n\n解决方案针对每个由参数 $(N, \\delta, \\alpha, \\sigma, \\text{seed})$ 定义的测试用例，通过以下步骤进行。\n\n1.  **数据集生成**：首先，我们生成一个包含 $N$ 个样本的数据集。每个样本由一个输入向量 $\\mathbf{x}^{(i)} = (x_1^{(i)}, x_2^{(i)})$ 和一个对应的真实标签 $y^{(i)}$ 组成。\n    -   使用指定的 `seed` 初始化一个伪随机数生成器，以确保结果的确定性和可复现性。\n    -   输入分量从独立的均匀分布中采样：$x_1^{(i)} \\sim \\mathrm{Uniform}(-\\delta, \\delta)$ 和 $x_2^{(i)} \\sim \\mathrm{Uniform}(-1, 1)$，其中 $i=1, \\dots, N$。参数 $\\delta$ 控制输入到决策边界 $x_1 = 0$ 的接近程度。\n    -   一个独立的加性噪声项 $\\varepsilon^{(i)}$ 从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中采样。参数 $\\sigma$ 是噪声的标准差。\n    -   真实标签 $y^{(i)}$ 由输入的带噪声线性组合的符号确定。它使用指示函数 $\\mathbb{I}\\{\\cdot\\}$ 定义如下：\n        $$y^{(i)} = \\mathbb{I}\\{x_1^{(i)} + \\alpha\\, x_2^{(i)} + \\varepsilon^{(i)} \\ge 0\\}$$\n        如果条件为真，该函数返回 $1$，否则返回 $0$。\n\n2.  **模型预测**：我们定义两个分类器，并为数据集中的每个样本生成它们的预测。\n    -   **恒等基线分类器**仅使用输入的 $x_1$ 分量。其 logit 为 $I(\\mathbf{x}) = x_1$，其预测 $\\hat{y}_I$ 基于该 logit 的符号：\n        $$\\hat{y}_I^{(i)} = \\mathbb{I}\\{I(\\mathbf{x}^{(i)}) \\ge 0\\} = \\mathbb{I}\\{x_1^{(i)} \\ge 0\\}$$\n    -   **残差化分类器**将来自 $x_2$ 的贡献作为残差修正项并入。其 logit 为 $H(\\mathbf{x}) = I(\\mathbf{x}) + F(\\mathbf{x})$，其中恒等映射为 $I(\\mathbf{x}) = x_1$，残差函数为 $F(\\mathbf{x}) = \\alpha\\, x_2$。预测值 $\\hat{y}_R$ 为：\n        $$\\hat{y}_R^{(i)} = \\mathbb{I}\\{H(\\mathbf{x}^{(i)}) \\ge 0\\} = \\mathbb{I}\\{x_1^{(i)} + \\alpha\\, x_2^{(i)} \\ge 0\\}$$\n        该模型有效地尝试学习真实的决策边界，但无法获取噪声项 $\\varepsilon$。\n\n3.  **性能评估**：我们计算每个分类器的准确率以及由此产生的提升。\n    -   分类器的准确率是正确分类样本的比例。恒等基线准确率 $\\mathrm{Acc}_I$ 计算如下：\n        $$\\mathrm{Acc}_I = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_I^{(i)} = y^{(i)}\\}$$\n    -   同样，残差化准确率 $\\mathrm{Acc}_R$ 为：\n        $$\\mathrm{Acc}_R = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\{\\hat{y}_R^{(i)} = y^{(i)}\\}$$\n    -   我们最终关注的指标是准确率提升 $\\Delta \\mathrm{Acc}$，定义为两个准确率之差：\n        $$\\Delta \\mathrm{Acc} = \\mathrm{Acc}_R - \\mathrm{Acc}_I$$\n        $\\Delta \\mathrm{Acc}$ 的正值表示在给定条件下，残差连接为分类性能提供了切实的益处。\n\n对于无噪声情况（$\\sigma = 0$），真实标签为 $y^{(i)} = \\mathbb{I}\\{x_1^{(i)} + \\alpha x_2^{(i)} \\ge 0\\}$。残差化分类器的预测 $\\hat{y}_R^{(i)}$ 与真实标签 $y^{(i)}$ 完全相同。因此，在这种情况下，$\\mathrm{Acc}_R$ 将恰好为 $1$。模拟中任何与此的偏差都将归因于浮点精度问题，但预计是可忽略不计的。准确率提升则变为 $\\Delta \\mathrm{Acc} = 1 - \\mathrm{Acc}_I$。\n\n所提供的测试用例探究了模型在不同机制下的行为：\n-   **用例 1 ($\\alpha=0.5$, $\\delta=0.05$, $\\sigma=0.0$)：** 一个标准场景，其中残差项包含有用信息，输入接近恒等边界，且没有标签噪声。我们预计会得到一个显著的正值 $\\Delta \\mathrm{Acc}$。\n-   **用例 2 ($\\alpha=0.0$, $\\delta=0.05$, $\\sigma=0.0$)：** 在此情况下，残差项 $F(\\mathbf{x}) = 0 \\cdot x_2 = 0$。两个分类器变得完全相同，因此必然有 $\\mathrm{Acc}_I = \\mathrm{Acc}_R$，导致 $\\Delta \\mathrm{Acc} = 0$。\n-   **用例 3 ($\\alpha=0.5$, $\\delta=10^{-6}$, $\\sigma=0.0$)：** 由于 $\\delta$ 极小，输入非常紧密地聚集在 $x_1=0$ 边界周围。恒等分类器的性能预计会接近随机猜测的水平（准确率 0.5），使得残差项至关重要。$\\Delta \\mathrm{Acc}$ 应接近 0.5。\n-   **用例 4 ($\\alpha=0.5$, $\\delta=0.5$, $\\sigma=0.0$)：** 随着 $\\delta$ 增大，更大比例的输入远离 $x_1=0$ 边界。对于这些点，$x_1$ 的符号能更好地预测 $x_1 + \\alpha x_2$ 的符号，从而提高了 $\\mathrm{Acc}_I$ 并减少了*相对*提升 $\\Delta \\mathrm{Acc}$。\n-   **用例 5 ($\\alpha=0.5$, $\\delta=0.05$, $\\sigma=0.3$)：** 引入显著的标签噪声（$\\sigma  0$）使问题在根本上是随机的。相对于无噪声的边界，真实标签的符号可能会翻转。两个分类器的准确率预计都会下降，但残差分类器由于与底层数据结构更好地对齐，应能保持优势，从而产生一个正的 $\\Delta \\mathrm{Acc}$。\n\n以下程序实现了计算每个测试用例的 $\\Delta \\mathrm{Acc}$ 的完整过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the accuracy improvement from a residual connection in a simplified\n    binary classification task for a set of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (N, delta, alpha, sigma, seed)\n    test_cases = [\n        (5000, 0.05, 0.5, 0.0, 42),\n        (5000, 0.05, 0.0, 0.0, 123),\n        (5000, 1e-6, 0.5, 0.0, 7),\n        (5000, 0.5, 0.5, 0.0, 99),\n        (5000, 0.05, 0.5, 0.3, 2023),\n    ]\n\n    results = []\n    for N, delta, alpha, sigma, seed in test_cases:\n        # 1. Dataset Generation\n        # Initialize a pseudorandom number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Sample x1 and x2 from their respective uniform distributions.\n        x1 = rng.uniform(-delta, delta, size=N)\n        x2 = rng.uniform(-1, 1, size=N)\n\n        # Sample additive noise from a normal distribution. scale=sigma (std dev).\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=N)\n\n        # Determine ground-truth labels.\n        # y = I{x1 + alpha*x2 + epsilon = 0}\n        ground_truth_logit = x1 + alpha * x2 + epsilon\n        y_true = (ground_truth_logit >= 0).astype(int)\n\n        # 2. Model Predictions\n        # Identity-baseline classifier prediction.\n        # y_hat_I = I{x1 = 0}\n        y_pred_I = (x1 >= 0).astype(int)\n\n        # Residualized classifier prediction.\n        # y_hat_R = I{x1 + alpha*x2 = 0}\n        y_pred_R = (x1 + alpha * x2 >= 0).astype(int)\n\n        # 3. Performance Evaluation\n        # Calculate accuracy for the identity-baseline classifier.\n        acc_I = np.mean(y_pred_I == y_true)\n\n        # Calculate accuracy for the residualized classifier.\n        acc_R = np.mean(y_pred_R == y_true)\n\n        # Calculate the accuracy improvement.\n        delta_acc = acc_R - acc_I\n        \n        results.append(delta_acc)\n\n    # Format results to 6 decimal places and print in the specified format.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3169949"}, {"introduction": "理解了残差连接的有效性之后，一个自然的问题是：模型在多大程度上依赖于快捷连接（shortcut connection）？本练习 [@problem_id:3169993] 将指导你设计一个诊断工具来回答这个问题。通过对模型的不同路径注入噪声并分析其性能变化，你将推导并实现一个“快捷连接依赖指数”（Shortcut Reliance Index, SRI），从而学会一种量化分析网络组件重要性的强大方法。", "problem": "给定一个来自残差网络（ResNet）的单残差块。该块通过规则 $y = f(x; B) + x$ 将输入向量 $x \\in \\mathbb{R}^d$ 映射到输出向量 $y \\in \\mathbb{R}^d$，其中 $f(\\cdot; B)$ 是由矩阵 $B \\in \\mathbb{R}^{d \\times d}$ 参数化的残差变换。对于本问题，假设残差分支是线性的，即 $f(x; B) = B x$。考虑一个带有目标 $t \\in \\mathbb{R}^d$ 的监督预测任务，并将损失定义为均方误差（MSE），即 $L = \\mathbb{E}\\left[\\lVert y - t \\rVert_2^2 \\right]$，其中期望是关于数据分布的。您需要设计一种诊断方法，通过测量损失对微小加性扰动的敏感性，来量化该块对恒等快捷连接（identity shortcut）相对于残差分支的依赖程度。\n\n待实现的诊断定义：\n1) 对于固定的噪声尺度 $\\epsilon  0$，定义一个仅跳跃（skip-only）扰动，在加法器处向快捷路径添加零均值高斯噪声：受扰动的输出为 $y_{\\text{skip}}(\\epsilon) = Bx + \\left( x + \\epsilon z \\right)$，其中 $z \\sim \\mathcal{N}(0, I_d)$ 且 $I_d$ 是 $d \\times d$ 的单位矩阵。将跳跃扰动性能下降定义为 $\\Delta L_{\\text{skip}}(\\epsilon) = \\mathbb{E}\\left[\\lVert y_{\\text{skip}}(\\epsilon) - t \\rVert_2^2\\right] - \\mathbb{E}\\left[\\lVert (Bx + x) - t \\rVert_2^2 \\right]$。\n2) 对于相同的 $\\epsilon$，定义一个输入扰动（input-perturbation），在分支前向块输入添加相同类型的噪声：受扰动的输出为 $y_{\\text{in}}(\\epsilon) = B(x + \\epsilon z) + (x + \\epsilon z)$。将输入扰动性能下降定义为 $\\Delta L_{\\text{in}}(\\epsilon) = \\mathbb{E}\\left[\\lVert y_{\\text{in}}(\\epsilon) - t \\rVert_2^2\\right] - \\mathbb{E}\\left[\\lVert (Bx + x) - t \\rVert_2^2 \\right]$。\n3) 将快捷连接依赖指数（Shortcut Reliance Index, SRI）定义为小 $\\epsilon$ 下的比率 $SRI = \\dfrac{\\Delta L_{\\text{skip}}(\\epsilon)}{\\Delta L_{\\text{in}}(\\epsilon)}$。该比率分离了损失对仅跳跃噪声与输入噪声的相对敏感性，并且需要对线性残差块进行精确计算。\n\n从第一性原理出发，仅使用上述定义以及高斯随机变量的标准性质，推导一个计算效率高的表达式，用于在不执行随机模拟的情况下，对任意给定的 $B$ 和 $d$ 计算 $SRI$。然后实现一个程序，在以下测试集上评估 $SRI$。对于每个案例，请使用指定的参数：\n- 测试案例 1：$d = 3$, $B = 0.5 \\, I_d$, $\\epsilon = 0.1$。\n- 测试案例 2：$d = 3$, $B = 0 \\, I_d$, $\\epsilon = 0.1$。\n- 测试案例 3：$d = 3$, $B = \\mathrm{diag}(1, -0.5, 0)$, $\\epsilon = 0.1$。\n- 测试案例 4：$d = 3$, $B = 4 \\, I_d$, $\\epsilon = 0.1$。\n- 测试案例 5：$d = 3$, $B = -0.9 \\, I_d$, $\\epsilon = 0.1$。\n\n您的程序必须输出一行，其中包含按案例 1 到 5 的顺序为每个测试案例计算出的 $SRI$ 值，形式为用逗号分隔并包含在方括号内的列表（例如，$[a,b,c,d,e]$）。所有输出必须是实数值。本问题不涉及物理单位。", "solution": "本问题要求推导线性残差块的快捷连接依赖指数（SRI），并针对一组给定参数进行求值。SRI 定义为仅跳跃路径噪声注入导致的损失增加与块输入噪声注入导致的损失增加之比。我们将从第一性原理推导这个量。\n\n设块的输入为 $x \\in \\mathbb{R}^d$，目标为 $t \\in \\mathbb{R}^d$，线性残差分支由矩阵 $B \\in \\mathbb{R}^{d \\times d}$ 定义。块的输出为 $y = Bx + x = (B+I_d)x$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。损失函数是均方误差，$L = \\mathbb{E}\\left[\\lVert y - t \\rVert_2^2 \\right]$，其中期望是关于 $(x, t)$ 的数据分布以及任何注入噪声的。未受扰动的损失为 $L_0 = \\mathbb{E}_{x,t}\\left[\\lVert (B+I_d)x - t \\rVert_2^2 \\right]$。我们定义给定数据樣本的误差向量为 $e = (B+I_d)x - t$。那么 $L_0 = \\mathbb{E}_{x,t}[\\lVert e \\rVert_2^2]$。\n\n首先，我们分析仅跳跃扰动。受扰动的输出由 $y_{\\text{skip}}(\\epsilon) = Bx + (x + \\epsilon z) = (B+I_d)x + \\epsilon z$ 给出，其中 $z \\sim \\mathcal{N}(0, I_d)$ 是独立同分布（i.i.d.）的标准高斯随机变量向量，与 $x$ 和 $t$ 无关。噪声尺度为 $\\epsilon  0$。\n\n带跳跃路径扰动的损失为 $L_{\\text{skip}}(\\epsilon) = \\mathbb{E}_{x,t,z}\\left[\\lVert y_{\\text{skip}}(\\epsilon) - t \\rVert_2^2\\right]$。我们可以将其写作：\n$$ L_{\\text{skip}}(\\epsilon) = \\mathbb{E}_{x,t,z}\\left[\\lVert ((B+I_d)x - t) + \\epsilon z \\rVert_2^2\\right] = \\mathbb{E}_{x,t,z}\\left[\\lVert e + \\epsilon z \\rVert_2^2\\right] $$\n展开平方L2范数，即向量与其自身的内积：\n$$ \\lVert e + \\epsilon z \\rVert_2^2 = (e + \\epsilon z)^T (e + \\epsilon z) = e^T e + 2\\epsilon e^T z + \\epsilon^2 z^T z = \\lVert e \\rVert_2^2 + 2\\epsilon e^T z + \\epsilon^2 \\lVert z \\rVert_2^2 $$\n根据期望的线性性质，我们有：\n$$ L_{\\text{skip}}(\\epsilon) = \\mathbb{E}_{x,t,z}[\\lVert e \\rVert_2^2] + 2\\epsilon \\mathbb{E}_{x,t,z}[e^T z] + \\epsilon^2 \\mathbb{E}_{x,t,z}[\\lVert z \\rVert_2^2] $$\n我们评估每一项：\n1.  $\\mathbb{E}_{x,t,z}[\\lVert e \\rVert_2^2] = \\mathbb{E}_{x,t}[\\lVert e \\rVert_2^2] = L_0$，因为 $e$ 不是 $z$ 的函数。\n2.  $\\mathbb{E}_{x,t,z}[e^T z] = \\mathbb{E}_{x,t}[\\mathbb{E}_z[e^T z | x,t]]$。由于 $e$ 相对于 $z$ 是常数，这变为 $\\mathbb{E}_{x,t}[e^T \\mathbb{E}_z[z]]$。因为 $z \\sim \\mathcal{N}(0, I_d)$，所以 $\\mathbb{E}_z[z] = 0$。因此，该项为 $0$。\n3.  $\\mathbb{E}_{x,t,z}[\\lVert z \\rVert_2^2] = \\mathbb{E}_z[\\lVert z \\rVert_2^2] = \\mathbb{E}_z[\\sum_{i=1}^d z_i^2] = \\sum_{i=1}^d \\mathbb{E}_z[z_i^2]$。对于标准正态变量 $z_i$，$\\mathbb{E}[z_i] = 0$ 且 $\\mathrm{Var}(z_i) = 1$。由于 $\\mathrm{Var}(z_i) = \\mathbb{E}[z_i^2] - (\\mathbb{E}[z_i])^2$，我们有 $\\mathbb{E}[z_i^2] = 1$。因此，$\\mathbb{E}_z[\\lVert z \\rVert_2^2] = \\sum_{i=1}^d 1 = d$。\n\n代入这些结果，我们得到 $L_{\\text{skip}}(\\epsilon) = L_0 + \\epsilon^2 d$。\n跳跃扰动性能下降定义为 $\\Delta L_{\\text{skip}}(\\epsilon) = L_{\\text{skip}}(\\epsilon) - L_0$，可简化为：\n$$ \\Delta L_{\\text{skip}}(\\epsilon) = \\epsilon^2 d $$\n\n接下来，我们分析输入擾动。受扰动的输出为 $y_{\\text{in}}(\\epsilon) = B(x+\\epsilon z) + (x+\\epsilon z) = (B+I_d)(x+\\epsilon z) = (B+I_d)x + \\epsilon(B+I_d)z$。\n带输入扰动的损失为 $L_{\\text{in}}(\\epsilon) = \\mathbb{E}_{x,t,z}\\left[\\lVert y_{\\text{in}}(\\epsilon) - t \\rVert_2^2\\right]$：\n$$ L_{\\text{in}}(\\epsilon) = \\mathbb{E}_{x,t,z}\\left[\\lVert ((B+I_d)x - t) + \\epsilon(B+I_d)z \\rVert_2^2\\right] = \\mathbb{E}_{x,t,z}\\left[\\lVert e + \\epsilon(B+I_d)z \\rVert_2^2\\right] $$\n展开平方范数：\n$$ \\lVert e + \\epsilon(B+I_d)z \\rVert_2^2 = \\lVert e \\rVert_2^2 + 2\\epsilon e^T(B+I_d)z + \\epsilon^2 z^T(B+I_d)^T(B+I_d)z $$\n取期望：\n$$ L_{\\text{in}}(\\epsilon) = \\mathbb{E}_{x,t,z}[\\lVert e \\rVert_2^2] + 2\\epsilon \\mathbb{E}_{x,t,z}[e^T(B+I_d)z] + \\epsilon^2 \\mathbb{E}_{x,t,z}[z^T(B+I_d)^T(B+I_d)z] $$\n我们评估各项：\n1.  第一项是 $L_0$。\n2.  交叉项为 $\\mathbb{E}_{x,t,z}[e^T(B+I_d)z] = \\mathbb{E}_{x,t}[e^T(B+I_d)\\mathbb{E}_z[z]] = 0$。\n3.  第三项涉及高斯向量 $z$ 的二次型。令 $M = (B+I_d)^T(B+I_d)$。期望为 $\\mathbb{E}_{x,t,z}[z^T M z] = \\mathbb{E}_z[z^T M z]$。对于均值为 $\\mu=0$、协方差为 $\\Sigma=I_d$ 的随机向量 $z$，我们有 $\\mathbb{E}_z[z^T M z] = \\mathrm{Tr}(M\\Sigma) + \\mu^T M \\mu = \\mathrm{Tr}(M I_d) + 0 = \\mathrm{Tr}(M)$。\n所以，$\\mathbb{E}_z[z^T(B+I_d)^T(B+I_d)z] = \\mathrm{Tr}((B+I_d)^T(B+I_d))$。\n\n代回后，我们发现 $L_{\\text{in}}(\\epsilon) = L_0 + \\epsilon^2 \\mathrm{Tr}((B+I_d)^T(B+I_d))$。\n输入扰动性能下降为 $\\Delta L_{\\text{in}}(\\epsilon) = L_{\\text{in}}(\\epsilon) - L_0$：\n$$ \\Delta L_{\\text{in}}(\\epsilon) = \\epsilon^2 \\mathrm{Tr}\\left((B+I_d)^T(B+I_d)\\right) $$\n项 $\\mathrm{Tr}((B+I_d)^T(B+I_d))$ 等价于矩阵 $B+I_d$ 的弗罗贝尼乌斯范数（Frobenius norm）的平方，记作 $\\lVert B+I_d \\rVert_F^2$。\n\n最后，快捷连接依赖指数（SRI）是这两个量的比值：\n$$ SRI = \\frac{\\Delta L_{\\text{skip}}(\\epsilon)}{\\Delta L_{\\text{in}}(\\epsilon)} = \\frac{\\epsilon^2 d}{\\epsilon^2 \\mathrm{Tr}\\left((B+I_d)^T(B+I_d)\\right)} $$\n$\\epsilon^2$ 项相互抵消，得到一个与噪声尺度 $\\epsilon$ 和数据分布无关的表达式。最终表达式为：\n$$ SRI = \\frac{d}{\\mathrm{Tr}\\left((B+I_d)^T(B+I_d)\\right)} = \\frac{d}{\\lVert B+I_d \\rVert_F^2} $$\n这个表达式的计算效率很高，因为它只需要矩阵加法和弗罗贝尼乌斯范数的平方计算，后者是矩阵所有元素平方和，其复杂度为 $O(d^2)$。我们现在将实现这个公式来评估指定的测试案例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Shortcut Reliance Index (SRI) for several test cases\n    of a linear residual block.\n    \"\"\"\n    \n    # Test cases are defined as tuples of (dimension, B_matrix).\n    #\n    # Test case 1: d = 3, B = 0.5 * I_d\n    # Test case 2: d = 3, B = 0 * I_d\n    # Test case 3: d = 3, B = diag(1, -0.5, 0)\n    # Test case 4: d = 3, B = 4 * I_d\n    # Test case 5: d = 3, B = -0.9 * I_d\n    \n    test_cases = [\n        (3, 0.5 * np.identity(3)),\n        (3, 0.0 * np.identity(3)),\n        (3, np.diag([1.0, -0.5, 0.0])),\n        (3, 4.0 * np.identity(3)),\n        (3, -0.9 * np.identity(3))\n    ]\n\n    results = []\n    for d, B in test_cases:\n        # The derived formula for SRI is:\n        # SRI = d / Tr((B + I)^T * (B + I))\n        # The denominator is the squared Frobenius norm of the matrix (B + I).\n        \n        # Construct the identity matrix of dimension d.\n        I = np.identity(d)\n        \n        # Compute the matrix B + I.\n        B_plus_I = B + I\n        \n        # The squared Frobenius norm ||A||_F^2 is the sum of the squares of all elements.\n        # This is computationally more efficient than matrix multiplication followed by trace.\n        # np.sum(np.square(A)) computes this directly.\n        denominator = np.sum(np.square(B_plus_I))\n        \n        # According to the problem setup, the denominator will not be zero for the\n        # given test cases. If it were, SRI would be infinite, signifying that\n        # the block is perfectly robust to input noise but not skip-path noise.\n        if denominator == 0:\n            sri = float('inf')\n        else:\n            sri = d / denominator\n            \n        results.append(sri)\n\n    # The final print statement must match the specified format: [a,b,c,d,e]\n    # We use map(str, ...) and ','.join(...) to format the list of floats.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3169993"}, {"introduction": "ResNet 的思想可以进一步扩展，例如在 ResNeXt 架构中，将残差函数分解为多个并行的变换分支。这个最终练习 [@problem_id:3169937] 旨在探索这种“聚合变换”（aggregated transformations）的设计思想。你将分析这些并行分支之间的“多样性”如何影响模型的整体性能，从而将残差网络的设计原则与集成学习理论联系起来，为构建更强大的网络结构提供深刻见解。", "problem": "给定一个带有 ResNeXt 风格聚合变换的残差网络 (ResNet) 块的形式化表示，其中块的输出被建模为并行分支的聚合。假设有 $C$ 个分支，每个分支实现一个标量变换 $F_i$，并将输入 $x$ 上的聚合变换定义为 $F(x) = \\sum_{i=1}^{C} F_i(x)$。考虑一个具有标量目标的有限数据集 $\\mathcal{D} = \\{(x_n, y_n)\\}_{n=1}^{N}$。为了进行分析，定义 $\\mathbb{R}^{N}$ 上的经验内积为 $\\langle u, v \\rangle_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^{N} u_n v_n$，以及其诱导范数为 $\\|u\\|_{\\mathcal{D}} = \\sqrt{\\langle u, u \\rangle_{\\mathcal{D}}}$。将整个数据集上的分支输出堆叠成一个矩阵 $Z \\in \\mathbb{R}^{N \\times C}$，其元素为 $Z_{n,i} = F_i(x_n)$，并定义目标向量 $y \\in \\mathbb{R}^{N}$，其元素为 $y_n$。令 $z_i \\in \\mathbb{R}^{N}$ 表示 $Z$ 的第 $i$ 列，经验残差为 $r_i = z_i - y$，其中 $i \\in \\{1,\\dots,C\\}$。\n\n你的任务是：\n- 仅使用上述定义和内积的线性性质，将分支输出之间的平均非对角余弦相似度定义为\n$$\nS = \\frac{2}{C(C-1)} \\sum_{1 \\le i  j \\le C} \\frac{\\langle z_i, z_j \\rangle_{\\mathcal{D}}}{\\|z_i\\|_{\\mathcal{D}} \\, \\|z_j\\|_{\\mathcal{D}}}\n$$\n约定：对于任何范数 $\\|z_i\\|_{\\mathcal{D}}=0$ 或 $\\|z_j\\|_{\\mathcal{D}}=0$ 的配对 $(i,j)$，将其从平均值计算中排除；如果排除后没有剩余的配对，则设 $S = 0$。定义多样性得分 $D = 1 - S$。\n- 定义平均集成 $\\bar{F}(x) = \\frac{1}{C} \\sum_{i=1}^{C} F_i(x)$ 及其在 $\\mathcal{D}$ 上的经验均方误差 (MSE) 为 $\\mathrm{MSE}_{\\mathrm{avg}} = \\left\\|\\frac{1}{C} \\sum_{i=1}^{C} r_i \\right\\|_{\\mathcal{D}}^{2}$。定义平均单分支经验 MSE 为 $\\mathrm{MSE}_{\\mathrm{single}} = \\frac{1}{C} \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}$。仅使用内积的双线性和对称性，将精确提升比\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\mathrm{MSE}_{\\mathrm{avg}}}{\\mathrm{MSE}_{\\mathrm{single}}}\n$$\n表示为 $\\|r_i\\|_{\\mathcal{D}}^{2}$ 和 $\\langle r_i, r_j \\rangle_{\\mathcal{D}}$（其中 $1 \\le i  j \\le C$）的函数。\n- 定义平均残差余弦相似度\n$$\n\\hat{\\rho} = \\frac{2}{C(C-1)} \\sum_{1 \\le i  j \\le C} \\frac{\\langle r_i, r_j \\rangle_{\\mathcal{D}}}{\\|r_i\\|_{\\mathcal{D}} \\, \\|r_j\\|_{\\mathcal{D}}}\n$$\n同样地，排除分母中范数为零的任何配对；如果没有剩余配对，则设 $\\hat{\\rho} = 0$。在所有分支具有相等残差方差和共同的成对相关的简化近似下，定义近似提升比\n$$\n\\mathrm{IR}_{\\rho} = \\frac{1}{C} \\left( 1 + (C-1)\\hat{\\rho} \\right).\n$$\n\n程序要求：\n- 实现一个程序，给定一个由小型合成案例 $(Z, y)$ 组成的测试套件，为每个案例计算以下五个量：$S$、$D$、$\\hat{\\rho}$、$\\mathrm{IR}_{\\mathrm{exact}}$、$\\mathrm{IR}_{\\rho}$。在计算余弦相似度的平均值时，按上述规定排除未定义的配对；如果所有配对都被排除，则平均值使用默认值 $0$。如果 $\\mathrm{MSE}_{\\mathrm{single}} = 0$，则定义 $\\mathrm{IR}_{\\mathrm{exact}} = 0$。\n- 程序必须输出单行，该行为每个案例的结果列表组成的列表。每个案例的结果列表必须按 $[S, D, \\hat{\\rho}, \\mathrm{IR}_{\\mathrm{exact}}, \\mathrm{IR}_{\\rho}]$ 的顺序排列，每个值都打印为四舍五入到六位小数的十进制浮点数。\n\n用于覆盖不同情况的测试套件：\n- 案例 1（正常情况，中等相关性）：$N=6$, $C=4$, $y = [1.0, -1.0, 0.5, -0.5, 0.0, 1.0]$，以及\n  $z_1 = y + [0.1, -0.1, 0.2, -0.2, 0.0, 0.1]$,\n  $z_2 = y + [-0.1, 0.1, 0.1, -0.1, 0.0, -0.05]$,\n  $z_3 = y + [0.05, 0.0, -0.1, 0.1, 0.0, -0.05]$,\n  $z_4 = y + [0.0, 0.05, 0.0, 0.0, 0.0, 0.1]$。\n- 案例 2（无多样性，分支相同）：$N=5$, $C=3$, $y = [0.5, -0.5, 1.0, -1.0, 0.0]$，以及 $z_1 = z_2 = z_3 = y + [0.2, -0.2, 0.1, -0.1, 0.0]$。\n- 案例 3（两个分支的残差呈最大负相关）：$N=4$, $C=2$, $y = [1.0, 0.0, -1.0, 1.0]$, $z_1 = y + [0.5, -0.5, 0.5, -0.5]$, $z_2 = y - [0.5, -0.5, 0.5, -0.5]$。\n- 案例 4（近似正交的残差）：$N=4$, $C=2$, $y = [0.0, 0.0, 0.0, 0.0]$, $z_1 = [1.0, -1.0, 1.0, -1.0]$, $z_2 = [1.0, 1.0, -1.0, -1.0]$。\n- 案例 5（一个完美分支，其他为噪声分支；存在退化的零范数残差）：$N=2$, $C=3$, $y = [1.0, -1.0]$, $z_1 = [1.0, -1.0]$, $z_2 = [1.5, -1.5]$, $z_3 = [0.5, -0.5]$。\n\n最终输出格式：\n- 你的程序应生成单行，其中包含一个 Python 风格的列表的列表，每个内部列表对应一个测试案例，按指定顺序排列，每个浮点数四舍五入到六位小数，例如：\"[[s1,d1,rho1,ir_e1,ir_r1],[s2,d2,rho2,ir_e2,ir_r2],...]\"。", "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于应用于神经网络架构的集成学习理论原理，在数学上是适定的，提供了所有必要的定义和数据，并以客观、正式的语言表达。没有矛盾、歧义或不合理的假设。因此，我们可以着手解决。\n\n主要任务是推导精确提升比 $\\mathrm{IR}_{\\mathrm{exact}}$ 的显式公式，并实现一个程序来为给定的测试套件计算五个指定的度量指标。这些度量指标是：分支输出的平均非对角余弦相似度 $S$；多样性得分 $D$；平均残差余弦相似度 $\\hat{\\rho}$；精确提升比 $\\mathrm{IR}_{\\mathrm{exact}}$；以及近似提升比 $\\mathrm{IR}_{\\rho}$。\n\n首先，我们来推导精确提升比 $\\mathrm{IR}_{\\mathrm{exact}}$。问题将该比率定义为：\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\mathrm{MSE}_{\\mathrm{avg}}}{\\mathrm{MSE}_{\\mathrm{single}}}\n$$\n其中，平均集成均方误差 (MSE) 为 $\\mathrm{MSE}_{\\mathrm{avg}} = \\left\\|\\frac{1}{C} \\sum_{i=1}^{C} r_i \\right\\|_{\\mathcal{D}}^{2}$，而平均单分支 MSE 为 $\\mathrm{MSE}_{\\mathrm{single}} = \\frac{1}{C} \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}$。向量 $r_i \\in \\mathbb{R}^N$ 代表 $C$ 个分支中每个分支的经验残差。\n\n使用诱导范数的定义 $\\|u\\|_{\\mathcal{D}}^2 = \\langle u, u \\rangle_{\\mathcal{D}}$，我们可以展开分子 $\\mathrm{MSE}_{\\mathrm{avg}}$：\n$$\n\\mathrm{MSE}_{\\mathrm{avg}} = \\left\\langle \\frac{1}{C} \\sum_{i=1}^{C} r_i, \\frac{1}{C} \\sum_{j=1}^{C} r_j \\right\\rangle_{\\mathcal{D}}\n$$\n根据内积的双线性性质，我们可以将常数因子提出并展开求和：\n$$\n\\mathrm{MSE}_{\\mathrm{avg}} = \\frac{1}{C^2} \\left\\langle \\sum_{i=1}^{C} r_i, \\sum_{j=1}^{C} r_j \\right\\rangle_{\\mathcal{D}} = \\frac{1}{C^2} \\sum_{i=1}^{C} \\sum_{j=1}^{C} \\langle r_i, r_j \\rangle_{\\mathcal{D}}\n$$\n这个双重求和可以分为 $i = j$ 的项和 $i \\neq j$ 的项。根据内积的对称性 $\\langle r_i, r_j \\rangle_{\\mathcal{D}} = \\langle r_j, r_i \\rangle_{\\mathcal{D}}$，对 $i \\neq j$ 的求和可以表示为对 $i  j$ 的配对求和的两倍：\n$$\n\\sum_{i=1}^{C} \\sum_{j=1}^{C} \\langle r_i, r_j \\rangle_{\\mathcal{D}} = \\sum_{i=1}^{C} \\langle r_i, r_i \\rangle_{\\mathcal{D}} + \\sum_{i \\neq j} \\langle r_i, r_j \\rangle_{\\mathcal{D}} = \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}}\n$$\n将此代回 $\\mathrm{MSE}_{\\mathrm{avg}}$ 的表达式中，得到：\n$$\n\\mathrm{MSE}_{\\mathrm{avg}} = \\frac{1}{C^2} \\left( \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}} \\right)\n$$\n现在，我们构建比率 $\\mathrm{IR}_{\\mathrm{exact}}$：\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\frac{1}{C^2} \\left( \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}} \\right)}{\\frac{1}{C} \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}}\n$$\n通过消去一个因子 $1/C$ 并重新整理表达式，可以得到所需的用 $\\|r_i\\|_{\\mathcal{D}}^{2}$ 和 $\\langle r_i, r_j \\rangle_{\\mathcal{D}}$ 表示的形式：\n$$\n\\mathrm{IR}_{\\mathrm{exact}} = \\frac{\\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2} + 2 \\sum_{1 \\le i  j \\le C} \\langle r_i, r_j \\rangle_{\\mathcal{D}}}{C \\sum_{i=1}^{C} \\|r_i\\|_{\\mathcal{D}}^{2}}\n$$\n此推导完成了任务的分析部分。\n\n计算策略包括实现一个程序来为每个测试案例 $(Z, y)$ 计算五个指定的量。关键步骤如下：\n\n1.  **数据表示**：每个案例的输入数据，包括目标向量 $y$ 和分支输出矩阵 $Z$，将表示为 `numpy` 数组。这有助于高效的向量化操作。从 $Z$ 和 $y$ 中，我们获得数据点数 $N$、分支数 $C$、分支输出向量 $z_i$ 以及残差向量 $r_i = z_i - y$。\n\n2.  **内积和范数**：将实现辅助函数来计算经验内积 $\\langle u, v \\rangle_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^{N} u_n v_n$ 和诱导范数 $\\|u\\|_{\\mathcal{D}} = \\sqrt{\\langle u, u \\rangle_{\\mathcal{D}}}$。这些分别对应于 `numpy.dot(u, v) / N` 和 `numpy.sqrt(numpy.dot(u, u) / N)`。\n\n3.  **相似度平均值（$S$ 和 $\\hat{\\rho}$）**：通过遍历所有唯一的向量对 $(v_i, v_j)$（其中 $i  j$）来计算平均余弦相似度 $S$（对于输出 $z_i$）和 $\\hat{\\rho}$（对于残差 $r_i$）。对于每对向量，我们计算范数 $\\|v_i\\|_{\\mathcal{D}}$ 和 $\\|v_j\\|_{\\mathcal{D}}$。如果任一范数为零，则根据问题陈述的规定，将该对从计算中排除。对于所有有效的配对，计算余弦相似度 $\\frac{\\langle v_i, v_j \\rangle_{\\mathcal{D}}}{\\|v_i\\|_{\\mathcal{D}} \\|v_j\\|_{\\mathcal{D}}}$。最终值（$S$ 或 $\\hat{\\rho}$）是这些相似度的算术平均值。如果不存在有效配对，则该值设为 $0$。\n\n4.  **多样性得分 ($D$)**：直接由 $S$ 计算得出，$D = 1 - S$。\n\n5.  **提升比（$\\mathrm{IR}_{\\mathrm{exact}}$ 和 $\\mathrm{IR}_{\\rho}$）**：\n    *   $\\mathrm{IR}_{\\mathrm{exact}}$ 的计算方法是首先根据残差向量 $r_i$ 计算 $\\mathrm{MSE}_{\\mathrm{avg}}$ 和 $\\mathrm{MSE}_{\\mathrm{single}}$。如果 $\\mathrm{MSE}_{\\mathrm{single}}$ 为零，则根据问题约定将 $\\mathrm{IR}_{\\mathrm{exact}}$ 设为 $0$。否则，计算该比率。此方法在数值上等同于使用上面推导的公式。\n    *   $\\mathrm{IR}_{\\rho}$ 使用其定义 $\\mathrm{IR}_{\\rho} = \\frac{1}{C} ( 1 + (C-1)\\hat{\\rho} )$ 和先前计算出的 $\\hat{\\rho}$ 值进行计算。\n\n该计算计划系统地解决了所有要求，包括对边缘情况的指定处理，确保了实现的鲁棒性和正确性。每个测试案例的结果将被收集并格式化为所需的输出字符串。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ResNeXt analysis problem for the given test suite.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1 (happy path, moderate correlation)\n        {\n            \"N\": 6, \"C\": 4, \n            \"y\": np.array([1.0, -1.0, 0.5, -0.5, 0.0, 1.0]),\n            \"Z\": np.array([\n                [1.1, -1.1, 0.7, -0.7, 0.0, 1.1],     # z1\n                [0.9, -0.9, 0.6, -0.6, 0.0, 0.95],    # z2\n                [1.05, -1.0, 0.4, -0.4, 0.0, 0.95],   # z3\n                [1.0, -0.95, 0.5, -0.5, 0.0, 1.1]     # z4\n            ]).T\n        },\n        # Case 2 (no diversity, identical branches)\n        {\n            \"N\": 5, \"C\": 3,\n            \"y\": np.array([0.5, -0.5, 1.0, -1.0, 0.0]),\n            \"Z\": np.array([\n                [0.7, -0.7, 1.1, -1.1, 0.0], # z1\n                [0.7, -0.7, 1.1, -1.1, 0.0], # z2\n                [0.7, -0.7, 1.1, -1.1, 0.0]  # z3\n            ]).T\n        },\n        # Case 3 (maximally negative residual correlation)\n        {\n            \"N\": 4, \"C\": 2,\n            \"y\": np.array([1.0, 0.0, -1.0, 1.0]),\n            \"Z\": np.array([\n                [1.5, -0.5, -0.5, 0.5], # z1\n                [0.5, 0.5, -1.5, 1.5]  # z2\n            ]).T\n        },\n        # Case 4 (approximately orthogonal residuals)\n        {\n            \"N\": 4, \"C\": 2,\n            \"y\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"Z\": np.array([\n                [1.0, -1.0, 1.0, -1.0], # z1\n                [1.0, 1.0, -1.0, -1.0]  # z2\n            ]).T\n        },\n        # Case 5 (one perfect branch, degenerate zero-norm residual)\n        {\n            \"N\": 2, \"C\": 3,\n            \"y\": np.array([1.0, -1.0]),\n            \"Z\": np.array([\n                [1.0, -1.0],   # z1 (perfect)\n                [1.5, -1.5],   # z2\n                [0.5, -0.5]    # z3\n            ]).T\n        }\n    ]\n\n    # Helper function for empirical inner product\n    def empirical_inner_product(u, v, N):\n        return np.dot(u, v) / N\n\n    # Helper function for empirical norm\n    def empirical_norm(u, N):\n        return np.sqrt(empirical_inner_product(u, u, N))\n\n    def compute_average_cosine_similarity(vectors, C, N):\n        if C  2:\n            return 0.0\n        \n        similarities = []\n        for i in range(C):\n            for j in range(i + 1, C):\n                u, v = vectors[i], vectors[j]\n                norm_u = empirical_norm(u, N)\n                norm_v = empirical_norm(v, N)\n                \n                if norm_u == 0 or norm_v == 0:\n                    continue\n                \n                inner_prod = empirical_inner_product(u, v, N)\n                cos_sim = inner_prod / (norm_u * norm_v)\n                similarities.append(cos_sim)\n                \n        if not similarities:\n            return 0.0\n        \n        return np.mean(similarities)\n\n    def calculate_metrics(Z, y, C, N):\n        # Extract z_i vectors\n        z_vectors = [Z[:, i] for i in range(C)]\n        \n        # Calculate S and D\n        S = compute_average_cosine_similarity(z_vectors, C, N)\n        D = 1 - S\n        \n        # Calculate residual vectors r_i\n        r_vectors = [z - y for z in z_vectors]\n        \n        # Calculate rho_hat\n        rho_hat = compute_average_cosine_similarity(r_vectors, C, N)\n        \n        # Calculate IR_exact\n        per_branch_mses = [empirical_inner_product(r, r, N) for r in r_vectors]\n        mse_single = np.mean(per_branch_mses)\n        \n        if mse_single == 0:\n            ir_exact = 0.0\n        else:\n            avg_residual = np.sum(r_vectors, axis=0) / C\n            mse_avg = empirical_inner_product(avg_residual, avg_residual, N)\n            ir_exact = mse_avg / mse_single\n            \n        # Calculate IR_rho\n        ir_rho = (1 + (C - 1) * rho_hat) / C if C > 0 else 0.0\n        \n        return [S, D, rho_hat, ir_exact, ir_rho]\n\n    all_results = []\n    for case in test_cases:\n        Z, y, C, N = case['Z'], case['y'], case['C'], case['N']\n        results = calculate_metrics(Z, y, C, N)\n        all_results.append(results)\n\n    # Format the final output string\n    formatted_cases = []\n    for res_list in all_results:\n        # Round each value and format to 6 decimal places\n        formatted_list_str = '[' + ','.join([f'{val:.6f}' for val in res_list]) + ']'\n        formatted_cases.append(formatted_list_str)\n\n    final_output = '[' + ','.join(formatted_cases) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3169937"}]}