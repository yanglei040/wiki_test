{"hands_on_practices": [{"introduction": "卷积神经网络（CNN）的核心创新之一是参数共享，这极大地减少了模型中的可训练参数数量，使其能够高效地从图像数据中学习。本练习将通过一个具体的计算任务，帮助你量化这种效率的巨大优势 [@problem_id:3118617]。你将比较 AlexNet 第一个卷积层的参数量和一个假设的、不共享权重的局部连接层，从而直观地理解权重共享的威力。此外，练习还将引导你从贝叶斯推断的视角重新审视常见的 $L_2$ 正则化技术，揭示其与高斯先验之间的深刻联系。", "problem": "一个数据集 $\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$ 用于训练一个卷积神经网络 (CNN)，方法是最小化带有 $\\ell_2$ 权重衰减的经验负对数似然。训练目标为\n$$\n\\mathcal{J}(w) \\equiv \\sum_{i=1}^N -\\log p(y_i \\mid x_i, w) \\;+\\; \\frac{\\lambda}{2}\\,\\|w\\|_2^2,\n$$\n其中 $w$ 表示所有可训练的权重，$\\lambda > 0$ 是正则化系数。从贝叶斯推断和最大后验 (MAP) 估计的基本原理出发，将 $\\ell_2$ 权重衰减项解释为 $w$ 的一个先验，并精确说明是哪种先验分布会得到上述目标函数作为其 MAP 估计量，包括 $\\lambda$ 和先验协方差之间的关系。\n\n接着，考虑 AlexNet 的第一个卷积层，其输入空间尺寸为 $227 \\times 227$，有 $3$ 个通道，使用 $96$ 个空间尺寸为 $11 \\times 11$ 的滤波器，步幅 $s = 4$，无零填充，生成一个空间尺寸为 $55 \\times 55$，有 $96$ 个通道的输出。忽略偏置项，计算：\n- 这个卷积层（有权重共享）中的权重总数，以及\n- 一个局部连接层（无权重共享）中的权重总数，该层使用相同的感受野大小、步幅和输出通道数，但允许每个输出空间位置和通道都有不同的权重，\n并报告局部连接层权重与卷积层权重的比率。\n\n选择一个同时正确陈述了 MAP-先验解释以及参数数量和比率的选项。\n\nA. 先验是零均值各向同性高斯分布 $\\mathcal{N}(0, \\sigma^2 I)$，其中 $\\sigma^2 = \\lambda^{-1}$。卷积层有 $11 \\times 11 \\times 3 \\times 96 = 34{,}848$ 个权重。局部连接层有 $55 \\times 55 \\times 11 \\times 11 \\times 3 \\times 96 = 105{,}415{,}200$ 个权重。比率为 $3{,}025$。\n\nB. 先验是零均值各向同性高斯分布 $\\mathcal{N}(0, \\sigma^2 I)$，其中 $\\sigma^2 = \\lambda$。卷积层有 $34{,}848$ 个权重。局部连接层有 $54 \\times 54 \\times 11 \\times 11 \\times 3 \\times 96 = 101{,}616{,}768$ 个权重。比率为 $2{,}916$。\n\nC. 先验是零均值拉普拉斯分布，卷积层有 $11 \\times 11 \\times 96 = 11{,}616$ 个权重。局部连接层有 $55 \\times 55 \\times 11 \\times 11 \\times 96 = 35{,}138{,}400$ 个权重。比率为 $3{,}025$。\n\nD. 先验是零均值各向同性高斯分布 $\\mathcal{N}(0, \\sigma^2 I)$，其协方差为 $(2\\lambda)^{-1} I$。卷积层有 $34{,}848$ 个权重。局部连接层有 $56 \\times 56 \\times 11 \\times 11 \\times 3 \\times 96 = 109{,}333{,}248$ 个权重。比率为 $3{,}136$。", "solution": "用户提供了一个包含两部分的问题。第一部分要求对 $\\ell_2$ 权重衰减进行贝叶斯解释。第二部分涉及计算一个特定卷积层和相应的局部连接层中的参数数量，然后求出它们的比率。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n- 数据集：$\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N$。\n- 要最小化的训练目标：$\\mathcal{J}(w) \\equiv \\sum_{i=1}^N -\\log p(y_i \\mid x_i, w) \\;+\\; \\frac{\\lambda}{2}\\,\\|w\\|_2^2$。\n- 正则化系数：$\\lambda > 0$。\n- AlexNet 第一个卷积层的规格：\n    - 输入体大小：$227 \\times 227 \\times 3$。\n    - 滤波器数量：$96$。\n    - 滤波器空间尺寸：$11 \\times 11$。\n    - 步幅：$s = 4$。\n    - 零填充：无。\n    - 输出体大小：$55 \\times 55 \\times 96$。\n    - 偏置项将被忽略。\n- 任务 1：从最大后验 (MAP) 估计的角度，将 $\\ell_2$ 权重衰减项解释为权重 $w$ 的先验，并指明其分布以及 $\\lambda$ 与先验参数之间的关系。\n- 任务 2：计算所述卷积层的权重数量。\n- 任务 3：计算一个具有相同感受野、步幅和输出通道数但没有权重共享的局部连接层的权重数量。\n- 任务 4：计算局部连接层权重与卷积层权重的比率。\n\n**步骤 2：使用提取的已知条件进行验证**\n该问题具有科学依据，基于贝叶斯统计（MAP 估计）和深度学习（CNN 架构，权重共享）的基本原理。$\\ell_2$ 正则化与高斯先验之间的关系是机器学习中的一个经典结论。为 AlexNet 层提供的架构细节在历史上是准确的，并且内部一致。\n\n卷积层的输出空间维度 $O$ 由公式 $O = \\lfloor \\frac{W - K + 2P}{S} \\rfloor + 1$ 给出，其中 $W$ 是输入尺寸，$K$ 是滤波器尺寸，$P$ 是填充，$S$ 是步幅。使用给定的值，$W = 227$，$K = 11$，$P = 0$，$S = 4$：\n$$O = \\left\\lfloor \\frac{227 - 11 + 2 \\cdot 0}{4} \\right\\rfloor + 1 = \\left\\lfloor \\frac{216}{4} \\right\\rfloor + 1 = 54 + 1 = 55.$$\n计算出的输出尺寸 $55$ 与问题陈述相符，证实了所提供参数的一致性。\n\n该问题是适定的、客观的，并包含得出唯一解所需的所有必要信息。没有矛盾、歧义或事实错误。\n\n**步骤 3：结论与行动**\n问题陈述是有效的。我将继续进行解题推导。\n\n### 解题推导\n\n**第一部分：MAP 估计与先验解释**\n\n最大后验 (MAP) 估计旨在寻找能最大化给定数据 $\\mathcal{D}$ 时参数的后验概率的参数 $w$。使用贝叶斯定理，后验概率为：\n$$p(w \\mid \\mathcal{D}) = \\frac{p(\\mathcal{D} \\mid w) p(w)}{p(\\mathcal{D})}$$\n为了找到 MAP 估计 $\\hat{w}_{MAP}$，我们对 $w$ 最大化这个量：\n$$\\hat{w}_{MAP} = \\arg\\max_{w} p(w \\mid \\mathcal{D}) = \\arg\\max_{w} \\frac{p(\\mathcal{D} \\mid w) p(w)}{p(\\mathcal{D})}$$\n由于对数是单调递增函数，且 $p(\\mathcal{D})$ 与 $w$ 无关，这等价于最大化对数后验概率（不包括常数项 $\\log p(\\mathcal{D})$）：\n$$\\hat{w}_{MAP} = \\arg\\max_{w} \\left( \\log p(\\mathcal{D} \\mid w) + \\log p(w) \\right)$$\n假设数据点是独立同分布 (i.i.d.) 的，似然项 $\\log p(\\mathcal{D} \\mid w)$ 可以写成对所有训练样本求和的形式：\n$$\\log p(\\mathcal{D} \\mid w) = \\log \\prod_{i=1}^N p(y_i \\mid x_i, w) = \\sum_{i=1}^N \\log p(y_i \\mid x_i, w)$$\n于是 MAP 估计问题变为：\n$$\\hat{w}_{MAP} = \\arg\\max_{w} \\left( \\sum_{i=1}^N \\log p(y_i \\mid x_i, w) + \\log p(w) \\right)$$\n这等价于最小化负对数后验概率：\n$$\\hat{w}_{MAP} = \\arg\\min_{w} \\left( -\\sum_{i=1}^N \\log p(y_i \\mid x_i, w) - \\log p(w) \\right)$$\n我们给出的要最小化的训练目标是：\n$$\\mathcal{J}(w) = \\sum_{i=1}^N -\\log p(y_i \\mid x_i, w) + \\frac{\\lambda}{2}\\,\\|w\\|_2^2$$\n通过将 MAP 最小化目标与给定的训练目标 $\\mathcal{J}(w)$ 进行比较，我们可以将那些依赖于 $w$ 但不依赖于数据的项等同起来。这就建立了先验项和正则化项之间的对应关系。\n$$-\\log p(w) = \\frac{\\lambda}{2} \\|w\\|_2^2 + C$$\n其中 $C$ 是一个常数（先验分布归一化常数的负对数）。对先验概率 $p(w)$ 进行整理：\n$$\\log p(w) = -\\frac{\\lambda}{2} \\|w\\|_2^2 - C$$\n$$p(w) = \\exp\\left(-\\frac{\\lambda}{2} \\|w\\|_2^2 - C\\right) \\propto \\exp\\left(-\\frac{\\lambda}{2} \\|w\\|_2^2\\right)$$\n这个函数形式对应于一个多元高斯分布。一个零均值多元高斯分布 $w \\sim \\mathcal{N}(0, \\Sigma)$ 的概率密度函数为：\n$$p(w) \\propto \\exp\\left(-\\frac{1}{2} w^T \\Sigma^{-1} w\\right)$$\n对于零均值各向同性高斯分布，其协方差矩阵为 $\\Sigma = \\sigma^2 I$，其中 $I$ 是单位矩阵，$\\sigma^2$ 是方差。其逆矩阵为 $\\Sigma^{-1} = \\frac{1}{\\sigma^2} I$。指数中的项变为：\n$$-\\frac{1}{2} w^T (\\sigma^{-2}I) w = -\\frac{1}{2\\sigma^2} w^T w = -\\frac{1}{2\\sigma^2} \\|w\\|_2^2$$\n令目标函数和高斯先验中 $\\|w\\|_2^2$ 的系数相等：\n$$\\frac{\\lambda}{2} = \\frac{1}{2\\sigma^2} \\implies \\lambda = \\frac{1}{\\sigma^2} \\implies \\sigma^2 = \\lambda^{-1}$$\n因此，$\\ell_2$ 权重衰减项对应于权重上的一个零均值各向同性高斯先验，即 $w \\sim \\mathcal{N}(0, \\sigma^2 I)$，其方差为 $\\sigma^2 = \\lambda^{-1}$。\n\n**第二部分：卷积层的参数数量**\n\n具有权重共享的卷积层在输入的所有空间位置上应用同一组滤波器。每个滤波器的深度等于输入通道的数量。\n- 滤波器高度，$K_h = 11$。\n- 滤波器宽度，$K_w = 11$。\n- 输入通道数，$C_{in} = 3$。\n- 滤波器数量（等于输出通道数），$C_{out} = 96$。\n\n卷积层中的权重总数，记为 $W_{conv}$，是这些维度的乘积：\n$$W_{conv} = K_h \\times K_w \\times C_{in} \\times C_{out}$$\n$$W_{conv} = 11 \\times 11 \\times 3 \\times 96 = 121 \\times 3 \\times 96 = 363 \\times 96 = 34{,}848$$\n\n**第三部分：局部连接层的参数数量**\n\n局部连接层不在输出图的不同空间位置之间共享权重。这意味着对于输出特征图中的每个位置，都有一组不同的权重。\n- 输出高度，$O_h = 55$。\n- 输出宽度，$O_w = 55$。\n更精确地说，对于 $O_h \\times O_w$ 个输出位置中的每一个，以及 $C_{out}$ 个输出通道中的每一个，都有一个大小为 $K_h \\times K_w \\times C_{in}$ 的唯一滤波器。\n\n局部连接层中的权重总数 $W_{local}$ 为：\n$$W_{local} = (O_h \\times O_w) \\times (K_h \\times K_w \\times C_{in} \\times C_{out})$$\n$$W_{local} = (55 \\times 55) \\times (11 \\times 11 \\times 3 \\times 96)$$\n$$W_{local} = 3{,}025 \\times 34{,}848 = 105{,}415{,}200$$\n\n**第四部分：权重比率**\n\n局部连接层与卷积层的权重数量之比为：\n$$\\text{Ratio} = \\frac{W_{local}}{W_{conv}} = \\frac{(O_h \\times O_w) \\times W_{conv}}{W_{conv}} = O_h \\times O_w$$\n$$\\text{Ratio} = 55 \\times 55 = 3{,}025$$\n\n### 逐项分析\n\n**A. 先验是零均值各向同性高斯分布 $\\mathcal{N}(0, \\sigma^2 I)$，其中 $\\sigma^2 = \\lambda^{-1}$。卷积层有 $11 \\times 11 \\times 3 \\times 96 = 34{,}848$ 个权重。局部连接层有 $55 \\times 55 \\times 11 \\times 11 \\times 3 \\times 96 = 105{,}415{,}200$ 个权重。比率为 $3{,}025$。**\n- 先验解释：正确。我们的推导表明 $\\sigma^2 = \\lambda^{-1}$。\n- 卷积权重：正确。我们的计算得出 $34{,}848$。\n- 局部连接权重：正确。我们的计算得出 $105{,}415{,}200$。\n- 比率：正确。我们的计算得出 $3{,}025$。\n- **结论：正确。**\n\n**B. 先验是零均值各向同性高斯分布 $\\mathcal{N}(0, \\sigma^2 I)$，其中 $\\sigma^2 = \\lambda$。卷积层有 $34{,}848$ 个权重。局部连接层有 $54 \\times 54 \\times 11 \\times 11 \\times 3 \\times 96 = 101{,}616{,}768$ 个权重。比率为 $2{,}916$。**\n- 先验解释：不正确。方差是 $\\sigma^2 = \\lambda^{-1}$，而不是 $\\lambda$。\n- 卷积权重：正确。\n- 局部连接权重：不正确。这里使用了不正确的输出维度 $54 \\times 54$。正确的维度是 $55 \\times 55$。\n- 比率：不正确。这是 $54 \\times 54 = 2{,}916$，基于不正确的输出维度。\n- **结论：不正确。**\n\n**C. 先验是零均值拉普拉斯分布，卷积层有 $11 \\times 11 \\times 96 = 11{,}616$ 个权重。局部连接层有 $55 \\times 55 \\times 11 \\times 11 \\times 96 = 35{,}138{,}400$ 个权重。比率为 $3{,}025$。**\n- 先验解释：不正确。拉普拉斯先验对应于 $\\ell_1$ 正则化，而不是 $\\ell_2$。\n- 卷积权重：不正确。计算 $11 \\times 11 \\times 96$ 忽略了输入通道数 $C_{in}=3$。\n- 局部连接权重：不正确。该表达式格式错误，并且也忽略了输入通道。\n- 比率：数值 $3{,}025$ 对于 $55 \\times 55$ 是正确的，但它是在错误的权重计算背景下提出的。\n- **结论：不正确。**\n\n**D. 先验是零均值各向同性高斯分布 $\\mathcal{N}(0, \\sigma^2 I)$，其协方差为 $(2\\lambda)^{-1} I$。卷积层有 $34{,}848$ 个权重。局部连接层有 $56 \\times 56 \\times 11 \\times 11 \\times 3 \\times 96 = 109{,}333{,}248$ 个权重。比率为 $3{,}136$。**\n- 先验解释：不正确。协方差是 $\\lambda^{-1}I$，而不是 $(2\\lambda)^{-1}I$。协方差为 $(2\\lambda)^{-1}I$ 对应于正则化项 $\\lambda\\|w\\|_2^2$，缺少了因子 $\\frac{1}{2}$。\n- 卷积权重：正确。\n- 局部连接权重：不正确。这里使用了不正确的输出维度 $56 \\times 56$。\n- 比率：不正确。这是 $56 \\times 56 = 3{,}136$，基于不正确的输出维度。\n- **结论：不正确。**\n\n基于以上分析，只有选项 A 在其所有陈述中都是正确的。", "answer": "$$\\boxed{A}$$", "id": "3118617"}, {"introduction": "成功训练一个像 LeNet-5 这样的深度网络，不仅需要好的架构，还需要精细的训练策略，其中最关键的挑战之一是防止过拟合。本练习将带你亲手实现一种基于验证损失曲率的早停（Early Stopping）算法 [@problem_id:3118548]。你将学习如何通过分析损失函数曲线的几何形状（一阶和二阶导数），来精确地判断模型从有效学习转向过拟合的临界点，这是一个将理论洞察转化为实用代码的宝贵实践。", "problem": "您正在训练一个 LeNet-$5$ 风格的卷积神经网络（CNN）分类器，并观察其在不同轮次（epoch）下的验证损失。您希望实现一个基于原则的早停规则，该规则利用验证损失序列的曲率（二阶导数）来检测过拟合的开始。您的任务是编写一个完整的程序，在给定几个验证损失序列和规则参数的情况下，返回应停止训练的整数轮次索引；如果规则从未触发，则返回 $-1$。\n\n从以下基本原则开始：\n- 经验风险最小化定义了一个在留出数据上计算的验证损失序列 $\\{v_t\\}_{t=0}^{T-1}$，其中 $t$ 是轮次索引。\n- 過擬合的開始對應於驗證損失從下降到上升的轉變；在形狀上，這表現為正曲率（二階導數）以及非負的近期斜率。\n- 在离散时间中，一阶和二阶导数通过有限差分进行近似。对于任何序列 $\\{v_t\\}$，定义一阶差分 $d^{(1)}_t = v_t - v_{t-1}$（其中 $t \\in \\{1,\\dots,T-1\\}$）和中心二阶差分 $d^{(2)}_t = v_{t+1} - 2 v_t + v_{t-1}$（其中 $t \\in \\{1,\\dots,T-2\\}$）。\n\n定义以下基于二阶导数的早停规则，该规则由曲率窗口长度 $w \\in \\mathbb{N}$（$w \\ge 1$）、斜率窗口长度 $m \\in \\mathbb{N}$（$m \\ge 1$）、曲率阈值 $\\theta \\in \\mathbb{R}$（$\\theta > 0$）以及非负松弛量 $\\epsilon \\in \\mathbb{R}$（$\\epsilon \\ge 0$）参数化：\n- 对于每个可同时使用两个后向窗口的轮次 $t \\in \\{1,\\dots,T-2\\}$（即 $t \\ge w$ 且 $t \\ge m$），计算后向平均曲率\n$$\nc_t \\;=\\; \\frac{1}{w} \\sum_{i=0}^{w-1} d^{(2)}_{t-i}\n$$\n和后向平均斜率\n$$\ng_t \\;=\\; \\frac{1}{m} \\sum_{i=0}^{m-1} d^{(1)}_{t-i}.\n$$\n- 将最早的检测索引 $t^\\star$ 定义为满足 $c_t \\ge \\theta$ 和 $g_t \\ge -\\epsilon$ 的最小 $t$。\n- 如果存在这样的 $t^\\star$，则输出早停轮次 $e^\\star = \\arg\\min_{0 \\le s \\le t^\\star} v_s$。如果存在多个最小值点，选择最小的索引。如果不存在这样的 $t^\\star$，则输出 $-1$。\n\n请完全按照所述实现此规则。程序的输入固定在程序内部；您不得读取任何外部输入。使用以下测试套件，其中每个测试用例是一个元组，包含一个验证损失序列和参数 $(w, m, \\theta, \\epsilon)$。为清晰起见，所有序列均无单位：\n\n- 测试用例 1（典型的先降后增且曲率平滑）：\n  - 损失：[$1.20$, $0.95$, $0.80$, $0.78$, $0.79$, $0.81$, $0.85$, $0.90$]\n  - 参数：$w = 2$, $m = 2$, $\\theta = 0.01$, $\\epsilon = 0.0$\n\n- 测试用例 2（边界长度序列，使用宽松的松弛量）：\n  - 损失：[$0.50$, $0.49$, $0.495$]\n  - 参数：$w = 1$, $m = 1$, $\\theta = 0.01$, $\\epsilon = 0.02$\n\n- 测试用例 3（单调递减，无过拟合）：\n  - 损失：[$1.00$, $0.90$, $0.85$, $0.84$, $0.83$]\n  - 参数：$w = 2$, $m = 2$, $\\theta = 0.02$, $\\epsilon = 0.0$\n\n- 测试用例 4（含噪声的波谷；需要平滑和小的曲率阈值）：\n  - 损失：[$0.90$, $0.82$, $0.80$, $0.805$, $0.802$, $0.808$, $0.815$, $0.814$, $0.820$]\n  - 参数：$w = 3$, $m = 3$, $\\theta = 0.0005$, $\\epsilon = 0.002$\n\n- 测试用例 5（立即过拟合）：\n  - 损失：[$0.90$, $0.905$, $0.915$, $0.930$]\n  - 参数：$w = 1$, $m = 2$, $\\theta = 0.001$, $\\epsilon = 0.0$\n\n- 测试用例 6（边界窗口过大无法应用）：\n  - 损失：[$0.60$, $0.59$, $0.58$, $0.57$]\n  - 参数：$w = 3$, $m = 1$, $\\theta = 0.001$, $\\epsilon = 0.0$\n\n您的程序必须使用上述规则为每个测试用例计算早停轮次 $e^\\star$，并生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，[$e_1,e_2,\\dots$]）。每个测试用例的输出类型必须是整数。此问题不涉及角度或物理单位。请通过精确实现所定义的离散有限差分和后向窗口平均来确保科学真实性。", "solution": "我们从经验风险最小化开始：训练像 LeNet-$5$ 或 AlexNet 这样的模型会最小化训练数据上的经验损失，但其泛化能力是通过验证损失序列 $\\{v_t\\}_{t=0}^{T-1}$ 来评估的。过拟合的特征是验证损失停止下降并开始上升，这对应于形状从下凹变为上凹的变化。在连续时间中，这发生在二阶导数变为正且一阶导数变为非负的地方。在以轮次为单位的离散时间中，适当的近似是有限差分。\n\n设 $v_t$ 表示在轮次 $t$ 的验证损失。一阶差分 $d^{(1)}_t = v_t - v_{t-1}$ 估计斜率，而中心二阶差分 $d^{(2)}_t = v_{t+1} - 2 v_t + v_{t-1}$ 估计在轮次 $t$ 的曲率。正的 $d^{(2)}_t$ 表明局部呈凸性行为。由于验证损失的测量值存在噪声，我们不依赖于单个轮次的差分。相反，我们对后向窗口进行平均：后向平均曲率\n$$\nc_t \\;=\\; \\frac{1}{w} \\sum_{i=0}^{w-1} d^{(2)}_{t-i}, \\quad \\text{for } t \\ge w,\n$$\n和后向平均斜率\n$$\ng_t \\;=\\; \\frac{1}{m} \\sum_{i=0}^{m-1} d^{(1)}_{t-i}, \\quad \\text{for } t \\ge m.\n$$\n\n该规则检测满足 $c_{t^\\star} \\ge \\theta$（持续的正曲率超过一个阈值）和 $g_{t^\\star} \\ge -\\epsilon$（斜率在容忍噪声的松弛量 $\\epsilon$ 范围内为非负）的最早轮次 $t^\\star$。这将“碗底”的几何证据与近期的非递减趋势相结合，从而追踪过拟合的开始。一旦检测到，审慎的早停轮次 $e^\\star$ 是到检测时为止的最佳验证轮次，\n$$\ne^\\star \\;=\\; \\arg\\min_{0 \\le s \\le t^\\star} v_s,\n$$\n这是在检测到过拟合开始时选择迄今为止最佳模型检查点的标准做法。如果不存在 $t^\\star$（序列长度不足或未超过阈值），我们返回 $-1$。\n\n基于原理的算法设计：\n- 通过直接有限差分计算 $d^{(1)}_t$（其中 $t \\in \\{1,\\dots,T-1\\}$）和 $d^{(2)}_t$（其中 $t \\in \\{1,\\dots,T-2\\}$）；这些源于数值微分的第一性原理。\n- 对于每个轮次索引 $t \\in \\{1,\\dots,T-2\\}$ 且 $t \\ge w$ 和 $t \\ge m$，使用简单算术平均值计算后向平均值 $c_t$ 和 $g_t$，这利用大数定律减少了噪声测量中的方差。\n- 选择满足 $c_t \\ge \\theta$ 和 $g_t \\ge -\\epsilon$ 的最小 $t$。如果找到，则将 $e^\\star$ 计算为实现 $\\min_{0 \\le s \\le t} v_s$ 的最小索引；否则返回 $-1$。\n- 边界情况：当 $T  3$ 时，$d^{(2)}_t$ 未定义；当 $t  w$ 或 $t  m$ 时，相应的平均值未定义。在这两种情况下，都无法发生检测。\n\n将此应用于提供的测试套件：\n- 测试用例1有一个清晰的波谷，随后损失上升；曲率变为正，近期斜率变为非负，从而产生一个检测点 $t^\\star$，因此得到一个 $e^\\star$，其等于检测时或检测前最小验证损失的索引。\n- 测试用例2演示了边界长度检测，其中松弛量 $\\epsilon$ 允许近期斜率略为负值；曲率阈值被超过，从而产生早期检测，并将 $e^\\star$ 设在当前的最佳索引处。\n- 测试用例3永远不满足斜率条件（始终递减），因此输出为 $-1$。\n- 测试用例4显示了一个含噪声的波谷；使用 $w$ 和 $m$ 进行平均可以抑制噪声，因此一个小的正曲率阈值和非负的平均斜率可以识别出波谷附近的过拟合开始点；$e^\\star$ 是波谷的索引。\n- 测试用例5立即出现过拟合（持续增加且为凸函数），因此检测发生得早，且 $e^\\star$ 是迄今为止给出最小值的最早索引。\n- 测试用例6说明，当曲率窗口相对于序列长度过大时，该规则不适用，并返回 $-1$。\n\n程序精确地实现了这些步骤，并输出一个整数列表 [$e^\\star_1, e^\\star_2, \\dots$]，对应六个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef early_stop_epoch_from_curvature(losses, w, m, theta, epsilon):\n    \"\"\"\n    Implements the second-derivative-based early stopping rule as specified.\n\n    Parameters:\n        losses (list or np.ndarray): Validation loss sequence v[0..T-1].\n        w (int): Trailing window length for curvature average (w = 1).\n        m (int): Trailing window length for slope average (m = 1).\n        theta (float): Curvature threshold (theta  0).\n        epsilon (float): Non-negativity slack (epsilon = 0).\n\n    Returns:\n        int: Early stopping epoch index e* if detected; otherwise -1.\n    \"\"\"\n    v = np.asarray(losses, dtype=float)\n    T = v.shape[0]\n    if T  3 or w  1 or m  1:\n        return -1\n\n    # First differences d1[i] = v[i+1] - v[i], for i = 0..T-2\n    d1 = v[1:] - v[:-1]\n    # Second differences d2[i] = v[i+2] - 2.0*v[i+1] + v[i], for i = 0..T-3\n    d2 = v[2:] - 2.0 * v[1:-1] + v[:-2]\n\n    # Search earliest t satisfying conditions.\n    # Epoch index t ranges from 1 .. T-2 inclusive (in original indexing).\n    # Trailing windows require t = w and t = m.\n    detected_t = None\n    # t is the epoch index where the curvature is *centered*, from 1 to T-2\n    # The corresponding index in d2 is t-1\n    for t in range(1, T - 1):\n        # We need w values of d2 ending at index t-1\n        # The first index of d2 is 0. The window is [t-1-w+1, t-1] = [t-w, t-1]\n        # This requires t-w = 0, so t = w\n        if t  w:\n            continue\n        \n        # We need m values of d1 ending at index t-1\n        # The window is [t-1-m+1, t-1] = [t-m, t-1]\n        # This requires t-m = 0, so t = m\n        if t  m:\n            continue\n\n        c_window = d2[t - w : t]\n        if c_window.size != w: # should not happen with the guards above\n             continue\n        c_t = float(np.mean(c_window))\n\n        g_window = d1[t - m : t]\n        if g_window.size != m: # should not happen with the guards above\n             continue\n        g_t = float(np.mean(g_window))\n\n        if c_t = theta and g_t = -epsilon:\n            detected_t = t\n            break\n\n    if detected_t is None:\n        return -1\n\n    # Early stop at the best-so-far validation epoch up to and including detected_t.\n    # Argmin returns first occurrence of minimum by default.\n    best_idx = int(np.argmin(v[: detected_t + 1]))\n    return best_idx\n\ndef solve():\n    # Define the test cases from the problem statement as (losses, w, m, theta, epsilon).\n    test_cases = [\n        # 1) Typical decrease then increase with smooth curvature\n        ([1.20, 0.95, 0.80, 0.78, 0.79, 0.81, 0.85, 0.90], 2, 2, 0.01, 0.0),\n        # 2) Boundary-length sequence with permissive slack\n        ([0.50, 0.49, 0.495], 1, 1, 0.01, 0.02),\n        # 3) Monotone decrease, no overfitting\n        ([1.00, 0.90, 0.85, 0.84, 0.83], 2, 2, 0.02, 0.0),\n        # 4) Noisy trough; smoothing and small curvature threshold\n        ([0.90, 0.82, 0.80, 0.805, 0.802, 0.808, 0.815, 0.814, 0.820], 3, 3, 0.0005, 0.002),\n        # 5) Immediate overfitting\n        ([0.90, 0.905, 0.915, 0.930], 1, 2, 0.001, 0.0),\n        # 6) Boundary window too large to be applicable\n        ([0.60, 0.59, 0.58, 0.57], 3, 1, 0.001, 0.0),\n    ]\n\n    results = []\n    for losses, w, m, theta, epsilon in test_cases:\n        result = early_stop_epoch_from_curvature(losses, w, m, theta, epsilon)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3118548"}, {"introduction": "像 AlexNet 这样的大型模型，其训练过程在计算上是极其昂贵的，通常需要使用多-GPU 进行数据并行训练。本练习将引导你分析这种分布式训练场景下的性能瓶颈 [@problem_id:3118605]。你将为一个经典的梯度同步算法——环形全归约（Ring All-Reduce）建立性能模型，计算其通信开销，并最终预测并行效率。这个过程将帮助你理解在扩展深度学习训练时，计算与通信之间的基本权衡关系。", "problem": "考虑在多个图形处理单元 (GPU) 上，使用随机梯度下降 (SGD) 对早期的卷积神经网络 (CNN) 架构 AlexNet 进行同步数据并行训练。假设存在以下科学上真实且自洽的设置：\n\n- AlexNet 中可训练参数的数量为 $N_{p} = 61{,}000{,}000$，以单精度浮点数形式存储，因此每个参数占用 $4$ 字节。\n- 在每个训练步骤中，所有参数的梯度在每个 GPU 上本地计算，并且必须在所有 GPU 之间进行平均，以确保与在合并批次上进行单 GPU SGD 的数学等效性。\n- 梯度聚合通过环形 all-reduce 算法实现：每个设备将梯度向量划分为 $P$ 个相等的连续块，并执行一个包含 $P-1$ 次迭代的 scatter-reduce 阶段（将一个块发送到其邻居并累积接收到的块），随后是一个包含 $P-1$ 次迭代的 all-gather 阶段（发送和接收块以在本地组装完整的平均梯度）。每次迭代通信一个大小为 $\\frac{S}{P}$ 的块，其中 $S$ 是梯度的总大小（以字节为单位）。\n- GPU 的数量为 $P = 8$。\n- 有效的点对点通信带宽为 $B = 3.125 \\times 10^{9}$ 字节/秒。忽略延迟和协议开销；将通信时间建模为 $\\frac{\\text{字节数}}{B}$。\n- 对于固定的单 GPU 小批量数据，每个 GPU 的前向加后向计算时间为 $t_{\\mathrm{comp}} = 0.080$ 秒，且通信和计算之间没有重叠。\n\n任务：\n1. 在此设置下，计算环形 all-reduce 单个训练步骤中每个 GPU 的通信量（以字节为单位）。\n2. 使用提供的带宽模型，计算该通信量所需的每 GPU 通信时间（以秒为单位）。\n3. 将并行效率 $E$ 定义为：在保持每 GPU 小批量大小不变的情况下，$P$ 个 GPU 上实现的吞吐量与 $P$ 个 GPU 上理想线性加速吞吐量的比率。使用此定义和上述步骤时间模型，计算此设置的预测 $E$。\n\n以十进制形式表示您的最终答案 $E$ 的值。将您的最终答案四舍五入到四位有效数字。", "solution": "首先验证问题陈述的科学性、一致性和清晰度。\n\n**问题验证**\n\n**步骤 1：提取给定信息**\n-   CNN 架构：AlexNet\n-   可训练参数数量：$N_{p} = 61{,}000{,}000$\n-   参数数据类型：单精度浮点（占用 $4$ 字节）\n-   训练范式：同步数据并行随机梯度下降 (SGD)\n-   梯度聚合算法：环形 all-reduce\n-   环形 all-reduce 模型：总大小为 $S$ 的梯度向量被分成 $P$ 个块。该过程包括一个 $P-1$ 次迭代的 scatter-reduce 阶段和一个 $P-1$ 次迭代的 all-gather 阶段。每次迭代通信一个大小为 $\\frac{S}{P}$ 的块。\n-   GPU 数量：$P = 8$\n-   有效点对点通信带宽：$B = 3.125 \\times 10^{9}$ 字节/秒\n-   通信时间模型：$t_{\\text{comm}} = \\frac{\\text{通信字节数}}{B}$，忽略延迟和协议开销。\n-   每 GPU 计算时间（前向 + 后向传播）：$t_{\\mathrm{comp}} = 0.080$ 秒\n-   重叠：通信和计算之间无重叠。\n-   并行效率 $E$ 的定义：在每 GPU 小批量大小保持不变的情况下，$P$ 个 GPU 上实现的吞吐量与 $P$ 个 GPU 上理想线性加速吞吐量的比率。\n\n**步骤 2：使用提取的给定信息进行验证**\n该问题具有科学依据，提法明确且客观。它描述了分布式深度学习领域中一个标准且真实的场景。为 AlexNet 的参数数量 ($N_{p}$)、GPU 数量 ($P$)、通信带宽 ($B$) 和每 GPU 计算时间 ($t_{\\mathrm{comp}}$) 提供的值是一致的，并且对于高性能计算系统是合理的。环形 all-reduce 通信模型和整体步长时间模型（计算 + 通信）是用于性能分析的标准且明确定义的简化模型。并行效率的定义是弱扩展性分析中的标准定义。该问题是自洽的，没有内部矛盾、歧义或事实错误。\n\n**步骤 3：结论与行动**\n问题有效。将制定完整的解决方案。\n\n**求解过程**\n\n求解需要计算并行效率 $E$。这涉及到确定使用 $P$ 个 GPU 进行单个训练步骤的总时间 $T(P)$，以及使用单个 GPU 的总时间 $T(1)$。\n\n首先，我们计算每一步中必须在所有 GPU 之间同步的梯度向量的总大小 $S$。该网络有 $N_p = 61{,}000{,}000$ 个可训练参数，每个参数的梯度由一个 $4$ 字节的单精度浮点数表示。\n$$S = N_p \\times 4 \\text{ 字节} = 61{,}000{,}000 \\times 4 = 244{,}000{,}000 \\text{ 字节}$$\n这可以用科学记数法表示为 $S = 2.44 \\times 10^8$ 字节。\n\n其次，我们计算环形 all-reduce 操作所需的通信时间 $t_{\\mathrm{comm}}$。该算法按 $2(P-1)$ 个顺序通信步骤进行。在每个步骤中，大小为 $\\frac{S}{P}$ 的数据块通过点对点链接发送。这样一个步骤的时间是块大小除以带宽 $B$。由于这些步骤是顺序的且忽略了延迟，总通信时间是每一步时间的总和：\n$$t_{\\mathrm{comm}} = (\\text{步骤数}) \\times (\\text{每步时间}) = 2(P-1) \\times \\frac{S/P}{B} = \\frac{2(P-1)S}{PB}$$\n我们代入给定值：$P = 8$，$S = 2.44 \\times 10^8$ 字节，以及 $B = 3.125 \\times 10^9$ 字节/秒。\n$$t_{\\mathrm{comm}} = \\frac{2(8-1)(2.44 \\times 10^8)}{8 \\times (3.125 \\times 10^9)} = \\frac{14 \\times 2.44 \\times 10^8}{25 \\times 10^9}$$\n$$t_{\\mathrm{comm}} = \\frac{34.16 \\times 10^8}{25 \\times 10^9} = \\frac{3.416 \\times 10^9}{2.5 \\times 10^{10}} = 0.13664 \\text{ 秒}$$\n\n第三，我们计算并行效率 $E$。在 $P$ 个 GPU 上进行一个训练步骤的总时间 $T(P)$ 是计算时间和通信时间的总和，因为没有重叠。\n$$T(P) = t_{\\mathrm{comp}} + t_{\\mathrm{comm}}$$\n在单个 GPU 上进行一个训练步骤的时间 $T(1)$ 只涉及计算，因为不需要 GPU 间的通信。因此，$t_{\\mathrm{comm}}(P=1) = 0$。\n$$T(1) = t_{\\mathrm{comp}}$$\n并行效率 $E$ 定义为实现吞吐量与理想吞吐量之比。设 $B_0$ 为恒定的每 GPU 小批量大小。\n$P$ 个 GPU 上的实现吞吐量为 $\\text{Throughput}(P) = \\frac{\\text{每步总样本数}}{\\text{每步时间}} = \\frac{P \\cdot B_0}{T(P)}$。\n$P$ 个 GPU 上的理想吞吐量是单个 GPU 吞吐量的 $P$ 倍：$\\text{Ideal Throughput}(P) = P \\times \\text{Throughput}(1) = P \\times \\frac{1 \\cdot B_0}{T(1)}$。\n效率 $E$ 是该比率：\n$$E = \\frac{\\text{Throughput}(P)}{\\text{Ideal Throughput}(P)} = \\frac{P \\cdot B_0 / T(P)}{P \\cdot B_0 / T(1)} = \\frac{T(1)}{T(P)}$$\n代入 $T(1)$ 和 $T(P)$ 的表达式：\n$$E = \\frac{t_{\\mathrm{comp}}}{t_{\\mathrm{comp}} + t_{\\mathrm{comm}}}$$\n现在，我们代入数值 $t_{\\mathrm{comp}} = 0.080$ 秒和我们计算出的 $t_{\\mathrm{comm}} = 0.13664$ 秒：\n$$E = \\frac{0.080}{0.080 + 0.13664} = \\frac{0.080}{0.21664} \\approx 0.3692762...$$\n问题要求将最终答案四舍五入到四位有效数字。\n$$E \\approx 0.3693$$", "answer": "$$\\boxed{0.3693}$$", "id": "3118605"}]}