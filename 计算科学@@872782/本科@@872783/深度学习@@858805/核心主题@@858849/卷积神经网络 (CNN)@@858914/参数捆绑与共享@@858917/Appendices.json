{"hands_on_practices": [{"introduction": "要真正理解参数绑定的威力，最好的方法就是亲手实践。这个练习将通过一个简洁的例子，直观地展示参数绑定如何为模型赋予特定的不变性。我们将构建一个简单的分类器[@problem_id:3161977]，并通过实验验证，当模型的不同部分共享参数时，它便能对输入数据的特定变换（如坐标交换）保持预测结果不变。", "problem": "考虑一个二元分类器，它被实现为一个前馈神经网络，该网络具有一个隐藏层，处理一个二维输入向量 $x = (x_1, x_2)$。隐藏层使用修正线性单元（ReLU）激活函数，其中修正线性单元（ReLU）定义为 $\\mathrm{ReLU}(z) = \\max(0, z)$。输出层是隐藏层激活值的标量仿射组合，最终的类别标签通过以 $0$ 为阈值来确定：如果标量输出大于或等于 $0$，则预测标签为 $1$，否则为 $0$。\n\n您将实现该分类器的两个变体：\n\n- 一个非绑定参数模型，其隐藏单元为\n  $$h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1), \\quad h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2),$$\n  输出为\n  $$y = v_1 h_1 + v_2 h_2 + c.$$\n\n- 一个绑定参数模型，其隐藏单元为\n  $$h_1 = \\mathrm{ReLU}(a x_1 + b), \\quad h_2 = \\mathrm{ReLU}(a x_2 + b),$$\n  输出为\n  $$y = v (h_1 + h_2) + c.$$\n\n这里的参数绑定意味着在结构对称的组件之间设置相等的参数，例如 $a_1 = a_2$，$b_1 = b_2$ 和 $v_1 = v_2$。\n\n我们如下定义在输入空间的一个变换 $T$ 下的不变性：对于一个将输入映射到标签的模型 $f$，$f$ 在集合 $X$ 上于变换 $T$ 下不变，当且仅当对于所有 $x \\in X$ 都有 $f(x) = f(T(x))$。需要测试的两个变换是：\n- 坐标交换变换 $S$，定义为 $S(x_1, x_2) = (x_2, x_1)$。\n- 符号翻转变换 $F$，定义为 $F(x_1, x_2) = (-x_1, -x_2)$。\n\n从前馈网络计算仿射映射和逐点非线性的组合这一基本基础出发，并结合上述不变性的定义，构建一个程序，该程序：\n- 实现非绑定和绑定两种模型。\n- 通过从区间 $[-1, 1]$ 上的均匀分布中独立采样每个坐标来生成合成测试输入 $x$。\n- 对于给定的变换 $T \\in \\{S, F\\}$，通过检查在采样集合上所有预测的标签是否都满足 $f(x) = f(T(x))$ 来经验性地验证不变性。\n\n您的程序必须使用以下测试套件，其中 $N$ 是样本数，“seed”指定伪随机数生成器种子：\n\n- 测试用例 1（非绑定，预期在 $S$ 下不具有不变性）：参数 $a_1 = 1.0$, $b_1 = 0.1$, $a_2 = 2.0$, $b_2 = -0.3$, $v_1 = 1.0$, $v_2 = -0.5$, $c = 0.05$，变换 $T = S$，$N = 200$，seed $= 42$。\n- 测试用例 2（绑定，预期在 $S$ 下具有不变性）：参数 $a = 1.5$, $b = -0.2$, $v = 0.7$, $c = -0.1$，变换 $T = S$，$N = 200$，seed $= 42$。\n- 测试用例 3（非绑定但参数相等，预期在 $S$ 下具有不变性）：参数 $a_1 = 0.8$, $b_1 = 0.0$, $a_2 = 0.8$, $b_2 = 0.0$, $v_1 = 1.0$, $v_2 = 1.0$, $c = 0.0$，变换 $T = S$，$N = 200$，seed $= 7$。\n- 测试用例 4（绑定，预期在 $F$ 下不具有不变性）：参数 $a = 1.0$, $b = 0.0$, $v = 0.5$, $c = 0.05$，变换 $T = F$，$N = 200$，seed $= 42$。\n- 测试用例 5（绑定且输出权重为零，预期因平凡恒定而在 $S$ 下具有不变性）：参数 $a = 1.0$, $b = 0.5$, $v = 0.0$, $c = 0.3$，变换 $T = S$，$N = 200$，seed $= 123$。\n\n对于每个测试用例，计算一个布尔值，指示模型在采样集上对于指定变换是否具有不变性。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result_1,result_2,\\dots]$），其中每个 $result_i$ 是按顺序排列的测试用例 $i$ 的布尔值。", "solution": "该问题要求对一个简单神经网络中参数绑定与其对特定输入变换的不变性之间的关系进行分析和经验性验证。我们将首先建立预期行为的数学基础，然后使用此框架来验证规定测试用例的结果。\n\n一个二元分类器由一个函数 $f(x)$ 定义，该函数将输入向量 $x$ 映射到 $\\{0, 1\\}$ 中的一个标签。如果对于给定域中的所有输入 $x$，都有 $f(x) = f(T(x))$，则称该分类器在变换 $T$ 下是不变的。分类规则由标量输出函数 $y$ 的符号给出，如果 $y \\ge 0$ 则标签为 $1$，否则为 $0$。因此，不变性 $f(x) = f(T(x))$ 成立，当且仅当 $y(x)$ 的符号与 $y(T(x))$ 的符号相同，即 $(\\,y(x) \\ge 0 \\land y(T(x)) \\ge 0\\,) \\lor (\\,y(x)  0 \\land y(T(x))  0\\,)$。对此，一个充分但不必要条件是 $y(x) = y(T(x))$。\n\n输入向量是 $x = (x_1, x_2)$。激活函数是修正线性单元，$\\mathrm{ReLU}(z) = \\max(0, z)$。\n\n两个模型是：\n1.  **非绑定模型**：隐藏单元是 $h_1 = \\mathrm{ReLU}(a_1 x_1 + b_1)$ 和 $h_2 = \\mathrm{ReLU}(a_2 x_2 + b_2)$。标量输出是 $y_{untied}(x_1, x_2) = v_1 h_1 + v_2 h_2 + c$。该结构是不对称的，用不同的参数处理每个输入坐标。\n\n2.  **绑定模型**：隐藏单元是 $h_1 = \\mathrm{ReLU}(a x_1 + b)$ 和 $h_2 = \\mathrm{ReLU}(a x_2 + b)$。标量输出是 $y_{tied}(x_1, x_2) = v (h_1 + h_2) + c$。隐藏层的参数 $(a, b)$ 和输出层的权重 $v$ 在两个输入分支间是共享（或“绑定”）的。\n\n两个变换是：\n1.  **坐标交换 ($S$)**：$S(x_1, x_2) = (x_2, x_1)$。\n2.  **符号翻转 ($F$)**：$F(x_1, x_2) = (-x_1, -x_2)$。\n\n**坐标交换 ($S$) 不变性分析**\n\n让我们分析每个模型对于交换后输入 $S(x) = (x_2, x_1)$ 的输出。\n\n*   **绑定模型**：\n    原始输入的输出是：\n    $$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\n    交换后输入的输出是：\n    $$y_{tied}(x_2, x_1) = v (\\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)) + c$$\n    由于加法交换律，$\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b) = \\mathrm{ReLU}(a x_2 + b) + \\mathrm{ReLU}(a x_1 + b)$。因此，对于所有 $x$，$y_{tied}(x_1, x_2) = y_{tied}(x_2, x_1)$。这意味着预测的标签也将是相同的，$f_{tied}(x) = f_{tied}(S(x))$。绑定模型架构内在地保证了对坐标交换变换的不变性。\n\n*   **非绑定模型**：\n    原始输入的输出是：\n    $$y_{untied}(x_1, x_2) = v_1 \\mathrm{ReLU}(a_1 x_1 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_2 + b_2) + c$$\n    交换后输入的输出是：\n    $$y_{untied}(x_2, x_1) = v_1 \\mathrm{ReLU}(a_1 x_2 + b_1) + v_2 \\mathrm{ReLU}(a_2 x_1 + b_2) + c$$\n    通常情况下，$y_{untied}(x_1, x_2) \\neq y_{untied}(x_2, x_1)$，除非参数具有特定的对称性。如果函数关于交换 $x_1$ 和 $x_2$ 是对称的，则可以实现不变性。这要求项可以被置换，即需要 $a_1=a_2$，$b_1=b_2$ 和 $v_1=v_2$。这些正是参数绑定所施加的约束。\n\n**符号翻转 ($F$) 不变性分析**\n\n让我们分析绑定模型对于符号翻转后输入 $F(x) = (-x_1, -x_2)$ 的输出。\n原始输入的输出是：\n$$y_{tied}(x_1, x_2) = v (\\mathrm{ReLU}(a x_1 + b) + \\mathrm{ReLU}(a x_2 + b)) + c$$\n翻转后输入的输出是：\n$$y_{tied}(-x_1, -x_2) = v (\\mathrm{ReLU}(a(-x_1) + b) + \\mathrm{ReLU}(a(-x_2) + b)) + c = v (\\mathrm{ReLU}(-a x_1 + b) + \\mathrm{ReLU}(-a x_2 + b)) + c$$\n为保证不变性，我们需要 $y_{tied}(x_1, x_2)$ 和 $y_{tied}(-x_1, -x_2)$ 具有相同的符号。通常情况下这是不成立的。$\\mathrm{ReLU}$ 函数不是偶函数，即 $\\mathrm{ReLU}(z) \\neq \\mathrm{ReLU}(-z)$，除非 $z=0$。因此，绑定架构并不能内在地保证对符号翻转变换的不变性。\n\n**通过测试用例进行经验性验证**\n\n我们现在将此推理应用于具体的测试用例。对于每个用例，我们生成 $N=200$ 个 $x=(x_1, x_2)$ 样本，其坐标从 $U[-1, 1]$ 中抽取，并检查是否对所有样本都满足 $f(x) = f(T(x))$。\n\n*   **测试用例 1**：非绑定模型，具有不对称参数（$a_1=1.0, b_1=0.1, a_2=2.0, b_2=-0.3, v_1=1.0, v_2=-0.5, c=0.05$），在变换 $S$ 下。正如我们的分析所预测的，参数对称性的缺乏将破坏不变性。预期结果为 `False`。\n\n*   **测试用例 2**：绑定模型（$a=1.5, b=-0.2, v=0.7, c=-0.1$），在变换 $S$ 下。如解析所示，绑定模型对坐标交换是不变的。预期结果为 `True`。\n\n*   **测试用例 3**：非绑定模型，但参数被手动设置为对称的（$a_1=0.8, b_1=0.0, a_2=0.8, b_2=0.0, v_1=1.0, v_2=1.0, c=0.0$）。该模型在功能上等价于一个绑定模型：$y_{untied} = \\mathrm{ReLU}(0.8 x_1) + \\mathrm{ReLU}(0.8 x_2)$。因此，它将对变换 $S$ 不变。这个用例强调了不变性是计算出的函数的属性，它由参数值决定，而不仅仅是模型的结构分类为“非绑定”。预期结果为 `True`。\n\n*   **测试用例 4**：绑定模型（$a=1.0, b=0.0, v=0.5, c=0.05$），在变换 $F$ 下。问题预期不具有不变性。让我们分析输出函数：\n    $$y = 0.5 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.0) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.0)) + 0.05 = 0.5 \\cdot (\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)) + 0.05$$\n    输入坐标 $x_1, x_2$ 从 $[-1, 1]$ 中采样。$\\mathrm{ReLU}$ 函数是非负的。\n    $\\mathrm{ReLU}(x_1) + \\mathrm{ReLU}(x_2)$ 的最小值是 $0$（当 $x_1 \\le 0$ 且 $x_2 \\le 0$ 时）。\n    在这种情况下，最小输出是 $y_{min} = 0.5 \\cdot 0 + 0.05 = 0.05$。\n    由于输出 $y$ 总是大于或等于 $0.05$，它总是大于阈值 $0$。这意味着预测的标签总是 $1$，即对于域中的所有输入，$f(x) \\equiv 1$。一个常数函数对于任何变换（包括 $F$）都是平凡不变的。因此，与问题的非正式预期相反，该模型将被经验性地验证为不变。结果将是 `True`。\n\n*   **测试用例 5**：绑定模型，输出权重为零（$a=1.0, b=0.5, v=0.0, c=0.3$），在变换 $S$ 下。输出函数是：\n    $$y = 0.0 \\cdot (\\mathrm{ReLU}(1.0 \\cdot x_1 + 0.5) + \\mathrm{ReLU}(1.0 \\cdot x_2 + 0.5)) + 0.3 = 0.3$$\n    输出是一个常数 $0.3$。由于 $0.3 \\ge 0$，预测的标签总是 $1$。与前一个情况一样，分类器是一个常数函数，因此对任何变换都是平凡不变的。结果为 `True`。\n\n程序将实现这些模型和检查，以生成对应这五个用例的布尔值列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests untied and tied neural network models for invariance\n    under specified transformations.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 1.0, \"b1\": 0.1, \"a2\": 2.0, \"b2\": -0.3, \"v1\": 1.0, \"v2\": -0.5, \"c\": 0.05},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.5, \"b\": -0.2, \"v\": 0.7, \"c\": -0.1},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"untied\",\n            \"params\": {\"a1\": 0.8, \"b1\": 0.0, \"a2\": 0.8, \"b2\": 0.0, \"v1\": 1.0, \"v2\": 1.0, \"c\": 0.0},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 7\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.0, \"v\": 0.5, \"c\": 0.05},\n            \"transformation\": \"F\", \"N\": 200, \"seed\": 42\n        },\n        {\n            \"model\": \"tied\",\n            \"params\": {\"a\": 1.0, \"b\": 0.5, \"v\": 0.0, \"c\": 0.3},\n            \"transformation\": \"S\", \"N\": 200, \"seed\": 123\n        }\n    ]\n\n    def relu(z):\n        \"\"\"Rectified Linear Unit activation function.\"\"\"\n        return np.maximum(0, z)\n\n    def untied_model(X, params):\n        \"\"\"Computes labels for the untied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a1\"] * x1 + p[\"b1\"])\n        h2 = relu(p[\"a2\"] * x2 + p[\"b2\"])\n        y = p[\"v1\"] * h1 + p[\"v2\"] * h2 + p[\"c\"]\n        return (y = 0).astype(int)\n\n    def tied_model(X, params):\n        \"\"\"Computes labels for the tied-parameter model.\"\"\"\n        x1, x2 = X[:, 0], X[:, 1]\n        p = params\n        h1 = relu(p[\"a\"] * x1 + p[\"b\"])\n        h2 = relu(p[\"a\"] * x2 + p[\"b\"])\n        y = p[\"v\"] * (h1 + h2) + p[\"c\"]\n        return (y = 0).astype(int)\n\n    model_funcs = {\n        \"untied\": untied_model,\n        \"tied\": tied_model\n    }\n\n    transform_funcs = {\n        \"S\": lambda X: X[:, ::-1],  # Coordinate-swap\n        \"F\": lambda X: -X           # Sign-flip\n    }\n\n    results = []\n    for case in test_cases:\n        # 1. Set seed and generate data\n        np.random.seed(case[\"seed\"])\n        X_original = np.random.uniform(-1, 1, size=(case[\"N\"], 2))\n\n        # 2. Apply transformation\n        transform_func = transform_funcs[case[\"transformation\"]]\n        X_transformed = transform_func(X_original)\n\n        # 3. Select model and compute labels\n        model_func = model_funcs[case[\"model\"]]\n        labels_original = model_func(X_original, case[\"params\"])\n        labels_transformed = model_func(X_transformed, case[\"params\"])\n\n        # 4. Check for invariance\n        is_invariant = np.array_equal(labels_original, labels_transformed)\n        results.append(is_invariant)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161977"}, {"introduction": "在实践中，强制参数完全相等（硬绑定）有时过于严格。“软绑定”提供了一种更灵活的方案，它不强制参数相等，而是“鼓励”它们变得相似。在这个练习[@problem_id:3161931]中，我们将探索如何通过对参数差异施加正则化惩罚来实现软绑定，并分析这种方法在模型压缩和预测精度之间带来的权衡。", "problem": "您需要在一个简单的二元分类器中，通过对参数向量之差进行正则化，来实现和分析软参数绑定。本问题探讨不同的范数如何影响压缩（即有多少参数变得有效共享）和预测准确率。实现必须是一个完整的、可运行的程序。\n\n本问题的基础是经验风险最小化（ERM）、二元交叉熵（BCE）、基于梯度的优化和凸范数正则化。令 $\\sigma(u) = \\frac{1}{1 + e^{-u}}$ 表示逻辑 sigmoid 函数。对于预测概率为 $p(x)$ 的单个样本 $(x, y)$，其 BCE 为 $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$，而 ERM 通过样本均值来近似期望风险。\n\n模型规格：\n- 输入为向量 $x \\in \\mathbb{R}^d$，标签为 $y \\in \\{0,1\\}$。\n- 模型维护两个权重向量 $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$。\n- 预测值为 $p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$。\n- 训练目标是训练集上的平均二元交叉熵（BCE）加上应用于参数差 $d = w^{(a)} - w^{(b)}$ 的软绑定惩罚项。\n\n待比较的软绑定惩罚项：\n- $L_1$ 惩罚项：$\\lambda_1 \\|d\\|_1$。\n- $L_2$ 惩罚项（平方）：$\\lambda_2 \\|d\\|_2^2$。\n- 弹性网络惩罚项：$\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$。\n\n数据生成（确定性）：\n- 所有随机性均使用伪随机种子 $0$。\n- 维度 $d = 10$。\n- 训练集大小 $n_{\\text{train}} = 512$，验证集大小 $n_{\\text{val}} = 256$。\n- 抽取一次 $w^\\star \\sim \\mathcal{N}(0, I_d)$ 并对两个集合固定该值。\n- 对所有样本独立地抽取输入 $x \\sim \\mathcal{N}(0, I_d)$。\n- 对所有样本独立地抽取噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，其中 $\\sigma^2 = 0.25$。\n- 将标签定义为 $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon > 0\\}$。\n\n训练过程：\n- 在 $\\mathbb{R}^d$ 中初始化 $w^{(a)} = 0$ 和 $w^{(b)} = 0$。\n- 使用全批量梯度下降法优化目标函数，迭代 $T = 400$ 次，步长为 $\\eta = 0.05$。\n- 对于 $L_1$ 惩罚项，使用次梯度 $\\text{sign}(d)$。\n- 对于 $L_2$ 惩罚项（平方），使用梯度 $2\\lambda_2 d$。\n- 对于弹性网络，将各自的梯度相加。\n\n评估指标：\n- 压缩率：满足 $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$ 的索引 $i \\in \\{1,\\dots,d\\}$ 的比例，容差为 $\\tau = 10^{-2}$。\n- 准确率：在验证集上，使用决策规则 $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$ 正确分类的样本比例。\n\n测试套件：\n实现并运行以下 5 个测试用例，每个用例由 $(\\lambda_1, \\lambda_2)$ 和惩罚类型定义：\n1. 基线，无绑定：$\\lambda_1 = 0$，$\\lambda_2 = 0$。\n2. $L_2$ 中等：$\\lambda_1 = 0$，$\\lambda_2 = 0.1$。\n3. $L_1$ 中等：$\\lambda_1 = 0.05$，$\\lambda_2 = 0$。\n4. $L_2$ 强：$\\lambda_1 = 0$，$\\lambda_2 = 1.0$。\n5. 弹性网络平衡：$\\lambda_1 = 0.05$，$\\lambda_2 = 0.2$。\n\n您的程序应实现上述设置，并按给定顺序为每个测试用例生成一个 $[\\text{compression\\_ratio}, \\text{accuracy}]$ 对，其中两个值均为浮点数。最终输出格式必须是包含这 5 个有序对的单行列表，例如：\n$[[c_1,a_1],[c_2,a_2],[c_3,a_3],[c_4,a_4],[c_5,a_5]]$，其中每个 $c_i$ 和 $a_i$ 都是四舍五入到 4 位小数的浮点数。", "solution": "首先根据科学合理性、适定性和客观性标准对问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **模型**：具有两个权重向量 $w^{(a)}, w^{(b)} \\in \\mathbb{R}^d$ 的二元分类器。\n- **预测函数**：$p(x) = \\sigma\\!\\left(\\frac{1}{2}\\left(w^{(a)\\top}x + w^{(b)\\top}x\\right)\\right)$，其中 $\\sigma(u) = (1 + e^{-u})^{-1}$ 是逻辑 sigmoid 函数。\n- **损失函数**：训练集上的平均二元交叉熵（BCE），其中对于单个样本 $(x, y)$，损失为 $-\\left(y \\log p(x) + (1-y)\\log(1-p(x))\\right)$。\n- **正则化**：对差分向量 $d = w^{(a)} - w^{(b)}$ 应用一个惩罚项。\n    - $L_1$ 惩罚项：$\\lambda_1 \\|d\\|_1$。\n    - $L_2$ 平方惩罚项：$\\lambda_2 \\|d\\|_2^2$。\n    - 弹性网络惩罚项：$\\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2$。\n- **数据生成**：\n    - 伪随机种子：$0$。\n    - 维度 $d = 10$。\n    - 训练集大小 $n_{\\text{train}} = 512$。\n    - 验证集大小 $n_{\\text{val}} = 256$。\n    - 真实权重 $w^\\star \\sim \\mathcal{N}(0, I_d)$。\n    - 输入 $x \\sim \\mathcal{N}(0, I_d)$。\n    - 噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$，噪声方差为 $\\sigma^2 = 0.25$。\n    - 标签 $y = \\mathbf{1}\\{w^{\\star\\top} x + \\epsilon  0\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- **优化**：\n    - 算法：全批量梯度下降法。\n    - 初始化：$w^{(a)} = 0$, $w^{(b)} = 0$。\n    - 迭代次数：$T = 400$。\n    - 步长：$\\eta = 0.05$。\n    - 惩罚项的梯度：$L_1$ 使用次梯度 $\\text{sign}(d)$，$L_2$ 平方使用梯度 $2\\lambda_2 d$。\n- **评估**：\n    - **压缩率**：索引 $i$ 中满足 $|w^{(a)}_i - w^{(b)}_i| \\le \\tau$ 的比例，容差为 $\\tau = 10^{-2}$。\n    - **准确率**：在验证集上做出正确预测的比例，决策规则为 $\\hat{y} = \\mathbf{1}\\{p(x) \\ge 0.5\\}$。\n- **测试套件**：5 个用例，其 $(\\lambda_1, \\lambda_2)$ 值分别为：(1) $(0, 0)$，(2) $(0, 0.1)$，(3) $(0.05, 0)$，(4) $(0, 1.0)$，(5) $(0.05, 0.2)$。\n- **输出格式**：一个包含 5 个 $[\\text{compression\\_ratio}, \\text{accuracy}]$ 对的列表，四舍五入到 4 位小数。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，基于机器学习的标准原理，包括经验风险最小化、逻辑回归、基于梯度的优化以及正则化技术（$L_1$、$L_2$、弹性网络）。参数绑定的概念是神经网络设计中一种成熟的做法。\n\n该问题是适定的。目标函数是平均 BCE 损失（模型参数的凸函数）和一个凸正则化惩罚项的和。最终的目标函数是凸函数，通过梯度下降法对其进行最小化是一个标准且稳定的数值问题。所有参数、数据生成过程和评估指标都得到了精确无歧义的规定，确保可以计算出唯一且有意义的解。\n\n该问题是客观、完整且一致的。它使用形式化的数学语言描述，没有主观陈述。提供了实现所需的所有必要信息，并且没有内部矛盾。\n\n### 步骤 3：结论与行动\n该问题被判定为**有效**。将开发一个完整的解决方案。\n\n### 解题推导\n\n目标是通过使用梯度下降法最小化一个复合目标函数 $J(w^{(a)}, w^{(b)})$ 来训练模型。该目标函数结合了一个数据拟合项（平均 BCE 损失）和一个促进参数绑定的正则化项。\n\n**目标函数**\n\n设训练数据为 $\\{(x_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$。模型对样本 $x_i$ 的预测为 $p_i = p(x_i) = \\sigma(z_i)$，其中 logit 为 $z_i = \\frac{1}{2}(w^{(a)\\top}x_i + w^{(b)\\top}x_i)$。总目标函数为：\n$$\nJ(w^{(a)}, w^{(b)}) = J_{\\text{BCE}}(w^{(a)}, w^{(b)}) + R(w^{(a)} - w^{(b)})\n$$\n第一项是平均 BCE 损失：\n$$\nJ_{\\text{BCE}}(w^{(a)}, w^{(b)}) = -\\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\left[ y_i \\log(p_i) + (1-y_i) \\log(1-p_i) \\right]\n$$\n第二项是作用于差分向量 $d = w^{(a)} - w^{(b)}$ 的软绑定惩罚项的一般形式：\n$$\nR(d) = \\lambda_1 \\|d\\|_1 + \\lambda_2 \\|d\\|_2^2\n$$\n\n**梯度计算**\n为实现梯度下降法，我们需要计算 $J$ 相对于 $w^{(a)}$ 和 $w^{(b)}$ 的梯度。我们使用链式法则来计算这些梯度。\n\n单个样本的 BCE 损失 $L_i$ 相对于 logit $z_i$ 的梯度是众所周知：\n$$\n\\frac{\\partial L_i}{\\partial z_i} = p_i - y_i\n$$\nlogit $z_i$ 相对于权重向量的梯度为：\n$$\n\\nabla_{w^{(a)}} z_i = \\frac{1}{2}x_i \\quad \\text{and} \\quad \\nabla_{w^{(b)}} z_i = \\frac{1}{2}x_i\n$$\n对整个训练集上的 BCE 项应用链式法则：\n$$\n\\nabla_{w^{(a)}} J_{\\text{BCE}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\frac{\\partial L_i}{\\partial z_i} \\nabla_{w^{(a)}} z_i = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (p_i - y_i) \\frac{1}{2}x_i = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\n根据对称性，相对于 $w^{(b)}$ 的梯度是相同的：\n$$\n\\nabla_{w^{(b)}} J_{\\text{BCE}} = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y)\n$$\n其中 $X$ 是 $n_{\\text{train}} \\times d$ 的数据矩阵，$P$ 是预测向量，$Y$ 是真实标签向量。\n\n接下来，我们求正则化项 $R(d) = R(w^{(a)} - w^{(b)})$ 的梯度。\n$$\n\\nabla_{w^{(a)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(a)}} = \\nabla_{d} R(d)\n$$\n$$\n\\nabla_{w^{(b)}} R(d) = \\nabla_{d} R(d) \\cdot \\frac{\\partial d}{\\partial w^{(b)}} = -\\nabla_{d} R(d)\n$$\n（次）梯度 $\\nabla_{d} R(d)$ 由下式给出：\n$$\n\\nabla_{d} R(d) = \\lambda_1 \\text{sign}(d) + 2\\lambda_2 d\n$$\n这里，$\\text{sign}(d)$ 是 $L_1$ 范数的次梯度。在算法实现中，$\\text{sign}(0)$ 可以取为 $0$。\n\n结合这些项，完整的梯度为：\n$$\n\\nabla_{w^{(a)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) + (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n$$\n\\nabla_{w^{(b)}} J = \\frac{1}{2n_{\\text{train}}} X^\\top(P-Y) - (\\lambda_1 \\text{sign}(d) + 2\\lambda_2 d)\n$$\n\n**优化算法**\n权重使用全批量梯度下降法进行更新，迭代 $T=400$ 次，学习率为 $\\eta=0.05$。从 $w^{(a)}_0 = 0$ 和 $w^{(b)}_0 = 0$ 开始，第 $t$ 步的更新规则是：\n$$\nw^{(a)}_{t+1} = w^{(a)}_t - \\eta \\nabla_{w^{(a)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\n$$\nw^{(b)}_{t+1} = w^{(b)}_t - \\eta \\nabla_{w^{(b)}} J(w^{(a)}_t, w^{(b)}_t)\n$$\n对由 $(\\lambda_1, \\lambda_2)$ 对定义的 5 个测试用例中的每一个都重复此过程。\n\n**评估**\n训练结束后，计算两个指标：\n1.  **压缩率**：该指标衡量参数共享的程度。它是对应权重之间的绝对差值低于容差 $\\tau = 10^{-2}$ 的维度所占的比例。\n    $$\n    \\text{Compression Ratio} = \\frac{1}{d} \\sum_{i=1}^d \\mathbf{1}\\{|w^{(a)}_i - w^{(b)}_i| \\le \\tau\\}\n    $$\n2.  **准确率**：该指标衡量在未见过的验证集上的预测性能。如果预测概率 $p(x) \\ge 0.5$，则预测标签 $\\hat{y}$ 为 1，否则为 0。这等价于检查 logit $z = \\frac{1}{2}(w^{(a)\\top}x + w^{(b)\\top}x) \\ge 0$。\n    $$\n    \\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{i=1}^{n_{\\text{val}}} \\mathbf{1}\\{\\hat{y}_i = y_i^{\\text{val}}\\}\n    $$\n实现将遵循这些推导来生成所需的输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes soft parameter tying in a binary classifier.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    SEED = 0\n    D = 10  # Dimension of input vectors\n    N_TRAIN = 512\n    N_VAL = 256\n    NOISE_VAR = 0.25\n    \n    T = 400  # Number of iterations\n    ETA = 0.05  # Step size (learning rate)\n    TAU = 1e-2  # Tolerance for compression ratio\n\n    # --- Test Cases ---\n    test_cases = [\n        # (lambda_1, lambda_2), penalty_type\n        (0.0, 0.0),      # 1. Baseline, no tying\n        (0.0, 0.1),      # 2. L2 moderate\n        (0.05, 0.0),     # 3. L1 moderate\n        (0.0, 1.0),      # 4. L2 strong\n        (0.05, 0.2),     # 5. Elastic net balanced\n    ]\n\n    # --- Helper Functions ---\n    def sigmoid(u):\n        \"\"\"Numerically stable logistic sigmoid function.\"\"\"\n        return 1 / (1 + np.exp(-u))\n\n    def generate_data(n_samples, d, w_star, noise_std, rng):\n        \"\"\"Generates synthetic data for binary classification.\"\"\"\n        X = rng.standard_normal(size=(n_samples, d))\n        epsilon = rng.normal(0, noise_std, size=n_samples)\n        logits = X @ w_star + epsilon\n        y = (logits  0).astype(int)\n        return X, y\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(SEED)\n    w_star = rng.standard_normal(size=D)\n    noise_std = np.sqrt(NOISE_VAR)\n    \n    X_train, y_train = generate_data(N_TRAIN, D, w_star, noise_std, rng)\n    X_val, y_val = generate_data(N_VAL, D, w_star, noise_std, rng)\n\n    results = []\n\n    # --- Main Loop: Train and Evaluate for each test case ---\n    for lambda_1, lambda_2 in test_cases:\n        # Initialize weights\n        w_a = np.zeros(D)\n        w_b = np.zeros(D)\n\n        # Full-batch gradient descent\n        for _ in range(T):\n            # Forward pass\n            logits = 0.5 * (X_train @ w_a + X_train @ w_b)\n            predictions = sigmoid(logits)\n            \n            # --- Gradient Calculation ---\n            # Gradient of BCE loss term\n            error = predictions - y_train\n            grad_bce = (1 / (2 * N_TRAIN)) * X_train.T @ error\n\n            # Gradient of regularization term\n            d = w_a - w_b\n            grad_reg = lambda_1 * np.sign(d) + 2 * lambda_2 * d\n            \n            # Full gradients for w_a and w_b\n            grad_w_a = grad_bce + grad_reg\n            grad_w_b = grad_bce - grad_reg\n\n            # --- Weight Update ---\n            w_a -= ETA * grad_w_a\n            w_b -= ETA * grad_w_b\n\n        # --- Evaluation ---\n        # 1. Compression Ratio\n        final_d = w_a - w_b\n        compression_ratio = np.mean(np.abs(final_d) = TAU)\n\n        # 2. Accuracy on validation set\n        val_logits = 0.5 * (X_val @ w_a + X_val @ w_b)\n        y_hat_val = (val_logits = 0).astype(int)\n        accuracy = np.mean(y_hat_val == y_val)\n        \n        results.append((compression_ratio, accuracy))\n\n    # --- Format and Print Output ---\n    output_str = \"[\" + \",\".join([f\"[{c:.4f},{a:.4f}]\" for c, a in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3161931"}, {"introduction": "参数绑定是构建高级神经网络架构（如等变网络）的基石。这个高级练习[@problem_id:3161942]将带你实现一个基于旋转群 $C_4$ 的群卷积层，这是一种精巧的参数共享形式，专门用于处理旋转等对称性。我们将通过从一个基础滤波器旋转生成一组绑定的滤波器，并定量地证明这种结构在处理旋转对称任务时所具有的优越性。", "problem": "给定一个二维离散信号，要求您在一个四阶循环群（记为 $C_4$）上实现群卷积（Group Convolution, GC），该群由 $\\pi/2$ 弧度的倍数构成的旋转组成。目标是通过生成单个基础滤波器的旋转版本来实现参数共享，使用这些版本计算方向索引的特征图，并量化 GC 的等变性。您必须将这些性质与一个代表“单独数据增强”的基线进行比较，此处该基线模型为将单个未旋转的滤波器应用于输入及其旋转版本，而不进行任何方向通道的参数共享或滤波器变换。比较将在不变性和等变性相关的旋转结构化任务上进行。\n\n从以下基础定义开始：\n- 卷积神经网络（CNN）中使用的离散互相关定义为：对于输入 $x \\in \\mathbb{R}^{H \\times W}$ 和核 $k \\in \\mathbb{R}^{m \\times n}$，使用零填充以保持空间尺寸，\n$$\n(y = x \\star k)_{i,j} = \\sum_{u=0}^{m-1}\\sum_{v=0}^{n-1} x_{i+u-p, j+v-q}\\,k_{u,v},\n$$\n其中 $p = \\lfloor m/2 \\rfloor$ 和 $q = \\lfloor n/2 \\rfloor$，图像外的索引被视为零。\n- 循环群 $C_4 = \\{0,1,2,3\\}$ 表示在网格上通过 $r \\cdot \\pi/2$ 弧度进行旋转的操作 $R_r$，通过使用整数网格旋转 $R_r(x) = \\operatorname{rot90}(x, r)$ 的 $90^{\\circ}$ 增量旋转来精确实现。\n- 带有参数共享的 $C_4$ 上的群卷积（GC）从单个基础核 $k$ 构建旋转后的核 $k_g = R_g(k)$，并为每个 $g \\in C_4$ 返回方向索引的特征图 $y[g] = x \\star k_g$。\n- GC 的等变性是指将输入旋转 $r$ 会引起 GC 输出的可预测变换的性质：对于旋转 $R_r$，当使用零填充和精确的 $90^{\\circ}$ 网格旋转时，以下关系\n$$\nx_r = R_r(x), \\quad y_r[g] = x_r \\star k_g, \\quad \\text{且} \\quad y_r[g] \\stackrel{?}{=} R_r\\!\\Big(y\\big[(g-r) \\bmod 4\\big]\\Big)\n$$\n应成立。\n- 不变性可以通过对群进行池化来引入，例如通过求平均值：$P(x) = \\frac{1}{4}\\sum_{g=0}^{3} y[g]$。在旋转 $R_r$ 下，预测为 $P(x_r) \\stackrel{?}{=} R_r\\big(P(x)\\big)$。\n\n您必须实现：\n1. 一个函数，用于执行带有零填充的离散互相关，以产生空间尺寸“相同”的输出。\n2. 一个函数，用于使用整数网格旋转将图像和核旋转 $r \\in C_4$。\n3. 带有参数共享（从单个基础核旋转得到的核）的 $C_4$ 上的群卷积，产生四个方向索引的特征图。\n4. 一个代表推理中“单独数据增强”的基线，使用单个未旋转的滤波器 $k$ 应用于原始输入和旋转后的输入，而不生成旋转的滤波器或方向通道。\n\n您必须计算以下定量指标：\n- GC 的等变性误差：\n$$\nE_{\\text{GC}}(x, k, r) = \\frac{1}{4HW}\\sum_{g=0}^{3}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} \\Big( y_r[g]_{i,j} - \\big(R_r\\!\\big(y[(g-r)\\bmod 4]\\big)\\big)_{i,j} \\Big)^2.\n$$\n- 池化后 GC 的不变性误差：\n$$\nE_{\\text{pool}}(x, k, r) = \\frac{1}{HW}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} \\Big( P(x_r)_{i,j} - \\big(R_r(P(x))\\big)_{i,j} \\Big)^2.\n$$\n- 仅增强的单滤波器基线的不变性误差：\n$$\nE_{\\text{aug-single}}(x, k, r) = \\frac{1}{HW}\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} \\Big( S(x_r)_{i,j} - \\big(R_r(S(x))\\big)_{i,j} \\Big)^2,\n$$\n其中 $S(x) = x \\star k$ 是未旋转滤波器的单通道输出。\n\n测试套件。使用尺寸为 $9 \\times 9$、二元条目为 $0$ 或 $1$ 的图像，基础核\n$$\nk = \\begin{bmatrix}\n0  -1  0 \\\\\n0  \\phantom{-}2  0 \\\\\n0  -1  0\n\\end{bmatrix},\n$$\n以及旋转角度索引 $r=1$（即 $\\pi/2$ 弧度）。\n构建以下输入：\n- 测试用例 1（理想情况）：图像中居中的垂直条，定义为对于所有 $i \\in \\{0,\\dots,8\\}$，$x_{i,4} = 1$，否则 $x_{i,j} = 0$。\n- 测试用例 2（旋转对称的特殊情况）：图像中居中的加号形状，定义为对于所有 $i$，$x_{i,4} = 1$，以及对于所有 $j$，$x_{4,j} = 1$（因此中心行和中心列都为1），否则为 $0$。\n- 测试用例 3（非平凡方向的特殊情况）：主对角线条，定义为对于所有 $i$，$x_{i,i} = 1$，否则为 $0$。\n\n对于每个测试用例，计算并返回三元组 $\\big(E_{\\text{GC}}, E_{\\text{pool}}, E_{\\text{aug-single}}\\big)$ 作为实数。您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，按测试用例 1、2、3 的顺序排列，每个用例贡献三个数字，因此最终输出总共包含九个逗号分隔的浮点数，例如 $[e_{11}, e_{12}, e_{13}, e_{21}, e_{22}, e_{23}, e_{31}, e_{32}, e_{33}]$。除了群索引定义外，无需报告任何物理单位或角度，所有输出都必须是实数。", "solution": "该问题要求实现和定量分析四阶循环群 $C_4$ 上的群卷积（GC），$C_4$ 在数学上表示二维网格上的 $90$ 度旋转。问题的核心是验证 GC 的等变性，并将其性能与基线模型进行比较。\n\n群卷积背后的关键原理是构建保证遵循特定对称性（在此情况下是旋转对称性）的神经网络层。如果对一个函数 $f$ 的输入应用一个变换 $g \\in G$，其输出会发生可预测的变换，则称该函数对变换群 $G$ 具有等变性。对于具有 $C_4$ 群的卷积层，此性质正式表示为：对旋转后的输入进行卷积等同于旋转原始输入的卷积输出，尽管方向通道会发生相应的移位。问题陈述将其表述为检查是否 $y_r[g] = R_r(y[(g-r) \\bmod 4])$，其中 $y_r[g]$ 是旋转后输入 $x_r=R_r(x)$ 的第 $g$ 个特征图，而 $R_r$ 是旋转算子。\n\n参数共享是 GC 设计的核心。它不是为四个期望的方向分别学习一个独立的核，而是学习一个单一的基础核 $k$。其他的核 $k_g$ 是通过将群作用（旋转）应用于此基础核来生成的：$k_g = R_g(k)$，其中 $g \\in C_4 = \\{0, 1, 2, 3\\}$。这极大地减少了可学习参数的数量，并将旋转对称性直接嵌入到模型架构中。\n\n离散网格上的群作用 $R_r$ 通过对像素值矩阵进行 $90$ 度旋转来实现，该操作表示为 $\\operatorname{rot90}(x, r)$。卷积操作被定义为离散互相关。一个关键细节是使用零填充来保持输入（“same”卷积）的空间维度。这个选择对等变性有深远的影响。用零填充定义的卷积算子 $\\star$ 与旋转算子 $R_r$ 不可交换。也就是说，$R_r(x \\star k) \\neq R_r(x) \\star R_r(k)$。这是因为填充是相对于数组的固定边界施加的；先旋转会改变哪些像素靠近边界，从而受到填充的不同影响。结果，完美的等变性在特征图的边界处被破坏。等变性误差 $E_{\\text{GC}}$ 是一个旨在量化由这些边界效应引起的与完美等变性的偏差的度量。\n\n从等变特征图 $y[g]$ 中，可以构建对旋转不变的特征。一种标准方法是在群维度上进行池化。问题定义了一个平均池化操作，$P(x) = \\frac{1}{4}\\sum_{g=0}^{3} y[g]$。如果底层是完美等变的，这个池化输出将是等变的：$P(R_r(x)) = R_r(P(x))$。不变性误差 $E_{\\text{pool}}$ 衡量了与此性质的偏差。所提供的基础核 $k$ 的一个特殊特性是其 $C_4$ 旋转之和为零矩阵：$\\sum_{g \\in C_4} R_g(k) = 0$。这是因为 $k_0+k_2=0$ 和 $k_1+k_3=0$。因此，池化输出 $P(x) = x \\star (\\frac{1}{4}\\sum_g k_g) = x \\star 0 = 0$ 始终是零矩阵，与输入 $x$ 无关。这使得池化表示 $P(x)$ 是平凡且完美旋转对称的，意味着 $P(R_r(x)) = 0$ 和 $R_r(P(x)) = 0$。因此，对于这个问题，不变性误差 $E_{\\text{pool}}$ 将恰好为零。\n\n基线模型代表了一种朴素的方法，有时被称为“测试时数据增强”，即单个未旋转的滤波器 $k$ 应用于原始输入 $x$ 及其旋转版本 $x_r$。输出为 $S(x) = x \\star k$ 和 $S(x_r) = x_r \\star k$。不变性误差 $E_{\\text{aug-single}}$ 衡量旋转输入是否等同于旋转输出，即 $S(x_r) \\stackrel{?}{=} R_r(S(x))$。由于滤波器 $k$ 是方向特定的（它检测垂直边缘），并且没有对旋转后的输入进行调整，预计该性质不成立。例如，将一个垂直边缘检测器应用于一个水平条，其结果将与旋转应用于垂直条的同一检测器的输出大不相同。我们预计 $E_{\\text{aug-single}}$ 将显著大于 $E_{\\text{GC}}$，这证明了这种方法在正确处理旋转对称性方面的失败。\n\n实现计划如下：\n1.  使用 `numpy.rot90` 定义旋转 $R_r$ 的函数。\n2.  使用 `scipy.signal.correlate2d` 并设置 `mode='same'` 和零填充，定义互相关 $\\star$ 的函数。\n3.  实现群卷积函数，该函数生成旋转后的核并计算四个特征图。\n4.  实现一个主函数，对于每个测试用例，计算原始输入和旋转输入的输出（$y$, $y_r$, $S(x)$, $S(x_r)$），然后根据其指定公式计算三个误差指标 $E_{\\text{GC}}$、$E_{\\text{pool}}$ 和 $E_{\\text{aug-single}}$。然后聚合结果并格式化输出。\n\n因此，该问题是一个定义明确的计算练习，旨在验证群等变网络的理论性质，并理解像填充这样的实现选择的实际影响。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import correlate2d\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute equivariance/invariance errors.\n    \"\"\"\n\n    def rotate(array, r):\n        \"\"\"\n        Rotates a 2D numpy array counter-clockwise by r * 90 degrees.\n        r is an element of the cyclic group C4 = {0, 1, 2, 3}.\n        \"\"\"\n        # np.rot90(m, k) rotates k times by 90 degrees CCW.\n        return np.rot90(array, k=r)\n\n    def cross_correlate(x, k):\n        \"\"\"\n        Performs 2D cross-correlation with zero-padding for 'same' output size.\n        This matches the definition provided in the problem statement.\n        \"\"\"\n        return correlate2d(x, k, mode='same', boundary='fill', fillvalue=0)\n\n    def group_convolution(x, k_base):\n        \"\"\"\n        Performs Group Convolution over C4 for a given input x and base kernel k_base.\n        Returns a list of 4 orientation-indexed feature maps.\n        \"\"\"\n        C4_indices = [0, 1, 2, 3]\n        \n        # Generate the set of rotated kernels from the base kernel\n        kernels = [rotate(k_base, g) for g in C4_indices]\n        \n        # Compute the feature map for each rotated kernel\n        feature_maps = [cross_correlate(x, k_g) for k_g in kernels]\n        \n        return feature_maps\n\n    def calculate_metrics(x, k, r):\n        \"\"\"\n        Calculates the three error metrics for a given test case.\n        \"\"\"\n        H, W = x.shape\n        C4_indices = [0, 1, 2, 3]\n        \n        # --- Group Convolution (GC) Calculations ---\n        \n        # 1. Compute outputs for original input x\n        y = group_convolution(x, k)  # y is a list of 4 feature maps y[g]\n        \n        # 2. Compute outputs for rotated input x_r\n        x_r = rotate(x, r)\n        y_r = group_convolution(x_r, k)  # y_r is a list of 4 feature maps y_r[g]\n        \n        # 3. Calculate E_GC (Equivariance Error for GC)\n        total_sq_error_gc = 0.0\n        for g in C4_indices:\n            # The equivariance property states: y_r[g] should be R_r(y[(g-r) mod 4])\n            expected_y_g = rotate(y[(g - r) % 4], r)\n            diff_gc = y_r[g] - expected_y_g\n            total_sq_error_gc += np.sum(diff_gc**2)\n            \n        E_GC = total_sq_error_gc / (4 * H * W)\n        \n        # --- Pooled GC Calculations ---\n        \n        # 1. Pool the outputs for x\n        P_x = np.sum(y, axis=0) / 4.0\n        \n        # 2. Pool the outputs for x_r\n        P_xr = np.sum(y_r, axis=0) / 4.0\n        \n        # 3. Calculate E_pool (Invariance Error for Pooled GC)\n        expected_P_xr = rotate(P_x, r)\n        diff_pool = P_xr - expected_P_xr\n        E_pool = np.sum(diff_pool**2) / (H * W)\n        \n        # --- Augmentation-only Single-Filter Baseline Calculations ---\n        \n        # 1. Compute output S(x) = x * k\n        S_x = cross_correlate(x, k)\n        \n        # 2. Compute output S(x_r) = x_r * k\n        S_xr = cross_correlate(x_r, k)\n        \n        # 3. Calculate E_aug_single (Invariance Error for Baseline)\n        expected_S_xr = rotate(S_x, r)\n        diff_aug = S_xr - expected_S_xr\n        E_aug_single = np.sum(diff_aug**2) / (H * W)\n        \n        return (E_GC, E_pool, E_aug_single)\n\n    # Define common parameters for the test suite\n    H, W = 9, 9\n    r_rot = 1  # Rotation by r * pi/2, with r=1\n    base_kernel = np.array([\n        [0, -1, 0],\n        [0,  2, 0],\n        [0, -1, 0]\n    ], dtype=np.float64)\n\n    # Define the test cases from the problem statement.\n    # Test case 1 (happy path): Vertical bar\n    x1 = np.zeros((H, W), dtype=np.float64)\n    x1[:, 4] = 1.0\n\n    # Test case 2 (rotationally symmetric edge case): Plus shape\n    x2 = np.zeros((H, W), dtype=np.float64)\n    x2[:, W // 2] = 1.0\n    x2[H // 2, :] = 1.0\n\n    # Test case 3 (nontrivial orientation edge case): Main diagonal bar\n    x3 = np.eye(H, dtype=np.float64)\n\n    test_cases = [x1, x2, x3]\n\n    results = []\n    for case_input in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        e_gc, e_pool, e_aug = calculate_metrics(case_input, base_kernel, r_rot)\n        results.extend([e_gc, e_pool, e_aug])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3161942"}]}