{"hands_on_practices": [{"introduction": "构建任何复杂的卷积神经网络（CNN）架构，第一步是掌握其基础算术——精确计算张量形状和模型参数。现代计算机视觉模型，如目标检测器，广泛使用特征金字塔网络（FPN）来融合多尺度信息。这个练习将带你深入FPN的内部，通过手动计算其横向连接和自顶向下通路中的特征图形状与参数数量，来锻炼你对网络构建中基本运算的掌控能力。[@problem_id:3103702]", "problem": "一个卷积神经网络 (CNN) 主干网络处理一个大小为 $640 \\times 640$ 的输入图像，并在连续的阶段生成三个特征图，分别表示为 $C_3$、$C_4$ 和 $C_5$。这些特征图相对于输入的空间步幅分别为 $8$、$16$ 和 $32$，其形状如下：\n- $C_3$: $80 \\times 80 \\times 128$,\n- $C_4$: $40 \\times 40 \\times 256$,\n- $C_5$: $20 \\times 20 \\times 512$.\n\n通过横向连接和自顶向下路径构建一个特征金字塔网络 (FPN)：\n- 每个 $C_i$ 首先通过一个步幅为 $1$、零填充的横向 $1 \\times 1$ 卷积，映射到 $F$ 个通道，其中 $F = 192$。假设每个卷积都包含偏置。\n- 定义 $M_5 = \\text{Conv}_{1 \\times 1}(C_5)$、$M_4 = \\text{Conv}_{1 \\times 1}(C_4) + \\text{Upsample}_{\\times 2}(M_5)$ 和 $M_3 = \\text{Conv}_{1 \\times 1}(C_3) + \\text{Upsample}_{\\times 2}(M_4)$，其中 $\\text{Upsample}_{\\times 2}$ 表示在高度和宽度上进行因子为 $2$ 的最近邻上采样。\n- 然后，每个 $M_i$ 通过一个步幅为 $1$、填充为 $1$ 的 $3 \\times 3$ 卷积，生成 $P_i = \\text{Conv}_{3 \\times 3}(M_i)$，输出通道数为 $F$。\n\n使用以下基本事实：\n- 对于一个卷积核大小为 $K_h \\times K_w$、输入通道数为 $C_{\\text{in}}$、输出通道数为 $C_{\\text{out}}$ 的卷积，其可训练参数数量（包括偏置）为 $K_h \\cdot K_w \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} + C_{\\text{out}}$。\n- 因子为 $2$ 的最近邻上采样将一个形状为 $H \\times W \\times C$ 的张量映射为 $2H \\times 2W \\times C$。\n- 张量的逐元素相加要求其在高度、宽度和通道数上具有相同的形状。\n- 当步幅为 $1$ 并使用适当的零填充时，$1 \\times 1$ 卷积会保留空间维度；填充为 $1$ 的 $3 \\times 3$ 卷积也会保留空间维度。\n\n任务：\n1. 推导 $M_5$、$M_4$ 和 $M_3$ 的形状，确保横向路径中的张量相加是形状对齐的。\n2. 推导 $P_5$、$P_4$ 和 $P_3$ 的形状。\n3. 计算此 FPN 路径中所有六个卷积（三个 $1 \\times 1$ 横向卷积和应用于 $M_5$、$M_4$ 和 $M_3$ 的三个 $3 \\times 3$ 卷积）所使用的可训练参数总数。将最终答案表示为一个精确的整数。无需四舍五入。", "solution": "该问题要求计算一个指定的特征金字塔网络 (FPN) 架构中的特征图形状和可訓練参数总数。解决方案将通过依次解决问题陈述中给出的三个任务来得出。所有形状均以“高度 $\\times$ 宽度 $\\times$ 通道数”的格式呈现。\n\n### 任务1：推导 $M_5$、$M_4$ 和 $M_3$ 的形状\n\nFPN 是由一组主干特征图 $C_3$、$C_4$ 和 $C_5$ 构建的，它们的形状如下：\n- $S(C_3) = 80 \\times 80 \\times 128$\n- $S(C_4) = 40 \\times 40 \\times 256$\n- $S(C_5) = 20 \\times 20 \\times 512$\n\nFPN 路径将每个 $C_i$ 映射到一个共同的通道维度 $F=192$。\n\n**$M_5$ 的形状**：\n$M_5$ 定义为 $M_5 = \\text{Conv}_{1 \\times 1}(C_5)$。\n输入是形状为 $20 \\times 20 \\times 512$ 的 $C_5$。该卷积是一个步幅为 $1$、零填充的 $1 \\times 1$ 层，它保留了空间维度。输出通道数为 $F=192$。\n因此，$M_5$ 的形状是 $20 \\times 20 \\times 192$。\n\n**$M_4$ 的形状**：\n$M_4$ 定义为 $M_4 = \\text{Conv}_{1 \\times 1}(C_4) + \\text{Upsample}_{\\times 2}(M_5)$。\n为了求出 $M_4$ 的形状，我们必须首先确定相加的两个张量的形状。\n1.  第一项是 $\\text{Conv}_{1 \\times 1}(C_4)$。输入是形状为 $40 \\times 40 \\times 256$ 的 $C_4$。$1 \\times 1$ 卷积保留空间维度，并将通道数映射到 $F=192$。这一项的形状是 $40 \\times 40 \\times 192$。\n2.  第二项是 $\\text{Upsample}_{\\times 2}(M_5)$。输入是形状为 $20 \\times 20 \\times 192$ 的 $M_5$。因子为 $2$ 的最近邻上采样将高度和宽度加倍。这一项的形状是 $(2 \\times 20) \\times (2 \\times 20) \\times 192$，即 $40 \\times 40 \\times 192$。\n\n由于两项的形状相同，均为 $40 \\times 40 \\times 192$，因此逐元素相加是有效的。得到的张量 $M_4$ 的形状是 $40 \\times 40 \\times 192$。\n\n**$M_3$ 的形状**：\n$M_3$ 定义为 $M_3 = \\text{Conv}_{1 \\times 1}(C_3) + \\text{Upsample}_{\\times 2}(M_4)$。遵循同样的逻辑：\n1.  第一项是 $\\text{Conv}_{1 \\times 1}(C_3)$。输入是形状为 $80 \\times 80 \\times 128$ 的 $C_3$。$1 \\times 1$ 卷积将其映射为形状 $80 \\times 80 \\times 192$。\n2.  第二项是 $\\text{Upsample}_{\\times 2}(M_4)$。输入是形状为 $40 \\times 40 \\times 192$ 的 $M_4$。因子为 $2$ 的上采样产生形状 $(2 \\times 40) \\times (2 \\times 40) \\times 192$，即 $80 \\times 80 \\times 192$。\n\n两项的形状相同，均为 $80 \\times 80 \\times 192$。相加有效，得到的张量 $M_3$ 的形状是 $80 \\times 80 \\times 192$。\n\n### 任务2：推导 $P_5$、$P_4$ 和 $P_3$ 的形状\n\n金字塔特征图 $P_i$ 是通过对每个 $M_i$ 应用一个 $3 \\times 3$ 卷积得到的，即 $P_i = \\text{Conv}_{3 \\times 3}(M_i)$。该卷积的步幅为 $1$，填充为 $1$，这会保留空间维度。输出通道数为 $F=192$。\n\n**$P_5$ 的形状**：\n输入是形状为 $20 \\times 20 \\times 192$ 的 $M_5$。$3 \\times 3$ 卷积保留了空间维度和通道数。因此，$P_5$ 的形状是 $20 \\times 20 \\times 192$。\n\n**$P_4$ 的形状**：\n输入是形状为 $40 \\times 40 \\times 192$ 的 $M_4$。$3 \\times 3$ 卷积保留了空间维度和通道数。因此，$P_4$ 的形状是 $40 \\times 40 \\times 192$。\n\n**$P_3$ 的形状**：\n输入是形状为 $80 \\times 80 \\times 192$ 的 $M_3$。$3 \\times 3$ 卷积保留了空间维度和通道数。因此，$P_3$ 的形状是 $80 \\times 80 \\times 192$。\n\n### 任务3：计算可训练参数的总数\n\n一个卷积层（包括偏置）的可训练参数数量由公式 $K_h \\cdot K_w \\cdot C_{\\text{in}} \\cdot C_{\\text{out}} + C_{\\text{out}}$ 给出，其中 $K_h$ 和 $K_w$ 是卷积核维度，$C_{\\text{in}}$ 是输入通道数，$C_{\\text{out}}$ 是输出通道数。\n\n我们计算 FPN 路径中六个卷积的参数。\n\n**三个横向 $1 \\times 1$ 卷积的参数：**\n卷积核大小为 $1 \\times 1$，$C_{\\text{out}} = F = 192$。\n1.  对 $C_5$ 的卷积：$C_{\\text{in}} = 512$。\n    参数 = $1 \\cdot 1 \\cdot 512 \\cdot 192 + 192 = 98304 + 192 = 98496$。\n2.  对 $C_4$ 的卷积：$C_{\\text{in}} = 256$。\n    参数 = $1 \\cdot 1 \\cdot 256 \\cdot 192 + 192 = 49152 + 192 = 49344$。\n3.  对 $C_3$ 的卷积：$C_{\\text{in}} = 128$。\n    参数 = $1 \\cdot 1 \\cdot 128 \\cdot 192 + 192 = 24576 + 192 = 24768$。\n\n横向卷积的总参数 = $98496 + 49344 + 24768 = 172608$。\n\n**三个 $3 \\times 3$ 卷积的参数：**\n这些卷积应用于 $M_5$、$M_4$ 和 $M_3$ 以生成 $P_5$、$P_4$ 和 $P_3$。\n卷积核大小为 $3 \\times 3$。对于所有三个卷积，输入和输出通道维度均为 $C_{\\text{in}} = F = 192$ 和 $C_{\\text{out}} = F = 192$。\n因此，所有三个卷积具有相同数量的参数。\n每个卷积的参数 = $3 \\cdot 3 \\cdot 192 \\cdot 192 + 192 = 9 \\cdot (192^2) + 192 = 9 \\cdot 36864 + 192 = 331776 + 192 = 331968$。\n\n三个 $3 \\times 3$ 卷积的总参数 = $3 \\times 331968 = 995904$。\n\n**FPN 路径中的可训练参数总数：**\n总数是两组卷积参数的总和。\n总参数 = (横向卷积的参数) + ($3 \\times 3$ 卷积的参数)\n总参数 = $172608 + 995904 = 1168512$。", "answer": "$$\\boxed{1168512}$$", "id": "3103702"}, {"introduction": "掌握了基本的形状计算后，我们来解决一个更微妙的几何挑战：在U-Net等编码器-解码器架构中实现像素级的精确对齐。由于下采样操作，解码器路径上采样后的特征图很容易与来自编码器的跳跃连接特征图产生“差一像素”的错位。这个练习要求你仔细甄别不同的上采样技术（如转置卷积和双线性插值），以确保几何信息的完美传递，这是构建高质量分割模型的关键一步。[@problem_id:3103688]", "problem": "一个用于语义分割的卷积神经网络 (CNN) 编码器-解码器正在被构建。输入图像的空间尺寸为 $129 \\times 129$。编码器使用两个连续的二维卷积，每个卷积核大小为 $3 \\times 3$，步长 $s=2$，并选择填充（padding）使得卷积是“相同（same）”的，即沿每个空间维度的输出尺寸为 $\\lceil N/s \\rceil$。假设膨胀（dilation）为单位值，并且对于奇数大小的卷积核，所有卷积都按照通常的离散方式中心对齐。因此，第一次下采样产生一个尺寸为 $65 \\times 65$ 的特征图，第二次下采样产生一个尺寸为 $33 \\times 33$ 的瓶颈特征图。从第一次下采样阶段（$65 \\times 65$）引出一条跳跃连接，与解码器相同空间尺寸的输出进行拼接。\n\n你的任务是选择一个解码器上采样块，将 $33 \\times 33$ 的瓶颈特征图上采样到恰好 $65 \\times 65$ 的尺寸，并与 $65 \\times 65$ 的跳跃特征图保持像素中心对齐，从而使得拼接操作不需要裁剪，也不会引入差一像素的空间偏移。这里的像素中心对齐意味着，当输出像素中心的坐标集被表示回原始输入的坐标系时，它们与编码器的 $65 \\times 65$ 跳跃特征图的像素中心坐标相吻合。\n\n请使用以下经过充分检验的事实和定义作为你的出发点：\n\n- 对于一个标准卷积，其输入长度为 $N$，卷积核大小为 $K$，步长为 $s$，填充为 $P$（两侧均为零填充），膨胀为 $d=1$，输出长度为 $N_{\\text{out}} = \\left\\lfloor \\frac{N + 2P - K}{s} \\right\\rfloor + 1$。对于步长 $s$，\"same\" 填充约定保证了 $N_{\\text{out}} = \\lceil N/s \\rceil$，并且对于奇数大小的 $K$，它将输出中心锚定在以 $s$ 为步长间隔的输入中心上，没有小数偏移。\n- 对于一个转置卷积（也称为分数步长卷积或反卷积），其输入长度为 $N$，卷积核大小为 $K$，步长为 $s$，填充为 $P$，输出填充为 $OP$，输出长度为 $N_{\\text{out}} = (N-1)s - 2P + K + OP$。\n- 通过因子 $r$ 进行最近邻上采样会复制像素值，使得输出像素中心的索引是输入网格的整数细化。\n- 当“align corners”设置为 $\\text{True}$ 的双线性上采样使用坐标映射 $u = q \\cdot \\frac{N_{\\text{in}} - 1}{N_{\\text{out}} - 1}$，将输出索引 $q$ 映射到连续的输入坐标 $u$，从而对齐输入和输出的第一个和最后一个中心；当“align corners”设置为 $\\text{False}$ 时，映射关系为 $u = \\left( q + \\frac{1}{2} \\right) \\cdot \\frac{N_{\\text{in}}}{N_{\\text{out}}} - \\frac{1}{2}$，这避免了角点对齐，并且通常会相对于输入网格引入半个像素的偏移。\n\n考虑以下候选的解码器模块（每个都应用于 $33 \\times 33$ 的瓶颈特征图），旨在生成一个可与 $65 \\times 65$ 跳跃特征图拼接的输出：\n\nA. 一个转置卷积，卷积核大小为 $4 \\times 4$，步长为 2，填充为 1，输出填充为 0。\n\nB. 一个转置卷积，卷积核大小为 $3 \\times 3$，步长为 2，填充为 1，输出填充为 1。\n\nC. 通过比例因子 2 进行最近邻上采样，然后是一个卷积核大小为 $3 \\times 3$、步长为 1 且使用“same”填充的卷积。\n\nD. 通过比例因子 2 进行双线性上采样，其中“align corners”被禁用，然后是一个卷积核大小为 $3 \\times 3$、步长为 1 且使用“same”填充的卷积。\n\nE. 双线性上采样到明确的目标尺寸 $65 \\times 65$，其中“align corners”被启用，然后是一个卷积核大小为 $3 \\times 3$、步长为 1 且使用“same”填充的卷积。\n\n哪个（些）选项能生成一个尺寸恰好为 $65 \\times 65$ 且像素中心与 $65 \\times 65$ 编码器跳跃特征图对齐的输出，从而使得拼接操作有效，无需裁剪，也没有差一像素的中心不匹配问题？选择所有适用的选项。", "solution": "问题要求我们确定所提出的解码器上采样模块中，哪一个能够将一个 $33 \\times 33$ 的特征图正确地-上采样到一个 $65 \\times 65$ 的特征图，同时确保与一个尺寸为 $65 \\times 65$ 的相应跳跃连接实现完美的像素中心对齐。\n\n### 第一步：提取已知条件\n- 输入图像尺寸：$N_0 = 129 \\times 129$。\n- 编码器，第一次卷积：输入尺寸 $N_0=129$，卷积核 $K=3$，步长 $s=2$。输出为“same”卷积，尺寸为 $N_1 = \\lceil N_0/s \\rceil = \\lceil 129/2 \\rceil = 65$。这是跳跃连接的特征图。\n- 编码器，第二次卷积：输入尺寸 $N_1=65$，卷积核 $K=3$，步长 $s=2$。输出为“same”卷积，尺寸为 $N_2 = \\lceil N_1/s \\rceil = \\lceil 65/2 \\rceil = 33$。这是瓶颈特征图。\n- 解码器任务：将 $N_2=33 \\times 33$ 的瓶颈特征图上采样，以匹配 $N_1=65 \\times 65$ 的跳跃特征图的尺寸和像素中心对齐。\n- 卷积输出尺寸公式：$N_{\\text{out}} = \\left\\lfloor \\frac{N + 2P - K}{s} \\right\\rfloor + 1$。\n- 转置卷积输出尺寸公式：$N_{\\text{out}} = (N-1)s - 2P + K + OP$。\n- 双线性上采样 (`align_corners=True`) 坐标映射：$u = q \\cdot \\frac{N_{\\text{in}} - 1}{N_{\\text{out}} - 1}$。\n- 双线性上采样 (`align_corners=False`) 坐标映射：$u = \\left( q + \\frac{1}{2} \\right) \\cdot \\frac{N_{\\text{in}}}{N_{\\text{out}}} - \\frac{1}{2}$。\n\n### 第二步：验证问题陈述\n问题陈述定义明确，科学上基于深度学习的原理，并且内部一致。根据所提供的“same”卷积规则，下采样计算是正确的：从尺寸为 $129$ 的输入，输出为 $\\lceil 129/2 \\rceil = 65$；从尺寸为 $65$ 的输入，输出为 $\\lceil 65/2 \\rceil = 33$。所提供的卷积、转置卷积和插值公式是标准的，或在问题背景下有明确定义。核心任务是逆转带步长卷积的几何变换，这是CNN设计中一个实际且适定的问题。问题是有效的。\n\n### 第三步：推导与选项分析\n\n首先，我们建立坐标系。设 $65 \\times 65$ 跳跃特征图的像素中心位于一维整数坐标 $\\{0, 1, 2, \\dots, 64\\}$ 上。问题指出，带步长的卷积（使用奇数大小的卷积核，并且按照“same”填充的标准，采用对称填充）从此网格中采样。步长 $s=2$ 意味着所得到的 $33 \\times 33$ 瓶颈特征图的中心，在 $65 \\times 65$ 特征图的坐标系中，对应于坐标 $\\{0 \\cdot 2, 1 \\cdot 2, \\dots, 32 \\cdot 2\\} = \\{0, 2, 4, \\dots, 64\\}$。\n\n上采样模块的目标是，接收 $33 \\times 33$ 的特征图（其中心位于物理位置 $\\{0, 2, \\dots, 64\\}$），并生成一个 $65 \\times 65$ 的特征图，其中心位于物理位置 $\\{0, 1, 2, \\dots, 64\\}$。这确保了拼接操作的完美对齐。\n\n我们现在将根据两个标准评估每个选项：\n1.  **输出尺寸**：该模块必须生成一个尺寸为 $65 \\times 65$ 的特征图。\n2.  **像素中心对齐**：输出像素的中心必须与整数网格 $\\{0, 1, \\dots, 64\\}$ 对齐。\n\n所有候选模块的输入尺寸均为 $N_{\\text{in}} = 33$。\n\n**A. 一个转置卷积，卷积核大小为 $4 \\times 4$，步长为 2，填充为 1，输出填充为 0。**\n我们使用提供的转置卷积输出长度公式：$N_{\\text{out}} = (N_{\\text{in}}-1)s - 2P + K + OP$。\n这里，$N_{\\text{in}}=33$，$s=2$，$K=4$，$P=1$，以及 $OP=0$。\n$$N_{\\text{out}} = (33-1) \\cdot 2 - 2 \\cdot 1 + 4 + 0 = 32 \\cdot 2 - 2 + 4 = 64 - 2 + 4 = 66$$\n输出尺寸为 $66 \\times 66$，而不是 $65 \\times 65$。该选项不满足尺寸要求。\n**结论：不正确。**\n\n**B. 一个转置卷积，卷积核大小为 $3 \\times 3$，步长为 2，填充为 1，输出填充为 1。**\n我们使用相同的公式。这里，$N_{\\text{in}}=33$，$s=2$，$K=3$，$P=1$，以及 $OP=1$。\n$$N_{\\text{out}} = (33-1) \\cdot 2 - 2 \\cdot 1 + 3 + 1 = 32 \\cdot 2 - 2 + 3 + 1 = 64 - 2 + 4 = 66$$\n输出尺寸为 $66 \\times 66$，而不是 $65 \\times 65$。该选项同样不满足尺寸要求。\n**结论：不正确。**\n\n**C. 通过比例因子 2 进行最近邻上采样，然后是一个卷积核大小为 $3 \\times 3$、步长为 1 且使用“same”填充的卷积。**\n第一步是最近邻上采样。将尺寸为 $N$ 的输入按比例因子 $r$ 进行上采样，通常会产生尺寸为 $N \\cdot r$ 的输出。\n对于 $N_{\\text{in}} = 33$ 和 $r=2$，输出尺寸为 $33 \\cdot 2 = 66$。\n第二步是步长为 $s=1$ 且使用“same”填充的卷积，这种卷积旨在保持其输入空间维度。因此，最终输出尺寸为 $66 \\times 66$。这不满足尺寸要求。\n**结论：不正确。**\n\n**D. 通过比例因子 2 进行双线性上采样，其中“align corners”被禁用，然后是一个卷积核大小为 $3 \\times 3$、步长为 1 且使用“same”填充的卷积。**\n与选项 C 类似，对尺寸为 $N_{\\text{in}}=33$ 的输入按比例因子 $r=2$ 进行上采样，会得到一个尺寸为 $33 \\cdot 2 = 66$ 的中间特征图。随后的步长为 $s=1$ 的“same”卷积会保持此尺寸。最终输出为 $66 \\times 66$，这不满足尺寸要求。此外，`align_corners` 设置为 `False` 的双线性上采样会在其坐标映射中引入半个像素的偏移，即使尺寸正确，这也会破坏像素中心的对齐。\n**结论：不正确。**\n\n**E. 双线性上采样到明确的目标尺寸 $65 \\times 65$，其中“align corners”被启用，然后是一个卷积核大小为 $3 \\times 3$、步长为 1 且使用“same”填充的卷积。**\n第一步将 $33 \\times 33$ 的输入明确地调整大小为 $65 \\times 65$ 的特征图。随后的步长为 $s=1$ 且使用“same”填充的卷积保持了 $65 \\times 65$ 的维度。因此，该选项满足尺寸要求。\n\n现在，我们必须检查像素中心对齐情况。`align_corners=True` 模式使用坐标映射 $u = q \\cdot \\frac{N_{\\text{in}} - 1}{N_{\\text{out}} - 1}$，其中 $q$ 是输出像素索引，$u$ 是输入索引空间中对应的连续坐标。\n这里，$N_{\\text{in}}=33$ 且 $N_{\\text{out}}=65$。设 $q \\in \\{0, 1, \\dots, 64\\}$ 是输出网格中的一个索引。\n$$u(q) = q \\cdot \\frac{33 - 1}{65 - 1} = q \\cdot \\frac{32}{64} = \\frac{q}{2}$$\n这意味着为了计算输出索引 $q$ 处的值，我们在索引坐标 $u=q/2$ 处对输入特征图进行采样。输入特征图的索引是 $\\{0, 1, \\dots, 32\\}$，我们已经确定输入索引 $k$ 的物理坐标是 $2k$。因此，与连续输入索引 $u$ 对应的物理坐标是 $2u$。\n因此，输出像素 $q$ 中心的物理坐标为：\n$$ \\text{物理坐标}(q) = 2 \\cdot u(q) = 2 \\cdot \\left(\\frac{q}{2}\\right) = q $$\n当输出索引 $q$ 的范围从 $0$ 到 $64$ 时，输出像素中心的物理坐标为 $\\{0, 1, 2, \\dots, 64\\}$。这与 $65 \\times 65$ 跳跃连接特征图的网格完全匹配。随后的步长为1的“same”卷积不会改变这种空间对齐。\n**结论：正确。**", "answer": "$$\\boxed{E}$$", "id": "3103688"}, {"introduction": "在掌握了架构的算术和几何原理之后，让我们转向一个更具概念性的设计问题：网络应如何处理图像的边界？标准的零填充虽然简单，但会在图像边缘引入可能误导网络的人工信号。本练习将引导你探索一种创新的替代方案，即将边界信息作为显式输入通道提供给网络。通过比较这两种设计哲学，你将学会批判性地思考CNN设计中的默认选择，并探索更智能地向模型提供上下文信息的方法。[@problem_id:3103755]", "problem": "要求您实现并比较两种卷积神经网络（CNN）架构，用于图像边界附近的逐像素边界预测。比较的重点在于如何处理边界上下文：一种是使用标准零填充的基线模型，另一种是将边界上下文编码为显式输入通道（通过反射瓦片将填充作为输入），这样网络就不依赖于填充语义来推断边界信息。\n\n该问题的基本基础包括离散卷积、填充策略和逐点非线性激活的定义。令 $X \\in \\mathbb{R}^{H \\times W}$ 表示一个单通道图像。$X$ 与一个核 $K \\in \\mathbb{R}^{k \\times k}$ 之间，步幅 $s = 1$ 且使用零填充的离散二维卷积定义为\n$$\n(Y)_{i,j} \\;=\\; \\sum_{u=-(k-1)/2}^{(k-1)/2} \\sum_{v=-(k-1)/2}^{(k-1)/2} K_{u+(k+1)/2,\\;v+(k+1)/2} \\cdot \\tilde{X}_{i+u,\\;j+v},\n$$\n其中 $\\tilde{X}$ 表示在图像边界之外用零扩展后的 $X$，$H$ 是以像素为单位的高度，$W$ 是以像素为单位的宽度。对于多通道输入，卷积对输入通道求和；而对于多个滤波器，独立的核会生成一组堆叠的输出特征图。逐点非线性激活函数 $f$ 的作用方式为 $(f(Y))_{i,j} = f\\left( (Y)_{i,j} \\right)$。\n\n零填充在边界处引入了人为的上下文（图像外部为 $0$），这可能产生虚假梯度。另一种方法是避免填充，并将边界上下文显式编码为输入通道。对于给定的 $X$，定义四个差异通道，这些通道捕捉了在反射映射下，$3 \\times 3$ 邻域在边界紧邻外部所看到的内容，但直接编码在输入中：\n- 顶部差异通道 $D^{\\text{top}} \\in \\mathbb{R}^{H \\times W}$:\n$$\nD^{\\text{top}}_{i,j} \\;=\\;\n\\begin{cases}\nX_{0,j} - X_{1,j},  \\text{if } i = 0, \\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n- 底部差异通道 $D^{\\text{bot}} \\in \\mathbb{R}^{H \\times W}$:\n$$\nD^{\\text{bot}}_{i,j} \\;=\\;\n\\begin{cases}\nX_{H-1,j} - X_{H-2,j},  \\text{if } i = H-1, \\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n- 左侧差异通道 $D^{\\text{left}} \\in \\mathbb{R}^{H \\times W}$:\n$$\nD^{\\text{left}}_{i,j} \\;=\\;\n\\begin{cases}\nX_{i,0} - X_{i,1},  \\text{if } j = 0, \\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n- 右侧差异通道 $D^{\\text{right}} \\in \\mathbb{R}^{H \\times W}$:\n$$\nD^{\\text{right}}_{i,j} \\;=\\;\n\\begin{cases}\nX_{i,W-1} - X_{i,W-2},  \\text{if } j = W-1, \\\\\n0,  \\text{otherwise.}\n\\end{cases}\n$$\n这些通道仅在边界环上编码反射邻居的差异，在其他位置均为 $0$。令增强输入为 $Z \\in \\mathbb{R}^{H \\times W \\times C}$，其中 $C = 5$，通过拼接 $X$ 和四个差异通道来定义：\n$$\nZ_{i,j,:} \\;=\\; \\left[X_{i,j}, \\; D^{\\text{top}}_{i,j}, \\; D^{\\text{bot}}_{i,j}, \\; D^{\\text{left}}_{i,j}, \\; D^{\\text{right}}_{i,j}\\right].\n$$\n\n需要实现的架构：\n- 使用零填充的基线CNN：\n  1. 第一个卷积层：使用单个 $3 \\times 3$ 的核来近似离散拉普拉斯算子，例如\n  $$\n  K^{\\Delta} \\;=\\;\n  \\begin{bmatrix}\n  0  1  0 \\\\\n  1  -4  1 \\\\\n  0  1  0\n  \\end{bmatrix},\n  $$\n  应用于带零填充的 $X$，以生成 $Y^{(1)} \\in \\mathbb{R}^{H \\times W}$。\n  2. 逐点激活函数 $f(x) = x^2$，生成 $A^{(1)}_{i,j} = \\left(Y^{(1)}_{i,j}\\right)^2$。\n  3. 第二个 $1 \\times 1$ 卷积（每个像素的线性映射），使用标量权重 $\\alpha \\in \\mathbb{R}$ 和偏置 $\\beta \\in \\mathbb{R}$：\n  $$\n  L^{\\text{base}}_{i,j} \\;=\\; \\alpha \\cdot A^{(1)}_{i,j} - \\beta.\n  $$\n  4. Sigmoid函数 $\\sigma(x) = \\frac{1}{1 + e^{-x}}$，生成边界概率 $P^{\\text{base}}_{i,j} = \\sigma\\!\\left(L^{\\text{base}}_{i,j}\\right)$。\n\n- 将填充作为输入的CNN（无空间填充；边界上下文编码在输入通道中）：\n  1. 第一个 $1 \\times 1$ 卷积，将差异通道通过四个独立的滤波器，生成四个特征图 $Y^{(1)}_{i,j,c}$ ($c \\in \\{1,2,3,4\\}$)，每个滤波器选择一个差异通道并忽略 $X$。具体来说，如果权重张量为 $W^{(1)} \\in \\mathbb{R}^{1 \\times 1 \\times 5 \\times 4}$，则设置\n  $$\n  W^{(1)}_{:,:,2,1} = 1, \\quad W^{(1)}_{:,:,3,2} = 1, \\quad W^{(1)}_{:,:,4,3} = 1, \\quad W^{(1)}_{:,:,5,4} = 1,\n  $$\n  且所有其他条目为 $0$。这将产生 $Y^{(1)} \\in \\mathbb{R}^{H \\times W \\times 4}$，其中包含四个边界差异。\n  2. 对每个通道应用逐点激活函数 $f(x) = x^2$：$A^{(1)}_{i,j,c} = \\left(Y^{(1)}_{i,j,c}\\right)^2$。\n  3. 第二个 $1 \\times 1$ 卷积，用相等的权重 $\\gamma \\in \\mathbb{R}$ 将四个激活后的通道相加，得到每个像素的单个logit值：\n  $$\n  L^{\\text{aug}}_{i,j} \\;=\\; \\gamma \\cdot \\sum_{c=1}^{4} A^{(1)}_{i,j,c} \\;-\\; \\beta.\n  $$\n  4. 使用Sigmoid函数生成概率 $P^{\\text{aug}}_{i,j} = \\sigma\\!\\left(L^{\\text{aug}}_{i,j}\\right)$。\n\n评估仅针对边界环像素。定义边界环掩码 $B \\subset \\{(i,j)\\}$，其中 $B = \\{(i,j) \\mid i \\in \\{0, H-1\\} \\text{ or } j \\in \\{0, W-1\\}\\}$。真实标签 $G_{i,j} \\in \\{0,1\\}$ 是根据图像内部邻居的差异在边界像素上定义的：对于 $(i,j) \\in B$，令 $\\mathcal{N}_{\\text{in}}(i,j)$ 为图像内部有效的四邻域位置集合，\n$$ \\mathcal{N}_{\\text{in}}(i,j) = \\{(i-1,j) \\mid i > 0\\} \\cup \\{(i+1,j) \\mid i  H-1\\} \\cup \\{(i,j-1) \\mid j > 0\\} \\cup \\{(i,j+1) \\mid j  W-1\\} $$\n如果 $\\max_{(i',j') \\in \\mathcal{N}_{\\text{in}}(i,j)} |X_{i,j} - X_{i',j'}|  \\kappa$，则 $G_{i,j} = 1$；否则 $G_{i,j}=0$。预测标签 $\\hat{G}$ 是通过在阈值 $\\tau$ 处对输出概率图 $P$ 进行阈值处理得到的。$F_1$ 分数计算为 $F_1 = 2 \\cdot \\mathrm{TP} / (2 \\cdot \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN})$，其中 $\\mathrm{TP}$、$\\mathrm{FP}$ 和 $\\mathrm{FN}$ 是在边界环上计算的真阳性、假阳性和假阴性。如果 $\\mathrm{TP}=\\mathrm{FP}=\\mathrm{FN}=0$，则 $F_1=1.0$。\n\n您的任务是计算 $\\Delta = F_1^{\\text{aug}} - F_1^{\\text{base}}$，其中 $\\alpha = 4.0$、$\\beta = 0.9$、$\\gamma = 1.0$、$\\kappa = 0.4$ 和 $\\tau = 0.5$。为以下每个测试用例计算此差异：\n1. $H=8, W=8$, $X_{i,j} = 0.5$ for all $i,j$.\n2. $H=8, W=8$, $X_{i,j} = 1.0$ if $2 \\le i  6$ and $2 \\le j  6$, and $0.0$ otherwise.\n3. $H=8, W=8$, $X_{i,j} = 1.0$ if $i=0$, and $0.0$ otherwise.\n4. $H=5, W=5$, $X_{i,j} = 0.05 \\cdot j$ for all $i,j$.\n\n将结果表示为一个包含四个浮点数的列表，对应于每个测试用例的 $\\Delta$ 值。", "solution": "用户提供了一个问题，要求实现并比较两种用于图像边界预测的不同卷积神经网络（CNN）架构。问题的核心在于处理图像边界的方式：一个模型采用标准的零填充，而另一个模型则将边界上下文显式编码到额外的输入通道中。\n\n### 步骤1：问题验证\n\n首先，有必要对问题陈述进行细致的验证，以确保其科学合理性、一致性和完整性。\n\n**提取的已知条件：**\n- **输入数据：** 单通道图像 $X \\in \\mathbb{R}^{H \\times W}$。\n- **架构：**\n    1.  **基线CNN：** 一个双层网络，使用一个带零填充的 $3 \\times 3$ 拉普拉斯核，后跟一个 $1 \\times 1$ 卷积。\n        - 第一层：使用核 $K^{\\Delta} = [[0, 1, 0], [1, -4, 1], [0, 1, 0]]$ 进行卷积。\n        - 激活函数1：$f(x) = x^2$。\n        - 第二层：逐像素线性映射 $L^{\\text{base}}_{i,j} = \\alpha \\cdot A^{(1)}_{i,j} - \\beta$。\n        - 输出：Sigmoid激活 $\\sigma(L^{\\text{base}})$。\n    2.  **将填充作为输入的CNN：** 一个在增强输入上使用 $1 \\times 1$ 卷积的双层网络，无空间填充。\n        - 输入增强：将 $X$ 与四个从图像边界梯度派生出的差异通道（$D^{\\text{top}}, D^{\\text{bot}}, D^{\\text{left}}, D^{\\text{right}}$）进行拼接。\n        - 第一层：设计用于选择四个差异通道的 $1 \\times 1$ 卷积。\n        - 激活函数1：$f(x) = x^2$。\n        - 第二层：用于对激活后的通道求和的 $1 \\times 1$ 卷积：$L^{\\text{aug}}_{i,j} = \\gamma \\cdot \\sum_{c=1}^{4} A^{(1)}_{i,j,c} - \\beta$。\n        - 输出：Sigmoid激活 $\\sigma(L^{\\text{aug}})$。\n- **超参数：** $\\alpha = 4.0$, $\\beta = 0.9$, $\\gamma = 1.0$, $\\kappa = 0.4$, $\\tau = 0.5$。\n- **评估：**\n    - **目标：** 边界环 $B = \\{(i,j) \\mid i \\in \\{0, H-1\\} \\text{ or } j \\in \\{0, W-1\\}\\}$ 上的像素。\n    - **真实标签 $G$：** 对于 $(i,j) \\in B$，如果 $X_{i,j}$ 与其内部邻居之间的最大绝对差值超过阈值 $\\kappa$，则 $G_{i,j} = 1$；否则 $G_{i,j}=0$。\n    - **预测标签 $\\hat{G}$：** 在 $\\tau$ 处对输出概率图 $P$ 进行阈值处理。\n    - **指标：** $F_1$ 分数，附带特殊条件：如果真阳性、假阳性和假阴性均为零，则 $F_1 = 1.0$。\n    - **最终结果：** $F_1$ 分数的差异，$\\Delta = F_1^{\\text{aug}} - F_1^{\\text{base}}$。\n- **测试用例：** 四个由其维度（$H, W$）和像素值函数定义的特定图像。\n\n**验证结论：**\n该问题是**有效的**。\n1.  **科学依据：** 它基于数字图像处理（卷积、拉普拉斯算子）和深度学习（CNN、激活函数、填充策略）的既定原则。所提出的架构是为明确定义的任务设计的简化但合理的模型。\n2.  **适定性：** 所有必要组件——架构、操作、超参数、输入数据和评估指标——都得到了明确无误的定义。该问题是确定性的，并允许为每个测试用例提供唯一的、可计算的解决方案。\n3.  **客观性：** 语言是形式化和数学化的，没有主观或含糊的陈述。\n4.  **完整性和一致性：** 问题是自洽的。所提供的定义相互一致。例如，为增强模型的第一个层指定的权重正确地实现了所描述的选择差异通道的功能。$F_1$ 分数的特殊情况以标准方式处理了潜在的 $0/0$ 不确定性。\n5.  **可行性：** 所需的计算是直接的，并且可以使用标准的数值库实现。图像尺寸很小，确保了手动验证和计算执行不会很繁重。\n\n该问题通过了所有验证检查。因此，将开发一个解决方案。\n\n### 步骤2：原则性解决方案设计\n\n该解决方案需要实现两种指定的神经网络架构及相应的评估流程。\n\n**核心组件：**\n1.  **卷积操作：** 问题定义了一个离散二维卷积。更准确地说，此操作是一个二维互相关，因为核没有翻转。对于对称的拉普拉斯核，卷积和互相关是相同的。`scipy.signal.correlate2d` 函数使用参数 `mode='same'` 和 `boundary='fill', fillvalue=0` 可以正确实现指定的带零填充的操作。\n\n2.  **差异通道生成：** 四个差异通道（$D^{\\text{top}}, D^{\\text{bot}}, D^{\\text{left}}, D^{\\text{right}}$）根据其定义进行计算。对于大小为 $H \\times W$ 的图像 $X$，初始化四个相同大小的零数组。然后，更新指定的边界行或列。例如，对于 $D^{\\text{top}}$，第一行计算为 $D^{\\text{top}}_{0,j} = X_{0,j} - X_{1,j}$（对所有 $j$）。这需要仔细的索引操作。\n\n3.  **模型实现：**\n    - **基线模型：** 这是一个顺序应用互相关、逐点平方（`numpy.square`）、标量乘法和减法，以及sigmoid激活函数的过程。\n    - **增强模型：** 这首先涉及创建差异通道。第一个“卷积”层是通道选择，可以通过简单地从差异图中创建一个多通道数组来实现。后续步骤包括逐点平方、跨通道求和（`numpy.sum`）、标量乘法和减法，以及sigmoid激活。\n\n4.  **真实标签与评估：**\n    - 创建一个用于边界环的布尔掩码。\n    - 通过遍历边界像素，找到其有效的内部邻居，计算值的最大绝对差，并用 $\\kappa$ 对其进行阈值处理，来生成真实标签 $G$。\n    - $F_1$ 分数是通过首先计算边界像素上的真阳性（TP）、假阳性（FP）和假阴性（FN）来计算的。应用公式 $F_1 = 2 \\cdot \\mathrm{TP} / (2 \\cdot \\mathrm{TP} + \\mathrm{FP} + \\mathrm{FN})$。$\\mathrm{TP}=\\mathrm{FP}=\\mathrm{FN}=0$ 的特殊情况通过返回 $F_1=1.0$ 来处理，这表示在全为负例的情况下分类完美。\n\n最终程序将通过此流程系统地处理每个测试用例，计算两个模型的 $F_1$ 分数，计算它们的差异 $\\Delta$，并按规定格式化结果集合。", "answer": "```python\nimport numpy as np\nfrom scipy import signal\n\ndef solve():\n    \"\"\"\n    Implements and compares two CNN architectures for boundary prediction,\n    calculating the performance improvement of an augmented model over a baseline.\n    \"\"\"\n    \n    # Fixed hyperparameters from the problem statement\n    alpha = 4.0\n    beta = 0.9\n    gamma = 1.0\n    kappa = 0.4\n    tau = 0.5\n\n    # Discrete Laplacian kernel\n    K_laplacian = np.array([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=float)\n\n    def sigmoid(x):\n        \"\"\"Pointwise sigmoid activation function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def generate_border_mask(H, W):\n        \"\"\"Creates a boolean mask for pixels on the border ring.\"\"\"\n        mask = np.zeros((H, W), dtype=bool)\n        if H  0:\n            mask[0, :] = True\n            mask[H - 1, :] = True\n        if W  0:\n            mask[:, 0] = True\n            mask[:, W - 1] = True\n        return mask\n\n    def generate_ground_truth(X, kappa, border_mask):\n        \"\"\"Generates ground-truth labels G on the border ring.\"\"\"\n        H, W = X.shape\n        G = np.zeros_like(X, dtype=int)\n        border_pixels = np.argwhere(border_mask)\n        \n        for i, j in border_pixels:\n            val = X[i, j]\n            max_diff = 0.0\n            # Inner-neighborhood N_in(i,j)\n            if i  0:\n                max_diff = max(max_diff, abs(val - X[i - 1, j]))\n            if i  H - 1:\n                max_diff = max(max_diff, abs(val - X[i + 1, j]))\n            if j  0:\n                max_diff = max(max_diff, abs(val - X[i, j - 1]))\n            if j  W - 1:\n                max_diff = max(max_diff, abs(val - X[i, j + 1]))\n            \n            if max_diff  kappa:\n                G[i, j] = 1\n        return G\n\n    def calculate_f1(G, P, tau, border_mask):\n        \"\"\"Computes the F1 score on the border ring.\"\"\"\n        G_hat = (P = tau).astype(int)\n        \n        G_border = G[border_mask]\n        G_hat_border = G_hat[border_mask]\n        \n        tp = np.sum((G_border == 1)  (G_hat_border == 1))\n        fp = np.sum((G_border == 0)  (G_hat_border == 1))\n        fn = np.sum((G_border == 1)  (G_hat_border == 0))\n        \n        # Special case: TP=FP=FN=0 means perfect prediction on a negative-only set.\n        if tp == 0 and fp == 0 and fn == 0:\n            return 1.0\n            \n        denominator = 2 * tp + fp + fn\n        if denominator == 0:\n            return 0.0 # Denominator is 0 only if TP=FP=FN=0, handled above. Safety return.\n            \n        return (2 * tp) / denominator\n\n    def baseline_model(X, K_laplacian, alpha, beta):\n        \"\"\"Implements the baseline CNN with zero padding.\"\"\"\n        # Layer 1: The problem formula is cross-correlation. For a symmetric kernel,\n        # it is identical to convolution.\n        Y1 = signal.correlate2d(X, K_laplacian, mode='same', boundary='fill', fillvalue=0)\n        \n        # Activation 1: f(x) = x^2\n        A1 = np.square(Y1)\n        \n        # Layer 2: 1x1 convolution\n        L_base = alpha * A1 - beta\n        \n        # Output: Sigmoid\n        return sigmoid(L_base)\n\n    def generate_diff_channels(X):\n        \"\"\"Generates the four explicit border context channels.\"\"\"\n        H, W = X.shape\n        D_top = np.zeros((H, W))\n        D_bot = np.zeros((H, W))\n        D_left = np.zeros((H, W))\n        D_right = np.zeros((H, W))\n\n        if H  1:\n            D_top[0, :] = X[0, :] - X[1, :]\n            D_bot[H - 1, :] = X[H - 1, :] - X[H - 2, :]\n        if W  1:\n            D_left[:, 0] = X[:, 0] - X[:, 1]\n            D_right[:, W - 1] = X[:, W - 1] - X[:, W - 2]\n            \n        return D_top, D_bot, D_left, D_right\n\n    def augmented_model(X, gamma, beta):\n        \"\"\"Implements the padding-as-input CNN.\"\"\"\n        D_top, D_bot, D_left, D_right = generate_diff_channels(X)\n        \n        # Layer 1: 1x1 conv selects the diff channels.\n        Y1 = np.stack([D_top, D_bot, D_left, D_right], axis=-1)\n        \n        # Activation 1: f(x) = x^2\n        A1 = np.square(Y1)\n        \n        # Layer 2: 1x1 conv sums the channels\n        L_aug = gamma * np.sum(A1, axis=2) - beta\n        \n        # Output: Sigmoid\n        return sigmoid(L_aug)\n\n    # Define test cases\n    H1, W1 = 8, 8\n    X1 = np.full((H1, W1), 0.5)\n\n    H2, W2 = 8, 8\n    X2 = np.zeros((H2, W2))\n    X2[2:6, 2:6] = 1.0\n\n    H3, W3 = 8, 8\n    X3 = np.zeros((H3, W3))\n    X3[0, :] = 1.0\n\n    H4, W4 = 5, 5\n    X4 = np.zeros((H4, W4))\n    X4[:, :] = 0.05 * np.arange(W4)\n\n    test_cases = [X1, X2, X3, X4]\n    results = []\n\n    for X in test_cases:\n        H, W = X.shape\n        \n        border_mask = generate_border_mask(H, W)\n        G = generate_ground_truth(X, kappa, border_mask)\n        \n        # Evaluate baseline model\n        P_base = baseline_model(X, K_laplacian, alpha, beta)\n        f1_base = calculate_f1(G, P_base, tau, border_mask)\n        \n        # Evaluate augmented model\n        P_aug = augmented_model(X, gamma, beta)\n        f1_aug = calculate_f1(G, P_aug, tau, border_mask)\n        \n        # Calculate performance difference and store it\n        delta = f1_aug - f1_base\n        results.append(delta)\n        \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3103755"}]}