{"hands_on_practices": [{"introduction": "Cutout 是一种简单而强大的正则化方法，它通过从图像中移除一个随机区块来工作。这迫使模型超越最明显的特征进行学习。但是，我们能否比随机选择更智能呢？本练习 [@problem_id:3111355] 探讨了显著性感知 Cutout 的概念，即我们策略性地遮盖对模型最重要的区域，从而提供更强的训练信号并增强正则化效果。你将量化这种智能放置与随机遮盖相比所带来的影响。", "problem": "要求您编写一个完整、可运行的程序，来量化对于一个作用于图像的简单线性模型，CutOut 蒙版相对于显著区域的放置方式如何影响训练损失的增加，我们使用训练损失的增加作为正则化强度的代理指标。该问题以纯数学术语进行描述，适合用任何现代编程语言实现。\n\n考虑一个带有线性预测器的单图像二元回归模型。设图像表示为一个实数二维数组 $x \\in \\mathbb{R}^{H \\times W}$，模型权重为 $w \\in \\mathbb{R}^{H \\times W}$。定义预测为 $y = \\langle w, x \\rangle = \\sum_{i=1}^{H}\\sum_{j=1}^{W} w_{ij}\\,x_{ij}$，均方误差 (MSE) 损失为 $L(x; w, t) = (y - t)^2$，其中 $t \\in \\mathbb{R}$ 是目标值。一个大小为 $h \\times w$ 的 CutOut 蒙版会将输入的一个连续矩形区域置零。设 $m \\in \\{0,1\\}^{H \\times W}$ 是一个二元蒙版，其中一个 $h \\times w$ 的矩形区域为零，所有其他条目为一，并将经过蒙版处理的输入表示为 $x' = m \\odot x$，其中 $\\odot$ 是 Hadamard（逐元素）乘积。经过蒙版处理的损失为 $L(x'; w, t) = \\big(\\langle w, x' \\rangle - t\\big)^2$。\n\n将显著图定义为损失函数对输入求梯度的绝对值，在未经蒙版处理的输入上进行评估，即 $s = \\left| \\nabla_x L(x; w, t) \\right|$。对于带有线性模型的 MSE 损失，这可以简化为 $s_{ij} = 2\\,\\big| \\langle w, x \\rangle - t \\big|\\,|w_{ij}|$。因此，选择一个大小为 $h \\times w$ 的显著性感知 CutOut 放置，以最大化蒙版区域内 $s_{ij}$ 的总和，这等同于最大化该矩形区域内 $|w_{ij}|$ 的总和，因为因子 $2\\,\\big| \\langle w, x \\rangle - t \\big|$ 对于所有 $(i,j)$ 都是常数。\n\n我们用蒙版处理导致的损失增加来衡量正则化强度。具体来说，定义未经蒙版处理的损失为 $L_0 = L(x; w, t)$，在均匀随机放置 CutOut $h \\times w$ 窗口（在所有有效的左上角位置上）下的期望蒙版损失为 $\\mathbb{E}_{\\text{rand}}[L]$，以及在显著性感知放置下的蒙版损失为 $L_{\\text{sal}}$。定义随机蒙版带来的增量为 $\\Delta_{\\text{rand}} = \\mathbb{E}_{\\text{rand}}[L] - L_0$，显著性感知蒙版带来的增量为 $\\Delta_{\\text{sal}} = L_{\\text{sal}} - L_0$。同时定义增益 $G = \\Delta_{\\text{sal}} - \\Delta_{\\text{rand}}$ 和比率 $R = \\Delta_{\\text{sal}} / \\Delta_{\\text{rand}}$（如果分母为零，则将 $R$ 视为正无穷）。\n\n您必须精确计算这些量（不使用 Monte Carlo 近似），通过枚举随机情况下的所有有效左上角位置，并选择上述定义的显著性感知位置。为确保普遍的可复现性，输入 $x$ 和 $w$ 必须由一个固定的线性同余生成器 (LCG) 生成，其模数为 $m = 2^{32}$，乘数为 $a = 1664525$，增量为 $c = 1013904223$，种子为 $s \\in \\{0,1,\\dots,2^{32}-1\\}$。生成器的演化方式为 $u_{k+1} = (a\\,u_k + c) \\bmod m$，每个伪随机实数值 $r_k = u_k / m \\in [0,1)$。要从种子 $s$ 构建一个 $H \\times W$ 数组，需要按行主序生成 $H \\cdot W$ 个值 $r_k$，并将每个值映射到 $v_k = 2\\,r_k - 1 \\in [-1,1)$。\n\n对于每个测试用例，给定 $H$、$W$、$h$、$w$、用于 $x$ 的种子 $s_x$、用于 $w$ 的种子 $s_w$ 以及目标值 $t$。按照上述方法构建 $x$ 和 $w$。精确计算 $L_0$、$\\Delta_{\\text{rand}}$、$\\Delta_{\\text{sal}}$、$G$ 和 $R$。CutOut 的放置位置是所有左上角索引 $(i,j)$ 的集合，其中 $i \\in \\{0,1,\\dots,H-h\\}$ 且 $j \\in \\{0,1,\\dots,W-w\\}$。\n\n使用的重要数学细节：\n- 经验风险最小化 (ERM) 和 MSE 的定义：$L(x; w, t) = (\\langle w, x \\rangle - t)^2$。\n- 通过梯度幅值定义显著性：$s = \\left| \\nabla_x L \\right|$。\n- 线性模型预测：$\\langle w, x \\rangle = \\sum_{i,j} w_{ij} x_{ij}$。\n- 在将区域 $R$ 置零的矩形下的蒙版预测：$\\langle w, x' \\rangle = \\langle w, x \\rangle - \\sum_{(i,j)\\in R} w_{ij} x_{ij}$。\n- 显著性感知选择：选择使 $\\sum_{(i,j)\\in R} |w_{ij}|$ 最大化的矩形。\n\n不涉及角度。没有物理单位。您必须将最终结果表示为精确到 $6$ 位小数的实数。\n\n测试套件：\n计算以下五个测试用例的输出：\n1. $H = 8$, $W = 8$, $h = 3$, $w = 3$, $s_x = 12345$, $s_w = 54321$, $t = 1.0$。\n2. $H = 8$, $W = 8$, $h = 1$, $w = 1$, $s_x = 1$, $s_w = 2$, $t = 1.0$。\n3. $H = 8$, $W = 8$, $h = 7$, $w = 7$, $s_x = 987654321$, $s_w = 123456789$, $t = -1.0$。\n4. $H = 6$, $W = 10$, $h = 2$, $w = 5$, $s_x = 555$, $s_w = 777$, $t = 1.0$。\n5. $H = 5$, $W = 5$, $h = 5$, $w = 5$, $s_x = 42$, $s_w = 43$, $t = -1.0$。\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含一个长度为 $5$ 的列表，每个元素是对应一个测试用例的列表。对于每个测试用例，输出一个包含四个浮点数的列表：$[\\Delta_{\\text{rand}}, \\Delta_{\\text{sal}}, G, R]$，每个浮点数都精确到 $6$ 位小数。最终输出必须是以下形式的单行\n$[[d_{1r}, d_{1s}, g_1, r_1],[d_{2r}, d_{2s}, g_2, r_2],[d_{3r}, d_{3s}, g_3, r_3],[d_{4r}, d_{4s}, g_4, r_4],[d_{5r}, d_{5s}, g_5, r_5]]$\n除了分隔列表元素所必需的空格外，不添加任何多余的空格。如果 $\\Delta_{\\text{rand}} = 0$，则在该情况下，在四舍五入和打印之前定义 $R = +\\infty$。\n\n您的程序必须完全按照规定实现 LCG 和数据生成，通过枚举所有有效位置来计算精确的平均值，并遵守四舍五入规则。不需要用户输入。输出必须与指定格式完全匹配。", "solution": "该问题要求我们量化 CutOut 数据增强技术在简单线性回归模型上的不同放置策略的效果。我们将通过均方误差 (MSE) 损失的增加来衡量此效果，该增加值可作为由该增强方法带来的正则化强度的代理指标。\n\n### 1. 模型与损失函数\n设输入图像为矩阵 $x \\in \\mathbb{R}^{H \\times W}$，模型权重为相应的矩阵 $w \\in \\mathbb{R}^{H \\times W}$。该模型是线性的，其预测值 $y$ 由 Frobenius 内积给出：\n$$\ny = \\langle w, x \\rangle = \\sum_{i=1}^{H}\\sum_{j=1}^{W} w_{ij}\\,x_{ij}\n$$\n给定一个标量目标值 $t \\in \\mathbb{R}$，损失函数为均方误差 (MSE)：\n$$\nL(x; w, t) = (\\langle w, x \\rangle - t)^2\n$$\n\n### 2. CutOut 增强\nCutOut 是一种数据增强方法，它通过将输入图像的某个连续矩形区域的像素值置零来对其进行蒙版处理。一个大小为 $h \\times w$ 的 CutOut 蒙版可以表示为一个二元矩阵 $m \\in \\{0,1\\}^{H \\times W}$，其中对应于 cutout 矩形的条目为 $0$，所有其他条目为 $1$。\n增强后的输入 $x'$ 通过 Hadamard (逐元素) 乘积获得：\n$$\nx' = m \\odot x\n$$\n设一个特定的 cutout 矩形表示为 $R$。在蒙版输入上的预测值 $y'$ 为：\n$$\ny' = \\langle w, x' \\rangle = \\sum_{(i,j) \\notin R} w_{ij}x_{ij} = \\sum_{i,j} w_{ij}x_{ij} - \\sum_{(i,j) \\in R} w_{ij}x_{ij}\n$$\n我们定义原始预测为 $y_0 = \\langle w, x \\rangle$，因蒙版引起的预测变化为 $\\delta_R = \\sum_{(i,j) \\in R} w_{ij}x_{ij}$。那么，蒙版处理后的预测为 $y' = y_0 - \\delta_R$。相应的损失为：\n$$\nL(x'; w, t) = (y' - t)^2 = (y_0 - \\delta_R - t)^2\n$$\n\n### 3. 放置策略与评估指标\n题目要求我们比较 CutOut 蒙版的两种放置策略：均匀随机和显著性感知。\n\n**无蒙版基线：** 没有任何增强的基线损失为 $L_0 = (y_0 - t)^2$。\n\n**均匀随机放置：**\n该策略是从所有可能的有效位置中均匀随机地选择 $h \\times w$ cutout 窗口的左上角。有效左上角位置的集合为 $\\{(i,j) \\mid 0 \\le i \\le H-h, 0 \\le j \\le W-w\\}$。设此类位置的总数为 $N_p = (H-h+1)(W-w+1)$。\n随机放置下的期望损失 $\\mathbb{E}_{\\text{rand}}[L]$ 是所有可能放置位置的平均损失：\n$$\n\\mathbb{E}_{\\text{rand}}[L] = \\frac{1}{N_p} \\sum_{\\text{all valid } R} (y_0 - \\delta_R - t)^2\n$$\n随机蒙版导致的损失增加为 $\\Delta_{\\text{rand}} = \\mathbb{E}_{\\text{rand}}[L] - L_0$。\n\n**显著性感知放置：**\n此策略将 cutout 蒙版放置在被认为最“重要”或“显著”的区域上。显著图定义为损失函数对输入的梯度的幅值：$s = \\left| \\nabla_x L \\right|$。\n$$\n\\frac{\\partial L}{\\partial x_{ij}} = \\frac{\\partial}{\\partial x_{ij}} (\\langle w, x \\rangle - t)^2 = 2(\\langle w, x \\rangle - t) \\frac{\\partial}{\\partial x_{ij}} (\\sum_{k,l} w_{kl}x_{kl}) = 2(y_0 - t)w_{ij}\n$$\n因此，显著图为 $s_{ij} = |\\frac{\\partial L}{\\partial x_{ij}}| = 2|y_0 - t||w_{ij}|$。\n为了最大化 cutout 区域内的总显著性，我们必须找到使 $\\sum_{(i,j) \\in R^*} s_{ij}$ 最大化的矩形 $R^*$。由于对于给定的无蒙版输入，项 $2|y_0 - t|$ 是一个恒定的正标量，这等价于找到使绝对权重幅值之和最大化的区域 $R^*$：\n$$\nR^* = \\arg\\max_{R} \\sum_{(i,j) \\in R} |w_{ij}|\n$$\n在这种显著性感知放置下的损失为 $L_{\\text{sal}} = (y_0 - \\delta_{R^*} - t)^2$。\n损失的增加为 $\\Delta_{\\text{sal}} = L_{\\text{sal}} - L_0$。\n\n**比较指标：**\n最后，我们计算两个指标来比较这两种策略：\n1.  增益：$G = \\Delta_{\\text{sal}} - \\Delta_{\\text{rand}}$\n2.  比率：$R = \\Delta_{\\text{sal}} / \\Delta_{\\text{rand}}$（如果 $\\Delta_{\\text{rand}}=0$，则定义为 $+\\infty$）\n\n### 4. 计算算法\n直接实现对所有滑动窗口的求和效率低下。我们可以使用积分图（也称为 summed-area tables）来显著优化计算。\n\n**数据生成：** 输入数组 $x$ 和 $w$ 是使用特定的线性同余生成器 (LCG) 生成的：$u_{k+1} = (a u_k + c) \\pmod m$，其中 $m = 2^{32}$，$a = 1664525$，$c = 1013904223$。初始状态 $u_0$ 是给定的种子 $s$。每个伪随机整数 $u_k$ 被转换为实数 $r_k = u_k/m \\in [0,1)$，然后映射到 $v_k = 2r_k - 1 \\in [-1,1)$。这些值按行主序填充数组。\n\n**使用积分图进行高效求和：**\n数组 $A$ 的积分图 $I_A$ 是一个数组，其中 $I_A(i,j)$ 存储了 $A$ 中从原点 $(0,0)$ 到 $(i,j)$ 的矩形内所有元素的总和。这允许使用四次查找在 $O(1)$ 时间内计算任何任意矩形的和。我们将预先计算两个积分图：\n1.  $I_{w \\odot x}$，用于逐元素乘积数组 $w_{ij}x_{ij}$。这用于快速计算任何区域 $R$ 的 $\\delta_R$。\n2.  $I_{|w|}$，用于绝对权重数组 $|w_{ij}|$。这用于通过快速计算所有候选区域的 $\\sum_{(i,j) \\in R} |w_{ij}|$ 来找到显著性感知区域 $R^*$。\n\n**分步计算：**\n1.  对于每个测试用例，使用指定的 LCG 和种子生成大小为 $H \\times W$ 的矩阵 $x$ 和 $w$。\n2.  计算无蒙版预测 $y_0 = \\sum_{i,j} w_{ij}x_{ij}$ 和无蒙版损失 $L_0 = (y_0 - t)^2$。\n3.  计算积分图 $I_{w \\odot x}$ 和 $I_{|w|}$。\n4.  初始化总损失变量 `total_loss_sum = 0`，以及用于跟踪最佳显著性放置的变量 `max_sal_sum = -1` 和 `best_placement = None`。\n5.  遍历 $h \\times w$ 窗口的所有 $N_p$ 个有效左上角位置 $(i,j)$。对于每个位置：\n    a. 使用 $I_{|w|}$ 找到当前窗口中绝对权重的总和。如果此总和大于 `max_sal_sum`，则更新 `max_sal_sum` 并将当前位置存储在 `best_placement` 中。\n    b. 使用 $I_{w \\odot x}$ 找到当前窗口的和 $\\delta_{ij}$。\n    c. 计算蒙版损失 $L_{ij} = (y_0 - \\delta_{ij} - t)^2$。\n    d. 将 $L_{ij}$ 加到 `total_loss_sum` 中。\n6.  循环结束后，计算期望的随机损失 $\\mathbb{E}_{\\text{rand}}[L] = \\text{total_loss_sum} / N_p$。然后计算 $\\Delta_{\\text{rand}} = \\mathbb{E}_{\\text{rand}}[L] - L_0$。\n7.  使用存储的 `best_placement`，利用 $I_{w \\odot x}$ 找到相应的 $\\delta_{R^*}$。\n8.  计算显著性感知损失 $L_{\\text{sal}} = (y_0 - \\delta_{R^*} - t)^2$。然后计算 $\\Delta_{\\text{sal}} = L_{\\text{sal}} - L_0$。\n9.  计算最终指标 $G = \\Delta_{\\text{sal}} - \\Delta_{\\text{rand}}$ 和 $R = \\Delta_{\\text{sal}} / \\Delta_{\\text{rand}}$。处理 $\\Delta_{\\text{rand}}$ 为零（或数值上接近零）的情况。\n10. 将四个结果指标（$\\Delta_{\\text{rand}}$, $\\Delta_{\\text{sal}}$, $G$, $R$）四舍五入到 $6$ 位小数并存储它们。\n此过程确保了按要求进行的精确计算，无需借助 Monte Carlo 近似，同时保持了计算效率。", "answer": "```python\nimport numpy as np\n# No other libraries are imported, as scipy is not strictly needed.\n# numpy provides sufficient functionality for this problem.\n\n# LCG parameters from the problem description.\nLCG_M = 2**32\nLCG_A = 1664525\nLCG_C = 1013904223\n\ndef generate_array(seed, H, W):\n    \"\"\"\n    Generates an HxW numpy array using the specified LCG.\n    Values are mapped to the range [-1, 1).\n    \"\"\"\n    n_values = H * W\n    u = seed\n    values = []\n    for _ in range(n_values):\n        u = (LCG_A * u + LCG_C) % LCG_M\n        # Convert to float in [0, 1) then to [-1, 1)\n        v = 2 * (u / LCG_M) - 1\n        values.append(v)\n    \n    return np.array(values, dtype=np.float64).reshape((H, W))\n\ndef integral_image(arr):\n    \"\"\"\n    Computes the summed-area table (integral image) of a 2D array.\n    \"\"\"\n    S = np.zeros((arr.shape[0] + 1, arr.shape[1] + 1), dtype=np.float64)\n    S[1:, 1:] = np.cumsum(np.cumsum(arr, axis=0), axis=1)\n    return S\n\ndef sum_rect(integral_img, r, c, h, w):\n    \"\"\"\n    Calculates the sum over a rectangle using the integral image.\n    r, c are 0-indexed top-left corner coordinates.\n    \"\"\"\n    r1, c1 = r, c\n    r2, c2 = r + h, c + w\n    # The integral_img is 1-padded, so coordinates map directly.\n    return integral_img[r2, c2] - integral_img[r1, c2] - integral_img[r2, c1] + integral_img[r1, c1]\n\ndef compute_metrics(H, W, h, w_mask, s_x, s_w, t):\n    \"\"\"\n    Computes all required metrics for a single test case.\n    \"\"\"\n    # 1. Generate data\n    x = generate_array(s_x, H, W)\n    weights = generate_array(s_w, H, W)\n\n    # 2. Calculate unmasked baseline\n    y0 = np.sum(weights * x)\n    L0 = (y0 - t)**2\n\n    # 3. Pre-compute for efficient summation\n    integral_wx = integral_image(weights * x)\n    integral_abs_w = integral_image(np.abs(weights))\n    \n    num_placements = (H - h + 1) * (W - w_mask + 1)\n    \n    total_masked_loss = 0.0\n    \n    max_sal_sum = -1.0\n    best_placement_ij = (0, 0)\n    \n    # 4. Iterate over all possible placements\n    for i in range(H - h + 1):\n        for j in range(W - w_mask + 1):\n            # Saliency-aware placement search\n            current_sal_sum = sum_rect(integral_abs_w, i, j, h, w_mask)\n            if current_sal_sum > max_sal_sum:\n                max_sal_sum = current_sal_sum\n                best_placement_ij = (i, j)\n\n            # Random placement calculation\n            delta_ij = sum_rect(integral_wx, i, j, h, w_mask)\n            loss_ij = (y0 - delta_ij - t)**2\n            total_masked_loss += loss_ij\n            \n    # 5. Calculate random masking metrics\n    E_rand_L = total_masked_loss / num_placements\n    delta_rand = E_rand_L - L0\n\n    # 6. Calculate saliency-aware masking metrics\n    i_star, j_star = best_placement_ij\n    delta_sal = sum_rect(integral_wx, i_star, j_star, h, w_mask)\n    L_sal = (y0 - delta_sal - t)**2\n    delta_sal_val = L_sal - L0\n    \n    # 7. Calculate final comparative metrics\n    G = delta_sal_val - delta_rand\n    \n    if abs(delta_rand)  1e-12: # Treat as zero\n        R = np.inf\n    else:\n        R = delta_sal_val / delta_rand\n\n    return [delta_rand, delta_sal_val, G, R]\n    \ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (H, W, h, w_mask, s_x, s_w, t)\n        (8, 8, 3, 3, 12345, 54321, 1.0),\n        (8, 8, 1, 1, 1, 2, 1.0),\n        (8, 8, 7, 7, 987654321, 123456789, -1.0),\n        (6, 10, 2, 5, 555, 777, 1.0),\n        (5, 5, 5, 5, 42, 43, -1.0),\n    ]\n\n    all_results = []\n    \n    def format_float(val):\n        \"\"\"Formats floats to .6f, handles infinity.\"\"\"\n        if val == np.inf:\n            return 'inf'\n        return f'{val:.6f}'\n\n    for case in test_cases:\n        H, W, h, w_mask, s_x, s_w, t = case\n        result = compute_metrics(H, W, h, w_mask, s_x, s_w, t)\n        all_results.append(result)\n\n    # Format the output string as per problem specification.\n    # e.g., [[d1,d2,g1,r1],[d3,d4,g2,r2]]\n    result_strings = []\n    for res in all_results:\n        formatted_res = [format_float(v) for v in res]\n        result_strings.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3111355"}, {"introduction": "Mixup 通过对成对的图像及其标签进行线性插值来生成新的训练样本，这项技术以提高模型的泛化能力和鲁棒性而闻名。然而，当类别之间的真实关系是高度非线性时，这种在特征和标签空间中的线性插值可能会带来问题。在本练习 [@problem_id:3111279] 中，你将通过模拟来研究 Mixup 的一个关键局限性，探索在何种条件下，它会促使模型学习一个过于简化的线性决策边界，而忽略了本应由曲线分离的类别。", "problem": "给定一个关于两个细粒度图像类别的综合性纯数学模型，其贝叶斯最优决策边界是高度弯曲的。目标是研究在何种情况下，使用参数 $\\alpha$ 的 Mixup 增强会产生有害的标签插值，从而鼓励形成过于线性的决策边界。您必须编写一个完整、可运行的程序，通过蒙特卡洛模拟，根据一个从第一性原理和经过充分检验的事实推导出的原则性标准，计算出现这种有害效应的 $\\alpha$ 值范围。\n\n设置如下。考虑 $\\mathbb{R}^2$ 中的两个类别，由两个同心圆上的点定义。类别 $\\mathcal{C}_0$ 是半径为 $r_0$ 的内圆，类别 $\\mathcal{C}_1$ 是半径为 $r_1$ 的外圆。这两个类别的贝叶斯最优分类器是径向阈值 $R$ 的指示函数，即，如果 $\\|x\\|  R$，则点 $x \\in \\mathbb{R}^2$ 被分类为 $\\mathcal{C}_0$；如果 $\\|x\\| \\ge R$，则被分类为 $\\mathcal{C}_1$。该决策边界是一个圆，因此是高度弯曲的。Mixup 通过凸组合生成增强样本：给定点 $x_i \\in \\mathcal{C}_0$，$x_j \\in \\mathcal{C}_1$，混合系数 $\\lambda \\sim \\operatorname{Beta}(\\alpha,\\alpha)$，以及独热（one-hot）标签 $y_i = [1,0]$，$y_j = [0,1]$，则增强点为 $x_{\\text{mix}} = \\lambda x_i + (1 - \\lambda) x_j$，增强软标签为 $y_{\\text{mix}} = \\lambda [1,0] + (1 - \\lambda) [0,1]$。对于具有弯曲边界的细粒度类别，Mixup 可能会将接近 0.5 的模糊标签应用于空间上远离贝叶斯决策边界的位置，从而迫使学习到的决策面向弦方向线性化，而不是遵循原有的曲率。\n\n您必须设计一个量化标准，用于判断 Mixup 何时是“有害线性化的”。使用以下定义、参数和单位：\n\n- 内外半径分别为 $r_0 = 1.0$ 和 $r_1 = 2.0$。\n- 贝叶斯决策阈值半径为 $R = 1.5$。\n- 角度在 $[0, 2\\pi)$ 范围内以弧度为单位均匀采样。\n- 如果 $|\\|x_{\\text{mix}}\\| - R| \\ge m$，其中边距 $m = 0.2$，则一个 Mixup 样本被认为是相对于贝叶斯边界“深”的。一个“深”样本在空间上明确属于某一类别区域，不靠近边界。\n- 如果 $|\\lambda - 0.5| \\le \\delta$，其中 $\\delta = 0.1$，则一个 Mixup 标签被认为是“模糊的”，这意味着软标签接近于各占一半的插值。\n- 将有害性得分 $H(\\alpha)$ 定义为既“深”又具有“模糊”标签的增强样本的比例。直观地说，较大的 $H(\\alpha)$ 表明 Mixup 频繁地将接近 0.5 的标签分配给明确属于某一类别区域的点，这鼓励了跨越弯曲区域的过于线性的决策边界。\n- 如果 $H(\\alpha) \\ge \\tau$，其中阈值 $\\tau = 0.15$，则宣称 Mixup 在给定的 $\\alpha$ 下是“过于线性的”。\n\n您的程序必须执行以下操作：\n\n1.  为保证可复现性，固定随机种子。生成 $N = 10000$ 个随机点对 $(x_i, x_j)$，其中 $x_i$ 在半径为 $r_0$ 的内圆上，$x_j$ 在半径为 $r_1$ 的外圆上，每个点都由从 $[0, 2\\pi)$ 范围内以弧度为单位均匀采样的独立角度通过 $x = r[\\cos(\\theta), \\sin(\\theta)]$ 构建。\n\n2.  对于一个由从 $\\alpha_{\\text{min}} = 0.1$ 到 $\\alpha_{\\text{max}} = 64.0$ 的 $M_{\\alpha} = 41$ 个线性间隔点组成的 $\\alpha$ 值网格 $\\mathcal{A}$，通过以下方式估计 $H(\\alpha)$：\n    -   为 $N$ 个点对中的每一个独立采样 $\\lambda_k \\sim \\operatorname{Beta}(\\alpha, \\alpha)$。\n    -   形成 $x_{\\text{mix},k} = \\lambda_k x_{i,k} + (1 - \\lambda_k) x_{j,k}$。\n    -   计算同时满足 $|\\|x_{\\text{mix},k}\\| - R| \\ge m$ 和 $|\\lambda_k - 0.5| \\le \\delta$ 的索引 $k$ 的比例。\n\n3.  在网格 $\\mathcal{A}$ 内，识别出满足 $H(\\alpha) \\ge \\tau$ 的连续区间 $[\\alpha_{\\mathrm{low}}, \\alpha_{\\mathrm{high}}]$。如果没有网格点满足 $H(\\alpha) \\ge \\tau$，则定义 $\\alpha_{\\mathrm{low}} = 0.0$ 和 $\\alpha_{\\mathrm{high}} = 0.0$。\n\n4.  对于以下测试集中的 $\\alpha$ 值：$\\alpha \\in \\{0.2, 0.5, 1.0, 2.0, 8.0, 64.0\\}$，评估条件 $H(\\alpha) \\ge \\tau$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。前两个元素必须是区间端点 $\\alpha_{\\mathrm{low}}$ 和 $\\alpha_{\\mathrm{high}}$，四舍五入到 3 位小数，然后是测试集中每个 $\\alpha$ 值的布尔结果，按给定顺序排列。例如，输出格式必须严格遵循 $[\\alpha_{\\mathrm{low}},\\alpha_{\\mathrm{high}},b_1,b_2,b_3,b_4,b_5,b_6]$ 的形式，其中每个 $b_i$ 为 $\\text{True}$ 或 $\\text{False}$。\n\n所有角度必须以弧度为单位。不涉及物理单位。阈值 $\\tau$ 必须被视为小数（而非百分比）。\n\n该问题设计为可测试的，并包含以下覆盖情况：\n- 一般情况：$\\alpha = 2.0$。\n- 类似边界的情况：$\\alpha = 1.0$ 和 $\\alpha = 0.5$。\n- 极端情况：非常小的 $\\alpha = 0.2$ 和非常大的 $\\alpha = 64.0$。\n- 较高但非极端的情况：$\\alpha = 8.0$。\n\n您的程序必须是自包含的，不需要用户输入，并严格遵守指定的输出格式。", "solution": "该问题要求进行蒙特卡洛模拟，以确定 Mixup 参数 $\\alpha$ 在何种范围内，会对一个具有弯曲决策边界的合成数据集产生“有害的线性化”效应。解决方案涉及将问题标准形式化、实施模拟并分析结果。\n\n### 1. 几何模型与贝叶斯最优分类器\n\n该问题定义了 $\\mathbb{R}^2$ 中的一个二分类任务。类别 $\\mathcal{C}_0$ 由半径为 $r_0 = 1.0$ 的圆上的点组成，类别 $\\mathcal{C}_1$ 由半径为 $r_1 = 2.0$ 的同心圆上的点组成。最小化分类误差的贝叶斯最优决策边界将这两个类别分开。对于这个同心圆模型，最优边界是一个半径为 $R$ 的圆，其半径介于 $r_0$ 和 $r_1$ 之间。问题指定这个贝叶斯决策阈值半径为 $R = 1.5$。如果一个点 $x \\in \\mathbb{R}^2$ 的欧几里得范数 $\\|x\\|  R$，则它被分类为 $\\mathcal{C}_0$；如果 $\\|x\\| \\ge R$，则被分类为 $\\mathcal{C}_1$。这个边界是“高度弯曲的”，理想的分类器应该能学习到这个圆形的分割。\n\n用于模拟的点是通过从 $[0, 2\\pi)$ 均匀采样一个角度 $\\theta$ 并使用极坐标到笛卡尔坐标的转换来生成的：\n- 对于类别 $\\mathcal{C}_0$：$x_i = [r_0 \\cos(\\theta_i), r_0 \\sin(\\theta_i)]$\n- 对于类别 $\\mathcal{C}_1$：$x_j = [r_1 \\cos(\\theta_j), r_1 \\sin(\\theta_j)]$\n\n### 2. Mixup 增强\n\nMixup 通过对现有样本对进行凸组合来生成新的训练样本。给定一个来自类别 $\\mathcal{C}_0$ 的点 $x_i$ 和一个来自类别 $\\mathcal{C}_1$ 的点 $x_j$，以及相应的独热（one-hot）标签 $y_i = [1, 0]$ 和 $y_j = [0, 1]$，一个新的样本 $(x_{\\text{mix}}, y_{\\text{mix}})$ 按如下方式创建：\n$$\n\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)\n$$\n$$\nx_{\\text{mix}} = \\lambda x_i + (1 - \\lambda) x_j\n$$\n$$\ny_{\\text{mix}} = \\lambda y_i + (1 - \\lambda) y_j = [\\lambda, 1-\\lambda]\n$$\n参数 $\\alpha$ 控制混合系数 $\\lambda$ 的分布。当 $\\alpha \\to \\infty$ 时，$\\lambda$ 会急剧集中在 $0.5$ 附近。当 $\\alpha \\to 0$ 时，$\\lambda$ 集中在端点 $0$ 和 $1$。当 $\\alpha=1$ 时，$\\lambda$ 在 $[0,1]$ 上均匀分布。\n\n其几何解释是，$x_{\\text{mix}}$ 位于连接 $x_i$ 和 $x_j$ 的线段（即弦）上。问题在于，这种空间上的线性插值与标签的线性插值相配对。对于弯曲的边界，一个空间上远离边界的点可能会被赋予一个高度模糊的标签（例如 $[0.5, 0.5]$），这会鼓励模型学习沿弦的线性决策边界，而不是遵循真实的曲率。\n\n### 3. “有害线性化”的量化标准\n\n该问题提供了一种精确的量化方法来识别这种有害效应。它定义了两个条件：\n\n1.  **“模糊”标签**：如果插值标签 $y_{\\text{mix}} = [\\lambda, 1-\\lambda]$ 的分量接近 $0.5$，则认为它是模糊的。这被形式化为 $|\\lambda - 0.5| \\le \\delta$，其中 $\\delta = 0.1$。这对应于 $\\lambda \\in [0.4, 0.6]$。\n\n2.  **“深”样本**：如果生成的点 $x_{\\text{mix}}$ 在空间上远离贝叶斯边界，则它是“深”的。这被形式化为 $|\\|x_{\\text{mix}}\\| - R| \\ge m$，其中边距为 $m = 0.2$。这意味着该点明确位于某个类别区域内部，而不是在半径为 $R=1.5$ 的边界圆附近的模糊区域。\n\n**有害性得分 $H(\\alpha)$** 被定义为这两个事件同时发生的联合概率：即 Mixup 样本在空间上既“深”又具有“模糊”标签的比例。较高的 $H(\\alpha)$ 表示 Mixup 正在为分类器频繁生成混淆信号。\n\n如果这个分数超过一个阈值，即 $H(\\alpha) \\ge \\tau$（其中 $\\tau = 0.15$），则认为 Mixup 是**“过于线性的”**。\n\n### 4. 蒙特卡洛模拟设计\n\n我们使用蒙特卡洛模拟来估计 $H(\\alpha)$。流程如下：\n\n1.  **固定随机性**：设置一个全局随机种子以确保可复现性。\n2.  **生成基础几何结构**：生成一个固定的包含 $N = 10000$ 个点对 $(x_{i,k}, x_{j,k})$ 的集合，其中 $k=1, \\dots, N$。角度 $\\theta_{i,k}$ 和 $\\theta_{j,k}$ 是从 $[0, 2\\pi)$ 中独立均匀采样的。这组几何构型在整个模拟过程中保持不变，以减少方差。\n3.  **遍历 $\\alpha$**：对于指定的网格和测试集中的每个 $\\alpha$ 值：\n    a.  **采样混合系数**：从 $\\operatorname{Beta}(\\alpha, \\alpha)$ 分布中抽取 $N$ 个独立的样本 $\\lambda_k$。\n    b.  **创建 Mixup 样本**：对每个 $k$ 计算 $x_{\\text{mix},k} = \\lambda_k x_{i,k} + (1 - \\lambda_k) x_{j,k}$。\n    c.  **评估条件**：对于每个生成的样本 $k$，检查是否满足“模糊”和“深”的条件：\n        -   $C_{\\text{ambiguous}, k}: |\\lambda_k - 0.5| \\le \\delta$\n        -   $C_{\\text{deep}, k}: |\\|x_{\\text{mix},k}\\| - R| \\ge m$\n    d.  **估计 $H(\\alpha)$**：该得分是两个条件都为真的样本的经验比例：\n        $$\n        H(\\alpha) \\approx \\frac{1}{N} \\sum_{k=1}^{N} \\mathbb{I}(C_{\\text{ambiguous}, k} \\land C_{\\text{deep}, k})\n        $$\n        其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n### 5. 算法实现与分析\n\n该模拟使用 Python 的 `numpy` 库实现，以进行高效的向量化操作。\n\n-   一个辅助函数 `calculate_H` 封装了为给定 $\\alpha$ 计算 $H(\\alpha)$ 的逻辑，并使用预先生成的几何点对。\n-   首先，对从 $\\alpha_{\\text{min}}=0.1$ 到 $\\alpha_{\\text{max}}=64.0$ 的线性间隔网格上的每个 $\\alpha$ 调用此函数。\n-   将得到的 $H(\\alpha)$ 值数组与阈值 $\\tau=0.15$ 进行比较。网格中满足 $H(\\alpha) \\ge \\tau$ 的最小和最大 $\\alpha$ 值定义了区间 $[\\alpha_{\\mathrm{low}}, \\alpha_{\\mathrm{high}}]$。如果没有值满足条件，则区间默认为 $[0.0, 0.0]$。\n-   然后，对特定测试集 $\\{0.2, 0.5, 1.0, 2.0, 8.0, 64.0\\}$ 中的每个 $\\alpha$ 调用 `calculate_H` 函数，以确定每种情况下 $H(\\alpha) \\ge \\tau$ 的布尔结果。\n-   最后，将结果格式化为所需的字符串输出，浮点数四舍五入到三位小数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the range of Mixup alpha values that cause \"harmful linearization\"\n    and evaluates this condition for a specific test suite.\n    \"\"\"\n    # 1. Define constants and set random seed for reproducibility\n    SEED = 42\n    np.random.seed(SEED)\n\n    r0 = 1.0\n    r1 = 2.0\n    R = 1.5\n    m = 0.2\n    delta = 0.1\n    tau = 0.15\n\n    N = 10000\n    M_alpha = 41\n    alpha_min = 0.1\n    alpha_max = 64.0\n\n    # 2. Generate N random pairs of points (xi, xj)\n    # This geometric data is fixed for all subsequent alpha evaluations\n    theta_i = np.random.uniform(0.0, 2.0 * np.pi, N)\n    theta_j = np.random.uniform(0.0, 2.0 * np.pi, N)\n    \n    xi = r0 * np.column_stack([np.cos(theta_i), np.sin(theta_i)])\n    xj = r1 * np.column_stack([np.cos(theta_j), np.sin(theta_j)])\n\n    def calculate_H(alpha, n_samples, xi_data, xj_data):\n        \"\"\"\n        Calculates the harmfulness score H(alpha) for a given alpha.\n        \"\"\"\n        if alpha == 0:\n            return 0.0\n            \n        # Sample N mixing coefficients from the Beta(alpha, alpha) distribution\n        lambdas = np.random.beta(alpha, alpha, n_samples)\n        \n        # Form the mixed points\n        # Reshape lambdas for broadcasting: (n_samples,) -> (n_samples, 1)\n        x_mix = lambdas[:, np.newaxis] * xi_data + (1 - lambdas)[:, np.newaxis] * xj_data\n        \n        # Calculate the Euclidean norms of the mixed points\n        x_mix_norm = np.linalg.norm(x_mix, axis=1)\n        \n        # Check the \"deep\" and \"ambiguous\" conditions\n        is_deep = np.abs(x_mix_norm - R) >= m\n        is_ambiguous = np.abs(lambdas - 0.5) = delta\n        \n        # The harmfulness score is the fraction of samples meeting both criteria\n        H = np.mean(is_deep  is_ambiguous)\n        return H\n\n    # 3. Compute H(alpha) over the specified grid of alpha values\n    alpha_grid = np.linspace(alpha_min, alpha_max, M_alpha)\n    H_values = np.array([calculate_H(alpha, N, xi, xj) for alpha in alpha_grid])\n\n    # 4. Identify the interval [alpha_low, alpha_high]\n    is_overly_linear = H_values >= tau\n    harmful_alphas = alpha_grid[is_overly_linear]\n    \n    if harmful_alphas.size > 0:\n        alpha_low = np.min(harmful_alphas)\n        alpha_high = np.max(harmful_alphas)\n    else:\n        alpha_low = 0.0\n        alpha_high = 0.0\n\n    # 5. Evaluate the condition for the test suite of alpha values\n    test_suite = [0.2, 0.5, 1.0, 2.0, 8.0, 64.0]\n    test_results = []\n    for alpha_test in test_suite:\n        H_test = calculate_H(alpha_test, N, xi, xj)\n        test_results.append(H_test >= tau)\n\n    # 6. Format the final output string\n    # Format: [alpha_low, alpha_high, b1, b2, b3, b4, b5, b6]\n    output_list = [f\"{alpha_low:.3f}\", f\"{alpha_high:.3f}\"] + [str(b) for b in test_results]\n    print(f\"[{','.join(output_list)}]\")\n\nsolve()\n```", "id": "3111279"}, {"introduction": "CutMix 在 Cutout 和 Mixup 的基础上进行了改进，它将一个图像区域替换为另一个训练样本的图块，并按比例混合它们的标签。这鼓励模型在不依赖完整视觉上下文的情况下识别对象。但当上下文本身就是特征时，会发生什么呢？这个动手实验 [@problem_id:3151909] 要求你创建一个合成数据集，其中类别身份完全由全局空间模式定义。你将通过这个实验来检验 CutMix 的区域替换是否会破坏这些关键的远距离线索，从而阻碍模型的学习过程。", "problem": "您必须编写一个完整、可运行的程序，该程序构建一个合成图像数据集，其中类别身份由全局空间排列决定，在不同的增强配置下训练一个多项逻辑回归模型，并评估这些增强是否会破坏模型学习全局上下文的能力。您的方法必须基于 softmax 模型下的交叉熵进行经验风险最小化，并基于增强的正式定义。数据集和学习问题必须是纯粹数学上和逻辑上定义的，不依赖任何外部文件。您的程序必须从头开始使用线性代数运算实现所有步骤。\n\n数据集设计：创建尺寸为 $H \\times W$ 的灰度图像，其强度值在 $[0,1]$ 范围内。数据集有两个类别，使用 $\\{[1,0],[0,1]\\}$ 中的独热向量进行编码。每张图像被划分为四个相等的象限。设 $c_{\\mathrm{lo}} \\in (0,1)$ 和 $c_{\\mathrm{hi}} \\in (0,1)$，且 $c_{\\mathrm{lo}}  c_{\\mathrm{hi}}$。类别条件生成规则定义如下：\n- 对于类别 $0$：左上和右下象限填充 $c_{\\mathrm{hi}}$，右上和左下象限填充 $c_{\\mathrm{lo}}$。\n- 对于类别 $1$：左上和右下象限填充 $c_{\\mathrm{lo}}$，右上和左下象限填充 $c_{\\mathrm{hi}}$。\n完成此确定性构造后，向每个像素添加独立的 高斯噪声 $\\mathcal{N}(0,\\sigma^{2})$，并将结果裁剪到 $[0,1]$ 范围内。这种构造确保了类别仅通过象限的全局排列来区分，而非局部纹理。\n\n模型和学习目标：使用具有两个类别的多项逻辑回归（softmax 回归）。对于一个输入向量 $x \\in \\mathbb{R}^{D}$（其中 $D = H \\cdot W$），类别概率建模为\n$$\np_{\\theta}(y=k \\mid x) = \\frac{\\exp(w_{k}^{\\top} x + b_{k})}{\\sum_{j=0}^{1} \\exp(w_{j}^{\\top} x + b_{j})}, \\quad k \\in \\{0,1\\},\n$$\n其中参数为 $\\theta = \\{W,b\\}$，$W \\in \\mathbb{R}^{D \\times 2}$ 且 $b \\in \\mathbb{R}^{2}$。通过最小化交叉熵下的经验风险来训练，可能使用软目标：\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{k=0}^{1} y_{i,k} \\log p_{\\theta}(y=k \\mid x_{i}),\n$$\n其中，对于未增强的样本，$y_{i} \\in [0,1]^{2}$ 是独热编码；对于如下文所述的 CutMix，$y_{i}$ 可以是凸组合。使用小批量梯度下降法优化 $\\mathcal{L}$。\n\n需要实现的增强方法：\n- Cutout：给定一张图像 $x \\in [0,1]^{H \\times W}$ 和一个边长为 $s = \\lfloor \\sqrt{f} \\cdot H \\rfloor$（对于比例 $f \\in (0,1)$）的方形掩码，选择一个均匀随机的左上角位置以确保方形在边界内，并用一个常数值 $m \\in [0,1]$（使用数据集的平均强度）填充被掩盖的区域，标签保持不变。\n- CutMix：给定两张图像 $x_{a}, x_{b}$ 及其标签 $y_{a}, y_{b}$，从 $\\operatorname{Beta}(\\alpha,\\alpha)$（其中 $\\alpha0$）分布中采样 $\\lambda$。通过将其边长设置为与 $\\sqrt{1-\\lambda}$ 成比例，并将其放置在一个均匀随机的位置，计算一个面积比例为 $(1-\\lambda)$ 的矩形区域。用来自 $x_{b}$ 的相应图像块替换 $x_{a}$ 中的该区域以获得 $\\tilde{x}$，并使用混合标签 $\\tilde{y} = \\lambda y_{a} + (1-\\lambda) y_{b}$。作为一个边界情况，允许当 $\\lambda = 0$ 时完全替换，这对应于使用整个 $x_{b}$ 图像和标签 $y_{b}$。\n\n训练和评估协议：将图像展平为 $\\mathbb{R}^{D}$ 中的向量，使用固定的周期数、学习率和批量大小训练模型，然后报告在一个留出的测试集上定义的测试准确率（正确预测类别索引的比例）。使用固定的随机种子以确保确定性行为。\n\n测试套件：您的程序必须运行以下四种配置，并按所列顺序返回每种配置的测试准确率。\n- 情况 1（理想路径）：无增强。\n- 情况 2（覆盖变体）：使用 Cutout，比例 $f = 0.50$，填充值等于数据集的平均强度 $m$。\n- 情况 3（研究中的增强）：使用 CutMix，Beta 分布 $\\operatorname{Beta}(\\alpha,\\alpha)$ 中的 $\\alpha = 1.0$。\n- 情况 4（边界条件）：使用 CutMix 进行完全替换，即确定性地设置 $\\lambda = 0$，使得粘贴的矩形是整个图像。\n\n在所有情况下使用的固定超参数和数据规格：\n- 图像高度 $H = 16$ 和宽度 $W = 16$。\n- 低强度值 $c_{\\mathrm{lo}} = 0.20$ 和高强度值 $c_{\\mathrm{hi}} = 0.80$。\n- 噪声标准差 $\\sigma = 0.05$。\n- 训练集大小 $N_{\\mathrm{train}} = 400$ 和测试集大小 $N_{\\mathrm{test}} = 200$。\n- 批量大小 $B = 64$，周期数 $E = 60$，学习率 $\\eta = 0.1$。\n- 随机种子 $s_{0} = 42$。\n- 对两个标签使用独热编码，仅在应用 CutMix 时使用软凸组合。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含一个逗号分隔的浮点准确率列表，格式为 $[a_{1},a_{2},a_{3},a_{4}]$，分别对应情况 1 到 4，该列表包含在方括号中，且不含多余的空格或文本（例如，$[0.9750,0.9600,0.9100,0.5200]$）。此问题不涉及单位，任何分数量必须在程序输出中表示为小数。", "solution": "目标是研究空间数据增强——特别是 Cutout 和 CutMix——对多项逻辑回归模型性能的影响。该学习任务的设计使得类别身份完全由特征的全局空间排列决定，而非局部内容。我们将从基本原理出发实现整个实验流程，包括数据集生成、通过梯度下降进行模型训练以及增强算法，以评估四种不同的训练配置。\n\n首先，我们正式定义合成数据集。图像尺寸为 $H \\times W$，其中 $H=16$，$W=16$。有两个类别，$k \\in \\{0, 1\\}$。图像画布被划分为四个相等的 $8 \\times 8$ 象限。对于类别 $k=0$ 的图像，左上和右下象限填充高强度值 $c_{\\mathrm{hi}}=0.80$，而右上和左下象限填充低强度值 $c_{\\mathrm{lo}}=0.20$。对于类别 $k=1$，此分配相反。在此确定性构造之后，我们为每个像素添加从 $\\mathcal{N}(0, \\sigma^2)$（其中 $\\sigma=0.05$）中抽取的独立同分布的高斯噪声。最终的像素强度被裁剪到 $[0, 1]$ 范围内。此过程生成的数据集中，核心区别特征是强度的全局“棋盘格”模式。我们生成大小为 $N_{\\mathrm{train}}=400$ 的训练集和 $N_{\\mathrm{test}}=200$ 的测试集，类别均衡。所有随机过程的随机种子固定为 $s_0=42$，以确保可复现性。\n\n所采用的模型是多项逻辑回归，也称为 softmax 回归。输入图像首先被展平为一个向量 $x \\in \\mathbb{R}^{D}$，其中维度 $D = H \\cdot W = 256$。模型参数是一个权重矩阵 $W \\in \\mathbb{R}^{D \\times 2}$ 和一个偏置向量 $b \\in \\mathbb{R}^{2}$。对于给定的输入 $x$，模型为每个类别 $k$ 计算分数 $z_k = w_k^\\top x + b_k$。这些分数使用 softmax 函数转换为概率：\n$$\np_{\\theta}(y=k \\mid x) = \\frac{\\exp(z_k)}{\\sum_{j=0}^{1} \\exp(z_j)}\n$$\n其中 $\\theta = \\{W, b\\}$。\n\n模型参数通过最小化经验风险来学习，具体是指在训练数据集上的平均交叉熵损失。对于一个包含 $N$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^N$ 的训练集，其中 $y_i$ 是标签向量，损失函数为：\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=0}^{1} y_{i,k} \\log p_{\\theta}(y=k \\mid x_{i})\n$$\n对于标准分类，标签向量 $y_i$ 是一个独热编码（例如，类别 0 为 $[1, 0]$）。对于由 CutMix 创建的样本，$y_i$ 变成一个“软标签”，表示原始独热标签的凸组合。\n\n优化过程使用小批量梯度下降法进行。对于大小为 $B$ 的小批量，损失函数关于参数的梯度为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{B} X_{\\text{batch}}^\\top (P - Y_{\\text{batch}})\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{B} \\sum_{i=1}^{B} (p_i - y_i)\n$$\n其中 $X_{\\text{batch}} \\in \\mathbb{R}^{B \\times D}$ 是输入向量矩阵，$Y_{\\text{batch}} \\in \\mathbb{R}^{B \\times 2}$ 是标签向量矩阵，$P \\in \\mathbb{R}^{B \\times 2}$ 是预测概率矩阵。参数使用学习率 $\\eta=0.1$ 进行 $E=60$ 个周期的迭代更新：\n$$\nW \\leftarrow W - \\eta \\frac{\\partial \\mathcal{L}}{\\partial W}\n$$\n$$\nb \\leftarrow b - \\eta \\frac{\\partial \\mathcal{L}}{\\partial b}\n$$\n\n我们实现并评估两种数据增强技术：\n\n1.  **Cutout**：对于批次中的每张图像，选择一个方形区域并将其像素替换为一个常数值。方形的边长为 $s = \\lfloor \\sqrt{f} \\cdot H \\rfloor$，其中比例 $f=0.50$，得到 $s = \\lfloor \\sqrt{0.50} \\cdot 16 \\rfloor = 11$。方形的左上角被均匀随机地选择，以确保方形保持在图像边界内。填充值是训练数据集的平均强度。图像的类别标签保持不变。这种增强遮挡了图像的很大一部分，可能会破坏全局模式。\n\n2.  **CutMix**：此技术结合了成对的训练样本。对于一对图像 $(x_a, x_b)$ 及其标签 $(y_a, y_b)$，从贝塔分布 $\\lambda \\sim \\operatorname{Beta}(\\alpha, \\alpha)$ 中采样一个混合系数 $\\lambda$，其中 $\\alpha=1.0$（这等价于一个均匀分布 $\\mathcal{U}[0,1]$）。从 $x_b$ 中剪切一个矩形图像块并粘贴到 $x_a$ 上。该图像块的面积是总图像面积的 $(1-\\lambda)$ 倍，其边长与 $\\sqrt{1-\\lambda}$ 成比例。图像块的位置是均匀随机选择的。得到的合成图像 $\\tilde{x}$ 被赋予一个软标签 $\\tilde{y} = \\lambda y_a + (1-\\lambda) y_b$。这迫使模型从碎片化的模式中学习，并将它们与按比例混合的标签关联起来。\n\n我们将执行四个测试案例以评估这些增强的影响：\n- **情况 1**：无增强，作为基线。\n- **情况 2**：使用 Cutout ($f=0.50$) 进行训练。\n- **情况 3**：使用 CutMix ($\\alpha=1.0$) 进行训练。\n- **情况 4**：CutMix 的一个边界条件，其中 $\\lambda$ 被确定性地设置为 $0$。这导致用同一批次中另一个随机选择的样本 $(x_b, y_b)$ 替换一个训练样本 $(x_a, y_a)$。\n\n对于每种情况，我们都在相同的训练数据上从头开始训练一个模型，并在相同的留出测试集上报告其最终准确率。这种受控比较将阐明这些增强如何与一个严重依赖全局上下文的学习问题相互作用。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n    \n    # --- Fixed Hyperparameters and Data Specifications ---\n    H, W = 16, 16\n    c_lo, c_hi = 0.20, 0.80\n    sigma = 0.05\n    N_train, N_test = 400, 200\n    B = 64\n    E = 60\n    eta = 0.1\n    s0 = 42\n    \n    # --- Case-specific parameters ---\n    case_params = [\n        {'aug': 'none'},\n        {'aug': 'cutout', 'f': 0.50},\n        {'aug': 'cutmix', 'alpha': 1.0},\n        {'aug': 'cutmix_lambda_0'},\n    ]\n\n    results = []\n    for params in case_params:\n        # Each case must be fully deterministic and reproducible\n        accuracy = train_and_evaluate(\n            H=H, W=W, c_lo=c_lo, c_hi=c_hi, sigma=sigma,\n            N_train=N_train, N_test=N_test, B=B, E=E, eta=eta,\n            seed=s0, aug_params=params\n        )\n        results.append(f\"{accuracy:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef train_and_evaluate(H, W, c_lo, c_hi, sigma, N_train, N_test, B, E, eta, seed, aug_params):\n    \"\"\"\n    Generates data, trains a model under a specific augmentation, and evaluates it.\n    \"\"\"\n    \n    # --- Seeding for reproducibility ---\n    rng = np.random.default_rng(seed)\n\n    # --- Helper Functions ---\n    def softmax(z):\n        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n\n    def cross_entropy_loss(y_true, y_pred):\n        # Clip y_pred to avoid log(0)\n        y_pred = np.clip(y_pred, 1e-12, 1. - 1e-12)\n        return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n        \n    # --- Dataset Generation ---\n    def generate_dataset(N, H, W, c_lo, c_hi, sigma, rng_gen):\n        n_per_class = N // 2\n        X = np.zeros((N, H, W), dtype=np.float32)\n        Y = np.zeros((N, 2), dtype=np.float32)\n        \n        h_half, w_half = H // 2, W // 2\n        \n        # Class 0\n        for i in range(n_per_class):\n            img = np.full((H, W), c_lo, dtype=np.float32)\n            img[:h_half, :w_half] = c_hi\n            img[h_half:, w_half:] = c_hi\n            X[i] = img\n            Y[i] = [1, 0]\n            \n        # Class 1\n        for i in range(n_per_class, N):\n            img = np.full((H, W), c_hi, dtype=np.float32)\n            img[:h_half, :w_half] = c_lo\n            img[h_half:, w_half:] = c_lo\n            X[i] = img\n            Y[i] = [0, 1]\n\n        # Add noise and clip\n        X += rng_gen.normal(0, sigma, X.shape)\n        X = np.clip(X, 0.0, 1.0)\n        \n        # Shuffle dataset\n        indices = np.arange(N)\n        rng_gen.shuffle(indices)\n        X, Y = X[indices], Y[indices]\n        \n        return X, Y\n\n    X_train, Y_train = generate_dataset(N_train, H, W, c_lo, c_hi, sigma, rng)\n    X_test, Y_test = generate_dataset(N_test, H, W, c_lo, c_hi, sigma, rng)\n\n    # --- Model Initialization ---\n    D = H * W\n    K = 2 \n    # Use the same RNG for reproducible weight initialization\n    w_rng = np.random.default_rng(seed)\n    W_mat = w_rng.normal(0, 0.01, (D, K))\n    b_vec = np.zeros((1, K))\n\n    # --- Augmentation setup ---\n    aug = aug_params['aug']\n    \n    # Calculate dataset mean for Cutout\n    mean_intensity = 0.0\n    if aug == 'cutout':\n        mean_intensity = np.mean(X_train)\n\n    # --- Training Loop ---\n    for epoch in range(E):\n        indices = np.arange(N_train)\n        rng.shuffle(indices)\n        X_train_shuffled, Y_train_shuffled = X_train[indices], Y_train[indices]\n\n        for i in range(0, N_train, B):\n            X_batch_orig = X_train_shuffled[i:i+B]\n            Y_batch_orig = Y_train_shuffled[i:i+B]\n            \n            actual_B = X_batch_orig.shape[0]\n            if actual_B == 0: continue\n\n            # Apply augmentations\n            X_batch_aug, Y_batch_aug = X_batch_orig.copy(), Y_batch_orig.copy()\n\n            if aug == 'cutout':\n                f = aug_params['f']\n                s = int(np.floor(np.sqrt(f) * H))\n                for j in range(actual_B):\n                    y1 = rng.integers(0, H - s + 1)\n                    x1 = rng.integers(0, W - s + 1)\n                    X_batch_aug[j, y1:y1+s, x1:x1+s] = mean_intensity\n            \n            elif aug == 'cutmix':\n                alpha = aug_params['alpha']\n                for j in range(actual_B):\n                    lam = rng.beta(alpha, alpha)\n                    rand_index = rng.integers(actual_B)\n                    \n                    xa, ya = X_batch_aug[j], Y_batch_aug[j]\n                    xb, yb = X_batch_aug[rand_index], Y_batch_aug[rand_index]\n                    \n                    ratio = np.sqrt(1. - lam)\n                    patch_h = int(H * ratio)\n                    patch_w = int(W * ratio)\n\n                    if patch_h > 0 and patch_w > 0:\n                        cy = rng.integers(H - patch_h + 1)\n                        cx = rng.integers(W - patch_w + 1)\n                        xa[cy:cy+patch_h, cx:cx+patch_w] = xb[cy:cy+patch_h, cx:cx+patch_w]\n\n                    X_batch_aug[j] = xa\n                    Y_batch_aug[j] = lam * ya + (1. - lam) * yb\n\n            elif aug == 'cutmix_lambda_0':\n                # Deterministic lambda = 0\n                for j in range(actual_B):\n                    rand_index = rng.integers(actual_B)\n                    # Complete replacement of image and label\n                    X_batch_aug[j] = X_batch_orig[rand_index]\n                    Y_batch_aug[j] = Y_batch_orig[rand_index]\n\n            # Flatten images\n            X_batch_flat = X_batch_aug.reshape(actual_B, D)\n\n            # Forward pass\n            scores = X_batch_flat @ W_mat + b_vec\n            probs = softmax(scores)\n\n            # Backward pass (gradient calculation)\n            grad_scores = (probs - Y_batch_aug) / actual_B\n            grad_W = X_batch_flat.T @ grad_scores\n            grad_b = np.sum(grad_scores, axis=0, keepdims=True)\n\n            # Update parameters\n            W_mat -= eta * grad_W\n            b_vec -= eta * grad_b\n\n    # --- Evaluation ---\n    X_test_flat = X_test.reshape(N_test, D)\n    test_scores = X_test_flat @ W_mat + b_vec\n    test_probs = softmax(test_scores)\n    \n    predictions = np.argmax(test_probs, axis=1)\n    ground_truth = np.argmax(Y_test, axis=1)\n    \n    accuracy = np.mean(predictions == ground_truth)\n    return accuracy\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3151909"}]}