{"hands_on_practices": [{"introduction": "理论告诉我们，不同的激活函数需要搭配不同的权重初始化策略来维持信号方差的稳定。这个动手实践旨在通过编码模拟来将理论付诸实践。你将构建一个简单的深度网络，并亲眼见证当 He 初始化与 ReLU 激活函数配对时，以及 Xavier 初始化与 $\\tanh$ 配对时，信号方差是如何在网络层间保持稳定的，而错配的组合则会导致方差爆炸或消失。[@problem_id:3199598]", "problem": "给定一个包含 $L$ 层的全连接前馈网络，将第 $l$ 层的预激活定义为 $z^{(l)} = W^{(l)} a^{(l-1)}$，后激活定义为 $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$，其中 $a^{(0)} = x$。假设偏置为零，权重独立同分布，输入 $x \\in \\mathbb{R}^{n}$ 的分量独立、均值为零且方差有限。考虑两种广泛使用的随机权重初始化策略：Xavier (Glorot) 正态初始化和 He (Kaiming) 正态初始化，以及两种激活函数：$\\tanh$ 和修正线性单元 ($\\mathrm{ReLU}$)。目标是通过蒙特卡洛模拟，经验性地验证在哪种情况下，初始化策略能够近似保持各层预激活的方差，即在给定激活函数下，对于所有层 $l \\in \\{1,\\dots,L\\}$，都有 $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$。\n\n使用的基本原理：\n- 初始化时权重和激活在各个坐标上的独立性，以及独立变量和的方差线性性质。\n- $\\tanh$ 和 $\\mathrm{ReLU}$ 作为逐点非线性函数的定义。\n- 对于一个数组 $Y \\in \\mathbb{R}^{s \\times d}$，沿 $s$ 个样本的样本方差估计量定义为 $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$。\n\n你的程序必须：\n1. 构建具有指定 $L$ 和层宽度的网络，其中每一层的形状为 $(n_{\\text{in}}, n_{\\text{out}})$，为简化起见，这里 $n_{\\text{in}} = n_{\\text{out}}$。使用 Xavier 正态初始化或 He 正态初始化为每一层的权重 $W^{(l)}$ 进行初始化。使用由每种策略规定的方差进行零均值正态初始化；不添加任何偏置。\n2. 将输入 $x$ 抽取为 $s$ 个维度为 $n$ 的独立样本，每个分量服从 $\\mathcal{N}(0,1)$ 分布，即零均值和单位方差。\n3. 对每一层 $l$，通过对 $s$ 个样本求每个坐标样本方差的平均值，计算经验预激活方差 $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$，并类似地计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。将第 $l$ 层的相对偏差定义为 $$ \\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)} $$ 如果一个测试用例满足 $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$（容差 $\\varepsilon = 0.25$），则认为其保持了方差。\n4. 使用带有固定种子 $12345$ 的伪随机数生成器以确保可复现性。\n\n测试套件：\n- 测试用例 1：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态分布。\n- 测试用例 2：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 He 正态分布。\n- 测试用例 3：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态分布。\n- 测试用例 4：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 Xavier 正态分布。\n- 测试用例 5：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\tanh$，初始化 Xavier 正态分布。\n- 测试用例 6：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态分布。\n- 测试用例 7：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态分布。\n- 测试用例 8：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态分布。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$），其中每个 $\\text{result}_i$ 是一个布尔值，表示第 $i$ 个测试用例是否保持了方差，顺序与上述测试套件完全一致。", "solution": "该问题被评估为有效。它在科学上基于深度学习的既定原则，特别是关于权重初始化及其对信号传播的影响。问题提法明确，提供了所有必要的参数、定义和清晰、客观的成功标准。它没有矛盾、歧义和事实错误。因此，我们可以着手解决。\n\n目标是经验性地验证在何种条件下，深度神经网络各层的预激活 $z^{(l)}$ 的方差能够得以保持。此分析的核心在于连续层中预激活方差之间的递归关系。\n\n让我们考虑第 $l$ 层单个神经元的预激活：\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\n这里，$W_{ij}^{(l)}$ 是连接第 $l-1$ 层神经元 $j$ 与第 $l$ 层神经元 $i$ 的权重，$a_j^{(l-1)}$ 是前一层神经元 $j$ 的激活值。问题规定偏置为零，权重 $W_{ij}^{(l)}$ 从零均值分布中抽取，输入分量 $x_j$（构成 $a_j^{(0)}$）也具有零均值。我们假设在初始化时，所有 $j$ 的激活值 $a_j^{(l-1)}$ 都独立于权重 $W_{ij}^{(l)}$ 并且是同分布的。此外，如果激活值 $a^{(l-1)}$ 是对称激活函数应用于零均值输入 $z^{(l-1)}$ 的输出，它们也将具有零均值。对于 $\\mathrm{ReLU}$ 激活函数，情况并非如此，但得到的预激活 $z^{(l)}$ 仍将具有零均值，因为权重本身是零均值的：$\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$。\n\n在这些条件下，$z_i^{(l)}$ 的方差由下式给出：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\n由于和中各项的独立性（因为权重和前一层的激活是独立的），和的方差是方差的和：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\n对于两个独立的随机变量 $U$ 和 $V$，如果至少有一个均值为零（例如 $\\mathbb{E}[U]=0$），则 $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$。由于 $\\mathbb{E}[W_{ij}^{(l)}] = 0$，我们有：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\n假设第 $l$ 层的所有权重都是从方差为 $\\operatorname{Var}(W^{(l)})$ 的分布中独立同分布地抽取的，并且第 $l-1$ 层的所有激活值都是方差为 $\\operatorname{Var}(a^{(l-1)})$ 的独立同分布变量，则上式简化为：\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\n其中 $n_{l-1}$ 是第 $l-1$ 层的神经元数量。为了保持稳定的信号传播，我们需要保持方差，即 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$。这要求我们仔细选择权重初始化的方差 $\\operatorname{Var}(W^{(l)})$，以抵消激活函数 $\\phi$ 对方差的影响，该影响由项 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$ 捕捉。\n\n**激活函数分析**\n\n1.  **$\\tanh$ 激活函数：** 双曲正切函数 $\\tanh(z)$ 在原点附近是对称的（$\\tanh(0)=0$），并且对于小输入其行为类似于恒等函数（当 $z \\approx 0$ 时，$\\tanh(z) \\approx z$）。如果我们假设预激活 $z^{(l-1)}$ 集中在零附近（这是初始训练阶段的理想状态），那么 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$。将此代入我们的传播方程中得到：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    为实现 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$，我们必须设置 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。\n    **Xavier (Glorot) 正态初始化**正是为此情况设计的。它将从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的权重 $W^{(l)}$ 的方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    在我们特定的问题中，$n_{l-1} = n_l = n$，所以这变成 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$。这个选择完美地满足了条件 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。因此，Xavier 初始化预计能为 $\\tanh$ 激活函数保持方差。\n\n2.  **$\\mathrm{ReLU}$ 激活函数：** 修正线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$ 不是对称的。对于一个零均值、对称的输入分布 $z^{(l-1)}$（如高斯分布），恰好一半的输入将被置为零。这会影响方差。设 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$。输出 $a = \\mathrm{ReLU}(z)$ 的方差为 $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$。\n    激活平方的期望为 $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$。由于正态分布 $p(z)$ 的对称性，该积分是 $z^2$ 整个积分的一半：$\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$。\n    所以，对于 $\\mathrm{ReLU}$，我们有 $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$。方差传播方程变为：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    为了保持方差，我们必须有 $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$，这意味着 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$。\n    **He (Kaiming) 正态初始化**正是为此情况设计的。它将方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    这个选择完全满足条件。因此，He 初始化预计能为 $\\mathrm{ReLU}$ 激活函数保持方差。不匹配的组合（例如，$\\tanh$ 与 He，$\\mathrm{ReLU}$ 与 Xavier）预计将分别导致方差爆炸或消失。\n\n**模拟过程**\n\n程序将为8个测试用例中的每一个实现蒙特卡洛模拟。对于每个用例：\n1.  用值 $12345$ 为伪随机数生成器设置种子，以确保可复现性。\n2.  生成一个输入数据矩阵 $x \\in \\mathbb{R}^{s \\times n}$，其中每个元素都从 $\\mathcal{N}(0, 1)$ 中抽取。使用提供的公式计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。\n3.  从 $l=1$ 到 $L$ 逐层处理网络。在每一层中，权重矩阵 $W^{(l)}$ 从具有指定策略（Xavier 或 He）所规定方差的零均值正态分布中初始化。\n4.  计算预激活 $z^{(l)} = a^{(l-1)} W^{(l)}$。\n5.  计算经验方差 $\\widehat{\\operatorname{Var}}(z^{(l)})$。计算相对偏差 $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$。\n6.  跟踪所有层中的最大相对偏差 $\\max_{1 \\le l \\le L} \\delta^{(l)}$。\n7.  计算后激活 $a^{(l)} = \\phi(z^{(l)})$，作为下一层的输入。\n8.  遍历所有层后，如果 $\\max_{l} \\delta^{(l)} \\le \\varepsilon$（其中 $\\varepsilon = 0.25$），则认为该测试用例保持了方差。此检查将为每个用例产生一个布尔结果，然后进行报告。", "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3199598"}, {"introduction": "在前向传播中维持方差稳定性的最终目的是为了保证反向传播中梯度的有效流动。这个练习将引导你从第一性原理出发，推导梯度在网络中反向传播时的范数变化。通过计算不同初始化与激活函数组合下的梯度范数缩放因子，你将从数学上深刻理解为什么 He 初始化对于基于 ReLU 的网络至关重要，因为它能有效避免梯度消失或爆炸问题。[@problem_id:3125165]", "problem": "考虑一个具有 $L$ 个隐藏层的全连接前馈神经网络，每个隐藏层的宽度为 $n$，权重矩阵为 $W^{(1)}, \\dots, W^{(L)} \\in \\mathbb{R}^{n \\times n}$，偏置为零，激活函数为按元素的 $\\phi(\\cdot)$。设损失是网络输出的标量函数 $\\mathcal{L}$。假设在初始化时满足以下条件：(i) 每层中的权重是独立同分布的（i.i.d.），均值为零；(ii) 激活前的值 $z^{(l)}$ 在单元和层之间是独立的，关于零对称，对于 $\\tanh$ 激活函数，其分布为标准高斯随机变量 $z^{(l)} \\sim \\mathcal{N}(0,1)$；(iii) 输出层的反向传播梯度 $g^{(L)} = \\nabla_{h^{(L)}} \\mathcal{L}$ 的分量是独立同分布的，均值为零且具有有限二阶矩；(iv) 在初始化时，权重、激活值和梯度在各层之间是独立的。\n\n考虑两种权重初始化方案：Glorot (Xavier) 初始化，其中 $\\operatorname{Var}(W^{(l)}_{ij}) = 1/n$；以及 Kaiming He 初始化，其中 $\\operatorname{Var}(W^{(l)}_{ij}) = 2/n$。考虑两种激活函数：整流线性单元（ReLU），定义为 $\\phi(z) = \\max\\{0,z\\}$；以及双曲正切函数，定义为 $\\phi(z) = \\tanh(z)$。\n\n仅从反向传播的微分链式法则、独立随机变量的性质以及上述定义出发，推导梯度范数的平方期望值 $\\mathbb{E}\\left[\\|g^{(l)}\\|^{2}\\right]$ 在反向传播经过一层时变化的乘法因子表达式。然后，计算比率\n$$\nR \\;=\\; \\frac{\\mathbb{E}\\left[\\|g^{(0)}\\|^{2}\\right]}{\\mathbb{E}\\left[\\|g^{(L)}\\|^{2}\\right]}\n$$\n对于以下四种组合中的每一种：Xavier 与 ReLU，He 与 ReLU，Xavier 与 $\\tanh$，以及 He 与 $\\tanh$。对于 ReLU，利用 $z^{(l)}$ 的对称性来精确计算 $\\mathbb{E}\\left[(\\phi'(z^{(l)}))^{2}\\right]$。对于 $\\tanh$，将 $\\mathbb{E}\\left[(\\phi'(z^{(l)}))^{2}\\right]$ 表示为一个关于标准正态变量的一维高斯积分。将你的最终答案表示为一个单行矩阵，其四个条目按顺序对应上述四种情况，并将每个条目写成依赖于 $L$ 的闭式解析表达式。不需要四舍五入。不涉及物理单位。", "solution": "用户希望我验证并解决一个关于前馈神经网络在不同初始化方案和激活函数下梯度传播的问题。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- 网络：全连接前馈网络，$L$ 个隐藏层，宽度 $n$。\n- 权重：$W^{(1)}, \\dots, W^{(L)} \\in \\mathbb{R}^{n \\times n}$。\n- 偏置：零。\n- 激活函数：按元素的 $\\phi(\\cdot)$。\n- 损失：$\\mathcal{L}$，网络输出的标量函数。\n- 初始化时的统计条件：\n    - (i) 权重 $W^{(l)}_{ij}$ 是独立同分布的（i.i.d.），且 $\\mathbb{E}[W^{(l)}_{ij}] = 0$。\n    - (ii) 激活前的值 $z^{(l)}$ 在单元和层之间是独立的，关于零对称。对于 $\\tanh$，它们是标准高斯分布：$z^{(l)} \\sim \\mathcal{N}(0,1)$。\n    - (iii) 输出梯度 $g^{(L)} = \\nabla_{h^{(L)}} \\mathcal{L}$ 具有独立同分布的分量，零均值和有限二阶矩。\n    - (iv) 权重、激活值和梯度在各层之间是独立的。\n- 初始化方案：\n    - Glorot (Xavier)：$\\operatorname{Var}(W^{(l)}_{ij}) = 1/n$。\n    - Kaiming He：$\\operatorname{Var}(W^{(l)}_{ij}) = 2/n$。\n- 激活函数：\n    - ReLU：$\\phi(z) = \\max\\{0,z\\}$。\n    - 双曲正切：$\\phi(z) = \\tanh(z)$。\n- 任务：推导每层梯度范数平方期望值变化的乘法因子，并计算比率 $R = \\frac{\\mathbb{E}\\left[\\|g^{(0)}\\|^{2}\\right]}{\\mathbb{E}\\left[\\|g^{(L)}\\|^{2}\\right]}$，针对四种组合：（Xavier, ReLU）、（He, ReLU）、（Xavier, tanh）、（He, tanh）。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题具有科学依据，是深度学习中梯度流的标准理论分析，基于 Glorot  Bengio 和 He 等人的开创性论文。问题陈述清晰，目标明确，并有充分、一致的假设来推导出唯一解。语言精确客观。该问题不违反任何无效性标准。这些假设虽然理想化（例如，独立性、特定分布），但对于获得可行的解析解是必要的，并且在该领域是标准做法。\n\n**步骤3：结论与行动**\n该问题是**有效的**。我将继续进行解答。\n\n### 推导过程\n\n第 $l$ 层的正向传播规则由 $z^{(l)} = W^{(l)}h^{(l-1)}$ 和 $h^{(l)} = \\phi(z^{(l)})$ 给出，其中 $h^{(l-1)}$ 是前一层的输出。损失 $\\mathcal{L}$ 相对于第 $l$ 层激活值的梯度表示为 $g^{(l)} = \\nabla_{h^{(l)}} \\mathcal{L}$。\n\n使用链式法则，我们可以将第 $l-1$ 层的梯度与第 $l$ 层的梯度联系起来。梯度 $g^{(l-1)}$ 是一个向量，其第 $j$ 个分量为 $g^{(l-1)}_j = \\frac{\\partial \\mathcal{L}}{\\partial h^{(l-1)}_j}$。\n$$\ng^{(l-1)}_j = \\sum_{k=1}^n \\frac{\\partial \\mathcal{L}}{\\partial z^{(l)}_k} \\frac{\\partial z^{(l)}_k}{\\partial h^{(l-1)}_j}\n$$\n求和中的各项为：\n- $\\frac{\\partial \\mathcal{L}}{\\partial z^{(l)}_k} = \\frac{\\partial \\mathcal{L}}{\\partial h^{(l)}_k} \\frac{\\partial h^{(l)}_k}{\\partial z^{(l)}_k} = g^{(l)}_k \\phi'(z^{(l)}_k)$。\n- $\\frac{\\partial z^{(l)}_k}{\\partial h^{(l-1)}_j} = \\frac{\\partial}{\\partial h^{(l-1)}_j} \\sum_{m=1}^n W^{(l)}_{km} h^{(l-1)}_m = W^{(l)}_{kj}$。\n\n将这些代回，得到 $g^{(l-1)}$ 的一个分量的表达式：\n$$\ng^{(l-1)}_j = \\sum_{k=1}^n W^{(l)}_{kj} g^{(l)}_k \\phi'(z^{(l)}_k)\n$$\n我们需要找到这个向量的欧几里得范数平方的期望值，$\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right]$。\n$$\n\\|g^{(l-1)}\\|^2 = \\sum_{j=1}^n (g^{(l-1)}_j)^2 = \\sum_{j=1}^n \\left( \\sum_{k=1}^n W^{(l)}_{kj} g^{(l)}_k \\phi'(z^{(l)}_k) \\right)^2\n$$\n展开平方：\n$$\n\\|g^{(l-1)}\\|^2 = \\sum_{j=1}^n \\left( \\sum_{k=1}^n W^{(l)}_{kj} g^{(l)}_k \\phi'(z^{(l)}_k) \\right) \\left( \\sum_{m=1}^n W^{(l)}_{mj} g^{(l)}_m \\phi'(z^{(l)}_m) \\right) = \\sum_{j=1}^n \\sum_{k=1}^n \\sum_{m=1}^n W^{(l)}_{kj} W^{(l)}_{mj} g^{(l)}_k g^{(l)}_m \\phi'(z^{(l)}_k) \\phi'(z^{(l)}_m)\n$$\n现在，我们取期望。根据独立性假设（iv），积的期望等于期望的积。\n$$\n\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right] = \\sum_{j,k,m} \\mathbb{E}\\left[W^{(l)}_{kj} W^{(l)}_{mj}\\right] \\mathbb{E}\\left[g^{(l)}_k g^{(l)}_m \\phi'(z^{(l)}_k) \\phi'(z^{(l)}_m)\\right]\n$$\n基于 $g^{(l)}$ 和 $z^{(l)}$ 的独立性（这是由（iv）隐含的该上下文中的标准假设），第二个期望项也分离：\n$$\n\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right] = \\sum_{j,k,m} \\mathbb{E}\\left[W^{(l)}_{kj} W^{(l)}_{mj}\\right] \\mathbb{E}\\left[g^{(l)}_k g^{(l)}_m\\right] \\mathbb{E}\\left[\\phi'(z^{(l)}_k) \\phi'(z^{(l)}_m)\\right]\n$$\n我们使用给定的统计特性来计算每个期望项：\n1.  **权重**：$W^{(l)}_{ij}$ 是独立同分布的，均值为 0（i）。设 $\\operatorname{Var}(W^{(l)}_{ij}) = \\sigma_W^2$。\n    $\\mathbb{E}\\left[W^{(l)}_{kj} W^{(l)}_{mj}\\right] = \\mathbb{E}[W^{(l)}_{kj}] \\mathbb{E}[W^{(l)}_{mj}] = 0$，如果 $k \\neq m$。\n    $\\mathbb{E}\\left[(W^{(l)}_{kj})^2\\right] = \\operatorname{Var}(W^{(l)}_{kj}) + (\\mathbb{E}[W^{(l)}_{kj}])^2 = \\sigma_W^2 + 0 = \\sigma_W^2$，如果 $k = m$。\n    这可以写作 $\\mathbb{E}\\left[W^{(l)}_{kj} W^{(l)}_{mj}\\right] = \\sigma_W^2 \\delta_{km}$，其中 $\\delta_{km}$ 是克罗内克δ函数。\n\n2.  **梯度**：$g^{(l)}_k$ 是独立同分布的，均值为 0（iii）。设 $\\operatorname{Var}(g^{(l)}_k) = \\sigma_{g,l}^2$。\n    类似的论证给出 $\\mathbb{E}\\left[g^{(l)}_k g^{(l)}_m\\right] = \\sigma_{g,l}^2 \\delta_{km}$。\n\n来自权重的 $\\delta_{km}$ 项迫使对 $m$ 的求和收缩，令 $m=k$。\n$$\n\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right] = \\sum_{j=1}^n \\sum_{k=1}^n \\mathbb{E}\\left[(W^{(l)}_{kj})^2\\right] \\mathbb{E}\\left[(g^{(l)}_k)^2\\right] \\mathbb{E}\\left[(\\phi'(z^{(l)}_k))^2\\right]\n$$\n由于所有变量在其各自的索引上都是独立同分布的，期望值不依赖于 $j$ 或 $k$。\n- $\\mathbb{E}\\left[(W^{(l)}_{kj})^2\\right] = \\sigma_W^2 = \\operatorname{Var}(W^{(l)}_{ij})$。\n- $\\mathbb{E}\\left[(g^{(l)}_k)^2\\right] = \\sigma_{g,l}^2$。注意 $\\mathbb{E}\\left[\\|g^{(l)}\\|^2\\right] = \\sum_{k=1}^n \\mathbb{E}\\left[(g^{(l)}_k)^2\\right] = n\\sigma_{g,l}^2$。\n- $\\mathbb{E}\\left[(\\phi'(z^{(l)}_k))^2\\right]$ 对于该层是一个常数，我们称之为 $\\mathbb{E}\\left[(\\phi')^2\\right]$。\n\n将这些代入求和：\n$$\n\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right] = \\sum_{j=1}^n \\sum_{k=1}^n \\sigma_W^2 \\sigma_{g,l}^2 \\mathbb{E}\\left[(\\phi')^2\\right] = n^2 \\sigma_W^2 \\sigma_{g,l}^2 \\mathbb{E}\\left[(\\phi')^2\\right]\n$$\n我们代入 $\\sigma_{g,l}^2 = \\frac{1}{n}\\mathbb{E}\\left[\\|g^{(l)}\\|^2\\right]$：\n$$\n\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right] = n^2 \\sigma_W^2 \\left(\\frac{1}{n}\\mathbb{E}\\left[\\|g^{(l)}\\|^2\\right]\\right) \\mathbb{E}\\left[(\\phi')^2\\right] = \\left(n \\sigma_W^2 \\mathbb{E}\\left[(\\phi')^2\\right]\\right) \\mathbb{E}\\left[\\|g^{(l)}\\|^2\\right]\n$$\n这就得到了乘法因子 $C = \\frac{\\mathbb{E}\\left[\\|g^{(l-1)}\\|^2\\right]}{\\mathbb{E}\\left[\\|g^{(l)}\\|^2\\right]} = n \\operatorname{Var}(W^{(l)}_{ij}) \\mathbb{E}\\left[(\\phi')^2\\right]$。\n将此递推关系从 $l=L$ 向下应用到 $l=1$，共 $L$ 次：\n$$\n\\mathbb{E}\\left[\\|g^{(0)}\\|^2\\right] = C \\cdot \\mathbb{E}\\left[\\|g^{(1)}\\|^2\\right] = \\dots = C^L \\cdot \\mathbb{E}\\left[\\|g^{(L)}\\|^2\\right]\n$$\n所求的比率为 $R = \\frac{\\mathbb{E}\\left[\\|g^{(0)}\\|^{2}\\right]}{\\mathbb{E}\\left[\\|g^{(L)}\\|^{2}\\right]} = C^L = \\left(n \\operatorname{Var}(W^{(l)}_{ij}) \\mathbb{E}\\left[(\\phi')^2\\right]\\right)^L$。\n\n现在我们必须对这四种情况进行评估。首先，我们为每个激活函数求出 $\\mathbb{E}\\left[(\\phi')^2\\right]$。\n\n- **ReLU**：$\\phi(z) = \\max\\{0,z\\}$。其导数为 $\\phi'(z) = 1$（当 $z0$）和 $0$（当 $z0$）。因此 $(\\phi'(z))^2 = 1$（当 $z0$）和 $0$（当 $z0$）。激活前的值 $z^{(l)}$ 关于零对称（ii），所以 $P(z0)=1/2$。\n$$\n\\mathbb{E}\\left[(\\phi')^2\\right] = 1^2 \\cdot P(z0) + 0^2 \\cdot P(z0) = \\frac{1}{2}\n$$\n\n- **tanh**：$\\phi(z) = \\tanh(z)$。其导数为 $\\phi'(z) = 1 - \\tanh^2(z)$。在这种情况下，$z \\sim \\mathcal{N}(0,1)$（ii）。我们将期望表示为高斯积分：\n$$\n\\mathbb{E}\\left[(\\phi')^2\\right] = \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right] = \\int_{-\\infty}^{\\infty} (1-\\tanh^2(z))^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz\n$$\n\n我们现在可以为四种组合中的每一种计算 $R$。\n\n1.  **Xavier 与 ReLU**：$\\operatorname{Var}(W) = 1/n$，$\\mathbb{E}[(\\phi')^2] = 1/2$。\n    $C = n \\cdot \\frac{1}{n} \\cdot \\frac{1}{2} = \\frac{1}{2}$。\n    $R = \\left(\\frac{1}{2}\\right)^L$。\n\n2.  **He 与 ReLU**：$\\operatorname{Var}(W) = 2/n$，$\\mathbb{E}[(\\phi')^2] = 1/2$。\n    $C = n \\cdot \\frac{2}{n} \\cdot \\frac{1}{2} = 1$。\n    $R = (1)^L = 1$。\n\n3.  **Xavier 与 tanh**：$\\operatorname{Var}(W) = 1/n$，$\\mathbb{E}[(\\phi')^2] = \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]$。\n    $C = n \\cdot \\frac{1}{n} \\cdot \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right] = \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]$。\n    $R = \\left(\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]\\right)^L$。\n\n4.  **He 与 tanh**：$\\operatorname{Var}(W) = 2/n$，$\\mathbb{E}[(\\phi')^2] = \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]$。\n    $C = n \\cdot \\frac{2}{n} \\cdot \\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right] = 2\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]$。\n    $R = \\left(2\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]\\right)^L$。\n\n最终答案以行矩阵的形式呈现，按顺序包含这四个结果。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\left(\\frac{1}{2}\\right)^L  1  \\left(\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]\\right)^L  \\left(2\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}\\left[(1-\\tanh^2(Z))^2\\right]\\right)^L \\end{pmatrix}}\n$$", "id": "3125165"}, {"introduction": "He 初始化不仅是一个固定的公式，其核心是一种维持方差平衡的设计原则。这个高级练习挑战你将此原则应用于更复杂的现代网络模块中，例如包含 Dropout 和残差连接的模块。通过为一个结合了 ReLU、Dropout 和残差连接的模块推导定制的初始化方差，你将学会如何将基础理论推广到新颖的网络架构设计中，这对于前沿研究与开发至关重要。[@problem_id:3134404]", "problem": "考虑一个基于注意力的多层感知机（MLP）内部的残差块，它将一个输入向量 $x \\in \\mathbb{R}^{d}$ 映射到一个输出 $y \\in \\mathbb{R}^{d}$，其计算方式为\n$$\ny \\;=\\; x \\;+\\; \\text{Dropout}\\big(\\phi(Wx)\\big)\n$$\n其中 $\\phi(x)$ 表示逐元素应用的修正线性单元（ReLU）非线性函数，并且所使用的随机失活是标准的“反向”随机失活：对于每个坐标 $i$，$\\text{Dropout}(u)_i = \\frac{m_i}{p} u_i$，其中掩码 $m_i \\sim \\text{Bernoulli}(p)$ 是独立的，$p \\in (0,1]$ 是保留概率。假设：\n- $x$ 的分量是独立同分布的，均值为零，每个坐标的二阶矩为 $\\mathbb{E}[x_i^{2}] = v$，其中 $v  0$。\n- 权重矩阵 $W \\in \\mathbb{R}^{d \\times d}$ 的元素是独立的，均值为零，方差为 $\\mathbb{V}[W_{ij}] = \\sigma_w^{2}/n$，其中 $n=d$ 是扇入数。\n- 在平均场机制下，激活前的值 $z = Wx$ 的坐标近似为零均值的高斯分布，并且在初始化时被视为与 $x$ 独立。\n\n仅使用关于独立和、二阶矩以及类高斯输入的对称性的核心定义和经过充分检验的事实，为该残差块建立一个定制的 He 风格准则：选择 $\\sigma_w^{2}$ 使得 $y$ 中两个相加分量的逐坐标二阶矩相平衡，\n$$\n\\mathbb{E}[x_i^{2}] \\;=\\; \\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right].\n$$\n从第一性原理出发，推导满足上述平衡条件的、用 $p$ 表示的 $\\sigma_w^{2}$ 的闭式表达式。你的最终答案必须是单一的解析表达式。无需四舍五入，不涉及单位。最终的 $\\sigma_w^{2}$ 必须仅用 $p$ 的符号形式表示。", "solution": "我们从给定的块结构和假设出发。目标是对于每个坐标 $i$，在初始化时强制实现平衡\n$$\n\\mathbb{E}[x_i^{2}] \\;=\\; \\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right]\n$$\n。根据假设，$\\mathbb{E}[x_i^{2}] = v$。我们必须计算等式右侧，用 $\\sigma_w^{2}$ 和 $p$ 来表示。\n\n步骤1：计算激活前值的二阶矩。令 $z = Wx$ 并将其第 $i$ 个坐标表示为 $z_i = \\sum_{j=1}^{n} W_{ij} x_j$。利用独立性、零均值以及独立项之和的二阶矩等于二阶矩之和这一事实，我们有\n$$\n\\mathbb{E}[z_i^{2}] \\;=\\; \\sum_{j=1}^{n} \\mathbb{E}[W_{ij}^{2}]\\,\\mathbb{E}[x_j^{2}]\n\\;=\\; \\sum_{j=1}^{n} \\Big(\\frac{\\sigma_w^{2}}{n}\\Big)\\, v\n\\;=\\; \\sigma_w^{2}\\, v.\n$$\n这里使用了扇入数 $n$ 以及每个权重的方差为 $\\sigma_w^{2}/n$。\n\n步骤2：通过修正线性单元（ReLU）。定义 $\\phi(z_i) = \\max(0, z_i)$。对于零均值对称输入分布（如我们对 $z_i$ 的假设），通过ReLU后的二阶矩遵循一个对称性质。具体来说，对于任何零均值对称随机变量 $Z$，指示函数 $1_{\\{Z0\\}}$ 恰好选择了其一半的概率质量，并且由于 $Z^{2}$ 是一个偶函数，正半部分对 $\\mathbb{E}[Z^{2}]$ 的贡献恰好是一半：\n$$\n\\mathbb{E}[(\\max(0, Z))^{2}] \\;=\\; \\mathbb{E}[Z^{2}\\,1_{\\{Z0\\}}] \\;=\\; \\frac{1}{2}\\,\\mathbb{E}[Z^{2}].\n$$\n将此应用于 $z_i$，我们得到\n$$\n\\mathbb{E}[\\phi(z_i)^{2}] \\;=\\; \\frac{1}{2}\\,\\mathbb{E}[z_i^{2}] \\;=\\; \\frac{1}{2}\\,\\sigma_w^{2}\\, v.\n$$\n\n步骤3：应用反向随机失活。对于反向随机失活，每个坐标 $i$ 的计算如下：\n$$\n\\text{Dropout}(\\phi(z))_i \\;=\\; \\frac{m_i}{p}\\,\\phi(z_i),\n$$\n其中 $m_i \\sim \\text{Bernoulli}(p)$ 独立于 $\\phi(z_i)$ 以及其他所有变量。其二阶矩为\n$$\n\\mathbb{E}\\!\\left[\\big(\\tfrac{m_i}{p}\\,\\phi(z_i)\\big)^{2}\\right]\n\\;=\\; \\mathbb{E}\\!\\left[\\big(\\tfrac{m_i}{p}\\big)^{2}\\right]\\;\\mathbb{E}[\\phi(z_i)^{2}]\n\\;=\\; \\frac{\\mathbb{E}[m_i]}{p^{2}}\\;\\mathbb{E}[\\phi(z_i)^{2}]\n\\;=\\; \\frac{p}{p^{2}}\\;\\mathbb{E}[\\phi(z_i)^{2}]\n\\;=\\; \\frac{1}{p}\\,\\mathbb{E}[\\phi(z_i)^{2}],\n$$\n因为对于伯努利掩码有 $m_i^{2} = m_i$ 且 $\\mathbb{E}[m_i] = p$。代入步骤2的结果，得到\n$$\n\\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right]\n\\;=\\; \\frac{1}{p}\\cdot \\frac{1}{2}\\,\\sigma_w^{2}\\, v\n\\;=\\; \\frac{\\sigma_w^{2}}{2p}\\, v.\n$$\n\n步骤4：强制执行平衡准则。问题所要求的条件是\n$$\n\\mathbb{E}[x_i^{2}] \\;=\\; \\mathbb{E}\\!\\left[\\big(\\text{Dropout}(\\phi(Wx))_i\\big)^{2}\\right].\n$$\n代入 $\\mathbb{E}[x_i^{2}] = v$ 和上面的表达式，我们得到\n$$\nv \\;=\\; \\frac{\\sigma_w^{2}}{2p}\\, v.\n$$\n因为 $v0$，我们可以在等式两边同除以 $v$ 来分离出 $\\sigma_w^{2}$：\n$$\n1 \\;=\\; \\frac{\\sigma_w^{2}}{2p}\n\\quad\\Longrightarrow\\quad\n\\sigma_w^{2} \\;=\\; 2p.\n$$\n\n因此，为了在初始化时平衡跳跃连接路径和ReLU-随机失活残差路径的逐坐标二阶矩，应选择权重方差缩放因子，使得 $W$ 的每个元素具有方差 $\\mathbb{V}[W_{ij}] = \\frac{2p}{n}$，即在此处使用的扇入参数化中，$\\sigma_w^{2} = 2p$。", "answer": "$$\\boxed{2p}$$", "id": "3134404"}]}