{"hands_on_practices": [{"introduction": "深度神经网络的一个核心挑战是确保信号在层间传播时既不爆炸也不消失。本练习将通过经验性验证来揭示现代权重初始化策略的基石：如何为特定的激活函数（如$ \\tanh $和$ \\mathrm{ReLU} $）选择合适的权重方差，以在网络初始化时维持信号强度的稳定。通过这个动手实践，你将直观地理解为何Xavier初始化与$ \\tanh $匹配，而He初始化与$ \\mathrm{ReLU} $匹配 [@problem_id:3199598]。", "problem": "给定一个包含 $L$ 层的全连接前馈网络，定义第 $l$ 层的预激活值为 $z^{(l)} = W^{(l)} a^{(l-1)}$，激活值为 $a^{(l)} = \\phi\\!\\left(z^{(l)}\\right)$，其中 $a^{(0)} = x$。假设偏置为零，权重独立同分布，输入 $x \\in \\mathbb{R}^{n}$ 的分量是独立的，具有零均值和有限方差。考虑两种广泛使用的随机权重初始化策略：Xavier (Glorot) 正态初始化和 He (Kaiming) 正态初始化，以及两种激活函数：$\\tanh$ 和修正线性单元 ($\\mathrm{ReLU}$)。目标是通过蒙特卡洛模拟，在给定激活函数下，经验性地验证何种初始化策略能近似保持各层预激活值的方差不变，即对于所有层 $l \\in \\{1,\\dots,L\\}$，都有 $\\operatorname{Var}\\!\\left(z^{(l)}\\right) \\approx \\operatorname{Var}(x)$。\n\n使用的基本原理：\n- 初始化时权重和激活在各个坐标上的独立性，以及独立变量和的方差线性性质。\n- $\\tanh$ 和 $\\mathrm{ReLU}$ 作为逐点非线性函数的定义。\n- 对于一个数组 $Y \\in \\mathbb{R}^{s \\times d}$，沿 $s$ 个样本定义的样本方差估计量为 $\\widehat{\\operatorname{Var}}(Y) = \\frac{1}{d}\\sum_{j=1}^{d} \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}^{2} - \\left(\\frac{1}{s}\\sum_{i=1}^{s} Y_{ij}\\right)^{2} \\right)$。\n\n您的程序必须：\n1. 构建具有指定 $L$ 和层宽度的网络，其中为简化起见，每层的形状为 $(n_{\\text{in}}, n_{\\text{out}})$ 且 $n_{\\text{in}} = n_{\\text{out}}$。对每一层的权重 $W^{(l)}$ 使用 Xavier 正态初始化或 He 正态初始化。使用每种策略规定的方差进行零均值正态初始化；不添加任何偏置。\n2. 将输入 $x$ 抽取为 $s$ 个维度为 $n$ 的独立样本，每个分量服从 $\\mathcal{N}(0,1)$ 分布，即均值为零，方差为 1。\n3. 对每一层 $l$，通过对每个坐标的样本方差求平均，计算 $s$ 个样本上的经验预激活方差 $\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right)$，并类似地计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。定义第 $l$ 层的相对偏差为 $$\\delta^{(l)} = \\frac{\\left|\\widehat{\\operatorname{Var}}\\!\\left(z^{(l)}\\right) - \\widehat{\\operatorname{Var}}(x)\\right|}{\\widehat{\\operatorname{Var}}(x)}.$$ 如果 $\\max_{1 \\le l \\le L} \\delta^{(l)} \\le \\varepsilon$（容差 $\\varepsilon = 0.25$），则认为该测试用例保持了方差。\n4. 使用固定种子 $12345$ 的伪随机数生成器以确保可复现性。\n\n测试套件：\n- 用例 1：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 用例 2：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 He 正态。\n- 用例 3：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 用例 4：$L=5$，宽度 $n=32$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 Xavier 正态。\n- 用例 5：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 用例 6：$L=1$，宽度 $n=32$，样本数 $s=20000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n- 用例 7：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\tanh$，初始化 Xavier 正态。\n- 用例 8：$L=15$，宽度 $n=16$，样本数 $s=8000$，激活函数 $\\mathrm{ReLU}$，初始化 He 正态。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如 $\\left[\\text{result}_1,\\text{result}_2,\\dots\\right]$），其中每个 $\\text{result}_i$ 是一个布尔值，指示第 $i$ 个测试用例的方差是否得以保持，其顺序与上述测试套件的顺序完全一致。", "solution": "该问题经评估是有效的。它在科学上基于深度学习的既定原则，特别是关于权重初始化及其对信号传播的影响。问题定义良好，提供了所有必要的参数、定义以及一个清晰、客观的成功标准。它没有矛盾、歧义和事实错误。因此，我们可以着手提供解决方案。\n\n目标是经验性地验证深度神经网络中预激活值 $z^{(l)}$ 的方差在各层之间得以保持的条件。该分析的核心在于连续层中预激活值方差之间的递归关系。\n\n让我们考虑第 $l$ 层单个神经元的预激活值：\n$$ z_i^{(l)} = \\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)} $$\n在这里，$W_{ij}^{(l)}$ 是连接第 $l-1$ 层神经元 $j$ 与第 $l$ 层神经元 $i$ 的权重，$a_j^{(l-1)}$ 是来自前一层的神经元 $j$ 的激活值。问题规定偏置为零，权重 $W_{ij}^{(l)}$ 从零均值分布中抽取，并且输入分量 $x_j$（构成 $a_j^{(0)}$）也具有零均值。我们假设在初始化时，对于所有 $j$，激活值 $a_j^{(l-1)}$ 都与权重 $W_{ij}^{(l)}$ 无关且服从同一分布。此外，如果激活值 $a^{(l-1)}$ 是对称激活函数应用于零均值输入 $z^{(l-1)}$ 的输出，那么它们也将具有零均值。对于 $\\mathrm{ReLU}$ 激活函数，情况并非如此，但由于权重本身是零均值的，得到的预激活值 $z^{(l)}$ 仍将具有零均值：$\\mathbb{E}[z_i^{(l)}] = \\sum_j \\mathbb{E}[W_{ij}^{(l)}] \\mathbb{E}[a_j^{(l-1)}] = \\sum_j 0 \\cdot \\mathbb{E}[a_j^{(l-1)}] = 0$。\n\n在这些条件下，$z_i^{(l)}$ 的方差由下式给出：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\operatorname{Var}\\left(\\sum_{j=1}^{n_{l-1}} W_{ij}^{(l)} a_j^{(l-1)}\\right) $$\n由于和中各项是独立的（因为权重和前一层的激活是独立的），和的方差等于各项方差的和：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)} a_j^{(l-1)}) $$\n对于两个独立的随机变量 $U$ 和 $V$，其中至少一个均值为零（例如 $\\mathbb{E}[U]=0$），则 $\\operatorname{Var}(UV) = \\mathbb{E}[U^2V^2] - (\\mathbb{E}[UV])^2 = \\mathbb{E}[U^2]\\mathbb{E}[V^2] - (\\mathbb{E}[U]\\mathbb{E}[V])^2 = \\operatorname{Var}(U)\\operatorname{Var}(V)$。由于 $\\mathbb{E}[W_{ij}^{(l)}] = 0$，我们有：\n$$ \\operatorname{Var}(z_i^{(l)}) = \\sum_{j=1}^{n_{l-1}} \\operatorname{Var}(W_{ij}^{(l)}) \\operatorname{Var}(a_j^{(l-1)}) $$\n假设第 $l$ 层的所有权重都是从方差为 $\\operatorname{Var}(W^{(l)})$ 的分布中独立同分布地抽取的，并且来自第 $l-1$ 层的所有激活值也是从方差为 $\\operatorname{Var}(a^{(l-1)})$ 的分布中独立同分布地抽取的，那么上式可以简化为：\n$$ \\operatorname{Var}(z^{(l)}) = n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(a^{(l-1)}) $$\n其中 $n_{l-1}$ 是第 $l-1$ 层的神经元数量。为了保持稳定的信号传播，我们需要保持方差不变，即 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$。这需要谨慎选择权重初始化方差 $\\operatorname{Var}(W^{(l)})$，以抵消激活函数 $\\phi$ 对方差的影响，该影响由项 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\phi(z^{(l-1)}))$ 所体现。\n\n**激活函数分析**\n\n1.  **$\\tanh$ 激活函数：** 双曲正切函数 $\\tanh(z)$ 关于原点对称（$\\tanh(0)=0$），并且对于小输入其行为类似于恒等函数（当 $z \\approx 0$ 时，$\\tanh(z) \\approx z$）。如果我们假设预激活值 $z^{(l-1)}$ 集中在零附近（这是初始训练阶段的理想状态），那么 $\\operatorname{Var}(a^{(l-1)}) = \\operatorname{Var}(\\tanh(z^{(l-1)})) \\approx \\operatorname{Var}(z^{(l-1)})$。将此代入我们的传播方程可得：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\operatorname{Var}(z^{(l-1)}) $$\n    为了实现 $\\operatorname{Var}(z^{(l)}) \\approx \\operatorname{Var}(z^{(l-1)})$，我们必须设置 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。\n    **Xavier (Glorot) 正态初始化**正是为这种情况设计的。它将从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的权重 $W^{(l)}$ 的方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1} + n_l} $$\n    在我们的特定问题中，$n_{l-1} = n_l = n$，所以这变成 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{2n} = \\frac{1}{n} = \\frac{1}{n_{l-1}}$。这个选择完美地满足了条件 $n_{l-1} \\operatorname{Var}(W^{(l)}) = 1$。因此，对于 $\\tanh$ 激活函数，Xavier 初始化预期会保持方差不变。\n\n2.  **$\\mathrm{ReLU}$ 激活函数：** 修正线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$ 不是对称的。对于一个零均值、对称的 $z^{(l-1)}$ 输入分布（如高斯分布），恰好一半的输入将被设置为零。这会影响方差。设 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$。输出 $a = \\mathrm{ReLU}(z)$ 的方差是 $\\operatorname{Var}(a) = \\mathbb{E}[a^2] - (\\mathbb{E}[a])^2$。\n    激活值平方的期望是 $\\mathbb{E}[a^2] = \\mathbb{E}[\\max(0, z)^2] = \\int_0^\\infty z^2 p(z) dz$。由于正态分布 $p(z)$ 的对称性，这个积分是 $z^2$ 的总积分的一半：$\\mathbb{E}[a^2] = \\frac{1}{2} \\int_{-\\infty}^\\infty z^2 p(z) dz = \\frac{1}{2}\\mathbb{E}[z^2] = \\frac{1}{2}\\operatorname{Var}(z)$。\n    因此，对于 $\\mathrm{ReLU}$，我们有 $\\operatorname{Var}(a^{(l-1)}) \\approx \\frac{1}{2}\\operatorname{Var}(z^{(l-1)})$。方差传播方程变为：\n    $$ \\operatorname{Var}(z^{(l)}) \\approx n_{l-1} \\operatorname{Var}(W^{(l)}) \\left(\\frac{1}{2}\\operatorname{Var}(z^{(l-1)})\\right) $$\n    为了保持方差，我们必须有 $n_{l-1} \\operatorname{Var}(W^{(l)}) \\frac{1}{2} = 1$，这意味着 $\\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}}$。\n    **He (Kaiming) 正态初始化**就是为这种情况设计的。它将方差设置为：\n    $$ \\operatorname{Var}(W^{(l)}) = \\frac{2}{n_{l-1}} $$\n    这个选择恰好满足条件。因此，对于 $\\mathrm{ReLU}$ 激活函数，He 初始化预期会保持方差不变。不匹配的组合（例如，$\\tanh$ 与 He，$\\mathrm{ReLU}$ 与 Xavier）预计将分别导致方差爆炸或消失。\n\n**模拟过程**\n\n程序将为 8 个测试用例中的每一个实现蒙特卡洛模拟。对于每个用例：\n1.  使用值 $12345$ 为伪随机数生成器设置种子，以确保可复现性。\n2.  生成一个输入数据矩阵 $x \\in \\mathbb{R}^{s \\times n}$，其中每个元素都从 $\\mathcal{N}(0, 1)$ 中抽取。使用提供的公式计算经验输入方差 $\\widehat{\\operatorname{Var}}(x)$。\n3.  从 $l=1$ 到 $L$ 逐层处理网络。在每一层中，权重矩阵 $W^{(l)}$ 从一个零均值正态分布中初始化，其方差由指定的策略（Xavier 或 He）决定。\n4.  计算预激活值 $z^{(l)} = a^{(l-1)} W^{(l)}$。\n5.  计算经验方差 $\\widehat{\\operatorname{Var}}(z^{(l)})$。计算相对偏差 $\\delta^{(l)} = |\\widehat{\\operatorname{Var}}(z^{(l)}) - \\widehat{\\operatorname{Var}}(x)| / \\widehat{\\operatorname{Var}}(x)$。\n6.  追踪所有层中的最大相对偏差 $\\max_{1 \\le l \\le L} \\delta^{(l)}$。\n7.  计算激活值 $a^{(l)} = \\phi(z^{(l)})$，以用作下一层的输入。\n8.  遍历所有层后，如果 $\\max_{l} \\delta^{(l)} \\le \\varepsilon$（其中 $\\varepsilon = 0.25$），则认为该测试用例保持了方差。此检查将为每个用例产生一个布尔结果，然后报告该结果。", "answer": "```python\nimport numpy as np\n\ndef tanh(x):\n    \"\"\"Hyperbolic tangent activation function.\"\"\"\n    return np.tanh(x)\n\ndef relu(x):\n    \"\"\"Rectified Linear Unit (ReLU) activation function.\"\"\"\n    return np.maximum(0, x)\n\ndef empirical_variance(Y):\n    \"\"\"\n    Computes the empirical variance as defined in the problem statement.\n    For an array Y of shape (s, d), this is the average of the biased sample \n    variances of each of the d columns.\n    \n    Formula: Var(Y) = (1/d) * sum_j [ (1/s) * sum_i(Y_ij^2) - ((1/s) * sum_i(Y_ij))^2 ]\n    This is equivalent to the mean of np.var(Y, axis=0).\n    \"\"\"\n    if Y.ndim == 1:\n        # If input is a 1D array, treat it as (s, 1)\n        return np.var(Y)\n    return np.mean(np.var(Y, axis=0))\n\ndef run_simulation(L, n, s, activation_func, init_strategy, seed, epsilon):\n    \"\"\"\n    Runs a single simulation for a given network configuration.\n\n    Args:\n        L (int): Number of layers.\n        n (int): Width of each layer.\n        s (int): Number of samples.\n        activation_func (callable): The activation function to use.\n        init_strategy (str): The weight initialization strategy ('xavier' or 'he').\n        seed (int): The seed for the random number generator.\n        epsilon (float): The tolerance for variance preservation.\n\n    Returns:\n        bool: True if variance is preserved, False otherwise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate input data x of shape (s, n) from N(0, 1)\n    x = rng.normal(loc=0.0, scale=1.0, size=(s, n))\n    var_x = empirical_variance(x)\n    \n    a_prev = x\n    max_deviation = 0.0\n    \n    # Propagate through the network for L layers\n    for l in range(1, L + 1):\n        # Layer dimensions are (n_in, n_out)\n        n_in = n\n        n_out = n\n        \n        # Determine standard deviation for weight initialization\n        if init_strategy == 'xavier':\n            # Variance = 2 / (n_in + n_out)\n            std_dev = np.sqrt(2.0 / (n_in + n_out))\n        elif init_strategy == 'he':\n            # Variance = 2 / n_in\n            std_dev = np.sqrt(2.0 / n_in)\n        else:\n            raise ValueError(f\"Unknown initialization strategy: {init_strategy}\")\n            \n        # Initialize weights W of shape (n_in, n_out)\n        W = rng.normal(loc=0.0, scale=std_dev, size=(n_in, n_out))\n        \n        # Compute pre-activations z = a_prev @ W of shape (s, n_out)\n        z = a_prev @ W\n        \n        # Compute empirical variance and relative deviation\n        var_z = empirical_variance(z)\n        if var_x > 1e-9: # Avoid division by zero\n            deviation = np.abs(var_z - var_x) / var_x\n        else:\n            deviation = np.abs(var_z)\n\n        if deviation > max_deviation:\n            max_deviation = deviation\n            \n        # Compute post-activations for the next layer\n        a_prev = activation_func(z)\n        \n    # Check if the maximum deviation is within the tolerance\n    return max_deviation = epsilon\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases as (L, n, s, activation_name, init_strategy_name)\n    test_cases = [\n        (5, 32, 8000, 'tanh', 'xavier'),\n        (5, 32, 8000, 'tanh', 'he'),\n        (5, 32, 8000, 'relu', 'he'),\n        (5, 32, 8000, 'relu', 'xavier'),\n        (1, 32, 20000, 'tanh', 'xavier'),\n        (1, 32, 20000, 'relu', 'he'),\n        (15, 16, 8000, 'tanh', 'xavier'),\n        (15, 16, 8000, 'relu', 'he'),\n    ]\n    \n    # Map string names to functions and parameters\n    activation_map = {'tanh': tanh, 'relu': relu}\n    epsilon = 0.25\n    seed = 12345\n    \n    results = []\n    \n    # Run simulation for each test case\n    for case in test_cases:\n        L, n, s, act_name, init_name = case\n        activation_func = activation_map[act_name]\n        \n        # Each test case is an independent experiment, so we use the same seed,\n        # creating a new RNG instance for each to ensure reproducibility.\n        result = run_simulation(L, n, s, activation_func, init_name, seed, epsilon)\n        results.append(result)\n        \n    # Format and print the final output as a single-line list of booleans\n    # e.g., [True,False,True,False,True,True,True,True]\n    # np.bool_ maps to Python's True/False, and str() correctly converts them.\n    # The problem asks for booleans, and Python's `str(True)` is 'True'.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n```", "id": "3199598"}, {"introduction": "在掌握了网络内部的信号传播原理后，我们必须将目光投向数据本身。当输入特征的尺度（即方差）差异巨大时，“一刀切”的初始化策略可能并非最优。本练习将通过精确的光谱分析方法，而不是模拟迭代，来量化一种适应性初始化策略对收敛速度的显著提升，从而揭示了根据数据特性调整初始化方案的重要性 [@problem_id:3199538]。", "problem": "您将设计并分析一个用于线性模型的第一层权重初始化策略。该线性模型在具有异构尺度输入特征的情况下，通过批量梯度下降法对一个二次平滑目标进行训练。从均方误差（MSE）的定义、线性预测和批量梯度下降更新等基本概念出发。具体来说，考虑一个带有参数 $\\mathbf{w} \\in \\mathbb{R}^d$、数据矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ 和目标 $\\mathbf{y} \\in \\mathbb{R}^N$ 的线性模型，训练该模型以最小化 MSE。请使用纯粹的数学和算法规范，不涉及任何物理单位。\n\n您的任务是构建一个具有特征尺度异构性的可复现合成数据集，定义两种不同的第一层初始化策略，并在相同学习率下比较它们的收敛速度。此比较必须使用从二次目标上批量梯度下降的谱特性中获得的确切步数，而不是通过逐步模拟更新来完成。\n\n用作基础的定义和约束：\n- 均方误差 (MSE)：最小化 $L(\\mathbf{w}) = \\frac{1}{2N}\\lVert \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\rVert_2^2$。\n- 固定步长 $\\eta$ 的批量梯度下降：迭代 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla L(\\mathbf{w}_t)$，其中 $\\nabla L(\\mathbf{w}) = \\frac{1}{N}\\mathbf{X}^\\top(\\mathbf{X}\\mathbf{w}-\\mathbf{y})$。\n- 为保证可复现性，您必须使用下面指定的精确种子，通过伪随机数生成器来生成数据和初始权重。\n\n每个测试用例的数据集构建：\n- 令 $N$ 为样本数，$d$ 为特征数。\n- 令 $\\mathbf{s} \\in \\mathbb{R}^d$ 表示正的特征尺度。抽取一个标准化的矩阵 $\\mathbf{Z} \\in \\mathbb{R}^{N \\times d}$，其条目是独立的标准正态分布，并设置 $\\mathbf{X} = \\mathbf{Z} \\operatorname{diag}(\\mathbf{s})$，即第 $j$ 列的经验尺度近似为 $s_j$。\n- 使用零目标 $\\mathbf{y} = \\mathbf{0}$（等价于真实参数 $\\mathbf{w}_\\star = \\mathbf{0}$），以隔离特征尺度和初始化对收敛速度的影响。\n\n第一层（即 $\\mathbf{w}_0$）的初始化策略：\n- 标准均匀缩放（表示为 “uniform”）：抽取 $\\tilde{\\mathbf{u}} \\in \\mathbb{R}^d$，其条目独立地从一个对称均匀分布中采样，分布宽度根据单输出线性层的标准扇入/扇出方案选择。然后设置 $\\mathbf{w}_0^{\\text{uni}} = \\tilde{\\mathbf{u}}$。在两种策略中使用相同的 $\\tilde{\\mathbf{u}}$，以隔离缩放变换的影响。\n- 按特征缩放的初始化（表示为 “scaled”）：从完全相同的 $\\tilde{\\mathbf{u}}$ 出发，通过相应输入特征的经验标准差来重新缩放每个分量，即将分量 $j$ 除以 $\\mathbf{X}$ 第 $j$ 列的经验标准差，得到 $\\mathbf{w}_0^{\\text{sc}}$。\n\n需精确计算的收敛速度度量：\n- 令 $\\mathbf{H} = \\frac{1}{N}\\mathbf{X}^\\top \\mathbf{X}$ 为 $L(\\mathbf{w})$ 的 Hessian 矩阵。选择学习率 $\\eta = \\frac{1}{L}$，其中 $L$ 是 $\\mathbf{H}$ 的最大特征值。\n- 定义初始误差 $\\mathbf{e}_0 = \\mathbf{w}_0 - \\mathbf{w}_\\star$。由于 $\\mathbf{w}_\\star = \\mathbf{0}$，因此 $\\mathbf{e}_0 = \\mathbf{w}_0$。\n- 仅使用线性代数（不迭代模拟参数更新），计算满足 $L(\\mathbf{w}_t) \\le \\varepsilon$ 的确切最小非负整数 $t$，其中 $\\varepsilon$ 是一个预设的容差。您可以使用 $\\mathbf{H}$ 的谱分解将 $L(\\mathbf{w}_t)$ 表示为 $t$ 的函数，然后确定满足该不等式的最小 $t$。\n\n学习率和容差：\n- 对于每个测试用例，计算学习率 $\\eta = 1/L$，其中 $L$ 是该测试用例中 $\\mathbf{H}$ 的最大特征值。\n- 使用每个测试用例中提供的损失容差 $\\varepsilon$。\n\n测试套件：\n对于下面的每个测试用例，按上述规定构建 $\\mathbf{X}$ 和 $\\mathbf{y}$，使用给定的种子从标准均匀缩放方案（扇入为 $d$，扇出为 1）中抽取原始初始化向量 $\\tilde{\\mathbf{u}}$，然后从同一个 $\\tilde{\\mathbf{u}}$ 推导出两种初始化。计算每种初始化达到 $L(\\mathbf{w}_t) \\le \\varepsilon$ 所需的梯度下降确切步数，并输出步数之差，定义为 $\\Delta = t_{\\text{uni}} - t_{\\text{sc}}$。\n\n- 用例 1：$N = 128$，$d = 5$，$\\mathbf{s} = [1,1,1,1,1]$，种子 $= 1$，$\\varepsilon = 10^{-4}$。\n- 用例 2：$N = 128$，$d = 5$，$\\mathbf{s} = [1,10,10,1,5]$，种子 $= 2$，$\\varepsilon = 10^{-4}$。\n- 用例 3：$N = 256$，$d = 10$，$\\mathbf{s} = [1,3,9,27,9,3,1,27,9,3]$，种子 $= 3$，$\\varepsilon = 10^{-4}$。\n- 用例 4：$N = 64$，$d = 8$，$\\mathbf{s} = [0.5,2,4,8,16,1,0.5,16]$，种子 $= 4$，$\\varepsilon = 10^{-4}$。\n\n要求的最终输出：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。第 $k$ 个条目必须等于第 $k$ 个用例的 $\\Delta$，顺序与上文一致。例如，一个包含四个整数的输出必须看起来像 $[r_1,r_2,r_3,r_4]$。\n- 每个结果必须是整数。如果由于数值原因，某个用例无法达到容差，则返回一个您为两种初始化一致选择的较大整数上限（但使用指定的学习率和方法，此处的用例应该是良态且收敛的）。", "solution": "该问题要求设计并分析两种权重初始化策略，用于一个在线性模型上通过批量梯度下降法训练的均方误差（MSE）目标。分析必须是精确的，利用 Hessian 矩阵的谱特性，而不是通过迭代模拟梯度更新。问题的核心是比较标准均匀初始化与所提出的按特征缩放的初始化的收敛速度，尤其是在输入特征具有异构尺度的情况下。\n\n首先，我们将问题中的各个组成部分形式化。\n\n模型是一个线性预测器 $\\hat{\\mathbf{y}} = \\mathbf{Xw}$，其中 $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ 是数据矩阵，$\\mathbf{w} \\in \\mathbb{R}^d$ 是权重向量，$N$ 是样本数，$d$ 是特征数。目标是最小化 MSE 损失。鉴于目标向量是 $\\mathbf{y} = \\mathbf{0}$，损失函数为：\n$$L(\\mathbf{w}) = \\frac{1}{2N} \\lVert \\mathbf{Xw} - \\mathbf{0} \\rVert_2^2 = \\frac{1}{2N}(\\mathbf{Xw})^\\top(\\mathbf{Xw}) = \\frac{1}{2}\\mathbf{w}^\\top \\left(\\frac{1}{N}\\mathbf{X}^\\top\\mathbf{X}\\right) \\mathbf{w}$$\n这是一个二次型。我们确定损失函数的 Hessian 矩阵，它相对于 $\\mathbf{w}$ 是一个常数：\n$$\\mathbf{H} = \\nabla^2 L(\\mathbf{w}) = \\frac{1}{N}\\mathbf{X}^\\top\\mathbf{X}$$\n因此，损失可以紧凑地写为 $L(\\mathbf{w}) = \\frac{1}{2}\\mathbf{w}^\\top\\mathbf{Hw}$。损失的梯度是 $\\nabla L(\\mathbf{w}) = \\mathbf{Hw}$。最小化损失的最优权重向量显然是 $\\mathbf{w}_\\star = \\mathbf{0}$，因为 $\\mathbf{H}$ 是半正定的，并且对于指定的用例（$N  d$），它几乎必然是正定的。\n\n优化算法是具有固定学习率 $\\eta$ 的批量梯度下降：\n$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta \\nabla L(\\mathbf{w}_t) = \\mathbf{w}_t - \\eta \\mathbf{Hw}_t = (\\mathbf{I} - \\eta \\mathbf{H})\\mathbf{w}_t$$\n这是一个线性递推关系。第 $t$ 步的权重向量可以表示为初始权重向量 $\\mathbf{w}_0$ 的函数：\n$$\\mathbf{w}_t = (\\mathbf{I} - \\eta \\mathbf{H})^t \\mathbf{w}_0$$\n\n为了分析损失 $L(\\mathbf{w}_t)$ 的收敛性，我们使用 Hessian 矩阵 $\\mathbf{H}$ 的谱分解。由于 $\\mathbf{H}$ 是一个实对称矩阵，它可以被正交对角化：$\\mathbf{H} = \\mathbf{Q\\Lambda Q}^\\top$，其中 $\\mathbf{Q}$ 是一个正交矩阵（$\\mathbf{Q}\\mathbf{Q}^\\top = \\mathbf{I}$），其列是 $\\mathbf{H}$ 的特征向量，$\\mathbf{\\Lambda}$ 是一个包含相应实数特征值 $\\lambda_1, \\lambda_2, \\ldots, \\lambda_d$ 的对角矩阵。我们按非递减顺序对它们进行排序：$0 \\le \\lambda_1 \\le \\dots \\le \\lambda_d$。学习率设置为 $\\eta = 1/L$，其中 $L = \\lambda_{\\max}(\\mathbf{H}) = \\lambda_d$。\n\n第 $t$ 步的损失是 $L(\\mathbf{w}_t) = \\frac{1}{2}\\mathbf{w}_t^\\top \\mathbf{Hw}_t$。代入 $\\mathbf{w}_t$ 和 $\\mathbf{H}$ 的表达式：\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\left((\\mathbf{I} - \\eta \\mathbf{H})^t \\mathbf{w}_0\\right)^\\top \\mathbf{H} \\left((\\mathbf{I} - \\eta \\mathbf{H})^t \\mathbf{w}_0\\right)$$\n由于 $\\mathbf{H}$ 和 $(\\mathbf{I} - \\eta\\mathbf{H})$ 可交换，我们可以写成：\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\mathbf{w}_0^\\top (\\mathbf{I} - \\eta \\mathbf{H})^{2t} \\mathbf{H} \\mathbf{w}_0$$\n使用谱分解，$\\mathbf{I} - \\eta \\mathbf{H} = \\mathbf{Q}(\\mathbf{I} - \\eta\\mathbf{\\Lambda})\\mathbf{Q}^\\top$。这得到：\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\mathbf{w}_0^\\top \\mathbf{Q} (\\mathbf{I} - \\eta\\mathbf{\\Lambda})^{2t} \\mathbf{\\Lambda} \\mathbf{Q}^\\top \\mathbf{w}_0$$\n令 $\\tilde{\\mathbf{w}}_0 = \\mathbf{Q}^\\top\\mathbf{w}_0$ 为初始权重在特征向量基上的投影。损失变成了关于各个特征模式的求和：\n$$L(\\mathbf{w}_t) = \\frac{1}{2} \\tilde{\\mathbf{w}}_0^\\top (\\mathbf{I} - \\eta\\mathbf{\\Lambda})^{2t} \\mathbf{\\Lambda} \\tilde{\\mathbf{w}}_0 = \\frac{1}{2} \\sum_{i=1}^{d} \\tilde{w}_{0,i}^2 \\lambda_i (1-\\eta\\lambda_i)^{2t}$$\n其中 $\\tilde{w}_{0,i}$ 是 $\\tilde{\\mathbf{w}}_0$ 的第 $i$ 个分量。\n\n任务是找到使 $L(\\mathbf{w}_t) \\le \\varepsilon$ 成立的最小非负整数 $t$。右侧是关于 $t$ 的单调递减函数，因为对于所有特征值 $\\lambda_i$，$0 \\le (1-\\eta\\lambda_i) \\le 1$。$t$ 的闭式解是难以处理的。但是，我们可以从 $t=0$ 开始，为连续的整数 $t$ 计算 $L(\\mathbf{w}_t)$，并在条件满足时停止。这可以在不模拟权重更新的情况下提供确切的整数 $t$。\n\n两种初始化策略是：\n1.  **标准均匀缩放（“uniform”）：** 根据标准实践（例如，Glorot/Xavier 均匀初始化），对于一个扇入为 $d_{in}$、扇出为 $d_{out}$ 的线性层，权重从 $U[-a, a]$ 中抽取，其中 $a = \\sqrt{6 / (d_{in} + d_{out})}$。此处，$d_{in}=d$ 且 $d_{out}=1$。因此，原始初始化向量 $\\tilde{\\mathbf{u}}$ 的分量从 $U\\left[-\\sqrt{\\frac{6}{d+1}}, \\sqrt{\\frac{6}{d+1}}\\right]$ 中抽取。对于此策略，$\\mathbf{w}_0^{\\text{uni}} = \\tilde{\\mathbf{u}}$。\n2.  **按特征缩放的初始化（“scaled”）：** 此策略根据输入特征的尺度调整初始化。令 $\\hat{\\sigma}_j$ 为 $\\mathbf{X}$ 第 $j$ 列的经验标准差。初始权重向量构造为 $(\\mathbf{w}_0^{\\text{sc}})_j = \\tilde{u}_j / \\hat{\\sigma}_j$，使用与均匀情况相同的 $\\tilde{\\mathbf{u}}$。这对初始权重进行预处理，旨在平衡不同尺度特征的影响。具有较大标准差 $\\hat{\\sigma}_j$ 的特征会获得较小的初始权重，反之亦然。\n\n解决每个测试用例的算法如下：\n1.  使用指定的参数（$N, d, \\mathbf{s}$）和随机种子生成数据矩阵 $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$。\n2.  计算 Hessian 矩阵 $\\mathbf{H} = \\frac{1}{N}\\mathbf{X}^\\top\\mathbf{X}$。\n3.  对 $\\mathbf{H}$ 进行特征分解，找到其特征值 $\\{\\lambda_i\\}_{i=1}^d$ 和特征向量矩阵 $\\mathbf{Q}$。\n4.  确定学习率 $\\eta = 1/\\lambda_d$，其中 $\\lambda_d$ 是最大特征值。\n5.  使用指定的种子生成原始初始化向量 $\\tilde{\\mathbf{u}} \\in \\mathbb{R}^d$。\n6.  构造两个初始权重向量：$\\mathbf{w}_0^{\\text{uni}}$ 和 $\\mathbf{w}_0^{\\text{sc}}$。\n7.  对于每个初始化 $\\mathbf{w}_0$：\n    a. 将 $\\mathbf{w}_0$ 投影到特征基上：$\\tilde{\\mathbf{w}}_0 = \\mathbf{Q}^\\top\\mathbf{w}_0$。\n    b. 定义系数 $c_i = \\frac{1}{2}\\tilde{w}_{0,i}^2 \\lambda_i$ 和底数 $b_i = (1-\\eta\\lambda_i)^2$。\n    c. 计算损失函数 $L(t) = \\sum_{i=1}^d c_i (b_i)^t$。\n    d. 迭代 $t=0, 1, 2, \\ldots$ 并找到使 $L(t) \\le \\varepsilon$ 的最小 $t$。这给出 $t_{\\text{uni}}$ 和 $t_{\\text{sc}}$。\n8.  计算差值 $\\Delta = t_{\\text{uni}} - t_{\\text{sc}}$。\n\n此过程将应用于每个测试用例，以找到所需的收敛步数差异。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the convergence speed of two different\n    weight initialization strategies for a linear model.\n    \"\"\"\n\n    test_cases = [\n        {'N': 128, 'd': 5, 's': [1, 1, 1, 1, 1], 'seed': 1, 'epsilon': 1e-4},\n        {'N': 128, 'd': 5, 's': [1, 10, 10, 1, 5], 'seed': 2, 'epsilon': 1e-4},\n        {'N': 256, 'd': 10, 's': [1, 3, 9, 27, 9, 3, 1, 27, 9, 3], 'seed': 3, 'epsilon': 1e-4},\n        {'N': 64, 'd': 8, 's': [0.5, 2, 4, 8, 16, 1, 0.5, 16], 'seed': 4, 'epsilon': 1e-4},\n    ]\n\n    results = []\n    for case in test_cases:\n        delta = calculate_step_difference(\n            N=case['N'],\n            d=case['d'],\n            s=np.array(case['s']),\n            seed=case['seed'],\n            epsilon=case['epsilon']\n        )\n        results.append(delta)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_steps_to_convergence(w0, Q, eigenvalues, L, epsilon):\n    \"\"\"\n    Calculates the exact number of steps t to reach the loss tolerance.\n\n    Args:\n        w0 (np.ndarray): The initial weight vector.\n        Q (np.ndarray): The matrix of eigenvectors of the Hessian.\n        eigenvalues (np.ndarray): The eigenvalues of the Hessian.\n        L (float): The largest eigenvalue of the Hessian (Lipschitz constant).\n        epsilon (float): The loss tolerance.\n\n    Returns:\n        int: The minimum number of steps t.\n    \"\"\"\n    # Project initial weights onto the eigenvector basis\n    w0_tilde = Q.T @ w0\n\n    # Pre-compute coefficients and bases for the loss expression\n    # L(t) = 1/2 * sum_{i} (w0_tilde_i^2 * lambda_i * (1 - lambda_i/L)^(2t))\n    # L(t) = sum_{i} coeffs_i * (bases_i)^t\n    coeffs = 0.5 * (w0_tilde**2) * eigenvalues\n    bases = (1 - eigenvalues / L)**2\n\n    # Handle the case t=0\n    loss_at_0 = np.sum(coeffs)\n    if loss_at_0 = epsilon:\n        return 0\n\n    t = 0\n    loss = loss_at_0\n    \n    # Iteratively evaluate the loss L(t) for t = 1, 2, ...\n    # This is an exact calculation of the loss at step t, not a simulation.\n    while loss > epsilon:\n        t += 1\n        loss = np.sum(coeffs * (bases**t))\n        # Add a safeguard for potential infinite loops, though not expected\n        # for these well-posed problems.\n        if t > 1_000_000:\n             # Returning a large number as per problem notes if tolerance not reached\n            return 1_000_000\n    \n    return t\n\ndef calculate_step_difference(N, d, s, seed, epsilon):\n    \"\"\"\n    Performs the full analysis for a single test case.\n\n    Args:\n        N (int): Number of samples.\n        d (int): Number of features.\n        s (np.ndarray): Feature scales.\n        seed (int): Seed for the pseudorandom number generator.\n        epsilon (float): Loss tolerance.\n\n    Returns:\n        int: The difference in steps, t_uni - t_sc.\n    \"\"\"\n    # 1. Reproducible dataset construction\n    rng = np.random.default_rng(seed)\n    Z = rng.standard_normal(size=(N, d))\n    X = Z @ np.diag(s)\n\n    # 2. Hessian and its spectral decomposition\n    H = (1/N) * X.T @ X\n    eigenvalues, Q = np.linalg.eigh(H)\n    \n    # 3. Learning rate\n    L = eigenvalues[-1]  # np.linalg.eigh returns eigenvalues in ascending order\n    # Ensure L is not zero to avoid division by zero, though highly unlikely for N>d\n    if L == 0:\n        # This case should not be reached with the given problem setup\n        # as H is positive definite almost surely.\n        raise ValueError(\"Largest eigenvalue must be positive.\")\n    eta = 1.0 / L\n\n    # 4. Initialization strategies\n    # Raw initialization vector based on Glorot/Xavier uniform scheme\n    limit = np.sqrt(6 / (d + 1))\n    u_tilde = rng.uniform(-limit, limit, size=d)\n\n    # Strategy 1: Standard uniform scaling\n    w0_uni = u_tilde\n\n    # Strategy 2: Per-feature scaled initialization\n    empirical_std = np.std(X, axis=0)\n    # Avoid division by zero for features with no variance\n    empirical_std[empirical_std == 0] = 1.0 \n    w0_sc = u_tilde / empirical_std\n\n    # 5. Compute steps to convergence for both initializations\n    t_uni = get_steps_to_convergence(w0_uni, Q, eigenvalues, L, epsilon)\n    t_sc = get_steps_to_convergence(w0_sc, Q, eigenvalues, L, epsilon)\n\n    # 6. Return the difference\n    return t_uni - t_sc\n\nsolve()\n```", "id": "3199538"}, {"introduction": "一个优秀的初始化策略不仅能确保梯度稳定，更能塑造特征学习的整个过程。这个实践练习将建立起初始权重方差、神经元激活的初始稀疏度以及网络在第一步训练后所学特征的“有效维度”之间的具体联系。通过这个实验，你将深入理解初始化选择如何超越简单的数值稳定性问题，直接影响到底层表征的丰富性和质量 [@problem_id:3199561]。", "problem": "您需要设计并实现一个计算协议，用于测量在不同权重方差下，修正线性单元（ReLU）层在初始化时的神经元激活稀疏度，并将此稀疏度与训练第一个周期后特征的有效维度关联起来。该协议必须实例化为一个完整的可运行程序，该程序需创建一个合成数据集，初始化一个单隐藏层网络，为不同的方差超参数测量初始化稀疏度，在均方误差目标上执行一次全批量梯度下降更新，然后量化那次单次更新后获得的隐藏表示的有效维度。程序必须输出一行内容，其中包含按下面指定的确切格式聚合的所有测试用例的结果。\n\n推导和协议设计的基本依据：\n- 修正线性单元（ReLU）非线性由函数 $ \\phi(z) = \\max(0,z) $ 定义，并逐元素应用。\n- 如果 $ z $ 是一个零均值、对称的实值随机变量（例如，高斯分布 $ \\mathcal{N}(0,\\sigma_z^2) $），那么 $ \\mathbb{P}(z  0) = \\mathbb{P}(z \\le 0) = \\tfrac{1}{2} $。\n- 对于具有独立同分布（i.i.d.）条目的独立输入 $ x \\in \\mathbb{R}^d $ 和其条目独立、零均值且对称的权重 $ w \\in \\mathbb{R}^d $，根据中心极限定理，预激活 $ z = w^\\top x $ 近似对称且零均值，其方差由权重方差控制。\n- 特征矩阵 $ H \\in \\mathbb{R}^{N \\times m} $ 的有效维度可以通过稳定秩来量化，其定义为 $ \\mathrm{sr}(H) = \\lVert H \\rVert_F^2 / \\lVert H \\rVert_2^2 $，其中 $ \\lVert \\cdot \\rVert_F $ 是弗罗贝尼乌斯范数，$ \\lVert \\cdot \\rVert_2 $ 是谱范数（最大奇异值）。该度量对 $ H $ 的全局缩放不变，并被广泛用于捕捉奇异值的分布情况，从而指示有效维度。\n\n需实现的协议：\n1. 生成一个合成数据集 $ X \\in \\mathbb{R}^{N \\times d} $，其条目为独立同分布的标准正态分布，以及一个回归目标 $ y \\in \\mathbb{R}^N $，定义为 $ y = X u $，其中 $ u \\in \\mathbb{R}^d $ 的条目为独立同分布的标准正态分布。使用固定的随机种子以确保可复现性。\n2. 考虑一个具有 $ m $ 个隐藏单元的单隐藏层网络，其参数为 $ W \\in \\mathbb{R}^{m \\times d} $ 和 $ v \\in \\mathbb{R}^m $，输出为 $ \\hat{y} = \\phi(X W^\\top) v $。将偏置初始化为 $ 0 $。\n3. 对于每个测试用例的权重方差参数 $ \\sigma^2 $：\n   - 用独立同分布的条目 $ \\sim \\mathcal{N}\\!\\left(0, \\frac{\\sigma^2}{d}\\right) $ 初始化 $ W $，并用独立同分布的条目 $ \\sim \\mathcal{N}\\!\\left(0, \\frac{1}{m}\\right) $ 初始化 $ v $。这种缩放方式使得预激活方差近似由 $ \\sigma^2 $ 控制。\n   - 计算初始化时的隐藏激活 $ H = \\phi(X W^\\top) $。\n   - 测量初始化稀疏度，即 $ H $ 中零元素的比例，记为 $ s = \\frac{1}{N m} \\sum_{i=1}^N \\sum_{j=1}^m \\mathbf{1}\\{ H_{ij} = 0 \\} $。\n   - 对均方误差（MSE）目标 $ L = \\frac{1}{N} \\lVert \\hat{y} - y \\rVert_2^2 $ 执行一步学习率为 $ \\alpha  0 $ 的全批量梯度下降，以获得更新后的参数 $ W' $ 和 $ v' $。\n   - 计算更新后的隐藏激活 $ H' = \\phi(X (W')^\\top) $。通过减去列均值来中心化 $ H' $，得到 $ \\tilde{H}' $，并计算稳定秩 $ r = \\lVert \\tilde{H}' \\rVert_F^2 / \\lVert \\tilde{H}' \\rVert_2^2 $ 作为第一个周期后的有效维度。\n   - 记录数据对 $ [s, r] $。\n4. 聚合所有测试用例的结果，并以指定的精确格式将其打印在单行上。\n\n具体参数值和测试套件：\n- 使用 $ N = 512 $ 个样本，$ d = 128 $ 的输入维度，$ m = 256 $ 个隐藏单元，以及学习率 $ \\alpha = 0.01 $。\n- 使用以下权重方差测试套件 $ \\sigma^2 $: $ [0.01, 0.5, 2.0, 5.0, 10.0] $。该集合覆盖了小方差、中等方差（包括与修正线性单元（ReLU）的 He 初始化相当的值，其预激活使用 $ \\sigma^2 \\approx 2 $）以及大方差的情况。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，本身是一个双浮点数列表 $ [s, r] $，其中 $ s $ 是初始化稀疏度，$ r $ 是一个周期后的稳定秩。例如，输出必须是 $ [[s_1,r_1],[s_2,r_2],\\dots] $ 的形式，不含任何额外文本。\n\n本问题不涉及物理单位或角度。所有报告的量都必须是实数。程序必须是自包含的，仅使用指定的库，并且不得要求任何输入。", "solution": "该问题要求设计并实现一个计算实验，以研究单隐藏层修正线性单元（ReLU）网络中初始权重的方差、初始化时产生的激活稀疏度以及单步梯度下降后隐藏层表示的有效维度之间的关系。\n\n该解决方案首先阐述协议中每个组件的理论基础，然后进行计算实现。\n\n**理论框架**\n\n1.  **数据集和网络模型**：我们构建一个合成回归问题。输入数据矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 包含 $N$ 个维度为 $d$ 的样本，其条目独立地从标准正态分布中抽取，$X_{ij} \\sim \\mathcal{N}(0, 1)$。目标向量 $y \\in \\mathbb{R}^N$ 通过输入的线性组合生成，$y = X u$，其中真实权重向量 $u \\in \\mathbb{R}^d$ 的条目也从 $\\mathcal{N}(0, 1)$ 中抽取。这种设置提供了一个简单、可控的环境。\n\n    网络模型是一个具有 $m$ 个隐藏单元且无偏置项的单隐藏层感知器。输出由 $\\hat{y} = \\phi(X W^\\top) v$ 给出，其中 $W \\in \\mathbb{R}^{m \\times d}$ 是输入到隐藏层的权重矩阵，$v \\in \\mathbb{R}^m$ 是隐藏层到输出层的权重向量，$\\phi(z) = \\max(0, z)$ 是逐元素应用的 ReLU 激活函数。\n\n2.  **权重初始化和激活稀疏度**：权重 $W$ 的条目通过从零均值正态分布 $W_{jk} \\sim \\mathcal{N}(0, \\sigma^2/d)$ 中独立同分布（i.i.d.）地抽取来初始化。缩放因子 $1/d$ 至关重要。考虑样本 $i$ 和神经元 $j$ 的预激活 $z_{ij}$：$z_{ij} = \\sum_{k=1}^d W_{jk} X_{ik}$。由于 $W_{jk}$ 和 $X_{ik}$ 都是独立的零均值随机变量，它们乘积的期望为 $\\mathbb{E}[W_{jk}X_{ik}] = \\mathbb{E}[W_{jk}]\\mathbb{E}[X_{ik}] = 0$，因此 $\\mathbb{E}[z_{ij}] = 0$。\n\n    方差由下式给出：\n    $$ \\mathrm{Var}(z_{ij}) = \\sum_{k=1}^d \\mathrm{Var}(W_{jk} X_{ik}) = \\sum_{k=1}^d \\mathbb{E}[(W_{jk} X_{ik})^2] - (\\mathbb{E}[W_{jk}X_{ik}])^2 $$\n    $$ \\mathrm{Var}(z_{ij}) = \\sum_{k=1}^d \\mathbb{E}[W_{jk}^2] \\mathbb{E}[X_{ik}^2] = \\sum_{k=1}^d \\mathrm{Var}(W_{jk}) \\mathrm{Var}(X_{ik}) = \\sum_{k=1}^d \\left(\\frac{\\sigma^2}{d}\\right)(1) = d \\cdot \\frac{\\sigma^2}{d} = \\sigma^2 $$\n    根据中心极限定理，对于大的 $d$，预激活 $z_{ij}$ 近似服从正态分布，$z_{ij} \\approx \\mathcal{N}(0, \\sigma^2)$。如果 ReLU 神经元的预激活为非正数，它就会变得“非激活”或“死亡”。由于 $\\mathcal{N}(0, \\sigma^2)$ 是一个以零为中心的对称分布，发生这种情况的概率为 $\\mathbb{P}(z_{ij} \\le 0) = 0.5$。因此，后激活矩阵 $H = \\phi(X W^\\top)$ 中零条目的期望比例，即初始化稀疏度 $s$，理论上为 $0.5$，与方差参数 $\\sigma^2$ 无关。本实验将通过经验测量该值。\n\n3.  **梯度下降更新**：对均方误差（MSE）损失函数 $L = \\frac{1}{N} \\lVert \\hat{y} - y \\rVert_2^2$ 执行单次全批量梯度下降步骤。需要计算损失函数关于参数 $W$ 和 $v$ 的梯度。令 $e = \\hat{y} - y$ 为误差向量，$Z = XW^\\top$ 为预激活矩阵。使用链式法则推导梯度：\n\n    -   对于输出层权重 $v$：\n        $$ \\frac{\\partial L}{\\partial v} = \\frac{2}{N} H^\\top e $$\n    -   对于输入层权重 $W$：\n        $$ \\frac{\\partial L}{\\partial W} = \\left( \\left( \\frac{2}{N} e v^\\top \\right) \\odot \\phi'(Z) \\right)^\\top X $$\n        其中 $\\odot$ 表示哈达玛（逐元素）积，$\\phi'(Z)$ 是 ReLU 函数的逐元素导数，它是一个指示函数矩阵 $\\mathbf{1}_{Z_{ij}0}$。\n\n    使用指定的学习率 $\\alpha$，参数更新为 $W' = W - \\alpha \\frac{\\partial L}{\\partial W}$ 和 $v' = v - \\alpha \\frac{\\partial L}{\\partial v}$。\n\n4.  **有效维度**：更新后，计算新的隐藏激活 $H' = \\phi(X(W')^\\top)$。为了评估这些激活向量所张成的特征空间的维度，我们使用稳定秩。首先，通过减去每列（特征）的均值来中心化激活矩阵，得到 $\\tilde{H}'$。然后计算稳定秩 $r$：\n    $$ r = \\mathrm{sr}(\\tilde{H}') = \\frac{\\lVert \\tilde{H}' \\rVert_F^2}{\\lVert \\tilde{H}' \\rVert_2^2} = \\frac{\\sum_{i=1}^{\\min(N, m)} \\lambda_i}{\\lambda_{\\max}} $$\n    其中 $\\lVert \\cdot \\rVert_F$ 是弗罗贝尼乌斯范数，$\\lVert \\cdot \\rVert_2$ 是谱范数（最大奇异值），$\\{\\lambda_i\\}$ 是协方差矩阵 $\\tilde{H}'^\\top \\tilde{H}'$ 的特征值（它们是 $\\tilde{H}'$ 的奇异值的平方）。稳定秩衡量了矩阵的能量在其奇异值之间分布的均匀程度。一个接近矩阵真实秩的值表示一个条件良好的特征空间，其奇异值分布均匀；而一个接近 $1$ 的值则表明矩阵的能量集中在一个主导方向上。\n\n整个协议系统地改变初始权重方差 $\\sigma^2$，以观察其通过梯度更新机制对学习到的特征表示结构（由稳定秩量化）的影响。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the computational protocol to measure neuron activation sparsity\n    at initialization and effective dimensionality after one training epoch.\n    \"\"\"\n    # 4. Concrete parameter values and test suite\n    N = 512  # number of samples\n    d = 128  # input dimension\n    m = 256  # number of hidden units\n    alpha = 0.01  # learning rate\n    sigma_squared_values = [0.01, 0.5, 2.0, 5.0, 10.0]\n\n    # Use a fixed random seed for reproducibility of the entire experiment.\n    rng = np.random.default_rng(seed=42)\n\n    # 1. Generate a single synthetic dataset for all test cases.\n    X = rng.standard_normal(size=(N, d))\n    u = rng.standard_normal(size=(d,))\n    y = X @ u\n\n    # Utility function for the ReLU activation.\n    def relu(z):\n        return np.maximum(0, z)\n\n    # Utility function for the derivative of the ReLU activation.\n    def relu_prime(z):\n        return (z > 0).astype(z.dtype)\n\n    results = []\n    # 3. Loop through each test case for the weight variance parameter.\n    for sigma_sq in sigma_squared_values:\n        # Initialize network parameters W and v for the current test case.\n        # The initialization must be inside the loop to be fresh for each sigma_sq.\n        w_std = np.sqrt(sigma_sq / d)\n        v_std = np.sqrt(1 / m)\n        W = rng.normal(loc=0.0, scale=w_std, size=(m, d))\n        v = rng.normal(loc=0.0, scale=v_std, size=(m,))\n\n        # Compute pre-activations and activations at initialization.\n        Z = X @ W.T\n        H = relu(Z)\n\n        # Measure initialization sparsity (s).\n        s = np.mean(H == 0)\n\n        # Perform one full-batch gradient descent step.\n        # Compute the prediction and the error.\n        y_hat = H @ v\n        e = y_hat - y\n\n        # Compute gradients.\n        grad_v = (2 / N) * (H.T @ e)\n        \n        grad_Z_elementwise = (2 / N) * np.outer(e, v)\n        grad_Z = grad_Z_elementwise * relu_prime(Z)\n        grad_W = grad_Z.T @ X\n\n        # Update parameters to get W' and v'.\n        W_prime = W - alpha * grad_W\n        # v_prime is not used for the final calculation but is part of the update.\n        # v_prime = v - alpha * grad_v\n\n        # Compute updated hidden activations H' with the new weights W'.\n        H_prime = relu(X @ W_prime.T)\n\n        # Center H' by subtracting column means to get tilde(H').\n        H_prime_centered = H_prime - H_prime.mean(axis=0)\n\n        # Compute the Frobenius norm squared of the centered activations.\n        fro_norm_sq = np.sum(H_prime_centered**2)\n\n        # To compute the spectral norm squared, we find the largest eigenvalue\n        # of the covariance matrix H_tilde.T @ H_tilde. This is more stable\n        # and efficient than a full SVD for this purpose.\n        # The matrix is symmetric and positive semi-definite.\n        covariance_matrix = H_prime_centered.T @ H_prime_centered\n        eigenvalues = np.linalg.eigvalsh(covariance_matrix)\n        spec_norm_sq = np.max(eigenvalues)\n\n        # Compute the stable rank (r), handling the case of a zero matrix.\n        if spec_norm_sq > 1e-12:\n            r = fro_norm_sq / spec_norm_sq\n        else:\n            # If the largest eigenvalue is zero, all are zero. This implies\n            # the matrix is zero, so its effective dimension is zero.\n            r = 0.0\n\n        # Record the pair [s, r].\n        results.append([s, r])\n    \n    # 4. Aggregate and print results in the specified format.\n    # The map(str, ...) converts each inner list [s, r] into its string\n    # representation, e.g., \"[0.5, 128.3]\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3199561"}]}