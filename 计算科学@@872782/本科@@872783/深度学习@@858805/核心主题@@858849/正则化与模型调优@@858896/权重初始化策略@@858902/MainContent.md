## 引言
在构建和训练[深度神经网络](@entry_id:636170)的宏伟工程中，[权重初始化](@entry_id:636952)扮演着一个看似微小却至关重要的奠基角色。它是在训练开始前为网络参数赋予初始值的过程，一个恰当的起点是模型能否快速收敛、达到理想性能，甚至成功启动训练的关键。许多初学者常常忽视这一环节，或简单地采用默认设置，却未意识到不当的初始化正是导致训练停滞、梯度消失或爆炸等常见问题的根源。本文旨在填补这一认知空白，系统性地揭示[权重初始化](@entry_id:636952)背后的科学与艺术。

为此，我们将分三步深入探索。在“原理与机制”一章中，我们将从根本上剖析为何需要精心的初始化，并推导出现代[深度学习](@entry_id:142022)中如Xavier和He等核心初始化策略的数学原理。接着，在“应用与跨学科联系”一章中，我们将展示这些原理如何应用于Transformer等前沿架构，并探讨其与正则化、优化等其他环节的相互作用，甚至将其思想延伸至强化学习、物理学等更广阔的领域。最后，“动手实践”部分将提供一系列精心设计的实验，让你亲手验证理论，并直观感受不同初始化策略对模型行为的深刻影响。

## 原理与机制

在[神经网](@entry_id:276355)络的训练过程中，[权重初始化](@entry_id:636952)是一个基础但至关重要的步骤。它虽然看似简单，即在训练开始前为网络参数赋上初始值，但其选择对模型的[收敛速度](@entry_id:636873)、泛化能力乃至能否成功训练具有决定性的影响。本章将深入探讨[权重初始化](@entry_id:636952)的核心原理与关键机制，从解释为何需要审慎的初始化策略开始，逐步推导出现代深度学习中广泛应用的各类初始化方法，并探讨其背后的数学依据。

### 不良初始化的危害：对称性失效与梯度问题

在深入研究“如何”初始化之前，我们必须首先理解“为何”需要它。不当的初始化主要会导致两个严重的问题：网络对称性失效和梯度消失/爆炸。

#### 对称性失效与随机初始化的必要性

[神经网](@entry_id:276355)络的强大能力来源于其众多神经元能够学习到数据中丰富且多样的特征。然而，如果一个层内的所有神经元被赋予完全相同的初始权重，它们将永远无法摆脱这种对称性。

考虑一个具有两个隐藏神经元的简单网络。如果它们的权重向量 $w_1$ 和 $w_2$ 以及偏置被初始化为完全相同的值，那么对于任何输入样本，这两个神经元将产生完全相同的输出。更关键的是，在[反向传播](@entry_id:199535)过程中，根据链式法则计算出的关于它们权重的梯度也将完全相同。例如，如果损失函数为 $\mathcal{L}$，则在任何训练步骤中，我们都会有 $\nabla_{w_1} \mathcal{L} = \nabla_{w_2} \mathcal{L}$。因此，当使用[梯度下降法](@entry_id:637322)更新权重时，两个权重向量将获得完全相同的更新量，导致它们在整个训练过程中始终保持一致。

这种现象被称为**对称性失效** (symmetry breaking failure)。在这种情况下，同一层内的多个神经元等效于一个单一的、更“强”的神经元，这极大地限制了模型的[表示能力](@entry_id:636759)，使其无法学习到层次化的复杂特征 [@problem_id:3199505]。为了避免这种情况，我们必须引入随机性，通过为每个权重赋予一个从某个[分布](@entry_id:182848)中独立采样的随机初始值来**打破对称性** (break the symmetry)，从而确保不同的神经元从一开始就走上不同的学习轨迹。

#### 梯度消失与[梯度爆炸](@entry_id:635825)

在深度神经网络中，梯度通过[链式法则](@entry_id:190743)从输出层[反向传播](@entry_id:199535)到输入层。每一层权重的微小不当都可能在多层传播中被放大，导致梯度信号呈指数级衰减（**梯度消失**, vanishing gradients）或增长（**[梯度爆炸](@entry_id:635825)**, exploding gradients）。梯度消失会使靠近输入层的网络层几乎不被更新，从而无法学习；而[梯度爆炸](@entry_id:635825)则会导致更新步长过大，使训练过程不稳定甚至发散。[权重初始化](@entry_id:636952)的核心目标之一，就是通过精巧的设计来维持梯度信号在网络中传播时的稳定性。

### [方差保持](@entry_id:634352)原则

解决[梯度消失与爆炸](@entry_id:634312)问题的核心思想是**[方差保持](@entry_id:634352)** (variance preservation)。这个原则主张，一个设计良好的初始化方案应确保在[前向传播](@entry_id:193086)过程中，激活值的[方差](@entry_id:200758)在逐层传递时保持大致恒定；同时，在[反向传播](@entry_id:199535)过程中，梯度的[方差](@entry_id:200758)也应保持稳定。如果每一层的输出[方差](@entry_id:200758)都与输入[方差](@entry_id:200758)相近，信号就不会在深层网络中消失或爆炸。

#### [前向传播](@entry_id:193086)：线性网络的简单情况

为了直观地理解[方差保持](@entry_id:634352)原则，我们首先分析一个不包含[非线性激活函数](@entry_id:635291)的深度线性网络 [@problem_id:3199493]。考虑网络中的第 $l$ 层，其操作可以表示为 $h^{(l)} = W^{(l)} h^{(l-1)}$，其中 $h^{(l-1)} \in \mathbb{R}^{d_{l-1}}$ 是上一层的输出（或网络的输入），$W^{(l)} \in \mathbb{R}^{d_l \times d_{l-1}}$ 是权重矩阵。

我们做出以下基本假设：
1.  权重矩阵 $W^{(l)}$ 的元素 $W^{(l)}_{ij}$ 独立同分布，其均值为零 $\mathbb{E}[W^{(l)}_{ij}] = 0$，[方差](@entry_id:200758)为 $\operatorname{Var}(W^{(l)}_{ij}) = \sigma_l^2$。
2.  输入 $h^{(l-1)}$ 的各个分量无关，均值为零，且[方差](@entry_id:200758)相同，记为 $\operatorname{Var}(h^{(l-1)}_j) = q_{l-1}$。
3.  $W^{(l)}$ 与 $h^{(l-1)}$ 相互独立。

$h^{(l)}$ 的第 $i$ 个分量的计算公式为 $h^{(l)}_i = \sum_{j=1}^{d_{l-1}} W^{(l)}_{ij} h^{(l-1)}_j$。由于所有变量的均值都为零，所以 $h^{(l)}_i$ 的均值也为零。因此，其[方差](@entry_id:200758)等于其二阶矩：
$$
\operatorname{Var}(h^{(l)}_i) = \mathbb{E}[(h^{(l)}_i)^2] = \mathbb{E}\left[\left(\sum_{j=1}^{d_{l-1}} W^{(l)}_{ij} h^{(l-1)}_j\right)^2\right]
$$
由于各项的独立性和[零均值性质](@entry_id:636100)，[交叉](@entry_id:147634)项的期望为零，因此[方差](@entry_id:200758)的和等于和的[方差](@entry_id:200758)：
$$
\operatorname{Var}(h^{(l)}_i) = \sum_{j=1}^{d_{l-1}} \mathbb{E}[(W^{(l)}_{ij} h^{(l-1)}_j)^2] = \sum_{j=1}^{d_{l-1}} \mathbb{E}[(W^{(l)}_{ij})^2] \mathbb{E}[(h^{(l-1)}_j)^2]
$$
代入[方差](@entry_id:200758)的定义，我们得到：
$$
\operatorname{Var}(h^{(l)}_i) = \sum_{j=1}^{d_{l-1}} \sigma_l^2 q_{l-1} = d_{l-1} \sigma_l^2 q_{l-1}
$$
这里，$d_{l-1}$ 是第 $l$ 层的输入维度，通常被称为**输入扇区** (fan-in)。令 $n_{in} = d_{l-1}$，则输出[方差](@entry_id:200758) $q_l$ 和输入[方差](@entry_id:200758) $q_{l-1}$ 之间的关系为：
$$
q_l = n_{in} \sigma_l^2 q_{l-1}
$$
为了保持[方差](@entry_id:200758)恒定，即 $q_l = q_{l-1}$，我们需要满足条件 $n_{in} \sigma_l^2 = 1$。这导出了一个关键的初始化规则：
$$
\sigma_l^2 = \operatorname{Var}(W^{(l)}_{ij}) = \frac{1}{n_{in}}
$$
这个简单的结论构成了现代[权重初始化](@entry_id:636952)策略的基石。它告诉我们，权重的[方差](@entry_id:200758)应该与该层输入的神经元数量成反比。

### 针对[非线性](@entry_id:637147)网络的初始化策略

实际的[神经网](@entry_id:276355)络包含[非线性激活函数](@entry_id:635291)，这使得[方差](@entry_id:200758)传播的分析变得更加复杂。然而，上述基本原则仍然适用，只需根据激活函数的具体性质进行调整。

#### Xavier (Glorot) 初始化

Xavier Glorot 和 Yoshua Bengio 在2010年的开创性工作中，将[方差保持](@entry_id:634352)原则应用于包含对称[激活函数](@entry_id:141784)（如 $\tanh$）的网络。他们分析发现，为了稳定[前向传播](@entry_id:193086)（激活值[方差](@entry_id:200758)），需要满足 $\operatorname{Var}(W) = 1/n_{in}$。然而，当分析[反向传播](@entry_id:199535)时，梯度的[方差](@entry_id:200758)要保持稳定，则需要满足 $\operatorname{Var}(W) = 1/n_{out}$，其中 $n_{out}$ 是层的输出维度或**输出扇区** (fan-out) [@problem_id:3199585]。

这两个条件通常是矛盾的（除非 $n_{in} = n_{out}$）。为了在这两者之间取得平衡，Glorot 和 Bengio 提出了一个折衷方案，即著名的 **Xavier 初始化**（或称 Glorot 初始化）：
$$
\operatorname{Var}(W) = \frac{2}{n_{in} + n_{out}}
$$
对于从均值为零的[均匀分布](@entry_id:194597)中采样，即 $W \sim U[-b, b]$，其[方差](@entry_id:200758)为 $b^2/3$。因此，边界 $b$ 应设置为 $b = \sqrt{\frac{6}{n_{in} + n_{out}}}$。在实践中，使用更简单的 $1/n_{in}$ 规则也十分常见，因为它直接稳定了[前向传播](@entry_id:193086)的信号。

#### He 初始化

Xavier 初始化在对称激活函数上表现良好，但当与非对称的[修正线性单元](@entry_id:636721)（ReLU）一起使用时，效果并不理想。ReLU 的定义为 $\phi(z) = \max(0, z)$，它会将所有负输入置为零。

假设一个神经元的预激活值 $z$ 来自一个关于零对称的[分布](@entry_id:182848)（例如高斯分布），那么大约有一半的输入会被 ReLU 函数置零。这会导致激活值的[方差](@entry_id:200758)减半。我们可以通过一个简化的模型来精确描述这一点 [@problem_id:3134394]。如果预激活值 $z$ 中只有比例为 $\pi$ 的值为正，那么激活值的二阶矩（在零均值下即[方差](@entry_id:200758)）为 $\mathbb{E}[\phi(z)^2] = \pi \mathbb{E}[z^2]$。对于对称[分布](@entry_id:182848)，$\pi = 1/2$，因此输出[方差](@entry_id:200758)变为输入[方差](@entry_id:200758)的一半。

为了补偿 ReLU 导致的[方差](@entry_id:200758)减半，我们需要将预激活值的[方差](@entry_id:200758)加倍。回顾我们的[方差](@entry_id:200758)传播公式 $q_l = n_{in} \sigma_l^2 q_{l-1}$，这里的 $q_l$ 指的是预激活值的[方差](@entry_id:200758)。为了让激活后的[方差保持](@entry_id:634352)不变，我们需要预激活值的[方差](@entry_id:200758)是之前的两倍。更直接地，为了让激活后的[方差](@entry_id:200758) $\frac{1}{2} \operatorname{Var}(z)$ 等于上一层的[方差](@entry_id:200758) $\operatorname{Var}(h^{(l-1)})$，我们需要 $\frac{1}{2} (n_{in} \sigma_w^2) = 1$。这导出了 Kaiming He 等人提出的 **He 初始化** 规则 [@problem_id:3125165]：
$$
\operatorname{Var}(W) = \frac{2}{n_{in}}
$$
通过这个简单的修正，He 初始化确保了信号[方差](@entry_id:200758)在通过 ReLU 网络时能够保持稳定，极大地改善了深度 ReLU 网络的训练性能。

### 高级与专业化策略

除了基于统计[方差保持](@entry_id:634352)的标准方法外，还存在其他精巧的初始化策略，它们从不同的角度来确保信号传播的稳定性。

#### 正交初始化

与依赖于[大数定律](@entry_id:140915)的统计方法不同，**正交初始化** (orthogonal initialization) 采用了一种确定性的几何方法。其核心思想是使用**[正交矩阵](@entry_id:169220)** (orthogonal matrix) 来初始化权重。一个[正交矩阵](@entry_id:169220) $Q$ 的定义是 $Q^T Q = I$，它具有一个优美的性质：它能完全保持向量的欧几里得范数，即 $\|Qx\|_2 = \|x\|_2$。

如果在深度线性网络中每一层的权重矩阵都是正交的，那么信号的范数在整个[前向传播](@entry_id:193086)过程中将保持不变，从根本上杜绝了信号的消失或爆炸 [@problem_id:3199533]。对于一个由 $L$ 个正交矩阵构成的网络，其总的[变换矩阵](@entry_id:151616) $W_{prod} = Q^{(L)} \cdots Q^{(1)}$ 本身也是一个[正交矩阵](@entry_id:169220)，因此 $\|x^{(L)}\|_2 = \|x^{(0)}\|_2$。

在实践中，通常使用缩放的正交矩阵 $W = \alpha Q$。在这种情况下，经过 $L$ 层后，信号范数的放大因子将是 $\alpha^L$。为了保持稳定，缩放因子 $\alpha$ 必须非常接近 1。生成[正交矩阵](@entry_id:169220)通常通过对一个从标准正态分布中采样的[随机矩阵](@entry_id:269622)进行 QR 分解来完成。正交初始化因其强大的理论保证，在[循环神经网络](@entry_id:171248)（RNNs）等对梯度稳定性要求极高的架构中尤其受到青睐。

#### [自归一化](@entry_id:636594)网络 (SELU)

另一种更进一步的思路是共同设计[权重初始化](@entry_id:636952)方案和[激活函数](@entry_id:141784)，以创造出**[自归一化](@entry_id:636594)** (self-normalizing) 的特性。其目标是让神经元的激活值在网络传递过程中自动趋向于一个固定的[分布](@entry_id:182848)，通常是零均值和单位[方差](@entry_id:200758)。

**缩放[指数线性单元](@entry_id:634506)** (SELU) 就是为此目的而设计的激活函数 [@problem_id:3197614]。SELU 的形式为：
$$
\mathrm{SELU}(x) = \lambda \cdot
\begin{cases}
x, & x > 0 \\
\alpha(\exp(x) - 1), & x \le 0
\end{cases}
$$
这里的参数 $\lambda$ 和 $\alpha$ 并非随意设置，而是通过求解一个复杂的数学[方程组](@entry_id:193238)得到的特定“魔术数字”（约为 $\lambda \approx 1.0507, \alpha \approx 1.6733$）。当 SELU 与一种特定的[权重初始化](@entry_id:636952)方案（[方差](@entry_id:200758)为 $\sigma^2 = 1/n_{in}$ 的[高斯分布](@entry_id:154414)）配合使用时，可以证明，无论网络多深，激活值的[分布](@entry_id:182848)都会收敛到一个稳定的[不动点](@entry_id:156394)（零均值，单位[方差](@entry_id:200758)）。这种[自归一化](@entry_id:636594)特性使得深度全连接网络可以不依赖于[批量归一化](@entry_id:634986)（Batch Normalization）等技术就能进行有效训练。

综上所述，[权重初始化](@entry_id:636952)策略从最初解决简单的对称性问题，发展到基于[方差保持](@entry_id:634352)原则的 Xavier 和 He 初始化，再到利用几何性质的正交初始化和与[激活函数](@entry_id:141784)协同设计的[自归一化](@entry_id:636594)方法。尽管具体机制各异，但它们共同的目标都是为了在[深度神经网络](@entry_id:636170)中创建一个稳定的[信号传播](@entry_id:165148)环境，从而为高效、可靠的训练奠定基础。