{"hands_on_practices": [{"introduction": "在归一化层的公式中，参数 $\\epsilon$ 通常被简单地解释为“防止除以零”。本练习将深入探讨其更重要的作用。[@problem_id:3133999] 通过分析组归一化（Group Normalization）的反向传播动力学，您将研究当一组激活值的方差趋近于零时，梯度大小会如何变化。这项练习揭示了 $\\epsilon$ 在防止梯度爆炸和确保训练过程数值稳定性方面的关键作用，让您能够严谨地理解如何调整这一重要的超参数。", "problem": "考虑一个训练样本和组归一化 (GN) 中的一个组，组大小为 $m \\geq 2$。设该组的激活值为 $x_{1}, \\dots, x_{m} \\in \\mathbb{R}$，其组均值为 $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_{i}$，其组方差为 $\\sigma_{g}^{2} = \\frac{1}{m} \\sum_{i=1}^{m} (x_{i} - \\mu)^{2}$。GN 生成归一化的激活值 $\\hat{x}_{i} = \\frac{x_{i} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$ 并输出 $y_{i} = \\gamma \\hat{x}_{i} + \\beta$，其中 $\\gamma \\in \\mathbb{R}$ 和 $\\beta \\in \\mathbb{R}$ 是在该组的 $m$ 个元素之间共享的可学习参数。设标量损失为 $L(y_{1}, \\dots, y_{m})$。假设对于所有 $i \\in \\{1,\\dots,m\\}$，传入的梯度满足一个统一界限 $| \\frac{\\partial L}{\\partial y_{i}} | \\leq G$，其中 $G  0$ 是一个给定的常数。\n\n仅使用上述定义和基本的多元微积分（链式法则）作为基本依据：\n\n1) 当 $\\sigma_{g}^{2} \\to 0$ 时，推导梯度 $\\frac{\\partial L}{\\partial x_{i}}$ 的极限形式，并用 $\\gamma$、$\\epsilon$ 和传入的梯度表示。\n\n2) 从该极限形式出发，当 $\\sigma_{g}^{2} \\to 0$ 时，推导 $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$ 关于 $m$、$\\gamma$、$\\epsilon$ 和 $G$ 的最紧可能上界。\n\n3) 给定一个目标梯度幅值阈值 $\\tau  0$，求解在 $\\sigma_{g}^{2} \\to 0$ 的极限情况下，保证 $\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$ 的最小 $\\epsilon$ 值。\n\n4) 提出一种选择 $\\epsilon$ 的自适应策略，使其作为 $\\sigma_{g}^{2}$ 和其他问题参数的函数，在 $\\sigma_{g}^{2} \\to 0$ 时保持数值稳定性，并解释其工作原理。\n\n你的最终答案必须仅为第 3 部分中最小 $\\epsilon$ 的闭式表达式。无需单位。如果你的答案包含常数，请用符号表示。不要进行四舍五入。", "solution": "解决方案是一个分为四部分的推导与分析，其基础是将多元微积分原理应用于组归一化 (GN) 函数。\n\n### 第 1 部分：梯度 $\\frac{\\partial L}{\\partial x_{i}}$ 的极限形式推导\n\n损失 $L$ 关于输入激活值 $x_i$ 的全导数可以通过多元链式法则求得，即对所有依赖于 $x_i$ 的输出 $y_j$ 进行求和：\n$$ \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_{j}} \\frac{\\partial y_{j}}{\\partial x_{i}} $$\n输出 $y_j$ 定义为 $y_{j} = \\gamma \\hat{x}_{j} + \\beta$，其中 $\\hat{x}_{j} = \\frac{x_{j} - \\mu}{\\sqrt{\\sigma_{g}^{2} + \\epsilon}}$。因此，$\\frac{\\partial y_{j}}{\\partial x_{i}} = \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$。\n\n为了求得 $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$，我们必须考虑到均值 $\\mu$ 和方差 $\\sigma_g^2$ 都是组内所有 $x_k$ 的函数。均值 $\\mu$ 和方差 $\\sigma_g^2$ 关于 $x_i$ 的偏导数如下：\n$$ \\frac{\\partial \\mu}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} x_{k} \\right) = \\frac{1}{m} $$\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{\\partial}{\\partial x_{i}} \\left( \\frac{1}{m} \\sum_{k=1}^{m} (x_{k} - \\mu)^{2} \\right) = \\frac{1}{m} \\sum_{k=1}^{m} 2(x_{k} - \\mu) \\left( \\frac{\\partial x_{k}}{\\partial x_{i}} - \\frac{\\partial \\mu}{\\partial x_{i}} \\right) $$\n$$ = \\frac{2}{m} \\sum_{k=1}^{m} (x_{k} - \\mu) ( \\delta_{ki} - \\frac{1}{m} ) = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) - \\frac{1}{m}\\sum_{k \\neq i} (x_k - \\mu) \\right) $$\n使用性质 $\\sum_{k=1}^{m} (x_{k} - \\mu) = 0$，我们有 $\\sum_{k \\neq i} (x_{k} - \\mu) = -(x_i - \\mu)$。代入可得：\n$$ \\frac{\\partial \\sigma_{g}^{2}}{\\partial x_{i}} = \\frac{2}{m} \\left( (x_i - \\mu)(1-\\frac{1}{m}) + \\frac{1}{m}(x_i - \\mu) \\right) = \\frac{2}{m} (x_i - \\mu) $$\n现在，我们对 $\\hat{x}_j = \\frac{u}{v}$ 应用商法则，其中 $u = x_j - \\mu$，$v = \\sqrt{\\sigma_g^2 + \\epsilon}$。\n$$ \\frac{\\partial u}{\\partial x_i} = \\frac{\\partial (x_j - \\mu)}{\\partial x_i} = \\delta_{ij} - \\frac{1}{m} $$\n$$ \\frac{\\partial v}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{\\partial \\sigma_g^2}{\\partial x_i} = \\frac{1}{2\\sqrt{\\sigma_g^2 + \\epsilon}} \\frac{2}{m}(x_i - \\mu) = \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}} $$\n导数 $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{v \\frac{\\partial u}{\\partial x_i} - u \\frac{\\partial v}{\\partial x_i}}{v^2}$ 为：\n$$ \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\frac{\\sqrt{\\sigma_g^2 + \\epsilon}(\\delta_{ij} - \\frac{1}{m}) - (x_j-\\mu) \\frac{x_i - \\mu}{m\\sqrt{\\sigma_g^2 + \\epsilon}}}{\\sigma_g^2 + \\epsilon} = \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) - \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} $$\n我们关心 $\\sigma_g^2 \\to 0$ 时的极限。这个条件意味着对所有 $k \\in \\{1, \\dots, m\\}$ 都有 $(x_k - \\mu) \\to 0$。$(x_i-\\mu)(x_j-\\mu)$ 项是 $O(\\sigma_g^2)$ 阶的。让我们分析 $\\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}}$ 表达式中的第二项：\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{(x_i - \\mu)(x_j - \\mu)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{O(\\sigma_g^2)}{m(\\sigma_g^2 + \\epsilon)^{3/2}} = \\frac{0}{m \\epsilon^{3/2}} = 0 $$\n因此，在 $\\sigma_g^2 \\to 0$ 的极限下，导数简化为：\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} = \\lim_{\\sigma_g^2 \\to 0} \\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) = \\frac{1}{\\sqrt{\\epsilon}} \\left( \\delta_{ij} - \\frac{1}{m} \\right) $$\n将此代回 $\\frac{\\partial L}{\\partial x_i}$ 的表达式中，并使用 $g_j = \\frac{\\partial L}{\\partial y_j}$ 表示传入的梯度：\n$$ \\lim_{\\sigma_g^2 \\to 0} \\frac{\\partial L}{\\partial x_{i}} = \\sum_{j=1}^{m} g_j \\left( \\lim_{\\sigma_g^2 \\to 0} \\gamma \\frac{\\partial \\hat{x}_{j}}{\\partial x_{i}} \\right) = \\sum_{j=1}^{m} g_j \\frac{\\gamma}{\\sqrt{\\epsilon}} (\\delta_{ij} - \\frac{1}{m}) $$\n$$ = \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\frac{1}{m} \\sum_{j=1}^{m} g_j \\right) $$\n这就是梯度 $\\frac{\\partial L}{\\partial x_i}$ 的极限形式。\n\n### 第 2 部分：$\\max_{i} \\left| \\frac{\\partial L}{\\partial x_{i}} \\right|$ 的最紧上界\n\n根据第 1 部分，我们必须找到 $\\max_i \\left| \\frac{\\gamma}{\\sqrt{\\epsilon}} \\left( g_i - \\bar{g} \\right) \\right|$ 的最紧上界，其中 $\\bar{g} = \\frac{1}{m} \\sum_j g_j$ 且给定对所有 $j$ 都有 $|g_j| \\leq G$。这等价于最大化 $\\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\max_i |g_i - \\bar{g}|$。我们专注于最大化 $|g_i - \\bar{g}|$。\n$$ |g_i - \\bar{g}| = \\left| g_i - \\frac{1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| = \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| $$\n根据三角不等式：\n$$ \\left| \\frac{m-1}{m} g_i - \\frac{1}{m} \\sum_{j \\neq i} g_j \\right| \\leq \\frac{m-1}{m} |g_i| + \\frac{1}{m} \\sum_{j \\neq i} |g_j| $$\n使用界限 $|g_j| \\leq G$：\n$$ \\leq \\frac{m-1}{m} G + \\frac{1}{m} (m-1) G = 2G \\frac{m-1}{m} $$\n这个界是紧的。可以通过将一个梯度设置为一个极端值，而将其他梯度设置为相反的极端值来达到这个界。对于一个固定的 $i$，令 $g_i = G$ 且对所有 $j \\neq i$ 令 $g_j = -G$。\n平均梯度为 $\\bar{g} = \\frac{1}{m} (G + (m-1)(-G)) = \\frac{G(1 - m + 1)}{m} = \\frac{G(2-m)}{m}$。\n那么，$g_i - \\bar{g} = G - G\\frac{2-m}{m} = G \\left( \\frac{m - 2 + m}{m} \\right) = G \\frac{2m-2}{m} = 2G \\frac{m-1}{m}$。\n其幅值为 $|g_i - \\bar{g}| = 2G \\frac{m-1}{m}$。对于任意 $j \\neq i$，有 $|g_j - \\bar{g}| = |-G - G\\frac{2-m}{m}| = |-G \\frac{m+2-m}{m}| = \\frac{2G}{m}$。\n因为 $m \\geq 2$，所以 $m-1 \\geq 1$，故 $2G\\frac{m-1}{m} \\geq \\frac{2G}{m}$。\n与均值的最大偏差确实是 $2G \\frac{m-1}{m}$。\n因此，梯度幅值的最紧上界是：\n$$ \\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\frac{|\\gamma|}{\\sqrt{\\epsilon}} \\left( 2G \\frac{m-1}{m} \\right) $$\n\n### 第 3 部分：求解最小的 $\\epsilon$\n\n我们给定的要求是在 $\\sigma_g^2 \\to 0$ 的极限下 $\\max_i \\left| \\frac{\\partial L}{\\partial x_{i}} \\right| \\leq \\tau$。使用第 2 部分得到的最紧上界，我们建立不等式：\n$$ \\frac{2G |\\gamma|}{\\sqrt{\\epsilon}} \\frac{m-1}{m} \\leq \\tau $$\n为了找到保证此条件对任何有效的传入梯度配置都成立的最小 $\\epsilon  0$，我们对 $\\epsilon$ 求解：\n$$ \\sqrt{\\epsilon} \\geq \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} $$\n该不等式为 $\\sqrt{\\epsilon}$ 定义了一个下界。当此不等式取等号时，可获得 $\\epsilon$ 的最小值。两边平方得到所需的最小 $\\epsilon$ 值：\n$$ \\epsilon = \\left( \\frac{2G |\\gamma|}{\\tau} \\frac{m-1}{m} \\right)^2 = \\frac{4 G^2 \\gamma^2}{\\tau^2} \\left( 1 - \\frac{1}{m} \\right)^2 $$\n\n### 第 4 部分：提出一种选择 $\\epsilon$ 的自适应策略\n\n不稳定性来源于梯度表达式中的 $\\frac{1}{\\sqrt{\\sigma_g^2 + \\epsilon}}$ 项，如果 $\\sigma_g^2$ 和 $\\epsilon$ 都接近于零，该项可能会发生爆炸。第 3 部分得出的固定 $\\epsilon$ 提供了一个最坏情况下的保证，但可能过于保守。自适应策略可以根据当前条件设置 $\\epsilon$，使其恰好足够大以确保稳定性。\n\n一种有原则的自适应策略是动态地执行第 3 部分的梯度约束。对于一个给定的组，在每次反向传播中，我们可以访问传入的梯度 $g_j = \\frac{\\partial L}{\\partial y_j}$ 和参数 $\\gamma$。我们可以计算驱动梯度幅值的项：$C_{\\max} = |\\gamma| \\max_j |g_j - \\bar{g}|$。\n梯度幅值的主要部分约为 $\\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}}$。为了将其限制在阈值 $\\tau$ 内，我们需要：\n$$ \\frac{C_{\\max}}{\\sqrt{\\sigma_g^2 + \\epsilon}} \\leq \\tau \\implies \\sigma_g^2 + \\epsilon \\geq \\left( \\frac{C_{\\max}}{\\tau} \\right)^2 $$\n这引出了设置 $\\epsilon$ 以等式形式满足此条件的策略，同时确保其不为负，并包含一个小的下限值以保证鲁棒性：\n$$ \\epsilon = \\max\\left(\\epsilon_{\\text{floor}}, \\left(\\frac{C_{\\max}}{\\tau}\\right)^2 - [\\sigma_g^2]_{\\text{sg}}\\right) $$\n此处，$\\epsilon_{\\text{floor}}$ 是一个小的正常数（例如，$10^{-8}$），$\\tau$ 是期望的梯度范数上限的超参数，而 $[\\sigma_g^2]_{\\text{sg}}$ 表示在计算 $\\epsilon$ 时使用批方差 $\\sigma_g^2$ 的值，但不通过它传播梯度（即停止梯度操作）。\n\n该策略有效的原因是：\n1. 如果 $\\sigma_g^2$ 较大（即 $(\\frac{C_{\\max}}{\\tau})^2 \\leq \\sigma_g^2$），$\\epsilon$ 默认为 $\\epsilon_{\\text{floor}}$。梯度分母 $\\sqrt{\\sigma_g^2+\\epsilon}$ 很大，因此梯度小且稳定。\n2. 如果 $\\sigma_g^2$ 较小（即 $(\\frac{C_{\\max}}{\\tau})^2  \\sigma_g^2$），$\\epsilon$ 的选择使得 $\\sigma_g^2 + \\epsilon = (C_{\\max}/\\tau)^2$。梯度分母变为 $\\sqrt{(C_{\\max}/\\tau)^2} = C_{\\max}/\\tau$。因此，梯度幅值被限制在约 $\\frac{C_{\\max}}{C_{\\max}/\\tau} = \\tau$，从而防止了梯度爆炸。\n这种对 $\\epsilon$ 的自适应选择直接针对了数值不稳定性的根源。", "answer": "$$\n\\boxed{\\frac{4G^2\\gamma^2}{\\tau^2}\\left(\\frac{m-1}{m}\\right)^2}\n$$", "id": "3133999"}, {"introduction": "标准的统计估计量，如均值和方差，众所周知对异常值非常敏感。这个弱点同样会影响深度学习中的归一化层。[@problem_id:3134016] 在本练习中，您将构建一个包含异常值通道的“病态”场景，并观察其如何影响组内统计量的计算。您将亲手实现标准的组归一化（GN）及其一个鲁棒的变体，后者使用“均值的中位数”（Median-of-Means）作为估计器，通过对比两者的表现，您将深入理解GN的一个关键局限性，并掌握提升其鲁棒性的原则性方法。", "problem": "考虑一个表示深度神经网络中小批量特征图的四维张量。设该张量表示为 $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$，其中 $N$ 为批量大小，$C$ 为通道数，$H$ 和 $W$ 为空间维度。本任务要求实现、分析和比较两种应用于每个样本和每组通道的归一化方案：Group Normalization (GN) 以及一种鲁棒的替代方案，该方案使用 Median-of-Means (MoM) 替代经典估计量来计算均值和二阶矩。\n\n您必须从以下基本原理出发，推导归一化变换：\n\n- 有限实数集 $\\{x_i\\}_{i=1}^m$ 的算术平均值定义为 $\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i$。\n- (总体)方差定义为 $\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu)^2$。\n- 原点二阶矩为 $M_2 = \\frac{1}{m} \\sum_{i=1}^m x_i^2$，且满足 $\\sigma^2 = M_2 - \\mu^2$。\n\n一种分组配置将 $C$ 个通道划分为 $G$ 个大小相等的非重叠组，每组大小为 $\\frac{C}{G}$（假设 $C$ 可被 $G$ 整除）。对于每个样本 $n \\in \\{1,\\dots,N\\}$ 和每个组 $g \\in \\{1,\\dots,G\\}$，收集该组中所有通道及其空间位置上的所有元素；将此多重集表示为 $\\mathcal{S}_{n,g}$，其基数为 $m = |\\mathcal{S}_{n,g}|$。Group Normalization 使用在 $\\mathcal{S}_{n,g}$ 上计算的算术平均值和方差，将 $X$ 的元素映射到一个无量纲表示，其组均值为 $0$，组二阶中心矩为 $1$（不考虑数值稳定性常数 $\\varepsilon  0$）。鲁棒的替代方案将算术平均值替换为 Median-of-Means 估计量，将二阶矩替换为 $M_2$ 的 Median-of-Means 估计值，然后通过 $\\widehat{\\sigma}^2_{\\text{robust}} = \\max\\{ \\widehat{M}_2 - \\widehat{\\mu}^2, 0 \\}$ 构建一个鲁棒方差，同样使用相同的数值稳定性常数 $\\varepsilon$。\n\n$\\mathcal{S}_{n,g}$ 上的 Median-of-Means (MoM) 过程定义如下。将 $\\mathcal{S}_{n,g}$ 按固定、确定性的顺序展平为一个序列 $(x_1,\\dots,x_m)$。选择一个整数块数 $k$，满足 $k = \\min\\{3, m\\}$，并将序列划分为 $k$ 个大小尽可能相等的连续块。对于均值，计算每个块的均值，然后取其中位数得到 $\\widehat{\\mu}$。对于二阶矩，计算 $x_i^2$ 的块平均值，然后取其中位数得到 $\\widehat{M}_2$。\n\n实现这两种归一化方法时，不进行任何仿射重缩放或偏置（即，使用 $\\gamma = 1$ 和 $\\beta = 0$），并使用固定的数值稳定性常数 $\\varepsilon = 10^{-5}$。\n\n构建以下确定性测试套件。在所有情况下，$N = 1$，并且用于 MoM 分区的每个组内元素的排序是默认的行主序展平，其中通道变化最慢，空间维度变化最快。\n\n- 测试用例 1 (组内的病态主导): $C = 4$, $H = 2$, $W = 2$, $G = 2$。通道 $\\{0,1\\}$ 构成组 $0$，通道 $\\{2,3\\}$ 构成组 $1$。定义空间符号模式 $s(h,w) = 1$ 如果 $(h + w)$ 是偶数，否则 $s(h,w) = -1$，其中 $h \\in \\{0,1\\}$ 且 $w \\in \\{0,1\\}$。设置通道值为：\n  - 通道 $0$: $x_{0,h,w} = \\mathbf{1}\\{(h + w) \\text{ is odd}\\}$，\n  - 通道 $1$: $x_{1,h,w} = 1000 \\cdot s(h,w)$，\n  - 通道 $2$: $x_{2,h,w} = 0.5 \\cdot s(h,w)$，\n  - 通道 $3$: $x_{3,h,w} = -1 \\cdot s(h,w)$。\n  使用异常值通道索引 $c_{\\text{out}} = 1$ 和小方差通道索引 $c_{\\text{small}} = 0$。\n\n- 测试用例 2 (平衡的方差，无极端异常值): $C = 4$, $H = 2$, $W = 2$, $G = 2$。使用相同的 $s(h,w)$ 并设置振幅 $a_0 = 1$, $a_1 = 1.2$, $a_2 = 0.8$, $a_3 = 1.5$ 且\n  - 通道 $c$: $x_{c,h,w} = a_c \\cdot s(h,w)$ 对于 $c \\in \\{0,1,2,3\\}$。\n  使用 $c_{\\text{out}} = 1$ 和 $c_{\\text{small}} = 2$。\n\n- 测试用例 3 (边界情况：组大小为 1): $C = 4$, $H = 2$, $W = 2$, $G = 4$。使用与测试用例 1 中相同的通道定义。这里每个组只包含一个通道。使用 $c_{\\text{out}} = 1$ 和 $c_{\\text{small}} = 0$。\n\n- 测试用例 4 (每组元素最少): $C = 2$, $H = 1$, $W = 1$, $G = 1$。设置\n  - 通道 $0$: $x_{0,0,0} = 0.1$，\n  - 通道 $1$: $x_{1,0,0} = 1000$。\n  使用 $c_{\\text{out}} = 1$ 和 $c_{\\text{small}} = 0$。\n\n对于每个测试用例，计算四个量：\n- $r_{\\text{den}}$：包含 $c_{\\text{out}}$ 的组的分母之比 $\\frac{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}}$，\n- $r_{\\text{out}}$：异常值通道的最大绝对归一化值之比 $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{out}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{out}})|}$，\n- $r_{\\text{small}}$：小方差通道的最大绝对归一化值之比 $\\frac{\\max |Y_{\\text{GN}}(c_{\\text{small}})|}{\\max |Y_{\\text{MoM}}(c_{\\text{small}})|}$，\n- $d_{\\mu}$：包含 $c_{\\text{out}}$ 的组的组均值估计量的绝对差 $| \\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}} |$。\n\n所有这四个量都必须是实数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个包含四个值的列表，顺序为 $[r_{\\text{den}}, r_{\\text{out}}, r_{\\text{small}}, d_{\\mu}]$。例如，输出格式必须类似于 $[[v_{1,1}, v_{1,2}, v_{1,3}, v_{1,4}], [v_{2,1}, v_{2,2}, v_{2,3}, v_{2,4}], [v_{3,1}, v_{3,2}, v_{3,3}, v_{3,4}], [v_{4,1}, v_{4,2}, v_{4,3}, v_{4,4}]]$，其中包含您的实现所产生的精确数值。", "solution": "当前任务要求实现并对比分析两种用于深度学习特征图的归一化方案：标准的 Group Normalization (GN) 和一种基于 Median-of-Means (MoM) 估计量的鲁棒变体。分析将通过一个确定性的测试套件进行，该套件旨在评估这些方法在存在异常值时的行为。\n\n### 1. 理论基础\n\n设 $X \\in \\mathbb{R}^{N \\times C \\times H \\times W}$ 是一个表示小批量特征图的张量。两种归一化方案都作用于通道组。$C$ 个通道被划分为 $G$ 个组。对于每个样本 $n$ 和组 $g$，我们定义一个多重集 $\\mathcal{S}_{n,g}$，它包含该组中所有通道及其空间维度上的所有激活值。该集合的大小为 $m = (C/G) \\cdot H \\cdot W$。归一化的目的是标准化每个集合 $\\mathcal{S}_{n,g}$ 内数值分布的矩（例如，均值和方差）。\n\n#### 1.1. 标准 Group Normalization (GN)\n\nGroup Normalization 作为一种标准技术，使用经典的统计估计量，这些估计量虽然高效，但对异常值很敏感。\n\n集合 $\\mathcal{S}_{n,g}$（其元素表示为 $x_i$）的均值和方差计算如下：\n-   **算术平均值**：一阶原点矩（均值）的估计量是样本均值：\n    $$ \\mu_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m x_i $$\n-   **总体方差**：二阶中心矩（方差）的估计量是：\n    $$ \\sigma^2_{\\text{GN}} = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_{\\text{GN}})^2 $$\n    为了计算稳定性和效率，通常使用二阶原点矩 $M_{2, \\text{GN}} = \\frac{1}{m} \\sum_{i=1}^{m} x_i^2$，通过恒等式 $\\sigma^2_{\\text{GN}} = M_{2, \\text{GN}} - \\mu_{\\text{GN}}^2$ 来计算。\n\n对每个元素 $x_i \\in \\mathcal{S}_{n,g}$ 的归一化变换为：\n$$ y_i = \\frac{x_i - \\mu_{\\text{GN}}}{\\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon}} $$\n其中 $\\varepsilon$ 是一个小的常数（在本问题中为 $10^{-5}$），用于增加数值稳定性以防止除以零。此变换将每组中的激活值重缩放，使其均值近似为 $0$，方差近似为 $1$。\n\n#### 1.2. 使用 Median-of-Means (MoM) 的鲁棒 Group Normalization\n\n为了提高对异常值的鲁棒性，可以用鲁棒的替代方案替换经典的均值和方差估计量。Median-of-Means (MoM) 是一种简单而有效的均值鲁棒估计量。\n\n对于一个包含 $m$ 个值的集合，MoM 过程如下：\n1.  将该集合划分为 $k$ 个不重叠的块，其中 $k$ 被指定为 $k = \\min\\{3, m\\}$。划分时要使块的大小尽可能相等。\n2.  计算 $k$ 个块中每个块内值的算术平均值。\n3.  MoM 估计值是这些块均值的中位数。\n\n应用此原理，我们定义了均值和二阶矩的鲁棒估计量：\n-   **MoM 均值估计量 ($\\widehat{\\mu}_{\\text{MoM}}$)**：将数据 $\\{x_i\\}_{i=1}^m$ 划分为 $k$ 个块。设 $B_1, \\dots, B_k$ 为这些块。均值的 MoM 估计为：\n    $$ \\widehat{\\mu}_{\\text{MoM}} = \\text{median}\\left( \\left\\{ \\text{mean}(B_j) \\right\\}_{j=1}^k \\right) $$\n-   **MoM 二阶矩估计量 ($\\widehat{M}_{2, \\text{MoM}}$)**：将相同的过程应用于平方后的数据 $\\{x_i^2\\}_{i=1}^m$。将这些数据划分为 $k$ 个块，二阶矩的 MoM 估计是平方数据块平均值的中位数。\n-   **鲁棒方差估计量 ($\\widehat{\\sigma}^2_{\\text{MoM}}$)**：使用矩的鲁棒估计量，通过插入原则构造鲁棒方差估计：\n    $$ \\widehat{\\sigma}^2_{\\text{MoM}} = \\max\\left( \\widehat{M}_{2, \\text{MoM}} - \\widehat{\\mu}_{\\text{MoM}}^2, 0 \\right) $$\n    $\\max(\\cdot, 0)$ 操作确保了非负性，这可能会因为估计误差而被违反。\n\n对每个元素 $x_i \\in \\mathcal{S}_{n,g}$ 的鲁棒归一化变换类似于 GN：\n$$ y_i = \\frac{x_i - \\widehat{\\mu}_{\\text{MoM}}}{\\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}} $$\n\n### 2. 算法实现与分析\n\n实现遵循上述定义。对于每个测试用例，构造一个输入张量 $X$。两个独立的函数分别应用 GN 和基于 MoM 的归一化方案。每个函数遍历 $G$ 个通道组。对于给定的组，它从 $X$ 中提取相应的数据切片，将其展平为一维数组，并计算所需的统计量（$\\mu_{\\text{GN}}, \\sigma^2_{\\text{GN}}$ 或 $\\widehat{\\mu}_{\\text{MoM}}, \\widehat{\\sigma}^2_{\\text{MoM}}$）。然后使用这些统计量对该组中的数据进行归一化。\n\nMoM 实现的一个关键细节是分块。对于大小为 $m$ 的展平数组和 $k$ 个块，使用 `numpy.array_split`，它创建大小尽可能相等的块，与问题规范相匹配。中位数使用 `numpy.median` 计算，它可以正确处理奇数和偶数个数的块统计量。\n\n通过为每个测试用例计算四个指标来进行分析：\n-   $d_{\\mu} = |\\mu_{\\text{GN}} - \\widehat{\\mu}_{\\text{MoM}}|$：该指标直接衡量包含异常值通道的组的经典均值估计和鲁棒均值估计之间的差异。值较大表明异常值显著扭曲了经典均值。\n-   $r_{\\text{den}} = \\sqrt{\\sigma^2_{\\text{GN}} + \\varepsilon} / \\sqrt{\\widehat{\\sigma}^2_{\\text{MoM}} + \\varepsilon}$：该指标比较了尺度估计（归一化分母）。该比率表明与经典方法相比，鲁棒方法感知到的数据离散程度是更大还是更小。\n-   $r_{\\text{out}} = \\max |Y_{\\text{GN}}(c_{\\text{out}})| / \\max |Y_{\\text{MoM}}(c_{\\text{out}})|$：该比率衡量了异常值通道归一化后的相对大小。大于 $1$ 的值表明基于 MoM 的方法在抑制异常值的大小方面更有效。\n-   $r_{\\text{small}} = \\max |Y_{\\text{GN}}(c_{\\text{small}})| / \\max |Y_{\\text{MoM}}(c_{\\text{small}})|$：该指标评估了归一化方案对“常规”通道的影响。理想的鲁棒方法应对非异常值数据产生最小的扭曲影响。\n\n测试用例旨在探究不同的行为：\n-   **测试用例 1** 引入一个强异常值，以突显鲁棒性方面的核心差异。\n-   **测试用例 2** 使用没有极端异常值的平衡数据，预计两种方法会产生相似的结果。\n-   **测试用例 3 和 4** 探讨了与每组元素数量 ($m$) 及其对 MoM 估计量影响相关的边界条件，特别是在 $k  3$ 时。当 $m=2, k=2$ 时，均值和二阶矩的 MoM 估计量与其经典对应物相同，因为两个值的中位数是它们的平均值。当 $m=4, k=3$ 时，方法之间的差异取决于数据分布以及分区的形成方式。", "answer": "[[1414.213562373095, 1414.213562373095, 0.7071067811865475, 124.875], [1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0]]", "id": "3134016"}, {"introduction": "在神经网络中，卷积和归一化等操作的顺序会显著影响模型的性能和行为。一个有趣的问题是：我们能否随意交换它们的位置？[@problem_id:3134023] 本练习将探讨组归一化（Group Normalization）与卷积运算的可交换性，即在何种条件下 $\\mathrm{Conv}(\\mathrm{GN}(x)) = \\mathrm{GN}(\\mathrm{Conv}(x))$ 成立。您将运用线性代数的框架，分析当卷积操作不混合不同组别的通道时，需要满足哪些代数条件才能使两者交换。这项高级理论练习将帮助您深入理解层与层之间的结构性相互作用，并揭示归一化的放置为何是一项关键的架构决策。", "problem": "考虑一个张量为 $x \\in \\mathbb{R}^{C \\times H \\times W}$ 的单个输入样本，它被分成 $G$ 个不相交的组，每组大小相等，包含 $C/G$ 个通道。对于每个组 $g \\in \\{1,\\dots,G\\}$，通过堆叠该组中所有的 $(C/G)\\cdot H \\cdot W$ 个条目来定义向量化的组 $z_g \\in \\mathbb{R}^{m}$，其中 $m = (C/G)\\,H\\,W$。令 $1 \\in \\mathbb{R}^{m}$ 表示全1向量，并令中心化投影算子为 $P_g = I_m - \\frac{1}{m} 1 1^{\\top}$。组均值和方差为\n$$\n\\mu_g(z_g) = \\frac{1}{m} 1^{\\top} z_g, \n\\qquad \n\\sigma_g^{2}(z_g) = \\frac{1}{m} \\| P_g z_g \\|_2^{2}。\n$$\n带有逐通道仿射参数 $\\{\\gamma_c, \\beta_c\\}_{c=1}^{C}$ 的组归一化（Group Normalization, GN）作用于每个组 $g$ 的方式如下\n$$\n\\mathrm{GN}_g(z_g) \\;=\\; \\Gamma_g \\frac{P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} \\;+\\; \\beta_g,\n$$\n其中 $\\varepsilon \\ge 0$ 是一个固定常数，$\\Gamma_g \\in \\mathbb{R}^{m\\times m}$ 是一个对角矩阵，其对角线元素是组 $g$ 中各通道的 $\\gamma_c$ 在空间位置上复制而成，而 $\\beta_g \\in \\mathbb{R}^{m}$ 是将组 $g$ 中各通道的 $\\beta_c$ 在空间位置上复制并堆叠而成的向量。完整的输出是各组结果的拼接。\n\n设一个卷积层（Conv）表示为一个跨组的分块对角线性算子，其每个组的线性映射为 $L_g \\in \\mathbb{R}^{m\\times m}$，偏置为 $b_g \\in \\mathbb{R}^{m}$，因此\n$$\n\\mathrm{Conv}_g(z_g) \\;=\\; L_g z_g \\;+\\; b_g,\n$$\n组间通道不混合（即，总的 $L$ 是以 $L_g$ 为块的分块对角矩阵）。\n\n仅使用上述定义，以及卷积是线性的和 $P_g$ 是到零均值子空间上的正交投影算子这两个事实，完成以下任务：\n\n1) 推导在何种必要的和充分的代数条件下，\n$$\n\\mathrm{Conv}(\\mathrm{GN}(x)) \\;=\\; \\mathrm{GN}(\\mathrm{Conv}(x))\n$$\n对每个输入 $x$ 都成立。这些条件应施加于每个组的卷积映射 $L_g$ 和偏置 $b_g$、组归一化的仿射参数 $\\Gamma_g$ 和 $\\beta_g$ 以及稳定项 $\\varepsilon$。你的推导必须明确地使用 $P_g$ 和 $1$ 进行表述，并应精确说明 $L_g$ 在子空间 $\\mathrm{range}(P_g)$ 和 $\\mathrm{span}\\{1\\}$ 上必须具有的结构。\n\n2) 定义交换子残差\n$$\nC(x) \\;=\\; \\mathrm{Conv}(\\mathrm{GN}(x)) \\;-\\; \\mathrm{GN}(\\mathrm{Conv}(x)),\n$$\n和标量\n$$\nJ(x) \\;=\\; \\sum_{g=1}^{G} \\| C_g(x) \\|_2^{2},\n$$\n其中 $C_g(x)$ 是 $C(x)$ 在 $\\mathbb{R}^{m}$ 中检视的第 $g$ 个组块。在你第1部分推导出的条件下，确定对于任意非零 $x$，$J(x)$ 的精确值。以一个不带单位的实数形式提供你的最终答案。不需要四舍五入。", "solution": "我们从所述的组归一化（GN）和卷积的定义开始。对于每个组 $g$，我们使用向量化表示 $z_g \\in \\mathbb{R}^{m}$，其中 $m = (C/G)\\,H\\,W$。中心化投影算子 $P_g = I_m - \\frac{1}{m} 1 1^{\\top}$ 满足 $P_g^{2} = P_g$， $P_g^{\\top} = P_g$，$\\mathrm{range}(P_g) = \\{v : 1^{\\top} v = 0\\}$ 以及 $\\ker(P_g) = \\mathrm{span}\\{1\\}$。方差可以写作 $\\sigma_g^{2}(z_g) = \\frac{1}{m} \\|P_g z_g\\|_2^{2}$。\n\n第1部分：交换条件的推导。\n\n我们写出在组 $g$ 上的两种复合运算：\n\n左侧（先GN后Conv）：\n$$\n\\mathrm{Conv}_g(\\mathrm{GN}_g(z_g)) \n= L_g \\left( \\Gamma_g \\frac{P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} + \\beta_g \\right) + b_g \n= \\frac{L_g \\Gamma_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} + L_g \\beta_g + b_g.\n$$\n\n右侧（先Conv后GN）：\n首先计算 $\\mathrm{Conv}_g(z_g) = L_g z_g + b_g$。其组均值和方差为\n$$\n\\mu_g(L_g z_g + b_g) = \\frac{1}{m} 1^{\\top} (L_g z_g + b_g), \n\\qquad \n\\sigma_g^{2}(L_g z_g + b_g) = \\frac{1}{m} \\| P_g (L_g z_g + b_g)\\|_2^{2}.\n$$\n然后\n$$\n\\mathrm{GN}_g(\\mathrm{Conv}_g(z_g))\n= \\Gamma_g \\frac{P_g (L_g z_g + b_g)}{\\sqrt{\\sigma_g^{2}(L_g z_g + b_g) + \\varepsilon}} + \\beta_g.\n$$\n\n要对所有 $z_g$ 相等，需要\n$$\n\\frac{L_g \\Gamma_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}} + L_g \\beta_g + b_g\n\\;=\\;\n\\Gamma_g \\frac{P_g (L_g z_g + b_g)}{\\sqrt{\\sigma_g^{2}(L_g z_g + b_g) + \\varepsilon}} + \\beta_g\n\\quad \\text{对所有 } z_g.\n$$\n\n我们将依赖于 $z_g$ 的项与不依赖于 $z_g$ 的项分开。\n\nA) 偏置项。比较不依赖于 $z_g$ 的项，我们得到\n$$\nL_g \\beta_g + b_g \\;=\\; \\Gamma_g \\frac{P_g b_g}{\\sqrt{\\sigma_g^{2}(b_g) + \\varepsilon}} + \\beta_g.\n$$\n为了使此式对所有配置都成立，特别是为了避免右侧出现依赖于输入的分母，一个充分且必要的方法是消除 $b_g$，并防止左侧的项 $L_g \\beta_g$ 引入不匹配。一个简洁的必要条件是\n$$\nb_g = 0.\n$$\n当 $b_g=0$ 时，条件简化为 $L_g \\beta_g = \\beta_g$。为了在不依赖于 $L_g$ 在中心化子空间上的任何进一步结构的情况下保证交换性，一个稳妥的选择是\n$$\n\\beta_g = 0.\n$$\n如果希望允许 $\\beta_g \\neq 0$，它必须满足 $L_g \\beta_g = \\beta_g$，即 $\\beta_g$ 位于 $L_g$ 的不动点子空间中。由于我们寻求的条件要确保对每个输入 $x$ 都相等，并且对所有允许的 $L_g$ 都一致，我们采纳 $\\beta_g=0$ 和 $b_g=0$ 作为偏置/移位项不破坏交换性的充分必要条件。\n\n因此，我们假设\n$$\nb_g = 0, \\qquad \\beta_g = 0.\n$$\n\nB) 缩放与中心化相互作用。当 $b_g=\\beta_g=0$ 时，等式简化为\n$$\n\\frac{L_g \\Gamma_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}}\n\\;=\\;\n\\Gamma_g \\frac{P_g L_g z_g}{\\sqrt{\\sigma_g^{2}(L_g z_g) + \\varepsilon}}\n\\quad \\text{对所有 } z_g.\n$$\n这可以重新排列为\n$$\nL_g \\Gamma_g P_g z_g \\cdot \\sqrt{\\sigma_g^{2}(L_g z_g) + \\varepsilon}\n\\;=\\;\n\\Gamma_g P_g L_g z_g \\cdot \\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}\n\\quad \\text{对所有 } z_g.\n$$\n\n我们现在要求 $L_g$ 与 $P_g$ 交换以避免均值泄漏：我们需要 $P_g L_g z_g$ 只依赖于 $P_g z_g$（而不依赖于 $\\mu_g(z_g)$）。这等价于要求\n$$\nP_g L_g = L_g P_g,\n$$\n展开后即 $L_g 1 \\in \\mathrm{span}\\{1\\}$ 和 $1^{\\top} L_g \\in \\mathrm{span}\\{1^{\\top}\\}$。具体来说，必须存在一个标量 $\\alpha_g \\in \\mathbb{R}$ 使得\n$$\nL_g 1 = \\alpha_g 1, \n\\qquad \n1^{\\top} L_g = \\alpha_g 1^{\\top}.\n$$\n在这个双特征向量条件下，$P_g L_g = L_g P_g$ 成立。\n\n在中心化子空间 $\\mathrm{range}(P_g)$ 上，将 $L_g$ 的限制写为\n$$\nL_g \\big|_{\\mathrm{range}(P_g)} = A_g,\n$$\n其中 $A_g:\\mathrm{range}(P_g) \\to \\mathrm{range}(P_g)$ 是线性的。那么对任意 $z_g$，\n$$\nP_g L_g z_g = A_g (P_g z_g),\n\\qquad \n\\sigma_g^{2}(L_g z_g) = \\frac{1}{m} \\| A_g (P_g z_g)\\|_2^{2}.\n$$\n\nC) 分母的均衡。为了使归一化后的幅度对所有 $z_g$ 都匹配，我们要求在中心化子空间上，变换能以一个与 $z_g$ 无关的因子统一地缩放范数（至多相差一个正交变换）。也就是说，存在 $\\alpha_g \\in \\mathbb{R}$ 和一个在 $\\mathrm{range}(P_g)$ 上的正交映射 $U_g$ 使得\n$$\nA_g = \\alpha_g U_g,\n\\qquad \nU_g^{\\top} U_g = U_g U_g^{\\top} = I \\text{ on } \\mathrm{range}(P_g).\n$$\n那么\n$$\n\\|A_g (P_g z_g)\\|_2 = |\\alpha_g| \\, \\| P_g z_g \\|_2 \\;\\Rightarrow\\; \\sigma_g^{2}(L_g z_g) = \\alpha_g^{2}\\, \\sigma_g^{2}(z_g).\n$$\n有了 $\\sigma_g^{2}(L_g z_g) = \\alpha_g^{2} \\sigma_g^{2}(z_g)$，分母变为\n$$\n\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}\n\\quad \\text{和} \\quad\n\\sqrt{\\alpha_g^{2}\\,\\sigma_g^{2}(z_g) + \\varepsilon}.\n$$\n为了对每个 $z_g$（因此对每个 $\\sigma_g^{2}(z_g) \\ge 0$）都相等，我们必须有\n$$\n\\sqrt{\\alpha_g^{2}\\,\\sigma_g^{2}(z_g) + \\varepsilon} \\;=\\; \\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon} \\quad \\text{对所有 } \\sigma_g^{2}(z_g) \\ge 0,\n$$\n这迫使\n$$\n\\alpha_g^{2} = 1.\n$$\n因此 $\\alpha_g \\in \\{+1,-1\\}$。这是协调所有输入的 $\\varepsilon$-稳定化分母的唯一方法。\n\nD) 与GN仿射缩放的交换性。对角矩阵 $\\Gamma_g$ 将组 $g$ 内的每个坐标相乘。为了使等式\n$$\nL_g \\Gamma_g P_g z_g \\;=\\; \\Gamma_g P_g L_g z_g\n$$\n对所有 $z_g$ 成立，我们需要 $\\Gamma_g$ 与 $L_g$ 在 $\\mathrm{range}(P_g)$ 上交换。由于 $L_g$ 可能通过 $\\mathrm{range}(P_g)$ 上的 $U_g$ 混合组内的坐标，唯一能与所有此类 $U_g$ 交换的对角矩阵 $\\Gamma_g$ 是该组上单位矩阵的标量倍数：\n$$\n\\Gamma_g = \\gamma_g I_m \\quad \\text{对于某个 } \\gamma_g \\in \\mathbb{R}.\n$$\n换句话说，每个通道的缩放因子在每个组内必须是常数。再加上 $\\beta_g=0$，这相当于GN的仿射变换是每个组的标量缩放，没有移位。\n\n综合A–D，每个组 $g$ 的交换性的充分必要条件是：\n- 无跨组混合和零偏置：$b_g = 0$。\n- GN仿射参数满足 $\\beta_g = 0$ 且 $\\Gamma_g = \\gamma_g I_m$，对于某个 $\\gamma_g \\in \\mathbb{R}$（即，逐通道的 $\\gamma_c$ 在组内是常数；逐通道的 $\\beta_c$ 是零）。\n- 卷积映射保持均值，并且在中心化子空间上是等距映射（可能带符号）：\n存在 $\\alpha_g \\in \\{+1,-1\\}$ 和 $\\mathrm{range}(P_g)$ 上的一个正交映射 $U_g$ 使得\n$$\nL_g 1 = \\alpha_g 1, \n\\qquad \n1^{\\top} L_g = \\alpha_g 1^{\\top}, \n\\qquad \nL_g P_g = \\alpha_g U_g P_g.\n$$\n等价地，$P_g L_g = L_g P_g$，$L_g$ 在 $\\mathrm{span}\\{1\\}$ 上的作用是乘以 $\\alpha_g$，在 $\\mathrm{range}(P_g)$ 上的作用是 $\\alpha_g U_g$。\n\n在这些条件下，对所有 $z_g$，\n$$\n\\mathrm{Conv}_g(\\mathrm{GN}_g(z_g)) \n= \\frac{\\gamma_g \\alpha_g U_g P_g z_g}{\\sqrt{\\sigma_g^{2}(z_g) + \\varepsilon}}\n= \\gamma_g \\frac{P_g L_g z_g}{\\sqrt{\\sigma_g^{2}(L_g z_g) + \\varepsilon}}\n= \\mathrm{GN}_g(\\mathrm{Conv}_g(z_g)).\n$$\n因此，对于每个输入 $x$，都有 $\\mathrm{Conv}(\\mathrm{GN}(x)) = \\mathrm{GN}(\\mathrm{Conv}(x))$。\n\n第2部分：在这些条件下 $J(x)$ 的值。\n\n根据定义，\n$$\nC(x) = \\mathrm{Conv}(\\mathrm{GN}(x)) - \\mathrm{GN}(\\mathrm{Conv}(x)),\n$$\n且 $J(x) = \\sum_{g=1}^{G} \\| C_g(x) \\|_2^{2}$，其中 $C_g(x)$ 是第 $g$ 个组块。\n\n在第1部分推导的条件下，我们有 $\\mathrm{Conv}(\\mathrm{GN}(x)) = \\mathrm{GN}(\\mathrm{Conv}(x))$ 对所有 $x$ 恒成立，因此对所有 $g$ 和 $x$ 都有 $C_g(x) = 0$。因此，\n$$\nJ(x) = \\sum_{g=1}^{G} \\| 0 \\|_2^{2} = 0.\n$$\n这个精确值对任意非零 $x$ 以及 $x=0$ 都成立。", "answer": "$$\\boxed{0}$$", "id": "3134023"}]}