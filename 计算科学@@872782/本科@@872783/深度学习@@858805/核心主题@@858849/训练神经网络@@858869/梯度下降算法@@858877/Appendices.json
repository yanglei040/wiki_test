{"hands_on_practices": [{"introduction": "梯度下降法是优化领域的核心算法，其基本思想是沿着目标函数梯度的反方向迭代前进，以寻找函数的最小值。这个练习将这一抽象概念应用于一个具体的物理场景：计算化学中的莫尔斯势能。通过亲手实现带有回溯线搜索的最速下降算法来最小化双原子分子的势能，你将不仅掌握梯度下降的核心逻辑，还能体会到学习率（步长）选择在确保算法稳定收敛中的关键作用。[@problem_id:2463075]", "problem": "给定一维空间中的双原子莫尔斯势（Morse potential），其由键长变量 $r$ 定义为\n$$\nV(r; D_e, a, r_e) = D_e \\left(1 - e^{-a \\, (r - r_e)}\\right)^2,\n$$\n其中 $D_e \\gt 0$ 是解离能参数，$a \\gt 0$ 控制曲率，$r_e \\gt 0$ 是平衡键长。对于本问题，所有量均为无量纲。考虑一个在 $r_0$ 处初始化并通过以下方式更新的迭代序列 $\\{r_k\\}_{k=0}^{\\infty}$：\n$$\nr_{k+1} = r_k - \\alpha_k \\, \\frac{dV}{dr}(r_k),\n$$\n其中步长 $\\alpha_k \\gt 0$ 在每次迭代时被选择，以确保能量序列 $\\{V(r_k)\\}$ 是单调非增的。当梯度大小满足 $\\left|\\frac{dV}{dr}(r_k)\\right| \\le \\varepsilon$ 或达到最大迭代次数时，迭代终止。设充分下降条件为\n$$\nV(r_k - \\alpha_k \\, \\tfrac{dV}{dr}(r_k)) \\le V(r_k) - c \\, \\alpha_k \\left(\\tfrac{dV}{dr}(r_k)\\right)^2,\n$$\n其中使用一个收缩因子 $\\rho \\in (0,1)$ 来减小 $\\alpha_k$ 直至该条件成立。对于每个测试用例，将 $c$ 和 $\\rho$ 视为给定参数。您的任务是为每个测试用例计算最终位置 $r_\\star$、最终能量 $V(r_\\star)$、执行的迭代次数、能量序列是否在所有记录的迭代中单调非增，以及在集合 $\\{0,1,2,4,8,16\\}$ 中属于已执行迭代范围内的迭代索引处的能量采样列表（仅包括那些存在的索引）。\n\n使用以下参数集的测试套件。对于每种情况，所有符号和数字的定义如上，且所有量均为无量纲：\n- 情况 1（高度压缩的起始状态）：$D_e = 1.0$，$a = 1.5$，$r_e = 1.4$，$r_0 = 0.2$，初始试验步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-8}$，最大迭代次数 $= 2000$。\n- 情况 2（平衡起始状态）：$D_e = 1.0$，$a = 1.5$，$r_e = 1.4$，$r_0 = 1.4$，初始试验步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-12}$，最大迭代次数 $= 100$。\n- 情况 3（拉伸起始状态）：$D_e = 1.0$，$a = 1.5$，$r_e = 1.4$，$r_0 = 4.0$，初始试验步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-10}$，最大迭代次数 $= 3000$。\n- 情况 4（更陡峭的势，高度压缩的起始状态）：$D_e = 2.0$，$a = 3.0$，$r_e = 1.2$，$r_0 = 0.1$，初始试验步长 $= 1.0$，$c = 1.0 \\times 10^{-4}$，$\\rho = 0.5$，$\\varepsilon = 1.0 \\times 10^{-8}$，最大迭代次数 $= 5000$。\n\n对于每个测试用例，您的程序必须计算：\n- $r_\\star$ 作为最终迭代值，\n- $V(r_\\star)$，\n- 执行的总迭代次数，\n- 一个布尔值，指示整个记录的能量序列是否单调非增，\n- 在 $\\{0,1,2,4,8,16\\}$ 中对该情况可用的迭代索引处的采样能量列表。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含所有四个测试用例结果的 JSON 数组。输出必须是一个包含四个元素的单个 JSON 数组，每个元素对应一个测试用例，并且本身是一个形式如下的 JSON 数组：\n$$\n[r_\\star, \\, V(r_\\star), \\, \\text{number\\_of\\_iterations}, \\, \\text{monotone\\_flag}, \\, \\text{sampled\\_energies}],\n$$\n其中 $\\text{monotone\\_flag}$ 是一个 JSON 布尔值，$\\text{sampled\\_energies}$ 是在 $\\{0,1,2,4,8,16\\}$ 中可用索引处的浮点数值的 JSON 数组。例如，一个有效的整体输出形状是\n$$\n\\bigl[ [\\cdots], [\\cdots], [\\cdots], [\\cdots] \\bigr].\n$$\n所有计算均使用上面定义的无量纲莫尔斯势和指定的参数。不生成任何绘图，也不需要用户输入。唯一的输出是单行的 JSON 数组。", "solution": "问题陈述已经过严格评估，被认为是有效的。其科学基础扎实、提法明确、客观，并包含所有必要信息，可以得出一个唯一且可验证的解。任务是实现带有回溯线搜索的最速下降优化算法，为几组参数找到一维莫尔斯势的最小值。\n\n莫尔斯势描述了双原子分子随其核间距 $r$ 变化的势能，其表达式为：\n$$\nV(r; D_e, a, r_e) = D_e \\left(1 - e^{-a \\, (r - r_e)}\\right)^2\n$$\n这里，$D_e  0$ 是势阱深度（解离能），$a  0$ 是一个控制势阱宽度的参数，$r_e  0$ 是对应于势能最小值的平衡键距。\n\n任务的核心是执行几何优化，在这个一维情况下，这等同于找到使 $V(r)$ 最小化的 $r$ 值。最速下降法是一种迭代优化算法，它通过在与函数梯度相反的方向上迈出一步来更新当前位置 $r_k$。在此一维背景下，梯度即为一阶导数 $\\frac{dV}{dr}$。更新规则是：\n$$\nr_{k+1} = r_k - \\alpha_k \\frac{dV}{dr}(r_k)\n$$\n其中 $\\alpha_k  0$ 是第 $k$ 次迭代的步长。\n\n首先，我们必须计算莫尔斯势的解析导数。使用链式法则，我们得到：\n$$\n\\frac{dV}{dr}(r) = \\frac{d}{dr} \\left[ D_e \\left(1 - e^{-a \\, (r - r_e)}\\right)^2 \\right]\n$$\n$$\n\\frac{dV}{dr}(r) = 2 D_e \\left(1 - e^{-a \\, (r - r_e)}\\right) \\cdot \\frac{d}{dr} \\left(1 - e^{-a \\, (r - r_e)}\\right)\n$$\n$$\n\\frac{dV}{dr}(r) = 2 D_e \\left(1 - e^{-a \\, (r - r_e)}\\right) \\cdot \\left( -e^{-a \\, (r - r_e)} \\cdot (-a) \\right)\n$$\n$$\n\\frac{dV}{dr}(r) = 2 a D_e \\left(1 - e^{-a \\, (r - r_e)}\\right) e^{-a \\, (r - r_e)}\n$$\n这个梯度，我们称之为 $g_k = \\frac{dV}{dr}(r_k)$，决定了最速下降方向 $-g_k$。\n\n该算法的一个关键组成部分是步长 $\\alpha_k$ 的确定。步长过大可能导致算法越过最小值并最终发散，而步长过小则可能导致收敛速度过慢。问题指定了一种由 Armijo 充分下降条件控制的回溯线搜索策略。该条件确保所采取的步骤能在势能上实现有意义的降低。该条件为：\n$$\nV(r_{k+1}) \\le V(r_k) + c \\, \\alpha_k \\, \\nabla V(r_k)^T p_k\n$$\n在我们的一维情况下，搜索方向为 $p_k = -g_k = -\\frac{dV}{dr}(r_k)$，上式变为：\n$$\nV(r_k - \\alpha_k g_k) \\le V(r_k) - c \\, \\alpha_k g_k^2\n$$\n其中 $c$ 是一个小的正常数，对于所有测试用例，给定值为 $c = 1.0 \\times 10^{-4}$。回溯过程从一个初始试验步长 $\\alpha_{trial}$（给定为 $1.0$）开始，并用一个收缩因子 $\\rho \\in (0,1)$（给定为 $0.5$）反复减小它，直到满足 Armijo 条件。\n\n每个测试用例的总体算法流程如下：\n1. 用给定的起始值初始化迭代计数器 $k=0$ 和位置 $r_0$。\n2. 开始一个循环，最多持续设定的最大迭代次数。\n3. 在每次迭代 $k$ 中，计算梯度 $g_k = \\frac{dV}{dr}(r_k)$。\n4. 检查收敛性：如果梯度的大小 $|g_k|$ 小于或等于指定的容差 $\\varepsilon$，则算法已收敛，循环终止。\n5. 如果未收敛，执行回溯线搜索以找到合适的步长 $\\alpha_k$：\n    a. 初始化 $\\alpha = \\alpha_{trial}$。\n    b. 当 $V(r_k - \\alpha g_k)  V(r_k) - c \\alpha g_k^2$ 时，更新 $\\alpha \\leftarrow \\rho \\alpha$。\n    c. 设置 $\\alpha_k = \\alpha$。\n6. 更新位置：$r_{k+1} = r_k - \\alpha_k g_k$。\n7. 在每一步存储能量 $V(r_k)$，以监控优化进程并验证单调性。\n8. 增加迭代计数器，$k \\leftarrow k+1$。\n9. 如果循环因达到最大迭代次数而完成，则以当前状态终止过程。\n\n终止时，将最终位置 $r_\\star$、最终能量 $V(r_\\star)$、总迭代次数、一个用于表示能量序列单调性的布尔标志，以及在指定迭代次数 $(\\{0, 1, 2, 4, 8, 16\\})$ 处的采样能量列表进行汇编。由于强制执行了充分下降条件，预计单调性检查 $V(r_{i+1}) \\le V(r_i)$（对于所有 $i$）将会通过。\n\n这个系统性步骤将被应用于四个指定的测试用例中的每一个。情况2可作为一个有用的诊断案例，因为它从精确的最小值点 $r_0 = r_e = 1.4$ 开始，此处的梯度为零。算法应在第 $k=0$ 次迭代时正确终止，梯度为零。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Morse potential optimization problem for a suite of test cases\n    using the steepest descent algorithm with a backtracking line search.\n    \"\"\"\n\n    def morse_potential(r, De, a, re):\n        \"\"\"Calculates the Morse potential V(r).\"\"\"\n        return De * (1 - np.exp(-a * (r - re)))**2\n\n    def morse_gradient(r, De, a, re):\n        \"\"\"Calculates the gradient (derivative) of the Morse potential dV/dr.\"\"\"\n        exp_term = np.exp(-a * (r - re))\n        return 2 * a * De * (1 - exp_term) * exp_term\n\n    def run_steepest_descent(params):\n        \"\"\"\n        Performs the steepest descent optimization for a single test case.\n        \"\"\"\n        De = params['De']\n        a = params['a']\n        re = params['re']\n        r_current = params['r0']\n        initial_alpha = params['initial_alpha']\n        c = params['c']\n        rho = params['rho']\n        epsilon = params['epsilon']\n        max_iter = params['max_iter']\n\n        energies = []\n        sample_indices = {0, 1, 2, 4, 8, 16}\n        sampled_energies = []\n        \n        k = 0\n        while k  max_iter:\n            # Calculate energy and gradient at current position\n            V_current = morse_potential(r_current, De, a, re)\n            energies.append(V_current)\n\n            # Sample energy if current iteration index is in the sample set\n            if k in sample_indices:\n                sampled_energies.append(V_current)\n            \n            grad = morse_gradient(r_current, De, a, re)\n\n            # Check for convergence\n            if np.abs(grad) = epsilon:\n                break\n\n            # Backtracking line search to find step size alpha\n            alpha = initial_alpha\n            while True:\n                r_next = r_current - alpha * grad\n                V_next = morse_potential(r_next, De, a, re)\n                \n                # Armijo condition for sufficient decrease\n                if V_next = V_current - c * alpha * grad**2:\n                    break\n                alpha *= rho\n\n            # Update position\n            r_current = r_next\n            k += 1\n\n        # Final state after loop termination\n        r_star = r_current\n        V_star = morse_potential(r_star, De, a, re)\n        \n        # In case max_iter is reached, the last energy hasn't been added\n        if k == max_iter:\n            energies.append(V_star)\n            if k in sample_indices:\n                sampled_energies.append(V_star)\n\n        num_iterations = k\n        \n        # Check if the sequence of energies is monotonically nonincreasing\n        is_monotone = True\n        if len(energies) > 1:\n            # Using a small tolerance for floating point comparisons\n            for i in range(len(energies) - 1):\n                if energies[i+1] > energies[i] + 1e-12: # V(k+1) should be = V(k)\n                    is_monotone = False\n                    break\n        \n        return [r_star, V_star, num_iterations, is_monotone, sampled_energies]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'name': 'Case 1', 'De': 1.0, 'a': 1.5, 're': 1.4, 'r0': 0.2, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-8, 'max_iter': 2000},\n        {'name': 'Case 2', 'De': 1.0, 'a': 1.5, 're': 1.4, 'r0': 1.4, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-12, 'max_iter': 100},\n        {'name': 'Case 3', 'De': 1.0, 'a': 1.5, 're': 1.4, 'r0': 4.0, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-10, 'max_iter': 3000},\n        {'name': 'Case 4', 'De': 2.0, 'a': 3.0, 're': 1.2, 'r0': 0.1, 'initial_alpha': 1.0, 'c': 1.0e-4, 'rho': 0.5, 'epsilon': 1.0e-8, 'max_iter': 5000},\n    ]\n\n    all_results = []\n    for case_params in test_cases:\n        result = run_steepest_descent(case_params)\n        all_results.append(result)\n\n    # Format the final output into a single JSON-like string.\n    result_strings = []\n    for res in all_results:\n        r_f, v_f, n_iter, mono, sampled_e = res\n        \n        # Format numbers to a consistent precision for clean output\n        r_f_str = f\"{r_f:.15f}\"\n        v_f_str = f\"{v_f:.15e}\"\n        sampled_e_str_list = [f\"{e:.15e}\" for e in sampled_e]\n        \n        mono_str = \"true\" if mono else \"false\"\n        sampled_str = f\"[{','.join(sampled_e_str_list)}]\"\n        result_strings.append(f\"[{r_f_str},{v_f_str},{n_iter},{mono_str},{sampled_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "2463075"}, {"introduction": "虽然梯度下降法很强大，但其成功在很大程度上依赖于关键超参数——学习率（或步长）的选择。一个不当的学习率不仅会减慢收敛速度，甚至可能导致算法完全失效。这个练习通过一个看似简单的二次势能面，揭示了梯度下降法一个令人惊讶的陷阱：通过精确计算，你将找到一个特定的恒定步长，它会使优化器在两个点之间无限振荡，永远无法收敛到最小值。[@problem_id:2463052]", "problem": "在计算化学中，通常通过最小化势能面 (PES) 来进行局域几何优化。考虑一个近似于局域二次势阱的二维势能面 (PES)，其在无量纲单位下由下式给出：\n$$\nE(x,y) = \\tfrac{1}{2}\\big(x^{2} + 3\\,y^{2}\\big).\n$$\n应用步长参数 $s$ 为常数的最速下降算法 (SD) 来最小化 $E(x,y)$。根据定义，坐标 $\\mathbf{r}_{k} = (x_{k},y_{k})$ 的最速下降更新公式为：\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s\\,\\nabla E(\\mathbf{r}_{k}).\n$$\n假设初始条件为 $\\mathbf{r}_{0} = (0,y_{0})$，其中 $y_{0} \\neq 0$。\n\n确定常数步长 $s$ 的值（表示为一个精确的简化分数），使得最速下降迭代陷入一个非收敛的两点极限环中，永远在 $(0,y_{0})$ 和 $(0,-y_{0})$ 之间交替。使用无量纲单位，答案中不包含单位。无需四舍五入。", "solution": "所述问题是适定的且科学上是合理的。我们将着手进行推导。\n\n势能面 (PES) 由以下函数给出：\n$$\nE(x,y) = \\frac{1}{2}(x^{2} + 3y^{2})\n$$\n最速下降 (SD) 算法根据以下规则更新坐标 $\\mathbf{r}_{k} = (x_{k}, y_{k})$：\n$$\n\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - s \\nabla E(\\mathbf{r}_{k})\n$$\n其中 $s$ 是常数步长。\n\n首先，我们必须计算能量函数 $\\nabla E(x,y)$ 的梯度。其偏导数为：\n$$\n\\frac{\\partial E}{\\partial x} = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = x\n$$\n$$\n\\frac{\\partial E}{\\partial y} = \\frac{\\partial}{\\partial y} \\left( \\frac{1}{2}x^{2} + \\frac{3}{2}y^{2} \\right) = 3y\n$$\n因此，梯度向量为：\n$$\n\\nabla E(x,y) = \\begin{pmatrix} x \\\\ 3y \\end{pmatrix}\n$$\n将梯度代入最速下降更新规则，我们得到坐标 $(x_{k}, y_{k})$ 的分量递推关系：\n$$\n\\begin{pmatrix} x_{k+1} \\\\ y_{k+1} \\end{pmatrix} = \\begin{pmatrix} x_{k} \\\\ y_{k} \\end{pmatrix} - s \\begin{pmatrix} x_{k} \\\\ 3y_{k} \\end{pmatrix}\n$$\n这得到了两个独立的方程：\n$$\nx_{k+1} = x_{k} - s x_{k} = (1 - s) x_{k}\n$$\n$$\ny_{k+1} = y_{k} - s (3y_{k}) = (1 - 3s) y_{k}\n$$\n初始条件给定为 $\\mathbf{r}_{0} = (0, y_{0})$，其中 $y_{0} \\neq 0$。\n\n让我们分析 $x$ 坐标的行为。当 $x_{0} = 0$ 时，$x$ 的递推关系给出：\n$$\nx_{1} = (1-s)x_{0} = (1-s)(0) = 0\n$$\n通过数学归纳法，可以轻易证明对于所有整数 $k \\ge 0$，$x_{k} = 0$。这与指定的位于 $y$ 轴上的极限环点 $(0, y_{0})$ 和 $(0, -y_{0})$ 一致。\n\n现在，我们分析 $y$ 坐标的行为。问题指出，系统进入一个两点极限环，在 $(0, y_{0})$ 和 $(0, -y_{0})$ 之间交替。这意味着对于偶数 $k$，$\\mathbf{r}_{k} = (0, y_{0})$，对于奇数 $k$，$\\mathbf{r}_{k} = (0, -y_{0})$。\n\n让我们考虑迭代的第一步，从 $k=0$ 到 $k=1$。\n我们从 $\\mathbf{r}_{0} = (0, y_{0})$ 开始。下一个点必须是 $\\mathbf{r}_{1} = (0, -y_{0})$。\n使用 $k=0$ 时 $y$ 的递推关系：\n$$\ny_{1} = (1 - 3s) y_{0}\n$$\n为了使迭代与极限环匹配，我们需要 $y_{1} = -y_{0}$。因此：\n$$\n-y_{0} = (1 - 3s) y_{0}\n$$\n因为给定 $y_{0} \\neq 0$，我们可以将等式两边同时除以 $y_{0}$：\n$$\n-1 = 1 - 3s\n$$\n解此方程求步长 $s$：\n$$\n3s = 1 - (-1) = 2\n$$\n$$\ns = \\frac{2}{3}\n$$\n我们必须验证这个 $s$ 值能够无限期地维持极限环。如果 $s = \\frac{2}{3}$，则 $y$ 的递推关系变为：\n$$\ny_{k+1} = \\left(1 - 3 \\cdot \\frac{2}{3}\\right) y_{k} = (1 - 2) y_{k} = -y_{k}\n$$\n让我们检查迭代序列：\n对于 $k=0$：$\\mathbf{r}_{0} = (0, y_{0})$。\n对于 $k=1$：$y_{1} = -y_{0}$，所以 $\\mathbf{r}_{1} = (0, -y_{0})$。\n对于 $k=2$：$y_{2} = -y_{1} = -(-y_{0}) = y_{0}$，所以 $\\mathbf{r}_{2} = (0, y_{0})$。\n对于 $k=3$：$y_{3} = -y_{2} = -y_{0}$，所以 $\\mathbf{r}_{3} = (0, -y_{0})$。\n点序列为 $(0, y_{0}), (0, -y_{0}), (0, y_{0}), (0, -y_{0}), \\dots$，这正是指定的两点极限环。该算法永远不会收敛到位于 $(0,0)$ 的最小值点。\n\n产生这种行为的常数步长 $s$ 的值为 $\\frac{2}{3}$。", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "2463052"}, {"introduction": "在理解了梯度下降法的基础和潜在 pitfalls 之后，是时候探索如何改进它了。这个高级练习深入探讨了梯度下降最重要的变体之一：动量法。通过对一个狭长山谷状的二次目标函数进行分析，你将从第一性原理出发，推导出普通梯度下降（GD）和重球动量法（Heavy-Ball, HB）的最优参数。这个过程将清晰地揭示动量法为何能有效克服 GD 在病态（ill-conditioned）问题上的“之”字形低效路径，并显著加速收敛。[@problem_id:3186112]", "problem": "考虑严格凸二次目标函数 $f(x,y) = \\frac{1}{2}\\left(a x^{2} + b y^{2}\\right)$，其中 $a  b  0$ 且 $a \\gg b$。其梯度为 $\\nabla f(x,y) = \\left(a x, b y\\right)$，Hessian矩阵为对角矩阵，特征值为 $a$ 和 $b$。你将从第一性原理出发，比较梯度下降法 (GD) 和重球动量法 (HB) 在此目标函数上的表现。\n\n仅从以下迭代算法的定义出发\n$$(\\text{GD})\\quad \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}),$$\n$$(\\text{HB})\\quad \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) + \\beta\\left(\\mathbf{z}_{t} - \\mathbf{z}_{t-1}\\right),$$\n其中 $\\mathbf{z}_{t} = (x_{t}, y_{t})$，推导并分析沿Hessian矩阵的每个特征方向所引发的一维递归关系。\n\n你的任务是：\n1. 对于GD，推导沿特征值 $\\lambda \\in \\{a,b\\}$ 的标量收缩因子，并确定能够最小化集合 $\\{a,b\\}$ 上最坏情况谱半径的学习率 $\\eta$。用 $a$ 和 $b$ 表示你的结果。\n2. 对于HB，推导沿特征值 $\\lambda \\in \\{a,b\\}$ 的标量二阶递归关系，然后选择 $\\eta$ 和 $\\beta$ 以在满足稳定性的条件下最小化集合 $\\{a,b\\}$ 上的最坏情况谱半径。仅使用上述算法定义和基本的线性动力系统推理。将你的最优 $\\eta$ 和 $\\beta$ 表示为关于 $a$ 和 $b$ 的解析闭式表达式，并说明HB所能达到的、作为 $a$ 和 $b$ 函数的相应最小化最坏情况谱半径。\n3. 用文字讨论当 $a \\gg b$ 时出现的过冲和振荡之间的权衡，包括 $\\beta$ 和 $\\eta$ 如何影响狭窄谷地中的轨迹行为，并提出了一个具体的实验：指定 $a \\gg b$ 的 $(a,b)$、初始化 $(x_{0},y_{0})$ 和 $(x_{-1},y_{-1})$，以及在你推导出的最优参数下，你会记录哪些指标来比较GD与HB。\n\n将你的最终答案报告为以 $a$ 和 $b$ 表示的最优重球法参数对 $(\\eta^{\\star}, \\beta^{\\star})$。无需四舍五入。使用 LaTeX 的 $\\texttt{pmatrix}$ 环境将该参数对格式化为 $1 \\times 2$ 的行矩阵。", "solution": "该问题要求分析梯度下降法 (GD) 和重球动量法 (HB) 在严格凸二次目标函数 $f(x,y) = \\frac{1}{2}(ax^2 + by^2)$（其中 $a  b  0$）上的表现。分析将从第一性原理出发，通过研究沿Hessian矩阵特征方向的一维递归关系来进行。\n\n目标函数可以写成向量形式 $f(\\mathbf{z}) = \\frac{1}{2}\\mathbf{z}^T H \\mathbf{z}$，其中 $\\mathbf{z} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$ 且 $H = \\begin{pmatrix} a  0 \\\\ 0  b \\end{pmatrix}$。梯度为 $\\nabla f(\\mathbf{z}) = H\\mathbf{z}$。Hessian矩阵 $H$ 的特征值为 $\\lambda_1 = a$ 和 $\\lambda_2 = b$，对应的特征向量沿坐标轴方向。优化算法的动力学沿这些轴解耦。我们可以通过研究对应于一般特征值 $\\lambda \\in \\{a,b\\}$ 的标量递归来分析收敛性。\n\n### 1. 梯度下降法 (GD) 分析\n\nGD的更新规则为：\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) $$\n代入 $\\nabla f(\\mathbf{z}_t) = H\\mathbf{z}_t$，我们得到：\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_t - \\eta H \\mathbf{z}_t = (I - \\eta H) \\mathbf{z}_t $$\n这是一个线性动力系统。对于 $\\mathbf{z}_t$ 沿对应于 $H$ 的特征值 $\\lambda$ 的特征向量的分量，其标量更新规则为：\n$$ z_{t+1}^{(\\lambda)} = (1 - \\eta \\lambda) z_t^{(\\lambda)} $$\n项 $c(\\eta, \\lambda) = 1 - \\eta\\lambda$ 是该分量的收缩因子。为使迭代方法收敛，其所有模式的收缩因子的模都必须小于 $1$。总体的收敛速度由最大的模决定，即迭代矩阵 $I - \\eta H$ 的谱半径：\n$$ \\rho(I - \\eta H) = \\max_{\\lambda \\in \\{a, b\\}} |1 - \\eta \\lambda| $$\n为了收敛，我们要求 $\\rho  1$，这意味着对于 $\\lambda=a$ 和 $\\lambda=b$ 都有 $|1 - \\eta\\lambda|  1$。即 $-1  1 - \\eta\\lambda  1$，化简得 $0  \\eta\\lambda  2$。由于 $ab0$，此条件必须对 $\\lambda=a$ 成立，从而得到稳定性条件 $0  \\eta  \\frac{2}{a}$。\n\n我们的目标是找到能最小化最坏情况谱半径的学习率 $\\eta$：\n$$ \\eta^{\\star} = \\arg\\min_{\\eta} \\max(|1 - \\eta a|, |1 - \\eta b|) $$\n两个关于 $\\eta$ 的线性函数的最大值的最小值，出现在它们的绝对值相等时。即 $|1 - \\eta a| = |1 - \\eta b|$。由于 $\\eta  0$ 且 $a  b$，有 $1-\\eta a  1-\\eta b$。为使其模相等，我们必须有：\n$$ 1 - \\eta b = -(1 - \\eta a) = \\eta a - 1 $$\n解出 $\\eta$：\n$$ 2 = \\eta a + \\eta b = \\eta(a+b) $$\n$$ \\eta_{GD}^{\\star} = \\frac{2}{a+b} $$\n此 $\\eta$ 值在稳定范围内，因为 $a+b  a$，所以 $\\frac{2}{a+b}  \\frac{2}{a}$。相应的最小谱半径为：\n$$ \\rho_{GD} = \\left|1 - b \\left(\\frac{2}{a+b}\\right)\\right| = \\left|\\frac{a+b-2b}{a+b}\\right| = \\frac{a-b}{a+b} $$\n用条件数 $\\kappa = a/b$ 表示，此即 $\\rho_{GD} = \\frac{\\kappa-1}{\\kappa+1}$。\n\n### 2. 重球 (HB) 动量法分析\n\nHB的更新规则为：\n$$ \\mathbf{z}_{t+1} = \\mathbf{z}_{t} - \\eta \\nabla f(\\mathbf{z}_{t}) + \\beta(\\mathbf{z}_{t} - \\mathbf{z}_{t-1}) $$\n代入 $\\nabla f(\\mathbf{z}_t) = H\\mathbf{z}_t$ 并考虑对应于特征值 $\\lambda$ 的模式的标量递归：\n$$ z_{t+1} = z_t - \\eta \\lambda z_t + \\beta(z_t - z_{t-1}) $$\n$$ z_{t+1} = (1 - \\eta\\lambda + \\beta)z_t - \\beta z_{t-1} $$\n这是一个二阶线性齐次递推关系。其动力学由其特征多项式的根决定：\n$$\n\\mu^2 - (1 - \\eta\\lambda + \\beta)\\mu + \\beta = 0\n$$\n为使系统稳定，两个根 $\\mu_1$ 和 $\\mu_2$ 的模都必须小于 $1$。收敛速度由 $\\rho_\\lambda = \\max(|\\mu_1|, |\\mu_2|)$ 决定。根为：\n$$\n\\mu = \\frac{1 - \\eta\\lambda + \\beta \\pm \\sqrt{(1 - \\eta\\lambda + \\beta)^2 - 4\\beta}}{2}\n$$\n根的性质取决于判别式 $D = (1 - \\eta\\lambda + \\beta)^2 - 4\\beta$。\n若 $D \\ge 0$，根为实数，谱半径为 $\\rho_\\lambda = \\frac{|1 - \\eta\\lambda + \\beta| + \\sqrt{D}}{2}$。\n若 $D  0$，根为一对共轭复数，其模为 $|\\mu| = \\sqrt{\\text{根之积}} = \\sqrt{\\beta}$。谱半径为 $\\rho_\\lambda = \\sqrt{\\beta}$。\n\n为了最小化最坏情况谱半径 $\\max_{\\lambda \\in \\{a,b\\}} \\rho_\\lambda$，我们希望使 $\\lambda=a$ 和 $\\lambda=b$ 的谱半径相等且尽可能小。最优解出现在两个特征值的动力学都处于复数根状态，且位于实数根状态的边界上，即判别式为非正，且两者的谱半径均为 $\\sqrt{\\beta}$。这要求：\n$$ (1 - \\eta\\lambda + \\beta)^2 - 4\\beta \\le 0 \\quad \\text{对于 } \\lambda \\in \\{a,b\\} $$\n$$ |1 - \\eta\\lambda + \\beta| \\le 2\\sqrt{\\beta} $$\n这等价于 $-2\\sqrt{\\beta} \\le 1 - \\eta\\lambda + \\beta \\le 2\\sqrt{\\beta}$，整理得：\n$$\n(1-\\sqrt{\\beta})^2 \\le \\eta\\lambda \\le (1+\\sqrt{\\beta})^2\n$$\n为了最小化 $\\beta$（从而最小化谱半径 $\\sqrt{\\beta}$），我们希望这个区间尽可能小，同时仍然包含 $[\\eta b, \\eta a]$。这可以通过让两个区间的端点匹配来实现：\n$$ \\eta b = (1 - \\sqrt{\\beta})^2 $$\n$$ \\eta a = (1 + \\sqrt{\\beta})^2 $$\n将第二个方程除以第一个方程得到：\n$$ \\frac{a}{b} = \\left(\\frac{1 + \\sqrt{\\beta}}{1 - \\sqrt{\\beta}}\\right)^2 $$\n取平方根并解出 $\\sqrt{\\beta}$：\n$$ \\sqrt{\\frac{a}{b}} = \\frac{1 + \\sqrt{\\beta}}{1 - \\sqrt{\\beta}} \\implies \\sqrt{a}(1-\\sqrt{\\beta}) = \\sqrt{b}(1+\\sqrt{\\beta}) $$\n$$ \\sqrt{a} - \\sqrt{a}\\sqrt{\\beta} = \\sqrt{b} + \\sqrt{b}\\sqrt{\\beta} $$\n$$ \\sqrt{a} - \\sqrt{b} = (\\sqrt{a} + \\sqrt{b})\\sqrt{\\beta} $$\n这得到 $\\sqrt{\\beta^{\\star}} = \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}$。因为 $ab0$，所以我们有 $0  \\sqrt{\\beta^{\\star}}  1$，这是一个有效的参数。最优动量参数为：\n$$ \\beta^{\\star} = \\left(\\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}\\right)^2 $$\n现在，我们使用 $\\eta a = (1+\\sqrt{\\beta})^2$ 来求最优学习率 $\\eta^{\\star}$。首先，我们计算 $1+\\sqrt{\\beta^{\\star}}$：\n$$ 1+\\sqrt{\\beta^{\\star}} = 1 + \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}} = \\frac{(\\sqrt{a}+\\sqrt{b}) + (\\sqrt{a}-\\sqrt{b})}{\\sqrt{a}+\\sqrt{b}} = \\frac{2\\sqrt{a}}{\\sqrt{a}+\\sqrt{b}} $$\n将此代入 $\\eta$ 的方程中：\n$$ \\eta^{\\star} a = \\left(\\frac{2\\sqrt{a}}{\\sqrt{a}+\\sqrt{b}}\\right)^2 = \\frac{4a}{(\\sqrt{a}+\\sqrt{b})^2} $$\n$$ \\eta^{\\star} = \\frac{4}{(\\sqrt{a}+\\sqrt{b})^2} $$\n使用这些最优参数，两种模式的谱半径均为 $\\sqrt{\\beta^{\\star}}$，所以HB的最坏情况谱半径为：\n$$ \\rho_{HB} = \\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}} $$\n用条件数 $\\kappa=a/b$ 表示，此即 $\\rho_{HB} = \\frac{\\sqrt{\\kappa}-1}{\\sqrt{\\kappa}+1}$。这个速率明显优于GD的速率 $\\frac{\\kappa-1}{\\kappa+1}$，特别是在 $\\kappa$ 很大时。\n\n### 3. 讨论与实验提议\n\n当 $a \\gg b$ 时，目标函数 $f(x,y)$ 具有一个细长的窄谷形状。条件数 $\\kappa = a/b$ 很大。$f$ 的等高线是高度拉长的椭圆。梯度在 $x$ 方向上（特征值 $a$）陡峭，在 $y$ 方向上（特征值 $b$）平缓。\n\n对于GD，最优学习率 $\\eta_{GD}^{\\star} = \\frac{2}{a+b} \\approx \\frac{2}{a}$ 非常小，受限于最大特征值 $a$ 以确保稳定性。这个小学习率导致沿平坦谷底（$y$方向）的进展非常缓慢，因为更新量 $- \\eta \\nabla_y f = -\\eta b y$ 极其微小。该算法表现出典型的“之”字形行为，在窄谷上来回振荡，同时沿着谷底缓慢爬向最小值。\n\n对于HB，动量项 $\\beta(\\mathbf{z}_t - \\mathbf{z}_{t-1})$ 充当对过去更新的记忆。对于 $a \\gg b$ 时的最优参数，我们有 $\\beta^{\\star} \\approx (1 - 2\\sqrt{b/a})^2 \\approx 1 - 4\\sqrt{b/a}$，这个值接近于 $1$。这个较高的 $\\beta$ 值确保了更新是在多个步骤上进行平均的。跨越谷地（在 $x$ 方向上）的振荡梯度分量会随着时间的推移趋于相互抵消，而沿着谷底（在 $y$ 方向上）的一致的、较小的梯度分量会累积起来，从而建立速度并加速进程。最优学习率 $\\eta_{HB}^{\\star} = \\frac{4}{(\\sqrt{a}+\\sqrt{b})^2} \\approx \\frac{4}{a}$ 大约是GD的两倍。这个较大的学习率由动量项来稳定。其权衡之处在于潜在的过冲；动量可能导致迭代点“飞过”最小值并以比GD更大的振幅在其周围振荡，但总体轨迹更平滑，收敛速度也快得多。\n\n一个比较这两种方法的具体实验如下：\n- **问题设置**: 设 $a=100$ 和 $b=1$。这给出的条件数为 $\\kappa=100$。目标函数为 $f(x,y) = 50x^2 + 0.5y^2$。\n- **算法参数**:\n  - GD: $\\eta_{GD}^{\\star} = \\frac{2}{100+1} = \\frac{2}{101} \\approx 0.0198$。\n  - HB:\n    - $\\beta^{\\star} = \\left(\\frac{\\sqrt{100}-\\sqrt{1}}{\\sqrt{100}+\\sqrt{1}}\\right)^2 = \\left(\\frac{9}{11}\\right)^2 = \\frac{81}{121} \\approx 0.6694$。\n    - $\\eta^{\\star} = \\frac{4}{(\\sqrt{100}+\\sqrt{1})^2} = \\frac{4}{11^2} = \\frac{4}{121} \\approx 0.0331$。\n- **初始化**: 从一个在陡峭方向上偏离中心、在平缓方向上远离最小值的点开始，例如 $\\mathbf{z}_0 = (x_0, y_0) = (1, 10)$。对于HB，通过设置 $\\mathbf{z}_{-1} = \\mathbf{z}_0$ 来以零速度初始化。\n- **指标**: 两种算法都运行固定的迭代次数（例如，$N=100$）。对于每次迭代 $t=0, 1, \\dots, N$，记录：\n  1. 迭代点的位置 $\\mathbf{z}_t = (x_t, y_t)$。\n  2. 目标函数值 $f(\\mathbf{z}_t)$。\n  3. 梯度的范数 $\\|\\nabla f(\\mathbf{z}_t)\\|_2$。\n- **分析**:\n  - 在 $f(x,y)$ 的二维等高线图上绘制GD和HB的轨迹 $(\\mathbf{z}_0, \\mathbf{z}_1, \\dots, \\mathbf{z}_N)$。这将直观地展示GD的“之”字形行为与HB更平滑、更快的路径。\n  - 在半对数坐标图上（$f$ 的对数 vs. 线性 $t$）绘制两种方法的 $f(\\mathbf{z}_t)$ 与 $t$ 的关系。这将清楚地显示线性收敛率，并证明HB的收敛斜率更陡，表明收敛更快。\n  - 在半对数坐标图上绘制 $\\|\\nabla f(\\mathbf{z}_t)\\|_2$ 与 $t$ 的关系，这为收敛到临界点提供了另一个视角。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{4}{(\\sqrt{a}+\\sqrt{b})^{2}}  \\left(\\frac{\\sqrt{a}-\\sqrt{b}}{\\sqrt{a}+\\sqrt{b}}\\right)^{2} \\end{pmatrix}}\n$$", "id": "3186112"}]}