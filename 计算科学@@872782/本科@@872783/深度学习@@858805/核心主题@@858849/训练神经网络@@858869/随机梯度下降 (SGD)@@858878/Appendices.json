{"hands_on_practices": [{"introduction": "理论知识是基础，但只有通过实践才能真正掌握随机梯度下降（SGD）的精髓。这个练习将引导你完成一个最基本的 SGD 更新步骤。通过在一个简单的二次函数上进行计算，你将亲身体验算法如何利用单个数据点的梯度信息来迭代地调整参数，从而向着最优解迈出第一步。[@problem_id:2206637]", "problem": "一个迭代优化算法被用来寻找一个参数 $x$，以最小化一个成本函数。总成本函数是几个分量函数的平均值：$F(x) = \\frac{1}{N}\\sum_{i=1}^{N} f_i(x)$。在这个具体案例中，分量函数是二次的，由 $f_i(x) = (x - c_i)^2$ 给出，其中常数是 $c_i = i$ (对于 $i = 1, 2, \\dots, 10$)，因此 $N=10$。\n\n优化过程从参数的一个初始猜测值 $x_0$ 开始。在每一步中，新的估计值 $x_{k+1}$ 是通过仅使用一个随机选择的分量函数 $f_j(x)$，从当前估计值 $x_k$ 计算得出的。更新规则定义为：\n$$x_{k+1} = x_k - \\eta \\left( \\frac{d f_j(x)}{dx} \\bigg|_{x=x_k} \\right)$$\n其中 $\\eta$ 是一个称为学习率的常数。\n\n给定初始参数值 $x_0 = 10.0$ 和学习率 $\\eta = 0.1$，计算一次更新步骤后参数 $x_1$ 的值。对于这第一步，使用的分量函数是索引为 $j=5$ 的 $f_j(x)$。", "solution": "我们给定的分量函数形式为 $f_{i}(x) = (x - c_{i})^{2}$，其中 $c_{i} = i$。对于第一次更新，选择的索引是 $j=5$，所以 $f_{5}(x) = (x - 5)^{2}$。\n\n更新规则是\n$$\nx_{k+1} = x_{k} - \\eta \\left.\\frac{d f_{j}(x)}{dx}\\right|_{x=x_{k}}.\n$$\n使用幂法则和链式法则，所选分量函数的导数是\n$$\n\\frac{d f_{5}(x)}{dx} = \\frac{d}{dx}\\left[(x - 5)^{2}\\right] = 2(x - 5).\n$$\n在当前迭代点 $x_{0} = 10$ 处求值得到\n$$\n\\left.\\frac{d f_{5}(x)}{dx}\\right|_{x=10} = 2(10 - 5) = 10.\n$$\n学习率为 $\\eta = 0.1$，更新变为\n$$\nx_{1} = x_{0} - \\eta \\cdot 10 = 10 - 0.1 \\times 10 = 10 - 1 = 9.\n$$\n因此，使用 $f_{5}$ 进行一次更新步骤后，参数值为 $x_{1} = 9$。", "answer": "$$\\boxed{9}$$", "id": "2206637"}, {"introduction": "学习率 $\\eta$ 是 SGD 算法中最关键的超参数之一，它直接决定了学习过程的稳定性和效率。这个练习通过一个反例，生动地展示了当学习率设置过大时可能发生的灾难性后果——算法不仅不会收敛，反而会离最优点越来越远。理解这种发散行为，对于在实践中调试和优化你的模型至关重要。[@problem_id:2206673]", "problem": "在机器学习背景下，我们通常通过最小化损失函数来优化模型的参数。考虑一个具有单个标量参数 $w$ 的简化模型。与单个数据点相关的损失函数由 $L(w) = \\frac{1}{2} c w^2$ 给出，其中最小损失出现在 $w=0$ 处。该参数使用随机梯度下降（SGD）算法进行更新。在步骤 $k$ 处参数的更新规则为 $w_{k+1} = w_k - \\eta \\nabla L(w_k)$，其中 $\\nabla L(w_k)$ 是在 $w_k$ 处计算的损失函数的梯度，$\\eta$ 是学习率。\n\n假设参数的初始值为 $w_0 = 4.0$。模型参数设置为 $c = 0.75$，学习率为 $\\eta = 3.2$。计算经过3次更新步骤后参数 $w$ 的值（即，求 $w_3$）。\n\n将您的最终答案四舍五入到三位有效数字。", "solution": "损失为 $L(w)=\\frac{1}{2} c w^{2}$。其梯度通过微分求得：\n$$\n\\nabla L(w)=\\frac{\\mathrm{d}}{\\mathrm{d}w}\\left(\\frac{1}{2} c w^{2}\\right)=c w.\n$$\nSGD 更新规则为\n$$\nw_{k+1}=w_{k}-\\eta \\nabla L(w_{k})=w_{k}-\\eta c w_{k}=(1-\\eta c)\\,w_{k}.\n$$\n这个线性递推关系的解为\n$$\nw_{k}=(1-\\eta c)^{k} w_{0}.\n$$\n代入 $c=0.75$、$\\eta=3.2$ 和 $w_{0}=4.0$，\n$$\n1-\\eta c=1-(3.2)(0.75)=1-2.4=-1.4,\n$$\n所以\n$$\nw_{3}=(-1.4)^{3}\\cdot 4.0=-10.976.\n$$\n四舍五入到三位有效数字得到 $-11.0$。", "answer": "$$\\boxed{-11.0}$$", "id": "2206673"}, {"introduction": "掌握了基本的更新规则并理解了学习率的重要性后，让我们进入一个更真实的机器学习场景。在这个练习中，你将为一个非线性模型推导 SGD 更新公式，该模型用于预测正数值。这项任务要求你运用向量微积分的知识来计算损失函数相对于模型权重的梯度，这是将 SGD 应用于复杂模型的关键一步。[@problem_id:2206657]", "problem": "在一个用于预测正值的机器学习模型中，预测值 $\\hat{y}$ 与一个 $d$ 维特征向量 $x$ 通过公式 $\\hat{y} = \\exp(w^T x)$ 相关联，其中 $w$ 是一个 $d$ 维权重向量。目标是调整权重 $w$，以使预测值尽可能接近真实的观测值。\n\n模型的性能由一个损失函数来衡量。对于一个包含 $m$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^m$ 的数据集，总损失定义为模型预测值与真实值之间平方差的和：\n$$\nL(w) = \\sum_{i=1}^{m} \\left( \\exp(w^T x_i) - y_i \\right)^2\n$$\n我们使用随机梯度下降 (SGD)，一种迭代优化算法，来最小化这个损失。在每次迭代中，SGD 仅使用单个数据样本来近似计算总损失的梯度。\n\n假设当前的权重向量是 $w$。选取一个数据样本 $(x_k, y_k)$。使用这个样本，以学习率 $\\eta  0$ 执行一个 SGD 更新步骤。推导更新后的权重向量 $w_{new}$ 的表达式。你的最终表达式应该用 $w$、$\\eta$、$x_k$ 和 $y_k$ 来表示。", "solution": "我们考虑选定样本 $(x_{k}, y_{k})$ 的单样本损失，\n$$\n\\ell_{k}(w) = \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right)^{2}.\n$$\n为了执行 SGD 更新，我们需要计算 $\\ell_{k}(w)$ 关于 $w$ 的梯度。令 $f(w) = \\exp(w^{T} x_{k}) - y_{k}$。那么 $\\ell_{k}(w) = f(w)^{2}$，所以根据链式法则，\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 f(w)\\, \\nabla_{w} f(w).\n$$\n接下来，计算 $\\nabla_{w} f(w)$。由于 $f(w) = \\exp(w^{T} x_{k}) - y_{k}$ 且 $\\nabla_{w}(w^{T} x_{k}) = x_{k}$，我们有\n$$\n\\nabla_{w} f(w) = \\exp\\!\\left(w^{T} x_{k}\\right) \\, x_{k}.\n$$\n因此，\n$$\n\\nabla_{w} \\ell_{k}(w) = 2 \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\!\\left(w^{T} x_{k}\\right) x_{k}.\n$$\n学习率为 $\\eta0$ 的 SGD 更新是\n$$\nw_{\\text{new}} = w - \\eta \\nabla_{w} \\ell_{k}(w),\n$$\n得到\n$$\nw_{\\text{new}} = w - 2 \\eta \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right) \\exp\\!\\left(w^{T} x_{k}\\right) x_{k}.\n$$", "answer": "$$\\boxed{w - 2 \\eta \\left(\\exp\\!\\left(w^{T} x_{k}\\right) - y_{k}\\right)\\exp\\!\\left(w^{T} x_{k}\\right) x_{k}}$$", "id": "2206657"}]}