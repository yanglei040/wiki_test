{"hands_on_practices": [{"introduction": "为了真正掌握梯度消失问题，亲眼观察其发生过程并进行量化是很有帮助的。本练习将指导你构建一个诊断工具，用于衡量经典深度S型网络（sigmoid network）中训练停滞的两个关键指标：神经元饱和度和梯度信号衰减。通过这个练习 [@problem_id:3194533]，你将在理论原因（激活函数饱和）和实际效果（训练缓慢）之间建立起具体的联系，从而获得对该问题更直观的理解。", "problem": "给定一个由 logistic sigmoid 激活函数链组成的深度网络。设 logistic sigmoid 激活函数由函数 $\\sigma(z)$ 定义，其中 $z \\in \\mathbb{R}$ 表示一个单元的激活前输入。考虑一个通过深度自编码器的由 $L$ 层组成的一维路径，其中该路径上的前向映射是仿射缩放和 sigmoid 函数的复合。您的任务是：\n\n1) 仅从 logistic sigmoid 的定义和微积分的链式法则出发，论证为什么对于大幅值的预激活 $\\lvert z \\rvert \\gg 0$，导数 $\\sigma^{\\prime}(z)$ 会接近于 $0$，并解释这如何导致沿深层 sigmoid 链的梯度消失。\n\n2) 使用两个指标量化训练停滞：\n- 饱和单元的比例，定义为相对于一个阈值 $t > 0$，满足 $\\lvert z \\rvert \\ge t$ 的预激活 $z$ 的比例。\n- 沿一维路径的梯度范数衰减因子，定义为各层绝对缩放值 $\\lvert a_{\\ell} \\rvert$ 与激活导数 $\\sigma^{\\prime}(z_{\\ell})$ 的乘积，其中 $a_{\\ell} \\in \\mathbb{R}$ 是路径上第 $\\ell$ 层的标量权重，$z_{\\ell} \\in \\mathbb{R}$ 是该层的预激活值。\n\n形式上，对于给定的长度为 $L$ 的路径，定义\n$$\nR \\;=\\; \\prod_{\\ell=1}^{L} \\left(\\lvert a_{\\ell} \\rvert \\cdot \\sigma^{\\prime}(z_{\\ell})\\right),\n$$\n并为一组预激活 $\\{z^{(i)}\\}_{i=1}^{N}$ 定义饱和比例\n$$\n\\text{frac\\_sat} \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} \\mathbf{1}\\left(\\lvert z^{(i)} \\rvert \\ge t\\right),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数。如果 $\\text{frac\\_sat} \\ge s_{\\min}$ 和 $R  \\varepsilon$ 同时成立，则声明一个二元停滞指示器为 $1$，否则为 $0$，其中 $s_{\\min} \\in (0,1]$ 和 $\\varepsilon > 0$ 是给定的阈值。\n\n3) 实现一个程序，对每个测试用例计算：\n- 饱和比例 $\\text{frac\\_sat}$。\n- 梯度衰减因子 $R$。\n- 给定阈值 $s_{\\min}$ 和 $\\varepsilon$ 时的停滞指示器（作为整数）。\n\n所有浮点数结果必须四舍五入到六位小数。停滞指示器必须是 $\\{0,1\\}$ 中的整数。\n\n测试套件：\n每个测试用例 $k$ 提供一个阈值 $t_k$，一个停滞比例阈值 $s_{\\min,k}$，一个梯度阈值 $\\varepsilon_k$，一个路径权重列表 $\\{a_{\\ell}\\}_{\\ell=1}^{L_k}$，一个对应层的路径预激活列表 $\\{z_{\\ell}\\}_{\\ell=1}^{L_k}$，以及一个用于计算饱和比例的全网络预激活快照 $\\{z^{(i)}\\}_{i=1}^{N_k}$。使用以下四种情况：\n\n- 情况 1（正常路径，大部分未饱和）：\n  - $t = 4.0$, $s_{\\min} = 0.6$, $\\varepsilon = 10^{-4}$，\n  - 路径权重 $[\\,1.0,\\,1.0,\\,1.0\\,]$，\n  - 路径预激活 $[\\, -0.5,\\, 0.0,\\, 0.3 \\,]$，\n  - 网络快照 $[\\, -0.5,\\, 0.1,\\, 0.3,\\, -1.2,\\, 2.0,\\, -3.0,\\, 0.0,\\, 0.8,\\, 5.0 \\,]$。\n\n- 情况 2（强饱和，明显停滞）：\n  - $t = 4.0$, $s_{\\min} = 0.6$, $\\varepsilon = 10^{-4}$，\n  - 路径权重 $[\\,1.0,\\, 0.9,\\, 1.1,\\, 1.0,\\, 0.95\\,]$，\n  - 路径预激活 $[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0 \\,]$，\n  - 网络快照 $[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0,\\, 0.2,\\, -0.1,\\, 5.0,\\, -4.5,\\, 4.1 \\,]$。\n\n- 情况 3（混合饱和，梯度小但饱和比例低）：\n  - $t = 4.0$, $s_{\\min} = 0.5$, $\\varepsilon = 10^{-4}$，\n  - 路径权重 $[\\,0.8,\\, 0.7,\\, 0.9,\\, 0.85\\,]$，\n  - 路径预激活 $[\\, 0.0,\\, 5.5,\\, -0.2,\\, 4.2 \\,]$，\n  - 网络快照 $[\\, -0.1,\\, 5.5,\\, 0.0,\\, -0.2,\\, 4.2,\\, 0.3,\\, -0.7,\\, 0.6 \\,]$。\n\n- 情况 4（处于阈值边界，饱和度高但因权重较大衰减有限）：\n  - $t = 4.0$, $s_{\\min} = 0.9$, $\\varepsilon = 10^{-4}$，\n  - 路径权重 $[\\,3.0,\\, 3.0\\,]$，\n  - 路径预激活 $[\\, 4.0,\\, -4.0 \\,]$，\n  - 网络快照 $[\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0 \\,]$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个逗号分隔的用例结果列表，并用方括号括起来。每个用例结果本身必须是一个形式为 $[\\,\\text{frac\\_sat},\\, R,\\, \\text{stall}\\,]$ 的列表，其中两个浮点值四舍五入到六位小数，停滞指示器为一个整数。例如：\n\"[ [0.123456,0.000789,1],[0.000000,0.015625,0],... ]\"（不带额外空格也可以接受）。不应打印任何其他文本。", "solution": "该问题是有效的，因为它科学上基于深度学习的原理，在数学上是适定的，并为所需的计算提供了一套完整且一致的数据。要求从 logistic sigmoid 函数的定义出发，虽然没有明确提供其公式，但合理地假设了对这个基本函数 $\\sigma(z) = (1 + e^{-z})^{-1}$ 的了解，这在该领域是标准知识。\n\n分析分为两部分进行。首先，从第一性原理推导出梯度消失现象的概念性论证。其次，为每个测试用例计算指定的指标以量化训练停滞。\n\n**1. Sigmoid 激活函数下的梯度消失现象**\n\nlogistic sigmoid 激活函数定义为：\n$$\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n其中 $z$ 是神经元的预激活输入。使用链式法则，其关于 $z$ 的导数为：\n$$\n\\sigma^{\\prime}(z) = \\frac{d}{dz} \\left( (1 + e^{-z})^{-1} \\right) = -1 \\cdot (1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n$$\n这个导数的表达式可以改写为用 sigmoid 函数本身表示：\n$$\n\\sigma^{\\prime}(z) = \\frac{1}{1 + e^{-z}} \\cdot \\frac{e^{-z}}{1 + e^{-z}} = \\frac{1}{1 + e^{-z}} \\cdot \\frac{(1 + e^{-z}) - 1}{1 + e^{-z}} = \\sigma(z) \\left( 1 - \\sigma(z) \\right)\n$$\n为了理解在大幅值预激活下的行为，我们考察 $\\sigma(z)$ 和 $\\sigma^{\\prime}(z)$ 的极限：\n- 当 $z \\to \\infty$ 时，$e^{-z} \\to 0$。因此，$\\sigma(z) \\to \\frac{1}{1+0} = 1$。导数变为 $\\sigma^{\\prime}(z) \\to \\sigma(z)(1 - \\sigma(z)) \\to 1(1-1) = 0$。\n- 当 $z \\to -\\infty$ 时，$e^{-z} \\to \\infty$。因此，$\\sigma(z) \\to \\lim_{u \\to \\infty} \\frac{1}{1+u} = 0$。导数变为 $\\sigma^{\\prime}(z) \\to \\sigma(z)(1 - \\sigma(z)) \\to 0(1-0) = 0$。\n\n在这两种情况下，对于 $\\lvert z \\rvert \\gg 0$，sigmoid 函数饱和（其输出接近 $0$ 或 $1$），其导数 $\\sigma^{\\prime}(z)$ 接近 $0$。导数的最大值出现在 $z=0$ 处，此时 $\\sigma^{\\prime}(0) = \\sigma(0)(1-\\sigma(0)) = 0.5 \\cdot 0.5 = 0.25$。对于任何非零的 $z$，$\\sigma^{\\prime}(z)  0.25$。\n\n现在，考虑一个深度网络。损失函数 $\\mathcal{L}$ 对早期层中某个参数（例如权重或偏置）的梯度是通过链式法则计算的，这涉及到将梯度逐层反向相乘。让我们分析沿给定一维路径的梯度流。第 $\\ell+1$ 层的预激活是第 $\\ell$ 层激活 $h_{\\ell} = \\sigma(z_{\\ell})$ 通过一个缩放 $a_{\\ell+1}$ 的函数。因此，$z_{\\ell+1}$ 对 $z_{\\ell}$ 的偏导数为：\n$$\n\\frac{\\partial z_{\\ell+1}}{\\partial z_{\\ell}} = \\frac{\\partial z_{\\ell+1}}{\\partial h_{\\ell}} \\frac{\\partial h_{\\ell}}{\\partial z_{\\ell}} = a_{\\ell+1} \\cdot \\sigma^{\\prime}(z_{\\ell})\n$$\n损失对某层 $k$ 中预激活 $z_k$ 的梯度从最后一层 $L$ 传播回来：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial z_k} = \\frac{\\partial \\mathcal{L}}{\\partial z_L} \\frac{\\partial z_L}{\\partial z_{L-1}} \\frac{\\partial z_{L-1}}{\\partial z_{L-2}} \\cdots \\frac{\\partial z_{k+1}}{\\partial z_k} = \\frac{\\partial \\mathcal{L}}{\\partial z_L} \\prod_{\\ell=k}^{L-1} \\frac{\\partial z_{\\ell+1}}{\\partial z_{\\ell}} = \\frac{\\partial \\mathcal{L}}{\\partial z_L} \\prod_{\\ell=k}^{L-1} \\left(a_{\\ell+1} \\sigma^{\\prime}(z_{\\ell})\\right)\n$$\n问题中定义为 $R$ 的项 $\\prod_{\\ell=1}^{L} (\\lvert a_{\\ell} \\rvert \\cdot \\sigma^{\\prime}(z_{\\ell}))$ 是衡量整个路径上该梯度信号传播幅度的度量。由于每个项 $\\sigma^{\\prime}(z_{\\ell})$ 最多为 $0.25$，并且对于饱和单元接近于 $0$，这个乘积对于深度网络（大的 $L$）可能变得极小。梯度在反向传播到早期层时其幅度的这种指数级减小就是梯度消失问题。它会严重减慢或停滞早期层的训练，因为它们的参数更新变得微不足道。\n\n**2. 测试用例的量化分析**\n\n对四个测试用例中的每一个都执行了所需的计算。计算所需的函数是 $\\sigma^{\\prime}(z) = \\sigma(z)(1-\\sigma(z))$，其中 $\\sigma(z) = (1+e^{-z})^{-1}$。\n\n**情况 1：**\n- 参数：$t=4.0$, $s_{\\min}=0.6$, $\\varepsilon=10^{-4}$。\n- 网络快照 $\\{z^{(i)}\\}_{i=1}^{9}$: $[\\, -0.5,\\, 0.1,\\, 0.3,\\, -1.2,\\, 2.0,\\, -3.0,\\, 0.0,\\, 0.8,\\, 5.0 \\,]$。\n- $\\lvert z^{(i)} \\rvert \\ge 4.0$ 的单元数量为 $1$ (对于 $z=5.0$）。\n- $\\text{frac\\_sat} = 1/9 \\approx 0.111111$。\n- 路径细节：权重 $\\{a_{\\ell}\\}=[\\,1.0,\\,1.0,\\,1.0\\,]$，预激活 $\\{z_{\\ell}\\}=[\\, -0.5,\\, 0.0,\\, 0.3 \\,]$。\n- 导数：$\\sigma^{\\prime}(-0.5) \\approx 0.235004$，$\\sigma^{\\prime}(0.0) = 0.25$，$\\sigma^{\\prime}(0.3) \\approx 0.244458$。\n- $R = (\\lvert 1.0 \\rvert \\cdot \\sigma^{\\prime}(-0.5)) \\cdot (\\lvert 1.0 \\rvert \\cdot \\sigma^{\\prime}(0.0)) \\cdot (\\lvert 1.0 \\rvert \\cdot \\sigma^{\\prime}(0.3)) \\approx 0.235004 \\cdot 0.25 \\cdot 0.244458 \\approx 0.014365$。\n- 停滞条件：$\\text{frac\\_sat} \\ge s_{\\min}$ ($0.111111 \\ge 0.6$) 为假。停滞指示器为 $0$。\n- 结果：$[\\, 0.111111, 0.014365, 0 \\,]$。\n\n**情况 2：**\n- 参数：$t=4.0$, $s_{\\min}=0.6$, $\\varepsilon=10^{-4}$。\n- 网络快照 $\\{z^{(i)}\\}_{i=1}^{10}$: $[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0,\\, 0.2,\\, -0.1,\\, 5.0,\\, -4.5,\\, 4.1 \\,]$。\n- $\\lvert z^{(i)} \\rvert \\ge 4.0$ 的单元数量为 $8$。\n- $\\text{frac\\_sat} = 8/10 = 0.8$。\n- 路径细节：权重 $\\{a_{\\ell}\\}=[\\,1.0,\\, 0.9,\\, 1.1,\\, 1.0,\\, 0.95\\,]$，预激活 $\\{z_{\\ell}\\}=[\\, 8.0,\\, -9.0,\\, 7.5,\\, -6.0,\\, 10.0 \\,]$。\n- 对于这些大幅值的预激活，导数非常小：$\\sigma^{\\prime}(8.0) \\approx 3.35 \\times 10^{-4}$，$\\sigma^{\\prime}(-9.0) \\approx 1.23 \\times 10^{-4}$，$\\sigma^{\\prime}(7.5) \\approx 5.53 \\times 10^{-4}$，$\\sigma^{\\prime}(-6.0) \\approx 2.47 \\times 10^{-3}$，$\\sigma^{\\prime}(10.0) \\approx 4.54 \\times 10^{-5}$。\n- $R = (\\lvert 1.0 \\rvert \\sigma^{\\prime}(8.0)) \\cdot (\\lvert 0.9 \\rvert \\sigma^{\\prime}(-9.0)) \\cdot (\\lvert 1.1 \\rvert \\sigma^{\\prime}(7.5)) \\cdot (\\lvert 1.0 \\rvert \\sigma^{\\prime}(-6.0)) \\cdot (\\lvert 0.95 \\rvert \\sigma^{\\prime}(10.0)) \\approx 2.408 \\times 10^{-18}$，四舍五入为 $0.000000$。\n- 停滞条件：$\\text{frac\\_sat} \\ge s_{\\min}$ ($0.8 \\ge 0.6$) 为真。$R  \\varepsilon$ ($0.0  10^{-4}$) 为真。两者都为真，所以停滞指示器为 $1$。\n- 结果：$[\\, 0.800000, 0.000000, 1 \\,]$。\n\n**情况 3：**\n- 参数：$t=4.0$, $s_{\\min}=0.5$, $\\varepsilon=10^{-4}$。\n- 网络快照 $\\{z^{(i)}\\}_{i=1}^{8}$: $[\\, -0.1,\\, 5.5,\\, 0.0,\\, -0.2,\\, 4.2,\\, 0.3,\\, -0.7,\\, 0.6 \\,]$。\n- $\\lvert z^{(i)} \\rvert \\ge 4.0$ 的单元数量为 $2$ (对于 $z=5.5, 4.2$）。\n- $\\text{frac\\_sat} = 2/8 = 0.25$。\n- 路径细节：权重 $\\{a_{\\ell}\\}=[\\,0.8,\\, 0.7,\\, 0.9,\\, 0.85\\,]$，预激活 $\\{z_{\\ell}\\}=[\\, 0.0,\\, 5.5,\\, -0.2,\\, 4.2 \\,]$。\n- 导数：$\\sigma^{\\prime}(0.0) = 0.25$，$\\sigma^{\\prime}(5.5) \\approx 0.004073$，$\\sigma^{\\prime}(-0.2) \\approx 0.247516$，$\\sigma^{\\prime}(4.2) \\approx 0.014696$。\n- $R = (\\lvert 0.8 \\rvert \\sigma^{\\prime}(0.0)) \\cdot (\\lvert 0.7 \\rvert \\sigma^{\\prime}(5.5)) \\cdot (\\lvert 0.9 \\rvert \\sigma^{\\prime}(-0.2)) \\cdot (\\lvert 0.85 \\rvert \\sigma^{\\prime}(4.2)) \\approx 0.000002$。\n- 停滞条件：$\\text{frac\\_sat} \\ge s_{\\min}$ ($0.25 \\ge 0.5$) 为假。停滞指示器为 $0$。\n- 结果：$[\\, 0.250000, 0.000002, 0 \\,]$。\n\n**情况 4：**\n- 参数：$t=4.0$, $s_{\\min}=0.9$, $\\varepsilon=10^{-4}$。\n- 网络快照 $\\{z^{(i)}\\}_{i=1}^{10}$: $[\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0,\\, 4.0,\\, -4.0 \\,]$。\n- 所有 $10$ 个单元的 $\\lvert z^{(i)} \\rvert = 4.0$，因此都满足 $\\lvert z^{(i)} \\rvert \\ge 4.0$。\n- $\\text{frac\\_sat} = 10/10 = 1.0$。\n- 路径细节：权重 $\\{a_{\\ell}\\}=[\\,3.0,\\, 3.0\\,]$，预激活 $\\{z_{\\ell}\\}=[\\, 4.0,\\, -4.0 \\,]$。\n- 导数：$\\sigma^{\\prime}(4.0) = \\sigma^{\\prime}(-4.0) \\approx 0.017663$。\n- $R = (\\lvert 3.0 \\rvert \\cdot \\sigma^{\\prime}(4.0)) \\cdot (\\lvert 3.0 \\rvert \\cdot \\sigma^{\\prime}(-4.0)) \\approx (3.0 \\cdot 0.017663)^2 \\approx 0.052988^2 \\approx 0.002808$。\n- 停滞条件：$\\text{frac\\_sat} \\ge s_{\\min}$ ($1.0 \\ge 0.9$) 为真。$R  \\varepsilon$ ($0.002808  0.0001$) 为假。停滞指示器为 $0$。\n- 结果：$[\\, 1.000000, 0.002808, 0 \\,]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the vanishing gradient problem for the given test cases.\n    It calculates the saturated fraction, gradient decay factor, and a stall indicator.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"t\": 4.0, \"s_min\": 0.6, \"epsilon\": 1.0e-4,\n            \"path_weights\": [1.0, 1.0, 1.0],\n            \"path_pre_activations\": [-0.5, 0.0, 0.3],\n            \"network_snapshot\": [-0.5, 0.1, 0.3, -1.2, 2.0, -3.0, 0.0, 0.8, 5.0]\n        },\n        {\n            \"t\": 4.0, \"s_min\": 0.6, \"epsilon\": 1.0e-4,\n            \"path_weights\": [1.0, 0.9, 1.1, 1.0, 0.95],\n            \"path_pre_activations\": [8.0, -9.0, 7.5, -6.0, 10.0],\n            \"network_snapshot\": [8.0, -9.0, 7.5, -6.0, 10.0, 0.2, -0.1, 5.0, -4.5, 4.1]\n        },\n        {\n            \"t\": 4.0, \"s_min\": 0.5, \"epsilon\": 1.0e-4,\n            \"path_weights\": [0.8, 0.7, 0.9, 0.85],\n            \"path_pre_activations\": [0.0, 5.5, -0.2, 4.2],\n            \"network_snapshot\": [-0.1, 5.5, 0.0, -0.2, 4.2, 0.3, -0.7, 0.6]\n        },\n        {\n            \"t\": 4.0, \"s_min\": 0.9, \"epsilon\": 1.0e-4,\n            \"path_weights\": [3.0, 3.0],\n            \"path_pre_activations\": [4.0, -4.0],\n            \"network_snapshot\": [4.0, -4.0, 4.0, -4.0, 4.0, -4.0, 4.0, -4.0, 4.0, -4.0]\n        }\n    ]\n\n    def sigmoid_derivative(z):\n        \"\"\"\n        Calculates the derivative of the logistic sigmoid function.\n        sigma'(z) = sigma(z) * (1 - sigma(z))\n        \"\"\"\n        # To avoid overflow in exp(-z) for large negative z, handle cases.\n        # However, numpy's exp is robust enough for the given inputs.\n        s = 1.0 / (1.0 + np.exp(-z))\n        return s * (1.0 - s)\n\n    results = []\n    for case in test_cases:\n        t = case[\"t\"]\n        s_min = case[\"s_min\"]\n        epsilon = case[\"epsilon\"]\n        path_weights = case[\"path_weights\"]\n        path_pre_activations = case[\"path_pre_activations\"]\n        network_snapshot = case[\"network_snapshot\"]\n\n        # 1. Calculate the saturated fraction (frac_sat)\n        N = len(network_snapshot)\n        saturated_count = sum(1 for z in network_snapshot if abs(z) = t)\n        frac_sat = saturated_count / N if N  0 else 0.0\n\n        # 2. Calculate the gradient decay factor (R)\n        R = 1.0\n        for a_ell, z_ell in zip(path_weights, path_pre_activations):\n            R *= abs(a_ell) * sigmoid_derivative(z_ell)\n        \n        # 3. Determine the stall indicator\n        is_stalled = (frac_sat = s_min) and (R  epsilon)\n        stall_indicator = 1 if is_stalled else 0\n        \n        # Collect results for this case\n        results.append([frac_sat, R, stall_indicator])\n\n    # Format the final output string\n    case_strings = []\n    for res in results:\n        # Round floats to six decimal places for output\n        frac_sat_str = f\"{res[0]:.6f}\"\n        R_str = f\"{res[1]:.6f}\"\n        stall_str = str(res[2])\n        case_strings.append(f\"[{frac_sat_str},{R_str},{stall_str}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3194533"}, {"introduction": "在上一练习的基础上，我们现在来研究在网络的关键部分——输出层，梯度消失问题是如何被加剧或缓解的。这个练习 [@problem_id:3194463] 要求进行一项理论推导，比较均方误差（Mean Squared Error, MSE）损失和交叉熵（Cross-Entropy）损失在一个S型输出神经元上的表现。它将揭示为什么精心选择损失函数对于维持健康的梯度信号至关重要，尤其是在模型“自信地犯错”时，这对于有效学习至关重要。", "problem": "考虑一个二元分类神经元，其预激活值为 $z \\in \\mathbb{R}$，激活值为 $a = \\sigma(z)$，其中 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$，目标值为 $y \\in \\{0, 1\\}$。在输出层使用两种常见的损失函数：均方误差 (MSE) 和交叉熵。均方误差 (MSE) 损失定义为 $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$。交叉熵损失定义为 $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$。请仅使用上述定义和基础微积分（特别是链式法则），推导出 $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z}$ 和 $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z}$ 关于 $a$、$y$ 和 $z$ 的表达式。然后，分析这些梯度在 $|z|$ 很大的饱和区域中的行为。你的推导过程应从 $\\sigma(z)$ 的确切定义开始，而不是假定任何预先引用的导数公式。根据你的推导和分析，以下哪个陈述最准确？\n\nA. 对于均方误差 (MSE)，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，因此当 $|z|$ 很大时，由于 $\\sigma^{\\prime}(z) \\approx 0$，梯度趋于消失。对于交叉熵，梯度简化为 $\\sigma(z) - y$，它不包含额外的因子 $\\sigma^{\\prime}(z)$，使得当 $|z|$ 很大时，梯度消失问题不那么严重。\n\nB. 对于交叉熵，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，这使得梯度消失问题比均方误差 (MSE) 更严重，而对于均方误差，梯度简化为 $\\sigma(z) - y$。\n\nC. 均方误差 (MSE) 和交叉熵都产生相同的关于 $z$ 的梯度，即 $\\sigma(z) - y$，因此当 $|z|$ 很大时，在梯度消失行为上没有差异。\n\nD. 当 $|z|$ 很大时，导数 $\\sigma^{\\prime}(z)$ 近似为常数，所以均方误差 (MSE) 和交叉熵在输出层都不会遇到梯度消失问题。\n\n选择唯一最佳选项。", "solution": "在尝试解答之前，将首先验证问题陈述的科学合理性、自洽性和清晰度。\n\n### 步骤1：提取已知条件\n-   考虑一个二元分类神经元。\n-   预激活值：$z \\in \\mathbb{R}$。\n-   激活函数：$a = \\sigma(z)$，其中 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$。\n-   目标标签：$y \\in \\{0, 1\\}$。\n-   均方误差 (MSE) 损失：$L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$。\n-   交叉熵 (CE) 损失：$L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$。\n-   任务是推导梯度 $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z}$ 和 $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z}$ 并分析它们在 $|z|$ 很大时的行为。\n-   推导必须仅使用所提供的定义和基础微积分，不得假定任何预先引用的导数公式。\n\n### 步骤2：使用提取的已知条件进行验证\n问题陈述是有效的。\n-   **科学依据：** sigmoid激活函数、均方误差损失和二元交叉熵损失的定义在机器学习和神经网络领域是标准和基础的。该问题探讨的是一个公认的现象——梯度消失。\n-   **良态问题：** 问题定义清晰，并提供了推导所需量和进行分析所需的所有信息。通过标准微积分可以得到唯一确定的答案。\n-   **客观性：** 问题使用精确的数学语言表述，没有歧义或主观论断。\n\n该问题不违反任何无效性标准。这是一个关于神经网络组件微分学的、形式良好且标准化的练习题。\n\n### 步骤3：推导与分析\n\n问题的核心是应用链式法则来求损失函数 $L$ 关于预激活值 $z$ 的梯度。依赖链为 $z \\rightarrow a \\rightarrow L$。因此，梯度的一般形式是：\n$$\n\\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z}\n$$\n项 $\\frac{\\partial a}{\\partial z}$ 是激活函数 $\\sigma'(z)$ 的导数。我们必须按照规定从第一性原理推导它。\n\n**1. Sigmoid 函数的导数**\n\nSigmoid 函数是 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}} = (1 + e^{-z})^{-1}$。\n使用幂法则和链式法则，它关于 $z$ 的导数是：\n$$\n\\frac{\\partial a}{\\partial z} = \\sigma'(z) = \\frac{d}{dz} (1 + e^{-z})^{-1} = -1 \\cdot (1 + e^{-z})^{-2} \\cdot \\frac{d}{dz}(1 + e^{-z})\n$$\n内部项的导数是 $\\frac{d}{dz}(1 + e^{-z}) = -e^{-z}$。将其代回，我们得到：\n$$\n\\sigma'(z) = - (1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\frac{e^{-z}}{(1 + e^{-z})^2}\n$$\n为了用更常见的形式表示，我们可以对表达式进行变换：\n$$\n\\sigma'(z) = \\left( \\frac{1}{1 + e^{-z}} \\right) \\cdot \\left( \\frac{e^{-z}}{1 + e^{-z}} \\right)\n$$\n第一项是 $\\sigma(z)$。对于第二项，我们可以写成：\n$$\n\\frac{e^{-z}}{1 + e^{-z}} = \\frac{1 + e^{-z} - 1}{1 + e^{-z}} = 1 - \\frac{1}{1 + e^{-z}} = 1 - \\sigma(z)\n$$\n因此，sigmoid 函数的导数是：\n$$\n\\sigma'(z) = \\sigma(z) (1 - \\sigma(z))\n$$\n由于 $a = \\sigma(z)$，我们也可以将其写为 $\\dfrac{\\partial a}{\\partial z} = a(1-a)$。\n\n**2. 均方误差 (MSE) 损失的梯度**\n\nMSE 损失是 $L_{\\text{MSE}} = \\dfrac{1}{2} (a - y)^{2}$。\n首先，我们求损失关于激活值 $a$ 的导数：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{2} (a - y)^{2} \\right] = 2 \\cdot \\frac{1}{2} (a - y) \\cdot 1 = a - y\n$$\n现在，应用链式法则求关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = \\frac{\\partial L_{\\text{MSE}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = (a - y) \\cdot \\sigma'(z)\n$$\n代入 $\\sigma'(z)$ 的表达式：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = (a - y) a (1-a)\n$$\n或者，用 $\\sigma(z)$ 表示：\n$$\n\\frac{\\partial L_{\\text{MSE}}}{\\partial z} = (\\sigma(z) - y) \\sigma'(z)\n$$\n\n**3. 交叉熵 (CE) 损失的梯度**\n\nCE 损失是 $L_{\\text{CE}} = - \\left[ y \\log(a) + (1 - y) \\log(1 - a) \\right]$。\n首先，我们求损失关于激活值 $a$ 的导数：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ -y \\log(a) - (1-y) \\log(1-a) \\right] = - \\left[ \\frac{y}{a} + (1-y) \\frac{-1}{1-a} \\right]\n$$\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a} = \\frac{-y(1-a) + a(1-y)}{a(1-a)} = \\frac{-y + ay + a - ay}{a(1-a)} = \\frac{a-y}{a(1-a)}\n$$\n现在，应用链式法则求关于 $z$ 的梯度：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\frac{\\partial L_{\\text{CE}}}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} = \\left( \\frac{a-y}{a(1-a)} \\right) \\cdot (a(1-a))\n$$\n$a(1-a)$ 项相互抵消，留下一个非常简洁的结果：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = a - y\n$$\n或者，用 $\\sigma(z)$ 表示：\n$$\n\\frac{\\partial L_{\\text{CE}}}{\\partial z} = \\sigma(z) - y\n$$\n\n**4. 梯度行为分析**\n\n问题要求分析在 $|z|$ 很大的饱和区域中的行为。\n-   当 $z \\to \\infty$ 时，$e^{-z} \\to 0$，所以 $a = \\sigma(z) \\to \\dfrac{1}{1+0} = 1$。\n-   当 $z \\to -\\infty$ 时，$e^{-z} \\to \\infty$，所以 $a = \\sigma(z) \\to \\dfrac{1}{1+\\infty} \\to 0$。\n\n让我们在这些区域中考察 sigmoid 导数 $\\sigma'(z) = a(1-a)$：\n-   当 $z \\to \\infty$ 时，$a \\to 1$，所以 $\\sigma'(z) \\to 1(1-1) = 0$。\n-   当 $z \\to -\\infty$ 时，$a \\to 0$，所以 $\\sigma'(z) \\to 0(1-0) = 0$。\n在两个饱和区域中，$\\sigma'(z)$ 都趋近于零。\n\n现在，让我们分析两种损失的梯度：\n-   **MSE 梯度：** $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z} = (a - y) \\sigma'(z)$。因为这个梯度与 $\\sigma'(z)$ 成正比，无论误差 $(a-y)$ 的大小如何，当 $|z|$ 变大时它都将趋近于 $0$。这就是**梯度消失问题**。例如，如果目标是 $y=0$ 但神经元在 $z \\gg 0$ 时饱和（因此 $a \\approx 1$），误差 $(a-y)$ 很大（$\\approx 1$），但由于 $\\sigma'(z) \\approx 0$，梯度将接近于 0。学习将变得极其缓慢。\n\n-   **CE 梯度：** $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z} = a - y$。这个梯度*不*包含 $\\sigma'(z)$ 项。让我们考虑同样的“置信地错误”情况：$y=0$ 且 $a \\approx 1$。梯度约为 $1-0=1$，这是更新参数的强信号。如果 $y=1$ 且 $a \\approx 0$，梯度约为 $0-1 = -1$，同样是一个强信号。只有当预测正确时，即 $a \\approx y$ 时，梯度才会变小。因此，将交叉熵损失与 sigmoid 激活函数相结合，在很大程度上缓解了输出层的梯度消失问题。\n\n### 逐项评估选项\n\n**A. 对于均方误差 (MSE)，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，因此当 $|z|$ 很大时，由于 $\\sigma^{\\prime}(z) \\approx 0$，梯度趋于消失。对于交叉熵，梯度简化为 $\\sigma(z) - y$，它不包含额外的因子 $\\sigma^{\\prime}(z)$，使得当 $|z|$ 很大时，梯度消失问题不那么严重。**\n-   我们对 MSE 的推导得出了 $\\dfrac{\\partial L_{\\text{MSE}}}{\\partial z} = (\\sigma(z)-y)\\sigma'(z)$。我们的分析证实，由于 $\\sigma'(z)$ 项，当 $|z|$ 很大时，该梯度会消失。这部分是正确的。\n-   我们对 CE 的推导得出了 $\\dfrac{\\partial L_{\\text{CE}}}{\\partial z} = \\sigma(z)-y$。我们的分析证实，这种形式避免了由 $\\sigma'(z)$ 项引起的梯度消失。这部分也是正确的。\n-   **结论：正确。**\n\n**B. 对于交叉熵，关于 $z$ 的梯度形式为 $(\\sigma(z) - y)\\,\\sigma^{\\prime}(z)$，这使得梯度消失问题比均方误差 (MSE) 更严重，而对于均方误差，梯度简化为 $\\sigma(z) - y$。**\n-   该陈述对调了 MSE 和 CE 的梯度表达式。根据我们的推导，这在事实上是错误的。\n-   **结论：错误。**\n\n**C. 均方误差 (MSE) 和交叉熵都产生相同的关于 $z$ 的梯度，即 $\\sigma(z) - y$，因此当 $|z|$ 很大时，在梯度消失行为上没有差异。**\n-   我们的推导表明梯度是不同的：MSE 的梯度是 $(\\sigma(z) - y)\\sigma'(z)$，而 CE 的梯度是 $\\sigma(z) - y$。因此，该陈述是错误的。\n-   **结论：错误。**\n\n**D. 当 $|z|$ 很大时，导数 $\\sigma^{\\prime}(z)$ 近似为常数，所以均方误差 (MSE) 和交叉熵在输出层都不会遇到梯度消失问题。**\n-   这个前提是错误的。我们的分析表明，当 $|z| \\to \\infty$ 时，$\\sigma'(z) \\to 0$。它不是一个非零常数。\n-   结论也是错误的，因为 MSE 确实会遭受梯度消失问题，这正是因为 $\\sigma'(z) \\to 0$。\n-   **结论：错误。**\n\n基于严谨的推导，陈述 A 是对情况的唯一准确描述。", "answer": "$$\\boxed{A}$$", "id": "3194463"}, {"introduction": "虽然更换损失函数能解决输出层的问题，但对于真正深层的网络，我们需要一个更根本的解决方案来处理内部隐藏层的梯度消失。这个练习 [@problem_id:3194508] 将探讨一种现代且强大的方法：采用精巧的权重初始化方案。通过理论推导和蒙特卡洛模拟，你将对比He初始化与Xavier初始化在ReLU网络中的效果，并证明前者是如何通过在正向和反向传播中精确地保持信号方差来从一开始就预防梯度消失的。", "problem": "考虑一个深度为 $L$、使用整流线性单元 (ReLU) 非线性的全连接前馈神经网络。设层索引为 $l \\in \\{1,\\dots,L\\}$，每层的宽度（单元数）为 $n$，输入批量大小为 $B$。将第 $l$ 层的激活后值表示为 $x_l \\in \\mathbb{R}^{B \\times n}$，激活前值表示为 $z_l \\in \\mathbb{R}^{B \\times n}$。前向传播遵循规则 $z_l = x_{l-1} W_l$ 和 $x_l = \\phi(z_l)$，其中 $\\phi$ 是 ReLU 函数，逐元素定义为 $\\phi(u) = \\max(0,u)$，而 $W_l \\in \\mathbb{R}^{n \\times n}$ 是第 $l$ 层的权重矩阵。假设 $x_0$ 的条目是独立同分布 (i.i.d.) 的，服从零均值和单位方差的高斯分布。在初始化时，假设每个 $W_l$ 的条目是独立同分布的、零均值的，并且独立于 $x_{l-1}$。考虑两种初始化方案：He 初始化，其方差为 $\\mathrm{Var}(W_{l,ij}) = \\frac{2}{n}$；以及 Xavier (Glorot) 初始化，其方差为 $\\mathrm{Var}(W_{l,ij}) = \\frac{1}{n}$。偏置为零。\n\n目标是从第一性原理出发，分析梯度消失问题。您必须从以下基础出发：\n- 用于前馈神经网络反向传播的微积分链式法则。\n- 方差的定义 $\\mathrm{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$ 以及独立零均值随机变量之和的方差独立性法则，即如果 $U_i$ 独立且均值为零，则 $\\mathrm{Var}\\left[\\sum_i U_i\\right] = \\sum_i \\mathrm{Var}[U_i]$。\n- ReLU 作用于零均值对称输入时的性质：其导数是一个指示函数 $\\phi'(u) = \\mathbb{1}\\{u0\\}$，在对称性下，它以 $\\frac{1}{2}$ 的概率充当门。\n\n使用这些基础，推断在初始化时矩（moment）是如何前向和后向传播的。具体来说，将第 $l$ 层的前向“能量”定义为二阶原点矩 $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_l^2\\big]$（期望是对批次和单元计算的），并将第 $l$ 层的后向“能量”定义为梯度相对于激活前值的二阶原点矩 $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_l)^2\\big]$，其中损失函数为平方范数损失 $L = \\tfrac{1}{2}\\lVert x_L\\rVert_2^2$，该损失是针对每个样本计算的，没有对批次进行平均。在 He 初始化和独立性假设下，证明为什么在初始化时，$m_l^{\\text{fwd}}$ 和 $m_l^{\\text{bwd}}$ 在各层之间都保持在 $1$ 附近。将其与 ReLU 下的 Xavier 初始化进行对比，并解释为什么随着深度的增加，Xavier 初始化倾向于减小 $m_l^{\\text{fwd}}$，从而造成梯度消失的情景。\n\n您的程序必须通过蒙特卡洛模拟来经验性地验证这些陈述。对于每个测试用例，按规定构建一个随机网络和批次，执行一次前向传播以计算所有的 $z_l$ 和 $x_l$，并使用链式法则执行一次反向传播以计算所有 $l$ 的 $\\partial L/\\partial z_l$。对于每一层，计算：\n- 前向二阶原点矩 $m_l^{\\text{fwd}}$，即 $x_l$ 中条目平方的经验均值。\n- 后向二阶原点矩 $m_l^{\\text{bwd}}$，即 $\\partial L/\\partial z_l$ 中条目平方的经验均值。\n通过计算前向和后向在各层上的最大绝对偏差来汇总与 $1$ 的偏差，即 $\\Delta^{\\text{fwd}} = \\max_l \\lvert m_l^{\\text{fwd}} - 1\\rvert$ 和 $\\Delta^{\\text{bwd}} = \\max_l \\lvert m_l^{\\text{bwd}} - 1\\rvert$。如果 $\\Delta^{\\text{fwd}}$ 和 $\\Delta^{\\text{bwd}}$ 都严格小于容差 $\\varepsilon = 0.2$，则测试用例“通过”。\n\n测试套件：\n- 案例 1：He 初始化，$L=1$，$n=64$，$B=2000$。\n- 案例 2：He 初始化，$L=20$，$n=64$，$B=2000$。\n- 案例 3：He 初始化，$L=40$，$n=64$，$B=2000$。\n- 案例 4：Xavier 初始化，$L=20$，$n=64$，$B=2000$。\n\n答案格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的通过/失败结果，格式为方括号括起来的逗号分隔列表，例如 $\\texttt{[True,True,True,False]}$。\n\n不涉及物理单位或角度单位。所有的期望和方差都是无量纲的。请完全按照描述实现模拟和计算，不使用任何外部文件或输入，并使用固定的随机种子以确保可复现性。", "solution": "该问题是有效的，因为它科学地基于深度学习的原理，问题陈述清晰且提供了所有必要信息，并且表述客观。我们接下来进行理论分析和随后的经验验证。\n\n分析的核心在于推导初始化时激活值（前向传播）和梯度（反向传播）的二阶原点矩在网络各层中传播的递推关系。我们将前向“能量”定义为 $m_l^{\\text{fwd}} = \\mathbb{E}\\big[x_{l,ik}^2\\big]$，后向“能量”定义为 $m_l^{\\text{bwd}} = \\mathbb{E}\\big[(\\partial L/\\partial z_{l,ik})^2\\big]$，其中期望是在批次维度 $i$、单元维度 $k$ 和随机初始化上计算的。\n\n**前向传播分析**\n\n前向传播由 $z_l = x_{l-1} W_l$ 和 $x_l = \\phi(z_l)$ 定义，其中 $\\phi$ 是 ReLU 函数。我们考虑单个激活前元素 $z_{l,ik} = \\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}$。在初始化时，权重 $W_{l,jk}$ 独立于输入 $x_{l-1, ij}$，并且两者均值为零。因此，在 $\\mathbb{E}[x_{l-1, ij}] = 0$ 的假设下，$\\mathbb{E}[z_{l,ik}] = \\sum_{j=1}^{n} \\mathbb{E}[x_{l-1, ij}] \\mathbb{E}[W_{l,jk}] = 0$。这个假设对输入层 $x_0$ 成立，并且由于更新的对称性，我们假设它对后续层也近似成立。\n\n$z_{l,ik}$ 的方差是 $\\mathrm{Var}(z_{l,ik}) = \\mathbb{E}[z_{l,ik}^2]$，因为其均值为零。利用和式中各项的独立性：\n$$ \\mathrm{Var}(z_{l,ik}) = \\mathrm{Var}\\left(\\sum_{j=1}^{n} x_{l-1, ij} W_{l,jk}\\right) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij} W_{l,jk}) $$\n根据 $x_{l-1}$ 和 $W_l$ 的独立性及其零均值特性，$\\mathrm{Var}(AB) = \\mathbb{E}[A^2]\\mathbb{E}[B^2] = \\mathrm{Var}(A)\\mathrm{Var}(B)$。因此：\n$$ \\mathrm{Var}(z_{l,ik}) = \\sum_{j=1}^{n} \\mathrm{Var}(x_{l-1, ij}) \\mathrm{Var}(W_{l,jk}) = n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\n这里，$\\mathrm{Var}(x_{l-1})$ 是 $x_{l-1}$ 中任意元素的方差，$\\mathrm{Var}(W_l)$ 是 $W_l$ 中任意元素的方差。\n激活值为 $x_l = \\phi(z_l)$。我们需要二阶矩 $m_l^{\\text{fwd}} = \\mathbb{E}[x_l^2] = \\mathbb{E}[\\phi(z_l)^2]$。对于一个零均值的对称输入 $z_l$（根据中心极限定理近似为高斯分布），ReLU 函数 $\\phi(u)=\\max(0,u)$ 实际上将分布的一半置零。输出的二阶矩是输入二阶矩的一半：$\\mathbb{E}[\\phi(z_l)^2] = \\frac{1}{2}\\mathbb{E}[z_l^2]$。\n因此，$m_l^{\\text{fwd}} = \\frac{1}{2} \\mathbb{E}[z_l^2] = \\frac{1}{2} \\mathrm{Var}(z_l)$。\n\n结合这些结果，我们得到前向能量的递推关系：\n$$ m_l^{\\text{fwd}} = \\frac{1}{2} n \\cdot \\mathrm{Var}(x_{l-1}) \\cdot \\mathrm{Var}(W_l) $$\n我们做一个标准近似，即激活值的均值保持在零附近，所以 $\\mathrm{Var}(x_{l-1}) \\approx \\mathbb{E}[x_{l-1}^2] = m_{l-1}^{\\text{fwd}}$。这得到：\n$$ m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\mathrm{Var}(W_l) $$\n基准情况是输入层 $x_0$，其条目是独立同分布的，均值为 0，方差为 1。因此，$m_0^{\\text{fwd}} = \\mathbb{E}[x_0^2] = \\mathrm{Var}(x_0) = 1$。\n\n*   **He 初始化**：当 $\\mathrm{Var}(W_l) = \\frac{2}{n}$ 时，递推关系变为 $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{2}{n} = m_{l-1}^{\\text{fwd}}$。从 $m_0^{\\text{fwd}}=1$ 开始，前向能量在各层之间得以保持，即对于所有 $l$，$m_l^{\\text{fwd}} \\approx 1$。\n*   **Xavier 初始化**：当 $\\mathrm{Var}(W_l) = \\frac{1}{n}$ 时，递推关系变为 $m_l^{\\text{fwd}} \\approx \\frac{1}{2} n \\cdot m_{l-1}^{\\text{fwd}} \\cdot \\frac{1}{n} = \\frac{1}{2} m_{l-1}^{\\text{fwd}}$。这导致指数衰减：$m_l^{\\text{fwd}} \\approx (\\frac{1}{2})^l m_0^{\\text{fwd}} = (\\frac{1}{2})^l$。随着网络深度 $L$ 的增加，激活值的能量会消失。\n\n**反向传播分析**\n\n关于激活前值的梯度由链式法则给出：$\\frac{\\partial L}{\\partial z_l} = \\frac{\\partial L}{\\partial z_{l+1}} \\frac{\\partial z_{l+1}}{\\partial x_l} \\frac{\\partial x_l}{\\partial z_l}$。设 $\\delta_l = \\frac{\\partial L}{\\partial z_l}$。单个分量为：\n$$ \\delta_{l,ik} = \\left( (\\delta_{l+1} W_{l+1}^T)_{ik} \\right) \\cdot \\phi'(z_{l,ik}) = \\left( \\sum_{j=1}^n \\delta_{l+1, ij} W_{l+1, kj} \\right) \\cdot \\phi'(z_{l,ik}) $$\n我们想求 $m_l^{\\text{bwd}} = \\mathbb{E}[\\delta_{l,ik}^2]$。在初始化时，括号中的项与 $\\phi'(z_{l,ik})$ 独立。因此，$\\mathbb{E}[\\delta_{l,ik}^2] = \\mathbb{E}[(\\sum_j \\dots)^2] \\cdot \\mathbb{E}[(\\phi'(z_{l,ik}))^2]$。\nReLU 的导数是 $\\phi'(u) = \\mathbb{1}\\{u0\\}$，所以 $(\\phi'(u))^2 = \\phi'(u)$。对于对称的零均值 $z_{l,ik}$，$\\mathbb{P}(z_{l,ik}0) = \\frac{1}{2}$，所以 $\\mathbb{E}[(\\phi'(z_{l,ik}))^2] = \\frac{1}{2}$。\n和的方差是 $\\mathrm{Var}(\\sum_j \\delta_{l+1, ij} W_{l+1, kj}) = n \\cdot \\mathrm{Var}(\\delta_{l+1}) \\cdot \\mathrm{Var}(W_{l+1})$。假设 $\\mathbb{E}[\\delta_{l+1}] \\approx 0$，则该值为 $n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1})$。\n\n结合这些，我们得到后向递推关系：\n$$ m_l^{\\text{bwd}} \\approx \\frac{1}{2} n \\cdot m_{l+1}^{\\text{bwd}} \\cdot \\mathrm{Var}(W_{l+1}) $$\n对于第 $L$ 层的基准情况，损失是每个样本的 $L = \\frac{1}{2} \\|x_L\\|_2^2$。梯度为 $\\delta_L = \\frac{\\partial L}{\\partial z_L} = \\frac{\\partial L}{\\partial x_L}\\frac{\\partial x_L}{\\partial z_L}$。这里，$\\frac{\\partial L}{\\partial x_L} = x_L$，而 $\\frac{\\partial x_L}{\\partial z_L}$ 是一个对角矩阵，其对角线元素为 $\\phi'(z_L)$。所以 $\\delta_L = x_L \\odot \\phi'(z_L)$。因为 $x_L = \\phi(z_L)$，我们有 $\\delta_L = \\phi(z_L) \\odot \\phi'(z_L) = \\phi(z_L) = x_L$。\n因此，后向能量的基准情况是 $m_L^{\\text{bwd}} = \\mathbb{E}[\\delta_L^2] = \\mathbb{E}[x_L^2] = m_L^{\\text{fwd}}$。\n\n*   **He 初始化**：当 $\\mathrm{Var}(W_{l+1}) = \\frac{2}{n}$ 时，递推关系为 $m_l^{\\text{bwd}} \\approx m_{l+1}^{\\text{bwd}}$。根据前向传播的分析，$m_L^{\\text{fwd}} \\approx 1$，所以 $m_L^{\\text{bwd}} \\approx 1$。向后传播，我们发现对于所有 $l$，$m_l^{\\text{bwd}} \\approx 1$。梯度能量得以保持。\n*   **Xavier 初始化**：当 $\\mathrm{Var}(W_{l+1}) = \\frac{1}{n}$ 时，递推关系为 $m_l^{\\text{bwd}} \\approx \\frac{1}{2} m_{l+1}^{\\text{bwd}}$。梯度能量在从第 $L$ 层传播到第 $1$ 层时呈指数衰减。这就是梯度消失问题。早期层的梯度变得过小，无法支持有效的学习。\n\n模拟将经验性地验证这两种截然不同的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs Monte Carlo simulations to verify the signal propagation properties of\n    He and Xavier initializations in deep ReLU networks.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'init': 'he', 'L': 1, 'n': 64, 'B': 2000, 'name': 'Case 1'},\n        {'init': 'he', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 2'},\n        {'init': 'he', 'L': 40, 'n': 64, 'B': 2000, 'name': 'Case 3'},\n        {'init': 'xavier', 'L': 20, 'n': 64, 'B': 2000, 'name': 'Case 4'},\n    ]\n\n    results = []\n    for case in test_cases:\n        passes_test = run_simulation(\n            init_scheme=case['init'],\n            L=case['L'],\n            n=case['n'],\n            B=case['B']\n        )\n        results.append(passes_test)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(init_scheme: str, L: int, n: int, B: int) - bool:\n    \"\"\"\n    Performs a single forward and backward pass for a given network configuration\n    and checks if the deviation criteria are met.\n\n    Args:\n        init_scheme: Either 'he' or 'xavier'.\n        L: Depth of the network.\n        n: Width of each layer.\n        B: Batch size.\n\n    Returns:\n        A boolean indicating if the test case passes.\n    \"\"\"\n    # 1. Initialization\n    # Input data: i.i.d. Gaussian with zero mean and unit variance.\n    x0 = np.random.randn(B, n)\n\n    # Determine weight variance based on initialization scheme.\n    if init_scheme == 'he':\n        var_w = 2.0 / n\n    elif init_scheme == 'xavier':\n        var_w = 1.0 / n\n    else:\n        raise ValueError(\"Unknown initialization scheme.\")\n    \n    std_w = np.sqrt(var_w)\n\n    # Initialize weights for L layers.\n    weights = [np.random.randn(n, n) * std_w for _ in range(L)]\n\n    # 2. Forward Pass\n    z_values = {}\n    x_values = {0: x0}\n    \n    x_current = x0\n    for l in range(1, L + 1):\n        # Linear transformation\n        z_l = x_current @ weights[l-1]\n        # ReLU activation\n        x_l = np.maximum(0, z_l)\n        \n        z_values[l] = z_l\n        x_values[l] = x_l\n        x_current = x_l\n    \n    # 3. Compute Forward Moments\n    m_fwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(x_values[l]**2)\n        m_fwd.append(moment)\n\n    # 4. Backward Pass\n    dz_values = {}\n    \n    # Base case: Gradient of the loss w.r.t final pre-activations.\n    # For L = 0.5 * ||x_L||^2, dL/dz_L = x_L.\n    # This simplification comes from dL/dx_L = x_L and dL/dz_L = dL/dx_L * phi'(z_L),\n    # which simplifies to x_L * phi'(z_L) = phi(z_L) = x_L.\n    dz_L = x_values[L]\n    dz_values[L] = dz_L\n    \n    # Propagate gradients backward from L-1 to 1.\n    dz_current = dz_L\n    for l in range(L - 1, 0, -1):\n        # Gradient w.r.t previous activation layer\n        dx_l = dz_current @ weights[l].T\n        \n        # Derivative of ReLU\n        phi_prime_l = (z_values[l]  0).astype(float)\n        \n        # Gradient w.r.t pre-activation layer\n        dz_l = dx_l * phi_prime_l\n        \n        dz_values[l] = dz_l\n        dz_current = dz_l\n\n    # 5. Compute Backward Moments\n    m_bwd = []\n    for l in range(1, L + 1):\n        # Calculate empirical second raw moment over batch and units.\n        moment = np.mean(dz_values[l]**2)\n        m_bwd.append(moment)\n\n    # 6. Check Pass/Fail Condition\n    epsilon = 0.2\n    \n    # Maximum absolute deviation from 1 for forward moments\n    delta_fwd = np.max(np.abs(np.array(m_fwd) - 1))\n    \n    # Maximum absolute deviation from 1 for backward moments\n    delta_bwd = np.max(np.abs(np.array(m_bwd) - 1))\n\n    return delta_fwd  epsilon and delta_bwd  epsilon\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3194508"}]}