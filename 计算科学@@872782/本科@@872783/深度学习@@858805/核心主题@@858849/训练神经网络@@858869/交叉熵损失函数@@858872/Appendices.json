{"hands_on_practices": [{"introduction": "任何深度学习实践者工具箱中的核心技能之一是理解模型如何学习。对于分类任务，学习过程的核心在于反向传播算法如何利用损失函数的梯度来更新模型参数。本练习将指导您从第一性原理出发，推导 Softmax 交叉熵损失函数相对于其输入（logits）的梯度，揭示其简洁而优美的形式，并理解为什么这个结果是现代分类器训练的基础。[@problem_id:3110719]", "problem": "考虑一个多类分类器，它为 $K$ 个类别输出一个 logits 向量 $\\mathbf{z} \\in \\mathbb{R}^{K}$。预测的类别概率是通过应用 softmax 函数得到的，对每个类别索引 $i \\in \\{1,2,\\dots,K\\}$，其定义为\n$$\n\\hat{p}_{i}(\\mathbf{z}) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}.\n$$\n设观测到的标签表示为一个独热（one-hot）向量 $\\mathbf{y} \\in \\{0,1\\}^{K}$，满足 $\\sum_{i=1}^{K} y_{i} = 1$。训练目标是交叉熵（Cross-Entropy, CE）损失，\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big).\n$$\n仅从这些定义出发，推导交叉熵损失关于 logits 向量 $\\mathbf{z}$ 的梯度 $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$。你的推导必须使用第一性原理和标准的微积分法则，清楚地指出在何处应用了乘法法则、链式法则以及 softmax 函数的性质。然后，根据损失函数如何响应预测分布与独热标签之间的差异，解释梯度中出现的每一项。\n\n最后，为了进行具体计算，请在 logits $\\mathbf{z} = [0,\\,0,\\,\\ln 2]$ 和标签 $\\mathbf{y} = [0,\\,1,\\,0]$ 处计算你推导出的梯度。以精确值的形式表示最终的数值结果，无需四舍五入。", "solution": "该问题是有效的。softmax 函数和交叉熵损失的定义在深度学习领域是标准的，推导它们的梯度是一个定义明确的数学练习。\n\n我们的目标是计算交叉熵（CE）损失函数关于 logits 向量 $\\mathbf{z}$ 的梯度，记为 $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$。梯度是一个向量，其第 $k$ 个分量是损失函数关于第 $k$ 个 logit 的偏导数，即 $\\frac{\\partial \\text{CE}}{\\partial z_k}$。\n\n损失函数由下式给出：\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = L = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big)\n$$\n我们首先应用求和法则求导，以找到关于任意 logit $z_k$ 的偏导数：\n$$\n\\frac{\\partial L}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left( -\\sum_{i=1}^{K} y_{i} \\ln(\\hat{p}_{i}) \\right) = -\\sum_{i=1}^{K} y_{i} \\frac{\\partial}{\\partial z_k} \\ln(\\hat{p}_{i})\n$$\n使用链式法则，自然对数的导数是 $\\frac{d}{dx}\\ln(u) = \\frac{1}{u}\\frac{du}{dx}$。应用此法则，我们得到：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k}\n$$\n下一步是计算 softmax 函数的输出 $\\hat{p}_i$ 关于 logit $z_k$ 的偏导数。softmax 函数是：\n$$\n\\hat{p}_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}\n$$\n我们使用除法法则求导，即 $(\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$。令 $f(z_k) = \\exp(z_i)$ 和 $g(z_k) = \\sum_{j=1}^{K} \\exp(z_j)$。\n分子和分母关于 $z_k$ 的导数分别是：\n$$\n\\frac{\\partial}{\\partial z_k} \\exp(z_i) = \\begin{cases} \\exp(z_i)  \\text{if } i=k \\\\ 0  \\text{if } i \\neq k \\end{cases} = \\delta_{ik} \\exp(z_i)\n$$\n其中 $\\delta_{ik}$ 是克罗内克（Kronecker）δ函数。\n$$\n\\frac{\\partial}{\\partial z_k} \\sum_{j=1}^{K} \\exp(z_j) = \\exp(z_k)\n$$\n应用除法法则：\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{(\\delta_{ik} \\exp(z_i)) \\left( \\sum_{j=1}^{K} \\exp(z_j) \\right) - \\exp(z_i) \\exp(z_k)}{\\left( \\sum_{j=1}^{K} \\exp(z_j) \\right)^2}\n$$\n我们可以通过将其拆分为两部分来简化：\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{\\delta_{ik} \\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} - \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}\n$$\n识别出 softmax 函数的定义，即 $\\hat{p}_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$ 和 $\\hat{p}_k = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}$，我们可以将表达式重写为：\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\delta_{ik} \\hat{p}_i - \\hat{p}_i \\hat{p}_k = \\hat{p}_i (\\delta_{ik} - \\hat{p}_k)\n$$\nsoftmax 函数导数的这个通用形式可以分为两种情况：\n1.  如果 $i = k$：$\\frac{\\partial \\hat{p}_{i}}{\\partial z_i} = \\hat{p}_i(1 - \\hat{p}_i)$\n2.  如果 $i \\neq k$：$\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\hat{p}_i(0 - \\hat{p}_k) = -\\hat{p}_i \\hat{p}_k$\n\n现在，我们将此结果代回到损失梯度的表达式中：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\left( \\hat{p}_i (\\delta_{ik} - \\hat{p}_k) \\right)\n$$\n$\\hat{p}_i$ 项相互抵消，从而大大简化了表达式：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} y_{i} (\\delta_{ik} - \\hat{p}_k)\n$$\n我们可以将求和拆分为两部分：\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\left( \\sum_{i=1}^{K} y_{i}\\delta_{ik} - \\sum_{i=1}^{K} y_{i}\\hat{p}_k \\right)\n$$\n我们来计算每个求和：\n-  第一个和，$\\sum_{i=1}^{K} y_{i}\\delta_{ik}$，仅在 $i=k$ 时非零，此时其值为 $y_k$。因此，$\\sum_{i=1}^{K} y_{i}\\delta_{ik} = y_k$。\n-  在第二个和中，$\\hat{p}_k$ 相对于求和索引 $i$ 是一个常数。所以，$\\sum_{i=1}^{K} y_{i}\\hat{p}_k = \\hat{p}_k \\sum_{i=1}^{K} y_i$。因为 $\\mathbf{y}$ 是一个独热向量，我们知道 $\\sum_{i=1}^{K} y_i = 1$。因此，这个和等于 $\\hat{p}_k$。\n\n将这些结果代回：\n$$\n\\frac{\\partial L}{\\partial z_k} = -(y_k - \\hat{p}_k) = \\hat{p}_k - y_k\n$$\n这个非常简洁的结果表明，交叉熵损失关于第 $k$ 个 logit 的偏导数是类别 $k$ 的预测概率与类别 $k$ 的真实标签之差。因此，完整的梯度向量是：\n$$\n\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y}) = \\hat{\\mathbf{p}}(\\mathbf{z}) - \\mathbf{y}\n$$\n\n项 $\\hat{p}_k - y_k$ 的解释如下。在通过梯度下降进行训练期间，logit $z_k$ 根据 $z_k \\leftarrow z_k - \\alpha (\\hat{p}_k - y_k)$ 进行更新，其中 $\\alpha  0$ 是学习率。\n- 如果类别 $k$ 是真实类别，则 $y_k=1$。梯度分量是 $\\hat{p}_k - 1$，这是一个负数或零（因为 $\\hat{p}_k \\leq 1$）。更新规则变为 $z_k \\leftarrow z_k - \\alpha(\\text{负值})$，这会增加 $z_k$。这将推高正确类别的 logit，从而使其概率 $\\hat{p}_k$ 趋向于目标值 $1$。\n- 如果类别 $k$ 不是真实类别，则 $y_k=0$。梯度分量是 $\\hat{p}_k - 0 = \\hat{p}_k$，这是一个正数或零（因为 $\\hat{p}_k \\geq 0$）。更新规则变为 $z_k \\leftarrow z_k - \\alpha(\\text{正值})$，这会减小 $z_k$。这将推低不正确类别的 logits，从而使其概率趋向于目标值 $0$。\n本质上，梯度向量 $\\hat{\\mathbf{p}} - \\mathbf{y}$ 代表了预测概率分布与真实独热分布之间的误差或残差。\n\n最后，我们针对给定的具体情况计算此梯度：\n- Logits: $\\mathbf{z} = [0, 0, \\ln 2]$。这意味着 $K=3$。\n- 标签: $\\mathbf{y} = [0, 1, 0]$。\n\n首先，我们使用 softmax 函数计算预测概率 $\\hat{\\mathbf{p}}$：\n分母是 $\\sum_{j=1}^{3} \\exp(z_j) = \\exp(0) + \\exp(0) + \\exp(\\ln 2) = 1 + 1 + 2 = 4$。\n概率为：\n$$\n\\hat{p}_1 = \\frac{\\exp(z_1)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_2 = \\frac{\\exp(z_2)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_3 = \\frac{\\exp(z_3)}{4} = \\frac{\\exp(\\ln 2)}{4} = \\frac{2}{4} = \\frac{1}{2}\n$$\n所以，预测的概率向量是 $\\hat{\\mathbf{p}} = [\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}]$。\n\n现在，我们计算梯度 $\\nabla_{\\mathbf{z}} \\text{CE} = \\hat{\\mathbf{p}} - \\mathbf{y}$：\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4},  \\frac{1}{4},  \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 0,  1,  0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4} - 0,  \\frac{1}{4} - 1,  \\frac{1}{2} - 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4},  -\\frac{3}{4},  \\frac{1}{2} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{4}  -\\frac{3}{4}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3110719"}, {"introduction": "在训练分类模型时，我们通常会监控验证集上的指标以决定何时停止训练，防止过拟合。虽然准确率（Accuracy）是一个直观的指标，但它可能非常“粗糙”。本练习通过一个具体的数值案例，让您亲手计算并比较准确率和交叉熵损失（也称为负对数似然 NLL），您将发现即使在准确率停滞不前的情况下，模型仍然可以通过优化交叉熵来提升其预测的“质量”。[@problem_id:3110736]", "problem": "一个多类别分类器为每个输入 $x$ 生成一个关于类别 $\\{A,B,C\\}$ 的概率向量 $\\hat{\\mathbf{p}}(x)$。考虑一个包含 $n=4$ 个样本的固定验证集，其真实标签以及模型在轮次 $t=1$ 和轮次 $t=2$ 的预测概率如下所示。对于每个样本 $i$，真实类别表示为 $y_i \\in \\{A,B,C\\}$，模型输出一个概率向量 $(\\hat{p}_A,\\hat{p}_B,\\hat{p}_C)$。\n\n- 样本 $1$：$y_1=A$\n  - 轮次 $1$：$(0.55,\\,0.40,\\,0.05)$\n  - 轮次 $2$：$(0.70,\\,0.25,\\,0.05)$\n- 样本 $2$：$y_2=B$\n  - 轮次 $1$：$(0.60,\\,0.35,\\,0.05)$\n  - 轮次 $2$：$(0.55,\\,0.40,\\,0.05)$\n- 样本 $3$：$y_3=C$\n  - 轮次 $1$：$(0.24,\\,0.24,\\,0.52)$\n  - 轮次 $2$：$(0.20,\\,0.20,\\,0.60)$\n- 样本 $4$：$y_4=B$\n  - 轮次 $1$：$(0.49,\\,0.51,\\,0.00)$\n  - 轮次 $2$：$(0.49,\\,0.51,\\,0.00)$\n\n仅使用以下基本定义。\n\n- 轮次 $t$ 的验证准确率是分数 $\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{\\arg\\max_{c\\in\\{A,B,C\\}}\\hat{p}_c^{(t)}(x_i)=y_i\\}$。\n- 轮次 $t$ 的平均负对数似然 (NLL)，等效于独热标签的平均交叉熵 (CE)，是 $\\frac{1}{n}\\sum_{i=1}^{n}\\big(-\\log \\hat{p}^{(t)}_{y_i}(x_i)\\big)$，其中 $\\log$ 是自然对数。\n- 定义 NLL 的指数移动平均 (EMA) 为 $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$，其中 $L_t$ 是轮次 $t$ 的平均 NLL，$\\alpha\\in(0,1]$，并初始化 $\\text{EMA}_1=L_1$。取 $\\alpha=0.5$。\n\n根据以上定义和数据，以下哪个陈述是正确的？\n\nA. 从轮次 $1$ 到轮次 $2$，验证准确率保持不变，而平均负对数似然严格减小。因此，与监控移动平均负对数似然的规则相比，当准确率停滞 $1$ 个轮次就停止的早停规则会过早。\n\nB. 从轮次 $1$ 到轮次 $2$，验证准确率和平均负对数似然都保持不变；因此，在轮次 $2$ 时，两种指标都会产生相同的停止决策。\n\nC. 从轮次 $1$ 到轮次 $2$，验证准确率增加，平均负对数似然也增加。\n\nD. 负对数似然的平滑参数为 $\\alpha=0.5$ 的 $2$ 轮次指数移动平均值从轮次 $1$ 到轮次 $2$ 减小。", "solution": "为了解答这个问题，我们需要计算并比较两个轮次的验证准确率和平均负对数似然 (NLL)。\n\n**1. 验证准确率计算**\n\n验证准确率是正确分类的样本所占的比例。预测类别由概率最高的类别决定。\n\n*   **轮次 1 准确率 ($Acc_1$)**：\n    *   样本 1 (真: A): $\\hat{\\mathbf{p}}^{(1)}=(0.55, 0.40, 0.05)$。预测 A。正确。\n    *   样本 2 (真: B): $\\hat{\\mathbf{p}}^{(1)}=(0.60, 0.35, 0.05)$。预测 A。错误。\n    *   样本 3 (真: C): $\\hat{\\mathbf{p}}^{(1)}=(0.24, 0.24, 0.52)$。预测 C。正确。\n    *   样本 4 (真: B): $\\hat{\\mathbf{p}}^{(1)}=(0.49, 0.51, 0.00)$。预测 B。正确。\n    $Acc_1 = \\frac{3}{4} = 0.75$。\n\n*   **轮次 2 准确率 ($Acc_2$)**：\n    *   样本 1 (真: A): $\\hat{\\mathbf{p}}^{(2)}=(0.70, 0.25, 0.05)$。预测 A。正确。\n    *   样本 2 (真: B): $\\hat{\\mathbf{p}}^{(2)}=(0.55, 0.40, 0.05)$。预测 A。错误。\n    *   样本 3 (真: C): $\\hat{\\mathbf{p}}^{(2)}=(0.20, 0.20, 0.60)$。预测 C。正确。\n    *   样本 4 (真: B): $\\hat{\\mathbf{p}}^{(2)}=(0.49, 0.51, 0.00)$。预测 B。正确。\n    $Acc_2 = \\frac{3}{4} = 0.75$。\n\n结论：从轮次 1 到 2，验证准确率保持不变。\n\n**2. 平均负对数似然 (NLL) 计算**\n\nNLL 是每个样本真实类别预测概率的负对数之和的平均值。\n*   **轮次 1 NLL ($L_1$)**:\n    $$L_1 = \\frac{1}{4} \\left( -\\log(0.55) -\\log(0.35) -\\log(0.52) -\\log(0.51) \\right)$$\n*   **轮次 2 NLL ($L_2$)**:\n    $$L_2 = \\frac{1}{4} \\left( -\\log(0.70) -\\log(0.40) -\\log(0.60) -\\log(0.51) \\right)$$\n\n为了比较 $L_1$ 和 $L_2$，我们观察每个样本真实类别的概率变化：\n*   样本 1: $0.55 \\to 0.70$ (增加)\n*   样本 2: $0.35 \\to 0.40$ (增加)\n*   样本 3: $0.52 \\to 0.60$ (增加)\n*   样本 4: $0.51 \\to 0.51$ (不变)\n\n由于 $-\\log(x)$ 是一个严格递减函数，当概率增加时，负对数似然会减小。因此，样本 1、2、3 的损失减小，样本 4 的损失不变。所以，总体平均 NLL 严格减小，即 $L_2  L_1$。\n\n**3. NLL 的指数移动平均 (EMA) 计算**\n\n*   $\\text{EMA}_1 = L_1$\n*   $\\text{EMA}_2 = 0.5 L_2 + 0.5 \\text{EMA}_1 = 0.5 L_2 + 0.5 L_1$\n\n由于 $L_2  L_1$，我们有 $\\text{EMA}_2 = 0.5(L_1 + L_2)  0.5(L_1 + L_1) = L_1 = \\text{EMA}_1$。因此，EMA 从轮次 1 到 2 减小了。\n\n**4. 逐项分析**\n\n*   **A**: “验证准确率保持不变，而平均负对数似然严格减小。” 这部分是正确的。“因此...早停规则会过早。” 这个推论也是正确的，因为 NLL 的改善表明模型仍在学习。所以陈述 A 正确。\n*   **B**: “...都保持不变”。错误，因为 NLL 减小了。\n*   **C**: “验证准确率增加...”。错误，因为准确率不变。\n*   **D**: “...指数移动平均值从轮次 1 到轮次 2 减小。” 这是正确的，如上计算所示。\n\n由于陈述 A 和 D 均为真，所以正确答案是 AD。", "answer": "$$\\boxed{AD}$$", "id": "3110736"}, {"introduction": "一个优秀的模型不仅要预测准确，其输出的概率也应当是“校准”良好的，即预测的置信度能真实反映事件发生的可能性。本练习探讨了一种名为“温度缩放”（temperature scaling）的高级技术，它通过调整交叉熵损失函数的地形来影响模型的校准特性。通过推导在不同温度 $T$ 下使损失最小化的最优参数，您将深入理解如何从理论上控制和改善模型的置信度输出。[@problem_id:3110717]", "problem": "考虑一个深度学习中的二元分类模型，其中两个类别的对数几率由单个实数参数 $a \\in \\mathbb{R}$ 参数化为 $\\mathbf{z}(a) = (a, 0)$。该模型通过softmax函数输出概率。通过一个标量 $T  0$ 对对数几率进行温度缩放，即将其除以 $T$，得到缩放后的对数几率 $\\mathbf{z}(a)/T = (a/T, 0)$，类别1的模型概率由缩放后对数几率的softmax给出。数据生成过程的标签为 $y \\in \\{0,1\\}$，其真实类别概率为 $\\mathbb{P}(y=1) = q$ 和 $\\mathbb{P}(y=0) = 1 - q$，其中 $q \\in (0,1)$ 是未知但固定的。训练目标是在温度缩放 $T$ 下，$y$ 的真实伯努利分布与模型预测分布之间的期望交叉熵损失。交叉熵定义为在真实数据分布上，观测标签的预测概率的负对数的期望值。仅使用这些定义并从第一性原理出发，推导使温度缩放的期望交叉熵损失最小化的参数值 $a^{\\ast}(T)$ 作为 $T$ 和 $q$ 的函数的闭式表达式。此外，在定义的层面上解释温度缩放 $T$ 如何改变参数空间中的最优解，以及在此最优解下，诸如负对数似然（NLL）和期望校准误差（ECE）等标准校准指标会发生什么变化。将您的最终答案表示为关于 $T$ 和 $q$ 的 $a^{\\ast}(T)$ 的单一闭式解析表达式。", "solution": "所述问题具有科学依据、良定且客观。它基于深度学习和信息论的既定原理，包括对数几率、softmax函数、温度缩放和交叉熵损失。所提供的定义和约束是自洽且一致的，允许进行严格的数学推导。因此，该问题被认为是有效的。\n\n目标是找到使温度缩放的期望交叉熵损失最小化的参数值 $a^{\\ast}(T)$。我们首先定义模型的概率输出。两个类别的对数几率由向量 $\\mathbf{z}(a) = (a, 0)$ 给出，其中 $a \\in \\mathbb{R}$。应用一个标量 $T  0$ 进行温度缩放，得到缩放后的对数几率 $\\mathbf{z}(a)/T = (a/T, 0)$。模型对类别1（表示为 $p_1$）和类别0（表示为 $p_0$）的预测概率通过对这些缩放后的对数几率应用softmax函数获得：\n$$p_1(a, T) = \\frac{\\exp(a/T)}{\\exp(a/T) + \\exp(0)} = \\frac{\\exp(a/T)}{\\exp(a/T) + 1}$$\n$$p_0(a, T) = \\frac{\\exp(0)}{\\exp(a/T) + \\exp(0)} = \\frac{1}{\\exp(a/T) + 1}$$\n注意 $p_1(a, T) + p_0(a, T) = 1$。$p_1(a,T)$ 的函数等价于应用于缩放后对数几率差的sigmoid函数，即 $p_1(a,T) = \\sigma(a/T)$。\n\n真实的数据生成过程是一个伯努利分布，其中标签 $y \\in \\{0, 1\\}$ 的概率为 $\\mathbb{P}(y=1) = q$ 和 $\\mathbb{P}(y=0) = 1-q$，对于某个固定的 $q \\in (0, 1)$。训练目标是这个真实分布与模型预测分布之间的期望交叉熵损失 $L(a, T)$。根据定义，这是在真实数据分布上，观测标签的预测概率的负对数的期望值：\n$$L(a, T) = \\mathbb{E}_{y \\sim \\mathbb{P}(y)}[-\\ln(p_y(a,T))] = - \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(y) \\ln(p_y(a,T))$$\n$$L(a, T) = - \\mathbb{P}(y=1) \\ln(p_1(a,T)) - \\mathbb{P}(y=0) \\ln(p_0(a,T))$$\n$$L(a, T) = -q \\ln(p_1(a,T)) - (1-q) \\ln(p_0(a,T))$$\n代入 $p_1(a,T)$ 和 $p_0(a,T)$ 的表达式：\n$$L(a, T) = -q \\ln\\left(\\frac{\\exp(a/T)}{\\exp(a/T) + 1}\\right) - (1-q) \\ln\\left(\\frac{1}{\\exp(a/T) + 1}\\right)$$\n使用对数性质 $\\ln(x/y) = \\ln(x) - \\ln(y)$ 和 $\\ln(1/y) = -\\ln(y)$，我们简化损失函数：\n$$L(a, T) = -q \\left[ \\ln(\\exp(a/T)) - \\ln(\\exp(a/T) + 1) \\right] - (1-q) \\left[ -\\ln(\\exp(a/T) + 1) \\right]$$\n$$L(a, T) = -q \\left[ \\frac{a}{T} - \\ln(\\exp(a/T) + 1) \\right] + (1-q) \\ln(\\exp(a/T) + 1)$$\n$$L(a, T) = -q \\frac{a}{T} + q \\ln(\\exp(a/T) + 1) + (1-q) \\ln(\\exp(a/T) + 1)$$\n$$L(a, T) = -q \\frac{a}{T} + \\ln(\\exp(a/T) + 1)$$\n为了找到使该损失最小化的参数值 $a^{\\ast}(T)$，我们计算 $L(a, T)$ 对 $a$ 的导数并将其设为零：\n$$\\frac{\\partial L}{\\partial a} = \\frac{\\partial}{\\partial a} \\left(-q \\frac{a}{T} + \\ln\\left(\\exp\\left(\\frac{a}{T}\\right) + 1\\right)\\right)$$\n$$\\frac{\\partial L}{\\partial a} = -\\frac{q}{T} + \\frac{1}{\\exp(a/T) + 1} \\cdot \\frac{\\partial}{\\partial a}\\left(\\exp\\left(\\frac{a}{T}\\right)\\right)$$\n$$\\frac{\\partial L}{\\partial a} = -\\frac{q}{T} + \\frac{1}{\\exp(a/T) + 1} \\cdot \\exp\\left(\\frac{a}{T}\\right) \\cdot \\frac{1}{T} = \\frac{1}{T} \\left( \\frac{\\exp(a/T)}{\\exp(a/T) + 1} - q \\right)$$\n认出 $p_1(a, T)$ 项，我们有：\n$$\\frac{\\partial L}{\\partial a} = \\frac{1}{T} \\left( p_1(a, T) - q \\right)$$\n将此导数设为 $0$ 以找到最优参数 $a^{\\ast}(T)$：\n$$\\frac{1}{T} (p_1(a^{\\ast}, T) - q) = 0$$\n由于 $T  0$，这意味着 $p_1(a^{\\ast}, T) = q$。当模型对类别1的预测概率等于其真实概率时，达到最小损失。我们求解这个方程以得到 $a^{\\ast}(T)$：\n$$\\frac{\\exp(a^{\\ast}/T)}{\\exp(a^{\\ast}/T) + 1} = q$$\n令 $x = \\exp(a^{\\ast}/T)$。方程为 $\\frac{x}{x+1} = q$，得到 $x = q(x+1)$，所以 $x - qx = q$，且 $x(1-q) = q$。这得出 $x = \\frac{q}{1-q}$。代回 $x$：\n$$\\exp\\left(\\frac{a^{\\ast}}{T}\\right) = \\frac{q}{1-q}$$\n对两边取自然对数，得到缩放后的最优参数：\n$$\\frac{a^{\\ast}}{T} = \\ln\\left(\\frac{q}{1-q}\\right)$$\n最后，最优参数 $a^{\\ast}(T)$ 的闭式表达式为：\n$$a^{\\ast}(T) = T \\ln\\left(\\frac{q}{1-q}\\right)$$\n为了确认这是一个最小值，我们检查损失函数的二阶导数。\n$$\\frac{\\partial^2 L}{\\partial a^2} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{T} (p_1(a, T) - q) \\right] = \\frac{1}{T} \\frac{\\partial p_1}{\\partial a}$$\n$p_1(a, T) = \\sigma(a/T)$ 对 $a$ 的导数是 $\\frac{\\partial p_1}{\\partial a} = \\sigma'(a/T) \\cdot \\frac{1}{T} = p_1(a, T)(1-p_1(a, T)) \\cdot \\frac{1}{T}$。\n因此，损失的二阶导数是 $\\frac{\\partial^2 L}{\\partial a^2} = \\frac{1}{T^2} p_1(a, T) p_0(a, T)$。由于 $T  0$，且概率 $p_1, p_0$ 在 $(0, 1)$ 内，所以 $\\frac{\\partial^2 L}{\\partial a^2}  0$。这证实了 $L(a, T)$ 在 $a$ 上是严格凸的，且 $a^{\\ast}(T)$ 是唯一的全局最小值。\n\n项 $\\ln(q/(1-q))$ 是真实概率 $q$ 的对数优势比（或logit）。最优参数 $a^{\\ast}(T)$ 由温度 $T$ 进行线性缩放。这意味着在参数 $a$ 的空间中，损失最小点的位置随 $T$ 的变化而变化。为达到最优输出概率 $p_1=q$，缩放后的对数几率差 $a/T$ 必须等于固定值 $\\ln(q/(1-q))$。因此，$a^{\\ast}$ 必须随 $T$ 缩放。\n\n在此最优解 $a^{\\ast}(T)$ 下，模型对类别1的预测概率为 $p_1(a^{\\ast}(T), T) = q$。这意味着模型是完美校准的：其置信度（$q$）与正类的真实频率（$q$）完全匹配。因此，期望校准误差（ECE）为 $0$。此最小值处的负对数似然（NLL）是损失函数自身的值，即 $L(a^{\\ast}(T), T) = -q \\ln(q) - (1-q) \\ln(1-q)$。这是真实数据生成分布的香农熵 $H(q)$，也是可能达到的最低交叉熵损失。最小NLL和由此产生的0 ECE都与温度 $T$ 无关。", "answer": "$$\n\\boxed{T \\ln\\left(\\frac{q}{1-q}\\right)}\n$$", "id": "3110717"}]}