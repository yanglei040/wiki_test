{"hands_on_practices": [{"introduction": "现代深度学习框架使得反向传播看起来如同魔法。本练习将通过指导您从零开始构建一个微型自动微分引擎来揭开它的神秘面纱。通过实现一个记录计算操作的“计算带”（tape）并反向回溯它，您将对梯度如何在任意计算图中被高效计算获得根本性的理解 ([@problem_id:3100018])。", "problem": "您的任务是为一个小型标量表达式语言实现逆向模式自动微分（AD），并演示如何通过反向回放操作磁带（tape）来计算伴随变量，记为 $\\bar{x} = \\partial L / \\partial x$。逆向模式 AD 与计算图中的反向传播算法是同义词。您的实现必须从第一性原理出发，特别是复合函数的链式求导法则，以及将计算图定义为一个以标量损失 $L$ 为根节点的原始操作的有向无环图。您必须设计一个磁带结构，用于记录原始操作的正向执行过程，然后反向回放这个磁带，利用链式法则来累积伴随变量。\n\n您的小型表达式语言必须支持标量变量和常量，以及以下原始操作：二元加法 $+$、二元减法 $-$、二元乘法 $\\cdot$、二元除法 $\\div$、一元正弦 $\\sin(\\cdot)$、一元指数 $\\exp(\\cdot)$ 和一元自然对数 $\\log(\\cdot)$。三角函数中的所有角度都必须是弧度制。必须遵守定义域约束，例如 $\\log(\\cdot)$ 的输入必须为严格正数。您必须设计计算磁带，以记录每个非叶节点操作及其操作数和正向计算值，以确保反向回放的正确性。\n\n您的程序必须：\n- 在评估标量损失 $L$ 时，构建一个内部计算图和磁带。\n- 通过从 $\\bar{L} = \\partial L / \\partial L = 1$ 开始对磁带进行单次反向回放，为每个输入变量 $x_i$ 计算伴随变量，即 $\\partial L / \\partial x_i$。\n- 为每个测试用例生成一个列表，其第一个元素是标量损失值 $L$，随后的元素是按照变量引入顺序排列的伴随变量。\n\n仅从基本原理出发：复合函数的链式法则、中间值 $v$ 的伴随变量定义 $\\bar{v} = \\partial L / \\partial v$，以及计算图的语义。不要依赖那些跳过推导路径的预打包微分公式；相反，应为每个原始操作使用基础微积分推导并实现反向回放所需的局部偏导数。\n\n实现并运行以下测试套件。在每个用例中，按指定顺序定义变量，使用原始操作构建表达式，并计算输出。所有角度均为弧度制，本问题不涉及物理单位。\n\n- 测试用例 $1$（通用复合）：变量 $x, y$，损失 $L = \\sin(x \\cdot y) + \\exp(y)$，其中 $x = 0.5$，$y = -1.0$。此用例的输出格式为：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 $2$（零和常数的边界情况）：变量 $x$，损失 $L = x \\cdot 0 + \\sin(0) + \\log(1)$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 $3$（变量重复使用）：变量 $x$，损失 $L = (x \\cdot x) \\cdot x$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 $4$（除法和对数）：变量 $x, y$，损失 $L = x \\div y + \\log(y)$，其中 $x = 1.0$，$y = 1.5$。输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 $5$（嵌套一元复合）：变量 $x$，损失 $L = \\exp(\\sin(x))$，其中 $x = 0.0$。输出格式：$[L, \\partial L / \\partial x]$。\n\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为一个以方括号括起来的逗号分隔列表，每个测试用例的结果本身也是一个以方括号括起来的逗号分隔列表。例如，两个测试用例的输出应类似于 $[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$。您的最终输出必须严格遵循此格式，使用标准的浮点数。", "solution": "该问题要求从第一性原理出发，实现逆向模式自动微分（AD），通常也称为反向传播。该方法通过以下步骤计算标量损失函数 $L$ 相对于一组输入变量 $x_i$ 的梯度：首先对表达式 $L$ 进行正向求值，以计算中间值并记录计算图；然后反向遍历该图，根据链式法则传播梯度。\n\n**基本原理：链式法则和伴随变量**\n\n逆向模式AD的基础是微积分的链式法则。如果一个标量损失 $L$ 是一个中间变量 $v_j$ 的函数，而 $v_j$ 本身又是其他变量 $v_i$ 的函数，那么 $L$ 相对于 $v_i$ 的梯度由所有从 $v_i$ 到 $L$ 的路径的贡献之和给出。对于单一路径 $L \\to v_j \\to v_i$，链式法则表明：\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\n在自动微分的语境中，我们定义变量 $v$ 的“伴随变量”为 $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$。使用此表示法，链式法则变为：\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\n逆向模式AD算法利用此关系，首先计算 $L$ 的值，然后将伴随变量从 $L$ 反向传播至输入变量。该过程从设定损失函数自身的伴随变量为种子开始，即 $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$。\n\n**计算图和磁带**\n\n任何标量表达式都可以分解为一系列原始操作（例如，加法、乘法、正弦函数）。这种分解自然形成一个有向无环图（DAG），其中节点代表数值（输入变量、常量和中间结果），边代表原始操作。\n\n从输入到最终损失 $L$ 的表达式正向求值过程被用来构建这个图。在我们的实现中，我们使用一种“磁带”（tape）数据结构，它是图的一种线性化表示。磁带是在正向传播过程中记录的一个有序操作列表。磁带上的每个条目存储了操作类型、对其输入节点的引用以及对其输出节点的引用。这种记录方式确保我们拥有完整的结构和所有必要的中间值以进行反向传播。\n\n**正向传播过程：求值与磁带记录**\n\n正向传播过程如下：\n1. 输入变量和常量被初始化为图中的起始节点。\n2. 表达式被顺序求值。每次应用一个原始操作时，会发生两件事：\n    a. 计算操作的数值结果，并将其作为图中的一个新节点存储。\n    b. 向磁带中添加一个条目，记录操作类型、其输入节点，以及新创建的输出节点。\n\n例如，对于表达式 $z = x \\cdot y$，我们会使用 $x$ 和 $y$ 的当前值计算出 $z$ 的值，为 $z$ 创建一个新节点，并在磁带上记录 `('mul', [x_node, y_node], z_node)`。\n\n**反向传播过程：伴随变量累积**\n\n一旦正向传播完成并计算出最终的损失值 $L$，反向传播过程便开始。它以与创建时相反的顺序遍历磁带。\n1. 初始化一个与图中每个节点对应的伴随变量数组，其所有元素均为零。\n2. 将最终损失节点的伴随变量设置为 $1$，即 $\\bar{L} = 1$。\n3. 对于磁带上的每个操作 $z = f(x_1, \\dots, x_n)$（按逆序处理）：\n    a. 我们检索已经计算出的输出的伴随变量 $\\bar{z}$。\n    b. 我们使用链式法则计算 $\\bar{z}$ 对输入伴随变量的贡献。每个输入 $x_i$ 的伴随变量通过累积这个贡献来更新：\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    使用累积（$\\mathrel{+}=$）至关重要，因为单个变量可能在多个操作中使用（即，在图中它可以是多个子节点的父节点）。它的总伴随变量是所有从其子节点回流的梯度信号之和。反转磁带保证了在一个节点的伴随变量（$\\bar{z}$）被传播到其自身的输入（$x_i$）之前，它已经被完全计算出来。\n\n**原始操作的伴随变量更新规则**\n\n每个原始操作的局部偏导数 $\\frac{\\partial z}{\\partial x_i}$ 都是已知的。这些导数所需的输入值（例如，对于 $z = x \\cdot y$，$\\frac{\\partial z}{\\partial x} = y$）可以从正向传播过程中获得。\n\n- **加法：** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z}$，$\\bar{y} \\mathrel{+}= \\bar{z}$。\n\n- **减法：** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z}$，$\\bar{y} \\mathrel{+}= -\\bar{z}$。\n\n- **乘法：** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$，$\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$。\n\n- **除法：** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$，$\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$。\n\n- **正弦：** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$。\n\n- **指数：** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$。\n\n- **自然对数：** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$。\n  - 更新规则：$\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$。\n\n**实现设计**\n\n该实现使用了两个主要类：`Graph` 和 `Node`。`Graph` 类管理计算的状态：它存储所有节点的 `values`（值）、操作 `tape`（磁带）和计算出的 `adjoints`（伴随变量）。`Node` 类作为节点 ID 的包装器，通过重载 Python 的算术运算符（`+`、`*` 等）提供了一个直观的接口。当对 `Node` 对象执行像 `c = a + b` 这样的操作时，它会透明地调用关联 `Graph` 对象上的一个方法，该方法执行正向计算，在磁带上记录操作，并为结果 `c` 返回一个新的 `Node`。这种面向对象的设计允许以自然的方式构建表达式，同时在后台正确地构建计算图。在最终的损失 `Node` 计算出来后，调用 `Graph.compute_gradients()` 将如上所述执行反向传播过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "在您实现了反向传播算法之后，如何能确信您的代码是完全正确的呢？本练习将介绍梯度检验（gradient checking），这是一种强大的调试技术，它通过将反向传播计算出的解析梯度与数值近似梯度进行比较来验证其正确性。掌握这种方法，您不仅能为自己的实现提供有力的验证，还能增强对算法背后微积分原理的信心 ([@problem_id:3100954])。", "problem": "构建一个反向传播算法的验证过程，通过将一个双层神经网络的解析梯度与有限差分近似进行比较。目标是数值上确认，在使用前向有限差分时，两种梯度之间的差异表现为$\\epsilon$阶的截断误差，即误差为$\\mathcal{O}(\\epsilon)$。\n\n使用以下纯数学设定。\n\n- 网络架构与数据：\n  - 输入维度为 $d = 3$，隐藏层有 $h = 3$ 个单元，使用双曲正切激活函数，输出层维度为 $o = 1$，使用线性输出。\n  - 给定大小为 $n = 4$ 的小批量，输入矩阵 $X \\in \\mathbb{R}^{4 \\times 3}$ 和目标向量 $y \\in \\mathbb{R}^{4 \\times 1}$ 如下：\n    $$\n    X =\n    \\begin{bmatrix}\n    0.2  -0.1  0.4 \\\\\n    -0.5  0.3  0.1 \\\\\n    0.0  -0.2  0.2 \\\\\n    0.1  0.4  -0.3\n    \\end{bmatrix}, \\quad\n    y =\n    \\begin{bmatrix}\n    0.5 \\\\ -0.1 \\\\ 0.2 \\\\ 0.0\n    \\end{bmatrix}.\n    $$\n  - 测试用例 A 的参数固定如下：\n    $$\n    W_1 =\n    \\begin{bmatrix}\n    0.3  -0.1  0.2 \\\\\n    -0.4  0.5  0.1 \\\\\n    0.2  0.3  -0.2\n    \\end{bmatrix}, \\quad\n    b_1 =\n    \\begin{bmatrix}\n    0.05 \\\\ -0.02 \\\\ 0.01\n    \\end{bmatrix}, \\quad\n    W_2 =\n    \\begin{bmatrix}\n    0.6  -0.7  0.2\n    \\end{bmatrix}, \\quad\n    b_2 =\n    \\begin{bmatrix}\n    0.03\n    \\end{bmatrix}.\n    $$\n    对于测试用例 B，使用相同的形状，但将 $W_1, b_1, W_2, b_2$ 的每个条目乘以因子 $0.1$。\n\n- 前向模型与损失函数：\n  - 对于 $X$ 的每一行 $x_i^\\top$，定义隐藏层预激活 $z_{1,i}^\\top = x_i^\\top W_1^\\top + b_1^\\top$，隐藏层激活 $h_i^\\top = \\tanh(z_{1,i}^\\top)$，输出层预激活 $z_{2,i} = h_i^\\top W_2^\\top + b_2$，以及预测值 $\\hat{y}_i = z_{2,i}$。\n  - 定义均方误差损失函数\n    $$\n    L(W_1,b_1,W_2,b_2) = \\frac{1}{2n} \\sum_{i=1}^{n} \\left( \\hat{y}_i - y_i \\right)^2.\n    $$\n\n- 通过反向传播计算解析梯度：\n  - 使用多元微积分、链式法则以及导数恒等式 $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$，推导并实现 $L$ 相对于所有参数的梯度。\n  - 使用以下顺序和内存布局，将参数集展平为单个向量 $\\theta \\in \\mathbb{R}^{p}$（其中 $p = 16$）：\n    1. 以行主序展平 $W_1 \\in \\mathbb{R}^{3 \\times 3}$。\n    2. 追加 $b_1 \\in \\mathbb{R}^{3}$。\n    3. 以行主序展平 $W_2 \\in \\mathbb{R}^{1 \\times 3}$。\n    4. 追加 $b_2 \\in \\mathbb{R}^{1}$。\n\n- 有限差分近似：\n  - 对于给定的 $\\epsilon  0$ 和 $\\mathbb{R}^p$ 中的标准基向量 $e_k$，通过前向差分来近似 $\\nabla_{\\theta} L$ 的第 $k$ 个分量\n    $$\n    \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}.\n    $$\n  - 使用以下步长列表\n    $$\n    \\mathcal{E} = \\left[ 10^{-1}, \\; 3 \\cdot 10^{-2}, \\; 10^{-2}, \\; 3 \\cdot 10^{-3}, \\; 10^{-3}, \\; 3 \\cdot 10^{-4}, \\; 10^{-4} \\right].\n    $$\n\n- 误差度量与阶数验证：\n  - 对于每个 $\\epsilon \\in \\mathcal{E}$，计算解析梯度与有限差分梯度之差的欧几里得范数，\n    $$\n    \\mathrm{err}(\\epsilon) = \\left\\| \\nabla_{\\theta} L - g_{\\mathrm{FD}}(\\epsilon) \\right\\|_2.\n    $$\n  - 对于连续的 $\\epsilon_i  \\epsilon_{i+1}$，计算经验阶\n    $$\n    s_i = \\frac{\\log\\left( \\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}) \\right)}{\\log\\left( \\epsilon_i / \\epsilon_{i+1} \\right)}.\n    $$\n  - 为每个测试用例定义两个布尔检查：\n    1. 设 $s_{\\mathrm{med}}$ 为 $\\{ s_i \\}$ 的中位数。如果 $0.8 \\le s_{\\mathrm{med}} \\le 1.2$，则定义 $\\mathrm{pass\\_order}$ 为真。\n    2. 如果 $\\mathrm{err}(\\epsilon)$ 在 $\\mathcal{E}$ 的前5个值上严格递减，即对于 $\\epsilon \\in \\{ 10^{-1}, 3\\cdot 10^{-2}, 10^{-2}, 3\\cdot 10^{-3}, 10^{-3} \\}$，则定义 $\\mathrm{pass\\_mono}$ 为真。\n\n- 测试套件：\n  - 通过参数集指定两个测试用例：\n    - 测试用例 A：参数与上述完全相同。\n    - 测试用例 B：参数形状与测试用例 A 相同，但每个条目都乘以 $0.1$。\n  - 在这两种情况下，都使用相同的 $X$、$y$ 和 $\\mathcal{E}$。\n\n- 程序行为要求及最终输出格式：\n  - 您的程序必须实现前向模型，从第一性原理推导并计算反向传播的解析梯度，为每个 $\\epsilon \\in \\mathcal{E}$ 计算有限差分梯度，并评估误差范数和经验阶。\n  - 对于每个测试用例，生成一个包含四个条目的列表：$[ \\mathrm{err}(\\min \\mathcal{E}), \\; s_{\\mathrm{med}}, \\; \\mathrm{pass\\_order}, \\; \\mathrm{pass\\_mono} ]$。\n  - 程序的最终输出必须是单行，包含一个含有两个用例列表的列表，其格式严格为用方括号括起来的逗号分隔列表，例如：\n    $$\n    \\left[ [a_1, a_2, a_3, a_4], [b_1, b_2, b_3, b_4] \\right]\n    $$\n    其中每个 $a_j$ 和 $b_j$ 是布尔值或浮点数。不得打印任何其他文本。\n  - 此问题不涉及任何物理单位。\n\n您的实现必须是自包含的，并且不得读取输入。它必须严格按照上面提供的数值进行计算。", "solution": "目标是数值验证一个双层神经网络的反向传播算法的正确性。这是通过将解析计算的梯度与通过有限差分法获得的数值近似进行比较来实现的。主要的验证标准是确认解析梯度和数值梯度之间的误差随有限差分步长 $\\epsilon$ 线性减小，这符合前向差分格式的一阶截断误差特性，即 $\\mathcal{O}(\\epsilon)$。\n\n### 数学模型与损失函数\n\n该神经网络架构由一个输入层、一个隐藏层和一个输出层组成。\n- 输入 $X \\in \\mathbb{R}^{n \\times d}$ ($n=4, d=3$)\n- 隐藏层有 $h=3$ 个单元和 $\\tanh$ 激活函数。\n- 输出层有 $o=1$ 个单元和线性激活函数。\n- 参数：$W_1 \\in \\mathbb{R}^{h \\times d}$，$b_1 \\in \\mathbb{R}^{h \\times 1}$，$W_2 \\in \\mathbb{R}^{o \\times h}$，$b_2 \\in \\mathbb{R}^{o \\times 1}$。\n\n小批量数据 $X$ 的前向传播由以下矩阵运算定义：\n1.  **隐藏层预激活**：隐藏层的线性变换由 $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$ 给出，其中 $\\mathbf{1} \\in \\mathbb{R}^{n \\times 1}$ 是一个全为1的向量，其与 $b_1^\\top$ 的乘积通过广播机制处理。结果矩阵 $Z_1 \\in \\mathbb{R}^{n \\times h}$。\n2.  **隐藏层激活**：双曲正切激活函数被逐元素应用：$H = \\tanh(Z_1)$，其中 $H \\in \\mathbb{R}^{n \\times h}$。\n3.  **输出层预激活**：第二次线性变换产生输出预激活：$Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$。结果矩阵 $Z_2 \\in \\mathbb{R}^{n \\times o}$。\n4.  **预测**：网络输出是线性的，因此预测值 $\\hat{Y}$ 等于预激活值：$\\hat{Y} = Z_2 \\in \\mathbb{R}^{n \\times o}$。\n\n网络的性能通过均方误差（MSE）损失函数来量化，该函数在小批量数据上取平均值：\n$$\nL = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{2n} \\|\\hat{Y} - Y\\|_F^2\n$$\n其中 $Y \\in \\mathbb{R}^{n \\times o}$ 是真实目标值的矩阵。\n\n### 通过反向传播计算解析梯度\n\n此任务的核心是使用多元链式法则推导损失函数 $L$ 相对于每个参数（$W_1, b_1, W_2, b_2$）的梯度。这个过程被称为反向传播。我们将损失函数对矩阵 $M$ 的梯度表示为 $\\delta_M = \\frac{\\partial L}{\\partial M}$。\n\n1.  **输出端梯度**：损失函数相对于网络预测值 $\\hat{Y}$ 的梯度为：\n    $$\n    \\delta_{\\hat{Y}} = \\frac{\\partial L}{\\partial \\hat{Y}} = \\frac{1}{n} (\\hat{Y} - Y)\n    $$\n    由于 $\\hat{Y} =Z_2$，我们有 $\\delta_{Z_2} = \\delta_{\\hat{Y}}$。\n\n2.  **输出层梯度（$W_2, b_2$）**：\n    对 $Z_2 = H W_2^\\top + \\mathbf{1}b_2^\\top$ 使用链式法则：\n    $$\n    \\delta_{W_2} = \\frac{\\partial L}{\\partial W_2} = \\delta_{Z_2}^\\top H\n    $$\n    $$\n    \\delta_{b_2} = \\frac{\\partial L}{\\partial b_2} = (\\text{对}\\delta_{Z_2}\\text{按axis=0求和})^\\top = \\delta_{Z_2}^\\top \\mathbf{1}\n    $$\n\n3.  **将梯度传播到隐藏层**：\n    梯度被反向传播到隐藏层的激活值 $H$：\n    $$\n    \\delta_H = \\frac{\\partial L}{\\partial H} = \\delta_{Z_2} W_2\n    $$\n    接下来，使用 $\\frac{d}{dz}\\tanh(z) = 1 - \\tanh^2(z)$，将梯度通过 $\\tanh$ 激活函数传播：\n    $$\n    \\delta_{Z_1} = \\frac{\\partial L}{\\partial Z_1} = \\delta_H \\odot (1 - H^2)\n    $$\n    其中 $\\odot$ 表示逐元素（Hadamard）乘积。\n\n4.  **隐藏层梯度（$W_1, b_1$）**：\n    最后，对 $Z_1 = X W_1^\\top + \\mathbf{1}b_1^\\top$ 使用链式法则：\n    $$\n    \\delta_{W_1} = \\frac{\\partial L}{\\partial W_1} = \\delta_{Z_1}^\\top X\n    $$\n    $$\n    \\delta_{b_1} = \\frac{\\partial L}{\\partial b_1} = (\\text{对}\\delta_{Z_1}\\text{按axis=0求和})^\\top = \\delta_{Z_1}^\\top \\mathbf{1}\n    $$\n\n这些矩阵形式的方程提供了一个计算解析梯度的完整算法。\n\n### 数值验证\n\n为了验证解析梯度，我们将其与数值近似进行比较。\n\n- **参数向量化**：所有网络参数（$W_1, b_1, W_2, b_2$）被展平并连接成一个单一向量 $\\theta \\in \\mathbb{R}^{p}$，其中 $p=16$。指定的顺序是 $W_1$（行主序）、$b_1$、$W_2$（行主序）和 $b_2$。\n\n- **有限差分近似**：使用一阶前向差分公式来近似梯度。梯度向量的第 $k$ 个分量估计为：\n  $$\n  g_{\\mathrm{FD},k}(\\epsilon) = \\frac{L(\\theta + \\epsilon e_k) - L(\\theta)}{\\epsilon}\n  $$\n  其中 $e_k$ 是第 $k$ 个标准基向量，$\\epsilon$ 是一个很小的步长。\n\n- **误差分析与阶数验证**：\n  解析梯度 $\\nabla_\\theta L$ 与有限差分近似 $g_{\\mathrm{FD}}(\\epsilon)$ 之间的差异通过它们差值的欧几里得范数来衡量：\n  $$\n  \\mathrm{err}(\\epsilon) = \\| \\nabla_\\theta L - g_{\\mathrm{FD}}(\\epsilon) \\|_2\n  $$\n  对于一阶方法，此误差预期与 $\\epsilon$ 成正比，即 $\\mathrm{err}(\\epsilon) \\approx C\\epsilon$。这意味着对于两个步长 $\\epsilon_i$ 和 $\\epsilon_{i+1}$，其误差之比应约等于步长本身之比。为了量化此关系，我们计算经验收敛阶 $s_i$：\n  $$\n  s_i = \\frac{\\log(\\mathrm{err}(\\epsilon_i) / \\mathrm{err}(\\epsilon_{i+1}))}{\\log(\\epsilon_i / \\epsilon_{i+1})}\n  $$\n  $s_i \\approx 1$ 的值确认了预期的一阶收敛性，从而验证了解析梯度的实现。为增强鲁棒性，我们使用计算出的 $s_i$ 值的中位数。如果此中位阶数 $s_{\\mathrm{med}}$ 在 $[0.8, 1.2]$ 范围内，并且对于初始较大的 $\\epsilon$ 值，误差是单调递减的，则验证被认为是成功的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the backpropagation verification problem.\n    \"\"\"\n    \n    # --- Givens from the problem statement ---\n    X = np.array([\n        [0.2, -0.1, 0.4],\n        [-0.5, 0.3, 0.1],\n        [0.0, -0.2, 0.2],\n        [0.1, 0.4, -0.3]\n    ])\n    y = np.array([\n        [0.5],\n        [-0.1],\n        [0.2],\n        [0.0]\n    ])\n    n = X.shape[0]  # Mini-batch size, n=4\n\n    # Parameters for Test Case A\n    W1_a = np.array([\n        [0.3, -0.1, 0.2],\n        [-0.4, 0.5, 0.1],\n        [0.2, 0.3, -0.2]\n    ])\n    b1_a = np.array([[0.05], [-0.02], [0.01]])\n    W2_a = np.array([[0.6, -0.7, 0.2]])\n    b2_a = np.array([[0.03]])\n    \n    test_cases_params = [\n        (W1_a, b1_a, W2_a, b2_a),\n        (W1_a * 0.1, b1_a * 0.1, W2_a * 0.1, b2_a * 0.1) # Test Case B\n    ]\n\n    epsilons = [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4, 1e-4]\n    \n    # --- Helper functions for parameter manipulation ---\n    def params_to_vec(W1, b1, W2, b2):\n        return np.concatenate([\n            W1.flatten(),\n            b1.flatten(),\n            W2.flatten(),\n            b2.flatten()\n        ])\n\n    def vec_to_params(theta):\n        W1 = theta[0:9].reshape(3, 3)\n        b1 = theta[9:12].reshape(3, 1)\n        W2 = theta[12:15].reshape(1, 3)\n        b2 = theta[15:16].reshape(1, 1)\n        return W1, b1, W2, b2\n        \n    # --- Core functions for forward/backward pass ---\n    def forward_pass(W1, b1, W2, b2):\n        Z1 = X @ W1.T + b1.T\n        H = np.tanh(Z1)\n        Z2 = H @ W2.T + b2.T\n        Y_hat = Z2\n        loss = (1 / (2 * n)) * np.sum((Y_hat - y)**2)\n        return loss, Z1, H, Y_hat\n\n    def analytical_gradient(W1, b1, W2, b2):\n        _loss, _Z1, H, Y_hat = forward_pass(W1, b1, W2, b2)\n        \n        # Backward pass\n        dZ2 = (1 / n) * (Y_hat - y)\n        dW2 = dZ2.T @ H\n        db2 = np.sum(dZ2, axis=0, keepdims=True)\n        \n        dH = dZ2 @ W2\n        dZ1 = dH * (1 - H**2)\n        dW1 = dZ1.T @ X\n        db1 = np.sum(dZ1, axis=0, keepdims=True).T\n        \n        return params_to_vec(dW1, db1, dW2, db2)\n\n    def run_verification(params):\n        W1, b1, W2, b2 = params\n        \n        # 1. Compute analytical gradient\n        grad_analytic = analytical_gradient(W1, b1, W2, b2)\n        \n        # 2. Compute finite-difference gradient for each epsilon\n        theta_base = params_to_vec(W1, b1, W2, b2)\n        loss_base, _, _, _ = forward_pass(W1, b1, W2, b2)\n        \n        errors = []\n        for eps in epsilons:\n            grad_fd = np.zeros_like(theta_base)\n            for k in range(len(theta_base)):\n                theta_p = np.copy(theta_base)\n                theta_p[k] += eps\n                \n                W1_p, b1_p, W2_p, b2_p = vec_to_params(theta_p)\n                loss_p, _, _, _ = forward_pass(W1_p, b1_p, W2_p, b2_p)\n                \n                grad_fd[k] = (loss_p - loss_base) / eps\n            \n            error = np.linalg.norm(grad_analytic - grad_fd)\n            errors.append(error)\n\n        # 3. Compute empirical orders\n        orders = []\n        for i in range(len(epsilons) - 1):\n            s_i = np.log(errors[i] / errors[i+1]) / np.log(epsilons[i] / epsilons[i+1])\n            orders.append(s_i)\n        \n        s_med = np.median(orders)\n        \n        # 4. Perform boolean checks\n        pass_order = 0.8 = s_med = 1.2\n        pass_mono = all(errors[i] > errors[i+1] for i in range(4))\n\n        # 5. Collect results\n        err_min_eps = errors[-1]\n        \n        return [err_min_eps, s_med, bool(pass_order), bool(pass_mono)]\n\n    # --- Main execution loop ---\n    final_results = []\n    for case_params in test_cases_params:\n        result = run_verification(case_params)\n        final_results.append(result)\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string representation\n    outer_list_str = []\n    for res_list in final_results:\n        inner_list_str = []\n        for item in res_list:\n            if isinstance(item, bool):\n                inner_list_str.append(str(item))\n            else:\n                inner_list_str.append(f\"{item}\")\n        outer_list_str.append(f\"[{','.join(inner_list_str)}]\")\n    \n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n\n```", "id": "3100954"}, {"introduction": "一个理论上完全正确的公式，在计算机上实现时可能会因为溢出或下溢等数值限制而彻底失效。本练习将通过对数-指数和（Log-Sum-Exp）函数——分类模型中的一个常见构件——来探讨科学计算中这一至关重要的问题。您将亲眼见证一个朴素的实现为何不稳定，并学会如何实现一个数值稳定的健壮版本，从而理解数值稳定性对于构建可靠深度学习模型的重要性 ([@problem_id:3181541])。", "problem": "考虑函数 $f:\\mathbb{R}^n\\to\\mathbb{R}$，其定义为 $f(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$，其中 $\\mathbf{z}=(z_1,z_2,\\dots,z_n)\\in\\mathbb{R}^n$。该函数在深度学习中被广泛用作模型和损失函数的构建模块。您的任务是为 $f(\\mathbf{z})$ 实现一个数值稳定的反向传播，以计算梯度 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$，并探讨当 $\\mathbf{z}$ 的分量绝对值很大时的数值稳定性问题。请从基本微积分原理（链式法则、指数函数的导数以及自然对数的导数）出发，不要使用任何预先推导的“捷径”公式。仅使用这些原理，推导出 $\\frac{\\partial f}{\\partial z_k}$ 关于 $\\mathbf{z}$ 的数学表达式。然后，设计一个数值稳定的算法来计算 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$，该算法应能避免在 $\\lvert z_i\\rvert$ 较大时发生上溢和下溢。具体而言，您需要实现 Log-Sum-Exp (LSE) 技巧：计算 $m=\\max_i z_i$，并使用恒等式 $\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)=m+\\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$ 来稳定前向和反向计算。\n\n您的程序必须：\n- 实现一个数值稳定的前向函数 $F(\\mathbf{z})=\\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$，使用上述恒等式，其中 $m=\\max_i z_i$。\n- 实现反向函数，该函数使用您推导出的表达式返回 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$，并通过减去 $m$ 的方式进行数值稳定的计算。\n- 实现一个朴素（不稳定）的反向函数，该函数直接使用 $e^{z_i}$ 而不通过减去 $m$ 来进行移位。\n- 通过使用步长 $\\varepsilon$ 逐个扰动坐标，实现 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ 的中心有限差分近似。此过程应使用数值稳定的前向计算。使用 $\\varepsilon=10^{-6}$。\n\n对于下面测试套件中的每个测试用例向量 $\\mathbf{z}$，计算：\n$1.$ 您的数值稳定解析梯度与中心有限差分近似之间的最大绝对差 $d_{\\text{stable}}=\\max_k \\left\\lvert \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)_k - \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k \\right\\rvert$。\n$2.$ 一个布尔值 $b_{\\text{naive}}$，如果朴素梯度包含任何非有限值（即 $\\infty$ 或 $\\mathrm{NaN}$），则为 $\\text{True}$，否则为 $\\text{False}$。\n\n测试套件（每个 $\\mathbf{z}$ 是一个行向量）：\n- 案例 $1$: $\\mathbf{z}=[1000,1000,1000]$。\n- 案例 $2$: $\\mathbf{z}=[1000,-1000,0]$。\n- 案例 $3$: $\\mathbf{z}=[-1000,-1000,-999]$。\n- 案例 $4$: $\\mathbf{z}=[800,-800,800,-800]$。\n\n覆盖率设计原理：\n- 案例 1 测试对称性和大的相等值（梯度分量预计相等）。\n- 案例 2 测试同时包含极大正值和极大负值的极端差异情况。\n- 案例 3 测试当所有条目都为非常大的负数但彼此接近时，可能出现的近似抵消和下溢风险。\n- 案例 4 测试包含重复的多个大幅值条目。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，条目顺序为 $[d_{\\text{stable}}^{(1)},b_{\\text{naive}}^{(1)},d_{\\text{stable}}^{(2)},b_{\\text{naive}}^{(2)},d_{\\text{stable}}^{(3)},b_{\\text{naive}}^{(3)},d_{\\text{stable}}^{(4)},b_{\\text{naive}}^{(4)}]$，其中上标表示案例编号。所有浮点数量必须以默认浮点表示法输出，布尔值必须输出为 $\\text{True}$ 或 $\\text{False}$。此问题不涉及任何物理单位、角度单位或百分比。", "solution": "该问题是有效的。它在科学上基于微积分和数值分析的原理，问题陈述清晰，目标明确，并为得出唯一解提供了所有必要的信息和约束。\n\n本题要求我们推导并实现函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 的数值稳定反向传播，该函数定义为 $f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right)$，其中 $\\mathbf{z} = (z_1, z_2, \\dots, z_n)$。\n\n### 1. 梯度的数学推导\n\n主要任务是求 $f(\\mathbf{z})$ 的梯度，即偏导数向量 $\\nabla_{\\mathbf{z}} f(\\mathbf{z}) = \\left(\\frac{\\partial f}{\\partial z_1}, \\frac{\\partial f}{\\partial z_2}, \\dots, \\frac{\\partial f}{\\partial z_n}\\right)$。我们将使用基本微积分原理推导单个分量 $\\frac{\\partial f}{\\partial z_k}$ 的表达式。\n\n让我们使用链式法则来分解函数 $f(\\mathbf{z})$。定义一个中间变量 $S(\\mathbf{z})$ 和一个函数 $g(S)$：\n$$ S(\\mathbf{z}) = \\sum_{i=1}^{n} e^{z_i} $$\n$$ g(S) = \\log(S) $$\n因此，$f(\\mathbf{z}) = g(S(\\mathbf{z}))$。\n\n根据多元链式法则，$f$ 对 $z_k$ 的偏导数为：\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{d g}{d S} \\cdot \\frac{\\partial S}{\\partial z_k} $$\n\n首先，我们计算 $g(S)$ 对 $S$ 的导数：\n$$ \\frac{d g}{d S} = \\frac{d}{dS} \\log(S) = \\frac{1}{S} $$\n\n接下来，我们计算 $S(\\mathbf{z})$ 对 $z_k$ 的偏导数：\n$$ \\frac{\\partial S}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left(\\sum_{i=1}^{n} e^{z_i}\\right) $$\n由于微分算子的线性性质，我们可以将导数移到求和号内部：\n$$ \\frac{\\partial S}{\\partial z_k} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial z_k} (e^{z_i}) $$\n导数 $\\frac{\\partial}{\\partial z_k} (e^{z_i})$ 的值取决于 $i$ 是否等于 $k$。\n- 如果 $i=k$，则 $\\frac{\\partial}{\\partial z_k} (e^{z_k}) = e^{z_k}$。\n- 如果 $i \\neq k$，则 $z_i$ 相对于 $z_k$ 被视为常数，所以 $\\frac{\\partial}{\\partial z_k} (e^{z_i}) = 0$。\n\n因此，求和式简化为仅剩 $i=k$ 的一项：\n$$ \\frac{\\partial S}{\\partial z_k} = e^{z_k} $$\n\n将这些结果代回链式法则方程，我们得到：\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{1}{S} \\cdot e^{z_k} = \\frac{1}{\\sum_{i=1}^{n} e^{z_i}} \\cdot e^{z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}} $$\n这个表达式给出了梯度 $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ 的第 $k$ 个分量。因此，梯度向量是对输入向量 $\\mathbf{z}$ 应用 softmax 函数的结果。\n\n### 2. 数值稳定性与 Log-Sum-Exp 技巧\n\n推导出的表达式 $\\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$ 在数值上是不稳定的。\n- **上溢**：如果任何 $z_i$ 是一个大的正数（例如 $1000$），$e^{z_i}$ 将会超出标准浮点类型能表示的最大值，导致无穷大 (`inf`)。随后的除法 `inf/inf` 将导致 `NaN` (非数值)。\n- **下溢**：如果所有 $z_i$ 都是大的负数（例如 $-1000$），每个 $e^{z_i}$ 将会下溢为 $0$。分母中的和将变为 $0$，导致除以零，这也会产生 `inf` 或 `NaN`。\n\n为了解决这个问题，我们使用“Log-Sum-Exp”(LSE) 技巧。令 $m = \\max_{i} z_i$。我们可以用非零常数 $e^{-m}$ 乘以梯度表达式的分子和分母：\n$$ \\frac{\\partial f}{\\partial z_k} = \\frac{e^{z_k} \\cdot e^{-m}}{\\left(\\sum_{i=1}^{n} e^{z_i}\\right) \\cdot e^{-m}} = \\frac{e^{z_k - m}}{\\sum_{i=1}^{n} e^{z_i - m}} $$\n这个稳定化的公式避免了上溢，因为每个指数的参数 $z_i - m$ 总是小于或等于 $0$。因此，每个项 $e^{z_i - m}$ 都在 $(0, 1]$ 的范围内。最大值为 $1$，当 $z_i = m$ 时出现。这个稳定的公式也避免了由下溢引起的除零错误，因为分母求和项中至少有一项是 $e^0 = 1$，确保了和总是至少为 $1$。\n\n同样地，这个技巧也应用于 $f(\\mathbf{z})$ 本身的前向传播计算中：\n$$ f(\\mathbf{z}) = \\log\\left(\\sum_{i=1}^{n} e^{z_i}\\right) = \\log\\left(e^m \\sum_{i=1}^{n} e^{z_i-m}\\right) = \\log(e^m) + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) = m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right) $$\n这个形式用于数值稳定的前向计算。\n\n### 3. 算法实现\n\n该问题要求实现四个函数并比较它们的结果。\n\n1.  **数值稳定的前向函数，$F(\\mathbf{z})$**：该函数首先计算 $m = \\max_i z_i$，然后返回 $m + \\log\\left(\\sum_{i=1}^{n} e^{z_i-m}\\right)$ 的值。\n\n2.  **数值稳定的反向函数，$\\nabla_{\\mathbf{z}} f(\\mathbf{z})$**：该函数使用稳定的 softmax 公式计算梯度。它首先找到 $m = \\max_i z_i$。然后计算分子向量 $[e^{z_1-m}, e^{z_2-m}, \\dots, e^{z_n-m}]$ 和分母的和 $\\sum_{i=1}^{n} e^{z_i-m}$。最终的梯度向量通过将每个分子除以和得到。\n\n3.  **朴素反向函数**：该函数直接为每个分量 $k$ 实现原始公式 $\\frac{e^{z_k}}{\\sum_{i=1}^{n} e^{z_i}}$。由于上溢或下溢，预计它会对给定的测试用例产生非有限值（`inf` 或 `NaN`）。\n\n4.  **中心有限差分近似，$(\\nabla_{\\mathbf{z}} f(\\mathbf{z}))^{\\text{FD}}$**：这作为一个数值基准，用于验证解析梯度实现的正确性。对于每个分量 $k$，它使用中心差分公式和一个小的步长 $\\varepsilon = 10^{-6}$ 来近似偏导数：\n    $$ \\left(\\nabla_{\\mathbf{z}} f(\\mathbf{z})\\right)^{\\text{FD}}_k = \\frac{F(\\mathbf{z} + \\varepsilon \\mathbf{e}_k) - F(\\mathbf{z} - \\varepsilon \\mathbf{e}_k)}{2\\varepsilon} $$\n    其中 $\\mathbf{e}_k$ 是在索引 $k$ 处为 $1$、其余处为 $0$ 的标准基向量。为了对大幅值输入获得有意义的结果，此计算必须使用数值稳定的前向函数 $F(\\mathbf{z})$。\n\n程序将对提供的测试向量执行这些函数，并计算稳定解析梯度与有限差分近似之间的最大绝对差 $d_{\\text{stable}}$，以及一个布尔标志 $b_{\\text{naive}}$，指示朴素梯度计算是否产生了任何非有限数。", "answer": "```python\nimport numpy as np\n\ndef stable_forward_lse(z: np.ndarray) -> float:\n    \"\"\"\n    Computes the Log-Sum-Exp function in a numerically stable way.\n    f(z) = log(sum(exp(z_i)))\n    \"\"\"\n    m = np.max(z)\n    # The Log-Sum-Exp identity: m + log(sum(exp(z_i - m)))\n    return m + np.log(np.sum(np.exp(z - m)))\n\ndef backward_stable(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function in a stable manner.\n    This is equivalent to the softmax function.\n    grad_k = exp(z_k - m) / sum(exp(z_i - m))\n    \"\"\"\n    m = np.max(z)\n    shifted_exp_z = np.exp(z - m)\n    return shifted_exp_z / np.sum(shifted_exp_z)\n\ndef backward_naive(z: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function naively.\n    This is prone to overflow/underflow.\n    grad_k = exp(z_k) / sum(exp(z_i))\n    \"\"\"\n    # This is expected to fail for large inputs\n    exp_z = np.exp(z)\n    sum_exp_z = np.sum(exp_z)\n    return exp_z / sum_exp_z\n\ndef gradient_finite_difference(z: np.ndarray, epsilon: float = 1e-6) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the Log-Sum-Exp function using central finite differences.\n    \"\"\"\n    n = z.shape[0]\n    grad_fd = np.zeros(n, dtype=np.float64)\n    \n    # Use a copy of z to avoid modifying the original\n    z_temp = z.astype(np.float64)\n\n    for i in range(n):\n        # Store original value\n        original_zi = z_temp[i]\n        \n        # Calculate f(z + epsilon * e_i)\n        z_temp[i] = original_zi + epsilon\n        f_plus = stable_forward_lse(z_temp)\n        \n        # Calculate f(z - epsilon * e_i)\n        z_temp[i] = original_zi - epsilon\n        f_minus = stable_forward_lse(z_temp)\n        \n        # Restore original value\n        z_temp[i] = original_zi\n        \n        # Central difference formula\n        grad_fd[i] = (f_plus - f_minus) / (2 * epsilon)\n        \n    return grad_fd\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        np.array([1000.0, 1000.0, 1000.0]),\n        np.array([1000.0, -1000.0, 0.0]),\n        np.array([-1000.0, -1000.0, -999.0]),\n        np.array([800.0, -800.0, 800.0, -800.0])\n    ]\n    \n    epsilon = 1e-6\n    results = []\n\n    for z in test_cases:\n        # 1. Compute stable analytical gradient and finite difference approximation\n        grad_stable = backward_stable(z)\n        grad_fd = gradient_finite_difference(z, epsilon)\n        \n        # Calculate the maximum absolute difference\n        d_stable = np.max(np.abs(grad_stable - grad_fd))\n        \n        # 2. Compute naive gradient and check for non-finite values\n        # Suppress RuntimeWarning for overflow in exp and invalid value in divide\n        with np.errstate(over='ignore', invalid='ignore'):\n             grad_naive = backward_naive(z)\n        \n        b_naive = np.any(~np.isfinite(grad_naive))\n        \n        results.append(d_stable)\n        results.append(b_naive)\n        \n    output_parts = []\n    for item in results:\n        if isinstance(item, (bool, np.bool_)):\n            output_parts.append(str(item))\n        else:\n            output_parts.append(f\"{item}\")\n            \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3181541"}]}