{"hands_on_practices": [{"introduction": "在许多现实世界的分类任务中，我们最终关心的评估指标（如 F1-score、AUC）往往是不可微的，这使得它们无法直接通过梯度下降进行优化。本练习 [@problem_id:3145404] 旨在引导你理解这一核心挑战，并亲手构建一个平滑、可微的代理损失函数（Tversky loss）。这是一种在机器学习中解决此类问题的常用且强大的技术，能够有效提升你在理论分析和损失函数设计方面的能力。", "problem": "您正在训练一个二元分类器，其网络输出产生分数 $s_{i} \\in (0,1)$，该分数被解释为第 $i$ 个样本为正例的概率。训练集包含 $n$ 个带标签的样本，其真实标签为 $y_{i} \\in \\{0,1\\}$。目标是理解为什么直接优化 $F_{1}$-分数是困难的，并基于Tversky指数构建和分析一个平滑的代理损失。在整个过程中，仅使用以下基本定义：基于决策阈值的真阳性、假阳性和假阴性的定义；精确率和召回率作为这些计数的比率的定义；以及 $F_{1}$ 作为精确率和召回率的调和平均数的定义。\n\n1) 仅使用精确率、召回率和 $F_{1}$ 的定义，从第一性原理出发，论证为什么将 $s_{i}$ 视为连续的网络输出并通过阈值 $t \\in (0,1)$ 将其转换为硬决策，会使经验 $F_{1}$-分数成为关于分数向量 $\\mathbf{s} = (s_{1},\\dots,s_{n})$ 的一个不可分解且几乎处处不可微的函数。您的论证必须从混淆矩阵计数以 $\\mathbf{1}[s_{i} \\ge t]$ 表示的定义开始，并避免使用任何特殊用途的优化公式。\n\n2) 为了获得平滑的代理，通过将指示函数替换为由 $s_{i}$ 和 $1 - s_{i}$ 构建的软对应项，来构造一个混淆矩阵计数的可微替代。引入非负权重 $\\alpha$ 和 $\\beta$，分别用来缩放假阳性和假阴性的相对贡献。从这些要素出发，推导出一个小批量上的软Tversky指数的可微表达式，并将相关的Tversky损失定义为1减去该指数。引入一个小的 $\\epsilon \\ge 0$ 以确保分母非零，但在此步骤中不要代入数值。\n\n3) 对您的Tversky损失函数关于单个分数 $s_{k}$ 求导，以获得 $\\frac{\\partial \\mathcal{L}}{\\partial s_{k}}$ 的一个闭式表达式，该表达式用批量软计数、$\\alpha$、$\\beta$ 和 $\\epsilon$ 表示。简化您的表达式，以明确显示其符号和大小如何依赖于 $y_{k} = 1$ 还是 $y_{k} = 0$。\n\n4) 设 $n = 4$，标签为 $y = (1,0,1,0)$，分数为 $s = (0.8,0.6,0.4,0.3)$。对于 $\\alpha = 0.3$，$\\beta = 0.7$ 和 $\\epsilon = 0$，计算此批次上的梯度，并以精确形式（不进行四舍五入）报告四个分量 $\\left(\\frac{\\partial \\mathcal{L}}{\\partial s_{1}},\\frac{\\partial \\mathcal{L}}{\\partial s_{2}},\\frac{\\partial \\mathcal{L}}{\\partial s_{3}},\\frac{\\partial \\mathcal{L}}{\\partial s_{4}}\\right)$。\n\n5) 现在假设分数是校准概率，即样本 $i$ 的真实正例概率等于 $q_{i} = s_{i}$，并假设各样本之间统计独立。对于一个固定的阈值 $t \\in (0,1)$，通过将每个硬决策替换为其指示函数 $\\mathbf{1}[s_{i} \\ge t]$ 并将每个未观察到的标签替换为其概率 $q_{i}$，来定义代理期望计数。仅使用这些替换和计数的定义，将代理期望 $F_{1}$ 写成代理期望精确率和代理期望召回率的调和平均数，并将其简化为代理期望计数的线性函数的单个比率。对于给定的分数 $s = (0.8,0.6,0.4,0.3)$，确定在所有 $t \\in (0,1)$ 上最大化此代理期望 $F_{1}$ 的阈值 $t^{\\star}$；如果存在多个最大化器，报告其中最大的阈值。您的最终答案必须是一个行矩阵，包含第4部分得到的四个梯度分量，后跟此阈值，全部以精确形式写出。不需要四舍五入。", "solution": "我们从给定的定义开始，一步步进行。\n\n1) 根据定义，对于阈值 $t \\in (0,1)$ 和分数 $s_{i}$，硬预测为 $\\hat{y}_{i} = \\mathbf{1}[s_{i} \\ge t]$。混淆矩阵计数由下式给出\n$$\nTP = \\sum_{i=1}^{n} \\mathbf{1}[y_{i} = 1] \\, \\mathbf{1}[s_{i} \\ge t], \\quad FP = \\sum_{i=1}^{n} \\mathbf{1}[y_{i} = 0] \\, \\mathbf{1}[s_{i} \\ge t], \\quad FN = \\sum_{i=1}^{n} \\mathbf{1}[y_{i} = 1] \\, \\mathbf{1}[s_{i}  t].\n$$\n精确率为 $P = \\frac{TP}{TP + FP}$（当 $TP + FP > 0$ 时有定义），召回率为 $R = \\frac{TP}{TP + FN}$（当 $TP + FN > 0$ 时有定义），而 $F_{1}$-分数是调和平均数 $F_{1} = \\frac{2PR}{P + R}$，在分母非零时等价于 $F_{1} = \\frac{2TP}{2TP + FP + FN}$。作为 $\\mathbf{s}$ 的函数，每个计数都是指示函数 $\\mathbf{1}[s_{i} \\ge t]$ 的和，这些函数是 $s_{i}$ 的不连续阶跃函数，并且除了在 $s_{i} = t$ 处外都是常数。因此，$TP$、$FP$ 和 $FN$ 都是 $\\mathbf{s}$ 的分段常数函数，$F_{1}$ 也是如此，其跳跃间断点出现在超平面 $s_{i} = t$ 上。因此，相对于连续输出 $\\mathbf{s}$，经验 $F_{1}$-分数在样本间是不可分解的（因为它依赖于全局计数），并且几乎处处不可微（梯度在有定义的地方为零，在决策边界上无定义），这使得直接基于梯度的优化成为一个不适定问题。\n\n2) 为了构建一个可微的代理，我们将硬贡献替换为软贡献。定义小批量上的混淆矩阵计数的软对应项为\n$$\n\\widetilde{TP} = \\sum_{i=1}^{n} y_{i} s_{i}, \\quad \\widetilde{FP} = \\sum_{i=1}^{n} (1 - y_{i}) s_{i}, \\quad \\widetilde{FN} = \\sum_{i=1}^{n} y_{i} (1 - s_{i}).\n$$\n引入非负权重 $\\alpha \\ge 0$ 和 $\\beta \\ge 0$ 来控制 $\\widetilde{FP}$ 和 $\\widetilde{FN}$ 的相对惩罚，并定义软Tversky指数为\n$$\n\\mathrm{TI}(\\mathbf{s},\\mathbf{y};\\alpha,\\beta,\\epsilon) = \\frac{\\widetilde{TP}}{\\widetilde{TP} + \\alpha \\, \\widetilde{FP} + \\beta \\, \\widetilde{FN} + \\epsilon},\n$$\n其中 $\\epsilon \\ge 0$ 是一个确保分母非零的小常数。相关的Tversky损失为\n$$\n\\mathcal{L}(\\mathbf{s},\\mathbf{y};\\alpha,\\beta,\\epsilon) = 1 - \\mathrm{TI}(\\mathbf{s},\\mathbf{y};\\alpha,\\beta,\\epsilon) = 1 - \\frac{\\widetilde{TP}}{\\widetilde{TP} + \\alpha \\, \\widetilde{FP} + \\beta \\, \\widetilde{FN} + \\epsilon}.\n$$\n对于 $\\epsilon > 0$，此构造在 $\\mathbf{s}$ 上是可微的，并且即使在 $\\epsilon = 0$ 时，只要分母非零，它也保持良定义。\n\n3) 令\n$$\nN = \\widetilde{TP} = \\sum_{i=1}^{n} y_{i} s_{i}, \\quad A = \\widetilde{FP} = \\sum_{i=1}^{n} (1 - y_{i}) s_{i}, \\quad B = \\widetilde{FN} = \\sum_{i=1}^{n} y_{i} (1 - s_{i}),\n$$\n并用\n$$\nD = N + \\alpha A + \\beta B + \\epsilon.\n$$\n表示分母。那么损失为 $\\mathcal{L} = 1 - \\frac{N}{D}$。对 $s_{k}$ 求导。首先注意软计数的偏导数：\n$$\n\\frac{\\partial N}{\\partial s_{k}} = y_{k}, \\quad \\frac{\\partial A}{\\partial s_{k}} = 1 - y_{k}, \\quad \\frac{\\partial B}{\\partial s_{k}} = - y_{k}.\n$$\n根据链式法则，\n$$\n\\frac{\\partial D}{\\partial s_{k}} = \\frac{\\partial N}{\\partial s_{k}} + \\alpha \\frac{\\partial A}{\\partial s_{k}} + \\beta \\frac{\\partial B}{\\partial s_{k}} = y_{k} + \\alpha (1 - y_{k}) - \\beta y_{k} = y_{k} (1 - \\beta) + \\alpha (1 - y_{k}).\n$$\n对 $-N/D$ 使用商法则，我们得到\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} = - \\frac{ \\left(\\frac{\\partial N}{\\partial s_{k}}\\right) D - N \\left(\\frac{\\partial D}{\\partial s_{k}}\\right)}{D^{2}} = - \\frac{ y_{k} D - N \\left( y_{k} (1 - \\beta) + \\alpha (1 - y_{k}) \\right)}{D^{2}}.\n$$\n这可以按情况简化为\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} =\n\\begin{cases}\n- \\dfrac{ D - N (1 - \\beta) }{ D^{2} },   \\text{if } y_{k} = 1, \\\\\n\\dfrac{ N \\alpha }{ D^{2} },   \\text{if } y_{k} = 0.\n\\end{cases}\n$$\n因此，对于正标签（$y_{k} = 1$），当 $D > N (1 - \\beta)$ 时，梯度分量为负，将 $s_{k}$ 向上推以增加重叠；对于负标签（$y_{k} = 0$），梯度分量为非负且与 $N \\alpha$ 成正比，将 $s_{k}$ 向下推以减少假阳性。\n\n4) 对于 $y = (1,0,1,0)$ 和 $s = (0.8,0.6,0.4,0.3)$，以及 $\\alpha = 0.3$，$\\beta = 0.7$ 和 $\\epsilon = 0$，计算\n$$\nN = \\sum_{i=1}^{4} y_{i} s_{i} = 0.8 + 0.4 = 1.2 = \\frac{6}{5},\n$$\n$$\nA = \\sum_{i=1}^{4} (1 - y_{i}) s_{i} = 0.6 + 0.3 = 0.9 = \\frac{9}{10},\n$$\n$$\nB = \\sum_{i=1}^{4} y_{i} (1 - s_{i}) = (1 - 0.8) + (1 - 0.4) = 0.2 + 0.6 = 0.8 = \\frac{4}{5}.\n$$\n分母为\n$$\nD = N + \\alpha A + \\beta B = \\frac{6}{5} + \\frac{3}{10} \\cdot \\frac{9}{10} + \\frac{7}{10} \\cdot \\frac{4}{5} = \\frac{120}{100} + \\frac{27}{100} + \\frac{56}{100} = \\frac{203}{100}.\n$$\n对于 $y_{k} = 1$ 的索引（即 $k = 1,3$），\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} = - \\frac{ D - N (1 - \\beta) }{ D^{2} } = - \\frac{ \\frac{203}{100} - \\frac{6}{5} \\cdot \\frac{3}{10} }{ \\left( \\frac{203}{100} \\right)^{2} } = - \\frac{ \\frac{203}{100} - \\frac{18}{50} }{ \\frac{41209}{10000} } = - \\frac{ \\frac{203}{100} - \\frac{36}{100} }{ \\frac{41209}{10000} } = - \\frac{ \\frac{167}{100} }{ \\frac{41209}{10000} } = - \\frac{16700}{41209}.\n$$\n对于 $y_{k} = 0$ 的索引（即 $k = 2,4$），\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial s_{k}} = \\frac{ N \\alpha }{ D^{2} } = \\frac{ \\frac{6}{5} \\cdot \\frac{3}{10} }{ \\left( \\frac{203}{100} \\right)^{2} } = \\frac{ \\frac{18}{50} }{ \\frac{41209}{10000} } = \\frac{ \\frac{36}{100} }{ \\frac{41209}{10000} } = \\frac{3600}{41209}.\n$$\n因此，\n$$\n\\left( \\frac{\\partial \\mathcal{L}}{\\partial s_{1}}, \\frac{\\partial \\mathcal{L}}{\\partial s_{2}}, \\frac{\\partial \\mathcal{L}}{\\partial s_{3}}, \\frac{\\partial \\mathcal{L}}{\\partial s_{4}} \\right) = \\left( - \\frac{16700}{41209}, \\frac{3600}{41209}, - \\frac{16700}{41209}, \\frac{3600}{41209} \\right).\n$$\n\n5) 在校准假设下，取 $q_{i} = s_{i}$ 并假设独立性。对于阈值 $t \\in (0,1)$，代理期望计数为\n$$\n\\mathbb{E}[\\mathrm{TP}](t) = \\sum_{i=1}^{n} q_{i} \\, \\mathbf{1}[s_{i} \\ge t], \\quad \\mathbb{E}[\\mathrm{FP}](t) = \\sum_{i=1}^{n} (1 - q_{i}) \\, \\mathbf{1}[s_{i} \\ge t], \\quad \\mathbb{E}[\\mathrm{FN}](t) = \\sum_{i=1}^{n} q_{i} \\, \\mathbf{1}[s_{i}  t].\n$$\n通过将比率中的计数替换为这些期望值来定义代理期望 $F_{1}$：\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\, \\mathbb{E}[\\mathrm{TP}](t)}{2 \\, \\mathbb{E}[\\mathrm{TP}](t) + \\mathbb{E}[\\mathrm{FP}](t) + \\mathbb{E}[\\mathrm{FN}](t)}.\n$$\n对于 $s = (0.8,0.6,0.4,0.3)$ 和 $q = s$，$\\widetilde{F}_{1}(t)$ 在不同的分数之间是分段常数，仅当 $t$ 穿过 $\\{0.8, 0.6, 0.4, 0.3\\}$ 中的一个值时才发生变化。在由这些断点确定的五个区域上进行评估：\n\n(i) $t > 0.8$: 没有预测为正例的。那么 $\\mathbb{E}[\\mathrm{TP}] = 0$，$\\mathbb{E}[\\mathrm{FP}] = 0$，$\\mathbb{E}[\\mathrm{FN}] = 0.8 + 0.6 + 0.4 + 0.3 = 2.1$，所以 $\\widetilde{F}_{1}(t) = 0$。\n\n(ii) $0.8 \\ge t > 0.6$: 预测为正例的有 $\\{0.8\\}$。那么 $\\mathbb{E}[\\mathrm{TP}] = 0.8$，$\\mathbb{E}[\\mathrm{FP}] = 1 - 0.8 = 0.2$，$\\mathbb{E}[\\mathrm{FN}] = 0.6 + 0.4 + 0.3 = 1.3$，所以\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 0.8}{2 \\cdot 0.8 + 0.2 + 1.3} = \\frac{1.6}{3.1}.\n$$\n\n(iii) $0.6 \\ge t > 0.4$: 预测为正例的有 $\\{0.8, 0.6\\}$。那么 $\\mathbb{E}[\\mathrm{TP}] = 0.8 + 0.6 = 1.4$，$\\mathbb{E}[\\mathrm{FP}] = (1 - 0.8) + (1 - 0.6) = 0.2 + 0.4 = 0.6$，$\\mathbb{E}[\\mathrm{FN}] = 0.4 + 0.3 = 0.7$，所以\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 1.4}{2 \\cdot 1.4 + 0.6 + 0.7} = \\frac{2.8}{4.1}.\n$$\n\n(iv) $0.4 \\ge t > 0.3$: 预测为正例的有 $\\{0.8, 0.6, 0.4\\}$。那么 $\\mathbb{E}[\\mathrm{TP}] = 0.8 + 0.6 + 0.4 = 1.8$，$\\mathbb{E}[\\mathrm{FP}] = (1 - 0.8) + (1 - 0.6) + (1 - 0.4) = 0.2 + 0.4 + 0.6 = 1.2$，$\\mathbb{E}[\\mathrm{FN}] = 0.3$，所以\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 1.8}{2 \\cdot 1.8 + 1.2 + 0.3} = \\frac{3.6}{5.1}.\n$$\n\n(v) $0.3 \\ge t > 0$: 预测为正例的有 $\\{0.8, 0.6, 0.4, 0.3\\}$。那么 $\\mathbb{E}[\\mathrm{TP}] = 0.8 + 0.6 + 0.4 + 0.3 = 2.1$，$\\mathbb{E}[\\mathrm{FP}] = (1 - 0.8) + (1 - 0.6) + (1 - 0.4) + (1 - 0.3) = 0.2 + 0.4 + 0.6 + 0.7 = 1.9$，$\\mathbb{E}[\\mathrm{FN}] = 0$，所以\n$$\n\\widetilde{F}_{1}(t) = \\frac{2 \\cdot 2.1}{2 \\cdot 2.1 + 1.9 + 0} = \\frac{4.2}{6.1}.\n$$\n比较精确值，\n$$\n\\frac{1.6}{3.1} \\approx 0.516129, \\quad \\frac{2.8}{4.1} \\approx 0.682927, \\quad \\frac{3.6}{5.1} \\approx 0.705882, \\quad \\frac{4.2}{6.1} \\approx 0.688525,\n$$\n最大值在区域(iv)上达到，即对于任何满足 $0.4 \\ge t > 0.3$ 的阈值 $t$。根据选择最大化阈值中最大者的平局打破规则，我们报告 $t^{\\star} = 0.4 = \\frac{2}{5}$。\n\n按指定顺序收集所求数量，最终答案是由四个梯度分量和 $t^{\\star}$ 组成的行矩阵，全部以精确形式表示。", "answer": "$$\\boxed{\\begin{pmatrix}-\\frac{16700}{41209}  \\frac{3600}{41209}  -\\frac{16700}{41209}  \\frac{3600}{41209}  \\frac{2}{5}\\end{pmatrix}}$$", "id": "3145404"}, {"introduction": "传统的监督学习任务通常处理固定大小和顺序的输出，但现代应用（如目标检测）常常需要预测一个元素的无序集合。本练习 [@problem_id:3145459] 探索了如何设计一个能够尊重集合的“置换不变性”的损失函数。你将学习使用基于分配的方法（如匈牙利算法的思想）来动态匹配预测与目标，并发现这类损失函数的一个关键特性——梯度不连续点，这对模型训练具有重要启示。", "problem": "考虑一个集合预测任务，其中模型输出一个包含两个元素的无序集合。每个元素由一个标量坐标和一个二元类别概率组成。无序的目标集合也包含两个元素，每个元素有一个标量坐标和一个二元类别标签。训练目标必须相对于预测集合中元素的顺序具有排列不变性。从集合没有内在顺序的原则以及旨在通过最小化总成本来将预测与目标配对的分配问题公式出发，通过结合用于分类的二元交叉熵 (CE) 和用于坐标回归的均方误差 (MSE) 来推导一个排列不变的损失。\n\n使用以下具体实例：\n- 预测元素 $1$ 的类别 $1$ 的概率为 $\\hat{p}_1 = 0.1$，坐标为 $\\hat{z}_1(t) = t$，其中 $t \\in \\mathbb{R}$ 是一个标量参数。\n- 预测元素 $2$ 的类别 $1$ 的概率为 $\\hat{p}_2 = 0.8$，坐标为 $\\hat{z}_2(t) = \\frac{6}{5}$。\n- 目标元素 $a$ 的类别标签为 $c_a = 0$，坐标为 $z_a^{\\star} = 0$。\n- 目标元素 $b$ 的类别标签为 $c_b = 1$，坐标为 $z_b^{\\star} = 1$。\n\n将每对的损失定义为一个二元交叉熵 (CE) 分类项和一个由带有因子 $\\frac{1}{2}$ 的均方误差 (MSE) 给出的坐标回归项之和：\n- 对于一个预测概率 $\\hat{p}$ 和目标标签 $c \\in \\{0,1\\}$，二元交叉熵为 $-\\big(c \\ln(\\hat{p}) + (1-c) \\ln(1-\\hat{p})\\big)$。\n- 对于一个预测坐标 $\\hat{z}$ 和目标坐标 $z^{\\star}$，回归项为 $\\frac{1}{2}(\\hat{z} - z^{\\star})^{2}$。\n\n你的任务：\n1. 使用分配问题的视角和集合的排列不变性定义，将排列不变的总损失 $L(t)$ 推导为两个预测到两个目标的所有分配中，两个配对损失之和的最小值。\n2. 在所描述的 $2 \\times 2$ 情况下，将 $L(t)$ 明确写成两个候选总和（两种可能的一一对应分配）的最小值。\n3. 确定最优分配发生变化的临界值 $t^{\\star}$。\n4. 计算单侧导数 $\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}-0}$ 和 $\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}+0}$，然后报告在 $t^{\\star}$ 处梯度的不连续性的大小，即 $\\left|\\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}+0} - \\frac{\\partial L}{\\partial t}\\big|_{t^{\\star}-0}\\right|$。\n\n将最终所求量表示为精确值。无需四舍五入。最终答案必须是单个实数。", "solution": "该问题要求为一个集合预测任务构建一个排列不变的损失函数，并分析其在预测与目标之间的最优分配发生变化点处的导数。总损失 $L(t)$被表述为分配问题的解，具体来说，是通过取所有可能的预测到目标的排列（一一对应分配）的总成本的最小值。\n\n设预测集合为 $\\hat{Y} = \\{\\hat{y}_1, \\hat{y}_2\\}$，目标集合为 $Y^{\\star} = \\{y_a^{\\star}, y_b^{\\star}\\}$。\n预测给出如下：\n$\\hat{y}_1 = (\\hat{z}_1, \\hat{p}_1) = (t, 0.1)$\n$\\hat{y}_2 = (\\hat{z}_2, \\hat{p}_2) = (\\frac{6}{5}, 0.8)$\n\n目标给出如下：\n$y_a^{\\star} = (z_a^{\\star}, c_a) = (0, 0)$\n$y_b^{\\star} = (z_b^{\\star}, c_b) = (1, 1)$\n\n将一个预测 $\\hat{y} = (\\hat{z}, \\hat{p})$ 分配给一个目标 $y^{\\star} = (z^{\\star}, c)$ 的每对损失（或成本）是一个二元交叉熵 (CE) 项和一个均方误差 (MSE) 项之和：\n$C(\\hat{y}, y^{\\star}) = L_{cls}(\\hat{p}, c) + L_{reg}(\\hat{z}, z^{\\star})$\n其中 $L_{cls}(\\hat{p}, c) = -\\big(c \\ln(\\hat{p}) + (1-c) \\ln(1-\\hat{p})\\big)$ 且 $L_{reg}(\\hat{z}, z^{\\star}) = \\frac{1}{2}(\\hat{z} - z^{\\star})^{2}$。\n\n我们首先计算四种可能的配对成本，记为 $C_{ij}$，表示将预测 $i$ 分配给目标 $j$。\n\n1.  将 $\\hat{y}_1$ 分配给 $y_a^{\\star}$ 的成本 ($C_{1a}$):\n    $L_{cls}(\\hat{p}_1, c_a) = -(0 \\cdot \\ln(0.1) + (1-0) \\cdot \\ln(1-0.1)) = -\\ln(0.9) = \\ln(\\frac{1}{0.9}) = \\ln(\\frac{10}{9})$。\n    $L_{reg}(\\hat{z}_1, z_a^{\\star}) = \\frac{1}{2}(t - 0)^2 = \\frac{1}{2}t^2$。\n    $C_{1a}(t) = \\ln(\\frac{10}{9}) + \\frac{1}{2}t^2$。\n\n2.  将 $\\hat{y}_2$ 分配给 $y_b^{\\star}$ 的成本 ($C_{2b}$):\n    $L_{cls}(\\hat{p}_2, c_b) = -(1 \\cdot \\ln(0.8) + (1-1) \\cdot \\ln(1-0.8)) = -\\ln(0.8) = \\ln(\\frac{1}{0.8}) = \\ln(\\frac{5}{4})$。\n    $L_{reg}(\\hat{z}_2, z_b^{\\star}) = \\frac{1}{2}(\\frac{6}{5} - 1)^2 = \\frac{1}{2}(\\frac{1}{5})^2 = \\frac{1}{50}$。\n    $C_{2b} = \\ln(\\frac{5}{4}) + \\frac{1}{50}$。\n\n3.  将 $\\hat{y}_1$ 分配给 $y_b^{\\star}$ 的成本 ($C_{1b}$):\n    $L_{cls}(\\hat{p}_1, c_b) = -(1 \\cdot \\ln(0.1) + (1-1) \\cdot \\ln(1-0.1)) = -\\ln(0.1) = \\ln(10)$。\n    $L_{reg}(\\hat{z}_1, z_b^{\\star}) = \\frac{1}{2}(t - 1)^2$。\n    $C_{1b}(t) = \\ln(10) + \\frac{1}{2}(t - 1)^2$。\n\n4.  将 $\\hat{y}_2$ 分配给 $y_a^{\\star}$ 的成本 ($C_{2a}$):\n    $L_{cls}(\\hat{p}_2, c_a) = -(0 \\cdot \\ln(0.8) + (1-0) \\cdot \\ln(1-0.8)) = -\\ln(0.2) = \\ln(\\frac{1}{0.2}) = \\ln(5)$。\n    $L_{reg}(\\hat{z}_2, z_a^{\\star}) = \\frac{1}{2}(\\frac{6}{5} - 0)^2 = \\frac{1}{2} \\cdot \\frac{36}{25} = \\frac{18}{25}$。\n    $C_{2a} = \\ln(5) + \\frac{18}{25}$。\n\n对于一个 $2 \\times 2$ 问题，存在两种可能的一一对应分配（排列）。\n分配 1：$(\\hat{y}_1 \\to y_a^{\\star}, \\hat{y}_2 \\to y_b^{\\star})$。总损失为 $L_1(t) = C_{1a}(t) + C_{2b}$。\n$L_1(t) = \\left(\\ln(\\frac{10}{9}) + \\frac{1}{2}t^2\\right) + \\left(\\ln(\\frac{5}{4}) + \\frac{1}{50}\\right) = \\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50}$。\n\n分配 2：$(\\hat{y}_1 \\to y_b^{\\star}, \\hat{y}_2 \\to y_a^{\\star})$。总损失为 $L_2(t) = C_{1b}(t) + C_{2a}$。\n$L_2(t) = \\left(\\ln(10) + \\frac{1}{2}(t-1)^2\\right) + \\left(\\ln(5) + \\frac{18}{25}\\right) = \\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}$。\n\n排列不变的总损失 $L(t)$ 是所有可能分配的损失中的最小值。\n$L(t) = \\min(L_1(t), L_2(t))$。这就完成了任务1和任务2。\n\n为了找到最优分配发生变化的临界值 $t^{\\star}$ (任务3)，我们令 $L_1(t) = L_2(t)$：\n$\\frac{1}{2}t^2 + \\ln(\\frac{10}{9}) + \\ln(\\frac{5}{4}) + \\frac{1}{50} = \\frac{1}{2}(t-1)^2 + \\ln(10) + \\ln(5) + \\frac{18}{25}$\n$\\frac{1}{2}t^2 + \\ln(10) - \\ln(9) + \\ln(5) - \\ln(4) + \\frac{1}{50} = \\frac{1}{2}(t^2 - 2t + 1) + \\ln(10) + \\ln(5) + \\frac{36}{50}$\n从两边消去公有项 ($\\frac{1}{2}t^2, \\ln(10), \\ln(5)$) 得：\n$-\\ln(9) - \\ln(4) + \\frac{1}{50} = -t + \\frac{1}{2} + \\frac{36}{50}$\n$-\\ln(36) + \\frac{1}{50} = -t + \\frac{25}{50} + \\frac{36}{50}$\n$-\\ln(36) + \\frac{1}{50} = -t + \\frac{61}{50}$\n求解 $t$：\n$t^{\\star} = \\ln(36) + \\frac{61}{50} - \\frac{1}{50} = \\ln(36) + \\frac{60}{50} = \\ln(36) + \\frac{6}{5}$。\n\n对于任务4，我们需要计算 $L(t)$ 在 $t^{\\star}$ 处的单侧导数以及梯度不连续性的大小。\n两个候选损失关于 $t$ 的导数是：\n$\\frac{\\partial L_1}{\\partial t} = \\frac{\\partial}{\\partial t} \\left(\\frac{1}{2}t^2 + \\text{constant}\\right) = t$。\n$\\frac{\\partial L_2}{\\partial t} = \\frac{\\partial}{\\partial t} \\left(\\frac{1}{2}(t-1)^2 + \\text{constant}\\right) = \\frac{1}{2} \\cdot 2(t-1) \\cdot 1 = t-1$。\n\n为了确定最小函数的哪个分支是活动的，我们分析 $L_1(t) - L_2(t)$ 的符号。从 $t^{\\star}$ 的计算中，我们有：\n$L_1(t) - L_2(t) = t - \\frac{1}{2} - \\ln(36) - \\frac{7}{10} = t - (\\ln(36) + \\frac{5}{10} + \\frac{7}{10}) = t - (\\ln(36) + \\frac{12}{10}) = t - (\\ln(36) + \\frac{6}{5})$。\n所以 $L_1(t) - L_2(t) = t - t^{\\star}$。\n- 如果 $t  t^{\\star}$，那么 $L_1(t) - L_2(t)  0$，所以 $L_1(t)  L_2(t)$。因此，$L(t) = L_1(t)$。\n- 如果 $t > t^{\\star}$，那么 $L_1(t) - L_2(t) > 0$，所以 $L_2(t)  L_1(t)$。因此，$L(t) = L_2(t)$。\n\n总损失函数是分段的：\n$L(t) = \\begin{cases} L_1(t)  \\text{if } t \\le t^{\\star} \\\\ L_2(t)  \\text{if } t > t^{\\star} \\end{cases}$\n\n在 $t=t^{\\star}$ 处的单侧导数是：\n左侧导数是 $t \\le t^{\\star}$ 时活动函数的导数：\n$\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}-0} = \\lim_{t \\to t^{\\star-}} \\frac{\\partial L}{\\partial t} = \\lim_{t \\to t^{\\star-}} \\frac{\\partial L_1}{\\partial t} = \\lim_{t \\to t^{\\star-}} t = t^{\\star}$。\n\n右侧导数是 $t > t^{\\star}$ 时活动函数的导数：\n$\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}+0} = \\lim_{t \\to t^{\\star+}} \\frac{\\partial L}{\\partial t} = \\lim_{t \\to t^{\\star+}} \\frac{\\partial L_2}{\\partial t} = \\lim_{t \\to t^{\\star+}} (t-1) = t^{\\star}-1$。\n\n在 $t^{\\star}$ 处梯度的不连续性大小是右侧导数和左侧导数之间的绝对差：\n$\\left|\\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}+0} - \\frac{\\partial L}{\\partial t}\\bigg|_{t^{\\star}-0}\\right| = \\left| (t^{\\star}-1) - t^{\\star} \\right| = |-1| = 1$。", "answer": "$$\\boxed{1}$$", "id": "3145459"}, {"introduction": "损失函数不仅可以衡量预测的准确性，更可以作为一种强大的工具，将领域知识或期望的模型行为（如公平性、单调性）融入到训练过程中。本实践练习 [@problem_id:3145476] 将通过一个编码任务，让你实现一个强制模型输出满足单调性的损失函数。你将通过设计一个自定义惩罚项，亲身体验并分析模型在“预测准确度”和“满足约束”之间的经典权衡，这是定制化机器学习解决方案的关键一步。", "problem": "给定一个标量回归任务，其中模型必须从一个二维输入预测一个实值目标，同时还要鼓励模型对于每个输入坐标都具有单调非递减的行为。你的目标是实现并评估一个组合目标函数，该函数在经验准确性和单调性违规惩罚之间进行权衡。\n\n定义与设置：\n- 考虑输入 $x \\in \\mathbb{R}^2$（写作 $x = (x_1, x_2)$）和目标 $y \\in \\mathbb{R}$。\n- 模型是线性的：$f_{\\theta}(x) = v^\\top x + c$，其中 $\\theta = (v, c)$，$v \\in \\mathbb{R}^2$，$c \\in \\mathbb{R}$。\n- 训练数据是一个确定性网格：\n  - 令 $x_1$ 和 $x_2$ 在集合 $\\{0, 0.1, 0.2, \\dots, 1.0\\}$ 上取值，通过完全笛卡尔积生成 $N = 121$ 个点。\n  - 对每个网格点，通过 $y = 2 x_1 - 1.5 x_2$ 定义目标。\n- 经验准确性项是在标准平方误差损失下的经验风险，对整个数据集取平均值：\n  $$L_{\\mathrm{mse}}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{2}\\left(f_{\\theta}(x^{(n)}) - y^{(n)}\\right)^2.$$\n- 单调性惩罚通过惩罚负的输入偏导数来强制实现相对于两个坐标的非递减行为。由于模型是线性的，输入梯度是恒定的，即 $\\frac{\\partial f_{\\theta}}{\\partial x_i} = v_i$。该惩罚项为\n  $$L_{\\mathrm{mono}}(\\theta) = \\sum_{i=1}^{2} \\max\\left(0, -\\frac{\\partial f_{\\theta}}{\\partial x_i}\\right) = \\sum_{i=1}^{2} \\max(0, -v_i).$$\n- 对于给定的非负权重 $\\lambda$，组合目标为\n  $$J_{\\lambda}(\\theta) = L_{\\mathrm{mse}}(\\theta) + \\lambda \\, L_{\\mathrm{mono}}(\\theta).$$\n\n你的任务：\n1. 从经验风险最小化的定义以及线性函数相对于其输入的梯度是常数这一事实出发，推导出 $J_{\\lambda}(\\theta)$ 相对于 $v$ 和 $c$ 的显式次梯度，这些次梯度足以实现批量梯度下降。你必须使用第一性原理来论证每一项，包括 $\\max(0, -v_i)$ 的次梯度。\n2. 实现一个批量梯度下降算法，在上述指定的数据集上针对固定的 $\\lambda$ 最小化 $J_{\\lambda}(\\theta)$，使用以下超参数：\n   - 初始化：$v = (0, 0)$ 和 $c = 0$，\n   - 学习率：$\\alpha = 0.05$，\n   - 迭代次数：$T = 2000$。\n3. 对于评估（与训练损失的定义不同），计算以下两者：\n   - 不带 $\\frac{1}{2}$ 因子的均方误差，\n     $$\\mathrm{MSE}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\left(f_{\\theta}(x^{(n)}) - y^{(n)}\\right)^2,$$\n   - 单调性违规度量，\n     $$\\mathrm{Viol}(\\theta) = \\sum_{i=1}^{2} \\max(0, -v_i).$$\n4. 分析权衡关系：通过增加 $\\lambda$，优化过程会更侧重于单调性，并可能牺牲准确性。通过使用以下惩罚权重测试套件进行训练，以数值方式确认此效应：\n   - $\\lambda = 0$（无单调性压力），\n   - $\\lambda = 0.5$（中等），\n   - $\\lambda = 20$（强）。\n5. 程序输出格式：你的程序应生成单行输出，其中包含一个逗号分隔的双元素列表，格式如下：\n   - 输出必须是\n     $$\\big[\\,[\\mathrm{MSE}_{\\lambda_1}, \\mathrm{Viol}_{\\lambda_1}], [\\mathrm{MSE}_{\\lambda_2}, \\mathrm{Viol}_{\\lambda_2}], [\\mathrm{MSE}_{\\lambda_3}, \\mathrm{Viol}_{\\lambda_3}] \\,\\big],$$\n     其中 $\\lambda$ 值按 $\\lambda \\in \\{0, 0.5, 20\\}$ 的顺序排列。\n   - $\\mathrm{MSE}_{\\lambda}$ 和 $\\mathrm{Viol}_{\\lambda}$ 都必须是实数。\n\n约束与注意事项：\n- 仅使用所提供的数据集构建方法和超参数。\n- 学习率 $\\alpha$、迭代次数 $T$ 和初始化值是固定的，不得更改。\n- 本问题不涉及角度和物理单位；无需进行单位转换。\n- 确保你的代码是自包含和确定性的，并且仅按指定的确切格式打印所需的单行输出，不含任何额外文本。", "solution": "本任务旨在实现并分析一个正则化的线性回归模型。其正则化项用于惩罚非单调性。这需要推导组合目标函数的次梯度，并实现一个批量次梯度下降算法。\n\n### 1. 问题表述与目标函数\n\n模型是一个线性函数 $f_{\\theta}(x) = v^\\top x + c$，其参数为 $\\theta = (v, c)$，其中 $v = (v_1, v_2)^\\top \\in \\mathbb{R}^2$ 且 $c \\in \\mathbb{R}$。目标是最小化组合目标函数 $J_{\\lambda}(\\theta)$，该函数是均方误差损失和单调性惩罚项之和：\n$$\nJ_{\\lambda}(\\theta) = L_{\\mathrm{mse}}(\\theta) + \\lambda \\, L_{\\mathrm{mono}}(\\theta)\n$$\n这里，$\\lambda \\ge 0$ 是一个控制惩罚强度的超参数。\n\n两个组成部分是：\n1.  **均方误差 (MSE) 损失**：经验风险定义为 $L_{\\mathrm{mse}}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{2}\\left(f_{\\theta}(x^{(n)}) - y^{(n)}\\right)^2$。\n2.  **单调性惩罚**：惩罚项定义为 $L_{\\mathrm{mono}}(\\theta) = \\sum_{i=1}^{2} \\max\\left(0, -\\frac{\\partial f_{\\theta}}{\\partial x_i}\\right)$。\n\n对于给定的线性模型，其相对于输入的偏导数是常数，且等于权重向量的分量，即 $\\frac{\\partial f_{\\theta}}{\\partial x_i} = v_i$。因此，惩罚项可以简化为关于 $v$ 的函数：\n$$\nL_{\\mathrm{mono}}(\\theta) = \\max(0, -v_1) + \\max(0, -v_2)\n$$\n\n### 2. 次梯度的推导\n\n为了使用批量梯度下降最小化 $J_{\\lambda}(\\theta)$，我们必须计算它关于参数 $v$ 和 $c$ 的（次）梯度。目标函数 $J_{\\lambda}(\\theta)$ 是凸函数，因为它是一个凸二次函数 ($L_{\\mathrm{mse}}$) 与另一个凸函数 ($L_{\\mathrm{mono}}$，它是一系列合页损失函数之和) 的和。然而，$L_{\\mathrm{mono}}$ 并非处处可微，特别是在 $v_1=0$ 或 $v_2=0$ 时。因此，我们必须使用次梯度的概念。\n\n多个凸函数之和的次微分是它们各自次微分的和。因此，$J_{\\lambda}(\\theta)$ 的次梯度集中的一个元素（记作 $g_\\theta \\in \\partial J_{\\lambda}(\\theta)$）可以通过将 MSE 项的梯度与惩罚项的次梯度相加得到：\n$$\ng_\\theta = \\nabla_{\\theta} L_{\\mathrm{mse}}(\\theta) + \\lambda \\, g_{\\mathrm{mono}}(\\theta)\n$$\n其中 $g_{\\mathrm{mono}}(\\theta) \\in \\partial L_{\\mathrm{mono}}(\\theta)$。我们将分别推导关于 $v$ 和 $c$ 的分量。\n\n#### 2.1. MSE 项的梯度\n\n损失项 $L_{\\mathrm{mse}}(\\theta)$ 是关于 $\\theta$ 的可微函数。我们计算其偏导数。\n关于向量 $v$ 的梯度是：\n$$\n\\nabla_v L_{\\mathrm{mse}}(\\theta) = \\frac{\\partial}{\\partial v} \\left( \\frac{1}{2N} \\sum_{n=1}^{N} (v^\\top x^{(n)} + c - y^{(n)})^2 \\right)\n$$\n使用链式法则：\n$$\n\\nabla_v L_{\\mathrm{mse}}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)}) x^{(n)}\n$$\n关于标量偏置 $c$ 的梯度是：\n$$\n\\frac{\\partial L_{\\mathrm{mse}}(\\theta)}{\\partial c} = \\frac{\\partial}{\\partial c} \\left( \\frac{1}{2N} \\sum_{n=1}^{N} (v^\\top x^{(n)} + c - y^{(n)})^2 \\right)\n$$\n使用链式法则：\n$$\n\\frac{\\partial L_{\\mathrm{mse}}(\\theta)}{\\partial c} = \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)})\n$$\n\n#### 2.2. 单调性惩罚的次梯度\n\n惩罚项 $L_{\\mathrm{mono}}(\\theta) = \\max(0, -v_1) + \\max(0, -v_2)$ 仅依赖于 $v$。因此，其关于 $c$ 的次梯度为零：\n$$\n\\partial_c L_{\\mathrm{mono}}(\\theta) = \\{0\\}\n$$\n为了找到关于 $v$ 的次梯度，我们分析函数 $h(z) = \\max(0, -z)$。其次微分 $\\partial h(z)$ 由下式给出：\n$$\n\\partial h(z) =\n\\begin{cases}\n    \\{-1\\}         \\text{if } z  0 \\\\\n    [-1, 0]        \\text{if } z = 0 \\\\\n    \\{0\\}          \\text{if } z > 0\n\\end{cases}\n$$\n对于次梯度下降，我们需要为每个分量从这个集合中选择一个元素。一个常见且有效的选择是：\n$$\ng_h(z) =\n\\begin{cases}\n    -1         \\text{if } z  0 \\\\\n    0          \\text{if } z \\ge 0\n\\end{cases}\n$$\n惩罚项在 $v_1$ 和 $v_2$ 上是可分的。因此，$L_{\\mathrm{mono}}(\\theta)$ 关于 $v$ 的一个次梯度是一个向量 $g_v = (g_{v_1}, g_{v_2})^\\top$，其中每个分量都从对应项的次微分中选取，即 $g_{v_i} \\in \\partial_{v_i} \\max(0, -v_i)$。使用我们选择的 $g_h$ 形式，我们得到一个特定的次梯度向量：\n$$\ng_{v} = \\begin{pmatrix} g_h(v_1) \\\\ g_h(v_2) \\end{pmatrix}, \\quad \\text{where } g_h(v_i) = \\begin{cases} -1  \\text{if } v_i  0 \\\\ 0  \\text{if } v_i \\ge 0 \\end{cases}\n$$\n\n#### 2.3. 组合次梯度\n\n结合这些结果，可以构建完整目标 $J_{\\lambda}(\\theta)$ 的一个有效次梯度。次微分 $\\partial_v J_{\\lambda}(\\theta)$ 的一个元素 $g_v$ 是：\n$$\ng_v = \\nabla_v L_{\\mathrm{mse}}(\\theta) + \\lambda \\, g_v^{\\mathrm{mono}}\n= \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)}) x^{(n)} + \\lambda \\begin{pmatrix}\ng_h(v_1) \\\\\ng_h(v_2)\n\\end{pmatrix}\n$$\n关于 $c$ 的梯度是：\n$$\n\\frac{\\partial J_{\\lambda}(\\theta)}{\\partial c} = \\frac{\\partial L_{\\mathrm{mse}}(\\theta)}{\\partial c} + \\lambda \\cdot 0\n= \\frac{1}{N} \\sum_{n=1}^{N} (f_{\\theta}(x^{(n)}) - y^{(n)})\n$$\n\n### 3. 批量次梯度下降算法\n\n批量次梯度下降算法通过迭代更新参数 $\\theta = (v, c)$ 来最小化 $J_{\\lambda}(\\theta)$。从一个初始猜测 $\\theta^{(0)}$ 开始，每次迭代 $k$ 使用学习率 $\\alpha$ 执行以下更新：\n$$\nv^{(k+1)} = v^{(k)} - \\alpha \\, g_v^{(k)}\n$$\n$$\nc^{(k+1)} = c^{(k)} - \\alpha \\, \\frac{\\partial J_{\\lambda}(\\theta^{(k)})}{\\partial c}\n$$\n其中 $g_v^{(k)}$ 是在当前参数 $\\theta^{(k)}=(v^{(k)}, c^{(k)})$ 处计算的关于 $v$ 的次梯度。这些更新会执行固定的迭代次数。该实现将使用推导出的次梯度、指定的数据集以及提供的超参数。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularized linear regression problem using batch subgradient descent.\n    \"\"\"\n\n    # --- 1. Define hyperparameters and problem setup ---\n    alpha = 0.05\n    T = 2000\n    lambda_test_suite = [0.0, 0.5, 20.0]\n    results = []\n\n    # --- 2. Generate the deterministic grid dataset ---\n    grid_coords = np.arange(0, 1.01, 0.1)\n    x1, x2 = np.meshgrid(grid_coords, grid_coords, indexing='ij')\n    X = np.vstack([x1.ravel(), x2.ravel()]).T\n    y = 2 * X[:, 0] - 1.5 * X[:, 1]\n    N = X.shape[0]\n\n    # --- 3. Run training and evaluation for each lambda ---\n    for lam in lambda_test_suite:\n        # Initialization\n        v = np.zeros(2)\n        c = 0.0\n\n        # Batch subgradient descent\n        for _ in range(T):\n            # Model predictions and errors\n            y_pred = X @ v + c\n            errors = y_pred - y\n\n            # Gradient of MSE term\n            grad_v_mse = (1.0 / N) * X.T @ errors\n            grad_c_mse = np.mean(errors)\n\n            # Subgradient of monotonicity penalty term\n            subgrad_v_mono = np.array([-1.0 if vi  0 else 0.0 for vi in v])\n\n            # Combined subgradient\n            grad_v = grad_v_mse + lam * subgrad_v_mono\n            grad_c = grad_c_mse  # Monotonicity penalty is not a function of c\n\n            # Update parameters\n            v -= alpha * grad_v\n            c -= alpha * grad_c\n\n        # --- 4. Evaluate the trained model ---\n        # Recalculate errors with final parameters\n        final_y_pred = X @ v + c\n        final_errors = final_y_pred - y\n        \n        # Mean Squared Error (without 1/2 factor)\n        mse = np.mean(final_errors**2)\n        \n        # Monotonicity Violation\n        viol = np.sum(np.maximum(0, -v))\n\n        results.append([mse, viol])\n\n    # --- 5. Format and print the final output ---\n    formatted_pairs = [f\"[{mse},{viol}]\" for mse, viol in results]\n    output_string = f\"[{','.join(formatted_pairs)}]\"\n    print(output_string)\n\nsolve()\n```", "id": "3145476"}]}