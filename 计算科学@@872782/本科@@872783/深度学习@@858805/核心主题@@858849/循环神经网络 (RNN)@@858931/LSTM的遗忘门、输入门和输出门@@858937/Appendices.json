{"hands_on_practices": [{"introduction": "要真正掌握 LSTM 的工作原理，我们必须深入其核心的数学机制。这个练习将引导你通过一个“纸笔”推导，在简化的假设下，将 LSTM 的状态更新方程展开为一个清晰的闭式解。通过这个过程，你将揭示 LSTM 的记忆行为如何与一种经典的时间序列分析工具——指数加权移动平均——紧密相连，从而对“长期”和“短期”记忆有一个更具体、更数学化的理解。[@problem_id:3188449]", "problem": "考虑一个长短期记忆（LSTM）单元的单个标量单位。根据定义，在时间步 $t$ 的细胞状态更新由 $c_t = f_t c_{t-1} + i_t \\tilde{c}_t$ 给出，其中 $f_t$ 是遗忘门的值，$i_t$ 是输入门的值，$\\tilde{c}_t$ 是候选细胞状态，每一个都由相应的门控和候选机制产生（例如，门使用 logistic sigmoid 函数，候选状态使用双曲正切函数）。假设门是时不变的，即 $f_t \\equiv f$ 和 $i_t \\equiv i$，其中 $0  f  1$ 和 $0  i \\leq 1$，并且初始细胞状态 $c_0$ 是给定的。候选细胞状态序列 $\\{\\tilde{c}_k\\}_{k=1}^t$ 是任意的，但有界且已知。\n\n从上述核心更新定义出发，并且不引入任何额外的快捷公式，推导出一个关于 $f$、$i$、$c_0$ 以及序列 $\\tilde{c}_1, \\tilde{c}_2, \\dots, \\tilde{c}_t$ 的 $c_t$ 的闭式解析表达式。然后，将所得表达式解释为应用于候选序列的一种指数平滑形式，确定在时间 $t$ 赋给每个 $\\tilde{c}_k$ 的权重，并解释初始条件 $c_0$ 在此平滑视角下的作用。\n\n你的最终答案必须是你推导出的 $c_t$ 的单一闭式解析表达式。不需要数值近似。", "solution": "该问题是有效的，因为它在科学上基于循环神经网络的原理，问题提法良好，具有足够的信息以获得唯一解，并且以客观、正式的语言表述。我们可以进行推导。\n\n起点是 LSTM 细胞状态 $c_t$ 在时间步 $t$ 的核心更新定义。在给定的时不变门的假设下，递推关系为：\n$$c_t = f c_{t-1} + i \\tilde{c}_t$$\n其中 $f$ 是恒定的遗忘门值，$i$ 是恒定的输入门值，$c_{t-1}$ 是前一时间步的细胞状态，$\\tilde{c}_t$ 是当前时间步的候选细胞状态。初始细胞状态为 $c_0$。\n\n我们的目标是通过展开此递推关系来推导 $c_t$ 的闭式解析表达式。让我们展开前几个时间步的表达式以识别模式。\n\n对于 $t=1$:\n$$c_1 = f c_0 + i \\tilde{c}_1$$\n\n对于 $t=2$:\n$$c_2 = f c_1 + i \\tilde{c}_2$$\n代入 $c_1$ 的表达式：\n$$c_2 = f(f c_0 + i \\tilde{c}_1) + i \\tilde{c}_2 = f^2 c_0 + f i \\tilde{c}_1 + i \\tilde{c}_2$$\n\n对于 $t=3$:\n$$c_3 = f c_2 + i \\tilde{c}_3$$\n代入 $c_2$ 的表达式：\n$$c_3 = f(f^2 c_0 + f i \\tilde{c}_1 + i \\tilde{c}_2) + i \\tilde{c}_3 = f^3 c_0 + f^2 i \\tilde{c}_1 + f i \\tilde{c}_2 + i \\tilde{c}_3$$\n\n从这些展开式中，一个通用模式浮现出来。$c_t$ 的表达式似乎由两部分组成：一个涉及初始状态 $c_0$ 的项和一个关于候选状态序列 $\\{\\tilde{c}_k\\}$ 的求和。我们假设以下闭式表达式：\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$$\n\n为了严格地建立这个公式，我们对时间步 $t$ 使用数学归纳法进行证明。\n\n**基础情形：** 对于 $t=1$，假设的公式得出：\n$$c_1 = f^1 c_0 + i \\sum_{k=1}^{1} f^{1-k} \\tilde{c}_k = f c_0 + i (f^{1-1} \\tilde{c}_1) = f c_0 + i f^0 \\tilde{c}_1 = f c_0 + i \\tilde{c}_1$$\n这个结果与直接从 $t=1$ 的递推关系推导出的表达式相匹配。基础情形成立。\n\n**归纳步骤：** 假设该公式对于任意时间步 $t-1$（其中 $t > 1$）成立。这是我们的归纳假设：\n$$c_{t-1} = f^{t-1} c_0 + i \\sum_{k=1}^{t-1} f^{(t-1)-k} \\tilde{c}_k$$\n我们必须证明该公式对于时间步 $t$ 也成立。我们从 $c_t$ 的递推关系开始：\n$$c_t = f c_{t-1} + i \\tilde{c}_t$$\n代入 $c_{t-1}$ 的归纳假设：\n$$c_t = f \\left( f^{t-1} c_0 + i \\sum_{k=1}^{t-1} f^{t-1-k} \\tilde{c}_k \\right) + i \\tilde{c}_t$$\n分配因子 $f$：\n$$c_t = f \\cdot f^{t-1} c_0 + f \\cdot i \\sum_{k=1}^{t-1} f^{t-1-k} \\tilde{c}_k + i \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t-1} f^{1 + (t-1-k)} \\tilde{c}_k + i \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t-1} f^{t-k} \\tilde{c}_k + i \\tilde{c}_t$$\n最后一项 $i\\tilde{c}_t$ 可以写成 $i f^0 \\tilde{c}_t = i f^{t-t} \\tilde{c}_t$。这使我们能将其合并到求和中，将求和上限从 $t-1$ 扩展到 $t$：\n$$c_t = f^t c_0 + \\left( i \\sum_{k=1}^{t-1} f^{t-k} \\tilde{c}_k \\right) + i f^{t-t} \\tilde{c}_t$$\n$$c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$$\n这正是假设的对于时间步 $t$ 的公式。因此，根据数学归纳法原理，该闭式表达式对于所有 $t \\ge 1$ 均有效。\n\n**解释为指数平滑：**\n推导出的表达式 $c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$ 可以被解释为一种指数平滑形式。\n\n$c_t$ 的表达式是两个分量的和：\n1. 初始状态贡献：$f^t c_0$。\n2. 候选状态的加权和：$i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k$。\n\n第一项 $f^t c_0$ 代表初始细胞状态 $c_0$ 的持续影响。在约束 $0  f  1$ 的条件下，因子 $f^t$ 随着 $t \\to \\infty$ 指数衰减至零。这展示了“遗忘”机制：对初始状态的记忆随时间逐渐消失。\n\n第二项是从 $\\tilde{c}_1$到 $\\tilde{c}_t$ 的候选状态序列的加权和。赋给每个候选状态 $\\tilde{c}_k$（其中 $1 \\le k \\le t$）的权重是 $w_k = i f^{t-k}$。让我们检查这些权重：\n- 对于最近的候选状态 $\\tilde{c}_t$，权重为 $w_t = i f^{t-t} = i$。\n- 对于前一个候选状态 $\\tilde{c}_{t-1}$，权重为 $w_{t-1} = i f^{t-(t-1)} = i f$。\n- 对于过去的候选状态 $\\tilde{c}_{t-j}$，权重为 $w_{t-j} = i f^{j}$。\n\n由于 $0  f  1$，当我们追溯到更远的过去时（即，随着 $k$ 减小，$t-k$ 增大，以及 $f^{t-k}$ 减小），权重 $w_k$ 会几何级数地减小。这种不相等的加权方式，即最近的输入比旧的输入获得指数级更高的权重，是指数加权移动平均（也称为指数平滑）的决定性特征。遗忘门值 $f$ 充当衰减率或平滑因子。\n\n总而言之，初始条件 $c_0$ 的作用是为此平滑过程提供一个起始值。它的影响由 $f^t$ 加权并呈指数衰减。因此，细胞状态 $c_t$ 是其初始状态的衰退记忆与截至时间 $t$遇到的所有候选输入的指数平滑平均值的组合。在时间 $t$ 赋给特定候选状态 $\\tilde{c}_k$ 的权重恰好是 $i f^{t-k}$。输入门值 $i$ 充当所有新信息影响的缩放因子。", "answer": "$$\\boxed{c_t = f^t c_0 + i \\sum_{k=1}^{t} f^{t-k} \\tilde{c}_k}$$", "id": "3188449"}, {"introduction": "掌握了分析和衡量 LSTM 记忆的技能后，让我们进入一个更高级的主题：主动工程化。这个练习挑战你像一名机器学习工程师一样思考，不再仅仅被动地观察门控行为。你的任务是构思一个学习环境，通过设计一个合成任务和一个定制的损失函数，来引导遗忘门采纳一种理想的、分段式的记忆策略——即在信息段内保持记忆，在段落边界重置记忆。[@problem_id:3188539]", "problem": "考虑一个单细胞长短期记忆 (LSTM) 单元，其中长短期记忆 (LSTM) 指的是一种带有乘法门的门控循环结构，这些门可以调节信息流。细胞状态是一个标量 $c_t \\in \\mathbb{R}$，输入是一个标量 $x_t \\in \\mathbb{R}$，遗忘门、输入门和输出门是标量 $f_t, i_t, o_t \\in (0,1)$，它们由应用于 $(x_t, h_{t-1})$ 的仿射函数的 logistic sigmoid 非线性函数产生。细胞更新遵循核心定义，即下一个状态结合了前一个状态的门控传递和一个门控的候选更新。假设使用标准自回归训练，具有可微损失函数和随时间反向传播。\n\n您的任务是构建一个合成序列和一个训练目标，使得最优遗忘门 $f_t$ 遵循分段切换模式 $f_t \\in \\{0,1\\}$：在已知的片段边界处 $f_t \\approx 0$ 以清除先前的状态，在片段内部 $f_t \\approx 1$ 以向前传递记忆。设有一个已知的边界指示符 $b_t \\in \\{0,1\\}$，其中 $b_t = 1$ 表示在时间 $t$ 处一个新片段的开始，否则 $b_t = 0$。预测目标是一个可观测量 $y_t$，其设计旨在实现低预测误差需要信息在片段内传递并在边界处重置。\n\n根据门控状态累积和基于梯度的优化的第一性原理，选择正确指定以下内容的选项：\n- 一个科学上合理的合成序列和目标 $y_t$，对于该序列和目标，最优的 $f_t$ 表现出所述的切换行为，\n- 一个可微的惩罚项，它仅使用可微项来鼓励 $f_t$ 逼近期望的二元模式，并且不破坏学习其他参数所需的梯度，\n- 以及一个关于训练动态的真实讨论，包括关于遗忘门预激活值的梯度形状、logistic sigmoid 的饱和效应，以及如何随时间平衡门惩罚项与预测损失。\n\n选项：\n\nA. 通过 $b_t$ 定义片段，并选择 $x_t$ 为独立的、零均值的有界随机变量。令目标为当前片段内的累加和，$y_t = \\sum_{k=\\tau(t)}^{t} x_k$，其中 $\\tau(t)$ 是最近一次 $b_{\\tau(t)} = 1$ 的时间索引。设定一个门目标 $g_t = 1 - b_t$，并对遗忘门添加一个伯努利交叉熵正则化器，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_{t} \\big[ g_t \\cdot (-\\log f_t) + (1-g_t) \\cdot (-\\log(1 - f_t)) \\big],\n$$\n其中 $\\lambda > 0$，该正则化器是可微的，并且在 $f_t \\in \\{0,1\\}$ 与 $g_t$ 对齐时最小化。在 logistic sigmoid 函数 $f_t = \\sigma(a_t)$ 下，$\\mathcal{L}_{\\text{gate}}$ 对梯度的贡献是\n$$\n\\frac{\\partial \\mathcal{L}_{\\text{gate}}}{\\partial a_t} = \\lambda (f_t - g_t),\n$$\n这个梯度是稳定的，当 $f_t$ 错误时非零，并且仅在 $f_t$ 与 $g_t$ 匹配时才消失。该讨论强调，在片段内部，当 $f_t \\approx 1$ 时，LSTM 的细胞状态路径支持长程梯度，而在边界处，$f_t \\approx 0$ 有意地截断状态传递。它建议对 $\\lambda$ 进行退火，并可选择性地使用温度缩放的 sigmoid 函数 $f_t = \\sigma(a_t / \\tau)$，其中 $\\tau$ 在训练过程中减小，以避免过早的硬饱和，并允许与主预测损失协同。\n\nB. 令 $x_t$ 在 $x_t = 1$ 和 $x_t = -1$ 之间确定性地交替，并将 $y_t$ 定义为近期 $x_t=1$ 值计数的奇偶性。使用一个简单的平方惩罚项，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_t \\big( f_t - (1 - b_t) \\big)^2,\n$$\n该惩罚项是可微的。声称即使 $f_t$ 在接近 $0$ 或 $1$ 时饱和，仅此一项也能防止梯度消失，因为平方误差总是提供非零梯度。结论是不需要退火或温度控制，并且在所有时间均匀地应用该惩罚项可以确保梯度通过门时不受阻碍。\n\nC. 在片段内选择 $x_t$ 为随机变量，并将 $y_t$ 定义为直到时间 $t$ 的整个序列的平均值。添加一个与 $b_t$ 无关的无监督双阱正则化器，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_t f_t (1 - f_t),\n$$\n以将 $f_t$ 推向 $\\{0,1\\}$，而没有关于何时遗忘的显式监督。论证认为通过预测损失的反向传播会自动将门切换与 $b_t$ 对齐，因为最小化预测误差足以使 $f_t$ 与边界同步。\n\nD. 使用分段常数 $x_t$，在片段内具有固定值。将 $y_t$ 设置为在片段开始时最后观察到的 $x_{\\tau(t)}$。添加一个带间隔的类 hinge 非平滑惩罚项，\n$$\n\\mathcal{L}_{\\text{gate}} = \\lambda \\sum_t \\max\\big( 0, 1 - \\alpha \\lvert f_t - (1 - b_t) \\rvert \\big),\n$$\n其中 $\\alpha > 0$。声称非平滑性是无害的，因为自动微分可以处理不可微点，并且该间隔将产生比平滑替代方案更强的梯度，从而消除 sigmoid 中的饱和问题并加速训练。\n\n选择最佳选项，并根据 LSTM 状态更新的基本定义、logistic sigmoid 门的性质以及基于梯度的训练动态来证明您的选择是正确的。", "solution": "选项A是最佳选择。\n**科学合理性分析:**\n1.  **任务设计 ($y_t$):** 将目标设为片段内的累加和 ($y_t = \\sum_{k=\\tau(t)}^{t} x_k$) 是一个绝佳的设计。为了正确计算这个和，LSTM的细胞状态 $c_t$ 必须扮演累加器的角色。这自然地要求：(a) 在片段内部（$b_t=0$），必须保留前一时间步的累加和 $c_{t-1}$，这意味着遗忘门 $f_t$ 需要接近1；(b) 在片段边界（$b_t=1$），累加器必须重置为0（或当前值 $x_t$），这意味着必须丢弃旧的累加和，即 $f_t$ 需要接近0。因此，该任务的预测损失本身就强烈地激励了所期望的门控行为。\n2.  **惩罚项 ($\\mathcal{L}_{\\text{gate}}$):** 伯努利交叉熵（BCE）损失是监督二元分类问题的标准和理论上最稳健的选择。当目标 $g_t$ 是0或1时，它为模型的输出概率 $f_t$ 提供了一个对数尺度的惩罚，这种惩罚在模型非常自信但错误时会变得极大，从而提供强烈的修正信号。\n3.  **梯度动态:** 选项A的分析是完全正确的。对于 $f_t = \\sigma(a_t)$ 和BCE损失，梯度对预激活值 $a_t$ 的贡献是 $\\frac{\\partial \\mathcal{L}_{\\text{gate}}}{\\partial a_t} = \\lambda (f_t - g_t)$。这个梯度形式非常优雅，因为它不受sigmoid函数自身导数 $\\sigma'(a_t) = f_t(1-f_t)$ 的影响。这意味着即使当门 $f_t$ 饱和（接近0或1）时，只要它与目标 $g_t$ 不匹配，仍然会有一个恒定大小的梯度（$\\lambda$ 或 $-\\lambda$）来推动预激活值 $a_t$ 朝着正确的方向移动，从而避免了其他损失函数（如选项B中的均方误差）在饱和区的梯度消失问题。\n4.  **训练策略:** 承认并提出解决训练动态复杂性的方案（如退火$\\lambda$和温度缩放）表明了对实际训练挑战的深刻理解。早期训练中，一个大的$\\lambda$可能会主导损失，阻止模型学习预测任务。后期，一个小的$\\lambda$可能不足以强制执行门控行为。因此，动态调整$\\lambda$或使用温度$\\tau$来控制饱和的“硬度”是高级且实用的策略。\n\n**其他选项的缺陷:**\n*   **选项B:** 其核心论点“平方误差总是提供非零梯度”是错误的。如上所述，$\\frac{\\partial (f_t - g_t)^2}{\\partial a_t} = 2(f_t - g_t) \\cdot f_t(1-f_t)$。当 $f_t$ 饱和时，$f_t(1-f_t) \\to 0$，导致梯度消失。\n*   **选项C:** 任务（整个序列的平均值）并不强制要求在片段边界重置记忆。双阱正则化器 $f_t(1-f_t)$ 只鼓励门取0或1，但没有提供何时取哪个值的监督信号。依赖预测损失“自动”对齐门控过于理想化，很可能失败。\n*   **选项D:** Hinge-like损失引入了非平滑性，虽然在实践中可以使用次梯度，但通常不如平滑损失稳定。更重要的是，一旦误差在间隔内（$ \\lvert f_t - (1 - b_t) \\rvert  1/\\alpha$），梯度就为零，这可能会导致训练过早停滞，即使门控不完美。声称它能“消除”饱和问题是不准确的。", "answer": "$$\\boxed{A}$$", "id": "3188539"}]}