{"hands_on_practices": [{"introduction": "上下文向量在编码器和解码器之间充当信息瓶颈。但我们如何量化这个瓶颈的容量呢？这个练习应用信息论的基石——率失真理论，来估算完成特定任务所需的最小上下文向量维度。通过这个练习，你将为理解信息压缩（上下文向量大小）和重构质量之间的权衡奠定坚实的理论基础。[@problem_id:3184063]", "problem": "考虑一个执行复制任务的序列到序列编码器-解码器，它将输入序列 $\\{x_t\\}_{t=1}^{T}$ 映射到输出序列 $\\{y_t\\}_{t=1}^{T}$，目标是 $y_{1:T} \\approx x_{1:T}$。每个输入 $x_t \\in \\mathbb{R}^{m}$ 都是从协方差为 $\\sigma^{2} I_{m}$ 的零均值多元高斯分布中独立抽取的，即 $x_t \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。编码器将整个输入序列映射到一个定长的上下文向量 $c \\in \\mathbb{R}^{d_c}$，然后该向量被送入解码器以产生重构序列 $\\{\\hat{x}_t\\}_{t=1}^{T}$。假设如下：\n- 编码器-解码器允许对所有 $T m$ 个实值分量使用最优块编码。\n- 上下文向量 $c$ 的每个标量分量用 $b$ 比特进行存储或传输，因此可用于表示该序列的总比特数为 $d_c b$。\n- 性能度量是整个序列上每个分量的平均均方误差，由下式给出\n$$\nD \\triangleq \\frac{1}{T m} \\sum_{t=1}^{T} \\|x_t - \\hat{x}_t\\|_{2}^{2}.\n$$\n- 目标是在大块编码机制下以高概率实现 $D \\le \\epsilon$，其中 $\\epsilon \\in (0, \\sigma^{2})$。\n\n仅使用信息论的基本定义和关于高斯信源在平方误差失真下的率失真函数的成熟结论，推导出一个表达式，该表达式描述了以高概率达到目标失真所必需的最小整数上下文维度 $d_c$ 作为 $T$、$m$、$\\sigma^{2}$、$\\epsilon$ 和 $b$ 的函数。请将您的最终答案表示为 $d_c$（最小整数值）的单一闭式解析表达式，不带单位。无需进行数值计算。", "solution": "该问题要求确定序列到序列编码器-解码器模型为满足给定的失真约束所需的最小整数上下文维度 $d_c$。这个问题可以形式上映射为信息论中的一个率失真问题。编码器作为信源编码器，将输入序列 $\\{x_t\\}_{t=1}^{T}$ 压缩成一个表示，即上下文向量 $c$。然后，上下文向量被传输或存储，解码器再重构该序列。此过程的保真度由均方误差来衡量。\n\n首先，我们来描述信源的特性。输入是一个由 $T$ 个向量组成的序列，$x_t \\in \\mathbb{R}^m$。每个向量 $x_t$ 都是从一个具有对角协方差矩阵的零均值多元高斯分布中独立抽取的，即 $x_t \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$。这意味着单个向量 $x_t$ 的 $m$ 个分量中的每一个都是一个独立同分布 (i.i.d.) 的高斯随机变量，其均值为零，方差为 $\\sigma^2$。由于向量 $x_t$ 在不同的时间步 $t$ 也是独立的，整个序列可以看作是 $N = T \\times m$ 个独立同分布的标量高斯随机变量的集合，每个变量都从 $\\mathcal{N}(0, \\sigma^2)$ 中抽取。\n\n接下来，我们确定可用的通信速率。上下文向量 $c \\in \\mathbb{R}^{d_c}$ 用于存储整个输入序列的压缩表示。$c$ 的 $d_c$ 个标量分量中的每一个都使用 $b$ 比特进行量化。因此，可用于表示整个序列 $N = Tm$ 个信源符号的总比特数是 $B = d_c b$。此编码方案的速率定义为每个信源符号的比特数，由下式给出：\n$$\nR = \\frac{\\text{Total bits}}{\\text{Number of source symbols}} = \\frac{d_c b}{T m}\n$$\n\n性能由每个分量的平均均方误差 $D$ 来衡量。问题要求该失真小于或等于目标值 $\\epsilon$，即 $D \\le \\epsilon$。\n$$\nD = \\frac{1}{T m} \\sum_{t=1}^{T} \\|x_t - \\hat{x}_t\\|_{2}^{2} \\le \\epsilon\n$$\n\n问题假设在“大块编码机制”下采用“最优块编码”以“高概率”达到目标失真。这正是 Shannon 的信源编码定理（也称为率失真定理）的精确设定。该定理指出，对于给定的信源分布和失真度量，当且仅当可用速率 $R$ 大于或等于率失真函数 $R(D)$ 时，才可能达到平均失真 $D$。也就是说，如果满足以下条件，则失真水平 $\\epsilon$ 是可达到的：\n$$\nR \\ge R(\\epsilon)\n$$\n其中 $R(\\epsilon)$ 是率失真函数在失真为 $\\epsilon$ 时的取值。\n\n对于一个每个符号方差为 $\\sigma^2$ 的无记忆（独立同分布）高斯信源，并且采用均方误差失真度量，其率失真函数是信息论中一个众所周知的结果。它以比特/符号为单位，由下式给出：\n$$\nR(D) = \n\\begin{cases} \n\\frac{1}{2} \\log_2\\left(\\frac{\\sigma^2}{D}\\right)  \\text{for } 0 \\le D \\le \\sigma^2 \\\\\n0  \\text{for } D  \\sigma^2 \n\\end{cases}\n$$\n问题规定目标失真 $\\epsilon$ 在范围 $\\epsilon \\in (0, \\sigma^2)$ 内，这对应于需要进行有损压缩的非平凡情况。因此，要达到至多为 $\\epsilon$ 的失真所需的最小速率为：\n$$\nR_{min} = R(\\epsilon) = \\frac{1}{2} \\log_2\\left(\\frac{\\sigma^2}{\\epsilon}\\right)\n$$\n\n为了满足成功重构的条件，可用速率 $R$ 必须至少等于这个最小所需速率 $R_{min}$：\n$$\n\\frac{d_c b}{T m} \\ge \\frac{1}{2} \\log_2\\left(\\frac{\\sigma^2}{\\epsilon}\\right)\n$$\n\n我们现在可以从此不等式中解出上下文维度 $d_c$：\n$$\nd_c \\ge \\frac{T m}{b} \\cdot \\frac{1}{2} \\log_2\\left(\\frac{\\sigma^2}{\\epsilon}\\right)\n$$\n$$\nd_c \\ge \\frac{T m}{2 b} \\log_2\\left(\\frac{\\sigma^2}{\\epsilon}\\right)\n$$\n问题要求满足此条件的 $d_c$ 的最小整数值。大于或等于一个给定实数的最小整数由上取整函数定义。因此，最小整数上下文维度为：\n$$\nd_c = \\left\\lceil \\frac{T m}{2 b} \\log_2\\left(\\frac{\\sigma^2}{\\epsilon}\\right) \\right\\rceil\n$$\n该表达式提供了信息瓶頸的必要大小，它由上下文向量的维度表示，并作为序列长度 $T$、输入维度 $m$、信源方差 $\\sigma^2$、目标失真 $\\epsilon$ 以及上下文表示的比特深度 $b$ 的函数。", "answer": "$$\\boxed{\\left\\lceil \\frac{T m}{2 b} \\log_2\\left(\\frac{\\sigma^2}{\\epsilon}\\right) \\right\\rceil}$$", "id": "3184063"}, {"introduction": "在建立了上下文向量的理论极限之后，我们现在来探究信息是如何从输入序列的不同部分实际汇集而来的。这个动手练习使用一个简化的模型和输入掩码技术，来直观地展示编码器的结构和类似注意力的机制如何决定模型是关注局部依赖还是长程依赖。通过分析掩码操作后输出的变化，你将对序列到序列系统中的信息流动有一个具体的理解。[@problem_id:3183979]", "problem": "给定一个用于分析上下文向量在捕获局部与长程依赖关系中作用的合成序列到序列（Seq2Seq）编码器-解码器模型。编码器是一个具有有限感受野的一维线性卷积，上下文向量是编码器隐藏状态的加权和，解码器是上下文向量到标量输出的线性投影。您必须从核心定义出发，推导屏蔽输入序列的部分如何影响标量输出，并将此效应与编码器的感受野联系起来。\n\n基本定义：\n- 一个长度为 $T$ 的输入序列 $x_{1:T}$ 由编码器处理，通过一个长度为 $L$ 的因果线性有限脉冲响应生成隐藏状态 $h_{t}$：\n$$\nh_{t} = \\sum_{k=0}^{L-1} r_{k}\\, x_{t-k}\n$$\n约定当 $t-k  1$ 时，$x_{t-k} = 0$。系数 $r_{k}$ 定义了编码器的感受野。\n- 上下文向量 $c$ 通过固定的非负权重（注意力权重）$\\alpha_{t}$ 聚合编码器状态：\n$$\nc = \\sum_{t=1}^{T} \\alpha_{t}\\, h_{t}.\n$$\n- 解码器通过 $y^{\\wedge} = d\\, c$ 生成一个标量输出，其中 $d$ 是一个固定的标量。\n\n仅使用这些定义，推导每个输入位置 $x_{j}$ 对 $y^{\\wedge}$ 的影响，并用此来计算当输入子集被屏蔽时 $y^{\\wedge}$ 的变化幅度。屏蔽索引 $j$ 意味着将 $x_{j}$ 替换为 $0$，同时保持所有其他输入不变。\n\n性能代理指标：\n- 将屏蔽操作的“下降”幅度定义为\n$$\n\\Delta = \\left| y^{\\wedge}_{\\text{unmasked}} - y^{\\wedge}_{\\text{masked}} \\right|,\n$$\n它量化了因移除被屏蔽输入而导致的输出变化。\n- 较大的 $\\Delta$ 值表明相应的被屏蔽输入对上下文向量 $c$ 的贡献更大，从而对 $y^{\\wedge}$ 的贡献也更大。\n\n您的任务是实现一个程序，为下面的每个测试用例计算两个量：\n- 局部屏蔽下降值（屏蔽最后 $M_{\\text{local}}$ 个位置，即索引 $j \\in \\{T - M_{\\text{local}} + 1, \\dots, T\\}$）。\n- 长程屏蔽下降值（屏蔽前 $M_{\\text{long}}$ 个位置，即索引 $j \\in \\{1, \\dots, M_{\\text{long}}\\}$）。\n\n测试套件：\n- 案例 1（常规顺利路径）：\n    - $T = 8$\n    - $x = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]$\n    - $L = 3$, $r = [1.0, 0.5, 0.25]$\n    - $\\alpha = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$\n    - $d = 1.0$\n    - $M_{\\text{local}} = 2$, $M_{\\text{long}} = 3$\n- 案例 2（通过注意力和更宽的感受野强调长程依赖）：\n    - $T = 8$\n    - $x = [0.5, 0.1, 0.4, 0.6, 0.3, 0.9, 0.2, 0.8]$\n    - $L = 5$, $r = [1.0, 0.8, 0.64, 0.512, 0.4096]$\n    - $\\alpha = [1.0, 0.8, 0.64, 0.512, 0.4096, 0.32768, 0.262144, 0.2097152]$\n    - $d = 1.0$\n    - $M_{\\text{local}} = 3$, $M_{\\text{long}} = 3$\n- 案例 3（边界情况：严格局部编码器，后期注意力强调）：\n    - $T = 6$\n    - $x = [0.9, 0.0, 0.1, 0.0, 0.2, 1.0]$\n    - $L = 1$, $r = [1.0]$\n    - $\\alpha = [1.0, 1.3, 1.69, 2.197, 2.8561, 3.71293]$\n    - $d = 1.0$\n    - $M_{\\text{local}} = 2$, $M_{\\text{long}} = 2$\n- 案例 4（边缘情况：仅早期输入非零，均匀注意力）：\n    - $T = 7$\n    - $x = [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]$\n    - $L = 4$, $r = [1.0, 0.7, 0.49, 0.343]$\n    - $\\alpha = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]$\n    - $d = 1.0$\n    - $M_{\\text{local}} = 2$, $M_{\\text{long}} = 3$\n\n实现要求：\n- 从定义中推导如何将 $y^{\\wedge}$ 直接表示为输入 $x_{j}$ 的加权和，其中每个权重取决于 $(r_{k})$、$(\\alpha_{t})$ 和 $d$。使用该推导为每种屏蔽方案计算 $y^{\\wedge}_{\\text{unmasked}}$ 和 $y^{\\wedge}_{\\text{masked}}$，然后计算如上定义的 $\\Delta$。\n- 您的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果表示为一个二元列表 $[\\Delta_{\\text{local}}, \\Delta_{\\text{long}}]$。例如：$[[\\dots,\\dots],[\\dots,\\dots],\\dots]$。\n- 所有输出必须是浮点数。此问题不涉及物理单位或角度单位。将所有数值结果直接表示为十进制浮点数。\n- 程序必须是自包含的，并且不得要求任何输入。它必须为上面指定的四个案例精确实现计算，并按所描述的精确格式打印单行聚合输出。", "solution": "该问题要求推导输入屏蔽对一个简化的序列到序列模型输出的影响。这可以通过首先将标量输出 $y^{\\wedge}$ 表示为输入序列元素 $x_j$ 的直接线性组合来实现。此组合的系数代表每个输入对最终输出的总影响。\n\n该模型由三个基本方程定义：\n1.  编码器的隐藏状态 $h_t$ 是通过输入序列 $x_{1:T}$ 与长度为 $L$ 的核 $r$ 进行线性卷积生成的：\n    $$h_{t} = \\sum_{k=0}^{L-1} r_{k}\\, x_{t-k}$$\n    此操作对从 $1$ 到 $T$ 的每个时间步 $t$ 执行。按照惯例，对于任何索引 $i  1$，输入值 $x_i$ 均为零。\n\n2.  上下文向量 $c$ 是所有隐藏状态的聚合，由非负的注意力权重 $\\alpha_t$ 加权：\n    $$c = \\sum_{t=1}^{T} \\alpha_{t}\\, h_{t}$$\n\n3.  解码器通过一个标量 $d$ 对上下文向量 $c$ 进行线性投影，生成最终的标量输出 $y^{\\wedge}$：\n    $$y^{\\wedge} = d\\, c$$\n\n为了确定每个输入元素 $x_j$ 对 $y^{\\wedge}$ 的影响，我们将 $h_t$ 和 $c$ 的表达式代入 $y^{\\wedge}$ 的方程中。\n\n首先，将 $c$ 的定义代入 $y^{\\wedge}$ 的方程中：\n$$y^{\\wedge} = d \\left( \\sum_{t=1}^{T} \\alpha_{t}\\, h_{t} \\right)$$\n接下来，代入 $h_t$ 的定义：\n$$y^{\\wedge} = d \\sum_{t=1}^{T} \\alpha_{t} \\left( \\sum_{k=0}^{L-1} r_{k}\\, x_{t-k} \\right)$$\n通过分配项 $\\alpha_t$ 和 $d$，我们得到一个关于索引 $t$ 和 $k$ 的双重求和：\n$$y^{\\wedge} = \\sum_{t=1}^{T} \\sum_{k=0}^{L-1} d\\, \\alpha_{t}\\, r_{k}\\, x_{t-k}$$\n这个表达式显示 $y^{\\wedge}$ 是所有输入贡献的总和，但它尚未按每个不同的输入 $x_j$ 进行分组。我们的目标是将其改写为 $y^{\\wedge} = \\sum_{j=1}^{T} w_j x_j$ 的形式，其中 $w_j$ 是输入 $x_j$ 的总权重。\n\n为了找到 $w_j$，我们改变求和变量。设输入索引为 $j = t-k$。这意味着 $t = j+k$。我们现在可以对从 $1$ 到 $T$ 的输入索引 $j$ 进行求和。对于一个固定的 $j$，只要在其各自的求和边界（$1 \\le t \\le T$ 和 $0 \\le k \\le L-1$）内的某个 $(t,k)$ 对满足 $t-k = j$，输入 $x_j$ 就会对总和产生贡献。\n\n权重 $w_j$ 是所有满足 $t-k=j$ 的系数 $d\\,\\alpha_t\\,r_k$ 的总和。代入 $t=j+k$，我们可以对核索引 $k$ 进行求和，并找到相应的时间步 $t$。索引的约束条件转换为 $1 \\le j+k \\le T$ 和 $0 \\le k \\le L-1$。因此，对于每个 $j \\in \\{1, \\dots, T\\}$，其权重 $w_j$ 是：\n$$w_j = d \\sum_{k=0}^{L-1} \\alpha_{j+k}\\, r_k$$\n其中求和只包括索引 $j+k$ 有效的项，即 $1 \\le j+k \\le T$。这个公式量化了输入 $x_j$ 对 $y^{\\wedge}$ 的总影响。这种影响是编码器感受野（通过 $r_k$）和注意力机制（通过 $\\alpha_{j+k}$）的组合。一个输入 $x_j$ 会影响多个后续的隐藏状态 $h_{j+k}$，而这些状态中的每一个都由相应的注意力 $\\alpha_{j+k}$ 加权。\n\n当输出表示为 $y^{\\wedge} = \\sum_{j=1}^{T} w_j x_j$ 时，屏蔽的效果就变得直接易算了。屏蔽一组输入 $S$ 意味着对所有 $j \\in S$ 设置 $x_j = 0$。原始输出是 $y^{\\wedge}_{\\text{unmasked}} = \\sum_{j=1}^{T} w_j x_j$。屏蔽后的新输出是 $y^{\\wedge}_{\\text{masked}} = \\sum_{j \\notin S} w_j x_j$。差值为：\n$$y^{\\wedge}_{\\text{unmasked}} - y^{\\wedge}_{\\text{masked}} = \\sum_{j \\in S} w_j x_j$$\n性能指标 $\\Delta$ 是这个差值的绝对大小：\n$$\\Delta = \\left| \\sum_{j \\in S} w_j x_j \\right|$$\n这为计算任何给定屏蔽集 $S$ 的“下降值”提供了一种直接的方法。\n\n每个测试用例的算法如下，为方便实现，使用从零开始的索引（即索引从 $0$ 到 $T-1$）：\n1.  给定参数 $T, x, L, r, \\alpha, d$。\n2.  计算长度为 $T$ 的权重向量 $w$。对于每个 $j \\in \\{0, \\dots, T-1\\}$：\n    $$w_j = d \\sum_{k=0}^{L-1} \\alpha_{j+k}\\, r_k, \\quad \\text{约束条件为 } j+k  T$$\n3.  对于局部屏蔽，索引集为 $S_{\\text{local}} = \\{T - M_{\\text{local}}, \\dots, T-1\\}$。计算下降值：\n    $$\\Delta_{\\text{local}} = \\left| \\sum_{j \\in S_{\\text{local}}} w_j x_j \\right|$$\n4.  对于长程屏蔽，索引集为 $S_{\\text{long}} = \\{0, \\dots, M_{\\text{long}}-1\\}$。计算下降值：\n    $$\\Delta_{\\text{long}} = \\left| \\sum_{j \\in S_{\\text{long}}} w_j x_j \\right|$$\n将此过程应用于每个测试用例以获得所需的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Seq2Seq analysis problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"T\": 8,\n            \"x\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n            \"L\": 3, \"r\": [1.0, 0.5, 0.25],\n            \"alpha\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            \"d\": 1.0,\n            \"M_local\": 2, \"M_long\": 3\n        },\n        {\n            \"T\": 8,\n            \"x\": [0.5, 0.1, 0.4, 0.6, 0.3, 0.9, 0.2, 0.8],\n            \"L\": 5, \"r\": [1.0, 0.8, 0.64, 0.512, 0.4096],\n            \"alpha\": [1.0, 0.8, 0.64, 0.512, 0.4096, 0.32768, 0.262144, 0.2097152],\n            \"d\": 1.0,\n            \"M_local\": 3, \"M_long\": 3\n        },\n        {\n            \"T\": 6,\n            \"x\": [0.9, 0.0, 0.1, 0.0, 0.2, 1.0],\n            \"L\": 1, \"r\": [1.0],\n            \"alpha\": [1.0, 1.3, 1.69, 2.197, 2.8561, 3.71293],\n            \"d\": 1.0,\n            \"M_local\": 2, \"M_long\": 2\n        },\n        {\n            \"T\": 7,\n            \"x\": [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n            \"L\": 4, \"r\": [1.0, 0.7, 0.49, 0.343],\n            \"alpha\": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            \"d\": 1.0,\n            \"M_local\": 2, \"M_long\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        T = case[\"T\"]\n        x = np.array(case[\"x\"])\n        L = case[\"L\"]\n        r = np.array(case[\"r\"])\n        alpha = np.array(case[\"alpha\"])\n        d = case[\"d\"]\n        M_local = case[\"M_local\"]\n        M_long = case[\"M_long\"]\n\n        # 1. Compute the weight vector w\n        w = np.zeros(T)\n        for j in range(T):\n            # Sum over the kernel r\n            for k in range(L):\n                t = j + k\n                if t  T:\n                    w[j] += alpha[t] * r[k]\n        w *= d\n\n        # 2. Calculate local-mask drop\n        local_mask_indices = range(T - M_local, T)\n        local_drop = np.sum(w[local_mask_indices] * x[local_mask_indices])\n        delta_local = np.abs(local_drop)\n\n        # 3. Calculate long-mask drop\n        long_mask_indices = range(M_long)\n        long_drop = np.sum(w[long_mask_indices] * x[long_mask_indices])\n        delta_long = np.abs(long_drop)\n\n        results.append([delta_local, delta_long])\n\n    # Final print statement in the exact required format.\n    formatted_results = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "3183979"}, {"introduction": "上下文向量可能有一个很大的名义维度 $d_c$，但模型是否有效地利用了所有这些维度？这个诊断性练习介绍了一种强大的技术来衡量上下文向量的“有效维度”。通过沿着其主要变化方向扰动上下文向量，并测量解码器的敏感度，你可以分析信息在该向量内部是如何组织的，并识别潜在的维度冗余或效率低下的问题。[@problem_id:3184008]", "problem": "考虑一个使用定长上下文向量的编码器-解码器模型。令上下文向量表示为 $\\boldsymbol{c} \\in \\mathbb{R}^{d_c}$，其名义维度为 $d_c$。假设解码器通过一个线性映射和随后的 Softmax 函数，从 $\\boldsymbol{c}$ 生成一个关于 $k$ 个类别的分类分布：$$\\boldsymbol{z} = W \\boldsymbol{c} + \\boldsymbol{b}, \\quad \\boldsymbol{y} = \\operatorname{softmax}(\\boldsymbol{z}),$$ 其中 $W \\in \\mathbb{R}^{k \\times d_c}$ 且 $\\boldsymbol{b} \\in \\mathbb{R}^k$。设上下文向量被建模为一个随机变量，其均值为 $\\boldsymbol{\\mu}$，协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{d_c \\times d_c}$。\n\n你需要实现一个诊断程序，以量化解码器对 $\\boldsymbol{c}$ 沿 $\\Sigma$ 主方向的扰动的敏感度，并将此敏感度与 $\\boldsymbol{c}$ 的有效维度与名义维度 $d_c$ 进行关联。该诊断程序应按以下方式运行：\n\n1. 计算协方差矩阵的特征分解：$$\\Sigma = V \\Lambda V^\\top,$$ 其中 $V$ 包含标准正交的特征向量 $\\boldsymbol{v}_i$，$ \\Lambda = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_{d_c})$ 包含按非递增顺序排列的相应特征值，$\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{d_c} \\ge 0$。\n\n2. 对每个主方向 $i \\in \\{1, \\ldots, d_c\\}$，构造均值上下文向量的缩放扰动：$$\\Delta_i^{(+)} = +\\varepsilon \\sqrt{\\lambda_i} \\, \\boldsymbol{v}_i, \\quad \\Delta_i^{(-)} = -\\varepsilon \\sqrt{\\lambda_i} \\, \\boldsymbol{v}_i,$$ 其中给定一个小的标量 $\\varepsilon > 0$。\n\n3. 计算基准解码器输出 $$\\boldsymbol{y}_0 = \\operatorname{softmax}\\!\\left(W \\boldsymbol{\\mu} + \\boldsymbol{b}\\right).$$ 然后计算扰动后的输出 $$\\boldsymbol{y}_i^{(+)} = \\operatorname{softmax}\\!\\left(W (\\boldsymbol{\\mu} + \\Delta_i^{(+)}) + \\boldsymbol{b}\\right), \\quad \\boldsymbol{y}_i^{(-)} = \\operatorname{softmax}\\!\\left(W (\\boldsymbol{\\mu} + \\Delta_i^{(-)}) + \\boldsymbol{b}\\right).$$\n\n4. 使用以 $\\boldsymbol{y}_0$ 为参考的对称化 Kullback–Leibler (KL) 散度来衡量解码器沿方向 $i$ 的敏感度：$$s_i = \\tfrac{1}{2}\\left[ D_{\\mathrm{KL}}\\!\\left(\\boldsymbol{y}_i^{(+)} \\,\\big\\|\\, \\boldsymbol{y}_0\\right) + D_{\\mathrm{KL}}\\!\\left(\\boldsymbol{y}_i^{(-)} \\,\\big\\|\\, \\boldsymbol{y}_0\\right) \\right],$$ 其中 $$D_{\\mathrm{KL}}\\!\\left(\\boldsymbol{p} \\,\\big\\|\\, \\boldsymbol{q}\\right) = \\sum_{j=1}^k p_j \\log\\!\\left(\\frac{p_j}{q_j}\\right)。$$\n\n5. 给定一个敏感度阈值 $\\tau > 0$，定义有效维度 $$d_{\\mathrm{eff}} = \\left| \\left\\{ i \\in \\{1, \\ldots, d_c\\} \\,:\\, s_i > \\tau \\right\\} \\right|,$$ 并报告 $d_{\\mathrm{eff}}$ 和比率 $d_{\\mathrm{eff}} / d_c$。\n\n实现一个程序，为下面测试套件中的每个测试用例计算所有 $i$ 的 $s_i$、$d_{\\mathrm{eff}}$、$d_c$ 和 $d_{\\mathrm{eff}}/d_c$。Softmax 必须实现为 $$\\operatorname{softmax}(\\boldsymbol{z})_j = \\frac{e^{z_j}}{\\sum_{\\ell=1}^k e^{z_\\ell}}$$ 并通过在进行指数运算前减去 $\\boldsymbol{z}$ 的最大分量来进行数值稳定的评估。\n\n测试套件：\n- 测试用例 #1 (正常路径，各向异性协方差)：\n  - $d_c = 4$, $k = 3$。\n  - $\\boldsymbol{\\mu} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^\\top$。\n  - $\\Sigma = \\operatorname{diag}\\!\\left(1.0,\\, 0.5,\\, 0.1,\\, 0.01\\right)$。\n  - $W = \\begin{bmatrix}\n    0.3  -0.1  0.2  0.05 \\\\\n    -0.2  0.4  -0.1  0.3 \\\\\n    0.1  0.0  0.25  -0.2\n  \\end{bmatrix}$，$\\boldsymbol{b} = \\begin{bmatrix} 0.0  0.1  -0.1 \\end{bmatrix}^\\top$。\n  - $\\varepsilon = 0.2$, $\\tau = 0.005$。\n\n- 测试用例 #2 (退化协方差，降维方差方向)：\n  - $d_c = 4$, $k = 3$。\n  - $\\boldsymbol{\\mu} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^\\top$。\n  - $\\Sigma = \\operatorname{diag}\\!\\left(1.0,\\, 0.2,\\, 0.0,\\, 0.0\\right)$。\n  - $W = \\begin{bmatrix}\n    0.3  -0.1  0.2  0.05 \\\\\n    -0.2  0.4  -0.1  0.3 \\\\\n    0.1  0.0  0.25  -0.2\n  \\end{bmatrix}$，$\\boldsymbol{b} = \\begin{bmatrix} 0.0  0.1  -0.1 \\end{bmatrix}^\\top$。\n  - $\\varepsilon = 0.2$, $\\tau = 0.005$。\n\n- 测试用例 #3 (因饱和导致的解码器不敏感)：\n  - $d_c = 4$, $k = 3$。\n  - $\\boldsymbol{\\mu} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^\\top$。\n  - $\\Sigma = \\operatorname{diag}\\!\\left(1.5,\\, 1.5,\\, 1.5,\\, 1.5\\right)$。\n  - $W = \\begin{bmatrix}\n    0.5  0.5  0.5  0.5 \\\\\n    -0.1  -0.1  -0.1  -0.1 \\\\\n    0.05  0.05  0.05  0.05\n  \\end{bmatrix}$，$\\boldsymbol{b} = \\begin{bmatrix} 3.0  0.0  -3.0 \\end{bmatrix}^\\top$。\n  - $\\varepsilon = 0.2$, $\\tau = 0.001$。\n\n你的程序必须为每个测试用例计算敏感度列表 $\\left[s_1, s_2, \\ldots, s_{d_c}\\right]$、有效维度 $d_{\\mathrm{eff}}$、名义维度 $d_c$ 以及比率 $d_{\\mathrm{eff}}/d_c$。最终输出格式必须是一行，将所有测试用例的结果聚合到一个 Python 风格的列表中，其中每个测试用例的结果本身是一个结构为 $$\\left[ d_{\\mathrm{eff}},\\, d_c,\\, \\frac{d_{\\mathrm{eff}}}{d_c},\\, \\left[s_1, s_2, \\ldots, s_{d_c}\\right] \\right]$$ 的列表。例如，输出应类似于 $$\\left[ \\left[ \\cdots \\right], \\left[ \\cdots \\right], \\left[ \\cdots \\right] \\right],$$ 打印为一行，条目用逗号分隔并用方括号括起。", "solution": "该问题提出了一个明确定义的计算诊断方法，用于分析神经网络解码器的敏感度。其目标是量化上下文向量 $\\boldsymbol{c}$ 沿其主要变化方向的扰动如何影响解码器的输出分布，并利用这些信息来确定上下文表示的*有效维度*。这种分析对于理解信息流和识别编码器-解码器架构中的潜在瓶颈至关重要。该过程在科学上是合理的，它基于线性代数（特征分解）、信息论（Kullback-Leibler 散度）和神经网络建模（softmax 激活）的既定原则。所提供的测试用例是完整的，旨在探究不同且有意义的场景。\n\n解决方案通过为每个测试用例实现指定的五步诊断程序来展开。\n\n**步骤 1：协方差矩阵的特征分解**\n上下文向量 $\\boldsymbol{c}$ 被建模为一个均值为 $\\boldsymbol{\\mu}$、协方差矩阵为 $\\Sigma$ 的随机变量。协方差矩阵 $\\Sigma$ 捕捉了 $\\boldsymbol{c}$ 各分量的方差以及分量对之间的协方差。为了理解主要的变异模式，我们对 $\\Sigma$ 进行特征分解：\n$$ \\Sigma = V \\Lambda V^\\top $$\n在这里，$V$ 是一个正交矩阵，其列 $\\boldsymbol{v}_i$ 是 $\\Sigma$ 的特征向量。这些特征向量构成了上下文空间 $\\mathbb{R}^{d_c}$ 的一个标准正交基，并代表了主要的变异方向。矩阵 $\\Lambda$ 是一个对角矩阵，包含相应的按非递增顺序排列的特征值 $\\lambda_i$：$\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_{d_c} \\ge 0$。每个特征值 $\\lambda_i$ 量化了上下文向量沿主方向 $\\boldsymbol{v}_i$ 的方差。大的特征值表示上下文向量所编码数据中一个显著变化的维度，而小的或零特征值则表示一个变化很小或没有变化的维度。\n\n**步骤 2：构造缩放扰动**\n为了评估解码器的敏感度，我们沿着每个主方向 $\\boldsymbol{v}_i$ 扰动均值上下文向量 $\\boldsymbol{\\mu}$。扰动定义为：\n$$ \\Delta_i^{(+)} = +\\varepsilon \\sqrt{\\lambda_i} \\, \\boldsymbol{v}_i, \\quad \\Delta_i^{(-)} = -\\varepsilon \\sqrt{\\lambda_i} \\, \\boldsymbol{v}_i $$\n扰动的幅度按 $\\sqrt{\\lambda_i}$ 进行缩放，这是上下文向量分布沿方向 $\\boldsymbol{v}_i$ 的标准差。这确保了扰动代表了与均值的合理偏差，且与上下文向量的自然变异性成比例。小标量 $\\varepsilon$ 控制了此偏差的相对大小。如果一个特征值 $\\lambda_i$ 为零，则相应的扰动 $\\Delta_i^{(\\pm)}$ 为零向量，这正确地反映了在该方向上没有可供探测的变异。\n\n**步骤 3：计算基准输出和扰动后输出**\n解码器将一个上下文向量转换为一个关于 $k$ 个类别的概率分布。首先，我们计算与均值上下文向量 $\\boldsymbol{\\mu}$ 相对应的基准输出分布 $\\boldsymbol{y}_0$：\n$$ \\boldsymbol{z}_0 = W \\boldsymbol{\\mu} + \\boldsymbol{b} $$\n$$ \\boldsymbol{y}_0 = \\operatorname{softmax}(\\boldsymbol{z}_0) $$\n接下来，对于每个主方向 $i$，我们应用扰动来创建扰动后的上下文向量 $\\boldsymbol{\\mu} + \\Delta_i^{(+)}$ 和 $\\boldsymbol{\\mu} + \\Delta_i^{(-)}$，并计算得到的输出分布 $\\boldsymbol{y}_i^{(+)}$ 和 $\\boldsymbol{y}_i^{(-)}$：\n$$ \\boldsymbol{y}_i^{(+)} = \\operatorname{softmax}\\big(W (\\boldsymbol{\\mu} + \\Delta_i^{(+)}) + \\boldsymbol{b}\\big) $$\n$$ \\boldsymbol{y}_i^{(-)} = \\operatorname{softmax}\\big(W (\\boldsymbol{\\mu} + \\Delta_i^{(-)}) + \\boldsymbol{b}\\big) $$\nsoftmax 函数，通过数值稳定技术（减去最大 logit 值）实现，将 logit 向量 $\\boldsymbol{z}$ 转换为概率分布。\n\n**步骤 4：使用对称化 KL 散度衡量敏感度**\n解码器输出分布的变化使用 Kullback-Leibler (KL) 散度 $D_{\\mathrm{KL}}(\\boldsymbol{p} \\,\\|\\, \\boldsymbol{q})$ 来衡量。这个量衡量了从先验分布 $\\boldsymbol{q}$ 转移到后验分布 $\\boldsymbol{p}$ 时以比特或奈特为单位的信息增益。在这里，它量化了从基准分布 $\\boldsymbol{y}_0$ 到扰动后分布的“距离”。沿第 $i$ 个主方向的敏感度 $s_i$ 定义为正向和负向扰动的 KL 散度的对称化平均值：\n$$ s_i = \\tfrac{1}{2}\\left[ D_{\\mathrm{KL}}\\!\\left(\\boldsymbol{y}_i^{(+)} \\,\\big\\|\\, \\boldsymbol{y}_0\\right) + D_{\\mathrm{KL}}\\!\\left(\\boldsymbol{y}_i^{(-)} \\,\\big\\|\\, \\boldsymbol{y}_0\\right) \\right] $$\n其中 $D_{\\mathrm{KL}}(\\boldsymbol{p} \\,\\|\\, \\boldsymbol{q}) = \\sum_{j=1}^k p_j \\log(p_j/q_j)$。$s_i$ 的高值表明沿方向 $\\boldsymbol{v}_i$ 的变异强烈影响解码器的输出。\n\n**步骤 5：确定有效维度**\n最后，有效维度 $d_{\\mathrm{eff}}$ 定义为敏感度 $s_i$ 超过给定阈值 $\\tau$ 的主方向的数量：\n$$ d_{\\mathrm{eff}} = \\left| \\left\\{ i \\in \\{1, \\ldots, d_c\\} \\,:\\, s_i > \\tau \\right\\} \\right| $$\n该度量提供了一个功能性的、数据驱动的衡量解码器处理信息维度的方法。如果 $d_{\\mathrm{eff}}$ 远小于名义维度 $d_c$，这表明模型管道存在信息瓶颈，即解码器对上下文向量中存在的许多方差方向上的变化不敏感。比率 $d_{\\mathrm{eff}} / d_c$ 将此度量归一化，提供了一个关于维度利用率的直观分数。\n\n对每个测试用例都系统地实施这些步骤，以生成所需的输出：$d_{\\mathrm{eff}}$、$d_c$、它们的比率以及敏感度向量 $[s_1, \\ldots, s_{d_c}]$。", "answer": "```python\nimport numpy as np\nfrom scipy.special import kl_div\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified in the problem statement.\n    \"\"\"\n\n    def stable_softmax(z: np.ndarray) - np.ndarray:\n        \"\"\"\n        Computes the softmax of a vector in a numerically stable way.\n        \n        Args:\n            z: A 1D numpy array of logits.\n\n        Returns:\n            A 1D numpy array of probabilities.\n        \"\"\"\n        # Shift z by its max value to avoid overflow\n        z_shifted = z - np.max(z)\n        exp_z = np.exp(z_shifted)\n        return exp_z / np.sum(exp_z)\n\n    def compute_kullback_leibler(p: np.ndarray, q: np.ndarray) - float:\n        \"\"\"\n        Computes the Kullback-Leibler divergence D_KL(p || q).\n        \n        Args:\n            p: The first probability distribution (1D numpy array).\n            q: The second probability distribution (1D numpy array).\n        \n        Returns:\n            The KL divergence as a float.\n        \"\"\"\n        # scipy.special.kl_div(p, q) computes p * log(p/q) - p + q element-wise.\n        # Summing this over all elements gives sum(p*log(p/q)) because sum(p)=sum(q)=1.\n        return np.sum(kl_div(p, q))\n\n    def analyze_decoder_sensitivity(\n        dc: int, k: int, mu: np.ndarray, Sigma: np.ndarray, \n        W: np.ndarray, b: np.ndarray, epsilon: float, tau: float\n    ) - list:\n        \"\"\"\n        Performs the full diagnostic analysis for a single test case.\n\n        Args:\n            dc: Nominal dimensionality of the context vector.\n            k: Number of output classes.\n            mu: Mean context vector.\n            Sigma: Covariance matrix of the context vector.\n            W: Weight matrix of the decoder's linear layer.\n            b: Bias vector of the decoder's linear layer.\n            epsilon: Perturbation scaling factor.\n            tau: Sensitivity threshold.\n\n        Returns:\n            A list containing [d_eff, dc, d_eff/dc, [s_1, ..., s_dc]].\n        \"\"\"\n        # Step 1: Eigen-decomposition of the covariance matrix\n        eigenvalues, eigenvectors = np.linalg.eigh(Sigma)\n        # np.linalg.eigh returns eigenvalues in ascending order. We need descending.\n        sort_indices = np.argsort(eigenvalues)[::-1]\n        eigenvalues = eigenvalues[sort_indices]\n        eigenvectors = eigenvectors[:, sort_indices]\n\n        # Step 3 (part 1): Compute baseline decoder output\n        z0 = W @ mu + b\n        y0 = stable_softmax(z0)\n\n        sensitivities = []\n        for i in range(dc):\n            lambda_i = eigenvalues[i]\n            v_i = eigenvectors[:, i]\n\n            # Since Sigma is positive semi-definite, lambda_i = 0.\n            # Handle potential floating point inaccuracies.\n            sqrt_lambda_i = np.sqrt(max(0, lambda_i))\n\n            # Step 2: Form scaled perturbations\n            if sqrt_lambda_i == 0:\n                # No variation in this direction, so sensitivity is zero.\n                s_i = 0.0\n            else:\n                delta_i_plus = epsilon * sqrt_lambda_i * v_i\n                \n                # Step 3 (part 2): Compute perturbed outputs\n                y_i_plus = stable_softmax(W @ (mu + delta_i_plus) + b)\n                y_i_minus = stable_softmax(W @ (mu - delta_i_plus) + b)\n\n                # Step 4: Measure sensitivity\n                d_kl_plus = compute_kullback_leibler(y_i_plus, y0)\n                d_kl_minus = compute_kullback_leibler(y_i_minus, y0)\n                s_i = 0.5 * (d_kl_plus + d_kl_minus)\n            \n            sensitivities.append(s_i)\n        \n        # Step 5: Compute effective dimensionality\n        sensitivities_arr = np.array(sensitivities)\n        d_eff = int(np.sum(sensitivities_arr  tau))\n        \n        ratio = d_eff / dc if dc  0 else 0.0\n\n        return [d_eff, dc, ratio, sensitivities]\n\n    # Test Suite Definition\n    test_cases = [\n        # Test Case #1\n        {\n            \"dc\": 4, \"k\": 3,\n            \"mu\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"Sigma\": np.diag([1.0, 0.5, 0.1, 0.01]),\n            \"W\": np.array([\n                [0.3, -0.1, 0.2, 0.05],\n                [-0.2, 0.4, -0.1, 0.3],\n                [0.1, 0.0, 0.25, -0.2]\n            ]),\n            \"b\": np.array([0.0, 0.1, -0.1]),\n            \"epsilon\": 0.2, \"tau\": 0.005\n        },\n        # Test Case #2\n        {\n            \"dc\": 4, \"k\": 3,\n            \"mu\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"Sigma\": np.diag([1.0, 0.2, 0.0, 0.0]),\n            \"W\": np.array([\n                [0.3, -0.1, 0.2, 0.05],\n                [-0.2, 0.4, -0.1, 0.3],\n                [0.1, 0.0, 0.25, -0.2]\n            ]),\n            \"b\": np.array([0.0, 0.1, -0.1]),\n            \"epsilon\": 0.2, \"tau\": 0.005\n        },\n        # Test Case #3\n        {\n            \"dc\": 4, \"k\": 3,\n            \"mu\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"Sigma\": np.diag([1.5, 1.5, 1.5, 1.5]),\n            \"W\": np.array([\n                [0.5, 0.5, 0.5, 0.5],\n                [-0.1, -0.1, -0.1, -0.1],\n                [0.05, 0.05, 0.05, 0.05]\n            ]),\n            \"b\": np.array([3.0, 0.0, -3.0]),\n            \"epsilon\": 0.2, \"tau\": 0.001\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = analyze_decoder_sensitivity(**case)\n        all_results.append(result)\n    \n    # Format the final output string as a Python-style list of lists.\n    # The str() function on a list provides the required '[...]' format.\n    # Joining these string representations with a comma creates the final structure.\n    output_str = f\"[{','.join(str(r) for r in all_results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3184008"}]}