{"hands_on_practices": [{"introduction": "堆叠循环神经网络（RNN）的核心思想是构建一个表示的层次结构，其中每一层都在前一层输出的时间序列上进行操作。这个练习通过一个思想实验将这一概念具体化，你将构建一个确定性的、带时钟的堆叠RNN，以模拟对输入序列的二叉树归约操作 ([@problem_id:3176027])。通过这个练习，你将直观地理解更深的层如何处理更长的时间尺度，并探索网络深度与解决不同类型任务（如线性的“多数”任务和非线性的“奇偶校验”任务）能力之间的关系。", "problem": "您的任务是形式化并实现一个确定性的、时钟控制的堆叠式循环神经网络 (RNN)，该网络模拟对一维二进制输入序列的二叉树规约。该模型应用于解决长度为2的幂的序列上的两个任务：分层奇偶校验任务和分层多数投票任务。实现必须严格遵守下文的定义和约束，最终程序必须以单行、机器可检查的格式为指定的测试套件生成结果。\n\n基本原理和定义：\n- 循环神经网络 (RNN) 由形式为 $h_t = \\phi(W_h h_{t-1} + W_x x_t + b)$ 的隐藏状态更新定义，其中 $h_t \\in \\mathbb{R}^d$ 是时间 $t$ 的隐藏状态，$x_t \\in \\mathbb{R}^m$ 是时间 $t$ 的输入，$W_h \\in \\mathbb{R}^{d \\times d}$ 和 $W_x \\in \\mathbb{R}^{d \\times m}$ 是权重矩阵，$b \\in \\mathbb{R}^d$ 是偏置，$\\phi$ 是非线性激活函数，例如双曲正切函数 $\\tanh$。\n- 堆叠式（深度）RNN 是由 $L$ 个循环层组成的，其中第 $l-1$ 层的输出序列作为第 $l$ 层在相同时序索引处的输入序列。\n- 在本问题中，您将使用堆叠式 RNN 的解释来实现一个确定性的、时钟控制的二叉树规约方案：对于序列长度 $N = 2^L$，层 $l \\in \\{1, 2, \\dots, L\\}$ 仅在时间索引 $t$ 满足 $t \\bmod 2^l = 2^l - 1$（块的末尾）时发出一个值，该值通过一个固定的、已知的成对单元（pairwise cell）组合来自第 $l-1$ 层在时间 $t - 2^{l-1}$ 和 $t$ 的两个值。在非发射时间，该层发出一个中性的占位符 $0$，该占位符不会被进一步使用。这种时钟控制实现了一个深度为 $L$ 的二叉树电路，明确了层深度与布尔电路深度之间的关系。\n\n输入和编码约定：\n- 输入字母表为二进制 $\\{0, 1\\}$。每个比特 $b_t \\in \\{0,1\\}$ 在第 $0$ 层的输入处被映射为一个带符号的值 $s_t = 2 b_t - 1 \\in \\{-1, +1\\}$。\n- 对于分层奇偶校验任务，在每一层，成对组合函数必须通过精确计算乘积 $u v$（当 $u, v \\in \\{-1, +1\\}$ 时）来近似比特上的布尔异或 (XOR) 运算，因为当且仅当两个比特相等时 $u v = +1$，当且仅当它们不同时 $u v = -1$。您必须实现一个固定的双层 $\\tanh$ 网络，在四个角点输入 $(u, v) \\in \\{-1, +1\\}^2$ 上实现此映射：\n  1. 使用带有偏移量的仿射半空间定义隐藏特征，以分离四个角点：\n     $$h_1 = \\tanh\\big(\\alpha \\, (u + v - 0.5)\\big), \\quad h_2 = \\tanh\\big(\\alpha \\, (-u - v - 0.5)\\big),$$\n     $$h_3 = \\tanh\\big(\\alpha \\, (u - v - 0.5)\\big), \\quad h_4 = \\tanh\\big(\\alpha \\, (v - u - 0.5)\\big).$$\n  2. 将这些特征线性组合以形成预输出\n     $$y_{\\text{lin}} = h_1 + h_2 - h_3 - h_4.$$\n  3. 应用输出非线性\n     $$y = \\tanh(\\beta \\, y_{\\text{lin}}).$$\n  对于 $u, v \\in \\{-1, +1\\}$ 以及足够大的正增益 $\\alpha$ 和 $\\beta$，这将产生 $y \\approx \\operatorname{sign}(u v) \\in \\{-1, +1\\}$，在四个角点上精确匹配所需的乘积。\n- 对于分层多数投票任务，正确的最终决策取决于和 $S = \\sum_{t=1}^N s_t$ 的符号，其中 $s_t \\in \\{-1, +1\\}$。您必须使用线性成对组合函数\n  $$g_{\\text{sum}}(u, v) = u + v,$$\n  使得第 $L$ 层的输出等于精确的总和 $S$。最终的多数决策是，如果 $S > 0$ 则为 $1$，否则为 $0$（平局映射为 $0$）。这种线性路径是允许的，并突显了多数投票是线性可聚合的，而奇偶校验则需要非线性。\n- 在奇偶校验单元中选择并使用固定常数 $\\alpha = 6$ 和 $\\beta = 3$。\n\n堆叠式、时钟控制的规约：\n- 设 $N = 2^L$，其中 $L \\ge 2$ 为某个整数。设 $z^{(0)}_t = 2 b_t - 1$，对于 $t \\in \\{0, 1, \\dots, N-1\\}$。\n- 对于层 $l \\in \\{1, \\dots, L\\}$ 和每个时间 $t \\in \\{0, 1, \\dots, N-1\\}$：\n  - 如果 $t \\bmod 2^l = 2^l - 1$，则设置\n    $$z^{(l)}_t = g\\left(z^{(l-1)}_{t - 2^{l-1}}, \\; z^{(l-1)}_{t}\\right),$$\n    其中对于奇偶校验任务 $g = g_{\\text{parity}}$，对于多数投票任务 $g = g_{\\text{sum}}$。\n  - 否则设置 $z^{(l)}_t = 0$（未使用的占位符）。\n- 深度 $L$ 的最终标量输出是在时间 $t = N - 1$ 的发射值，即 $o = z^{(L)}_{N-1}$。\n\n标签解码：\n- 奇偶校验标签：设 $P = \\prod_{t=1}^N (2 b_t - 1) \\in \\{-1, +1\\}$ 为带符号比特的乘积。对于偶数 $N$，当且仅当 $P = -1$ 时，XOR 奇偶校验位为 $1$；当且仅当 $P = +1$ 时为 $0$。因此，对于奇偶校验任务和偶数 $N$，将模型输出 $o$ 解码为：如果 $o < 0$ 则 $\\hat{y}_{\\text{parity}} = 1$，否则为 $0$。\n- 多数投票标签：设 $S = \\sum_{t=1}^N (2 b_t - 1)$。将模型输出 $o$ 解码为：如果 $o > 0$ 则 $\\hat{y}_{\\text{majority}} = 1$，否则为 $0$。\n\n测试套件：\n- 评估涵盖不同深度和任务的四种情况：\n  1. 在所有长度为 $N = 4$（即 $L = 2$）的序列上进行奇偶校验，以浮点数形式报告准确率。\n  2. 在所有长度为 $N = 8$（即 $L = 3$）的序列上进行奇偶校验，以浮点数形式报告准确率。\n  3. 在所有长度为 $N = 4$ 的序列上进行多数投票，以浮点数形式报告准确率。\n  4. 在所有长度为 $N = 8$ 的序列上进行多数投票，以浮点数形式报告准确率。\n- 每个准确率必须在完整的序列集 $\\{0,1\\}^N$ 上计算，产生一个在 $[0,1]$ 区间内的值，作为正确预测的标准分数（不带百分号）。\n- 不涉及任何物理单位或角度单位。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按顺序为\n  $$[\\text{acc\\_parity\\_4}, \\text{acc\\_parity\\_8}, \\text{acc\\_majority\\_4}, \\text{acc\\_majority\\_8}],$$\n  其中每个准确率是四舍五入到三位小数的浮点数。例如，一个有效的输出行可能如下所示\n  $$[0.875,0.996,1.000,1.000].$$\n\n实现约束：\n- 使用固定的常数 $\\alpha = 6$ 和 $\\beta = 3$，完全按照规定实现成对奇偶校验单元。\n- 将多数投票单元实现为精确的线性和。\n- 按照描述实现时钟控制的堆叠式规约，序列长度为 $N \\in \\{4, 8\\}$。\n- 不要训练任何参数；所有计算必须是确定性的，并基于指定的公式。\n- 为每个 $N$ 枚举所有二进制序列以计算精确的准确率。\n\n您的最终提交必须是一个完整的、可运行的程序，该程序执行这些计算并以所要求的精确单行格式打印结果。", "solution": "用户提供了一个有效的、定义明确的问题陈述，该陈述基于深度学习和计算神经科学的原理。任务是形式化并实现一种特定类型的堆叠式循环神经网络 (RNN)，该网络确定性地模拟二叉树规约算法。然后应用此模型来解决长度为 $N=2^L$ 的二进制序列上的分层奇偶校验和多数投票任务。解决方案需要实现指定的算法和函数，在所有可能的 $N=4$ 和 $N=8$ 输入序列上评估其性能，并报告准确率。\n\n解决方案的结构如下：首先，实现每个任务的成对组合函数。其次，构建主时钟控制规约算法。最后，将这些组件集成到一个评估框架中，以计算所需的准确率。\n\n### 1. 成对组合函数\n\n二叉树规约的核心是一个递归应用的成对组合函数 $g$。问题为每个任务指定了两个不同的函数。\n\n**多数投票任务**：多数投票任务是线性可分的。目标是确定输入序列中 $1$ 的数量是否超过 $0$ 的数量。使用指定的输入编码，其中比特 $b_t \\in \\{0, 1\\}$ 被映射为带符号值 $s_t=2b_t-1 \\in \\{-1, +1\\}$，这等同于检查总和 $S = \\sum_{t=0}^{N-1} s_t$ 的符号。由于加法的结合律，这个和可以分层计算。指定的成对函数是一个简单的线性和：\n$$g_{\\text{sum}}(u, v) = u + v$$\n在二叉树结构中应用此函数将导致根节点的最终输出是所有叶（输入）值的精确总和。\n\n**奇偶校验任务**：奇偶校验任务（等同于两个比特的异或）是一个经典的非线性问题。目标是确定 $1$ 的数量是否为奇数。使用带符号比特编码，这对应于计算乘积 $P = \\prod_{t=0}^{N-1} s_t$ 的符号。因此，成对组合函数必须近似其两个输入的乘积，$g(u, v) \\approx u \\cdot v$。问题提供了一个特定的固定参数双层神经网络来实现此函数 $g_{\\text{parity}}$：\n$$g_{\\text{parity}}(u, v) = \\tanh(\\beta \\, y_{\\text{lin}})$$\n其中线性预输出 $y_{\\text{lin}}$ 由下式给出：\n$$y_{\\text{lin}} = h_1 + h_2 - h_3 - h_4$$\n隐藏特征 $h_i$ 通过仿射变换并经过 $\\tanh$ 激活函数定义：\n$$h_1 = \\tanh\\big(\\alpha \\, (u + v - 0.5)\\big)$$\n$$h_2 = \\tanh\\big(\\alpha \\, (-u - v - 0.5)\\big)$$\n$$h_3 = \\tanh\\big(\\alpha \\, (u - v - 0.5)\\big)$$\n$$h_4 = \\tanh\\big(\\alpha \\, (v - u - 0.5)\\big)$$\n常数固定为 $\\alpha = 6$ 和 $\\beta = 3$。高增益 $\\alpha$ 确保隐藏单元在其饱和区工作，在 $(u,v)$ 空间中创建清晰的决策边界，以分离四个角点 $(\\pm 1, \\pm 1)$。线性组合被加权，使得当 $u$ 和 $v$ 符号相同时产生正的 $y_{\\text{lin}}$，当它们符号相反时产生负的 $y_{\\text{lin}}$。最终的高增益 $\\beta$ 使输出变得尖锐，确保对于输入 $(u,v) \\in \\{-1, +1\\}^2$，输出 $y$ 极其接近期望的乘积 $uv$。这种设计是鲁棒的，意味着即使输入 $u$ 和 $v$ 不完全是 $\\pm 1$ 而是接近（这可能在网络的更高层发生），该函数仍然能计算出一个非常接近正确符号值的输出。\n\n### 2. 堆叠式、时钟控制的规约算法\n\n二叉树规约是使用具有特定“时钟控制”机制的堆叠式 RNN 架构实现的。该网络在输入层（第 $0$ 层）之上有 $L=\\log_2 N$ 层。\n\n设 $z^{(l)}_t$ 为第 $l$ 层、时间步 $t$ 的神经元激活值。\n- **第 0 层**：输入层仅包含带符号的二进制序列：$z^{(0)}_t = 2b_t - 1$，对于 $t \\in \\{0, 1, \\dots, N-1\\}$。\n- **第 $l \\in \\{1, \\dots, L\\}$ 层**：激活值 $z^{(l)}_t$ 仅在特定的时间步计算。计算的条件是 $t \\bmod 2^l = 2^l - 1$。这意味着第 $l$ 层的神经元仅在大小为 $2^l$ 的块的末尾产生输出。在这些时刻，它组合来自前一层 $l-1$ 的两个输出，这两个输出本身标记了两个大小为 $2^{l-1}$ 的相邻子块的末尾。更新规则是：\n$$z^{(l)}_t = g\\left(z^{(l-1)}_{t - 2^{l-1}}, \\; z^{(l-1)}_{t}\\right)$$\n对于所有不满足条件的其他时间步，输出是一个占位符值 $z^{(l)}_t = 0$。这确保了函数 $g$ 总是在前一层有效的、已计算的输入上被调用。\n整个网络的最终输出是在最后一层 $L$、最后一个时间步 $N-1$ 计算出的单个值：\n$$o = z^{(L)}_{N-1}$$\n这个过程精确地反映了二叉树计算，其中叶节点是输入序列，每个内部节点对其两个子节点计算函数 $g$。\n\n### 3. 评估和预期准确率\n\n通过在 $N=4$ 和 $N=8$ 的所有 $2^N$ 个可能的二进制输入序列上计算准确率来评估性能。\n\n- **多数投票任务**：模型输出 $o$ 是精确的总和 $S = \\sum s_t$。解码规则是如果 $o > 0$ 则 $\\hat{y}_{\\text{majority}} = 1$，否则为 $0$。真实标签基于 $S$ 遵循相同的规则。由于模型的计算是精确的，预测将始终与真实标签匹配。因此，对于 $N=4$ 和 $N=8$，准确率预期均为 $1.0$。\n\n- **奇偶校验任务**：模型输出 $o$ 是真实乘积 $P=\\prod s_t$ 的高精度近似。解码规则是如果 $o < 0$ 则 $\\hat{y}_{\\text{parity}} = 1$，否则为 $0$。如果 $P=-1$，真实标签为 $1$；如果 $P=+1$，真实标签为 $0$。$g_{\\text{parity}}$ 函数的鲁棒设计，凭借其高增益饱和非线性，确保了输出 $o$ 的符号将正确匹配真实乘积 $P$ 的符号。因此，对于所有输入，预测都预期是正确的，从而在 $N=4$ 和 $N=8$ 上都产生 $1.0$ 的准确率。\n\n实现将系统地生成每个二进制序列，运行规约算法以获得模型输出，解码预测，计算真实标签，并统计正确的预测以得出最终的准确率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a deterministic, clocked, stacked RNN \n    that emulates a binary-tree reduction for parity and majority tasks.\n    \"\"\"\n\n    ALPHA = 6.0\n    BETA = 3.0\n\n    def g_parity(u, v):\n        \"\"\"\n        Pairwise combine function for the parity task, approximating u*v.\n        \"\"\"\n        h1 = np.tanh(ALPHA * (u + v - 0.5))\n        h2 = np.tanh(ALPHA * (-u - v - 0.5))\n        h3 = np.tanh(ALPHA * (u - v - 0.5))\n        h4 = np.tanh(ALPHA * (v - u - 0.5))\n        y_lin = h1 + h2 - h3 - h4\n        return np.tanh(BETA * y_lin)\n\n    def g_sum(u, v):\n        \"\"\"\n        Pairwise combine function for the majority task, computing u+v.\n        \"\"\"\n        return u + v\n\n    def run_reduction(binary_sequence, task):\n        \"\"\"\n        Runs the stacked, clocked reduction algorithm on a binary sequence.\n        \"\"\"\n        N = len(binary_sequence)\n        # Sequence length N must be a power of two.\n        L = int(math.log2(N))\n        \n        # z_history[l] stores the activations for layer l.\n        z_history = [np.zeros(N, dtype=np.float64) for _ in range(L + 1)]\n\n        # Layer 0: Input layer with signed bits.\n        z_history[0] = np.array([2 * b - 1 for b in binary_sequence], dtype=np.float64)\n        \n        g = g_parity if task == 'parity' else g_sum\n        \n        # Layers l = 1 to L\n        for l in range(1, L + 1):\n            block_size = 2**l\n            step_size = 2**(l-1)\n            # Iterate through all time steps t = 0 to N-1.\n            for t in range(N):\n                # An output is emitted only at the end of a block.\n                if (t + 1) % block_size == 0:\n                    input1 = z_history[l-1][t - step_size]\n                    input2 = z_history[l-1][t]\n                    z_history[l][t] = g(input1, input2)\n        \n        # The final output is at layer L, time N-1.\n        return z_history[L][N-1]\n\n    def get_true_label(binary_sequence, task):\n        \"\"\"\n        Computes the ground truth label for a given sequence and task.\n        \"\"\"\n        signed_sequence = np.array([2 * b - 1 for b in binary_sequence], dtype=np.float64)\n        \n        if task == 'parity':\n            # For even N, parity is 1 iff the product of signed bits is -1.\n            product = np.prod(signed_sequence)\n            return 1 if product  0 else 0\n        elif task == 'majority':\n            # Majority is 1 iff the sum of signed bits is > 0.\n            total_sum = np.sum(signed_sequence)\n            return 1 if total_sum > 0 else 0\n        else:\n            raise ValueError(\"Unknown task\")\n\n    def evaluate_task(N, task):\n        \"\"\"\n        Evaluates the model's accuracy for a given N and task over all 2^N sequences.\n        \"\"\"\n        num_correct = 0\n        num_total = 2**N\n        \n        for i in range(num_total):\n            # Generate the i-th binary sequence of length N.\n            # Bits are ordered from most to least significant.\n            binary_sequence = [(i >> j)  1 for j in range(N - 1, -1, -1)]\n            \n            # Get model prediction.\n            model_output = run_reduction(binary_sequence, task)\n            \n            if task == 'parity':\n                # Decode model output for parity.\n                prediction = 1 if model_output  0 else 0\n            else: # task == 'majority'\n                # Decode model output for majority.\n                prediction = 1 if model_output > 0 else 0\n            \n            # Get true label.\n            true_label = get_true_label(binary_sequence, task)\n            \n            if prediction == true_label:\n                num_correct += 1\n                \n        return num_correct / num_total\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (4, 'parity'),\n        (8, 'parity'),\n        (4, 'majority'),\n        (8, 'majority')\n    ]\n\n    results = []\n    for N, task in test_cases:\n        accuracy = evaluate_task(N, task)\n        results.append(accuracy)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "3176027"}, {"introduction": "堆叠RNN不仅能学习特征层次，还具备执行复杂算法的计算能力。本练习将挑战你从第一性原理出发，设计一个双层堆叠长短期记忆（LSTM）网络，以模拟一个下推自动机（PDA）来识别经典的上下文无关语言 $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$ ([@problem_id:3175992])。通过手动设置网络参数，你将探索如何让不同层承担专门的角色（例如，一个层用于阶段检测，另一个层用作计数器），从而深入理解LSTM门控机制和层级结构在实现算法推理中的作用。", "problem": "你的任务是确定一个由长短期记忆 (LSTM) 层组成的堆叠循环神经网络 (RNN) 何时能够模拟一个下推自动机 (PDA) 来识别嵌套结构。考虑形式语言 $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$，其中 $\\mathbb{N}_0$ 表示非负整数集。字母表为 $\\{a, b\\}$，输入是从该字母表中抽取的有限符号序列。此任务的基础包括深度学习和自动机理论中以下经过充分检验的定义。\n\n对于单个 LSTM 层，在时间步 $t$ 时，输入向量为 $x_t$，前一个隐藏状态为 $h_{t-1}$，前一个单元状态为 $c_{t-1}$，其门和状态定义如下：\n$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o), $$\n$$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c), \\quad c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, \\quad h_t = o_t \\odot \\tanh(c_t), $$\n其中 $\\sigma$ 是 logistic sigmoid 函数，$\\tanh$ 是双曲正切函数，$\\odot$ 表示逐元素乘法。在一个包含 $\\ell$ 层的堆叠架构中，第 $\\ell$ 层的输入是 $x_t^{(\\ell)} = h_t^{(\\ell-1)}$，而 $x_t^{(1)}$ 由外部输入编码定义。一个下推自动机 (PDA) 通过以下方式识别 $L$：读取 $a$ 符号时增加一个计数器，读取 $b$ 符号时减少该计数器，同时强制要求一旦观察到任何 $b$ 之后，就不能再出现 $a$，并且计数器永远不能为负。如果在输入结束时计数器回到 $0$，则接受该输入。\n\n你的任务是：\n- 从上述定义出发，推导出一个双层堆叠 LSTM 能够模拟用于识别语言 $L$ 的 PDA 行为所需满足的门和内部信号的条件。具体来说，要确定第一层如何产生一个内部信号来指示是否已见过任何 $b$，以及第二层如何在其单元状态 $c_t^{(2)}$ 中更新一个计数器，以便在 $b$ 阶段开始前对 $a$ 执行 $+1$ 更新，此后对 $b$ 执行 $-1$ 更新。你的推导必须从给定的 LSTM 方程和 PDA 的接受语义出发，并论证一些原则性要求，例如门饱和到接近 $0$ 或 $1$ 的值、对计数器维度的单调更新，以及各层之间的角色分离。避免使用跳过这些推理步骤的快捷公式。\n- 构建一个独立的程序，该程序模拟一个双层堆叠 LSTM，其具有精心选择的、与你推导的条件一致的确定性门行为，且无需训练。第一层应产生包含以下内容的输出：\n  1. 当前符号身份（是 $a$ 还是 $b$）的指示符式直通信号。\n  2. 一个单调的内部信号，当观察到 $b$ 时该信号增加（以便模型能够检测是否已见过任何 $b$）。\n  第二层应在其单元状态 $c_t^{(2)}$ 中维护一个有符号计数器，在读取 $a$ 且尚未出现 $b$ 时增加 $+1$，在读取 $b$ 时减少 $-1$。你的程序必须强制执行 PDA 的约束：一旦见过任何 $b$，再遇到 $a$ 便是无效的，并且计数器决不能低于 $0$。程序当且仅当最终计数器 $c_T^{(2)}$ 在一个微小容差 $\\epsilon$ 内为 $0$，没有在 $b$ 之后出现无效的 $a$，且没有任何前缀导致计数器变为负数时，才接受输入。\n- 使用以下独热输入编码表示符号：$a \\mapsto [1, 0]$，$b \\mapsto [0, 1]$。跟踪第一层的演变单元状态 $c_t^{(1)}$ 和第二层的 $c_t^{(2)}$。根据你的设计，使用根据需要饱和到 $0$ 或 $1$ 的门值，显式计算 $c_t^{(2)}$ 中的计数器。\n- 实现并评估以下基于 $\\{a,b\\}$ 的序列测试套件：\n  1. 空字符串 $\"\"$。\n  2. $\"ab\"$。\n  3. $\"aaabbb\"$。\n  4. $\"aabbb\"$。\n  5. $\"aaabb\"$。\n  6. $\"aba\"$。\n  7. $\"b\"$。\n  8. $\"aaaabbbb\"$。\n- 使用布尔决策为每个序列定义精确的接受条件。使用一个具有合理小正值（例如 $\\epsilon = 10^{-3}$）的容差 $\\epsilon$ 来将最终计数器与 $0$ 进行比较。\n- 最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表顺序与测试套件相同（例如 $[r_1,r_2,\\dots,r_8]$），其中每个 $r_i$ 是测试用例 $i$ 的布尔接受结果。\n\n不涉及物理单位或角度；所有输出均为布尔值。程序必须是完全独立的，无需用户输入或外部文件即可运行。其逻辑必须从数学编程的角度来看是普遍可理解的，并且应可在任何现代编程语言中实现，但你必须按规定生成 Python 代码。", "solution": "我们从长短期记忆 (LSTM) 的定义开始。对于一个 LSTM，其单元状态更新公式为\n$$ c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t, $$\n门激活值为\n$$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o), $$\n候选状态为\n$$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c). $$\n在堆叠架构中，第 $\\ell$ 层的输入是前一层的隐藏输出：\n$$ x_t^{(\\ell)} = h_t^{(\\ell-1)}, \\quad h_t^{(\\ell)} = o_t^{(\\ell)} \\odot \\tanh(c_t^{(\\ell)}). $$\n\n用于语言 $L = \\{ a^n b^n \\mid n \\in \\mathbb{N}_0 \\}$ 的下推自动机 (PDA) 必须遵循以下语义约束：\n- 读取 $a$ 符号时，执行入栈操作，每个 $a$ 使栈高增加 $+1$。\n- 遇到第一个 $b$ 时，转换到出栈阶段，此后对每个 $b$ 执行出栈操作，使栈高减少 $-1$。\n- 在见过任何 $b$ 之后，不能再出现 $a$。\n- 在任何前缀处，栈高都不得为负。\n- 接受条件要求在输入结束时栈高恰好为 $0$。\n\n为了用一个双层堆叠 LSTM 模拟这些行为，我们需要满足以下从 LSTM 方程推导出的原则性条件。\n\n第 1 层条件（特征提取和阶段检测）：\n- 该层必须输出信号，以编码当前符号的身份以及到目前为止是否遇到过任何 $b$。设单元状态 $c_t^{(1)} \\in \\mathbb{R}^3$ 的结构如下：\n  - $c_{t,0}^{(1)}$ 在时间 $t$ 跟踪 $a$ 的直通指示符，\n  - $c_{t,1}^{(1)}$ 在时间 $t$ 跟踪 $b$ 的直通指示符，\n  - $c_{t,2}^{(1)}$ 累加到目前为止见过的 $b$ 符号的数量。\n- 为实现此目的，选择使用 $\\sigma$ 函数饱和到接近 $0$ 或 $1$ 的门，以便：\n  - 对于维度 $0$（$a$ 指示符），设置 $f_{t,0}^{(1)} \\approx 0$ 以在每一步重置，设置 $i_{t,0}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ 是 } a\\}$ 和 $\\tilde{c}_{t,0}^{(1)} \\approx 1$，从而得到 $c_{t,0}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ 是 } a\\}$。此处 $\\mathbb{I}\\{\\cdot\\}$ 表示指示函数。这与 LSTM 的更新是一致的，因为当 $x_t$ 是 $a$ 时，$c_{t,0}^{(1)} = f_{t,0}^{(1)} c_{t-1,0}^{(1)} + i_{t,0}^{(1)} \\tilde{c}_{t,0}^{(1)} \\approx 0 + 1 \\cdot 1$，而当 $x_t$ 是 $b$ 时，结果 $\\approx 0$。\n  - 对于维度 $1$（$b$ 指示符），设置 $f_{t,1}^{(1)} \\approx 0$、$i_{t,1}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ 是 } b\\}$ 和 $\\tilde{c}_{t,1}^{(1)} \\approx 1$，得到 $c_{t,1}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ 是 } b\\}$。\n  - 对于维度 $2$（$b$ 阶段累加器），设置 $f_{t,2}^{(1)} \\approx 1$ 以保留记忆，设置 $i_{t,2}^{(1)} \\approx \\mathbb{I}\\{x_t \\text{ 是 } b\\}$ 和 $\\tilde{c}_{t,2}^{(1)} \\approx 1$，从而产生 $c_{t,2}^{(1)} \\approx c_{t-1,2}^{(1)} + \\mathbb{I}\\{x_t \\text{ 是 } b\\}$，即一个关于 $b$ 观测次数的非递减计数器。当 $o_{t}^{(1)} \\approx 1$ 时，我们有 $h_t^{(1)} \\approx \\tanh(c_t^{(1)})$，其第三个分量在见到任何 $b$ 之前接近 $0$，一旦见到 $b$ 符号后便向 $1$ 增加。\n- 通过选择合适的 $W_i$、$U_i$ 和 $b_i$ 以在期望情况下产生大的正预激活值，在其他情况下产生大的负预激活值，从而确保饱和，这些门的行是可以通过 LSTM 方程实现的。同时，$W_c$、$U_c$ 和 $b_c$ 将指示符维度的 $\\tilde{c}_t$ 设置为近似常数值，并将累加器的 $\\tilde{c}_t$ 设置为 $1$。\n\n第 2 层条件（模拟栈高的计数器管理）：\n- 第二层在其单元状态 $c_t^{(2)} \\in \\mathbb{R}$ 中维护一个代表栈高的标量计数器。它必须：\n  - 在 $b$ 阶段尚未开始时，对 $a$ 增加 $+1$，\n  - 在任何时候都对 $b$ 减少 $-1$，\n  - 在 $b$ 阶段开始后，对 $a$ 保持不变（因为这是无效情况），同时升起一个外部错误标志以阻止接受。\n- 设 $\\phi_t \\in \\{0,1\\}$ 是从第一层导出的阶段指示符，其中如果 $c_{t,2}^{(1)}$ 接近 $0$，则 $\\phi_t \\approx 0$；一旦见过任何 $b$，则 $\\phi_t \\approx 1$。然后选择门，使得 $f_t^{(2)} \\approx 1$，$i_t^{(2)} \\approx 1$，并将 $\\tilde{c}_t^{(2)}$ 设计为\n  $$ \\tilde{c}_t^{(2)} \\approx (1 - \\phi_t) \\cdot \\mathbb{I}\\{x_t \\text{ is } a\\} - \\mathbb{I}\\{x_t \\text{ is } b\\}, $$\n  这会得到\n  $$ c_t^{(2)} \\approx c_{t-1}^{(2)} + \\tilde{c}_t^{(2)}. $$\n  这与 LSTM 的动态特性是一致的，因为 $f_t^{(2)} \\approx 1$ 保留了前一个计数器的值，而 $i_t^{(2)} \\approx 1$ 则加上了有符号的更新量。使用 $\\tanh$ 确保 $\\tilde{c}_t^{(2)} \\in [-1,1]$，与 $+1$ 的增量和 $-1$ 的减量相符。\n- 错误检测条件：\n  - 如果 $\\phi_t \\approx 1$ 且 $x_t$ 是 $a$，则因 $a$ 出现在 $b$ 之后而标记为无效。\n  - 如果 $c_t^{(2)}$ 曾降至 $0$ 以下，则因下溢（PDA 试图从空栈中弹出）而标记为无效。\n- 接受条件：\n  $$ \\text{接受} \\iff \\left(|c_T^{(2)}| \\le \\epsilon\\right) \\land \\left(\\text{没有在 } b \\text{ 之后出现无效的 } a\\right) \\land \\left(\\text{没有发生下溢}\\right), $$\n  其中 $\\epsilon$ 是一个小的正容差（例如，$\\epsilon = 10^{-3}$），用于解释由门饱和引起的任何微小数值偏差。\n\n与这些原则一致的算法设计：\n- 用 $a \\mapsto [1,0]$ 和 $b \\mapsto [0,1]$ 对输入进行编码。\n- 实现第一层，包含三个单元维度，如前所述，使用具有大增益的 sigmoid 函数来近似指示符。使用 $f_{t,0}^{(1)} = 0$、$i_{t,0}^{(1)} \\approx \\sigma(K(x_{a,t} - 0.5))$、$\\tilde{c}_{t,0}^{(1)} = 1$；对于维度 $1$ 中的 $b$ 也类似；对于维度 $2$，使用 $f_{t,2}^{(1)} = 1$、$i_{t,2}^{(1)} \\approx \\sigma(K(x_{b,t} - 0.5))$、$\\tilde{c}_{t,2}^{(1)} = 1$，其中 $K$ 是一个大数（例如，$K = 20$）。\n- 实现第二层计数器，设置 $f_t^{(2)} = 1$、$i_t^{(2)} = 1$，并将 $\\tilde{c}_t^{(2)}$ 设置为 $(1 - \\phi_t)\\cdot \\mathbb{I}\\{x_t \\text{ 是 } a\\} - \\mathbb{I}\\{x_t \\text{ 是 } b\\}$，其中 $\\phi_t$ 通过对 $c_{t,2}^{(1)}$ 进行阈值判断来确定（例如，如果 $c_{t,2}^{(1)}  0.5$，则 $\\phi_t = 1$，否则为 $0$）。\n- 跟踪计数器 $c_t^{(2)}$、下溢条件 $c_t^{(2)}  0$ 以及无效的 $a$ 在 $b$ 之后的条件。\n\n正确性论证：\n- 在出现任何 $b$ 之前，$\\phi_t \\approx 0$，并且 $\\tilde{c}_t^{(2)} \\approx \\mathbb{I}\\{x_t \\text{ 是 } a\\} - 0$，因此计数器每个 $a$ 增加 $+1$。在观察到 $b$ 之后，$\\phi_t \\approx 1$，并且 $\\tilde{c}_t^{(2)} \\approx 0 - \\mathbb{I}\\{x_t \\text{ 是 } b\\}$，因此每个 $b$ 减少 $-1$。这精确地模拟了 PDA 对 $a^n b^n$ 的入栈/出栈计数。\n- 包含在 $b$ 之后出现 $a$ 的无效序列不会增加计数器，并被标记为无效；包含过多 $b$ 的序列会导致计数器变为负数，从而触发下溢。接受条件要求计数器在没有任何违规的情况下返回到 $0$，这与 PDA 的接受语义相匹配。\n- 第 1 层的门将角色分离为指示符直通和单调阶段检测，而第 2 层仅专注于计数器更新，这提供了堆叠架构中典型的必要关注点分离。使用饱和门来近似 $0$ 和 $1$ 确保了更新在时间上是鲁棒和一致的。\n\n测试套件覆盖范围基本原理：\n- 空字符串 $\"\"$ 测试边界情况 $n = 0$。\n- $\"ab\"$ 测试最小的非空有效情况。\n- $\"aaabbb\"$ 和 $\"aaaabbbb\"$ 测试具有多次增加和减少的常规有效情况。\n- $\"aabbb\"$ 和 $\"aaabb\"$ 测试计数不匹配以及下溢与非零最终计数器的情况。\n- $\"aba\"$ 测试在 $b$ 之后出现无效 $a$ 的情况。\n- $\"b\"$ 测试在空栈上以出栈开始，导致下溢的情况。\n\n最终程序实现了此设计，并按指定顺序输出单行布尔值 $[r_1,r_2,\\dots,r_8]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sigma(z):\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef tanh(z):\n    return np.tanh(z)\n\ndef accept_sequence(seq, K=20.0, eps=1e-3):\n    \"\"\"\n    Simulate a two-layer stacked LSTM designed to emulate a PDA for L = {a^n b^n}.\n    Layer 1 (size 3):\n      - c1[0]: indicator-like pass-through for 'a' at time t\n      - c1[1]: indicator-like pass-through for 'b' at time t\n      - c1[2]: monotone accumulator of whether any 'b' has been seen (counts b's)\n    Layer 2 (size 1):\n      - c2: counter of stack height, +1 for 'a' before any 'b', -1 for 'b'\n    Acceptance: final c2 ~ 0, no 'a' after 'b', and no underflow (c2  0 at any prefix).\n    \"\"\"\n    # Initialize states for layer 1\n    c1 = np.zeros(3, dtype=float)\n    h1 = np.zeros(3, dtype=float)\n    # Initialize state for layer 2 counter\n    c2 = 0.0\n\n    invalid_a_after_b = False\n    underflow = False\n\n    for ch in seq:\n        # One-hot encoding for input at time t\n        if ch == 'a':\n            xa, xb = 1.0, 0.0\n        elif ch == 'b':\n            xa, xb = 0.0, 1.0\n        else:\n            # Invalid character: reject immediately\n            return False\n\n        # Layer 1 gate computations and updates\n        # Dimension 0: 'a' indicator-like pass-through\n        i0 = sigma(K * (xa - 0.5))  # ~1 if 'a', ~0 if 'b'\n        f0 = 0.0\n        g0 = 1.0\n        c1[0] = f0 * c1[0] + i0 * g0\n        h1[0] = tanh(c1[0])  # output gate ~1\n\n        # Dimension 1: 'b' indicator-like pass-through\n        i1 = sigma(K * (xb - 0.5))  # ~1 if 'b', ~0 if 'a'\n        f1 = 0.0\n        g1 = 1.0\n        c1[1] = f1 * c1[1] + i1 * g1\n        h1[1] = tanh(c1[1])  # output gate ~1\n\n        # Dimension 2: accumulator of b's (phase detector)\n        i2 = sigma(K * (xb - 0.5))  # ~1 on 'b', ~0 on 'a'\n        f2 = 1.0\n        g2 = 1.0\n        c1[2] = f2 * c1[2] + i2 * g2\n        h1[2] = tanh(c1[2])  # increases toward 1 after any 'b'\n\n        # Phase indicator: whether any 'b' has been seen (threshold on c1[2])\n        b_phase = c1[2] > 0.5\n\n        # Detect invalid 'a' after 'b'\n        if xa > 0.5 and b_phase:\n            invalid_a_after_b = True\n\n        # Layer 2 counter update: +1 for 'a' before any 'b', -1 for 'b'\n        inc = 1 if (xa > 0.5 and not b_phase) else 0\n        dec = -1 if xb > 0.5 else 0\n        # LSTM-like update with f=1, i=1, candidate = inc + dec\n        c2 = c2 + (inc + dec)\n\n        # Underflow check: counter must never go negative\n        if c2  -eps:\n            underflow = True\n\n    # Acceptance: final counter ~ 0, and no invalid events\n    accept = (abs(c2) = eps) and (not invalid_a_after_b) and (not underflow)\n    return accept\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        \"\",           # empty string\n        \"ab\",         # minimal valid\n        \"aaabbb\",     # valid, n=3\n        \"aabbb\",      # invalid: too many b\n        \"aaabb\",      # invalid: too many a\n        \"aba\",        # invalid: a after b\n        \"b\",          # invalid: starts with b, underflow\n        \"aaaabbbb\"    # valid, n=4\n    ]\n\n    results = []\n    for seq in test_cases:\n        result = accept_sequence(seq)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3175992"}, {"introduction": "在现代深度学习中，RNN经常与注意力机制结合使用，以创建更强大的混合模型。这个练习旨在探究注意力机制如何利用堆叠RNN学习到的层次化表示 ([@problem_id:3175991])。你将通过一个数值实验，分析当注意力机制的“键”（key）和“值”（value）取自RNN不同深度层时，模型的“有效感受野”如何变化。这项实践为你提供了分析和设计前沿序列模型的宝贵经验，帮助你理解不同组件如何协同工作以处理序列数据。", "problem": "本任务要求您设计并实现一个确定性数值实验，以分离研究在堆叠循环神经网络 (RNN) 的不同深度附加一个单头、因果、缩放点积注意力机制如何改变序列上时间延迟的有效感受野。请从第一性原理出发：仅使用堆叠循环神经网络和缩放点积注意力的核心定义，并推导出当查询取自顶层、键/值取自指定层时，由注意力机制引起的注意力跨度分布。\n\n首先，从以下经过充分检验的定义和事实开始：\n\n- 循环神经网络 (RNN) 由隐藏状态递推指定。对于层索引 $l \\in \\{1,\\dots,L\\}$ 和时间 $t \\in \\{0,\\dots,T-1\\}$，隐藏状态 $h_t^{(l)} \\in \\mathbb{R}^{d_h^{(l)}}$ 定义为\n  $$h_t^{(1)} = \\tanh\\!\\left(U^{(1)} x_t + R^{(1)} h_{t-1}^{(1)} + b^{(1)}\\right), \\quad h_{-1}^{(1)} = 0,$$\n  $$h_t^{(l)} = \\tanh\\!\\left(U^{(l)} h_t^{(l-1)} + R^{(l)} h_{t-1}^{(l)} + b^{(l)}\\right), \\quad h_{-1}^{(l)} = 0 \\quad \\text{for } l \\ge 2,$$\n  其中，$x_t \\in \\mathbb{R}^{d_x}$ 是在时间 $t$ 的输入，$U^{(l)} \\in \\mathbb{R}^{d_h^{(l)} \\times d_{in}^{(l)}}$，$R^{(l)} \\in \\mathbb{R}^{d_h^{(l)} \\times d_h^{(l)}}$，$b^{(l)} \\in \\mathbb{R}^{d_h^{(l)}}$，$d_{in}^{(1)} = d_x$，$l \\ge 2$ 时 $d_{in}^{(l)} = d_h^{(l-1)}$。非线性函数为双曲正切函数。\n\n- 对于查询维度为 $d_k$ 的单头缩放点积注意力，其定义如下。给定查询 $q_t \\in \\mathbb{R}^{d_k}$、键 $k_s \\in \\mathbb{R}^{d_k}$ 以及一个只允许关注索引 $s \\le t$ 的因果掩码，注意力权重为\n  $$\\alpha_{t,s} = \\frac{\\exp\\!\\left(\\frac{q_t^\\top k_s}{\\sqrt{d_k}}\\right)}{\\sum_{u=0}^t \\exp\\!\\left(\\frac{q_t^\\top k_u}{\\sqrt{d_k}}\\right)} \\quad \\text{for } s \\in \\{0,\\dots,t\\}.$$\n  查询和键通过线性投影 $q_t = W_Q h_t^{(L)}$ 和 $k_s = W_K h_s^{(\\ell)}$ 形成，其中 $W_Q \\in \\mathbb{R}^{d_k \\times d_h^{(L)}}$ 和 $W_K \\in \\mathbb{R}^{d_k \\times d_h^{(\\ell)}}$，$\\ell \\in \\{1,\\dots,L\\}$ 表示提供键（和值，尽管此处未使用值）的层。\n\n定义在非负时间延迟 $d \\in \\{0,1,\\dots,T-1\\}$ 上的注意力跨度分布，该分布通过在所有时间步上均匀聚合分配给每个延迟的注意力权重来得到：\n$$p(d) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\sum_{s=0}^{t} \\alpha_{t,s} \\, \\mathbf{1}\\{t-s = d\\}.$$\n根据构造，$\\sum_{d=0}^{T-1} p(d) = 1$。这个 $p(d)$ 量化了纯粹由注意力引起的、在相对位置上的有效感受野，其条件是哪个内部层提供键。\n\n您的任务是：\n\n- 实现一个如上所述的具有 $L$ 层的堆叠 RNN。\n- 生成输入 $x_t$，作为独立同分布的、均值为零、方差为一的高斯向量，即 $x_t$ 的每个分量都从 $\\mathcal{N}(0,1)$ 中采样。\n- 对于指定的提供键的层索引 $\\ell$，通过 $q_t = W_Q h_t^{(L)}$ 和 $k_s = W_K h_s^{(\\ell)}$ 构建查询和键，然后计算所有 $t$ 和 $s \\le t$ 的因果注意力权重 $\\alpha_{t,s}$。\n- 计算上面定义的注意力跨度分布 $p(d)$。\n- 所有权重矩阵必须从独立的、均值为零的高斯分布中初始化，并采用层级适用的缩放以避免轻易饱和。使用 $U^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_{in}^{(l)})$，$R^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_h^{(l)})$，$W_Q \\sim \\mathcal{N}(0, 1/d_h^{(L)})$，$W_K \\sim \\mathcal{N}(0, 1/d_h^{(\\ell)})$，且 $b^{(l)} = 0$。所有随机抽取必须通过指定的伪随机数种子来保证确定性。\n\n为以下测试套件计算 $p(d)$。每个测试用例指定 $L$、输入维度 $d_x$、隐藏维度 $\\{d_h^{(l)}\\}_{l=1}^L$、查询/键维度 $d_k$、序列长度 $T$、提供键的层索引 $\\ell$ 以及两个种子：一个用于模型参数，一个用于输入序列。当两个用例共享相同的种子和维度，仅在 $\\ell$ 上有所不同时，除了提供键的层的差异外，其模型和输入必须完全相同。\n\n- 测试用例 1（正常路径，双层基线）：\n  - $L = 2$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}) = (8, 8)$，$d_k = 6$，$T = 20$，$\\ell = 1$，$\\text{model\\_seed} = 2025$，$\\text{input\\_seed} = 100$。\n- 测试用例 2（与用例 1 模型和输入相同，但键来自顶层）：\n  - $L = 2$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}) = (8, 8)$，$d_k = 6$，$T = 20$，$\\ell = 2$，$\\text{model\\_seed} = 2025$，$\\text{input\\_seed} = 100$。\n- 测试用例 3（更深的网络栈，键来自最深层）：\n  - $L = 3$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}, d_h^{(3)}) = (8, 6, 10)$，$d_k = 6$，$T = 30$，$\\ell = 3$，$\\text{model\\_seed} = 777$，$\\text{input\\_seed} = 555$。\n- 测试用例 4（与用例 3 模型和输入相同，但键来自第一层）：\n  - $L = 3$，$d_x = 5$，$(d_h^{(1)}, d_h^{(2)}, d_h^{(3)}) = (8, 6, 10)$，$d_k = 6$，$T = 30$，$\\ell = 1$，$\\text{model\\_seed} = 777$，$\\text{input\\_seed} = 555$。\n- 测试用例 5（边界情况：最小序列长度）：\n  - $L = 2$，$d_x = 4$，$(d_h^{(1)}, d_h^{(2)}) = (6, 6)$，$d_k = 4$，$T = 1$，$\\ell = 2$，$\\text{model\\_seed} = 9$，$\\text{input\\_seed} = 9$。\n\n实现和数值要求：\n\n- 使用双曲正切非线性函数和将注意力限制在索引 $s \\le t$ 的因果掩码。\n- 严格按照规定使用缩放点积因子 $1/\\sqrt{d_k}$。\n- 为了 Softmax 中的数值稳定性，在进行指数运算前，减去允许窗口内的最大分值。\n- 将每个输出分布 $p(d)$ 的每个元素四舍五入到 6 位小数。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个元素对应一个测试用例，并且其本身必须是该用例的四舍五入后的分布值列表 $[p(0), p(1), \\dots, p(T-1)]$。例如，整体结构必须是\n  $$\\text{[ [p\\_case1(0),...,p\\_case1(T\\_1-1)], [p\\_case2(0),...,p\\_case2(T\\_2-1)], \\dots ]}$$\n  的形式，前后不带任何额外文本。\n\n此问题不涉及任何物理单位、角度或百分比。所有量都是无量纲的实数。请通过使用上述精确定义来确保科学真实性，不要引入任何辅助的启发式方法或近似。", "solution": "该问题要求设计并实现一个数值实验，以确定由集成在堆叠循环神经网络 (RNN) 中的单头因果注意力机制所产生的注意力跨度分布 $p(d)$。该分布量化了时间延迟 $d$ 上的有效感受野，并且其形状预计会根据 RNN 的哪一层 $\\ell$ 为注意力机制提供键和值而改变。整个过程必须是确定性的，由指定的伪随机数种子控制。\n\n解决方案按照问题陈述中阐述的原则，分四个主要阶段实施。\n\n### 1. 模型和数据初始化\n\n第一步是构建计算模型和输入数据。此过程由 `model_seed` 和 `input_seed` 控制，以确保可复现性。\n\n**模型参数**：堆叠 RNN 有 $L$ 层，参数为 $\\{U^{(l)}, R^{(l)}, b^{(l)}\\}_{l=1}^L$。注意力机制的参数为 $W_Q$ 和 $W_K$。所有偏置向量 $b^{(l)}$ 都初始化为零。权重矩阵通过从零均值高斯分布中抽样来初始化，其方差按扇入(fan-in)进行缩放，这是一种促进稳定信号传播的标准技术。\n- 对于 RNN 层，权重初始化如下：\n  $$U^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_{in}^{(l)}), \\quad R^{(l)}_{ij} \\sim \\mathcal{N}(0, 1/d_h^{(l)})$$\n  其中 $d_{in}^{(1)} = d_x$（输入维度），$l \\ge 2$ 时 $d_{in}^{(l)} = d_h^{(l-1)}$。$d_h^{(l)}$ 是第 $l$ 层的隐藏状态维度。\n- 对于注意力机制，查询和键的投影矩阵初始化如下：\n  $$W_Q \\sim \\mathcal{N}(0, 1/d_h^{(L)}), \\quad W_K \\sim \\mathcal{N}(0, 1/d_h^{(\\ell)})$$\n  其中 $L$ 是总层数，$\\ell$ 是提供键的层的索引。$W_K$ 的初始化明确依赖于所选源层 $\\ell$ 的维度。对于共享 `model_seed` 但 $\\ell$ 不同的测试用例，共享参数（$U^{(l)}, R^{(l)}, W_Q$）的随机抽样序列是相同的，这确保了底层模型除了功能上依赖于 $\\ell$ 的组件（$W_K$）之外都是相同的。\n\n**输入数据**：输入序列 $\\{x_t\\}_{t=0}^{T-1}$ 由 $T$ 个向量组成，每个向量的维度为 $d_x$。每个向量 $x_t$ 的每个分量都使用以 `input_seed` 为种子的随机数生成器从标准正态分布 $x_{t,i} \\sim \\mathcal{N}(0, 1)$ 中独立抽取。\n\n### 2. 堆叠 RNN 前向传播\n\n在模型和数据初始化后，下一步是计算所有层 $l \\in \\{1, \\dots, L\\}$ 和所有时间步 $t \\in \\{0, \\dots, T-1\\}$ 的隐藏状态 $h_t^{(l)}$。这是通过沿时间展开网络来完成的。隐藏状态初始化为零，即对所有 $l$ 都有 $h_{-1}^{(l)} = 0$。\n\n计算按每个时间步 $t$ 顺序进行：\n- 对于第一层 ($l=1$)，隐藏状态根据当前输入 $x_t$ 和同一层的前一个隐藏状态 $h_{t-1}^{(1)}$ 计算：\n  $$h_t^{(1)} = \\tanh(U^{(1)} x_t + R^{(1)} h_{t-1}^{(1)})$$\n- 对于所有后续层 ($l \\ge 2$)，隐藏状态根据当前时间步下一层的隐藏状态 $h_t^{(l-1)}$ 和同一层的前一个隐藏状态 $h_{t-1}^{(l)}$ 计算：\n  $$h_t^{(l)} = \\tanh(U^{(l)} h_t^{(l-1)} + R^{(l)} h_{t-1}^{(l)})$$\n这个过程从 $t=0$ 到 $T-1$ 迭代，并且对于每个 $t$，从 $l=1$ 到 $L$ 迭代。所有计算出的隐藏状态 $\\{h_t^{(l)}\\}$ 都被存储起来，以供下一阶段使用。\n\n### 3. 因果注意力权重计算\n\n注意力机制计算一组权重 $\\alpha_{t,s}$，这些权重指定了时间 $t$ 的输出对来自时间 $s$ 的输入的“关注”程度。\n\n- **查询和键**：查询由 RNN 的最后一层 $h_t^{(L)}$ 生成，而键由来自由指定的层 $\\ell$ 的 $h_s^{(\\ell)}$ 生成。这是实验的核心，因为改变 $\\ell$ 会改变注意力机制的信息来源。\n  $$q_t = W_Q h_t^{(L)}, \\quad k_s = W_K h_s^{(\\ell)}$$\n- **注意力分数**：对于每个查询 $q_t$，都会根据所有满足 $s \\le t$ 的键 $k_s$ 计算分数，这尊重了时间序列数据的因果结构。分数使用缩放点积计算：\n  $$\\text{score}(t,s) = \\frac{q_t^\\top k_s}{\\sqrt{d_k}}$$\n  其中 $d_k$ 是查询和键的维度。\n- **Softmax 归一化**：使用 Softmax 函数将分数转换为概率分布（即注意力权重 $\\alpha_{t,s}$），该函数应用于所有允许的源位置 $u \\in \\{0, \\dots, t\\}$：\n  $$\\alpha_{t,s} = \\frac{\\exp(\\text{score}(t,s))}{\\sum_{u=0}^t \\exp(\\text{score}(t,u))}$$\n  为了数值稳定性，在进行指数运算之前，会从所有分数中减去给定 $t$ 的最大分数 $\\max_{u \\le t} \\{\\text{score}(t,u)\\}$。这可以防止浮点数溢出，同时不改变最终的分布。此计算对每个时间步 $t \\in \\{0, \\dots, T-1\\}$ 执行。\n\n### 4. 注意力跨度分布计算\n\n最后一步是将计算出的注意力权重 $\\alpha_{t,s}$ 聚合成注意力跨度分布 $p(d)$。该分布衡量了分配给特定时间延迟 $d = t-s$ 的平均注意力权重。\n\n该分布定义为：\n$$p(d) = \\frac{1}{T} \\sum_{t=0}^{T-1} \\sum_{s=0}^{t} \\alpha_{t,s} \\, \\mathbf{1}\\{t-s = d\\}$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。该公式可以重写为对每个延迟 $d$ 的贡献求平均值：\n$$p(d) = \\frac{1}{T} \\sum_{t=d}^{T-1} \\alpha_{t, t-d}$$\n在计算上，这等同于对 $T \\times T$ 注意力矩阵（其中 $\\alpha_{t,s}$ 是其元素）的第 $d$ 条次对角线上的元素求和，然后除以 $T$。得到的向量 $p = [p(0), p(1), \\dots, p(T-1)]$ 是一个概率分布。然后，该向量的每个元素都按要求四舍五入到 6 位小数。通过对每个测试用例执行这整个过程，我们获得了所需的分布集。", "answer": "```python\nimport numpy as np\n\ndef compute_dist_for_case(L, d_x, d_h, d_k, T, ell, model_seed, input_seed):\n    \"\"\"\n    Computes the attention span distribution for a single test case.\n    \"\"\"\n    # 1. Initialize RNGs and Model Parameters\n    model_rng = np.random.default_rng(model_seed)\n    \n    U_mats, R_mats = [], []\n    d_in_l = d_x\n    for l in range(L):\n        d_h_l = d_h[l]\n        \n        # U^(l) matrix\n        U_std = (1 / d_in_l)**0.5\n        U_mats.append(model_rng.normal(loc=0.0, scale=U_std, size=(d_h_l, d_in_l)))\n        \n        # R^(l) matrix\n        R_std = (1 / d_h_l)**0.5\n        R_mats.append(model_rng.normal(loc=0.0, scale=R_std, size=(d_h_l, d_h_l)))\n        \n        d_in_l = d_h_l\n    \n    # W_Q matrix for queries\n    d_h_L = d_h[L - 1]\n    W_Q_std = (1 / d_h_L)**0.5\n    W_Q = model_rng.normal(loc=0.0, scale=W_Q_std, size=(d_k, d_h_L))\n    \n    # W_K matrix for keys (ell is 1-indexed, convert to 0-indexed)\n    d_h_ell = d_h[ell - 1]\n    W_K_std = (1 / d_h_ell)**0.5\n    W_K = model_rng.normal(loc=0.0, scale=W_K_std, size=(d_k, d_h_ell))\n\n    # Initialize Input Data\n    input_rng = np.random.default_rng(input_seed)\n    X = input_rng.normal(loc=0.0, scale=1.0, size=(T, d_x))\n    \n    if T == 0:\n        return []\n\n    # 2. Stacked RNN Forward Pass\n    # Store hidden states h_t^(l) for t in -1..T-1\n    # H[l][t+1] stores h_t^(l)\n    H = [np.zeros((T + 1, d)) for d in d_h]\n    \n    for t in range(T):\n        t_idx = t + 1 # Index for current time step in H\n        \n        # Layer l=1 (0-indexed)\n        h_tm1_1 = H[0][t_idx - 1]\n        h_t_1 = np.tanh(U_mats[0] @ X[t] + R_mats[0] @ h_tm1_1)\n        H[0][t_idx] = h_t_1\n        \n        # Layers l > 1\n        for l in range(1, L):\n            h_t_lm1 = H[l - 1][t_idx]\n            h_tm1_l = H[l][t_idx - 1]\n            h_t_l = np.tanh(U_mats[l] @ h_t_lm1 + R_mats[l] @ h_tm1_l)\n            H[l][t_idx] = h_t_l\n    \n    # Extract hidden states for t=0..T-1\n    H_L_states = H[L - 1][1:]\n    H_ell_states = H[ell - 1][1:]\n\n    # 3. Attention Weight Calculation\n    Q = H_L_states @ W_Q.T  # Shape (T, d_k)\n    K = H_ell_states @ W_K.T  # Shape (T, d_k)\n    \n    alpha = np.zeros((T, T))\n    sqrt_dk = np.sqrt(d_k)\n    \n    for t in range(T):\n        q_t = Q[t]\n        # Causal keys: s = t\n        K_causal = K[0:t + 1]\n        \n        scores = (q_t @ K_causal.T) / sqrt_dk\n        \n        # Numerically stable softmax\n        stable_scores = scores - np.max(scores)\n        exp_scores = np.exp(stable_scores)\n        attention_weights = exp_scores / np.sum(exp_scores)\n        \n        alpha[t, 0:t + 1] = attention_weights\n\n    # 4. Attention Span Distribution\n    p = np.zeros(T)\n    for d in range(T):\n        # Summing alpha_{t, t-d} for t from d to T-1\n        # This is the -d'th diagonal of the alpha matrix\n        p[d] = np.sum(np.diag(alpha, k=-d))\n    \n    p /= T\n    \n    return np.round(p, 6).tolist()\n\ndef solve():\n    test_cases = [\n        { \"L\": 2, \"d_x\": 5, \"d_h\": (8, 8), \"d_k\": 6, \"T\": 20, \"ell\": 1, \"model_seed\": 2025, \"input_seed\": 100 },\n        { \"L\": 2, \"d_x\": 5, \"d_h\": (8, 8), \"d_k\": 6, \"T\": 20, \"ell\": 2, \"model_seed\": 2025, \"input_seed\": 100 },\n        { \"L\": 3, \"d_x\": 5, \"d_h\": (8, 6, 10), \"d_k\": 6, \"T\": 30, \"ell\": 3, \"model_seed\": 777, \"input_seed\": 555 },\n        { \"L\": 3, \"d_x\": 5, \"d_h\": (8, 6, 10), \"d_k\": 6, \"T\": 30, \"ell\": 1, \"model_seed\": 777, \"input_seed\": 555 },\n        { \"L\": 2, \"d_x\": 4, \"d_h\": (6, 6), \"d_k\": 4, \"T\": 1, \"ell\": 2, \"model_seed\": 9, \"input_seed\": 9 },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_dist = compute_dist_for_case(**case)\n        results.append(p_dist)\n\n    # Format output as a list of lists string representation\n    output_str = \"[\" + \",\".join(str(res).replace(\" \", \"\") for res in results) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3175991"}]}