{"hands_on_practices": [{"introduction": "在我们依赖像随时间反向传播（BPTT）这样的高效算法之前，理解它所计算的梯度的完整数学结构至关重要。本练习将引导你为一个简单的线性循环神经网络（RNN）推导梯度的闭式解，从而揭示其作为跨时间相关项之和的内在构成 [@problem_id:3192146]。这个过程不仅能阐明BPTT的计算目标，还能通过复杂度分析，让你深刻理解为何对于长序列而言，BPTT的算法效率是不可或缺的。", "problem": "考虑一个线性循环神经网络（RNN），其隐藏状态维度为 $d_h$，输入维度为 $d_x$，输出维度为 $d_y$，由单个长度为 $T$ 的序列驱动。其递归和读出由以下核心定义给出：\n$h_t = W_h h_{t-1} + W_x x_t$，其中 $h_0 = 0$，以及 $y_t = W_y h_t$，\n对于 $t \\in \\{1,2,\\dots,T\\}$，其中 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$，$W_x \\in \\mathbb{R}^{d_h \\times d_x}$，以及 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入向量为 $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$，目标为 $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$。考虑二次损失函数：\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$。\n令每个时间步 $t$ 的输出残差为 $e_t = y_t - s_t$。请仅使用递归定义、读出定义、多变量微积分中的链式法则以及标准的线性代数知识，推导梯度 $\\nabla_{W_h} L$ 的闭式解析表达式，该表达式需显式地将其分解为形如 $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$ 的时移相关项之和。然后，简要讨论在实现时评估此梯度的计算复杂度（使用大$O$表示法）：(i) 通过对时移项求和，直接根据您的闭式表达式进行计算；(ii) 通过使用伴随信号的一阶递归的 BPTT (Backpropagation Through Time, 随时间反向传播) 算法进行计算。您的最终答案必须是关于 $\\nabla_{W_h} L$ 的单一闭式解析矩阵表达式，且仅用 $W_h$、$W_y$、$\\{e_t\\}$ 和 $\\{h_t\\}$ 表示。请勿包含任何数值计算。", "solution": "该问题被评估为有效。这是一个来自深度学习理论领域的、适定的、有科学依据的、客观的问题陈述。它要求推导标准线性循环神经网络（RNN）的梯度，这是一个基于多变量微积分和线性代数的既定原则的形式化且可解的任务。所有必要的定义和条件均已提供。\n\n我们首先陈述问题中提供的核心定义：\n递归关系：$h_t = W_h h_{t-1} + W_x x_t$，对于 $t \\in \\{1, 2, \\dots, T\\}$，初始状态 $h_0 = 0$。\n读出方程：$y_t = W_y h_t$。\n损失函数：$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$，其中 $e_t = y_t - s_t$。\n参数是矩阵 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$、$W_x \\in \\mathbb{R}^{d_h \\times d_x}$ 和 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入是向量 $x_t \\in \\mathbb{R}^{d_x}$，目标是 $s_t \\in \\mathbb{R}^{d_y}$。\n\n我们的目标是推导损失函数 $L$ 相对于循环权重矩阵 $W_h$ 的梯度 $\\nabla_{W_h} L$ 的闭式表达式。我们将使用矩阵微积分的链式法则。\n\n损失 $L$ 是 $W_h$ 的函数，通过其对隐藏状态 $\\{h_t\\}_{t=1}^T$ 的影响。梯度可以表示为前向传播中每个使用 $W_h$ 的时间步的贡献之和：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\n其中，“局部”梯度指的是在计算 $h_t = W_h h_{t-1} + W_x x_t$ 中使用的 $W_h$ 实例的梯度。使用链式法则，该贡献由损失相对于状态 $h_t$ 的梯度与状态更新项相对于 $W_h$ 的梯度的外积给出。令 $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ 为总损失相对于隐藏状态 $h_t$ 的梯度。预激活 $a_t = W_h h_{t-1} + W_x x_t$ 相对于 $W_h$ 的梯度是 $h_{t-1}^T$。因此，时间步 $t$ 对梯度的贡献是 $\\delta_t h_{t-1}^T$。对所有时间步求和得到总梯度：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\n这是 BPTT 中循环权重梯度的标准公式。然而，问题要求一个用给定数量 $W_h, W_y, \\{e_t\\}, \\{h_t\\}$ 表示的闭式表达式。为此，我们必须推导 $\\delta_t$ 的闭式形式。\n\n向量 $\\delta_t$ 表示 $h_t$ 对损失 $L$ 的总影响。状态 $h_t$ 通过所有后续时间步 $k \\ge t$ 的输出 $y_k$ 来影响损失。\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\n这里，我们应用链式法则，对从 $h_t$ 到最终损失的所有路径进行求和。偏导数如下（使用分子布局的矩阵微积分约定，其中标量对列向量的梯度是列向量）：\n1. $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$。这是一个大小为 $d_y$ 的列向量。\n2. $\\frac{\\partial y_k}{\\partial h_k} = W_y$。这是大小为 $d_y \\times d_h$ 的雅可比矩阵。链式法则中的项是它的转置，$(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$。\n3. $\\frac{\\partial h_k}{\\partial h_t}$。我们通过展开递归关系来找到它：\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\n当 $k  t$ 时，$h_k$ 对 $h_t$ 的依赖关系由下式给出：\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$。\n因此，当 $k \\ge t$ 时，雅可比矩阵为 $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$。链式法则组合中的项是它的转置，$(W_h^{k-t})^T = (W_h^T)^{k-t}$。\n\n将这些导数代回 $\\delta_t$ 的表达式中：\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\n这为从所有未来时间步 $k \\ge t$ 反向传播的梯度信号 $\\delta_t$ 提供了闭式表达式。\n\n最后，我们将 $\\delta_t$ 的这个表达式代入总梯度 $\\nabla_{W_h} L$ 的方程中：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\n这就是所要求的闭式解析表达式。它将梯度分解为关于时间的双重求和。每一项 $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ 都可以解释为未来时间 $k$ 的误差 $e_k$ 与过去时间 $t-1$ 的隐藏状态 $h_{t-1}$ 之间的“相关性”，该相关性由矩阵 $(W_h^T)^{k-t} W_y^T$ 介导，该矩阵解释了影响通过网络动态和读出层的传播。\n\n接下来，我们讨论计算复杂度。\n(i) 直接根据闭式表达式求值：\n公式为 $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$。\n假设前向传播已经运行，因此所有 $e_t \\in \\mathbb{R}^{d_y}$ 和 $h_t \\in \\mathbb{R}^{d_h}$ 都是已知的。\n双重循环涉及 $O(T^2)$ 项。对于每一项 $(t, k)$，计算涉及：\n- 矩阵幂 $(W_h^T)^{k-t}$，如果未预先计算，需要 $O((k-t) d_h^3)$。预先计算所有需要的 $W_h^T$ 的幂次直到 $T-1$ 需要 $O(T d_h^3)$。\n- 矩阵-向量乘积 $W_y^T e_k$，成本为 $O(d_h d_y)$。\n- 矩阵-向量乘积 $(W_h^T)^{k-t} (W_y^T e_k)$，成本为 $O(d_h^2)$。\n- 与 $h_{t-1}^T$ 的外积，成本为 $O(d_h^2)$。\n一个朴素的实现会非常昂贵。一个更优化的直接求值方法如下：\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\n最昂贵的部分是双重循环结构。大约有 $T^2/2$ 对 $(t,k)$。对于每一对，主要成本是计算与矩阵幂的矩阵-向量乘积，假设幂已预先计算，则为 $O(d_h^2)$。预计算成本为 $O(T d_h^3)$。因此，总复杂度由嵌套循环和预计算主导，导致复杂度为 $O(T d_h^3 + T^2 d_h^2)$。对序列长度 $T$ 的二次方依赖使得这种方法对于长序列来说计算上是令人望而却步的。\n\n(ii) 通过 BPTT (Backpropagation Through Time) 进行求值：\nBPTT 通过使用动态规划避免了双重求和的显式计算。它通过单次随时间的反向传播来计算梯度。关键是 $\\delta_t$ 的递归关系：\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$，对于 $t = T-1, \\dots, 1$。\n计算 $\\nabla_{W_h} L$ 的 BPTT 算法是：\n1. 执行从 $t=1$ 到 $T$ 的前向传播，计算并存储所有的 $h_t$ 和 $e_t$。成本：$O(T(d_h^2 + d_h d_x + d_y d_h))$。\n2. 初始化 $\\nabla_{W_h} L = 0$ 和 $\\delta_{T+1} = 0$。\n3. 执行从 $t=T$ 向下到 $1$ 的反向传播：\n   a. 计算 $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$。这涉及两次矩阵-向量乘积和一次向量加法，成本为 $O(d_h d_y + d_h^2)$。\n   b. 将贡献加到梯度上：$\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$。这是一个外积和矩阵加法，成本为 $O(d_h^2)$。\n反向传播包含一个长度为 $T$ 的单次循环，每一步的成本为 $O(d_h^2 + d_h d_y)$。反向传播的总复杂度为 $O(T(d_h^2 + d_h d_y))$。\n因此，BPTT 的总复杂度为 $O(T(d_h^2 + d_h d_x + d_y d_h))$，这在序列长度 $T$ 上是线性的。这比直接求值闭式表达式的效率要高得多。", "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$", "id": "3192146"}, {"introduction": "在训练深度循环神经网络时，梯度消失和爆炸是一个核心挑战。一种有效的缓解策略是在序列的多个时间步提供“监督信号”，而不是仅仅在序列末尾计算损失。本练习通过一个简化的标量RNN模型，让你能够从解析上证明，与仅在最终时刻计算损失相比，分布式损失（在每个时间步计算损失）如何能更有效地维持梯度流，从而将有用的训练信号传播回初始时间步 [@problem_id:3101225]。", "problem": "考虑一个标量线性循环神经网络，其在离散时间索引上由以下递归关系定义\n$$h_{t+1} = w\\,h_{t}, \\quad t = 0, 1, \\dots, T-1,$$\n其中初始隐藏状态为 $$h_{0} \\neq 0,$$ 标量参数 $$w \\in \\mathbb{R}$$ 满足 $$0  |w| \\neq 1,$$ 整数时间域 $$T \\in \\mathbb{N}$$ 满足 $$T \\geq 1.$$ 每个时间的输出与隐藏状态相同，即 $$y_{t} = h_{t}.$$ 每个时间步的损失为\n$$\\ell_{t} = \\frac{1}{2}\\,h_{t}^{2}.$$\n考虑两种总损失的放置方式：\n- 时间上分布的损失，\n$$L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t},$$\n- 以及仅在最终时间的损失，\n$$L_{\\mathrm{final}} = \\ell_{T}.$$\n\n仅使用微积分的链式法则、将时间反向传播（BPTT）定义为沿展开的计算图重复应用链式法则，以及上述模型方程，推导对于任意 $$t \\in \\{0,1,\\dots,T\\}$$ 的 $$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$$ 和 $$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$$ 的显式符号表达式。然后，作为损失放置方式如何影响梯度传播回序列起始位置的定量总结，计算比率\n$$R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$$\n作为关于 $$w$$ 和 $$T$$ 的单个闭式解析表达式。\n\n你的最终答案必须是 $$R(w,T)$$ 的简化闭式表达式。不需要进行数值计算，也不需要四舍五入。不要包含单位，并使用标准的 LaTeX 数学符号表示所有量。", "solution": "首先验证问题，以确保其是适定的、有科学依据的且客观的。\n\n### 第1步：提取已知条件\n- 递归关系：$h_{t+1} = w h_{t}$，对于 $t = 0, 1, \\dots, T-1$。\n- 初始条件：$h_{0} \\neq 0$。\n- 参数约束：$w \\in \\mathbb{R}$，$0  |w| \\neq 1$。\n- 时间域：$T \\in \\mathbb{N}$，$T \\geq 1$。\n- 输出定义：$y_{t} = h_{t}$。\n- 每个时间步的损失：$\\ell_{t} = \\frac{1}{2} h_{t}^{2}$。\n- 时间上分布的损失：$L_{\\mathrm{sum}} = \\sum_{t=1}^{T} \\ell_{t}$。\n- 仅在最终时间的损失：$L_{\\mathrm{final}} = \\ell_{T}$。\n- 需要推导的量：\n    1. 对于 $t \\in \\{0, 1, \\dots, T\\}$ 的 $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$ 的显式符号表达式。\n    2. 对于 $t \\in \\{0, 1, \\dots, T\\}$ 的 $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$ 的显式符号表达式。\n    3. 比率 $R(w,T) \\equiv \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$。\n\n### 第2步：使用提取的已知条件进行验证\n- **有科学依据**：该问题描述了一个简化的线性循环神经网络，并要求使用时间反向传播（BPTT）计算梯度。这是深度学习中的一个标准和基础课题，基于微积分的链式法则。所有定义和关系在数学上都是合理的。\n- **适定性**：所有变量、常数和函数都有明确的定义。约束条件 $h_0 \\neq 0$ 和 $0  |w| \\neq 1$ 确保了梯度表达式和最终比率中的分母不为零，从而得到一个唯一且有意义的解。\n- **客观性**：该问题以精确的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有表现出验证标准中列出的任何缺陷。\n\n### 第3步：结论与行动\n该问题是**有效的**。将提供完整的解答。\n\n### 解答推导\n\n首先，我们建立隐藏状态 $h_t$ 的闭式表达式。递归关系 $h_{t+1} = w h_t$ 是一个几何级数。从初始状态 $h_0$ 展开它得到：\n$$h_t = w^t h_0$$\n该表达式对所有 $t \\in \\{0, 1, \\dots, T\\}$ 成立。\n\n#### $\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}}$ 的推导\n最终时间损失为 $L_{\\mathrm{final}} = \\ell_T = \\frac{1}{2} h_T^2$。\n损失对中间隐藏状态 $h_t$ 的梯度是通过沿从 $h_t$ 到 $h_T$ 的计算图应用链式法则找到的。状态 $h_t$ 仅通过其对 $h_T$ 的影响来影响 $L_{\\mathrm{final}}$。\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = \\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} \\frac{\\partial h_T}{\\partial h_{t}}$$\n第一项是损失函数对其直接输入的导数：\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_T} = \\frac{\\partial}{\\partial h_T} \\left( \\frac{1}{2} h_T^2 \\right) = h_T$$\n第二项是未来状态对过去状态的导数。使用 $h_T = w^{T-t} h_t$，我们得到：\n$$\\frac{\\partial h_T}{\\partial h_{t}} = \\frac{\\partial}{\\partial h_{t}} (w^{T-t} h_t) = w^{T-t}$$\n结合这些结果，对于任何 $t \\in \\{0, 1, \\dots, T\\}$ 的梯度是：\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = h_T w^{T-t}$$\n这是所要求的显式表达式之一。我们也可以通过代入 $h_T = w^T h_0$ 将其表示为 $h_0$ 的函数：\n$$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{t}} = (w^T h_0) w^{T-t} = h_0 w^{2T-t}$$\n\n#### $\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}}$ 的推导\n分布损失为 $L_{\\mathrm{sum}} = \\sum_{k=1}^{T} \\ell_k = \\sum_{k=1}^{T} \\frac{1}{2} h_k^2$。\n隐藏状态 $h_t$ 影响所有 $k \\ge t$ 的损失 $\\ell_k$。我们必须对从 $h_t$ 到总损失的所有路径求和。\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\frac{\\partial}{\\partial h_t} \\left( \\sum_{k=1}^{T} \\frac{1}{2} h_k^2 \\right)$$\n由于对于 $k  t$ 的 $h_k$ 不依赖于 $h_t$，我们可以限制求和范围：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial}{\\partial h_t} \\left( \\frac{1}{2} h_k^2 \\right) = \\sum_{k=t}^{T} \\frac{\\partial(\\frac{1}{2} h_k^2)}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t} = \\sum_{k=t}^{T} h_k \\frac{\\partial h_k}{\\partial h_t}$$\n注意求和从 $k=t$ 开始，但损失 $L_{\\mathrm{sum}}$ 是从 $t=1$ 定义的。我们必须考虑求和下界的两种情况。\n\n**情况1：$t \\in \\{1, 2, \\dots, T\\}$**\n求和如上式有效。我们有 $\\frac{\\partial h_k}{\\partial h_t} = w^{k-t}$。\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} h_k w^{k-t}$$\n代入 $h_k = w^{k-t} h_t$：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_t} = \\sum_{k=t}^{T} (w^{k-t} h_t) w^{k-t} = h_t \\sum_{k=t}^{T} w^{2(k-t)}$$\n令 $j = k-t$。该和变为一个几何级数：\n$$h_t \\sum_{j=0}^{T-t} (w^2)^j = h_t \\left( \\frac{(w^2)^{T-t+1} - 1}{w^2 - 1} \\right) = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\n因此，对于 $t \\in \\{1, \\dots, T\\}$，显式表达式为：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{t}} = h_t \\frac{w^{2(T-t+1)} - 1}{w^2 - 1}$$\n\n**情况2：$t = 0$**\n损失和从 $k=1$ 开始，所以 $h_0$ 对任何 $\\ell_k$ 都没有直接贡献。其影响完全通过 $k \\ge 1$ 的 $h_k$ 产生。\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} \\frac{\\partial \\ell_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_0} = \\sum_{k=1}^{T} h_k w^k$$\n代入 $h_k = w^k h_0$：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = \\sum_{k=1}^{T} (w^k h_0) w^k = h_0 \\sum_{k=1}^{T} w^{2k}$$\n这是一个首项为 $w^2$，公比为 $w^2$，共有 $T$ 项的几何级数。\n$$\\sum_{k=1}^{T} (w^2)^k = w^2 \\frac{(w^2)^T - 1}{w^2 - 1} = \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\n因此，对于 $t=0$，显式表达式为：\n$$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_0} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$$\n\n#### 比率 $R(w,T)$ 的计算\n该比率定义为 $R(w,T) = \\frac{\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}}}{\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_{0}}}$。\n根据上面的推导，我们有：\n- 分子：$\\frac{\\partial L_{\\mathrm{sum}}}{\\partial h_{0}} = h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}$\n- 分母：对于 $t=0$，$\\frac{\\partial L_{\\mathrm{final}}}{\\partial h_0} = h_0 w^{2T-0} = h_0 w^{2T}$。\n\n现在我们计算这个比率。条件 $h_0 \\neq 0$ 允许消去。\n$$R(w,T) = \\frac{h_0 \\frac{w^2(w^{2T} - 1)}{w^2 - 1}}{h_0 w^{2T}} = \\frac{w^2(w^{2T} - 1)}{(w^2 - 1)w^{2T}}$$\n我们可以简化这个表达式。\n$$R(w,T) = \\frac{w^2}{w^{2T}} \\cdot \\frac{w^{2T} - 1}{w^2 - 1} = w^{2-2T} \\frac{w^{2T} - 1}{w^2 - 1}$$\n通过分解出 $w$ 的幂，可以得到一个更对称的替代形式：\n$$R(w,T) = \\frac{w^{2T} - 1}{w^{2T}} \\cdot \\frac{w^2}{w^2 - 1} = \\left(1 - \\frac{1}{w^{2T}}\\right) \\cdot \\frac{1}{\\frac{w^2-1}{w^2}} = \\frac{1 - w^{-2T}}{1 - w^{-2}}$$\n这就是该比率的最终简化闭式表达式。条件 $0  |w| \\neq 1$ 确保了分母 $1 - w^{-2}$ 不为零。", "answer": "$$\\boxed{\\frac{1 - w^{-2T}}{1 - w^{-2}}}$$", "id": "3101225"}, {"introduction": "从理论转向一个常见的实现陷阱：如何使用填充（padding）处理可变长度的序列。在实践中，错误地应用掩码（mask）会导致梯度从被填充的、本应无关的时间步“泄漏”，从而污染整个训练信号 [@problem_id:3101197]。本练习提供了一个具体的数值案例，你将需要为正确和错误的两种实现方式追踪BPTT的计算过程，并量化这一常见编程错误所带来的影响。", "problem": "考虑一个循环神经网络 (RNN) 中的单序列训练样本，该样本使用时间反向传播 (BPTT) 和序列掩码来处理可变长度的输入。该循环是线性的，使用恒等激活函数，并由以下状态更新和输出方程定义\n$$h_t = a\\,h_{t-1} + b\\,x_t,\\quad y_t = c\\,h_t,$$\n其中 $h_t$ 是隐藏状态，$x_t$ 是时刻 $t$ 的输入，$y_t$ 是输出，$a, b, c$ 是标量参数。初始隐藏状态为 $h_0 = 0$。每个时间步的平方误差数据拟合损失和一个输出正则化项被结合起来，并应用掩码以遵循真实的序列长度：\n$$L_{\\text{proper}} = \\frac{1}{2}\\sum_{t=1}^{T_{\\max}} m_t\\left((y_t - r_t)^2 + \\lambda\\,y_t^2\\right),$$\n其中 $m_t \\in \\{0,1\\}$ 是时刻 $t$ 的掩码，$r_t$ 是时刻 $t$ 的目标值，$\\lambda  0$ 是正则化强度，$T_{\\max}$ 是用于填充的最大序列长度。\n\n现在假设存在一个实现错误，其中正则化项被错误地在所有填充的时间步上计算，而没有应用掩码，从而得到\n$$L_{\\text{improper}} = \\frac{1}{2}\\sum_{t=1}^{T_{\\max}} m_t\\,(y_t - r_t)^2 + \\frac{1}{2}\\lambda\\sum_{t=1}^{T_{\\max}} y_t^2.$$\n\n给定一个填充样本，其 $T_{\\max} = 3$，真实长度为 $2$，掩码为 $(m_1, m_2, m_3) = (1, 1, 0)$，输入为 $(x_1, x_2, x_3) = (2, 3, 0)$，目标值为 $(r_1, r_2, r_3) = (1, -1, 0)$，参数为 $a = 0.5$, $b = 1$, $c = 1$, $\\lambda = 0.2$, $h_0 = 0$。请仅以微积分的链式法则为基础，并利用上述定义，推导遵循掩码的梯度，并分析不当处理如何导致梯度在填充的时间步 $t=3$ 发生泄漏。\n\n计算此单个样本的标量差异\n$$\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a}$$。\n将最终答案表示为一个精确的实数。无需四舍五入。答案必须是单个实数值。", "solution": "该问题要求计算一个简单循环神经网络的差异 $\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a}$。这涉及到应用时间反向传播 (BPTT) 算法，该算法是微积分链式法则在按时间展开的 RNN 计算图上的直接应用。\n\n计算的核心是求取每个损失函数关于参数 $a$ 的梯度。参数 $a$ 用于每个时间步 $t$ 的状态更新方程中：$h_t = a\\,h_{t-1} + b\\,x_t$。\n应用链式法则，损失函数 $L$ 关于 $a$ 的全导数是其通过每个隐藏状态 $h_t$ 施加影响的总和：\n$$\n\\frac{\\partial L}{\\partial a} = \\sum_{t=1}^{T_{\\max}} \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial a}\n$$\n在 BPTT 的背景下，$\\frac{\\partial L}{\\partial h_t}$ 是损失关于状态 $h_t$ 的总梯度，而 $\\frac{\\partial h_t}{\\partial a}$ 是来自状态更新方程的局部偏导数。这个局部偏导数就是 $h_{t-1}$。\n我们将流回状态 $h_t$ 的梯度定义为 $\\delta_t = \\frac{\\partial L}{\\partial h_t}$。那么关于 $a$ 的梯度就是随时间的累积：\n$$\n\\frac{\\partial L}{\\partial a} = \\sum_{t=1}^{T_{\\max}} \\delta_t h_{t-1}\n$$\n梯度 $\\delta_t$ 本身是通过一个同样遵循链式法则的递推关系来计算的。状态 $h_t$ 通过两种方式影响损失：直接通过输出 $y_t$，以及间接通过下一个状态 $h_{t+1}$。\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L}{\\partial y_t}\\frac{\\partial y_t}{\\partial h_t} + \\frac{\\partial L}{\\partial h_{t+1}}\\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\n使用给定的模型方程 $y_t = c\\,h_t$ 和 $h_{t+1} = a\\,h_t + b\\,x_{t+1}$，我们求得偏导数 $\\frac{\\partial y_t}{\\partial h_t}=c$ 和 $\\frac{\\partial h_{t+1}}{\\partial h_t}=a$。这就给出了状态梯度的 BPTT 遞推关系：\n$$\n\\delta_t = \\frac{\\partial L}{\\partial y_t} c + \\delta_{t+1} a\n$$\n这个递推关系在序列末尾用 $\\delta_{T_{\\max}+1}=0$ 初始化，并从 $t=T_{\\max}$ 向后计算至 $t=1$。$\\frac{\\partial L}{\\partial y_t}$ 项是损失函数在时间步 $t$ 关于输出的局部梯度。\n\n计算过程分为三个主要步骤：\n1.  一次前向传播，计算隐藏状态 $h_t$ 和输出 $y_t$。\n2.  对每个损失函数（$L_{\\text{proper}}$ 和 $L_{\\text{improper}}$）进行一次反向传播，以计算梯度 $\\delta_t$ 并累积关于 $a$ 的总梯度。\n3.  计算最终差异 $\\Delta$。\n\n**步骤 1：前向传播**\n给定值：$a = 0.5$, $b = 1$, $c = 1$，以及初始状态 $h_0 = 0$。\n输入序列为 $(x_1, x_2, x_3) = (2, 3, 0)$。\n\n当 $t=1$ 时：\n$h_1 = a\\,h_0 + b\\,x_1 = (0.5)(0) + (1)(2) = 2$\n$y_1 = c\\,h_1 = (1)(2) = 2$\n\n当 $t=2$ 时：\n$h_2 = a\\,h_1 + b\\,x_2 = (0.5)(2) + (1)(3) = 1 + 3 = 4$\n$y_2 = c\\,h_2 = (1)(4) = 4$\n\n当 $t=3$ 时：\n$h_3 = a\\,h_2 + b\\,x_3 = (0.5)(4) + (1)(0) = 2 + 0 = 2$\n$y_3 = c\\,h_3 = (1)(2) = 2$\n\n前向传播总结：$h_0=0, h_1=2, h_2=4, h_3=2$ 以及 $y_1=2, y_2=4, y_3=2$。\n\n**步骤 2a：$L_{\\text{proper}}$ 的反向传播**\n局部输出梯度为 $\\frac{\\partial L_{\\text{proper}}}{\\partial y_t} = m_t((1+\\lambda)y_t - r_t)$。\n给定：$\\lambda=0.2$，$(m_1, m_2, m_3)=(1, 1, 0)$，$(r_1, r_2, r_3) = (1, -1, 0)$。\n\n当 $t=1$ 时：$\\frac{\\partial L_{\\text{proper}}}{\\partial y_1} = 1 \\times ((1+0.2)(2) - 1) = 1.2 \\times 2 - 1 = 2.4 - 1 = 1.4$。\n当 $t=2$ 时：$\\frac{\\partial L_{\\text{proper}}}{\\partial y_2} = 1 \\times ((1+0.2)(4) - (-1)) = 1.2 \\times 4 + 1 = 4.8 + 1 = 5.8$。\n当 $t=3$ 时：$\\frac{\\partial L_{\\text{proper}}}{\\partial y_3} = 0 \\times ((1+0.2)(2) - 0) = 0$。\n\n现在我们从 $\\delta_4^{\\text{proper}} = 0$ 开始，反向计算 $\\delta_t^{\\text{proper}}$。\n当 $t=3$ 时：$\\delta_3^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_3} c + \\delta_4^{\\text{proper}} a = (0)(1) + (0)(0.5) = 0$。\n当 $t=2$ 时：$\\delta_2^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_2} c + \\delta_3^{\\text{proper}} a = (5.8)(1) + (0)(0.5) = 5.8$。\n当 $t=1$ 时：$\\delta_1^{\\text{proper}} = \\frac{\\partial L_{\\text{proper}}}{\\partial y_1} c + \\delta_2^{\\text{proper}} a = (1.4)(1) + (5.8)(0.5) = 1.4 + 2.9 = 4.3$。\n\n最后，我们计算关于 $a$ 的总梯度：\n$$\n\\frac{\\partial L_{\\text{proper}}}{\\partial a} = \\sum_{t=1}^{3} \\delta_t^{\\text{proper}} h_{t-1} = \\delta_1^{\\text{proper}}h_0 + \\delta_2^{\\text{proper}}h_1 + \\delta_3^{\\text{proper}}h_2\n$$\n$$\n\\frac{\\partial L_{\\text{proper}}}{\\partial a} = (4.3)(0) + (5.8)(2) + (0)(4) = 0 + 11.6 + 0 = 11.6\n$$\n\n**步骤 2b：$L_{\\text{improper}}$ 的反向传播**\n局部输出梯度为 $\\frac{\\partial L_{\\text{improper}}}{\\partial y_t} = m_t(y_t - r_t) + \\lambda y_t$。\n\n当 $t=1$ 时：$\\frac{\\partial L_{\\text{improper}}}{\\partial y_1} = 1 \\times (2 - 1) + (0.2)(2) = 1 + 0.4 = 1.4$。\n当 $t=2$ 时：$\\frac{\\partial L_{\\text{improper}}}{\\partial y_2} = 1 \\times (4 - (-1)) + (0.2)(4) = 5 + 0.8 = 5.8$。\n当 $t=3$ 时：$\\frac{\\partial L_{\\text{improper}}}{\\partial y_3} = 0 \\times (2 - 0) + (0.2)(2) = 0 + 0.4 = 0.4$。\n\n在 $t=3$ 处的非零梯度是“泄漏”的来源。尽管数据拟合项被掩码掉（$m_3=0$），正则化项却没有，从而在这个填充的时间步上产生了一个梯度信号。\n\n现在我们从 $\\delta_4^{\\text{improper}} = 0$ 开始，反向计算 $\\delta_t^{\\text{improper}}$。\n当 $t=3$ 时：$\\delta_3^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_3} c + \\delta_4^{\\text{improper}} a = (0.4)(1) + (0)(0.5) = 0.4$。\n这个在 $h_3$ 处的非零梯度现在将传播到更早的时间步。\n当 $t=2$ 时：$\\delta_2^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_2} c + \\delta_3^{\\text{improper}} a = (5.8)(1) + (0.4)(0.5) = 5.8 + 0.2 = 6.0$。\n当 $t=1$ 时：$\\delta_1^{\\text{improper}} = \\frac{\\partial L_{\\text{improper}}}{\\partial y_1} c + \\delta_2^{\\text{improper}} a = (1.4)(1) + (6.0)(0.5) = 1.4 + 3.0 = 4.4$。\n\n最后，我们计算关于 $a$ 的总梯度：\n$$\n\\frac{\\partial L_{\\text{improper}}}{\\partial a} = \\sum_{t=1}^{3} \\delta_t^{\\text{improper}} h_{t-1} = \\delta_1^{\\text{improper}}h_0 + \\delta_2^{\\text{improper}}h_1 + \\delta_3^{\\text{improper}}h_2\n$$\n$$\n\\frac{\\partial L_{\\text{improper}}}{\\partial a} = (4.4)(0) + (6.0)(2) + (0.4)(4) = 0 + 12.0 + 1.6 = 13.6\n$$\n\n**步骤 3：计算差异**\n差异 $\\Delta$ 是两个计算出的梯度之差。\n$$\n\\Delta = \\frac{\\partial L_{\\text{improper}}}{\\partial a} - \\frac{\\partial L_{\\text{proper}}}{\\partial a} = 13.6 - 11.6 = 2\n$$\n来自 $t=3$ 处未被掩码的正则化项的梯度泄漏在网络中反向传播，改变了所有先前时间步的梯度，最终导致参数 $a$ 的差异为 $2$。", "answer": "$$\\boxed{2}$$", "id": "3101197"}]}