{"hands_on_practices": [{"introduction": "循环神经网络（RNN）的训练过程充满挑战，其中最著名的就是梯度消失和梯度爆炸问题。这些问题直接影响模型学习长期依赖关系的能力，而激活函数的选择是影响梯度流的关键因素之一。本练习 [@problem_id:3168374] 将引导你通过一个具体的计算实验，亲手实现并比较两种常用激活函数——双曲正切（$\\tanh$）和修正线性单元（ReLU）——在简单 RNN 中的表现，从而直观地理解它们如何影响梯度传播和“神经元死亡”现象。", "problem": "要求您实现并分析一个简单的、全连接的标准循环神经网络 (RNN)，该网络使用两种激活函数：双曲正切 ($\\tanh$) 和修正线性单元 (ReLU)。您的分析必须基于标准RNN的基本定义以及离散时间动态系统中梯度的链式法则。目标是比较随着序列长度的增长，梯度流和“死亡”单元频率的变化。\n\n基本原理（使用的定义）：\n- RNN的隐藏状态遵循递推关系 $h_t = \\phi\\!\\left(W_{hh}\\,h_{t-1} + W_{xh}\\,x_t + b\\right)$，对于 $t \\in \\{1,\\dots,T\\}$，其中 $h_0 = 0$。\n- 损失定义为 $L = \\frac{1}{2}\\left\\|h_T\\right\\|_2^2$。\n- 根据链式法则，梯度随时间反向传播，遵循 $\\delta_t = \\frac{\\partial L}{\\partial h_t}$，其中 $\\delta_T = h_T$。并且，对于 $t \\ge 1$，$\\delta_{t-1} = W_{hh}^\\top \\left(\\phi'(a_t) \\odot \\delta_t\\right)$，其中 $a_t = W_{hh}\\,h_{t-1} + W_{xh}\\,x_t + b$，而 $\\odot$ 表示逐元素乘法。\n\n激活函数：\n- 对于 $\\tanh$，使用 $\\phi(z) = \\tanh(z)$ 和 $\\phi'(z) = 1 - \\tanh^2(z)$。\n- 对于 ReLU，使用 $\\phi(z) = \\max(0,z)$ 和 $\\phi'(z) = \\mathbf{1}_{\\{z>0\\}}$。\n\n初始化与输入：\n- 隐藏层大小为 $n$，输入大小为 $d$，序列长度为 $T$。\n- 逐元素初始化 $W_{hh} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^2}{n}\\right)$ 和 $W_{xh} \\sim \\mathcal{N}\\!\\left(0, \\frac{g^2}{d}\\right)$，偏置 $b=0$。\n- 对于一批 $N$ 个独立的序列，在每个时间步为每个序列从标准正态分布中独立同分布地抽取输入 $x_t \\in \\mathbb{R}^d$。\n- 对所有序列使用 $N = 64$ 个序列/测试，$n = 64$，$d = 32$，以及 $h_0 = 0$。\n- 每个测试用例使用相同的固定随机种子 $0$ 以确保可复现性。\n\n每个测试用例需要计算的量：\n- 梯度流度量：对于每个序列，通过上述递推关系计算 $\\delta_0 = \\frac{\\partial L}{\\partial h_0}$，然后计算其欧几里得范数并记录 $\\log_{10}\\!\\left(\\|\\delta_0\\|_2 + 10^{-12}\\right)$。将此量在批次上的平均值报告为单个浮点数。\n- “死亡”单元频率：\n  - 对于 ReLU，如果一个隐藏单元的导数在所有时间步和所有序列中都为零，即对于所有 $t \\in \\{1,\\dots,T\\}$ 和所有 $N$ 个序列，$\\phi'(a_t)=0$，则称该单元为“死亡”单元。报告 $n$ 个隐藏单元中“死亡”单元的比例，结果为一个在 $[0,1]$ 区间的浮点数。\n  - 对于 $\\tanh$，如果一个隐藏单元的导数绝对值在所有时间步和所有 $N$ 个序列中都保持在一个小阈值 $\\epsilon$ 以下，即对于所有 $t$ 和所有 $N$ 个序列，$\\left|\\phi'(a_t)\\right|  \\epsilon$，则称该单元为“近乎死亡”单元。使用 $\\epsilon = 10^{-3}$。报告 $n$ 个隐藏单元中“近乎死亡”单元的比例，结果为一个在 $[0,1]$ 区间的浮点数。\n\n数值单位：\n- 本问题中不涉及物理单位。如果存在角度，根据 $\\tanh$ 和 ReLU 的定义，其单位为弧度，这两个函数本身不需要角度单位。所有报告的比例必须以小数形式表示。\n\n测试套件：\n对于每个测试用例，参数按以下顺序排列：$(\\text{activation}, n, d, T, g)$。\n- 用例 1：$(\\tanh, 64, 32, 5, 0.5)$。\n- 用例 2：$(\\tanh, 64, 32, 50, 0.5)$。\n- 用例 3：$(\\tanh, 64, 32, 50, 1.5)$。\n- 用例 4：$(\\text{ReLU}, 64, 32, 5, 0.5)$。\n- 用例 5：$(\\text{ReLU}, 64, 32, 50, 0.5)$。\n- 用例 6：$(\\text{ReLU}, 64, 32, 50, 1.5)$。\n- 用例 7（边界情况）：$(\\text{ReLU}, 64, 32, 20, 0.0)$。\n\n程序要求：\n- 完全按照上述说明实现 RNN 的前向和反向传播。\n- 对于每个测试用例，计算：\n  - $N$ 个序列上的平均梯度流度量。\n  - 如上定义的“死亡”（或“近乎死亡”）单元的比例。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的配对列表，每个测试用例对应一个配对，顺序与上文相同。每个配对必须是 $[\\text{avg\\_log10\\_grad\\_norm}, \\text{dead\\_fraction}]$ 的形式，并且整个输出必须包含在一对顶层方括号中。例如：$[[x_1,y_1],[x_2,y_2],\\dots]$。浮点数应精确到小数点后 $6$ 位进行打印。", "solution": "该问题是有效的。它提出了一个定义明确的计算实验，用以分析标准循环神经网络（RNN）的行为。问题中的定义、方程、初始条件和参数都足够精确，可以得出一个唯一的、可验证的解。该问题在科学上基于神经网络理论的既定原则，特别是随时间反向传播（BPTT），并探讨了诸如梯度消失/爆炸问题和“神经元死亡”问题等公认现象。\n\n分析过程是通过实现指定的RNN动态，然后为每个测试用例计算所需的度量。\n\n### 数学公式\n\n问题的核心在于RNN的离散时间动态。\n\n**1. 前向传播：**\n在时间步 $t$ 的隐藏状态 $h_t \\in \\mathbb{R}^n$ 是基于前一个隐藏状态 $h_{t-1}$ 和当前输入 $x_t \\in \\mathbb{R}^d$ 计算得出的。对于一批 $N$ 个序列，我们可以将其写成矩阵形式。设 $H_{t-1}$ 为批次在时间 $t-1$ 的隐藏状态矩阵（$N \\times n$），$X_t$ 为输入矩阵（$N \\times d$）。预激活矩阵 $A_t \\in \\mathbb{R}^{N \\times n}$ 和新的隐藏状态矩阵 $H_t \\in \\mathbb{R}^{N \\times n}$ 由以下公式给出：\n$$A_t = H_{t-1} W_{hh}^\\top + X_t W_{xh}^\\top + \\mathbf{1}b^\\top$$\n$$H_t = \\phi(A_t)$$\n其中 $\\phi$ 是逐元素的激活函数，$W_{hh} \\in \\mathbb{R}^{n \\times n}$ 和 $W_{xh} \\in \\mathbb{R}^{n \\times d}$ 是权重矩阵，$b \\in \\mathbb{R}^n$ 是偏置向量（给定为 $b=0$），$\\mathbf{1}$ 是一个大小为 $N$ 的全一列向量。这个过程从初始隐藏状态 $H_0$ 开始，它是一个大小为 $(N \\times n)$ 的零矩阵。此递推关系应用于 $t = 1, \\dots, T$。\n\n**2. 损失函数：**\n损失 $L$ 是一个标量值，根据批次的最终隐藏状态 $H_T$ 计算得出。\n$$L = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{2} \\|h_{T,i}\\|_2^2 = \\frac{1}{2N} \\text{Tr}(H_T H_T^\\top)$$\n问题陈述将单个序列的损失定义为 $L_i = \\frac{1}{2}\\|h_{T,i}\\|_2^2$。接下来的梯度推导是针对这单个序列的损失。\n\n**3. 反向传播（随时间反向传播）：**\n目标是计算损失相对于初始状态的梯度 $\\frac{\\partial L}{\\partial h_0}$。我们将损失在时间 $t$ 对隐藏状态的梯度定义为 $\\delta_t = \\frac{\\partial L}{\\partial h_t}$。\n链式法则给出了该梯度的一个反向递推关系。该过程从最终时间步 $T$ 开始：\n$$\\delta_T = \\frac{\\partial L}{\\partial h_T} = \\frac{\\partial}{\\partial h_T} \\left( \\frac{1}{2}h_T^\\top h_T \\right) = h_T$$\n对于时间步 $t  T$，梯度 $\\delta_t$ 依赖于 $\\delta_{t+1}$。$L$ 相对于 $h_t$ 的全导数为：\n$$\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial h_{t+1}}{\\partial h_t}^\\top \\frac{\\partial L}{\\partial h_{t+1}} = \\frac{\\partial h_{t+1}}{\\partial h_t}^\\top \\delta_{t+1}$$\n雅可比矩阵 $\\frac{\\partial h_{t+1}}{\\partial h_t}$ 是从前向传播方程 $h_{t+1} = \\phi(W_{hh}h_t + W_{xh}x_{t+1} + b)$ 推导出来的。\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = \\text{diag}(\\phi'(a_{t+1})) W_{hh}$$\n其中 $a_{t+1} = W_{hh}h_t + W_{xh}x_{t+1} + b$，$\\phi'$ 是激活函数的导数。符号 $\\text{diag}(v)$ 表示从向量 $v$ 创建一个对角矩阵。\n将其代回，我们得到梯度递推关系：\n$$\\delta_t = W_{hh}^\\top \\text{diag}(\\phi'(a_{t+1})) \\delta_{t+1} = W_{hh}^\\top (\\phi'(a_{t+1}) \\odot \\delta_{t+1})$$\n其中 $\\odot$ 表示逐元素（哈达玛）乘积。这与问题中提供的方程相符。该递推关系从 $t = T-1$ 执行到 $0$。\n\n### 算法流程\n\n对于每个测试用例 $(\\text{activation}, n, d, T, g)$：\n\n1.  **初始化：**\n    -   为保证可复现性，将随机种子设置为 $0$。\n    -   设置参数 $N=64$ 和 $\\epsilon=10^{-3}$。\n    -   通过从高斯分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽样来初始化权重 $W_{hh} \\in \\mathbb{R}^{n \\times n}$ 和 $W_{xh} \\in \\mathbb{R}^{n \\times d}$，其中标准差分别为 $\\sigma_{hh} = g / \\sqrt{n}$ 和 $\\sigma_{xh} = g / \\sqrt{d}$。\n    -   从标准正态分布 $\\mathcal{N}(0, 1)$ 中初始化输入 $X \\in \\mathbb{R}^{N \\times T \\times d}$。\n    -   将第一个隐藏状态 $H_0 \\in \\mathbb{R}^{N \\times n}$ 初始化为零矩阵。\n\n2.  **前向传播：**\n    -   储存所有的隐藏状态 $H_t$ 和预激活值 $A_t$，对于 $t=1, \\dots, T$。\n    -   从 $t=1$ 迭代到 $T$：\n        -   计算 $A_t = H_{t-1} W_{hh}^\\top + X_{:, t-1, :} W_{xh}^\\top$。\n        -   根据指定使用 $\\tanh$ 或 $\\text{ReLU}$ 计算 $H_t = \\phi(A_t)$。\n        -   储存 $A_t$ 和 $H_t$。\n\n3.  **反向传播：**\n    -   初始化梯度批次 $\\Delta_T = H_T$。\n    -   创建一个形状为 $(T \\times N \\times n)$ 的容器 `All_Phi_Primes` 来储存导数。\n    -   从 $t=T$ 向下迭代到 $1$：\n        -   检索预激活值 $A_t$ 和隐藏状态 $H_t$。\n        -   计算导数矩阵 $\\Phi'_t$：\n            -   对于 $\\tanh$：$\\Phi'_t = 1 - H_t \\odot H_t$。\n            -   对于 $\\text{ReLU}$：$\\Phi'_t = \\mathbf{1}_{\\{A_t > 0\\}}$。\n        -   将 $\\Phi'_t$ 储存在 `All_Phi_Primes` 中。\n        -   更新梯度：$\\Delta_{t-1} = (\\Delta_t \\odot \\Phi'_t) W_{hh}$。令 $\\Delta_t$ 为更新后的梯度变量。此步之后，我们称为 $\\Delta_t$ 的变量现在持有 $\\Delta_{t-1}$ 的值，为下一次迭代做好了准备。\n    -   循环结束后梯度变量的最终值为 $\\Delta_0$。\n\n4.  **度量计算：**\n    -   **梯度流度量：**\n        -   计算 $\\Delta_0$ 的每一行的欧几里得范数（即批次中的每个序列）。这将得到一个包含 $N$ 个范数的向量 $\\|\\delta_{0,i}\\|_2$。\n        -   对每个范数应用变换 $\\log_{10}(\\|\\delta_{0,i}\\|_2 + 10^{-12})$。\n        -   计算这 $N$ 个值的平均值。\n    -   **“死亡”单元频率：**\n        -   对于每个隐藏单元 $j \\in \\{1, \\dots, n\\}$：\n            -   检查该单元的导数在所有 $T$ 个时间步和所有 $N$ 个序列中是否为“死亡”状态。\n            -   对于 $\\text{ReLU}$：检查是否对所有的 $t, i$ 都有 $(\\text{All\\_Phi\\_Primes})_{t,i,j} == 0$。\n            -   对于 $\\tanh$：检查是否对所有的 $t, i$ 都有 $|\\text{All\\_Phi\\_Primes}_{t,i,j}|  \\epsilon$。\n        -   计算死亡单元的数量，然后除以隐藏单元的总数 $n$。\n\n此流程被系统地应用于所有测试用例，并按规定格式化结果。当 $g=0$ 的特殊情况下，权重为零，导致整个前向传播过程中的激活值和隐藏状态都为零。因此，初始梯度 $\\delta_T$ 为零，所有后续的梯度 $\\delta_t$ 也都为零，得出的梯度流度量为 $\\log_{10}(10^{-12}) = -12$。对于 $g=0$ 的 ReLU，所有预激活值都为零，因此它们的导数也为零，导致所有单元都“死亡”，频率为 $1.0$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the RNN analysis for all test cases and print the results.\n    \"\"\"\n\n    def run_rnn_analysis(activation, n, d, T, g, N=64, seed=0):\n        \"\"\"\n        Implements a single run of the RNN forward and backward pass for analysis.\n        \n        Args:\n            activation (str): 'tanh' or 'ReLU'.\n            n (int): Hidden size.\n            d (int): Input size.\n            T (int): Sequence length.\n            g (float): Gain parameter for weight initialization.\n            N (int): Batch size.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            tuple: A pair containing (avg_log10_grad_norm, dead_fraction).\n        \"\"\"\n        np.random.seed(seed)\n        \n        # 1. Initialization\n        # Weight matrices\n        # std_dev = g / sqrt(fan_in)\n        w_hh = (g / np.sqrt(n)) * np.random.randn(n, n)\n        w_xh = (g / np.sqrt(d)) * np.random.randn(n, d)\n        \n        # Input data: (N, T, d) for batch-first processing\n        x = np.random.randn(N, T, d)\n        \n        # Initial hidden state (batch of zero vectors)\n        h0 = np.zeros((N, n))\n\n        # 2. Forward Pass\n        h_states = [h0]\n        a_states = []\n        \n        h_prev = h0\n        for t in range(T):\n            # a_t = h_{t-1} W_hh^T + x_t W_xh^T\n            # Dimensions: (N, n) @ (n, n) + (N, d) @ (d, n) - (N, n)\n            a = h_prev @ w_hh.T + x[:, t, :] @ w_xh.T\n            \n            if activation == 'tanh':\n                h_curr = np.tanh(a)\n            elif activation == 'ReLU':\n                h_curr = np.maximum(0, a)\n            else:\n                raise ValueError(\"Unsupported activation function\")\n\n            a_states.append(a)\n            h_states.append(h_curr)\n            h_prev = h_curr\n\n        # 3. Backward Pass (BPTT)\n        # delta_t = dL/dh_t. Start with delta_T = h_T\n        delta = h_states[-1]  # H_T, shape (N, n)\n        \n        # Store phi'(a_t) for all t and sequences\n        all_phi_primes = np.zeros((T, N, n))\n\n        # Loop from t=T down to t=1\n        for t in range(T - 1, -1, -1):\n            a_t = a_states[t] # Pre-activation a_{t+1} from problem notation\n            h_t = h_states[t+1] # Hidden state h_{t+1}\n\n            if activation == 'tanh':\n                # phi'(z) = 1 - tanh^2(z) = 1 - h^2\n                phi_prime = 1.0 - h_t**2\n            elif activation == 'ReLU':\n                # phi'(z) = 1 if z  0, 0 otherwise\n                phi_prime = (a_t  0).astype(float)\n            \n            all_phi_primes[t, :, :] = phi_prime\n            \n            # delta_{t-1} = (delta_t * phi'(a_t)) @ W_hh\n            # Dimensions: ((N, n) * (N, n)) @ (n, n) - (N, n)\n            delta = (delta * phi_prime) @ w_hh\n\n        # After loop, delta is delta_0 = dL/dh_0\n\n        # 4. Metric Computation\n        # 4.1 Gradient Flow Metric\n        grad_norms = np.linalg.norm(delta, axis=1)\n        log_grad_norms = np.log10(grad_norms + 1e-12)\n        avg_log10_grad_norm = np.mean(log_grad_norms)\n\n        # 4.2 Dead-Unit Frequency\n        if activation == 'tanh':\n            # Near-dead if |phi'(a_t)|  epsilon for all t, N\n            epsilon = 1e-3\n            is_near_dead_condition = np.abs(all_phi_primes)  epsilon\n            # Check if condition holds for all t (axis 0) and all N (axis 1)\n            unit_is_dead_overall = np.all(is_near_dead_condition, axis=(0, 1))\n        elif activation == 'ReLU':\n            # Dead if phi'(a_t) == 0 for all t, N\n            is_dead_condition = (all_phi_primes == 0)\n            unit_is_dead_overall = np.all(is_dead_condition, axis=(0, 1))\n\n        dead_fraction = np.mean(unit_is_dead_overall)\n\n        return avg_log10_grad_norm, dead_fraction\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('tanh', 64, 32, 5, 0.5),\n        ('tanh', 64, 32, 50, 0.5),\n        ('tanh', 64, 32, 50, 1.5),\n        ('ReLU', 64, 32, 5, 0.5),\n        ('ReLU', 64, 32, 50, 0.5),\n        ('ReLU', 64, 32, 50, 1.5),\n        ('ReLU', 64, 32, 20, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        activation, n, d, T, g = case\n        result_pair = run_rnn_analysis(activation, n, d, T, g)\n        results.append(result_pair)\n\n    # Final print statement in the exact required format.\n    output_str = ','.join([f'[{r[0]:.6f},{r[1]:.6f}]' for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3168374"}, {"introduction": "在分析了 RNN 的基本动态后，让我们进入一个更具挑战性的任务：训练一个 RNN 来执行算法。二进制加法是一个绝佳的例子，因为它要求网络在处理序列时记住一个关键信息——进位。本练习 [@problem_id:3167589] 将指导你训练一个 RNN 来完成这项任务，并通过主动限制其循环权重的谱半径来探索其记忆的极限。你将学习如何从理论上估算网络的记忆长度，并用它来预测和诊断模型在面对需要长距离信息传递（即长进位链）时的失败。", "problem": "要求您设计、分析并实现一个完整的程序，该程序构建并训练一个循环神经网络（RNN），以模拟一个简单的算法任务（二进制加法），并诊断因有限记忆而产生的原则性故障案例。该任务必须纯粹用数学术语来描述，并使用统计学习和循环神经网络的第一性原理来解决。程序的最终输出必须将来自固定测试套件的结果汇总到指定格式的单行中。\n\n这个问题的基本基础是标准Elman循环神经网络的定义以及二进制加法的因果结构。考虑一个使用双曲正切激活函数的循环神经网络（RNN），其隐藏状态更新由以下递归式定义：\n$$\n\\mathbf{h}_t = \\tanh\\!\\Big(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h\\Big),\n$$\n其输出对数几率为\n$$\n\\mathbf{o}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y,\n$$\n概率输出为\n$$\n\\mathbf{y}_t = \\sigma(\\mathbf{o}_t),\n$$\n其中 $\\sigma(\\cdot)$ 表示逐元素的逻辑S型函数。在时间 $t$ 的输入是比特对 $\\mathbf{x}_t = (a_t, b_t)$，其中 $a_t \\in \\{0,1\\}$ 且 $b_t \\in \\{0,1\\}$，从最低有效位（$t = 0$）扫描到最高有效位（$t = T-1$）。网络在每个时间步必须输出两个比特：和比特 $s_t$ 和进位输出比特 $c_t$。真实目标 $(s_t, c_t)$ 由第一性原理的因果加法规则生成：初始化 $c_{-1} = 0$，并在每个时间步 $t$ 计算\n$$\nu_t = a_t + b_t + c_{t-1}, \\quad s_t = u_t \\bmod 2, \\quad c_t = \\left\\lfloor \\frac{u_t}{2} \\right\\rfloor.\n$$\n经过 $T$ 步后，完整的和恢复为\n$$\nS = \\sum_{t=0}^{T-1} s_t 2^t + c_{T-1} \\cdot 2^T.\n$$\n\n您的程序必须完成以下任务：\n\n- 实现并训练上述RNN，使用随机生成的 $T_{\\text{train}}$-比特整数对来模拟按位加法算法，其中 $T_{\\text{train}} = 8$。使用在时间和输出上求和的二元交叉熵损失。使用带有梯度裁剪的随机梯度下降。初始化 $\\mathbf{h}_{-1} = \\mathbf{0}$。每次参数更新后，强制执行谱半径约束\n$$\n\\rho\\big(\\mathbf{W}_{hh}\\big) \\le \\rho_{\\max},\n$$\n其中 $\\rho_{\\max} = 0.7$，通过按需重新缩放 $\\mathbf{W}_{hh}$ 来实现。此约束是强制性的，旨在引入有限的记忆长度。\n\n- 从基本定义出发，推导并实现一个原则性估计器，用于计算已训练网络的有效记忆长度 $L_{\\varepsilon}$。$L_{\\varepsilon}$ 被表示为满足以下条件的最小整数 $L$：过去 $L$ 步的信息对当前隐藏状态的影响被限制在容差 $\\varepsilon$ 之内。您的推导必须从RNN的递归式开始，通过雅可比积表示隐藏状态到隐藏状态的影响，并使用基于范数或谱半径的界来获得一个可计算的估计器 $L_{\\varepsilon}$，该估计器用学习到的参数和平均激活导数表示。在代码中，设置 $\\varepsilon = 0.05$。\n\n- 对于一个特定的输入对 $(\\{a_t\\}, \\{b_t\\})$，将其进位链长度定义为：当从最低有效位到最高有效位进行因果计算时，运行进位比特等于1的连续时间步的最大数量。也就是说，如果 $\\{c_t\\}$ 是由上述真实加法规则产生的进位序列，则进位链长度为\n$$\n\\max_{i \\le j} \\Big\\{ (j-i+1) \\; \\text{such that} \\; c_i = c_{i+1} = \\cdots = c_j = 1 \\Big\\}.\n$$\n\n- 通过比较估计的记忆长度 $L_{\\varepsilon}$ 和输入的进位链长度，来预测给定输入对的充分性与失败。如果 $L_{\\varepsilon}$ 大于或等于进位链长度，则预测“充分”；否则，预测“不充分”。然后，通过计算每个时间步输出得到的完整预测和，并将其与真实整数和进行比较，来对该输入对上的已训练RNN进行经验性评估。\n\n- 测试套件。对所有测试输入使用固定的序列长度 $T = 12$ 比特，并在以下五个非负整数输入对上评估训练好的网络，每个整数都用12比特表示（在计算过程中，最低有效位在前）：\n    - 情况1（正常路径）：$(A,B) = (25, 6)$。\n    - 情况2（边界邻近）：$(A,B) = (255, 1)$。\n    - 情况3（超出边界）：$(A,B) = (1023, 1)$。\n    - 情况4（零的边缘情况）：$(A,B) = (0, 0)$。\n    - 情况5（极端进位传播）：$(A,B) = (4095, 1)$。\n  选择这些输入是为了测试关于进位传播的典型行为、边界条件和重要的边缘情况。\n\n- 最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个测试用例，按顺序输出两个布尔值：\n    1. 第一个布尔值表示估计的记忆长度 $L_{\\varepsilon}$ 对于该用例的最大进位链长度是否充分。\n    2. 第二个布尔值表示训练好的RNN是否为该用例生成了完全正确的整数和。\n  因此，最后一行必须总共包含10个布尔值，格式如下\n$$\n[\\text{p}_1,\\text{a}_1,\\text{p}_2,\\text{a}_2,\\text{p}_3,\\text{a}_3,\\text{p}_4,\\text{a}_4,\\text{p}_5,\\text{a}_5],\n$$\n其中 $\\text{p}_i$ 是对情况 $i$ 的预测充分性，$\\text{a}_i$ 是实际正确性。\n\n本说明中的所有数学量和数字都必须按其标准数学意义进行解释，并且在适用情况下，答案必须表示为布尔值并完全按照定义进行评估。此问题不涉及任何物理单位或角度单位。", "solution": "该问题要求设计、实现并分析一个用于执行二进制加法的循环神经网络（RNN）。问题的一个关键方面是从理论和经验上研究网络的记忆局限性，这些局限性是由循环权重矩阵上的谱半径约束有意引入的。解决方案分为三个主要阶段：首先，定义模型和训练过程；其次，为网络的有效记忆长度推导一个原则性估计器；第三，实现整个系统，以检验一个假设，即网络在长进位加法上的失败是可以通过比较此记忆长度与任务所需的进位传播长度来预测的。\n\n### 1. RNN模型与二进制加法任务\n\nRNN架构是一个标准的Elman网络。隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^{d_h}$ 根据以下递归式演化：\n$$\n\\mathbf{h}_t = \\tanh(\\mathbf{W}_{hh} \\mathbf{h}_{t-1} + \\mathbf{W}_{xh} \\mathbf{x}_t + \\mathbf{b}_h)\n$$\n其中 $\\mathbf{x}_t \\in \\{0,1\\}^2$ 是表示两个比特 $(a_t, b_t)$ 的输入向量，$\\mathbf{W}_{hh} \\in \\mathbb{R}^{d_h \\times d_h}$、$\\mathbf{W}_{xh} \\in \\mathbb{R}^{d_h \\times 2}$ 和 $\\mathbf{b}_h \\in \\mathbb{R}^{d_h}$ 是参数。初始隐藏状态为 $\\mathbf{h}_{-1} = \\mathbf{0}$。\n\n在每个时间步 $t$，网络产生输出对数几率 $\\mathbf{o}_t \\in \\mathbb{R}^2$ 和概率输出 $\\mathbf{y}_t \\in (0,1)^2$：\n$$\n\\mathbf{o}_t = \\mathbf{W}_{hy} \\mathbf{h}_t + \\mathbf{b}_y\n$$\n$$\n\\mathbf{y}_t = \\sigma(\\mathbf{o}_t)\n$$\n其中 $\\mathbf{W}_{hy} \\in \\mathbb{R}^{2 \\times d_h}$、$\\mathbf{b}_y \\in \\mathbb{R}^2$，$\\sigma(\\cdot)$ 是逐元素的逻辑S型函数。$\\mathbf{y}_t$ 的两个分量是网络对和比特 $s_t$ 和进位输出比特 $c_t$ 的概率性预测。\n\n训练的基准真相（ground truth）由二进制加法规则生成。给定输入比特 $(a_t, b_t)$ 和进位输入 $c_{t-1}$（其中 $c_{-1}=0$），真实和比特 $s_t$ 和进位输出比特 $c_t$ 为：\n$$\nu_t = a_t + b_t + c_{t-1}\n$$\n$$\ns_t = u_t \\pmod 2\n$$\n$$\nc_t = \\left\\lfloor \\frac{u_t}{2} \\right\\rfloor\n$$\n\n### 2. 训练过程\n\n网络通过最小化在长度为 $T$ 的序列上，其预测值 $\\mathbf{y}_t = (y_{t,s}, y_{t,c})$ 与真实目标 $(s_t, c_t)$ 之间的总二元交叉熵（BCE）损失来进行训练：\n$$\nL = -\\sum_{t=0}^{T-1} \\left[ s_t \\log(y_{t,s}) + (1-s_t) \\log(1-y_{t,s}) + c_t \\log(y_{t,c}) + (1-c_t) \\log(1-y_{t,c}) \\right]\n$$\n参数使用随机梯度下降（SGD）进行更新，梯度通过随时间反向传播（BPTT）计算。为防止梯度爆炸，所有梯度都被裁剪到一个最大范数。训练的一个关键特征是对循环权重矩阵强制执行谱半径约束 $\\rho(\\mathbf{W}_{hh}) \\le \\rho_{\\max}$，其中 $\\rho(\\cdot)$ 表示谱半径（最大绝对特征值）。如果违反了该约束，则在每次梯度更新后通过重新缩放 $\\mathbf{W}_{hh}$ 来实现：\n$$\n\\text{if } \\rho(\\mathbf{W}_{hh})  \\rho_{\\max}, \\quad \\mathbf{W}_{hh} \\leftarrow \\mathbf{W}_{hh} \\frac{\\rho_{\\max}}{\\rho(\\mathbf{W}_{hh})}\n$$\n这个约束限制了RNN的长期记忆容量，因为它迫使循环动态的线性部分是收缩的。对于这个问题，我们使用 $\\rho_{\\max} = 0.7$ 并在 $T_{\\text{train}}=8$ 比特的数字上进行训练。必须适当地选择诸如隐藏维度、学习率和训练周期等超参数；我们选择 $d_h=4$，学习率 $\\eta=0.05$，以及3000个训练周期。\n\n### 3. 有效记忆长度 $L_{\\varepsilon}$ 的推导\n\n有效记忆长度 $L_{\\varepsilon}$ 是指信息在其影响衰减到阈值 $\\varepsilon$ 以下之前可以传播的时间步数。我们可以通过分析时间 $t$ 的隐藏状态相对于过去时间 $t-L$ 的隐藏状态的雅可比矩阵 $\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-L}}$ 来将其形式化。\n\n在递归关系上使用链式法则：\n$$\n\\frac{\\partial \\mathbf{h}_k}{\\partial \\mathbf{h}_{k-1}} = \\frac{\\partial \\tanh(\\mathbf{z}_k)}{\\partial \\mathbf{z}_k} \\frac{\\partial \\mathbf{z}_k}{\\partial \\mathbf{h}_{k-1}} = \\text{diag}(1-\\tanh^2(\\mathbf{z}_k)) \\cdot \\mathbf{W}_{hh} = \\mathbf{D}_k \\mathbf{W}_{hh}\n$$\n其中 $\\mathbf{z}_k = \\mathbf{W}_{hh} \\mathbf{h}_{k-1} + \\dots$ 是时间 $k$ 的预激活值，$\\mathbf{D}_k$ 是激活导数的对角矩阵。完整的雅可比矩阵是这些项的乘积：\n$$\n\\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-L}} = \\frac{\\partial \\mathbf{h}_t}{\\partial \\mathbf{h}_{t-1}} \\frac{\\partial \\mathbf{h}_{t-1}}{\\partial \\mathbf{h}_{t-2}} \\cdots \\frac{\\partial \\mathbf{h}_{t-L+1}}{\\partial \\mathbf{h}_{t-L}} = \\prod_{k=t-L+1}^{t} (\\mathbf{D}_k \\mathbf{W}_{hh})\n$$\n这个雅可比矩阵的范数界定了影响的大小。为了获得一个易于处理的估计器，我们做了两个近似。首先，我们将时变矩阵 $\\mathbf{D}_k$ 替换为标量矩阵近似 $\\bar{d} \\mathbf{I}$，其中 $\\bar{d}$ 是导数 $1-\\tanh^2(z)$ 在所有隐藏单元和时间步上的平均值，该值从训练数据样本中估计得出。其次，我们利用这样一个性质：对于大的 $L$，矩阵幂的范数 $\\|(\\bar{d}\\mathbf{W}_{hh})^L\\|$ 由其谱半径 $\\rho(\\bar{d}\\mathbf{W}_{hh})^L$ 主导。\n因此，经过 $L$ 步的影响衰减因子近似为 $(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))^L$。我们寻求使该值小于或等于 $\\varepsilon$ 的最小整数 $L$：\n$$\n(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))^L \\le \\varepsilon\n$$\n对两边取对数求解 $L$ 可得：\n$$\nL \\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh})) \\le \\log(\\varepsilon)\n$$\n由于 $\\rho(\\mathbf{W}_{hh}) \\le \\rho_{\\max} = 0.7  1$ 且 $\\bar{d} \\le 1$，项 $\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh})$ 小于1，使其对数为负。除以这个负数会反转不等号：\n$$\nL \\ge \\frac{\\log(\\varepsilon)}{\\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))}\n$$\n有效记忆长度 $L_\\varepsilon$ 的估计器是满足此条件的最小整数：\n$$\nL_{\\varepsilon} = \\left\\lceil \\frac{\\log(\\varepsilon)}{\\log(\\bar{d} \\cdot \\rho(\\mathbf{W}_{hh}))} \\right\\rceil\n$$\n在实现中，我们使用 $\\varepsilon = 0.05$。一个更大的 $L_\\varepsilon$ 意味着网络可以更长时间地维持信息。\n\n### 4. 失败预测与评估\n\n二进制加法任务的记忆需求由其最长进位链决定。一个输入对的“进位链长度”是真实进位输出比特 $c_t$ 为1的连续时间步的最大数量。为了让网络正确执行加法，其有效记忆长度 $L_{\\varepsilon}$ 必须至少与所需的进位链长度 $L_{\\text{carry}}$ 一样长。这给了我们一个预测规则：\n- 如果 $L_{\\varepsilon} \\ge L_{\\text{carry}}$，预测“充分”。\n- 如果 $L_{\\varepsilon}  L_{\\text{carry}}$，预测“不充分”。\n\n然后将这个预测与训练好的RNN在测试用例上的实际性能进行比较。经验正确性通过以下方式确定：将测试整数的 $T=12$ 比特表示输入网络，将其输出 $(y_{t,s}, y_{t,c})$ 二值化以获得预测比特 $(s'_t, c'_t)$，计算预测的整数和 $S_{\\text{pred}}$，并将其与真实和 $S_{\\text{true}}$ 进行比较。\n$$\nS_{\\text{pred}} = \\sum_{t=0}^{T-1} s'_t 2^t + c'_{T-1} 2^T\n$$\n当且仅当 $S_{\\text{pred}} = S_{\\text{true}}$ 时，网络的性能被视为正确。最终输出汇总了给定测试套件的这些比较结果。", "answer": "```python\nimport numpy as np\nimport scipy.linalg\nimport math\n\n# Use a fixed random seed for reproducibility of training.\nnp.random.seed(42)\n\ndef solve():\n    \"\"\"\n    Main function to construct, train, and evaluate the RNN for binary addition.\n    \"\"\"\n\n    # --- Configuration ---\n    # RNN architecture\n    INPUT_DIM = 2\n    HIDDEN_DIM = 4  # A small hidden size is sufficient for this task.\n    OUTPUT_DIM = 2\n\n    # Training parameters\n    LEARNING_RATE = 0.05\n    N_EPOCHS = 3000\n    GRAD_CLIP_THRESHOLD = 1.0\n    T_TRAIN = 8  # Train on 8-bit numbers.\n\n    # Memory analysis parameters\n    RHO_MAX = 0.7\n    EPSILON = 0.05\n\n    # Testing parameters\n    T_TEST = 12 # Test on 12-bit numbers.\n\n    # --- Helper Functions ---\n    def int_to_binary_array(n, num_bits):\n        binary_str = format(n, f'0{num_bits}b')\n        return np.array([int(bit) for bit in reversed(binary_str)], dtype=np.float64)\n\n    def get_true_targets(a_bits, b_bits):\n        seq_len = len(a_bits)\n        s_bits = np.zeros(seq_len, dtype=np.float64)\n        c_bits = np.zeros(seq_len, dtype=np.float64)\n        carry_in = 0\n        for t in range(seq_len):\n            u_t = a_bits[t] + b_bits[t] + carry_in\n            s_bits[t] = u_t % 2\n            c_bits[t] = u_t // 2\n            carry_in = c_bits[t]\n        return s_bits, c_bits\n\n    class RNN:\n        \"\"\"A simple Elman Recurrent Neural Network.\"\"\"\n        def __init__(self, input_dim, hidden_dim, output_dim):\n            # Xavier/Glorot initialization for weights\n            limit_xh = np.sqrt(6.0 / (input_dim + hidden_dim))\n            limit_hh = np.sqrt(6.0 / (hidden_dim + hidden_dim))\n            limit_hy = np.sqrt(6.0 / (hidden_dim + output_dim))\n            \n            self.W_xh = np.random.uniform(-limit_xh, limit_xh, (hidden_dim, input_dim))\n            self.W_hh = np.random.uniform(-limit_hh, limit_hh, (hidden_dim, hidden_dim))\n            self.W_hy = np.random.uniform(-limit_hy, limit_hy, (output_dim, hidden_dim))\n            \n            self.b_h = np.zeros((hidden_dim, 1))\n            self.b_y = np.zeros((output_dim, 1))\n\n        def _sigmoid(self, x):\n            return 1.0 / (1.0 + np.exp(-x))\n\n        def forward(self, x_seq):\n            seq_len = x_seq.shape[0]\n            h = np.zeros((self.W_hh.shape[0], 1))\n            \n            history = {\n                'h': { -1: h }, # Store h_t values, key is time index\n                'y': [],\n                'deriv_activations': []\n            }\n            \n            for t in range(seq_len):\n                x_t = x_seq[t].reshape(-1, 1)\n                h_pre_act = self.W_xh @ x_t + self.W_hh @ h + self.b_h\n                h = np.tanh(h_pre_act)\n                o_t = self.W_hy @ h + self.b_y\n                y_t = self._sigmoid(o_t)\n                \n                history['h'][t] = h\n                history['y'].append(y_t)\n                history['deriv_activations'].append(1 - h**2)\n                \n            return history\n\n        def rescale_W_hh(self, rho_max):\n            eigenvalues = scipy.linalg.eigvals(self.W_hh)\n            rho = np.max(np.abs(eigenvalues))\n            if rho > rho_max:\n                self.W_hh *= rho_max / rho\n\n    def train_rnn(model, num_epochs, seq_len, learning_rate, clip_threshold, rho_max):\n        \"\"\"Train the RNN model using SGD and BPTT.\"\"\"\n        for epoch in range(num_epochs):\n            # Generate a random training sample\n            a_int = np.random.randint(0, 2**seq_len)\n            b_int = np.random.randint(0, 2**seq_len)\n            a_bits = int_to_binary_array(a_int, seq_len)\n            b_bits = int_to_binary_array(b_int, seq_len)\n            x_seq = np.vstack((a_bits, b_bits)).T\n            s_true, c_true = get_true_targets(a_bits, b_bits)\n            y_true_seq = np.vstack((s_true, c_true)).T\n\n            # Forward pass\n            history = model.forward(x_seq)\n            \n            # --- Backward Pass (BPTT) ---\n            dW_xh, dW_hh, dW_hy = np.zeros_like(model.W_xh), np.zeros_like(model.W_hh), np.zeros_like(model.W_hy)\n            db_h, db_y = np.zeros_like(model.b_h), np.zeros_like(model.b_y)\n            \n            delta_h_future = np.zeros_like(model.b_h)\n            \n            for t in reversed(range(seq_len)):\n                y_pred_t = history['y'][t]\n                y_true_t = y_true_seq[t].reshape(-1, 1)\n                h_t = history['h'][t]\n                h_prev = history['h'][t - 1]\n                x_t = x_seq[t].reshape(-1, 1)\n                deriv_act_t = history['deriv_activations'][t]\n\n                delta_o_t = y_pred_t - y_true_t\n                dW_hy += delta_o_t @ h_t.T\n                db_y += delta_o_t\n                \n                delta_h_from_output = model.W_hy.T @ delta_o_t\n                delta_h_total = delta_h_from_output + delta_h_future\n                \n                delta_pre_act_h = delta_h_total * deriv_act_t\n                \n                dW_hh += delta_pre_act_h @ h_prev.T\n                dW_xh += delta_pre_act_h @ x_t.T\n                db_h += delta_pre_act_h\n                \n                delta_h_future = model.W_hh.T @ delta_pre_act_h\n            \n            # Gradient clipping\n            grads = [dW_xh, dW_hh, dW_hy, db_h, db_y]\n            total_norm = np.sqrt(sum(np.sum(g**2) for g in grads))\n            if total_norm > clip_threshold:\n                for g in grads:\n                    g *= clip_threshold / total_norm\n\n            # SGD update\n            model.W_xh -= learning_rate * dW_xh\n            model.W_hh -= learning_rate * dW_hh\n            model.W_hy -= learning_rate * dW_hy\n            model.b_h -= learning_rate * db_h\n            model.b_y -= learning_rate * db_y\n            \n            # Enforce spectral radius constraint\n            model.rescale_W_hh(rho_max)\n        \n        return model\n\n    def calculate_memory_length(model, rho_max, epsilon):\n        # Estimate average activation derivative\n        num_samples = 100\n        all_deriv_activations = []\n        for _ in range(num_samples):\n            a_int = np.random.randint(0, 2**T_TRAIN)\n            b_int = np.random.randint(0, 2**T_TRAIN)\n            a_bits = int_to_binary_array(a_int, T_TRAIN)\n            b_bits = int_to_binary_array(b_int, T_TRAIN)\n            x_seq = np.vstack((a_bits, b_bits)).T\n            history = model.forward(x_seq)\n            all_deriv_activations.extend([d.flatten() for d in history['deriv_activations']])\n        \n        d_avg_scalar = np.mean(np.concatenate(all_deriv_activations))\n        \n        rho_W_hh = np.max(np.abs(scipy.linalg.eigvals(model.W_hh)))\n        \n        # Calculate L_epsilon\n        log_numerator = np.log(epsilon)\n        decay_factor = d_avg_scalar * rho_W_hh\n        \n        if decay_factor >= 1.0:\n            return float('inf')\n        if decay_factor = 0:\n            return 1\n            \n        log_denominator = np.log(decay_factor)\n        L_eps = math.ceil(log_numerator / log_denominator)\n        return L_eps\n\n    def calculate_carry_chain_length(A, B, num_bits):\n        a_bits = int_to_binary_array(A, num_bits)\n        b_bits = int_to_binary_array(B, num_bits)\n        _, c_bits = get_true_targets(a_bits, b_bits)\n        \n        max_len = 0\n        current_len = 0\n        for bit in c_bits:\n            if bit == 1:\n                current_len += 1\n            else:\n                max_len = max(max_len, current_len)\n                current_len = 0\n        max_len = max(max_len, current_len)\n        return max_len\n\n    # --- Main Execution ---\n    rnn = RNN(INPUT_DIM, HIDDEN_DIM, OUTPUT_DIM)\n    \n    # Train the RNN\n    trained_rnn = train_rnn(rnn, N_EPOCHS, T_TRAIN, LEARNING_RATE, GRAD_CLIP_THRESHOLD, RHO_MAX)\n    \n    # Analyze the trained network's memory\n    L_epsilon = calculate_memory_length(trained_rnn, RHO_MAX, EPSILON)\n    \n    # Test Suite\n    test_cases = [\n        (25, 6),\n        (255, 1),\n        (1023, 1),\n        (0, 0),\n        (4095, 1),\n    ]\n\n    results = []\n    for A, B in test_cases:\n        # 1. Predict sufficiency based on memory length\n        L_carry = calculate_carry_chain_length(A, B, T_TEST)\n        p_sufficient = (L_epsilon >= L_carry)\n        \n        # 2. Evaluate actual performance\n        a_bits = int_to_binary_array(A, T_TEST)\n        b_bits = int_to_binary_array(B, T_TEST)\n        x_seq = np.vstack((a_bits, b_bits)).T\n        \n        history = trained_rnn.forward(x_seq)\n        \n        s_pred_bits = np.round([y[0,0] for y in history['y']])\n        c_pred_bits = np.round([y[1,0] for y in history['y']])\n        \n        S_pred = 0\n        for t in range(T_TEST):\n            S_pred += s_pred_bits[t] * (2**t)\n        # Add final carry-out\n        S_pred += c_pred_bits[T_TEST - 1] * (2**T_TEST)\n        \n        S_true = A + B\n        a_correct = (S_pred == S_true)\n        \n        results.extend([p_sufficient, a_correct])\n        \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3167589"}, {"introduction": "前面的练习展示了通过统计学习训练的 RNN 的能力与局限。然而，RNN 的计算能力远不止于此，它们在理论上可以精确地模拟像有限状态自动机（FSA）这样的计算模型。本练习 [@problem_id:3167650] 将引导你摒弃梯度下降，转而像工程师一样精确地构建一个 RNN。你的任务是手动设置网络权重，使其完美模拟一个用于识别特定正则语言的 FSA，并分析其隐藏状态如何以几何形式清晰地表征自动机的离散状态。", "problem": "您的任务是构建一个确定性循环神经网络，该网络精确地实现一个用于正则形式语言的确定性有限自动机，并分析其隐藏状态嵌入的几何结构。请以纯数学术语进行工作，并从以下基础出发推导您的构造：确定性有限自动机、具有固定单元的循环神经网络的定义，以及修正线性单元（ReLU）和通过最大值参数（argmax）的硬决策规则的定义。不要使用任何统计估计或训练；您的构造必须是显式的和精确的。\n\n考虑字母表 $\\Sigma = \\{a,b\\}$ 上的正则语言，定义如下\n$$\nL \\;=\\; \\{\\, w \\in \\{a,b\\}^\\ast \\;:\\; w \\text{ is a concatenation of zero or more repetitions of the substring } ab \\,\\},\n$$\n即 $L = (ab)^\\ast$。将确定性有限自动机定义为一个元组 $(\\mathcal{Q},\\Sigma,\\delta,q_0,\\mathcal{F})$，包含以下组件：\n- 状态集 $\\mathcal{Q} = \\{q_0,q_1,q_d\\}$，其中 $q_0$ 是起始状态，$q_1$ 是读取一个开始块的 $a$ 之后的状态，$q_d$ 是一个捕获任何无效延续的死亡状态。\n- 输入字母表 $\\Sigma = \\{a,b\\}$。\n- 转移函数 $\\delta: \\mathcal{Q}\\times\\Sigma \\to \\mathcal{Q}$ 的逻辑定义如下：\n  - $\\delta(q_0,a)=q_1$, $\\delta(q_0,b)=q_d$,\n  - $\\delta(q_1,a)=q_d$, $\\delta(q_1,b)=q_0$,\n  - $\\delta(q_d,a)=q_d$, $\\delta(q_d,b)=q_d$。\n- 起始状态 $q_0$。\n- 接受状态集 $\\mathcal{F}=\\{q_0\\}$。\n\n构建一个循环神经网络，该网络一次处理一个符号，并通过以下设计决策精确地模拟此自动机：\n- 将时间 $t$ 的 RNN 隐藏状态表示为 $h_t \\in \\mathbb{R}^3$，并约束其为当前自动机状态的独热向量，即 $h_t \\in \\{e_0,e_1,e_2\\}$，其中 $e_i$ 是 $\\mathbb{R}^3$ 的第 $i$ 个标准基向量，对应关系为 $e_0 \\leftrightarrow q_0$, $e_1 \\leftrightarrow q_1$, $e_2 \\leftrightarrow q_d$。\n- 将时间 $t$ 的输入符号表示为 $x_t \\in \\mathbb{R}^2$，它是一个独热向量 $x_t \\in \\{u_a,u_b\\}$，其中 $u_a=(1,0)^\\top$ 和 $u_b=(0,1)^\\top$ 分别对应于 $a$ 和 $b$。\n- 定义一个双层单元，它接受拼接向量 $u_t = [h_{t-1}; x_t] \\in \\mathbb{R}^{5}$，计算中间激活值 $g_t = \\mathrm{ReLU}(W_1 u_t + b_1) \\in \\mathbb{R}^{6}$，计算 logits $z_t = W_2 g_t + b_2 \\in \\mathbb{R}^3$，并将下一个隐藏状态设置为独热向量 $h_t = \\text{one\\_hot}(\\arg\\max_k z_t[k])$。此处，$\\mathrm{ReLU}(s)=\\max\\{0,s\\}$ 是按元素应用的。\n\n您的任务：\n1. 指定 $W_1 \\in \\mathbb{R}^{6\\times 5}$ 和 $b_1 \\in \\mathbb{R}^{6}$ 的一个构造性选择，使得 $g_t$ 中的 6 个单元均为配对检测器，当且仅当特定配对 $(h_{t-1},x_t)$ 出现时，该检测器严格正激活，否则为 $0$。在 $W_1$ 中仅使用集合 $\\{-1,0,1\\}$ 中的常数，并对所有 6 个隐藏单元使用相同的常数偏置 $b_1$。\n2. 指定 $W_2 \\in \\mathbb{R}^{3\\times 6}$ 和 $b_2 \\in \\mathbb{R}^{3}$ 的一个构造性选择，使得对于每个有效配对 $(h_{t-1},x_t)$，对三个 logits $z_t$ 的 argmax 操作能精确地产生 $\\delta(q,x)$ 的索引，从而保证 $h_t = e_{\\delta(q,x)}$。\n3. 根据上述定义的第一性原理，证明对于任何有限长度的输入序列，在处理任何前缀后，您的构造所产生的 $h_t$ 等于自动机状态的精确独热编码。您的证明不应使用概率性论证；它必须遵循 $\\mathrm{ReLU}$ 的代数形式和 argmax 规则。\n4. 在您的构造下，分析隐藏状态嵌入 $\\{h_t\\}$ 的几何结构。描述三个状态嵌入之间的两两欧几里得距离，并论证以相同自动机状态结束的不同前缀的嵌入是否重合。\n\n程序规范和测试套件：\n- 您的程序必须使用固定的数值数组 $W_1$、$b_1$、$W_2$ 和 $b_2$ 来实现您的显式构造，必须处理一批在 $\\{a,b\\}$ 上的测试字符串，并且必须计算两个输出：\n  1. 以下 10 个测试字符串的接受结果，顺序如下：空字符串 $\\varepsilon$、$ab$、$abab$、$a$、$aba$、$abb$、$b$、$ba$、$aaa$、$ababa$。将每个接受结果报告为一个布尔值，其中布尔值以该语言的标准布尔字面量形式打印，接受意味着最终状态在 $\\mathcal{F}$ 中。\n  2. 三个状态嵌入之间的三个欧几里得距离列表，顺序为 $d(q_0,q_1)$、$d(q_0,q_d)$、$d(q_1,q_d)$。通过计算所有长度不超过 4 的字符串的隐藏状态 $h_T$，并对那些以相同状态结束的 $h_T$ 进行平均以获得每个状态的质心，然后计算这三个质心之间的欧几里得距离，来经验性地估计这些距离。将这些距离表示为十进制浮点数。\n- 最终输出格式：您的程序应生成单行输出，包含一个有两个元素的顶层列表，其中第一个元素是按上述顺序列出的 10 个布尔值的列表，第二个元素是按指定顺序列出的三个浮点数的列表。例如，您的输出必须如下所示\n  $$\n  [ [\\text{True}, \\ldots, \\text{False}], [d_{01}, d_{0d}, d_{1d}] ].\n  $$\n程序不提供外部输入，也不应读取或写入任何外部文件。所有结果必须在内存中计算并严格按照规定打印。此任务不涉及物理单位或角度，因此不需要额外的单位约定。您推导和构造中出现的所有数字必须与上面提供的定义一致。", "solution": "该问题要求构建一个循环神经网络（RNN），该网络确定性地模拟一个给定的、用于正则语言 $L = (ab)^\\ast$ 的确定性有限自动机（DFA）。该构造必须是精确的，并从第一性原理推导得出。\n\n用于 $L=(ab)^\\ast$ 的 DFA 由 $(\\mathcal{Q}, \\Sigma, \\delta, q_0, \\mathcal{F})$ 给出，其中：\n- 状态集 $\\mathcal{Q} = \\{q_0, q_1, q_d\\}$。\n- 字母表 $\\Sigma = \\{a, b\\}$。\n- 转移函数 $\\delta$：$\\delta(q_0,a)=q_1$，$\\delta(q_0,b)=q_d$，$\\delta(q_1,a)=q_d$，$\\delta(q_1,b)=q_0$，$\\delta(q_d,a)=q_d$，$\\delta(q_d,b)=q_d$。\n- 起始状态是 $q_0$。\n- 接受状态集 $\\mathcal{F} = \\{q_0\\}$。\n\nRNN 必须对状态和输入使用独热编码。状态 $q_0, q_1, q_d$ 分别映射到 $\\mathbb{R}^3$ 中的标准基向量 $e_0=(1,0,0)^\\top$、$e_1=(0,1,0)^\\top$ 和 $e_2=(0,0,1)^\\top$。输入符号 $a,b$ 映射到 $\\mathbb{R}^2$ 中的 $u_a=(1,0)^\\top$ 和 $u_b=(0,1)^\\top$。初始隐藏状态为 $h_0 = e_0$，对应于起始状态 $q_0$。\n\n从状态 $h_{t-1}$ 和输入 $x_t$ 到下一状态 $h_t$ 的 RNN 更新规则是：\n$u_t = [h_{t-1}; x_t] \\in \\mathbb{R}^5$\n$g_t = \\mathrm{ReLU}(W_1 u_t + b_1) \\in \\mathbb{R}^6$\n$z_t = W_2 g_t + b_2 \\in \\mathbb{R}^3$\n$h_t = \\text{one\\_hot}(\\arg\\max_k z_t[k])$\n\n我们现在将指定权重矩阵 $W_1, W_2$ 和偏置向量 $b_1, b_2$ 以实现 DFA 的逻辑。\n\n**1. $W_1$ 和 $b_1$ 的构造**\n\n第一层的目的是创建六个“配对检测器”神经元，每个神经元对应于 $\\mathcal{Q} \\times \\Sigma$ 中六个可能的（状态，输入）配对之一。让中间层 $g_t$ 中的神经元按以下顺序对应这些配对：$(q_0, a)$、$(q_0, b)$、$(q_1, a)$、$(q_1, b)$、$(q_d, a)$、$(q_d, b)$。一个用于配对 $(q_i, \\text{sym}_j)$（其中 $\\text{sym}_0=a, \\text_sym_1=b$）的检测器必须当且仅当上一个状态是 $h_{t-1}=e_i$ 且当前输入是 $x_t=u_{\\text{sym}_j}$ 时激活（即输出一个大于 0 的值）。\n\n拼接输入向量为 $u_t = (h_{t-1}[0], h_{t-1}[1], h_{t-1}[2], x_t[0], x_t[1])^\\top$。对于一个特定的输入配对 $(e_i, u_{\\text{sym}_j})$，向量 $u_t$ 在索引 $i$ 和索引 $3+j$ 处的值为 1，在各自的独热块中的其他位置为零。例如，配对 $(q_0, a)$ 对应于 $h_{t-1}=e_0$ 和 $x_t=u_a$，因此 $u_t=(1,0,0,1,0)^\\top$。\n\n我们来设计第 $k$ 个神经元来检测对应于状态索引 $i \\in \\{0,1,2\\}$ 和输入索引 $j \\in \\{0,1\\}$ 的配对。我们希望预激活值 $w_{1,k}^\\top u_t + b_{1,k}$ 对于目标配对是严格正的，而对于所有其他配对是小于或等于零的。\n实现这一点的一个简单方法是，将对应于目标配对的分量相加，然后减去一个偏置。让 $W_1$ 的第 $k$ 行是一个在对应于状态 $i$ 和输入 $j$ 的位置上为 1、其他位置为 0 的向量。对于配对 $(e_i, u_j)$，预激活值将是 $u_t[i] + u_t[3+j]$。\n- 对于目标配对，此和为 $1+1=2$。\n- 对于具有相同状态但不同输入的配对，此和为 $1+0=1$。\n- 对于具有不同状态但相同输入的配对，此和为 $0+1=1$。\n- 对于具有不同状态和不同输入的配对，此和为 $0+0=0$。\n\n我们被要求对所有单元使用一个常数偏置 $b_1$。如果我们将此偏置设置为 -1，则预激活值变为：\n- 目标配对：$2-1=1  0$，因此 $\\mathrm{ReLU}(1)=1$。\n- 不匹配的配对：$1-1=0$，因此 $\\mathrm{ReLU}(0)=0$。\n- 完全不同的配对：$0-1=-1$，因此 $\\mathrm{ReLU}(-1)=0$。\n\n这个构造完美有效。矩阵 $W_1 \\in \\mathbb{R}^{6 \\times 5}$是通过为 6 个配对 $(q_i, \\text{sym}_j)$ 中的每一个创建一个行来构造的：\n$$\nW_1 =\n\\begin{pmatrix}\n1  0  0  1  0 \\\\  % (q0, a) 检测器\n1  0  0  0  1 \\\\  % (q0, b) 检测器\n0  1  0  1  0 \\\\  % (q1, a) 检测器\n0  1  0  0  1 \\\\  % (q1, b) 检测器\n0  0  1  1  0 \\\\  % (qd, a) 检测器\n0  0  1  0  1     % (qd, b) 检测器\n\\end{pmatrix}\n$$\n偏置向量 $b_1 \\in \\mathbb{R}^6$ 是：\n$$\nb_1 = (-1, -1, -1, -1, -1, -1)^\\top\n$$\n通过这种构造，对于任何有效的独热输入 $h_{t-1}$ 和 $x_t$，向量 $g_t$ 将是一个独热向量，其中唯一的 1 表示六种可能的转移中发生了哪一种。\n\n**2. $W_2$ 和 $b_2$ 的构造**\n\n第二层必须将 $g_t$ 中激活的配对检测器映射到下一个状态。向量 $g_t$ 是一个指示转移的独热向量。logits $z_t = W_2 g_t + b_2$ 必须使得 $\\arg\\max_k z_t[k]$ 能够根据 DFA 的转移函数 $\\delta$ 给出下一个状态的索引。\n\n由于 $g_t$ 是一个在索引 $k$ 处为 1 的独热向量，乘积 $W_2 g_t$ 就是 $W_2$ 的第 $k$ 列。为简单起见，我们可以选择 $b_2 = (0,0,0)^\\top$。因此，我们需要设计 $W_2$ 的列，使得每一列的 argmax 对应于正确的下一个状态索引。一个可靠的方法是使 $W_2$ 的每一列成为所需下一个状态的独热编码。\n\nDFA 转移和对应的下一个状态索引是：\n- $\\delta(q_0, a) = q_1$ (索引 $1$)。这对应于 $g_t$ 的第 1 个分量。\n- $\\delta(q_0, b) = q_d$ (索引 $2$)。这对应于 $g_t$ 的第 2 个分量。\n- $\\delta(q_1, a) = q_d$ (索引 $2$)。这对应于 $g_t$ 的第 3 个分量。\n- $\\delta(q_1, b) = q_0$ (索引 $0$)。这对应于 $g_t$ 的第 4 个分量。\n- $\\delta(q_d, a) = q_d$ (索引 $2$)。这对应于 $g_t$ 的第 5 个分量。\n- $\\delta(q_d, b) = q_d$ (索引 $2$)。这对应于 $g_t$ 的第 6 个分量。\n\n$W_2$ 的列是下一个状态的独热向量：\n- 第 0 列对应 $(q_0, a) \\to q_1$：$(0,1,0)^\\top$\n- 第 1 列对应 $(q_0, b) \\to q_d$：$(0,0,1)^\\top$\n- 第 2 列对应 $(q_1, a) \\to q_d$：$(0,0,1)^\\top$\n- 第 3 列对应 $(q_1, b) \\to q_0$：$(1,0,0)^\\top$\n- 第 4 列对应 $(q_d, a) \\to q_d$：$(0,0,1)^\\top$\n- 第 5 列对应 $(q_d, b) \\to q_d$：$(0,0,1)^\\top$\n\n所以，矩阵 $W_2 \\in \\mathbb{R}^{3 \\times 6}$ 是：\n$$\nW_2 =\n\\begin{pmatrix}\n0  0  0  1  0  0 \\\\  % q0 的 logit\n1  0  0  0  0  0 \\\\  % q1 的 logit\n0  1  1  0  1  1     % qd 的 logit\n\\end{pmatrix}\n$$\n并且偏置向量 $b_2 \\in \\mathbb{R}^3$ 是：\n$$\nb_2 = (0, 0, 0)^\\top\n$$\n通过这种构造，如果 $g_t$ 是独热向量 $e_k$（在 $\\mathbb{R}^6$ 中），那么 $z_t = W_2 e_k = (W_2 \\text{ 的第 k 列})$，这是正确的下一个状态的独热向量。$\\arg\\max$ 将唯一地确定这个下一个状态。\n\n**3. 正确性证明**\n\n我们通过对输入字符串 $w$ 的长度进行归纳来证明 RNN 的隐藏状态 $h_t$ 是处理完 $w$ 后 DFA 状态的独热编码。\n- **基础情况 ($t=0$)：**对于空字符串 $\\varepsilon$，RNN 初始化为 $h_0 = e_0 = (1,0,0)^\\top$。这正确地表示了 DFA 的起始状态 $q_0$。\n- **归纳假设：**假设在处理长度为 $t-1$ 的前缀 $w'$ 后，RNN 状态 $h_{t-1}$ 为 $e_i$，其中 $i$ 是 DFA 当前状态 $q' = \\delta^\\ast(q_0, w')$ 的索引。\n- **归纳步骤：**设下一个输入符号为 $s_t$。DFA 转移到状态 $q_{new} = \\delta(q', s_t)$。设 $q_{new}$ 的索引为 $m$。我们需要证明 RNN 转移到状态 $h_t = e_m$。\n1. RNN 单元接收输入 $u_t = [h_{t-1}; x_t]$，其中 $h_{t-1} = e_i$，$x_t$ 是 $s_t$ 的独热编码。\n2. 如第 1 部分所示，中间激活 $g_t = \\mathrm{ReLU}(W_1 u_t + b_1)$ 将是一个独热向量 $e_k \\in \\mathbb{R}^6$，其中 $k$ 是对应于配对 $(q', s_t)$ 的索引。\n3. logits 计算为 $z_t = W_2 g_t + b_2$。由于 $g_t=e_k$ 且 $b_2=0$，$z_t$ 是 $W_2$ 的第 $k$ 列。\n4. 如第 2 部分所示，$W_2$ 的第 $k$ 列被构造成 $e_m$，即下一个状态 $q_{new}$ 的独热编码。因此，$z_t = e_m$。\n5. 新的隐藏状态是 $h_t = \\text{one\\_hot}(\\arg\\max_l z_t[l])$。由于 $z_t = e_m$，最大值在索引 $m$ 处为 1。因此，$\\arg\\max_l z_t[l] = m$。\n6. 应用独热函数产生 $h_t = e_m$，这是新 DFA 状态 $q_{new}$ 的正确独热编码。\n\n根据数学归纳法原理，对于任何有限长度的输入字符串，该 RNN 都能正确模拟 DFA。\n\n**4. 隐藏状态嵌入的几何结构**\n\n隐藏状态嵌入被约束为 $\\mathbb{R}^3$ 中的三个标准基向量之一：$\\{e_0, e_1, e_2\\}$。\n- $h(q_0) = e_0 = (1,0,0)^\\top$\n- $h(q_1) = e_1 = (0,1,0)^\\top$\n- $h(q_d) = e_2 = (0,0,1)^\\top$\n\n这三个向量在 $\\mathbb{R}^3$ 中构成一个等边三角形的顶点。两两之间的欧几里得距离是：\n- $d(q_0, q_1) = ||e_0 - e_1||_2 = ||(1, -1, 0)^\\top||_2 = \\sqrt{1^2 + (-1)^2 + 0^2} = \\sqrt{2}$。\n- $d(q_0, q_d) = ||e_0 - e_2||_2 = ||(1, 0, -1)^\\top||_2 = \\sqrt{1^2 + 0^2 + (-1)^2} = \\sqrt{2}$。\n- $d(q_1, q_d) = ||e_1 - e_2||_2 = ||(0, 1, -1)^\\top||_2 = \\sqrt{0^2 + 1^2 + (-1)^2} = \\sqrt{2}$。\n\n关于以相同自动机状态结束的前缀：我们的证明表明，RNN 在处理任何前缀后的隐藏状态*仅*由该前缀的 DFA 最终状态决定。例如，前缀 $\\varepsilon$ 和 $ab$ 都使 DFA 进入状态 $q_0$。在处理完这两个前缀后，RNN 的隐藏状态将完全相同，都是 $e_0$。同样，前缀 $a$ 和 $aba$ 都导致状态 $q_1$，它们最终的 RNN 隐藏状态都将是 $e_1$。\n\n因此，导致相同自动机状态的不同前缀的嵌入不仅仅是聚集在一起；它们是完全相同的。这是精确、确定性构造的直接结果，与统计训练的 RNN 形成对比，后者的状态表示会形成分布或簇，而不是离散的点。因此，问题中要求的“经验估计”将得出这些精确的几何结果，因为相同向量的平均值就是该向量本身。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes an RNN that simulates a DFA for the language (ab)*.\n    \"\"\"\n    # Task 1  2: Define the weight matrices and bias vectors for the RNN.\n    # W1 and b1 create 6 pair-detectors for (state, symbol) pairs.\n    # The order of detectors is (q0,a), (q0,b), (q1,a), (q1,b), (qd,a), (qd,b).\n    W1 = np.array([\n        [1, 0, 0, 1, 0],  # (q0, a)\n        [1, 0, 0, 0, 1],  # (q0, b)\n        [0, 1, 0, 1, 0],  # (q1, a)\n        [0, 1, 0, 0, 1],  # (q1, b)\n        [0, 0, 1, 1, 0],  # (qd, a)\n        [0, 0, 1, 0, 1]   # (qd, b)\n    ], dtype=np.float64)\n    b1 = np.full((6,), -1.0, dtype=np.float64)\n\n    # W2 and b2 map the activated detector to the next state's one-hot encoding.\n    # DFA transitions: d(q0,a)=q1, d(q0,b)=qd, d(q1,a)=qd, d(q1,b)=q0, d(qd,.)=qd\n    # Next state indices: q1=1, qd=2, qd=2, q0=0, qd=2, qd=2\n    W2 = np.array([\n        [0, 0, 0, 1, 0, 0],  # Logit for q0\n        [1, 0, 0, 0, 0, 0],  # Logit for q1\n        [0, 1, 1, 0, 1, 1]   # Logit for qd\n    ], dtype=np.float64)\n    b2 = np.zeros(3, dtype=np.float64)\n\n    # Input symbol encodings\n    symbol_map = {\n        'a': np.array([1, 0], dtype=np.float64),\n        'b': np.array([0, 1], dtype=np.float64)\n    }\n\n    # States: q0 (index 0), q1 (index 1), qd (index 2)\n    # Accepting state is q0.\n    accepting_idx = 0\n\n    def relu(x):\n        return np.maximum(0, x)\n    \n    def one_hot(index, size):\n        vec = np.zeros(size)\n        vec[index] = 1.0\n        return vec\n\n    def run_rnn(input_string):\n        \"\"\"Processes a string and returns the final state index.\"\"\"\n        # Initial state is q0\n        h = np.array([1, 0, 0], dtype=np.float64)\n        \n        for char in input_string:\n            x = symbol_map[char]\n            u = np.concatenate((h, x))\n            \n            g = relu(W1 @ u + b1)\n            z = W2 @ g + b2\n            \n            next_state_idx = np.argmax(z)\n            h = one_hot(next_state_idx, 3)\n            \n        final_state_idx = np.argmax(h)\n        return final_state_idx\n\n    # Part 1: Acceptance results\n    test_strings = [\"\", \"ab\", \"abab\", \"a\", \"aba\", \"abb\", \"b\", \"ba\", \"aaa\", \"ababa\"]\n    acceptance_results = []\n    for s in test_strings:\n        final_state = run_rnn(s)\n        acceptance_results.append(final_state == accepting_idx)\n\n    # Part 2: Euclidean distances between state embeddings\n    # Generate all strings of length 0 to 4\n    alphabet = ('a', 'b')\n    all_strings = []\n    for length in range(5):\n        for p in itertools.product(alphabet, repeat=length):\n            all_strings.append(\"\".join(p))\n    \n    # Store final hidden state vectors for each ending state\n    state_vectors = {0: [], 1: [], 2: []}\n    for s in all_strings:\n        final_state_idx = run_rnn(s)\n        final_h = one_hot(final_state_idx, 3)\n        state_vectors[final_state_idx].append(final_h)\n\n    # Compute centroids for each state\n    centroids = []\n    for i in range(3):\n        if state_vectors[i]:\n            centroids.append(np.mean(np.array(state_vectors[i]), axis=0))\n        else:\n            # Handle case where a state is unreachable by short strings\n            # In our case, all are reachable.\n            centroids.append(np.zeros(3))\n            \n    c0, c1, c2 = centroids[0], centroids[1], centroids[2]\n\n    # Compute pairwise Euclidean distances\n    dist_01 = np.linalg.norm(c0 - c1)\n    dist_02 = np.linalg.norm(c0 - c2)\n    dist_12 = np.linalg.norm(c1 - c2)\n    \n    distance_results = [dist_01, dist_02, dist_12]\n\n    # Combine results and print in the specified format\n    final_output = [acceptance_results, distance_results]\n    print(f\"[{[bool(b) for b in acceptance_results]}, {distance_results}]\")\n\nsolve()\n\n```", "id": "3167650"}]}