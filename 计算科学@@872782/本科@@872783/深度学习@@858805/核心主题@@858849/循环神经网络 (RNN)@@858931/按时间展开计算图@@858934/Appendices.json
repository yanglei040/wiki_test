{"hands_on_practices": [{"introduction": "循环网络能够“记忆”多久的过去信息？这个练习将带你超越定性讨论，通过推导一个量化指标——记忆时间常数——来回答这个问题。通过该实践，你将直接把网络结构参数与其在时间中保持信息的能力联系起来，从而具体地审视梯度消失问题的根源 [@problem_id:3197405]。", "problem": "考虑一个泄漏积分器循环神经网络 (RNN)，其隐藏状态动态由下式给出\n$$h_{t+1} = (1-\\alpha) h_t + \\alpha \\,\\phi(W h_t + U x_t),$$\n其中 $h_t \\in \\mathbb{R}$，$x_t \\in \\mathbb{R}$，$W \\in \\mathbb{R}$，$U \\in \\mathbb{R}$，$\\alpha \\in (0,1)$，并且 $\\phi$ 是一个二阶连续可微的激活函数。您将通过在时间上展开计算图来分析梯度流，并将结果与记忆与稳定性联系起来。\n\n假设标量情况，其中 $\\phi(z) = \\tanh(z)$，且对于所有 $t \\in \\{0,1,\\dots,T-1\\}$，输入恒为零，$x_t = 0$。设损失为\n$$L = \\frac{1}{2} h_T^2.$$\n假设轨迹保持在原点附近，因此对于所有 $t$，围绕 $h_t \\approx 0$ 的线性化是有效的，并进一步假设 $|(1-\\alpha) + \\alpha W|  1$，以使线性化的隐藏动态是稳定的。\n\n仅从链式法则和导数的定义出发，推导 $\\frac{\\partial h_T}{\\partial h_0}$ 的时间展开表达式，并用它通过关系式\n$$\\left|\\frac{\\partial h_T}{\\partial h_0}\\right| \\approx \\exp\\!\\left(-\\frac{T}{\\tau}\\right)$$\n定义一个特征记忆时间常数 $\\tau$（以时间步为单位）。\n将 $\\tau$ 表示为 $\\alpha$ 和 $W$ 的闭式函数。您的最终答案必须是单一的解析表达式。无需四舍五入。", "solution": "该问题要求在特定假设下，泄漏积分器循环神经网络 (RNN) 的特征记忆时间常数 $\\tau$。推导过程首先对系统动态进行线性化，然后通过在时间上展开计算图来计算最终状态相对于初始状态的导数，最后使用给定的 $\\tau$ 的定义。\n\n隐藏状态动态由下式给出：\n$$h_{t+1} = (1-\\alpha) h_t + \\alpha \\,\\phi(W h_t + U x_t)$$\n问题指定了以下条件：激活函数为 $\\phi(z) = \\tanh(z)$，并且在所有时间步 $t$ 输入恒为零，$x_t = 0$。将这些代入动态方程得到：\n$$h_{t+1} = (1-\\alpha) h_t + \\alpha \\tanh(W h_t)$$\n问题进一步假设系统轨迹保持在原点附近，使得对于所有 $t$ 都有 $h_t \\approx 0$。这允许对动态进行线性化。我们使用 $\\tanh$ 函数在 $z=0$ 附近的泰勒级数展开：\n$$\\tanh(z) = z - \\frac{z^3}{3} + O(z^5)$$\n对于接近 $0$ 的 $z$ 值，我们可以使用一阶近似 $\\tanh(z) \\approx z$。在我们的例子中，$\\tanh$ 函数的参数是 $W h_t$。由于 $h_t \\approx 0$，因此 $W h_t \\approx 0$。所以我们可以近似 $\\tanh(W h_t) \\approx W h_t$。\n\n将此近似代入动态方程，得到线性化的动态：\n$$h_{t+1} \\approx (1-\\alpha) h_t + \\alpha(W h_t)$$\n$$h_{t+1} \\approx ((1-\\alpha) + \\alpha W) h_t$$\n这是一个线性递推关系。问题陈述了线性化动态是稳定的，这由给定条件 $|(1-\\alpha) + \\alpha W|  1$ 保证。\n\n下一步是推导导数 $\\frac{\\partial h_T}{\\partial h_0}$ 的时间展开表达式。状态 $h_T$ 是通过 $T$ 次递归应用更新规则得到的初始状态 $h_0$ 的函数。使用链式法则，我们可以将此导数表示为单步导数的乘积：\n$$\\frac{\\partial h_T}{\\partial h_0} = \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_2}{\\partial h_1} \\frac{\\partial h_1}{\\partial h_0} = \\prod_{t=0}^{T-1} \\frac{\\partial h_{t+1}}{\\partial h_t}$$\n为了计算通项 $\\frac{\\partial h_{t+1}}{\\partial h_t}$，我们对完整的非线性动态方程关于 $h_t$ 求导：\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = \\frac{\\partial}{\\partial h_t} \\left[ (1-\\alpha) h_t + \\alpha \\tanh(W h_t) \\right]$$\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = (1-\\alpha) + \\alpha \\frac{\\partial}{\\partial h_t}(\\tanh(W h_t))$$\n使用链式法则以及 $\\tanh(z)$ 的导数是 $1 - \\tanh^2(z)$ 这一事实，我们得到：\n$$\\frac{\\partial}{\\partial h_t}(\\tanh(W h_t)) = W(1 - \\tanh^2(W h_t))$$\n因此，精确的导数是：\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} = (1-\\alpha) + \\alpha W (1 - \\tanh^2(W h_t))$$\n现在，我们应用线性化假设，$h_t \\approx 0$。当 $h_t$ 趋近于 $0$ 时，$W h_t$ 也趋近于 $0$，因此 $\\tanh(W h_t)$ 趋近于 $0$。所以 $\\tanh^2(W h_t)$ 项消失。导数简化为一个常数：\n$$\\frac{\\partial h_{t+1}}{\\partial h_t} \\approx (1-\\alpha) + \\alpha W (1 - 0) = (1-\\alpha) + \\alpha W$$\n这个常数是线性化系统的有效循环权重。我们将其表示为 $J = (1-\\alpha) + \\alpha W$。将这个常数近似值代回到完整导数的乘积中：\n$$\\frac{\\partial h_T}{\\partial h_0} \\approx \\prod_{t=0}^{T-1} J = J^T$$\n这就给出了导数的最终时间展开表达式：\n$$\\frac{\\partial h_T}{\\partial h_0} \\approx ((1-\\alpha) + \\alpha W)^T$$\n问题通过以下关系式定义了特征记忆时间常数 $\\tau$：\n$$\\left|\\frac{\\partial h_T}{\\partial h_0}\\right| \\approx \\exp\\left(-\\frac{T}{\\tau}\\right)$$\n代入我们推导出的导数表达式：\n$$\\left| ((1-\\alpha) + \\alpha W)^T \\right| \\approx \\exp\\left(-\\frac{T}{\\tau}\\right)$$\n使用性质 $|z^n| = |z|^n$：\n$$|(1-\\alpha) + \\alpha W|^T \\approx \\exp\\left(-\\frac{T}{\\tau}\\right)$$\n为了求解 $\\tau$，我们对两边取自然对数：\n$$\\ln\\left( |(1-\\alpha) + \\alpha W|^T \\right) \\approx \\ln\\left( \\exp\\left(-\\frac{T}{\\tau}\\right) \\right)$$\n$$T \\ln(|(1-\\alpha) + \\alpha W|) \\approx -\\frac{T}{\\tau}$$\n假设 $T$ 是一个代表时间步数的非零整数，我们可以将两边都除以 $T$：\n$$\\ln(|(1-\\alpha) + \\alpha W|) \\approx -\\frac{1}{\\tau}$$\n最后，求解 $\\tau$ 得到闭式表达式：\n$$\\tau \\approx -\\frac{1}{\\ln(|(1-\\alpha) + \\alpha W|)}$$\n基于在给定近似下的此推导，时间常数的表达式为：\n$$\\tau = -\\frac{1}{\\ln(|(1-\\alpha) + \\alpha W|)}$$\n稳定性条件 $|(1-\\alpha) + \\alpha W|  1$ 确保了对数的参数是一个小于 $1$ 的正数，这使得对数值为负。因此，$\\tau$ 表达式中的负号确保了 $\\tau$ 是一个正量，这符合对时间常数的预期。", "answer": "$$\\boxed{-\\frac{1}{\\ln(|(1-\\alpha) + \\alpha W|)}}$$", "id": "3197405"}, {"introduction": "完整的时间反向传播（BPTT）在计算上通常过于昂贵。本练习将探讨最常见的解决方案——截断时间反向传播（TBPTT），并揭示其根本局限性。通过一个简单的反例，你将证明这种实用算法对长期依赖是“盲目”的，从而理解为何需要像长短期记忆（LSTM）网络这样更复杂的架构 [@problem_id:3101258]。", "problem": "考虑一个由以下递推公式定义的标量循环神经网络\n$$h_{t} \\;=\\; a\\,h_{t-1} \\;+\\; b\\,x_{t}, \\qquad y_{t} \\;=\\; c\\,h_{t},$$\n其中 $a$、$b$ 和 $c$ 是固定的非零实数标量，$\\{x_{s}\\}$ 是一个标量输入序列。假设在时间 $t$ 的训练目标是单步平方误差\n$$L \\;=\\; \\frac{1}{2}\\,\\big(y_{t} - r\\big)^{2},$$\n其中 $r$ 是时间 $t$ 的一个固定标量目标。假设初始条件为 $h_{t-\\tau-1} = 0$，并且对于 $s \\le t-\\tau-1$ 的所有输入 $\\{x_{s}\\}$ 均为零，因此 $y_{t}$ 对 $x_{t-\\tau}$ 的任何依赖关系仅通过该递推产生。\n\n你使用截断时间反向传播（Truncated Backpropagation Through Time, TBPTT）算法训练该模型，仅展开 $k$ 步，即从时间 $t$ 反向传播到时间 $t-k+1$，并将 $h_{t-k}$ 视为相对于所有早于 $t-k+1$ 的变量的常数。设 $\\tau$ 为一个正整数，且满足 $\\tau  k \\ge 1$。\n\n任务：\n- 仅使用链式法则和以上定义，确定当 $\\tauk$ 时，通过窗口大小为 $k$ 的 TBPTT 计算出的截断梯度 $\\frac{\\partial L}{\\partial x_{t-\\tau}}$。\n- 简要解释为什么这构成了一个反例，表明即使在前向递推中存在滞后 $\\tauk$ 的依赖关系，窗口大小为 $k$ 的 TBPTT 也无法学习到这种依赖关系。\n- 简要讨论至少两种在不违反 TBPTT 截断约束的情况下解决此失效模式的有原则的补救措施。\n\n给出截断梯度 $\\frac{\\partial L}{\\partial x_{t-\\tau}}$ 的最终答案，形式为单个精确值或表达式。无需四舍五入。", "solution": "**解答**\n\n**第 1 部分：确定截断梯度 $\\frac{\\partial L}{\\partial x_{t-\\tau}}$**\n\n该 RNN 的训练使用截断时间反向传播（TBPTT），历史截断步数为 $k$。这意味着，对于在时间 $t$ 计算的损失 $L$，用于梯度计算的计算图从时间步 $t$ 向后展开到时间步 $t-k+1$。隐藏状态 $h_{t-k}$ 被视为这个截断图的输入，其对先前状态（$h_{t-k-1}$）和先前输入（$x_{t-k}$、$x_{t-k-1}$ 等）的依赖关系被切断。\n\n损失 $L$ 是输出 $y_t$ 的函数，而 $y_t$ 又是截断窗口内隐藏状态和输入序列的函数。具体来说，展开的计算表明 $y_t$（以及 $L$）是变量 $\\{h_{t-k}, x_{t-k+1}, x_{t-k+2}, \\dots, x_t\\}$ 的函数。我们将这个截断图计算的函数表示为 $L_{\\text{trunc}}$。\n$$L = L_{\\text{trunc}}(h_{t-k}, x_{t-k+1}, \\dots, x_t)$$\n我们需要计算截断梯度 $\\frac{\\partial L}{\\partial x_{t-\\tau}}$。所讨论的变量是时间 $t-\\tau$ 的输入。\n\n问题陈述了 $\\tau  k \\ge 1$。这意味着变量 $x_{t-\\tau}$ 的时间索引满足 $t-\\tau  t-k$。\n构成截断计算图一部分的输入变量集合是 $\\{x_s | t-k+1 \\le s \\le t\\}$。\n由于 $t-\\tau  t-k  t-k+1$，变量 $x_{t-\\tau}$ 不会出现在基于截断图定义的损失函数 $L$ 的表达式中。\n\n一个函数对于一个不影响它的变量的导数为零。形式上，由于 $x_{t-\\tau}$ 不是 $L_{\\text{trunc}}(h_{t-k}, x_{t-k+1}, \\dots, x_t)$ 的参数，其偏导数为：\n$$ \\frac{\\partial L}{\\partial x_{t-\\tau}} = \\frac{\\partial L_{\\text{trunc}}}{\\partial x_{t-\\tau}} = 0 $$\n因此，对于滞后 $\\tau  k$ 的输入，通过窗口大小为 $k$ 的 TBPTT 计算出的梯度恰好为零。\n\n**第 2 部分：作为反例的解释**\n\n计算出的梯度为零这一事实，是展示 TBPTT 局限性的一个反例。我们可以证明，在完整的、未截断的模型中， $L$ 对 $x_{t-\\tau}$ 的真实依赖关系是存在的。\n\n隐藏状态 $h_t$ 可以通过展开递推关系来表示：$h_{s} = a h_{s-1} + b x_s$。通过重复代入，我们得到：\n$$h_t = a h_{t-1} + b x_t = a(a h_{t-2} + b x_{t-1}) + b x_t = a^2 h_{t-2} + a b x_{t-1} + b x_t$$\n将其推广到 $j$ 步，得到：\n$$h_t = a^j h_{t-j} + \\sum_{i=0}^{j-1} a^i b x_{t-i}$$\n为了找到对 $x_{t-\\tau}$ 的依赖关系，我们展开 $\\tau+1$ 步，直到初始条件 $h_{t-\\tau-1}=0$：\n$$h_t = a^{\\tau+1} h_{t-\\tau-1} + \\sum_{i=0}^{\\tau} a^i b x_{t-i}$$\n给定 $h_{t-\\tau-1}=0$，上式简化为：\n$$h_t = \\sum_{i=0}^{\\tau} a^i b x_{t-i} = b x_t + a b x_{t-1} + \\dots + a^{\\tau} b x_{t-\\tau}$$\n现在，我们可以计算 $h_t$ 相对于 $x_{t-\\tau}$ 的真实偏导数：\n$$ \\frac{\\partial h_t}{\\partial x_{t-\\tau}} = a^{\\tau} b $$\n因为已知 $a$ 和 $b$ 非零，所以这个导数非零。然后，使用链式法则可以找到完整的、未截断的梯度 $\\frac{\\partial L}{\\partial x_{t-\\tau}}$：\n$$ \\frac{\\partial L}{\\partial x_{t-\\tau}} = \\frac{\\partial L}{\\partial y_t} \\frac{\\partial y_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial x_{t-\\tau}} = (y_t - r) \\cdot c \\cdot (a^{\\tau} b) $$\n这个真实梯度通常是非零的。\n\nTBPTT 计算出的梯度（$0$）与真实梯度（非零）之间的差异证明了这种失败。TBPTT 使得学习算法对跨度超过 $k$ 个时间步的依赖关系“视而不见”。因为梯度信号为零，模型的参数（或者在本例中，是其输入，这可以作为它如何学习权重的代理）无法根据在时间 $t-\\tau$ 发生的事件来更新以减小误差 $L$。这种无法捕捉长程依赖关系是朴素 TBPTT 的一个根本问题。\n\n**第 3 部分：有原则的补救措施**\n\n这里有两种有原则的补救措施，可以在不违反使用截断反向传播这一核心约束的情况下解决此失效模式。\n\n1.  **门控 RNN 架构（例如 LSTM, GRU）：** 简单 RNN 的失败部分原因在于其架构，信息在每个时间步都不断地混合和转换，导致难以长期保存特定信息。像长短期记忆（LSTM）和门控循环单元（GRU）这样的架构引入了门控机制。例如，一个 LSTM 维护一个独立的细胞状态 $C_t$，它充当信息高速公路。其更新规则是 $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$，其中 $f_t$ 是一个“遗忘门”。如果网络学会将 $f_t$ 设置为约等于 1，并将输入门 $i_t$ 设置为约等于 0，那么 $C_{t-1}$ 中的信息几乎可以原封不动地传递给 $C_t$。这使得关于远处输入 $x_{t-\\tau}$ 的信息能够以最小的衰减被带到时间 $t-k$ 的状态中。当应用 TBPTT 时，反向传播的初始状态 $h_{t-k}$（及其相关的细胞状态 $C_{t-k}$）就包含了关于遥远过去更有意义的摘要。然后，算法可以学会将这个摘要与时间 $t$ 的误差关联起来，从而在梯度路径本身被截断的情况下有效地学习长程依赖关系。\n\n2.  **合成梯度（Synthetic Gradients）：** 这种方法直接解决了截断点处梯度信号缺失的问题。我们不再假设从未来流入 $h_{t-k}$ 的梯度为零，而是可以训练一个单独的模型来预测这个梯度。设未来总损失相对于状态的真实梯度为 $\\delta_{t-k} = \\frac{\\partial L_{t-k}}{\\partial h_{t-k}}$。其思想是训练一个模型 $M$ 来近似这个梯度：$\\hat{\\delta}_{t-k} = M(h_{t-k})$。在从时间 $t$ 开始反向传播时，当我们到达边界状态 $h_{t-k}$ 时，我们不停止，而是使用 $\\hat{\\delta}_{t-k}$ 作为传入的梯度，继续向过去反向传播。这使得误差信号的估计值能够跨越整个序列传播，即使前向和后向传播在 $k$ 步的间隔处是解耦的。这种方法允许对展开网络的不同部分进行异步或并行训练。", "answer": "$$\\boxed{0}$$", "id": "3101258"}, {"introduction": "RNN 的计算图不仅包含循环连接。这个动手编程练习将研究一个看似简单的操作——在时间轴上添加归一化层——如何产生意想不到的“非局部”依赖关系。你将推导并实现梯度计算，并发现单个时间步的变化会影响整个序列，这对于设计和调试复杂的时间模型是一个至关重要的洞见 [@problem_id:3197441]。", "problem": "给定一个序列处理模型，其计算图在 $T$ 个离散时间步上按时间展开。在每个时间步 $t \\in \\{1,2,\\dots,T\\}$，模型会生成一个预归一化标量输出 $y_t$。然后，对单个序列的时间轴应用一个类似于批量归一化 (Batch Normalization, BN) 的逐时归一化步骤，通过以下变换生成归一化输出 $\\hat{y}_t$：\n$$\n\\mu = \\frac{1}{T}\\sum_{j=1}^{T} y_j,\\qquad\n\\sigma^2 = \\frac{1}{T}\\sum_{j=1}^{T} (y_j - \\mu)^2,\\qquad\ns = \\sqrt{\\sigma^2 + \\varepsilon},\\qquad\n\\hat{y}_t = \\frac{y_t - \\mu}{s}.\n$$\n这里 $\\varepsilon  0$ 是一个很小的稳定化常数。监督目标是归一化输出与给定目标 $z_t$ 之间的平方误差和：\n$$\nL(\\mathbf{y}) = \\frac{1}{2}\\sum_{t=1}^{T} \\left(\\hat{y}_t - z_t\\right)^2,\n$$\n其中 $\\mathbf{y} = [y_1,\\dots,y_T]$ 且 $\\mathbf{z} = [z_1,\\dots,z_T]$。\n\n解决此问题的基础：\n- 微积分中复合函数的链式法则，\n- 均值和方差的定义，\n- 上述给出的归一化定义。\n\n任务 1 (推导)：从链式法则和上述定义出发，推导梯度 $\\nabla_{\\mathbf{y}} L(\\mathbf{y})$ 的闭式表达式，明确证明每个分量 $\\frac{\\partial L}{\\partial y_k}$ 都通过 $\\mu$ 和 $s$ 依赖于所有时间索引 $t \\in \\{1,\\dots,T\\}$。您的推导必须揭示时间归一化所引入的非局部耦合。\n\n任务 2 (实现与测试)：实现一个程序，该程序\n- 计算归一化输出，\n- 计算完整的雅可比矩阵 $J \\in \\mathbb{R}^{T \\times T}$，其元素为 $J_{t,k} = \\frac{\\partial \\hat{y}_t}{\\partial y_k}$，\n- 根据您推导的公式计算解析梯度向量 $\\nabla_{\\mathbf{y}} L(\\mathbf{y})$，\n- 使用一个小的步长 $\\delta$ 通过中心有限差分计算数值梯度，\n- 通过检查 $J$ 的任何非对角线元素的绝对值是否大于一个小阈值 $\\tau$ 来量化非局部耦合，\n- 报告解析梯度和数值梯度之间的最大绝对差。\n\n科学真实性与单位：不涉及物理单位。不使用角度。不需要百分比。\n\n测试套件：使用以下合成序列和目标运行您的程序，所有测试均使用 $\\varepsilon = 10^{-5}$、中心差分步长 $\\delta = 10^{-6}$ 和阈值 $\\tau = 10^{-12}$。\n1. 正常流程用例：$T = 5$，$\\mathbf{y} = [1.0,-2.0,3.0,0.0,2.0]$，$\\mathbf{z} = [0.5,-0.5,1.0,-1.0,0.0]$。\n2. 常数序列边界用例：$T = 4$，$\\mathbf{y} = [3.0,3.0,3.0,3.0]$，$\\mathbf{z} = [0.0,0.0,0.0,0.0]$。\n3. 单时间步边界用例：$T = 1$，$\\mathbf{y} = [2.5]$，$\\mathbf{z} = [1.0]$。\n4. 符号交替长序列用例：$T = 8$，$\\mathbf{y} = [0.1,-0.2,0.3,-0.4,0.5,-0.6,0.7,-0.8]$，$\\mathbf{z} = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个逗号分隔的列表，并用方括号括起来。对于每个测试用例 $i \\in \\{1,2,3,4\\}$，按顺序输出两个值：\n- 一个布尔值，指示是否存在非局部耦合（即 $J$ 的任何非对角线元素的绝对值是否大于 $\\tau$），\n- 一个浮点数，等于解析梯度和数值梯度之间的最大绝对差。\n\n因此，最终输出必须采用以下格式：\n$$\n[\\text{coupling}_1,\\text{error}_1,\\text{coupling}_2,\\text{error}_2,\\text{coupling}_3,\\text{error}_3,\\text{coupling}_4,\\text{error}_4].\n$$", "solution": "用户的要求是推导一个损失函数关于一个逐时归一化层输入的梯度，然后实现一个程序，用数值微分来验证所推导的公式。\n\n### 任务 1：梯度推导\n\n目标是计算损失函数 $L(\\mathbf{y}) = \\frac{1}{2}\\sum_{t=1}^{T} (\\hat{y}_t - z_t)^2$ 相对于输入序列 $\\mathbf{y} = [y_1, \\dots, y_T]$ 的梯度 $\\nabla_{\\mathbf{y}} L(\\mathbf{y})$。该梯度的分量是偏导数 $\\frac{\\partial L}{\\partial y_k}$，其中 $k \\in \\{1, \\dots, T\\}$。\n\n我们首先应用多元链式法则：\n$$\n\\frac{\\partial L}{\\partial y_k} = \\sum_{t=1}^{T} \\frac{\\partial L}{\\partial \\hat{y}_t} \\frac{\\partial \\hat{y}_t}{\\partial y_k}\n$$\n求和中的第一项是损失相对于归一化输出 $\\hat{y}_t$ 的偏导数。这很容易计算：\n$$\n\\frac{\\partial L}{\\partial \\hat{y}_t} = \\frac{\\partial}{\\partial \\hat{y}_t} \\left[ \\frac{1}{2}\\sum_{j=1}^{T} (\\hat{y}_j - z_j)^2 \\right] = \\hat{y}_t - z_t\n$$\n第二项 $\\frac{\\partial \\hat{y}_t}{\\partial y_k}$ 代表归一化函数雅可比矩阵的 $(t, k)$ 项。它量化了时间 $t$ 的归一化输出随时间 $k$ 的输入变化而变化的程度。该函数为 $\\hat{y}_t = \\frac{y_t - \\mu}{s}$。它对 $y_k$ 的依赖关系很复杂，因为 $\\mu$ 和 $s$ 都是所有输入 $y_1, \\dots, y_T$ 的函数。\n\n我们必须系统地计算中间量相对于 $y_k$ 的偏导数。\n\n1.  **均值 $\\mu$ 的导数**：\n    $$\n    \\frac{\\partial \\mu}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{T}\\sum_{j=1}^{T} y_j \\right) = \\frac{1}{T}\n    $$\n\n2.  **方差 $\\sigma^2$ 的导数**：\n    一个方便的方差形式是 $\\sigma^2 = \\left(\\frac{1}{T}\\sum_{j=1}^{T} y_j^2\\right) - \\mu^2$。\n    $$\n    \\frac{\\partial \\sigma^2}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{T}\\sum_{j=1}^{T} y_j^2 - \\mu^2 \\right) = \\frac{1}{T} (2y_k) - 2\\mu \\frac{\\partial \\mu}{\\partial y_k} = \\frac{2y_k}{T} - 2\\mu \\left(\\frac{1}{T}\\right) = \\frac{2}{T}(y_k - \\mu)\n    $$\n\n3.  **稳定化标准差 $s$ 的导数**：\n    给定 $s = \\sqrt{\\sigma^2 + \\varepsilon}$，我们使用链式法则：\n    $$\n    \\frac{\\partial s}{\\partial y_k} = \\frac{\\partial s}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial y_k} = \\frac{1}{2\\sqrt{\\sigma^2 + \\varepsilon}} \\left( \\frac{2}{T}(y_k - \\mu) \\right) = \\frac{y_k - \\mu}{Ts}\n    $$\n\n现在，我们可以使用除法法则对 $\\hat{y}_t = (y_t - \\mu)s^{-1}$ 进行微分来计算 $\\frac{\\partial \\hat{y}_t}{\\partial y_k}$：\n$$\n\\frac{\\partial \\hat{y}_t}{\\partial y_k} = \\frac{s \\cdot \\frac{\\partial}{\\partial y_k}(y_t - \\mu) - (y_t - \\mu) \\cdot \\frac{\\partial s}{\\partial y_k}}{s^2}\n$$\n分子项的导数是 $\\frac{\\partial}{\\partial y_k}(y_t - \\mu) = \\frac{\\partial y_t}{\\partial y_k} - \\frac{\\partial \\mu}{\\partial y_k} = \\delta_{tk} - \\frac{1}{T}$，其中 $\\delta_{tk}$ 是克罗内克 δ (Kronecker delta)。\n\n将中间导数代入除法法则表达式中：\n$$\n\\frac{\\partial \\hat{y}_t}{\\partial y_k} = \\frac{1}{s^2} \\left[ s \\left( \\delta_{tk} - \\frac{1}{T} \\right) - (y_t - \\mu) \\left( \\frac{y_k - \\mu}{Ts} \\right) \\right]\n$$\n$$\n= \\frac{1}{s} \\left( \\delta_{tk} - \\frac{1}{T} \\right) - \\frac{(y_t - \\mu)(y_k - \\mu)}{Ts^3}\n$$\n通过代入定义 $\\hat{y}_t = \\frac{y_t - \\mu}{s}$ 和 $\\hat{y}_k = \\frac{y_k - \\mu}{s}$（后者是一种符号上的便利），这可以简化为雅可比矩阵元素 $J_{t,k}$ 一个非常简洁的形式：\n$$\nJ_{t,k} = \\frac{\\partial \\hat{y}_t}{\\partial y_k} = \\frac{1}{s} \\left[ \\delta_{tk} - \\frac{1}{T} - \\frac{1}{T} \\hat{y}_t \\hat{y}_k \\right] = \\frac{1}{s} \\left[ \\delta_{tk} - \\frac{1}{T} (1 + \\hat{y}_t \\hat{y}_k) \\right]\n$$\n这个表达式展示了**非局部耦合**：对于任何 $t \\neq k$，导数 $\\frac{\\partial \\hat{y}_t}{\\partial y_k}$ 不为零，而是等于 $-\\frac{1}{Ts}(1 + \\hat{y}_t \\hat{y}_k)$。这意味着在时间步 $k$ 处输入 $y_k$ 的变化会影响到*每一个*其他时间步 $t$ 的归一化输出 $\\hat{y}_t$，这是因为共享的均值 $\\mu$ 和标准差 $s$。\n\n最后，我们组装完整的梯度分量 $\\frac{\\partial L}{\\partial y_k}$：\n$$\n\\frac{\\partial L}{\\partial y_k} = \\sum_{t=1}^{T} (\\hat{y}_t - z_t) J_{t,k} = \\sum_{t=1}^{T} (\\hat{y}_t - z_t) \\frac{1}{s} \\left[ \\delta_{tk} - \\frac{1}{T} (1 + \\hat{y}_t \\hat{y}_k) \\right]\n$$\n根据克罗内克 δ 分离求和：\n$$\n\\frac{\\partial L}{\\partial y_k} = \\frac{1}{s} \\left( (\\hat{y}_k - z_k) - \\sum_{t=1}^{T} (\\hat{y}_t - z_t) \\frac{1}{T}(1 + \\hat{y}_t \\hat{y}_k) \\right)\n$$\n这是梯度的闭式表达式。求和项清楚地表明，对于 $y_k$ 的梯度分量依赖于所有时间步 $t \\in \\{1, \\dots, T\\}$ 的输出 $\\hat{y}_t$ 和目标 $z_t$。在向量表示法中，如果 $\\nabla_{\\hat{\\mathbf{y}}} L = \\hat{\\mathbf{y}} - \\mathbf{z}$，则完整梯度为 $\\nabla_{\\mathbf{y}} L = \\mathbf{J}^T (\\hat{\\mathbf{y}} - \\mathbf{z})$。\n\n### 任务 2：实现与测试\n\n该实现为每个测试用例计算了必要的量：归一化输出 $\\hat{\\mathbf{y}}$、雅可比矩阵 $\\mathbf{J}$、使用推导公式计算的解析梯度 $\\nabla_{\\mathbf{y}} L$（实现为 $\\mathbf{J}^T(\\hat{\\mathbf{y}}-\\mathbf{z})$）以及用于验证的数值梯度。然后，它报告了非局部耦合的存在性以及解析梯度和数值梯度之间的最大误差。$T=1$ 的情况很特殊：由于 $\\mu=y_1$ 和 $\\sigma^2=0$，输出 $\\hat{y}_1=0$ 是一个常数，这使得雅可比矩阵和梯度都为零。代码正确处理了此情况，此时不存在可供检查耦合的非对角线元素。常数序列的情况产生零方差，但 $\\varepsilon$ 项防止了除以零，并且梯度正确地计算为零。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite, deriving and comparing\n    analytic and numerical gradients for a time-wise normalization layer.\n    \"\"\"\n    \n    # Define constants for the test suite\n    epsilon = 1e-5\n    delta = 1e-6\n    tau = 1e-12\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([1.0, -2.0, 3.0, 0.0, 2.0]), np.array([0.5, -0.5, 1.0, -1.0, 0.0])),\n        (np.array([3.0, 3.0, 3.0, 3.0]), np.array([0.0, 0.0, 0.0, 0.0])),\n        (np.array([2.5]), np.array([1.0])),\n        (np.array([0.1, -0.2, 0.3, -0.4, 0.5, -0.6, 0.7, -0.8]), np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))\n    ]\n\n    results = []\n    \n    for y, z in test_cases:\n        T = y.shape[0]\n\n        # Helper function to compute the loss for numerical gradient calculation\n        def calculate_loss(y_vec, z_vec):\n            # This function must handle the T=1 case carefully\n            if y_vec.shape[0] == 1:\n                # For T=1, mu=y, var=0, s=sqrt(epsilon), y_hat=0\n                y_hat_vec = np.zeros_like(y_vec)\n            else:\n                mu_vec = np.mean(y_vec)\n                # Note: np.var computes biased variance (divides by N), as per problem statement\n                var_vec = np.var(y_vec)\n                s_vec = np.sqrt(var_vec + epsilon)\n                y_hat_vec = (y_vec - mu_vec) / s_vec\n            \n            loss = 0.5 * np.sum((y_hat_vec - z_vec)**2)\n            return loss\n\n        # 1. Compute normalized outputs (y_hat)\n        if T == 1:\n            mu = y[0]\n            var = 0.0\n            s = np.sqrt(var + epsilon)\n            y_hat = np.array([0.0])\n        else:\n            mu = np.mean(y)\n            var = np.var(y)\n            s = np.sqrt(var + epsilon)\n            y_hat = (y - mu) / s\n        \n        # 2. Compute the full Jacobian matrix J\n        J = np.zeros((T, T))\n        if T > 0:\n            # Vectorized computation of the Jacobian using the derived formula:\n            # J_tk = (1/s) * [delta_tk - (1/T) * (1 + y_hat_t * y_hat_k)]\n            identity = np.eye(T)\n            y_hat_outer = np.outer(y_hat, y_hat)\n            J = (1 / s) * (identity - (1 / T) * (1 + y_hat_outer))\n\n        # 3. Compute analytic gradient\n        # dL/dy_hat = y_hat - z\n        # dL/dy = J^T * (dL/dy_hat)\n        dL_dy_hat = y_hat - z\n        grad_analytic = J.T @ dL_dy_hat\n\n        # 4. Compute numerical gradient by central finite differences\n        grad_numerical = np.zeros(T)\n        for k in range(T):\n            y_plus = y.copy()\n            y_plus[k] += delta\n            L_plus = calculate_loss(y_plus, z)\n\n            y_minus = y.copy()\n            y_minus[k] -= delta\n            L_minus = calculate_loss(y_minus, z)\n            \n            grad_numerical[k] = (L_plus - L_minus) / (2 * delta)\n\n        # 5. Quantify non-local coupling\n        # Check if any off-diagonal entry of J has magnitude > tau\n        coupling_exists = False\n        if T > 1:\n            # Extract off-diagonal elements by setting diagonal to 0\n            off_diagonal_J = J - np.diag(np.diag(J))\n            coupling_exists = np.any(np.abs(off_diagonal_J) > tau)\n\n        # 6. Report the maximum absolute difference\n        max_error = np.max(np.abs(grad_analytic - grad_numerical))\n\n        results.append(str(coupling_exists))\n        results.append(max_error)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3197441"}]}