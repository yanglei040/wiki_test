{"hands_on_practices": [{"introduction": "这个练习将长短期记忆（LSTM）网络中的遗忘门（forget gate）分离出来进行研究，它是控制信息流的关键组件。通过在一个简化的设定中调控其偏置（bias），我们可以直接观察到它如何使记忆单元能够长期保留或丢弃信息。这项实践旨在为遗忘门在缓解梯度消失问题中的作用提供清晰的直观理解。[@problem_id:3191179]", "problem": "考虑一个初始化后的单单元长短期记忆（LSTM; Long Short-Term Memory）循环，用于研究遗忘门偏置对长序列信息保持能力的影响。标准的单元更新由以下经过充分检验的方程定义：单元状态 $c_t$ 按 $c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$ 演化，其中遗忘门 $f_t = \\sigma(z_f(t))$，输入门 $i_t = \\sigma(z_i(t))$，候选更新 $g_t = \\tanh(z_g(t))$。logistic sigmoid 函数为 $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$，双曲正切为 $\\tanh(x)$。在使用零均值权重的随机初始化中，通常通过设置遗忘门偏置 $b_f$ 来控制遗忘门保持信息的初始趋势。\n\n我们定义一个受控的、纯数学的初始化方案变体，以分离出保持动态：\n- 对于所有时间步 $t \\geq 1$，门和候选值的激活前值满足 $z_f(t) = b_f$，$z_i(t) = 0$ 和 $z_g(t) = 0$，这意味着 $f_t = \\sigma(b_f)$，$i_t = \\sigma(0)$ 和 $g_t = \\tanh(0)$。\n- 在时间 $t = 0$ 时，一次性写入操作设定了初始记忆 $c_0$，其值为 $c_0 = 1$（无量纲）。对于 $t \\geq 1$，没有进一步的写入，因此对于 $t \\geq 1$ 有 $i_t \\cdot g_t = 0$。\n\n仅从上述定义和事实出发，推导经过 $L$ 步后的保持率，其定义为比率 $R(L, b_f) = \\dfrac{c_L}{c_0}$，并实现一个程序，该程序为一系列遗忘门偏置值 $b_f$ 和依赖长度 $L$ 计算此保持率。\n\n测试套件：\n- 遗忘门偏置值 $b_f \\in \\{-3, -1, 0, 1, 3\\}$，即 $b_f$ 等于 $-3, -1, 0, 1$ 和 $3$。\n- 依赖长度 $L \\in \\{0, 5, 20, 100, 200\\}$，即 $L$ 等于 $0, 5, 20, 100$ 和 $200$。\n\n对于每个有序对 $(b_f, L)$，其中 $b_f$ 按升序迭代，然后 $L$ 按升序迭代（即对该有序对按字典序），计算保持率 $R(L, b_f)$，结果为一个实数，并将其报告为四舍五入到 $6$ 位小数的浮点数。本问题中没有物理单位；所有量都是无量纲的。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，该列表按确切指定的有序对顺序排列：首先是 $b_f = -3$ 与所有 $L$ 的组合，然后是 $b_f = -1$ 与所有 $L$ 的组合，接着是 $b_f = 0$ 与所有 $L$ 的组合，然后是 $b_f = 1$ 与所有 $L$ 的组合，最后是 $b_f = 3$ 与所有 $L$ 的组合。例如，输出格式必须类似于“[r_1,r_2,...,r_{25}]”，其中每个 $r_k$ 是对应测试用例计算出的浮点数。每个测试用例的答案都是浮点数。", "solution": "用户希望我解决一个关于 LSTM 单元的问题。\n\n### 步骤 1：问题验证\n\n我将首先验证问题陈述。\n\n#### 步骤 1.1：提取已知条件\n\n已知条件如下：\n- LSTM 单元状态更新方程：$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$\n- 遗忘门激活：$f_t = \\sigma(z_f(t))$\n- 输入门激活：$i_t = \\sigma(z_i(t))$\n- 候选更新：$g_t = \\tanh(z_g(t))$\n- Logistic sigmoid 函数：$\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$\n- 双曲正切函数：$\\tanh(x)$\n- 对于所有时间步 $t \\geq 1$ 的受控模型条件：\n    - 遗忘门的激活前值：$z_f(t) = b_f$\n    - 输入门的激活前值：$z_i(t) = 0$\n    - 候选更新的激活前值：$z_g(t) = 0$\n- 对于 $t \\geq 1$ 的写入条件：$i_t \\cdot g_t = 0$\n- $t = 0$ 时的初始条件：$c_0 = 1$\n- 保持率的定义：$R(L, b_f) = \\dfrac{c_L}{c_0}$\n- 测试套件参数：\n    - 遗忘门偏置值 $b_f \\in \\{-3, -1, 0, 1, 3\\}$\n    - 依赖长度 $L \\in \\{0, 5, 20, 100, 200\\}$\n- 输出格式：一个逗号分隔的浮点值列表，四舍五入到 $6$ 位小数，按字典序对 $(b_f, L)$ 排列。\n\n#### 步骤 1.2：使用提取的已知条件进行验证\n\n根据既定标准对问题进行验证。\n\n1.  **科学或事实的健全性**：该问题具有科学依据。它使用了长短期记忆（LSTM）单元的标准、普遍接受的方程。引入的简化条件（$z_f(t) = b_f$，$z_i(t) = 0$，$z_g(t) = 0$）是一个明确定义的数学构造，旨在分离并研究遗忘门偏置对记忆保持的影响。这是科学建模中一种常用且有效的技术，通过分析简化案例来理解复杂系统。对于 $t \\geq 1$，$i_t \\cdot g_t = 0$ 的陈述与给定的激活前值一致，因为 $g_t = \\tanh(z_g(t)) = \\tanh(0) = 0$，这使得无论 $i_t$ 的值如何，该乘积都为零。\n\n2.  **适定性与完备性**：该问题是适定的且自洽的。它提供了一个明确的初始条件（$c_0=1$），一个用于单元状态 $c_t$ 的确定性递推关系，以及一个待计算量（$R(L, b_f)$）的精确定义。所有必要的变量和函数都已明确定义。测试用例已指定，所需的输出格式也毫不含糊。这种结构保证了存在唯一、稳定且有意义的解，并且该解是可以计算的。\n\n3.  **客观性**：该问题以精确、客观的数学语言陈述。它没有歧义、主观断言或基于观点的陈述。\n\n#### 步骤 1.3：结论与行动\n\n问题陈述是**有效的**。这是一个明确定义的数学问题，根植于深度学习的既定原则。我现在将着手推导解决方案。\n\n### 步骤 2：推导与求解\n\n目标是为保持率 $R(L, b_f) = \\dfrac{c_L}{c_0}$ 推导一个表达式，并为给定的测试套件计算其值。\n\nLSTM 单元状态的基本递推关系如下：\n$$c_t = f_t \\cdot c_{t-1} + i_t \\cdot g_t$$\n\n问题定义了一个简化方案，其中对于所有时间步 $t \\geq 1$，输入写入项为零：\n$$i_t \\cdot g_t = 0 \\quad \\text{for } t \\geq 1$$\n这个简化与提供的激活前值是一致的，因为 $z_g(t) = 0$ 意味着 $g_t = \\tanh(0) = 0$。\n\n将此条件代入单元状态方程，对于 $t \\geq 1$ 的递推关系简化为一个纯乘法更新：\n$$c_t = f_t \\cdot c_{t-1}$$\n\n遗忘门激活值 $f_t$ 由其激活前的值 $z_f(t)$ 决定。对于 $t \\geq 1$，该值保持不变：\n$$z_f(t) = b_f$$\n因此，对于所有 $t \\geq 1$，遗忘门的输出 $f_t$ 也是恒定的。我们用 $f$ 表示这个恒定值：\n$$f = f_t = \\sigma(z_f(t)) = \\sigma(b_f) = \\frac{1}{1 + e^{-b_f}}$$\n\n对于 $t \\geq 1$ 的递推关系现在变成了一个公比为 $f$ 的简单几何级数：\n$$c_t = f \\cdot c_{t-1}$$\n\n我们可以从初始状态 $c_0$ 开始展开这个递推关系：\n- 对于 $t=1$: $c_1 = f \\cdot c_0$\n- 对于 $t=2$: $c_2 = f \\cdot c_1 = f \\cdot (f \\cdot c_0) = f^2 \\cdot c_0$\n- 对于 $t=3$: $c_3 = f \\cdot c_2 = f \\cdot (f^2 \\cdot c_0) = f^3 \\cdot c_0$\n\n通过归纳法，对于任何时间步 $L \\geq 1$，单元状态 $c_L$ 由下式给出：\n$$c_L = f^L \\cdot c_0$$\n\n保持率 $R(L, b_f)$ 定义为比率 $\\dfrac{c_L}{c_0}$。\n对于 $L \\geq 1$，我们有：\n$$R(L, b_f) = \\frac{c_L}{c_0} = \\frac{f^L \\cdot c_0}{c_0} = f^L$$\n代入 $f$ 的表达式，我们得到对于 $L \\geq 1$ 的保持率公式：\n$$R(L, b_f) = \\left(\\sigma(b_f)\\right)^L = \\left(\\frac{1}{1 + e^{-b_f}}\\right)^L$$\n\n一个特殊情况是 $L=0$。根据定义，经过 $0$ 步后的保持率为：\n$$R(0, b_f) = \\frac{c_0}{c_0} = 1$$\n如果我们采用 $x^0 = 1$ 的约定，这个结果也与通用公式一致。\n\n所以，完整的保持率公式是：\n$$\nR(L, b_f) =\n\\begin{cases}\n1  \\text{if } L = 0 \\\\\n\\left(\\dfrac{1}{1 + e^{-b_f}}\\right)^L  \\text{if } L  0\n\\end{cases}\n$$\n\n实现将包含一个程序，该程序按字典序遍历指定的 $(b_f, L)$ 对。对于每一对，它将使用推导出的公式计算 $R(L, b_f)$，并将 $L=0$ 的情况作为基本条件处理。结果将被收集、格式化为6位小数，并以单个用方括号括起来的逗号分隔字符串的形式打印。`numpy` 库适合用于指数计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the memory retention in a simplified LSTM cell for a sweep of\n    forget-gate biases and dependency lengths.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    b_f_values = [-3, -1, 0, 1, 3]\n    L_values = [0, 5, 20, 100, 200]\n\n    # This structure ensures lexicographic ordering on the pair (b_f, L).\n    test_cases = [(b_f, L) for b_f in b_f_values for L in L_values]\n\n    results = []\n    \n    for b_f, L in test_cases:\n        # The retention R(L, b_f) is defined as c_L / c_0.\n        # The recurrence relation c_t = f * c_{t-1} for t >= 1 with c_0 given.\n        # This leads to c_L = f^L * c_0, so R(L, b_f) = f^L.\n        # The forget gate activation f is constant: f = sigma(b_f).\n\n        # Case 1: L = 0 (0 steps)\n        # By definition, R(0, b_f) = c_0 / c_0 = 1.\n        if L == 0:\n            retention = 1.0\n        # Case 2: L > 0\n        else:\n            # Calculate the sigmoid function for the given bias b_f.\n            # sigma(x) = 1 / (1 + e^(-x))\n            sigma_b_f = 1.0 / (1.0 + np.exp(-b_f))\n            \n            # Calculate retention as sigma(b_f) raised to the power of L.\n            retention = sigma_b_f ** L\n        \n        results.append(retention)\n\n    # Format the final output string as a comma-separated list of floats\n    # rounded to 6 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    # The final print statement must produce only the specified output format.\n    print(output_string)\n\nsolve()\n\n```", "id": "3191179"}, {"introduction": "长期依赖问题常常涉及处理层次化结构，比如在自然语言或代码中。本练习使用一个经典的括号匹配任务，来对比循环神经网络（RNN）在梯度传播上的根本局限性与注意力机制（attention mechanism）在提供直接访问路径上的优势。通过这个对比，我们将揭示为什么像Transformer这样基于注意力的模型在这种任务上表现出色。[@problem_id:3191175]", "problem": "考虑经典的嵌套括号识别任务，该任务需要一个类堆栈内存来处理层次结构。设输入为一个长度为 $T$ 的序列，其字母表为 $\\{ \"(\", \")\" \\}$，编码为 $x_t \\in \\{+1, -1\\}$，其中 $t \\in \\{1,\\dots,T\\}$，$+1$ 表示左括号，$-1$ 表示右括号。定义累积和前缀过程 $s_t = \\sum_{i=1}^t x_i$，且 $s_0 = 0$。一个字符串是平衡的，当且仅当 $s_T = 0$ 且对于所有 $t \\in \\{0,\\dots,T\\}$ 都有 $s_t \\ge 0$。最大嵌套深度等于 $\\max_{t} s_t$。深度为 $D$ 的最难的平衡字符串是长度为 $T = 2D$ 的字符串，由 $D$ 个左括号后跟 $D$ 个右括号组成，其中第一个左括号必须与最后一个右括号匹配，从而产生一个 $\\delta = 2D - 1$ 的依赖距离。\n\n您将在此任务上比较两种抽象模型族：\n1. 一个普通的循环神经网络 (RNN)，其隐藏状态更新方式为 $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$，以及一个读取 $h_T$ 的最终分类器。假设每一步的雅可比算子范数受一个常数 $\\lambda  0$ 的限制，即对于所有步骤，$\\| J_t \\|_2 \\le \\lambda$，其中 $J_t = \\partial h_{t+1}/\\partial h_t$。这是一个标准的、广泛使用的界限，其中 $\\lambda = \\| W_h \\|_2 L_\\phi$， $L_\\phi$ 是激活函数的利普希茨常数。\n2. 一个单层的点积自注意力机制，其局部注意力窗口大小为 $w \\in \\mathbb{N}$，意味着每个位置只能直接关注距离不超过 $w$ 的位置。令 $p \\in \\{0, 1\\}$ 表示是否存在绝对位置编码（$p=1$）或不存在（$p=0$）。缺少位置编码意味着在单个注意力层中，内容相同的词符在位置上是无法区分的。\n\n根据第一性原理，使用以下推理来判断在所述抽象下，每个模型原则上是否可以在未经训练的情况下成功处理深度为 $D$ 的最坏情况平衡字符串：\n- 对于 RNN，使用链式法则和算子范数的次可乘性，可以得到最早的词符对 $h_T$ 影响的上界为\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2 \\le \\lambda^T.\n$$\n使用一个容差阈值 $\\varepsilon = 10^{-3}$，当且仅当满足以下条件时，我们判定 RNN 成功：\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1},\n$$\n这排除了梯度消失 ($\\lambda^T  \\varepsilon$) 和梯度爆炸 ($\\lambda^T > \\varepsilon^{-1}$) 这两种学习长期依赖的失败模式。\n- 对于单层局部自注意力，模型只能直接关联距离在 $w$ 之内的词符。最坏情况下的匹配距离是 $\\delta = 2D - 1$。因此，模型只有在 $p = 1$（位置可区分）且 $w \\ge \\delta$ 时才能成功。否则，它会因为局部性限制（$w  \\delta$）或位置不可区分性（$p=0$）而失败。\n\n您的任务是实现一个程序，对每个测试用例应用上述标准，并为每个用例返回一个分类整数代码：\n- 如果 RNN 和注意力模型都成功，则返回 $3$。\n- 如果只有 RNN 成功，则返回 $2$。\n- 如果只有注意力模型成功，则返回 $1$。\n- 如果两者都失败，则返回 $0$。\n\n使用以下测试套件，它涵盖了一般情况、局部性受限的注意力、梯度消失的RNN、在更大深度下的权衡、缺少位置编码以及一个浅层边缘情况：\n- 情况 1: $D = 5$, $\\lambda = 0.95$, $w = 10$, $p = 1$。\n- 情况 2: $D = 5$, $\\lambda = 0.95$, $w = 5$, $p = 1$。\n- 情况 3: $D = 50$, $\\lambda = 0.9$, $w = 200$, $p = 1$。\n- 情况 4: $D = 100$, $\\lambda = 0.99$, $w = 50$, $p = 1$。\n- 情况 5: $D = 100$, $\\lambda = 1.05$, $w = 1000$, $p = 0$。\n- 情况 6: $D = 1$, $\\lambda = 0.5$, $w = 1$, $p = 1$。\n\n实现细节：\n- 对每个情况，计算 $T = 2D$，$\\delta = 2D - 1$，并使用 $\\varepsilon = 10^{-3}$。\n- 通过关于 $\\lambda^T$ 相对于 $\\varepsilon$ 的不等式来确定 RNN 是否成功。\n- 通过检查 $p = 1$ 和 $w \\ge \\delta$ 来确定注意力模型是否成功。\n- 按照上述定义，将每个情况的结果编码为单个整数。\n\n最终输出格式：\n- 您的程序应该生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，例如 $[r_1,r_2,\\dots,r_6]$，其中每个 $r_i$ 是按上面列出的顺序对测试用例 $i$ 的整数代码。", "solution": "该问题要求对一个普通的循环神经网络 (RNN) 和一个单层局部自注意力模型，在识别一类特定平衡括号字符串任务上的表现进行比较分析。该分析基于处理长期依赖关系的抽象第一性原理标准。\n\n首先，我们对问题背景进行形式化描述。输入是长度为 $T$ 的序列 $x_1, \\dots, x_T$，其中 $x_t=+1$ 代表左括号 `(`，$x_t=-1$ 代表右括号 `)`。深度为 $D$ 的“最难”平衡字符串被定义为 $D$ 个左括号后跟 $D$ 个右括号。这种结构意味着序列长度为 $T = 2D$。关键的依赖关系存在于时间 $t=1$ 的第一个左括号和时间 $t=T=2D$ 的最后一个右括号之间。这个依赖的距离是 $\\delta = T-1 = 2D-1$。\n\n每个模型的成功与否由一组预定义的标准决定。我们将根据这些规则对每个模型类别进行分析。\n\n**模型 1：普通循环神经网络 (RNN)**\n\nRNN 逐步处理序列，通过递推关系 $h_{t+1} = \\phi(W_h h_t + W_x x_{t+1} + b)$ 更新其隐藏状态 $h_t$。网络学习长程依赖的能力从根本上与早期输入的信息如何通过这种递推关系传播以影响最终状态 $h_T$ 有关。这种传播在数学上由最终状态相对于初始状态的梯度 $\\frac{\\partial h_T}{\\partial h_0}$ 来描述。使用链式法则，这可以表示为每个时间步的雅可比矩阵的乘积：\n$$\n\\frac{\\partial h_T}{\\partial h_0} = \\frac{\\partial h_T}{\\partial h_{T-1}} \\frac{\\partial h_{T-1}}{\\partial h_{T-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0} = \\prod_{t=0}^{T-1} J_t\n$$\n其中 $J_t = \\frac{\\partial h_{t+1}}{\\partial h_t}$。取算子范数并使用其次可乘性（$\\|AB\\| \\le \\|A\\|\\|B\\|$)，我们得到该梯度范数的上界：\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\prod_{t=0}^{T-1} \\|J_t\\|_2\n$$\n问题提供了一个统一的界限 $\\|J_t\\|_2 \\le \\lambda$ 对所有 $t$ 成立。这可将表达式简化为：\n$$\n\\left\\| \\frac{\\partial h_T}{\\partial h_0} \\right\\|_2 \\le \\lambda^T\n$$\n这个界限是梯度消失和梯度爆炸概念的核心。如果 $\\lambda  1$，随着序列长度 $T$ 的增加，界限 $\\lambda^T$ 会指数级衰减到 $0$。这意味着早期输入的影响可能会变得微乎其微，使得模型无法学习长期依赖。这就是梯度消失问题。相反，如果 $\\lambda > 1$，界限会指数级增长，可能导致训练不稳定。这就是梯度爆炸问题。\n\nRNN 的成功标准被定义为避免这些病态区间。使用一个容差阈值 $\\varepsilon = 10^{-3}$，当且仅当梯度界限在一个稳定范围内时，RNN 被认为是成功的：\n$$\n\\varepsilon \\le \\lambda^T \\le \\varepsilon^{-1}\n$$\n\n**模型 2：单层局部自注意力**\n\n自注意力机制将每个词符的表示计算为其他词符表示的加权平均值。在一个窗口大小为 $w$ 的局部自注意力模型中，位置 $i$ 处的每个词符只能直接关注（即交换信息）范围在 $[i-w, i+w]$ 内的词符。为了解决给定任务，模型必须能够将 $t=1$ 处的第一个左括号与 $t=T$ 处的最后一个右括号关联起来。这些词符之间的距离是 $\\delta = T-1 = 2D-1$。对于单层局部注意力来说，要在这两个词符之间创建直接路径，其窗口大小 $w$ 必须足够大以跨越这个距离，即 $w \\ge \\delta$。\n\n此外，问题指定了最坏情况的字符串由 $D$ 个相同的左括号后跟 $D$ 个相同的右括号组成。如果没有按位置区分词符的机制，自注意力层无法区分第一个 `(` 和第二个 `(`，从而无法学习正确的嵌套结构。绝对位置编码提供了这一关键信息。参数 $p=1$ 表示其存在，而 $p=0$ 表示其不存在。\n\n因此，单层局部自注意力模型的成功取决于两个条件：\n1. 位置信息必须可用：$p=1$。\n2. 注意力窗口必须足够宽以捕捉最长的依赖关系：$w \\ge \\delta$。\n当且仅当两个条件都满足时，模型才能成功。\n\n**逐案评估**\n\n现在我们使用 $\\varepsilon = 10^{-3}$ 将这些标准应用于每个测试用例。\n\n**情况 1：** $D=5$, $\\lambda = 0.95$, $w=10$, $p=1$。\n- 派生值：$T = 2D = 10$，$\\delta = 2D-1 = 9$。\n- RNN：我们计算 $\\lambda^T = 0.95^{10} \\approx 0.5987$。条件是 $0.001 \\le 0.5987 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（为真）和 $w \\ge \\delta$，即 $10 \\ge 9$（为真）。**注意力模型成功。**\n- 结果：两者都成功，代码 = $3$。\n\n**情况 2：** $D=5$, $\\lambda = 0.95$, $w=5$, $p=1$。\n- 派生值：$T = 10$，$\\delta = 9$。\n- RNN：我们计算 $\\lambda^T = 0.95^{10} \\approx 0.5987$。条件是 $0.001 \\le 0.5987 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（为真）和 $w \\ge \\delta$，即 $5 \\ge 9$（为假）。**注意力模型失败。**\n- 结果：只有 RNN 成功，代码 = $2$。\n\n**情况 3：** $D=50$, $\\lambda = 0.9$, $w=200$, $p=1$。\n- 派生值：$T = 2D = 100$，$\\delta = 2D-1 = 99$。\n- RNN：我们计算 $\\lambda^T = 0.9^{100} \\approx 2.656 \\times 10^{-5}$。此值小于 $\\varepsilon = 0.001$。梯度消失。**RNN 失败。**\n- 注意力模型：我们检查 $p=1$（为真）和 $w \\ge \\delta$，即 $200 \\ge 99$（为真）。**注意力模型成功。**\n- 结果：只有注意力模型成功，代码 = $1$。\n\n**情况 4：** $D=100$, $\\lambda = 0.99$, $w=50$, $p=1$。\n- 派生值：$T = 2D = 200$，$\\delta = 2D-1 = 199$。\n- RNN：我们计算 $\\lambda^T = 0.99^{200} \\approx 0.134$。条件是 $0.001 \\le 0.134 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（为真）和 $w \\ge \\delta$，即 $50 \\ge 199$（为假）。**注意力模型失败。**\n- 结果：只有 RNN 成功，代码 = $2$。\n\n**情况 5：** $D=100$, $\\lambda = 1.05$, $w=1000$, $p=0$。\n- 派生值：$T = 200$，$\\delta = 199$。\n- RNN：我们计算 $\\lambda^T = 1.05^{200} \\approx 17293$。此值大于 $\\varepsilon^{-1} = 1000$。梯度爆炸。**RNN 失败。**\n- 注意力模型：我们检查 $p=1$。这为假（$p=0$）。模型无法区分词符位置。**注意力模型失败。**\n- 结果：两者都失败，代码 = $0$。\n\n**情况 6：** $D=1$, $\\lambda = 0.5$, $w=1$, $p=1$。\n- 派生值：$T = 2D = 2$，$\\delta = 2D-1 = 1$。字符串是 `()`。\n- RNN：我们计算 $\\lambda^T = 0.5^2 = 0.25$。条件是 $0.001 \\le 0.25 \\le 1000$。该条件成立。**RNN 成功。**\n- 注意力模型：我们检查 $p=1$（为真）和 $w \\ge \\delta$，即 $1 \\ge 1$（为真）。**注意力模型成功。**\n- 结果：两者都成功，代码 = $3$。\n\n这些结果被综合成一个最终的整数代码列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Applies success criteria to an RNN and a local self-attention model\n    for a series of test cases on a parenthesis-matching task.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (D, lambda, w, p).\n    test_cases = [\n        (5, 0.95, 10, 1),  # Case 1\n        (5, 0.95, 5, 1),   # Case 2\n        (50, 0.9, 200, 1), # Case 3\n        (100, 0.99, 50, 1),# Case 4\n        (100, 1.05, 1000, 0),# Case 5\n        (1, 0.5, 1, 1),    # Case 6\n    ]\n\n    # The tolerance threshold epsilon.\n    epsilon = 1e-3\n\n    results = []\n    for D, lam, w, p in test_cases:\n        # Calculate derived problem parameters.\n        T = 2 * D\n        delta = 2 * D - 1\n\n        # --- RNN Success Criterion ---\n        # The RNN succeeds if its gradient norm bound lambda^T is within a stable range.\n        # This prevents vanishing (lambda^T  epsilon) and exploding (lambda^T > 1/epsilon) gradients.\n        lambda_T = np.power(lam, T)\n        rnn_succeeds = (epsilon = lambda_T = 1 / epsilon)\n\n        # --- Attention Success Criterion ---\n        # The single-layer local attention model succeeds if:\n        # 1. Positional encodings are present (p=1) to distinguish identical tokens.\n        # 2. The local window size (w) is large enough to span the maximum dependency distance (delta).\n        attention_succeeds = (p == 1 and w >= delta)\n\n        # --- Determine the final result code ---\n        # 3: both succeed\n        # 2: only RNN succeeds\n        # 1: only attention succeeds\n        # 0: both fail\n        if rnn_succeeds and attention_succeeds:\n            code = 3\n        elif rnn_succeeds and not attention_succeeds:\n            code = 2\n        elif not rnn_succeeds and attention_succeeds:\n            code = 1\n        else: # both fail\n            code = 0\n            \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3191175"}, {"introduction": "成功处理长序列不仅取决于具体架构，更涉及到在记忆容量、序列长度和信息复杂度之间的根本权衡。这个练习将这些挑战抽象成一个信息论模型，使我们能够量化记忆衰减如何影响模型的能力——不仅仅是记住训练数据，更重要的是泛化到未见过的序列变体。[@problem_id:3191156]", "problem": "考虑经典的序列复制任务，该任务用于探究循环模型（如循环神经网络 RNN）中的长期依赖问题。一个输入流包含一个长度为 $L$、基于大小为 $A$ 的字母表的连续目标块，其后跟着 $D$ 个干扰符元。在这 $D$ 个干扰符元之后，出现一个分隔符，该分隔符向模型发出信号，要求其按顺序准确地输出（复制）原始目标块。假设目标块在字母表上是均匀随机的，并且与干扰符元无关。模型具有 $B_0$ 比特的固定基础内存容量。由于随时间反向传播中的乘法效应，跨越 $D$ 个步骤保留的信息会受到指数衰减的影响，该衰减由一个保留因子 $\\gamma \\in (0, 1]$ 控制。假设在 $D$ 个干扰符元之后，任何已存储信息的保留分数由一个以 $\\gamma$ 为底、以 $D$ 为指数的指数定律描述。进一步假设：\n- 存储来自大小为 $A$ 的字母表中的 $1$ 个符号需要 $\\log_2 A$ 比特，因此无损地存储 $L$ 个符号需要 $L \\log_2 A$ 比特。\n- 模型可以优先决定存储哪些符号，任何被完全存储的符号都会被正确复现。\n- 未被完全存储的符号将从 $A$ 种可能性中独立且均匀随机地进行猜测。\n\n仅根据这些原则和定义（不引入额外假设），推导出以下表达式：\n1. 在 $D$ 个干扰符元后可用于回忆目标块的有效内存容量（以比特为单位），表示为 $B_0$、$\\gamma$ 和 $D$ 的函数。\n2. 可以确定地复现的目标符号的最大数量，表示为有效容量和 $\\log_2 A$ 的整数函数。\n3. 在延迟后复制长度为 $L$ 的目标块时的期望单符号准确率，考虑到完全存储的符号是正确的，其余符号是均匀猜测的。\n4. 整个 $L$ 符号块被完全复制（所有位置都正确）的概率，同样是在对任何未完全存储的位置采用均匀猜测模型的条件下。\n\n定义一个模型从训练配置“泛化”到测试配置，当且仅当以下两个条件都成立：\n- 在训练参数下，模型可以完美地复制训练目标块（所有 $L_{\\mathrm{train}}$ 个符号都正确）。\n- 在测试参数下，完全复制概率（对整个块而言）至少为指定的阈值 $\\tau$。\n\n您的任务是实现一个程序，该程序：\n- 为每个测试配置计算期望的单符号准确率和完全复制概率。\n- 在给定训练和测试配置以及相同的 $(B_0, \\gamma)$ 的情况下，根据上述标准确定模型是否“泛化”。\n\n使用以下参数集测试套件。每个测试用例提供 $(A, L, D, B_0, \\gamma, A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}}, \\tau)$：\n- 测试 1（中度衰减的理想情况）：$(A, L, D, B_0, \\gamma) = (10, 5, 20, 100, 0.98)$，$(A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}}) = (10, 5, 10)$，$\\tau = 0.9$。\n- 测试 2（精确容量且无衰减的边界情况）：$(A, L, D, B_0, \\gamma) = (256, 4, 100, 32, 1.0)$，$(A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}}) = (256, 4, 0)$，$\\tau = 0.999$。\n- 测试 3（大字母表压力测试；训练和测试延迟相同）：$(A, L, D, B_0, \\gamma) = (64, 8, 5, 25, 0.95)$，$(A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}}) = (4, 8, 5)$，$\\tau = 0.9$。\n- 测试 4（长延迟导致的极端遗忘）：$(A, L, D, B_0, \\gamma) = (10, 10, 20, 100, 0.5)$，$(A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}}) = (10, 10, 0)$，$\\tau = 0.5$。\n- 测试 5（小字母表，长延迟）：$(A, L, D, B_0, \\gamma) = (2, 16, 40, 32, 0.9)$，$(A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}}) = (2, 8, 0)$，$\\tau = 0.75$。\n\n程序要求：\n- 对每个测试用例，计算：\n  - 期望的单符号准确率（浮点数）。\n  - 完全复制概率（整个块正确）（浮点数）。\n  - 根据上述两部分定义得出的泛化决策（布尔值）。\n- 数值输出必须是浮点数，用于表示准确率和概率，并四舍五入到 $12$ 位小数，决策则为布尔值。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。每个测试用例对应一个形式为 $[\\text{单符号准确率}, \\text{完全复制概率}, \\text{是否泛化}]$ 的子列表，子列表内部没有空格。例如：$[[0.123000000000,0.045000000000,True],[...],...]$。\n\n此问题不涉及物理单位或角度。请确保算法是从上述原则推导出来的，而不是来自任何外部未声明的公式。每个测试用例的最终答案必须可按规定计算为布尔值或浮点数。", "solution": "问题陈述经评估有效。它在信息论方面有科学依据，定义明确，并提供了一套自洽、客观的定义和原则，可以从中推导出唯一的解决方案。我们继续进行推导和求解。\n\n该解决方案是通过将问题陈述中描述的原则形式化而推导出来的。\n\n**1. 有效内存容量 ($B_{\\mathrm{eff}}$)**\n模型具有 $B_0$ 比特的基础内存容量。经过 $D$ 步延迟后保留的信息会因保留因子 $\\gamma$ 而发生指数衰减。信息保留的比例由 $\\gamma^D$ 给出。因此，延迟后可用于回忆目标块的有效内存容量 $B_{\\mathrm{eff}}$ 是基础容量乘以该衰减因子。\n$$B_{\\mathrm{eff}}(B_0, \\gamma, D) = B_0 \\cdot \\gamma^D$$\n\n**2. 可存储符号的最大数量**\n存储一个来自大小为 $A$ 的字母表的符号需要 $B_{\\mathrm{symbol}} = \\log_2 A$ 比特的信息。模型可以利用其有效内存容量 $B_{\\mathrm{eff}}$ 完美地存储一定数量的符号。可以完全存储的最大符号数 $N_{\\mathrm{max\\_store}}$ 是总有效内存除以每个符号的成本。由于符号是离散单位，我们取该比率的整数部分。\n$$N_{\\mathrm{max\\_store}} = \\left\\lfloor \\frac{B_{\\mathrm{eff}}}{B_{\\mathrm{symbol}}} \\right\\rfloor = \\left\\lfloor \\frac{B_0 \\gamma^D}{\\log_2 A} \\right\\rfloor$$\n在复制特定长度为 $L$ 的目标块时，模型存储的符号数不能超过块中存在的符号数。因此，被存储并可以确定地复现的实际符号数（我们记为 $N_{\\mathrm{stored}}$）是最大可存储数和块长度 $L$ 中的较小者。\n$$N_{\\mathrm{stored}} = \\min(L, N_{\\mathrm{max\\_store}}) = \\min\\left(L, \\left\\lfloor \\frac{B_0 \\gamma^D}{\\log_2 A} \\right\\rfloor\\right)$$\n\n**3. 期望单符号准确率 ($Acc_{\\mathrm{symbol}}$)**\n目标块由 $L$ 个符号组成。其中，$N_{\\mathrm{stored}}$ 个符号被完美回忆，每个符号的准确率为 $1$。剩下的 $L - N_{\\mathrm{stored}}$ 个符号未存储在内存中，必须进行猜测。问题陈述指出，这些猜测是从大小为 $A$ 的字母表中独立且均匀随机地进行的。因此，任何单个符号被正确猜中的概率是 $1/A$。\n\n正确符号的总期望数 $E[C]$ 是确定正确的符号数与期望正确猜测的符号数之和：\n$$E[C] = N_{\\mathrm{stored}} \\cdot 1 + (L - N_{\\mathrm{stored}}) \\cdot \\frac{1}{A}$$\n期望的单符号准确率是正确符号的总期望数除以块的总长度 $L$（假设 $L0$）。\n$$Acc_{\\mathrm{symbol}} = \\frac{E[C]}{L} = \\frac{N_{\\mathrm{stored}} + (L - N_{\\mathrm{stored}})/A}{L} = \\frac{N_{\\mathrm{stored}}}{L} \\left(1 - \\frac{1}{A}\\right) + \\frac{1}{A}$$\n\n**4. 完全复制概率 ($P_{\\mathrm{exact}}$)**\n要完全复制长度为 $L$ 的整个块，每个符号都必须正确。$N_{\\mathrm{stored}}$ 个符号以概率 $1$ 正确。剩下的 $L - N_{\\mathrm{stored}}$ 个符号必须全部被正确猜中。由于每次猜测都是一个独立事件，其正确的概率为 $1/A$，因此所有 $L - N_{\\mathrm{stored}}$ 次猜测都正确的概率是它们各自概率的乘积。\n$$P_{\\mathrm{exact}} = \\left(\\frac{1}{A}\\right)^{L - N_{\\mathrm{stored}}}$$\n如果 $N_{\\mathrm{stored}} = L$，所有符号都被正确存储，因此需要猜测的符号数量为 $0$。该公式正确地得出 $P_{\\mathrm{exact}} = (1/A)^0 = 1$。\n\n**泛化标准**\n问题定义，在给定训练配置 $(A_{\\mathrm{train}}, L_{\\mathrm{train}}, D_{\\mathrm{train}})$ 和测试配置 $(A_{\\mathrm{test}}, L_{\\mathrm{test}}, D_{\\mathrm{test}})$，以及共享参数 $(B_0, \\gamma)$ 和概率阈值 $\\tau$ 的情况下，如果满足两个条件，则模型“泛化”。\n\n**条件 1：在训练数据上完美复制。**\n如果模型在训练期间的有效内存足以存储所有 $L_{\\mathrm{train}}$ 个符号，则模型可以完美复制训练块。所需的内存为 $L_{\\mathrm{train}} \\log_2 A_{\\mathrm{train}}$。可用的内存为 $B_{\\mathrm{eff,train}} = B_0 \\gamma^{D_{\\mathrm{train}}}$。因此，条件是：\n$$B_0 \\gamma^{D_{\\mathrm{train}}} \\ge L_{\\mathrm{train}} \\log_2 A_{\\mathrm{train}}$$\n这等价于检查在训练配置中可存储的符号数量是否至少为 $L_{\\mathrm{train}}$：\n$$\\left\\lfloor \\frac{B_0 \\gamma^{D_{\\mathrm{train}}}}{\\log_2 A_{\\mathrm{train}}} \\right\\rfloor \\ge L_{\\mathrm{train}}$$\n\n**条件 2：在测试数据上足够高的完全复制概率。**\n测试块的完全复制概率 $P_{\\mathrm{exact,test}}$ 必须至少为阈值 $\\tau$。使用上面为测试参数推导的公式：\n$$P_{\\mathrm{exact,test}} = \\left(\\frac{1}{A_{\\mathrm{test}}}\\right)^{L_{\\mathrm{test}} - N_{\\mathrm{stored,test}}} \\ge \\tau$$\n其中 $N_{\\mathrm{stored,test}} = \\min\\left(L_{\\mathrm{test}}, \\left\\lfloor \\frac{B_0 \\gamma^{D_{\\mathrm{test}}}}{\\log_2 A_{\\mathrm{test}}} \\right\\rfloor\\right)$。\n\n当且仅当条件 1 和条件 2 都为真时，模型才被认为已经泛化。\n\n现在我们将实现一个程序，将这些推导出的公式应用于提供的测试套件。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the sequence copy generalization problem for a suite of test cases.\n    \"\"\"\n    # Test cases as (A, L, D, B0, gamma, A_train, L_train, D_train, tau)\n    test_cases = [\n        # Test 1\n        (10, 5, 20, 100, 0.98, 10, 5, 10, 0.9),\n        # Test 2\n        (256, 4, 100, 32, 1.0, 256, 4, 0, 0.999),\n        # Test 3\n        (64, 8, 5, 25, 0.95, 4, 8, 5, 0.9),\n        # Test 4\n        (10, 10, 20, 100, 0.5, 10, 10, 0, 0.5),\n        # Test 5\n        (2, 16, 40, 32, 0.9, 2, 8, 0, 0.75),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, L, D, B0, gamma, A_train, L_train, D_train, tau = case\n\n        # --- Test Configuration Calculations ---\n        \n        # Effective memory and bits per symbol\n        b_eff_test = B0 * np.power(gamma, D)\n        bits_per_symbol_test = np.log2(A)\n        \n        # Number of symbols stored\n        if bits_per_symbol_test == 0: # Handles A=1 case, though not in tests\n            n_max_store_test = float('inf')\n        else:\n            n_max_store_test = np.floor(b_eff_test / bits_per_symbol_test)\n        \n        n_stored_test = int(min(L, n_max_store_test))\n        \n        # 1. Per-symbol accuracy\n        if L == 0:\n            acc_symbol = 1.0\n        else:\n            expected_correct = n_stored_test + (L - n_stored_test) * (1 / A)\n            acc_symbol = expected_correct / L\n\n        # 2. Exact-copy probability\n        guessed_symbols_count = L - n_stored_test\n        exact_copy_prob = np.power(1 / A, guessed_symbols_count)\n\n        # --- Generalization Check ---\n\n        # Condition 1: Perfect copy on training data\n        b_eff_train = B0 * np.power(gamma, D_train)\n        bits_per_symbol_train = np.log2(A_train)\n        \n        if bits_per_symbol_train == 0:\n             n_max_store_train = float('inf')\n        else:\n             n_max_store_train = np.floor(b_eff_train / bits_per_symbol_train)\n        \n        cond1_met = n_max_store_train >= L_train\n\n        # Condition 2: High enough exact-copy probability on test data\n        cond2_met = exact_copy_prob >= tau\n\n        # Final generalization decision\n        generalizes = cond1_met and cond2_met\n        \n        results.append([\n            acc_symbol,\n            exact_copy_prob,\n            generalizes\n        ])\n    \n    # Format the final output string\n    result_strings = []\n    for res in results:\n        acc_str = f\"{res[0]:.12f}\"\n        prob_str = f\"{res[1]:.12f}\"\n        gen_str = str(res[2])\n        result_strings.append(f\"[{acc_str},{prob_str},{gen_str}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3191156"}]}