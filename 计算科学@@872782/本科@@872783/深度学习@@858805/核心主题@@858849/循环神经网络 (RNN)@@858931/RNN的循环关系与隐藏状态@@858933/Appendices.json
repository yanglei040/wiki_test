{"hands_on_practices": [{"introduction": "为了理解循环神经网络（RNN）如何学习，我们必须首先掌握其参数的梯度计算方法。这个练习将引导我们为一个简化的线性RNN推导损失函数关于循环权重矩阵的梯度。通过推导出梯度的闭式解 [@problem_id:3192146]，我们将揭示误差信号如何通过时间反向传播，并从根本上理解时间反向传播（BPTT）算法的计算效率来源。", "problem": "考虑一个线性的循环神经网络 (RNN)，其隐藏状态维度为 $d_h$，输入维度为 $d_x$，输出维度为 $d_y$，由一个长度为 $T$ 的单一序列驱动。其循环和读出由以下核心定义给出：\n$h_t = W_h h_{t-1} + W_x x_t$，其中 $h_0 = 0$，以及 $y_t = W_y h_t$，\n对于 $t \\in \\{1,2,\\dots,T\\}$，其中 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$，$W_x \\in \\mathbb{R}^{d_h \\times d_x}$，以及 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入向量为 $\\{x_t \\in \\mathbb{R}^{d_x}\\}_{t=1}^T$，目标为 $\\{s_t \\in \\mathbb{R}^{d_y}\\}_{t=1}^T$。考虑二次损失函数\n$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2$。\n设每个 $t$ 的输出残差为 $e_t = y_t - s_t$。仅使用循环定义、读出定义、多元微积分中的链式法则以及标准的线性代数知识，推导梯度 $\\nabla_{W_h} L$ 的闭式解析表达式，该表达式应明确地将其分解为形如 $\\sum e_{\\cdot}\\, h_{\\cdot}^{\\top}$ 的时移相关项之和。然后，简要讨论在实现以下两种情况时评估此梯度的计算复杂性（使用大$O$表示法）：(i) 通过对时移项求和直接根据您的闭式表达式计算，以及 (ii) 通过使用伴随信号的一阶递归进行时间反向传播 (BPTT) 计算。您的最终答案必须是仅用 $W_h$、$W_y$、$\\{e_t\\}$ 和 $\\{h_t\\}$ 表示的 $\\nabla_{W_h} L$ 的单一闭式解析矩阵表达式。不应包含任何数值计算。", "solution": "此问题经评估有效。它是一个适定、有科学依据且客观的深度学习理论领域的问题陈述。它要求推导标准线性循环神经网络 (RNN) 的梯度，这是一个基于多元微积分和线性代数的既定原则的形式化且可解的任务。所有必要的定义和条件均已提供。\n\n我们首先陈述问题中提供的核心定义：\n循环关系：$h_t = W_h h_{t-1} + W_x x_t$，对于 $t \\in \\{1, 2, \\dots, T\\}$，初始状态 $h_0 = 0$。\n读出方程：$y_t = W_y h_t$。\n损失函数：$L = \\frac{1}{2} \\sum_{t=1}^T \\|y_t - s_t\\|_2^2 = \\frac{1}{2} \\sum_{t=1}^T \\|e_t\\|_2^2$，其中 $e_t = y_t - s_t$。\n参数为矩阵 $W_h \\in \\mathbb{R}^{d_h \\times d_h}$，$W_x \\in \\mathbb{R}^{d_h \\times d_x}$，以及 $W_y \\in \\mathbb{R}^{d_y \\times d_h}$。输入为向量 $x_t \\in \\mathbb{R}^{d_x}$，目标为 $s_t \\in \\mathbb{R}^{d_y}$。\n\n我们的目标是推导损失函数 $L$ 关于循环权重矩阵 $W_h$ 的梯度的闭式表达式，记为 $\\nabla_{W_h} L$。我们将使用矩阵微积分的链式法则。\n\n损失 $L$ 是 $W_h$ 的函数，通过其对隐藏状态 $\\{h_t\\}_{t=1}^T$ 的影响而实现。梯度可以表示为前向传播中每个使用 $W_h$ 的时间步的贡献之和：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\nabla_{W_h}^{(\\text{local at } t)} L\n$$\n其中“局部”梯度指的是在计算 $h_t = W_h h_{t-1} + W_x x_t$ 中使用的 $W_h$ 实例的梯度。使用链式法则，该贡献由损失相对于状态 $h_t$ 的梯度与状态更新项相对于 $W_h$ 的梯度的外积给出。令 $\\delta_t = \\frac{\\partial L}{\\partial h_t} \\in \\mathbb{R}^{d_h}$ 为总损失相对于隐藏状态 $h_t$ 的梯度。预激活 $a_t = W_h h_{t-1} + W_x x_t$ 相对于 $W_h$ 的梯度是 $h_{t-1}^T$。因此，时间步 $t$ 对梯度的贡献是 $\\delta_t h_{t-1}^T$。对所有时间步求和得到总梯度：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\delta_t h_{t-1}^T\n$$\n这是 BPTT 中循环权重梯度的标准公式。然而，问题要求用给定的量 $W_h, W_y, \\{e_t\\}, \\{h_t\\}$ 来表示一个闭式表达式。为实现此目的，我们必须推导出 $\\delta_t$ 的闭式形式。\n\n向量 $\\delta_t$ 表示 $h_t$ 对损失 $L$ 的总影响。状态 $h_t$ 通过所有后续时间步 $k \\ge t$ 的输出 $y_k$ 影响损失。\n$$\n\\delta_t = \\frac{\\partial L}{\\partial h_t} = \\sum_{k=t}^{T} \\frac{\\partial L}{\\partial y_k} \\frac{\\partial y_k}{\\partial h_k} \\frac{\\partial h_k}{\\partial h_t}\n$$\n在这里，我们应用链式法则，对从 $h_t$到最终损失的所有路径求和。偏导数如下（使用分子布局矩阵微积分约定，其中标量对列向量的梯度是列向量）：\n$1.$ $\\frac{\\partial L}{\\partial y_k} = \\frac{\\partial}{\\partial y_k} \\left( \\frac{1}{2} \\sum_{j=1}^T \\|y_j - s_j\\|_2^2 \\right) = y_k - s_k = e_k$。这是一个大小为 $d_y$ 的列向量。\n$2.$ $\\frac{\\partial y_k}{\\partial h_k} = W_y$。这是大小为 $d_y \\times d_h$ 的雅可比矩阵。链式法则中的项是其转置，即 $(\\frac{\\partial y_k}{\\partial h_k})^T = W_y^T$。\n$3.$ $\\frac{\\partial h_k}{\\partial h_t}$。我们通过展开循环关系来求得此项：\n$h_k = W_h h_{k-1} + W_x x_k = W_h (W_h h_{k-2} + W_x x_{k-1}) + W_x x_k = \\dots$\n$k > t$ 时 $h_k$ 对 $h_t$ 的依赖关系由下式给出：\n$h_k = W_h^{k-t} h_t + \\sum_{j=t+1}^{k} W_h^{k-j} W_x x_j$。\n因此，对于 $k \\ge t$，雅可比矩阵为 $\\frac{\\partial h_k}{\\partial h_t} = W_h^{k-t}$。链式法则组合中的项是其转置，即 $(W_h^{k-t})^T = (W_h^T)^{k-t}$。\n\n将这些导数代回到 $\\delta_t$ 的表达式中：\n$$\n\\delta_t = \\sum_{k=t}^{T} \\left( \\frac{\\partial h_k}{\\partial h_t} \\right)^T \\left( \\frac{\\partial y_k}{\\partial h_k} \\right)^T \\frac{\\partial L}{\\partial y_k} = \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k\n$$\n这为从所有未来时间步 $k \\ge t$ 反向传播的梯度信号 $\\delta_t$ 提供了闭式表达式。\n\n最后，我们将这个 $\\delta_t$ 的表达式代入总梯度 $\\nabla_{W_h} L$ 的方程中：\n$$\n\\nabla_{W_h} L = \\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T\n$$\n这就是所要求的闭式解析表达式。它将梯度分解为关于时间的双重求和。每一项 $(W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$ 可以解释为未来时间 $k$ 的误差 $e_k$ 与过去时间 $t-1$ 的隐藏状态 $h_{t-1}$ 之间的“相关性”，该相关性由矩阵 $(W_h^T)^{k-t} W_y^T$ 介导，该矩阵说明了影响通过网络动态和读出层的传播。\n\n接下来，我们讨论计算复杂性。\n(i) 直接从闭式表达式求值：\n公式为 $\\sum_{t=1}^{T} \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k h_{t-1}^T$。\n假设前向传播已经运行，因此所有的 $e_t \\in \\mathbb{R}^{d_y}$ 和 $h_t \\in \\mathbb{R}^{d_h}$ 都是已知的。\n双重循环涉及 $O(T^2)$ 个项。对于每个项 $(t, k)$，计算涉及：\n- 矩阵幂 $(W_h^T)^{k-t}$，如果未预先计算，需要 $O((k-t) d_h^3)$。预先计算所有需要的 $W_h^T$ 的幂次直到 $T-1$ 需要 $O(T d_h^3)$。\n- 矩阵向量积 $W_y^T e_k$，成本为 $O(d_h d_y)$。\n- 矩阵向量积 $(W_h^T)^{k-t} (W_y^T e_k)$，成本为 $O(d_h^2)$。\n- 与 $h_{t-1}^T$ 的外积，成本为 $O(d_h^2)$。\n一个朴素的实现将非常昂贵。一个更优化的直接求值过程如下：\n`total_grad = 0`\n`for t = 1 to T:`\n  `inner_sum_vec = 0`\n  `for k = t to T:`\n    `vec = ... compute (W_h^T)^{k-t} W_y^T e_k ...`\n    `inner_sum_vec += vec`\n  `total_grad += outer(inner_sum_vec, h_{t-1})`\n最昂贵的部分是双重循环结构。大约有 $T^2/2$ 对 $(t,k)$。对于每一对，假设幂已预先计算，主要成本是计算矩阵幂与向量的乘积，为 $O(d_h^2)$。预计算成本为 $O(T d_h^3)$。因此，总复杂度由嵌套循环和预计算主导，导致复杂度为 $O(T d_h^3 + T^2 d_h^2)$。对序列长度 $T$ 的二次依赖性使得这种方法对于长序列来说计算成本过高。\n\n(ii) 通过时间反向传播 (BPTT) 求值：\nBPTT 通过使用动态规划避免了双重求和的显式计算。它通过一次时间上的反向传播来计算梯度。关键是 $\\delta_t$ 的递归关系：\n$\\delta_T = W_y^T e_T$\n$\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$ for $t = T-1, \\dots, 1$.\n计算 $\\nabla_{W_h} L$ 的 BPTT 算法如下：\n$1.$ 执行从 $t=1$ 到 $T$ 的前向传播，以计算并存储所有的 $h_t$ 和 $e_t$。成本：$O(T(d_h^2 + d_h d_x + d_y d_h))$。\n$2.$ 初始化 $\\nabla_{W_h} L = 0$ 和 $\\delta_{T+1} = 0$。\n$3.$ 执行从 $t=T$ 到 $1$ 的反向传播：\n   a. 计算 $\\delta_t = W_y^T e_t + W_h^T \\delta_{t+1}$。这涉及两个矩阵向量乘积和一个向量加法，成本为 $O(d_h d_y + d_h^2)$。\n   b. 将贡献加到梯度上：$\\nabla_{W_h} L \\leftarrow \\nabla_{W_h} L + \\delta_t h_{t-1}^T$。这是一个外积和矩阵加法，成本为 $O(d_h^2)$。\n反向传播包含一个长度为 $T$ 的单循环，每步成本为 $O(d_h^2 + d_h d_y)$。反向传播的总复杂度为 $O(T(d_h^2 + d_h d_y))$。\n因此，BPTT 的总复杂度为 $O(T(d_h^2 + d_h d_x + d_y d_h))$，这在序列长度 $T$ 上是线性的。这比直接求值闭式表达式要高效得多。", "answer": "$$\n\\boxed{\\sum_{t=1}^{T} \\left( \\sum_{k=t}^{T} (W_h^T)^{k-t} W_y^T e_k \\right) h_{t-1}^T}\n$$", "id": "3192146"}, {"introduction": "真实世界中的RNN很少是完全线性的；它们的强大能力源于非线性激活函数。本练习将探讨修正线性单元（ReLU）激活函数如何影响RNN的动态行为。通过分析一个二维ReLU-RNN的状态演化 [@problem_id:3192158]，我们将亲身体验激活稀疏性如何随时间持续，并推导隐藏状态被“困”在低维空间中的条件，从而深化对非线性动力学在RNN中的关键作用的理解。", "problem": "考虑一个离散时间循环神经网络（RNN），其具有二维隐藏状态和修正线性单元（ReLU）非线性。设其递归关系由对预激活值逐元素应用 ReLU（修正线性单元）函数定义，因此隐藏状态的更新由下式给出：\n$$\n\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right),\n$$\n其中 $\\phi$ 逐分量作用，$\\phi(z)=\\max(0,z)$，$\\mathbf{h}_{t}\\in\\mathbb{R}^{2}$ 是在时间 $t$ 的隐藏状态，$\\mathbf{W}\\in\\mathbb{R}^{2\\times 2}$，$\\mathbf{U}\\in\\mathbb{R}^{2\\times 1}$，$\\mathbf{b}\\in\\mathbb{R}^{2}$，且输入是在每个时间步应用的恒定标量 $u\\in\\mathbb{R}$。参数和初始隐藏状态为：\n$$\n\\mathbf{W}=\\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix},\\quad\n\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix},\\quad\n\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix},\\quad\n\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}.\n$$\n定义一维锥体\n$$\n\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}0,\\ h_{2}=0\\right\\}.\n$$\n从 RNN 递归和 ReLU 非线性的基本定义出发，分析激活稀疏性（由 ReLU 产生的零值）如何随时间持续存在，并推导隐藏状态在所有 $t\\geq 1$ 时都被困在低维锥体 $\\mathcal{C}$ 中的条件。然后，计算最大的实数 $u$，使得在每个步骤施加恒定输入 $u$ 的情况下，隐藏状态对所有 $t\\geq 1$ 都满足 $\\mathbf{h}_{t}\\in\\mathcal{C}$。最终答案必须是一个实数。不需要四舍五入。", "solution": "用户要求我找到最大的实数 $u$，使得对于给定的循环神经网络（RNN），其隐藏状态 $\\mathbf{h}_t$ 在所有时间步 $t \\geq 1$ 都被约束在特定的锥体 $\\mathcal{C}$ 内。\n\n### 步骤1：问题验证\n\n**1.1. 提取已知条件**\n- **递归关系**：$\\mathbf{h}_{t}=\\phi\\!\\left(\\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}\\right)$，其中 $\\phi(z)=\\max(0,z)$ (ReLU) 逐元素应用。\n- **状态向量**：$\\mathbf{h}_{t} = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix} \\in\\mathbb{R}^{2}$。\n- **权重矩阵**：$\\mathbf{W}=\\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix}$。\n- **输入矩阵**：$\\mathbf{U}=\\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}$。\n- **偏置向量**：$\\mathbf{b}=\\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}$。\n- **输入**：$u \\in \\mathbb{R}$，对所有 $t$ 恒定。\n- **初始状态**：$\\mathbf{h}_{0}=\\begin{pmatrix}0.1\\\\ 0\\end{pmatrix}$。\n- **目标锥体**：$\\mathcal{C}=\\left\\{(h_{1},h_{2})\\in\\mathbb{R}^{2}\\,:\\,h_{1}0,\\ h_{2}=0\\right\\}$。\n- **条件**：找到最大的 $u$，使得对所有 $t \\geq 1$，都有 $\\mathbf{h}_{t} \\in \\mathcal{C}$。\n\n**1.2. 使用提取的已知条件进行验证**\n这个问题是一个定义明确的练习，旨在分析离散时间切换线性系统的动力学，这是带有 ReLU 激活函数的 RNN 的模型。\n- **科学依据**：该问题基于深度学习和动力系统理论的标准定义。\n- **良构性**：它要求在一个可推导的约束集合下求一个参数的最大值。问题陈述是自洽的，并提供了确定唯一解所需的所有信息。\n- **目标**：所有术语都经过数学定义，目标是计算一个单一的数值。\n\n该问题是有效的，因为它没有违反任何指定的标准。我将继续进行求解。\n\n### 步骤2：推导状态约束的条件\n\n条件 $\\mathbf{h}_{t} \\in \\mathcal{C}$ 对所有 $t \\geq 1$ 成立，意味着对于每个 $t \\geq 1$，隐藏状态向量 $\\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}$ 必须满足：\n1.  $h_{t,1}  0$\n2.  $h_{t,2} = 0$\n\n令时间 $t$ 的预激活向量为 $\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1}+\\mathbf{U}u+\\mathbf{b}$。那么 $\\mathbf{h}_t = \\phi(\\mathbf{z}_t)$。\n逐元素的 ReLU 函数意味着：\n$h_{t,1} = \\max(0, z_{t,1})$\n$h_{t,2} = \\max(0, z_{t,2})$\n\n为了使 $\\mathbf{h}_t$ 在 $\\mathcal{C}$ 内，我们必须有：\n1.  $\\max(0, z_{t,1})  0 \\implies z_{t,1}  0$。在这种情况下，$h_{t,1} = z_{t,1}$。\n2.  $\\max(0, z_{t,2}) = 0 \\implies z_{t,2} \\leq 0$。\n\n因此，对于所有 $t \\geq 1$，我们必须满足 $z_{t,1}  0$ 和 $z_{t,2} \\leq 0$。\n\n### 步骤3：对时间步 t=1 的分析\n\n我们从初始状态 $\\mathbf{h}_0 = \\begin{pmatrix} 0.1 \\\\ 0 \\end{pmatrix}$ 开始。我们计算预激活值 $\\mathbf{z}_1$：\n$$\n\\mathbf{z}_1 = \\mathbf{W}\\mathbf{h}_0 + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix} \\begin{pmatrix}0.1\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3\\\\ 0.1\\end{pmatrix}u + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_1 = \\begin{pmatrix}0.6 \\times 0.1\\\\ -0.5 \\times 0.1\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix} = \\begin{pmatrix}0.06 + 0.3u - 0.2\\\\ -0.05 + 0.1u - 0.05\\end{pmatrix} = \\begin{pmatrix}0.3u - 0.14\\\\ 0.1u - 0.1\\end{pmatrix}\n$$\n为了使 $\\mathbf{h}_1 \\in \\mathcal{C}$，我们需要 $z_{1,1}  0$ 和 $z_{1,2} \\leq 0$：\n1.  $z_{1,1} = 0.3u - 0.14  0 \\implies 0.3u  0.14 \\implies u  \\frac{0.14}{0.3} = \\frac{14}{30} = \\frac{7}{15}$。\n2.  $z_{1,2} = 0.1u - 0.1 \\leq 0 \\implies 0.1u \\leq 0.1 \\implies u \\leq 1$。\n\n因此，对 $u$ 的一个必要条件是 $\\frac{7}{15}  u \\leq 1$。\n如果这个条件成立，在 $t=1$ 时的状态是 $\\mathbf{h}_1 = \\begin{pmatrix} z_{1,1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0.3u - 0.14 \\\\ 0 \\end{pmatrix}$。\n\n### 步骤4：对 t = 2 的归纳分析\n\n我们通过归纳法进行。假设对于某个 $t-1 \\geq 1$，状态在锥体中，即 $\\mathbf{h}_{t-1} = \\begin{pmatrix} h_{t-1,1} \\\\ 0 \\end{pmatrix}$ 且 $h_{t-1,1}  0$。我们需要找到使 $\\mathbf{h}_t$ 也位于 $\\mathcal{C}$ 内的关于 $u$ 的条件。\n\n让我们计算 $\\mathbf{z}_t$：\n$$\n\\mathbf{z}_t = \\mathbf{W}\\mathbf{h}_{t-1} + \\mathbf{U}u + \\mathbf{b} = \\begin{pmatrix}0.6  -0.4\\\\ -0.5  0.2\\end{pmatrix} \\begin{pmatrix}h_{t-1,1}\\\\ 0\\end{pmatrix} + \\begin{pmatrix}0.3u\\\\ 0.1u\\end{pmatrix} + \\begin{pmatrix}-0.2\\\\ -0.05\\end{pmatrix}\n$$\n$$\n\\mathbf{z}_t = \\begin{pmatrix} 0.6 h_{t-1,1} + 0.3u - 0.2 \\\\ -0.5 h_{t-1,1} + 0.1u - 0.05 \\end{pmatrix}\n$$\n为了使 $\\mathbf{h}_t \\in \\mathcal{C}$，我们需要 $z_{t,1}  0$ 和 $z_{t,2} \\leq 0$：\n1.  $z_{t,1}  0 \\implies h_{t,1} = 0.6 h_{t-1,1} + 0.3u - 0.2  0$。\n2.  $z_{t,2} \\leq 0 \\implies -0.5 h_{t-1,1} + 0.1u - 0.05 \\leq 0$。\n\n第二个不等式提供了对 $h_{t-1,1}$ 的一个下界：\n$$\n0.5 h_{t-1,1} \\geq 0.1u - 0.05 \\implies h_{t-1,1} \\geq \\frac{0.1u - 0.05}{0.5} \\implies h_{t-1,1} \\geq 0.2u - 0.1\n$$\n这个不等式必须对所有 $t-1 \\geq 1$ 成立。因此，第一个隐藏状态分量的序列，我们称之为 $x_t = h_{t,1}$，必须对所有 $t \\geq 1$ 满足 $x_t \\geq 0.2u - 0.1$。\n\n$x_t$ 的动力学由第一个条件给出的线性递归关系决定：\n$x_t = 0.6 x_{t-1} + (0.3u - 0.2)$，对 $t \\geq 2$，且 $x_1 = 0.3u - 0.14$。\n\n首先，考虑下界为非正的情况：$0.2u - 0.1 \\leq 0 \\implies u \\leq 0.5$。到目前为止 $u$ 的完整范围是 $\\frac{7}{15}  u \\leq 1$。如果 $u \\in (\\frac{7}{15}, 0.5]$，那么 $0.3u - 0.2$ 是负的。$x_t$ 递归关系的不动点是 $x_{fp} = \\frac{0.3u - 0.2}{1 - 0.6} = \\frac{0.3u - 0.2}{0.4} = 0.75u - 0.5$，当 $u \\leq 0.5$ 时它也是负的。序列 $x_t$ 从一个正值 $x_1  0$ 开始，并收敛到一个负的不动点 $x_{fp}  0$。因此，$x_t$ 最终必须变为负值，这违反了条件 $x_t0$。因此，对于 $u \\leq 0.5$ 没有解。\n\n我们必须有 $u  0.5$。在这种情况下，$0.2u - 0.1  0$。条件 $x_t \\geq 0.2u-0.1$ 比 $x_t  0$ 更强并能推导出后者。所以我们只需要确保对所有 $t \\geq 1$ 都有 $x_t \\geq 0.2u - 0.1$。这等价于要求序列 $\\{x_t\\}_{t\\geq 1}$ 的最小值满足此界限：$\\min_{t \\geq 1} x_t \\geq 0.2u - 0.1$。\n\n序列 $x_t$ 是单调的，因为它是一个一阶线性递归。其行为取决于起始点 $x_1$ 和不动点 $x_{fp}$ 之间的关系。\n- $x_1 = 0.3u - 0.14$\n- $x_{fp} = 0.75u - 0.5$\n差值为 $x_1 - x_{fp} = (0.3u - 0.14) - (0.75u - 0.5) = -0.45u + 0.36 = 0.09(4-5u)$。\n差值的符号取决于 $u$ 是小于还是大于 $\\frac{4}{5} = 0.8$。\n\n**情况1：$u  0.8$**\n在这种情况下，$x_1 - x_{fp}  0$，所以 $x_1  x_{fp}$。序列 $\\{x_t\\}$ 是单调递减的，并从上方收敛到 $x_{fp}$。\n最小值为下确界，$\\min_{t \\geq 1} x_t = \\inf_{t \\geq 1} x_t = x_{fp}$。\n条件变为 $x_{fp} \\geq 0.2u - 0.1$。\n$0.75u - 0.5 \\geq 0.2u - 0.1 \\implies 0.55u \\geq 0.4 \\implies u \\geq \\frac{0.4}{0.55} = \\frac{40}{55} = \\frac{8}{11}$。\n所以对于这种情况我们需要 $u \\in [\\frac{8}{11}, 0.8)$。注意 $\\frac{8}{11} \\approx 0.727$，这与 $u  0.5$ 和 $u  7/15 \\approx 0.467$ 一致。\n\n**情况2：$u  0.8$**\n在这种情况下，$x_1 - x_{fp}  0$，所以 $x_1  x_{fp}$。序列 $\\{x_t\\}$ 是单调递增的，并从下方收敛到 $x_{fp}$。\n最小值为第一项，$\\min_{t \\geq 1} x_t = x_1$。\n条件变为 $x_1 \\geq 0.2u - 0.1$。\n$0.3u - 0.14 \\geq 0.2u - 0.1 \\implies 0.1u \\geq 0.04 \\implies u \\geq 0.4$。\n对于这种情况，我们有 $u  0.8$。条件 $u \\geq 0.4$ 被自动满足。我们还必须满足初始约束 $u \\leq 1$。所以这种情况给出了范围 $u \\in (0.8, 1]$。\n\n**情况3：$u = 0.8$**\n在这种情况下，$x_1 = x_{fp}$。序列是恒定的：$x_t = x_1$ 对所有 $t\\geq 1$ 成立。\n$x_1 = 0.3(0.8) - 0.14 = 0.24 - 0.14 = 0.1$。\n条件是 $x_t \\geq 0.2u - 0.1$。\n$0.1 \\geq 0.2(0.8) - 0.1 = 0.16 - 0.1 = 0.06$。\n这是成立的。初始约束 $u \\in (\\frac{7}{15}, 1]$ 对于 $u=0.8$ 也被满足。所以 $u=0.8$ 是一个有效的解。\n\n### 步骤5：最终解\n\n综合所有情况的区间：\n- 来自情况1：$u \\in [\\frac{8}{11}, 0.8)$\n- 来自情况3：$u = 0.8$\n- 来自情况2：$u \\in (0.8, 1]$\n\n这些集合的并集给出了 $u$ 的有效值的总范围：\n$$\nu \\in \\left[\\frac{8}{11}, 1\\right]\n$$\n问题要求满足条件的最大实数 $u$。在区间 $[\\frac{8}{11}, 1]$ 中的最大值是 $1$。", "answer": "$$\n\\boxed{1}\n$$", "id": "3192158"}, {"introduction": "在许多科学和工程应用中，我们的任务不是从给定的方程出发去模拟系统，而是反过来——从观测到的数据中推断出系统的内在动力学。这个练习将我们置于“系统辨识”的情境中，要求我们根据一系列输入和隐藏状态的观测值来恢复一个线性RNN的未知参数矩阵 [@problem_id:3192180]。通过将此问题构建为一个最小二乘回归问题，我们不仅能练习关键的线性代数技巧，还能体会到RNN模型与更广泛的动力系统理论之间的深刻联系。", "problem": "考虑一个线性循环神经网络（RNN），其中隐藏状态更新使用恒等激活函数。隐藏状态 $\\mathbf{h}_t \\in \\mathbb{R}^2$ 根据以下递推关系演化\n$$\n\\mathbf{h}_t = A \\mathbf{h}_{t-1} + B x_t,\n$$\n其中状态转移矩阵 $A \\in \\mathbb{R}^{2 \\times 2}$ 和输入矩阵 $B \\in \\mathbb{R}^{2 \\times 1}$ 未知。给定初始隐藏状态 $\\mathbf{h}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 以及在 $t = 1, 2, 3, 4, 5$ 时观测到的以下输入-状态对 $(x_t, \\mathbf{h}_t)$：\n$$\n\\begin{aligned}\nx_1 = 1, \\quad \\mathbf{h}_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\\\\nx_2 = 2, \\quad \\mathbf{h}_2 = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix}, \\\\\nx_3 = 1, \\quad \\mathbf{h}_3 = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix}, \\\\\nx_4 = 0, \\quad \\mathbf{h}_4 = \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix}, \\\\\nx_5 = -1, \\quad \\mathbf{h}_5 = \\begin{pmatrix} 27 \\\\ 3 \\end{pmatrix}.\n\\end{aligned}\n$$\n从最小二乘估计的基本定义（最小化残差平方和）出发，通过从递推关系中显式地构建一个线性回归问题，并求解相关的正规方程，来推导 $A$ 和 $B$ 的估计量，过程中不使用任何现成的公式。然后，将您的推导应用于给定数据，以恢复 $A$ 和 $B$ 的数值。\n\n此外，请从线性系统的第一性原理出发，通过讨论由时间序列数据构建的块汉克尔（block Hankel）矩阵的作用以及当输出即为隐藏状态本身时状态的可观测性，来论证恢复的唯一性。您的论证应清楚说明秩条件是如何从这些构造中产生的，以及为什么它们对 $A$ 和 $B$ 的可辨识性至关重要。\n\n您的最终答案必须是一个单一的复合解析表达式，其中包含 $A$ 和 $B$ 的所有元素，按 $(a_{11}, a_{12}, a_{21}, a_{22}, b_1, b_2)$ 的顺序列于一个行矩阵中。无需四舍五入。", "solution": "该问题是有效的。这是一个系统辨识领域中的适定问题，该领域是工程学和机器学习中的一个核心课题。该问题具有科学依据，是客观的，并包含足够的信息来确定唯一解。任务要求从第一性原理推导估计量并论证其唯一性，这些都是在此背景下标准且适当的问题。\n\n隐藏状态的演化由以下线性递推关系给出：\n$$\n\\mathbf{h}_t = A \\mathbf{h}_{t-1} + B x_t\n$$\n其中 $\\mathbf{h}_t \\in \\mathbb{R}^2$，$A \\in \\mathbb{R}^{2 \\times 2}$，$B \\in \\mathbb{R}^{2 \\times 1}$。待估计的参数是矩阵 $A$ 和 $B$ 的元素。\n让我们定义矩阵和向量如下：\n$$\nA = \\begin{pmatrix} a_{11}  a_{12} \\\\ a_{21}  a_{22} \\end{pmatrix}, \\quad B = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix}, \\quad \\mathbf{h}_t = \\begin{pmatrix} h_{t,1} \\\\ h_{t,2} \\end{pmatrix}\n$$\n该递推关系可以分解为针对每一行的两个独立的标量方程：\n$$\nh_{t,1} = a_{11} h_{t-1,1} + a_{12} h_{t-1,2} + b_1 x_t \\\\\nh_{t,2} = a_{21} h_{t-1,1} + a_{22} h_{t-1,2} + b_2 x_t\n$$\n这种结构揭示了两个独立的线性回归问题。对于第一行，参数为 $\\boldsymbol{\\theta}_1 = \\begin{pmatrix} a_{11}  a_{12}  b_1 \\end{pmatrix}^T$，目标变量是 $h_{t,1}$。对于第二行，参数为 $\\boldsymbol{\\theta}_2 = \\begin{pmatrix} a_{21}  a_{22}  b_2 \\end{pmatrix}^T$，目标是 $h_{t,2}$。两个回归共享同一组回归量，可以排列成一个向量 $\\mathbf{z}_t = \\begin{pmatrix} h_{t-1,1}  h_{t-1,2}  x_t \\end{pmatrix}^T$。因此，方程变为：\n$$\nh_{t,1} = \\mathbf{z}_t^T \\boldsymbol{\\theta}_1 \\\\\nh_{t,2} = \\mathbf{z}_t^T \\boldsymbol{\\theta}_2\n$$\n为了使用 $t = 1, \\dots, 5$ 的给定数据估计参数，我们将数据组装成矩阵。设 $N=5$ 为观测次数。我们定义一个设计矩阵 $Z$，其中每一行对应一个给定时间步的回归量，以及两个标量子问题的目标向量 $Y_1$ 和 $Y_2$。\n$$\nZ = \\begin{pmatrix} \\mathbf{z}_1^T \\\\ \\mathbf{z}_2^T \\\\ \\vdots \\\\ \\mathbf{z}_N^T \\end{pmatrix} = \\begin{pmatrix} \\mathbf{h}_0^T  x_1 \\\\ \\mathbf{h}_1^T  x_2 \\\\ \\vdots \\\\ \\mathbf{h}_{N-1}^T  x_N \\end{pmatrix}, \\quad Y_1 = \\begin{pmatrix} h_{1,1} \\\\ h_{2,1} \\\\ \\vdots \\\\ h_{N,1} \\end{pmatrix}, \\quad Y_2 = \\begin{pmatrix} h_{1,2} \\\\ h_{2,2} \\\\ \\vdots \\\\ h_{N,2} \\end{pmatrix}\n$$\n所有时间步的整个方程组可以写成 $Y_1 \\approx Z \\boldsymbol{\\theta}_1$ 和 $Y_2 \\approx Z \\boldsymbol{\\theta}_2$。\n\n最小二乘原理要求我们找到使残差（误差）平方和最小化的参数向量 $\\boldsymbol{\\theta}$。对于第一个子问题，目标函数是残差向量的欧几里得范数的平方：\n$$\nS(\\boldsymbol{\\theta}_1) = || Y_1 - Z \\boldsymbol{\\theta}_1 ||_2^2 = (Y_1 - Z \\boldsymbol{\\theta}_1)^T (Y_1 - Z \\boldsymbol{\\theta}_1) = Y_1^T Y_1 - 2 Y_1^T Z \\boldsymbol{\\theta}_1 + \\boldsymbol{\\theta}_1^T Z^T Z \\boldsymbol{\\theta}_1\n$$\n为了找到最小值，我们计算 $S(\\boldsymbol{\\theta}_1)$ 相对于 $\\boldsymbol{\\theta}_1$ 的梯度并将其设为零。使用标准的矩阵微积分恒等式，我们有：\n$$\n\\nabla_{\\boldsymbol{\\theta}_1} S(\\boldsymbol{\\theta}_1) = -2 Z^T Y_1 + 2 Z^T Z \\boldsymbol{\\theta}_1 = 0\n$$\n这就得到了著名的正规方程：\n$$\n(Z^T Z) \\boldsymbol{\\theta}_1 = Z^T Y_1\n$$\n假设矩阵 $Z^T Z$ 是可逆的，则 $\\boldsymbol{\\theta}_1$ 的唯一最小二乘估计量为：\n$$\n\\hat{\\boldsymbol{\\theta}}_1 = (Z^T Z)^{-1} Z^T Y_1\n$$\n对 $\\boldsymbol{\\theta}_2$ 进行相同的推导，得到 $\\hat{\\boldsymbol{\\theta}}_2 = (Z^T Z)^{-1} Z^T Y_2$。\n\n现在，我们将此推导应用于给定数据。已知值为 $\\mathbf{h}_0 = \\begin{pmatrix} 0  0 \\end{pmatrix}^T$ 以及：\n$\\mathbf{h}_1 = \\begin{pmatrix} 1  1 \\end{pmatrix}^T$ 对于 $x_1=1$；\n$\\mathbf{h}_2 = \\begin{pmatrix} 5  3 \\end{pmatrix}^T$ 对于 $x_2=2$；\n$\\mathbf{h}_3 = \\begin{pmatrix} 12  4 \\end{pmatrix}^T$ 对于 $x_3=1$；\n$\\mathbf{h}_4 = \\begin{pmatrix} 20  4 \\end{pmatrix}^T$ 对于 $x_4=0$；\n$\\mathbf{h}_5 = \\begin{pmatrix} 27  3 \\end{pmatrix}^T$ 对于 $x_5=-1$。\n\n我们构建矩阵 $Z$、$Y_1$ 和 $Y_2$：\n$$\nZ = \\begin{pmatrix} \\mathbf{h}_0^T  x_1 \\\\ \\mathbf{h}_1^T  x_2 \\\\ \\mathbf{h}_2^T  x_3 \\\\ \\mathbf{h}_3^T  x_4 \\\\ \\mathbf{h}_4^T  x_5 \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\\\ 1  1  2 \\\\ 5  3  1 \\\\ 12  4  0 \\\\ 20  4  -1 \\end{pmatrix}, \\quad Y_1 = \\begin{pmatrix} 1 \\\\ 5 \\\\ 12 \\\\ 20 \\\\ 27 \\end{pmatrix}, \\quad Y_2 = \\begin{pmatrix} 1 \\\\ 3 \\\\ 4 \\\\ 4 \\\\ 3 \\end{pmatrix}\n$$\n我们可以不直接计算 $(Z^T Z)^{-1}$，而是检验是否存在一个完美解。一个完美解将产生零误差，这是残差平方和的绝对最小值，因此它将是唯一的最小二乘解。\n\n根据 $\\mathbf{h}_1 = A \\mathbf{h}_0 + B x_1$：\n$$\n\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + B (1) \\implies B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n根据 $\\mathbf{h}_2 = A \\mathbf{h}_1 + B x_2$：\n$$\n\\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (2) \\implies A \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\n这给出了线性方程：$a_{11} + a_{12} = 3$ 和 $a_{21} + a_{22} = 1$。\n\n根据 $\\mathbf{h}_3 = A \\mathbf{h}_2 + B x_3$：\n$$\n\\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} = A \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (1) \\implies A \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 11 \\\\ 3 \\end{pmatrix}\n$$\n这给出了线性方程：$5a_{11} + 3a_{12} = 11$ 和 $5a_{21} + 3a_{22} = 3$。\n\n我们现在求解关于 $A$ 的行的两个方程组：\n对于 $A$ 的第一行：\n1) $a_{11} + a_{12} = 3 \\implies a_{12} = 3 - a_{11}$\n2) $5a_{11} + 3a_{12} = 11 \\implies 5a_{11} + 3(3 - a_{11}) = 11 \\implies 2a_{11} + 9 = 11 \\implies 2a_{11}=2 \\implies a_{11}=1$。\n因此，$a_{12} = 3 - 1 = 2$。\n\n对于 $A$ 的第二行：\n1) $a_{21} + a_{22} = 1 \\implies a_{22} = 1 - a_{21}$\n2) $5a_{21} + 3a_{22} = 3 \\implies 5a_{21} + 3(1 - a_{21}) = 3 \\implies 2a_{21} + 3 = 3 \\implies 2a_{21}=0 \\implies a_{21}=0$。\n因此，$a_{22} = 1 - 0 = 1$。\n\n这给出了候选解：$A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。我们必须用剩余的数据点来验证这个解。\n对于 $t=4$：\n$$ \\mathbf{h}_4 = A \\mathbf{h}_3 + B x_4 = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 12 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (0) = \\begin{pmatrix} 12+8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix} $$\n这与给定的 $\\mathbf{h}_4$ 相符。\n对于 $t=5$：\n$$ \\mathbf{h}_5 = A \\mathbf{h}_4 + B x_5 = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} (-1) = \\begin{pmatrix} 20+8 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 28-1 \\\\ 4-1 \\end{pmatrix} = \\begin{pmatrix} 27 \\\\ 3 \\end{pmatrix} $$\n这与给定的 $\\mathbf{h}_5$ 相符。\n由于所提出的解完全拟合所有数据点，残差平方和为零。这是非负目标函数 $S(\\boldsymbol{\\theta})$ 的全局最小值。因此，这是最小二乘法提供的精确且唯一的解。\n\n恢复出的矩阵为 $A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。\n\n**恢复唯一性的论证**\n最小二乘解 $\\hat{\\boldsymbol{\\theta}} = (Z^T Z)^{-1} Z^T Y$ 的唯一性取决于矩阵 $Z^T Z$ 的可逆性。该矩阵可逆当且仅当设计矩阵 $Z$ 具有满列秩。在我们的例子中，$Z$ 是一个 $5 \\times 3$ 矩阵，所以它必须有秩 3。这意味着它的三列必须是线性无关的。\n\n从线性系统的第一性原理角度来看，从数据中唯一辨识系统参数 $(A, B)$ 的能力依赖于两个关键属性：输入信号的丰富性以及系统本身的结构特性，即可控性和可观测性。\n\n1.  **可观测性**：如果一个系统的内部状态可以从其输出中唯一确定，则该系统是可观测的。对于输出方程为 $\\mathbf{y}_t = C \\mathbf{h}_t$ 的系统，标准的可观测性矩阵是 $\\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ \\dots \\\\ CA^{n-1} \\end{pmatrix}$。在这个问题中，输出是隐藏状态本身，这对应于将输出矩阵设为 $C=I$（单位矩阵）。可观测性矩阵为 $\\mathcal{O} = \\begin{pmatrix} I \\\\ A \\\\ \\dots \\\\ A^{n-1} \\end{pmatrix}$。由于该矩阵包含单位块 $I$，其列保证是线性无关的，并且其秩为 $n=2$。因此，任何状态被直接测量的线性系统都是平凡可观测的。\n\n2.  **可控性与持续激励**：可控性确保输入 $x_t$ 可以将状态向量 $\\mathbf{h}_t$ 驱动到状态空间中的任意点。它由可控性矩阵 $\\mathcal{C}_n = \\begin{pmatrix} B  AB  \\dots  A^{n-1}B \\end{pmatrix}$ 的秩决定。如果一个系统不可控，其某些内部动态对输入是免疫的，状态向量 $\\mathbf{h}_t$ 将被限制在一个子空间内。即使有直接的状态观测，如果我们不能激励系统的所有模态，我们就无法学习控制这些模态的参数。同时，输入序列 $\\{x_t\\}$ 必须是“持续激励的”，意味着它必须足够丰富以防止退化。例如，一个恒定的输入可能不足以区分 $A$ 和 $B$ 的影响。可控性和持续激励的输入共同确保回归量向量 $\\mathbf{z}_t^T = (\\mathbf{h}_{t-1}^T, x_t)$ 能够充分张成回归量空间，从而使设计矩阵 $Z$ 的列线性无关。\n\n输入和输出数据的块汉克尔（block Hankel）矩阵的构建是子空间辨识方法（例如 N4SID）的基石，这些方法是为从输入-输出对 $(x_t, y_t)$ 中辨识 $(A, B, C)$ 这一更普遍的问题而设计的。这些汉克尔矩阵的某个投影的秩揭示了系统阶数 $n$，并且这个秩条件与可观测性矩阵和可控性矩阵的乘积有根本的联系。在我们的特殊情况下，由于可观测性得到保证（$C=I$），可辨识性条件得以简化，并更直接地由矩阵 $Z$ 具有满秩的要求来表示。\n\n对于我们的具体问题，我们已经找到 $A = \\begin{pmatrix} 1  2 \\\\ 0  1 \\end{pmatrix}$ 和 $B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$。可控性矩阵是 $\\mathcal{C}_2 = \\begin{pmatrix} B  AB \\end{pmatrix} = \\begin{pmatrix} 1  3 \\\\ 1  1 \\end{pmatrix}$，其行列式为 $1-3 = -2 \\neq 0$。该系统是可控的。输入序列 $\\{x_t\\}$ 和产生的状态 $\\{ \\mathbf{h}_t \\}$ 生成了一个设计矩阵 $Z$，其列是线性无关的，从而使得 $Z^T Z$ 可逆，并保证了 $(A, B)$ 的唯一解。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  2  0  1  1  1 \\end{pmatrix}}\n$$", "id": "3192180"}]}