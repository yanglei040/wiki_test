{"hands_on_practices": [{"introduction": "要理解权重和偏置的作用，最直接的方法就是观察它们如何共同决定一个神经元的输出。本练习将剥离网络的复杂性，聚焦于单个线性单元。通过分析缩放输入和权重如何影响决策结果，我们可以揭示出偏置作为可调决策阈值的基本角色，从而为净输入函数 $z = \\mathbf{w}^\\top\\mathbf{x} + b$ 建立清晰的数学直觉。[@problem_id:3199772]", "problem": "考虑人工神经网络（ANN）中的单个线性单元（一个神经元），其净输入由核心线性模型和偏置定义，即 $z=\\mathbf{w}^{\\top}\\mathbf{x}+b$，其中 $\\mathbf{w}\\in\\mathbb{R}^{d}$ 是权重向量，$\\mathbf{x}\\in\\mathbb{R}^{d}$ 是输入向量，$b\\in\\mathbb{R}$ 是偏置。神经元的决策是净输入的符号，由函数 $\\operatorname{sign}(z)$ 给出，其中如果 $t0$，$\\operatorname{sign}(t)$ 返回 $+1$；如果 $t=0$，返回 $0$；如果 $t  0$，返回 $-1$。假设输入和权重分别同时被实数因子 $\\alpha$ 和 $\\beta$ 缩放，而偏置保持不变，即 $\\mathbf{x}\\leftarrow\\alpha\\mathbf{x}$，$\\mathbf{w}\\leftarrow\\beta\\mathbf{w}$，$b\\leftarrow b$。\n\n仅从净输入和决策函数的定义出发，推导在缩放后神经元的决策 $\\operatorname{sign}(z)$ 保持不变时，$(\\alpha,\\beta)$ 必须满足的充要条件。根据内积 $s=\\mathbf{w}^{\\top}\\mathbf{x}$ 和偏置 $b$ 对各种情况进行全面描述，区分平凡与非平凡场景，并解释这些区别产生的原因。\n\n最后，令 $\\gamma=\\alpha\\beta$ 表示作用于内积的有效净缩放。以闭式解的形式，确定 $\\gamma$ 的临界值 $\\gamma^{\\star}$（表示为 $s$ 和 $b$ 的符号表达式），在该临界值下，神经元的决策在缩放变换下会翻转其符号。请以单一解析表达式的形式提供最终答案。无需四舍五入。", "solution": "神经元的净输入由带偏置的线性模型定义，$z=\\mathbf{w}^{\\top}\\mathbf{x}+b$。决策是 $\\operatorname{sign}(z)$，取值于 $\\{-1,0,+1\\}$。在同时进行缩放 $\\mathbf{x}\\leftarrow\\alpha\\mathbf{x}$ 和 $\\mathbf{w}\\leftarrow\\beta\\mathbf{w}$ 并保持 $b$ 不变的情况下，变换后的净输入变为\n$$\nz'=(\\beta\\mathbf{w})^{\\top}(\\alpha\\mathbf{x})+b=\\alpha\\beta\\,\\mathbf{w}^{\\top}\\mathbf{x}+b.\n$$\n引入简写 $s=\\mathbf{w}^{\\top}\\mathbf{x}$。那么原始和变换后的净输入为\n$$\nz=s+b,\\qquad z'=\\alpha\\beta\\,s+b.\n$$\n我们要求决策保持不变，即 $\\operatorname{sign}(z')=\\operatorname{sign}(z)$，这等价于要求 $z'$ 与 $z$ 保持在零的同一侧，或者当且仅当 $z$ 等于零时，$z'$ 也等于零。\n\n我们根据 $s$ 和 $b$ 的值分情况讨论：\n\n1. 情况 $s=0$。此时 $z=b$ 且 $z'=\\alpha\\beta\\cdot 0+b=b$。因此，对于所有实数 $\\alpha$ 和 $\\beta$，都有 $\\operatorname{sign}(z')=\\operatorname{sign}(b)=\\operatorname{sign}(z)$。这是一个平凡不变情况：内积对净输入没有贡献，因此缩放 $\\mathbf{x}$ 和 $\\mathbf{w}$ 没有影响。\n\n2. 情况 $s\\neq 0$。此时 $z=s+b$ 且 $z'=\\alpha\\beta\\,s+b$。定义 $\\gamma=\\alpha\\beta$。不变性条件 $\\operatorname{sign}(z')=\\operatorname{sign}(z)$ 等价于要求 $z'$ 和 $z$ 具有相同的符号（包括零）。符号可能改变的 $\\gamma$ 的边界发生在 $z'=0$ 时，即\n$$\n\\gamma\\,s+b=0.\n$$\n解出 $\\gamma$ 可得临界值\n$$\n\\gamma^{\\star}=-\\frac{b}{s}.\n$$\n这个值将产生正净输入和负净输入的 $\\gamma$ 区域分开。具体来说：\n- 如果 $s0$，则 $z'= \\gamma s + b$ 是 $\\gamma$ 的增函数。因此：\n  - 如果 $z=s+b0$，要保持不变性，则要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b0$，要保持不变性，则要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b=0$，在边界上要保持不变性，则要求 $\\gamma=\\gamma^{\\star}$（任何偏离都会改变符号）。\n- 如果 $s0$，则 $z'=\\gamma s + b$ 是 $\\gamma$ 的减函数。因此：\n  - 如果 $z=s+b0$，要保持不变性，则要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b0$，要保持不变性则要求 $\\gamma\\gamma^{\\star}$。\n  - 如果 $z=s+b=0$，在边界上要保持不变性，则要求 $\\gamma=\\gamma^{\\star}$（任何偏离都会改变符号）。\n\n这些结果表明，对于 $s\\neq 0$ 和 $z\\neq 0$（非平凡场景），保持决策不变的 $(\\alpha,\\beta)$ 集合是由 $s$ 的符号以及原始净输入位于边界 $\\gamma^{\\star}$ 的哪一侧所决定的，以 $\\gamma=\\alpha\\beta$ 表示的半空间。决策翻转的精确阈值是那个唯一的、能使变换后的净输入为零的缩放乘积。\n\n特殊子情况 $b=0$（且 $s\\neq 0$）展示了一个经典的缩放不变性：$z=s$ 且 $z'=\\gamma s$。决策保持不变当且仅当 $\\gamma0$（$\\gamma=0$ 会使得 $z'=0$）。此处，$\\gamma^{\\star}=-b/s=0$，这与符号可能改变的边界一致。\n\n因此，导致决策翻转的有效缩放乘积的临界值，完全由偏置与内积的比值给出，并带一个负号：\n$$\n\\gamma^{\\star}=-\\frac{b}{s}.\n$$\n这个表达式简洁地描述了非平凡情况 $s\\neq 0$；在平凡情况 $s=0$ 时，缩放对决策没有影响，并且在 $\\gamma$ 中没有有意义的有限阈值，因为 $z'$ 与 $\\gamma$ 无关。", "answer": "$$\\boxed{-\\frac{b}{s}}$$", "id": "3199772"}, {"introduction": "在我们理解了权重和偏置的不同角色后，接下来我们将探究它们在训练过程中的动态行为有何不同。本练习要求分析损失函数表面相对于权重和偏置的曲率。通过计算海森矩阵(Hessian)，我们可以量化这种差异，并理解为什么在优化过程中区别对待这些参数（例如，为它们设置不同的学习率）可能会更有效率。[@problem_id:3199848]", "problem": "考虑二元逻辑回归，其净输入函数定义为 $z_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b$，概率模型为 $p_i = \\sigma(z_i)$，其中 $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$ 是logistic S型函数。损失函数是二元交叉熵（BCE），定义为 $L(\\mathbf{w}, b) = \\sum_{i=1}^{n}\\left[-y_i \\ln(p_i) - (1 - y_i)\\ln(1 - p_i)\\right]$。给定 $n = 3$ 个训练样本，其特征在 $\\mathbb{R}^{2}$ 中，标签为\n$$(\\mathbf{x}_1, y_1) = \\left(\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, 0\\right), \\quad (\\mathbf{x}_2, y_2) = \\left(\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, 1\\right), \\quad (\\mathbf{x}_3, y_3) = \\left(\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}, 1\\right)。$$\n在当前迭代点，参数为 $\\mathbf{w} = \\begin{pmatrix}0.5 \\\\ -1.0\\end{pmatrix}$ 和 $b = 0.2$。\n\n从上述定义和多元微积分的基本原理出发，推导损失函数关于偏置和权重的Hessian矩阵块，即标量块 $H_{bb}$ 和矩阵块 $H_{\\mathbf{w}\\mathbf{w}}$。然后，对这些块进行数值计算。令 $\\lambda_{\\max}$ 表示 $H_{\\mathbf{w}\\mathbf{w}}$ 的最大特征值。\n\n为了设计一个利用梯度下降法中 $b$ 更快收敛的训练方案，我们为 $b$ 和 $\\mathbf{w}$ 设置不同的学习率 $\\alpha_b$ 和 $\\alpha_{\\mathbf{w}}$。假设我们通过选择比率\n$$\\frac{\\alpha_b}{\\alpha_{\\mathbf{w}}} = \\frac{\\lambda_{\\max}}{H_{bb}}$$\n来使最大曲率归一化步长相等。\n请计算在给定点处的这个比率，并提供其四舍五入到四位有效数字的数值。你的最终答案必须是一个实数。", "solution": "该问题要求计算学习率之比，该比率由逻辑回归模型的二元交叉熵（BCE）损失函数的Hessian矩阵的分量定义。这个过程包括三个主要部分：首先，从基本原理出发，推导所需Hessian矩阵块的解析表达式；其次，在给定点计算它们的数值；第三，计算所求的比率。\n\n总损失由 $L(\\mathbf{w}, b) = \\sum_{i=1}^{n}L_i$ 给出，其中 $L_i = \\left[-y_i \\ln(p_i) - (1 - y_i)\\ln(1 - p_i)\\right]$。概率 $p_i$ 是模型参数 $\\mathbf{w}$ 和 $b$ 的函数，通过关系式 $p_i = \\sigma(z_i)$ 和 $z_i = \\mathbf{w}^{\\top}\\mathbf{x}_i + b$ 联系起来。函数 $\\sigma(u)$ 是logistic S型函数，$\\sigma(u) = (1 + \\exp(-u))^{-1}$。\n\n首先，我们推导S型函数的一个关键性质，即其导数。使用链式法则：\n$$ \\frac{d\\sigma(u)}{du} = \\frac{d}{du}(1 + \\exp(-u))^{-1} = -1(1 + \\exp(-u))^{-2} (-\\exp(-u)) = \\frac{\\exp(-u)}{(1 + \\exp(-u))^2} $$\n这可以用 $\\sigma(u)$ 本身来表示：\n$$ \\frac{d\\sigma(u)}{du} = \\frac{1}{1 + \\exp(-u)} \\cdot \\frac{\\exp(-u)}{1 + \\exp(-u)} = \\sigma(u) (1 - \\sigma(u)) $$\n我们将其记为 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$。\n\n现在，我们推导Hessian矩阵块 $H_{bb} = \\frac{\\partial^2 L}{\\partial b^2}$ 和 $H_{\\mathbf{w}\\mathbf{w}} = \\frac{\\partial^2 L}{\\partial \\mathbf{w} \\partial \\mathbf{w}^{\\top}}$。Hessian矩阵是每个数据点Hessian矩阵的和，即 $H = \\sum_{i=1}^n H_i$。\n\n我们从 $L$ 的一阶导数开始。使用链式法则，$L_i$ 关于一个通用参数 $\\theta$（可以是 $\\mathbf{w}$ 的一个分量或 $b$）的导数是：\n$$ \\frac{\\partial L_i}{\\partial \\theta} = \\frac{\\partial L_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\theta} $$\n各项为：\n$$ \\frac{\\partial L_i}{\\partial p_i} = -\\frac{y_i}{p_i} + \\frac{1-y_i}{1-p_i} = \\frac{-y_i(1-p_i) + p_i(1-y_i)}{p_i(1-p_i)} = \\frac{p_i - y_i}{p_i(1-p_i)} $$\n$$ \\frac{\\partial p_i}{\\partial z_i} = \\sigma'(z_i) = p_i(1 - p_i) $$\n结合这些，我们得到 $\\frac{\\partial L_i}{\\partial z_i} = p_i - y_i$。\n\n对于偏置 $b$，我们有 $\\frac{\\partial z_i}{\\partial b} = 1$。总损失关于 $b$ 的一阶导数是：\n$$ \\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{n} \\frac{\\partial L_i}{\\partial b} = \\sum_{i=1}^{n} (p_i - y_i) $$\n二阶导数，即Hessian矩阵块 $H_{bb}$，是：\n$$ H_{bb} = \\frac{\\partial^2 L}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\sum_{i=1}^{n} (p_i - y_i) = \\sum_{i=1}^{n} \\frac{\\partial p_i}{\\partial b} = \\sum_{i=1}^{n} \\frac{\\partial p_i}{\\partial z_i}\\frac{\\partial z_i}{\\partial b} = \\sum_{i=1}^{n} p_i(1-p_i) $$\n\n对于权重向量 $\\mathbf{w}$，我们有 $\\nabla_{\\mathbf{w}} z_i = \\mathbf{x}_i$。总损失关于 $\\mathbf{w}$ 的梯度是：\n$$ \\nabla_{\\mathbf{w}} L = \\sum_{i=1}^{n} \\nabla_{\\mathbf{w}} L_i = \\sum_{i=1}^{n} (p_i - y_i) \\mathbf{x}_i $$\nHessian矩阵块 $H_{\\mathbf{w}\\mathbf{w}}$ 是该梯度关于 $\\mathbf{w}^{\\top}$ 的雅可比矩阵。\n$$ H_{\\mathbf{w}\\mathbf{w}} = \\frac{\\partial}{\\partial \\mathbf{w}^{\\top}} \\left( \\sum_{i=1}^{n} (p_i - y_i) \\mathbf{x}_i \\right) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\mathbf{w}^{\\top}} \\left( (\\sigma(\\mathbf{w}^{\\top}\\mathbf{x}_i + b) - y_i) \\mathbf{x}_i \\right) $$\n使用雅可比矩阵的乘法法则 $J(u\\mathbf{v}) = (\\nabla u)\\mathbf{v}^{\\top} + u J(\\mathbf{v})$。这里，$\\mathbf{v}=\\mathbf{x}_i$ 是常数，所以其雅可比矩阵为零。标量函数是 $u = p_i - y_i$。其梯度是 $\\nabla_{\\mathbf{w}} p_i = \\sigma'(z_i) \\nabla_{\\mathbf{w}} z_i = p_i(1-p_i)\\mathbf{x}_i$。所以，我们有：\n$$ H_{\\mathbf{w}\\mathbf{w}} = \\sum_{i=1}^{n} (\\nabla_{\\mathbf{w}} p_i) \\mathbf{x}_i^{\\top} = \\sum_{i=1}^{n} (p_i(1-p_i)\\mathbf{x}_i) \\mathbf{x}_i^{\\top} = \\sum_{i=1}^{n} p_i(1-p_i) \\mathbf{x}_i \\mathbf{x}_i^{\\top} $$\n\n接下来，我们计算数值。参数为 $\\mathbf{w} = \\begin{pmatrix}0.5 \\\\ -1.0\\end{pmatrix}$ 和 $b = 0.2$。\n数据点为 $(\\mathbf{x}_1, y_1) = (\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, 0)$，$(\\mathbf{x}_2, y_2) = (\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}, 1)$，$(\\mathbf{x}_3, y_3) = (\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}, 1)$。\n\n为每个点计算 $z_i$ 和 $p_i(1-p_i)$。为方便起见，令 $s_i = p_i(1-p_i)$。\n当 $i=1$ 时：\n$z_1 = \\mathbf{w}^{\\top}\\mathbf{x}_1 + b = (0.5)(1) + (-1.0)(0) + 0.2 = 0.7$。\n$p_1 = \\sigma(0.7) = \\frac{1}{1+\\exp(-0.7)}$。\n$s_1 = p_1(1-p_1) = \\sigma(0.7)(1-\\sigma(0.7)) = \\frac{\\exp(-0.7)}{(1+\\exp(-0.7))^2} \\approx 0.221714$。\n\n当 $i=2$ 时：\n$z_2 = \\mathbf{w}^{\\top}\\mathbf{x}_2 + b = (0.5)(0) + (-1.0)(1) + 0.2 = -0.8$。\n$p_2 = \\sigma(-0.8) = \\frac{1}{1+\\exp(0.8)}$。\n$s_2 = p_2(1-p_2) = \\sigma(-0.8)(1-\\sigma(-0.8)) = \\frac{\\exp(0.8)}{(1+\\exp(0.8))^2} \\approx 0.213910$。\n\n当 $i=3$ 时：\n$z_3 = \\mathbf{w}^{\\top}\\mathbf{x}_3 + b = (0.5)(1) + (-1.0)(1) + 0.2 = -0.3$。\n$p_3 = \\sigma(-0.3) = \\frac{1}{1+\\exp(0.3)}$。\n$s_3 = p_3(1-p_3) = \\sigma(-0.3)(1-\\sigma(-0.3)) = \\frac{\\exp(0.3)}{(1+\\exp(0.3))^2} \\approx 0.244458$。\n\n现在，计算Hessian矩阵块：\n$H_{bb} = \\sum_{i=1}^3 s_i = s_1 + s_2 + s_3 \\approx 0.221714 + 0.213910 + 0.244458 = 0.680082$。\n\n$H_{\\mathbf{w}\\mathbf{w}} = s_1 \\mathbf{x}_1\\mathbf{x}_1^{\\top} + s_2 \\mathbf{x}_2\\mathbf{x}_2^{\\top} + s_3 \\mathbf{x}_3\\mathbf{x}_3^{\\top}$\n$H_{\\mathbf{w}\\mathbf{w}} = s_1\\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} + s_2\\begin{pmatrix}0  0 \\\\ 0  1\\end{pmatrix} + s_3\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix} = \\begin{pmatrix}s_1+s_3  s_3 \\\\ s_3  s_2+s_3\\end{pmatrix}$。\n数值上，\n$H_{\\mathbf{w}\\mathbf{w}} \\approx \\begin{pmatrix} 0.221714 + 0.244458  0.244458 \\\\ 0.244458  0.213910 + 0.244458 \\end{pmatrix} = \\begin{pmatrix} 0.466172  0.244458 \\\\ 0.244458  0.458368 \\end{pmatrix}$。\n\n接下来，我们求 $H_{\\mathbf{w}\\mathbf{w}}$ 的最大特征值 $\\lambda_{\\max}$。对于一个 $2 \\times 2$ 的对称矩阵 $A = \\begin{pmatrix}a  c \\\\ c  b\\end{pmatrix}$，特征值 $\\lambda$ 是特征方程 $\\det(A-\\lambda I)=0$ 的根，即 $\\lambda^2 - (a+b)\\lambda + (ab-c^2)=0$。其解为 $\\lambda = \\frac{(a+b) \\pm \\sqrt{(a-b)^2+4c^2}}{2}$。\n最大特征值为 $\\lambda_{\\max} = \\frac{(a+b) + \\sqrt{(a-b)^2+4c^2}}{2}$。\n令 $a = s_1+s_3 \\approx 0.466172$，$b = s_2+s_3 \\approx 0.458368$，$c = s_3 \\approx 0.244458$。\n$a+b = s_1+s_2+2s_3 \\approx 0.924540$。\n$a-b = s_1-s_2 \\approx 0.007804$。\n$\\lambda_{\\max} \\approx \\frac{0.924540 + \\sqrt{(0.007804)^2 + 4(0.244458)^2}}{2}$。\n$\\lambda_{\\max} \\approx \\frac{0.924540 + \\sqrt{0.0000609 + 4(0.0597596)}}{2} = \\frac{0.924540 + \\sqrt{0.0000609 + 0.2390384}}{2}$。\n$\\lambda_{\\max} \\approx \\frac{0.924540 + \\sqrt{0.2390993}}{2} = \\frac{0.924540 + 0.4889778}{2} = \\frac{1.4135178}{2} = 0.706759$。\n\n使用更高精度的中间值可得：\n$H_{bb} \\approx 0.68008209$\n$\\lambda_{\\max} \\approx 0.70675868$\n\n最后，我们计算所需比率：\n$$ \\frac{\\alpha_b}{\\alpha_{\\mathbf{w}}} = \\frac{\\lambda_{\\max}}{H_{bb}} \\approx \\frac{0.70675868}{0.68008209} \\approx 1.0392244 $$\n将此值四舍五入到四位有效数字，得到 $1.039$。", "answer": "$$ \\boxed{1.039} $$", "id": "3199848"}, {"introduction": "最后的这个实践展示了一项强大的现实世界应用：独立于权重来控制偏置。我们将执行“事后校准”（post-hoc calibration），这是一种在模型训练完成后改善其概率预测质量的技术。通过推导交叉熵损失函数相对于偏置的梯度并实现一个简单的优化程序，我们将能够微调模型的输出概率，这直接展示了理解和操控偏置项的实用价值。[@problem_id:3199738]", "problem": "给定一个多类别线性-logit分类器，其对于输入向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 的类别 $k$ 的净输入函数定义为 $z_k(\\mathbf{x}) = \\mathbf{w}_k^\\top \\mathbf{x} + b_k$，其中 $\\mathbf{w}_k \\in \\mathbb{R}^d$ 是固定权重，而 $b_k \\in \\mathbb{R}$ 是特定于类别的偏置。预测的类别概率由 softmax 函数定义为 $p_k(\\mathbf{x}) = \\exp(z_k(\\mathbf{x}))/\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))$，对于一个带标签的样本 $(\\mathbf{x}, y)$，其中 $y \\in \\{0,1,\\dots,K-1\\}$，交叉熵损失为 $\\ell(\\mathbf{x}, y) = -\\log p_{y}(\\mathbf{x})$。\n\n任务 A (推导)：从上述净输入函数、softmax 函数和交叉熵损失的定义出发，为一个带标签的样本推导损失函数关于偏置 $b_k$ 的梯度。您的推导必须从这些定义开始，并遵循微积分的基本原理，包括乘法法则和链式法则。最终表达式必须适用于偏置的算法优化。不要引用快捷公式或预先推导出的结果。\n\n任务 B (算法校准与测量)：考虑一种事后校准方法，该方法仅调整偏置 $b_k$，同时保持所有权重 $\\mathbf{w}_k$ 固定。对于一个校准数据集，其 logits 矩阵为 $L^{\\text{val}} \\in \\mathbb{R}^{N_{\\text{val}} \\times K}$，标签为 $\\mathbf{y}^{\\text{val}} \\in \\{0,1,\\dots,K-1\\}^{N_{\\text{val}}}$，将校准后的 logits 定义为 $L^{\\text{val}} + \\mathbf{1} \\mathbf{b}^\\top$，其中 $\\mathbf{b} \\in \\mathbb{R}^K$ 是待学习的偏置向量，$\\mathbf{1} \\in \\mathbb{R}^{N_{\\text{val}}}$ 是全一向量。校准目标是校准数据集上的平均交叉熵。学习到 $\\mathbf{b}$ 之后，在一个单独的测试数据集上评估校准的效果，该测试数据集的 logits 为 $L^{\\text{test}} \\in \\mathbb{R}^{N_{\\text{test}} \\times K}$，标签为 $\\mathbf{y}^{\\text{test}} \\in \\{0,1,\\dots,K-1\\}^{N_{\\text{test}}}$。评估方法是计算测试数据集上平均负对数似然（交叉熵）的变化量，该变化量定义为校准前的平均交叉熵减去将学习到的偏置应用于 $L^{\\text{test}}$ 后的平均交叉熵。将此变化量表示为一个实数（浮点数）。\n\n您必须实现一个完整的、可运行的程序，该程序：\n- 将任务 A 中推导出的梯度转换为用于任务 B 中优化 $\\mathbf{b}$ 的算法形式。\n- 对每个校准数据集，使用基于梯度的方法通过最小化平均交叉熵来优化 $\\mathbf{b}$。\n- 按上文所述计算在测试数据集上的改善值，并四舍五入到 $6$ 位小数。\n\n使用以下数据集测试套件。每种情况由 $(L^{\\text{val}}, \\mathbf{y}^{\\text{val}}, L^{\\text{test}}, \\mathbf{y}^{\\text{test}})$ 定义，且 $K = 3$：\n\n情况 $1$ (一般情况)：\n$$\nL^{\\text{val}(1)} = \\begin{bmatrix}\n2.0  1.0  0.0\\\\\n1.5  -0.5  0.0\\\\\n0.0  1.0  1.0\\\\\n1.0  0.5  -1.0\\\\\n-0.5  0.0  0.5\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(1)} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\\\ 0\\\\ 2 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(1)} = \\begin{bmatrix}\n1.8  1.2  0.0\\\\\n1.4  -0.3  0.1\\\\\n0.1  0.9  1.0\\\\\n0.9  0.4  -0.8\\\\\n-0.6  0.1  0.6\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(1)} = \\begin{bmatrix} 0\\\\ 0\\\\ 2\\\\ 0\\\\ 2 \\end{bmatrix}.\n$$\n\n情况 $2$ (近乎完美的预测)：\n$$\nL^{\\text{val}(2)} = \\begin{bmatrix}\n3.0  0.0  -1.0\\\\\n0.0  3.0  -1.0\\\\\n-1.0  0.0  3.0\\\\\n3.5  -0.5  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 2\\\\ 0 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(2)} = \\begin{bmatrix}\n3.0  0.0  -1.0\\\\\n0.0  3.0  -1.0\\\\\n-1.0  0.0  3.0\\\\\n3.5  -0.5  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(2)} = \\begin{bmatrix} 0\\\\ 1\\\\ 2\\\\ 0 \\end{bmatrix}.\n$$\n\n情况 $3$ (无信息量的 logits，不平衡的标签)：\n$$\nL^{\\text{val}(3)} = \\begin{bmatrix}\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{val}(3)} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 1\\\\ 2 \\end{bmatrix},\n$$\n$$\nL^{\\text{test}(3)} = \\begin{bmatrix}\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\\\\\n0.0  0.0  0.0\n\\end{bmatrix},\\quad\n\\mathbf{y}^{\\text{test}(3)} = \\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 1\\\\ 2 \\end{bmatrix}.\n$$\n\n最终输出格式：您的程序应生成单行输出，其中包含分别为情况 $1$、情况 $2$ 和情况 $3$ 的三个改善值，四舍五入到 $6$ 位小数，格式为用方括号括起来的逗号分隔列表，例如 $[0.012345,0.000000,0.086421]$。", "solution": "该问题要求推导交叉熵损失函数相对于模型偏置的梯度，并为多类别线性-logit分类器实现一个偏置校准程序。该问题是适定的、科学上合理的，并为得出唯一解提供了所有必要信息。\n\n### A部分：梯度的推导\n\n第一个任务是推导交叉熵损失 $\\ell$ 关于特定类别偏置 $b_k$ 的梯度。该推导将从所提供的基本原理出发。\n\n对于单个带标签的样本 $(\\mathbf{x}, y)$，交叉熵损失定义为：\n$$\n\\ell(\\mathbf{x}, y) = -\\log p_{y}(\\mathbf{x})\n$$\n其中 $y$ 是真实类别索引，$p_{y}(\\mathbf{x})$ 是真实类别的预测概率。概率由 softmax 函数给出：\n$$\np_k(\\mathbf{x}) = \\frac{\\exp(z_k(\\mathbf{x}))}{\\sum_{j=1}^K \\exp(z_j(\\mathbf{x}))}\n$$\n类别 $k$ 的净输入函数（或 logit）是：\n$$\nz_k(\\mathbf{x}) = \\mathbf{w}_k^\\top \\mathbf{x} + b_k\n$$\n将 softmax 的定义代入损失函数，得到：\n$$\n\\ell = -\\log \\left( \\frac{\\exp(z_y)}{\\sum_{j=1}^K \\exp(z_j)} \\right)\n$$\n使用对数性质 $\\log(a/b) = \\log(a) - \\log(b)$，损失函数可以重写为：\n$$\n\\ell = - \\left( \\log(\\exp(z_y)) - \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right)\n$$\n由于 $\\log(\\exp(u)) = u$，这可以简化为 log-sum-exp 形式：\n$$\n\\ell = -z_y + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right)\n$$\n我们要求解 $\\ell$ 关于偏置项 $b_k$ 的偏导数，对于任意类别 $k \\in \\{0, 1, \\dots, K-1\\}$。使用微分学的求和法则：\n$$\n\\frac{\\partial \\ell}{\\partial b_k} = \\frac{\\partial}{\\partial b_k} \\left( -z_y + \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right) = -\\frac{\\partial z_y}{\\partial b_k} + \\frac{\\partial}{\\partial b_k}\\left( \\log\\left(\\sum_{j=1}^K \\exp(z_j)\\right) \\right)\n$$\n我们分别计算每一项。\n\n对于第一项，我们对 $z_y$ 关于 $b_k$ 求导。净输入 $z_j = \\mathbf{w}_j^\\top \\mathbf{x} + b_j$ 仅在 $j=k$ 时依赖于 $b_k$。因此：\n$$\n\\frac{\\partial z_j}{\\partial b_k} = \\delta_{jk}\n$$\n其中 $\\delta_{jk}$ 是克罗内克 δ，当 $j=k$ 时为 $1$，否则为 $0$。将此应用于 $z_y$：\n$$\n\\frac{\\partial z_y}{\\partial b_k} = \\delta_{yk}\n$$\n对于第二项，我们应用链式法则。令 $S = \\sum_{j=1}^K \\exp(z_j)$。该项为 $\\log(S)$。\n$$\n\\frac{\\partial}{\\partial b_k} \\log(S) = \\frac{1}{S} \\cdot \\frac{\\partial S}{\\partial b_k}\n$$\n现在，我们求解 $S$ 关于 $b_k$ 的导数：\n$$\n\\frac{\\partial S}{\\partial b_k} = \\frac{\\partial}{\\partial b_k} \\left( \\sum_{j=1}^K \\exp(z_j) \\right) = \\sum_{j=1}^K \\frac{\\partial}{\\partial b_k} \\exp(z_j)\n$$\n对和中的每一项应用链式法则：\n$$\n\\frac{\\partial}{\\partial b_k} \\exp(z_j) = \\exp(z_j) \\cdot \\frac{\\partial z_j}{\\partial b_k} = \\exp(z_j) \\cdot \\delta_{jk}\n$$\n求和式 $\\sum_{j=1}^K \\exp(z_j) \\cdot \\delta_{jk}$ 只包含一个非零项，即当 $j=k$ 时。因此，该和简化为 $\\exp(z_k)$。\n$$\n\\frac{\\partial S}{\\partial b_k} = \\exp(z_k)\n$$\n将此代回对数项的导数表达式中：\n$$\n\\frac{\\partial}{\\partial b_k} \\log(S) = \\frac{1}{S} \\cdot \\exp(z_k) = \\frac{\\exp(z_k)}{\\sum_{j=1}^K \\exp(z_j)} = p_k(\\mathbf{x})\n$$\n最后，我们将两部分结合起来，得到损失 $\\ell$ 关于偏置 $b_k$ 的完整梯度：\n$$\n\\frac{\\partial \\ell}{\\partial b_k} = p_k(\\mathbf{x}) - \\delta_{yk}\n$$\n该表达式是单个样本的梯度。如果 $k$ 是正确类别 ($k=y$)，梯度为 $p_y - 1$。如果 $k$ 是任何其他类别 ($k \\neq y$)，梯度为 $p_k$。以向量形式表示，即为 $\\nabla_{\\mathbf{b}}\\ell = \\mathbf{p} - \\mathbf{e}_y$，其中 $\\mathbf{p}$ 是 softmax 概率向量，$\\mathbf{e}_y$ 是真实标签 $y$ 的独热编码向量。\n\n### B部分：算法实现\n\n第二个任务是执行事后偏置校准并测量由此带来的改善。这涉及优化偏置 $\\mathbf{b}$ 以最小化校准数据集上的平均交叉熵损失。\n\n要最小化的目标函数是 $N_{\\text{val}}$ 个样本上的平均交叉熵：\n$$\n\\mathcal{L}(\\mathbf{b}) = \\frac{1}{N_{\\text{val}}} \\sum_{i=1}^{N_{\\text{val}}} \\ell_i(\\mathbf{b})\n$$\n其中 $\\ell_i(\\mathbf{b})$ 是第 $i$ 个样本的损失，其校准后的 logits 为 $z_{ik} = L^{\\text{val}}_{ik} + b_k$。该目标函数关于 $b_k$ 的梯度是各个梯度的平均值：\n$$\n\\nabla_{b_k} \\mathcal{L}(\\mathbf{b}) = \\frac{1}{N_{\\text{val}}} \\sum_{i=1}^{N_{\\text{val}}} (p_{ik} - \\delta_{y_i k})\n$$\n其中 $p_{ik}$ 是使用校准后的 logits 对样本 $i$ 的类别 $k$ 的 softmax 概率，而 $y_i$ 是样本 $i$ 的真实标签。\n\n我们将使用一种基于梯度的优化算法，特别是 `scipy.optimize.minimize` 提供的 L-BFGS-B 算法，来找到最小化 $\\mathcal{L}(\\mathbf{b})$ 的最优偏置向量 $\\mathbf{b}^*$。对于此类光滑、无约束（或箱约束）的优化问题，这是一种稳健且高效的方法。偏置的初始猜测值将是 $\\mathbf{b} = \\mathbf{0}$。\n\n为保证数值稳定性，交叉熵损失和 softmax 的计算将采用 log-sum-exp 技巧。单个样本的交叉熵损失计算为 $\\ell_i = \\log\\left(\\sum_j \\exp(z_{ij})\\right) - z_{iy_i}$。这可以避免在处理较大或较小的 logit 值时出现浮点数上溢和下溢问题。\n\n在使用校准集 $(L^{\\text{val}}, \\mathbf{y}^{\\text{val}})$ 确定最优偏置向量 $\\mathbf{b}^*$ 后，我们在测试集 $(L^{\\text{test}}, \\mathbf{y}^{\\text{test}})$ 上评估其性能。评估指标是平均负对数似然 (NLL) 的变化量：\n$$\n\\Delta_{\\text{NLL}} = \\text{NLL}_{\\text{before}} - \\text{NLL}_{\\text{after}}\n$$\n其中：\n- $\\text{NLL}_{\\text{before}}$ 是使用原始 logits $L^{\\text{test}}$ 在测试集上的平均交叉熵。\n- $\\text{NLL}_{\\text{after}}$ 是使用校准后的 logits $L^{\\text{test}} + \\mathbf{1}(\\mathbf{b}^*)^\\top$ 在测试集上的平均交叉熵。\n\n$\\Delta_{\\text{NLL}}$ 的正值表示模型校准有所改善，即调整偏置后，预测的概率更加准确。最终的程序将为所提供的三个测试用例分别实现此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.special import softmax, logsumexp\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the bias calibration problem for the three given test cases.\n    \"\"\"\n    \n    # CASE 1\n    L_val_1 = np.array([\n        [2.0, 1.0, 0.0],\n        [1.5, -0.5, 0.0],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.5, -1.0],\n        [-0.5, 0.0, 0.5]\n    ])\n    y_val_1 = np.array([0, 0, 2, 0, 2])\n    L_test_1 = np.array([\n        [1.8, 1.2, 0.0],\n        [1.4, -0.3, 0.1],\n        [0.1, 0.9, 1.0],\n        [0.9, 0.4, -0.8],\n        [-0.6, 0.1, 0.6]\n    ])\n    y_test_1 = np.array([0, 0, 2, 0, 2])\n\n    # CASE 2\n    L_val_2 = np.array([\n        [3.0, 0.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-1.0, 0.0, 3.0],\n        [3.5, -0.5, 0.0]\n    ])\n    y_val_2 = np.array([0, 1, 2, 0])\n    L_test_2 = np.array([\n        [3.0, 0.0, -1.0],\n        [0.0, 3.0, -1.0],\n        [-1.0, 0.0, 3.0],\n        [3.5, -0.5, 0.0]\n    ])\n    y_test_2 = np.array([0, 1, 2, 0])\n\n    # CASE 3\n    L_val_3 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y_val_3 = np.array([0, 0, 0, 1, 1, 2])\n    L_test_3 = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0]\n    ])\n    y_test_3 = np.array([0, 0, 0, 1, 1, 2])\n\n    test_cases = [\n        (L_val_1, y_val_1, L_test_1, y_test_1),\n        (L_val_2, y_val_2, L_test_2, y_test_2),\n        (L_val_3, y_val_3, L_test_3, y_test_3)\n    ]\n\n    results = []\n\n    def avg_cross_entropy_loss(logits, labels):\n        \"\"\"\n        Computes the average cross-entropy loss using the log-sum-exp trick\n        for numerical stability.\n        \"\"\"\n        num_samples = logits.shape[0]\n        # Log-sum-exp over classes for each sample\n        lse = logsumexp(logits, axis=1)\n        # Get the logits for the true classes\n        true_class_logits = logits[np.arange(num_samples), labels]\n        # Negative log-likelihood for each sample\n        nll = lse - true_class_logits\n        return np.mean(nll)\n\n    def objective_function(b, L, y):\n        \"\"\"\n        The objective function to minimize: average cross-entropy on the\n        calibration set with the current bias vector b.\n        \"\"\"\n        calibrated_logits = L + b\n        return avg_cross_entropy_loss(calibrated_logits, y)\n\n    def gradient_function(b, L, y):\n        \"\"\"\n        The gradient of the objective function with respect to the bias vector b.\n        \"\"\"\n        num_samples, num_classes = L.shape\n        calibrated_logits = L + b\n        \n        # Calculate softmax probabilities\n        probs = softmax(calibrated_logits, axis=1)\n        \n        # Create one-hot encoded labels\n        y_one_hot = np.zeros((num_samples, num_classes))\n        y_one_hot[np.arange(num_samples), y] = 1.0\n        \n        # Gradient is the average of (probs - one_hot_labels) over samples\n        grad = np.mean(probs - y_one_hot, axis=0)\n        return grad\n\n    for L_val, y_val, L_test, y_test in test_cases:\n        # Number of classes\n        K = L_val.shape[1]\n        \n        # Initial guess for the bias vector\n        b_initial = np.zeros(K)\n        \n        # Optimize the biases using the validation set\n        opt_result = minimize(\n            fun=objective_function,\n            x0=b_initial,\n            args=(L_val, y_val),\n            method='L-BFGS-B',\n            jac=gradient_function\n        )\n        b_optimal = opt_result.x\n        \n        # Evaluate on the test set\n        # 1. NLL before calibration\n        nll_before = avg_cross_entropy_loss(L_test, y_test)\n        \n        # 2. NLL after calibration\n        calibrated_L_test = L_test + b_optimal\n        nll_after = avg_cross_entropy_loss(calibrated_L_test, y_test)\n        \n        # 3. Compute the improvement\n        improvement = nll_before - nll_after\n        \n        results.append(f\"{improvement:.6f}\")\n\n    # Print the final results in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3199738"}]}