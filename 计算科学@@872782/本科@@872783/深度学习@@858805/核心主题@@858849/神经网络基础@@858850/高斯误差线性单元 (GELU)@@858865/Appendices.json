{"hands_on_practices": [{"introduction": "为了深入理解激活函数在神经网络训练初期的行为，尤其是在输入信号较弱时，我们可以使用多项式对其进行局部近似。这个练习将引导你使用泰勒级数来揭示 GELU 在原点附近的局部特性 [@problem_id:3128551]。通过推导其多项式形式，你将清晰地看到 GELU 并非简单的线性函数，而是带有特定的曲率，这对于理解小信号下的梯度流动至关重要。", "problem": "一个单隐层神经网络在原点附近（输入值较小）使用高斯误差线性单元（GELU）激活函数。为了研究局部梯度流，请使用其在 $x=0$ 附近的泰勒级数来近似该激活函数，直到并包括 $x^{5}$ 次项。\n\n使用以下基础知识：\n\n- 标准正态概率密度函数（PDF）为 $\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$。\n- 标准正态累积分布函数（CDF）为 $\\Phi(x)=\\int_{-\\infty}^{x}\\phi(t)\\,\\mathrm{d}t$。\n- 高斯误差线性单元（GELU）激活函数定义为 $\\mathrm{GELU}(x)=x\\,\\Phi(x)$。\n- 当函数 $f$ 的所有阶导数都存在时，其麦克劳林级数为 $f(x)=\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(0)}{k!}x^{k}$。\n- 您可以从标准正态CDF在原点附近经过充分检验的局部近似开始，\n$$\n\\Phi(x)\\approx \\frac{1}{2}+\\frac{1}{\\sqrt{2\\pi}}x-\\frac{1}{6\\sqrt{2\\pi}}x^{3}+\\cdots,\n$$\n并使用泰勒级数的方法以及 $\\Phi$ 和 $\\phi$ 之间的关系来确定捕捉 $\\mathrm{GELU}(x)$ 中直到 $x^{5}$ 的项所需的任何附加系数。\n\n推导 $\\mathrm{GELU}(x)$ 在 $x=0$ 附近的泰勒展开式，直到并包括 $x^{5}$ 次项。请将您的最终答案表示为关于 $x$ 的单一封闭形式解析表达式，并使用精确系数。无需四舍五入。", "solution": "用户希望找到高斯误差线性单元（GELU）函数在原点 $x=0$ 附近的泰勒级数展开式，直到并包括 $x^5$ 次项。\n\n首先，我们验证问题陈述。\n\n### 第1步：提取已知条件\n-   标准正态概率密度函数（PDF）为 $\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$。\n-   标准正态累积分布函数（CDF）为 $\\Phi(x)=\\int_{-\\infty}^{x}\\phi(t)\\,\\mathrm{d}t$。\n-   高斯误差线性单元（GELU）激活函数定义为 $\\mathrm{GELU}(x)=x\\,\\Phi(x)$。\n-   函数 $f$ 的麦克劳林级数为 $f(x)=\\sum_{k=0}^{\\infty}\\frac{f^{(k)}(0)}{k!}x^{k}$。\n-   提供了一个CDF的近似式：$\\Phi(x)\\approx \\frac{1}{2}+\\frac{1}{\\sqrt{2\\pi}}x-\\frac{1}{6\\sqrt{2\\pi}}x^{3}+\\cdots$。\n-   任务是推导 $\\mathrm{GELU}(x)$ 在 $x=0$ 附近的泰勒展开式，直到并包括 $x^5$ 次项。\n\n### 第2步：使用提取的已知条件进行验证\n该问题在科学上和数学上都是合理的。$\\phi(x)$、$\\Phi(x)$ 和 $\\mathrm{GELU}(x)$ 的定义在统计学和深度学习中是标准的。寻找泰勒级数展开式的要求是一个适定的数学问题。函数 $\\mathrm{GELU}(x)$ 是无限可微的，这保证了其麦克劳林级数的存在。该问题是自洽且客观的。所提供的 $\\Phi(x)$ 近似式是正确的，但对于本任务来说是不完整的，问题陈述本身也承认了这一点，它指示求解者确定额外的系数，这是问题挑战的一部分，而不是一个缺陷。\n\n### 第3步：结论与行动\n问题有效。我们将继续进行求解。\n\n$\\mathrm{GELU}(x)$ 在 $x=0$ 附近的泰勒展开式由麦克劳林级数给出：\n$$ \\mathrm{GELU}(x) = \\sum_{k=0}^{\\infty} \\frac{\\mathrm{GELU}^{(k)}(0)}{k!} x^k $$\n我们需要找到直到 $x^5$ 的项，这需要计算 $\\mathrm{GELU}(x)$ 在 $x=0$ 处的前五阶导数。\n\n一种替代的、更直接的方法是首先找到 $\\Phi(x)$ 的泰勒级数，然后将其乘以 $x$。设 $\\Phi(x)$ 的泰勒级数为\n$$ \\Phi(x) = c_0 + c_1 x + c_2 x^2 + c_3 x^3 + c_4 x^4 + O(x^5) $$\n那么 $\\mathrm{GELU}(x)$ 的级数将是\n$$ \\mathrm{GELU}(x) = x \\Phi(x) = c_0 x + c_1 x^2 + c_2 x^3 + c_3 x^4 + c_4 x^5 + O(x^6) $$\n为了找到 $\\mathrm{GELU}(x)$ 直到5次的展开式，我们需要 $\\Phi(x)$ 直到4次的展开式。系数 $c_k$ 由 $c_k = \\frac{\\Phi^{(k)}(0)}{k!}$ 给出。\n\n我们使用微积分基本定理：$\\Phi'(x) = \\frac{d}{dx}\\int_{-\\infty}^{x}\\phi(t)\\,\\mathrm{d}t = \\phi(x)$。\n这意味着对于 $k \\ge 1$，有 $\\Phi^{(k)}(x) = \\phi^{(k-1)}(x)$。\n\n$\\Phi(x)$ 级数的系数为：\n$c_0 = \\Phi(0) = \\int_{-\\infty}^{0} \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{t^2}{2}) dt = \\frac{1}{2}$，这是由于标准正态分布的对称性。\n$c_1 = \\frac{\\Phi'(0)}{1!} = \\phi(0)$。\n$c_2 = \\frac{\\Phi''(0)}{2!} = \\frac{\\phi'(0)}{2}$。\n$c_3 = \\frac{\\Phi'''(0)}{3!} = \\frac{\\phi''(0)}{6}$。\n$c_4 = \\frac{\\Phi^{(4)}(0)}{4!} = \\frac{\\phi'''(0)}{24}$。\n\n现在我们计算 $\\phi(x)$ 在 $x=0$ 处的导数：\n$\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$。\n1.  $\\phi(0) = \\frac{1}{\\sqrt{2\\pi}}\\exp(0) = \\frac{1}{\\sqrt{2\\pi}}$。\n2.  $\\phi'(x) = \\frac{d}{dx}\\left[\\phi(x)\\right] = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})(-x) = -x\\phi(x)$。\n    在 $x=0$ 处，$\\phi'(0) = -0 \\cdot \\phi(0) = 0$。\n3.  $\\phi''(x) = \\frac{d}{dx}\\left[-x\\phi(x)\\right] = -1 \\cdot \\phi(x) - x \\cdot \\phi'(x) = -\\phi(x) - x(-x\\phi(x)) = (x^2-1)\\phi(x)$。\n    在 $x=0$ 处，$\\phi''(0) = (0^2-1)\\phi(0) = -\\phi(0) = -\\frac{1}{\\sqrt{2\\pi}}$。\n4.  $\\phi'''(x) = \\frac{d}{dx}\\left[(x^2-1)\\phi(x)\\right] = 2x\\phi(x) + (x^2-1)\\phi'(x) = 2x\\phi(x) - x(x^2-1)\\phi(x) = (2x - x^3 + x)\\phi(x) = (3x-x^3)\\phi(x)$。\n    在 $x=0$ 处，$\\phi'''(0) = (0-0)\\phi(0) = 0$。\n\n现在我们可以计算 $\\Phi(x)$ 的系数 $c_k$：\n$c_0 = \\frac{1}{2}$。\n$c_1 = \\phi(0) = \\frac{1}{\\sqrt{2\\pi}}$。\n$c_2 = \\frac{\\phi'(0)}{2} = \\frac{0}{2} = 0$。\n$c_3 = \\frac{\\phi''(0)}{6} = \\frac{-1/\\sqrt{2\\pi}}{6} = -\\frac{1}{6\\sqrt{2\\pi}}$。\n$c_4 = \\frac{\\phi'''(0)}{24} = \\frac{0}{24} = 0$。\n\n$\\Phi(x)$ 直到4次的泰勒展开式为：\n$$ \\Phi(x) \\approx \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}x + 0 \\cdot x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^3 + 0 \\cdot x^4 $$\n$$ \\Phi(x) \\approx \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}x - \\frac{1}{6\\sqrt{2\\pi}}x^3 $$\n这表明给定的近似式正确地识别了非零项，但省略了系数为零的项。\n\n为了找到 $\\mathrm{GELU}(x)$ 直到5次的泰勒展开式，我们将 $\\Phi(x)$ 直到4次的展开式乘以 $x$：\n$$ \\mathrm{GELU}(x) = x \\Phi(x) \\approx x \\left( \\frac{1}{2} + \\frac{1}{\\sqrt{2\\pi}}x - \\frac{1}{6\\sqrt{2\\pi}}x^3 \\right) $$\n$$ \\mathrm{GELU}(x) \\approx \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4 $$\n5次泰勒多项式是包含所有直到并包括 $x^{5}$ 次幂的项的多项式。$x^3$ 项的系数是 $c_2 = 0$，$x^5$ 项的系数是 $c_4 = 0$。因此，所要求的展开式为：\n$$ \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 + 0 \\cdot x^3 - \\frac{1}{6\\sqrt{2\\pi}}x^4 + 0 \\cdot x^5 $$\n在写最终表达式时，我们省略系数为零的项。\n\n$\\mathrm{GELU}(x)$ 在 $x=0$ 附近直到并包括 $x^5$ 项的最终泰勒展开式为：\n$$ \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4 $$", "answer": "$$\n\\boxed{\\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4}\n$$", "id": "3128551"}, {"introduction": "虽然 GELU 具有优秀的数学特性，但其精确计算（依赖于误差函数）可能相对耗时，这催生了如 HardSwish 等更高效的近似函数。本练习将对 GELU 和 HardSwish 进行直接比较 [@problem_id:3128657]。你不仅需要验证它们在原点处是否具有相似的斜率，还将通过一个量化的“平滑度惩罚”来评估它们的曲率和拐点差异，从而深入理解在数学纯粹性与计算性能之间的工程权衡。", "problem": "考虑深度神经网络中使用的两种激活函数。第一种是高斯误差线性单元 (GELU)，其操作定义如下：对于输入 $x \\in \\mathbb{R}$，抽取 $Z \\sim \\mathcal{N}(0,1)$，输出为期望门控输入 $\\mathbb{E}_{Z}\\big[x \\cdot \\mathbf{1}\\{Z \\le x\\}\\big]$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。第二种是硬饱和 swish，$\\operatorname{HardSwish}(x)=x \\cdot \\max\\big(0,\\min\\big(1,\\frac{x+3}{6}\\big)\\big)$，当 $x \\in [-3,3]$ 时等于 $x \\cdot \\frac{x+3}{6}$，当 $x \\le -3$ 时等于 $0$，当 $x \\ge 3$ 时等于 $x$。在优化中，通常会对比两个局部性质：$x=0$ 处的斜率和一个量化曲率和扭结的平滑度惩罚。\n\n在区间 $[-3,3]$ 上采用以下惩罚泛函：\n- 仅曲率惩罚 $P_2(f) \\equiv \\int_{-3}^{3} \\big(f''(x)\\big)^2 \\, dx$。\n- 曲率加扭结惩罚 $P_{2+\\kappa}(f) \\equiv P_2(f) + \\sum_{x_i \\in \\{-3,3\\}} \\big(\\Delta f'(x_i)\\big)^2$，其中 $\\Delta f'(x_i) \\equiv \\lim_{h \\downarrow 0} \\big(f'(x_i+h) - f'(x_i-h)\\big)$ 是 $x_i$ 处一阶导数的跳跃。\n\n仅使用导数的基本定义、标准正态概率密度函数 (PDF) 和累积分布函数 (CDF) 以及上面所述的 $\\operatorname{HardSwish}$ 的分段结构，确定下列哪些陈述是正确的：\n\nA. 在 $x=0$ 处的斜率匹配：$f'_{\\mathrm{GELU}}(0) = f'_{\\mathrm{HS}}(0)$。\n\nB. 在仅曲率惩罚 $P_2$ 下，$\\operatorname{HardSwish}$ 在 $[-3,3]$ 上的惩罚小于 $\\mathrm{GELU}$。\n\nC. 在曲率加扭结惩罚 $P_{2+\\kappa}$ 下，$\\operatorname{HardSwish}$ 的惩罚大于 $\\mathrm{GELU}$。\n\nD. 在 $x=0$ 处的斜率匹配意味着在 $x=0$ 处的二阶导数相等。", "solution": "首先验证问题陈述，以确保其具有科学依据、是适定的且客观的。\n\n### 步骤 1：提取已知条件\n-   **高斯误差线性单元 (GELU)**：对于输入 $x \\in \\mathbb{R}$，抽取 $Z \\sim \\mathcal{N}(0,1)$。输出为 $\\mathbb{E}_{Z}\\big[x \\cdot \\mathbf{1}\\{Z \\le x\\}\\big]$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n-   **HardSwish**：$\\operatorname{HardSwish}(x)=x \\cdot \\max\\big(0,\\min\\big(1,\\frac{x+3}{6}\\big)\\big)$。\n-   **HardSwish 的分段结构**：\n    -   当 $x \\in [-3,3]$ 时，等于 $x \\cdot \\frac{x+3}{6}$。\n    -   当 $x \\le -3$ 时，等于 $0$。\n    -   当 $x \\ge 3$ 时，等于 $x$。\n-   **仅曲率惩罚**：$P_2(f) \\equiv \\int_{-3}^{3} \\big(f''(x)\\big)^2 \\, dx$。\n-   **曲率加扭结惩罚**：$P_{2+\\kappa}(f) \\equiv P_2(f) + \\sum_{x_i \\in \\{-3,3\\}} \\big(\\Delta f'(x_i)\\big)^2$。\n-   **一阶导数的跳跃**：$\\Delta f'(x_i) \\equiv \\lim_{h \\downarrow 0} \\big(f'(x_i+h) - f'(x_i-h)\\big)$。\n-   **假设**：仅使用导数的基本定义、标准正态 PDF ($\\phi$) 和 CDF ($\\Phi$)，以及给定的 HardSwish 分段结构。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学依据**：该问题涉及深度学习中使用的两个真实的激活函数，GELU 和 HardSwish。它们的定义是标准的。惩罚泛函是用于量化函数平滑度的明确定义的数学对象，这个概念植根于泛函分析。该问题在科学上和数学上都是合理的。\n2.  **适定性**：函数和惩罚被明确定义。问题是定量的比较，可以从已知条件中推导出唯一答案。该问题是适定的。\n3.  **客观性**：语言精确且数学化，没有主观或模糊的术语。\n4.  **一致性**：HardSwish 的分段定义与其闭式表达式一致。对于 $x \\le -3$，$\\frac{x+3}{6} \\le 0$，所以 $\\max(0, \\min(1, \\frac{x+3}{6})) = 0$，得到 $x \\cdot 0 = 0$。对于 $x \\ge 3$，$\\frac{x+3}{6} \\ge 1$，所以 $\\max(0, \\min(1, \\frac{x+3}{6})) = 1$，得到 $x \\cdot 1 = x$。对于 $x \\in [-3,3]$，$0 \\le \\frac{x+3}{6} \\le 1$，所以 $\\max(0, \\min(1, \\frac{x+3}{6})) = \\frac{x+3}{6}$，得到 $x \\cdot \\frac{x+3}{6}$。定义是一致的。\n\n### 步骤 3：结论与行动\n问题陈述有效。将推导完整解答。\n\n### 解答推导\n\n令 $f_{\\mathrm{GELU}}(x)$ 和 $f_{\\mathrm{HS}}(x)$ 分别表示 GELU 和 HardSwish 函数。\n\n**函数分析：GELU**\nGELU 的定义是 $f_{\\mathrm{GELU}}(x) = \\mathbb{E}_{Z}\\big[x \\cdot \\mathbf{1}\\{Z \\le x\\}\\big]$。由于 $x$ 相对于 $Z$ 的期望是常数，我们有：\n$$f_{\\mathrm{GELU}}(x) = x \\cdot \\mathbb{E}_{Z}\\big[\\mathbf{1}\\{Z \\le x\\}\\big] = x \\cdot P(Z \\le x)$$\n令 $\\Phi(x)$ 为标准正态分布 $\\mathcal{N}(0,1)$ 的累积分布函数 (CDF)，$\\phi(x)$ 为其概率密度函数 (PDF)。\n$$f_{\\mathrm{GELU}}(x) = x\\Phi(x)$$\n使用乘积法则，一阶导数为：\n$$f'_{\\mathrm{GELU}}(x) = \\frac{d}{dx}(x\\Phi(x)) = 1 \\cdot \\Phi(x) + x \\cdot \\Phi'(x) = \\Phi(x) + x\\phi(x)$$\n二阶导数为：\n$$f''_{\\mathrm{GELU}}(x) = \\frac{d}{dx}(\\Phi(x) + x\\phi(x)) = \\phi(x) + (1 \\cdot \\phi(x) + x\\phi'(x)) = 2\\phi(x) + x\\phi'(x)$$\nPDF 为 $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}$，所以其导数为 $\\phi'(x) = -x \\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2} = -x\\phi(x)$。\n将此代入二阶导数表达式：\n$$f''_{\\mathrm{GELU}}(x) = 2\\phi(x) - x^2\\phi(x) = (2-x^2)\\phi(x)$$\n\n**函数分析：HardSwish**\n在区间 $x \\in [-3,3]$ 上，函数为：\n$$f_{\\mathrm{HS}}(x) = x \\cdot \\frac{x+3}{6} = \\frac{1}{6}(x^2 + 3x)$$\n在 $x \\in (-3,3)$ 上的一阶导数为：\n$$f'_{\\mathrm{HS}}(x) = \\frac{1}{6}(2x+3) = \\frac{1}{3}x + \\frac{1}{2}$$\n在 $x \\in (-3,3)$ 上的二阶导数为：\n$$f''_{\\mathrm{HS}}(x) = \\frac{1}{3}$$\n对于 $x  -3$，$f_{\\mathrm{HS}}(x) = 0$，所以 $f'_{\\mathrm{HS}}(x) = 0$。\n对于 $x > 3$，$f_{\\mathrm{HS}}(x) = x$，所以 $f'_{\\mathrm{HS}}(x) = 1$。\n\n### 逐项分析\n\n**A. 在 $x=0$ 处的斜率匹配：$f'_{\\mathrm{GELU}}(0) = f'_{\\mathrm{HS}}(0)$。**\n\n对于 GELU，在 $x=0$ 处：\n$$f'_{\\mathrm{GELU}}(0) = \\Phi(0) + 0 \\cdot \\phi(0) = \\Phi(0)$$\n标准正态分布的中位数为 $0$，所以 $\\Phi(0) = 1/2$。\n$$f'_{\\mathrm{GELU}}(0) = \\frac{1}{2}$$\n对于 HardSwish，在 $x=0$ 处（该点在 $[-3,3]$ 区间内）：\n$$f'_{\\mathrm{HS}}(0) = \\frac{1}{3}(0) + \\frac{1}{2} = \\frac{1}{2}$$\n在 $x=0$ 处的斜率确实相等。\n- **A 的结论**：**正确**\n\n**B. 在仅曲率惩罚 $P_2$ 下，$\\operatorname{HardSwish}$ 在 $[-3,3]$ 上的惩罚小于 $\\mathrm{GELU}$。**\n\n首先，计算 HardSwish 的惩罚：\n$$P_2(f_{\\mathrm{HS}}) = \\int_{-3}^{3} \\big(f''_{\\mathrm{HS}}(x)\\big)^2 dx = \\int_{-3}^{3} \\left(\\frac{1}{3}\\right)^2 dx = \\int_{-3}^{3} \\frac{1}{9} dx = \\frac{1}{9}[x]_{-3}^{3} = \\frac{1}{9}(3 - (-3)) = \\frac{6}{9} = \\frac{2}{3}$$\n接下来，建立 GELU 的惩罚：\n$$P_2(f_{\\mathrm{GELU}}) = \\int_{-3}^{3} \\big((2-x^2)\\phi(x)\\big)^2 dx = \\int_{-3}^{3} (2-x^2)^2 \\left(\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\right)^2 dx$$\n$$P_2(f_{\\mathrm{GELU}}) = \\frac{1}{2\\pi} \\int_{-3}^{3} (4 - 4x^2 + x^4) e^{-x^2} dx$$\n区间 $[-3,3]$ 覆盖了函数 $e^{-x^2}$ 的绝大部分质量。我们可以通过将积分限扩展到 $\\pm\\infty$ 来近似该积分。\n使用标准高斯积分 $\\int_{-\\infty}^{\\infty} e^{-ax^2} dx = \\sqrt{\\frac{\\pi}{a}}$，$\\int_{-\\infty}^{\\infty} x^2 e^{-ax^2} dx = \\frac{1}{2a}\\sqrt{\\frac{\\pi}{a}}$，以及 $\\int_{-\\infty}^{\\infty} x^4 e^{-ax^2} dx = \\frac{3}{4a^2}\\sqrt{\\frac{\\pi}{a}}$，其中 $a=1$：\n$$\\int_{-\\infty}^{\\infty} (4 - 4x^2 + x^4) e^{-x^2} dx = 4\\sqrt{\\pi} - 4\\left(\\frac{1}{2}\\sqrt{\\pi}\\right) + \\frac{3}{4}\\sqrt{\\pi} = \\left(4 - 2 + \\frac{3}{4}\\right)\\sqrt{\\pi} = \\frac{11}{4}\\sqrt{\\pi}$$\n因此，惩罚近似为：\n$$P_2(f_{\\mathrm{GELU}}) \\approx \\frac{1}{2\\pi} \\left(\\frac{11\\sqrt{\\pi}}{4}\\right) = \\frac{11}{8\\sqrt{\\pi}} \\approx \\frac{11}{8 \\times 1.772} \\approx \\frac{11}{14.176} \\approx 0.776$$\n比较两个惩罚：\n$$P_2(f_{\\mathrm{HS}}) = \\frac{2}{3} \\approx 0.667$$\n$$P_2(f_{\\mathrm{GELU}}) \\approx 0.776$$\n由于 $0.667  0.776$，HardSwish 在此区间上的仅曲率惩罚更小。\n- **B 的结论**：**正确**\n\n**C. 在曲率加扭结惩罚 $P_{2+\\kappa}$ 下，$\\operatorname{HardSwish}$ 的惩罚大于 $\\mathrm{GELU}$。**\n\n惩罚为 $P_{2+\\kappa}(f) = P_2(f) + (\\Delta f'(-3))^2 + (\\Delta f'(3))^2$。\n\n对于 GELU，函数 $f_{\\mathrm{GELU}}(x)$ 及其所有阶导数在所有 $x \\in \\mathbb{R}$ 上都是连续的。因此，导数跳跃为零：$\\Delta f'_{\\mathrm{GELU}}(-3) = 0$ 和 $\\Delta f'_{\\mathrm{GELU}}(3) = 0$。\n$$P_{2+\\kappa}(f_{\\mathrm{GELU}}) = P_2(f_{\\mathrm{GELU}}) \\approx 0.776$$\n\n对于 HardSwish，我们必须计算在 $x_i = \\{-3, 3\\}$ 处的导数跳跃。\n在 $x=-3$ 处：\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(-3-h) = 0 $$\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(-3+h) = \\frac{1}{3}(-3) + \\frac{1}{2} = -1 + \\frac{1}{2} = -\\frac{1}{2} $$\n$$ \\Delta f'_{\\mathrm{HS}}(-3) = \\lim_{h \\downarrow 0} (f'_{\\mathrm{HS}}(-3+h) - f'_{\\mathrm{HS}}(-3-h)) = -\\frac{1}{2} - 0 = -\\frac{1}{2} $$\n在 $x=3$ 处：\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(3-h) = \\frac{1}{3}(3) + \\frac{1}{2} = 1 + \\frac{1}{2} = \\frac{3}{2} $$\n$$ \\lim_{h \\downarrow 0} f'_{\\mathrm{HS}}(3+h) = 1 $$\n$$ \\Delta f'_{\\mathrm{HS}}(3) = \\lim_{h \\downarrow 0} (f'_{\\mathrm{HS}}(3+h) - f'_{\\mathrm{HS}}(3-h)) = 1 - \\frac{3}{2} = -\\frac{1}{2} $$\nHardSwish 的总扭结惩罚为：\n$$ \\sum_{x_i \\in \\{-3,3\\}} \\big(\\Delta f'(x_i)\\big)^2 = \\left(-\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2} $$\nHardSwish 的总惩罚为：\n$$ P_{2+\\kappa}(f_{\\mathrm{HS}}) = P_2(f_{\\mathrm{HS}}) + \\frac{1}{2} = \\frac{2}{3} + \\frac{1}{2} = \\frac{4}{6} + \\frac{3}{6} = \\frac{7}{6} $$\n比较总惩罚：\n$$P_{2+\\kappa}(f_{\\mathrm{HS}}) = \\frac{7}{6} \\approx 1.167$$\n$$P_{2+\\kappa}(f_{\\mathrm{GELU}}) \\approx 0.776$$\n由于 $1.167 > 0.776$，HardSwish 有更大的曲率加扭结惩罚。\n- **C 的结论**：**正确**\n\n**D. 在 $x=0$ 处的斜率匹配意味着在 $x=0$ 处的二阶导数相等。**\n\n这是一个关于函数的普遍性陈述。设有两个函数 $g(x)$ 和 $h(x)$。该陈述声称，如果 $g'(0) = h'(0)$，那么必然有 $g''(0) = h''(0)$。这在逻辑上是错误的。一个函数在某点的导数值并不能决定其在该点的二阶导数值。\n\n作为一个反例，考虑 $g(x) = 2x$ 和 $h(x)=2x+x^2$。\n$g'(x)=2$，所以 $g'(0)=2$。$h'(x)=2+2x$，所以 $h'(0)=2$。在 $x=0$ 处的斜率匹配。\n然而，$g''(x)=0$ 且 $h''(x)=2$。因此 $g''(0)=0 \\neq h''(0)=2$。\n\n对于本问题中的具体函数，我们已经证明了 $f'_{\\mathrm{GELU}}(0) = f'_{\\mathrm{HS}}(0) = 1/2$。让我们比较它们在 $x=0$ 处的二阶导数。\n$$ f''_{\\mathrm{HS}}(0) = \\frac{1}{3} $$\n$$ f''_{\\mathrm{GELU}}(0) = (2-0^2)\\phi(0) = 2\\phi(0) = 2 \\cdot \\frac{1}{\\sqrt{2\\pi}} = \\frac{\\sqrt{2}}{\\sqrt{\\pi}} \\approx 0.798 $$\n显然，$1/3 \\neq \\sqrt{2}/\\sqrt{\\pi}$。该陈述对于这个具体情况也是错误的。\n- **D 的结论**：**不正确**", "answer": "$$\\boxed{ABC}$$", "id": "3128657"}, {"introduction": "在了解了近似函数的概念后，我们可以更进一步，亲手构建一个高保真的近似。本练习将指导你通过数值优化的方法，将一个灵活的参数化函数（修正的 $\\tanh$ 函数）拟合到 GELU 曲线上 [@problem_id:3128590]。这个实践任务不仅模拟了著名模型（如 BERT）中近似函数的开发过程，还将让你通过分析拟合参数，直观地理解它们在匹配目标函数形状时所扮演的角色。", "problem": "令 $x \\in \\mathbb{R}$ 表示深度神经网络前馈层中的预激活值。高斯误差线性单元 (GELU) 激活函数由经过充分检验的公式 $ \\mathrm{GELU}(x) = \\dfrac{1}{2} x \\left( 1 + \\mathrm{erf}\\!\\left(\\dfrac{x}{\\sqrt{2}}\\right) \\right) $ 定义，其中 $ \\mathrm{erf}(\\cdot) $ 是误差函数。考虑参数化近似族 $ f_{\\alpha,\\beta}(x) = \\dfrac{1}{2} x \\left( 1 + \\tanh\\!\\left( \\alpha \\left( x + \\beta x^3 \\right) \\right) \\right) $，其参数为 $ \\alpha \\in \\mathbb{R} $ 和 $ \\beta \\in \\mathbb{R} $。目标是通过最小二乘法拟合 $ \\alpha $ 和 $ \\beta $，以在指定权重函数 $ w(x) $ 和域 $ D \\subset \\mathbb{R} $ 下，最小化 $ f_{\\alpha,\\beta} $ 和 $ \\mathrm{GELU} $ 之间的平方 $ L^2 $ 差异。\n\n从上述核心定义以及关于数值积分和最小二乘拟合的标准事实出发，构建最小化平方 $ L^2 $ 误差的公式\n$$\n\\left\\| f_{\\alpha,\\beta} - \\mathrm{GELU} \\right\\|_{L^2(w;D)}^2\n=\n\\int_{D} \\left( f_{\\alpha,\\beta}(x) - \\mathrm{GELU}(x) \\right)^2 w(x) \\, dx,\n$$\n并通过在均匀间隔的网格上离散化积分来对其进行数值求解。在 $ D = [a,b] $ 上使用一个包含 $ N $ 个点的均匀网格，步长为 $ \\Delta = \\dfrac{b-a}{N-1} $，并通过黎曼和来近似该积分\n$$\nJ(\\alpha,\\beta) \\approx \\sum_{i=1}^{N} \\left( f_{\\alpha,\\beta}(x_i) - \\mathrm{GELU}(x_i) \\right)^2 w(x_i) \\, \\Delta,\n$$\n其中 $ x_i $ 是网格点。实现一个数值优化器，搜索 $ (\\alpha,\\beta) $ 以在指定的简单边界条件下最小化 $ J(\\alpha,\\beta) $。对于有约束的情况，固定一个参数，仅对剩余的自由参数进行优化。\n\n通过使用小 $x$ 展开，将得到的最优参数 $ \\alpha $ 和 $ \\beta $ 与 $ x = 0 $ 附近的局部行为联系起来进行解释，并说明 $ \\alpha $如何控制局部斜率缩放，以及 $ \\beta $如何提供三次修正以更好地与高斯形状对齐。\n\n您的程序必须评估以下测试套件并返回数值结果：\n\n- 测试用例 $ 1 $ (正常路径)：$ D = [-10,10] $, $ N = 10001 $, $ w(x) = \\dfrac{1}{\\sqrt{2\\pi}} e^{-x^2/2} $，边界约束为 $ \\alpha \\in [0,2] $, $ \\beta \\in [0,0.2] $。优化 $ \\alpha $ 和 $ \\beta $。\n- 测试用例 $ 2 $ (边界条件)：$ D = [-10,10] $, $ N = 10001 $, $ w(x) = \\dfrac{1}{\\sqrt{2\\pi}} e^{-x^2/2} $，边界约束为 $ \\alpha \\in [0,2] $ 且固定 $ \\beta = 0 $。仅优化 $ \\alpha $。\n- 测试用例 $ 3 $ (边缘域)：$ D = [-1,1] $, $ N = 5001 $, $ w(x) = 1 $ (均匀)，边界约束为 $ \\alpha \\in [0,2] $, $ \\beta \\in [0,0.2] $。优化 $ \\alpha $ 和 $ \\beta $。\n\n对于每个测试用例，计算并返回包含三个浮点数的元组 $ [\\alpha^\\star,\\beta^\\star,J^\\star] $，其中 $ \\alpha^\\star $ 和 $ \\beta^\\star $ 是根据指定约束最小化 $ J(\\alpha,\\beta) $ 的拟合参数，而 $ J^\\star $ 是平方 $ L^2 $ 误差的最小化离散积分值。如果一个参数是固定的，则报告其固定值作为 $ \\beta^\\star $（对于第二种情况）。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个内部结果格式化为包含三个浮点数的列表，保留六位小数，例如：\n$$\n\\text{[}[\\alpha_1,\\beta_1,J_1],[\\alpha_2,\\beta_2,J_2],[\\alpha_3,\\beta_3,J_3]\\text{]}.\n$$\n此问题不涉及物理单位、角度单位或百分比；所有量都是无单位的实数。程序必须是自包含的，并且不得要求任何用户输入。", "solution": "用户提供了一个定义明确的数值优化问题。该问题在数值分析和深度学习方面有科学依据，是适定的、客观且完整的。所有必要的函数、参数和约束都已明确提供。因此，该问题被视为有效，并将提供完整的解决方案。\n\n核心任务是为函数族 $f_{\\alpha,\\beta}(x)$ 找到最优参数 $(\\alpha, \\beta)$，使其最佳逼近高斯误差线性单元 ($\\mathrm{GELU}$) 函数。 “最佳”的衡量标准是在域 $D$ 上最小化加权平方 $L^2$ 误差。\n\n$\\mathrm{GELU}$ 函数定义为：\n$$\n\\mathrm{GELU}(x) = \\frac{1}{2} x \\left( 1 + \\mathrm{erf}\\left(\\frac{x}{\\sqrt{2}}\\right) \\right)\n$$\n其中 $\\mathrm{erf}(\\cdot)$ 是高斯误差函数。\n\n参数化近似由下式给出：\n$$\nf_{\\alpha,\\beta}(x) = \\frac{1}{2} x \\left( 1 + \\tanh\\left( \\alpha \\left( x + \\beta x^3 \\right) \\right) \\right)\n$$\n\n目标是最小化由以下积分定义的平方 $L^2$ 误差：\n$$\nE(\\alpha, \\beta) = \\int_{D} \\left( f_{\\alpha,\\beta}(x) - \\mathrm{GELU}(x) \\right)^2 w(x) \\, dx\n$$\n其中 $w(x)$ 是给定的权重函数。\n\n为了数值求解此问题，我们首先将积分离散化。域 $D = [a,b]$ 被划分为一个包含 $N$ 个点的均匀网格，$x_i = a + i \\cdot \\Delta$，$i = 0, 1, \\dots, N-1$，步长为 $\\Delta = \\frac{b-a}{N-1}$。然后通过黎曼和来近似该积分，得到离散目标函数 $J(\\alpha, \\beta)$：\n$$\nJ(\\alpha, \\beta) = \\sum_{i=0}^{N-1} \\left( f_{\\alpha,\\beta}(x_i) - \\mathrm{GELU}(x_i) \\right)^2 w(x_i) \\Delta\n$$\n请注意，问题中说明求和是从 $i=1$ 到 $N$，这是一个常见的约定，但对于一个在 $N$ 个点上使用基于 0 的索引的实现来说，从 $i=0$ 到 $N-1$ 的求和是等价的。\n\n此函数 $J(\\alpha, \\beta)$ 是两个参数 $\\alpha$ 和 $ \\beta$ 的标量函数。问题简化为在给定的边界约束下寻找 $(\\alpha^\\star, \\beta^\\star) = \\arg\\min_{\\alpha, \\beta} J(\\alpha, \\beta)$。这是一个标准的非线性约束优化问题，可以使用数值算法来解决。我们将采用 L-BFGS-B 算法，这是一种高效的拟牛顿法，能够处理简单的箱形约束，正如 SciPy 库中实现的那样。\n\n通过使用泰勒级数展开分析函数在 $x=0$ 附近的行为，可以更深入地理解 $\\alpha$ 和 $\\beta$ 的作用。\n\n$\\mathrm{GELU}(x)$ 在 $x=0$ 附近的泰勒级数为：\n$$\n\\mathrm{GELU}(x) = \\frac{1}{2}x + \\frac{1}{\\sqrt{2\\pi}}x^2 - \\frac{1}{6\\sqrt{2\\pi}}x^4 + O(x^6)\n$$\n\n近似函数 $f_{\\alpha,\\beta}(x)$ 在 $x=0$ 附近的泰勒级数可以通过展开 $\\tanh$ 函数得到。使用 $\\tanh(u) = u - u^3/3 + O(u^5)$，其中 $u = \\alpha(x + \\beta x^3)$：\n$$\n\\begin{align*}\nf_{\\alpha,\\beta}(x) = \\frac{1}{2}x \\left( 1 + \\tanh(\\alpha x + \\alpha \\beta x^3) \\right) \\\\\n= \\frac{1}{2}x \\left( 1 + \\left[ (\\alpha x + \\alpha \\beta x^3) - \\frac{1}{3}(\\alpha x + \\alpha \\beta x^3)^3 + \\dots \\right] \\right) \\\\\n= \\frac{1}{2}x \\left( 1 + \\alpha x + \\alpha \\beta x^3 - \\frac{\\alpha^3 x^3}{3} + O(x^5) \\right) \\\\\n= \\frac{1}{2}x + \\frac{\\alpha}{2}x^2 + \\left( \\frac{\\alpha \\beta}{2} - \\frac{\\alpha^3}{6} \\right)x^4 + O(x^6)\n\\end{align*}\n$$\n\n通过匹配两个级数的系数，我们可以确定参数的理论最优值：\n1.  线性项 $\\frac{1}{2}x$ 完全匹配，这是一个理想的属性。\n2.  匹配 $x^2$ 项：$\\frac{\\alpha}{2} = \\frac{1}{\\sqrt{2\\pi}} \\implies \\alpha = \\sqrt{\\frac{2}{\\pi}}$。这表明 $\\alpha$ 主要控制近似函数在原点附近的二次行为。其理论值为 $\\alpha \\approx 0.79788$。\n3.  匹配 $x^4$ 项：$\\frac{\\alpha \\beta}{2} - \\frac{\\alpha^3}{6} = -\\frac{1}{6\\sqrt{2\\pi}}$。代入 $\\alpha$ 的值，我们可以解出 $\\beta$：\n    $$\n    \\beta = \\frac{2}{\\alpha} \\left( \\frac{\\alpha^3}{6} - \\frac{1}{6\\sqrt{2\\pi}} \\right) = \\frac{\\alpha^2}{3} - \\frac{1}{3\\alpha\\sqrt{2\\pi}} = \\frac{1}{3}\\left(\\frac{2}{\\pi}\\right) - \\frac{1}{3\\sqrt{2/\\pi}\\sqrt{2\\pi}} = \\frac{2}{3\\pi} - \\frac{1}{6}\n    $$\n    这揭示了 $\\beta$ 为 $\\tanh$ 的参数提供了三次修正，这转化为最终函数的四次修正 ($x^4$)，从而能够更精细地拟合 GELU 的形状。其理论值为 $\\beta \\approx 0.04551$。\n\n数值优化预期会产生接近这些理论值的参数，特别是在测试用例 1 中，因为高斯权重函数 $w(x)$ 强调了 $x=0$ 附近的区域，而泰勒级数近似在该区域最为准确。\n\n对于每个测试用例，步骤如下：\n1.  定义点网格 $x_i$ 和积分步长 $\\Delta$。\n2.  在网格上预先计算 $\\mathrm{GELU}(x_i)$ 的值和权重 $w(x_i)$。\n3.  定义一个目标函数，该函数接受一个参数向量（例如 $(\\alpha, \\beta)$）并返回 $J(\\alpha, \\beta)$ 的值。\n4.  使用 `scipy.optimize.minimize` 和 'L-BFGS-B' 方法，提供目标函数、参数的初始猜测值以及指定的边界。\n5.  对于测试用例 2，其中 $\\beta$ 是固定的，优化只针对单个变量 $\\alpha$ 进行。\n6.  收集并格式化得到的最优参数 $(\\alpha^\\star, \\beta^\\star)$ 和最小化的目标函数值 $J^\\star$。", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves for the optimal parameters (alpha, beta) for an approximation\n    of the GELU function by minimizing the discretized weighted L^2 error.\n    \"\"\"\n\n    def gelu(x):\n        \"\"\"Gaussian Error Linear Unit (GELU) activation function.\"\"\"\n        return 0.5 * x * (1.0 + erf(x / np.sqrt(2.0)))\n\n    def f_approx(x, alpha, beta):\n        \"\"\"Parametric approximation of GELU.\"\"\"\n        arg = alpha * (x + beta * x**3)\n        return 0.5 * x * (1.0 + np.tanh(arg))\n\n    test_cases = [\n        {\n            \"D\": (-10.0, 10.0), \"N\": 10001,\n            \"w_func\": lambda x: (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-x**2 / 2.0),\n            \"bounds\": [(0.0, 2.0), (0.0, 0.2)],\n            \"fixed_params\": {},\n            \"initial_guess\": [0.8, 0.05]\n        },\n        {\n            \"D\": (-10.0, 10.0), \"N\": 10001,\n            \"w_func\": lambda x: (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-x**2 / 2.0),\n            \"bounds\": [(0.0, 2.0)],\n            \"fixed_params\": {\"beta\": 0.0},\n            \"initial_guess\": [0.8]\n        },\n        {\n            \"D\": (-1.0, 1.0), \"N\": 5001,\n            \"w_func\": lambda x: 1.0,\n            \"bounds\": [(0.0, 2.0), (0.0, 0.2)],\n            \"fixed_params\": {},\n            \"initial_guess\": [0.8, 0.05]\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a, b = case[\"D\"]\n        N = case[\"N\"]\n        w_func = case[\"w_func\"]\n        bounds = case[\"bounds\"]\n        fixed_params = case[\"fixed_params\"]\n        initial_guess = case[\"initial_guess\"]\n\n        x_grid = np.linspace(a, b, N)\n        delta = (b - a) / (N - 1)\n\n        gelu_values = gelu(x_grid)\n        weights = w_func(x_grid)\n\n        def objective_function(params):\n            if \"beta\" in fixed_params:\n                alpha = params[0]\n                beta = fixed_params[\"beta\"]\n            else:\n                alpha, beta = params\n\n            f_values = f_approx(x_grid, alpha, beta)\n            squared_errors = (f_values - gelu_values)**2\n            weighted_errors = squared_errors * weights\n            integral = np.sum(weighted_errors) * delta\n            return integral\n\n        opt_result = minimize(\n            objective_function,\n            x0=initial_guess,\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n\n        J_star = opt_result.fun\n        if \"beta\" in fixed_params:\n            alpha_star = opt_result.x[0]\n            beta_star = fixed_params[\"beta\"]\n        else:\n            alpha_star, beta_star = opt_result.x\n\n        results.append([alpha_star, beta_star, J_star])\n\n    # Format the output as specified\n    formatted_results = [f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3128590"}]}