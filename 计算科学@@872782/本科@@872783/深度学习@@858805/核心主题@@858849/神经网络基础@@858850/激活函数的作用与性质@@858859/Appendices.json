{"hands_on_practices": [{"introduction": "神经网络的强大威力源于其非线性建模能力，而激活函数正是这种能力的核心。本练习将通过一个精心设计的思想实验，为您提供一个关于此关键概念的直观且深刻的动手体验。我们将构建一个特殊的场景，其中广泛使用的ReLU激活函数因其分段线性的特性而退化为线性模型，从而无法学习一个简单的非线性目标函数。通过将其与平滑的非线性激活函数（如双曲正切函数 $g_{\\tanh}(z)=\\tanh(z)$）的性能进行对比 [@problem_id:3171992]，您将清晰地看到，激活函数的有效非线性是决定模型表达能力上限的关键。", "problem": "您将研究激活函数的选择如何塑造假设类，从而如何影响高曲率目标函数的近似误差。在一维空间中进行研究。考虑修正线性单元（ReLU）激活函数 $g_{\\mathrm{ReLU}}(z)=\\max(0,z)$ 和双曲正切激活函数 $g_{\\tanh}(z)=\\tanh(z)$。设目标函数为 $f^\\ast(x)=\\sin(\\omega x)$，其中角度以弧度为单位。您将构建一个数据集和两个宽度均为 $m$ 的单隐藏层模型族，然后比较它们通过最小二乘法拟合时的经验均方误差（MSE）。\n\n使用的基本定义：\n- 一个具有固定隐藏参数 $\\{a_i,b_i\\}_{i=1}^m$ 和激活函数 $g$ 的单隐藏层模型形式为 $h(x)=c+\\sum_{i=1}^m w_i\\,g(a_i x+b_i)$，其中 $c$ 和 $\\{w_i\\}$ 是可训练的输出层参数。对于一个数据集 $\\{(x_j,y_j)\\}_{j=1}^N$，经验均方误差为 $\\frac{1}{N}\\sum_{j=1}^N\\left(h(x_j)-y_j\\right)^2$。\n- 对输出层进行最小二乘拟合意味着选择 $(c,\\{w_i\\})$ 来最小化经验均方误差，这等价于对由 $g(a_i x_j+b_i)$ 构成的、并增广了一个偏置特征的特征矩阵求解一个线性最小二乘问题。\n\n按如下方式构建数据集和模型。\n1) 数据集：设 $N=1001$，$x_j$ 为在 $[-1,1]$ 区间内的 $N$ 个等距点。设 $y_j=f^\\ast(x_j)=\\sin(\\omega x_j)$，其中 $\\omega$ 根据每个测试用例指定，并以弧度解释。\n2) ReLU 模型族 $H_{\\mathrm{ReLU}}$：固定宽度 $m$。选择斜率 $a_i$ 为在 $[-\\alpha,\\alpha]$ 区间内的 $m$ 个等距值，其中 $\\alpha=0.5$。为所有隐藏单元选择一个共享偏置 $b_i^{\\mathrm{ReLU}}=1.0$。对于 $x\\in[-1,1]$，这些选择保证对所有 $i$ 都有 $a_i x + b_i^{\\mathrm{ReLU}}\\ge 0.50$，因此 $g_{\\mathrm{ReLU}}(a_i x+b_i^{\\mathrm{ReLU}})=a_i x+b_i^{\\mathrm{ReLU}}$。因此，任何 $h\\in H_{\\mathrm{ReLU}}$ 都简化为 $x$ 的一个仿射函数。\n3) Tanh 模型族 $H_{\\tanh}$：固定宽度 $m$。选择斜率 $a_i$ 为在 $[-A,A]$ 区间内的 $m$ 个等距值，其中 $A=\\max(1.0,0.7\\,\\omega)$，并为所有隐藏单元设置 $b_i^{\\tanh}=0$。这将隐藏层的预激活值置于 $g_{\\tanh}$ 的高斜率区域附近，从而使 $H_{\\tanh}$ 能够生成一个更丰富的、平滑的非线性函数族。\n\n对于每个模型族，构建一个特征矩阵 $\\Phi\\in\\mathbb{R}^{N\\times(m+1)}$，其第一列是偏置特征 $\\phi_0(x)\\equiv 1$，其余 $m$ 列是在 $N$ 个输入上求值的 $\\phi_i(x)=g(a_i x+b_i)$。计算最小二乘解 $\\hat{\\theta}=\\arg\\min_{\\theta}\\|\\Phi\\theta - y\\|_2^2$ 并报告经验均方误差 $E=\\frac{1}{N}\\|\\Phi\\hat{\\theta}-y\\|_2^2$。\n\n角度单位要求：所有三角函数求值均使用弧度。输出不得使用百分号；任何比率必须以小数形式表示。\n\n测试套件和要求的输出：\n- 使用以下参数集 $(\\omega,m)$：$(0.0,5)$、$(3.0,5)$、$(9.0,5)$、$(9.0,2)$、$(15.0,10)$。\n- 对于每个参数集，计算三个浮点数：$E_{\\mathrm{ReLU}}$、$E_{\\tanh}$ 和差值 $E_{\\mathrm{ReLU}}-E_{\\tanh}$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。列表中的每个元素是对应一个测试用例的三元组，该三元组本身也格式化为用方括号括起来的逗号分隔列表。例如：$[[e_{1,\\mathrm{ReLU}},e_{1,\\tanh},d_1],[e_{2,\\mathrm{ReLU}},e_{2,\\tanh},d_2],\\ldots]$。\n- 交付物：提供一个完整的、可运行的程序，该程序能够构建数据集，为每个测试构建两个特征矩阵，解决最小二乘问题，计算所要求的误差，并以所需格式精确打印一行输出。不允许用户输入或使用外部文件。", "solution": "用户提供的问题是计算机器学习领域一个有效、适定且具有科学依据的练习。它要求比较两种单隐藏层神经网络模型的近似能力，其中一个使用修正线性单元（ReLU）激活函数，另一个使用双曲正切（tanh）激活函数。该分析的关键在于，固定的隐藏层参数的选择如何影响每个模型可以表示的函数类（假设空间）的丰富程度。\n\n解决方案首先将问题形式化为一个线性最小二乘任务。随后，我们为给定的测试参数构建与每个模型族对应的特征矩阵。然后计算最小二乘解，以找到每个族中的最佳拟合模型，并计算和比较它们各自的均方误差。\n\n**1. 将问题形式化为线性最小二乘**\n\n单隐藏层模型定义为 $h(x) = c + \\sum_{i=1}^m w_i\\,g(a_i x+b_i)$。根据每个模型族的问题规范，隐藏层参数，即斜率 $\\{a_i\\}_{i=1}^m$ 和偏置 $\\{b_i\\}_{i=1}^m$，是固定的。可训练的参数是输出层的偏置 $c$ 和权重 $\\{w_i\\}_{i=1}^m$。\n\n对于一个包含 $N$ 个点的数据集 $\\{(x_j, y_j)\\}_{j=1}^N$，我们可以将模型的预测表示为矩阵-向量乘积。设 $\\theta = [c, w_1, \\ldots, w_m]^T$ 为可训练参数的列向量。我们定义一个特征矩阵 $\\Phi \\in \\mathbb{R}^{N \\times (m+1)}$，其中每一行对应一个输入点 $x_j$，每一列对应一个基函数。第一列是常数偏置项，对所有 $j$ 都有 $\\phi_0(x_j) = 1$。随后的 $m$ 列是隐藏神经元的输出，即 $\\phi_i(x_j) = g(a_i x_j + b_i)$，其中 $i=1, \\ldots, m$。\n\n对所有 $x_j$ 的模型预测向量由 $\\hat{y} = \\Phi\\theta$ 给出。目标是找到最小化经验均方误差（MSE）的参数向量 $\\theta$，该误差与残差向量 $r = y - \\hat{y}$ 的欧几里得范数的平方成正比：\n$$\nE = \\frac{1}{N} \\sum_{j=1}^N (h(x_j) - y_j)^2 = \\frac{1}{N} \\| \\Phi\\theta - y \\|_2^2\n$$\n这是一个标准的线性最小二乘问题。最小化此误差的最优参数向量 $\\hat{\\theta}$ 由法方程的解给出：\n$$\n(\\Phi^T \\Phi) \\hat{\\theta} = \\Phi^T y\n$$\n在数值计算上，使用 QR 分解或奇异值分解（SVD）等方法求解 $\\hat{\\theta}$ 更为稳定，这些方法已在标准数值库中实现。一旦找到 $\\hat{\\theta}$，最小 MSE 就可计算为 $E = \\frac{1}{N} \\| \\Phi\\hat{\\theta} - y \\|_2^2$。\n\n**2. ReLU 模型族 ($H_{\\mathrm{ReLU}}$) 的分析**\n\nReLU 模型族使用激活函数 $g_{\\mathrm{ReLU}}(z) = \\max(0, z)$ 定义。隐藏参数选择如下：宽度为 $m$，斜率 $a_i$ 为 $[-\\alpha, \\alpha]$ 区间内的 $m$ 个等距值（其中 $\\alpha=0.5$），以及所有单元共享的偏置 $b_i^{\\mathrm{ReLU}} = 1.0$。\n\n一个关键的洞见是预激活项 $z_i(x) = a_i x + b_i^{\\mathrm{ReLU}}$ 在输入 $x \\in [-1, 1]$ 时的行为。由于 $|a_i| \\le 0.5$ 且 $|x| \\le 1$，项 $a_i x$ 是有界的：$|a_i x| \\le 0.5$。因此，预激活值的下界为：\n$$\nz_i(x) = a_i x + 1.0 \\ge -0.5 + 1.0 = 0.5\n$$\n由于在定义域内对所有 $i$ 和所有 $x$ 都有 $z_i(x) \\ge 0.5$，ReLU 激活函数始终在其线性区域内运行，即 $g_{\\mathrm{ReLU}}(z_i(x)) = z_i(x) = a_i x + 1.0$。\n\n因此，模型函数 $h(x) \\in H_{\\mathrm{ReLU}}$ 简化为：\n$$\nh(x) = c + \\sum_{i=1}^m w_i (a_i x + 1.0) = \\left(c + \\sum_{i=1}^m w_i\\right) + \\left(\\sum_{i=1}^m w_i a_i\\right) x\n$$\n这是一个形式为 $C_0 + C_1 x$ 的仿射函数。这意味着，尽管被表述为神经网络，但这种 ReLU 模型的特定构造只能表示线性函数。它近似非线性目标函数（如 $f^\\ast(x) = \\sin(\\omega x)$）的能力从根本上被限制在最佳可能的线性近似上。对于 $m \\ge 2$，假设空间不会随着宽度 $m$ 的增加而扩展，因为基函数 $\\{1, a_1 x+1, \\ldots, a_m x+1\\}$ 都位于 $\\{1, x\\}$ 的张成空间内。\n\n**3. Tanh 模型族 ($H_{\\tanh}$) 的分析**\n\nTanh 模型族使用激活函数 $g_{\\tanh}(z) = \\tanh(z)$。隐藏参数为：宽度 $m$，斜率 $a_i$ 为 $[-A, A]$ 区间内的 $m$ 个等距值（其中 $A = \\max(1.0, 0.7\\,\\omega)$），以及偏置 $b_i^{\\tanh} = 0$。\n\n模型函数为 $h(x) = c + \\sum_{i=1}^m w_i \\tanh(a_i x)$。与 ReLU 的情况不同，$\\tanh$ 函数在其整个定义域（除了 $z=0$ 处）都是非线性的。基函数 $\\phi_i(x) = \\tanh(a_i x)$ 是平滑的 S 型函数。斜率范围 $[-A, A]$ 根据目标函数的频率 $\\omega$ 进行调整，确保基函数具有不同的陡峭程度，以适当地捕捉 $\\sin(\\omega x)$ 的振荡。这些非线性基函数的线性组合可以形成一个丰富的平滑非线性近似函数类，其表达能力远强于 $H_{\\mathrm{ReLU}}$ 的仿射函数类。\n\n**4. 计算算法**\n\n对于测试套件中提供的每个参数集 $(\\omega, m)$，我们执行以下步骤：\n1.  **生成数据集**：创建一个包含 $N=1001$ 个在 $[-1, 1]$ 内均匀间隔的输入点向量 $x$，以及对应的目标向量 $y = \\sin(\\omega x)$。\n2.  **计算 $E_{\\mathrm{ReLU}}$**：\n    -   确定 ReLU 的斜率 $a_i$ 和偏置 $b_i^{\\mathrm{ReLU}}$。\n    -   构建 $N \\times (m+1)$ 的特征矩阵 $\\Phi_{\\mathrm{ReLU}}$，其列为 $[1, g_{\\mathrm{ReLU}}(a_1 x + b_1^{\\mathrm{ReLU}}), \\ldots, g_{\\mathrm{ReLU}}(a_m x + b_m^{\\mathrm{ReLU}})]$。\n    -   求解最小二乘问题 $\\hat{\\theta}_{\\mathrm{ReLU}} = \\arg\\min_{\\theta} \\|\\Phi_{\\mathrm{ReLU}}\\theta - y\\|_2^2$。\n    -   计算误差 $E_{\\mathrm{ReLU}} = \\frac{1}{N} \\| \\Phi_{\\mathrm{ReLU}}\\hat{\\theta}_{\\mathrm{ReLU}} - y \\|_2^2$。\n3.  **计算 $E_{\\tanh}$**：\n    -   确定 Tanh 的斜率 $a_i$（基于 $\\omega$）和偏置 $b_i^{\\tanh}$。\n    -   构建 $N \\times (m+1)$ 的特征矩阵 $\\Phi_{\\tanh}$，其列为 $[1, g_{\\tanh}(a_1 x + b_1^{\\tanh}), \\ldots, g_{\\tanh}(a_m x + b_m^{\\tanh})]$。\n    -   求解 $\\hat{\\theta}_{\\tanh} = \\arg\\min_{\\theta} \\|\\Phi_{\\tanh}\\theta - y\\|_2^2$。\n    -   计算误差 $E_{\\tanh} = \\frac{1}{N} \\| \\Phi_{\\tanh}\\hat{\\theta}_{\\tanh} - y \\|_2^2$。\n4.  **存储结果**：计算并存储三元组 $(E_{\\mathrm{ReLU}}, E_{\\tanh}, E_{\\mathrm{ReLU}} - E_{\\tanh})$。\n\n对所有测试用例重复此过程，并将最终结果按指定格式化为单个字符串。预期的结果是，对于任何非平凡的目标（$\\omega  0$），$E_{\\mathrm{ReLU}}$ 将显著大于 $E_{\\tanh}$，这表明了激活函数中的非线性在近似复杂函数时所起的关键作用。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the approximation errors of two single-hidden-layer\n    neural network models (ReLU and Tanh) on a sinusoidal target function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 5),\n        (3.0, 5),\n        (9.0, 5),\n        (9.0, 2),\n        (15.0, 10),\n    ]\n\n    all_results = []\n    \n    # Constants\n    N = 1001\n\n    for omega, m in test_cases:\n        # 1. Construct the dataset\n        x = np.linspace(-1.0, 1.0, N)\n        y = np.sin(omega * x)\n\n        # 2. ReLU Model Family Calculation\n        alpha = 0.5\n        a_relu = np.linspace(-alpha, alpha, m)\n        b_relu = 1.0\n\n        # Construct ReLU feature matrix Phi_relu\n        Phi_relu = np.ones((N, m + 1))\n        # Vectorized computation of hidden unit outputs\n        # x is (N,), a_relu is (m,). We need an (N, m) matrix.\n        # x[:, np.newaxis] is (N, 1), a_relu[np.newaxis, :] is (1, m)\n        # Broadcasting (N, 1) * (1, m) - (N, m)\n        pre_acts_relu = x[:, np.newaxis] * a_relu[np.newaxis, :] + b_relu\n        # Per problem statement logic, pre_acts_relu is always  0,\n        # so np.maximum(0, a) is equivalent to a. We use the formal definition.\n        Phi_relu[:, 1:] = np.maximum(0, pre_acts_relu)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_relu, _, _, _ = np.linalg.lstsq(Phi_relu, y, rcond=None)\n            y_pred_relu = Phi_relu @ theta_relu\n            E_relu = np.mean((y - y_pred_relu)**2)\n        except np.linalg.LinAlgError:\n            # Should not happen with np.linalg.lstsq, but as a safeguard\n            E_relu = np.nan\n\n        # 3. Tanh Model Family Calculation\n        A = max(1.0, 0.7 * omega)\n        a_tanh = np.linspace(-A, A, m)\n        b_tanh = 0.0\n\n        # Construct Tanh feature matrix Phi_tanh\n        Phi_tanh = np.ones((N, m + 1))\n        pre_acts_tanh = x[:, np.newaxis] * a_tanh[np.newaxis, :] + b_tanh\n        Phi_tanh[:, 1:] = np.tanh(pre_acts_tanh)\n\n        # Solve least squares and compute MSE\n        try:\n            theta_tanh, _, _, _ = np.linalg.lstsq(Phi_tanh, y, rcond=None)\n            y_pred_tanh = Phi_tanh @ theta_tanh\n            E_tanh = np.mean((y - y_pred_tanh)**2)\n        except np.linalg.LinAlgError:\n            E_tanh = np.nan\n            \n        # 4. Difference\n        diff = E_relu - E_tanh\n        \n        all_results.append((E_relu, E_tanh, diff))\n\n    # 5. Format the final output string\n    output_parts = []\n    for e_relu, e_tanh, d in all_results:\n        part = f\"[{e_relu:.6f},{e_tanh:.6f},{d:.6f}]\"\n        output_parts.append(part)\n    \n    final_output_string = f\"[{','.join(output_parts)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3171992"}, {"introduction": "在认识到非线性的重要性之后，我们将深入研究当前最流行的激活函数ReLU所面临的一个著名挑战：“死亡ReLU问题”（Dying ReLU problem）。本练习将引导您从概率论的视角，量化分析ReLU神经元在训练过程中为何会变得“不活跃”，即其梯度变为零的概率。您还将探索其变体——Leaky ReLU——如何通过引入一个微小的负斜率 $\\alpha$ 来有效缓解这一问题 [@problem_id:3171941]，从而确保更稳定和可靠的梯度流，这对于成功的模型训练至关重要。", "problem": "考虑修正线性单元（ReLU）激活函数及其渗漏变体。设修正线性单元（ReLU）定义为 $f(z) = \\max(0, z)$，渗漏修正线性单元（Leaky ReLU）定义为 $f_{\\alpha}(z) = \\max(\\alpha z, z)$，其中 $\\alpha$ 是负侧斜率，满足 $0 \\le \\alpha \\le 1$。修正线性单元（ReLU）的导数 $f'(z)$ 在 $z  0$ 时为 $1$，在 $z  0$ 时为 $0$。在 $z = 0$ 处的导数被认为是未定义的，但在连续分布下其发生的概率为 $0$。类似地，渗漏修正线性单元（Leaky ReLU）的导数在 $z \\ge 0$ 时为 $1$，在 $z  0$ 时为 $\\alpha$。假设在每个训练步骤中，预激活值 $z$ 是一个服从标准正态分布（SND）的随机变量，$z \\sim \\mathcal{N}(0,1)$。\n\n从导数的基本定义、标准正态分布（SND）下的概率以及上述指定的激活函数出发，推导以下量：\n\n1. 在 $z \\sim \\mathcal{N}(0,1)$ 条件下，修正线性单元（ReLU）的概率 $P(f'(z) = 0)$。\n2. 在 $z \\sim \\mathcal{N}(0,1)$ 条件下，给定 $\\alpha$ 的渗漏修正线性单元（Leaky ReLU）的概率 $P(f'(z) = 0)$。\n3. 一个 $\\alpha$ 阈值，通过对导数的期望幅值施加下界来保证避免神经元失活。具体来说，选择最小的 $\\alpha \\in [0,1]$，使得在 $z \\sim \\mathcal{N}(0,1)$ 条件下，期望绝对导数 $\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,]$ 满足 $\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,] \\ge \\gamma$，同时强制要求严格为正的斜率 $\\alpha \\ge \\varepsilon$，以使得在 $z \\sim \\mathcal{N}(0,1)$ 条件下 $P(f'(z) = 0)$ 为零。您的任务是基于标准正态分布（SND）和激活函数的基本定义和性质，计算 $\\alpha_{\\text{threshold}} = \\max(2\\gamma - 1, \\varepsilon)$，并将其裁剪到区间 $[0,1]$ 内。\n\n为符合科学现实，假设分布 $z \\sim \\mathcal{N}(0,1)$ 在所有训练步骤中保持不变。将所有概率表示为 $[0,1]$ 内的实数。\n\n测试套件：\n为以下参数集提供计算，每个参数集表示为 $(T, \\alpha, \\gamma, \\varepsilon)$，其中 $T$ 是训练步骤数（用于显示跨步骤的时间序列），$\\alpha$ 是给定的渗漏修正线性单元（Leaky ReLU）的负侧斜率，$\\gamma$ 是所需的下界，$\\varepsilon$ 是最小的严格正斜率：\n\n- 测试用例 1：$(T=\\;3,\\;\\alpha=\\;0.0,\\;\\gamma=\\;0.5,\\;\\varepsilon=\\;0.001)$。\n- 测试用例 2：$(T=\\;4,\\;\\alpha=\\;0.05,\\;\\gamma=\\;0.6,\\;\\varepsilon=\\;0.001)$。\n- 测试用例 3：$(T=\\;2,\\;\\alpha=\\;0.0,\\;\\gamma=\\;1.0,\\;\\varepsilon=\\;0.01)$。\n\n每个测试用例所需的输出：\n- 一个包含 $T$ 个概率的列表 $[P_1^{\\text{ReLU}}, P_2^{\\text{ReLU}}, \\dots, P_T^{\\text{ReLU}}]$，其中 $P_t^{\\text{ReLU}} = P(f'(z)=0)$ 是在训练步骤 $t$ 时修正线性单元（ReLU）的概率。\n- 一个包含 $T$ 个概率的列表 $[P_1^{\\text{Leaky}}, P_2^{\\text{Leaky}}, \\dots, P_T^{\\text{Leaky}}]$，其中 $P_t^{\\text{Leaky}} = P(f'(z)=0)$ 是在训练步骤 $t$ 时给定 $\\alpha$ 的渗漏修正线性单元（Leaky ReLU）的概率。\n- 使用上述原理根据 $\\gamma$ 和 $\\varepsilon$ 计算出的单一值 $\\alpha_{\\text{threshold}}$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个元素对应一个测试用例，并且其本身必须是一个结构为 $[\\text{ReLU\\_series},\\;\\text{Leaky\\_series},\\;\\alpha_{\\text{threshold}}]$ 的列表。例如，整体输出格式应为 $[[\\dots],[\\dots],[\\dots]]$，其中每个内部列表包含一个测试用例所需的三个输出。", "solution": "该问题要求计算与 ReLU 和 Leaky ReLU 激活函数相关的特定概率量，其简化假设是预激活输入 $z$ 从标准正态分布 $z \\sim \\mathcal{N}(0,1)$ 中抽取。该问题被认为是有效的，因为其前提在科学上基于概率论和微积分的基本原理，并且问题是适定的，能够导出一个唯一且有意义的解。定义和约束是自洽且一致的。尽管假设 $z$ 的分布在训练步骤中保持静态是一种简化，并且将 $\\alpha_{\\text{threshold}}$ 的公式作为推导任务来呈现有些循环论证，但这些方面并不影响核心数学练习的有效性。我们将逐步严格推导所需的量。\n\n分析结构如下：\n1.  计算标准修正线性单元（ReLU）的零梯度事件概率。\n2.  计算渗漏修正线性单元（Leaky ReLU）的相同概率。\n3.  推导和计算阈值参数 $\\alpha_{\\text{threshold}}$。\n4.  将这些结果应用于所提供的具体测试用例。\n\n### 1. ReLU 的零梯度概率\n\n修正线性单元（ReLU）函数定义为 $f(z) = \\max(0, z)$。为进行此分析，其导数由下式给出：\n$$\nf'(z) = \\begin{cases} \n1  \\text{if } z  0 \\\\\n0  \\text{if } z  0 \n\\end{cases}\n$$\n在 $z=0$ 处的导数是未定义的，但对于像 $z \\sim \\mathcal{N}(0,1)$ 这样的连续随机变量，任何单点的概率都为零，即 $P(z=0) = 0$。因此，这个不可导点不影响我们的概率计算。\n\n我们需要求出概率 $P(f'(z) = 0)$。根据导数的定义，事件 $f'(z) = 0$ 等价于事件 $z  0$。因此，我们必须计算 $P(z  0)$，其中 $z$ 服从标准正态分布。\n\n标准正态分布 $\\mathcal{N}(0,1)$ 的概率密度函数（PDF）关于均值 $z=0$ 对称。其累积分布函数（CDF）通常表示为 $\\Phi(z) = P(Z \\le z)$。根据对称性，随机变量小于均值的概率恰好为 $0.5$。\n$$\nP(f'(z) = 0) = P(z  0) = \\Phi(0) = 0.5\n$$\n问题指出 $z$ 的分布在训练步骤中保持不变。因此，对于所有训练步骤 $t=1, \\dots, T$，这个概率是恒定的。对于给定的 $T$，概率序列是一个包含 $T$ 个相同值 $0.5$ 的列表。\n\n### 2. Leaky ReLU 的零梯度概率\n\n渗漏 ReLU 函数定义为 $f_{\\alpha}(z) = \\max(\\alpha z, z)$，其中斜率参数 $0 \\le \\alpha \\le 1$。其导数由下式给出：\n$$\nf'_{\\alpha}(z) = \\begin{cases} \n1  \\text{if } z \\ge 0 \\\\\n\\alpha  \\text{if } z  0 \n\\end{cases}\n$$\n我们要求解概率 $P(f'_{\\alpha}(z) = 0)$。导数 $f'_{\\alpha}(z)$ 只能取两个值：$1$ 和 $\\alpha$。事件 $f'_{\\alpha}(z) = 0$ 仅当这两个值之一为 $0$ 时才可能发生。由于问题指定 $0 \\le \\alpha \\le 1$，我们考虑给定参数 $\\alpha$ 的两种情况：\n\n-   **情况 1：$\\alpha  0$**。在这种情况下，导数的可能值为 $1$ 和 $\\alpha$，两者都严格为正。因此，导数永远不可能为零。事件 $f'_{\\alpha}(z) = 0$ 是一个不可能事件，其概率为 $0$。\n    $$\n    P(f'_{\\alpha}(z) = 0) = 0 \\quad \\text{if } \\alpha  0\n    $$\n-   **情况 2：$\\alpha = 0$**。在这种情况下，渗漏 ReLU 函数变为标准 ReLU 函数，$f_{0}(z) = \\max(0, z) = f(z)$。当 $z  0$ 时，导数变为 $0$。如前一节所计算，该事件的概率为 $0.5$。\n    $$\n    P(f'_{0}(z) = 0) = P(z  0) = 0.5 \\quad \\text{if } \\alpha = 0\n    $$\n总而言之，如果 $\\alpha=0$，则 $P(f'_{\\alpha}(z)=0)$ 为 $0.5$；如果 $\\alpha0$，则为 $0$。与之前一样，这个概率在所有训练步骤 $T$ 中都是恒定的。\n\n### 3. 阈值 $\\alpha_{\\text{threshold}}$ 的推导\n\n问题要求找到满足两个条件的最小 $\\alpha \\in [0,1]$：\n1.  期望绝对导数满足一个下界：$\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,] \\ge \\gamma$。\n2.  斜率 $\\alpha$ 严格为正且有一个余量：$\\alpha \\ge \\varepsilon$。\n\n首先，我们推导期望 $\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,]$ 的表达式。导数的绝对值 $|f'_{\\alpha}(z)|$ 是一个由 $z$ 决定的随机变量。\n因为 $0 \\le \\alpha \\le 1$，我们有 $|\\alpha| = \\alpha$。\n$$\n|f'_{\\alpha}(z)| = \\begin{cases} \n|1| = 1  \\text{if } z \\ge 0 \\\\\n|\\alpha| = \\alpha  \\text{if } z  0 \n\\end{cases}\n$$\n这个离散随机变量的期望是其可能值乘以各自概率的总和：\n$$\n\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,] = 1 \\cdot P(z \\ge 0) + \\alpha \\cdot P(z  0)\n$$\n对于 $z \\sim \\mathcal{N}(0,1)$，我们有 $P(z \\ge 0) = 0.5$ 和 $P(z  0) = 0.5$。代入这些值：\n$$\n\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,] = 1 \\cdot (0.5) + \\alpha \\cdot (0.5) = 0.5(1 + \\alpha)\n$$\n现在，我们应用第一个条件，$\\mathbb{E}[\\,|f'_{\\alpha}(z)|\\,] \\ge \\gamma$：\n$$\n0.5(1 + \\alpha) \\ge \\gamma \\implies 1 + \\alpha \\ge 2\\gamma \\implies \\alpha \\ge 2\\gamma - 1\n$$\n此推导证实了问题陈述中提供的表达式。为了同时满足这两个条件，$\\alpha$ 必须大于或等于 $2\\gamma - 1$ 和 $\\varepsilon$。满足此条件的最小 $\\alpha$ 值是这两个下界中的最大值：\n$$\n\\alpha \\ge \\max(2\\gamma - 1, \\varepsilon)\n$$\n最后，问题要求 $\\alpha_{\\text{threshold}}$ 必须在 Leaky ReLU 斜率的有效范围 $[0, 1]$ 内。因此，我们必须将计算出的值裁剪到这个区间。\n$$\n\\alpha_{\\text{threshold}} = \\min(1.0, \\max(0.0, \\max(2\\gamma - 1, \\varepsilon)))\n$$\n由于应用了 $\\max(0.0, \\cdot)$，表达式简化为：\n$$\n\\alpha_{\\text{threshold}} = \\min(1.0, \\max(2\\gamma - 1, \\varepsilon))\n$$\n这个公式将用于测试用例。\n\n### 4. 应用于测试用例\n\n我们现在为每个测试用例计算所需的输出。\n\n**测试用例 1：** $(T=3, \\alpha=0.0, \\gamma=0.5, \\varepsilon=0.001)$\n-   **ReLU 序列**：$P(f'(z) = 0) = 0.5$。$T=3$ 的概率列表是 $[0.5, 0.5, 0.5]$。\n-   **Leaky ReLU 序列**：给定的斜率为 $\\alpha = 0.0$。因此，$P(f'_{\\alpha}(z) = 0) = 0.5$。$T=3$ 的概率列表是 $[0.5, 0.5, 0.5]$。\n-   **$\\alpha_{\\text{threshold}}$**：我们计算 $\\max(2\\gamma - 1, \\varepsilon) = \\max(2(0.5) - 1, 0.001) = \\max(0, 0.001) = 0.001$。将其裁剪到 $[0,1]$ 区间内得到 $\\alpha_{\\text{threshold}} = 0.001$。\n\n**测试用例 2：** $(T=4, \\alpha=0.05, \\gamma=0.6, \\varepsilon=0.001)$\n-   **ReLU 序列**：$P(f'(z) = 0) = 0.5$。$T=4$ 的概率列表是 $[0.5, 0.5, 0.5, 0.5]$。\n-   **Leaky ReLU 序列**：给定的斜率为 $\\alpha = 0.05  0$。因此，$P(f'_{\\alpha}(z) = 0) = 0$。$T=4$ 的概率列表是 $[0.0, 0.0, 0.0, 0.0]$。\n-   **$\\alpha_{\\text{threshold}}$**：我们计算 $\\max(2\\gamma - 1, \\varepsilon) = \\max(2(0.6) - 1, 0.001) = \\max(0.2, 0.001) = 0.2$。将其裁剪到 $[0,1]$ 区间内得到 $\\alpha_{\\text{threshold}} = 0.2$。\n\n**测试用例 3：** $(T=2, \\alpha=0.0, \\gamma=1.0, \\varepsilon=0.01)$\n-   **ReLU 序列**：$P(f'(z) = 0) = 0.5$。$T=2$ 的概率列表是 $[0.5, 0.5]$。\n-   **Leaky ReLU 序列**：给定的斜率为 $\\alpha = 0.0$。因此，$P(f'_{\\alpha}(z) = 0) = 0.5$。$T=2$ 的概率列表是 $[0.5, 0.5]$。\n-   **$\\alpha_{\\text{threshold}}$**：我们计算 $\\max(2\\gamma - 1, \\varepsilon) = \\max(2(1.0) - 1, 0.01) = \\max(1.0, 0.01) = 1.0$。将其裁剪到 $[0,1]$ 区间内得到 $\\alpha_{\\text{threshold}} = 1.0$。\n\n这些计算在以下程序中实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating probabilities and thresholds for\n    ReLU and Leaky ReLU activation functions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (T, alpha, gamma, epsilon).\n    test_cases = [\n        (3, 0.0, 0.5, 0.001),\n        (4, 0.05, 0.6, 0.001),\n        (2, 0.0, 1.0, 0.01),\n    ]\n\n    all_results = []\n    \n    for T, alpha, gamma, epsilon in test_cases:\n        # Part 1: Calculate the series for ReLU.\n        # The probability of the derivative being zero for ReLU is P(z  0), where\n        # z ~ N(0,1). This probability is 0.5. Since the distribution is static\n        # over training steps T, the probability is constant.\n        prob_relu = 0.5\n        relu_series = [prob_relu] * T\n\n        # Part 2: Calculate the series for Leaky ReLU.\n        # The derivative is alpha for z  0 and 1 for z >= 0.\n        # The derivative is zero if and only if alpha is zero.\n        # If alpha = 0, the probability is P(z  0) = 0.5.\n        # If alpha > 0, the probability is 0.\n        prob_leaky = 0.5 if alpha == 0.0 else 0.0\n        leaky_series = [prob_leaky] * T\n\n        # Part 3: Calculate alpha_threshold.\n        # Derivation shows alpha_threshold = min(1, max(0, max(2*gamma - 1, epsilon))).\n        # The equation from the problem simplifies this, as `max` with `epsilon` (>=0)\n        # handles the lower bound of 0.\n        # alpha_threshold = max(2*gamma - 1, epsilon), clipped to the interval [0,1].\n        val = max(2 * gamma - 1, epsilon)\n        alpha_threshold = min(1.0, max(0.0, val)) # Clipping ensures value is in [0,1]\n        \n        case_result = [relu_series, leaky_series, alpha_threshold]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, [[series1, series2, num1], [seriesA, seriesB, numA], ...]\n    # We construct the string representation manually to ensure no unwanted spaces.\n    result_strings = []\n    for res in all_results:\n        # res = [relu_series, leaky_series, alpha_threshold]\n        s_relu = f\"[{','.join(map(str, res[0]))}]\"\n        s_leaky = f\"[{','.join(map(str, res[1]))}]\"\n        s_alpha = str(res[2])\n        result_strings.append(f\"[{s_relu},{s_leaky},{s_alpha}]\")\n\n    final_output_string = f\"[{','.join(result_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3171941"}, {"introduction": "在深度神经网络中，维持信号的稳定传播是避免梯度消失或爆炸的关键。本项高级练习将带您探索“自归一化网络”这一前沿概念，其核心思想是设计一种激活函数，使其能自动将神经元的输出推向一个稳定的分布。您将通过理论推导和数值模拟相结合的方式，揭示缩放指数型线性单元（SELU）如何通过建立并收敛到一个特定的均值与方差不动点 $(\\mu^*, \\sigma^{*2}) = (0, 1)$，来实现这一非凡的自归一化特性 [@problem_id:3171997]。这个练习将深化您对激活函数在稳定深度学习训练动态中所扮演的复杂角色的理解。", "problem": "考虑一个宽度为 $n \\gg 1$ 的全连接前馈层，其权重 $w_i$ 独立同分布于 $\\mathcal{N}(0, 1/n)$，且偏置为零。设输入坐标 $x_i$ 独立同分布，均值为 $\\mu$，方差为 $\\sigma^2$。在宽层极限下，根据中心极限定理 (CLT)，预激活值 $z = \\sum_{i=1}^{n} w_i x_i$ 近似服从高斯分布，均值为 0，方差为 $\\sigma_z^2 = \\sigma^2 + \\mu^2$。激活函数是缩放指数线性单元 (SELU)，定义为 $f(z) = \\lambda \\, g(z)$，其中\n$$\ng(z) =\n\\begin{cases}\nz,  \\text{if } z  0, \\\\\n\\alpha \\left(e^z - 1\\right),  \\text{if } z \\le 0,\n\\end{cases}\n$$\n且 $\\alpha  0$ 和 $\\lambda  0$ 是待确定的常数。\n\n定义层映射 $T$，它将输入均值和方差 $(\\mu, \\sigma^2)$ 映射到将 $f$ 应用于 $z \\sim \\mathcal{N}(0, \\sigma_z^2)$ 后产生的输出均值和方差 $(\\mu', \\sigma'^2)$，其中\n$$\n\\sigma_z^2 = \\sigma^2 + \\mu^2.\n$$\n使用高斯随机变量的基本概率性质（不假设 $\\alpha$ 或 $\\lambda$ 的任何特定值），推导输出均值 $\\mu'$ 和输出方差 $\\sigma'^2$ 关于 $\\sigma_z^2$、$\\alpha$ 和 $\\lambda$ 的表达式。然后，推导 $T$ 的不动点 $(\\mu^*, \\sigma^{*2})$，并确定 $\\alpha$ 和 $\\lambda$ 的值，使得 $(\\mu^*, \\sigma^{*2}) = (0, 1)$ 是一个不动点，即当输入均值和方差为 $(0, 1)$ 时，输出均值和方差也为 $(0, 1)$。\n\n在推导出所需公式后，实现一个程序，该程序：\n1. 计算使 $(0, 1)$ 成为 $T$ 的不动点的 $\\alpha$ 和 $\\lambda$。\n2. 对于一组给定的初始 $(\\mu, \\sigma^2)$ 测试用例，将映射 $T$ 迭代 10 层，并返回到不动点的最终欧几里得距离，定义为\n$$\nd = \\sqrt{(\\mu_{10} - 0)^2 + (\\sigma_{10}^2 - 1)^2}.\n$$\n\n你的程序必须使用以下初始条件测试套件，以 $(\\mu, \\sigma^2)$ 的形式给出：\n- $(0.0, 1.0)$，\n- $(0.1, 0.9)$，\n- $(1.0, 1.0)$，\n- $(0.0, 3.0)$，\n- $(-0.5, 0.5)$，\n- $(2.0, 4.0)$。\n\n你的程序应生成单行输出，其中包含六个测试用例的距离，形式为用方括号括起来的逗号分隔列表（例如，$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$）。所有数值均为无量纲量。输出值必须是实数（浮点数）。", "solution": "该问题是有效的，因为它在科学上基于神经网络动力学理论，问题提法良好，有足够的信息得到唯一解，并且陈述客观。我们将进行完整的推导和求解。\n\n目标是分析激活值的均值和方差在通过一个使用缩放指数线性单元（SELU）激活函数的全连接神经网络层时的传播情况。我们首先将推导映射 $T: (\\mu, \\sigma^2) \\to (\\mu', \\sigma'^2)$，该映射描述了输入均值 $\\mu$ 和方差 $\\sigma^2$ 如何变换为输出均值 $\\mu'$ 和方差 $\\sigma'^2$。然后，我们将找出 SELU 函数的特定参数 $\\alpha$ 和 $\\lambda$，以确保在 $(\\mu, \\sigma^2) = (0, 1)$ 处存在一个稳定的不动点。\n\n预激活信号 $z$ 由输入 $x_i$ 的加权和形成：$z = \\sum_{i=1}^{n} w_i x_i$。输入 $x_i$ 是独立同分布的（i.i.d.），均值为 $E[x_i] = \\mu$，方差为 $Var(x_i) = \\sigma^2$。权重 $w_i$ 是从正态分布 $\\mathcal{N}(0, 1/n)$ 中抽取的独立同分布随机变量。根据中心极限定理，对于大的层宽 $n$，预激活值 $z$ 近似为一个高斯随机变量。其均值为 $E[z] = \\sum_{i=1}^{n} E[w_i x_i] = \\sum_{i=1}^{n} E[w_i]E[x_i] = \\sum_{i=1}^{n} 0 \\cdot \\mu = 0$。其方差由于独立性为 $Var(z) = \\sum_{i=1}^{n} Var(w_i x_i)$。我们有 $Var(w_i x_i) = E[(w_i x_i)^2] - (E[w_i x_i])^2 = E[w_i^2]E[x_i^2] - 0 = (Var(w_i) + E[w_i]^2)(Var(x_i) + E[x_i]^2) = (1/n + 0)(\\sigma^2 + \\mu^2) = (\\sigma^2 + \\mu^2)/n$。因此，$Var(z) = \\sum_{i=1}^{n} (\\sigma^2 + \\mu^2)/n = n \\cdot (\\sigma^2 + \\mu^2)/n = \\sigma^2 + \\mu^2$。我们将此方差表示为 $\\sigma_z^2 = \\sigma^2 + \\mu^2$。所以，预激活值 $z$ 是一个随机变量 $Z \\sim \\mathcal{N}(0, \\sigma_z^2)$。\n\n激活函数是 $f(z) = \\lambda g(z)$，其中\n$$\ng(z) =\n\\begin{cases}\nz,  \\text{if } z  0, \\\\\n\\alpha \\left(e^z - 1\\right),  \\text{if } z \\le 0,\n\\end{cases}\n$$\n其中 $\\alpha  0$ 和 $\\lambda  0$ 是常数。输出激活值是一个随机变量 $Y = f(Z)$。输出均值为 $\\mu' = E[Y]$，输出方差为 $\\sigma'^2 = Var(Y)$。\n\n$\\mu' = E[f(Z)] = \\lambda E[g(Z)]$\n$\\sigma'^2 = Var[f(Z)] = \\lambda^2 Var[g(Z)] = \\lambda^2 \\left( E[g(Z)^2] - (E[g(Z)])^2 \\right)$\n\n我们必须计算 $g(Z)$ 的前两阶矩，其中 $Z \\sim \\mathcal{N}(0, \\sigma_z^2)$，其概率密度函数 (PDF) 为 $\\phi(z; \\sigma_z^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_z^2}}e^{-z^2/(2\\sigma_z^2)}$。\n\n**1. 输出均值 $\\mu'$ 的推导**\n\n一阶矩 $E[g(Z)]$ 为：\n$$\nE[g(Z)] = \\int_{-\\infty}^{\\infty} g(z) \\phi(z; \\sigma_z^2) dz = \\int_{-\\infty}^{0} \\alpha(e^z-1)\\phi(z; \\sigma_z^2)dz + \\int_{0}^{\\infty} z \\phi(z; \\sigma_z^2)dz\n$$\n第二个积分是半正态分布的一阶矩，按 $\\sigma_z$ 缩放后为：\n$\\int_{0}^{\\infty} z \\frac{1}{\\sqrt{2\\pi\\sigma_z^2}}e^{-z^2/(2\\sigma_z^2)}dz = \\frac{\\sigma_z}{\\sqrt{2\\pi}}$。\n\n第一个积分为：\n$\\alpha \\left( \\int_{-\\infty}^{0} e^z \\phi(z; \\sigma_z^2)dz - \\int_{-\\infty}^{0} \\phi(z; \\sigma_z^2)dz \\right)$。\n第二项是 $\\alpha \\cdot P(Z \\le 0) = \\alpha/2$。对于第一项，我们在 $e^z \\phi(z; \\sigma_z^2)$ 的指数部分进行配方：\n$z - \\frac{z^2}{2\\sigma_z^2} = -\\frac{1}{2\\sigma_z^2}(z^2 - 2\\sigma_z^2 z) = -\\frac{(z-\\sigma_z^2)^2}{2\\sigma_z^2} + \\frac{\\sigma_z^2}{2}$。\n该积分变为：\n$\\int_{-\\infty}^{0} e^{\\sigma_z^2/2} \\frac{1}{\\sqrt{2\\pi\\sigma_z^2}}e^{-\\frac{(z-\\sigma_z^2)^2}{2\\sigma_z^2}}dz = e^{\\sigma_z^2/2} P(X \\le 0)$，其中 $X \\sim \\mathcal{N}(\\sigma_z^2, \\sigma_z^2)$。\n对 $X$ 进行标准化得到 $P(\\frac{X-\\sigma_z^2}{\\sigma_z} \\le \\frac{0-\\sigma_z^2}{\\sigma_z}) = P(S \\le -\\sigma_z) = \\Phi(-\\sigma_z)$，其中 $S \\sim \\mathcal{N}(0,1)$，$\\Phi$ 是其累积分布函数 (CDF)。\n因此，$E[g(Z)] = \\frac{\\sigma_z}{\\sqrt{2\\pi}} + \\alpha \\left( e^{\\sigma_z^2/2} \\Phi(-\\sigma_z) - \\frac{1}{2} \\right)$。\n输出均值为 $\\mu' = \\lambda E[g(Z)]$。\n\n**2. 输出方差 $\\sigma'^2$ 的推导**\n\n二阶矩 $E[g(Z)^2]$ 为：\n$$\nE[g(Z)^2] = \\int_{-\\infty}^{\\infty} g(z)^2 \\phi(z; \\sigma_z^2) dz = \\int_{-\\infty}^{0} \\alpha^2(e^z-1)^2\\phi(z; \\sigma_z^2)dz + \\int_{0}^{\\infty} z^2 \\phi(z; \\sigma_z^2)dz\n$$\n第二个积分是 $Z$ 的方差的一半：$\\int_{0}^{\\infty} z^2 \\phi(z; \\sigma_z^2)dz = \\frac{1}{2} E[Z^2] = \\frac{\\sigma_z^2}{2}$。\n第一个积分是 $\\alpha^2 \\int_{-\\infty}^{0} (e^{2z} - 2e^z + 1)\\phi(z; \\sigma_z^2)dz$。\n使用与之前相同的技巧：\n$\\int_{-\\infty}^{0} \\phi(z; \\sigma_z^2)dz = 1/2$。\n$\\int_{-\\infty}^{0} e^z\\phi(z; \\sigma_z^2)dz = e^{\\sigma_z^2/2} \\Phi(-\\sigma_z)$。\n$\\int_{-\\infty}^{0} e^{2z}\\phi(z; \\sigma_z^2)dz = e^{2\\sigma_z^2} \\Phi(-2\\sigma_z)$。\n因此，$E[g(Z)^2] = \\frac{\\sigma_z^2}{2} + \\alpha^2 \\left( e^{2\\sigma_z^2}\\Phi(-2\\sigma_z) - 2e^{\\sigma_z^2/2}\\Phi(-\\sigma_z) + \\frac{1}{2} \\right)$。\n输出方差为 $\\sigma'^2 = \\lambda^2 \\left( E[g(Z)^2] - (E[g(Z)])^2 \\right)$。\n\n**3. 为不动点 $(0, 1)$ 确定 $\\alpha$ 和 $\\lambda$**\n\n我们寻求参数 $\\alpha, \\lambda$，使得当输入为 $(\\mu, \\sigma^2)=(0, 1)$ 时，输出为 $(\\mu', \\sigma'^2)=(0, 1)$。\n对于此输入，$\\sigma_z^2 = \\mu^2 + \\sigma^2 = 0^2 + 1^2 = 1$，因此 $\\sigma_z = 1$，预激活值为 $Z \\sim \\mathcal{N}(0, 1)$。\n\n条件 1：$\\mu' = 0$。由于 $\\lambda  0$，这意味着 $E[g(Z)] = 0$。\n$E[g(Z)]|_{\\sigma_z=1} = \\frac{1}{\\sqrt{2\\pi}} + \\alpha \\left( e^{1/2} \\Phi(-1) - \\frac{1}{2} \\right) = 0$。\n求解 $\\alpha$：\n$$\n\\alpha = \\frac{-1/\\sqrt{2\\pi}}{e^{1/2} \\Phi(-1) - 1/2}\n$$\n条件 2：$\\sigma'^2 = 1$。\n$\\sigma'^2 = \\lambda^2 \\left( E[g(Z)^2] - (E[g(Z)])^2 \\right)$。由于 $E[g(Z)]=0$，该式简化为 $1 = \\lambda^2 E[g(Z)^2]|_{\\sigma_z=1}$。\n求解 $\\lambda$：\n$$\n\\lambda = \\frac{1}{\\sqrt{E[g(Z)^2]|_{\\sigma_z=1}}}\n$$\n其中 $E[g(Z)^2]|_{\\sigma_z=1} = \\frac{1}{2} + \\alpha^2 \\left( e^{2}\\Phi(-2) - 2e^{1/2}\\Phi(-1) + \\frac{1}{2} \\right)$，使用上面确定的 $\\alpha$ 值。\n\n**4. 迭代映射和实现**\n\n参数 $\\alpha$ 和 $\\lambda$ 是仅需计算一次的常数。层映射 $T$ 按照如下方式将 $(\\mu_k, \\sigma_k^2)$ 更新为 $(\\mu_{k+1}, \\sigma_{k+1}^2)$：\n1.  计算预激活方差：$\\sigma_{z,k}^2 = \\mu_k^2 + \\sigma_k^2$。令 $\\sigma_{z,k} = \\sqrt{\\sigma_{z,k}^2}$。\n2.  使用推导出的公式和 $\\sigma_z = \\sigma_{z,k}$ 计算中间矩：\n    $E_k[g(Z)] = \\frac{\\sigma_{z,k}}{\\sqrt{2\\pi}} + \\alpha \\left( e^{\\sigma_{z,k}^2/2} \\Phi(-\\sigma_{z,k}) - \\frac{1}{2} \\right)$。\n    $E_k[g(Z)^2] = \\frac{\\sigma_{z,k}^2}{2} + \\alpha^2 \\left( e^{2\\sigma_{z,k}^2}\\Phi(-2\\sigma_{z,k}) - 2e^{\\sigma_{z,k}^2/2}\\Phi(-\\sigma_{z,k}) + \\frac{1}{2} \\right)$。\n3.  计算输出均值和方差：\n    $\\mu_{k+1} = \\lambda E_k[g(Z)]$。\n    $\\sigma_{k+1}^2 = \\lambda^2 \\left( E_k[g(Z)^2] - (E_k[g(Z)])^2 \\right)$。\n\n程序将首先计算常数 $\\alpha$ 和 $\\lambda$。然后，对于每个初始条件 $(\\mu_0, \\sigma_0^2)$，它将迭代此映射 10 次以获得 $(\\mu_{10}, \\sigma_{10}^2)$。最后，它将计算到不动点 $(0, 1)$ 的欧几里得距离：$d = \\sqrt{(\\mu_{10}-0)^2+(\\sigma_{10}^2-1)^2}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes SELU parameters for a fixed point at (0, 1),\n    iterates the mean/variance map for several test cases, and\n    calculates the final distance to the fixed point.\n    \"\"\"\n\n    def calculate_selu_params():\n        \"\"\"\n        Calculates alpha and lambda for the SELU activation function\n        such that (mu=0, sigma^2=1) is a fixed point.\n        The fixed point condition implies z ~ N(0, 1), so sigma_z = 1.\n        \"\"\"\n        # For E[g(Z)] = 0, where Z ~ N(0,1):\n        # E[g(Z)] = 1/sqrt(2*pi) + alpha * [exp(1/2) * Phi(-1) - 1/2] = 0\n        phi_neg_1 = norm.cdf(-1.0)\n        numerator_alpha = -1.0 / np.sqrt(2.0 * np.pi)\n        denominator_alpha = np.exp(0.5) * phi_neg_1 - 0.5\n        alpha = numerator_alpha / denominator_alpha\n\n        # For Var(f(Z)) = 1, where Z ~ N(0,1) and E[g(Z)] = 0:\n        # Var(f(Z)) = lambda^2 * E[g(Z)^2] = 1, so lambda = 1 / sqrt(E[g(Z)^2])\n        # E[g(Z)^2] = 1/2 + alpha^2 * [exp(2)*Phi(-2) - 2*exp(1/2)*Phi(-1) + 1/2]\n        phi_neg_2 = norm.cdf(-2.0)\n        \n        term1 = np.exp(2.0) * phi_neg_2\n        term2 = -2.0 * np.exp(0.5) * phi_neg_1\n        term3 = 0.5\n        \n        e_g_z_squared = 0.5 + alpha**2 * (term1 + term2 + term3)\n        lambda_ = 1.0 / np.sqrt(e_g_z_squared)\n        \n        return alpha, lambda_\n\n    def layer_map(mu, sigma2, alpha, lambda_):\n        \"\"\"\n        Applies the layer map T to transform (mu, sigma^2) to (mu', sigma'^2).\n        \"\"\"\n        # Pre-activation variance\n        sigma_z2 = mu**2 + sigma2\n        # Handle potential numerical issues where sigma_z2 may be slightly negative\n        if sigma_z2  0:\n            sigma_z2 = 0\n        sigma_z = np.sqrt(sigma_z2)\n\n        # First moment of g(Z)\n        phi_neg_sigma_z = norm.cdf(-sigma_z)\n        e_g_z = (sigma_z / np.sqrt(2 * np.pi)) + alpha * (np.exp(sigma_z2 / 2.0) * phi_neg_sigma_z - 0.5)\n        \n        # Second moment of g(Z)\n        phi_neg_2_sigma_z = norm.cdf(-2.0 * sigma_z)\n        term1 = np.exp(2.0 * sigma_z2) * phi_neg_2_sigma_z\n        term2 = -2.0 * np.exp(sigma_z2 / 2.0) * phi_neg_sigma_z\n        term3 = 0.5\n        e_g_z_squared = (sigma_z2 / 2.0) + alpha**2 * (term1 + term2 + term3)\n        \n        # Output mean and variance\n        mu_prime = lambda_ * e_g_z\n        sigma2_prime = lambda_**2 * (e_g_z_squared - e_g_z**2)\n        \n        return mu_prime, sigma2_prime\n\n    # Compute SELU parameters alpha and lambda\n    alpha, lambda_ = calculate_selu_params()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1.0),\n        (0.1, 0.9),\n        (1.0, 1.0),\n        (0.0, 3.0),\n        (-0.5, 0.5),\n        (2.0, 4.0),\n    ]\n\n    results = []\n    num_layers = 10\n    \n    for case in test_cases:\n        mu, sigma2 = case\n        for _ in range(num_layers):\n            mu, sigma2 = layer_map(mu, sigma2, alpha, lambda_)\n        \n        # Final values after 10 layers\n        mu_10, sigma2_10 = mu, sigma2\n        \n        # Euclidean distance to the fixed point (0, 1)\n        distance = np.sqrt(mu_10**2 + (sigma2_10 - 1.0)**2)\n        results.append(distance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3171997"}]}