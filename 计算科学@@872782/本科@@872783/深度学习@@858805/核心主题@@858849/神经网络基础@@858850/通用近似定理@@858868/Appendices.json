{"hands_on_practices": [{"introduction": "“通用近似定理”保证了网络的存在性，但一个关键的实际问题是：我们需要一个多大的网络？这个实践练习将揭示，所需神经元的数量与目标函数的“复杂度”密切相关。我们将通过近似一个具有局部高曲率的函数，来探索如何利用经典插值理论估算单层ReLU网络所需的神经元数量，从而在理论和实践之间建立起一座桥梁。[@problem_id:3194159]", "problem": "考虑由参数 $\\alpha0$ 和 $\\delta\\in(0,1)$ 定义的一维目标函数 $f:[-1,1]\\to \\mathbb{R}$，其定义如下：\n$$\nf(x) = \n\\begin{cases}\n\\alpha x^{2},  |x|\\le \\delta \\\\\n\\alpha \\delta^{2} + 2\\alpha \\delta(|x|-\\delta),  \\delta  |x|\\le 1\n\\end{cases}\n$$\n因此，$f$ 在 $x=0$ 附近有一个宽度为 $2\\delta$ 的局部高曲率区域，并在此区域外是线性的。设 $\\epsilon\\in(0,1)$ 为目标均匀误差。您将使用单隐层修正线性单元（ReLU）网络来逼近 $[-1,1]$ 上的 $f$，其中 ReLU 激活函数为 $\\sigma(t)=\\max\\{0,t\\}$。因此，该网络在 $[-1,1]$ 上实现连续分段线性函数，其线性片段的数量最多为隐藏神经元的数量加 $1$。\n\n请仅使用关于二次可微函数的线性样条插值误差的基本性质，以及单隐层 ReLU 网络作为连续分段线性函数的表示属性，推导出一个显式函数 $U(\\epsilon,\\alpha,\\delta)$，使得存在一个宽度至多为 $U(\\epsilon,\\alpha,\\delta)$ 的单隐层 ReLU 网络，其输出 $g$ 满足\n$$\n\\sup_{x\\in[-1,1]} |f(x)-g(x)| \\le \\epsilon.\n$$\n您的答案必须是 $U(\\epsilon,\\alpha,\\delta)$ 的单个封闭形式表达式（在最终表达式中不要使用不等式或取整函数）。不需要进行数值取整。", "solution": "用户希望找到一个函数 $U(\\epsilon,\\alpha,\\delta)$，该函数提供了一个单隐层修正线性单元（ReLU）网络宽度的上界，此宽度足以在至多为 $\\epsilon$ 的均匀误差下逼近给定的目标函数 $f(x)$。\n\n### 步骤 1：问题验证\n首先验证问题陈述。\n\n**逐字提取的已知条件：**\n- 目标函数：$f:[-1,1]\\to \\mathbb{R}$\n- 参数：$\\alpha0$ 和 $\\delta\\in(0,1)$\n- 函数定义：$f(x) = \\begin{cases} \\alpha x^{2},  |x|\\le \\delta \\\\ \\alpha \\delta^{2} + 2\\alpha \\delta(|x|-\\delta),  \\delta  |x|\\le 1 \\end{cases}$\n- 目标均匀误差：$\\epsilon\\in(0,1)$\n- 逼近网络：单隐层 ReLU 网络，其中 $\\sigma(t)=\\max\\{0,t\\}$。\n- 网络属性：该网络在 $[-1,1]$ 上实现连续分段线性函数，其线性片段的数量最多为隐藏神经元的数量加 1。\n- 目标：找到一个函数 $U(\\epsilon,\\alpha,\\delta)$，使得存在一个宽度至多为 $U(\\epsilon,\\alpha,\\delta)$ 的网络，其输出 $g$ 满足 $\\sup_{x\\in[-1,1]} |f(x)-g(x)| \\le \\epsilon$。\n- 方法约束：仅使用关于二次可微函数的线性样条插值误差的基本性质，以及单隐层 ReLU 网络的表示属性。\n- 最终答案约束：$U(\\epsilon,\\alpha,\\delta)$ 的单个封闭形式表达式，不含不等式或取整函数。\n\n**验证结论：**\n该问题是**有效的**。这是一个神经网络逼近理论中适定的问题。函数 $f(x)$ 是连续且连续可微的，其二阶导数是分段常数，这是此类分析的标准设置。所有术语在数学上都是精确的，目标定义清晰。对方法论和最终答案格式的约束是严格但可行的。该问题具有科学依据且是客观的。\n\n### 步骤 2：解法推导\n策略是构建一个连续分段线性（CPWL）函数 $g(x)$，以期望的精度 $\\epsilon$ 逼近 $f(x)$，然后确定 ReLU 网络表示 $g(x)$ 所需的神经元数量。\n\n函数 $f(x)$ 有两个不同的行为区域：\n1.  对于 $|x| \\in (\\delta, 1]$，函数定义为 $f(x) = \\alpha \\delta^{2} + 2\\alpha \\delta(|x|-\\delta)$。函数的这部分已经是分段线性的。具体来说，它在 $[-1, -\\delta)$ 和 $(\\delta, 1]$ 上是线性的。因此，一个 ReLU 网络可以精确表示函数的这部分，在这些区域不引入逼近误差。\n2.  对于 $|x| \\le \\delta$，函数由 $f(x) = \\alpha x^{2}$ 给出。这是需要逼近的非线性部分。我们将使用线性样条插值来逼近区间 $[-\\delta, \\delta]$ 上的这个二次函数。\n\n根据指示，我们使用线性样条插值的误差界。对于在区间 $[a, b]$ 上二次可微的函数 $h(x)$，其线性插值函数 $L(x)$ 的均匀误差由以下公式界定：\n$$ \\sup_{x \\in [a, b]} |h(x) - L(x)| \\le \\frac{1}{8} (b-a)^{2} \\sup_{t \\in [a, b]} |h''(t)| $$\n\n在我们的问题中，要逼近的函数是在区间 $[-\\delta, \\delta]$ 上的 $h(x) = \\alpha x^{2}$。其二阶导数是常数：\n$$ h''(x) = \\frac{d^2}{dx^2}(\\alpha x^2) = 2\\alpha $$\n由于 $\\alpha  0$，二阶导数绝对值在 $[-\\delta, \\delta]$ 上的上确界是 $M_2 = 2\\alpha$。\n\n为了在 $[-\\delta, \\delta]$ 上构建 CPWL 逼近函数 $g(x)$，我们将此区间划分为 $N$ 个等宽的子区间，其中 $N$ 是一个正整数。每个子区间的宽度为 $h_s = \\frac{\\delta - (-\\delta)}{N} = \\frac{2\\delta}{N}$。\n将误差公式应用于每个子区间，任何单个子区间上的最大误差为：\n$$ \\sup_{x \\in [x_i, x_{i+1}]} |f(x) - g(x)| \\le \\frac{1}{8} h_s^2 M_2 = \\frac{1}{8} \\left(\\frac{2\\delta}{N}\\right)^2 (2\\alpha) = \\frac{1}{8} \\frac{4\\delta^2}{N^2} (2\\alpha) = \\frac{\\alpha \\delta^2}{N^2} $$\n为确保在整个区间 $[-\\delta, \\delta]$ 上的均匀误差至多为 $\\epsilon$，我们需要：\n$$ \\frac{\\alpha \\delta^2}{N^2} \\le \\epsilon $$\n求解 $N$，我们得到所需的子区间数量：\n$$ N^2 \\ge \\frac{\\alpha \\delta^2}{\\epsilon} \\implies N \\ge \\sqrt{\\frac{\\alpha \\delta^2}{\\epsilon}} = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} $$\n\n接下来，我们确定在整个定义域 $[-1, 1]$ 上表示所得到的 CPWL 逼近函数 $g(x)$ 所需的隐藏神经元数量。一个 CPWL 函数可以由一个单隐层 ReLU 网络实现，其中神经元的数量等于“节点”（函数斜率改变的点）的数量。\n\n我们的逼近函数 $g(x)$ 的节点是：\n- 区间 $(-\\delta, \\delta)$ 内的 $N-1$ 个内部插值点。\n- 两个点 $x=-\\delta$ 和 $x=\\delta$。在这些点上，样条逼近的线性段与 $|x|\\delta$ 时 $f(x)$ 的线性部分相连接。斜率通常不匹配，从而产生两个额外的节点。\n\n节点总数为 $(N-1) + 2 = N+1$。因此，我们需要一个宽度（神经元数量）至少为 $W = N+1$ 的网络。\n\n将此与我们对 $N$ 的条件相结合：\n$$ W = N+1 \\ge \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1 $$\n由于神经元数量 $W$ 必须是整数，所需的最小宽度是满足此不等式的最小整数：\n$$ W_{\\text{min}} = \\left\\lceil \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1 \\right\\rceil $$\n问题要求找到一个函数 $U(\\epsilon, \\alpha, \\delta)$，使得存在一个宽度至多为 $U$ 的网络。这意味着我们必须找到一个 $U$ 使得 $W_{\\text{min}} \\le U$。$U$ 的表达式不能包含取整函数。\n\n我们可以利用对任何实数 $y$ 都有 $\\lceil y \\rceil \\le y + 1$ 的性质，找到 $W_{\\text{min}}$ 的一个简单的封闭形式上界。\n设 $y = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1$。那么：\n$$ W_{\\text{min}} = \\lceil y \\rceil \\le y + 1 = \\left(\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 1\\right) + 1 = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2 $$\n因此，我们可以选择我们的界定函数 $U(\\epsilon, \\alpha, \\delta)$ 为此表达式。对于一个宽度为 $W = W_{\\text{min}}$ 的网络，逼近误差保证至多为 $\\epsilon$，并且我们有 $W_{\\text{min}} \\le \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2$。因此，$U(\\epsilon,\\alpha,\\delta) = \\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2$ 是一个有效的选择。该函数是根据给定参数的封闭形式表达式，并满足所有问题约束。", "answer": "$$\\boxed{\\delta\\sqrt{\\frac{\\alpha}{\\epsilon}} + 2}$$", "id": "3194159"}, {"introduction": "我们已经看到ReLU网络如何通过分段线性函数来处理曲率，现在让我们转向另一种常见函数——分段常数函数。在此练习中，我们将使用经典的S型（Sigmoid）激活函数，并展示如何将其视为一个“软开关”。通过巧妙地组合这些软开关，我们可以构建出具有任意数量平台的函数，这进一步证明了从简单构建块构造复杂函数是通用近似的核心思想。[@problem_id:3194212]", "problem": "考虑一个单隐层人工神经网络 (ANN)，其隐层使用逻辑S型激活函数 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$，并具有线性输出。该网络接受一个标量输入 $x \\in [0,1]$ 并输出一个标量。设 $f:[0,1]\\to\\mathbb{R}$ 是一个具有 $k$ 个平台区的分段常数函数，其断点为 $0 = s_{0}  s_{1}  \\dots  s_{k-1}  s_{k} = 1$，平台水平为 $h_{1}, h_{2}, \\dots, h_{k}$，其中每个 $h_{i} \\in [0,1]$。假设 $f$ 的真实过渡仅发生在内部断点的 $\\delta$-邻域内，即 $f$ 在每个区间 $[s_{i-1}+\\frac{\\delta}{2},\\,s_{i}-\\frac{\\delta}{2}]$ 上是常数，并且仅在每个区间 $(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2})$ 内发生变化，其中 $i \\in \\{1,\\dots,k-1\\}$。\n\n要求您通过一个形如下式的单隐层ANN构造一个显式近似 $\\hat{f}$：\n$$\n\\hat{f}(x) = c_{0} + \\sum_{j=1}^{N} c_{j}\\,\\sigma\\!\\big(\\alpha(x - b_{j})\\big),\n$$\n其中 $N$ 是隐藏神经元的数量，$c_{0}, c_{1}, \\dots, c_{N} \\in \\mathbb{R}$ 是输出层权重（$c_{0}$ 是偏置），$b_{1},\\dots,b_{N} \\in [0,1]$ 是定位过渡的隐层偏置，而 $\\alpha  0$ 是一个控制平滑过渡宽度的公共陡峭度参数。\n\n在将 $b_{j}$ 选在内部断点处，并选择合适的 $c_{j}$ 和 $c_{0}$ 来匹配平台水平的条件下，通过选择一个合适的 $\\alpha$ 值以产生宽度至多为 $\\delta$ 的平滑过渡，来强制执行以下均匀误差约束：\n$$\n\\sup_{x \\in [0,1]\\setminus \\bigcup_{i=1}^{k-1}\\left(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2}\\right)}\\big|\\,\\hat{f}(x)-f(x)\\,\\big| \\le \\epsilon,\n$$\n\n从逻辑S型函数和 Heaviside 阶跃函数的定义出发，并且仅使用关于单隐层ANN函数复合和线性组合的经过充分验证的事实，推导出一个显式构造和误差界，并确定在远离过渡邻域的情况下，用上述均匀误差约束实现 $k$ 个平台区所需隐藏神经元的最小数量 $N$。请将 $N$ 表示为 $k$ 的函数。\n\n您的最终答案必须是 $N$ 关于 $k$ 的单个闭式表达式。无需四舍五入，也不涉及任何物理单位。", "solution": "我们从逻辑S型函数的定义 $\\sigma(z) = \\frac{1}{1+\\exp(-z)}$ 开始。Heaviside 阶跃函数 $H(x - s)$（当 $x  s$ 时等于 $0$，当 $x \\ge s$ 时等于 $1$）可以用平滑的S型函数 $\\sigma(\\alpha(x - s))$ 来近似，其中 $\\alpha  0$ 控制陡峭度，从而控制过渡宽度。当 $\\alpha \\to \\infty$ 时，对于所有 $x \\neq s$，$\\sigma(\\alpha(x - s))$ 逐点收敛到 $H(x - s)$。\n\n一个具有断点 $s_{1},\\dots,s_{k-1}$ 和平台高度 $h_{1},\\dots,h_{k}$ 的分段常数函数可以使用 Heaviside 阶跃函数精确表示为：\n$$\nf(x) = h_{1} + \\sum_{i=1}^{k-1}\\big(h_{i+1}-h_{i}\\big)\\,H(x - s_{i}).\n$$\n为了验证这个恒等式，注意到对于任何 $x \\in [s_{m-1}, s_{m})$ 且 $m \\in \\{1,\\dots,k\\}$，我们有当 $i \\ge m$ 时 $H(x - s_{i}) = 0$，当 $i  m$ 时 $H(x - s_{i}) = 1$，所以\n$$\nf(x) = h_{1} + \\sum_{i=1}^{m-1}\\big(h_{i+1}-h_{i}\\big) = h_{m},\n$$\n这是通过裂项求和得到的。\n\n为了获得单隐层ANN近似，我们将 $H(x - s_{i})$ 替换为 $\\sigma\\!\\big(\\alpha(x - s_{i})\\big)$ 并定义\n$$\n\\hat{f}(x) = c_{0} + \\sum_{i=1}^{k-1} c_{i}\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big),\n$$\n其中系数选择为 $c_{0} = h_{1}$ 和 $c_{i} = h_{i+1} - h_{i}$，对于 $i \\in \\{1,\\dots,k-1\\}$。这得到\n$$\n\\hat{f}(x) = h_{1} + \\sum_{i=1}^{k-1}\\big(h_{i+1}-h_{i}\\big)\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big).\n$$\n为了在远离过渡区的地方强制执行均匀误差界，我们必须界定每个S型函数在其理想阶跃值之外的偏差，即在 $(s_{i}-\\frac{\\delta}{2},\\,s_{i}+\\frac{\\delta}{2})$ 之外。\n\n固定一个内部断点 $s$，并考虑 $x \\le s - \\frac{\\delta}{2}$。那么 $\\alpha(x - s) \\le -\\alpha\\frac{\\delta}{2}$，所以\n$$\n\\sigma\\!\\big(\\alpha(x - s)\\big) \\le \\sigma\\!\\big(-\\alpha\\frac{\\delta}{2}\\big) = \\frac{1}{1+\\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big)}.\n$$\n类似地，对于 $x \\ge s + \\frac{\\delta}{2}$，我们有\n$$\n\\sigma\\!\\big(\\alpha(x - s)\\big) \\ge \\sigma\\!\\big(\\alpha\\frac{\\delta}{2}\\big) = \\frac{1}{1+\\exp\\!\\big(-\\alpha\\frac{\\delta}{2}\\big)}.\n$$\n因此，如果我们要求对于每个过渡\n$$\n\\sigma\\!\\big(-\\alpha\\frac{\\delta}{2}\\big) \\le \\varepsilon' \\quad\\text{and}\\quad \\sigma\\!\\big(\\alpha\\frac{\\delta}{2}\\big) \\ge 1 - \\varepsilon',\n$$\n那么在 $s$ 的 $\\delta$-邻域之外，S型函数在 $\\varepsilon'$ 范围内均匀地近似阶跃函数。两个不等式都等价于对 $\\alpha$ 的相同约束：\n\n$$\n\\frac{1}{1+\\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big)} \\le \\varepsilon' \\quad\\Longleftrightarrow\\quad \\exp\\!\\big(\\alpha\\frac{\\delta}{2}\\big) \\ge \\frac{1-\\varepsilon'}{\\varepsilon'} \\quad\\Longleftrightarrow\\quad \\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\varepsilon'}{\\varepsilon'}\\Big).\n$$\n\n因此，选择\n$$\n\\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\varepsilon'}{\\varepsilon'}\\Big)\n$$\n可以保证每个S型函数在相应的 $\\delta$-邻域之外，与它的理想阶跃值的差距在 $\\varepsilon'$ 以内。\n\n我们现在界定 $\\hat{f}$ 在远离所有过渡邻域的并集处的均匀误差。对于任何远离过渡区的 $x$，每个 $\\sigma\\!\\big(\\alpha(x - s_{i})\\big)$ 与其理想的 $H(x - s_{i})$ 值的差异至多为 $\\varepsilon'$。使用三角不等式，\n$$\n\\big|\\,\\hat{f}(x) - f(x)\\,\\big| \\le \\sum_{i=1}^{k-1} |c_{i}|\\,\\big|\\,\\sigma\\!\\big(\\alpha(x - s_{i})\\big) - H(x - s_{i})\\,\\big| \\le \\varepsilon' \\sum_{i=1}^{k-1} |h_{i+1}-h_{i}|.\n$$\n令跨断点的总变差为\n$$\nV \\equiv \\sum_{i=1}^{k-1} |h_{i+1}-h_{i}|.\n$$\n为了强制执行目标均匀误差 $\\epsilon$，只需设置\n$$\n\\varepsilon' = \\frac{\\epsilon}{V} \\quad \\text{if } V  0,\n$$\n并约定如果 $V = 0$（即函数是常数，也即 $k=1$），则不需要隐藏神经元，且当 $c_{0} = h_{1}$ 时近似是精确的。\n\n因为每个 $h_{i} \\in [0,1]$，一个最坏情况下的界是 $V \\le k-1$，当平台区在0和1之间交替时达到。因此，对于 $k \\ge 2$，一个保守的选择是 $\\varepsilon' = \\frac{\\epsilon}{k-1}$，这导出一个充分的陡峭度\n$$\n\\alpha \\ge \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{1-\\frac{\\epsilon}{k-1}}{\\frac{\\epsilon}{k-1}}\\Big) = \\frac{2}{\\delta}\\,\\ln\\!\\Big(\\frac{k-1-\\epsilon}{\\epsilon}\\Big).\n$$\n这保证了在远离过渡邻域的情况下满足均匀误差约束。\n\n我们现在计算所需隐藏神经元的最小数量 $N$。上述构造性表示对每个内部断点 $s_i$ 使用一个S型函数，即 $N = k-1$，以及输出偏置 $c_{0}=h_{1}$。为了论证其最小性，注意到对于一个每个神经元都使用单调激活函数的单隐层网络，$\\hat{f}$ 对 $x$ 的导数可以写为\n$$\n\\frac{d\\hat{f}}{dx}(x) = \\alpha \\sum_{i=1}^{N} c_{i}\\,\\sigma\\!\\big(\\alpha(x - b_{i})\\big)\\,\\big(1 - \\sigma\\!\\big(\\alpha(x - b_{i})\\big)\\big),\n$$\n这是一个由 $N$ 个中心在 $b_i$ 附近的局域化、单峰的凸起之和。一个具有 $k$ 个平台区且仅在 $(k-1)$ 个不相交的 $\\delta$-邻域内变化的函数，必须表现出 $(k-1)$ 次局域化的值变化，因此其导数在远离平坦区域的地方至少有 $(k-1)$ 个局域化的凸起。由于每个隐藏神经元最多贡献一个这样的局域化凸起，任何实现都必须满足 $N \\ge k-1$。将这个下界与我们给出的达到 $N = k-1$ 的显式构造相结合，我们得出结论，所需隐藏神经元的最小数量是\n$$\nN(k) = k - 1.\n$$\n\n因此，使用带有逻辑S型激活函数和线性输出的单隐层ANN，在 $[0,1]$ 上构建具有 $k$ 个平台区、过渡宽度为 $\\delta$、且在远离过渡区处的均匀误差至多为 $\\epsilon$ 的函数，所需隐藏神经元的最小数量是 $k-1$。", "answer": "$$\\boxed{k-1}$$", "id": "3194212"}, {"introduction": "理论的理解最好通过实践来巩固。在这个最终的动手挑战中，你将编写代码来近似一个具有跳跃间断点的函数。这项任务不仅能让你亲手构建一个ReLU近似器，还将揭示通用近似定理的一个重要细微之处：对于不连续函数，虽然无法实现均匀收敛（即$L^\\infty$误差无法趋近于零），但我们仍然可以在$L^p$积分意义下达到任意高的精度。[@problem_id:3194151]", "problem": "要求您用代码形式化并测试一个 $L^p$ 近似实验，该实验在一个紧凑区间上，从 $L^p$ 收敛的意义上阐释了深度学习的通用近似定理 (UAT)。通用近似定理 (UAT) 指出，具有至少一个隐藏层和适当激活函数的前馈神经网络，可以在 $\\mathbb{R}^n$ 的紧凑子集上以任意精度近似任何连续函数。本实验研究一个不连续的目标函数，并演示了如何通过一个在不连续点周围引入一个狭窄过渡区（边界层）的网络来进行 $L^p$ 近似。本任务的科学基础仅限于以下内容：$L^p$ 范数的定义、积分的基本性质，以及带有修正线性单元 (ReLU) 激活函数的单隐藏层前馈网络的定义。\n\n定义目标函数 $f:[0,1]\\to\\mathbb{R}$ 为 $f(x)=\\mathbf{1}_{\\{x1/2\\}}$，其中 $\\mathbf{1}_{\\{x1/2\\}}$ 表示集合 $\\{x1/2\\}$ 的指示函数。考虑一族一维单隐藏层网络 $g_\\delta:[0,1]\\to\\mathbb{R}$，该网络使用修正线性单元 (ReLU) 激活函数，由一个正实数 $\\delta\\in(0,1/2)$ 参数化，它在以 $x=1/2$ 为中心、宽度为 $2\\delta$ 的边界层上实现一个线性斜坡，并在此层之外与 $f(x)$ 完全匹配。具体来说，隐藏单元计算 $h_1(x)=\\max\\{0,x-(1/2-\\delta)\\}$ 和 $h_2(x)=\\max\\{0,x-(1/2+\\delta)\\}$，输出为 $g_\\delta(x)=\\frac{h_1(x)-h_2(x)}{2\\delta}$。\n\n对于给定的 $p\\in[1,\\infty]$，将 $f$ 和 $g_\\delta$ 之间的 $L^p$ 误差定义如下。对于有限的 $p$， $L^p$ 误差为 $\\|f-g_\\delta\\|_{L^p([0,1])}=\\left(\\int_0^1 |f(x)-g_\\delta(x)|^p\\,dx\\right)^{1/p}$。对于 $p=\\infty$，将 $L^\\infty$ 误差定义为 $\\|f-g_\\delta\\|_{L^\\infty([0,1])}=\\sup_{x\\in[0,1]}|f(x)-g_\\delta(x)|$。在本实验中，对于有限的 $p$，应使用均匀网格上的数值积分来近似该积分；对于 $p=\\infty$，应使用均匀网格搜索。\n\n您的任务是：\n- 按照规定实现网络 $g_\\delta(x)$。\n- 对每个测试用例，使用 $[0,1]$ 上的均匀网格，对于有限的 $p$ 使用梯形法则，对于 $p=\\infty$ 使用最大绝对差，来计算 $\\|f-g_\\delta\\|_{L^p([0,1])}$ 的数值近似值。\n- 对每个测试用例，输出一个布尔值，表示计算出的误差是否小于或等于给定的容差 $\\epsilon$。\n\n使用以下参数值 $(p,\\epsilon,\\delta)$ 的测试套件：\n1. $(p,\\epsilon,\\delta)=\\left(1,\\,0.1,\\,0.1\\right)$,\n2. $(p,\\epsilon,\\delta)=\\left(1,\\,0.01,\\,0.01\\right)$,\n3. $(p,\\epsilon,\\delta)=\\left(2,\\,0.05,\\,0.0135\\right)$,\n4. $(p,\\epsilon,\\delta)=\\left(4,\\,0.05,\\,2.25\\times 10^{-4}\\right)$,\n5. $(p,\\epsilon,\\delta)=\\left(2,\\,0.05,\\,0.05\\right)$,\n6. $(p,\\epsilon,\\delta)=\\left(\\infty,\\,0.4,\\,0.1\\right)$,\n7. $(p,\\epsilon,\\delta)=\\left(\\infty,\\,0.5,\\,0.1\\right)$.\n\n在用例 1 到 4 中，边界层宽度 $\\delta$ 被选择为相对于 $\\epsilon$ 较小的值，因此 $L^p$ 误差预计将小于或等于 $\\epsilon$。用例 5 是一个特意设置的错配选项，用于测试当 $p1$ 时，$\\delta$ 按 $\\epsilon$ 而不是 $\\epsilon^p$ 比例缩放时的失败情况。用例 6 和 7 探究 $L^\\infty$ 行为；由于在 $x=1/2$ 处的跳跃不连续性，连续的斜坡函数无法将 $L^\\infty$ 误差降低到某个阈值以下。\n\n您的程序必须：\n- 在 $[0,1]$ 上构建均匀网格并执行所需的数值计算。\n- 生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，例如 $\\left[\\text{result1},\\text{result2},\\dots\\right]$，其中每个条目都是一个布尔值。\n\n不允许用户输入。不涉及物理单位。所有角度（如果出现）都将以弧度为单位，但此处不需要角度。", "solution": "问题陈述提出了一个有效且定义明确的数值实验，用以研究在 $L^p$ 范数下，使用简单神经网络对不连续函数进行近似的问题。其中函数、误差度量和数值程序都得到了足够清晰和数学上严谨的说明，从而可以得出一个唯一且可验证的解。其基本原理源于实分析和神经网络理论。因此，我们将着手提供一个完整的解法。\n\n目标是为一组参数计算 $L^p$ 误差，并验证该误差是否在给定的容差 $\\epsilon$ 之内。这可以作为与通用近似定理相关概念的一个实际例证，特别是该定理在积分范数意义下（而非一致范数）对不连续函数近似的扩展。\n\n首先，我们在紧凑区间 $[0,1]$ 上定义目标函数 $f(x)$。它是一个在 $x=1/2$ 处具有不连续性的阶跃函数：\n$$\nf(x) = \\mathbf{1}_{\\{x1/2\\}} =\n\\begin{cases}\n0  \\text{if } x \\le 1/2 \\\\\n1  \\text{if } x  1/2\n\\end{cases}\n$$\n\n接着，我们定义近似函数 $g_\\delta(x)$，它是一个带有修正线性单元 (ReLU) 激活函数的单隐藏层神经网络的输出。ReLU 函数定义为 $\\text{ReLU}(z) = \\max\\{0, z\\}$。该网络由 $\\delta \\in (0, 1/2)$ 参数化，该参数控制不连续点周围过渡区域的宽度。两个隐藏单元为：\n$$\nh_1(x) = \\max\\{0, x - (1/2 - \\delta)\\} \\quad \\text{and} \\quad h_2(x) = \\max\\{0, x - (1/2 + \\delta)\\}\n$$\n网络的输出是隐藏单元激活的加权和：\n$$\ng_\\delta(x) = \\frac{1}{2\\delta} h_1(x) - \\frac{1}{2\\delta} h_2(x) = \\frac{h_1(x) - h_2(x)}{2\\delta}\n$$\n通过分析 ReLU 函数的行为，我们可以将 $g_\\delta(x)$ 表示为分段线性函数：\n$$\ng_\\delta(x) =\n\\begin{cases}\n0  \\text{if } x \\le 1/2 - \\delta \\\\\n\\frac{x - 1/2 + \\delta}{2\\delta}  \\text{if } 1/2 - \\delta  x \\le 1/2 + \\delta \\\\\n1  \\text{if } x  1/2 + \\delta\n\\end{cases}\n$$\n该函数在区间 $[1/2 - \\delta, 1/2 + \\delta]$ 上创建了一个从 $0$到 $1$ 的线性斜坡，并在这个宽度为 $2\\delta$ 的“边界层”之外与 $f(x)$ 完全匹配。\n\n近似误差由差函数 $f(x) - g_\\delta(x)$ 的 $L^p$ 范数来衡量。对于有限的 $p \\in [1, \\infty)$，误差为：\n$$\n\\|f - g_\\delta\\|_{L^p([0,1])} = \\left( \\int_0^1 |f(x) - g_\\delta(x)|^p \\,dx \\right)^{1/p}\n$$\n对于 $p = \\infty$，误差为上确界范数：\n$$\n\\|f - g_\\delta\\|_{L^\\infty([0,1])} = \\sup_{x \\in [0,1]} |f(x) - g_\\delta(x)|\n$$\n差值 $f(x) - g_\\delta(x)$ 仅在区间 $(1/2 - \\delta, 1/2 + \\delta)$ 上非零。我们可以解析地计算 $L^p$ 误差的积分。误差的 $p$ 次方（积分部分）是：\n$$\nE_p^p = \\int_{1/2-\\delta}^{1/2+\\delta} |f(x) - g_\\delta(x)|^p \\,dx = \\int_{1/2-\\delta}^{1/2} |0 - g_\\delta(x)|^p \\,dx + \\int_{1/2}^{1/2+\\delta} |1 - g_\\delta(x)|^p \\,dx\n$$\n代入 $g_\\delta(x)$ 的表达式并进行积分，得到：\n$$\nE_p^p = \\int_{1/2-\\delta}^{1/2} \\left(\\frac{x - 1/2 + \\delta}{2\\delta}\\right)^p \\,dx + \\int_{1/2}^{1/2+\\delta} \\left(1 - \\frac{x - 1/2 + \\delta}{2\\delta}\\right)^p \\,dx\n$$\n变量替换表明两个积分相等。总积分的计算结果为：\n$$\nE_p^p = 2 \\int_{1/2-\\delta}^{1/2} \\left(\\frac{x - 1/2 + \\delta}{2\\delta}\\right)^p \\,dx = 2 \\cdot (2\\delta) \\int_0^{1/2} u^p \\,du = 4\\delta \\left[ \\frac{u^{p+1}}{p+1} \\right]_0^{1/2} = \\frac{4\\delta}{p+1} \\left(\\frac{1}{2}\\right)^{p+1} = \\frac{\\delta}{(p+1)2^{p-1}}\n$$\n因此，解析的 $L^p$ 误差为：\n$$\n\\|f - g_\\delta\\|_{L^p([0,1])} = \\left( \\frac{\\delta}{(p+1)2^{p-1}} \\right)^{1/p}\n$$\n这个有力的结果表明，对于任何有限的 $p$，通过选择足够小的 $\\delta$，误差可以变得任意小。具体来说，要实现 $\\|f - g_\\delta\\|_{L^p} \\le \\epsilon$，我们需要 $\\delta \\propto \\epsilon^p$。\n\n对于 $L^\\infty$ 的情况，我们求 $|f(x) - g_\\delta(x)|$ 的上确界。差值在不连续点 $x=1/2$ 处达到最大。在该点，$f(1/2) = 0$ 且 $g_\\delta(1/2) = \\frac{1/2 - 1/2 + \\delta}{2\\delta} = 1/2$。绝对差为 $|0 - 1/2| = 1/2$。回顾函数 $|f(x) - g_\\delta(x)|$ 在区间 $(1/2-\\delta, 1/2+\\delta)$ 上的行为，可以确认其最大值确实是 $1/2$。\n$$\n\\|f - g_\\delta\\|_{L^\\infty([0,1])} = \\frac{1}{2}\n$$\n值得注意的是，这个误差是一个常数 $1/2$，与 $\\delta$ 无关。这表明，虽然对于有限的 $p$，$g_\\delta$ 可以在 $L^p$ 意义上近似 $f$，但它并不能一致收敛。连续函数序列 $\\{g_\\delta\\}$ 无法一致地近似一个不连续函数，这是分析学中的一个基本结果。\n\n实现将以数值方式执行这些计算。对于有限的 $p$，将在 $[0,1]$ 上生成一个均匀点网格，在该网格上计算被积函数 $|f(x) - g_\\delta(x)|^p$，并使用梯形法则来近似积分。对于 $p=\\infty$，将在同一个网格上计算最大绝对差。然后将得到的数值误差与给定的容差 $\\epsilon$进行比较。必须使用足够精细的网格以确保数值近似的准确性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the L^p-error for a set of test cases to illustrate\n    the Universal Approximation Theorem for a discontinuous function.\n    \"\"\"\n    \n    # Test cases: (p, epsilon, delta)\n    test_cases = [\n        (1, 0.1, 0.1),\n        (1, 0.01, 0.01),\n        (2, 0.05, 0.0135),\n        (4, 0.05, 2.25e-4),\n        (2, 0.05, 0.05),\n        (np.inf, 0.4, 0.1),\n        (np.inf, 0.5, 0.1),\n    ]\n\n    results = []\n    \n    # Use a fine grid for numerical accuracy. N-1 intervals.\n    # N = 10**6 + 1 is a large number of points (~1 million).\n    N = 10**6 + 1\n    x = np.linspace(0.0, 1.0, N)\n\n    def target_function(x_vals):\n        \"\"\"\n        Computes f(x) = 1_{x  1/2}.\n        \"\"\"\n        return (x_vals  0.5).astype(float)\n\n    def network_approximator(x_vals, delta):\n        \"\"\"\n        Computes g_delta(x) = (h1(x) - h2(x)) / (2*delta).\n        \"\"\"\n        h1 = np.maximum(0, x_vals - (0.5 - delta))\n        h2 = np.maximum(0, x_vals - (0.5 + delta))\n        # Handle the case where delta is extremely small to avoid division by zero,\n        # although the problem constraints ensure delta  0.\n        if delta == 0:\n            # The limit of g_delta as delta - 0 is the step function f.\n            return target_function(x_vals)\n        return (h1 - h2) / (2.0 * delta)\n\n    # Evaluate the target function on the grid once.\n    f_vals = target_function(x)\n\n    for p, epsilon, delta in test_cases:\n        # Evaluate the network approximator for the given delta.\n        g_vals = network_approximator(x, delta)\n        \n        # Calculate the absolute difference.\n        abs_diff = np.abs(f_vals - g_vals)\n        \n        error = 0.0\n        if np.isinf(p):\n            # For p = infinity, compute the L-infinity norm (supremum norm).\n            # This is approximated by the maximum absolute difference on the grid.\n            error = np.max(abs_diff)\n        else:\n            # For finite p, compute the L^p norm via numerical integration.\n            # 1. Compute the integrand |f(x) - g_delta(x)|^p.\n            integrand = abs_diff**p\n            # 2. Integrate using the trapezoidal rule.\n            # np.trapz(y, x) computes the integral of y(x) dx.\n            integral_val = np.trapz(integrand, x)\n            # 3. Take the p-th root to get the L^p norm.\n            error = integral_val**(1.0 / p)\n            \n        # Compare the computed error with the tolerance epsilon.\n        results.append(error = epsilon)\n\n    # Print results in the specified format. The `str` of a boolean\n    # is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3194151"}]}