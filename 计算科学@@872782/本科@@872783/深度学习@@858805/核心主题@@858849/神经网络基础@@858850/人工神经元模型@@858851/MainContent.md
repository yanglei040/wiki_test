## 引言
作为现代人工智能和深度学习的基石，[人工神经网络](@entry_id:140571)（ANN）的强大能力源于其最基本的构成单元：人工神经元。在对[神经网](@entry_id:276355)络有一个宏观的认识之后，我们必须深入其内部，理解这个核心组件的运作机制。本文旨在填补这一知识鸿沟，从最基础的层面彻底剖析人工神经元，揭示其简单结构背后蕴含的深刻计算原理和广泛应用潜力。

本文将带领读者踏上一段从理论到实践的探索之旅，分为三个核心章节。在“**原理与机制**”中，我们将从数学定义出发，详细解析神经元的线性部分、[激活函数](@entry_id:141784)的[非线性](@entry_id:637147)魔法，以及参数初始化、[数据归一化](@entry_id:265081)和正则化等确保其有效学习的关键技术。接着，在“**应用与跨学科联系**”中，我们将视野扩展到更广阔的领域，展示这个看似简单的模型如何作为逻辑单元、科学建模工具和[统计决策](@entry_id:170796)实体，在物理学、生物学、神经科学乃至[统计物理学](@entry_id:142945)中发挥着至关重要的作用。最后，“**动手实践**”部分将通过精心设计的编程练习，帮助您将理论知识转化为实际技能。

通过本章的学习，您将不仅掌握人工神经元的“是什么”和“怎么做”，更能深刻理解其“为什么”如此设计，为进一步探索复杂的[深度学习](@entry_id:142022)世界打下坚实的基础。让我们从这个小而强大的单元开始，揭开智能计算的奥秘。

## 原理与机制

继前一章对[人工神经网络](@entry_id:140571)的宏观介绍之后，本章将深入剖析其最基本的构成单元——人工神经元。我们将从其数学定义出发，逐步揭示其核心组件（权重与偏置）的几何意义、[激活函数](@entry_id:141784)的关键作用，以及在学习系统中影响其行为的各种机制，如初始化、[数据预处理](@entry_id:197920)和正则化。本章旨在为您构建一个关于神经元如何工作、为何如此设计以及如何有效驾驭其行为的坚实理论基础。

### 神经元的基础模型：一个[线性分类器](@entry_id:637554)

在数学上，一个人工神经元首先对其输入向量 $\boldsymbol{x} \in \mathbb{R}^d$ 进行[仿射变换](@entry_id:144885)，计算出一个称为**对数（logit）**或**预激活值（pre-activation）**的标量 $z$。这个过程由权重向量 $\boldsymbol{w} \in \mathbb{R}^d$ 和偏置标量 $b \in \mathbb{R}$ 定义：

$$
z = \boldsymbol{w}^\top \boldsymbol{x} + b = \sum_{i=1}^d w_i x_i + b
$$

这里的权重 $w_i$衡量了第 $i$ 个输入特征 $x_i$ 的重要性，而偏置 $b$ 则是一个独立于任何输入的基准项。这个线性计算构成了神经元的核心。对于最简单的线性[分类任务](@entry_id:635433)，神经元的输出可以直接由 $z$ 的符号决定，即 $\mathrm{sign}(z)$。

#### 几何解释：[决策边界](@entry_id:146073)与偏置的角色

这个简单的线性模型具有深刻的几何意义。方程 $\boldsymbol{w}^\top \boldsymbol{x} + b = 0$ 在 $d$ 维输入空间中定义了一个**[超平面](@entry_id:268044)（hyperplane）**。这个超平面就是神经元的**决策边界**，它将输入空间分割成两个半空间：一个对应 $z > 0$（正类），另一个对应 $z < 0$（负类）。权重向量 $\boldsymbol{w}$ 的方向决定了该超平面的**法线方向**，即其朝向。

那么，偏置 $b$ 的作用是什么呢？它决定了[决策边界](@entry_id:146073)相对于原点的位置或**偏移量**。我们可以通过一个思想实验来理解这一点 [@problem_id:3190756]。假设我们对整个数据集进行一个平移变换，即每个数据点 $\boldsymbol{x}$ 都被移动到一个新的位置 $\boldsymbol{x}' = \boldsymbol{x} + \boldsymbol{t}$，其中 $\boldsymbol{t}$ 是一个固定的平移向量。为了保持对每个数据点的分类结果不变，我们需要找到新的参数 $(\boldsymbol{w}', b')$，使得新的决策函数值与旧的决策函数值保持一致。

原始的对数值为 $z = \boldsymbol{w}^\top \boldsymbol{x} + b$。用新坐标 $\boldsymbol{x}'$ 表示旧坐标 $\boldsymbol{x} = \boldsymbol{x}' - \boldsymbol{t}$，我们得到：

$$
z = \boldsymbol{w}^\top (\boldsymbol{x}' - \boldsymbol{t}) + b = \boldsymbol{w}^\top \boldsymbol{x}' - \boldsymbol{w}^\top \boldsymbol{t} + b = \boldsymbol{w}^\top \boldsymbol{x}' + (b - \boldsymbol{w}^\top \boldsymbol{t})
$$

这个结果表明，要在一个平移了的数据集上重现相同的决策几何，我们只需保持权重向量 $\boldsymbol{w}$ 不变（即 $\boldsymbol{w}' = \boldsymbol{w}$），并将偏置更新为 $b' = b - \boldsymbol{w}^\top \boldsymbol{t}$。这直观地说明了偏置 $b$ 的作用是补偿由于数据整体偏移（或特征的内在均值）所带来的影响。因为权重向量 $\boldsymbol{w}$ 没有改变，决策超平面的**方向**保持不变，改变的仅仅是它的位置。

一种常见的处理方式是将偏置 $b$ 视为一个特殊的权重。我们可以通过在输入向量 $\boldsymbol{x}$ 中增加一个恒为 $1$ 的维度 $x_0$ 来实现这一点，即增广输入向量 $\tilde{\boldsymbol{x}} = (1, x_1, \dots, x_d)^\top$。相应地，增广权重向量为 $\tilde{\boldsymbol{w}} = (b, w_1, \dots, w_d)^\top$。这样，原始的[仿射变换](@entry_id:144885)就变成了一个纯粹的[线性变换](@entry_id:149133)（[内积](@entry_id:158127)）：$z = \tilde{\boldsymbol{w}}^\top \tilde{\boldsymbol{x}}$。在这种视角下，偏置 $b$ 就是与恒定输入 $x_0=1$ 相关联的权重。

#### 偏置与特征均值的关系

偏置的作用与数据特征的统计特性密切相关 [@problem_id:3180438]。在一个理想的、经过优化的分类器中，偏置 $b$ 的值通常会调整以抵消数据特征的全局平均效应。考虑一个二[分类问题](@entry_id:637153)，其中类别 $y=1$ 和 $y=0$ 的数据分别服从均值为 $\boldsymbol{\mu}_+$ 和 $\boldsymbol{\mu}_-$ 的高斯分布。可以证明，最优的线性[决策边界](@entry_id:146073)会穿过两个类别均值的中心点附近。

如果整个数据集的全局特征均值 $\mathbf{m} = \mathbb{E}[\mathbf{x}]$ 不为零（例如，所有特征都是正数），那么对数值 $\boldsymbol{w}^\top\boldsymbol{x}$ 在数据点上平均会有一个非零的偏移 $\boldsymbol{w}^\top\mathbf{m}$。为了将决策阈值置于正确的位置，最优偏置 $b^*$ 必须补偿这个偏移，通常会导致 $b^* \approx -\boldsymbol{w}^\top\mathbf{m}$。例如，如果特征均值 $\mathbf{m}$ 和权重 $\boldsymbol{w}$ 的分量大多是正数，那么最优偏置 $b^*$ 很可能是负数。

这引出了一个重要的[数据预处理](@entry_id:197920)步骤：**[特征中心化](@entry_id:634384)**。如果我们首先对数据进行中心化处理，即使用新的特征 $\boldsymbol{x}' = \boldsymbol{x} - \mathbf{m}$，那么新特征的均值就为零。在这种情况下，可以证明，在新[坐标系](@entry_id:156346)下训练的模型的最佳偏置 $b'$ 将会是零。原始模型的偏置效应被完全“吸收”到了[特征中心化](@entry_id:634384)的过程中。这不仅简化了模型，也说明了[数据预处理](@entry_id:197920)与模型参数之间的深刻联系。

### 激活函数：引入[非线性](@entry_id:637147)

如果仅仅将多个上述的线性神经元堆叠起来，其总体效果仍然是一个线性变换。为了让[神经网](@entry_id:276355)络能够学习和表示复杂的非线性关系（这是其强大能力的关键），我们在每个神经元的线性计算之后引入一个固定的[非线性](@entry_id:637147)函数，称为**激活函数（activation function）**，记为 $a(\cdot)$。神经元的最终输出（或称为**激活值**）为 $y = a(z) = a(\boldsymbol{w}^\top \boldsymbol{x} + b)$。

#### 常见[激活函数](@entry_id:141784)一览

历史上和现代[神经网](@entry_id:276355)络中使用了多种[激活函数](@entry_id:141784)，每种都有其独特的属性：

- **[Sigmoid函数](@entry_id:137244)**：$\sigma(z) = \frac{1}{1 + \exp(-z)}$。它将任意实数输入压缩到 $(0, 1)$ 区间，常被解释为概率。其“S”形曲线在两端会**饱和**，即当 $|z|$ 很大时，其导数趋近于零。
- **[双曲正切函数](@entry_id:634307)（Tanh）**：$\tanh(z) = \frac{\exp(z) - \exp(-z)}{\exp(z) + \exp(-z)}$。它与Sigmoid类似，但将输入压缩到 $(-1, 1)$ 区间。其输出是零中心的，这在某些情况下比Sigmoid更受欢迎。它同样存在饱和问题。
- **[修正线性单元](@entry_id:636721)（ReLU）**：$a(z) = \max(0, z)$。这是现代深度学习中最常用和默认的激活函数。它的计算非常简单，并且在正[数域](@entry_id:155558)（$z > 0$）上不会饱和，从而在很大程度上缓解了[梯度消失问题](@entry_id:144098)。
- **ReLU的平滑近似**：为了克服ReLU在 $z=0$ 处的“尖点”（导数不连续），研究者提出了多种平滑版本，如**Softplus** ($a(z) = \ln(1 + \exp(z))$) 和**[高斯误差线性单元](@entry_id:638032)（GELU）** ($a(z) = z \cdot \Phi(z)$，其中 $\Phi(z)$ 是标准[高斯分布](@entry_id:154414)的[累积分布函数](@entry_id:143135))。

#### 激活函数的性质与权衡

选择哪种[激活函数](@entry_id:141784)对网络的学习动态和性能有重大影响。

- **激活稀疏性（ReLU）**：ReLU的一个重要特性是其“单侧”抑制性。当其输入 $z \le 0$ 时，其输出为零。这意味着对于任何给定的输入，网络中只有一部分神经元是“激活”的（输出非零）。这种由输入数据驱动的**稀疏激活**被认为是其成功的一个原因 [@problem_id:3180377]。如果输入数据围绕[原点对称](@entry_id:172995)且神经元偏置为零，那么大约50%的神经元会被激活。

- **有界 vs. 无界激活（鲁棒性）**：激活函数的输出范围会影响模型[对异常值的鲁棒性](@entry_id:634485) [@problem_id:3180400]。像Tanh这样的**有界**激活函数，其输出被限制在一个小范围内（例如 $[-1, 1]$）。即使输入 $x$ 或其权重 $w$ 是一个极大的异常值，其输出 $\tanh(wx)$ 也不会超过 $1$。相比之下，像ReLU这样的**无界**[激活函数](@entry_id:141784)，其输出可以随输入线性增长。当面对来自[重尾分布](@entry_id:142737)的异常输入时，ReLU的输出可能变得非常大，导致损失函数的值也极大，从而对模型的训练造成干扰。因此，有界激活函数在存在大量噪声或异常值的场景下通常更为稳健。

- **平滑度与曲率**：[激活函数](@entry_id:141784)的平滑性对[基于梯度的优化](@entry_id:169228)有影响，尤其是对于[二阶优化](@entry_id:175310)方法 [@problem_id:3180369]。ReLU在 $z=0$ 处导数不连续，其[二阶导数](@entry_id:144508)几乎处处为零（除了在原点未定义）。这意味着损失函数的[曲面](@entry_id:267450)在大部分区域是平的或者是一个斜坡，缺乏曲率信息。相比之下，像Softplus这样的平滑函数，其[二阶导数](@entry_id:144508)处处为正。这意味着损失函数的[曲面](@entry_id:267450)是严格凸的（在简单模型中），为[牛顿法](@entry_id:140116)等[二阶优化](@entry_id:175310)算法提供了有用的曲率信息，从而可能实现更有效的优化。然而，在实践中，ReLU的简单性和计算效率往往胜过其非平滑性带来的理论问题。

#### 温度参数：一个统一的视角

我们可以通过引入一个“温度”参数 $\alpha > 0$ 来统一和探索不同[激活函数](@entry_id:141784)的行为，定义温度缩放的[激活函数](@entry_id:141784)为 $\phi_\alpha(z) = \phi(\alpha z)$ [@problem_id:3180391]。

- 当 $\alpha \to \infty$（低温极限）时，[激活函数](@entry_id:141784)的[非线性](@entry_id:637147)会变得非常**尖锐**。例如，[Sigmoid函数](@entry_id:137244) $\sigma(\alpha z)$ 会逐点收敛于一个**[Heaviside阶跃函数](@entry_id:275119)**（在$z>0$处为1，在$z0$处为0）。GELU函数 $g(\alpha z)$ 则会趋向于一个缩放的[ReLU函数](@entry_id:273016) $\alpha \cdot \max(0, z)$。这种尖锐的[非线性](@entry_id:637147)使得神经元像一个“硬”开关。
- 当 $\alpha \to 0$（高温极限）时，[激活函数](@entry_id:141784)会变得近似**线性**。例如，对于小的 $\alpha z$，$\sigma(\alpha z) \approx \frac{1}{2} + \frac{\alpha z}{4}$，而 $g(\alpha z) \approx \frac{\alpha z}{2}$。神经元的行为退化为近乎线性的。

这揭示了一个关键的权衡：较大的 $\alpha$ 带来了更强的[非线性](@entry_id:637147)区分能力，但其导数要么在大部分区域消失为零（如Sigmoid），要么在某些区域变得非常大（如GELU），导致梯度不稳定（**梯度消失或爆炸**）。而较小的 $\alpha$ 使梯度更稳定且[分布](@entry_id:182848)均匀，但削弱了[非线性](@entry_id:637147)能力，可能导致模型学习能力不足和训练缓慢。

### 神经元在学习系统中的角色

单个神经元只有在能够从数据中学习的系统中才有意义。这个学习过程通常是通过最小化一个**[损失函数](@entry_id:634569)**（如[交叉熵](@entry_id:269529)或均方误差）来实现的，而**梯度下降**及其变体是驱动这一过程的核心算法。

#### 梯度信号与更新规则

在训练过程中，我们需要计算损失函数 $L$ 相对于神经元参数（权重 $\boldsymbol{w}$ 和偏置 $b$）的梯度。根据[链式法则](@entry_id:190743)，损失对权重的梯度为：

$$
\nabla_{\boldsymbol{w}} L = \frac{\partial L}{\partial a} \frac{\partial a}{\partial z} \nabla_{\boldsymbol{w}} z = \frac{\partial L}{\partial a} a'(z) \boldsymbol{x}
$$

其中 $a'(z)$ 是[激活函数](@entry_id:141784)对预激活值 $z$ 的导数。这个公式揭示了梯度信号的三个组成部分：来自损失函数并流经网络更深层的误差信号 $\frac{\partial L}{\partial a}$，激活函数自身的局部梯度 $a'(z)$，以及该神经元的输入 $\boldsymbol{x}$。

以ReLU为例，其导数 $a'(z)$ 在 $z > 0$ 时为1，在 $z  0$ 时为0。因此，其权重梯度可以写为 $\nabla_{\boldsymbol{w}} L \propto \mathbb{I}\{z > 0\} \cdot \boldsymbol{x}$ [@problem_id:3180377]。这意味着：
1.  只有当神经元被**激活**（$z > 0$）时，其权重才会收到非零的梯度信号并进行更新。
2.  当神经元被激活时，权重的更新方向与输入向量 $\boldsymbol{x}$ 成正比。
3.  如果一个神经元的预激活值 $z$ 持续为负（例如，由于一个大的负偏置），它将永远不会被激活，其梯度将始终为零。这个神经元将停止学习，这种现象被称为“**[死亡ReLU](@entry_id:145121)**”。

#### 初始化：为学习设定舞台

参数的初始值对深度网络的训练至关重要。糟糕的初始化可能导致梯度在[反向传播](@entry_id:199535)过程中指数级消失或爆炸，从而使网络无法学习。一个好的初始化策略旨在使信号（激活值和梯度）在网络中传播时保持稳定的幅度（或[方差](@entry_id:200758)）。

我们可以通过分析梯度范数的期望来严格地理解这一点 [@problem_id:3180442]。考虑输出 $y=a(\boldsymbol{w}^\top \boldsymbol{x})$ 关于输入 $\boldsymbol{x}$ 的梯度 $\nabla_{\boldsymbol{x}} y = a'(z) \boldsymbol{w}$。其平方范数的[期望值](@entry_id:153208) $G = \mathbb{E}[\|\nabla_{\boldsymbol{x}} y\|^2]$ 可以看作是梯度信号在反向传播通过该层时被缩放的因子。为了保持梯度稳定，我们希望 $G \approx 1$。

通过数学推导，可以得到：
$$
G \approx n \sigma_w^2 \cdot \mathbb{E}_{Z \sim \mathcal{N}(0, n\sigma_w^2\sigma_x^2)}[(a'(Z))^2]
$$
其中 $n$ 是输入维度，$σ_w^2$ 是权重的初始[方差](@entry_id:200758)，$σ_x^2$ 是输入的[方差](@entry_id:200758)。

- 对于**ReLU**激活函数，$\mathbb{E}[(a'(Z))^2] = \frac{1}{2}$。为了使 $G=1$，我们需要 $n \sigma_w^2 \cdot \frac{1}{2} = 1$，即 $\sigma_w^2 = \frac{2}{n}$。这正是**[He初始化](@entry_id:634276)**的核心思想。
- 对于**Tanh**[激活函数](@entry_id:141784)，在 $z$ 接近0的[线性区](@entry_id:276444)域，$a'(z) \approx 1$。为了使 $G=1$，我们需要 $n \sigma_w^2 \approx 1$，即 $\sigma_w^2 = \frac{1}{n}$。这正是**Xavier（或Glorot）初始化**的核心思想。

这个分析为现代[神经网](@entry_id:276355)络中常用的初始化方案提供了坚实的理论依据，展示了如何通过精心选择权重的初始[方差](@entry_id:200758)来匹配激活函数的特性，从而确保有效的学习。

#### [数据预处理](@entry_id:197920)：改善学习动态

正如我们之前看到的，中心化可以简化偏置项。更普遍地，**输入归一化**（将每个特征减去其均值并除以其标准差）是改善学习动态的强大技术 [@problem_id:3180381]。

考虑逻辑回归的[损失函数](@entry_id:634569)，其Hessian矩阵（损失函数关于权重的[二阶导数](@entry_id:144508)矩阵）在训练初期可以近似为 $\boldsymbol{H} \approx \frac{1}{4} \mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top]$。Hessian矩阵的**条件数**（最大[特征值](@entry_id:154894)与最小特征值之比）决定了[损失函数](@entry_id:634569)[等高线](@entry_id:268504)的形状。一个很大的条件数意味着[等高线](@entry_id:268504)在某些方向上非常狭长，[梯度下降](@entry_id:145942)算法将在此类“峡谷”中缓慢[振荡](@entry_id:267781)。一个接近1的条件数意味着[等高线](@entry_id:268504)接近圆形，梯度下降会更直接地指向最小值。

当输入特征具有不同的尺度或存在相关性时，$\mathbb{E}[\boldsymbol{x}\boldsymbol{x}^\top]$ 的条件数可能很大。通过将输入归一化为 $\boldsymbol{z}$（使其均值为0，[协方差矩阵](@entry_id:139155)为相关系数矩阵），相应的Hessian矩阵变为 $\boldsymbol{H}_{\text{norm}} \approx \frac{1}{4} \mathbb{E}[\boldsymbol{z}\boldsymbol{z}^\top]$。可以证明，归一化后的Hessian矩阵的条件数通常远小于原始Hessian[矩阵的条件数](@entry_id:150947)。例如，在一个具有不同尺度和相关性的特征的数值实验中 [@problem_id:3180381]，归一化可以将[条件数](@entry_id:145150)降低数倍，从而显著加速训练过程。

### 正则化：控制[模型复杂度](@entry_id:145563)

当模型参数过多或训练数据有限时，[神经网](@entry_id:276355)络很容易**[过拟合](@entry_id:139093)**，即它记住了训练数据的细节和噪声，而不是学习到底层的普适规律。**正则化**是通过向[损失函数](@entry_id:634569)添加一个惩罚项来限制[模型复杂度](@entry_id:145563)的技术。

#### 偏置-[方差](@entry_id:200758)权衡

正则化的一个基本视角是**偏置-[方差](@entry_id:200758)权衡** [@problem_id:3180371]。在一个简单的[线性回归](@entry_id:142318)神经元中，我们可以分析**[岭回归](@entry_id:140984)（Ridge Regression）**的效果，它在[损失函数](@entry_id:634569)中增加了一个权重的**L2范数**平方的惩罚项：$\mathcal{L}(b,w) = \sum (y_i - (b+wx_i))^2 + \lambda w^2$。

与没有正则化的[普通最小二乘法](@entry_id:137121)相比，这个 $\lambda w^2$ 项会“拉动”权重 $w$ 朝向零。这会给模型的估计带来一些**偏置**（即估计值的期望偏离了真实值），但它同时显著降低了估计的**[方差](@entry_id:200758)**（即估计值在不同训练数据集上的波动）。对于合适的正则化强度 $\lambda$，[方差](@entry_id:200758)的减小所带来的好处超过了偏置增加所带来的坏处，从而降低了总体的**期望[预测误差](@entry_id:753692)**。

#### L1 vs. L2 正则化：几何视角

最常见的两种正则化是[L1和L2正则化](@entry_id:636768)，它们对权重施加了不同的几何约束 [@problem_id:3180413]。我们可以通过一个简化的代理问题来理解它们的差异：在给定一个固定的梯度方向 $\boldsymbol{r}$ 的情况下，寻找一个权重向量 $\boldsymbol{w}$ 来最小化 $\boldsymbol{r}^\top \boldsymbol{w}$，同时满足范数约束 $\lVert \boldsymbol{w} \rVert_p \le t$（其中 $p=1$ 或 $p=2$）。

- **[L2正则化](@entry_id:162880)（[岭回归](@entry_id:140984)）**：约束 $\lVert \boldsymbol{w} \rVert_2 \le t$ 在参数空间中定义了一个**超球面（hypersphere）**。为了最小化 $\boldsymbol{r}^\top \boldsymbol{w}$，最优解 $\boldsymbol{w}^*$ 位于球面上，且方向与 $\boldsymbol{r}$ 完全相反，即 $\boldsymbol{w}^* \propto -\boldsymbol{r}$。除非 $\boldsymbol{r}$ 的某个分量恰好为零，否则 $\boldsymbol{w}^*$ 的所有分量都将是非零的。[L2正则化](@entry_id:162880)倾向于产生**稠密**的、所有分量都很小的权重向量。

- **[L1正则化](@entry_id:751088)（[LASSO](@entry_id:751223)）**：约束 $\lVert \boldsymbol{w} \rVert_1 \le t$ 在[参数空间](@entry_id:178581)中定义了一个**交叉[多面体](@entry_id:637910)（cross-polytope）**，其特点是在坐标轴方向上有“尖角”。当最小化一个线性函数时，最优解很可能出现在这些尖角上。这些尖角对应于只有单个坐标非零的向量。因此，[L1正则化](@entry_id:751088)倾向于产生**稀疏**的权重向量，即许多权重分量**恰好为零**。

这种[稀疏性](@entry_id:136793)使得[L1正则化](@entry_id:751088)具有**特征选择**的特性。如果一个特征是无关的，它对应的权重很可能被设为零，该特征就被有效地从模型中移除了。这在特征维度远大于样本数的高维稀疏问题中尤其有用。然而，[L1正则化](@entry_id:751088)也有其不稳定性：当一组特征高度相关时，它可能随机地选择其中一个特征并赋予其权重，而将其他相关特征的权重设为零。相比之下，[L2正则化](@entry_id:162880)会倾向于将权重均匀地分配给所有相关特征。

总结而言，[L2正则化](@entry_id:162880)通过平滑地收缩所有权重来降低[模型复杂度](@entry_id:145563)，而[L1正则化](@entry_id:751088)则通过一种“赢者通吃”的方式进行[特征选择](@entry_id:177971)，实现了更为激进的[模型简化](@entry_id:171175)。这两种机制为我们提供了控制神经元行为、[防止过拟合](@entry_id:635166)的强大工具。