{"hands_on_practices": [{"introduction": "Sigmoid 和 tanh 函数的一个关键区别在于它们的输出范围，这直接影响到神经网络中激活值的“零中心”特性，而零中心激活值通常能够加速模型收敛。本练习将引导你通过编程实践，量化这一差异，并探索如何利用 tanh 函数的对称性来设计一个能抵抗输入偏差的“偏置校正”层，从而加深对激活函数选择重要性的理解。[@problem_id:3174564]", "problem": "考虑一个深度学习中的前馈计算过程，其中输入向量在通过逐点非线性变换之前，先经过一个仿射平移。设输入向量为固定列表 $x = [ -3, -1, -0.5, 0, 0.5, 1, 3 ]$。设一个偏置参数 $b \\in \\mathbb{R}$ 被逐元素地应用于该向量，以产生预激活向量 $z = x + b$，其中加法是逐元素进行的。我们感兴趣的两个激活函数是逻辑S型函数 $\\sigma$ 和双曲正切函数 $\\tanh$。仅使用它们的标准数学定义和基本性质，实现这些逐点函数，并比较它们在有偏置输入下的行为。\n\n您的任务是：\n- 实现作用于 $\\mathbb{R}$ 上的激活函数 $\\sigma$ 和 $\\tanh$，并将其逐元素地应用于向量。\n- 将逐点函数 $f$ 在向量 $u$ 上的零中心化得分定义为 $$S(f,u) = \\left| \\frac{1}{n} \\sum_{i=1}^{n} f(u_i) \\right|,$$ 其中 $n$ 是 $u$ 的长度，$u_i$ 表示第 $i$ 个元素。\n- 对于测试集中的每个偏置 $b$，计算 $z = x + b$ 对应的得分 $S(\\sigma,z)$ 和 $S(\\tanh,z)$。\n- 提出、从第一性原理推导并实现一个偏置校正层，该层利用 $\\tanh$ 的奇对称性来重新中心化激活值。具体来说，使用预激活值的经验均值作为校正偏移量：设 $$\\hat{\\mu}(z) = \\frac{1}{n}\\sum_{i=1}^{n} z_i,$$ 并定义校正后的激活 $$c(z) = \\tanh\\!\\big(z - \\hat{\\mu}(z)\\big),$$ 其中减法是逐元素进行的。然后计算得分 $S(c,z)$。\n\n使用以下偏置测试集，其选择旨在探究正常和极端条件： $$B = [\\,0,\\,0.5,\\,-0.5,\\,2,\\,-2,\\,5,\\,-5\\,].$$ 该集合包括无平移、小幅正负平移、中等平移以及会导致饱和趋势的大幅平移情况。\n\n您的程序必须：\n- 对于每个 $b \\in B$，计算得分三元组 $[\\,S(\\sigma,z),\\, S(\\tanh,z),\\, S(c,z)\\,]$，其中 $z = x + b$。\n- 将每个得分四舍五入到 $6$ 位小数。\n- 生成单行输出，包含一个由逗号分隔的列表组成的列表，并用方括号括起来。列表的顺序必须与 $B$ 中偏置的给定顺序相对应。例如，要求的输出格式为 $$[\\,[s_{1,1},s_{1,2},s_{1,3}],\\,[s_{2,1},s_{2,2},s_{2,3}],\\,\\dots,\\, [s_{m,1},s_{m,2},s_{m,3}]\\,],$$ 其中 $m$ 是测试用例的数量，每个 $s_{j,k}$ 是一个保留 $6$ 位小数的浮点数。", "solution": "问题陈述是有效的。它在科学上基于神经网络激活函数的原理，在数学上是适定的，并且表述客观。唯一解所需的所有定义、变量和常量都已提供。因此，我们可以进行形式化的求解。\n\n目标是分析和比较两种常见的激活函数——逻辑S型函数 $\\sigma$ 和双曲正切函数 $\\tanh$——在应用于经过仿射平移的输入向量时的零中心化程度。此外，我们将推导并评估一个旨在恢复激活值零中心化的偏置校正机制。\n\n首先，我们确立所涉及函数的数学定义。逻辑S型函数的定义为：\n$$ \\sigma(t) = \\frac{1}{1 + e^{-t}} $$\n它将任何实数输入 $t \\in \\mathbb{R}$ 映射到开区间 $(0, 1)$。双曲正切函数的定义为：\n$$ \\tanh(t) = \\frac{e^t - e^{-t}}{e^t + e^{-t}} $$\n它将任何实数输入 $t \\in \\mathbb{R}$ 映射到开区间 $(-1, 1)$。$\\tanh$ 的一个关键性质是它是一个奇函数，即 $\\tanh(-t) = -\\tanh(t)$。相比之下，S型函数既不是奇函数也不是偶函数，但它关于点 $(0, 0.5)$ 对称，满足 $\\sigma(t) + \\sigma(-t) = 1$。\n\n问题定义了一个从固定输入向量 $x$ 开始的计算过程。\n$$ x = [ -3, -1, -0.5, 0, 0.5, 1, 3 ] $$\n这个向量的长度为 $n=7$，且关于 $0$ 对称。该对称性的一个直接结果是其经验均值为零：\n$$ \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{7} (-3 - 1 - 0.5 + 0 + 0.5 + 1 + 3) = 0 $$\n\n一个逐元素的偏置 $b \\in \\mathbb{R}$ 被加到 $x$ 上，以产生预激活向量 $z$：\n$$ z = x + b $$\n$z$ 的分量是 $z_i = x_i + b$。这些预激活值随后通过一个逐点的激活函数。\n\n所得激活向量的零中心化程度由得分 $S(f, u)$ 量化：\n$$ S(f,u) = \\left| \\frac{1}{n} \\sum_{i=1}^{n} f(u_i) \\right| $$\n该得分是函数输出均值的绝对值。得分为 $0$ 表示完美的零中心化。\n\n我们的任务是为测试集 $B = [\\,0,\\,0.5,\\,-0.5,\\,2,\\,-2,\\,5,\\,-5\\,]$ 中的每个偏置 $b$ 计算三个得分：\n1. $S(\\sigma, z)$：S型函数的得分。由于对所有 $t$ 都有 $\\sigma(t)  0$，因此和 $\\sum \\sigma(z_i)$ 将始终为正，从而 $S(\\sigma, z)  0$。激活值本质上不是零中心的。\n2. $S(\\tanh, z)$：双曲正切函数的得分。如果 $\\tanh$ 的输入关于 $0$ 对称，其输出也将对称，它们的和将为 $0$。向量 $x$ 是对称的，因此当 $b=0$ 时，$z=x$，我们预期 $S(\\tanh, x) = 0$。对于任何 $b \\neq 0$，向量 $z=x+b$ 不再关于 $0$ 对称，因此我们预期 $S(\\tanh, z)  0$。\n3. $S(c, z)$：偏置校正后激活的得分。此过程旨在恢复零中心化。首先，计算预激活值的经验均值 $\\hat{\\mu}(z)$：\n$$ \\hat{\\mu}(z) = \\frac{1}{n}\\sum_{i=1}^{n} z_i $$\n我们可以基于 $x$ 的性质推导出 $\\hat{\\mu}(z)$ 的简化表达式：\n$$ \\hat{\\mu}(z) = \\frac{1}{n}\\sum_{i=1}^{n} (x_i + b) = \\left(\\frac{1}{n}\\sum_{i=1}^{n} x_i\\right) + \\left(\\frac{1}{n}\\sum_{i=1}^{n} b\\right) = 0 + \\frac{1}{n}(nb) = b $$\n因此，预激活值的经验均值恰好是所施加的偏置 $b$。\n\n然后，偏置校正层通过减去这个经验均值来中心化预激活值，之后再应用 $\\tanh$ 函数：\n$$ c(z) = \\tanh(z - \\hat{\\mu}(z)) $$\n代入我们的发现，$\\tanh$ 的参数变为：\n$$ z - \\hat{\\mu}(z) = (x + b) - b = x $$\n因此，校正后的激活就是 $c(z) = \\tanh(x)$，与偏置 $b$ 无关。$\\tanh$ 的输入是原始向量 $x$，它关于 $0$ 对称。由于 $\\tanh$ 是一个奇函数，对于每个分量 $x_i$，其加法逆元 $-x_i$ 也存在于向量 $x$ 中，并且 $\\tanh(-x_i) = -\\tanh(x_i)$。因此，激活值的和保证为零：\n$$ \\sum_{i=1}^{n} \\tanh(x_i) = \\tanh(-3) + \\tanh(-1) + \\tanh(-0.5) + \\tanh(0) + \\tanh(0.5) + \\tanh(1) + \\tanh(3) $$\n$$ = (-\\tanh(3)) + (-\\tanh(1)) + (-\\tanh(0.5)) + 0 + \\tanh(0.5) + \\tanh(1) + \\tanh(3) = 0 $$\n因此，对于所有 $b$ 值，校正后激活的得分必须为零：\n$$ S(c, z) = S(\\tanh, x) = \\left| \\frac{1}{n} \\sum_{i=1}^{n} \\tanh(x_i) \\right| = \\left| \\frac{0}{7} \\right| = 0 $$\n这个从第一性原理出发的推导表明，由于初始输入向量 $x$ 的零均值特性，所提出的校正方法对于任何给定的偏置 $b$ 都能完美地重新中心化激活值。\n\n对每个 $b \\in B$ 的计算过程如下：\n1. 构建预激活向量 $z = x + b$。\n2. 计算S型激活值 $\\sigma(z_i)$ 及其得分 $S(\\sigma, z)$。\n3. 计算双曲正切激活值 $\\tanh(z_i)$ 及其得分 $S(\\tanh, z)$。\n4. 计算校正后的激活值，其简化为 $\\tanh(x_i)$，并计算其得分 $S(c, z)$，理论上为 $0$。\n5. 收集三个得分 $[\\,S(\\sigma,z),\\, S(\\tanh,z),\\, S(c,z)\\,]$ 并将每个得分四舍五入到 $6$ 位小数。\n\n此过程将对整个测试集 $B$ 进行实现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes zero-centeredness scores for sigmoid, tanh, and a corrected tanh activation\n    across a suite of bias parameters.\n    \"\"\"\n    # Define the fixed input vector from the problem statement.\n    x = np.array([-3.0, -1.0, -0.5, 0.0, 0.5, 1.0, 3.0])\n\n    # Define the test suite of bias parameters.\n    test_biases = [0.0, 0.5, -0.5, 2.0, -2.0, 5.0, -5.0]\n\n    # --- Activation Function Implementations ---\n    def sigmoid(t):\n        \"\"\"Elementwise logistic sigmoid function.\"\"\"\n        return 1.0 / (1.0 + np.exp(-t))\n\n    def tanh(t):\n        \"\"\"Elementwise hyperbolic tangent function.\"\"\"\n        # np.tanh is a standard, numerically stable implementation.\n        return np.tanh(t)\n\n    # --- Zero-Centeredness Score Implementation ---\n    def zero_centeredness_score(activations):\n        \"\"\"\n        Computes the score S(f,u) as the absolute value of the mean of activations.\n        \"\"\"\n        return np.abs(np.mean(activations))\n\n    results = []\n    for b in test_biases:\n        # 1. Compute the pre-activation vector z = x + b\n        z = x + b\n\n        # 2. Compute the score for the sigmoid function\n        activations_sigma = sigmoid(z)\n        score_sigma = zero_centeredness_score(activations_sigma)\n\n        # 3. Compute the score for the hyperbolic tangent function\n        activations_tanh = tanh(z)\n        score_tanh = zero_centeredness_score(activations_tanh)\n\n        # 4. Compute the score for the bias-corrected tanh activation\n        # The derivation shows the corrected pre-activation is z - mean(z) = x.\n        # This is because mean(z) = mean(x+b) = mean(x) + b = 0 + b = b.\n        # So, (x+b) - b = x.\n        activations_corrected = tanh(x)\n        score_corrected = zero_centeredness_score(activations_corrected)\n\n        # 5. Round scores to 6 decimal places and store the triple\n        current_scores = [\n            round(score_sigma, 6),\n            round(score_tanh, 6),\n            round(score_corrected, 6)\n        ]\n        results.append(current_scores)\n\n    # --- Format the final output string ---\n    # The required format is a string representing a list of lists, with no spaces\n    # and with each number formatted to 6 decimal places.\n    # e.g., [[0.500000,0.000000,0.000000],[...]]\n\n    final_string_parts = []\n    for triple in results:\n        # Format each number in the triple to exactly 6 decimal places\n        formatted_nums = ','.join([f\"{num:.6f}\" for num in triple])\n        # Enclose in brackets to form a list-like string, e.g., \"[num1,num2,num3]\"\n        triple_str = f\"[{formatted_nums}]\"\n        final_string_parts.append(triple_str)\n\n    # Join the list-like strings and enclose in outer brackets\n    final_output = f\"[{','.join(final_string_parts)}]\"\n\n    print(final_output)\n\nsolve()\n\n```", "id": "3174564"}, {"introduction": "掌握了基本属性后，我们将更进一步，将 tanh 函数应用于实际的模型设计中。在信号处理和稀疏编码领域，一个重要的算子“软阈值”（soft-thresholding）由于其在某些点的不可导性，无法直接用于基于梯度的优化。本练习旨在挑战你利用双曲正切函数光滑、可导的特性，构建其可微近似版本，这在设计定制化神经网络层时是一项核心技能。[@problem_id:3174524]", "problem": "您的任务是在深度学习中的稀疏编码和去噪背景下，开发和评估基于双曲正切函数的可微收缩算子。从以下基本概念开始：logistic sigmoid 函数 $\\sigma(x)$ 定义为 $\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$，双曲正切函数 $\\tanh(x)$ 定义为 $\\tanh(x) = \\dfrac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$，并遵循恒等式 $\\tanh(x) = 2\\sigma(2x) - 1$。标准软阈值（也称为 $\\ell_{1}$范数的近端算子）在阈值 $\\lambda  0$ 下定义为 $S_{\\lambda}(x) = \\mathrm{sign}(x)\\cdot \\max(|x| - \\lambda, 0)$，并且是最小绝对收缩和选择算子 (LASSO) 的核心。\n\n您的目标是构建与 $\\tanh$ 相关的平滑收缩算子，分析它们的性质，并量化它们对 $S_{\\lambda}(x)$ 的近似程度。考虑两个由 $\\lambda  0$ 参数化的候选可微算子：\n- $A_{\\lambda}(x) = \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$,\n- $B_{\\lambda}(x) = x - \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$。\n\n使用第一性原理、logistic sigmoid 定义和双曲正切恒等式，以及软阈值定义，论证在稀疏编码和去噪的应用场景下，为什么每个算子可能是或可能不是 $S_{\\lambda}(x)$ 的合适可微近似。特别地，将 $A_{\\lambda}$ 和 $B_{\\lambda}$ 在 $x = 0$ 附近、 $x = \\pm \\lambda$ 处以及对于大的 $|x|$ 的定性行为与 $S_{\\lambda}$ 进行关联。\n\n然后，编写一个完整的程序来实现 $S_{\\lambda}$、$A_{\\lambda}$ 和 $B_{\\lambda}$，并针对指定的测试套件定量评估以下指标。使用以下阈值参数测试套件：\n- $\\lambda \\in \\{0.1, 0.5, 1.0, 2.0\\}$。\n\n对于测试套件中的每个 $\\lambda$：\n1. 构建一个由 $N = 801$ 个在 $[-4\\lambda, 4\\lambda]$ 区间内（含两端）均匀间隔的点组成的 $x$ 值网格。\n2. 计算在网格上 $A_{\\lambda}$ 和 $S_{\\lambda}$ 之间，以及 $B_{\\lambda}$ 和 $S_{\\lambda}$ 之间的均方误差 (MSE)，其定义为 $\\mathrm{MSE}(f, g) = \\dfrac{1}{N} \\sum_{i=1}^{N} \\left(f(x_{i}) - g(x_{i})\\right)^{2}$。\n3. 计算在网格上 $A_{\\lambda}$ 与 $S_{\\lambda}$ 之间以及 $B_{\\lambda}$ 与 $S_{\\lambda}$ 之间的最大绝对误差，其定义为 $\\max_{i} \\left| f(x_{i}) - g(x_{i}) \\right|$。\n4. 计算 $A_{\\lambda}$ 和 $B_{\\lambda}$ 在 $x=0$ 处的导数。使用 $\\tanh$ 的解析导数，即 $\\dfrac{d}{dx}\\tanh(x) = \\mathrm{sech}^{2}(x)$，其中 $\\mathrm{sech}(x) = \\dfrac{1}{\\cosh(x)}$ 且 $\\cosh(x) = \\dfrac{e^{x} + e^{-x}}{2}$。\n5. 计算在阈值点 $x = \\lambda$ 和 $x = -\\lambda$ 处的点态误差，即 $A_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$、$B_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$、$A_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$ 和 $B_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$。\n\n您的程序必须生成单行输出，其中包含按 $\\lambda$ 升序排列的结果，形式为方括号括起来的逗号分隔列表。对于每个 $\\lambda$，按以下顺序输出一个包含10个浮点数的序列，每个数都四舍五入到六位小数：\n- $\\mathrm{MSE}(A_{\\lambda}, S_{\\lambda})$,\n- $\\mathrm{MSE}(B_{\\lambda}, S_{\\lambda})$,\n- $\\max |A_{\\lambda} - S_{\\lambda}|$,\n- $\\max |B_{\\lambda} - S_{\\lambda}|$,\n- $A'_{\\lambda}(0)$,\n- $B'_{\\lambda}(0)$,\n- $A_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$,\n- $B_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda)$,\n- $A_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$,\n- $B_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda)$。\n\n因此，对于四个 $\\lambda$ 值，最终输出将是一个包含40个数字的列表。不涉及物理单位；不使用角度。确保所有计算都是纯数值的并且在数学上是合理的。最终输出格式必须严格为单行形式，如 $[r_{1},r_{2},\\dots,r_{40}]$，其中每个 $r_{k}$ 是一个四舍五入到六位小数的浮点数。", "solution": "该问题已经过验证并被认为是有效的。它在科学上基于与深度学习相关的既定数学函数，问题阐述清晰，具有明确的定义和目标，并且没有任何列出的使其无效的缺陷。\n\n任务是分析两个可微算子 $A_{\\lambda}(x)$ 和 $B_{\\lambda}(x)$，作为软阈值函数 $S_{\\lambda}(x)$ 的潜在平滑近似。$S_{\\lambda}(x)$ 对稀疏编码和 LASSO 至关重要，但它是不可微的。在这种情况下，一个近似是否合适取决于三个主要标准：\n1.  **对小值的收缩**：它应将小幅值（假定为噪声）的输入映射到接近零的值。\n2.  **对大值的保留**：它应将大幅值（假定为信号）的输入映射到接近输入原始值的值。\n3.  **可微性**：函数必须处处可微，才能用于深度学习中常见的标准基于梯度的优化算法。\n\n标准软阈值算子定义为：\n$$S_{\\lambda}(x) = \\mathrm{sign}(x) \\cdot \\max(|x| - \\lambda, 0), \\quad \\lambda  0$$\n该函数完美体现了前两个标准：它将所有满足 $|x| \\le \\lambda$ 的输入 $x$ 设置为 $0$；对于 $|x|  \\lambda$，它返回 $x - \\lambda \\cdot \\mathrm{sign}(x)$，在恒定位移下保留了输入。然而，它在 $x = \\pm \\lambda$ 处不可微，违反了第三个标准。\n\n我们现在将根据这些标准分析两个候选近似 $A_{\\lambda}(x)$ 和 $B_{\\lambda}(x)$。\n\n### $A_{\\lambda}(x) = \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$ 的分析\n\n算子 $A_{\\lambda}(x)$ 基于双曲正切函数 $\\tanh(u)$ 构建，该函数具有以下性质：$\\tanh(0)=0$，$\\lim_{u\\to\\pm\\infty} \\tanh(u) = \\pm 1$，且对于小的 $u$，$\\tanh(u) \\approx u$。\n\n1.  **$x=0$ 附近的行为**：当 $x$ 接近 $0$ 时，其参数 $u = x/\\lambda$ 也很小。使用 $\\tanh$ 的线性近似，我们得到：\n    $$A_{\\lambda}(x) = \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right) \\approx \\lambda \\left(\\dfrac{x}{\\lambda}\\right) = x$$\n    这种行为与收缩原则相悖。对于小输入，$A_{\\lambda}(x)$ 的作用类似于恒等函数，直接通过它们而不是将它们设为零。这不满足第一个标准。\n\n2.  **大 $|x|$ 时的行为**：当 $|x| \\to \\infty$ 时，参数 $x/\\lambda \\to \\pm\\infty$。因此：\n    $$A_{\\lambda}(x) \\to \\lambda \\cdot (\\pm 1) = \\pm \\lambda$$\n    该函数在大值处饱和或削波至 $\\pm\\lambda$。这与 $S_{\\lambda}(x)$ 有着根本的不同，后者在 $|x|$ 较大时线性增长，即 $S_{\\lambda}(x) \\approx x$。削波大幅值信号将导致严重的信息损失，不满足第二个标准。\n\n3.  **可微性**：$A_{\\lambda}(x)$ 是可微函数的复合，因此处处可微。这满足了第三个标准。\n\n**关于 $A_{\\lambda}(x)$ 的结论**：尽管 $A_{\\lambda}(x)$ 是可微的，但它作为软阈值算子的近似效果很差。它既没有以稀疏编码所需的方式收缩小值，也没有保留大值。它的行为更像一个带饱和的线性函数，而不是一个收缩算子。\n\n### $B_{\\lambda}(x) = x - \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right)$ 的分析\n\n该算子可以看作 $B_{\\lambda}(x) = x - A_{\\lambda}(x)$。\n\n1.  **$x=0$ 附近的行为**：使用与之前相同的近似，当 $x$ 接近 $0$ 时：\n    $$B_{\\lambda}(x) = x - A_{\\lambda}(x) \\approx x - x = 0$$\n    这种行为与 $S_{\\lambda}(x)$ 在区域 $|x| \\le \\lambda$ 内的行为相匹配。它有效地将小值收缩至零，满足了第一个标准。\n\n2.  **大 $|x|$ 时的行为**：当 $|x| \\to \\infty$ 时：\n    $$B_{\\lambda}(x) = x - \\lambda \\tanh\\left(\\dfrac{x}{\\lambda}\\right) \\to x - \\lambda \\cdot \\mathrm{sign}(x)$$\n    这种渐进行为与 $|x|  \\lambda$ 时 $S_{\\lambda}(x)$ 的行为相同。它保留了大值，并具有软阈值的特征位移，满足了第二个标准。\n\n3.  **可微性**：与 $A_{\\lambda}(x)$ 一样，$B_{\\lambda}(x)$ 也处处可微。这满足了第三个标准。\n\n**关于 $B_{\\lambda}(x)$ 的结论**：算子 $B_{\\lambda}(x)$ 是软阈值函数 $S_{\\lambda}(x)$ 的一个优秀的平滑可微近似。它正确地模仿了收缩小值和保留大值的关键特性，使其成为深度学习中学习型迭代收缩阈值算法 (LISTA) 等应用场景下的一个可行且广泛使用的替代方案。\n\n### 定量评估指标\n\n问题要求计算几个指标。必要的解析公式如下：\n\n*   **$x=0$ 处的导数**：我们使用链式法则和给定的导数 $\\dfrac{d}{du}\\tanh(u) = \\mathrm{sech}^{2}(u)$。\n    $$A'_{\\lambda}(x) = \\dfrac{d}{dx} \\left[ \\lambda \\tanh\\left(\\frac{x}{\\lambda}\\right) \\right] = \\lambda \\cdot \\mathrm{sech}^2\\left(\\frac{x}{\\lambda}\\right) \\cdot \\frac{1}{\\lambda} = \\mathrm{sech}^2\\left(\\frac{x}{\\lambda}\\right)$$\n    在 $x=0$ 处，$A'_{\\lambda}(0) = \\mathrm{sech}^2(0) = 1$，因为 $\\cosh(0)=1$。\n    $$B'_{\\lambda}(x) = \\dfrac{d}{dx} \\left[ x - A_{\\lambda}(x) \\right] = 1 - A'_{\\lambda}(x) = 1 - \\mathrm{sech}^2\\left(\\frac{x}{\\lambda}\\right) = \\tanh^2\\left(\\frac{x}{\\lambda}\\right)$$\n    在 $x=0$ 处，$B'_{\\lambda}(0) = \\tanh^2(0) = 0$。\n    对于任何 $\\lambda  0$，这些导数都是常数。\n\n*   **$x=\\pm\\lambda$ 处的点态误差**：\n    在 $x=\\lambda$ 处，$S_{\\lambda}(\\lambda)=0$。误差为：\n    $A_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda) = \\lambda \\tanh(1) - 0 = \\lambda \\tanh(1)$。\n    $B_{\\lambda}(\\lambda) - S_{\\lambda}(\\lambda) = (\\lambda - \\lambda \\tanh(1)) - 0 = \\lambda(1 - \\tanh(1))$。\n    在 $x=-\\lambda$ 处，$S_{\\lambda}(-\\lambda)=0$。误差为：\n    $A_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda) = \\lambda \\tanh(-1) - 0 = -\\lambda \\tanh(1)$。\n    $B_{\\lambda}(-\\lambda) - S_{\\lambda}(-\\lambda) = (-\\lambda - \\lambda \\tanh(-1)) - 0 = -\\lambda + \\lambda\\tanh(1) = -\\lambda(1 - \\tanh(1))$。\n\n其余的指标（MSE 和最大绝对误差）在指定的网格上进行数值计算。以下程序实现了这些计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates differentiable shrinkage operators against soft-thresholding.\n    \"\"\"\n    \n    # Define the test suite of threshold parameters\n    test_cases = [0.1, 0.5, 1.0, 2.0]\n    \n    N = 801 # Number of grid points\n\n    def S_lambda(x, lam):\n        \"\"\"Standard soft-thresholding operator.\"\"\"\n        return np.sign(x) * np.maximum(np.abs(x) - lam, 0)\n\n    def A_lambda(x, lam):\n        \"\"\"Differentiable operator A_lambda(x).\"\"\"\n        return lam * np.tanh(x / lam)\n\n    def B_lambda(x, lam):\n        \"\"\"Differentiable operator B_lambda(x).\"\"\"\n        return x - lam * np.tanh(x / lam)\n\n    results = []\n    \n    for lam in test_cases:\n        # 1. Construct the grid of x values\n        x_grid = np.linspace(-4 * lam, 4 * lam, N)\n\n        # Compute function values on the grid\n        y_S = S_lambda(x_grid, lam)\n        y_A = A_lambda(x_grid, lam)\n        y_B = B_lambda(x_grid, lam)\n\n        # 2. Compute Mean Squared Error (MSE)\n        mse_A = np.mean((y_A - y_S)**2)\n        mse_B = np.mean((y_B - y_S)**2)\n        results.extend([mse_A, mse_B])\n\n        # 3. Compute maximum absolute error\n        max_err_A = np.max(np.abs(y_A - y_S))\n        max_err_B = np.max(np.abs(y_B - y_S))\n        results.extend([max_err_A, max_err_B])\n\n        # 4. Compute derivatives at x = 0\n        # A'_lambda(0) = sech^2(0) = 1\n        # B'_lambda(0) = tanh^2(0) = 0\n        deriv_A_at_0 = 1.0\n        deriv_B_at_0 = 0.0\n        results.extend([deriv_A_at_0, deriv_B_at_0])\n        \n        # 5. Compute pointwise errors at x = lambda and x = -lambda\n        # For a more precise calculation, use np.tanh(1) instead of a pre-computed constant\n        tanh_one = np.tanh(1)\n\n        # At x = lambda, S_lambda(lambda) = 0\n        err_A_pos = lam * tanh_one\n        err_B_pos = lam - lam * tanh_one\n        \n        # At x = -lambda, S_lambda(-lambda) = 0\n        err_A_neg = -lam * tanh_one\n        err_B_neg = -lam + lam * tanh_one\n\n        results.extend([err_A_pos, err_B_pos, err_A_neg, err_B_neg])\n\n    # Format the results for the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3174524"}, {"introduction": "作为一项综合性练习，我们将探索如何用简单的 Sigmoid 单元构建复杂的阶梯函数，并分析其与训练稳定性相关的核心问题。通过动手实现一个可微的多值开关，你将具体计算模型参数（如陡峭度 $\\alpha$ 和阈值 $\\theta_k$）的梯度，直观地感受参数变化如何导致梯度消失或爆炸，这是理解和解决深度学习训练难题的关键一步。[@problem_id:3174535]", "problem": "您的任务是在深度学习常见的数学框架内，使用 logistic S型函数和双曲正切函数，在一维上构建和分析可微多值开关。设 logistic S型函数定义为 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$，双曲正切函数定义为 $\\tanh(z) = \\dfrac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$。考虑阈值 $\\{\\theta_k\\}_{k=1}^{K}$ 和一个正的陡度参数 $\\alpha  0$。定义堆叠的开关组件 $s_k(x) = \\sigma\\left(\\alpha(x - \\theta_k)\\right)$ 和阶梯近似\n$$\nf_\\alpha(x) = \\sum_{k=1}^{K} s_k(x).\n$$\n使用赫维赛德阶跃函数 $H(u)$ 定义目标阶梯函数，约定 $H(0) = \\tfrac{1}{2}$，如下所示\n$$\nf^\\star(x) = \\sum_{k=1}^{K} H(x - \\theta_k).\n$$\n您必须编写一个完整的、可运行的程序，对于每个测试用例，计算量化指标，以表征参数化函数 $f_\\alpha(x)$ 相对于 $f^\\star(x)$ 的近似质量和训练稳定性。\n\n程序必须为每个测试用例计算以下指标，使用在 $[x_{\\min}, x_{\\max}]$（含端点）上均匀分布的 $N$ 个 $x$ 样本组成的网格：\n\n1. 近似的均方误差 (MSE)，\n$$\n\\text{MSE} = \\dfrac{1}{N} \\sum_{i=1}^{N} \\left( f_\\alpha(x_i) - f^\\star(x_i) \\right)^2.\n$$\n\n2. 近似器 $f_\\alpha$ 在网格上相对于 $x$ 的最大绝对斜率，\n$$\nS_{\\max} = \\max_{i \\in \\{1,\\dots,N\\}} \\left| \\dfrac{d}{dx} f_\\alpha(x_i) \\right|.\n$$\n\n3. 在网格上求值的、关于阈值向量 $\\boldsymbol{\\theta} = (\\theta_1,\\dots,\\theta_K)$ 的半均方误差 $L = \\dfrac{1}{2N} \\sum_{i=1}^{N} \\left( f_\\alpha(x_i) - f^\\star(x_i) \\right)^2$ 的梯度的欧几里得范数，\n$$\n\\left\\| \\nabla_{\\boldsymbol{\\theta}} L \\right\\|_2 = \\sqrt{ \\sum_{k=1}^{K} \\left( \\dfrac{\\partial L}{\\partial \\theta_k} \\right)^2 }.\n$$\n\n4. 在网格上求值的、关于陡度参数 $\\alpha$ 的 $L$ 的梯度的绝对值，\n$$\n\\left| \\dfrac{\\partial L}{\\partial \\alpha} \\right|.\n$$\n\n5. 在网格上遇到的所有 $z = \\alpha(x_i - \\theta_k)$ 和所有阈值下，logistic S型函数 $\\sigma(z)$ 与其双曲正切表示 $\\dfrac{1}{2}\\left(\\tanh\\left(\\dfrac{z}{2}\\right) + 1\\right)$ 之间的最大绝对差异，\n$$\nD_{\\max} = \\max_{i \\in \\{1,\\dots,N\\}, k \\in \\{1,\\dots,K\\}} \\left| \\sigma\\!\\left(\\alpha(x_i - \\theta_k)\\right) - \\dfrac{1}{2}\\left(\\tanh\\!\\left(\\dfrac{\\alpha(x_i - \\theta_k)}{2}\\right) + 1\\right) \\right|.\n$$\n\n使用以下测试套件。对于每个测试用例，参数以元组 $(\\alpha, \\{\\theta_k\\}, x_{\\min}, x_{\\max}, N)$ 的形式给出：\n\n- 测试用例 $1$ (一般情况): $\\left( 5.0, \\{-2.0, 0.0, 2.0\\}, -6.0, 6.0, 10001 \\right)$。\n- 测试用例 $2$ (小陡度): $\\left( 0.5, \\{-2.0, 0.0, 2.0\\}, -6.0, 6.0, 10001 \\right)$。\n- 测试用例 $3$ (极大陡度): $\\left( 50.0, \\{-2.0, 0.0, 2.0\\}, -6.0, 6.0, 10001 \\right)$。\n- 测试用例 $4$ (退化阈值): $\\left( 5.0, \\{-1.0, -1.0, 1.0\\}, -6.0, 6.0, 10001 \\right)$。\n- 测试用例 $5$ (未排序阈值): $\\left( 5.0, \\{3.0, -3.0, 0.0\\}, -6.0, 6.0, 10001 \\right)$。\n\n实现和评估要求：\n\n- 为保证数值稳定性和自洽性，在构建 $f^\\star(x)$ 之前，内部对阈值集 $\\{\\theta_k\\}$ 进行排序，并在构建 $f_\\alpha(x)$ 时应用相同的排序集。\n- 对于 $x_i = \\theta_k$ 的样本，严格使用赫维赛德约定 $H(0) = \\tfrac{1}{2}$。\n- 所有输出均表示为四舍五入到 $6$ 位小数的实数。\n- 您的程序应生成单行输出，其中包含所有测试用例的聚合结果，形式为方括号括起来的逗号分隔列表：具体来说，首先连接测试用例 $1$ 的五个指标，顺序为 $\\left[\\text{MSE}, S_{\\max}, \\left\\| \\nabla_{\\boldsymbol{\\theta}} L \\right\\|_2, \\left| \\dfrac{\\partial L}{\\partial \\alpha} \\right|, D_{\\max} \\right]$，然后是测试用例 $2$ 的，以此类推，最终产生一个扁平列表。例如，格式必须严格为 $\\left[\\text{m}_{1,1},\\text{m}_{1,2},\\dots,\\text{m}_{1,5},\\text{m}_{2,1},\\dots,\\text{m}_{5,5}\\right]$，其中 $\\text{m}_{t,j}$ 表示测试用例 $t$ 的第 $j$ 个指标。\n\n您的解决方案必须从第一性原理出发构建所有要求的量，仅从 $\\sigma(z)$、$\\tanh(z)$ 和 $H(u)$ 的定义以及标准的微分法则开始，而不使用为目标指标预先推导的捷径。", "solution": "该问题要求计算五个不同的指标，以分析一个由 logistic S型函数构建的阶梯函数的近似。我将首先按照规定，从第一性原理推导每个所需量的解析表达式。然后，我将概述基于推导出的公式计算这些指标的数值算法。\n\n基本函数是 logistic S型函数 $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ 和赫维赛德阶跃函数 $H(u)$，定义为当 $u  0$ 时为 $1$，当 $u  0$ 时为 $0$，当 $u=0$ 时为 $\\tfrac{1}{2}$。近似函数是 $f_\\alpha(x) = \\sum_{k=1}^{K} \\sigma(\\alpha(x - \\theta_k))$，目标函数是 $f^\\star(x) = \\sum_{k=1}^{K} H(x - \\theta_k)$，其中 $\\{\\theta_k\\}_{k=1}^K$ 是给定的阈值列表，$\\alpha  0$ 是一个陡度参数。问题要求在计算前对阈值进行排序以保证一致性。计算在区间 $[x_{\\min}, x_{\\max}]$ 上的一个包含 $N$ 个点的均匀网格 $\\{x_i\\}_{i=1}^N$ 上进行。\n\n### 数学推导\n\n首先，我们确定S型函数的导数，这对于大多数指标至关重要。设 $z$ 为一个变量。\n$$\n\\dfrac{d\\sigma}{dz} = \\dfrac{d}{dz} (1 + e^{-z})^{-1} = -1 \\cdot (1 + e^{-z})^{-2} \\cdot (-e^{-z}) = \\dfrac{e^{-z}}{(1 + e^{-z})^2}\n$$\n这可以改写成更常见的形式：\n$$\n\\dfrac{d\\sigma}{dz} = \\dfrac{1}{1 + e^{-z}} \\cdot \\dfrac{e^{-z}}{1 + e^{-z}} = \\sigma(z) \\cdot \\dfrac{(1 + e^{-z}) - 1}{1 + e^{-z}} = \\sigma(z) (1 - \\sigma(z))\n$$\n\n有了这个结果，我们就可以推导所需指标的表达式。\n\n**1. 均方误差 (MSE)**\nMSE 定义为近似值 $f_\\alpha(x)$ 与目标值 $f^\\star(x)$ 在网格上差值的平方的平均值：\n$$\n\\text{MSE} = \\dfrac{1}{N} \\sum_{i=1}^{N} \\left( f_\\alpha(x_i) - f^\\star(x_i) \\right)^2\n$$\n这是一个直接计算，其中对于每个网格点 $x_i$，我们计算 $f_\\alpha(x_i) = \\sum_{k=1}^{K} \\sigma(\\alpha(x_i - \\theta_k))$ 和 $f^\\star(x_i) = \\sum_{k=1}^{K} H(x_i - \\theta_k)$。\n\n**2. 最大绝对斜率 ($S_{\\max}$)**\n该指标是 $f_\\alpha(x)$ 关于 $x$ 的导数的大小的最大值：\n$$\nS_{\\max} = \\max_{i \\in \\{1,\\dots,N\\}} \\left| \\dfrac{d}{dx} f_\\alpha(x_i) \\right|\n$$\n使用链式法则和微分的求和法则：\n$$\n\\dfrac{d}{dx} f_\\alpha(x) = \\dfrac{d}{dx} \\sum_{k=1}^{K} \\sigma(\\alpha(x - \\theta_k)) = \\sum_{k=1}^{K} \\dfrac{d}{dx} \\sigma(\\alpha(x - \\theta_k))\n$$\n对于每一项，令 $z_k(x) = \\alpha(x - \\theta_k)$。则 $\\dfrac{dz_k}{dx} = \\alpha$。\n$$\n\\dfrac{d}{dx} \\sigma(z_k(x)) = \\dfrac{d\\sigma}{dz_k} \\dfrac{dz_k}{dx} = \\sigma(z_k(x))(1 - \\sigma(z_k(x))) \\cdot \\alpha\n$$\n代回后，完整的导数是：\n$$\n\\dfrac{d}{dx} f_\\alpha(x) = \\sum_{k=1}^{K} \\alpha \\cdot \\sigma(\\alpha(x - \\theta_k))(1 - \\sigma(\\alpha(x - \\theta_k)))\n$$\n在每个网格点 $x_i$ 上计算此表达式，以找到最大绝对值。\n\n**3. 关于阈值的梯度范数 ($\\left\\| \\nabla_{\\boldsymbol{\\theta}} L \\right\\|_2$)**\n损失函数是 $L = \\dfrac{1}{2N} \\sum_{i=1}^{N} (f_\\alpha(x_i) - f^\\star(x_i))^2$。我们需要 $L$ 关于阈值向量 $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_K)$ 的梯度。我们计算关于每个分量 $\\theta_j$ 的偏导数。\n$$\n\\dfrac{\\partial L}{\\partial \\theta_j} = \\dfrac{1}{2N} \\sum_{i=1}^{N} 2 (f_\\alpha(x_i) - f^\\star(x_i)) \\dfrac{\\partial}{\\partial \\theta_j} (f_\\alpha(x_i) - f^\\star(x_i))\n$$\n假设目标值 $f^\\star(x_i)$ 是相对于模型参数的固定常数（在机器学习中是标准做法），则 $\\dfrac{\\partial f^\\star(x_i)}{\\partial \\theta_j} = 0$。\n$$\n\\dfrac{\\partial L}{\\partial \\theta_j} = \\dfrac{1}{N} \\sum_{i=1}^{N} (f_\\alpha(x_i) - f^\\star(x_i)) \\dfrac{\\partial f_\\alpha(x_i)}{\\partial \\theta_j}\n$$\n$f_\\alpha(x_i)$ 关于 $\\theta_j$ 的导数是：\n$$\n\\dfrac{\\partial f_\\alpha(x_i)}{\\partial \\theta_j} = \\dfrac{\\partial}{\\partial \\theta_j} \\sum_{k=1}^{K} \\sigma(\\alpha(x_i - \\theta_k)) = \\dfrac{\\partial}{\\partial \\theta_j} \\sigma(\\alpha(x_i - \\theta_j))\n$$\n令 $z_j(x_i) = \\alpha(x_i - \\theta_j)$。则 $\\dfrac{\\partial z_j}{\\partial \\theta_j} = -\\alpha$。使用链式法则：\n$$\n\\dfrac{\\partial f_\\alpha(x_i)}{\\partial \\theta_j} = \\dfrac{d\\sigma}{dz_j} \\dfrac{\\partial z_j}{\\partial \\theta_j} = \\sigma(z_j)(1 - \\sigma(z_j)) \\cdot (-\\alpha)\n$$\n将此代入 $\\dfrac{\\partial L}{\\partial \\theta_j}$ 的表达式中：\n$$\n\\dfrac{\\partial L}{\\partial \\theta_j} = \\dfrac{1}{N} \\sum_{i=1}^{N} (f_\\alpha(x_i) - f^\\star(x_i)) \\left( -\\alpha \\cdot \\sigma(\\alpha(x_i - \\theta_j))(1 - \\sigma(\\alpha(x_i - \\theta_j))) \\right)\n$$\n计算完每个偏导数 $\\dfrac{\\partial L}{\\partial \\theta_j}$（对于 $j=1, \\dots, K$）后，计算梯度向量的欧几里得范数：\n$$\n\\left\\| \\nabla_{\\boldsymbol{\\theta}} L \\right\\|_2 = \\sqrt{ \\sum_{j=1}^{K} \\left( \\dfrac{\\partial L}{\\partial \\theta_j} \\right)^2 }\n$$\n\n**4. 关于陡度的梯度 ($|\\partial L / \\partial \\alpha|$)**\n与上述类似，我们求 $L$ 关于 $\\alpha$ 的偏导数：\n$$\n\\dfrac{\\partial L}{\\partial \\alpha} = \\dfrac{1}{N} \\sum_{i=1}^{N} (f_\\alpha(x_i) - f^\\star(x_i)) \\dfrac{\\partial f_\\alpha(x_i)}{\\partial \\alpha}\n$$\n$f_\\alpha(x_i)$ 关于 $\\alpha$ 的导数是：\n$$\n\\dfrac{\\partial f_\\alpha(x_i)}{\\partial \\alpha} = \\dfrac{\\partial}{\\partial \\alpha} \\sum_{k=1}^{K} \\sigma(\\alpha(x_i - \\theta_k)) = \\sum_{k=1}^{K} \\dfrac{\\partial}{\\partial \\alpha} \\sigma(\\alpha(x_i - \\theta_k))\n$$\n令 $z_k(x_i) = \\alpha(x_i - \\theta_k)$。则 $\\dfrac{\\partial z_k}{\\partial \\alpha} = x_i - \\theta_k$。使用链式法则：\n$$\n\\dfrac{\\partial}{\\partial \\alpha} \\sigma(z_k(x_i)) = \\dfrac{d\\sigma}{dz_k} \\dfrac{\\partial z_k}{\\partial \\alpha} = \\sigma(z_k(x_i))(1 - \\sigma(z_k(x_i))) \\cdot (x_i - \\theta_k)\n$$\n因此，完整的导数是：\n$$\n\\dfrac{\\partial f_\\alpha(x_i)}{\\partial \\alpha} = \\sum_{k=1}^{K} (x_i - \\theta_k) \\cdot \\sigma(\\alpha(x_i - \\theta_k))(1 - \\sigma(\\alpha(x_i - \\theta_k)))\n$$\n然后将这个求和的表达式代入 $\\dfrac{\\partial L}{\\partial \\alpha}$ 的公式中。最后，我们取绝对值。\n\n**5. Sigmoid-Tanh 差异 ($D_{\\max}$)**\n该指标量化了 logistic S型函数 $\\sigma(z)$ 与其涉及双曲正切的恒等式 $\\dfrac{1}{2}(\\tanh(z/2) + 1)$ 之间的数值差异。我们证明它们的解析等价性：\n$$\n\\tanh(w) = \\dfrac{e^w - e^{-w}}{e^w + e^{-w}}\n$$\n令 $w = z/2$：\n$$\n\\tanh(z/2) = \\dfrac{e^{z/2} - e^{-z/2}}{e^{z/2} + e^{-z/2}}\n$$\n分子分母同乘以 $e^{z/2}$：\n$$\n\\tanh(z/2) = \\dfrac{e^z - 1}{e^z + 1}\n$$\n现在，将此代入所讨论的表达式中：\n$$\n\\dfrac{1}{2}(\\tanh(z/2) + 1) = \\dfrac{1}{2}\\left(\\dfrac{e^z - 1}{e^z + 1} + 1\\right) = \\dfrac{1}{2}\\left(\\dfrac{e^z - 1 + e^z + 1}{e^z + 1}\\right) = \\dfrac{1}{2}\\left(\\dfrac{2e^z}{e^z + 1}\\right) = \\dfrac{e^z}{e^z + 1}\n$$\n分子分母同乘以 $e^{-z}$：\n$$\n\\dfrac{e^z \\cdot e^{-z}}{(e^z + 1) \\cdot e^{-z}} = \\dfrac{1}{1 + e^{-z}} = \\sigma(z)\n$$\n因此，恒等式 $\\sigma(z) = \\dfrac{1}{2}(\\tanh(z/2) + 1)$ 得以证实。指标 $D_{\\max}$ 衡量了在计算网格上所有参数 $z_{ik} = \\alpha(x_i - \\theta_k)$ 的这两个表达式时，由于浮点运算限制而产生的最大绝对差异。\n$$\nD_{\\max} = \\max_{i \\in \\{1,\\dots,N\\}, k \\in \\{1,\\dots,K\\}} \\left| \\sigma\\!\\left(\\alpha(x_i - \\theta_k)\\right) - \\dfrac{1}{2}\\left(\\tanh\\!\\left(\\dfrac{\\alpha(x_i - \\theta_k)}{2}\\right) + 1\\right) \\right|\n$$\n\n### 计算算法\n\n每个测试用例的计算过程如下：\n1.  **初始化**: 给定参数 $(\\alpha, \\{\\theta_k\\}_{k=1}^K, x_{\\min}, x_{\\max}, N)$，首先对阈值列表 $\\{\\theta_k\\}$ 进行排序，以确保顺序一致。\n2.  **网格生成**: 创建一个一维数组 `x`，包含从 $x_{\\min}$到 $x_{\\max}$（含端点）的 $N$ 个等距点。\n3.  **核心计算**: 为利用向量化操作提高效率，我们使用广播机制。将 `x` 重塑为 `(N, 1)`，将排序后的阈值 `thetas` 重塑为 `(1, K)`。\n    -   计算参数矩阵 $Z = [z_{ik}] = \\alpha (x - \\boldsymbol{\\theta}^T)$，其形状为 $(N, K)$。\n    -   计算S型函数值矩阵 $S = [\\sigma(z_{ik})]$。\n    -   通过沿 K 轴求和 $S$ 来计算近似值 $f_\\alpha$：`f_alpha = np.sum(S, axis=1)`。\n    -   使用 $H(0)=\\tfrac{1}{2}$ 的约定计算赫维赛德值矩阵 $H = [H(x_i - \\theta_k)]$。\n    -   通过沿 K 轴求和 $H$ 来计算目标值 $f^\\star$：`f_star = np.sum(H, axis=1)`。\n    -   计算误差向量 $e = f_\\alpha - f^\\star$。\n4.  **指标计算**:\n    -   **MSE**: 计算 `np.mean(e**2)`。\n    -   **$S_{\\max}$**: 计算S型函数导数矩阵 $S' = [\\alpha \\sigma(z_{ik})(1-\\sigma(z_{ik}))]$。沿 K 轴求和 $S'$ 得到 $f'_\\alpha(x_i)$ 值的向量。找到该向量中的最大绝对值。\n    -   **$\\|\\nabla_{\\boldsymbol{\\theta}} L\\|_2$**: 对于每个阈值 $\\theta_j$，通过计算误差向量 $e$ 与相应导数向量 $\\frac{\\partial f_\\alpha}{\\partial \\theta_j}$ 的点积，并乘以 $1/N$，来计算偏导数 $\\frac{\\partial L}{\\partial \\theta_j}$。向量 $\\frac{\\partial f_\\alpha}{\\partial \\theta_j}$ 是从预先计算的包含 $-\\alpha \\sigma(z_{ij})(1-\\sigma(z_{ij}))$ 的矩阵的第 $j$ 列获得的。将所有 $K$ 个偏导数收集到一个梯度向量中，并计算其欧几里得范数。\n    -   **$|\\partial L / \\partial \\alpha|$**: 首先计算矩阵 $[(x_i - \\theta_k) \\sigma(z_{ik})(1-\\sigma(z_{ik}))]$ 并沿 K 轴求和，以计算向量 $\\frac{\\partial f_\\alpha}{\\partial \\alpha}$。然后计算该向量与误差向量 $e$ 的点积，乘以 $1/N$，并取绝对值。\n    -   **$D_{\\max}$**: 计算差异矩阵 $|\\sigma(Z) - (\\frac{1}{2}(\\tanh(Z/2)+1))|$ 并找到其最大元素。\n5.  **输出格式化**: 将计算出的五个指标均四舍五入到 $6$ 位小数并存储。处理完所有测试用例后，将结果聚合成一个扁平列表，并以指定的逗号分隔格式打印在方括号内。\n\n这个详细的计划确保了所有问题要求，包括从第一性原理推导和向量化实现，都得到满足。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and analyzes differentiable multi-valued switches using logistic sigmoid functions.\n    \n    For each test case, it computes five metrics:\n    1. Mean Squared Error (MSE)\n    2. Maximum absolute slope (S_max)\n    3. Euclidean norm of the loss gradient w.r.t. thresholds (||grad L||_2)\n    4. Absolute value of the loss gradient w.r.t. steepness (|dL/d(alpha)|)\n    5. Maximum discrepancy between sigmoid and its tanh representation (D_max)\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (5.0, [-2.0, 0.0, 2.0], -6.0, 6.0, 10001),  # Test case 1 (general case)\n        (0.5, [-2.0, 0.0, 2.0], -6.0, 6.0, 10001),  # Test case 2 (small steepness)\n        (50.0, [-2.0, 0.0, 2.0], -6.0, 6.0, 10001), # Test case 3 (very large steepness)\n        (5.0, [-1.0, -1.0, 1.0], -6.0, 6.0, 10001), # Test case 4 (degenerate thresholds)\n        (5.0, [3.0, -3.0, 0.0], -6.0, 6.0, 10001),  # Test case 5 (unsorted thresholds)\n    ]\n\n    results = []\n\n    def sigmoid(z):\n        return 1.0 / (1.0 + np.exp(-z))\n\n    def heaviside(u):\n        h = np.zeros_like(u, dtype=float)\n        h[u > 0] = 1.0\n        h[u == 0] = 0.5\n        # h[u  0] is already 0.0\n        return h\n\n    for case in test_cases:\n        alpha, theta_list, x_min, x_max, N = case\n        \n        # Sort the threshold set as required\n        thetas = np.sort(np.array(theta_list))\n        K = len(thetas)\n\n        # Create a uniform grid of x samples\n        x = np.linspace(x_min, x_max, N)\n        \n        # Use broadcasting for vectorized computation\n        x_reshaped = x[:, np.newaxis]  # Shape (N, 1)\n        thetas_reshaped = thetas[np.newaxis, :] # Shape (1, K)\n\n        # --- Core Computations ---\n        z_ik = alpha * (x_reshaped - thetas_reshaped) # Shape (N, K)\n        s_k_values = sigmoid(z_ik)                   # Shape (N, K)\n        f_alpha = np.sum(s_k_values, axis=1)         # Shape (N,)\n\n        h_k_values = heaviside(x_reshaped - thetas_reshaped) # Shape (N, K)\n        f_star = np.sum(h_k_values, axis=1)                  # Shape (N,)\n\n        error = f_alpha - f_star # Shape (N,)\n\n        # Common derivative term: sigma'(z) = sigma(z)*(1-sigma(z))\n        sigma_prime_arg = s_k_values * (1 - s_k_values) # Shape (N, K)\n\n        # --- Metric 1: Mean Squared Error (MSE) ---\n        mse = np.mean(error**2)\n\n        # --- Metric 2: Maximum absolute slope (S_max) ---\n        df_dx_contrib = alpha * sigma_prime_arg  # Shape (N, K)\n        df_dx = np.sum(df_dx_contrib, axis=1)    # Shape (N,)\n        s_max = np.max(np.abs(df_dx))\n\n        # --- Metric 3: Gradient norm w.r.t. thresholds (||grad_theta L||_2) ---\n        grad_L_theta = np.zeros(K)\n        # From derivation: dL/d(theta_j) = (1/N) * sum_i(error_i * df_alpha_i/d(theta_j))\n        # df_alpha_i/d(theta_j) = -alpha * sigma'(alpha*(x_i - theta_j))\n        df_dtheta_contrib = -alpha * sigma_prime_arg # Shape (N, K)\n        for j in range(K):\n            grad_L_theta[j] = (1 / N) * np.sum(error * df_dtheta_contrib[:, j])\n        \n        norm_grad_theta = np.linalg.norm(grad_L_theta)\n\n        # --- Metric 4: Absolute gradient w.r.t. steepness (|dL/d(alpha)|) ---\n        # From derivation: dL/d(alpha) = (1/N) * sum_i(error_i * df_alpha_i/d(alpha))\n        # df_alpha_i/d(alpha) = sum_k((x_i - theta_k)*sigma'(alpha*(x_i - theta_k)))\n        df_dalpha_contrib = (x_reshaped - thetas_reshaped) * sigma_prime_arg # Shape (N, K)\n        df_dalpha = np.sum(df_dalpha_contrib, axis=1) # Shape (N,)\n        dL_dalpha = (1 / N) * np.sum(error * df_dalpha)\n        abs_dL_dalpha = np.abs(dL_dalpha)\n        \n        # --- Metric 5: Sigmoid-Tanh Discrepancy (D_max) ---\n        # Identity: sigma(z) = 0.5 * (tanh(z/2) + 1)\n        tanh_val = np.tanh(z_ik / 2.0)\n        sigma_from_tanh = 0.5 * (tanh_val + 1.0)\n        discrepancy_matrix = np.abs(s_k_values - sigma_from_tanh)\n        d_max = np.max(discrepancy_matrix)\n        \n        # Append formatted results for the current test case\n        results.append(f\"{mse:.6f}\")\n        results.append(f\"{s_max:.6f}\")\n        results.append(f\"{norm_grad_theta:.6f}\")\n        results.append(f\"{abs_dL_dalpha:.6f}\")\n        results.append(f\"{d_max:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3174535"}]}