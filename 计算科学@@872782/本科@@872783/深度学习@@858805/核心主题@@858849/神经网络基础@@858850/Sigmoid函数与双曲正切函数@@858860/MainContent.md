## 引言
在构建能够学习复杂模式的深度神经网络时，激活函数的选择是至关重要的一步。它们是赋予网络[非线性](@entry_id:637147)[表达能力](@entry_id:149863)的核心组件，使得模型能够超越简单的[线性变换](@entry_id:149133)，捕捉现实世界数据中丰富的层次化结构。在众多[激活函数](@entry_id:141784)中，逻辑[S型函数](@entry_id:137244)（Sigmoid）和[双曲正切函数](@entry_id:634307)（[tanh](@entry_id:636446)）作为早期被广泛应用的经典代表，其设计思想和行为特性至今仍对我们理解更现代的[网络架构](@entry_id:268981)具有深刻的指导意义。

然而，初学者常常只知其然，而不知其所以然：为何在特定场景下选择其一而非另一个？它们看似微小的数学差异，如何在训练过程中引发梯度消失等关键问题？又是什么让它们在[循环神经网络](@entry_id:171248)的[门控机制](@entry_id:152433)等高级应用中依然不可或缺？本文旨在系统性地回答这些问题，深入剖析这两种基础激活函数的内在机制与实践智慧。

为实现这一目标，本文将分为三个核心部分。首先，在“原理与机制”一章中，我们将从数学定义出发，揭示它们的导数性质、对称性及其对梯度传播的直接影响。接着，在“应用与跨学科联系”一章中，我们将探索它们在分类、[门控机制](@entry_id:152433)、[概率建模](@entry_id:168598)乃至统计物理和经济学等领域的广泛应用，展示其作为建模工具的普适性。最后，“动手实践”部分将提供一系列编程练习，让你在实践中巩固理论知识，亲身体验这些函数在模型设计中的具体作用。

通过本次学习，你将不仅掌握Sigmoid和[tanh函数](@entry_id:634307)的使用方法，更能深刻理解其背后的设计哲学，为未来学习更复杂的模型和激活函数打下坚实的基础。

## 原理与机制

在深度学习的工具箱中，[激活函数](@entry_id:141784)扮演着至关重要的角色，它们为[神经网](@entry_id:276355)络引入[非线性](@entry_id:637147)，使其能够学习和表示复杂的模式。在本章中，我们将深入探讨两种最早被广泛使用且至今仍在特定场景下发挥重要作用的[激活函数](@entry_id:141784)：**逻辑[S型函数](@entry_id:137244) (logistic sigmoid function)** 和 **[双曲正切函数](@entry_id:634307) (hyperbolic tangent function, [tanh](@entry_id:636446))**。我们将从它们的基本数学性质出发，逐步揭示它们在[神经计算](@entry_id:154058)、梯度动力学以及模型设计中的作用机制与实践考量。

### 基本数学性质

理解一个激活函数的行为，首先要从其数学定义和核心性质开始。

#### 定义与函数形式

**逻辑[S型函数](@entry_id:137244)**，通常简称为 **sigmoid函数**，其数学表达式为：
$$
\sigma(x) = \frac{1}{1 + \exp(-x)}
$$
其中 $x \in \mathbb{R}$ 是神经元的输入，即**前激活值 (pre-activation)** 或 **logit**。此函数将任意实数输入压缩到 $(0, 1)$ 的开区间内，这个特性使其在历史上常被用作表示概率的输出层[激活函数](@entry_id:141784)。

**[双曲正切函数](@entry_id:634307)**，简称 **[tanh函数](@entry_id:634307)**，定义如下：
$$
\tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}
$$
与 sigmoid 函数不同，[tanh](@entry_id:636446) 函数将任意实数输入压缩到 $(-1, 1)$ 的[开区间](@entry_id:157577)内。

#### 单调性与有界性

[激活函数](@entry_id:141784)的导数决定了梯度在反向传播过程中的行为。通过应用[微分](@entry_id:158718)的链式法则，我们可以求得 sigmoid 函数的导数：
$$
\sigma'(x) = \frac{d}{dx} \left( \frac{1}{1 + \exp(-x)} \right) = \frac{\exp(-x)}{(1 + \exp(-x))^2}
$$
这个表达式可以被巧妙地改写为原函数的形式。注意到 $1 - \sigma(x) = 1 - \frac{1}{1 + \exp(-x)} = \frac{\exp(-x)}{1 + \exp(-x)}$，我们得到一个简洁而重要的关系：
$$
\sigma'(x) = \sigma(x) (1 - \sigma(x))
$$
由于 $\sigma(x)$ 的值域为 $(0, 1)$，其导数 $\sigma'(x)$ 显然恒为正。这表明 sigmoid 函数在整个[实数域](@entry_id:151347)上是**严格单调递增**的。此外，通过求解导数的最大值，可以发现当 $\sigma(x) = 0.5$ (即 $x=0$) 时，导数取到其最大值 $\frac{1}{4}$。因此，sigmoid 函数的导[数值域](@entry_id:752817)为 $(0, \frac{1}{4}]$。[@problem_id:3174525]

同样，我们可以求得 [tanh](@entry_id:636446) 函数的导数：
$$
\tanh'(x) = 1 - \tanh^2(x)
$$
由于 $\tanh(x)$ 的值域为 $(-1, 1)$，其平方 $\tanh^2(x)$ 的值域为 $[0, 1)$。因此，导数 $\tanh'(x)$ 也恒为正，表明 [tanh](@entry_id:636446) 函数同样是**严格单调递增**的。其导数的最大值在 $x=0$ 处取得，为 $1$。

这两个函数的有界性是它们的核心特征。通过分析极限可以确认它们的输出范围：
- 当 $x \to +\infty$ 时，$\exp(-x) \to 0$，所以 $\sigma(x) \to \frac{1}{1+0} = 1$。
- 当 $x \to -\infty$ 时，$\exp(-x) \to +\infty$，所以 $\sigma(x) \to 0$。
- 当 $x \to \pm\infty$ 时，$\tanh(x) \to \pm 1$。

因此，sigmoid 和 [tanh](@entry_id:636446) 的值域分别为 $(0, 1)$ 和 $(-1, 1)$。这种将无限的输入空间映射到有限的输出区间的特性，是它们“压缩”或“挤压” (squashing) 名称的由来。[@problem_id:3174525]

#### 对称性

函数的对称性对其在[神经网](@entry_id:276355)络中的表现有重要影响。
- 对于 [tanh](@entry_id:636446) 函数，我们有：
  $$
  \tanh(-x) = \frac{\exp(-x) - \exp(x)}{\exp(-x) + \exp(x)} = - \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(x)} = -\tanh(x)
  $$
  这表明 [tanh](@entry_id:636446) 是一个**奇函数**，其图像关于原点 $(0, 0)$ 对称。这意味着其输出是**零中心 (zero-centered)** 的。

- 对于 sigmoid 函数，我们有：
  $$
  \sigma(-x) = \frac{1}{1 + \exp(x)} = \frac{\exp(-x)}{e^{-x}(1 + \exp(x))} = \frac{\exp(-x)}{\exp(-x) + 1} = 1 - \frac{1}{1 + \exp(-x)} = 1 - \sigma(x)
  $$
  这个关系表明 sigmoid 函数的图像关于点 $(0, \frac{1}{2})$ 中心对称，但它既不是奇函数也不是偶函数。其输出的值域 $(0, 1)$ 并不以零为中心。[@problem_id:3174525] 这一差异在网络训练的动力学中具有深远的影响，我们稍后会详细讨论。

#### 代数关联

尽管 sigmoid 和 [tanh](@entry_id:636446) 的形式不同，但它们之间存在一个简单的线性关系。从 [tanh](@entry_id:636446) 的定义出发，将分子分母同除以 $\exp(x)$：
$$
\tanh(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} = \frac{2 - (1 + \exp(-2x))}{1 + \exp(-2x)} = \frac{2}{1 + \exp(-2x)} - 1
$$
我们发现，表达式中的 $\frac{1}{1 + \exp(-2x)}$ 正是 $\sigma(2x)$。因此，我们得到了一个关键的恒等式：
$$
\tanh(x) = 2\sigma(2x) - 1
$$
这个关系揭示了 [tanh](@entry_id:636446) 本质上只是一个经过缩放和平移的 sigmoid 函数。这意味着，一个使用 [tanh](@entry_id:636446) 作为激活函数的[神经网](@entry_id:276355)络层，原则上可以通过调整其权重和偏置，被精确地转换成一个功能完[全等](@entry_id:273198)价的、使用 sigmoid 作为[激活函数](@entry_id:141784)的网络层，反之亦然。例如，一个形如 $y = U \tanh(Wx+b) + c$ 的网络层，可以被重写为 $y = (2U)\sigma(2Wx + 2b) + (c - U\mathbf{1})$ 的形式，其中 $\mathbf{1}$ 是全一向量。尽管网络的整体输入输出映射保持不变，但其内部的**隐藏表示 (hidden representation)** 已经从一个零中心的 $(-1, 1)$ 空间被转换到了一个非零中心的 $(0, 1)$ 空间。[@problem_id:3174577]

### 在[神经计算](@entry_id:154058)与梯度动力学中的作用

激活函数的选择直接影响[神经网](@entry_id:276355)络的学习过程，尤其是梯度的传播和收敛动态。

#### 饱和问题与梯度消失

**饱和 (saturation)** 是指当激活函数的输入值变得非常大（正向或负向）时，其输出值趋于一个常数，函数曲线变得平坦。从导数公式 $\sigma'(x) = \sigma(x)(1 - \sigma(x))$ 和 $\tanh'(x) = 1 - \tanh^2(x)$ 可以看出，当 $|x| \to \infty$ 时，这两个函数的导数都趋向于 $0$。

在深度神经网络中，梯度的计算依赖于[链式法则](@entry_id:190743)。从输出层到输入层的梯度是各层导数的连乘。如果网络中许多神经元都工作在饱和区，它们的局部梯度（即激活函数的导数）将非常小。这些接近零的数值在反向传播过程中连乘，会导致最终的梯度信号呈指数级衰减，变得极其微弱。这个问题被称为**梯度消失 (vanishing gradients)**，它会严重阻碍深层网络的训练，使得远离输出层的网络参数几乎无法得到更新。

我们可以通过一个简化的[概率模型](@entry_id:265150)来更形式化地理解这个问题。假设一个深度为 $d$ 的网络，梯度沿某条路径的衰减因子可近似为 $\prod_{i=1}^{d} \tanh'(z_i)$。如果每一层的预激活值 $z_i$ 有至少 $p$ 的概率落入[饱和区](@entry_id:262273)（例如 $|z_i| \ge a$），那么单层梯度的[期望值](@entry_id:153208)将被一个小于1的因子 $1 - p\tanh^2(a)$ 所约束。经过 $d$ 层之后，总的梯度期望将以 $(1 - p\tanh^2(a))^d$ 的速度指数级地衰减至零。[@problem_id:3174494] 这定量地说明了为什么饱和[激活函数](@entry_id:141784)与[网络深度](@entry_id:635360)之间存在固有的紧张关系。

#### 损失函数的影响

梯度消失不仅与[激活函数](@entry_id:141784)自身有关，还与其和[损失函数](@entry_id:634569)的相互作用紧密相连。这一点在[分类任务](@entry_id:635433)的输出层尤为突出。

考虑一个使用 sigmoid 输出的[二元分类](@entry_id:142257)器，其输出 $p = \sigma(z)$ 表示预测为正类的概率。如果使用**[均方误差](@entry_id:175403) (Mean Squared Error, MSE)** 作为[损失函数](@entry_id:634569) $L_{MSE} = \frac{1}{2}(p-y)^2$，其中 $y \in \{0, 1\}$ 是真实标签。根据链式法则，损失对 logit $z$ 的梯度为：
$$
\frac{\partial L_{MSE}}{\partial z} = \frac{\partial L_{MSE}}{\partial p} \cdot \frac{dp}{dz} = (p-y) \cdot \sigma'(z) = (\sigma(z)-y)\sigma(z)(1-\sigma(z))
$$
关键在于梯度表达式中包含了 $\sigma'(z)$ 这一项。当神经元做出一个非常自信但错误的预测时（例如 $z \to +\infty$ 导致 $\sigma(z) \to 1$，但真实标签 $y=0$），$\sigma'(z)$ 因子将趋近于 $0$，导致整个梯度消失。这意味着模型“错了但很自信”，却几乎收不到任何学习信号来纠正错误，从而使训练陷入停滞。[@problem_id:3174561] [@problem_id:3174495]

相比之下，如果我们使用**[二元交叉熵](@entry_id:636868) (Binary Cross-Entropy, BCE)** 作为损失函数 $L_{BCE} = -[y \ln(p) + (1-y)\ln(1-p)]$，情况则大为改观。损失对 logit $z$ 的梯度为：
$$
\frac{\partial L_{BCE}}{\partial p} = -\left( \frac{y}{p} - \frac{1-y}{1-p} \right) = \frac{p-y}{p(1-p)}
$$
$$
\frac{\partial L_{BCE}}{\partial z} = \frac{\partial L_{BCE}}{\partial p} \cdot \frac{dp}{dz} = \frac{\sigma(z)-y}{\sigma(z)(1-\sigma(z))} \cdot \sigma(z)(1-\sigma(z)) = \sigma(z) - y
$$
这个结果非常简洁。sigmoid 函数的导数项被完美地抵消了。现在，当模型做出自信但错误的预测时（例如 $\sigma(z) \to 1$ 但 $y=0$），梯度 $\frac{\partial L_{BCE}}{\partial z} \to 1-0=1$。这个非零的、大小为 $1$ 的梯度提供了一个强有力的[纠错](@entry_id:273762)信号。因此，BCE 损失函数与 sigmoid 输出层的组合，能够有效缓解在输出层的梯度饱和问题，这也是它们成为[分类任务](@entry_id:635433)标准配置的根本原因。[@problem_id:3174495]

#### 近[线性区](@entry_id:276444)与初始化

尽管饱和区存在[梯度消失问题](@entry_id:144098)，但在原点附近，sigmoid 和 [tanh](@entry_id:636446) 函数的行为近似于线性函数。通过在 $x=0$ 处进行一阶[泰勒展开](@entry_id:145057)，我们得到：
- $\tanh(x) \approx \tanh(0) + \tanh'(0)x = 0 + 1 \cdot x = x$
- $\sigma(x) \approx \sigma(0) + \sigma'(0)x = \frac{1}{2} + \frac{1}{4}x$

在[神经网络初始化](@entry_id:637333)的早期阶段，让大多数神经元工作在这个**近[线性区](@entry_id:276444) (near-linear regime)** 是至关重要的，这样可以确保梯度能够有效地在网络中传播。著名的[权重初始化](@entry_id:636952)方法，如 Glorot (Xavier) 初始化，其核心思想之一就是通过控制权重的[方差](@entry_id:200758)，使得每层激活值的[方差保持](@entry_id:634352)稳定，从而避免信号过早地被放大或缩小，将神经元推向[饱和区](@entry_id:262273)。

两种激活函数在近[线性区](@entry_id:276444)的斜率差异（$\tanh$ 为 $1$，$\sigma$ 为 $1/4$）直接影响了合适的[权重初始化](@entry_id:636952)策略。例如，为了保持信号[方差](@entry_id:200758)不变，使用中心化 sigmoid（即 $\sigma(x) - 1/2 \approx \frac{1}{4}x$）的网络层，其权重的[方差](@entry_id:200758)需要被设置为使用 [tanh](@entry_id:636446) 网络的 $16$ 倍，以补偿其较小的斜率。[@problem_id:3174539] 更精确地，对于均值为零、[方差](@entry_id:200758)为 $\sigma_z^2$ 的小幅预激活值 $z$，平均梯度强度可以近似为 $\mathbb{E}[\tanh'(z)] \approx 1 - \sigma_z^2$ 和 $\mathbb{E}[\sigma'(z)] \approx \frac{1}{4} - \frac{1}{16}\sigma_z^2$。这进一步证实，在初始化阶段，[tanh](@entry_id:636446) 能够比 sigmoid 传递更强的梯度信号。[@problem_id:3174499]

### 实践应用与设计模式

基于上述原理，我们可以总结出一些在实践中行之有效的[网络设计](@entry_id:267673)模式和技术。

#### 隐藏层与输出层的[激活函数](@entry_id:141784)选择

一个常见且高效的设计模式是：在**隐藏层使用 [tanh](@entry_id:636446)**，而在**输出层使用 sigmoid**。[@problem_id:3174499]

- **隐藏层**: [tanh](@entry_id:636446) 的主要优势在于其**零中心**的输出。当隐藏层的输入（即前一层的激活值）也是零中心时，权重的梯度更新会更加高效。相比之下，sigmoid 的输出总是正的，这会导致下一层权重的梯度要么全部为正，要么全部为负（取决于反向传播的误差项符号），造成所谓的“Z字形”更新路径，降低了学习效率。[tanh](@entry_id:636446) 的零中心特性打破了这种束缚，使得学习过程更加平滑。[@problem_id:3174499] [@problem_id:3174527]

- **输出层**: 对于[二元分类](@entry_id:142257)问题，目标是输出一个概率。sigmoid 函数的 $(0, 1)$ 值域天然地满足了这一需求。如前所述，它与[交叉熵损失](@entry_id:141524)函数的完美结合，提供了稳定且高效的梯度，使其成为概率输出层的标准选择。[@problem_id:3174499] [@problem_id:3174495]

#### 与[归一化层](@entry_id:636850)的交互

**[批量归一化](@entry_id:634986) (Batch Normalization, BN)** 是一种旨在[稳定训练](@entry_id:635987)过程、加速收敛的技术。它通过对每一批数据的神经元输入进行标准化（使其具有零均值和单位[方差](@entry_id:200758)）来实现这一目标。BN 与饱和型激活函数的交互尤其值得关注。

将 BN 应用在[激活函数](@entry_id:141784)**之前**（即对预激活值 $z$ 进行归一化）是关键所在。这样做可以直接控制激活函数的输入，使其大部分保持在函数的动态、非饱和区域内，从而有效防止梯度消失。如果将 BN 应用在[激活函数](@entry_id:141784)**之后**，那么当输入已经饱和时，梯度信息已经丢失，此时再对饱和的输出进行归一化已为时已晚。[@problem_id:3174527]

此外，预激活 BN 进一步凸显了 [tanh](@entry_id:636446) 在隐藏层中的优势。由于 BN 强制使预激活值 $z$ 的[分布](@entry_id:182848)以 $0$ 为中心，这正好匹配了 [tanh](@entry_id:636446) 函数的对称性，使其输出的激活值也能保持零中心，从而优化下一层的学习。对于 sigmoid 而言，即使输入是零中心的，其输出的均值也会是 $0.5$ 左右，无法获得[零中心激活](@entry_id:636117)值带来的好处。[@problem_id:3174527]

#### 针对饱和的[正则化技术](@entry_id:261393)

尽管有 BN 等技术，过度拟合和不恰当的权重更新仍可能将神经元推向饱和区。为此，研究者们开发了多种[正则化技术](@entry_id:261393)。

- **[标签平滑](@entry_id:635060) (Label Smoothing)**：在[分类任务](@entry_id:635433)中，使用 $\{0, 1\}$ 这样的**硬标签 (hard labels)** 会激励模型输出无限大的 logit（$z \to \pm \infty$）以使预测概率无限接近 $0$ 或 $1$。这种对无限值的追求是导致整个网络饱和的驱动力。[标签平滑](@entry_id:635060)通过使用**软标签 (soft labels)**，如 $\{0.1, 0.9\}$，来缓解这一问题。此时，为了最小化[交叉熵损失](@entry_id:141524)，模型的最优 logit 目标不再是无穷大，而是一个有限值 $z^\star = \ln(\frac{y}{1-y})$（例如，对于 $y=0.9$，最优 $z^\star = \ln(9)$）。这个有限的目标 logit 阻止了模型变得“过分自信”，从而减轻了对隐藏层神经元饱和的压力，保持了梯度流的通畅。[@problem_id:3174512]

- **直接激活正则化**：当网络中大量神经元的激活值都集中在 $\pm 1$ 附近时，我们称之为**表示坍塌 (representation collapse)**，因为此时隐藏层失去了区分不同输入的能力。我们可以通过在[损失函数](@entry_id:634569)中添加正则化项来直接惩罚这种行为。例如，可以设计一个**激活[熵正则化](@entry_id:749012)器**，通过计算激活值[分布](@entry_id:182848)的熵来鼓励其多样性，惩罚低熵的尖峰[分布](@entry_id:182848)；或者设计一个**导数惩罚项**，直接对那些导数值 $1 - a^2$ 过小的激活值 $a$ 进行惩罚，激励神经元保持在更具响应性的状态。[@problem_id:3174536]

#### [参数化](@entry_id:272587)受限变量

除了作为激活函数，sigmoid 和 [tanh](@entry_id:636446) 的有界性也使它们成为在模型中对有界参数进行建模的有力工具。在许多[统计模型](@entry_id:165873)或物理模型中，参数需要满足特定约束，例如为正或位于某个区间内。通过使用这些函数，我们可以让[神经网](@entry_id:276355)络输出一个无约束的实数 $z$，然后通过一个固定的、可微的变换将其映射到所需的有界区间。

- 若需要一个参数 $\lambda$ 严格为正，且小于一个常数 $c$，可以令 $\lambda = c \cdot \sigma(z)$。
- 若需要一个参数 $m$ 位于区间 $(a, b)$ 内，可以使用 $m = a + (b-a)\sigma(z)$，或者更对称的方式 $m = \frac{a+b}{2} + \frac{b-a}{2}\tanh(z)$。

这种方法确保了无论[神经网](@entry_id:276355)络的内部输出 $z$ 为何值，最终的参数始终满足约束，同时整个[计算图](@entry_id:636350)保持可微，使得端到端的梯度优化成为可能。[@problem_id:3174525]