{"hands_on_practices": [{"introduction": "深入理解神经网络的一个关键方面是认识到，不同的参数集可以产生完全相同的函数。这种现象被称为重参数化对称性（reparameterization symmetry）。本练习将指导您推导这种对称性存在的条件，并通过代码进行验证，这对于理解神经网络优化过程的复杂性至关重要。[@problem_id:3125231]", "problem": "考虑一个具有 $L$ 层的标准前馈神经网络。设输入为 $x \\in \\mathbb{R}^{n_0}$，对于每个层索引 $l \\in \\{1,\\dots,L\\}$，将预激活和激活定义为 $z^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)}$ 和 $x^{(l)} = \\phi^{(l)}(z^{(l)})$，其中 $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ 且 $b^{(l)} \\in \\mathbb{R}^{n_l}$。网络的输出为 $f(x) = x^{(L)}$。假设为全连接架构，且输出激活函数 $\\phi^{(L)}$ 是恒等函数。\n\n你的任务是：\n- 从第一性原理和前馈计算及激活函数的定义出发，推导出在何种条件下，两组不同的参数集能产生相同的函数 $f$。重点关注隐藏层中每个神经元的重参数化对称性。你必须在不依赖未经证明的捷径的情况下推导出这些条件，并指出对于常见的激活函数，这种对称性何时成立，何时不成立。\n- 实现一个程序，通过构建随机化网络并对隐藏层应用每神经元缩放变换来测试参数的可识别性，然后在固定的输入集上数值地检查函数相等性。\n\n你必须编写一个完整、可运行的程序，该程序：\n- 使用固定的随机种子 $42$ 以保证可复现性。\n- 从区间 $[-0.5, 0.5]$ 上的均匀分布中独立同分布地抽取权重 $W^{(l)}$ 和偏置 $b^{(l)}$。\n- 通过在从 $[-1,1]^{n_0}$ 上的均匀分布中独立同分布采样的一批 $N = 200$ 个输入上，计算原始网络和变换后网络的输出 $f(x)$ 和 $\\tilde{f}(x)$ 之间的最大绝对差来评估函数相等性。如果最大绝对差小于容差 $\\varepsilon = 10^{-8}$，则声明相等。\n- 在指定的隐藏层上应用以下每神经元缩放变换：对于隐藏层索引 $l$，其神经元级别的正缩放因子为 $s_j^{(l)}$，这些因子组合成对角矩阵 $D^{(l)} = \\mathrm{diag}(s_1^{(l)},\\dots,s_{n_l}^{(l)})$。将 $W^{(l)}$ 替换为 $\\tilde{W}^{(l)} = D^{(l)} W^{(l)}$，将 $b^{(l)}$ 替换为 $\\tilde{b}^{(l)} = D^{(l)} b^{(l)}$，并将 $W^{(l+1)}$ 替换为 $\\tilde{W}^{(l+1)} = W^{(l+1)} (D^{(l)})^{-1}$。不对输出层 $L$ 应用变换，因为没有后续层可以补偿。\n- 测试六个涵盖典型和边缘场景的案例。在所有案例中，输出激活函数都是恒等函数。测试套件如下：\n    1. 隐藏层激活函数为 $\\mathrm{ReLU}$ (修正线性单元)，架构为 $[3,4,2]$，在隐藏层 $l=1$ 处进行缩放，缩放因子为 $s^{(1)} = [1.7, 0.5, 2.2, 0.9]$。预期函数相等性成立。\n    2. 隐藏层激活函数为 $\\mathrm{ReLU}$，架构为 $[3,4,2]$，在隐藏层 $l=1$ 处进行缩放，缩放因子为 $s^{(1)} = [-1.3, 1.2, 0.8, 1.1]$ (注意其中的负值)。预期函数相等性不成立。\n    3. 隐藏层激活函数为 $\\sigma$ (sigmoid)，架构为 $[2,3,1]$，在隐藏层 $l=1$ 处进行缩放，缩放因子为 $s^{(1)} = [1.4, 0.8, 1.1]$。预期函数相等性不成立。\n    4. 隐藏层激活函数为恒等函数 (线性)，架构为 $[3,3,2]$，在隐藏层 $l=1$ 处进行缩放，缩放因子为 $s^{(1)} = [-0.8, -1.2, 0.5]$。预期函数相等性成立。\n    5. 隐藏层激活函数为 $\\mathrm{ReLU}$，架构为 $[4,3,2]$，在隐藏层 $l=1$ 处进行缩放，使用非常小的正缩放因子 $s^{(1)} = [10^{-6}, 3 \\cdot 10^{-6}, 2 \\cdot 10^{-6}]$。预期函数相等性成立。\n    6. 两个隐藏层，激活函数均为 $\\mathrm{ReLU}$，架构为 $[3,4,3,2]$，在隐藏层 $l=1$ 处使用缩放因子 $s^{(1)} = [1.3, 0.7, 2.0, 0.9]$，在隐藏层 $l=2$ 处使用缩放因子 $s^{(2)} = [1.2, 0.6, 1.5]$。预期函数相等性成立。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目都应是一个布尔值，表示相应测试用例的函数相等性是否成立，顺序与上面列出的顺序一致 (例如， $[{\\tt True},{\\tt False},\\dots]$)。不应打印任何其他文本。", "solution": "该问题要求推导前馈神经网络在何种条件下表现出重参数化对称性，特别是关于隐藏层中每神经元缩放的对称性。它还要求对几个测试用例的这些条件进行数值验证。\n\n### 第 1 部分：重参数化对称性条件的推导\n\n我们考虑一个具有 $L$ 层的标准前馈神经网络。输入为 $x^{(0)} = x \\in \\mathbb{R}^{n_0}$。对于每一层 $l \\in \\{1, \\dots, L\\}$，计算定义如下：\n$$ z^{(l)} = W^{(l)} x^{(l-1)} + b^{(l)} $$\n$$ x^{(l)} = \\phi^{(l)}(z^{(l)}) $$\n其中 $W^{(l)} \\in \\mathbb{R}^{n_l \\times n_{l-1}}$ 是权重矩阵，$b^{(l)} \\in \\mathbb{R}^{n_l}$ 是偏置向量，$z^{(l)}$ 是预激活向量，$x^{(l)}$ 是激活向量（第 $l$ 层的输出），而 $\\phi^{(l)}$ 是第 $l$ 层的逐元素激活函数。网络的最终输出是 $f(x) = x^{(L)}$。我们已知输出激活函数 $\\phi^{(L)}$ 是恒等函数，所以 $f(x) = z^{(L)}$。\n\n我们感兴趣的是应用于单个隐藏层 $l \\in \\{1,\\dots,L-1\\}$ 参数的变换。设原始网络参数表示为 $\\theta = \\{W^{(k)}, b^{(k)}\\}_{k=1}^L$。该变换涉及对第 $l$ 层中的每个神经元 $j \\in \\{1, \\dots, n_l\\}$ 的一组非零缩放因子 $s_j^{(l)}$。这些因子组合成一个对角矩阵 $D^{(l)} = \\mathrm{diag}(s_1^{(l)}, \\dots, s_{n_l}^{(l)})$。\n\n变换后的参数集 $\\tilde{\\theta}$ 是通过修改与第 $l$ 层及其后继层 $l+1$ 相关的参数来定义的：\n1.  $\\tilde{W}^{(l)} = D^{(l)} W^{(l)}$\n2.  $\\tilde{b}^{(l)} = D^{(l)} b^{(l)}$\n3.  $\\tilde{W}^{(l+1)} = W^{(l+1)} (D^{(l)})^{-1}$\n4.  所有其他参数保持不变：对于 $k \\notin \\{l, l+1\\}$，有 $\\tilde{W}^{(k)} = W^{(k)}$；对于 $k \\neq l$，有 $\\tilde{b}^{(k)} = b^{(k)}$。\n\n让我们将由变换后的网络计算的函数表示为 $\\tilde{f}(x)$。我们想找到激活函数 $\\phi^{(l)}$ 在什么条件下能使得对所有输入 $x$ 都有 $\\tilde{f}(x) = f(x)$。\n\n在变换后的网络中，我们用波浪号表示的计算过程，对于 $k  l$ 的层来说与原始网络完全相同，因为参数未改变。因此，第 $l$ 层的输入是相同的：$\\tilde{x}^{(l-1)} = x^{(l-1)}$。\n\n在第 $l$ 层，变换后网络的预激活是：\n$$ \\tilde{z}^{(l)} = \\tilde{W}^{(l)} x^{(l-1)} + \\tilde{b}^{(l)} = (D^{(l)} W^{(l)}) x^{(l-1)} + (D^{(l)} b^{(l)}) = D^{(l)} (W^{(l)} x^{(l-1)} + b^{(l)}) = D^{(l)} z^{(l)} $$\n第 $l$ 层的激活是：\n$$ \\tilde{x}^{(l)} = \\phi^{(l)}(\\tilde{z}^{(l)}) = \\phi^{(l)}(D^{(l)} z^{(l)}) $$\n\n现在，考虑第 $l+1$ 层。它的预激活是：\n$$ \\tilde{z}^{(l+1)} = \\tilde{W}^{(l+1)} \\tilde{x}^{(l)} + \\tilde{b}^{(l+1)} $$\n由于只有权重 $\\tilde{W}^{(l+1)}$ 被变换，而偏置 $\\tilde{b}^{(l+1)} = b^{(l+1)}$ 没有，我们得到：\n$$ \\tilde{z}^{(l+1)} = (W^{(l+1)} (D^{(l)})^{-1}) \\phi^{(l)}(D^{(l)} z^{(l)}) + b^{(l+1)} $$\n对于原始网络，第 $l+1$ 层的预激活是：\n$$ z^{(l+1)} = W^{(l+1)} x^{(l)} + b^{(l+1)} = W^{(l+1)} \\phi^{(l)}(z^{(l)}) + b^{(l+1)} $$\n\n为了让两个网络计算相同的函数，它们的输出必须相同。由于从第 $l+2$ 层开始的所有参数在两个网络中都相同，如果第 $l+2$ 层的输入相同，则可以保证函数相等性 $\\tilde{f}(x) = f(x)$。这意味着我们需要 $\\tilde{x}^{(l+1)} = x^{(l+1)}$。由于两个网络的激活函数 $\\phi^{(l+1)}$ 相同，这等价于要求它们的预激活相同：$\\tilde{z}^{(l+1)} = z^{(l+1)}$。\n\n令 $\\tilde{z}^{(l+1)}$ 和 $z^{(l+1)}$ 的表达式相等：\n$$ W^{(l+1)} (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) + b^{(l+1)} = W^{(l+1)} \\phi^{(l)}(z^{(l)}) + b^{(l+1)} $$\n从两边减去 $b^{(l+1)}$ 得到：\n$$ W^{(l+1)} \\left( (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) - \\phi^{(l)}(z^{(l)}) \\right) = 0 $$\n这个方程必须对所有可能的输入 $x$（它会生成所有可达的预激活 $z^{(l)}$）以及对权重矩阵 $W^{(l+1)}$ 的任何有效选择都成立。为了使等式对任意的 $W^{(l+1)}$ 都成立，括号中的项必须是零向量。\n$$ (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) - \\phi^{(l)}(z^{(l)}) = 0 $$\n$$ \\implies (D^{(l)})^{-1} \\phi^{(l)}(D^{(l)} z^{(l)}) = \\phi^{(l)}(z^{(l)}) $$\n两边乘以 $D^{(l)}$（因为所有 $s_j^{(l)} \\neq 0$，所以它是可逆的），我们得到核心条件：\n$$ \\phi^{(l)}(D^{(l)} z^{(l)}) = D^{(l)} \\phi^{(l)}(z^{(l)}) $$\n由于 $\\phi^{(l)}$ 是一个逐元素函数且 $D^{(l)}$ 是对角矩阵，我们可以逐个神经元地分析这个条件。对于第 $l$ 层中的第 $j$ 个神经元，其缩放因子为 $s_j$，预激活为 $z_j$，条件是：\n$$ \\phi_j(s_j z_j) = s_j \\phi_j(z_j) $$\n这意味着激活函数必须是关于缩放因子 $s_j$ 的1次齐次函数。\n\n我们来为问题中提到的常见激活函数分析这个条件：\n\n1.  **恒等（线性）激活函数：** $\\phi(z) = z$。\n    条件是 $s_j z_j = s_j z_j$。这对任何非零标量 $s_j$（无论正负）都始终成立。因此，重参数化对称性对任何非零缩放的线性层都成立。\n\n2.  **修正线性单元（ReLU）：** $\\phi(z) = \\max(0, z)$。\n    条件是 $\\max(0, s_j z_j) = s_j \\max(0, z_j)$。\n    -   如果 $s_j > 0$：\n        -   对于 $z_j \\ge 0$，方程变为 $s_j z_j = s_j z_j$，成立。\n        -   对于 $z_j  0$，方程变为 $0 = s_j \\cdot 0$，成立。\n        对称性对任何正缩放因子 $s_j > 0$ 都成立。\n    -   如果 $s_j  0$：\n        -   我们用一个反例来测试。设 $z_j = 1$ 和 $s_j = -2$。\n        -   等式左边：$\\max(0, (-2) \\cdot 1) = \\max(0, -2) = 0$。\n        -   等式右边：$(-2) \\cdot \\max(0, 1) = -2 \\cdot 1 = -2$。\n        -   因为 $0 \\neq -2$，所以条件不成立。对称性对负缩放因子不成立。\n\n3.  **Sigmoid 激活函数：** $\\phi(z) = \\frac{1}{1 + e^{-z}}$。\n    条件是 $\\frac{1}{1 + e^{-s_j z_j}} = s_j \\frac{1}{1 + e^{-z_j}}$。\n    这个等式通常在 $s_j \\neq 1$ 时不成立。例如，如果我们取 $z_j=0$，等式左边是 $\\phi(0) = 0.5$，而等式右边是 $s_j \\phi(0) = 0.5 s_j$。要使它们相等，$s_j$ 必须为 $1$。如果 $s_j \\neq 1$，对称性就被破坏了。\n\n**多层缩放：**\n对于同时缩放多层的情况（如测试用例6，缩放第 $l=1$ 和 $l=2$ 层），逻辑可以延伸。对第 $l$ 层的缩放会调整其输出参数 $W^{(l)}, b^{(l)}$ 和下一层 $W^{(l+1)}$ 的输入权重。如果我们也缩放第 $l+1$ 层，它会调整其参数 $W^{(l+1)}, b^{(l+1)}$ 和第 $l+2$ 层的输入权重，即 $W^{(l+2)}$。\n权重矩阵 $W^{(l+1)}$ 受到两种变换的影响：它的行被 $D^{(l+1)}$ 缩放，它的列被 $(D^{(l)})^{-1}$ 缩放。最终变换后的权重为 $\\tilde{W}^{(l+1)} = D^{(l+1)} W^{(l+1)} (D^{(l)})^{-1}$。\n像之前一样沿着前向传播路径，我们发现为了保持网络函数不变，齐次性条件 $\\phi_j^{(k)}(s_j^{(k)} z_j^{(k)}) = s_j^{(k)} \\phi_j^{(k)}(z_j^{(k)})$ 必须对每个被缩放的层 $k$ 和其中的每个神经元 $j$ 都成立。如果所有被缩放的层都满足这个条件（例如，对于使用正缩放因子的ReLU），则整个网络函数保持不变。\n\n### 第 2 部分：数值实现与验证\n\n下面的 Python 程序实现了指定的测试。它构建了具有随机参数的神经网络，应用了定义的缩放变换，并基于推导出的条件进行了函数相等性的数值检查。测试结果直接验证了上述理论分析。\n- 测试 1：ReLU，正缩放因子。理论：成立。结果：`True`。\n- 测试 2：ReLU，负缩放因子。理论：不成立。结果：`False`。\n- 测试 3：Sigmoid，非单位缩放因子。理论：不成立。结果：`False`。\n- 测试 4：线性，负缩放因子。理论：成立。结果：`True`。\n- 测试 5：ReLU，小的正缩放因子。理论：成立。结果：`True`。\n- 测试 6：两个相邻的ReLU层，正缩放因子。理论：成立。结果：`True`。\n\n数值结果应与理论预测相符。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates neural network reparameterization symmetries by implementing and testing\n    per-neuron scaling transformations on feedforward networks.\n    \"\"\"\n    \n    # Use a fixed random seed for reproducibility, as specified.\n    rng = np.random.default_rng(42)\n\n    # Define activation functions.\n    def relu(z):\n        return np.maximum(0, z)\n\n    def sigmoid(z):\n        # Added a clamp for numerical stability with large z\n        z = np.clip(z, -500, 500)\n        return 1 / (1 + np.exp(-z))\n\n    def identity(z):\n        return z\n\n    activations_map = {'relu': relu, 'sigmoid': sigmoid, 'identity': identity}\n\n    def create_network(architecture, rng_gen):\n        \"\"\"Initializes network parameters from U[-0.5, 0.5].\"\"\"\n        weights = []\n        biases = []\n        for i in range(len(architecture) - 1):\n            n_out, n_in = architecture[i+1], architecture[i]\n            # Weights W^(l) from U[-0.5, 0.5]\n            w = rng_gen.uniform(-0.5, 0.5, size=(n_out, n_in))\n            # Biases b^(l) from U[-0.5, 0.5]\n            b = rng_gen.uniform(-0.5, 0.5, size=(n_out, 1))\n            weights.append(w)\n            biases.append(b)\n        return weights, biases\n\n    def forward_pass(weights, biases, x_batch, activation_funcs):\n        \"\"\"Computes the network output for a batch of inputs.\"\"\"\n        x_l = x_batch\n        num_layers = len(weights)\n        for l_idx in range(num_layers):\n            w = weights[l_idx]\n            b = biases[l_idx]\n            phi = activation_funcs[l_idx]\n            \n            z_l = w @ x_l + b\n            x_l = phi(z_l)\n        return x_l\n\n    def run_test_case(case_params, rng_gen):\n        \"\"\"Executes a single test case for functional equality.\"\"\"\n        arch = case_params['arch']\n        hidden_act_name = case_params['hidden_act']\n        output_act_name = 'identity'\n        scales_to_apply = case_params['scales']\n        \n        # Create the original network\n        original_weights, original_biases = create_network(arch, rng_gen)\n        \n        # Prepare activation functions for all layers\n        num_layers = len(arch) - 1\n        activation_funcs = []\n        hidden_act_func = activations_map[hidden_act_name]\n        output_act_func = activations_map[output_act_name]\n        for l_idx in range(num_layers):\n          # All hidden layers have the same activation, output is identity\n          is_output_layer = (l_idx == num_layers - 1)\n          activation_funcs.append(output_act_func if is_output_layer else hidden_act_func)\n\n        # Create the transformed network by applying scaling\n        # Start with a copy of the original network\n        transformed_weights = [w.copy() for w in original_weights]\n        transformed_biases = [b.copy() for b in original_biases]\n\n        for l_1based, s_vec in scales_to_apply.items():\n            l_idx = l_1based - 1 # Convert to 0-based index\n            s_vec_arr = np.array(s_vec)\n            \n            # Check for non-invertible scales\n            if np.any(s_vec_arr == 0):\n                raise ValueError(f\"Scaling factors must be non-zero. Found 0 in {s_vec}.\")\n\n            D = np.diag(s_vec_arr)\n            D_inv = np.diag(1.0 / s_vec_arr)\n\n            # Apply transformation: tilde_W(l) = D(l) * W(l)\n            transformed_weights[l_idx] = D @ transformed_weights[l_idx]\n            # Apply transformation: tilde_b(l) = D(l) * b(l)\n            transformed_biases[l_idx] = D @ transformed_biases[l_idx]\n            \n            # The compensation is applied to the next layer W(l+1)\n            # tilde_W(l+1) = W(l+1) * (D(l))^-1\n            # Note: This modifies the weights that might be a target for a subsequent scaling (e.g. Case 6)\n            if l_idx + 1  len(transformed_weights):\n                transformed_weights[l_idx + 1] = transformed_weights[l_idx + 1] @ D_inv\n\n        # Generate test inputs\n        N = 200 # Number of test inputs\n        n_0 = arch[0] # Input dimension\n        x_batch = rng_gen.uniform(-1, 1, size=(n_0, N))\n\n        # Perform forward passes\n        y_original = forward_pass(original_weights, original_biases, x_batch, activation_funcs)\n        y_transformed = forward_pass(transformed_weights, transformed_biases, x_batch, activation_funcs)\n        \n        # Check for functional equality\n        max_abs_diff = np.max(np.abs(y_original - y_transformed))\n        tolerance = 1e-8\n        \n        return max_abs_diff  tolerance\n\n    # Define the 6 test cases as specified in the problem statement.\n    test_cases = [\n        # 1. ReLU, positive scales → should hold\n        {'arch': [3, 4, 2], 'hidden_act': 'relu', 'scales': {1: [1.7, 0.5, 2.2, 0.9]}},\n        # 2. ReLU, negative scale → should fail\n        {'arch': [3, 4, 2], 'hidden_act': 'relu', 'scales': {1: [-1.3, 1.2, 0.8, 1.1]}},\n        # 3. Sigmoid, positive scales → should fail\n        {'arch': [2, 3, 1], 'hidden_act': 'sigmoid', 'scales': {1: [1.4, 0.8, 1.1]}},\n        # 4. Identity, negative scales → should hold\n        {'arch': [3, 3, 2], 'hidden_act': 'identity', 'scales': {1: [-0.8, -1.2, 0.5]}},\n        # 5. ReLU, small positive scales → should hold\n        {'arch': [4, 3, 2], 'hidden_act': 'relu', 'scales': {1: [1e-6, 3e-6, 2e-6]}},\n        # 6. Two ReLU layers, positive scales → should hold\n        {'arch': [3, 4, 3, 2], 'hidden_act': 'relu', 'scales': {1: [1.3, 0.7, 2.0, 0.9], 2: [1.2, 0.6, 1.5]}},\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(case, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3125231"}, {"introduction": "在训练过程中，深层网络中每一层的输入分布都会发生变化，这可能导致训练不稳定。批量归一化（Batch Normalization, BN）是应对此问题的一项关键技术。通过一次集中的推导，本练习揭示了批量归一化的一个核心机制：它如何使一个层的输出对其输入的缩放变化保持不变，从而帮助稳定训练过程。[@problem_id:3125166]", "problem": "考虑一个实值前馈神经网络，它有一个隐藏层，输入维度为 $d$，隐藏层维度为 $m$，输出维度为 $1$。隐藏层的非线性激活函数是整流线性单元（ReLU），其逐元素定义为 $\\sigma(u) = \\max\\{0,u\\}$，并对所有 $c \\geq 0$ 满足正齐次性 $\\sigma(cu) = c\\,\\sigma(u)$。不带归一化的网络计算公式为\n$$\nf(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right),\n$$\n其中 $x \\in \\mathbb{R}^{d}$，$W_1 \\in \\mathbb{R}^{m \\times d}$，$w_2 \\in \\mathbb{R}^{m}$。网络中没有偏置项。现在假设输入被一个严格正标量 $\\alpha  0$ 重缩放，因此新的输入为 $x' = \\alpha x$。你只能通过将第一层权重矩阵 $W_1$ 乘以一个标量 $\\beta  0$ 来进行补偿，得到受扰动的第一层权重 $W_1' = \\beta W_1$，同时保持 $w_2$ 不变。请确定能使每个输入 $x$ 的原始输出保持不变的 $\\beta$ 值。\n\n接下来，考虑在第一个线性层之后、ReLU激活函数之前立即插入批量归一化（BN），它作用于每个隐藏单元，并带有固定的可训练缩放参数 $\\gamma_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$ 和平移参数 $\\beta_{\\mathrm{BN}} \\in \\mathbb{R}^{m}$。对于给定的小批量数据，BN将预激活值 $a = W_1 x$ 转换为\n$$\n\\hat{a} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}},\n$$\n其中 $\\mu \\in \\mathbb{R}^{m}$ 和 $\\sigma \\in \\mathbb{R}^{m}$ 是在小批量上逐元素计算的批量均值和标准差，$\\odot$ 表示逐元素乘法，并且在本分析中，数值稳定器设为零（即 $\\epsilon = 0$）。带有BN的网络计算公式为\n$$\ng(x) = w_2^{\\top}\\,\\sigma\\!\\left(\\hat{a}\\right).\n$$\n再次假设输入被严格正标量 $\\alpha  0$ 重缩放，并且你只能通过将第一层权重矩阵乘以相同的标量 $\\beta  0$ 来进行补偿，同时保持 $w_2$、$\\gamma_{\\mathrm{BN}}$ 和 $\\beta_{\\mathrm{BN}}$ 不变。请确定对于每个输入 $x$ 以及整个小批量数据，都能保持原始输出不变的 $\\beta$ 值。\n\n你的任务是从第一性原理出发，推导在每种情况下参数缩放 $\\beta$ 如何补偿输入缩放 $\\alpha$。请将你的最终答案表示为一个 $1 \\times 2$ 的行向量，其中按顺序包含无BN和有BN情况下 $\\beta$ 的闭式表达式。不需要进行数值四舍五入，也不涉及物理单位。", "solution": "### 推导\n\n我们分别分析这两种情况。\n\n**情况1：不带批量归一化的网络**\n\n原始网络计算输出 $f(x)$ 如下：\n$$\nf(x) = w_2^{\\top}\\,\\sigma\\!\\left(W_1 x\\right)\n$$\n输入被一个因子 $\\alpha  0$ 重缩放，因此新的输入为 $x' = \\alpha x$。第一层权重矩阵被一个因子 $\\beta  0$ 缩放，得到 $W_1' = \\beta W_1$。新的网络输出，我们记为 $f_{\\text{new}}(x')$，是：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left(W_1' x'\\right)\n$$\n代入 $x'$ 和 $W_1'$ 的表达式得到：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left((\\beta W_1) (\\alpha x)\\right)\n$$\n利用矩阵乘法的结合律和标量的性质：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\sigma\\!\\left((\\alpha\\beta) (W_1 x)\\right)\n$$\n预激活值被因子 $\\alpha\\beta$ 缩放。由于 $\\alpha  0$ 且 $\\beta  0$，其乘积 $\\alpha\\beta$ 也是严格正的。我们可以使用ReLU函数的正齐次性，即对 $c \\geq 0$ 有 $\\sigma(cu) = c\\,\\sigma(u)$：\n$$\nf_{\\text{new}}(x') = w_2^{\\top}\\,\\left( (\\alpha\\beta) \\sigma(W_1 x) \\right)\n$$\n由于 $\\alpha\\beta$ 是一个标量，我们可以将其从点积中提出：\n$$\nf_{\\text{new}}(x') = (\\alpha\\beta) \\left( w_2^{\\top} \\sigma(W_1 x) \\right) = (\\alpha\\beta) f(x)\n$$\n为了对每个输入 $x$ 都保持原始输出不变，我们必须有 $f_{\\text{new}}(x') = f(x)$。这意味着：\n$$\n(\\alpha\\beta) f(x) = f(x)\n$$\n为了使该等式对任何任意的、非平凡的网络（即 $f(x)$ 不恒等于零的网络）都成立，标量系数必须等于 $1$：\n$$\n\\alpha\\beta = 1\n$$\n解出 $\\beta$，我们得到第一层权重所需的缩放因子：\n$$\n\\beta = \\frac{1}{\\alpha}\n$$\n\n**情况2：带有批量归一化的网络**\n\n在这种情况下，在ReLU非线性激活函数之前，对预激活值 $a = W_1 x$ 应用批量归一化层。对于一个小批量的输入 $\\{x_i\\}_{i=1}^N$，其预激活值为 $\\{a_i = W_1 x_i\\}_{i=1}^N$。BN层首先计算该小批量中这些预激活值的逐元素均值 $\\mu$ 和标准差 $\\sigma$：\n$$\n\\mu = \\frac{1}{N} \\sum_{i=1}^N a_i \\quad ; \\quad \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (a_i - \\mu)^2}\n$$\n然后，它计算归一化后的激活值 $\\hat{a}_i$：\n$$\n\\hat{a}_i = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}}\n$$\n对于一个输入 $x_i$，最终的网络输出是 $g(x_i) = w_2^{\\top}\\,\\sigma(\\hat{a}_i)$。\n\n现在，我们考虑带有重缩放输入 $x_i' = \\alpha x_i$ 和重缩放权重 $W_1' = \\beta W_1$ 的受扰动网络。新的预激活值 $a_i'$ 为：\n$$\na_i' = W_1' x_i' = (\\beta W_1)(\\alpha x_i) = (\\alpha\\beta) (W_1 x_i) = (\\alpha\\beta) a_i\n$$\n批量中的每个预激活值都被因子 $\\alpha\\beta$ 缩放。让我们为这组新的预激活值 $\\{a_i'\\}$ 计算新的批量统计量 $\\mu'$ 和 $\\sigma'$：\n新的均值 $\\mu'$ 是：\n$$\n\\mu' = \\frac{1}{N} \\sum_{i=1}^N a_i' = \\frac{1}{N} \\sum_{i=1}^N (\\alpha\\beta) a_i = (\\alpha\\beta) \\left(\\frac{1}{N} \\sum_{i=1}^N a_i\\right) = (\\alpha\\beta) \\mu\n$$\n新的标准差 $\\sigma'$ 是：\n$$\n\\sigma' = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (a_i' - \\mu')^2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N ((\\alpha\\beta)a_i - (\\alpha\\beta)\\mu)^2} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (\\alpha\\beta)^2(a_i - \\mu)^2}\n$$\n$$\n\\sigma' = \\sqrt{(\\alpha\\beta)^2 \\left(\\frac{1}{N} \\sum_{i=1}^N (a_i - \\mu)^2\\right)} = \\sqrt{(\\alpha\\beta)^2 \\sigma^2} = |\\alpha\\beta| \\sigma\n$$\n由于 $\\alpha  0$ 且 $\\beta  0$，我们有 $|\\alpha\\beta| = \\alpha\\beta$。因此，$\\sigma' = (\\alpha\\beta) \\sigma$。\n\n现在我们使用这些新的统计量来计算新的归一化激活值 $\\hat{a}_i'$：\n$$\n\\hat{a}_i' = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i' - \\mu'}{\\sigma'} + \\beta_{\\mathrm{BN}} = \\gamma_{\\mathrm{BN}} \\odot \\frac{(\\alpha\\beta)a_i - (\\alpha\\beta)\\mu}{(\\alpha\\beta)\\sigma} + \\beta_{\\mathrm{BN}}\n$$\n假设批量方差不为零（因此对于每个分量 $j$ 都有 $\\sigma_j \\neq 0$），我们可以从分子和分母中约去标量因子 $\\alpha\\beta$：\n$$\n\\hat{a}_i' = \\gamma_{\\mathrm{BN}} \\odot \\frac{(\\alpha\\beta)(a_i - \\mu)}{(\\alpha\\beta)\\sigma} + \\beta_{\\mathrm{BN}} = \\gamma_{\\mathrm{BN}} \\odot \\frac{a_i - \\mu}{\\sigma} + \\beta_{\\mathrm{BN}} = \\hat{a}_i\n$$\n批量归一化层的输出与原始输出相同，即 $\\hat{a}_i' = \\hat{a}_i$。这对于小批量中的每个样本 $i$ 都成立。因此，ReLU函数的输入保持不变，最终的网络输出 $g(x_i)$ 也保持不变。\n\n这种不变性对缩放因子 $\\beta > 0$ 的*任何*选择都成立。问题要求的是*补偿*输入缩放 $\\alpha$ 的 $\\beta$ 值。我们的分析表明，批量归一化层本身就完美地补偿了其输入的任何缩放。BN层输入的缩放因子是 $\\alpha\\beta$。由于该层的输出对这个因子是不变的，因此不需要参数 $\\beta$ 进行额外的补偿来抵消 $\\alpha$ 的影响。一个不产生任何变化的缩放操作就是乘以 $1$。因此，尽管任何 $\\beta > 0$ 都可以，但代表所需（零）补偿的特定 $\\beta$ 值是 $1$。\n\n### 结果总结\n为保持网络输出不变所需的缩放因子 $\\beta$ 是：\n1. 无BN：$\\beta = \\frac{1}{\\alpha}$\n2. 有BN：$\\beta = 1$\n\n最终答案表示为一个 $1 \\times 2$ 的行向量。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\alpha}  1 \\end{pmatrix}}\n$$", "id": "3125166"}, {"introduction": "神经网络并非一个孤立的模型范式，它与机器学习的其他领域（如核方法）有着深刻的联系。本动手编码练习将向您展示，一个具有随机特征的浅层神经网络的训练过程与核岭回归（Kernel Ridge Regression）在形式上是等价的。通过亲手实现这两种方法，您将搭建起神经网络与经典机器学习之间的桥梁，并加深对对偶原理的理解。[@problem_id:3125270]", "problem": "要求您实现并分析一个具有固定的随机第一层特征的单隐藏层前馈神经网络，并将其训练后的输出层与在相同特征上构建的核岭回归（KRR）进行比较。该比较应基于线性模型、前馈神经网络和核方法的基本原理。\n\n基本原理：\n- 前馈神经网络通过仿射变换和非线性函数的复合，定义了从输入向量 $x \\in \\mathbb{R}^d$ 到标量输出的映射。在一个具有 $h$ 个隐藏单元、固定的第一层权重和偏置 $(W_1, b)$ 以及按元素激活函数 $\\sigma(\\cdot)$ 的单隐藏层网络中，网络输出是通过隐藏层激活值的线性组合获得的。\n- 岭回归（也称为 $L_2$ 正则化最小二乘法）通过惩罚参数向量的 $L_2$ 范数的平方来稳定解并控制过拟合。\n- 核方法通过特征的内积来表示预测，从而允许解以核矩阵的形式写出。\n\n您的程序必须：\n1. 生成合成回归数据。对于给定的输入维度 $d$、训练集大小 $n$ 和测试集大小 $m$，使用固定的随机种子从标准正态分布中抽取输入 $X_{\\text{train}} \\in \\mathbb{R}^{n \\times d}$ 和 $X_{\\text{test}} \\in \\mathbb{R}^{m \\times d}$。对 $x \\in \\mathbb{R}^d$ 定义目标函数为\n   $$ f(x) = \\sum_{j=1}^{d} \\sin(x_j) + 0.1 \\, \\|x\\|_2^2, $$\n   并观察带有标准差为 $\\sigma_{\\text{noise}}$ 的加性高斯噪声的输出：\n   $$ y_{\\text{train}} = f(X_{\\text{train}}) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_n). $$\n2. 使用固定的随机第一层构建随机特征。对于隐藏维度 $h$，从标准正态分布中抽样 $W_1 \\in \\mathbb{R}^{d \\times h}$ 和 $b \\in \\mathbb{R}^{1 \\times h}$（使用与数据构建相同的种子以确保每个测试用例内的确定性）。定义特征映射\n   $$ \\phi(x) = \\sigma(x W_1 + b), $$\n   该映射按元素应用，其中 $\\sigma$ 是以下激活函数之一：\n   - 修正线性单元（ReLU）：$\\sigma(z) = \\max(0, z)$，按元素应用。\n   - 双曲正切（tanh）：$\\sigma(z) = \\tanh(z)$，按元素应用。\n   - 恒等（线性）：$\\sigma(z) = z$，按元素应用。\n   对于矩阵，通过对 $X_{\\text{train}}$ 和 $X_{\\text{test}}$ 的每一行应用 $\\phi(\\cdot)$ 来分别定义 $Z_{\\text{train}} \\in \\mathbb{R}^{n \\times h}$ 和 $Z_{\\text{test}} \\in \\mathbb{R}^{m \\times h}$。\n3. 仅训练网络的输出层权重 $W_2 \\in \\mathbb{R}^{h \\times 1}$，通过最小化一个正则化强度为 $\\lambda  0$ 的 $L_2$ 正则化最小二乘目标函数：\n   $$ \\min_{W_2} \\; \\frac{1}{n} \\, \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\, \\|W_2\\|_2^2. $$\n   使用数值稳定的线性求解方法来获得正规方程所蕴含的最小化器，然后计算得到的测试预测值\n   $$ \\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2. $$\n4. 定义由相同固定随机特征导出的输入核函数：\n   $$ k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle, $$\n   这会产生训练格拉姆矩阵 (Gram matrix) $K \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k(x_i, x_j)$，即 $K = Z_{\\text{train}} Z_{\\text{train}}^\\top$，以及测试-训练核块 $K_{\\text{test}} = Z_{\\text{test}} Z_{\\text{train}}^\\top \\in \\mathbb{R}^{m \\times n}$。使用相同的正则化参数 $\\lambda$ 执行核岭回归，并计算测试预测值\n   $$ \\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha, $$\n   其中 $\\alpha \\in \\mathbb{R}^n$ 是通过求解一个涉及 $K$ 和 $\\lambda$ 的线性系统得到的。\n5. 对于每个测试用例，通过计算最大绝对差异，对两组测试预测值进行数值比较\n   $$ \\Delta = \\max_{1 \\le i \\le m} \\left| \\hat{y}_{\\text{test}, i}^{\\text{primal}} - \\hat{y}_{\\text{test}, i}^{\\text{kernel}} \\right|. $$\n\n测试套件：\n为以下五个测试用例实现上述流程，每个用例由一个元组 $(\\text{seed}, d, h, \\text{activation}, n, m, \\lambda, \\sigma_{\\text{noise}})$ 指定：\n- 用例 $1$：$(0, 3, 50, \\text{ReLU}, 64, 32, 10^{-2}, 0.05)$。\n- 用例 $2$：$(1, 5, 10, \\tanh, 80, 40, 10^{-4}, 0.10)$。\n- 用例 $3$：$(2, 2, 5, \\text{identity}, 40, 20, 1.0, 0.00)$。\n- 用例 $4$：$(3, 4, 20, \\text{ReLU}, 60, 30, 10^{6}, 0.20)$。\n- 用例 $5$：$(4, 3, 1, \\tanh, 50, 25, 10^{-2}, 0.05)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含五个测试用例的结果，格式为方括号内以逗号分隔的列表，按用例 1 到 5 的顺序排列，其中每个条目是该用例的单个浮点数 $\\Delta$。例如，\n$$ [\\Delta_1, \\Delta_2, \\Delta_3, \\Delta_4, \\Delta_5]. $$\n不应打印任何其他文本。所有值都是无单位的实数。", "solution": "### 基于原理的解法\n\n该问题要求实现并比较在给定特征集上解决正则化最小二乘问题的两种方法。这是机器学习中原始-对偶等价性的一个经典例子。\n\n**1. 原始形式：特征空间中的岭回归**\n单层神经网络的输出由 $g(x) = \\phi(x)W_2$ 给出，其中 $\\phi(x) = \\sigma(x W_1 + b)$ 是固定的、随机生成的特征向量。第二层权重 $W_2 \\in \\mathbb{R}^{h \\times 1}$ 通过最小化 $L_2$ 正则化最小二乘目标函数（也称为岭回归）来训练：\n$$\n\\mathcal{L}(W_2) = \\frac{1}{n} \\|Z_{\\text{train}} W_2 - y_{\\text{train}}\\|_2^2 + \\lambda \\|W_2\\|_2^2\n$$\n此处，$Z_{\\text{train}} \\in \\mathbb{R}^{n \\times h}$ 是训练集的特征向量矩阵，$y_{\\text{train}} \\in \\mathbb{R}^{n \\times 1}$ 是目标值。该目标函数是关于 $W_2$ 的凸函数。为了找到最小值点，我们计算关于 $W_2$ 的梯度并将其设为零向量：\n$$\n\\nabla_{W_2} \\mathcal{L}(W_2) = \\frac{2}{n} Z_{\\text{train}}^\\top (Z_{\\text{train}} W_2 - y_{\\text{train}}) + 2\\lambda W_2 = 0\n$$\n重新排列各项以求解 $W_2$，得到该问题的正规方程：\n$$\n\\left(\\frac{1}{n} Z_{\\text{train}}^\\top Z_{\\text{train}} + \\lambda I_h\\right) W_2 = \\frac{1}{n} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n两边乘以 $n$，我们得到一个更常见的形式：\n$$\n(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h) W_2 = Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n其中 $I_h$ 是 $h \\times h$ 的单位矩阵。当 $\\lambda  0$ 时，矩阵 $(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)$ 是正定的，因此是可逆的。$W_2$ 的解为：\n$$\nW_2 = (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n这涉及到求解一个包含 $h \\times h$ 矩阵的线性系统。然后，对测试集 $Z_{\\text{test}}$ 的预测计算如下：\n$$\n\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2\n$$\n\n**2. 对偶形式：核岭回归**\n“核技巧”允许我们在不显式构造特征向量的情况下解决相同的问题，前提是我们能用内积来表达算法。核函数定义为特征空间中的内积：\n$$\nk(x_i, x_j) = \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\phi(x_i) \\phi(x_j)^\\top\n$$\n在这个问题中，我们可以显式地访问特征映射 $\\phi$，因此可以直接计算训练格拉姆矩阵 (Gram matrix) $K \\in \\mathbb{R}^{n \\times n}$ 为 $K = Z_{\\text{train}} Z_{\\text{train}}^\\top$。\n\n表示定理指出，最优权重向量 $W_2$ 可以表示为训练数据特征向量的线性组合，即 $W_2 = Z_{\\text{train}}^\\top \\alpha$，其中 $\\alpha \\in \\mathbb{R}^{n \\times 1}$ 是某个对偶系数向量。将此代入 $W_2$ 的原始解中可以求解 $\\alpha$。在核岭回归中，与原始目标一致的 $\\alpha$ 的标准解是以下线性系统的解：\n$$\n(K + n\\lambda I_n) \\alpha = y_{\\text{train}}\n$$\n其中 $I_n$ 是 $n \\times n$ 的单位矩阵。求解该系统以得到 $\\alpha$：\n$$\n\\alpha = (K + n\\lambda I_n)^{-1} y_{\\text{train}}\n$$\n这涉及到求解一个包含 $n \\times n$ 矩阵的线性系统。对测试集的预测是使用测试点和训练点之间的核函数进行的：\n$$\n\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha = (Z_{\\text{test}} Z_{\\text{train}}^\\top) \\alpha\n$$\n\n**3. 原始-对偶等价性**\n理论上，两种方法的预测必须相同。我们可以通过将 $W_2$ 和 $\\alpha$ 的表达式代入预测方程来证明这一点。\n$$\n\\hat{y}_{\\text{test}}^{\\text{primal}} = Z_{\\text{test}} W_2 = Z_{\\text{test}} (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top y_{\\text{train}}\n$$\n$$\n\\hat{y}_{\\text{test}}^{\\text{kernel}} = K_{\\text{test}} \\alpha = (Z_{\\text{test}} Z_{\\text{train}}^\\top) (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1} y_{\\text{train}}\n$$\n这两个表达式的等价性取决于一个矩阵恒等式（Woodbury 矩阵恒等式的一种形式，也称为推-通恒等式）：\n$$\n(A^\\top A + cI_h)^{-1} A^\\top = A^\\top (A A^\\top + cI_n)^{-1}\n$$\n令 $A = Z_{\\text{train}}$ 和 $c = n\\lambda$，我们有：\n$$\n(Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top = Z_{\\text{train}}^\\top (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1}\n$$\n左乘 $Z_{\\text{test}}$ 表明作用于 $y_{\\text{train}}$ 的算子是相同的：\n$$\nZ_{\\text{test}} (Z_{\\text{train}}^\\top Z_{\\text{train}} + n\\lambda I_h)^{-1} Z_{\\text{train}}^\\top = Z_{\\text{test}} Z_{\\text{train}}^\\top (Z_{\\text{train}} Z_{\\text{train}}^\\top + n\\lambda I_n)^{-1}\n$$\n因此，在精确算术中，$\\hat{y}_{\\text{test}}^{\\text{primal}} = \\hat{y}_{\\text{test}}^{\\text{kernel}}$。\n\n原始形式和对偶形式之间的计算选择取决于训练样本数 $n$ 和特征维度 $h$ 的相对大小。如果 $n \\gg h$，原始形式更有效，因为它需要求解一个 $h \\times h$ 的系统。相反，如果 $h \\gg n$，则首选对偶形式，因为它需要求解一个 $n \\times n$ 的系统。所要求的差异 $\\Delta$ 将是一个小的非零值，反映了两种不同计算路径中浮点误差的不同累积。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares primal and dual solutions for ridge regression\n    on random features for a series of test cases.\n    \"\"\"\n\n    test_cases = [\n        # (seed, d, h, activation, n, m, lambda, sigma_noise)\n        (0, 3, 50, 'ReLU', 64, 32, 1e-2, 0.05),\n        (1, 5, 10, 'tanh', 80, 40, 1e-4, 0.10),\n        (2, 2, 5, 'identity', 40, 20, 1.0, 0.00),\n        (3, 4, 20, 'ReLU', 60, 30, 1e6, 0.20),\n        (4, 3, 1, 'tanh', 50, 25, 1e-2, 0.05),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        seed, d, h, activation_name, n, m, lam, sigma_noise = case\n        \n        # Set the seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate synthetic regression data\n        X_train = np.random.randn(n, d)\n        X_test = np.random.randn(m, d)\n\n        def target_function(X):\n            sin_term = np.sum(np.sin(X), axis=1)\n            norm_term = 0.1 * np.sum(X**2, axis=1) # Faster than np.linalg.norm for this purpose\n            return (sin_term + norm_term).reshape(-1, 1)\n\n        y_true_train = target_function(X_train)\n        noise = np.random.randn(n, 1) * sigma_noise\n        y_train = y_true_train + noise\n\n        # 2. Construct random features\n        W1 = np.random.randn(d, h)\n        b = np.random.randn(1, h)\n\n        activations = {\n            'ReLU': lambda z: np.maximum(0, z),\n            'tanh': lambda z: np.tanh(z),\n            'identity': lambda z: z\n        }\n        activation_func = activations[activation_name]\n\n        Z_train = activation_func(X_train @ W1 + b)\n        Z_test = activation_func(X_test @ W1 + b)\n\n        # 3. Primal solution (Ridge Regression)\n        # Solve (Z_train.T @ Z_train + n * lambda * I_h) W2 = Z_train.T @ y_train\n        h_dim = Z_train.shape[1]\n        A_primal = Z_train.T @ Z_train + n * lam * np.identity(h_dim)\n        c_primal = Z_train.T @ y_train\n        \n        W2 = np.linalg.solve(A_primal, c_primal)\n        \n        y_hat_primal = Z_test @ W2\n\n        # 4. Dual solution (Kernel Ridge Regression)\n        # Solve (K + n * lambda * I_n) alpha = y_train\n        # where K = Z_train @ Z_train.T\n        K = Z_train @ Z_train.T\n        A_kernel = K + n * lam * np.identity(n)\n        \n        alpha = np.linalg.solve(A_kernel, y_train)\n        \n        # Predictions: y_hat_kernel = K_test @ alpha\n        # where K_test = Z_test @ Z_train.T\n        K_test = Z_test @ Z_train.T\n        y_hat_kernel = K_test @ alpha\n\n        # 5. Compare predictions\n        delta = np.max(np.abs(y_hat_primal - y_hat_kernel))\n        results.append(delta)\n\n    # Format and print the final results\n    print(f\"[{','.join(f'{r:.12e}' for r in results)}]\")\n\nsolve()\n```", "id": "3125270"}]}