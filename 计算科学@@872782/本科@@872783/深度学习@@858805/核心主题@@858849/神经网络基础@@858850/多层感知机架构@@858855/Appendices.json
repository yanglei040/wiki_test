{"hands_on_practices": [{"introduction": "多层感知器的强大能力源于其能够组合简单的非线性单元来构建复杂的函数。这个练习将带你从最基础的层面入手，探索如何利用修正线性单元（ReLU）这一最常见的激活函数来精确地表示一个基础但非线性的函数——绝对值函数 $f(x) = |x|$。通过亲手构建这个网络 [@problem_id:3151215]，你将直观地理解 ReLU 网络是如何通过拼接线性片段来形成目标函数的。", "problem": "考虑一个具有整流线性单元（ReLU）激活函数的一维两层前馈网络，其中整流线性单元（ReLU）定义为 $\\sigma(u)=\\max\\{0,u\\}$。该网络有 $m$ 个隐藏神经元，并计算\n$$f(x)=w_{2}^{\\top}\\sigma(W_{1}x+b_{1})+b_{2},$$\n其中输入为 $x\\in\\mathbb{R}$，权重矩阵为 $W_{1}\\in\\mathbb{R}^{m\\times 1}$，偏置向量为 $b_{1}\\in\\mathbb{R}^{m}$，输出权重为 $w_{2}\\in\\mathbb{R}^{m}$，标量输出偏置为 $b_{2}\\in\\mathbb{R}$。令目标函数为 $f^{\\star}(x)=|x|$。\n\n仅使用前馈架构和整流线性单元（ReLU）的核心定义，完成以下任务：\n- 构造明确的权重和偏置 $(W_{1},b_{1},w_{2},b_{2})$，以在所有 $x\\in\\mathbb{R}$ 上对 $f^{\\star}(x)$ 进行一致逼近，其最坏情况误差至多为 $\\epsilon$，其中 $\\epsilon>0$ 是一个任意容差。\n- 确定实现此一致误差保证所需的最小隐藏神经元数量 $m$，并将其表示为 $\\epsilon$ 的函数。\n\n你的最终答案必须是关于 $m$ 作为 $\\epsilon$ 函数的单个闭式表达式。无需四舍五入，也无需单位。", "solution": "首先验证问题，确保其是适定的、有科学依据且无歧义的。\n\n### 第1步：提取已知条件\n- 网络架构：一个计算 $f(x)=w_{2}^{\\top}\\sigma(W_{1}x+b_{1})+b_{2}$ 的两层前馈网络。\n- 输入域：$x \\in \\mathbb{R}$。\n- 激活函数：整流线性单元（ReLU），$\\sigma(u)=\\max\\{0,u\\}$。\n- 网络参数：$W_{1}\\in\\mathbb{R}^{m\\times 1}$（第一层权重），$b_{1}\\in\\mathbb{R}^{m}$（第一层偏置），$w_{2}\\in\\mathbb{R}^{m}$（第二层权重），以及 $b_{2}\\in\\mathbb{R}$（第二层偏置）。$m$ 是隐藏神经元的数量。\n- 目标函数：$f^{\\star}(x)=|x|$。\n- 逼近要求：在 $\\mathbb{R}$ 上的一致逼近，使得最坏情况误差以 $\\epsilon$ 为界。即，对于任意容差 $\\epsilon>0$，有 $\\sup_{x\\in\\mathbb{R}} |f(x) - f^{\\star}(x)| \\le \\epsilon$。\n- 目标1：构造满足逼近要求的参数 $(W_{1},b_{1},w_{2},b_{2})$。\n- 目标2：确定满足此保证所需的最小隐藏神经元数量 $m$，并表示为 $\\epsilon$ 的函数。\n\n### 第2步：使用已知条件进行验证\n该问题定义明确且科学合理。它属于使用神经网络的逼近理论的标准框架。目标函数 $f^{\\star}(x)=|x|$ 是连续的，通用近似定理保证了两层网络可以逼近它。此问题要求进行具体构造并分析最小网络规模。问题是自洽且客观的。尽管“作为 $\\epsilon$ 的函数”的表述可能暗示一个非恒定的关系，但常数函数是一个有效的答案。该问题没有缺陷；其结构旨在检验对ReLU网络对某些函数的精确表示能力的理解。该问题被认为是 **有效的**。\n\n### 第3步：求解推导\n求解过程分为两部分：首先，我们确定所需的最小神经元数量，其次，我们提供一个明确的构造。\n\n让我们分析所需的神经元数量 $m$。我们首先研究 $m=1$ 是否足够。\n对于 $m=1$，网络参数是标量。设 $W_1 = a \\in \\mathbb{R}$，$b_1 = d \\in \\mathbb{R}$，$w_2 = c \\in \\mathbb{R}$，以及 $b_2 = e \\in \\mathbb{R}$。网络函数为：\n$$f(x) = c \\cdot \\sigma(ax+d) + e = c \\cdot \\max\\{0, ax+d\\} + e$$\n我们必须满足对于所有 $x \\in \\mathbb{R}$，都有 $|f(x) - |x|| \\le \\epsilon$。\n\n考虑 $f(x)$ 和 $|x|$ 的渐近行为。\n- 如果 $a=0$，$f(x) = c \\cdot \\max\\{0,d\\} + e$，这是一个常数。当 $|x| \\to \\infty$ 时，误差 $|f(x) - |x||$ 会变得无界。所以，我们必须有 $a \\ne 0$。\n- 如果 $a > 0$：\n  - 对于大的正数 $x$，$ax+d > 0$，所以 $f(x) = c(ax+d)+e = (ac)x + (cd+e)$。为了确保当 $x \\to \\infty$ 时误差 $|f(x) - x|$ 是有界的，线性项必须抵消。这要求 $ac=1$。\n  - 对于大的负数 $x$（即 $x \\to -\\infty$），$ax+d < 0$，所以 $f(x) = e$。误差为 $|f(x) - |x|| = |e - (-x)| = |e+x|$。当 $x \\to -\\infty$ 时，这个误差是无界的。\n- 如果 $a < 0$：\n  - 对于大的负数 $x$，$ax+d > 0$，所以 $f(x) = c(ax+d)+e = (ac)x + (cd+e)$。为了确保当 $x \\to -\\infty$ 时误差 $|f(x) - (-x)|$ 是有界的，我们必须有 $ac=-1$。\n  - 对于大的正数 $x$（即 $x \\to \\infty$），$ax+d < 0$，所以 $f(x) = e$。误差为 $|f(x) - |x|| = |e - x|$。当 $x \\to \\infty$ 时，这个误差是无界的。\n\n在 $m=1$ 的所有情况下，都不可能在整个定义域 $\\mathbb{R}$ 上保持有界误差。因此，单个隐藏神经元是不够的，所以我们必须有 $m > 1$。\n\n现在，让我们研究 $m=2$ 的情况。我们将尝试构造 $f(x)$ 来精确表示 $f^{\\star}(x)=|x|$。考虑以下恒等式：\n$$|x| = \\max\\{0, x\\} + \\max\\{0, -x\\}$$\n这个恒等式将 $|x|$ 表示为两个函数的和，每个函数都具有ReLU激活函数的形式。我们可以用一个含有两个神经元的网络来实现这一点。\n网络输出为 $f(x) = w_{2,1}\\sigma(W_{1,1}x+b_{1,1}) + w_{2,2}\\sigma(W_{1,2}x+b_{1,2}) + b_2$。\n\n为了实现恒等式 $|x| = \\sigma(x) + \\sigma(-x)$，我们可以按如下方式选择参数：\n- 对于第一个神经元，我们想计算 $\\sigma(x)$。我们将其对应的权重和偏置设置为 $W_{1,1}=1$ 和 $b_{1,1}=0$。\n- 对于第二个神经元，我们想计算 $\\sigma(-x)$。我们将其权重和偏置设置为 $W_{1,2}=-1$ 和 $b_{1,2}=0$。\n- 输出层必须将这两个结果相加。我们设置输出权重为 $w_{2,1}=1$ 和 $w_{2,2}=1$。\n- 没有整体偏移量，所以我们设置输出偏置 $b_2=0$。\n\n以矩阵/向量形式，这些参数是：\n- $W_{1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\in \\mathbb{R}^{2\\times 1}$\n- $b_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- $w_{2} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- $b_{2} = 0 \\in \\mathbb{R}$\n\n有了这些参数，网络函数为：\n$$f(x) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\sigma\\left(\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}x + \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\right) + 0$$\n$$f(x) = \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} \\sigma(x) \\\\ \\sigma(-x) \\end{pmatrix} = \\sigma(x) + \\sigma(-x)$$\n$$f(x) = \\max\\{0, x\\} + \\max\\{0, -x\\}$$\n我们验证这等于 $|x|$：\n- 如果 $x \\ge 0$，$f(x) = x + 0 = x = |x|$。\n- 如果 $x < 0$，$f(x) = 0 + (-x) = -x = |x|$。\n这个构造提供了 $f^{\\star}(x) = |x|$ 的一个精确表示。\n\n逼近误差为 $|f(x) - f^{\\star}(x)| = ||x| - |x|| = 0$。\n问题要求误差对于任何 $\\epsilon > 0$ 至多为 $\\epsilon$。由于我们的构造实现了0误差，且对于任何 $\\epsilon > 0$ 都有 $0 \\le \\epsilon$，因此这个构造对于任何容差都是一个有效的解。\n\n我们已经证明 $m=1$ 是不够的，而 $m=2$ 是足够的（实现了零误差）。因此，所需的最小隐藏神经元数量是 $m=2$。这个结果对于 $\\epsilon > 0$ 的任何选择都成立。因此，最小神经元数量 $m$ 是 $\\epsilon$ 的一个常数函数：\n$$m(\\epsilon) = 2$$", "answer": "$$\\boxed{2}$$", "id": "3151215"}, {"introduction": "掌握了构建一维函数的方法后，我们自然会问：多层感知器能否逼近更复杂的多变量函数，例如乘法 $f(x, y) = x \\cdot y$？这个问题触及了神经网络“万能逼近定理”的核心。本练习 [@problem_id:3155543] 将展示一个巧妙的思路：通过一个数学恒等式，将逼近双变量乘法的问题简化为逼近单变量的平方函数 $s(t) = t^2$。在此过程中，你不仅会设计出一个能计算乘法的网络，还将深入探讨实现同等精度时，网络深度与宽度之间的重要权衡。", "problem": "您的任务是构建一个多层感知机 (MLP)，该网络仅使用由修正线性单元 (ReLU) 激活函数构建的分段线性函数，来近似平方域 $[0,1]^2$ 上的乘法函数 $f(x,y) = x \\cdot y$。所需近似误差为一个非负容差 $\\epsilon$。您的目标是从第一性原理出发推导其架构，并实现一个程序来验证误差并报告所需的架构复杂度。该构建过程必须科学严谨，并基于以下基本原理进行推导：MLP的定义、将 $x \\cdot y$ 简化为单变量函数组合的代数恒等式，以及二次可微函数的线性插值的标准误差界。\n\n作为基础的定义：\n- 多层感知机 (MLP) 是一个由交替的仿射变换和非线性函数构成的函数。使用修正线性单元 (ReLU) 激活函数时，一个层计算 $\\sigma(z)$，其中 $\\sigma(t) = \\max\\{0,t\\}$，$z$ 是其输入的仿射变换。\n- 一个断点为 $\\{c_i\\}$ 的分段线性单变量函数可以由一个单隐藏层的 ReLU 网络精确表示为 $g(t) = \\alpha_0 + \\alpha_1 t + \\sum_i \\beta_i \\max\\{0, t - c_i\\}$，其中系数由各段的斜率确定。\n- 目标函数为 $[0,1]^2$ 上的 $f(x,y) = x \\cdot y$。\n\n任务：\n1. 从第一性原理出发，推导如何将 $f(x,y) = x \\cdot y$ 简化为单变量函数的组合，以便一个能精确计算单变量函数的分段线性近似的 MLP 可以用来近似 $f(x,y)$。从一个用单变量函数表示 $x \\cdot y$ 的有效恒等式开始，然后使用 ReLU 单元推导出最终的 MLP 结构。\n2. 使用数值分析中经过验证的结论，推导在区间 $[-R,R]$ 上用 $M$ 个等长线段线性插值一个二次可微函数 $s(t)$ 时的一致近似误差界。将此应用于 $s(t) = t^2$，然后通过您对 $x \\cdot y$ 的简化式来传播误差，以说明如何选择 $M$（从而确定 MLP 隐藏层的宽度），使得对 $x \\cdot y$ 的最终近似误差在 $[0,1]^2$ 上一致地满足 $\\le \\epsilon$。\n3. 分析深度与宽度的权衡。提供两种渐近构造：\n   - 一个浅而宽的网络，它使用单个隐藏层来精确计算 $[-R,R]$ 上具有 $M$ 个线段的 $t^2$ 的分段线性近似（深度恒定，宽度随 $M$ 增长）。\n   - 一个深而窄的网络，它通过复合操作获得相同数量的有效线性段，其中线性段的数量随深度呈指数增长；证明深度可以是 $O(\\log M)$，而宽度受一个不依赖于 $M$ 的小常数限制，前提是假设存在一个使用 ReLU 操作的构造性复合方案。\n   使用基本定义和逻辑来证明这两种构造的合理性，而不依赖未经证明的捷径。\n4. 实现要求：\n   - 具体实现浅而宽的构造。使用 $M$ 个等长线段构建 $[-2,2]$ 上 $t^2$ 的单变量分段线性近似 $S(t)$，将 $S$ 精确表示为一个在段边界处有断点的单隐藏层 ReLU 网络，然后通过您的简化式，仅使用 $S(x)$、$S(y)$ 和 $S(x+y)$ 的仿射组合来计算 $x \\cdot y$ 的近似值。您的程序必须计算 $[0,1]^2$ 上网格的一致近似误差，并验证其 $\\le \\epsilon$。\n   - 计算并报告此浅而宽网络作为 $\\epsilon$ 函数的理论宽度（使用最宽隐藏层中的隐藏单元数量作为宽度），并报告使用复合论证达到相同误差容差所需的深而窄网络的理论深度（对于深而窄的构造，您可以假设其宽度为一个固定的小常数）。\n5. 测试套件规范：\n   - 使用误差容差 $\\epsilon \\in \\{\\, 10^{-1}, \\, 10^{-2}, \\, 2.5 \\times 10^{-3} \\,\\}$。\n   - 对于每个 $\\epsilon$，根据您推导的规则选择 $M$，并在一个均匀的笛卡尔网格上评估浅而宽网络的最大绝对误差 $\\sup_{(x,y) \\in [0,1]^2} | \\hat{f}(x,y) - x y |$，其中 $x$ 和 $y$ 各自在 $[0,1]$（含端点）上以 $N$ 个等距点采样，$N$ 是一个足够大的正整数，以确保在实践中能捕捉到上确界。显式地包括 $x=0$, $x=1$, $y=0$, 和 $y=1$。\n   - 对于每个 $\\epsilon$，报告三个量：测量的一致误差（一个浮点数）、浅而宽网络的理论宽度（一个整数），以及为达到相同误差容差的深而窄网络的理论深度（一个整数）。\n6. 最终输出格式：\n   - 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为逗号分隔的列表的列表，每个内部列表的形式为 $[\\text{误差}, \\text{宽度}, \\text{深度}]$。即，打印一个类似 $[[e_1,w_1,d_1],[e_2,w_2,d_2],[e_3,w_3,d_3]]$ 的字符串，不含多余的空格或文本。\n\n在此问题陈述中，所有数学实体必须以 LaTeX 表示法出现。不涉及物理单位、角度和百分比；以小数或整数形式报告数值。", "solution": "在尝试解答之前，该问题需经过验证过程。\n\n### 步骤 1：提取已知条件\n- **目标函数**：在域 $[0,1]^2$ 上的 $f(x,y) = x \\cdot y$。\n- **近似容差**：一个非负误差容差 $\\epsilon$。\n- **激活函数**：修正线性单元 (ReLU)，$\\sigma(t) = \\max\\{0,t\\}$。\n- **MLP 定义**：一个由交替的仿射变换和非线性函数构成的函数。\n- **分段线性函数表示**：一个断点为 $\\{c_i\\}$ 的单变量分段线性函数 $g(t)$ 可以表示为 $g(t) = \\alpha_0 + \\alpha_1 t + \\sum_i \\beta_i \\max\\{0, t - c_i\\}$。\n- **任务**：\n    1.  推导将 $f(x,y)$ 简化为单变量函数的方法，以适用于 MLP 近似。\n    2.  推导近似的误差界，以及选择线性段数 $M$ 作为 $\\epsilon$ 函数的规则。\n    3.  分析浅而宽与深而窄架构的权衡。\n    4.  实现浅而宽的构造。用 $M$ 段近似 $[-2,2]$ 上的 $s(t) = t^2$。验证误差 $\\le \\epsilon$。\n    5.  计算 $\\epsilon \\in \\{10^{-1}, 10^{-2}, 2.5 \\times 10^{-3}\\}$ 的结果。报告每个 $\\epsilon$ 的测量误差、浅层网络宽度和深层网络深度。\n    6.  最终输出必须是格式为 `[[e1,w1,d1],[e2,w2,d2],[e3,w3,d3]]` 的单个字符串。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据预定义标准对问题进行评估：\n\n-   **科学依据**：该问题基于成熟的数学领域——近似理论及其在神经网络中的应用，特别是通用近似定理和 ReLU 网络的构造性证明。使用极化恒等式和标准数值分析误差界是科学严谨的。\n-   **适定性**：该问题是适定的。它要求一个基于给定误差容差 $\\epsilon$ 的构造性过程和分析，这会导出一组确定的架构参数和一个可验证的误差度量。\n-   **客观性**：语言正式、无歧义。所有术语在数学和计算机科学领域都是标准术语。\n-   **完整性与一致性**：问题提供了所有必要的定义和约束，足以进行唯一的推理。没有矛盾之处。\n-   **现实性与可行性**：这些任务在数学上是可行的，构成了深度学习中的一个标准理论练习。\n-   **结构与其他标准**：问题结构良好、具有挑战性且可验证。\n\n### 步骤 3：结论与行动\n问题有效。将提供完整解答。\n\n### 基于原理的解答\n\n#### 1. 将乘法简化为单变量函数\n任务的核心是近似二元函数 $f(x,y) = x \\cdot y$。通过使用极化恒等式，可以将其简化为对单变量函数的近似。和的平方由 $(x+y)^2 = x^2 + 2xy + y^2$ 给出。重新整理此恒等式以求解 $x \\cdot y$ 可得：\n$$\nx \\cdot y = \\frac{1}{2} \\left( (x+y)^2 - x^2 - y^2 \\right)\n$$\n这个恒等式表明，乘法可以仅通过单变量平方函数 $s(t) = t^2$ 和线性组合来表示。因此，如果我们能构建一个近似 $s(t) = t^2$ 的 MLP，我们就能构建一个近似 $x \\cdot y$ 的 MLP。\n设 $S(t)$ 是由 ReLU 网络计算的 $s(t) = t^2$ 的一个分段线性近似。$f(x,y)$ 相应的近似，我们记为 $\\hat{f}(x,y)$，是：\n$$\n\\hat{f}(x,y) = \\frac{1}{2} \\left( S(x+y) - S(x) - S(y) \\right)\n$$\n$(x,y)$ 的问题域是 $[0,1]^2$。函数 $S(t)$ 的参数将是 $x \\in [0,1]$、$y \\in [0,1]$ 和 $x+y \\in [0,2]$。实现要求指定在区间 $[-2,2]$ 上近似 $t^2$，这舒适地包含了所需的域 $[0,2]$。我们将设置 $s(t)$ 的近似范围为 $[-R, R]$，其中 $R=2$。\n\n#### 2. 误差分析与架构规模确定\n我们必须确定分段近似 $S(t)$ 所需的线性段数 $M$，以实现最终误差 $|\\hat{f}(x,y) - xy| \\le \\epsilon$。策略是先界定 $S(t)$ 的误差，然后通过简化恒等式传播该误差。\n\n对于在区间 $[a,b]$ 上用 $M$ 个等长线段对一个二次可微函数 $s(t)$ 进行分段线性插值，其一致误差界为 $|s(t) - S(t)| \\le \\frac{h^2}{8} \\sup_{t \\in [a,b]} |s''(t)|$，其中 $h=(b-a)/M$ 是线段长度。\n我们正在创建一个分段线性函数 $S(t)$，它在区间 $[-R,R] = [-2,2]$ 上用 $M$ 个等长线段插值 $s(t)=t^2$。每个线段的长度是 $h = \\frac{2R}{M} = \\frac{4}{M}$。$S(t)$ 在 $[-2,2]$ 上的一致误差界由下式给出：\n$$\n\\epsilon_S = \\sup_{t \\in [-2,2]} |S(t) - s(t)| \\le \\frac{h^2}{8} \\sup_{t \\in [-2,2]} |s''(t)|\n$$\n对于 $s(t) = t^2$，其二阶导数是 $s''(t) = 2$，是一个常数。该界变为：\n$$\n\\epsilon_S \\le \\frac{1}{8} \\left(\\frac{4}{M}\\right)^2 \\cdot 2 = \\frac{1}{8} \\frac{16}{M^2} \\cdot 2 = \\frac{4}{M^2}\n$$\n现在，我们将这个误差传播到 $x \\cdot y$ 的近似中。总误差是：\n$$\n|\\hat{f}(x,y) - f(x,y)| = \\left| \\frac{1}{2}(S(x+y) - S(x) - S(y)) - \\frac{1}{2}((x+y)^2 - x^2 - y^2) \\right|\n$$\n$$\n= \\frac{1}{2} |(S(x+y) - (x+y)^2) - (S(x) - x^2) - (S(y) - y^2)|\n$$\n根据三角不等式：\n$$\n\\le \\frac{1}{2} \\left( |S(x+y) - (x+y)^2| + |S(x) - x^2| + |S(y) - y^2| \\right)\n$$\n由于 $x,y,x+y$ 都在定义 $S(t)$ 的域 $[-2,2]$ 内，每一项都由 $\\epsilon_S$ 界定：\n$$\n|\\hat{f}(x,y) - f(x,y)| \\le \\frac{1}{2}(\\epsilon_S + \\epsilon_S + \\epsilon_S) = \\frac{3}{2}\\epsilon_S\n$$\n代入 $\\epsilon_S$ 的界，我们得到总误差界：\n$$\n\\epsilon_{xy} \\le \\frac{3}{2} \\left(\\frac{4}{M^2}\\right) = \\frac{6}{M^2}\n$$\n为确保误差在容差 $\\epsilon$ 之内，我们需要 $\\frac{6}{M^2} \\le \\epsilon$，这意味着 $M^2 \\ge \\frac{6}{\\epsilon}$。由于 $M$ 必须是整数，我们必须选择：\n$$\nM = \\left\\lceil \\sqrt{\\frac{6}{\\epsilon}} \\right\\rceil\n$$\n\n#### 3. 架构权衡：浅而宽网络 vs. 深而窄网络\n\n**浅而宽构造：**\n一个具有 $M$ 个线段（因此有 $M-1$ 个断点）的分段线性函数可以由一个单隐藏层的 ReLU 网络精确表示。具有断点 $\\{c_i\\}_{i=1}^{M-1}$ 的函数 $S(t)$ 由下式给出：\n$$\nS(t) = \\alpha_0 + \\alpha_1 t + \\sum_{i=1}^{M-1} \\beta_i \\max\\{0, t - c_i\\}\n$$\n这需要在隐藏层中使用 $M-1$ 个 ReLU 神经元。为了实现 $\\hat{f}(x,y)$，我们需要计算 $S(x)$、$S(y)$ 和 $S(x+y)$。这可以在一个更宽的单一隐藏层内并行完成。所需的隐藏单元是 $\\max\\{0, x-c_i\\}$、$\\max\\{0, y-c_i\\}$ 和 $\\max\\{0, x+y-c_i\\}$，对于所有 $i=1, \\dots, M-1$。这导致总共有 $3(M-1)$ 个隐藏神经元。输出是这些神经元输出的线性组合。该架构是：输入（2个神经元）$\\to$ 隐藏层（$3(M-1)$个神经元）$\\to$ 输出（1个神经元）。这个浅层网络的宽度，定义为最宽隐藏层的大小，是：\n$$\nW = 3(M-1) = 3\\left(\\left\\lceil \\sqrt{\\frac{6}{\\epsilon}} \\right\\rceil - 1\\right)\n$$\n这是一个深度恒定（深度为2，即一个隐藏层）的网络，其宽度以 $O(1/\\sqrt{\\epsilon})$ 的速度增长。\n\n**深而窄构造：**\n或者，一个深而窄的网络可以实现相同的近似效果。关键思想是，复合 ReLU 单元层可以使线性段的数量相对于深度呈指数增长。可以构建一个简单的、恒定宽度的网络模块，以有效地将其输入函数的线性段数量加倍。对于定义在 $[0,1]$ 上的函数，一个模块可以将其映射到一个具有两倍“频率”的新函数。\n假设一个恒定宽度的单隐藏层能够将一个有 $k$ 个线性段的函数转换成一个有 $2k$ 个段的函数。通过复合 $d$ 个这样的层，我们可以生成一个具有 $2^d k_0$ 个段的函数，其中 $k_0$是起始时的段数。要达到所需的 $M$ 个段，我们需要 $2^d \\approx M$，这意味着 $d \\approx \\log_2 M$。所需的最小整数层数，即深度，为：\n$$\nD = \\lceil \\log_2 M \\rceil = \\left\\lceil \\log_2 \\left\\lceil \\sqrt{\\frac{6}{\\epsilon}} \\right\\rceil \\right\\rceil\n$$\n这代表一个深度为 $O(\\log(1/\\sqrt{\\epsilon}))$ 且宽度为小的常数的网络。\n\n#### 4. 实现细节与测试用例计算\n为了实现浅而宽网络，我们需要 $S(t)$ 的具体系数。\n断点为 $c_k = -2 + k \\cdot h$，其中 $k=1, \\dots, M-1$，且 $h=4/M$。\n$S(t)$ 的斜率在每个断点 $c_k$ 处改变。斜率的变化量是 $\\beta_k$。分段线性函数在节点处的斜率等于其插值的原函数在两节点中点的导数值。段 $[c_{k-1}, c_k]$ 的斜率是 $s'( (c_{k-1}+c_k)/2 ) = c_{k-1}+c_k$。因此，斜率在 $c_k$ 处的变化是 $(c_k+c_{k+1}) - (c_{k-1}+c_k) = c_{k+1}-c_{k-1} = 2h = 8/M$。所以，对于所有 $k$，$\\beta_k = 8/M$。\n第一段的斜率是 $\\alpha_1 = c_0+c_1 = -2 + (-2+h) = h-4 = 4/M-4$。\n起始点的值是 $S(-2) = s(-2) = (-2)^2 = 4$。使用公式，$S(-2) = \\alpha_0 + \\alpha_1(-2) = 4$。\n这得到 $\\alpha_0 = 4 + 2\\alpha_1 = 4 + 2(4/M - 4) = 4 + 8/M - 8 = 8/M - 4$。\n$S(t)$ 的系数是：\n- $\\alpha_0 = \\frac{8}{M} - 4$\n- $\\alpha_1 = \\frac{4}{M} - 4$\n- $\\beta_k = \\frac{8}{M}$ 对于 $k=1, \\dots, M-1$。\n\n对于给定的测试用例，架构参数为：\n1.  **$\\epsilon = 10^{-1}$**:\n    $M = \\lceil \\sqrt{6/0.1} \\rceil = \\lceil \\sqrt{60} \\rceil = 8$。\n    宽度 $W = 3(8-1) = 21$。\n    深度 $D = \\lceil \\log_2 8 \\rceil = 3$。\n2.  **$\\epsilon = 10^{-2}$**:\n    $M = \\lceil \\sqrt{6/0.01} \\rceil = \\lceil \\sqrt{600} \\rceil = 25$。\n    宽度 $W = 3(25-1) = 72$。\n    深度 $D = \\lceil \\log_2 25 \\rceil = 5$。\n3.  **$\\epsilon = 2.5 \\times 10^{-3}$**:\n    $M = \\lceil \\sqrt{6/0.0025} \\rceil = \\lceil \\sqrt{2400} \\rceil = 49$。\n    宽度 $W = 3(49-1) = 144$。\n    深度 $D = \\lceil \\log_2 49 \\rceil = 6$。\n\n最终答案中的程序将实现此逻辑并验证误差界。", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Derives and verifies an MLP architecture for approximating f(x,y)=xy.\n\n    This function implements the shallow-wide network construction derived from\n    first principles. It calculates the required number of linear segments (M) for a\n    given error tolerance epsilon, constructs the piecewise linear approximation\n    S(t) to t^2, and evaluates the uniform approximation error for xy on a\n    fine grid over [0,1]^2. It also reports the theoretical width of this\n    network and the theoretical depth of an equivalent deep-narrow network.\n    \"\"\"\n\n    test_cases = [1e-1, 1e-2, 2.5e-3]\n    results = []\n    \n    # Use a sufficiently large grid for error verification.\n    N_GRID = 201\n\n    def S_approx(t, M):\n        \"\"\"\n        Computes the piecewise linear approximation of s(t) = t^2 on [-2, 2]\n        using M equal-length segments.\n        S(t) = alpha_0 + alpha_1*t + sum(beta_i * max(0, t - c_i))\n        \n        Args:\n            t (np.ndarray): Input values.\n            M (int): Number of linear segments.\n            \n        Returns:\n            np.ndarray: The approximated values s(t).\n        \"\"\"\n        if M == 0:\n            # Should not happen with the formula for M\n            return t**2\n        \n        is_scalar = not isinstance(t, np.ndarray)\n        if is_scalar:\n            t = np.array([t])\n            \n        # Interval [-R, R] with R=2\n        R = 2.0\n        h = 2.0 * R / float(M)\n\n        # Coefficients for the ReLU network representation\n        alpha_0 = (2.0 * h) - (2.0 * R) # Simplified from 8/M - 4\n        alpha_1 = h - (2.0 * R)       # Simplified from 4/M - 4\n        beta = 2.0 * h                # Simplified from 8/M\n\n        # Breakpoints\n        c = -R + np.arange(1, M) * h\n\n        # Reshape for broadcasting\n        t_reshaped = t.reshape(-1, 1)\n        c_reshaped = c.reshape(1, -1)\n        \n        # ReLU activations\n        relu_terms = np.sum(np.maximum(0, t_reshaped - c_reshaped), axis=1)\n        \n        # Final piecewise linear function\n        s_val = alpha_0 + alpha_1 * t + beta * relu_terms.reshape(t.shape)\n\n        if is_scalar:\n            return s_val.item()\n        return s_val\n\n    def f_hat(x, y, M):\n        \"\"\"\n        Approximates f(x,y) = xy using the identity and S_approx.\n        f_hat(x,y) = 0.5 * (S(x+y) - S(x) - S(y))\n        \"\"\"\n        return 0.5 * (S_approx(x + y, M) - S_approx(x, M) - S_approx(y, M))\n\n    for epsilon in test_cases:\n        # 1. Determine M based on the derived formula\n        M = math.ceil(math.sqrt(6.0 / epsilon))\n\n        # 2. Evaluate the uniform error on a grid\n        grid_points = np.linspace(0, 1, N_GRID)\n        X, Y = np.meshgrid(grid_points, grid_points)\n        \n        Z_true = X * Y\n        Z_hat = f_hat(X, Y, M)\n        \n        measured_error = np.max(np.abs(Z_hat - Z_true))\n\n        # 3. Calculate theoretical width of the shallow-wide network\n        # Width = 3 * (M - 1)\n        theoretical_width = 3 * (M - 1)\n\n        # 4. Calculate theoretical depth of the deep-narrow network\n        # Depth = ceil(log2(M))\n        theoretical_depth = math.ceil(math.log2(M)) if M > 0 else 0\n        \n        results.append([measured_error, theoretical_width, theoretical_depth])\n    \n    # Format the final output string exactly as required.\n    # str(list) produces \"[item1, item2, ...]\" so this will create the nested structure\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str.replace(\" \", \"\"))\n\nsolve()\n```", "id": "3155543"}, {"introduction": "构建网络只是第一步，分析其行为和属性同样至关重要。一个训练好的网络对输入的微小扰动有多敏感？这个问题在鲁棒性、对抗性攻击等领域中至关重要。本练习 [@problem_id:3155417] 将引导你使用微积分中的雅可比矩阵（Jacobian matrix）作为工具，来量化和分析多层感知器从输入到输出的敏感度。你将通过计算逐层雅可比范数，学会一种诊断和控制网络稳定性的有效方法。", "problem": "考虑一个多层感知器 (MLP)，其层由 $i \\in \\{1,2,\\dots,L\\}$ 索引，其中每一层都应用一个仿射变换，然后是一个逐元素的非线性变换。设输入为向量 $x \\in \\mathbb{R}^{n_0}$，权重为矩阵 $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$，偏置为向量 $b_i \\in \\mathbb{R}^{n_i}$，激活函数为逐元素应用的函数 $\\phi_i:\\mathbb{R} \\to \\mathbb{R}$。定义预激活 $z_i = W_i h_{i-1} + b_i$ 和激活输出 $h_i = \\phi_i(z_i)$，其中 $h_0 = x$。假设最后一层 $L$ 使用恒等激活函数（线性输出），即 $\\phi_L(u) = u$。网络输出 $h_L$ 对输入 $x$ 中微小扰动的敏感度由雅可比矩阵 $\\frac{\\partial h_L}{\\partial x}$ 捕获，根据链式法则，该矩阵是各层局部雅可比矩阵的乘积。令 $D_i = \\mathrm{diag}\\left(\\phi_i'(z_i)\\right)$ 表示第 $i$ 层激活函数导数的对角矩阵。第 $i$ 层相对于其输入的局部雅可比矩阵定义为 $J_i = D_i W_i$（对于 $i \\in \\{1,\\dots,L-1\\}$），对于最后的线性层，则为 $J_L = W_L$。全局输入-输出雅可比矩阵为 $J = J_L J_{L-1} \\cdots J_1$。矩阵 $A$ 的弗罗贝尼乌斯范数 $\\|A\\|_F$ 和谱范数 $\\|A\\|_2$ 按标准方式定义。可以通过惩罚局部雅可比范数来设计一种逐层正则化策略，以降低敏感度。\n\n你的任务是：\n- 对于给定的 MLP 和输入 $x$，计算局部雅可比矩阵 $J_i$、它们的弗罗贝尼乌斯范数 $\\|J_i\\|_F$、全局雅可比矩阵 $J$、其弗罗贝尼乌斯范数 $\\|J\\|_F$ 及其谱范数 $\\|J\\|_2$。\n- 计算由局部谱范数的乘积 $\\prod_{i=1}^{L} \\|J_i\\|_2$ 给出的 $\\|J\\|_2$ 的一个上界。\n- 对于给定的非负标量 $\\lambda_i$，提出并评估一个形式为 $R = \\sum_{i=1}^{L} \\lambda_i \\|J_i\\|_F^2$ 的逐层敏感度正则化项。\n\n从上述核心定义出发，不要假设任何无法从这些定义推导出的快捷公式。实现算法以计算所有必需的量。对隐藏层使用双曲正切函数 $\\tanh$，其导数为 $\\frac{d}{du}\\tanh(u) = 1 - \\tanh^2(u)$；对输出层使用恒等函数。\n\n测试套件：\n使用以下四个测试用例，每个用例描述一个三层 MLP ($L = 3$)，其输入维度 $n_0 = 2$，隐藏层维度 $n_1 = 2$、$n_2 = 2$，输出维度 $n_3 = 1$。隐藏层使用 $\\tanh$ 函数，输出层是线性的。对于每个用例，都指定了输入向量 $x$、权重矩阵 $W_1, W_2, W_3$、偏置向量 $b_1, b_2, b_3$ 以及逐层正则化系数 $\\lambda_1, \\lambda_2, \\lambda_3$。\n\n用例 1 (中等权重):\n- $x = \\begin{bmatrix} 0.3 \\\\ -0.2 \\end{bmatrix}$,\n- $W_1 = \\begin{bmatrix} 0.7 & -0.4 \\\\ 0.1 & 0.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.1 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} -0.2 & 0.9 \\\\ 0.3 & -0.1 \\end{bmatrix}$, $b_2 = \\begin{bmatrix} -0.05 \\\\ 0.2 \\end{bmatrix}$,\n- $W_3 = \\begin{bmatrix} 0.4 & -0.6 \\end{bmatrix}$, $b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}$,\n- $(\\lambda_1,\\lambda_2,\\lambda_3) = (0.1, 0.1, 0.1)$.\n\n用例 2 (隐藏层权重为零):\n- $x = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$,\n- $W_1 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}$, $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_3 = \\begin{bmatrix} 0.5 & 0.5 \\end{bmatrix}$, $b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}$,\n- $(\\lambda_1,\\lambda_2,\\lambda_3) = (0.5, 0.5, 0.5)$.\n\n用例 3 (饱和激活):\n- $x = \\begin{bmatrix} 5.0 \\\\ -5.0 \\end{bmatrix}$,\n- $W_1 = \\begin{bmatrix} 3.0 & 2.5 \\\\ -2.0 & -3.5 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} 2.0 & -2.0 \\\\ 3.0 & 3.0 \\end{bmatrix}$, $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_3 = \\begin{bmatrix} 1.0 & -1.0 \\end{bmatrix}$, $b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}$,\n- $(\\lambda_1,\\lambda_2,\\lambda_3) = (0.1, 1.0, 0.01)$.\n\n用例 4 (零输入时的近似恒等映射):\n- $x = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_1 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$, $b_1 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$, $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n- $W_3 = \\begin{bmatrix} 1.0 & 0.0 \\end{bmatrix}$, $b_3 = \\begin{bmatrix} 0.0 \\end{bmatrix}$,\n- $(\\lambda_1,\\lambda_2,\\lambda_3) = (1.0, 0.1, 0.01)$.\n\n程序要求：\n- 实现前向传播，以计算 $i \\in \\{1,2,3\\}$ 的 $z_i$ 和 $h_i$。\n- 使用在 $z_1$ 和 $z_2$ 处求值的导数 $1 - \\tanh^2(u)$ 来构造 $D_1$ 和 $D_2$。\n- 计算局部雅可比矩阵 $J_1 = D_1 W_1$, $J_2 = D_2 W_2$, $J_3 = W_3$；全局雅可比矩阵 $J = J_3 J_2 J_1$；局部弗罗贝尼乌斯范数 $\\|J_i\\|_F$；全局弗罗贝尼乌斯范数 $\\|J\\|_F$；通过奇异值分解计算的全局谱范数 $\\|J\\|_2$；上界 $\\prod_{i=1}^{3}\\|J_i\\|_2$；以及正则化项 $R = \\sum_{i=1}^{3} \\lambda_i \\|J_i\\|_F^2$。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表，列表内无空格。每个测试用例的结果必须是形式为 $[\\|J\\|_F,\\|J\\|_2,[\\|J_1\\|_F,\\|J_2\\|_F,\\|J_3\\|_F],\\prod_{i=1}^{3}\\|J_i\\|_2,R]$ 的列表，其中所有浮点数都精确到小数点后六位。\n- 因此，最终输出必须看起来像 $[[\\ldots],[\\ldots],[\\ldots],[\\ldots]]$，其中每个内部列表按给定顺序包含相应案例的指定量。", "solution": "该问题是有效的，因为它科学地基于应用于神经网络的微分学和线性代数原理，问题陈述清晰，提供了所有必要信息，并且其表述是客观的。任务是计算与指定多层感知器 (MLP) 的雅可比矩阵相关的各种量，该雅可比矩阵衡量网络输出对其输入的敏感度。\n\n我们将首先建立理论框架，然后将其应用于所需的具体计算。\n\n一个具有 $L$ 层的多层感知器通过一系列变换来处理输入向量 $x \\in \\mathbb{R}^{n_0}$。设 $h_0 = x$。对于随后的每一层 $i \\in \\{1, 2, \\dots, L\\}$，计算分两步进行：\n1. 一次仿射变换计算出预激活向量 $z_i \\in \\mathbb{R}^{n_i}$：\n$$z_i = W_i h_{i-1} + b_i$$\n其中 $W_i \\in \\mathbb{R}^{n_i \\times n_{i-1}}$ 是权重矩阵，$b_i \\in \\mathbb{R}^{n_i}$ 是偏置向量。\n2. 一个逐元素的激活函数 $\\phi_i: \\mathbb{R} \\to \\mathbb{R}$ 被应用以获得该层的输出激活向量 $h_i \\in \\mathbb{R}^{n_i}$：\n$$h_i = \\phi_i(z_i)$$\n问题指定隐藏层（$i < L$）使用双曲正切激活函数 $\\phi_i(u) = \\tanh(u)$，最终输出层使用恒等函数 $\\phi_L(u) = u$。\n\n我们关注的核心量是输入-输出雅可比矩阵 $J = \\frac{\\partial h_L}{\\partial x}$，它描述了最终输出 $h_L$ 如何随输入 $x$ 的无穷小变化而变化。根据微积分的链式法则，这个全局雅可比矩阵可以表示为局部雅可比矩阵的乘积，每个局部雅可比矩阵对应于单层的变换：\n$$J = \\frac{\\partial h_L}{\\partial h_{L-1}} \\frac{\\partial h_{L-1}}{\\partial h_{L-2}} \\cdots \\frac{\\partial h_1}{\\partial h_0}$$\n其中 $h_0=x$。每个局部雅可比矩阵 $\\frac{\\partial h_i}{\\partial h_{i-1}}$ 可以进一步分解为：\n$$\\frac{\\partial h_i}{\\partial h_{i-1}} = \\frac{\\partial h_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial h_{i-1}}$$\n项 $\\frac{\\partial z_i}{\\partial h_{i-1}}$ 是仿射变换 $z_i = W_i h_{i-1} + b_i$ 相对于 $h_{i-1}$ 的雅可比矩阵，它就是权重矩阵 $W_i$。\n项 $\\frac{\\partial h_i}{\\partial z_i}$ 是逐元素激活函数 $h_i = \\phi_i(z_i)$ 的雅可比矩阵。这会产生一个对角矩阵，我们记为 $D_i$，其对角线元素是在预激活向量 $z_i$ 的各分量处求值的激活函数导数：\n$$D_i = \\mathrm{diag}\\left(\\phi_i'(z_{i,1}), \\phi_i'(z_{i,2}), \\dots, \\phi_i'(z_{i,n_i})\\right) = \\mathrm{diag}(\\phi_i'(z_i))$$\n对于指定的隐藏层（$i=1, 2$），$\\phi_i(u) = \\tanh(u)$，因此其导数为 $\\phi_i'(u) = 1 - \\tanh^2(u)$。\n对于最后一层（$i=L=3$），$\\phi_L(u) = u$，因此其导数为 $\\phi_L'(u) = 1$。所以，$D_L$ 是单位矩阵 $I \\in \\mathbb{R}^{n_L \\times n_L}$。\n\n综合这些，隐藏层 $i$ 的局部雅可比矩阵是 $J_i = D_i W_i$。对于线性输出层 $L$，$J_L = D_L W_L = I W_L = W_L$。全局雅可比矩阵是其乘积：\n$$J = J_L J_{L-1} \\cdots J_1$$\n\n为给定测试用例计算所需量的算法如下：\n\n**步骤 1：前向传播**\n- 设置 $h_0 = x$。\n- 对于 $i=1, 2$：\n  - 计算 $z_i = W_i h_{i-1} + b_i$。\n  - 计算 $h_i = \\tanh(z_i)$。\n- 对于 $i=3$（输出层）：\n  - 计算 $z_3 = W_3 h_2 + b_3$。\n  - 计算 $h_3 = z_3$。\n\n**步骤 2：计算局部雅可比矩阵**\n- 对于 $i=1, 2$：\n  - 计算激活导数向量：$\\phi_i'(z_i) = 1 - h_i^2$（逐元素）。\n  - 构建对角矩阵 $D_i = \\mathrm{diag}(\\phi_i'(z_i))$。\n  - 计算局部雅可比矩阵 $J_i = D_i W_i$。\n- 对于 $i=3$：\n  - 局部雅可比矩阵为 $J_3 = W_3$。\n\n**步骤 3：计算全局雅可比矩阵**\n- 计算矩阵乘积 $J = J_3 J_2 J_1$。\n\n**步骤 4：计算范数和正则化项**\n- 对于每个矩阵 $A$（代表 $J_1, J_2, J_3,$ 和 $J$），我们计算两种范数：\n  - 弗罗贝尼乌斯范数：$\\|A\\|_F = \\sqrt{\\sum_{j,k} |A_{jk}|^2}$。\n  - 谱范数（或 $2$-范数）：$\\|A\\|_2 = \\max_{\\sigma \\in \\sigma(A)} \\sigma$，其中 $\\sigma(A)$ 是 $A$ 的奇异值集合。\n- 使用谱范数的次乘性质计算全局谱范数的上界：$\\|J\\|_2 = \\|J_3 J_2 J_1\\|_2 \\le \\|J_3\\|_2 \\|J_2\\|_2 \\|J_1\\|_2$。我们评估右侧的乘积。\n- 计算正则化项 $R$：\n  $$R = \\sum_{i=1}^{3} \\lambda_i \\|J_i\\|_F^2$$\n  其中 $\\lambda_i$ 是给定的非负标量。\n\n**步骤 5：格式化输出**\n- 每个测试用例的计算值都按指定的列表格式排列：$[\\|J\\|_F, \\|J\\|_2, [\\|J_1\\|_F, \\|J_2\\|_F, \\|J_3\\|_F], \\prod_{i=1}^{3}\\|J_i\\|_2, R]$。所有浮点数都格式化为六位小数。\n\n对所提供的四个测试用例中的每一个都实施此程序。", "answer": "```python\nimport numpy as np\n\ndef format_result(result):\n    \"\"\"Formats a single case's result list into the required string format.\"\"\"\n    J_F, J_2, local_F_norms, spec_norm_bound, R = result\n    \n    J_F_str = f\"{J_F:.6f}\"\n    J_2_str = f\"{J_2:.6f}\"\n    local_F_norms_str = f\"[{local_F_norms[0]:.6f},{local_F_norms[1]:.6f},{local_F_norms[2]:.6f}]\"\n    spec_norm_bound_str = f\"{spec_norm_bound:.6f}\"\n    R_str = f\"{R:.6f}\"\n    \n    return f\"[{J_F_str},{J_2_str},{local_F_norms_str},{spec_norm_bound_str},{R_str}]\"\n\n\ndef process_case(x, Ws, bs, lambdas):\n    \"\"\"\n    Computes all required quantities for a single MLP test case.\n    \"\"\"\n    W1, W2, W3 = Ws\n    b1, b2, b3 = bs\n    lambda1, lambda2, lambda3 = lambdas\n\n    # 1. Forward Pass\n    h0 = x\n    \n    # Layer 1\n    z1 = W1 @ h0 + b1\n    h1 = np.tanh(z1)\n    \n    # Layer 2\n    z2 = W2 @ h1 + b2\n    h2 = np.tanh(z2)\n    \n    # Layer 3 (Output) is linear, no activation function needed for h3\n\n    # 2. Local Jacobians\n    # Layer 1 (tanh)\n    phi_prime_z1 = 1 - h1**2\n    D1 = np.diag(phi_prime_z1.flatten())\n    J1 = D1 @ W1\n\n    # Layer 2 (tanh)\n    phi_prime_z2 = 1 - h2**2\n    D2 = np.diag(phi_prime_z2.flatten())\n    J2 = D2 @ W2\n\n    # Layer 3 (linear)\n    J3 = W3\n\n    # 3. Global Jacobian\n    J = J3 @ J2 @ J1\n\n    # 4. Norms and Regularization\n    # Local Frobenius norms\n    J1_F_norm = np.linalg.norm(J1, 'fro')\n    J2_F_norm = np.linalg.norm(J2, 'fro')\n    J3_F_norm = np.linalg.norm(J3, 'fro')\n    \n    # Local Spectral norms\n    J1_2_norm = np.linalg.norm(J1, 2)\n    J2_2_norm = np.linalg.norm(J2, 2)\n    J3_2_norm = np.linalg.norm(J3, 2)\n\n    # Global norms\n    J_F_norm = np.linalg.norm(J, 'fro')\n    J_2_norm = np.linalg.norm(J, 2) if J.size > 0 else 0.0\n\n    # Spectral norm upper bound\n    spec_norm_bound = J1_2_norm * J2_2_norm * J3_2_norm\n\n    # Regularization term R\n    R = (lambda1 * J1_F_norm**2 +\n         lambda2 * J2_F_norm**2 +\n         lambda3 * J3_F_norm**2)\n\n    # 5. Assemble Output\n    return [\n        J_F_norm, \n        J_2_norm, \n        [J1_F_norm, J2_F_norm, J3_F_norm], \n        spec_norm_bound, \n        R\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"x\": np.array([[0.3], [-0.2]]),\n            \"Ws\": [\n                np.array([[0.7, -0.4], [0.1, 0.5]]),\n                np.array([[-0.2, 0.9], [0.3, -0.1]]),\n                np.array([[0.4, -0.6]])\n            ],\n            \"bs\": [\n                np.array([[0.0], [0.1]]),\n                np.array([[-0.05], [0.2]]),\n                np.array([[0.0]])\n            ],\n            \"lambdas\": (0.1, 0.1, 0.1)\n        },\n        # Case 2\n        {\n            \"x\": np.array([[1.0], [-1.0]]),\n            \"Ws\": [\n                np.array([[0.0, 0.0], [0.0, 0.0]]),\n                np.array([[0.0, 0.0], [0.0, 0.0]]),\n                np.array([[0.5, 0.5]])\n            ],\n            \"bs\": [\n                np.array([[0.0], [0.0]]),\n                np.array([[0.0], [0.0]]),\n                np.array([[0.0]])\n            ],\n            \"lambdas\": (0.5, 0.5, 0.5)\n        },\n        # Case 3\n        {\n            \"x\": np.array([[5.0], [-5.0]]),\n            \"Ws\": [\n                np.array([[3.0, 2.5], [-2.0, -3.5]]),\n                np.array([[2.0, -2.0], [3.0, 3.0]]),\n                np.array([[1.0, -1.0]])\n            ],\n            \"bs\": [\n                np.array([[0.0], [0.0]]),\n                np.array([[0.0], [0.0]]),\n                np.array([[0.0]])\n            ],\n            \"lambdas\": (0.1, 1.0, 0.01)\n        },\n        # Case 4\n        {\n            \"x\": np.array([[0.0], [0.0]]),\n            \"Ws\": [\n                np.array([[1.0, 0.0], [0.0, 1.0]]),\n                np.array([[1.0, 0.0], [0.0, 1.0]]),\n                np.array([[1.0, 0.0]])\n            ],\n            \"bs\": [\n                np.array([[0.0], [0.0]]),\n                np.array([[0.0], [0.0]]),\n                np.array([[0.0]])\n            ],\n            \"lambdas\": (1.0, 0.1, 0.01)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result_data = process_case(case[\"x\"], case[\"Ws\"], case[\"bs\"], case[\"lambdas\"])\n        results.append(format_result(result_data))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3155417"}]}