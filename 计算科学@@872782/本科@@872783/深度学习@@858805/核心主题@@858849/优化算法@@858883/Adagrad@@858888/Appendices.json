{"hands_on_practices": [{"introduction": "要真正掌握一个算法，我们不仅要了解它的运作方式，还要洞悉其背后的理论支撑。这个练习旨在揭示Adagrad自适应学习率的核心思想与优化理论中的“预处理”（preconditioning）概念之间的深刻联系。通过从第一性原理出发，推导并比较Adagrad和理想化的白化特征梯度下降的更新方向[@problem_id:3095404]，你将亲身体会到Adagrad的对角线缩放是如何近似实现更复杂的预处理技术的效果的。", "problem": "考虑由经验风险 $$L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2},$$ 定义的岭正则化线性回归，其中 $X \\in \\mathbb{R}^{n \\times d}$ 是特征矩阵，$\\mathbf{y} \\in \\mathbb{R}^{n}$ 是目标向量，$\\mathbf{w} \\in \\mathbb{R}^{d}$ 是参数向量，$\\lambda \\geq 0$ 是岭系数。梯度 $\\nabla L(\\mathbf{w})$ 由基本多元微积分明确定义。设初始参数为 $\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$，并假设在 $\\mathbf{w}_{0}$ 处已知以下经验量：\n- 特征协方差矩阵为 $$S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix},$$ 它是正定的。\n- 数据-梯度项为 $$\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}.$$\n\n定义自适应梯度（Adagrad）方法，其逐坐标累加器初始化为零，且不使用数值稳定器，即 $s_{0} = \\mathbf{0}$ 且 $\\epsilon = 0$。整个过程使用固定的学习率 $\\eta > 0$。同时，考虑在完全白化的特征上执行标准梯度下降，这意味着对特征进行线性变换后得到单位协方差；在白化坐标系中执行更新，然后将结果映射回原始参数空间。\n\n仅使用基本定义和第一性原理：\n1. 从 $\\mathbf{w}_{0}$ 开始，推导在原始参数空间中的第一步 Adagrad 更新方向。\n2. 推导在白化特征空间中从相应原点开始执行一步梯度下降所产生的、在原始参数空间中的第一步更新方向。\n3. 计算这两个更新方向之间夹角的余弦值。\n\n将余弦值的精确值作为最终答案。不要四舍五入。不需要单位。你的推导过程必须从给定的基本定义（损失、梯度、协方差、白化）开始，并通过明确的推导进行；不要调用任何快捷公式。", "solution": "该问题要求计算对于一个岭回归问题，两种不同优化算法——Adagrad 和在白化特征空间中的梯度下降——其第一步更新方向之间夹角的余弦值。过程始于验证问题陈述。\n\n### 问题验证\n**第 1 步：提取已知条件**\n- 损失函数：$L(\\mathbf{w}) = \\frac{1}{2n}\\|X\\mathbf{w} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^{2}$\n- 初始参数：$\\mathbf{w}_{0} = \\mathbf{0} \\in \\mathbb{R}^{2}$\n- 特征协方差矩阵：$S = \\frac{1}{n}X^{\\top}X = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$\n- 数据-梯度项：$\\frac{1}{n}X^{\\top}\\mathbf{y} = \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix}$\n- Adagrad 参数：$s_{0} = \\mathbf{0}$，$\\epsilon = 0$，学习率 $\\eta > 0$。\n- 白化空间：梯度下降在一个特征被变换为具有单位协方差矩阵的空间中执行。\n\n**第 2 步：使用提取的已知条件进行验证**\n该问题具有科学依据，使用了优化和机器学习中的标准定义。问题是良定义的，为获得唯一解提供了所有必要的量。语言客观而精确。数据是一致的。因此，该问题被认为是有效的。\n\n### 解题推导\n首先，我们求损失函数 $L(\\mathbf{w})$ 的梯度。\n$L(\\mathbf{w}) = \\frac{1}{2n}(X\\mathbf{w} - \\mathbf{y})^{\\top}(X\\mathbf{w} - \\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\n$L(\\mathbf{w}) = \\frac{1}{2n}(\\mathbf{w}^{\\top}X^{\\top}X\\mathbf{w} - 2\\mathbf{y}^{\\top}X\\mathbf{w} + \\mathbf{y}^{\\top}\\mathbf{y}) + \\frac{\\lambda}{2}\\mathbf{w}^{\\top}\\mathbf{w}$\n关于 $\\mathbf{w}$ 的梯度是：\n$\\nabla L(\\mathbf{w}) = \\frac{1}{n}X^{\\top}X\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\n使用给定的定义，我们可以将其写为：\n$\\nabla L(\\mathbf{w}) = S\\mathbf{w} - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda\\mathbf{w}$\n\n我们需要在初始参数 $\\mathbf{w}_{0} = \\mathbf{0}$ 处计算梯度：\n$\\mathbf{g}_{0} = \\nabla L(\\mathbf{w}_{0}) = S(\\mathbf{0}) - \\frac{1}{n}X^{\\top}\\mathbf{y} + \\lambda(\\mathbf{0}) = - \\frac{1}{n}X^{\\top}\\mathbf{y}$\n代入给定值：\n$\\mathbf{g}_{0} = - \\begin{pmatrix} -2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n注意，在原点处的梯度 $\\mathbf{g}_{0}$ 与正则化参数 $\\lambda$ 无关。\n\n**1. 第一步 Adagrad 更新方向**\nAdagrad 更新规则由下式给出：\n$\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{t+1}} + \\epsilon} \\odot \\mathbf{g}_{t}$\n其中 $\\mathbf{s}_{t+1} = \\mathbf{s}_{t} + \\mathbf{g}_{t} \\odot \\mathbf{g}_{t}$（逐元素乘积）。\n对于第一步（从 $t=0$ 到 $t=1$），我们有 $\\mathbf{w}_{0} = \\mathbf{0}$，$\\mathbf{s}_{0} = \\mathbf{0}$，以及 $\\epsilon = 0$。梯度为 $\\mathbf{g}_{0}$。\n\n首先，我们更新累加器 $\\mathbf{s}$：\n$\\mathbf{s}_{1} = \\mathbf{s}_{0} + \\mathbf{g}_{0} \\odot \\mathbf{g}_{0} = \\mathbf{0} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2^2 \\\\ 1^2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}$\n接下来，我们计算参数的更新。更新向量为 $\\Delta\\mathbf{w}_{\\text{Adagrad}} = \\mathbf{w}_{1} - \\mathbf{w}_{0}$。\n$\\mathbf{w}_{1} = \\mathbf{w}_{0} - \\frac{\\eta}{\\sqrt{\\mathbf{s}_{1}} + \\epsilon} \\odot \\mathbf{g}_{0} = \\mathbf{0} - \\frac{\\eta}{\\sqrt{\\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}} + 0} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$\n$\\Delta\\mathbf{w}_{\\text{Adagrad}} = -\\eta \\begin{pmatrix} 1/\\sqrt{4} \\\\ 1/\\sqrt{1} \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1/2 \\\\ 1 \\end{pmatrix} \\odot \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} (1/2) \\cdot 2 \\\\ 1 \\cdot 1 \\end{pmatrix} = -\\eta \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$\n更新方向是任何与 $\\Delta\\mathbf{w}_{\\text{Adagrad}}$ 成比例的向量。我们可以选择方向向量 $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$。\n\n**2. 第一步白化梯度下降更新方向**\n特征白化意味着找到一个变换矩阵 $W$，使得新的特征 $\\tilde{X} = XW$ 具有单位协方差矩阵。新的协方差是 $\\tilde{S} = \\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = W^{\\top}(\\frac{1}{n}X^{\\top}X)W = W^{\\top}SW$。我们要求 $\\tilde{S} = I$。白化矩阵的一个标准选择是 $W = S^{-1/2}$，其中 $S^{-1/2}$ 是 $S$ 的逆矩阵的主平方根。由于 $S$ 是对称且正定的，所以 $S^{-1/2}$ 也是对称且正定的。使用这个选择，$W^{\\top}SW = (S^{-1/2})^{\\top} S S^{-1/2} = S^{-1/2} S S^{-1/2} = I$。\n\n为了保持预测不变，参数必须相应地进行变换：$X\\mathbf{w} = (XW)\\tilde{\\mathbf{w}} = \\tilde{X}\\tilde{\\mathbf{w}}$，这意味着 $\\mathbf{w} = W\\tilde{\\mathbf{w}}$。初始参数 $\\mathbf{w}_0 = \\mathbf{0}$ 映射到 $\\tilde{\\mathbf{w}}_0 = W^{-1}\\mathbf{0} = \\mathbf{0}$。\n\n我们在以 $\\tilde{\\mathbf{w}}$ 表示的损失函数上执行梯度下降：\n$\\tilde{L}(\\tilde{\\mathbf{w}}) = L(W\\tilde{\\mathbf{w}}) = \\frac{1}{2n}\\|\\tilde{X}\\tilde{\\mathbf{w}} - \\mathbf{y}\\|^{2} + \\frac{\\lambda}{2}\\|W\\tilde{\\mathbf{w}}\\|^{2}$\n梯度为 $\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\frac{1}{n}\\tilde{X}^{\\top}(\\tilde{X}\\tilde{\\mathbf{w}}-\\mathbf{y}) + \\lambda W^{\\top}W\\tilde{\\mathbf{w}}$。\n代入 $\\frac{1}{n}\\tilde{X}^{\\top}\\tilde{X} = I$ 和 $W^{\\top}W = (S^{-1/2})^{\\top}S^{-1/2} = S^{-1}$：\n$\\nabla_{\\tilde{\\mathbf{w}}}\\tilde{L}(\\tilde{\\mathbf{w}}) = \\tilde{\\mathbf{w}} - \\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} + \\lambda S^{-1}\\tilde{\\mathbf{w}} = (I + \\lambda S^{-1})\\tilde{\\mathbf{w}} - W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y})$。\n在 $\\tilde{\\mathbf{w}}_0 = \\mathbf{0}$ 处，梯度为：\n$\\tilde{\\mathbf{g}}_0 = -\\frac{1}{n}\\tilde{X}^{\\top}\\mathbf{y} = -W^{\\top}(\\frac{1}{n}X^{\\top}\\mathbf{y}) = -S^{-1/2}(-\\mathbf{g}_0) = S^{-1/2}\\mathbf{g}_0$。\n在白化空间中的梯度下降更新为 $\\tilde{\\mathbf{w}}_1 = \\tilde{\\mathbf{w}}_0 - \\eta' \\tilde{\\mathbf{g}}_0 = -\\eta' \\tilde{\\mathbf{g}}_0$。\n\n我们将此更新映射回原始参数空间：$\\mathbf{w}_1 = W\\tilde{\\mathbf{w}}_1$。\n更新向量为 $\\Delta\\mathbf{w}_{\\text{white}} = \\mathbf{w}_1 - \\mathbf{w}_0 = W\\tilde{\\mathbf{w}}_1 - \\mathbf{0} = S^{-1/2}(-\\eta' \\tilde{\\mathbf{g}}_0) = -\\eta' S^{-1/2} (S^{-1/2}\\mathbf{g}_0) = -\\eta' S^{-1}\\mathbf{g}_0$。\n更新方向与 $-S^{-1}\\mathbf{g}_0$ 成比例。我们来计算这个向量。\n我们有 $S = \\begin{pmatrix} 3  2 \\\\ 2  3 \\end{pmatrix}$ 和 $\\mathbf{g}_{0} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n首先，我们求 $S$ 的逆矩阵：\n$\\det(S) = 3 \\times 3 - 2 \\times 2 = 9 - 4 = 5$。\n$S^{-1} = \\frac{1}{\\det(S)}\\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix}$。\n现在我们计算方向向量：\n$-S^{-1}\\mathbf{g}_0 = -\\frac{1}{5}\\begin{pmatrix} 3  -2 \\\\ -2  3 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 3(2) - 2(1) \\\\ -2(2) + 3(1) \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 6-2 \\\\ -4+3 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} = \\frac{1}{5}\\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$。\n更新方向由任何与此成比例的向量指定。我们可以选择 $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$。\n\n**3. 两个更新方向之间夹角的余弦值**\n我们需要求 $\\mathbf{d}_{\\text{Adagrad}} = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$ 和 $\\mathbf{d}_{\\text{white}} = \\begin{pmatrix} -4 \\\\ 1 \\end{pmatrix}$ 之间夹角 $\\theta$ 的余弦值。\n余弦值由公式 $\\cos(\\theta) = \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{u}\\| \\|\\mathbf{v}\\|}$ 给出。\n点积为：\n$\\mathbf{d}_{\\text{Adagrad}} \\cdot \\mathbf{d}_{\\text{white}} = (-1)(-4) + (-1)(1) = 4 - 1 = 3$。\n向量的模（范数）为：\n$\\|\\mathbf{d}_{\\text{Adagrad}}\\| = \\sqrt{(-1)^2 + (-1)^2} = \\sqrt{1+1} = \\sqrt{2}$。\n$\\|\\mathbf{d}_{\\text{white}}\\| = \\sqrt{(-4)^2 + 1^2} = \\sqrt{16+1} = \\sqrt{17}$。\n夹角的余弦值为：\n$\\cos(\\theta) = \\frac{3}{\\sqrt{2} \\sqrt{17}} = \\frac{3}{\\sqrt{34}}$。\n为了使分母有理化，这可以写成 $\\frac{3\\sqrt{34}}{34}$，但题目不要求这样做。我们将其保留为更简单的精确形式。", "answer": "$$\\boxed{\\frac{3}{\\sqrt{34}}}$$", "id": "3095404"}, {"introduction": "理论的价值最终体现在实践中。这个编码练习将让你亲手构建一个病态条件（ill-conditioned）的优化问题，直观地观察和量化Adagrad相比于标准梯度下降（SGD）的巨大优势。通过在一个各维度曲率差异巨大的二次函数上进行实验[@problem_id:3095498]，你将看到SGD如何在不同维度上步调不一，而Adagrad如何通过其自适应机制，为每个参数量身定制学习率，从而实现更均衡、高效的收敛。", "problem": "你的任务是实现并比较随机梯度下降（SGD）和自适应梯度算法（Adagrad）在一个严格凸、可分离的二次目标函数上的表现，并量化当曲率在不同坐标轴上相差数个数量级时它们的行为。你的实现必须是一个完整的、可运行的程序，仅使用标准库和指定的科学计算库，并以精确的格式打印所需的输出。\n\n考虑由可分离凸二次函数定义的目标函数\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2,\n$$\n其中 $d$ 是维度，$\\boldsymbol{\\theta}\\in\\mathbb{R}^d$，且 $a_i \\gt 0$ 是曲率系数。该函数是可微的，其梯度由偏导数向量给出，其Hessian矩阵是对角矩阵，对角线元素为 $a_i$，这确保了函数的严格凸性。\n\n你必须使用的基本事实：\n- 对于任何可微函数 $f$，梯度是偏导数的向量，当步长足够小时，沿梯度相反方向移动会局部减小函数值（一阶最优性原理）。\n- 对于上述可分离二次函数，关于 $\\theta_i$ 的偏导数等于曲率系数与该坐标值的乘积，这是幂法则和微分线性性质的结果。\n- 随机梯度下降（SGD）是一种通过减去一个经学习率缩放的梯度来迭代更新参数的方法。\n- Adagrad（自适应梯度算法）是一种按坐标自适应步长的方法，它通过将每个坐标的步长按该坐标过去所有平方梯度累积和的平方根的倒数进行缩放，并加上一个小的正常数以确保数值稳定性。\n\n你的任务：\n1) 从相同的初始点 $\\boldsymbol{\\theta}_0$ 开始，实现两个优化过程：\n   - 随机梯度下降（SGD）：在每一步应用经典的迭代更新规则，即减去当前梯度的恒定步长倍数。\n   - 自适应梯度算法（Adagrad）：应用经典的按坐标自适应步长方案，该方案累积平方梯度，并通过该累积值加上一个固定的正常数的平方根的倒数来逐坐标地缩放步长。\n\n2) 对于每个优化器和每个测试用例，精确模拟 $T$ 步并计算：\n   - 每个优化器分别得到的最终参数向量 $\\boldsymbol{\\theta}_T$。\n   - 每个优化器的目标函数值 $f(\\boldsymbol{\\theta}_T)$。\n   - 每个优化器的每轴进度分数\n     $$\n     \\rho_i \\;=\\; \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert},\n     $$\n     该值被裁剪到区间 $\\left[0,1\\right]$ 内，以处理潜在的数值异常。\n   - 每个优化器的轴向进度各向异性指数，定义为\n     $$\n     \\mathrm{anisotropy} \\;=\\; \\frac{\\max_i \\rho_i}{\\min_i \\rho_i},\n     $$\n     约定如果 $\\min_i \\rho_i = 0$，则各向异性指数视为 $+\\infty$。\n   - 每个优化器的经验条件数敏感度，定义为对数对 $\\left(\\log a_i,\\, \\log \\rho_i\\right)$（其中 $i\\in\\{1,\\dots,d\\}$）进行最小二乘线性拟合得到的斜率。其中，对数为自然对数，并且如果需要，通过将 $\\rho_i$ 裁剪到一个小的正下界来确保其严格为正。具体来说，令 $x_i = \\log a_i$ 和 $y_i = \\log\\left(\\max(\\rho_i, \\delta)\\right)$（对于一个极小的 $\\delta \\gt 0$），计算\n     $$\n     s \\;=\\; \\frac{\\sum_{i=1}^{d} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{d} (x_i - \\bar{x})^2},\n     $$\n     其中 $\\bar{x}$ 和 $\\bar{y}$ 分别是 $\\{x_i\\}$ 和 $\\{y_i\\}$ 的样本均值。$\\lvert s\\rvert$ 越大表示进度对曲率的依赖性越强；接近 $0$ 的值表示对条件数的敏感度降低。\n\n3) 对于每个测试用例，还需计算条件数\n   $$\n   \\kappa \\;=\\; \\frac{\\max_i a_i}{\\min_i a_i}。\n   $$\n\n测试套件：\n精确实现以下三个测试用例，每个用例都包含维度 $d$、曲率向量 $\\boldsymbol{a}$、初始点 $\\boldsymbol{\\theta}_0$、步数 $T$ 和学习率超参数。以下所有数值必须完全按所提供的值使用。\n\n- 测试用例 1（理想情况，多数量级曲率，中等步数）：\n  - $d = 3$\n  - $\\boldsymbol{a} = \\left[10^{-2},\\, 1,\\, 10^{2}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10,\\, -10,\\, 10\\right]$\n  - $T = 200$\n  - SGD 学习率 $\\eta_{\\mathrm{sgd}} = 0.015$\n  - Adagrad 基础学习率 $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad 稳定性常数 $\\epsilon = 10^{-8}$\n\n- 测试用例 2（病态条件严重，混合小曲率和大曲率）：\n  - $d = 4$\n  - $\\boldsymbol{a} = \\left[10^{-4},\\, 10^{-2},\\, 1,\\, 10^{3}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[1,\\, 1,\\, 1,\\, 1\\right]$\n  - $T = 400$\n  - SGD 学习率 $\\eta_{\\mathrm{sgd}} = 0.001$\n  - Adagrad 基础学习率 $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad 稳定性常数 $\\epsilon = 10^{-8}$\n\n- 测试用例 3（极端曲率和大量步数的边界条件）：\n  - $d = 2$\n  - $\\boldsymbol{a} = \\left[1,\\, 10^{6}\\right]$\n  - $\\boldsymbol{\\theta}_0 = \\left[10^{3},\\, 10^{3}\\right]$\n  - $T = 5000$\n  - SGD 学习率 $\\eta_{\\mathrm{sgd}} = 10^{-6}$\n  - Adagrad 基础学习率 $\\eta_{\\mathrm{ada}} = 1.0$\n  - Adagrad 稳定性常数 $\\epsilon = 10^{-8}$\n\n输出规范：\n- 对于每个测试用例，按以下顺序生成一个包含 $7$ 个实数的列表：\n  $$\n  \\left[\\kappa,\\ \\mathrm{anisotropy}_{\\mathrm{sgd}},\\ \\mathrm{anisotropy}_{\\mathrm{ada}},\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{sgd}}_{T}\\right),\\ f\\!\\left(\\boldsymbol{\\theta}^{\\mathrm{ada}}_{T}\\right),\\ s_{\\mathrm{sgd}},\\ s_{\\mathrm{ada}}\\right]。\n  $$\n- 将三个测试用例的结果列表聚合到一个外部列表中，并将该外部列表打印在单一行上，不带空格，使用方括号和逗号分隔符，例如：\n  $$\n  \\left[[x_{1,1},x_{1,2},\\dots,x_{1,7}],[x_{2,1},\\dots,x_{2,7}],[x_{3,1},\\dots,x_{3,7}]\\right]。\n  $$\n- 所有输出都是不带单位的纯数字。不涉及角度。如果任何中间值可能导致出现 $\\log 0$ 的情况，你的实现必须在取对数之前将参数裁剪为一个极小的正数（例如 $\\delta = 10^{-300}$），以确保得到有限的结果。\n\n你的程序不得读取任何输入，并且必须精确实现上述值和顺序。最终的打印输出必须只包含指定格式的单行输出。", "solution": "该问题已经过验证，被认为是可靠、适定且可形式化的。所提供的参数和定义足以得到唯一的解。\n\n这个问题的核心是比较两种基本的基于梯度的优化算法——随机梯度下降（SGD）和自适应梯度算法（Adagrad）——在一种特定类型的目标函数上的性能。该函数是一个可分离的凸二次函数，它是一个理想的测试平台，因为其坐标方向上的曲率可以被精确控制，从而能够清晰地分析每种算法如何处理病态条件问题。\n\n目标函数定义为：\n$$\nf(\\boldsymbol{\\theta}) \\;=\\; \\tfrac{1}{2}\\sum_{i=1}^{d} a_i\\, \\theta_i^2\n$$\n其中 $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_d) \\in\\mathbb{R}^d$ 是参数向量，$\\boldsymbol{a} = (a_1, \\dots, a_d)$ 且 $a_i > 0$ 是曲率系数向量。该函数的全局最小值唯一地位于 $\\boldsymbol{\\theta} = \\mathbf{0}$。\n\n梯度向量 $\\nabla f(\\boldsymbol{\\theta})$ 的分量由偏导数给出：\n$$\n\\frac{\\partial f}{\\partial \\theta_i} \\;=\\; a_i \\theta_i\n$$\nHessian矩阵是一个对角矩阵，其对角线元素为 $a_i$，这证实了 $a_i$ 值确实是函数等值面（超椭球体）沿各个主轴的曲率。\n\n**随机梯度下降 (SGD)**\n\nSGD 是一种迭代优化算法，它通过将参数向梯度的反方向移动来更新参数。在步骤 $t$，整个参数向量 $\\boldsymbol{\\theta}$ 的更新规则是：\n$$\n\\boldsymbol{\\theta}_{t+1} \\;=\\; \\boldsymbol{\\theta}_t - \\eta_{\\mathrm{sgd}} \\nabla f(\\boldsymbol{\\theta}_t)\n$$\n其中 $\\eta_{\\mathrm{sgd}}$ 是学习率，一个正常数标量。由于函数是可分离的，我们可以独立分析每个分量 $\\theta_i$ 的更新：\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\eta_{\\mathrm{sgd}} (a_i \\theta_{t, i}) \\;=\\; (1 - \\eta_{\\mathrm{sgd}} a_i) \\theta_{t, i}\n$$\n这揭示了每个坐标的值都以几何级数向 $0$ 衰减。衰减速率由因子 $(1 - \\eta_{\\mathrm{sgd}} a_i)$ 决定。为使算法收敛，必须满足 $|1 - \\eta_{\\mathrm{sgd}} a_i| < 1$，这意味着 $0 < \\eta_{\\mathrm{sgd}} a_i < 2$。为确保所有坐标同时收敛，学习率的选择必须满足最大曲率的情况，即 $\\eta_{\\mathrm{sgd}} < 2 / \\max_i a_i$。然而，这样的选择会对曲率较小（$a_i$ 较小）的坐标施加一个非常慢的收敛速率，因为它们的衰减因子 $(1 - \\eta_{\\mathrm{sgd}} a_i)$ 会非常接近 $1$。这是 SGD 在病态条件问题中的根本弱点：单一的学习率无法对所有坐标都达到最优。\n\n**自适应梯度算法 (Adagrad)**\n\nAdagrad 通过使用按参数自适应的学习率来解决 SGD 的缺点。它将每个参数的学习率与该参数过去所有梯度平方和的平方根成反比进行缩放。在步骤 $t$，分量 $\\theta_i$ 的更新规则是：\n$$\n\\theta_{t+1, i} \\;=\\; \\theta_{t, i} - \\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}} g_{t,i}\n$$\n其中 $\\eta_{\\mathrm{ada}}$ 是基础学习率，$g_{t,i} = a_i \\theta_{t,i}$ 是梯度分量，$\\epsilon$ 是一个小的稳定性常数，以防止除以零。$S_{t,i}$ 项是平方梯度的累加器：\n$$\nS_{t,i} \\;=\\; \\sum_{k=1}^{t} g_{k,i}^2\n$$\n在步骤 $t$，参数 $\\theta_i$ 的有效学习率是 $\\frac{\\eta_{\\mathrm{ada}}}{\\sqrt{S_{t,i} + \\epsilon}}$。对于具有高曲率（大 $a_i$）的坐标，梯度 $g_{t,i}$ 的量级会很大，导致 $S_{t,i}$ 快速增长。这会迅速减小有效学习率，防止发散和过冲。相反，对于具有低曲率（小 $a_i$）的坐标，梯度很小，$S_{t,i}$ 增长缓慢，有效学习率保持较高水平，从而允许取得实质性进展。这种机制自动平衡了学习率，即使在条件差的问题中也能导致所有维度上更均匀的进展。\n\n**分析指标**\n\n该问题要求计算几个指标来量化每个优化器的性能和行为：\n1.  **条件数 ($\\kappa$)**: $\\kappa = \\frac{\\max_i a_i}{\\min_i a_i}$。这衡量了问题的病态程度。大的 $\\kappa$ 表示曲率之间存在巨大差异。\n2.  **每轴进度分数 ($\\rho_i$)**: $\\rho_i = \\frac{\\lvert \\theta_{0,i}\\rvert - \\lvert \\theta_{T,i}\\rvert}{\\lvert \\theta_{0,i}\\rvert}$，裁剪到 $[0,1]$。这衡量了沿每个轴向最小值前进的进度分数，其中 $1$ 表示完全前进，$0$ 表示没有前进。\n3.  **进度各向异性 ($\\mathrm{anisotropy}$)**: $\\mathrm{anisotropy} = \\frac{\\max_i \\rho_i}{\\min_i \\rho_i}$。该比率量化了各轴进展的不均匀性。值为 $1$ 表示完全各向同性（均匀）的进展。我们期望 Adagrad 产生的各向异性指数低于 SGD。\n4.  **条件数敏感度 ($s$)**: 这是 $\\log \\rho_i$ 对 $\\log a_i$ 进行线性回归的斜率。它衡量一个轴上的进展对其曲率的依赖强度。对于 SGD，我们期望存在强烈的负相关，意味着更高的曲率导致更少的进展（负斜率 $s$）。对于旨在抵消这种效应的 Adagrad，我们期望进展 $\\rho_i$ 在很大程度上独立于 $a_i$，从而导致敏感度 $s$ 接近 $0$。\n\n下面的实现将为指定的测试用例模拟 SGD 和 Adagrad，并计算这些指标，以展示它们截然不同的行为。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares SGD and Adagrad optimizers on a separable quadratic\n    objective function for a suite of test cases, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"d\": 3,\n            \"a\": np.array([1e-2, 1.0, 1e2]),\n            \"theta_0\": np.array([10.0, -10.0, 10.0]),\n            \"T\": 200,\n            \"eta_sgd\": 0.015,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 4,\n            \"a\": np.array([1e-4, 1e-2, 1.0, 1e3]),\n            \"theta_0\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"T\": 400,\n            \"eta_sgd\": 0.001,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n        {\n            \"d\": 2,\n            \"a\": np.array([1.0, 1e6]),\n            \"theta_0\": np.array([1e3, 1e3]),\n            \"T\": 5000,\n            \"eta_sgd\": 1e-6,\n            \"eta_ada\": 1.0,\n            \"epsilon\": 1e-8,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        theta_0 = case[\"theta_0\"]\n        T = case[\"T\"]\n        eta_sgd = case[\"eta_sgd\"]\n        eta_ada = case[\"eta_ada\"]\n        epsilon = case[\"epsilon\"]\n        \n        # --- SGD Simulation ---\n        theta_sgd = theta_0.copy()\n        for _ in range(T):\n            grad = a * theta_sgd\n            theta_sgd -= eta_sgd * grad\n        \n        # --- Adagrad Simulation ---\n        theta_ada = theta_0.copy()\n        sum_sq_grad = np.zeros_like(theta_ada)\n        for _ in range(T):\n            grad = a * theta_ada\n            sum_sq_grad += grad**2\n            theta_ada -= eta_ada * grad / (np.sqrt(sum_sq_grad) + epsilon)\n\n        # --- Metrics Calculation Function ---\n        def calculate_optimizer_metrics(theta_T, theta_0, a):\n            # Final objective value\n            f_val = 0.5 * np.sum(a * theta_T**2)\n            \n            # Per-axis progress fraction (rho)\n            abs_theta_0 = np.abs(theta_0)\n            # Avoid division by zero if an initial coordinate is 0\n            safe_abs_theta_0 = np.where(abs_theta_0 == 0, 1.0, abs_theta_0)\n            rho = (abs_theta_0 - np.abs(theta_T)) / safe_abs_theta_0\n            rho = np.clip(rho, 0, 1)\n\n            # Anisotropy\n            min_rho = np.min(rho)\n            if min_rho == 0.0:\n                anisotropy = np.inf\n            else:\n                anisotropy = np.max(rho) / min_rho\n            \n            # Empirical condition-number sensitivity (s)\n            # Use a tiny delta to avoid log(0) as per problem statement\n            delta = 1e-300\n            log_a = np.log(a)\n            log_rho = np.log(np.maximum(rho, delta))\n            \n            x = log_a\n            y = log_rho\n            x_mean = np.mean(x)\n            y_mean = np.mean(y)\n            \n            # The denominator is zero only if all a_i are identical.\n            numerator = np.sum((x - x_mean) * (y - y_mean))\n            denominator = np.sum((x - x_mean)**2)\n            s = numerator / denominator if denominator != 0 else 0.0\n            \n            return anisotropy, f_val, s\n\n        # --- Aggregate Results for the Case ---\n        kappa = np.max(a) / np.min(a)\n        \n        anisotropy_sgd, f_sgd, s_sgd = calculate_optimizer_metrics(theta_sgd, theta_0, a)\n        anisotropy_ada, f_ada, s_ada = calculate_optimizer_metrics(theta_ada, theta_0, a)\n\n        case_results = [\n            kappa,\n            anisotropy_sgd,\n            anisotropy_ada,\n            f_sgd,\n            f_ada,\n            s_sgd,\n            s_ada\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified\n    sublist_strings = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(sublist_strings)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3095498"}, {"introduction": "没有完美的算法，理解一个方法的局限性与了解其优势同样重要。这个练习将我们带入一个更接近现实世界的非平稳（non-stationary）场景，其中数据特征的重要性随时间发生改变。通过模拟这个过程[@problem_id:3095442]，你将发现Adagrad由于其学习率单调递减的特性，在适应新模式时可能表现迟缓，并探索一种通过“遗忘”旧梯度信息来改进其适应性的策略，这为我们后续理解RMSprop和Adam等更先进的优化器奠定了基础。", "problem": "您将构建并分析一个流式优化场景，在该场景中，一个特征在某个变化点之后变得相关。您还需要比较两种自适应步长策略。请从以下基本设定开始：线性预测器为 $f_{\\boldsymbol{w}}(\\boldsymbol{x}) = \\boldsymbol{w}^{\\top}\\boldsymbol{x}$，其通过随机梯度下降在每步平方损失 $\\ell_t(\\boldsymbol{w}) = \\tfrac{1}{2}\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)^2$ 上进行训练，该损失的梯度为 $\\nabla \\ell_t(\\boldsymbol{w}) = -\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)\\boldsymbol{x}_t$。任务是实现两种优化器，它们根据过去的梯度来修改每个坐标的学习率：一种累积所有历史平方梯度，另一种则对累积量进行指数衰减，从而更重地加权近期的梯度。\n\n构建一个包含 $2$ 个特征的非平稳数据集，其中第二个特征仅在变化时间 $t^*$ 之后才变得相关。使用 $T$ 步的流式范围，恒定的特征 $\\boldsymbol{x}_t = [1, 1]$，以及真实权重 $\\boldsymbol{w}^*_t = [1, w^*_2(t)]$，其中当 $t < t^*$ 时 $w^*_2(t) = 0$，当 $t \\ge t^*$ 时 $w^*_2(t) = 1$。数据没有观测噪声，即 $y_t = \\boldsymbol{w}^{*}_t{}^{\\top}\\boldsymbol{x}_t$。模型参数初始化为 $\\boldsymbol{w}_0 = [0,0]$。\n\n实现两种更新方案，并分别应用于相同的流数据：\n1. 一个基线累积量，它对每个坐标的平方梯度进行求和。在每个时间 $t$，设累积量为 $\\boldsymbol{G}_t$，梯度为 $\\boldsymbol{g}_t = \\nabla \\ell_t(\\boldsymbol{w}_t)$。通过加上 $\\boldsymbol{g}_t$ 的逐元素平方来更新 $\\boldsymbol{G}_t$，并通过减去梯度与一个按坐标步长的逐元素乘积来更新 $\\boldsymbol{w}_t$。该步长与累积量平方根成反比，并由一个小的常数 $\\,\\epsilon\\,$ 进行稳定。\n2. 一个衰减累积量，它构成平方梯度的指数移动平均。在每个时间 $t$，设累积量为 $\\boldsymbol{H}_t$，并以衰减参数 $\\alpha \\in (0,1)$ 将 $\\boldsymbol{H}_t$ 更新为 $\\boldsymbol{g}_t^2$ 的指数移动平均，然后使用与 $\\sqrt{\\boldsymbol{H}_t + \\epsilon}$ 成反比的按坐标步长来更新 $\\boldsymbol{w}_t$。\n\n将某个优化器下第二个坐标的适应延迟定义为变化时间 $t^*$ 之后，使得 $\\left|w_{t^*+s,\\,2} - 1\\right| \\le \\tau$ 成立的最小非负整数 $s$。该条件在每次更新后立即评估。如果在可用步数内始终未达到此阈值，则返回 $T - t^*$作为延迟。两种优化器使用相同的基础学习率 $\\eta$ 和稳定常数 $\\epsilon$。\n\n您的程序必须实现此模拟，并为每个测试用例计算一对整数 $[L_{\\text{sum}}, L_{\\text{decay}}]$，它们分别代表基线求和累积量和衰减累积量的适应延迟。\n\n测试套件：\n- 案例 1：$T=200$, $t^*=50$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$。\n- 案例 2：$T=200$, $t^*=0$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$。\n- 案例 3：$T=200$, $t^*=199$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.9$, $\\tau=0.1$。\n- 案例 4：$T=200$, $t^*=50$, $\\eta=0.2$, $\\epsilon=10^{-8}$, $\\alpha=0.99$, $\\tau=0.1$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素都是一个测试用例的结果对。例如，确切格式为 $\\left[ [L_{\\text{sum},1},L_{\\text{decay},1}], [L_{\\text{sum},2},L_{\\text{decay},2}], [L_{\\text{sum},3},L_{\\text{decay},3}], [L_{\\text{sum},4},L_{\\text{decay},4}] \\right]$。不涉及物理单位，所有角度（此处没有）均可视为无关。所有输出必须是整数。", "solution": "该问题要求在一个模拟的非平稳在线学习环境中，对两种自适应学习率优化算法进行比较分析。核心任务是实现并评估一个 Adagrad 风格的优化器（它会累积所有过去的平方梯度）和一个 RMSprop 风格的优化器（它使用平方梯度的指数移动平均），并进行比较。比较的依据是“适应延迟”，这是一个衡量指标，用于评估在底层数据生成过程发生突变后，每个优化器将模型参数调整到其新的真实值的速度。\n\n首先，我们根据规定将模拟的各个组成部分形式化。\n\n**1. 模型与学习任务**\n预测模型是输入特征 $\\boldsymbol{x}_t \\in \\mathbb{R}^2$ 的线性函数：\n$$f_{\\boldsymbol{w}}(\\boldsymbol{x}_t) = \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t$$\n其中 $\\boldsymbol{w} \\in \\mathbb{R}^2$ 是模型参数的向量。学习过程通过在每个时间步 $t$ 最小化平方误差损失来驱动：\n$$\\ell_t(\\boldsymbol{w}) = \\frac{1}{2}\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)^2$$\n损失函数关于参数 $\\boldsymbol{w}$ 的梯度（表示为 $\\boldsymbol{g}_t$）对于优化更新至关重要：\n$$\\boldsymbol{g}_t = \\nabla_{\\boldsymbol{w}} \\ell_t(\\boldsymbol{w}) = -\\left(y_t - \\boldsymbol{w}^{\\top}\\boldsymbol{x}_t\\right)\\boldsymbol{x}_t$$\n\n**2. 非平稳数据生成**\n模拟总共运行 $T$ 个时间步。该环境的特点是在时间 $t^*$ 有一个“变化点”。\n- 特征向量是恒定的：$\\boldsymbol{x}_t = [1, 1]^{\\top}$ 对于所有 $t=0, \\dots, T-1$。\n- 真实参数向量 $\\boldsymbol{w}^*_t$ 是非平稳的：\n  $$ \\boldsymbol{w}^*_t = [1, w^*_2(t)]^{\\top} \\quad \\text{其中} \\quad w^*_2(t) = \\begin{cases} 0  &\\text{若 } t < t^* \\\\ 1  &\\text{若 } t \\ge t^* \\end{cases} $$\n- 目标值 $y_t$ 由真实模型无噪声地生成：$y_t = (\\boldsymbol{w}^*_t)^{\\top}\\boldsymbol{x}_t$。这导致：\n  $$ y_t = \\begin{cases} [1, 0]^{\\top}[1, 1] = 1  &\\text{若 } t < t^* \\\\ [1, 1]^{\\top}[1, 1] = 2  &\\text{若 } t \\ge t^* \\end{cases} $$\n- 初始模型参数设置为零：$\\boldsymbol{w}_0 = [0, 0]^{\\top}$。\n\n**3. 优化器实现**\n实现了两个独立的优化器，每个都从相同的初始状态 $\\boldsymbol{w}_0$ 开始，并接收相同的数据流 $(\\boldsymbol{x}_t, y_t)$。对于 $t = 0, \\dots, T-1$：\n\n**优化器 1：平方梯度求和（Adagrad 风格）**\n该优化器维护一个累积量 $\\boldsymbol{G}_t$，它对截至时间 $t$ 所见到的所有梯度的逐元素平方进行求和。\n- **初始化**：$\\boldsymbol{G}_{-1} = \\boldsymbol{0}$。\n- **累积量更新**：$\\boldsymbol{G}_t = \\boldsymbol{G}_{t-1} + \\boldsymbol{g}_t^2$，其中平方是逐元素的。\n- **参数更新**：$\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\frac{\\eta}{\\sqrt{\\boldsymbol{G}_t + \\epsilon}} \\odot \\boldsymbol{g}_t$，其中 $\\eta$ 是基础学习率，$\\epsilon$ 是一个小的稳定常数，$\\odot$ 表示逐元素乘法，除法和平方根也是逐元素的。\n\n该方法的原理是为那些接收到较大或频繁梯度更新的参数降低学习率。每个参数的学习率 $\\eta_j = \\frac{\\eta}{\\sqrt{G_{t,j} + \\epsilon}}$ 是单调递减的。虽然这对于在凸问题上收敛是有益的，但在非平稳环境中却构成了挑战。在变化点之前累积在 $G_{t,j}$ 中的巨大数值会严重减慢对新目标值的适应速度。\n\n**优化器 2：衰减平方梯度（RMSprop 风格）**\n该优化器对累积量使用指数移动平均（EMA），这使其能够“遗忘”旧的梯度并适应近期的梯度统计信息。\n- **初始化**：$\\boldsymbol{H}_{-1} = \\boldsymbol{0}$。\n- **累积量更新**：$\\boldsymbol{H}_t = \\alpha \\boldsymbol{H}_{t-1} + (1-\\alpha) \\boldsymbol{g}_t^2$，其中 $\\alpha \\in (0,1)$ 是衰减参数。此更新是 EMA 的标准公式。\n- **参数更新**：$\\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t - \\frac{\\eta}{\\sqrt{\\boldsymbol{H}_t + \\epsilon}} \\odot \\boldsymbol{g}_t$。\n\n基于 EMA 的累积量确保了如果近期梯度较小，则每个参数的学习率可以增加；如果近期梯度较大，则可以减小。据推测，这种自适应性在给定的非平稳设置中更为优越，因为来自变化点前时期（$t < t^*$）的梯度的影响会随着时间的推移而减弱。\n\n**4. 适应延迟计算**\n用于比较的指标是适应延迟 $L$。它量化了在变化点 $t^*$ 之后，第二个参数 $w_2$ 收敛到其新的目标值 $1$ 所需的步数。\n虽然问题陈述使用了符号 $\\left|w_{t^*+s,\\,2} - 1\\right| \\le \\tau$，但字面解释存在问题，因为 $\\boldsymbol{w}_{t^*}$ 是在变化之前计算的。我们采用一种更直接的、程序化的解释，与问题的框架保持一致。我们寻找代表在变化点 $t^*$ 之后收敛所需的最小非负整数 $s$，其中 $s$ 是更新步数。\n- 模拟循环运行 $t = 0, \\dots, T-1$。\n- 收敛性检查发生在步长 $t = t^*, t^*+1, \\dots, T-1$ 的更新中。\n- 对于每个这样的步长 $t$，我们计算更新后的权重向量 $\\boldsymbol{w}_{t+1}$。\n- 然后我们检查是否满足 $|w_{t+1, 2} - 1| \\le \\tau$。\n- 如果在步长 $t$ 首次满足此条件，则适应延迟记录为 $s = t - t^*$。\n- 如果在最终更新（在 $t=T-1$ 时）之前条件都未满足，则将延迟赋予默认值 $T - t^*$，表示未能在可用时间窗口内收敛。\n\n此模拟将针对测试套件中的每组参数运行，并为每种情况确定所得的延迟对 $[L_{\\text{sum}}, L_{\\text{decay}}]$。预期在有流中变化点的情况下（例如，案例1），$L_{\\text{decay}} < L_{\\text{sum}}$，这表明衰减累积量具有更优越的自适应性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes two adaptive optimizers on a nonstationary\n    linear regression problem.\n    \"\"\"\n\n    test_cases = [\n        {'T': 200, 't_star': 50, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 0, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 199, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.9, 'tau': 0.1},\n        {'T': 200, 't_star': 50, 'eta': 0.2, 'epsilon': 1e-8, 'alpha': 0.99, 'tau': 0.1},\n    ]\n\n    results = []\n\n    for params in test_cases:\n        T = params['T']\n        t_star = params['t_star']\n        eta = params['eta']\n        epsilon = params['epsilon']\n        alpha = params['alpha']\n        tau = params['tau']\n\n        # --- Initialize parameters for both optimizers ---\n        # Weight vectors\n        w_sum = np.zeros(2)\n        w_decay = np.zeros(2)\n\n        # Accumulators\n        G = np.zeros(2)  # Summed accumulator\n        H = np.zeros(2)  # Decayed accumulator\n\n        # Lag tracking\n        lag_sum = -1\n        lag_decay = -1\n\n        # Constant feature vector\n        x_t = np.ones(2)\n\n        # --- Main simulation loop ---\n        for t in range(T):\n            # Determine true weight and target value y_t\n            if t  t_star:\n                w_star_2 = 0.0\n            else:\n                w_star_2 = 1.0\n            w_star_t = np.array([1.0, w_star_2])\n            y_t = np.dot(w_star_t, x_t)\n\n            # --- Optimizer 1: Summed Accumulator (Adagrad-style) ---\n            pred_sum = np.dot(w_sum, x_t)\n            error_sum = y_t - pred_sum\n            g_t_sum = -error_sum * x_t\n\n            G += g_t_sum**2\n            step_size_sum = eta / (np.sqrt(G) + epsilon)\n            w_sum -= step_size_sum * g_t_sum\n\n            # --- Optimizer 2: Decayed Accumulator (RMSprop-style) ---\n            pred_decay = np.dot(w_decay, x_t)\n            error_decay = y_t - pred_decay\n            g_t_decay = -error_decay * x_t\n\n            H = alpha * H + (1 - alpha) * g_t_decay**2\n            step_size_decay = eta / (np.sqrt(H) + epsilon)\n            w_decay -= step_size_decay * g_t_decay\n\n            # --- Check for adaptation lag after change point ---\n            if t >= t_star:\n                # Check for summed accumulator\n                if lag_sum == -1 and abs(w_sum[1] - 1.0) = tau:\n                    lag_sum = t - t_star\n\n                # Check for decayed accumulator\n                if lag_decay == -1 and abs(w_decay[1] - 1.0) = tau:\n                    lag_decay = t - t_star\n\n        # If threshold was never met, set lag to the default value\n        if lag_sum == -1:\n            lag_sum = T - t_star\n        if lag_decay == -1:\n            lag_decay = T - t_star\n            \n        results.append([lag_sum, lag_decay])\n\n    # Format the final output string exactly as specified\n    result_str = '[' + ','.join(f'[{res[0]},{res[1]}]' for res in results) + ']'\n    print(result_str)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3095442"}]}