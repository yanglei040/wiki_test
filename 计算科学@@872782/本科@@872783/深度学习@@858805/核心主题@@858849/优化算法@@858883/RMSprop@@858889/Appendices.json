{"hands_on_practices": [{"introduction": "要掌握 RMSprop，我们必须首先对其核心机制——“记忆”功能——建立直观的理解。这项练习 [@problem_id:3170912] 设计了一个带有周期性梯度“尖峰”的简化思想实验，以帮助您分析衰减参数 $\\rho$ 的作用。通过推导累加器的稳态行为，您将看到 RMSprop 的记忆功能如何帮助其平滑并缓冲突然的大梯度，从而防止不稳定的更新。", "problem": "考虑均方根传播（RMSprop），这是一种随机优化方法，它通过维护过去平方梯度的衰减指数加权平均值来稳定参数更新。设衰减率参数为 $\\rho \\in (0,1)$，并设平方梯度序列由周期为 $k \\in \\mathbb{N}$ 的周期性模式定义：具体来说，对于时间索引 $t \\in \\mathbb{N}$，当 $t$ 是 $k$ 的正整数倍时，令 $g_t^2 = M$，否则 $g_t^2 = 1$，其中 $M$ 是一个满足 $M \\gg 1$ 的固定常数。假设 RMSprop 累加器初始化为 $v_0 = 0$，并且训练已经进行了足够长的时间，使得 $v_t$ 达到了与梯度序列同步的时间周期性稳态。\n\n从 RMSprop 形成一个几何权重之和为1的平方梯度指数加权移动平均这一原理出发，推导累加器的递推关系，并分析其在上述周期性输入下的行为。在稳态下，设 $v_t$ 在每个峰值时间 $t = nk$（对于 $n \\in \\mathbb{N}$）之后立即进行评估；将此值记为 $s$，在稳态下，该值在不同峰值之间是恒定的。\n\n确定 $s$ 作为 $\\rho$、$M$ 和 $k$ 的函数的闭式解析表达式。请以单个简化的符号表达式形式提供最终答案。无需进行四舍五入。", "solution": "问题要求在特定的平方梯度周期序列下，求出均方根传播（RMSprop）累加器 $v_t$ 的稳态值 $s$ 的闭式表达式。\n\n首先，我们建立 RMSprop 累加器 $v_t$ 的基本递推关系。它被定义为平方梯度 $g_t^2$ 的指数加权移动平均。更新规则如下：\n$$v_t = \\rho v_{t-1} + (1-\\rho) g_t^2$$\n其中 $\\rho \\in (0,1)$ 是衰减率参数，$v_t$ 是时间步 $t$ 时的累加器值。问题给出了初始条件 $v_0 = 0$，但重点是经过足够多的时间步后达到的时间周期性稳态。\n\n平方梯度序列 $g_t^2$ 遵循周期为 $k \\in \\mathbb{N}$ 的周期性模式：\n$$g_t^2 = \\begin{cases} M  & \\text{if } t = nk \\text{ for some } n \\in \\mathbb{N} \\\\ 1  & \\text{otherwise} \\end{cases}$$\n其中 $M$ 是一个常数。这意味着在是 $k$ 的倍数的时间步，存在一个值为 $M$ 的“峰值”，而在所有其他时间步，值为 $1$。\n\n我们关心的是累加器的稳态值。问题将 $s$ 定义为在每个峰值之后（即对于大的 $n$，在时间 $t=nk$ 时）立即评估的 $v_t$ 的值。稳态条件意味着这个值从一个峰值到下一个峰值是恒定的。数学上，如果我们将时间 $(n-1)k$ 峰值刚过的累加器值记为 $v_{(n-1)k}$，将时间 $nk$ 峰值刚过的累加器值记为 $v_{nk}$，那么在稳态下：\n$$v_{(n-1)k} = v_{nk} = s$$\n\n我们的策略是将累加器值在一个完整周期内（从时间 $t=(n-1)k$ 到 $t=nk$）进行传播，然后施加稳态条件。让我们假设周期开始时的累加器值为 $v_{(n-1)k} = s$。\n\n该周期包含 $k-1$ 个 $g_t^2=1$ 的步骤，然后是一个 $g_t^2=M$ 的步骤。\n\n对于从 $t = (n-1)k + 1$ 到 $t = nk - 1$ 的步骤，平方梯度为 $g_t^2 = 1$。让我们计算在时间 $nk$ 峰值之前的累加器值，即 $v_{nk-1}$。\n从 $v_{(n-1)k} = s$ 开始：\n$$v_{(n-1)k+1} = \\rho v_{(n-1)k} + (1-\\rho)g_{(n-1)k+1}^2 = \\rho s + (1-\\rho)(1)$$\n$$v_{(n-1)k+2} = \\rho v_{(n-1)k+1} + (1-\\rho)g_{(n-1)k+2}^2 = \\rho(\\rho s + 1-\\rho) + (1-\\rho) = \\rho^2 s + \\rho(1-\\rho) + (1-\\rho)$$\n通过归纳法，经过 $j$ 步（其中 $j \\in \\{1, 2, \\dots, k-1\\}$），累加器的值为：\n$$v_{(n-1)k+j} = \\rho^j v_{(n-1)k} + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i g_{(n-1)k+j-i}^2$$\n由于在这个区间内 $g_t^2=1$，这可以简化为：\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\sum_{i=0}^{j-1} \\rho^i$$\n该和是一个有限几何级数：$\\sum_{i=0}^{j-1} \\rho^i = \\frac{1-\\rho^j}{1-\\rho}$。\n将此代入 $v_{(n-1)k+j}$ 的表达式中：\n$$v_{(n-1)k+j} = \\rho^j s + (1-\\rho) \\frac{1-\\rho^j}{1-\\rho} = \\rho^j s + 1 - \\rho^j$$\n为了找到在 $t=nk$ 峰值前的值，我们设 $j=k-1$：\n$$v_{nk-1} = \\rho^{k-1} s + 1 - \\rho^{k-1}$$\n\n现在，我们对 $t=nk$ 处的峰值执行最后的更新步骤。在这个时间步，平方梯度为 $g_{nk}^2 = M$。\n$$v_{nk} = \\rho v_{nk-1} + (1-\\rho) g_{nk}^2$$\n代入 $v_{nk-1}$ 的表达式：\n$$v_{nk} = \\rho (\\rho^{k-1} s + 1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho(1 - \\rho^{k-1}) + (1-\\rho)M$$\n$$v_{nk} = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\n\n我们现在应用稳态条件 $v_{nk} = s$：\n$$s = \\rho^k s + \\rho - \\rho^k + M(1-\\rho)$$\n我们的目标是解出这个关于 $s$ 的方程。\n$$s - \\rho^k s = M(1-\\rho) + \\rho - \\rho^k$$\n$$s(1-\\rho^k) = M(1-\\rho) + \\rho - \\rho^k$$\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k}$$\n\n为了得到一个更易于解释的形式，我们可以对分子进行处理。一个更有条理的重排方法是分离表达式：\n$$s = \\frac{M(1-\\rho) + \\rho - \\rho^k}{1-\\rho^k} = \\frac{M(1-\\rho) - (1-\\rho) + 1 - \\rho^k}{1-\\rho^k}$$\n等式成立，因为 $- (1-\\rho) + 1 - \\rho^k = -1 + \\rho + 1 - \\rho^k = \\rho - \\rho^k$。\n现在，对各项进行分组：\n$$s = \\frac{(M-1)(1-\\rho) + (1-\\rho^k)}{1-\\rho^k}$$\n将分数拆分为两部分：\n$$s = \\frac{(M-1)(1-\\rho)}{1-\\rho^k} + \\frac{1-\\rho^k}{1-\\rho^k}$$\n$$s = 1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}$$\n这就是 $s$ 的最终简化闭式表达式。这个形式有一个清晰的解释：稳态值是一个基线值 $1$（如果 $g_t^2$ 始终为 $1$ 时的结果）加上一个附加项，该附加项表示来自幅度为 $M$ 的周期性峰值的贡献。", "answer": "$$\n\\boxed{1 + \\frac{(M-1)(1-\\rho)}{1-\\rho^k}}\n$$", "id": "3170912"}, {"introduction": "最后的这项练习 [@problem_id:3170932] 通过解决 RMSprop 的一个常见弱点——在存在持续梯度偏差时性能下降的问题，将理论与实践联系起来。您将实现标准算法以及两种旨在修正这种偏差的高级变体：中心化 RMSprop 和去趋势化 RMSprop。通过亲手编码和模拟，您将直接比较它们的收敛行为，并理解这些基于原则的修改所带来的实际好处。", "problem": "考虑一个一维随机优化问题，学习者旨在最小化一个具有不完美梯度预言机的凸二次目标函数。设底层目标函数为 $f(x)$，该函数是强凸且可微的。假设学习者使用随机梯度下降（SGD）以及典型的更新规则 $x_{t+1} = x_t - \\alpha \\, g_t$，其中 $g_t$ 是一个随机梯度，$\\alpha$ 是一个正学习率，$t$ 是离散时间的索引。随机梯度被建模为 $g_t = h(x_t) + b + \\xi_t$，其中 $h(x_t)$ 是目标函数的真实梯度，$b$ 是一个常数偏差项，反映了梯度预言机非零的期望（例如，由于测量漂移），而 $\\xi_t$ 是零均值、有限方差的噪声。期望算子表示为 $\\mathbb{E}[\\cdot]$。\n\n均方根传播（Root Mean Square Propagation, RMSprop）是一种自适应方法，它通过对近期梯度幅度的运行估计来缩放 SGD 的更新步长，该估计是通过指数加权移动平均（Exponentially Weighted Moving Average, EWMA）构建的。对于一个序列 $\\{s_t\\}$，衰减参数为 $\\beta \\in (0,1)$ 的 EWMA 是一种基于指数加权基础定义的递归估计器，它为过去的值赋予几何衰减的权重。平方梯度的非中心化 EWMA 跟踪的是二阶矩 $\\mathbb{E}[g_t^2]$，而不是方差。当 $\\mathbb{E}[g_t] \\neq 0$ 时，二阶矩等于方差加上均值的平方，这可能导致运行估计因偏差而被夸大。一个有原则的替代方法是使用中心化的 EWMA（即减去均值的运行估计），从而近似方差 $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$。另一种干预措施是去趋势更新，它在更新参数之前直接从梯度中减去运行均值。\n\n你的任务是，严格基于上述 SGD 和 EWMA 的定义，推导并实现三种算法：\n- 非中心化 RMSprop：使用 $g_t^2$ 的 EWMA 来缩放 SGD 步长。\n- 中心化 RMSprop：通过结合 $g_t$ 和 $g_t^2$ 的 EWMA 来估计 $\\operatorname{Var}(g_t)$，并使用此估计来缩放步长。\n- 去趋势 RMSprop：在求平方和更新参数之前，从 $g_t$ 中减去 $\\mathbb{E}[g_t]$ 的运行估计。\n\n然后，在一个对应于二次目标函数的一维随机梯度的受控类别上模拟它们的行为。具体来说，设真实目标函数为 $f(x) = \\tfrac{1}{2} a (x - x^\\star)^2$，其梯度为 $h(x) = a (x - x^\\star)$，其中 $a > 0$，$x^\\star$ 是唯一的最小化点。梯度预言机提供 $g_t = a (x_t - x^\\star) + b + \\xi_t$，其中 $b$ 是一个固定的偏差，$\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$ 是独立噪声。为保证科学真实性，请确保在每个测试案例中，所有三种算法都使用相同的噪声实现 $\\{\\xi_t\\}_{t=1}^T$，以便差异仅由更新规则产生。\n\n对每个测试案例，使用相同的超参数实现所有三种算法：学习率 $\\alpha$、EWMA 衰减率 $\\beta$、分母中的加性稳定项 $\\varepsilon$ 以及步数 $T$。在 $x_0$ 处初始化参数，并将所有 EWMA 初始化为零。在 $T$ 步之后，报告绝对误差 $|x_T - x^\\star|$。\n\n使用以下参数值的测试套件，该套件旨在探讨不同的方面：\n- 测试案例 1（偏差和噪声适中的标准情形）：$a = 1.5$, $x^\\star = 2.0$, $x_0 = -5.0$, $b = 0.6$, $\\sigma = 0.2$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 1000$，噪声序列的固定随机种子为 $42$。\n- 测试案例 2（边界情况，无噪声，无偏差）：$a = 1.0$, $x^\\star = -3.0$, $x_0 = 10.0$, $b = 0.0$, $\\sigma = 0.0$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 500$，固定随机种子为 $123$。\n- 测试案例 3（高噪声和非零偏差的显著边缘情况）：$a = 0.8$, $x^\\star = 1.0$, $x_0 = -1.0$, $b = 0.5$, $\\sigma = 2.0$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 1500$，固定随机种子为 $999$。\n- 测试案例 4（接近最优解的起点，噪声小，有非零偏差）：$a = 2.5$, $x^\\star = 0.0$, $x_0 = 0.1$, $b = 0.2$, $\\sigma = 0.05$, $\\alpha = 0.01$, $\\beta = 0.99$, $\\varepsilon = 10^{-8}$, $T = 800$，固定随机种子为 $777$。\n\n对于每个测试案例，计算三个浮点数：非中心化 RMSprop、中心化 RMSprop 和去趋势 RMSprop 的最终绝对误差 $|x_T - x^\\star|$，按此顺序排列。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表，内含 12 个结果，按测试案例排序，每个案例贡献三个数字，顺序与上述方法顺序一致。例如，输出格式必须完全像 $[r_{1, \\text{unc}}, r_{1, \\text{cent}}, r_{1, \\text{det}}, r_{2, \\text{unc}}, r_{2, \\text{cent}}, r_{2, \\text{det}}, r_{3, \\text{unc}}, r_{3, \\text{cent}}, r_{3, \\text{det}}, r_{4, \\text{unc}}, r_{4, \\text{cent}}, r_{4, \\text{det}}]$，其中每个 $r_{\\cdot}$ 都是一个浮点数。", "solution": "该任务是基于所提供的形式化描述，推导并实现均方根传播（RMSprop）算法的三种变体，然后在特定的一维优化问题上模拟它们的性能。这三种变体是非中心化 RMSprop、中心化 RMSprop 和去趋势 RMSprop。模拟将在一个具有带偏、带噪声的梯度预言机的二次目标函数 $f(x) = \\frac{1}{2} a (x - x^\\star)^2$ 上进行。\n\n首先，我们建立通用的数学框架。在时间步 $t$ 的随机梯度由下式给出：\n$$g_t = a(x_t - x^\\star) + b + \\xi_t$$\n其中 $x_t$ 是参数值，$a$ 是曲率，$x^\\star$ 是最小化点，$b$ 是常数偏差，$\\xi_t$ 是一个均值为零、方差为 $\\sigma^2$ 的噪声项。\n\nRMSprop 及其变体的核心是使用指数加权移动平均（EWMA）来估计梯度的统计特性。对于一个序列 $\\{s_t\\}$，衰减参数为 $\\beta \\in (0,1)$ 的 EWMA 的递归公式是：\n$$E_t[s] = \\beta E_{t-1}[s] + (1-\\beta)s_t$$\n其中 $E_t[s]$ 是在时间 $t$ 的移动平均值。所有 EWMA 累加器都初始化为 0。\n\n这些自适应方法的参数更新的通用结构是：\n$$x_{t+1} = x_t - \\alpha \\cdot \\text{scaled\\_gradient}_t$$\n其中 $\\alpha$ 是学习率。我们现在将推导这三种算法的具体更新规则。模拟从给定的 $x_0$ 开始，运行 $t = 0, 1, \\dots, T-1$。\n\n**1. 非中心化 RMSprop 推导**\n\n这是标准的 RMSprop 算法。它通过梯度的均方根来缩放学习率。“非中心化”指的是使用梯度的原始二阶矩 $\\mathbb{E}[g_t^2]$作为缩放因子的基础，该二阶矩通过 $g_t^2$ 的 EWMA 进行估计。\n\n设 $v_t$ 是平方梯度 $g_t^2$ 的 EWMA。\n$$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$$\n参数 $x$ 的更新规则使用随机梯度 $g_t$，并由该移动平均的平方根的倒数进行缩放。分母中包含一个加性稳定项 $\\varepsilon > 0$ 以防止除以零。\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} g_t$$\n该算法很简单，但当梯度的均值 $\\mathbb{E}[g_t]$ 非零时（即当存在偏差 $b$ 或梯度在远离最优点时持续非零），其表现可能不是最优的。在这种情况下，$v_t$ 估计的是 $\\mathbb{E}[g_t^2] = \\operatorname{Var}(g_t) + (\\mathbb{E}[g_t])^2$，导致运行估计被均值的平方所夸大，可能导致步长过于保守。\n\n**2. 中心化 RMSprop 推导**\n\n该变体旨在通过使用梯度的方差估计 $\\operatorname{Var}(g_t)$ 而不是二阶矩来进行缩放，以修正非中心化版本的问题。方差由 $\\operatorname{Var}(g_t) = \\mathbb{E}[g_t^2] - (\\mathbb{E}[g_t])^2$ 给出。这需要估计梯度的一阶矩（均值）和二阶矩。\n\n设 $m_t$ 是梯度 $g_t$ 的 EWMA，用于估计 $\\mathbb{E}[g_t]$。\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\n设 $v_t$ 是平方梯度 $g_t^2$ 的 EWMA，用于估计 $\\mathbb{E}[g_t^2]$（与非中心化 RMSprop 中相同）。\n$$v_t = \\beta v_{t-1} + (1-\\beta) g_t^2$$\n然后，根据当前的移动平均构造在步骤 $t$ 的方差估计：\n$$\\hat{\\sigma}_t^2 = v_t - m_t^2$$\n参数更新规则使用此方差估计进行缩放。更新方向仍然使用原始的随机梯度 $g_t$。\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{\\hat{\\sigma}_t^2} + \\varepsilon} g_t$$\n当存在显著偏差时，这种方法通过更准确地衡量梯度的噪声水平来归一化步长，从而可能改善收敛性。\n\n**3. 去趋势 RMSprop 推导**\n\n该算法采用更直接的方法来处理非零均值梯度。它首先估计梯度的均值，然后从原始随机梯度中减去这个“趋势”。这个“去趋势”后的梯度随后被用于参数更新和自适应缩放因子的计算。\n\n首先，我们计算梯度的 EWMA $m_t$，就像在中心化 RMSprop 中一样。\n$$m_t = \\beta m_{t-1} + (1-\\beta) g_t$$\n接下来，我们通过从原始梯度中减去当前的运行均值估计来形成去趋势梯度 $\\tilde{g}_t$。\n$$\\tilde{g}_t = g_t - m_t$$\n缩放因子基于*去趋势*梯度平方的 EWMA。设这个 EWMA 为 $v_t$。\n$$v_t = \\beta v_{t-1} + (1-\\beta) \\tilde{g}_t^2$$\n参数更新使用去趋势梯度 $\\tilde{g}_t$ 作为步长方向，并由从 $v_t$ 导出的自适应速率进行缩放。\n$$x_{t+1} = x_t - \\frac{\\alpha}{\\sqrt{v_t} + \\varepsilon} \\tilde{g}_t$$\n此方法试图通过主动移除估计的偏差来同时校正更新步长的大小和方向。这在概念上与 Adam 优化器相似，尽管它省略了针对初始迭代的正式偏差校正步骤。\n\n实现将为每个测试案例模拟这三种算法，使用相同的随机噪声序列 $\\xi_t$ 以确保公平比较。对每个算法和测试案例，报告最终的绝对误差 $|x_T - x^\\star|$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_uncentered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Uncentered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        v = beta * v + (1 - beta) * g**2\n        x = x - (alpha / (np.sqrt(v) + epsilon)) * g\n    \n    return np.abs(x - x_star)\n\ndef run_centered_rmsprop(params, noise):\n    \"\"\"\n    Simulates Centered RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        v = beta * v + (1 - beta) * g**2\n        var_est = v - m**2\n        \n        # In case var_est becomes slightly negative due to floating point inaccuracies,\n        # we take the max with 0 before sqrt.\n        denominator = np.sqrt(max(0, var_est)) + epsilon\n        x = x - (alpha / denominator) * g\n\n    return np.abs(x - x_star)\n\ndef run_detrended_rmsprop(params, noise):\n    \"\"\"\n    Simulates Detrended RMSprop.\n    \"\"\"\n    a, x_star, x0, b, _, alpha, beta, epsilon, T = (\n        params['a'], params['x_star'], params['x0'], params['b'],\n        params['sigma'], params['alpha'], params['beta'], params['epsilon'], params['T']\n    )\n    x = float(x0)\n    m = 0.0\n    v = 0.0\n\n    for t in range(T):\n        g = a * (x - x_star) + b + noise[t]\n        m = beta * m + (1 - beta) * g\n        g_tilde = g - m\n        v = beta * v + (1 - beta) * g_tilde**2\n\n        denominator = np.sqrt(v) + epsilon\n        x = x - (alpha / denominator) * g_tilde\n\n    return np.abs(x - x_star)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": 1.5, \"x_star\": 2.0, \"x0\": -5.0, \"b\": 0.6, \"sigma\": 0.2, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1000, \"seed\": 42\n        },\n        {\n            \"a\": 1.0, \"x_star\": -3.0, \"x0\": 10.0, \"b\": 0.0, \"sigma\": 0.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 500, \"seed\": 123\n        },\n        {\n            \"a\": 0.8, \"x_star\": 1.0, \"x0\": -1.0, \"b\": 0.5, \"sigma\": 2.0, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 1500, \"seed\": 999\n        },\n        {\n            \"a\": 2.5, \"x_star\": 0.0, \"x0\": 0.1, \"b\": 0.2, \"sigma\": 0.05, \n            \"alpha\": 0.01, \"beta\": 0.99, \"epsilon\": 1e-8, \"T\": 800, \"seed\": 777\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        noise = rng.normal(0, case['sigma'], case['T'])\n        \n        # Uncentered RMSprop\n        # In the original Centered RMSProp code, a potential NaN issue was noted if var_est  0.\n        # While the derivation is correct, in implementation floating point errors can make v_t - m_t^2 slightly negative.\n        # The provided code in the problem for Centered RMSProp didn't handle this. A robust implementation would add max(0, var_est).\n        # To strictly follow the derivation which doesn't specify this, I'll run as is, but a small modification in the\n        # run_centered_rmsprop function to np.sqrt(max(0, var_est)) makes it more robust without changing the core logic.\n        # I have added this minor safeguard to the python code to prevent NaNs.\n\n        result_unc = run_uncentered_rmsprop(case, noise)\n        all_results.append(result_unc)\n        \n        result_cen = run_centered_rmsprop(case, noise)\n        all_results.append(result_cen)\n\n        result_det = run_detrended_rmsprop(case, noise)\n        all_results.append(result_det)\n\n    # Format the output as specified. Using a higher precision for floating point numbers.\n    print(f\"[{','.join(f'{r:.12f}' for r in all_results)}]\")\n\nsolve()\n```", "id": "3170932"}]}