{"hands_on_practices": [{"introduction": "像 Adam 这样的自适应算法的有效性，很大程度上取决于它们跟踪损失曲面几何形状的能力。本练习将探讨超参数 $\\beta_2$ 如何控制二阶矩估计器 $v_t$ 的“记忆”，从而影响其对梯度统计量（在此作为曲率的代理）突变的响应速度。通过模拟这一确定性过程，你将对优化中的稳定性和敏捷性之间的权衡有一个定量的认识。[@problem_id:3096921]", "problem": "要求你设计并分析一个简化的训练场景，以研究自适应矩估计（Adam）中的二阶原始矩估计器如何对损失曲率的突变做出反应。考虑一个一维参数，其损失由一个二次函数局部近似，该函数的曲率在指定的迭代索引处会发生突变。在此设定下，梯度平方的真实二阶矩被建模为一个分段常数序列，该序列会经历突然的状态转换（相变）。\n\n基本原理和设置：\n- 令每次迭代的随机梯度表示为 $g_t$。自适应矩估计（Adam）中使用的二阶原始矩估计器是一个指数移动平均（EMA），定义为\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, g_t^2,\n$$\n其中 $v_t$ 是第 $t$ 次迭代时的估计值，$\\beta_2 \\in [0,1)$ 是二阶矩的衰减参数。\n- 为了在不受优化动态混淆的情况下独立研究对曲率变化的响应性，我们将随机输入 $g_t^2$ 替换为其依赖于状态的真实二阶矩 $m_t = \\mathbb{E}[g_t^2]$。我们假设 $m_t$ 随时间分段恒定，并在指定的迭代索引处发生突变。在此替换下，估计器通过以下方式确定性地演化\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t,\n$$\n且 $v_0 = 0$。\n- 在 $m_t$ 的每次突变之后，将响应性度量定义为最小的非负整数延迟 $\\ell$（以迭代次数衡量），使得 $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta \\, m_{\\text{new}}$ 成立。其中，$t_c$ 是变化的迭代索引，$m_{\\text{new}}$ 是变化后 $m_t$ 的新常数值，$\\delta \\in (0,1)$ 是指定的容差。如果在给定的时间范围 $T$ 内未能满足容差，则将延迟记录为 $T - t_c$。一个测试用例的最终分数是该用例中所有变化的延迟的平均值，四舍五入到最接近的整数。\n\n你的任务：\n- 对几个测试场景，在固定的时间范围 $T$ 内实现确定性更新 $v_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t$。每个场景由 $(\\beta_2, T, \\{t_c\\}, \\{m\\}, \\delta)$ 定义，其中 $\\{t_c\\}$ 是一个递增的变化索引列表，$\\{m\\}$ 是 $m_t$ 的状态值列表，其长度为 $|\\{t_c\\}| + 1$。\n- 对每个场景，计算所有变化的平均稳定时间，并四舍五入到最接近的整数。\n\n测试套件：\n使用以下六个场景，它们探索了一系列行为，包括一般情况、大幅度的向上和向下相变、多次转变以及极慢的适应过程。在下面的每一项中，所有给定的数学实体都必须被精确使用。\n\n1. 场景 $1$：$\\beta_2 = 0.0$，$T = 200$，变化点在 $\\{50, 120\\}$，状态值 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n2. 场景 $2$：$\\beta_2 = 0.9$，$T = 200$，变化点在 $\\{50, 120\\}$，状态值 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n3. 场景 $3$：$\\beta_2 = 0.99$，$T = 200$，变化点在 $\\{50, 120\\}$，状态值 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n4. 场景 $4$：$\\beta_2 = 0.999$，$T = 200$，变化点在 $\\{50, 120\\}$，状态值 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n5. 场景 $5$：$\\beta_2 = 0.99$，$T = 200$，变化点在 $\\{20, 40, 80, 120\\}$，状态值 $\\{1.0, 4.0, 0.5, 2.0, 1.5\\}$，$\\delta = 0.05$。\n6. 场景 $6$：$\\beta_2 = 0.9999$，$T = 1000$，变化点在 $\\{50, 120\\}$，状态值 $\\{1.0, 10.0, 0.1\\}$，$\\delta = 0.05$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目对应一个场景的四舍五入后的平均稳定时间，顺序为场景 1 到 6（例如，$[r_1,r_2,r_3,r_4,r_5,r_6]$）。所有条目都必须是整数。本问题不涉及任何物理单位或角度；所有量都是无量纲的。", "solution": "用户提供了一个问题，要求分析自适应矩估计（Adam）算法中简化的二阶矩估计器对损失景观曲率突变的响应。该分析将在一个确定性模型下进行，其中随机的梯度平方被其分段常数的期望值所取代。\n\n问题的核心是控制二阶矩估计 $v_t$ 演化的确定性一阶线性递推关系：\n$$\nv_t = \\beta_2 \\, v_{t-1} + (1 - \\beta_2)\\, m_t\n$$\n初始条件为 $v_0 = 0$。此处，$v_t$ 是第 $t$ 次迭代时的估计量，$\\beta_2 \\in [0, 1)$ 是指数衰减率，而 $m_t$ 是梯度的真实二阶原始矩，它被给定为一个关于 $t$ 的分段常数函数。\n\n为了理解其动态，考虑一个 $m_t$ 恒定的时期，例如 $m_t = m_{\\text{const}}$。该递推关系的固定点是 $v = m_{\\text{const}}$。让我们将与此固定点的误差或偏差定义为 $\\epsilon_t = v_t - m_{\\text{const}}$。将其代入更新规则可得：\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 (\\epsilon_{t-1} + m_{\\text{const}}) + (1 - \\beta_2) m_{\\text{const}}\n$$\n$$\n\\epsilon_t + m_{\\text{const}} = \\beta_2 \\epsilon_{t-1} + \\beta_2 m_{\\text{const}} + m_{\\text{const}} - \\beta_2 m_{\\text{const}}\n$$\n$$\n\\epsilon_t = \\beta_2 \\epsilon_{t-1}\n$$\n这表明与目标矩的偏差在每次迭代中都以因子 $\\beta_2$ 指数衰减。因此，在进入一个新的恒定状态 $\\ell$ 次迭代后，初始偏差 $\\epsilon_0$ 将会减少一个因子 $\\beta_2^\\ell$。$\\beta_2$ 的值接近 1 意味着衰减缓慢，因此对新矩的适应也慢，因为估计器保留了对过去值的长期记忆。相反，$\\beta_2$ 为 0 意味着即时适应，因为 $v_t = m_t$。\n\n问题要求一个响应性度量：延迟 $\\ell$，即在迭代 $t_c$ 发生变化后，$v_t$ 稳定到新目标矩 $m_{\\text{new}}$ 的一个容差范围 $\\delta$ 内所需的迭代次数。具体来说，我们必须找到最小的非负整数 $\\ell$，使得 $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$。\n\n虽然可以为孤立的状态变化推导出 $\\ell$ 的解析表达式，但问题指定了具有多个连续变化的场景。在 $t_c$ 发生变化后，$v_t$ 的演化不仅受新矩 $m_{\\text{new}}$ 的影响，还受在 $v_t$ 完全收敛之前发生的任何后续 $m_t$ 变化的影响。这种复杂性使得直接模拟该过程成为解决此问题最稳健和最忠实的方法。\n\n算法流程如下：\n1.  **构建矩序列**：对于每个场景，我们首先根据给定的变化点 $\\{t_c\\}$ 和状态值 $\\{m\\}$，为从 1 到时间范围 $T$ 的所有迭代 $t$ 构建完整的真实矩序列 $m_t$。\n2.  **模拟估计器的演化**：然后，我们模拟估计器 $v_t$ 在整个时间范围内的演化。从 $v_0 = 0$ 开始，我们使用更新规则 $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) m_t$ 和预先计算的矩序列，迭代计算从 $t=1$ 到 $T$ 的每个 $v_t$。\n3.  **计算稳定延迟**：对于场景中每个指定的变化点 $t_c$，我们分析生成的 $v_t$ 序列以找到稳定延迟。\n    - 新的目标矩是 $m_{\\text{new}}$，它是从 $t_c$ 开始的状态下 $m_t$ 序列的值。\n    - 收敛判据是 $|v_{t_c+\\ell} - m_{\\text{new}}| \\le \\delta m_{\\text{new}}$。\n    - 我们搜索满足此条件的最小非负整数 $\\ell$（从 $\\ell=0$ 到 $T-t_c$）。\n    - 如果在搜索窗口内（即直到时间 $t=T$）的任何 $\\ell$ 都不满足该条件，则根据问题规范，将延迟记录为最大可能值 $T-t_c$。\n4.  **计算最终分数**：每个场景的最终分数是所有计算出的延迟的算术平均值，四舍五入到最接近的整数。\n\n这个计算过程精确地模拟了指定的确定性系统，并允许在复杂的多阶段状态转换下精确计算响应性度量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_mean_lag(beta2, T, changes, regimes, delta):\n    \"\"\"\n    Calculates the rounded mean settling time for a single scenario.\n    \"\"\"\n    # Step 1: Construct the piecewise-constant moment sequence m_t\n    # m_seq is 1-indexed for problem clarity, so size is T+1\n    m_seq = np.zeros(T + 1)\n    \n    # Define the start and end of each regime\n    regime_starts = [1] + changes\n    regime_ends = changes + [T + 1]\n    \n    for i, m_val in enumerate(regimes):\n        start_idx = regime_starts[i]\n        end_idx = regime_ends[i]\n        if start_idx  end_idx: # Ensure the interval is valid\n            m_seq[start_idx:end_idx] = m_val\n\n    # Step 2: Simulate the evolution of the estimator v_t\n    # v_seq is 0-indexed for v_0, so size is T+1\n    v_seq = np.zeros(T + 1)  # v_seq[0] is v_0 = 0\n    \n    for t in range(1, T + 1):\n        v_seq[t] = beta2 * v_seq[t-1] + (1 - beta2) * m_seq[t]\n        \n    # Step 3: Calculate settling lags for each change\n    all_lags = []\n    for i, tc in enumerate(changes):\n        m_new = regimes[i + 1]\n        threshold = delta * m_new\n        lag_found = False\n        \n        # Search for the smallest non-negative lag l\n        max_l = T - tc\n        for l in range(max_l + 1):\n            t = tc + l\n            if abs(v_seq[t] - m_new) = threshold:\n                all_lags.append(l)\n                lag_found = True\n                break\n        \n        # If tolerance is not met, use the capped value\n        if not lag_found:\n            all_lags.append(max_l)\n            \n    # Step 4: Compute the final score (rounded mean lag)\n    if not all_lags:\n        return 0\n        \n    mean_lag = np.mean(all_lags)\n    return int(round(mean_lag))\n\ndef solve():\n    \"\"\"\n    Main function to run all test scenarios and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (beta2, T, changes, regimes, delta)\n        (0.0, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.9, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.999, 200, [50, 120], [1.0, 10.0, 0.1], 0.05),\n        (0.99, 200, [20, 40, 80, 120], [1.0, 4.0, 0.5, 2.0, 1.5], 0.05),\n        (0.9999, 1000, [50, 120], [1.0, 10.0, 0.1], 0.05)\n    ]\n\n    results = []\n    for case in test_cases:\n        beta2, T, changes, regimes, delta = case\n        result = calculate_mean_lag(beta2, T, changes, regimes, delta)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3096921"}, {"introduction": "在实践中，梯度可能表现出复杂的模式，例如在不同方向之间持续振荡。本练习深入研究了结合动量和自适应缩放的优化器，在面对确定性的周期性梯度信号时的稳态行为。通过推导和计算其长期动态，你将揭示动量和自适应缩放如何以某些非直观的方式相互作用，共同决定最终的优化方向。[@problem_id:3097011]", "problem": "您需要分析一个结合了动量和自适应步长缩放的一维优化器。考虑一个标量参数序列 $ \\theta_t $，它使用动量累积量 $ m_t $ 和二阶矩累积量 $ v_t $ 按如下方式更新。动量是梯度的指数移动平均（Exponential Moving Average, EMA），定义为 $ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $。二阶矩累积量是梯度平方的EMA，定义为 $ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $。参数通过一个缩放后的步长进行更新 $ \\theta_{t+1} = \\theta_t - \\alpha \\, m_t / \\sqrt{v_t + \\varepsilon} $，其中 $ \\alpha  0 $ 是基础学习率，$ \\varepsilon  0 $ 是一个很小的常数。\n\n假设一个周期为 $ 2 $ 的确定性周期梯度信号：\n- 在偶数步长 $ t = 0, 2, 4, \\dots $，梯度为 $ g_t = +a $。\n- 在奇数步长 $ t = 1, 3, 5, \\dots $，梯度为 $ g_t = -b $。\n这里 $ a  0 $ 和 $ b  0 $ 是常数。这个场景模拟了在两个梯度大小不等、符号相反的循环批次之间交替的情况。\n\n从 $ m_t $、$ v_t $ 的定义和上述更新规则出发，完成以下任务：\n1) 当周期性梯度已施加很长时间后，推导内部状态 $ (m_t, v_t) $ 趋近的稳态双周期值 $ (m_{\\mathrm{even}}, v_{\\mathrm{even}}) $ 和 $ (m_{\\mathrm{odd}}, v_{\\mathrm{odd}}) $。请仅用 $ a $、$ b $ 和 $ \\beta_1 $ 表示 $ m_{\\mathrm{even}} $ 和 $ m_{\\mathrm{odd}} $，并仅用 $ a $、$ b $ 和 $ \\beta_2 $ 表示 $ v_{\\mathrm{even}} $ 和 $ v_{\\mathrm{odd}} $。您必须直接从EMA定义出发进行推导，不得引入任何未经证明的简化。\n\n2) 使用稳态值，推导参数在一个完整周期内的净两步漂移，\n$$\n\\Delta_\\theta \\equiv \\theta_{t+2} - \\theta_t = - \\alpha \\left( \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}} + \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}} \\right),\n$$\n并将其尽可能地简化为 $ \\alpha, \\beta_1, \\beta_2, a, b, \\varepsilon $ 的函数。\n\n3) 实现一个程序，该程序：\n- 对于下面测试套件中的每组参数，使用第1部分得到的精确闭式稳态值计算 $ \\Delta_\\theta $。\n- 生成单行输出，其中包含用逗号分隔的十进制数列表形式的结果，四舍五入到 $ 12 $ 位小数，并用方括号括起来，例如，$ [0.123000000000,-0.045600000000] $。\n\n使用以下参数集 $ (\\alpha, \\beta_1, \\beta_2, a, b, \\varepsilon) $ 的测试套件：\n- 情况 $ 1 $ (理想情况): $ (0.001, 0.9, 0.999, 1.0, 0.5, 1\\mathrm{e}{-8}) $。\n- 情况 $ 2 $ (无动量): $ (0.001, 0.0, 0.9, 1.0, 0.5, 1\\mathrm{e}{-8}) $。\n- 情况 $ 3 $ (瞬时二阶矩): $ (0.001, 0.9, 0.0, 1.0, 0.5, 1\\mathrm{e}{-8}) $。\n- 情况 $ 4 $ (对称幅度): $ (0.001, 0.9, 0.999, 1.0, 1.0, 1\\mathrm{e}{-8}) $。\n- 情况 $ 5 $ (极端平滑和不平衡): $ (0.001, 0.99, 0.9999, 1.5, 0.2, 1\\mathrm{e}{-8}) $。\n\n所有量均为无量纲。您的程序必须按顺序计算这五种情况下的五个 $ \\Delta_\\theta $ 值，并按照上述确切格式在单行中打印它们。不需要也不允许用户输入。最终输出必须只有一行，不含任何额外文本。", "solution": "该问题要求在确定性的周期性梯度信号下，分析一个带​​有动量和自适应缩放的一维优化器，类似于Adam优化器。我们必须首先验证问题陈述。该问题具有科学依据，定义明确且客观。它基于数值优化和指数移动平均的既定原则，提供了一套完整且一致的定义和参数。在简化的周期性输入下分析优化器的稳态行为是理解其动态特性的标准且有价值的技术。因此，该问题被认为是有效的。我们着手求解。\n\n问题的核心是确定在双周期梯度下，动量累积量 $m_t$ 和二阶矩累积量 $v_t$ 的稳态行为。\n\n更新规则如下：\n$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n$\\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\varepsilon}}$\n\n梯度 $g_t$ 遵循一个双周期：\n$g_t = +a$ 对于偶数 $t$ ($t=0, 2, \\dots$)\n$g_t = -b$ 对于奇数 $t$ ($t=1, 3, \\dots$)\n其中 $a  0$ 且 $b  0$。\n\n**1. 稳态双周期值的推导**\n\n在稳态下，系统进入一个双周期，其中偶数步结束时的累积量值是恒定的，奇数步结束时的值也是恒定的。我们将这些稳态值表示为 $(m_{\\mathrm{even}}, v_{\\mathrm{even}})$ 和 $(m_{\\mathrm{odd}}, v_{\\mathrm{odd}})$。\n\n**动量累积量 ($m_t$)**\n\n考虑一个偶数时间步 $t$。动量是 $m_t$，在稳态下趋近于 $m_{\\mathrm{even}}$。前一个动量是 $m_{t-1}$，趋近于 $m_{\\mathrm{odd}}$。梯度是 $g_t = a$。递推关系变为：\n$$m_{\\mathrm{even}} = \\beta_1 m_{\\mathrm{odd}} + (1 - \\beta_1) a \\quad (1)$$\n\n现在考虑随后的奇数时间步 $t+1$。动量是 $m_{t+1}$，趋近于 $m_{\\mathrm{odd}}$。前一个动量是 $m_t$，趋近于 $m_{\\mathrm{even}}$。梯度是 $g_{t+1} = -b$。递推关系变为：\n$$m_{\\mathrm{odd}} = \\beta_1 m_{\\mathrm{even}} + (1 - \\beta_1) (-b) \\quad (2)$$\n\n我们得到了一个关于 $m_{\\mathrm{even}}$ 和 $m_{\\mathrm{odd}}$ 的二元线性方程组。为了求解它，我们可以将方程(2)代入方程(1)：\n$$m_{\\mathrm{even}} = \\beta_1 (\\beta_1 m_{\\mathrm{even}} - (1 - \\beta_1) b) + (1 - \\beta_1) a$$\n$$m_{\\mathrm{even}} = \\beta_1^2 m_{\\mathrm{even}} - \\beta_1 (1 - \\beta_1) b + (1 - \\beta_1) a$$\n$$m_{\\mathrm{even}} (1 - \\beta_1^2) = (1 - \\beta_1) a - \\beta_1 (1 - \\beta_1) b$$\n假设 $\\beta_1 \\neq 1$，我们可以两边同除以 $(1 - \\beta_1)$:\n$$m_{\\mathrm{even}} (1 + \\beta_1) = a - \\beta_1 b$$\n$$m_{\\mathrm{even}} = \\frac{a - \\beta_1 b}{1 + \\beta_1}$$\n\n现在，将此结果代回方程(2)以求 $m_{\\mathrm{odd}}$:\n$$m_{\\mathrm{odd}} = \\beta_1 \\left(\\frac{a - \\beta_1 b}{1 + \\beta_1}\\right) - (1 - \\beta_1) b$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b}{1 + \\beta_1} - \\frac{(1 - \\beta_1)(1 + \\beta_1)b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b - (1 - \\beta_1^2)b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - \\beta_1^2 b - b + \\beta_1^2 b}{1 + \\beta_1}$$\n$$m_{\\mathrm{odd}} = \\frac{\\beta_1 a - b}{1 + \\beta_1}$$\n\n因此，稳态动量值为：\n$$m_{\\mathrm{even}} = \\frac{a - \\beta_1 b}{1 + \\beta_1} \\quad \\text{和} \\quad m_{\\mathrm{odd}} = \\frac{\\beta_1 a - b}{1 + \\beta_1}$$\n\n**二阶矩累积量 ($v_t$)**\n\n$v_t$ 的推导是完全类似的。递推关系为 $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$。梯度平方为：\n$g_t^2 = a^2$ 对于偶数 $t$。\n$g_t^2 = (-b)^2 = b^2$ 对于奇数 $t$。\n\n稳态值 $v_{\\mathrm{even}}$ 和 $v_{\\mathrm{odd}}$ 的方程组为：\n$$v_{\\mathrm{even}} = \\beta_2 v_{\\mathrm{odd}} + (1 - \\beta_2) a^2 \\quad (3)$$\n$$v_{\\mathrm{odd}} = \\beta_2 v_{\\mathrm{even}} + (1 - \\beta_2) b^2 \\quad (4)$$\n\n该方程组的结构与动量的方程组相同，只需进行替换：$\\beta_1 \\to \\beta_2$，$a \\to a^2$，以及 $-b \\to b^2$。将这些替换应用到 $m_t$ 的解上得到：\n$$v_{\\mathrm{even}} = \\frac{a^2 - \\beta_2 (-b^2)}{1 + \\beta_2} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + (-b^2)}{1 + \\beta_2} = \\frac{\\beta_2 a^2 - b^2}{1 + \\beta_2} \\quad \\text{（错误的类比应用）}$$\n\n为避免符号错误，直接重新推导更安全。将(4)代入(3)：\n$$v_{\\mathrm{even}} = \\beta_2 (\\beta_2 v_{\\mathrm{even}} + (1 - \\beta_2) b^2) + (1 - \\beta_2) a^2$$\n$$v_{\\mathrm{even}} (1 - \\beta_2^2) = \\beta_2 (1 - \\beta_2) b^2 + (1 - \\beta_2) a^2$$\n$$v_{\\mathrm{even}} (1 + \\beta_2) = \\beta_2 b^2 + a^2$$\n$$v_{\\mathrm{even}} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}$$\n\n将此结果代入(4)：\n$$v_{\\mathrm{odd}} = \\beta_2 \\left(\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2}\\right) + (1 - \\beta_2) b^2$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + \\beta_2^2 b^2 + (1 - \\beta_2^2)b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + \\beta_2^2 b^2 + b^2 - \\beta_2^2 b^2}{1 + \\beta_2}$$\n$$v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2}$$\n\n正确的稳态二阶矩值为：\n$$v_{\\mathrm{even}} = \\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} \\quad \\text{和} \\quad v_{\\mathrm{odd}} = \\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2}$$\n\n**2. 净两步漂移（$\\Delta_\\theta$）的推导**\n\n一个完整周期（两步）内的净漂移 $\\Delta_\\theta$ 定义为 $\\theta_{t+2} - \\theta_t$。假设步长 $t$ 是一个偶数步。参数更新如下：\n第1步（从 $t$ 到 $t+1$）：\n$$\\theta_{t+1} = \\theta_t - \\alpha \\frac{m_t}{\\sqrt{v_t + \\varepsilon}}$$\n在稳态下，这变为：\n$$\\theta_{t+1} - \\theta_t = - \\alpha \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}}$$\n\n第2步（从 $t+1$ 到 $t+2$）：\n$$\\theta_{t+2} = \\theta_{t+1} - \\alpha \\frac{m_{t+1}}{\\sqrt{v_{t+1} + \\varepsilon}}$$\n在稳态下，这变为：\n$$\\theta_{t+2} - \\theta_{t+1} = - \\alpha \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}}$$\n\n总漂移是这两次变化的总和：\n$$\\Delta_\\theta = (\\theta_{t+2} - \\theta_{t+1}) + (\\theta_{t+1} - \\theta_t) = - \\alpha \\left( \\frac{m_{\\mathrm{even}}}{\\sqrt{v_{\\mathrm{even}} + \\varepsilon}} + \\frac{m_{\\mathrm{odd}}}{\\sqrt{v_{\\mathrm{odd}} + \\varepsilon}} \\right)$$\n\n代入推导出的稳态表达式，得到净漂移的最终公式：\n$$ \\Delta_\\theta = - \\alpha \\left( \\frac{\\frac{a - \\beta_1 b}{1 + \\beta_1}}{\\sqrt{\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} + \\varepsilon}} + \\frac{\\frac{\\beta_1 a - b}{1 + \\beta_1}}{\\sqrt{\\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2} + \\varepsilon}} \\right) $$\n该表达式可以稍微进行因式分解：\n$$ \\Delta_\\theta = - \\frac{\\alpha}{1 + \\beta_1} \\left( \\frac{a - \\beta_1 b}{\\sqrt{\\frac{a^2 + \\beta_2 b^2}{1 + \\beta_2} + \\varepsilon}} + \\frac{\\beta_1 a - b}{\\sqrt{\\frac{\\beta_2 a^2 + b^2}{1 + \\beta_2} + \\varepsilon}} \\right) $$\n此表达式是两步漂移的闭式解，将用于计算数值结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the net two-step drift of a parameter for a one-dimensional\n    optimizer with periodic gradients, based on a derived closed-form solution\n    for the steady-state behavior of its internal accumulators.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (alpha, beta1, beta2, a, b, epsilon)\n    test_cases = [\n        (0.001, 0.9, 0.999, 1.0, 0.5, 1e-8),      # Case 1 (happy path)\n        (0.001, 0.0, 0.9, 1.0, 0.5, 1e-8),       # Case 2 (no momentum)\n        (0.001, 0.9, 0.0, 1.0, 0.5, 1e-8),       # Case 3 (instantaneous second moment)\n        (0.001, 0.9, 0.999, 1.0, 1.0, 1e-8),      # Case 4 (symmetric magnitudes)\n        (0.001, 0.99, 0.9999, 1.5, 0.2, 1e-8),   # Case 5 (extreme smoothing and imbalance)\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, beta1, beta2, a, b, epsilon = case\n\n        # Calculate steady-state momentum values\n        # m_even = (a - beta1 * b) / (1 + beta1)\n        # m_odd = (beta1 * a - b) / (1 + beta1)\n        # Handle beta1 = -1 case to avoid division by zero, though not in test cases\n        if beta1 == -1.0:\n            # The recurrence does not converge to a 2-cycle in this case.\n            # However, problem constraints imply beta1 is not -1.\n            # For a numerically robust implementation, one might add handling.\n            # We proceed assuming beta1 != -1 as per standard optimizer design.\n            pass\n\n        m_even = (a - beta1 * b) / (1.0 + beta1)\n        m_odd = (beta1 * a - b) / (1.0 + beta1)\n\n        # Calculate steady-state second-moment values\n        # v_even = (a**2 + beta2 * b**2) / (1 + beta2)\n        # v_odd = (beta2 * a**2 + b**2) / (1 + beta2)\n        # Handle beta2 = -1 case.\n        if beta2 == -1.0:\n            pass # Similar logic as for beta1\n        \n        v_even = (a**2 + beta2 * b**2) / (1.0 + beta2)\n        v_odd = (beta2 * a**2 + b**2) / (1.0 + beta2)\n\n        # Calculate the two terms of the parameter update\n        term_even = m_even / np.sqrt(v_even + epsilon)\n        term_odd = m_odd / np.sqrt(v_odd + epsilon)\n\n        # Calculate the net two-step drift delta_theta\n        delta_theta = -alpha * (term_even + term_odd)\n        \n        results.append(delta_theta)\n\n    # Final print statement in the exact required format.\n    # The format specifier .12f ensures rounding to 12 decimal places.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3097011"}, {"introduction": "自适应方法虽然功能强大，但可能对罕见的、幅度极大的梯度值非常敏感，从而导致不稳定的参数更新。本练习将介绍一种精巧的解决方案：尺度感知梯度裁剪（scale-aware gradient clipping），其裁剪阈值会根据优化器的内部状态（即梯度的历史尺度）进行自适应调整。你将首先从第一性原理出发推导该技术的数学合理性，然后通过编程实现来验证其在抑制爆炸性更新的同时，如何保留自适应学习率的核心优势。[@problem_id:3097005]", "problem": "要求您从第一性原理出发，对自适应学习率算法中的尺度感知裁剪（scale-aware clipping）进行推理，并在一个完整的、可运行的程序中实现并测试您的结论。从标准的均方根传播（Root Mean Square Propagation, RMSProp）更新开始，这是一种自适应方法，通过梯度的平方的指数移动平均来重新缩放每个参数的梯度。具体来说，设 $i$ 为参数索引，$t$ 为步骤索引。设 $g_{i,t}$ 表示在步骤 $t$ 处可微目标函数的随机梯度，二阶矩累积量 $v_{i,t}$ 由指数递归式 $v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$ 定义，其中 $v_{i,0} = 0$ 且 $\\beta \\in [0,1)$。在时间 $t$ 时，参数 $i$ 的基准 RMSProp 步长为 $\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\, g_{i,t} / \\left(\\sqrt{v_{i,t-1}} + \\varepsilon\\right)$，其中学习率 $\\alpha  0$，稳定项 $\\varepsilon \\ge 0$，然后进行 $\\theta_{i,t} \\leftarrow \\theta_{i,t-1} + \\Delta \\theta_{i,t}$ 更新。请注意，预处理使用 $v_{i,t-1}$ 是为了避免在步骤 $t$ 内出现循环依赖。\n\n任务 A（推导）。提出并论证一个逐参数的裁剪规则，该规则与二阶矩累积量的平方根成正比，并由一个常数 $\\lambda  0$ 和相同的稳定项 $\\varepsilon \\ge 0$ 参数化。您的规则应以裁剪后的梯度 $\\tilde{g}_{i,t}$ 关于 $v_{i,t-1}$、$\\lambda$ 和 $\\varepsilon$ 的显式不等式形式给出。根据上述 RMSProp 的定义和您的裁剪规则，推导出一个形式为 $\\lvert \\Delta \\theta_{i,t} \\rvert \\le \\text{(与 $g_{i,t}$ 无关的表达式)}$ 的步长界限，该界限不依赖于 $g_{i,t}$ 的无界大小，并解释为什么这种方法有望在减少更新爆炸的同时，保留 RMSProp 的自适应缩放优势。\n\n任务 B（实现与测试）。在一个合成的凸二次目标函数 $f(\\theta) = \\tfrac{1}{2} \\sum_{i=1}^{d} c_i \\, \\theta_i^2$ 上实现两个优化器，其中 $d$ 是维度，曲率向量 $c \\in \\mathbb{R}^{d}_{0}$ 在每个测试中提供。精确梯度为 $g_{i,t}^{\\text{base}} = c_i \\, \\theta_{i,t-1}$。为模拟突发性或重尾随机性，步骤 $t$ 中用于更新的观测梯度为 $g_{i,t} = g_{i,t}^{\\text{base}} + s_{i,t}$，其中尖峰计划（spike schedule）$s_{i,t}$ 是一个稀疏向量，其非零项仅出现在指定的 $(t,i)$ 对上。实现：\n- 基准 RMSProp 变体（无裁剪）：在步长计算中使用 $g_{i,t}$，并按标准递归式更新 $v_{i,t}$。\n- 尺度感知裁剪变体：将您提出的逐参数裁剪应用于 $g_{i,t}$ 以获得 $\\tilde{g}_{i,t}$，然后在步长计算中使用 $\\tilde{g}_{i,t}$，同时仍使用未修改的 $g_{i,t}$ 按标准递归式更新 $v_{i,t}$。\n\n对于每次运行，跟踪最大绝对逐参数步长大小 $M = \\max_{t,i} \\lvert \\Delta \\theta_{i,t} \\rvert$ 和在 $T$ 步结束时的最终目标函数值 $f(\\theta_T)$。\n\n为每个测试用例 $j$ 定义两个评估谓词：\n- 更新爆炸减少谓词 $E_j$：如果基准变体相对于提供的阈值 $\\tau$ 表现出过大的步长，而裁剪变体的最大步长受您推导的界限约束，则为真。形式上，如果 $\\left(M^{\\text{base}}  \\tau\\right)$ 且 $\\left(M^{\\text{clip}} \\le \\alpha \\lambda + \\text{tol}\\right)$，则 $E_j$ 为真，其中 $\\text{tol}  0$ 是一个小的数值容差。\n- 自适应优势保持谓词 $P_j$：如果裁剪变体的最终损失不比基准变体的最终损失差过一个提供的相对容差 $\\delta  0$，即 $f^{\\text{clip}}(\\theta_T) \\le (1+\\delta) \\, f^{\\text{base}}(\\theta_T)$，则为真。\n\n对于每个测试用例 $j$，如果 $E_j$ 和 $P_j$ 均为真，则输出 $r_j = 1$，否则输出 $r_j = 0$。\n\n测试套件。您的程序必须精确运行以下 3 个测试用例，每个用例由一个包含 $(d, c, \\theta_0, T, \\alpha, \\beta, \\varepsilon, \\lambda, \\text{spikes}, \\tau, \\delta, \\text{tol})$ 的元组指定：\n- 用例 1（在低曲率坐标上出现大尖峰的理想路径）：\n  - $d = 3$\n  - $c = [1.0, 0.1, 10.0]$\n  - $\\theta_0 = [1.0, 1.0, 1.0]$\n  - $T = 60$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 2.0$\n  - spikes: 在步骤 $t=10$ 时，在坐标 $i=2$（从零开始的索引）上增加 $+200.0$；在步骤 $t=20$ 时，在坐标 $i=2$ 上增加 $-150.0$\n  - $\\tau = 50.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n- 用例 2（无尖峰，裁剪应处于非活动状态的边界条件）：\n  - $d = 3$\n  - $c = [1.0, 0.3, 3.0]$\n  - $\\theta_0 = [1.5, -0.5, 0.75]$\n  - $T = 100$\n  - $\\alpha = 0.05$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 5.0$\n  - spikes: 无\n  - $\\tau = 1.0$\n  - $\\delta = 0.05$\n  - $\\text{tol} = 10^{-12}$\n- 用例 3（在一个极低曲率坐标上出现多个尖峰，并在其他地方出现一个中等尖峰）：\n  - $d = 4$\n  - $c = [1.0, 0.2, 5.0, 0.05]$\n  - $\\theta_0 = [1.0, 1.0, 1.0, 1.0]$\n  - $T = 120$\n  - $\\alpha = 0.04$\n  - $\\beta = 0.9$\n  - $\\varepsilon = 10^{-3}$\n  - $\\lambda = 1.5$\n  - spikes: 在步骤 $t=5$ 时，在坐标 $i=3$ 上增加 $+300.0$；在步骤 $t=6$ 时，在坐标 $i=3$ 上增加 $-300.0$；在步骤 $t=30$ 时，在坐标 $i=0$ 上增加 $+150.0$\n  - $\\tau = 30.0$\n  - $\\delta = 0.5$\n  - $\\text{tol} = 10^{-12}$\n\n角度单位不适用。未使用物理单位。您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，无空格，并按测试用例的顺序排列。例如，如果 $r_1 = 1$，$r_2 = 0$，$r_3 = 1$，则要求的输出为“[1,0,1]”。", "solution": "我们对问题陈述进行了严格的验证。该问题被认为是科学基础扎实、提法恰当的，并且为得到唯一且有意义的解提供了所有必要的数据和条件。因此，该问题被视为**有效**的。在测试用例 1 的描述性文本中发现了一个微小的不一致之处，该文本提到在“低曲率坐标”上存在一个尖峰，而提供的数据 `c = [1.0, 0.1, 10.0]` 和 `spikes` 在坐标 `i=2` 上的设置，实际上是将尖峰放在了曲率*最高*的坐标上 ($c_2=10.0$)。这被认为是一个非关键性的描述错误，因为问题的形式化规范是明确且自洽的。我们现在开始进行解答。\n\n### 任务 A：尺度感知裁剪和步长界限的推导\n\n目标是为 RMSProp 优化器提出并论证一个逐参数的裁剪规则，该规则与梯度的估计尺度成正比。RMSProp 算法维护一个逐参数的二阶矩累积量 $v_{i,t}$，它跟踪梯度平方的指数移动平均：\n$$v_{i,t} \\leftarrow \\beta v_{i,t-1} + (1-\\beta) g_{i,t}^{2}$$\n这里，$g_{i,t}$ 是目标函数在步骤 $t$ 时关于参数 $\\theta_i$ 的梯度，$\\beta \\in [0,1)$ 是一个衰减因子。参数 $i$ 最近梯度的量级由 $\\sqrt{v_{i,t-1}}$ 估计。标准的 RMSProp 更新通过这个尺度估计（加上一个小的稳定项 $\\varepsilon \\ge 0$）来归一化梯度：\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{g_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n\n**提议的裁剪规则**\n\n为了使裁剪具有“尺度感知”能力，我们定义一个裁剪阈值，该阈值本身与梯度尺度的估计值成正比。问题指定该阈值应由一个常数 $\\lambda  0$ 参数化。对于步骤 $t$ 的参数 $i$，一个自然的选择是裁剪阈值 $C_{i,t} = \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$。这直接将裁剪界限与 RMSProp 更新本身所使用的自适应缩放因子联系起来。\n\n该裁剪规则将原始梯度 $g_{i,t}$ 转换为裁剪后的梯度 $\\tilde{g}_{i,t}$。我们强制要求裁剪后梯度的量级不超过此阈值。这可以表述为关于裁剪后梯度 $\\tilde{g}_{i,t}$ 的以下不等式：\n$$|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$$\n此规则可以通过将 $g_{i,t}$ 的值裁剪到范围 $[-C_{i,t}, C_{i,t}]$ 内来实现。\n\n**步长界限的推导**\n\n建立了尺度感知裁剪规则后，我们现在考虑使用裁剪后梯度 $\\tilde{g}_{i,t}$ 的修改后的参数更新：\n$$\\Delta \\theta_{i,t} \\leftarrow -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n为了推导此更新步长量级 $|\\Delta \\theta_{i,t}|$ 的界限，我们取表达式的绝对值：\n$$|\\Delta \\theta_{i,t}| = \\left| -\\alpha \\frac{\\tilde{g}_{i,t}}{\\sqrt{v_{i,t-1}} + \\varepsilon} \\right| = \\alpha \\frac{|\\tilde{g}_{i,t}|}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n现在，我们代入我们提议的裁剪规则中的不等式 $|\\tilde{g}_{i,t}| \\le \\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)$:\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\frac{\\lambda (\\sqrt{v_{i,t-1}} + \\varepsilon)}{\\sqrt{v_{i,t-1}} + \\varepsilon}$$\n项 $(\\sqrt{v_{i,t-1}} + \\varepsilon)$ 被消去，得到最终的步长界限：\n$$|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$$\n这个界限是一个优雅的结果：参数更新的量级保证不大于学习率 $\\alpha$ 和裁剪参数 $\\lambda$ 的乘积。关键是，这个界限完全独立于原始、未裁剪梯度 $g_{i,t}$ 的量级。\n\n**论证与优势**\n\n这种尺度感知裁剪机制提供两个主要优势：\n\n1. **减少更新爆炸**：推导出的界限 $|\\Delta \\theta_{i,t}| \\le \\alpha \\lambda$ 表明，大的异常梯度（尖峰）不会导致任意大的参数更新。通过在更新步骤使用梯度之前对其进行裁剪，该算法可以防止此类事件破坏训练过程的稳定性，并将参数推向损失景观中的糟糕区域。\n\n2. **保留自适应缩放**：裁剪不是使用单一的全局阈值执行的。相反，阈值 $C_{i,t}$ 是自适应的且特定于参数的，因为它依赖于 $v_{i,t-1}$。对于天生具有大梯度的参数，$v_{i,t-1}$ 将会很大，从而导致一个更宽松的裁剪阈值。相反，对于梯度较小的参数，阈值将更紧。这保留了 RMSProp 的根本优势，即根据其历史梯度尺度，在逐参数的基础上调整有效学习率。此外，问题指定二阶矩累积量 $v_{i,t}$ 使用*未修改*的梯度 $g_{i,t}$ 进行更新。这是一个关键的设计选择。它确保 $v_{i,t}$ 始终是梯度真实二阶矩（包括任何尖峰）的准确、无偏估计量。如果 $v_{i,t}$ 使用裁剪后的梯度 $\\tilde{g}_{i,t}$ 进行更新，它将系统地低估梯度方差，从而随着时间的推移损害自适应缩放本身的完整性。\n\n### 任务 B：实现与测试\n\n我们现在进入实现和测试阶段。将在一个合成的凸二次目标函数上模拟两个优化器：一个基准 RMSProp 和我们的尺度感知裁剪变体。它们的性能将根据指定的更新爆炸减少和自适应优势保持的谓词，在三个测试用例中进行评估。最终的程序将封装整个模拟和评估过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the complete simulation and print the final result.\n    It defines test cases and calls helper functions to perform the optimization\n    and evaluation logic.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Spike on the highest-curvature coordinate.\n        (3, [1.0, 0.1, 10.0], [1.0, 1.0, 1.0], 60, 0.05, 0.9, 1e-3, 2.0, {(10, 2): 200.0, (20, 2): -150.0}, 50.0, 0.5, 1e-12),\n        # Case 2: No spikes, testing behavior from initial state.\n        (3, [1.0, 0.3, 3.0], [1.5, -0.5, 0.75], 100, 0.05, 0.9, 1e-3, 5.0, {}, 1.0, 0.05, 1e-12),\n        # Case 3: Multiple spikes on different coordinates.\n        (4, [1.0, 0.2, 5.0, 0.05], [1.0, 1.0, 1.0, 1.0], 120, 0.04, 0.9, 1e-3, 1.5, {(5, 3): 300.0, (6, 3): -300.0, (30, 0): 150.0}, 30.0, 0.5, 1e-12),\n    ]\n\n    def _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, mode):\n        \"\"\"\n        Runs a single optimization trajectory for either baseline or clipped RMSProp.\n        \n        Returns:\n            - max_abs_step (float): The maximum absolute step magnitude over the trajectory.\n            - final_loss (float): The final value of the objective function.\n        \"\"\"\n        # Use np.float64 for higher numerical precision and stability.\n        theta = np.array(theta0, dtype=np.float64)\n        c_arr = np.array(c, dtype=np.float64)\n        v = np.zeros(d, dtype=np.float64)\n        max_abs_step = 0.0\n\n        for t_step in range(1, T + 1):\n            # Calculate the base gradient from the convex quadratic objective\n            grad_base = c_arr * theta\n            \n            # Construct the sparse spike vector for the current step\n            spikes = np.zeros(d, dtype=np.float64)\n            for i in range(d):\n                if (t_step, i) in spikes_dict:\n                    spikes[i] = spikes_dict.get((t_step, i), 0.0)\n            \n            # The observed gradient is the sum of the base gradient and the spike\n            grad_observed = grad_base + spikes\n\n            # Denominator for the RMSProp update\n            denominator = np.sqrt(v) + epsilon\n\n            if mode == 'baseline':\n                # Standard RMSProp update uses the observed gradient\n                step = -alpha * grad_observed / denominator\n            elif mode == 'clipped':\n                # Scale-aware clipping: threshold is proportional to the scale estimate\n                clip_threshold = lam * denominator\n                grad_clipped = np.clip(grad_observed, -clip_threshold, clip_threshold)\n                # The step uses the clipped gradient\n                step = -alpha * grad_clipped / denominator\n            else:\n                raise ValueError(\"Invalid optimizer mode specified.\")\n\n            # Track the maximum absolute per-parameter step magnitude\n            current_max_abs_step = np.max(np.abs(step))\n            if current_max_abs_step > max_abs_step:\n                max_abs_step = current_max_abs_step\n            \n            # Update parameters\n            theta += step\n            \n            # Update the second-moment accumulator using the unclipped gradient\n            v = beta * v + (1 - beta) * np.square(grad_observed)\n\n        # Calculate the final objective value\n        final_loss = 0.5 * np.sum(c_arr * np.square(theta))\n        \n        return max_abs_step, final_loss\n\n    def _run_simulation(case_params):\n        \"\"\"\n        Manages a single test case, running both optimizers and evaluating predicates.\n        \n        Returns:\n            - 1 if both predicates (E_j and P_j) are true, 0 otherwise.\n        \"\"\"\n        d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, tau, delta, tol = case_params\n\n        # Run both baseline and clipped optimizers\n        M_base, f_final_base = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'baseline')\n        M_clip, f_final_clip = _run_optimizer(d, c, theta0, T, alpha, beta, epsilon, lam, spikes_dict, 'clipped')\n\n        # Evaluate Predicate E_j: Exploding-update reduction\n        # Checks if the baseline step exploded while the clipped step remained bounded as derived.\n        e_j = (M_base > tau) and (M_clip = alpha * lam + tol)\n        \n        # Evaluate Predicate P_j: Adaptive-benefit preservation\n        # Checks if the clipped version's final loss is not substantially worse than the baseline.\n        p_j = f_final_clip = (1 + delta) * f_final_base\n\n        return 1 if e_j and p_j else 0\n\n    results = []\n    for case in test_cases:\n        result = _run_simulation(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3097005"}]}