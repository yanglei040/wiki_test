{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本章节的第一个动手实践将引导您从最基本的原理出发，亲手实现阶梯衰减和指数衰减这两种核心的学习率策略。通过将它们应用于一个简单的一维优化问题，您将直观地比较它们的动态行为和最终性能，为理解更复杂的场景打下坚实的基础。[@problem_id:3176459]", "problem": "要求您实现并分析离散时间梯度下降在一维严格凸二次损失函数上的学习率调度方案。重点是对比指数衰减（由半衰期参数化）和步进衰减（由一组下降时间参数化），并评估它们在多个数据集上性能的一致性。您的程序必须是一个单一、完整、可运行的程序，无需任何用户输入即可产生指定的输出。\n\n从以下基本原理开始：\n\n- 对于带有标量参数的可微函数，离散时间梯度下降的更新规则如下：对于形式为 $f(w) = \\tfrac{1}{2} a (w - b)^{2}$（其中 $a > 0$）的二次损失，其梯度为 $\\nabla f(w) = a (w - b)$。在第 $t$ 步使用时变学习率 $\\eta_{t}$ 时，更新规则为\n$$\nw_{t+1} = w_{t} - \\eta_{t} \\, a \\, (w_{t} - b).\n$$\n- 定义误差 $e_{t} = w_{t} - b$。则其动态满足\n$$\ne_{t+1} = \\left(1 - a \\, \\eta_{t}\\right) e_{t}.\n$$\n这些关系是唯一的出发点；您使用的任何其他关系都必须从中推导出来。\n\n需要实现的调度方案：\n\n- 由半衰期参数化的指数衰减调度。半衰期为 $h$ 步意味着在第 $h$ 步的学习率恰好是其初始值的一半。设 $\\eta_{0}$ 表示初始学习率，$T$ 表示总更新步数。您必须确定衰减率，以使半衰期条件对于给定的训练长度分数 $f$ 成立，其中 $h = f \\, T$。\n\n- 步进衰减调度，在预定的里程碑步骤上按因子 $1/2$ 进行乘法下降。对于一组里程碑分数 $\\{f_{1}, f_{2}, \\ldots, f_{k}\\}$，下降步骤是整数里程碑 $\\{\\lfloor f_{1} T \\rfloor, \\lfloor f_{2} T \\rfloor, \\ldots, \\lfloor f_{k} T \\rfloor\\}$。在每个这样的步骤 $t$，应用该步骤的更新之前，学习率会乘以 $1/2$。如果多个分数映射到同一个整数步骤，则通过最多使用每个里程碑一次来合并重复项。\n\n必须精确比较六个调度方案，索引如下：\n\n- 索引 $0$：指数衰减，半衰期分数为 $0.05$。\n- 索引 $1$：指数衰减，半衰期分数为 $0.10$。\n- 索引 $2$：指数衰减，半衰期分数为 $0.20$。\n- 索引 $3$：步进衰减，在分数 $\\{0.05\\}$ 处下降。\n- 索引 $4$：步进衰减，在分数 $\\{0.05, 0.10\\}$ 处下降。\n- 索引 $5$：步进衰减，在分数 $\\{0.05, 0.10, 0.20\\}$ 处下降。\n\n对于每个测试用例中的每个调度方案和每个数据集，计算 $T$ 步后的确切最终损失，\n$$\nL = \\tfrac{1}{2} a \\, e_{T}^{2},\n$$\n其中 $e_{T}$ 由更新动态和所选的调度方案确定。不要模拟带噪声的梯度；使用更新所隐含的精确二次动态。\n\n测试用例中跨数据集的评估：\n\n- 对于每个调度索引 $s \\in \\{0,1,2,3,4,5\\}$，计算该测试用例中各数据集最终损失的均值；将此均值表示为 $M_{s}$。\n- 设 $s^{\\star}$ 是实现 $\\min_{s} M_{s}$ 的最小索引。\n- 对于每个数据集，单独找到在该数据集上产生最小最终损失的最小调度索引；计算有多少个数据集选择了 $s^{\\star}$。将此计数表示为 $C$。\n- 为该测试用例报告三元组 $[s^{\\star}, C, M_{s^{\\star}}]$。\n\n约定和精确定义：\n\n- 总步数 $T$ 是一个正整数，步骤由 $t \\in \\{0,1,\\ldots,T-1\\}$ 索引。\n- 对于指数衰减，第 $t$ 步的学习率必须在 $t=0$ 时满足 $\\eta_{t} = \\eta_{0}$，并在 $t=h$ 时对于指定的分数 $f$ 满足半衰期条件。您必须从第一性原理推导出相应的衰减率。\n- 对于步进衰减，在里程碑步骤 $t_{\\mathrm{milestone}}$，用于该步骤的学习率在应用该步骤的更新之前，通过乘法下降进行更新。\n- 对于每个数据集，在同一测试用例中对所有调度方案使用相同的初始参数 $w_{0}$。\n- 对于任何对数推导，请使用自然对数。\n- 此任务不涉及任何物理单位。\n\n测试套件：\n\n实现您的程序以运行以下三个测试用例。每个测试用例包含一个初始学习率 $\\eta_{0}$、一个总步数 $T$、一个共享初始参数 $w_{0}$，以及一个数据集列表，其中每个数据集是一对 $(a,b)$，定义了 $f(w) = \\tfrac{1}{2} a (w-b)^{2}$。\n\n- 测试用例 1：\n  - $\\eta_{0} = 0.15$, $T = 100$, $w_{0} = 5.0$。\n  - 数据集：$(a,b) \\in \\{(1.0, 2.0), (3.0, -1.0), (10.0, 0.5)\\}$。\n\n- 测试用例 2：\n  - $\\eta_{0} = 0.12$, $T = 120$, $w_{0} = -3.0$。\n  - 数据集：$(a,b) \\in \\{(0.8, 0.0), (2.5, 1.0), (6.0, -2.0)\\}$。\n\n- 测试用例 3：\n  - $\\eta_{0} = 0.18$, $T = 60$, $w_{0} = 2.0$。\n  - 数据集：$(a,b) \\in \\{(1.5, 1.0), (4.0, -2.0), (9.0, 3.0)\\}$。\n\n最终输出格式：\n\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序为每个测试用例包含一个元素，其中每个元素是上面定义的列表 $[s^{\\star}, C, M_{s^{\\star}}]$。例如，一个语法上有效的输出行看起来像\n$[ [0,3,0.00123], [2,2,0.00456], [5,1,0.00789] ]$\n但其中的实际数值由您的实现确定。", "solution": "该问题要求分析两种学习率衰减方案——指数衰减和步进衰减——在一维二次损失函数上的表现。分析的核心在于误差项 $e_t = w_t - b$ 的离散时间动态，它表示参数 $w_t$ 与最优值 $b$ 之间的偏差。\n\n第 $t+1$ 步误差的控制方程源自梯度下降更新规则：\n$$\nw_{t+1} = w_t - \\eta_t \\nabla f(w_t)\n$$\n对于指定的二次损失函数 $f(w) = \\frac{1}{2} a (w - b)^2$，其梯度为 $\\nabla f(w) = a(w-b)$。代入此梯度和误差 $e_t$ 的定义可得：\n$$\nw_{t+1} - b = (w_t - b) - \\eta_t a (w_t - b)\n$$\n$$\ne_{t+1} = e_t - \\eta_t a e_t = (1 - a \\eta_t) e_t\n$$\n这个递推关系是模拟优化过程的基础。经过 $T$ 步后，最终误差 $e_T$ 可以表示为初始误差 $e_0 = w_0 - b$ 与每一步的缩减因子的乘积：\n$$\ne_T = e_0 \\prod_{t=0}^{T-1} (1 - a \\eta_t)\n$$\n然后，最终损失计算为 $L = \\frac{1}{2} a e_T^2$。我们的分析涉及为不同的学习率调度方案 $\\eta_t$ 计算这个最终损失。\n\n两种类型的学习率调度方案定义如下：\n\n1.  **指数衰减调度**：第 $t$ 步的学习率由等比数列 $\\eta_t = \\eta_0 \\gamma^t$ 给出，其中 $\\eta_0$ 是初始学习率，$\\gamma \\in (0, 1)$ 是衰减率。问题规定衰减率 $\\gamma$ 必须由半衰期条件确定：在第 $h = fT$ 步的学习率是其初始值的一半，其中 $f$ 是给定的分数，$T$ 是总步数。我们从此条件推导 $\\gamma$：\n    $$\n    \\eta_h = \\eta_0 \\gamma^h = \\frac{1}{2} \\eta_0\n    $$\n    $$\n    \\gamma^h = \\frac{1}{2}\n    $$\n    对两边取自然对数：\n    $$\n    h \\ln(\\gamma) = \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2)\n    $$\n    求解 $\\gamma$：\n    $$\n    \\ln(\\gamma) = -\\frac{\\ln(2)}{h} \\implies \\gamma = \\exp\\left(-\\frac{\\ln(2)}{h}\\right)\n    $$\n    代入 $h=fT$，衰减率为 $\\gamma = \\exp\\left(-\\frac{\\ln(2)}{fT}\\right)$，第 $t$ 步的学习率为：\n    $$\n    \\eta_t = \\eta_0 \\exp\\left(-\\frac{t \\ln(2)}{fT}\\right)\n    $$\n\n2.  **步进衰减调度**：学习率保持恒定，然后在预定义的里程碑步骤上进行乘法衰减。给定一组里程碑分数 $\\{f_1, f_2, \\ldots, f_k\\}$，相应的整数里程碑步骤为 $T_{\\text{miles}} = \\{\\lfloor f_1 T \\rfloor, \\lfloor f_2 T \\rfloor, \\ldots, \\lfloor f_k T \\rfloor\\}$，并移除重复项。学习率从 $\\eta_0$ 开始。在每个时间步 $t \\in T_{\\text{miles}}$，学习率在用于该步骤的梯度下降更新*之前*乘以 $1/2$ 进行更新。设 $\\eta'_t$ 为第 $t$ 步可能发生下降前的学习率。那么 $\\eta'_0 = \\eta_0$，且对于 $t0$，有 $\\eta'_{t} = \\eta_{t-1}$。用于第 $t$ 步更新的学习率 $\\eta_t$ 为：\n    $$\n    \\eta_t = \\begin{cases} \\frac{1}{2} \\eta'_t  \\text{if } t \\in T_{\\text{miles}} \\\\ \\eta'_t  \\text{otherwise} \\end{cases}\n    $$\n\n模拟过程如下：首先初始化误差 $e_0 = w_0 - b$，然后对于 $t \\in \\{0, 1, \\ldots, T-1\\}$，使用六个指定调度方案中相应的 $\\eta_t$ 来迭代应用误差更新 $e_{t+1} = (1 - a \\eta_t) e_t$。\n\n在为每个调度方案和每个数据集计算出最终损失 $L$ 后，对每个测试用例进行评估。\n-   对于每个调度索引 $s \\in \\{0, \\dots, 5\\}$，计算该测试用例中所有数据集的平均最终损失 $M_s$。\n-   平均表现最佳的调度方案 $s^\\star$ 被确定为实现最小平均损失的最小索引：$s^\\star = \\arg\\min_s M_s$。\n-   为了衡量这个最佳调度方案的一致性，我们统计有多少个独立的数据集也认为 $s^\\star$ 是它们的最优调度方案。这个计数表示为 $C$。\n-   该测试用例的最终结果以三元组 $[s^\\star, C, M_{s^\\star}]$ 的形式报告。\n\n这个结构化的流程允许基于所提供的动态和参数，对学习率调度方案进行严谨且可复现的比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        {\n            \"eta0\": 0.15, \"T\": 100, \"w0\": 5.0,\n            \"datasets\": [(1.0, 2.0), (3.0, -1.0), (10.0, 0.5)]\n        },\n        {\n            \"eta0\": 0.12, \"T\": 120, \"w0\": -3.0,\n            \"datasets\": [(0.8, 0.0), (2.5, 1.0), (6.0, -2.0)]\n        },\n        {\n            \"eta0\": 0.18, \"T\": 60, \"w0\": 2.0,\n            \"datasets\": [(1.5, 1.0), (4.0, -2.0), (9.0, 3.0)]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = run_simulation(case[\"eta0\"], case[\"T\"], case[\"w0\"], case[\"datasets\"])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [ [r1], [r2], ... ] from the example is illustrative of structure.\n    # The code template provides the authoritative format generation rule.\n    # str([1, 2, 3]) - '[1, 2, 3]' which includes spaces.\n    # ','.join(['[1,2]', '[3,4]']) - '[1,2],[3,4]' which has no space between elements.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef run_simulation(eta0, T, w0, datasets):\n    \"\"\"\n    Runs the simulation and evaluation for a single test case.\n    \"\"\"\n    # Schedule definitions\n    exp_decay_fractions = [0.05, 0.10, 0.20]\n    step_decay_fractions_sets = [[0.05], [0.05, 0.10], [0.05, 0.10, 0.20]]\n    num_schedules = len(exp_decay_fractions) + len(step_decay_fractions_sets)\n    \n    all_schedules_losses = [[] for _ in range(num_schedules)]\n\n    # Loop through each of the 6 schedules\n    for s_idx in range(num_schedules):\n        # Loop through each dataset for the current schedule\n        for a, b in datasets:\n            e = w0 - b\n            \n            # Run gradient descent for T steps\n            if s_idx  3: # Exponential decay schedules\n                f = exp_decay_fractions[s_idx]\n                h = f * T\n                # Handle potential h=0, though not possible with given data\n                gamma = np.exp(-np.log(2.0) / h) if h > 0 else 1.0\n                \n                for t in range(T):\n                    eta_t = eta0 * (gamma ** t)\n                    e = e * (1.0 - a * eta_t)\n            \n            else: # Step decay schedules\n                fracs = step_decay_fractions_sets[s_idx - 3]\n                milestones = {int(f * T) for f in fracs}\n                current_eta = eta0\n                \n                for t in range(T):\n                    if t in milestones:\n                        current_eta /= 2.0\n                    eta_t = current_eta\n                    e = e * (1.0 - a * eta_t)\n            \n            # Calculate final loss\n            final_loss = 0.5 * a * e**2\n            all_schedules_losses[s_idx].append(final_loss)\n            \n    # Evaluation\n    mean_losses = [np.mean(losses) for losses in all_schedules_losses]\n    \n    s_star = int(np.argmin(mean_losses))\n    m_s_star = mean_losses[s_star]\n    \n    # Count how many datasets selected s_star as their best schedule\n    c = 0\n    losses_by_dataset = np.array(all_schedules_losses).T\n    for dataset_losses in losses_by_dataset:\n        best_s_for_dataset = int(np.argmin(dataset_losses))\n        if best_s_for_dataset == s_star:\n            c += 1\n            \n    return [s_star, c, m_s_star]\n\n# Execute the main function\nsolve()\n```", "id": "3176459"}, {"introduction": "学习率策略并非孤立存在，它们如何与像 Adam 这样复杂的现代优化器相互作用？这个练习将带您探索学习率突变所带来的实际影响，并引入“更新震荡”与“方向失配”等关键诊断概念。您将通过模拟发现，阶梯衰减的突变可能导致优化器的内部状态（如动量）与新的学习率不匹配，这是调试和优化真实世界模型时一个至关重要的洞察。[@problem_id:3176478]", "problem": "要求您在一个受控的、纯算法的环境中，探究在发生突变时，学习率调度与自适应矩估计 (Adam) 算法内部状态之间的相互作用。您的任务是编写一个完整且可运行的程序，该程序在不同的学习率调度和梯度机制下，使用 Adam 模拟一维优化，量化调度变更后立即出现的状态失配，并比较突变的阶梯衰减与平滑的指数衰减。\n\n本探究基于以下基础且广为接受的定义。对于整数时间 $t \\in \\{1,2,\\ldots,T\\}$，标量梯度为 $g_t \\in \\mathbb{R}$，Adam 维护移动平均值 $m_t$ 和 $v_t$，超参数为 $\\beta_1 \\in (0,1)$，$\\beta_2 \\in (0,1)$ 和 $\\epsilon > 0$：\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t,\\quad\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2,\n$$\n偏差校正后的估计值为\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{p_t}},\\quad\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{q_t}},\n$$\n其中 $p_t$ 和 $q_t$ 记录从初始化或从偏差校正计数器重置以来的更新次数。在时间 $t$ 的 Adam 参数更新为\n$$\n\\Delta \\theta_t = - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n$$\n其中 $\\eta_t > 0$ 是学习率。考虑两种学习率调度：\n1. 阶梯衰减，阶跃时间为 $t_s$：\n$$\n\\eta_t = \\begin{cases}\n\\eta_0,  t  t_s,\\\\\n\\gamma \\eta_0,  t \\ge t_s,\n\\end{cases}\n$$\n其中 $\\eta_0 > 0$ 且 $\\gamma \\in (0,1)$。\n2. 选择在 $t=t_s$ 时与阶梯衰减的值相匹配的指数衰减：\n$$\n\\eta_t = \\eta_0 \\exp(-k (t-1)),\\quad \\text{其中 } k = \\frac{\\ln(1/\\gamma)}{t_s-1}.\n$$\n施加一个分段常数梯度机制来模拟分布偏移：\n$$\ng_t = \\begin{cases}\ng_\\text{pre},  t  t_s,\\\\\ng_\\text{post},  t \\ge t_s.\n\\end{cases}\n$$\n\n在调度变更时间 $t=t_s$ 定义两个定量诊断指标：\n- 更新冲击幅度\n$$\nJ = \\left| \\Delta \\theta_{t_s} - \\Delta \\theta_{t_s-1} \\right|,\n$$\n它衡量更新变化的突兀程度；较大的 $J$ 表示更强的冲击。\n- 方向失准指示器\n$$\nB = \\begin{cases}\n\\text{True},  \\Delta \\theta_{t_s} \\cdot g_{t_s} > 0,\\\\\n\\text{False},  \\Delta \\theta_{t_s} \\cdot g_{t_s} \\le 0,\n\\end{cases}\n$$\n当更新方向导致损失在一维上局部增加（即违反了当前梯度的下降方向）时，该值为 True；否则为 False。在一维中，与下降方向对齐的更新应满足 $\\Delta \\theta_{t_s} \\cdot g_{t_s}  0$。\n\n为了减轻学习率阶跃变化后立即出现的失配，实现一种偏差校正调整，在 $t=t_s$ 时重置内部状态：\n- 矩重置：在计算 $m_{t_s}$ 和 $v_{t_s}$ 之前，设置 $m_{t_s-1} \\leftarrow 0$，$v_{t_s-1} \\leftarrow 0$。\n- 偏差校正计数器重置：重置用于偏差校正的幂，使得 $p_{t_s}=1$ 且 $q_{t_s}=1$。\n\n这种重置强制 $t_s$ 时的偏差校正矩无滞后地反映当前梯度。\n\n实现一个程序，对于以下每个测试用例，模拟 $T$ 个步骤，计算如上定义的数对 $(J,B)$，并将所有结果作为扁平列表打印在单行中。使用以下超参数和常量\n- $T = 60$，\n- $t_s = 30$，\n- $\\eta_0 = 10^{-3}$，\n- $\\gamma = 10^{-1}$，\n- $\\beta_1 = 9 \\times 10^{-1}$，\n- $\\beta_2 = 9.99 \\times 10^{-1}$，\n- $\\epsilon = 10^{-8}$。\n\n测试套件（每个用例返回 $t=t_s$ 时的 $(J,B)$）：\n1. 阶梯衰减，无重置，在 $t_s$ 时梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = -1$。\n2. 阶梯衰减，在 $t_s$ 时重置，在 $t_s$ 时梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = -1$。\n3. 阶梯衰减，无重置，无梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = 10^{-1}$。\n4. 指数衰减，无重置，在 $t_s$ 时梯度符号翻转：$g_\\text{pre} = 10^{-1}$，$g_\\text{post} = -1$。\n\n您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序如下\n$$\n[J_1, B_1, J_2, B_2, J_3, B_3, J_4, B_4].\n$$\n不涉及物理单位。所有角度（如有）都应被视为无量纲实数。最终输出必须是使用该语言默认字符串格式的精确计算值，其中允许布尔值，并以其语言原生字面量形式出现。", "solution": "该问题陈述经过仔细验证，并被确定为有效。它在科学上基于数值优化的既定原则，特别是 Adam 算法，并且是适定的，所有必要的常量、初始条件和函数形式都已指定。定义在数学上是精确和客观的，从而能够得到唯一且可验证的解。\n\n任务是在时间范围 $T$ 内，在不同的学习率调度和梯度配置下，模拟一维 Adam 优化器。分析的核心集中在优化器在特定时间点 $t_s$ 的行为，此时学习率和梯度都可能发生突变。我们将使用两个定义的度量 $J$ 和 $B$ 来量化由此对优化过程产生的冲击。\n\nAdam 算法维护过去梯度的指数衰减移动平均（一阶矩，$m_t$）和过去梯度平方的指数衰减移动平均（二阶矩，$v_t$）。在每个时间步 $t$，对于给定的梯度 $g_t$，这些矩按如下方式更新，从 $m_0 = 0$ 和 $v_0 = 0$ 开始：\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n$$\n$$\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n$$\n此处，$\\beta_1$ 和 $\\beta_2$ 是区间 $(0, 1)$ 内的超参数，用于控制移动平均的衰减率。由于矩被初始化为零，它们会偏向于零，尤其是在初始步骤中。通过计算以下各项来校正这种偏差：\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^{p_t}}\n$$\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^{q_t}}\n$$\n指数 $p_t$ 和 $q_t$ 表示对矩有贡献的更新次数，通常对应于当前时间步 $t$。然后使用当前学习率 $\\eta_t$、偏差校正后的矩和一个小的稳定常数 $\\epsilon > 0$ 来计算最终的参数更新 $\\Delta \\theta_t$：\n$$\n\\Delta \\theta_t = - \\eta_t \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n$$\n模拟将执行 $T=60$ 步，在第 $t_s=30$ 步发生一个关键事件。梯度遵循分段常数机制：\n$$\ng_t = \\begin{cases}\ng_\\text{pre},  t  t_s \\\\\ng_\\text{post},  t \\ge t_s\n\\end{cases}\n$$\n我们研究两种学习率调度 $\\eta_t$：\n1.  **阶梯衰减**：学习率在 $\\eta_0$ 处保持不变，在时间 $t_s$ 时降至其初始值的 $\\gamma$ 倍。\n    $$\n    \\eta_t = \\begin{cases}\n    \\eta_0,  t  t_s \\\\\n    \\gamma \\eta_0,  t \\ge t_s\n    \\end{cases}\n    $$\n2.  **指数衰减**：学习率随时间平滑衰减。选择衰减率 $k$，使得在步骤 $t_s$ 时的学习率与阶梯衰减调度的学习率相匹配。\n    $$\n    \\eta_t = \\eta_0 \\exp(-k (t-1)), \\quad \\text{with } k = \\frac{\\ln(1/\\gamma)}{t_s-1}\n    $$\n在关键步骤 $t_s$，我们测量两个诊断指标：\n-   **更新冲击幅度 ($J$)**：这衡量了从 $t_s-1$ 到 $t_s$ 更新步长的变化幅度。大的 $J$ 表示优化轨迹受到了显著的颠簸。\n    $$\n    J = \\left| \\Delta \\theta_{t_s} - \\Delta \\theta_{t_s-1} \\right|\n    $$\n-   **方向失准指示器 ($B$)**：如果 $t_s$ 时的更新指向损失*上升*而非下降的方向，则此布尔指示器为 $\\text{True}$。标准的梯度下降步长沿着 $-\\alpha g_t$ 方向移动，因此更新与梯度的乘积应为负。$B$ 标记了违反此条件的情况。\n    $$\n    B = (\\Delta \\theta_{t_s} \\cdot g_{t_s} > 0)\n    $$\n一个测试用例探讨了状态失配的缓解策略。在 $t=t_s$ 时，优化器的状态 ($m_{t_s-1}$, $v_{t_s-1}$) 反映了梯度 $g_\\text{pre}$ 的历史，这可能与新的梯度 $g_\\text{post}$ 不一致。**重置机制**通过在计算新矩 $m_{t_s}$ 和 $v_{t_s}$ 之前将 $m_{t_s-1}$ 和 $v_{t_s-1}$ 设置为 0 来解决此问题。同时，偏差校正计数器被重置，因此 $p_{t_s}=1$ 且 $q_{t_s}=1$。这有效地重新初始化了优化器，使其能够快速适应新的梯度机制。\n\n模拟将针对四个指定的测试用例中的每一个进行，使用提供的超参数：$\\eta_0 = 10^{-3}$，$\\gamma = 10^{-1}$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$ 和 $\\epsilon = 10^{-8}$。对于每个用例，将在 $t = t_s = 30$ 时计算并报告数对 $(J, B)$。\n\n实现将包含一个从 $t=1$ 到 $T=60$ 迭代的主循环。在循环内部，根据当前步骤 $t$ 和正在模拟的用例，确定适当的梯度 $g_t$ 和学习率 $\\eta_t$。如果指定，在 $t=t_s$ 时应用重置机制的逻辑，更改该步骤的矩输入和偏差校正幂。然后顺序应用 Adam 更新方程来计算 $\\Delta\\theta_t$，并将其存储起来。最后，在模拟之后，使用存储在 $t=29$ 和 $t=30$ 的更新来计算诊断指标 $J$ 和 $B$。", "answer": "```python\nimport numpy as np\n\ndef simulate_adam(\n    lr_schedule_type,\n    reset,\n    g_pre,\n    g_post,\n    T,\n    t_s,\n    eta_0,\n    gamma,\n    beta1,\n    beta2,\n    epsilon,\n):\n    \"\"\"\n    Simulates one-dimensional Adam optimization for a given scenario.\n\n    Args:\n        lr_schedule_type (str): 'step' or 'exp'.\n        reset (bool): Whether to apply the reset mechanism at t_s.\n        g_pre (float): Gradient value for t  t_s.\n        g_post (float): Gradient value for t >= t_s.\n        T (int): Total number of simulation steps.\n        t_s (int): The step at which changes occur.\n        eta_0 (float): Initial learning rate.\n        gamma (float): Decay factor for step decay.\n        beta1 (float): Exponential decay rate for the 1st moment estimates.\n        beta2 (float): Exponential decay rate for the 2nd moment estimates.\n        epsilon (float): Term added to the denominator for numerical stability.\n\n    Returns:\n        tuple: A pair (J, B) containing the update shock amplitude and the\n               directional misalignment indicator.\n    \"\"\"\n    m = 0.0\n    v = 0.0\n    delta_thetas = []\n\n    # Calculate k for exponential decay if needed\n    k = 0.0\n    if lr_schedule_type == \"exp\":\n        k = np.log(1 / gamma) / (t_s - 1)\n\n    for t in range(1, T + 1):\n        # 1. Determine current gradient\n        g_t = g_pre if t  t_s else g_post\n\n        # 2. Determine current learning rate\n        if lr_schedule_type == \"step\":\n            eta_t = eta_0 if t  t_s else gamma * eta_0\n        else:  # 'exp'\n            eta_t = eta_0 * np.exp(-k * (t - 1))\n\n        # 3. Handle reset mechanism and bias-correction powers\n        m_prev_eff = m\n        v_prev_eff = v\n\n        if reset and t >= t_s:\n            p_t = t - t_s + 1\n            q_t = t - t_s + 1\n            if t == t_s:\n                m_prev_eff = 0.0\n                v_prev_eff = 0.0\n        else:\n            p_t = t\n            q_t = t\n\n        # 4. Adam update equations\n        # Update biased first moment estimate\n        m = beta1 * m_prev_eff + (1 - beta1) * g_t\n        # Update biased second raw moment estimate\n        v = beta2 * v_prev_eff + (1 - beta2) * (g_t**2)\n\n        # Compute bias-corrected first moment estimate\n        m_hat = m / (1 - beta1**p_t)\n        # Compute bias-corrected second raw moment estimate\n        v_hat = v / (1 - beta2**q_t)\n\n        # Compute parameter update\n        delta_theta_t = -eta_t * m_hat / (np.sqrt(v_hat) + epsilon)\n        delta_thetas.append(delta_theta_t)\n\n    # 5. Compute diagnostics at t=t_s\n    delta_theta_ts_minus_1 = delta_thetas[t_s - 2]  # t_s-1 corresponds to index t_s-2\n    delta_theta_ts = delta_thetas[t_s - 1]          # t_s corresponds to index t_s-1\n    g_ts = g_post\n\n    # Update shock amplitude\n    J = np.abs(delta_theta_ts - delta_theta_ts_minus_1)\n\n    # Directional misalignment indicator\n    B = (delta_theta_ts * g_ts) > 0\n\n    return J, B\n\n\ndef solve():\n    \"\"\"\n    Runs the simulation for all test cases and prints the results.\n    \"\"\"\n    # Hyperparameters and constants\n    T = 60\n    t_s = 30\n    eta_0 = 1e-3\n    gamma = 1e-1\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    # Test suite\n    test_cases = [\n        # 1. Step decay, no reset, gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": -1.0},\n        # 2. Step decay, with reset, gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": True, \"g_pre\": 1e-1, \"g_post\": -1.0},\n        # 3. Step decay, no reset, no gradient sign flip\n        {\"lr_schedule\": \"step\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": 1e-1},\n        # 4. Exponential decay, no reset, gradient sign flip\n        {\"lr_schedule\": \"exp\", \"reset\": False, \"g_pre\": 1e-1, \"g_post\": -1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        J, B = simulate_adam(\n            lr_schedule_type=case[\"lr_schedule\"],\n            reset=case[\"reset\"],\n            g_pre=case[\"g_pre\"],\n            g_post=case[\"g_post\"],\n            T=T,\n            t_s=t_s,\n            eta_0=eta_0,\n            gamma=gamma,\n            beta1=beta1,\n            beta2=beta2,\n            epsilon=epsilon,\n        )\n        results.extend([J, B])\n\n    # Format the final output string\n    # Booleans are correctly converted to 'True'/'False' by str()\n    # Floats use default Python formatting\n    formatted_results = [str(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3176478"}, {"introduction": "在深度学习中，由于随机初始化、数据抽样和梯度噪声，训练过程本质上是随机的。本章的最后一个实践将探讨这种随机性如何与不同的学习率衰减策略相互作用。通过在多个随机种子下进行模拟训练，您将亲手验证一个重要假设：与平滑的指数衰减相比，阶梯衰减的突变特性是否会放大随机性的影响，从而降低训练结果的可复现性。[@problem_id:3176492]", "problem": "考虑深度学习中迭代参数更新的训练动态，其中单个标量参数 $w$ 被优化以最小化一个以目标 $w^\\star$ 为中心的凸目标函数。作为一个基本模型，我们假设采用标准的梯度下降更新，其学习率随时间变化，并且每次迭代都带有附加的、零均值的、独立的梯度噪声。具体来说，迭代值遵循以下规则：对于每个离散时间步 $t \\in \\{0,1,\\dots,T-1\\}$，应用形式为 $w_{t+1} = w_t - \\eta_t \\cdot g_t$ 的更新，其中 $g_t$ 是梯度加上噪声扰动，而 $\\eta_t$ 是在时间 $t$ 的学习率。目标函数是一个以 $w^\\star$ 为中心的可微二次函数，并且噪声在不同的迭代步和不同的随机种子之间是独立的。\n\n你的任务是实现两种在深度学习中被广泛研究的学习率策略：\n- 一种在固定的迭代阈值处以离散方式降低学习率的策略。\n- 一种在每次迭代中平滑地降低学习率的策略。\n\n你将比较这两种策略在不同随机种子间的可复现性。可复现性定义为最终训练结果对随机种子选择的敏感程度。对于这个问题，敏感度将量化为最终目标函数值在不同随机种子间的经验方差。待检验的假设是，在存在噪声的情况下，由于阈值效应，采用离散阈值下降的策略会在不同种子间引发更高的敏感度。\n\n使用以下建模假设：\n- 参数是标量，其初始值 $w_0$ 从一个依赖于种子的分布中抽取。\n- 目标值固定为 $w^\\star = 1$。\n- 目标函数是二次函数 $f(w) = \\tfrac{1}{2} (w - w^\\star)^2$，因此在迭代 $t$ 时的无噪声梯度是 $w_t - w^\\star$。\n- 在迭代 $t$ 时的随机梯度是 $g_t = (w_t - w^\\star) + \\epsilon_t$，其中 $\\epsilon_t$ 在不同的 $t$ 和种子之间是独立的，且 $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n- 更新规则为 $w_{t+1} = w_t - \\eta_t \\cdot g_t$，其中 $t \\in \\{0,1,\\dots,T-1\\}$。\n\n对于下面测试套件中的每个测试用例，你必须：\n1. 使用相同的随机种子和噪声参数，分别模拟离散阈值策略和平滑策略下的训练过程。\n2. 对于每种策略，计算最终目标函数值 $f(w_T)$ 在不同种子间的经验方差。\n3. 对该测试用例，生成一个布尔结果，表明离散阈值策略的方差是否严格大于平滑策略的方差。\n\n测试套件和参数定义：\n- 设随机种子集合为 $\\{0,1,\\dots,S-1\\}$，其中 $S$ 为给定值。\n- 对于每个测试用例，你会得到一组参数 $(T,S,\\eta_0,\\gamma,s,\\sigma)$，其中：\n  - $T$ 是总迭代次数。\n  - $S$ 是种子数量。\n  - $\\eta_0$ 是初始学习率。\n  - $\\gamma$ 是一个介于 $0$ 和 $1$ 之间的衰减因子，控制学习率缩减的速度。\n  - $s$ 是阈值下降的步长间隔。\n  - $\\sigma$ 是梯度噪声的标准差。\n- 你必须构建离散阈值策略，使其在由 $s$ 和 $\\gamma$ 决定的迭代阈值处进行衰减；同时构建平滑策略，使其在每次迭代中连续降低学习率，并使用 $s$ 和 $\\gamma$ 以确保每 $s$ 次迭代的总体衰减具有可比性。\n\n使用以下测试用例：\n- 用例 $1$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,40,0.1)$。\n- 用例 $2$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,40,0)$。\n- 用例 $3$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.05,0.5,40,0.1)$。\n- 用例 $4$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.2,40,0.2)$。\n- 用例 $5$: $(T,S,\\eta_0,\\gamma,s,\\sigma) = (240,200,0.4,0.5,10,0.2)$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result_1,result_2,\\dots]$），其中每个 $result_i$ 是一个对应于用例 $i$ 的布尔值。如果离散阈值策略产生的 $f(w_T)$ 的经验方差严格大于平滑策略，则该值为 $True$，否则为 $False$。不涉及物理单位，也无需报告角度或百分比。对于给定的测试套件，输出必须是确定性的。", "solution": "该问题要求在随机梯度下降的背景下，对两种学习率策略——离散步长衰减和平滑指数衰减——的稳定性进行比较分析。比较的度量标准是最终目标函数值在多次模拟中的经验方差，每次模拟都使用不同的随机种子进行初始化。更高的方差表明对随机性的具体实现（初始参数和梯度噪声）具有更高的敏感度，这意味着可复现性较低。\n\n首先，我们对动力系统进行形式化。待优化的参数是一个标量 $w$，其在每个时间步 $t \\in \\{0, 1, \\dots, T-1\\}$ 的更新规则如下：\n$$\nw_{t+1} = w_t - \\eta_t \\cdot g_t\n$$\n其中 $\\eta_t$ 是在步骤 $t$ 的学习率，$g_t$ 是随机梯度。目标函数是一个简单的二次碗型函数，$f(w) = \\frac{1}{2}(w - w^\\star)^2$，目标参数固定为 $w^\\star = 1$。真实梯度为 $\\nabla f(w_t) = w_t - w^\\star$。随机梯度 $g_t$ 包含一个附加噪声项 $\\epsilon_t$：\n$$\ng_t = (w_t - w^\\star) + \\epsilon_t\n$$\n噪声项 $\\epsilon_t$ 被建模为一个从零均值正态分布 $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ 中抽取的独立同分布（i.i.d.）随机变量。\n\n为了分析该系统的动态，我们考虑误差项 $\\delta_t = w_t - w^\\star$。将其代入更新规则，得到一个关于误差的线性递推关系：\n$$\n\\delta_{t+1} = \\delta_t - \\eta_t (\\delta_t + \\epsilon_t) = (1 - \\eta_t)\\delta_t - \\eta_t \\epsilon_t\n$$\n该方程描述了参数与最优值之间偏差的演化过程，该过程由学习率策略 $\\eta_t$ 和噪声序列 $\\{\\epsilon_t\\}_{t=0}^{T-1}$ 驱动。\n\n模拟的随机性源于两个方面，两者都依赖于选定的种子 $k \\in \\{0, 1, \\dots, S-1\\}$：\n1.  初始参数 $w_0^{(k)}$，由此确定初始误差 $\\delta_0^{(k)} = w_0^{(k)} - w^\\star$。我们将从标准正态分布 $w_0 \\sim \\mathcal{N}(0, 1)$ 中抽取 $w_0^{(k)}$。\n2.  梯度噪声项序列 $\\{\\epsilon_t^{(k)}\\}_{t=0}^{T-1}$。\n\n对于每个种子，必须使用相同的初始条件 $w_0^{(k)}$ 和噪声序列 $\\{\\epsilon_t^{(k)}\\}$ 来模拟两种学习率策略的轨迹。这确保了任何观察到的结果差异都完全归因于策略本身。\n\n接下来，我们根据初始学习率 $\\eta_0$、衰减因子 $\\gamma \\in (0,1)$ 和步长间隔 $s$ 来定义这两种学习率策略。\n\n1.  **离散阈值策略 ($\\eta_t^{\\text{disc}}$):** 学习率在 $s$ 步内保持不变，然后乘以 $\\gamma$ 进行缩减。这表示为：\n    $$\n    \\eta_t^{\\text{disc}} = \\eta_0 \\cdot \\gamma^{\\lfloor t/s \\rfloor}\n    $$\n    指数 $\\lfloor t/s \\rfloor$ 是一个阶梯函数，每 $s$ 次迭代增加一次，导致学习率在 $s$ 的倍数处发生突降。\n\n2.  **平滑策略 ($\\eta_t^{\\text{smth}}$):** 为了提供可比较的衰减，该策略被形式化为连续指数衰减。每步的衰减率 $\\gamma_{\\text{smth}}$ 的选择使得在 $s$ 步内的总衰减等于 $\\gamma$。这意味着 $(\\gamma_{\\text{smth}})^s = \\gamma$，因此 $\\gamma_{\\text{smth}} = \\gamma^{1/s}$。在步骤 $t$ 的学习率为：\n    $$\n    \\eta_t^{\\text{smth}} = \\eta_0 \\cdot (\\gamma^{1/s})^t = \\eta_0 \\cdot \\gamma^{t/s}\n    $$\n    该策略在每一次迭代中都将学习率减小一个很小的因子。\n\n问题的核心是检验假设 $\\text{Var}_{\\text{seeds}}(f(w_T^{\\text{disc}})) > \\text{Var}_{\\text{seeds}}(f(w_T^{\\text{smth}}))$，其中 $f(w_T)=\\frac{1}{2}\\delta_T^2$ 是最终目标值。\n\n对于每个测试用例 $(T, S, \\eta_0, \\gamma, s, \\sigma)$，模拟算法如下：\n1.  初始化两个大小为 $S$ 的数组，$V_{\\text{disc}}$ 和 $V_{\\text{smth}}$，用于存储最终目标值。\n2.  对于从 $0$ 到 $S-1$ 的每个种子 $k$：\n    a.  实例化一个以 $k$ 为种子的随机数生成器。\n    b.  生成一个共同的初始参数 $w_0$ 和一个长度为 $T$ 的共同噪声向量 $(\\epsilon_0, \\dots, \\epsilon_{T-1})$。\n    c.  模拟离散策略的轨迹：从 $w = w_0$ 开始，对 $t=0, \\dots, T-1$ 迭代更新规则 $w_{t+1} = w_t - \\eta_t^{\\text{disc}} g_t$。计算最终目标值 $f(w_T)$ 并将其存储在 $V_{\\text{disc}}[k]$ 中。\n    d.  模拟平滑策略的轨迹：从相同的 $w_0$ 开始，对 $t=0, \\dots, T-1$ 迭代更新规则 $w_{t+1} = w_t - \\eta_t^{\\text{smth}} g_t$。计算最终目标值 $f(w_T)$ 并将其存储在 $V_{\\text{smth}}[k]$ 中。\n3.  计算 $V_{\\text{disc}}$ 和 $V_{\\text{smth}}$ 中元素的经验方差。\n4.  如果 $V_{\\text{disc}}$ 的方差严格大于 $V_{\\text{smth}}$ 的方差，则结果为 `True`，否则为 `False`。对所有提供的测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(T, S, eta0, gamma, s, sigma):\n    \"\"\"\n    Simulates training for discrete and smooth LR schedules and compares their final variance.\n\n    Args:\n        T (int): Total number of iterations.\n        S (int): Number of random seeds to simulate.\n        eta0 (float): Initial learning rate.\n        gamma (float): Decay factor.\n        s (int): Step interval for discrete decay.\n        sigma (float): Standard deviation of gradient noise.\n\n    Returns:\n        bool: True if the variance of the discrete schedule's final objective\n              is strictly greater than the smooth schedule's, False otherwise.\n    \"\"\"\n    w_star = 1.0\n\n    final_objs_disc = np.zeros(S)\n    final_objs_smth = np.zeros(S)\n\n    for seed in range(S):\n        # Use a seed-specific random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate the same initial weight and noise sequence for both schedules.\n        w0 = rng.normal(loc=0.0, scale=1.0)\n        noise = rng.normal(loc=0.0, scale=sigma, size=T)\n\n        # --- Discrete-Threshold Schedule Simulation ---\n        w = w0\n        for t in range(T):\n            lr_disc = eta0 * (gamma ** (t // s))\n            gradient = (w - w_star) + noise[t]\n            w = w - lr_disc * gradient\n        final_objs_disc[seed] = 0.5 * (w - w_star)**2\n\n        # --- Smooth Schedule Simulation ---\n        w = w0  # Reset to the same initial weight\n        for t in range(T):\n            lr_smth = eta0 * (gamma ** (t / s))\n            gradient = (w - w_star) + noise[t]\n            w = w - lr_smth * gradient\n        final_objs_smth[seed] = 0.5 * (w - w_star)**2\n\n    # Compute the empirical variance across all seeds for each schedule.\n    # np.var computes the population variance by default (ddof=0).\n    var_disc = np.var(final_objs_disc)\n    var_smth = np.var(final_objs_smth)\n\n    return var_disc > var_smth\n\ndef solve():\n    \"\"\"\n    Solves the problem by running simulations for all specified test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (T, S, eta0, gamma, s, sigma)\n    test_cases = [\n        (240, 200, 0.4, 0.5, 40, 0.1),\n        (240, 200, 0.4, 0.5, 40, 0.0),\n        (240, 200, 0.05, 0.5, 40, 0.1),\n        (240, 200, 0.4, 0.2, 40, 0.2),\n        (240, 200, 0.4, 0.5, 10, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(*case)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3176492"}]}