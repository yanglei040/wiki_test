{"hands_on_practices": [{"introduction": "理解前向模式自动微分的最佳方式之一是亲手实现它。这个练习将指导你使用一种被称为“对偶数”（$\\text{Dual Number}$）的优雅数学结构，它自然地将一个数的值和其导数封装在一起。通过对基本算术运算符（如加法和乘法）进行重载，你将能够构建一个系统，该系统可以自动计算任何由这些运算构成的复杂函数的导数[@problem_id:3207038]。", "problem": "要求您在一种高级语言中使用运算符重载来实现前向模式自动微分（forward mode AD）。其核心思想是通过基本运算传播导数，从而追踪表达式相对于单个标量输入的值和导数。您将创建一个类，该类表示一个由实数值及其相对于某个自变量的导数组成的数对。然后，您必须使用该类在一组输入点上评估一个特定多项式及其导数的值。该实现不得依赖任何预构建的自动微分工具或符号计算库。\n\n实现一个类，其实例表示带有导数信息的数字。该类必须支持标准算术运算，以便在计算中使用该类时，可以自动计算出值和导数。实现必须支持加法、减法、乘法、除法和整数幂运算。与内置数值类型的交互在运算符的任意一侧都应表现自然。\n\n使用该类评估多项式函数\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11\n$$\n及其在几个输入点上的导数。对于每个输入 $x$，您的程序应使用前向模式自动微分计算函数值 $f(x)$ 和导数 $f'(x)$，同时使用普通浮点运算和标准微积分方法计算这两个量以进行验证。对于每个输入 $x$，计算绝对误差\n$e_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert,$\n其中 $f_{\\text{AD}}(x)$ 和 $f'_{\\text{AD}}(x)$ 分别表示由您的自动微分评估所返回的值和导数。\n\n不涉及角度单位。不涉及物理单位。\n\n测试套件：\n- 在以下有序输入列表上进行评估：$[0.0, 1.0, -1.0, 2.5, 10.0]$。这些输入涵盖了边界输入 $0.0$、简单整数输入 $1.0$ 和 $-1.0$、非整数输入 $2.5$ 以及一个数量级较大的输入 $10.0$。\n\n程序要求：\n- 使用算术运算符唯一定义多项式 $f(x)$；无论 $x$ 是内置实数还是您的类的实例，该单一定义必须以相同的方式工作。\n- 对于指定顺序中的每个测试输入 $x$，创建一个导数为 $1.0$ 的实例来表示自变量，评估多项式以获得 $f_{\\text{AD}}(x)$ 和 $f'_{\\text{AD}}(x)$，使用普通浮点算术评估同一多项式以获得 $f(x)$，使用标准微积分评估解析导数 $f'(x)$，并计算如上定义的 $e_{\\text{val}}$ 和 $e_{\\text{der}}$。\n- 按与输入相同的顺序收集结果。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素必须是对应输入的双元素列表 $[e_{\\text{val}}, e_{\\text{der}}]$。例如，打印的行应类似于\n\"[ [e_val_for_x0, e_der_for_x0], [e_val_for_x1, e_der_for_x1], ... ]\"\n除了您的打印函数产生的空格外，没有其他空格要求，但它必须是单行文本，以自然语言的列表语法编码一个列表的列表。\n\n您的程序必须是自包含的，并且不得从用户或任何外部文件读取任何输入。所提供测试套件的结果必须是打印输出行的唯一内容。", "solution": "我们使用成熟的微分法则，为单个标量输入推导前向模式自动微分（automatic differentiation (AD)）。其基础包括标准的微分法则：线性性质、乘法法则、除法法则、链式法则以及整数次幂的幂法则。\n\n我们通过为每个中间量关联一个数对 $\\left(v, d\\right)$ 来表示对自变量 $x$ 的计算，其中 $v$ 是实数值， $d = \\frac{dv}{dx}$ 是关于 $x$ 的导数。现在我们直接从微积分推导如何通过算术运算传播这些数对。\n\n加法和减法：\n假设 $u = (u, u')$ 和 $v = (v, v')$ 代表两个关于 $x$ 的可微量。那么\n$$\n(u + v, \\, \\frac{d}{dx}(u+v)) = (u + v, \\, u' + v'),\n$$\n减法也类似，\n$$\n(u - v, \\, \\frac{d}{dx}(u-v)) = (u - v, \\, u' - v').\n$$\n这些可由微分的线性性质得出。\n\n乘法：\n使用乘法法则，\n$$\n\\frac{d}{dx}(uv) = u'v + uv'.\n$$\n因此，对于数对 $(u,u')$ 和 $(v,v')$，其乘积变为\n$$\n(uv, \\, u'v + uv').\n$$\n\n除法：\n使用除法法则，\n$$\n\\frac{d}{dx}\\left(\\frac{u}{v}\\right) = \\frac{u'v - uv'}{v^{2}},\n$$\n所以传播的数对是\n$$\n\\left(\\frac{u}{v}, \\, \\frac{u'v - uv'}{v^{2}}\\right),\n$$\n前提是 $v \\neq 0$。\n\n整数次幂：\n对于整数 $n$，根据幂法则和链式法则，\n$$\n\\frac{d}{dx}\\left(u^{n}\\right) = n u^{n-1} u'.\n$$\n因此传播的数对是\n$$\n\\left(u^{n}, \\, n u^{n-1} u'\\right).\n$$\n对于一个实数常数指数 $p$，当 $u > 0$ 时，同样的表达式 $\\left(u^{p}, \\, p u^{p-1} u'\\right)$ 成立，但对于本任务而言，正确支持整数指数就足够了，因为目标函数是一个多项式。\n\n前向模式 AD 实现：\n我们将每个量编码为一个类实例，存储两个实数 $(v,d)$。该类重载了算术运算符，以便当它参与表达式运算时，会应用上述传播公式。与内置实数的交互通过将实数 $c$ 视为数对 $(c, 0)$ 来处理。自变量 $x$ 表示为 $(x,1)$。\n\n目标函数：\n我们考虑多项式\n$$\nf(x) = 3x^{5} - 2x^{3} + 7x - 11.\n$$\n因为 $f$ 是一个多项式，当它通过重载算术运算一次性写出时，它会自动计算出值和导数。为了验证，我们使用从标准微积分中获得的其解析导数：\n$$\nf'(x) = 15x^{4} - 6x^{2} + 7.\n$$\n\n算法步骤：\n1. 定义一个类，其实例存储 $(v,d)$，并实现上面推导出的重载运算符 $+$、$-$、$\\times$、$\\div$ 和整数次幂运算。\n2. 使用算术运算符唯一定义 $f(x)$。这个单一定义可以应用于内置实数或该类的实例。\n3. 对于每个测试输入 $x \\in \\{0.0, 1.0, -1.0, 2.5, 10.0\\}$，创建种子变量 $(x,1)$，评估 $f$ 以获得 $(f_{\\text{AD}}(x), f'_{\\text{AD}}(x))$，将 $f(x)$ 和 $f'(x)$ 作为普通浮点运算进行评估，并计算\n$$\ne_{\\text{val}} = \\lvert f_{\\text{AD}}(x) - f(x) \\rvert, \\quad e_{\\text{der}} = \\lvert f'_{\\text{AD}}(x) - f'(x) \\rvert.\n$$\n4. 按指定顺序为每个 $x$ 收集数对 $[e_{\\text{val}}, e_{\\text{der}}]$，并将其作为单个列表打印在一行上。\n\n正确性：\n传播法则是直接从微分的线性性质、乘法法则、除法法则和幂法则推导出来的。由于多项式是通过重复应用这些基本运算构成的，因此在浮点舍入误差范围内，前向模式 AD 评估在任何点 $x$ 处都能得出 $f$ 的精确解析导数。因此，在所有测试输入中，绝对误差 $e_{\\text{val}}$ 和 $e_{\\text{der}}$ 预计会非常小，通常在浮点舍入误差范围内接近于零。\n\n输出：\n程序打印单行文本，表示一个由双元素列表 $[e_{\\text{val}}, e_{\\text{der}}]$ 组成的列表，每个测试用例一个，顺序与输入顺序相同。不产生其他输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Dual:\n    \"\"\"\n    Dual number for forward-mode automatic differentiation with respect to a single scalar variable.\n    Each instance represents a pair (value, derivative).\n    \"\"\"\n    __slots__ = (\"val\", \"der\")\n\n    def __init__(self, val, der=0.0):\n        self.val = float(val)\n        self.der = float(der)\n\n    @staticmethod\n    def _coerce(other):\n        if isinstance(other, Dual):\n            return other\n        else:\n            return Dual(other, 0.0)\n\n    # Addition\n    def __add__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val + o.val, self.der + o.der)\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    # Subtraction\n    def __sub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(self.val - o.val, self.der - o.der)\n\n    def __rsub__(self, other):\n        o = Dual._coerce(other)\n        return Dual(o.val - self.val, o.der - self.der)\n\n    # Multiplication\n    def __mul__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') * (v, v') = (uv, u'v + uv')\n        return Dual(self.val * o.val, self.der * o.val + self.val * o.der)\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    # True division\n    def __truediv__(self, other):\n        o = Dual._coerce(other)\n        # (u, u') / (v, v') = (u/v, (u'v - uv')/v^2)\n        denom = o.val * o.val\n        return Dual(self.val / o.val, (self.der * o.val - self.val * o.der) / denom)\n\n    def __rtruediv__(self, other):\n        o = Dual._coerce(other)\n        # o / self\n        denom = self.val * self.val\n        return Dual(o.val / self.val, (o.der * self.val - o.val * self.der) / denom)\n\n    # Unary negation\n    def __neg__(self):\n        return Dual(-self.val, -self.der)\n\n    # Power: support real (int/float) exponents, commonly used for integer exponents in polynomials\n    def __pow__(self, power):\n        if isinstance(power, (int, float)):\n            if power == 0:\n                # x**0 = 1, derivative 0\n                return Dual(1.0, 0.0)\n            # For real power p: d(x**p) = p * x**(p - 1) * dx\n            primal = self.val ** power\n            deriv = power * (self.val ** (power - 1.0)) * self.der\n            return Dual(primal, deriv)\n        else:\n            raise TypeError(\"Power must be a real number for Dual.__pow__\")\n\ndef poly(x):\n    # f(x) = 3x^5 - 2x^3 + 7x - 11\n    return 3 * (x ** 5) - 2 * (x ** 3) + 7 * x - 11\n\ndef poly_float(x):\n    return 3.0 * (x ** 5) - 2.0 * (x ** 3) + 7.0 * x - 11.0\n\ndef poly_derivative_float(x):\n    # f'(x) = 15x^4 - 6x^2 + 7\n    return 15.0 * (x ** 4) - 6.0 * (x ** 2) + 7.0\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [0.0, 1.0, -1.0, 2.5, 10.0]\n\n    results = []\n    for x in test_cases:\n        # Seed the independent variable: derivative w.r.t. x is 1\n        dx = Dual(x, 1.0)\n        y = poly(dx)              # Dual result: (value, derivative)\n        f_val = poly_float(x)     # Float value\n        f_der = poly_derivative_float(x)  # Analytical derivative\n\n        val_err = abs(y.val - f_val)\n        der_err = abs(y.der - f_der)\n\n        results.append([val_err, der_err])\n\n    # Final print statement in the exact required format: a single line list of [val_err, der_err] pairs.\n    # Format with reasonable precision for readability.\n    formatted = \"[\" + \",\".join(f\"[{val:.12g},{der:.12g}]\" for val, der in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3207038"}, {"introduction": "虽然前向模式很直观，但当函数有大量输入和单个输出时（例如深度学习中的损失函数），反向模式的效率要高得多。本练习将带你深入反向模式的核心机制，即所谓的“回传”或矢量-雅可比积（$\\text{Vector-Jacobian Product, VJP}$）拉回。你将手动构建一个计算图（也称为“磁带”），并逐步追踪梯度从输出回传到输入的全过程，从而揭开现代AD框架中反向传播算法的神秘面紗[@problem_id:3100431]。", "problem": "考虑在深度学习背景下的反向模式自动微分（AD），其中通过使用链式法则反向遍历计算图来高效地计算标量损失函数关于参数的梯度。我们感兴趣的函数是标量映射 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$，由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y\\neq 0$。仅使用与计算图兼容的基本运算（乘法、正弦、指数和除法），构建一个用于评估 $f(x,y)$ 的最小中间变量集合，以及一个记录这些运算的父子关系的磁带。然后，利用复合函数的链式法则原理和向量-雅可比积（VJP）的概念，手动推导获得梯度 $\\nabla f(x,y)$ 所需的反向传播（VJP 拉回）的确切序列。你的推导应清楚地指明反向遍历磁带的顺序，以及在每一步中对输入的伴随变量的局部贡献。以行向量的形式给出 $\\nabla f(x,y)$ 的最终解析表达式。不要四舍五入；最终答案必须是精确的符号表达式。", "solution": "问题陈述被认定为有效。它具有科学依据，问题定义良好，客观，并包含足够的信息来推导出唯一且有意义的解。该任务涉及将反向模式自动微分（AD）——计算微积分和深度学习中的一个基石算法——应用于一个可微函数。该过程是可形式化的，并与既定原则相符。\n\n我们的任务是使用反向模式AD的原理，计算函数 $f:\\mathbb{R}^{2}\\to\\mathbbR$（由 $f(x,y)=\\sin(xy)+\\frac{\\exp(x)}{y}$ 给出，其中 $y \\neq 0$）的梯度 $\\nabla f(x,y)$。这包括一个构建计算图并评估函数的正向过程，以及一个传播梯度的反向过程。\n\n首先，我们将函数分解为一系列基本运算。这个序列定义了计算图，或称为“磁带”。设输入为 $v_1 = x$ 和 $v_2 = y$。\n\n**正向过程：构建计算图**\n\n$f(x,y)$ 的评估可以由以下最小中间变量集合表示：\n1.  $v_3 = v_1 \\cdot v_2 = x \\cdot y$\n2.  $v_4 = \\sin(v_3) = \\sin(xy)$\n3.  $v_5 = \\exp(v_1) = \\exp(x)$\n4.  $v_6 = \\frac{v_5}{v_2} = \\frac{\\exp(x)}{y}$\n5.  $v_7 = v_4 + v_6 = \\sin(xy) + \\frac{\\exp(x)}{y} = f(x,y)$\n\n这个序列构成了正向过程。磁带记录了这些运算及其依赖关系：$(v_3, \\text{mul}, v_1, v_2)$、$(v_4, \\sin, v_3)$、$(v_5, \\exp, v_1)$、$(v_6, \\text{div}, v_5, v_2)$、$(v_7, \\text{add}, v_4, v_6)$。\n\n**反向过程：使用链式法则计算梯度**\n\n反向过程计算最终输出 $v_7$ 相对于每个中间变量 $v_i$ 的偏导数，这些偏导数表示为伴随变量 $\\bar{v}_i = \\frac{\\partial f}{\\partial v_i} = \\frac{\\partial v_7}{\\partial v_i}$。该过程首先将输出节点的伴随变量初始化为 $1$，即 $\\bar{v}_7 = \\frac{\\partial v_7}{\\partial v_7} = 1$。所有其他伴随变量初始化为 $0$。然后我们以反向拓扑顺序遍历计算图。\n\n核心原理是链式法则。对于一个运算 $v_k = g(v_i, v_j, \\dots)$，其父节点的伴随变量通过累加子节点的伴随变量与局部偏导数的乘积来更新：\n$$ \\bar{v}_i = \\bar{v}_i + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_i} $$\n$$ \\bar{v}_j = \\bar{v}_j + \\bar{v}_k \\frac{\\partial v_k}{\\partial v_j} $$\n......以此类推。此操作实际上是一个向量-雅可比积（VJP）拉回。\n\n让我们按照正向过程的逆序计算伴随变量：\n\n1.  **开始：** 初始化伴随变量：$\\bar{v}_1=0, \\bar{v}_2=0, \\bar{v}_3=0, \\bar{v}_4=0, \\bar{v}_5=0, \\bar{v}_6=0$。\n    设置反向过程的种子：$\\bar{v}_7 = 1$。\n\n2.  **节点 $v_7 = v_4 + v_6$：**\n    父节点是 $v_4$ 和 $v_6$。\n    局部偏导数：$\\frac{\\partial v_7}{\\partial v_4} = 1$，$\\frac{\\partial v_7}{\\partial v_6} = 1$。\n    更新父节点的伴随变量：\n    $\\bar{v}_4 \\leftarrow \\bar{v}_4 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_4} = 0 + 1 \\cdot 1 = 1$。\n    $\\bar{v}_6 \\leftarrow \\bar{v}_6 + \\bar{v}_7 \\cdot \\frac{\\partial v_7}{\\partial v_6} = 0 + 1 \\cdot 1 = 1$。\n    当前状态：$\\bar{v}_4=1, \\bar{v}_6=1$。\n\n3.  **节点 $v_6 = \\frac{v_5}{v_2}$：**\n    父节点是 $v_5$ 和 $v_2$。\n    局部偏导数：$\\frac{\\partial v_6}{\\partial v_5} = \\frac{1}{v_2}$，$\\frac{\\partial v_6}{\\partial v_2} = -\\frac{v_5}{v_2^2}$。\n    更新父节点的伴随变量：\n    $\\bar{v}_5 \\leftarrow \\bar{v}_5 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_5} = 0 + 1 \\cdot \\frac{1}{v_2} = \\frac{1}{y}$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_6 \\cdot \\frac{\\partial v_6}{\\partial v_2} = 0 + 1 \\cdot \\left(-\\frac{v_5}{v_2^2}\\right) = -\\frac{\\exp(x)}{y^2}$。\n    当前状态：$\\bar{v}_5 = \\frac{1}{y}$，$\\bar{v}_2=-\\frac{\\exp(x)}{y^2}$。\n\n4.  **节点 $v_5 = \\exp(v_1)$：**\n    父节点是 $v_1$。\n    局部偏导数：$\\frac{\\partial v_5}{\\partial v_1} = \\exp(v_1)$。\n    更新父节点的伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_5 \\cdot \\frac{\\partial v_5}{\\partial v_1} = 0 + \\frac{1}{y} \\cdot \\exp(v_1) = \\frac{\\exp(x)}{y}$。\n    当前状态：$\\bar{v}_1 = \\frac{\\exp(x)}{y}$。\n\n5.  **节点 $v_4 = \\sin(v_3)$：**\n    父节点是 $v_3$。\n    局部偏导数：$\\frac{\\partial v_4}{\\partial v_3} = \\cos(v_3)$。\n    更新父节点的伴随变量：\n    $\\bar{v}_3 \\leftarrow \\bar{v}_3 + \\bar{v}_4 \\cdot \\frac{\\partial v_4}{\\partial v_3} = 0 + 1 \\cdot \\cos(v_3) = \\cos(xy)$。\n    当前状态：$\\bar{v}_3 = \\cos(xy)$。\n\n6.  **节点 $v_3 = v_1 \\cdot v_2$：**\n    父节点是 $v_1$ 和 $v_2$。注意 $v_1$ 和 $v_2$ 已经从其他路径接收了梯度；我们累加新的贡献。\n    局部偏导数：$\\frac{\\partial v_3}{\\partial v_1} = v_2$，$\\frac{\\partial v_3}{\\partial v_2} = v_1$。\n    更新父节点的伴随变量：\n    $\\bar{v}_1 \\leftarrow \\bar{v}_1 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_1} = \\frac{\\exp(x)}{y} + \\cos(xy) \\cdot v_2 = \\frac{\\exp(x)}{y} + y \\cos(xy)$。\n    $\\bar{v}_2 \\leftarrow \\bar{v}_2 + \\bar{v}_3 \\cdot \\frac{\\partial v_3}{\\partial v_2} = -\\frac{\\exp(x)}{y^2} + \\cos(xy) \\cdot v_1 = -\\frac{\\exp(x)}{y^2} + x \\cos(xy)$。\n\n当我们计算完所有输入节点的伴随变量后，该过程终止。\n最终的梯度是输入变量伴随变量的最终值：\n$$ \\frac{\\partial f}{\\partial x} = \\bar{v}_1 = y \\cos(xy) + \\frac{\\exp(x)}{y} $$\n$$ \\frac{\\partial f}{\\partial y} = \\bar{v}_2 = x \\cos(xy) - \\frac{\\exp(x)}{y^2} $$\n\n梯度向量 $\\nabla f(x,y)$ 是这些偏导数组成的行向量：\n$$ \\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} y \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2} \\end{pmatrix} $$\n此推导严格遵循了反向模式自动微分的机械步骤。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\ny \\cos(xy) + \\frac{\\exp(x)}{y} & x \\cos(xy) - \\frac{\\exp(x)}{y^2}\n\\end{pmatrix}\n}\n$$", "id": "3100431"}, {"introduction": "掌握了前向和反向模式的基本原理后，我们将探讨一个更微妙但至关重要的主题：数值稳定性。即使微分法则在数学上是完美的，其在有限精度浮点数下的实现也可能导致严重的精度损失。这个练习以倒数函数 $y = 1/x$ 为例，特别是在 $x$ 接近零时，通过比较“朴素”实现和“稳定”实现的梯度精度，揭示了AD规则实现方式对结果准确性的深远影响[@problem_id:3207130]。", "problem": "使用前向模式和反向模式，为标量倒数函数实现一个小型自动微分系统。仅从以下基本原理出发：将导数定义为由链式法则给出的线性映射、使用满足 $\\varepsilon^2 = 0$ 的无穷小量 $\\varepsilon$ 的前向模式对偶数模型，以及反向模式的伴随（也称敏感度）传播观点，推导倒数节点的数值稳定更新规则，并在一系列输入（包括接近零的值）上测量梯度精度。\n\n您的任务是：\n\n1) 为倒数节点推导一个前向模式规则，其中原始值为 $y = 1/x$。使用对偶数定义，即前向模式值表示为 $x + \\dot{x}\\,\\varepsilon$，且其复合遵循链式法则。以此为基础，推导出切线量 $\\dot{y}$ 的更新公式，该公式应使用原始输出而不是重新计算输入的幂。这将产生一个重用已计算的 $y$ 的表达式，从而形成一个数值稳定的更新。\n\n2) 为倒数节点推导一个反向模式规则。在反向模式中，定义 $y$ 的伴随（敏感度）$\\bar{y}$，并展示如何通过一个依赖于原始值的拉回（pullback）将其传播到输入敏感度 $\\bar{x}$。使用原始输出 $y$ 来表示该更新。\n\n3) 在一个程序中实现四个规则：\n- 前向模式朴素倒数：直接将 $\\dot{y}$ 作为 $x$ 的函数进行传播。\n- 前向模式稳定倒数：尽可能只使用已计算的原始输出来传播 $\\dot{y}$。\n- 反向模式朴素倒数：直接将 $\\bar{x}$ 作为 $x$ 的函数进行传播。\n- 反向模式稳定倒数：尽可能只使用已计算的原始输出来传播 $\\bar{x}$。\n\n4) 验证方法。对于给定集合中的每个 $x$，使用微积分基本定义计算倒数节点的解析导数，并将其与四种自动微分实现所产生的导数进行比较。使用相对误差，对每个 $x$ 定义如下\n$$\n\\mathrm{rel\\_err}(x) \\;=\\; \\frac{\\lvert g_{\\mathrm{AD}}(x) - g_{\\mathrm{exact}}(x)\\rvert}{\\lvert g_{\\mathrm{exact}}(x)\\rvert},\n$$\n其中分母为有限且非零。在误差聚合中，忽略任何会产生非有限精确导数或非有限自动微分导数的 $x$。\n\n5) 测试套件。在以下三个输入集上评估并聚合最大相对误差，这些输入集旨在覆盖典型范围、近零输入以及接近双精度浮点数不应溢出的最大有限导数的边界值：\n- 情况 A（一般范围）：$[-123.45,\\,-0.0314159,\\,-10^{-8},\\,10^{-8},\\,0.0314159,\\,123.45]$。\n- 情况 B（近零但有限的导数）：$[1.2345\\times 10^{-120},\\,-4.321\\times 10^{-110},\\,7.89\\times 10^{-130},\\,-9.99\\times 10^{-140},\\,5.0\\times 10^{-150},\\,-8.0\\times 10^{-150}]$。\n- 情况 C（导数接近溢出但仍有限的边界）：$[1.5\\times 10^{-154},\\,-1.5\\times 10^{-154},\\,2.2\\times 10^{-154},\\,-2.2\\times 10^{-154}]$。\n\n对于每种情况，计算四个聚合指标：\n- 前向模式朴素算法的最大相对误差。\n- 前向模式稳定算法的最大相对误差。\n- 反向模式朴素算法的最大相对误差。\n- 反向模式稳定算法的最大相对误差。\n\n6) 最终输出格式。您的程序应生成单行输出，包含所有十二个结果，按 A、B、C 的顺序排列，每个案例贡献上述四个指标，形式为用方括号括起来的逗号分隔列表。顺序如下：\n$[\\mathrm{A\\_FwdNaive},\\mathrm{A\\_FwdStab},\\mathrm{A\\_RevNaive},\\mathrm{A\\_RevStab},\\mathrm{B\\_FwdNaive},\\mathrm{B\\_FwdStab},\\mathrm{B\\_RevNaive},\\mathrm{B\\_RevStab},\\mathrm{C\\_FwdNaive},\\mathrm{C\\_FwdStab},\\mathrm{C\\_RevNaive},\\mathrm{C\\_RevStab}]$。\n仅作为格式示例，形如 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9,r_{10},r_{11},r_{12}]$ 的列表是可以接受的。您的程序不得读取输入，且必须使用双精度算术计算并打印这十二个值。不涉及物理单位。所有角度（如有）必须以弧度为单位；但是，此任务中不需要三角函数。", "solution": "该问题要求为标量倒数函数 $y = 1/x$ 推导并实现四种不同的自动微分 (AD) 规则。这些规则分为前向模式和反向模式的“朴素”和“稳定”版本。目标是比较它们的数值精度，特别是对于那些挑战双精度浮点运算极限的输入。\n\n### 1. 预备知识：解析导数\n\n我们考虑的函数是倒数函数：\n$$\ny = f(x) = \\frac{1}{x} = x^{-1}\n$$\n其解析导数是我们进行精度验证的基准，通过微分的幂法则得到：\n$$\ng_{\\mathrm{exact}}(x) = \\frac{dy}{dx} = -1 \\cdot x^{-2} = -\\frac{1}{x^2}\n$$\n问题的核心是评估自动微分方法能在多大程度上逼近这个精确导数 $g_{\\mathrm{exact}}(x)$。\n\n### 2. 前向模式自动微分\n\n前向模式自动微分基于在计算图中前向传播导数。我们使用对偶数表示法，将一个数 $v$ 及其关于某个自变量的导数 $\\dot{v}$ 组合成一个单一实体 $v + \\dot{v}\\varepsilon$，并具有属性 $\\varepsilon^2 = 0$。\n\n假设输入由对偶数 $x + \\dot{x}\\varepsilon$ 表示。我们希望找到输出对应的对偶数 $y + \\dot{y}\\varepsilon$。\n$$\ny + \\dot{y}\\varepsilon = f(x + \\dot{x}\\varepsilon) = \\frac{1}{x + \\dot{x}\\varepsilon}\n$$\n为了分离原始 ($y$) 和切线 ($\\dot{y}$) 分量，我们进行类似于一阶泰勒展开的代数操作：\n$$\n\\frac{1}{x + \\dot{x}\\varepsilon} = \\frac{1}{x(1 + \\frac{\\dot{x}}{x}\\varepsilon)} = \\frac{1}{x} \\left(1 + \\frac{\\dot{x}}{x}\\varepsilon\\right)^{-1}\n$$\n使用几何级数展开 $(1+u)^{-1} = 1 - u + u^2 - \\dots$，其中 $u = \\frac{\\dot{x}}{x}\\varepsilon$，并利用 $\\varepsilon^2=0$ 的性质将级数截断到线性项：\n$$\n\\frac{1}{x} \\left(1 - \\frac{\\dot{x}}{x}\\varepsilon + \\mathcal{O}(\\varepsilon^2)\\right) = \\frac{1}{x} - \\frac{\\dot{x}}{x^2}\\varepsilon\n$$\n通过将此结果与 $y + \\dot{y}\\varepsilon$ 的形式进行比较，我们提取出更新规则：\n- 原始值更新：$y = \\frac{1}{x}$\n- 切线量更新：$\\dot{y} = -\\frac{\\dot{x}}{x^2}$\n\n由此，我们推导出所需的两种前向模式规则。为了计算导数 $f'(x)$，我们设置种子切线量 $\\dot{x}=1$。\n\n**前向模式朴素规则：**\n该规则通过直接使用输入原始值 $x$ 来计算切线量。\n$$\n\\dot{y} = -\\frac{1}{x^2}\n$$\n此规则需要计算 $x^2$ 然后执行一次除法。\n\n**前向模式稳定规则：**\n该规则被构造成重用已计算的原始输出 $y = 1/x$。将 $y$ 简单代入切线量更新公式可得：\n$$\n\\dot{y} = -\\frac{\\dot{x}}{x^2} = -\\dot{x} \\left(\\frac{1}{x}\\right)^2 = -\\dot{x} y^2\n$$\n当 $\\dot{x}=1$ 时，规则变为：\n$$\n\\dot{y} = -y^2\n$$\n该变体首先计算 $y=1/x$，然后对结果进行平方，避免了涉及输入 $x$ 的重新计算。\n\n### 3. 反向模式自动微分\n\n反向模式自动微分将敏感度（或伴随）从计算的输出反向传播到其输入。变量 $v$ 的伴随，记作 $\\bar{v}$，定义为最终标量目标函数 $L$ 关于 $v$ 的偏导数，即 $\\bar{v} = \\frac{\\partial L}{\\partial v}$。\n\n对于函数 $y=f(x)$，伴随的链式法则将输入伴随 $\\bar{x}$ 与输出伴随 $\\bar{y}$ 联系起来：\n$$\n\\bar{x} = \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\frac{dy}{dx} = \\bar{y} \\frac{dy}{dx}\n$$\n项 $\\frac{dy}{dx}$ 是节点的局部偏导数。在我们的例子中，$\\frac{dy}{dx} = -1/x^2$。将此代入伴随传播规则得到：\n$$\n\\bar{x} = \\bar{y} \\left(-\\frac{1}{x^2}\\right)\n$$\n为了求函数 $f(x)$ 本身的导数，我们可以将目标函数概念化为函数输出，即 $L=y$。在这种情况下，种子伴随是 $\\bar{y} = \\frac{\\partial L}{\\partial y} = \\frac{\\partial y}{\\partial y} = 1$。得到的输入伴随 $\\bar{x}$ 便是 $\\frac{dL}{dx} = \\frac{dy}{dx}$。\n\n**反向模式朴素规则：**\n更新直接用输入原始值 $x$ 表示。当 $\\bar{y}=1$ 时：\n$$\n\\bar{x} = -\\frac{1}{x^2}\n$$\n这个计算规则与朴素前向模式规则相同。\n\n**反向模式稳定规则：**\n此规则利用了原始值 $y = 1/x$，该值是在任何反向模式自动微分系统中都必须执行的反向传播之前的正向传播过程中计算的。\n$$\n\\bar{x} = \\bar{y} \\left(-\\frac{1}{x^2}\\right) = -\\bar{y} \\left(\\frac{1}{x}\\right)^2 = -\\bar{y} y^2\n$$\n当 $\\bar{y}=1$ 时，规则变为：\n$$\n\\bar{x} = -y^2\n$$\n这个计算规则与稳定前向模式规则相同。区别在于自动微分的概念框架，但执行的浮点运算是相同的。\n\n### 4. 验证与实现\n\n将推导出的四个规则实现为不同的函数。根据解析导数 $g_{\\mathrm{exact}}(x) = -1/x^2$ 评估其数值精度。比较的指标是每个输入值 $x$ 的相对误差：\n$$\n\\mathrm{rel\\_err}(x) = \\frac{\\lvert g_{\\mathrm{AD}}(x) - g_{\\mathrm{exact}}(x)\\rvert}{\\lvert g_{\\mathrm{exact}}(x)\\rvert}\n$$\n测试协议要求对指定的输入集合，为四个规则中的每一个聚合最大相对误差。任何导致自动微分计算的导数或精确导数为非有限值的输入 $x$ 都将从误差计算中排除，分母为零的情况（对于有限非零的 $x$ 不会发生）也同样被排除。实现将使用 `np.double` 来强制执行双精度运算，并使用 `np.isfinite` 进行有效性检查。\n\n“朴素”实现和“稳定”实现之间的区别在于浮点运算的顺序。\n- 朴素：`g = -1.0 / (x * x)`\n- 稳定：`y = 1.0 / x`, `g = -y * y`\n尽管在代数上等价，但由于舍入误差，这两个操作序列可能产生不同的结果，而本实验旨在测量的正是这种差异的大小，这是数值分析中的一个基本概念。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and compares four AD rules for the reciprocal function y = 1/x.\n    The rules are: forward naive, forward stabilized, reverse naive, and reverse stabilized.\n    Their numerical accuracy is tested against the analytical derivative on three sets of inputs.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = {\n        'A': np.array([-123.45, -0.0314159, -1e-8, 1e-8, 0.0314159, 123.45], dtype=np.double),\n        'B': np.array([1.2345e-120, -4.321e-110, 7.89e-130, -9.99e-140, 5.0e-150, -8.0e-150], dtype=np.double),\n        'C': np.array([1.5e-154, -1.5e-154, 2.2e-154, -2.2e-154], dtype=np.double)\n    }\n\n    # 1. Analytical derivative (ground truth)\n    def g_exact(x: np.double) -> np.double:\n        \"\"\"Computes the exact derivative of 1/x, which is -1/x^2.\"\"\"\n        return np.double(-1.0) / (x * x)\n\n    # 3. Implementations of the four AD rules\n    def forward_naive(x: np.double) -> np.double:\n        \"\"\"Forward mode, naive rule: computes derivative from input x.\"\"\"\n        x_dot = np.double(1.0)\n        # Derivative is computed directly as -x_dot / (x*x)\n        y_dot = -x_dot / (x * x)\n        return y_dot\n\n    def forward_stabilized(x: np.double) -> np.double:\n        \"\"\"Forward mode, stabilized rule: reuses primal output y.\"\"\"\n        x_dot = np.double(1.0)\n        # Primal computation\n        y = np.double(1.0) / x\n        # Derivative computed using primal output y: -x_dot * y^2\n        y_dot = -x_dot * y * y\n        return y_dot\n        \n    def reverse_naive(x: np.double) -> np.double:\n        \"\"\"Reverse mode, naive rule: computes adjoint update from input x.\"\"\"\n        y_bar = np.double(1.0)\n        # Adjoint update computed directly as -y_bar / (x*x)\n        x_bar = -y_bar / (x * x)\n        return x_bar\n\n    def reverse_stabilized(x: np.double) -> np.double:\n        \"\"\"Reverse mode, stabilized rule: reuses primal output y.\"\"\"\n        y_bar = np.double(1.0)\n        # Primal computation (from forward pass)\n        y = np.double(1.0) / x\n        # Adjoint update computed using primal output y: -y_bar * y^2\n        x_bar = -y_bar * y * y\n        return x_bar\n\n    # 4. Verification method\n    def rel_err(g_ad: np.double, g_e: np.double) -> np.double:\n        \"\"\"Computes relative error.\"\"\"\n        return np.abs(g_ad - g_e) / np.abs(g_e)\n\n    all_results = []\n    \n    # Process cases in the specified order A, B, C\n    case_order = ['A', 'B', 'C']\n    ad_funcs = [forward_naive, forward_stabilized, reverse_naive, reverse_stabilized]\n\n    for case_name in case_order:\n        inputs = test_cases[case_name]\n        # max_errors for [FwdNaive, FwdStab, RevNaive, RevStab]\n        max_errors = [np.double(0.0), np.double(0.0), np.double(0.0), np.double(0.0)]\n\n        for x_val in inputs:\n            g_e = g_exact(x_val)\n            \n            # Compute derivatives from all 4 methods\n            ad_results = [f(x_val) for f in ad_funcs]\n\n            # Validation step: ignore non-finite results or zero denominator\n            if not np.isfinite(g_e) or g_e == 0.0:\n                continue\n            if not all(np.isfinite(g) for g in ad_results):\n                continue\n            \n            # Calculate and update max relative errors for the current case\n            for i in range(4):\n                error = rel_err(ad_results[i], g_e)\n                if error > max_errors[i]:\n                    max_errors[i] = error\n        \n        all_results.extend(max_errors)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3207130"}]}