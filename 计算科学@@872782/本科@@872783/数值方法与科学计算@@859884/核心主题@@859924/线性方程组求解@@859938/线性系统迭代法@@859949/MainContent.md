## 引言
[线性方程组](@entry_id:148943) $A\mathbf{x} = \mathbf{b}$ 的求解是科学与工程计算的基石。然而，当面临由真实世界问题离散化而产生的数百万维度的[稀疏系统](@entry_id:168473)时，传统的高斯消元等直接方法因其高昂的计算和存储成本而变得不再适用。这就引出了一个核心问题：我们如何才能高效、可靠地求解这些规模庞大的线性系统？答案在于[迭代法](@entry_id:194857)——一种通过逐步逼近来寻找解的强大[范式](@entry_id:161181)。

本文将系统地引导你进入迭代法的世界。在“原理与机制”一章中，我们将从经典的[雅可比法](@entry_id:147508)和[高斯-赛德尔法](@entry_id:145727)出发，建立起迭代思想的核心，并深入探讨其收敛性理论与现代高级方法（如GMRES）。接着，在“应用与[交叉](@entry_id:147634)学科联系”一章中，我们将跨越学科界限，见证这些方法如何在物理仿真、图像处理、[网络科学](@entry_id:139925)乃至经济模型中大放异彩。最后，“动手实践”一章将提供具体练习，让你亲手巩固所学知识。

通过本次学习，你将不仅掌握一系列数值算法，更能理解其背后统一的数学思想，并有能力将其应用于解决复杂的实际问题。让我们首先深入这些方法的内部，探索它们的“原理与机制”。

## 原理与机制

在上一章中，我们介绍了[求解线性系统](@entry_id:146035) $A\mathbf{x} = \mathbf{b}$ 的直接方法，例如高斯消元法或 LU 分解。这些方法通过一系列有限的、精确的代数运算来找到解。然而，在科学与工程计算的许多前沿领域，我们面临的线性系统规模极其庞大，矩阵 $A$ 的维度可能达到数百万甚至更高。更重要的是，这些大型矩阵通常是**稀疏**的，即绝大多数元素为零。在这种情况下，直接方法不仅计算成本高昂，还可能因为“填充”（fill-in，即在分解过程中产生大量非零元素）而导致存储需求变得无法承受。

为了应对这些挑战，我们转向**[迭代法](@entry_id:194857) (iterative methods)**。与直接法不同，迭代法从一个初始猜测解 $\mathbf{x}^{(0)}$ 开始，通过一个递推过程生成一个近似解序列 $\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots$，并希望这个序列能够收敛到真实的解 $\mathbf{x}$。本章将深入探讨这些方法的基本原理、核心机制以及它们的收敛性理论。

### 经典[定常迭代法](@entry_id:144014)

最基础的一类[迭代法](@entry_id:194857)被称为**[定常迭代法](@entry_id:144014) (stationary iterative methods)**，因为它们的迭代规则在每一步都是相同的。这类方法的核心思想源于一个非常直观的代数变换：将矩阵 $A$ 分解为一个易于求逆的矩阵 $M$ 和一个余项 $N$，即 $A = M - N$。于是，原方程 $A\mathbf{x} = \mathbf{b}$ 可以重写为 $M\mathbf{x} = N\mathbf{x} + \mathbf{b}$。这自然地引出一个迭代格式：

$$
M\mathbf{x}^{(k+1)} = N\mathbf{x}^{(k)} + \mathbf{b}
$$

其中 $\mathbf{x}^{(k)}$ 是第 $k$ 次迭代的近似解。如果矩阵 $M$ 的选择得当（例如，为对角矩阵或[三角矩阵](@entry_id:636278)），那么求解 $\mathbf{x}^{(k+1)}$ 的计算将非常高效。[雅可比法](@entry_id:147508)和[高斯-赛德尔法](@entry_id:145727)是这一思想的两个经典实现。

#### [雅可比法](@entry_id:147508)：并行更新的策略

[雅可比](@entry_id:264467) (Jacobi) 方法是构建迭代法最直接的方式之一。其思想是，在[方程组](@entry_id:193238)的第 $i$ 个方程中，将变量 $x_i$ 单独分离出来，用其他变量来表示它。考虑 $A\mathbf{x} = \mathbf{b}$ 的第 $i$ 个方程：

$$
\sum_{j=1}^{n} a_{ij} x_j = b_i
$$

我们可以将包含 $x_i$ 的对角项 $a_{ii}x_i$ 和其余项分开 [@problem_id:2182332]：

$$
a_{ii} x_i + \sum_{j \neq i} a_{ij} x_j = b_i
$$

假设对角元素 $a_{ii}$ 均不为零，我们可以解出 $x_i$：

$$
x_i = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j \right)
$$

这个关系式构成了[雅可比迭代法](@entry_id:270947)的核心。我们将第 $k$ 次迭代的解 $\mathbf{x}^{(k)}$ 的分量代入上式右侧，来计算第 $k+1$ 次迭代的新分量 $x_i^{(k+1)}$。这样，我们得到[雅可比法](@entry_id:147508)的**分量形式更新公式**：

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j \neq i} a_{ij} x_j^{(k)} \right), \quad \text{for } i = 1, 2, \dots, n
$$

从这个公式可以看出，计算 $\mathbf{x}^{(k+1)}$ 的每一个分量 $x_i^{(k+1)}$ 都只依赖于**上一次**迭代的完整解向量 $\mathbf{x}^{(k)}$。这意味着所有分量的更新可以**[并行计算](@entry_id:139241)**，这在现代多核处理器和并行计算架构上是一个显著的优势。

让我们通过一个具体的例子来理解这个过程。考虑线性系统 [@problem_id:2182306]：

$$
\begin{pmatrix} 5  -1  2 \\ 2  8  -1 \\ -1  1  4 \end{pmatrix}
\begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}
=
\begin{pmatrix} 12 \\ -16.5 \\ 7 \end{pmatrix}
$$

根据[雅可比法](@entry_id:147508)的更新规则，我们得到每个分量的迭代公式：

$$
\begin{align*}
x_1^{(k+1)}  = \frac{1}{5} (12 - (-1)x_2^{(k)} - 2x_3^{(k)}) \\
x_2^{(k+1)}  = \frac{1}{8} (-16.5 - 2x_1^{(k)} - (-1)x_3^{(k)}) \\
x_3^{(k+1)}  = \frac{1}{4} (7 - (-1)x_1^{(k)} - 1x_2^{(k)})
\end{align*}
$$

如果我们从一个零向量的初始猜测 $\mathbf{x}^{(0)} = (0, 0, 0)^T$ 开始，第一次迭代 ($k=0$) 的结果是：

$$
\begin{align*}
x_1^{(1)}  = \frac{1}{5} (12) = 2.4 \\
x_2^{(1)}  = \frac{1}{8} (-16.5) = -2.0625 \\
x_3^{(1)}  = \frac{1}{4} (7) = 1.75
\end{align*}
$$

接着，我们用 $\mathbf{x}^{(1)} = (2.4, -2.0625, 1.75)^T$ 来进行第二次迭代 ($k=1$)，计算 $x_2^{(2)}$：

$$
x_2^{(2)} = \frac{1}{8} (-16.5 - 2(2.4) + 1.75) = \frac{1}{8}(-16.5 - 4.8 + 1.75) = \frac{-19.55}{8} \approx -2.444
$$

通过不断重复这个过程，如果方法收敛，解向量 $\mathbf{x}^{(k)}$ 将会越来越接近真实的解。

#### [高斯-赛德尔法](@entry_id:145727)：即时更新的串行策略

观察[雅可比法](@entry_id:147508)的过程，我们会发现一个潜在的低效之处：在计算 $x_i^{(k+1)}$ 时，我们已经算出了 $x_1^{(k+1)}, \dots, x_{i-1}^{(k+1)}$，但我们仍然使用它们在第 $k$ 步的旧值。高斯-赛德尔 (Gauss-Seidel) 方法正是对这一点进行了改进。它的核心思想是：**在计算任何一个新分量时，立即使用所有已更新的最新分量值** [@problem_id:2182322]。

假设我们按 $i = 1, 2, \dots, n$ 的顺序更新分量。在计算 $x_i^{(k+1)}$ 时，我们已经拥有了本次迭代中计算出的 $x_1^{(k+1)}, \dots, x_{i-1}^{(k+1)}$，而对于 $j > i$ 的分量，我们只有上一次迭代的值 $x_j^{(k)}$。因此，[高斯-赛德尔法](@entry_id:145727)的更新公式变为：

$$
x_i^{(k+1)} = \frac{1}{a_{ii}} \left( b_i - \sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \sum_{j=i+1}^{n} a_{ij} x_j^{(k)} \right)
$$

这种串行、即时更新的特性意味着[高斯-赛德尔法](@entry_id:145727)本质上是顺序执行的，无法像[雅可比法](@entry_id:147508)那样直接并行化。然而，由于它总是使用最新的信息，通常情况下，[高斯-赛德尔法](@entry_id:145727)的[收敛速度](@entry_id:636873)会比[雅可比法](@entry_id:147508)更快。

让我们看一个二维的例子 [@problem_id:1369748]。考虑系统：
$$
\begin{align*}
4x_1 - x_2 = 10 \\
-x_1 + 3x_2 = 5
\end{align*}
$$
高斯-赛德尔的迭代公式是：
$$
\begin{align*}
x_1^{(k+1)}  = \frac{1}{4} (10 + x_2^{(k)}) \\
x_2^{(k+1)}  = \frac{1}{3} (5 + x_1^{(k+1)})
\end{align*}
$$
注意在第二个式子中，我们使用了刚刚计算出的 $x_1^{(k+1)}$。从 $\mathbf{x}^{(0)} = (0, 0)^T$ 开始：
**第一次迭代：**
$$
x_1^{(1)} = \frac{1}{4} (10 + 0) = \frac{5}{2}
$$
$$
x_2^{(1)} = \frac{1}{3} \left(5 + \frac{5}{2}\right) = \frac{5}{2}
$$
所以 $\mathbf{x}^{(1)} = (\frac{5}{2}, \frac{5}{2})^T$。

**第二次迭代：**
$$
x_1^{(2)} = \frac{1}{4} \left(10 + \frac{5}{2}\right) = \frac{25}{8}
$$
$$
x_2^{(2)} = \frac{1}{3} \left(5 + \frac{25}{8}\right) = \frac{65}{24}
$$
所以 $\mathbf{x}^{(2)} = (\frac{25}{8}, \frac{65}{24})^T \approx (3.125, 2.708)^T$。

这个过程有一个优美的**几何解释**。在二维平面上，两个线性方程分别代表两条直线，它们的交点就是系统的解。[高斯-赛德尔法](@entry_id:145727)的每一步都可以看作是两次移动：
1.  **更新 $x_1$**：保持 $x_2$ 不变（即 $x_2 = x_2^{(k)}$），沿着水平方向移动当前点 $(x_1^{(k)}, x_2^{(k)})$，直到它落在第一条直线 $4x_1 - x_2 = 10$ 上。
2.  **更新 $x_2$**：保持新的 $x_1$ 不变（即 $x_1 = x_1^{(k+1)}$），沿着垂直方向移动，直到它落在第二条直线 $-x_1 + 3x_2 = 5$ 上。
如此交替地在平行于坐标轴的方向上移动，使得迭代点依次满足每个方程，从而逐步逼近两条直线的交点。

### 收敛性理论

一个迭代法只有在能够保证收敛到真解时才有实用价值。我们如何判断一个迭代法是否收敛呢？

#### [迭代矩阵](@entry_id:637346)与[误差传播](@entry_id:147381)

所有[定常迭代法](@entry_id:144014)都可以写成统一的矩阵形式 [@problem_id:1369779]：

$$
\mathbf{x}^{(k+1)} = T \mathbf{x}^{(k)} + \mathbf{c}
$$

其中 $T$ 是**[迭代矩阵](@entry_id:637346) (iteration matrix)**，$\mathbf{c}$ 是一个常数向量。例如，对于[雅可比法](@entry_id:147508)，如果我们将 $A$ 分解为对角部分 $D$、严格下三角部分 $L$ 和严格上三角部分 $U$ (即 $A = D+L+U$)，则其[迭代矩阵](@entry_id:637346)为 $T_J = -D^{-1}(L+U)$。

设 $\mathbf{x}$ 是系统的真解，它必然满足 $\mathbf{x} = T\mathbf{x} + \mathbf{c}$。定义第 $k$ 步的误差向量为 $\mathbf{e}^{(k)} = \mathbf{x} - \mathbf{x}^{(k)}$。将迭代公式与真解满足的方程相减，我们得到误差的传播规律：

$$
\mathbf{x} - \mathbf{x}^{(k+1)} = (T\mathbf{x} + \mathbf{c}) - (T\mathbf{x}^{(k)} + \mathbf{c}) = T(\mathbf{x} - \mathbf{x}^{(k)})
$$

即：

$$
\mathbf{e}^{(k+1)} = T \mathbf{e}^{(k)}
$$

这是一个至关重要的关系。它表明，每迭代一次，误差向量就会被[迭代矩阵](@entry_id:637346) $T$ 左乘一次。递推下去，我们有 $\mathbf{e}^{(k)} = T^k \mathbf{e}^{(0)}$。为了让[迭代法](@entry_id:194857)对**任意**初始猜测 $\mathbf{x}^{(0)}$（即任意初始误差 $\mathbf{e}^{(0)}$）都收敛，即 $\lim_{k \to \infty} \mathbf{e}^{(k)} = \mathbf{0}$，我们必须要求 $\lim_{k \to \infty} T^k = O$ (零矩阵)。

#### 谱半径判据

矩阵理论给出了 $T^k \to O$ 的充要条件：[迭代矩阵](@entry_id:637346) $T$ 的**谱半径 (spectral radius)** $\rho(T)$ 必须小于 1。谱半径定义为矩阵 $T$ 的所有[特征值](@entry_id:154894) $\lambda_i$ 的模的最大值：

$$
\rho(T) = \max_i |\lambda_i|
$$

**收敛性定理**：[定常迭代法](@entry_id:144014) $\mathbf{x}^{(k+1)} = T \mathbf{x}^{(k)} + \mathbf{c}$ 对任意初始向量 $\mathbf{x}^{(0)}$ 收敛的充要条件是 $\rho(T)  1$。

这个定理是迭代法理论的基石。要判断一个方法是否收敛，我们只需计算其[迭代矩阵](@entry_id:637346)的谱半径。例如，对于矩阵 $A = \begin{pmatrix} 5  2 \\ 1  -4 \end{pmatrix}$，其[雅可比迭代](@entry_id:139235)矩阵 $T_J = -D^{-1}(L+U)$ [@problem_id:2182298] 为：

$$
T_J = - \begin{pmatrix} 1/5  0 \\ 0  -1/4 \end{pmatrix} \begin{pmatrix} 0  2 \\ 1  0 \end{pmatrix} = \begin{pmatrix} 0  -2/5 \\ 1/4  0 \end{pmatrix}
$$

该矩阵的[特征值](@entry_id:154894) $\lambda$ 满足 $\det(T_J - \lambda I) = \lambda^2 - (-2/5)(1/4) = \lambda^2 + 1/10 = 0$，解得 $\lambda = \pm i/\sqrt{10}$。因此，[谱半径](@entry_id:138984)为：

$$
\rho(T_J) = \left| \pm \frac{i}{\sqrt{10}} \right| = \frac{1}{\sqrt{10}} = \frac{\sqrt{10}}{10} \approx 0.316
$$

因为 $\rho(T_J)  1$，所以对于这个系统，[雅可比法](@entry_id:147508)保证收敛。

#### 一个实用的充分条件：[对角占优](@entry_id:748380)

虽然谱半径是最终的判据，但计算[特征值](@entry_id:154894)本身可能就很复杂。幸运的是，存在一些更易于检查的**充分条件**，它们可以直接通过观察原始矩阵 $A$ 的性质来保证收敛。其中最著名和最有用的是**[严格对角占优](@entry_id:154277) (strictly diagonally dominant, SDD)** 条件。

一个矩阵 $A$ 被称为（按行）[严格对角占优](@entry_id:154277)，如果它的每一行中，对角元素的[绝对值](@entry_id:147688)都**大于**该行所有非对角元素的[绝对值](@entry_id:147688)之和：

$$
|a_{ii}|  \sum_{j \neq i} |a_{ij}|, \quad \text{for all } i = 1, \dots, n
$$

**对角占优收敛定理** (Levy–Desplanques theorem 的一个推论): 如果矩阵 $A$ 是[严格对角占优](@entry_id:154277)的，那么[雅可比法](@entry_id:147508)和[高斯-赛德尔法](@entry_id:145727)都保证对任意初始猜测收敛。

这个定理非常强大。例如，考虑矩阵 [@problem_id:2182352]：

$$
A = \begin{pmatrix} 10  -3  5 \\ 4  -8  -2.5 \\ 6  -5  12 \end{pmatrix}
$$

我们逐行检查对角占优条件：
- **行 1**：$|10| = 10  |-3| + |5| = 8$ (成立)
- **行 2**：$|-8| = 8  |4| + |-2.5| = 6.5$ (成立)
- **行 3**：$|12| = 12  |6| + |-5| = 11$ (成立)

由于所有行都满足条件，该矩阵是[严格对角占优](@entry_id:154277)的。因此，我们无需计算任何[谱半径](@entry_id:138984)，就可以直接断定[雅可比法](@entry_id:147508)（以及[高斯-赛德尔法](@entry_id:145727)）应用于此系统时，必定会收敛到其唯一解。许[多源](@entry_id:170321)于物理问题（如[热传导](@entry_id:147831)、[电网络](@entry_id:271009)）的离散化系统天然地具有这种性质，这也是[迭代法](@entry_id:194857)在这些领域广泛应用的原因之一 [@problem_id:2182368]。

### 基于优化的现代[迭代法](@entry_id:194857)

经典[定常迭代法](@entry_id:144014)虽然简单，但[收敛速度](@entry_id:636873)可能较慢，且收敛性有较强限制。现代[迭代法](@entry_id:194857)通常基于优化思想，将求解 $A\mathbf{x}=\mathbf{b}$ 的问题转化为一个等价的[函数最小化](@entry_id:138381)问题。这类方法通常收敛更快，适用范围也更广。

#### [最速下降法](@entry_id:140448)

当矩阵 $A$ **对称正定 (symmetric positive definite, SPD)** 时，求解 $A\mathbf{x}=\mathbf{b}$ 等价于最小化下面的二次型函数：

$$
f(\mathbf{x}) = \frac{1}{2} \mathbf{x}^T A \mathbf{x} - \mathbf{x}^T \mathbf{b}
$$

我们可以通过计算梯度来验证这一点：$\nabla f(\mathbf{x}) = A\mathbf{x} - \mathbf{b}$。令梯度为零，即得 $A\mathbf{x} = \mathbf{b}$。因此，寻找线性系统的解等价于寻找这个二次[凸函数](@entry_id:143075)的唯一最小值点。

**最速下降法 (method of steepest descent)** 就是一个基于此思想的[迭代算法](@entry_id:160288)。在第 $k$ 步，它从当前点 $\mathbf{x}_k$ 出发，沿着使函数 $f(\mathbf{x})$ 下降最快的方向——即负梯度方向——进行移动。这个方向恰好是该点的**残差 (residual)** 向量：

$$
\mathbf{p}_k = -\nabla f(\mathbf{x}_k) = \mathbf{b} - A \mathbf{x}_k = \mathbf{r}_k
$$

确定了方向后，我们需要决定沿这个方向走多远，即确定一个**步长 (step size)** $\alpha_k$。迭代更新规则为 $\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k$。最优的步长 $\alpha_k$ 应该使得 $f(\mathbf{x}_{k+1})$ 最小。这可以通过求解一个单变量最小化问题得到 [@problem_id:2182362]：

$$
\min_{\alpha \ge 0} g(\alpha) = f(\mathbf{x}_k + \alpha \mathbf{r}_k)
$$

通过对 $g(\alpha)$ 求导并令其为零，可以导出[最优步长](@entry_id:143372)的解析表达式：

$$
\alpha_k = \frac{\mathbf{r}_k^T \mathbf{r}_k}{\mathbf{r}_k^T A \mathbf{r}_k}
$$

这个公式在每一步都为我们提供了最佳的前进距离，确保了函数值的最大程度下降。

#### [克雷洛夫子空间](@entry_id:751067)法：GMRES

最速下降法虽然思想直观，但收敛路径可能呈“之”字形，效率不高。共轭梯度法 (Conjugate Gradient, CG) 是对其的重大改进，但它仍然要求矩阵 $A$ 是[对称正定](@entry_id:145886)的。对于许多应用中出现的**非对称**线性系统，我们需要更普适的方法。

**[克雷洛夫子空间](@entry_id:751067)法 (Krylov subspace methods)** 是当今最高效的迭代方法之一。其核心思想是在一个维度逐步增大的特殊[子空间](@entry_id:150286)中寻找最优近似解。对于矩阵 $A$ 和初始残差 $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0$，第 $m$ 维**[克雷洛夫子空间](@entry_id:751067)**定义为：

$$
\mathcal{K}_m(A, \mathbf{r}_0) = \text{span}\{\mathbf{r}_0, A\mathbf{r}_0, A^2\mathbf{r}_0, \dots, A^{m-1}\mathbf{r}_0\}
$$

这个[子空间](@entry_id:150286)包含了 $A$ 重复作用于初始残差所能到达的所有方向。

**[广义最小残差法](@entry_id:139566) (Generalized Minimal Residual method, GMRES)** 是一种著名的[克雷洛夫子空间](@entry_id:751067)法，专为求解一般（包括非对称）[线性系统](@entry_id:147850)而设计。GMRES 在第 $m$ 步寻找一个解 $\mathbf{x}_m \in \mathbf{x}_0 + \mathcal{K}_m(A, \mathbf{r}_0)$，使得其对应的残差向量 $\mathbf{r}_m = \mathbf{b} - A\mathbf{x}_m$ 的[欧几里得范数](@entry_id:172687) $\|\mathbf{r}_m\|_2$ 最小。

要实现这一点，GMRES 需要为克雷洛夫子空间构建一组[标准正交基](@entry_id:147779) $\{ \mathbf{q}_1, \mathbf{q}_2, \dots, \mathbf{q}_m \}$。这个过程通过**阿诺尔迪迭代 (Arnoldi iteration)** 完成。让我们来看一下这个过程的最初几步，以理解其机制 [@problem_id:2182305]。

1.  **[标准化](@entry_id:637219)初始残差**：计算 $\mathbf{r}_0 = \mathbf{b} - A\mathbf{x}_0$，然后得到第一个[基向量](@entry_id:199546) $\mathbf{q}_1 = \mathbf{r}_0 / \|\mathbf{r}_0\|_2$。
2.  **生成新方向**：计算向量 $\mathbf{w} = A\mathbf{q}_1$，这个向量是 $\mathcal{K}_2$ 中的新方向。
3.  **[正交化](@entry_id:149208)**：为了得到第二个[基向量](@entry_id:199546) $\mathbf{q}_2$，需要从 $\mathbf{w}$ 中减去它在 $\mathbf{q}_1$ 方向上的投影。投影系数为 $h_{11} = \mathbf{q}_1^T \mathbf{w}$。得到正交化后的向量 $\hat{\mathbf{w}} = \mathbf{w} - h_{11}\mathbf{q}_1$。
4.  **标准化**：新[基向量](@entry_id:199546)的模长为 $h_{21} = \|\hat{\mathbf{w}}\|_2$。如果 $h_{21} \neq 0$，则第二个[基向量](@entry_id:199546)为 $\mathbf{q}_2 = \hat{\mathbf{w}} / h_{21}$。

这些系数 $h_{ij}$ 会构成一个上 Hessenberg 矩阵 $H_m$。GMRES 巧妙地利用这个矩阵将原始的 $n$ 维最小化问题转化为一个 $m$ 维的[最小二乘问题](@entry_id:164198)，而 $m$ 通常远小于 $n$。这个过程优雅地处理了非对称性，使其成为求解大规模非对称系统最强大的工具之一。

本章我们从经典的[雅可比](@entry_id:264467)和高斯-赛德尔方法出发，建立了迭代法的基本概念和收敛性理论，然后过渡到基于优化的现代方法，如最速下降法和更高级的GMRES。理解这些原理与机制，是掌握和应用数值方法解决复杂[科学计算](@entry_id:143987)问题的关键一步。