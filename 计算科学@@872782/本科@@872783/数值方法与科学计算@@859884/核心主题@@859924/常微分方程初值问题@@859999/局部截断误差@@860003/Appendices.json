{"hands_on_practices": [{"introduction": "这个首个练习是一项基础训练。要真正理解局部截断误差，我们必须超越其定义，学会如何从第一性原理推导它。本题要求你通过对精确解和数值格式同时进行泰勒级数展开，来分析一种常见的二阶求解器——休恩（Heun）方法。通过比较这些展开式，你将分离出主要的误差项，从而具体地理解一个方法的代数结构是如何决定其精度的。[@problem_id:2185092]", "problem": "考虑由 $y'(t) = \\cos(t) + y(t)^2$ 给定的一阶常微分方程。Heun 方法，也称为改进的欧拉方法，是一种用于近似求解的数值格式。它使用以下两步过程将解从点 $(t_n, y_n)$ 推进到点 $(t_{n+1}, y_{n+1})$：\n\n1.  预测步：$y_{n+1}^* = y_n + h f(t_n, y_n)$\n2.  校正步：$y_{n+1} = y_n + \\frac{h}{2} [f(t_n, y_n) + f(t_{n+1}, y_{n+1}^*)]$\n\n这里，$h = t_{n+1} - t_n$ 是常数步长，且 $f(t,y) = y'(t)$。\n\n局部截断误差 $\\tau_{n+1}$ 定义为在步长起始点的值是精确的（即 $y_n = y(t_n)$）这一假设下，$t_{n+1}$ 处的精确解值与单步数值近似值之差。因此，局部截断误差由 $\\tau_{n+1} = y(t_{n+1}) - y_{n+1}$ 给出。\n\n确定对于给定的微分方程，局部截断误差 $\\tau_{n+1}$ 的首项。首项是 $h$ 的幂次中系数非零的最低次项。你的答案需要用步长 $h$、精确解 $y(t)$ 及其导数 $y''(t)$ 和 $y'''(t)$ 来表示，所有这些量均在 $t=t_n$ 处取值。", "solution": "我们考虑常微分方程（ODE）$y'(t)=f(t,y)=\\cos(t)+y^{2}$ 和 Heun 方法\n$$\ny_{n+1}=y_{n}+\\frac{h}{2}\\bigl[f(t_{n},y_{n})+f(t_{n+1},y_{n}+h f(t_{n},y_{n}))\\bigr],\n$$\n其中，为计算局部截断误差，设 $y_{n}=y(t_{n})$。令 $t=t_{n}$，并记 $y=y(t)$、$y'=y'(t)$、$y''=y''(t)$、$y'''=y'''(t)$，所有这些量均在 $t=t_{n}$ 处取值。\n\n精确解满足泰勒展开式\n$$\ny(t+h)=y+h y'+\\frac{h^{2}}{2}y''+\\frac{h^{3}}{6}y'''+\\mathcal{O}(h^{4}).\n$$\nHeun 方法使用预测值 $y^{*}=y+h f(t,y)$。将 $f(t+h,y^{*})$ 在 $(t,y)$ 点附近展开到二阶，得到\n$$\nf(t+h,y^{*})=f+h f_{t}+h f_{y}f+\\frac{h^{2}}{2}f_{tt}+h^{2}f_{ty}f+\\frac{h^{2}}{2}f_{yy}f^{2}+\\mathcal{O}(h^{3}),\n$$\n所以单步数值解为\n$$\ny_{n+1}=y+h f+\\frac{h^{2}}{2}\\bigl(f_{t}+f_{y}f\\bigr)+\\frac{h^{3}}{4}\\bigl(f_{tt}+2 f_{ty}f+f_{yy}f^{2}\\bigr)+\\mathcal{O}(h^{4}).\n$$\n沿着精确解，我们有 $y'=f$，$y''=f_{t}+f_{y}f$，以及\n$$\ny'''=f_{tt}+2 f_{ty}f+f_{yy}f^{2}+f_{y}y''.\n$$\n因此，\n$$\nf_{tt}+2 f_{ty}f+f_{yy}f^{2}=y'''-f_{y}y'',\n$$\n于是局部截断误差变为\n$$\n\\tau_{n+1}=y(t+h)-y_{n+1}=h^{3}\\left(\\frac{1}{6}y'''-\\frac{1}{4}\\bigl(y'''-f_{y}y''\\bigr)\\right)+\\mathcal{O}(h^{4})\n=h^{3}\\left(-\\frac{1}{12}y'''+\\frac{1}{4}f_{y}y''\\right)+\\mathcal{O}(h^{4}).\n$$\n对于给定的 $f(t,y)=\\cos(t)+y^{2}$，我们有 $f_{y}=2y$。因此首项为\n$$\n\\tau_{n+1}=h^{3}\\left(-\\frac{1}{12}y'''(t_{n})+\\frac{1}{2}y(t_{n})\\,y''(t_{n})\\right)+\\mathcal{O}(h^{4}).\n$$\n因此，局部截断误差的首项是 $h^{3}\\left(-\\frac{1}{12}y'''(t_{n})+\\frac{1}{2}y(t_{n})y''(t_{n})\\right)$。", "answer": "$$\\boxed{h^{3}\\left(-\\frac{1}{12}\\,y'''(t_{n})+\\frac{1}{2}\\,y(t_{n})\\,y''(t_{n})\\right)}$$", "id": "2185092"}, {"introduction": "在实际应用中，我们很少知道常微分方程（ODE）的精确解，那么我们如何控制误差呢？本练习将探讨自适应步长求解器背后的巧妙机制，例如著名的龙格-库塔-费尔贝格（Runge-Kutta-Fehlberg, RKF）方法。通过同时计算两个不同阶的近似解，这些方法可以在不知道真实解的情况下估算局部截断误差，从而动态调整步长以满足预期的精度要求。这个练习将揭示现代求解器是如何兼顾效率与可靠性的。[@problem_id:3248991]", "problem": "考虑一个常微分方程（ODE）的初值问题 $y'(t) = f(t,y(t))$，其中 $y(t_0) = y_0$，$f$ 足够光滑。一个 $p$ 阶和 $p+1$ 阶的 Runge-Kutta-Fehlberg (RKF) 嵌入式对（例如，经典的 RKF $4(5)$ 对）在大小为 $h$ 的单步内，使用相同的内部阶段求值但不同的输出权重，计算出在 $t_{n+1} = t_n + h$ 处的两个近似值：一个 $p$ 阶的低阶解 $y_{n+1}^{[p]}$ 和一个 $p+1$ 阶的高阶解 $y_{n+1}^{[p+1]}$。对于单步法，在第 $n$ 步的局部截断误差（LTE）定义为从 $t_n$ 处的精确值开始，在一步内产生的误差。自适应步长控制器旨在将局部截断误差控制在用户指定的容差范围内，而无需获取精确解 $y(t_{n+1})$。\n\n下列哪个陈述正确解释了这种自适应控制器如何在不知道 $y(t_{n+1})$ 的情况下估计局部截断误差并调整步长？选择所有适用的选项。\n\nA. 嵌入式差值 $\\delta_{n+1} = y_{n+1}^{[p+1]} - y_{n+1}^{[p]}$ 在主阶上是低阶方法局部截断误差的一个 $\\mathcal{O}(h^{p+1})$ 近似；控制器使用 $\\delta_{n+1}$ 来估计局部截断误差的大小，同时接受 $y_{n+1}^{[p+1]}$ 作为该步的结果。\n\nB. 因为 $\\delta_{n+1}$ 直接估计了高阶解 $y_{n+1}^{[p+1]}$ 的局部截断误差，一个保守的控制器应该拒绝任何不满足 $\\lVert \\delta_{n+1} \\rVert \\le \\text{tol}$ 的步，并保留 $y_{n+1}^{[p]}$ 作为接受值，以避免低估误差。\n\nC. 如果主导的局部截断误差项表现为 $C h^{p+1}$（其中 $C$ 为某个常数），那么，在从 $\\delta_{n+1}$ 构建一个标量误差度量后，下一步的步长近似选择为 $h_{\\text{new}} = h \\,\\alpha \\left(\\frac{\\text{tol}}{\\text{err}}\\right)^{1/(p+1)}$，其中 $\\alpha \\in (0,1)$ 是一个安全因子，从而使估计的局部截断误差按比例缩放到容差。\n\nD. 为了比较不同量级的误差分量，控制器通常通过权重 $w_i = \\text{atol}_i + \\text{rtol}_i \\max\\!\\left(|y_{n,i}|,\\,|y_{n+1,i}^{[p+1]}|\\right)$ 来缩放 $\\delta_{n+1}$ 的每个分量 $i \\in \\{1,\\dots,d\\}$，并使用诸如加权均方根（RMS）范数 $\\left(\\frac{1}{d}\\sum_{i=1}^d (\\delta_{n+1,i}/w_i)^2\\right)^{1/2}$ 的范数来决定接受/拒绝。\n\nE. 由于两种公式共享所有内部阶段，嵌入式差值 $\\delta_{n+1}$ 消除了所有直到 $h^{p+1}$ 阶的截断误差项，因此 $\\delta_{n+1} = \\mathcal{O}(h^{p+2})$，这直接估计了高阶解的局部截断误差。", "solution": "该问题陈述已经过验证并且是合理的。它准确地描述了使用嵌入式 Runge-Kutta-Fehlberg (RKF) 方法对常微分方程 (ODE) 进行自适应步长控制的背景。其术语和概念在数值分析领域是标准的。我们可以开始解答。\n\n一个 $p$ 阶和 $p+1$ 阶嵌入式 RKF 方法的核心原理是，在大小为 $h$ 的单步内，使用同一组函数求值（阶段）生成两个不同的数值近似。让我们将从 $(t_n, y_n)$ 开始的初值问题的精确解表示为 $\\tilde{y}(t)$。在 $t_{n+1} = t_n + h$ 处的两个数值近似与精确解有以下关系：\n\n1.  低阶近似 $y_{n+1}^{[p]}$ 的局部截断误差 (LTE) 为 $p+1$ 阶：\n    $$y_{n+1}^{[p]} = \\tilde{y}(t_{n+1}) - \\boldsymbol{C}_{p+1} h^{p+1} + \\mathcal{O}(h^{p+2})$$\n    其中 $\\boldsymbol{C}_{p+1}$ 是 $p$ 阶方法的主误差系数向量。局部截断误差定义为 $LTE^{[p]} = \\tilde{y}(t_{n+1}) - y_{n+1}^{[p]} = \\boldsymbol{C}_{p+1} h^{p+1} + \\mathcal{O}(h^{p+2})$。\n\n2.  高阶近似 $y_{n+1}^{[p+1]}$ 的局部截断误差为 $p+2$ 阶：\n    $$y_{n+1}^{[p+1]} = \\tilde{y}(t_{n+1}) - \\boldsymbol{C}_{p+2} h^{p+2} + \\mathcal{O}(h^{p+3})$$\n    局部截断误差为 $LTE^{[p+1]} = \\tilde{y}(t_{n+1}) - y_{n+1}^{[p+1]} = \\boldsymbol{C}_{p+2} h^{p+2} + \\mathcal{O}(h^{p+3})$。\n\n自适应控制器需要一个局部截断误差的估计值来决定是否接受该步并选择下一步的步长。由于真解 $\\tilde{y}(t_{n+1})$ 是未知的，局部截断误差无法直接计算。取而代之的是，它使用两个数值近似之间的差值 $\\delta_{n+1}$ 来进行估计：\n$$\\delta_{n+1} = y_{n+1}^{[p+1]} - y_{n+1}^{[p]}$$\n代入上面的表达式，我们得到：\n$$\\delta_{n+1} = \\left(\\tilde{y}(t_{n+1}) - \\boldsymbol{C}_{p+2} h^{p+2} + \\dots\\right) - \\left(\\tilde{y}(t_{n+1}) - \\boldsymbol{C}_{p+1} h^{p+1} + \\dots\\right)$$\n$$\\delta_{n+1} = \\boldsymbol{C}_{p+1} h^{p+1} - \\boldsymbol{C}_{p+2} h^{p+2} + \\dots$$\n在主阶上，这个差值为：\n$$\\delta_{n+1} = \\boldsymbol{C}_{p+1} h^{p+1} + \\mathcal{O}(h^{p+2})$$\n将此与低阶方法的局部截断误差 $LTE^{[p]} = \\boldsymbol{C}_{p+1} h^{p+1} + \\mathcal{O}(h^{p+2})$ 进行比较，我们看到 $\\delta_{n+1}$ 是 $p$ 阶方法局部截断误差的一个良好近似。\n$$LTE^{[p]} \\approx \\delta_{n+1}$$\n这个误差估计是自适应控制机制的基础。\n\n基于此基础，我们可以分析每个选项。\n\n**A. 嵌入式差值 $\\delta_{n+1} = y_{n+1}^{[p+1]} - y_{n+1}^{[p]}$ 在主阶上是低阶方法局部截断误差的一个 $\\mathcal{O}(h^{p+1})$ 近似；控制器使用 $\\delta_{n+1}$ 来估计局部截断误差的大小，同时接受 $y_{n+1}^{[p+1]}$ 作为该步的结果。**\n这个陈述完全正确。如上所述，$\\delta_{n+1}$ 是 $p$ 阶方法局部截断误差 $LTE^{[p]}$ 的一个 $\\mathcal{O}(h^{p+1})$ 近似。这个估计值用于误差控制。此外，被称为局部外推法的标准做法是使用更精确的高阶解 $y_{n+1}^{[p+1]}$ 作为传播到下一步的值，因为它的局部误差是更高阶的 $\\mathcal{O}(h^{p+2})$，比被控制的误差阶数更高。\n结论：**正确**。\n\n**B. 因为 $\\delta_{n+1}$ 直接估计了高阶解 $y_{n+1}^{[p+1]}$ 的局部截断误差，一个保守的控制器应该拒绝任何不满足 $\\lVert \\delta_{n+1} \\rVert \\le \\text{tol}$ 的步，并保留 $y_{n+1}^{[p]}$ 作为接受值，以避免低估误差。**\n这个陈述有根本性错误。误差估计 $\\delta_{n+1}$ 是 $\\mathcal{O}(h^{p+1})$ 阶的。高阶解的局部截断误差 $LTE^{[p+1]}$ 是 $\\mathcal{O}(h^{p+2})$ 阶的。一个 $\\mathcal{O}(h^{p+1})$ 的量不可能是 $\\mathcal{O}(h^{p+2})$ 量的直接估计。这个前提是错误的。此外，虽然一些控制器可能会传播 $y_{n+1}^{[p]}$，但所提供的理由很弱，并且该陈述的主要主张是不正确的。\n结论：**不正确**。\n\n**C. 如果主导的局部截断误差项表现为 $C h^{p+1}$（其中 $C$ 为某个常数），那么，在从 $\\delta_{n+1}$ 构建一个标量误差度量后，下一步的步长近似选择为 $h_{\\text{new}} = h \\,\\alpha \\left(\\frac{\\text{tol}}{\\text{err}}\\right)^{1/(p+1)}$，其中 $\\alpha \\in (0,1)$ 是一个安全因子，从而使估计的局部截断误差按比例缩放到容差。**\n这个陈述正确地描述了步长自适应公式。设 $\\text{err}$ 为误差估计 $\\delta_{n+1}$ 的一个标量范数。我们有 $\\text{err} \\approx K h^{p+1}$，其中 $K$ 为某个常数。目标是找到一个新的步长 $h_{\\text{new}}$，使得该步的误差 $\\text{err}_{\\text{new}}$ 大约等于期望的容差 $\\text{tol}$。假设 $K$ 是常数，我们有 $\\text{tol} \\approx K h_{\\text{new}}^{p+1}$。对这两个关系式取比值得到 $\\frac{\\text{tol}}{\\text{err}} \\approx \\left(\\frac{h_{\\text{new}}}{h}\\right)^{p+1}$。解出 $h_{\\text{new}}$ 得 $h_{\\text{new}} \\approx h \\left(\\frac{\\text{tol}}{\\text{err}}\\right)^{1/(p+1)}$。为了保守起见，包含一个安全因子 $\\alpha \\in (0,1)$ 是标准做法。选项中给出的公式是自适应 ODE 求解器中使用的正确且标准的公式。\n结论：**正确**。\n\n**D. 为了比较不同量级的误差分量，控制器通常通过权重 $w_i = \\text{atol}_i + \\text{rtol}_i \\max\\!\\left(|y_{n,i}|,\\,|y_{n+1,i}^{[p+1]}|\\right)$ 来缩放 $\\delta_{n+1}$ 的每个分量 $i \\in \\{1,\\dots,d\\}$，并使用诸如加权均方根（RMS）范数 $\\left(\\frac{1}{d}\\sum_{i=1}^d (\\delta_{n+1,i}/w_i)^2\\right)^{1/2}$ 的范数来决定接受/拒绝。**\n这个陈述准确地描述了一种为常微分方程组（$y \\in \\mathbb{R}^d$）计算标量误差度量的标准方法。当解向量 $y$ 的分量具有不同尺度时，一个简单的欧几里得范数是不合适的。此时使用混合绝对/相对误差控制。权重 $w_i = \\text{atol}_i + \\text{rtol}_i \\max\\!\\left(|y_{n,i}|,\\,|y_{n+1,i}^{[p+1]}|\\right)$ 提供了一个分量级的容差。使用 $\\max(|y_{n,i}|, |y_{n+1,i}^{[p+1]}|)$ 是在步长上估计解分量大小的一种稳健方法。然后，缩放后的误差 $\\delta_{n+1,i}/w_i$ 使用一个范数（如此处所示的加权 RMS 范数）进行组合。如果这个最终的标量误差 $\\le 1$，则该步被接受。这是对现代误差控制策略的正确描述。\n结论：**正确**。\n\n**E. 由于两种公式共享所有内部阶段，嵌入式差值 $\\delta_{n+1}$ 消除了所有直到 $h^{p+1}$ 阶的截断误差項，因此 $\\delta_{n+1} = \\mathcal{O}(h^{p+2})$，这直接估计了高阶解的局部截断误差。**\n这个陈述是不正确的。如初始推导所示，差值 $\\delta_{n+1} = y_{n+1}^{[p+1]} - y_{n+1}^{[p]}$ 导致 $\\delta_{n+1} = \\boldsymbol{C}_{p+1}h^{p+1} + \\mathcal{O}(h^{p+2})$。低阶方法的主导截断误差项是 $\\mathcal{O}(h^{p+1})$ 阶的，它*不会*被消除。嵌入式对设计的目的正在于，两种方法有不同的主误差系数，因此它们的差值分离出了精度较低方法的主误差项。因此，$\\delta_{n+1}$ 是 $\\mathcal{O}(h^{p+1})$ 阶的，而不是 $\\mathcal{O}(h^{p+2})$ 阶的。因此，它不能估计高阶解的局部截断误差，后者的阶数是 $\\mathcal{O}(h^{p+2})$。\n结论：**不正确**。\n\n总之，陈述 A、C 和 D 正确地描述了使用嵌入式 Runge-Kutta 对进行自适应步长控制的基本方面。", "answer": "$$\\boxed{ACD}$$", "id": "3248991"}, {"introduction": "理论给出了数值方法的预期阶数，但在实践中我们如何验证这一点，特别是对于一个“黑箱”求解器？本练习将指导你完成一次收敛阶的实验性研究（Experimental Order of Convergence, EOC），这是数值代码验证的基石。你将编写一个程序，用于测量不同求解器在不同步长下的全局误差，并通过数据分析来确定它们的有效精度阶数。这个动手计算实验连接了局部截断误差和可观测的全局误差，为你提供一个测试和验证数值软件的强大工具。[@problem_id:3248932]", "problem": "考虑一个单步黑箱常微分方程 (ODE) 求解器，对于一个足够光滑的初值问题，其数值输出的最终时刻全局误差据信与步长之间存在一种可预测的缩放关系。目标是通过测量不同步长下的全局误差并确定其缩放指数，从而通过实验确定该求解器的阶 $p$。从以下基本概念出发：单步法的局部截断误差定义、相容性以及零稳定性。然后，设计一种不依赖求解器任何内部知识，仅使用可观测的全局误差来推断阶 $p$ 的方法。\n\n使用的定义与假设：\n- 将一个单步法应用于常微分方程 (ODE) 的初值问题：$$\\frac{dy}{dt} = f(t,y), \\quad y(t_0) = y_0.$$\n- 局部截断误差是指当提供精确数据时，方法在单步内产生的误差；对于一个 $p$ 阶方法，每步的局部截断误差在步长 $h$ 足够小时，其缩放关系为 $$\\tau(h) = \\mathcal{O}(h^{p+1})$$。\n- 在相容性和零稳定性的标准假设下，固定最终时刻 $T$ 的全局误差的缩放关系为 $$E(h) = \\mathcal{O}(h^{p}).$$\n\n任务：\n1. 基于上述基础，推导一种仅使用若干步长下最终时刻全局误差的测量值来估计黑箱单步求解器阶 $p$ 的方法。你的推导必须解释如何使用一个与上述基础一致且有充分数学依据的模型，将测量数据转换为对 $p$ 的估计，并清晰地陈述使该估计有意义所需的假设。\n2. 将所推导的方法实现为一个完整、可运行的程序，该程序需：\n   - 选择具有已知解析解 $y(t)$ 的常微分方程，以计算固定最终时刻 $T$ 的全局误差。\n   - 针对多个步长 $h$ 运行黑箱求解器，并测量绝对全局误差 $$E(h) = \\left|y_{\\text{num}}(T;h) - y_{\\text{exact}}(T)\\right|.$$\n   - 将误差与步长数据拟合到幂律模型，并计算缩放指数作为估计的阶 $p$。\n3. 程序必须通过包装三种经典的单步法来实现黑箱求解器接口，每种方法都作为黑箱处理：前向欧拉法、显式梯形法（休恩法）和经典四阶龙格-库塔法。除了将其作为黑箱应用于常微分方程外，该实现不应使用任何特定于方法的解析知识。\n4. 使用以下测试套件。对于下述每个测试用例，使用所述程序估计 $p$ 并报告一个浮点数。在所有情况下，都使用最终时刻全局误差的绝对值。不涉及物理单位。\n   - 测试用例 1：求解器为应用于 $$\\frac{dy}{dt} = -y, \\quad y(0) = 1,$$ 的前向欧拉法，解析解为 $$y(t) = e^{-t},$$ 最终时刻为 $$T = 1,$$ 步长为 $$h \\in \\left\\{\\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}\\right\\}.$$\n   - 测试用例 2：求解器为应用于相同 ODE 的显式梯形法（休恩法），具有相同的初始条件和解析解，其中 $$T = 1$$ 且步长为 $$h \\in \\left\\{\\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}\\right\\}.$$\n   - 测试用例 3：求解器为应用于相同 ODE 的经典四阶龙格-库塔法，具有相同的初始条件和解析解，其中 $$T = 1$$ 且步长为 $$h \\in \\left\\{\\frac{1}{10}, \\frac{1}{20}, \\frac{1}{40}, \\frac{1}{80}\\right\\}.$$\n   - 测试用例 4：求解器为应用于相同 ODE 的经典四阶龙格-库塔法，具有相同的初始条件和解析解，其中 $$T = 1$$ 且步长为 $$h \\in \\left\\{2^{-8}, 2^{-9}, 2^{-10}, 2^{-11}, 2^{-12}, 2^{-13}, 2^{-14}, 2^{-15}\\right\\}.$$ 此用例旨在探究在极小步长下潜在的舍入误差效应，以测试估计方法的鲁棒性。\n   - 测试用例 5：求解器为应用于 $$\\frac{dy}{dt} = y, \\quad y(0) = 1,$$ 的前向欧拉法，解析解为 $$y(t) = e^{t},$$ 最终时刻为 $$T = 1,$$ 步长为 $$h \\in \\left\\{\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}\\right\\}.$$ 此用例旨在通过相对粗糙的步长探究偏离渐近区域时的行为。\n5. 你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表的结果，例如 $$[p_1,p_2,p_3,p_4,p_5].$$ 每个 $p_i$ 必须是一个浮点数。\n\n程序必须是自包含的，无需任何输入，并且仅使用 Python 标准库和指定的数值库。估计过程必须通过在对数尺度上进行线性拟合来实现，以推断指数 $p$。", "solution": "该问题要求推导并实现一种方法，用以通过实验确定一个黑箱单步常微分方程 (ODE) 求解器的精度阶 $p$。这一过程被称为实验收敛阶 (EOC) 研究，是数值分析中的一种标准验证技术。\n\n该方法的基础在于一个收敛的数值方法的全局误差与步长之间的关系。应用于初值问题\n$$ \\frac{dy}{dt} = f(t,y), \\quad y(t_0) = y_0 $$\n的单步法，如果其局部截断误差 (LTE) $\\tau(h)$ 的缩放关系为 $\\mathcal{O}(h^{p+1})$，则称该方法为 $p$ 阶方法。LTE 是指假设从精确解开始，单步计算中引入的误差。对于一个既是相容的（即 $p \\ge 1$）又是零稳定的（所有合理的单步法，包括此处考虑的方法，都满足此条件）方法，其在固定最终时刻 $T$ 的全局误差 $E(h)$ 被证明与步长 $h$ 的缩放关系如下：\n$$ E(h) = \\mathcal{O}(h^p) $$\n这个结果源于局部误差在约 $(T-t_0)/h$ 个步长上的累积。\n\n为了推导出一个估计 $p$ 的实用方法，我们将渐近关系形式化。对于足够小的步长 $h$，$\\mathcal{O}(h^p)$ 的行为意味着全局误差可以由其误差展开式的主项来近似：\n$$ E(h) \\approx C h^p $$\n其中 $C$ 是一个常数，它依赖于具体的 ODE、所用方法以及积分区间长度 $T-t_0$，但与步长 $h$ 无关。核心假设是所选的步长足够小，以至于处于该近似有效的*渐近区域*。\n\n我们的目标是从一组测量数据点 $(h_i, E_i)$ 中确定指数 $p$，其中 $E_i = E(h_i)$ 是在给定步长 $h_i$ 下观测到的全局误差。幂律关系 $E \\approx C h^p$ 可以通过对两边取自然对数来进行线性化：\n$$ \\ln(E(h)) \\approx \\ln(C h^p) $$\n利用对数的性质，上式变为：\n$$ \\ln(E(h)) \\approx \\ln(C) + \\ln(h^p) = \\ln(C) + p \\ln(h) $$\n该方程具有线性方程 $Y = mX + b$ 的形式，其中：\n- 因变量是 $Y = \\ln(E(h))$。\n- 自变量是 $X = \\ln(h)$。\n- 直线的斜率是 $m = p$。\n- y 轴截距是 $b = \\ln(C)$。\n\n这一见解构成了估计方法的基础。给定一组对应于步长序列 $\\{ h_1, h_2, \\dots, h_N \\}$ 的 $N$ 个全局误差测量值 $\\{ E_1, E_2, \\dots, E_N \\}$，我们可以按如下方式估计 $p$：\n1.  对每个测量对 $(h_i, E_i)$，计算变换后的坐标 $x_i = \\ln(h_i)$ 和 $y_i = \\ln(E_i)$。\n2.  对变换后的点集 $\\{(x_i, y_i)\\}_{i=1}^N$ 执行线性回归，以找到最佳拟合直线的斜率。一种标准技术是最小二乘法。\n3.  这条直线的最终斜率即为收敛阶 $p$ 的实验估计值。\n\n对于数据点 $(x_i, y_i)$，最小二乘回归直线的斜率 $p$ 由以下公式给出：\n$$ p = \\frac{N \\sum_{i=1}^{N}(x_i y_i) - \\left(\\sum_{i=1}^{N} x_i\\right) \\left(\\sum_{i=1}^{N} y_i\\right)}{N \\sum_{i=1}^{N}(x_i^2) - \\left(\\sum_{i=1}^{N} x_i\\right)^2} $$\n\n为使该方法得出有意义的 $p$ 的估计值，必须满足两个关键假设：\n1.  **渐近区域**：步长 $h_i$ 必须足够小，以使近似式 $E(h) \\approx C h^p$ 成立。如果 $h$ 太大，误差展开式中的高阶项将不可忽略，对数-对数图将不是线性的。\n2.  **截断误差主导**：步长 $h_i$ 必须足够大，以使全局误差由截断误差主导，而非浮点舍入误差。当 $h \\to 0$ 时，截断误差消失，但舍入误差（会随着步数增加而累积）可能成为主要的误差来源。这种效应可能导致全局误差在 $h$ 非常小时趋于平稳甚至增加，从而破坏线性拟合。\n\n该实现将把此程序应用于指定的测试用例。它涉及：\n- 将三种单步法（前向欧拉法、显式梯形法/休恩法和经典 RK4）实现为黑箱函数。\n- 一个通用的驱动函数，它接受一个求解器函数、一个 ODE 和一个步长 $h$，并计算最终时刻 $T$ 的数值解。\n- 一个分析函数，它为每个测试用例协调一系列步长的运行，对照已知的解析解计算全局误差，并对经过对数变换的数据执行线性回归以提取斜率 $p$。\n- `numpy.polyfit` 函数为执行所需的线性回归提供了一种直接而鲁棒的方法。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements a method to experimentally determine the convergence order\n    of black-box ODE solvers and applies it to a suite of test cases.\n    \"\"\"\n\n    # --- Black-box solver implementations ---\n    def forward_euler(f, t, y, h):\n        \"\"\"First-order forward Euler method.\"\"\"\n        return y + h * f(t, y)\n\n    def heun_method(f, t, y, h):\n        \"\"\"Second-order explicit trapezoidal method (Heun's method).\"\"\"\n        k1 = f(t, y)\n        y_pred = y + h * k1\n        k2 = f(t + h, y_pred)\n        return y + (h / 2.0) * (k1 + k2)\n\n    def rk4_method(f, t, y, h):\n        \"\"\"Fourth-order classical Runge-Kutta method.\"\"\"\n        k1 = f(t, y)\n        k2 = f(t + h / 2.0, y + (h / 2.0) * k1)\n        k3 = f(t + h / 2.0, y + (h / 2.0) * k2)\n        k4 = f(t + h, y + h * k3)\n        return y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # --- Generic ODE solver runner ---\n    def run_ode_solver(solver_func, f, t0, y0, T, h):\n        \"\"\"\n        Integrates an ODE from t0 to T using a given solver and step size.\n\n        Args:\n            solver_func: The black-box one-step solver function.\n            f: The ODE function dy/dt = f(t, y).\n            t0: Initial time.\n            y0: Initial value.\n            T: Final time.\n            h: Step size.\n\n        Returns:\n            The numerical solution at time T.\n        \"\"\"\n        t = float(t0)\n        y = float(y0)\n        \n        # Use round to robustly handle floating-point representation of h\n        num_steps = int(round((T - t0) / h))\n\n        for _ in range(num_steps):\n            y = solver_func(f, t, y, h)\n            t += h\n        \n        return y\n\n    # --- Order estimation logic ---\n    def estimate_order(solver_func, f, y_exact_func, t0, y0, T, h_values):\n        \"\"\"\n        Estimates the convergence order p by fitting a line to log-log data.\n        \"\"\"\n        log_h_list = []\n        log_error_list = []\n\n        for h in h_values:\n            y_numerical = run_ode_solver(solver_func, f, t0, y0, T, h)\n            y_exact = y_exact_func(T)\n            error = np.abs(y_numerical - y_exact)\n            \n            # Avoid log(0) which is -inf; this happens if the error is below\n            # machine precision. Such points do not follow the truncation\n            # error model and should be excluded from the fit.\n            if error  1e-16:\n                continue\n\n            log_h_list.append(np.log(h))\n            log_error_list.append(np.log(error))\n\n        # Perform linear regression on log-log data.\n        # np.polyfit(x, y, 1) returns [slope, intercept] for a linear fit.\n        # The slope is the estimated order p.\n        if len(log_h_list)  2:\n            return np.nan # Not enough data points for a fit\n\n        p_estimate = np.polyfit(log_h_list, log_error_list, 1)[0]\n        return p_estimate\n\n    # --- Test Case Definitions ---\n    \n    # ODE and analytical solution for cases 1, 2, 3, 4\n    def f1(t, y):\n        return -y\n    def y_exact1(t):\n        return np.exp(-t)\n\n    # ODE and analytical solution for case 5\n    def f2(t, y):\n        return y\n    def y_exact2(t):\n        return np.exp(t)\n\n    test_cases = [\n        # Case 1: Forward Euler on y'=-y\n        {'solver': forward_euler, 'f': f1, 'y_exact': y_exact1, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n         'h_list': [1.0/4, 1.0/8, 1.0/16, 1.0/32]},\n        # Case 2: Heun's method on y'=-y\n        {'solver': heun_method, 'f': f1, 'y_exact': y_exact1, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n         'h_list': [1.0/4, 1.0/8, 1.0/16, 1.0/32]},\n        # Case 3: RK4 method on y'=-y\n        {'solver': rk4_method, 'f': f1, 'y_exact': y_exact1, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n         'h_list': [1.0/10, 1.0/20, 1.0/40, 1.0/80]},\n        # Case 4: RK4 with very small steps (probing roundoff effects)\n        {'solver': rk4_method, 'f': f1, 'y_exact': y_exact1, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n         'h_list': [2.0**-i for i in range(8, 16)]},\n        # Case 5: Forward Euler on y'=y (probing non-asymptotic regime)\n        {'solver': forward_euler, 'f': f2, 'y_exact': y_exact2, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n         'h_list': [1.0/2, 1.0/4, 1.0/8]},\n    ]\n    \n    results = []\n    for case in test_cases:\n        p = estimate_order(case['solver'], case['f'], case['y_exact'], case['t0'], case['y0'], case['T'], case['h_list'])\n        results.append(p)\n\n    # Format output as a comma-separated list of floats in brackets\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3248932"}]}