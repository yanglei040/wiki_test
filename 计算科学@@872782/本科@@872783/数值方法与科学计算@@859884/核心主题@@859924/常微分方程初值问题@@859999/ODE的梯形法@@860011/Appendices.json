{"hands_on_practices": [{"introduction": "梯形法等隐式方法的核心挑战在于，每一步都需要求解一个代数方程。对于刚性常微分方程，这个求解过程的效率和稳定性至关重要。通过这个实践练习 [@problem_id:3284122]，你将亲手实现并比较两种经典的求解策略——不动点迭代法和牛顿法，从而深刻理解为何牛顿法在处理刚性问题时是更优越的选择。", "problem": "实现一个程序，该程序为一个从基本积分形式推导出的标量初值问题，构建并分析一个隐式单步法。考虑由常微分方程 $y'(t) = -100\\,y(t) + 100\\,t + 101$ 和初始条件 $y(0) = 1$ 定义的初值问题。从恒等式 $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\,dt$ 出发，并利用数值积分可以在短区间上近似光滑函数积分这一事实，推导将经典梯形积分法则应用于该积分所产生的隐式离散方程。不要写下任何已有的步进公式；相反，应仅使用梯形数值积分法则的定义作为唯一输入，并以 $t_n$ 处的已知量和 $t_{n+1}$ 处的未知量来表示定义 $y_{n+1}$ 的非线性方程。\n\n为了在每一步从隐式方程计算 $y_{n+1}$，请为 $y_{n+1}$ 必须满足的非线性标量方程实现以下两种内部求解器：\n- 一种基于梯形构造中出现的右侧表达式进行重复代换的不动点（Picard）迭代，初始值为 $y^{(0)} = y_n$。\n- 牛顿法，使用隐式方程左侧关于 $y$ 的导数，初始猜测值同样为 $y^{(0)} = y_n$。\n\n对两种内部求解器均使用 $10^{-12}$ 的绝对停止容差，牛顿法应用于方程的自然残差，不动点迭代应用于连续迭代值之间的差。每一步内部迭代的最大次数为 $100$ 次。如果在任何一步中，不动点迭代未能在此上限内收敛，则认为该步长下整个不动点求解失败，并报告如下指定的哨兵值。对于两种方法，都应计算所有步数累积的内部迭代总数。\n\n为量化精度，使用积分因子法推导给定初值问题的精确解，并用它来计算最终时间 $T$ 时的绝对误差 $|y_N - y_{\\text{exact}}(T)|$。积分区间为 $[0, T]$，其中 $T = 1$，步长为 $h$，因此 $t_n = n h$ 且 $N = T/h$ 是一个整数。\n\n测试套件：\n- 在步长 $h \\in \\{0.001, 0.01, 0.02, 0.05\\}$ 下积分至 $T = 1$，按此顺序。即，$h = 0.001$，$h = 0.01$，$h = 0.02$ 和 $h = 0.05$。\n- 对于每个 $h$，计算：\n  1) 当隐式求解由牛顿法执行时，梯形法产生的最终值 $y_N^{\\text{(Newton)}}$，\n  2) 在 $T$ 时的绝对误差，即 $|y_N^{\\text{(Newton)}} - y_{\\text{exact}}(1)|$，\n  3) 所有步数累加的内部牛顿迭代总数，\n  4) 如果所有步数都收敛，则为所有步数累加的内部不动点迭代总数；否则输出整数 $-1$ 表示失败。\n\n你的程序应生成单行输出，其中包含一个包含在方括号内的、由逗号分隔的扁平结果列表，结果按上述步长顺序排列，每个步长有四个数字，顺序如上指定。具体来说，输出格式必须为\n$[y_N^{\\text{(Newton)}}(h_1), |y_N^{\\text{(Newton)}}(h_1) - y_{\\text{exact}}(1)|, \\text{iters}^{\\text{(Newton)}}(h_1), \\text{iters}^{\\text{(Fixed)}}(h_1), \\dots, y_N^{\\text{(Newton)}}(h_4), |y_N^{\\text{(Newton)}}(h_4) - y_{\\text{exact}}(1)|, \\text{iters}^{\\text{(Newton)}}(h_4), \\text{iters}^{\\text{(Fixed)}}(h_4)]$，其中 $h_1 = 0.001$，$h_2 = 0.01$，$h_3 = 0.02$，$h_4 = 0.05$。如果对于给定的 $h$，不动点迭代在任何一步失败，则该 $h$ 的 $\\text{iters}^{\\text{(Fixed)}}(h)$ 打印为 $-1$。所有数字必须在单行上以所示的确切列表格式打印为纯十进制数字，不带单位，也没有多余的空格。", "solution": "用户提供的问题是常微分方程（ODE）数值分析领域中一个明确定义的任务。它要求推导、实现和分析隐式梯形法。\n\n### 步骤 1：问题验证\n\n根据所需标准对问题进行验证。\n\n*   **提取给定条件**：\n    *   **ODE**：标量初值问题（IVP）是 $y'(t) = -100\\,y(t) + 100\\,t + 101$。\n    *   **初始条件**：$y(0) = 1$。\n    *   **ODE 的积分形式**：$y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\,dt$。\n    *   **方法推导**：数值方法必须通过将梯形积分法则应用于积分形式来推导。\n    *   **内部求解器**：每一步的隐式方程将使用两种方法求解：不动点（Picard）迭代和牛顿法。\n    *   **内部求解器的初始猜测值**：两种方法均为 $y^{(0)} = y_n$。\n    *   **内部求解器容差**：绝对容差为 $10^{-12}$。对于牛顿法，这适用于残差。对于不动点法，这适用于连续迭代值之间的差。\n    *   **内部求解器迭代限制**：每个时间步长最多 $100$ 次迭代。\n    *   **不动点法失败**：如果在任何一步中，未能在迭代限制内实现收敛，则该步长 $h$ 的总迭代次数报告为 $-1$。\n    *   **分析**：计算最终时间 $T=1$ 时的绝对误差 $|y_N - y_{\\text{exact}}(T)|$。\n    *   **离散化**：时间区间为 $[0, 1]$。步长 $h$ 导致 $t_n = n h$ 和总步数 $N = T/h$。\n    *   **测试套件**：步长 $h \\in \\{0.001, 0.01, 0.02, 0.05\\}$。\n    *   **每个 `h` 的要求输出**：$y_N^{\\text{(Newton)}}$、 $|y_N^{\\text{(Newton)}} - y_{\\text{exact}}(1)|$、牛顿法总迭代次数、不动点法总迭代次数（或 $-1$）。\n\n*   **验证结论**：\n    *   **科学上成立**：是的。该问题是数值分析中的一个标准练习，涉及将已建立的方法（梯形法则、牛顿法、不动点迭代）应用于一个线性 ODE。\n    *   **适定性**：是的。线性 IVP 是适定的，保证了唯一解的存在。数值任务被清晰地指定。\n    *   **客观性**：是的。该问题以精确的数学和计算要求定义。\n    *   该问题是完整的、一致的并且计算上是可行的。这是一项既需要推导又需要实现的实质性且非平凡的任务。\n\n*   **结论**：问题有效。\n\n### 步骤 2：求解推导与方法分析\n\n在实现之前，求解需要几个解析推导。ODE 的形式为 $y'(t) = f(t, y(t))$，其中 $f(t, y) = -100y + 100t + 101$。\n\n#### 精确解的推导\nODE $y' + 100y = 100t + 101$ 是一阶线性 ODE。我们使用积分因子 $I(t) = e^{\\int 100 dt} = e^{100t}$ 来求解。将 ODE 乘以 $I(t)$ 得到：\n$$ e^{100t}y' + 100e^{100t}y = (100t + 101)e^{100t} $$\n左边是一个乘积的导数：\n$$ \\frac{d}{dt}(y(t)e^{100t}) = (100t + 101)e^{100t} $$\n对两边关于 $t$ 积分：\n$$ y(t)e^{100t} = \\int (100t + 101)e^{100t} dt $$\n右边的积分使用分部积分法求解，$\\int u dv = uv - \\int v du$，设 $u = 100t+101$ 和 $dv = e^{100t}dt$。这得到 $du = 100dt$ 和 $v = \\frac{1}{100}e^{100t}$。\n$$ \\int (100t + 101)e^{100t} dt = (100t+101)\\frac{e^{100t}}{100} - \\int \\frac{e^{100t}}{100} (100) dt = (t + 1.01)e^{100t} - \\int e^{100t}dt $$\n$$ = (t + 1.01)e^{100t} - \\frac{1}{100}e^{100t} + C = (t+1)e^{100t} + C $$\n因此，通解为：\n$$ y(t) = t + 1 + Ce^{-100t} $$\n应用初始条件 $y(0) = 1$：\n$$ 1 = 0 + 1 + Ce^0 \\implies C = 0 $$\n精确解是 $y_{\\text{exact}}(t) = t + 1$。在最终时间 $T=1$ 时的值为 $y_{\\text{exact}}(1) = 1+1=2$。\n\n#### 梯形法的推导\n从 ODE 在单个步长 $[t_n, t_{n+1}]$ 上的积分形式出发：\n$$ y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) dt $$\n我们使用梯形法则近似该积分，$\\int_a^b g(x)dx \\approx \\frac{b-a}{2}(g(a)+g(b))$。设 $h=t_{n+1}-t_n$，$y_n \\approx y(t_n)$，以及 $y_{n+1} \\approx y(t_{n+1})$。\n$$ \\int_{t_n}^{t_{n+1}} f(t, y(t)) dt \\approx \\frac{h}{2}(f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1}))) $$\n将此代入积分方程，得到隐式梯形法：\n$$ y_{n+1} = y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, y_{n+1})) $$\n这是一个隐式方程，因为未知数 $y_{n+1}$ 出现在方程的两边。\n\n#### 内部求解器的设置\n为了在每一步找到 $y_{n+1}$，我们必须解这个方程。设 $w$ 是 $y_{n+1}$ 的未知值。\n\n**1. 不动点迭代：**\n方程被重排以形成不动点映射 $w = G(w)$：\n$$ w = y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) $$\n迭代为 $w^{(k+1)} = G(w^{(k)})$，从 $w^{(0)} = y_n$ 开始。重复此过程直到 $|w^{(k+1)} - w^{(k)}|  10^{-12}$。\n收敛性由压缩映射原理决定，该原理要求 $|G'(w)|  1$。导数为：\n$$ G'(w) = \\frac{d}{dw} \\left( y_n + \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) \\right) = \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, w) $$\n对于给定的问题，$\\frac{\\partial f}{\\partial y} = -100$。因此，$G'(w) = -50h$。如果 $|-50h|  1$ 或 $h  0.02$，迭代收敛。\n*   对于 $h=0.001$：$|-50(0.001)|=0.05  1$。收敛。\n*   对于 $h=0.01$：$|-50(0.01)|=0.5  1$。收敛。\n*   对于 $h=0.02$：$|-50(0.02)|=1$。这是一个边界情况，不保证收敛，收敛会极其缓慢，或者可能失败。迭代将不会在 $100$ 步内满足容差。\n*   对于 $h=0.05$：$|-50(0.05)|=2.5 > 1$。发散。\n因此，预计不动点法对于 $h=0.02$ 和 $h=0.05$ 会失败。\n\n**2. 牛顿法：**\n我们求解残差函数 $F(w) = 0$ 的根，其中：\n$$ F(w) = w - y_n - \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) = 0 $$\n牛顿-拉夫逊迭代为 $w^{(k+1)} = w^{(k)} - \\frac{F(w^{(k)})}{F'(w^{(k)})}$，从 $w^{(0)} = y_n$ 开始。重复此过程直到残差足够小：$|F(w^{(k)})|  10^{-12}$。\n导数 $F'(w)$ 是：\n$$ F'(w) = \\frac{d}{dw} \\left( w - y_n - \\frac{h}{2}(f(t_n, y_n) + f(t_{n+1}, w)) \\right) = 1 - \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, w) $$\n对于我们的问题，$F'(w) = 1 - \\frac{h}{2}(-100) = 1 + 50h$。\n由于 $f(t,y)$ 关于 $y$ 是线性的，残差 $F(w)$ 是 $w$ 的线性函数。因此，牛顿法将在单次迭代中找到此线性方程的精确根（不考虑浮点不精确性）。因此，对于每个时间步，牛顿迭代的次数预计为 $1$。\n\n### 步骤 3：计算算法\n总体算法如下：\n1.  初始化一个列表以存储最终结果。\n2.  定义函数 $f(t,y) = -100y + 100t + 101$ 及其偏导数 $\\frac{\\partial f}{\\partial y} = -100$。\n3.  定义精确解 $y_{\\text{exact}}(t) = t+1$。最终值为 $y_{\\text{exact}}(1)=2$。\n4.  遍历每个步长 $h \\in \\{0.001, 0.01, 0.02, 0.05\\}$。\n5.  对于每个 $h$，从 $t=0$ 到 $t=1$ 运行两个独立的模拟：一个使用牛顿法作为内部求解器，另一个使用不动点法。\n6.  在每个模拟中，初始化 $t=0$，$y=1$，以及总迭代计数器。\n7.  迭代 $N=T/h$ 次。在每一步中，调用各自的内部求解器（牛顿法或不动点法）从 $y_n$ 中找到 $y_{n+1}$。累加求解器返回的迭代次数。\n8.  对于不动点模拟，如果任何一步未能在 $100$ 次迭代内收敛，则设置一个失败标志。最终迭代计数将为 $-1$。\n9.  循环结束后，计算牛顿法结果的最终误差。\n10. 存储当前 $h$ 的四个所需值：$y_N^{\\text{(Newton)}}$、误差、牛顿法迭代次数和不动点法迭代次数。\n11. 处理完所有 $h$ 值后，将结果列表扁平化并格式化以便打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the trapezoidal method for a given ODE\n    using both Newton and Fixed-Point inner solvers.\n    \"\"\"\n\n    # --- Problem Definition ---\n    # ODE: y'(t) = -100*y(t) + 100*t + 101\n    # Initial Condition: y(0) = 1\n    # Time interval: [0, 1]\n    \n    y0 = 1.0\n    t_end = 1.0\n    tol = 1e-12\n    max_inner_iters = 100\n    \n    test_h_values = [0.001, 0.01, 0.02, 0.05]\n\n    def f(t, y):\n        \"\"\"RHS of the ODE y' = f(t, y).\"\"\"\n        return -100.0 * y + 100.0 * t + 101.0\n\n    def dfdy(t, y):\n        \"\"\"Partial derivative of f with respect to y.\"\"\"\n        # For this problem, df/dy is constant.\n        return -100.0\n\n    def y_exact(t):\n        \"\"\"Exact solution to the IVP.\"\"\"\n        return t + 1.0\n\n    # --- Solver Implementations ---\n\n    def run_simulation(h, inner_solver_type):\n        \"\"\"\n        Integrates the ODE from t=0 to t=T with step size h,\n        using the specified inner solver.\n\n        Args:\n            h (float): The step size.\n            inner_solver_type (str): 'newton' or 'fixed_point'.\n\n        Returns:\n            A tuple (final_y, total_iters). `total_iters` is -1 on failure.\n        \"\"\"\n        y = y0\n        t = 0.0\n        n_steps = int(round(t_end / h))\n        total_iters = 0\n        \n        for n in range(n_steps):\n            tn = n * h\n            tn1 = (n + 1) * h\n        \n            if inner_solver_type == 'newton':\n                y_next, iters_step = solve_newton_step(y, tn, h)\n            elif inner_solver_type == 'fixed_point':\n                y_next, iters_step = solve_fp_step(y, tn, h)\n            \n            if iters_step == -1:\n                return None, -1 # Failure to converge\n            \n            y = y_next\n            total_iters += iters_step\n            \n        return y, total_iters\n\n    def solve_newton_step(yn, tn, h):\n        \"\"\"Solves for y_{n+1} using Newton's method.\"\"\"\n        tn1 = tn + h\n        fn = f(tn, yn)\n        \n        # F'(w) is constant for this linear ODE\n        F_prime = 1.0 - (h / 2.0) * dfdy(tn1, 0)\n        \n        w = yn  # Initial guess w^(0)\n        \n        # Check initial guess residual\n        fw = f(tn1, w)\n        F_w = w - yn - (h / 2.0) * (fn + fw)\n        if np.abs(F_w)  tol:\n            return w, 0\n\n        for iters in range(1, max_inner_iters + 1):\n            w = w - F_w / F_prime\n            \n            # Check residual of the new iterate\n            fw = f(tn1, w)\n            F_w = w - yn - (h / 2.0) * (fn + fw)\n            \n            if np.abs(F_w)  tol:\n                return w, iters\n                \n        return None, -1  # Did not converge\n\n    def solve_fp_step(yn, tn, h):\n        \"\"\"Solves for y_{n+1} using fixed-point iteration.\"\"\"\n        tn1 = tn + h\n        fn = f(tn, yn)\n        \n        w_k = yn # Initial guess w^(0)\n        \n        for iters in range(1, max_inner_iters + 1):\n            fw_k = f(tn1, w_k)\n            w_k_plus_1 = yn + (h / 2.0) * (fn + fw_k)\n            \n            if np.abs(w_k_plus_1 - w_k)  tol:\n                return w_k_plus_1, iters\n            \n            w_k = w_k_plus_1\n        \n        return None, -1 # Did not converge\n\n    # --- Main Execution Logic ---\n    results = []\n    y_final_exact = y_exact(t_end)\n\n    for h in test_h_values:\n        # 1. Newton's method results\n        yN_newton, iters_newton = run_simulation(h, 'newton')\n        error_newton = np.abs(yN_newton - y_final_exact)\n        \n        # 2. Fixed-point method results\n        # We don't need the final y value, just the iteration count.\n        _, iters_fp = run_simulation(h, 'fixed_point')\n\n        # 3. Collect results for this h\n        results.extend([yN_newton, error_newton, iters_newton, iters_fp])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3284122"}, {"introduction": "在许多科学模型（如种群动态模型）中，解必须保持正值才具有物理意义，一个好的数值方法应该能再现这种性质。这个练习 [@problem_id:3284056] 将引导你通过理论分析和编程实践，探究梯形法在何种条件下能够保持解的正性。你将把抽象的稳定性理论与具体的数值行为联系起来，从而切身体会到数值方法的选择如何直接影响计算结果的物理真实性。", "problem": "考虑一个常微分方程（ODE）的标量初值问题\n$$\n\\frac{dy}{dt} = f(t,y), \\quad y(t_0) = y_0,\n$$\n要求其真解对于所有 $t \\ge t_0$ 都满足 $y(t)  0$。许多模型，例如种群动态模型，将 $y(t)  0$ 作为一个物理约束。如果一个数值方法从 $y_0  0$ 出发，其产生的数值近似解 $y_n$ 在所有步长 $n$ 上都保持严格为正，则称该方法是保正的。\n\n你的任务是研究梯形法是否保正。从单个时间步长上初值问题的基本积分形式开始：\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\, dt,\n$$\n并回想一下，梯形求积法通过被积函数在端点的平均值乘以区间长度来近似一个区间上的积分。仅以此为基础，推导梯形法的时间步进关系式。然后，将其特化到线性测试方程\n$$\n\\frac{dy}{dt} = \\mu y, \\quad \\mu \\in \\mathbb{R},\n$$\n并从第一性原理出发，确定在假设 $y_0  0$ 的情况下，梯形法保持或不保持正性的时间步长 $h$（与 $\\mu$ 相关）所满足的条件。\n\n编写一个程序：\n- 为线性测试方程实现梯形法更新，该更新能够从 $y_0  0$ 和均匀步长 $h$ 开始，生成离散序列 $\\{y_n\\}_{n=0}^N$。\n- 对于下面的每个测试用例，返回一个布尔值，指示是否对于所有的 $n = 1,2,\\ldots,N$ 都有 $y_n  0$（每一步都严格为正）。\n\n使用以下参数集 $(\\mu, y_0, h, N)$ 的测试套件：\n- 情况 $1$：$(\\mu, y_0, h, N) = (-1, 1.0, 0.1, 10)$。\n- 情况 $2$：$(\\mu, y_0, h, N) = (-1, 1.0, 3.0, 5)$。\n- 情况 $3$：$(\\mu, y_0, h, N) = (1, 0.1, 0.5, 6)$。\n- 情况 $4$：$(\\mu, y_0, h, N) = (1, 1.0, 3.0, 1)$。\n- 情况 $5$：$(\\mu, y_0, h, N) = (-10, 0.5, 0.19, 3)$。\n\n不涉及物理单位。不涉及角度。不涉及百分比。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的布尔值列表，且无空格。例如，格式必须完全如下所示\n$$\n[{\\rm True},{\\rm False},{\\rm True},{\\rm False},{\\rm True}].\n$$", "solution": "该问题要求研究常微分方程（ODE）梯形法的保正性。此验证将分三步进行：第一步，从所提供的积分形式推导梯形法；第二步，将此方法特化到线性测试方程；第三步，推导保证维持正性的时间步长 $h$ 的条件。\n\n**第一步：推导梯形法**\n\n将微积分基本定理应用于时间区间 $[t_n, t_{n+1}]$ 上的 ODE $\\frac{dy}{dt} = f(t,y)$，可得到精确关系式：\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t))\\, dt\n$$\n问题指定使用梯形求积法则来近似积分。该法则通过由函数在区间端点的值构成的梯形面积来近似曲线下的面积。对于区间 $[a, b]$ 上的被积函数 $g(t)$，其近似为 $\\int_a^b g(t) dt \\approx \\frac{b-a}{2}(g(a) + g(b))$。\n\n将此方法应用于我们的积分，我们设定区间为 $[t_n, t_{n+1}]$，长度为 $h = t_{n+1} - t_n$，被积函数为 $f(t, y(t))$。其近似为：\n$$\n\\int_{t_n}^{t_{n+1}} f(t, y(t))\\, dt \\approx \\frac{h}{2} \\left[ f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1})) \\right]\n$$\n我们将数值近似解定义为 $y_n \\approx y(t_n)$ 和 $y_{n+1} \\approx y(t_{n+1})$。将此近似代入精确的积分关系式中，可得到梯形法的时间步进公式：\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right]\n$$\n这是一个隐式方法，因为未知值 $y_{n+1}$ 出现在方程的两边，作为右侧函数 $f$ 的一个参数。\n\n**第二步：特化到线性测试方程**\n\n线性测试方程由 $\\frac{dy}{dt} = \\mu y$ 给出，其中 $\\mu \\in \\mathbb{R}$。对于此 ODE，函数 $f(t, y)$ 简化为 $f(t, y) = \\mu y$。将 $f$ 的这种特定形式代入梯形公式，我们得到：\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left[ (\\mu y_n) + (\\mu y_{n+1}) \\right]\n$$\n为了获得一个显式更新规则，我们必须通过代数方法解出 $y_{n+1}$：\n$$\ny_{n+1} - \\frac{h\\mu}{2} y_{n+1} = y_n + \\frac{h\\mu}{2} y_n\n$$\n在左侧提出 $y_{n+1}$，在右侧提出 $y_n$，得到：\n$$\ny_{n+1} \\left(1 - \\frac{h\\mu}{2}\\right) = y_n \\left(1 + \\frac{h\\mu}{2}\\right)\n$$\n假设 $1 - \\frac{h\\mu}{2} \\neq 0$，我们可以分离出 $y_{n+1}$：\n$$\ny_{n+1} = y_n \\left( \\frac{1 + \\frac{h\\mu}{2}}{1 - \\frac{h\\mu}{2}} \\right)\n$$\n这就是将梯形法应用于线性测试方程得到的离散递推关系。括号中的项是放大因子，它决定了解如何从一个步长演化到下一个步长。\n\n**第三步：推导保正条件**\n\n如果一个方法从初始条件 $y_0  0$ 开始，所有后续的近似解 $y_n$ 都保持严格为正，则该方法保正。根据上面推导的递推关系，如果我们假设 $y_n  0$，那么 $y_{n+1}$ 的符号完全由放大因子的符号决定：\n$$\nR(h\\mu) = \\frac{1 + \\frac{h\\mu}{2}}{1 - \\frac{h\\mu}{2}}\n$$\n为了使 $y_{n+1}$ 严格为正，我们需要 $R(h\\mu)  0$。令乘积 $z = h\\mu$。该条件为：\n$$\n\\frac{1 + z/2}{1 - z/2}  0\n$$\n一个商为正，当且仅当分子和分母同号。这等价于在分母不为零的情况下，它们的乘积为正。\n$$\n\\left(1 + \\frac{z}{2}\\right) \\left(1 - \\frac{z}{2}\\right)  0 \\quad \\text{且} \\quad 1 - \\frac{z}{2} \\neq 0\n$$\n该乘积可简化为平方差：\n$$\n1 - \\left(\\frac{z}{2}\\right)^2  0\n$$\n$$\n1  \\frac{z^2}{4}\n$$\n$$\n4  z^2\n$$\n对两边取平方根，得到：\n$$\n|z|  2\n$$\n将 $z = h\\mu$ 代回，我们得到严格保正的条件：\n$$\n|h\\mu|  2\n$$\n如果 $|h\\mu| = 2$，那么 $z^2 = 4$，这意味着 $1-z^2/4=0$。这表明 $R(z)$ 的分子或分母（但不是两者同时）为零。如果 $z=2$，分母为零，导致除以零。如果 $z=-2$，分子为零，导致 $y_{n+1}=0$，这违反了严格正性。如果 $|h\\mu| > 2$，放大因子为负，导致解在每一步都改变符号，从而违反了正性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates the positivity preservation of the trapezoidal method for the\n    linear test ODE dy/dt = mu*y.\n    \"\"\"\n    # Define the test cases as tuples of (mu, y0, h, N).\n    test_cases = [\n        (-1.0, 1.0, 0.1, 10),\n        (-1.0, 1.0, 3.0, 5),\n        (1.0, 0.1, 0.5, 6),\n        (1.0, 1.0, 3.0, 1),\n        (-10.0, 0.5, 0.19, 3),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu, y0, h, N = case\n        \n        # Initialize the solution variable and the positivity flag.\n        y = np.float64(y0)\n        preserves_positivity = True\n\n        # The amplification factor for the trapezoidal method on the test equation is\n        # R = (1 + h*mu/2) / (1 - h*mu/2).\n        # We check the condition |h*mu|  2, which is equivalent to R > 0.\n        # This pre-check is sufficient, but the simulation is performed as requested.\n        \n        # Calculate the numerator and denominator of the amplification factor.\n        # Use np.float64 for precision consistent with typical scientific computing.\n        numerator = np.float64(1.0) + h * mu / np.float64(2.0)\n        denominator = np.float64(1.0) - h * mu / np.float64(2.0)\n\n        # A zero denominator means h*mu = 2, violating positivity.\n        if denominator == 0.0:\n            preserves_positivity = False\n        else:\n            amp_factor = numerator / denominator\n            \n            # If the amplification factor is not positive, positivity is violated.\n            if amp_factor = 0:\n                preserves_positivity = False\n            else:\n                # If the amplification factor is positive, y will never become non-positive\n                # starting from y0 > 0, as it's just repeated multiplication by a\n                # positive number. The simulation loop is technically redundant if we\n                # trust the analysis, but we run it to confirm step-by-step.\n                for _ in range(N):\n                    y = y * amp_factor\n                    # Check for strict positivity at each step.\n                    if y = 0.0:\n                        preserves_positivity = False\n                        break\n        \n        results.append(preserves_positivity)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3284056"}, {"introduction": "使用固定步长求解常微分方程通常效率低下，一个能根据所需精度自动调整步长的自适应求解器则要强大得多。本综合练习 [@problem_id:3284120] 将指导你从基本原理出发，构建一个完整的自适应梯形法求解器。你将学习如何通过比较不同步长的计算结果来估计局部截断误差，并利用该误差来动态控制步长，从而掌握设计高效、精确的现代数值求解器的核心思想。", "problem": "设计并实现一个用于隐式梯形法的自适应步长常微分方程积分器，该积分器可在不使用下文允许的标准科学计算工具之外的任何外部包的情况下控制局部截断误差。积分器必须从初值问题的积分形式和用于数值积分的经典梯形法则推导得出。步长控制器必须基于局部截断误差的估计，该估计通过比较一个完整步长与两个半步长的结果，并从相容性和局部截断误差的基本定义出发，对误差阶进行推断而得出。该算法必须针对形式为 $y^{\\prime}(t) = f(t,y)$ 且在有限区间 $[t_{0},T]$ 上满足 $y(t_{0}) = y_{0}$ 的标量初值问题进行实现。每一步的隐式方程必须通过基于第一性原理的求根迭代来求解。您不能假设精确的雅可比矩阵可用；如果需要导数，您必须构造一个数值上可靠的有限差分近似。\n\n您的推导必须仅基于以下出发点：\n- 初值问题的积分形式：$y(t_{n+1}) - y(t_{n}) = \\int_{t_{n}}^{t_{n+1}} f(t,y(t))\\,dt$。\n- 单个子区间上定积分的复合梯形法则：$\\int_{a}^{b} g(t)\\,dt \\approx \\tfrac{b-a}{2}\\big(g(a)+g(b)\\big)$，在适当的光滑性条件下，其局部积分误差阶为 $O((b-a)^{3})$。\n- 局部截断误差、全局误差阶的定义，以及通过步长加倍和 Richardson 型推断思想，利用不同步长下的近似值来构造误差估计的方法。\n\n您的程序必须实现：\n- 一个从积分形式和梯形法则推导出的隐式梯形单步更新。所得到的关于新值的非线性方程必须使用基于求根的、有原则的迭代方法求解，其停止准则需与迭代值的大小成比例。\n- 一个自适应步长控制器，它能：\n  1. 通过比较大小为 $h$ 的单步与两个大小为 $h/2$ 的连续步，构造一个可计算的局部误差估计。\n  2. 使用隐式梯形法的已知局部误差阶来归一化该差异，并生成一个单步误差指标。\n  3. 基于用户指定的绝对和相对容差，使用 $\\text{atol} + \\text{rtol}\\cdot\\max(\\lvert y_{\\text{coarse}}\\rvert,\\lvert y_{\\text{fine}}\\rvert)$ 形式的尺度，接受或拒绝一个尝试步。\n  4. 使用从局部误差阶推导出的幂律规则，并结合一个安全因子以及步长增长和缩小的上下界，来更新下一步的步长。\n\n实现约束和要求：\n- 仅限标量问题。\n- 每一步的隐式求解必须使用带有有限差分导数的求根迭代，其停止阈值需与 $\\epsilon \\cdot \\max(1,\\lvert y\\rvert)$ 成比例，其中 $\\epsilon$ 是与所要求的精度相关的一个小量。\n- 如果对于某个尝试步长，隐式求解未能收敛，算法必须稳健地减小步长并重试。\n- 强制执行最小和最大步长 $h_{\\min}$ 和 $h_{\\max}$，并确保在必要时通过截断最后一步来精确到达 $T$。\n- 角度（如适用）必须以弧度为单位。\n- 所有输出必须是无量纲的实数。\n\n用于实现和评估的测试套件：\n- 案例 $\\mathbf{1}$ (稳定线性衰减): $f(t,y) = -y$, $t_{0} = 0$, $y_{0} = 1$, $T = 5$, 绝对容差 $\\text{atol} = 10^{-12}$, 相对容差 $\\text{rtol} = 10^{-10}$, 初始步长 $h_{0} = 5\\times 10^{-1}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$。\n- 案例 $\\mathbf{2}$ (刚性但可精确求解): $f(t,y) = -1000\\,(y - e^{-t}) - e^{-t}$, $t_{0} = 0$, $y_{0} = 1$, $T = 1$, 绝对容差 $\\text{atol} = 10^{-10}$, 相对容差 $\\text{rtol} = 10^{-8}$, 初始步长 $h_{0} = 10^{-2}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$。\n- 案例 $\\mathbf{3}$ (非线性逻辑斯蒂增长): $f(t,y) = y\\,(1-y)$, $t_{0} = 0$, $y_{0} = 10^{-1}$, $T = 5$, 绝对容差 $\\text{atol} = 10^{-12}$, 相对容差 $\\text{rtol} = 10^{-10}$, 初始步长 $h_{0} = 5\\times 10^{-1}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$。\n- 案例 $\\mathbf{4}$ (带衰减的受迫线性振子): $f(t,y) = \\cos(t) - y$, $t_{0} = 0$, $y_{0} = 0$, $T = 10$, 绝对容差 $\\text{atol} = 10^{-12}$, 相对容差 $\\text{rtol} = 10^{-10}$, 初始步长 $h_{0} = 5\\times 10^{-1}$, $h_{\\min} = 10^{-12}$, $h_{\\max} = 1$。\n\n需要计算和返回的内容：\n- 对每个案例，从 $t_{0}$ 积分到 $T$，并以实数形式返回对 $y(T)$ 的数值近似。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $\\texttt{[result1,result2,result3,result4]}$），结果按案例 $\\mathbf{1}$, $\\mathbf{2}$, $\\mathbf{3}$, $\\mathbf{4}$ 的顺序列出。每个条目必须是一个浮点数。", "solution": "该问题被评估为有效。它在常微分方程（ODE）数值方法领域内是一个适定的、有科学依据的、客观的任务。说明书为基于隐式梯形法的自适应步长积分器的设计和实现提供了清晰而完整的规范，包括推导、隐式求解器、自适应控制逻辑以及一整套测试案例。\n\n以下是根据问题陈述所作的推导和算法设计。\n\n### 1. 隐式梯形法\n\n初值问题（IVP）定义为 $y^{\\prime}(t) = f(t,y)$，初始条件为 $y(t_0) = y_0$。在一个从 $t_n$到 $t_{n+1}$ 的单步上，解可以表示为积分形式：\n$$\ny(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt\n$$\n令步长为 $h = t_{n+1} - t_n$。我们使用梯形法则 $\\int_{a}^{b} g(t)\\,dt \\approx \\tfrac{b-a}{2}\\big(g(a)+g(b)\\big)$ 来近似该积分。将此应用于 $f(t, y(t))$ 的积分，得到：\n$$\n\\int_{t_n}^{t_{n+1}} f(t, y(t)) \\, dt \\approx \\frac{h}{2} \\left[ f(t_n, y(t_n)) + f(t_{n+1}, y(t_{n+1})) \\right]\n$$\n将此代入积分形式，得到用于近似值 $y_n \\approx y(t_n)$ 的单步数值方法：\n$$\ny_{n+1} = y_n + \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right]\n$$\n这就是隐式梯形法则。它之所以是“隐式”的，是因为未知值 $y_{n+1}$ 出现在方程的两边。为了计算 $y_{n+1}$，我们必须求解一个（通常是非线性的）代数方程。\n\n### 2. 使用牛顿法求解隐式方程\n\n为了找到 $y_{n+1}$，我们必须求函数 $G(y_{n+1})$ 的根：\n$$\nG(y_{n+1}) = y_{n+1} - y_n - \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y_{n+1}) \\right] = 0\n$$\n牛顿法是一种合适的迭代求根算法。给定一个猜测值 $y_{n+1}^{(k)}$，下一次迭代为：\n$$\ny_{n+1}^{(k+1)} = y_{n+1}^{(k)} - \\frac{G(y_{n+1}^{(k)})}{G'(y_{n+1}^{(k)})}\n$$\n$G$ 对其自变量（我们记为 $y^* \\equiv y_{n+1}$）的导数是：\n$$\nG'(y^*) = \\frac{d}{dy^*} \\left( y^* - y_n - \\frac{h}{2} \\left[ f(t_n, y_n) + f(t_{n+1}, y^*) \\right] \\right) = 1 - \\frac{h}{2} \\frac{\\partial f}{\\partial y}(t_{n+1}, y^*)\n$$\n由于不假设雅可比矩阵 $\\frac{\\partial f}{\\partial y}$ 可解析获得，我们使用前向有限差分来近似它。函数 $f$ 在点 $(t, y)$ 处对 $y$ 的导数估计如下：\n$$\n\\frac{\\partial f}{\\partial y}(t, y) \\approx \\frac{f(t, y + \\delta) - f(t, y)}{\\delta}\n$$\n扰动量 $\\delta$ 的一个数值上可靠的选择取决于机器精度 $\\epsilon_{\\text{mach}}$ 和 $y$ 的尺度。我们使用 $\\delta = \\sqrt{\\epsilon_{\\text{mach}}} \\cdot \\max(1, |y|)$。\n\n迭代从一个初始猜测开始，例如 $y_{n+1}^{(0)} = y_n$。当更新步长的大小 $|y_{n+1}^{(k+1)} - y_{n+1}^{(k)}|$ 小于某个阈值 $\\epsilon_{\\text{Newton}} \\cdot \\max(1, |y_{n+1}^{(k+1)}|)$ 时，过程终止。其中 $\\epsilon_{\\text{Newton}}$ 是一个很小的容差（例如 $10^{-14}$），用以确保代数方程被高精度求解。如果在最大迭代次数内未达到收敛，则该过程失败，这表明步长 $h$ 太大。\n\n### 3. 自适应步长控制\n\n步长 $h$ 被自适应调整以控制每步的局部误差。这是通过比较走一个大步与走两个小步的结果来实现的。\n\n1.  **步长计算**：从 $(t_n, y_n)$ 出发，我们计算解在 $t_n + h$ 处的两个近似值：\n    -   $y_{\\text{coarse}}$：大小为 $h$ 的一个梯形步的结果。\n    -   $y_{\\text{fine}}$：两个连续的、大小各为 $h/2$ 的梯形步的结果。\n\n2.  **误差估计**：梯形法的全局误差阶为 $O(h^2)$，局部误差阶为 $O(h^3)$。对于从一个精确值开始的单步，其误差 $e(h)$ 为 $e(h) = y(t_n+h) - y_{\\text{step}}(h) \\approx C h^3$。\n    -   粗略步的误差为 $E_{\\text{coarse}} \\approx C h^3$。因此，$y_{\\text{coarse}} \\approx y(t_n+h) - C h^3$。\n    -   两个精细步的误差是局部误差之和：$E_{\\text{fine}} \\approx 2 \\cdot C (h/2)^3 = C h^3 / 4$。因此，$y_{\\text{fine}} \\approx y(t_n+h) - C h^3 / 4$。\n    \n    将这两个近似值相减得到：\n    $$\n    y_{\\text{fine}} - y_{\\text{coarse}} \\approx \\left(y(t_n+h) - \\frac{Ch^3}{4}\\right) - \\left(y(t_n+h) - Ch^3\\right) = \\frac{3}{4}Ch^3\n    $$\n    更精确的精细解中的误差 $E_{\\text{fine}} \\approx C h^3/4$，可以从计算值中估计出来：\n    $$\n    \\text{err\\_est} = |E_{\\text{fine}}| \\approx \\frac{1}{3} |y_{\\text{fine}} - y_{\\text{coarse}}|\n    $$\n\n3.  **步长接受/拒绝**：基于用户指定的绝对容差 (`atol`) 和相对容差 (`rtol`) 计算一个容差 `tol`：\n    $$\n    \\text{tol} = \\text{atol} + \\text{rtol} \\cdot \\max(|y_{\\text{coarse}}|, |y_{\\text{fine}}|)\n    $$\n    如果 $\\text{err\\_est} \\le \\text{tol}$，则接受该步。若接受，则使用更精确的结果将解推进到 $(t_{n+1}, y_{\\text{fine}})$（这是一种称为局部外推的技术）。若拒绝，则使用更小的 $h$ 重试该步。\n\n4.  **新步长选择**：计算一个新的步长 $h_{\\text{new}}$，目标是在下一次尝试中使误差等于容差。由于 $\\text{err\\_est} \\propto h^3$，我们有：\n    $$\n    \\frac{\\text{tol}}{\\text{err\\_est}} \\approx \\frac{K h_{\\text{new}}^3}{K h^3} \\implies h_{\\text{new}} = h \\left( \\frac{\\text{tol}}{\\text{err\\_est}} \\right)^{1/3}\n    $$\n    为了保守起见，引入一个安全因子 $S  1$（例如 $S=0.9$）。步长的增长和缩小也受到乘法因子（例如，介于 $0.2$ 和 $5.0$ 之间）的限制。最终提议的步长被限制在指定的最小值 ($h_{\\text{min}}$) 和最大值 ($h_{\\text{max}}$) 之间。\n\n### 4. 算法摘要\n\n积分器按以下步骤进行：\n- 初始化 $t=t_0, y=y_0, h=h_0$。\n- 当 $t  T$ 时循环：\n    a. 如果 $t+h > T$，则截断 $h$ 以精确地落在 $T$ 上。\n    b. 进入一个内部循环以寻找可接受的步长：\n        i. 尝试计算 $y_{\\text{coarse}}$（大小为 $h$ 的一步）和 $y_{\\text{fine}}$（大小为 $h/2$ 的两步）。这涉及三次对牛顿求解器的调用。\n        ii. 如果任何一次牛顿求解失败，则减小 $h$（例如，减半），确保 $h \\ge h_{\\text{min}}$，并重新启动内部循环。\n        iii. 计算 `err_est` 和 `tol`。\n        iv. 如果 $\\text{err\\_est} \\le \\text{tol}$（步长被接受）：\n            - 推进状态：$t \\leftarrow t+h$, $y \\leftarrow y_{\\text{fine}}$。\n            - 基于误差比计算*下一个*步长的 $h_{\\text{new}}$。\n            - 设置 $h \\leftarrow h_{\\text{new}}$ 并跳出内部循环。\n        v. 如果 $\\text{err\\_est} > \\text{tol}$（步长被拒绝）：\n            - 基于误差比计算 $h_{\\text{new}}$（它会更小）。\n            - 设置 $h \\leftarrow h_{\\text{new}}$，确保 $h \\ge h_{\\text{min}}$，并继续内部循环以重试当前步。\n- 返回 $y$ 的最终值。\n这构成了一个根据要求从第一性原理构建的稳健、自适应的 ODE 求解器。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef trapezoid_step_solver(f, t_n, y_n, h):\n    \"\"\"\n    Solves for a single step of the implicit trapezoidal method using Newton's method.\n    y_{n+1} = y_n + h/2 * (f(t_n, y_n) + f(t_{n+1}, y_{n+1}))\n    \"\"\"\n    t_np1 = t_n + h\n    fn_val = f(t_n, y_n)\n    \n    # Newton's method parameters\n    newton_rtol = 1e-14\n    max_iter = 10\n    \n    # Initial guess for y_{n+1}\n    y_guess = y_n\n    \n    for _ in range(max_iter):\n        f_np1_guess = f(t_np1, y_guess)\n        \n        # G(y_guess) = 0 is the equation to solve.\n        G = y_guess - y_n - h / 2.0 * (fn_val + f_np1_guess)\n        \n        # Check for convergence of the residual G itself\n        if abs(G)  1e-15:\n            return y_guess, True\n\n        # Finite difference for the derivative of f w.r.t y\n        mach_eps = np.finfo(float).eps\n        delta = np.sqrt(mach_eps) * max(1.0, abs(y_guess))\n        dfdy = (f(t_np1, y_guess + delta) - f_np1_guess) / delta\n        \n        # Derivative of G w.r.t y_guess\n        dG_dy = 1.0 - h / 2.0 * dfdy\n        \n        if abs(dG_dy)  1e-15:\n            # Jacobian is singular, Newton's method fails.\n            return y_guess, False\n        \n        # Newton update\n        update = -G / dG_dy\n        y_guess += update\n        \n        # Check for convergence based on update size\n        newton_tol = newton_rtol * max(1.0, abs(y_guess))\n        if abs(update)  newton_tol:\n            return y_guess, True\n            \n    # If loop finishes, convergence failed.\n    return y_guess, False\n\ndef adaptive_trapezoid_solver(f, t0, y0, T, atol, rtol, h0, h_min, h_max):\n    \"\"\"\n    An adaptive step size ODE integrator using the implicit trapezoidal method.\n    \"\"\"\n    t = float(t0)\n    y = float(y0)\n    h = float(h0)\n\n    # Controller parameters\n    safety = 0.9\n    min_factor = 0.2\n    max_factor = 5.0\n    \n    while t  T:\n        if t + h > T:\n            h = T - t\n        \n        # Ensure h does not fall below h_min, except for the final step.\n        h = max(h, h_min)\n\n        while True: # Inner loop to find an acceptable step\n            # Coarse step\n            y_coarse, coarse_ok = trapezoid_step_solver(f, t, y, h)\n            if not coarse_ok:\n                h = max(h * 0.5, h_min)\n                if h == h_min and t + h  T:\n                     raise RuntimeError(f\"Newton solver failed at t={t} with minimum step size h={h}.\")\n                continue\n\n            # Fine steps (two half-steps)\n            y_mid, mid_ok = trapezoid_step_solver(f, t, y, h / 2.0)\n            if not mid_ok:\n                h = max(h * 0.5, h_min)\n                if h == h_min and t + h  T:\n                     raise RuntimeError(f\"Newton solver failed at t={t} with minimum step size h={h}.\")\n                continue\n            \n            y_fine, fine_ok = trapezoid_step_solver(f, t + h / 2.0, y_mid, h / 2.0)\n            if not fine_ok:\n                h = max(h * 0.5, h_min)\n                if h == h_min and t + h  T:\n                     raise RuntimeError(f\"Newton solver failed at t={t} with minimum step size h={h}.\")\n                continue\n\n            # Error estimation\n            error_norm = abs(y_fine - y_coarse)\n            err_est = error_norm / 3.0 # Based on local error O(h^3)\n                                       # Error_fine = 1/3 * (y_fine - y_coarse)\n            \n            # Tolerance scale\n            y_scale = max(abs(y_coarse), abs(y_fine))\n            tol = atol + rtol * y_scale\n            \n            # Step size update logic\n            if err_est = tol:\n                # Step accepted\n                t += h\n                y = y_fine # Local extrapolation\n                \n                # Calculate next step size\n                if error_norm == 0.0:\n                    factor = max_factor\n                else:\n                    factor = safety * (tol / err_est)**(1.0/3.0)\n                    factor = min(max_factor, max(min_factor, factor))\n                \n                h_new = h * factor\n                h = min(h_max, max(h_min, h_new))\n                \n                break # Exit inner loop, proceed to next step\n            else:\n                # Step rejected, reduce step size and retry\n                factor = safety * (tol / err_est)**(1.0/3.0)\n                factor = min(max_factor, max(min_factor, factor)) # factor should be  1\n                \n                h_new = h * factor\n                h = max(h_min, h_new)\n                # Continue inner loop with smaller h\n                if h == h_min:\n                    # If on minimum step, must accept to move forward\n                    t += h\n                    y = y_fine\n                    break\n\n\n    return y\n\ndef solve():\n    # Test cases\n    # Case 1: Stable linear decay\n    f1 = lambda t, y: -y\n    case1 = {'f': f1, 't0': 0.0, 'y0': 1.0, 'T': 5.0, \n             'atol': 1e-12, 'rtol': 1e-10, 'h0': 0.5, 'h_min': 1e-12, 'h_max': 1.0}\n\n    # Case 2: Stiff but exactly solvable\n    f2 = lambda t, y: -1000.0 * (y - np.exp(-t)) - np.exp(-t)\n    case2 = {'f': f2, 't0': 0.0, 'y0': 1.0, 'T': 1.0,\n             'atol': 1e-10, 'rtol': 1e-8, 'h0': 0.01, 'h_min': 1e-12, 'h_max': 1.0}\n\n    # Case 3: Nonlinear logistic growth\n    f3 = lambda t, y: y * (1.0 - y)\n    case3 = {'f': f3, 't0': 0.0, 'y0': 0.1, 'T': 5.0,\n             'atol': 1e-12, 'rtol': 1e-10, 'h0': 0.5, 'h_min': 1e-12, 'h_max': 1.0}\n    \n    # Case 4: Forced linear oscillator with decay\n    f4 = lambda t, y: np.cos(t) - y\n    case4 = {'f': f4, 't0': 0.0, 'y0': 0.0, 'T': 10.0,\n             'atol': 1e-12, 'rtol': 1e-10, 'h0': 0.5, 'h_min': 1e-12, 'h_max': 1.0}\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        y_T = adaptive_trapezoid_solver(**case)\n        results.append(y_T)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3284120"}]}