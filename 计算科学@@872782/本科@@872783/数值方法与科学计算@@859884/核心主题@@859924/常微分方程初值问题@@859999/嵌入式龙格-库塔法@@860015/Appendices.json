{"hands_on_practices": [{"introduction": "嵌入式龙格-库塔方法的核心在于其系数（即布彻表）。但这些系数从何而来？本练习将引导你通过匹配泰勒级数展开，从第一性原理推导出方法达到特定阶数所需的“阶条件”，从而亲手构建一个简单的二阶/一阶嵌入式方法对 [@problem_id:3224413]。这个过程将揭示这些数值方法设计的内在逻辑。", "problem": "考虑一个常微分方程的初值问题，$y^{\\prime}(t) = f(t,y(t))$，初值为 $y(t_{0}) = y_{0}$，其中函数 $f$ 足够光滑，可以对其两个参数进行泰勒展开。您需要设计一个两级显式嵌入式龙格-库塔(RK)方法，其主更新为 $2$ 阶，嵌入式更新为 $1$ 阶。嵌入式龙格-库塔方法（以龙格-库塔-费尔伯格(RKF)族为代表）通过共享的级计算提供两个不同阶的近似值，以估计局部截断误差。\n\n设两级显式RK格式由级斜率\n$$\nk_{1} = f(t_{n},y_{n}), \\quad\nk_{2} = f\\!\\big(t_{n} + c_{2} h, \\; y_{n} + a_{21} h k_{1}\\big),\n$$\n和主更新\n$$\ny_{n+1} = y_{n} + h\\!\\left(b_{1} k_{1} + b_{2} k_{2}\\right),\n$$\n以及嵌入式更新\n$$\n\\widehat{y}_{n+1} = y_{n} + h\\!\\left(\\widehat{b}_{1} k_{1} + \\widehat{b}_{2} k_{2}\\right)\n$$\n所定义。\n从基本原理出发，即精确解 $y(t_{n}+h)$ 在 $(t_{n},y_{n})$ 点的泰勒展开，以及 $f(t_{n}+c_{2} h,\\; y_{n}+a_{21} h k_{1})$ 在 $(t_{n},y_{n})$ 点的链式法则展开，推导主更新达到 $2$ 阶和嵌入式更新达到 $1$ 阶时系数必须满足的阶条件。施加显式RK级的标准内部一致性条件，并要求所有系数均为实数且非负。\n\n然后，选择 $c_{2}$ 为与所推导的阶条件和非负性要求一致的最小正整数，并唯一确定系数 $a_{21}$、$c_{2}$、$b_{1}$、$b_{2}$、$\\widehat{b}_{1}$ 和 $\\widehat{b}_{2}$。\n\n以单行矩阵的形式提供您的最终答案，顺序为 $\\big(a_{21},\\, c_{2},\\, b_{1},\\, b_{2},\\, \\widehat{b}_{1},\\, \\widehat{b}_{2}\\big)$。无需四舍五入。", "solution": "问题要求推导一个两级显式嵌入式龙格-库塔方法的系数。主方法必须为 $2$ 阶，嵌入式方法为 $1$ 阶。我们首先对精确解和数值近似进行泰勒级数展开。\n\n设初值问题为 $y^{\\prime}(t) = f(t,y(t))$，初值为 $y(t_n) = y_n$。精确解 $y(t)$ 在 $t_n$ 附近的泰勒展开为：\n$$\ny(t_n + h) = y(t_n) + h y^{\\prime}(t_n) + \\frac{h^2}{2} y^{\\prime\\prime}(t_n) + O(h^3)\n$$\n我们用 $f$ 及其在 $(t_n, y_n)$ 处的偏导数来表示 $y$ 的导数。为简洁起见，我们将 $f(t_n, y_n)$ 记为 $f$，$\\frac{\\partial f}{\\partial t}(t_n, y_n)$ 记为 $f_t$，$\\frac{\\partial f}{\\partial y}(t_n, y_n)$ 记为 $f_y$。\n一阶导数很简单，$y^{\\prime}(t_n) = f(t_n, y_n) = f$。\n二阶导数通过链式法则得到：\n$$\ny^{\\prime\\prime}(t) = \\frac{d}{dt}f(t, y(t)) = \\frac{\\partial f}{\\partial t} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} = f_t + f_y f\n$$\n将这些代入泰勒展开式，得到精确解直到 $h^2$ 阶的表达式：\n$$\ny(t_n+h) = y_n + hf + \\frac{h^2}{2}(f_t + f_y f) + O(h^3)\n$$\n接下来，我们展开数值近似。两个级值由以下公式给出：\n$$\nk_1 = f(t_n, y_n) = f\n$$\n$$\nk_2 = f(t_n + c_2 h, y_n + a_{21} h k_1) = f(t_n + c_2 h, y_n + a_{21} h f)\n$$\n我们使用多变量泰勒级数在 $(t_n, y_n)$ 点展开 $k_2$：\n$$\nk_2 = f(t_n, y_n) + (c_2 h) f_t + (a_{21} h f) f_y + O(h^2) = f + c_2 h f_t + a_{21} h f f_y + O(h^2)\n$$\n主、二阶更新为 $y_{n+1} = y_n + h(b_1 k_1 + b_2 k_2)$。代入 $k_1$ 和 $k_2$ 的展开式：\n$$\ny_{n+1} = y_n + h(b_1 f + b_2 [f + c_2 h f_t + a_{21} h f f_y + O(h^2)])\n$$\n$$\ny_{n+1} = y_n + (b_1 + b_2)hf + (b_2 c_2)h^2 f_t + (b_2 a_{21})h^2 f f_y + O(h^3)\n$$\n为使 $y_{n+1}$ 达到 $2$ 阶，其展开式必须与 $y(t_n+h)$ 的展开式在 $h^2$ 项之前都匹配。通过比较包含 $f$、$f_t$ 和 $f_y f$ 的项的系数，我们得到以下阶条件：\n\\begin{itemize}\n    \\item $h$ 阶：$hf$ 的系数必须匹配。\n    $$b_1 + b_2 = 1$$\n    \\item $h^2$ 阶：$h^2 f_t$ 和 $h^2 f f_y$ 的系数必须匹配。\n    $$b_2 c_2 = \\frac{1}{2}$$\n    $$b_2 a_{21} = \\frac{1}{2}$$\n\\end{itemize}\n嵌入式、一阶更新为 $\\widehat{y}_{n+1} = y_n + h(\\widehat{b}_1 k_1 + \\widehat{b}_2 k_2)$。我们类似地展开此式：\n$$\n\\widehat{y}_{n+1} = y_n + h(\\widehat{b}_1 f + \\widehat{b}_2 [f + O(h)]) = y_n + (\\widehat{b}_1 + \\widehat{b}_2)hf + O(h^2)\n$$\n为使 $\\widehat{y}_{n+1}$ 达到 $1$ 阶，其展开式必须与 $y(t_n+h)$ 的展开式在 $h^1$ 项之前都匹配。这产生一个阶条件：\n$$\n\\widehat{b}_1 + \\widehat{b}_2 = 1\n$$\n我们现在利用提供的附加约束来求解系数。两级显式RK方法的标准内部一致性条件是 $c_2 = a_{21}$。从 $2$ 阶条件中，我们看到 $b_2 c_2 = b_2 a_{21}$。假设这是一个真正的两级方法，其中第二级有影响，我们必须有 $b_2 \\neq 0$。因此，$c_2 = a_{21}$ 自动满足。问题指定 $c_2$ 是最小的正整数，所以我们设 $c_2 = 1$。通过这个选择，主方法的系数被唯一确定：\n由 $c_2=1$，一致性条件给出 $a_{21} = 1$。\n由 $b_2 c_2 = 1/2$，我们有 $b_2(1) = 1/2$，这给出 $b_2 = 1/2$。\n由 $b_1 + b_2 = 1$，我们有 $b_1 + 1/2 = 1$，这给出 $b_1 = 1/2$。\n这些系数（$a_{21}=1$，$c_2=1$，$b_1=1/2$，$b_2=1/2$）都按要求为非负。这定义了二阶方法，通常称为显式中点法或Heun方法。\n\n对于嵌入式方法，我们有条件 $\\widehat{b}_1 + \\widehat{b}_2 = 1$ 和非负性约束 $\\widehat{b}_1 \\ge 0$ 和 $\\widehat{b}_2 \\ge 0$。这个系统是欠定的。然而，问题指出系数必须被唯一确定。这意味着我们必须援引方法设计中的一个标准原则，即将低阶方法构建得尽可能简单。一个 $1$ 阶方法仅用一级计算（$k_1$）即可构建。这对应于前向欧拉法，$\\widehat{y}_{n+1} = y_n + h k_1$。为实现这种结构，我们必须将第二级的系数 $\\widehat{b}_2$ 设为零。这个选择使解唯一。\n设 $\\widehat{b}_2 = 0$。根据阶条件 $\\widehat{b}_1 + \\widehat{b}_2 = 1$，我们得到 $\\widehat{b}_1 = 1$。\n这些系数都是非负的。\n\n因此，整套唯一确定的系数是：\n$a_{21} = 1$\n$c_2 = 1$\n$b_1 = 1/2$\n$b_2 = 1/2$\n$\\widehat{b}_1 = 1$\n$\\widehat{b}_2 = 0$\n我们以指定的矩阵格式呈现这些系数。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  \\frac{1}{2}  \\frac{1}{2}  1  0\n\\end{pmatrix}\n}\n$$", "id": "3224413"}, {"introduction": "嵌入式方法最强大的应用在于自适应步长控制，它能根据问题的“难度”动态调整计算量。本练习要求你实现一个经典的RKF45求解器，并用它来解决几个典型的常微分方程 [@problem_id:3224450]。通过这个过程，你将直观地看到求解器如何工作，并凭经验验证误差容限与所需计算步数之间的理论关系，这是衡量自适应算法效率的关键指标。", "problem": "考虑一个常微分方程（ODE）的初值问题，定义为 $y'(t) = f(t,y)$，其中 $y(t_0) = y_0$ 且 $f$ 足够光滑。实现一个基于 $4(5)$ 阶嵌入式 Runge–Kutta–Fehlberg (RKF) 方法的自适应步长求解器，其中误差估计器的阶数为 $p$，并随步长 $h$ 按 $h^p$ 比例缩放。使用高阶解来推进状态，并使用嵌入式差分来估计每一步的局部误差。步长应进行自适应调整，使得局部误差估计值约等于一个预设的标量容差 $\\,\\text{tol}\\,$，并设有适当的安全系数和界限以确保稳定的求解进程。\n\n根据单步法局部截断误差的基本定义，以及局部误差估计 $E$ 的行为符合 $E \\approx C h^p$（其中 $C$ 是一个与问题相关的常数）的标准假设，可以推断：选择 $h$ 以维持 $E \\approx \\text{tol}$ 会导致 $h \\propto \\text{tol}^{1/p}$，因此在固定区间上，总接受步数 $N$ 的缩放关系为 $N \\propto \\text{tol}^{-1/p}$。你的任务是使用嵌入式 Runge–Kutta–Fehlberg 方法通过实验来研究此缩放关系。\n\n实现该求解器并执行以下测试组。对于每种情况，计算容差集合 $\\{10^{-2},10^{-3},10^{-4},10^{-5},10^{-6}\\}$ 中每个容差对应的总接受步数 $N$，然后基于这五个点对 $\\log N$ 与 $\\log \\text{tol}$ 进行线性拟合，估计其斜率 $s$。将 $s$ 与理论值 $-1/p$ 进行比较，并报告是否满足 $\\lvert s - (-1/p)\\rvert \\le \\epsilon$，其中 $\\epsilon$ 是一个预设的阈值。\n\n对于 Runge–Kutta–Fehlberg $4(5)$ 嵌入式对，取 $p=5$，并使用 $\\epsilon = 0.15$。常微分方程、初始条件和区间如下：\n\n- 情况 A（光滑指数衰减）：$f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 10$。\n- 情况 B（中度振荡强迫项）：$f(t,y) = \\cos(10 t)$, $t_0 = 0$, $y_0 = 0$, $t_{\\text{end}} = 10$。\n- 情况 C（远离奇点的非线性增长）：$f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 0.9$。\n\n对于每种情况，生成一个布尔值，指示测量到的斜率 $s$ 是否在理论值 $-1/p$ 的 $\\epsilon$ 范围之内。你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表，该列表按 A、B、C 的顺序包含三个布尔结果（例如，`[{\\tt True},{\\tt False},{\\tt True}]`）。此问题不涉及物理单位，如果出现角度，则默认为弧度；此处无需进行角度转换。", "solution": "用户要求实现一个 $4(5)$ 阶的自适应 Runge-Kutta-Fehlberg 方法，以通过实验验证积分步数与预设误差容差之间的理论缩放关系。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **常微分方程**：$y'(t) = f(t,y)$，初始条件为 $y(t_0) = y_0$。\n*   **方法**：$4(5)$ 阶嵌入式 Runge-Kutta-Fehlberg (RKF) 方法。\n*   **误差估计器**：阶数 $p=5$，误差估计 $E$ 按 $E \\approx C h^p$ 缩放。\n*   **状态推进**：使用高阶（5 阶）解来推进状态。\n*   **步长控制**：自适应调整步长 $h$，使局部误差估计约等于标量容差 $\\text{tol}$。\n*   **理论缩放关系**：总接受步数 $N$ 预计按 $N \\propto \\text{tol}^{-1/p}$ 缩放。\n*   **任务**：对于三个特定的测试用例，计算 $\\text{tol} \\in \\{10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}\\}$ 中每个容差对应的 $N$。\n*   **分析**：对 $\\log N$ 与 $\\log \\text{tol}$ 进行线性拟合，估计斜率 $s$。\n*   **验证**：检查是否满足条件 $\\lvert s - (-1/p)\\rvert \\le \\epsilon$。\n*   **参数**：$p=5$, $\\epsilon = 0.15$。\n*   **测试用例**：\n    *   **情况 A**：$f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 10$。\n    *   **情况 B**：$f(t,y) = \\cos(10 t)$, $t_0 = 0$, $y_0 = 0$, $t_{\\text{end}} = 10$。\n    *   **情况 C**：$f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $t_{\\text{end}} = 0.9$。\n*   **输出**：一个包含三个布尔值的列表，对应每个用例的验证结果。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n1.  **科学依据**：该问题是求解常微分方程的数值分析中的一个标准练习。Runge-Kutta-Fehlberg 方法和自适应步长控制理论是科学计算中的基本概念。该问题是完善且科学合理的。\n2.  **良态性**：该问题定义清晰。常微分方程、初始条件、积分区间和容差都已明确指定。实现求解器、收集数据、执行线性回归以及根据标准检查结果的任务是明确无误的。\n3.  **客观性**：语言精确，没有主观或基于观点的陈述。评估标准是定量的客观测试。\n4.  **完整性和一致性**：问题提供了继续进行所需的所有必要信息。虽然没有给出 RKF45 方法的具体 Butcher 表，但这是一套标准的、有据可查的系数，可以从数值分析的权威参考文献中轻易获得。误差估计器的阶数 $p=5$ 与一个 $4(5)$ 对是一致的，其中正在估计的是 4 阶方法的误差。该问题是自洽且完整的。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。这是一个来自数值方法领域的、良态的、有科学依据的问题。将提供完整的解决方案。\n\n### 基于原理的设计与求解\n\n这个问题的核心是验证自适应步长常微分方程（ODE）求解器的一个基本性质。我们给定一个以下形式的初值问题（IVP）：\n$$ y'(t) = f(t, y), \\quad y(t_0) = y_0 $$\n其中 $y(t)$ 可以是标量或向量。这里提供的问题都是标量问题。\n\n**1. 嵌入式 Runge-Kutta 方法**\n\n嵌入式 Runge-Kutta 方法使用两个不同阶的单步公式在每一步中逼近解。Runge-Kutta-Fehlberg $4(5)$ 方法计算两个估计值：一个四阶精度解 $y_{n+1}^{(4)}$ 和一个五阶精度解 $y_{n+1}^{(5)}$。四阶方法的局部截断误差（LTE）为 $\\mathcal{O}(h^5)$，而五阶方法的 LTE 为 $\\mathcal{O}(h^6)$。\n\n这两个估计值之间的差值为低阶解的误差提供了一个高效的近似：\n$$ E_{n+1} = \\| y_{n+1}^{(5)} - y_{n+1}^{(4)} \\| \\approx \\| y(t_{n+1}) - y_{n+1}^{(4)} \\| $$\n这个误差估计 $E_{n+1}$ 的阶数为 $p=5$，意味着它随步长 $h$ 按 $E_{n+1} \\approx C h^5$ 变化，其中 $C$ 是一个依赖于解的导数的常数。问题指定应使用高阶解 $y_{n+1}^{(5)}$ 来推进状态，这是一种称为局部外推法的技术，通常能提高精度。\n\nRKF45 方法每一步需要六次函数求值（阶段 $k_1, \\dots, k_6$）来计算两个解。公式如下：\n$$ k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right) $$\n$$ y_{n+1}^{(4)} = y_n + h \\sum_{i=1}^{6} b_i k_i $$\n$$ y_{n+1}^{(5)} = y_n + h \\sum_{i=1}^{6} b_i^* k_i $$\n系数 $c_i$, $a_{ij}$, $b_i$ 和 $b_i^*$ 定义了具体的方法。对于此实现，将使用由 Fehlberg 推导出的标准系数。\n\n**2. 自适应步长控制**\n\n自适应求解器的目标是调整步长 $h$，以将局部误差估计 $E$ 维持在指定的容差 $\\text{tol}$ 以下。假设误差按 $E \\approx C h^p$ 缩放，我们可以推导出选择下一步长的规则。如果一个步长 $h_{old}$ 产生了误差 $E_{old}$，我们希望找到一个新的步长 $h_{new}$，它能产生约等于 $\\text{tol}$ 的误差。\n$$ \\frac{E_{old}}{\\text{tol}} \\approx \\frac{C h_{old}^p}{C h_{new}^p} = \\left(\\frac{h_{old}}{h_{new}}\\right)^p $$\n解出 $h_{new}$ 得到更新公式：\n$$ h_{new} = h_{old} \\left( \\frac{\\text{tol}}{E_{old}} \\right)^{1/p} $$\n为确保稳定性并防止步长变化过于激进，引入一个安全系数 $S  1$（通常 $S \\approx 0.9$），并对变化因子进行限制：\n$$ h_{new} = h_{old} \\cdot \\min\\left(f_{max}, \\max\\left(f_{min}, S \\left( \\frac{\\text{tol}}{E_{old}} \\right)^{1/p}\\right)\\right) $$\n如果 $E_{old} \\le \\text{tol}$，则接受该步，并使用 $h_{new}$ 进行下一步。如果 $E_{old}  \\text{tol}$，则拒绝该步，并使用较小的步长 $h_{new}$ 重试当前步。\n\n**3. 缩放定律的实验验证**\n\n对于一个长度为 $L = t_{\\text{end}} - t_0$ 的固定积分区间，总接受步数 $N$ 与平均步长 $h_{avg}$ 的关系为 $N \\approx L/h_{avg}$。如果自适应算法成功地在每一步都维持 $E \\approx \\text{tol}$，那么平均而言 $h_{avg} \\propto \\text{tol}^{1/p}$。因此，总步数应按以下方式缩放：\n$$ N \\propto \\frac{1}{h_{avg}} \\propto \\frac{1}{\\text{tol}^{1/p}} = \\text{tol}^{-1/p} $$\n对两边取对数，揭示了一个线性关系：\n$$ \\log N = \\log(\\text{constant}) - \\frac{1}{p} \\log(\\text{tol}) $$\n这个方程的形式为 $Y = C + sX$，其中 $Y = \\log N$，$X = \\log(\\text{tol})$，理论斜率为 $s = -1/p$。对于本问题，$p=5$，理论斜率为 $-0.2$。\n\n流程是为一组容差运行求解器，记录每个容差的步数，并对 $\\log N$ vs. $\\log \\text{tol}$ 数据进行线性回归。最佳拟合线的斜率 $s$ 由以下公式给出：\n$$ s = \\frac{n \\sum(x_i y_i) - (\\sum x_i)(\\sum y_i)}{n \\sum(x_i^2) - (\\sum x_i)^2} $$\n其中 $x_i = \\log(\\text{tol}_i)$，$y_i = \\log(N_i)$，$n$ 是数据点的数量。然后将计算出的斜率 $s$ 与理论值 $-1/p$ 进行比较，以验证其是否落在给定的阈值 $\\epsilon$ 内。这确认了所实现的求解器是否符合数值理论的行为。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements an adaptive RKF45 solver and verifies its step-count-to-tolerance scaling.\n    \"\"\"\n    \n    # --- Define ODEs and problem parameters ---\n    def f_A(t, y):\n        return -y\n\n    def f_B(t, y):\n        return np.cos(10 * t)\n\n    def f_C(t, y):\n        # The ODE y' = y^2 has a singularity at t=1 for y(0)=1.\n        # The solution is y(t) = 1 / (1-t).\n        return y**2\n\n    test_cases = [\n        {'name': 'A', 'f': f_A, 't0': 0.0, 'y0': 1.0, 't_end': 10.0},\n        {'name': 'B', 'f': f_B, 't0': 0.0, 'y0': 0.0, 't_end': 10.0},\n        {'name': 'C', 'f': f_C, 't0': 0.0, 'y0': 1.0, 't_end': 0.9}\n    ]\n    tols = np.array([1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n    p = 5.0\n    epsilon = 0.15\n\n    # --- RKF45 adaptive solver implementation ---\n    def rkf45_solver(f, t0, y0, t_end, tol):\n        \"\"\"\n        Solves a scalar ODE using the adaptive Runge-Kutta-Fehlberg 4(5) method.\n        \"\"\"\n        # Butcher tableau coefficients for RKF45\n        c = np.array([0, 1/4, 3/8, 12/13, 1, 1/2], dtype=float)\n        a21 = 1/4\n        a31, a32 = 3/32, 9/32\n        a41, a42, a43 = 1932/2197, -7200/2197, 7296/2197\n        a51, a52, a53, a54 = 439/216, -8, 3680/513, -845/4104\n        a61, a62, a63, a64, a65 = -8/27, 2, -3544/2565, 1859/4104, -11/40\n\n        # Coefficients for the 5th-order solution (used for advancing state)\n        b_star = np.array([16/135, 0, 6656/12825, 28561/56430, -9/50, 2/55], dtype=float)\n        # Coefficients for error estimation (b_star - b)\n        err_coeffs = b_star - np.array([25/216, 0, 1408/2565, 2197/4104, -1/5, 0], dtype=float)\n\n        # Solver parameters\n        safety = 0.9\n        fac_min, fac_max = 0.1, 5.0\n        h_max = (t_end - t0)\n        h_min = 1e-12\n\n        t = t0\n        y = y0\n        h = (t_end - t0) * 1e-3  # Initial guess for step size\n        accepted_steps = 0\n\n        while t  t_end:\n            h = min(h, t_end - t)\n            \n            step_accepted = False\n            while not step_accepted:\n                # Prevent step size from becoming too small\n                if abs(h)  h_min:\n                    raise RuntimeError(f\"Step size {h} is below minimum {h_min}\")\n                \n                 # Calculate the six stages (k_i)\n                k1 = f(t, y)\n                k2 = f(t + c[1] * h, y + h * (a21 * k1))\n                k3 = f(t + c[2] * h, y + h * (a31 * k1 + a32 * k2))\n                k4 = f(t + c[3] * h, y + h * (a41 * k1 + a42 * k2 + a43 * k3))\n                k5 = f(t + c[4] * h, y + h * (a51 * k1 + a52 * k2 + a53 * k3 + a54 * k4))\n                k6 = f(t + c[5] * h, y + h * (a61 * k1 + a62 * k2 + a63 * k3 + a64 * k4 + a65 * k5))\n                ks = np.array([k1, k2, k3, k4, k5, k6])\n\n                # Estimate the local error\n                error_est = abs(h * np.dot(err_coeffs, ks))\n\n                if error_est = tol:\n                    step_accepted = True\n                    t += h\n                    y += h * np.dot(b_star, ks) # Advance with higher-order solution\n                    accepted_steps += 1\n                \n                # Determine scaling factor for the next step size\n                if error_est == 0.0:\n                    h_scale_factor = fac_max\n                else:\n                    h_scale_factor = safety * (tol / error_est)**(1.0 / p)\n\n                h_scale_factor = min(fac_max, max(fac_min, h_scale_factor))\n                h *= h_scale_factor\n                h = min(h, h_max)\n\n        return accepted_steps\n    \n    # --- Main test loop ---\n    results = []\n    for case in test_cases:\n        N_values = []\n        for tol_val in tols:\n            N = rkf45_solver(case['f'], case['t0'], case['y0'], case['t_end'], tol_val)\n            N_values.append(N)\n        \n        # --- Linear regression to find the slope ---\n        log_tols = np.log(tols)\n        log_N = np.log(np.array(N_values))\n        n = len(log_tols)\n\n        sum_x = np.sum(log_tols)\n        sum_y = np.sum(log_N)\n        sum_xy = np.sum(log_tols * log_N)\n        sum_x2 = np.sum(log_tols**2)\n        \n        # Calculate slope 's' of the line log(N) vs log(tol)\n        numerator = n * sum_xy - sum_x * sum_y\n        denominator = n * sum_x2 - sum_x**2\n        \n        if denominator == 0:\n            raise ValueError(\"Cannot perform linear regression: zero denominator.\")\n        \n        slope = numerator / denominator\n        \n        # --- Validation check ---\n        theoretical_slope = -1.0 / p\n        is_close = np.abs(slope - theoretical_slope) = epsilon\n        results.append(is_close)\n        \n    final_output = f\"[{','.join(str(r) for r in results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3224450"}, {"introduction": "任何强大的工具都有其局限性，理解这些局限性是进行可靠科学计算的前提。本练习是一个思想实验，旨在挑战你寻找一种特殊情况：即对于某个非平凡的常微分方程，嵌入式方法的误差估计量恒为零 [@problem_id:3224363]。这种情况会使自适应步长机制“失明”，这个练习有助于培养对数值方法进行批判性评估的能力。", "problem": "设 $y(t)$ 是初值问题 $y^{\\prime}(t)=f(t,y(t))$（满足 $y(0)=y_{0}$）的解。一个嵌入式显式 Runge–Kutta 对，也称为 Runge–Kutta–Fehlberg (RKF) 方法，使用一组共同的内嵌阶段导数评估和两组不同的输出权重，来产生两个不同代数阶的一步近似解。这两个近似解之间的差异被用作局部误差估计器，以自适应地调整步长。\n\n仅使用相容的显式 Runge–Kutta 对中的两个公式由相同阶段形成更新，并且每组输出权重之和为 $1$ 这两个定义性质，判断是否存在一个非恒定的精确解，使得对于任意步长 $h0$，RKF 局部误差估计器恒为零。如果存在，请构造一个具体的初值问题，并指明一个出现这种情况的标准 RKF 对，然后计算该对在从任意状态 $y_{n}$ 开始，经过一个任意大小为 $h$ 的单步后产生的局部误差估计器的值（即高阶和低阶单步更新值之差）。\n\n你的最终答案必须是该估计器作为一个实数的值。不需要四舍五入。", "solution": "问题在于，是否存在一个初值问题（IVP）的非恒定精确解，使得嵌入式 Runge-Kutta-Fehlberg (RKF) 方法的局部误差估计器对于任何步长 $h0$ 都恒为零。如果存在这种情况，我们必须提供一个具体的初值问题，指明一个标准的 RKF 对，并计算误差估计器的值。\n\n答案是肯定的，存在这种情况。\n\n设初值问题由常微分方程 $y^{\\prime}(t) = f(t, y(t))$ 和初始条件 $y(t_0) = y_0$ 定义。\n一个嵌入式显式 Runge–Kutta 对在下一步 $t_{n+1} = t_n + h$ 处提供两个解的近似值。这些近似值通常具有不同的阶数，比如 $p$ 和 $\\hat{p}$。设 $y_{n+1}$ 为 $p$ 阶近似值，$\\hat{y}_{n+1}$ 为 $\\hat{p}$ 阶近似值。\n这两个近似值都是使用同一组 $s$ 个内嵌阶段导数 $k_i$ 构建的，但使用不同的权重集 $\\mathbf{b} = (b_1, \\dots, b_s)$ 和 $\\mathbf{\\hat{b}} = (\\hat{b}_1, \\dots, \\hat{b}_s)$。\n\n单步的公式为：\n$$y_{n+1} = y_n + h \\sum_{i=1}^{s} b_i k_i$$\n$$\\hat{y}_{n+1} = y_n + h \\sum_{i=1}^{s} \\hat{b}_i k_i$$\n其中阶段 $k_i$ 的计算方式如下：\n$$k_i = f\\left(t_n + c_i h, y_n + h \\sum_{j=1}^{i-1} a_{ij} k_j\\right), \\quad i=1, \\dots, s$$\n\n局部误差估计器 $E$ 定义为这两个数值近似值之差：\n$$E = \\hat{y}_{n+1} - y_{n+1} = \\left(y_n + h \\sum_{i=1}^{s} \\hat{b}_i k_i\\right) - \\left(y_n + h \\sum_{i=1}^{s} b_i k_i\\right) = h \\sum_{i=1}^{s} (\\hat{b}_i - b_i) k_i$$\n\n我们正在寻找一个具有非恒定解 $y(t)$ 的初值问题，使得对于所有步长 $h  0$ 都有 $E = 0$。这要求满足以下条件：\n$$\\sum_{i=1}^{s} (\\hat{b}_i - b_i) k_i = 0$$\n\n让我们考虑能产生非恒定解的最简单的一类初值问题：导数为非零常数的常微分方程。\n设初值问题为 $y'(t) = C$，初始条件为 $y(0) = y_0$，其中 $C$ 是一个非零实常数。一个具体的例子是 $y'(t)=1$，初始条件为 $y(0)=0$。其精确解为 $y(t) = y_0 + Ct$，这是一个关于 $t$ 的非恒定函数。\n\n对于 $f(t,y) = C$ 这个具体选择，我们来评估阶段 $k_i$：\n第一阶段是 $k_1 = f(t_n, y_n) = C$。\n第二阶段是 $k_2 = f(t_n + c_2 h, y_n + h a_{21} k_1) = C$，因为函数 $f$ 是常数，其值不依赖于其自变量。\n通过归纳法，对于任何显式 Runge-Kutta 方法，所有阶段都将是相同的：\n$$k_i = C \\quad \\text{对于所有 } i=1, \\dots, s$$\n\n现在，将此结果代入局部误差估计器 $E$ 的表达式中：\n$$E = h \\sum_{i=1}^{s} (\\hat{b}_i - b_i) C = h C \\sum_{i=1}^{s} (\\hat{b}_i - b_i)$$\n\n问题陈述提供了一个关键性质，即对于一个相容的 Runge-Kutta 对，“每组输出权重之和为 $1$”。这是任何 Runge-Kutta 方法至少为 1 阶的必要条件。因此，我们有：\n$$\\sum_{i=1}^{s} b_i = 1$$\n$$\\sum_{i=1}^{s} \\hat{b}_i = 1$$\n\n利用这个性质，我们可以计算 $E$ 表达式中的和：\n$$\\sum_{i=1}^{s} (\\hat{b}_i - b_i) = \\sum_{i=1}^{s} \\hat{b}_i - \\sum_{i=1}^{s} b_i = 1 - 1 = 0$$\n\n将此代回 $E$ 的公式，得到：\n$$E = h C \\cdot 0 = 0$$\n\n这个结果表明，对于任何形式为 $y'(t)=C$（其中 $C \\neq 0$）的初值问题，任何相容的嵌入式 Runge-Kutta 对的局部误差估计器，对于任何步长 $h0$ 和从任何状态 $y_n$ 开始，都恒为零。这是因为任何相容的 RK 方法对于这类常微分方程都是精确的；低阶和高阶方法都得出精确解 $y_{n+1} = \\hat{y}_{n+1} = y_n + hC$，所以它们的差为零。\n\n为了满足问题的要求：\n1.  **一个具体的初值问题：** $y'(t) = 1$，初始条件为 $y(0)=0$。解是 $y(t)=t$，这是一个非恒定解。\n2.  **一个标准的 RKF 对：** 经典的 Runge-Kutta-Fehlberg 4(5) 方法 (RKF45) 或更现代的 Dormand-Prince 5(4) 对 (DOPRI5) 都是标准例子。\n3.  **局部误差估计器的值：** 如计算所示，对于这个初值问题和任何这样的 RKF 对，估计器的值为 $0$。\n\n所选 RKF 对在从任意状态 $y_n$ 开始，经过一个任意大小为 $h$ 的单步后产生的局部误差估计器的值为 $0$。", "answer": "$$\n\\boxed{0}\n$$", "id": "3224363"}]}