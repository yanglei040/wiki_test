{"hands_on_practices": [{"introduction": "我们从第一性原理出发，构建最小二乘法。这个练习将引导你通过直接最小化平方误差之和，来推导二次拟合的最优性条件。掌握这个过程对于理解“正规方程组”的来源和建立坚实的理论基础至关重要([@problem_id:3262996])。", "problem": "一个实验室正在校准一个传感器，其输出由一个二次多项式建模。在输入值 $x \\in \\{0,1,1,2\\}$ 处进行了四次独立测量，按相同顺序产生的输出为 $y \\in \\{10,8,6,3\\}$。重复的输入 $x=1$ 是为了减少不确定性而进行的有意重复实验；两个重复的观测值都必须被视为对拟合准则的独立贡献。模型是一个二次多项式 $p(x) = a x^{2} + b x + c$。在最小二乘法的意义上，最佳近似被定义为选择系数 $a$、$b$ 和 $c$，以最小化给定数据点上的残差平方和 $\\sum (p(x_{i}) - y_{i})^{2}$。\n\n仅从最小二乘最小化的基本定义和标准线性代数知识出发，通过将目标函数关于系数的梯度设为零，推导最优性的必要条件，并仔细考虑在 $x=1$ 处存在两个不同输出的测量值这一事实。然后精确求解得到的方程组，以确定最佳拟合二次多项式的首项系数 $a$。\n\n将你的最终答案精确地表示为一个数字；不要四舍五入。", "solution": "首先根据指定标准验证该问题。\n\n### 第一步：提取已知条件\n-   输入值：$x \\in \\{0,1,1,2\\}$。\n-   输出值：$y \\in \\{10,8,6,3\\}$，按顺序对应于输入值。\n-   数据点 $(x_i, y_i)$：$(0, 10)$、$(1, 8)$、$(1, 6)$、$(2, 3)$。测量总数为 $n=4$。\n-   模型：二次多项式 $p(x) = a x^{2} + b x + c$。\n-   目标：最小化残差平方和 $S = \\sum_{i=1}^{4} (p(x_{i}) - y_{i})^{2}$，以找到最佳拟合系数 $a$、$b$ 和 $c$。\n-   任务：通过将 $S$ 的梯度设为零，从第一性原理推导最小值的必要条件，然后求解系数 $a$。\n\n### 第二步：使用提取的已知条件进行验证\n-   **科学依据：** 该问题描述了一个标准的多项式最小二乘拟合过程，这是数值方法、统计学和数据分析中一个基础且广泛使用的技术。它基于公认的数学原理。\n-   **适定性：** 该问题提供了四个数据点来确定三个未知系数（$a, b, c$）。该系统是超定的，这是最小二乘拟合的标准情况。当且仅当设计矩阵的列线性无关时，存在唯一解。该问题的设计矩阵 $A$ 由下式给出\n    $$A = \\begin{pmatrix} x_1^2  x_1  1 \\\\ x_2^2  x_2  1 \\\\ x_3^2  x_3  1 \\\\ x_4^2  x_4  1 \\end{pmatrix} = \\begin{pmatrix} 0^2  0  1 \\\\ 1^2  1  1 \\\\ 1^2  1  1 \\\\ 2^2  2  1 \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\\\ 1  1  1 \\\\ 1  1  1 \\\\ 4  2  1 \\end{pmatrix}$$\n    各列是线性无关的，如下所示，将线性组合设为零：$\\alpha_1 [0,1,1,4]^T + \\alpha_2 [0,1,1,2]^T + \\alpha_3 [1,1,1,1]^T = [0,0,0,0]^T$。第一行意味着 $\\alpha_3=0$。这将系统简化为 $\\alpha_1+\\alpha_2=0$ 和 $4\\alpha_1+2\\alpha_2=0$，解得 $\\alpha_1=\\alpha_2=0$。因此，各列线性无关，矩阵 $A^T A$ 可逆，存在唯一解。该问题是适定的。\n-   **目标明确性：** 问题陈述清晰、精确，并使用标准术语。将重复测量值视为独立贡献的指令是明确的。\n\n### 第三步：结论与行动\n该问题是有效的。它具有科学依据、适定性且目标明确。我将继续进行求解。\n\n### 推导与求解\n\n传感器输出的模型是二次多项式 $p(x) = a x^{2} + b x + c$。数据点为 $(x_1, y_1) = (0, 10)$、$(x_2, y_2) = (1, 8)$、$(x_3, y_3) = (1, 6)$ 和 $(x_4, y_4) = (2, 3)$。\n\n目标是找到使残差平方和 $S(a, b, c)$ 最小的系数 $a, b, c$。函数 $S$ 定义为：\n$$S(a, b, c) = \\sum_{i=1}^{4} (p(x_i) - y_i)^2$$\n代入给定的数据和多项式模型：\n$$S(a, b, c) = (a(0)^2 + b(0) + c - 10)^2 + (a(1)^2 + b(1) + c - 8)^2 + (a(1)^2 + b(1) + c - 6)^2 + (a(2)^2 + b(2) + c - 3)^2$$\n$$S(a, b, c) = (c - 10)^2 + (a + b + c - 8)^2 + (a + b + c - 6)^2 + (4a + 2b + c - 3)^2$$\n\n为了最小化 $S(a,b,c)$，我们必须找到其关于系数的梯度为零向量的点。这给出了最优性的必要条件：\n$$\\frac{\\partial S}{\\partial a} = 0, \\quad \\frac{\\partial S}{\\partial b} = 0, \\quad \\frac{\\partial S}{\\partial c} = 0$$\n\n我们使用链式法则计算每个偏导数：\n\n1.  关于 $a$ 的偏导数：\n    $$\\frac{\\partial S}{\\partial a} = 0 + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(4) = 0$$\n    两边除以 $2$：\n    $$(a + b + c - 8) + (a + b + c - 6) + 4(4a + 2b + c - 3) = 0$$\n    $$(1+1+16)a + (1+1+8)b + (1+1+4)c = 8 + 6 + 12$$\n    $$18a + 10b + 6c = 26$$\n    $$9a + 5b + 3c = 13 \\quad (1)$$\n\n2.  关于 $b$ 的偏导数：\n    $$\\frac{\\partial S}{\\partial b} = 0 + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(2) = 0$$\n    两边除以 $2$：\n    $$(a + b + c - 8) + (a + b + c - 6) + 2(4a + 2b + c - 3) = 0$$\n    $$(1+1+8)a + (1+1+4)b + (1+1+2)c = 8 + 6 + 6$$\n    $$10a + 6b + 4c = 20$$\n    $$5a + 3b + 2c = 10 \\quad (2)$$\n\n3.  关于 $c$ 的偏导数：\n    $$\\frac{\\partial S}{\\partial c} = 2(c - 10)(1) + 2(a + b + c - 8)(1) + 2(a + b + c - 6)(1) + 2(4a + 2b + c - 3)(1) = 0$$\n    两边除以 $2$：\n    $$(c - 10) + (a + b + c - 8) + (a + b + c - 6) + (4a + 2b + c - 3) = 0$$\n    $$(1+1+4)a + (1+1+2)b + (1+1+1+1)c = 10 + 8 + 6 + 3$$\n    $$6a + 4b + 4c = 27 \\quad (3)$$\n\n我们现在得到了一个关于三个未知系数 $a$、$b$ 和 $c$ 的三元线性方程组，称为正规方程组：\n1.  $9a + 5b + 3c = 13$\n2.  $5a + 3b + 2c = 10$\n3.  $6a + 4b + 4c = 27$\n\n问题要求解出首项系数 $a$。我们可以使用消元法来解这个方程组。我们先消去 $c$。\n将方程 (2) 乘以 $3$ 并将方程 (1) 乘以 $2$：\n$$2 \\times (1): \\quad 18a + 10b + 6c = 26$$\n$$3 \\times (2): \\quad 15a + 9b + 6c = 30$$\n用第一个新方程减去第二个新方程：\n$$(18a - 15a) + (10b - 9b) + (6c - 6c) = 26 - 30$$\n$$3a + b = -4 \\quad (4)$$\n\n接下来，使用方程 (2) 和 (3) 来消去 $c$。将方程 (2) 乘以 $2$：\n$$2 \\times (2): \\quad 10a + 6b + 4c = 20$$\n$$(3): \\quad 6a + 4b + 4c = 27$$\n用方程(3)减去上面那个新方程：\n$$(6a - 10a) + (4b - 6b) + (4c - 4c) = 27 - 20$$\n$$-4a - 2b = 7 \\quad (5)$$\n\n现在我们得到一个关于 $a$ 和 $b$ 的二元方程组：\n4.  $3a + b = -4$\n5.  $-4a - 2b = 7$\n\n从方程 (4)，我们可以用 $a$ 表示 $b$：\n$$b = -4 - 3a$$\n将 $b$ 的这个表达式代入方程 (5)：\n$$-4a - 2(-4 - 3a) = 7$$\n$$-4a + 8 + 6a = 7$$\n$$2a = 7 - 8$$\n$$2a = -1$$\n$$a = -\\frac{1}{2}$$\n\n最佳拟合二次多项式的首项系数恰好是 $-\\frac{1}{2}$。", "answer": "$$\\boxed{-\\frac{1}{2}}$$", "id": "3262996"}, {"introduction": "在基本框架的基础上，我们现在来解决一个常见的现实世界挑战：数据点的确定性水平不同。本练习介绍了加权最小二乘法，它允许你将每次测量的可靠性纳入拟合过程。你将学习如何给予更精确的数据点更大的影响力，这是实验科学中的一种标准而强大的技术([@problem_id:3262884])。", "problem": "一个实验室在输入 $x_{1}=-1$、$x_{2}=0$ 和 $x_{3}=1$ 处收集了某个潜在线性响应的 $3$ 个标量测量值。报告的测量值为 $y_{1}=0$、$y_{2}=1$ 和 $y_{3}=2$。测量误差是独立的、零均值的，并且具有已知的标准差 $\\sigma_{1}=1$、$\\sigma_{2}=\\tfrac{1}{2}$ 和 $\\sigma_{3}=1$。您需要寻找一个线性多项式 $p(x)=c_{0}+c_{1}x$ 的多项式最小二乘逼近，该逼近通过为更可靠的数据分配更高的权重来考虑测量值的不同可靠性。使用对角矩阵 $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_{1}},\\tfrac{1}{\\sigma_{2}},\\tfrac{1}{\\sigma_{3}}\\big)$ 将目标定义为加权残差的欧几里得二范数（L2）的平方，即最小化 $\\|W(Ac-y)\\|_{2}^{2}$，其中 $A$ 是设计矩阵，$c=\\begin{pmatrix}c_{0}\\\\c_{1}\\end{pmatrix}$，$y=\\begin{pmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\end{pmatrix}$。\n\n任务：\n1) 写出与此问题相关的显式矩阵和向量 $A$、$W$ 和 $y$。\n2) 从基本原理出发，即通过将可微标量目标函数对 $c$ 的梯度设为零来求其最小值，推导出刻画该最小化子的线性方程组。\n3) 求解 $c_{0}$ 和 $c_{1}$，然后在 $x=2$ 处计算拟合多项式的值。\n\n提供 $p(2)$ 的唯一最终值，要求为精确数。不要四舍五入。", "solution": "该问题是适定的且有科学依据，是数值分析中加权最小二乘法的标准应用。获得唯一解所需的所有数据和条件均已提供且自洽。\n\n### 第1步：问题验证\n\n**逐字提取的已知条件：**\n*   测量次数：$3$。\n*   输入：$x_{1}=-1$，$x_{2}=0$，$x_{3}=1$。\n*   测量值：$y_{1}=0$，$y_{2}=1$，$y_{3}=2$。\n*   标准差：$\\sigma_{1}=1$，$\\sigma_{2}=\\tfrac{1}{2}$，$\\sigma_{3}=1$。\n*   模型：线性多项式 $p(x)=c_{0}+c_{1}x$。\n*   权重矩阵：$W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_{1}},\\tfrac{1}{\\sigma_{2}},\\tfrac{1}{\\sigma_{3}}\\big)$。\n*   目标函数：最小化 $\\|W(Ac-y)\\|_{2}^{2}$。\n*   参数向量：$c=\\begin{pmatrix}c_{0}\\\\c_{1}\\end{pmatrix}$。\n*   测量向量：$y=\\begin{pmatrix}y_{1}\\\\y_{2}\\\\y_{3}\\end{pmatrix}$。\n\n**验证分析：**\n该问题是加权线性回归中一个定义明确的数学练习。它在科学上是合理的，因为加权最小二乘法是用于将模型拟合到具有非均匀方差的数据的基本统计技术。问题是自洽的，提供了所有必要的数值和函数形式。输入 $x_i$ 导出一个具有线性无关列的设计矩阵 $A$，确保了得到的正规方程组有唯一解，因此该问题是适定的。术语精确客观。问题是有效的。\n\n### 第2步：求解推导\n\n问题要求找到线性多项式 $p(x) = c_0 + c_1x$ 的系数 $c_0$ 和 $c_1$，使得加权残差向量的欧几里得范数的平方最小。\n\n#### 任务1：显式矩阵和向量\n\n首先，我们根据给定数据构造矩阵和向量 $A$、$W$ 和 $y$。\n\n方程组为 $p(x_i) \\approx y_i$，其中 $i=1, 2, 3$：\n$$\n\\begin{cases}\nc_0 + c_1(-1) \\approx 0 \\\\\nc_0 + c_1(0) \\approx 1 \\\\\nc_0 + c_1(1) \\approx 2\n\\end{cases}\n$$\n这可以写成矩阵形式 $Ac \\approx y$。设计矩阵 $A$ 是通过在每个输入 $x_i$ 处计算基函数（$1$ 和 $x$）的值形成的：\n$$\nA = \\begin{pmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\end{pmatrix}\n$$\n测量向量 $y$ 是：\n$$\ny = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\n权重矩阵 $W$ 由 $W=\\mathrm{diag}\\!\\big(\\tfrac{1}{\\sigma_{1}},\\tfrac{1}{\\sigma_{2}},\\tfrac{1}{\\sigma_{3}}\\big)$ 给出。当 $\\sigma_{1}=1$，$\\sigma_{2}=\\tfrac{1}{2}$ 和 $\\sigma_{3}=1$ 时，我们有：\n$$\nW = \\mathrm{diag}\\!\\big(\\tfrac{1}{1},\\tfrac{1}{1/2},\\tfrac{1}{1}\\big) = \\mathrm{diag}(1, 2, 1) = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n\n#### 任务2：线性系统的推导\n\n目标是最小化标量函数 $J(c) = \\|W(Ac-y)\\|_{2}^{2}$。\n使用欧几里得范数的定义（$ \\|v\\|_2^2 = v^T v $），我们可以将目标函数写为：\n$$\nJ(c) = \\big(W(Ac-y)\\big)^T \\big(W(Ac-y)\\big)\n$$\n使用转置性质 $(XY)^T = Y^T X^T$：\n$$\nJ(c) = (Ac-y)^T W^T W (Ac-y)\n$$\n为求最小值，我们计算 $J(c)$ 关于 $c$ 的梯度并将其设为零。让我们展开 $J(c)$ 的表达式：\n$$\nJ(c) = (c^T A^T - y^T) W^T W (Ac - y)\n$$\n$$\nJ(c) = c^T A^T W^T W A c - c^T A^T W^T W y - y^T W^T W A c + y^T W^T W y\n$$\n项 $c^T A^T W^T W y$ 和 $y^T W^T W A c$ 是标量，并且互为转置。因此，它们相等。\n$$\nJ(c) = c^T (A^T W^T W A) c - 2 c^T (A^T W^T W y) + y^T W^T W y\n$$\n关于向量 $c$ 的梯度是：\n$$\n\\nabla_c J(c) = \\nabla_c \\Big( c^T (A^T W^T W A) c - 2 c^T (A^T W^T W y) + y^T W^T W y \\Big)\n$$\n使用矩阵微积分恒等式 $\\nabla_c(c^T M c) = 2Mc$（对于对称矩阵 $M$）和 $\\nabla_c(c^T b) = b$：\n$$\n\\nabla_c J(c) = 2(A^T W^T W A) c - 2(A^T W^T W y)\n$$\n将梯度设为零，$\\nabla_c J(c) = 0$：\n$$\n2(A^T W^T W A) c - 2(A^T W^T W y) = 0\n$$\n$$\n(A^T W^T W A) c = A^T W^T W y\n$$\n这就是刻画最小化子 $c$ 的线性系统，称为加权最小二乘问题的正规方程组。\n\n#### 任务3：求解 $c_0$、$c_1$ 并计算 $p(2)$\n\n我们现在求解方程组 $(A^T W^T W A) c = A^T W^T W y$。\n由于 $W$ 是实对角矩阵，所以 $W^T = W$。该系统变为 $(A^T W^2 A) c = A^T W^2 y$。\n首先，我们计算矩阵 $W^2$：\n$$\nW^2 = W^T W = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  4  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n该矩阵对角线上包含逆方差 $1/\\sigma_i^2$，这与标准加权最小二乘法的预期一致。\n\n接下来，我们计算左侧的矩阵 $A^T W^2 A$：\n$$\nA^T W^2 A = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  4  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\end{pmatrix}\n$$\n$$\nA^T W^2 A = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\\\ 4  0 \\\\ 1  1 \\end{pmatrix}\n$$\n$$\nA^T W^2 A = \\begin{pmatrix} (1)(1)+(1)(4)+(1)(1)  (1)(-1)+(1)(0)+(1)(1) \\\\ (-1)(1)+(0)(4)+(1)(1)  (-1)(-1)+(0)(0)+(1)(1) \\end{pmatrix} = \\begin{pmatrix} 6  0 \\\\ 0  2 \\end{pmatrix}\n$$\n现在，我们计算右侧的向量 $A^T W^2 y$：\n$$\nA^T W^2 y = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  4  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}\n$$\n$$\nA^T W^2 y = \\begin{pmatrix} 1  1  1 \\\\ -1  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\n$$\nA^T W^2 y = \\begin{pmatrix} (1)(0)+(1)(4)+(1)(2) \\\\ (-1)(0)+(0)(4)+(1)(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}\n$$\n需求解的关于 $c = \\begin{pmatrix} c_0 \\\\ c_1 \\end{pmatrix}$ 的线性系统是：\n$$\n\\begin{pmatrix} 6  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} c_0 \\\\ c_1 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}\n$$\n这个对角系统直接给出解：\n$$\n6c_0 = 6 \\implies c_0 = 1\n$$\n$$\n2c_1 = 2 \\implies c_1 = 1\n$$\n拟合的多项式是 $p(x) = c_0 + c_1 x = 1 + x$。\n\n最后，我们在 $x=2$ 处计算该多项式的值：\n$$\np(2) = 1 + 2 = 3\n$$\n数据点 $(-1, 0)$、$(0, 1)$ 和 $(1, 2)$ 共线，并且完全位于直线 $y=x+1$ 上。因此，无论权重如何，最小二乘拟合都会得到这条精确的直线，从而导致残差为零。我们推导出的系数 $c_0=1$ 和 $c_1=1$ 与这一观察结果一致。", "answer": "$$\\boxed{3}$$", "id": "3262884"}, {"introduction": "在数据范围内的成功拟合可能具有欺骗性，理解模型的局限性与构建模型本身同样重要。这个计算练习揭示了多项式拟合的一个关键弱点：外推的危险。通过拟合一个周期函数，你将看到一个高次多项式，尽管在训练数据上非常精确，但在预测时却可能灾难性地失败，这为模型验证提供了重要的一课([@problem_id:3262865])。", "problem": "您将实现并分析对一个无噪声采样的周期性目标函数的多项式最小二乘近似。目标是通过计算证明，多项式拟合在训练区间上可以达到很高的精度，但在该区间外进行外推时却会灾难性地失败，从而无法捕捉函数的周期性。\n\n您必须基于最小二乘近似的核心原理进行推导和构建算法。请使用以下经过充分检验的事实和定义作为基础出发点：\n- 离散最小二乘近似旨在从一个有限维空间中寻找一个函数，该函数使给定样本点上的残差平方和最小化。\n- 对于多项式模型，最小二乘问题可以使用范德蒙德矩阵写成矩阵形式，该矩阵编码了单项式基在样本点上的求值。\n- 一组点上的均方根误差（RMSE）是残差平方的平均值的平方根。\n\n任务：\n- 考虑目标函数 $f(x)=\\sin(x)$，其中 $x$ 以弧度为单位。\n- 对于每个测试用例，通过在区间 $[x_{\\min},x_{\\max}]$ 上均匀采样 $N_{\\text{train}}$ 个点来生成训练数据集。使用 $y_i=f(x_i)$，不加噪声。\n- 在离散最小二乘的意义下，将一个 $d$ 次多项式 $p_d(x)$ 拟合到此数据。使用数值稳定的方法来求解由范德蒙德矩阵定义的线性最小二乘问题。如果您的环境中有更稳定的方法可用，请不要直接使用正规方程。\n- 计算训练点上的样本内均方根误差，记为 $\\text{RMSE}_{\\text{train}}=\\sqrt{\\frac{1}{N_{\\text{train}}}\\sum_{i=1}^{N_{\\text{train}}}\\left(p_d(x_i)-f(x_i)\\right)^2}$。\n- 为评估外推行为，定义一个测试区间 $[u_{\\min},u_{\\max}]$，在其上均匀生成 $N_{\\text{test}}$ 个点（选择 $N_{\\text{test}}=200$），并以类似方式计算样本外均方根误差 $\\text{RMSE}_{\\text{test}}$。\n- 使用以下决策规则来判断多项式拟合是否同时满足样本内精确且样本外灾难性失败的条件：\n  - 定义阈值 $\\varepsilon_{\\text{in}}=10^{-2}$ 和 $\\varepsilon_{\\text{out}}=5\\times 10^{-1}$。\n  - 如果 $\\text{RMSE}_{\\text{train}}\\le \\varepsilon_{\\text{in}}$ 且 $\\text{RMSE}_{\\text{test}}\\ge \\varepsilon_{\\text{out}}$，则输出整数 $1$；否则输出整数 $0$。\n\n角度单位：所有三角函数求值必须使用弧度。\n\n测试套件：\n- 用例 1：$d=9$, $N_{\\text{train}}=64$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[10\\pi,12\\pi]$。\n- 用例 2：$d=15$, $N_{\\text{train}}=100$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[8\\pi,10\\pi]$。\n- 用例 3：$d=20$, $N_{\\text{train}}=120$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[-12\\pi,-10\\pi]$。\n- 用例 4（用于测试当样本内拟合不够精确时决策规则的边界情况）：$d=1$, $N_{\\text{train}}=64$, $[x_{\\min},x_{\\max}]=[0,2\\pi]$, $[u_{\\min},u_{\\max}]=[8\\pi,10\\pi]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按测试用例顺序排列的结果，结果为逗号分隔的列表，并用方括号括起。例如，输出形式应为 $[r_1,r_2,r_3,r_4]$，其中每个 $r_k$ 是根据上述定义得到的 $0$ 或 $1$。", "solution": "该问题要求实现并分析对周期函数 $f(x) = \\sin(x)$ 的多项式最小二乘近似。核心目标是证明多项式模型虽然能够在指定的训练区间上实现高精度拟合，但由于其非周期性，从根本上不适合外推。当模型在远离训练数据域的区间上进行评估时，这会导致灾难性的失败。\n\n该过程涉及将一个 $d$ 次多项式拟合到区间 $[x_{\\min}, x_{\\max}]$ 上 $N_{\\text{train}}$ 个 $f(x)$ 的无噪声样本集。拟合质量的评估分别在训练数据（样本内）和一个独立的、不重叠的测试区间 $[u_{\\min}, u_{\\max}]$（样本外）上进行。然后应用一个决策规则来分类拟合是否同时满足样本内良好和样本外差的条件。\n\n设 $d$ 次多项式模型表示为 $p_d(x)$：\n$$\np_d(x) = \\sum_{j=0}^{d} c_j x^j = c_0 + c_1 x + c_2 x^2 + \\dots + c_d x^d\n$$\n任务是确定系数向量 $\\mathbf{c} = [c_0, c_1, \\dots, c_d]^T$，使其在最小二乘意义下最佳地拟合训练数据。训练数据由 $N_{\\text{train}}$ 个点对 $(x_i, y_i)$ 组成，其中点 $x_i$ 从 $[x_{\\min}, x_{\\max}]$ 中均匀采样，且 $y_i = f(x_i) = \\sin(x_i)$。\n\n离散最小二乘问题是找到系数向量 $\\mathbf{c}$，以最小化残差平方和 $S$：\n$$\nS = \\sum_{i=1}^{N_{\\text{train}}} (p_d(x_i) - y_i)^2\n$$\n这可以被表述为一个线性代数问题。令 $\\mathbf{y} = [y_1, y_2, \\dots, y_{N_{\\text{train}}}]^T$ 为观测函数值的向量。对于 $i=1, \\dots, N_{\\text{train}}$ 的方程组 $p_d(x_i) = y_i$ 构成一个超定线性系统 $A\\mathbf{c} \\approx \\mathbf{y}$，其中 $A$ 是一个 $N_{\\text{train}} \\times (d+1)$ 的范德蒙德矩阵：\n$$\nA =\n\\begin{pmatrix}\n1  x_1  x_1^2  \\dots  x_1^d \\\\\n1  x_2  x_2^2  \\dots  x_2^d \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  x_{N_{\\text{train}}}  x_{N_{\\text{train}}}^2  \\dots  x_{N_{\\text{train}}}^d\n\\end{pmatrix}\n$$\n最小二乘问题因此等价于找到向量 $\\mathbf{c}$，以最小化残差向量的欧几里得范数 $\\|\\mathbf{y} - A\\mathbf{c}\\|_2$。\n\n虽然这个最小化问题可以通过正规方程 $A^TA\\mathbf{c} = A^T\\mathbf{y}$ 来形式化求解，但这种方法在数值上是不稳定的。矩阵 $A^TA$ 的条件数是 $A$ 的条件数的平方，而范德蒙德矩阵是出了名的病态矩阵，尤其是在高次 $d$ 的情况下。一种数值上更优越的方法是使用矩阵分解，例如对 $A$ 进行奇异值分解（SVD），本题将采用此方法。现代数值库提供了基于这类方法的鲁棒线性最小二乘求解器。我们将为此使用 `numpy.linalg.lstsq`。\n\n每个测试用例的算法流程如下：\n1.  在 $[x_{\\min}, x_{\\max}]$ 上均匀生成 $N_{\\text{train}}$ 个训练点 $x_i$，并计算对应的目标值 $y_i = \\sin(x_i)$。\n2.  构建 $N_{\\text{train}} \\times (d+1)$ 的范德蒙德矩阵 $A$，其中 $A_{ij} = x_i^j$，$i=1, \\dots, N_{\\text{train}}$ 且 $j=0, \\dots, d$。\n3.  求解线性最小二乘系统 $A\\mathbf{c} \\approx \\mathbf{y}$，得到系数向量 $\\mathbf{c}$。\n4.  计算样本内均方根误差（$\\text{RMSE}_{\\text{train}}$）。训练集上的预测值为 $\\hat{y}_i = p_d(x_i)$。\n    $$\n    \\text{RMSE}_{\\text{train}} = \\sqrt{\\frac{1}{N_{\\text{train}}}\\sum_{i=1}^{N_{\\text{train}}} (\\hat{y}_i - y_i)^2}\n    $$\n5.  在外推区间 $[u_{\\min}, u_{\\max}]$ 上均匀生成 $N_{\\text{test}} = 200$ 个测试点 $u_k$，并计算真实函数值 $v_k = \\sin(u_k)$。\n6.  在测试点上评估多项式 $p_d(x)$，得到预测值 $\\hat{v}_k = p_d(u_k)$。\n7.  计算样本外均方根误差（$\\text{RMSE}_{\\text{test}}$）：\n    $$\n    \\text{RMSE}_{\\text{test}} = \\sqrt{\\frac{1}{N_{\\text{test}}}\\sum_{k=1}^{N_{\\text{test}}} (\\hat{v}_k - v_k)^2}\n    $$\n8.  应用决策规则：如果 $\\text{RMSE}_{\\text{train}} \\le \\varepsilon_{\\text{in}}$ 且 $\\text{RMSE}_{\\text{test}} \\ge \\varepsilon_{\\text{out}}$，其中 $\\varepsilon_{\\text{in}} = 10^{-2}$ 且 $\\varepsilon_{\\text{out}} = 5 \\times 10^{-1}$，则该测试用例的结果为 $1$。否则，结果为 $0$。\n\n将此程序系统地应用于所有指定的测试用例，以确定最终输出。高次多项式模型预计能很好地拟合正弦波的单个周期，但由于多项式是非周期性的，并且当 $|x| \\to \\infty$ 时会无界增长，因此在远离原点的区间上，它们将与周期性的正弦函数产生急剧偏离，从而导致较大的 $\\text{RMSE}_{\\text{test}}$。低次线性模型（用例4）的灵活性不足以捕捉正弦函数的曲率，导致样本内拟合效果差，从而不满足决策规则的第一个条件。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes polynomial least squares approximation to demonstrate\n    the failure of extrapolation for non-periodic models of periodic functions.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: d=9, N_train=64, [x_min,x_max]=[0,2pi], [u_min,u_max]=[10pi,12pi]\n        {'d': 9, 'N_train': 64, 'x_range': [0, 2 * np.pi], 'u_range': [10 * np.pi, 12 * np.pi]},\n        # Case 2: d=15, N_train=100, [x_min,x_max]=[0,2pi], [u_min,u_max]=[8pi,10pi]\n        {'d': 15, 'N_train': 100, 'x_range': [0, 2 * np.pi], 'u_range': [8 * np.pi, 10 * np.pi]},\n        # Case 3: d=20, N_train=120, [x_min,x_max]=[0,2pi], [u_min,u_max]=[-12pi,-10pi]\n        {'d': 20, 'N_train': 120, 'x_range': [0, 2 * np.pi], 'u_range': [-12 * np.pi, -10 * np.pi]},\n        # Case 4: d=1, N_train=64, [x_min,x_max]=[0,2pi], [u_min,u_max]=[8pi,10pi]\n        {'d': 1, 'N_train': 64, 'x_range': [0, 2 * np.pi], 'u_range': [8 * np.pi, 10 * np.pi]},\n    ]\n\n    # Constants defined in the problem\n    N_test = 200\n    eps_in = 1e-2\n    eps_out = 0.5\n    \n    results = []\n\n    for case in test_cases:\n        d = case['d']\n        N_train = case['N_train']\n        x_min, x_max = case['x_range']\n        u_min, u_max = case['u_range']\n\n        # 1. Generate training data\n        x_train = np.linspace(x_min, x_max, N_train)\n        y_train = np.sin(x_train)\n\n        # 2. Construct the Vandermonde matrix for the training data\n        # The polynomial is p(x) = c_0 + c_1*x + ... + c_d*x^d\n        # increasing=True makes columns 1, x, x^2, ...\n        A_train = np.vander(x_train, N=d + 1, increasing=True)\n        \n        # 3. Solve for the polynomial coefficients c = [c_0, c_1, ..., c_d]\n        # np.linalg.lstsq provides a numerically stable SVD-based solution.\n        coeffs, _, _, _ = np.linalg.lstsq(A_train, y_train, rcond=None)\n\n        # 4. Compute in-sample RMSE\n        # Evaluate polynomial on training points\n        y_pred_train = A_train @ coeffs\n        rmse_train = np.sqrt(np.mean((y_pred_train - y_train)**2))\n\n        # 5. Generate test data\n        x_test = np.linspace(u_min, u_max, N_test)\n        y_test_true = np.sin(x_test)\n\n        # 6. Compute out-of-sample RMSE\n        # Evaluate polynomial on test points.\n        # np.polyval expects coefficients for descending powers (c_d, c_{d-1}, ...),\n        # so we reverse the 'coeffs' array.\n        y_pred_test = np.polyval(coeffs[::-1], x_test)\n        rmse_test = np.sqrt(np.mean((y_pred_test - y_test_true)**2))\n        \n        # 7. Apply the decision rule\n        if rmse_train = eps_in and rmse_test >= eps_out:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3262865"}]}