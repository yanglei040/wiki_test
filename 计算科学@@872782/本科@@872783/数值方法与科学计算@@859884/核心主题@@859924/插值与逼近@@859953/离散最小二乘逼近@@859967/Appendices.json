{"hands_on_practices": [{"introduction": "为了真正掌握离散最小二乘法的精髓，我们必须回到其基本原理：最小化残差平方和。这个练习将引导你手动推导一组特定约束条件下的正规方程。通过解决一个二次多项式必须经过原点的拟合问题 ([@problem_id:3223314])，你将体会到模型假设是如何直接影响求解过程的，这比单纯套用标准公式更能加深理解。", "problem": "考虑由$7$个点$(x_i,y_i)$组成的离散数据集，这些点为\n$(x_1,y_1)=(-3,18)$、$(x_2,y_2)=(-2,9)$、$(x_3,y_3)=(-1,4)$、$(x_4,y_4)=(0,3)$、$(x_5,y_5)=(1,6)$、$(x_6,y_6)=(2,13)$、$(x_7,y_7)=(3,24)$。这些值来自于在所列$x$值上采样的基本关系$y=2x^2+x+3$。设$p(x)$是一个受限于通过原点的二次多项式，即$p(0)=0$，并将其写作$p(x)=a x^2 + b x$。\n\n仅使用离散最小二乘逼近的基本定义——即$p(x)$在给定数据上最小化残差平方和$\\sum_{i=1}^{7} \\big(p(x_i)-y_i\\big)^2$——确定系数$a$和$b$，并给出满足$p(0)=0$的最佳拟合二次多项式$p(x)$的显式解析表达式。以精确形式表示您的最终答案；无需四舍五入。", "solution": "问题是要求解二次多项式$p(x) = ax^2 + bx$的系数$a$和$b$，使得该多项式对给定的7个数据点提供最佳的离散最小二乘逼近。该多项式被约束通过原点，其形式$p(0) = a(0)^2 + b(0) = 0$满足此约束。\n\n最小二乘法的基本原理是最小化残差平方和$E$，$E$是模型预测值$p(x_i)$与观测数据值$y_i$之间差的平方和。误差函数$E(a,b)$由下式给出：\n$$E(a,b) = \\sum_{i=1}^{7} \\left(p(x_i) - y_i\\right)^2 = \\sum_{i=1}^{7} \\left(ax_i^2 + bx_i - y_i\\right)^2$$\n为了找到使该函数最小化的$a$和$b$的值，我们必须找到$E$关于$a$和$b$的偏导数都等于零的临界点。\n$$ \\frac{\\partial E}{\\partial a} = 0 \\quad \\text{和} \\quad \\frac{\\partial E}{\\partial b} = 0 $$\n计算关于$a$的偏导数：\n$$ \\frac{\\partial E}{\\partial a} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial a}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i^2) $$\n令其为零并除以$2$：\n$$ \\sum_{i=1}^{7} (ax_i^4 + bx_i^3 - y_i x_i^2) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^4\\right) + b\\left(\\sum_{i=1}^{7} x_i^3\\right) = \\sum_{i=1}^{7} y_i x_i^2 $$\n这是两个正规方程中的第一个。\n\n计算关于$b$的偏导数：\n$$ \\frac{\\partial E}{\\partial b} = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i) \\cdot \\frac{\\partial}{\\partial b}(ax_i^2 + bx_i - y_i) = \\sum_{i=1}^{7} 2(ax_i^2 + bx_i - y_i)(x_i) $$\n令其为零并除以$2$：\n$$ \\sum_{i=1}^{7} (ax_i^3 + bx_i^2 - y_i x_i) = 0 $$\n$$ a\\left(\\sum_{i=1}^{7} x_i^3\\right) + b\\left(\\sum_{i=1}^{7} x_i^2\\right) = \\sum_{i=1}^{7} y_i x_i $$\n这是第二个正规方程。\n\n下一步是从给定的数据集中计算所需的和：\n$(x_1, y_1)=(-3, 18)$、$(x_2, y_2)=(-2, 9)$、$(x_3, y_3)=(-1, 4)$、$(x_4, y_4)=(0, 3)$、$(x_5, y_5)=(1, 6)$、$(x_6, y_6)=(2, 13)$、$(x_7, y_7)=(3, 24)$。\n$x$值的集合是$\\{ -3, -2, -1, 0, 1, 2, 3 \\}$。这个集合关于$x=0$对称，这使得$x_i$的奇次幂的和简化为零。\n\n我们计算各项和：\n$$ \\sum_{i=1}^{7} x_i^2 = (-3)^2 + (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 + 3^2 = 9 + 4 + 1 + 0 + 1 + 4 + 9 = 28 $$\n$$ \\sum_{i=1}^{7} x_i^3 = (-3)^3 + (-2)^3 + (-1)^3 + 0^3 + 1^3 + 2^3 + 3^3 = -27 - 8 - 1 + 0 + 1 + 8 + 27 = 0 $$\n$$ \\sum_{i=1}^{7} x_i^4 = (-3)^4 + (-2)^4 + (-1)^4 + 0^4 + 1^4 + 2^4 + 3^4 = 81 + 16 + 1 + 0 + 1 + 16 + 81 = 196 $$\n接下来，我们计算包含$y_i$的和：\n$$ \\sum_{i=1}^{7} y_i x_i = (18)(-3) + (9)(-2) + (4)(-1) + (3)(0) + (6)(1) + (13)(2) + (24)(3) \\\\ = -54 - 18 - 4 + 0 + 6 + 26 + 72 = -76 + 104 = 28 $$\n$$ \\sum_{i=1}^{7} y_i x_i^2 = (18)(-3)^2 + (9)(-2)^2 + (4)(-1)^2 + (3)(0)^2 + (6)(1)^2 + (13)(2)^2 + (24)(3)^2 \\\\ = 18(9) + 9(4) + 4(1) + 3(0) + 6(1) + 13(4) + 24(9) \\\\ = 162 + 36 + 4 + 0 + 6 + 52 + 216 = 476 $$\n现在，我们将这些数值代入正规方程：\n1. $a(196) + b(0) = 476$\n2. $a(0) + b(28) = 28$\n\n这就形成了一个关于$a$和$b$的简单的、解耦的线性方程组。\n由第一个方程可得：\n$$ 196a = 476 \\implies a = \\frac{476}{196} $$\n为了简化分数，我们可以用公因数去除分子和分母。两者都可以被$4$整除：\n$$ a = \\frac{476 \\div 4}{196 \\div 4} = \\frac{119}{49} $$\n现在我们发现$119 = 7 \\times 17$且$49 = 7 \\times 7$：\n$$ a = \\frac{7 \\times 17}{7 \\times 7} = \\frac{17}{7} $$\n由第二个方程可得：\n$$ 28b = 28 \\implies b = 1 $$\n系数为$a = \\frac{17}{7}$和$b = 1$。得到的最佳拟合二次多项式是：\n$$ p(x) = \\frac{17}{7}x^2 + x $$\n题目说明数据来自于关系式$y = 2x^2 + x + 3$。最小二乘拟合精确地得到了系数$b=1$。这是因为对于对称的$x_i$值集合和由$y_i=f(x_i)$生成的数据，函数偶部和奇部之间的互相关项会消失。系数$a$是$\\frac{17}{7} \\approx 2.428$，这与原始系数$2$不同，因为模型$p(x)$被强制在$x=0$时为$0$，而数据点是$(0,3)$。最小二乘法调整曲率参数$a$以最小化所有点上的总平方误差，以适应在原点处的较大残差。", "answer": "$$\\boxed{p(x) = \\frac{17}{7}x^2 + x}$$", "id": "3223314"}, {"introduction": "现实世界中的许多关系并非简单的线性或多项式形式。本练习将向你介绍一个强大的技巧：通过对数变换将幂律关系 ($y = ax^k$) 这样的非线性模型线性化 ([@problem_id:3223297])。这样一来，我们就可以应用标准的线性最小二乘法来估计模型参数，极大地扩展了该方法的应用范围。", "problem": "给定一组离散数据点，假设它们遵循 $y = a x^k$ 形式的幂律关系，其中 $x > 0$ 且 $y > 0$。任务是使用离散最小二乘逼近法，通过自然对数将模型线性化为 $\\ln(y) = \\ln(a) + k \\log(x)$，来估计参数 $a$ 和 $k$。此推导的基本原理是，离散最小二乘逼近法寻找的参数能够最小化残差平方和。对于线性模型 $m(u) = \\beta_0 + \\beta_1 u$，残差平方和由目标函数 $\\sum_{i=1}^n (v_i - m(u_i))^2$ 定义，其中 $(u_i,v_i)$ 是变换后的数据。您必须：\n- 使用自然对数（底为 $e$）在变换空间中构建离散最小二乘问题。\n- 从最小化残差平方和的基本原理出发，推导出刻画线性化模型最小化解的条件，然后实现一个算法来计算给定数据集的估计值 $\\hat{a}$ 和 $\\hat{k}$。\n- 使用以下测试数据集，每个数据集都指定为一个 $x$ 值列表和一个对应的 $y$ 值列表。所有 $x$ 和 $y$ 均为严格正实数。\n\n测试数据集：\n1. 理想情况（具有整数指数的精确幂律）：\n   - $x$: $[1,2,3,4,5]$\n   - $y$: $[3,12,27,48,75]$\n2. 亚线性幂律关系附近的含噪声测量值：\n   - $x$: $[0.2,0.5,0.8,1.1,1.4,1.7,2.0,2.3]$\n   - $y$: $[0.4145670020285,0.6236681800069,0.8130343166190,0.9439279633550,1.0489213395436,1.1969255615947,1.2600642840744,1.3990405194275]$\n3. 负指数和小 $x$ 值（宽动态范围，精确值）：\n   - $x$: $[0.01,0.02,0.05,0.1,0.2,0.5]$\n   - $y$: $[199.0535852767,114.2585902281,54.9185573277,31.5478672240,18.1194915919,8.70550563296]$\n4. 重复的 $x$ 值，大动态范围和轻微测量噪声：\n   - $x$: $[1,1,10,10,100,1000]$\n   - $y$: $[1.25,1.2575,10.0283949630875,9.87945741908125,80.4460614212,620.21920059]$\n\n您的程序必须：\n- 对每个测试用例，通过自然对数变换数据，建立线性最小二乘问题，求解变换空间中线性模型的参数，然后将解映射回原始空间中 $a$ 和 $k$ 的估计值。\n- 生成单行输出，其中包含所有测试用例的结果，形式为一个用方括号括起来的逗号分隔列表，其中每个测试用例的结果本身是一个 $[\\hat{a},\\hat{k}]$ 形式的双元素列表。每个 $\\hat{a}$ 和 $\\hat{k}$ 都必须打印为四舍五入到六位小数的浮点数。\n\n最终输出格式必须严格为：\n- 一行，除了分隔逗号内数字所需的空格外，没有多余的空格，呈现为 $[[\\hat{a}_1,\\hat{k}_1],[\\hat{a}_2,\\hat{k}_2],[\\hat{a}_3,\\hat{k}_3],[\\hat{a}_4,\\hat{k}_4]]$。\n\n全程使用自然对数（即底为 $e$）。不涉及物理单位、角度或百分比；所有量均为无量纲实数。程序必须是自包含的，不需要输入，并且仅使用与通过线性化实现的离散最小二乘逼近一致的数值运算。", "solution": "问题在于，对于给定的一组数据点 $(x_i, y_i)$（$i=1, \\dots, n$），确定幂律模型 $y = a x^k$ 中能最佳拟合这些数据点的参数 $a$ 和 $k$。问题指明数据满足 $x_i > 0$ 和 $y_i > 0$。所用方法是对模型的线性化版本应用离散最小二乘逼近。\n\n**步骤1：问题构建与线性化**\n给定的幂律关系为：\n$$y = a x^k$$\n为将此模型线性化，我们对等式两边取自然对数（底为 $e$）。由于 $x$ 和 $y$ 严格为正，此操作是允许的。\n$$\\ln(y) = \\ln(a x^k)$$\n利用对数性质 $\\ln(AB) = \\ln(A) + \\ln(B)$ 和 $\\ln(A^p) = p \\ln(A)$，我们得到：\n$$\\ln(y) = \\ln(a) + \\ln(x^k)$$\n$$\\ln(y) = \\ln(a) + k \\ln(x)$$\n该方程呈线性关系形式。为明确起见，我们引入变量替换。令：\n- $v = \\ln(y)$\n- $u = \\ln(x)$\n- $\\beta_0 = \\ln(a)$\n- $\\beta_1 = k$\n\n将这些变量代入变换后的方程，得到线性模型：\n$$v = \\beta_0 + \\beta_1 u$$\n给定一组 $n$ 个数据点 $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$，我们可以将其变换为一组新点 $(u_1, v_1), (u_2, v_2), \\dots, (u_n, v_n)$，其中 $u_i = \\ln(x_i)$ 且 $v_i = \\ln(y_i)$。我们的目标是找到定义这些变换后数据点的最佳拟合直线的参数 $\\beta_0$ 和 $\\beta_1$。\n\n**步骤2：离散最小二乘原理**\n离散最小二乘原理指出，最佳拟合直线是使残差平方和最小化的那条直线。第 $i$ 个数据点的残差是观测值 $v_i$ 与模型预测值 $\\beta_0 + \\beta_1 u_i$ 之间的差。残差平方和 $S$ 是参数 $\\beta_0$ 和 $\\beta_1$ 的函数：\n$$S(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (v_i - (\\beta_0 + \\beta_1 u_i))^2$$\n\n**步骤3：正规方程组的推导**\n为了找到使 $S$ 最小化的 $\\beta_0$ 和 $\\beta_1$ 的值，我们应用微积分中的一个标准方法：求 $S$ 关于 $\\beta_0$ 和 $\\beta_1$ 的偏导数，并令它们等于零。这样可以确定函数 $S$ 的驻点。\n\n首先，关于 $\\beta_0$ 的偏导数：\n$$ \\frac{\\partial S}{\\partial \\beta_0} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_0} (v_i - \\beta_0 - \\beta_1 u_i)^2 = \\sum_{i=1}^{n} 2(v_i - \\beta_0 - \\beta_1 u_i)(-1) $$\n令其等于零：\n$$ -2 \\sum_{i=1}^{n} (v_i - \\beta_0 - \\beta_1 u_i) = 0 $$\n$$ \\sum_{i=1}^{n} v_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 u_i = 0 $$\n$$ \\sum_{i=1}^{n} v_i - n\\beta_0 - \\beta_1 \\sum_{i=1}^{n} u_i = 0 $$\n整理后得到第一个正规方程：\n$$ n\\beta_0 + \\left(\\sum_{i=1}^{n} u_i\\right) \\beta_1 = \\sum_{i=1}^{n} v_i $$\n\n其次，关于 $\\beta_1$ 的偏导数：\n$$ \\frac{\\partial S}{\\partial \\beta_1} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta_1} (v_i - \\beta_0 - \\beta_1 u_i)^2 = \\sum_{i=1}^{n} 2(v_i - \\beta_0 - \\beta_1 u_i)(-u_i) $$\n令其等于零：\n$$ -2 \\sum_{i=1}^{n} u_i(v_i - \\beta_0 - \\beta_1 u_i) = 0 $$\n$$ \\sum_{i=1}^{n} (u_i v_i - \\beta_0 u_i - \\beta_1 u_i^2) = 0 $$\n$$ \\sum_{i=1}^{n} u_i v_i - \\beta_0 \\sum_{i=1}^{n} u_i - \\beta_1 \\sum_{i=1}^{n} u_i^2 = 0 $$\n整理后得到第二个正规方程：\n$$ \\left(\\sum_{i=1}^{n} u_i\\right) \\beta_0 + \\left(\\sum_{i=1}^{n} u_i^2\\right) \\beta_1 = \\sum_{i=1}^{n} u_i v_i $$\n\n**步骤4：求解正规方程组**\n现在我们得到了一个关于两个未知数 $\\beta_0$ 和 $\\beta_1$ 的二元一次方程组：\n$$\n\\begin{cases}\nn\\beta_0 + (\\sum u_i) \\beta_1 = \\sum v_i \\\\\n(\\sum u_i) \\beta_0 + (\\sum u_i^2) \\beta_1 = \\sum u_i v_i\n\\end{cases}\n$$\n求解这个方程组（例如，使用克莱姆法则或代入法）可以得到 $\\beta_0$ 和 $\\beta_1$ 的解。解的分母是系数矩阵的行列式 $D = n(\\sum u_i^2) - (\\sum u_i)^2$。$\\beta_1$ 的解为：\n$$ \\beta_1 = \\frac{n(\\sum u_i v_i) - (\\sum u_i)(\\sum v_i)}{n(\\sum u_i^2) - (\\sum u_i)^2} $$\n一旦 $\\beta_1$ 已知，我们就可以从第一个正规方程中求出 $\\beta_0$：\n$$ \\beta_0 = \\frac{1}{n} \\left( \\sum v_i - \\beta_1 \\sum u_i \\right) = \\bar{v} - \\beta_1 \\bar{u} $$\n其中 $\\bar{u} = \\frac{1}{n}\\sum u_i$ 和 $\\bar{v} = \\frac{1}{n}\\sum v_i$ 是算术平均值。\n\n**步骤5：映射回原始参数**\n原始参数的估计值（记作 $\\hat{a}$ 和 $\\hat{k}$）可以通过逆转变量替换得到：\n$$ \\hat{k} = \\beta_1 $$\n从 $\\beta_0 = \\ln(a)$，我们得到：\n$$ \\hat{a} = e^{\\beta_0} $$\n这个过程提供了幂律模型参数的估计值，这些估计值能够最小化对数变换空间中的误差平方和。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for power-law parameters a and k for given datasets using\n    discrete least squares on the linearized model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Happy path (exact power-law with integer exponent)\n        (\n            [1, 2, 3, 4, 5],\n            [3, 12, 27, 48, 75]\n        ),\n        # 2. Noisy measurements around a sub-linear power-law\n        (\n            [0.2, 0.5, 0.8, 1.1, 1.4, 1.7, 2.0, 2.3],\n            [0.4145670020285, 0.6236681800069, 0.8130343166190, 0.9439279633550,\n             1.0489213395436, 1.1969255615947, 1.2600642840744, 1.3990405194275]\n        ),\n        # 3. Negative exponent with small x (wide dynamic range, exact values)\n        (\n            [0.01, 0.02, 0.05, 0.1, 0.2, 0.5],\n            [199.0535852767, 114.2585902281, 54.9185573277, 31.5478672240,\n             18.1194915919, 8.70550563296]\n        ),\n        # 4. Repeated x values with large dynamic range and mild measurement noise\n        (\n            [1, 1, 10, 10, 100, 1000],\n            [1.25, 1.2575, 10.0283949630875, 9.87945741908125,\n             80.4460614212, 620.21920059]\n        )\n    ]\n\n    results = []\n    for x_vals, y_vals in test_cases:\n        # Convert data to numpy arrays for vectorized operations\n        x = np.array(x_vals, dtype=np.float64)\n        y = np.array(y_vals, dtype=np.float64)\n\n        # Linearize the model: log(y) = log(a) + k*log(x)\n        # Transformed variables: v = log(y), u = log(x)\n        # Linear model: v = beta0 + beta1*u, where beta0 = log(a), beta1 = k\n        u = np.log(x)\n        v = np.log(y)\n\n        # Number of data points\n        n = len(u)\n\n        # Calculate the necessary sums for the normal equations\n        sum_u = np.sum(u)\n        sum_v = np.sum(v)\n        sum_uv = np.sum(u * v)\n        sum_u_sq = np.sum(u**2)\n\n        # Solve the normal equations for beta0 and beta1\n        # beta1 = (n * sum_uv - sum_u * sum_v) / (n * sum_u_sq - sum_u**2)\n        # beta0 = (sum_v - beta1 * sum_u) / n\n        \n        denominator = n * sum_u_sq - sum_u**2\n        \n        # This condition is met if not all x_i are the same\n        if denominator == 0:\n            # Should not happen with the given test data\n            raise ValueError(\"All x values are the same; cannot solve.\")\n\n        beta1 = (n * sum_uv - sum_u * sum_v) / denominator\n        beta0 = (sum_v - beta1 * sum_u) / n\n\n        # Map beta0 and beta1 back to the original power-law parameters a and k\n        # k = beta1\n        # a = exp(beta0)\n        k_hat = beta1\n        a_hat = np.exp(beta0)\n        \n        # Round the results to six decimal places as required\n        a_rounded = round(a_hat, 6)\n        k_rounded = round(k_hat, 6)\n        \n        results.append([a_rounded, k_rounded])\n    \n    # Format the final output string exactly as specified, without extra spaces\n    # Example format: [[a1,k1],[a2,k2],[a3,k3],[a4,k4]]\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_string = f\"[{','.join(formatted_results)}]\"\n\n    print(final_string)\n\nsolve()\n```", "id": "3223297"}, {"introduction": "理论上等价的算法在实际计算中可能表现出截然不同的性能，其关键在于数值稳定性。这个练习将通过一个经典案例——比较正规方程法和QR分解法——来揭示这一问题 ([@problem_id:3223242])。你将看到，当数据点非常接近导致范德蒙德矩阵病态时，数值上更稳健的QR分解法为何是更优越的选择。", "problem": "您将执行一项离散数据拟合任务，其中使用离散最小二乘逼近将一个多项式模型拟合到一组数据点上。目标是检验极其接近但又不同的横坐标如何影响数值稳定性。您必须实现两种求解器，并通过精心选择的度量指标来量化其对稳定性的影响。\n\n从以下基本原理开始：给定一组点 $\\{(x_i,y_i)\\}_{i=1}^n$ 和一个 $m$ 次多项式模型，构建设计矩阵（范德蒙矩阵）$V$，其 $(i,j)$ 项为 $x_i^{j-1}$，其中 $i=1,\\dots,n$ 且 $j=1,\\dots,m+1$。那么，离散最小二乘逼近问题就是寻找系数 $c \\in \\mathbb{R}^{m+1}$，以最小化残差范数的平方 $\\lVert Vc - y \\rVert_2^2$，其中 $y \\in \\mathbb{R}^n$ 是观测值向量。\n\n您的程序必须：\n- 使用两种方法计算多项式系数：\n  1. 求解与残差平方最小化相关的一阶最优性条件，使用最优性条件的矩阵获得系数向量 $c_{\\mathrm{NE}}$（这种方法通常称为通过正规方程求解）。\n  2. 使用 QR 分解（正交三角分解），将 $V$ 写成 $V = QR$，其中 $Q$ 为正交矩阵，$R$ 为上三角矩阵，然后通过回代法求解系数向量 $c_{\\mathrm{QR}}$。\n- 通过计算以下指标来量化数值稳定性：\n  - 范德蒙矩阵 $V$ 的 $2$-范数条件数 $\\kappa_2(V)$。\n  - 矩阵 $V^\\top V$ 的 $2$-范数条件数 $\\kappa_2(V^\\top V)$。\n  - 两个系数向量之间的相对差异：$\\dfrac{\\lVert c_{\\mathrm{NE}} - c_{\\mathrm{QR}} \\rVert_2}{\\lVert c_{\\mathrm{QR}} \\rVert_2}$。\n  - 残差范数的绝对差异：$\\big|\\lVert y - V c_{\\mathrm{NE}} \\rVert_2 - \\lVert y - V c_{\\mathrm{QR}} \\rVert_2\\big|$。\n\n使用底层三次多项式 $p(x) = 1 - 2x + 3x^2 - x^3$作为生成模型，并通过 $y_i = p(x_i) + \\delta_i$ 生成数据，其中 $\\delta_i = 10^{-8}(i+1)$，对于 $i \\in \\{0,1,\\dots,n-1\\}$。此问题中没有随机性；所有值都是确定性的。没有物理单位。\n\n为以下测试套件实现解决方案，每个案例指定了横坐标 $x_i$ 和次数 $m$：\n- 案例 1（点分布均匀，理想情况）：$x = [\\,0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0\\,]$，$m = 3$。\n- 案例 2（两个点极其接近）：$x = [\\,0.0,\\,1.0,\\,1.0 + 10^{-12},\\,2.0,\\,3.0\\,]$，$m = 3$。\n- 案例 3（点更接近，接近双精度浮点数的相等极限）：$x = [\\,-1.0,\\,0.0,\\,1.0,\\,1.0 + 10^{-15},\\,2.0\\,]$，$m = 3$。\n\n对于每个案例，您的程序必须生成一个列表 $[\\,\\kappa_2(V),\\,\\kappa_2(V^\\top V),\\,\\text{relative\\_coefficient\\_difference},\\,\\text{residual\\_norm\\_difference}\\,]$。最终输出应为单行文本，包含一个由这些列表组成的逗号分隔列表，并用方括号括起来，格式如 $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]]$，其中所有的 $a_k$、$b_k$、$c_k$ 和 $d_k$ 均为程序计算出的浮点数。\n\n程序不应读取任何用户输入。输出必须是确定性且可复现的。不涉及角度，也没有物理单位。所有量都应表示为浮点数。最终输出格式必须严格遵守上述规定，为单行打印输出。", "solution": "本任务旨在分析用于求解离散多项式最小二乘问题的两种不同方法的数值稳定性：正规方程法和 QR 分解法。我们将在输入数据点极其接近的条件下评估其稳定性，众所周知，这种情况会导致问题设计矩阵的病态。\n\n离散最小二乘逼近问题旨在寻找一个多项式 $p_m(x) = \\sum_{j=0}^{m} c_j x^j$ 的系数 $c \\in \\mathbb{R}^{m+1}$，使其能最佳拟合给定的一组共 $n$ 个数据点 $\\{(x_i, y_i)\\}_{i=0}^{n-1}$。“最佳拟合”的定义是最小化残差平方和 $S(c) = \\sum_{i=0}^{n-1} (p_m(x_i) - y_i)^2$。该目标函数可以写成矩阵形式，即最小化残差向量的 2-范数平方：\n$$\nS(c) = \\lVert Vc - y \\rVert_2^2\n$$\n其中 $y = [y_0, y_1, \\dots, y_{n-1}]^\\top$ 是观测值向量，$c = [c_0, c_1, \\dots, c_m]^\\top$ 是多项式系数向量，$V$ 是一个 $n \\times (m+1)$ 的范德蒙矩阵，其元素定义为 $V_{ij} = x_i^j$，其中 $i \\in \\{0, \\dots, n-1\\}$ 且 $j \\in \\{0, \\dots, m\\}$。\n\n问题指定使用生成模型 $p(x) = 1 - 2x + 3x^2 - x^3$ 来创建数据。观测值 $y_i$ 由 $y_i = p(x_i) + \\delta_i$ 给出，其中 $\\delta_i = 10^{-8}(i+1)$ 是一个小的确定性扰动。我们将比较两种寻找系数向量 $c$ 的方法。\n\n**方法 1：正规方程**\n\n残差平方和 $S(c)$ 的最小值出现在其关于 $c$ 的梯度为零处。该梯度为：\n$$\n\\nabla_c S(c) = \\nabla_c (Vc - y)^\\top(Vc - y) = \\nabla_c (c^\\top V^\\top V c - 2y^\\top V c + y^\\top y) = 2V^\\top V c - 2V^\\top y\n$$\n将梯度设为零，$\\nabla_c S(c) = 0$，得到称为正规方程的线性方程组：\n$$\n(V^\\top V) c = V^\\top y\n$$\n通过求解这个 $(m+1) \\times (m+1)$ 的方程组，可以得到解，我们记为 $c_{\\mathrm{NE}}$。这种方法的一个关键问题是矩阵 $V^\\top V$ 的条件数是矩阵 $V$ 条件数的平方：$\\kappa_2(V^\\top V) = [\\kappa_2(V)]^2$。如果 $V$ 是病态的（即条件数很大），$V^\\top V$ 将会是严重病态的。这可能导致计算出的解 $c_{\\mathrm{NE}}$ 对扰动和浮点误差高度敏感。\n\n**方法 2：QR 分解**\n\n这种方法避免了显式构造 $V^\\top V$。它依赖于设计矩阵 $V$ 的 QR 分解，将其分解为乘积 $V=QR$，其中 $Q$ 是一个具有标准正交列的 $n \\times (m+1)$ 矩阵（$Q^\\top Q = I_{m+1}$），$R$ 是一个 $(m+1) \\times (m+1)$ 的上三角矩阵。这就是所谓的“瘦”QR 分解或“简化”QR 分解。\n\n将 $V=QR$ 代入目标函数，我们得到：\n$$\n\\lVert Vc - y \\rVert_2^2 = \\lVert QRc - y \\rVert_2^2\n$$\n由于 $Q$ 具有标准正交列，从左侧乘以 $Q^\\top$ 不会改变 $Q$ 列空间中向量的 2-范数。虽然完整的论证需要使用完全 QR 分解，但其结果是该最小化问题等价于求解以下上三角方程组：\n$$\nRc = Q^\\top y\n$$\n这个方程组可以使用回代法高效且稳定地求解，得到系数向量，记为 $c_{\\mathrm{QR}}$。该方法的数值优势在于，它操作的矩阵（$Q$ 和 $R$）的条件数与原始矩阵 $V$ 相关，而不是与 $V$ 的平方相关。因此，对于求解最小二乘问题，QR 分解通常比正规方程法在数值上稳定得多。\n\n**数值稳定性分析**\n\n为了量化数值稳定性的差异，我们将为每个测试案例计算四个度量指标：\n1. 范德蒙矩阵的 2-范数条件数 $\\kappa_2(V)$。它衡量了最小二乘问题本身的敏感性。\n2. 正规方程矩阵的 2-范数条件数 $\\kappa_2(V^\\top V)$。这突显了条件数的平方效应。\n3. 计算出的系数向量之间的相对差异 $\\dfrac{\\lVert c_{\\mathrm{NE}} - c_{\\mathrm{QR}} \\rVert_2}{\\lVert c_{\\mathrm{QR}} \\rVert_2}$。这衡量了两种方法得出的解的偏离程度。解 $c_{\\mathrm{QR}}$ 被认为是更准确的基准。\n4. 最终残差向量范数的绝对差异 $\\big|\\lVert y - V c_{\\mathrm{NE}} \\rVert_2 - \\lVert y - V c_{\\mathrm{QR}} \\rVert_2\\big|$。一个稳定的算法应该能得出更接近最小化真实残差的解。\n\n测试案例旨在展示随着横坐标 $x_i$ 变得更近，这些度量指标如何变化，这会加剧 $V$ 的病态程度。\n\n**各测试案例的步骤：**\n1. 给定横坐标 $x$ 和多项式次数 $m=3$，使用 $y_i = (1 - 2x_i + 3x_i^2 - x_i^3) + 10^{-8}(i+1)$ 构建 $y$ 值向量。\n2. 构造 $5 \\times 4$ 的范德蒙矩阵 $V$。\n3. 求解 $(V^\\top V)c_{\\mathrm{NE}} = V^\\top y$ 以获得 $c_{\\mathrm{NE}}$。\n4. 计算 QR 分解 $V=QR$ 并求解 $Rc_{\\mathrm{QR}} = Q^\\top y$ 以获得 $c_{\\mathrm{QR}}$。\n5. 计算并记录四个指定的度量指标。\n此过程将为所提供的所有三个测试案例实施。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the discrete least squares problem using Normal Equations and QR factorization\n    for three test cases, and computes metrics to evaluate numerical stability.\n    \"\"\"\n\n    # Generative polynomial: p(x) = 1 - 2x + 3x^2 - x^3\n    # The coefficients are for powers x^0, x^1, x^2, x^3\n    poly_coeffs_gen = np.array([1.0, -2.0, 3.0, -1.0])\n    \n    def p(x_vals, coeffs):\n        \"\"\"Evaluates a polynomial with given coefficients at x_vals.\"\"\"\n        # Using Horner's method implicitly via np.polyval\n        # The coefficients need to be in descending order of power\n        return np.polyval(coeffs[::-1], x_vals)\n\n    test_cases = [\n        # Case 1: Well-spaced points\n        {'x': np.array([0.0, 0.5, 1.0, 1.5, 2.0]), 'm': 3},\n        # Case 2: Two points extremely close\n        {'x': np.array([0.0, 1.0, 1.0 + 1e-12, 2.0, 3.0]), 'm': 3},\n        # Case 3: Closer still, near double precision limits\n        {'x': np.array([-1.0, 0.0, 1.0, 1.0 + 1e-15, 2.0]), 'm': 3},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        x = case['x']\n        m = case['m']\n        n = len(x)\n\n        # 1. Generate data\n        delta = 1e-8 * (np.arange(n) + 1)\n        y = p(x, poly_coeffs_gen) + delta\n\n        # 2. Construct Vandermonde matrix V\n        # V_ij = x_i^j, for j=0,...,m\n        V = np.vander(x, m + 1, increasing=True)\n\n        # 3. Solve with Normal Equations\n        # (V^T V) c = V^T y\n        try:\n            VTV = V.T @ V\n            VTy = V.T @ y\n            c_NE = np.linalg.solve(VTV, VTy)\n        except np.linalg.LinAlgError:\n            # Handle cases where VTV is singular to machine precision\n            c_NE = np.full(m + 1, np.nan)\n\n\n        # 4. Solve with QR factorization\n        # V = QR, solve Rc = Q^T y\n        Q, R = np.linalg.qr(V, mode='reduced')\n        QTY = Q.T @ y\n        c_QR = np.linalg.solve(R, QTY)\n\n        # 5. Calculate metrics\n        # Metric 1: Condition number of V\n        kappa_V = np.linalg.cond(V, 2)\n\n        # Metric 2: Condition number of V^T V\n        kappa_VTV = np.linalg.cond(VTV, 2)\n\n        # Metric 3: Relative difference in coefficients\n        # Use QR solution as the more accurate baseline\n        # Handle NaN case for c_NE\n        if np.any(np.isnan(c_NE)):\n            rel_coeff_diff = np.inf\n        else:\n            norm_c_QR = np.linalg.norm(c_QR, 2)\n            if norm_c_QR == 0:\n                rel_coeff_diff = np.linalg.norm(c_NE - c_QR, 2)\n            else:\n                rel_coeff_diff = np.linalg.norm(c_NE - c_QR, 2) / norm_c_QR\n\n        # Metric 4: Absolute difference in residual norms\n        if np.any(np.isnan(c_NE)):\n            res_norm_diff = np.inf\n        else:\n            resid_NE = y - V @ c_NE\n            resid_QR = y - V @ c_QR\n            norm_resid_NE = np.linalg.norm(resid_NE, 2)\n            norm_resid_QR = np.linalg.norm(resid_QR, 2)\n            res_norm_diff = abs(norm_resid_NE - norm_resid_QR)\n        \n        case_results = [kappa_V, kappa_VTV, rel_coeff_diff, res_norm_diff]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # str(results).replace(' ', '') ensures the output matches the required format\n    # e.g., \"[[a,b,c],[d,e,f]]\" with no spaces.\n    print(str(results).replace(' ', ''))\n\nsolve()\n\n```", "id": "3223242"}]}