{"hands_on_practices": [{"introduction": "非线性最小二乘（NLLS）求解器通过迭代方式逼近最优解。为了揭开这一过程的神秘面纱，我们的第一个练习将任务分解为其最基本的组成部分：计算一个更新步长。通过为放射性衰变模型手动执行单次高斯-牛顿迭代，你将亲身体验算法如何利用局部线性近似（通过雅可比矩阵）来寻找一组更好的参数 [@problem_id:2191241]。这个练习将为你理解更复杂的概念打下坚实的基础。", "problem": "一位实验物理学家正在研究一种新合成同位素的放射性衰变。样本的活度，即每秒的衰变次数，在几个时间点进行了测量。收集到的数据如下：\n\n- 在时间 $t=0.0$ 小时，活度为 $95$ 贝克勒尔 (Bq)。\n- 在时间 $t=1.0$ 小时，活度为 $35$ Bq。\n- 在时间 $t=2.0$ 小时，活度为 $13$ Bq。\n\n活度 $A(t)$ 作为时间 $t$ 的函数的理论模型由指数衰变定律给出：\n$$ A(t; C, \\lambda) = C e^{-\\lambda t} $$\n其中 $C$ 是 $t=0$ 时的初始活度，$\\lambda$ 是衰变常数，单位为小时的倒数。\n\n为了从实验数据中估计参数 $C$ 和 $\\lambda$，该物理学家决定使用高斯-牛顿法进行非线性最小二乘拟合。目标是找到参数矢量 $\\mathbf{x} = (C, \\lambda)^T$，以最小化模型预测值与测量数据之间差值的平方和。\n\n高斯-牛顿算法通过迭代来优化参数估计。从一个初始猜测 $\\mathbf{x}_0$ 开始，参数的更新量 $\\Delta\\mathbf{x}$ 通过求解称为正规方程的线性系统来找到：\n$$ (J^T J) \\Delta\\mathbf{x} = -J^T \\mathbf{f} $$\n其中 $\\mathbf{f}$ 是残差矢量（模型预测值减去测量数据），$J$ 是模型函数关于参数的雅可比矩阵，两者都在当前猜测值处进行计算。然后新的估计值为 $\\mathbf{x}_1 = \\mathbf{x}_0 + \\Delta\\mathbf{x}$。\n\n从初始猜测 $\\mathbf{x}_0 = (C_0, \\lambda_0) = (100.0, 0.900)$ 开始，执行恰好一次高斯-牛顿方法的迭代，以找到更新后的参数估计值 $\\mathbf{x}_1 = (C_1, \\lambda_1)$。\n\n提供更新后参数 $C_1$ 和 $\\lambda_1$ 的数值。将您的最终答案四舍五入到三位有效数字。", "solution": "我们用 $A(t;C,\\lambda)=C\\exp(-\\lambda t)$ 来建模活度。对于数据 $(t_{i},y_{i})$，其中 $(t_{0},y_{0})=(0,95)$，$(t_{1},y_{1})=(1,35)$，$(t_{2},y_{2})=(2,13)$，在 $(C,\\lambda)$ 处的残差为\n$$\nr_{i}(C,\\lambda)=C\\exp(-\\lambda t_{i})-y_{i}.\n$$\n残差矢量关于 $(C,\\lambda)$ 的雅可比矩阵的行向量为\n$$\n\\left[\\frac{\\partial r_{i}}{\\partial C},\\frac{\\partial r_{i}}{\\partial \\lambda}\\right]=\\left[\\exp(-\\lambda t_{i}),-Ct_{i}\\exp(-\\lambda t_{i})\\right].\n$$\n在初始猜测 $\\mathbf{x}_{0}=(C_{0},\\lambda_{0})=(100,0.9)$ 处，定义\n$$\na_{0}=\\exp(-0.9\\cdot 0)=1,\\quad a_{1}=\\exp(-0.9)=0.4065696597405991,\\quad a_{2}=\\exp(-1.8)=0.16529888822158653,\n$$\n并注意 $a_{2}=a_{1}^{2}$。模型值为\n$$\nA(t_{0})=100a_{0}=100,\\quad A(t_{1})=100a_{1}=40.65696597405991,\\quad A(t_{2})=100a_{2}=16.529888822158653,\n$$\n因此残差矢量 $\\mathbf{f}$（模型值减去数据值）为\n$$\n\\mathbf{f}=\\begin{pmatrix}100-95\\\\ 100a_{1}-35\\\\ 100a_{2}-13\\end{pmatrix}=\\begin{pmatrix}5\\\\ 5.65696597405991\\\\ 3.529888822158653\\end{pmatrix}.\n$$\n在 $\\mathbf{x}_{0}$ 处的雅可比矩阵为\n$$\nJ=\\begin{pmatrix}\na_{0}  & -C_{0}\\cdot 0\\cdot a_{0}\\\\\na_{1}  & -C_{0}\\cdot 1\\cdot a_{1}\\\\\na_{2}  & -C_{0}\\cdot 2\\cdot a_{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1  & 0\\\\\na_{1}  & -100a_{1}\\\\\na_{2}  & -200a_{2}\n\\end{pmatrix}.\n$$\n计算 $J^{T}J$ 和 $J^{T}\\mathbf{f}$。使用 $a_{2}=a_{1}^{2}$，\n$$\nJ^{T}J=\\begin{pmatrix}\n1+a_{1}^{2}+a_{2}^{2}  & a_{1}(-100a_{1})+a_{2}(-200a_{2})\\\\\na_{1}(-100a_{1})+a_{2}(-200a_{2})  & (100a_{1})^{2}+(200a_{2})^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1+a_{2}+a_{2}^{2}  & -100a_{2}-200a_{2}^{2}\\\\\n-100a_{2}-200a_{2}^{2}  & 10000a_{2}+40000a_{2}^{2}\n\\end{pmatrix}.\n$$\n数值上，\n$$\nJ^{T}J=\\begin{pmatrix}\n1.1926226106688791  & -21.994633311617165\\\\\n-21.994633311617165  & 2745.937780107568\n\\end{pmatrix}.\n$$\n接下来，\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}\\\\\n0\\cdot r_{0}+(-100a_{1})r_{1}+(-200a_{2})r_{2}\n\\end{pmatrix}.\n$$\n使用 $r_{1}=100a_{1}-35$ 和 $r_{2}=100a_{2}-13$，第一个分量简化为\n$$\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}=5+100a_{1}^2 - 35a_1 +100a_{2}^2 - 13a_2,\n$$\n第二个分量简化为\n$$\n(-100a_{1})r_{1}+(-200a_{2})r_{2}=3500a_{1} - 10000a_1^2 + 2600a_2 - 20000a_2^2.\n$$\n数值上，\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}7.883437429086315\\\\ -346.69241269349484\\end{pmatrix}.\n$$\n高斯-牛顿正规方程为 $(J^{T}J)\\Delta\\mathbf{x}=-J^{T}\\mathbf{f}$。令 $M=J^{T}J$ 且 $\\mathbf{b}=-J^{T}\\mathbf{f}=\\begin{pmatrix}-7.883437429086315\\\\ 346.69241269349484\\end{pmatrix}$。通过求解线性系统 $M\\Delta\\mathbf{x}=\\mathbf{b}$，我们得到\n$$\n\\Delta\\mathbf{x} = \\begin{pmatrix} \\Delta C \\\\ \\Delta\\lambda \\end{pmatrix} \\approx \\begin{pmatrix} -5.0238394 \\\\ 0.0860161 \\end{pmatrix}.\n$$\n更新参数：\n$$\nC_{1}=C_{0}+\\Delta C=100-5.0238394\\approx 94.9762,\\quad \\lambda_{1}=\\lambda_{0}+\\Delta\\lambda=0.9+0.0860161\\approx 0.986016.\n$$\n四舍五入到三位有效数字，\n$$\nC_{1}\\approx 95.0,\\qquad \\lambda_{1}\\approx 0.986.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}95.0 & 0.986\\end{pmatrix}}$$", "id": "2191241"}, {"introduction": "在理解了迭代步骤的运作方式之后，认识到非线性最小二乘法为何“困难”也至关重要。本练习将探讨 NLLS 目标函数的非凸性，这是其与线性最小二乘问题的核心区别 [@problem_id:3256675]。通过一个关于圆拟合的解析练习，你将运用数学方法证明虚假局部最小值的存在，它可能使算法陷入离真实解很远的陷阱，这也凸显了良好初始猜测和稳健算法的必要性。", "problem": "考虑将一个圆拟合到一组平面点的非线性最小二乘 (NLLS) 问题。设该圆由圆心 $(a,b)$ 和半径 $r$ 参数化，并将点 $(x_i,y_i)$ 的残差定义为该点到圆心的欧几里得距离与半径之差，即 $f_i(a,b,r) = \\sqrt{(x_i - a)^{2} + (y_i - b)^{2}} - r$。NLLS 目标函数是残差平方和 $S(a,b,r) = \\sum_{i} f_i(a,b,r)^{2}$。已知圆拟合的 NLLS 目标函数关于 $(a,b,r)$ 是非凸的，并且可能存在伪局部最小值。\n\n为了揭示并量化一个特定的大半径局部最小值，请考虑以下对称数据集\n$$\n(-2,\\,0.16),\\quad (-1,\\,0.04),\\quad (1,\\,0.04),\\quad (2,\\,0.16).\n$$\n将注意力限制在圆心为 $(0,R)$ 且半径为 $R$ 的单参数圆族上，即 $(a,b,r) = (0,R,R)$，因此目标函数简化为标量函数\n$$\nS(R) = \\sum_{i=1}^{4} \\left(\\sqrt{x_i^{2} + (y_i - R)^{2}} - R\\right)^{2}.\n$$\n从 $S(R)$ 的定义和第一性原理出发，使用大 $R$ 渐近展开（在目标函数中保留到 $R^{-2}$ 阶的项）来：\n- 推导 $S(R)$ 关于 $R^{-1}$ 幂次的近似表达式，\n- 证明在此近似下，对于一个有限但大的 $R$，$S(R)$ 存在一个局部最小值，以及\n- 计算给定数据的最小化半径 $R^{\\ast}$ 的主阶值。\n\n请将最终答案表示为单个实数值。无需四舍五入；请报告您推导出的精确数值。", "solution": "本题要求我们为一个特定的非线性最小二乘圆拟合目标函数寻找一个大半径的局部最小值。圆的圆心被约束在 $(a, b) = (0, R)$，半径为 $r=R$。需要最小化的目标函数由下式给出\n$$\nS(R) = \\sum_{i=1}^{4} \\left(\\sqrt{x_i^{2} + (y_i - R)^{2}} - R\\right)^{2}\n$$\n数据集为 $\\{(-2, 0.16), (-1, 0.04), (1, 0.04), (2, 0.16)\\}$。根据题目要求，我们需要使用大 $R$ 渐近展开，并在目标函数 $S(R)$ 中保留至 $R^{-2}$ 阶的项。\n\n我们首先分析单个点 $(x_i, y_i)$ 的残差项的平方 $f_i(R)^2 = (\\sqrt{x_i^{2} + (y_i - R)^{2}} - R)^{2}$。对于大的 $R$，我们使用泰勒级数展开 $\\sqrt{1+u} = 1 + \\frac{1}{2}u - \\frac{1}{8}u^2 + O(u^3)$，其中 $u = \\frac{x_i^2 + y_i^2 - 2y_i R}{R^2}$。\n残差项 $f_i(R)$ 可以展开为：\n$$\nf_i(R) = R \\sqrt{1 - \\frac{2y_i}{R} + \\frac{x_i^2+y_i^2}{R^2}} - R = R \\left(1 - \\frac{y_i}{R} + \\frac{x_i^2+y_i^2}{2R^2} - \\frac{y_i^2}{2R^2} + O(R^{-3}) \\right) - R = -y_i + \\frac{x_i^2}{2R} + O(R^{-2})\n$$\n为了在 $S(R)$ 中保留到 $R^{-2}$ 阶的项，我们需要对 $f_i(R)$ 进行更精确的展开：\n$$\nf_i(R) = -y_i + \\frac{x_i^2}{2R} + \\frac{y_i x_i^2}{2R^2} + O(R^{-3})\n$$\n现在，我们将此表达式平方以求得 $f_i(R)^2$，并保留到 $R^{-2}$ 阶的项：\n$$\nf_i(R)^2 = \\left(-y_i + \\frac{x_i^2}{2R} + O(R^{-2})\\right)^2 = y_i^2 - \\frac{y_i x_i^2}{R} + \\frac{x_i^4}{4R^2} + O(R^{-3})\n$$\n为了得到在 $R^{-2}$ 阶上更精确的表达式，我们必须考虑所有对 $R^{-2}$ 阶有贡献的项：\n$$\nf_i(R)^2 = \\left(-y_i + \\frac{x_i^2}{2R} + \\frac{y_i x_i^2}{2R^2} + \\dots\\right)^2 = y_i^2 - \\frac{y_i x_i^2}{R} + \\left(\\frac{x_i^4}{4} - y_i^2 x_i^2\\right)\\frac{1}{R^2} + O(R^{-3})\n$$\n目标函数 $S(R)$ 是对四个数据点的总和：\n$$\nS(R) \\approx \\sum_{i=1}^4 y_i^2 - \\left(\\sum_{i=1}^4 y_i x_i^2\\right)\\frac{1}{R} + \\left(\\frac{1}{4}\\sum_{i=1}^4 x_i^4 - \\sum_{i=1}^4 y_i^2 x_i^2\\right)\\frac{1}{R^2}\n$$\n我们将近似表达式记为 $S_{approx}(R) = C_0 - C_1 R^{-1} + C_2 R^{-2}$。\n我们使用给定的数据点 $(x_1,y_1)=(-2,0.16)$、$(x_2,y_2)=(-1,0.04)$、$(x_3,y_3)=(1,0.04)$、$(x_4,y_4)=(2,0.16)$ 来计算这些和。\n$$\nC_1 = \\sum_{i=1}^4 y_i x_i^2 = 2 \\left( y_1 x_1^2 + y_2 x_2^2 \\right) = 2 \\left( (0.16)(4) + (0.04)(1) \\right) = 2(0.64 + 0.04) = 2(0.68) = 1.36\n$$\n系数 $C_2$ 由两部分组成：\n$$\n\\frac{1}{4}\\sum_{i=1}^4 x_i^4 = \\frac{1}{4} \\cdot 2 \\left( x_1^4 + x_2^4 \\right) = \\frac{1}{2} (16 + 1) = \\frac{17}{2} = 8.5\n$$\n$$\n\\sum_{i=1}^4 y_i^2 x_i^2 = 2 \\left( y_1^2 x_1^2 + y_2^2 x_2^2 \\right) = 2 \\left( (0.16)^2(4) + (0.04)^2(1) \\right) = 2(0.0256 \\cdot 4 + 0.0016) = 2(0.1024 + 0.0016) = 2(0.104) = 0.208\n$$\n所以，$C_2 = 8.5 - 0.208 = 8.292$。\n为了找到 $S_{approx}(R)$ 的最小值，我们将其关于 $R$ 的导数设为零：\n$$\n\\frac{dS_{approx}}{dR} = C_1 R^{-2} - 2C_2 R^{-3} = 0 \\implies \\frac{C_1}{R^2} = \\frac{2C_2}{R^3}\n$$\n对于非零且有限的 $R$，这给出了最小化半径 $R^{\\ast}$：\n$$\nR^{\\ast} = \\frac{2C_2}{C_1}\n$$\n问题要求的是 $R^{\\ast}$ 的“主阶值”。这表明我们应该分析 $R^{\\ast}$ 表达式的结构。与 $x_i$ 值相比，$y_i$ 值很小。因此，我们可以在 $C_2$ 的表达式中忽略较小的项。$C_2$ 的主导项是 $\\frac{1}{4}\\sum x_i^4 = 8.5$，而 $\\sum y_i^2 x_i^2 = 0.208$ 是一个高阶修正。因此，$R^{\\ast}$ 的主阶值由下式给出：\n$$\nR^{\\ast}_{\\text{leading}} = \\frac{2\\left(\\frac{1}{4}\\sum x_i^4\\right)}{\\sum y_i x_i^2} = \\frac{\\frac{1}{2}\\sum x_i^4}{\\sum y_i x_i^2}\n$$\n二阶导数 $\\frac{d^2S_{approx}}{dR^2} = -2C_1 R^{-3} + 6C_2 R^{-4}$ 在 $R^{\\ast} = 2C_2/C_1$ 处为正，因为 $\\frac{C_1^3}{4C_2^2} > 0$，这确认了它是一个局部最小值。\n\n我们现在计算 $R^{\\ast}_{\\text{leading}}$ 的数值：\n$$\n\\frac{1}{2}\\sum_{i=1}^4 x_i^4 = \\frac{1}{2}((-2)^4+(-1)^4+1^4+2^4) = \\frac{1}{2}(16+1+1+16) = 17\n$$\n$$\n\\sum_{i=1}^4 y_i x_i^2 = 1.36\n$$\n$$\nR^{\\ast}_{\\text{leading}} = \\frac{17}{1.36} = \\frac{1700}{136} = \\frac{100}{8} = 12.5\n$$\n因此，最小化半径的主阶值为 12.5。这个值与数据点的坐标相比很大，证明了使用大 $R$ 展开是合理的。", "answer": "$$\\boxed{12.5}$$", "id": "3256675"}, {"introduction": "当我们掌握了基本迭代步骤并了解其潜在的理论陷阱后，最后的这个练习将引导我们构建一个完整而稳健的求解器。你将亲手实现 Levenberg-Marquardt (LM) 算法，这是解决许多 NLLS 问题的黄金标准 [@problem_id:3256843]。该练习展示了如何通过自适应阻尼参数，将高斯-牛顿法的速度与梯度下降法的稳定性结合起来，从而创建一个既高效又可靠的优化工具。", "problem": "您的任务是实现一个完整的、自包含的程序，该程序使用 Levenberg–Marquardt 算法解决一个非线性最小二乘拟合问题，其中阻尼参数根据平方和的实际减少量与预测减少量之比自动调整。推导和实现必须从非线性最小二乘目标的基本定义开始，并最终形成一个在科学上和数值上都合理的算法。\n\n从非线性最小二乘目标的核心定义开始。设参数向量为 $p \\in \\mathbb{R}^m$，自变量向量为 $x \\in \\mathbb{R}^n$，观测数据（因变量）为 $y \\in \\mathbb{R}^n$，模型是一个函数 $f:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}^n$，它根据参数预测数据。定义残差向量 $r(p) \\in \\mathbb{R}^n$ 为 $r(p) = y - f(x,p)$，目标函数为 $F(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2$。定义雅可比矩阵 $J(p) \\in \\mathbb{R}^{n \\times m}$，其元素为 $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$。您的程序必须在每次迭代中使用中心有限差分来数值逼近雅可比矩阵。\n\n在每次迭代中，通过在当前参数 $p$ 周围使用 $r(p)$ 和 $J(p)$ 进行二阶近似，构建 $F(p)$ 的一个局部二次模型，并通过求解一个阻尼范式方程来计算一个参数增量，该增量在 Gauss–Newton 方向和梯度下降方向之间取得平衡。该算法必须使用一个增益比来决定是接受还是拒绝建议的步长，该增益比比较 $F(p)$ 的实际减少量与用于计算步长的局部模型所预测的减少量。阻尼参数必须根据此增益比自动调整：当步长有效时减小阻尼，当步长无效时增加阻尼。使用一个终止准则，当梯度的无穷范数很小、参数更新很小或目标函数的变化可以忽略不计时，停止迭代。\n\n正弦模型中使用的角度量 $\\phi$ 必须以弧度为单位进行处理和计算。\n\n您的程序必须为以下三个测试用例实现上述算法，每个用例都有特定的模型、数据集和初始参数猜测。对于每个测试用例，返回拟合后的参数向量，形式为浮点数列表，四舍五入到六位小数。\n\n测试用例 1（通用收敛性）：\n- 模型：$f(x,p) = p_1 \\exp(p_2 x) + p_3$，参数为 $p = [p_1,p_2,p_3]$。\n- 数据：$x$ 由 $x_i = 0.0 + i\\Delta$ 定义，其中 $i=0,1,\\dots,30$ 且 $\\Delta = 0.1$，因此 $x \\in [0,3.0]$，共 $31$ 个点。观测值由 $y = 2.0\\exp(-0.7 x) + 0.5 + 0.01\\sin(5x)$ 定义。\n- 初始参数：$p^{(0)} = [1.0,-0.2,0.0]$。\n\n测试用例 2（陡峭非线性）：\n- 模型：逻辑斯谛曲线 $f(x,p) = \\frac{p_1}{1 + \\exp(-p_2(x - p_3))}$，参数为 $p = [p_1,p_2,p_3]$。\n- 数据：$x$ 由 $x_i = -1.5 + i\\Delta$ 定义，其中 $i=0,1,\\dots,14$ 且 $\\Delta = \\frac{2.0 - (-1.5)}{14}$，因此 $x \\in [-1.5,2.0]$，共 $15$ 个点。观测值由 $y = \\frac{1.5}{1 + \\exp(-3.0(x - 0.5))} + 0.02\\cos(3x)$ 定义。\n- 初始参数：$p^{(0)} = [1.0,1.0,0.0]$。\n\n测试用例 3（灵敏度近似简并）：\n- 模型：角频率固定为 $\\omega = 2.5$ 弧度/单位 $x$ 的正弦曲线，$f(x,p) = p_1 \\cos(\\omega x + p_2) + p_3$，参数为 $p = [p_1,p_2,p_3]$ 且 $\\phi = p_2$（以弧度为单位）。\n- 数据：$x$ 由 $x_i = 0.0 + i\\Delta$ 定义，其中 $i=0,1,\\dots,20$ 且 $\\Delta = 0.1$，因此 $x \\in [0,2.0]$，共 $21$ 个点。观测值由 $y = 0.8\\cos(2.5 x + 0.4) - 0.05 + 0.01\\sin(7x)$ 定义。\n- 初始参数：$p^{(0)} = [0.5,0.0,0.0]$。\n\n算法要求：\n- 使用中心有限差分计算雅可比矩阵，扰动步长与 $\\sqrt{\\varepsilon}(1 + |p_j|)$ 成比例，其中 $\\varepsilon$ 是双精度浮点运算的机器精度。\n- 在每次迭代中，构建并求解一个阻尼范式方程以获得参数增量。\n- 计算目标函数的实际减少量和步长计算中使用的局部模型所预测的减少量。使用它们的比率来决定是否接受步长并调整阻尼参数。\n- 当满足任何标准的微小量准则时终止：梯度的无穷范数很小，参数变化很小，或目标函数的变化很小。同时包含一个最大迭代次数上限，以确保算法能够停止。\n- 正弦模型中的角度 $\\phi$ 必须始终以弧度计算。\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。每个测试用例的结果必须是一个包含拟合参数值的列表，这些值四舍五入到六位小数。最终输出格式必须严格符合以下形式：\n\"[[p11,p12,p13],[p21,p22,p23],[p31,p32,p33]]\"", "solution": "用户提供的问题被评估为**有效**。这是一个来自数值方法领域的、定义明确、科学上成立且计算上可行的任务。它要求实现用于非线性最小二乘的 Levenberg-Marquardt 算法，这是科学计算中一种标准且重要的技术。所有必要的组成部分，包括目标函数、算法步骤、测试模型、数据集和初始条件，都已提供且内部一致。\n\n本文从第一性原理出发，推导出一个完整的解决方案。\n\n### 1. 非线性最小二乘问题\n\n问题的核心是找到一组参数，使得模型能够最好地拟合一组观测数据。设参数向量为 $p \\in \\mathbb{R}^m$，自变量向量为 $x \\in \\mathbb{R}^n$，相应的观测数据向量为 $y \\in \\mathbb{R}^n$。一个模型函数 $f(x, p)$ 将参数 $p$ 和自变量 $x$ 映射到一组预测数据点。目标是找到参数向量 $p$，以最小化观测数据 $y$ 与预测数据 $f(x,p)$ 之间平方差的和。\n\n这可以通过定义残差向量 $r(p) \\in \\mathbb{R}^n$ 来正式表示：\n$$\nr(p) = y - f(x, p)\n$$\n目标是最小化标量目标函数 $F(p)$，其定义为残差向量的平方欧几里得范数的一半：\n$$\nF(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2 = \\frac{1}{2} \\sum_{i=1}^{n} r_i(p)^2\n$$\n由于模型 $f(x,p)$ 通常是参数 $p$ 的非线性函数，这是一个非线性最小二乘问题，必须使用迭代方法求解。\n\n### 2. 通过局部近似的迭代解法\n\nLevenberg-Marquardt 算法是一种迭代过程，用于寻找 $F(p)$ 的局部最小值。从一个初始猜测 $p^{(0)}$ 开始，该算法生成一系列参数向量 $p^{(1)}, p^{(2)}, \\dots$，这些向量逐步减小 $F(p)$ 的值。每次迭代计算一个步长向量 $h$ 来更新当前参数向量：$p_{k+1} = p_k + h$。\n\n步长 $h$ 是通过在当前点 $p_k$ 周围构建目标函数 $F(p)$ 的局部二次模型来找到的。该模型是通过使用一阶泰勒级数展开来线性化残差向量 $r(p_k+h)$ 构建的：\n$$\nr(p_k + h) \\approx r(p_k) + J(p_k)h\n$$\n其中 $J(p_k)$ 是在 $p_k$ 处评估的残差向量 $r$ 的雅可比矩阵。雅可比矩阵的元素由 $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$ 给出。由于 $r(p) = y - f(x,p)$，这等价于 $J_{ij}(p) = -\\frac{\\partial f_i(x,p)}{\\partial p_j}$。\n\n将这个线性近似的残差代入目标函数 $F(p_k+h)$，得到一个局部二次模型 $L(h)$：\n$$\nF(p_k + h) = \\frac{1}{2}\\lVert r(p_k+h) \\rVert_2^2 \\approx \\frac{1}{2}\\lVert r(p_k) + J(p_k)h \\rVert_2^2 = L(h)\n$$\n展开平方范数得到：\n$$\nL(h) = \\frac{1}{2} (r_k + J_k h)^T (r_k + J_k h) = \\frac{1}{2} (r_k^T r_k + 2h^T J_k^T r_k + h^T J_k^T J_k h)\n$$\n$$\nL(h) = F(p_k) + h^T (J_k^T r_k) + \\frac{1}{2} h^T (J_k^T J_k) h\n$$\n其中 $r_k = r(p_k)$ 且 $J_k = J(p_k)$。\n\n### 3. Gauss-Newton 步与 Levenberg-Marquardt 步\n\n选择步长 $h$ 来最小化这个二次模型 $L(h)$。将 $L(h)$ 关于 $h$ 的梯度设为零，得到：\n$$\n\\nabla_h L(h) = J_k^T r_k + (J_k^T J_k) h = 0\n$$\n这导出了 **Gauss-Newton 范式方程**：\n$$\n(J_k^T J_k) h_{gn} = -J_k^T r_k\n$$\n步长 $h_{gn}$ 是 Gauss-Newton 步。当矩阵 $J_k^T J_k$ 是良态时，该方法效果很好。然而，如果 $J_k^T J_k$ 是奇异或病态的，步长可能会过大且无效。\n\n**Levenberg-Marquardt 算法** 通过引入一个阻尼参数 $\\lambda \\ge 0$ 来解决这个问题。步长 $h_{lm}$ 通过求解一个修正的（或阻尼的）范式方程来找到：\n$$\n(J_k^T J_k + \\lambda I) h_{lm} = -J_k^T r_k\n$$\n其中 $I$ 是单位矩阵。项 $\\lambda I$ 是一个正则化项。\n- 如果 $\\lambda$ 很小，步长 $h_{lm}$ 近似于 Gauss-Newton 步 $h_{gn}$。\n- 如果 $\\lambda$ 很大，项 $\\lambda I$ 将主导 $J_k^T J_k$，方程近似为 $\\lambda I h_{lm} \\approx -J_k^T r_k$，这意味着 $h_{lm} \\approx -\\frac{1}{\\lambda} (J_k^T r_k)$。由于目标函数的梯度是 $\\nabla F(p_k) = J_k^T r_k$，步长变为最速下降方向上的一个小步长。\n\n因此，参数 $\\lambda$ 自适应地融合了快速收敛的 Gauss-Newton 方法和稳健但较慢的梯度下降方法。\n\n### 4. 算法实现细节\n\n#### 数值雅可比矩阵逼近\n雅可比矩阵 $J_k$ 在每次迭代中使用 **中心有限差分** 计算。对于每个参数 $p_j$（向量 $p$ 的第 $j$ 个分量），计算一个小的扰动 $\\delta_j$：\n$$\n\\delta_j = \\sqrt{\\varepsilon} (1 + |p_j|)\n$$\n其中 $\\varepsilon$ 是双精度浮点数的机器精度。雅可比矩阵的第 $j$ 列，表示残差对参数 $p_j$ 的敏感度，然后近似为：\n$$\nJ_{\\cdot, j}(p_k) = \\frac{\\partial r(p_k)}{\\partial p_j} \\approx \\frac{r(p_k + \\delta_j e_j) - r(p_k - \\delta_j e_j)}{2\\delta_j} = -\\frac{f(x, p_k + \\delta_j e_j) - f(x, p_k - \\delta_j e_j)}{2\\delta_j}\n$$\n其中 $e_j$ 是第 $j$ 个标准基向量。\n\n#### 阻尼参数自适应\n一个建议步长 $h=h_{lm}$ 的成功与否使用 **增益比** $\\rho$ 来评估。该比率比较了目标函数的实际减少量与局部二次模型 $L(h)$ 预测的减少量。\n- **实际减少量**：$\\Delta F_{actual} = F(p_k) - F(p_k + h)$\n- **预测减少量**：$\\Delta F_{pred} = L(0) - L(h) = -h^T J_k^T r_k - \\frac{1}{2}h^T (J_k^T J_k) h$\n\n增益比为 $\\rho = \\frac{\\Delta F_{actual}}{\\Delta F_{pred}}$。自适应策略如下：\n1.  如果 $\\rho$ 为正且显著（例如，$\\rho > 10^{-4}$），则该步长是“有效的”。接受该步长（$p_{k+1} = p_k + h$），并减小阻尼 $\\lambda$ 以在下一次迭代中更接近于更快的 Gauss-Newton 方法。一个常见的更新规则是 $\\lambda \\leftarrow \\lambda \\cdot \\max(1/3, 1-(2\\rho-1)^3)$。\n2.  如果 $\\rho$ 很小或为负，则该步长是“无效的”。拒绝该步长（$p_{k+1} = p_k$），并增加阻尼 $\\lambda$ 以转向更稳健的梯度下降方向。然后算法在同一次迭代 $k$ 中使用更大的 $\\lambda$ 重新计算步长 $h$。一个常见的规则是 $\\lambda \\leftarrow \\lambda \\cdot v$，其中 $v$ 是一个乘法因子，通常从 $v=2$ 开始，并在连续拒绝时增加。\n\n$\\lambda$ 的初始值根据问题的规模来选择，例如，$\\lambda_0 = \\tau \\cdot \\max_{ii}((J_0^T J_0)_{ii})$，其中 $\\tau$ 是一个小常数（例如，$10^{-4}$）。\n\n#### 终止准则\n当满足以下条件之一时，迭代过程终止，表明已找到满意的解：\n1.  **小梯度**：目标函数的梯度大小接近于零：$\\lVert J_k^T r_k \\rVert_{\\infty} < \\epsilon_g$。\n2.  **小参数更新**：参数向量的相对变化可以忽略不计：$\\lVert p_{k+1} - p_k \\rVert_2 < \\epsilon_p (\\lVert p_k \\rVert_2 + \\epsilon_p)$。\n3.  **小目标函数变化**：目标函数值的相对变化可以忽略不计：$|F(p_{k+1}) - F(p_k)| < \\epsilon_f (F(p_k) + \\epsilon_f)$。\n4.  **最大迭代次数**：达到预定的最大迭代次数 $k_{max}$，以防止无限循环。\n\n这里，$\\epsilon_g, \\epsilon_p, \\epsilon_f$ 是小的容差值（例如，$10^{-8}$）。\n\n### 5. Levenberg-Marquardt 算法总结\n完整的算法如下：\n\n1.  **初始化**：\n    - 选择一个初始参数猜测 $p_0$。\n    - 设置容差 $\\epsilon_g, \\epsilon_p, \\epsilon_f$ 和最大迭代次数 $k_{max}$。\n    - 计算初始残差 $r_0 = y - f(x, p_0)$ 和目标函数 $F_0 = \\frac{1}{2}r_0^T r_0$。\n    - 计算初始雅可比矩阵 $J_0$ 和梯度 $g_0 = J_0^T r_0$。\n    - 初始化阻尼参数 $\\lambda_0$ 和增长因子 $v$。\n    - 设置迭代计数器 $k = 0$。\n\n2.  **主循环**：当终止准则未满足时：\n    a. 求解阻尼范式方程 $(J_k^T J_k + \\lambda_k I) h = -g_k$ 以获得步长 $h$。\n    b. 评估建议的新参数向量 $p_{new} = p_k + h$。\n    c. 计算增益比 $\\rho = \\frac{F(p_k) - F(p_{new})}{\\Delta F_{pred}}$。\n    d. **如果 $\\rho > \\epsilon_{\\rho}$**：\n        i. 接受步长：$p_{k+1} = p_{new}$。\n        ii. 更新目标函数 $F_{k+1}$、残差 $r_{k+1}$、雅可比矩阵 $J_{k+1}$ 和梯度 $g_{k+1}$。\n        iii. 减小阻尼：$\\lambda_{k+1} = \\lambda_k \\cdot \\max(1/3, 1-(2\\rho-1)^3)$ 并重置 $v=2$。\n        iv. 检查关于步长和目标函数变化的终止准则。\n        v. 递增 $k \\leftarrow k+1$。\n    e. **否则 ($\\rho \\le \\epsilon_{\\rho}$)**：\n        i. 拒绝步长。参数不更新。\n        ii. 增加阻尼：$\\lambda_k \\leftarrow \\lambda_k \\cdot v$，然后 $v \\leftarrow 2v$。\n        iii. 从步骤 2a 开始，使用新的 $\\lambda_k$ 重复。\n\n3.  **终止**：返回最终的参数向量 $p_k$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef levenberg_marquardt(model_func, p_init, x_data, y_data, max_iter=100, tol_g=1e-8, tol_p=1e-8, tol_f=1e-8):\n    \"\"\"\n    Implements the Levenberg-Marquardt algorithm for nonlinear least squares.\n\n    Args:\n        model_func: The model function f(x, p).\n        p_init: Initial guess for the parameters.\n        x_data: Independent variable data.\n        y_data: Dependent variable data (observations).\n        max_iter: Maximum number of iterations.\n        tol_g: Tolerance for the infinity norm of the gradient.\n        tol_p: Tolerance for the parameter update step.\n        tol_f: Tolerance for the change in the objective function.\n\n    Returns:\n        The fitted parameter vector.\n    \"\"\"\n    p_k = np.array(p_init, dtype=float)\n    m = len(p_k)\n    eps = np.finfo(np.float64).eps\n\n    # Initial evaluation\n    r_k = y_data - model_func(x_data, p_k)\n    F_k = 0.5 * np.dot(r_k, r_k)\n\n    # Jacobian calculation function\n    def calculate_jacobian(p):\n        J = np.zeros((len(x_data), m))\n        for j in range(m):\n            delta = np.sqrt(eps) * (1.0 + np.abs(p[j]))\n            p_plus = p.copy()\n            p_minus = p.copy()\n            p_plus[j] += delta\n            p_minus[j] -= delta\n            \n            # Central difference for jacobian of residual r = y - f\n            # dr/dp = -df/dp\n            # Using (f(p - d) - f(p + d)) / 2d to get -df/dp\n            f_plus = model_func(x_data, p_plus)\n            f_minus = model_func(x_data, p_minus)\n\n            J[:, j] = (f_minus - f_plus) / (2.0 * delta)\n        return J\n\n    J_k = calculate_jacobian(p_k)\n    g_k = np.dot(J_k.T, r_k)\n\n    if np.linalg.norm(g_k, np.inf) < tol_g:\n        return p_k\n\n    # Initial damping parameter\n    A = np.dot(J_k.T, J_k)\n    tau = 1e-4\n    mu = tau * np.max(np.diag(A))\n    nu = 2.0\n    \n    k = 0\n    while k < max_iter:\n        \n        while True:\n            H_lm = A + mu * np.identity(m)\n            try:\n                h = np.linalg.solve(H_lm, -g_k)\n            except np.linalg.LinAlgError:\n                mu *= nu\n                nu *= 2.0\n                continue\n\n            p_new = p_k + h\n            r_new = y_data - model_func(x_data, p_new)\n            F_new = 0.5 * np.dot(r_new, r_new)\n\n            # Predicted reduction\n            # pred_red = - (h^T * g_k + 0.5 * h^T * A * h)\n            pred_red = - (np.dot(h, g_k) + 0.5 * np.dot(h, np.dot(A, h)))\n            \n            # Gain ratio\n            actual_red = F_k - F_new\n            \n            if pred_red > 0: # Avoid division by zero or negative\n                rho = actual_red / pred_red\n            else:\n                rho = -1.0\n\n            if rho > 0:  # Step is accepted\n                # Check termination criteria on parameter and function change\n                p_norm = np.linalg.norm(p_k)\n                if np.linalg.norm(h) < tol_p * (p_norm + tol_p):\n                    return p_new\n                \n                F_norm = F_k\n                if abs(actual_red) < tol_f * (F_norm + tol_f):\n                    return p_new\n\n                p_k = p_new\n                F_k = F_new\n                r_k = r_new\n                \n                J_k = calculate_jacobian(p_k)\n                A = np.dot(J_k.T, J_k)\n                g_k = np.dot(J_k.T, r_k)\n\n                # Check termination on gradient\n                if np.linalg.norm(g_k, np.inf) < tol_g:\n                    return p_k\n\n                mu = mu * max(1/3.0, 1 - (2*rho - 1)**3)\n                nu = 2.0\n                break # Exit inner loop and start next iteration\n            else: # Step is rejected\n                mu *= nu\n                nu *= 2.0\n        \n        k += 1\n\n    return p_k\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the three test cases specified in the problem.\n    \"\"\"\n    \n    # Test Case 1: Exponential Model\n    def model1(x, p):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    \n    x1 = np.linspace(0.0, 3.0, 31)\n    y1 = 2.0 * np.exp(-0.7 * x1) + 0.5 + 0.01 * np.sin(5 * x1)\n    p_init1 = [1.0, -0.2, 0.0]\n    \n    # Test Case 2: Logistic Model\n    def model2(x, p):\n        return p[0] / (1.0 + np.exp(-p[1] * (x - p[2])))\n        \n    x2 = np.linspace(-1.5, 2.0, 15)\n    y2 = 1.5 / (1.0 + np.exp(-3.0 * (x2 - 0.5))) + 0.02 * np.cos(3 * x2)\n    p_init2 = [1.0, 1.0, 0.0]\n    \n    # Test Case 3: Sinusoidal Model\n    def model3(x, p):\n        omega = 2.5\n        return p[0] * np.cos(omega * x + p[1]) + p[2]\n        \n    x3 = np.linspace(0.0, 2.0, 21)\n    y3 = 0.8 * np.cos(2.5 * x3 + 0.4) - 0.05 + 0.01 * np.sin(7 * x3)\n    p_init3 = [0.5, 0.0, 0.0]\n\n    test_cases = [\n        (model1, p_init1, x1, y1),\n        (model2, p_init2, x2, y2),\n        (model3, p_init3, x3, y3),\n    ]\n\n    results = []\n    for model_func, p_init, x_data, y_data in test_cases:\n        p_fit = levenberg_marquardt(model_func, p_init, x_data, y_data)\n        rounded_p = [round(p, 6) for p in p_fit]\n        results.append(str(rounded_p).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3256843"}]}