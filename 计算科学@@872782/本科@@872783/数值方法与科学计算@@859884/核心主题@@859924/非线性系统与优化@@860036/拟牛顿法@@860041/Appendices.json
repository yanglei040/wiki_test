{"hands_on_practices": [{"introduction": "在优化算法中，我们总是希望每一步都能朝着函数值减小的方向前进，这个方向被称为“下降方向”。在拟牛顿法中，保证搜索方向是下降方向的关键在于Hessian近似矩阵 $B_k$ 的正定性。这个练习将通过一个具体的反例，揭示当 $B_k$ 非正定时算法可能会计算出一个“上升方向”，从而帮助你直观地理解正定性为何至关重要。[@problem_id:2195908]", "problem": "在采用拟牛顿法的迭代优化算法中，第 $k$ 次迭代的搜索方向 $p_k$ 通过求解线性系统 $B_k p_k = -\\nabla f_k$ 来确定，其中 $B_k$ 是海森矩阵的当前近似，$\\nabla f_k$ 是目标函数 $f$ 在迭代点 $x_k$ 处的梯度。如果沿着一个方向 $p_k$ 移动一小步会使函数值减小，那么该方向就被认为是下降方向，这在数学上对应于条件 $p_k^T \\nabla f_k  0$。\n\n假设在某次迭代 $k$ 中，梯度由下式给出\n$$\n\\nabla f_k = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n且对称的海森近似矩阵为\n$$\nB_k = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}\n$$\n根据这些值，以下哪个陈述正确地描述了计算出的搜索方向 $p_k$？\n\nA. 搜索方向是上升方向。\nB. 搜索方向是下降方向。\nC. 搜索方向与梯度正交。\nD. 拟牛顿方程没有关于搜索方向的解。\nE. 搜索方向是零向量，表示算法应该停止。", "solution": "我们使用拟牛顿搜索方向的定义：求解线性系统 $B_{k} p_{k} = -\\nabla f_{k}$，然后检验下降条件 $p_{k}^{T} \\nabla f_{k}  0$。\n\n给定 $\\nabla f_{k} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$ 和 $B_{k} = \\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix}$，我们求解\n$$\n\\begin{pmatrix} 1  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} p_{1} \\\\ p_{2} \\end{pmatrix} = - \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}.\n$$\n这得到线性系统\n$$\np_{1} + 2 p_{2} = -1, \\quad 2 p_{1} + p_{2} = 2.\n$$\n从第一个方程，我们得到 $p_{1} = -1 - 2 p_{2}$。将其代入第二个方程得到\n$$\n2(-1 - 2 p_{2}) + p_{2} = 2 \\;\\Rightarrow\\; -2 - 4 p_{2} + p_{2} = 2 \\;\\Rightarrow\\; -3 p_{2} = 4 \\;\\Rightarrow\\; p_{2} = -\\frac{4}{3}.\n$$\n那么\n$$\np_{1} = -1 - 2\\left(-\\frac{4}{3}\\right) = -1 + \\frac{8}{3} = \\frac{5}{3}.\n$$\n因此\n$$\np_{k} = \\begin{pmatrix} \\frac{5}{3} \\\\ -\\frac{4}{3} \\end{pmatrix}.\n$$\n为了对该方向进行分类，我们计算方向导数：\n$$\np_{k}^{T} \\nabla f_{k} = \\begin{pmatrix} \\frac{5}{3}  -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} = \\frac{5}{3} + \\frac{8}{3} = \\frac{13}{3} > 0.\n$$\n由于 $p_{k}^{T} \\nabla f_{k} > 0$，沿着 $p_k$ 移动一个小的正步长会增加 $f$ 的值，因此 $p_k$ 是一个上升方向。该系统有唯一解，因为 $\\det(B_{k}) = 1 \\cdot 1 - 2 \\cdot 2 = -3 \\neq 0$，并且 $p_{k} \\neq 0$ 且不与梯度正交。", "answer": "$$\\boxed{A}$$", "id": "2195908"}, {"introduction": "BFGS方法的核心是其对Hessian近似矩阵的秩二校正公式，这个公式初看起来可能有些复杂。本练习旨在化繁为简，让你只计算两个秩一校正项中的一个。通过这个聚焦的练习，你将能熟练掌握构成BFGS更新的基础矩阵和向量运算，为理解更复杂的行为打下坚实基础。[@problem_id:2195881]", "problem": "在数值优化领域，Broyden–Fletcher–Goldfarb–Shanno (BFGS) 算法是一种流行的拟牛顿法，用于求解无约束非线性优化问题。在每次迭代中，该算法都会近似目标函数的海森矩阵。从一个初始近似 $B_k$ 开始，下一个近似 $B_{k+1}$ 使用以下更新公式计算：\n$$B_{k+1} = B_k + \\frac{y_k y_k^T}{y_k^T s_k} - \\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$$\n其中 $s_k$ 是所取的步长，而 $y_k$ 是梯度的变化量。该更新包括将两个秩一矩阵加到当前的近似 $B_k$ 上。\n\n考虑 BFGS 算法的一次迭代，其中当前的海森矩阵近似是 $2 \\times 2$ 的单位矩阵，$B_k = I$。步长向量由 $s_k = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 给出，梯度变化向量为 $y_k = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n\n计算 $B_{k+1}$ 的负秩一更新项，该项由表达式 $-\\frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k}$ 给出。你的最终答案应该是一个 $2 \\times 2$ 矩阵。", "solution": "我们被要求计算负秩一更新项\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}}$$\n其中 $B_{k}=I$，$s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，而 $y_{k}$ 已给出但此项计算无需用到。\n\n首先，利用 $B_{k}=I$ 这一事实来简化分子：\n$$B_{k}s_{k} = Is_{k} = s_{k}, \\quad \\text{以及} \\quad B_{k}s_{k}s_{k}^{T}B_{k} = s_{k}s_{k}^{T}。$$\n\n接下来，计算分母：\n$$s_{k}^{T}B_{k}s_{k} = s_{k}^{T}Is_{k} = s_{k}^{T}s_{k}。$$\n对于 $s_{k}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，我们有\n$$s_{k}^{T}s_{k} = 1^{2} + 0^{2} = 1。$$\n\n因此，负秩一更新项变为\n$$-\\frac{B_{k}s_{k}s_{k}^{T}B_{k}}{s_{k}^{T}B_{k}s_{k}} = -\\frac{s_{k}s_{k}^{T}}{s_{k}^{T}s_{k}} = -s_{k}s_{k}^{T}。$$\n\n显式地计算 $s_{k}s_{k}^{T}$：\n$$s_{k}s_{k}^{T} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}\\begin{pmatrix}1  0\\end{pmatrix} = \\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix}。$$\n\n因此，负秩一更新项是\n$$-\\begin{pmatrix}1  0 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  0\\end{pmatrix}。$$", "answer": "$$\\boxed{\\begin{pmatrix}-1  0 \\\\ 0  0\\end{pmatrix}}$$", "id": "2195881"}, {"introduction": "真正的理解源于亲手实现。这个练习将指导你构建一个完整的BFGS求解器，包括线搜索和Hessian更新，并将其应用于一个非严格凸的特殊函数。通过观察算法在Hessian矩阵奇异情况下的行为，你将对该方法的性能特点及其与目标函数几何形状的相互作用有更深刻的认识。[@problem_id:3264863]", "problem": "考虑一个最小化二次连续可微但非严格凸函数的任务。按如下方式构造这样一个函数：令 $f:\\mathbb{R}^2\\to\\mathbb{R}$ 定义为 $f(x_1,x_2)=x_1^2$。该函数是凸函数但非严格凸函数，因为其 Hessian 矩阵是半正定的，且具有非平凡的零空间。实现 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 拟牛顿法，以在 $\\mathbb{R}^2$ 上最小化函数 $f$，需要使用逆 Hessian 矩阵近似和满足 Armijo 充分下降条件的回溯线搜索。当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $N_{\\max}$ 时，算法应停止。\n\n从以下基本概念开始：\n- 通过二阶导数表征的凸性定义：如果函数 $f$ 的 Hessian 矩阵 $\\nabla^2 f(x)$ 在其定义域内的所有点 $x$ 上都是半正定的，则该函数是凸函数。\n- 基于梯度的下降范式：迭代更新 $x_{k+1}=x_k+\\alpha_k p_k$，其中 $p_k$ 是一个下降方向，$\\alpha_k>0$ 满足充分下降条件。\n- 拟牛顿法：使用逆 Hessian 矩阵的迭代近似 $H_k$ 来定义搜索方向 $p_k=-H_k\\nabla f(x_k)$，其中 $H_k$ 的更新需满足割线条件并保持正定性。\n\n你的程序必须：\n- 实现函数 $f(x_1,x_2)=x_1^2$ 及其梯度 $\\nabla f(x_1,x_2)=(2x_1,0)$。\n- 初始化 $H_0=\\gamma I$，其中 $I$ 是单位矩阵，$\\gamma>0$ 是每个测试用例提供的标量。\n- 使用带有 Armijo 条件 $f(x_k+\\alpha p_k)\\le f(x_k)+c_1\\alpha\\nabla f(x_k)^\\top p_k$ 的回溯线搜索，其中固定的 $c_1$ 和回溯因子 $\\beta\\in(0,1)$ 由你选择，但在所有测试用例中保持不变。\n- 根据从拟牛顿割线条件导出的标准 BFGS 逆矩阵更新公式来更新 $H_k$，前提是曲率条件 $s_k^\\top y_k>0$ 成立，其中 $s_k=x_{k+1}-x_k$ 且 $y_k=\\nabla f(x_{k+1})-\\nabla f(x_k)$。\n\n设计一个测试套件，包含以下参数集，每个参数集指定初始点 $(x_1^{(0)},x_2^{(0)})$、容差 $\\varepsilon$、最大迭代次数 $N_{\\max}$ 和初始缩放因子 $\\gamma$：\n- 测试 1：$(x_1^{(0)},x_2^{(0)})=(5,7)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=1$。\n- 测试 2：$(x_1^{(0)},x_2^{(0)})=(0,3)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=1$。\n- 测试 3：$(x_1^{(0)},x_2^{(0)}) =(-10,-2)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=1$。\n- 测试 4：$(x_1^{(0)},x_2^{(0)})=(10^6,10^6)$，$\\varepsilon=10^{-12}$，$N_{\\max}=100$，$\\gamma=1$。\n- 测试 5：$(x_1^{(0)},x_2^{(0)})=(5,7)$，$\\varepsilon=10^{-9}$，$N_{\\max}=50$，$\\gamma=10$。\n\n对于每个测试，还需通过检查 $|x_{2,\\text{final}}-x_{2,\\text{initial}}|\\le\\tau$ 来计算第二个坐标是否在用户定义的容差 $\\tau$ 内保持不变，其中你应该为该测试设置 $\\tau=\\varepsilon$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试的结果必须是 $[x_{1,\\text{final}},x_{2,\\text{final}},f(x_{\\text{final}}),\\text{iterations},\\text{x2\\_unchanged}]$ 形式的列表，其中 $x_{1,\\text{final}}$ 和 $x_{2,\\text{final}}$ 是浮点数，$f(x_{\\text{final}})$ 是一个浮点数，$\\text{iterations}$ 是一个整数，而 $\\text{x2\\_unchanged}$ 是一个布尔值，指示 $x_2$ 是否在容差内保持不变。最终输出将所有提供的测试用例的结果汇总到一个列表中，例如 $[[x_{1,\\text{final}}^{(1)},x_{2,\\text{final}}^{(1)},\\dots],[x_{1,\\text{final}}^{(2)},x_{2,\\text{final}}^{(2)},\\dots],\\dots]$。", "solution": "该问题要求实现并分析 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 拟牛顿优化算法，用于最小化一个非严格凸函数。问题陈述的有效性已得到确认，因为它在科学上基于数值优化理论，是适定的、客观的，并为实现和测试提供了完整的规范。\n\n任务的核心是应用形如 $x_{k+1} = x_k + \\alpha_k p_k$ 的迭代下降方法，其中 $x_k \\in \\mathbb{R}^2$ 是当前对最小值的估计，$p_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。\n\n**1. 目标函数及其性质**\n待最小化的函数是 $f:\\mathbb{R}^2 \\to \\mathbb{R}$，定义为：\n$$ f(x_1, x_2) = x_1^2 $$\n$f$ 的梯度是其偏导数的向量：\n$$ \\nabla f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 0 \\end{pmatrix} $$\n$f$ 的 Hessian 矩阵包含其二阶偏导数：\n$$ \\nabla^2 f(x_1, x_2) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} $$\n一个二次可微函数是凸函数的充要条件是其 Hessian 矩阵在其定义域的所有点上都是半正定的。$\\nabla^2 f$ 的特征值为 $\\lambda_1 = 2$ 和 $\\lambda_2 = 0$。由于所有特征值都是非负的，Hessian 矩阵是半正定的，因此 $f$ 是凸函数。然而，一个函数要是严格凸函数，其 Hessian 矩阵必须是正定的（所有特征值都严格为正）。零特征值的存在意味着 $f$ 不是严格凸函数。$f$ 的最小化子集是 $x_1=0$ 的整条直线，即 $\\{(0, c) \\mid c \\in \\mathbb{R}\\}$，因为 $f(0, c) = 0$ 是全局最小值。\n\n**2. BFGS 算法**\nBFGS 方法是一种拟牛顿法，它避免了 Hessian 矩阵及其逆矩阵的显式计算。取而代之的是，它维护一个逆 Hessian 矩阵的近似值，记为 $H_k$，并在每次迭代中更新它。\n\n**a. 初始化**\n过程从一个初始点 $x_0$ 开始。初始的逆 Hessian 矩阵近似 $H_0$ 通常被设置为一个缩放的单位矩阵，$H_0 = \\gamma I$，其中 $\\gamma > 0$ 是一个缩放因子，$I$ 是单位矩阵。一个常见的选择是 $\\gamma=1$。\n\n**b. 迭代步骤**\n对每次迭代 $k = 0, 1, 2, \\dots$：\n1.  **计算搜索方向**：搜索方向 $p_k$ 使用当前的逆 Hessian 矩阵近似和梯度来计算：\n    $$ p_k = -H_k \\nabla f(x_k) $$\n    该方向类似于牛顿方向 $p_k = -[\\nabla^2 f(x_k)]^{-1} \\nabla f(x_k)$。\n\n2.  **使用线搜索确定步长**：找到步长 $\\alpha_k > 0$ 以确保函数值有足够的下降。采用回溯线搜索。从一个初始猜测（例如 $\\alpha = 1$）开始，步长被一个因子 $\\beta \\in (0,1)$ 连续减小，直到满足 Armijo 条件：\n    $$ f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k $$\n    这里，$c_1$ 是一个很小的常数，通常为 $c_1 = 10^{-4}$。我们将使用 $c_1 = 10^{-4}$ 和 $\\beta = 0.5$。一旦找到合适的 $\\alpha$，我们就设置 $\\alpha_k = \\alpha$。\n\n3.  **更新位置**：通过执行以下步骤找到下一个迭代点：\n    $$ x_{k+1} = x_k + \\alpha_k p_k $$\n\n4.  **更新逆 Hessian 矩阵**：将 $H_k$ 更新为 $H_{k+1}$，以合并从最近一步收集到的关于函数曲率的信息。我们定义：\n    -   $s_k = x_{k+1} - x_k = \\alpha_k p_k$（位置步长）\n    -   $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$（梯度变化）\n    仅当曲率条件 $y_k^\\top s_k > 0$ 成立时，才执行 BFGS 更新。此条件确保如果 $H_k$ 是正定的，更新后的矩阵 $H_{k+1}$ 也保持正定。逆 Hessian 矩阵的更新公式为：\n    $$ H_{k+1} = \\left(I - \\frac{s_k y_k^\\top}{y_k^\\top s_k}\\right) H_k \\left(I - \\frac{y_k s_k^\\top}{y_k^\\top s_k}\\right) + \\frac{s_k s_k^\\top}{y_k^\\top s_k} $$\n    如果不满足曲率条件，则跳过更新，并设置 $H_{k+1} = H_k$。\n\n**c. 终止条件**\n当梯度的范数低于指定的容差 $\\varepsilon$，即 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$，或者达到最大迭代次数 $N_{\\max}$ 时，算法终止。\n\n**3. 对特定函数的应用**\n对于此问题的一个关键观察是梯度 $\\nabla f(x) = (2x_1, 0)^\\top$ 的结构。第二个分量始终为零。这对 BFGS 算法的行为有着深远的影响。\n\n让我们假设初始逆 Hessian 矩阵是对角的，$H_0 = \\gamma I = \\begin{pmatrix} \\gamma  0 \\\\ 0  \\gamma \\end{pmatrix}$。初始搜索方向为 $p_0 = -H_0 \\nabla f(x_0) = -\\begin{pmatrix} \\gamma  0 \\\\ 0  \\gamma \\end{pmatrix} \\begin{pmatrix} 2x_{1,0} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2\\gamma x_{1,0} \\\\ 0 \\end{pmatrix}$。\n搜索方向的第二个分量为零。因此，步进 $x_1 = x_0 + \\alpha_0 p_0$ 只会改变 $x_0$ 的第一个坐标。第二个坐标 $x_{2,1}$ 将等于 $x_{2,0}$。\n\n现在考虑用于 BFGS 更新的向量：\n$s_0 = x_1 - x_0 = \\alpha_0 p_0 = (-2\\alpha_0\\gamma x_{1,0}, 0)^\\top$。\n$y_0 = \\nabla f(x_1) - \\nabla f(x_0) = (2x_{1,1} - 2x_{1,0}, 0)^\\top$。\n$s_0$ 和 $y_0$ 的第二个分量都等于零。我们把它们表示为 $s_0 = (s_{1,0}, 0)^\\top$ 和 $y_0 = (y_{1,0}, 0)^\\top$。\n\n当我们计算更新所需的矩阵时，我们发现它们的结构不会在新的 Hessian 近似中引入任何非对角元素：\n$s_0 y_0^\\top = \\begin{pmatrix} s_{1,0}y_{1,0}  0 \\\\ 0  0 \\end{pmatrix}$\n$s_0 s_0^\\top = \\begin{pmatrix} s_{1,0}^2  0 \\\\ 0  0 \\end{pmatrix}$\n鉴于 $H_0$ 是对角矩阵，并且更新公式中的所有矩阵要么是对角的，要么具有保持对角性的结构（$V H V^\\top$，其中 $H$ 和 $V$ 是对角的），因此生成的矩阵 $H_1$ 也将是对角的。\n\n通过归纳法，如果 $H_k$ 是对角的，那么 $p_k = -H_k \\nabla f(x_k)$ 的第二个分量将为零。这意味着 $s_k$ 的第二个分量将为零。$y_k$ 的第二个分量也将为零。因此，$H_{k+1}$ 将保持对角性。\n\n这证明了此问题的一个关键性质：**如果初始逆 Hessian 矩阵近似 $H_0$ 是对角的，那么所有后续的近似 $H_k$ 也将是对角的，并且该算法永远不会改变迭代点中的 $x_2$ 坐标。** 优化完全在由 $x_1$ 轴定义的子空间内进行，从而有效地将问题简化为在一维空间中搜索 $g(x_1) = x_1^2$ 的最小值。\n\n提供的测试用例都使用 $H_0 = \\gamma I$，这是一个对角矩阵。因此，我们预期在所有测试中 $x_2$ 坐标将保持不变。算法将收敛到点 $(0, x_{2,0})$，这是全局最小化子集中的一点。对于二次函数，已知具有精确线搜索的 BFGS 方法在至多 $n$ 次迭代内收敛，其中 $n$ 是空间的维度。在这里，由于问题的特定结构，我们观察到更快的收敛速度（1 或 2 步）。", "answer": "```python\n# 完整且可运行的 Python 3 代码如下。\n# 导入的库必须符合指定的执行环境。\nimport numpy as np\n\ndef bfgs_minimize(f, grad_f, x0, epsilon, n_max, gamma):\n    \"\"\"\n    使用 BFGS 拟牛顿法最小化一个函数。\n\n    参数:\n        f: 目标函数。\n        grad_f: 目标函数的梯度。\n        x0: 初始点。\n        epsilon: 梯度范数的停止判据容差。\n        n_max: 最大迭代次数。\n        gamma: 初始逆 Hessian 矩阵近似的缩放因子。\n    \n    返回:\n        一个包含最终点、最终函数值和迭代次数的元组 (x_final, f_final, k)。\n    \"\"\"\n    # 线搜索参数\n    c1 = 1e-4\n    beta = 0.5\n    \n    x = np.array(x0, dtype=float)\n    n = len(x)\n    H = gamma * np.identity(n)\n    \n    for k in range(n_max):\n        g = grad_f(x)\n        \n        # 检查停止判据\n        if np.linalg.norm(g) = epsilon:\n            return x, f(x), k\n            \n        # 计算搜索方向\n        p = -H @ g\n        \n        # 回溯线搜索（Armijo 条件）\n        alpha = 1.0\n        f_x = f(x)\n        g_dot_p = g.T @ p\n        \n        while True:\n            x_new = x + alpha * p\n            if f(x_new) = f_x + c1 * alpha * g_dot_p:\n                break\n            alpha *= beta\n        \n        s = x_new - x\n        x = x_new\n        \n        g_new = grad_f(x)\n        y = g_new - g\n        \n        # BFGS 逆 Hessian 矩阵更新\n        # 检查曲率条件\n        y_dot_s = y.T @ s\n        if y_dot_s > 1e-12: # 使用一个小的阈值以保证数值稳定性\n            rho = 1.0 / y_dot_s\n            I = np.identity(n)\n            \n            term1 = I - rho * np.outer(s, y)\n            term2 = I - rho * np.outer(y, s)\n            \n            H = term1 @ H @ term2 + rho * np.outer(s, s)\n            \n    return x, f(x), n_max\n\ndef solve():\n    \"\"\"\n    主函数，在指定的测试用例上运行 BFGS 算法。\n    \"\"\"\n    # 目标函数 f(x1, x2) = x1^2\n    def f(x):\n        return x[0]**2\n\n    # 目标函数的梯度\n    def grad_f(x):\n        return np.array([2 * x[0], 0.0])\n\n    # 定义来自问题陈述的测试用例。\n    test_cases = [\n        {'x0': (5, 7), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (0, 3), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (-10, -2), 'eps': 1e-9, 'n_max': 50, 'gamma': 1},\n        {'x0': (10**6, 10**6), 'eps': 1e-12, 'n_max': 100, 'gamma': 1},\n        {'x0': (5, 7), 'eps': 1e-9, 'n_max': 50, 'gamma': 10},\n    ]\n\n    results = []\n    for case in test_cases:\n        x0 = case['x0']\n        eps = case['eps']\n        \n        x_final, f_final, iterations = bfgs_minimize(\n            f, grad_f, x0, eps, case['n_max'], case['gamma']\n        )\n        \n        x1_final, x2_final = x_final[0], x_final[1]\n        \n        # 检查第二个坐标是否在容差范围内保持不变\n        tau = eps\n        x2_unchanged = abs(x2_final - x0[1]) = tau\n        \n        results.append([x1_final, x2_final, f_final, iterations, x2_unchanged])\n\n    # 将最终输出字符串格式化为指定的列表的列表形式。\n    sub_results_str = []\n    for r in results:\n        # 手动格式化每个子列表以控制间距和布尔值表示\n        sub_results_str.append(f\"[{r[0]},{r[1]},{r[2]},{r[3]},{str(r[4]).lower()}]\")\n    \n    final_output_str = f\"[{','.join(sub_results_str)}]\"\n    \n    # 以精确要求的格式进行最终打印。\n    print(final_output_str)\n\nsolve()\n```", "id": "3264863"}]}