{"hands_on_practices": [{"introduction": "回溯线搜索是优化算法中的一个基本构建块，它通过迭代减小步长来满足 Armijo 条件，从而保证目标函数的充分下降。本练习将引导您手动执行这一过程，通过对一个简单函数进行逐步计算 [@problem_id:2154925]，您将直观地掌握该算法的核心机制。", "problem": "在数值优化领域，线搜索算法是在迭代过程中确定合适的步长以最小化一个函数的基本方法。最常用的方法之一是回溯线搜索，其目的是找到一个步长 $\\alpha > 0$，使得目标函数有充分的下降。这通常通过 Armijo 条件来强制执行。\n\nArmijo 条件由以下不等式给出：\n$$f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k$$\n这里，$f$ 是目标函数，$x_k$ 是当前迭代点，$p_k$ 是搜索方向，$\\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度，$c_1$ 是一个控制所需下降量的常数，通常选择在 $(0, 1)$ 范围内。\n\n回溯算法的运作方式如下：\n1.  从一个初始试探步长 $\\alpha = \\bar{\\alpha}$ 开始。\n2.  检查当前 $\\alpha$ 是否满足 Armijo 条件。\n3.  如果条件不满足，则将步长乘以一个回溯因子 $\\rho \\in (0, 1)$ 进行缩减，即新步长变为 $\\alpha \\leftarrow \\rho \\alpha$。\n4.  从第2步开始重复，直到找到一个可接受的 $\\alpha$。\n\n考虑优化一维目标函数 $f(x) = x^4$。我们当前位于迭代点 $x_k = 1$，并选择了一个下降方向 $p_k = -1$。回溯线搜索的参数配置如下：\n- 初始试探步长：$\\bar{\\alpha} = 1$\n- 充分下降常数：$c_1 = 0.8$\n- 回溯因子：$\\rho = 0.5$\n\n你的任务是执行回溯线搜索过程，并确定第一个被接受的步长 $\\alpha$。", "solution": "我们给定的目标函数为 $f(x) = x^{4}$，当前迭代点为 $x_{k} = 1$，搜索方向为 $p_{k} = -1$。对于一个试探步长 $\\alpha > 0$，Armijo 条件为\n$$\nf(x_{k} + \\alpha p_{k}) \\le f(x_{k}) + c_{1}\\alpha \\nabla f(x_{k})^{T} p_{k}.\n$$\n计算梯度：在一维情况下，$\\nabla f(x) = f'(x) = 4x^{3}$。在 $x_{k} = 1$ 处，我们有\n$$\nf(1) = 1, \\qquad \\nabla f(1) = 4, \\qquad \\nabla f(1)^{T} p_{k} = 4 \\cdot (-1) = -4.\n$$\n当 $c_{1} = 0.8 = \\frac{4}{5}$ 时，Armijo 条件变为\n$$\nf(1 + \\alpha(-1)) \\le 1 + \\frac{4}{5}\\alpha(-4) \\quad \\Longleftrightarrow \\quad (1 - \\alpha)^{4} \\le 1 - \\frac{16}{5}\\alpha.\n$$\n回溯从 $\\alpha = \\bar{\\alpha} = 1$ 开始，并以 $\\rho = \\frac{1}{2}$ 的比例缩减步长，即 $\\alpha \\leftarrow \\rho \\alpha$，直到不等式成立。\n\n测试 $\\alpha = 1$：\n$$\n\\text{左侧} = (1 - 1)^{4} = 0, \\qquad \\text{右侧} = 1 - \\frac{16}{5} = -\\frac{11}{5}.\n$$\n检验 $0 \\le -\\frac{11}{5}$：不成立。拒绝 $\\alpha = 1$。\n\n测试 $\\alpha = \\frac{1}{2}$：\n$$\n\\text{左侧} = \\left(1 - \\frac{1}{2}\\right)^{4} = \\left(\\frac{1}{2}\\right)^{4} = \\frac{1}{16}, \\qquad \\text{右侧} = 1 - \\frac{16}{5}\\cdot \\frac{1}{2} = 1 - \\frac{8}{5} = -\\frac{3}{5}.\n$$\n检验 $\\frac{1}{16} \\le -\\frac{3}{5}$：不成立。拒绝 $\\alpha = \\frac{1}{2}$。\n\n测试 $\\alpha = \\frac{1}{4}$：\n$$\n\\text{左侧} = \\left(1 - \\frac{1}{4}\\right)^{4} = \\left(\\frac{3}{4}\\right)^{4} = \\frac{81}{256}, \\qquad \\text{右侧} = 1 - \\frac{16}{5}\\cdot \\frac{1}{4} = 1 - \\frac{4}{5} = \\frac{1}{5}.\n$$\n检验 $\\frac{81}{256} \\le \\frac{1}{5}$，即 $81 \\cdot 5 \\le 256 \\cdot 1 \\iff 405 \\le 256$：不成立。拒绝 $\\alpha = \\frac{1}{4}$。\n\n测试 $\\alpha = \\frac{1}{8}$：\n$$\n\\text{左侧} = \\left(1 - \\frac{1}{8}\\right)^{4} = \\left(\\frac{7}{8}\\right)^{4} = \\frac{2401}{4096}, \\qquad \\text{右侧} = 1 - \\frac{16}{5}\\cdot \\frac{1}{8} = 1 - \\frac{2}{5} = \\frac{3}{5}.\n$$\n检验 $\\frac{2401}{4096} \\le \\frac{3}{5}$，即 $2401 \\cdot 5 \\le 4096 \\cdot 3 \\iff 12005 \\le 12288$：成立。接受 $\\alpha = \\frac{1}{8}$ 作为第一个满足 Armijo 条件的步长。", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "2154925"}, {"introduction": "在基于梯度的优化方法中，选择合适的步长对算法效率至关重要。本练习旨在比较两种步长选择策略：使用预先设定的固定步长，以及执行精确线搜索来找到当前迭代中的最优步长 [@problem_id:2184823]。通过计算并比较两种方法的结果，您将能直接观察到步长选择对优化进程的影响，并理解为何需要更精细的线搜索技术。", "problem": "考虑最小化函数 $f(x_1, x_2) = x_1^2 + 4x_2^2$ 的无约束优化问题。我们希望使用迭代法求此函数的最小值。从点 $\\mathbf{x}_0 = (4, 1)^T$ 开始，根据更新规则 $\\mathbf{x}_1 = \\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0)$ 生成新的迭代点 $\\mathbf{x}_1$，其中 $\\nabla f(\\mathbf{x}_0)$ 是 $f$ 在 $\\mathbf{x}_0$ 处的梯度，而 $\\alpha > 0$ 是一个称为步长的标量。\n\n我们将比较两种选择步长的不同策略。\n\n策略 A：使用固定步长 $\\alpha_A = 0.1$。令得到的迭代点为 $\\mathbf{x}_{1,A}$。\n策略 B：执行精确线搜索以找到最优步长 $\\alpha_B$，该值是使一维函数 $g(\\alpha) = f(\\mathbf{x}_0 - \\alpha \\nabla f(\\mathbf{x}_0))$ 最小化的 $\\alpha$ 值。令得到的迭代点为 $\\mathbf{x}_{1,B}$。\n\n计算比率 $R = \\frac{f(\\mathbf{x}_{1,A})}{f(\\mathbf{x}_{1,B})}$。报告 $R$ 的值，四舍五入到三位有效数字。", "solution": "我们最小化函数 $f(x_{1},x_{2})=x_{1}^{2}+4x_{2}^{2}$，其梯度为 $\\nabla f(x_{1},x_{2})=(2x_{1},8x_{2})^{T}$。在点 $\\mathbf{x}_{0}=(4,1)^{T}$ 处，梯度为\n$$\n\\nabla f(\\mathbf{x}_{0})=(2\\cdot 4,\\,8\\cdot 1)^{T}=(8,8)^{T}.\n$$\n更新规则为 $\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha \\nabla f(\\mathbf{x}_{0})$。\n\n策略 A：当 $\\alpha_{A}=0.1=\\frac{1}{10}$ 时，\n$$\n\\mathbf{x}_{1,A}=(4,1)^{T}-\\tfrac{1}{10}(8,8)^{T}=\\left(4-\\tfrac{8}{10},\\,1-\\tfrac{8}{10}\\right)^{T}=\\left(\\tfrac{16}{5},\\,\\tfrac{1}{5}\\right)^{T}.\n$$\n那么\n$$\nf(\\mathbf{x}_{1,A})=\\left(\\tfrac{16}{5}\\right)^{2}+4\\left(\\tfrac{1}{5}\\right)^{2}=\\tfrac{256}{25}+\\tfrac{4}{25}=\\tfrac{260}{25}=\\tfrac{52}{5}.\n$$\n\n策略 B：对 $g(\\alpha)=f(\\mathbf{x}_{0}-\\alpha \\nabla f(\\mathbf{x}_{0}))$ 执行精确线搜索。使用 $\\mathbf{x}_{0}=(4,1)$ 和 $\\nabla f(\\mathbf{x}_{0})=(8,8)$，\n$$\ng(\\alpha)=f(4-8\\alpha,\\,1-8\\alpha)=(4-8\\alpha)^{2}+4(1-8\\alpha)^{2}.\n$$\n展开得：\n$$\ng(\\alpha)=(16-64\\alpha+64\\alpha^{2})+4(1-16\\alpha+64\\alpha^{2})=20-128\\alpha+320\\alpha^{2}.\n$$\n求导并令其为零：\n$$\ng'(\\alpha)=-128+640\\alpha=0 \\;\\Rightarrow\\; \\alpha_{B}=\\tfrac{128}{640}=\\tfrac{1}{5}.\n$$\n因此\n$$\n\\mathbf{x}_{1,B}=(4,1)^{T}-\\tfrac{1}{5}(8,8)^{T}=\\left(4-\\tfrac{8}{5},\\,1-\\tfrac{8}{5}\\right)^{T}=\\left(\\tfrac{12}{5},\\,-\\tfrac{3}{5}\\right)^{T},\n$$\n并且\n$$\nf(\\mathbf{x}_{1,B})=\\left(\\tfrac{12}{5}\\right)^{2}+4\\left(-\\tfrac{3}{5}\\right)^{2}=\\tfrac{144}{25}+\\tfrac{36}{25}=\\tfrac{180}{25}=\\tfrac{36}{5}.\n$$\n\n计算比率\n$$\nR=\\frac{f(\\mathbf{x}_{1,A})}{f(\\mathbf{x}_{1,B})}=\\frac{\\tfrac{52}{5}}{\\tfrac{36}{5}}=\\frac{52}{36}=\\frac{13}{9}\\approx 1.444\\ldots\n$$\n四舍五入到三位有效数字，得到 $R=1.44$。", "answer": "$$\\boxed{1.44}$$", "id": "2184823"}, {"introduction": "将理论应用于实践，需要将优化算法转化为代码，而 Rosenbrock 函数是测试算法性能的经典难题。本练习将指导您实现一个完整的陡峭下降优化器，它采用更高级的强 Wolfe 条件进行线搜索，并包含回溯作为备用方案 [@problem_id:3247710]。通过这个实践，您将学会如何将复杂的数学条件转化为稳健的程序代码，从而掌握在科学计算中应用线搜索方法的关键技能。", "problem": "实现一个完整的、可运行的程序，在最速下降法中对 Rosenbrock 函数执行线搜索。目标函数是为向量 $x = (x_1, x_2)$ 定义的二维 Rosenbrock 函数 $f(x) = (1 - x_1)^2 + 100 (x_2 - x_1^2)^2$。该算法必须使用基于确保沿下降方向有充分下降和适当曲率的条件的原则性线搜索，并且在主线搜索未能找到步长时，必须能够回退到简单的回溯策略。程序应计算每次迭代中选择的步长序列$\\alpha_k$，直到收敛或达到最大迭代次数。程序必须以数值形式输出这些序列，而不是进行绘图。\n\n程序必须实现以下要求：\n\n- 在每次迭代$k$中，使用最速下降方向 $p_k = - \\nabla f(x_k)$。\n- 实现一个强 Wolfe 线搜索，寻找一个步长$\\alpha$，使其满足沿直线 $x_k + \\alpha p_k$ 的充分下降条件和曲率条件。\n- 如果强 Wolfe 线搜索未能返回一个有效的$\\alpha$，则回退到带有缩减因子和最小步长阈值的 Armijo 回溯法。\n- 当梯度$\\lVert \\nabla f(x_k) \\rVert_2$的欧几里得范数小于指定的容差，或迭代次数达到指定的最大值时，终止最速下降迭代。\n\n假设所有计算都是无单位的。不使用角度。不使用百分比。\n\n您的实现必须在以下一组案例上进行测试，这些案例共同涵盖了典型、边界和边缘行为：\n\n- 测试用例 $1$ (典型的困难起点)：$x_0 = (-1.2, 1.0)$，$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长猜测$\\alpha_0 = 1.0$，最大迭代次数 $30$，容差 $10^{-8}$。\n- 测试用例 $2$ (原点起点)：$x_0 = (0.0, 0.0)$，$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长猜测$\\alpha_0 = 1.0$，最大迭代次数 $30$，容差 $10^{-8}$。\n- 测试用例 $3$ (在极小值点的边界情况)：$x_0 = (1.0, 1.0)$，$c_1 = 10^{-4}$，$c_2 = 0.9$，初始步长猜测$\\alpha_0 = 1.0$，最大迭代次数 $10$，容差 $10^{-12}$。这应该产生一个空序列，因为起始点已经是最优的。\n- 测试用例 $4$ (备选曲率参数和初始猜测)：$x_0 = (-1.2, 1.0)$，$c_1 = 10^{-4}$，$c_2 = 0.1$，初始步长猜测$\\alpha_0 = 0.5$，最大迭代次数 $20$，容差 $10^{-8}$。\n\n对于 Armijo 回溯回退策略，使用缩减因子$ \\tau = 0.5 $和最小步长阈值$ \\alpha_{\\min} = 10^{-8} $。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，不含空格，每个结果本身是针对一个测试用例按顺序选择的步长$\\alpha_k$的列表。例如，最终输出必须采用格式 $[R_1,R_2,R_3,R_4]$，其中 $R_i$ 是测试用例 $i$ 的 $\\alpha_k$ 的浮点数列表（例如，`[[0.5,0.25],[0.75],[],[0.5,0.5,0.25]]`，但不含任何空格）。", "solution": "该问题要求为二维 Rosenbrock 函数实现一个最速下降优化算法，并结合一个复杂的线搜索过程。解决方案首先详细介绍数学原理，然后描述算法组件。\n\n### 1. 数学公式\n\n目标函数是 Rosenbrock 函数，这是一个用于优化算法的标准基准测试函数，以其狭窄的抛物线形山谷而闻名。对于向量 $x = (x_1, x_2)$，其定义为：\n$$\nf(x) = (1 - x_1)^2 + 100(x_2 - x_1^2)^2\n$$\n为了实现像最速下降这样的基于梯度的方法，我们必须计算 $f(x)$ 的梯度，记为 $\\nabla f(x)$。其偏导数为：\n$$\n\\frac{\\partial f}{\\partial x_1} = -2(1 - x_1) + 100 \\cdot 2(x_2 - x_1^2) \\cdot (-2x_1) = -2(1 - x_1) - 400x_1(x_2 - x_1^2)\n$$\n$$\n\\frac{\\partial f}{\\partial x_2} = 100 \\cdot 2(x_2 - x_1^2) = 200(x_2 - x_1^2)\n$$\n因此，梯度向量为：\n$$\n\\nabla f(x) = \\begin{pmatrix} -2(1 - x_1) - 400x_1(x_2 - x_1^2) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n### 2. 最速下降算法\n\n最速下降法是一种迭代算法，用于寻找函数的局部最小值。从一个初始点 $x_0$ 开始，它使用以下更新规则生成一系列点 $\\{x_k\\}$：\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\n其中 $k$ 是迭代索引，$p_k$ 是搜索方向，$\\alpha_k > 0$ 是步长。对于最速下降法，搜索方向选择为下降最快的方向，即梯度的负方向：\n$$\np_k = - \\nabla f(x_k)\n$$\n迭代持续进行，直到满足终止准则。在此问题中，如果梯度的欧几里得范数低于指定的容差，即 $\\lVert \\nabla f(x_k) \\rVert_2  \\text{tolerance}$，或者迭代次数达到最大值，即 $k \\ge \\text{max\\_iterations}$，则过程停止。\n\n### 3. 线搜索子问题与 Wolfe 条件\n\n算法的一个关键部分是在每次迭代中确定步长 $\\alpha_k$。这是一个一维优化问题：找到一个 $\\alpha_k$ 来最小化函数在搜索方向上的值，即 $\\phi(\\alpha) = f(x_k + \\alpha p_k)$。与其寻找精确的最小值，不如找到一个能提供足够进展的 $\\alpha_k$ 更为高效。强 Wolfe 条件为此提供了一种标准且有效的方法。\n\n**强 Wolfe 条件**包括步长 $\\alpha$ 必须满足的两个不等式：\n1.  **充分下降条件 (Armijo 条件):**\n    $$\n    f(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k\n    $$\n    此条件确保步长 $\\alpha$ 在目标函数 $f$ 上实现了足够的下降。其中 $c_1$ 是一个常数，通常很小，例如 $c_1 = 10^{-4}$。\n\n2.  **曲率条件:**\n    $$\n    |\\nabla f(x_k + \\alpha p_k)^T p_k| \\le c_2 |\\nabla f(x_k)^T p_k|\n    $$\n    此条件确保新点的函数斜率得到充分减小，从而防止算法采取过小的步长。参数 $c_2$ 满足 $c_1  c_2  1$。对于下降方向，$\\nabla f(x_k)^T p_k  0$，因此该条件可写为 $|\\nabla f(x_k + \\alpha p_k)^T p_k| \\le -c_2 \\nabla f(x_k)^T p_k$。\n\n### 4. 线搜索实现\n\n一个能够找到满足强 Wolfe 条件的步长的稳健线搜索算法通常包括一个两阶段过程：区间限定阶段和缩放阶段。\n\n1.  **区间限定 (Bracketing)**：从一个初始的 $\\alpha$ 猜测值开始，算法生成一系列试验步长，扩大搜索区间，直到它框定了一个或多个满足 Wolfe 条件的点。\n2.  **缩放 (Zooming)**：一旦确定了一个已知包含合适步长的区间 $[\\alpha_{\\text{lo}}, \\alpha_{\\text{hi}}]$，就使用一种程序（例如，二分法或插值法）来放大该区间，以找到满足强 Wolfe 条件的点。\n\n该实现将为此搜索过程设置一个内部迭代限制。如果在此限制内未找到合适的 $\\alpha$，则认为线搜索“失败”，并触发回退机制。\n\n### 5. 回退策略：Armijo 回溯法\n\n如果主强 Wolfe 线搜索失败，算法将回退到一种更简单但更稳健的方法：Armijo 回溯法。此方法仅保证满足充分下降条件。算法如下：\n\n1.  从一个初始步长开始，例如 $\\alpha = \\alpha_0$。\n2.  当充分下降条件不满足时：\n    $$\n    f(x_k + \\alpha p_k) > f(x_k) + c_1 \\alpha \\nabla f(x_k)^T p_k\n    $$\n    将步长乘以一个缩减因子 $\\tau \\in (0, 1)$：\n    $$\n    \\alpha \\leftarrow \\tau \\alpha\n    $$\n3.  当条件满足，或 $\\alpha$ 小于最小阈值 $\\alpha_{\\min}$ 时，过程停止。\n\n### 6. 完整算法摘要\n\n最终程序为每个指定的测试用例集成了这些组件：\n\n1.  初始化起始点 $x_0$ 和参数（$c_1, c_2, \\alpha_0, \\text{max\\_iterations}, \\text{tolerance}$）。\n2.  开始主循环，$k = 0, 1, \\dots, \\text{max\\_iterations}-1$。\n3.  在每次迭代 $k$ 中，计算梯度 $\\nabla f(x_k)$ 及其范数。如果范数低于容差，则终止并返回迄今为止找到的步长序列。\n4.  设置搜索方向 $p_k = - \\nabla f(x_k)$。\n5.  使用初始猜测值 $\\alpha_0$ 调用强 Wolfe 线搜索函数以找到步长 $\\alpha_k$。\n6.  如果 Wolfe 搜索失败（返回失败信号），则调用 Armijo 回溯函数来找到 $\\alpha_k$。回溯的起始 $\\alpha$ 是当前迭代的初始猜测值。\n7.  存储计算出的步长 $\\alpha_k$。\n8.  更新位置：$x_{k+1} = x_k + \\alpha_k p_k$。\n9.  循环终止后（通过收敛或达到最大迭代次数），以指定格式为每个测试用例输出所有存储的 $\\alpha_k$ 值的列表。测试用例 3 中的起始点 $x_0 = (1.0, 1.0)$ 是全局极小值点，因此梯度范数为 $0$，导致立即终止和空的步长列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    \"\"\"\n    Computes the Rosenbrock function value.\n    f(x) = (1 - x_1)^2 + 100 * (x_2 - x_1^2)^2\n    \"\"\"\n    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n\ndef rosenbrock_grad(x):\n    \"\"\"\n    Computes the gradient of the Rosenbrock function.\n    \"\"\"\n    grad_x1 = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n    grad_x2 = 200 * (x[1] - x[0]**2)\n    return np.array([grad_x1, grad_x2])\n\ndef _zoom(phi, phi_prime, alpha_lo, alpha_hi, phi_0, phi_prime_0, c1, c2, max_iter=20):\n    \"\"\"\n    Helper function for strong_wolfe_linesearch.\n    Narrows down an interval known to contain a point satisfying the Wolfe conditions.\n    Based on Algorithm 3.6 in Nocedal  Wright, \"Numerical Optimization\".\n    \"\"\"\n    for _ in range(max_iter):\n        # Use bisection to find a trial step length.\n        alpha_j = (alpha_lo + alpha_hi) / 2.0\n        \n        phi_j = phi(alpha_j)\n\n        if phi_j > phi_0 + c1 * alpha_j * phi_prime_0 or phi_j >= phi(alpha_lo):\n            alpha_hi = alpha_j\n        else:\n            phi_prime_j = phi_prime(alpha_j)\n            if abs(phi_prime_j) = -c2 * phi_prime_0:\n                return alpha_j\n            \n            if phi_prime_j * (alpha_hi - alpha_lo) >= 0:\n                alpha_hi = alpha_lo\n            \n            alpha_lo = alpha_j\n        \n        # Terminate if interval becomes too small to prevent precision issues\n        if abs(alpha_hi - alpha_lo)  1e-12:\n            break\n            \n    # If the loop finishes without finding a point, it failed.\n    # Return the last valid point satisfying sufficient decrease, if any.\n    if phi(alpha_lo) = phi_0 + c1 * alpha_lo * phi_prime_0 and abs(phi_prime(alpha_lo)) = -c2 * phi_prime_0:\n        return alpha_lo\n\n    return None\n\ndef strong_wolfe_linesearch(f, grad, x, p, alpha_init, c1, c2, max_iter=20):\n    \"\"\"\n    Performs a line search to find a step length satisfying the strong Wolfe conditions.\n    Based on Algorithm 3.5 in Nocedal  Wright.\n    Returns the step length alpha, or None if it fails.\n    \"\"\"\n    phi = lambda alpha: f(x + alpha * p)\n    phi_prime = lambda alpha: np.dot(grad(x + alpha * p), p)\n\n    phi_0 = phi(0)\n    phi_prime_0 = phi_prime(0)\n\n    alpha_prev = 0.0\n    alpha_i = alpha_init\n\n    for i in range(max_iter):\n        phi_i = phi(alpha_i)\n\n        if (phi_i > phi_0 + c1 * alpha_i * phi_prime_0) or (i > 0 and phi_i >= phi(alpha_prev)):\n            return _zoom(phi, phi_prime, alpha_prev, alpha_i, phi_0, phi_prime_0, c1, c2)\n\n        phi_prime_i = phi_prime(alpha_i)\n\n        if abs(phi_prime_i) = -c2 * phi_prime_0:\n            return alpha_i\n\n        if phi_prime_i >= 0:\n            return _zoom(phi, phi_prime, alpha_i, alpha_prev, phi_0, phi_prime_0, c1, c2)\n        \n        # If we reach here, expand the search. A simple expansion is used.\n        alpha_prev = alpha_i\n        alpha_i = alpha_i * 2.0\n\n    return None\n\ndef armijo_backtracking(f, grad, x, p, alpha_init, c1, tau, alpha_min):\n    \"\"\"\n    Fallback line search using Armijo backtracking.\n    \"\"\"\n    phi_0 = f(x)\n    phi_prime_0 = np.dot(grad(x), p)\n    \n    alpha = alpha_init\n\n    while f(x + alpha * p) > phi_0 + c1 * alpha * phi_prime_0:\n        alpha = tau * alpha\n        if alpha  alpha_min:\n            return alpha_min\n    \n    return alpha\n\ndef steepest_descent(x0, c1, c2, alpha_init_guess, max_iter, tol, tau, alpha_min):\n    \"\"\"\n    Main steepest descent optimization loop.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    alphas = []\n\n    for k in range(max_iter):\n        grad_val = rosenbrock_grad(x)\n        grad_norm = np.linalg.norm(grad_val)\n\n        if grad_norm  tol:\n            break\n\n        p = -grad_val\n        \n        # Primary line search: strong Wolfe\n        alpha = strong_wolfe_linesearch(rosenbrock, rosenbrock_grad, x, p, alpha_init_guess, c1, c2)\n        \n        # Fallback line search: Armijo\n        if alpha is None:\n            alpha = armijo_backtracking(rosenbrock, rosenbrock_grad, x, p, alpha_init_guess, c1, tau, alpha_min)\n            \n        alphas.append(alpha)\n        x = x + alpha * p\n\n    return alphas\n\ndef solve():\n    \"\"\"\n    Main execution function to run all test cases.\n    \"\"\"\n    test_cases = [\n        # x0, c1, c2, alpha_init_guess, max_iter, tol\n        {'x0': [-1.2, 1.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 30, 'tol': 1e-8},\n        {'x0': [0.0, 0.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 30, 'tol': 1e-8},\n        {'x0': [1.0, 1.0], 'c1': 1e-4, 'c2': 0.9, 'alpha_init': 1.0, 'max_iter': 10, 'tol': 1e-12},\n        {'x0': [-1.2, 1.0], 'c1': 1e-4, 'c2': 0.1, 'alpha_init': 0.5, 'max_iter': 20, 'tol': 1e-8},\n    ]\n\n    # Armijo fallback parameters\n    tau = 0.5\n    alpha_min = 1e-8\n\n    all_results = []\n    for case in test_cases:\n        alphas = steepest_descent(\n            case['x0'], case['c1'], case['c2'], case['alpha_init'],\n            case['max_iter'], case['tol'], tau, alpha_min\n        )\n        all_results.append(alphas)\n\n    # Format the output string to match the required format exactly.\n    result_strings = [str(r).replace(' ', '') for r in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3247710"}]}