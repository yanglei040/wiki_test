{"hands_on_practices": [{"introduction": "最速下降法是求解无约束优化问题最直观的迭代算法。沿着负梯度方向前进的想法虽然简单，但其性能对问题的具体特性和步长 $\\alpha$ 的选择极为敏感。本练习 [@problem_id:3285028] 提供了一个绝佳的动手实践机会，让您能够通过理论分析来精确确定收敛速率，并亲眼见证一个不合适的步长选择如何导致收敛缓慢甚至发散，从而深刻理解该方法的核心动态。", "problem": "考虑在 $\\mathbb{R}^n$ 上对一个二次连续可微的目标函数进行无约束最小化。在数值方法中，一个基本情况是严格凸二次目标函数，\n$$\nf(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x,\n$$\n其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵，$b \\in \\mathbb{R}^{n}$。唯一的最小化子 $x^{\\star}$ 满足 $Q x^{\\star} = b$。采用固定步长 $\\alpha > 0$ 的 Steepest Descent (SD) 方法按如下方式更新\n$$\nx_{k+1} = x_k - \\alpha \\nabla f(x_k),\n$$\n其中 $\\nabla f(x) = Q x - b$。\n\n对于固定步长 $\\alpha$，SD 方法的解析行为可以表现为快速收敛、慢速收敛或发散。本问题中的分类必须基于误差序列的渐进线性速率。定义误差 $e_k = x_k - x^{\\star}$ 和渐进线性速率\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2}.\n$$\n使用以下规则对每个测试用例的行为进行分类：\n- 如果 $r_{\\mathrm{asym}}(\\alpha) \\ge 1$（包括边界情况 $r_{\\mathrm{asym}}(\\alpha) = 1$），则返回整数 $0$ 表示发散或不收敛。\n- 如果 $0.95 \\le r_{\\mathrm{asym}}(\\alpha)  1$，则返回整数 $1$ 表示慢速收敛。\n- 如果 $0 \\le r_{\\mathrm{asym}}(\\alpha)  0.95$，则返回整数 $2$ 表示可接受的收敛速度。\n\n你的程序必须：\n- 对给定的二次实例，从指定的初始点 $x_0$ 开始，实现固定步长 $\\alpha$ 的 SD 迭代，并运行 $N$ 次迭代。\n- 通过求解 $Q x^{\\star} = b$ 来计算 $x^{\\star}$，并经验性地观察误差范数 $\\|e_k\\|_2$。\n- 使用与渐进线性速率定义一致的、有数学原理支持的准则（例如，通过分析误差上的导出线性迭代及其谱性质），来证明每个测试用例分类的合理性。\n- 生成一行输出，其中包含所有测试用例的分类结果，形式为方括号括起来的逗号分隔列表（例如 `[0,1,2]`）。每个列表元素必须是如上定义的整数。\n\n本问题中没有物理单位，也没有角度。所有数值阈值（如 $0.95$）必须视为十进制数。\n\n测试套件（每个测试提供 $(Q, b, x_0, \\alpha, N)$）：\n1. $Q = \\operatorname{diag}(1, 100)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.05$，$N = 50$。\n2. $Q = \\operatorname{diag}(1, 100)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.02$，$N = 50$。\n3. $Q = \\operatorname{diag}(1, 100)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.019$，$N = 200$。\n4. $Q = \\operatorname{diag}(2, 2)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 0.2$，$N = 50$。\n5. $Q = \\operatorname{diag}(0.2, 0.1)$，$b = (1, 1)^{\\top}$，$x_0 = (10, -10)^{\\top}$，$\\alpha = 12$，$N = 50$。\n\n你的程序应生成一行输出，其中包含用方括号括起来的结果，并以逗号分隔（例如 `[result1,result2,result3,result4,result5]`）。", "solution": "该问题要求对 Steepest Descent (SD) 方法的收敛行为进行分类。该方法应用于一个无约束最小化问题，其目标函数为严格凸的二次函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x$，其中 $x \\in \\mathbb{R}^n$，$Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定 (SPD) 矩阵，$b \\in \\mathbb{R}^{n}$。分类必须基于给定固定步长 $\\alpha > 0$ 的渐进线性收敛速率 $r_{\\mathrm{asym}}(\\alpha)$。\n\nSD 迭代由更新规则 $x_{k+1} = x_k - \\alpha \\nabla f(x_k)$ 定义。对于给定的二次目标函数，梯度为 $\\nabla f(x) = Qx - b$。将其代入迭代公式可得：\n$$\nx_{k+1} = x_k - \\alpha(Qx_k - b) = (I - \\alpha Q)x_k + \\alpha b\n$$\n其中 $I$ 是相应维度的单位矩阵。\n\n$f(x)$ 的唯一最小化子（记为 $x^{\\star}$）是线性系统 $\\nabla f(x^\\star) = Qx^{\\star} - b = 0$ 的解，这意味着 $Qx^{\\star} = b$。第 k 次迭代的误差定义为向量 $e_k = x_k - x^{\\star}$。我们可以为误差向量建立一个递归关系：\n$$\ne_{k+1} = x_{k+1} - x^{\\star} = \\left( (I - \\alpha Q)x_k + \\alpha b \\right) - x^{\\star}\n$$\n代入 $b = Qx^{\\star}$，我们可以写出 $\\alpha b = \\alpha Qx^{\\star}$。这使我们能够将误差递推式表示为：\n$$\ne_{k+1} = (I - \\alpha Q)x_k + \\alpha Qx^{\\star} - x^{\\star} = (I - \\alpha Q)x_k - (I - \\alpha Q)x^{\\star}\n$$\n通过提取矩阵 $(I - \\alpha Q)$，我们得到线性误差动态：\n$$\ne_{k+1} = (I - \\alpha Q) e_k\n$$\n该方程表明，每一步的误差是通过将前一步的误差乘以固定的迭代矩阵 $G = I - \\alpha Q$ 得到的。误差大小 $\\|e_k\\|_2$ 的长期行为由该迭代矩阵的谱半径 $\\rho(G)$ 决定，谱半径是其特征值中的最大绝对值。渐进线性收敛速率恰好就是这个谱半径：\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\limsup_{k \\to \\infty} \\frac{\\|e_{k+1}\\|_2}{\\|e_k\\|_2} = \\rho(G) = \\rho(I - \\alpha Q)\n$$\n由于矩阵 $Q$ 是对称的，它是可对角化的，并且其所有特征值（记为 $\\lambda_i$）都是实数。又因为 $Q$ 是正定的，所以其所有特征值都是正数，即 $\\lambda_i > 0$。矩阵 $G = I - \\alpha Q$ 的特征值由 $\\mu_i = 1 - \\alpha \\lambda_i$ 给出。因此，$G$ 的谱半径为：\n$$\n\\rho(G) = \\max_i |\\mu_i| = \\max_i |1 - \\alpha \\lambda_i|\n$$\n对于固定的 $\\alpha > 0$，函数 $g(\\lambda) = |1 - \\alpha \\lambda|$ 是一个 V 形函数，其值取决于 $\\lambda$ 到 $1/\\alpha$ 的距离。该函数在 $Q$ 的特征值集合上的最大值将在最小特征值 $\\lambda_{\\min}$ 或最大特征值 $\\lambda_{\\max}$ 处取得。因此，仅使用 $Q$ 的极端特征值（最小和最大特征值）即可直接计算渐进速率：\n$$\nr_{\\mathrm{asym}}(\\alpha) = \\max \\left( |1 - \\alpha \\lambda_{\\min}|, |1 - \\alpha \\lambda_{\\max}| \\right)\n$$\n该公式为渐进速率提供了一个精确的解析值，这优于通过有限次迭代 ($N$) 进行的经验估计。收敛分类仅依赖于 $Q$ 和 $\\alpha$；初始点 $x_0$ 和迭代次数 $N$ 与确定渐进速率无关。收敛的条件是 $r_{\\mathrm{asym}}(\\alpha)  1$，当且仅当 $0  \\alpha  2/\\lambda_{\\max}$ 时成立。\n\n现在我们将此解析结果应用于每个测试用例。\n\n用例 1: $Q = \\operatorname{diag}(1, 100)$，$\\alpha = 0.05$。\n对角矩阵 $Q$ 的特征值是其对角元素。因此，$\\lambda_{\\min} = 1$ 且 $\\lambda_{\\max} = 100$。\n$r_{\\mathrm{asym}}(0.05) = \\max(|1 - (0.05)(1)|, |1 - (0.05)(100)|) = \\max(|1 - 0.05|, |1 - 5|) = \\max(0.95, 4.0) = 4.0$。\n由于 $r_{\\mathrm{asym}}(\\alpha) = 4.0 \\ge 1$，该方法发散。分类为 $0$。\n\n用例 2: $Q = \\operatorname{diag}(1, 100)$，$\\alpha = 0.02$。\n特征值为 $\\lambda_{\\min} = 1$ 且 $\\lambda_{\\max} = 100$。步长 $\\alpha=0.02$ 正好在稳定性边界上，即 $\\alpha = 2/\\lambda_{\\max} = 2/100 = 0.02$。\n$r_{\\mathrm{asym}}(0.02) = \\max(|1 - (0.02)(1)|, |1 - (0.02)(100)|) = \\max(|1 - 0.02|, |1 - 2|) = \\max(0.98, 1.0) = 1.0$。\n由于 $r_{\\mathrm{asym}}(\\alpha) = 1.0 \\ge 1$，该方法未能收敛。分类为 $0$。\n\n用例 3: $Q = \\operatorname{diag}(1, 100)$，$\\alpha = 0.019$。\n特征值为 $\\lambda_{\\min} = 1$ 且 $\\lambda_{\\max} = 100$。\n$r_{\\mathrm{asym}}(0.019) = \\max(|1 - (0.019)(1)|, |1 - (0.019)(100)|) = \\max(|1 - 0.019|, |1 - 1.9|) = \\max(0.981, 0.9) = 0.981$。\n由于 $0.95 \\le 0.981  1$，收敛被分类为慢速。分类为 $1$。\n\n用例 4: $Q = \\operatorname{diag}(2, 2)$，$\\alpha = 0.2$。\n特征值相同：$\\lambda_{\\min} = \\lambda_{\\max} = 2$。\n$r_{\\mathrm{asym}}(0.2) = |1 - (0.2)(2)| = |1 - 0.4| = 0.6$。\n由于 $0 \\le 0.6  0.95$，收敛被分类为可接受。分类为 $2$。\n\n用例 5: $Q = \\operatorname{diag}(0.2, 0.1)$，$\\alpha = 12$。\n特征值为 $\\lambda_{\\min} = 0.1$ 且 $\\lambda_{\\max} = 0.2$。稳定性边界是 $2/\\lambda_{\\max} = 2/0.2 = 10$。步长 $\\alpha = 12$ 在此范围之外。\n$r_{\\mathrm{asym}}(12) = \\max(|1 - (12)(0.1)|, |1 - (12)(0.2)|) = \\max(|1 - 1.2|, |1 - 2.4|) = \\max(0.2, 1.4) = 1.4$。\n由于 $r_{\\mathrm{asym}}(\\alpha) = 1.4 \\ge 1$，该方法发散。分类为 $0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the convergence classification for several Steepest Descent scenarios\n    on a quadratic objective function. The classification is based on the analytical\n    asymptotic convergence rate derived from the spectral properties of the iteration matrix.\n    \"\"\"\n    \n    # Test suite format: (Q, b, x0, alpha, N)\n    test_cases = [\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.05, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.02, 50),\n        (np.diag([1.0, 100.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.019, 200),\n        (np.diag([2.0, 2.0]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 0.2, 50),\n        (np.diag([0.2, 0.1]), np.array([1.0, 1.0]), np.array([10.0, -10.0]), 12.0, 50),\n    ]\n\n    results = []\n    for Q, b, x0, alpha, N in test_cases:\n        # The convergence classification for a quadratic objective depends only on the \n        # matrix Q and the step size alpha. The parameters b, x0, and N do not affect\n        # the asymptotic rate.\n        \n        # The analytical asymptotic rate, r_asym, is the spectral radius of the \n        # iteration matrix G = I - alpha*Q. For a symmetric matrix Q, this is:\n        # r_asym = max(|1 - alpha*lambda_min|, |1 - alpha*lambda_max|)\n        # where lambda_min and lambda_max are the minimum and maximum eigenvalues of Q.\n        \n        # For the diagonal matrices in the test cases, the eigenvalues are simply the\n        # diagonal elements.\n        eigenvalues = np.diag(Q)\n        lambda_min = np.min(eigenvalues)\n        lambda_max = np.max(eigenvalues)\n        \n        # Calculate the asymptotic convergence rate\n        rate = max(abs(1.0 - alpha * lambda_min), abs(1.0 - alpha * lambda_max))\n        \n        # Classify the behavior based on the problem's rules for the rate.\n        classification = 0 # Default: divergence or non-convergence\n        if rate  1.0:\n            if rate  0.95:\n                classification = 2 # Acceptable convergence\n            else: # 0.95 = rate  1.0\n                classification = 1 # Slow convergence\n        \n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3285028"}, {"introduction": "从一阶方法的最速下降法更进一步，我们来探索使用二阶信息（曲率）的牛顿法。牛顿法利用海森矩阵 (Hessian matrix) 来构建目标函数的二次近似模型，从而能够计算出更高效的下降步长。然而，这种强大能力也伴随着一个有趣的特性：牛顿法的目标是寻找梯度为零的任何驻点，而不仅仅是局部最小值。这个练习 [@problem_id:3285127] 将引导您设计一个包含鞍点的目标函数，并观察牛顿法如何根据初始点的不同，或被吸引到鞍点，或被排斥向最小值点，这对于理解其在非凸问题中的行为至关重要。", "problem": "考虑二维无约束优化问题。对于一个二次连续可微的目标函数 $f:\\mathbb{R}^2 \\to \\mathbb{R}$，如果其梯度 $\\nabla f(x^\\star)$ 等于 $0$，则点 $x^\\star \\in \\mathbb{R}^2$ 是一个驻点。驻点可以通过海森矩阵 $\\nabla^2 f(x^\\star)$ 的特征值进行分类：正定矩阵对应局部最小值，负定矩阵对应局部最大值，而混合符号（不定矩阵）则对应鞍点。无约束优化的牛顿法是通过在当前迭代点 $x_k$ 处构建 $f$ 的二阶泰勒模型，然后选择一个步长 $p_k$ 使得该二次模型的梯度在 $x_k+p_k$ 处为零，并更新 $x_{k+1}=x_k+p_k$。在实践中，当海森矩阵为不定时，可以引入关于梯度范数的回溯线搜索，以促进稳定的收敛进程。\n\n设计如下的二维目标函数：\n$$\nf(x,y) = x^2 - y^2 + (x^2 + y^2)^2,\n$$\n该函数在 $\\mathbb{R}^2$ 上是二次连续可微的。从驻点的基本定义和 $f$ 的二阶泰勒模型出发，推导此问题的牛顿迭代法，并将其实现为一个程序，该程序能够：\n- 符号化地计算梯度 $\\nabla f(x,y)$ 和海森矩阵 $\\nabla^2 f(x,y)$，并在任意点 $(x,y)$ 处进行数值评估。\n- 应用牛顿法，通过求解线性系统来获得步长 $p_k$（该步长使二次模型的梯度在 $x_k+p_k$ 处为零），并使用关于梯度范数的回溯线搜索来更新迭代点，以避免在海森矩阵不定时出现不稳定的步长。\n- 当梯度范数低于一个很小的容差，或步长范数极小，或达到指定的最大迭代次数时，程序终止。\n\n使用收敛点处 $\\nabla^2 f$ 的特征值，将该驻点分为以下几类：\n- $0$ 表示鞍点（特征值符号混合）。\n- $1$ 表示局部最小值（两个特征值均为严格正数）。\n- $2$ 表示局部最大值（两个特征值均为严格负数）。\n- $3$ 表示在迭代次数限制内未能收敛到驻点。\n\n您的程序必须在以下起始点测试集上运行牛顿法（每个点是一个有序对 $(x_0,y_0)$）：\n- $(0.05,\\,0.05)$\n- $(0.05,\\,0.9)$\n- $(0.05,\\,-0.9)$\n- $(0.8,\\,0.1)$\n- $(2.0,\\,2.0)$\n- $(0.0,\\,0.0)$\n\n对于每个测试用例，返回一个列表，其中包含分类整数、所用迭代次数以及保留六位小数的终点坐标 $(x^\\star,y^\\star)$。最终的输出格式必须是单行文本，包含一个由方括号括起来的、逗号分隔的各用例列表，例如：\n$$\n[[c_1,n_1,x^\\star_1,y^\\star_1], [c_2,n_2,x^\\star_2,y^\\star_2], \\ldots]\n$$\n本问题不涉及物理单位或角度。使用梯度范数容差 $10^{-8}$，步长范数容差 $10^{-12}$，以及最大迭代次数 $100$。如果在任何一次迭代中海森矩阵是奇异的或病态的，通过向海森矩阵添加一个小的对角正则化项来稳定线性求解，并使用回溯法来单调减小梯度范数。", "solution": "目标函数 $f(x,y) = x^2 - y^2 + (x^2 + y^2)^2$ 是一个多项式，因此是二次连续可微的。我们从无约束优化的基本定义开始。驻点 $x^\\star$ 满足 $\\nabla f(x^\\star)=0$。$f$ 在 $x_k$ 周围的二阶泰勒展开为\n$$\nm_k(p) = f(x_k) + \\nabla f(x_k)^\\top p + \\frac{1}{2} p^\\top \\nabla^2 f(x_k) p,\n$$\n其中 $p \\in \\mathbb{R}^2$ 是从 $x_k$ 开始的步长。牛顿法通过选择 $p_k$ 使得模型梯度在 $p$ 处为零来获得，即\n$$\n\\nabla m_k(p) = \\nabla f(x_k) + \\nabla^2 f(x_k) p = 0,\n$$\n这导出了线性系统\n$$\n\\nabla^2 f(x_k) \\, p_k = -\\nabla f(x_k).\n$$\n迭代点按 $x_{k+1}=x_k+p_k$ 更新。当 $\\nabla^2 f(x_k)$ 是不定的或病态的时，纯牛顿步可能不稳定。一种广泛使用的稳定化方法是添加一个小的对角正则化项 $\\lambda I$ 和进行回溯线搜索；这里我们监控梯度范数以确保其单调递减：\n$$\n\\|\\nabla f(x_k + \\alpha p_k)\\|  \\|\\nabla f(x_k)\\|, \\quad \\text{with backtracking on } \\alpha \\in (0,1].\n$$\n\n现在我们推导 $f$ 的梯度和海森矩阵。定义 $r^2 = x^2 + y^2$。那么\n$$\nf(x,y) = x^2 - y^2 + r^4.\n$$\n梯度的分量为\n$$\n\\frac{\\partial f}{\\partial x} = 2x + 4x r^2 = 2x + 4x(x^2 + y^2), \\quad\n\\frac{\\partial f}{\\partial y} = -2y + 4y r^2 = -2y + 4y(x^2 + y^2).\n$$\n因此\n$$\n\\nabla f(x,y) = \\begin{bmatrix} 2x + 4x(x^2 + y^2) \\\\ -2y + 4y(x^2 + y^2) \\end{bmatrix}.\n$$\n对于海森矩阵，对梯度进行微分：\n$$\n\\frac{\\partial^2 f}{\\partial x^2} = 2 + 12x^2 + 4y^2, \\quad\n\\frac{\\partial^2 f}{\\partial y^2} = -2 + 4x^2 + 12y^2, \\quad\n\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x} = 8xy.\n$$\n所以\n$$\n\\nabla^2 f(x,y) = \\begin{bmatrix}\n2 + 12x^2 + 4y^2  8xy \\\\\n8xy  -2 + 4x^2 + 12y^2\n\\end{bmatrix}.\n$$\n\n通过求解 $\\nabla f(x,y)=0$ 来获得驻点。$x$ 分量方程为\n$$\n2x + 4x(x^2+y^2) = 2x\\left(1 + 2(x^2+y^2)\\right) = 0,\n$$\n这意味着 $x=0$ 或 $1 + 2(x^2+y^2)=0$，但后者没有实数解，所以 $x=0$。$y$ 分量方程为\n$$\n-2y + 4y(x^2+y^2) = 2y\\left(-1 + 2(x^2+y^2)\\right) = 0,\n$$\n这意味着 $y=0$ 或 $-1 + 2(x^2+y^2) = 0$。当 $x=0$ 时，第二种情况给出 $2y^2 = 1$，因此 $y = \\pm \\frac{1}{\\sqrt{2}}$。因此，驻点为 $(0,0)$、$\\left(0, \\frac{1}{\\sqrt{2}}\\right)$ 和 $\\left(0, -\\frac{1}{\\sqrt{2}}\\right)$。\n\n为了对驻点进行分类，我们检查海森矩阵。在点 $(0,0)$ 处，\n$$\n\\nabla^2 f(0,0) = \\begin{bmatrix} 2  0 \\\\ 0  -2 \\end{bmatrix},\n$$\n该矩阵是不定的（一个正特征值和一个负特征值）。因此 $(0,0)$ 是一个鞍点。在点 $\\left(0, \\pm \\frac{1}{\\sqrt{2}}\\right)$ 处，使用 $x=0$ 和 $y^2 = \\frac{1}{2}$，\n$$\n\\nabla^2 f\\left(0, \\pm \\tfrac{1}{\\sqrt{2}}\\right) = \\begin{bmatrix} 2 + 4\\cdot \\tfrac{1}{2}  0 \\\\ 0  -2 + 12\\cdot \\tfrac{1}{2} \\end{bmatrix} = \\begin{bmatrix} 4  0 \\\\ 0  4 \\end{bmatrix},\n$$\n该矩阵是正定的。因此 $\\left(0, \\pm \\frac{1}{\\sqrt{2}}\\right)$ 是严格的局部最小值点。\n\n牛顿法在 $(0,0)$ 附近的行为可以通过局部线性化来分析。对于小的 $(x,y)$，梯度近似为 $\\nabla f(x,y) \\approx \\begin{bmatrix} 2x \\\\ -2y \\end{bmatrix}$，海森矩阵近似为 $\\nabla^2 f(x,y) \\approx \\begin{bmatrix} 2  0 \\\\ 0  -2 \\end{bmatrix}$。牛顿步长 $p$ 求解\n$$\n\\begin{bmatrix} 2  0 \\\\ 0  -2 \\end{bmatrix} p = - \\begin{bmatrix} 2x \\\\ -2y \\end{bmatrix},\n$$\n这得到 $p \\approx \\begin{bmatrix} -x \\\\ y \\end{bmatrix}$，因此在一次迭代中 $x_{k+1} \\approx \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。这表明当两个坐标都足够小时，迭代点会被吸引到鞍点。相反，当 $y$ 的值适中，使得 $-1 + 2(x^2 + y^2)$ 变为正数时，$y$ 方向的曲率变为强正，牛顿步结合回溯线搜索会倾向于向 $y=\\pm \\frac{1}{\\sqrt{2}}$ 处的最小值移动，因此根据起始点的不同，会沿着不稳定方向被鞍点排斥。这种二分性说明了无约束优化的牛顿法是如何求解 $\\nabla f(x)=0$ 而不考虑驻点类型的：如果初始猜测位于其吸引盆地内，它可能被吸引到鞍点，否则会被排斥到最小值点。\n\n实现的算法设计：\n- 精确计算推导出的 $\\nabla f(x,y)$ 和 $\\nabla^2 f(x,y)$。\n- 在每次迭代中，尝试求解 $\\nabla^2 f(x_k)\\, p_k = -\\nabla f(x_k)$。如果海森矩阵是奇异或接近奇异的，在求解前用一个小的对角阵 $\\lambda I$ 进行稳定化。\n- 对步长 $\\alpha$ 使用回溯法，以单调减小 $\\|\\nabla f(x_k + \\alpha p_k)\\|$。\n- 当 $\\|\\nabla f(x_k)\\| \\le 10^{-8}$ 或 $\\| \\alpha p_k \\| \\le 10^{-12}$，或在 $100$ 次迭代后终止。\n- 根据海森矩阵特征值的符号对收敛点进行分类。\n\n基于分析，对指定测试集的预期结果：\n- 从 $(0.05,0.05)$ 开始：被吸引到鞍点 $(0,0)$，分类为 $0$。\n- 从 $(0.05,0.9)$ 开始：沿 $y$ 轴被鞍点排斥，收敛到 $\\left(0,\\tfrac{1}{\\sqrt{2}}\\right)$，分类为 $1$。\n- 从 $(0.05,-0.9)$ 开始：与前一情况对称，收敛到 $\\left(0,-\\tfrac{1}{\\sqrt{2}}\\right)$，分类为 $1$。\n- 从 $(0.8,0.1)$ 开始：通过两个坐标的减小被吸引到鞍点，分类为 $0$。\n- 从 $(2.0,2.0)$ 开始：沿 $y$ 轴被鞍点排斥，收敛到 $\\left(0,\\tfrac{1}{\\sqrt{2}}\\right)$ 附近的局部最小值，分类为 $1$。\n- 从 $(0.0,0.0)$ 开始：立即识别为驻点（鞍点），分类为 $0$。\n\n程序将为每个用例报告分类整数、迭代次数和保留六位小数的最终点 $(x^\\star,y^\\star)$，并采用规定的单行格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(xy: np.ndarray) - float:\n    x, y = xy\n    r2 = x*x + y*y\n    return x*x - y*y + r2*r2\n\ndef grad(xy: np.ndarray) - np.ndarray:\n    x, y = xy\n    r2 = x*x + y*y\n    return np.array([2.0*x + 4.0*x*r2, -2.0*y + 4.0*y*r2], dtype=float)\n\ndef hess(xy: np.ndarray) - np.ndarray:\n    x, y = xy\n    h11 = 2.0 + 12.0*x*x + 4.0*y*y\n    h22 = -2.0 + 4.0*x*x + 12.0*y*y\n    h12 = 8.0*x*y\n    return np.array([[h11, h12], [h12, h22]], dtype=float)\n\ndef newton_optimize(x0: tuple,\n                    tol_grad: float = 1e-8,\n                    tol_step: float = 1e-12,\n                    max_iter: int = 100) - tuple[np.ndarray, int, bool]:\n    x = np.array([float(x0[0]), float(x0[1])], dtype=float)\n    iters = 0\n    for k in range(max_iter):\n        g = grad(x)\n        normg = float(np.linalg.norm(g))\n        if normg = tol_grad:\n            return x, k, True\n        H = hess(x)\n\n        # Stabilize Hessian if singular or ill-conditioned\n        # Try small diagonal regularization if needed.\n        lam = 0.0\n        p = None\n        for attempt in range(6):\n            try:\n                if lam > 0.0:\n                    Hp = H + lam * np.eye(2)\n                else:\n                    Hp = H\n                p = np.linalg.solve(Hp, -g)\n                break\n            except np.linalg.LinAlgError:\n                lam = 1e-8 if lam == 0.0 else lam * 10.0\n                continue\n        if p is None:\n            return x, k, False\n\n        # Backtracking line search to reduce gradient norm\n        alpha = 1.0\n        accepted = False\n        for _ in range(20):\n            x_new = x + alpha * p\n            if np.linalg.norm(grad(x_new))  normg:\n                x = x_new\n                accepted = True\n                break\n            alpha *= 0.5\n        if not accepted:\n            # Even if not strictly decreasing, take a very small step to avoid stagnation\n            x = x + alpha * p\n\n        step_norm = float(np.linalg.norm(alpha * p))\n        iters = k + 1\n        if step_norm = tol_step:\n            # If step is tiny, check convergence by gradient\n            if float(np.linalg.norm(grad(x))) = tol_grad:\n                return x, iters, True\n            else:\n                return x, iters, False\n\n    # Max iterations reached\n    return x, max_iter, float(np.linalg.norm(grad(x))) = tol_grad\n\ndef classify_stationary_point(xy: np.ndarray) - int:\n    # 0: saddle (mixed signs), 1: minimum (all positive), 2: maximum (all negative), 3: failure (handled outside)\n    H = hess(xy)\n    eigs = np.linalg.eigvals(H)\n    tol = 1e-7\n    if np.all(eigs > tol):\n        return 1\n    elif np.all(eigs  -tol):\n        return 2\n    else:\n        return 0\n\ndef format_result(code: int, iters: int, xy: np.ndarray) - str:\n    # Produce no-space list representation: [code,iters,x,y] with six decimal places\n    return f\"[{code},{iters},{xy[0]:.6f},{xy[1]:.6f}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.05, 0.05),\n        (0.05, 0.9),\n        (0.05, -0.9),\n        (0.8, 0.1),\n        (2.0, 2.0),\n        (0.0, 0.0),\n    ]\n\n    results_strs = []\n    for case in test_cases:\n        xy_star, iters, success = newton_optimize(case, tol_grad=1e-8, tol_step=1e-12, max_iter=100)\n        if success:\n            code = classify_stationary_point(xy_star)\n        else:\n            code = 3\n        results_strs.append(format_result(code, iters, xy_star))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_strs)}]\")\n\nsolve()\n```", "id": "3285127"}]}