## 引言
在[优化问题](@entry_id:266749)中，寻找一个函数的最小值是一项核心任务。当我们处理单变量函数，特别是那些导数未知或难以计算的[单峰函数](@entry_id:143107)时，我们需要一种不依赖导数、既可靠又高效的搜索方法。传统的[网格搜索](@entry_id:636526)虽然直观，但效率低下，尤其是在要求高精度时。这便引出了一个关键问题：我们能否设计一种更智能的策略，以最小的计算成本系统性地逼近最优点？

本文将深入探讨[黄金分割搜索](@entry_id:146661)（Golden Section Search）算法，这正是上述问题的经典答案。通过三个章节的系统学习，你将全面掌握这一强大的优化工具。第一章“原理与机制”将从第一性原理出发，揭示算法如何利用黄金比例实现最优的区间缩减效率，并分析其收敛性与实际应用中的挑战。第二章“应用与跨学科联系”将展示该算法在工程、金融、机器学习等多个领域的广泛应用，让你领会理论在解决真实世界问题时的力量。最后，在“动手实践”部分，你将通过编程练习将理论知识转化为实践技能。让我们首先进入第一章，探究[黄金分割搜索](@entry_id:146661)算法精妙的内部工作原理。

## 原理与机制

在上一章中，我们介绍了单变量优化的基本概念，特别是对于[单峰函数](@entry_id:143107)在一维区间上寻找最小值的挑战。现在，我们将深入探讨解决这一问题的[黄金分割搜索](@entry_id:146661)算法（Golden Section Search, GSS）的内部原理和机制。本章将从第一性原理出发，推导该算法的核心思想，分析其效率和收敛性，并讨论在实际应用中必须考虑的各种实际问题，如设置初始区间、选择终止条件以及处理有限精度计算和噪声数据带来的影响。

### 区间缩减的基本原理

所有区间缩减方法的核心思想都是：通过在当前包含最优解的区间内进行“探测”，获取函数信息，从而判断并舍弃一个不包含最优解的子区间，得到一个更小的、但仍然保证包含最优解的新区间。不断重复此过程，直到区间的长度小到可以接受的程度。

那么，为了可靠地缩小区间，我们需要多少信息呢？假设我们有一个定义在 $[a,b]$ 上的[单峰函数](@entry_id:143107) $f(x)$。

- 如果我们只在区间内的一个点 $x_1$ 处计算函数值 $f(x_1)$，我们无法获得任何关于最小值位置的有效信息。最小值可能在 $x_1$ 的左侧，也可能在右侧。

- 如果我们在两个点 $x_1  x_2$ 处计算函数值，情况会如何？
    - 若 $f(x_1)  f(x_2)$，根据单峰性定义，最小值 $x^\star$ 不可能在 $x_2$ 的右侧（因为函数在 $x^\star$ 右侧是严格单增的），所以 $x^\star$ 必然在 $[a, x_2]$ 内。但我们无法舍弃 $[a, x_1]$，因为最小值可能就在那里。我们仅仅将区间的右端点从 $b$ 缩减到了 $x_2$。
    - 若 $f(x_1) > f(x_2)$，同理，最小值 $x^\star$ 必然在 $[x_1, b]$ 内。
    - 若 $f(x_1) = f(x_2)$，最小值 $x^\star$ 必然在 $[x_1, x_2]$ 内。

虽然两点探测可以缩小区间，但效率并不理想。关键问题在于，我们无法在每一步都舍弃一个固定比例的区间。为了构建一个更高效、更具确定性的算法，我们需要一种方法，能在每一步都明确地排除掉一个“外部”的子区间。

这引出了一个根本性的问题：证明一个函数在某个区间上**不是**单峰的，最少需要多少个点？考虑三个点 $x_1  x_2  x_3$。一个[单峰函数](@entry_id:143107)的形状就像一个“山谷”，它不能先上升后下降。因此，如果我们观测到 $f(x_1)  f(x_2)$ 并且 $f(x_2) > f(x_3)$ 这样的“山峰”形状，我们就得到了一个明确的证据，证明该函数在包含这三点的区间内不是（用于最小化的）[单峰函数](@entry_id:143107)。反过来，这也揭示了维持“括号”的秘诀：我们需要找到一个满足 $f(x_1) > f(x_2)  f(x_3)$ 的三点组 $(x_1, x_2, x_3)$。这样的一个三点组就构成了一个有效的“括号”，因为它保证了最小值位于区间 $(x_1, x_3)$ 内部。[@problem_id:3237509]

因此，所有高效的区间缩减算法都基于一个四点模式：区间的两个端点 $a$ 和 $b$，以及两个内部探测点 $c$ 和 $d$（$a  c  d  b$）。通过比较 $f(c)$ 和 $f(d)$ 的值，我们可以做出决策：
- 如果 $f(c)  f(d)$，由于函数在 $x^\star$ 右侧是单增的，所以最小值 $x^\star$ 不可能位于 $[d, b]$ 区间。因此，我们可以安全地将新的搜索区间缩减为 $[a, d]$。
- 如果 $f(c) > f(d)$，由于函数在 $x^\star$ 左侧是单减的，所以最小值 $x^\star$ 不可能位于 $[a, c]$ 区间。新的搜索区间可以缩减为 $[c, b]$。

这个简单的比较规则是[黄金分割搜索](@entry_id:146661)以及类似算法的基石。它保证了在每一步迭代中，我们都能舍弃区间的一部分，同时确保最小值仍在我们的掌控之中。

### [黄金分割](@entry_id:139097)：最优比例的推导

我们已经确定了通过两点探测来缩减区间的基本策略。现在的问题是：这两个内部点 $c$ 和 $d$ 应该放在哪里最合适？为了实现最高的效率，我们希望每次迭代都能以一个恒定的比例 $\tau$ 缩减区间长度，并且尽可能地复用上一步的计算结果。

让我们在长度为 $L_k = b_k - a_k$ 的区间 $[a_k, b_k]$ 内设置探测点。为了对称和一致性，我们将探测点的位置与区间长度关联起来。假设点 $c_k$ 和 $d_k$ 分别为：
$$
c_k = a_k + (1-\tau) L_k \\
d_k = a_k + \tau L_k
$$
为了满足 $c_k  d_k$，必须有 $1-\tau  \tau$，即 $\tau > 1/2$。

现在，我们来分析区间缩减后的情况：
1.  如果 $f(c_k)  f(d_k)$，新区间为 $[a_{k+1}, b_{k+1}] = [a_k, d_k]$。其长度 $L_{k+1} = d_k - a_k = \tau L_k$。
2.  如果 $f(c_k) \ge f(d_k)$，新区间为 $[a_{k+1}, b_{k+1}] = [c_k, b_k]$。其长度 $L_{k+1} = b_k - c_k = b_k - (a_k + (1-\tau)L_k) = (b_k - a_k) - (1-\tau)L_k = L_k(1-(1-\tau)) = \tau L_k$。

无论哪种情况，区间的长度都以相同的比例 $\tau$ 缩小。这是我们所期望的。接下来，考虑“复用计算”这一关键效率要求。我们希望下一轮迭代 $[a_{k+1}, b_{k+1}]$ 中的一个新探测点恰好是当前这一轮迭代中被保留下来的那个旧探测点。

让我们以情况1为例进行分析。新区间是 $[a_k, d_k]$，其长度为 $\tau L_k$。在这一轮中，我们保留了点 $c_k$。下一轮的两个新探测点 $c_{k+1}$ 和 $d_{k+1}$ 将位于 $[a_k, d_k]$ 内。为了保持几何结构的[自相似性](@entry_id:144952)，新的探测点应该与旧的探测点在缩放后的[坐标系](@entry_id:156346)下位置相同。具体来说，点 $c_k$ 在区间 $[a_k, d_k]$ 内的相对位置，必须等于下一轮迭代中某个探测点（比如 $c_{k+1}$ 或 $d_{k+1}$）在 $[a_k, d_k]$ 内的相对位置。

将区间 $[a_k, b_k]$ 归一化为 $[0, 1]$。则 $c_k$ 在 $1-\tau$ 处， $d_k$ 在 $\tau$ 处。新区间是 $[0, \tau]$。在 $[0, \tau]$ 这个新区间里，原来的 $c_k$（其绝对位置是 $1-\tau$）仍然存在。而新一轮的两个探测点，按照同样的比例规则，应该位于新区间 $[0, \tau]$ 的 $(1-\tau)$ 和 $\tau$ 比例处，即绝对位置为 $(1-\tau)\tau$ 和 $\tau^2$。

为了复用计算，旧点 $c_k$ 的位置必须与其中一个新点的位置重合。也就是说，必须满足：
$$
1-\tau = (1-\tau)\tau \quad \text{或} \quad 1-\tau = \tau^2
$$
第一个方程给出 $\tau^2 - 2\tau + 1 = 0$，解得 $\tau=1$，这不符合 $\tau \in (1/2, 1)$ 的要求。
第二个方程是 $\tau^2 + \tau - 1 = 0$。利用[二次方程](@entry_id:163234)[求根](@entry_id:140351)公式，我们得到：
$$
\tau = \frac{-1 \pm \sqrt{1^2 - 4(1)(-1)}}{2} = \frac{-1 \pm \sqrt{5}}{2}
$$
由于 $\tau$ 必须是正数，我们取[正根](@entry_id:199264)：
$$
\tau = \frac{\sqrt{5}-1}{2} \approx 0.618
$$
这个数值正是[黄金比例](@entry_id:139097) $\phi = \frac{1+\sqrt{5}}{2}$ 的倒数，即 $\tau = 1/\phi$。这个比例保证了每一步迭代都只需要进行一次新的函数求值，极大地提高了算法的效率。对情况2（$f(c_k) \ge f(d_k)$）进行对称分析，会得到完全相同的结果。[@problem_id:3237403]

这种选择不仅是出于几何上的巧妙，它在概率意义上也是最优的。如果我们假设最小值的位置在初始区间内是[均匀分布](@entry_id:194597)的，那么采用黄金分割比例可以最小化单步迭代后剩余区间长度的[期望值](@entry_id:153208)（即期望误差）。[@problem_id:3237501]

### 算法流程与[收敛性分析](@entry_id:151547)

基于以上原理，我们可以明确[黄金分割搜索](@entry_id:146661)的算法流程：
1.  给定包含最小值的初始区间 $[a_0, b_0]$ 和[收敛判据](@entry_id:158093)。
2.  计算初始的两个内部探测点 $c_0$ 和 $d_0$。这需要两次函数求值。
3.  进入循环（对于 $k=0, 1, 2, \dots$）：
    a. 比较 $f(c_k)$ 和 $f(d_k)$。
    b. 若 $f(c_k)  f(d_k)$，则令 $[a_{k+1}, b_{k+1}] = [a_k, d_k]$。新的 $d_{k+1}$ 是旧的 $c_k$（无需重算），只需计算一个新的 $c_{k+1}$。
    c. 若 $f(c_k) \ge f(d_k)$，则令 $[a_{k+1}, b_{k+1}] = [c_k, b_k]$。新的 $c_{k+1}$ 是旧的 $d_k$（无需重算），只需计算一个新的 $d_{k+1}$。
    d. 检查是否满足终止条件。若满足，则停止；否则，继续下一次迭代。

**收敛性与效率**

[黄金分割搜索](@entry_id:146661)的收敛性是其最重要的特性之一。在每次迭代中，区间长度都精确地缩减为原来的 $\tau$ 倍。因此，经过 $k$ 次迭代后，区间长度 $L_k$ 为：
$$
L_k = \tau^k L_0 = \left( \frac{\sqrt{5}-1}{2} \right)^k L_0
$$
由于 $\tau \approx 0.618  1$，当 $k \to \infty$ 时，$L_k \to 0$。这意味着算法保证收敛。这种[收敛方式](@entry_id:189917)被称为**[线性收敛](@entry_id:163614)**，收敛因子为 $\tau$。

这种收敛效率有多高？让我们将其与一种朴素的“[网格搜索](@entry_id:636526)”或“均匀采样”方法进行比较。假设我们需要将长度为 $L_0=1$ 的区间缩小到长度不超过 $\tau_{tol} = 10^{-3}$。
- 对于**[黄金分割搜索](@entry_id:146661)**，我们需要进行的函数求值次数 $m$ 满足 $L_0 \tau^{m-1} \le \tau_{tol}$。解得 $m-1 \ge \frac{\ln(\tau_{tol}/L_0)}{\ln(\tau)}$，即 $m \ge 1 + \frac{\ln(10^{-3})}{\ln(0.618)} \approx 1 + \frac{-6.907}{ -0.481} \approx 15.3$。因此，总共需要约 $16$ 次函数求值。
- 对于**均匀采样**，为了保证最终的[不确定性区间](@entry_id:269091)长度不超过 $\tau_{tol}$，我们需要在区间内均匀放置 $N$ 个点。在计算完所有 $f(x_i)$ 后，找到最小值 $f(x_{min})$ 所在的点 $x_{min}$。由于函数的单峰性，真正的最小值 $x^\star$ 位于 $x_{min}$ 的两个相邻采样点之间，这个区间的长度大约是 $2 \times (\text{采样间隔})$。为了保证这个不确定区间长度不大于 $\tau_{tol}$，采样间隔需要小于 $\tau_{tol}/2$。因此，需要的采样点数 $N \approx L_0 / (\tau_{tol}/2) = 2/\tau_{tol} = 2000$。

假设每次函数求值都非常耗时（例如，需要运行一个复杂的计算机模拟），成本为 $1.2$ 秒。[黄金分割搜索](@entry_id:146661)的总成本约为 $16 \times 1.2 = 19.2$ 秒，而均匀采样的成本约为 $2000 \times 1.2 = 2400$ 秒。在这个例子中，[黄金分割搜索](@entry_id:146661)的效率是均匀采样的一百多倍。从渐进复杂度来看，[黄金分割搜索](@entry_id:146661)的函数求值次数与 $\log(1/\tau_{tol})$ 成正比，而均匀采样则与 $1/\tau_{tol}$ 成正比。当要求的精度越来越高时（$\tau_{tol} \to 0$），[黄金分割搜索](@entry_id:146661)的优势会愈发明显。[@problem_id:2421080]

最后，由于区间 $[a_k, b_k]$ 始终包含真解 $x^\star$，且其长度 $L_k \to 0$，根据[夹逼定理](@entry_id:147218)，我们有 $\lim_{k\to\infty} a_k = \lim_{k\to\infty} b_k = x^\star$。设 $x_k$ 为第 $k$ 步找到的最佳点（即 $c_k$ 和 $d_k$ 中函数值较小的那个），则 $a_k \le x_k \le b_k$，因此 $\lim_{k\to\infty} x_k = x^\star$。又因为函数 $f(x)$ 是连续的，所以 $\lim_{k\to\infty} f(x_k) = f(\lim_{k\to\infty} x_k) = f(x^\star)$。这从理论上保证了算法不仅能定位到最小值点，其对应的函数值也会收敛到最小值。[@problem_id:3237403]

### 实际应用中的考量

理论上的完美算法在付诸实践时，总会遇到各种现实世界的挑战。

**寻找初始括号**

[黄金分割搜索](@entry_id:146661)的前提是有一个已知的、包含最小值的有效括号 $[a,b]$。但在很多问题中，我们只有一个出发点 $x_0$。如何从这个点开始，自动地找到一个有效的括号？一种标准的方法是**指数扩展搜索**。基本思想是：
1. 从 $x_0$ 出发，选择一个初始步长 $h > 0$。
2. 确定一个“下山”方向。例如，比较 $f(x_0)$ 和 $f(x_0+h)$。如果 $f(x_0+h)  f(x_0)$，则搜索方向为正方向；否则，可能需要检查负方向。
3. 沿着下山方向，以指数级增大的步长进行探索，生成一系列点 $x_1, x_2, x_3, \dots$。例如，步长可以按 $h, h\phi, h\phi^2, \dots$ 的序列增长。
4. 在每一步都检查函数值。一旦发现 $f(x_{k+1}) > f(x_k)$，说明我们已经“越过”了山谷的最低点。
5. 此时，点组 $(x_{k-1}, x_k, x_{k+1})$ 就构成了一个有效的三点括号，因为我们找到了 $f(x_{k-1}) > f(x_k)  f(x_{k+1})$（近似地）。因此，区间 $[x_{k-1}, x_{k+1}]$ 就是一个可以用于启动[黄金分割搜索](@entry_id:146661)的有效初始括号。[@problem_id:3237430]

**终止条件的设计**

什么时候停止迭代？最直接的判据是区间宽度 $\Delta x_k = b_k - a_k$ 小于某个预设的容差 $\tau_x$。然而，这可能导致过[早停](@entry_id:633908)止。想象一个函数，其最小值附近非常“平坦”，即使 $x$ 值已经很接近 $x^\star$，函数值 $f(x)$ 的变化仍然很小。相反，如果函数在最小值附近非常“陡峭”，即使区间宽度已经很小，函数值仍有很大的下降空间。

一个更稳健的策略是结合两种类型的容差：位置容差 $\tau_x$ 和函数值容差 $\tau_f$。具体来说，我们可以监测区间宽度的变化，同时也要监测函数值的相对变化量，例如 $\Delta f_k = \frac{|f_{k-1}^\star - f_k^\star|}{\max(1, |f_{k-1}^\star|)}$，其中 $f_k^\star$ 是第 $k$ 步的最小函数值。
一个好的[终止准则](@entry_id:136282)是：**当且仅当**区间宽度足够小**并且**函数值的改进也变得微不足道时，才停止迭代。用逻辑语言表达即：
$$
\text{停止条件} \iff (\Delta x_k \le \tau_x) \land (\Delta f_k \le \tau_f)
$$
这种“与”逻辑可以防止在函数仍有显著下降潜力时，仅仅因为区间宽度达标而过早退出。例如，当 $\Delta x_k = 10^{-5}$（已满足 $\tau_x=10^{-4}$）但 $\Delta f_k = 0.12$（远大于 $\tau_f=10^{-3}$）时，该准则会判定继续搜索，这是正确的决策。[@problem_id:3237478]

**有限精度计算的影响**

在真实的计算机上，所有计算都是使用有限精度的[浮点数](@entry_id:173316)完成的。这会对[黄金分割搜索](@entry_id:146661)的最终精度产生根本性的限制。算法的核心操作是计算新的探测点，例如 $x_{k,2} = a_k + \tau(b_k-a_k)$。当区间 $[a_k, b_k]$ 变得非常小时，增量 $\tau(b_k-a_k)$ 可能小到无法被浮点数表示。具体来说，如果这个增量小于 $a_k$ 的“最小可表示单位”（Unit in the Last Place, ULP），那么浮[点加法](@entry_id:177138) $a_k \oplus \tau(b_k-a_k)$ 的结果将仍然是 $a_k$。此时，新的探测点与旧的端点重合，区间无法再被缩小，算法停滞。

这个停滞的极限直接与浮点数的机器精度 $\varepsilon_{\text{mach}}$ 相关。对于一个尺度约为 $1$ 的区间，可达到的最小区间长度 $L_{\min}$ 大约与 $\varepsilon_{\text{mach}}$ 成正比。
- 对于**单精度**浮点数（[IEEE 754](@entry_id:138908) single-precision），$\varepsilon_{\text{single}} \approx 1.19 \times 10^{-7}$。这意味着[黄金分割搜索](@entry_id:146661)能达到的最高精度（最小区间长度）大约就在 $10^{-7}$ 这个量级。
- 对于**双精度**[浮点数](@entry_id:173316)（[IEEE 754](@entry_id:138908) double-precision），$\varepsilon_{\text{double}} \approx 2.22 \times 10^{-16}$。可达到的精度则高得多，大约在 $10^{-16}$ 量级。

因此，如果你设定的目标精度 $\tau_{tol} = 10^{-12}$，使用[双精度](@entry_id:636927)计算完全可以达到；但如果使用单精度，算法会在区间长度缩小到 $10^{-7}$ 左右时就因舍入误差而停滞，永远无法满足 $L_k \le 10^{-12}$ 的终止条件。[@problem_id:3237491]

### 鲁棒性与扩展

**最大化问题**

[黄金分割搜索](@entry_id:146661)是为最小化问题设计的。如果我们需要找一个函数的**最大值**怎么办？一个简单而优雅的转换是：寻找 $f(x)$ 的最大值，等价于寻找 $-f(x)$ 的最小值。如果 $f(x)$ 是一个具有单个峰值的（最大化意义上的）[单峰函数](@entry_id:143107)，那么 $g(x) = -f(x)$ 就是一个具有单个谷底的（最小化意义上的）[单峰函数](@entry_id:143107)。因此，我们无需修改[黄金分割搜索](@entry_id:146661)算法的任何内部逻辑，只需将待优化的目标函数从 $f(x)$ 替换为 $-f(x)$ 即可。[@problem_id:3237524]

**单峰性假设的破坏**

[黄金分割搜索](@entry_id:146661)的正确性严格依赖于函数的单峰性假设。如果这个假设被破坏，哪怕只是在一个点上，算法也可能失败。假设函数 $f(x)$ 在除了一点 $x_v$ 之外的所有地方都满足单峰性。
- 只要算法的探测点 $c_k$ 和 $d_k$ 没有恰好落在 $x_v$ 上，那么 $f(c_k)$ 和 $f(d_k)$ 之间的关系就能正确反映最小值 $x^\star$ 的相对位置，算法会继续正确地缩减区间。
- 但是，一旦某个探测点（比如 $c_k$）恰好等于 $x_v$，那么 $f(x_v)$ 的值可能会“欺骗”算法。例如，即使 $x^\star$ 在 $c_k$ 的右边，但如果 $f(x_v)$ 的值异常地低，导致 $f(c_k)  f(d_k)$，算法就会错误地舍弃包含 $x^\star$ 的右半部分区间 $[d_k, b_k]$。
- 一旦发生这种错误，括号的“[不变性](@entry_id:140168)”（即区间始终包含真解）就被打破了。此后，算法将在一个不包含真正最小值的区间上继续迭代，最终收敛到一个错误的次优解。值得注意的是，算法的收敛**速率**（即区间长度按 $\tau$ 比例缩小）并不会改变，因为它是由算法的几何结构决定的，与函数值无关。改变的是收敛的**目标**。[@problem_id:3237392]

**带噪声的函数求值**

在许多科学和工程应用中，函数值本身可能来自于充满噪声的测量或模拟。也就是说，我们能得到的不是 $f(x)$，而是 $Y(x) = f(x) + \epsilon$，其中 $\epsilon$ 是一个[随机误差](@entry_id:144890)。
- 在这种情况下，单次比较 $Y(c_k)$ 和 $Y(d_k)$ 的结果可能是错误的，尤其是在 $f(c_k)$ 和 $f(d_k)$ 的真实差值很小（即接近最小值时）而噪声很大的情况下。一次错误的比较就可能导致算法收敛失败。
- 一种有效的缓解策略是进行**重复采样和平均**。在每个探测点，我们不只进行一次求值，而是进行 $k$ 次独立的求值，然后用样本均值 $\bar{Y}(c_k)$ 和 $\bar{Y}(d_k)$ 来进行比较。根据大数定律，只要噪声的[方差](@entry_id:200758)有限，随着 $k$ 的增大，样本均值会越来越接近真实的函数值，从而使得做出错误决策的概率大大降低。
- 理论上，如果我们可以根据迭代的进展动态增加样本量 $k_n$，使得错误决策的概率之和是可收敛的（$\sum \mathbb{P}(\text{错误})  \infty$），那么根据[Borel-Cantelli引理](@entry_id:158432)，算法将以概率1只犯有限次错误，并最终收敛到真解 $x^\star$。
- 然而，这种 averaging 策略并非万能。对于一些“病态”的噪声[分布](@entry_id:182848)，例如没有[有限方差](@entry_id:269687)的[柯西分布](@entry_id:266469)，其样本均值的[分布](@entry_id:182848)与单次采样的[分布](@entry_id:182848)完全相同。在这种情况下，增加采样次数并不能减小不确定性，也无法提高决策的可靠性。[@problem_id:3237472]

### 与其他方法的比较：二分法

最后，将[黄金分割搜索](@entry_id:146661)与另一种著名的区间缩减算法——**二分法（Bisection Method）** 进行比较是很有启发性的。需要强调的是，标准的二分法是用于求根（即解 $g(x)=0$）的，而不是用于优化。但是，如果一个函数 $f(x)$ 是可微的，那么它的内部[最小值点](@entry_id:634980) $x^\star$ 必然满足 $f'(x^\star)=0$。因此，我们可以将寻找 $f(x)$ 的最小值问题转化为寻找其导函数 $f'(x)$ 的根的问题，然后对 $f'(x)$ 应用二分法。

**适用性**
- **[黄金分割搜索](@entry_id:146661)**：只需要函数是单峰的。函数值 $f(x)$ 即可，无需导数。这使其[适用范围](@entry_id:636189)非常广，包括那些不可微或导数难以计算的函数。
- **[二分法](@entry_id:140816)** (作用于 $f'$): 需要 $f(x)$ 不仅可微，其导数 $f'(x)$ 还需要是连续的。更关键的是，它要求 $f'(x)$ 在初始区间的两个端点处异号（例如 $f'(a)  0$ 且 $f'(b) > 0$），这样才能保证区间内存在根。这排除了最小值在端点的情况。

**收敛速率**
- **[黄金分割搜索](@entry_id:146661)**：区间长度每次乘以 $\tau \approx 0.618$。
- **[二分法](@entry_id:140816)**：区间长度每次精确地减半，即乘以 $0.5$。
由于 $0.5  0.618$，二分法的收敛速率**更快**。它每一步能排除掉更大比例的区间。

**总结**
- 当函数导数可用且满足符号变化条件时，基于[二分法](@entry_id:140816)的[优化方法](@entry_id:164468)收敛更快。
- 当函数导数不可用、计算成本高昂，或我们对函数的性质知之甚少（只知道其单峰性）时，[黄金分割搜索](@entry_id:146661)是一个更普适、更稳健的选择。[@problem_id:3237542]

通过本章的探讨，我们不仅学习了[黄金分割搜索](@entry_id:146661)的具体操作步骤，更重要的是理解了其背后的数学原理、效率优势以及在面对现实世界复杂性时的行为和局限。这些深刻的理解是成为一名优秀的计算科学家或工程师所必备的。