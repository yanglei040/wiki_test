{"hands_on_practices": [{"introduction": "幂法是求解特征值问题最基础的迭代算法，但其简洁性依赖于严格的收敛条件。本练习 [@problem_id:3282313] 旨在挑战你对这些条件的批判性思考，特别是关于矩阵必须拥有唯一的最大模特征值这一要求。通过分析不同矩阵的谱特性，你将学会在编写代码之前，就能预判算法的收敛行为，从而为更复杂的数值分析奠定坚实的直觉基础。", "problem": "考虑实方阵 $A \\in \\mathbb{R}^{n \\times n}$ 的代数特征值问题 $A \\mathbf{x} = \\lambda \\mathbf{x}$。幂法是一种广泛使用的迭代技术，用于近似求解 $A$ 的主特征对。在实践中，归一化迭代向量能否收敛到单一的特征向量方向，依赖于是否存在唯一的最大绝对值特征值。选择下面所有的矩阵，对于这些矩阵，当使用一个通用的实初始向量（该向量在与最大模特征值相关联的每个不变子空间中都有非零分量）时，归一化幂迭代会因为最大绝对值的特征值不唯一而无法收敛到单一的特征向量方向。\n\nA. $A = \\mathrm{diag}(3,-3,1)$\n\nB. $A = \\begin{bmatrix} 2  1  0 \\\\ 1  2  0 \\\\ 0  0  0.5 \\end{bmatrix}$\n\nC. $A = \\mathrm{diag}(2,2,0.2)$\n\nD. $A = \\begin{bmatrix} 0  -3 \\\\ 3  0 \\end{bmatrix}$", "solution": "用户想要从给定列表中找出哪些矩阵会导致归一化幂法无法收敛到单一的特征向量方向，其具体原因在于不存在唯一的最大绝对值特征值。\n\n幂法是一种迭代算法，用于寻找矩阵 $A$ 的主特征值及其对应的特征向量。向量序列由 $\\mathbf{v}_{k+1} = A \\mathbf{v}_k$ 生成，从一个初始向量 $\\mathbf{v}_0$ 开始。相应的归一化向量序列为 $\\mathbf{q}_{k+1} = \\frac{A \\mathbf{q}_k}{\\|A \\mathbf{q}_k\\|}$。\n\n幂法的收敛性取决于矩阵 $A$ 的特征值结构。设矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的特征值为 $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$，按其绝对值排序为：$|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_n|$。\n\n幂法的标准收敛定理指出，如果存在唯一的主特征值（即 $|\\lambda_1|  |\\lambda_2|$），并且初始向量 $\\mathbf{v}_0$ 在对应特征向量 $\\mathbf{x}_1$ 方向上的分量不为零，那么归一化迭代序列 $\\{\\mathbf{q}_k\\}$ 会收敛到由 $\\mathbf{x}_1$ 张成的一维特征空间中的一个向量。\n\n问题要求找出迭代“无法收敛到单一特征向量方向”的情况，其原因是最大绝对值的特征值不唯一。这意味着我们正在寻找那些谱半径 $\\rho(A) = |\\lambda_1|$ 由多个不同特征值达到的矩阵。也就是说，存在至少两个不同的特征值 $\\lambda_i$ 和 $\\lambda_j$，使得 $|\\lambda_i| = |\\lambda_j| = |\\lambda_1|$。\n\n问题指定了一个“通用的实初始向量（其在与最大模特征值相关联的每个不变子空间中都有非零分量）”。这确保了初始向量不是特殊的（例如，与某个主特征向量方向正交），因此迭代的行为将反映所有主特征空间的影响。无法收敛意味着向量方向序列 $\\{\\mathbf{q}_k\\}$ 不趋近于一个极限。\n\n我们现在来分析每个矩阵。\n\n**A. $A = \\mathrm{diag}(3,-3,1)$**\n\n对角矩阵的特征值是其对角线上的元素。$A$ 的特征值为 $\\lambda_1 = 3$，$\\lambda_2 = -3$ 和 $\\lambda_3 = 1$。\n特征值的绝对值为 $|\\lambda_1| = 3$， $|\\lambda_2| = 3$ 和 $|\\lambda_3| = 1$。\n谱半径为 $\\rho(A) = 3$。有两个不同的特征值 $\\lambda_1 = 3$ 和 $\\lambda_2 = -3$，它们的绝对值等于谱半径。因此，不满足唯一最大绝对值特征值的条件。\n\n让我们来考察幂迭代的行为。特征向量是标准基向量：$\\mathbf{x}_1 = [1, 0, 0]^T$，$\\mathbf{x}_2 = [0, 1, 0]^T$ 和 $\\mathbf{x}_3 = [0, 0, 1]^T$。一个通用的初始向量 $\\mathbf{v}_0$ 可以写成 $\\mathbf{v}_0 = c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + c_3 \\mathbf{x}_3$，根据通用起始向量的假设，有 $c_1 \\neq 0$ 和 $c_2 \\neq 0$。\n\n第 $k$ 次迭代为：\n$$ \\mathbf{v}_k = A^k \\mathbf{v}_0 = c_1 \\lambda_1^k \\mathbf{x}_1 + c_2 \\lambda_2^k \\mathbf{x}_2 + c_3 \\lambda_3^k \\mathbf{x}_3 = c_1 (3)^k \\mathbf{x}_1 + c_2 (-3)^k \\mathbf{x}_2 + c_3 (1)^k \\mathbf{x}_3 $$\n$$ \\mathbf{v}_k = 3^k \\left( c_1 \\mathbf{x}_1 + c_2 (-1)^k \\mathbf{x}_2 + c_3 \\left(\\frac{1}{3}\\right)^k \\mathbf{x}_3 \\right) $$\n对于大的 $k$，含有 $(1/3)^k$ 的项变得可以忽略不计。向量 $\\mathbf{v}_k$ 的方向由向量 $c_1 \\mathbf{x}_1 + c_2 (-1)^k \\mathbf{x}_2$ 决定。\n- 如果 $k$ 是偶数，方向平行于 $c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2$。\n- 如果 $k$ 是奇数，方向平行于 $c_1 \\mathbf{x}_1 - c_2 \\mathbf{x}_2$。\n由于 $c_1, c_2 \\neq 0$，这是两个不同的方向。归一化迭代序列在这两个方向之间振荡，不收敛。这符合问题中描述的失败条件。\n\n对 A 的判断：**正确**。\n\n**B. $A = \\begin{bmatrix} 2  1  0 \\\\ 1  2  0 \\\\ 0  0  0.5 \\end{bmatrix}$**\n\n这是一个块对角矩阵。左上角 $2 \\times 2$ 块 $\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$ 的特征值由特征方程 $\\det \\begin{pmatrix} 2-\\lambda  1 \\\\ 1  2-\\lambda \\end{pmatrix} = (2-\\lambda)^2 - 1 = 0$ 给出。这得到 $2-\\lambda = \\pm 1$，所以特征值是 $\\lambda = 3$ 和 $\\lambda = 1$。第三个特征值来自右下角的 $1 \\times 1$ 块，即 $\\lambda = 0.5$。\n$A$ 的特征值为 $\\lambda_1 = 3$，$\\lambda_2 = 1$ 和 $\\lambda_3 = 0.5$。\n绝对值为 $|\\lambda_1| = 3$， $|\\lambda_2| = 1$ 和 $|\\lambda_3| = 0.5$。\n谱半径为 $\\rho(A) = 3$。存在一个唯一的特征值 $\\lambda_1=3$ 具有这个最大的绝对值（$|\\lambda_1|  |\\lambda_2|$）。\n因此，幂法将收敛到对应于 $\\lambda_1 = 3$ 的特征向量。该矩阵不满足指定的失败条件。\n\n对 B 的判断：**不正确**。\n\n**C. $A = \\mathrm{diag}(2,2,0.2)$**\n\n特征值为 $\\lambda_1 = 2$，$\\lambda_2 = 2$ 和 $\\lambda_3 = 0.2$。\n特征值的绝对值为 $|\\lambda_1| = 2$， $|\\lambda_2| = 2$ 和 $|\\lambda_3| = 0.2$。\n谱半径为 $\\rho(A) = 2$。问题询问的是*最大绝对值的特征值*是否不唯一。在这里，这个值是 $2$。没有其他不同的特征值具有这个模。然而，这个主特征值的代数重数为 $2$。让我们分析这对收敛性的影响。\n\n与主特征值 $\\lambda=2$ 相关联的特征空间是 $(A - 2I)$ 的零空间。\n$$ A - 2I = \\mathrm{diag}(2-2, 2-2, 0.2-2) = \\mathrm{diag}(0, 0, -1.8) $$\n零空间由向量 $\\mathbf{x}_1 = [1, 0, 0]^T$ 和 $\\mathbf{x}_2 = [0, 1, 0]^T$ 张成。这是一个二维特征空间。\n\n设初始向量为 $\\mathbf{v}_0 = c_1 \\mathbf{x}_1 + c_2 \\mathbfx_2 + c_3 \\mathbf{x}_3$，其中 $\\mathbf{x}_3 = [0, 0, 1]^T$ 是对应 $\\lambda_3=0.2$ 的特征向量。通用假设意味着 $\\mathbf{v}_0$ 在主特征空间上的投影 $c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2$ 是非零的。\n第 $k$ 次迭代为：\n$$ \\mathbf{v}_k = A^k \\mathbf{v}_0 = c_1 (2)^k \\mathbf{x}_1 + c_2 (2)^k \\mathbf{x}_2 + c_3 (0.2)^k \\mathbf{x}_3 = 2^k \\left( c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2 + c_3 (0.1)^k \\mathbf{x}_3 \\right) $$\n对于大的 $k$，含有 $(0.1)^k$ 的项消失。向量 $\\mathbf{v}_k$ 变得与 $c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2$ 成比例。归一化该向量得到\n$$ \\mathbf{q}_k \\to \\frac{c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2}{\\|c_1 \\mathbf{x}_1 + c_2 \\mathbf{x}_2\\|} \\quad \\text{当 } k \\to \\infty $$\n对于一个*给定*的初始向量 $\\mathbf{v}_0$，归一化迭代序列 $\\{\\mathbf{q}_k\\}$ 会收敛到一个单一的极限向量。这个极限向量是主特征空间中的一个特征向量。序列本身*并没有不收敛*。问题要求的是迭代“无法收敛”的情况。由于序列确实收敛，所以这个矩阵不符合失败的描述。\n\n对 C 的判断：**不正确**。\n\n**D. $A = \\begin{bmatrix} 0  -3 \\\\ 3  0 \\end{bmatrix}$**\n\n特征方程为 $\\det(A - \\lambda I) = \\det \\begin{pmatrix} -\\lambda  -3 \\\\ 3  -\\lambda \\end{pmatrix} = \\lambda^2 - (-3)(3) = \\lambda^2 + 9 = 0$。\n特征值为 $\\lambda^2 = -9$，即 $\\lambda_1 = 3i$ 和 $\\lambda_2 = -3i$。\n绝对值为 $|\\lambda_1| = |3i| = 3$ 和 $|\\lambda_2| = |-3i| = 3$。\n谱半径为 $\\rho(A) = 3$。有两个不同的特征值，是一对共轭复数，它们的绝对值等于谱半径。不满足唯一最大绝对值特征值的条件。\n\n让我们来考察使用实初始向量 $\\mathbf{v}_0$ 的迭代行为。矩阵 $A$可以写成\n$$ A = 3 \\begin{bmatrix} 0  -1 \\\\ 1  0 \\end{bmatrix} = 3 \\begin{bmatrix} \\cos(\\pi/2)  -\\sin(\\pi/2) \\\\ \\sin(\\pi/2)  \\cos(\\pi/2) \\end{bmatrix} = 3 R(\\pi/2) $$\n其中 $R(\\theta)$ 是旋转角度为 $\\theta$ 的旋转矩阵。\n第 $k$ 次迭代为 $\\mathbf{v}_k = A^k \\mathbf{v}_0 = (3 R(\\pi/2))^k \\mathbf{v}_0 = 3^k (R(\\pi/2))^k \\mathbf{v}_0$。\n根据旋转矩阵的性质，$(R(\\theta))^k = R(k\\theta)$。所以，\n$$ \\mathbf{v}_k = 3^k R(k\\pi/2) \\mathbf{v}_0 $$\n向量 $\\mathbf{v}_k$是通过将 $\\mathbf{v}_0$ 缩放 $3^k$ 倍并将其旋转 $k\\pi/2$ 的角度得到的。\n对于 $k=0, 1, 2, 3, \\dots$，向量 $\\mathbf{v}_k$ 的方向在四个不同的正交方向上循环（旋转角度为 $0, \\pi/2, \\pi, 3\\pi/2, 2\\pi, \\dots$）。\n归一化向量方向的序列不收敛；它无限期地旋转。这构成了无法收敛到单一方向的情况。\n\n对 D 的判断：**正确**。", "answer": "$$\\boxed{AD}$$", "id": "3282313"}, {"introduction": "理论上的收敛并不能完全等同于实践中的成功，尤其是在有限精度计算中。这个编码练习 [@problem_id:3282259] 将引导你探索一个棘手的“近似亏损”矩阵，虽然它满足幂法收敛的理论条件，但收敛速度极其缓慢，以至于在有限次迭代后会产生误导性结果。通过亲手实现该算法，你将深刻体会到病态问题和数值稳定性在科学计算中的重要性。", "problem": "您将研究幂法在代数特征值问题中，在一个精心构造的近亏损矩阵上的行为。幂法通过对一个向量重复应用一个矩阵并进行归一化，来尝试近似一个主特征对。本研究的基础包括特征值和特征向量的定义、可对角化性的概念以及幂迭代的定义。具体来说，如果矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 有一个模最大的特征值（谱半径），并且初始向量在对应的右特征向量方向上有一个非零分量，那么幂法会趋向于与该特征向量对齐；否则，它将无法做到。\n\n按如下方式构造一个 $2 \\times 2$ 的近亏损矩阵。令 $S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$，其中 $\\varepsilon = 10^{-8}$，并令 $J = \\operatorname{diag}(\\lambda_1,\\lambda_2)$，其中 $\\lambda_1 = 1 + \\delta$ 且 $\\lambda_2 = 1$，$\\delta = 10^{-4}$。定义\n$$\nA \\;=\\; S J S^{-1}.\n$$\n根据构造，$A$ 是可对角化的，其右特征向量由 $S$ 的列给出，即与 $\\lambda_1$ 关联的 $s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 和与 $\\lambda_2$ 关联的 $s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$。因为 $\\varepsilon$ 非常小，这两个特征向量近乎平行，特征基是病态的，这使得 $A$ 成为近亏损矩阵。左特征向量（$S^{-1}$ 的行）决定了初始向量在右特征向量基中表示时的分解系数。\n\n实现由\n$$\ny_{k+1} \\;=\\; A x_k,\\quad x_{k+1} \\;=\\; \\frac{y_{k+1}}{\\|y_{k+1}\\|_2},\n$$\n定义的幂法迭代，从给定的初始向量 $x_0 \\in \\mathbb{R}^2 \\setminus \\{0\\}$ 开始。在指定的迭代次数 $N$ 之后，报告瑞利商\n$$\n\\rho(x) \\;=\\; \\frac{x^\\top A x}{x^\\top x},\n$$\n作为与最终方向 $x$ 相关联的特征值的估计值。通过比较绝对差 $|\\rho(x) - \\lambda_1|$ 和 $|\\rho(x) - \\lambda_2|$，使用此估计值来判断迭代在数值上是与主特征值 $\\lambda_1$ 对齐还是与次主特征值 $\\lambda_2$ 对齐。\n\n您的程序必须：\n- 根据指定的 $S$（其中 $\\varepsilon = 10^{-8}$）和 $J$（其中 $\\lambda_1 = 1 + 10^{-4}$ 且 $\\lambda_2 = 1$）构造 $A$。\n- 精确执行 $N = 1000$ 次幂法迭代（不要使用自适应停止准则）。\n- 对于下面测试套件中的每个初始向量，计算最终的迭代向量 $x_N$（在每一步都进行归一化）及其瑞利商 $\\rho(x_N)$，然后输出一个布尔值：如果最终的瑞利商更接近 $\\lambda_1$ 而不是 $\\lambda_2$，则为 true，否则为 false。\n\n初始向量测试套件：\n- 情况 $1$（精确的次主特征向量）：$x_0^{(1)} = s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix}$。\n- 情况 $2$（极度接近次主特征向量）：$x_0^{(2)} = s_2 + 10^{-30} s_1$。\n- 情况 $3$（精确的主特征向量）：$x_0^{(3)} = s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$。\n- 情况 $4$（与左主特征向量近乎正交）：$x_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$。\n\n基于第一性原理的预期定性行为：\n- 在情况 $1$ 中，因为 $x_0$ 正是 $\\lambda_2$ 的一个特征向量，所有的迭代向量都将保持在 $s_2$ 的生成空间内，因此幂法将产生一个（在舍入误差范围内）等于 $\\lambda_2$ 的瑞利商，即相对于主特征值而言是错误的特征值。\n- 在情况 $2$ 中，$s_1$ 的系数小到天文数字级别，并且由于 $|\\lambda_1/\\lambda_2|$ 仅略大于 $1$，经过 $1000$ 次迭代后，主分量仍然可以忽略不计；该方法看起来收敛到了错误的特征向量。\n- 在情况 $3$ 中，该方法立即产生主特征对。\n- 在情况 $4$ 中，$s_1$ 的系数与 $s_2$ 的系数相比非常小，因此在 $1000$ 次迭代内，该方法在数值上仍将与次主特征向量对齐。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含上述四种情况的四个布尔结果，格式为方括号内以逗号分隔的列表，例如，“[True,False,True,False]”。\n- 不涉及物理单位或角度；所有数值均为无量纲实数。\n\n该问题要求您从特征值和特征向量、对角化、左右特征向量以及幂法结构的定义出发进行推理。除了这些定义之外，您必须避免使用任何捷径。通过使用指定的参数确保科学真实性，并按照上述说明提供一个鲁棒的实现。", "solution": "用户提出了一个来自数值线性代数领域的问题，具体涉及幂法在近亏损矩阵上的行为。该问题要求实现幂法，并为一组精心选择的初始向量分析其收斂性。\n\n### 问题验证\n问题陈述已经过验证，并被认为是合理的。\n- **已知条件**：所有参数（$\\varepsilon = 10^{-8}$, $\\delta = 10^{-4}$）、矩阵（$S$、$J$ 以及由此产生的 $A$）、算法规范（$N=1000$ 次迭代的幂法和 $L_2$ 归一化）、初始条件（四个测试向量 $x_0$）和输出要求（将最终瑞利商与已知特征值进行布尔比较）都已明确给出。\n- **科学依据**：该问题基于代数特征值问题的基本和标准原理，包括矩阵对角化、左右特征向量和幂迭代法。构造近亏损矩阵以研究数值稳定性和收敛性是数值分析中的经典教学工具。\n- **良定性与客观性**：该问题是无歧义、自洽且客观的。它指定了一个确定性的计算任务，对每种情况都会产生唯一、可验证的结果。\n\n该问题是有效的，将提供解决方案。\n\n### 基于原理的解决方案\n核心任务是分析幂法在特征基病态的矩阵 $A$ 上的性能。幂法是一种迭代算法，旨在寻找矩阵的主特征值及对应的特征向量。其行为由初始向量在矩阵特征向量基中的分解决定。\n\n**1. 矩阵构造与性质**\n\n矩阵 $A$ 被构造为 $A = S J S^{-1}$，其中：\n$S = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix}$，其中 $\\varepsilon = 10^{-8}$。\n$J = \\operatorname{diag}(\\lambda_1, \\lambda_2)$，其中 $\\lambda_1 = 1 + \\delta = 1 + 10^{-4}$ 且 $\\lambda_2 = 1$。\n\n$S$ 的列是 $A$ 的右特征向量：\n- $s_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ 对应于主特征值 $\\lambda_1 = 1.0001$。\n- $s_2 = \\begin{bmatrix} 1 \\\\ \\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 10^{-8} \\end{bmatrix}$ 对应于次主特征值 $\\lambda_2 = 1$。\n\n由于 $\\varepsilon$ 非常小，特征向量 $s_1$ 和 $s_2$ 近乎平行，使得特征向量基是病态的。这意味着 $A$ 是一个近亏损矩阵。\n\n$S$ 的逆计算为 $S^{-1} = \\frac{1}{\\det(S)} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\frac{1}{\\varepsilon} \\begin{bmatrix} \\varepsilon  -1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}$。$S^{-1}$ 的行是 $A$ 的左特征向量。\n\n矩阵 $A$ 可以显式计算：\n$$\nA = S J S^{-1} = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix}\n$$\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 0  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -(1+\\delta)/\\varepsilon + 1/\\varepsilon \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1+\\delta  -\\delta/\\varepsilon \\\\ 0  1 \\end{bmatrix}\n$$\n所以，$A = \\begin{bmatrix} 1.0001  -10^{-4}/10^{-8} \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1.0001  -10000 \\\\ 0  1 \\end{bmatrix}$。\n\n**2. 幂法动力学**\n\n幂法迭代为 $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$。从 $x_0$ 开始进行 $N$ 次迭代后，得到的向量与 $A^N x_0$ 成比例。设初始向量 $x_0$ 在特征向量基中分解为 $x_0 = c_1 s_1 + c_2 s_2$。则：\n$$\nA^N x_0 = A^N (c_1 s_1 + c_2 s_2) = c_1 \\lambda_1^N s_1 + c_2 \\lambda_2^N s_2\n$$\n向量 $A^N x_0$ 可以重写为：\n$$\nA^N x_0 = \\lambda_2^N \\left( c_1 \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N s_1 + c_2 s_2 \\right)\n$$\n迭代向量 $x_N$ 的方向取决于括号中两项的相对大小。要收敛到主特征向量 $s_1$，需要第一项主导第二项。这种情况发生于放大因子 $(\\lambda_1/\\lambda_2)^N$ 足够大，能够克服可能很小的初始系数比 $|c_1/c_2|$。\n\n在本问题中，放大因子为 $(\\frac{1+\\delta}{1})^N = (1+10^{-4})^{1000}$。使用近似 $(1+x)^n \\approx e^{nx}$（对于小 $x$），我们有 $(1+10^{-4})^{1000} \\approx e^{1000 \\cdot 10^{-4}} = e^{0.1} \\approx 1.10517$。这个相对较小的放大因子意味着幂法收敛会非常缓慢。\n\n**3. 测试用例分析**\n\n如果计算出的瑞利商 $\\rho(x_N) = x_N^\\top A x_N$ 更接近 $\\lambda_1$ 而不是 $\\lambda_2$，则每个用例的最终输出为 `True`。这等价于检查是否 $\\rho(x_N)  (\\lambda_1 + \\lambda_2)/2 = 1 + \\delta/2$。\n\n- **情况 1**：$x_0^{(1)} = s_2$。\n初始向量恰好是次主特征向量。这里，$c_1=0$ 且 $c_2=1$。幂法迭代将始终保持在 $\\lambda_2$ 的特征空间内：对于所有 $k0$，都有 $x_k = s_2/\\|s_2\\|_2$。因此，$\\rho(x_N)$ 将等于 $\\lambda_2 = 1$（在浮点误差范围内）。这比 $\\lambda_1$ 更接近 $\\lambda_2$。结果为 **False**。\n\n- **情况 2**：$x_0^{(2)} = s_2 + 10^{-30} s_1$。\n初始向量的系数为 $c_1 = 10^{-30}$ 和 $c_2 = 1$。经过 $N=1000$ 次迭代后，分量大小之比为：\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N s_1\\|}{\\|c_2 s_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N \\approx 10^{-30} \\times 1.105 \\approx 1.105 \\times 10^{-30}\n$$\n这个比例极小。向量 $x_{1000}$ 在数值上将与 $s_2$ 无法区分。瑞利商将极其接近 $\\lambda_2$。结果为 **False**。\n\n- **情况 3**：$x_0^{(3)} = s_1$。\n初始向量是主特征向量，所以 $c_1=1$ 且 $c_2=0$。迭代立即产生主特征向量：对于所有 $k$，都有 $x_k = s_1/\\|s_1\\|_2$。瑞利商 $\\rho(x_N)$ 将等于 $\\lambda_1 = 1.0001$。这更接近 $\\lambda_1$。结果为 **True**。\n\n- **情况 4**：$x_0^{(4)} = \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix}$。\n为求系数 $c_1, c_2$，我们求解 $x_0^{(4)} = c_1 s_1 + c_2 s_2$。这等价于 $\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = S^{-1} x_0^{(4)}$。\n$$\n\\begin{pmatrix} c_1 \\\\ c_2 \\end{pmatrix} = \\begin{bmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{bmatrix} \\begin{bmatrix} \\varepsilon^{-1} + 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} (\\varepsilon^{-1} + 1) - 1/\\varepsilon \\\\ 1/\\varepsilon \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1/\\varepsilon \\end{bmatrix}\n$$\n所以，$c_1 = 1$ 且 $c_2 = 1/\\varepsilon = 10^8$。初始向量沿 $s_2$ 的分量比沿 $s_1$ 的分量大 $10^8$ 倍。经过 $N=1000$ 次迭代后，分量大小之比为：\n$$\n\\frac{\\|c_1 (\\lambda_1/\\lambda_2)^N s_1\\|}{\\|c_2 s_2\\|} \\approx \\frac{|c_1|}{|c_2|} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^N = \\varepsilon \\times (1+\\delta)^{1000} \\approx 10^{-8} \\times 1.105 \\approx 1.105 \\times 10^{-8}\n$$\n这个比例非常小。尽管在主特征向量方向上存在非零分量，但由于收敛缓慢和极端的初始不平衡，迭代向量 $x_{1000}$ 仍然被次主特征向量 $s_2$ 压倒性地主导。$\\rho(x_N)$ 将非常接近 $\\lambda_2$。结果为 **False**。\n\n因此，预期的输出是 `[False, False, True, False]`。这表明，对于特征值彼此接近的近亏损矩阵，如果初始向量没有与主特征向量充分对齐，幂法可能会表現出极其缓慢的收敛速度，看似收敛到次主特征向量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the power method problem for a nearly-defective matrix.\n    \"\"\"\n    # 1. Define constants and parameters from the problem statement.\n    epsilon = 1e-8\n    delta = 1e-4\n    lambda1 = 1.0 + delta\n    lambda2 = 1.0\n    N = 1000\n\n    # 2. Construct the matrix A.\n    # A = S J S^-1, where S = [[1, 1], [0, epsilon]] and J = diag(lambda1, lambda2).\n    # This simplifies to A = [[1+delta, -delta/epsilon], [0, 1]].\n    A = np.array([\n        [1.0 + delta, -delta / epsilon],\n        [0.0, 1.0]\n    ], dtype=np.float64)\n\n    # 3. Define the initial vectors for the test suite.\n    # Right eigenvectors s1 and s2\n    s1 = np.array([1.0, 0.0], dtype=np.float64)\n    s2 = np.array([1.0, epsilon], dtype=np.float64)\n\n    # Case 1: Exact subdominant eigenvector\n    x0_1 = s2\n    \n    # Case 2: Extremely close to subdominant eigenvector\n    x0_2 = s2 + 1e-30 * s1\n    \n    # Case 3: Exact dominant eigenvector\n    x0_3 = s1\n    \n    # Case 4: Nearly orthogonal to the left dominant eigenvector\n    x0_4 = np.array([1.0/epsilon + 1.0, 1.0], dtype=np.float64)\n\n    test_cases = [x0_1, x0_2, x0_3, x0_4]\n    \n    results = []\n\n    # 4. Iterate through each test case.\n    for x0 in test_cases:\n        # Check for zero vector, though not expected in these cases.\n        if np.linalg.norm(x0) == 0:\n            # Handle this unlikely edge case.\n            # A zero vector will remain zero, rho is undefined.\n            # The problem assumes x0 is non-zero.\n            # Let's assume it doesn't happen.\n            pass\n\n        x = x0.copy()\n\n        # 5. Implement the power method for N iterations.\n        # The normalization is part of the loop.\n        for _ in range(N):\n            y = A @ x\n            norm_y = np.linalg.norm(y)\n            if norm_y == 0: # Should not happen with this non-singular matrix A\n                x = np.zeros_like(x)\n                break\n            x = y / norm_y\n        \n        x_N = x\n\n        # 6. Compute the Rayleigh quotient for the final iterate x_N.\n        # rho(x) = (x.T * A * x) / (x.T * x)\n        # Since x_N is normalized, its L2 norm is 1, so x_N.T @ x_N = 1.\n        # rho(x_N) = x_N.T @ (A @ x_N)\n        rho = x_N.T @ A @ x_N\n        \n        # 7. Compare absolute differences to determine which eigenvalue is closer.\n        is_closer_to_lambda1 = abs(rho - lambda1)  abs(rho - lambda2)\n        results.append(is_closer_to_lambda1)\n\n    # 8. Format and print the final output as specified.\n    # str(bool) gives 'True' or 'False' with capital letters, as in the example.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3282259"}, {"introduction": "为了克服幂法的局限性，特别是在处理大型矩阵时，我们需要更强大的工具。本练习 [@problem_id:3282272] 介绍了 Arnoldi 迭代法，这是一种现代数值线性代数的核心算法，它通过构建 Krylov 子空间来同时逼近多个特征值。实现 Arnoldi 迭代不仅能让你掌握一种更高效的特征值计算方法，更能让你深入理解投影法，这一构成众多现代大规模特征值求解器基石的核心思想。", "problem": "考虑代数特征值问题：对于一个方阵 $A \\in \\mathbb{R}^{n \\times n}$，寻找标量 $\\lambda \\in \\mathbb{C}$ 和非零向量 $x \\in \\mathbb{C}^n$，使得 $A x = \\lambda x$。一种基于投影的近似特征值方法是通过构建一个维度为 $m$ 的 Krylov 子空间 $K_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}$ 来进行的，其中 $b \\in \\mathbb{R}^n$ 是一个非零的初始向量。使用 Arnoldi 迭代，可以为 $K_m(A,b)$ 构建一个标准正交基，然后形成一个小的上 Hessenberg 矩阵，该矩阵表示算子 $A$ 在此子空间上的作用。这个小矩阵的特征值被称为 Ritz 特征值，它们提供了对 $A$ 的真实特征值的近似。\n\n任务：从上述定义出发，实现 Arnoldi 迭代以构建 $K_m(A,b)$ 的标准正交基和相关的上 Hessenberg 矩阵，然后计算 Ritz 特征值作为 $A$ 的特征值的近似值。对于下面的每个测试用例，通过计算以下量来衡量近似的质量：对于 Ritz 特征值集合 $\\{r_i\\}$ 和 $A$ 的真实特征值集合 $\\{\\lambda_j\\}$，返回单个浮点数\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|.\n$$\n这个量是在复平面中，对所有 Ritz 特征值计算其与最近的真实特征值的绝对差，然后取这些差值的最大值。报告每个测试用例的这个值。\n\n使用以下参数值的测试套件来实现程序。每个测试用例都是一个三元组 $(A, b, m)$：\n\n- 测试用例1（一般非对称、可对角化、“理想情况”）：$A$ 由 $A = S D S^{-1}$ 形成，其中\n$$\nS = \\begin{bmatrix}\n1  0.2  -0.1  0  0  0 \\\\\n0  1  0.3  0  0  0 \\\\\n0  0  1  0.4  0  0 \\\\\n0  0  0  1  0.5  0 \\\\\n0  0  0  0  1  0.6 \\\\\n0  0  0  0  0  1\n\\end{bmatrix},\\quad\nD = \\operatorname{diag}(5,4,3,2,1,-1),\n$$\n$b = [1,1,1,1,1,1]^T$，以及 $m = 4$。\n\n- 测试用例2（边界条件 $m = 1$）：$A$ 是一个 $5 \\times 5$ 的三对角矩阵，其主对角线上为 $2$，次对角线和超对角线上为 $-1$，\n$$\nA = \\begin{bmatrix}\n2  -1  0  0  0 \\\\\n-1  2  -1  0  0 \\\\\n0  -1  2  -1  0 \\\\\n0  0  -1  2  -1 \\\\\n0  0  0  -1  2\n\\end{bmatrix},\n$$\n$b = [1,0,0,0,0]^T$，以及 $m = 1$。\n\n- 测试用例3（亏损矩阵边缘情况）：$A$ 是一个大小为 $6 \\times 6$、特征值为 $3$ 的 Jordan 块，即 $A = 3 I + J$，其中 $J$ 的超对角线上为 $1$，其他位置为 $0$，\n$$\nA = \\begin{bmatrix}\n3  1  0  0  0  0 \\\\\n0  3  1  0  0  0 \\\\\n0  0  3  1  0  0 \\\\\n0  0  0  3  1  0 \\\\\n0  0  0  0  3  1 \\\\\n0  0  0  0  0  3\n\\end{bmatrix},\n$$\n$b = [1,1,1,1,1,1]^T$，以及 $m = 3$。\n\n- 测试用例4（满维 $m = n$，对称情况）：$A$ 是一个对称的 $4 \\times 4$ 矩阵，\n$$\nA = \\begin{bmatrix}\n4  1  0  0 \\\\\n1  3  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  1\n\\end{bmatrix},\n$$\n$b = [1,1,1,1]^T$，以及 $m = 4$。\n\n您的程序必须为每个测试用例计算量 $\\max_i \\min_j |r_i - \\lambda_j|$，并生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个值都必须是四舍五入到8位小数的浮点数，不含任何附加文本。例如，一个输出行的格式为 $[v_1,v_2,v_3,v_4]$，其中每个 $v_k$ 是为测试用例 k 计算并四舍五入到8位小数的浮点数。不涉及物理单位或角度，所有计算都是纯数值的。", "solution": "用户提供了一个来自数值线性代数领域的有效问题陈述。任务是实现用于近似矩阵特征值的 Arnoldi 迭代，并针对几个测试用例评估这些近似的质量。\n\n### Arnoldi 迭代简介\n\n代数特征值问题旨在为给定的方阵 $A \\in \\mathbb{R}^{n \\times n}$ 找到标量 $\\lambda$ 和非零向量 $x$，使得 $A x = \\lambda x$。对于大型矩阵，直接计算所有特征值的成本可能高得令人望而却步。投影方法通过用一个小得多的矩阵的特征值来近似 $A$ 的特征值，提供了一种高效的替代方案。\n\nArnoldi 迭代是一种在 Krylov 子空间上操作的投影方法。对于一个矩阵 $A$ 和一个初始向量 $b$，第 $m$ 个 Krylov 子空间定义为：\n$$\nK_m(A,b) = \\operatorname{span}\\{b, A b, A^2 b, \\dots, A^{m-1} b\\}\n$$\n这个子空间的维数最多为 $m$。其核心思想是在此子空间内找到对 $A$ 的特征值的最佳近似。这是通过为 $K_m(A,b)$ 构建一个标准正交基，然后表示 $A$ 相对于该基的作用来实现的。\n\n### 算法\n\nArnoldi 迭代采用改进的 Gram-Schmidt 过程来为 Krylov 子空间 $K_m(A,b)$ 构建一个标准正交基 $\\{q_1, q_2, \\dots, q_m\\}$。这些基向量作为矩阵 $Q_m = [q_1, q_2, \\dots, q_m] \\in \\mathbb{R}^{n \\times m}$ 的列进行存储。\n\n该算法按以下步骤进行：\n\n1.  **初始化**：归一化初始向量 $b$ 以获得第一个基向量。假设 $b \\neq 0$：\n    $$\n    q_1 = \\frac{b}{\\|b\\|_2}\n    $$\n2.  **迭代**：对于 $k=1, 2, \\dots, m$：\n    a. 通过将 $A$ 应用于最新的基向量来生成一个新向量：\n       $$\n       v = A q_k\n       $$\n    b. 将 $v$ 与所有先前的基向量 $q_1, \\dots, q_k$ 正交化。此投影的系数构成了 Hessenberg 矩阵 $H_m$ 的第 $k$ 列：\n       $$\n       \\text{for } j=1, \\dots, k: \\quad h_{j,k} = q_j^T v \\quad \\text{and} \\quad v \\leftarrow v - h_{j,k} q_j\n       $$\n    c. 所得向量 $v$ 的范数成为 Hessenberg 矩阵的次对角线元素：\n       $$\n       h_{k+1, k} = \\|v\\|_2\n       $$\n    d. 如果 $h_{k+1, k}$ 为零（或数值上接近于零），则算法找到了一个 $A$-不变子空间并终止。这被称为“崩溃 (breakdown)”。\n    e. 归一化 $v$ 以获得下一个基向量：\n       $$\n       q_{k+1} = \\frac{v}{h_{k+1, k}}\n       $$\n\n经过 $m$ 步（假设没有发生崩溃），此过程产生 Arnoldi 分解：\n$$\nA Q_m = Q_{m+1} \\tilde{H}_m\n$$\n其中 $Q_m \\in \\mathbb{R}^{n \\times m}$ 和 $Q_{m+1} \\in \\mathbb{R}^{n \\times (m+1)}$ 是列向量标准正交的矩阵，$\\tilde{H}_m \\in \\mathbb{R}^{(m+1) \\times m}$ 是一个上 Hessenberg 矩阵。矩阵 $H_m \\in \\mathbb{R}^{m \\times m}$ 由 $\\tilde{H}_m$ 的前 $m$ 行构成。这个 $H_m$ 是 $A$ 投影到 Krylov 子空间上的表示，即 $H_m = Q_m^T A Q_m$。\n\n### Ritz 特征值与误差度量\n\n这个小的 $m \\times m$ Hessenberg 矩阵 $H_m$ 的特征值被称为 **Ritz 特征值**。这些值，记为 $\\{r_i\\}_{i=1}^m$，可作为 $A$ 的真实特征值（我们记为 $\\{\\lambda_j\\}_{j=1}^n$）的近似值。\n\n为了衡量这种近似的质量，我们计算 Ritz 特征值集合与真实特征值集合之间的豪斯多夫距离 (Hausdorff distance)。具体的度量标准是：\n$$\n\\max_i \\min_j \\left| r_i - \\lambda_j \\right|\n$$\n该公式为每个 Ritz 特征值 $r_i$ 计算其到最近的真实特征值 $\\lambda_j$ 的绝对距离。最终报告的值是这些最小距离中的最大值。\n\n### 实现计划\n\n解决方案将实现为一个 Python 程序，对每个测试用例 $(A, b, m)$ 遵循以下步骤：\n\n1.  **构建** 指定的矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和初始向量 $b \\in \\mathbb{R}^n$。\n2.  **计算真实特征值**：使用标准数值库函数计算 $A$ 的全部特征值集合 $\\{\\lambda_j\\}$。\n3.  **执行 Arnoldi 迭代**：实现上述算法以生成 $m \\times m$ 的上 Hessenberg 矩阵 $H_m$。实现将处理潜在的崩溃情况，如果在完成 $m$ 次迭代之前找到了不变子空间，则返回一个更小的矩阵。\n4.  **计算 Ritz 特征值**：计算生成的矩阵 $H_m$ 的特征值 $\\{r_i\\}$。\n5.  **计算度量**：通过在复平面中找到两组特征值之间最小距离的最大值，来计算误差度量 $\\max_i \\min_j |r_i - \\lambda_j|$。\n6.  **格式化输出**：收集每个测试用例的度量值，并将最终输出格式化为用方括号括起来的、以逗号分隔的、四舍五入到8位小数的浮点数列表。\n\n所有的矩阵和向量运算都将使用 `numpy` 库来执行。由于输入的矩阵和向量都是实数，Arnoldi 迭代将使用实数算术进行。所得的特征值可能是复数，`numpy` 的特征值求解器会自动处理这种情况。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef arnoldi_iteration(A, b, m):\n    \"\"\"\n    Performs Arnoldi iteration to find the Hessenberg matrix H.\n\n    Args:\n        A (np.ndarray): The square matrix.\n        b (np.ndarray): The starting vector.\n        m (int): The dimension of the Krylov subspace.\n\n    Returns:\n        np.ndarray: The m x m Hessenberg matrix H_m.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Use real types since A and b are real. Eigenvalues may be complex later.\n    H = np.zeros((m + 1, m), dtype=np.float64)\n    Q = np.zeros((n, m + 1), dtype=np.float64)\n\n    Q[:, 0] = b / np.linalg.norm(b)\n\n    for k in range(m):\n        v = A @ Q[:, k]\n        for j in range(k + 1):\n            H[j, k] = np.dot(Q[:, j].T, v)\n            v = v - H[j, k] * Q[:, j]\n\n        h_next_k = np.linalg.norm(v)\n\n        # Handle breakdown: invariant subspace found\n        if h_next_k  1e-12:\n            return H[:k + 1, :k + 1]\n\n        H[k + 1, k] = h_next_k\n        Q[:, k + 1] = v / h_next_k\n        \n    return H[:m, :m]\n\ndef calculate_approximation_error(ritz_vals, true_vals):\n    \"\"\"\n    Calculates the approximation error metric max_i min_j |r_i - lambda_j|.\n\n    Args:\n        ritz_vals (np.ndarray): Array of Ritz eigenvalues.\n        true_vals (np.ndarray): Array of true eigenvalues.\n\n    Returns:\n        float: The calculated error metric.\n    \"\"\"\n    if ritz_vals.size == 0:\n        return 0.0\n\n    # Reshape for broadcasting\n    ritz_vals_col = ritz_vals[:, np.newaxis]\n    \n    # Calculate the matrix of absolute differences\n    abs_diff_matrix = np.abs(ritz_vals_col - true_vals)\n    \n    # Find the minimum distance for each Ritz value to any true eigenvalue\n    min_dists = np.min(abs_diff_matrix, axis=1)\n    \n    # The error is the maximum of these minimum distances\n    error = np.max(min_dists)\n    \n    return error\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Test case 1\n    S1 = np.array([\n        [1, 0.2, -0.1, 0, 0, 0],\n        [0, 1, 0.3, 0, 0, 0],\n        [0, 0, 1, 0.4, 0, 0],\n        [0, 0, 0, 1, 0.5, 0],\n        [0, 0, 0, 0, 1, 0.6],\n        [0, 0, 0, 0, 0, 1]\n    ], dtype=np.float64)\n    D1 = np.diag([5, 4, 3, 2, 1, -1])\n    A1 = S1 @ D1 @ np.linalg.inv(S1)\n    b1 = np.ones(6)\n    m1 = 4\n\n    # Test case 2\n    n2 = 5\n    A2 = np.diag(2 * np.ones(n2)) + np.diag(-1 * np.ones(n2 - 1), k=1) + np.diag(-1 * np.ones(n2 - 1), k=-1)\n    b2 = np.zeros(n2)\n    b2[0] = 1\n    m2 = 1\n\n    # Test case 3\n    n3 = 6\n    A3 = 3 * np.eye(n3) + np.diag(np.ones(n3 - 1), k=1)\n    b3 = np.ones(n3)\n    m3 = 3\n\n    # Test case 4\n    A4 = np.array([\n        [4, 1, 0, 0],\n        [1, 3, 1, 0],\n        [0, 1, 2, 1],\n        [0, 0, 1, 1]\n    ], dtype=np.float64)\n    b4 = np.ones(4)\n    m4 = 4\n\n    test_cases = [\n        (A1, b1, m1),\n        (A2, b2, m2),\n        (A3, b3, m3),\n        (A4, b4, m4)\n    ]\n\n    results = []\n    for A, b, m in test_cases:\n        # Step 1: Compute true eigenvalues\n        true_eigenvalues = np.linalg.eigvals(A)\n        \n        # Step 2: Perform Arnoldi iteration\n        H_m = arnoldi_iteration(A, b, m)\n        \n        # Step 3: Compute Ritz eigenvalues\n        ritz_eigenvalues = np.linalg.eigvals(H_m)\n        \n        # Step 4: Calculate the error metric\n        error = calculate_approximation_error(ritz_eigenvalues, true_eigenvalues)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.8f}' for r in results])}]\")\n\nsolve()\n```", "id": "3282272"}]}