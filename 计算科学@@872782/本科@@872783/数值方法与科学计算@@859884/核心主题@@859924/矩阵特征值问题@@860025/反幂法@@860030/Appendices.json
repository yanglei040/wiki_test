{"hands_on_practices": [{"introduction": "反幂法的核心是反复求解一个特定的线性系统。这个练习 [@problem_id:1395843] 将该方法简化为其最基本的组成部分：单步迭代。通过亲手求解 $(A-\\sigma I)y_1 = x_0$，你将在处理完整的迭代过程之前，对核心的代数运算有一个扎实的理解。", "problem": "在一个数值算法中，从一个初始向量 $x_0$ 开始生成一个向量序列。该序列中的第一个未归一化向量（记为 $y_1$）通过求解线性系统 $(A-\\sigma I)y_1 = x_0$ 得到，其中 $A$ 是一个方阵，$\\sigma$ 是一个标量位移，$I$ 是与 $A$ 维度相同的单位矩阵。\n\n给定矩阵 $A = \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}$，位移 $\\sigma = 1.5$，以及初始向量 $x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，请确定向量 $y_1$ 的分量。请将答案表示为一个行矩阵，其中每个分量为精确分数或小数。", "solution": "我们需要求解线性系统 $(A-\\sigma I) y_{1} = x_{0}$ 以得到 $y_{1}$，其中 $A = \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix}$，$\\sigma = \\frac{3}{2}$，以及 $x_{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n首先计算位移后的矩阵：\n$$\nA - \\sigma I = \\begin{pmatrix} 3  -1 \\\\ -1  3 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  -1 \\\\ -1  \\frac{3}{2} \\end{pmatrix}.\n$$\n记 $M = A - \\sigma I$。则 $y_{1} = M^{-1} x_{0}$。对于一个 $2 \\times 2$ 矩阵 $M = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，我们使用公式 $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。此处 $a = d = \\frac{3}{2}$ 且 $b = c = -1$，所以\n$$\n\\det(M) = \\left(\\frac{3}{2}\\right)\\left(\\frac{3}{2}\\right) - (-1)(-1) = \\frac{9}{4} - 1 = \\frac{5}{4},\n$$\n且\n$$\n\\operatorname{adj}(M) = \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix}.\n$$\n因此，\n$$\nM^{-1} = \\frac{1}{\\frac{5}{4}} \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix}.\n$$\n乘以 $x_{0}$，\n$$\ny_{1} = M^{-1} x_{0} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2}  1 \\\\ 1  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{4}{5} \\begin{pmatrix} \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{5} \\\\ \\frac{4}{5} \\end{pmatrix}.\n$$\n因此，$y_{1}$ 的分量是 $\\frac{6}{5}$ 和 $\\frac{4}{5}$，我们将其表示为一个行矩阵。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{6}{5}  \\frac{4}{5} \\end{pmatrix}}$$", "id": "1395843"}, {"introduction": "你已经掌握了基本计算，现在让我们从概念上探讨该方法的行为。这个练习 [@problem_id:2216129] 提出了一个“理想情况”的假设：如果你的初始猜测恰好是一个完美的特征向量，会发生什么？解答这个问题揭示了为何迭代能够放大你所寻找的特征向量，从而为你理解其收敛性提供了关键的洞察。", "problem": "考虑一个实对称 $n \\times n$ 矩阵 $A$，它有 $n$ 个不同的特征值 $\\lambda_1, \\lambda_2, \\dots, \\lambda_n$ 和它们对应的归一化特征向量 $v_1, v_2, \\dots, v_n$。带位移的反幂法是一种用于寻找 $A$ 的特征向量的迭代算法。从一个初始猜测向量 $x_0$ 开始，该方法使用以下递推关系生成一个向量序列：\n$$\nx_{k+1} = \\frac{(A - \\sigma I)^{-1} x_k}{\\| (A - \\sigma I)^{-1} x_k \\|}\n$$\n其中 $\\sigma$ 是一个实数值位移，$I$ 是 $n \\times n$ 单位矩阵，$\\| \\cdot \\|$ 表示欧几里得范数。\n\n假设初始猜测向量被选为 $A$ 的一个特征向量，具体为 $x_0 = v_j$，其中索引 $j \\in \\{1, 2, \\dots, n\\}$。假设选择的位移 $\\sigma$ 不等于 $A$ 的任何一个特征值（即，对于所有的 $i=1, \\dots, n$，都有 $\\sigma \\neq \\lambda_i$）。\n\n在该方法的一次迭代之后，计算出一个新的向量 $x_1$。下列哪个陈述正确地描述了得到的向量 $x_1$ 和初始向量 $x_0$ 之间的关系？\n\nA. 向量 $x_1$ 与 $x_0$ 平行。\n\nB. 向量 $x_1$ 与 $x_0$ 正交。\n\nC. 向量 $x_1$ 是零向量。\n\nD. 向量 $x_1$ 是对应于另一个不同特征值 $\\lambda_k$ 的特征向量，其中 $k \\neq j$。\n\nE. 向量 $x_1$ 未定义，因为矩阵 $(A - \\sigma I)$ 不可逆。", "solution": "因为 $A$ 是具有不同特征值的实对称矩阵，每个特征向量 $v_{j}$ 都满足 $A v_{j} = \\lambda_{j} v_{j}$，并且 $v_{j}$ 是归一化的。对于任何不等于任何特征值的实数 $\\sigma$，矩阵 $A - \\sigma I$ 都是可逆的，并且\n$$\n(A - \\sigma I) v_{j} = (\\lambda_{j} - \\sigma) v_{j}.\n$$\n对其求逆可得\n$$\n(A - \\sigma I)^{-1} v_{j} = (\\lambda_{j} - \\sigma)^{-1} v_{j}.\n$$\n当初始猜测为 $x_{0} = v_{j}$ 时，未归一化的迭代向量为\n$$\ny := (A - \\sigma I)^{-1} x_{0} = (\\lambda_{j} - \\sigma)^{-1} v_{j}.\n$$\n它的欧几里得范数是\n$$\n\\|y\\| = | \\lambda_{j} - \\sigma |^{-1} \\| v_{j} \\| = | \\lambda_{j} - \\sigma |^{-1},\n$$\n因为 $\\|v_{j}\\| = 1$。因此，归一化的迭代向量是\n$$\nx_{1} = \\frac{y}{\\|y\\|} = \\frac{(\\lambda_{j} - \\sigma)^{-1}}{| \\lambda_{j} - \\sigma |^{-1}} v_{j} = \\frac{| \\lambda_{j} - \\sigma |}{\\lambda_{j} - \\sigma} \\, v_{j}.\n$$\n因为 $\\lambda_{j}$ 和 $\\sigma$ 是实数且 $\\lambda_{j} \\neq \\sigma$，所以因子 $\\frac{| \\lambda_{j} - \\sigma |}{\\lambda_{j} - \\sigma}$ 等于 $\\pm 1$。因此 $x_{1} = \\pm v_{j}$，它与 $x_{0}$ 平行。它既不与 $x_{0}$ 正交，也不是零向量，并且根据假设 $(A - \\sigma I)$ 是可逆的，所以迭代是定义的。它不会变为另一个不同特征值对应的特征向量。\n\n因此，正确的选项是 A。", "answer": "$$\\boxed{A}$$", "id": "2216129"}, {"introduction": "现在是时候将理论与实际应用联系起来了。这最后一个挑战 [@problem_id:3243386] 将指导你实现瑞利商迭代法 (Rayleigh Quotient Iteration)，这是反幂法的一种高效变体。你将把求解移位系统与动态更新的移位量相结合，从而创造一个寻找特征值的强大工具，并直面诸如收敛性和数值稳定性等实际问题。", "problem": "实现一个完整、可运行的程序，使用带有由当前迭代向量的瑞利商给出的动态位移的逆幂法，来计算一个实方阵的特征值。目标是通过迭代求解一个带位移的线性系统并从第一性原理出发更新位移，来近似一个实矩阵的特征值。\n\n您在设计算法时必须从以下基本概念出发：\n- 特征对的定义：一个非零向量 $\\boldsymbol{v} \\in \\mathbb{R}^n$ 和一个标量 $\\lambda \\in \\mathbb{R}$，满足 $A \\boldsymbol{v} = \\lambda \\boldsymbol{v}$。\n- 对于一个实矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 的非零向量 $\\boldsymbol{x} \\in \\mathbb{R}^n$，其瑞利商被定义为：当用 $\\boldsymbol{x}$ 的标量倍来近似 $A \\boldsymbol{x}$ 时，在最小二乘意义下使残差最小化的那个唯一标量。当 $\\boldsymbol{x}$ 是一个特征向量时，该商等于对应的特征值。\n- 逆迭代思想：如果 $\\sigma$ 接近一个特征值 $\\lambda$，那么线性系统 $(A - \\sigma I)\\boldsymbol{y} = \\boldsymbol{x}$ 是近奇异的，其解倾向于放大 $\\boldsymbol{x}$ 在相应特征向量方向上的分量。对 $\\boldsymbol{y}$ 进行归一化会得到一个改进的迭代向量。\n\n除了矩阵元素是实数外，您不能假设任何特殊的矩阵结构。您的实现必须：\n- 仅使用实数运算。\n- 在每一步使用欧几里得范数对迭代向量进行归一化。\n- 使用当前迭代向量的瑞利商作为动态位移。\n- 使用带部分主元的高斯消去法（标准的稠密直接解法）求解线性系统。如果求解因奇异或近奇异而失败，则通过求解 $(A - \\sigma I + \\epsilon I)$（其中 $\\epsilon = 10^{-14} \\max(1, \\lVert A \\rVert_2)$）来应用最小对角正则化，然后重试。如果初始迭代向量已经在容差范围内满足特征对方程，则立即终止，不要尝试求解奇异系统。\n- 当残差范数 $\\lVert A \\boldsymbol{x}_k - \\mu_k \\boldsymbol{x}_k \\rVert_2$ 小于或等于给定的容差时，或当达到最大迭代次数时终止。\n\n不需要输入；您的程序必须硬编码以下测试套件，并按如下规定生成单行输出。\n\n测试套件：\n- 案例 $1$（正常路径，对角矩阵）：\n  - $A_1 = \\mathrm{diag}(1, 2, 5)$。\n  - $\\boldsymbol{x}_0^{(1)} = [0.2, 0.2, 0.96]^T$。\n  - 容差 $= 10^{-12}$，最大迭代次数 $= 100$。\n- 案例 $2$（边界情况，因初始向量是精确特征向量（只差一个缩放因子）而立即收敛）：\n  - $A_2 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}$。\n  - $\\boldsymbol{x}_0^{(2)} = [1, 1]^T$。\n  - 容差 $= 10^{-12}$，最大迭代次数 $= 50$。\n- 案例 $3$（谱聚集）：\n  - $A_3 = \\begin{bmatrix} 1  10^{-3}  0 \\\\ 10^{-3}  1  0 \\\\ 0  0  2 \\end{bmatrix}$。\n  - $\\boldsymbol{x}_0^{(3)} = [1, -1, 0.1]^T$。\n  - 容差 $= 10^{-12}$，最大迭代次数 $= 100$。\n- 案例 $4$（对称三对角矩阵）：\n  - $A_4 = \\begin{bmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{bmatrix}$。\n  - $\\boldsymbol{x}_0^{(4)} = [0.5, 0.5, 0.5]^T$。\n  - 容差 $= 10^{-12}$，最大迭代次数 $= 100$。\n\n对于每个案例，您的程序必须计算：\n- 最终的近似特征值 $\\mu$，由最后一次迭代的瑞利商给出。\n- 最终的残差范数 $r = \\lVert A \\boldsymbol{x} - \\mu \\boldsymbol{x} \\rVert_2$。\n\n最终输出格式：\n- 生成一个单行输出，包含一个长度为 4 的列表，其中每个元素是对应于上述案例顺序的二元列表 $[\\mu, r]$。\n- $\\mu$ 和 $r$ 都必须四舍五入到小数点后 10 位。\n- 该行不得包含任何空格。例如：\"[[mu1,res1],[mu2,res2],[mu3,res3],[mu4,res4]]\"，其中 \"mu1\" 和 \"res1\" 是四舍五入到小数点后 10 位的小数。\n\n不涉及任何物理单位或角度单位。所有小数都应表示为普通十进制数。最终输出必须是确定性的，且不得需要任何用户输入或外部数据。", "solution": "该问题要求实现瑞利商迭代 (Rayleigh Quotient Iteration, RQI)，这是一种用于寻找实方阵 $A \\in \\mathbb{R}^{n \\times n}$ 的一个特征对 $(\\lambda, \\boldsymbol{v})$ 的数值方法。一个特征对由基本关系 $A \\boldsymbol{v} = \\lambda \\boldsymbol{v}$ 定义，其中 $\\lambda$ 是一个标量特征值，$\\boldsymbol{v}$ 是一个非零特征向量。该算法需要根据要求从第一性原理进行开发。\n\nRQI 方法的核心结合了两个关键概念：瑞利商和逆迭代。\n\n1.  **瑞利商 (The Rayleigh Quotient)**：对于给定的实矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和非零向量 $\\boldsymbol{x} \\in \\mathbb{R}^n$，瑞利商定义为：\n    $$\n    \\mu(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}^{T}A\\boldsymbol{x}}{\\boldsymbol{x}^{T}\\boldsymbol{x}}\n    $$\n    对于给定的向量 $\\boldsymbol{x}$，这个标量值在最小二乘意义上提供了对特征值的最佳近似。具体来说，$\\mu(\\boldsymbol{x})$ 是使残差的欧几里得范数 $\\lVert A\\boldsymbol{x} - \\alpha\\boldsymbol{x} \\rVert_2$ 最小化的标量 $\\alpha$。如果 $\\boldsymbol{x}$ 是一个特征向量，其瑞利商 $\\mu(\\boldsymbol{x})$ 就精确地等于对应的特征值 $\\lambda$。在我们的算法中，由于特征向量迭代值 $\\boldsymbol{x}_k$ 总是被归一化为单位欧几里得范数 ($\\lVert \\boldsymbol{x}_k \\rVert_2 = 1$)，其分母 $\\boldsymbol{x}_k^T \\boldsymbol{x}_k = 1$，从而将计算简化为 $\\mu(\\boldsymbol{x}_k) = \\boldsymbol{x}_k^T A \\boldsymbol{x}_k$。\n\n2.  **带动态位移的逆迭代 (Inverse Iteration with a Dynamic Shift)**：标准的逆迭代方法通过 $\\boldsymbol{x}_{k+1} \\propto (A - \\sigma I)^{-1}\\boldsymbol{x}_k$ 计算迭代向量，其中 $\\sigma$ 是一个固定的位移。该方法收敛到与 $\\sigma$ 最接近的 $A$ 的特征值所对应的特征向量。RQI 是一种高级变体，其中位移 $\\sigma$ 不是固定的，而是在每一步动态更新，使用当前迭代向量的瑞利商作为下一步的位移。这种位移选择，$\\sigma_k = \\mu(\\boldsymbol{x}_k)$，通常会导致向特征对的非常快速（三次）收敛。\n\nRQI 的迭代过程可以总结如下。从一个初始向量 $\\boldsymbol{x}_0$ 开始，对于每次迭代 $k = 0, 1, 2, \\dots$：\na. 当前的特征向量近似值为 $\\boldsymbol{x}_k$。\nb. 计算一个改进的特征值近似值（即位移）：$\\mu_k = \\mu(\\boldsymbol{x}_k) = \\boldsymbol{x}_k^T A \\boldsymbol{x}_k$。\nc. 求解线性系统以找到一个在所需特征向量方向上富集的向量：$(A - \\mu_k I) \\boldsymbol{y}_{k+1} = \\boldsymbol{x}_k$。\nd. 通过对解进行归一化获得新的特征向量近似值：$\\boldsymbol{x}_{k+1} = \\frac{\\boldsymbol{y}_{k+1}}{\\lVert \\boldsymbol{y}_{k+1} \\rVert_2}$。\n\n完整的算法，包括根据问题要求进行的初始化、终止和特殊处理，如下所示：\n\n**算法：瑞利商迭代**\n\n1.  **初始化**：\n    - 给定一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$、一个初始向量 $\\boldsymbol{x}_0$、一个容差 `tol` 和最大迭代次数 `max_iter`。\n    - 归一化初始向量：$\\boldsymbol{x} \\leftarrow \\boldsymbol{x}_0 / \\lVert \\boldsymbol{x}_0 \\rVert_2$。\n    - 为可能的正则化预先计算矩阵的 2-范数 $\\lVert A \\rVert_2$。正则化参数为 $\\epsilon = 10^{-14} \\max(1, \\lVert A \\rVert_2)$。令 $I$ 为 $n \\times n$ 的单位矩阵。\n\n2.  **迭代循环**：对于 $k$ 从 $0$ 到 `max_iter - 1`：\n    a. **计算特征值估计并检查收敛性**：计算瑞利商 $\\mu = \\boldsymbol{x}^T A \\boldsymbol{x}$。通过计算残差的范数来检查终止条件：$r = \\lVert A\\boldsymbol{x} - \\mu\\boldsymbol{x} \\rVert_2$。如果 $r \\le \\text{tol}$，则算法已收敛。终止并返回最终的对 $(\\mu, r)$。这一步也处理了初始向量 $\\boldsymbol{x}_0$ 已经是足够精确的特征向量而导致立即收敛的情况。\n\n    b. **求解带位移的线性系统**：构建位移矩阵 $M = A - \\mu I$。求解线性系统 $M \\boldsymbol{y} = \\boldsymbol{x}$ 以得到 $\\boldsymbol{y}$。这一步使用标准的直接解法（带部分主元的高斯消去法）执行。\n\n    c. **处理奇异性**：当 $\\mu$ 接近一个特征值时，矩阵 $M$ 变得近奇异。如果求解器失败（例如，引发奇异性错误），则对系统进行正则化。转而求解一个新的系统 $(M + \\epsilon I) \\boldsymbol{y} = \\boldsymbol{x}$。这个微小的对角位移使矩阵可逆，同时对解的扰动最小。\n\n    d. **归一化下一个迭代向量**：通过归一化解向量来更新特征向量的近似值：$\\boldsymbol{x} \\leftarrow \\boldsymbol{y} / \\lVert \\boldsymbol{y} \\rVert_2$。\n\n3.  **终止**：如果循环完成而未满足容差标准（即达到 `max_iter`），算法终止。最终的特征值估计 $\\mu$ 和残差范数 $r$ 将基于最后计算的迭代向量 $\\boldsymbol{x}$ 进行计算并返回。\n\n此过程是确定性的，并且对于给定的测试案例，预期会收敛到与初始向量的瑞利商最接近的特征值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rayleigh_quotient_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Computes an eigenvalue of a matrix using the inverse power method\n    with Rayleigh quotient dynamic shift.\n\n    Args:\n        A (np.ndarray): The real square matrix.\n        x0 (np.ndarray): The initial non-zero vector.\n        tol (float): The convergence tolerance for the residual norm.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        tuple: A tuple containing the final approximate eigenvalue (mu) and\n               the final residual norm.\n    \"\"\"\n    if np.linalg.norm(x0) == 0:\n        raise ValueError(\"Initial vector x0 cannot be the zero vector.\")\n\n    x = x0 / np.linalg.norm(x0)\n    \n    n = A.shape[0]\n    I = np.eye(n)\n    \n    # Pre-compute matrix 2-norm for regularization\n    A_norm = np.linalg.norm(A, 2)\n    epsilon = 1e-14 * max(1.0, A_norm)\n\n    mu = 0.0\n    res_norm = float('inf')\n\n    for _ in range(max_iter):\n        # 1. Calculate Rayleigh quotient and check for convergence\n        # Since x is a unit vector, x.T @ x = 1.\n        mu = x.T @ A @ x\n        \n        residual_vec = A @ x - mu * x\n        res_norm = np.linalg.norm(residual_vec)\n        \n        if res_norm = tol:\n            return mu, res_norm\n\n        # 2. Setup and solve the shifted linear system\n        M = A - mu * I\n        try:\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # 3. Handle singularity with regularization\n            M_reg = M + epsilon * I\n            y = np.linalg.solve(M_reg, x)\n        \n        # 4. Normalize to get the next iterate\n        norm_y = np.linalg.norm(y)\n        if norm_y == 0:\n            # In the unlikely event of a zero vector solution\n            return mu, res_norm\n        x = y / norm_y\n        \n    # After max_iter, calculate final values from the last iterate\n    mu = x.T @ A @ x\n    res_norm = np.linalg.norm(A @ x - mu * x)\n    return mu, res_norm\n\ndef solve():\n    \"\"\"\n    Runs the defined test suite for the Rayleigh Quotient Iteration algorithm\n    and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.diag([1.0, 2.0, 5.0]),\n            \"x0\": np.array([0.2, 0.2, 0.96]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        },\n        {\n            \"A\": np.array([[0.0, 1.0], [1.0, 0.0]]),\n            \"x0\": np.array([1.0, 1.0]),\n            \"tol\": 1e-12,\n            \"max_iter\": 50\n        },\n        {\n            \"A\": np.array([[1.0, 1e-3, 0.0], [1e-3, 1.0, 0.0], [0.0, 0.0, 2.0]]),\n            \"x0\": np.array([1.0, -1.0, 0.1]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        },\n        {\n            \"A\": np.array([[2.0, -1.0, 0.0], [-1.0, 2.0, -1.0], [0.0, -1.0, 2.0]]),\n            \"x0\": np.array([0.5, 0.5, 0.5]),\n            \"tol\": 1e-12,\n            \"max_iter\": 100\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, r = rayleigh_quotient_iteration(case[\"A\"], case[\"x0\"], case[\"tol\"], case[\"max_iter\"])\n        \n        # Round to 10 decimal places as required\n        mu_rounded = round(mu, 10)\n        r_rounded = round(r, 10)\n        \n        # Format as a string representation of a list\n        results.append(f\"[{str(mu_rounded)},{str(r_rounded)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3243386"}]}