{"hands_on_practices": [{"introduction": "理论告诉我们，标准牛顿法在处理重根时收敛速度会从二次下降到线性。但理论的感受不如亲眼所见来得深刻。本练习将通过编写代码，直接比较标准牛顿法和修正牛顿法在处理已知重根问题时的表现，让你通过具体的迭代次数来量化收敛速度的差异。", "problem": "考虑求解 $f(x)=0$ 的任务，其中根的重数已知。对于一个充分可微的函数 $f$，如果 $f(x^{\\star})=0$，对于所有满足 $1 \\le k \\le m-1$ 的整数 $k$ 都有 $f^{(k)}(x^{\\star})=0$，并且 $f^{(m)}(x^{\\star}) \\ne 0$，则点 $x^{\\star}$ 是 $f$ 的一个 $m$ 重根。标准牛顿法是通过在当前迭代点处用其一阶泰勒近似局部替换 $f$，并求解得到的线性化方程而导出的。当根的重数大于1时，未经修改的迭代可能会失去其通常的二次收敛性。一种针对已知重数 $m$ 的修正牛顿法通过重新调整迭代步长来恢复快速收敛性。\n\n你的任务是编写一个完整的、可运行的程序，对于函数族 $f_m(x)=(x-1)^m$（其中 $m \\in \\{2,3,5\\}$），比较标准牛顿法与知晓重数的修正牛顿法。对于每个函数，使用三个初始猜测值 $x_0 \\in \\{2,1,-5\\}$，并应用这两种方法来计算达到固定容差以下的残差所需的迭代次数。残差应以 $|f_m(x_n)|$ 来衡量，停止条件是 $|f_m(x_n)| \\le \\tau$，其中 $\\tau=10^{-12}$。如果初始猜测值已经满足 $|f_m(x_0)| \\le \\tau$，则迭代次数为 $0$。如果方法在最多 $N_{\\text{max}}=1000$ 次迭代内未能达到容差，则报告整数 $-1$ 作为该方法在该测试用例上的迭代次数。\n\n实现要求：\n- 使用切线线性化原理实现用于求根的标准牛顿法。\n- 实现针对已知重数 $m$ 的修正牛顿法，适当调整步长以处理重根问题。\n- 在两种方法中都使用 $f_m(x)=(x-1)^m$ 及其导数 $f_m'(x)$。\n- 使用残差 $|f_m(x_n)|$ 和容差 $\\tau=10^{-12}$ 进行停止检查。\n- 使用最大迭代次数上限 $N_{\\text{max}}=1000$。\n\n测试套件规范：\n- 有序的测试用例列表是 $m \\in \\{2,3,5\\}$ 和 $x_0 \\in \\{2,1,-5\\}$ 的笛卡尔积，按 $m$ 然后按 $x_0$ 的字典序排列。具体来说，测试用例 $(m,x_0)$ 分别是 $(2,2)$、$(2,1)$、$(2,-5)$、$(3,2)$、$(3,1)$、$(3,-5)$、$(5,2)$、$(5,1)$、$(5,-5)$。\n\n对于每个测试用例，你的程序必须输出一对整数 $[n_{\\text{std}},n_{\\text{mod}}]$，其中 $n_{\\text{std}}$ 是标准牛顿法的迭代次数，$n_{\\text{mod}}$ 是修正牛顿法的迭代次数。最终输出必须是单行文本，包含所有九对整数对，按照指定的测试用例顺序排列，形式为用逗号分隔的列表，并用方括号括起来，不含空格。例如，一个包含三对假设结果的输出看起来像这样：`[[1,1],[0,0],[7,2]]`。你的程序应产生确切的最终格式：\n- 单行文本：`[[n_{\\text{std},1},n_{\\text{mod},1}],[n_{\\text{std},2},n_{\\text{mod},2}],\\dots,[n_{\\text{std},9},n_{\\text{mod},9}]]`。", "solution": "该问题要求比较标准牛顿法和修正牛顿法在寻找已知重数 $m$ 的根时的表现。分析将在函数族 $f_m(x)=(x-1)^m$ 上进行，该函数族在 $x^{\\star}=1$ 处有一个 $m$ 重根。\n\n如果函数 $f$ 在点 $x^{\\star}$ 处的值 $f(x^{\\star})=0$，其前 $m-1$ 阶导数在 $x^{\\star}$ 处也为零，即对于 $1 \\le k \\le m-1$ 有 $f^{(k)}(x^{\\star})=0$，而其 $m$ 阶导数不为零 $f^{(m)}(x^{\\star}) \\ne 0$，则点 $x^{\\star}$ 被定义为 $f$ 的一个 $m$ 重根。\n\n### 标准牛顿法\n\n标准牛顿-拉夫逊方法通过构造一系列逐次逼近的近似值来求解方程 $f(x)=0$ 的根。迭代过程由 $f(x)$ 在迭代点 $x_n$ 附近的一阶泰勒级数展开导出：\n$$f(x) \\approx f(x_n) + f'(x_n)(x - x_n)$$\n为了找到根，我们令 $f(x)=0$ 并求解 $x$，得到的解即为下一个迭代点 $x_{n+1}$：\n$$0 = f(x_n) + f'(x_n)(x_{n+1} - x_n)$$\n这就得到了标准的牛顿迭代公式：\n$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n对于单根（其中 $m=1$），该方法表现出二次收敛性，这意味着每次迭代后正确的小数位数大约会翻倍。然而，对于重数 $m>1$ 的根，收敛速度会退化为线性收敛，收敛率为 $(m-1)/m$。\n\n对于特定函数 $f_m(x)=(x-1)^m$，其导数为 $f_m'(x)=m(x-1)^{m-1}$。将这些代入迭代公式可得：\n$$x_{n+1} = x_n - \\frac{(x_n-1)^m}{m(x_n-1)^{m-1}} = x_n - \\frac{x_n-1}{m}$$\n设第 $n$ 步的误差为 $e_n = x_n-1$。误差更新规则为：\n$$e_{n+1} = x_{n+1}-1 = \\left(x_n - \\frac{x_n-1}{m}\\right) - 1 = (x_n - 1) - \\frac{x_n-1}{m} = e_n \\left(1 - \\frac{1}{m}\\right)$$\n这证实了线性收敛性，因为每一步的误差都会减少一个常数因子 $1 - 1/m$。\n\n### 针对重根的修正牛顿法\n\n为了恢复对已知重数 $m$ 的根的二次收敛性，可以修改标准的牛顿迭代步长。修正后的迭代公式为：\n$$x_{n+1} = x_n - m \\frac{f(x_n)}{f'(x_n)}$$\n这个公式有几种推导方式。一种方法是对辅助函数 $u(x) = [f(x)]^{1/m}$ 应用标准牛顿法。如果 $f(x)$ 在 $x^{\\star}$ 处有一个 $m$ 重根，那么 $u(x)$ 在 $x^{\\star}$ 处有一个单根，标准牛顿法对其具有二次收敛性。$u(x)$ 的导数是 $u'(x) = \\frac{1}{m} f(x)^{(1/m)-1}f'(x)$。对 $u(x)$ 的牛顿迭代是：\n$$x_{n+1} = x_n - \\frac{u(x_n)}{u'(x_n)} = x_n - \\frac{f(x_n)^{1/m}}{\\frac{1}{m} f(x_n)^{(1/m)-1}f'(x_n)} = x_n - m \\frac{f(x_n)}{f'(x_n)}$$\n这正是修正后的公式。\n\n对于特定函数 $f_m(x)=(x-1)^m$，修正后的迭代变为：\n$$x_{n+1} = x_n - m \\frac{(x_n-1)^m}{m(x_n-1)^{m-1}} = x_n - (x_n-1) = 1$$\n这个显著的结果表明，对于函数族 $f_m(x)=(x-1)^m$，修正牛顿法对于任何不等于1的初始猜测值 $x_0 \\ne 1$，都能在单次迭代中收敛到精确根 $x^{\\star}=1$。\n\n### 算法实现\n\n对于由数对 $(m, x_0)$（其中 $m \\in \\{2,3,5\\}$ 且 $x_0 \\in \\{2,1,-5\\}$）定义的每个测试用例，我们将对标准方法和修正方法执行以下步骤：\n\n1.  初始化迭代次数 $n=0$ 和当前猜测值 $x=x_0$。\n2.  检查初始停止条件：如果残差 $|f_m(x_0)| = |(x_0 - 1)^m|$ 小于或等于容差 $\\tau=10^{-12}$，则过程终止，迭代次数为 $0$。这种情况在 $x_0=1$ 时发生。\n3.  开始一个迭代循环，最多进行 $N_{\\text{max}}=1000$ 次迭代。在每一步 $n = 1, 2, \\dots, N_{\\text{max}}$ 中：\n    a. 使用相应的公式（标准或修正）计算下一个迭代点 $x_{n}$。\n    b. 更新当前猜测值：$x \\leftarrow x_{n}$。\n    c. 计算残差 $|f_m(x)| = |(x-1)^m|$。\n    d. 如果残差小于或等于 $\\tau$，则循环终止，并返回当前迭代次数 $n$。\n4.  如果循环完成时仍未满足容差（即 $n=N_{\\text{max}}$），则该方法未在允许的迭代次数内收敛。在这种情况下，报告值 $-1$。\n\n此过程将对所有九个指定的测试用例执行，并且将收集迭代次数对 $[n_{\\text{std}}, n_{\\text{mod}}]$ 并格式化为最终输出。根据以上分析，对于任何不等于1的初始猜测值 $x_0 \\ne 1$，修正方法预计将在 $n_{\\text{mod}}=1$ 次迭代内收敛。对于 $x_0=1$，两种方法都将报告 $n=0$ 次迭代。标准方法将表现出线性收敛，所需的迭代次数会随着重数 $m$ 和与根的初始距离的增加而增加。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef newton_standard(m, x0, tau, n_max):\n    \"\"\"\n    Performs the standard Newton's method for f(x) = (x-1)^m.\n    The update rule simplifies to x_next = x - (x-1)/m.\n\n    Args:\n        m (int): The multiplicity of the root.\n        x0 (float): The initial guess.\n        tau (float): The tolerance for the residual |f(x)|.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required, or -1 if not converged.\n    \"\"\"\n    x = np.float64(x0)\n\n    # Check if the initial guess already satisfies the tolerance.\n    # The exponentiation can be numerically sensitive, but for the given\n    # test cases, the values are well within float64 range.\n    try:\n        initial_residual = np.abs(np.power(x - 1.0, m))\n    except OverflowError:\n        # If the initial guess is very far, residual could overflow.\n        # This is safe for the given test cases but good practice.\n        initial_residual = np.inf\n\n    if initial_residual = tau:\n        return 0\n\n    for n in range(1, n_max + 1):\n        # The update rule is derived as:\n        # f(x) = (x-1)^m\n        # f'(x) = m * (x-1)^(m-1)\n        # x_next = x - f(x)/f'(x) = x - (x-1)/m\n        if x == 1.0:\n            # Landed exactly on the root. This will satisfy the check below,\n            # but this handles the case explicitly.\n            return n\n        \n        x = x - (x - 1.0) / np.float64(m)\n\n        try:\n            residual = np.abs(np.power(x - 1.0, m))\n        except OverflowError:\n            residual = np.inf\n\n        if residual = tau:\n            return n\n\n    return -1\n\ndef newton_modified(m, x0, tau, n_max):\n    \"\"\"\n    Performs the modified Newton's method for f(x) = (x-1)^m.\n    The update rule simplifies to x_next = 1.\n\n    Args:\n        m (int): The multiplicity of the root.\n        x0 (float): The initial guess.\n        tau (float): The tolerance for the residual |f(x)|.\n        n_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations required.\n    \"\"\"\n    x = np.float64(x0)\n\n    try:\n        initial_residual = np.abs(np.power(x - 1.0, m))\n    except OverflowError:\n        initial_residual = np.inf\n\n    if initial_residual = tau:\n        return 0\n\n    # The modified Newton's method for f(x) = (x-1)^m converges in one step:\n    # x_next = x - m * f(x)/f'(x) = x - m * (x-1)^m / (m*(x-1)^(m-1))\n    #        = x - (x-1) = 1\n    # The residual at step 1 will be |(1-1)^m| = 0, which is = tau.\n    return 1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the problem parameters\n    tau = 1.0e-12\n    n_max = 1000\n    multiplicities = [2, 3, 5]\n    initial_guesses = [2, 1, -5]\n\n    # Generate the ordered list of test cases\n    test_cases = []\n    for m in multiplicities:\n        for x0 in initial_guesses:\n            test_cases.append((m, x0))\n\n    results = []\n    for m, x0 in test_cases:\n        n_std = newton_standard(m, x0, tau, n_max)\n        n_mod = newton_modified(m, x0, tau, n_max)\n        results.append([n_std, n_mod])\n\n    # Format the output string as specified: [[r1_1,r1_2],[r2_1,r2_2],...]\n    # with no spaces.\n    result_strings = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3254008"}, {"introduction": "在上一个练习([@problem_id:3254008])中，我们看到了修正牛顿法的威力，但它有一个前提——我们必须预先知道根的重数$m$。这个练习将带你探究一个更实际的问题：如何在未知$m$的情况下，利用函数在某点附近的信息来估计它？我们将推导一个重数估计量，并通过数值实验来检验它在有限精度计算机上的表现和偏差。", "problem": "考虑一个实值函数 $f$，它在 $x=\\alpha$ 处有一个已知重数结构的根。具体来说，假设 $f(x)=(x-\\alpha)^m g(x)$，其中 $m\\ge 1$ 是一个整数，$g$ 是一个二阶连续可微函数，且 $g(\\alpha)\\ne 0$。这保证了 $f(\\alpha)=0$ 并且 $m$ 等于该根的重数。在精确算术中，可以根据 $x=\\alpha$ 附近的局部微分信息推断出 $m$。在遵循美国电气和电子工程师协会（IEEE）754 双精度标准的浮点算术中，由于舍入和抵消误差，这种推断可能会出现偏差。\n\n任务：\n- 给定结构性假设 $f(x)=(x-\\alpha)^m g(x)$ 且 $g(\\alpha)\\ne 0$，推导当 $x\\to \\alpha$ 时比率 $\\dfrac{f(x) f''(x)}{\\left(f'(x)\\right)^2}$ 的精确极限，结果仅用 $m$ 表示。\n- 提出并论证一个关于重数 $m$ 的逐点估计量 $\\hat m(x)$，该估计量是 $x\\ne \\alpha$ 的函数，并且仅使用 $f(x)$、$f'(x)$ 和 $f''(x)$。\n- 实现一个数值实验，以评估当在非常接近 $\\alpha$ 的点 $x$ 处计算该估计量时，在 IEEE 754 双精度下产生的偏差。\n\n使用以下函数和参数的测试套件。在每种情况下，使用点 $x=\\alpha+\\delta$ 在五个偏移量 $\\delta\\in\\{10^{-1},10^{-4},10^{-8},10^{-12},10^{-16}\\}$ 处评估该估计量。\n1. 情况 A（理想情况）：$m=3$, $\\alpha=0$, $g(x)=1+x$。\n2. 情况 B（非零 $\\alpha$ 和弯曲的 $g$）：$m=2$, $\\alpha=0.3$, $g(x)=e^{x}$。\n3. 情况 C（单根边界情况）：$m=1$, $\\alpha=-0.2$, $g(x)=1+x^2$。\n4. 情况 D（更高重数和振荡的 $g$）：$m=5$, $\\alpha=0$, $g(x)=\\cos(x)$。\n\n对于每种情况，定义 $f(x)=(x-\\alpha)^m g(x)$，使用乘法法则符号化地计算 $f'(x)$ 和 $f''(x)$，在每个指定的 $\\delta$ 处评估估计量 $\\hat m(x)$，然后报告每种情况的单一数值摘要：五个偏移量上的平均绝对偏差，定义为\n$$\n\\frac{1}{5}\\sum_{\\delta\\in\\{10^{-1},10^{-4},10^{-8},10^{-12},10^{-16}\\}} \\left|\\hat m(\\alpha+\\delta)-m\\right|.\n$$\n所有计算都应在 IEEE 754 双精度浮点算术中执行。不涉及物理单位。当角度作为三角函数的参数出现时，必须以弧度为单位进行解释。\n\n您的程序应生成单行输出，其中包含四个情况的结果，格式为方括号内以逗号分隔的列表，顺序为 A、B、C、D，例如 `[r_A,r_B,r_C,r_D]`，其中每个 $r_\\cdot$ 是该情况下的平均绝对偏差，表示为浮点数。", "solution": "该问题要求推导与具有已知重数结构的根的函数 $f(x)$ 相关的极限，提出重数 $m$ 的估计量，并对该估计量在浮点算术中的偏差进行数值评估。\n\n### 第一部分：极限的推导\n\n给定一个函数 $f(x)$，它在 $x=\\alpha$ 处有一个重数为 $m$ 的根。这可以正式表示为 $f(x) = (x-\\alpha)^m g(x)$，其中 $m \\ge 1$ 是一个整数，$g(x)$ 是一个二阶连续可微函数，且 $g(\\alpha) \\ne 0$。我们的目标是求当 $x \\to \\alpha$ 时比率 $Q(x) = \\dfrac{f(x) f''(x)}{\\left(f'(x)\\right)^2}$ 的极限。\n\n首先，我们使用乘法法则计算 $f(x)$ 的一阶和二阶导数。\n函数为：\n$$f(x) = (x-\\alpha)^m g(x)$$\n\n一阶导数 $f'(x)$ 为：\n$$f'(x) = \\frac{d}{dx}\\left[(x-\\alpha)^m g(x)\\right] = m(x-\\alpha)^{m-1}g(x) + (x-\\alpha)^m g'(x)$$\n提出公因式 $(x-\\alpha)^{m-1}$，我们得到：\n$$f'(x) = (x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)]$$\n\n二阶导数 $f''(x)$ 是通过对 $f'(x)$ 求导得到的：\n$$f''(x) = \\frac{d}{dx}\\left[(x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)]\\right]$$\n$$f''(x) = (m-1)(x-\\alpha)^{m-2}[m g(x) + (x-\\alpha) g'(x)] + (x-\\alpha)^{m-1}[m g'(x) + g'(x) + (x-\\alpha) g''(x)]$$\n提出公因式 $(x-\\alpha)^{m-2}$：\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ (m-1)(m g(x) + (x-\\alpha) g'(x)) + (x-\\alpha)((m+1)g'(x) + (x-\\alpha)g''(x)) \\right]$$\n展开并合并同类项：\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + (m-1+m+1)(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n$$f''(x) = (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n\n现在我们构造比率 $Q(x)$ 的必要组成部分。\n乘积 $f(x) f''(x)$ 为：\n$$f(x) f''(x) = (x-\\alpha)^m g(x) \\cdot (x-\\alpha)^{m-2} \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n$$f(x) f''(x) = (x-\\alpha)^{2m-2} g(x) \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]$$\n项 $(f'(x))^2$ 为：\n$$\\left(f'(x)\\right)^2 = \\left( (x-\\alpha)^{m-1} [m g(x) + (x-\\alpha) g'(x)] \\right)^2$$\n$$\\left(f'(x)\\right)^2 = (x-\\alpha)^{2m-2} [m g(x) + (x-\\alpha) g'(x)]^2$$\n\n对于 $x \\ne \\alpha$，我们可以构建比率 $Q(x)$ 并消去公因式 $(x-\\alpha)^{2m-2}$：\n$$Q(x) = \\frac{g(x) \\left[ m(m-1)g(x) + 2m(x-\\alpha)g'(x) + (x-\\alpha)^2 g''(x) \\right]}{[m g(x) + (x-\\alpha) g'(x)]^2}$$\n\n为了求出当 $x \\to \\alpha$ 时的极限，我们利用 $g(x)$、$g'(x)$ 和 $g''(x)$ 的连续性。所有包含因子 $(x-\\alpha)$ 的项都将趋于零。\n$$\\lim_{x\\to \\alpha} Q(x) = \\frac{g(\\alpha) \\left[ m(m-1)g(\\alpha) + 2m(0)g'(\\alpha) + (0)^2 g''(\\alpha) \\right]}{[m g(\\alpha) + (0) g'(\\alpha)]^2}$$\n$$\\lim_{x\\to \\alpha} Q(x) = \\frac{g(\\alpha) [m(m-1)g(\\alpha)]}{[m g(\\alpha)]^2} = \\frac{m(m-1)g(\\alpha)^2}{m^2 g(\\alpha)^2}$$\n由于 $m \\ge 1$ 且 $g(\\alpha) \\ne 0$，我们可以简化此表达式。对于 $m=1$，分子为 $0$，因此极限为 $0$。对于 $m > 1$，我们可以消去 $m g(\\alpha)^2$。所得公式对所有 $m \\ge 1$ 都成立：\n$$\\lim_{x\\to \\alpha} \\frac{f(x) f''(x)}{\\left(f'(x)\\right)^2} = \\frac{m-1}{m} = 1 - \\frac{1}{m}$$\n\n### 第二部分：重数估计量\n\n推导出的极限建立了重数 $m$ 与比率 $Q(x)$ 的渐近值之间的关系。令 $L = \\lim_{x\\to \\alpha} Q(x)$。我们有 $L = 1 - \\frac{1}{m}$。我们可以解此方程求 $m$：\n$$\\frac{1}{m} = 1 - L$$\n$$m = \\frac{1}{1 - L}$$\n这启发我们通过将在一个接近但 不等于 $\\alpha$ 的点 $x$ 处计算的比率 $Q(x)$ 来替换极限 $L$，从而得到一个关于重数的逐点估计量 $\\hat{m}(x)$。\n$$\\hat{m}(x) = \\frac{1}{1 - Q(x)} = \\frac{1}{1 - \\frac{f(x) f''(x)}{(f'(x))^2}}$$\n简化 $\\hat{m}(x)$ 的表达式：\n$$\\hat{m}(x) = \\frac{(f'(x))^2}{(f'(x))^2 - f(x) f''(x)}$$\n这是一个用于重数估计的标准公式，它构成了修正牛顿法的基础，该方法能够为重根恢复二次收敛性。此估计量的理由是，当 $x \\to \\alpha$ 时，$\\hat{m}(x) \\to m$。\n\n### 第三部分：数值实验的基本原理\n\n该数值实验旨在量化此估计量在有限精度算术（IEEE 754 双精度）中的偏差。$\\hat{m}(x)$ 中的误差有两个来源：\n1.  **近似误差**：对于 $x \\ne \\alpha$，$\\hat{m}(x)$ 只是 $m$ 的一个近似值。通常，$x$ 离 $\\alpha$ 越远，这个误差越大。\n2.  **舍入误差**：对于非常接近 $\\alpha$ 的 $x$，分子 $(f'(x))^2$ 和分母 $(f'(x))^2 - f(x) f''(x)$ 都趋近于零。分母涉及两个几乎相等的数相减，因为它们的比率 $\\frac{f(x) f''(x)}{(f'(x))^2} \\to 1 - \\frac{1}{m}$。这种减法会导致灾难性抵消，这是浮点算术中相对精度的重大损失。\n\n该实验评估了一系列偏移量 $\\delta$ 下的 $\\hat{m}(\\alpha+\\delta)$。我们预计会观察到总误差曲线随 $\\log(\\delta)$ 变化呈 U 形：大的 $\\delta$ 会因近似误差导致高偏差，而非常小的 $\\delta$ 会因舍入误差导致高偏差，在某个中间值 $\\delta$ 处偏差最小。例外情况是 $m=1$，此时项 $f(x)f''(x)$ 比 $(f'(x))^2$更快地趋于零，因此灾难性抵消不是问题，偏差应该随 $\\delta$ 单调减小。\n\n下面的 Python 代码实现了这个实验，它通过解析方式计算导数，然后评估估计量，以计算每个测试用例的平均绝对偏差。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a numerical experiment to assess the bias of a multiplicity estimator\n    for roots of a function.\n    \"\"\"\n\n    # Define the set of offsets from the root\n    deltas = np.array([1e-1, 1e-4, 1e-8, 1e-12, 1e-16])\n\n    # Case A: m=3, alpha=0, g(x)=1+x\n    m_a, alpha_a = 3, 0.0\n    g_a = lambda x: 1.0 + x\n    gp_a = lambda x: 1.0\n    gpp_a = lambda x: 0.0\n\n    # Case B: m=2, alpha=0.3, g(x)=e^x\n    m_b, alpha_b = 2, 0.3\n    g_b = lambda x: np.exp(x)\n    gp_b = lambda x: np.exp(x)\n    gpp_b = lambda x: np.exp(x)\n\n    # Case C: m=1, alpha=-0.2, g(x)=1+x^2\n    m_c, alpha_c = 1, -0.2\n    g_c = lambda x: 1.0 + x**2\n    gp_c = lambda x: 2.0 * x\n    gpp_c = lambda x: 2.0\n\n    # Case D: m=5, alpha=0, g(x)=cos(x)\n    m_d, alpha_d = 5, 0.0\n    g_d = lambda x: np.cos(x)\n    gp_d = lambda x: -np.sin(x)\n    gpp_d = lambda x: -np.cos(x)\n\n    test_cases = [\n        (m_a, alpha_a, g_a, gp_a, gpp_a),\n        (m_b, alpha_b, g_b, gp_b, gpp_b),\n        (m_c, alpha_c, g_c, gp_c, gpp_c),\n        (m_d, alpha_d, g_d, gp_d, gpp_d),\n    ]\n\n    def calculate_m_hat(m, alpha, g, gp, gpp, x):\n        \"\"\"\n        Calculates the estimated multiplicity m_hat at a point x.\n        \"\"\"\n        xa = x - alpha\n        \n        gx = g(x)\n        gpx = gp(x)\n        gppx = gpp(x)\n\n        # f(x) = (x-alpha)^m * g(x)\n        f = (xa**m) * gx\n        \n        # f'(x) = (x-alpha)^(m-1) * [m*g(x) + (x-alpha)*g'(x)]\n        fp = (xa**(m-1)) * (m * gx + xa * gpx)\n        \n        # f''(x) is computed based on m to ensure numerical stability.\n        # For m=1, the general formula involves 0 * inf which can be NaN.\n        # f''(x) = (x-alpha)^(m-2) * [m(m-1)g + 2m(x-alpha)g' + (x-alpha)^2 g'']\n        if m == 1:\n            # Simplified formula for m=1: f''(x) = 2g'(x) + (x-alpha)g''(x)\n            fpp = 2.0 * gpx + xa * gppx\n        else:\n            fpp = (xa**(m - 2)) * (m * (m - 1) * gx + xa * (2 * m * gpx + xa * gppx))\n\n        # Estimator: m_hat = (f')^2 / ((f')^2 - f*f'')\n        numerator = fp**2\n        denominator = fp**2 - f * fpp\n        \n        if denominator == 0.0:\n            # Handle cases where the denominator is exactly zero.\n            return np.nan if numerator == 0.0 else np.inf\n\n        return numerator / denominator\n\n    all_mean_biases = []\n    for m, alpha, g, gp, gpp in test_cases:\n        biases = []\n        for delta in deltas:\n            x = alpha + delta\n            m_hat = calculate_m_hat(m, alpha, g, gp, gpp, x)\n            bias = np.abs(m_hat - m)\n            biases.append(bias)\n        \n        mean_abs_bias = np.mean(biases)\n        all_mean_biases.append(mean_abs_bias)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, all_mean_biases))}]\")\n\nsolve()\n```", "id": "3254009"}, {"introduction": "现在，我们已经掌握了所有关键模块：我们理解了标准牛顿法为何在重根上表现不佳([@problem_id:3254008])，也学会了如何估计根的重数([@problem_id:3254009])。是时候将这些知识融会贯通，构建一个真正“智能”的算法了。这个终极练习将挑战你设计一个混合自适应牛顿法，它能够自动诊断自身的收敛状态，并在检测到线性收敛时估算重数，从而切换到更高效的修正策略。", "problem": "设计并实现一个完整的、可运行的程序，该程序基于牛顿法为标量非线性方程构建一个混合求根算法。该算法必须以标准的牛顿迭代开始，并在运行过程中自主检测观察到的渐近收敛是否呈线性，根据观察到的迭代值估计根的重数 $m$，并且如果估计的重数表明存在重根，则切换到一种补偿重数的修正牛顿策略。您可以假定的基础理论包括：对于一个足够光滑的函数，根的重数的定义；对于单根，标准的牛顿迭代；以及光滑函数的泰勒展开的基本性质。您不能先验地假设任何专门的重数加速公式；相反，您必须从重根的基本模型以及牛顿迭代在此类根附近的行为中推导出所有必要的关系。\n\n您的程序必须遵循以下要求。\n\n- 输入在程序内部固定；不允许用户输入。您将实现该混合方法并将其应用于指定的测试套件。\n- 从基本定义开始：如果在一个足够光滑的函数 $f$ 的根 $r$ 附近，可以写出 $f(x) = (x - r)^m g(x)$，其中 $g$ 是光滑函数且 $g(r) \\neq 0$，则点 $r$ 是该函数的一个重数为 $m \\in \\mathbb{N}$ 的根。您也可以假设对于一个在根附近的可微函数 $f$，当 $f'(x_k) \\neq 0$ 时，标准的牛顿迭代为 $x_{k+1} = x_k - f(x_k)/f'(x_k)$。\n- 您的算法必须在运行期间判断当前迭代是否呈线性收敛，如果是，则根据不依赖于已知根 $r$ 的观测值动态地估计重数 $m$。在估计出 $m$ 后，算法必须从标准的牛顿迭代切换到适当的修正牛顿迭代，以恢复对重根的快速收敛。\n- 检测必须是鲁棒的：基于最近几次迭代的一个短窗口做出决策，并要求一个可观测比率的稳定性来判断线性收敛。避免将二次收敛误判为线性收敛。提供合理的数值保护措施。\n- 停止准则必须同时包括函数值容差和步长容差。使用函数值容差 $|f(x_k)| \\le 10^{-12}$，步长容差 $|x_{k+1} - x_k| \\le 10^{-14}$，以及最大迭代次数 $100$ 次。在适用的情况下，角度必须以弧度为单位。\n\n测试套件。将您的算法应用于以下每个测试用例；对于每个用例，都指定了函数 $f$、其导数 $f'$ 和初始猜测值 $x_0$。下面所有的常数和数字都是精确的，必须按给定值使用。\n\n- 用例 1：$f(x) = (x - 2)^3$，$f'(x) = 3(x - 2)^2$，$x_0 = 2.8$。\n- 用例 2：$f(x) = (x + 1)^2$，$f'(x) = 2(x + 1)$，$x_0 = -1.6$。\n- 用例 3：$f(x) = \\sin(x)$，$f'(x) = \\cos(x)$，$x_0 = 0.1$。角度以弧度为单位。\n- 用例 4：$f(x) = (x - 0.5)^4$，$f'(x) = 4(x - 0.5)^3$，$x_0 = 1.25$。\n- 用例 5：$f(x) = (x - 1)^3 e^{x}$，$f'(x) = e^{x}(x - 1)^2(x + 2)$，$x_0 = 1.3$。\n- 用例 6：$f(x) = x^3 - x - 2$，$f'(x) = 3x^2 - 1$，$x_0 = 1.0$。\n\n对于每个测试用例，您的程序必须返回一个包含恰好三个值的列表：\n- 最终估计的重数 $m$（一个非负整数，如果方法确定不需要切换，则使用 $m = 1$），\n- 一个整数指示符（如果算法至少切换到修正牛顿阶段一次，则为 $1$，否则为 $0$），\n- 以及直到收敛或达到最大迭代次数所执行的总迭代次数。\n\n最终输出格式。您的程序应生成单行输出，其中包含 6 个用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素本身是一个不含空格的三元素列表。例如，输出必须看起来像 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots,[a_6,b_6,c_6]]$，其中每个 $a_i$，$b_i$ 和 $c_i$ 都是如上所述的整数。", "solution": "我们从定义和局部模型开始。设 $f$ 是一个具有重数为 $m \\in \\mathbb{N}$ 的根 $r$ 的足够光滑的标量函数。根据定义，在 $r$ 的一个邻域内，存在一个光滑函数 $g$ 使得 $g(r) \\neq 0$ 且 $f(x) = (x - r)^m g(x)$。求解 $f(x) = 0$ 的标准牛顿迭代是 $x_{k+1} = x_k - f(x_k)/f'(x_k)$，假设 $f'(x_k) \\neq 0$。我们将误差表示为 $e_k = x_k - r$。\n\n重根附近的局部行为。使用 $f(x) = (x - r)^m g(x)$ 且 $g(r) \\neq 0$，我们有 $f'(x) = m(x - r)^{m-1} g(x) + (x - r)^m g'(x)$。在 $x = r$ 处，如果 $m \\ge 2$，则 $f'(r) = 0$，这解释了标准牛顿法为何从二次收敛退化为线性收敛。在 $r$ 附近的一阶近似使用 $g(x) \\approx g(r)$ 并得出\n$$\n\\frac{f(x)}{f'(x)} \\approx \\frac{(x - r)^m g(r)}{m (x - r)^{m-1} g(r)} = \\frac{x - r}{m}.\n$$\n代入牛顿更新公式可得\n$$\nx_{k+1} \\approx x_k - \\frac{e_k}{m} = r + \\left(1 - \\frac{1}{m}\\right) e_k.\n$$\n因此，标准牛顿法在重根处的渐近误差递推关系为\n$$\ne_{k+1} \\approx q e_k, \\quad q = \\frac{m - 1}{m},\n$$\n当 $m \\ge 2$ 时，这是速率为 $q \\in (0,1)$ 的线性收敛。对于 $m=1$ 的单根，一个更精确的泰勒展开显示 $e_{k+1} \\approx C e_k^2$，其中某个常数 $C \\neq 0$，即二次收敛。\n\n从观察到的迭代值估计重数。由于真实的根 $r$ 是未知的，误差 $e_k$ 无法直接获得。我们转而使用观察到的牛顿步长\n$$\ns_k = x_k - x_{k+1}.\n$$\n由 $e_{k+1} \\approx q e_k$ 我们有\n$$\ns_k = x_k - x_{k+1} = e_k - e_{k+1} \\approx (1 - q) e_k.\n$$\n类似地，$s_{k+1} \\approx (1 - q) e_{k+1} \\approx (1 - q) q e_k$。因此，连续步长大小的比值会稳定到\n$$\n\\frac{|s_{k+1}|}{|s_k|} \\approx q = \\frac{m - 1}{m}.\n$$\n这为检测线性收敛提供了一个可观测的判据：如果连续步长大小的比值稳定在 $(0,1)$ 内一个远离零的值，则迭代行为是线性的而非二次的。此外，我们可以通过在最近比率的一个短窗口上使用一个稳定的统计量（例如中位数）来估计 $q$，然后推断出\n$$\nm \\approx \\frac{1}{1 - q}.\n$$\n四舍五入到最接近的 $\\ge 2$ 的整数，得到一个整数重数估计值 $\\widehat{m}$，我们可以在修正的迭代中使用它。\n\n修正牛顿迭代。基本局部模型 $f(x) = (x - r)^m g(x)$ 且 $g(r) \\neq 0$ 表明，可以通过缩放牛顿修正项来补偿重根。考虑修正后的更新公式\n$$\nx_{k+1} = x_k - \\widehat{m} \\frac{f(x_k)}{f'(x_k)}.\n$$\n代入 $f/f'$ 的局部近似值可得\n$$\nx_{k+1} \\approx x_k - \\widehat{m} \\frac{e_k}{m} = r + \\left(1 - \\frac{\\widehat{m}}{m}\\right) e_k.\n$$\n如果 $\\widehat{m} = m$，一阶项消失，泰勒展开中的下一项决定了更高阶（通常是二次）的收敛性。因此，一旦获得一个可靠的估计值 $\\widehat{m}$，切换到修正的更新公式就能恢复快速收敛。\n\n检测逻辑和保护措施。为避免将二次收敛（单根）误判为线性收敛，我们施加以下要求：\n- 稳定性要求：在最近比率 $r_i = |s_{i+1}|/|s_i|$ 的一个窗口内，它们的相对变化必须很小；\n- 大小要求：稳定后的比率必须明显远离零，例如，$r_i \\in [\\rho_{\\min}, \\rho_{\\max}]$ 且 $\\rho_{\\min} > 0$；\n- 一致性要求：窗口内没有强烈的下降趋势，因为二次收敛会使比率趋向于零。\n\n有了这些措施，我们将窗口化比率的中位数计算为 $\\widehat{q}$，并设置 $\\widehat{m} = \\max\\{2, \\min\\{M_{\\max}, \\mathrm{round}(1/(1 - \\widehat{q}))\\}\\}$，其中 $M_{\\max}$ 是一个合理的上限以保证鲁棒性。每次运行我们只切换一次，并且如果标准未满足则不进行切换。\n\n停止准则。如果 $|f(x_k)| \\le 10^{-12}$ 或 $|x_{k+1} - x_k| \\le 10^{-14}$，或者如果 $k$ 达到 $100$，我们就停止。我们还通过在 $|f'(x_k)|$ 过小时优雅地中止来防范导数接近于零的情况。\n\n算法摘要。\n- 使用标准牛顿更新公式 $x_{k+1} = x_k - f(x_k)/f'(x_k)$ 初始化。\n- 维护最近几次步长比率 $|s_{k+1}|/|s_k|$ 的一个滑动窗口。\n- 如果比率在容差范围内稳定并且位于指定区间内，则宣布为线性收敛并估计 $\\widehat{m} \\approx 1/(1 - \\widehat{q})$。\n- 如果 $\\widehat{m} \\ge 2$，则切换到修正的更新公式 $x_{k+1} = x_k - \\widehat{m} f(x_k)/f'(x_k)$ 并继续迭代直到收敛。\n- 对于每个测试用例，输出最终的 $\\widehat{m}$（如果不切换则为 $1$）、一个表示是否发生切换的指示符以及总迭代次数。\n\n应用于测试套件。这些用例包括已知重数的重根：\n- $(x - 2)^3$，重数为 $m = 3$，\n- $(x + 1)^2$，重数为 $m = 2$，\n- $(x - 0.5)^4$，重数为 $m = 4$，\n- $(x - 1)^3 e^{x}$，重数为 $m = 3$，\n以及两个单根用例：\n- $\\sin(x)$ 在 $x=0$ 处，\n- $x^3 - x - 2$ 在 $x \\approx 1.521\\dots$ 附近的唯一实根处。\n对于重根，步长比率稳定到 $q = (m - 1)/m$，算法会相应地估计出 $\\widehat{m}$，进行切换，并实现快速收敛。对于单根，步长比率会趋向于零而不会在远离零的位置稳定下来，因此算法不会切换，且 $\\widehat{m} = 1$。最终程序为每个用例计算所要求的三元组，并按照指定格式将其打印为单个带方括号的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hybrid_newton(f, df, x0, tol_f=1e-12, tol_x=1e-14, max_iter=100):\n    \"\"\"\n    Hybrid Newton solver:\n    - Starts with standard Newton.\n    - Detects linear convergence via stabilization of step ratios.\n    - Estimates multiplicity m_hat = round(1/(1 - q_hat)) from ratios and switches to modified Newton.\n    Returns: (m_hat_final, switched_flag, iterations)\n    \"\"\"\n    x = float(x0)\n    switched = False\n    m_hat = 1  # default multiplicity estimate\n    step_history = []\n    ratio_window = []\n    window_size = 4\n\n    # thresholds for detection of linear convergence\n    rho_min = 0.15   # away from zero to avoid misclassifying quadratic convergence\n    rho_max = 0.98   # bounded away from 1 to avoid extremely slow cases\n    stab_rel_tol = 0.03  # max relative spread across the window\n    deriv_eps = 1e-16\n\n    it = 0\n    # We'll run at most max_iter iterations\n    while it  max_iter:\n        fx = f(x)\n        dfx = df(x)\n        if abs(fx) = tol_f:\n            break\n        if abs(dfx)  deriv_eps:\n            # Derivative too small, abort iteration\n            break\n\n        # Choose the update depending on switch state\n        if not switched:\n            delta = -fx / dfx\n        else:\n            delta = -m_hat * fx / dfx\n\n        x_new = x + delta\n        it += 1\n\n        # Check step tolerance\n        if abs(delta) = tol_x:\n            x = x_new\n            break\n\n        # Collect step and ratio info only during standard Newton before switching\n        if not switched:\n            step_history.append(abs(delta))\n            # compute ratio if possible\n            if len(step_history) >= 2:\n                r = step_history[-1] / step_history[-2] if step_history[-2] != 0.0 else np.inf\n                ratio_window.append(r)\n                if len(ratio_window) > window_size:\n                    ratio_window.pop(0)\n\n                # Detection: require full window\n                if len(ratio_window) == window_size:\n                    # Check bounds and stability\n                    ratios = np.array(ratio_window)\n                    # Ensure all ratios in [rho_min, rho_max]\n                    in_range = np.all((ratios >= rho_min)  (ratios = rho_max)  np.isfinite(ratios))\n                    # Stability via relative spread: (max - min)/max = tol\n                    rel_spread = (ratios.max() - ratios.min()) / max(ratios.max(), 1e-300)\n                    stable = rel_spread = stab_rel_tol\n                    # Optional: ensure no strong downward trend\n                    non_decreasingish = ratios[-1] >= 0.9 * ratios[0]\n                    if in_range and stable and non_decreasingish:\n                        q_hat = float(np.median(ratios))\n                        if q_hat  1.0:\n                            m_est = 1.0 / (1.0 - q_hat)\n                            # Round to nearest integer and clamp to [2, 20]\n                            m_hat_candidate = int(max(2, min(20, int(np.floor(m_est + 0.5)))))\n                            # Switch only if candidate >= 2\n                            if m_hat_candidate >= 2:\n                                m_hat = m_hat_candidate\n                                switched = True\n                                # Clear windows to avoid re-triggering\n                                ratio_window.clear()\n                                step_history.clear()\n\n        # Advance iteration\n        x = x_new\n        \n\n    # Ensure m_hat reported as 1 if no switch occurred\n    if not switched:\n        m_hat_report = 1\n    else:\n        m_hat_report = int(m_hat)\n\n    return (int(m_hat_report), int(1 if switched else 0), int(it))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (f, df, x0)\n    def f1(x): return (x - 2.0) ** 3\n    def df1(x): return 3.0 * (x - 2.0) ** 2\n\n    def f2(x): return (x + 1.0) ** 2\n    def df2(x): return 2.0 * (x + 1.0)\n\n    def f3(x): return np.sin(x)\n    def df3(x): return np.cos(x)\n\n    def f4(x): return (x - 0.5) ** 4\n    def df4(x): return 4.0 * (x - 0.5) ** 3\n\n    def f5(x): return ((x - 1.0) ** 3) * np.exp(x)\n    def df5(x): return np.exp(x) * (x - 1.0) ** 2 * (x + 2.0)\n\n    def f6(x): return x ** 3 - x - 2.0\n    def df6(x): return 3.0 * x ** 2 - 1.0\n\n    test_cases = [\n        (f1, df1, 2.8),\n        (f2, df2, -1.6),\n        (f3, df3, 0.1),\n        (f4, df4, 1.25),\n        (f5, df5, 1.3),\n        (f6, df6, 1.0),\n    ]\n\n    results = []\n    for f, df, x0 in test_cases:\n        m_hat, switched, iters = hybrid_newton(f, df, x0, tol_f=1e-12, tol_x=1e-14, max_iter=100)\n        results.append([m_hat, switched, iters])\n\n    # Format without spaces as required: [[a,b,c],[...],...]\n    inner = \",\".join(\"[\" + \",\".join(str(v) for v in triple) + \"]\" for triple in results)\n    print(f\"[{inner}]\")\n\nsolve()\n```", "id": "3253971"}]}