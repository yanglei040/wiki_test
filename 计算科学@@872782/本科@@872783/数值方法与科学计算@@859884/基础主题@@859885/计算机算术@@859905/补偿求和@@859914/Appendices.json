{"hands_on_practices": [{"introduction": "在编写代码之前，通过一个思想实验来直观地理解补偿求和的必要性至关重要。这个练习 [@problem_id:3214532] 使用了一个包含极大和极小数值的简单序列，揭示了在浮点数运算中“大数吃小数”（淹没）的现象如何导致朴素求和方法得出完全错误的结论。通过分析不同求和顺序以及 Kahan 补偿求和算法下的结果，我们可以清晰地看到标准算法的陷阱以及为何需要更精密的技巧。", "problem": "考虑在电气和电子工程师协会 (IEEE) 754 浮点数算术标准 (IEEE 754) 的 binary64 格式中，使用“舍入到最近，偶数优先”的规则对三个实数求和。设序列为 $a_{1} = 1$、$a_{2} = 10^{100}$ 和 $a_{3} = -10^{100}$。在实数算术中，精确和为 $a_{1} + a_{2} + a_{3}$。在浮点运算中，定义朴素求和为通过递推式 $s \\leftarrow \\operatorname{fl}(s + a_{k})$ 从左到右累加一个运行总和 $s$，其中 $\\operatorname{fl}(\\cdot)$ 表示在 binary64 格式下的一次正确舍入的浮点运算。定义 Kahan 补偿求和为一个变体，它维护一个补偿变量 $c$ 并根据以下方式更新：$y \\leftarrow a_{k} - c$，$t \\leftarrow \\operatorname{fl}(s + y)$，$c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$，$s \\leftarrow t$。\n\n仅使用 binary64 中浮点数舍入到最近的基本定义和最后一位单位的概念，分析舍入对朴素累加法在 $(a_{1}, a_{2}, a_{3})$ 和 $(a_{2}, a_{3}, a_{1})$ 顺序下的部分和的影响，以及对 Kahan 补偿求和在 $(a_{1}, a_{2}, a_{3})$ 顺序下的影响。然后，确定 Kahan 补偿求和对于顺序 $(a_{1}, a_{2}, a_{3})$ 返回的最终值。报告的答案无需舍入，且不适用任何物理单位。请以单个实数的形式提供您的答案。", "solution": "用户希望在使用 IEEE $754$ binary64 算术的 Kahan 补偿求和算法下，求出对三个数序列 $a_{1} = 1$、$a_{2} = 10^{100}$ 和 $a_{3} = -10^{100}$ 求和的结果。\n\n**问题验证**\n\n**步骤 1：提取已知条件**\n- **数字：** $a_{1} = 1$、$a_{2} = 10^{100}$ 和 $a_{3} = -10^{100}$。\n- **算术：** IEEE $754$ binary64 格式，舍入到最近，偶数优先。\n- **浮点运算：** $\\operatorname{fl}(\\cdot)$ 表示一次正确舍入的浮点运算。\n- **朴素求和：** $s \\leftarrow \\operatorname{fl}(s + a_{k})$，其中 $s$ 初始化为 $0$。\n- **Kahan 补偿求和：** 初始化 $s=0, c=0$。对于每个 $a_k$：\n  $y \\leftarrow a_{k} - c$\n  $t \\leftarrow \\operatorname{fl}(s + y)$\n  $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$\n  $s \\leftarrow t$\n  问题描述中对 $y$ 的更新没有明确使用 $\\operatorname{fl}(\\cdot)$，但在实现数值算法的上下文中，此赋值意味着一次浮点运算。因此，我们将其解释为 $y \\leftarrow \\operatorname{fl}(a_k - c)$。\n- **任务：** 分析朴素求和在 $(a_1, a_2, a_3)$ 和 $(a_2, a_3, a_1)$ 顺序下的情况，并确定 Kahan 求和在 $(a_1, a_2, a_3)$ 顺序下的最终值。精确和为 $1 + 10^{100} + (-10^{100}) = 1$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题牢固地植根于数值分析以及由 IEEE $754$ 标准定义的浮点运算细节。这是一个用于说明求和算法属性的经典示例。\n- **适定性：** 这个问题是适定的。输入、算术规则和算法都已指定，从而导出一个唯一的、确定性的结果。\n- **客观性：** 该问题以精确、正式的语言陈述，没有歧义或主观看法。\n\n该问题被认为是有效的，因为它符合所有标准。这是科学计算领域内一个严谨而正式的问题。\n\n**解题过程**\n\n首先，让我们确定输入数字在 binary64 格式下的浮点表示。binary64 格式具有 $53$ 位的尾数精度（包括隐含的前导位）。\n- $a_1 = 1$：这个数在 binary64 中可以精确表示为 $1.0 \\times 2^0$。我们将其表示记为 $\\hat{a}_1 = 1$。\n- $a_2 = 10^{100}$：这个数无法精确表示。它必须舍入到最接近的 binary64 数。我们将其表示记为 $\\hat{a}_2 = \\operatorname{fl}(10^{100})$。$\\hat{a}_2$ 的值非常大。我们可以估计其数量级：$\\log_2(10^{100}) = 100 \\log_2(10) \\approx 100 \\times 3.3219 \\approx 332.19$。因此，$\\hat{a}_2$ 大约是 $m \\times 2^{332}$，其中尾数 $m \\in [1, 2)$。\n- $a_3 = -10^{100}$：其表示为 $\\hat{a}_3 = \\operatorname{fl}(-10^{100})$。由于舍入到最近的对称性，$\\hat{a}_3 = -\\operatorname{fl}(10^{100}) = -\\hat{a}_2$。\n\n这个问题中的一个关键概念是 **淹没 (swamping)**，它发生在对两个数量级差异巨大的浮点数进行加减法时。如果 $|x| \\gg |y|$，则 $\\operatorname{fl}(x+y)$ 的结果可能等于 $x$，实际上“淹没”了 $y$。这种情况发生在 $y$ 小于 $x$ 周围可表示数之间的间距时。对于 $x = m \\times 2^E$（$1\\le m  2$），最后一位的单位 (unit in the last place) $\\mathrm{ulp}(x)$ 是 $2^{E-52}$。对于 $\\hat{a}_2 \\approx 2^{332}$，ulp 大约是 $2^{332-52} = 2^{280}$，这是一个远大于 $1$ 的数。因此，将 $1$ 加到 $\\hat{a}_2$ 上对其浮点表示没有影响。\n\n**朴素求和分析**\n- **顺序 $(a_1, a_2, a_3)$：**\n  1. $s_0 = 0$。\n  2. $s_1 = \\operatorname{fl}(s_0 + a_1) = \\operatorname{fl}(0 + 1) = 1$。\n  3. $s_2 = \\operatorname{fl}(s_1 + a_2) = \\operatorname{fl}(1 + 10^{100}) = \\operatorname{fl}(1 + \\hat{a}_2) = \\hat{a}_2$（由于淹没）。\n  4. $s_3 = \\operatorname{fl}(s_2 + a_3) = \\operatorname{fl}(\\hat{a}_2 + (-\\hat{a}_2)) = 0$。\n  结果是 $0$。\n\n- **顺序 $(a_2, a_3, a_1)$：**\n  1. $s_0 = 0$。\n  2. $s_1 = \\operatorname{fl}(s_0 + a_2) = \\operatorname{fl}(0 + 10^{100}) = \\hat{a}_2$。\n  3. $s_2 = \\operatorname{fl}(s_1 + a_3) = \\operatorname{fl}(\\hat{a}_2 + (-\\hat{a}_2)) = 0$。\n  4. $s_3 = \\operatorname{fl}(s_2 + a_1) = \\operatorname{fl}(0 + 1) = 1$。\n  结果是 $1$，这是正确的和。这说明了求和顺序的重要性。\n\n**对顺序 $(a_1, a_2, a_3)$ 的 Kahan 补偿求和分析**\n我们将跟踪算法每一步的状态变量 $s$（和）和 $c$（补偿）。\n\n**初始化：**\n- $s = 0$\n- $c = 0$\n\n**步骤 1：加上 $a_1 = 1$**\n- $y \\leftarrow \\operatorname{fl}(a_1 - c) = \\operatorname{fl}(1 - 0) = 1$。\n- $t \\leftarrow \\operatorname{fl}(s + y) = \\operatorname{fl}(0 + 1) = 1$。\n- $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y) = \\operatorname{fl}(\\operatorname{fl}(1 - 0) - 1) = \\operatorname{fl}(1 - 1) = 0$。\n- $s \\leftarrow t = 1$。\n**步骤 1 后状态：** $s = 1$，$c = 0$。和是正确的，不需要补偿。\n\n**步骤 2：加上 $a_2 = 10^{100}$**\n- $y \\leftarrow \\operatorname{fl}(a_2 - c) = \\operatorname{fl}(10^{100} - 0) = \\hat{a}_2$。\n- $t \\leftarrow \\operatorname{fl}(s + y) = \\operatorname{fl}(1 + \\hat{a}_2)$。如前所述，$\\hat{a}_2$ 的数量级非常大，以至于加 $1$ 的操作会因淹没而丢失。结果被舍入为 $\\hat{a}_2$。所以，$t = \\hat{a}_2$。\n- $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$。我们有 $t=\\hat{a}_2$，$s=1$ 和 $y=\\hat{a}_2$。\n  - 内层运算：$\\operatorname{fl}(t - s) = \\operatorname{fl}(\\hat{a}_2 - 1)$。同样，由于淹没，减去 $1$ 对浮点结果没有影响。所以，$\\operatorname{fl}(\\hat{a}_2 - 1) = \\hat{a}_2$。\n  - 外层运算：$c \\leftarrow \\operatorname{fl}(\\hat{a}_2 - y) = \\operatorname{fl}(\\hat{a}_2 - \\hat{a}_2) = 0$。\n- $s \\leftarrow t = \\hat{a}_2$。\n**步骤 2 后状态：** $s = \\hat{a}_2$，$c = 0$。Kahan 算法未能将丢失的项 $1$ 捕获到补偿变量 $c$ 中。这个失败发生的原因是，正确计算误差项所需的条件 $|s| \\ge |y|$ 未被满足。\n\n**步骤 3：加上 $a_3 = -10^{100}$**\n- $y \\leftarrow \\operatorname{fl}(a_3 - c) = \\operatorname{fl}(-10^{100} - 0) = \\hat{a}_3 = -\\hat{a}_2$。\n- $t \\leftarrow \\operatorname{fl}(s + y) = \\operatorname{fl}(\\hat{a}_2 + (-\\hat{a}_2)) = 0$。这次减法是精确的，因为浮点数有精确的相反数。\n- $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y) = \\operatorname{fl}(\\operatorname{fl}(0 - \\hat{a}_2) - (-\\hat{a}_2))$。\n  - 内层运算：$\\operatorname{fl}(0 - \\hat{a}_2) = -\\hat{a}_2$（精确）。\n  - 外层运算：$c \\leftarrow \\operatorname{fl}((-\\hat{a}_2) - (-\\hat{a}_2)) = \\operatorname{fl}(-\\hat{a}_2 + \\hat{a}_2) = 0$。\n- $s \\leftarrow t = 0$。\n**最终状态：** $s = 0$，$c = 0$。\n\nKahan 求和算法返回的最终值，也就是 $s$ 的最终值，是 $0$。这个例子展示了一种情况，即当运行总和远小于要相加的项时，由于发生淹没，标准的 Kahan 算法未能比朴素求和有所改进。", "answer": "$$\\boxed{0}$$", "id": "3214532"}, {"introduction": "理论知识需要通过实践来巩固。这个练习 [@problem_id:3214610] 将带你从理论走向实践，通过编程实现朴素求和与补偿求和两种方法来计算著名的交错调和级数。通过将计算结果与理论真值 $\\ln(2)$ 进行比较，你将亲眼见证补偿求和如何显著减少累积的舍入误差，并探索求和顺序（正向与逆向）这一简单启发式方法对提升计算精度的影响。", "problem": "本题要求您研究一个条件收敛级数的数值求和，并量化补偿对浮点舍入误差的影响。考虑由下式定义的交错调和级数\n$$\nS = \\sum_{k=1}^{\\infty} (-1)^{k+1}\\frac{1}{k}.\n$$\n一个经过充分检验的事实是，自然对数的泰勒级数满足\n$$\n\\ln(1+x) = \\sum_{k=1}^{\\infty} (-1)^{k+1}\\frac{x^k}{k}\n$$\n对于 $|x| \\leq 1$ 且 $x \\neq -1$ 成立，这意味着\n$$\n\\sum_{k=1}^{\\infty} (-1)^{k+1}\\frac{1}{k} = \\ln(2).\n$$\n交错级数审敛法进一步保证了对于部分和\n$$\nS_N = \\sum_{k=1}^{N} (-1)^{k+1}\\frac{1}{k},\n$$\n其截断误差 $|S - S_N|$ 的上界为下一项的绝对值：\n$$\n|S - S_N| \\leq \\frac{1}{N+1}.\n$$\n\n从数值计算的角度来看，设浮点运算遵循电气与电子工程师协会（IEEE）$754$ 双精度标准下的标准舍入模型：对于加法和减法，\n$$\n\\operatorname{fl}(a \\oplus b) = (a \\oplus b)(1 + \\delta), \\quad |\\delta| \\leq \\varepsilon_{\\text{mach}},\n$$\n其中 $\\varepsilon_{\\text{mach}}$ 是机器ε，$\\operatorname{fl}(\\cdot)$ 表示浮点运算结果。在长序列求和中，当数量级差异巨大或符号相反的数字相加时，许多低位比特会丢失，导致累积的舍入误差。一种原则性的缓解方法是补偿求和，它为每一步因舍入而丢失的低位部分维护一个动态补偿，并将其反馈到下一次加法中。\n\n您的任务是：\n\n- 为计算 $S_N$ 实现两种数值求和程序：\n  1. 一种朴素前向求和，按指定顺序累加各项，不进行补偿。\n  2. 一种补偿求和，使用一个单项补偿变量，在每次加法时更新它，以捕获并重新引入因舍入而丢失的低位部分。\n\n- 支持两种累加顺序：\n  - 前向顺序：迭代 $k = 1,2,\\dots,N$。\n  - 反向顺序：迭代 $k = N,N-1,\\dots,1$。\n\n- 对每个部分和，计算：\n  1. 绝对误差 $E_{\\text{naive}} = |\\operatorname{fl}(S_N^{\\text{naive}}) - \\ln(2)|$，\n  2. 绝对误差 $E_{\\text{comp}} = |\\operatorname{fl}(S_N^{\\text{comp}}) - \\ln(2)|$，\n  3. 归一化误差 $E_{\\text{naive}} / \\left(\\frac{1}{N+1}\\right)$，\n  4. 归一化误差 $E_{\\text{comp}} / \\left(\\frac{1}{N+1}\\right)$，\n  5. 一个布尔标志，指示补偿是否提高了精度，即 $E_{\\text{comp}} \\leq E_{\\text{naive}}$。\n\n设计并实现您的程序，使其使用双精度（$\\text{IEEE } 754$ 双精度）算术（Python 的内置 $float$ 类型），并为以下测试套件生成结果：\n- 情况 $1$：$N = 1$，前向顺序。\n- 情况 $2$：$N = 10$，前向顺序。\n- 情况 $3$：$N = 1000$，前向顺序。\n- 情况 $4$：$N = 200000$，前向顺序。\n- 情况 $5$：$N = 200000$，反向顺序。\n\n该测试套件涵盖：\n- 一个边界情况（$N = 1$），只对少数几项求和。\n- 一个小规模情况（$N = 10$），展示早期的收敛行为。\n- 一个中等规模情况（$N = 1000$），此时截断误差很小，但舍入误差开始变得重要。\n- 一个大规模情况（$N = 200000$），此时舍入误差可能累积。\n- 大 $N$ 值时对求和顺序的敏感性。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按以下顺序精确输出五项内容：\n$$\n[E_{\\text{naive}}, E_{\\text{comp}}, \\; E_{\\text{naive}}/(1/(N+1)), \\; E_{\\text{comp}}/(1/(N+1)), \\; (E_{\\text{comp}} \\leq E_{\\text{naive}})]\n$$\n将所有五个测试用例的结果连接成一个单一的扁平列表。例如，输出格式必须为\n$$\n[\\text{case1\\_naive\\_err},\\text{case1\\_comp\\_err},\\text{case1\\_naive\\_norm},\\text{case1\\_comp\\_norm},\\text{case1\\_improved},\\dots,\\text{case5\\_improved}]\n$$\n不得包含多余的字符或换行。本问题不涉及物理单位或角度，因此无需进行单位转换。所有数值均应以标准十进制表示法打印。", "solution": "该问题要求对补偿求和在计算交错调和级数部分和 $S_N = \\sum_{k=1}^{N} (-1)^{k+1}\\frac{1}{k}$ 时的精度影响进行数值研究。已知该无穷级数的精确值为 $S = \\ln(2)$。$S_N$ 的数值计算总误差为 $|\\operatorname{fl}(S_N) - S|$，它是两个分量的和：截断误差 $|S_N - S|$ 和舍入误差 $|\\operatorname{fl}(S_N) - S_N|$。问题给出了截断误差的上界 $|S - S_N| \\leq \\frac{1}{N+1}$。我们的目标是实现并比较朴素求和与补偿求和两种算法，并在前向和反向求和顺序下评估它们的性能，从而量化舍入误差分量的影响。所有计算均使用 IEEE $754$ 双精度浮点运算进行。\n\n朴素求和算法通过按给定顺序依次相加各项来计算部分和 $S_N$。对于一个项序列 $\\{x_k\\}_{k=1}^N$，其和 $S_N = \\sum_{k=1}^N x_k$ 通过递推关系 $s_k = s_{k-1} + x_k$（其中 $s_0=0$）计算。在浮点运算中，这变为 $\\operatorname{fl}(s_k) = \\operatorname{fl}(\\operatorname{fl}(s_{k-1}) + x_k)$。根据标准模型，$\\operatorname{fl}(a+b) = (a+b)(1+\\delta)$，其中 $|\\delta| \\leq \\varepsilon_{\\text{mach}}$。每一步都会引入一个小的舍入误差。当将一个项 $x_k$ 加到一个累加和 $s_{k-1}$ 上时，如果 $|s_{k-1}| \\gg |x_k|$，那么 $x_k$ 的低位比特会因移出尾数而丢失。这种精度损失在多次运算后会累积，导致最终结果中可能出现显著的舍入误差。\n\n为缓解这种舍入误差的累积，我们采用补偿求和，具体来说是 Kahan 求和算法。该方法引入一个补偿变量 $c$，用于追踪每次加法中引入的误差。设 $s$ 为累加和，$c$ 为补偿，两者均初始化为 $0$。为了加上一个新项 $x$，算法按以下步骤进行：\n$1$. 通过从新项中减去前一次的误差来计算修正项 $y$：$y = x - c$。\n$2$. 将修正项加到和上：$t = s + y$。由于精度有限，此操作可能不精确。\n$3. 上一步引入的新误差计算为 $c_{\\text{new}} = (t - s) - y$。在精确算术中，这个值应为零。然而，在浮点运算中，$(t - s)$ 恢复了已成功加到 $s$ 上的 $y$ 的高位部分，因此从中减去 $y$ 就会得到被丢失的 $y$ 的低位部分的相反数。\n$4$. 将和更新为 $s = t$，并将新的补偿项 $c$ 设置为 $c_{\\text{new}}$ 用于下一次迭代。\n这个过程有效地将丢失的精度从一步“携带”到下一步，并将其重新引入求和中，从而显著减少最终的累积舍入误差。\n\n求和的顺序也会影响朴素方法的精度。对于项的绝对值递减的级数，例如交错调和级数，有两种自然的顺序：\n- **前向顺序**：从 $k=1$ 到 $N$ 求和。项 $x_k = (-1)^{k+1}/k$ 的绝对值递减。累加和 $s_k$ 迅速接近其最终值，而要加的项则越来越小。这导致了典型的 $|s_k| \\gg |x_{k+1}|$ 情况，从而引起显著的舍入误差。\n- **反向顺序**：从 $k=N$ 到 $1$ 求和。此策略首先将绝对值最小的项相加。累加和在初始阶段保持较小，这使得后续小项的加法能以更高的相对精度执行。这种启发式方法通常能提高朴素求和的精度。\n\n对于指定的测试用例，我们预计会有以下行为。对于较小的 $N$（例如 $N=1, 10$），截断误差占主导地位，不同求和方法之间的差异可以忽略不计。对于较大的 $N$（例如 $N=200000$），舍入误差变得显著。我们预计朴素前向求和的误差 $E_{\\text{naive}}$ 将远大于截断误差界。相比之下，补偿求和的误差 $E_{\\text{comp}}$ 应保持接近截断误差界，从而证明该算法的有效性。我们还预计朴素反向求和比朴素前向求和更精确。归一化误差 $E / (\\frac{1}{N+1})$ 提供了一个度量，衡量总误差超出理论截断误差界的程度；一个接近 $1$ 的值表明舍入误差得到了很好的控制。\n\n分析过程是通过实现这些算法并将其应用于给定的测试套件来进行的。对于每种情况，我们计算相对于 $\\ln(2)$ 的高精度值的绝对误差 $E_{\\text{naive}}$ 和 $E_{\\text{comp}}$、归一化误差，以及一个指示补偿是否改善了结果的布尔标志。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef naive_sum(N, order):\n    \"\"\"\n    Computes the partial sum of the alternating harmonic series using naive summation.\n\n    Args:\n        N (int): The number of terms to sum.\n        order (str): The order of summation, 'forward' or 'reverse'.\n\n    Returns:\n        float: The computed sum.\n    \"\"\"\n    s = 0.0\n    k_range = range(1, N + 1) if order == 'forward' else range(N, 0, -1)\n    \n    for k in k_range:\n        term = ((-1.0)**(k + 1)) / k\n        s += term\n        \n    return s\n\ndef compensated_sum(N, order):\n    \"\"\"\n    Computes the partial sum of the alternating harmonic series using\n    Kahan compensated summation.\n\n    Args:\n        N (int): The number of terms to sum.\n        order (str): The order of summation, 'forward' or 'reverse'.\n\n    Returns:\n        float: The computed sum.\n    \"\"\"\n    s = 0.0  # The running sum\n    c = 0.0  # A running compensation for the lost low-order bits\n    k_range = range(1, N + 1) if order == 'forward' else range(N, 0, -1)\n    \n    for k in k_range:\n        term = ((-1.0)**(k + 1)) / k\n        y = term - c\n        t = s + y\n        # (t - s) is the high-order part of y that was successfully added.\n        # (t - s) - y isolates the low-order part of y that was lost.\n        # c stores the negative of this lost part.\n        c = (t - s) - y\n        s = t\n        \n    return s\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 'forward'),\n        (10, 'forward'),\n        (1000, 'forward'),\n        (200000, 'forward'),\n        (200000, 'reverse'),\n    ]\n\n    # Use numpy's high-precision value for ln(2) as the \"exact\" value.\n    ln2 = np.log(2.0)\n\n    # List to store all results in a flat structure.\n    all_results = []\n\n    for N, order in test_cases:\n        # 1. Compute sums\n        s_naive = naive_sum(N, order)\n        s_comp = compensated_sum(N, order)\n\n        # 2. Compute absolute errors\n        err_naive = abs(s_naive - ln2)\n        err_comp = abs(s_comp - ln2)\n\n        # 3. Compute truncation error bound\n        trunc_err_bound = 1.0 / (N + 1)\n\n        # 4. Compute normalized errors\n        norm_err_naive = err_naive / trunc_err_bound\n        norm_err_comp = err_comp / trunc_err_bound\n\n        # 5. Determine if compensation improved accuracy\n        improved = err_comp = err_naive\n\n        # Append the five required metrics for this test case\n        all_results.extend([\n            err_naive,\n            err_comp,\n            norm_err_naive,\n            norm_err_comp,\n            improved\n        ])\n\n    # Final print statement in the exact required format.\n    # The boolean `True`/`False` is converted to string 'True'/'False'.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the solver.\nsolve()\n```", "id": "3214610"}, {"introduction": "补偿求和不仅仅是一个独立的算法，更是一种可以嵌入到其他数值方法中以增强其稳定性的强大技术。这个练习 [@problem_id:3214603] 将这一概念应用于一个核心的统计量——方差的计算中。通过对比传统的双遍法、Welford 单遍法以及集成了补偿求和的改进 Welford 算法，你将理解如何在更复杂的实际问题中应用补偿求和来获得更可靠的科学计算结果。", "problem": "给定一个有限的实数序列，要求您使用三种方法在浮点运算中计算总体方差：一种两遍扫描法 (two-pass method)，一种通常归功于 Welford 的单遍在线更新法，以及一种改进的单遍更新法，该方法在二阶中心矩的累加中集成了补偿求和。目标是通过量化相对于由精确浮点输入计算出的高精度参考值的绝对误差，来评估和比较这些方法的数值稳定性。总体方差从第一性原理定义如下：对于一个有限序列 $\\{x_i\\}_{i=1}^n$，其均值为 $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$，总体方差为 $\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2$。\n\n请将您的推理建立在以下基本事实之上：\n- 由于实数表示的有限精度模型，浮点运算中的 $n$ 项求和可能会产生舍入误差。在广泛使用的电气和电子工程师协会 (Institute of Electrical and Electronics Engineers, IEEE) 754 浮点运算标准中，加法和乘法等基本运算的结果会被舍入到最接近的可表示值。\n- 总体方差 $\\sigma^2$ 的定义需要均值 $\\mu$ 和平方偏差和 $\\sum_{i=1}^n (x_i - \\mu)^2$。当 $x_i$ 和 $\\mu$ 的量级相近时，在减法 $x_i - \\mu$ 中可能会发生数值相消 (numerical cancellation)。\n\n在单个程序中实现以下内容：\n- 一种两遍扫描的总体方差计算方法，该方法首先通过直接的从左到右求和来计算均值，然后在第二遍中通过直接的从左到右求和来计算平方偏差和。在两遍扫描中，均使用机器的标准浮点运算，不采用任何特殊的稳定化技术。\n- 一种单遍在线的总体方差计算方法，该方法使用归功于 Welford 的经典递推公式，在每个新样本上更新运行均值和二阶中心矩的运行累加器，不采用任何特殊的稳定化技术。\n- 一种单遍在线的总体方差计算方法，该方法使用与 Welford 更新相同的结构，但在二阶中心矩贡献的累加中集成了补偿求和（一个校正项），以减少运行总和中的舍入误差。\n\n对于所有三种方法，计算相对于高精度参考方差的绝对误差。该参考方差通过评估与二进制浮点表示完全相同的输入来获得，但在方差公式的计算中使用了任意精度的十进制算术。为确保参考值与精确的浮点输入相对应，在计算方差之前，将每个浮点输入精确地转换为高精度的十进制数；不要将这些数字重新解释为其精确浮点表示之外的有理数或符号值。\n\n使用以下输入序列的测试套件，这些测试套件旨在检验相消、尺度差异、对称性和边界行为：\n- 测试用例 1：$[10^{16}, 1.0, 1.0, 1.0, 10^{16} + 1.0]$。\n- 测试用例 2：$[1.0, 1.0, 1.0, 1.0, 1.0]$。\n- 测试用例 3：$[1.0 + 10^{-10}, 1.0 - 10^{-10}, 1.0 + 2 \\cdot 10^{-10}, 1.0 - 2 \\cdot 10^{-10}, 1.0, 1.0]$。\n- 测试用例 4：$[-10^{9}, 10^{9}, -3.0, 3.0, -2.0, 2.0, -1.0, 1.0]$。\n- 测试用例 5：$[123456789.0]$。\n\n对于每个测试用例，计算三个浮点结果：\n- 两遍扫描法计算的总体方差的绝对误差。\n- 不带补偿的 Welford 单遍算法计算的总体方差的绝对误差。\n- 在二阶矩累加器中集成补偿求和的 Welford 单遍算法计算的总体方差的绝对误差。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。结果按测试用例和方法排序，即 $[e_{1, \\text{two-pass}}, e_{1, \\text{welford}}, e_{1, \\text{comp}}, e_{2, \\text{two-pass}}, e_{2, \\text{welford}}, e_{2, \\text{comp}}, \\ldots, e_{5, \\text{comp}}]$，其中每个 $e_{k, \\cdot}$ 是一个浮点绝对误差。不涉及物理单位。不涉及角度。所有答案均以浮点数表示。", "solution": "该问题要求实现并数值比较三种用于计算浮点数序列总体方差的不同算法。比较将基于每种方法相对于高精度参考计算的绝对误差。问题的核心在于理解和展示这些算法在有限精度算术中执行时固有的不同程度的数值稳定性。\n\n对于一个包含 $n$ 个数的有限序列 $\\{x_i\\}_{i=1}^n$，其均值为 $\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i$，总体方差 $\\sigma^2$ 定义为与均值的平方偏差的平均值：\n$$\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2\n$$\n在浮点运算中，由于舍入误差和灾难性相消 (catastrophic cancellation)，直接评估此公式及其变体会导致严重的精度损失。我们将分析三种数值稳健性递增的方法。\n\n**1. 两遍扫描算法 (The Two-Pass Algorithm)**\n\n这是对该定义最直接的实现。它通过对数据进行两次独立的扫描来完成计算。\n\n- **第一遍：计算均值。** 通过对所有元素求和并除以计数 $n$ 来计算均值 $\\mu$：\n  $$\n  \\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n  $$\n- **第二遍：计算平方偏差和。** 使用第一遍计算出的均值 $\\mu$，计算平方偏差和 $S_2$：\n  $$\n  S_2 = \\sum_{i=1}^n (x_i - \\mu)^2\n  $$\n- **最终方差：** 方差即为 $\\sigma^2 = S_2 / n$。\n\n该方法主要的数值弱点是**灾难性相消 (catastrophic cancellation)**。如果数据点 $x_i$ 密集地聚集在一起，它们的值将非常接近均值 $\\mu$。此时，减法 $x_i - \\mu$ 将涉及两个几乎相等的数，导致结果带有很大的相对误差和严重的精度损失。随后的平方和求和操作会加剧这种初始误差。此外，如果数据集包含数量级差异巨大的数字，第一遍中的求和会遭受舍入误差的影响，导致均值 $\\mu$ 不准确，进而破坏整个第二遍的计算。\n\n**2. Welford 在线算法 (Welford's Online Algorithm)**\n\n该算法在单遍扫描中计算方差，每当有新数据点到达时，就更新运行均值和平方偏差和。这种“在线”特性对于流数据处理非常有利。该算法基于以下递推关系。\n\n设 $M_k$ 为前 $k$ 个元素的运行均值，设 $S_k$ 为前 $k$ 个元素的运行平方偏差和，即 $S_k = \\sum_{i=1}^k (x_i - M_k)^2$。对于第一个元素 $x_1$，初始条件为 $M_1 = x_1$ 和 $S_1 = 0$。对于后续元素 $k = 2, 3, \\ldots, n$，更新公式为：\n$$\nM_k = M_{k-1} + \\frac{x_k - M_{k-1}}{k}\n$$\n$$\nS_k = S_{k-1} + (x_k - M_{k-1})(x_k - M_k)\n$$\n处理完所有 $n$ 个点后，总体方差为 $\\sigma^2 = S_n / n$。\n\nWelford 的算法通常比两遍扫描法具有更好的数值稳定性。它通过计算与*运行*均值的偏差，而不是与最终均值的偏差，来缓解灾难性相消。与两遍扫描法中的 $x_i - \\mu$ 相比，项 $x_k - M_{k-1}$ 不太可能出现两个几乎相等的大数相减的情况，尤其是在数据发生漂移时。然而，对 $S_k$ 的求和仍然会受到累积舍入误差的影响，特别是当一个非常小的更新项被加到一个很大的运行总和 $S_{k-1}$ 上时。\n\n**3. 带补偿求和的 Welford 算法 (Welford's Algorithm with Compensated Summation)**\n\n为了进一步提高 Welford 算法中求和 $S_k$ 的准确性，我们可以采用一种补偿求和技术，例如 Kahan 求和算法。该方法会跟踪求和过程中每一步产生的舍入误差，并将其作为校正项并入下一步计算中。\n\n对运行总和 $S_k$ 的更新修改如下，引入一个初始化为 $c_0 = 0$ 的校正项 $c_k$。对于每一步 $k$：\n1. 计算要加到平方和中的新项：\n   $$\n   \\text{term}_k = (x_k - M_{k-1})(x_k - M_k)\n   $$\n2. 并入上一步的校正项：\n   $$\n   y = \\text{term}_k - c_{k-1}\n   $$\n3. 将此校正后的项加到运行总和 $S_{k-1}$ 上。这个加法本身会引入新的误差：\n   $$\n   t = S_{k-1} + y\n   $$\n4. 计算新的校正项，它捕捉了上一次加法中丢失的低位比特：\n   $$\n   c_k = (t - S_{k-1}) - y\n   $$\n5. 更新总和：\n   $$\n   S_k = t\n   $$\n均值 $M_k$ 的更新保持不变。对 $S_k$ 进行的这种补偿求和有效地恢复了在浮点加法中本会丢失的精度，从而实现了平方偏差和的高度精确累加。这种方法非常稳健，即使对于病态数据集也能提供准确的结果。\n\n**参考计算与误差度量 (Reference Computation and Error Metric)**\n\n为了客观地评估这三种方法的准确性，需要计算一个高精度的参考方差 $\\sigma^2_{\\text{reference}}$。其实现方式如下：\n1. 获取输入的浮点数（它们是二进制的，例如 IEEE 754 `double` 精度）。\n2. 使用任意精度算术将每个数字转换为其精确的十进制表示。Python 的 `decimal` 模块非常适合此任务。\n3. 使用这种高精度算术执行总体方差公式 $\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\mu)^2$ 的所有计算，并使用足够大的位数（例如 100 位），以使舍入误差可以忽略不计。\n\n然后，通过与该参考值的绝对误差 $e$ 来量化每种已实现算法的性能：\n$$\ne = |\\sigma^2_{\\text{computed}} - \\sigma^2_{\\text{reference}}|\n$$\n这为每种计算方法在每个测试用例的特定条件下所造成的精度损失提供了直接的度量。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef reference_variance(data: list[float]) - float:\n    \"\"\"Computes population variance using high-precision decimal arithmetic.\"\"\"\n    if not data:\n        return 0.0\n\n    getcontext().prec = 100  # Set high precision for reference calculation\n    n = len(data)\n    d_data = [Decimal(x) for x in data]\n    \n    # Handle single-element case\n    if n == 1:\n        return 0.0\n\n    mean = sum(d_data) / Decimal(n)\n    sum_sq_dev = sum((x - mean)**2 for x in d_data)\n    variance = sum_sq_dev / Decimal(n)\n    \n    return float(variance)\n\ndef two_pass_variance(x: np.ndarray) - np.float64:\n    \"\"\"Computes population variance using a standard two-pass algorithm.\"\"\"\n    n = x.size\n    if n = 1:\n        return np.float64(0.0)\n    \n    # Pass 1: Compute mean\n    mean = np.sum(x) / n\n    \n    # Pass 2: Compute sum of squared deviations\n    sum_sq_dev = np.sum((x - mean)**2)\n    \n    return sum_sq_dev / n\n\ndef welford_variance(x: np.ndarray) - np.float64:\n    \"\"\"Computes population variance using Welford's one-pass online algorithm.\"\"\"\n    n = x.size\n    if n = 1:\n        return np.float64(0.0)\n        \n    M = np.float64(0.0)  # Running mean\n    S = np.float64(0.0)  # Running sum of squared deviations\n    \n    for k, val in enumerate(x, 1):\n        M_old = M\n        M += (val - M_old) / k\n        S += (val - M_old) * (val - M)\n        \n    return S / n\n\ndef compensated_welford_variance(x: np.ndarray) - np.float64:\n    \"\"\"\n    Computes population variance using Welford's algorithm with compensated summation\n    for the sum of squared deviations.\n    \"\"\"\n    n = x.size\n    if n = 1:\n        return np.float64(0.0)\n        \n    M = np.float64(0.0)  # Running mean\n    S = np.float64(0.0)  # Running sum of squared deviations\n    c = np.float64(0.0)  # Compensation for S\n    \n    for k, val in enumerate(x, 1):\n        # Update mean\n        M_old = M\n        M += (val - M) / k\n        \n        # Update sum of squares with compensation\n        term = (val - M_old) * (val - M)\n        y = term - c\n        t = S + y\n        c = (t - S) - y\n        S = t\n        \n    return S / n\n\ndef solve():\n    \"\"\"\n    Runs the variance comparison for the specified test cases and prints the\n    absolute errors in the required format.\n    \"\"\"\n    test_cases = [\n        [10**16, 1.0, 1.0, 1.0, 10**16 + 1.0],\n        [1.0, 1.0, 1.0, 1.0, 1.0],\n        [1.0 + 1e-10, 1.0 - 1e-10, 1.0 + 2e-10, 1.0 - 2e-10, 1.0, 1.0],\n        [-10**9, 10**9, -3.0, 3.0, -2.0, 2.0, -1.0, 1.0],\n        [123456789.0],\n    ]\n\n    results = []\n    for case in test_cases:\n        # It's crucial to first convert to np.float64 to simulate the\n        # exact inputs as they would be stored in standard double precision.\n        np_case = np.array(case, dtype=np.float64)\n        \n        # The reference must be computed from the list of python floats,\n        # which will be identical to np.float64 in value.\n        ref_var = reference_variance(case)\n\n        # Compute variance with each method\n        var_2pass = two_pass_variance(np_case)\n        var_welford = welford_variance(np_case)\n        var_comp = compensated_welford_variance(np_case)\n\n        # Calculate and store absolute errors\n        results.append(abs(var_2pass - ref_var))\n        results.append(abs(var_welford - ref_var))\n        results.append(abs(var_comp - ref_var))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3214603"}]}