{"hands_on_practices": [{"introduction": "牛顿法之所以强大，是因为它利用二阶信息来寻找稳定点。然而，当函数非凸时会发生什么呢？本练习 [@problem_id:2417369] 将使用一个简单的非凸二次函数来揭示一个令人意外的现象：即使牛顿法成功地将梯度降为零，一个完整的牛顿步也可能实际*增加*函数值。理解这一局限性是构建稳健优化算法的第一步。", "problem": "设 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 由 $f(x_1,x_2)=\\tfrac{1}{2}x_1^2-\\tfrac{1}{2}x_2^2$ 定义。从 $x_0=(0,1)$ 出发，使用全步长更新 $x_{k+1}=x_k-\\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$ 生成牛顿迭代序列以寻找一个驻点。证明梯度的欧几里得范数 $\\|\\nabla f(x_k)\\|_2$ 沿生成的迭代序列单调递减，而函数值 $f(x_k)$ 在第一次迭代时增加，即 $f(x_1)>f(x_0)$。计算 $\\Delta f=f(x_1)-f(x_0)$ 的精确值。请以精确值形式给出答案，不要四舍五入。", "solution": "目标函数由 $f(x_1, x_2) = \\frac{1}{2}x_1^2 - \\frac{1}{2}x_2^2$ 给出。我们使用牛顿法寻找一个驻点。驻点 $x^*$ 满足 $\\nabla f(x^*) = 0$。\n\n首先，我们计算一个通用点 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 的梯度向量 $\\nabla f(x)$ 和海森矩阵 $\\nabla^2 f(x)$。\n梯度为：\n$$\n\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ -x_2 \\end{pmatrix}\n$$\n海森矩阵为：\n$$\n\\nabla^2 f(x) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 f}{\\partial x_2^2} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}\n$$\n对于所有 $x \\in \\mathbb{R}^2$，海森矩阵是常数。$\\nabla^2 f$ 的特征值为 $\\lambda_1 = 1$ 和 $\\lambda_2 = -1$。由于一个特征值为正，一个为负，所以海森矩阵是不定的。这意味着该函数的驻点是一个鞍点。\n海森矩阵的逆矩阵是：\n$$\n\\left[\\nabla^2 f(x)\\right]^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix}\n$$\n海森矩阵是其自身的逆矩阵。\n\n牛顿更新规则是 $x_{k+1} = x_k - \\left[\\nabla^2 f(x_k)\\right]^{-1}\\nabla f(x_k)$。对于我们的特定函数，这变为：\n$$\nx_{k+1} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} x_{1,k} \\\\ -x_{2,k} \\end{pmatrix} = \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} - \\begin{pmatrix} x_{1,k} \\\\ x_{2,k} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n这表明对于任何起始点 $x_k \\neq (0,0)$，牛顿法在一次迭代中收敛到驻点 $(0,0)$。\n\n我们从 $x_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$ 开始。\n第一个迭代点 $x_1$ 是：\n$$\nx_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n所有后续的迭代点将保持在驻点：对于所有 $k \\ge 1$，$x_k = (0,0)$。\n\n现在，我们来解决问题中指定的任务。\n\n任务1：证明 $\\|\\nabla f(x_k)\\|_2$ 单调递减。\n我们计算每个迭代点的梯度。\n对于 $k=0$：\n$$\n\\nabla f(x_0) = \\nabla f(0,1) = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}\n$$\n其欧几里得范数为 $\\|\\nabla f(x_0)\\|_2 = \\sqrt{0^2 + (-1)^2} = 1$。\n对于 $k=1$：\n$$\n\\nabla f(x_1) = \\nabla f(0,0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n其欧几里得范数为 $\\|\\nabla f(x_1)\\|_2 = \\sqrt{0^2 + 0^2} = 0$。\n对于所有 $k > 1$，由于 $x_k = (0,0)$，我们有 $\\nabla f(x_k) = (0,0)$ 且 $\\|\\nabla f(x_k)\\|_2 = 0$。\n梯度范数序列为 $1, 0, 0, 0, \\ldots$。这是一个单调非增（因此也是单调递减）序列，因为 $1 > 0$ 且 $0 \\ge 0$。条件得到满足。\n\n任务2：证明 $f(x_1) > f(x_0)$。\n我们计算 $x_0$ 和 $x_1$ 处的函数值。\n在起始点 $x_0 = (0,1)$ 处：\n$$\nf(x_0) = f(0,1) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(1)^2 = -\\frac{1}{2}\n$$\n在第一个迭代点 $x_1 = (0,0)$ 处：\n$$\nf(x_1) = f(0,0) = \\frac{1}{2}(0)^2 - \\frac{1}{2}(0)^2 = 0\n$$\n比较这两个值，我们有 $f(x_1) = 0$ 和 $f(x_0) = -1/2$。确实，$0 > -1/2$，这证明了 $f(x_1) > f(x_0)$。函数值增加了。出现这种情况是因为海森矩阵不是正定的，因此牛顿方向不保证是下降方向。搜索方向是 $p_0 = -[\\nabla^2 f(x_0)]^{-1}\\nabla f(x_0) = -\\begin{pmatrix} 1  0 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$。从 $x_0$ 沿 $p_0$ 的方向导数为 $\\nabla f(x_0)^T p_0 = \\begin{pmatrix} 0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = 1 > 0$，这证实了 $p_0$ 是一个上升方向。\n\n任务3：计算 $\\Delta f = f(x_1) - f(x_0)$ 的精确值。\n使用上一步计算的值：\n$$\n\\Delta f = f(x_1) - f(x_0) = 0 - \\left(-\\frac{1}{2}\\right) = \\frac{1}{2}\n$$\n第一次迭代后函数值的变化量的精确值为 $\\frac{1}{2}$。", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "2417369"}, {"introduction": "像BFGS这样的拟牛顿法通过构建近似矩阵来避免计算Hessian矩阵的高昂成本。这个近似矩阵的更新公式并非随意设计，而是为了维持一些关键属性。本思想实验 [@problem_id:2417335] 邀请您探究在BFGS更新公式中犯一个简单错误的后果——交换步长向量 $s_k$ 和梯度差向量 $y_k$ 的角色——从而理解标准公式为何对于保持正定性等性质至关重要。", "problem": "考虑严格凸二次函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ 的无约束最小化问题，该函数定义为\n$$\nf(x) = \\tfrac{1}{2}\\, x^{\\mathsf{T}} A x,\n$$\n其中\n$$\nA=\\begin{pmatrix}4  1 \\\\ 1  3\\end{pmatrix}.\n$$\n设迭代点为 $x_{k}=\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$ 和 $x_{k+1}=\\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$，并设当前的逆Hessian近似为 $H_{k}=I_{2}$，即 $2\\times 2$ 单位矩阵。定义位移向量和梯度差向量为\n$$\ns_{k}=x_{k+1}-x_{k},\\qquad y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k}).\n$$\n标准的Broyden–Fletcher–Goldfarb–Shanno (BFGS) 逆Hessian更新公式为\n$$\nH_{k+1}=\\left(I-\\rho_{k}s_{k}y_{k}^{\\mathsf{T}}\\right)H_{k}\\left(I-\\rho_{k}y_{k}s_{k}^{\\mathsf{T}}\\right)+\\rho_{k}s_{k}s_{k}^{\\mathsf{T}},\n$$\n其中 $\\rho_{k}=\\left(y_{k}^{\\mathsf{T}}s_{k}\\right)^{-1}$。然而，假设在构建更新时，你错误地将所有的 $s_{k}$ 和 $y_{k}$ 交换了位置，从而得到\n$$\n\\widetilde{H}_{k+1}=\\left(I-\\rho_{k}y_{k}s_{k}^{\\mathsf{T}}\\right)H_{k}\\left(I-\\rho_{k}s_{k}y_{k}^{\\mathsf{T}}\\right)+\\rho_{k}y_{k}y_{k}^{\\mathsf{T}}.\n$$\n计算 $\\widetilde{H}_{k+1}$ 的最小特征值。请以精确的简化表达式形式给出答案，不要使用小数近似。", "solution": "待最小化的函数是二次型 $f(x) = \\frac{1}{2} x^{\\mathsf{T}} A x$，其中 $x \\in \\mathbb{R}^{2}$，对称矩阵为 $A = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$。该函数的梯度由 $\\nabla f(x) = A x$ 给出。\n\n位移向量 $s_{k}$ 定义为迭代点 $x_{k+1}$ 和 $x_{k}$ 之差。给定 $x_{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $x_{k+1} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，我们有：\n$$\ns_{k} = x_{k+1} - x_{k} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n梯度差向量 $y_{k}$ 定义为 $y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$。使用梯度表达式，这变为 $y_{k} = A x_{k+1} - A x_{k} = A(x_{k+1} - x_{k}) = A s_{k}$。我们计算这个乘积：\n$$\ny_{k} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 4 \\cdot 1 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + 3 \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}\n$$\n标量 $\\rho_{k}$ 由 $\\rho_{k} = (y_{k}^{\\mathsf{T}}s_{k})^{-1}$ 给出。首先，我们计算标量积 $y_{k}^{\\mathsf{T}}s_{k}$：\n$$\ny_{k}^{\\mathsf{T}}s_{k} = \\begin{pmatrix} 3  -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 3 \\cdot 1 + (-2) \\cdot (-1) = 3 + 2 = 5\n$$\n因此，$\\rho_{k} = \\frac{1}{5}$。\n\n题目指定了一个错误的逆Hessian近似 $\\widetilde{H}_{k+1}$ 的更新公式：\n$$\n\\widetilde{H}_{k+1}=\\left(I-\\rho_{k}y_{k}s_{k}^{\\mathsf{T}}\\right)H_{k}\\left(I-\\rho_{k}s_{k}y_{k}^{\\mathsf{T}}\\right)+\\rho_{k}y_{k}y_{k}^{\\mathsf{T}}\n$$\n我们已知当前的近似是单位矩阵 $H_{k}=I_{2}$。将 $H_k = I$ 代入公式得到：\n$$\n\\widetilde{H}_{k+1}=\\left(I-\\rho_{k}y_{k}s_{k}^{\\mathsf{T}}\\right)\\left(I-\\rho_{k}s_{k}y_{k}^{\\mathsf{T}}\\right)+\\rho_{k}y_{k}y_{k}^{\\mathsf{T}}\n$$\n我们展开第一项：\n$$\n\\left(I-\\rho_{k}y_{k}s_{k}^{\\mathsf{T}}\\right)\\left(I-\\rho_{k}s_{k}y_{k}^{\\mathsf{T}}\\right) = I - \\rho_{k}s_{k}y_{k}^{\\mathsf{T}} - \\rho_{k}y_{k}s_{k}^{\\mathsf{T}} + \\rho_{k}^{2}(y_{k}s_{k}^{\\mathsf{T}})(s_{k}y_{k}^{\\mathsf{T}})\n$$\n最后一项根据矩阵乘法的结合律可以简化为：$\\rho_{k}^{2} y_{k}(s_{k}^{\\mathsf{T}}s_{k})y_{k}^{\\mathsf{T}}$。中间项 $s_{k}^{\\mathsf{T}}s_{k}$ 是 $s_k$ 的欧几里得范数的平方：\n$$\ns_{k}^{\\mathsf{T}}s_{k} = \\|s_{k}\\|^{2} = 1^{2} + (-1)^{2} = 2\n$$\n因此，展开式变为 $I - \\rho_{k}s_{k}y_{k}^{\\mathsf{T}} - \\rho_{k}y_{k}s_{k}^{\\mathsf{T}} + 2\\rho_{k}^{2}y_{k}y_{k}^{\\mathsf{T}}$。\n将此代回 $\\widetilde{H}_{k+1}$ 的表达式中：\n$$\n\\widetilde{H}_{k+1} = I - \\rho_{k}(s_{k}y_{k}^{\\mathsf{T}} + y_{k}s_{k}^{\\mathsf{T}}) + (2\\rho_{k}^{2} + \\rho_{k})y_{k}y_{k}^{\\mathsf{T}}\n$$\n我们现在代入 $\\rho_k = \\frac{1}{5}$ 的值。最后一项的系数是 $2(\\frac{1}{5})^{2} + \\frac{1}{5} = \\frac{2}{25} + \\frac{5}{25} = \\frac{7}{25}$。\n我们计算必要的外积矩阵：\n$$\ns_{k}y_{k}^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 3  -2 \\end{pmatrix} = \\begin{pmatrix} 3  -2 \\\\ -3  2 \\end{pmatrix}\n$$\n$$\ny_{k}s_{k}^{\\mathsf{T}} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}\\begin{pmatrix} 1  -1 \\end{pmatrix} = \\begin{pmatrix} 3  -3 \\\\ -2  2 \\end{pmatrix}\n$$\n$$\ny_{k}y_{k}^{\\mathsf{T}} = \\begin{pmatrix} 3 \\\\ -2 \\end{pmatrix}\\begin{pmatrix} 3  -2 \\end{pmatrix} = \\begin{pmatrix} 9  -6 \\\\ -6  4 \\end{pmatrix}\n$$\n前两项的和是 $s_{k}y_{k}^{\\mathsf{T}} + y_{k}s_{k}^{\\mathsf{T}} = \\begin{pmatrix} 6  -5 \\\\ -5  4 \\end{pmatrix}$。\n现在，我们组合出 $\\widetilde{H}_{k+1}$：\n$$\n\\widetilde{H}_{k+1} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{5}\\begin{pmatrix} 6  -5 \\\\ -5  4 \\end{pmatrix} + \\frac{7}{25}\\begin{pmatrix} 9  -6 \\\\ -6  4 \\end{pmatrix}\n$$\n$$\n\\widetilde{H}_{k+1} = \\begin{pmatrix} 1 - \\frac{6}{5} + \\frac{63}{25}  0 - (-\\frac{5}{5}) - \\frac{42}{25} \\\\ 0 - (-\\frac{5}{5}) - \\frac{42}{25}  1 - \\frac{4}{5} + \\frac{28}{25} \\end{pmatrix} = \\begin{pmatrix} \\frac{25-30+63}{25}  \\frac{25-42}{25} \\\\ \\frac{25-42}{25}  \\frac{25-20+28}{25} \\end{pmatrix} = \\begin{pmatrix} \\frac{58}{25}  -\\frac{17}{25} \\\\ -\\frac{17}{25}  \\frac{33}{25} \\end{pmatrix}\n$$\n为了求 $\\widetilde{H}_{k+1}$ 的特征值 $\\lambda$，我们解特征方程 $\\det(\\widetilde{H}_{k+1} - \\lambda I) = 0$：\n$$\n\\det \\begin{pmatrix} \\frac{58}{25}-\\lambda  -\\frac{17}{25} \\\\ -\\frac{17}{25}  \\frac{33}{25}-\\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(\\frac{58}{25}-\\lambda\\right)\\left(\\frac{33}{25}-\\lambda\\right) - \\left(-\\frac{17}{25}\\right)^{2} = 0\n$$\n这导出一个二次方程 $\\lambda^{2} - \\mathrm{tr}(\\widetilde{H}_{k+1})\\lambda + \\det(\\widetilde{H}_{k+1}) = 0$。\n迹为 $\\mathrm{tr}(\\widetilde{H}_{k+1}) = \\frac{58}{25} + \\frac{33}{25} = \\frac{91}{25}$。\n行列式为 $\\det(\\widetilde{H}_{k+1}) = \\left(\\frac{58}{25}\\right)\\left(\\frac{33}{25}\\right) - \\left(-\\frac{17}{25}\\right)^{2} = \\frac{1914 - 289}{625} = \\frac{1625}{625} = \\frac{13}{5}$。\n特征方程为 $\\lambda^{2} - \\frac{91}{25}\\lambda + \\frac{13}{5} = 0$。两边乘以 $25$ 得到 $25\\lambda^{2} - 91\\lambda + 65 = 0$。\n使用二次方程求根公式，特征值为：\n$$\n\\lambda = \\frac{91 \\pm \\sqrt{(-91)^2 - 4(25)(65)}}{2(25)} = \\frac{91 \\pm \\sqrt{8281 - 6500}}{50} = \\frac{91 \\pm \\sqrt{1781}}{50}\n$$\n两个特征值为 $\\lambda_{1} = \\frac{91 + \\sqrt{1781}}{50}$ 和 $\\lambda_{2} = \\frac{91 - \\sqrt{1781}}{50}$。因为 $\\sqrt{1781}$ 是一个正实数，所以两者中较小的一个是 $\\lambda_{2}$。数字 $1781$ 是质数 $13$ 和 $137$ 的乘积，所以 $\\sqrt{1781}$ 不能再被化简。\n最小的特征值是 $\\frac{91 - \\sqrt{1781}}{50}$。", "answer": "$$\n\\boxed{\\frac{91 - \\sqrt{1781}}{50}}\n$$", "id": "2417335"}, {"introduction": "理论为我们奠定了基础，但实际性能是检验优化算法的最终标准。本练习 [@problem_id:2417339] 挑战您在一个经典的病态基准函数——Rosenbrock函数上，实现并比较改进的牛顿法与BFGS算法。通过同时研究不同的终止准则，您将亲身体会到这些强大方法之间的实际权衡，以及判断收敛的微妙之处。", "problem": "你需要通过构建和分析由二阶搜索方向和准二阶搜索方向生成的序列，研究不同终止准则对一个病态优化问题的影响。考虑由下式定义的函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$\n$$\nf(x) \\equiv f\\left(\\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix}\\right) = 100\\left(x_{2}-x_{1}^{2}\\right)^{2} + \\left(1-x_{1}\\right)^{2},\n$$\n已知该函数具有一个狭窄的弯曲谷底，其唯一极小值点位于 $x^{\\star}=\\begin{bmatrix}1\\\\1\\end{bmatrix}$。\n\n从一个初始点 $x_{0}\\in\\mathbb{R}^{2}$ 定义一个迭代过程如下。在第 $k$ 次迭代时，通过求解\n$$\nB_{k}p_{k}=-\\nabla f(x_{k}),\n$$\n来计算搜索方向 $p_{k}$，然后执行一步\n$$\nx_{k+1}=x_{k}+\\alpha_{k}p_{k},\n$$\n其中 $\\alpha_{k}$ 通过 Armijo 回溯线搜索选择，参数为 $c_{1}=10^{-4}$，收缩因子为 $\\beta=\\tfrac{1}{2}$。从 $\\alpha_{k}=1$ 开始，不断令 $\\alpha_{k}\\leftarrow \\beta \\alpha_{k}$ 直到\n$$\nf(x_{k}+\\alpha_{k}p_{k}) \\le f(x_{k}) + c_{1}\\alpha_{k}\\nabla f(x_{k})^{\\top}p_{k},\n$$\n成立，或者直到 $\\alpha_{k}  10^{-16}$，此时接受当前的 $\\alpha_{k}$。对于矩阵 $B_{k}$，考虑以下两种情况：\n- 情况 H (精确二阶)：$B_{k}=\\nabla^{2} f(x_{k})$，如果需要，则加上一个对角移位 $\\tau I$ 以使 $B_{k}$ 是对称正定的，其中 $\\tau \\ge \\max\\{0,\\delta-\\lambda_{\\min}(\\nabla^{2}f(x_{k}))\\}$，$\\delta=10^{-6}$ 且 $\\lambda_{\\min}(\\cdot)$ 表示最小特征值。如果从 $B_{k}$ 计算出的 $p_{k}$ 不满足 $\\nabla f(x_{k})^{\\top}p_{k}  0$，则用 $-\\nabla f(x_{k})$ 替换 $p_{k}$。\n- 情况 Q (准二阶)：$B_{k}^{-1}$ 通过 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 逆 Hessian 更新来维护，从 $B_{0}^{-1}=I$ 开始，使用标准的逆 BFGS 公式，其中 $s_{k}=x_{k+1}-x_{k}$，$y_{k}=\\nabla f(x_{k+1})-\\nabla f(x_{k})$，前提是 $y_{k}^{\\top}s_{k}>0$，否则跳过更新。搜索方向为 $p_{k}=-B_{k}^{-1}\\nabla f(x_{k})$。\n\n采用最大迭代次数 $N_{\\max}=2000$。研究以下两种终止准则：\n- 准则 G：当 $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}  \\varepsilon$ 时停止。\n- 准则 S：当 $\\lVert x_{k}-x_{k-1}\\rVert_{2}  \\varepsilon$ 时停止（该准则仅在 $k\\ge 1$ 时应用）。\n\n你的程序必须对每个测试用例运行迭代过程，记录所执行的接受迭代次数、最终目标函数值 $f(x_{k})$、最终梯度无穷范数 $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}$ 以及最终步长二范数 $\\lVert x_{k}-x_{k-1}\\rVert_{2}$（如果没有执行任何步骤，则此值为 $0$）。如果在达到最大迭代次数 $N_{\\max}$ 后仍未满足终止准则，则返回当前的各项指标。\n\n测试套件。对于每个测试用例，元组包含：方法选择（H 或 Q）、终止准则（G 或 S）、初始点 $x_{0}$ 和容差 $\\varepsilon$。使用以下 6 个用例：\n- 用例 1：方法 H，准则 G，$x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$，$\\varepsilon=10^{-8}$。\n- 用例 2：方法 H，准则 S，$x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$，$\\varepsilon=10^{-4}$。\n- 用例 3：方法 Q，准则 G，$x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$，$\\varepsilon=10^{-8}$。\n- 用例 4：方法 Q，准则 S，$x_{0}=\\begin{bmatrix}-1.2\\\\ 1.0\\end{bmatrix}$，$\\varepsilon=10^{-4}$。\n- 用例 5：方法 H，准则 G，$x_{0}=\\begin{bmatrix}1.0\\\\ 1.0\\end{bmatrix}$，$\\varepsilon=10^{-12}$。\n- 用例 6：方法 Q，准则 S，$x_{0}=\\begin{bmatrix}1.0\\\\ 1.0\\end{bmatrix}$，$\\varepsilon=10^{-12}$。\n\n最终输出格式。你的程序应生成单行输出，其中包含一个结果列表，每个测试用例一个，顺序与上述相同。每个结果必须是以下列表：\n$$\n[\\ \\text{method\\_id},\\ \\text{criterion\\_id},\\ \\text{iterations},\\ f(x_{k}),\\ \\lVert \\nabla f(x_{k})\\rVert_{\\infty},\\ \\lVert x_{k}-x_{k-1}\\rVert_{2}\\ ],\n$$\n其中方法标识符 H 为 $0$，Q 为 $1$；准则标识符 G 为 $0$，S 为 $1$；迭代次数为整数，其余条目为实数。因此，整行必须是包含 6 个列表的单个列表，例如\n$$\n[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots],\n$$\n不打印任何其他字符。", "solution": "所提出的问题是计算工程领域，特别是数值优化领域中一个定义明确的任务。它要求在 Rosenbrock 函数上实现和比较两种标准的无约束非线性优化迭代方法——修正的牛顿法和 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 准牛顿法。分析的重点是不同终止准则的影响。该问题具有科学依据，没有矛盾之处，并提供了所有必要的参数。因此，可以构建一个严谨的解决方案。\n\n目标是找到 Rosenbrock 函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ 的极小值点，其定义为：\n$$\nf(x) = f\\left(\\begin{bmatrix}x_{1}\\\\ x_{2}\\end{bmatrix}\\right) = 100\\left(x_{2}-x_{1}^{2}\\right)^{2} + \\left(1-x_{1}\\right)^{2}\n$$\n该函数是优化算法的经典基准测试，因为它具有非凸性以及一个通向全局最小值的狭窄弯曲谷底，这使得收敛具有挑战性。已知其唯一极小值点为 $x^{\\star} = \\begin{bmatrix}1 \\\\ 1\\end{bmatrix}$，在该点处 $f(x^{\\star}) = 0$。\n\n对于迭代方法，我们需要梯度向量 $\\nabla f(x)$，对于牛顿法，还需要 Hessian 矩阵 $\\nabla^{2} f(x)$。\n梯度由下式给出：\n$$\n\\nabla f(x) = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} -400x_{1}(x_{2}-x_{1}^{2}) - 2(1-x_{1}) \\\\ 200(x_{2}-x_{1}^{2}) \\end{bmatrix}\n$$\nHessian 矩阵为：\n$$\n\\nabla^{2} f(x) = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}  \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}  \\frac{\\partial^2 f}{\\partial x_2^2} \\end{bmatrix} = \\begin{bmatrix} 1200x_{1}^{2} - 400x_{2} + 2  -400x_{1} \\\\ -400x_{1}  200 \\end{bmatrix}\n$$\n所有考虑的迭代方法都遵循以下通用结构：\n$$\nx_{k+1} = x_{k} + \\alpha_{k} p_{k}\n$$\n其中 $x_{k}$ 是当前迭代点，$p_{k}$ 是搜索方向，$\\alpha_{k}$ 是步长。\n\n**搜索方向计算**\n\n这些方法之间的主要区别在于搜索方向 $p_{k}$ 的计算。通常，这是基于求解一个形如 $B_{k}p_{k} = -\\nabla f(x_{k})$ 的线性系统，其中 $B_k$ 是一个对称正定矩阵，它要么是真实的 Hessian 矩阵，要么是其近似。\n\n1.  **情况 H：修正牛顿法**\n    此方法使用二阶信息，通过将 $B_{k}$ 设置为精确的 Hessian 矩阵，即 $B_{k} = \\nabla^{2} f(x_{k})$。搜索方向则为 $p_{k} = -(\\nabla^{2} f(x_{k}))^{-1} \\nabla f(x_{k})$。对于函数的局部二次模型，此方向是最优的。然而，如果 Hessian 矩阵 $\\nabla^{2} f(x_{k})$ 不是正定的，则二次模型不是凸的，$p_{k}$ 可能不是一个下降方向。为了克服这一点，采用了一种修正方法。矩阵 $B_k$ 被调整为 $B_k = \\nabla^{2} f(x_{k}) + \\tau I$，其中 $I$ 是单位矩阵，$\\tau$ 是一个非负移位，其选择是为了确保 $B_k$ 是充分正定的。具体来说，$\\tau = \\max\\{0, \\delta - \\lambda_{\\min}(\\nabla^{2}f(x_{k}))\\}$，对于一个小的正常数 $\\delta > 0$。这个过程，被称为修正 Cholesky 分解或特征值移位方法，保证了得到的搜索方向是一个下降方向。作为进一步的保障，如果计算出的 $p_k$ 未能满足下降条件 $\\nabla f(x_k)^\\top p_k  0$，算法将退回到最基本的下降方向，即最速下降方向 $p_k = -\\nabla f(x_k)$。\n\n2.  **情况 Q：BFGS 准牛顿法**\n    此方法避免了在每次迭代中计算和求逆 Hessian 矩阵的计算开销和潜在的复杂性。取而代之的是，它构建了 Hessian 矩阵*逆*的近似，此处表示为 $B_{k}^{-1}$。然后可以轻松地计算搜索方向为 $p_{k} = -B_{k}^{-1}\\nabla f(x_{k})$。从一个初始猜测（通常为 $B_{0}^{-1} = I$）开始，该近似在每一步都使用最近一步的信息进行更新。逆 Hessian 的 BFGS 更新公式为：\n    $$\n    B_{k+1}^{-1} = \\left(I - \\frac{s_{k}y_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\\right) B_{k}^{-1} \\left(I - \\frac{y_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\\right) + \\frac{s_{k}s_{k}^{\\top}}{y_{k}^{\\top}s_{k}}\n    $$\n    其中 $s_{k} = x_{k+1} - x_{k}$，$y_{k} = \\nabla f(x_{k+1}) - \\nabla f(x_{k})$。此更新的一个关键要求是曲率条件 $y_{k}^{\\top}s_{k} > 0$。此条件确保如果 $B_{k}^{-1}$ 是正定的，则更新后的矩阵 $B_{k+1}^{-1}$ 保持正定。如果不满足该条件，则跳过更新，并设置 $B_{k+1}^{-1}$ 为 $B_{k}^{-1}$。\n\n**步长计算**\n\n对于这两种方法，一旦找到下降方向 $p_k$，步长 $\\alpha_k$ 就由 Armijo 回溯线搜索确定。这种搜索确保了函数值的充分下降，同时避免了采取过小的步长。从一个完整步长 $\\alpha_k = 1$ 开始，它被连续地乘以一个因子 $\\beta \\in (0, 1)$，直到满足 Armijo 条件：\n$$\nf(x_{k}+\\alpha_{k}p_{k}) \\le f(x_{k}) + c_{1}\\alpha_{k}\\nabla f(x_{k})^{\\top}p_{k}\n$$\n参数 $c_{1}$（例如 $10^{-4}$）控制了何种程度的下降被认为是充分的。这可以防止算法采取微小的、几乎没有进展的步长。对 $\\alpha_k$ 施加一个下界，以防止线搜索无限期地运行。\n\n**终止准则**\n\n选择何时停止迭代过程是至关重要的。本问题研究了两种常见的准则：\n\n1.  **准则 G：基于梯度范数。** 当 $\\lVert \\nabla f(x_{k})\\rVert_{\\infty}  \\varepsilon$ 时，算法终止。这在理论上是合理的，因为点 $x^{\\star}$ 成为局部极小值点的必要条件是 $\\nabla f(x^{\\star}) = 0$。小的梯度范数表明迭代点接近一个驻点。\n\n2.  **准则 S：基于步长大小。** 当 $\\lVert x_{k}-x_{k-1}\\rVert_{2}  \\varepsilon$ 时，算法终止。这个准则在实践中经常使用，因为它似乎很直观，即当我们接近解时，步长应该变小。然而，它可能不可靠。如果算法遇到困难（例如，在狭窄的山谷中），它可能会在远离解的地方采取非常小的步长，导致过早终止。反之，在高曲率区域，即使位置的微小变化也可能对应于梯度的大幅变化。这个问题的设计允许在一个具有挑战性的函数上直接比较这两种准则。最大迭代次数 $N_{\\max} = 2000$ 作为防止不收敛的保险措施。\n\n对六个测试用例的分析将阐明这些标准优化方案在不同终止策略下的实际性能、收敛速度和鲁棒性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problem for all test cases and print the results.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (method, criterion, x0, epsilon)\n        ('H', 'G', np.array([-1.2, 1.0]), 1e-8),\n        ('H', 'S', np.array([-1.2, 1.0]), 1e-4),\n        ('Q', 'G', np.array([-1.2, 1.0]), 1e-8),\n        ('Q', 'S', np.array([-1.2, 1.0]), 1e-4),\n        ('H', 'G', np.array([1.0, 1.0]), 1e-12),\n        ('Q', 'S', np.array([1.0, 1.0]), 1e-12),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        method, criterion, x0, epsilon = case\n        method_id = 0 if method == 'H' else 1\n        criterion_id = 0 if criterion == 'G' else 1\n        \n        iterations, final_f, final_grad_norm, final_step_norm = run_optimizer(\n            method, criterion, x0, epsilon\n        )\n        \n        result = [\n            method_id, \n            criterion_id, \n            iterations, \n            final_f, \n            final_grad_norm, \n            final_step_norm\n        ]\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\n\ndef rosenbrock_f(x):\n    \"\"\"Rosenbrock function.\"\"\"\n    return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n\ndef rosenbrock_grad(x):\n    \"\"\"Gradient of the Rosenbrock function.\"\"\"\n    g1 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n    g2 = 200 * (x[1] - x[0]**2)\n    return np.array([g1, g2])\n\ndef rosenbrock_hess(x):\n    \"\"\"Hessian of the Rosenbrock function.\"\"\"\n    h11 = 1200 * x[0]**2 - 400 * x[1] + 2\n    h12 = -400 * x[0]\n    h22 = 200\n    return np.array([[h11, h12], [h12, h22]])\n\n\ndef run_optimizer(method, criterion, x0, epsilon):\n    \"\"\"\n    Runs the optimization loop for a given configuration.\n    \"\"\"\n    \n    # Parameters\n    N_max = 2000\n    c1 = 1e-4\n    beta = 0.5\n    delta = 1e-6\n    alpha_min = 1e-16\n\n    x_k = np.copy(x0).astype(float)\n    x_prev = np.copy(x_k)\n    \n    # Initialize BFGS inverse Hessian approximation\n    if method == 'Q':\n        B_inv_k = np.identity(2)\n    \n    k = 0\n    while k  N_max:\n        grad_k = rosenbrock_grad(x_k)\n        \n        # --- Check termination criteria ---\n        if criterion == 'G':\n            if np.linalg.norm(grad_k, ord=np.inf)  epsilon:\n                break\n        elif criterion == 'S' and k > 0:\n            if np.linalg.norm(x_k - x_prev, ord=2)  epsilon:\n                break\n        \n        # --- Compute search direction p_k ---\n        if method == 'H':\n            H_k = rosenbrock_hess(x_k)\n            try:\n                # Eigenvalue modification to ensure positive definiteness\n                min_eig = np.min(np.linalg.eigvalsh(H_k))\n                tau = max(0, delta - min_eig)\n                B_k = H_k + tau * np.identity(2)\n                p_k = np.linalg.solve(B_k, -grad_k)\n                # Fallback to steepest descent if not a descent direction\n                if grad_k.T @ p_k >= 0:\n                    p_k = -grad_k\n            except np.linalg.LinAlgError:\n                # Fallback in case of singular matrix\n                p_k = -grad_k\n        \n        elif method == 'Q':\n            p_k = -B_inv_k @ grad_k\n\n        # --- Line search (Armijo backtracking) ---\n        alpha = 1.0\n        f_k = rosenbrock_f(x_k)\n        grad_dot_p = grad_k.T @ p_k\n        \n        while alpha >= alpha_min:\n            x_candidate = x_k + alpha * p_k\n            f_candidate = rosenbrock_f(x_candidate)\n            if f_candidate = f_k + c1 * alpha * grad_dot_p:\n                break\n            alpha *= beta\n        else: # loop exhausted without break\n            # This part handles the case where the loop completes without finding a suitable alpha.\n            # According to the problem, we should accept the current alpha, which would be the last value before dropping below alpha_min.\n            # However, the problem statement is a bit ambiguous: \"此时接受当前的 alpha_k\".\n            # The current code sets it to alpha_min. This is a reasonable safeguard.\n            # Let's stick to the code's explicit logic.\n            alpha = alpha_min\n\n        x_prev = np.copy(x_k)\n        x_k = x_k + alpha * p_k\n        \n        # --- BFGS update for inverse Hessian (Method Q) ---\n        if method == 'Q':\n            s_k = x_k - x_prev\n            # Compute gradient at new point\n            grad_k_plus_1 = rosenbrock_grad(x_k)\n            y_k = grad_k_plus_1 - grad_k\n            \n            y_dot_s = y_k.T @ s_k\n            if y_dot_s > 0:\n                rho_k = 1.0 / y_dot_s\n                I = np.identity(2)\n                V = I - rho_k * np.outer(s_k, y_k)\n                B_inv_k = V @ B_inv_k @ V.T + rho_k * np.outer(s_k, s_k)\n\n        k += 1\n\n    # --- Collect final metrics ---\n    final_f = rosenbrock_f(x_k)\n    final_grad_norm = np.linalg.norm(rosenbrock_grad(x_k), ord=np.inf)\n    if k == 0:\n        final_step_norm = 0.0\n    else:\n        final_step_norm = np.linalg.norm(x_k - x_prev, ord=2)\n\n    return k, final_f, final_grad_norm, final_step_norm\n\nsolve()\n```", "id": "2417339"}]}