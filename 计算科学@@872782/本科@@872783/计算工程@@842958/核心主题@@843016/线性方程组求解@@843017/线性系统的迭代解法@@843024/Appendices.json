{"hands_on_practices": [{"introduction": "迭代法是求解大型线性系统的基石，而高斯-赛德尔（Gauss-Seidel）法是其中最经典的方法之一。在实现时，我们必须选择一个顺序来更新方程组中的未知数，这个选择会影响求解过程。本练习 [@problem_id:2406634] 旨在通过一个二维热传导问题，动手比较两种经典的更新顺序——字典序（lexicographic）和红黑棋盘（red-black）序——对收敛速度的经验性影响，从而让你直观感受算法内部机制如何影响其性能。", "problem": "考虑单位正方形上的稳态热传导问题（一个二维（$2$D）泊松方程），其边界条件为齐次狄利克雷边界条件。设 $u(x,y)$ 在 $(0,1)\\times(0,1)$ 上满足 $-\\Delta u = f(x,y)$，并在边界上 $u=0$。使用均匀笛卡尔网格对域进行离散化，每个坐标方向有 $n$ 个内部点，网格间距 $h = \\frac{1}{n+1}$，并采用标准的五点有限差分格式。这将产生一个大小为 $n^2 \\times n^2$ 的线性系统 $A \\mathbf{u} = \\mathbf{b}$，其中，对于每个内部网格索引对 $(i,j)$（$i \\in \\{1,\\dots,n\\}$ 且 $j \\in \\{1,\\dots,n\\}$），离散算子应用如下：\n$$\n\\left(A \\mathbf{u}\\right)_{i,j} = \\frac{1}{h^2}\\left(4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}\\right),\n$$\n其中 $u_{0,j} = u_{n+1,j} = u_{i,0} = u_{i,n+1} = 0$ 强制执行边界值。设源项恒为零，$f(x,y) \\equiv 0$，因此 $\\mathbf{b} = \\mathbf{0}$，精确解为 $\\mathbf{u}^\\star = \\mathbf{0}$。\n\n将一次高斯-赛德尔（GS）扫描定义如下：对于每个内部网格点 $(i,j)$，通过在该点求解 $u_{i,j}$ 的离散方程来更新 $u_{i,j}$，同时保持最近可用的相邻值固定。具体来说，局部更新为\n$$\nu_{i,j} \\leftarrow \\frac{1}{4}\\left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - h^2 f_{i,j}\\right),\n$$\n其中对于所有内部索引，$f_{i,j} = 0$。\n\n在执行每次 GS 扫描时，考虑两种未知数的排序方式：\n- 字典序：按 $i$（行索引）递增的顺序访问点 $(i,j)$，并在每行内按 $j$（列索引）递增的顺序访问，即 $(1,1)$, $(1,2)$, $\\dots$, $(1,n)$, $(2,1)$, $\\dots$, $(n,n)$。\n- 棋盘格（红黑）排序：根据 $i+j$ 的奇偶性为内部网格点着色；$i+j$ 为偶数的点是“红色”的，$i+j$ 为奇数的点是“黑色”的。一次扫描包括更新所有红色点（以任何顺序），然后更新所有黑色点（以任何顺序），使用最近可用的邻居值。\n\n对于给定的排序方式，将第 $k$ 次扫描时的残差定义为 $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{u}^{(k)}$，以及欧几里得范数 $\\|\\mathbf{r}^{(k)}\\|_2$。对于给定的排序方式和网格大小 $n$，渐近线性收敛因子将通过在固定扫描次数结束时，连续残差范数比率的几何平均值进行经验性估计。具体来说，初始猜测值设为对所有内部点 $(i,j)$，$u_{i,j}^{(0)} = 1$，执行 $K$ 次完整扫描并记录比率\n$$\n\\rho_k = \\frac{\\|\\mathbf{r}^{(k+1)}\\|_2}{\\|\\mathbf{r}^{(k)}\\|_2}, \\quad k = 0,1,\\dots,K-1.\n$$\n将估计的收敛因子定义为\n$$\nq = \\exp\\left(\\frac{1}{T}\\sum_{k=K-T}^{K-1} \\log \\rho_k\\right),\n$$\n使用最后 $T$ 个比率，前提是对于这些索引，$\\|\\mathbf{r}^{(k)}\\|_2 \\neq 0$。如果在任何时候 $\\|\\mathbf{r}^{(k)}\\|_2 = 0$，则对于该排序方式和 $n$，定义 $q = 0.0$。\n\n测试套件和要求的输出：\n- 在上述定义中使用 $K = 80$ 次扫描和 $T = 30$ 个尾部比率。\n- 使用三种网格大小 $n \\in \\{1, 8, 31\\}$。\n- 对于每个 $n$（按 $n=1$, $n=8$, $n=31$ 的顺序），计算两个浮点值：字典序的估计收敛因子 $q_{\\text{lex}}$，以及棋盘格（红黑）排序的估计收敛因子 $q_{\\text{rb}}$，两者都四舍五入到小数点后四位。\n- 边界情况约定：如果对于给定的 $n$ 和排序方式，残差范数在任何扫描中变为精确的 $0$，则该情况输出 $0.0000$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序聚合的六个结果，形式为逗号分隔的列表，并用方括号括起来，不含空格，顺序为 $[q_{\\text{lex}}(1), q_{\\text{rb}}(1), q_{\\text{lex}}(8), q_{\\text{rb}}(8), q_{\\text{lex}}(31), q_{\\text{rb}}(31)]$。例如，输出行必须看起来像 $[0.1234,0.5678,0.9012,0.3456,0.7890,0.1234]$，但使用此问题实际计算出的值。", "solution": "该问题陈述是有效的。它提出了一个明确定义的数值实验，用以经验性地估计高斯-赛德尔迭代法在求解离散化二维泊松方程时的收敛因子。该问题在科学上是合理的，内部一致，并包含了求解所需的所有必要参数和定义。\n\n该问题属于计算工程领域，特别是偏微分方程的数值解。控制方程是稳态热方程，或称泊松方程，即在单位正方形 $(0,1)\\times(0,1)$ 上的 $-\\Delta u = f(x,y)$，源项为零 $f(x,y) \\equiv 0$，并带有齐次狄利克雷边界条件 $u=0$。这种设定意味着精确解为 $u(x,y) \\equiv 0$。\n\n离散化是使用标准的五点有限差分格式在均匀网格上进行的，每个方向有 $n$ 个内部点，导致网格间距为 $h = 1/(n+1)$。这将连续问题转化为一个线性代数方程组 $A \\mathbf{u} = \\mathbf{b}$，其中 $\\mathbf{u}$ 是内部网格点上未知值 $u_{i,j}$ 的向量，右侧项为 $\\mathbf{b} = \\mathbf{0}$。矩阵 $A$ 代表负的离散拉普拉斯算子。一个众所周知的结果是，这个矩阵是对称正定的，这保证了高斯-赛德尔迭代法对于任何初始猜测都会收敛。\n\n高斯-赛德尔法是一种迭代平滑器，它使用其邻居的最新计算值来更新每个未知数 $u_{i,j}$。更新规则源自点 $(i,j)$ 处的离散方程：\n$$\n\\frac{1}{h^2}\\left(4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}\\right) = f_{i,j} = 0\n$$\n解出 $u_{i,j}$ 得到局部更新公式：\n$$\nu_{i,j} \\leftarrow \\frac{1}{4}\\left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1}\\right)\n$$\n访问点 $(i,j)$ 的顺序定义了该方法的具体变体。问题指定了两种排序方式：\n1.  字典序：逐行更新点，每行内再逐列更新。$u_{i,j}$ 的更新使用当前扫描中新计算的 $u_{i-1,j}$ 和 $u_{i,j-1}$ 的值，以及前一次扫描中 $u_{i+1,j}$ 和 $u_{i,j+1}$ 的旧值。\n2.  棋盘格（红黑）排序：网格点被划分为两个集合，“红色”点（$i+j$ 为偶数）和“黑色”点（$i+j$ 为奇数）。一次完整扫描首先更新所有红色点，然后更新所有黑色点。所有红色点可以并行更新，因为它们的值仅依赖于它们的黑色邻居。随后，所有黑色点使用它们红色邻居的新计算值进行更新。\n\n收敛率是经验性估计的。$k$ 次扫描后的残差向量是 $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{u}^{(k)} = -A \\mathbf{u}^{(k)}$。对 $K=80$ 次扫描计算连续残差欧几里得范数的比率 $\\rho_k = \\|\\mathbf{r}^{(k+1)}\\|_2 / \\|\\mathbf{r}^{(k)}\\|_2$。然后，渐近收敛因子 $q$ 被估计为最后 $T=30$ 个这些比率的几何平均值：\n$$\nq = \\exp\\left(\\frac{1}{T}\\sum_{k=K-T}^{K-1} \\ln \\rho_k\\right)\n$$\n这个经验因子 $q$ 近似于相应高斯-赛德尔迭代矩阵的谱半径 $\\rho(G)$，理论上它决定了渐近收敛速率。对于二维模型问题，理论谱半径对于字典序和红黑排序都是 $\\rho(G) = \\cos^2(\\pi h)$。然而，由于迭代矩阵的非正规性相关的瞬态效应，有限次迭代的经验估计 $q$ 在不同排序之间可能会有所不同。\n\n对于每个指定的网格大小 $n \\in \\{1, 8, 31\\}$ 和两种排序方式中的每一种，算法按以下步骤进行：\n1.  初始化一个 $(n+2) \\times (n+2)$ 的网格 $u$，边界单元设为 $0$，所有 $n^2$ 个内部单元初始化为 $1$。\n2.  计算初始残差范数 $\\|\\mathbf{r}^{(0)}\\|_2$。当 $n=1$ 时出现一个特殊情况，单次迭代即得到精确解 $\\mathbf{u}=\\mathbf{0}$，因此 $\\|\\mathbf{r}^{(1)}\\|_2 = 0$。根据问题的约定，这导致估计的收敛因子 $q=0.0$。\n3.  迭代 $K=80$ 次扫描。在每次扫描 $k$ 中：\n    a. 根据指定的排序方式（字典序或红黑），对所有内部网格点应用高斯-赛德尔更新。\n    b. 计算新的残差范数 $\\|\\mathbf{r}^{(k+1)}\\|_2$。\n    c. 如果 $\\|\\mathbf{r}^{(k+1)}\\|_2$ 为零，则该情况的计算终止，得到 $q=0.0$。否则，计算并存储比率 $\\rho_k$。\n4.  迭代结束后，如果解没有精确收敛到零，则从存储的最后 $T=30$ 个比率计算几何平均值 $q$。\n5.  对于每个 $n$，计算出的 $q_{\\text{lex}}$ 和 $q_{\\text{rb}}$ 的值四舍五入到小数点后四位，并聚合成最终的输出字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the computation for all test cases and print the final result.\n    \"\"\"\n    \n    def compute_factor(n, ordering, K, T):\n        \"\"\"\n        Computes the empirical convergence factor for a given grid size and GS ordering.\n\n        Args:\n            n (int): Number of interior grid points in each dimension.\n            ordering (str): 'lexicographic' or 'red_black'.\n            K (int): Total number of sweeps.\n            T (int): Number of tail ratios for the geometric mean.\n\n        Returns:\n            float: The estimated convergence factor q.\n        \"\"\"\n        # For n=1, the system has one unknown. GS converges in one step to the exact\n        # solution u=0. The residual becomes zero, so q=0.0 by problem definition.\n        if n == 1:\n            return 0.0\n\n        h = 1.0 / (n + 1)\n        \n        # Initialize grid u of size (n+2)x(n+2). Boundary is at index 0 and n+1.\n        u = np.zeros((n + 2, n + 2), dtype=np.float64)\n        # Set initial guess for interior points (1 to n).\n        u[1:n+1, 1:n+1] = 1.0\n\n        def calculate_residual_norm(u_grid):\n            \"\"\"Calculates the Euclidean norm of the residual vector r = -A*u.\"\"\"\n            # A*u applied to interior points\n            Au_interior = (4 * u_grid[1:n+1, 1:n+1]\n                           - u_grid[0:n, 1:n+1]    # u_{i-1,j}\n                           - u_grid[2:n+2, 1:n+1]  # u_{i+1,j}\n                           - u_grid[1:n+1, 0:n]    # u_{i,j-1}\n                           - u_grid[1:n+1, 2:n+2]) # u_{i,j+1}\n            Au_interior /= (h**2)\n            \n            return np.linalg.norm(Au_interior)\n\n        ratios = []\n        zero_residual_found = False\n\n        # Calculate initial residual norm.\n        norm_k = calculate_residual_norm(u)\n        if np.isclose(norm_k, 0.0):\n            zero_residual_found = True\n\n        if not zero_residual_found:\n            for _ in range(K):\n                # Perform one Gauss-Seidel sweep\n                if ordering == 'lexicographic':\n                    for i in range(1, n + 1):\n                        for j in range(1, n + 1):\n                            u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n                elif ordering == 'red_black':\n                    # Update \"red\" points (i+j is even)\n                    for i in range(1, n + 1):\n                        for j in range(1, n + 1):\n                            if (i + j) % 2 == 0:\n                                u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n                    # Update \"black\" points (i+j is odd)\n                    for i in range(1, n + 1):\n                        for j in range(1, n + 1):\n                            if (i + j) % 2 != 0:\n                                u[i, j] = 0.25 * (u[i-1, j] + u[i+1, j] + u[i, j-1] + u[i, j+1])\n\n                # Calculate post-sweep residual norm.\n                norm_k_plus_1 = calculate_residual_norm(u)\n                \n                if np.isclose(norm_k_plus_1, 0.0):\n                    zero_residual_found = True\n                    break\n                \n                # Store ratio and update norm for next iteration\n                ratios.append(norm_k_plus_1 / norm_k)\n                norm_k = norm_k_plus_1\n\n        if zero_residual_found:\n            return 0.0\n            \n        # Calculate the geometric mean of the last T ratios.\n        # Ratios are for k=0...K-1. We need k from K-T to K-1.\n        log_ratios_tail = np.log(ratios[K - T:])\n        q = np.exp(np.mean(log_ratios_tail))\n        \n        return q\n\n    # Define the test cases from the problem statement.\n    test_cases = [1, 8, 31]\n    K = 80\n    T = 30\n    \n    results = []\n    for n_val in test_cases:\n        # Lexicographic ordering\n        q_lex = compute_factor(n_val, 'lexicographic', K, T)\n        results.append(q_lex)\n        \n        # Red-Black ordering\n        q_rb = compute_factor(n_val, 'red_black', K, T)\n        results.append(q_rb)\n\n    # Format the results rounded to four decimal places.\n    formatted_results = [f\"{res:.4f}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406634"}, {"introduction": "虽然像高斯-赛德尔这样的基本迭代法很有启发性，但在实践中，我们通常需要更强大的工具。共轭梯度（CG）法是求解对称正定（SPD）系统的首选方法，但其收敛速度严重依赖于系统矩阵的条件数。本练习 [@problem_id:2406599] 将引导你探索如何通过预处理技术来改善矩阵性质，从而加速收敛。你将亲手实现并比較无预处理、简单的雅可比（Jacobi）预处理以及更高级的多项式预处理，见证一个好的预处理器如何能戏剧性地减少求解所需的迭代次数。", "problem": "给定一个形如 $A_n x = b$ 的对称正定线性方程组族，其中 $A_n \\in \\mathbb{R}^{n \\times n}$ 是在开区间 $(0,1)$ 上带齐次 Dirichlet 边界条件的一维负拉普拉斯算子的标准二阶有限差分格式离散。具体来说，$A_n$ 是一个三对角矩阵，其主对角线上的元素为 $2$，第一副对角线和第一超对角线上的元素为 $-1$，其余位置均为零。设 $b \\in \\mathbb{R}^n$ 是所有分量均为 $1$ 的向量，初始猜测解为 $x_0 = 0$。记 $D_n = \\mathrm{diag}(A_n)$ 为由 $A_n$ 的对角元素构成的对角矩阵，记 $R_n = D_n - A_n$ 为其严格非对角部分。对于一个非负整数 $m$，定义 $m$ 阶多项式近似逆（截断的 Neumann 级数）为\n$$\nP_m \\;=\\; \\sum_{k=0}^{m} \\left(D_n^{-1} R_n\\right)^k \\, D_n^{-1}.\n$$\n对于下文指定的每个测试用例，考虑以下三个迭代次数，每个迭代次数定义为最小的整数 $k \\in \\{0,1,2,\\dots,n\\}$，使得所述方法产生的迭代解 $x_k$ 满足相对残差 $\\|b - A_n x_k\\|_2 / \\|b\\|_2 \\le \\tau$，其中 $\\tau = 10^{-8}$，最大迭代次数为 $n$：\n$1)$ $k_{\\mathrm{unpre}}(n)$，通过生成 $A_n$-共轭搜索方向并且在连续的仿射 Krylov 子空间 $x_0 + \\mathcal{K}_k(A_n, r_0)$（其中 $r_0 = b - A_n x_0$）上最小化 $\\|b - A_n x\\|_2$ 残差的方法获得。\n$2)$ $k_{\\mathrm{jac}}(n)$，通过将相同方法应用于由对角预处理算子 $D_n^{-1}$ 定义的左预处理系统获得，即在每次迭代中使用辅助更新 $z = D_n^{-1} r$ 来代替 $z = r$。\n$3)$ $k_{\\mathrm{poly}}(n,m)$，通过将相同方法应用于由多项式算子 $P_m$ 定义的左预处理系统获得，即在每次迭代中使用辅助更新 $z = P_m r$。\n\n测试套件。对于下面的每一对 $(n,m)$，计算并报告整数三元组 $[\\,k_{\\mathrm{unpre}}(n),\\,k_{\\mathrm{jac}}(n),\\,k_{\\mathrm{poly}}(n,m)\\,]$：\n- $(n,m) = (100,0)$\n- $(n,m) = (100,3)$\n- $(n,m) = (200,3)$\n- $(n,m) = (5,10)$\n\n您的程序应生成单行输出，其中包含这些三元组的结果，形式为逗号分隔的列表，并用方括号括起来。具体而言，输出格式必须为\n$[\\,[k_{\\mathrm{unpre}}(n_1),k_{\\mathrm{jac}}(n_1),k_{\\mathrm{poly}}(n_1,m_1)],\\,[k_{\\mathrm{unpre}}(n_2),k_{\\mathrm{jac}}(n_2),k_{\\mathrm{poly}}(n_2,m_2)],\\,\\dots\\,]$，不含多余的空格或文本。", "solution": "按规定对问题进行验证。\n\n**步骤 1：提取已知条件**\n-   **线性系统**：$A_n x = b$，其中 $A_n \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）矩阵。\n-   **矩阵 $A_n$**：一个三对角矩阵，主对角线元素为 $2$，第一副对角线和第一超对角线元素为 $-1$。这代表了一维有限差分拉普拉斯算子。\n-   **向量 $b$**：$b \\in \\mathbb{R}^n$，其中对于所有 $i=1, \\dots, n$，$b_i = 1$。\n-   **初始猜测解 $x_0$**：零向量，$x_0 = 0$。\n-   **矩阵分解**：$A_n = D_n - R_n$，其中 $D_n = \\mathrm{diag}(A_n) = 2I_n$，$R_n$ 是严格非对角部分。\n-   **多项式预处理器 $P_m$**：$P_m = \\sum_{k=0}^{m} (D_n^{-1} R_n)^k D_n^{-1}$。\n-   **迭代方法**：共轭梯度（CG）法，根据其生成 $A_n$-共轭搜索方向并在 Krylov 子空间上以特定方式最小化残差的特性可以正确识别。\n-   **停止准则**：相对残差2-范数必须小于或等于容差 $\\tau = 10^{-8}$，即 $\\|b - A_n x_k\\|_2 / \\|b\\|_2 \\le \\tau$。\n-   **最大迭代次数**：$n$。\n-   **待计算量**：每个测试用例 $(n,m)$ 的三个迭代次数：\n    1.  $k_{\\mathrm{unpre}}(n)$：标准的、无预处理的 CG。\n    2.  $k_{\\mathrm{jac}}(n)$：使用 Jacobi 预处理器 $M=D_n$ 进行预处理的 CG。\n    3.  $k_{\\mathrm{poly}}(n,m)$：使用多项式预处理器 $M^{-1}=P_m$ 进行预处理的 CG。\n-   **测试用例**：$(n,m) \\in \\{(100,0), (100,3), (200,3), (5,10)\\}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n根据指定标准对问题进行评估。\n-   **科学依据**：该问题在根本上是合理的。它涉及对共轭梯度法的分析，这是数值线性代数中的一个基石算法，应用于由 Poisson 方程离散化产生的经典 SPD 矩阵。所提出的预处理器——Jacobi 和多项式（Neumann 级数）——都是标准技术。\n-   **适定性**：所有组成部分都得到了精确定义。矩阵 $A_n$ 已知为 SPD，确保了 CG 方法的适用性并将收敛到唯一解。迭代次数是由明确的停止准则决定的适定数量。多项式预处理器 $P_m$ 基于 $A_n^{-1}$ 的 Neumann 级数，对于指定的矩阵 $A_n$，该级数是收敛的，因为 Jacobi 迭代矩阵 $J=D_n^{-1}R_n$ 的谱半径为 $\\rho(J) = \\cos(\\pi/(n+1))  1$。\n-   **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观内容。\n\n**步骤 3：结论与行动**\n问题是**有效的**。这是一个计算工程领域中的适定的、有科学依据的问题。将构建一个解决方案。\n\n**方法论**\n解决方案的核心在于实现预处理共轭梯度（PCG）算法。标准的（无预处理的）CG 方法是 PCG 的一个特例，其中预处理器是单位矩阵。\n\n使用预处理器 $M$ 求解 $Ax=b$ 的 PCG 算法如下：\n1.  初始化：$k=0$， $x_0 = 0$， $r_0 = b - A x_0 = b$。\n2.  如果 $\\|r_0\\|_2 / \\|b\\|_2 \\le \\tau$，则以迭代次数 $k=0$ 终止。\n3.  求解 $z_0$: $M z_0 = r_0$。\n4.  设置搜索方向：$p_0 = z_0$。\n5.  从 $k = 0, 1, 2, \\dots$ 迭代，最多到 $n-1$：\n    a. 计算步长：$\\alpha_k = \\frac{r_k^T z_k}{p_k^T A p_k}$。\n    b. 更新解：$x_{k+1} = x_k + \\alpha_k p_k$。\n    c. 更新残差：$r_{k+1} = r_k - \\alpha_k A p_k$。\n    d. 检查收敛性：如果 $\\|r_{k+1}\\|_2 / \\|b\\|_2 \\le \\tau$，则以迭代次数 $k+1$ 终止。\n    e. 应用预处理器：求解 $M z_{k+1} = r_{k+1}$。\n    f. 计算改进因子：$\\beta_k = \\frac{r_{k+1}^T z_{k+1}}{r_k^T z_k}$。\n    g. 更新搜索方向：$p_{k+1} = z_{k+1} + \\beta_k p_k$。\n\n这一个算法将通过改变‘求解’步骤（$M z = r$）来计算所有需要的迭代次数。\n\n**预处理器实现**\n三种所需场景对应于预处理步骤 $z_k = M^{-1} r_k$ 的三种不同定义。\n\n1.  **无预处理 ($k_{\\mathrm{unpre}}$)**：这等同于设置 $M=I_n$，即单位矩阵。预处理步骤是平凡的：$z_k = r_k$。\n\n2.  **Jacobi 预处理 ($k_{\\mathrm{jac}}$)**：预处理器为 $M=D_n = \\mathrm{diag}(A_n)$。对于给定的矩阵 $A_n$，$D_n = 2I_n$。预处理步骤是一个简单的缩放：$z_k = D_n^{-1} r_k = \\frac{1}{2}r_k$。\n\n3.  **多项式预处理 ($k_{\\mathrm{poly}}$)**：预处理器由其逆定义，$M^{-1} = P_m$。预处理步骤是一个矩阵-向量乘积：$z_k = P_m r_k$。算子 $P_m$ 由下式给出\n    $$\n    P_m = \\sum_{j=0}^{m} (D_n^{-1} R_n)^j D_n^{-1}\n    $$\n    计算 $z_k = P_m r_k$ 时，可以高效地进行而无需显式构造矩阵 $P_m$。令 $T = D_n^{-1} R_n$ 且 $v = D_n^{-1} r_k$。问题简化为计算 $z_k = (\\sum_{j=0}^{m} T^j) v$。这可以通过一个迭代循环来实现：\n    -   初始化和 $s = v$。\n    -   初始化项 $t = v$。\n    -   对于 $j = 1, \\dots, m$：\n        -   更新项：$t = Tt$。\n        -   更新和：$s = s + t$。\n    -   结果是 $z_k = s$。\n    由于 $D_n = 2I_n$ 且 $R_n$ 是一个在其第一条副对角线上元素为1的稀疏矩阵，矩阵-向量乘积 $Tt$ 的计算成本很低，其浮点运算成本为 $O(n)$。应用多项式预处理器的总成本为 $O(m \\cdot n)$。\n\n对于测试用例 $(n,m)=(100,0)$，多项式预处理器 $P_0$ 简化为 $P_0 = (D_n^{-1}R_n)^0 D_n^{-1} = I_n D_n^{-1} = D_n^{-1}$。这与 Jacobi 预处理器相同。因此，我们预期 $k_{\\mathrm{jac}}(100) = k_{\\mathrm{poly}}(100,0)$，这可以作为实现的一致性检查。\n\n最终的实现将构造稀疏矩阵 $A_n$ 和 $R_n$ 以及向量 $b$，然后针对每个测试用例 $(n,m)$ 为三种预处理策略中的每一种执行 PCG 算法。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\n\ndef get_matrices(n):\n    \"\"\"Constructs the sparse matrices A_n and R_n.\"\"\"\n    # A_n is the 1D finite difference matrix: tridiagonal(-1, 2, -1)\n    diagonals_A = [-1 * np.ones(n - 1), 2 * np.ones(n), -1 * np.ones(n - 1)]\n    offsets_A = [-1, 0, 1]\n    A_n = sparse.diags(diagonals_A, offsets_A, shape=(n, n), format='csr')\n\n    # R_n = D_n - A_n. Since D_n = 2*I, R_n has 0 on diagonal, 1 on off-diagonals.\n    diagonals_R = [np.ones(n - 1), np.ones(n - 1)]\n    offsets_R = [-1, 1]\n    R_n = sparse.diags(diagonals_R, offsets_R, shape=(n, n), format='csr')\n    \n    return A_n, R_n\n\ndef poly_preconditioner_solve(r, R_matrix, m):\n    \"\"\"Computes z = P_m * r.\"\"\"\n    n = r.shape[0]\n    \n    # D_n = 2*I, so D_n_inv is a scaling by 0.5\n    d_inv_r = 0.5 * r\n    \n    # T = D_n_inv * R_n is a scaling of R_n by 0.5\n    T_mat = 0.5 * R_matrix\n\n    z = d_inv_r.copy()\n    t = d_inv_r.copy()\n    \n    for _ in range(m):\n        t = T_mat @ t\n        z += t\n        \n    return z\n\ndef pcg_solve(A, b, x_init, m_solve, tol, max_iter):\n    \"\"\"\n    Solves Ax=b using the Preconditioned Conjugate Gradient method.\n    \n    Args:\n        A: The system matrix (sparse).\n        b: The right-hand side vector.\n        x_init: The initial guess vector.\n        m_solve: A function that computes z = M_inv * r.\n        tol: The relative residual tolerance.\n        max_iter: The maximum number of iterations.\n        \n    Returns:\n        The number of iterations performed.\n    \"\"\"\n    x = x_init.copy()\n    r = b - A @ x\n    \n    norm_b = np.linalg.norm(b)\n    if norm_b == 0.0:\n        return 0\n\n    if np.linalg.norm(r) / norm_b = tol:\n        return 0\n\n    z = m_solve(r)\n    p = z.copy()\n    rz_old = r.dot(z)\n\n    for k in range(max_iter):\n        Ap = A @ p\n        alpha = rz_old / p.dot(Ap)\n        \n        x += alpha * p\n        r_new = r - alpha * Ap\n\n        if np.linalg.norm(r_new) / norm_b = tol:\n            return k + 1\n\n        z_new = m_solve(r_new)\n        rz_new = r_new.dot(z_new)\n        \n        beta = rz_new / rz_old\n        \n        p = z_new + beta * p\n        r = r_new\n        rz_old = rz_new\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (100, 0),\n        (100, 3),\n        (200, 3),\n        (5, 10),\n    ]\n\n    results = []\n    tau = 1e-8\n\n    for n, m in test_cases:\n        A_n, R_n = get_matrices(n)\n        b = np.ones(n)\n        x0 = np.zeros(n)\n        max_iter = n\n\n        # 1. Unpreconditioned CG\n        m_solve_unpre = lambda r: r\n        k_unpre = pcg_solve(A_n, b, x0, m_solve_unpre, tau, max_iter)\n        \n        # 2. Jacobi preconditioned CG\n        m_solve_jac = lambda r: 0.5 * r\n        k_jac = pcg_solve(A_n, b, x0, m_solve_jac, tau, max_iter)\n        \n        # 3. Polynomial preconditioned CG\n        m_solve_poly = lambda r: poly_preconditioner_solve(r, R_n, m)\n        k_poly = pcg_solve(A_n, b, x0, m_solve_poly, tau, max_iter)\n        \n        results.append([k_unpre, k_jac, k_poly])\n\n    # Format the output string as specified, with no extra whitespace.\n    inner_strings = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2406599"}, {"introduction": "当我们使用预处理迭代法时，通常通过监控残差的范数来判断收敛。但是，我们应该监控哪个残差——是原始的“真实”残差 $r_k = b - A x_k$，还是经过预处理的残差 $z_k = M^{-1} r_k$？本练习 [@problem_id:2406627] 构建了一个精巧的教学示例，你将看到在最速下降法中，真实残差的范数 $\\lVert r_k \\rVert_2$ 单调递减，而预处理残差的范数 $\\lVert z_k \\rVert_2$ 却并非如此。这个实践揭示了预处理方法中一个微妙但至关重要的概念，教导我们如何正确理解和解读迭代求解器的收敛行为。", "problem": "要求您构建并探究一个数学上受控的教学示例，其中用于求解线性系统的真实残差序列 $r_k$ 的欧几里得范数随迭代过程减小，而预处理残差序列 $z_k = M^{-1} r_k$ 的欧几里得范数在每一步不一定减小。其目的是从第一性原理出发，推理算法选择和线性变换如何与范数及单调性相互作用。\n\n从以下基本出发点开始：\n- 线性系统为 $A x = b$，其中 $A$ 是对称正定 (SPD) 矩阵。\n- 最小化严格凸二次函数 $f(x) = \\tfrac{1}{2} x^\\top A x - b^\\top x$ 等价于求解 $A x = b$。\n- 最速下降法使用负梯度方向来更新迭代解 $x_k$，通过选择一个沿搜索方向能使 $f$ 最小化的步长。$f$ 的梯度是 $-\\nabla f(x) = b - A x$，即真实残差 $r(x) = b - A x$。\n- 预处理器是一个线性算子 $M$，且 $M$ 是对称正定的；它对残差向量的作用由预处理残差 $z = M^{-1} r$ 表示。$z$ 的欧几里得范数通常与 $r$ 的欧几里得范数不同，并且在一个没有显式最小化该范数的算法下，它不一定是单调的。\n\n您的程序必须：\n- 为对称正定系统实现使用精确线搜索的最速下降法，从初始向量 $x_0$ 开始，并为指定的迭代次数 $K$ 生成真实残差序列 $\\{r_k\\}_{k=0}^K$。\n- 对于给定的对称正定预处理器 $M$，在每个记录的迭代索引 $k$ 处计算 $z_k = M^{-1} r_k$。\n- 对于每个测试用例，判断序列 $\\{\\lVert r_k \\rVert_2\\}_{k=0}^K$ 和序列 $\\{\\lVert z_k \\rVert_2\\}_{k=0}^K$ 是否严格递减。为此，使用固定的数值容差 $\\varepsilon$ 以避免因舍入误差导致的假阴性。对于本问题，使用 $\\varepsilon = 10^{-12}$，并且如果对于所有的 $k$ 都有 $a_{k+1} \\le (1 - \\varepsilon) a_k$，则称序列 $\\{a_k\\}$ 是严格递减的。\n- 对于每个测试用例，报告两个整数：如果被测试的序列根据上述规则是严格递减的，则为 $1$，否则为 $0$。该数对的顺序为 $[\\text{flag\\_true\\_residual}, \\text{flag\\_preconditioned\\_residual}]$。\n\n测试套件（所有矩阵和向量都已明确给出，并在适用的情况下是对称正定的）：\n- 用例 A（说明即使当 $\\lVert z_k \\rVert_2$ 不减小时，$\\lVert r_k \\rVert_2$ 也可以减小）：\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 预处理器作用 $M^{-1} = \\begin{bmatrix} 1  0.9 \\\\ 0.9  1 \\end{bmatrix}$。\n  - 迭代次数 $K = 3$。\n- 用例 B（单位预处理器，因此两个序列重合）：\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 预处理器作用 $M^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$。\n  - 迭代次数 $K = 3$。\n- 用例 C（无迭代的边界情况，单调性被“空泛地”满足）：\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$，$x_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n  - 预处理器作用 $M^{-1} = \\begin{bmatrix} 1  0.9 \\\\ 0.9  1 \\end{bmatrix}$。\n  - 迭代次数 $K = 0$。\n\n数值和实现细节：\n- 使用双精度浮点运算。\n- 对残差范数使用欧几里得范数 $\\lVert \\cdot \\rVert_2$。\n- 使用容差 $\\varepsilon = 10^{-12}$ 来判断上述指定的严格递减。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个双整数列表 $[r\\_flag, z\\_flag]$。\n- 因此，对于上述三个用例，格式是单行无空格，形式为\n  - $[[r\\_A,z\\_A],[r\\_B,z\\_B],[r\\_C,z\\_C]]$\n  其中 $r\\_A, z\\_A, r\\_B, z\\_B, r\\_C, z\\_C$ 中的每一个都是 $1$ 或 $0$。", "solution": "所述问题已经过验证，被认为是科学合理、良定且客观的。唯一解所需的所有必要数据和定义均已提供。指定为对称正定（$A$ 和 $M^{-1}$）的矩阵已被验证具有此性质。该问题是数值线性代数中的一个标准练习，旨在阐明迭代法的基本性质。因此，我们可以进行严谨的求解。\n\n该问题要求实现并分析用于求解线性系统 $A x = b$ 的最速下降法，其中 $A$ 是一个 $n \\times n$ 的对称正定 (SPD) 矩阵。该方法等价于寻找严格凸二次函数 $f(x) = \\frac{1}{2} x^\\top A x - b^\\top x$ 的最小值。此函数的梯度 $\\nabla f(x) = A x - b$ 是真实残差 $r(x) = b - A x$ 的负值。\n\n最速下降算法从一个初始猜测 $x_0$ 开始，生成一个迭代序列 $\\{x_k\\}$。在每次迭代 $k$ 中，更新沿着负梯度方向进行，即残差 $r_k = b - A x_k$ 的方向。更新规则是：\n$$\nx_{k+1} = x_k + \\alpha_k r_k\n$$\n其中步长 $\\alpha_k$ 的选择是为了最小化函数 $f$ 在搜索方向上的值。也就是说，$\\alpha_k$ 是 $\\min_{\\alpha \\in \\mathbb{R}} f(x_k + \\alpha r_k)$ 的解。为了找到这个最优的 $\\alpha_k$，我们将 $f(x_k + \\alpha r_k)$ 对 $\\alpha$ 的导数设为零：\n$$\n\\frac{d}{d\\alpha} f(x_k + \\alpha r_k) = \\nabla f(x_k + \\alpha r_k)^\\top r_k = (A(x_k + \\alpha r_k) - b)^\\top r_k = 0\n$$\n$$\n(A x_k - b + \\alpha A r_k)^\\top r_k = (-r_k + \\alpha A r_k)^\\top r_k = -r_k^\\top r_k + \\alpha r_k^\\top A r_k = 0\n$$\n由于 $A$ 是对称正定且 $r_k \\neq 0$，因此 $r_k^\\top A r_k  0$，我们可以解出 $\\alpha_k$：\n$$\n\\alpha_k = \\frac{r_k^\\top r_k}{r_k^\\top A r_k}\n$$\n这就是精确线搜索步长的公式。\n\n残差序列 $\\{r_k\\}$ 可以通过迭代计算。根据定义 $r_{k+1} = b - A x_{k+1}$ 和 $x_{k+1}$ 的更新规则，我们推导出以下递推关系：\n$$\nr_{k+1} = b - A(x_k + \\alpha_k r_k) = (b - A x_k) - \\alpha_k A r_k = r_k - \\alpha_k A r_k\n$$\n这个递推关系允许从 $r_0 = b - A x_0$ 开始直接计算残差序列。\n\n该问题要求分析两个范数序列的单调性：真实残差的欧几里得范数序列 $\\{\\|r_k\\|_2\\}_{k=0}^K$，以及预处理残差的欧几里得范数序列 $\\{\\|z_k\\|_2\\}_{k=0}^K$，其中 $z_k = M^{-1} r_k$。\n\n对于对称正定系统上的最速下降法，目标函数值序列 $f(x_k)$ 是严格递减的。这等价于误差的能量范数 $\\|x_k - x^*\\|_A^2$ 的严格递减，其中 $x^*$ 是真实解。这也意味着残差的 $A^{-1}$-范数 $\\|r_k\\|_{A^{-1}}^2 = r_k^\\top A^{-1} r_k$ 严格递减。虽然这在 $n  2$ 的一般情况下不保证欧几里得范数 $\\|r_k\\|_2$ 的严格递减，但对于给定的二维二次问题，可以证明 $\\|r_k\\|_2$ 确实是严格递减的。这与相邻残差正交的性质 $r_{k+1}^\\top r_k = 0$ 有关，对于二维空间，这意味着搜索方向在两个正交方向之间循环。\n\n预处理残差为 $z_k = M^{-1} r_k$。从 $r_k$ 到 $z_k$ 的映射是一个线性变换。最速下降法不对 $\\|z_k\\|_2$ 进行任何优化。序列 $\\{\\|z_k\\|_2\\}$ 的行为取决于变换 $M^{-1}$ 如何与残差向量序列 $\\{r_k\\}$ 相互作用。如果一个残差向量 $r_{k+1}$ 的方向与 $r_k$ 的方向相比，被 $M^{-1}$ 显著放大，那么即使 $\\|r_{k+1}\\|_2  \\|r_k\\|_2$，也有可能出现 $\\|z_{k+1}\\|_2  \\|z_k\\|_2$ 的情况。本问题就是为了展示这种现象而构建的。\n\n要实现的算法如下：\n1. 对每个测试用例，定义矩阵 $A$、$M^{-1}$、向量 $b$、$x_0$ 以及整数 $K$。\n2. 初始化两个列表 `r_norms` 和 `z_norms`，用于存储范数序列。\n3. 计算初始残差 $r_0 = b - A x_0$ 和初始预处理残差 $z_0 = M^{-1} r_0$。\n4. 计算它们的欧几里得范数 $\\|r_0\\|_2$ 和 $\\|z_0\\|_2$，并将它们附加到各自的列表中。\n5. 对于 $k$ 从 $0$ 到 $K-1$ 循环：\n   a. 计算步长 $\\alpha_k = (r_k^\\top r_k) / (r_k^\\top A r_k)$。\n   b. 计算下一个残差 $r_{k+1} = r_k - \\alpha_k A r_k$。\n   c. 计算下一个预处理残差 $z_{k+1} = M^{-1} r_{k+1}$。\n   d. 计算范数 $\\|r_{k+1}\\|_2$ 和 $\\|z_{k+1}\\|_2$ 并将它们附加到列表中。\n6. 循环结束后，分析 `r_norms` 和 `z_norms` 的单调性。如果对于所有 $j=0, \\dots, K-1$ 都有 $a_{j+1} \\le (1 - \\varepsilon) a_j$，则序列 $\\{a_j\\}_{j=0}^K$ 是严格递减的，其中给定容差 $\\varepsilon = 10^{-12}$。\n7. 如果 $K=0$，序列只有一个元素，条件空泛地为真。两个单调性标志都设置为 $1$。否则，遍历序列对并检查条件。如果有任何一对违反了该条件，则将相应的标志设置为 $0$。\n\n此过程将应用于三个测试用例中的每一个，以生成所需的输出。用例 A 旨在显示 $\\{\\|z_k\\|_2\\}$ 的非单调性。用例 B 中，$M^{-1}=I$，由于 $z_k=r_k$，将显示两个序列都是单调的。用例 C 涵盖了 $K=0$ 的边界条件。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the steepest descent method and\n    checking the monotonicity of true and preconditioned residual norms.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 4.0]]),\n            \"b\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n            \"M_inv\": np.array([[1.0, 0.9], [0.9, 1.0]]),\n            \"K\": 3,\n            \"epsilon\": 1e-12\n        },\n        # Case B\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 4.0]]),\n            \"b\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n            \"M_inv\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"K\": 3,\n            \"epsilon\": 1e-12\n        },\n        # Case C\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 4.0]]),\n            \"b\": np.array([1.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n            \"M_inv\": np.array([[1.0, 0.9], [0.9, 1.0]]),\n            \"K\": 0,\n            \"epsilon\": 1e-12\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        x0 = case[\"x0\"]\n        M_inv = case[\"M_inv\"]\n        K = case[\"K\"]\n        epsilon = case[\"epsilon\"]\n\n        r_norms = []\n        z_norms = []\n\n        # Initial step (k=0)\n        r = b - A @ x0\n        z = M_inv @ r\n        \n        r_norms.append(np.linalg.norm(r, 2))\n        z_norms.append(np.linalg.norm(z, 2))\n\n        # Iterations from k=0 to K-1\n        for _ in range(K):\n            r_dot_r = r @ r\n            Ar = A @ r\n            r_dot_Ar = r @ Ar\n            \n            # Avoid division by zero if r is already zero.\n            if abs(r_dot_Ar)  1e-15:\n                # If r is effectively zero, the sequence has converged and stays constant.\n                # This would fail the strict decrease test, but we can stop.\n                # Just append the same norm to maintain sequence length.\n                r_norms.append(r_norms[-1])\n                z_norms.append(z_norms[-1])\n                continue\n\n            alpha = r_dot_r / r_dot_Ar\n            r = r - alpha * Ar\n            z = M_inv @ r\n            \n            r_norms.append(np.linalg.norm(r, 2))\n            z_norms.append(np.linalg.norm(z, 2))\n        \n        # Check for strict decrease\n        r_flag = 1\n        z_flag = 1\n\n        if K > 0:\n            for k in range(K):\n                # Check true residual norm\n                if r_norms[k+1] > (1 - epsilon) * r_norms[k]:\n                    r_flag = 0\n                    \n                # Check preconditioned residual norm\n                if z_norms[k+1] > (1 - epsilon) * z_norms[k]:\n                    z_flag = 0\n        \n        # For K=0, the loops do not run, and the flags remain 1, which is\n        # the correct 'vacuously true' result.\n\n        results.append([r_flag, z_flag])\n\n    # Final print statement in the exact required format.\n    # The format requires no spaces, e.g., [[1,0],[1,1],[1,1]].\n    # str(results).replace(' ', '') achieves this.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "2406627"}]}