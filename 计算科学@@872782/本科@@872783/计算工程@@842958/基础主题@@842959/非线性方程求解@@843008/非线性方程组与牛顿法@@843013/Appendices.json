{"hands_on_practices": [{"introduction": "牛顿法的核心在于使用雅可比矩阵（Jacobian matrix）来线性化非线性系统。虽然精确的解析雅可比矩阵能保证方法的二次收敛性，但在实际问题中，推导其解析形式可能非常困难甚至不可行。本练习将引导你实践一种常见的替代方案——有限差分近似，并让你亲手比较解析雅可比矩阵和数值近似在收敛速度和效率上的差异。[@problem_id:2441924]", "problem": "考虑由向量值函数 $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 定义的非线性方程组，该函数由下式给出：\n$$\n\\mathbf{F}(\\mathbf{x})=\n\\begin{bmatrix}\nf_1(x_1,x_2)\\\\\nf_2(x_1,x_2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nx_1-\\cos(x_2)\\\\\nx_2-\\sin(x_1)\n\\end{bmatrix},\n$$\n其中 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ 且三角函数使用弧度制角。$\\mathbf{F}$ 的精确雅可比矩阵 $\\mathbf{J}(\\mathbf{x})$ 为\n$$\n\\mathbf{J}(\\mathbf{x})=\n\\begin{bmatrix}\n\\dfrac{\\partial f_1}{\\partial x_1}  \\dfrac{\\partial f_1}{\\partial x_2}\\\\\n\\dfrac{\\partial f_2}{\\partial x_1}  \\dfrac{\\partial f_2}{\\partial x_2}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  \\sin(x_2)\\\\\n-\\cos(x_1)  1\n\\end{bmatrix}.\n$$\n\n对于给定的扰动大小 $h>0$，定义前向有限差分雅可比近似 $\\mathbf{J}_h(\\mathbf{x})$ 为\n$$\n\\mathbf{J}_h(\\mathbf{x})=\\begin{bmatrix}\n\\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_1(\\mathbf{x})}{h}  \\displaystyle\\frac{f_1(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_1(\\mathbf{x})}{h}\\\\\n\\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_1)-f_2(\\mathbf{x})}{h}  \\displaystyle\\frac{f_2(\\mathbf{x}+h\\,\\mathbf{e}_2)-f_2(\\mathbf{x})}{h}\n\\end{bmatrix},\n$$\n其中 $\\mathbf{e}_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ 和 $\\mathbf{e}_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$。\n\n对于初始猜测 $\\mathbf{x}^{(0)}\\in\\mathbb{R}^2$，递归地定义序列 $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$ 如下：\n$$\n\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big),\\qquad \\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)},\n$$\n其中 $\\mathbf{J}_\\star$ 表示精确雅可比矩阵 $\\mathbf{J}$ 或有限差分雅可比矩阵 $\\mathbf{J}_h$，更新量 $\\mathbf{s}^{(k)}$ 是该线性系统的任意解。设残差范数为 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$，当满足 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon$ 的最小索引 $k$ 出现时，迭代终止，容差 $\\varepsilon=10^{-10}$；或在 $k_{\\max}=50$ 次迭代后终止，以先发生者为准。\n\n对于下方的每个测试用例，从给定的初始猜测 $\\mathbf{x}^{(0)}$ 开始执行两次运行：\n- 运行 A 使用 $\\mathbf{J}_\\star=\\mathbf{J}$。\n- 运行 B 使用 $\\mathbf{J}_\\star=\\mathbf{J}_h$ 并指定 $h$。\n\n对于每次运行，记录：\n- 满足 $\\|\\mathbf{F}(\\mathbf{x}^{(n)})\\|_2 \\le \\varepsilon$ 所需的迭代次数 $n$（如果未达到容差，则 $n=k_{\\max}$）。\n- 根据最后三个可用的残差范数 $\\{e_{m-2},e_{m-1},e_m\\}$ 计算的局部收敛阶估计值 $\\hat{p}$，计算公式为\n$$\n\\hat{p}=\\frac{\\ln\\left(\\dfrac{e_m}{e_{m-1}}\\right)}{\\ln\\left(\\dfrac{e_{m-1}}{e_{m-2}}\\right)},\n$$\n其中 $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$， $m$ 是运行中使用的最终迭代索引（使用终止时可用的最后三个残差；所有对数均为自然对数）。三角函数中的所有角度均为弧度。\n\n使用以下测试套件，其中每个用例是一个数对 $(\\mathbf{x}^{(0)},h)$：\n- 用例 1：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$。\n- 用例 2：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$。\n- 用例 3：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$。\n- 用例 4：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$。\n\n您的程序必须输出单行，其中包含一个结果列表，每个测试用例一个结果，顺序与所列顺序相同。每个测试用例的结果必须是包含四个条目的列表 $[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}]$，其中 $n_{\\text{exact}}$ 和 $\\hat{p}_{\\text{exact}}$ 对应于运行 A，$n_{\\text{fd}}$ 和 $\\hat{p}_{\\text{fd}}$ 对应于运行 B。最终输出格式必须是单行，形式为用方括号括起来的、由逗号分隔的各用例列表，例如 $[[a_1,a_2,a_3,a_4],[b_1,b_2,b_3,b_4],\\dots]$。", "solution": "此问题需进行验证。\n\n### 步骤 1：提取已知条件\n- **非线性系统**：函数 $\\mathbf{F}:\\mathbb{R}^2\\to\\mathbb{R}^2$ 定义为 $\\mathbf{F}(\\mathbf{x})= \\begin{bmatrix} f_1(x_1,x_2)\\\\ f_2(x_1,x_2) \\end{bmatrix} = \\begin{bmatrix} x_1-\\cos(x_2)\\\\ x_2-\\sin(x_1) \\end{bmatrix}$，其中 $\\mathbf{x}=\\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$。三角函数使用弧度制。\n- **精确雅可比矩阵**：$\\mathbf{F}$ 的雅可比矩阵为 $\\mathbf{J}(\\mathbf{x}) = \\begin{bmatrix} 1  \\sin(x_2)\\\\ -\\cos(x_1)  1 \\end{bmatrix}$。\n- **有限差分雅可比矩阵**：对于扰动 $h>0$，近似雅可比矩阵 $\\mathbf{J}_h(\\mathbf{x})$ 的第 $j$ 列由 $\\frac{\\mathbf{F}(\\mathbf{x}+h\\,\\mathbf{e}_j)-\\mathbf{F}(\\mathbf{x})}{h}$ 定义，其中 $\\mathbf{e}_j$ 是标准基向量。\n- **迭代方案**：从初始猜测 $\\mathbf{x}^{(0)}$ 开始生成序列 $\\{\\mathbf{x}^{(k)}\\}_{k\\ge 0}$，通过求解 $\\mathbf{J}_\\star\\big(\\mathbf{x}^{(k)}\\big)\\,\\mathbf{s}^{(k)}=-\\mathbf{F}\\big(\\mathbf{x}^{(k)}\\big)$ 并设置 $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\mathbf{s}^{(k)}$。此处，$\\mathbf{J}_\\star$ 是精确雅可比矩阵 $\\mathbf{J}$ 或其近似 $\\mathbf{J}_h$。\n- **终止准则**：迭代在残差范数 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2 \\le \\varepsilon = 10^{-10}$ 的最小索引 $k$ 处停止，或在 $k_{\\max}=50$ 次迭代后停止。\n- **任务**：对每个测试用例执行两次运行：运行 A 使用 $\\mathbf{J}_\\star=\\mathbf{J}$，运行 B 使用 $\\mathbf{J}_\\star=\\mathbf{J}_h$。对每次运行，记录两个量：\n    1. 迭代次数 $n$。\n    2. 收敛阶的估计值 $\\hat{p}=\\frac{\\ln(e_m/e_{m-1})}{\\ln(e_{m-1}/e_{m-2})}$，其中 $e_j=\\|\\mathbf{F}(\\mathbf{x}^{(j)})\\|_2$，$m$ 是最终迭代索引。\n- **测试用例**：\n    - 用例 1：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-6}$。\n    - 用例 2：$\\mathbf{x}^{(0)}=\\begin{bmatrix}0.5\\\\0.5\\end{bmatrix}$, $h=10^{-3}$。\n    - 用例 3：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-6}$。\n    - 用例 4：$\\mathbf{x}^{(0)}=\\begin{bmatrix}1.0\\\\1.0\\end{bmatrix}$, $h=10^{-2}$。\n- **输出格式**：一个单行列表，形式为 $[[n_{\\text{exact}},n_{\\text{fd}},\\hat{p}_{\\text{exact}},\\hat{p}_{\\text{fd}}], \\dots]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n评估问题的有效性。\n- **科学依据**：该问题是数值分析中的一个标准练习，特别是在求解非线性方程组的计算方法领域。牛顿法及其拟牛顿变体（使用有限差分雅可比矩阵）是基础且成熟的算法。数学公式是正确的。\n- **适定性**：问题定义清晰。函数、雅可比矩阵、迭代方案、终止条件以及所需输出都已明确指定。给定的方程组在感兴趣的域内有唯一解，且雅可比矩阵在解附近非奇异，确保了待解的线性系统是适定的。例如，雅可比矩阵的行列式是 $\\det(\\mathbf{J}) = 1 + \\cos(x_1)\\sin(x_2)$。对于像 $(0.5, 0.5)$ 或 $(1.0, 1.0)$ 这样的初始猜测，行列式远不为零，表明局部收敛是可实现的。\n- **客观性**：语言正式、客观，不含主观或非科学内容。\n\n该问题没有表现出任何诸如科学上不健全、不完整、矛盾或模糊等缺陷。计算 $\\hat{p}$ 至少需要三个残差范数，对应于至少两次迭代（$n\\ge 2$）。鉴于初始条件和牛顿法的性质，这是一个合理的预期。\n\n### 步骤 3：结论与行动\n此问题是**有效的**。将提供一个解决方案。\n\n---\n\n该问题要求实现牛顿法的两种变体来求解非线性系统 $\\mathbf{F}(\\mathbf{x}) = \\mathbf{0}$。第一种变体是经典的牛顿-拉夫逊方法，它使用 $\\mathbf{F}$ 的精确解析雅可比矩阵。第二种是拟牛顿法，其中雅可比矩阵通过前向有限差分格式进行近似。我们将根据收敛所需的迭代次数和估计的局部收敛阶来比较这两种方法的性能。\n\n该方法的核心是迭代更新规则。在每一步 $k$，我们用其在当前迭代点 $\\mathbf{x}^{(k)}$ 附近的线性泰勒展开来近似非线性函数 $\\mathbf{F}$：\n$$\n\\mathbf{F}(\\mathbf{x}) \\approx \\mathbf{F}(\\mathbf{x}^{(k)}) + \\mathbf{J}_\\star(\\mathbf{x}^{(k)})(\\mathbf{x} - \\mathbf{x}^{(k)})\n$$\n我们通过将此近似值设为零来寻找下一个迭代点 $\\mathbf{x}^{(k+1)}$，即 $\\mathbf{F}(\\mathbf{x}^{(k+1)}) = \\mathbf{0}$。将更新步长定义为 $\\mathbf{s}^{(k)} = \\mathbf{x}^{(k+1)} - \\mathbf{x}^{(k)}$，我们得到更新量的线性系统：\n$$\n\\mathbf{J}_\\star(\\mathbf{x}^{(k)}) \\mathbf{s}^{(k)} = -\\mathbf{F}(\\mathbf{x}^{(k)})\n$$\n一旦通过求解该系统找到 $\\mathbf{s}^{(k)}$，下一个迭代点就计算为 $\\mathbf{x}^{(k+1)} = \\mathbf{x}^{(k)} + \\mathbf{s}^{(k)}$。重复此过程，直到残差向量的范数 $\\|\\mathbf{F}(\\mathbf{x}^{(k)})\\|_2$ 小于指定的容差 $\\varepsilon = 10^{-10}$。\n\n**运行 A：精确雅可比矩阵（牛顿-拉夫逊方法）**\n在此运行中，$\\mathbf{J}_\\star$ 是精确的雅可比矩阵 $\\mathbf{J}(\\mathbf{x})$：\n$$\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{bmatrix}\n1  \\sin(x_2)\\\\\n-\\cos(x_1)  1\n\\end{bmatrix}\n$$\n当初始猜测足够接近解且雅可比矩阵在解处非奇异时，已知该方法表现出二次收敛（即收敛阶 $p=2$）。这意味着解的正确位数在每次迭代中大约翻倍。因此，估计的收敛阶 $\\hat{p}$ 应接近 2。\n\n**运行 B：有限差分雅可比矩阵（拟牛顿法）**\n在此运行中，雅可比矩阵使用前向有限差分公式进行近似。近似雅可比矩阵 $\\mathbf{J}_h(\\mathbf{x})$ 的第 $j$ 列由下式给出：\n$$\n[\\mathbf{J}_h(\\mathbf{x})]_{:,j} = \\frac{\\mathbf{F}(\\mathbf{x} + h\\mathbf{e}_j) - \\mathbf{F}(\\mathbf{x})}{h}\n$$\n其中 $\\mathbf{e}_j$ 是第 $j$ 个标准基向量，$h$ 是一个小的扰动参数。对于给定的 $2 \\times 2$ 系统，这会产生：\n$$\n\\mathbf{J}_h(\\mathbf{x}) = \\begin{bmatrix}\n\\frac{f_1(x_1+h, x_2) - f_1(x_1, x_2)}{h}  \\frac{f_1(x_1, x_2+h) - f_1(x_1, x_2)}{h} \\\\\n\\frac{f_2(x_1+h, x_2) - f_2(x_1, x_2)}{h}  \\frac{f_2(x_1, x_2+h) - f_2(x_1, x_2)}{h}\n\\end{bmatrix}\n$$\n这种方法避免了对雅可比矩阵进行解析推导的需要，对于某些函数而言，这可能很复杂或不可能。然而，它引入了近似误差。$\\mathbf{J}_h$ 中每个元素的误差约为 $O(h)$。这个误差会扰动牛顿步长，影响收敛速率。对于一个非常小的 $h$（例如 $10^{-6}$），近似是准确的，收敛应接近二次。随着 $h$ 的增加（例如 $10^{-3}$ 或 $10^{-2}$），近似变差，预计收敛速率会下降，可能变为线性收敛（$p=1$），并需要更多迭代。\n\n**算法与实现**\n对于每个测试用例 $(\\mathbf{x}^{(0)}, h)$，我们为运行 A 和运行 B 执行以下步骤：\n1. 初始化 $k=0$ 和当前解 $\\mathbf{x} = \\mathbf{x}^{(0)}$。创建一个列表来存储残差范数。\n2. 开始一个循环，只要 $k \\le k_{\\max} = 50$ 就继续。\n3. 计算残差向量 $\\mathbf{F}(\\mathbf{x})$ 及其欧几里得范数 $e_k = \\|\\mathbf{F}(\\mathbf{x})\\|_2$。存储 $e_k$。\n4. 检查终止条件：如果 $e_k \\le \\varepsilon=10^{-10}$，设置最终迭代次数 $n=k$ 并退出循环。\n5. 如果循环继续（即 $k  k_{\\max}$），计算相应的雅可比矩阵 $\\mathbf{J}_\\star(\\mathbf{x})$（精确或有限差分）。\n6. 求解线性系统 $\\mathbf{J}_\\star(\\mathbf{x})\\mathbf{s} = -\\mathbf{F}(\\mathbf{x})$ 以获得步长 $\\mathbf{s}$。\n7. 更新解：$\\mathbf{x} \\leftarrow \\mathbf{x} + \\mathbf{s}$。\n8. 增加迭代计数器：$k \\leftarrow k+1$。\n9. 如果循环因 $k$ 达到 $k_{\\max}$ 而完成，则设置 $n=k_{\\max}$。\n10. 循环终止后，使用最后三个可用的残差范数 $e_{n-2}, e_{n-1}, e_{n}$ 计算估计的收敛阶 $\\hat{p}$。如果可用的范数少于三个（即 $n  2$），则认为 $\\hat{p}$ 无法计算。\n将两次运行的结果 $(n, \\hat{p})$ 收集起来，为每个测试用例形成最终输出。这个过程将在实际计算环境中展示牛顿族方法的理论特性。", "answer": "```python\nimport numpy as np\n\n# Global constants as specified in the problem\nTOLERANCE = 1e-10\nK_MAX = 50\n\ndef F(x):\n    \"\"\"\n    Computes the vector-valued function F(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        x[0] - np.cos(x[1]),\n        x[1] - np.sin(x[0])\n    ])\n\ndef J_exact(x):\n    \"\"\"\n    Computes the exact Jacobian matrix J(x).\n    x must be a NumPy array [x1, x2].\n    \"\"\"\n    return np.array([\n        [1.0, np.sin(x[1])],\n        [-np.cos(x[0]), 1.0]\n    ])\n\ndef J_fd(x, h):\n    \"\"\"\n    Computes the forward finite-difference approximation of the Jacobian.\n    x must be a NumPy array [x1, x2].\n    h is the perturbation size.\n    \"\"\"\n    n = len(x)\n    jac = np.zeros((n, n), dtype=float)\n    fx = F(x)\n    for j in range(n):\n        x_plus_h = x.copy()\n        x_plus_h[j] += h\n        fx_plus_h = F(x_plus_h)\n        jac[:, j] = (fx_plus_h - fx) / h\n    return jac\n\ndef newton_solver(x0, h, use_exact_jacobian):\n    \"\"\"\n    Solves the nonlinear system F(x)=0 using a Newton-like method.\n    \n    Args:\n        x0 (list or np.ndarray): The initial guess.\n        h (float): The perturbation size for the finite-difference Jacobian.\n        use_exact_jacobian (bool): If True, use the exact Jacobian. \n                                  If False, use the finite-difference approximation.\n\n    Returns:\n        tuple: (n_iter, p_hat) where n_iter is the number of iterations and\n               p_hat is the estimated convergence order.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    k = 0\n    residuals = []\n    n_iter = K_MAX\n\n    while k = K_MAX:\n        F_val = F(x)\n        norm = np.linalg.norm(F_val)\n        residuals.append(norm)\n\n        if norm = TOLERANCE:\n            n_iter = k\n            break\n        \n        if k == K_MAX:\n            break\n\n        if use_exact_jacobian:\n            Jacobian = J_exact(x)\n        else:\n            Jacobian = J_fd(x, h)\n\n        try:\n            # Solve the linear system J*s = -F\n            s = np.linalg.solve(Jacobian, -F_val)\n            x += s\n        except np.linalg.LinAlgError:\n            # If Jacobian is singular, the solver fails.\n            n_iter = K_MAX\n            break\n        \n        k += 1\n    \n    # Estimate convergence order p_hat from the last three residuals\n    p_hat = np.nan\n    if len(residuals) >= 3:\n        # Use residuals e_n, e_{n-1}, e_{n-2}\n        e_m, e_m1, e_m2 = residuals[-1], residuals[-2], residuals[-3]\n        \n        # Avoid division by zero or log of non-positive numbers.\n        # Ratios must be  1 for convergence.\n        ratio1 = e_m / e_m1 if e_m1 != 0 else 0\n        ratio2 = e_m1 / e_m2 if e_m2 != 0 else 0\n\n        if 0  ratio1  1 and 0  ratio2  1:\n            p_hat = np.log(ratio1) / np.log(ratio2)\n            \n    return n_iter, p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        ([0.5, 0.5], 1e-6),\n        ([0.5, 0.5], 1e-3),\n        ([1.0, 1.0], 1e-6),\n        ([1.0, 1.0], 1e-2)\n    ]\n\n    all_results = []\n    for x0, h in test_cases:\n        # Run A: Exact Jacobian\n        n_exact, p_exact = newton_solver(x0, h, use_exact_jacobian=True)\n        \n        # Run B: Finite-Difference Jacobian\n        n_fd, p_fd = newton_solver(x0, h, use_exact_jacobian=False)\n        \n        # Assemble results for the current test case\n        case_result = [n_exact, n_fd, p_exact, p_fd]\n        all_results.append(case_result)\n\n    # The final print statement must match the format exactly.\n    # The default string representation of a list of lists is \"[...], [...]\"\n    # and the default representation of a float is what we need.\n    print(str(all_results).replace(\"], [\", \"],[\"))\n\nsolve()\n```", "id": "2441924"}, {"introduction": "在理解了雅可比矩阵的重要性后，我们来关注一个实际工程中的关键问题：计算效率。无论是解析计算还是数值近似，计算雅可比矩阵并求解线性系统通常是牛顿法中最耗时的步骤。Shamanskii方法提供了一种巧妙的折衷策略，即在多次迭代中重复使用同一个雅可bi矩阵。通过这个练习，你将学会如何权衡收敛速度与单次迭代的计算成本，这是优化大规模计算问题时的核心考量。[@problem_id:2441976]", "problem": "您需要实现一个求解器，用于求解非线性方程组。该求解器基于牛顿法的一种变体，称为 Shamanskii 方法。此计算任务纯粹是数学问题，不需要任何物理单位。所有三角函数都必须以弧度为单位解释角度。目标是从向量值函数的求根问题定义和一阶泰勒展开出发，构建此方法。该方法必须在一个“外部”迭代点计算雅可比矩阵，在指定的若干后续“内部”校正步中重用该雅可比矩阵，然后再次刷新雅可比矩阵，直至满足终止准则。\n\n从核心定义开始：给定一个连续可微函数 $F:\\mathbb{R}^n\\to\\mathbb{R}^n$，其根是一个向量 $x^\\star\\in\\mathbb{R}^n$，满足 $F(x^\\star)=0$。利用一个经过充分检验的事实：函数在点 $x\\in\\mathbb{R}^n$ 周围的一阶泰勒展开可近似为 $F(x+s)\\approx F(x)+J(x)s$，其中 $J(x)$ 是雅可比矩阵。在牛顿类方法中，通过求解从此展开式导出的线性系统来构造校正量 $s$，以减小残差 $F(x)$。在 Shamanskii 方法中，雅可比矩阵在若干内部步骤中保持不变，以分摊其计算成本：在一个外部迭代点计算雅可比矩阵及其分解一次，然后使用这个固定的雅可比矩阵执行指定数量的内部校正步骤，仅在这些内部步骤完成后或满足终止准则时才重新计算雅可比矩阵。您必须实现稳健的停止条件，当残差范数或步长范数足够小时，算法将停止。\n\n实现要求：\n- 实现一个函数，该函数给定 $F$、其雅可比矩阵 $J$、一个初始猜测值 $x_0\\in\\mathbb{R}^n$、一个重用参数 $m\\in\\mathbb{N}$、一个最大总迭代次数 $k_{\\max}\\in\\mathbb{N}$、一个残差容差 $\\varepsilon_F>0$ 以及一个步长容差 $\\varepsilon_s>0$，尝试寻找 $F(x)=0$ 的一个近似根。\n- 该方法必须按“外部”块进行：在每个块的开始，评估当前迭代点的雅可比矩阵并对其进行分解。然后使用这个固定的雅可比矩阵分解执行最多 $m$ 次“内部”校正步骤，并用当前残差更新右侧项。在 $m$ 次内部步骤后，除非方法已经收敛或达到迭代次数限制，否则通过重新计算雅可比矩阵来开始一个新的外部块。\n- 对残差范数和步长范数均使用欧几里得范数。\n- 当残差范数小于或等于 $\\varepsilon_F$ 或步长范数小于或等于 $\\varepsilon_s$ 时，成功终止。\n- 所有三角函数中的角度必须是弧度。\n- 为便于输出，将最终近似解的每个分量四舍五入到 $10$ 位小数。\n\n测试套件：\n实现您的方法，并将其应用于以下四个测试用例。对于每个用例，提供初始猜测值、系统 $F$、其雅可比矩阵 $J$、重用参数 $m$、最大总迭代次数 $k_{\\max}$ 以及容差 $(\\varepsilon_F,\\varepsilon_s)$。\n\n- 测试用例 #1（两个变量，代数，基准）：\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}x^2+y^2-1\\\\ x-y\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y)=\\begin{bmatrix}2x  2y\\\\ 1  -1\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}0.8\\\\ 0.6\\end{bmatrix}$。\n  - 重用参数 $m=1$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n- 测试用例 #2（两个变量，含三角非线性，角度以弧度为单位）：\n  - 令 $x^\\star=0.8$ 且 $y^\\star=0.6$。定义 $c=\\sin(x^\\star)+y^\\star$。\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}\\sin(x)+y-c\\\\ x^2+y^2-1\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y)=\\begin{bmatrix}\\cos(x)  1\\\\ 2x  2y\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}0.7\\\\ 0.7\\end{bmatrix}$。\n  - 重用参数 $m=3$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n- 测试用例 #3（三个变量，多项式与指数耦合，根已知）：\n  - 令 $e$ 表示自然对数的底数。定义 $F:\\mathbb{R}^3\\to\\mathbb{R}^3$，\n    $$F(x,y,z)=\\begin{bmatrix}x+y+z-3\\\\ x^2+y^2+z^2-3\\\\ e^x+yz-(e+1)\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y,z)=\\begin{bmatrix}1  1  1\\\\ 2x  2y  2z\\\\ e^x  z  y\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}$。\n  - 重用参数 $m=2$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n- 测试用例 #4（两个变量，类 Rosenbrock 系统）：\n  - $F:\\mathbb{R}^2\\to\\mathbb{R}^2$,\n    $$F(x,y)=\\begin{bmatrix}10(y-x^2)\\\\ 1-x\\end{bmatrix}.$$\n  - 雅可比矩阵，\n    $$J(x,y)=\\begin{bmatrix}-20x  10\\\\ -1  0\\end{bmatrix}.$$\n  - 初始猜测值 $x_0=\\begin{bmatrix}-1.2\\\\ 1\\end{bmatrix}$。\n  - 重用参数 $m=5$。\n  - 最大迭代次数 $k_{\\max}=50$。\n  - 容差 $\\varepsilon_F=10^{-12}$ 和 $\\varepsilon_s=10^{-12}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为方括号内包含的逗号分隔列表。\n- 每个测试用例的结果必须是最终近似解各分量的列表，每个分量四舍五入到 $10$ 位小数。例如，整体结构必须如下所示：\n  [[a11,a12],[a21,a22],[a31,a32,a33],[a41,a42]]\n其中括号和逗号的位置必须完全匹配，每个 a 值都是一个浮点数，小数点后精确打印 $10$ 位。", "solution": "用户的问题是实现用于求解非线性方程组的 Shamanskii 方法。该问题在科学上是合理的，在数学上是适定的，并为完整且明确的解决方案提供了所有必要信息。因此，我们着手进行推导和实现。\n\n核心目标是找到一个向量 $x^\\star \\in \\mathbb{R}^n$，它是一个连续可微的向量值函数 $F: \\mathbb{R}^n \\to \\mathbb{R}^n$ 的根，使得 $F(x^\\star) = 0$。\n\n此类方法的基础是函数 $F$ 在点 $x_k$ 周围的一阶泰勒级数展开。对于一个小的校正向量 $s_k$，在新点 $x_{k+1} = x_k + s_k$ 处的函数值可以通过一个线性模型来近似：\n$$F(x_k + s_k) \\approx F(x_k) + J(x_k)s_k$$\n这里，$J(x_k)$ 表示在迭代点 $x_k$ 处求值的 $F$ 的雅可比矩阵。雅可比矩阵第 $i$ 行第 $j$ 列的元素由偏导数 $J_{ij}(x) = \\frac{\\partial F_i}{\\partial x_j}(x)$ 给出。\n\n为了使函数值趋近于零，我们将线性近似设为零向量：\n$$F(x_k) + J(x_k)s_k = 0$$\n这就得到了牛顿法的基本线性系统，必须求解该系统以获得校正步长 $s_k$：\n$$J(x_k) s_k = -F(x_k)$$\n求解该系统可得到校正量，下一个迭代点定义为 $x_{k+1} = x_k + s_k$。在标准的牛顿法中，这个过程（包括计算成本高昂的雅可比矩阵 $J(x_k)$ 的求值和分解）在每一次迭代 $k$ 中都会执行。\n\nShamanskii 方法提供了一种修改，以减轻这种计算负担。它基于这样一种观察：雅可比矩阵在解附近通常变化缓慢。因此，该方法会在固定次数的后续迭代中重用先前计算的雅可比矩阵。这由一个重用参数 $m \\in \\mathbb{N}$ 控制。\n\nShamanskii 方法的算法流程如下：\n\n设 $x_0$ 为初始猜测值，$m$ 为重用参数，$k_{\\max}$ 为最大迭代次数，$\\varepsilon_F$ 和 $\\varepsilon_s$ 分别为残差范数和步长范数的容差。总迭代次数从 $k=0$ 开始。\n\n1.  **初始状态检查**：在进行任何迭代之前，算法首先检查初始猜测值 $x_0$ 是否已经是一个足够精确的根。这通过计算残差的欧几里得范数 $\\|F(x_0)\\|$ 来完成。如果 $\\|F(x_0)\\| \\le \\varepsilon_F$，则过程立即终止，返回 $x_0$ 作为解。\n\n2.  **迭代过程**：如果初始猜测值不是解，则主迭代循环开始，并只要总迭代次数 $k$ 小于 $k_{\\max}$ 就一直持续。\n\n3.  **雅可比矩阵更新（外部块）**：在“外部”迭代块的开始，更新雅可比矩阵。这发生在迭代计数器 $k$ 是重用参数 $m$ 的倍数时（即，在 $k=0, m, 2m, \\ldots$ 时）。在这些特定的迭代中，在当前迭代点 $x_k$ 处对雅可比矩阵进行求值：\n    $$J_{fixed} = J(x_k)$$\n    该矩阵立即使用带主元选择的 LU 分解进行分解，$P J_{fixed} = LU$。得到的因子 $L$ 和 $U$（以及置换矩阵 $P$）被存储起来，用于接下来的“内部”步骤。\n\n4.  **校正步骤（内部块）**：对于每次迭代 $k$（包括更新雅可比矩阵的迭代），执行以下步骤：\n    a. 计算负残差向量 $b_k = -F(x_k)$。这将作为线性系统的右侧项。\n    b. 求解线性系统 $J_{fixed} s_k = b_k$ 以获得校正向量 $s_k$。这通过使用存储的 LU 分解执行前向和后向替换来高效完成。\n    c. 通过应用校正来计算新的迭代点：$x_{k+1} = x_k + s_k$。\n    d. 迭代计数器递增：$k \\leftarrow k+1$。\n\n5.  **终止准则检查**：在计算步长 $s_{k-1}$ 并将迭代点更新为 $x_k$ 之后，算法检查是否收敛。如果满足以下两个条件之一，则过程成功终止并返回当前迭代点 $x_k$：\n    - 新点的残差的欧几里得范数在容差范围内：$\\|F(x_k)\\| \\le \\varepsilon_F$。\n    - 前一步的步长的欧几里得范数在容差范围内：$\\|s_{k-1}\\| \\le \\varepsilon_s$。\n\n6.  **失败条件**：如果循环完成，即 $k$ 达到 $k_{\\max}$，而收敛准则仍未满足，则算法终止。它返回最后计算的迭代点 $x_{k_{\\max}}$，表明方法未在指定的迭代限制内收敛。\n\n对于 $m=1$ 的特殊情况，Shamanskii 方法与标准的牛顿法相同。对于大的 $m$，它接近修正牛顿法，即在所有步骤中都使用在 $x_0$ 处计算的雅可比矩阵。因此，参数 $m$ 允许在收敛速度（通常 $m$ 较小时更快）和每次迭代的计算成本（$m$ 较大时更低）之间进行权衡。所提供的测试用例通过不同的系统和 $m$ 的选择来探究这种行为。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lu_factor, lu_solve, LinAlgError\n\ndef shamanskii_solver(F, J, x0, m, k_max, eps_F, eps_s):\n    \"\"\"\n    Implements Shamanskii's method for solving systems of nonlinear equations F(x) = 0.\n\n    Args:\n        F (callable): The vector-valued function F(x).\n        J (callable): The Jacobian function J(x).\n        x0 (list or np.ndarray): The initial guess.\n        m (int): The Jacobian reuse parameter.\n        k_max (int): Maximum number of iterations.\n        eps_F (float): Tolerance for the residual norm.\n        eps_s (float): Tolerance for the step norm.\n\n    Returns:\n        np.ndarray: The approximate solution vector.\n    \"\"\"\n    x = np.array(x0, dtype=float)\n    \n    # Initial check: if the starting point is already a solution.\n    F_val = F(x)\n    if np.linalg.norm(F_val) = eps_F:\n        return x\n\n    k = 0\n    lu_piv = None\n\n    while k  k_max:\n        # Recompute the Jacobian and its LU factorization periodically.\n        if k % m == 0:\n            J_val = J(x)\n            try:\n                # lu_factor returns (LU matrix, permutation indices)\n                lu_piv = lu_factor(J_val)\n            except LinAlgError:\n                # Jacobian is singular, cannot proceed.\n                # The method fails, return the last valid iterate.\n                return x\n\n        # F_val is from the previous iterate's end-of-loop calculation.\n        # It's -F(x_k) that forms the RHS of the linear system.\n        s = lu_solve(lu_piv, -F_val)\n\n        # Calculate the norm of the correction step.\n        step_norm = np.linalg.norm(s)\n        \n        # Update the solution vector. This is x_{k+1}.\n        x = x + s\n        k += 1\n\n        # Calculate the new residual at x_{k+1} for the next iteration's RHS\n        # and for the current iteration's convergence check.\n        F_val = F(x)\n        residual_norm = np.linalg.norm(F_val)\n\n        # Check for convergence based on residual norm or step norm.\n        if residual_norm = eps_F or step_norm = eps_s:\n            return x\n\n    # If the loop completes without convergence, return the last computed vector.\n    return x\n\ndef solve():\n    \"\"\"\n    Sets up and runs the four test cases for the Shamanskii solver.\n    \"\"\"\n    # Test Case 1\n    F1 = lambda v: np.array([v[0]**2 + v[1]**2 - 1.0, v[0] - v[1]])\n    J1 = lambda v: np.array([[2.0*v[0], 2.0*v[1]], [1.0, -1.0]])\n    x0_1 = [0.8, 0.6]\n    params1 = (F1, J1, x0_1, 1, 50, 1e-12, 1e-12)\n\n    # Test Case 2\n    x_star, y_star = 0.8, 0.6\n    c = np.sin(x_star) + y_star\n    F2 = lambda v: np.array([np.sin(v[0]) + v[1] - c, v[0]**2 + v[1]**2 - 1.0])\n    J2 = lambda v: np.array([[np.cos(v[0]), 1.0], [2.0*v[0], 2.0*v[1]]])\n    x0_2 = [0.7, 0.7]\n    params2 = (F2, J2, x0_2, 3, 50, 1e-12, 1e-12)\n\n    # Test Case 3\n    F3 = lambda v: np.array([v[0] + v[1] + v[2] - 3.0,\n                             v[0]**2 + v[1]**2 + v[2]**2 - 3.0,\n                             np.exp(v[0]) + v[1]*v[2] - (np.e + 1.0)])\n    J3 = lambda v: np.array([[1.0, 1.0, 1.0],\n                             [2.0*v[0], 2.0*v[1], 2.0*v[2]],\n                             [np.exp(v[0]), v[2], v[1]]])\n    x0_3 = [1.0, 1.0, 1.0]\n    params3 = (F3, J3, x0_3, 2, 50, 1e-12, 1e-12)\n\n    # Test Case 4\n    F4 = lambda v: np.array([10.0 * (v[1] - v[0]**2), 1.0 - v[0]])\n    J4 = lambda v: np.array([[-20.0 * v[0], 10.0], [-1.0, 0.0]])\n    x0_4 = [-1.2, 1.0]\n    params4 = (F4, J4, x0_4, 5, 50, 1e-12, 1e-12)\n\n    test_cases = [params1, params2, params3, params4]\n    \n    results = []\n    for F, J, x0, m, k_max, eps_F, eps_s in test_cases:\n        solution = shamanskii_solver(F, J, x0, m, k_max, eps_F, eps_s)\n        \n        # Format each component to 10 decimal places as a string.\n        rounded_solution = [f\"{comp:.10f}\" for comp in solution]\n        \n        # Format the list of components into the required string format, e.g., \"[1.0,2.0]\".\n        results.append(f\"[{','.join(rounded_solution)}]\")\n\n    # Print the final combined string of all results.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2441976"}, {"introduction": "掌握了牛顿法及其变体的实现技巧后，是时候将它应用于一个更真实的系统了。这个练习将挑战你为一个具有非线性供给和需求曲线的竞争市场建立模型，并求解其均衡点。你需要实现一个更稳健的牛顿法，例如结合线搜索（line search）策略，以处理实际模型带来的复杂性，并确保解的物理意义，如价格必须为正值。[@problem_id:2441954]", "problem": "考虑一个具有非线性供给和需求的双商品竞争市场。设价格向量为 $\\mathbf{p} = (p_1,p_2)$，其中 $p_1 > 0$ 且 $p_2 > 0$。商品 $1$ 和 $2$ 的需求函数由下式给出\n$$\nQ_1^d(\\mathbf{p}) = a_1 - b_1\\, p_1^{\\alpha_1} + c_{12}\\, p_2^{\\gamma_{12}}, \\quad\nQ_2^d(\\mathbf{p}) = a_2 - b_2\\, p_2^{\\alpha_2} + c_{21}\\, p_1^{\\gamma_{21}}.\n$$\n商品 $1$ 和 $2$ 的供给函数由下式给出\n$$\nQ_1^s(p_1) = d_1 + e_1\\, p_1^{\\beta_1}, \\quad\nQ_2^s(p_2) = d_2 + e_2\\, p_2^{\\beta_2}.\n$$\n均衡由强制每个商品市场出清的非线性方程组定义：\n$$\nF_1(\\mathbf{p}) = Q_1^d(\\mathbf{p}) - Q_1^s(p_1) = 0, \\quad\nF_2(\\mathbf{p}) = Q_2^d(\\mathbf{p}) - Q_2^s(p_2) = 0.\n$$\n您必须仅使用第一性原理和一般数值分析知识来计算均衡价格和数量。具体而言：\n- 从均衡的定义 $F(\\mathbf{p}) = \\mathbf{0}$ 开始，其中 $F(\\mathbf{p}) = (F_1(\\mathbf{p}),F_2(\\mathbf{p}))^\\top$。\n- 构建并实现一个迭代求根方法，该方法在当前迭代点上使用 $F$ 的一阶局域线性化来生成搜索方向，并应用步长规则以保证所有迭代值保持严格为正且残差范数减小。\n- 当残差的欧几里得范数 $\\lVert F(\\mathbf{p}) \\rVert_2$ 低于某个容差或达到最大迭代次数时，停止迭代。如果在达到最大迭代次数时仍未满足容差，则返回满足正值条件的最后一次迭代结果。\n\n单位：以货币单位 (CU) 报告价格，以数量单位 (QU) 报告数量。打印结果时，将所有报告的数字四舍五入到 $6$ 位小数。\n\n使用以下参数集测试套件。对于每种情况，使用初始猜测值 $\\mathbf{p}^{(0)} = (10,10)$、停止容差 $\\varepsilon = 10^{-10}$ 以及最大 $50$ 次外部迭代。在所有情况下，要求迭代值满足 $p_i \\ge 10^{-12}$，并在需要时使用 $1/2$ 的回溯因子进行步长缩减。\n\n- 情况 $1$（基准线，中等交叉价格效应）：\n  - $a_1 = 120.0$, $b_1 = 1.2$, $\\alpha_1 = 1.5$, $c_{12} = 0.4$, $\\gamma_{12} = 1.2$, $d_1 = 10.0$, $e_1 = 0.9$, $\\beta_1 = 1.3$。\n  - $a_2 = 100.0$, $b_2 = 1.0$, $\\alpha_2 = 1.4$, $c_{21} = 0.3$, $\\gamma_{21} = 1.1$, $d_2 = 12.0$, $e_2 = 1.1$, $\\beta_2 = 1.2$。\n\n- 情况 $2$（小市场规模，低价格）：\n  - $a_1 = 35.0$, $b_1 = 0.9$, $\\alpha_1 = 1.2$, $c_{12} = 0.15$, $\\gamma_{12} = 1.3$, $d_1 = 5.0$, $e_1 = 0.8$, $\\beta_1 = 1.1$。\n  - $a_2 = 30.0$, $b_2 = 0.7$, $\\alpha_2 = 1.25$, $c_{21} = 0.2$, $\\gamma_{21} = 1.15$, $d_2 = 4.0$, $e_2 = 0.6$, $\\beta_2 = 1.05$。\n\n- 情况 $3$（强交叉价格替代，仍为良态）：\n  - $a_1 = 80.0$, $b_1 = 1.1$, $\\alpha_1 = 1.6$, $c_{12} = 0.9$, $\\gamma_{12} = 1.05$, $d_1 = 8.0$, $e_1 = 1.0$, $\\beta_1 = 1.25$。\n  - $a_2 = 90.0$, $b_2 = 1.0$, $\\alpha_2 = 1.55$, $c_{21} = 0.8$, $\\gamma_{21} = 1.1$, $d_2 = 7.0$, $e_2 = 0.95$, $\\beta_2 = 1.2$。\n\n对于每个参数集，计算均衡价格 $\\hat{p}_1$ 和 $\\hat{p}_2$ (CU) 以及相应的均衡数量 $\\hat{q}_1$ 和 $\\hat{q}_2$ (QU)，其中 $\\hat{q}_1 = Q_1^d(\\hat{\\mathbf{p}}) = Q_1^s(\\hat{p}_1)$ 且 $\\hat{q}_2 = Q_2^d(\\hat{\\mathbf{p}}) = Q_2^s(\\hat{p}_2)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，数字四舍五入到 $6$ 位小数。按给定顺序连接各案例的结果，为每个案例附加四个数字 $\\hat{p}_1$、$\\hat{p}_2$、$\\hat{q}_1$、$\\hat{q}_2$。例如，输出结构应为如下形式\n$$\n[\\hat{p}_1^{(1)},\\hat{p}_2^{(1)},\\hat{q}_1^{(1)},\\hat{q}_2^{(1)},\\hat{p}_1^{(2)},\\hat{p}_2^{(2)},\\hat{q}_1^{(2)},\\hat{q}_2^{(2)},\\hat{p}_1^{(3)},\\hat{p}_2^{(3)},\\hat{q}_1^{(3)},\\hat{q}_2^{(3)}].\n$$", "solution": "所提出的问题在科学上是合理的，在数学上是适定的。它要求计算一个双商品市场中的经济均衡，这等同于求解一个由两个非线性方程组成的方程组的根。所规定的方法是一种一阶迭代格式，它被恰当地实现为带有线搜索的牛顿法，用于全局化并施加正值约束。我们将根据要求从第一性原理构建解决方案。\n\n该系统由市场出清条件定义：\n$$F_1(p_1, p_2) = Q_1^d(p_1, p_2) - Q_1^s(p_1) = 0$$\n$$F_2(p_1, p_2) = Q_2^d(p_1, p_2) - Q_2^s(p_2) = 0$$\n设 $\\mathbf{p} = (p_1, p_2)^\\top$ 为价格向量。该方程组可以写为 $F(\\mathbf{p}) = \\mathbf{0}$，其中 $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$。代入给定的供给和需求函数，我们得到：\n$$F_1(\\mathbf{p}) = (a_1 - d_1) - b_1 p_1^{\\alpha_1} - e_1 p_1^{\\beta_1} + c_{12} p_2^{\\gamma_{12}} = 0$$\n$$F_2(\\mathbf{p}) = (a_2 - d_2) - b_2 p_2^{\\alpha_2} - e_2 p_2^{\\beta_2} + c_{21} p_1^{\\gamma_{21}} = 0$$\n这些方程必须在 $p_1 > 0$ 和 $p_2 > 0$ 的条件下求解。\n\n问题指定了一种基于 $F(\\mathbf{p})$ 的一阶局域线性化的迭代方法。这正是牛顿法。在迭代点 $\\mathbf{p}^{(k)}$，我们通过求解由 $F$ 在 $\\mathbf{p}^{(k)}$ 周围的泰勒展开所产生的线性系统来找到搜索方向 $\\Delta\\mathbf{p}^{(k)}$：\n$$F(\\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}) \\approx F(\\mathbf{p}^{(k)}) + J_F(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}^{(k)}$$\n将左侧设为 $\\mathbf{0}$，得到牛顿步长方程：\n$$J_F(\\mathbf{p}^{(k)}) \\Delta\\mathbf{p}^{(k)} = -F(\\mathbf{p}^{(k)})$$\n其中 $J_F(\\mathbf{p})$ 是 $F$ 关于 $\\mathbf{p}$ 的雅可比矩阵。雅可比矩阵的分量是 $F_1$ 和 $F_2$ 的偏导数：\n$$J_F(\\mathbf{p}) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial p_1}  \\frac{\\partial F_1}{\\partial p_2} \\\\ \\frac{\\partial F_2}{\\partial p_1}  \\frac{\\partial F_2}{\\partial p_2} \\end{pmatrix}$$\n所需的偏导数为：\n$$\\frac{\\partial F_1}{\\partial p_1} = -b_1 \\alpha_1 p_1^{\\alpha_1 - 1} - e_1 \\beta_1 p_1^{\\beta_1 - 1}$$\n$$\\frac{\\partial F_1}{\\partial p_2} = c_{12} \\gamma_{12} p_2^{\\gamma_{12} - 1}$$\n$$\\frac{\\partial F_2}{\\partial p_1} = c_{21} \\gamma_{21} p_1^{\\gamma_{21} - 1}$$\n$$\\frac{\\partial F_2}{\\partial p_2} = -b_2 \\alpha_2 p_2^{\\alpha_2 - 1} - e_2 \\beta_2 p_2^{\\beta_2 - 1}$$\n然后，下一个迭代点通过 $\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\Delta\\mathbf{p}^{(k)}$ 求得。然而，为了确保从远离解的初始点收敛并保持价格的正值性，使用了阻尼牛顿步：\n$$\\mathbf{p}^{(k+1)} = \\mathbf{p}^{(k)} + \\lambda^{(k)} \\Delta\\mathbf{p}^{(k)}$$\n步长 $\\lambda^{(k)} \\in (0, 1]$ 由回溯线搜索确定。从 $\\lambda=1$ 开始，我们测试候选点 $\\mathbf{p}_{trial} = \\mathbf{p}^{(k)} + \\lambda \\Delta\\mathbf{p}^{(k)}$。如果满足两个条件，我们接受该步长：\n$1$。正值性：$\\mathbf{p}_{trial}$ 的所有分量必须大于或等于一个小的正阈值，指定为 $10^{-12}$。\n$2$。充分下降：试验点的残差欧几里得范数必须小于当前点的范数：$\\lVert F(\\mathbf{p}_{trial}) \\rVert_2  \\lVert F(\\mathbf{p}^{(k)}) \\rVert_2$。\n\n如果任一条件不满足，步长 $\\lambda$ 将乘以一个 $1/2$ 的回溯因子进行缩减，并重复尝试。此过程持续进行，直到找到可接受的步长。\n\n总体算法如下：\n$1$。初始化价格向量 $\\mathbf{p}^{(0)} = (10, 10)$，迭代计数器 $k=0$，容差 $\\varepsilon = 10^{-10}$，最大迭代次数 $N_{max} = 50$，最小价格界限 $p_{min} = 10^{-12}$，以及回溯因子 $\\rho = 1/2$。\n$2$。对于 $k = 0, 1, 2, \\dots, N_{max}-1$：\n    a. 计算残差向量 $F_k = F(\\mathbf{p}^{(k)})$ 及其欧几里得范数 $\\lVert F_k \\rVert_2$。\n    b. 检查收敛性：如果 $\\lVert F_k \\rVert_2  \\varepsilon$，则终止并将均衡价格设为 $\\hat{\\mathbf{p}} = \\mathbf{p}^{(k)}$。\n    c. 计算雅可比矩阵 $J_k = J_F(\\mathbf{p}^{(k)})$。\n    d. 求解线性系统 $J_k \\Delta\\mathbf{p}^{(k)} = -F_k$ 以获得牛顿方向 $\\Delta\\mathbf{p}^{(k)}$。\n    e. 初始化步长 $\\lambda = 1$。\n    f. 执行回溯线搜索：\n        i. 计算试验点 $\\mathbf{p}_{trial} = \\mathbf{p}^{(k)} + \\lambda \\Delta\\mathbf{p}^{(k)}$。\n        ii. 如果 $\\mathbf{p}_{trial}$ 的所有元素都 $\\ge p_{min}$ 并且 $\\lVert F(\\mathbf{p}_{trial}) \\rVert_2  \\lVert F_k \\rVert_2$，则接受该步长：设置 $\\mathbf{p}^{(k+1)} = \\mathbf{p}_{trial}$ 并中断线搜索。\n        iii. 否则，缩减步长 $\\lambda \\leftarrow \\rho \\lambda$。如果 $\\lambda$ 变得小于机器精度阈值，则中断线搜索以防止无限循环，并且迭代以不变的 $\\mathbf{p}$ 继续进行。\n$3$。如果循环在没有收敛的情况下完成，则将最后一次有效的迭代结果 $\\mathbf{p}^{(N_{max})}$ 作为结果。\n$4$。一旦确定了均衡价格向量 $\\hat{\\mathbf{p}} = (\\hat{p}_1, \\hat{p}_2)$，就可以使用需求函数（或等效地，供给函数）计算出相应的均衡数量：\n$$\\hat{q}_1 = Q_1^d(\\hat{\\mathbf{p}}) = a_1 - b_1 \\hat{p}_1^{\\alpha_1} + c_{12} \\hat{p}_2^{\\gamma_{12}}$$\n$$\\hat{q}_2 = Q_2^d(\\hat{\\mathbf{p}}) = a_2 - b_2 \\hat{p}_2^{\\alpha_2} + c_{21} \\hat{p}_1^{\\gamma_{21}}$$\n将对所提供的三个参数集中的每一个实施此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve for market equilibrium for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple contains parameters in the order:\n    # (a1, b1, alpha1, c12, gamma12, d1, e1, beta1,\n    #  a2, b2, alpha2, c21, gamma21, d2, e2, beta2)\n    test_cases = [\n        (120.0, 1.2, 1.5, 0.4, 1.2, 10.0, 0.9, 1.3,\n         100.0, 1.0, 1.4, 0.3, 1.1, 12.0, 1.1, 1.2),\n        (35.0, 0.9, 1.2, 0.15, 1.3, 5.0, 0.8, 1.1,\n         30.0, 0.7, 1.25, 0.2, 1.15, 4.0, 0.6, 1.05),\n        (80.0, 1.1, 1.6, 0.9, 1.05, 8.0, 1.0, 1.25,\n         90.0, 1.0, 1.55, 0.8, 1.1, 7.0, 0.95, 1.2)\n    ]\n    \n    # Common numerical parameters for the solver\n    p0 = np.array([10.0, 10.0])\n    tol = 1e-10\n    max_iter = 50\n    min_p_val = 1e-12\n    backtrack_factor = 0.5\n    \n    results = []\n    for params in test_cases:\n        p_hat = find_equilibrium(params, p0, tol, max_iter, min_p_val, backtrack_factor)\n        \n        # Unpack parameters for quantity calculation\n        a1, b1, alpha1, c12, gamma12, _, _, _, \\\n        a2, b2, alpha2, c21, gamma21, _, _, _ = params\n        p1, p2 = p_hat\n        \n        # Calculate equilibrium quantities using demand functions\n        q1_hat = a1 - b1 * p1**alpha1 + c12 * p2**gamma12\n        q2_hat = a2 - b2 * p2**alpha2 + c21 * p1**gamma21\n        \n        results.extend([p1, p2, q1_hat, q2_hat])\n\n    # Format the final output string with numbers rounded to 6 decimal places.\n    output_str = f\"[{','.join([f'{x:.6f}' for x in results])}]\"\n    print(output_str)\n\ndef find_equilibrium(params, p_init, tol, max_iter, min_p, backtrack_factor):\n    \"\"\"\n    Implements Newton's method with backtracking to find equilibrium prices.\n    \n    Args:\n        params (tuple): A tuple of all model parameters.\n        p_init (np.ndarray): Initial guess for the price vector.\n        tol (float): Convergence tolerance for the residual norm.\n        max_iter (int): Maximum number of iterations.\n        min_p (float): Minimum allowed price value.\n        backtrack_factor (float): Factor for reducing step size in line search.\n\n    Returns:\n        np.ndarray: The equilibrium price vector.\n    \"\"\"\n    p = np.copy(p_init)\n    \n    # Unpack parameters\n    a1, b1, alpha1, c12, gamma12, d1, e1, beta1, \\\n    a2, b2, alpha2, c21, gamma21, d2, e2, beta2 = params\n    \n    def F(pr):\n        p1, p2 = pr[0], pr[1]\n        f1 = (a1 - d1) - b1 * p1**alpha1 - e1 * p1**beta1 + c12 * p2**gamma12\n        f2 = (a2 - d2) - b2 * p2**alpha2 - e2 * p2**beta2 + c21 * p1**gamma21\n        return np.array([f1, f2])\n\n    def J(pr):\n        p1, p2 = pr[0], pr[1]\n        j11 = -b1 * alpha1 * p1**(alpha1 - 1) - e1 * beta1 * p1**(beta1 - 1)\n        j12 = c12 * gamma12 * p2**(gamma12 - 1)\n        j21 = c21 * gamma21 * p1**(gamma21 - 1)\n        j22 = -b2 * alpha2 * p2**(alpha2 - 1) - e2 * beta2 * p2**(beta2 - 1)\n        return np.array([[j11, j12], [j21, j22]])\n\n    for _ in range(max_iter):\n        F_val = F(p)\n        res_norm = np.linalg.norm(F_val)\n        \n        if res_norm  tol:\n            break\n            \n        J_val = J(p)\n        try:\n            delta_p = np.linalg.solve(J_val, -F_val)\n        except np.linalg.LinAlgError:\n            # Jacobian is singular, cannot proceed with Newton step.\n            # This indicates a problem; break and return the current best guess.\n            break\n\n        # Backtracking line search\n        lambda_step = 1.0\n        while lambda_step > 1e-8: # Prevent excessively small steps\n            p_trial = p + lambda_step * delta_p\n            if np.all(p_trial >= min_p):\n                res_norm_trial = np.linalg.norm(F(p_trial))\n                if res_norm_trial  res_norm:\n                    p = p_trial\n                    break  # Step accepted\n            \n            lambda_step *= backtrack_factor\n        else: # This else belongs to the while loop, executes if loop finishes without break\n            # No acceptable step found, break the main loop\n            break\n    \n    return p\n\nsolve()\n```", "id": "2441954"}]}