{"hands_on_practices": [{"introduction": "每次信赖域迭代的核心是求解子问题：在给定的可信半径内，最小化目标函数的二次模型。这个基础练习将引导你求解一个简单的一维子问题。通过这个过程，你将理解如何确定最优步长 $p$，无论是区域内部的无约束极小值点，还是位于其边界上的有约束步长。[@problem_id:2224504]", "problem": "在无约束优化的背景下，信赖域方法通过在当前点周围使用一个更简单的模型函数 $m(p)$ 来迭代地近似一个复杂的函数。下一步的步长 $p$ 是通过求解信赖域子问题来确定的，该子问题涉及在一个半径为 $\\Delta  0$ 的“信赖域”内最小化此模型。在该区域内，该模型被认为是原始函数的可靠近似。该子问题可正式表述为：\n$$\n\\min_{p} m(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta\n$$\n考虑一个一维优化场景，其中对于步长 $p \\in \\mathbb{R}$ 的模型函数是一个二次函数，由下式给出：\n$$\nm(p) = g p + \\frac{1}{2} H p^2 + c\n$$\n其中梯度项 $g = 2$，Hessian项 $H = 6$，以及一个任意常数 $c$。步长受限于一个信赖域半径 $\\Delta = 0.1$。\n\n确定解决此一维信賴域子问题的最优步长 $p$。请以精确小数形式提供您的答案。", "solution": "我们必须在信赖域约束 $|p| \\le \\Delta$ 下最小化二次模型 $m(p) = g p + \\frac{1}{2} H p^{2} + c$，其中给定值为 $g=2$，$H=6$ 和 $\\Delta=0.1$。常数 $c$ 不影响最小值点，可以忽略。\n\n考虑信赖域子问题的拉格朗日函数：\n$$\n\\mathcal{L}(p,\\lambda) = g p + \\frac{1}{2} H p^{2} + \\lambda \\left(p^{2} - \\Delta^{2}\\right),\n$$\n其中 $\\lambda \\ge 0$。Karush-Kuhn-Tucker (KKT) 条件如下：\n1. 平稳性条件：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = g + H p + 2 \\lambda p = 0.\n$$\n2. 原始可行性：$|p| \\le \\Delta$。\n3. 对偶可行性：$\\lambda \\ge 0$。\n4. 互补松弛性：$\\lambda \\left(p^{2} - \\Delta^{2}\\right) = 0$。\n\n情况1（内部解）：如果 $|p|  \\Delta$，那么 $\\lambda = 0$，平稳性条件给出\n$$\ng + H p = 0 \\quad \\Rightarrow \\quad p_{u} = -\\frac{g}{H}.\n$$\n对于 $g=2$ 和 $H=6$，这给出 $p_{u} = -\\frac{2}{6} = -\\frac{1}{3}$。检查可行性：$|p_{u}| = \\frac{1}{3}  0.1 = \\Delta$，因此内部解是不可行的。\n\n情况2（边界解）：此时 $p^{2} = \\Delta^{2}$，所以 $p = \\pm \\Delta$。平稳性条件变为\n$$\ng + H p + 2 \\lambda p = 0 \\quad \\Rightarrow \\quad (H + 2 \\lambda) p = -g.\n$$\n因为 $H + 2 \\lambda \\ge 0$，所以 $p$ 的符号必须与 $-g$ 的符号相匹配。当 $g = 2  0$ 时，我们必须取 $p = -\\Delta = -0.1$。为了验证对偶可行性，求解 $\\lambda$：\n$$\n(H + 2 \\lambda)(-\\Delta) = -g \\quad \\Rightarrow \\quad (H + 2 \\lambda)\\Delta = g \\quad \\Rightarrow \\quad 2 \\lambda = \\frac{g}{\\Delta} - H.\n$$\n代入 $g=2$，$\\Delta=0.1$ 和 $H=6$ 得\n$$\n2 \\lambda = \\frac{2}{0.1} - 6 = 20 - 6 = 14 \\quad \\Rightarrow \\quad \\lambda = 7 \\ge 0,\n$$\n这满足了对偶可行性和互补松弛性。\n\n因此，最优信赖域步长是在负梯度方向上的边界步长：\n$$\np^{\\star} = -\\Delta = -0.1.\n$$", "answer": "$$\\boxed{-0.1}$$", "id": "2224504"}, {"introduction": "计算出试探步长后，我们如何判断这一步的好坏？信赖域算法通过比较模型预测的改善量与目标函数的实际改善量来回答这个问题。本练习专注于计算“实际下降量”（actual reduction），这是决定接受或拒绝步长，并调整下一次迭代信赖域大小 $\\Delta$ 的关键指标。[@problem_id:2224484]", "problem": "在信赖域方法的无约束优化背景下，我们的目标是最小化目标函数 $f(x)$。在给定的第 $k$ 次迭代中，我们位于点 $x_k$ 处，并通过求解信赖域子问题计算出了一个试验步 $p_k$。评估该步质量的一个关键指标是目标函数的实际下降量，定义为 $\\text{ared}_k = f(x_k) - f(x_k + p_k)$。\n\n考虑一维目标函数 $f(x) = \\cos(x)$。假设在当前迭代中，我们位于点 $x_k = 0$ 处，且算法提出了一个试验步 $p_k = 0.1$。计算此步的实际下降量 $\\text{ared}_k$。所有角度均以弧度为单位。将最终答案以科学记数法表示，并四舍五入到三位有效数字。", "solution": "我们使用信赖域方法中实际下降量的定义：\n$$\\text{ared}_{k} = f(x_{k}) - f(x_{k} + p_{k}).$$\n给定 $f(x) = \\cos(x)$，$x_{k} = 0$ 以及 $p_{k} = 0.1$ (弧度)，我们有\n$$\\text{ared}_{k} = \\cos(0) - \\cos(0.1).$$\n因为 $\\cos(0) = 1$，上式变为\n$$\\text{ared}_{k} = 1 - \\cos(0.1).$$\n为了计算 $\\cos(0.1)$，使用其在 $0$ 点的泰勒级数展开：\n$$\\cos(x) = 1 - \\frac{x^{2}}{2} + \\frac{x^{4}}{24} - \\frac{x^{6}}{720} + \\cdots.$$\n代入 $x = 0.1$，\n$$\\cos(0.1) \\approx 1 - \\frac{(0.1)^{2}}{2} + \\frac{(0.1)^{4}}{24} - \\frac{(0.1)^{6}}{720} = 1 - 5.0 \\times 10^{-3} + 4.166\\overline{6} \\times 10^{-6} - 1.388\\overline{8} \\times 10^{-9}.$$\n因此，\n$$\\cos(0.1) \\approx 0.9950041653,$$\n并且\n$$\\text{ared}_{k} = 1 - \\cos(0.1) \\approx 0.0049958347.$$\n用科学记数法表示并四舍五入到三位有效数字，得到\n$$\\text{ared}_{k} \\approx 5.00 \\times 10^{-3}.$$", "answer": "$$\\boxed{5.00 \\times 10^{-3}}$$", "id": "2224484"}, {"introduction": "最后的这个练习将前面的概念整合成一个完整、可运行的算法。通过代码实现一个简单的信赖域方法，你将亲眼看到求解子问题和评估步长质量如何在一个迭代过程中协同工作。这个练习突出了该方法的稳健性，展示了它如何智能地调整自身以找到最小值，成功地通过其他方法可能失败的复杂曲率区域。[@problem_id:2461247]", "problem": "考虑以无量纲单位表示的单变量目标函数 $f(x)=x^4-x^2$，其一阶导数为 $f'(x)=4x^3-2x$，二阶导数为 $f''(x)=12x^2-2$。驻点为位于 $x=0$ 处的局部极大值点和位于 $x_{\\pm}=\\pm 1/\\sqrt{2}$ 处的局部极小值点。对于一个迭代点 $x_k\\in\\mathbb{R}$，定义二次模型 $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ 和信赖域半径 $\\Delta_k0$。信赖域子问题是找到一个步长 $s_k\\in\\mathbb{R}$，它在约束 $\\lvert s\\rvert\\le \\Delta_k$ 下最小化 $m_k(s)$。设预测减少量为 $\\operatorname{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$，实际减少量为 $\\operatorname{ared}_k=f(x_k)-f(x_k+s_k)$。当 $\\operatorname{pred}_k0$ 时，定义接受率 $\\rho_k=\\operatorname{ared}_k/\\operatorname{pred}_k$；如果 $\\operatorname{pred}_k\\le 0$，则设 $\\rho_k=-\\infty$。如果 $\\rho_k\\ge \\eta_1$，则接受步长，否则拒绝。信赖域半径根据以下规则更新，其中参数 $\\eta_1\\in(0,1)$、$\\eta_2\\in(\\eta_1,1)$、$\\gamma_1\\in(0,1)$ 和 $\\gamma_21$ 为固定值：\n- 如果 $\\rho_k  \\eta_1$，设置 $\\Delta_{k+1}=\\gamma_1\\Delta_k$。\n- 如果 $\\rho_k\\ge \\eta_2$ 且 $\\lvert s_k\\rvert\\ge 0.8\\,\\Delta_k$，设置 $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$。\n- 否则，设置 $\\Delta_{k+1}=\\Delta_k$。\n如果一个步长被拒绝，则保持 $x_{k+1}=x_k$；如果被接受，则设置 $x_{k+1}=x_k+s_k$。当 $\\lvert f'(x_k)\\rvert\\le \\varepsilon_g$、对于一个接受的步长有 $\\lvert s_k\\rvert\\le \\varepsilon_s$、$\\Delta_k\\le \\varepsilon_s$ 或达到固定的迭代次数上限时，迭代终止。\n\n在一维空间中，二次模型在约束 $\\lvert s\\rvert\\le \\Delta$ 下的唯一全局极小化子可按如下方式表征。对于给定的 $g\\in\\mathbb{R}$ 和 $h\\in\\mathbb{R}$，其中 $g=f'(x)$ 和 $h=f''(x)$，\n- 如果 $h0$ 且无约束极小化子 $s_N=-g/h$ 满足 $\\lvert s_N\\rvert\\le \\Delta$，则 $s^\\star=s_N$；否则 $s^\\star=-\\operatorname{sign}(g)\\,\\Delta$。\n- 如果 $h\\le 0$，则当 $g0$ 时 $s^\\star=\\Delta$，当 $g>0$ 时 $s^\\star=-\\Delta$，当 $g=0$ 时 $s^\\star=\\Delta$。\n\n固定数值参数 $\\eta_1=0.1$、$\\eta_2=0.9$、$\\gamma_1=0.25$、$\\gamma_2=2$、$\\Delta_{\\max}=10$、梯度容差 $\\varepsilon_g=10^{-8}$、步长容差 $\\varepsilon_s=10^{-10}$，以及最大迭代次数 $100$ 次。对于下面的每个测试用例，从给定的初始点 $x_0$ 和给定的初始半径 $\\Delta_0$ 开始，并在每一步应用上述迭代，使用一维模型极小化子 $s_k$。\n\n为每个测试用例定义三个标量输出：\n- $b_1$：一个逻辑值，当且仅当所有接受的步长都不会导致目标函数值增加时为真，即对于所有接受的步长 $k$，都有 $f(x_{k+1})\\le f(x_k)$。\n- $b_2$：一个逻辑值，当且仅当在第一次迭代时的完全无约束牛顿步 $s_N=-f'(x_0)/f''(x_0)$（当 $f''(x_0)\\ne 0$ 时；如果 $f''(x_0)=0$，则定义 $b_2$ 为假）会产生一个更高的目标函数值时为真，即 $f(x_0+s_N)f(x_0)$。\n- $d$：最终接受的迭代点 $x_{\\mathrm{final}}$ 与最近的局部极小点之间的绝对距离，即 $d=\\min\\{\\lvert x_{\\mathrm{final}}-1/\\sqrt{2}\\rvert,\\lvert x_{\\mathrm{final}}+1/\\sqrt{2}\\rvert\\}$。\n\n需要评估的测试参数套件：\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，输出一个列表 $[b_1,b_2,d]$，其中 $b_1$ 和 $b_2$ 是小写字符串 \"true\" 或 \"false\"，$d$ 是一个小数点后恰好有六位数字的十进制舍入浮点数。因此，最终输出必须是以下形式的单行文本：\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$，\n该行中任何地方都不得有空格。", "solution": "该问题是有效的。它提出了一个适定、自洽且科学严谨的数值优化练习，具体涉及将信赖域算法应用于一维势能函数。所有必要的参数、算法规则、函数和终止准则都以数学精度给出。我们将继续提供一个完整的解决方案。\n\n此问题的核心是实现并分析一个信赖域优化算法。这类方法是计算科学的基础，尤其在计算化学中用于定位稳定的分子构型，这些构型对应于势能面上的极小点。目标函数 $f(x) = x^4 - x^2$ 是一个典型的一维双阱势，代表一个具有两个稳定态（极小点）$x_{\\pm} = \\pm 1/\\sqrt{2}$ 和一个不稳定过渡态（极大点）$x=0$ 的系统。\n\n信赖域算法通过构建一个简化的目标函数模型来迭代地寻找极小点，该模型仅在当前迭代点 $x_k$ 的一个邻域内是可信的。这个邻域是一个半径为 $\\Delta_k$ 的“信赖域”。该模型是一个二次函数 $m_k(s)$，由 $f(x)$ 在 $x_k$ 附近的二阶泰勒展开式导出：\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\n其中 $s$ 是从 $x_k$ 出发的步长。该模型在约束 $\\lvert s \\rvert \\le \\Delta_k$ 下关于 $s$ 进行最小化。这种约束最小化问题被称为信赖域子问题。\n\n如问题所述，一维子问题的解取决于模型的曲率，由二阶导数 $h = f''(x_k)$ 给出。\n1.  如果 $h  0$，模型是凸的（一个开口向上的抛物线）。无约束极小化子是牛顿步 $s_N = -g/h$，其中 $g = f'(x_k)$。如果此步长在信赖域内，即 $\\lvert s_N \\rvert \\le \\Delta_k$，它就是最优步长 $s_k$。否则，模型在信赖域的边界处取得最小值，即 $s_k = -\\operatorname{sign}(g)\\Delta_k$，这表示在最速下降方向上尽可能地移动。\n2.  如果 $h \\le 0$，模型是局部凹的或线性的。它在区间 $[-\\Delta_k, \\Delta_k]$ 上的最小值必定在其中一个边界上。在 $s = \\Delta_k$ 和 $s = -\\Delta_k$ 之间的选择由梯度 $g$ 的符号决定，梯度符号指示了下降的方向。\n\n一旦计算出试验步长 $s_k$，就需要通过比较目标函数的*实际减少量* $\\operatorname{ared}_k = f(x_k) - f(x_k + s_k)$ 和模型的*预测减少量* $\\operatorname{pred}_k = m_k(0) - m_k(s_k)$ 来评估其质量。它们的比率 $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$ 衡量了模型的保真度。\n\n-   如果 $\\rho_k$ 接近 1，说明模型是一个极好的预测器。步长被接受，并且我们可以扩大信赖域（$\\Delta_{k+1} = \\gamma_2 \\Delta_k$）以允许更激进的步长，前提是当前步长已经接近信赖域边界。\n-   如果 $\\rho_k$ 为正但不大，说明模型是充分的。步长被接受，但信赖域大小保持不变（$\\Delta_{k+1} = \\Delta_k$）。\n-   如果 $\\rho_k$ 很小或为负，说明模型很差。步长被拒绝（$x_{k+1} = x_k$），并且信赖域被缩小（$\\Delta_{k+1} = \\gamma_1 \\Delta_k$），以在后续迭代中提高模型精度。\n\n步长接受规则是 $\\rho_k \\ge \\eta_1$。由于 $\\eta_1 = 0.1  0$ 且子问题的解保证了 $\\operatorname{pred}_k \\ge 0$，任何接受的步长都必须满足 $\\operatorname{ared}_k \\ge \\eta_1 \\operatorname{pred}_k \\ge 0$。如果 $\\operatorname{pred}_k  0$，则 $\\operatorname{ared}_k  0$，这保证了 $f(x_{k+1})  f(x_k)$。只有当模型预测下降时（$\\operatorname{pred}_k  0$），步长才可能被接受。因此，$b_1$ 的条件——即没有一个接受的步长会增加目标函数值——是由该算法的构造本身所保证的。任何偏差都将表明实现存在缺陷。\n\n输出 $b_2$ 探究了纯牛顿-拉夫逊方法的局限性。无约束牛顿步 $s_N = -f'(x_0)/f''(x_0)$ 寻找二次模型的极值点。在极大值点附近，如在 $x_0=0.0$ 或 $x_0=0.1$ 处，海森矩阵（此处为二阶导数）$f''(x_0)$ 是负的。因此，牛顿步会寻找局部二次模型的*最大值*，这对于最小化全局函数 $f(x)$ 来说是一个糟糕的策略，并很可能导致一个上升步，即 $f(x_0+s_N)  f(x_0)$。信赖域框架通过约束步长大小来纠正这一缺陷。\n\n最终输出 $d$ 衡量了收敛到其中一个真实极小点 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 的精度。鉴于所有初始点都是非负的，预计算法将收敛到正的极小点 $x_+ = 1/\\sqrt{2}$。\n\n实现将首先定义目标函数及其导数。然后，对每个测试用例，计算 $b_2$ 的值。接着执行主迭代循环，该循环在每一步 $k$ 中包括：检查终止条件，求解信赖域子问题以得到 $s_k$，通过 $\\rho_k$ 评估步长质量，并根据指定规则更新状态变量 $x_k$ 和 $\\Delta_k$。在整个迭代过程中跟踪 $b_1$ 的值。终止时，计算最终距离 $d$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region optimization problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x):\n        return x**4 - x**2\n\n    def f_prime(x):\n        return 4 * x**3 - 2 * x\n\n    def f_double_prime(x):\n        return 12 * x**2 - 2\n\n    # --- Algorithm Parameters ---\n    eta1 = 0.1\n    eta2 = 0.9\n    gamma1 = 0.25\n    gamma2 = 2.0\n    delta_max = 10.0\n    eps_g = 1e-8\n    eps_s = 1e-10\n    max_iter = 100\n    \n    minimizers = [-1/np.sqrt(2), 1/np.sqrt(2)]\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.1, 0.05),\n        (0.1, 2.0),\n        (0.0, 0.5),\n        (0.1, 0.001)\n    ]\n\n    results = []\n\n    for x0, delta0 in test_cases:\n        # --- Output variable initialization ---\n        b1_flag = True\n        \n        # --- Calculate b2 before starting iterations ---\n        g0 = f_prime(x0)\n        h0 = f_double_prime(x0)\n        \n        if h0 == 0:\n            b2 = False\n        else:\n            s_N = -g0 / h0\n            b2 = f(x0 + s_N) > f(x0)\n            \n        # --- Main Trust-Region Loop ---\n        x_k = x0\n        delta_k = delta0\n        final_x = x0 # Will hold the last accepted iterate value\n        \n        for _ in range(max_iter):\n            g_k = f_prime(x_k)\n            \n            # --- Termination check 1: Gradient ---\n            if abs(g_k) = eps_g:\n                final_x = x_k\n                break\n\n            # --- Solve the trust-region subproblem ---\n            h_k = f_double_prime(x_k)\n            s_k = 0.0\n            \n            if h_k > 0:\n                s_N = -g_k / h_k\n                if abs(s_N) = delta_k:\n                    s_k = s_N\n                else:\n                    s_k = -np.sign(g_k) * delta_k\n            else: # h_k = 0\n                if g_k  0:\n                    s_k = delta_k\n                elif g_k > 0:\n                    s_k = -delta_k\n                else: # g_k == 0\n                    s_k = delta_k\n\n            # --- Evaluate step quality ---\n            pred_k = -(g_k * s_k + 0.5 * h_k * s_k**2)\n            ared_k = f(x_k) - f(x_k + s_k)\n            \n            rho_k = 0.0\n            # Use small tolerance for pred_k to avoid division by zero instability\n            if pred_k > 1e-12: # Check for meaningful predicted reduction\n                rho_k = ared_k / pred_k\n            else:\n                rho_k = -np.inf # Model predicts no improvement or a trivial step\n\n            delta_kp1 = delta_k\n            \n            # --- Step acceptance/rejection and state update ---\n            if rho_k >= eta1: # Accept step\n                if f(x_k + s_k) > f(x_k):\n                    b1_flag = False\n                \n                x_k += s_k\n                final_x = x_k\n                \n                # --- Termination check 2: Step size for an accepted step ---\n                if abs(s_k) = eps_s:\n                    break\n                    \n                # --- Update trust radius (for accepted step) ---\n                if rho_k >= eta2 and abs(s_k) >= 0.8 * delta_k:\n                    delta_kp1 = min(gamma2 * delta_k, delta_max)\n                # else: delta_kp1 remains delta_k\n                \n            else: # Reject step\n                # x_k remains the same\n                delta_kp1 = gamma1 * delta_k\n            \n            delta_k = delta_kp1\n            \n            # --- Termination check 3: Trust radius size ---\n            if delta_k = eps_s:\n                break\n        \n        # --- Calculate final distance d ---\n        d = min(abs(final_x - m) for m in minimizers)\n        \n        # Format results for the current test case\n        b1_str = \"true\" if b1_flag else \"false\"\n        b2_str = \"true\" if b2 else \"false\"\n        d_str = \"{:.6f}\".format(d)\n        results.append(f'[{b1_str},{b2_str},{d_str}]')\n        \n    # --- Final Print ---\n    # The final output is a single line, formatted as a list of lists.\n    print(f\"[[{','.join(results)}]]\")\n\nsolve()\n```", "id": "2461247"}]}