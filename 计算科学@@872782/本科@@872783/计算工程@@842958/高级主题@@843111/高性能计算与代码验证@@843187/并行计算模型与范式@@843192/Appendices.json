{"hands_on_practices": [{"introduction": "在并行计算中，确保程序的正确性是首要且最关键的一步。本练习将引导你亲手实践并行编程中最基本也最常见的陷阱——数据竞争。通过在一个并发哈希表中故意制造并观察一个数据竞争，你将深刻理解其如何破坏程序的正确性，并学会使用互斥锁和原子操作等基本同步工具来修复它，从而为编写正确且可靠的并行程序打下坚实的基础。[@problem_id:2422625]", "problem": "创建一个完整的、可运行的程序，该程序演示哈希表中的数据竞争，然后使用两种不同的同步范式消除它。该程序必须实现一个固定容量、开放寻址的哈希表，用于存储整数键和整数值。设容量为一个正整数 $m$。哈希函数为 $h(k) = k \\bmod m$，冲突通过线性探测解决。该表必须支持一个操作：给定一个键 $k$，将其关联的值增加 1。\n\n精确指定语义如下：\n- 哈希表是一个从整数到整数的映射。对于给定的键 $k$，由 $T$ 个线程每个执行 $R$ 次并发增量操作，其数学上正确的效果是，最终存储的值等于 $T \\times R$，前提是该值初始化为 $0$ 且没有其他操作发生。\n- 未同步的增量操作实现会执行读取当前值、通过加 1 计算后继值、然后将结果写回的操作序列，此读-改-写序列没有任何同步保护。\n- 互斥（mutex）实现使用单个全局互斥锁，以确保每次增量操作都作为一个不可分割的临界区执行。\n- 原子操作实现使用一个“逐键”(per-key)的原子“取后加”(fetch-and-add)原语，其线性化规范如下：调用 atomic_fetch_add(x, \\Delta) 会返回存储在 $x$ 处的先前值，并将 $x$ 设置为 $x \\leftarrow x + \\Delta$，就好像该操作是瞬时且不可分割的一样。在此变体中，增量操作通过 atomic_fetch_add(x, 1) 执行。\n\n您的程序必须在未同步的变体中构造一个确定性的数据竞争，方法是确保对同一键的两个并发增量操作在任一方写入之前都读取到相同的旧值。测试的构造必须使两个线程都读取值 $0$ 并都写回 $1$，导致最终值为 $1$，而数学上正确的值为 $2$。\n\n使用以下测试套件。对于所有测试，使用固定容量 $m = 8$ 和相同的键 $k_0 = 7$，在任何增量操作之前将该键插入表中，并将其值初始化为 $0$。\n\n- 测试 1（确定性数据竞争）：\n  - 变体：未同步。\n  - 线程数：$T = 2$。\n  - 每线程增量次数：$R = 1$。\n  - 调度这两个线程，使得它们都在任一方写入前完成读取，从而确保数据竞争和更新丢失。\n  - 预期的数学正确值：$2$。\n  - 测试结果是一个布尔值，指示观察到的最终值是否等于 $2$。\n\n- 测试 2（粗粒度互斥，一般情况）：\n  - 变体：互斥。\n  - 线程数：$T = 4$。\n  - 每线程增量次数：$R = 1000$。\n  - 预期的数学正确值：$4000$。\n  - 测试结果是一个布尔值，指示观察到的最终值是否等于 $4000$。\n\n- 测试 3（原子取后加，一般情况）：\n  - 变体：原子操作。\n  - 线程数：$T = 8$。\n  - 每线程增量次数：$R = 1000$。\n  - 预期的数学正确值：$8000$。\n  - 测试结果是一个布尔值，指示观察到的最终值是否等于 $8000$。\n\n- 测试 4（零工作量的边界条件）：\n  - 变体：互斥。\n  - 线程数：$T = 5$。\n  - 每线程增量次数：$R = 0$。\n  - 预期的数学正确值：$0$。\n  - 测试结果是一个布尔值，指示观察到的最终值是否等于 $0$。\n\n您的程序必须生成单行输出，其中包含按测试 1 到 4 的顺序排列的结果，格式为方括号内以逗号分隔的列表。例如，如果所有测试都通过，则输出必须严格采用 `[True,True,True,True]` 的形式，不含空格。不涉及物理单位。不使用角度。不使用百分比。", "solution": "所提供的问题陈述是有效的。它在科学上基于并发编程的原理，问题设定良好，具有明确的目标和充足的数据，并且语言客观。该问题要求实现一个并发哈希表，并演示数据竞争及其使用标准同步技术的解决方案，这是计算工程中的一个经典且可形式化的问题。\n\n该解决方案通过首先设计一个哈希表类 `ConcurrentHashTable` 来实现，该类能够以三种不同的模式运行：`unsynchronized`（未同步）、基于 `mutex`（互斥锁）和 `atomic`（原子操作）。然后，程序执行一套包含四个测试的测试套件，以演示和验证这些模式的行为。\n\n**1. 哈希表设计**\n\n该哈希表使用开放寻址法和线性探测。其存储由两个 NumPy 数组组成：`_keys` 和 `_values`，每个数组的大小等于容量 $m$。空槽位由 `_keys` 数组中的一个特殊哨兵对象标记。哈希函数为 $h(k) = k \\bmod m$，与规定一致。`_find_slot` 方法实现线性探测，为给定的键 $k$ 定位正确的槽位，从索引 $h(k)$ 开始顺序搜索，直到找到该键或一个空槽位。\n\n**2. 增量操作和同步变体**\n\n核心逻辑位于 `increment` 方法中，该方法修改与键 $k$ 关联的值。其行为由表实例化时指定的 `variant` 决定。\n\n-   **未同步变体 (Unsynchronized Variant)**：此实现执行非原子的读-改-写序列。它读取当前值，将其递增，然后不使用任何锁就将其写回。此版本有意设计为易受数据竞争影响。为了进行确定性测试，该方法增加了一个可选的 `barrier` 参数。当提供一个 `threading.Barrier` 时，线程在读取值后会暂停，从而可以精确控制执行调度以强制产生数据竞争。\n\n-   **互斥（粗粒度锁定）变体 (Mutex (Coarse-Grained Locking) Variant)**：此实现为整个哈希表使用一个单一的、全局的 `threading.Lock` 实例。任何希望执行增量操作的线程都必须首先获取此锁，从而使整个增量操作成为一个临界区。这确保了在任何给定时间只有一个线程可以修改表，从而防止了所有数据竞争，但可能会限制并发性。\n\n-   **原子操作（细粒度锁定）变体 (Atomic Operations (Fine-Grained Locking) Variant)**：问题规定了一个“逐键的原子取后加原语”(\"per-key atomic fetch-and-add primitive\")。这是通过创建一个 `threading.Lock` 对象数组来实现的，其中每个锁对应哈希表中的一个槽位。在对键 $k$ 进行增量操作时，线程首先找到其指定的槽位 $idx$，然后在修改 `_values[idx]` 之前获取锁 `_slot_locks[idx]`。这提供了细粒度同步，因为映射到不同槽位的键上的操作可以并行进行。这正确地模拟了逐键原子性的语义。\n\n**3. 测试套件执行**\n\n`solve` 函数协调执行指定的四个测试。\n\n-   **测试 1（确定性数据竞争）**：此测试演示了更新丢失。它使用 `unsynchronized` 变体，有 $T=2$ 个线程，每个线程对键 $k_0=7$ 执行 $R=1$ 次增量操作。一个 `threading.Barrier(2)` 被传递给 `increment` 方法。两个线程都启动。每个线程读取初始值 $0$。然后它们都在屏障处等待。一旦两者都到达屏障，它们会同时被解除阻塞。然后两个线程都尝试将值 $0+1=1$ 写入同一位置。表中的最终值为 $1$。数学上正确的值是 $T \\times R = 2 \\times 1 = 2$。测试将观察到的值（$1$）与正确的值（$2$）进行比较，得出 `False`，这正确地指出了由于数据竞争导致的失败。\n\n-   **测试 2（互斥同步）**：此测试验证了粗粒度锁定方法在较高负载下的正确性。使用 $T=4$ 个线程，每个线程执行 $R=1000$ 次增量，全局锁确保所有 $4000$ 次增量都顺序应用，尽管顺序是非确定性的。最终值保证是数学上正确的值 $4000$。测试结果为 `True`。\n\n-   **测试 3（原子操作）**：此测试验证了细粒度锁定机制。使用 $T=8$ 个线程，每个线程执行 $R=1000$ 次增量，特定于槽位的锁保护了对键值的更新。最终值将是所有增量的正确总和，即 $8000$。测试结果为 `True`。\n\n-   **测试 4（边界条件）**：此测试使用 `mutex` 变体，有 $T=5$ 个线程和 $R=0$ 次增量。由于增量次数为 $0$，工作线程不对哈希表执行任何操作。初始值 $0$ 保持不变。期望值为 $T \\times R = 5 \\times 0 = 0$。测试结果为 `True`，证实了在零工作量情况下的正确行为。\n\n程序收集每个测试的布尔结果，并以指定的格式 `[result1,result2,result3,result4]` 打印它们。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport threading\n\nclass ConcurrentHashTable:\n    \"\"\"\n    A fixed-capacity, open-addressed hash table supporting concurrent increments.\n    \n    The table can be configured to use different synchronization paradigms:\n    - 'unsynchronized': No locking, vulnerable to data races.\n    - 'mutex': A single global lock for coarse-grained synchronization.\n    - 'atomic': A lock per hash slot for fine-grained synchronization.\n    \"\"\"\n\n    def __init__(self, capacity, variant):\n        if not isinstance(capacity, int) or capacity = 0:\n            raise ValueError(\"Capacity must be a positive integer.\")\n        if variant not in ['unsynchronized', 'mutex', 'atomic']:\n            raise ValueError(\"Invalid variant specified.\")\n\n        self.capacity = capacity\n        self.variant = variant\n        \n        # Sentinel object to mark empty slots in the key array.\n        self._EMPTY = object()\n        \n        self._keys = np.full(self.capacity, self._EMPTY, dtype=object)\n        self._values = np.zeros(self.capacity, dtype=np.int64)\n\n        if self.variant == 'mutex':\n            self._lock = threading.Lock()\n        elif self.variant == 'atomic':\n            self._slot_locks = [threading.Lock() for _ in range(self.capacity)]\n\n    def _hash(self, key):\n        return key % self.capacity\n\n    def _find_slot(self, key):\n        \"\"\"Finds the slot for a key using linear probing.\"\"\"\n        start_idx = self._hash(key)\n        for i in range(self.capacity):\n            idx = (start_idx + i) % self.capacity\n            # Return slot if key is found or an empty slot is available for insertion.\n            if self._keys[idx] == key or self._keys[idx] == self._EMPTY:\n                return idx\n        raise RuntimeError(\"Hash table is full and key not found.\")\n\n    def insert(self, key, value=0):\n        \"\"\"Inserts a key-value pair. Assumes non-concurrent insertion.\"\"\"\n        idx = self._find_slot(key)\n        if self._keys[idx] == self._EMPTY:\n            self._keys[idx] = key\n            self._values[idx] = value\n        else: # Key already exists, update value.\n            self._values[idx] = value\n            \n    def get_value(self, key):\n        \"\"\"Retrieves the value for a given key.\"\"\"\n        idx = self._find_slot(key)\n        if self._keys[idx] == key:\n            return self._values[idx]\n        raise KeyError(\"Key not found in table.\")\n\n    def increment(self, key, barrier=None):\n        \"\"\"\n        Increments the value associated with a key by 1.\n        The synchronization behavior depends on the table's variant.\n        A barrier can be passed for deterministic testing of race conditions.\n        \"\"\"\n        idx = self._find_slot(key)\n        if self._keys[idx] != key:\n            raise KeyError(\"Key not found in table for increment.\")\n\n        if self.variant == 'unsynchronized':\n            # Non-atomic read-modify-write.\n            old_val = self._values[idx]\n            # Barrier for deterministic scheduling in Test 1.\n            if barrier:\n                try:\n                    barrier.wait()\n                except threading.BrokenBarrierError:\n                    pass # Barrier might be reset between threads\n            self._values[idx] = old_val + 1\n        \n        elif self.variant == 'mutex':\n            # Coarse-grained lock on the entire table.\n            with self._lock:\n                self._values[idx] += 1\n        \n        elif self.variant == 'atomic':\n            # Fine-grained lock on the specific hash slot.\n            with self._slot_locks[idx]:\n                self._values[idx] += 1\n\ndef solve():\n    \"\"\"\n    Executes the test suite to demonstrate data races and synchronization.\n    \"\"\"\n    test_params = [\n        {'id': 1, 'variant': 'unsynchronized', 'T': 2, 'R': 1, 'm': 8, 'k0': 7, 'correct': 2},\n        {'id': 2, 'variant': 'mutex',          'T': 4, 'R': 1000, 'm': 8, 'k0': 7, 'correct': 4000},\n        {'id': 3, 'variant': 'atomic',         'T': 8, 'R': 1000, 'm': 8, 'k0': 7, 'correct': 8000},\n        {'id': 4, 'variant': 'mutex',          'T': 5, 'R': 0, 'm': 8, 'k0': 7, 'correct': 0},\n    ]\n\n    results = []\n    \n    # --- Test 1: Deterministic Data Race ---\n    case = test_params[0]\n    table = ConcurrentHashTable(capacity=case['m'], variant=case['variant'])\n    table.insert(key=case['k0'], value=0)\n    \n    barrier = threading.Barrier(case['T'])\n    \n    def race_worker(tbl, key, bar):\n        tbl.increment(key, barrier=bar)\n\n    threads = []\n    for _ in range(case['T']):\n        thread = threading.Thread(target=race_worker, args=(table, case['k0'], barrier))\n        threads.append(thread)\n        thread.start()\n    \n    for thread in threads:\n        thread.join()\n        \n    observed = table.get_value(case['k0'])\n    results.append(observed == case['correct']) # Expected: False\n\n    # --- Tests 2, 3, 4: General Cases ---\n    def general_worker(tbl, key, num_increments):\n        for _ in range(num_increments):\n            tbl.increment(key)\n\n    for case in test_params[1:]:\n        table = ConcurrentHashTable(capacity=case['m'], variant=case['variant'])\n        table.insert(key=case['k0'], value=0)\n\n        threads = []\n        for _ in range(case['T']):\n            thread = threading.Thread(target=general_worker, args=(table, case['k0'], case['R']))\n            threads.append(thread)\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n            \n        observed = table.get_value(case['k0'])\n        results.append(observed == case['correct'])\n    \n    # Final print statement in the exact required format.\n    # The str() of a bool in Python is 'True' or 'False' (capitalized).\n    # The problem example shows \"[True,True,True,True]\".\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2422625"}, {"introduction": "在保证程序正确之后，下一个目标就是追求极致的性能，而这通常需要对底层硬件架构有深入的理解。本练习将目光投向现代高性能计算的核心——图形处理器（GPU），并聚焦于一个关键的性能瓶颈：共享内存的银行冲突。通过构建一个精确的成本模型，你将量化比较不同的内存布局策略如何影响并行归约操作的效率，从而学会如何通过架构感知的编程思维来优化性能。[@problem_id:2422580]", "problem": "您将获得一个关于图形处理单元 (GPU) 共享内存系统上单指令多线程 (SIMT) 归约的形式化模型。一个协作线程块由 $T$ 个线程组成，这些线程被划分为大小为 $w$ 的线程束 (warp)。共享内存被组织成 $B$ 个存储体 (bank)。每个共享内存元素占用 $e_b$ 字节，共享内存存储体字宽为 $b_w$ 字节，位于逻辑索引 $i \\in \\mathbb{Z}_{\\ge 0}$ 的元素的存储体索引定义如下：\n$$\n\\operatorname{bank}(i) = \\left\\lfloor \\frac{i \\cdot e_b}{b_w} \\right\\rfloor \\bmod B.\n$$\n假设基地址已对齐，使得第一个元素的字节地址为 $0$。对 $M$ 个元素（其中 $M = \\min(N, T)$）的并行归约，采用标准的步长减半二叉树模式进行，并在共享内存中进行原地更新，过程如下。对于每个步长 $s \\in \\{1, 2, 4, \\dots\\}$，当 $s  M$ 时，活动线程索引的集合为\n$$\nA_s = \\{\\, t \\in \\mathbb{Z} \\mid 0 \\le t  M,\\ t \\bmod (2s) = 0,\\ t + s  M \\,\\}.\n$$\n每个活动线程 $t \\in A_s$ 概念上按顺序执行三个共享内存操作：读取索引 $t$ 处的元素，读取索引 $t+s$ 处的元素，计算它们的和，然后将结果写回索引 $t$。设 $f:\\mathbb{Z}_{\\ge 0} \\to \\mathbb{Z}_{\\ge 0}$ 是一个索引重映射函数，它为减轻存储体冲突对共享内存的可选填充进行建模：\n- 无填充时，$f(i) = i$。\n- 有避免冲突的填充时，$f(i) = i + \\left\\lfloor \\frac{i}{B} \\right\\rfloor$。\n\n对于给定的步长 $s$，定义三个子阶段，对应于一个线程束中活动线程所访问的地址集：阶段 1 使用地址 $\\{ f(t) \\mid t \\in A_s \\cap W \\}$，阶段 2 使用地址 $\\{ f(t+s) \\mid t \\in A_s \\cap W \\}$，阶段 3 为写入操作再次使用地址 $\\{ f(t) \\mid t \\in A_s \\cap W \\}$，其中 $W$ 是特定线程束中的线程索引集合。对于线程束 $j \\in \\{0,1,\\dots,\\lceil T/w \\rceil - 1\\}$，其线程索引集定义为\n$$\nW_j = \\{\\, jw, jw+1, \\dots, \\min(T-1, jw + w - 1) \\,\\}.\n$$\n在任何子阶段和线程束内，如果多个活动线程寻址的元素其存储体索引相同，则这些访问将被串行化；该线程束的子阶段成本等于在该子阶段期间，该线程束中映射到同一存储体的活动线程的最大数量。如果一个线程束在某个子阶段没有活动线程，则其在该子阶段的成本为 $0$。步长 $s$ 的步骤成本是所有线程束的三个子阶段成本之和，而归约的总成本是所有步长 $s$ (其中 $s  M$) 的步骤成本之和。\n\n您的任务是编写一个完整的、可运行的程序，为每个测试用例计算两个整数：\n- $C_{\\text{naive}}$：$f(i) = i$ 时的总成本。\n- $C_{\\text{padded}}$：$f(i) = i + \\left\\lfloor \\frac{i}{B} \\right\\rfloor$ 时的总成本。\n\n假设和要求：\n- 将 $N$、$T$、$w$、$B$、$e_b$ 和 $b_w$ 视为每个测试用例给定的整数。\n- 归约仅考虑 $M = \\min(N, T)$ 个元素。\n- 不涉及角度和物理单位；所有输出均为纯整数。\n- 通过对其成本求和来聚合线程束的成本；在成本模型中，不同线程束或子阶段之间没有重叠。\n\n测试套件（每个元组列出 $(N, T, w, B, e_b, b_w)$，所有数字均为十进制）：\n1. $(N, T, w, B, e_b, b_w) = (\\, $1024$, $256$, $32$, $32$, $4$, $4$ \\,)$\n2. $(N, T, w, B, e_b, b_w) = (\\, $1$, $1$, $32$, $32$, $4$, $4$ \\,)$\n3. $(N, T, w, B, e_b, b_w) = (\\, $1000$, $256$, $32$, $32$, $4$, $4$ \\,)$\n4. $(N, T, w, B, e_b, b_w) = (\\, $512$, $256$, $32$, $16$, $4$, $4$ \\,)$\n5. $(N, T, w, B, e_b, b_w) = (\\, $192$, $64$, $32$, $32$, $4$, $4$ \\,)$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的列表形式的结果，其中每个元素对应一个测试用例，并且本身是一个双元素列表 $[C_{\\text{naive}},C_{\\text{padded}}]$。输出中不得有任何空白字符。例如，一个可能的输出结构是\n$[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4],[x_5,y_5]]$\n其中每个 $x_i$ 和 $y_i$ 是您的程序为测试用例 $i$ 计算出的整数。", "solution": "所提供的问题是一个定义明确的计算建模练习，要求在一个简化的图形处理单元 (GPU) 共享内存架构上模拟并行归约算法。问题陈述在科学上基于并行计算的原理，逻辑上一致且完整。因此，该问题被认为是有效的，并将按规定构建解决方案。\n\n任务是计算在两种不同的内存布局方案下，一次并行归约操作的总成本，该成本根据内存访问的串行化来定义。计算必须精确遵循所描述的形式化模型。该解决方案可以通过系统地、一步一步地模拟归约过程，并按定义汇总成本来实现。\n\n总体成本是归约过程中每一步成本的总和。对 $M$ 个元素的归约分阶段进行，由步长 $s$ 索引，$s$ 从 1 开始，在每个阶段加倍，并持续到 $s  M$。因此，总成本 $C$ 为：\n$$\nC = \\sum_{s \\in \\{1, 2, 4, \\dots\\} \\text{ s.t. } s  M} \\text{StepCost}(s)\n$$\n其中 $M = \\min(N, T)$ 是参与归约的元素数量。\n\n步长为 $s$ 的单个步骤成本 $\\text{StepCost}(s)$ 是线程块中所有线程束产生的成本之和。一个包含 $T$ 个线程的线程块被划分为 $\\lceil T/w \\rceil$ 个大小为 $w$ 的线程束。\n$$\n\\text{StepCost}(s) = \\sum_{j=0}^{\\lceil T/w \\rceil - 1} \\text{WarpCost}(W_j, s)\n$$\n其中 $W_j$ 是线程束 $j$ 中的线程索引集合。\n\n在步长 $s$ 时，单个线程束 $W_j$ 的成本 $\\text{WarpCost}(W_j, s)$ 是由活动线程执行的内存操作所对应的三个不同子阶段的成本之和。步长为 $s$ 的活动线程集合是 $A_s = \\{\\, t \\in \\mathbb{Z} \\mid 0 \\le t  M,\\ t \\bmod (2s) = 0,\\ t + s  M \\,\\}$。在一个线程束 $W_j$ 内，活动线程为 $A_{s,j} = A_s \\cap W_j$。\n这三个子阶段是：\n1.  对每个 $t \\in A_{s,j}$，从逻辑索引 $t$ 读取。\n2.  对每个 $t \\in A_{s,j}$，从逻辑索引 $t+s$ 读取。\n3.  对每个 $t \\in A_{s,j}$，写入到逻辑索引 $t$。\n\n一个线程束的一个子阶段的成本是存储体冲突的程度，定义为该线程束中同时访问同一内存存储体的活动线程的最大数量。设 $\\mathcal{I}$ 是一个子阶段中访问的逻辑索引集合。其成本为：\n$$\n\\text{SubPhaseCost}(\\mathcal{I}) = \\max_{k \\in \\{0, \\dots, B-1\\}} \\left| \\{\\, i \\in \\mathcal{I} \\mid \\operatorname{bank}(f(i)) = k \\,\\} \\right|\n$$\n如果 $\\mathcal{I}$ 为空，则成本为 $0$。逻辑索引 $k$ 的存储体索引由 $\\operatorname{bank}(k) = \\left\\lfloor \\frac{k \\cdot e_b}{b_w} \\right\\rfloor \\bmod B$ 给出。函数 $f(i)$ 将逻辑索引映射到内存索引，从而对填充进行建模。我们必须计算两种情况下的总成本：\n-   **朴素布局 (Naive layout)**：$C_{\\text{naive}}$，其中 $f(i) = i$。\n-   **填充布局 (Padded layout)**：$C_{\\text{padded}}$，其中 $f(i) = i + \\left\\lfloor \\frac{i}{B} \\right\\rfloor$。\n\n解决该问题的算法是此模型的直接实现。它涉及一个遍历步长 $s$ 的主循环，一个遍历线程束 $j$ 的内循环，并为每个线程束计算三个子阶段的成本。对于每个子阶段，我们识别出该线程束中活动线程访问的逻辑索引集，应用适当的索引重映射函数 $f$，为每个得到的内存索引计算存储体索引，并找到任何单个存储体索引的最大出现频率。将所有子阶段、线程束和步长的这些成本相加，即可得出最终的总成本。这个过程对 $C_{\\text{naive}}$ 和 $C_{\\text{padded}}$ 各执行一次。对于所有提供的测试用例，$e_b = b_w = 4$，这将存储体索引的计算简化为 $\\operatorname{bank}(k) = k \\bmod B$。\n\n实现将包含一个主函数，该函数根据一组给定的参数和指定的填充方案来计算总成本。对于每个测试用例的朴素方案和填充方案，都将调用此函数。一个辅助函数将通过确定一组给定逻辑索引的存储体索引并计算最大冲突数来计算子阶段成本。", "answer": "```python\nimport math\nfrom collections import Counter\n\ndef solve():\n    \"\"\"\n    Solves the GPU shared memory reduction cost problem for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        (1024, 256, 32, 32, 4, 4),\n        (1, 1, 32, 32, 4, 4),\n        (1000, 256, 32, 32, 4, 4),\n        (512, 256, 32, 16, 4, 4),\n        (192, 64, 32, 32, 4, 4),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, T, w, B, eb, bw = case\n        c_naive = compute_total_cost(N, T, w, B, eb, bw, 'naive')\n        c_padded = compute_total_cost(N, T, w, B, eb, bw, 'padded')\n        results.append([c_naive, c_padded])\n    \n    case_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(case_strings)}]\")\n\ndef compute_subphase_cost(indices, padding_type, B, eb, bw):\n    \"\"\"\n    Computes the cost of a single sub-phase for a warp, which is the maximum\n    number of threads accessing the same memory bank.\n    \"\"\"\n    if not indices:\n        return 0\n\n    bank_indices = []\n    for i in indices:\n        mapped_i = 0\n        if padding_type == 'naive':\n            mapped_i = i\n        elif padding_type == 'padded':\n            mapped_i = i + (i // B)\n\n        # Bank index is defined by floor((i * e_b) / b_w) mod B.\n        bank_idx = (mapped_i * eb // bw) % B\n        bank_indices.append(bank_idx)\n\n    if not bank_indices:\n        return 0\n    \n    # The cost is the max number of accesses to any single bank.\n    counts = Counter(bank_indices)\n    return max(counts.values())\n\ndef compute_total_cost(N, T, w, B, eb, bw, padding_type):\n    \"\"\"\n    Computes the total cost of a parallel reduction based on the given model.\n    \"\"\"\n    M = min(N, T)\n    if M  2:\n        return 0\n        \n    total_cost = 0\n    s = 1\n    num_warps = (T + w - 1) // w\n\n    while s  M:\n        step_cost = 0\n        for j in range(num_warps):\n            warp_start_tid = j * w\n            warp_end_tid = min(T, (j + 1) * w)\n\n            active_threads_in_warp = []\n            for t in range(warp_start_tid, warp_end_tid):\n                # An active thread t must satisfy:\n                # 1. Be part of the reduction: t  M\n                # 2. Be the thread responsible for the sum: t % (2s) == 0\n                # 3. Have a partner to sum with: t + s  M\n                if t  M and t % (2 * s) == 0 and t + s  M:\n                    active_threads_in_warp.append(t)\n            \n            if not active_threads_in_warp:\n                continue\n\n            # Phase 1: Read from logical index t\n            cost1 = compute_subphase_cost(active_threads_in_warp, padding_type, B, eb, bw)\n            \n            # Phase 2: Read from logical index t+s\n            indices_phase2 = [t + s for t in active_threads_in_warp]\n            cost2 = compute_subphase_cost(indices_phase2, padding_type, B, eb, bw)\n\n            # Phase 3: Write to logical index t (same addresses as phase 1)\n            cost3 = cost1\n\n            step_cost += (cost1 + cost2 + cost3)\n\n        total_cost += step_cost\n        s *= 2\n        \n    return total_cost\n\nsolve()\n```", "id": "2422580"}, {"introduction": "当计算任务的规模超越单机内存或处理能力时，我们便进入了分布式内存计算的领域，其中消息传递接口（MPI）是标准范式。在分布式系统中，节点间的通信开销往往是决定整体性能的主要因素，因此高效的数据传输至关重要。本练习通过一个性能建模任务，让你分析和比较两种处理非连续数据传输的策略——手动打包与使用 MPI 派生数据类型，这有助于你理解在实际的 MPI 编程中如何根据具体情况选择最优的通信方式。[@problem_id:2422623]", "problem": "您需要建模并比较两种并行通信策略，这两种策略用于在消息传递接口（MPI）的两个进程之间发送矩阵的非连续子块。比较必须使用基于 Hockney 延迟-带宽通信模型和简单的基于带宽的内存操作模型的第一性原理性能模型来完成。\n\n非连续子块定义在一个以行主序存储的稠密矩阵上。该子块由其左上角的列索引、宽度和高度来描述。当宽度严格小于矩阵的列数时，子块的每一行贡献一个连续的内存段；这导致了跨行的跨步式布局。\n\n您的程序必须计算两种方法的预测端到端时间（以秒为单位）：\n- 方法 A（手动打包/解包）：发送方手动将子块打包到一个连续的缓冲区中，发送它，然后接收方手动将其解包到目标子块中。\n- 方法 B（MPI 派生数据类型）：发送方和接收方使用派生数据类型来描述非连续子块，通过单条消息传输它，并依赖 MPI 库来执行任何内部的打包和解包。\n\n基础性能模型假设：\n- Hockney 延迟-带宽模型指出，通过网络发送大小为 $n$ 字节的连续消息的时间为 $T_{\\mathrm{net}} = \\alpha + \\dfrac{n}{B_{\\mathrm{net}}}$，其中 $\\alpha$ 是延迟（以秒为单位），$B_{\\mathrm{net}}$ 是持续网络带宽（以字节/秒为单位）。\n- 内存操作被建模为受带宽限制。对于使用高度优化路径进行的大小为 $n$ 字节的连续复制，时间为 $T = \\dfrac{n}{B_{\\mathrm{copy}}}$。对于大小为 $n$ 字节的跨步式收集或散布操作，时间分别为 $T = \\dfrac{n}{B_{\\mathrm{gather}}}$ 或 $T = \\dfrac{n}{B_{\\mathrm{scatter}}}$。不同的有效带宽反映了局部性和实现上的差异。\n- 对于派生数据类型，每条消息有额外的固定设置开销 $L_{\\mathrm{ddt}}$（以秒为单位），并且对于发送方和接收方上由该类型描述的每个连续段，有每段开销 $s_{\\mathrm{seg}}$（以秒为单位）。一个段是应用程序缓冲区的最大连续部分；对于在一个行长为 $N$、起始列索引为 $j_0$ 的矩阵内，宽度为 $w$ 的子块，段的数量为：\n  - 当且仅当 $w = N$ 且 $j_0 = 0$ 时为 $1$（子块是单个连续区域），以及\n  - 否则为 $h$，每个子块行一个段。\n\n需要实现的定义和公式：\n- 该矩阵以行主序存储，有 $N$ 列。子块从列 $j_0$ 开始，宽度为 $w$，高度为 $h$。元素大小为 $e$ 字节。要传输的总字节数为 $n = e \\cdot w \\cdot h$。\n- 两种方法的网络通信时间均为 $T_{\\mathrm{net}} = \\alpha + \\dfrac{n}{B_{\\mathrm{net}}}$。\n- 对于方法 A（手动打包/解包），总时间为\n  $$T_{\\mathrm{manual}} = \\left(\\frac{n}{B_{\\mathrm{gather,man}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,man}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right)。$$\n  如果子块是连续的（即 $w = N$ 且 $j_0 = 0$），则使用 $B_{\\mathrm{gather,man}} = B_{\\mathrm{copy}}$ 和 $B_{\\mathrm{scatter,man}} = B_{\\mathrm{copy}}$。\n- 对于方法 B（MPI 派生数据类型），总时间为\n  $$T_{\\mathrm{ddt}} = \\left(\\frac{n}{B_{\\mathrm{gather,ddt}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,ddt}}}\\right) + \\left(L_{\\mathrm{ddt}} + s_{\\mathrm{seg}} \\cdot S\\right)，$$\n  其中 $S$ 是如上定义的段数。如果子块是连续的，则使用 $B_{\\mathrm{gather,ddt}} = B_{\\mathrm{copy}}$ 和 $B_{\\mathrm{scatter,ddt}} = B_{\\mathrm{copy}}$。\n\n您的程序必须为每个测试用例计算比率\n$$r = \\frac{T_{\\mathrm{manual}}}{T_{\\mathrm{ddt}}}。$$\n$r  1$ 的值表示在该模型下，派生数据类型方法更快；$r  1$ 的值表示手动方法更快。\n\n单位：\n- 所有时间必须以秒计算。\n- 所有带宽必须以字节/秒为单位。\n- 所有尺寸（$N$, $w$, $h$, $j_0$）是无单位的元素计数；元素大小 $e$ 的单位是字节。\n\n在所有测试用例中使用以下固定参数：\n- 延迟：$\\alpha = 1.0 \\times 10^{-6}$。\n- 网络带宽：$B_{\\mathrm{net}} = 1.2 \\times 10^{10}$。\n- 连续复制带宽：$B_{\\mathrm{copy}} = 2.4 \\times 10^{10}$。\n- 手动收集带宽：$B_{\\mathrm{gather,man}} = 8.0 \\times 10^{9}$。\n- 手动散布带宽：$B_{\\mathrm{scatter,man}} = 7.0 \\times 10^{9}$。\n- 派生数据类型收集带宽：$B_{\\mathrm{gather,ddt}} = 1.4 \\times 10^{10}$。\n- 派生数据类型散布带宽：$B_{\\mathrm{scatter,ddt}} = 1.2 \\times 10^{10}$。\n- 派生数据类型固定设置：$L_{\\mathrm{ddt}} = 5.0 \\times 10^{-7}$。\n- 派生数据类型每段开销：$s_{\\mathrm{seg}} = 5.0 \\times 10^{-8}$。\n- 元素大小：$e = 8$。\n\n测试套件：\n- 案例 1（典型的中等大小，非连续）：$N = 2048$, $w = 256$, $h = 256$, $j_0 = 128$。\n- 案例 2（非常小，延迟主导，非连续）：$N = 2048$, $w = 8$, $h = 8$, $j_0 = 100$。\n- 案例 3（高度跨步：许多短段）：$N = 8192$, $w = 8$, $h = 1024$, $j_0 = 16$。\n- 案例 4（连续矩形）：$N = 2048$, $w = 2048$, $h = 256$, $j_0 = 0$。\n- 案例 5（大型，非连续）：$N = 4096$, $w = 1024$, $h = 1024$, $j_0 = 0$。\n\n最终输出格式：\n- 您的程序必须生成一行，其中包含上述案例的五个比率，以逗号分隔的列表形式并用方括号括起来。每个比率必须四舍五入到小数点后恰好六位数字。", "solution": "该问题要求计算和比较在使用消息传递接口 (MPI) 框架的两种不同策略下，在两个进程之间传输矩阵的非连续子块的端到端传输时间。比较将通过计算手动打包/解包方法的总时间与使用 MPI 派生数据类型方法的总时间之比来执行。该分析基于一个第一性原理性能模型。\n\n主要目标是为一组给定的测试用例计算比率 $r = \\frac{T_{\\mathrm{manual}}}{T_{\\mathrm{ddt}}}$。这里，$T_{\\mathrm{manual}}$ 是方法 A（手动打包/解包）的总时间，$T_{\\mathrm{ddt}}$ 是方法 B（MPI 派生数据类型）的总时间。所有计算都必须严格遵守提供的公式和参数。\n\n首先，我们定义公共参数和变量。\n矩阵有 $N$ 列，元素大小为 $e$ 字节。子块由其高度 $h$、宽度 $w$ 以及其左上角的列索引 $j_0$ 定义。要传输的数据总大小为 $n = e \\cdot w \\cdot h$ 字节。\n固定的性能参数是：\n- 网络延迟, $\\alpha = 1.0 \\times 10^{-6}$ 秒。\n- 网络带宽, $B_{\\mathrm{net}} = 1.2 \\times 10^{10}$ 字节/秒。\n- 连续内存复制带宽, $B_{\\mathrm{copy}} = 2.4 \\times 10^{10}$ 字节/秒。\n- 手动收集带宽, $B_{\\mathrm{gather,man}} = 8.0 \\times 10^{9}$ 字节/秒。\n- 手动散布带宽, $B_{\\mathrm{scatter,man}} = 7.0 \\times 10^{9}$ 字节/秒。\n- 派生数据类型收集带宽, $B_{\\mathrm{gather,ddt}} = 1.4 \\times 10^{10}$ 字节/秒。\n- 派生数据类型散布带宽, $B_{\\mathrm{scatter,ddt}} = 1.2 \\times 10^{10}$ 字节/秒。\n- 派生数据类型固定设置成本, $L_{\\mathrm{ddt}} = 5.0 \\times 10^{-7}$ 秒。\n- 派生数据类型每段开销, $s_{\\mathrm{seg}} = 5.0 \\times 10^{-8}$ 秒/段。\n- 元素大小, $e = 8$ 字节。\n\n一个关键条件是子块是否为单个连续的内存块。这当且仅当子块跨越矩阵的整个宽度并从第一列开始时发生，即 $w = N$ 且 $j_0 = 0$。\n\n让我们分析每种方法的总时间。\n\n对于方法 A（手动打包/解包），总时间 $T_{\\mathrm{manual}}$ 由发送方打包成本、网络传输时间以及接收方解包成本的总和给出。提供的模型是：\n$$T_{\\mathrm{manual}} = \\left(\\frac{n}{B_{\\mathrm{gather,man,eff}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,man,eff}}} + \\frac{n}{B_{\\mathrm{copy}}}\\right)$$\n这里，$B_{\\mathrm{gather,man,eff}}$ 和 $B_{\\mathrm{scatter,man,eff}}$ 分别是手动收集和散布操作的有效带宽。它们的值取决于子块的内存布局。\n- 如果子块是非连续的（$w \\neq N$ 或 $j_0 \\neq 0$），那么 $B_{\\mathrm{gather,man,eff}} = B_{\\mathrm{gather,man}}$ 且 $B_{\\mathrm{scatter,man,eff}} = B_{\\mathrm{scatter,man}}$。\n- 如果子块是连续的（$w = N$ 且 $j_0 = 0$），收集和散布操作等同于连续内存复制，所以 $B_{\\mathrm{gather,man,eff}} = B_{\\mathrm{copy}}$ 且 $B_{\\mathrm{scatter,man,eff}} = B_{\\mathrm{copy}}$。\n\n对于方法 B（MPI 派生数据类型），总时间 $T_{\\mathrm{ddt}}$ 是由 MPI 库处理的有效内存操作成本、网络传输时间以及特定于数据类型的开销的总和：\n$$T_{\\mathrm{ddt}} = \\left(\\frac{n}{B_{\\mathrm{gather,ddt,eff}}}\\right) + \\left(\\alpha + \\frac{n}{B_{\\mathrm{net}}}\\right) + \\left(\\frac{n}{B_{\\mathrm{scatter,ddt,eff}}}\\right) + \\left(L_{\\mathrm{ddt}} + s_{\\mathrm{seg}} \\cdot S\\right)$$\n有效带宽 $B_{\\mathrm{gather,ddt,eff}}$ 和 $B_{\\mathrm{scatter,ddt,eff}}$ 的确定方式类似：\n- 如果子块是非连续的，$B_{\\mathrm{gather,ddt,eff}} = B_{\\mathrm{gather,ddt}}$ 且 $B_{\\mathrm{scatter,ddt,eff}} = B_{\\mathrm{scatter,ddt}}$。\n- 如果子块是连续的，$B_{\\mathrm{gather,ddt,eff}} = B_{\\mathrm{copy}}$ 且 $B_{\\mathrm{scatter,ddt,eff}} = B_{\\mathrm{copy}}$。\n\n参数 $S$ 代表构成子块的连续段的数量。根据行主序矩阵的问题定义：\n- 如果子块是连续的（$w = N$ 且 $j_0 = 0$），它形成单个段，所以 $S=1$。\n- 如果子块是非连续的，它的 $h$ 行中的每一行构成一个独立的段，所以 $S=h$。\n\n每个测试用例 $(N, w, h, j_0)$ 的过程如下：\n1.  计算总数据大小：$n = e \\cdot w \\cdot h$。\n2.  通过检查是否 $w = N$ 且 $j_0 = 0$ 来确定子块是否连续。\n3.  根据连续性，确定段数 $S$，并为 $T_{\\mathrm{manual}}$ 和 $T_{\\mathrm{ddt}}$ 的计算选择适当的有效带宽。\n4.  将值代入 $T_{\\mathrm{manual}}$ 和 $T_{\\mathrm{ddt}}$ 的公式中以计算它们的数值。\n5.  计算最终比率 $r = T_{\\mathrm{manual}} / T_{\\mathrm{ddt}}$。\n\n将对五个指定的测试用例中的每一个执行此过程，并报告所得的比率。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the performance ratio of manual packing vs. MPI derived datatypes\n    for transferring a non-contiguous matrix sub-block based on a first-principles model.\n    \"\"\"\n    \n    # Fixed performance parameters\n    alpha = 1.0e-6  # Network latency (s)\n    B_net = 1.2e10  # Network bandwidth (bytes/s)\n    B_copy = 2.4e10  # Contiguous copy bandwidth (bytes/s)\n    B_gather_man = 8.0e9  # Manual gather bandwidth (bytes/s)\n    B_scatter_man = 7.0e9  # Manual scatter bandwidth (bytes/s)\n    B_gather_ddt = 1.4e10  # Derived datatype gather bandwidth (bytes/s)\n    B_scatter_ddt = 1.2e10  # Derived datatype scatter bandwidth (bytes/s)\n    L_ddt = 5.0e-7  # Derived datatype fixed setup cost (s)\n    s_seg = 5.0e-8  # Derived datatype per-segment overhead (s/segment)\n    e = 8  # Element size (bytes)\n\n    # Test suite\n    test_cases = [\n        # (N, w, h, j_0)\n        (2048, 256, 256, 128),  # Case 1: typical moderately sized, non-contiguous\n        (2048, 8, 8, 100),      # Case 2: very small, latency-dominated, non-contiguous\n        (8192, 8, 1024, 16),    # Case 3: highly strided: many short segments\n        (2048, 2048, 256, 0),   # Case 4: contiguous rectangle\n        (4096, 1024, 1024, 0),  # Case 5: large, non-contiguous\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N, w, h, j_0 = case\n\n        # Total data size in bytes\n        n = e * w * h\n\n        # Determine if the sub-block is contiguous\n        is_contiguous = (w == N and j_0 == 0)\n\n        # Determine effective bandwidths and number of segments\n        if is_contiguous:\n            eff_B_gather_man = B_copy\n            eff_B_scatter_man = B_copy\n            eff_B_gather_ddt = B_copy\n            eff_B_scatter_ddt = B_copy\n            S = 1\n        else:\n            eff_B_gather_man = B_gather_man\n            eff_B_scatter_man = B_scatter_man\n            eff_B_gather_ddt = B_gather_ddt\n            eff_B_scatter_ddt = B_scatter_ddt\n            S = h\n\n        # Calculate T_manual\n        time_pack = n / eff_B_gather_man\n        time_copy_send = n / B_copy\n        time_net = alpha + n / B_net\n        time_unpack = n / eff_B_scatter_man\n        time_copy_recv = n / B_copy\n        \n        # The problem statement's formula for T_manual implies two copies on each side,\n        # one from strided to buffer (gather) and one from buffer to a (hypothetical) contiguous send buffer.\n        # This seems redundant. Let's strictly follow the formula:\n        # T_manual = (gather_time + copy_time)_sender + net_time + (scatter_time + copy_time)_receiver\n        # This interpretation is unusual, but required by the prompt.\n        # Let's re-read: \"(n/B_gather,man + n/B_copy) + (alpha + n/B_net) + (n/B_scatter,man + n/B_copy)\"\n        # This implies the manual pack (gather) AND a copy on the send side, and a copy AND unpack (scatter) on the recv side.\n        # We will follow this literal interpretation.\n        T_manual = (n / eff_B_gather_man + n / B_copy) + (alpha + n / B_net) + (n / eff_B_scatter_man + n / B_copy)\n\n\n        # Calculate T_ddt\n        time_gather_ddt = n / eff_B_gather_ddt\n        time_scatter_ddt = n / eff_B_scatter_ddt\n        time_overhead_ddt = L_ddt + s_seg * S\n        \n        T_ddt = time_gather_ddt + (alpha + n / B_net) + time_scatter_ddt + time_overhead_ddt\n        \n        # Calculate the ratio\n        ratio = T_manual / T_ddt\n        \n        results.append(f\"{ratio:.6f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2422623"}]}