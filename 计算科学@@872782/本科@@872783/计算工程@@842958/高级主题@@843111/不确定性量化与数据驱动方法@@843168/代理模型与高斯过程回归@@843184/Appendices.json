{"hands_on_practices": [{"introduction": "我们的第一个练习将从基础开始，通过一个一维回归问题来手动构建一个高斯过程模型。这个练习以一个直观的场景为背景——预测跑步者的心率与配速之间的关系，这不仅能帮助你理解如何生成预测值，更重要的是，如何量化这些预测的不确定性。通过这个练习[@problem_id:2441367]，你将深入探索高斯过程的核心机制，并直观地感受长度尺度（length scale）这一关键超参数如何影响模型的行为和泛化能力。", "problem": "一个标量值潜函数 $f$ 将跑步者的配速 $p$（单位为分钟/公里）映射到心率 $f(p)$（单位为次/分钟）。给定带噪声的观测值 $\\{(p_i, y_i)\\}_{i=1}^n$，其中 $y_i$ 是测量的心率，单位为次/分钟。假设一个加性噪声模型 $y_i = f(p_i) + \\varepsilon_i$，其中噪声 $\\varepsilon_i$ 是独立的，服从高斯分布 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$。\n\n假设 $f$ 的先验为一个零均值高斯过程，其协方差函数为\n$$\nk(p,p') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(p - p')^2}{2\\ell^2}\\right),\n$$\n其中 $\\sigma_f$ 是信号尺度（单位为次/分钟），$\\ell$ 是长度尺度（单位为分钟/公里）。\n\n训练数据为以下 $n = 7$ 个配速-心率对：\n- 配速（分钟/公里）：$[\\,3.5,\\,4.0,\\,4.5,\\,5.0,\\,5.5,\\,6.0,\\,6.5\\,]$。\n- 测量心率（次/分钟）：$[\\,176,\\,168,\\,160,\\,150,\\,140,\\,130,\\,122\\,]$。\n\n除非下文另有说明，否则假设 $\\sigma_n = 2.0$（单位为次/分钟）。对于下面列出的每个测试用例，在指定的超参数下，计算单个测试配速 $p_\\star$ 的后验预测均值 $\\mu_\\star$（单位为次/分钟）和后验预测标准差 $s_\\star$（单位为次/分钟）。报告 $\\mu_\\star$ 和 $s_\\star$，保留三位小数。\n\n测试集（每个用例相互独立）：\n- 用例 1：$p_\\star = 5.2$, $\\sigma_f = 60.0$, $\\ell = 0.6$, $\\sigma_n = 2.0$。\n- 用例 2：$p_\\star = 3.2$, $\\sigma_f = 60.0$, $\\ell = 0.6$, $\\sigma_n = 2.0$。\n- 用例 3：$p_\\star = 7.0$, $\\sigma_f = 60.0, \\ell = 0.6$, $\\sigma_n = 2.0$。\n- 用例 4：$p_\\star = 5.2$, $\\sigma_f = 60.0, \\ell = 0.2$, $\\sigma_n = 2.0$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\,\\mu_\\star^{(1)}, s_\\star^{(1)}, \\mu_\\star^{(2)}, s_\\star^{(2)}, \\mu_\\star^{(3)}, s_\\star^{(3)}, \\mu_\\star^{(4)}, s_\\star^{(4)}\\,]$。所有八个数字都必须以次/分钟为单位，并保留三位小数，不含空格。例如：$[\\,150.123,2.345, \\dots\\,]$（注意：实际值会有所不同）。", "solution": "首先对问题陈述进行严格验证。检查所有给定条件，确保其科学合理性、自洽性和客观性。\n\n该问题指定了一个回归任务，旨在为一个潜函数 $f(p)$ 建模，该函数将跑步者的配速 $p$（单位为分钟/公里）映射到心率 $f(p)$（单位为次/分钟）。该模型是一个高斯过程，具有零均值先验，$f \\sim \\mathcal{GP}(0, k(p, p'))$。协方差函数是平方指数核函数：\n$$k(p,p') = \\sigma_f^2 \\exp\\left(-\\frac{(p - p')^2}{2\\ell^2}\\right)$$\n观测值 $y_i$ 是潜函数的带噪声测量值，由模型 $y_i = f(p_i) + \\varepsilon_i$ 描述，其中噪声 $\\varepsilon_i$ 是独立同分布的，服从 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$。\n\n训练数据由 $n=7$ 对配速 $p_i$ 和观测心率 $y_i$ 组成：\n-   配速 $P = [3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5]$ 分钟/公里。\n-   测量心率 $Y = [176, 168, 160, 150, 140, 130, 122]$ 次/分钟。\n\n任务是为四个不同的测试用例计算单个测试配速 $p_\\star$ 的后验预测均值 $\\mu_\\star$ 和后验预测标准差 $s_\\star$。在所有用例中，噪声标准差均为 $\\sigma_n = 2.0$ 次/分钟。\n\n验证结论：该问题有效。它构成了代理建模领域（计算工程学的一个子学科）中一个适定且有科学依据的问题。问题提供了所有必要的数据和模型规范，没有内部矛盾，并且以客观、明确的语言表述。我们可以继续进行解析求解。\n\n该解决方案的理论基础是高斯过程模型的后验预测分布的推导。设训练输入由向量 $X = [p_1, \\dots, p_n]^T$ 表示，相应的训练输出由 $\\mathbf{y} = [y_1, \\dots, y_n]^T$ 表示。潜函数在新测试点 $p_\\star$ 处的值表示为 $f_\\star = f(p_\\star)$。\n\n根据高斯过程的定义，潜函数在训练点处的值 $\\mathbf{f} = [f(p_1), \\dots, f(p_n)]^T$ 和在测试点处的潜值 $f_\\star$ 服从一个联合高斯分布。由于先验均值为零，该分布为：\n$$ \\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) & K(X, p_\\star) \\\\ K(p_\\star, X) & k(p_\\star, p_\\star) \\end{pmatrix} \\right) $$\n其中 $K(X, X)$ 是 $n \\times n$ 的协方差矩阵，其元素为 $K_{ij} = k(p_i, p_j)$；$K(X, p_\\star)$ 是训练点和测试点之间协方差的 $n \\times 1$ 向量；$k(p_\\star, p_\\star)$ 是测试点处的先验方差。\n\n观测模型 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$（其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$）意味着观测到的训练输出 $\\mathbf{y}$ 和潜测试值 $f_\\star$ 的联合分布也是高斯分布：\n$$ \\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, p_\\star) \\\\ K(p_\\star, X) & k(p_\\star, p_\\star) \\end{pmatrix} \\right) $$\n为方便表示，令 $K = K(X, X)$，$\\mathbf{k}_\\star = K(X, p_\\star)$，以及 $k_{\\star\\star} = k(p_\\star, p_\\star)$。矩阵 $K_y = K + \\sigma_n^2 I$ 表示带噪声观测值的协方差。\n\n后验预测分布 $p(f_\\star | X, \\mathbf{y}, p_\\star)$ 是通过应用多元高斯分布的条件化规则推导出来的。这产生一个具有以下均值和方差的高斯后验：\n\n后验预测均值：\n$$ \\mu_\\star = \\mathbb{E}[f_\\star | X, \\mathbf{y}, p_\\star] = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y} $$\n后验预测方差：\n$$ \\sigma_\\star^2 = \\text{Var}[f_\\star | X, \\mathbf{y}, p_\\star] = k_{\\star\\star} - \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_\\star $$\n问题要求的是后验预测标准差，即 $s_\\star = \\sqrt{\\sigma_\\star^2}$。这个量度量的是对真实函数值 $f(p_\\star)$ 估计的不确定性，而不是对带噪声观测值 $y_\\star$ 的不确定性。\n\n每个测试用例的逐步计算过程如下：\n1. 定义大小为 $n=7$ 的训练数据向量 $X$ 和 $\\mathbf{y}$。\n2. 对于给定的测试用例，包括超参数 $\\sigma_f, \\ell$、噪声 $\\sigma_n$ 和测试点 $p_\\star$：\n3. 构建 $7 \\times 7$ 的训练协方差矩阵 $K$，其元素为 $K_{ij} = \\sigma_f^2 \\exp(-\\frac{(p_i - p_j)^2}{2\\ell^2})$。\n4. 构建 $7 \\times 1$ 的测试协方差向量 $\\mathbf{k}_\\star$，其元素为 $k_{\\star,i} = k(p_i, p_\\star) = \\sigma_f^2 \\exp(-\\frac{(p_i - p_\\star)^2}{2\\ell^2})$。\n5. 测试点处的先验方差为 $k_{\\star\\star} = k(p_\\star, p_\\star) = \\sigma_f^2$。\n6. 构造带噪声的协方差矩阵 $K_y = K + \\sigma_n^2 I$，其中 $I$ 是 $7 \\times 7$ 的单位矩阵，且 $\\sigma_n^2 = 2.0^2 = 4.0$。\n7. 为确保数值稳定性，避免直接进行矩阵求逆。取而代之，我们求解两个线性方程组。首先，求解 $(K_y) \\boldsymbol{\\alpha} = \\mathbf{y}$ 以得到向量 $\\boldsymbol{\\alpha}$。然后，后验均值由点积 $\\mu_\\star = \\mathbf{k}_\\star^T \\boldsymbol{\\alpha}$ 给出。\n8. 其次，求解 $(K_y) \\mathbf{v} = \\mathbf{k}_\\star$ 以得到向量 $\\mathbf{v}$。然后，后验方差计算为 $\\sigma_\\star^2 = k_{\\star\\star} - \\mathbf{k}_\\star^T \\mathbf{v}$。求解这些系统的最稳健方法是使用 $K_y$ 的 Cholesky 分解，因为 $K_y$ 保证是对称正定矩阵。\n9. 后验标准差为 $s_\\star = \\sqrt{\\sigma_\\star^2}$。必须注意处理因浮点运算错误可能导致的 $\\sigma_\\star^2$ 的微小负值，确保平方根的参数为非负数。\n10. 将得到的 $\\mu_\\star$ 和 $s_\\star$ 值四舍五入到三位小数。\n\n对四个指定的测试用例，以编程方式实现此完整过程。将结果按照要求组合成最终的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Computes the posterior predictive mean and standard deviation for a Gaussian Process\n    regression model based on the provided problem statement.\n    \"\"\"\n    \n    # Define the training data from the problem statement.\n    p_train = np.array([3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5])\n    y_train = np.array([176, 168, 160, 150, 140, 130, 122])\n\n    # Define the test cases from the problem statement.\n    # Each tuple is (p_star, sigma_f, l, sigma_n).\n    test_cases = [\n        (5.2, 60.0, 0.6, 2.0),\n        (3.2, 60.0, 0.6, 2.0),\n        (7.0, 60.0, 0.6, 2.0),\n        (5.2, 60.0, 0.2, 2.0),\n    ]\n\n    results = []\n    \n    # Reshape training inputs to be a column vector for broadcasting\n    p_train_col = p_train[:, np.newaxis]\n\n    for case in test_cases:\n        p_star, sigma_f, l, sigma_n = case\n\n        # Main logic to calculate the result for one case goes here.\n        \n        # 1. Define the squared exponential covariance function (kernel).\n        def kernel(a, b, sf, ls):\n            \"\"\"Squared exponential kernel.\"\"\"\n            # Using scipy.spatial.distance.cdist would be cleaner but not allowed.\n            # We use broadcasting to compute the squared Euclidean distances.\n            sqdist = (a - b.T)**2\n            return (sf**2) * np.exp(-sqdist / (2 * ls**2))\n\n        # 2. Compute the covariance matrices and vectors.\n        # K(X, X): Covariance of training inputs\n        K = kernel(p_train_col, p_train_col, sigma_f, l)\n        \n        # K_y = K(X, X) + sigma_n^2 * I: Covariance of noisy observations\n        K_y = K + (sigma_n**2) * np.eye(len(p_train))\n        \n        # k_star = K(X, x_star): Covariance between training and test inputs\n        k_star = kernel(p_train_col, np.array([[p_star]]), sigma_f, l).flatten()\n        \n        # k_star_star = k(x_star, x_star): Prior variance at the test point\n        k_star_star = sigma_f**2\n\n        # 3. Compute posterior predictive mean and variance.\n        # Use Cholesky decomposition for stable and efficient linear system solving.\n        # K_y is symmetric and positive definite. L is its lower-triangular Cholesky factor.\n        try:\n            L = cholesky(K_y, lower=True)\n        except np.linalg.LinAlgError:\n            # This should not happen with a valid kernel and non-zero noise.\n            # Handle as an error if it occurs.\n            results.extend([np.nan, np.nan])\n            continue\n            \n        # Solve (K + sigma_n^2*I) * alpha = y for alpha.\n        # This is equivalent to alpha = inv(K + sigma_n^2*I) @ y.\n        alpha = cho_solve((L, True), y_train)\n        \n        # Calculate posterior mean: mu_star = k_star.T @ alpha\n        mu_star = k_star.T @ alpha\n        \n        # Solve (K + sigma_n^2*I) * v = k_star for v.\n        # This is equivalent to v = inv(K + sigma_n^2*I) @ k_star.\n        v = cho_solve((L, True), k_star)\n        \n        # Calculate posterior variance: sigma_star^2 = k_star_star - k_star.T @ v\n        var_star = k_star_star - k_star.T @ v\n        \n        # Ensure variance is non-negative due to potential floating point errors.\n        s_star = np.sqrt(max(0, var_star))\n        \n        # 4. Append rounded results to the list.\n        results.append(round(mu_star, 3))\n        results.append(round(s_star, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441367"}, {"introduction": "现实世界中的工程和科学问题通常涉及多个输入变量。这个练习将我们的模型从一维扩展到二维，模拟细菌生长速率如何受营养物浓度和温度的共同影响。我们将在这里引入各向异性核（anisotropic kernel）的概念[@problem_id:2441369]，它允许模型为每个输入维度学习不同的特征长度尺度。这对于准确捕捉不同输入对输出影响各异的复杂系统至关重要。", "problem": "一个实验室表征了细菌菌落的比生长速率与营养物浓度和温度的函数关系。设输入为二维向量 $x = (n,t)$，其中 $n$ 是营养物浓度（单位：毫克/毫升），$t$ 是温度（单位：摄氏度）。响应为比生长速率 $g(x)$（单位：小时的倒数）。您必须使用高斯过程（GP）回归为 $g(x)$ 构建一个代理模型，该模型具有零均值先验、各向异性平方指数协方差和同方差高斯观测噪声。协方差函数为\n$$\nk\\big((n,t),(n',t')\\big) = \\sigma_f^2 \\exp\\left( -\\frac{1}{2}\\left(\\frac{(n-n')^2}{\\ell_n^2} + \\frac{(t-t')^2}{\\ell_t^2}\\right) \\right),\n$$\n超参数为 $\\sigma_f = 0.9$，$\\ell_n = 3.0$，$\\ell_t = 5.0$。观测值具有独立的加性高斯噪声，标准差为 $\\sigma_n = 0.05$。\n\n给出以下实验室实验数据，每组数据报告了 $(n,t)$ 和测量的生长速率 $g$：\n- $(n,t) = (0.5,25.0)$，$g = 0.10$。\n- $(n,t) = (1.0,30.0)$，$g = 0.25$。\n- $(n,t) = (2.0,30.0)$，$g = 0.40$。\n- $(n,t) = (4.0,35.0)$，$g = 0.80$。\n- $(n,t) = (6.0,37.0)$，$g = 1.00$。\n- $(n,t) = (8.0,40.0)$，$g = 0.85$。\n- $(n,t) = (10.0,42.0)$，$g = 0.60$。\n\n使用上述指定的高斯过程回归模型和提供的实验数据，计算在以下查询条件下 $g$ 的后验预测均值 $(n,t)$：\n- $(5.0,37.0)$，\n- $(0.5,20.0)$，\n- $(10.0,42.0)$，\n- $(2.0,30.0)$，\n- $(7.0,33.0)$，\n- $(0.0,37.0)$。\n\n将每个预测的平均生长速率以小时的倒数表示，并将每个值四舍五入到六位小数。查询的测试集分别涵盖了内点、边界条件、精确的训练输入以及超出观测营养物范围的外推。\n\n您的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序与上述查询相同（例如，$[r_1,r_2,r_3,r_4,r_5,r_6]$），其中每个 $r_i$ 是一个四舍五入到六位小数的浮点数，单位为小时的倒数。", "solution": "该问题要求使用高斯过程（GP）回归构建一个代理模型，以预测细菌菌落的比生长速率 $g$ 与营养物浓度 $n$ 和温度 $t$ 的函数关系。任务是在给定一组实验观测数据和指定的模型超参数的情况下，计算若干查询点处的生长速率的后验预测均值。\n\n高斯过程定义了函数上的一个分布。我们将未知的生长速率函数 $g(x)$ 建模为从一个高斯过程中抽取的样本。该模型由一个均值函数 $\\mu(x)$ 和一个协方差函数（或核函数）$k(x,x')$ 指定。问题陈述了零均值先验，因此 $\\mu(x) = 0$。假设观测值受到独立同分布的高斯噪声的干扰，因此如果 $f(x)$ 是真实的潜在函数值，则观测值为 $y = f(x) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$。\n\n对于一组包含 $N$ 个训练点 $X = \\{x_1, \\dots, x_N\\}$ 及其对应观测值 $y = \\{y_1, \\dots, y_N\\}^T$ 的集合，以及一组包含 $M$ 个测试点 $X_* = \\{x_{*1}, \\dots, x_{*M}\\}$ 的集合，观测值 $y$ 和测试点处的函数值 $f_*$ 的联合分布是高斯分布：\n$$\n\\begin{pmatrix} y \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N} \\left( 0,\n\\begin{pmatrix}\nK(X, X) + \\sigma_n^2 I & K(X, X_*) \\\\\nK(X_*, X) & K(X_*, X_*)\n\\end{pmatrix}\n\\right)\n$$\n其中 $K(A, B)$ 是在集合 $A$ 和 $B$ 中所有点对之间计算出的协方差矩阵，$I$ 是 $N \\times N$ 的单位矩阵，$\\sigma_n^2$ 是噪声方差。\n\n通过以已知观测值 $y$ 为条件对联合分布进行条件化，我们得到 $f_*$ 的后验预测分布，该分布也是高斯分布，$p(f_* | X, y, X_*) = \\mathcal{N}(\\bar{f}_*, \\text{cov}(f_*))$。这个后验分布的均值，也就是我们想要的预测值，由标准的高斯过程回归方程给出：\n$$\n\\bar{f}_* = K(X_*, X) [K(X, X) + \\sigma_n^2 I]^{-1} y\n$$\n计算过程遵循此方程。\n\n该问题的具体输入如下：\n- 超参数：信号标准差 $\\sigma_f = 0.9$，噪声标准差 $\\sigma_n = 0.05$，以及各向异性长度尺度 $\\ell_n = 3.0$ 和 $\\ell_t = 5.0$。\n- 协方差函数：各向异性平方指数核函数，\n$$\nk\\big((n,t),(n',t')\\big) = \\sigma_f^2 \\exp\\left( -\\frac{1}{2}\\left(\\frac{(n-n')^2}{\\ell_n^2} + \\frac{(t-t')^2}{\\ell_t^2}\\right) \\right)\n$$\n- 训练数据（$N=7$ 个点）：\n  输入为 $X = \\begin{bmatrix} 0.5 & 25.0 \\\\ 1.0 & 30.0 \\\\ 2.0 & 30.0 \\\\ 4.0 & 35.0 \\\\ 6.0 & 37.0 \\\\ 8.0 & 40.0 \\\\ 10.0 & 42.0 \\end{bmatrix}$，观测输出为 $y = \\begin{bmatrix} 0.10, 0.25, 0.40, 0.80, 1.00, 0.85, 0.60 \\end{bmatrix}^T$。\n- 查询点（$M=6$ 个点）：\n  查询输入为 $X_* = \\begin{bmatrix} 5.0 & 37.0 \\\\ 0.5 & 20.0 \\\\ 10.0 & 42.0 \\\\ 2.0 & 30.0 \\\\ 7.0 & 33.0 \\\\ 0.0 & 37.0 \\end{bmatrix}$。\n\n计算过程如下：\n\n步骤1：计算 $N \\times N$ 的训练协方差矩阵 $K(X, X)$。\n该矩阵的元素 $(i, j)$ 使用核函数 $k(x_i, x_j)$ 计算。例如，对角线元素为 $k(x_i, x_i) = \\sigma_f^2 \\exp(0) = 0.9^2 = 0.81$。非对角线元素衡量不同训练点之间的相关性。\n\n步骤2：构建含噪声的观测协方差矩阵 $K_y$。\n该矩阵通过将噪声方差 $\\sigma_n^2$ 加到 $K(X, X)$ 的对角线上形成。\n$$K_y = K(X, X) + \\sigma_n^2 I$$\n这里，$\\sigma_n^2 = 0.05^2 = 0.0025$。这一项对矩阵进行正则化，使其可逆，并解释了观测中的不确定性。\n\n步骤3：求解权重向量 $\\alpha$。\n为保证数值稳定性和效率，我们避免显式地进行矩阵求逆。而是求解线性方程组 $K_y \\alpha = y$ 以得到向量 $\\alpha$。这个向量可以解释为与每个训练点相对应的一组权重。\n$$\\alpha = [K(X, X) + \\sigma_n^2 I]^{-1} y$$\n\n步骤4：计算 $M \\times N$ 的交叉协方差矩阵 $K(X_*, X)$。\n该矩阵的元素 $(j, i)$ 是 $k(x_{*j}, x_i)$，表示第 $j$ 个测试点和第 $i$ 个训练点之间的协方差。\n\n步骤5：计算后验预测均值 $\\bar{f}_*$。\n最终的预测是通过对 $K(X_*, X)$ 的列进行加权求和得到的，权重由向量 $\\alpha$ 给出。这是一个简单的矩阵-向量乘积：\n$$\\bar{f}_* = K(X_*, X) \\alpha$$\n这会产生一个包含 $M$ 个预测均值的向量，每个查询点 $X_*$ 对应一个值。\n\n这些步骤的数值实现是使用矩阵运算完成的。对平方距离矩阵（作为核函数中指数函数的参数）进行向量化计算，为构建协方差矩阵 $K(X, X)$ 和 $K(X_*, X)$ 提供了一种高效的方法。然后将得到的预测均值向量四舍五入到六位小数，以生成最终答案。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a Gaussian Process regression model and computes the posterior\n    predictive mean for a given set of query points.\n    \"\"\"\n    # Define the hyperparameters for the GP model.\n    # sigma_f: Signal standard deviation for the squared-exponential kernel.\n    # l_n, l_t: Anisotropic length-scales for nutrient and temperature.\n    # sigma_n: Standard deviation of the homoscedastic Gaussian noise.\n    sigma_f = 0.9\n    l_n = 3.0\n    l_t = 5.0\n    sigma_n = 0.05\n\n    # Define the training data from laboratory experiments.\n    # X_train: (N x 2) matrix of input features (n, t).\n    # y_train: (N,) vector of observed growth rates g.\n    X_train = np.array([\n        [0.5, 25.0],\n        [1.0, 30.0],\n        [2.0, 30.0],\n        [4.0, 35.0],\n        [6.0, 37.0],\n        [8.0, 40.0],\n        [10.0, 42.0]\n    ])\n    y_train = np.array([0.10, 0.25, 0.40, 0.80, 1.00, 0.85, 0.60])\n\n    # Define the query points for which predictions are required.\n    # X_query: (M x 2) matrix of query inputs (n, t).\n    test_cases = [\n        (5.0, 37.0),\n        (0.5, 20.0),\n        (10.0, 42.0),\n        (2.0, 30.0),\n        (7.0, 33.0),\n        (0.0, 37.0),\n    ]\n    X_query = np.array(test_cases)\n\n    # Anisotropic length-scales organized into a vector.\n    length_scales = np.array([l_n, l_t])\n\n    def kernel(X1, X2, sigma_f_val, length_scales_val):\n        \"\"\"\n        Computes the anisotropic squared-exponential covariance matrix between\n        two sets of input points X1 and X2.\n        \n        Args:\n            X1 (np.ndarray): An (N x D) array of N points in D dimensions.\n            X2 (np.ndarray): An (M x D) array of M points in D dimensions.\n            sigma_f_val (float): The signal variance hyperparameter.\n            length_scales_val (np.ndarray): A (D,) array of length-scales.\n        \n        Returns:\n            np.ndarray: The (N x M) covariance matrix.\n        \"\"\"\n        # Scale inputs by their respective length scales for anisotropic kernel.\n        X1_scaled = X1 / length_scales_val\n        X2_scaled = X2 / length_scales_val\n\n        # Efficiently compute the matrix of squared Euclidean distances\n        # between all pairs of points from X1_scaled and X2_scaled.\n        sq_dists = np.sum(X1_scaled**2, axis=1)[:, np.newaxis] + \\\n                   np.sum(X2_scaled**2, axis=1) - \\\n                   2 * (X1_scaled @ X2_scaled.T)\n        \n        # Apply the squared-exponential formula.\n        return sigma_f_val**2 * np.exp(-0.5 * sq_dists)\n\n    # Step 1: Compute K(X, X), the covariance matrix of training data.\n    K_XX = kernel(X_train, X_train, sigma_f, length_scales)\n\n    # Step 2: Add observation noise variance to the diagonal to get K_y.\n    N = X_train.shape[0]\n    K_y = K_XX + (sigma_n**2) * np.eye(N)\n\n    # Step 3: Solve the linear system K_y * alpha = y_train for alpha.\n    # This is numerically more stable than computing K_y^-1 directly.\n    alpha = np.linalg.solve(K_y, y_train)\n\n    # Step 4: Compute K(X_*, X), the cross-covariance between query and training points.\n    K_Xstar_X = kernel(X_query, X_train, sigma_f, length_scales)\n    \n    # Step 5: Compute the posterior predictive mean by multiplying K(X_*, X) with alpha.\n    f_star_mean = K_Xstar_X @ alpha\n\n    # Format the results: round to six decimal places as specified.\n    results = [round(val, 6) for val in f_star_mean]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441369"}, {"introduction": "在掌握了如何应用高斯过程之后，理解其局限性同样重要。这个高级练习将指导你设计一个计算实验，以经验性地探究“维度灾难”对高斯过程模型的影响。通过测量模型达到特定精度所需的训练样本数量如何随输入维度的增加而增长[@problem_id:2441366]，你将对高维函数建模的挑战有更深刻的认识。这个过程不仅能巩固你的编程和建模技能，还能让你体会到在高维空间中有效采样和模型选择的根本重要性。", "problem": "要求您设计并实现一个完整、可运行的程序，用于经验性地估计高斯过程（GP）代理模型所需的最小训练样本数量随输入维度的增长情况。您的任务是，对于定义在单位超立方体上的一个指定的光滑函数族，在一个固定的、维度匹配的测试集上，测量达到目标均方根误差（RMSE）所需的最小训练集大小。该程序必须遵循严格的规范，并生成单行输出，该输出汇总了所提供测试套件的结果。\n\n从以下基本概念开始：\n- 高斯过程（GP）是随机变量的集合，其任何有限子集都服从联合高斯分布。一个高斯过程由其均值函数和协方差函数完全确定。\n- 平方指数协方差函数（也称为径向基函数核）是一种广泛使用的选择，它编码了平滑性。对于任何维度，它都是平稳且正定的。\n- 多元正态分布的条件属性给出了高斯过程回归的闭式后验预测均值，该均值通过求解由核矩阵加上一个用于数值稳定性的噪声项所定义的线性系统获得。\n- 均方根误差（RMSE）是预测误差平方的算术平均值的平方根。\n\n问题设置：\n- 对于维度 $D \\in \\mathbb{N}$，将目标函数 $f_D : [0,1]^D \\to \\mathbb{R}$ 定义为\n$$\nf_D(\\mathbf{x}) \\;=\\; \\frac{1}{\\sqrt{D}} \\sum_{i=1}^{D} \\sin\\!\\big(2\\pi x_i\\big),\n$$\n其中 $\\mathbf{x} = (x_1,\\dots,x_D)^\\top \\in [0,1]^D$。这种归一化操作可以产生大致与维度无关的振幅，并使函数在每个坐标上都保持光滑和周期性。\n- 使用零均值和平方指数协方差的高斯过程回归\n$$\nk(\\mathbf{x},\\mathbf{x}') \\;=\\; \\sigma_f^2 \\exp\\!\\left( -\\frac{1}{2} \\sum_{i=1}^{D} \\frac{(x_i - x_i')^2}{\\ell^2} \\right),\n$$\n使用固定的超参数 $\\sigma_f^2 = 1$ 和 $\\ell = 0.2$。在核矩阵的对角线上添加一个独立的、同方差的高斯噪声方差 $\\sigma_n^2 = 10^{-10}$，纯粹作为数值稳定性“金块”（nugget）；将训练观测值视为 $f_D$ 的无噪声目标。\n- 使用低差异的Halton序列在$[0,1]^D$中生成点集以避免随机性。对于每个维度 $D$，训练输入必须是加扰Halton序列（加扰种子为 $12345$）的前 $n$ 个点，测试输入必须是独立的加扰Halton序列（加扰种子为 $98765$）的前 $M$ 个点。两个序列必须为每个 $D$ 重新生成，并且通过构造确保训练集和测试集之间是独立的。设 $M = 512$。\n\n测量协议：\n- 对于每个测试用例 $(D, \\tau, n_{\\max})$，您必须找到最小整数 $n$（训练样本数），使得在Halton序列的前 $n$ 个点上训练的GP，在固定的 $M$ 点测试集上实现的RMSE小于或等于阈值 $\\tau$：\n$$\n\\mathrm{RMSE}(n) \\;=\\; \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} \\big(\\hat{f}_D(\\mathbf{x}^{(\\mathrm{test})}_j) - f_D(\\mathbf{x}^{(\\mathrm{test})}_j)\\big)^2} \\;\\le\\; \\tau,\n$$\n其中 $\\hat{f}_D$ 是GP后验预测均值。\n- 搜索策略要求：为确保可复现性和覆盖范围，您的程序必须使用以下两阶段搜索来寻找最小的 $n$。\n  1. 粗略的几何增长搜索以界定可行范围：对序列 $\\{4, 8, 16, 32, 64, 128, 256\\}$ 中的 $n$（截断至值 $\\le n_{\\max}$）评估 $\\mathrm{RMSE}(n)$。如果没有值满足阈值，则报告该测试用例在预算内不可行，为此用例返回 $-1$。\n  2. 如果在某个 $n^\\star$ 处首次满足阈值，并且之前在某个 $n^{-}  n^\\star$ 处未满足，则通过依次检查 $n = n^{-} + 1, n^{-} + 2, \\dots, n^\\star - 1$ 进行线性细化，以确定满足阈值的最小 $n$。如果阈值在第一个粗略值处就已满足（之前没有未满足的情况），则根据定义，该值就是最小的 $n$。\n- 在同一维度下，对所有不同的 $n$ 值，应使用大小为 $M$ 的同一个固定测试集。\n\n测试套件：\n- 您的程序必须按给定顺序评估以下六个测试用例，并为每个用例返回一个整数结果：\n  1. $(D=\\;1,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  2. $(D=\\;2,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  3. $(D=\\;4,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  4. $(D=\\;8,\\; \\tau=\\;0.05,\\; n_{\\max}=\\;256)$\n  5. $(D=\\;8,\\; \\tau=\\;0.02,\\; n_{\\max}=\\;64)$\n  6. $(D=\\;1,\\; \\tau=\\;0.20,\\; n_{\\max}=\\;8)$\n- 这些用例包括一个渐进的维度扫描，以经验性地揭示维度灾难；一个带有小预算的有意设置的严格阈值，以检验不可行情况的处理；以及一个带有小预算的宽松阈值，以检验接近边界的可行情况。\n\n最终输出规范：\n- 您的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，顺序与上述测试用例相同。例如，一个有效的输出格式是\n$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6]$\n其中每个 $\\text{result}_i$ 是一个整数，约定如果没有任何 $n \\le n_{\\max}$ 满足 $\\mathrm{RMSE}(n) \\le \\tau$，则 $\\text{result}_i=-1$。\n\n不涉及物理单位。根据构造，正弦函数中隐含的所有角度都以弧度为单位。所有误差阈值必须被视为实数，而不是百分比。最终输出必须是严格符合指定格式的单行文本，不含任何附加文本。", "solution": "所提出的问题是代理建模领域中一个定义明确的计算实验，具体涉及高斯过程（GP）回归的应用。其目标是经验性地确定一个GP模型在测试函数上达到指定预测精度所需的最小训练样本数（记为 $n$）。这项研究将针对不同的输入维度 $D$ 和精度阈值 $\\tau$ 进行，从而在此背景下探索所谓的“维度灾难”。该问题在科学上是合理的，在数学上是明确的，在计算上是可验证的。因此，我将着手提供一个完整的解决方案。\n\n解决方案的方法论基于应用于函数近似的贝叶斯推断原理，这也是高斯过程回归的理论基础。一个GP定义了函数空间上的先验分布。给定一组训练数据点，这个先验分布会被更新为拟合数据的函数后验分布。这个后验分布的均值即为对新的、未见过的点的函数值的预测。\n\n一个高斯过程由一个均值函数 $m(\\mathbf{x})$ 和一个协方差函数（或称核函数）$k(\\mathbf{x}, \\mathbf{x}')$ 完全确定。在这个问题中，我们被要求使用零均值函数，$m(\\mathbf{x}) = 0$，以及平方指数协方差函数：\n$$\nk(\\mathbf{x},\\mathbf{x}') = \\sigma_f^2 \\exp\\!\\left( -\\frac{1}{2\\ell^2} \\sum_{i=1}^{D} (x_i - x_i')^2 \\right) = \\sigma_f^2 \\exp\\!\\left( -\\frac{\\|\\mathbf{x} - \\mathbf{x}'\\|^2_2}{2\\ell^2} \\right)\n$$\n超参数给定为信号方差 $\\sigma_f^2 = 1$ 和长度尺度 $\\ell = 0.2$。该核函数假设要建模的函数是无限可微且平稳的。\n\n设训练数据为一组 $n$ 个输入点 $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ 及其对应的函数值 $\\mathbf{y} = \\{f_D(\\mathbf{x}_1), \\dots, f_D(\\mathbf{x}_n)\\}^\\top$。目标函数指定为：\n$$\nf_D(\\mathbf{x}) = \\frac{1}{\\sqrt{D}} \\sum_{i=1}^{D} \\sin(2\\pi x_i)\n$$\n对于一组 $M$ 个测试点 $\\mathbf{X}_* = \\{\\mathbf{x}_1^*, \\dots, \\mathbf{x}_M^*\\}$，我们希望预测其对应的函数值 $\\mathbf{f}_*$。根据GP理论，训练输出 $\\mathbf{y}$ 和测试输出 $\\mathbf{f}_*$ 的联合分布是一个多元高斯分布：\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}  \\mathbf{K}(\\mathbf{X}, \\mathbf{X}_*) \\\\ \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X})  \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}_*) \\end{pmatrix} \\right)\n$$\n其中 $[\\mathbf{K}(\\mathbf{A}, \\mathbf{B})]_{ij} = k(\\mathbf{a}_i, \\mathbf{b}_j)$。$\\sigma_n^2 \\mathbf{I}$ 项代表观测中的噪声。在这里，它被指定为一个很小的数值“金块”（nugget）$\\sigma_n^2 = 10^{-10}$，以确保协方差矩阵的正定性和数值稳定性，尽管训练数据被认为是无噪声的。\n\n通过对联合分布基于观测到的训练数据进行条件化，我们得到 $\\mathbf{f}_*$ 的后验分布，该分布也是高斯的。其均值即为我们的预测值 $\\hat{\\mathbf{f}}_*$，由下式给出：\n$$\n\\hat{\\mathbf{f}}_* = \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) [\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}]^{-1} \\mathbf{y}\n$$\n实现时将避免直接进行矩阵求逆，因为这种方法在数值上不稳定且计算效率低下。一个更稳健的方法是首先求解线性系统 $(\\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}) \\boldsymbol{\\alpha} = \\mathbf{y}$ 来得到向量 $\\boldsymbol{\\alpha}$。然后，预测值通过 $\\hat{\\mathbf{f}}_* = \\mathbf{K}(\\mathbf{X}_*, \\mathbf{X}) \\boldsymbol{\\alpha}$ 计算得出。由于矩阵 $\\mathbf{K}_{reg} = \\mathbf{K}(\\mathbf{X}, \\mathbf{X}) + \\sigma_n^2 \\mathbf{I}$ 是对称正定的，这个系统可以使用Cholesky分解高效求解。我们计算Cholesky因子 $\\mathbf{L}$ 使得 $\\mathbf{L}\\mathbf{L}^\\top = \\mathbf{K}_{reg}$，然后通过前向和后向替换求解 $\\boldsymbol{\\alpha}$。\n\n该问题要求使用一个特定的测量协议来找到最小的 $n$。对于每个测试用例 $(D, \\tau, n_{\\max})$，执行以下步骤：\n1.  **数据生成**：对于给定的维度 $D$，使用加扰Halton序列生成两个独立的、确定性的点集。\n    -   一个包含 $n_{\\max}$ 个点的训练池，位于 $[0,1]^D$ 中，使用加扰种子 $12345$。\n    -   一个包含 $M=512$ 个点的固定测试集，位于 $[0,1]^D$ 中，使用加扰种子 $98765$。\n    -   使用 $f_D(\\mathbf{x})$ 计算所有点对应的真实函数值。\n\n2.  **搜索最小 $n$**：执行一个两阶段搜索。\n    -   **粗略搜索**：对于几何增长的样本量序列 $n \\in \\{4, 8, 16, 32, 64, 128, 256\\}$（限制于 $n \\le n_{\\max}$），对模型进行训练和评估。对于每个 $n$，使用训练池中的前 $n$ 个点。计算均方根误差（RMSE）：\n    $$\n    \\mathrm{RMSE}(n) = \\sqrt{\\frac{1}{M} \\sum_{j=1}^{M} (\\hat{f}_D(\\mathbf{x}_j^*) - f_D(\\mathbf{x}_j^*))^2}\n    $$\n    - 如果 $\\mathrm{RMSE}(n) \\le \\tau$，我们就找到了最小 $n$ 的一个上界。粗略搜索将持续进行，直到找到第一个满足条件的 $n$，记为 $n^\\star$。\n    -   **精细搜索**：如果粗略搜索在 $n^\\star$ 处成功，并且前一个粗略点 $n^-  n^\\star$（未通过测试），则在 $[n^- + 1, n^\\star]$ 范围内对 $n$ 进行线性搜索。该范围内第一个满足 $\\mathrm{RMSE}(n) \\le \\tau$ 的 $n$ 值即为所需的最小样本量。如果测试的第一个粗略点就已经满足标准，则该点被声明为最小值。\n    -   **不可行性**：如果在粗略搜索中直到 $n_{\\max}$ 都没有 $n$ 满足RMSE阈值，则该用例被视为不可行，结果报告为 $-1$。\n\n整体算法通过遍历每个提供的测试用例元组 $(D, \\tau, n_{\\max})$，执行上述过程，并收集整数结果来进行。最终输出是将这些结果汇集成的一个格式化字符串。实现将依赖 `numpy` 进行高效的数值计算，`scipy.stats.qmc.Halton` 用于生成Halton序列，以及 `scipy.linalg.cho_solve` 用于稳定的GP预测。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import qmc\nfrom scipy.linalg import cholesky, cho_solve\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem specification for all test cases.\n    \"\"\"\n\n    def target_function(x, D):\n        \"\"\"\n        Computes the target function f_D(x).\n        x is an (N, D) array.\n        Returns an (N,) array.\n        \"\"\"\n        if D == 0:\n            return np.zeros(x.shape[0])\n        return (1.0 / np.sqrt(D)) * np.sum(np.sin(2.0 * np.pi * x), axis=1)\n\n    def squared_exponential_kernel(X1, X2, D, sigma_f_sq=1.0, length_scale=0.2):\n        \"\"\"\n        Computes the squared exponential kernel matrix between two sets of points.\n        X1 is (N1, D), X2 is (N2, D).\n        Returns an (N1, N2) kernel matrix.\n        \"\"\"\n        # Using cdist is more stable and often faster than manual broadcasting for squared Euclidean distances\n        sqdist = cdist(X1 / length_scale, X2 / length_scale, 'sqeuclidean')\n        return sigma_f_sq * np.exp(-0.5 * sqdist)\n\n    def predict_gp(X_train, y_train, X_test, D, sigma_n_sq=1e-10):\n        \"\"\"\n        Computes the GP posterior predictive mean.\n        \"\"\"\n        n_train = X_train.shape[0]\n        if n_train == 0:\n            return np.zeros(X_test.shape[0])\n\n        K = squared_exponential_kernel(X_train, X_train, D)\n        K += np.eye(n_train) * sigma_n_sq\n\n        try:\n            # Use Cholesky decomposition for stability and efficiency\n            L = cholesky(K, lower=True)\n            alpha = cho_solve((L, True), y_train)\n        except np.linalg.LinAlgError:\n            # Fallback to direct solve if Cholesky fails, though unlikely with nugget\n            alpha = np.linalg.solve(K, y_train)\n\n        K_star = squared_exponential_kernel(X_test, X_train, D)\n        y_pred = K_star @ alpha\n        return y_pred\n\n    def calculate_rmse(y_pred, y_true):\n        \"\"\"\n        Calculates the Root Mean Squared Error.\n        \"\"\"\n        return np.sqrt(np.mean((y_pred - y_true)**2))\n\n    def find_minimal_n(D, tau, n_max):\n        \"\"\"\n        Finds the minimal number of training samples n to achieve RMSE = tau.\n        \"\"\"\n        # Constants for the experiment\n        M = 512\n        training_seed = 12345\n        test_seed = 98765\n\n        # Generate fixed training pool and test set for the given dimension D\n        halton_train_gen = qmc.Halton(d=D, scramble=True, seed=training_seed)\n        X_train_pool = halton_train_gen.random(n=n_max)\n        y_train_pool = target_function(X_train_pool, D)\n\n        halton_test_gen = qmc.Halton(d=D, scramble=True, seed=test_seed)\n        X_test = halton_test_gen.random(n=M)\n        y_test = target_function(X_test, D)\n\n        coarse_n_values = [4, 8, 16, 32, 64, 128, 256]\n        \n        # Helper function to evaluate RMSE for a given n\n        def evaluate_rmse_at_n(n):\n            if n == 0:\n                return np.inf\n            X_train = X_train_pool[:n, :]\n            y_train = y_train_pool[:n]\n            y_pred = predict_gp(X_train, y_train, X_test, D)\n            return calculate_rmse(y_pred, y_test)\n\n        # 1. Coarse geometric search\n        n_minus = 0\n        n_star = -1\n\n        for n_coarse in coarse_n_values:\n            if n_coarse > n_max:\n                break\n            \n            rmse = evaluate_rmse_at_n(n_coarse)\n\n            if rmse = tau:\n                n_star = n_coarse\n                break\n            else:\n                n_minus = n_coarse\n        \n        # If no coarse point satisfies the threshold\n        if n_star == -1:\n            return -1\n\n        # If the first coarse point is a success, it's the minimum\n        if n_minus == 0:\n            return n_star\n\n        # 2. Fine linear search\n        for n_fine in range(n_minus + 1, n_star + 1):\n            if n_fine > n_max:  # Should not happen with n_star = n_max\n                break\n            rmse = evaluate_rmse_at_n(n_fine)\n            if rmse = tau:\n                return n_fine\n        \n        # This part should be unreachable if logic is correct, but as a safeguard\n        return n_star\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 0.05, 256),\n        (2, 0.05, 256),\n        (4, 0.05, 256),\n        (8, 0.05, 256),\n        (8, 0.02, 64),\n        (1, 0.20, 8),\n    ]\n\n    results = []\n    for D, tau, n_max in test_cases:\n        result = find_minimal_n(D, tau, n_max)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2441366"}]}