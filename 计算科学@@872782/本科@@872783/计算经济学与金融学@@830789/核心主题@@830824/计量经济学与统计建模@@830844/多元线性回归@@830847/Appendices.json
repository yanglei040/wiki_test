{"hands_on_practices": [{"introduction": "在构建回归模型时，正确地表示分类变量至关重要。此练习通过一个经典的“虚拟变量陷阱”场景，揭示了当一个预测变量可以被其他预测变量完美线性表示时出现的完全多重共线性问题，这会导致普通最小二乘法（OLS）估计量无法唯一确定。通过解决这个问题 ([@problem_id:2413177])，您将亲身体会到如何通过设置一个基准类别来避免这种陷阱，并正确地设定包含分类数据的模型，这是任何回归分析的基本技能。", "problem": "您正在研究一个用于横截面数据集中时薪的线性模型，该模型包含一个二元性别指示变量。设 $y \\in \\mathbb{R}^n$ 为工资向量，设 $X \\in \\mathbb{R}^{n \\times p}$ 为设计矩阵，其列根据不同情况可能包括截距项（如果存在，则为一列1）、男性虚拟变量和女性虚拟变量。考虑普通最小二乘法 (OLS) 问题，即在 $\\beta \\in \\mathbb{R}^p$ 上最小化残差平方和 $S(\\beta) = \\lVert y - X \\beta \\rVert_2^2$。如果存在唯一的最小化向量，记为 $\\hat{\\beta}$。如果最小化向量的集合不唯一，则将该情况下的报告系数向量定义为所有 $S(\\beta)$ 的最小化向量中欧几里得范数 $\\lVert \\beta \\rVert_2$ 最小的那个。对于下面的每个测试用例，您必须计算 $X$ 的列秩 $r$，确定格拉姆矩阵 $X^\\top X$ 是否可逆（等价于 $r=p$ 是否成立），并按规定计算系数向量。如果 $X^\\top X$ 可逆，则将可逆性指示符报告为整数 $1$，否则报告为 $0$。所有系数项必须四舍五入到小数点后4位。\n\n测试套件。对于每个用例，设计矩阵 $X$ 和响应向量 $y$ 都已明确给出。系数的顺序必须与该用例中 $X$ 的列顺序相匹配。\n\n- 用例 A（尝试在同时存在两种性别时，对截距项、男性虚拟变量和女性虚拟变量进行回归）。此处 $n=6$ 且 $p=3$，其中\n$$\nX_A=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix},\\quad\ny_A=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n\n- 用例 B（使用截距项和单个虚拟变量的正确编码）。此处 $n=6$ 且 $p=2$，其中\n$$\nX_B=\\begin{bmatrix}\n1 & 1\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 0\n\\end{bmatrix},\\quad\ny_B=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n系数顺序为 $[\\text{截距项},\\text{男性}]$。\n\n- 用例 C（当所有观测值均为男性时，尝试对截距项、男性虚拟变量和女性虚拟变量进行回归）。此处 $n=4$ 且 $p=3$，其中\n$$\nX_C=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\n\\end{bmatrix},\\quad\ny_C=\\begin{bmatrix}\n21\\\\\n22\\\\\n20\\\\\n23\n\\end{bmatrix}.\n$$\n\n- 用例 D（无截距项，但包含两个虚拟变量）。此处 $n=6$ 且 $p=2$，其中\n$$\nX_D=\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n1 & 0\\\\\n0 & 1\\\\\n1 & 0\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny_D=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n系数顺序为 $[\\text{男性},\\text{女性}]$。\n\n必需的最终输出格式。您的程序应生成单行输出，其中包含 4 个测试用例的结果，格式为方括号内以逗号分隔的列表。每个结果本身必须是 $[r, \\text{invertibility\\_indicator}, [\\beta_1,\\dots,\\beta_p]]$ 形式的列表，其中 $r$ 是一个整数，可逆性指示符是整数 $1$ 或 $0$，系数子列表的条目是四舍五入到小数点后 4 位的浮点数。例如，整体输出必须如下所示\n$$\n[[r_A, \\text{inv}_A, [\\dots]], [r_B, \\text{inv}_B, [\\dots]], [r_C, \\text{inv}_C, [\\dots]], [r_D, \\text{inv}_D, [\\dots]]].\n$$", "solution": "该问题要求分析线性回归模型的四种不同设定。目标是为每种情况确定设计矩阵 $X$ 的秩、格拉姆矩阵 $X^\\top X$ 的可逆性以及估计的系数向量 $\\beta$。模型是标准的普通最小二乘法 (OLS) 设置，我们旨在找到一个向量 $\\beta \\in \\mathbb{R}^p$ 来最小化残差平方和 $S(\\beta)$：\n$$ S(\\beta) = \\lVert y - X\\beta \\rVert_2^2 = (y - X\\beta)^\\top (y - X\\beta) $$\n其中 $y \\in \\mathbb{R}^n$ 是观测向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵。\n\n$S(\\beta)$ 的最小化向量 $\\hat{\\beta}$ 必须满足正规方程：\n$$ (X^\\top X)\\hat{\\beta} = X^\\top y $$\n解的性质取决于矩阵 $X^\\top X$。\n\n如果 $X$ 的列是线性无关的，则矩阵 $X$ 是满列秩的，即其秩 $r$ 等于列数 $p$。在这种情况下，格拉姆矩阵 $X^\\top X$ 是一个 $p \\times p$ 的正定矩阵，因此是可逆的。正规方程有一个由下式给出的唯一解：\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n\n如果 $X$ 的列是线性相关的，则矩阵 $X$ 是秩亏的，即 $r < p$。这种情况被称为完全多重共线性。格拉姆矩阵 $X^\\top X$ 是奇异的且不可逆。正规方程是一致的，但有无穷多个解。问题在这种情况下指定了一个唯一的选择解的标准：我们必须选择 $S(\\beta)$ 的最小化向量中欧几里得范数 $\\lVert \\beta \\rVert_2$ 最小的那个。这个特定的解由下式给出：\n$$ \\hat{\\beta} = X^+ y $$\n其中 $X^+$ 是 $X$ 的摩尔-彭若斯伪逆。这是由稳健的数值线性代数程序为最小二乘问题提供的标准解。\n\n我们现在开始分析每一种情况。\n\n**用例 A**\n设计矩阵 $X_A$ 和响应向量 $y_A$ 由下式给出：\n$$ X_A=\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix},\\quad y_A=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\n$X_A$ 的列对应于一个截距项、一个男性虚拟变量和一个女性虚拟变量。参数数量为 $p=3$。线性相关性是显而易见的：第一列（截距项）是第二列和第三列（男性和女性虚拟变量）的和。这是经典的“虚拟变量陷阱”。由于这种多重共线性，这些列是线性相关的。$X_A$ 的秩，记为 $r_A$，必须小于 $p$。由于第二列和第三列是线性无关的，所以秩为 $r_A = 2$。\n由于 $r_A < p$，格拉姆矩阵 $X_A^\\top X_A$ 是奇异的，因此其可逆性指示符为 $0$。系数向量 $\\hat{\\beta}_A$ 是最小范数解。所有最小二乘解的集合由方程 $\\hat{\\beta}_0 + \\hat{\\beta}_1 = \\bar{y}_{\\text{男性}} = 22$ 和 $\\hat{\\beta}_0 + \\hat{\\beta}_2 = \\bar{y}_{\\text{女性}} = 19$ 定义。解集可以用 $\\beta_0$ 参数化为：$[\\beta_0, 22-\\beta_0, 19-\\beta_0]^\\top$。最小化该向量的范数 $\\beta_0^2 + (22-\\beta_0)^2 + (19-\\beta_0)^2$，得到 $\\beta_0 = 41/3$。相应的最小范数系数向量是 $\\hat{\\beta}_A = [41/3, 25/3, 16/3]^\\top$。\n数值上，我们发现：\n- $r_A = 2$\n- 可逆性指示符：$0$\n- $\\hat{\\beta}_A \\approx [13.6667, 8.3333, 5.3333]$\n\n**用例 B**\n设计矩阵 $X_B$ 和响应向量 $y_B$ 为：\n$$ X_B=\\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\end{bmatrix},\\quad y_B=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\n这里，$p=2$。列对应于截距项和男性虚拟变量。女性类别是参照组。这两列是线性无关的。因此，该矩阵具有满列秩，$r_B = 2 = p$。\n因此，$X_B^\\top X_B$ 是可逆的，可逆性指示符为 $1$。计算唯一的 OLS 解。$\\hat{\\beta}_0$ 估计参照组（女性）的平均工资，而 $\\hat{\\beta}_1$ 估计男性和女性平均工资之间的差异。\n女性平均工资：$(\\frac{19+18+20}{3}) = 19$。因此，$\\hat{\\beta}_0 = 19$。\n男性平均工资：$(\\frac{21+23+22}{3}) = 22$。因此，$\\hat{\\beta}_0 + \\hat{\\beta}_1 = 22$，这意味着 $\\hat{\\beta}_1 = 22 - 19 = 3$。\n- $r_B = 2$\n- 可逆性指示符：$1$\n- $\\hat{\\beta}_B = [19.0000, 3.0000]$\n\n**用例 C**\n设计矩阵 $X_C$ 和响应向量 $y_C$ 为：\n$$ X_C=\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\end{bmatrix},\\quad y_C=\\begin{bmatrix} 21 \\\\ 22 \\\\ 20 \\\\ 23 \\end{bmatrix} $$\n参数数量为 $p=3$。所有观测值都是男性，这导致了两种形式的完全多重共线性。首先，截距项列与男性虚拟变量列完全相同。其次，女性虚拟变量列是一个零向量。列空间由一个全为1的向量张成。因此，秩为 $r_C = 1$。\n由于 $r_C < p$，矩阵 $X_C^\\top X_C$ 是奇异的，可逆性指示符为 $0$。我们必须找到最小范数解。该模型实际上是尝试使用预测变量的线性组合来估计平均工资 $\\bar{y}_C = \\frac{21+22+20+23}{4} = 21.5$。对任何观测值的预测都是 $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1$。因此，任何解都必须满足 $\\hat{\\beta}_0 + \\hat{\\beta}_1 = 21.5$。系数 $\\hat{\\beta}_2$ 不影响残差，所以为了最小范数，它必须为 $0$。然后我们最小化 $\\beta_0^2 + \\beta_1^2$，约束条件为 $\\beta_0 + \\beta_1 = 21.5$，得到 $\\hat{\\beta}_0 = \\hat{\\beta}_1 = 21.5 / 2 = 10.75$。\n- $r_C = 1$\n- 可逆性指示符：$0$\n- $\\hat{\\beta}_C = [10.7500, 10.7500, 0.0000]$\n\n**用例 D**\n设计矩阵 $X_D$ 和响应向量 $y_D$ 为：\n$$ X_D=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad y_D=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\n这个模型省略了截距项，并为每个类别（男性和女性）都包含一个虚拟变量。参数数量为 $p=2$。这些列是正交的，因此是线性无关的。该矩阵具有满列秩，$r_D = 2 = p$。\n因此，$X_D^\\top X_D$ 是可逆的（实际上，它是一个对角矩阵），可逆性指示符为 $1$。存在唯一的 OLS 解。在这种参数化中，系数直接估计每个组的条件均值。\n男性的系数 $\\hat{\\beta}_1$ 是男性的平均工资：$\\bar{y}_{\\text{男性}} = 22$。\n女性的系数 $\\hat{\\beta}_2$ 是女性的平均工资：$\\bar{y}_{\\text{女性}} = 19$。\n- $r_D = 2$\n- 可逆性指示符：$1$\n- $\\hat{\\beta}_D = [22.0000, 19.0000]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the OLS problem for four different regression model specifications.\n\n    For each case, it computes:\n    1. The column rank 'r' of the design matrix X.\n    2. An invertibility indicator for the Gram matrix X.T @ X (1 if invertible, 0 otherwise).\n    3. The OLS coefficient vector beta. If the solution is not unique, the minimum\n       Euclidean norm solution is computed.\n\n    The results are formatted into a list of lists as required.\n    \"\"\"\n\n    def analyze_case(X, y):\n        \"\"\"\n        Analyzes a single OLS regression case.\n        \n        Args:\n            X (np.ndarray): The design matrix of shape (n, p).\n            y (np.ndarray): The response vector of shape (n,).\n            \n        Returns:\n            list: A list containing [rank, invertibility_indicator, coefficients].\n        \"\"\"\n        # Get the number of parameters p (number of columns)\n        p = X.shape[1]\n\n        # 1. Compute the column rank of X\n        r = np.linalg.matrix_rank(X)\n\n        # 2. Determine if X.T @ X is invertible\n        # This is true if and only if X has full column rank (r == p).\n        invertibility_indicator = 1 if r == p else 0\n\n        # 3. Compute the coefficient vector beta\n        # np.linalg.lstsq correctly handles both full rank and rank-deficient cases.\n        # For rank-deficient cases, it returns the minimum L2-norm solution, which is\n        # the solution obtained via the Moore-Penrose pseudoinverse.\n        # rcond=None is specified to use the default machine precision-based cutoff.\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n\n        # Round coefficients to 4 decimal places\n        beta_rounded = [round(b, 4) for b in beta]\n\n        return [r, invertibility_indicator, beta_rounded]\n\n    # Test Suite Definition\n    test_cases = [\n        # Case A: Intercept, male dummy, female dummy (dummy variable trap)\n        (\n            np.array([[1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        ),\n        # Case B: Intercept and one dummy (correct specification)\n        (\n            np.array([[1, 1], [1, 0], [1, 1], [1, 0], [1, 1], [1, 0]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        ),\n        # Case C: Intercept, male dummy, female dummy (all obs are male)\n        (\n            np.array([[1, 1, 0], [1, 1, 0], [1, 1, 0], [1, 1, 0]]),\n            np.array([21, 22, 20, 23])\n        ),\n        # Case D: No intercept, two dummies (alternative correct specification)\n        (\n            np.array([[1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        )\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = analyze_case(X, y)\n        results.append(result)\n        \n    # The str() representation of a list in Python matches the required format.\n    # Ex: str([2, 0, [13.6667, 8.3333, 5.3333]]) gives '[2, 0, [13.6667, 8.3333, 5.3333]]'\n    # We join these string representations with commas.\n    results_str = ','.join(map(str, results))\n\n    # Print the final output in the exact required format.\n    print(f\"[{results_str}]\")\n\nsolve()\n```", "id": "2413177"}, {"introduction": "建立模型后，检验其基本假设是关键一步，其中之一便是同方差性，即误差项的方差为常数。然而，在许多经济和金融应用中，误差方差会随预测变量而变化，这种现象称为异方差性。虽然异方差性不会使 OLS 估计量产生偏误，但它会使经典的 OLS 标准误失效，导致错误的假设检验和置信区间。通过蒙特卡洛模拟 ([@problem_id:2413193])，您将直接观察到为什么传统标准误在异方差存在时是不可靠的，并理解为什么使用异方差稳健标准误对于进行可靠的统计推断至关重要。", "problem": "考虑一个带有异方差扰动的线性回归模型。对于每个观测值 $i \\in \\{1,\\dots,n\\}$，设数据生成过程为\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i,\n$$\n满足外生性条件 $E[\\varepsilon_i \\mid X] = 0$ 和异方差结构\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma^2 \\cdot \\text{income}_i,\n$$\n其中 $X$ 表示包含截距项的完整回归量矩阵。目标是使用蒙特卡洛模拟来证明，经典的普通最小二乘法（OLS）标准误在异方差情况下会失效，而异方差一致性（HC）标准误能够渐进地保持正确的覆盖率。\n\n您必须编写一个完整、可运行的程序，该程序能够：\n- 根据上述描述生成合成数据。\n- 使用 OLS 估计回归，计算经典的仅同方差标准误和异方差一致性（HC）稳健标准误。\n- 使用标准正态临界值为 $\\beta_1$ 构建名义覆盖率为 $0.95$ 的双侧置信区间。\n- 在 $R$ 次独立重复实验中重复该实验，并报告每种标准误方法的经验覆盖率（即包含真实 $\\beta_1$ 的置信区间的比例）。\n\n使用以下基本原理进行设计和论证：\n- OLS 估计量被定义为残差平方和的最小化器。残差是观测值 $y_i$ 与回归量线性组合所隐含的拟合值之间的偏差。\n- 经典的仅同方差标准误要求假设 $\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2 I$，当方差依赖于回归量时，该标准误是不一致的。\n- 异方差一致性（HC）标准误用一个估计量来替代不正确的同方差残差协方差，当 $\\operatorname{Var}(\\varepsilon \\mid X)$ 是对角矩阵但不与单位矩阵成比例时，该估计量仍然是一致的。\n\n每次重复实验的数据生成细节：\n- 从形状参数 $k = 4$、尺度参数 $\\theta = 12$ 的伽马分布中独立抽取 $\\text{income}_i$（因此 $E[\\text{income}_i] = k \\theta = 48$）。这一选择确保了 $\\text{income}_i$ 严格为正，并具有符合现实的右偏性。\n- 从均值为 $14$、标准差为 $2$ 的正态分布中独立抽取 $\\text{educ}_i$，然后将其截断到区间 $[8, 22]$ 内，以避免不合理的值。\n- 将真实参数向量设为 $(\\beta_0,\\beta_1,\\beta_2) = (1, 0.8, 0.5)$。\n- 对于每个 $i$，从均值为 $0$、方差为 $\\sigma^2 \\cdot \\text{income}_i$ 的正态分布中抽取 $\\varepsilon_i$。\n- 相应地构造 $y_i$。\n\n置信区间：\n- 对于每次重复实验，在使用 OLS 估计模型后，为 $\\beta_1$ 计算两个置信区间：一个使用经典的仅同方差标准误，另一个使用异方差一致性（HC）稳健标准误。在这两种情况下，都使用与名义覆盖率 $0.95$ 对应的标准正态临界值（即双侧临界值 $z_{0.975}$）。\n- 记录每个区间是否包含真实的 $\\beta_1$。\n\n测试套件：\n为以下参数集 $(n, \\sigma^2, R)$ 运行蒙特卡洛模拟：\n- 情况 A（理想情况）：$(n, \\sigma^2, R) = (500, 1.0, 1000)$。\n- 情况 B（小样本）：$(n, \\sigma^2, R) = (60, 1.0, 1000)$。\n- 情况 C（更严重的异方差性）：$(n, \\sigma^2, R) = (500, 3.0, 1000)$。\n- 情况 D（较温和的异方差性）：$(n, \\sigma^2, R) = (500, 0.2, 1000)$。\n\n随机性与可复现性：\n- 为伪随机数生成器使用固定种子 $20240519$，以确保结果可以精确复现。\n\n程序输出要求：\n- 对于每个测试用例，报告三项内容：使用仅同方差标准误的经验覆盖率（一个浮点数），使用 HC 稳健标准误的经验覆盖率（一个浮点数），以及一个布尔值，指示稳健覆盖率是否比同方差覆盖率更接近名义值 $0.95$（即 $|\\text{robust} - 0.95| < |\\text{classical} - 0.95|$ 是否成立）。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例对应一个内部列表，例如：\n\"[[cA_classical,cA_robust,bA],[cB_classical,cB_robust,bB],[cC_classical,cC_robust,bC],[cD_classical,cD_robust,bD]]\"\n- 所有覆盖率必须以小数形式报告（而非百分比）。\n\n角度单位不适用。无需物理单位。\n\n您的程序必须是完整且可直接运行的，无需任何用户输入或外部文件，并且必须严格遵守指定的输出格式。", "solution": "所述问题是有效的。它提出了一个定义明确且标准的计量经济学练习，用于展示异方差性对普通最小二乘法（OLS）估计量所产生推断的影响。数据生成过程已完全指定，理论概念健全，目标清晰且可量化。任务是执行蒙特卡洛模拟，这是计算统计学和计量经济学中用于评估估计量和统计检验的有限样本性质的一项基本技术。\n\n我们首先将指定的线性回归模型形式化。对于每个观测值 $i=1, \\dots, n$，模型为：\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i\n$$\n在矩阵表示法中，这可以写为 $y = X\\beta + \\varepsilon$，其中 $y$ 是因变量观测值的 $n \\times 1$ 向量，$X$ 是回归量的 $n \\times p$ 矩阵（$p=3$），$\\beta$ 是真实参数的 $p \\times 1$ 向量，$\\varepsilon$ 是扰动的 $n \\times 1$ 向量。观测值 $i$ 的回归量矩阵为 $x_i^T = [1, \\text{income}_i, \\text{educ}_i]$。\n\n问题对误差项 $\\varepsilon$ 提出了两个关键假设：\n1. 外生性条件：$E[\\varepsilon \\mid X] = 0$。这确保了 OLS 估计量是无偏的。\n2. 一种特定形式的异方差性：$\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma_i^2 = \\sigma^2 \\cdot \\text{income}_i$。这意味着误差项的方差不是恒定的，而是依赖于其中一个回归量。误差向量的完整协方差矩阵为 $\\Omega = \\operatorname{Var}(\\varepsilon \\mid X) = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$，它不与单位矩阵 $I_n$ 成比例。\n\n$\\beta$ 的 OLS 估计量是通过最小化残差平方和 $S(\\beta) = (y - X\\beta)^T(y - X\\beta)$ 得出的。其众所周知的解是：\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y\n$$\n在外生性假设下，该估计量是无偏的，即 $E[\\hat{\\beta}_{OLS} \\mid X] = \\beta$。然而，为了进行有效的统计推断（构建置信区间和进行假设检验），我们需要一个关于 $\\hat{\\beta}_{OLS}$ 协方差矩阵的一致估计量。在以 $X$ 为条件的 OLS 估计量的真实协方差矩阵是：\n$$\n\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid X) = (X^T X)^{-1} X^T \\Omega X (X^T X)^{-1}\n$$\n\n该模拟比较了此协方差矩阵的两种估计量。\n\n首先是经典的、仅限同方差的方差估计量。该估计量错误地假设 $\\Omega = \\sigma^2 I_n$。它由下式给出：\n$$\n\\widehat{\\operatorname{Var}}_{classical}(\\hat{\\beta}) = \\hat{s}^2 (X^T X)^{-1}\n$$\n其中 $\\hat{s}^2 = \\frac{1}{n-p} \\sum_{i=1}^n \\hat{\\varepsilon}_i^2$ 是误差方差的标准无偏估计量，而 $\\hat{\\varepsilon}_i = y_i - x_i^T \\hat{\\beta}$ 是 OLS 残差。当存在异方差性时，这个公式是不正确的，由此产生的标准误是有偏且不一致的。使用这些标准误构建的置信区间将不具有正确的名义覆盖概率。\n\n其次是异方差一致性（HC）方差估计量，特别是 White (1980) 提出的 HC0 估计量。该估计量不需要同方差性假设，并为真实的协方差矩阵提供了一个一致的估计。它用 OLS 残差平方的对角矩阵 $\\hat{\\Omega} = \\operatorname{diag}(\\hat{\\varepsilon}_1^2, \\dots, \\hat{\\varepsilon}_n^2)$ 来替代真实公式中未知的 $\\Omega$。HC0 估计量为：\n$$\n\\widehat{\\operatorname{Var}}_{HC0}(\\hat{\\beta}) = (X^T X)^{-1} (X^T \\hat{\\Omega} X) (X^T X)^{-1}\n$$\n术语 $X^T \\hat{\\Omega} X$ 通常被称为“三明治的肉”。从此估计量派生的标准误对未知形式的异方差性是“稳健的”。渐进地，基于 HC 标准误的置信区间将达到正确的名义覆盖率。\n\n对于每个测试用例 $(n, \\sigma^2, R)$，模拟将按以下步骤进行：\n1. 该过程重复 $R$ 次。\n2. 在每次重复实验中，根据指定的数据生成过程，使用真实参数 $(\\beta_0, \\beta_1, \\beta_2) = (1, 0.8, 0.5)$ 生成一个大小为 $n$ 的合成数据集 $(y, X)$。\n3. 使用 OLS 估计模型以获得 $\\hat{\\beta}$ 和残差 $\\hat{\\varepsilon}$。\n4. 使用经典公式和 HC0 稳健公式计算 $\\hat{\\beta}_1$ 的标准误。这涉及取相应估计协方差矩阵的第二个对角元素（索引为 1）的平方根。\n5. 构建两个关于 $\\beta_1$ 的 $95\\%$ 置信区间：$[\\hat{\\beta}_1 \\pm z_{0.975} \\cdot SE(\\hat{\\beta}_1)]$，其中 $z_{0.975}$ 是来自标准正态分布的临界值（$z_{0.975} \\approx 1.95996$），$SE(\\hat{\\beta}_1)$ 是相应的标准误估计值。\n6. 对每个区间，我们检查它是否包含真实值 $\\beta_1 = 0.8$。\n7. 所有重复实验结束后，计算每种方法的经验覆盖率，即包含真实参数的区间的比例。将这些覆盖率与名义水平 $0.95$ 进行比较。\n预期结果是，稳健置信区间的经验覆盖率将接近 $0.95$，而经典区间的覆盖率将显著偏离，从而证明了它们在异方差性下的失效。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_simulation(n, sigma2, R, true_beta, rng):\n    \"\"\"\n    Runs a single Monte Carlo simulation for a given parameter set.\n    \"\"\"\n    beta1_true = true_beta[1]\n    num_params = len(true_beta)\n    \n    # Pre-calculate the critical value for 95% CI\n    # z_crit = 1.959964 is norm.ppf(0.975)\n    z_crit = norm.ppf(1 - 0.05 / 2.0)\n\n    # Counters for coverage\n    classical_hits = 0\n    robust_hits = 0\n\n    for _ in range(R):\n        # 1. Generate data\n        income = rng.gamma(shape=4, scale=12, size=n)\n        educ = rng.normal(loc=14, scale=2, size=n)\n        educ = np.clip(educ, 8, 22)\n        \n        X = np.column_stack((np.ones(n), income, educ))\n        \n        # Generate heteroskedastic errors\n        error_variances = sigma2 * income\n        epsilon = rng.normal(loc=0, scale=np.sqrt(error_variances))\n        \n        y = X @ true_beta + epsilon\n\n        # 2. OLS Estimation\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            beta_hat = XTX_inv @ X.T @ y\n        except np.linalg.LinAlgError:\n            continue # Skip iteration if X is singular\n\n        beta1_hat = beta_hat[1]\n        residuals = y - X @ beta_hat\n\n        # 3. Calculate Classical (Homoskedastic) Standard Errors\n        s2_classical = np.sum(residuals**2) / (n - num_params)\n        var_beta_classical = s2_classical * XTX_inv\n        se_beta1_classical = np.sqrt(var_beta_classical[1, 1])\n\n        # 4. Calculate HC0 (Robust) Standard Errors\n        # Efficiently compute the sandwich meat: X^T * diag(res^2) * X\n        sandwich_meat = X.T @ (X * (residuals**2)[:, np.newaxis])\n        var_beta_hc0 = XTX_inv @ sandwich_meat @ XTX_inv\n        \n        # Ensure variance is non-negative due to potential floating point errors\n        var_beta1_hc0 = var_beta_hc0[1, 1]\n        if var_beta1_hc0  0:\n            var_beta1_hc0 = 0\n        se_beta1_hc0 = np.sqrt(var_beta1_hc0)\n\n        # 5. Construct Confidence Intervals and check coverage\n        # Classical CI\n        ci_classical_lower = beta1_hat - z_crit * se_beta1_classical\n        ci_classical_upper = beta1_hat + z_crit * se_beta1_classical\n        if ci_classical_lower = beta1_true = ci_classical_upper:\n            classical_hits += 1\n\n        # Robust CI\n        ci_robust_lower = beta1_hat - z_crit * se_beta1_hc0\n        ci_robust_upper = beta1_hat + z_crit * se_beta1_hc0\n        if ci_robust_lower = beta1_true = ci_robust_upper:\n            robust_hits += 1\n\n    # 6. Calculate empirical coverages\n    coverage_classical = classical_hits / R\n    coverage_robust = robust_hits / R\n\n    # 7. Compare performance\n    is_robust_better = abs(coverage_robust - 0.95)  abs(coverage_classical - 0.95)\n\n    return [coverage_classical, coverage_robust, is_robust_better]\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (n, sigma^2, R) = (500, 1.0, 1000)\n        (500, 1.0, 1000),\n        # Case B: (n, sigma^2, R) = (60, 1.0, 1000)\n        (60, 1.0, 1000),\n        # Case C: (n, sigma^2, R) = (500, 3.0, 1000)\n        (500, 3.0, 1000),\n        # Case D: (n, sigma^2, R) = (500, 0.2, 1000)\n        (500, 0.2, 1000),\n    ]\n\n    true_beta = np.array([1.0, 0.8, 0.5])\n    seed = 20240519\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for n, sigma2, R in test_cases:\n        result = run_simulation(n, sigma2, R, true_beta, rng)\n        results.append(result)\n\n    # Format the output string exactly as required\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        cov_c, cov_r, is_better = res\n        # Format boolean as lowercase 'true'/'false'\n        bool_str = 'true' if is_better else 'false'\n        output_str += f\"[{cov_c},{cov_r},{bool_str}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```", "id": "2413193"}, {"introduction": "这个综合性练习将模型构建、估计和诊断的各个环节整合到一个现实的商业场景中。您将从基本原理出发，为一个移动应用的需求建立一个多元线性回归模型，这需要您手动求解正规方程、进行预测、评估模型拟合度，并执行假设检验。这项练习 ([@problem_id:2413185]) 不仅能巩固您对 OLS 内部机制的理解，还能让您应用如条件数等诊断工具来评估潜在的多重共线性问题，从而将理论知识全面应用于实践。", "problem": "要求您在计算金融环境中实现并应用多元线性回归。任务是建立一个关于移动应用需求的普通最小二乘法（OLS）模型，其中目标变量是下载量。预测变量包括以美元（USD）计价的价格、1到5分的用户评分，以及在社交媒体上的广告支出（以美元计）。模型必须包含截距项。所有预测的下载量必须以计数（无单位）表示，所有货币输入均以美元计。不涉及任何角度计算。所有输出必须是指定的布尔、整数或浮点数类型的数值。\n\n您必须从基本定义出发，将 OLS 视作最小化训练数据上残差平方和的解。除了数值线性代数基本操作外，您不得依赖任何预先封装的黑箱模型。您必须使用所提供的训练数据来估计模型，然后解答接下来的测试组。\n\n训练数据：每行给出 $(\\text{价格}, \\text{用户评分}, \\text{社交媒体广告支出}) \\to \\text{下载量}$，其中下载量是计数。\n\n- 第 $1$ 行：$(0.99, 4.5, 12000) \\to 31320$\n- 第 $2$ 行：$(1.99, 4.2, 8000) \\to 24620$\n- 第 $3$ 行：$(0.00, 3.8, 5000) \\to 25700$\n- 第 $4$ 行：$(2.99, 4.7, 15000) \\to 29620$\n- 第 $5$ 行：$(4.99, 4.9, 20000) \\to 30970$\n- 第 $6$ 行：$(1.49, 3.5, 0) \\to 17370$\n- 第 $7$ 行：$(0.00, 4.0, 7000) \\to 27950$\n- 第 $8$ 行：$(3.99, 4.4, 11000) \\to 23820$\n- 第 $9$ 行：$(2.49, 3.9, 6000) \\to 21570$\n- 第 $10$ 行：$(0.99, 4.8, 18000) \\to 36720$\n- 第 $11$ 行：$(1.99, 4.1, 4000) \\to 21440$\n- 第 $12$ 行：$(0.49, 3.6, 2000) \\to 21540$\n\n用于样本外评估的验证数据：每行给出 $(\\text{价格}, \\text{用户评分}, \\text{社交媒体广告支出}) \\to \\text{下载量}$。\n\n- V$1$：$(2.99, 4.0, 9000) \\to 23320$\n- V$2$：$(0.00, 4.9, 16000) \\to 37300$\n- V$3$：$(1.49, 3.7, 4000) \\to 21370$\n\n您必须基于多元线性回归的基本原理，实现以下内容：\n\n- 通过最小化训练集上的残差平方和，拟合一个以截距、价格、用户评分和广告支出为函数的下载量线性模型。\n- 仅使用训练集计算估计的系数向量和残差方差。\n- 使用拟合好的模型计算下面测试组所需的量。\n\n测试组及要求输出：\n\n令 $n$ 表示训练观测值的数量，$k$ 表示模型参数的数量（包括截距）。此处 $n = 12$ 且 $k = 4$。\n\n您的程序必须按顺序计算以下五个结果：\n\n$1.$ 预测任务（正常路径）：使用拟合的模型，预测一个新应用在 $(\\text{价格} = 1.99,\\ \\text{用户评分} = 4.5,\\ \\text{社交媒体广告支出} = 10000)$ 情况下的下载量。输出四舍五入到最接近的整数（计数）的预测值。\n\n$2.$ 系数提取：输出价格的估计系数，以浮点数形式表示并四舍五入到 $2$ 位小数。\n\n$3.$ 样本外拟合优度：计算验证集上的决定系数，定义为 $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$，其中 $\\bar{y}_{\\text{test}}$ 是验证目标的均值。以浮点数形式输出该值，并四舍五入到 $4$ 位小数。\n\n$4.$ 价格的统计显著性（边缘情况：残差方差小）：使用双边 Student’s $t$-检验，在显著性水平 $\\alpha = 0.05$ 和自由度 $n - k$ 的条件下，检验价格系数等于 $0$ 的原假设，其备择假设为价格系数不等于 $0$。输出一个布尔值：如果显著则为 $True$，否则为 $False$。\n\n$5.$ 数值稳定性诊断（多重共线性检查）：使用 $2$-范数计算带截距项的训练设计矩阵的谱条件数，即其最大奇异值与最小奇异值之比。以浮点数形式输出该值，并四舍五入到 $2$ 位小数。\n\n最终输出格式：\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[result1,result2,result3,result4,result5]”）。结果必须按指定的确切顺序出现：预测值（整数）、价格系数（浮点数，保留 $2$ 位小数）、测试集 $R^{2}$（浮点数，保留 $4$ 位小数）、价格显著性的布尔值、条件数（浮点数，保留 $2$ 位小数）。", "solution": "此问题已通过验证。\n\n### 第 1 步：提取已知信息\n- **任务**：使用普通最小二乘法（OLS）实现并应用多元线性回归，以模拟移动应用需求。\n- **目标变量 ($y$)**：下载量（计数）。\n- **预测变量 ($x_j$)**：价格（美元）、用户评分（1-5分制）、广告支出（美元）。\n- **模型设定**：一个包含截距项的线性模型。\n- **方法论**：最小化残差平方和。不得使用“黑箱”模型，仅可使用线性代数基本操作。\n- **训练数据**（$n=12$ 个观测值）：\n  - $(\\text{价格}, \\text{用户评分}, \\text{广告支出}) \\to \\text{下载量}$\n  - $(0.99, 4.5, 12000) \\to 31320$\n  - $(1.99, 4.2, 8000) \\to 24620$\n  - $(0.00, 3.8, 5000) \\to 25700$\n  - $(2.99, 4.7, 15000) \\to 29620$\n  - $(4.99, 4.9, 20000) \\to 30970$\n  - $(1.49, 3.5, 0) \\to 17370$\n  - $(0.00, 4.0, 7000) \\to 27950$\n  - $(3.99, 4.4, 11000) \\to 23820$\n  - $(2.49, 3.9, 6000) \\to 21570$\n  - $(0.99, 4.8, 18000) \\to 36720$\n  - $(1.99, 4.1, 4000) \\to 21440$\n  - $(0.49, 3.6, 2000) \\to 21540$\n- **验证数据**（$3$ 个观测值）：\n  - $(2.99, 4.0, 9000) \\to 23320$\n  - $(0.00, 4.9, 16000) \\to 37300$\n  - $(1.49, 3.7, 4000) \\to 21370$\n- **常数**：训练观测值数量 $n=12$，模型参数数量 $k=4$。\n- **测试组**：\n  1.  预测 $(\\text{价格}=1.99, \\text{用户评分}=4.5, \\text{广告支出}=10000)$ 时的下载量。输出：整数。\n  2.  提取价格的估计系数。输出：浮点数，保留 $2$ 位小数。\n  3.  在验证集上计算 $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$。输出：浮点数，保留 $4$ 位小数。\n  4.  使用双边 t-检验在 $\\alpha = 0.05$ 水平下、自由度为 $n-k$ 时，检验价格系数的显著性。输出：布尔值。\n  5.  计算训练设计矩阵的谱条件数。输出：浮点数，保留 $2$ 位小数。\n\n### 第 2 步：使用提取的已知信息进行验证\n该问题是多元线性回归的标准、明确定义的应用，这是统计学和计量经济学中的一种基本方法。\n- **科学依据**：该问题基于普通最小二乘法（OLS），这是一个核心且成熟的统计理论。其背景对于计算经济学和金融学是现实的。\n- **适定性**：问题提供了足够的数据（12个观测值对应4个参数）来估计模型系数。测试组中的任务是具体的、无歧义的，并且基于标准统计公式有唯一解。\n- **客观性**：问题陈述语言精确、客观。数据是数值型的，所需的计算基于既定的数学公式，而非主观解释。\n- 该问题不违反任何无效性标准。它不是基于错误的前提，是可形式化的，数据是完整的，任务是可验证的。\n\n### 第 3 步：结论与行动\n该问题被判定为**有效**。将提供一个解法。\n\n### 解法\n该问题要求使用普通最小二乘法（OLS）拟合一个多元线性回归模型。模型形式如下：\n$$ \\text{downloads} = \\beta_0 + \\beta_1 \\cdot \\text{price} + \\beta_2 \\cdot \\text{user\\_rating} + \\beta_3 \\cdot \\text{ad\\_spend} + \\epsilon $$\n用矩阵表示法，这表示为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $\\mathbf{y}$ 是观测下载量的向量，$\\mathbf{X}$ 是设计矩阵，$\\boldsymbol{\\beta}$ 是系数向量，$\\boldsymbol{\\epsilon}$ 是误差向量。\n\nOLS 方法旨在找到最小化残差平方和（SSR）$S = \\sum_{i=1}^{n} \\epsilon_i^2$ 的系数向量 $\\hat{\\boldsymbol{\\beta}}$。\n$$ S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) $$\n为了最小化该量，我们对其关于 $\\boldsymbol{\\beta}$求梯度并令其为零：\n$$ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0} $$\n这得到了正规方程组：\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\nOLS 估计量 $\\hat{\\boldsymbol{\\beta}}$ 的解由下式给出：\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\n此方程将使用数值稳定的线性代数程序来求解。\n\n首先，我们从 $n=12$ 个训练观测值中构建设计矩阵 $\\mathbf{X}_{\\text{train}}$ 和目标向量 $\\mathbf{y}_{\\text{train}}$。矩阵 $\\mathbf{X}_{\\text{train}}$ 有 $k=4$ 列：一列用于截距项（$\\beta_0$），另外三列分别对应三个预测变量。\n$$\n\\mathbf{y}_{\\text{train}} = \\begin{pmatrix} 31320 \\\\ 24620 \\\\ 25700 \\\\ 29620 \\\\ 30970 \\\\ 17370 \\\\ 27950 \\\\ 23820 \\\\ 21570 \\\\ 36720 \\\\ 21440 \\\\ 21540 \\end{pmatrix},\n\\quad\n\\mathbf{X}_{\\text{train}} = \\begin{pmatrix}\n1  0.99  4.5  12000 \\\\\n1  1.99  4.2  8000 \\\\\n1  0.00  3.8  5000 \\\\\n1  2.99  4.7  15000 \\\\\n1  4.99  4.9  20000 \\\\\n1  1.49  3.5  0 \\\\\n1  0.00  4.0  7000 \\\\\n1  3.99  4.4  11000 \\\\\n1  2.49  3.9  6000 \\\\\n1  0.99  4.8  18000 \\\\\n1  1.99  4.1  4000 \\\\\n1  0.49  3.6  2000 \\\\\n\\end{pmatrix}\n$$\n求解 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3]^T$ 的正规方程组，得到估计的系数向量。接下来我们解答测试组中的问题。\n\n**1. 预测任务**\n一个新的观测值由预测向量 $(\\text{价格}=1.99, \\text{用户评分}=4.5, \\text{广告支出}=10000)$ 给出。我们构建一个带截距的设计向量 $\\mathbf{x}_{\\text{new}}$：\n$$ \\mathbf{x}_{\\text{new}} = \\begin{pmatrix} 1  1.99  4.5  10000 \\end{pmatrix}^T $$\n预测的下载量 $\\hat{y}_{\\text{new}}$ 计算如下：\n$$ \\hat{y}_{\\text{new}} = \\mathbf{x}_{\\text{new}}^T \\hat{\\boldsymbol{\\beta}} $$\n结果四舍五入到最接近的整数。\n\n**2. 系数提取**\n价格的估计系数 $\\hat{\\beta}_1$ 是向量 $\\hat{\\boldsymbol{\\beta}}$ 的第二个元素。提取该值并四舍五入到 $2$ 位小数。\n\n**3. 样本外拟合优度**\n验证集的决定系数 $R^{2}_{\\text{test}}$ 计算如下：\n$$ R^{2}_{\\text{test}} = 1 - \\frac{SSR_{\\text{test}}}{SST_{\\text{test}}} $$\n其中 $SSR_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2$ 是验证集上的残差平方和，$SST_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2$ 是验证集的总平方和。$\\bar{y}_{\\text{test}}$ 是验证数据中观测下载量的均值。预测值 $\\hat{y}_i$ 使用拟合模型生成：$\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$。结果四舍五入到 $4$ 位小数。\n\n**4. 价格的统计显著性**\n我们使用双边 Student's t-检验来检验原假设 $H_0: \\beta_1 = 0$ 与备择假设 $H_1: \\beta_1 \\neq 0$。t-统计量为：\n$$ t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} $$\n系数的标准误 $SE(\\hat{\\beta}_1)$ 是系数协方差矩阵 $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1}$ 对应对角线元素的平方根。残差方差 $\\hat{\\sigma}^2$ 是误差方差 $\\sigma^2$ 的无偏估计量，由训练数据计算得出：\n$$ \\hat{\\sigma}^2 = \\frac{SSR_{\\text{train}}}{n-k} = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-k} $$\nt-分布的自由度为 $df = n-k = 12-4=8$。我们将计算出的 t-统计量的绝对值 $|t|$ 与显著性水平 $\\alpha = 0.05$、自由度 $df=8$ 的 t-分布临界值 $t_{\\text{crit}}$ 进行比较。如果 $|t|  t_{\\text{crit}}$，我们拒绝 $H_0$，系数在统计上是显著的。\n\n**5. 数值稳定性诊断**\n训练设计矩阵 $\\mathbf{X}_{\\text{train}}$ 的谱条件数是其最大奇异值（$\\sigma_{\\max}$）与最小奇异值（$\\sigma_{\\min}$）之比：\n$$ \\kappa_2(\\mathbf{X}_{\\text{train}}) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} $$\n该值是衡量预测变量中多重共线性的一个指标。高值表示在估计系数时可能存在数值不稳定性。结果四舍五入到 $2$ 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Implements and applies a multiple linear regression model from first principles.\n    \"\"\"\n    # Define the problem data as specified.\n    # Training Data (n=12)\n    y_train_list = [31320, 24620, 25700, 29620, 30970, 17370, 27950, 23820, 21570, 36720, 21440, 21540]\n    X_train_predictors_list = [\n        [0.99, 4.5, 12000], [1.99, 4.2, 8000], [0.00, 3.8, 5000], [2.99, 4.7, 15000],\n        [4.99, 4.9, 20000], [1.49, 3.5, 0], [0.00, 4.0, 7000], [3.99, 4.4, 11000],\n        [2.49, 3.9, 6000], [0.99, 4.8, 18000], [1.99, 4.1, 4000], [0.49, 3.6, 2000]\n    ]\n\n    # Validation Data\n    y_val_list = [23320, 37300, 21370]\n    X_val_predictors_list = [\n        [2.99, 4.0, 9000], [0.00, 4.9, 16000], [1.49, 3.7, 4000]\n    ]\n\n    # New app data for prediction\n    x_new_predictors_list = [1.99, 4.5, 10000]\n    \n    # Convert lists to numpy arrays\n    y_train = np.array(y_train_list, dtype=float)\n    X_train_predictors = np.array(X_train_predictors_list, dtype=float)\n    y_val = np.array(y_val_list, dtype=float)\n    X_val_predictors = np.array(X_val_predictors_list, dtype=float)\n    x_new_predictors = np.array(x_new_predictors_list, dtype=float)\n\n    # Add intercept column to design matrices\n    n_train = X_train_predictors.shape[0]\n    n_val = X_val_predictors.shape[0]\n    X_train = np.hstack([np.ones((n_train, 1)), X_train_predictors])\n    X_val = np.hstack([np.ones((n_val, 1)), X_val_predictors])\n    x_new = np.hstack([1, x_new_predictors])\n    \n    # --- Model Fitting: Solve Normal Equations ---\n    # (X.T @ X) @ beta = X.T @ y\n    XTX = X_train.T @ X_train\n    XTy = X_train.T @ y_train\n    beta_hat = np.linalg.solve(XTX, XTy)\n\n    # --- Test Suite Computations ---\n    results = []\n\n    # 1. Prediction task\n    y_pred_new = x_new @ beta_hat\n    result1 = int(round(y_pred_new))\n    results.append(result1)\n\n    # 2. Coefficient extraction (price)\n    price_coeff = beta_hat[1]\n    result2 = round(price_coeff, 2)\n    results.append(result2)\n\n    # 3. Out-of-sample R-squared\n    y_pred_val = X_val @ beta_hat\n    residuals_val = y_val - y_pred_val\n    ssr_val = np.sum(residuals_val**2)\n    y_mean_val = np.mean(y_val)\n    sst_val = np.sum((y_val - y_mean_val)**2)\n    r2_val = 1 - (ssr_val / sst_val)\n    result3 = round(r2_val, 4)\n    results.append(result3)\n    \n    # 4. Statistical significance of price\n    n = n_train\n    k = X_train.shape[1] # number of parameters\n    df = n - k\n    y_pred_train = X_train @ beta_hat\n    residuals_train = y_train - y_pred_train\n    ssr_train = np.sum(residuals_train**2)\n    sigma_sq_hat = ssr_train / df\n    XTX_inv = np.linalg.inv(XTX)\n    var_beta_hat = sigma_sq_hat * XTX_inv\n    se_price_coeff = np.sqrt(var_beta_hat[1, 1])\n    t_statistic = price_coeff / se_price_coeff\n    alpha = 0.05\n    t_critical = t.ppf(1 - alpha/2, df)\n    result4 = bool(abs(t_statistic) > t_critical)\n    results.append(result4)\n    \n    # 5. Numerical stability diagnostic (condition number)\n    cond_num = np.linalg.cond(X_train, 2)\n    result5 = round(cond_num, 2)\n    results.append(result5)\n\n    # Final print statement in the exact required format.\n    # Example format: [28132,-1803.97,0.9744,True,1367.65]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2413185"}]}