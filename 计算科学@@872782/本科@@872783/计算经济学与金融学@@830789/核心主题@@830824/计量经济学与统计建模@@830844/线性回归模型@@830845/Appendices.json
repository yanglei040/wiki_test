{"hands_on_practices": [{"introduction": "线性回归模型的核心在于其系数的估计。理解普通最小二乘法 (OLS) 估计量 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$ 的计算过程，是掌握该模型的基石。这个练习 [@problem_id:1938980] 提供了一个简洁的数据集，让你能够亲手完成矩阵运算的每一步，从而揭示估算过程背后的数学机制。", "problem": "一位食品科学家正在开发一种新型烘焙零食，并希望了解烘焙条件如何影响其酥脆度。酥脆度在一个定量标度上进行测量。该科学家用四个批次进行了一项小型实验，改变了两个因素：烘焙温度和湿度。这些因素由编码变量表示，其中$-1$代表低水平设置，$+1$代表高水平设置。\n\n提出的统计模型是以下形式的多元线性回归模型：\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n其中：\n- $y$ 是酥脆度得分。\n- $x_1$ 是烘焙温度的编码变量。\n- $x_2$ 是湿度的编码变量。\n- $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 是未知的模型系数。\n- $\\epsilon$ 是随机误差项。\n\n四个实验批次的结果如下：\n- 批次1：温度编码 ($x_1$) = -1，湿度编码 ($x_2$) = -1，酥脆度 ($y$) = 2。\n- 批次2：温度编码 ($x_1$) = -1，湿度编码 ($x_2$) = 1，酥脆度 ($y$) = 4。\n- 批次3：温度编码 ($x_1$) = 1，湿度编码 ($x_2$) = -1，酥脆度 ($y$) = 6。\n- 批次4：温度编码 ($x_1$) = 1，湿度编码 ($x_2$) = 1，酥脆度 ($y$) = 8。\n\n使用最小二乘法，确定系数的估计值。将您的答案以单行矩阵的形式呈现，其中包含按 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ 顺序排列的三个估计系数。", "solution": "目标是求出多元线性回归模型中系数 $\\beta_0$、$\\beta_1$ 和 $\\beta_2$ 的最小二乘估计值。该模型可以写成矩阵形式 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$，其中 $\\mathbf{y}$ 是响应向量，$\\mathbf{X}$ 是设计矩阵，$\\boldsymbol{\\beta}$ 是系数向量，$\\boldsymbol{\\epsilon}$ 是误差向量。\n\n首先，我们根据给定的实验数据构建响应向量 $\\mathbf{y}$ 和设计矩阵 $\\mathbf{X}$。设计矩阵包含一列全为1的列，用于表示截距项 $\\beta_0$。\n\n响应向量 $\\mathbf{y}$ 包含酥脆度得分：\n$$\n\\mathbf{y} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix}\n$$\n\n设计矩阵 $\\mathbf{X}$ 由一个用于截距的全1前导列，以及其后跟随的编码变量 $x_1$ 和 $x_2$ 的列构成：\n$$\n\\mathbf{X} = \\begin{pmatrix} 1  x_{11}  x_{21} \\\\ 1  x_{12}  x_{22} \\\\ 1  x_{13}  x_{23} \\\\ 1  x_{14}  x_{24} \\end{pmatrix} = \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix}\n$$\n\n待估计的系数向量是 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\beta_2)^T$。最小二乘估计值，记为 $\\hat{\\boldsymbol{\\beta}}$，由正规方程组 $\\mathbf{X}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}$ 的解给出。假设 $\\mathbf{X}^T\\mathbf{X}$ 是可逆的，则解为：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n\n我们将分几步进行计算。首先，我们求 $\\mathbf{X}$ 的转置：\n$$\n\\mathbf{X}^T = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix}\n$$\n\n接下来，我们计算乘积 $\\mathbf{X}^T\\mathbf{X}$：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 1  -1  -1 \\\\ 1  -1  1 \\\\ 1  1  -1 \\\\ 1  1  1 \\end{pmatrix}\n$$\n结果矩阵的元素为：\n- $(1,1): 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(1,2): 1 \\cdot (-1) + 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(1,3): 1 \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(2,1): (-1) \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 0$\n- $(2,2): (-1) \\cdot (-1) + (-1) \\cdot (-1) + 1 \\cdot 1 + 1 \\cdot 1 = 4$\n- $(2,3): (-1) \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 1 = 0$\n- $(3,1): (-1) \\cdot 1 + 1 \\cdot 1 + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,2): (-1) \\cdot (-1) + 1 \\cdot (-1) + (-1) \\cdot 1 + 1 \\cdot 1 = 0$\n- $(3,3): (-1) \\cdot (-1) + 1 \\cdot 1 + (-1) \\cdot (-1) + 1 \\cdot 1 = 4$\n\n因此，该矩阵是对角矩阵：\n$$\n\\mathbf{X}^T\\mathbf{X} = \\begin{pmatrix} 4  0  0 \\\\ 0  4  0 \\\\ 0  0  4 \\end{pmatrix} = 4\\mathbf{I}_3\n$$\n其中 $\\mathbf{I}_3$ 是 $3 \\times 3$ 的单位矩阵。\n\n这个对角矩阵的逆矩阵很容易计算：\n$$\n(\\mathbf{X}^T\\mathbf{X})^{-1} = \\frac{1}{4}\\mathbf{I}_3 = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix}\n$$\n\n现在，我们计算乘积 $\\mathbf{X}^T\\mathbf{y}$：\n$$\n\\mathbf{X}^T\\mathbf{y} = \\begin{pmatrix} 1  1  1  1 \\\\ -1  -1  1  1 \\\\ -1  1  -1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 4 \\\\ 6 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 1(2)+1(4)+1(6)+1(8) \\\\ -1(2)-1(4)+1(6)+1(8) \\\\ -1(2)+1(4)-1(6)+1(8) \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix}\n$$\n\n最后，我们将 $(\\mathbf{X}^T\\mathbf{X})^{-1}$ 乘以 $\\mathbf{X}^T\\mathbf{y}$ 来求得 $\\hat{\\boldsymbol{\\beta}}$：\n$$\n\\hat{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  0  0 \\\\ 0  \\frac{1}{4}  0 \\\\ 0  0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 20 \\\\ 8 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}(20) \\\\ \\frac{1}{4}(8) \\\\ \\frac{1}{4}(4) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 2 \\\\ 1 \\end{pmatrix}\n$$\n\n最小二乘估计值为 $\\hat{\\beta}_0 = 5$，$\\hat{\\beta}_1 = 2$ 和 $\\hat{\\beta}_2 = 1$。问题要求以行矩阵 $(\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)$ 的形式给出答案。", "answer": "$$\n\\boxed{\\begin{pmatrix} 5  2  1 \\end{pmatrix}}\n$$", "id": "1938980"}, {"introduction": "除了代数公式，OLS 估计还有一个优美的几何解释：它确保了残差向量与所有解释变量构成的空间（即设计矩阵 $X$ 的列空间）正交。这个练习 [@problem_id:2407237] 要求你在多种情境下，通过计算来验证这一基本性质。这不仅能加深你对 OLS 工作原理的理解，也能让你体会到理论在计算实践中的具体表现。", "problem": "给定有限维欧几里得空间中的普通最小二乘法 (OLS) 回归设定。设 $X \\in \\mathbb{R}^{n \\times k}$ 表示一个设计矩阵，设 $y \\in \\mathbb{R}^{n}$ 表示一个响应向量。将普通最小二乘法 (OLS) 估计量 $\\hat{\\beta}$ 定义为以下目标的任意最小化子\n$$\n\\min_{b \\in \\mathbb{R}^{k}} \\ \\|y - X b\\|_{2}^{2}.\n$$\n将 OLS 残差向量定义为 $e = y - X \\hat{\\beta}$。任务是使用有限精度算术通过计算验证，在标准欧几里得内积下，$e$ 与 $X$ 的列空间正交。\n\n对于下面的每个测试用例，给定 $(X, y)$，计算所述问题的最小化子 $\\hat{\\beta}$，形成相应的残差 $e = y - X \\hat{\\beta}$，并在绝对容差 $\\tau = 10^{-9}$ 内确定 $e$ 是否与 $X$ 的列空间正交。正交性意味着 $e$ 与 $X$ 的每一列的内积均为零，即 $X^{\\top} e = 0$。在有限精度下，如果 $X^{\\top} e$ 的最大绝对值项小于或等于 $\\tau$，则声明该测试用例为真，否则为假。\n\n使用以下包含五个用例的测试套件。每个矩阵和向量都已明确给出。\n\n- 测试用例 1 (带截距的满列秩, $n = 5$, $k = 3$):\n$$\nX_{1} = \\begin{bmatrix}\n1  0.2  -1.0 \\\\\n1  -1.3  0.7 \\\\\n1  0.5  1.2 \\\\\n1  2.0  -0.3 \\\\\n1  -0.7  0.4\n\\end{bmatrix}, \\quad\ny_{1} = \\begin{bmatrix}\n0.5 \\\\ -1.2 \\\\ 0.9 \\\\ 2.3 \\\\ -0.4\n\\end{bmatrix}.\n$$\n\n- 测试用例 2 (列秩亏, $n = 4$, $k = 3$):\n$$\nX_{2} = \\begin{bmatrix}\n1  1  2 \\\\\n1  2  4 \\\\\n1  3  6 \\\\\n1  4  8\n\\end{bmatrix}, \\quad\ny_{2} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ 1 \\\\ 0\n\\end{bmatrix}.\n$$\n\n- 测试用例 3 (完美拟合, $n = 3$, $k = 2$):\n$$\nX_{3} = \\begin{bmatrix}\n1  1 \\\\\n1  2 \\\\\n1  3\n\\end{bmatrix}, \\quad\ny_{3} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ -1\n\\end{bmatrix}.\n$$\n\n- 测试用例 4 (欠定系统, $n = 3$, $k = 5$):\n$$\nX_{4} = \\begin{bmatrix}\n1  0  1  0  2 \\\\\n0  1  1  0  0 \\\\\n1  1  0  1  1\n\\end{bmatrix}, \\quad\ny_{4} = \\begin{bmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{bmatrix}.\n$$\n\n- 测试用例 5 (病态列，近似共线, $n = 5$, $k = 3$):\n$$\nX_{5} = \\begin{bmatrix}\n1  1  2.000001 \\\\\n1  2  3.999999 \\\\\n1  3  6.000001 \\\\\n1  4  7.999999 \\\\\n1  5  10.000001\n\\end{bmatrix}, \\quad\ny_{5} = \\begin{bmatrix}\n2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2\n\\end{bmatrix}.\n$$\n\n设绝对容差为 $\\tau = 10^{-9}$。\n\n要求的最终输出格式：您的程序应生成一行输出，其中包含五个测试用例的结果，按顺序排列，形式为方括号括起来的逗号分隔的布尔值列表，例如，`[True, False, ...]`。", "solution": "该问题要求对普通最小二乘法 (OLS) 回归估计量的一个基本性质进行计算验证。设设计矩阵为 $X \\in \\mathbb{R}^{n \\times k}$，响应向量为 $y \\in \\mathbb{R}^{n}$。OLS 估计量，记作 $\\hat{\\beta}$，被定义为残差平方和的任意最小化子。要最小化的目标函数是残差向量的欧几里得范数的平方：\n$$\nS(b) = \\|y - Xb\\|_{2}^{2}\n$$\n其中 $b \\in \\mathbb{R}^{k}$ 是系数向量。\n\n为了找到最小值，我们必须确定 $S(b)$ 关于 $b$ 的梯度为零向量的点。目标函数可以展开为 $b$ 的二次型：\n$$\nS(b) = (y - Xb)^{\\top}(y - Xb) = y^{\\top}y - y^{\\top}Xb - b^{\\top}X^{\\top}y + b^{\\top}X^{\\top}Xb\n$$\n由于 $b^{\\top}X^{\\top}y$ 是一个标量，它等于其自身的转置，即 $(b^{\\top}X^{\\top}y)^{\\top} = y^{\\top}Xb$。因此，我们可以简化表达式：\n$$\nS(b) = y^{\\top}y - 2b^{\\top}X^{\\top}y + b^{\\top}X^{\\top}Xb\n$$\n这个二次函数关于向量 $b$ 的梯度是：\n$$\n\\nabla_{b} S(b) = -2X^{\\top}y + 2X^{\\top}Xb\n$$\n将梯度设为零，$\\nabla_{b} S(b) = 0$，得到最小值的必要一阶条件。因此，任何解 $\\hat{\\beta}$ 都必须满足：\n$$\n-2X^{\\top}y + 2X^{\\top}X\\hat{\\beta} = 0\n$$\n$$\nX^{\\top}X\\hat{\\beta} = X^{\\top}y\n$$\n这个线性方程组被称为**正规方程**。任何解决最小二乘最小化问题的向量 $\\hat{\\beta}$ 都必须是正规方程的解。\n\nOLS 残差向量定义为 $e = y - X\\hat{\\beta}$。通过重新整理正规方程，我们可以揭示 OLS 解的几何意义：\n$$\nX^{\\top}y - X^{\\top}X\\hat{\\beta} = 0\n$$\n$$\nX^{\\top}(y - X\\hat{\\beta}) = 0\n$$\n将残差向量 $e$ 的定义代入此方程，我们得到基本正交条件：\n$$\nX^{\\top}e = 0\n$$\n该方程表明残差向量 $e$ 与矩阵 $X$ 的每一列都正交。$X$ 的列构成了 $X$ 的列空间（记作 $\\text{col}(X)$）的一个生成集。因此，OLS 残差向量 $e$ 与整个子空间 $\\text{col}(X)$ 正交。从几何上讲，这意味着拟合值向量 $\\hat{y} = X\\hat{\\beta}$ 是响应向量 $y$ 在 $X$ 的列空间上的唯一正交投影。\n\n无论矩阵 $X$ 的秩如何，这个理论结果都成立。如果 $X$ 是满列秩，则矩阵 $X^{\\top}X$ 是可逆的，唯一的 OLS 解是 $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$。如果 $X$ 是秩亏的，其列是线性相关的，使得矩阵 $X^{\\top}X$ 是奇异的。在这种情况下，$\\hat{\\beta}$ 有无穷多个解。然而，对于任何这样的解向量 $\\hat{\\beta}$，投影向量 $X\\hat{\\beta}$ 是唯一的，因此残差向量 $e = y - X\\hat{\\beta}$ 也是唯一的。所以，正交性质 $X^{\\top}e=0$ 是普遍适用的。\n\n任务是使用有限精度算术来验证这一性质。由于浮点计算中固有的舍入误差，计算出的 $X^{\\top}e$ 值预计不会恰好是零向量。相反，我们必须检查其分量的大小是否足够小。计算正交性的标准是向量 $X^{\\top}e$ 的最大绝对值项，即其无穷范数 $\\|X^{\\top}e\\|_{\\infty}$，小于或等于指定的容差 $\\tau = 10^{-9}$。\n\n为了以一种对所有给定测试用例（包括满秩、秩亏、欠定（$n  k$）和病态矩阵）都稳健的方式找到最小化子 $\\hat{\\beta}$，我们必须使用一种数值稳定的方法。基于 $X$ 的奇异值分解 (SVD) 的方法非常适合此目的。SVD 允许稳定地计算 $X$ 的 Moore-Penrose 伪逆，从而得到最小二乘问题的最小范数解。`scipy.linalg.lstsq` 函数实现了这样一个稳健的求解器。\n\n每个测试用例的计算算法如下：\n$1$. 给定一个矩阵 $X$ 和一个向量 $y$。\n$2$. 使用 `scipy.linalg.lstsq(X, y)` 函数计算问题 $\\min_{b} \\|y - Xb\\|_{2}^{2}$ 的一个最小二乘解 $\\hat{\\beta}$。\n$3$. 计算相应的残差向量 $e = y - X\\hat{\\beta}$。\n$4$. 计算内积向量 $v = X^{\\top}e$。$v$ 的每个分量是 $e$ 与 $X$ 的一个列向量的点积。\n$5$. 确定 $v$ 的分量的最大绝对值，即 $\\|v\\|_{\\infty} = \\max_j |v_j|$。\n$6$. 将此值与容差 $\\tau = 10^{-9}$进行比较。如果 $\\|X^{\\top}e\\|_{\\infty} \\le \\tau$，则认为正交条件得到满足，结果为真 (True)；否则为假 (False)。\n\n此过程将应用于所有五个测试用例。鉴于正交性是最小二乘最小化的直接理论推论，我们预期在数值求解器保持足够精度的情况下，所有用例的结果都为真。包含病态矩阵的用例是专门为了测试所选算法的数值稳定性。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lstsq\n\ndef solve():\n    \"\"\"\n    Verifies the orthogonality of the OLS residual vector to the column space of the design matrix.\n    \n    For each test case (X, y), it computes the OLS estimator beta_hat, the residual e,\n    and checks if the maximum absolute value of X.T @ e is within a given tolerance.\n    \"\"\"\n\n    # Absolute tolerance for checking orthogonality.\n    tau = 1e-9\n\n    # Test Case 1: Full column rank with intercept\n    X1 = np.array([\n        [1.0, 0.2, -1.0],\n        [1.0, -1.3, 0.7],\n        [1.0, 0.5, 1.2],\n        [1.0, 2.0, -0.3],\n        [1.0, -0.7, 0.4]\n    ])\n    y1 = np.array([0.5, -1.2, 0.9, 2.3, -0.4])\n\n    # Test Case 2: Rank-deficient columns\n    X2 = np.array([\n        [1.0, 1.0, 2.0],\n        [1.0, 2.0, 4.0],\n        [1.0, 3.0, 6.0],\n        [1.0, 4.0, 8.0]\n    ])\n    y2 = np.array([1.0, 0.0, 1.0, 0.0])\n\n    # Test Case 3: Perfect fit\n    X3 = np.array([\n        [1.0, 1.0],\n        [1.0, 2.0],\n        [1.0, 3.0]\n    ])\n    y3 = np.array([1.0, 0.0, -1.0])\n\n    # Test Case 4: Underdetermined system (n  k)\n    X4 = np.array([\n        [1.0, 0.0, 1.0, 0.0, 2.0],\n        [0.0, 1.0, 1.0, 0.0, 0.0],\n        [1.0, 1.0, 0.0, 1.0, 1.0]\n    ])\n    y4 = np.array([1.0, 2.0, 3.0])\n\n    # Test Case 5: Ill-conditioned columns (nearly collinear)\n    X5 = np.array([\n        [1.0, 1.0, 2.000001],\n        [1.0, 2.0, 3.999999],\n        [1.0, 3.0, 6.000001],\n        [1.0, 4.0, 7.999999],\n        [1.0, 5.0, 10.000001]\n    ])\n    y5 = np.array([2.0, 1.0, 0.0, 1.0, 2.0])\n\n    test_cases = [\n        (X1, y1),\n        (X2, y2),\n        (X3, y3),\n        (X4, y4),\n        (X5, y5)\n    ]\n\n    results = []\n    for X, y in test_cases:\n        # Solve the least squares problem min ||y - Xb||^2 for b.\n        # lstsq returns the solution b, residuals, rank, and singular values.\n        # We only need the solution vector, beta_hat.\n        # Using cond=None to use the default machine-precision based condition number\n        # and suppress potential FutureWarnings.\n        beta_hat, _, _, _ = lstsq(X, y, cond=None)\n\n        # Calculate the residual vector e = y - X * beta_hat.\n        e = y - X @ beta_hat\n\n        # Check for orthogonality: X^T * e should be close to the zero vector.\n        ortho_check_vector = X.T @ e\n\n        # The condition is met if the largest absolute component of X^T * e is within tolerance tau.\n        max_abs_error = np.max(np.abs(ortho_check_vector))\n        is_orthogonal = max_abs_error = tau\n        \n        results.append(is_orthogonal)\n\n    # Format the output as a comma-separated list of boolean strings in square brackets.\n    # The str() function converts True to 'True' and False to 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "2407237"}, {"introduction": "OLS 推断的一个关键假设是同方差性 (homoskedasticity)。当这个假设被违反时（这在经济和金融数据中很常见），传统的标准误和相关的假设检验可能会产生误导性的结论。这个练习 [@problem_id:2407232] 将向你展示如何计算异方差稳健标准误 (heteroskedasticity-consistent standard errors)，并将其与传统标准误进行比较，这是任何应用研究者都必须掌握的一项关键技能。", "problem": "考虑一个带截距项的线性模型，其中因变量表示为 $y \\in \\mathbb{R}^n$，回归变量矩阵（包含一列代表截距项的全1向量）表示为 $X \\in \\mathbb{R}^{n \\times k}$。令 $\\hat{\\beta} \\in \\mathbb{R}^k$ 为通过最小化残差平方和定义的普通最小二乘法（OLS）估计量。将常规（基于同方差性）协方差估计量和异方差稳健协方差估计量（White 的异方差稳健协方差估计量 (HC0)）定义如下。对于残差 $\\hat{u} = y - X \\hat{\\beta}$，令 $\\hat{\\sigma}^2 = \\hat{u}^\\top \\hat{u} / (n - k)$，并令 $V_{\\text{conv}} = \\hat{\\sigma}^2 (X^\\top X)^{-1}$ 和 $V_{\\text{HC0}} = (X^\\top X)^{-1} \\left( X^\\top \\operatorname{diag}(\\hat{u}_1^2,\\dots,\\hat{u}_n^2) X \\right) (X^\\top X)^{-1}$。对于任意系数索引 $j \\in \\{1,\\dots,k\\}$，将相应的标准误定义为 $se_{\\text{conv},j} = \\sqrt{(V_{\\text{conv}})_{jj}}$ 和 $se_{\\text{HC0},j} = \\sqrt{(V_{\\text{HC0}})_{jj}}$。使用自由度为 $n-k$ 的学生 $t$ 分布，将用于检验原假设 $H_0: \\beta_j = 0$ 的双边 $p$ 值定义为 $p = 2 \\left( 1 - F_{t, n-k} \\left( | \\hat{\\beta}_j / se_j | \\right) \\right)$，其中 $F_{t, n-k}$ 表示自由度为 $n-k$ 的学生 $t$ 分布的累积分布函数，而 $se_j$ 是在指定协方差估计量下的标准误。\n\n给定三个测试用例。在所有用例中，都必须通过向回归变量矩阵添加一列全1向量来包含截距项。在下文的每个用例中，用于报告结果的索引 $j$ 是第一个非常数回归变量，即截距项列之后 $X$ 的第一列对应的回归变量。\n\n测试套件：\n- 用例 $1$（通过构造得到的含异方差性的简单回归）：\n  - 对于 $i \\in \\{1,2,\\dots,10\\}$，定义 $x_i = i$。\n  - 对于奇数 $i$ 定义 $s_i = 1$，对于偶数 $i$ 定义 $s_i = -1$。\n  - 定义 $y_i = 1.5 + 0.8 x_i + 0.3 x_i s_i$。\n  - 回归变量矩阵 $X$ 的列为 $[\\,\\mathbf{1},\\, x\\,]$，其中 $\\mathbf{1}$ 是全1向量，$x$ 是以 $x_i$ 为元素的向量。\n- 用例 $2$（含同方差符号翻转误差的简单回归）：\n  - 对于 $i \\in \\{1,2,\\dots,10\\}$，定义 $x_i = i$。\n  - 对于奇数 $i$ 定义 $s_i = 1$，对于偶数 $i$ 定义 $s_i = -1$。\n  - 定义 $y_i = -0.5 + 2.0 x_i + 0.5 s_i$。\n  - 回归变量矩阵 $X$ 的列为 $[\\,\\mathbf{1},\\, x\\,]$。\n- 用例 $3$（通过构造得到的含近似共线性和异方差性的多元回归）：\n  - 对于 $i \\in \\{1,2,\\dots,8\\}$，定义 $x^{(1)}_i = i$。\n  - 对于奇数 $i$ 定义 $s_i = 1$，对于偶数 $i$ 定义 $s_i = -1$。\n  - 定义 $x^{(2)}_i = x^{(1)}_i + 0.1 s_i$。\n  - 定义 $y_i = 0.5 + 1.0 x^{(1)}_i - 0.5 x^{(2)}_i + 0.2 x^{(1)}_i s_i$。\n  - 回归变量矩阵 $X$ 的列为 $[\\,\\mathbf{1},\\, x^{(1)},\\, x^{(2)}\\,]$。\n\n对于每个用例，计算第一个非常数回归变量（即用例 $1$ 和 $2$ 中 $x$ 的系数，以及用例 $3$ 中 $x^{(1)}$ 的系数）的以下四个量：\n$se_{\\text{conv}}$、$se_{\\text{HC0}}$、$p_{\\text{conv}}$ 和 $p_{\\text{HC0}}$。\n\n要求：\n- 使用上述定义获取 $\\hat{\\beta}$、$se_{\\text{conv}}$、$se_{\\text{HC0}}$、$p_{\\text{conv}}$ 和 $p_{\\text{HC0}}$。在为双边 $p$ 值评估学生 $t$ 分布的累积分布函数时，使用自由度 $n-k$。\n- 数值输出规范：将每个报告的实数四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含用例 1、用例 2、用例 3 的四个值 $[se_{\\text{conv}}, se_{\\text{HC0}}, p_{\\text{conv}}, p_{\\text{HC0}}]$，并将它们平铺成一个长度为 $12$ 的单一列表。例如，其结构为 $[se_{\\text{conv}}^{(1)}, se_{\\text{HC0}}^{(1)}, p_{\\text{conv}}^{(1)}, p_{\\text{HC0}}^{(1)}, se_{\\text{conv}}^{(2)}, se_{\\text{HC0}}^{(2)}, p_{\\text{conv}}^{(2)}, p_{\\text{HC0}}^{(2)}, se_{\\text{conv}}^{(3)}, se_{\\text{HC0}}^{(3)}, p_{\\text{conv}}^{(3)}, p_{\\text{HC0}}^{(3)}]$。", "solution": "问题陈述已经过分析，并被确定为有效。它在科学上以计量经济学和统计学原理为基础，问题设定良好，定义和数据完整一致，并且表述客观。该问题要求应用普通最小二乘法 (OLS) 估计、常规和异方差稳健 (HC0) 协方差矩阵估计以及相关假设检验程序的标准公式。我们将给出一个完整、合理的解答。\n\n所考虑的基本模型是线性回归模型，由 $y = X \\beta + u$ 给出，其中 $y \\in \\mathbb{R}^n$ 是因变量的观测向量，$X \\in \\mathbb{R}^{n \\times k}$ 是回归变量矩阵（包括截距项），$\\beta \\in \\mathbb{R}^k$ 是系数向量，$u \\in \\mathbb{R}^n$ 是未观测到的误差项向量。\n\n$\\beta$ 的 OLS 估计量是通过最小化残差平方和得到的，这产生了众所周知的公式：\n$$\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$$\n然后，残差向量计算如下：\n$$\\hat{u} = y - X \\hat{\\beta}$$\n$\\hat{\\beta}$ 的统计特性由其协方差矩阵描述。问题指定了该矩阵的两种估计量。\n\n$1$. 常规协方差矩阵估计量，在同方差性假设下有效（即，对于所有 $i$，$E[u_i^2 | X] = \\sigma^2$）：\n$$V_{\\text{conv}} = \\hat{\\sigma}^2 (X^\\top X)^{-1}$$\n其中 $\\hat{\\sigma}^2$ 是误差方差的无偏估计量：\n$$\\hat{\\sigma}^2 = \\frac{\\hat{u}^\\top \\hat{u}}{n - k}$$\n\n$2$. White 的异方差稳健协方差矩阵估计量 (HC0)，它对未知形式的异方差性是稳健的：\n$$V_{\\text{HC0}} = (X^\\top X)^{-1} \\left( X^\\top \\Omega X \\right) (X^\\top X)^{-1}$$\n其中 $\\Omega$ 是残差平方的对角矩阵：\n$$\\Omega = \\operatorname{diag}(\\hat{u}_1^2, \\hat{u}_2^2, \\dots, \\hat{u}_n^2)$$\n\n对于任意系数 $\\hat{\\beta}_j$（其中 $j \\in \\{1,\\dots,k\\}$），其估计方差是估计协方差矩阵的第 $j$ 个对角元素。标准误是该方差的平方根。对于这两种估计量，我们有：\n$$se_{\\text{conv},j} = \\sqrt{(V_{\\text{conv}})_{jj}}$$\n$$se_{\\text{HC0},j} = \\sqrt{(V_{\\text{HC0}})_{jj}}$$\n为检验原假设 $H_0: \\beta_j = 0$，我们计算 $t$ 统计量：\n$$t_j = \\frac{\\hat{\\beta}_j}{se_j}$$\n然后使用自由度为 $n-k$ ($df$) 的学生 $t$ 分布计算双边 $p$ 值：\n$$p_j = 2 \\cdot (1 - F_{t, df}(|t_j|))$$\n其中 $F_{t, df}$ 是所述分布的累积分布函数 (CDF)。\n\n现在，我们将此过程应用于指定的三个用例。对于每个用例，我们报告第一个非常数回归变量的系数结果，该系数对应于矩阵 $X$ 的第二列（在基于 1 的系统中索引为 $j=2$，或在基于 0 的编程环境中索引为 $1$）。\n\n**用例 1：通过构造得到的含异方差性的简单回归**\n数据由 $n=10$ 生成。回归变量矩阵为 $X \\in \\mathbb{R}^{10 \\times 2}$，其列为 $[\\mathbf{1}, x]$，其中 $x_i=i$。因此，$k=2$，自由度为 $df = n-k = 8$。因变量为 $y_i = 1.5 + 0.8 x_i + 0.3 x_i s_i$，其中 $s_i$ 在 $1$ 和 $-1$ 之间交替。在 $y$ 对 $[\\mathbf{1}, x]$ 的回归中，隐含的误差项是异方差的，因为其大小与 $x_i$ 成正比。\n计算过程如下：\n$1$. 构建 $y$ 和 $X$。\n$2$. 计算 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$。\n$3$. 计算残差 $\\hat{u} = y - X \\hat{\\beta}$。\n$4$. 计算 $V_{\\text{conv}}$ 和 $V_{\\text{HC0}}$。\n$5$. 提取 $x$ 系数的标准误 $se_{\\text{conv}}$ 和 $se_{\\text{HC0}}$。\n$6$. 计算 $p$ 值 $p_{\\text{conv}}$ 和 $p_{\\text{HC0}}$。\n结果值四舍五入到 6 位小数后为：\n$se_{\\text{conv}} = 0.240748$\n$se_{\\text{HC0}} = 0.219848$\n$p_{\\text{conv}} = 0.003923$\n$p_{\\text{HC0}} = 0.002161$\n\n**用例 2：含同方差符号翻转误差的简单回归**\n数据由 $n=10$ 生成。回归变量矩阵 $X \\in \\mathbb{R}^{10 \\times 2}$ 与用例 1 相同。因此，$k=2$ 且 $df=8$。因变量为 $y_i = -0.5 + 2.0 x_i + 0.5 s_i$。回归中隐含的误差项为 $0.5s_i$，其大小恒定。这对应于同方差设定。\n计算过程与用例 1 类似。在同方差设定下，$V_{\\text{conv}}$ 是合适的估计量，我们预期 $V_{\\text{HC0}}$ 会得出相似的结果。\n结果值四舍五入到 6 位小数后为：\n$se_{\\text{conv}} = 0.057321$\n$se_{\\text{HC0}} = 0.057106$\n$p_{\\text{conv}} = 0.000000$\n$p_{\\text{HC0}} = 0.000000$\n\n**用例 3：含近似共线性和异方差性的多元回归**\n数据由 $n=8$ 生成。回归变量矩阵 $X \\in \\mathbb{R}^{8 \\times 3}$ 的列为 $[\\mathbf{1}, x^{(1)}, x^{(2)}]$，其中 $x_i^{(1)}=i$ 且 $x_i^{(2)} = x_i^{(1)} + 0.1 s_i$。回归变量 $x^{(1)}$ 和 $x^{(2)}$ 高度相关。因此，$k=3$ 且 $df = n-k = 5$。因变量为 $y_i = 0.5 + 1.0 x^{(1)}_i - 0.5 x^{(2)}_i + 0.2 x^{(1)}_i s_i$。模型设定意味着误差项的大小取决于 $x^{(1)}_i$，因此它是异方差的。近似共线性和异方差性的结合使得这是一个具有挑战性的估计问题。\n感兴趣的系数是 $x^{(1)}$ 的系数。计算遵循既定程序。\n结果值四舍五入到 6 位小数后为：\n$se_{\\text{conv}} = 0.954705$\n$se_{\\text{HC0}} = 1.056029$\n$p_{\\text{conv}} = 0.612459$\n$p_{\\text{HC0}} = 0.640697$\n\n收集到的结果在最终答案中以单一列表的形式呈现。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Computes conventional and HC0 standard errors and p-values for specified OLS regression models.\n    \"\"\"\n\n    def get_stats(y, X, coeff_idx=1):\n        \"\"\"\n        Calculates OLS statistics for a given regression setup.\n\n        Args:\n            y (np.ndarray): Dependent variable vector.\n            X (np.ndarray): Regressor matrix (with intercept).\n            coeff_idx (int): 0-based index of the coefficient of interest.\n\n        Returns:\n            tuple: A tuple containing se_conv, se_HC0, p_conv, p_HC0.\n        \"\"\"\n        n, k = X.shape\n        \n        # OLS estimator: beta_hat = (X'X)^{-1} X'y\n        try:\n            XTX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            print(\"Error: X'X is singular.\")\n            return (np.nan, np.nan, np.nan, np.nan)\n            \n        beta_hat = XTX_inv @ X.T @ y\n        \n        # Residuals\n        u_hat = y - X @ beta_hat\n        \n        # Degrees of freedom\n        df = n - k\n        \n        # Conventional (homoskedastic) covariance estimator\n        sigma2_hat = (u_hat.T @ u_hat) / df\n        V_conv = sigma2_hat * XTX_inv\n        se_conv = np.sqrt(V_conv[coeff_idx, coeff_idx])\n        \n        # HC0 (White's) heteroskedasticity-consistent covariance estimator\n        Omega = np.diag(u_hat**2)\n        meat = X.T @ Omega @ X\n        V_HC0 = XTX_inv @ meat @ XTX_inv\n        se_HC0 = np.sqrt(V_HC0[coeff_idx, coeff_idx])\n        \n        # Coefficient of interest\n        beta_j = beta_hat[coeff_idx]\n        \n        # t-statistics\n        t_conv = beta_j / se_conv\n        t_HC0 = beta_j / se_HC0\n        \n        # p-values\n        p_conv = 2 * (1 - t.cdf(np.abs(t_conv), df))\n        p_HC0 = 2 * (1 - t.cdf(np.abs(t_HC0), df))\n        \n        return se_conv, se_HC0, p_conv, p_HC0\n\n    results = []\n\n    # Case 1\n    n1 = 10\n    i_vals_1 = np.arange(1, n1 + 1)\n    x1 = i_vals_1\n    s1 = np.ones(n1)\n    s1[1::2] = -1  # even indices in 0-based array are odd numbers 2, 4,...\n    y1 = 1.5 + 0.8 * x1 + 0.3 * x1 * s1\n    X1 = np.ones((n1, 2))\n    X1[:, 1] = x1\n    \n    se_conv1, se_HC01, p_conv1, p_HC01 = get_stats(y1, X1, coeff_idx=1)\n    results.extend([round(se_conv1, 6), round(se_HC01, 6), round(p_conv1, 6), round(p_HC01, 6)])\n\n    # Case 2\n    n2 = 10\n    i_vals_2 = np.arange(1, n2 + 1)\n    x2 = i_vals_2\n    s2 = np.ones(n2)\n    s2[1::2] = -1\n    y2 = -0.5 + 2.0 * x2 + 0.5 * s2\n    X2 = np.ones((n2, 2))\n    X2[:, 1] = x2\n\n    se_conv2, se_HC02, p_conv2, p_HC02 = get_stats(y2, X2, coeff_idx=1)\n    results.extend([round(se_conv2, 6), round(se_HC02, 6), round(p_conv2, 6), round(p_HC02, 6)])\n\n    # Case 3\n    n3 = 8\n    i_vals_3 = np.arange(1, n3 + 1)\n    x1_3 = i_vals_3\n    s3 = np.ones(n3)\n    s3[1::2] = -1\n    x2_3 = x1_3 + 0.1 * s3\n    y3 = 0.5 + 1.0 * x1_3 - 0.5 * x2_3 + 0.2 * x1_3 * s3\n    X3 = np.ones((n3, 3))\n    X3[:, 1] = x1_3\n    X3[:, 2] = x2_3\n    \n    se_conv3, se_HC03, p_conv3, p_HC03 = get_stats(y3, X3, coeff_idx=1)\n    results.extend([round(se_conv3, 6), round(se_HC03, 6), round(p_conv3, 6), round(p_HC03, 6)])\n\n    # Final print statement in the exact required format.\n    # The format required is a string representation of the list, so we map each number to a string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2407232"}]}