{"hands_on_practices": [{"introduction": "这个问题介绍了一个梯度下降在金融领域的核心应用——寻找最优的套期保值比率。你将通过实现基础算法来最小化一个成本函数，从而亲身体验迭代更新规则及其如何收敛到解决方案 [@problem_id:2375264]。这个练习将一个理论概念转化为一个具体的计算任务，是绝佳的入门实践。", "problem": "一家航空公司寻求通过交易石油期货合约来对冲其航空燃油成本风险。设时间索引为 $t \\in \\{1, \\dots, T\\}$。对于每个时期 $t$，设 $s_t$ 表示观察到的航空公司单位体积航空燃油成本的变化（例如，以美元 (USD) /加仑为单位的变化），并设 $f_t$ 表示同期每个合约单位的期货价格的相应变化。该航空公司选择一个标量对冲比率 $h \\in \\mathbb{R}$，解释为每单位燃油风险敞口对应的期货合约单位数，以减少对冲后成本变化 $x_t(h) = s_t - h f_t$ 的波动。为了抑制过大的头寸，该航空公司会产生一个系数为 $\\gamma \\ge 0$ 的小的二次头寸惩罚。目标是选择 $h$ 以最小化带有惩罚项的样本均方对冲后变化\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2.\n$$\n从给定的初始猜测 $h_0 \\in \\mathbb{R}$ 开始，使用固定步长 $\\alpha > 0$ 的基本梯度下降法来最小化 $J(h)$。在第 $k$ 次迭代中，更新 $h_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)$，并在迭代值的绝对变化满足 $|h_{k+1} - h_k|  \\varepsilon$ 或达到最大迭代次数 $N$ 时终止。您的实现必须：\n- 基于 $J(h)$ 的定义，从第一性原理推导梯度 $\\nabla J(h)$。\n- 通过使用提供的容差 $\\varepsilon$ 和迭代上限 $N$ 来确保数值稳定性。\n- 返回最终的迭代值作为估计的最优对冲比率 $h^\\star$。\n\n实现一个程序，为以下每个测试案例计算 $h^\\star$。对于每个案例，$s$ 和 $f$ 以实数有序列表的形式给出，同时提供 $\\gamma$、$\\alpha$、$h_0$、$\\varepsilon$ 和 $N$。\n\n测试套件：\n- 案例 1：\n  - $s = [\\,\\$1.0,\\,-\\$0.5,\\,\\$0.75,\\,-\\$1.25,\\,\\$0.6,\\,-\\$0.8\\,]$\n  - $f = [\\,\\$0.9,\\,-\\$0.4,\\,\\$0.8,\\,-\\$1.1,\\,\\$0.5,\\,-\\$0.7\\,]$\n  - $\\gamma = 0.01$，$\\alpha = 0.05$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- 案例 2：\n  - $s = [\\,\\$1.0,\\,-\\$1.0,\\,\\$1.0,\\,-\\$1.0,\\,\\$0.5,\\,-\\$0.5\\,]$\n  - $f = [\\,-\\$0.8,\\,\\$0.8,\\,-\\$0.7,\\,\\$0.7,\\,-\\$0.4,\\,\\$0.4\\,]$\n  - $\\gamma = 0.02$，$\\alpha = 0.05$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- 案例 3（弱对冲工具；测试正则化）：\n  - $s = [\\,\\$1.0,\\,\\$0.5,\\,-\\$0.5,\\,-\\$1.0,\\,\\$0.3,\\,-\\$0.2\\,]$\n  - $f = [\\,\\$0.1,\\,-\\$0.1,\\,\\$0.1,\\,-\\$0.1,\\,\\$0.05,\\,-\\$0.05\\,]$\n  - $\\gamma = 0.5$，$\\alpha = 0.02$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n- 案例 4（更长样本，可忽略的惩罚）：\n  - $s = [\\,\\$0.2,\\,-\\$0.1,\\,\\$0.3,\\,-\\$0.25,\\,\\$0.15,\\,-\\$0.05,\\,\\$0.4,\\,-\\$0.35\\,]$\n  - $f = [\\,\\$0.18,\\,-\\$0.12,\\,\\$0.28,\\,-\\$0.22,\\,\\$0.14,\\,-\\$0.06,\\,\\$0.39,\\,-\\$0.33\\,]$\n  - $\\gamma = 0.0$，$\\alpha = 0.08$，$h_0 = 0.0$，$\\varepsilon = 10^{-12}$，$N = 100000$\n\n所有货币量均以美元 (USD) 计，但输出的对冲比率是无单位的。您的程序应生成单行输出，其中包含四个估计的对冲比率，形式为用方括号括起来的逗号分隔列表，每个比率四舍五入到恰好 $6$ 位小数（例如，$[1.234567,-0.123456,0.000001,2.000000]$）。不应打印任何其他文本。", "solution": "所述问题需要经过验证。\n\n**步骤1：提取已知条件**\n\n目标是找到对冲比率 $h \\in \\mathbb{R}$，以最小化以下带惩罚项的成本函数：\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\left(s_t - h f_t\\right)^2 + \\gamma h^2\n$$\n其中：\n- $t \\in \\{1, \\dots, T\\}$ 是时间索引。\n- $s_t$ 是单位体积航空燃油成本的变化。\n- $f_t$ 是每个合约单位的期货价格的变化。\n- $\\gamma \\ge 0$ 是二次头寸惩罚系数。\n\n优化将使用基本梯度下降法进行，更新规则为：\n$$\nh_{k+1} = h_k - \\alpha \\, \\nabla J(h_k)\n$$\n其中：\n- $k$ 是迭代索引。\n- $h_k$ 是第 $k$ 次迭代时的对冲比率估计值。\n- $\\alpha  0$ 是一个固定的步长。\n- $\\nabla J(h_k)$ 是在 $h_k$ 处计算的 $J(h)$ 的梯度。\n\n算法从初始猜测 $h_0$ 开始，并在满足以下两个条件之一时终止：\n1.  收敛：$|h_{k+1} - h_k|  \\varepsilon$，其中 $\\varepsilon$ 是指定的容差。\n2.  达到最大迭代次数：迭代次数达到最大值 $N$。\n\n提供的测试案例如下：\n- **案例 1**：\n  - $s = [1.0, -0.5, 0.75, -1.25, 0.6, -0.8]$\n  - $f = [0.9, -0.4, 0.8, -1.1, 0.5, -0.7]$\n  - $\\gamma = 0.01$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **案例 2**：\n  - $s = [1.0, -1.0, 1.0, -1.0, 0.5, -0.5]$\n  - $f = [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4]$\n  - $\\gamma = 0.02$, $\\alpha = 0.05$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **案例 3**：\n  - $s = [1.0, 0.5, -0.5, -1.0, 0.3, -0.2]$\n  - $f = [0.1, -0.1, 0.1, -0.1, 0.05, -0.05]$\n  - $\\gamma = 0.5$, $\\alpha = 0.02$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n- **案例 4**：\n  - $s = [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35]$\n  - $f = [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33]$\n  - $\\gamma = 0.0$, $\\alpha = 0.08$, $h_0 = 0.0$, $\\varepsilon = 10^{-12}$, $N = 100000$\n\n**步骤2：使用提取的已知条件进行验证**\n\n根据验证标准对问题进行评估。\n- **科学依据**：该问题是正则化线性回归在金融工程（对冲）标准问题中的应用，这是统计学和机器学习中的一种基本技术。目标函数是岭回归的一种形式，而梯度下降是其典型的优化方法。该公式在科学和数学上是合理的。\n- **适定性**：目标函数 $J(h)$ 是关于 $h$ 的二次函数。具体来说，$J(h) = A h^2 - B h + C$，其中二次项的系数是 $A = (\\frac{1}{T} \\sum_{t=1}^T f_t^2) + \\gamma$。由于 $\\gamma \\ge 0$ 且 $f_t^2 \\ge 0$，只要不是所有的 $f_t$ 都为零或 $\\gamma > 0$，系数 $A$ 就是严格为正的。在所有测试案例中，向量 $f$ 都不是零向量，因此 $A > 0$。这意味着 $J(h)$ 是一个严格凸函数，并拥有唯一的全局最小值。因此，该问题是适定的。\n- **客观性**：问题以精确的数学术语陈述，没有歧义、主观性或观点。\n- **完整性**：为每个测试案例提供了所有需要的数据和参数（$s, f, \\gamma, \\alpha, h_0, \\varepsilon, N$）。问题是自洽的。\n\n**步骤3：结论与行动**\n\n问题被判定为**有效**。将提供解答。\n\n**求解推导**\n\n梯度下降算法的核心是计算目标函数 $\\nabla J(h)$ 的梯度。我们通过对 $J(h)$ 关于 $h$ 求导，从第一性原理推导这个梯度。\n\n目标函数为：\n$$\nJ(h) = \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2\n$$\n\n梯度 $\\nabla J(h)$ 就是常导数 $\\frac{dJ}{dh}$：\n$$\n\\nabla J(h) = \\frac{d}{dh} \\left( \\frac{1}{T} \\sum_{t=1}^{T} (s_t - h f_t)^2 + \\gamma h^2 \\right)\n$$\n\n根据微分的线性性质，我们可以逐项求导：\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} \\frac{d}{dh} (s_t - h f_t)^2 + \\frac{d}{dh} (\\gamma h^2)\n$$\n\n对第一项应用链式法则，对第二项应用幂法则：\n$$\n\\frac{d}{dh} (s_t - h f_t)^2 = 2 (s_t - h f_t) \\cdot \\frac{d}{dh}(s_t - h f_t) = 2 (s_t - h f_t) (-f_t) = -2s_t f_t + 2h f_t^2\n$$\n$$\n\\frac{d}{dh} (\\gamma h^2) = 2 \\gamma h\n$$\n\n将这些代回梯度的表达式中：\n$$\n\\nabla J(h) = \\frac{1}{T} \\sum_{t=1}^{T} (-2s_t f_t + 2h f_t^2) + 2 \\gamma h\n$$\n\n我们可以将求和内的项分开：\n$$\n\\nabla J(h) = \\frac{1}{T} \\left( \\sum_{t=1}^{T} (-2s_t f_t) + \\sum_{t=1}^{T} (2h f_t^2) \\right) + 2 \\gamma h\n$$\n$$\n\\nabla J(h) = -\\frac{2}{T} \\sum_{t=1}^{T} s_t f_t + \\frac{2h}{T} \\sum_{t=1}^{T} f_t^2 + 2 \\gamma h\n$$\n\n提取公因式 $2h$：\n$$\n\\nabla J(h) = 2h \\left( \\frac{1}{T} \\sum_{t=1}^{T} f_t^2 + \\gamma \\right) - \\frac{2}{T} \\sum_{t=1}^{T} s_t f_t\n$$\n这是梯度的解析表达式。该表达式将用于迭代更新规则。\n\n算法流程如下：\n1.  用 $h_0$ 初始化 $h_k$。\n2.  对于从 $0$ 到 $N-1$ 的 $k$：\n    a. 使用推导出的公式计算梯度 $\\nabla J(h_k)$。为提高效率，我们首先预计算样本矩 $\\frac{1}{T} \\sum s_t f_t$ 和 $\\frac{1}{T} \\sum f_t^2$。\n    b. 更新估计值：$h_{k+1} = h_k - \\alpha \\nabla J(h_k)$。\n    c. 检查收敛性：如果 $|h_{k+1} - h_k|  \\varepsilon$，则算法已收敛。最终估计值为 $h^\\star = h_{k+1}$。终止。\n3.  如果循环完成而未满足收敛标准，则算法因达到最大迭代次数而终止。最终估计值为 $h^\\star = h_N$。\n\n该过程将对每个测试案例实施，以找到最优对冲比率 $h^\\star$。提供的步长 $\\alpha$ 足够小，以确保稳定地收敛到凸目标函数的唯一最小值。\n通过令 $\\nabla J(h) = 0$ 找到的最小值解析解为 $h^\\star = \\frac{\\sum s_t f_t}{\\sum f_t^2 + T\\gamma}$，这可以作为迭代算法收敛性的验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal hedging ratio using gradient descent for multiple test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"s\": [1.0, -0.5, 0.75, -1.25, 0.6, -0.8],\n            \"f\": [0.9, -0.4, 0.8, -1.1, 0.5, -0.7],\n            \"gamma\": 0.01,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, -1.0, 1.0, -1.0, 0.5, -0.5],\n            \"f\": [-0.8, 0.8, -0.7, 0.7, -0.4, 0.4],\n            \"gamma\": 0.02,\n            \"alpha\": 0.05,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [1.0, 0.5, -0.5, -1.0, 0.3, -0.2],\n            \"f\": [0.1, -0.1, 0.1, -0.1, 0.05, -0.05],\n            \"gamma\": 0.5,\n            \"alpha\": 0.02,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n        {\n            \"s\": [0.2, -0.1, 0.3, -0.25, 0.15, -0.05, 0.4, -0.35],\n            \"f\": [0.18, -0.12, 0.28, -0.22, 0.14, -0.06, 0.39, -0.33],\n            \"gamma\": 0.0,\n            \"alpha\": 0.08,\n            \"h0\": 0.0,\n            \"epsilon\": 1e-12,\n            \"N\": 100000,\n        },\n    ]\n\n    def compute_h_star(s_data, f_data, gamma, alpha, h0, epsilon, N):\n        \"\"\"\n        Computes the optimal hedging ratio h* using gradient descent.\n        \"\"\"\n        s = np.array(s_data)\n        f = np.array(f_data)\n        T = len(s)\n\n        # Pre-compute sample moments for efficiency\n        # E[f^2] = (1/T) * sum(f_t^2)\n        mean_f_sq = np.sum(f**2) / T\n        # E[sf] = (1/T) * sum(s_t * f_t)\n        mean_s_f = np.sum(s * f) / T\n        \n        h_k = h0\n\n        for _ in range(N):\n            # Gradient: grad_J(h) = 2 * (h * (E[f^2] + gamma) - E[sf])\n            grad_J = 2 * (h_k * (mean_f_sq + gamma) - mean_s_f)\n            \n            h_k_plus_1 = h_k - alpha * grad_J\n            \n            if np.abs(h_k_plus_1 - h_k)  epsilon:\n                return h_k_plus_1\n            \n            h_k = h_k_plus_1\n            \n        return h_k\n\n    results = []\n    for case in test_cases:\n        h_star = compute_h_star(\n            case[\"s\"],\n            case[\"f\"],\n            case[\"gamma\"],\n            case[\"alpha\"],\n            case[\"h0\"],\n            case[\"epsilon\"],\n            case[\"N\"],\n        )\n        results.append(h_star)\n\n    # Format the final output as a comma-separated list of strings,\n    # with each number rounded to 6 decimal places, enclosed in brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "2375264"}, {"introduction": "在掌握了基础知识之后，这项实践通过解决一个二维优化问题提升了难度：寻找新店的最佳位置以最大化客流量。你不仅将实现梯度上升（梯度下降的最大化版本），还将引入一项关键技术——回溯线搜索，以自适应地确定步长 [@problem_id:2375271]。这个练习展示了如何处理更贴近现实的场景，即最优步长是未知的。", "problem": "给定一个由空间需求中心构成的二维平滑、无约束目标函数。设有 $m \\in \\mathbb{N}$ 个中心，每个中心具有位置 $\\boldsymbol{\\mu}_i \\in \\mathbb{R}^2$、正权重 $w_i \\in \\mathbb{R}_{0}$ 和各向同性扩展 $s_i \\in \\mathbb{R}_{0}$。将客户流量强度函数 $T: \\mathbb{R}^2 \\to \\mathbb{R}$ 定义为\n$$\nT(\\boldsymbol{x}) \\equiv \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right),\n$$\n其中 $\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数。您的任务是使用第一性原理推导梯度 $\\nabla T(\\boldsymbol{x})$，然后实现带有回溯线搜索的梯度上升法，为下面的每个测试实例估计 $T(\\boldsymbol{x})$ 的一个最大化子 $\\boldsymbol{x}^\\star \\in \\mathbb{R}^2$。\n\n从以下基本原理出发：\n- 梯度作为偏导数向量的定义。\n- 用于可微函数复合的微分链式法则。\n- 方向导数在梯度方向上给出最大瞬时增长率这一事实。\n- 回溯线搜索的 Armijo 条件：对于步长 $\\alpha  0$ 和上升方向 $\\boldsymbol{d}$，如果满足\n$$\nT(\\boldsymbol{x} + \\alpha \\boldsymbol{d}) \\ge T(\\boldsymbol{x}) + c \\alpha \\, \\nabla T(\\boldsymbol{x})^\\top \\boldsymbol{d}\n$$\n则接受 $\\alpha$，其中 $c \\in (0,1)$ 是一个选定的常数。\n\n算法要求：\n- 使用最速上升法，方向为 $\\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)$，迭代公式为 $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$。\n- 使用回溯线搜索，从初始步长 $\\alpha_0$ 开始，以因子 $\\rho \\in (0,1)$ 缩减步长，直到满足 Armijo 条件或步长低于最小阈值时停止。\n- 当梯度范数低于容差 $\\varepsilon$ 或达到最大迭代次数时停止。\n- 除了 $\\boldsymbol{x} \\in \\mathbb{R}^2$ 之外，对 $\\boldsymbol{x}$ 没有其他约束，也无需报告物理单位。\n\n测试套件：\n- 案例 1（一般情况，峰值分离良好）：\n  - $m = 3$,\n  - $\\boldsymbol{\\mu}_1 = (1, 2)$, $w_1 = 100$, $s_1 = 1.0$;\n  - $\\boldsymbol{\\mu}_2 = (-2, -1)$, $w_2 = 80$, $s_2 = 1.5$;\n  - $\\boldsymbol{\\mu}_3 = (3, -3)$, $w_3 = 120$, $s_3 = 0.7$;\n  - 初始化 $\\boldsymbol{x}_0 = (0, 0)$。\n- 案例 2（对称，初始化时梯度平坦）：\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (-2, 0)$, $w_1 = 50$, $s_1 = 0.5$;\n  - $\\boldsymbol{\\mu}_2 = (2, 0)$, $w_2 = 50$, $s_2 = 0.5$;\n  - 初始化 $\\boldsymbol{x}_0 = (0, 0)$。\n- 案例 3（局部最大值与全局最大值，对初始化的敏感性）：\n  - $m = 2$,\n  - $\\boldsymbol{\\mu}_1 = (5, 5)$, $w_1 = 200$, $s_1 = 2.0$;\n  - $\\boldsymbol{\\mu}_2 = (-5, -5)$, $w_2 = 180$, $s_2 = 1.0$;\n  - 初始化 $\\boldsymbol{x}_0 = (-10, -10)$。\n\n对所有案例使用相同的线搜索和停止参数：\n- 初始步长 $\\alpha_0 = 1.0$,\n- 缩减因子 $\\rho = 0.5$,\n- Armijo 系数 $c = 10^{-4}$,\n- 梯度范数容差 $\\varepsilon = 10^{-8}$,\n- 最大迭代次数 $K_{\\max} = 10000$,\n- 回溯的最小步长阈值 $\\alpha_{\\min} = 10^{-12}$。\n\n输出规范：\n- 对于每个案例，返回估计的最大化子和目标值，形式为列表 $[\\hat{x}, \\hat{y}, T(\\hat{\\boldsymbol{x}})]$，其中 $\\hat{\\boldsymbol{x}} = (\\hat{x}, \\hat{y})$。\n- 每个标量必须是四舍五入到 $6$ 位小数的浮点数。\n- 您的程序应生成单行输出，其中包含三个案例结果的逗号分隔列表，并用方括号括起来，同时保留内部列表结构。具体来说，打印\n  $$\n  \\big[ [\\hat{x}_1,\\hat{y}_1,T_1],\\; [\\hat{x}_2,\\hat{y}_2,T_2],\\; [\\hat{x}_3,\\hat{y}_3,T_3] \\big],\n  $$\n  按规定进行四舍五入，不含多余的空格或文本。例如，一个语法上有效的输出应如下所示\n  $$\n  [[0.123456,-1.234567,9.876543],[\\dots],[\\dots]].\n  $$\n预期的答案类型是浮点数列表，最终输出将这三个列表聚合到单行的顶级列表中。", "solution": "所呈现的问题是一个标准的无约束非线性优化问题。任务是使用最速上升法（梯度上升法的一种具体形式）找到给定目标函数 $T(\\boldsymbol{x})$ 的一个局部最大值。该问题定义明确，科学上合理，并且为数值求解提供了所有必要的参数和条件。因此，该问题是有效的。\n\n解决方案分两个阶段进行。首先，我们推导目标函数梯度的解析形式。其次，我们实现带有回溯线搜索的梯度上升算法，为每个指定的测试案例数值求解一个最大化子。\n\n**1. 梯度 $\\nabla T(\\boldsymbol{x})$ 的推导**\n\n目标函数给定为：\n$$\nT(\\boldsymbol{x}) = \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right)\n$$\n其中 $\\boldsymbol{x} = (x_1, x_2)^\\top \\in \\mathbb{R}^2$ 是变量位置。\n\n$T(\\boldsymbol{x})$ 的梯度，记作 $\\nabla T(\\boldsymbol{x})$，是其关于 $\\boldsymbol{x}$ 各分量的偏导数组成的向量：\n$$\n\\nabla T(\\boldsymbol{x}) = \\begin{pmatrix} \\frac{\\partial T}{\\partial x_1} \\\\ \\frac{\\partial T}{\\partial x_2} \\end{pmatrix}\n$$\n\n由于微分算子的线性特性，和的梯度等于梯度的和。因此，我们可以独立计算求和中每一项的梯度：\n$$\n\\nabla T(\\boldsymbol{x}) = \\nabla \\left( \\sum_{i=1}^{m} w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right) = \\sum_{i=1}^{m} \\nabla \\left( w_i \\exp\\left( -\\frac{1}{2 s_i^2} \\left\\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\right\\|_2^2 \\right) \\right)\n$$\n\n我们来分析单项 $T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x}))$，其中指数为 $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2$。\n使用向量函数的链式法则，$\\nabla (\\exp(f(\\boldsymbol{x}))) = \\exp(f(\\boldsymbol{x})) \\nabla f(\\boldsymbol{x})$。应用此法则，我们得到：\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp(f_i(\\boldsymbol{x})) \\nabla f_i(\\boldsymbol{x})\n$$\n\n现在，我们必须计算 $f_i(\\boldsymbol{x})$ 的梯度。欧几里得范数的平方为 $\\| \\boldsymbol{z} \\|_2^2 = \\boldsymbol{z}^\\top \\boldsymbol{z}$。令 $\\boldsymbol{z} = \\boldsymbol{x} - \\boldsymbol{\\mu}_i$。则 $f_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)^\\top (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)$。\n二次型 $\\boldsymbol{x}^\\top A \\boldsymbol{x}$ 的梯度是 $(A+A^\\top)\\boldsymbol{x}$。这里，我们有一个更简单的情况。令 $\\boldsymbol{\\mu}_i = (\\mu_{i1}, \\mu_{i2})^\\top$。\n$$\nf_i(\\boldsymbol{x}) = -\\frac{1}{2s_i^2} \\left( (x_1 - \\mu_{i1})^2 + (x_2 - \\mu_{i2})^2 \\right)\n$$\n偏导数是：\n$$\n\\frac{\\partial f_i}{\\partial x_1} = -\\frac{1}{2s_i^2} \\cdot 2(x_1 - \\mu_{i1}) \\cdot 1 = -\\frac{1}{s_i^2}(x_1 - \\mu_{i1})\n$$\n$$\n\\frac{\\partial f_i}{\\partial x_2} = -\\frac{1}{2s_i^2} \\cdot 2(x_2 - \\mu_{i2}) \\cdot 1 = -\\frac{1}{s_i^2}(x_2 - \\mu_{i2})\n$$\n将它们组合成梯度向量 $\\nabla f_i(\\boldsymbol{x})$：\n$$\n\\nabla f_i(\\boldsymbol{x}) = \\begin{pmatrix} -\\frac{1}{s_i^2}(x_1 - \\mu_{i1}) \\\\ -\\frac{1}{s_i^2}(x_2 - \\mu_{i2}) \\end{pmatrix} = -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i)\n$$\n\n将其代回到 $\\nabla T_i(\\boldsymbol{x})$ 的表达式中：\n$$\n\\nabla T_i(\\boldsymbol{x}) = w_i \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) \\left( -\\frac{1}{s_i^2} (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right)\n$$\n\n最后，对所有 $i = 1, \\dots, m$ 求和，得到目标函数 $T(\\boldsymbol{x})$ 的完整梯度：\n$$\n\\nabla T(\\boldsymbol{x}) = \\sum_{i=1}^{m} \\left[ - \\frac{w_i}{s_i^2} \\exp\\left( -\\frac{1}{2s_i^2} \\| \\boldsymbol{x} - \\boldsymbol{\\mu}_i \\|_2^2 \\right) (\\boldsymbol{x} - \\boldsymbol{\\mu}_i) \\right]\n$$\n这个表达式是实现梯度上升算法的基础。\n\n**2. 算法实现**\n\n最速上升法通过沿梯度 $\\nabla T(\\boldsymbol{x}_k)$ 方向前进一步来迭代更新当前估计值 $\\boldsymbol{x}_k$。更新规则为：\n$$\n\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k, \\quad \\text{其中} \\quad \\boldsymbol{d}_k = \\nabla T(\\boldsymbol{x}_k)\n$$\n步长 $\\alpha_k$ 在每次迭代中使用回溯线搜索确定，以确保目标函数有足够的增长，具体由 Armijo 条件指定。\n\n**算法：带回溯线搜索的梯度上升法**\n\n1.  **初始化**：\n    -   设置迭代计数器 $k = 0$。\n    -   设置初始点 $\\boldsymbol{x}_0$。\n    -   设置参数：初始步长 $\\alpha_0$，缩减因子 $\\rho$，Armijo 常数 $c$，容差 $\\varepsilon$，最大迭代次数 $K_{\\max}$，最小步长 $\\alpha_{\\min}$。\n\n2.  **迭代**：对于 $k = 0, 1, 2, \\ldots, K_{\\max}-1$：\n    a.  **计算梯度**：使用推导出的公式计算梯度向量 $\\boldsymbol{g}_k = \\nabla T(\\boldsymbol{x}_k)$。\n    b.  **检查收敛性**：如果 $\\| \\boldsymbol{g}_k \\|_2  \\varepsilon$，则梯度足够小。终止并返回 $\\boldsymbol{x}_k$ 作为估计的最大化子。\n    c.  **回溯线搜索**：找到一个步长 $\\alpha_k$。\n        i.   初始化步长 $\\alpha = \\alpha_0$。\n        ii.  上升方向为 $\\boldsymbol{d}_k = \\boldsymbol{g}_k$。\n        iii. 计算 Armijo 条件的右侧：$RHS = T(\\boldsymbol{x}_k) + c \\alpha \\boldsymbol{g}_k^\\top \\boldsymbol{d}_k = T(\\boldsymbol{x}_k) + c \\alpha \\| \\boldsymbol{g}_k \\|_2^2$。\n        iv.  当 $\\alpha \\ge \\alpha_{\\min}$ 时：\n             -   计算候选点：$\\boldsymbol{x}_{\\text{new}} = \\boldsymbol{x}_k + \\alpha \\boldsymbol{d}_k$。\n             -   在新点评估目标函数：$LHS = T(\\boldsymbol{x}_{\\text{new}})$。\n             -   如果 $LHS \\ge RHS$，则 Armijo 条件满足。设置 $\\alpha_k = \\alpha$ 并跳出内部 while 循环。\n             -   否则，缩减步长：$\\alpha = \\rho \\alpha$。\n        v.   如果循环因 $\\alpha  \\alpha_{\\min}$ 而终止，则步长太小无法取得进展。终止主算法并返回当前点 $\\boldsymbol{x}_k$。\n    d.  **更新位置**：更新估计值：$\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + \\alpha_k \\boldsymbol{d}_k$。\n\n3.  **终止**：如果循环因达到 $K_{\\max}$ 而完成，则终止并返回最终点 $\\boldsymbol{x}_{K_{\\max}}$。\n\n此算法将使用指定的参数应用于三个测试案例中的每一个。\n\n-   **案例 1** 从 $\\boldsymbol{x}_0 = (0,0)$ 开始，预计会收敛到三个局部最大值之一。权重、扩展和距原点的距离的组合决定了算法将遵循哪个吸引盆。\n-   **案例 2** 从 $\\boldsymbol{x}_0 = (0,0)$ 开始，由于设置的对称性，该点是一个鞍点。在该点，$\\nabla T(0,0) = \\boldsymbol{0}$。由于梯度范数低于容差 $\\varepsilon$，算法预计会在第 0 次迭代时立即终止。\n-   **案例 3** 展示了对初始化的敏感性。起始点 $\\boldsymbol{x}_0 = (-10,-10)$ 位于靠近 $\\boldsymbol{\\mu}_2 = (-5,-5)$ 的局部最大值的吸引盆内，尽管全局最大值由于其更大的权重 $w_1=200 > w_2=180$ 而靠近 $\\boldsymbol{\\mu}_1 = (5,5)$。算法预计会收敛到这个局部最大值。\n\n实现将包括计算 $T(\\boldsymbol{x})$ 和 $\\nabla T(\\boldsymbol{x})$ 的函数，以及一个主求解器函数，该函数为每个测试案例组织上述迭代过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the result.\n    \"\"\"\n\n    # Define test cases as a list of dictionaries.\n    # Each dictionary contains: centers, initial_x, m\n    # 'centers' is a list of tuples (mu, w, s)\n    test_cases = [\n        {\n            \"m\": 3,\n            \"centers\": [\n                (np.array([1.0, 2.0]), 100.0, 1.0),\n                (np.array([-2.0, -1.0]), 80.0, 1.5),\n                (np.array([3.0, -3.0]), 120.0, 0.7),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([-2.0, 0.0]), 50.0, 0.5),\n                (np.array([2.0, 0.0]), 50.0, 0.5),\n            ],\n            \"initial_x\": np.array([0.0, 0.0]),\n        },\n        {\n            \"m\": 2,\n            \"centers\": [\n                (np.array([5.0, 5.0]), 200.0, 2.0),\n                (np.array([-5.0, -5.0]), 180.0, 1.0),\n            ],\n            \"initial_x\": np.array([-10.0, -10.0]),\n        },\n    ]\n\n    # Algorithmic parameters\n    params = {\n        \"alpha_0\": 1.0,\n        \"rho\": 0.5,\n        \"c\": 1e-4,\n        \"epsilon\": 1e-8,\n        \"k_max\": 10000,\n        \"alpha_min\": 1e-12,\n    }\n\n    results = []\n    for case in test_cases:\n        result_x = run_gradient_ascent(case, params)\n        final_T = T_func(result_x, case[\"centers\"])\n        # Format output as [x, y, T(x)] rounded to 6 decimal places\n        formatted_result = [\n            round(result_x[0], 6),\n            round(result_x[1], 6),\n            round(final_T, 6),\n        ]\n        results.append(formatted_result)\n    \n    # Final print statement in the exact required format\n    # Example: [[0.123456,-1.234567,9.876543],[...],[...]]\n    print(str(results).replace(\" \", \"\"))\n\n\ndef T_func(x, centers):\n    \"\"\"\n    Computes the objective function T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    total_intensity = 0.0\n    for mu_i, w_i, s_i in centers:\n        norm_sq = np.sum((x - mu_i)**2)\n        exponent = -1.0 / (2.0 * s_i**2) * norm_sq\n        total_intensity += w_i * np.exp(exponent)\n    return total_intensity\n\ndef grad_T_func(x, centers):\n    \"\"\"\n    Computes the gradient of the objective function nabla T(x).\n    x: np.array of shape (2,)\n    centers: list of tuples (mu, w, s)\n    \"\"\"\n    grad = np.zeros(2)\n    for mu_i, w_i, s_i in centers:\n        diff = x - mu_i\n        norm_sq = np.sum(diff**2)\n        s_i_sq = s_i**2\n        exp_term = np.exp(-1.0 / (2.0 * s_i_sq) * norm_sq)\n        grad += (-w_i / s_i_sq) * exp_term * diff\n    return grad\n\ndef run_gradient_ascent(case, params):\n    \"\"\"\n    Performs gradient ascent with backtracking line search for a single case.\n    \"\"\"\n    x_k = case[\"initial_x\"].copy()\n    centers = case[\"centers\"]\n    \n    alpha_0 = params[\"alpha_0\"]\n    rho = params[\"rho\"]\n    c = params[\"c\"]\n    epsilon = params[\"epsilon\"]\n    k_max = params[\"k_max\"]\n    alpha_min = params[\"alpha_min\"]\n\n    for _ in range(k_max):\n        grad_k = grad_T_func(x_k, centers)\n        grad_norm = np.linalg.norm(grad_k)\n\n        if grad_norm  epsilon:\n            break\n\n        # Backtracking line search\n        alpha = alpha_0\n        d_k = grad_k\n        T_k = T_func(x_k, centers)\n        armijo_rhs_const = c * np.dot(grad_k, d_k) # same as c * grad_norm**2\n\n        while alpha >= alpha_min:\n            x_new = x_k + alpha * d_k\n            T_new = T_func(x_new, centers)\n            \n            if T_new >= T_k + alpha * armijo_rhs_const:\n                x_k = x_new\n                break\n            \n            alpha *= rho\n        \n        if alpha  alpha_min:\n            # Cannot find a suitable step, terminate.\n            break\n            \n    return x_k\n\nsolve()\n```", "id": "2375271"}, {"introduction": "在你实现了算法之后，理解其局限性至关重要。本练习探讨了优化中的一个经典挑战：对于非凸函数，梯度下降的结果对初始点高度敏感。通过分析一个微小的初始点变化如何导致算法收敛到一个完全不同（且次优）的最小值，你将更深刻地理解为什么梯度下降是一种*局部*搜索方法，并体会到损失函数形态的重要性 [@problem_id:2375265]。", "problem": "一位计算金融领域的研究员正在一个代表性代理人资产定价模型中校准一个标量偏好参数 $\\theta$，方法是最小化一个捕捉了模型设定错误和微弱经验倾向的风格化非凸损失函数。该损失函数为\n$$\nL(\\theta) = (\\theta^{2} - 1)^{2} - 0.2\\theta\n$$\n研究员使用恒定步长 $\\alpha$ 的标准梯度下降法，其更新规则为\n$$\n\\theta_{t+1} = \\theta_{t} - \\alpha\\nabla L(\\theta_{t})\n$$\n考虑步长 $\\alpha = 0.1$ 和两个相差 $0.002$ 的初始值 $\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。以下哪个选项最能描述这两次梯度下降运行的渐近行为，并指出是否会达到次优最小值？\n\nA. $\\theta_{0}^{(-)}$ 和 $\\theta_{0}^{(+)}$ 都收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值，该值是全局最小值。\n\nB. $\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优）。\n\nC. $\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局）。\n\nD. 由于 $\\alpha = 0.1$ 对于此损失函数来说太大，无法保证稳定性，两个序列都发散到 $\\pm\\infty$。", "solution": "任何科学探究的首要任务是验证问题陈述的有效性。一个有缺陷的前提会导致一个毫无意义的结论。\n\n### 步骤 1：提取已知条件\n\n问题提供了以下信息：\n- 损失函数：$L(\\theta) = (\\theta^{2} - 1)^{2} - 0.2\\theta$。\n- 梯度下降更新规则：$\\theta_{t+1} = \\theta_{t} - \\alpha\\nabla L(\\theta_{t})$。\n- 步长：$\\alpha = 0.1$。\n- 初始条件：$\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n根据有效性所需标准对问题进行分析。\n- **科学依据**：该问题描述了使用标准梯度下降算法最小化一个标量多项式函数。这是数值优化和微积分中一个基础且被充分理解的问题。该函数是非凸的，这是优化中常见的挑战，使得问题具有非平凡性和相关性。“计算金融”的背景仅仅是一个设定；核心问题纯粹是数学问题。它在科学上是合理的。\n- **适定性**：损失函数被明确定义。更新规则、步长和初始条件都以精确的数值给出。问题要求序列的渐近行为，这是给定设置下的确定性结果。该问题是适定的。\n- **客观性**：问题以客观、精确的数学语言陈述。没有主观或模棱两可的术语。\n\n问题陈述没有违反任何有效性标准。它是一个标准的、自洽的、可解的数学问题。\n\n### 步骤 3：结论与行动\n\n问题陈述是**有效的**。我们继续进行求解。\n\n### 求解推导\n\n求解需要分析损失函数 $L(\\theta)$ 的拓扑结构和梯度下降算法的动力学。\n\n首先，我们必须通过找到其梯度的根来确定损失函数的临界点。损失函数为 $L(\\theta) = \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta$。\n梯度（一阶导数）是：\n$$\n\\nabla L(\\theta) = L'(\\theta) = \\frac{d}{d\\theta} \\left( \\theta^4 - 2\\theta^2 + 1 - 0.2\\theta \\right) = 4\\theta^3 - 4\\theta - 0.2\n$$\n临界点 $\\theta^*$ 是 $L'(\\theta^*) = 0$ 的解：\n$$\n4(\\theta^*)^3 - 4\\theta^* - 0.2 = 0\n$$\n这是一个三次方程。为了理解其根，我们观察到对于未扰动方程 $4\\theta^3 - 4\\theta = 0$，其根为 $\\theta = 0, \\pm 1$。项 $-0.2$ 是一个小扰动。\n- 在 $\\theta = 0$ 附近，$L'(\\theta) \\approx -4\\theta - 0.2$。令其为 $0$ 得到 $-4\\theta \\approx 0.2$，因此 $\\theta \\approx -0.05$。\n- 在 $\\theta = 1$ 附近，令 $\\theta = 1+\\epsilon$。$L'(1+\\epsilon) = 4(1+\\epsilon)^3 - 4(1+\\epsilon) - 0.2 \\approx 4(1+3\\epsilon) - 4(1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$。令其为 $0$ 得到 $8\\epsilon \\approx 0.2$，因此 $\\epsilon \\approx 0.025$，得出一个在 $\\theta \\approx 1.025$ 附近的根。\n- 在 $\\theta = -1$ 附近，令 $\\theta = -1+\\epsilon$。$L'(-1+\\epsilon) = 4(-1+\\epsilon)^3 - 4(-1+\\epsilon) - 0.2 \\approx 4(-1+3\\epsilon) - 4(-1+\\epsilon) - 0.2 = 8\\epsilon - 0.2$。令其为 $0$ 得到 $8\\epsilon \\approx 0.2$，因此 $\\epsilon \\approx 0.025$，得出一个在 $\\theta \\approx -1 + 0.025 = -0.975$ 附近的根。\n\n所以，我们有三个临界点，大约为 $\\theta_1^* \\approx -0.975$，$\\theta_2^* \\approx -0.05$ 和 $\\theta_3^* \\approx 1.025$。\n\n接下来，我们使用二阶导数检验来对这些临界点进行分类。二阶导数为：\n$$\nL''(\\theta) = \\frac{d}{d\\theta} (4\\theta^3 - 4\\theta - 0.2) = 12\\theta^2 - 4\n$$\n- 在 $\\theta_1^* \\approx -0.975$ 处：$L''(-0.975) = 12(-0.975)^2 - 4 > 12(0.9)^2 - 4 = 12(0.81) - 4 = 9.72 - 4 > 0$。这是一个局部最小值。\n- 在 $\\theta_2^* \\approx -0.05$ 处：$L''(-0.05) = 12(-0.05)^2 - 4 = 12(0.0025) - 4 = 0.03 - 4  0$。这是一个局部最大值。\n- 在 $\\theta_3^* \\approx 1.025$ 处：$L''(1.025) = 12(1.025)^2 - 4 > 12(1)^2 - 4 = 8 > 0$。这是一个局部最小值。\n\n我们有两个局部最小值和一个局部最大值。为了确定全局最小值，我们必须比较两个最小值点处的 $L(\\theta)$ 的值。\n- $L(\\theta_1^*) \\approx L(-0.975) = ((-0.975)^2 - 1)^2 - 0.2(-0.975) \\approx (0.95 - 1)^2 + 0.195 = 0.0025 + 0.195 = 0.1975$。\n- $L(\\theta_3^*) \\approx L(1.025) = ((1.025)^2 - 1)^2 - 0.2(1.025) \\approx (1.05 - 1)^2 - 0.205 = 0.0025 - 0.205 = -0.2025$。\n由于 $L(\\theta_3^*)  L(\\theta_1^*)$，$\\theta \\approx 1.025$ 附近的局部最小值是**全局最小值**，而 $\\theta \\approx -0.975$ 附近的局部最小值是**次优局部最小值**。\n\n位于 $\\theta_2^* \\approx -0.05$ 的局部最大值充当了梯度流的分界线，或称“分水岭”。位于该最大值左侧的初始点将下降到左侧的吸引盆地，而右侧的点将下降到右侧的吸引盆地。\n\n初始值为 $\\theta_{0}^{(-)} = -0.051$ 和 $\\theta_{0}^{(+)} = -0.049$。它们被策略性地放置在局部最大值 $\\theta_2^* \\approx -0.05$ 的两侧。\n- 对于 $\\theta_{0}^{(-)} = -0.051$：该点位于局部最大值的略微左侧。在局部最大值紧邻的左侧区间内，梯度 $L'(\\theta)$ 必须为正。我们来验证一下：$L'(-0.051) = 4(-0.051)^3 - 4(-0.051) - 0.2 \\approx 4(-0.00013) + 0.204 - 0.2 = -0.00052 + 0.004 > 0$。\n梯度下降更新为 $\\theta_{1}^{(-)} = \\theta_{0}^{(-)} - \\alpha L'(\\theta_{0}^{(-)})$。由于 $L'(\\theta_{0}^{(-)}) > 0$，更新将减去一个正数，使参数向左移动：$\\theta_{1}^{(-)}  \\theta_{0}^{(-)}$。该轨迹将收敛到左侧的局部最小值，这是次优的。\n- 对于 $\\theta_{0}^{(+)} = -0.049$：该点位于局部最大值的略微右侧。在局部最大值紧邻的右侧区间内，梯度 $L'(\\theta)$ 必须为负。我们来验证一下：$L'(-0.049) = 4(-0.049)^3 - 4(-0.049) - 0.2 \\approx 4(-0.00012) + 0.196 - 0.2 = -0.00048 - 0.004  0$。\n更新为 $\\theta_{1}^{(+)} = \\theta_{0}^{(+)} - \\alpha L'(\\theta_{0}^{(+)})$。由于 $L'(\\theta_{0}^{(+)})  0$，更新将加上一个正数，使参数向右移动：$\\theta_{1}^{(+)} > \\theta_{0}^{(+)}$。该轨迹将收敛到右侧的局部最小值，这是全局最小值。\n\n最后，我们评估关于发散的说法。对于梯度下降要收敛到最小值 $\\theta^*$，步长的一个充分条件是 $0  \\alpha  2/L''(\\theta^*)$。\n- 在左侧最小值 ($\\theta_1^*$) 处，$L''(\\theta_1^*) \\approx 7.4$。条件是 $\\alpha  2/7.4 \\approx 0.27$。我们的 $\\alpha = 0.1$ 满足这个条件。\n- 在右侧最小值 ($\\theta_3^*$) 处，$L''(\\theta_3^*) \\approx 8.6$。条件是 $\\alpha  2/8.6 \\approx 0.23$。我们的 $\\alpha = 0.1$ 也满足这个条件。\n步长 $\\alpha = 0.1$ 足够小以确保收敛，因此序列不会发散。\n\n### 逐项分析\n\nA. **$\\theta_{0}^{(-)}$ 和 $\\theta_{0}^{(+)}$ 都收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值，该值是全局最小值。**\n初始点位于局部最大值的两侧，因此落入不同的吸引盆地。它们不会收敛到同一个最小值。此外，左侧的最小值是次优的，而不是全局的。\n结论：**错误**。\n\nB. **$\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优）。**\n这与我们推导出的行为相反。$\\theta_{0}^{(-)}$ 被推向左侧，而 $\\theta_{0}^{(+)}$ 被推向右侧。\n结论：**错误**。\n\nC. **$\\theta_{0}^{(-)}$ 收敛到 $\\theta \\approx -1$ 附近的左侧局部最小值（次优），而 $\\theta_{0}^{(+)}$ 收敛到 $\\theta \\approx 1$ 附近的右侧局部最小值（全局）。**\n这与我们的分析完全吻合。$\\theta_{0}^{(-)} = -0.051$ 位于次优的左侧最小值的吸引盆地中。$\\theta_{0}^{(+)} = -0.049$ 位于全局的右侧最小值的吸引盆地中。\n结论：**正确**。\n\nD. **由于 $\\alpha = 0.1$ 对于此损失函数来说太大，无法保证稳定性，两个序列都发散到 $\\pm\\infty$。**\n我们对步长条件的分析表明，$\\alpha = 0.1$ 完全在收敛到任一最小值的稳定范围内。关于发散的说法是错误的。\n结论：**错误**。", "answer": "$$\\boxed{C}$$", "id": "2375265"}]}