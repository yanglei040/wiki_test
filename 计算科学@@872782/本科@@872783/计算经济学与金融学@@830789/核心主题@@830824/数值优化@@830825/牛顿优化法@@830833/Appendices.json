{"hands_on_practices": [{"introduction": "牛顿法在优化中的应用，实际上是其更普遍的求根功能的特例——即求解$f'(x)=0$。本练习将带你实践牛顿法最基本的求根迭代形式，将其应用于金融领域的一个常见问题：通过反转累积分布函数（CDF）来寻找分位数（例如风险价值 VaR）。这个过程不仅能让你熟练运用牛顿法的核心迭代公式，还能加深你对函数（CDF）与其导数（PDF）之间关系的理解。[@problem_id:2414686]", "problem": "考虑一个由两个高斯（正态）分布组成的混合模型，这是机制转换下金融回报的一个常用模型。设参数为混合权重 $w \\in (0,1)$，均值 $\\mu_1, \\mu_2 \\in \\mathbb{R}$，以及标准差 $\\sigma_1, \\sigma_2 \\in \\mathbb{R}_{++}$。用 $\\Phi(\\cdot)$ 表示标准正态累积分布函数（CDF），用 $\\varphi(\\cdot)$ 表示标准正态概率密度函数（PDF）。该混合分布的累积分布函数 $F:\\mathbb{R}\\to[0,1]$ 和概率密度函数 $f:\\mathbb{R}\\to\\mathbb{R}_{+}$ 定义如下\n$$\nF(x) \\;=\\; w\\,\\Phi\\!\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right) \\;+\\; (1-w)\\,\\Phi\\!\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right),\n$$\n$$\nf(x) \\;=\\; w\\,\\frac{1}{\\sigma_1}\\,\\varphi\\!\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right) \\;+\\; (1-w)\\,\\frac{1}{\\sigma_2}\\,\\varphi\\!\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right).\n$$\n对于任意概率水平 $p \\in (0,1)$，（左）分位数 $Q(p)$ 定义为\n$$\nQ(p) \\;=\\; \\inf\\{\\, x \\in \\mathbb{R} \\;:\\; F(x) \\ge p \\,\\}.\n$$\n您的任务是为下面的测试集中的每个参数组计算 $Q(p)$。所有计算必须在实数上进行。每个结果必须四舍五入到 $8$ 位小数，即精确到 $10^{-8}$ 的最近倍数。\n\n测试集（每个案例为一个六元组 $(w,\\mu_1,\\sigma_1,\\mu_2,\\sigma_2,p)$）：\n- 案例 $1$：$(\\,1.0,\\,0.0,\\,1.0,\\,0.0,\\,1.0,\\,0.975\\,)$\n- 案例 $2$：$(\\,0.3,\\,-1.0,\\,0.5,\\,1.5,\\,1.0,\\,0.5\\,)$\n- 案例 $3$：$(\\,0.9,\\,0.0,\\,0.2,\\,3.0,\\,0.5,\\,0.999\\,)$\n- 案例 $4$：$(\\,0.01,\\,-2.0,\\,0.3,\\,2.0,\\,0.5,\\,0.01\\,)$\n- 案例 $5$：$(\\,0.5,\\,-0.5,\\,1.5,\\,0.5,\\,0.5,\\,0.95\\,)$\n\n最终输出格式：您的程序应生成单行输出，其中包含五个结果，顺序与上述案例相同，以逗号分隔的列表形式包含在方括号内，无空格，例如 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 是按规定四舍五入到 $8$ 位小数的实数。", "solution": "问题陈述已经过验证，被认为是有效的。它在科学上基于标准概率论，特别是关于高斯混合模型，并且是适定 (well-posed) 的。每个测试案例都存在唯一解。注意到一个小的不一致之处：案例 $1$ 提供的参数 $w=1.0$ 位于指定参数空间 $w \\in (0,1)$ 的边界上。这使得问题退化为单个高斯分布，这是一个退化但可解的情况。这并不影响整个问题的结构有效性。\n\n目标是为给定的概率水平 $p \\in (0,1)$ 和两个高斯分布的混合计算（左）分位数 $Q(p)$。分位数 $Q(p)$ 定义为使累积分布函数 $F(x)$ 等于 $p$ 的值 $x$。形式上，我们必须求解方程 $F(x) = p$ 以得到 $x$。这是一个针对函数 $G(x) = F(x) - p = 0$ 的求根问题。\n\n累积分布函数 $F(x)$ 由下式给出：\n$$F(x) = w\\,\\Phi\\!\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right) + (1-w)\\,\\Phi\\!\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)$$\n其中 $w$ 是混合权重，$(\\mu_1, \\sigma_1)$ 和 $(\\mu_2, \\sigma_2)$ 是两个高斯分量的均值和标准差，$\\Phi(\\cdot)$ 是标准正态分布的累积分布函数。\n\n为了找到 $G(x) = F(x) - p$ 的根，我们必须分析 $G(x)$ 的性质。$G(x)$ 关于 $x$ 的导数是：\n$$G'(x) = \\frac{d}{dx}F(x) = f(x)$$\n其中 $f(x)$ 是混合分布的概率密度函数（PDF）：\n$$f(x) = w\\,\\frac{1}{\\sigma_1}\\,\\varphi\\!\\left(\\frac{x-\\mu_1}{\\sigma_1}\\right) + (1-w)\\,\\frac{1}{\\sigma_2}\\,\\varphi\\!\\left(\\frac{x-\\mu_2}{\\sigma_2}\\right)$$\n这里，$\\varphi(\\cdot)$ 是标准正态分布的 PDF。由于 $w \\in [0,1]$，$\\sigma_1, \\sigma_2 \\in \\mathbb{R}_{++}$，并且对于所有 $z \\in \\mathbb{R}$ 都有 $\\varphi(z) > 0$，因此导数 $G'(x) = f(x)$ 对于所有 $x \\in \\mathbb{R}$ 都是严格为正的。这证明了 $F(x)$ 是一个严格单调递增的函数。因此，对于任何给定的 $p \\in (0,1)$，方程 $F(x) = p$ 都存在唯一的解 $x$。\n\n因此，问题被简化为求解超越方程 $G(x) = 0$ 的唯一根。这需要一个数值算法。一个合适且稳健的选择是区间法，例如 Brent 方法，只要找到一个初始区间 $[a, b]$ 使得 $G(a)$ 和 $G(b)$ 异号，该方法就能保证收敛。考虑到 $F(x)$ 的单调性，且有 $\\lim_{x\\to-\\infty} F(x) = 0$ 和 $\\lim_{x\\to\\infty} F(x) = 1$，这样的区间总是可以找到的。例如，一个围绕各分量均值的足够大的区间，如 $[\\min(\\mu_1, \\mu_2) - 15 \\max(\\sigma_1, \\sigma_2), \\max(\\mu_1, \\mu_2) + 15 \\max(\\sigma_1, \\sigma_2)]$，对于任何 $p \\in (0,1)$ 都能可靠地包含根。\n\n对于每个测试案例 $(w, \\mu_1, \\sigma_1, \\mu_2, \\sigma_2, p)$ 的计算过程如下：\n$1.$ 定义目标函数 $G(x) = F(x) - p$。标准正态 CDF $\\Phi(z)$ 可通过标准科学计算库获得，例如 `scipy.stats.norm.cdf`。\n$2.$ 选择一个合适的数值求根算法。我们将使用 `scipy.optimize.root_scalar` 及其 Brent 方法，因为它高效且稳健。\n$3.$ 向求解器提供一个搜索区间 $[a, b]$，以确保根包含在内。\n$4.$ 求解器迭代地优化 $x$ 的估计值，直到 $|G(x)|$ 的值在指定的零容差范围内。得到的 $x$ 即为所求的分位数 $Q(p)$。\n$5.$ 根据要求，将每个案例的结果四舍五入到 $8$ 位小数。\n\n对于案例 $1$ ($w=1.0, \\mu_1=0.0, \\sigma_1=1.0, p=0.975$)，混合模型退化为单个标准正态分布。方程变为 $F(x) = \\Phi(x) = 0.975$。解为 $x = \\Phi^{-1}(0.975)$，其中 $\\Phi^{-1}$ 是标准正态累积分布函数的反函数，或称概率单位函数。这得出 $x \\approx 1.95996398$。这可以作为对通用方法的验证。\n对于其他案例，使用完整的混合模型并应用相同的数值计算过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import root_scalar\n\ndef solve():\n    \"\"\"\n    Solves for the quantile Q(p) of a two-component Gaussian mixture model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a sextuple (w, mu1, sigma1, mu2, sigma2, p).\n    test_cases = [\n        (1.0, 0.0, 1.0, 0.0, 1.0, 0.975),\n        (0.3, -1.0, 0.5, 1.5, 1.0, 0.5),\n        (0.9, 0.0, 0.2, 3.0, 0.5, 0.999),\n        (0.01, -2.0, 0.3, 2.0, 0.5, 0.01),\n        (0.5, -0.5, 1.5, 0.5, 0.5, 0.95),\n    ]\n\n    results = []\n    for case in test_cases:\n        w, mu1, sigma1, mu2, sigma2, p = case\n\n        # Define the function G(x) = F(x) - p, whose root we want to find.\n        # F(x) is the CDF of the Gaussian mixture model.\n        # norm.cdf is the standard normal cumulative distribution function.\n        def objective_function(x):\n            z1 = (x - mu1) / sigma1\n            z2 = (x - mu2) / sigma2\n            F_x = w * norm.cdf(z1) + (1 - w) * norm.cdf(z2)\n            return F_x - p\n\n        # For the degenerate case w=1 or w=0, the quantile can be computed directly\n        # using the inverse CDF (percent point function, ppf).\n        if w == 1.0:\n            quantile = norm.ppf(p, loc=mu1, scale=sigma1)\n        elif w == 0.0:\n            quantile = norm.ppf(p, loc=mu2, scale=sigma2)\n        else:\n            # For a proper mixture, use a numerical root-finding algorithm.\n            # We must provide a search interval [a, b] that brackets the root.\n            # A wide interval based on the component means and standard deviations\n            # is a safe choice.\n            min_mean = min(mu1, mu2)\n            max_mean = max(mu1, mu2)\n            max_sigma = max(sigma1, sigma2)\n            \n            # A generous interval to ensure the root is bracketed.\n            # For any p in (0,1), the quantile will be well within this range.\n            bracket_low = min_mean - 15 * max_sigma\n            bracket_high = max_mean + 15 * max_sigma\n            \n            # `root_scalar` with Brent's method is robust and efficient.\n            sol = root_scalar(objective_function, bracket=[bracket_low, bracket_high], method='brentq')\n            quantile = sol.root\n\n        results.append(quantile)\n\n    # Format the final output as a comma-separated list of numbers rounded\n    # to 8 decimal places, enclosed in square brackets.\n    formatted_results = [f\"{r:.8f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2414686"}, {"introduction": "尽管牛顿法非常强大，但一个未经审慎设计的实现可能会彻底失败。本诊断练习模拟了一个真实场景：一个本应最小化目标函数的优化器，其函数值却在迭代中不降反升。通过分析给定的诊断日志，你将有机会揭示回溯线搜索（backtracking line search）中一个常见但致命的实现错误，从而深刻理解为何以及如何正确地设置“安全保障”机制。[@problem_id:2414722]", "problem": "一家金融机构通过最小化逻辑斯谛规约的负对数似然来估计一个二元违约概率模型，该过程使用牛顿法和旨在强制执行 Armijo 充分下降条件的回溯线搜索。参数向量表示为 $\\beta \\in \\mathbb{R}^p$，目标函数为 $f(\\beta)$，梯度为 $\\nabla f(\\beta)$，Hessian 矩阵为 $\\nabla^2 f(\\beta)$。该实现打印出以下诊断信息。\n\n在第 $k=0$ 次迭代中：\n- $f(\\beta_0) = 120.35$。\n- $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$。\n- $\\nabla^2 f(\\beta_0)$ 的最小和最大特征值分别为 $\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\max} = 214.2$。\n- 求解牛顿方向的线性系统，报告的内积为 $\\nabla f(\\beta_0)^\\top s_0 = -28.9$。\n- 参数为 $c_1 = 10^{-4}$ 的回溯线搜索接受 $t_0 = 1$，并显示信息 “Armijo satisfied”（Armijo条件已满足），代码报告 $f(\\beta_0 + t_0 s_0) = 134.7$。\n\n在第 $k=1$ 次迭代中：\n- $f(\\beta_1) = 134.7$。\n- $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$。\n- 据报告，$\\nabla^2 f(\\beta_1)$ 的最小特征值为 $\\lambda_{\\min} = 2.7$。\n- 计算出的方向满足 $\\nabla f(\\beta_1)^\\top s_1 = -31.4$。\n- 线搜索再次接受 $t_1 = 1$，并显示信息 “Armijo satisfied”，且 $f(\\beta_1 + t_1 s_1) = 150.2$。\n\n假设逻辑斯谛模型的负对数似然被正确设定且二阶连续可微，并且数据集规模适中且条件良好，因此当 Hessian 矩阵为正定时，数值线性代数是稳定的。\n\n基于这些诊断信息和带 Armijo 回溯的无约束最小化牛顿法的标准性质，哪个实现问题与观测到的不收敛现象最一致？\n\nA. Hessian 矩阵在构建时未包含交叉偏导数项，导致其不定，并产生了非下降方向。\n\nB. 求解牛顿系统时符号错误，实际上使用了 $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$，这是一个上升方向。\n\nC. 线搜索中的 Armijo 充分下降测试在实现时不等号方向反了，因此错误地接受了使 $f$ 增加的步长。\n\nD. 算法关于 $\\lVert \\nabla f(\\beta_k) \\rVert_2$ 的停止准则过于严格，导致它在接近最小值点时继续迭代而不是终止。", "solution": "用户提供了一个问题陈述，描述了一个用于最小化负对数似然函数的牛顿法错误实现的输出。我必须验证该陈述，然后根据所提供的数据诊断最可能的实现错误。\n\n**问题验证**\n\n首先，我将提取给定信息并验证问题陈述。\n\n**给定条件：**\n-   **算法**：用于最小化目标函数 $f(\\beta)$ 的牛顿法。\n-   **更新规则**：$\\beta_{k+1} = \\beta_k + t_k s_k$，其中 $s_k$ 是牛顿方向。\n-   **牛顿方向**：$s_k$ 是 $\\nabla^2 f(\\beta_k) s_k = - \\nabla f(\\beta_k)$ 的解。\n-   **线搜索**：回溯法以强制执行 Armijo 充分下降条件，参数为 $c_1 = 10^{-4}$。\n-   **Armijo 条件**：$f(\\beta_k + t_k s_k) \\le f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$。\n-   **假设**：函数 $f$ 是逻辑斯谛模型的二阶连续可微负对数似然。当 Hessian 矩阵为正定时，假定数值计算是稳定的。\n\n**第 $k=0$ 次迭代的数据：**\n-   $f(\\beta_0) = 120.35$\n-   $\\lVert \\nabla f(\\beta_0) \\rVert_2 = 85.7$\n-   $\\nabla^2 f(\\beta_0)$ 的特征值：$\\lambda_{\\min} = 3.1$，$\\lambda_{\\max} = 214.2$。\n-   $\\nabla f(\\beta_0)^\\top s_0 = -28.9$\n-   接受的步长：$t_0 = 1$。\n-   得到的函数值：$f(\\beta_1) = f(\\beta_0 + t_0 s_0) = 134.7$。\n-   算法消息：“Armijo satisfied”。\n\n**第 $k=1$ 次迭代的数据：**\n-   $f(\\beta_1) = 134.7$\n-   $\\lVert \\nabla f(\\beta_1) \\rVert_2 = 92.4$\n-   $\\nabla^2 f(\\beta_1)$ 的最小特征值：$\\lambda_{\\min} = 2.7$。\n-   $\\nabla f(\\beta_1)^\\top s_1 = -31.4$\n-   接受的步长：$t_1 = 1$。\n-   得到的函数值：$f(\\beta_2) = f(\\beta_1 + t_1 s_1) = 150.2$。\n-   算法消息：“Armijo satisfied”。\n\n**验证分析：**\n该问题陈述在数值优化和统计学方面具有科学依据。所提供的数据似乎与算法的既定目标（通过 Armijo 准则进行最小化）相矛盾，但这正是该诊断问题的基础，而不是问题陈述本身的缺陷。该问题是适定的、客观的，并且包含足够的信息以进行逻辑诊断。对于逻辑斯谛回归的负对数似然，Hessian 矩阵保证是半正定的，而对于非退化数据集，它是正定的。给定的正特征值（$\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\min} = 2.7$）与此性质一致。因此该问题是有效的。\n\n**解题推导**\n\n该算法的目标是最小化 $f(\\beta)$。这要求函数值在每一步都减小，即 $f(\\beta_{k+1})  f(\\beta_k)$。Armijo 条件旨在保证这一点，甚至确保“充分”的下降。\n\n让我们分析第 $k=0$ 次迭代的 Armijo 条件：\n该条件为 $f(\\beta_0 + t_0 s_0) \\le f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0$。\n代入给定值：\n-   左侧 (LHS)：$f(\\beta_0 + s_0) = 134.7$。\n-   右侧 (RHS)：$f(\\beta_0) + c_1 t_0 \\nabla f(\\beta_0)^\\top s_0 = 120.35 + (10^{-4})(1)(-28.9) = 120.35 - 0.00289 = 120.34711$。\n\n正确的 Armijo 条件要求 $134.7 \\le 120.34711$，这是不成立的。尽管如此，算法仍然报告 “Armijo satisfied”。此外，函数值增加了：$f(\\beta_1) = 134.7 > f(\\beta_0) = 120.35$。对于一个最小化算法来说，这是一个失败。\n\n让我们对第 $k=1$ 次迭代进行同样的分析：\n该条件为 $f(\\beta_1 + t_1 s_1) \\le f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1$。\n代入给定值：\n-   LHS: $f(\\beta_1 + s_1) = 150.2$。\n-   RHS: $f(\\beta_1) + c_1 t_1 \\nabla f(\\beta_1)^\\top s_1 = 134.7 + (10^{-4})(1)(-31.4) = 134.7 - 0.00314 = 134.69686$。\n\n正确的 Armijo 条件要求 $150.2 \\le 134.69686$，这同样是不成立的。算法再次错误地报告“Armijo satisfied”，并且函数值增加：$f(\\beta_2) = 150.2 > f(\\beta_1) = 134.7$。\n\n该算法正在系统性地失败。它所采取的步骤增加了目标函数值，并正在远离最小值点，梯度范数的增加也表明了这一点（$\\lVert \\nabla f(\\beta_1) \\rVert_2 > \\lVert \\nabla f(\\beta_0) \\rVert_2$）。在函数值增加的情况下，唯一可能打印出“Armijo satisfied”的原因是该条件的逻辑测试实现不正确。\n\n让我们考虑实现中不等号方向反转的可能性：\n$f(\\beta_k + t_k s_k) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$。\n\n-   在 $k=0$ 时，这将测试是否 $134.7 \\ge 120.34711$。这是成立的。\n-   在 $k=1$ 时，这将测试是否 $150.2 \\ge 134.69686$。这也成立。\n\n这个反向的不等式完美地解释了为什么尽管步长 $t=1$ 导致目标函数值大幅增加，但在两次迭代中它都被接受了。\n\n**逐项分析**\n\nA. Hessian 矩阵在构建时未包含交叉偏导数项，导致其不定，并产生了非下降方向。\n**分析**：这一说法与数据相矛盾。两次迭代中 Hessian 矩阵的最小特征值分别被给出为 $\\lambda_{\\min} = 3.1$ 和 $\\lambda_{\\min} = 2.7$。由于最小特征值为正，Hessian 矩阵是正定的，而不是不定的。此外，计算出的方向是下降方向，因为它们与梯度的内积 $\\nabla f(\\beta_0)^\\top s_0 = -28.9$ 和 $\\nabla f(\\beta_1)^\\top s_1 = -31.4$ 都是负数。\n**结论**：不正确。\n\nB. 求解牛顿系统时符号错误，实际上使用了 $s_k = \\nabla^2 f(\\beta_k)^{-1} \\nabla f(\\beta_k)$，这是一个上升方向。\n**分析**：如果是这种情况，搜索方向 $s_k$ 将是一个上升方向。对于一个正定的 Hessian 矩阵 $\\nabla^2 f(\\beta_k)$，方向导数 $\\nabla f(\\beta_k)^\\top s_k$ 将是 $\\nabla f(\\beta_k)^\\top [\\nabla^2 f(\\beta_k)]^{-1} \\nabla f(\\beta_k) > 0$。然而，所提供的数据显示，在两次迭代中 $\\nabla f(\\beta_k)^\\top s_k  0$。因此，计算出的方向是下降方向，而不是上升方向。\n**结论**：不正确。\n\nC. 线搜索中的 Armijo 充分下降测试在实现时不等号方向反了，因此错误地接受了使 $f$ 增加的步长。\n**分析**：如上面的推导所示，如果 Armijo 条件检查被实现为 $f(\\beta_{k+1}) \\ge f(\\beta_k) + c_1 t_k \\nabla f(\\beta_k)^\\top s_k$，算法将接受步长 $t_0=1$ 和 $t_1=1$，并在每次迭代中报告“Armijo satisfied”。这一假设与观测到的函数值增加（$120.35 \\to 134.7 \\to 150.2$）完全一致，并与所有提供的数值数据相匹配。\n**结论**：正确。\n\nD. 算法关于 $\\lVert \\nabla f(\\beta_k) \\rVert_2$ 的停止准则过于严格，导致它在接近最小值点时继续迭代而不是终止。\n**分析**：停止准则的性质与在每次迭代*内部*观察到的错误行为无关。核心问题是算法正在发散（函数值和梯度范数都在增加），而不是它收敛缓慢且未能停止。严格的停止准则不会导致发散；它仅仅意味着如果算法正在收敛，它会运行更长时间。观测到的行为是优化步骤本身的根本性失败。\n**结论**：不正确。", "answer": "$$\\boxed{C}$$", "id": "2414722"}, {"introduction": "在了解了纯粹牛顿法可能遇到的陷阱后，最后的这项实践将指导你构建一个完整的解决方案。你将实现一个“带安全保障”的牛顿法，它结合了牛顿方向的快速收敛性与梯度下降方向的稳健性。当牛顿步存在问题时（例如，方向非下降或导致目标函数值增加），算法会自动回退到更可靠的梯度下降法，这种混合策略是现代实用优化软件的基石。[@problem_id:2414720]", "problem": "考虑光滑实值函数的无约束最小化问题，该问题采用一种在每次迭代中强制执行充分下降条件的更新规则。设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是一个二次连续可微函数。在迭代点 $x_k \\in \\mathbb{R}^n$ 处，定义梯度 $\\nabla f(x_k)$ 和海森矩阵 $\\nabla^2 f(x_k)$。一个候选更新 $x_{k+1} = x_k + \\alpha_k p_k$ 必须满足带有常数 $c_1 \\in (0,1)$ 和 $\\rho \\in (0,1)$ 的 Armijo 充分下降条件，即\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\n其中 $\\alpha_k$ 通过回溯法从 $\\alpha_k = 1$ 开始选择，其形式为 $\\alpha_k \\in \\{ \\rho^j : j \\in \\{0,1,2,\\ldots\\} \\}$。对于给定的 $x_k$，首先尝试选择线性系统\n$$\n\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)\n$$\n的解作为 $p_k$。如果这个候选方向未能在回溯序列中找到满足充分下降条件的步长 $\\alpha_k$，则拒绝该方向，转而选择 $p_k = -\\nabla f(x_k)$，并使用相同的回溯充分下降条件确定 $\\alpha_k$。当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数 $K$ 时，迭代终止。\n\n使用以下固定常数实现此迭代方案：$c_1 = 10^{-4}$，$\\rho = 0.5$，容差 $\\varepsilon = 10^{-8}$，每次迭代的回溯上限为 $M = 50$ 次缩减，以及最大迭代次数 $K = 50$。如果在某个 $x_k$ 处线性系统是奇异的或病态的，则将由 $\\nabla^2 f(x_k)\\, p_k = - \\nabla f(x_k)$ 定义的方向视为失败，并如上所述继续使用 $p_k = -\\nabla f(x_k)$。所有计算都应在实数算术中进行，无需任何外部数据输入。\n\n将此方案应用于以下测试套件。在所有情况下，使用欧几里得范数 $\\|\\cdot\\|_2$ 作为终止准则，并在每次迭代时从步长 $\\alpha_k = 1$ 开始回溯。报告每种情况下由终止条件返回的极小值点估计。将报告的每个数字表示为小数点后精确到六位的小数。\n\n测试用例 A（两个参数的二元选择负对数似然）。设参数为 $x = (b_0, b_1)^\\top \\in \\mathbb{R}^2$。设数据为 $\\{(x_i,y_i)\\}_{i=1}^5$，其中\n$$\n(x_1,y_1) = (-2,0),\\quad (x_2,y_2) = (-1,0),\\quad (x_3,y_3) = (0,1),\\quad (x_4,y_4) = (1,1),\\quad (x_5,y_5) = (2,0).\n$$\n定义负对数似然\n$$\nf(x) = \\sum_{i=1}^5 \\left[ \\log\\!\\left(1 + e^{b_0 + b_1 x_i}\\right) - y_i\\,(b_0 + b_1 x_i) \\right].\n$$\n使用初始点 $x_0 = (0,0)^\\top$。\n\n测试用例 B（病态二次型）。设 $x \\in \\mathbb{R}^2$。定义\n$$\nf(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x,\\quad Q = \\begin{bmatrix} 10^{-6}  0 \\\\ 0  1 \\end{bmatrix},\\quad b = \\begin{bmatrix} 10^{-6} \\\\ 1 \\end{bmatrix}.\n$$\n使用初始点 $x_0 = (10,-10)^\\top$。\n\n测试用例 C（非凸一维四次函数）。设 $x \\in \\mathbb{R}$。定义\n$$\nf(x) = x^4 - 3 x^2 + x.\n$$\n使用初始点 $x_0 = 0.2$。\n\n输出规范。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果按顺序显示为：\n- 对于测试用例 A：一个包含两个数字的列表 $[b_0^\\star,b_1^\\star]$，\n- 对于测试用例 B：一个包含两个数字的列表 $[x_1^\\star,x_2^\\star]$，\n- 对于测试用例 C：一个数字 $x^\\star$，\n每个数字都四舍五入到小数点后六位。例如，格式必须为\n$$\n\\big[ [b_0^\\star,b_1^\\star], [x_1^\\star,x_2^\\star], x^\\star \\big].\n$$\n打印的行中不允许有空格。所有数字都应表示为普通小数。", "solution": "所给出的问题是数值优化中一个明确定义的任务。它要求实现一个混合迭代算法，该算法结合了牛顿法和最速下降法，并通过回溯线搜索来增强，以确保每一步都有充分的下降。该问题在科学上是合理的，基于非线性优化的既定原则，并为三个不同的测试用例提供了所有必要的参数、初始条件和目标函数。因此，该问题被认为是有效的。\n\n待实现的算法是一个针对二次连续可微函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的无约束最小化过程。给定一个迭代点 $x_k$，下一个迭代点 $x_{k+1}$ 通过 $x_{k+1} = x_k + \\alpha_k p_k$ 找到，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n该算法的核心是搜索方向 $p_k$ 的选择。首选是牛顿方向，通过求解线性系统得到：\n$$\n\\nabla^2 f(x_k) p_k = - \\nabla f(x_k)\n$$\n其中 $\\nabla f(x_k)$ 是 $f$ 在 $x_k$ 处的梯度，$\\nabla^2 f(x_k)$ 是海森矩阵。对于二次函数，该方向是最优的，并且在海森矩阵为正定的极小值点附近提供快速的局部收敛（二次收敛）。然而，如果海森矩阵是奇异的，牛顿方向可能无定义；如果海森矩阵不是正定的，它也可能不是一个下降方向。如果 $\\nabla f(x_k)^\\top p_k  0$，则方向 $p_k$ 是一个下降方向。对于牛顿方向，$\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top (\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$，仅当 $\\nabla^2 f(x_k)$ 是正定的时候，该值为负。如果牛顿方向不是下降方向或未能产生合适的步长，算法必须切换到一个稳健的替代方案。\n\n备选方向是最速下降方向，$p_k = -\\nabla f(x_k)$。只要梯度非零，这个方向就保证是下降方向，因为 $\\nabla f(x_k)^\\top p_k = -\\nabla f(x_k)^\\top \\nabla f(x_k) = -\\|\\nabla f(x_k)\\|_2^2  0$。虽然它的收敛速度通常较慢（线性收敛），但它能确保在任何 $\\nabla f(x_k) \\neq 0$ 的点都能取得进展。\n\n步长 $\\alpha_k$ 由回溯线搜索确定。从试验步长 $\\alpha = 1$ 开始，步长被连续乘以一个因子 $\\rho \\in (0,1)$，直到满足 Armijo 充分下降条件：\n$$\nf(x_k + \\alpha p_k) \\le f(x_k) + c_1 \\alpha \\nabla f(x_k)^\\top p_k\n$$\n对于一个常数 $c_1 \\in (0,1)$。问题设定 $c_1 = 10^{-4}$ 和 $\\rho = 0.5$。回溯步数上限设置为 $M=50$。\n\n整个迭代过程如下：\n对于 $k = 0, 1, 2, \\ldots, K-1$：\n1. 计算梯度 $g_k = \\nabla f(x_k)$。如果 $\\|g_k\\|_2 \\le \\varepsilon$，则终止。\n2. 尝试计算牛顿方向 $p_{\\text{Newton}}$。如果 $\\nabla^2f(x_k)$ 是奇异的，或者得到的方向不是下降方向（$\\nabla f(x_k)^\\top p_{\\text{Newton}} \\ge 0$），则此尝试失败。\n3. 如果牛顿方向有效，则执行回溯线搜索。如果在 $M$ 次缩减内找到了步长 $\\alpha_k$，则使用 $p_k = p_{\\text{Newton}}$ 和 $\\alpha_k$ 执行更新。\n4. 如果牛顿方向步骤因任何原因（奇异性、非下降方向或回溯失败）失败，算法将退回到最速下降方向 $p_k = -\\nabla f(x_k)$，并执行另一次回溯线搜索以找到 $\\alpha_k$。\n5. 更新迭代点：$x_{k+1} = x_k + \\alpha_k p_k$。\n如果梯度范数低于容差 $\\varepsilon=10^{-8}$ 或达到最大迭代次数 $K=50$，则过程终止。\n\n实现将把这个逻辑应用于三个测试用例。对于每个用例，我们必须提供目标函数 $f(x)$、其梯度 $\\nabla f(x)$ 和其海森矩阵 $\\nabla^2 f(x)$ 的解析形式。\n\n**测试用例 A：二元选择负对数似然**\n目标函数是逻辑回归模型的负对数似然。\n$f(b_0, b_1) = \\sum_{i=1}^5 \\left[ \\log(1 + e^{b_0 + b_1 x_i}) - y_i(b_0 + b_1 x_i) \\right]$。\n设 $u_i(b) = b_0 + b_1 x_i$。sigmoid 函数为 $\\sigma(u) = 1/(1+e^{-u})$。\n梯度分量为：\n$$\n\\frac{\\partial f}{\\partial b_0} = \\sum_{i=1}^5 (\\sigma(u_i) - y_i)\n$$\n$$\n\\frac{\\partial f}{\\partial b_1} = \\sum_{i=1}^5 x_i (\\sigma(u_i) - y_i)\n$$\n海森矩阵分量为（使用 $\\sigma'(u) = \\sigma(u)(1-\\sigma(u))$）：\n$$\n\\frac{\\partial^2 f}{\\partial b_0^2} = \\sum_{i=1}^5 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1 \\partial b_0} = \\sum_{i=1}^5 x_i \\sigma(u_i)(1-\\sigma(u_i))\n$$\n$$\n\\frac{\\partial^2 f}{\\partial b_1^2} = \\sum_{i=1}^5 x_i^2 \\sigma(u_i)(1-\\sigma(u_i))\n$$\n该海森矩阵总是半正定的，确保牛顿方向（如果已定义）是一个下降方向。\n\n**测试用例 B：病态二次型**\n目标函数是 $f(x) = \\tfrac{1}{2} x^\\top Q x - b^\\top x$。\n梯度为 $\\nabla f(x) = Qx - b$。\n海森矩阵是常数：$\\nabla^2 f(x) = Q$。\n由于 $Q$ 是一个常数正定矩阵，使用完整步长（$\\alpha=1$）的牛顿法将在单次迭代中找到精确的极小值点 $x^*=Q^{-1}b$。\n\n**测试用例 C：非凸一维四次函数**\n目标函数是 $f(x) = x^4 - 3x^2 + x$。\n梯度（导数）为 $f'(x) = 4x^3 - 6x + 1$。\n海森矩阵（二阶导数）为 $f''(x) = 12x^2 - 6$。\n在初始点 $x_0 = 0.2$ 处，海森矩阵为 $f''(0.2) = 12(0.04) - 6 = -5.52$，是负数。因此，在 $x_0$ 处的牛顿方向将是一个上升方向，算法在第一次迭代时必须退回到最速下降法。\n\n以下 Python 代码实现了所述算法，并将其应用于三个测试用例，按规定格式化输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization for all test cases and print results.\n    \"\"\"\n    # Fixed constants for the algorithm\n    C1 = 1e-4\n    RHO = 0.5\n    EPS = 1e-8\n    MAX_ITER = 50\n    BT_LIMIT = 50\n\n    def optimizer(f, grad_f, hess_f, x0):\n        \"\"\"\n        Implements the hybrid Newton/Gradient-Descent optimization algorithm.\n        \"\"\"\n        x = np.array(x0, dtype=float)\n\n        for _ in range(MAX_ITER):\n            g = grad_f(x)\n            if np.linalg.norm(g) = EPS:\n                return x\n\n            p = None\n            alpha = None\n            newton_step_succeeded = False\n\n            # --- Attempt Newton's Method ---\n            try:\n                H = hess_f(x)\n                p_newton = np.linalg.solve(H, -g)\n\n                # Newton direction must be a descent direction\n                if np.dot(g, p_newton)  0:\n                    # Backtracking line search for Newton's direction\n                    alpha_bt = 1.0\n                    fk = f(x)\n                    g_dot_p = np.dot(g, p_newton)\n                    for _ in range(BT_LIMIT):\n                        if f(x + alpha_bt * p_newton) = fk + C1 * alpha_bt * g_dot_p:\n                            p = p_newton\n                            alpha = alpha_bt\n                            newton_step_succeeded = True\n                            break\n                        alpha_bt *= RHO\n            except np.linalg.LinAlgError:\n                # Hessian is singular or ill-posed\n                pass\n\n            # --- Fallback to Steepest Descent ---\n            if not newton_step_succeeded:\n                p_sd = -g\n                \n                # Backtracking line search for steepest descent\n                alpha_bt = 1.0\n                fk = f(x)\n                g_dot_p = np.dot(g, p_sd)\n                for _ in range(BT_LIMIT):\n                    if f(x + alpha_bt * p_sd) = fk + C1 * alpha_bt * g_dot_p:\n                        p = p_sd\n                        alpha = alpha_bt\n                        break\n                    alpha_bt *= RHO\n            \n            # If no step could be found (highly unlikely for steepest descent)\n            if p is None or alpha is None:\n                return x\n\n            # Update x\n            x = x + alpha * p\n        \n        return x\n\n    # --- Test Case A: Logistic Regression ---\n    x_data_A = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    y_data_A = np.array([0.0, 0.0, 1.0, 1.0, 0.0])\n    x0_A = np.array([0.0, 0.0])\n\n    def f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Use np.logaddexp for numerical stability: log(1+exp(x)) = logaddexp(0,x)\n        return np.sum(np.logaddexp(0, u) - y_data_A * u)\n\n    def grad_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        # Sigmoid function: 1 / (1 + exp(-u))\n        with np.errstate(over='ignore'):\n            sigma = 1 / (1 + np.exp(-u))\n        diff = sigma - y_data_A\n        return np.array([np.sum(diff), np.sum(diff * x_data_A)])\n\n    def hess_f_A(b):\n        u = b[0] + b[1] * x_data_A\n        with np.errstate(over='ignore'):\n            sigma = 1 / (1 + np.exp(-u))\n        weights = sigma * (1 - sigma)\n        h00 = np.sum(weights)\n        h01 = np.sum(weights * x_data_A)\n        h11 = np.sum(weights * x_data_A**2)\n        return np.array([[h00, h01], [h01, h11]])\n\n    result_A = optimizer(f_A, grad_f_A, hess_f_A, x0_A)\n\n    # --- Test Case B: Ill-Conditioned Quadratic ---\n    Q_B = np.array([[1e-6, 0.0], [0.0, 1.0]])\n    b_vec_B = np.array([1e-6, 1.0])\n    x0_B = np.array([10.0, -10.0])\n\n    def f_B(x):\n        return 0.5 * x.T @ Q_B @ x - b_vec_B.T @ x\n    \n    def grad_f_B(x):\n        return Q_B @ x - b_vec_B\n    \n    def hess_f_B(x):\n        return Q_B\n\n    result_B = optimizer(f_B, grad_f_B, hess_f_B, x0_B)\n\n    # --- Test Case C: Nonconvex Quartic ---\n    x0_C = np.array([0.2])\n\n    def f_C(x):\n        xv = x[0]\n        return xv**4 - 3 * xv**2 + xv\n    \n    def grad_f_C(x):\n        xv = x[0]\n        return np.array([4 * xv**3 - 6 * xv + 1])\n    \n    def hess_f_C(x):\n        xv = x[0]\n        return np.array([[12 * xv**2 - 6]])\n    \n    result_C = optimizer(f_C, grad_f_C, hess_f_C, x0_C)\n\n    # --- Format Output ---\n    results_list = [result_A, result_B, result_C]\n    formatted_parts = []\n    for res in results_list:\n        if res.size > 1:\n            num_strs = [f\"{val:.6f}\" for val in res]\n            formatted_parts.append(f\"[{','.join(num_strs)}]\")\n        else:\n            formatted_parts.append(f\"{res.item():.6f}\")\n\n    final_output = f\"[[{','.join(formatted_parts)}]]\"\n    # A small correction to match the strange output format from the prompt. [[...], [...], ...]\n    final_output = f\"[{formatted_parts[0]},{formatted_parts[1]},{formatted_parts[2]}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2414720"}]}