{"hands_on_practices": [{"introduction": "虽然最速下降法在概念上更直接，但其收敛在某些情况下可能非常缓慢。这个练习通过一个简单的二维投资组合优化问题，量化了最速下降方向和共轭梯度（CG）方向在优化过程中的表现差异。通过直接计算，您将亲眼见证CG方法如何利用其独特的搜索方向选择策略来避免最速下降法的低效，从而更快地达到最优解。", "problem": "考虑一个包含两种风险资产的无约束均值-方差投资组合选择问题。目标是最小化二次函数\n$$f(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\Sigma \\, w \\;-\\; \\mu^{\\top} w,$$\n其中协方差矩阵为\n$$\\Sigma \\;=\\; \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix},$$\n期望收益向量为\n$$\\mu \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},$$\n且初始投资组合为\n$$w_{0} \\;=\\; \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.$$\n\n在步骤 $k \\in \\{1,2\\}$，从相同的当前迭代点出发，比较两个搜索方向：最速下降方向 $d_{k}^{\\mathrm{SD}} \\,=\\, -\\nabla f(w_{k-1})$ 和一个共轭梯度 (CG) 方向 $d_{k}^{\\mathrm{CG}}$。该 CG 方向被构造为与前一个 CG 方向 $\\Sigma$-共轭（其中 $d_{1}^{\\mathrm{CG}} \\,=\\, -\\nabla f(w_{0})$ 且 $d_{2}^{\\mathrm{CG}}$ 满足 $\\big(d_{2}^{\\mathrm{CG}}\\big)^{\\top} \\Sigma \\, d_{1}^{\\mathrm{CG}} \\,=\\, 0$）。对于每一步的每个方向，采取能沿相应直线最小化 $f$ 的精确线搜索步长，并将得到的步后点表示为 $w_{k}^{\\mathrm{SD}}$ 和 $w_{k}^{\\mathrm{CG}}$。将步骤 $k$ 的次优性定义为\n$$s_{k} \\;=\\; f\\!\\big(w_{k}^{\\mathrm{SD}}\\big) \\;-\\; f\\!\\big(w_{k}^{\\mathrm{CG}}\\big).$$\n\n精确计算 $s_{1}$ 和 $s_{2}$。使用精确分数将你的最终答案以单个行向量 $\\big(s_{1} \\;\\; s_{2}\\big)$ 的形式报告。不包括单位，也不要进行四舍五入。", "solution": "问题陈述具有科学依据、适定且客观。它描述了一个标准的二次优化任务，对于该任务，所指定的数值算法——最速下降法 (SD) 和共轭梯度法 (CG)——是定义明确的。我们接下来进行正式推导。\n\n要最小化的目标函数由下式给出\n$$f(w) = \\frac{1}{2} w^{\\top} \\Sigma w - \\mu^{\\top} w$$\n该函数的梯度为\n$$\\nabla f(w) = \\Sigma w - \\mu$$\n初始点为 $w_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。初始梯度为 $\\nabla f(w_0) = \\Sigma w_0 - \\mu = -\\mu = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$。\n残差向量定义为 $r = -\\nabla f(w)$。初始残差为 $r_0 = -\\nabla f(w_0) = \\mu = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n\n从迭代点 $w_k$ 沿方向 $d_k$ 进行精确线搜索的最优步长 $\\alpha$ 由最小化 $f(w_k + \\alpha d_k)$ 的公式给出：\n$$\\alpha_k = -\\frac{\\nabla f(w_k)^{\\top} d_k}{d_k^{\\top} \\Sigma d_k}$$\n\n步骤 $k=1$：\n\n对于 SD 和 CG，初始搜索方向都是负梯度：\n$$d_1^{\\mathrm{SD}} = d_1^{\\mathrm{CG}} = -\\nabla f(w_0) = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n两种方法的步长 $\\alpha_1$ 是相同的：\n$$\\alpha_1 = -\\frac{\\nabla f(w_0)^{\\top} d_1}{d_1^{\\top} \\Sigma d_1} = \\frac{r_0^{\\top} r_0}{r_0^{\\top} \\Sigma r_0}$$\n我们计算各项：\n$$r_0^{\\top} r_0 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n$$r_0^{\\top} \\Sigma r_0 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\n步长为 $\\alpha_1 = \\frac{1}{2}$。\n新的迭代点是相同的：\n$$w_1^{\\mathrm{SD}} = w_1^{\\mathrm{CG}} = w_0 + \\alpha_1 d_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n我们将这个共同的迭代点记为 $w_1$。此时目标函数的值为：\n$$f(w_1) = \\frac{1}{2} w_1^{\\top} \\Sigma w_1 - \\mu^{\\top} w_1 = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2}  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n$$f(w_1) = \\frac{1}{2} \\begin{pmatrix} 1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) - \\frac{1}{2} = \\frac{1}{4} - \\frac{1}{2} = -\\frac{1}{4}$$\n由于 $f(w_1^{\\mathrm{SD}}) = f(w_1^{\\mathrm{CG}}) = -\\frac{1}{4}$，步骤 1 的次优性为：\n$$s_1 = f(w_1^{\\mathrm{SD}}) - f(w_1^{\\mathrm{CG}}) = 0$$\n\n步骤 $k=2$：\n\n我们从共同的迭代点 $w_1 = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$ 开始。该点的梯度为：\n$$\\nabla f(w_1) = \\Sigma w_1 - \\mu = \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\n新的残差为 $r_1 = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n\n对于 SD 方法，搜索方向为 $d_2^{\\mathrm{SD}} = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$。\n步长为 $\\alpha_2^{\\mathrm{SD}} = \\frac{r_1^{\\top} r_1}{r_1^{\\top} \\Sigma r_1}$。\n分子：$r_1^{\\top} r_1 = (0)^2 + (-\\frac{1}{2})^2 = \\frac{1}{4}$。\n分母：$r_1^{\\top} \\Sigma r_1 = \\begin{pmatrix} 0  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} = \\frac{3}{4}$。\n$\\alpha_2^{\\mathrm{SD}} = \\frac{1/4}{3/4} = \\frac{1}{3}$。\nSD 迭代点为 $w_2^{\\mathrm{SD}} = w_1 + \\alpha_2^{\\mathrm{SD}} d_2^{\\mathrm{SD}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n函数值为 $f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix} - \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$。\n$f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1-\\frac{1}{6} \\\\ \\frac{1}{2}-\\frac{3}{6} \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2}\\left(\\frac{5}{12}\\right) - \\frac{1}{2} = -\\frac{7}{24}$。\n\n对于 CG 方法，搜索方向为 $d_2^{\\mathrm{CG}} = r_1 + \\beta_1 d_1^{\\mathrm{CG}}$，其中 $\\beta_1 = \\frac{r_1^{\\top}r_1}{r_0^{\\top}r_0}$。\n已知 $r_1^{\\top} r_1 = \\frac{1}{4}$ 和 $r_0^{\\top} r_0 = 1$，我们得到 $\\beta_1 = \\frac{1}{4}$。\n$$d_2^{\\mathrm{CG}} = r_1 + \\frac{1}{4}d_1^{\\mathrm{CG}} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix}$$\n步长为 $\\alpha_2^{\\mathrm{CG}} = -\\frac{\\nabla f(w_1)^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}} = \\frac{r_1^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}}$。\n分子：$r_1^{\\top} d_2^{\\mathrm{CG}} = \\begin{pmatrix} 0  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\frac{1}{4}$。\n分母：$(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4}  -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{5}{4} \\end{pmatrix} = \\frac{5}{8}$。\n$\\alpha_2^{\\mathrm{CG}} = \\frac{1/4}{5/8} = \\frac{2}{5}$。\nCG 迭代点为 $w_2^{\\mathrm{CG}} = w_1 + \\alpha_2^{\\mathrm{CG}} d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{2}{5} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}+\\frac{1}{10} \\\\ -\\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{1}{5} \\end{pmatrix}$。\n这正是精确的最小值点 $w^* = \\Sigma^{-1}\\mu$，与在 $N=2$ 维空间中对 CG 方法的预期相符。\n最小函数值为 $f(w_2^{\\mathrm{CG}}) = f(w^*) = -\\frac{1}{2}\\mu^\\top w^* = -\\frac{1}{2}\\begin{pmatrix} 1  0 \\end{pmatrix}\\begin{pmatrix} 3/5 \\\\ -1/5 \\end{pmatrix} = -\\frac{3}{10}$。\n\n步骤 2 的次优性为：\n$$s_2 = f(w_2^{\\mathrm{SD}}) - f(w_2^{\\mathrm{CG}}) = -\\frac{7}{24} - \\left(-\\frac{3}{10}\\right) = -\\frac{7}{24} + \\frac{3}{10} = \\frac{-35 + 36}{120} = \\frac{1}{120}$$\n\n最终值为 $s_1 = 0$ 和 $s_2 = \\frac{1}{120}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0  \\frac{1}{120} \\end{pmatrix}\n}\n$$", "id": "2382887"}, {"introduction": "共轭梯度法的一个显著理论特性是其收敛速度与系统矩阵的特征值谱密切相关。这个练习构建了一个特殊的协方差矩阵，它只有两个不同的特征值，旨在揭示这一深刻联系。您将证明CG方法在这种情况下仅需两步即可精确收敛，并进一步将这种矩阵结构与金融中的单因子资产定价模型联系起来，从而将抽象的数值属性与具体的经济直觉联系起来。", "problem": "考虑一个四资产均值-方差模型，其中收益向量的协方差矩阵 $\\Sigma \\in \\mathbb{R}^{4 \\times 4}$ 由下式给出\n$$\n\\Sigma \\;=\\; I_{4} \\;+\\; v v^{\\top}, \\quad v \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n设超额收益向量为 $b \\in \\mathbb{R}^{4}$，由下式给出\n$$\nb \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n你需要求解线性系统\n$$\n\\Sigma x \\;=\\; b\n$$\n使用共轭梯度（CG）法，从 $x_{0} = 0_{4}$ 开始，并假设全程使用精确计算。\n\n任务：\n- 证明 $\\Sigma$ 恰好有两个不同的特征值，并且是对称正定矩阵。\n- 证明对于该系统，在精确计算下，共轭梯度法在两次迭代后精确终止，并计算满足 $\\Sigma x = b$ 的收敛解向量 $x$。\n- 简要解释 $\\Sigma$ 的特定形式对收益的联合分布所隐含的经济结构。\n\n仅报告收敛解向量 $x$ 作为你的最终数值答案，写成单行矩阵形式。无需四舍五入。", "solution": "首先将验证问题陈述的科学合理性和一致性。\n\n第 1 步：提取已知条件。\n- 协方差矩阵为 $\\Sigma = I_{4} + v v^{\\top}$，其中 $I_4$ 是 $4 \\times 4$ 单位矩阵。\n- 向量 $v$ 由 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 给出。\n- 超额收益向量为 $b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}$。\n- 待求解的线性系统是 $\\Sigma x = b$。\n- 求解方法是共轭梯度（CG）法。\n- 初始猜测是零向量，$x_{0} = 0_{4}$。\n- 计算假设为精确计算。\n\n任务是：\n1. 证明 $\\Sigma$ 恰好有两个不同的特征值，并且是对称正定（SPD）矩阵。\n2. 证明共轭梯度法在两次迭代后精确终止，并计算解向量 $x$。\n3. 解释 $\\Sigma$ 的形式所隐含的经济结构。\n\n第 2 步：使用提取的已知条件进行验证。\n该问题具有科学依据，是计算金融和数值线性代数中的一个标准问题。矩阵 $\\Sigma$ 是对单位矩阵的秩一更新，这是一种被充分理解的结构。共轭梯度法适用于矩阵为对称正定（SPD）的系统。为确认这一点，我们检查 $\\Sigma$ 的性质。\n\n对称性：$\\Sigma^{\\top} = (I_{4} + vv^{\\top})^{\\top} = I_{4}^{\\top} + (vv^{\\top})^{\\top} = I_{4} + (v^{\\top})^{\\top}v^{\\top} = I_{4} + vv^{\\top} = \\Sigma$。该矩阵是对称的。\n\n特征值：设 $u$ 是 $\\Sigma$ 的一个特征向量。特征值方程为 $(I_{4} + vv^{\\top})u = \\lambda u$，可重新整理为 $(vv^{\\top})u = (\\lambda - 1)u$。这表明 $u$ 也是秩一矩阵 $vv^{\\top}$ 的一个特征向量，其特征值为 $\\lambda - 1$。\n矩阵 $vv^{\\top}$ 最多有一个非零特征值。\n- 如果 $u$ 是 $v$ 的倍数，比如对于某个非零标量 $c$ 有 $u=cv$，那么 $vv^{\\top}(cv) = v(v^{\\top}cv) = c(v^{\\top}v)v$。$vv^{\\top}$ 对应的特征值为 $\\lambda_{vv^{\\top}} = v^{\\top}v = 1^2 + 1^2 + 0^2 + 0^2 = 2$。\n- 如果 $u$ 与 $v$ 正交（$v^{\\top}u = 0$），那么 $vv^{\\top}u = v(v^{\\top}u) = v(0) = 0$。特征值为 $0$。在 $\\mathbb{R}^{4}$ 中与 $v$ 正交的向量空间维度为 $4-1=3$。\n因此，$vv^{\\top}$ 的特征值为 $2$（重数为 $1$）和 $0$（重数为 $3$）。\n$\\Sigma = I_4 + vv^{\\top}$ 的特征值由 $\\lambda_{\\Sigma} = 1 + \\lambda_{vv^{\\top}}$ 给出。因此，$\\Sigma$ 的特征值为 $1+2=3$（重数为 $1$）和 $1+0=1$（重数为 $3$）。\n由于 $\\Sigma$ 恰好有两个不同的特征值（$1$ 和 $3$），并且它们都是正数，因此对称矩阵 $\\Sigma$ 是正定的。\n\n该问题是适定的、完整的和一致的。这是一个有效的问题。\n\n第 3 步：结论与行动。\n问题有效。我现在将提供解答。\n\n按照要求，问题分三部分解答。\n\n第 1 部分：$\\Sigma$ 的特征值和对称正定（SPD）性质。\n如在验证步骤中所证，矩阵 $\\Sigma = I_4 + vv^{\\top}$ 是对称的。其特征值为 $3$ 和 $1$。由于所有特征值均为正，$\\Sigma$ 是对称正定的。这证实了共轭梯度法是适用的。\n\n第 2 部分：共轭梯度终止和求解。\n共轭梯度法保证在最多 $k$ 次迭代内找到 $\\Sigma x = b$ 的精确解（假设精确计算），其中 $k$ 是 $\\Sigma$ 的不同特征值的数量。由于 $\\Sigma$ 恰好有两个不同的特征值，该方法将在两次迭代后精确终止。\n\n为了计算解 $x$，可以执行共轭梯度算法的两个步骤。然而，一种更有洞察力的方法是利用我们刚刚推导出的 $\\Sigma$ 的谱性质。这种方法更稳健，更不易出现计算错误。由于方法在两步内收敛且 $x_0=0$，解 $x$ 必须位于 Krylov 子空间 $\\mathcal{K}_{2}(\\Sigma, r_0)$ 中。该子空间由 $\\{r_0, \\Sigma r_0\\}$ 张成，其中 $r_0=b-\\Sigma x_0=b$。\n\n精确解由 $x = \\Sigma^{-1}b$ 给出。我们可以通过将 $b$ 分解到 $\\Sigma$ 的特征空间中来计算它。\n对应于 $\\lambda=3$ 的特征空间由向量 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ 张成。\n对应于 $\\lambda=1$ 的特征空间是 $v$ 所张成空间的正交补，即满足 $v^{\\top}u=0$ 的向量 $u$ 的集合。\n\n我们将 $b$ 分解为一个平行于 $v$ 的分量 $b_{\\parallel}$ 和一个正交于 $v$ 的分量 $b_{\\perp}$。\n$b = b_{\\parallel} + b_{\\perp}$。\n平行分量是 $b$ 在 $v$ 上的投影：\n$$\nb_{\\parallel} = \\frac{b^{\\top}v}{v^{\\top}v} v = \\frac{\\begin{pmatrix} 2  1  5  -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}}{\\begin{pmatrix} 1  1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}} v = \\frac{2(1)+1(1)}{1^2+1^2} v = \\frac{3}{2} v = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n正交分量是 $b_{\\perp} = b - b_{\\parallel}$：\n$$\nb_{\\perp} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n我们可以验证 $v^{\\top}b_{\\perp} = 1(1/2) + 1(-1/2) + 0 + 0 = 0$，所以 $b_{\\perp}$ 确实在对应 $\\lambda=1$ 的特征空间中。\n\n现在我们将 $\\Sigma^{-1}$ 应用于 $b = b_{\\parallel} + b_{\\perp}$。由于 $b_{\\parallel}$ 是对应 $\\lambda=3$ 的特征向量，而 $b_{\\perp}$ 是对应 $\\lambda=1$ 的特征向量：\n$$\nx = \\Sigma^{-1}b = \\Sigma^{-1}(b_{\\parallel} + b_{\\perp}) = \\Sigma^{-1}b_{\\parallel} + \\Sigma^{-1}b_{\\perp}\n$$\n$$\nx = \\frac{1}{3} b_{\\parallel} + \\frac{1}{1} b_{\\perp}\n$$\n代入向量：\n$$\nx = \\frac{1}{3} \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2+1/2 \\\\ 1/2-1/2 \\\\ 0+5 \\\\ 0-4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\n这就是精确解，共轭梯度算法在两步内找到此解。\n\n我们来验证一下解：\n$$\n\\Sigma x = (I_4 + vv^{\\top})x = x + v(v^{\\top}x)\n$$\n$$\nv^{\\top}x = \\begin{pmatrix} 1  1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} = 1(1) + 1(0) = 1.\n$$\n$$\n\\Sigma x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} 1+1 \\\\ 0+1 \\\\ 5+0 \\\\ -4+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} = b.\n$$\n解是正确的。\n\n第 3 部分：经济结构。\n资产收益的协方差矩阵 $\\Sigma = I_4 + vv^{\\top}$ 是单因素资产定价模型的典型特征。在此类模型中，资产 $i$ 的收益 $R_i$ 由其对共同市场因素 $F$ 的暴露度和资产特定的异质性冲击 $\\epsilon_i$ 来描述：$R_i = \\beta_i F + \\epsilon_i$。收益向量 $R$ 的协方差矩阵为 $\\text{Cov}(R) = (\\beta\\beta^{\\top})\\sigma_F^2 + D$，其中 $\\beta$ 是因子载荷向量，$\\sigma_F^2$ 是因子的方差，而 $D$ 是异质性方差的对角矩阵。\n\n在本问题中，$\\Sigma = vv^{\\top} + I_4$ 的结构意味着：\n1.  存在一个单位方差（$\\sigma_F^2=1$）的单一系统性风险因素。\n2.  因子暴露（载荷）向量为 $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$。这意味着只有前两种资产暴露于此共同风险因素，其载荷均为 $1$。资产 3 和资产 4 不受此因素影响。\n3.  异质性风险矩阵为 $D = I_4$。这意味着每种资产都有一个独立的、特定的风险成分，其方差为 $1$。\n\n因此，资产 1 和 2 之间的非零协方差 $(\\Sigma)_{12} = 1$ 完全由它们对共同因素的共同暴露度来解释。所有其他资产对都是不相关的，因为当 $i \\neq j$ 且 $\\{i,j\\} \\neq \\{1,2\\}$ 时，$(\\Sigma)_{ij} = 0$。资产 1 和 2 的总方差为 $(\\Sigma)_{11} = (\\Sigma)_{22} = 2$，由因子方差（$1^2 \\cdot 1 = 1$）和异质性方差（$1$）组成。资产 3 和 4 的总方差为 $(\\Sigma)_{33} = (\\Sigma)_{44} = 1$，这纯粹是异质性方差，因为它们的因子载荷为零。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  0  5  -4 \\end{pmatrix}}\n$$", "id": "2382876"}, {"introduction": "共轭梯度法的效率根植于其生成的搜索方向所满足的$A$-正交性。但这一关键属性有多脆弱？这个编码练习旨在通过一个主动的“破坏”实验，让您深入理解$A$-正交性的核心作用。您将通过在CG迭代过程中对一个搜索方向引入微小扰动，来观察其对算法收敛性的影响，从而获得对算法内部机制的直观认识。", "problem": "给定一个由惩罚均值-方差 Markowitz 投资组合模型产生的线性系统。考虑 $n$ 种资产，其收益向量为 $\\mu \\in \\mathbb{R}^n$，协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{n \\times n}$，该矩阵是对称正定 (SPD) 的。引入权重为 $\\eta  0$ 的软预算惩罚，得到以下无约束二次目标函数\n$$\nf(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2,\n$$\n其中 $\\mathbf{1} \\in \\mathbb{R}^n$ 是全一向量。一阶条件 $\\nabla f(x) = 0$ 可简化为以下 SPD 线性系统\n$$\nA x = b,\\quad\\text{with}\\quad A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top},\\quad b \\equiv \\mu + \\eta\\, \\mathbf{1}.\n$$\n您的任务是实现共轭梯度 (CG) 法来求解 $A x = b$，并演示扰动单个搜索方向如何破坏搜索方向的 $A$-正交性，并阻止算法在至多 $n$ 步内精确收敛。\n\n使用以下在计算金融学中具有代表性的明确指定实例：\n- 资产数量：$n = 6$。\n- 常数相关矩阵 $R \\in \\mathbb{R}^{6 \\times 6}$，相关系数为 $\\rho = 0.2$，定义为 $R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$。\n- 资产标准差 $s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]$；定义 $\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$。\n- 期望收益 $\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$。\n- 惩罚权重 $\\eta = 10^{-2}$。\n\n算法要求：\n- 实现从 $x_0 = 0$ 开始的共轭梯度 (CG) 法。您必须精确计算 $n$ 次迭代，不得提前终止，并使用在精确算术下能为 SPD 系统保持 $A$-共轭性的数学上合理的更新方式。\n- 实现一种机制来精确扰动一个搜索方向。具体来说，如果扰动迭代索引为 $k_{\\mathrm{perturb}} \\in \\{0,1,\\dots,n-1\\}$，标量扰动幅度为 $\\varepsilon \\geq 0$，则在迭代 $j = k_{\\mathrm{perturb}}$ 时，将当前搜索方向 $p_j$ 替换为\n$$\n\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1,\n$$\n其中 $e_1 = [1,0,\\dots,0]^{\\top} \\in \\mathbb{R}^n$ 是第一个标准基向量。如果 $k_{\\mathrm{perturb}}  0$ 或 $k_{\\mathrm{perturb}} \\ge n$ 或 $\\varepsilon = 0$，则不应施加扰动。\n- 将所有 $n$ 个搜索方向按其在 CG 中被使用的顺序列为矩阵 $P \\in \\mathbb{R}^{n \\times n}$ 的各列。\n\n对于每次运行，在精确 $n$ 次迭代后，计算以下两项：\n1. $n$ 次迭代后的残差范数，定义为 $\\lVert b - A x_n \\rVert_2$。\n2. 搜索方向的 $A$-正交性缺陷，定义为格拉姆矩阵 $G \\equiv P^{\\top} A P$ 的非对角线元素绝对值的最大值，即\n$$\n\\max_{i \\ne j} \\left| G_{ij} \\right|.\n$$\n\n测试套件：\n对上述固定的 $(A,b)$ 运行您的实现，使用以下五对参数 $(k_{\\mathrm{perturb}}, \\varepsilon)$：\n- 情况 1：$(-1, 0)$。\n- 情况 2：$(2, 1)$。\n- 情况 3：$(0, 1)$。\n- 情况 4：$(10, 1)$。\n- 情况 5：$(3, 10^{-3})$。\n\n最终输出格式：\n- 您的程序必须打印单行，其中包含一个扁平列表，每个测试用例含 2 个数字，顺序如下：对每个用例，首先是 $n$ 步后的残差范数，然后是 $A$-正交性缺陷。所有数字必须使用标准四舍五入精确到 8 位小数。\n- 具体而言，输出必须是以下形式的单行\n$$\n[\\text{res}_1,\\text{def}_1,\\text{res}_2,\\text{def}_2,\\dots,\\text{res}_5,\\text{def}_5],\n$$\n其中每个 $\\text{res}_i$ 和 $\\text{def}_i$ 都是小数点后恰好有 8 位数字的十进制数。", "solution": "该问题要求实现共轭梯度 (CG) 法来求解一个源自均值-方差投资组合优化问题的线性系统 $Ax=b$。核心任务是通过分析定向扰动的影响，来展示在搜索方向之间保持 $A$-正交性的重要性。\n\n首先，我们对问题陈述进行形式化验证。\n\n给定条件如下：\n- 目标函数：$f(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2$。\n- 线性系统：$A x = b$。\n- 系统矩阵：$A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top}$。\n- 右端向量：$b \\equiv \\mu + \\eta\\, \\mathbf{1}$。\n- 系统维度：$n = 6$。\n- 相关系数：$\\rho = 0.2$。\n- 相关矩阵：$R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$。\n- 资产标准差：$s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]^{\\top}$。\n- 协方差矩阵：$\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$。\n- 期望收益向量：$\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$。\n- 惩罚权重：$\\eta = 10^{-2}$。\n- CG 起始向量：$x_0 = \\mathbf{0}$。\n- 迭代次数：精确 $n=6$ 次。\n- 扰动规则：对于 $j = k_{\\text{perturb}}$，将搜索方向 $p_j$ 替换为 $\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1$。此规则在 $k_{\\mathrm{perturb}} \\in \\{0, 1, \\dots, n-1\\}$ 且 $\\varepsilon  0$ 时生效。\n- 度量指标：$\\lVert b - A x_n \\rVert_2$ 和 $\\max_{i \\ne j} \\left| (P^{\\top} A P)_{ij} \\right|$。\n\n该问题是有效的。它在科学上基于计算金融学和数值线性代数的既定原则。协方差矩阵 $\\Sigma$ 被构造为对称正定 (SPD) 矩阵。系统矩阵 $A = \\Sigma + \\eta \\mathbf{1}\\mathbf{1}^{\\top}$ 是一个 SPD 矩阵 $\\Sigma$ 与一个对称半正定矩阵 $\\eta \\mathbf{1}\\mathbf{1}^{\\top}$（因为 $\\eta = 10^{-2}  0$）之和，这确保了 $A$ 也是 SPD 的。因此，线性系统 $Ax=b$ 是适定的，并且 CG 法是一种合适且理论上可靠的求解方法。所有参数和算法要求都以足够的精度指定，并且是内部一致的。\n\n我们从构建系统组件开始。维度为 $n=6$。向量 $\\mu \\in \\mathbb{R}^6$ 和 $s \\in \\mathbb{R}^6$ 已给出。向量 $\\mathbf{1}$ 是 $\\mathbb{R}^6$ 中的全一向量，$I$ 是 $6 \\times 6$ 的单位矩阵。\n相关矩阵 $R$ 为：\n$$ R = (1-0.2)I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} = 0.8 I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n协方差矩阵 $\\Sigma$ 构造如下：\n$$ \\Sigma = \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) \\cdot R \\cdot \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) $$\n当 $\\eta = 0.01$ 时，系统矩阵 $A$ 和向量 $b$ 为：\n$$ A = \\Sigma + 0.01 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n$$ b = \\mu + 0.01 \\cdot \\mathbf{1} $$\n\n共轭梯度算法是一种迭代方法，通过生成一系列相互 $A$-正交（或共轭）的搜索方向 $\\{p_j\\}$（即对于 $i \\ne j$ 有 $p_i^{\\top} A p_j = 0$）来求解 $Ax=b$。标准算法流程如下，从 $x_0 = \\mathbf{0}$，$r_0 = b$ 和 $p_0 = r_0$ 开始：\n对于 $j = 0, 1, \\dots, n-1$：\n1. 计算步长：$\\alpha_j = \\frac{r_j^{\\top} r_j}{p_j^{\\top} A p_j}$\n2. 更新解：$x_{j+1} = x_j + \\alpha_j p_j$\n3. 更新残差：$r_{j+1} = r_j - \\alpha_j A p_j$\n4. 计算改进因子：$\\beta_j = \\frac{r_{j+1}^{\\top} r_{j+1}}{r_j^{\\top} r_j}$\n5. 更新搜索方向：$p_{j+1} = r_{j+1} + \\beta_j p_j$\n\n在精确算术中，此过程会生成一组 $A$-正交的搜索方向基，并在至多 $n$ 次迭代内找到精确解。这个问题的核心是破坏这一性质。在指定的迭代 $j=k_{\\text{perturb}}$，搜索方向 $p_j$ 被扰动：\n$$ \\tilde{p}_j = p_j + \\varepsilon \\lVert p_j \\rVert_2 e_1 $$\n其中 $e_1 = [1, 0, \\dots, 0]^{\\top}$。这个被扰动的方向 $\\tilde{p}_j$ 随后被用来替代 $p_j$ 进行 $x_{j+1}$ 和 $r_{j+1}$ 的更新。关键的是，后续的搜索方向 $p_{j+1}$ 是使用这个被扰动的方向构建的：\n$$ p_{j+1} = r_{j+1} + \\beta_j \\tilde{p}_j $$\n这个扰动破坏了共轭链。新的方向 $\\tilde{p}_j$ 通常不与之前的方向 $p_0, \\dots, p_{j-1}$ 相 $A$-正交。这个误差会传播下去，因为所有后续的方向都是在这个被污染的步骤上建立的。结果，执行的 $n$ 个搜索方向集合 $\\{p_0, \\dots, \\tilde{p}_j, \\dots, p_{n-1}\\}$ 不再是 $A$-正交基，因此在 $n$ 步内收敛的理论保证也就不复存在了。\n\n我们期望观察到：\n1. 对于未受扰动的运行（情况1和4），最终残差范数 $\\lVert b - A x_n \\rVert_2$ 和 $A$-正交性缺陷 $\\max_{i \\ne j} | (P^{\\top} A P)_{ij} |$ 将接近机器精度（接近于 $0$）。矩阵 $P^{\\top} A P$ 将几乎是完美的对角矩阵。\n2. 对于受扰动的运行（情况2、3和5），两个度量指标都将显著大于 $0$。这表明算法未能在 $n$ 步内收敛到精确解，并且搜索方向的底层共轭性质被破坏了。误差的大小将取决于扰动强度 $\\varepsilon$ 和迭代索引 $k_{\\text{perturb}}$。\n\n解决方案的实现首先根据问题数据定义系统矩阵。然后创建一个函数来执行 CG 算法。该函数包含了扰动指定搜索方向并存储所有使用过的方向的逻辑。在精确 $n=6$ 次迭代后，它从搜索方向矩阵中计算最终的残差范数和 $A$-正交性缺陷。对五个测试用例中的每一个都重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Sets up the portfolio optimization problem, runs the Conjugate Gradient (CG)\n    method for several test cases with and without perturbation, and\n    calculates the specified performance metrics.\n    \"\"\"\n    # Problem Constants and Data\n    n = 6\n    rho = 0.2\n    s = np.array([0.15, 0.20, 0.25, 0.30, 0.22, 0.18])\n    mu = np.array([0.08, 0.10, 0.12, 0.15, 0.11, 0.09])\n    eta = 1e-2\n\n    # Construct the linear system Ax = b\n    ones = np.ones((n, 1))\n    R = rho * (ones @ ones.T) + (1 - rho) * np.identity(n)\n    D = np.diag(s)\n    Sigma = D @ R @ D\n    A = Sigma + eta * (ones @ ones.T)\n    b = mu + eta * ones.flatten()\n\n    def run_cg_perturbed(A_mat, b_vec, k_perturb, epsilon, n_iter):\n        \"\"\"\n        Runs the Conjugate Gradient algorithm for n_iter steps with an\n        optional perturbation on a specified search direction.\n        \n        Args:\n            A_mat (np.ndarray): The system matrix (n x n).\n            b_vec (np.ndarray): The right-hand side vector (n,).\n            k_perturb (int): The index of the iteration to perturb.\n            epsilon (float): The magnitude of the perturbation.\n            n_iter (int): The number of iterations to run.\n\n        Returns:\n            tuple: A tuple containing:\n                - residual_norm (float): The L2 norm of the final residual.\n                - defect (float): The A-orthogonality defect.\n        \"\"\"\n        dim = A_mat.shape[0]\n        x = np.zeros(dim)\n        r = b_vec - A_mat @ x\n        p = r\n        rs_old = r.T @ r\n        \n        p_storage = []\n        e1 = np.zeros(dim)\n        e1[0] = 1.0\n\n        for j in range(n_iter):\n            p_effective = p\n            \n            # Apply perturbation if conditions are met\n            if j == k_perturb and epsilon > 0:\n                p_norm = np.linalg.norm(p)\n                perturbation = epsilon * p_norm * e1\n                p_effective = p + perturbation\n\n            p_storage.append(p_effective)\n            \n            Ap = A_mat @ p_effective\n            alpha = rs_old / (p_effective.T @ Ap)\n            \n            x = x + alpha * p_effective\n            r = r - alpha * Ap\n            \n            rs_new = r.T @ r\n            \n            beta = rs_new / rs_old\n            p = r + beta * p_effective\n            rs_old = rs_new\n\n        # 1. Calculate final residual norm\n        final_residual_norm = np.linalg.norm(b_vec - A_mat @ x)\n        \n        # 2. Calculate A-orthogonality defect\n        P = np.array(p_storage).T\n        G = P.T @ A_mat @ P\n        np.fill_diagonal(G, 0.0) # We only care about off-diagonal elements\n        orthogonality_defect = np.max(np.abs(G))\n        \n        return final_residual_norm, orthogonality_defect\n\n    test_cases = [\n        (-1, 0.0),    # Case 1: No perturbation (invalid index)\n        (2, 1.0),     # Case 2: Perturb p_2 with eps=1\n        (0, 1.0),     # Case 3: Perturb p_0 with eps=1\n        (10, 1.0),    # Case 4: No perturbation (index out of bounds)\n        (3, 1e-3),    # Case 5: Perturb p_3 with small eps\n    ]\n\n    results = []\n    for k_perturb, epsilon in test_cases:\n        res_norm, defect = run_cg_perturbed(A, b, k_perturb, epsilon, n)\n        results.append(f\"{res_norm:.8f}\")\n        results.append(f\"{defect:.8f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2382914"}]}