{"hands_on_practices": [{"introduction": "我们的第一个实践将深入探讨宏观经济学中的一个基石模型：新古典增长模型。在这个练习中，你将应用策略函数迭代（PFI）来求解一个代表性行为体的最优储蓄策略。这个模型的一个重要特征是包含了一个偶尔起作用的不等式约束[@problem_id:2419733]，这是一种在真实经济问题中常见的情形，而像PFI这样的数值方法可以优雅地处理它。", "problem": "考虑一个代表性代理人的无限期、确定性动态规划问题，该代理人选择下一期资本以最大化其生命周期效用。代理人对消费的效用由恒定相对风险厌恶 (CRRA) 效用函数 $u(c)$ 给出，当 $\\sigma \\neq 1$ 时定义为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，当 $\\sigma = 1$ 时定义为 $u(c) = \\log(c)$。资源约束为 $c + k' = A k^{\\alpha} + (1-\\delta) k$，其中 $k$ 是当期资本，$k'$ 是下一期资本，$A$ 是全要素生产率，$\\alpha$ 是资本份额，$\\delta$ 是折旧率。代理人以因子 $\\beta \\in (0,1)$ 对未来进行贴现。下一期资本受一个偶尔紧束的不等式约束 $k' \\ge \\bar{k}$ 的限制。可行性还要求非负消费，这意味着 $k' \\le A k^{\\alpha} + (1-\\delta) k$。资本的状态空间被限制在一个网格 $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$ 上，其中 $k_1 = \\bar{k}$ 且 $k_N = k_{\\max}$，该网格由 $N$ 个均匀分布的点组成。\n\n对于下面的每一组参数，计算稳态最优策略函数 $g: \\mathcal{K} \\to \\mathcal{K}$，该函数将每个网格点 $k \\in \\mathcal{K}$ 映射到同一网格上的一个最优选择 $k' = g(k)$，并满足约束 $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$。同时，定义以下诊断指标：\n\n- 诊断 1：下界约束为紧约束的网格点所占的比例，计算为满足 $g(k) \\le \\bar{k} + \\tau$ 的 $k \\in \\mathcal{K}$ 的比例，其中紧约束容差为 $\\tau = 10^{-10}$。\n\n- 诊断 2：中位数网格索引处的策略值，即 $g(k_m)$，其中 $m = \\lfloor (N-1)/2 \\rfloor + 1$ 是 $\\{1,2,\\dots,N\\}$ 中的中位数索引。\n\n- 诊断 3：在计算出的策略下，整个网格上的最大欧拉方程残差绝对值，对于每个 $k \\in \\mathcal{K}$ 定义为\n$$\nr(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|,\n$$\n其中 $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$，$u'(c)$ 是消费的边际效用，当 $\\sigma \\neq 1$ 时为 $u'(c) = c^{-\\sigma}$，当 $\\sigma = 1$ 时为 $u'(c) = \\frac{1}{c}$。需要报告的诊断 3 是 $\\max_{k \\in \\mathcal{K}} r(k)$。\n\n资本网格定义为在 $\\bar{k}$ 和 $k_{\\max}$ 之间（含两端）均匀分布的 $N$ 个点组成的集合 $\\mathcal{K}$。对于所有计算，在每个 $k \\in \\mathcal{K}$ 处使用可行性条件 $k' \\in [\\bar{k}, A k^{\\alpha} + (1-\\delta) k]$，并且仅在 $c  0$ 时评估 $u(c)$。\n\n测试集：\n- 测试 1：$(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$。\n- 测试 2：$(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$。\n- 测试 3：$(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$。\n\n您的程序必须为每个测试计算指定网格上的最优稳态策略函数 $g$，并按顺序为每个测试返回上述三个诊断指标，均为实数。最终输出格式必须将所有测试聚合为单行，包含一个用方括号括起来的逗号分隔列表，依次包含测试 1 的 3 个诊断指标、测试 2 的 3 个诊断指标以及测试 3 的 3 个诊断指标。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，\"[result1,result2,result3,...]\"）。", "solution": "用户提供了一个明确定义的计算宏观经济学问题。我将首先确认其有效性，然后提供一个完整的解决方案。\n\n### 第 1 步：提取已知条件\n\n- **效用函数**：当 $\\sigma \\neq 1$ 时为 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，当 $\\sigma = 1$ 时为 $u(c) = \\log(c)$。\n- **资源约束**：$c + k' = A k^{\\alpha} + (1-\\delta) k$。\n- **参数**：全要素生产率 $A$、资本份额 $\\alpha$、折旧率 $\\delta$、CRRA 系数 $\\sigma$ 和贴现因子 $\\beta \\in (0,1)$。\n- **状态和选择变量**：当期资本 $k$（状态），下一期资本 $k'$（选择）。\n- **约束**：$k' \\ge \\bar{k}$（下界）和 $k' \\le A k^{\\alpha} + (1-\\delta) k$（可行性）。要求 $c0$。\n- **状态空间**：一个离散网格 $\\mathcal{K} = \\{k_1, k_2, \\dots, k_N\\}$，它是一个从 $k_1 = \\bar{k}$ 到 $k_N = k_{\\max}$ 的包含 $N$ 个均匀分布点的集合。\n- **策略函数**：一个稳态映射 $g: \\mathcal{K} \\to \\mathcal{K}$，使得 $k' = g(k)$。\n- **诊断 1**：$g(k) \\le \\bar{k} + \\tau$（容差 $\\tau = 10^{-10}$）的网格点 $k \\in \\mathcal{K}$ 的比例。\n- **诊断 2**：中位数网格索引 $m = \\lfloor (N-1)/2 \\rfloor + 1$ 处的策略值 $g(k_m)$。\n- **诊断 3**：最大绝对欧拉方程残差，$\\max_{k \\in \\mathcal{K}} r(k)$，其中\n  $r(k) \\equiv \\left| u'(c(k)) - \\beta \\, u'(c(g(k))) \\left(\\alpha A \\, g(k)^{\\alpha-1} + 1 - \\delta \\right) \\right|$，\n  其中 $c(k) = A k^{\\alpha} + (1-\\delta)k - g(k)$，$u'(c)$ 是边际效用。\n- **测试用例**：\n  - 测试 1: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0)$\n  - 测试 2: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0)$\n  - 测试 3: $(\\alpha, \\beta, \\delta, A, \\sigma, \\bar{k}, N, k_{\\max}) = (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0)$\n\n### 第 2 步：使用提取的已知条件进行验证\n\n- **科学依据**：该问题描述了在离散时间、确定性设置下的标准新古典增长模型（也称为 Ramsey-Cass-Koopmans 模型）。这是宏观经济学中的一个基础模型。所有组成部分——CRRA 效用、柯布-道格拉斯生产技术、资本积累动态——都是标准的、经过严格建立的。该问题在科学上是合理的。\n- **适定性**：问题要求解一个贴现动态规划问题的稳态最优策略函数。状态空间 $\\mathcal{K}$ 是一个紧集，对于任何可行策略，效用函数都有界。根据压缩映射定理，存在唯一的稳态最优策略函数。该问题是适定的。\n- **客观性**：问题使用精确的数学定义和客观语言陈述。没有主观或含糊的术语。\n- **结论**：该问题不违反任何指定的无效性标准。它是一个完整、一致且可解的科学问题。\n\n### 第 3 步：判定与行动\n\n问题陈述是**有效的**。将提供一个解决方案。\n\n### 基于原则的解决方案设计\n\n该问题需要找到一个无限期动态规划问题的稳态最优策略函数 $g(k)$。此问题的贝尔曼方程为：\n$$\nV(k) = \\max_{k' \\in \\mathcal{K} \\text{ and feasible}} \\left\\{ u(A k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k') \\right\\}\n$$\n其中 $V(k)$ 是价值函数。最优策略函数 $g(k)$ 是为每个 $k$ 给出能达到最大值的 $k'$ 的函数。\n\n我们将使用**策略函数迭代 (PFI)** 算法来求解策略函数 $g(k)$。这种迭代方法寻找贝尔曼算子的不动点，该不动点对应于稳态最优策略。\n\nPFI 算法按以下步骤进行：\n1.  **初始化**：\n    -   将资本网格 $\\mathcal{K}$ 定义为一个从 $\\bar{k}$ 到 $k_{\\max}$ 的包含 $N$ 个点的数组。\n    -   从一个策略函数的初始猜测 $g_0(k)$ 开始。一个简单且可行的初始策略是为所有 $k \\in \\mathcal{K}$ 选择下一期的最小可能资本，即 $g_0(k) = \\bar{k}$。\n\n2.  **迭代**：重复以下两个步骤，直到策略函数收敛。设当前迭代为 $j$。\n    a. **策略评估**：给定当前策略 $g_j(k)$，计算相应的价值函数 $V_j(k)$。该价值函数是以下线性函数方程的解：\n       $$\n       V_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k)) + \\beta V_j(g_j(k))\n       $$\n       这代表了一个关于 $N$ 个值 $\\{V_j(k_i)\\}_{i=1}^N$ 的包含 $N$ 个线性方程的系统。与其通过矩阵求逆（计算密集，$O(N^3)$）直接求解该系统，我们可以通过对固定策略下的价值函数进行迭代来找到 $V_j$：\n       $$\n       V_{j, s+1}(k) = R_j(k) + \\beta V_{j,s}(g_j(k))\n       $$\n       其中 $R_j(k) = u(A k^{\\alpha} + (1-\\delta)k - g_j(k))$ 是策略 $g_j$ 下的报酬。此内循环一直迭代，直到 $V_{j,s}$ 收敛到 $V_j$。\n\n    b. **策略改进**：利用评估步骤得到的价值函数 $V_j$，通过为每个状态 $k \\in \\mathcal{K}$ 求解最大化问题来找到一个改进的策略 $g_{j+1}(k)$：\n       $$\n       g_{j+1}(k) = \\underset{k' \\in \\mathcal{K} \\text{ s.t. } \\bar{k} \\le k'  Y(k)}{\\arg\\max} \\left\\{ u(Y(k) - k') + \\beta V_j(k') \\right\\}\n       $$\n       其中 $Y(k) = A k^{\\alpha} + (1-\\delta)k$ 是总可用资源。最优 $k'$ 的搜索是在所有满足可行性约束的网格点上进行的。严格不等式 $k'  Y(k)$ 确保消费始终为正，这是对数和 CRRA 效用函数的要求。\n\n3.  **收敛**：当策略函数在两次迭代之间不再变化时，即对于所有 $k \\in \\mathcal{K}$ 都有 $g_{j+1}(k) = g_j(k)$，算法终止。得到的策略即为稳态最优策略 $g(k)$。\n\n一旦计算出最优策略函数 $g(k)$，三个诊断指标按如下方式计算：\n- **诊断 1**：资本下界为紧约束的网格点比例。由于策略选择 $g(k)$ 必须在网格 $\\mathcal{K}$ 上，且第一个网格点是 $\\bar{k}$，因此对于一个小的 $\\tau$，$g(k) \\le \\bar{k} + \\tau$ 的条件等价于 $g(k) = \\bar{k}$。我们计算 $g(k) = \\bar{k}$ 的状态 $k \\in \\mathcal{K}$ 的比例。\n- **诊断 2**：策略函数在中位数资本网格点 $k_m$ 处的值。问题中的索引 $m$ 是基于 1 的，因此对于一个大小为 $N$ 的 0 索引数组，这对应于索引 $\\lfloor(N-1)/2\\rfloor$。我们报告 $g(k_{\\lfloor(N-1)/2\\rfloor})$。\n- **诊断 3**：整个网格上的最大绝对欧拉方程误差。对于每个状态 $k$，我们计算消费 $c(k) = Y(k) - g(k)$ 和下一期的消费 $c(g(k)) = Y(g(k)) - g(g(k))$。然后，我们使用提供的公式计算残差 $r(k)$，并找到在所有 $k \\in \\mathcal{K}$ 上的最大值。这可作为我们计算的策略准确性的度量。\n\n这种基于原则的方法保证能找到离散最优策略，并允许直接计算所需的诊断指标。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the dynamic programming problem for all test cases\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.01, 200, 5.0),\n        (0.33, 0.96, 0.08, 1.0, 2.0, 0.50, 200, 5.0),\n        (0.35, 0.94, 0.10, 0.90, 1.0, 1.50, 200, 5.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        alpha, beta, delta, A, sigma, k_bar, N, k_max = params\n        diagnostics = compute_policy_and_diagnostics(\n            alpha, beta, delta, A, sigma, k_bar, int(N), k_max\n        )\n        results.extend(diagnostics)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_policy_and_diagnostics(alpha, beta, delta, A, sigma, k_bar, N, k_max):\n    \"\"\"\n    Computes the stationary optimal policy function using Policy Function Iteration\n    and calculates the required diagnostics.\n    \"\"\"\n    # 1. Setup the economic environment\n    k_grid = np.linspace(k_bar, k_max, N)\n\n    # Define utility and marginal utility functions based on sigma\n    if sigma == 1.0:\n        u = lambda c: np.log(c)\n        u_prime = lambda c: 1.0 / c\n    else:\n        u = lambda c: (c**(1.0 - sigma)) / (1.0 - sigma)\n        u_prime = lambda c: c**(-sigma)\n\n    # 2. Policy Function Iteration (PFI)\n    policy_idx = np.zeros(N, dtype=int)  # Initial policy: g(k) = k_bar for all k\n    \n    max_pfi_iter = 200\n    pfi_tol = 1e-9\n\n    V = np.zeros(N) # Initialize value function\n\n    for pfi_iter in range(max_pfi_iter):\n        # --- Policy Evaluation ---\n        # Iteratively find the value function for the current policy\n        max_v_iter = 5000\n        v_tol = 1e-9\n        \n        k_prime_eval = k_grid[policy_idx]\n        c_eval = A * k_grid**alpha + (1.0 - delta) * k_grid - k_prime_eval\n        \n        # Consumption must be positive\n        R = np.full(N, -np.inf)\n        positive_c_mask = c_eval > 0\n        R[positive_c_mask] = u(c_eval[positive_c_mask])\n        \n        for v_iter in range(max_v_iter):\n            V_next = R + beta * V[policy_idx]\n            if np.max(np.abs(V_next - V))  v_tol:\n                break\n            V = V_next\n        else: # This 'else' belongs to the for-loop, runs if no 'break'\n            pass # Convergence not reached, but proceed with current V\n\n        # --- Policy Improvement ---\n        new_policy_idx = np.zeros(N, dtype=int)\n        y = A * k_grid**alpha + (1.0 - delta) * k_grid\n\n        for i in range(N):\n            # Find feasible choices for k' (must allow for positive consumption)\n            # k_prime  y[i]\n            valid_k_prime_end_idx = np.searchsorted(k_grid, y[i], side='left')\n\n            if valid_k_prime_end_idx == 0:\n                # No feasible choice leads to positive consumption\n                new_policy_idx[i] = 0 # Default to k_bar\n                continue\n\n            choice_indices = np.arange(valid_k_prime_end_idx)\n            k_prime_choices = k_grid[choice_indices]\n            c_choices = y[i] - k_prime_choices\n            \n            value_choices = u(c_choices) + beta * V[choice_indices]\n            \n            best_choice_local_idx = np.argmax(value_choices)\n            new_policy_idx[i] = choice_indices[best_choice_local_idx]\n\n        # --- Convergence Check ---\n        if np.max(np.abs(new_policy_idx - policy_idx))  1:\n            policy_idx = new_policy_idx\n            break\n        \n        policy_idx = new_policy_idx\n\n    # 3. Calculate Diagnostics\n    # Final policy g(k) = k'\n    g_k = k_grid[policy_idx]\n    \n    # Diagnostic 1: Fraction of grid points where the lower bound is binding\n    # Since g(k) is on the grid and k_bar is the first grid point,\n    # g(k) = k_bar + tau is equivalent to g(k) == k_bar.\n    diag1 = np.mean(policy_idx == 0)\n    \n    # Diagnostic 2: Policy value at the median grid index\n    # Problem: m = floor((N-1)/2) + 1 (1-based), Python: (N-1)//2 (0-based)\n    median_idx = (N - 1) // 2\n    diag2 = g_k[median_idx]\n    \n    # Diagnostic 3: Maximum absolute Euler equation residual\n    c = A * k_grid**alpha + (1.0 - delta) * k_grid - g_k\n    \n    # Find g(g(k))\n    g_of_g_k_policy_idx = policy_idx[policy_idx]\n    g_of_g_k = k_grid[g_of_g_k_policy_idx]\n    \n    # Consumption at the next state, c' = c(g(k))\n    c_prime = A * g_k**alpha + (1.0 - delta) * g_k - g_of_g_k\n    \n    # Ensure all consumptions are positive before taking logs or negative powers\n    if np.any(c = 0) or np.any(c_prime = 0):\n        # This indicates a problem, as feasible policies should yield c > 0.\n        # However, to prevent crashes, we'll signal an error with a large residual.\n        # In a well-behaved model, this branch is not taken.\n        return diag1, diag2, np.inf\n\n    u_prime_c = u_prime(c)\n    u_prime_c_prime = u_prime(c_prime)\n    \n    # Marginal return on capital (k')\n    return_on_k_prime = alpha * A * g_k**(alpha - 1.0) + (1.0 - delta)\n    \n    residuals = np.abs(u_prime_c - beta * u_prime_c_prime * return_on_k_prime)\n    diag3 = np.max(residuals)\n    \n    return diag1, diag2, diag3\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2419733"}, {"introduction": "在掌握了确定性模型的基础上，我们现在引入不确定性来增加模型的真实感。现实世界的经济体常常受到生产率冲击的影响，这个练习[@problem_id:2419647]将展示如何将策略函数迭代应用于由马尔可夫链描述的随机环境中。你将学习如何在策略评估和改进步骤中处理期望值，并最终计算出系统在最优策略下的长期稳态分布。", "problem": "给定一个具有离散状态和行动空间的平稳无限期动态规划最优资本积累问题。状态是一个序对 $(k,z)$，其中 $k$ 是当期资本，$z$ 是当期全要素生产率。选择是下一期的资本 $k'$。每期效用为具有常数相对风险厌恶（CRRA）的 $u(c)$，资源约束为 $c = z k^{\\alpha} + (1-\\delta)k - k'$。跨期目标是贝尔曼方程的唯一不动点\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\},\n$$\n其中 $\\mathcal{K}$ 是一个有限的资本选择网格，$z$ 服从一个具有转移矩阵 $P$ 的有限状态时间齐次马尔可夫链。效用函数为当 $c  0$ 时 $u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，当 $c \\le 0$ 时 $u(c) = -10^{12}$（一个很大的负数以表示不可行）。期望 $\\mathbb{E}[V(k',z') \\mid z]$ 是关于下一期生产率 $z'$ 在当期 $z$ 条件下的期望，使用给定的马尔可夫转移矩阵 $P$ 计算。行动空间被限制在网格 $\\mathcal{K}$ 上。\n\n状态空间是离散的：$z$ 在有限集 $\\mathcal{Z} = \\{z_1,\\dots,z_S\\}$ 中取值，$k$ 在网格 $\\mathcal{K} = \\{k_1,\\dots,k_N\\}$ 中取值，其中 $k_1 = \\underline{k}$，$k_N = \\overline{k}$，并且网格在这些边界之间均匀分布。马尔可夫转移矩阵 $P$ 是一个 $S \\times S$ 的行随机矩阵，其元素为 $P_{ij} = \\Pr(z' = z_j \\mid z = z_i)$。\n\n对于下面的每一组参数，计算：\n1. 在给定网格上求解贝尔曼方程的最优平稳策略 $\\pi(k,z) \\in \\mathcal{K}$，平局规则为：如果多个 $k'$ 达到最大值，则选择 $\\mathcal{K}$ 中最小的那个 $k'$。\n2. 与 $\\pi$ 相关联的对应价值函数 $V(k,z)$。\n3. 由 $(\\pi,P)$ 在联合状态空间 $\\mathcal{K} \\times \\mathcal{Z}$ 上引出的马尔可夫转移矩阵 $Q$，其元素为 $Q\\left((k,z),(k',z')\\right) = \\mathbf{1}\\{k' = \\pi(k,z)\\} \\cdot P(z,z')$。\n4. 在 $\\mathcal{K} \\times \\mathcal{Z}$ 上的唯一平稳分布 $\\mu$，满足 $\\mu = \\mu Q$，通过将 $Q$ 反复应用于 $\\mathcal{K} \\times \\mathcal{Z}$ 上的初始均匀分布直至极限得到。\n5. 在平稳分布下的期望资本 $\\mathbb{E}_{\\mu}[k]$ 和期望消费 $\\mathbb{E}_{\\mu}[c]$，其中 $c(k,z) = z k^{\\alpha} + (1-\\delta)k - \\pi(k,z)$。\n6. 计算解处的贝尔曼残差，定义为\n$$\nR = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{z' \\in \\mathcal{Z}} P(z,z') V(k',z') \\right\\} \\right) - V(k,z) \\right|.\n$$\n\n您的程序必须为测试套件中的每一组参数计算上述内容。对于每一组参数，生成一个包含三个实数的列表：$[\\mathbb{E}_{\\mu}[k], \\mathbb{E}_{\\mu}[c], R]$，每个条目四舍五入到 $6$ 位小数。最终的程序输出必须是一行，包含所有测试用例的这些列表的串联，形式为单个逗号分隔的列表，并用方括号括起来，例如 $[x_1,x_2,x_3,x_4,x_5,x_6,\\dots]$。\n\n使用以下参数集测试套件：\n- 测试用例 $1$ (基准):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.08$, $\\sigma = 2.0$。\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$，转移矩阵为 $P = \\begin{bmatrix}0.9  0.1 \\\\ 0.1  0.9\\end{bmatrix}$。\n  - 资本网格 $\\mathcal{K}$：从 $\\underline{k} = 0.001$ 到 $\\overline{k} = 5.0$ 的 $N = 200$ 个均匀分布的点。\n- 测试用例 $2$ (高耐心):\n  - $\\alpha = 0.35$, $\\beta = 0.995$, $\\delta = 0.08$, $\\sigma = 2.0$。\n  - $\\mathcal{Z} = \\{0.9, 1.1\\}$，转移矩阵为 $P = \\begin{bmatrix}0.9  0.1 \\\\ 0.1  0.9\\end{bmatrix}$。\n  - 资本网格 $\\mathcal{K}$：从 $\\underline{k} = 0.001$ 到 $\\overline{k} = 5.0$ 的 $N = 200$ 个均匀分布的点。\n- 测试用例 $3$ (低波动性，不同曲率和折旧):\n  - $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.10$, $\\sigma = 3.0$。\n  - $\\mathcal{Z} = \\{1.0, 1.0\\}$，转移矩阵为 $P = \\begin{bmatrix}0.5  0.5 \\\\ 0.5  0.5\\end{bmatrix}$。\n  - 资本网格 $\\mathcal{K}$：从 $\\underline{k} = 0.001$ 到 $\\overline{k} = 4.0$ 的 $N = 180$ 个均匀分布的点。\n\n角度单位不适用。不涉及物理单位。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，条目排序为 $[\\mathbb{E}_{\\mu}[k]^{(1)},\\mathbb{E}_{\\mu}[c]^{(1)},R^{(1)},\\mathbb{E}_{\\mu}[k]^{(2)},\\mathbb{E}_{\\mu}[c]^{(2)},R^{(2)},\\mathbb{E}_{\\mu}[k]^{(3)},\\mathbb{E}_{\\mu}[c]^{(3)},R^{(3)}]$，其中上标表示测试用例索引。每个数值条目必须四舍五入到 $6$ 位小数。", "solution": "所呈现的问题是一个典型的平稳无限期动态规划问题，具体来说，是随机最优增长模型的一个离散化版本。任务是找到最优储蓄策略并描述系统的长期行为。该问题是适定的；在给定参数下，贝尔曼算子是一个收缩映射，这保证了价值函数和相关最优平稳策略的存在性和唯一性。我们将使用策略函数迭代（PFI）算法来解决此问题。\n\n系统的状态是一个序对 $(k,z)$，其中 $k \\in \\mathcal{K}$ 是资本存量，$z \\in \\mathcal{Z}$ 是生产率水平。代理人选择下一期的资本存量 $k' \\in \\mathcal{K}$ 以最大化未来效用的贴现和。目标是找到作为贝尔曼方程唯一不动点的价值函数 $V(k,z)$：\n$$\nV(k,z) = \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\, \\mathbb{E}\\left[ V(k',z') \\mid z \\right] \\right\\}\n$$\n效用函数是常数相对风险厌恶（CRRA）形式，$u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma}$，并对非正消费施以巨大的惩罚。期望是关于生产率 $z$ 的马尔可夫过程计算的，即 $\\mathbb{E}[V(k',z') \\mid z_i] = \\sum_{j=1}^S P_{ij} V(k', z_j)$。\n\n策略函数迭代（PFI）算法是一个迭代过程，通过交替执行以下两个步骤来找到最优策略 $\\pi(k,z)$：\n\n1.  **策略评估**：对于一个给定的策略函数 $\\pi_j$，我们计算无限期遵循此策略所关联的价值 $V^{\\pi_j}$。该价值函数是固定策略下贝尔曼算子定义的线性方程组的解：\n    $$\n    V^{\\pi_j}(k,z) = u\\left(z k^{\\alpha} + (1-\\delta)k - \\pi_j(k,z)\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(\\pi_j(k,z), z_{s'})\n    $$\n    这个系统可以通过矩阵求逆 $(I - \\beta M_{\\pi_j})^{-1} u_{\\pi_j}$ 来求解，其中 $M_{\\pi_j}$ 是由策略引出的转移矩阵；或者，对于大型状态空间更实用的方法是，迭代上述算子直到价值函数收敛。我们将在每个评估步骤中采用后一种迭代方法。\n\n2.  **策略改进**：给定评估步骤中的价值函数 $V^{\\pi_j}$，我们通过为每个状态 $(k,z)$ 解决单步最大化问题来找到一个新的、改进的策略 $\\pi_{j+1}$：\n    $$\n    \\pi_{j+1}(k,z) = \\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(z k^{\\alpha} + (1-\\delta)k - k'\\right) + \\beta \\sum_{s'=1}^S P(z, z_{s'}) V^{\\pi_j}(k', z_{s'}) \\right\\}\n    $$\n    应用指定的平局规则——在所有最优解中选择最小的 $k'$。\n\n该算法从一个初始策略猜测 $\\pi_0$ 开始，并重复这两个步骤，直到策略函数不再改变，即 $\\pi_{j+1} = \\pi_j$。此时，最优策略 $\\pi^*$ 及其对应的价值函数 $V^*$ 就被找到了。\n\n在获得收敛的策略 $\\pi^*$ 和价值函数 $V^*$ 后，我们计算所需的统计数据：\n\n-   **状态转移矩阵 $Q$**：构建联合离散状态空间 $\\mathcal{K} \\times \\mathcal{Z}$ 上的马尔可夫转移矩阵。其元素为 $Q((k,z),(k',z')) = \\mathbf{1}\\{k' = \\pi^*(k,z)\\} \\cdot P(z,z')$。这是一个 $(NS \\times NS)$ 矩阵，其中 $N=|\\mathcal{K}|$ 且 $S=|\\mathcal{Z}|$。\n\n-   **平稳分布 $\\mu$**：唯一的平稳分布 $\\mu$ 是一个长度为 $NS$ 的行向量，满足属性 $\\mu = \\mu Q$。它通过幂法计算得出：从一个初始均匀分布 $\\mu_0$ 开始，我们迭代 $\\mu_{m+1} = \\mu_m Q$ 直到收敛。\n\n-   **期望值**：将平稳分布 $\\mu$ 表示为每个状态 $(k_i, z_s)$ 的概率矩阵 $\\mu_{is}$，我们计算期望资本和期望消费：\n    $$\n    \\mathbb{E}_{\\mu}[k] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} k_i\n    $$\n    $$\n    \\mathbb{E}_{\\mu}[c] = \\sum_{i=1}^N \\sum_{s=1}^S \\mu_{is} \\left( z_s k_i^{\\alpha} + (1-\\delta)k_i - k_{\\pi^*(i,s)} \\right)\n    $$\n    其中 $k_{\\pi^*(i,s)}$ 是策略函数返回的索引所对应的资本水平。\n\n-   **贝尔曼残差 $R$**：此量度量了数值解的精度。它被定义为计算出的价值函数 $V^*$ 与再应用一次完整贝尔曼算子的结果之间的最大绝对差：\n    $$\n    R = \\max_{(k,z) \\in \\mathcal{K} \\times \\mathcal{Z}} \\left| \\left( \\max_{k' \\in \\mathcal{K}} \\left\\{ u\\left(c(k,z,k')\\right) + \\beta \\sum_{z'} P(z,z') V^*(k',z') \\right\\} \\right) - V^*(k,z) \\right|\n    $$\n\n该实现将使用向量化的 `NumPy` 操作来确保计算效率。平局规则由 `numpy.argmax` 自然处理，该函数返回第一个最大值的索引。", "answer": "```python\nimport numpy as np\n\ndef solve_model(alpha, beta, delta, sigma, z_grid, P, k_grid):\n    \"\"\"\n    Solves the optimal growth model using Policy Function Iteration.\n    \"\"\"\n    N = len(k_grid)\n    S = len(z_grid)\n    \n    # Utility function\n    # Note: The case sigma = 1 is not in test cases, so we ignore it.\n    def u(c):\n        utility = np.full(c.shape, -1e12)\n        positive_c = c > 0\n        utility[positive_c] = (c[positive_c]**(1 - sigma)) / (1 - sigma)\n        return utility\n\n    # --- Policy Function Iteration Setup ---\n    # V: value function (N, S)\n    # pi: policy function, stores index of k' (N, S)\n    V = np.zeros((N, S))\n    pi = np.zeros((N, S), dtype=int) \n\n    # PFI convergence parameters\n    max_pfi_iter = 200\n    pfi_iter = 0\n    policy_converged = False\n\n    # Policy Evaluation convergence parameters\n    max_eval_iter = 2000\n    eval_tol = 1e-9\n\n    # Pre-calculate consumption for all state-action pairs\n    # Shape: (N_k, N_z, N_k') -> (i, s, j)\n    k_matrix_i = k_grid[:, np.newaxis, np.newaxis]\n    z_matrix_s = z_grid[np.newaxis, :, np.newaxis]\n    k_matrix_j = k_grid[np.newaxis, np.newaxis, :]\n    \n    consumption = z_matrix_s * (k_matrix_i**alpha) + (1 - delta) * k_matrix_i - k_matrix_j\n    utility_all_choices = u(consumption)\n\n    # --- PFI Loop ---\n    while not policy_converged and pfi_iter  max_pfi_iter:\n        pfi_iter += 1\n        pi_old = pi.copy()\n\n        # 1. Policy Evaluation Step\n        V_eval = V.copy() # Use V from previous PFI step as starting point\n        for _ in range(max_eval_iter):\n            k_prime_indices = pi # Shape (N, S)\n            \n            # Expected value term: E[V(k', z')|z]\n            # V @ P.T gives E_z'[V(k, z')] for each k, z\n            EV = V_eval @ P.T # Shape (N, S)\n            \n            # Select the expected value for the policy-dictated k'\n            # EV[pi[i, s], s] for each (i,s)\n            expected_val_at_pi = EV[pi, np.arange(S)] # Shape (N, S)\n\n            # Consumption under current policy\n            k_prime_values = k_grid[k_prime_indices] # Shape (N, S)\n            c_at_pi = z_grid[None,:] * (k_grid[:,None]**alpha) + (1-delta)*k_grid[:,None] - k_prime_values\n            \n            # Value function update\n            V_new = u(c_at_pi) + beta * expected_val_at_pi\n            \n            if np.max(np.abs(V_new - V_eval))  eval_tol:\n                break\n            V_eval = V_new\n        V = V_eval\n\n        # 2. Policy Improvement Step\n        EV = V @ P.T # Expected value, shape (N,S)\n        \n        # RHS of Bellman equation for all choices\n        # utility_all_choices (N, S, N)\n        # EV.T (S, N) -> broadcast to (N, S, N)\n        rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n\n        # Find new policy and new value function\n        # argmax provides the tie-breaking rule (smallest index/k')\n        pi = np.argmax(rhs_bellman, axis=2)\n        \n        # Check for convergence\n        if np.array_equal(pi, pi_old):\n            policy_converged = True\n\n    if not policy_converged:\n        print(\"Warning: Policy function iteration did not converge.\")\n\n    # --- Post-convergence calculations ---\n    # Final value function from one VFI step on optimal policy\n    EV = V @ P.T\n    rhs_bellman = utility_all_choices + beta * EV.T[np.newaxis, :, :]\n    V_final = np.max(rhs_bellman, axis=2)\n\n    # Bellman Residual\n    residual = np.max(np.abs(V_final - V))\n\n    # Construct the transition matrix Q on the joint state space\n    NS = N * S\n    Q = np.zeros((NS, NS))\n    for i in range(N):\n        for s in range(S):\n            old_idx = s * N + i\n            policy_k_idx = pi[i, s]\n            for t in range(S):\n                new_idx = t * N + policy_k_idx\n                Q[old_idx, new_idx] = P[s, t]\n\n    # Stationary Distribution mu\n    mu = np.ones(NS) / NS\n    max_mu_iter = 5000\n    mu_tol = 1e-10\n    for _ in range(max_mu_iter):\n        mu_new = mu @ Q\n        if np.max(np.abs(mu_new - mu))  mu_tol:\n            mu = mu_new\n            break\n        mu = mu_new\n    \n    mu_matrix = mu.reshape((S, N)).T # Shape (N, S)\n\n    # Expected Capital\n    expected_k = np.sum(mu_matrix * k_grid[:, np.newaxis])\n\n    # Expected Consumption\n    k_prime_indices_opt = pi\n    k_prime_values_opt = k_grid[k_prime_indices_opt]\n    consumption_opt = z_grid[None,:] * (k_grid[:,None]**alpha) + \\\n                      (1 - delta) * k_grid[:,None] - k_prime_values_opt\n    expected_c = np.sum(mu_matrix * consumption_opt)\n    \n    return [round(expected_k, 6), round(expected_c, 6), round(residual, 6)]\n\ndef solve():\n    test_cases = [\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.995, \"delta\": 0.08, \"sigma\": 2.0,\n            \"z_grid\": np.array([0.9, 1.1]),\n            \"P\": np.array([[0.9, 0.1], [0.1, 0.9]]),\n            \"N\": 200, \"k_min\": 0.001, \"k_max\": 5.0\n        },\n        {\n            \"alpha\": 0.35, \"beta\": 0.96, \"delta\": 0.10, \"sigma\": 3.0,\n            \"z_grid\": np.array([1.0, 1.0]),\n            \"P\": np.array([[0.5, 0.5], [0.5, 0.5]]),\n            \"N\": 180, \"k_min\": 0.001, \"k_max\": 4.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_grid = np.linspace(case[\"k_min\"], case[\"k_max\"], case[\"N\"])\n        \n        result_triplet = solve_model(\n            case[\"alpha\"], case[\"beta\"], case[\"delta\"], case[\"sigma\"],\n            case[\"z_grid\"], case[\"P\"], k_grid\n        )\n        results.extend(result_triplet)\n    \n    # Format the output as per problem specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2419647"}, {"introduction": "这项高级练习展示了策略函数迭代的强大功能和通用性，即使在模型的潜在动态非常复杂且没有简单解析形式时也同样适用。我们将用一个确定性但高度非线性的函数——由一个小型神经网络定义——来取代简单的随机过程。通过使用神经网络来控制状态演化[@problem_id:2419687]，我们接触到了计算经济学与机器学习交叉的前沿领域，并实践了一种适用于确定性转移的精确策略评估技术。", "problem": "考虑一个在离散状态、离散行动设定下，通过策略函数迭代（Policy Function Iteration, PFI）求解的确定性无限期动态优化问题。代理人从一个固定的网格中选择下一期的资本，以最大化期望折现效用，其中外生生产率状态根据一个已知的人工神经网络（ANN）演化。该设定是纯数学和逻辑上的，可以在任何现代编程语言中实现。\n\n基本基础和模型定义：\n- 代理人最大化目标函数 $\\sum_{t=0}^{\\infty} \\beta^{t} u(c_{t})$，其中折现因子 $\\beta \\in (0,1)$，瞬时效用函数为 $u(c) = \\log(c)$（对于 $c  0$）。\n- 生产函数为 $f(k) = k^{\\alpha}$，其中 $\\alpha \\in (0,1)$。\n- 资本的运动定律遵循资源约束 $c + k' = z \\cdot f(k) + (1-\\delta) k$，其中 $\\delta \\in (0,1)$ 是折旧率，$k$ 是当期资本，$k'$ 是下一期资本，$z$ 是生产率状态。\n- 状态是序对 $(k, z)$，其中 $k$ 位于固定的资本网格 $\\mathcal{K}$ 上，$z$ 位于固定的生产率网格 $\\mathcal{Z}$ 上。\n- 生产率转移是确定性的，由一个已知的单隐藏层 ANN 控制，该网络将 $z$ 映射到 $z'$。设隐藏层为 $h(z) = \\tanh(W z + b)$，其中 $W \\in \\mathbb{R}^{2 \\times 1}$，$b \\in \\mathbb{R}^{2}$，输出为 $z' = 1 + c^{\\top} h(z)$，其中 $c \\in \\mathbb{R}^{2}$。常数如下：\n  - $W = \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix}$，$b = \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix}$，$c = \\begin{bmatrix} 0.06 \\\\ 0.03 \\end{bmatrix}$。\n  - 因此，$z' = 1 + \\begin{bmatrix} 0.06  0.03 \\end{bmatrix} \\tanh\\!\\left( \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix} z + \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix} \\right)$，其中 $\\tanh(\\cdot)$ 逐元素应用。\n- 生产率网格是 $\\mathcal{Z} = \\{ z_{j} \\}_{j=1}^{N_{z}}$，包含 $N_{z} = 21$ 个从 $0.9$ 到 $1.1$ 的等距点。资本网格是 $\\mathcal{K} = \\{ k_{i} \\}_{i=1}^{N_{k}}$，包含 $N_{k} = 61$ 个从 $0.5$ 到 $2.0$ 的等距点。\n- 从当前状态 $(k,z)$ 到下一状态的确定性转移分两步进行：\n  1. 给定 $z$，从 ANN 计算出 $z'$，然后将其映射到 $\\mathcal{Z}$ 中最近的网格点。\n  2. 给定所选的 $k' \\in \\mathcal{K}$，直接将其映射到该网格点索引。\n\n贝尔曼方程和 PFI 框架：\n- 令 $V(k,z)$ 为价值函数，$g(k,z) \\in \\mathcal{K}$ 为一个将当前状态映射到下一期资本的平稳策略。对于任意固定策略 $g$，定义单步回报为 $r(k,z; g) = u\\big(z k^{\\alpha} + (1-\\delta)k - g(k,z)\\big)$，当 $u(\\cdot)$ 的参数为严格正数时；否则，将 $r(k,z; g)$ 定义为一个非常大的负数（以强制执行可行性）。\n- 在固定策略下的贝尔曼方程为 $V(k,z) = r(k,z; g) + \\beta V(k', z')$，其中 $k' = g(k,z)$，$z'$ 是通过 ANN 映射并舍入到最近网格点的下一期生产率。在离散网格中，这产生了一个关于 $V$ 的线性方程组。\n- 策略函数迭代在以下两步之间交替进行：\n  1. 策略评估：求解由固定策略 $g$ 导出的线性系统以获得 $V$。\n  2. 策略改进：对于每个状态，选择 $k' \\in \\mathcal{K}$ 以最大化 $u\\big(z k^{\\alpha} + (1-\\delta)k - k'\\big) + \\beta V(k', z')$，其中 $z'$ 由 ANN 和最近网格映射给出。\n\n实现细节和要求：\n- 使用 $\\alpha = 0.33$。所有数值实体，如 $0.33$，都必须被视为精确的数值常数。\n- 对于不可行的消费（即 $z k^{\\alpha} + (1-\\delta)k - k' \\le 0$），分配一个很大的负回报以强制执行约束。\n- 通过利用固定策略的确定性转移结构来实现精确的策略评估。由策略导出的有向图将每个状态映射到一个唯一的下一状态。通过识别循环和汇入这些循环的树，并使用有限几何级数求解循环上的价值，然后沿着树向后传播，可以计算出精确的价值函数。\n- 通过短视最大化单步回报（在第一次改进中将续存价值设为零）或任何其他可行策略来初始化策略。迭代直至策略稳定（在所有状态上均无变化）或达到最大迭代次数。\n\n测试套件和待计算的答案：\n- 对每种参数情形，运行 PFI 直至收敛，并返回特定查询状态下的最优下一期资本选择 $g^{\\star}(k,z)$，以 $\\mathcal{K}$ 中的元素形式报告。\n- 所有情形的固定参数：$\\alpha = 0.33$，$\\delta = 0.10$，ANN 权重 $(W,b,c)$ 如上所述，以及资本网格 $\\mathcal{K}$ 从 $0.5$ 到 $2.0$（61个点）和生产率网格 $\\mathcal{Z}$ 从 $0.9$ 到 $1.1$（21个点）。\n- 各情形因折现因子 $\\beta$ 不同而异：\n  - 情形 A (标准情形): $\\beta = 0.95$。查询状态：$(k,z) = (1.0, 1.0)$, $(0.5, 0.9)$, $(2.0, 1.1)$。\n  - 情形 B (更具耐心): $\\beta = 0.98$。查询状态：$(k,z) = (1.0, 1.0)$, $(1.5, 1.1)$, $(0.6, 0.95)$。\n  - 情形 C (缺乏耐心): $\\beta = 0.90$。查询状态：$(k,z) = (1.0, 1.0)$, $(1.8, 0.9)$, $(0.7, 1.05)$。\n- 对于每个查询，使用 $\\mathcal{K}$ 和 $\\mathcal{Z}$ 中最接近的网格点来表示该状态。\n- 你的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表，顺序如下：情形 A 的三个结果，随后是情形 B 的三个结果，最后是情形 C 的三个结果。每个结果是收敛策略为相应查询状态选择的最优 $k'$，以浮点数形式表示（无单位）。例如，输出格式必须完全像：\"[x1,x2,x3,x4,x5,x6,x7,x8,x9]\"。", "solution": "所呈现的问题是一个确定性无限期动态优化问题，是计算宏观经济学中的一个标准表述。任务是使用策略函数迭代（Policy Function Iteration, PFI）算法求解最优策略函数。在尝试任何解决方案之前，对问题陈述进行批判性分析是强制性的第一步。\n\n### 步骤1：提取已知条件\n\n- **目标函数**：最大化 $\\sum_{t=0}^{\\infty} \\beta^{t} u(c_{t})$\n- **折现因子**：$\\beta \\in (0,1)$\n- **效用函数**：$u(c) = \\log(c)$，对于 $c  0$\n- **生产函数**：$f(k) = k^{\\alpha}$，其中 $\\alpha = 0.33$\n- **折旧率**：$\\delta = 0.10$\n- **资源约束**：$c + k' = z \\cdot f(k) + (1-\\delta) k$\n- **状态空间**：$(k, z)$，其中 $k \\in \\mathcal{K}$ 且 $z \\in \\mathcal{Z}$\n- **资本网格 ($\\mathcal{K}$)**：$N_{k} = 61$ 个从 $0.5$ 到 $2.0$ 的等距点\n- **生产率网格 ($\\mathcal{Z}$)**：$N_{z} = 21$ 个从 $0.9$ 到 $1.1$ 的等距点\n- **生产率转移**：确定性的，通过一个人工神经网络（ANN）。\n  - 隐藏层：$h(z) = \\tanh(W z + b)$\n  - 输出：$z' = 1 + c^{\\top} h(z)$\n  - ANN 参数：$W = \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix}$，$b = \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix}$，$c = \\begin{bmatrix} 0.06 \\\\ 0.03 \\end{bmatrix}$\n  - 网格映射：计算出的 $z'$ 被映射到 $\\mathcal{Z}$ 中最接近的点。\n- **算法**：策略函数迭代（PFI）。\n  - **策略评估**：通过识别由策略 $g$ 导出的状态转移图中的循环和树，精确求解线性系统 $V(s) = r(s, g(s)) + \\beta V(s')$。\n  - **策略改进**：对每个状态，通过最大化贝尔曼方程的右侧来更新策略：$\\arg\\max_{k' \\in \\mathcal{K}} \\left\\{ u(z k^{\\alpha} + (1-\\delta)k - k') + \\beta V(k', z') \\right\\}$。\n- **约束处理**：对非正消费水平分配一个非常大的负回报。\n- **初始化**：允许任何可行的策略，例如短视策略。\n- **测试情形**：\n  - 情形 A：$\\beta = 0.95$。查询状态 $(k,z)$: $(1.0, 1.0)$、$(0.5, 0.9)$、$(2.0, 1.1)$。\n  - 情形 B：$\\beta = 0.98$。查询状态 $(k,z)$: $(1.0, 1.0)$、$(1.5, 1.1)$、$(0.6, 0.95)$。\n  - 情形 C：$\\beta = 0.90$。查询状态 $(k,z)$: $(1.0, 1.0)$、$(1.8, 0.9)$、$(0.7, 1.05)$。\n- **查询状态处理**：查询状态被映射到其各自网格上最接近的点。\n\n### 步骤2：使用提取的已知条件进行验证\n\n- **科学依据**：该问题是一个新古典增长模型，是经济学中一个基础且成熟的框架。使用 ANN 来模拟外生冲击过程是一种现代但数学上严谨的扩展。该问题牢固地植根于动态优化和计算数学的原理。它是有效的。\n- **适定性**：该问题是适定的。目标函数是凹的，状态空间是紧凑的，并且折现因子 $\\beta$ 小于一。这些条件通过 Blackwell 充分条件保证了贝尔曼算子存在唯一的不动点，这对应于唯一的最优价值函数和相关的平稳最优策略。PFI 算法保证会收敛到此解。它是有效的。\n- **客观性**：该问题使用精确的数学和算法语言进行描述。所有参数都已用数值指定。没有主观或模糊的陈述。它是有效的。\n- **完整性和一致性**：该问题提供了解决所需的所有必要信息：模型参数、网格规格、转移动态（包括 ANN 架构和权重）以及要使用的算法。没有矛盾之处。它是有效的。\n- **现实性和可行性**：参数值在相关文献中是标准的。计算任务涉及一个 $61 \\times 21 = 1281$ 个状态的状态空间，使用现代计算资源是完全可行的。对于确定性转移函数，通过图遍历进行精确策略评估的要求是具体的，但在计算上是合理的。它是有效的。\n\n### 步骤3：结论与行动\n\n问题陈述是**有效的**。它是一个适定的、科学上合理的、完整的计算经济学问题。我们可以着手构建解决方案。\n\n### 算法解\n\n解决方案将通过遵循策略函数迭代（PFI）程序来实现。系统的状态由序对 $(k, z)$ 给出，我们将用整数索引 $(i, j)$ 来表示它们，分别对应于它们在资本网格 $\\mathcal{K}$ 和生产率网格 $\\mathcal{Z}$ 中的位置。状态总数为 $N = N_k \\times N_z$。\n\n**步骤1：初始化**\n首先，我们建立离散状态空间和状态转移机制。\n- 按规定构建资本网格 $\\mathcal{K}$ 和生产率网格 $\\mathcal{Z}$。\n- 预计算生产率转移映射。对于每个生产率网格点 $z_j \\in \\mathcal{Z}$，我们使用给定的 ANN 公式计算其后继 $z'$：\n  $$ z'_{\\text{raw}} = 1 + \\begin{bmatrix} 0.06  0.03 \\end{bmatrix} \\tanh\\!\\left( \\begin{bmatrix} 1.3 \\\\ -0.9 \\end{bmatrix} z_j + \\begin{bmatrix} 0.15 \\\\ -0.25 \\end{bmatrix} \\right) $$\n  然后，我们找到 $\\mathcal{Z}$ 中最接近 $z'_{\\text{raw}}$ 的网格点的索引 $j'$。这给了我们一个确定性映射 $j \\to j'$，我们将其存储在一个数组 `z_next_idx` 中。\n- 初始策略函数 $g_0(k_i, z_j)$ 对所有状态 $(i, j)$ 都设置为最低的资本水平 $k_0 \\in \\mathcal{K}$。这是一个简单且可行的起始策略。\n\n**步骤2：策略函数迭代循环**\n算法进行迭代，直到策略函数收敛。令 $g_t$ 为第 $t$ 次迭代时的策略。\n\n**步骤2a：策略评估**\n对于给定的策略 $g_t$，我们必须找到相应的价值函数 $V_{g_t}$（为简单起见记为 $V$），它解决了以下线性方程组：\n$$ V(k_i, z_j) = r(k_i, z_j; g_t) + \\beta V(k_{i'}, z_{j'}) $$\n其中 $k_{i'} = g_t(k_i, z_j)$ 并且 $z_{j'}$ 由从 $z_j$ 预计算的生产率转移确定。$r(\\cdot)$ 是单步回报 $\\log(c)$。\n\n由于在固定策略下的转移是确定性的，每个状态 $(i, j)$ 都映射到一个唯一的后继状态 $(i', j')$。这种结构定义了一个有向图，其中每个节点的出度恰好为一。这样的图由几个不相交的组件构成，每个组件都是一组汇入单个循环的树。\n\n$V$ 的解通过指定的基于图的方法找到：\n1.  **图构建**：对于每个状态索引 $s$，确定其唯一的后继状态索引 $s_{\\text{next}}$ 及其单步回报 $R_s = r(s, g_t(s))$。\n2.  **循环检测**：使用深度优先搜索（DFS），我们遍历图以识别所有循环。\n3.  **循环价值计算**：对于每个发现的长度为 $L$ 的循环，其节点为 $\\{s_1, \\dots, s_L\\}$，其中 $s_L$ 转移到 $s_1$：\n    循环中第一个节点 $s_1$ 的价值由一个折现几何级数的和给出：\n    $$ V(s_1) = \\frac{1}{1 - \\beta^L} \\sum_{m=0}^{L-1} \\beta^m R(s_{1+m})$$\n    其中回报 $R$ 对应于循环内的转移。一旦 $V(s_1)$ 已知，循环中所有其他节点 $V(s_2), \\dots, V(s_L)$ 的价值可以通过使用贝尔曼方程进行简单的回代求得，例如 $V(s_L) = R(s_L) + \\beta V(s_1)$。\n4.  **树的反向传播**：所有不在循环中的节点（即树上的节点）的价值，通过从其价值已知的节点（循环节点）开始向后计算。我们遍历所有价值未知的状态。如果一个状态 $s$ 的后继 $s_{\\text{next}}$ 的价值 $V(s_{\\text{next}})$ 已知，我们可以计算 $V(s) = R(s) + \\beta V(s_{\\text{next}})$。重复此过程，直到所有状态的价值都已计算完毕。\n\n**步骤2b：策略改进**\n计算出价值函数 $V_{g_t}$ 后，我们通过为每个状态 $(k_i, z_j)$ 最大化贝尔曼方程的右侧来找到一个新的、改进的策略 $g_{t+1}$：\n$$ g_{t+1}(k_i, z_j) = \\underset{k_m \\in \\mathcal{K}}{\\operatorname{argmax}} \\left\\{ \\log\\left(z_j k_i^{\\alpha} + (1-\\delta)k_i - k_m\\right) + \\beta V_{g_t}(k_m, z_{j'}) \\right\\} $$\n其中 $z_{j'}$ 是 $z_j$ 的后继。最大化是在网格 $\\mathcal{K}$ 中所有可能的下一期资本选择 $k_m$ 上进行的。如果消费 $c = z_j k_i^{\\alpha} + (1-\\delta)k_i - k_m$ 不严格为正，则效用取为 $-\\infty$，从而有效地强制执行消费的非负约束。\n\n**步骤3：收敛**\n将新策略 $g_{t+1}$ 与旧策略 $g_t$进行比较。如果对于所有状态 $s$，都有 $g_{t+1}(s) = g_t(s)$，则策略已收敛，算法终止。所得策略 $g_{t+1}$ 即为最优平稳策略 $g^*$。否则，我们设置 $g_t \\leftarrow g_{t+1}$ 并返回步骤2a。\n\n**步骤4：报告答案**\n一旦获得最优策略 $g^*$，我们就为每个测试情形回答查询。对于每个查询状态 $(k_q, z_q)$，我们首先找到 $\\mathcal{K}$ 和 $\\mathcal{Z}$ 中最接近的网格点 $k_i$ 和 $z_j$。报告的答案是最优的下一期资本选择 $g^*(k_i, z_j)$，它本身是 $\\mathcal{K}$ 的一个元素。收集结果并按规定格式化。\n\n此程序提供了一个完整而严谨的方法来解决给定问题，遵守所有指定的约束和方法要求。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the deterministic dynamic optimization problem using Policy Function Iteration.\n    \"\"\"\n\n    def setup_grids_and_transitions():\n        \"\"\"Initializes grids and pre-computes productivity transitions.\"\"\"\n        # Grid parameters\n        nk, k_min, k_max = 61, 0.5, 2.0\n        nz, z_min, z_max = 21, 0.9, 1.1\n\n        k_grid = np.linspace(k_min, k_max, nk)\n        z_grid = np.linspace(z_min, z_max, nz)\n\n        # ANN parameters\n        W = np.array([[1.3], [-0.9]])\n        b = np.array([[0.15], [-0.25]])\n        c = np.array([[0.06], [0.03]])\n\n        # Pre-compute productivity transitions\n        z_next_idx = np.zeros(nz, dtype=int)\n        for j, z in enumerate(z_grid):\n            h = np.tanh(W * z + b)\n            z_prime_raw = 1.0 + c.T @ h\n            z_next_idx[j] = np.argmin(np.abs(z_grid - z_prime_raw.item()))\n        \n        return k_grid, z_grid, z_next_idx\n\n    def find_nearest_idx(grid, value):\n        \"\"\"Finds the index of the nearest grid point to a given value.\"\"\"\n        return np.argmin(np.abs(grid - value))\n\n    def evaluate_policy_exact(policy, rewards, z_next_idx, beta, nk, nz):\n        \"\"\"\n        Computes the value function for a given policy using the exact graph-based method.\n        \"\"\"\n        n_states = nk * nz\n        values = np.full(n_states, np.nan)\n        next_state = np.zeros(n_states, dtype=int)\n        \n        # 1. Build graph structure (next_state map)\n        for i in range(nk):\n            for j in range(nz):\n                s_idx = i * nz + j\n                k_prime_idx = policy[i, j]\n                z_prime_idx = z_next_idx[j]\n                next_state[s_idx] = k_prime_idx * nz + z_prime_idx\n\n        # 2. Find cycles and compute their values\n        visited = np.zeros(n_states, dtype=bool)\n        for s_idx in range(n_states):\n            if not visited[s_idx]:\n                path = []\n                path_set = set()\n                curr = s_idx\n                # Trace path until a visited node or a cycle is found\n                while curr not in path_set and not visited[curr]:\n                    path.append(curr)\n                    path_set.add(curr)\n                    curr = next_state[curr]\n\n                # If a cycle is found in the current traversal\n                if curr in path_set:\n                    cycle_start_idx = path.index(curr)\n                    cycle = path[cycle_start_idx:]\n                    cycle_len = len(cycle)\n                    \n                    # Calculate value of the first node in the cycle using geometric series\n                    discounted_rewards_sum = 0.0\n                    for m in range(cycle_len):\n                        node_in_cycle = cycle[m]\n                        # next_node_in_cycle = cycle[(m + 1) % cycle_len] # Not needed for sum\n                        \n                        reward_val = rewards.flat[node_in_cycle]\n                        discounted_rewards_sum += (beta**m) * reward_val\n                    \n                    v_cycle_start = discounted_rewards_sum / (1.0 - beta**cycle_len)\n                    values[cycle[0]] = v_cycle_start\n\n                    # Propagate values around the cycle\n                    for m in range(cycle_len - 1):\n                        node = cycle[m]\n                        next_node = cycle[m + 1]\n                        values[next_node] = (values[node] - rewards.flat[node]) / beta\n                \n                # Mark entire path as visited\n                for node in path:\n                    visited[node] = True\n        \n        # 3. Backward propagation for tree nodes\n        while np.any(np.isnan(values)):\n            for s_idx in range(n_states):\n                if np.isnan(values[s_idx]):\n                    s_next_idx = next_state[s_idx]\n                    if not np.isnan(values[s_next_idx]):\n                        values[s_idx] = rewards.flat[s_idx] + beta * values[s_next_idx]\n                        \n        return values.reshape(nk, nz)\n\n    def run_pfi(beta, alpha, delta, k_grid, z_grid, z_next_idx, max_iter=100):\n        \"\"\"Runs the Policy Function Iteration algorithm.\"\"\"\n        nk, nz = len(k_grid), len(z_grid)\n        \n        # Initialize policy: choose lowest capital level\n        policy = np.zeros((nk, nz), dtype=int)\n        \n        # Pre-compute cash-on-hand for all states\n        k_mesh, z_mesh = np.meshgrid(k_grid, z_grid, indexing='ij')\n        cash_on_hand = z_mesh * (k_mesh**alpha) + (1.0 - delta) * k_mesh\n\n        for iteration in range(max_iter):\n            # Policy Evaluation Step\n            # Compute one-step rewards for the current policy\n            k_prime_vals = k_grid[policy]\n            consumption = cash_on_hand - k_prime_vals\n            \n            rewards = np.full((nk, nz), -np.inf)\n            feasible = consumption > 0\n            rewards[feasible] = np.log(consumption[feasible])\n\n            # Exact policy evaluation\n            value_function = evaluate_policy_exact(policy, rewards, z_next_idx, beta, nk, nz)\n\n            # Policy Improvement Step\n            # Vectorized calculation of Bellman equation RHS for all possible choices\n            consumption_all_choices = cash_on_hand[:, :, np.newaxis] - k_grid[np.newaxis, np.newaxis, :]\n            \n            utility_all_choices = np.full((nk, nz, nk), -np.inf)\n            feasible_choices = consumption_all_choices > 0\n            utility_all_choices[feasible_choices] = np.log(consumption_all_choices[feasible_choices])\n            \n            # Continuation values depend on k' (choice) and z' (from current z)\n            continuation_values = beta * value_function[:, z_next_idx] # Shape (nk, nz) -> (k', z)\n            \n            # Broadcast to match utility_all_choices (k, z, k')\n            rhs_bellman = utility_all_choices + continuation_values.T[np.newaxis, :, :]\n            \n            new_policy = np.argmax(rhs_bellman, axis=2)\n\n            # Check for convergence\n            if np.array_equal(policy, new_policy):\n                break\n            \n            policy = new_policy\n        \n        return policy\n\n    # Main execution\n    alpha = 0.33\n    delta = 0.10\n    k_grid, z_grid, z_next_idx = setup_grids_and_transitions()\n\n    test_cases = [\n        {'beta': 0.95, 'queries': [(1.0, 1.0), (0.5, 0.9), (2.0, 1.1)]},\n        {'beta': 0.98, 'queries': [(1.0, 1.0), (1.5, 1.1), (0.6, 0.95)]},\n        {'beta': 0.90, 'queries': [(1.0, 1.0), (1.8, 0.9), (0.7, 1.05)]}\n    ]\n\n    all_results = []\n    for case in test_cases:\n        beta = case['beta']\n        queries = case['queries']\n        \n        optimal_policy = run_pfi(beta, alpha, delta, k_grid, z_grid, z_next_idx)\n        \n        for k_q, z_q in queries:\n            k_idx = find_nearest_idx(k_grid, k_q)\n            z_idx = find_nearest_idx(z_grid, z_q)\n            \n            optimal_k_prime_idx = optimal_policy[k_idx, z_idx]\n            result = k_grid[optimal_k_prime_idx]\n            all_results.append(result)\n\n    # Format output as specified\n    formatted_results = [f\"{x:.10f}\".rstrip('0').rstrip('.') for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2419687"}]}