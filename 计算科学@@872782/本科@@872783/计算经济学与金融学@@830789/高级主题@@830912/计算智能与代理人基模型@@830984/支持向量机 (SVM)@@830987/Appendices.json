{"hands_on_practices": [{"introduction": "在将支持向量机应用于现实世界的金融问题时，一个核心任务是选择能够最佳捕捉数据复杂性的模型。本练习将指导您完成一个典型的信用风险评估场景，您将通过交叉验证来比较线性和非线性（RBF）核函数的性能。通过这个过程，您不仅能掌握SVM模型训练和评估的实用技能，还能学会如何根据模型表现来推断数据内在的非线性特性。[@problem_id:2435431]", "problem": "给定一个在计算经济学和金融学领域的二元分类任务：使用支持向量机（SVM）预测抵押贷款违约。标签为 $y \\in \\{-1, +1\\}$，其中 $+1$ 表示违约，$-1$ 表示不违约。每个观测值有 $3$ 个特征：贷款价值比 $(x_1)$（小数形式），债务收入比 $(x_2)$（小数形式），以及 FICO 分数 $(x_3)$（整数形式）。数据集包含 $20$ 个观测值，索引为 $i \\in \\{1,2,\\ldots,20\\}$，表示为 $(x_{i1}, x_{i2}, x_{i3}; y_i)$：\n- $1$: $(0.65, 0.18, 780; -1)$\n- $2$: $(0.70, 0.20, 760; -1)$\n- $3$: $(0.75, 0.25, 740; -1)$\n- $4$: $(0.80, 0.22, 770; -1)$\n- $5$: $(0.68, 0.30, 720; -1)$\n- $6$: $(0.72, 0.28, 730; -1)$\n- $7$: $(0.85, 0.20, 750; -1)$\n- $8$: $(0.90, 0.18, 760; -1)$\n- $9$: $(0.78, 0.26, 740; -1)$\n- $10$: $(0.82, 0.27, 735; -1)$\n- $11$: $(0.95, 0.45, 660; +1)$\n- $12$: $(1.02, 0.40, 680; +1)$\n- $13$: $(0.88, 0.55, 620; +1)$\n- $14$: $(0.92, 0.50, 600; +1)$\n- $15$: $(1.05, 0.35, 650; +1)$\n- $16$: $(0.90, 0.60, 590; +1)$\n- $17$: $(0.98, 0.48, 630; +1)$\n- $18$: $(1.10, 0.30, 610; +1)$\n- $19$: $(0.84, 0.58, 605; +1)$\n- $20$: $(0.70, 0.40, 580; +1)$\n\n构建一个软间隔二元支持向量机 (SVM) 分类器，并使用 $k=5$ 的 $k$-折交叉验证来评估样本外分类准确率。使用以下三组参数作为测试套件：\n- 测试 $A$：线性核函数，软间隔参数 $C = 10$。\n- 测试 $B$：高斯径向基函数 (RBF) 核，参数 $C = 10$ 且 $\\gamma = 0.5$。\n- 测试 $C$：高斯径向基函数 (RBF) 核，参数 $C = 10$ 且 $\\gamma = 2.0$。\n\n对于三个测试 $A$、$B$ 和 $C$ 中的每一个，计算平均 $5$-折样本外分类准确率，结果为 $[0,1]$ 区间内的实数。将每个准确率四舍五入到三位小数。\n\n最后，推断核函数的选择对于从特征到违约的信用风险映射的性质意味着什么。输出一个如下定义的整数指示器：如果RBF准确率中的最优值（来自测试B或C）比线性核准确率（来自测试A）高出至少 $0.03$，则输出 $1$；否则输出 $0$。\n\n您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为 $[\\text{acc}_A,\\text{acc}_B,\\text{acc}_C,\\text{indicator}]$，其中 $\\text{acc}_A$ 是测试A的平均准确率，$\\text{acc}_B$ 是测试B的，$\\text{acc}_C$ 是测试C的，而 $\\text{indicator}$ 是上述的整数。例如，格式应为 $[0.842,0.902,0.881,1]$。", "solution": "所呈现的问题是机器学习中一个标准的二元分类任务，应用于计算金融领域。目标是构建并评估一个用于预测抵押贷款违约的软间隔支持向量机 (SVM) 分类器。该问题具有科学依据、定义明确，并提供了所有必要的数据和参数。因此，它被认为是有效的，并将构建一个正式的解决方案。\n\n解决方案的核心涉及求解软间隔SVM的对偶优化问题。对于一个包含 $N$ 个数据点 $(\\mathbf{x}_i, y_i)$ 的训练集，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量， $y_i \\in \\{-1, +1\\}$ 是标签，其对偶问题是一个二次规划 (QP) 问题，表述如下：\n$$\n\\max_{\\boldsymbol{\\alpha}} \\mathcal{L}_D(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_i \\alpha_j y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)\n$$\n受以下约束：\n$$\n\\sum_{i=1}^{N} \\alpha_i y_i = 0\n$$\n$$\n0 \\le \\alpha_i \\le C, \\quad \\text{for } i=1, \\ldots, N\n$$\n此处，$\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_N)$ 是拉格朗日乘子向量，$C$ 是控制错分类惩罚的正则化参数，$K(\\mathbf{x}_i, \\mathbf{x}_j)$ 是核函数。该问题通过最小化对偶拉格朗日函数的负值 $-\\mathcal{L}_D(\\boldsymbol{\\alpha})$ 来解决，这等价于最小化 $L(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{\\alpha}^T P \\boldsymbol{\\alpha} - \\mathbf{1}^T \\boldsymbol{\\alpha}$，其中矩阵 $P$ 的元素为 $P_{ij} = y_i y_j K(\\mathbf{x}_i, \\mathbf{x}_j)$。\n\n问题指定了两种类型的核函数：\n$1$. **线性核 (Linear Kernel)**：此核函数对应于原始特征空间中的线性决策边界。其定义为：\n$$\nK(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j\n$$\n$2$. **高斯径向基函数 (RBF) 核 (Gaussian Radial Basis Function (RBF) Kernel)**：此核函数通过将数据映射到无限维特征空间，从而允许非线性决策边界。其定义为：\n$$\nK(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma \\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2)\n$$\n其中 $\\gamma > 0$ 是一个控制高斯函数宽度的参数。\n\n该解决方案通过使用 `scipy.optimize.minimize` 中可用的序列最小二乘规划 (SLSQP) 算法来数值求解此QP问题来实现。在找到最优向量 $\\boldsymbol{\\alpha}^*$ 后，计算偏置项 $b$。一个稳健的方法是，对所有满足 $0  \\alpha_i^*  C$ 的支持向量进行平均，因为对于这些点，Karush-Kuhn-Tucker (KKT) 条件意味着间隔被精确满足：\n$$\nb = y_s - \\sum_{i=1}^{N} \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}_s)\n$$\n如果不存在此类非边界支持向量，则通过取由边界支持向量（$\\alpha_i^* = C$）的间隔约束所定义的可行区间的中点来确定 $b$。\n\n对新数据点 $\\mathbf{x}$ 的决策函数则由下式给出：\n$$\nf(\\mathbf{x}) = \\text{sign}\\left(\\sum_{i=1}^{N} \\alpha_i^* y_i K(\\mathbf{x}_i, \\mathbf{x}) + b\\right)\n$$\n\n方法论中的一个关键步骤是**特征缩放**。输入特征（$x_1, x_2, x_3$）的尺度差异巨大。若不进行处理，FICO分数 $x_3$ 将在任何基于距离的计算中（例如在RBF核中）占据主导地位。为解决此问题，我们对特征应用标准化（Z-score归一化）。在交叉验证的每一折中，从训练数据计算均值和标准差，然后用它们来缩放该折的训练数据和测试数据。这可以防止测试集的数据泄露到训练过程中。\n\n模型性能通过 $k=5$ 的 $k$-折交叉验证进行评估。包含 $20$ 个观测值的数据集被划分为 $5$ 个不相交的折，每折包含 $4$ 个观测值。在 $5$ 次迭代的每一次中，一折用作测试集，其余 $4$ 折用于训练。计算每一折的分类准确率，该准确率定义为测试集上被正确预测的标签的比例。对于给定的参数集，最终的样本外准确率是这 $5$ 个准确率的平均值。\n\n对指定的三个测试用例（A, B, C）中的每一个都执行此过程。最后，计算一个指示变量。如果性能最佳的RBF核的准确率比线性核的准确率至少高出 $0.03$，则此变量为 $1$，否则为 $0$。这种比较可以让我们推断类别分离的几何性质；RBF性能显著更优意味着非线性决策边界更适合此信用风险分类问题。最终输出将计算出的准确率和指示器组合成所需的列表格式。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the SVM cross-validation problem.\n    \"\"\"\n    # Dataset definition\n    data = [\n        (0.65, 0.18, 780, -1), (0.70, 0.20, 760, -1), (0.75, 0.25, 740, -1),\n        (0.80, 0.22, 770, -1), (0.68, 0.30, 720, -1), (0.72, 0.28, 730, -1),\n        (0.85, 0.20, 750, -1), (0.90, 0.18, 760, -1), (0.78, 0.26, 740, -1),\n        (0.82, 0.27, 735, -1), (0.95, 0.45, 660, 1), (1.02, 0.40, 680, 1),\n        (0.88, 0.55, 620, 1), (0.92, 0.50, 600, 1), (1.05, 0.35, 650, 1),\n        (0.90, 0.60, 590, 1), (0.98, 0.48, 630, 1), (1.10, 0.30, 610, 1),\n        (0.84, 0.58, 605, 1), (0.70, 0.40, 580, 1)\n    ]\n    X = np.array([d[:3] for d in data])\n    y = np.array([d[3] for d in data])\n\n    # Test cases\n    test_cases = [\n        {'C': 10.0, 'kernel_type': 'linear', 'gamma': None},\n        {'C': 10.0, 'kernel_type': 'rbf', 'gamma': 0.5},\n        {'C': 10.0, 'kernel_type': 'rbf', 'gamma': 2.0},\n    ]\n\n    # --- Kernel Functions ---\n    def linear_kernel(x1, x2):\n        return np.dot(x1, x2)\n\n    def make_rbf_kernel(gamma):\n        def rbf_kernel(x1, x2):\n            return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n        return rbf_kernel\n\n    # --- SVM Solver ---\n    def train_svm(X_train, y_train, C, kernel_func):\n        n_samples = X_train.shape[0]\n        \n        # Build Kernel/Gram matrix\n        K = np.zeros((n_samples, n_samples))\n        for i in range(n_samples):\n            for j in range(n_samples):\n                K[i, j] = kernel_func(X_train[i], X_train[j])\n        \n        # QP problem formulation for scipy.optimize.minimize\n        # Minimize: 0.5 * alpha.T * P * alpha - 1.T * alpha\n        # P_ij = y_i * y_j * K_ij\n        P = np.outer(y_train, y_train) * K\n        # Add a small regularization for numerical stability\n        P += 1e-8 * np.eye(n_samples)\n\n\n        def objective(alpha):\n            return 0.5 * alpha.T @ P @ alpha - np.sum(alpha)\n\n        # Constraints: sum(alpha_i * y_i) = 0\n        eq_cons = {'type': 'eq',\n                   'fun': lambda alpha: y_train.T @ alpha,\n                   'jac': lambda alpha: y_train}\n\n        # Bounds: 0 = alpha_i = C\n        bounds = [(0, C) for _ in range(n_samples)]\n\n        # Initial guess\n        alpha0 = np.zeros(n_samples)\n\n        # Solve QP problem\n        res = minimize(objective, alpha0, method='SLSQP', bounds=bounds, constraints=[eq_cons])\n        alpha = res.x\n\n        # Find support vectors\n        sv_mask = alpha > 1e-6\n        \n        # Compute bias term 'b'\n        non_bound_sv_mask = (alpha > 1e-6)  (alpha  C - 1e-6)\n        if np.any(non_bound_sv_mask):\n            non_bound_sv_indices = np.where(non_bound_sv_mask)[0]\n            b_values = [y_train[s] - np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, s]) for s in non_bound_sv_indices]\n            b = np.mean(b_values)\n        else: # Fallback if no non-bound SVs are found\n            sv_indices = np.where(sv_mask)[0]\n            f_vals_pos = [np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, i]) for i in sv_indices if y_train[i] == 1]\n            f_vals_neg = [np.sum(alpha[sv_mask] * y_train[sv_mask] * K[sv_mask, i]) for i in sv_indices if y_train[i] == -1]\n            \n            if not f_vals_pos or not f_vals_neg:\n                # Can happen if all SVs are from one class. Use a simple mean.\n                b_vals = [y_train[i] - np.sum(alpha * y_train * K[:, i]) for i in sv_indices]\n                b = np.mean(b_vals) if b_vals else 0\n            else:\n                b = -0.5 * (np.min(f_vals_pos) + np.max(f_vals_neg))\n\n        return alpha, b\n\n    # --- Prediction Function ---\n    def predict(X_test, X_train, y_train, alpha, b, kernel_func):\n        sv_mask = alpha > 1e-6\n        alpha_sv = alpha[sv_mask]\n        y_train_sv = y_train[sv_mask]\n        X_train_sv = X_train[sv_mask]\n        \n        y_pred = np.zeros(X_test.shape[0])\n        for i in range(X_test.shape[0]):\n            s = 0\n            for j in range(alpha_sv.shape[0]):\n                s += alpha_sv[j] * y_train_sv[j] * kernel_func(X_train_sv[j], X_test[i])\n            y_pred[i] = s + b\n\n        pred_labels = np.sign(y_pred)\n        pred_labels[pred_labels == 0] = 1 # Assign a class if decision value is exactly 0\n        return pred_labels\n\n    # --- Cross-Validation ---\n    def run_cross_validation(X, y, C, kernel_func, k=5):\n        n_samples = X.shape[0]\n        fold_size = n_samples // k\n        indices = np.arange(n_samples)\n        \n        accuracies = []\n        for i in range(k):\n            # Split data into train and test sets for the current fold\n            start, end = i * fold_size, (i + 1) * fold_size\n            test_indices = indices[start:end]\n            train_indices = np.delete(indices, test_indices)\n\n            X_train, y_train = X[train_indices], y[train_indices]\n            X_test, y_test = X[test_indices], y[test_indices]\n            \n            # --- Feature Scaling ---\n            mean = np.mean(X_train, axis=0)\n            std = np.std(X_train, axis=0)\n            std[std == 0] = 1.0 # Avoid division by zero\n            \n            X_train_scaled = (X_train - mean) / std\n            X_test_scaled = (X_test - mean) / std\n            \n            # Train SVM\n            alpha, b = train_svm(X_train_scaled, y_train, C, kernel_func)\n            \n            # Predict on test set\n            y_pred = predict(X_test_scaled, X_train_scaled, y_train, alpha, b, kernel_func)\n            \n            # Calculate accuracy\n            accuracy = np.mean(y_pred == y_test)\n            accuracies.append(accuracy)\n            \n        return np.mean(accuracies)\n\n    # --- Main Execution Logic ---\n    results = []\n    for case in test_cases:\n        C = case['C']\n        if case['kernel_type'] == 'linear':\n            kernel = linear_kernel\n        else: # rbf\n            kernel = make_rbf_kernel(case['gamma'])\n        \n        mean_accuracy = run_cross_validation(X, y, C, kernel)\n        results.append(round(mean_accuracy, 3))\n    \n    acc_A, acc_B, acc_C = results\n    \n    # Compute the final indicator\n    indicator = 1 if max(acc_B, acc_C) >= acc_A + 0.03 else 0\n    results.append(indicator)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2435431"}, {"introduction": "支持向量机的强大威力在很大程度上源于“核技巧”，它允许我们在高维特征空间中构建复杂的决策边界，而无需显式地进行坐标映射。本练习将带您超越标准核函数，通过设计一个结合了时间衰减效应的自定义核函数，来解决金融时间序列预测问题。这不仅能加深您对核函数本质的理解，也展示了如何将特定领域的先验知识（如“新数据比旧数据更重要”）融入到模型之中。[@problem_id:2435408]", "problem": "给定一个二元分类任务，其动机源于金融时间序列预测。每个样本包含一个特征向量（表示近期回报率和波动率的摘要统计）、一个时间戳（指示其在时间序列中的位置）以及一个标签（指示次日回报率是上涨还是下跌）。考虑以下包含四个样本的训练数据，其中每个特征向量位于 $\\mathbb{R}^2$ 中，每个时间戳为整数，每个标签在 $\\{-1,+1\\}$ 中：\n- 样本 $1$：$x_1 = (0.00, 0.00)$，$t_1 = 1$，$y_1 = -1$。\n- 样本 $2$：$x_2 = (1.00, 0.20)$，$t_2 = 2$，$y_2 = +1$。\n- 样本 $3$：$x_3 = (0.90, 1.00)$，$t_3 = 3$，$y_3 = +1$。\n- 样本 $4$：$x_4 = (-0.80, -0.50)$，$t_4 = 4$，$y_4 = -1$。\n\n您必须使用支持向量机 (SVM)，其核函数会增加近期观测值的影响力。设任意两个样本 $(x_i,t_i)$ 和 $(x_j,t_j)$ 的核函数定义为\n$$\nK_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big) \\;=\\; \\exp\\!\\big(-\\gamma \\,\\|x_i - x_j\\|_2^2\\big)\\;\\cdot\\;\\exp\\!\\big(\\alpha\\,(t_i + t_j)\\big),\n$$\n其中 $\\alpha \\ge 0$ 控制时间衰减强度，$\\gamma > 0$ 控制径向基尺度。\n\n使用此核函数在对偶形式下训练一个软间隔SVM。令 $n$ 表示训练样本的数量。对偶优化问题为：\n$$\n\\max_{\\alpha_1,\\dots,\\alpha_n}\\;\\; \\sum_{i=1}^n \\alpha_i \\;-\\;\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n \\alpha_i\\alpha_j\\,y_i y_j\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big),\n$$\n约束条件为\n$$\n\\sum_{i=1}^n y_i \\alpha_i \\;=\\; 0,\\quad 0 \\le \\alpha_i \\le C \\;\\text{ for all } i.\n$$\n在获得最优解 $\\{\\alpha_i^\\star\\}_{i=1}^n$ 后，对任意输入 $(z,t_z)$ 定义决策函数为\n$$\nf(z,t_z) \\;=\\; \\sum_{i=1}^n \\alpha_i^\\star y_i\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(z,t_z)\\big) \\;+\\; b,\n$$\n其中偏置项 $b$ 的选择需满足卡鲁什-库恩-塔克 (Karush-Kuhn-Tucker) 条件。具体来说，如果存在至少一个索引 $i$ 满足 $0  \\alpha_i^\\star  C$，则强制执行 $y_i\\,f(x_i,t_i)=1$ 并将 $b$ 取为所有此类 $i$ 的 $y_i - \\sum_{j=1}^n \\alpha_j^\\star y_j K_{\\alpha,\\gamma}((x_j,t_j),(x_i,t_i))$ 的平均值。如果不存在这样的索引，则在与互补松弛不等式一致的值区间内选择 $b$，\n$$\n\\begin{aligned}\n\\text{for } \\alpha_i^\\star = 0:\\;\\; y_i\\,f(x_i,t_i) \\ge 1,\\\\\n\\text{for } \\alpha_i^\\star = C:\\;\\; y_i\\,f(x_i,t_i) \\le 1,\n\\end{aligned}\n$$\n并且为了使输出唯一，将 $b$ 选为这些不等式所隐含的最紧可行区间的中点。\n\n对以下三个测试输入进行分类，每个输入都在时间戳 $t_z = 5$ 时进行评估：\n- $z_1 = (0.95, 0.70)$，\n- $z_2 = (-0.70, -0.40)$，\n- $z_3 = (0.10, 0.05)$。\n使用 $f(z,t_z)$ 的符号来预测标签（在 $\\{-1,+1\\}$ 中），约定非负值映射为 $+1$。\n\n您的程序必须针对下方的每种参数配置，严格按照所给定的形式求解SVM对偶问题，并为每种配置按规定顺序输出三个测试输入的预测标签。参数配置的测试套件为：\n- 情况 $1$：$(\\alpha,\\gamma,C) = (0.0, 1.0, 10.0)$。\n- 情况 $２$：$(\\alpha,\\gamma,C) = (0.5, 1.0, 10.0)$。\n- 情况 $３$：$(\\alpha,\\gamma,C) = (1.0, 0.5, 10.0)$。\n- 情况 $４$：$(\\alpha,\\gamma,C) = (0.0, 5.0, 1.0)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，其中每个元素本身是对应于每个情况下 $(z_1,5)$、$(z_2,5)$ 和 $(z_3,5)$ 的预测标签的三个整数的列表，顺序如上。例如，输出格式必须是\n$$\n\\big[\\,[\\ell_{1,1},\\ell_{1,2},\\ell_{1,3}],\\;[\\ell_{2,1},\\ell_{2,2},\\ell_{2,3}],\\;[\\ell_{3,1},\\ell_{3,2},\\ell_{3,3}],\\;[\\ell_{4,1},\\ell_{4,2},\\ell_{4,3}]\\,\\big],\n$$\n其中每个 $\\ell_{k,j} \\in \\{-1,+1\\}$ 是一个整数。不应打印任何其他文本。", "solution": "该问题被判定为有效。它提出了一个标准的软间隔支持向量机 (SVM) 分类任务，尽管使用了自定义的核函数。该问题在科学上基于成熟的机器学习理论，被构建为一个凸优化问题，表述客观，并为获得唯一解提供了所有必要信息。\n\n对于每个给定的参数配置 $(\\alpha, \\gamma, C)$，求解过程包括几个不同的步骤。\n\n首先，我们将SVM对偶问题形式化为一个标准的二次规划 (QP) 问题，以适用于数值求解器。目标是找到拉格朗日乘子 $\\boldsymbol{\\alpha} = [\\alpha_1, \\dots, \\alpha_n]^T$ 以最大化对偶目标函数。这等价于最小化以下二次型：\n$$\n\\mathcal{L}(\\boldsymbol{\\alpha}) = \\frac{1}{2} \\boldsymbol{\\alpha}^T \\mathbf{H} \\boldsymbol{\\alpha} - \\mathbf{1}^T \\boldsymbol{\\alpha}\n$$\n约束条件为：\n$$\n\\mathbf{y}^T \\boldsymbol{\\alpha} = 0 \\quad \\text{and} \\quad \\mathbf{0} \\le \\boldsymbol{\\alpha} \\le C \\cdot \\mathbf{1}\n$$\n这里，$n=4$ 是训练样本的数量，$\\mathbf{y} = [-1, 1, 1, -1]^T$ 是标签向量，$\\mathbf{1}$ 是全一向量，$C$ 是正则化参数。矩阵 $\\mathbf{H}$ 是一个 $n \\times n$ 矩阵，其元素为 $H_{ij} = y_i y_j K_{ij}$，其中 $K_{ij}$ 是第 $i$ 个和第 $j$ 个训练样本的核函数值。\n\n核函数由下式给出：\n$$\nK_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big) \\;=\\; \\exp\\!\\big(-\\gamma \\,d_{ij}^2\\big)\\;\\cdot\\;\\exp\\!\\big(\\alpha\\,(t_i + t_j)\\big)\n$$\n其中 $d_{ij}^2 = \\|x_i - x_j\\|_2^2$ 是特征向量 $x_i$ 和 $x_j$ 之间欧氏距离的平方。对于给定的训练数据：\n$x_1 = (0.00, 0.00)$，$t_1 = 1$，$y_1 = -1$\n$x_2 = (1.00, 0.20)$，$t_2 = 2$，$y_2 = +1$\n$x_3 = (0.90, 1.00)$，$t_3 = 3$，$y_3 = +1$\n$x_4 = (-0.80, -0.50)$，$t_4 = 4$，$y_4 = -1$\n我们首先预计算 $n \\times n$ 的格拉姆矩阵（Gram matrix），记作 $\\mathbf{K}$，其条目为 $K_{ij} = K_{\\alpha,\\gamma}\\big((x_i,t_i),(x_j,t_j)\\big)$。然后，我们使用公式 $H_{ij} = y_i y_j K_{ij}$ 构建矩阵 $\\mathbf{H}$。\n\n这个约束优化问题使用数值QP求解器来解决。我们使用 `scipy.optimize.minimize` 库函数中的序列最小二乘规划 (Sequential Least Squares Programming, SLSQP) 算法。该函数接受目标函数 $\\mathcal{L}(\\boldsymbol{\\alpha})$、一个初始猜测（例如 $\\boldsymbol{\\alpha}_0 = \\mathbf{0}$）、每个 $i$ 的边界 $0 \\le \\alpha_i \\le C$ 以及线性等式约束 $\\sum_i y_i \\alpha_i = 0$。求解器返回最优向量 $\\boldsymbol{\\alpha}^\\star = [\\alpha_1^\\star, \\dots, \\alpha_n^\\star]^T$。由于数值精度的原因，将非常接近 $0$ 或 $C$ 的 $\\alpha_i^\\star$ 值钳制（clamp）到这些精确值。\n\n获得 $\\boldsymbol{\\alpha}^\\star$ 后，我们计算偏置项 $b$。计算方法取决于得到的 $\\alpha_i^\\star$ 值，并基于卡鲁什-库恩-塔克 (KKT) 条件。\n令 $S$ 为在间隔边界上的支持向量的索引集合，其中 $0  \\alpha_i^\\star  C$。\n如果 $S$ 非空，则计算偏置 $b$ 以确保对于任何 $i \\in S$，决策函数满足 $y_i f(x_i, t_i) = 1$。这导出表达式：\n$$\nb_i = y_i - \\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij}\n$$\n为了稳健性，我们对所有 $i \\in S$ 的这些值取平均值：$b = \\frac{1}{|S|} \\sum_{i \\in S} b_i$。\n\n如果 $S$ 为空，则所有的 $\\alpha_i^\\star$ 都在边界上（$0$ 或 $C$）。在这种情况下，$b$ 不是由单个方程唯一确定的，而是必须位于由KKT互补松弛条件定义的区间内：\n- 对于 $\\alpha_i^\\star = 0$：$y_i \\big(\\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij} + b\\big) \\ge 1$。\n- 对于 $\\alpha_i^\\star = C$：$y_i \\big(\\sum_{j=1}^n \\alpha_j^\\star y_j K_{ij} + b\\big) \\le 1$。\n这些不等式为 $b$ 定义了一个可行区间。令 $b_{lower}$ 为从这些条件导出的下界的最大值，$b_{upper}$ 为上界的最小值。问题要求选择 $b$ 作为这个最紧可行区间的中点：$b = (b_{lower} + b_{upper}) / 2$。\n\n最后，在确定 $\\boldsymbol{\\alpha}^\\star$ 和 $b$ 之后，我们对时间戳 $t_z = 5$ 时的三个新测试输入 $z_1, z_2, z_3$ 进行分类。对于每个测试点 $(z, t_z)$，评估决策函数：\n$$\nf(z,t_z) \\;=\\; \\sum_{i=1}^n \\alpha_i^\\star y_i\\, K_{\\alpha,\\gamma}\\big((x_i,t_i),(z,t_z)\\big) \\;+\\; b\n$$\n预测标签 $\\ell \\in \\{-1, +1\\}$ 由 $f(z,t_z)$ 的符号给出，约定非负值映射为 $+1$：\n$$\n\\ell = \\begin{cases} +1  \\text{if } f(z,t_z) \\ge 0 \\\\ -1  \\text{if } f(z,t_z)  0 \\end{cases}\n$$\n对四个指定的参数配置中的每一个都执行此完整过程，并将得出的预测结果汇总为所需的输出格式。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    # Define the training data from the problem statement.\n    X_train = np.array([\n        [0.00, 0.00],\n        [1.00, 0.20],\n        [0.90, 1.00],\n        [-0.80, -0.50]\n    ])\n    T_train = np.array([1, 2, 3, 4])\n    Y_train = np.array([-1, 1, 1, -1])\n    n_samples = len(X_train)\n\n    # Define the test inputs.\n    X_test = np.array([\n        [0.95, 0.70],\n        [-0.70, -0.40],\n        [0.10, 0.05]\n    ])\n    T_test = 5\n\n    # Define the parameter configurations from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 10.0),\n        (0.5, 1.0, 10.0),\n        (1.0, 0.5, 10.0),\n        (0.0, 5.0, 1.0),\n    ]\n\n    all_results = []\n    # Numerical tolerance for floating point comparisons\n    tol = 1e-9\n\n    for case in test_cases:\n        alpha_param, gamma, C = case\n        \n        # 1. Construct the Kernel Matrix K.\n        # Efficiently compute squared Euclidean distances between all pairs of training points.\n        sq_dists = np.sum(X_train**2, axis=1, keepdims=True) + np.sum(X_train**2, axis=1) - 2 * np.dot(X_train, X_train.T)\n        rbf_kernel = np.exp(-gamma * sq_dists)\n        time_component = np.exp(alpha_param * (T_train.reshape(-1, 1) + T_train))\n        K = rbf_kernel * time_component\n\n        # 2. Construct the matrix H for the QP problem.\n        H = np.outer(Y_train, Y_train) * K\n        \n        # 3. Define the QP objective function and constraints.\n        def objective_func(alphas):\n            return 0.5 * alphas.T @ H @ alphas - np.sum(alphas)\n        \n        constraints = ({'type': 'eq', 'fun': lambda alphas: np.dot(Y_train, alphas)})\n        bounds = Bounds([0] * n_samples, [C] * n_samples)\n        \n        # 4. Solve the QP problem for the optimal alphas.\n        result = minimize(objective_func, np.zeros(n_samples), method='SLSQP', bounds=bounds, constraints=constraints)\n        alpha_star = result.x\n        \n        # Clamp near-zero and near-C values for stable bias calculation.\n        alpha_star[alpha_star  tol] = 0\n        alpha_star[alpha_star > C - tol] = C\n        \n        # 5. Calculate the bias term b.\n        # Find support vectors on the margin (0  alpha_i  C).\n        sv_margin_indices = np.where((alpha_star > tol)  (alpha_star  C - tol))[0]\n\n        if len(sv_margin_indices) > 0:\n            # Case 1: bias calculation from margin support vectors.\n            b_values = [Y_train[i] - np.sum(alpha_star * Y_train * K[:, i]) for i in sv_margin_indices]\n            b = np.mean(b_values)\n        else:\n            # Case 2: bias calculation from KKT conditions on non-margin SVs.\n            b_lowers, b_uppers = [], []\n            f_preds_no_b = K.T @ (alpha_star * Y_train)\n\n            for i in range(n_samples):\n                if not ((alpha_star[i] > tol) and (alpha_star[i]  C - tol)):\n                    if np.isclose(alpha_star[i], 0): # alpha_i = 0 implies y_i(f_i+b) >= 1\n                        if Y_train[i] == 1: b_lowers.append(1 - f_preds_no_b[i])\n                        else: b_uppers.append(-1 - f_preds_no_b[i])\n                    elif np.isclose(alpha_star[i], C): # alpha_i = C implies y_i(f_i+b) = 1\n                        if Y_train[i] == 1: b_uppers.append(1 - f_preds_no_b[i])\n                        else: b_lowers.append(-1 - f_preds_no_b[i])\n            \n            max_lower = max(b_lowers) if b_lowers else -np.inf\n            min_upper = min(b_uppers) if b_uppers else np.inf\n            b = (max_lower + min_upper) / 2.0\n\n        # 6. Classify test inputs using the calculated model.\n        case_predictions = []\n        for z in X_test:\n            # Calculate kernel values between training points and the test point.\n            sq_dists_test = np.sum(X_train**2, axis=1) + np.sum(z**2) - 2 * np.dot(X_train, z)\n            rbf_kernel_test = np.exp(-gamma * sq_dists_test)\n            time_component_test = np.exp(alpha_param * (T_train + T_test))\n            K_test = rbf_kernel_test * time_component_test\n            \n            # Evaluate the decision function.\n            f_z = np.sum(alpha_star * Y_train * K_test) + b\n            \n            # Predict the label.\n            prediction = 1 if f_z >= 0 else -1\n            case_predictions.append(prediction)\n        \n        all_results.append(case_predictions)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2435408"}, {"introduction": "在金融领域部署机器学习模型时，我们不仅要关心其预测准确性，还必须考虑其稳健性和安全性。本练习将引导您从一个新颖的视角——对抗性攻击——来审视一个训练好的信用评分SVM模型。您将扮演一个试图“欺骗”模型的对手，通过以最小的成本修改申请特征，来将一个“拒绝”的分类结果翻转为“批准”，这涉及到构建和求解一个优化问题。这项实践对于理解和防范金融AI系统中的潜在风险至关重要。[@problem_id:2435491]", "problem": "给定一个用于信用评分的线性支持向量机 (SVM)。如果决策值为非负数，决策函数将特征向量分类为“批准”，否则分类为“拒绝”。形式上，对于特征向量 $x \\in \\mathbb{R}^n$、权重向量 $w \\in \\mathbb{R}^n$ 和偏置 $b \\in \\mathbb{R}$，决策值为 $f(x) = w^\\top x + b$。如果 $f(x) \\ge 0$，则分类为“批准”；如果 $f(x)  0$，则分类为“拒绝”。攻击者可以通过选择一个满足箱式约束和不可变性约束的扰动 $\\delta \\in \\mathbb{R}^n$ 来修改申请，旨在以最小的代价将“拒绝”翻转为“批准”。修改后的特征必须按分量满足下界和上界 $L \\le x + \\delta \\le U$，其中 $L,U \\in \\mathbb{R}^n$，并且只有标记为可变的特征才能被更改。\n\n攻击者修改申请的代价被建模为加权 $\\ell_1$ 代价，由 $C(\\delta) = \\sum_{j=1}^n c_j \\lvert \\delta_j \\rvert$ 给出，其中 $c \\in \\mathbb{R}^n$ 的所有条目均为严格正数。当且仅当掩码条目 $m_j$ 等于 $1$ 时，特征 $j$ 是可变的。如果 $m_j = 0$，则 $\\delta_j$ 必须等于 $0$。\n\n您的任务是为每个指定的测试用例计算，从给定的 $x$ 开始，为获得“批准”分类所需的最小可能代价，即在满足 $L \\le x + \\delta \\le U$ 和不可变性掩码的条件下，实现 $w^\\top (x + \\delta) + b \\ge 0$。如果原始的 $x$ 已经满足 $w^\\top x + b \\ge 0$，则最小代价为 $0$。如果不存在满足所有约束并实现 $w^\\top (x + \\delta) + b \\ge 0$ 的扰动 $\\delta$，则返回 $+\\infty$ 作为一个浮点数量。\n\n您可以使用的基本定义：\n- 线性 SVM 的线性决策规则 $f(x) = w^\\top x + b$。\n- 加权 $\\ell_1$ 代价的定义 $C(\\delta) = \\sum_{j=1}^n c_j \\lvert \\delta_j \\rvert$。\n- 在 $\\mathbb{R}^n$ 中定义一个多面体的线性不等式和等式约束。\n\n设计一个算法，通过仅使用上述基本定义来构建和解决一个数学上有效的优化问题，以计算最小代价。不要假设超出这些定义的任何快捷公式。对于数值计算，假设所有特征都是无单位的，不涉及物理单位。\n\n测试套件：\n对于每种情况，您将获得 $w$、$b$、$x$、$L$、$U$、$c$ 以及一个指示可变性的二进制掩码向量 $m$。每个向量的长度为 $n = 3$。\n\n- 情况 1（一般可行情况）：\n  - $w = (0.5, -1.0, 0.2)$\n  - $b = -0.3$\n  - $x = (0.4, 0.5, 0.0)$\n  - $L = (0.0, 0.0, 0.0)$\n  - $U = (1.0, 1.0, 1.0)$\n  - $c = (1.0, 2.0, 0.5)$\n  - $m = (1, 1, 1)$\n- 情况 2（已经批准；最小代价为零）：\n  - $w = (0.5, -1.0, 0.2)$\n  - $b = -0.3$\n  - $x = (1.0, 0.1, 0.0)$\n  - $L = (0.0, 0.0, 0.0)$\n  - $U = (1.0, 1.0, 1.0)$\n  - $c = (1.0, 2.0, 0.5)$\n  - $m = (1, 1, 1)$\n- 情况 3（由于不可变性和边界而不可行）：\n  - $w = (0.5, -1.0, 0.2)$\n  - $b = -0.3$\n  - $x = (0.4, 0.5, 0.0)$\n  - $L = (0.0, 0.0, 0.0)$\n  - $U = (1.0, 1.0, 1.0)$\n  - $c = (1.0, 2.0, 0.5)$\n  - $m = (0, 0, 1)$\n- 情况 4（部分特征可变，仍然可行）：\n  - $w = (0.5, -1.0, 0.2)$\n  - $b = -0.3$\n  - $x = (0.4, 0.2, 0.9)$\n  - $L = (0.0, 0.0, 0.0)$\n  - $U = (1.0, 1.0, 1.0)$\n  - $c = (1.0, 2.0, 0.5)$\n  - $m = (1, 0, 0)$\n- 情况 5（对代价敏感的修改改变了最优特征选择）：\n  - $w = (0.5, -1.0, 0.2)$\n  - $b = -0.3$\n  - $x = (0.4, 0.5, 0.0)$\n  - $L = (0.0, 0.0, 0.0)$\n  - $U = (1.0, 1.0, 1.0)$\n  - $c = (10.0, 0.1, 1.0)$\n  - $m = (1, 1, 1)$\n\n要求的最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的浮点数列表，用方括号括起来，顺序与上述情况相同（例如，$[0.0,1.5,+\\infty,0.2,0.7]$，尽管您的程序不得为无穷大打印加号）。如果某个情况不可行，则输出编程语言生成的浮点无穷大（例如，$[\\dots,\\mathrm{inf},\\dots]$）。所有数值输出都必须是浮点数。", "solution": "用户提供了一个问题，要求在满足一系列约束条件的情况下，找到一个最小代价来扰动特征向量 $x$，以从线性支持向量机 (SVM) 获得有利的分类结果。\n\n首先，我必须验证问题陈述。\n\n### 步骤 1：提取给定信息\n- 决策函数：$f(x) = w^\\top x + b$，其中 $x, w \\in \\mathbb{R}^n$ 且 $b \\in \\mathbb{R}$。\n- 分类规则：如果 $f(x) \\ge 0$ 则“批准”，如果 $f(x)  0$ 则“拒绝”。\n- 攻击目标：找到一个扰动 $\\delta \\in \\mathbb{R}^n$ 来将 $x$ 修改为 $x+\\delta$，使得 $f(x+\\delta) \\ge 0$。\n- 代价函数：最小化加权 $\\ell_1$ 代价 $C(\\delta) = \\sum_{j=1}^n c_j |\\delta_j|$，其中 $c_j > 0$。\n- 对扰动 $\\delta$ 的约束：\n    1.  箱式约束：$L \\le x + \\delta \\le U$（分量级），其中 $L, U \\in \\mathbb{R}^n$。\n    2.  不可变性约束：如果掩码条目 $m_j = 0$，则 $\\delta_j = 0$。\n- 特殊条件：\n    -   如果已经满足 $f(x) \\ge 0$，则最小代价为 $0$。\n    -   如果不存在这样的 $\\delta$，则代价为 $+\\infty$。\n- 测试用例：为 $n=3$ 提供了五组特定的参数 $(w, b, x, L, U, c, m)$。\n\n### 步骤 2：使用提取的给定信息进行验证\n1.  **科学依据**：该问题设置在对机器学习模型（特别是线性 SVM）进行对抗性攻击的背景下。该公式使用加权 $\\ell_1$ 范数作为代价函数和线性约束（箱式和不可变性）。这代表了机器学习研究中一个成熟的分支领域。该问题是一个约束优化问题，具体来说是一个线性规划（LP），这是数学和运筹学中的一个基本主题。该问题在科学上是合理的。\n2.  **适定性**：该问题是在一个凸集（约束定义了一个多面体）上最小化一个凸函数（$C(\\delta)$ 是一个加权 $\\ell_1$ 范数，是凸的）。如果可行集非空且有界，这样的问题有唯一的最小值。如果可行集为空，则问题不可行，最小值被正确定义为 $+\\infty$。该问题是适定的。\n3.  **客观性**：问题的所有组成部分都使用精确的数学语言定义。输入参数和期望输出是定量的、明确的。该问题是客观的。\n\n### 步骤 3：结论和行动\n该问题是有效的。它具有科学依据、适定性和客观性。我现在将着手解决。\n\n### 基于原则的解决方案设计\n\n任务是为扰动向量 $\\delta \\in \\mathbb{R}^n$ 解决以下优化问题：\n\n$$\n\\begin{aligned}\n\\text{最小化} \\quad  \\sum_{j=1}^n c_j |\\delta_j| \\\\\n\\text{满足约束} \\quad  w^\\top (x + \\delta) + b \\ge 0 \\\\\n L_j \\le x_j + \\delta_j \\le U_j, \\quad \\text{对于 } j=1, \\dots, n \\\\\n \\delta_j = 0, \\quad \\text{如果 } m_j = 0\n\\end{aligned}\n$$\n\n首先，我们检查初始分类。如果初始决策值 $f(x) = w^\\top x + b$ 是非负的，则条件已经满足，此时 $\\delta = 0$。代价 $C(0)$ 为 $0$，这是可能的最小值，因此问题可以轻易解决。\n\n如果 $f(x)  0$，我们必须找到一个非零扰动。让我们将初始分数赤字定义为 $S = -(w^\\top x + b)$。由于 $f(x)  0$，我们有 $S > 0$。分类约束可以重写为：\n$$\nw^\\top x + w^\\top \\delta + b \\ge 0 \\implies w^\\top \\delta \\ge -(w^\\top x + b) \\implies \\sum_{j=1}^n w_j \\delta_j \\ge S\n$$\n\n箱式约束可以表示为 $\\delta_j$ 的形式：\n$$\nL_j - x_j \\le \\delta_j \\le U_j - x_j\n$$\n\n不可变性约束通过将某些 $\\delta_j$ 固定为 $0$ 来简化问题。我们只需要考虑可变特征的扰动，即 $m_j=1$ 的情况。对于非平凡情况（$S>0$），问题是：\n\n$$\n\\begin{aligned}\n\\text{最小化} \\quad  \\sum_{j: m_j=1} c_j |\\delta_j| \\\\\n\\text{满足约束} \\quad  \\sum_{j: m_j=1} w_j \\delta_j \\ge S \\\\\n L_j - x_j \\le \\delta_j \\le U_j - x_j, \\quad \\text{对于所有 } j \\text{ 且 } m_j=1\n\\end{aligned}\n$$\n\n这是线性规划的一种形式，因为绝对值函数 $|\\delta_j|$ 可以通过引入非负变量 $\\delta_j = \\delta_j^+ - \\delta_j^-$ 和 $|\\delta_j| = \\delta_j^+ + \\delta_j^-$ 来线性化。\n\n然而，这个问题的特定结构允许一种更直接的贪心解法。要增加分数项 $\\sum w_j \\delta_j$，我们应该选择每个 $\\delta_j$ 的符号以匹配其相应权重 $w_j$ 的符号。\n- 如果 $w_j > 0$，我们应该选择 $\\delta_j > 0$。每当 $\\delta_j$ 增加一个单位，分数增加 $w_j$。\n- 如果 $w_j  0$，我们应该选择 $\\delta_j  0$。每当 $|\\delta_j|$ 增加一个单位，分数增加 $w_j \\delta_j = |w_j||\\delta_j|$。\n- 如果 $w_j = 0$，特征 $j$ 不能对分数做出贡献，不应被扰动，因为任何非零的 $\\delta_j$ 只会增加代价。\n\n对于任何 $w_j \\ne 0$ 的可变特征 $j$，我们可以定义一个分数生成的“效率”。扰动幅度 $|\\delta_j|$ 每增加一个单位，分数增加 $|w_j|$。每增加一个单位幅度的代价是 $c_j$。因此，分数-代价效率比为：\n$$\ne_j = \\frac{|w_j|}{c_j}\n$$\n为了以最小代价实现所需的分数增加 $S$，我们应该贪心地优先扰动效率最高的特征 $e_j$。这类似于分数背包问题，为了在给定的重量容量下最大化价值，首先用价值-重量比最高的物品填充背包。在这里，我们以每单位分数 $c_j/|w_j|$ 的价格“购买”分数增加；我们应该总是先从最便宜的来源（最高效率 $e_j$）购买。\n\n算法如下：\n1. 计算初始分数 $f_0 = w^\\top x + b$。如果 $f_0 \\ge 0$，代价为 $0$。\n2. 如果 $f_0  0$，计算所需的分数增加量 $S = -f_0$。\n3. 对于每个可变特征 $j$（其中 $m_j=1$ 且 $w_j \\ne 0$）：\n    a. 确定最大可能的有用扰动幅度 $\\Delta_j^{\\text{max}}$。\n        - 如果 $w_j > 0$，我们需要 $\\delta_j > 0$。允许的范围受 $U_j$ 约束。因此，最大的正 $\\delta_j$ 是 $\\max(0, U_j - x_j)$。令 $\\Delta_j^{\\text{max}} = \\max(0, U_j - x_j)$。\n        - 如果 $w_j  0$，我们需要 $\\delta_j  0$。允许的范围受 $L_j$ 约束。因此，负 $\\delta_j$ 的最大幅度是 $|\\min(0, L_j - x_j)|$。令 $\\Delta_j^{\\text{max}} = |\\min(0, L_j - x_j)|$。\n    b. 计算该特征能提供的最大分数增加量：$S_{j, \\text{max}} = |w_j| \\Delta_j^{\\text{max}}$。\n    c. 计算特征的效率：$e_j = |w_j|/c_j$。\n4. 检查可行性。计算总的可能分数增加量 $S_{\\text{total max}} = \\sum_j S_{j, \\text{max}}$。如果 $S_{\\text{total max}}  S$，则不可能满足约束。代价为 $+\\infty$。\n5. 如果可行，创建一个包含所有有用的可变特征（即 $\\Delta_j^{\\text{max}} > 0$ 的特征）的列表，并按其效率 $e_j$ 的降序排序。\n6. 遍历排序后的特征列表。维护一个运行的总代价 $C_{\\text{total}}$（初始化为 $0$）和剩余所需分数 $S_{\\text{rem}}$（初始化为 $S$）。\n    a. 对于当前特征 $j$，如果其最大分数贡献 $S_{j, \\text{max}}$ 小于 $S_{\\text{rem}}$：\n        - 完全利用该特征。扰动幅度为 $\\Delta_j^{\\text{max}}$。\n        - 将 $c_j \\Delta_j^{\\text{max}}$ 加到 $C_{\\text{total}}$。\n        - 从 $S_{\\text{rem}}$ 中减去 $S_{j, \\text{max}}$。\n    b. 如果 $S_{j, \\text{max}} \\ge S_{\\text{rem}}$：\n        - 部分利用该特征以满足剩余的分数要求。\n        - 所需的扰动幅度为 $|\\delta_j| = S_{\\text{rem}} / |w_j|$。\n        - 此部分扰动的代价为 $c_j |\\delta_j| = c_j (S_{\\text{rem}} / |w_j|) = S_{\\text{rem}} / e_j$。\n        - 将此代价加到 $C_{\\text{total}}$。\n        - 总要求现已满足。跳出循环。\n7. $C_{\\text{total}}$ 的最终值就是最小代价。\n\n这个贪心算法保证能找到最优解，因为该问题的结构允许从最高效的来源开始连续“填充”分数需求。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the adversarial SVM perturbation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general feasible case)\n        {\n            \"w\": np.array([0.5, -1.0, 0.2]),\n            \"b\": -0.3,\n            \"x\": np.array([0.4, 0.5, 0.0]),\n            \"L\": np.array([0.0, 0.0, 0.0]),\n            \"U\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([1.0, 2.0, 0.5]),\n            \"m\": np.array([1, 1, 1]),\n        },\n        # Case 2 (already approved)\n        {\n            \"w\": np.array([0.5, -1.0, 0.2]),\n            \"b\": -0.3,\n            \"x\": np.array([1.0, 0.1, 0.0]),\n            \"L\": np.array([0.0, 0.0, 0.0]),\n            \"U\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([1.0, 2.0, 0.5]),\n            \"m\": np.array([1, 1, 1]),\n        },\n        # Case 3 (infeasible)\n        {\n            \"w\": np.array([0.5, -1.0, 0.2]),\n            \"b\": -0.3,\n            \"x\": np.array([0.4, 0.5, 0.0]),\n            \"L\": np.array([0.0, 0.0, 0.0]),\n            \"U\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([1.0, 2.0, 0.5]),\n            \"m\": np.array([0, 0, 1]),\n        },\n        # Case 4 (subset mutable, feasible)\n        {\n            \"w\": np.array([0.5, -1.0, 0.2]),\n            \"b\": -0.3,\n            \"x\": np.array([0.4, 0.2, 0.9]),\n            \"L\": np.array([0.0, 0.0, 0.0]),\n            \"U\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([1.0, 2.0, 0.5]),\n            \"m\": np.array([1, 0, 0]),\n        },\n        # Case 5 (cost-sensitive)\n        {\n            \"w\": np.array([0.5, -1.0, 0.2]),\n            \"b\": -0.3,\n            \"x\": np.array([0.4, 0.5, 0.0]),\n            \"L\": np.array([0.0, 0.0, 0.0]),\n            \"U\": np.array([1.0, 1.0, 1.0]),\n            \"c\": np.array([10.0, 0.1, 1.0]),\n            \"m\": np.array([1, 1, 1]),\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        cost = calculate_min_cost(\n            params[\"w\"],\n            params[\"b\"],\n            params[\"x\"],\n            params[\"L\"],\n            params[\"U\"],\n            params[\"c\"],\n            params[\"m\"],\n        )\n        results.append(cost)\n    \n    # Format output as specified\n    print(\"[{}]\".format(\",\".join(map(str, results))))\n\ndef calculate_min_cost(w, b, x, L, U, c, m):\n    \"\"\"\n    Calculates the minimum cost to flip an SVM classification to 'approve'.\n    \"\"\"\n    # Calculate the initial decision score\n    initial_score = w.T @ x + b\n\n    # If already approved, cost is zero\n    if initial_score >= 0:\n        return 0.0\n\n    # Required score increase\n    required_score_increase = -initial_score\n\n    # Analyze mutable features\n    mutable_features = []\n    total_max_score_increase = 0.0\n\n    for j in range(len(x)):\n        # Consider only mutable features with non-zero weight\n        if m[j] == 1 and w[j] != 0:\n            max_perturbation_magnitude = 0.0\n            \n            # If w_j > 0, we want to increase x_j (delta_j > 0)\n            if w[j] > 0:\n                # The max positive delta is bounded by U_j\n                max_perturbation_magnitude = max(0, U[j] - x[j])\n            \n            # If w_j  0, we want to decrease x_j (delta_j  0)\n            else: # w[j]  0\n                # The max negative delta (in magnitude) is bounded by L_j\n                max_perturbation_magnitude = abs(min(0, L[j] - x[j]))\n\n            if max_perturbation_magnitude > 0:\n                efficiency = abs(w[j]) / c[j]\n                max_score_increase = abs(w[j]) * max_perturbation_magnitude\n                \n                mutable_features.append({\n                    \"index\": j,\n                    \"efficiency\": efficiency,\n                    \"max_score_increase\": max_score_increase,\n                    \"max_perturbation_magnitude\": max_perturbation_magnitude,\n                })\n                total_max_score_increase += max_score_increase\n\n    # Check for feasibility\n    if total_max_score_increase  required_score_increase:\n        return float('inf')\n\n    # Sort features by efficiency in descending order\n    mutable_features.sort(key=lambda f: f[\"efficiency\"], reverse=True)\n\n    # Greedily apply perturbations\n    total_cost = 0.0\n    score_rem = required_score_increase\n\n    for feature in mutable_features:\n        if score_rem = 0:\n            break\n\n        c_j = c[feature[\"index\"]]\n        w_j = w[feature[\"index\"]]\n\n        if feature[\"max_score_increase\"] >= score_rem:\n            # This feature is enough to achieve the goal\n            # Cost = (required score / score per unit) * cost per unit\n            #      = (score_rem / |w_j|) * c_j\n            #      = score_rem / efficiency\n            cost_to_add = score_rem / feature[\"efficiency\"]\n            total_cost += cost_to_add\n            score_rem = 0\n        else:\n            # Use this feature to its maximum extent\n            cost_to_add = c_j * feature[\"max_perturbation_magnitude\"]\n            total_cost += cost_to_add\n            score_rem -= feature[\"max_score_increase\"]\n\n    return total_cost\n\nsolve()\n```", "id": "2435491"}]}