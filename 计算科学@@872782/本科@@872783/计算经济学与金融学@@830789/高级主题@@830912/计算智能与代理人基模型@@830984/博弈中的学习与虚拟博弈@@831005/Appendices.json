{"hands_on_practices": [{"introduction": "要真正理解一个算法，最好的方法就是亲手实现它。这个练习将指导你为一个扩展版的“石头-剪刀-布”游戏——“石头-剪刀-布-蜥蜴-史波克”——编写一个虚构对策（fictitious play）模拟器。通过这个过程，你不仅能掌握虚构对策的核心机制，即如何根据对手过去的行为形成信念并做出最佳应对，还能学习到如何通过主成分分析（PCA）等技术来可视化高维信念轨迹的动态演化。[@problem_id:2405836]", "problem": "您需要设计并实现一个完整的、可运行的程序，该程序在一个双人零和的“石头-剪刀-布-蜥蜴-斯波克”(Rock-Paper-Scissors-Lizard-Spock)博弈中模拟确定性虚拟对局（deterministic fictitious play），记录一个玩家的五维信念轨迹，并在指定的时间快照生成可用于可视化的二维主成分坐标。最终输出必须是遵循精确指定格式的单行文本。\n\n基本依据和定义：\n- 考虑一个双人标准型零和博弈（two-player normal-form zero-sum game），其行动集 $\\mathcal{A}=\\{0,1,2,3,4\\}$ 分别对应于石头(Rock)、布(Paper)、剪刀(Scissors)、蜥蜴(Lizard)和斯波克(Spock)。\n- 设行玩家的支付矩阵（payoff matrix）为 $A\\in\\mathbb{R}^{5\\times 5}$，其条目 $A_{ij}\\in\\{-1,0,1\\}$ 满足反对称性（anti-symmetry）$A=-A^{\\top}$ 且对所有 $i$ 都有 $A_{ii}=0$。优势关系由有序对集合给出\n$$\n\\mathcal{W}=\\{(0,2),(0,3),(1,0),(1,4),(2,1),(2,3),(3,4),(3,1),(4,2),(4,0)\\},\n$$\n这意味着如果 $(i,j)\\in\\mathcal{W}$，则行动 $i$ 击败行动 $j$，因此 $A_{ij}=1$ 且 $A_{ji}=-1$；平局则得到 $A_{ii}=0$。\n- 确定性虚拟对局在离散时期 $t=1,2,\\dots,T$ 中进行。每个玩家根据对手过去行动的经验频率（empirical frequency）形成信念，然后选择一个对此信念的纯粹最佳响应（pure best response）。设行玩家对列玩家在时期 $t$ 的混合策略的信念是从列玩家在时期 $\\{1,\\dots,t-1\\}$ 的行动计算出的经验频率向量 $q_t\\in\\Delta^4$。类似地，设列玩家对行玩家在时期 $t$ 的混合策略的信念是从行玩家在时期 $\\{1,\\dots,t-1\\}$ 的行动计算出的 $p_t\\in\\Delta^4$。行玩家针对信念 $q_t$ 的期望支付向量为 $v_t=A\\,q_t$，纯粹最佳响应是任何 $i\\in\\arg\\max_{k\\in\\mathcal{A}} (v_t)_k$。列玩家旨在最小化行玩家的支付；给定 $p_t$ 时，其期望行支付向量为 $w_t=p_t^{\\top}A\\in\\mathbb{R}^5$，纯粹最佳响应是任何 $j\\in\\arg\\min_{\\ell\\in\\mathcal{A}} (w_t)_{\\ell}$。您必须实现确定性平局打破规则（deterministic tie-breaking），即始终在最佳响应的 argmax 或 argmin 集合中选择最小的索引。\n- 设行玩家的信念轨迹（belief trajectory）为序列 $(b_t)_{t=1}^T$，其中 $b_t\\in\\Delta^4$ 是列玩家在时期 $\\{1,\\dots,t\\}$ 内行动的经验频率。这是一个位于 $5$-单纯形（$5$-simplex）中的五维时间序列。\n\n需要实现的计算任务：\n1. 使用优势集合 $\\mathcal{W}$ 构建支付矩阵 $A$。\n2. 从时期 $t=1$ 行玩家和列玩家指定的初始纯粹行动开始，模拟 $T$ 个时期的确定性虚拟对局。对于每个时期 $t\\ge 2$，双方玩家同时根据对手在时期 $\\{1,\\dots,t-1\\}$ 的行动形成的信念，并采用指定的确定性平局打破规则，选择他们的纯粹最佳响应行动。记录行玩家信念的完整序列 $(b_t)_{t=1}^T$。\n3. 定义均匀混合策略（uniform mixed strategy）$u=\\left(\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5},\\frac{1}{5}\\right)$。计算最终的欧几里得距离（Euclidean distance）$\\|b_T-u\\|_2$。\n4. 为了“可视化”，计算信念轨迹的二维主成分投影。设 $X\\in\\mathbb{R}^{T\\times 5}$ 是一个矩阵，其第 $t$ 行为 $b_t^{\\top}$。通过减去列均值来中心化 $X$，得到 $X_c$。使用奇异值分解（singular value decomposition, SVD）$X_c=U\\Sigma V^{\\top}$，其中奇异值位于 $\\Sigma$ 的对角线上，将前两个主成分解释的方差分数（fraction of variance explained）定义为\n$$\n\\rho_2=\\frac{\\sigma_1^2+\\sigma_2^2}{\\sum_{k}\\sigma_k^2},\n$$\n约定如果 $\\sum_k \\sigma_k^2=0$，则 $\\rho_2=0$。将时间 $t$ 的二维坐标定义为 $U_{[:,1:2]}\\,\\mathrm{diag}(\\sigma_1,\\sigma_2)$ 的第 $t$ 行，即在前两个主成分上的得分。\n5. 在指定的采样时间返回二维坐标，作为“可视化就绪”数据。\n\n测试套件：\n为以下三组参数集实现上述过程。对于每种情况，使用提供的时期数、初始行动和用于投影输出的采样时间。行动索引遵循：石头(Rock) $=0$，布(Paper) $=1$，剪刀(Scissors) $=2$，蜥蜴(Lizard) $=3$，斯波克(Spock) $=4$。\n- 情况 $1$ (一般“顺利路径”)：$T=400$，初始行行动 $=0$，初始列行动 $=1$，采样时间 $=\\{1,2,50,200,400\\}$。\n- 情况 $2$ (小样本边界)：$T=5$，初始行行动 $=0$，初始列行动 $=0$，采样时间 $=\\{1,2,5\\}$。\n- 情况 $3$ (更长的时间范围，不同的起点)：$T=1000$，初始行行动 $=4$，初始列行动 $=2$，采样时间 $=\\{1,10,100,500,1000\\}$。\n\n每个测试用例的必需输出：\n- 设 $d=\\|b_T-u\\|_2$ 为与均匀策略的最终欧几里得距离。\n- 设 $\\rho_2$ 为如上定义的前两个主成分解释的方差分数。\n- 设 $C$ 为在指定采样时间的二维坐标列表，其顺序与采样时间的顺序相同，其中每个坐标是一对 $[c_{t,1},c_{t,2}]$。\n\n最终输出格式：\n- 您的程序必须生成一个单行文本，其中包含一个用方括号括起来的、由逗号分隔的三个测试用例结果的列表。每个测试用例的结果必须是 $[d,\\rho_2,C]$ 形式的列表，其中 $C$ 是一个包含 2 元素列表的列表。例如，一个语法上有效的总输出形式为\n$[[d_1,\\rho_{2,1},C_1],[d_2,\\rho_{2,2},C_2],[d_3,\\rho_{2,3},C_3]]$。\n- 程序没有输入；测试套件是硬编码的。\n- 角度和物理单位不适用；所有输出都是无单位的实数。", "solution": "问题陈述已经过严格验证，并被确定为有效。它在博弈论和统计学领域有科学依据，问题设定良好(well-posed)且具有唯一的确定性解，其定义和要求是客观的。所有必要的数据和条件都已提供，并且没有内部矛盾。因此，我们可以着手解决。\n\n该问题要求为一个双人零和的“石头-剪刀-布-蜥蜴-斯波克”博弈实现一个确定性虚拟对局的模拟。这个过程涉及构建博弈的支付矩阵，模拟在指定时期数内的学习动态，并使用主成分分析(Principal Component Analysis, PCA)来分析一个玩家产生的信念轨迹。\n\n首先，我们构建行玩家的支付矩阵 $A \\in \\mathbb{R}^{5 \\times 5}$。行动集为 $\\mathcal{A} = \\{0, 1, 2, 3, 4\\}$。问题提供了输赢对的集合 $\\mathcal{W}$。对于每个对 $(i, j) \\in \\mathcal{W}$，行动 $i$ 击败行动 $j$，产生支付 $A_{ij} = 1$。该博弈是零和的，这意味着反对称性 $A = -A^\\top$，因此 $A_{ji} = -A_{ij} = -1$。平局导致零支付，$A_{ii} = 0$。这些规则唯一地定义了矩阵 $A$。\n\n其次，我们模拟 $T$ 个时期的虚拟对局动态。模拟从时期 $t=1$ 给定的初始行动 $i_1$ 和 $j_1$ 开始。对于每个后续时期 $t \\in \\{2, \\dots, T\\}$，玩家形成信念并选择最佳响应。行玩家在时间 $t$ 的信念，表示为 $q_t \\in \\Delta^4$，是列玩家在所有先前时期 $\\{1, \\dots, t-1\\}$ 中行动的经验频率。行玩家选择行动 $k \\in \\mathcal{A}$ 的期望支付是向量 $v_t = A q_t$ 的第 $k$ 个分量。然后，行玩家选择一个能最大化此支付的行动 $i_t$：\n$$\ni_t \\in \\arg\\max_{k \\in \\mathcal{A}} (A q_t)_k\n$$\n类似地，列玩家根据行玩家的过去行动形成信念 $p_t \\in \\Delta^4$。列玩家旨在最小化行玩家的支付。列玩家的每个选择 $\\ell \\in \\mathcal{A}$ 的期望支付由向量 $w_t = p_t^\\top A$ 给出。列玩家选择一个能最小化此值的行动 $j_t$：\n$$\nj_t \\in \\arg\\min_{\\ell \\in \\mathcal{A}} (p_t^\\top A)_\\ell\n$$\n强制执行严格的平局打破规则：在有多个最佳响应的情况下，选择索引最小的行动。这使得模拟完全是确定性的。\n\n第三，我们分析行玩家的信念轨迹 $(b_t)_{t=1}^T$。注意，用于分析的信念 $b_t$ 定义为列玩家在时期 $\\{1, \\dots, t\\}$ 内行动的经验频率，这包括了时间 $t$ 的行动。这与在时间 $t$ 用于决策的信念 $q_t$ 不同。最终的信念偏差通过欧几里得距离 $d = \\|b_T - u\\|_2$ 来衡量，其中 $u = (\\frac{1}{5}, \\dots, \\frac{1}{5})^\\top$ 是均匀混合策略。\n\n第四，我们对信念轨迹执行 PCA 以获得二维表示。该轨迹被组织成一个数据矩阵 $X \\in \\mathbb{R}^{T \\times 5}$，其中第 $t$ 行是 $b_t^\\top$。通过从每行减去列均值向量来中心化数据，得到 $X_c$。中心化矩阵的 SVD, $X_c = U \\Sigma V^\\top$, 提供了主成分。位于 $\\Sigma$ 对角线上的奇异值 $\\sigma_k$ 用于计算前两个主成分解释的方差分数：\n$$\n\\rho_2 = \\frac{\\sigma_1^2 + \\sigma_2^2}{\\sum_{k=1}^{\\text{rank}(X_c)} \\sigma_k^2}\n$$\n如果总方差 $\\sum_k \\sigma_k^2 = 0$，我们设 $\\rho_2=0$。时间 $t$ 的二维坐标，也称为得分(scores)，由矩阵乘积 $U_{[:,1:2]} \\mathrm{diag}(\\sigma_1, \\sigma_2)$ 的第 $t$ 行给出。这代表了中心化数据在前两个主成分轴上的投影。\n\n最后，对于每个测试用例，我们计算距离 $d$、方差分数 $\\rho_2$ 以及在指定采样时间的二维坐标列表 $C$。然后将这些结果格式化为所需的输出结构。", "answer": "```python\nimport numpy as np\n\ndef run_case(T, initial_row_action, initial_col_action, sample_times):\n    \"\"\"\n    Runs a single test case for the fictitious play simulation.\n    \"\"\"\n    # Task 1: Build the payoff matrix A\n    A = np.zeros((5, 5), dtype=np.float64)\n    dominance_pairs = [\n        (0, 2), (0, 3), (1, 0), (1, 4), (2, 1),\n        (2, 3), (3, 4), (3, 1), (4, 2), (4, 0)\n    ]\n    for i, j in dominance_pairs:\n        A[i, j] = 1.0\n        A[j, i] = -1.0\n\n    # Task 2: Simulate deterministic fictitious play\n    row_actions = [initial_row_action]\n    col_actions = [initial_col_action]\n\n    row_counts = np.zeros(5, dtype=np.float64)\n    row_counts[initial_row_action] = 1.0\n    col_counts = np.zeros(5, dtype=np.float64)\n    col_counts[initial_col_action] = 1.0\n\n    for t in range(2, T + 1):\n        # Beliefs for period t are based on actions from 1 to t-1\n        q_t = col_counts / (t - 1)\n        p_t = row_counts / (t - 1)\n\n        # Row player's best response (maximizes payoff)\n        row_expected_payoffs = A @ q_t\n        i_t = np.argmax(row_expected_payoffs)\n\n        # Column player's best response (minimizes row's payoff)\n        col_expected_payoffs = p_t @ A\n        j_t = np.argmin(col_expected_payoffs)\n\n        # Record actions and update counts\n        row_actions.append(i_t)\n        col_actions.append(j_t)\n        row_counts[i_t] += 1.0\n        col_counts[j_t] += 1.0\n\n    # Compute row player's belief trajectory (b_t)\n    # b_t is the empirical frequency of column actions up to time t.\n    X = np.zeros((T, 5), dtype=np.float64)\n    temp_col_counts = np.zeros(5, dtype=np.float64)\n    for t in range(T):\n        action_idx = col_actions[t]\n        temp_col_counts[action_idx] += 1.0\n        X[t, :] = temp_col_counts / (t + 1)\n    \n    # Task 3: Compute final Euclidean distance from uniform strategy\n    b_T = X[-1, :]\n    u = np.full(5, 1.0/5.0)\n    d = np.linalg.norm(b_T - u)\n\n    # Task 4: Perform Principal Component Analysis\n    X_c = X - X.mean(axis=0)\n\n    # Use SVD for PCA. full_matrices=False is efficient.\n    try:\n        U, S, Vh = np.linalg.svd(X_c, full_matrices=False)\n    except np.linalg.LinAlgError: # Safety for unusual matrices, though not expected here\n        U, S, Vh = np.zeros((T,1)), np.array([]), np.zeros((1,5))\n\n    # Calculate fraction of variance explained by first two PCs\n    s2 = S**2\n    total_variance = np.sum(s2)\n    rho_2 = 0.0\n    if total_variance > 1e-15:\n        rho_2 = np.sum(s2[:2]) / total_variance\n\n    # Calculate 2D coordinates (scores on first two PCs)\n    scores = np.zeros((T, 2))\n    num_components = min(2, S.shape[0])\n    if num_components > 0:\n        scores[:, :num_components] = U[:, :num_components] * S[:num_components]\n\n    # Task 5: Return coordinates at specified sample times\n    # Sample times are 1-indexed, so we access index t-1\n    C = [scores[t - 1].tolist() for t in sample_times]\n\n    return [d, rho_2, C]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and generate the final output.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        {'T': 400, 'initial_row_action': 0, 'initial_col_action': 1, 'sample_times': [1, 2, 50, 200, 400]},\n        {'T': 5, 'initial_row_action': 0, 'initial_col_action': 0, 'sample_times': [1, 2, 5]},\n        {'T': 1000, 'initial_row_action': 4, 'initial_col_action': 2, 'sample_times': [1, 10, 100, 500, 1000]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(\n            case['T'],\n            case['initial_row_action'],\n            case['initial_col_action'],\n            case['sample_times']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation includes spaces, which we remove.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "2405836"}, {"introduction": "在掌握了虚构对策的基本原理后，一个自然而然的问题是：如果你知道你的对手正在使用这个学习算法，你该如何利用这一点来最大化自己的收益？这个练习让你扮演一个“老练的”参与者，你的对手是一个可预测的虚构对策学习者。你需要运用动态规划来计算你的最优策略，这要求你不仅要考虑当前的收益，还要预测你的行动将如何影响对手未来的信念和行为。[@problem_id:2405888]", "problem": "考虑一个重复的双人双行动策略形式博弈，该博弈在由 $t \\in \\{1,2,\\dots,T\\}$ 索引的 $T$ 个离散时期的有限范围内进行。参与者 $1$（精明参与者）从 $\\{0,1\\}$ 中选择行动，并寻求在整个博弈期间最大化阶段收益之和。参与者 $2$（对手）从 $\\{0,1\\}$ 中选择行动，并遵循经典虚拟对局：在每个时期 $t$ 之前，参与者 $2$ 基于初始先验和参与者 $1$ 过去行动的经验频率，形成关于参与者 $1$ 选择行动 $0$ 的概率的信念，然后对该信念采取最佳响应，以最大化参与者 $2$ 自身的期望阶段收益。参与者 $1$ 知道参与者 $2$ 的学习规则和收益矩阵，并选择行动以对该学习过程做出最佳响应，同时预测当前行动对参与者 $2$ 未来信念的影响。\n\n单次阶段收益如下。设 $U_1$ 为参与者 $1$ 的 $2 \\times 2$ 收益矩阵， $U_2$ 为参与者 $2$ 的 $2 \\times 2$ 收益矩阵，其中行索引对应参与者 $1$ 的行动，列索引对应参与者 $2$ 的行动。如果在某个时期，参与者 $1$ 选择行动 $a_1 \\in \\{0,1\\}$，参与者 $2$ 选择行动 $a_2 \\in \\{0,1\\}$，那么参与者 $1$ 获得收益 $U_1[a_1,a_2]$，参与者 $2$ 获得收益 $U_2[a_1,a_2]$。\n\n参与者 $2$ 在时期 $t$ 开始时的虚拟对局信念由下式给出\n$$\np_t \\equiv \\frac{\\alpha_0 + n_0(t-1)}{\\alpha_0 + \\alpha_1 + (t-1)},\n$$\n其中 $\\alpha_0,\\alpha_1 \\in \\mathbb{N}_0$ 分别是行动 $0$ 和 $1$ 的固定先验伪计数，而 $n_0(t-1)$ 是参与者 $1$ 在时期 $1$ 到 $t-1$ 中选择行动 $0$ 的总次数。假设 $\\alpha_0 + \\alpha_1 \\ge 1$，以确保在 $t=1$ 时分母为严格正数。在每个时期 $t$，参与者 $2$ 选择一个最佳响应行动\n$$\na_2(t) \\in \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ \\mathbb{E}[U_2[a_1,b] \\mid a_1 \\sim \\text{Bernoulli}(p_t)] \\right\\} \n= \\arg\\max_{b \\in \\{0,1\\}} \\left\\{ p_t \\cdot U_2[0,b] + (1-p_t)\\cdot U_2[1,b] \\right\\}.\n$$\n如果 argmax 不是唯一的，参与者 $2$ 通过选择较小的行动索引来打破平局，即在平局情况下 $a_2(t)=0$。\n\n在每个时期 $t$，事件的顺序如下：\n- 根据先验和参与者 $1$ 过去的行动形成信念 $p_t$。\n- 参与者 $2$ 根据上述定义，选择 $a_2(t)$作为对 $p_t$ 的最佳响应。\n- 参与者 $1$ 选择一个纯行动 $a_1(t) \\in \\{0,1\\}$。\n- 收益 $U_1[a_1(t),a_2(t)]$ 和 $U_2[a_1(t),a_2(t)]$ 得以实现。\n- 计数 $n_0(t)$ 通过 $n_0(t)=n_0(t-1)+\\mathbf{1}\\{a_1(t)=0\\}$ 进行更新。\n\n参与者 $1$ 观察整个历史，并自适应地选择行动以最大化未贴现总收益\n$$\n\\sum_{t=1}^{T} U_1[a_1(t),a_2(t)],\n$$\n同时预测参与者 $2$ 的信念更新和最佳响应。\n\n你的任务是编写一个程序，对于下面的每个测试用例，计算在上述动态下，参与者 $1$ 在整个博弈期间 $T$ 内可获得的最大总收益。假设参与者 $1$ 在每个时期都可以承诺采取任何纯行动，同时完全预测到参与者 $2$ 的虚拟对局行为和打破平局的规则。\n\n问题中不涉及物理量或角度，因此不需要单位。所有输出必须是精确的整数。\n\n输入不会提供给你的程序；相反，你的程序必须在内部使用以下测试套件，其中每个用例都指定了 $U_1$、$U_2$、博弈期 $T$ 和先验 $(\\alpha_0,\\alpha_1)$：\n\n- 测试用例 $1$（操纵有利）：\n  - $U_1 = \\begin{bmatrix} 2  0 \\\\ 1  1 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix}$,\n  - $T = 5$,\n  - $(\\alpha_0,\\alpha_1) = (0,1)$.\n\n- 测试用例 $2$（零和匹配硬币）：\n  - $U_1 = \\begin{bmatrix} 1  -1 \\\\ -1  1 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} -1  1 \\\\ 1  -1 \\end{bmatrix}$,\n  - $T = 6$,\n  - $(\\alpha_0,\\alpha_1) = (1,1)$.\n\n- 测试用例 $3$（开局时边界平局，协调博弈）：\n  - $U_1 = \\begin{bmatrix} 1  0 \\\\ 0  0 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} 2  0 \\\\ 0  2 \\end{bmatrix}$,\n  - $T = 3$,\n  - $(\\alpha_0,\\alpha_1) = (1,1)$.\n\n- 测试用例 $4$（阈值恰好在先验处）：\n  - $U_1 = \\begin{bmatrix} 2  0 \\\\ 1  1 \\end{bmatrix}$,\n  - $U_2 = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix}$,\n  - $T = 2$,\n  - $(\\alpha_0,\\alpha_1) = (2,3)$.\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与测试用例相同，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是测试用例 $i$ 的最大总收益（一个整数）。", "solution": "该问题已经过验证并被认为是有效的。这是一个计算博弈论中定义明确的问题，具体地为一个精明参与者针对使用虚拟对局学习规则的对手建立最优策略模型。该设定在数学上是一致的、自洽的，并且没有科学或逻辑上的缺陷。此问题可以使用动态规划算法求解。\n\n该问题要求为参与者 $1$ 找到最优策略，他是一个精明的代理人，在 $T$ 个时期的有限范围内面对一个可预测的对手，即参与者 $2$。参与者 $1$ 的行动会影响参与者 $2$ 的未来信念，从而影响其未来的行动。这是一个确定性环境下的序贯决策问题，因为参与者 $2$ 的行为是确定性的，这使其适合通过动态规划和逆向归纳法来解决。\n\n令系统在时期 $t \\in \\{1, 2, \\dots, T\\}$ 开始时的状态由参与者 $1$ 的行动历史定义。历史中唯一影响未来动态的方面是参与者 $1$ 选择行动 $0$ 的次数。因此，一个充分的状态表示是数对 $(t, k)$，其中 $t$ 是当前时期编号，$k = n_0(t-1)$ 是参与者 $1$ 在时期 $1$ 到 $t-1$ 中选择行动 $0$ 的次数。参与者 $1$ 选择行动 $1$ 的次数是 $(t-1)-k$。\n\n令 $V(t, k)$ 表示在系统处于状态 $(t, k)$ 的情况下，参与者 $1$ 从时期 $t$ 到博弈期结束 $T$ 所能获得的最大总收益。我们的目标是计算 $V(1, 0)$，因为在博弈开始时（$t=1$），还没有采取任何行动（$k=0$）。\n\n动态规划的解法通过逆向归纳进行，从最后一个时期开始。\n\n基本情况是博弈期结束后的时期。在 $t=T+1$ 时，博弈结束，不能再累积任何收益。\n$$V(T+1, k) = 0 \\quad \\text{for all valid } k \\in \\{0, 1, \\dots, T\\}.$$\n\n对于任何时期 $t \\in \\{T, T-1, \\dots, 1\\}$ 和任何有效状态 $(t, k)$（其中 $k \\in \\{0, 1, \\dots, t-1\\}$），我们可以为 $V(t, k)$ 写出贝尔曼方程。首先，我们确定参与者 $2$ 的行动 $a_2(t)$，它是状态 $(t, k)$ 的一个确定性函数。参与者 $2$ 关于参与者 $1$ 会选择行动 $0$ 的信念是：\n$$p_t = \\frac{\\alpha_0 + k}{\\alpha_0 + \\alpha_1 + t - 1}.$$\n参与者 $2$ 选择 $a_2(t) \\in \\{0,1\\}$ 以最大化其期望收益。如果选择 $0$ 的期望收益大于或等于（由于平局打破规则）选择 $1$ 的期望收益，他们将选择行动 $0$：\n$p_t \\cdot U_2[0,0] + (1-p_t)\\cdot U_2[1,0] \\ge p_t \\cdot U_2[0,1] + (1-p_t)\\cdot U_2[1,1]$。\n为避免浮点数运算和潜在的精度误差，我们可以使用整数来表示这个不等式。令 $N_t = \\alpha_0 + k$ 和 $D_t = \\alpha_0 + \\alpha_1 + t - 1$，所以 $p_t = N_t / D_t$。由于 $D_t > 0$，我们可以在不等式两边乘以 $D_t$ 并重新整理得到：\n$N_t \\cdot (U_2[0,0] - U_2[1,0] - U_2[0,1] + U_2[1,1]) \\ge D_t \\cdot (U_2[1,1] - U_2[1,0])$。\n这个比较只涉及整数运算，并完全确定了任何状态 $(t, k)$ 下的 $a_2(t)$。\n\n给定 $a_2(t)$，参与者 $1$ 选择 $a_1(t) \\in \\{0,1\\}$ 以最大化当前时期的收益与后续状态的价值之和。贝尔曼方程是：\n$$ V(t, k) = \\max \\left\\{\n    \\underbrace{U_1[0, a_2(t)] + V(t+1, k+1)}_{\\text{Value if } a_1(t)=0},\n    \\underbrace{U_1[1, a_2(t)] + V(t+1, k)}_{\\text{Value if } a_1(t)=1}\n\\right\\}. $$\n在这里，如果参与者 $1$ 选择行动 $0$，下一个时期 $t+1$ 的零计数变为 $k+1$。如果参与者 $1$ 选择行动 $1$，则计数保持为 $k$。\n\n该算法包括通过从 $T$ 向下迭代到 $1$ 的 $t$，并对每个 $t$ 从 $0$ 迭代到 $t-1$ 的 $k$，来计算所有相关状态的 $V(t, k)$。可以使用一个二维数组来存储计算出的 $V(t, k)$ 的值，这种技术称为记忆化。最终答案是 $V(1, 0)$ 的值。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the repeated game problem for a sophisticated player against a \n    fictitious play opponent for a given set of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: manipulation beneficial\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 5,\n            \"prior\": (0, 1),\n        },\n        # Case 2: zero-sum matching pennies\n        {\n            \"U1\": np.array([[1, -1], [-1, 1]], dtype=int),\n            \"U2\": np.array([[-1, 1], [1, -1]], dtype=int),\n            \"T\": 6,\n            \"prior\": (1, 1),\n        },\n        # Case 3: boundary tie at the start, coordination\n        {\n            \"U1\": np.array([[1, 0], [0, 0]], dtype=int),\n            \"U2\": np.array([[2, 0], [0, 2]], dtype=int),\n            \"T\": 3,\n            \"prior\": (1, 1),\n        },\n        # Case 4: threshold exactly at the prior\n        {\n            \"U1\": np.array([[2, 0], [1, 1]], dtype=int),\n            \"U2\": np.array([[3, 0], [0, 2]], dtype=int),\n            \"T\": 2,\n            \"prior\": (2, 3),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        U1 = case[\"U1\"]\n        U2 = case[\"U2\"]\n        T = case[\"T\"]\n        alpha0, alpha1 = case[\"prior\"]\n\n        # V[t][k] stores the max payoff from period t to T, given that Player 1\n        # has played action 0 a total of k times up to period t-1.\n        # Dimensions are (T+2) x (T+1) to handle t=T+1 and k=T.\n        V = np.zeros((T + 2, T + 1), dtype=int)\n\n        # Precompute constants for Player 2's best response to avoid redundant calculations.\n        # P2 plays 0 if: N * D_U2 >= D * C_U2\n        D_U2 = (U2[0, 0] - U2[1, 0]) - (U2[0, 1] - U2[1, 1])\n        C_U2 = U2[1, 1] - U2[1, 0]\n\n        # Backward induction from period T down to 1\n        for t in range(T, 0, -1):\n            # In period t, t-1 rounds have passed, so k (count of '0's) can be from 0 to t-1\n            for k in range(t):\n                # Belief p_t = N/D. We use integer arithmetic to determine P2's action.\n                N = alpha0 + k\n                D = alpha0 + alpha1 + t - 1\n                \n                # Determine Player 2's action a2_t\n                # Tie-breaking rule: P2 chooses action 0 in case of indifference.\n                # This is handled by '>=' in the main comparison.\n                \n                a2_t = 0 # Default action\n                if D_U2 > 0:\n                    if not (N * D_U2 >= D * C_U2):\n                        a2_t = 1\n                elif D_U2  0:\n                    if not (N * D_U2 = D * C_U2):  # Inequality flips\n                        a2_t = 1\n                else: # D_U2 == 0, p_t does not affect choice\n                    if not (0 = C_U2):\n                        a2_t = 1\n                \n                # Player 1's Bellman equation\n                # Value if P1 chooses action 0\n                val_if_0 = U1[0, a2_t] + V[t + 1, k + 1]\n                \n                # Value if P1 chooses action 1\n                val_if_1 = U1[1, a2_t] + V[t + 1, k]\n                \n                V[t, k] = max(val_if_0, val_if_1)\n        \n        # The result is the value at the beginning of the game (t=1, k=0)\n        results.append(V[1, 0])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2405888"}, {"introduction": "虚构对策的一个核心假设是对手的策略是平稳的，但现实世界中的对手往往会改变策略。这个练习旨在通过对比，加深你对不同学习算法适用场景的理解。你将模拟虚构对策和另一种强大的在线学习算法——Hedge（指数权重）算法，让它们在经典的“石头-剪刀-布”游戏中对抗一个非平稳的对手，并使用“遗憾”（regret）等关键指标来量化和比较它们的表现。[@problem_id:2405816]", "problem": "假设有一个标准的零和石头-剪刀-布博弈，其范式博弈中行参与者的收益矩阵为\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0  -1  1 \\\\\n1  0  -1 \\\\\n-1  1  0\n\\end{pmatrix},\n$$\n其中，行动索引 $1$、$2$ 和 $3$ 分别对应于石头、布和剪刀。行参与者（学习者）面对一个确定性的、非平稳的对手（列参与者），该对手在每一轮 $t\\in\\{1,\\dots,T\\}$ 选择一个纯行动 $j(t)\\in\\{1,2,3\\}$。互动持续 $T$ 轮。\n\n需要评估学习者的两种学习规则：\n\n$1.$ 虚拟博弈 (Fictitious play)。当 $t=1$ 时，选择行动 $i(1)=1$。当 $t\\ge 2$ 时，令 $\\hat{q}(t-1)\\in\\mathbb{R}^3$ 为对手截至第 $t-1$ 期过去行动的经验频率向量，即对每个 $k\\in\\{1,2,3\\}$，$\\hat{q}_k(t-1)=\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$。选择一个纯最佳响应 $i(t)\\in\\arg\\max_{i\\in\\{1,2,3\\}} \\left(A\\,\\hat{q}(t-1)\\right)_i$，并通过选择最小索引来打破平局。在第 $t$ 轮的实现收益为 $r^{\\mathrm{FP}}(t)=A_{i(t),\\,j(t)}$。定义经验混合策略 $\\bar{p}^{\\mathrm{FP}}\\in\\mathbb{R}^3$ 为 $\\bar{p}^{\\mathrm{FP}}_i=\\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$。\n\n$2.$ Hedge (指数权重) 算法，带有完全信息反馈。初始化权重 $w_i(1)=1$，对每个 $i\\in\\{1,2,3\\}$。在每一轮 $t\\in\\{1,\\dots,T\\}$，构建混合策略 $p_i(t)=\\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$。令 $g(t)\\in\\mathbb{R}^3$ 为对抗对手已实现的纯行动 $j(t)$ 的反事实收益向量，即 $g_i(t)=A_{i,\\,j(t)}$。期望收益为 $r^{\\mathrm{H}}(t)=\\sum_{i=1}^3 p_i(t)\\,g_i(t)$。通过 $w_i(t+1)=w_i(t)\\,\\exp\\!\\left(\\eta\\,g_i(t)\\right)$ 更新权重，其中学习率 $\\eta=\\sqrt{\\frac{2\\ln n}{T}}$，$n=3$。定义时间平均策略 $\\bar{p}^{\\mathrm{H}}\\in\\mathbb{R}^3$ 为 $\\bar{p}^{\\mathrm{H}}=\\frac{1}{T}\\sum_{t=1}^T p(t)$。\n\n对于这两种学习规则，在时间范围 $T$ 计算两个汇总统计量：\n\n$1.$ 平均外部遗憾，\n$$\n\\frac{R_T}{T}\n\\;=\\;\n\\frac{1}{T}\\left(\n\\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t)\n\\;-\\;\n\\sum_{t=1}^T r(t)\n\\right),\n$$\n其中，对于虚拟博弈，$r(t)=r^{\\mathrm{FP}}(t)$；对于 Hedge 算法，$r(t)=r^{\\mathrm{H}}(t)$。在两种情况下，$g_i(t)=A_{i,\\,j(t)}$。\n\n$2.$ 学习者的时间平均策略与唯一的混合策略纳什均衡 $u^\\star=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$ 的 $\\ell_1$ 距离，即\n$$\nD(\\bar{p})\n\\;=\\;\n\\sum_{i=1}^3 \\left| \\bar{p}_i - \\tfrac{1}{3} \\right|.\n$$\n\n测试套件。您的程序必须在以下四个对手时间表上评估这两种学习规则，每个时间表都由一个时间范围 $T$ 和一个分段常数的纯行动时间表指定：\n\n$1.$ 情况 A (理想路径): $T=90$。对手先玩30轮石头，然后玩30轮布，再玩30轮剪刀，即 $j(t)=1$ 对 $t\\in\\{1,\\dots,30\\}$，$j(t)=2$ 对 $t\\in\\{31,\\dots,60\\}$，以及 $j(t)=3$ 对 $t\\in\\{61,\\dots,90\\}$。\n\n$2.$ 情况 B (边界情况): $T=3$。对手在 $t=1$ 时玩石头，在 $t=2$ 时玩布，在 $t=3$ 时玩剪刀，即 $j(1)=1$，$j(2)=2$，$j(3)=3$。\n\n$3.$ 情况 C (非平稳边缘情况): $T=60$。对手先玩20轮石头，然后玩10轮布，再玩20轮剪刀，最后玩10轮布，即 $j(t)=1$ 对 $t\\in\\{1,\\dots,20\\}$，$j(t)=2$ 对 $t\\in\\{21,\\dots,30\\}$，$j(t)=3$ 对 $t\\in\\{31,\\dots,50\\}$，以及 $j(t)=2$ 对 $t\\in\\{51,\\dots,60\\}$。\n\n$4.$ 情况 D (平稳边缘情况): $T=60$。对手在所有轮次都玩石头，即 $j(t)=1$ 对所有 $t\\in\\{1,\\dots,60\\}$。\n\n所需输出。对于每种情况，生成一个包含四个实数的列表\n$$\n\\big[\\, \\tfrac{R_T^{\\mathrm{H}}}{T},\\;\\tfrac{R_T^{\\mathrm{FP}}}{T},\\; D(\\bar{p}^{\\mathrm{H}}),\\; D(\\bar{p}^{\\mathrm{FP}})\\,\\big],\n$$\n其中上标 $\\mathrm{H}$ 和 $\\mathrm{FP}$ 分别表示 Hedge 算法和虚拟博弈。每个数字必须四舍五入到 $6$ 位小数。您的程序应生成单行输出，其中包含一个以逗号分隔的四元素列表的列表，用方括号括起来，不含任何额外文本。例如，\n$$\n[\\,[x_1,x_2,x_3,x_4],[y_1,y_2,y_3,y_4],\\dots]\\,,\n$$\n其中每个 $x_k$ 和 $y_k$ 都是四舍五入到 $6$ 位的小数。此问题不涉及任何物理单位，所有角度均无关。不得有任何输入；仅针对指定的测试套件计算输出。", "solution": "该问题是有效的。它提出了一个在博弈论和在线学习领域（计算经济学的一个子领域）中定义明确的计算任务。所有参数、算法和评估指标都得到了足够精确的指定，从而可以得到唯一且可验证的解。该问题具有科学依据，使用了标准模型（石头-剪刀-布）、算法（虚拟博弈、Hedge）和性能指标（遗憾值、与纳什均衡的距离）。\n\n任务是为一个在石头-剪刀-布博弈中对抗一个确定性的、非平稳的列参与者的行参与者，模拟两种学习算法：虚拟博弈（Fictitious Play）和 Hedge。我们已知行参与者的收益矩阵 $A$：\n$$\nA = \\begin{pmatrix} 0  -1  1 \\\\ 1  0  -1 \\\\ -1  1  0 \\end{pmatrix}\n$$\n其中行和列分别对应于行动石头（索引 $1$）、布（索引 $2$）和剪刀（索引 $3$）。该模拟将针对四种不同的预定义对手行动时间表 $j(t)$ 运行 $T$ 轮。\n\n解决方案涉及实现这两种算法和指定的汇总统计量。模拟以迭代方式进行，从 $t = 1$ 到 $T$。\n\n**1. 虚拟博弈 (Fictitious Play, FP) 算法**\n\n虚拟博弈是一种迭代学习规则，其中参与者假设其对手正在玩一个平稳的混合策略，该策略通过其过去行动的经验频率来估计。然后，该参与者选择对这个估计策略的纯最佳响应。\n\n- **初始化 ($t=1$):** 按规定，学习者玩石头，所以 $i(1)=1$。\n- **迭代 ($t \\ge 2$):**\n    1.  学习者计算对手截至第 $t-1$ 轮的行动的经验频率。这构成了信念向量 $\\hat{q}(t-1) \\in \\mathbb{R}^3$，其中每个分量 $\\hat{q}_k(t-1)$ 由 $\\frac{1}{t-1}\\sum_{\\tau=1}^{t-1}\\mathbf{1}\\{j(\\tau)=k\\}$ 给出。\n    2.  学习者计算其三个纯行动中每一个对抗此信念的期望收益。行动 $i$ 的期望收益为 $(A\\hat{q}(t-1))_i$。\n    3.  学习者选择使此期望收益最大化的行动 $i(t)$：$i(t) \\in \\arg\\max_{i\\in\\{1,2,3\\}} (A\\hat{q}(t-1))_i$。平局通过选择具有最小索引的行动来打破。\n- **收益:** 第 $t$ 轮的实现收益为 $r^{\\mathrm{FP}}(t) = A_{i(t), j(t)}$。\n\n**2. Hedge (指数权重) 算法**\n\nHedge 是一种用于在线决策的算法，它在一组专家（在本例中为纯行动）上维护一个分布。它根据每个专家的表现来更新此分布。\n\n- **初始化 ($t=1$):** 学习者开始时为每个行动设置相等的权重：$w_i(1) = 1$，对每个 $i \\in \\{1, 2, 3\\}$。学习率 $\\eta$ 设置为 $\\sqrt{\\frac{2\\ln n}{T}}$，$n=3$。\n- **迭代 ($t = 1, \\dots, T$):**\n    1.  学习者通过归一化当前权重来形成混合策略 $p(t)$：$p_i(t) = \\frac{w_i(t)}{\\sum_{k=1}^3 w_k(t)}$。\n    2.  在对手玩出行动 $j(t)$ 后，学习者观察到自己所有行动的反事实收益。这由向量 $g(t) \\in \\mathbb{R}^3$ 给出，其中 $g_i(t) = A_{i, j(t)}$。该向量对应于矩阵 $A$ 的第 $j(t)$ 列。\n    3.  下一轮的权重通过乘法更新：$w_i(t+1) = w_i(t) \\exp(\\eta g_i(t))$。本可以产生更高收益的行动会获得更大的权重增加。\n- **收益:** 算法在第 $t$ 轮的性能由其在混合策略 $p(t)$下的期望收益来衡量，即 $r^{\\mathrm{H}}(t) = \\sum_{i=1}^3 p_i(t) g_i(t)$。\n\n**3. 汇总统计量**\n\n对于每个算法和每个对手时间表，在时间范围 $T$ 计算两个指标：\n\n- **平均外部遗憾:** 该指标衡量算法的累积收益与事后最佳单一行动的累积收益之间的平均每轮差异。计算公式为：\n$$\n\\frac{R_T}{T} = \\frac{1}{T}\\left( \\max_{i\\in\\{1,2,3\\}} \\sum_{t=1}^T g_i(t) - \\sum_{t=1}^T r(t) \\right)\n$$\n对于虚拟博弈，$r(t) = r^{\\mathrm{FP}}(t)$；对于 Hedge 算法，$r(t) = r^{\\mathrm{H}}(t)$。项 $\\sum_{t=1}^T g_i(t)$ 代表通过持续玩行动 $i$ 本可累积的总收益。\n\n- **与纳什均衡的 $\\ell_1$ 距离:** 该指标量化了学习者的时间平均策略与石头-剪刀-布博弈唯一的混合策略纳什均衡 $u^\\star = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$ 之间的差距。距离为：\n$$\nD(\\bar{p}) = \\sum_{i=1}^3 \\left| \\bar{p}_i - \\frac{1}{3} \\right|\n$$\n对于虚拟博弈，时间平均策略 $\\bar{p}^{\\mathrm{FP}}$ 是所玩行动的经验频率：$\\bar{p}^{\\mathrm{FP}}_i = \\frac{1}{T}\\sum_{t=1}^T \\mathbf{1}\\{i(t)=i\\}$。对于 Hedge 算法，它是每轮使用的混合策略的平均值：$\\bar{p}^{\\mathrm{H}} = \\frac{1}{T}\\sum_{t=1}^T p(t)$。\n\n实现将针对四个指定的测试用例模拟这些过程，为每个用例计算并收集四个所需的统计数据 ($R_T^{\\mathrm{H}}/T$、$R_T^{\\mathrm{FP}}/T$、$D(\\bar{p}^{\\mathrm{H}})$、$D(\\bar{p}^{\\mathrm{FP}})$)。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    # Payoff matrix for the row player (learner).\n    # Actions: 0=Rock, 1=Paper, 2=Scissors\n    A = np.array([[0, -1, 1],\n                  [1, 0, -1],\n                  [-1, 1, 0]])\n    n_actions = 3\n    u_star = np.full(n_actions, 1.0 / n_actions)\n\n    def generate_schedule(case, T):\n        \"\"\"Generates the opponent's action schedule for a given case.\"\"\"\n        # Note: actions are 1-indexed in problem, 0-indexed in code.\n        # This function returns 0-indexed actions.\n        if case == 'A':  # T=90\n            return np.concatenate([np.full(30, 0), np.full(30, 1), np.full(30, 2)])\n        elif case == 'B':  # T=3\n            return np.array([0, 1, 2])\n        elif case == 'C':  # T=60\n            return np.concatenate([np.full(20, 0), np.full(10, 1), np.full(20, 2), np.full(10, 1)])\n        elif case == 'D':  # T=60\n            return np.full(60, 0)\n        return None\n\n    def run_fictitious_play(T, opponent_schedule, A):\n        \"\"\"Simulates the Fictitious Play algorithm.\"\"\"\n        opponent_action_counts = np.zeros(n_actions)\n        learner_action_counts = np.zeros(n_actions)\n        total_payoff_fp = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n\n            if t == 0:\n                learner_action_idx = 0  # Rule: i(1)=1 (Rock)\n            else:\n                # Belief is the empirical frequency of opponent's past actions\n                opponent_empirical_freq = opponent_action_counts / t\n                expected_payoffs = A @ opponent_empirical_freq\n                # Best response, ties broken by smallest index (np.argmax default)\n                learner_action_idx = np.argmax(expected_payoffs)\n\n            # Record actions and payoffs\n            learner_action_counts[learner_action_idx] += 1\n            total_payoff_fp += A[learner_action_idx, opponent_action_idx]\n            opponent_action_counts[opponent_action_idx] += 1\n\n        # Calculate summary statistics\n        p_bar_fp = learner_action_counts / T\n        d_fp = np.sum(np.abs(p_bar_fp - u_star))\n\n        return total_payoff_fp, d_fp\n\n    def run_hedge(T, opponent_schedule, A):\n        \"\"\"Simulates the Hedge (Exponential Weights) algorithm.\"\"\"\n        eta = np.sqrt(2 * np.log(n_actions) / T)\n        weights = np.ones(n_actions)\n        \n        sum_p_t = np.zeros(n_actions)\n        total_expected_payoff_h = 0.0\n\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            \n            # Form mixed strategy\n            p_t = weights / np.sum(weights)\n            sum_p_t += p_t\n            \n            # Counterfactual payoffs\n            g_t = A[:, opponent_action_idx]\n            \n            # Expected payoff for this round\n            total_expected_payoff_h += p_t @ g_t\n            \n            # Update weights\n            weights *= np.exp(eta * g_t)\n            \n        # Calculate summary statistics\n        p_bar_h = sum_p_t / T\n        d_h = np.sum(np.abs(p_bar_h - u_star))\n\n        return total_expected_payoff_h, d_h\n\n    def calculate_regret(T, total_payoff, opponent_schedule, A):\n        \"\"\"Calculates the average external regret.\"\"\"\n        # Payoff of best fixed action in hindsight\n        total_g = np.zeros(n_actions)\n        for t in range(T):\n            opponent_action_idx = opponent_schedule[t]\n            total_g += A[:, opponent_action_idx]\n        \n        max_fixed_payoff = np.max(total_g)\n        avg_regret = (max_fixed_payoff - total_payoff) / T\n        return avg_regret\n\n    test_cases = [\n        ('A', 90),\n        ('B', 3),\n        ('C', 60),\n        ('D', 60)\n    ]\n\n    all_results = []\n\n    for case_label, T in test_cases:\n        opponent_schedule = generate_schedule(case_label, T)\n\n        # Run simulations\n        total_payoff_fp, d_fp = run_fictitious_play(T, opponent_schedule, A)\n        total_payoff_h, d_h = run_hedge(T, opponent_schedule, A)\n        \n        # Calculate regrets\n        avg_regret_fp = calculate_regret(T, total_payoff_fp, opponent_schedule, A)\n        avg_regret_h = calculate_regret(T, total_payoff_h, opponent_schedule, A)\n        \n        # Store results in the specified order\n        results = [avg_regret_h, avg_regret_fp, d_h, d_fp]\n        all_results.append(results)\n    \n    # Format the output string as required\n    formatted_cases = []\n    for case_res in all_results:\n        formatted_nums = [f\"{num:.6f}\" for num in case_res]\n        formatted_cases.append(f\"[{','.join(formatted_nums)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\nsolve()\n\n```", "id": "2405816"}]}