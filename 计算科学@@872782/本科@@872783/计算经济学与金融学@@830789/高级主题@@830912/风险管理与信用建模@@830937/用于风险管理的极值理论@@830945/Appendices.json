{"hands_on_practices": [{"introduction": "理论需要通过实践来巩固。本练习将指导您使用极值理论中的一个核心方法——分块最大值法（Block Maxima Method）。我们将模拟德克萨斯州每日的最高电力需求，这是能源市场定价和评估电网稳定性风险的关键任务。通过这个从数据生成到风险度量（如风险价值 $VaR$ 和预期短缺 $ES$）的完整流程，您将掌握广义极值（GEV）分布的实际应用。[@problem_id:2391840]", "problem": "编写一个完整的程序，应用极值理论 (EVT) 对德克萨斯州的每日最大电力需求进行建模，以支持峰值负荷电力衍生品的定价和评估电网稳定性风险。您的实现必须从以下基本基础开始：Fisher–Tippett–Gnedenko 定理、区块最大值和广义极值分布的定义、分位数和期望亏空的定义，以及风险中性定价的标准原则。您不得使用任何简便公式；必须从第一性原理计算各量，并在需要时通过数值积分进行计算。三角函数中使用的角度必须以弧度为单位。所有电力量必须以吉瓦 (GW) 为单位处理。货币尺度归一化为每吉瓦 $1$，以便衍生品价格与需求以相同的数值单位表示。所有概率必须以小数表示。\n\n合成每日电力需求的数据生成机制如下。设每日需求 $D_{t}$ (日指数 $t \\in \\{0,1,\\dots,N-1\\}$) 为\n$$\nD_{t} = \\max\\left\\{0, \\ \\mu + A \\sin\\left(\\frac{2\\pi \\, (t \\bmod 365)}{365}\\right) + \\sigma \\, \\varepsilon_{t}\\right\\},\n$$\n其中 $\\varepsilon_{t}$ 是来自具有 $\\nu$ 个自由度的 Student t 分布的独立同分布抽样。符号 $\\max\\{\\cdot,\\cdot\\}$ 强制非负性。参数 $N$ 等于 $365 \\times Y$，表示一个跨越 $Y$ 年的样本。\n\n您的程序必须为每个测试用例执行以下步骤：\n- 步骤 1：使用指定的随机种子和参数 $(Y,\\ \\mu,\\ A,\\ \\sigma,\\ \\nu)$ 模拟每日需求序列 $\\{D_{t}\\}_{t=0}^{N-1}$。\n- 步骤 2：将天数划分为 $B$ 天的不重叠区块（丢弃任何剩余的天数），以形成区块最大值 $\\{M_{i}\\}_{i=1}^{m}$，其中 $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$，且 $M_{i} = \\max\\{D_{t} : t \\in \\text{区块 } i\\}$。\n- 步骤 3：通过最大似然法将广义极值 (GEV) 分布拟合到区块最大值，得到参数 $(\\xi, \\mu_{\\text{gev}}, \\sigma_{\\text{gev}})$，其中 $\\xi$ 是形状参数，$\\mu_{\\text{gev}}$ 是位置参数，$\\sigma_{\\text{gev}}$ 是尺度参数。\n- 步骤 4：计算水平为 $\\alpha$ 的单步向前区块最大值风险价值 (Value-at-Risk)，定义为下一个长度为 $B$ 天的区块中最大值的拟合 GEV 分布的 $\\alpha$-分位数 $q_{\\alpha}$。\n- 步骤 5：计算水平为 $\\alpha$ 的单步向前区块最大值期望亏空 (Expected Shortfall)，定义为\n$$\n\\text{ES}_{\\alpha} \\equiv \\mathbb{E}\\!\\left[M \\mid M > q_{\\alpha}\\right] = \\frac{1}{1-\\alpha}\\int_{\\alpha}^{1} Q(u)\\,du,\n$$\n其中 $Q(u)$ 是 $M$ 的拟合 GEV 分布的分位数函数。以足够的精度对积分进行数值计算。\n- 步骤 6：考虑一个关于即将到来的区块最大值的欧式看涨期权，其到期时间等于区块长度，即 $\\tau = \\frac{B}{365}$ 年。期权收益为 $(M - K)^{+}$，其中 $K$ 是行权价，且 $(x)^{+} \\equiv \\max\\{x,0\\}$。在将拟合的 GEV 分布等同于风险中性分布的风险中性近似下，计算未折现的期望收益\n$$\n\\mathbb{E}\\!\\left[(M - K)^{+}\\right] = \\int_{0}^{1} \\max\\{Q(u) - K, 0\\}\\,du,\n$$\n并以连续复利无风险利率 $r$ 对其进行折现，以获得价格 $P = e^{-r \\tau} \\, \\mathbb{E}\\!\\left[(M - K)^{+}\\right]$。对任何所需的积分进行数值计算。\n- 步骤 7：给定一个容量水平 $L$，根据拟合的 GEV 分布计算下一个区块最大值的电网故障概率 $p_{\\text{fail}} = \\mathbb{P}[M > L]$。\n\n您的程序必须为每个测试用例返回一个列表，按顺序包含以下元素：\n$[q_{\\alpha},\\ \\text{ES}_{\\alpha},\\ P,\\ \\xi,\\ p_{\\text{fail}}]$，\n并按如下方式四舍五入：$q_{\\alpha}$ 保留 $3$ 位小数，$\\text{ES}_{\\alpha}$ 保留 $3$ 位小数，$P$ 保留 $3$ 位小数，$\\xi$ 保留 $4$ 位小数，以及 $p_{\\text{fail}}$ 保留 $6$ 位小数。所有量在输出中均为不带单位符号的浮点数。\n\n测试套件。使用以下四个测试用例，每个用例指定为一个有序元组\n$(\\text{seed},\\ Y,\\ B,\\ \\mu,\\ A,\\ \\sigma,\\ \\nu,\\ \\alpha,\\ K,\\ L,\\ r)$：\n\n- 案例 1：$(12345,\\ 12,\\ 30,\\ 55.0,\\ 12.0,\\ 5.0,\\ 3.5,\\ 0.99,\\ 80.0,\\ 95.0,\\ 0.02)$。\n- 案例 2：$(2024,\\ 5,\\ 7,\\ 45.0,\\ 8.0,\\ 4.0,\\ 5.0,\\ 0.975,\\ 65.0,\\ 85.0,\\ 0.01)$。\n- 案例 3：$(7,\\ 8,\\ 14,\\ 50.0,\\ 6.0,\\ 3.0,\\ 30.0,\\ 0.995,\\ 75.0,\\ 90.0,\\ 0.0)$。\n- 案例 4：$(999,\\ 3,\\ 30,\\ 60.0,\\ 15.0,\\ 7.0,\\ 2.8,\\ 0.98,\\ 85.0,\\ 100.0,\\ 0.03)$。\n\n最终输出格式。您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表由每个案例的结果列表组成，并用方括号括起来。例如，一个有效的形状将是\n$[[x_{1},y_{1},z_{1},u_{1},v_{1}],[x_{2},y_{2},z_{2},u_{2},v_{2}],\\dots]$，\n前后无任何附加文本。", "solution": "我们使用区块最大值和广义极值 (GEV) 分布来建模电力需求的极值。Fisher–Tippett–Gnedenko 定理指出，在广泛的条件下，独立同分布观测值的经适当归一化的最大值在分布上收敛于 GEV 族的一个成员。因此，我们提取每日需求的区块最大值，并通过最大似然法拟合 GEV 模型。\n\n数据生成。我们按如下方式模拟每日需求 $D_{t}$\n$$\nD_{t} = \\max\\{0,\\ \\mu + A \\sin(2\\pi \\, t/365) + \\sigma \\varepsilon_{t}\\},\n$$\n其中 $\\varepsilon_{t}$ 是来自具有 $\\nu$ 个自由度的 Student t 分布的独立同分布抽样。角度以弧度为单位。正弦项捕捉季节性变化，而重尾噪声捕捉罕见的尖峰。非负性由 $\\max$ 算子强制执行。\n\n区块最大值。我们将序列划分为 $B$ 天的区块，并为每个区块提取最大值 $M_{i}$。如果我们总共有 $N = 365 Y$ 天，则完整区块的数量为 $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$，任何剩余的天数都将被丢弃，以保持相等的区块大小。我们收集 $\\{M_{i}\\}_{i=1}^{m}$。\n\nGEV 拟合。我们通过最大似然法将 GEV 分布拟合到 $\\{M_{i}\\}$。GEV 的累积分布函数为\n$$\nF(x) = \\exp\\left\\{-\\left[1 + \\xi \\left(\\frac{x - \\mu_{\\text{gev}}}{\\sigma_{\\text{gev}}}\\right)\\right]^{-1/\\xi}\\right\\}\n$$\n在其支撑集 $1 + \\xi \\left(\\frac{x - \\mu_{\\text{gev}}}{\\sigma_{\\text{gev}}}\\right) > 0$ 上，其中 Gumbel 情况 $\\xi = 0$ 通过连续性来解释。在实现中，我们使用一个标准的科学计算库，其参数化采用形状参数 $c$，其中 $c = -\\xi$。拟合后，我们报告 $\\xi = -c$。\n\n风险度量。水平为 $\\alpha$ 的单步向前区块最大值风险价值 (Value-at-Risk) 是分位数\n$$\nq_{\\alpha} = Q(\\alpha),\n$$\n其中 $Q$ 是拟合 GEV 分布的分位数函数（逆累积分布函数）。水平为 $\\alpha$ 的期望亏空 (Expected Shortfall) 定义为\n$$\n\\text{ES}_{\\alpha} = \\mathbb{E}[M \\mid M > q_{\\alpha}],\n$$\n并且可以通过分位数函数写为\n$$\n\\text{ES}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} Q(u)\\,du.\n$$\n我们使用自适应求积法对积分进行数值计算。对于 $\\xi \\ge 1$ 的重尾情况，此积分会发散；在我们的合成案例中，拟合的 $\\xi$ 保持在 $1$ 以下，积分为有限。为避免在 $u = 1$ 处的数值问题，我们积分到 $u = 1 - \\varepsilon$，其中 $\\varepsilon$ 是一个非常小的数。\n\n衍生品定价。考虑一个关于下一个区块最大值 $M$ 的欧式看涨期权，其行权价为 $K$，到期时间为 $\\tau = B/365$ 年。在将拟合的 GEV 视为风险中性分布的风险中性近似下，看涨期权价格为\n$$\nP = e^{-r \\tau} \\, \\mathbb{E}[(M - K)^{+}].\n$$\n使用分位数函数，我们将未折现期望表示为\n$$\n\\mathbb{E}[(M - K)^{+}] = \\int_{0}^{1} \\max\\{Q(u) - K, 0\\}\\,du = \\int_{u_{0}}^{1} Q(u)\\,du - (1 - u_{0}) K,\n$$\n其中 $u_{0} = F(K)$ 是在 $K$ 处的累积分布函数。此恒等式成立，因为分位数函数在 $K$ 以上的尾部下面积等于超过 $K$ 的期望超额。我们在 $[u_{0}, 1 - \\varepsilon]$ 上对积分进行数值计算，并应用折现因子 $e^{-r \\tau}$。\n\n电网故障概率。给定一个容量 $L$，下一个区块的故障概率为\n$$\np_{\\text{fail}} = \\mathbb{P}[M > L] = 1 - F(L),\n$$\n直接从拟合的 GEV 累积分布函数计算。\n\n每个测试用例的算法步骤：\n- 固定随机种子并从具有 $\\nu$ 个自由度的 Student t 分布中生成 $N = 365 Y$ 天的噪声 $\\varepsilon_{t}$。计算季节性均值 $\\mu + A \\sin(2\\pi (t \\bmod 365)/365)$ 并加上缩放后的噪声 $\\sigma \\varepsilon_{t}$。在 $0$ 处截断以得到 $D_{t}$。\n- 划分为 $m = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ 个长度为 $B$ 的区块，计算每个区块中的最大值 $M_{i}$。\n- 通过最大似然法拟合 GEV。提取形状参数 $c$、位置参数 $\\mu_{\\text{gev}}$ 和尺度参数 $\\sigma_{\\text{gev}}$。设置 $\\xi = -c$。\n- 从拟合模型中计算 $q_{\\alpha} = Q(\\alpha)$。\n- 通过数值求积法计算 $\\text{ES}_{\\alpha} = \\frac{1}{1-\\alpha} \\int_{\\alpha}^{1} Q(u)\\,du$，使用一个小的上限截止值 $\\varepsilon$ 来避免奇异点。\n- 计算 $u_{0} = F(K)$。如果 $u_{0} \\ge 1$，将期权价值设为 $0$。否则，数值计算积分 $\\int_{u_{0}}^{1} Q(u)\\,du$ 并设置 $P = e^{-r \\tau}\\left(\\int_{u_{0}}^{1} Q(u)\\,du - (1 - u_{0}) K\\right)$，为了数值安全，在零处截断。\n- 计算 $p_{\\text{fail}} = 1 - F(L)$ 并为了数值稳定性将其限制在 $[0,1]$ 范围内。\n- 将 $q_{\\alpha}$ 和 $\\text{ES}_{\\alpha}$ 四舍五入到 3 位小数，$P$ 到 3 位小数，$\\xi$ 到 4 位小数，以及 $p_{\\text{fail}}$ 到 6 位小数。\n\n最终输出是包含四个测试用例的每个案例结果列表 $[[q_{\\alpha},\\ \\text{ES}_{\\alpha},\\ P,\\ \\xi,\\ p_{\\text{fail}}], \\dots]$ 的单行，无任何附加文本。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import genextreme\nfrom scipy.integrate import quad\n\ndef simulate_daily_demand(seed, years, mu, amplitude, sigma, df):\n    \"\"\"\n    Simulate daily electricity demand (in GW) over 'years' years,\n    with seasonality and heavy-tailed Student-t noise.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    days = 365 * years\n    t = np.arange(days)\n    # Angle in radians; seasonality repeats every 365 days\n    seasonal = amplitude * np.sin(2.0 * np.pi * (t % 365) / 365.0)\n    noise = sigma * rng.standard_t(df, size=days)\n    demand = mu + seasonal + noise\n    # Enforce non-negativity\n    demand = np.maximum(0.0, demand)\n    return demand\n\ndef block_maxima(series, block_days):\n    \"\"\"\n    Partition 'series' into non-overlapping blocks of length 'block_days'\n    and return the list of maxima in each block. Discard remainder.\n    \"\"\"\n    n = len(series)\n    m = n // block_days\n    if m == 0:\n        return np.array([])\n    trimmed = series[:m * block_days]\n    reshaped = trimmed.reshape(m, block_days)\n    maxima = reshaped.max(axis=1)\n    return maxima\n\ndef fit_gev_mle(maxima):\n    \"\"\"\n    Fit GEV distribution to block maxima using MLE via scipy.stats.genextreme.\n    SciPy's genextreme parameterization uses shape c = -xi.\n    Returns (xi, loc, scale).\n    \"\"\"\n    # Ensure we have variability\n    if len(maxima)  3 or np.allclose(np.std(maxima), 0.0):\n        # Fallback: trivial fit with tiny scale to avoid errors\n        loc = float(np.mean(maxima)) if len(maxima) > 0 else 0.0\n        scale = float(np.std(maxima)) if len(maxima) > 0 else 1.0\n        c = 0.0\n    else:\n        c, loc, scale = genextreme.fit(maxima)\n        # Sometimes fit may return non-positive scale; guard it\n        if scale = 0:\n            # Adjust scale to a small positive value\n            scale = max(1e-6, float(np.std(maxima)))\n    xi = -c\n    return xi, loc, scale, c\n\ndef gev_quantile(u, c, loc, scale):\n    \"\"\"Quantile function Q(u) for the fitted GEV in SciPy's parameterization.\"\"\"\n    return genextreme.ppf(u, c, loc=loc, scale=scale)\n\ndef gev_cdf(x, c, loc, scale):\n    \"\"\"CDF F(x) for the fitted GEV in SciPy's parameterization.\"\"\"\n    return genextreme.cdf(x, c, loc=loc, scale=scale)\n\ndef expected_shortfall_alpha(alpha, c, loc, scale):\n    \"\"\"\n    Compute ES_alpha = (1/(1-alpha)) * integral_{alpha}^{1} Q(u) du\n    via numerical quadrature. Integrate up to 1 - eps to avoid endpoint issues.\n    \"\"\"\n    eps = 1e-12\n    upper = 1.0 - eps\n    if alpha >= 1.0:\n        return float('inf')\n    # Define integrand with safety checks\n    def integrand(u):\n        q = gev_quantile(u, c, loc, scale)\n        return q\n    try:\n        val, _ = quad(integrand, alpha, upper, epsabs=1e-6, epsrel=1e-6, limit=200)\n        es = val / (1.0 - alpha)\n        return es\n    except Exception:\n        return float('inf')\n\ndef call_price_on_block_max(K, r, block_days, c, loc, scale):\n    \"\"\"\n    Price of a European call option on the next block maximum M with strike K,\n    maturity tau = block_days/365 years, under risk-neutral approximation\n    using the fitted GEV distribution. Uses quantile integral identity:\n    E[(M-K)+] = \\int_{u0}^{1} Q(u) du - (1-u0)K, where u0 = F(K).\n    \"\"\"\n    tau = block_days / 365.0\n    u0 = gev_cdf(K, c, loc, scale)\n    if not np.isfinite(u0):\n        return 0.0\n    if u0 >= 1.0:\n        undiscounted = 0.0\n    else:\n        eps = 1e-12\n        upper = 1.0 - eps\n        def integrand(u):\n            return gev_quantile(u, c, loc, scale)\n        try:\n            integral_val, _ = quad(integrand, u0, upper, epsabs=1e-6, epsrel=1e-6, limit=200)\n            undiscounted = integral_val - (1.0 - u0) * K\n            if not np.isfinite(undiscounted):\n                undiscounted = 0.0\n        except Exception:\n            undiscounted = 0.0\n    price = np.exp(-r * tau) * max(0.0, undiscounted)\n    return price\n\ndef risk_metrics_for_case(case):\n    \"\"\"\n    Compute [VaR_alpha, ES_alpha, call_price, xi, p_fail] for one test case.\n    Rounding:\n      VaR, ES, price -> 3 decimals\n      xi -> 4 decimals\n      p_fail -> 6 decimals\n    \"\"\"\n    (seed, Y, B, mu, A, sigma, nu, alpha, K, L, r) = case\n    # Simulate daily demand\n    demand = simulate_daily_demand(seed, Y, mu, A, sigma, nu)\n    # Block maxima\n    maxima = block_maxima(demand, B)\n    if len(maxima)  3:\n        # Ensure there are enough maxima; otherwise pad with mean\n        if len(maxima) == 0:\n            maxima = np.array([mu])\n        elif len(maxima) == 1:\n            maxima = np.array([maxima[0], maxima[0] * 0.99 + 0.01, maxima[0] * 1.01 - 0.01])\n        else:\n            maxima = np.concatenate([maxima, [np.mean(maxima)]*(3 - len(maxima))])\n    # Fit GEV\n    xi, loc, scale, c = fit_gev_mle(maxima)\n    # VaR\n    try:\n        q_alpha = float(gev_quantile(alpha, c, loc, scale))\n        if not np.isfinite(q_alpha):\n            # Fallback: use high empirical quantile\n            q_alpha = float(np.quantile(maxima, alpha))\n    except Exception:\n        q_alpha = float(np.quantile(maxima, alpha))\n    # ES\n    es_alpha = expected_shortfall_alpha(alpha, c, loc, scale)\n    # Call price\n    price = call_price_on_block_max(K, r, B, c, loc, scale)\n    # Failure probability\n    try:\n        p_fail = 1.0 - float(gev_cdf(L, c, loc, scale))\n        if not np.isfinite(p_fail):\n            # Fallback approximation\n            p_fail = max(0.0, min(1.0, 1.0 - float(np.mean(maxima > L))))\n    except Exception:\n        p_fail = max(0.0, min(1.0, 1.0 - float(np.mean(maxima > L))))\n    p_fail = max(0.0, min(1.0, p_fail))\n    # Rounding\n    result = [\n        round(q_alpha, 3),\n        round(es_alpha, 3),\n        round(price, 3),\n        round(xi, 4),\n        round(p_fail, 6),\n    ]\n    return result\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (seed, Y, B, mu, A, sigma, nu, alpha, K, L, r)\n    test_cases = [\n        (12345, 12, 30, 55.0, 12.0, 5.0, 3.5, 0.99, 80.0, 95.0, 0.02),\n        (2024, 5, 7, 45.0, 8.0, 4.0, 5.0, 0.975, 65.0, 85.0, 0.01),\n        (7, 8, 14, 50.0, 6.0, 3.0, 30.0, 0.995, 75.0, 90.0, 0.0),\n        (999, 3, 30, 60.0, 15.0, 7.0, 2.8, 0.98, 85.0, 100.0, 0.03),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = risk_metrics_for_case(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Ensure a single line with the nested list representation.\n    print(f\"{results}\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2391840"}, {"introduction": "在许多应用中，我们更关心超过某个高阈值的事件，而不是仅仅关心某个时间段内的最大值。本练习将介绍超阈值峰值法（Peaks-Over-Threshold, POT），这是一种数据利用效率更高的方法。我们将应用 POT 方法来模拟大型在线零售商的网站延迟峰值，以评估在销售高峰期间发生灾难性故障的运营风险。通过将广义帕累托分布（GPD）拟合到阈值以上的数据，您将学会如何估算罕见但具有破坏性的事件的发生概率。[@problem_id:2391805]", "problem": "给定一个大型在线零售商的操作风险管理场景。在销售高峰事件期间，网站延迟峰值可能引发级联故障，从而构成灾难性中断。极值理论 (EVT) 假定，超过一个足够高阈值的超额量在极限情况下可以很好地用广义帕累托分布 (GPD) 建模。您的任务是编写一个完整的程序，利用这一原理从合成的延迟数据中估计尾部指数（GPD 形状参数），并计算在有限事件期间，假设请求是独立的，至少发生一次灾难性故障的概率。\n\n基本原理：\n- 在超阈值峰值框架中，对于一个足够高的阈值 $u$，超过 $u$ 的超额量的分布作为一个极限定理收敛于广义帕累托分布 (GPD)。\n- 尾部指数是 GPD 的形状参数，通常用 $\\xi$ 表示。\n- 独立性下的稀有事件定律意味着，如果单次请求的灾难性故障概率为 $p$，那么在 $N$ 次独立请求中至少发生一次灾难性故障的概率是 $1-(1-p)^N$。\n\n对于每个测试用例，您的程序必须：\n1. 使用双组分混合模型生成一个大小为 $n$ 的网站延迟观测值（单位：毫秒）的合成数据集：\n   - 以 $p_{\\text{tail}}$ 的概率，使用变换 $X_{\\text{tail}} = x_m \\cdot (1 + P)$ 从最小值为 $x_m$、形状为 $\\alpha$ 的帕累托尾部中抽取，其中 $P$ 服从形状为 $\\alpha$、支撑集为 $(0,\\infty)$ 的标准帕累托分布。\n   - 以 $1-p_{\\text{tail}}$ 的概率，从对数位置参数为 $\\mu$、对数尺度参数为 $\\sigma$ 的对数正态分布中抽取，得到 $X_{\\text{base}} \\sim \\text{Lognormal}(\\mu,\\sigma)$，其支撑集为 $(0,\\infty)$。\n   - 以 $p_{\\text{tail}}$ 的概率将抽样合并为 $X = X_{\\text{tail}}$，以 $1-p_{\\text{tail}}$ 的概率合并为 $X = X_{\\text{base}}$。\n2. 选择一个高阈值 $u$，作为样本在水平 $q_u$ 上的经验分位数。\n3. 对于所有满足 $X  u$ 的观测值，形成超额量 $Y = X - u$，并使用最大似然法估计超额量的 GPD 参数（形状 $\\xi$ 和尺度 $\\beta$），强制 GPD 位置参数为 $0$。\n4. 将超过 $u$ 的尾部概率估计为 $p_u = k/n$，其中 $k$ 是超额量的数量。\n5. 对于灾难性延迟阈值 $z$（其中 $z  u$），通过 GPD 尾部近似结合经验超额率来近似延迟超过 $z$ 的概率。使用 $p_z \\approx p_u$ 乘以超过 $u$ 的条件尾部概率，并通过其连续极限处理 $\\xi = 0$ 的特殊情况。\n6. 使用独立性假设，计算在一个包含 $N$ 次独立请求的事件中，至少发生一次灾难性故障的概率。\n7. 将此最终概率报告为一个四舍五入到六位小数的浮点数。\n\n不涉及角度单位。延迟输入数据的单位是毫秒，但要求的输出是概率，因此无单位。您必须以小数形式表示最终的概率结果。\n\n测试套件：\n为以下三个参数集提供结果。对于每个案例，应用给定的确切值。\n\n- 案例 A：\n  - 种子 $= 202311$\n  - $n = 50000$\n  - $p_{\\text{tail}} = 0.15$\n  - 帕累托形状 $\\alpha = 2.0$\n  - 帕累托最小值 $x_m = 150$\n  - 对数正态 $\\mu = 3.7$\n  - 对数正态 $\\sigma = 0.35$\n  - 阈值分位数 $q_u = 0.95$\n  - 灾难性阈值 $z = 1000$\n  - 事件大小 $N = 200000$\n- 案例 B：\n  - 种子 $= 7$\n  - $n = 80000$\n  - $p_{\\text{tail}} = 0.05$\n  - 帕累托形状 $\\alpha = 1.4$\n  - 帕累托最小值 $x_m = 200$\n  - 对数正态 $\\mu = 3.5$\n  - 对数正态 $\\sigma = 0.5$\n  - 阈值分位数 $q_u = 0.97$\n  - 灾难性阈值 $z = 1500$\n  - 事件大小 $N = 1000000$\n- 案例 C：\n  - 种子 $= 4242$\n  - $n = 60000$\n  - $p_{\\text{tail}} = 0.10$\n  - 帕累托形状 $\\alpha = 3.5$\n  - 帕累托最小值 $x_m = 120$\n  - 对数正态 $\\mu = 3.6$\n  - 对数正态 $\\sigma = 0.4$\n  - 阈值分位数 $q_u = 0.90$\n  - 灾难性阈值 $z = 800$\n  - 事件大小 $N = 100000$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个案例的概率，形式为方括号内以逗号分隔的列表，每个概率四舍五入到六位小数。例如，格式必须完全像 $[r_1,r_2,r_3]$，其中每个 $r_i$ 是一个小数点后有六位数字的小数。", "solution": "问题陈述被认为是有效的。它提出了一个基于极值理论的量化风险管理中定义明确且有科学依据的问题。为得到一个独特且可验证的解，所有必要的组成部分都已提供。我们着手进行解的推导和实现。\n\n核心任务是通过对延迟分布的尾部进行建模，来估计灾难性网站延迟事件的概率。该方法遵循超阈值峰值 (POT) 框架的原则。\n\n1.  **合成数据生成**\n    延迟数据 $X$ 是从一个大小为 $n$ 的双组分混合模型中合成的。该模型同时捕捉了典型行为和极端事件。\n    -   一个“基础”组分，从对数正态分布 $X_{\\text{base}} \\sim \\text{Lognormal}(\\mu, \\sigma)$ 中抽取，代表了大部分延迟观测值。对数正态分布常用于为具有正偏度的非负量建模。\n    -   一个“尾部”组分，代表极端延迟，从帕累托分布中抽取。问题中指定其生成方式为 $X_{\\text{tail}} = x_m \\cdot (1 + P)$，其中 $P$ 是一个支撑集在 $(0, \\infty)$ 上、形状为 $\\alpha$ 的标准帕累托变量。支撑集在 $(0, \\infty)$ 上的标准帕累托分布被解释为尺度参数 $\\lambda=1$ 的 Lomax 分布。来自此分布的变量 $P$ 可以通过逆变换采样生成，即 $P = U^{-1/\\alpha} - 1$，其中 $U \\sim \\text{Uniform}(0,1)$。将此代入 $X_{\\text{tail}}$ 的表达式得到 $X_{\\text{tail}} = x_m (1 + (U^{-1/\\alpha} - 1)) = x_m U^{-1/\\alpha}$。这是从最小值为 $x_m$、形状参数为 $\\alpha$ 的第一类帕累托分布生成变量的公式。这种解释既是标准的也是自洽的。\n    每个样本以概率 $p_{\\text{tail}}$ 从尾部组分中抽取，以概率 $1 - p_{\\text{tail}}$ 从基础组分中抽取。\n\n2.  **阈值选择与超额量**\n    POT 方法需要定义一个高阈值 $u$ 以将极端事件与大部分数据分开。我们选择 $u$ 作为合成数据集在高概率水平 $q_u$ 上的经验分位数。超过此阈值的观测值 $X_i$ 产生超额量，定义为 $Y_i = X_i - u$。\n\n3.  **广义帕累托分布 (GPD) 拟合**\n    根据 Pickands–Balkema–de Haan 定理，对于足够高的阈值 $u$，超额量 $Y = X - u$ 的分布收敛于广义帕累托分布 (GPD)。GPD 的累积分布函数 (CDF) 由下式给出：\n    $$ G_{\\xi, \\beta}(y) = \\begin{cases} 1 - \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-1/\\xi}  \\text{if } \\xi \\neq 0 \\\\ 1 - \\exp(-y/\\beta)  \\text{if } \\xi = 0 \\end{cases} $$\n    对于 $y  0$。参数是形状 $\\xi$（尾部指数）和尺度 $\\beta  0$。根据超额量的定义，位置参数固定为 $0$。\n    参数 $(\\xi, \\beta)$ 是通过对观测到的超额量最大化 GPD 的对数似然函数来估计的。这是一个数值优化问题，可以通过使用成熟的库函数可靠地解决，例如 `scipy.stats.genpareto.fit`，它实现了最大似然估计 (MLE)。\n\n4.  **极端事件概率估计**\n    单次延迟 $X$ 超过灾难性阈值 $z$ （其中 $z  u$）的概率表示为 $p_z = P(X  z)$。使用全概率定律，我们将其分解为：\n    $$ p_z = P(X  z | X  u) \\cdot P(X  u) $$\n    -   项 $P(X  u)$ 是根据数据经验性估计的。设在总共 $n$ 个观测值中有 $k$ 个超过 $u$。那么，$P(X  u) \\approx p_u = k/n$。\n    -   条件概率 $P(X  z | X  u)$ 是超额量 $Y = X-u$ 大于 $z-u$ 的概率。这个概率由拟合的 GPD 的生存函数 $S_{\\text{GPD}}(y) = 1 - G_{\\text{GPD}}(y)$ 在 $y = z-u$ 处的值给出。\n    $$ P(X  z | X  u) = S_{\\text{GPD}}(z-u) = \\left(1 + \\frac{\\hat{\\xi} (z-u)}{\\hat{\\beta}}\\right)^{-1/\\hat{\\xi}} $$\n    其中 $\\hat{\\xi}$ 和 $\\hat{\\beta}$ 是 GPD 参数的 MLE 估计。$\\xi=0$ 的情况通过其连续极限处理，得到指数生存函数。\n    将这些结合起来，得到 $p_z$ 的最终估计值：\n    $$ p_z \\approx \\frac{k}{n} \\left(1 + \\frac{\\hat{\\xi} (z-u)}{\\hat{\\beta}}\\right)^{-1/\\hat{\\xi}} $$\n\n5.  **总风险概率**\n    最终关注的量是在包含 $N$ 次独立请求的事件中至少发生一次灾难性故障的概率。如果单次故障的概率是 $p_z$，那么在 $N$ 次试验中没有故障的概率是 $(1 - p_z)^N$。因此，至少发生一次故障的概率是：\n    $$ P(\\text{at least one}) = 1 - (1 - p_z)^N $$\n    对于小的 $p_z$，直接计算此表达式可能导致数值精度损失。通过使用对数和减一指数函数可以实现更稳定的计算：$P(\\text{at least one}) = -\\text{expm1}(N \\cdot \\text{log1p}(-p_z))$。这可以避免灾难性抵消并保持精度。最终结果按要求四舍五入到六位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import genpareto\n\ndef run_case(seed, n, p_tail, alpha, xm, mu, sigma, q_u, z, N):\n    \"\"\"\n    Solves a single test case for catastrophic failure probability estimation.\n    \"\"\"\n    # Step 1: Generate synthetic dataset from a two-component mixture model.\n    rng = np.random.default_rng(seed)\n    \n    # Determine which samples come from the tail vs. the base distribution.\n    is_tail = rng.random(size=n)  p_tail\n    n_tail = np.sum(is_tail)\n    n_base = n - n_tail\n\n    # Generate Pareto tail data using inverse transform sampling.\n    # The generation rule X_tail = xm * (1 + P) where P is standard Pareto on (0,inf)\n    # simplifies to sampling from a Pareto Type I distribution with minimum xm.\n    uniform_samples = rng.random(size=n_tail)\n    data_tail = xm / (uniform_samples**(1/alpha))\n\n    # Generate Lognormal base data.\n    data_base = rng.lognormal(mean=mu, sigma=sigma, size=n_base)\n\n    # Combine into a single dataset.\n    data = np.empty(n, dtype=float)\n    data[is_tail] = data_tail\n    data[~is_tail] = data_base\n\n    # Step 2: Choose a high threshold 'u' as an empirical quantile.\n    u = np.quantile(data, q_u)\n\n    # Step 3: Form exceedances and fit the Generalized Pareto Distribution (GPD).\n    exceedances = data[data > u] - u\n    \n    if len(exceedances) == 0:\n        # If there are no exceedances, the probability of an even more extreme event is zero.\n        return 0.0\n\n    # Use scipy's robust Maximum Likelihood Estimation for GPD parameters (xi, beta).\n    # 'floc=0' enforces the location parameter to be 0, as per the definition of exceedances.\n    # The fit returns (shape, location, scale) which correspond to (xi, 0, beta).\n    xi, _, beta = genpareto.fit(exceedances, floc=0)\n\n    # Step 4: Estimate the empirical probability of exceeding the threshold u.\n    k = len(exceedances)\n    p_u = k / n\n\n    # Step 5: Approximate the probability of a single latency exceeding z.\n    # p_z = P(X > z) = P(X > u) * P(X > z | X > u)\n    # The conditional probability is calculated using the GPD survival function.\n    # The problem statement ensures z > u, so (z - u) > 0.\n    y_z = z - u\n    prob_cond_exceed_z = genpareto.sf(y_z, c=xi, scale=beta, loc=0)\n    p_z = p_u * prob_cond_exceed_z\n\n    # Step 6: Compute the probability of at least one catastrophic failure in N requests.\n    # The calculation uses numerically stable functions to avoid precision loss.\n    # P(at least one) = 1 - (1 - p_z)^N = -expm1(N * log1p(-p_z))\n    if p_z >= 1.0:\n        # If a single event is guaranteed to be catastrophic, so is any sequence of events.\n        prob_final = 1.0\n    elif p_z = 0.0:\n        prob_final = 0.0\n    else:\n        prob_final = -np.expm1(N * np.log1p(-p_z))\n\n    return prob_final\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        (202311, 50000, 0.15, 2.0, 150, 3.7, 0.35, 0.95, 1000, 200000),\n        # Case B\n        (7, 80000, 0.05, 1.4, 200, 3.5, 0.5, 0.97, 1500, 1000000),\n        # Case C\n        (4242, 60000, 0.10, 3.5, 120, 3.6, 0.4, 0.90, 800, 100000),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Execute the main logic for one case.\n        final_probability = run_case(*case)\n        # Format the result to six decimal places.\n        results.append(f\"{final_probability:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2391805"}, {"introduction": "极值理论不仅是衡量静态风险的工具，它也可以用于动态的统计推断，以检验市场的根本性变化。这项高级练习将向您展示如何检验金融资产回报率的尾部行为是否在重大央行公告等事件前后发生了结构性变化。通过对不同时期的数据拟合 GPD 模型并运用沃尔德检验（Wald test），您将学习如何从统计上判断市场的风险特征是否发生了改变，从而掌握一项更高级的风险分析技能。[@problem_id:2391785]", "problem": "要求您使用极值理论 (EVT) 来形式化并检验一次重大央行公告前后高频货币收益率尾部行为的变化。您需要在基于 Pickands–Balkema–de Haan 定理的超出阈值峰值 (POT) 框架下进行操作。该定理指出，对于一个足够高的阈值，超出量的分布会收敛于一个广义帕累托分布 (GPD)。您必须从第一性原理出发，实现一个完整的程序，对公告前后的绝对收益率的上尾进行建模，估计尾部指数，并统计检验尾部指数是否发生变化。\n\n从以下基本原理开始：\n- 高于一个高阈值的超出量分布的定义，以及为使用广义帕累托分布 (GPD) 对超出量进行建模提供了理论依据的 Pickands–Balkema–de Haan 定理。\n- 用于估计模型参数的最大似然估计 (MLE) 原理。\n- 使用从观测 Fisher 信息（在最大似然估计处的对数似然函数的负 Hessian 矩阵）得到的渐近协方差，来检验两个样本间参数相等性的 Wald 检验原理。\n\n您的任务：\n- 考虑一个基于对称学生 t 分布的风格化高频收益率生成器，通过自由度生成具有可控尾部指数的重尾收益率。设公告前和公告后两个部分可以有不同的自由度和尺度参数。\n- 对于每个部分，将原始收益率转换为绝对收益率，在指定分位数处选择一个高经验阈值，并收集超出量。\n- 通过最大似然估计对超出量拟合一个 GPD。使用观测 Fisher 信息（在最大似然估计处的负对数似然函数的 Hessian 矩阵）来生成尾部指数估计量的渐近方差估计。\n- 在指定的显著性水平下，构建一个 Wald 检验，用于检验原假设“公告前和公告后的尾部指数相等”。决定是否拒绝原假设。\n- 为 GPD 实现仔细的参数约束处理，以确保似然计算的有效性。\n\n需要遵守的数学对象和约束：\n- 绝对收益率用 $x \\in \\mathbb{R}_{+}$ 表示。\n- 对于选定的阈值 $u$，超出量为 $y = x - u$（其中 $x  u$）。\n- 广义帕累托分布 (GPD) 具有形状（尾部指数）参数 $\\xi$ 和尺度参数 $\\beta$，其中 $\\beta  0$，其支撑集由 $1 + \\xi y / \\beta  0$ 定义。\n- $(\\xi,\\beta)$ 的最大似然估计 (MLE) 在优化过程中必须满足 GPD 的支撑集约束。\n- 使用在 MLE 处评估的负对数似然函数的 Hessian 矩阵计算得到的观测 Fisher 信息矩阵，来近似尾部指数估计量 $\\widehat{\\xi}$ 的方差。\n- 对于 $H_{0}: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ 对 $H_{1}: \\xi_{\\text{pre}} \\neq \\xi_{\\text{post}}$ 的 Wald 检验，使用统计量\n$$\nW = \\frac{\\left(\\widehat{\\xi}_{\\text{pre}} - \\widehat{\\xi}_{\\text{post}}\\right)^{2}}{\\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}}) + \\operatorname{Var}(\\widehat{\\xi}_{\\text{post}})},\n$$\n在 $H_{0}$ 下，该统计量渐近服从自由度为 1 的 $\\chi^{2}$ 分布。当 $W$ 在 $\\chi^{2}$ 分布下的上尾概率严格小于给定的显著性水平 $\\alpha$ 时，拒绝 $H_{0}$。\n\n为通用性和可测试性进行数据生成：\n- 对于每个部分，从一个自由度为 $\\nu$、尺度为 $s$ 的经过缩放的对称学生 t 分布（记为 $t_{\\nu}$）中生成独立同分布的收益率，然后乘以 $s$。使用模拟收益率的绝对值来定义 $x$。\n- 不涉及物理单位。纯粹使用无量纲的数值数据进行操作。\n\n实现要求：\n- 对于每种情况，使用给定的伪随机种子以确保可复现性。\n- 对于每个部分，将 $x$ 的样本分位数（水平为 $q \\in (0,1)$）计算为经验阈值 $u$。\n- 对超出量 $y = x - u$ 通过 MLE 估计 $(\\xi,\\beta)$。\n- 通过在 MLE 处的负对数似然函数的 Hessian 矩阵计算观测 Fisher 信息，并使用其逆矩阵来近似 $\\widehat{\\xi}$ 的方差。\n- 在水平 $\\alpha$ 下进行 Wald 检验，并返回一个布尔决策。\n\n测试套件：\n实现您的程序以运行以下五个测试用例。每个用例定义了公告前后的数据生成过程和测试设置。对于每个用例，输出一个布尔值，表示您是否在规定的水平 $\\alpha$ 下拒绝 $H_{0}: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$。\n\n- 情况 A (无变化，中等尾部)：seed $= 12345$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 8$, $\\nu_{\\text{post}} = 8$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.975$, $\\alpha = 0.05$。\n- 情况 B (明显变化，公告后尾部更重)：seed $= 202405$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 8$, $\\nu_{\\text{post}} = 3$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.975$, $\\alpha = 0.01$。\n- 情况 C (接近薄尾，无变化)：seed $= 424242$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 1000$, $\\nu_{\\text{post}} = 1000$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.990$, $\\alpha = 0.05$。\n- 情况 D (超出量少，尾部非常相似)：seed $= 777777$, $N_{\\text{pre}} = 5000$, $N_{\\text{post}} = 5000$, $\\nu_{\\text{pre}} = 5.0$, $\\nu_{\\text{post}} = 5.1$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 1.0$, $q = 0.990$, $\\alpha = 0.05$。\n- 情况 E (仅尺度变化，无尾部指数变化)：seed $= 314159$, $N_{\\text{pre}} = 10000$, $N_{\\text{post}} = 10000$, $\\nu_{\\text{pre}} = 4$, $\\nu_{\\text{post}} = 4$, $s_{\\text{pre}} = 1.0$, $s_{\\text{post}} = 2.0$, $q = 0.975$, $\\alpha = 0.05$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含五个测试用例的结果，形式为一个包含在方括号中的逗号分隔列表，例如，“[True,False,True,False,True]”。每个条目都是一个布尔值，与上面列出的用例按相同顺序对应。", "solution": "所述问题已经过验证，并被认定为有效。它在科学上是合理的，基于极值理论、最大似然估计和统计假设检验的既定原则。该问题是适定的，为获得唯一、可验证的解，所有必需的数据和条件都已指明。其语言客观、正式。因此，我们可以着手解决该问题。\n\n该问题要求对金融收益序列的尾部行为是否存在结构性断点进行统计检验。我们将使用超出阈值峰值 (POT) 框架，其理论依据是 Pickands–Balkema–de Haan 定理。该定理指出，对于一个随机变量 $X$ 和一个足够高的阈值 $u$，在 $X  u$ 的条件下，超出量 $Y = X-u$ 的分布可以由一个广义帕累托分布 (GPD) 来近似。\n\nGPD 的概率密度函数 (PDF) 由下式给出：\n$$\nf(y; \\xi, \\beta) = \n\\begin{cases} \n\\frac{1}{\\beta} \\left(1 + \\frac{\\xi y}{\\beta}\\right)^{-(1/\\xi + 1)}  \\text{for } \\xi \\neq 0 \\\\\n\\frac{1}{\\beta} \\exp\\left(-\\frac{y}{\\beta}\\right)  \\text{for } \\xi = 0 \n\\end{cases}\n$$\n参数包括形状参数（或尾部指数）$\\xi \\in \\mathbb{R}$ 和尺度参数 $\\beta  0$。当 $\\xi \\geq 0$ 时，分布的支撑集为 $y \\geq 0$；当 $\\xi  0$ 时，支撑集为 $0 \\leq y \\leq -\\beta/\\xi$。参数 $\\xi$ 控制着尾部的厚重程度：$\\xi  0$ 对应重尾（类帕累托）；$\\xi = 0$ 对应中等尾（类指数）；$\\xi  0$ 对应轻尾、有界尾。对于金融收益率，我们通常期望 $\\xi  0$。对于自由度为 $\\nu$ 的学生 t 分布，其理论尾部指数为 $\\xi = 1/\\nu$。\n\n我们的第一步是为公告前和公告后的收益序列估计参数 $(\\xi, \\beta)$。我们使用最大似然估计 (MLE) 方法。对于一组包含 $k$ 个独立超出量 $y_1, y_2, \\ldots, y_k$ 的样本，对数似然函数为 $\\ell(\\xi, \\beta) = \\sum_{i=1}^k \\log f(y_i; \\xi, \\beta)$。MLE 估计值 $(\\widehat{\\xi}, \\widehat{\\beta})$ 是使该函数最大化的 $(\\xi, \\beta)$ 值。这在数值上等价于最小化负对数似然函数：\n$$\n-\\ell(\\xi, \\beta) = \n\\begin{cases}\nk\\log\\beta + \\left(\\frac{1}{\\xi} + 1\\right)\\sum_{i=1}^k \\log\\left(1 + \\frac{\\xi y_i}{\\beta}\\right)  \\text{for } \\xi \\neq 0 \\\\\nk\\log\\beta + \\frac{1}{\\beta}\\sum_{i=1}^k y_i  \\text{for } \\xi = 0\n\\end{cases}\n$$\n优化过程必须遵守参数约束 $\\beta  0$ 和对所有 $i$ 均成立的 $1 + \\xi y_i/\\beta  0$。这些约束在提供给数值优化器的目标函数内部强制执行。如果在优化步骤中参数违反了这些条件，函数将返回一个代表无穷大的值，以引导优化器避开无效区域。\n\n第二步是量化我们对尾部指数估计值 $\\widehat{\\xi}$ 的不确定性。在标准正则性条件下，MLE $\\widehat{\\theta} = (\\widehat{\\xi}, \\widehat{\\beta})$ 是渐近正态分布的，其均值等于真实参数值 $\\theta$，协方差矩阵由 Fisher 信息矩阵的逆 $\\mathcal{I}(\\theta)^{-1}$ 给出。我们用观测 Fisher 信息 $J(\\widehat{\\theta})$ 来近似 $\\mathcal{I}(\\theta)$，它是指在 MLE 处评估的负对数似然函数的 Hessian 矩阵：\n$$\nJ(\\widehat{\\theta}) = -\\left. \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta \\partial \\theta^T} \\right|_{\\theta=\\widehat{\\theta}} = \\left. \\nabla^2 (-\\ell(\\theta)) \\right|_{\\theta=\\widehat{\\theta}}\n$$\n该 Hessian 矩阵使用二阶中心差分公式进行数值计算。渐近协方差矩阵则为 $\\operatorname{Cov}(\\widehat{\\theta}) \\approx J(\\widehat{\\theta})^{-1}$。尾部指数估计量的方差 $\\operatorname{Var}(\\widehat{\\xi})$ 是这个逆矩阵的第一个对角元素。\n\n第三步也是最后一步，是针对原假设 $H_0: \\xi_{\\text{pre}} = \\xi_{\\text{post}}$ 和备择假设 $H_1: \\xi_{\\text{pre}} \\neq \\xi_{\\text{post}}$ 进行 Wald 检验。由于公告前后的样本是独立的，它们各自的 MLE 也是独立的。估计量之差的方差是它们各自方差的和：$\\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}} - \\widehat{\\xi}_{\\text{post}}) = \\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}}) + \\operatorname{Var}(\\widehat{\\xi}_{\\text{post}})$。Wald 检验统计量为：\n$$\nW = \\frac{(\\widehat{\\xi}_{\\text{pre}} - \\widehat{\\xi}_{\\text{post}})^2}{\\operatorname{Var}(\\widehat{\\xi}_{\\text{pre}}) + \\operatorname{Var}(\\widehat{\\xi}_{\\text{post}})}\n$$\n在原假设下，$W$ 渐近服从自由度为 1 的卡方分布，即 $W \\\n\\xrightarrow{d} \\chi^2(1)$。我们计算 p 值，即在 $H_0$ 下观测到比 $W$ 更极端或同样极端的检验统计量的概率。该 p 值由 $P(\\chi^2(1) \\geq W)$ 给出。如果该 p 值严格小于显著性水平 $\\alpha$，我们则在水平 $\\alpha$ 下拒绝原假设。\n\n每个测试用例的完整算法流程如下：\n1.  使用提供的随机种子，为公告前和公告后时期从指定的缩放学生 t 分布生成绝对收益率。\n2.  对每个时期：\n    a. 将绝对收益率在指定水平 $q$ 的经验分位数确定为阈值 $u$。\n    b. 识别出所有数据点 $x  u$ 的超出量 $y = x - u$。\n    c. 执行 MLE 以获得估计值 $(\\widehat{\\xi}, \\widehat{\\beta})$。\n    d. 数值计算在 MLE 处的负对数似然函数的 Hessian 矩阵。\n    e. 对 Hessian 矩阵求逆以找到协方差矩阵，并提取尾部指数估计量的方差 $\\operatorname{Var}(\\widehat{\\xi})$。\n3.  使用估计值 $\\widehat{\\xi}_{\\text{pre}}, \\widehat{\\xi}_{\\text{post}}$ 及其方差来计算 Wald 统计量 $W$。\n4.  根据 $\\chi^2(1)$ 分布计算 p 值。\n5.  如果 p 值小于显著性水平 $\\alpha$，则返回 `True`（表示拒绝 $H_0$），否则返回 `False`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.stats import t as student_t\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for changes in tail behavior.\n    \"\"\"\n\n    def _gpd_neg_log_likelihood(params, y, epsilon=1e-8):\n        \"\"\"\n        Computes the negative log-likelihood for the Generalized Pareto Distribution.\n        Handles parameter constraints by returning infinity.\n        \"\"\"\n        xi, beta = params\n        k = len(y)\n\n        # Constraint: beta > 0\n        if beta = 0:\n            return np.inf\n\n        # Constraint: 1 + xi * y / beta > 0\n        try:\n            with np.errstate(divide='ignore', invalid='ignore'):\n                term = 1 + xi * y / beta\n            if np.any(term = 0):\n                return np.inf\n        except (FloatingPointError, ValueError):\n            return np.inf\n\n        # Case: xi is close to 0 (Exponential distribution)\n        if abs(xi)  epsilon:\n            return k * np.log(beta) + np.sum(y) / beta\n\n        # Case: xi is not 0\n        log_term = np.log(term)\n        return k * np.log(beta) + (1 / xi + 1) * np.sum(log_term)\n\n    def _calculate_hessian(func, params, args, h=1e-5):\n        \"\"\"\n        Numerically calculates the Hessian matrix of a function using central differences.\n        \"\"\"\n        n_params = len(params)\n        hessian = np.zeros((n_params, n_params))\n        \n        for i in range(n_params):\n            for j in range(i, n_params):\n                # Diagonal elements\n                if i == j:\n                    p_plus = np.copy(params)\n                    p_plus[i] += h\n                    p_minus = np.copy(params)\n                    p_minus[i] -= h\n                    f_0 = func(params, *args)\n                    f_plus = func(p_plus, *args)\n                    f_minus = func(p_minus, *args)\n                    hessian[i, i] = (f_plus - 2 * f_0 + f_minus) / (h * h)\n                # Off-diagonal elements\n                else:\n                    p_pp = np.copy(params); p_pp[i] += h; p_pp[j] += h\n                    p_pm = np.copy(params); p_pm[i] += h; p_pm[j] -= h\n                    p_mp = np.copy(params); p_mp[i] -= h; p_mp[j] += h\n                    p_mm = np.copy(params); p_mm[i] -= h; p_mm[j] -= h\n                    \n                    f_pp = func(p_pp, *args)\n                    f_pm = func(p_pm, *args)\n                    f_mp = func(p_mp, *args)\n                    f_mm = func(p_mm, *args)\n\n                    val = (f_pp - f_pm - f_mp + f_mm) / (4 * h * h)\n                    hessian[i, j] = val\n                    hessian[j, i] = val\n        return hessian\n\n    def analyze_segment(returns, q):\n        \"\"\"\n        Analyzes a single segment of returns to estimate GPD parameters and tail index variance.\n        \"\"\"\n        u = np.quantile(returns, q)\n        exceedances = returns[returns > u] - u\n\n        if len(exceedances) == 0:\n            return np.nan, np.nan\n\n        # Initial guess for optimization\n        beta_init = np.mean(exceedances)\n        xi_init = 0.1  # A small positive value, common for financial data.\n        initial_params = [xi_init, beta_init]\n\n        # Perform MLE\n        res = optimize.minimize(\n            _gpd_neg_log_likelihood,\n            initial_params,\n            args=(exceedances,),\n            method='Nelder-Mead',\n            options={'maxiter': 2000, 'adaptive': True}\n        )\n\n        if not res.success:\n            # Fallback to L-BFGS-B if Nelder-Mead fails\n            res = optimize.minimize(\n                _gpd_neg_log_likelihood,\n                initial_params,\n                args=(exceedances,),\n                method='L-BFGS-B',\n                bounds=((-0.5, 1.5), (1e-6, None))\n            )\n            if not res.success:\n                return np.nan, np.nan\n        \n        mle_params = res.x\n        xi_hat, _ = mle_params\n\n        # Compute observed Fisher information (Hessian of neg-log-likelihood)\n        try:\n            hessian = _calculate_hessian(_gpd_neg_log_likelihood, mle_params, args=(exceedances,))\n            cov_matrix = np.linalg.inv(hessian)\n            var_xi = cov_matrix[0, 0]\n            # Ensure variance is non-negative\n            if var_xi  0:\n                return xi_hat, np.nan\n        except (np.linalg.LinAlgError, ValueError):\n            return xi_hat, np.nan\n\n        return xi_hat, var_xi\n\n    def run_one_case(case_params):\n        \"\"\"\n        Runs one full test case for the Wald test.\n        \"\"\"\n        seed, N_pre, N_post, nu_pre, nu_post, s_pre, s_post, q, alpha = case_params\n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        returns_pre = np.abs(student_t.rvs(df=nu_pre, scale=s_pre, size=N_pre, random_state=rng))\n        returns_post = np.abs(student_t.rvs(df=nu_post, scale=s_post, size=N_post, random_state=rng))\n\n        # Analyze each segment\n        xi_pre, var_xi_pre = analyze_segment(returns_pre, q)\n        xi_post, var_xi_post = analyze_segment(returns_post, q)\n        \n        if np.isnan(xi_pre) or np.isnan(var_xi_pre) or np.isnan(xi_post) or np.isnan(var_xi_post):\n            return False # Cannot reject if estimation fails\n\n        # Wald test\n        numerator = (xi_pre - xi_post)**2\n        denominator = var_xi_pre + var_xi_post\n\n        if denominator = 0:\n            return False # Cannot perform test if total variance is not positive\n\n        W = numerator / denominator\n        p_value = chi2.sf(W, df=1)\n\n        return p_value  alpha\n\n    test_cases = [\n        # (seed, N_pre, N_post, nu_pre, nu_post, s_pre, s_post, q, alpha)\n        (12345, 10000, 10000, 8, 8, 1.0, 1.0, 0.975, 0.05),\n        (202405, 10000, 10000, 8, 3, 1.0, 1.0, 0.975, 0.01),\n        (424242, 10000, 10000, 1000, 1000, 1.0, 1.0, 0.990, 0.05),\n        (777777, 5000, 5000, 5.0, 5.1, 1.0, 1.0, 0.990, 0.05),\n        (314159, 10000, 10000, 4, 4, 1.0, 2.0, 0.975, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        decision = run_one_case(case)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2391785"}]}