{"hands_on_practices": [{"introduction": "要掌握近端梯度法，我们首先必须理解其核心组成部分：近端算子。第一个练习 [@problem_id:2195122] 为我们提供了直接根据定义计算近端算子的基础实践。通过求解平方 $L_2$ 范数这个简单而重要的例子，你将具体理解这个“迷你优化”步骤是如何工作的。", "problem": "在凸优化领域，函数 $g: \\mathbb{R}^n \\to \\mathbb{R}$ 的邻近算子是用于设计求解复杂优化问题的算法中的一个基本工具。对于一个给定的函数 $g$，其邻近算子记作 $\\text{prox}_{g}$，是从 $\\mathbb{R}^n$ 到 $\\mathbb{R}^n$ 的一个映射，对于任意向量 $v \\in \\mathbb{R}^n$，其定义为一个小型优化问题的解：\n$$\n\\text{prox}_{g}(v) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left( g(x) + \\frac{1}{2} \\|x-v\\|_2^2 \\right)\n$$\n这里，$\\| \\cdot \\|_2$ 表示标准的欧几里得范数（也称为L2范数），其中对于向量 $u \\in \\mathbb{R}^n$，$\\|u\\|_2^2 = u^T u = \\sum_{i=1}^n u_i^2$。\n\n考虑特定函数 $g(x) = \\frac{\\lambda}{2} \\|x\\|_2^2$，它代表一个缩放的平方欧几里得范数。向量 $x$ 属于 $\\mathbb{R}^n$，$\\lambda$ 是一个严格为正的实数常数（$\\lambda  0$）。\n\n通过直接应用邻近算子的定义，确定 $\\text{prox}_{g}(v)$ 的闭式表达式。你的最终答案应该是一个关于向量 $v$ 和标量常数 $\\lambda$ 的表达式。", "solution": "我们求解 $g(x) = \\frac{\\lambda}{2}\\|x\\|_{2}^{2}$ 的 $\\text{prox}_{g}(v)$，其定义为\n$$\n\\text{prox}_{g}(v) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left( \\frac{\\lambda}{2}\\|x\\|_{2}^{2} + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right).\n$$\n定义目标函数\n$$\nf(x) = \\frac{\\lambda}{2} x^{T} x + \\frac{1}{2}(x - v)^{T}(x - v).\n$$\n使用标准梯度 $\\nabla_{x}\\left(\\frac{1}{2}x^{T}x\\right) = x$ 和 $\\nabla_{x}\\left(\\frac{1}{2}\\|x - v\\|_{2}^{2}\\right) = x - v$，我们得到梯度\n$$\n\\nabla f(x) = \\lambda x + (x - v) = (\\lambda + 1)x - v.\n$$\n设一阶最优性条件 $\\nabla f(x) = 0$ 可得\n$$\n(\\lambda + 1)x - v = 0 \\quad \\Rightarrow \\quad x = \\frac{1}{\\lambda + 1} v.\n$$\n海森矩阵为\n$$\n\\nabla^{2} f(x) = (\\lambda + 1) I,\n$$\n由于 $\\lambda  0$，该矩阵是正定的，这保证了最小化子是唯一的。因此，\n$$\n\\text{prox}_{g}(v) = \\frac{1}{1 + \\lambda} \\, v.\n$$", "answer": "$$\\boxed{\\frac{1}{1+\\lambda}\\,v}$$", "id": "2195122"}, {"introduction": "在理解了近端算子之后，我们现在可以检验它在近端梯度算法单次迭代中的作用。这个练习 [@problem_id:2195110] 将指导你完成一个完整的更新步骤，展示该方法由两部分构成的过程：一个标准的梯度下降步骤，后跟一个近端映射。你将看到如何通过将近端算子用作投影来优雅地处理约束条件。", "problem": "考虑一个优化问题，寻找一个点 $x = (x_1, x_2) \\in \\mathbb{R}^2$ 来最小化函数 $F(x)$，并满足其分量的非负约束，即 $x_1 \\ge 0$ 和 $x_2 \\ge 0$。要最小化的函数是从 $x$ 到目标点 $a$ 的欧几里得距离的平方，由 $F(x) = \\frac{1}{2}\\|x - a\\|_2^2$ 给出。\n\n通过将光滑部分定义为 $f(x) = \\frac{1}{2}\\|x - a\\|_2^2$，非光滑部分 $g(x)$ 定义为非负象限的指示函数，该问题可以转化为近端算法的标准形式 $\\min_{x} f(x) + g(x)$。指示函数 $g(x)$ 在 $x_1 \\ge 0$ 和 $x_2 \\ge 0$ 时为零，否则为无穷大。\n\n你的任务是应用近端梯度法来解决此问题。近端梯度法的迭代更新规则如下：\n$$ x_{k+1} = \\text{prox}_{\\gamma g}(x_k - \\gamma \\nabla f(x_k)) $$\n其中 $\\gamma$ 是步长，$\\text{prox}_{\\gamma g}$ 是与函数 $g$ 相关联的近端算子。\n\n给定目标点 $a = (5, -4)$，初始点 $x_0 = (1, 1)$，以及步长 $\\gamma = 0.2$，计算下一次迭代的结果 $x_1$。将你的答案表示为行向量 $(x_{1,1}, x_{1,2})$，其中 $x_{1,1}$ 和 $x_{1,2}$ 是向量 $x_1$ 的分量。", "solution": "我们要在非负象限上最小化 $F(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$。在近端梯度分解中，设 $f(x)=\\frac{1}{2}\\|x-a\\|_{2}^{2}$ 和 $g(x)=\\iota_{\\mathbb{R}_{+}^{2}}(x)$，其中 $g(x)$ 是可行集 $\\mathbb{R}_{+}^{2}=\\{x\\in\\mathbb{R}^{2}:x_{1}\\ge 0,\\ x_{2}\\ge 0\\}$ 的指示函数。\n\n$f$ 的梯度由下式给出\n$$\n\\nabla f(x)=x-a.\n$$\n$\\gamma g$ 在点 $z$ 处的近端算子是到 $\\mathbb{R}_{+}^{2}$ 上的欧几里得投影：\n$$\n\\text{prox}_{\\gamma g}(z)=\\operatorname*{arg\\,min}_{y\\in\\mathbb{R}^{2}}\\left\\{\\iota_{\\mathbb{R}_{+}^{2}}(y)+\\frac{1}{2\\gamma}\\|y-z\\|_{2}^{2}\\right\\}=P_{\\mathbb{R}_{+}^{2}}(z),\n$$\n即在零处的分量截断：\n$$\nP_{\\mathbb{R}_{+}^{2}}(z)=\\big(\\max\\{z_{1},0\\},\\ \\max\\{z_{2},0\\}\\big).\n$$\n\n当 $a=(5,-4)$, $x_{0}=(1,1)$ 和 $\\gamma=0.2$ 时，计算在 $x_{0}$ 处的梯度：\n$$\n\\nabla f(x_{0})=x_{0}-a=(1,1)-(5,-4)=(-4,5).\n$$\n执行梯度步：\n$$\nx_{0}-\\gamma \\nabla f(x_{0})=(1,1)-0.2\\,(-4,5)=(1+0.8,\\ 1-1)=\\left(\\frac{9}{5},\\ 0\\right).\n$$\n应用近端映射，即到 $\\mathbb{R}_{+}^{2}$ 上的投影：\n$$\nx_{1}=\\text{prox}_{\\gamma g}\\big(x_{0}-\\gamma \\nabla f(x_{0})\\big)=P_{\\mathbb{R}_{+}^{2}}\\left(\\frac{9}{5},0\\right)=\\left(\\frac{9}{5},0\\right),\n$$\n因为两个分量都已为非负。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{9}{5}  0 \\end{pmatrix}}$$", "id": "2195110"}, {"introduction": "现在，让我们从单个步骤转向近端梯度法的完整（尽管简化了的）应用。这个实践 [@problem_id:2195138] 模拟了求解经典 LASSO 问题的算法，并引入了至关重要的软阈值算子，即 $L_1$ 正则化的近端算子。通过执行迭代并应用一个实用的停止准则，你将体验到算法如何逐步趋近一个解。", "problem": "考虑一个称为LASSO（最小绝对收缩和选择算子）的优化问题，其目标是找到一个向量 $x \\in \\mathbb{R}^n$，使得目标函数 $F(x) = f(x) + g(x)$ 最小化。该函数由光滑部分 $f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$ 和非光滑正则化部分 $g(x) = \\lambda \\|x\\|_1$ 组成，其中 $\\lambda  0$ 是一个正则化参数。\n\n这个问题可以使用近端梯度法求解，该方法根据以下更新规则生成一个迭代序列 $\\{x_k\\}$：\n$$x_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))$$\n其中 $\\alpha  0$ 是一个常数步长。$g(x) = \\lambda \\|x\\|_1$ 的近端算子是软阈值算子，$\\text{prox}_{\\alpha g}(v) = S_{\\alpha\\lambda}(v)$，它对向量 $v$ 的每个分量进行如下操作：\n$$[S_{c}(v)]_i = \\text{sign}(v_i) \\max(|v_i| - c, 0)$$\n对于一个正常数 $c$。\n\n你的任务是针对一个特定的配置模拟这个算法。设向量为 $x \\in \\mathbb{R}^2$。系统参数由下式给出：\n$$A = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix}, \\quad \\lambda = 4$$\n算法从初始点 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，并使用步长 $\\alpha = 0.25$。\n\n一个常见的实用停止准则是当连续迭代之间的变化足够小时终止算法。如果条件 $\\|x_{k+1} - x_k\\|_2  \\epsilon$ 首次被满足，则称算法在完成第 $k+1$ 次迭代后收敛并终止，其中 $\\epsilon$ 是一个预定义的容差。对于这个问题，使用容差 $\\epsilon = 1.0$。\n\n确定算法终止前执行的总迭代次数。", "solution": "问题要求根据给定的停止准则，计算近端梯度法收敛所需的迭代次数。目标函数为 $F(x) = \\frac{1}{2} \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1$。\n\n首先，我们需要求出光滑部分 $f(x) = \\frac{1}{2} \\|Ax - b\\|_2^2$ 的梯度。\n函数 $f(x)$ 可以写成 $f(x) = \\frac{1}{2} (Ax - b)^T (Ax - b) = \\frac{1}{2} (x^T A^T - b^T)(Ax - b) = \\frac{1}{2} (x^T A^T A x - x^T A^T b - b^T A x + b^T b)$。\n关于 $x$ 的梯度是 $\\nabla f(x) = \\frac{1}{2} (2 A^T A x - A^T b - (b^T A)^T) = A^T A x - A^T b = A^T(Ax - b)$。\n\n让我们计算矩阵 $A^T A$：\n$$A^T A = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1  1 \\cdot 1 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 1  1 \\cdot 1 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = 2I$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n我们再计算向量 $A^T b$：\n$$A^T b = \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 6 + 1 \\cdot 2 \\\\ 1 \\cdot 6 - 1 \\cdot 2 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$$\n所以，梯度是 $\\nabla f(x) = 2Ix - A^T b = 2x - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$。\n\n近端梯度迭代是 $x_{k+1} = \\text{prox}_{\\alpha g}(x_k - \\alpha \\nabla f(x_k))$。\n近端算子的参数是 $v_k = x_k - \\alpha \\nabla f(x_k) = x_k - \\alpha \\left(2x_k - \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}\\right) = (1 - 2\\alpha)x_k + \\alpha \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix}$。\n更新是 $x_{k+1} = S_{\\alpha\\lambda}(v_k)$。\n\n给定参数 $\\alpha = 0.25$ 和 $\\lambda = 4$。\n软阈值算子的常数是 $c = \\alpha \\lambda = 0.25 \\times 4 = 1$。\n$v_k$ 的表达式变为：\n$v_k = (1 - 2(0.25))x_k + 0.25 \\begin{pmatrix} 8 \\\\ 4 \\end{pmatrix} = 0.5 x_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$。\n因此，迭代规则是：\n$x_{k+1} = S_1\\left(0.5 x_k + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}\\right)$。\n\n我们从 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始，停止容差为 $\\epsilon = 1.0$。\n\n**迭代 1 (k=0):**\n我们从 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 开始。\n首先，计算 $v_0$：\n$$v_0 = 0.5 x_0 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n接下来，应用软阈值算子来求 $x_1$：\n$$x_1 = S_1(v_0) = \\begin{pmatrix} S_1(2) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2)\\max(|2|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n现在，检查停止准则：$\\|x_1 - x_0\\|_2  \\epsilon$。\n$$\\|x_1 - x_0\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{1^2 + 0^2} = 1$$\n条件是 $1  1.0$，这是不成立的。算法不终止。我们进行下一次迭代。\n\n**迭代 2 (k=1):**\n我们从 $x_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 开始。\n首先，计算 $v_1$：\n$$v_1 = 0.5 x_1 + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2.5 \\\\ 1 \\end{pmatrix}$$\n接下来，应用软阈值算子来求 $x_2$：\n$$x_2 = S_1(v_1) = \\begin{pmatrix} S_1(2.5) \\\\ S_1(1) \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(2.5)\\max(|2.5|-1,0) \\\\ \\text{sign}(1)\\max(|1|-1,0) \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\max(1.5,0) \\\\ 1 \\cdot \\max(0,0) \\end{pmatrix} = \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix}$$\n现在，检查停止准则：$\\|x_2 - x_1\\|_2  \\epsilon$。\n$$\\|x_2 - x_1\\|_2 = \\left\\| \\begin{pmatrix} 1.5 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} 0.5 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{0.5^2 + 0^2} = 0.5$$\n条件是 $0.5  1.0$，这是成立的。算法终止。\n\n第一次迭代 (k=0) 产生了 $x_1$。第二次迭代 (k=1) 产生了 $x_2$。在第二次迭代完成后，停止条件被满足。因此，执行的总迭代次数为 2。", "answer": "$$\\boxed{2}$$", "id": "2195138"}]}