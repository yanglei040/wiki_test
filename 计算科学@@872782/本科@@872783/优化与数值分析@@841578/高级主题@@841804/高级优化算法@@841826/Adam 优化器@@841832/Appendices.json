{"hands_on_practices": [{"introduction": "要真正掌握 Adam 优化器，最好的方法就是从它的核心计算开始。这个练习将带你完成 Adam 算法的第一步：根据给定的梯度计算初始的一阶矩估计 $m_t$ 和二阶矩估计 $v_t$。通过这个基本计算 [@problem_id:2152288]，你将熟悉 Adam 中最关键的更新规则，为理解整个算法打下坚实的基础。", "problem": "在机器学习领域，优化器是用于调整模型参数以最小化损失函数的算法。Adam (自适应矩估计) 优化器是一种常用的选择，它通过对梯度的一阶矩和二阶矩的估计来为每个参数计算自适应学习率。\n\n在时间步 $t$，一阶矩估计（梯度的移动平均值，$m$）和二阶矩估计（梯度平方的移动平均值，$v$）的更新规则如下：\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$$\n其中 $g_t$ 是在步骤 $t$ 时损失函数关于模型参数的梯度，平方运算 $g_t^2$ 是逐元素执行的。超参数 $\\beta_1$ 和 $\\beta_2$ 是移动平均的衰减率。\n\n考虑一个简单的双参数模型。在优化过程的开始（$t=1$），初始矩估计被初始化为零向量，即 $m_0 = [0, 0]^T$ 和 $v_0 = [0, 0]^T$。超参数设置为其常用值：$\\beta_1 = 0.9$ 和 $\\beta_2 = 0.999$。对于第一个训练步骤，计算出的损失函数梯度为 $g_1 = [2.0, -4.0]^T$。\n\n计算更新后的一阶矩向量 $m_1 = [m_{1,1}, m_{1,2}]^T$ 和更新后的二阶矩向量 $v_1 = [v_{1,1}, v_{1,2}]^T$。最终答案应为一个行矩阵，按特定顺序 $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$ 包含这四个数值分量。", "solution": "我们使用在步骤 $t=1$ 时的 Adam 矩更新方程：\n$$m_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2},$$\n其中 $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$\\beta_{1}=0.9$，$\\beta_{2}=0.999$，以及 $g_{1}=\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}$。平方 $g_{1}^{2}$ 是逐元素计算的。\n\n一阶矩：\n由于 $m_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，项 $\\beta_{1}m_{0}$ 为 $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$。因此，\n$$m_{1}=(1-\\beta_{1})g_{1}=0.1\\begin{bmatrix}2.0 \\\\ -4.0\\end{bmatrix}=\\begin{bmatrix}0.2 \\\\ -0.4\\end{bmatrix}.$$\n所以，$m_{1,1}=0.2$ 且 $m_{1,2}=-0.4$。\n\n二阶矩：\n计算逐元素的平方 $g_{1}^{2}$：\n$$g_{1}^{2}=\\begin{bmatrix}(2.0)^{2} \\\\ (-4.0)^{2}\\end{bmatrix}=\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}.$$\n由于 $v_{0}=\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，项 $\\beta_{2}v_{0}$ 为 $\\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$。因此，\n$$v_{1}=(1-\\beta_{2})g_{1}^{2}=0.001\\begin{bmatrix}4.0 \\\\ 16.0\\end{bmatrix}=\\begin{bmatrix}0.004 \\\\ 0.016\\end{bmatrix}.$$\n所以，$v_{1,1}=0.004$ 且 $v_{1,2}=0.016$。\n\n按顺序 $[m_{1,1}, m_{1,2}, v_{1,1}, v_{1,2}]$ 排列所要求的行矩阵为：\n$$ \\begin{pmatrix}0.2  -0.4  0.004  0.016\\end{pmatrix} $$", "answer": "$$\\boxed{\\begin{pmatrix}0.2  -0.4  0.004  0.016\\end{pmatrix}}$$", "id": "2152288"}, {"introduction": "在计算出原始矩估计之后，Adam 算法的下一步是进行偏差校正，并最终更新模型参数。这个练习将第一步的计算扩展为一个完整的优化周期。你将对一个简单的函数执行一次完整的 Adam 更新 [@problem_id:2152250]，从而将所有独立的步骤——梯度计算、矩估计、偏差校正和参数更新——联系起来，亲眼见证参数是如何被优化的。", "problem": "在数值优化领域，Adam 算法是一种广泛用于寻找函数最小值的优化方法。考虑一维代价函数 $f(x) = 5x^2$。我们希望从初始猜测值 $x_0 = 2$ 开始，找到使该函数最小化的 $x$ 值。\n\n使用 Adam 算法计算一次更新步骤后参数的值，记为 $x_1$。该算法配置了以下超参数：\n- 学习率, $\\alpha = 0.1$\n- 一阶矩估计的指数衰减率, $\\beta_1 = 0.9$\n- 二阶矩估计的指数衰减率, $\\beta_2 = 0.999$\n- 用于数值稳定性的一个很小的常数, $\\epsilon = 10^{-8}$\n\n初始的一阶矩和二阶矩估计值 $m_0$ 和 $v_0$ 均初始化为零。\n\n将您的最终答案四舍五入到五位有效数字。", "solution": "我们从 $x_{0}=2$ 开始，使用 Adam 算法来最小化 $f(x)=5x^{2}$。梯度为 $\\nabla f(x)=10x$。在第一步（$t=1$）时，在 $x_{0}$ 处计算的梯度为\n$$\ng_{1}=\\nabla f(x_{0})=10x_{0}=10\\cdot 2=20.\n$$\nAdam 的矩更新如下\n$$\nm_{1}=\\beta_{1}m_{0}+(1-\\beta_{1})g_{1}, \\quad v_{1}=\\beta_{2}v_{0}+(1-\\beta_{2})g_{1}^{2}.\n$$\n当 $m_{0}=0$、$v_{0}=0$、$\\beta_{1}=0.9$ 和 $\\beta_{2}=0.999$ 时，\n$$\nm_{1}=0.9\\cdot 0+(1-0.9)\\cdot 20=2, \\quad v_{1}=0.999\\cdot 0+(1-0.999)\\cdot 20^{2}=0.001\\cdot 400=0.4.\n$$\n偏差修正后的估计值为\n$$\n\\hat{m}_{1}=\\frac{m_{1}}{1-\\beta_{1}^{1}}=\\frac{2}{1-0.9}=\\frac{2}{0.1}=20, \\quad \\hat{v}_{1}=\\frac{v_{1}}{1-\\beta_{2}^{1}}=\\frac{0.4}{1-0.999}=\\frac{0.4}{0.001}=400.\n$$\nAdam 更新步骤为\n$$\nx_{1}=x_{0}-\\alpha\\frac{\\hat{m}_{1}}{\\sqrt{\\hat{v}_{1}}+\\epsilon}=2-0.1\\cdot \\frac{20}{\\sqrt{400}+10^{-8}}=2-0.1\\cdot \\frac{20}{20+10^{-8}}=2-\\frac{2}{20+10^{-8}}.\n$$\n计算该分数，\n$$\n\\frac{2}{20+10^{-8}} \\approx 0.1-5\\times 10^{-11},\n$$\n所以\n$$\nx_{1}\\approx 2-\\left(0.1-5\\times 10^{-11}\\right)=1.90000000005.\n$$\n四舍五入到五位有效数字，结果是 $1.9000$。", "answer": "$$\\boxed{1.9000}$$", "id": "2152250"}, {"introduction": "现在，让我们从纯粹的计算转向更深入的分析，以理解 Adam 名称中“动量”的含义。这个练习设计了一个思想实验：当梯度序列出现高频振荡时，一阶矩估计（即动量）会如何表现？通过分析这个特殊场景 [@problem_id:2152247]，你将揭示指数移动平均如何平滑梯度的剧烈波动，从而为优化提供一个更稳定、更可靠的前进方向。", "problem": "在机器学习领域，Adam 优化器是一种用于训练深度神经网络的流行算法。Adam 的一个关键组成部分是其一阶矩估计，通常称为动量，它是梯度的指数移动平均。对于单个模型参数，在时间步 $t$ 的动量 $m_t$ 使用以下规则进行更新：\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$$\n这里，$g_t$ 是损失函数在时间步 $t$ 相对于该参数的梯度，而 $\\beta_1$ 是一阶矩估计的指数衰减率，它是一个满足 $0  \\beta_1  1$ 的常数。动量通常初始化为零，即 $m_0 = 0$。\n\n考虑一个为测试动量更新行为而设计的特定病态场景。假设梯度序列不收敛，而是以恒定的幅度振荡。对于所有时间步 $t \\ge 1$，梯度由序列 $g_t = g_0 (-1)^t$ 给出，其中 $g_0$ 是一个正常数。\n\n求当 $t$ 通过偶数趋于无穷大时，一阶矩估计 $m_t$ 的极限值。换句话说，计算 $\\lim_{n \\to \\infty} m_{2n}$ 的值。将您的答案表示为关于 $g_0$ 和 $\\beta_1$ 的符号表达式。", "solution": "我们要求解一阶矩 $m_t$ 在梯度序列 $g_t = g_0 (-1)^t$ ($t \\ge 1$) 下的极限。动量更新规则为：\n$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$$\n其中 $m_0 = 0$。\n\n我们特别关心偶数时间步的序列 $m_{2n}$。\n对于偶数步 $t=2n$ ($n \\ge 1$)，梯度为 $g_{2n} = g_0 (-1)^{2n} = g_0$。\n对于奇数步 $t=2n-1$ ($n \\ge 1$)，梯度为 $g_{2n-1} = g_0 (-1)^{2n-1} = -g_0$。\n\n我们可以写出 $m_{2n}$ 和 $m_{2n-1}$ 的递推关系：\n$$m_{2n} = \\beta_1 m_{2n-1} + (1-\\beta_1)g_0$$\n$$m_{2n-1} = \\beta_1 m_{2n-2} - (1-\\beta_1)g_0$$\n\n将第二个方程代入第一个方程，以消除奇数项 $m_{2n-1}$：\n$$m_{2n} = \\beta_1 (\\beta_1 m_{2n-2} - (1-\\beta_1)g_0) + (1-\\beta_1)g_0$$\n$$m_{2n} = \\beta_1^2 m_{2n-2} - \\beta_1(1-\\beta_1)g_0 + (1-\\beta_1)g_0$$\n$$m_{2n} = \\beta_1^2 m_{2n-2} + (1-\\beta_1)^2 g_0$$\n\n这是一个关于偶数时间步动量 $m_{2n}$ 的线性递推关系。当 $n \\to \\infty$ 时，我们假设 $m_{2n}$ 收敛到一个极限值 $L$。在极限情况下，$m_{2n} \\to L$ 且 $m_{2n-2} \\to L$。代入递推关系：\n$$L = \\beta_1^2 L + (1-\\beta_1)^2 g_0$$\n$$L(1 - \\beta_1^2) = (1-\\beta_1)^2 g_0$$\n$$L = \\frac{(1-\\beta_1)^2}{1 - \\beta_1^2} g_0$$\n\n利用代数恒等式 $1 - \\beta_1^2 = (1 - \\beta_1)(1 + \\beta_1)$，我们简化表达式：\n$$L = \\frac{(1-\\beta_1)^2}{(1 - \\beta_1)(1 + \\beta_1)} g_0 = \\frac{1 - \\beta_1}{1 + \\beta_1} g_0$$\n\n因此，当 $t$ 通过偶数趋于无穷大时，一阶矩估计收敛到 $\\frac{1 - \\beta_1}{1 + \\beta_1} g_0$。", "answer": "$$\\boxed{\\frac{1 - \\beta_{1}}{1 + \\beta_{1}}\\,g_{0}}$$", "id": "2152247"}]}