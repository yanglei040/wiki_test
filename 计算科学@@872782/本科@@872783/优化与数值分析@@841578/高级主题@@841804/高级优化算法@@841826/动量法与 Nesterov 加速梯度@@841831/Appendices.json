{"hands_on_practices": [{"introduction": "要真正掌握一个优化算法，最好的方法莫过于亲手执行它的计算步骤。本练习将引导你完成经典动量法在简单二次函数上的应用，通过手动计算前两次迭代，你将直观地理解速度项 $v_t$ 如何累积历史梯度信息，并影响参数 $w_t$ 的更新路径，从而为后续更复杂的概念打下坚实的基础。通过这种方式，你可以亲身体验动量法是如何工作的 [@problem_id:2187765]。", "problem": "一个优化算法被用来最小化一个代价函数 $f(x, y)$，该函数依赖于两个参数 $x$ 和 $y$。代价函数由以下公式给出：\n$$f(x, y) = x^2 + 2y^2$$\n\n选用的算法是经典动量法。在第 $t$ 步，参数向量 $w_t = (x_t, y_t)$ 和速度向量 $v_t$ 的更新规则如下：\n1. $v_t = \\gamma v_{t-1} + \\eta \\nabla f(w_{t-1})$\n2. $w_t = w_{t-1} - v_t$\n\n其中 $\\nabla f(w_{t-1})$ 是在上一参数向量 $w_{t-1}$ 处计算的代价函数的梯度。\n\n优化从初始参数向量 $w_0 = (x_0, y_0) = (4, 2)$ 开始。初始速度向量为 $v_0 = (0, 0)$。算法的超参数设置为学习率 $\\eta = 0.1$ 和动量参数 $\\gamma = 0.9$。\n\n你的任务是计算经过两次完整更新步骤后的参数向量 $w_2 = (x_2, y_2)$。将你的答案表示为一个包含两个分量的行向量。", "solution": "代价函数为 $f(x,y)=x^{2}+2y^{2}$，所以其梯度为\n$$\n\\nabla f(x,y)=\\begin{pmatrix} \\frac{\\partial f}{\\partial x} \\\\[4pt] \\frac{\\partial f}{\\partial y} \\end{pmatrix}\n=\\begin{pmatrix} 2x \\\\[4pt] 4y \\end{pmatrix}.\n$$\n动量更新公式为 $v_{t}=\\gamma v_{t-1}+\\eta\\,\\nabla f(w_{t-1})$ 和 $w_{t}=w_{t-1}-v_{t}$，其中 $w_{0}=\\begin{pmatrix}4\\\\[2pt]2\\end{pmatrix}$，$v_{0}=\\begin{pmatrix}0\\\\[2pt]0\\end{pmatrix}$，$\\eta=\\frac{1}{10}$，且 $\\gamma=\\frac{9}{10}$。\n\n第一步 $t=1$：\n$$\n\\nabla f(w_{0})=\\begin{pmatrix}2\\cdot 4\\\\[2pt]4\\cdot 2\\end{pmatrix}\n=\\begin{pmatrix}8\\\\[2pt]8\\end{pmatrix},\\qquad\nv_{1}=\\frac{9}{10}\\begin{pmatrix}0\\\\[2pt]0\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}8\\\\[2pt]8\\end{pmatrix}\n=\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix},\n$$\n$$\nw_{1}=w_{0}-v_{1}=\\begin{pmatrix}4\\\\[2pt]2\\end{pmatrix}-\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{16}{5}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}.\n$$\n\n第二步 $t=2$：\n$$\n\\nabla f(w_{1})=\\begin{pmatrix}2\\cdot \\frac{16}{5}\\\\[2pt]4\\cdot \\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{32}{5}\\\\[2pt]\\frac{24}{5}\\end{pmatrix},\n$$\n$$\nv_{2}=\\frac{9}{10}\\begin{pmatrix}\\frac{4}{5}\\\\[2pt]\\frac{4}{5}\\end{pmatrix}+\\frac{1}{10}\\begin{pmatrix}\\frac{32}{5}\\\\[2pt]\\frac{24}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{36}{50}\\\\[2pt]\\frac{36}{50}\\end{pmatrix}+\\begin{pmatrix}\\frac{32}{50}\\\\[2pt]\\frac{24}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{68}{50}\\\\[2pt]\\frac{60}{50}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{34}{25}\\\\[2pt]\\frac{6}{5}\\end{pmatrix},\n$$\n$$\nw_{2}=w_{1}-v_{2}=\\begin{pmatrix}\\frac{16}{5}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}-\\begin{pmatrix}\\frac{34}{25}\\\\[2pt]\\frac{6}{5}\\end{pmatrix}\n=\\begin{pmatrix}\\frac{80}{25}-\\frac{34}{25}\\\\[2pt]0\\end{pmatrix}\n=\\begin{pmatrix}\\frac{46}{25}\\\\[2pt]0\\end{pmatrix}.\n$$\n因此，经过两次完整更新后，参数向量为行向量 $\\begin{pmatrix}\\frac{46}{25}  0\\end{pmatrix}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{46}{25}  0 \\end{pmatrix}}$$", "id": "2187765"}, {"introduction": "在掌握了经典动量法之后，我们来探索其更先进的变体——Nesterov加速梯度（NAG）。NAG的核心改进在于其“向前看”的策略，即在计算梯度之前，先根据当前的速度对参数进行一个预测性更新。这个练习 [@problem_id:2187811] 让你精确计算这个关键的“前瞻”点，通过聚焦于NAG与经典动量法的这一本质区别，帮助你清晰地理解其为何通常能实现更快的收敛。", "problem": "一位工程师正在应用 Nesterov 加速梯度 (Nesterov Accelerated Gradient, NAG) 算法来最小化一个一维目标函数。该函数为 $f(x) = 2x^2$。该算法根据以下规则迭代更新位置参数 $x$ 和速度项 $v$，从初始位置 $x_0$ 和初始速度 $v_0=0$ 开始：\n$$ v_{t} = \\gamma v_{t-1} + \\eta \\nabla f(x_{t-1} - \\gamma v_{t-1}) $$\n$$ x_{t} = x_{t-1} - v_{t} $$\n在这些方程中，下标表示迭代次数，因此 $t=1, 2, 3, \\ldots$。常数 $\\gamma$ 是动量参数，$\\eta$ 是学习率，$\\nabla f$ 是函数 $f(x)$ 的梯度。梯度的计算发生在“前瞻”点，由项 $x_{t-1} - \\gamma v_{t-1}$ 给出。\n\n对于初始位置 $x_0 = 10$，动量参数 $\\gamma = 0.9$，学习率 $\\eta = 0.1$，计算在算法的第二次迭代（即 $t=2$）中使用的前瞻点的数值。", "solution": "我们已知 Nesterov 加速梯度的更新规则：\n$$v_{t}=\\gamma v_{t-1}+\\eta \\nabla f\\!\\left(x_{t-1}-\\gamma v_{t-1}\\right), \\quad x_{t}=x_{t-1}-v_{t}。$$\n目标函数为 $f(x)=2x^{2}$，因此其梯度为\n$$\\nabla f(x)=\\frac{d}{dx}(2x^{2})=4x。$$\n\n给定 $x_{0}=10$，$v_{0}=0$，$\\gamma=0.9$ 和 $\\eta=0.1$，首先计算 $t=1$ 时的前瞻点：\n$$x_{0}-\\gamma v_{0}=10-0.9\\cdot 0=10。$$\n然后更新 $t=1$ 时的速度：\n$$v_{1}=\\gamma v_{0}+\\eta \\nabla f(10)=0.9\\cdot 0+0.1\\cdot 4\\cdot 10=4。$$\n更新位置：\n$$x_{1}=x_{0}-v_{1}=10-4=6。$$\n\n对于第二次迭代（$t=2$），前瞻点是\n$$x_{1}-\\gamma v_{1}=6-0.9\\cdot 4=6-3.6=2.4。$$\n因此，在第二次迭代中使用的前瞻点的数值是 $2.4$。", "answer": "$$\\boxed{2.4}$$", "id": "2187811"}, {"introduction": "虽然动量法通常能够加速优化过程，但它并非万能药，不恰当的超参数设置可能导致优化过程不稳定甚至发散。本练习 [@problem_id:2187798] 提供了一个深刻的警示案例，通过将动量法的表现与标准梯度下降法进行直接对比，你会发现一个看似合理的超参数组合如何导致动量法“脱轨”。这个实践凸显了超参数调优在应用高级优化算法时的极端重要性。", "problem": "在数值优化领域，人们使用不同的算法来寻找函数的最小值。考虑一个简单的一维凸目标函数 $f(x) = \\frac{1}{2} C x^2$，其中 $C=1.0$。一位分析师正在比较两种迭代优化算法的表现，从初始位置 $x_0 = 10.0$ 开始。\n\n算法 A 是标准梯度下降（GD）法。在第 $k$ 步，位置根据以下规则进行更新：\n$$x_{k+1} = x_k - \\eta_{A} \\nabla f(x_k)$$\n分析师使用的学习率为 $\\eta_A = 1.5$。\n\n算法 B 是动量法。该方法引入一个“速度”项 $v$ 来累积过去的梯度。从初始速度 $v_0 = 0$ 开始，更新公式如下：\n$$v_{k+1} = \\beta v_k + \\eta_{B} \\nabla f(x_k)$$\n$$x_{k+1} = x_k - v_{k+1}$$\n对于这个算法，分析师使用的学习率为 $\\eta_B = 0.8$，动量参数为 $\\beta = 0.8$。\n\n设 $x_5^{(A)}$ 为使用算法 A 迭代 5 步后的位置，$x_5^{(B)}$ 为使用算法 B 迭代 5 步后的位置。计算比值 $R = x_5^{(B)} / x_5^{(A)}$。报告最终答案 $R$，并四舍五入到三位有效数字。", "solution": "题目要求计算在使用两种不同的优化算法对函数 $f(x) = \\frac{1}{2} C x^2$（其中 $C=1.0$）进行 5 次迭代后，最终位置的比值 $R = x_5^{(B)} / x_5^{(A)}$。函数的梯度为 $\\nabla f(x) = \\frac{d}{dx}f(x) = C x = x$。两种算法的初始位置均为 $x_0 = 10.0$。\n\n**第一部分：算法 A（标准梯度下降）的计算**\n\n算法 A 遵循更新规则 $x_{k+1} = x_k - \\eta_{A} \\nabla f(x_k)$。\n当 $\\nabla f(x_k) = x_k$ 且 $\\eta_A = 1.5$ 时，该规则变为：\n$x_{k+1} = x_k - 1.5 x_k = (1 - 1.5) x_k = -0.5 x_k$。\n\n我们从 $x_0 = 10.0$ 开始，迭代 5 次：\n- 第 1 步: $x_1 = -0.5 \\times x_0 = -0.5 \\times 10.0 = -5.0$。\n- 第 2 步: $x_2 = -0.5 \\times x_1 = -0.5 \\times (-5.0) = 2.5$。\n- 第 3 步: $x_3 = -0.5 \\times x_2 = -0.5 \\times 2.5 = -1.25$。\n- 第 4 步: $x_4 = -0.5 \\times x_3 = -0.5 \\times (-1.25) = 0.625$。\n- 第 5 步: $x_5^{(A)} = -0.5 \\times x_4 = -0.5 \\times 0.625 = -0.3125$。\n\n该算法似乎正在收敛（虽然在振荡，但幅度在减小）。\n\n**第二部分：算法 B（动量法）的计算**\n\n算法 B 遵循以下更新规则：\n$v_{k+1} = \\beta v_k + \\eta_{B} \\nabla f(x_k)$\n$x_{k+1} = x_k - v_{k+1}$\n当 $\\nabla f(x_k) = x_k$，$\\eta_B = 0.8$ 且 $\\beta = 0.8$ 时，规则为：\n$v_{k+1} = 0.8 v_k + 0.8 x_k$\n$x_{k+1} = x_k - v_{k+1}$\n\n我们从 $x_0 = 10.0$ 和 $v_0 = 0$ 开始，迭代 5 次：\n\n- **第 1 步 (从 k=0 到 k=1):**\n  $v_1 = 0.8 v_0 + 0.8 x_0 = 0.8(0) + 0.8(10.0) = 8.0$。\n  $x_1 = x_0 - v_1 = 10.0 - 8.0 = 2.0$。\n\n- **第 2 步 (从 k=1 到 k=2):**\n  $v_2 = 0.8 v_1 + 0.8 x_1 = 0.8(8.0) + 0.8(2.0) = 6.4 + 1.6 = 8.0$。\n  $x_2 = x_1 - v_2 = 2.0 - 8.0 = -6.0$。\n\n- **第 3 步 (从 k=2 到 k=3):**\n  $v_3 = 0.8 v_2 + 0.8 x_2 = 0.8(8.0) + 0.8(-6.0) = 6.4 - 4.8 = 1.6$。\n  $x_3 = x_2 - v_3 = -6.0 - 1.6 = -7.6$。\n\n- **第 4 步 (从 k=3 到 k=4):**\n  $v_4 = 0.8 v_3 + 0.8 x_3 = 0.8(1.6) + 0.8(-7.6) = 1.28 - 6.08 = -4.8$。\n  $x_4 = x_3 - v_4 = -7.6 - (-4.8) = -2.8$。\n\n- **第 5 步 (从 k=4 到 k=5):**\n  $v_5 = 0.8 v_4 + 0.8 x_4 = 0.8(-4.8) + 0.8(-2.8) = -3.84 - 2.24 = -6.08$。\n  $x_5^{(B)} = x_4 - v_5 = -2.8 - (-6.08) = 3.28$。\n\n尽管所选参数可能看起来很合理，但该算法表现出发散的迹象，因为位置的幅度在初始下降后开始增加（$|x_0|=10, |x_1|=2, |x_2|=6, |x_3|=7.6, |x_4|=2.8, |x_5|=3.28$）。\n\n**第三部分：最终计算**\n\n我们需要计算比值 $R = x_5^{(B)} / x_5^{(A)}$。\n使用计算出的值：\n$R = \\frac{3.28}{-0.3125} = -10.496$。\n\n题目要求答案四舍五入到三位有效数字。\n$R \\approx -10.5$。", "answer": "$$\\boxed{-10.5}$$", "id": "2187798"}]}