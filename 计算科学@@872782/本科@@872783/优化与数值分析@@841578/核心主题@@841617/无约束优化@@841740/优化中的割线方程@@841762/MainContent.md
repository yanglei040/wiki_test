## 引言
在[数值优化](@entry_id:138060)的广阔领域中，求解[目标函数](@entry_id:267263)的梯度或Hessian矩阵计算成本高昂或不可解析的[无约束优化](@entry_id:137083)问题，是一个长期存在的挑战。传统的[牛顿法](@entry_id:140116)虽然收敛速度快，但对[二阶导数](@entry_id:144508)信息的依赖限制了其实用性。为了解决这一知识鸿沟，[拟牛顿法](@entry_id:138962)应运而生，它通过迭代地近似Hessian矩阵来兼顾效率与速度。而所有拟牛顿法的核心，都构建在一个优雅而深刻的数学关系之上——**[割线方程](@entry_id:164522)**。它是[连接函数](@entry_id:636388)局部几何信息与[迭代算法](@entry_id:160288)更新规则的桥梁，是理解现代[优化算法](@entry_id:147840)的关键所在。

本文将带领读者深入探索[割线方程](@entry_id:164522)的世界。在第一章**“原理与机制”**中，我们将从其基本定义出发，揭示其内在的欠定性，并探讨如何通过最小变化原则和曲率条件等机制，构建出如BFGS这样稳定而高效的更新公式。接着，在第二章**“应用与交叉学科联系”**中，我们将视野扩展到[割线方程](@entry_id:164522)在不同场景下的应用，包括其在大规模问题（[L-BFGS](@entry_id:167263)）、[约束优化](@entry_id:635027)（SQP）中的推广，以及它在机器学习、计算工程和理论化学等前沿领域的关键作用。最后，在第三章**“动手实践”**中，读者将通过一系列精心设计的问题，将理论知识转化为实践能力。让我们首先从[割线方程](@entry_id:164522)的基本原理开始，揭开它在优化算法中扮演的核心角色。

## 原理与机制

在[无约束优化](@entry_id:137083)问题中，尤其是对于[目标函数](@entry_id:267263) $f: \mathbb{R}^n \to \mathbb{R}$ 的梯度 $\nabla f$ 或 Hessian 矩阵 $\nabla^2 f$ 计算成本高昂或难以解析获得的情形，[拟牛顿法](@entry_id:138962)（Quasi-Newton Methods）提供了一套强大而高效的迭代求解策略。这些方法的核心思想是，在迭代过程中逐步构建和更新[目标函数](@entry_id:267263)曲率（Hessian矩阵）的近似。而所有这些近似更新的基础，都源于一个被称为 **[割线方程](@entry_id:164522)（Secant Equation）** 的核心关系。本章将深入探讨[割线方程](@entry_id:164522)的原理、其在理论上的根基，以及它在构建稳健且高效的优化算法中所扮演的关键角色。

### 从一维到多维：[割线方程](@entry_id:164522)的定义

理解[割线方程](@entry_id:164522)最直观的方式，是从[一维优化](@entry_id:635076)问题入手。考虑最小化一个光滑单变量函数 $f(x)$。[牛顿法](@entry_id:140116)的迭代步长为 $p_k = -[f''(x_k)]^{-1}f'(x_k)$，这需要计算[二阶导数](@entry_id:144508) $f''(x_k)$。为了避免这一计算，我们可以用一个近似值 $B_k$ 来代替 $f''(x_k)$。问题是，如何构造一个合理的近似 $B_{k+1}$ 呢？

一个自然的想法是，利用最近两次迭代点的信息。假设我们已经从点 $x_k$ 移动到了新的点 $x_{k+1}$。我们希望新的曲率近似 $B_{k+1}$ 能够反映函数在这一步的变化。我们可以构造一个关于[一阶导数](@entry_id:749425) $f'(x)$ 的[线性模型](@entry_id:178302) $m(x)$，该模型在点 $x_{k+1}$ 处展开：
$$ m(x) = f'(x_{k+1}) + B_{k+1}(x - x_{k+1}) $$
在这个模型中，$B_{k+1}$ 代表了一阶导数 $f'(x)$ 的变化率，因此它自然地成为了[二阶导数](@entry_id:144508) $f''(x_{k+1})$ 的近似。为了确定 $B_{k+1}$，我们施加一个条件：要求这个[线性模型](@entry_id:178302)在**上一个**迭代点 $x_k$ 处是精确的，即 $m(x_k) = f'(x_k)$。代入模型可得：
$$ f'(x_k) = f'(x_{k+1}) + B_{k+1}(x_k - x_{k+1}) $$
整理后，我们得到了 $B_{k+1}$ 的表达式 [@problem_id:2220297]：
$$ B_{k+1} = \frac{f'(x_{k+1}) - f'(x_k)}{x_{k+1} - x_k} $$
这个形式我们非常熟悉，它正是导数 $f'(x)$ 在 $x_k$ 和 $x_{k+1}$ 两点间的[差商](@entry_id:136462)，即用一条割线（secant line）的斜率来近似函数 $f'(x)$ 的导数（也就是 $f''(x)$）。这便是**一维情况下的[割线方程](@entry_id:164522)**。

这个近似的精确度如何？从[微分](@entry_id:158718)[中值定理](@entry_id:141085)（Mean Value Theorem）的角度看，如果 $f'$ 在 $[x_k, x_{k+1}]$ 上连续可导，那么必定存在一个点 $\xi \in (x_k, x_{k+1})$，使得：
$$ f''(\xi) = \frac{f'(x_{k+1}) - f'(x_k)}{x_{k+1} - x_k} $$
这意味着，由[割线方程](@entry_id:164522)计算出的 $B_{k+1}$ 并非在 $x_{k+1}$ 处的精确[二阶导数](@entry_id:144508)值，而是区间 $(x_k, x_{k+1})$ 内某一点 $\xi$ 的**精确**[二阶导数](@entry_id:144508)值 [@problem_id:2220270]。例如，在对函数 $f(x) = \frac{1}{4}x^4 - \frac{7}{2}x^2 - 6x$ 于区间 $[2, 4]$ 应用该关系时，可以精确计算出中值点 $\xi \approx 3.055$。当 $x_{k+1}$ 趋近于 $x_k$ 时，$\xi$ 也将趋近于 $x_{k+1}$，此时 $B_{k+1}$ 成为对 $f''(x_{k+1})$ 的一个良好近似。然而，在实际的迭代步中，$x_{k+1}-x_k$ 通常是有限大小的，因此 $B_{k+1}$ 与 $f''(x_{k+1})$ 之间存在偏差 [@problem_id:2220226]。

现在，我们将这个思想推广到多维空间 $\mathbb{R}^n$。设 $x_k$ 和 $x_{k+1}$ 是两个连续的迭代点。我们定义：
- **步长向量 (step vector)**: $s_k = x_{k+1} - x_k$
- **梯度变化向量 (gradient difference vector)**: $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$

类似于一维情况，我们要求新的 Hessian 近似矩阵 $B_{k+1}$ 能够解释最近一步的梯度变化。在多维空间中，一阶泰勒展开告诉我们 $y_k = \nabla f(x_{k+1}) - \nabla f(x_k) \approx \nabla^2 f(x_{k+1}) (x_{k+1} - x_k) = \nabla^2 f(x_{k+1}) s_k$。拟牛顿法的核心就是将这个近似关系式强制为等式，以此来约束新的 Hessian 近似矩阵 $B_{k+1}$：
$$ B_{k+1} s_k = y_k $$
这就是**多维情况下的[割线方程](@entry_id:164522)** [@problem_id:2220225]。它构成了几乎所有主流拟牛顿法（如 BFGS、DFP、Broyden族方法）更新规则的基石。同样，对于近似**逆 Hessian 矩阵**的方法（这在实践中更常见，因为它避免了[求解线性方程组](@entry_id:169069)），[割线方程](@entry_id:164522)的形式变为：
$$ H_{k+1} y_k = s_k $$
其中 $H_{k+1}$ 是对 $[\nabla^2 f(x_{k+1})]^{-1}$ 的近似。

### 约束的不足：[割线方程](@entry_id:164522)的欠定性

虽然[割线方程](@entry_id:164522) $B_{k+1} s_k = y_k$ 优雅地捕捉了函数曲率的核心信息，但它本身并不足以唯一确定 Hessian 近似矩阵 $B_{k+1}$。在 $n$ 维空间中，$B_{k+1}$ 是一个对称矩阵，有 $\frac{n(n+1)}{2}$ 个独立元素需要确定。然而，[割线方程](@entry_id:164522) $B_{k+1} s_k = y_k$ 是一个向量方程，只提供了 $n$ 个线性约束。当 $n > 1$ 时，这是一个**欠定（underdetermined）**系统。

为了更具体地理解这一点，考虑一个二维（$n=2$）的例子。设 $s_k = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ 和 $y_k = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$。我们寻找一个[对称矩阵](@entry_id:143130) $B = \begin{pmatrix} a  b \\ b  c \end{pmatrix}$ 满足 $B s_k = y_k$。计算可得：
$$ \begin{pmatrix} a  b \\ b  c \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} a \\ b \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix} $$
这个方程确定了 $a=2$ 和 $b=1$，但是对于元素 $c$ 没有施加任何约束。因此，任何形如 $\begin{pmatrix} 2  1 \\ 1  c \end{pmatrix}$ 的矩阵，无论 $c$ 取何值，都是一个满足[割线方程](@entry_id:164522)和对称性要求的有效 Hessian 近似。例如，$B_1 = \begin{pmatrix} 2  1 \\ 1  3 \end{pmatrix}$ 和 $B_2 = \begin{pmatrix} 2  1 \\ 1  -1 \end{pmatrix}$ 都是合法的解 [@problem_id:2220288]。

这种不唯一性意味着我们需要引入额外的准则来从无限多的可能性中挑选一个“最优”的 $B_{k+1}$。不同的准则将导出不同的拟牛顿更新公式。

### 选择的智慧：最小变化原则与更新公式

最著名和最成功的准则之一是**最小变化原则（least change principle）**。其哲学思想是：既然我们对函数的全局性质知之甚少，那么在满足新信息（即[割线方程](@entry_id:164522)）的前提下，我们应当对当前的知识（即 $B_k$）做出尽可能小的改动。

形式上，这意味着我们寻找的 $B_{k+1}$ 是以下[优化问题](@entry_id:266749)的解：
$$ \min_{B} \|B - B_k\| \quad \text{subject to} \quad B^T = B \text{ and } B s_k = y_k $$
其中，$\|\cdot\|$ 是某种[矩阵范数](@entry_id:139520)。不同的范数选择会产生不同的更新公式。

- **Broyden's Method**: 当使用 Frobenius 范数 $\|A\|_F^2 = \sum_{i,j} a_{ij}^2$ 时，这个问题的解是唯一的，并给出了著名的 **Broyden's "good" update** 公式 [@problem_id:2220262]：
  $$ B_{k+1} = B_k + \frac{(y_k - B_k s_k)s_k^T}{s_k^T s_k} $$
  这是一个秩一（rank-one）更新，因为它在原有矩阵 $B_k$ 的基础上增加了一个秩为1的矩阵。这个更新公式在[求解非线性方程](@entry_id:177343)组时非常流行。

- **BFGS Method**: 在[优化问题](@entry_id:266749)中，我们更希望 Hessian 近似矩阵 $B_k$ 保持**[正定性](@entry_id:149643)**，因为这能保证通过求解 $B_k p_k = -\nabla f(x_k)$ 得到的搜索方向 $p_k$ 是[下降方向](@entry_id:637058)。Broyden's update 不保证[正定性](@entry_id:149643)的维持。为此，发展出了 BFGS (Broyden–Fletcher–Goldfarb–Shanno) 更新公式。它源于一个加权的 Frobenius 范数，并能在线搜索满足特定条件时保持[正定性](@entry_id:149643)。其更新公式为：
  $$ B_{k+1} = B_k - \frac{B_k s_k s_k^T B_k}{s_k^T B_k s_k} + \frac{y_k y_k^T}{y_k^T s_k} $$
  相应地，逆 Hessian 的 BFGS 更新公式（在实践中更常用）为：
  $$ H_{k+1} = \left(I - \frac{s_k y_k^T}{y_k^T s_k}\right) H_k \left(I - \frac{y_k s_k^T}{y_k^T s_k}\right) + \frac{s_k s_k^T}{y_k^T s_k} $$
  一个完整的迭代步骤是：首先利用当前的逆 Hessian 近似 $H_k$ 和梯度 $\nabla f(x_k)$ 计算搜索方向 $p_k = -H_k \nabla f(x_k)$；然后通过线搜索找到步长 $\alpha_k$ 得到新点 $x_{k+1}$；最后利用 $s_k$ 和 $y_k$ 通过 BFGS 公式更新 $H_k$ 得到 $H_{k+1}$，为下一步计算做好准备 [@problem_id:2220257]。

### 确保稳健性：曲率条件与线搜索

在 BFGS 更新公式中，分母上都出现了 $y_k^T s_k$ 这一项。为了使更新有定义且数值稳定，必须有 $y_k^T s_k \neq 0$。更进一步，为了保持 Hessian 近似的正定性（假设 $B_k$ 已经正定），必须满足一个更强的条件，即**曲率条件（curvature condition）**：
$$ s_k^T y_k > 0 $$
这个条件在几何上意味着步长向量 $s_k$ 和梯度变化向量 $y_k$ 之间的夹角小于 $90$ 度。直观地看，它表明沿着 $s_k$ 方向移动时，函数梯度的投影有所增加，这意味着函数在该方向上是向上“弯曲”的，这与正定的 Hessian 矩阵所代表的[局部凸性](@entry_id:271002)是一致的。

如果曲率条件不满足，例如 $s_k^T y_k  0$，BFGS 更新可能会破坏正定性。在一个假设场景中，如果 $B_k=I$ （正定）但 $y_k = -c s_k$ 其中 $c>0$，那么 $s_k^T y_k = -c \|s_k\|^2  0$。经过 BFGS 更新后，新的矩阵 $B_{k+1}$ 会在 $s_k$ 方向上有一个负[特征值](@entry_id:154894)，从而失去正定性，这会导致后续的搜索方向不再是下降方向，算法可能失败 [@problem_id:2220236]。

幸运的是，我们不必被动地希望曲率条件得到满足。通过在算法中加入一个精心设计的**线搜索（line search）**程序，我们可以主动确保这一条件成立。标准的线搜索程序旨在寻找一个步长 $\alpha_k > 0$，使得 $x_{k+1} = x_k + \alpha_k p_k$ 不仅降低了函数值，还满足所谓的**[Wolfe条件](@entry_id:171378)**。其中，第二个[Wolfe条件](@entry_id:171378)（也称曲率条件）要求：
$$ \nabla f(x_{k+1})^T p_k \ge c_2 \nabla f(x_k)^T p_k $$
其中 $p_k$ 是搜索方向，$c_2$ 是一个满足 $0  c_2  1$ 的常数。由于 $p_k$ 是[下降方向](@entry_id:637058)，$\nabla f(x_k)^T p_k  0$。这个条件确保了新点的斜率（在 $p_k$ 方向上）不会“太负”，即斜率有足够的增加。

这个线搜索条件与我们之前讨论的曲率条件有着直接的联系。将 $s_k = \alpha_k p_k$ 和 $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$ 代入 $s_k^T y_k$：
$$ s_k^T y_k = (\alpha_k p_k)^T (\nabla f(x_{k+1}) - \nabla f(x_k)) = \alpha_k (\nabla f(x_{k+1})^T p_k - \nabla f(x_k)^T p_k) $$
应用第二个[Wolfe条件](@entry_id:171378)，我们得到：
$$ s_k^T y_k \ge \alpha_k (c_2 \nabla f(x_k)^T p_k - \nabla f(x_k)^T p_k) = \alpha_k (1 - c_2) (-\nabla f(x_k)^T p_k) $$
因为 $\alpha_k > 0$, $1-c_2 > 0$ 以及 $-\nabla f(x_k)^T p_k > 0$（因为 $p_k$ 是下降方向），所以我们能保证 $s_k^T y_k > 0$ [@problem_id:2220237]。这完美地展示了现代[优化算法](@entry_id:147840)设计的精妙之处：[线搜索](@entry_id:141607)程序不仅是为了找到更优的点，它还为 Hessian 近似更新的稳定性和有效性提供了理论保障。

### 通往高效之路：Dennis-Moré条件与[超线性收敛](@entry_id:141654)

一个稳定可靠的算法是基础，但我们同样追求其[收敛速度](@entry_id:636873)。拟牛顿法的一大吸[引力](@entry_id:175476)在于，它们通常能实现**[超线性收敛](@entry_id:141654)（superlinear convergence）**，即误差的[收敛速度](@entry_id:636873)快于任何线[性比](@entry_id:172643)率，但未必能达到牛顿法的二次收敛。那么，一个拟牛顿法要实现[超线性收敛](@entry_id:141654)，其 Hessian 近似 $B_k$ 需要满足什么条件呢？

答案由著名的 **Dennis-Moré 条件**给出。该条件指出，对于一个收敛到解 $x_*$ 的拟牛顿法序列 $x_k$，其实现[超线性收敛](@entry_id:141654)的充分必要条件是：
$$ \lim_{k \to \infty} \frac{\|(B_k - G_*) s_k\|}{\|s_k\|} = 0 $$
其中 $G_* = \nabla^2 f(x_*)$ 是在解 $x_*$ 处的真实 Hessian 矩阵。

这个条件非常深刻。它并不要求 Hessian 近似矩阵 $B_k$ 本身收敛到真实的 Hessian 矩阵 $G_*$（即 $\lim_{k \to \infty} B_k = G_*$），这是一个过强的要求。相反，它只要求 $B_k$ 作用在步长方向 $s_k$ 上的效果越来越接近真实 Hessian 矩阵 $G_*$ 的效果。换言之，只要 Hessian 近似在算法所探索的方向上足够精确，就足以保证快速收敛。

我们可以通过一个假设性的例子来具体理解这个条件。假设误差项 $(B_k - G_*)s_k$ 可以分解为平行于 $s_k$ 的分量和正交于 $s_k$ 的分量，其大小与步长 $\|s_k\|$ 的幂次相关：
$$ (B_k - G_*)s_k = (C_1 \|s_k\|^{p_1}) s_k + (C_2 \|s_k\|^{p_2}) u_k $$
其中 $u_k$ 是与 $s_k$ 正交的单位向量。将此代入 Dennis-Moré 条件的表达式，经过计算可以发现，为了使极限为零，指数必须满足 $p_1 > 0$ 和 $p_2 > 1$。这意味着，沿搜索方向的误差分量衰减速度可以较慢（只要其阶数 $p_1$ 大于0即可），但正交于搜索方向的误差分量必须以比 $\|s_k\|$ 更快的速度衰减（即阶数 $p_2$ 大于1）[@problem_id:2220243]。这一结论为设计和分析具有[超线性收敛](@entry_id:141654)性质的拟牛顿算法提供了精确的数学指引。

总之，[割线方程](@entry_id:164522)不仅是构建 Hessian [矩阵近似](@entry_id:149640)的起点，更是连接优化算法中迭代更新、[线搜索](@entry_id:141607)稳健性和收敛速率分析等多个核心环节的纽带。对它的深入理解，是掌握现代[非线性优化](@entry_id:143978)理论与实践的关键。