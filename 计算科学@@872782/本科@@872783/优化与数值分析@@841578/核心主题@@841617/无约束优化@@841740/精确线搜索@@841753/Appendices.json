{"hands_on_practices": [{"introduction": "为了开始我们的实践探索，我们将从一个基础示例入手。这个练习 [@problem_id:2170891] 将引导你对一个简单的二次函数执行一步最速下降法，并应用精确线搜索来找到最优步长。掌握这一核心流程对于理解更复杂的优化算法如何运作至关重要。", "problem": "考虑最小化二次目标函数 $f(x_1, x_2) = (x_1 + x_2 - 2)^2$ 的优化问题。我们将从初始点 $\\mathbf{x}_0 = (0, 0)^T$ 开始，执行一步最速下降算法。这一步的搜索方向是最速下降方向，定义为 $\\mathbf{p}_0 = -\\nabla f(\\mathbf{x}_0)$，其中 $\\nabla f$ 是函数的梯度。\n\n为了找到沿此方向移动的最优距离，我们执行精确线搜索。该过程涉及求解一个一维最小化问题，以找到使函数 $g(\\alpha) = f(\\mathbf{x}_0 + \\alpha\\mathbf{p}_0)$ 最小化的步长 $\\alpha  0$。\n\n确定由此过程得到的精确最优步长 $\\alpha^*$。", "solution": "给定二次函数 $f(x_{1}, x_{2}) = (x_{1} + x_{2} - 2)^{2}$ 和初始点 $\\mathbf{x}_{0} = (0, 0)^T$。最速下降方向定义为 $\\mathbf{p}_{0} = -\\nabla f(\\mathbf{x}_{0})$，其中梯度通过微分计算：\n$$\n\\nabla f(x_{1}, x_{2}) = \\left(\\frac{\\partial f}{\\partial x_{1}}, \\frac{\\partial f}{\\partial x_{2}}\\right).\n$$\n由于 $f(x_{1}, x_{2}) = (x_{1} + x_{2} - 2)^{2}$，根据链式法则，\n$$\n\\frac{\\partial f}{\\partial x_{1}} = 2(x_{1} + x_{2} - 2)\\cdot \\frac{\\partial}{\\partial x_{1}}(x_{1} + x_{2} - 2) = 2(x_{1} + x_{2} - 2),\n$$\n并且类似地，\n$$\n\\frac{\\partial f}{\\partial x_{2}} = 2(x_{1} + x_{2} - 2).\n$$\n因此，在 $\\mathbf{x}_{0} = (0, 0)^T$ 处，梯度为：\n$$\n\\nabla f(\\mathbf{x}_{0}) = \\begin{pmatrix} 2(0+0-2) \\\\ 2(0+0-2) \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -4 \\end{pmatrix}\n$$\n所以最速下降方向为\n$$\n\\mathbf{p}_{0} = -\\nabla f(\\mathbf{x}_{0}) = \\begin{pmatrix} 4 \\\\ 4 \\end{pmatrix}.\n$$\n对于精确线搜索，定义一元函数\n$$\ng(\\alpha) = f(\\mathbf{x}_{0} + \\alpha \\mathbf{p}_{0}) = f(0 + 4\\alpha, 0 + 4\\alpha) = f(4\\alpha, 4\\alpha).\n$$\n代入 $f$ 中，\n$$\ng(\\alpha) = (4\\alpha + 4\\alpha - 2)^{2} = (8\\alpha - 2)^{2}.\n$$\n为了在 $\\alpha  0$ 上最小化 $g(\\alpha)$，将其导数设为零：\n$$\ng'(\\alpha) = 2(8\\alpha - 2)\\cdot 8 = 16(8\\alpha - 2).\n$$\n求解 $g'(\\alpha) = 0$：\n$$\n16(8\\alpha - 2) = 0 \\;\\;\\Longrightarrow\\;\\; 8\\alpha - 2 = 0 \\;\\;\\Longrightarrow\\;\\; \\alpha^{*} = \\frac{1}{4}.\n$$\n此解满足 $\\alpha^{*}  0$ 并得到沿搜索方向的精确最小化子。", "answer": "$$\\boxed{\\frac{1}{4}}$$", "id": "2170891"}, {"introduction": "优化原理不仅限于多项式函数。在这个问题中 [@problem_id:2170899]，我们将精确线搜索技术应用于一个三角函数，这引入了处理多个局部最小值的挑战。这个练习将锻炼你在多个可能性中识别正确解的能力，并突显该方法在不同函数类型上的通用性。", "problem": "考虑使用迭代优化算法最小化单变量目标函数 $f(x)$ 的过程。此类算法中的一个关键步骤是线搜索，即从一个点 $x_k$ 出发，我们寻找一个步长 $\\alpha$ 来最小化函数在给定搜索方向 $p_k$ 上的值。序列中的下一个点则由 $x_{k+1} = x_k + \\alpha p_k$ 确定。\n\n设目标函数为 $f(x) = \\sin(x) + \\cos(x)$，其中参数 $x$ 的单位是弧度。假设我们位于初始点 $x_0 = 0$。搜索方向选择为最速下降方向，对于单变量函数，该方向由 $p_0 = -f'(x_0)$ 给出，其中 $f'(x_0)$ 是 $f(x)$ 在 $x_0$ 处的导数。\n\n确定沿此方向最小化函数的最小正步长 $\\alpha  0$。也就是说，找到最小的正 $\\alpha$，使得新函数 $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$ 最小化。\n\n请以闭式解析表达式的形式给出 $\\alpha$ 的精确值。", "solution": "给定 $f(x) = \\sin(x) + \\cos(x)$ 和 $x_{0} = 0$。最速下降方向为 $p_{0} = -f'(x_{0})$。计算其导数：\n$$\nf'(x) = \\cos(x) - \\sin(x).\n$$\n在 $x_{0} = 0$ 处求值：\n$$\nf'(0) = \\cos(0) - \\sin(0) = 1 - 0 = 1,\n$$\n所以\n$$\np_{0} = -f'(0) = -1.\n$$\n定义线搜索目标函数 $\\phi(\\alpha) = f(x_{0} + \\alpha p_{0})$：\n$$\n\\phi(\\alpha) = f(0 + \\alpha(-1)) = f(-\\alpha) = \\sin(-\\alpha) + \\cos(-\\alpha) = -\\sin(\\alpha) + \\cos(\\alpha).\n$$\n为了在 $\\alpha  0$ 的范围内最小化 $\\phi(\\alpha)$，我们对其求导并令其等于零：\n$$\n\\phi'(\\alpha) = -\\cos(\\alpha) - \\sin(\\alpha) = 0 \\quad \\Longleftrightarrow \\quad \\cos(\\alpha) + \\sin(\\alpha) = 0.\n$$\n这得到\n$$\n\\tan(\\alpha) = -1 \\quad \\text{with} \\quad \\cos(\\alpha) \\neq 0 \\quad \\Longrightarrow \\quad \\alpha = -\\frac{\\pi}{4} + n\\pi, \\quad n \\in \\mathbb{Z}.\n$$\n在正数解中，当 $n=1$ 时得到最小解：\n$$\n\\alpha = -\\frac{\\pi}{4} + \\pi = \\frac{3\\pi}{4}.\n$$\n通过二阶导数验证其为最小值：\n$$\n\\phi''(\\alpha) = \\sin(\\alpha) - \\cos(\\alpha).\n$$\n在 $\\alpha = \\frac{3\\pi}{4}$ 处，$\\sin\\left(\\frac{3\\pi}{4}\\right)  0$ 且 $\\cos\\left(\\frac{3\\pi}{4}\\right)  0$，因此 $\\phi''\\left(\\frac{3\\pi}{4}\\right)  0$，这确认了该点是一个局部最小值。由于 $\\phi$ 是周期为 $2\\pi$ 的函数，且所有最小值都出现在 $\\alpha = \\frac{3\\pi}{4} + 2\\pi k$（其中 $k \\in \\mathbb{Z}$）处，因此最小的正数最小值点是 $\\alpha = \\frac{3\\pi}{4}$。", "answer": "$$\\boxed{\\frac{3\\pi}{4}}$$", "id": "2170899"}, {"introduction": "超越直接计算，这个进阶练习 [@problem_id:2170940] 挑战你进行逆向思考。通过观察最速下降算法的特定行为，你将推断出目标函数本身的一个基本属性。这个问题揭示了算法轨迹与函数等值线的几何特性之间的深刻联系，有助于培养对优化动态更直观的理解。", "problem": "考虑一个二次目标函数 $f(x_1, x_2) = \\frac{A}{2} x_1^2 + \\frac{B}{2} x_2^2 - C x_2$，其中 $A, B, C$ 是正实数常数。使用带有精确线搜索的最速下降法来求此函数的最小值。观察到对于任意初始点 $\\mathbf{x}_0 = (x_{0,1}, 0)$ 且 $x_{0,1} \\neq 0$，该算法的第一次迭代结果 $\\mathbf{x}_1$ 总是落在 $x_2$ 轴上。根据此观察，确定比率 $B/A$ 的值。", "solution": "我们通过带有精确线搜索的最速下降法来最小化二次函数 $f(x_{1},x_{2})=\\frac{A}{2}x_{1}^{2}+\\frac{B}{2}x_{2}^{2}-C x_{2}$，其中 $A0$，$B0$，$C0$。其梯度和Hessian矩阵为\n$$\n\\nabla f(x_{1},x_{2})=\\begin{pmatrix}A x_{1}\\\\ B x_{2}-C\\end{pmatrix},\\qquad H=\\begin{pmatrix}A  0\\\\0  B\\end{pmatrix}.\n$$\n设初始点为 $\\mathbf{x}_{0}=(x_{0,1},0)^T$ 且 $x_{0,1}\\neq 0$。那么初始梯度为\n$$\n\\mathbf{g}_{0}=\\nabla f(\\mathbf{x}_{0})=\\begin{pmatrix}A x_{0,1}\\\\ -C\\end{pmatrix}.\n$$\n最速下降方向是 $\\mathbf{d}_{0}=-\\mathbf{g}_{0}$，并且对于精确线搜索，步长 $\\alpha_{0}$ 使得 $\\phi(\\alpha)=f(\\mathbf{x}_{0}+\\alpha \\mathbf{d}_{0})$ 最小化。对于二次函数，使用 $\\nabla f(\\mathbf{x}_{0}+\\alpha \\mathbf{d}_{0})=\\nabla f(\\mathbf{x}_{0})+\\alpha H \\mathbf{d}_{0}$，我们有\n$$\n\\phi'(\\alpha)=\\nabla f(\\mathbf{x}_{0}+\\alpha \\mathbf{d}_{0})^{\\top} \\mathbf{d}_{0}\n=\\left(\\mathbf{g}_{0}+\\alpha H \\mathbf{d}_{0}\\right)^{\\top} \\mathbf{d}_{0}\n=\\mathbf{g}_{0}^{\\top} \\mathbf{d}_{0}+\\alpha \\mathbf{d}_{0}^{\\top} H \\mathbf{d}_{0}.\n$$\n令 $\\phi'(\\alpha_{0})=0$ 可得\n$$\n\\alpha_{0}=\\frac{\\mathbf{g}_{0}^{\\top} \\mathbf{g}_{0}}{\\mathbf{g}_{0}^{\\top} H \\mathbf{g}_{0}}.\n$$\n计算所需的量：\n$$\n\\mathbf{g}_{0}^{\\top} \\mathbf{g}_{0}=(A x_{0,1})^{2}+C^{2}=A^{2}x_{0,1}^{2}+C^{2},\n$$\n$$\nH \\mathbf{g}_{0}=\\begin{pmatrix}A  0\\\\0  B\\end{pmatrix}\\begin{pmatrix}A x_{0,1}\\\\ -C\\end{pmatrix}\n=\\begin{pmatrix}A^{2} x_{0,1}\\\\ -B C\\end{pmatrix},\n$$\n$$\n\\mathbf{g}_{0}^{\\top} H \\mathbf{g}_{0}=(A x_{0,1})(A^{2} x_{0,1})+(-C)(-B C)=A^{3} x_{0,1}^{2}+B C^{2}.\n$$\n因此\n$$\n\\alpha_{0}=\\frac{A^{2} x_{0,1}^{2}+C^{2}}{A^{3} x_{0,1}^{2}+B C^{2}}.\n$$\n第一次迭代的结果是\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0} \\mathbf{g}_{0}=\\begin{pmatrix}x_{0,1}-\\alpha_{0} A x_{0,1}\\\\ 0-\\alpha_{0}(-C)\\end{pmatrix},\n$$\n所以其第一个分量是\n$$\nx_{1,1}=x_{0,1}\\left(1-\\alpha_{0} A\\right).\n$$\n观察结果表明，对于任意 $x_{0,1}\\neq 0$，$ \\mathbf{x}_{1} $ 总是落在 $x_2$ 轴上，因此对于所有这样的 $x_{0,1}$，都有 $x_{1,1}=0$。因此，对于所有的 $x_{0,1}$，必须有 $1-\\alpha_{0} A=0$ 成立，即\n$$\n\\alpha_{0}=\\frac{1}{A}.\n$$\n将此结果与精确线搜索的表达式相等，得到\n$$\n\\frac{A^{2} x_{0,1}^{2}+C^{2}}{A^{3} x_{0,1}^{2}+B C^{2}}=\\frac{1}{A}.\n$$\n交叉相乘并化简，\n$$\nA\\left(A^{2} x_{0,1}^{2}+C^{2}\\right)=A^{3} x_{0,1}^{2}+B C^{2}\n\\;\\;\\Longrightarrow\\;\\;\nA^{3} x_{0,1}^{2}+A C^{2}=A^{3} x_{0,1}^{2}+B C^{2}\n\\;\\;\\Longrightarrow\\;\\;\n(A-B) C^{2}=0.\n$$\n因为 $C0$，所以可得 $A=B$，因此\n$$\n\\frac{B}{A}=1.\n$$\n这个条件也是充分的，因为如果 $A=B$，那么对于每一个 $x_{0,1}$，都有 $\\alpha_{0}=(A^{2} x_{0,1}^{2}+C^{2})/(A^{3} x_{0,1}^{2}+A C^{2})=1/A$，从而得到 $x_{1,1}=x_{0,1}(1-\\alpha_{0} A)=0$。", "answer": "$$\\boxed{1}$$", "id": "2170940"}]}