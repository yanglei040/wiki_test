{"hands_on_practices": [{"introduction": "本练习将介绍如何使用读取深度数据（这是CNV检测的主要信号之一）来进行分析。你将建立一个统计检验，以区分真实的嵌合缺失和仅表现为系统性低覆盖度的基因组区域，这是基因组学中一个常见而关键的挑战。通过该练习[@problem_id:2382685]，你将加深对泊松计数模型以及如何应用最大似然原理来解决实际生物信息学问题的理解。", "problem": "您将处理一个基于读取深度拷贝数变异（CNV）分析的形式化决策问题。目标是在单个样本中，区分一个真实的、低振幅的嵌合型缺失，与一个仅在对照组中表现出系统性低测序覆盖度的基因组区域。\n\n假设在一个划分为 $n$ 个不重叠区段的固定基因组区域内，读取计数的生成模型如下。设有 $m$ 个对照样本和一个测试（病例）样本。对于区段 $i \\in \\{1,\\dots,n\\}$：\n- 潜在的、特定于区段的基线率为 $\\lambda_i > 0$。\n- 病例样本具有已知的文库大小（暴露量）$L_s > 0$。\n- 对于 $j \\in \\{1,\\dots,m\\}$，对照样本 $j$ 具有已知的文库大小（暴露量）$L_{cj} > 0$。\n- 观测到的读取计数在区段 $i$ 处，病例样本为 $S_i \\in \\mathbb{Z}_{\\ge 0}$，对照样本 $j$ 为 $C_{ij} \\in \\mathbb{Z}_{\\ge 0}$。\n\n假设以均值为条件的独立泊松抽样。考虑两个假设：\n- $H_0$（共享低覆盖度）：对于每个区段 $i$，病例和对照样本共享由其暴露量缩放的相同基线率，\n$$\nS_i \\sim \\mathrm{Pois}(L_s \\lambda_i), \\quad C_{ij} \\sim \\mathrm{Pois}(L_{cj} \\lambda_i).\n$$\n- $H_1$（仅病例中存在嵌合型缺失）：病例特异性率乘以一个因子 $(1-\\rho)$ 而降低，其中未知的嵌合比例参数 $\\rho \\in [0,1)$，而对照组则与 $H_0$ 下的情况保持一致，\n$$\nS_i \\sim \\mathrm{Pois}\\big(L_s (1-\\rho)\\lambda_i\\big), \\quad C_{ij} \\sim \\mathrm{Pois}(L_{cj} \\lambda_i).\n$$\n\n对于给定区域，定义总病例计数 $S_{\\mathrm{tot}} = \\sum_{i=1}^n S_i$，总对照计数 $C_{\\mathrm{tot}} = \\sum_{i=1}^n \\sum_{j=1}^m C_{ij}$，以及总计数 $T_{\\mathrm{tot}} = S_{\\mathrm{tot}} + C_{\\mathrm{tot}}$。定义 $A=L_s$ 和 $B=\\sum_{j=1}^m L_{cj}$。\n\n您的程序必须为每个区域判断，在最大似然意义上，$H_1$ 是否比 $H_0$ 提供了更好的解释。形式上，令 $\\ell_0^\\star$ 表示在 $H_0$ 下最大化的对数似然，$\\ell_1^\\star$ 表示在 $H_1$ 下最大化的对数似然（两者都对 $\\{\\lambda_i\\}_{i=1}^n$ 进行最大化，对于 $H_1$，还需对 $\\rho \\in [0,1)$ 进行最大化）。对于一个区域的决策必须是：\n- 如果 $\\ell_1^\\star > \\ell_0^\\star$，则输出布尔值 True。\n- 否则，输出布尔值 False。\n\n本问题中的输入数据是固定的（无用户输入）。对于以下所有测试用例，$n=5$，$m=2$，$L_s = 100$，$L_{c1} = 100$，$L_{c2} = 100$，因此 $A=100$，$B=200$。对于每个测试用例，程序应使用所提供的病例和两个对照组的各区段计数。\n\n测试套件（每个用例是一个独立的区域；所有计数均为整数，按区段 $i=1,\\dots,5$ 的顺序排列）：\n- 用例1（推定的嵌合型缺失）：\n  - 病例计数 $S = [60,55,65,60,60]$。\n  - 对照组1计数 $C_{\\cdot 1} = [110,105,100,95,90]$。\n  - 对照组2计数 $C_{\\cdot 2} = [110,105,100,95,90]$。\n- 用例2（在病例和对照组中均一致的系统性低覆盖区域）：\n  - 病例计数 $S = [40,40,40,40,40]$。\n  - 对照组1计数 $C_{\\cdot 1} = [40,40,40,40,40]$。\n  - 对照组2计数 $C_{\\cdot 2} = [40,40,40,40,40]$。\n- 用例3（计数消失的极端边缘情况）：\n  - 病例计数 $S = [0,0,0,0,0]$。\n  - 对照组1计数 $C_{\\cdot 1} = [0,0,0,0,0]$。\n  - 对照组2计数 $C_{\\cdot 2} = [0,0,0,0,0]$。\n- 用例4（类似低振幅嵌合的微小偏差）：\n  - 病例计数 $S = [55,50,45,55,45]$。\n  - 对照组1计数 $C_{\\cdot 1} = [60,50,55,90,50]$。\n  - 对照组2计数 $C_{\\cdot 2} = [50,45,40,60,10]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的4个用例的布尔结果列表，用方括号括起来，不含空格。例如，输出格式可能为“[True,False,True,False]”。\n\n此问题不涉及任何物理单位或角度单位。所有必需的量均为无量纲的计数或比例因子。唯一允许的输出是如上定义的布尔值，并按指定单行格式汇总4个测试用例的结果。", "solution": "所述问题是一个适定的统计决策问题，基于计数数据的最大似然估计原理。它在科学上是合理的，形式上是明确的，并且没有矛盾。目标是确定一个包含嵌合型缺失的模型（$H_1$）是否比一个共享系统性覆盖度变异的更简单的零模型（$H_0$）更能拟合观测到的读取计数数据。决策标准是在 $H_1$ 下的最大化对数似然（记为 $\\ell_1^\\star$）是否严格大于在 $H_0$ 下的最大化对数似然（记为 $\\ell_0^\\star$）。\n\n分析过程如下：首先推导出作为嵌合比例参数 $\\rho$ 函数的剖面对数似然，然后建立一个简单的条件来判断何时备择假设 $H_1$ 更优。\n\n在通用模型下，单个区段 $i$ 的观测计数的联合概率质量函数由独立的泊松分布的乘积给出：\n$$\nP(S_i, \\{C_{ij}\\}_{j=1}^m | \\lambda_i, \\rho) = P(S_i | \\lambda_i, \\rho) \\prod_{j=1}^m P(C_{ij} | \\lambda_i)\n$$\n忽略常数项（即计数的阶乘），所有 $n$ 个区段对应的对数似然为：\n$$\n\\ell(\\{\\lambda_i\\}_{i=1}^n, \\rho) = \\sum_{i=1}^n \\left[ S_i \\log\\big(L_s (1-\\rho)\\lambda_i\\big) - L_s (1-\\rho)\\lambda_i + \\sum_{j=1}^m \\left( C_{ij} \\log(L_{cj} \\lambda_i) - L_{cj} \\lambda_i \\right) \\right]\n$$\n通过分离涉及参数 $\\lambda_i$ 和 $\\rho$ 的项，可以重排此表达式：\n$$\n\\ell(\\{\\lambda_i\\}, \\rho) = \\sum_{i=1}^n \\left[ (S_i + \\sum_{j=1}^m C_{ij}) \\log \\lambda_i - (L_s(1-\\rho) + \\sum_{j=1}^m L_{cj}) \\lambda_i \\right] + (\\sum_{i=1}^n S_i) \\log(1-\\rho) + K\n$$\n其中 $K$ 合并了所有相对于 $\\lambda_i$ 和 $\\rho$ 为常数的项（即仅涉及计数和文库大小的项）。令 $T_i = S_i + \\sum_{j=1}^m C_{ij}$ 为区段 $i$ 的总计数，$S_{\\mathrm{tot}} = \\sum_i S_i$，$A = L_s$，以及 $B = \\sum_j L_{cj}$。对数似然变为：\n$$\n\\ell(\\{\\lambda_i\\}, \\rho) = \\sum_{i=1}^n \\left[ T_i \\log \\lambda_i - (A(1-\\rho) + B) \\lambda_i \\right] + S_{\\mathrm{tot}} \\log(1-\\rho) + K\n$$\n为了找到最大化的对数似然，我们首先对于固定的 $\\rho$，求出每个滋扰参数 $\\lambda_i$ 的最大似然估计（MLE）。参数 $\\lambda_i$ 在不同区段之间是解耦的，因此我们可以对每个参数单独进行优化。对 $\\lambda_i$求导并令其为零，得到：\n$$\n\\frac{\\partial \\ell}{\\partial \\lambda_i} = \\frac{T_i}{\\lambda_i} - (A(1-\\rho) + B) = 0\n$$\n这给出了作为 $\\rho$ 函数的 $\\lambda_i$ 的最大似然估计：\n$$\n\\hat{\\lambda}_i(\\rho) = \\frac{T_i}{A(1-\\rho) + B}\n$$\n将 $\\hat{\\lambda}_i(\\rho)$ 代回到对数似然表达式中，得到仅为 $\\rho$ 函数的剖面对数似然：\n$$\n\\ell(\\rho) = \\sum_{i=1}^n \\left[ T_i \\log\\left(\\frac{T_i}{A(1-\\rho) + B}\\right) - T_i \\right] + S_{\\mathrm{tot}} \\log(1-\\rho) + K\n$$\n令 $T_{\\mathrm{tot}} = \\sum_i T_i$。我们可以将依赖于 $\\rho$ 的项与不依赖于 $\\rho$ 的项分开：\n$$\n\\ell(\\rho) = \\left[ \\sum_i (T_i \\log T_i - T_i) + K \\right] - T_{\\mathrm{tot}} \\log(A(1-\\rho) + B) + S_{\\mathrm{tot}} \\log(1-\\rho)\n$$\n令方括号中的项为 $K'$，它是一个关于 $\\rho$ 的常数。对数似然中与对 $\\rho$ 进行优化相关的部分是：\n$$\nf(\\rho) = S_{\\mathrm{tot}} \\log(1-\\rho) - T_{\\mathrm{tot}} \\log(A(1-\\rho) + B)\n$$\n在 $H_0$ 下最大化的对数似然值通过设置 $\\rho=0$ 得到：\n$$\n\\ell_0^\\star = \\ell(0) = K' + f(0) = K' - T_{\\mathrm{tot}} \\log(A+B)\n$$\n在 $H_1$ 下最大化的对数似然是 $\\ell(\\rho)$ 在区间 $\\rho \\in [0, 1)$ 上的上确界：\n$$\n\\ell_1^\\star = \\sup_{\\rho \\in [0, 1)} \\ell(\\rho) = K' + \\sup_{\\rho \\in [0, 1)} f(\\rho)\n$$\n决策标准是 $\\ell_1^\\star > \\ell_0^\\star$，这等价于 $\\sup_{\\rho \\in [0, 1)} f(\\rho) > f(0)$。\n\n为了确定此条件何时成立，我们分析 $f(\\rho)$ 在边界点 $\\rho=0$ 处的行为。该函数的二阶导数是：\n$$\n\\frac{d^2 f}{d\\rho^2} = -\\frac{S_{\\mathrm{tot}}}{(1-\\rho)^2} - \\frac{A^2 T_{\\mathrm{tot}}}{(A(1-\\rho)+B)^2}\n$$\n由于 $S_{\\mathrm{tot}} \\ge 0$，$T_{\\mathrm{tot}} \\ge 0$，且 $A>0$，该二阶导数始终为非正。因此，$f(\\rho)$ 是一个凹函数。对于 $[0,1)$ 上的一个凹函数，其上确界严格大于其在 $\\rho=0$ 处的值，当且仅当该函数在 $\\rho=0$ 处是严格递增的。这由在 $\\rho=0$ 处的一阶导数的符号决定。\n$f(\\rho)$ 的一阶导数是：\n$$\n\\frac{df}{d\\rho} = \\frac{-S_{\\mathrm{tot}}}{1-\\rho} + \\frac{A \\cdot T_{\\mathrm{tot}}}{A(1-\\rho) + B}\n$$\n在 $\\rho=0$ 处求值：\n$$\n\\left. \\frac{df}{d\\rho} \\right|_{\\rho=0} = -S_{\\mathrm{tot}} + \\frac{A \\cdot T_{\\mathrm{tot}}}{A+B} = \\frac{-S_{\\mathrm{tot}}(A+B) + A(S_{\\mathrm{tot}} + C_{\\mathrm{tot}})}{A+B} = \\frac{A C_{\\mathrm{tot}} - B S_{\\mathrm{tot}}}{A+B}\n$$\n其中 $C_{\\mathrm{tot}} = T_{\\mathrm{tot}} - S_{\\mathrm{tot}} = \\sum_i \\sum_j C_{ij}$。\n条件 $\\ell_1^\\star > \\ell_0^\\star$ 等价于 $\\left. \\frac{df}{d\\rho} \\right|_{\\rho=0} > 0$。鉴于 $A+B > 0$，这可简化为以下简洁的决策规则：\n$$\nA C_{\\mathrm{tot}} > B S_{\\mathrm{tot}}\n$$\n这个不等式可以直观地解释。将其重排为 $\\frac{S_{\\mathrm{tot}}}{A}  \\frac{C_{\\mathrm{tot}}}{B}$ 表明，如果病例样本中的归一化总计数严格小于对照组中的归一化总计数，那么缺失假设（$H_1$）更受支持。\n\n现在将此分析应用于所提供的测试用例，参数为 $A = L_s = 100$ 和 $B = L_{c1} + L_{c2} = 100 + 100 = 200$。决策规则是 $100 \\cdot C_{\\mathrm{tot}} > 200 \\cdot S_{\\mathrm{tot}}$，可简化为 $C_{\\mathrm{tot}} > 2 \\cdot S_{\\mathrm{tot}}$。\n\n- **用例 1**：\n  - $S = [60,55,65,60,60] \\implies S_{\\mathrm{tot}} = 300$。\n  - $C_{\\cdot 1} = [110,105,100,95,90] \\implies \\sum_i C_{i1} = 500$。\n  - $C_{\\cdot 2} = [110,105,100,95,90] \\implies \\sum_i C_{i2} = 500$。\n  - $C_{\\mathrm{tot}} = 500 + 500 = 1000$。\n  - 检验：$1000 > 2 \\cdot 300 \\implies 1000 > 600$。此为真。结果：`True`。\n\n- **用例 2**：\n  - $S = [40,40,40,40,40] \\implies S_{\\mathrm{tot}} = 200$。\n  - $C_{\\cdot 1} = [40,40,40,40,40] \\implies \\sum_i C_{i1} = 200$。\n  - $C_{\\cdot 2} = [40,40,40,40,40] \\implies \\sum_i C_{i2} = 200$。\n  - $C_{\\mathrm{tot}} = 200 + 200 = 400$。\n  - 检验：$400 > 2 \\cdot 200 \\implies 400 > 400$。此为假。结果：`False`。\n\n- **用例 3**：\n  - $S = [0,0,0,0,0] \\implies S_{\\mathrm{tot}} = 0$。\n  - $C_{\\cdot 1} = [0,0,0,0,0] \\implies \\sum_i C_{i1} = 0$。\n  - $C_{\\cdot 2} = [0,0,0,0,0] \\implies \\sum_i C_{i2} = 0$。\n  - $C_{\\mathrm{tot}} = 0 + 0 = 0$。\n  - 检验：$0 > 2 \\cdot 0 \\implies 0 > 0$。此为假。结果：`False`。\n\n- **用例 4**：\n  - $S = [55,50,45,55,45] \\implies S_{\\mathrm{tot}} = 250$。\n  - $C_{\\cdot 1} = [60,50,55,90,50] \\implies \\sum_i C_{i1} = 305$。\n  - $C_{\\cdot 2} = [50,45,40,60,10] \\implies \\sum_i C_{i2} = 205$。\n  - $C_{\\mathrm{tot}} = 305 + 205 = 510$。\n  - 检验：$510 > 2 \\cdot 250 \\implies 510 > 500$。此为真。结果：`True`。\n\n四个用例的汇总结果为 `[True, False, False, True]`。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the CNV analysis decision problem for a fixed set of test cases.\n\n    The problem requires comparing two hypotheses, H0 (shared low coverage) and\n    H1 (mosaic deletion in a case sample), based on maximized log-likelihoods.\n    The decision rule simplifies to comparing the normalized total read counts\n    between the case and control cohorts.\n    \"\"\"\n    \n    # Define problem-wide parameters\n    L_s = 100.0  # Case sample library size (exposure)\n    L_c1 = 100.0 # Control 1 library size\n    L_c2 = 100.0 # Control 2 library size\n    \n    # Total exposures for case (A) and controls (B)\n    A = L_s\n    B = L_c1 + L_c2\n    \n    # Define the test cases from the problem statement.\n    # Each case is a dictionary containing count vectors for the case (S),\n    # control 1 (C1), and control 2 (C2).\n    test_cases = [\n        # Case 1: Putative mosaic deletion\n        {\n            \"S\": np.array([60, 55, 65, 60, 60]),\n            \"C1\": np.array([110, 105, 100, 95, 90]),\n            \"C2\": np.array([110, 105, 100, 95, 90])\n        },\n        # Case 2: Systematically low region\n        {\n            \"S\": np.array([40, 40, 40, 40, 40]),\n            \"C1\": np.array([40, 40, 40, 40, 40]),\n            \"C2\": np.array([40, 40, 40, 40, 40])\n        },\n        # Case 3: Extreme edge with vanishing counts\n        {\n            \"S\": np.array([0, 0, 0, 0, 0]),\n            \"C1\": np.array([0, 0, 0, 0, 0]),\n            \"C2\": np.array([0, 0, 0, 0, 0])\n        },\n        # Case 4: Subtle low-amplitude mosaic-like deviation\n        {\n            \"S\": np.array([55, 50, 45, 55, 45]),\n            \"C1\": np.array([60, 50, 55, 90, 50]),\n            \"C2\": np.array([50, 45, 40, 60, 10])\n        }\n    ]\n\n    results = []\n    \n    # As derived, the decision rule l1* > l0* simplifies to:\n    # A * C_tot > B * S_tot\n    # where S_tot and C_tot are total counts for the case and controls, respectively.\n    \n    for case in test_cases:\n        # Calculate total counts for the case sample.\n        S_tot = np.sum(case[\"S\"])\n        \n        # Calculate total counts for the control cohort.\n        C_tot = np.sum(case[\"C1\"]) + np.sum(case[\"C2\"])\n        \n        # Apply the decision rule.\n        # The hypothesis H1 (deletion) is favored if the inequality holds.\n        decision = A * C_tot  B * S_tot\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver.\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2382685"}, {"introduction": "在读取深度分析的基础上，本练习的重点从评估单个区域转向扫描整条染色体以确定CNV的边界。你将实现经典的变点检测方法——累积和 (CUSUM) 算法，来精确定位拷贝数状态发生变化的位点。这个动手实践[@problem_id:2382736]展示了许多CNV检测工具中使用的核心算法范式，并提供了序贯数据分析的实践经验。", "problem": "给定非负整数计数，表示在分段恒定拷贝数模型下，沿单条染色体的 bin 级读取深度测量值。假设以下生成性假设成立：对于由 $t \\in \\{1,\\dots,n\\}$ 索引的每个基因组 bin，观测到的计数 $x_t \\in \\mathbb{Z}_{\\ge 0}$ 是来自均值为 $\\lambda_t$ 的泊松分布的独立实现，其中均值 $\\lambda_t$ 在 $t$ 上是分段恒定的，并在有限集合 $\\{\\lambda^{(0)},\\lambda^{(u)},\\lambda^{(d)}\\}$ 中取值，且具有严格排序 $\\lambda^{(u)} > \\lambda^{(0)} > \\lambda^{(d)} > 0$。将 $\\lambda^{(0)}$ 解释为参考二倍体状态，$\\lambda^{(u)}$ 解释为重复状态，$\\lambda^{(d)}$ 解释为缺失状态。序列以参考状态 $\\lambda^{(0)}$ 开始。\n\n对于任何具有均值 $(\\mu_\\alpha,\\mu_\\beta)$ 的有序状态对 $(\\alpha \\to \\beta)$，定义单次观测的对数似然比增量\n$$\nw_{\\alpha \\to \\beta}(x) \\;=\\; \\log\\!\\left(\\frac{ \\Pr(X=x \\mid X \\sim \\text{Poisson}(\\mu_\\beta)) }{ \\Pr(X=x \\mid X \\sim \\text{Poisson}(\\mu_\\alpha)) }\\right) \\;=\\; x \\log\\!\\left(\\frac{\\mu_\\beta}{\\mu_\\alpha}\\right) - (\\mu_\\beta - \\mu_\\alpha).\n$$\n考虑以下具有允许转移的有限状态变化模型：\n- 从参考状态 $\\lambda^{(0)}$，可能发生到 $\\lambda^{(u)}$ 或 $\\lambda^{(d)}$ 的转移。\n- 从重复状态 $\\lambda^{(u)}$，只可能发生到 $\\lambda^{(0)}$ 的转移。\n- 从缺失状态 $\\lambda^{(d)}$，只可能发生到 $\\lambda^{(0)}$ 的转移。\n\n在时间 $t$，对于每个当前允许的转移 $(\\alpha \\to \\beta)$，递归定义累积和统计量为\n$$\nS^{(\\alpha \\to \\beta)}_t \\;=\\; \\max\\!\\left(0,\\; S^{(\\alpha \\to \\beta)}_{t-1} \\;+\\; w_{\\alpha \\to \\beta}(x_t)\\right), \\quad \\text{with } S^{(\\alpha \\to \\beta)}_0 = 0.\n$$\n设 $h > 0$ 为固定阈值。如果在时间 $t$ 出现 $S^{(\\alpha \\to \\beta)}_t > h$，则宣布发生了一次转移 $(\\alpha \\to \\beta)$。为此转移报告的估计断点索引是当前 $S^{(\\alpha \\to \\beta)}$ 严格正值运行中第一个 bin 的索引，即\n$$\n\\widehat{\\tau} \\;=\\; \\min\\{k \\in \\{1,\\dots,t\\} : S^{(\\alpha \\to \\beta)}_k > 0 \\text{ and } S^{(\\alpha \\to \\beta)}_{k-1} = 0\\}.\n$$\n在时间 $t$ 宣布一次转移 $(\\alpha \\to \\beta)$ 且估计断点为 $\\widehat{\\tau}$ 后，系统的当前状态从 $\\alpha$ 变为 $\\beta$，所有累积和统计量重置为零，并在新的当前状态下从下一个时间索引 $t+1$ 继续处理。如果在某个时间 $t$，有多个允许的转移超过阈值，则选择 $S^{(\\alpha \\to \\beta)}_t$ 最大的那个；如果仍然存在平局，则选择目标均值 $\\mu_\\beta$ 与当前状态均值 $\\mu_\\alpha$ 的绝对差 $|\\mu_\\beta - \\mu_\\alpha|$ 最大的那个。\n\n您需要精确实现上述数学规范，并为每个输入序列返回按升序排列的估计断点索引 $\\widehat{\\tau}$ 列表，索引从 1 开始。如果某个序列没有宣布任何转移，则返回空列表。\n\n所有 $\\log(\\cdot)$ 计算均使用自然对数。没有需要报告的物理单位。\n\n测试套件：\n对于所有测试用例，使用相同的参数 $\\lambda^{(0)} = 30$，$\\lambda^{(u)} = 45$，$\\lambda^{(d)} = 15$ 和阈值 $h = 8$。每个测试用例提供一个观测到的计数序列 $\\{x_t\\}_{t=1}^n$。\n\n- 测试用例 A (由参考区段包围的单个重复):\n  长度为 40 的序列:\n  $[\\, 28,\\, 31,\\, 33,\\, 29,\\, 30,\\, 27,\\, 34,\\, 32,\\, 31,\\, 29,\\, 44,\\, 47,\\, 42,\\, 46,\\, 45,\\, 43,\\, 48,\\, 44,\\, 46,\\, 43,\\, 45,\\, 47,\\, 44,\\, 46,\\, 43,\\, 31,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29,\\, 32,\\, 28,\\, 30,\\, 31,\\, 29 \\,]$.\n  预期定性结构：bin $1$–$10$ 为参考 ($\\lambda^{(0)}$)，bin $11$–$25$ 为重复 ($\\lambda^{(u)}$)，bin $26$–$40$ 为参考。\n\n- 测试用例 B (由参考区段包围的单个缺失):\n  长度为 30 的序列:\n  $[\\, 30,\\, 28,\\, 31,\\, 33,\\, 29,\\, 32,\\, 30,\\, 14,\\, 17,\\, 16,\\, 13,\\, 15,\\, 14,\\, 16,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29,\\, 32,\\, 28,\\, 30,\\, 31 \\,]$.\n  预期定性结构：bin $1$–$7$ 为参考，bin $8$–$14$ 为缺失 ($\\lambda^{(d)}$)，bin $15$–$30$ 为参考。\n\n- 测试用例 C (无变化；全部类参考):\n  长度为 20 的序列:\n  $[\\, 30,\\, 29,\\, 31,\\, 28,\\, 32,\\, 30,\\, 27,\\, 33,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29 \\,]$.\n\n- 测试用例 D (两个独立事件：先重复后缺失):\n  长度为 35 的序列:\n  $[\\, 30,\\, 29,\\, 32,\\, 28,\\, 46,\\, 44,\\, 47,\\, 45,\\, 43,\\, 31,\\, 28,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29,\\, 16,\\, 14,\\, 15,\\, 17,\\, 13,\\, 30,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30,\\, 29 \\,]$.\n  预期定性结构：bin $1$–$4$ 为参考，bin $5$–$9$ 为重复，bin $10$–$17$ 为参考，bin $18$–$22$ 为缺失，bin $23$–$35$ 为参考。\n\n- 测试用例 E (从第一个 bin 开始的事件):\n  长度为 18 的序列:\n  $[\\, 16,\\, 13,\\, 15,\\, 14,\\, 17,\\, 15,\\, 30,\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 31,\\, 30 \\,]$.\n  预期定性结构：bin $1$–$6$ 为缺失，bin $7$–$18$ 为参考。\n\n- 测试用例 F (延伸至最后一个 bin 的事件):\n  长度为 18 的序列:\n  $[\\, 29,\\, 31,\\, 30,\\, 28,\\, 32,\\, 29,\\, 30,\\, 33,\\, 27,\\, 44,\\, 46,\\, 45,\\, 47,\\, 43,\\, 46,\\, 44,\\, 45,\\, 47 \\,]$.\n  预期定性结构：bin $1$–$9$ 为参考，bin $10$–$18$ 为重复。\n\n您的程序不得读取任何输入，并且必须完全按照所提供的六个序列和参数进行操作。它必须输出单行内容，其中包含一个含六个项目的列表，每个项目是对应测试用例 A、B、C、D、E、F 的已检测断点索引列表。输出格式必须为单行，不含多余字符，形式为方括号内以逗号分隔的列表；例如，符合要求格式的输出如下\n[ [a_1,a_2], [b_1], [], [d_1,d_2,d_3], [e_1,e_2], [f_1] ]\n其中每个 $a_i,b_i,d_i,e_i,f_i$ 是一个整数索引。使用从 1 开始的索引。", "solution": "该问题要求实现一种基于累积和 (CUSUM) 方法的序列性变点检测算法，以识别基因组数据中的拷贝数变异 (CNV)。数据以读取深度计数序列的形式提供，这些计数被建模为来自泊松分布的观测值。该分布的均值被假定为分段恒定的，并在三个对应不同拷贝数状态的值中取其一：参考 ($\\lambda^{(0)}$)、重复 ($\\lambda^{(u)}$) 和缺失 ($\\lambda^{(d)}$)。\n\n该算法按顺序处理数据，在每一步更新一组 CUSUM 统计量。每个统计量追踪特定类型状态变化（例如，从参考到重复）的证据。当某个 CUSUM 统计量超过预定义的阈值 $h$ 时，就宣布发生了一次变化。检测到变化后，系统状态会更新，断点位置被记录下来，并且所有 CUSUM 统计量都会重置为零。\n\n以下是该方法的分步解析。\n\n首先，我们定义状态及其相关的均值参数：\n- 参考状态，$\\mu_0 = \\lambda^{(0)} = 30$。\n- 重复状态，$\\mu_u = \\lambda^{(u)} = 45$。\n- 缺失状态，$\\mu_d = \\lambda^{(d)} = 15$。\n检测阈值为 $h=8$。系统从参考状态 $\\lambda^{(0)}$ 开始。\n\n该检测方法的核心是在假定从均值为 $\\mu_\\alpha$ 的状态转移到均值为 $\\mu_\\beta$ 的状态下，观测值 $x_t$ 的对数似然比 (LLR) 增量：\n$$\nw_{\\alpha \\to \\beta}(x_t) = x_t \\log\\left(\\frac{\\mu_\\beta}{\\mu_\\alpha}\\right) - (\\mu_\\beta - \\mu_\\alpha)\n$$\n该值表示观测值 $x_t$ 提供的支持新状态 $\\beta$ 而非当前状态 $\\alpha$ 的证据权重。我们必须为模型定义的所有可能状态转移计算此方程的常数项：\n- 参考到重复 ($0 \\to u$): $w_{0 \\to u}(x_t) = x_t \\log(1.5) - 15$。\n- 参考到缺失 ($0 \\to d$): $w_{0 \\to d}(x_t) = x_t \\log(0.5) + 15$。\n- 重复到参考 ($u \\to 0$): $w_{u \\to 0}(x_t) = x_t \\log(2/3) + 15$。\n- 缺失到参考 ($d \\to 0$): $w_{d \\to 0}(x_t) = x_t \\log(2) - 15$。\n\n该算法为当前状态允许的每个转移维护一个 CUSUM 统计量 $S_t$。该统计量在每个时间步 $t$ 根据以下规则更新：\n$$\nS^{(\\alpha \\to \\beta)}_t = \\max(0, S^{(\\alpha \\to \\beta)}_{t-1} + w_{\\alpha \\to \\beta}(x_t))\n$$\n这个递归定义意味着 CUSUM 分数会累积正的 LLR 增量，并在累积证据变为负值时重置为零，从而防止了反对变化的证据的传播。\n\n对于给定的计数序列 $\\{x_t\\}_{t=1}^n$，算法按以下步骤进行：\n1.  将当前状态初始化为参考状态 ($\\lambda^{(0)}$)，断点列表为空，所有 CUSUM 统计量为 $0$。我们还为每个 CUSUM 统计量追踪其当前正值运行开始的时间索引。\n2.  从 $t=1$ 到 $n$ 遍历序列。\n3.  在每个时间 $t$，根据当前状态确定允许的转移集合。\n    - 如果处于状态 $\\lambda^{(0)}$，允许的转移是到 $\\lambda^{(u)}$ 和 $\\lambda^{(d)}$。\n    - 如果处于状态 $\\lambda^{(u)}$，唯一允许的转移是到 $\\lambda^{(0)}$。\n    - 如果处于状态 $\\lambda^{(d)}$，唯一允许的转移是到 $\\lambda^{(0)}$。\n4.  对于每个允许的转移 $(\\alpha \\to \\beta)$，计算 $w_{\\alpha \\to \\beta}(x_t)$ 并更新相应的 CUSUM 统计量 $S_t$。如果分数在 $t-1$ 时为 $0$ 并在 $t$ 时变为正数，则记录 $t$ 作为运行的开始。\n5.  检查更新后的允许转移的 CUSUM 统计量是否有任何一个超过阈值 $h$。\n6.  如果一个或多个阈值被超过，则宣布一个变点。如果多个转移被触发，则通过以下方式决定胜者：\n    a. 具有最大 CUSUM 分数 $S_t$ 的转移。\n    b. 如果分数平局，则选择均值绝对差 $|\\mu_\\beta - \\mu_\\alpha|$ 最大的转移。\n7.  在宣布一个胜出的转移 $(\\alpha \\to \\beta)$ 后，估计的断点索引 $\\widehat{\\tau}$ 是为该 CUSUM 统计量保存的正值运行的开始时间。此索引 $\\widehat{\\tau}$ 被追加到结果列表中。\n8.  系统状态更新为 $\\beta$，所有 CUSUM 统计量重置为 $0$，并且它们相关的开始时间追踪器被清除。\n9.  过程从下一个观测值 $t+1$ 继续。\n10. 如果循环完成，则返回记录的最终断点列表。\n\n必须对问题规范提出一个关键说明。在平局决胜程序中存在歧义。如果系统处于参考状态 $\\lambda^{(0)}$，并且 $S^{(0 \\to u)}_t$ 和 $S^{(0 \\to d)}_t$ 都超过 $h$ 且相等，则第一个平局决胜规则失败。第二个规则比较 $|\\mu_\\beta - \\mu_\\alpha|$，也无法解决这个特定的平局，因为 $|\\lambda^{(u)} - \\lambda^{(0)}| = |45 - 30| = 15$ 且 $|\\lambda^{(d)} - \\lambda^{(0)}| = |15 - 30| = 15$。这使得该问题成为一个不适定问题 (ill-posed)。然而，在使用浮点数算术和给定测试数据的情况下，这种特定的平局不太可能发生。我们假设不会遇到这种情况并继续进行。\n\n实现将根据这一确切逻辑处理每个测试用例序列，并按要求使用从 1 开始的索引报告断点。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the analysis of all test cases and print the final result.\n    \"\"\"\n    # Define model parameters\n    params = {\n        \"l0\": 30.0,\n        \"lu\": 45.0,\n        \"ld\": 15.0,\n        \"h\": 8.0,\n    }\n\n    # Define the six test cases from the problem statement\n    test_cases = [\n        # Test case A\n        [28, 31, 33, 29, 30, 27, 34, 32, 31, 29, 44, 47, 42, 46, 45, 43, 48, 44, 46, 43, 45, 47, 44, 46, 43, 31, 28, 32, 29, 30, 33, 27, 31, 30, 29, 32, 28, 30, 31, 29],\n        # Test case B\n        [30, 28, 31, 33, 29, 32, 30, 14, 17, 16, 13, 15, 14, 16, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30, 29, 32, 28, 30, 31],\n        # Test case C\n        [30, 29, 31, 28, 32, 30, 27, 33, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30, 29],\n        # Test case D\n        [30, 29, 32, 28, 46, 44, 47, 45, 43, 31, 28, 30, 33, 27, 31, 30, 29, 16, 14, 15, 17, 13, 30, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30, 29],\n        # Test case E\n        [16, 13, 15, 14, 17, 15, 30, 29, 31, 30, 28, 32, 29, 30, 33, 27, 31, 30],\n        # Test case F\n        [29, 31, 30, 28, 32, 29, 30, 33, 27, 44, 46, 45, 47, 43, 46, 44, 45, 47],\n    ]\n\n    # Process all test cases\n    all_results = [detect_breakpoints(seq, params) for seq in test_cases]\n\n    # Format the combined results into a single string as specified\n    # The result for each case is formatted as '[i,j,k]' (no spaces)\n    # These are then joined by commas into a larger list string: '[[i,j],[k],[]]'\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef detect_breakpoints(x_sequence, params):\n    \"\"\"\n    Implements the CUSUM-based change-point detection algorithm for a single sequence.\n    \"\"\"\n    l0, lu, ld, h = params[\"l0\"], params[\"lu\"], params[\"ld\"], params[\"h\"]\n    n = len(x_sequence)\n    \n    # Pre-calculate log-likelihood ratio terms\n    # w(x) = x * C1 - C2\n    C1 = {\n        '0_to_u': np.log(lu / l0), '0_to_d': np.log(ld / l0),\n        'u_to_0': np.log(l0 / lu), 'd_to_0': np.log(l0 / ld)\n    }\n    C2 = {\n        '0_to_u': lu - l0, '0_to_d': ld - l0,\n        'u_to_0': l0 - lu, 'd_to_0': l0 - ld\n    }\n\n    # System state\n    breakpoints = []\n    current_state_mean = l0\n\n    # CUSUM statistics and start times of positive runs (1-based index)\n    cusums = {key: 0.0 for key in C1}\n    start_times = {key: 0 for key in C1}\n\n    for i in range(n):\n        t = i + 1  # Use 1-based indexing for time and breakpoints\n        x = x_sequence[i]\n\n        allowed_transitions = []\n        if current_state_mean == l0:\n            allowed_transitions = ['0_to_u', '0_to_d']\n        elif current_state_mean == lu:\n            allowed_transitions = ['u_to_0']\n        elif current_state_mean == ld:\n            allowed_transitions = ['d_to_0']\n\n        # Update CUSUMs for allowed transitions\n        for trans_key in allowed_transitions:\n            w = x * C1[trans_key] - C2[trans_key]\n            prev_s = cusums[trans_key]\n            new_s = max(0.0, prev_s + w)\n            cusums[trans_key] = new_s\n\n            if prev_s == 0.0 and new_s  0.0:\n                start_times[trans_key] = t  # Record start of positive run\n\n        # Check for threshold crossings\n        triggered = [key for key in allowed_transitions if cusums[key]  h]\n\n        if triggered:\n            winner = None\n            if len(triggered) == 1:\n                winner = triggered[0]\n            else: # Must be '0_to_u' and '0_to_d'\n                s_0u = cusums['0_to_u']\n                s_0d = cusums['0_to_d']\n                if s_0u  s_0d:\n                    winner = '0_to_u'\n                elif s_0d  s_0u:\n                    winner = '0_to_d'\n                else: \n                    # Tie on scores. Second tie-breaker on |mu_beta - mu_alpha|.\n                    # |45-30|=15, |15-30|=15. This rule doesn't resolve the tie.\n                    # The problem is technically ill-posed. Assume this case is not hit by test data.\n                    # If it were, we'd have to make an arbitrary choice.\n                    # As a fallback, we could prefer duplication over deletion, for example.\n                    winner = '0_to_u' # Arbitrary choice, not expected to be used.\n\n            # Record breakpoint and update system state\n            breakpoints.append(start_times[winner])\n\n            # Update state\n            if winner == '0_to_u': current_state_mean = lu\n            elif winner == '0_to_d': current_state_mean = ld\n            elif winner in ['u_to_0', 'd_to_0']: current_state_mean = l0\n\n            # Reset all CUSUMs and start times\n            for key in cusums:\n                cusums[key] = 0.0\n                start_times[key] = 0\n\n    return breakpoints\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2382736"}, {"introduction": "当整合多个证据来源时，现实世界中的基因组分析最为强大。这最后一个练习介绍了一个复杂的贝叶斯框架，用于整合来自读取深度、不一致配对末端读取和分裂读取的信号，以推断拷贝数状态。通过完成这个练习[@problem_id:2382738]，你将学习如何运用概率建模来综合不同类型的数据，从而对拷贝数变异做出更准确、更可靠的评估。", "problem": "给定一个数学模型，用于结合来自三个独立测序信号的证据进行拷贝数变异 (CNV) 分析：这些信号分别是测序深度、不一致配对末端计数和分裂读段计数。目标是计算离散拷贝数状态的后验分布，并报告几个测试用例的最大后验 (MAP) 状态及其后验概率。\n\n数学设定：\n- 设拷贝数 (CN) 状态为一个离散随机变量 $CN \\in \\{0,1,2,3,4\\}$。\n- 设基因组区域的观测数据为三元组 $(D,PE,SR)$，其中 $D$ 是一个归一化的测序深度比率（无单位），$PE$ 是不一致配对末端的计数，而 $SR$ 是分裂读段的计数。\n- 假设在给定 $CN$ 的条件下，$D$、$PE$ 和 $SR$ 是条件独立的。\n- $CN$ 的先验分布由下式指定：\n$$\nP(CN=0)=0.03,\\quad P(CN=1)=0.07,\\quad P(CN=2)=0.80,\\quad P(CN=3)=0.07,\\quad P(CN=4)=0.03.\n$$\n- 似然模型如下：\n  1. 测序深度：$D \\mid CN=k \\sim \\mathcal{N}\\!\\left(\\mu_k,\\sigma^2\\right)$，其中 $\\mu_k = \\frac{k}{2}$ 且 $\\sigma^2 = 0.04$。\n  2. 不一致配对末端：$PE \\mid CN=k \\sim \\mathrm{Pois}\\!\\left(\\lambda_{PE}(k)\\right)$，其中 $\\lambda_{PE}(k) = \\beta_{PE} + \\alpha_{PE}\\,\\lvert k-2 \\rvert$，$\\alpha_{PE}=3.0$ 且 $\\beta_{PE}=0.1$。\n  3. 分裂读段：$SR \\mid CN=k \\sim \\mathrm{Pois}\\!\\left(\\lambda_{SR}(k)\\right)$，其中 $\\lambda_{SR}(k) = \\beta_{SR} + \\alpha_{SR}\\,\\lvert k-2 \\rvert$，$\\alpha_{SR}=2.0$ 且 $\\beta_{SR}=0.05$。\n\n贝叶斯目标：\n- 对于每个观测到的三元组 $(D,PE,SR)$，计算后验概率：\n$$\nP(CN=k \\mid D,PE,SR) \\propto P(D \\mid CN=k)\\;P(PE \\mid CN=k)\\;P(SR \\mid CN=k)\\;P(CN=k),\n$$\n该式对 $k \\in \\{0,1,2,3,4\\}$ 进行归一化，以获得一个总和为 $1$ 的有效概率分布。\n- 对于每个案例，报告 MAP 估计：\n$$\n\\hat{k} = \\arg\\max_{k \\in \\{0,1,2,3,4\\}} P(CN=k \\mid D,PE,SR),\n$$\n以及相应的后验概率 $P(CN=\\hat{k} \\mid D,PE,SR)$。\n\n测试套件：\n- 使用以下 $5$ 个观测三元组 $(D,PE,SR)$：\n  1. $(0.52, 4, 3)$\n  2. $(1.47, 3, 2)$\n  3. $(1.02, 0, 0)$\n  4. $(0.08, 7, 5)$\n  5. $(0.78, 1, 1)$\n\n答案规格：\n- 对于每个测试用例，输出整数 $\\hat{k}$ 和浮点数 $P(CN=\\hat{k} \\mid D,PE,SR)$，四舍五入到 $6$ 位小数。\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，按测试用例的顺序连接结果。具体来说，输出格式必须是：\n$$\n[\\hat{k}_1, p_1, \\hat{k}_2, p_2, \\hat{k}_3, p_3, \\hat{k}_4, p_4, \\hat{k}_5, p_5],\n$$\n其中 $p_i$ 表示 $P(CN=\\hat{k}_i \\mid D_i,PE_i,SR_i)$ 四舍五入到 $6$ 位小数的值。不应打印额外的文本或行。", "solution": "该问题要求计算离散拷贝数状态 $CN$ 的后验概率分布，给定三种独立的基因组数据来源：归一化测序深度 $D$、不一致配对末端计数 $PE$ 和分裂读段计数 $SR$。目标是为一组给定的观测值确定最大后验 (MAP) 状态及其相关的后验概率。\n\n分析框架基于贝叶斯定理。拷贝数状态 $CN$ 是一个离散随机变量，可以在集合 $k \\in \\{0, 1, 2, 3, 4\\}$ 中取值。对于每个潜在状态 $k$，我们寻求计算后验概率 $P(CN=k \\mid D, PE, SR)$。根据贝叶斯定理，这由下式给出：\n$$ P(CN=k \\mid D, PE, SR) = \\frac{P(D, PE, SR \\mid CN=k) P(CN=k)}{P(D, PE, SR)} $$\n$P(CN=k)$ 项是观测到拷贝数状态 $k$ 的先验概率。问题提供了这些先验概率：$P(CN=0)=0.03$，$P(CN=1)=0.07$，$P(CN=2)=0.80$，$P(CN=3)=0.07$，以及 $P(CN=4)=0.03$。\n\n$P(D, PE, SR \\mid CN=k)$ 项是在给定状态 $k$ 的条件下观测到数据的似然。问题指出，数据源 $D$、$PE$ 和 $SR$ 在给定 $CN$ 的条件下是条件独立的。这使我们能够分解似然：\n$$ P(D, PE, SR \\mid CN=k) = P(D \\mid CN=k) \\times P(PE \\mid CN=k) \\times P(SR \\mid CN=k) $$\n这些单独的似然项中的每一项都由一个特定的统计模型定义：\n\n1.  测序深度似然，$P(D \\mid CN=k)$：归一化测序深度 $D$ 由高斯（正态）分布建模，$D \\mid CN=k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$。其概率密度函数为：\n    $$ P(D \\mid CN=k) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(D - \\mu_k)^2}{2\\sigma^2}\\right) $$\n    参数给定为 $\\mu_k = \\frac{k}{2}$，方差 $\\sigma^2 = 0.04$。\n\n2.  不一致配对末端似然，$P(PE \\mid CN=k)$：不一致配对末端的计数 $PE$ 由泊松分布建模，$PE \\mid CN=k \\sim \\mathrm{Pois}(\\lambda_{PE}(k))$。其概率质量函数为：\n    $$ P(PE \\mid CN=k) = \\frac{\\lambda_{PE}(k)^{PE} e^{-\\lambda_{PE}(k)}}{PE!} $$\n    速率参数 $\\lambda_{PE}(k)$ 是拷贝数状态 $k$ 的函数，定义为 $\\lambda_{PE}(k) = \\beta_{PE} + \\alpha_{PE}\\,\\lvert k-2 \\rvert$，其中常数 $\\alpha_{PE} = 3.0$ 且 $\\beta_{PE} = 0.1$。\n\n3.  分裂读段似然，$P(SR \\mid CN=k)$：类似地，分裂读段计数 $SR$ 由泊松分布建模，$SR \\mid CN=k \\sim \\mathrm{Pois}(\\lambda_{SR}(k))$。其概率质量函数为：\n    $$ P(SR \\mid CN=k) = \\frac{\\lambda_{SR}(k)^{SR} e^{-\\lambda_{SR}(k)}}{SR!} $$\n    速率参数为 $\\lambda_{SR}(k) = \\beta_{SR} + \\alpha_{SR}\\,\\lvert k-2 \\rvert$，其中常数 $\\alpha_{SR} = 2.0$ 且 $\\beta_{SR} = 0.05$。\n\n贝叶斯定理中的分母 $P(D, PE, SR)$ 是边缘似然或“证据”。它作为一个归一化常数，确保后验概率在所有可能的状态 $k$ 上总和为 $1$。它通过对所有状态的未归一化后验概率求和来计算：\n$$ P(D, PE, SR) = \\sum_{j=0}^{4} P(D, PE, SR \\mid CN=j) P(CN=j) $$\n因此，对于每个状态 $k$，完整的后验概率是：\n$$ P(CN=k \\mid D, PE, SR) = \\frac{P(D \\mid CN=k) P(PE \\mid CN=k) P(SR \\mid CN=k) P(CN=k)}{\\sum_{j=0}^{4} P(D \\mid CN=j) P(PE \\mid CN=j) P(SR \\mid CN=j) P(CN=j)} $$\n对于 $5$ 个测试用例中的每一个，算法流程如下：\n1.  对于每个拷贝数状态 $k \\in \\{0, 1, 2, 3, 4\\}$，计算未归一化的后验概率，即先验概率与在给定数据 $(D, PE, SR)$ 下评估的三个似然的乘积。\n2.  将这些未归一化的后验值对所有 $k$ 求和，以计算归一化常数（证据）。\n3.  将每个未归一化的后验概率除以证据，得到每个状态 $k$ 的归一化后验概率。\n4.  识别具有最大后验概率的状态 $\\hat{k}$。这是 MAP 估计：$\\hat{k} = \\arg\\max_{k} P(CN=k \\mid D, PE, SR)$。\n5.  报告整数 MAP 状态 $\\hat{k}$ 及其对应的后验概率，四舍五入到 $6$ 位小数。对所有 $5$ 个提供的测试用例重复此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, poisson\n\ndef solve():\n    \"\"\"\n    Computes the MAP estimate and posterior probability for CNV states.\n    \"\"\"\n    # Define the parameter space and models as specified in the problem.\n    cn_states = np.array([0, 1, 2, 3, 4])\n    priors = np.array([0.03, 0.07, 0.80, 0.07, 0.03])\n\n    # 1. Read depth model: D | CN=k ~ N(mu_k, sigma^2)\n    rd_sigma_sq = 0.04\n    rd_sigma = np.sqrt(rd_sigma_sq)\n    rd_mu_k = cn_states / 2.0\n\n    # 2. Discordant paired-end model: PE | CN=k ~ Pois(lambda_pe(k))\n    pe_alpha = 3.0\n    pe_beta = 0.1\n    pe_lambda_k = pe_beta + pe_alpha * np.abs(cn_states - 2)\n\n    # 3. Split-read model: SR | CN=k ~ Pois(lambda_sr(k))\n    sr_alpha = 2.0\n    sr_beta = 0.05\n    sr_lambda_k = sr_beta + sr_alpha * np.abs(cn_states - 2)\n\n    # Test suite of observed data triples (D, PE, SR)\n    test_cases = [\n        (0.52, 4, 3),\n        (1.47, 3, 2),\n        (1.02, 0, 0),\n        (0.08, 7, 5),\n        (0.78, 1, 1),\n    ]\n\n    results = []\n    for D, PE, SR in test_cases:\n        # Array to hold the unnormalized posterior for each CN state k\n        unnormalized_posteriors = np.zeros_like(cn_states, dtype=float)\n\n        for i, k in enumerate(cn_states):\n            # Calculate the likelihood for each data type\n            likelihood_D = norm.pdf(D, loc=rd_mu_k[i], scale=rd_sigma)\n            likelihood_PE = poisson.pmf(PE, mu=pe_lambda_k[i])\n            likelihood_SR = poisson.pmf(SR, mu=sr_lambda_k[i])\n\n            # Combine likelihoods and prior based on conditional independence\n            # This is the unnormalized posterior (proportional to joint probability)\n            unnormalized_posteriors[i] = likelihood_D * likelihood_PE * likelihood_SR * priors[i]\n\n        # Normalize to get the posterior distribution\n        evidence = np.sum(unnormalized_posteriors)\n        \n        # Handle the case where all posteriors are zero (e.g., underflow),\n        # though unlikely with these parameters.\n        if evidence  0:\n            posterior_probs = unnormalized_posteriors / evidence\n        else:\n            # If evidence is zero, probabilities cannot be determined,\n            # though this indicates an issue. A uniform distribution is a fallback.\n            posterior_probs = np.full_like(cn_states, 1.0 / len(cn_states), dtype=float)\n\n        # Find the MAP estimate (state with the highest posterior probability)\n        map_k_index = np.argmax(posterior_probs)\n        map_k = cn_states[map_k_index]\n        map_prob = posterior_probs[map_k_index]\n\n        # Append results to the list\n        results.append(int(map_k))\n        results.append(f\"{map_prob:.6f}\")\n\n    # Format the final output string as per the specification\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382738"}]}