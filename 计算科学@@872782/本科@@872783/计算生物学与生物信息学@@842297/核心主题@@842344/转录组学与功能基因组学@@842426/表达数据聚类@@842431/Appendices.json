{"hands_on_practices": [{"introduction": "在分析基因表达数据时，原始测序读数往往会受到测序深度（文库大小）差异的严重影响，从而掩盖真实的生物学信号。这个练习将让你通过动手实践，直观地感受数据标准化的重要性。你将分别对原始数据和经过标准化的数据运行$k$-均值与层次聚类，并使用调整兰德指数（ARI）来量化结果的差异，从而深刻理解为何预处理是表达数据分析中不可或缺的一步。[@problem_id:2379247]", "problem": "您会获得几个小型基因表达矩阵，这些矩阵旨在为多个样本的核糖核酸测序 (RNA-seq) 读取计数建模。每个矩阵的行是基因，列是样本。对于每个矩阵，您必须在两种数据体系下计算样本的两种聚类：未经处理的原始计数，以及应用文库大小归一化后进行对数转换的版本。然后，您必须使用一个数学上定义的指数来量化原始聚类和转换后聚类的相似程度。\n\n定义和要求：\n\n- 令原始计数矩阵表示为 $X \\in \\mathbb{N}_{0}^{g \\times s}$，其中 $g$ 是基因数， $s$ 是样本数。条目 $X_{ij}$ 是基因 $i$ 在样本 $j$ 中的非负整数计数。\n\n- 将每个样本的文库大小定义为 $L_j = \\sum_{i=1}^{g} X_{ij}$，适用于每个样本 $j \\in \\{1,\\dots,s\\}$。将每百万计数 (CPM) 矩阵 $C \\in \\mathbb{R}^{g \\times s}$ 定义为\n  $$C_{ij} = \\begin{cases}\n  10^6 \\cdot \\dfrac{X_{ij}}{L_j},  & \\text{若 } L_j > 0, \\\\\n  0,  & \\text{若 } L_j = 0.\n  \\end{cases}$$\n  将对数转换归一化矩阵 $Y \\in \\mathbb{R}^{g \\times s}$ 定义为\n  $$Y_{ij} = \\log_2\\big(C_{ij} + 1\\big)。$$\n\n- 您必须使用两种方法对 $X$ 和 $Y$ 中的样本（即列）进行聚类：\n  1. 在 $\\mathbb{R}^{g}$ 中使用标准欧几里得距离 $d(\\mathbf{u},\\mathbf{v}) = \\left\\|\\mathbf{u} - \\mathbf{v}\\right\\|_2$ 进行 $k$-均值聚类，其中 $k$ 会为每个测试用例指定。初始化必须是确定性的：使用前 $k$ 个样本（按列顺序）作为初始中心。通过交替进行分配和更新步骤进行迭代，直到分配稳定或达到最大 $100$ 次迭代。如果在任何迭代中任何簇变为空，则立即将其中心重新初始化为与任何现有中心当前最小距离最大的样本。在分配过程中若距离完全相等，则将样本分配给索引最小的簇。\n  2. 在相同的欧几里得距离上使用平均链接法进行凝聚式层次聚类，通过切割树状图以获得 $k$ 个簇（使用基于基数的标准），精确生成 $k$ 个扁平簇。聚类必须在样本向量（列）上执行。\n\n- 对于每种方法，使用调整兰德指数 (ARI) 比较在 $X$ 上获得的簇标签与在 $Y$ 上获得的簇标签。给定 $s$ 个项目的两个聚类 $\\mathcal{U}$ 和 $\\mathcal{V}$，令 $n_{ij}$ 表示列联计数，$a_i = \\sum_{j} n_{ij}$，$b_j = \\sum_{i} n_{ij}$，并令\n  $$\\binom{n}{2} = \\frac{n(n-1)}{2}。$$\n  调整兰德指数为\n  $$\\mathrm{ARI}(\\mathcal{U},\\mathcal{V}) = \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}{\\frac{1}{2}\\left[\\sum_{i} \\binom{a_i}{2} + \\sum_{j} \\binom{b_j}{2}\\right] - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}。$$\n  如果分母为零，则当两个分区相同时定义 $\\mathrm{ARI} = 1$，否则定义为 $\\mathrm{ARI} = 0$。\n\n- 所有计算都没有单位。不涉及角度。将所有报告的实数四舍五入到 $6$ 位小数。\n\n测试套件：\n\n对于以下每个测试用例，将提供的矩阵视为 $X$，将指定的 $k$ 视为目标簇数。在每种情况下，计算两个 ARI 值：第一个是 $k$-均值（原始对转换后），第二个是层次聚类（原始对转换后）。\n\n- 测试用例 $1$（强烈的文库大小效应，掩盖了原始计数中的相对组成）：\n  - $X^{(1)}$ 有 $g=2$ 和 $s=4$，列对应于样本 $1$ 到 $4$：\n    - 样本 $1$：$\\begin{bmatrix} 100 \\\\ 10 \\end{bmatrix}$，\n    - 样本 $2$：$\\begin{bmatrix} 1000 \\\\ 100 \\end{bmatrix}$，\n    - 样本 $3$：$\\begin{bmatrix} 10 \\\\ 100 \\end{bmatrix}$，\n    - 样本 $4$：$\\begin{bmatrix} 100 \\\\ 1000 \\end{bmatrix}$。\n  - 因此 $X^{(1)} = \\begin{bmatrix} 100 & 1000 & 10 & 100 \\\\ 10 & 100 & 100 & 1000 \\end{bmatrix}$，且 $k=2$。\n\n- 测试用例 $2$（零文库大小的边界情况，避免距离相等）：\n  - $X^{(2)}$ 有 $g=2$ 和 $s=3$，列为：\n    - 样本 $1$：$\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，\n    - 样本 $2$：$\\begin{bmatrix} 12 \\\\ 0 \\end{bmatrix}$，\n    - 样本 $3$：$\\begin{bmatrix} 0 \\\\ 9 \\end{bmatrix}$。\n  - 因此 $X^{(2)} = \\begin{bmatrix} 0 & 12 & 0 \\\\ 0 & 0 & 9 \\end{bmatrix}$，且 $k=2$。\n\n- 测试用例 $3$（具有清晰相对表达结构的平衡总和）：\n  - $X^{(3)}$ 有 $g=3$ 和 $s=6$，列为：\n    - 样本 $1$：$\\begin{bmatrix} 100 \\\\ 50 \\\\ 10 \\end{bmatrix}$，\n    - 样本 $2$：$\\begin{bmatrix} 96 \\\\ 48 \\\\ 16 \\end{bmatrix}$，\n    - 样本 $3$：$\\begin{bmatrix} 104 \\\\ 52 \\\\ 4 \\end{bmatrix}$，\n    - 样本 $4$：$\\begin{bmatrix} 10 \\\\ 50 \\\\ 100 \\end{bmatrix}$，\n    - 样本 $5$：$\\begin{bmatrix} 16 \\\\ 48 \\\\ 96 \\end{bmatrix}$，\n    - 样本 $6$：$\\begin{bmatrix} 4 \\\\ 52 \\\\ 104 \\end{bmatrix}$。\n  - 因此 $X^{(3)} = \\begin{bmatrix} 100 & 96 & 104 & 10 & 16 & 4 \\\\ 50 & 48 & 52 & 50 & 48 & 52 \\\\ 10 & 16 & 4 & 100 & 96 & 104 \\end{bmatrix}$，且 $k=2$。\n\n要求的程序输出：\n\n- 您的程序必须生成一行，包含一个用方括号括起来的逗号分隔列表。该列表必须按此顺序包含 $6$ 个实数：\n  - 测试用例 $1$ 的 $k$-均值 ARI，测试用例 $1$ 的层次聚类 ARI，测试用例 $2$ 的 $k$-均值 ARI，测试用例 $2$ 的层次聚类 ARI，测试用例 $3$ 的 $k$-均值 ARI，测试用例 $3$ 的层次聚类 ARI。\n- 每个实数必须四舍五入到 $6$ 位小数。\n- 例如，一个语法上有效的输出应类似于 $[0.123456,0.000000,1.000000,1.000000,0.876543,0.876543]$，但其值是根据上述定义计算的。", "solution": "所呈现的问题是生物信息学中一个明确定义的计算任务，要求比较基因表达数据在标准归一化程序前后聚类结果的差异。所有定义、算法和参数都得到了明确的规定。该问题具有科学依据、客观且定义良好。因此，我将提供一个完整的解决方案。\n\n任务是获取一个原始基因表达计数矩阵 $X$，计算一个归一化和转换后的矩阵 $Y$，使用两种不同的方法（$k$-均值和层次聚类）对 $X$ 和 $Y$ 的样本（列）进行聚类，然后使用调整兰德指数 (ARI) 来量化原始数据聚类与转换后数据聚类之间的相似性。这个过程必须对三个不同的测试用例重复进行。\n\n首先，让我们将数据转换过程形式化。给定一个原始计数矩阵 $X \\in \\mathbb{N}_{0}^{g \\times s}$，其中 $g$ 是基因数， $s$ 是样本数，转换为矩阵 $Y$ 的过程包括两个步骤：\n1.  **每百万计数 (CPM) 归一化**：此步骤旨在校正样本之间测序深度（文库大小）的差异。样本 $j$ 的文库大小为 $L_j = \\sum_{i=1}^{g} X_{ij}$。CPM 矩阵 $C$ 的计算方式为 $C_{ij} = 10^6 \\cdot X_{ij} / L_j$（当 $L_j > 0$ 时），如果 $L_j=0$，则 $C_{ij} = 0$。这将每个样本中的计数重新缩放到一个共同的总和 $10^6$。\n2.  **对数转换**：为了稳定方差并使数据更对称，应用了 $\\log_2$ 变换。添加了一个伪计数 $1$ 以避免对零取对数。最终的矩阵是 $Y_{ij} = \\log_2(C_{ij} + 1)$。\n\n接下来，我们必须为样本实现两种指定的聚类算法，这些样本由矩阵 $X$ 和 $Y$ 的列向量表示。\n\n**$k$-均值聚类**：\n这是一种迭代划分方法。对于 $g$ 维空间中的一组 $s$ 个样本向量，算法流程如下：\n1.  **初始化**：选择前 $k$ 个样本作为初始的 $k$ 个质心。\n2.  **分配步骤**：根据欧几里得距离 $d(\\mathbf{u},\\mathbf{v}) = \\sqrt{\\sum_{i=1}^{g}(u_i-v_i)^2}$，将每个样本分配给质心最近的簇。如果距离出现平局，则将样本分配给索引最小的簇。\n3.  **更新步骤**：将每个簇的质心重新计算为分配给该簇的所有样本向量的均值。\n4.  **空簇处理**：如果更新步骤导致某个簇没有分配任何样本，则重新初始化其质心。新的质心选择为与当前所有存在的（非空）质心具有最大最小欧几里得距离的样本向量。这有助于将新质心放置在数据空间的稀疏区域。\n5.  **终止**：重复步骤 2-4，直到簇分配在迭代之间不再改变，或达到最大 100 次迭代。\n\n**凝聚式层次聚类**：\n这是一种自底向上的方法，用于构建一个簇的层次结构。\n1.  **初始化**：每个样本最初都自成一簇。\n2.  **迭代**：在每一步中，将两个最接近的簇合并成一个新的、更大的簇。此过程持续进行，直到只剩下一个包含所有样本的簇。两个簇之间的距离由一个链接准则定义。问题指定使用**平均链接法**（也称为 UPGMA），其中两个簇之间的距离是从每个簇中取一个样本的所有样本对之间欧几里得距离的平均值。\n3.  **扁平簇提取**：为了精确获得 $k$ 个簇，合并过程在执行了 $s-k$ 次合并后停止。这等同于在能够产生 $k$ 个不同分支的水平上切割所产生的树状图。\n\n最后，对于每种聚类方法，我们必须比较从原始矩阵 $X$ 获得的分区（我们称之为 $\\mathcal{U}$）与从转换后矩阵 $Y$ 获得的分区（我们称之为 $\\mathcal{V}$）。比较指标是**调整兰德指数 (ARI)**。ARI 衡量两个数据聚类之间的相似性，并对偶然性进行了校正。其值范围从 -1 到 1，其中 1 表示完全一致，接近 0 的值表示随机一致，负值表示一致性低于偶然预期。\n提供的公式为：\n$$\n\\mathrm{ARI}(\\mathcal{U},\\mathcal{V}) = \\frac{\\sum_{i,j} \\binom{n_{ij}}{2} - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}{\\frac{1}{2}\\left[\\sum_{i} \\binom{a_i}{2} + \\sum_{j} \\binom{b_j}{2}\\right] - \\left[\\sum_{i} \\binom{a_i}{2} \\sum_{j} \\binom{b_j}{2}\\right] \\Big/ \\binom{s}{2}}\n$$\n其中 $n_{ij}$ 是 $\\mathcal{U}$ 中簇 $i$ 和 $\\mathcal{V}$ 中簇 $j$ 的样本数量，$a_i$ 是 $\\mathcal{U}$ 中簇 $i$ 的总大小，$b_j$ 是 $\\mathcal{V}$ 中簇 $j$ 的总大小，$s$ 是总样本数。项 $\\binom{n}{2}$ 计算元素对的数量。对于分母为零的情况，给出了一个特殊条件：对于相同的分区，$\\mathrm{ARI}=1$；否则为 $0$。\n\n实现将通过应用这些步骤来处理每个测试用例，计算所需的两个 ARI 值（一个用于 $k$-均值，一个用于层次聚类），并将它们整理成一个单一的输出列表。所有数值计算都将使用指定的库，`numpy` 用于线性代数，`scipy` 用于专门的函数，如层次聚类和组合。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.special import comb\n\ndef transform_data(X):\n    \"\"\"\n    Applies library-size normalization and log2 transform to a raw count matrix.\n    X: Raw count matrix (genes x samples).\n    \"\"\"\n    g, s = X.shape\n    lib_sizes = np.sum(X, axis=0)\n    \n    C = np.zeros_like(X, dtype=np.float64)\n    # Using a loop to avoid division by zero without a mask\n    for j in range(s):\n        if lib_sizes[j] > 0:\n            C[:, j] = 1e6 * X[:, j] / lib_sizes[j]\n    \n    Y = np.log2(C + 1)\n    return Y\n\ndef custom_kmeans(data, k, max_iter=100):\n    \"\"\"\n    Performs k-means clustering with deterministic initialization and specific rules.\n    data: Data matrix (samples x features).\n    \"\"\"\n    n_samples, n_features = data.shape\n    \n    # Initialization: first k samples are initial centers\n    centroids = data[:k].copy().astype(np.float64)\n    \n    labels = -np.ones(n_samples, dtype=int)\n    \n    for iteration in range(max_iter):\n        # Assignment step\n        new_labels = np.zeros(n_samples, dtype=int)\n        for i in range(n_samples):\n            # Using squared Euclidean distance to avoid sqrt\n            distances_sq = np.sum((data[i] - centroids)**2, axis=1)\n            min_dist_sq = np.min(distances_sq)\n            # Tie-breaking: assign to cluster with smallest index\n            min_dist_indices = np.where(distances_sq == min_dist_sq)[0]\n            new_labels[i] = np.min(min_dist_indices)\n        \n        if np.array_equal(labels, new_labels):\n            break\n        \n        labels = new_labels\n        \n        # Update step\n        new_centroids = np.zeros((k, n_features), dtype=np.float64)\n        cluster_counts = np.bincount(labels, minlength=k)\n        \n        # Calculate new centroids for non-empty clusters\n        non_empty_mask = cluster_counts > 0\n        np.add.at(new_centroids, labels, data)\n        new_centroids[non_empty_mask] /= cluster_counts[non_empty_mask][:, np.newaxis]\n        \n        # Handle empty clusters\n        empty_clusters_indices = np.where(~non_empty_mask)[0]\n        if len(empty_clusters_indices) > 0:\n            existing_centroids = new_centroids[non_empty_mask]\n            \n            # Find sample(s) with largest minimum distance to any existing center\n            if existing_centroids.shape[0] > 0:\n                dists_to_centers_sq = np.array([np.sum((sample - existing_centroids)**2, axis=1) for sample in data])\n                min_dists_sq = np.min(dists_to_centers_sq, axis=1)\n                \n                # Sort samples by descending minimum distance to find candidates\n                furthest_samples_indices = np.argsort(-min_dists_sq)\n                \n                # Assign distinct furthest samples to empty clusters\n                candidate_idx = 0\n                for cluster_idx in empty_clusters_indices:\n                    new_centroids[cluster_idx] = data[furthest_samples_indices[candidate_idx]]\n                    candidate_idx += 1\n            else: # All clusters became empty, re-initialize from start\n                new_centroids[:len(empty_clusters_indices)] = data[:len(empty_clusters_indices)]\n        \n        centroids = new_centroids\n        \n    return labels\n\ndef hierarchical_clustering(data, k):\n    \"\"\"\n    Performs agglomerative hierarchical clustering with average linkage.\n    data: Data matrix (samples x features).\n    \"\"\"\n    if data.shape[0] < 2:\n        return np.zeros(data.shape[0], dtype=int)\n    Z = linkage(data, method='average', metric='euclidean')\n    labels = fcluster(Z, t=k, criterion='maxclust')\n    return labels - 1  # Convert to 0-based labels\n\ndef _canonicalize_labels(labels):\n    \"\"\"Converts labels to a canonical 0-indexed form for comparison.\"\"\"\n    mapping = {}\n    next_label = 0\n    new_labels = np.empty_like(labels)\n    for i, label in enumerate(labels):\n        if label not in mapping:\n            mapping[label] = next_label\n            next_label += 1\n        new_labels[i] = mapping[label]\n    return new_labels\n\ndef adjusted_rand_index(labels_true, labels_pred):\n    \"\"\"Computes the Adjusted Rand Index.\"\"\"\n    n_samples = len(labels_true)\n    if n_samples <= 1:\n        return 1.0\n\n    # Create contingency table\n    classes = np.unique(labels_true)\n    clusters = np.unique(labels_pred)\n    contingency = np.empty((classes.shape[0], clusters.shape[0]), dtype=int)\n    for i, class_val in enumerate(classes):\n        for j, cluster_val in enumerate(clusters):\n            contingency[i, j] = np.sum((labels_true == class_val) & (labels_pred == cluster_val))\n\n    sum_comb_nij = sum(comb(n, 2, exact=True) for n in contingency.flatten())\n    sum_comb_a = sum(comb(n, 2, exact=True) for n in np.sum(contingency, axis=1))\n    sum_comb_b = sum(comb(n, 2, exact=True) for n in np.sum(contingency, axis=0))\n    \n    comb_s = comb(n_samples, 2, exact=True)\n    if comb_s == 0:\n        return 1.0\n\n    expected_index = (sum_comb_a * sum_comb_b) / comb_s\n    numerator = sum_comb_nij - expected_index\n    denominator = 0.5 * (sum_comb_a + sum_comb_b) - expected_index\n    \n    if denominator == 0:\n        labels_true_canon = _canonicalize_labels(labels_true)\n        labels_pred_canon = _canonicalize_labels(labels_pred)\n        return 1.0 if np.array_equal(labels_true_canon, labels_pred_canon) else 0.0\n    \n    return numerator / denominator\n\ndef solve():\n    \"\"\"Main function to run all test cases and produce the final output.\"\"\"\n    test_cases = [\n        (np.array([[100, 1000, 10, 100], [10, 100, 100, 1000]]), 2),\n        (np.array([[0, 12, 0], [0, 0, 9]]), 2),\n        (np.array([[100, 96, 104, 10, 16, 4], \n                   [50, 48, 52, 50, 48, 52], \n                   [10, 16, 4, 100, 96, 104]]), 2)\n    ]\n\n    results = []\n    for X, k in test_cases:\n        Y = transform_data(X)\n        \n        # Samples are columns in X, so we transpose for standard clustering input (samples x features)\n        X_T = X.T\n        Y_T = Y.T\n\n        # k-means clustering\n        labels_x_kmeans = custom_kmeans(X_T, k)\n        labels_y_kmeans = custom_kmeans(Y_T, k)\n        ari_kmeans = adjusted_rand_index(labels_x_kmeans, labels_y_kmeans)\n        results.append(round(ari_kmeans, 6))\n\n        # Hierarchical clustering\n        labels_x_hclust = hierarchical_clustering(X_T, k)\n        labels_y_hclust = hierarchical_clustering(Y_T, k)\n        ari_hclust = adjusted_rand_index(labels_x_hclust, labels_y_hclust)\n        results.append(round(ari_hclust, 6))\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2379247"}, {"introduction": "层次聚类是一种强大的无监督学习方法，但其结果高度依赖于所选择的“连接方法”（linkage method），这决定了如何衡量簇与簇之间的距离。本练习通过一个精心设计的合成数据集，清晰地展示了“单连接”与“全连接”这两种极端情况的特性。通过分析它们在处理由“桥梁”连接的两个簇时的不同表现，你将学会如何根据数据结构选择合适的算法参数，并理解“链式效应”等关键概念。[@problem_id:2379235]", "problem": "给定一个二维点集族，旨在模拟两个表达样本组，并带有一个可选的、由中间点构成的连接桥。设环境空间为 $\\mathbb{R}^2$，欧几里得距离为 $d((x_1,y_1),(x_2,y_2))=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$。对于固定的参数 $L>0$、一个奇数 $m\\ge 3$、垂直间距 $\\Delta_y>0$、桥端点 $a < 0 < b$，以及一个整数 $k\\ge 0$，定义点集 $S$ 为 $S_L \\cup S_R \\cup S_B$，其中：\n- $S_L = \\{(-L,\\, j\\cdot\\Delta_y) \\mid j \\in \\{-\\frac{m-1}{2}, \\ldots, \\frac{m-1}{2}\\}\\}$。\n- $S_R = \\{(L,\\, j\\cdot\\Delta_y) \\mid j \\in \\{-\\frac{m-1}{2}, \\ldots, \\frac{m-1}{2}\\}\\}$。\n- $S_B = \\begin{cases} \\emptyset & \\text{if } k=0, \\\\ \\{(x_j,\\,0) \\mid x_j=a+j\\cdot\\frac{(b-a)}{(k-1)},\\ j\\in\\{0,\\ldots,k-1\\}\\} & \\text{if } k\\ge 1. \\end{cases}$\n\n每个点 $p \\in S$ 根据其 $x$ 坐标被赋予一个基准标签 $g(p)$，$g(p) = 0$ 若 $p_x  0$ 否则为 $1$。\n\n您必须实现凝聚式层次聚类，使用两种链接方法：单链接和全链接。对于每种链接方法，计算以下两个值：\n1.  **纯度 ($P$)**：将树状图切割成两个簇 $C_1, C_2$ 后，计算纯度为 $P = \\frac{1}{N} \\left( \\max(|C_1 \\cap G_0|, |C_1 \\cap G_1|) + \\max(|C_2 \\cap G_0|, |C_2 \\cap G_1|) \\right)$，其中 $N = |S|$，$G_i = \\{p \\in S \\mid g(p)=i\\}$。\n2.  **倒数第二个合并的高度 ($T$)**：在包含 $N$ 个点的层次结构中，有 $N-1$ 次合并。$T$ 是第 $N-2$ 次合并的高度，即最后一个使簇数从 3 变为 2 的合并的高度。\n\n所有计算均不带单位。不涉及角度。将所有报告的实数四舍五入到 $4$ 位小数。\n\n测试套件：\n对于以下每个测试用例，计算单链接的 $(P,T)$ 对和全链接的 $(P,T)$ 对。\n- $L=10$, $m=5$, $\\Delta_y=3.1$\n- 测试用例 1：$a=0, b=0, k=0$\n- 测试用例 2：$a=-4.8, b=5.2, k=9$\n- 测试用例 3：$a=-6.7, b=7.1, k=17$\n\n要求的程序输出：\n- 您的程序必须生成一行，包含一个用方括号括起来的逗号分隔列表。该列表必须包含所有三个测试用例的结果，每个结果都是一个形如 `[P_single, P_complete, T_single, T_complete]` 的列表。\n- 例如，一个语法上有效的输出应类似于 `[[1.0000,1.0000,2.0000,20.0000],[0.5000,1.0000,1.0000,20.0000],[...]]`。", "solution": "问题陈述经过严格审查，被确定为有效且可解，尽管存在一个微小歧义。桥集 $S_B = \\{(x_j,\\,0)\\ |\\ x_j=a+j\\cdot\\frac{(b-a)}{(k-1)},\\ j\\in\\{0,1,\\ldots,k-1\\}\\}$ 的定义，对于 $k=1$ 的情况，由于除以零而定义不明确。然而，所提供的测试用例使用了 $k \\in \\{0, 9, 17\\}$，所有这些情况都是明确定义的。此外，参数约束 $a < 0 < b$ 被遵守，除了在 $k=0$ 的情况下 $a$ 和 $b$ 的值无关紧要。解决方案将严格遵循测试用例中给出的值。\n\n该问题要求实现凝聚式层次聚类，并使用两种链接准则——单链接和全链接。对于每个测试用例中定义的点集，解决方案必须计算两个指标：切割为两个簇后的聚类纯度，以及倒数第二个合并的高度。\n\n首先，我们为每个测试用例生成点集 $S = S_L \\cup S_R \\cup S_B$。$S_L$ 和 $S_R$ 是两个垂直排列的、大小为 $m$ 的点簇，分别位于 $x=-L$ 和 $x=L$。$S_B$ 是一个由 $k$ 个点组成的水平桥，连接着左右两个区域。基准标签 $g(p)$ 根据点的 $x$ 坐标的符号进行分配，将点分为“左”($g=0$)和“右”($g=1$)两个集合。\n\n接下来，我们需要实现层次聚类算法。该算法从每个点自成一簇开始。然后，它迭代地合并两个最“相似”的簇，直到只剩下一个簇。关键在于如何定义簇之间的“相似度”（或距离）：\n-   **单链接（Single Linkage）**：两个簇之间的距离是它们各自成员中最接近的一对点之间的距离。即 $d(C_i, C_j) = \\min_{p \\in C_i, q \\in C_j} d(p,q)$。这种方法倾向于连接空间中被一系列中间点连接的簇，即使这些簇的整体相距很远，这被称为“链式效应”。\n-   **全链接（Complete Linkage）**：两个簇之间的距离是它们各自成员中最远的一对点之间的距离。即 $d(C_i, C_j) = \\max_{p \\in C_i, q \\in C_j} d(p,q)$。这种方法倾向于产生紧凑的、大致呈球形的簇，并且对异常值更敏感。\n\n在算法的每次迭代中，我们计算所有当前簇对之间的链接距离，找到最小距离的一对并将其合并。该合并操作的距离值被记录为合并高度。这个过程重复 $N-1$ 次，其中 $N$ 是点的总数。\n\n在获得完整的合并历史后，我们提取所需的两个量：\n1.  **倒数第二个合并的高度 ($T$)**：在 $N-1$ 次合并的序列中，这是第 $N-2$ 次合并的高度。这个高度对应于将系统从三个簇减少到两个簇的合并步骤。\n2.  **纯度 ($P$)**：为了计算纯度，我们需要将数据划分为两个簇。这可以通过停止层次聚类过程在还剩两个簇时（即在最后一次合并之前）来实现。得到两个簇 $C_1$ 和 $C_2$ 后，我们将它们与基准标签 $G_0$ 和 $G_1$ 进行比较。纯度是正确分类的点的比例，计算公式为 $P = \\frac{1}{N} \\sum_{i=1}^2 \\max_j |C_i \\cap G_j|$，该公式与问题中的公式等价。\n\n这个过程将对每个测试用例和每个链接方法（单链接和全链接）执行。\n-   **测试用例 1 ($k=0$)**：没有桥。我们预计两种方法都能很好地分离 $S_L$ 和 $S_R$，因为它们相距很远，从而得到高纯度。$T$ 的值将反映 $S_L$ 或 $S_R$ 内部的凝聚力，而全链接的最终合并高度将反映两个簇之间的距离。\n-   **测试用例 2 ($k=9$) 和 3 ($k=17$)**：存在一个桥。对于单链接，由于链式效应，桥点将把 $S_L$ 和 $S_R$ 连接成一个长链。这将导致非常低的纯度，因为切割成两个簇可能会随意地切断这个链。对于全链接，由于它寻找紧凑的簇，它应该能够抵抗桥的影响，并将 $S_L$ 和 $S_R$ 分开，从而保持高纯度。\n\n该解决方案将为每个测试用例生成四个值 $(P_{\\text{single}}, P_{\\text{complete}}, T_{\\text{single}}, T_{\\text{complete}})$，然后按照要求的格式将它们全部格式化为单行输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_point_set(L, m, delta_y, a, b, k):\n    \"\"\"Constructs the point set S and ground truth labels.\"\"\"\n    # S_L: Left set\n    j_indices = np.arange(-(m - 1) / 2, (m - 1) / 2 + 1)\n    s_l = np.array([[-L, j * delta_y] for j in j_indices])\n\n    # S_R: Right set\n    s_r = np.array([[L, j * delta_y] for j in j_indices])\n\n    # S_B: Bridge set\n    if k == 0:\n        s_b = np.empty((0, 2))\n    elif k == 1:\n        # The problem statement is ill-defined for k=1.\n        # Based on practical interpretation for a single point, place it at 'a'.\n        s_b = np.array([[a, 0.0]])\n    else:\n        s_b = np.array([[a + j * (b - a) / (k - 1), 0.0] for j in range(k)])\n\n    points = np.vstack([s_l, s_r, s_b])\n    \n    # Ground truth labels g(p)\n    labels = (points[:, 0] >= 0).astype(int)\n    \n    return points, labels\n\ndef pairwise_distance_matrix(points):\n    \"\"\"Computes the Euclidean distance matrix for all pairs of points.\"\"\"\n    # Using broadcasting for efficiency\n    return np.sqrt(np.sum((points[:, np.newaxis, :] - points[np.newaxis, :, :])**2, axis=-1))\n\ndef agglomerative_hierarchical_clustering(points, linkage):\n    \"\"\"\n    Performs agglomerative hierarchical clustering from scratch.\n    \n    Args:\n        points (np.ndarray): An N-by-2 array of point coordinates.\n        linkage (str): 'single' or 'complete'.\n\n    Returns:\n        tuple: A list of merge heights and the two-cluster partition.\n    \"\"\"\n    n = len(points)\n    if n <= 1:\n        return [], [[i for i in range(n)]]\n\n    # Pre-compute all point-to-point distances\n    dist_matrix = pairwise_distance_matrix(points)\n    \n    # Initialize each point as a cluster of indices\n    clusters = [{i} for i in range(n)]\n    merge_heights = []\n    \n    two_cluster_partition = None\n\n    for _ in range(n - 1):\n        min_dist = np.inf\n        best_pair = (-1, -1)\n        \n        # Find the two closest clusters\n        for i in range(len(clusters)):\n            for j in range(i + 1, len(clusters)):\n                cluster1_indices = list(clusters[i])\n                cluster2_indices = list(clusters[j])\n                \n                # Extract the submatrix of distances between points in the two clusters\n                sub_dist = dist_matrix[np.ix_(cluster1_indices, cluster2_indices)]\n                \n                if linkage == 'single':\n                    d = np.min(sub_dist)\n                elif linkage == 'complete':\n                    d = np.max(sub_dist)\n                else:\n                    raise ValueError(\"Invalid linkage method specified.\")\n                \n                if d < min_dist:\n                    min_dist = d\n                    best_pair = (i, j)\n        \n        merge_heights.append(min_dist)\n        \n        # Merge the closest pair\n        i, j = best_pair\n        if i > j: i, j = j, i  # Ensure i < j for correct popping\n        \n        clusters[i].update(clusters[j])\n        clusters.pop(j)\n\n        # Capture the partition when exactly two clusters remain\n        if len(clusters) == 2:\n            two_cluster_partition = [list(c) for c in clusters]\n\n    return merge_heights, two_cluster_partition\n\ndef calculate_purity(partition, labels):\n    \"\"\"Calculates the purity of a two-cluster partition.\"\"\"\n    n = len(labels)\n    if n == 0: return 1.0\n\n    c1_indices, c2_indices = partition\n    \n    c1_labels = labels[c1_indices]\n    c2_labels = labels[c2_indices]\n    \n    # Count members of each ground-truth class in the first cluster\n    num_c1_g0 = np.sum(c1_labels == 0)\n    num_c1_g1 = len(c1_labels) - num_c1_g0\n    \n    # Count members of each ground-truth class in the second cluster\n    num_c2_g0 = np.sum(c2_labels == 0)\n    num_c2_g1 = len(c2_labels) - num_c2_g0\n    \n    # Sum of majority class sizes\n    correctly_clustered = max(num_c1_g0, num_c1_g1) + max(num_c2_g0, num_c2_g1)\n    \n    purity = correctly_clustered / n\n    return purity\n\n\ndef solve():\n    \"\"\"Main function to solve the problem for all test cases.\"\"\"\n    test_cases = [\n        # (L, m, Delta_y, a, b, k)\n        (10, 5, 3.1, 0, 0, 0),        # Case 1\n        (10, 5, 3.1, -4.8, 5.2, 9),   # Case 2\n        (10, 5, 3.1, -6.7, 7.1, 17),  # Case 3\n    ]\n\n    all_results = []\n    for case in test_cases:\n        L, m, delta_y, a, b, k = case\n        points, labels = construct_point_set(L, m, delta_y, a, b, k)\n        n = len(points)\n\n        # Single linkage\n        heights_s, partition_s = agglomerative_hierarchical_clustering(points, 'single')\n        p_single = calculate_purity(partition_s, labels)\n        # h_{N-2} is the (N-2)-th element in a 1-indexed list of N-1 heights.\n        # This corresponds to index n-3 in a 0-indexed list.\n        t_single = heights_s[n - 3] if n > 2 else (heights_s[0] if n==2 else 0)\n\n        # Complete linkage\n        heights_c, partition_c = agglomerative_hierarchical_clustering(points, 'complete')\n        p_complete = calculate_purity(partition_c, labels)\n        t_complete = heights_c[n - 3] if n > 2 else (heights_c[0] if n==2 else 0)\n        \n        all_results.append([p_single, p_complete, t_single, t_complete])\n    \n    # Format the final output string\n    formatted_results = []\n    for res in all_results:\n        formatted_list = f\"[{res[0]:.4f},{res[1]:.4f},{res[2]:.4f},{res[3]:.4f}]\"\n        formatted_results.append(formatted_list)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2379235"}, {"introduction": "在实际应用中，单一聚类算法往往有其局限性，例如$k$-均值对初始中心点的选择非常敏感，容易陷入局部最优解。本练习将指导你实践一种结合了层次聚类和$k$-均值优点的混合策略。你将首先利用层次聚类探索数据的全局结构并获得一个合理的初始聚类方案，然后用此方案来初始化$k$-均值算法进行精细优化，最终得到更稳健、更可靠的聚类结果。[@problem_id:2379272]", "problem": "给定一个由实值矩阵 $X \\in \\mathbb{R}^{n \\times m}$ 表示的有限基因表达谱集合，其中每一行对应一个基因，每一列对应一个实验条件。设基因索引为 $0,1,\\ldots,n-1$，第 $i$ 行表示为 $x_i \\in \\mathbb{R}^m$。考虑欧几里得距离 $d(x_i,x_j) = \\lVert x_i - x_j \\rVert_2$。\n\n请用纯数学术语定义以下对象和约定。\n\n- 通过迭代地合并子集，在基因集合上构建一个凝聚层次结构。在任何阶段，设 $\\mathcal{C}$ 为当前由非空、两两不相交的子集构成的族，其并集为 $\\{0,1,\\ldots,n-1\\}$。对于任意两个不同的子集 $A,B \\in \\mathcal{C}$，定义平均链接不相似度为\n$$\n\\delta(A,B) = \\frac{1}{|A|\\,|B|} \\sum_{i \\in A}\\sum_{j \\in B} d(x_i,x_j).\n$$\n在每个合并步骤中，选择一个使 $\\delta(A,B)$ 在所有无序对 $\\{A,B\\} \\subset \\mathcal{C}$（其中 $A \\neq B$）中最小化的对 $(A^\\star,B^\\star)$。如果 $\\delta(A,B)$ 的值出现平局，则通过以下确定性规则打破平局：在所有平局的对中，选择其有序对 $(\\min A, \\min B)$ 在字典序上最小的对，其中对于任意集合 $A$ 和 $B$，我们假设 $\\min A < \\min B$。合并高度定义为 $h(A^\\star,B^\\star) = \\delta(A^\\star,B^\\star)$。在 $\\mathcal{C}$ 中用 $A^\\star \\cup B^\\star$ 替换 $A^\\star$ 和 $B^\\star$，并继续此过程，直到只剩下一个子集。该结果会产生一个具有明确定义的合并高度和合并总顺序的唯一树状图。\n\n- 对于任意阈值 $t \\in \\mathbb{R}_{\\ge 0}$，将在高度 $t$ 处切割树状图所诱导的划分定义如下。从单元素集合开始，按其高度 $h$ 的非递减顺序应用所有满足 $h \\le t$ 的合并；任何高度严格大于 $t$ 的合并都不应用。得到的子集族是 $\\{0,1,\\ldots,n-1\\}$ 的一个划分 $\\mathcal{P}_t = \\{C_1,\\ldots,C_k\\}$，其中 $k \\in \\{1,2,\\ldots,n\\}$。为确保这 $k$ 个部分的规范标记，对这些部分进行排序，使得如果 $a_\\ell = \\min C_\\ell$，则 $a_1 < a_2 < \\cdots < a_k$。定义 $k = |\\mathcal{P}_t|$。\n\n- 对于任意非空子集 $C \\subseteq \\{0,1,\\ldots,n-1\\}$，定义其质心为\n$$\n\\mu_C = \\frac{1}{|C|} \\sum_{i \\in C} x_i \\in \\mathbb{R}^m.\n$$\n\n- 对于 $\\{0,1,\\ldots,n-1\\}$ 的任意划分 $\\mathcal{A} = \\{A_1,\\ldots,A_k\\}$（$k \\ge 1$），以及任意选择的中心点 $\\{\\mu_1,\\ldots,\\mu_k\\} \\subset \\mathbb{R}^m$，定义簇内平方和 (WCSS) 目标函数为\n$$\nW(\\mathcal{A}, \\{\\mu_\\ell\\}_{\\ell=1}^k) = \\sum_{\\ell=1}^k \\sum_{i \\in A_\\ell} \\lVert x_i - \\mu_\\ell \\rVert_2^2.\n$$\n\n- 一个对 $(\\mathcal{A}, \\{\\mu_\\ell\\})$（其中 $\\mathcal{A} = \\{A_1,\\ldots,A_k\\}$）被称为自洽的，如果它同时满足以下两个性质：\n    1. 对于每个 $i \\in \\{0,1,\\ldots,n-1\\}$，如果 $i \\in A_\\ell$，则对于所有 $r \\in \\{1,\\ldots,k\\}$，都有 $\\lVert x_i - \\mu_\\ell \\rVert_2 \\le \\lVert x_i - \\mu_r \\rVert_2$，平局时通过选择最小索引的簇来打破（即，如果存在多个最小化距离的 $r$，则将 $i$ 分配给其中最小的 $r$）。\n    2. 对于每个 $\\ell \\in \\{1,\\ldots,k\\}$，$\\mu_\\ell$ 是 $A_\\ell$ 的质心，即 $\\mu_\\ell = \\frac{1}{|A_\\ell|}\\sum_{i \\in A_\\ell} x_i$。\n如果在这些条件下某个 $A_\\ell$ 会变为空集，则相应的 $\\mu_\\ell$ 将保持其先前的值不变，以维持更新的良定义性。\n\n- 考虑以下确定性方案，从给定的阈值 $t$ 获得一个自洽对：设 $\\mathcal{P}_t = \\{C_1,\\ldots,C_k\\}$ 是在高度 $t$ 处切割得到的划分，其中 $C_\\ell$ 如上所述按 $\\min C_\\ell$ 递增排序。初始化 $\\mu_\\ell^{(0)} = \\mu_{C_\\ell}$，其中 $\\ell \\in \\{1,\\ldots,k\\}$。通过在每次迭代 $r \\in \\{0,1,2,\\ldots\\}$ 时，将每个 $i \\in \\{0,\\ldots,n-1\\}$ 分配给使 $\\lVert x_i - \\mu_\\ell^{(r)} \\rVert_2$ 最小化的索引 $\\ell$ 来定义一个序列，平局时由最小的 $\\ell$ 打破，从而得到 $\\mathcal{A}^{(r+1)} = \\{A_1^{(r+1)},\\ldots,A_k^{(r+1)}\\}$。然后，当 $A_\\ell^{(r+1)}$ 非空时，更新 $\\mu_\\ell^{(r+1)}$ 为 $A_\\ell^{(r+1)}$ 的质心；如果 $A_\\ell^{(r+1)}$ 为空，则设 $\\mu_\\ell^{(r+1)} = \\mu_\\ell^{(r)}$。当 $\\mathcal{A}^{(r+1)} = \\mathcal{A}^{(r)}$（无变化）时，或在最多 $1000$ 次迭代后停止，以先发生者为准。输出是具有 $k$ 个部分的 $(\\mathcal{A}^\\star, \\{\\mu_\\ell^\\star\\})$，以及 $W^\\star = W(\\mathcal{A}^\\star, \\{\\mu_\\ell^\\star\\})$。\n\n使用上述定义，实现一个程序，对以下基因表达矩阵 $X \\in \\mathbb{R}^{8 \\times 3}$ 进行操作，该矩阵有 $n = 8$ 个基因和 $m = 3$ 个条件，行按基因索引排序：\n$$\nX = \\begin{bmatrix}\n10.0 & 11.2 & 9.5 \\\\\n9.5 & 10.8 & 9.7 \\\\\n10.8 & 11.0 & 10.2 \\\\\n49.5 & 50.1 & 51.2 \\\\\n51.2 & 49.7 & 50.4 \\\\\n49.9 & 51.0 & 49.8 \\\\\n100.2 & 98.9 & 101.5 \\\\\n101.1 & 100.3 & 98.8\n\\end{bmatrix}.\n$$\n\n您的程序必须对下面测试套件中的每个阈值 $t$ 计算：\n- $\\mathcal{P}_t$ 中的簇数量 $k$，\n- 自洽分配，表示为一个包含 $n$ 个整数的列表 $\\{0,1,\\ldots,k-1\\}$，指示每个基因索引 $i$ 在 $\\mathcal{A}^\\star$ 中被分配到的簇的索引，其中簇索引按指定的 $\\min C_\\ell$ 递增排序，\n- 最终的质心 $\\{\\mu_\\ell^\\star\\}_{\\ell=1}^k$，表示为一个包含 $k$ 个列表的列表，每个列表长度为 $m$，\n- 最终的簇内平方和 $W^\\star$。\n\n所有实值输出必须四舍五入到恰好 $6$ 位小数。测试套件的阈值是：\n- $t_1 = 0.01$，\n- $t_2 = 5.0$，\n- $t_3 = 1000.0$。\n\n程序的最终输出必须将所有测试用例的结果聚合到单行中，格式如下。对于每个测试用例 $t_j$，生成一个列表\n$$\n\\left[ k,\\, W^\\star,\\, [a_0,a_1,\\ldots,a_{n-1}],\\, [\\,[\\mu_{1,1},\\ldots,\\mu_{1,m}],\\,\\ldots,\\,[\\mu_{k,1},\\ldots,\\mu_{k,m}]\\,] \\right],\n$$\n其中 $a_i$ 是从零开始的簇分配索引，$\\mu_{\\ell,r}$ 是四舍五入到 $6$ 位小数的质心分量。程序应生成单行输出，其中包含所有测试用例的结果，形式为逗号分隔的列表，并用方括号括起来，例如：\n$[ \\text{result\\_for\\_}t_1, \\text{result\\_for\\_}t_2, \\text{result\\_for\\_}t_3 ]$。\n\n您的实现必须在提供的平局打破约定下是确定性的，并且不得需要任何外部输入。\n\n该测试套件旨在涵盖：当 $t = t_1$ 时 $k = n$ 的边界条件（无合并），当 $t = t_2$ 时具有中间 $k$ 值的典型情况，以及当 $t = t_3$ 时 $k = 1$ 的边界条件。", "solution": "该问题要求对给定的基因表达矩阵 $X \\in \\mathbb{R}^{8 \\times 3}$ 实现一个确定性的两阶段聚类分析。第一阶段涉及凝聚层次聚类，以生成数据的初始划分。第二阶段使用一个类似k-means的迭代算法来细化此划分，以获得一个自洽的结果。通过在每个阶段明确的平局打破规则，整个过程被设计为确定性的。\n\n首先，我们必须为 $n=8$ 个基因构建完整的凝聚层次结构。该过程从每个基因作为一个单元素簇开始，即 $\\mathcal{C}^{(0)} = \\{\\{0\\}, \\{1\\}, \\ldots, \\{7\\}\\}$。在随后的每一步中，我们合并两个表现出最小平均链接不相似度的簇 $A,B \\in \\mathcal{C}$，定义为：\n$$\n\\delta(A,B) = \\frac{1}{|A|\\,|B|} \\sum_{i \\in A}\\sum_{j \\in B} d(x_i,x_j)\n$$\n其中 $d(x_i, x_j) = \\lVert x_i - x_j \\rVert_2$ 是基因 $i$ 和基因 $j$ 表达谱之间的欧几里得距离。此步骤的一个关键方面是平局打破规则：如果多对簇产生相同的最小不相似度 $\\delta$，则选择对 $\\{A, B\\}$（其中 $\\min A < \\min B$），使得有序对 $(\\min A, \\min B)$ 在字典序上最小。此规则确保了最终的层次结构是唯一的，该层次结构由一个带有合并高度 $h(A, B) = \\delta(A, B)$ 的树状图表示。此过程执行 $n-1=7$ 步，直到所有基因都属于一个单一的簇。必须存储完整的合并序列及其对应的高度。\n\n接下来，对于每个给定的阈值 $t \\in \\{0.01, 5.0, 1000.0\\}$，我们导出一个初始划分 $\\mathcal{P}_t$。此划分是通过执行层次结构中所有高度 $h$ 小于或等于阈值 $t$ 的合并得到的。产生的划分 $\\mathcal{P}_t = \\{C_1, C_2, \\ldots, C_k\\}$ 由 $k$ 个不相交的簇组成。为了规范表示，这些簇被排序，使得它们的最小基因索引严格递增：$\\min C_1 < \\min C_2 < \\cdots < \\min C_k$。簇的数量 $k = |\\mathcal{P}_t|$ 由阈值 $t$ 决定。\n\n第三阶段是一个迭代细化过程，类似于k-means算法，旨在达到自洽状态。该过程使用划分 $\\mathcal{P}_t$ 进行初始化。具体来说，簇的数量为 $k$，初始质心是 $\\mathcal{P}_t$ 中各簇的均值：\n$$\n\\mu_\\ell^{(0)} = \\frac{1}{|C_\\ell|} \\sum_{i \\in C_\\ell} x_i \\quad \\text{for } \\ell = 1, \\ldots, k.\n$$\n然后，迭代过程交替执行两个步骤：\n1.  **分配步骤**：每个基因 $i$ 被分配到其质心 $\\mu_\\ell^{(r)}$ 与 $x_i$ 最接近的簇 $\\ell$。也就是说，我们找到 $\\ell^\\star = \\arg\\min_{\\ell \\in \\{1,\\ldots,k\\}} \\lVert x_i - \\mu_\\ell^{(r)} \\rVert_2$。同样，指定了平局打破规则：如果多个质心距离相等，则将基因分配给索引 $\\ell$ 最小的簇。此步骤产生一个新的划分 $\\mathcal{A}^{(r+1)}$。\n2.  **更新步骤**：根据新的分配重新计算质心。对于每个簇 $A_\\ell^{(r+1)}$，新的质心是其均值：$\\mu_\\ell^{(r+1)} = \\frac{1}{|A_\\ell^{(r+1)}|} \\sum_{i \\in A_\\ell^{(r+1)}} x_i$。如果任何簇 $A_\\ell^{(r+1)}$ 变为空，则其质心不更新；即 $\\mu_\\ell^{(r+1)} = \\mu_\\ell^{(r)}$。\n\n此过程持续进行，直到簇分配从一次迭代到下一次不再改变（$\\mathcal{A}^{(r+1)} = \\mathcal{A}^{(r)}$），或最多进行 $1000$ 次迭代。设最终的自洽划分为 $\\mathcal{A}^\\star$，最终质心为 $\\{\\mu_\\ell^\\star\\}$。\n\n最后，我们为最终的配置计算簇内平方和 (WCSS)：\n$$\nW^\\star = W(\\mathcal{A}^\\star, \\{\\mu_\\ell^\\star\\}) = \\sum_{\\ell=1}^k \\sum_{i \\in A_\\ell^\\star} \\lVert x_i - \\mu_\\ell^\\star \\rVert_2^2.\n$$\n对于每个测试阈值 $t$，收集结果——$k$、 $W^\\star$、每个基因的最终簇分配列表，以及最终质心列表。所有实值输出均四舍五入到 $6$ 位小数，并按规定格式化为单行。实现必须严格遵守所有定义和平局打破规则，以确保正确和确定性的结果。", "answer": "```python\nimport numpy as np\nimport sys\n\ndef solve():\n    \"\"\"\n    Main function to solve the clustering problem for the given test cases.\n    \"\"\"\n    X = np.array([\n        [10.0, 11.2, 9.5],\n        [9.5, 10.8, 9.7],\n        [10.8, 11.0, 10.2],\n        [49.5, 50.1, 51.2],\n        [51.2, 49.7, 50.4],\n        [49.9, 51.0, 49.8],\n        [100.2, 98.9, 101.5],\n        [101.1, 100.3, 98.8]\n    ])\n\n    ts = [0.01, 5.0, 1000.0]\n\n    # Perform hierarchical clustering once.\n    hc = HierarchicalClustering(X)\n    linkage_matrix = hc.get_linkage_matrix()\n\n    all_results = []\n    for t in ts:\n        result = solve_for_t(X, t, linkage_matrix)\n        all_results.append(result)\n\n    # Format output\n    results_strs = [format_result(r) for r in all_results]\n    print(f\"[{','.join(results_strs)}]\")\n\nclass HierarchicalClustering:\n    \"\"\"\n    Implements agglomerative hierarchical clustering with average linkage and\n    a specific tie-breaking rule.\n    \"\"\"\n    def __init__(self, X):\n        self.X = X\n        self.n = X.shape[0]\n        self.dist_matrix = self._compute_pairwise_distances()\n        self.linkage_matrix = self._build_linkage()\n\n    def get_linkage_matrix(self):\n        return self.linkage_matrix\n\n    def _compute_pairwise_distances(self):\n        dists = np.zeros((self.n, self.n))\n        for i in range(self.n):\n            for j in range(i + 1, self.n):\n                d = np.linalg.norm(self.X[i] - self.X[j])\n                dists[i, j] = dists[j, i] = d\n        return dists\n\n    def _get_cluster_dist(self, c1_indices, c2_indices):\n        total_dist = np.sum(self.dist_matrix[np.ix_(list(c1_indices), list(c2_indices))])\n        return total_dist / (len(c1_indices) * len(c2_indices))\n\n    def _build_linkage(self):\n        active_clusters = {i: {i} for i in range(self.n)}\n        linkage = []\n        next_cluster_id = self.n\n\n        for _ in range(self.n - 1):\n            best_pair_info = (sys.float_info.max,)\n\n            active_ids = sorted(list(active_clusters.keys()))\n            for i in range(len(active_ids)):\n                for j in range(i + 1, len(active_ids)):\n                    id1, id2 = active_ids[i], active_ids[j]\n                    c1, c2 = active_clusters[id1], active_clusters[id2]\n                    dist = self._get_cluster_dist(c1, c2)\n\n                    m1, m2 = min(c1), min(c2)\n                    if m1 > m2:\n                        m1, m2 = m2, m1\n                    \n                    current_pair_info = (dist, m1, m2, id1, id2)\n\n                    if current_pair_info < best_pair_info:\n                        best_pair_info = current_pair_info\n            \n            dist, _, _, id1, id2 = best_pair_info\n            id1, id2 = sorted((id1, id2))\n\n            c1 = active_clusters.pop(id1)\n            c2 = active_clusters.pop(id2)\n            \n            new_cluster = c1.union(c2)\n            linkage.append([float(id1), float(id2), dist, float(len(new_cluster))])\n            \n            active_clusters[next_cluster_id] = new_cluster\n            next_cluster_id += 1\n            \n        return np.array(linkage)\n\ndef solve_for_t(X, t, linkage_matrix):\n    \"\"\"\n    Solves the problem for a single threshold t.\n    \"\"\"\n    n = X.shape[0]\n    \n    # 1. Cut dendrogram\n    cluster_points = {i: {i} for i in range(n)}\n    dsu_parent = list(range(n))\n\n    def find_set(v):\n        if v == dsu_parent[v]:\n            return v\n        dsu_parent[v] = find_set(dsu_parent[v])\n        return dsu_parent[v]\n\n    def unite_sets(a, b):\n        a = find_set(a)\n        b = find_set(b)\n        if a != b:\n            dsu_parent[max(a, b)] = min(a,b)\n\n    next_cluster_id = n\n    for i in range(linkage_matrix.shape[0]):\n        h = linkage_matrix[i, 2]\n        if h > t:\n            break\n        id1, id2 = int(linkage_matrix[i, 0]), int(linkage_matrix[i, 1])\n        c1, c2 = cluster_points[id1], cluster_points[id2]\n        \n        rep1, rep2 = next(iter(c1)), next(iter(c2))\n        unite_sets(rep1, rep2)\n        \n        cluster_points[next_cluster_id] = c1.union(c2)\n        next_cluster_id += 1\n    \n    # Extract partition from DSU\n    initial_partition_map = {}\n    for i in range(n):\n        root = find_set(i)\n        if root not in initial_partition_map:\n            initial_partition_map[root] = set()\n        initial_partition_map[root].add(i)\n    \n    initial_clusters_list = sorted(list(initial_partition_map.values()), key=min)\n    k = len(initial_clusters_list)\n    \n    # 2. Initialize centroids\n    centroids = np.array([X[list(c)].mean(axis=0) for c in initial_clusters_list])\n    \n    # 3. K-means refinement\n    assignments = -np.ones(n, dtype=int)\n    \n    for iteration in range(1001):\n        new_assignments = np.zeros(n, dtype=int)\n        for i in range(n):\n            distances = np.linalg.norm(X[i] - centroids, axis=1)\n            min_dist = np.min(distances)\n            tied_indices = np.where(np.isclose(distances, min_dist))[0]\n            new_assignments[i] = np.min(tied_indices)\n            \n        if np.array_equal(assignments, new_assignments):\n            break\n        \n        assignments = new_assignments\n        if iteration == 1000:\n            break\n\n        new_centroids = np.copy(centroids)\n        for j in range(k):\n            cluster_indices = np.where(assignments == j)[0]\n            if len(cluster_indices) > 0:\n                new_centroids[j] = X[cluster_indices].mean(axis=0)\n        centroids = new_centroids\n\n    # 4. Final calculations\n    wcss = 0.0\n    for j in range(k):\n        cluster_indices = np.where(assignments == j)[0]\n        if len(cluster_indices) > 0:\n            wcss += np.sum(np.linalg.norm(X[cluster_indices] - centroids[j], axis=1)**2)\n            \n    return [k, wcss, assignments.tolist(), centroids.tolist()]\n\ndef format_result(res):\n    \"\"\"Formats a single test case result into the required string format.\"\"\"\n    k, W, assignments, centroids = res\n    k_str = str(k)\n    W_str = f\"{W:.6f}\"\n    assignments_str = str(assignments).replace(\" \", \"\")\n\n    centroid_strs = []\n    for cent_vec in centroids:\n        vec_strs = [f\"{val:.6f}\" for val in cent_vec]\n        centroid_strs.append(f\"[{','.join(vec_strs)}]\")\n    centroids_str = f\"[{','.join(centroid_strs)}]\"\n    \n    return f\"[{k_str},{W_str},{assignments_str},{centroids_str}]\"\n\nsolve()\n```", "id": "2379272"}]}