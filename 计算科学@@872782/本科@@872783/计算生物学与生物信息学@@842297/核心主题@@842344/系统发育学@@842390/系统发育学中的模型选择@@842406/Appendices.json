{"hands_on_practices": [{"introduction": "第一个实践是一项基础练习，旨在让您熟悉使用信息准则进行模型选择的核心机制。通过为一系列常见的核苷酸替换模型计算赤池信息准则（$AIC$）和贝叶斯信息准则（$BIC$），您将学会如何量化模型拟合度与模型复杂度之间的权衡。这项练习将巩固您对如何计算模型参数数量以及如何应用这些准则为给定数据集选择最合适模型的理解。", "problem": "一个长度为 $5000$ 个位点的核苷酸序列比对，在一个关联 $12$ 个分类单元的固定的、完全解析的无根拓扑结构上进行了分析。对于一个包含 $m$ 个分类单元的无根二叉系统发育树，有 $2m - 3$ 个分支，每个分支都有一个独立的支长参数。在五个时间同质、时间可逆的替换模型下获得的最大对数似然值（自然对数）如下所示，所有值都是在相同的拓扑结构和序列比对上估计的：\n\n- JC69: $\\ln \\hat{L}_{\\mathrm{JC69}} = -7423.6$\n- HKY: $\\ln \\hat{L}_{\\mathrm{HKY}} = -7089.2$\n- GTR: $\\ln \\hat{L}_{\\mathrm{GTR}} = -7012.8$\n- GTR+G（四个离散伽马类别，带有一个形状参数）：$\\ln \\hat{L}_{\\mathrm{GTR+G}} = -6844.5$\n- GTR+G+I（伽马分布如上，外加一个不变位点比例）：$\\ln \\hat{L}_{\\mathrm{GTR+G+I}} = -6839.7$\n\n假设核苷酸模型采用标准参数化：JC69 模型具有相等的碱基频率和相等的相对替换率，除了支长外不增加任何自由的替换过程参数；HKY 模型增加了一个转换/颠换速率比参数 $\\kappa$ 和三个独立的碱基频率参数；GTR 模型有五个独立的相对替换率参数和三个独立的碱基频率参数；$+G$ 扩展增加了一个伽马形状参数 $\\alpha$；$+I$ 扩展增加了一个不变位点比例的参数。离散伽马类别的数量是固定的，不增加参数。\n\n使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）的规范定义，完成以下任务。每个准则都在最大化对数似然值与一个惩罚项之间进行权衡，该惩罚项随自由参数数量的增加而增加，对于BIC，还随样本量（此处为序列比对位点的数量）的增加而增加：\n\n1. 确定每个模型的总自由参数数量 $k$，包括所有支长和模型特定参数。\n2. 计算每个模型的 AIC 和 BIC 值。\n3. 根据 AIC 和 BIC 确定最优模型（即各自准则下值最小的模型）。\n4. 作为您的最终数值答案，报告 AIC 最优模型的 Akaike 权重（基于 AIC 的归一化相对似然），以小数形式表示。将您的最终数值答案四舍五入到四位有效数字。", "solution": "该问题要求使用赤池信息准则（AIC）和贝叶斯信息准则（BIC）对五个嵌套的核苷酸替换模型进行比较分析。目标是确定最佳拟合模型，并通过其 Akaike 权重来量化其支持度。\n\n首先，我们必须定义这两个准则。AIC 由以下公式给出：\n$$\nAIC = 2k - 2\\ln \\hat{L}\n$$\n其中 $k$ 是模型中的自由参数数量，$\\ln \\hat{L}$ 是最大化自然对数似然值。BIC 由以下公式给出：\n$$\nBIC = k \\ln(n) - 2\\ln \\hat{L}\n$$\n其中 $n$ 是样本量，在此情境下为核苷酸序列比对的位点数，给定为 $n = 5000$。\n\n第一步是为五个模型中的每一个正确确定总自由参数数量 $k$。总参数数量是支长参数数量和替换过程参数数量的总和。\n对于一个有 $m$ 个分类单元的无根二叉系统发育树，其分支数量为 $2m - 3$。此处，有 $m = 12$ 个分类单元，分支数量为 $2(12) - 3 = 21$。每个分支都有一个长度参数，因此所有模型共享 $k_{branches} = 21$ 个参数。\n\n我们现在按描述确定每个模型的替换过程参数数量（$k_{subst}$）：\n- **JC69**：该模型假设碱基频率相等且只有一个替换速率。它没有用于替换过程的自由参数。因此，$k_{subst, \\mathrm{JC69}} = 0$。\n- **HKY**：该模型引入了一个转换/颠换速率比参数（$\\kappa$），并允许不相等的碱基频率。由于四个碱基频率之和必须为 $1$，因此有 $3$ 个自由的频率参数。因此，$k_{subst, \\mathrm{HKY}} = 1 + 3 = 4$。\n- **GTR**：这是通用时间可逆模型。它有 $6$ 个相对替换率参数，但它们是相对的，所以只有 $5$ 个是自由的。它还有 $3$ 个自由的碱基频率参数。因此，$k_{subst, \\mathrm{GTR}} = 5 + 3 = 8$。\n- **GTR+G**：该模型增加了一个用于描述位点间速率伽马分布的参数，即形状参数 $\\alpha$。因此，$k_{subst, \\mathrm{GTR+G}} = k_{subst, \\mathrm{GTR}} + 1 = 8 + 1 = 9$。\n- **GTR+G+I**：该模型增加了一个不变位点比例参数 $p_{inv}$。因此，$k_{subst, \\mathrm{GTR+G+I}} = k_{subst, \\mathrm{GTR+G}} + 1 = 9 + 1 = 10$。\n\n每个模型的总参数数量（$k$）为：\n- $k_{\\mathrm{JC69}} = 21 + 0 = 21$\n- $k_{\\mathrm{HKY}} = 21 + 4 = 25$\n- $k_{\\mathrm{GTR}} = 21 + 8 = 29$\n- $k_{\\mathrm{GTR+G}} = 21 + 9 = 30$\n- $k_{\\mathrm{GTR+G+I}} = 21 + 10 = 31$\n\n利用 $k$、$\\ln \\hat{L}$ 和 $n=5000$ 的值，我们可以计算每个模型的 AIC 和 BIC。\n\n1. **JC69**：$k = 21$，$\\ln \\hat{L} = -7423.6$\n   $AIC_{\\mathrm{JC69}} = 2(21) - 2(-7423.6) = 42 + 14847.2 = 14889.2$\n   $BIC_{\\mathrm{JC69}} = 21\\ln(5000) - 2(-7423.6) \\approx 21(8.5172) + 14847.2 = 178.8612 + 14847.2 = 15026.0612$\n\n2. **HKY**：$k = 25$，$\\ln \\hat{L} = -7089.2$\n   $AIC_{\\mathrm{HKY}} = 2(25) - 2(-7089.2) = 50 + 14178.4 = 14228.4$\n   $BIC_{\\mathrm{HKY}} = 25\\ln(5000) - 2(-7089.2) \\approx 25(8.5172) + 14178.4 = 212.9300 + 14178.4 = 14391.3300$\n\n3. **GTR**：$k = 29$，$\\ln \\hat{L} = -7012.8$\n   $AIC_{\\mathrm{GTR}} = 2(29) - 2(-7012.8) = 58 + 14025.6 = 14083.6$\n   $BIC_{\\mathrm{GTR}} = 29\\ln(5000) - 2(-7012.8) \\approx 29(8.5172) + 14025.6 = 246.9988 + 14025.6 = 14272.5988$\n\n4. **GTR+G**：$k = 30$，$\\ln \\hat{L} = -6844.5$\n   $AIC_{\\mathrm{GTR+G}} = 2(30) - 2(-6844.5) = 60 + 13689.0 = 13749.0$\n   $BIC_{\\mathrm{GTR+G}} = 30\\ln(5000) - 2(-6844.5) \\approx 30(8.5172) + 13689.0 = 255.5160 + 13689.0 = 13944.5160$\n\n5. **GTR+G+I**：$k = 31$，$\\ln \\hat{L} = -6839.7$\n   $AIC_{\\mathrm{GTR+G+I}} = 2(31) - 2(-6839.7) = 62 + 13679.4 = 13741.4$\n   $BIC_{\\mathrm{GTR+G+I}} = 31\\ln(5000) - 2(-6839.7) \\approx 31(8.5172) + 13679.4 = 264.0332 + 13679.4 = 13943.4332$\n\n最优模型是使信息准则得分最小化的模型。\n- 对于 AIC，最小值为 $AIC_{min} = 13741.4$，对应于 **GTR+G+I** 模型。\n- 对于 BIC，最小值为 $BIC_{min} \\approx 13943.43$，也对应于 **GTR+G+I** 模型。\n\n最后，我们必须计算 AIC 最优模型（即 GTR+G+I）的 Akaike 权重。模型 $i$ 的 Akaike 权重（$w_i$）是在候选模型集中衡量其相对支持度的指标。其计算公式为：\n$$\nw_i = \\frac{\\exp(-\\frac{1}{2}\\Delta_i)}{\\sum_{j=1}^{R} \\exp(-\\frac{1}{2}\\Delta_j)}\n$$\n其中 $\\Delta_i = AIC_i - AIC_{min}$，$R=5$ 是模型数量。\n\n首先，我们计算每个模型相对于 $AIC_{min} = AIC_{\\mathrm{GTR+G+I}} = 13741.4$ 的 $\\Delta_i$ 值。\n- $\\Delta_{\\mathrm{JC69}} = 14889.2 - 13741.4 = 1147.8$\n- $\\Delta_{\\mathrm{HKY}} = 14228.4 - 13741.4 = 487.0$\n- $\\Delta_{\\mathrm{GTR}} = 14083.6 - 13741.4 = 342.2$\n- $\\Delta_{\\mathrm{GTR+G}} = 13749.0 - 13741.4 = 7.6$\n- $\\Delta_{\\mathrm{GTR+G+I}} = 13741.4 - 13741.4 = 0$\n\n接下来，我们计算最优模型（GTR+G+I）的 Akaike 权重的分子：\n$\\exp(-\\frac{1}{2}\\Delta_{\\mathrm{GTR+G+I}}) = \\exp(-\\frac{1}{2} \\times 0) = \\exp(0) = 1$。\n\n现在，我们计算分母，即所有模型的相对似然之和：\n$\\sum_{j=1}^{5} \\exp(-\\frac{1}{2}\\Delta_j) = \\exp(-\\frac{1147.8}{2}) + \\exp(-\\frac{487.0}{2}) + \\exp(-\\frac{342.2}{2}) + \\exp(-\\frac{7.6}{2}) + \\exp(-\\frac{0}{2})$\n$\\sum_{j} \\exp(-\\frac{1}{2}\\Delta_j) = \\exp(-573.9) + \\exp(-243.5) + \\exp(-171.1) + \\exp(-3.8) + \\exp(0)$\n前三项小到可以忽略不计，在数值上可忽略。我们计算：\n$\\exp(-3.8) \\approx 0.0223708$\n$\\exp(0) = 1$\n所以，总和约等于 $0 + 0 + 0 + 0.0223708 + 1 = 1.0223708$。\n\nGTR+G+I 模型的 Akaike 权重为：\n$w_{\\mathrm{GTR+G+I}} = \\frac{1}{1.0223708} \\approx 0.978119$\n\n四舍五入到四位有效数字，AIC 最优模型的 Akaike 权重为 $0.9781$。这表明，在给定数据和候选模型集的情况下，GTR+G+I 模型在 Kullback-Leibler 意义上是最佳模型的概率约为 $97.81\\%$。", "answer": "$$\\boxed{0.9781}$$", "id": "2734801"}, {"introduction": "模型选择并非一个单一的领域；存在着多种方法，每种方法都有其自身的统计学原理。这项练习 [@problem_id:2406819] 展示了一个场景，其中赤池信息准则（$AIC$）和经典的似然比检验（$LRT$）对嵌套模型给出了相互矛盾的结果。通过分析这种差异，您将更深刻地理解信息论方法与零假设显著性检验之间的概念差异，并明白为何它们的结果并非总是一致。", "problem": "一位进化生物学家使用长度为 $1000$ 个位点的脱氧核糖核酸（DNA）序列比对，将两个时间可逆的核苷酸替换模型拟合到一个固定的、分辨率高的系统发育树上。这两个模型分别是带有伽马分布的位点间速率变异（记为 +$\\Gamma$）的Hasegawa–Kishino–Yano (HKY85)模型，以及通用时间可逆(GTR)+$\\Gamma$模型。GTR+$\\Gamma$模型通过放宽对相对可交换率的约束来嵌套HKY85+$\\Gamma$模型；相对于HKY85+$\\Gamma$模型，GTR+$\\Gamma$模型引入了 $4$ 个额外的自由替换过程参数，同时保持相同数量的枝长参数和相同的伽马形状参数。\n\n最大对数似然值（对所有模型特定参数，包括枝长和伽马形状参数进行优化后得到）如下：\n- $\\ell_{\\mathrm{HKY}+\\Gamma} = -2765.3$,\n- $\\ell_{\\mathrm{GTR}+\\Gamma} = -2760.7$.\n\n分析师在显著性水平 $\\alpha = 0.05$ 下应用了似然比检验（LRT），并为两个模型计算了Akaike信息准则（AIC）。假设满足标准正则性条件，因此LRT统计量近似服从自由度等于自由参数数量之差的卡方（$\\chi^2$）分布。\n\n哪个选项最能描述每种方法会偏好哪个模型，并给出它们在这种情况下可能产生分歧的正确统计学原因？\n\nA. AIC会选择GTR+$\\Gamma$，因为对数似然增益 $\\Delta \\ell = 4.6$ 使得 $2\\Delta \\ell = 9.2$ 超过了针对 $\\Delta k = 4$ 的AIC复杂度惩罚 $2\\Delta k = 8$，而LRT在 $\\alpha = 0.05$ 水平下不会拒绝HKY85+$\\Gamma$，因为 $2\\Delta \\ell = 9.2$ 小于 $\\chi^2_{4,0.95} \\approx 9.49$ 的阈值。它们产生分歧是因为AIC的目标是预期的样本外Kullback–Leibler (KL)散度，并对每个参数施加固定的惩罚值 $2$，而LRT通过使用卡方截断值来控制在选定 $\\alpha$ 水平下的I类错误，这个截断值可能比AIC的隐式阈值更严格。\n\nB. AIC和LRT都会选择GTR+$\\Gamma$，因为任何对数似然的正增长都足以证明增加参数的合理性；只有在样本量极小的情况下才会出现分歧。\n\nC. LRT会选择GTR+$\\Gamma$，因为它在 $\\Delta k > 0$ 时总是选择更复杂的嵌套模型，而AIC会偏好HKY85+$\\Gamma$，因为它比 $\\alpha = 0.05$ 的检验对复杂度的惩罚更重。\n\nD. 出现分歧是因为在 $n = 1000$ 时卡方近似无效，导致LRT偏好HKY85+$\\Gamma$而AIC偏好GTR+$\\Gamma$；在有效的渐近性下，它们会达成一致。\n\nE. 它们产生分歧是因为在原假设下，伽马形状参数位于参数空间的边界上，这使得LRT无效；AIC不受影响，因此偏好更复杂的模型。", "solution": "该问题陈述是统计模型选择应用于分子系统发育学的一个有效练习。它具有科学依据，提法恰当，并包含了进行所要求分析的所有必要信息。\n\n任务是比较两种嵌套的核苷酸替换模型，较简单的带有伽马分布速率的Hasegawa–Kishino–Yano模型（$\\text{HKY85}+\\Gamma$）和较复杂的带有伽马分布速率的通用时间可逆模型（$\\text{GTR}+\\Gamma$）。我们将使用两种标准方法：似然比检验（LRT）和Akaike信息准则（AIC）。\n\n令 $M_0$ 表示零模型，$\\text{HKY85}+\\Gamma$，令 $M_1$ 表示备择模型，$\\text{GTR}+\\Gamma$。\n提供的数据如下：\n- $M_0$ 的最大对数似然值：$\\ell_0 = \\ell_{\\mathrm{HKY}+\\Gamma} = -2765.3$。\n- $M_1$ 的最大对数似然值：$\\ell_1 = \\ell_{\\mathrm{GTR}+\\Gamma} = -2760.7$。\n- 两个模型自由参数数量的差异是 $\\Delta k = k_1 - k_0 = 4$。\n- LRT的显著性水平是 $\\alpha = 0.05$。\n\n首先，我们进行似然比检验（LRT）。LRT是适用的，因为模型是嵌套的（$M_0$ 是 $M_1$ 的一个特例）。原假设 $H_0$ 是较简单的模型 $M_0$ 是足够的。备择假设 $H_1$ 是较复杂的模型 $M_1$ 提供了显著更优的拟合。\n\nLRT统计量 $\\delta$ 的计算公式为：\n$$ \\delta = 2(\\ell_1 - \\ell_0) $$\n代入给定值：\n$$ \\delta = 2(-2760.7 - (-2765.3)) = 2(4.6) = 9.2 $$\n在原假设和标准正则性条件的假设下，该统计量服从自由度（$df$）等于自由参数数量之差的卡方（$\\chi^2$）分布，即 $df = \\Delta k = 4$。\n$$ \\delta \\sim \\chi^2_4 $$\n我们必须将计算出的 $\\delta$ 与显著性水平 $\\alpha = 0.05$ 下的 $\\chi^2_4$ 分布的临界值进行比较。这个临界值 $\\chi^2_{4, 0.95}$ 是被超过的概率为 $0.05$ 的值。根据标准统计表或计算，这个值约为 $9.488$。\n如果 $\\delta > \\chi^2_{4, 0.95}$，我们拒绝原假设 $H_0$。在我们的例子中，$9.2  9.488$。由于检验统计量小于临界值，我们未能拒绝原假设。LRT的结论是，没有足够的证据来证明 $\\text{GTR}+\\Gamma$ 模型增加的复杂性是合理的。因此，LRT偏好较简单的模型 $\\text{HKY85}+\\Gamma$。\n\n其次，我们使用Akaike信息准则（AIC）来评估模型。一个模型的AIC定义为：\n$$ \\text{AIC} = 2k - 2\\ell $$\n其中 $k$ 是自由参数的数量，$\\ell$ 是最大对数似然值。AIC分数较低的模型更受偏好。\n\n为了比较这两个模型，我们可以检查是否 $\\text{AIC}_1  \\text{AIC}_0$：\n$2k_1 - 2\\ell_1  2k_0 - 2\\ell_0$\n整理这个不等式得到：\n$2(\\ell_1 - \\ell_0) > 2(k_1 - k_0)$\n$2\\Delta\\ell > 2\\Delta k$\n让我们代入问题中的数值：\n- 对数似然增益为 $\\Delta\\ell = \\ell_1 - \\ell_0 = 4.6$。因此，$2\\Delta\\ell = 9.2$。\n- 参数增加量为 $\\Delta k = 4$。AIC对这种增加的复杂性的惩罚是 $2\\Delta k = 2(4) = 8$。\n\n不等式变为：\n$9.2 > 8$\n这个陈述是正确的。这意味着 $\\text{AIC}_{\\text{GTR}+\\Gamma}  \\text{AIC}_{\\text{HKY}+\\Gamma}$。因此，AIC偏好更复杂的模型 $\\text{GTR}+\\Gamma$。\n\n两种方法给出了相互矛盾的建议：LRT偏好较简单的 $\\text{HKY85}+\\Gamma$ 模型，而AIC偏好较复杂的 $\\text{GTR}+\\Gamma$ 模型。这种分歧源于它们对模型复杂度的不同惩罚函数。对于增加 $\\Delta k$ 个参数，AIC要求LRT统计量 $2\\Delta\\ell$ 超过一个固定的阈值 $2\\Delta k$。而LRT要求 $2\\Delta\\ell$ 超过一个阈值 $\\chi^2_{\\Delta k, 1-\\alpha}$，这个阈值取决于 $\\Delta k$ 和所选的显著性水平 $\\alpha$。\n对于本题，AIC的阈值是 $8$，而LRT在 $\\alpha=0.05$ 时的阈值约为 $9.49$。LRT对额外参数要求更高的证据标准，因此在这种情况下更为保守。\n\n现在我们评估给出的选项：\n\n**A. AIC会选择GTR+$\\Gamma$，因为对数似然增益 $\\Delta \\ell = 4.6$ 使得 $2\\Delta \\ell = 9.2$ 超过了针对 $\\Delta k = 4$ 的AIC复杂度惩罚 $2\\Delta k = 8$，而LRT在 $\\alpha = 0.05$ 水平下不会拒绝HKY85+$\\Gamma$，因为 $2\\Delta \\ell = 9.2$ 小于 $\\chi^2_{4,0.95} \\approx 9.49$ 的阈值。它们产生分歧是因为AIC的目标是预期的样本外Kullback–Leibler (KL)散度，并对每个参数施加固定的惩罚值 $2$，而LRT通过使用卡方截断值来控制在选定 $\\alpha$ 水平下的I类错误，这个截断值可能比AIC的隐式阈值更严格。**\n- 上面推导的AIC和LRT的计算和结论都是正确的。\n- 对分歧的解释也是正确的。AIC是一种信息论方法，试图估计预测准确性（近似KL散度），并使用固定的惩罚。LRT是一个假设检验框架，旨在将I类错误的发生率控制在预先指定的 $\\alpha$ 水平。LRT的临界值可以比（并且在本例中确实比）AIC的隐式阈值更严格。\n- 结论：**正确**。\n\n**B. AIC和LRT都会选择GTR+$\\Gamma$，因为任何对数似然的正增长都足以证明增加参数的合理性；只有在样本量极小的情况下才会出现分歧。**\n- 两种方法都会选择 $\\text{GTR}+\\Gamma$ 的前提是错误的。LRT选择了 $\\text{HKY85}+\\Gamma$。\n- 所给出的理由，“任何对数似然的正增长都足以证明增加参数的合理性”，是根本错误的。两种方法都对复杂性进行惩罚，并要求似然增益足够大以克服这种惩罚。\n- 结论：**不正确**。\n\n**C. LRT会选择GTR+$\\Gamma$，因为它在 $\\Delta k > 0$ 时总是选择更复杂的嵌套模型，而AIC会偏好HKY85+$\\Gamma$，因为它比 $\\alpha = 0.05$ 的检验对复杂度的惩罚更重。**\n- LRT会选择 $\\text{GTR}+\\Gamma$ 的说法是错误的。\n- AIC会偏好 $\\text{HKY85}+\\Gamma$ 的说法是错误的。\n- AIC比 $\\alpha=0.05$ 的LRT对复杂度的惩罚更重的说法在当前特定情况（$\\Delta k=4$）下是错误的。AIC的有效惩罚是 $2\\Delta k / 2 = \\Delta k = 4$ 的对数似然差异，对应于 $2\\Delta k = 8$ 的LRT统计量阈值。LRT的阈值是 $\\chi^2_{4,0.95} \\approx 9.49$。由于 $9.49 > 8$，在这种情况下LRT更严格（惩罚更重）。\n- 结论：**不正确**。\n\n**D. 出现分歧是因为在 $n = 1000$ 时卡方近似无效，导致LRT偏好HKY85+$\\Gamma$而AIC偏好GTR+$\\Gamma$；在有效的渐近性下，它们会达成一致。**\n- 题目明确说明要假设 $\\chi^2$ 近似是有效的。反驳这个假设违反了题目的约束。\n- 长度为 $n=1000$ 个位点的序列比对通常被认为足以使渐近性成为一个合理的近似。\n- AIC和LRT基于不同的统计学理念，即使在渐近情况下，也不能保证它们会达成一致。\n- 结论：**不正确**。\n\n**E. 它们产生分歧是因为在原假设下，伽马形状参数位于参数空间的边界上，这使得LRT无效；AIC不受影响，因此偏好更复杂的模型。**\n- 这里的参数检验是针对区分GTR和HKY85的替换率参数，而不是伽马形状参数。原假设没有将相关参数置于边界上。\n- 题目明确指示要假设标准正则性条件成立，而这个选项与此相矛盾。\n- 结论：**不正确**。\n\n选项A对两种方法的计算提供了完整而准确的描述，并对其结论差异给出了正确的统计学解释。", "answer": "$$\\boxed{A}$$", "id": "2406819"}, {"introduction": "在理想情况下，得分最高的模型总是最“真实”的，但现实世界的数据分析充满了潜在的陷阱。这项高级动手实践 [@problem_id:2406821] 将挑战您探索一个关键问题：模型错误指定。通过模拟和编码，您将研究一个高度灵活的替换模型在应用于错误的系统发育树时，是否会误导性地获得比一个更简单、正确的模型在真实树上更高的似然得分，从而揭示模型选择的一个关键局限性。", "problem": "要求您研究系统发育学中模型选择的一个基本问题：一个具有不正确树拓扑的复杂替换模型，是否能比在正确拓扑上的简单替换模型获得更高的最大似然值？请在时间可逆核苷酸替换模型的设定下，对固定的树拓扑进行研究，并应用标准的连续时间马尔可夫链 (CTMC) 在树上的似然函数。\n\n从以下基础开始：\n- 沿每个分支的核苷酸替换过程是一个 CTMC，其瞬时速率矩阵表示为 $Q$，沿持续时间为 $t$ 的分支的转移概率由 $P(t) = \\exp(Qt)$ 给出，其中 $\\exp$ 表示矩阵指数。\n- 在一个固定的有根二叉树上，对于一个时间可逆模型，比对序列的似然可以通过 Felsenstein 剪枝算法计算，该算法使用根节点的平稳分布 $\\boldsymbol{\\pi}$ 和每个分支的转移概率。对于一个位点，其似然是根节点各状态的平稳概率乘以从叶节点向上传播的条件似然乘积的总和。\n- 为了模拟位点间的速率变异，使用具有 $K$ 个类别的离散伽马近似：每个位点的速率 $r$ 从一个形状参数为 $\\alpha$、均值为 $1$ 的伽马分布中抽取，并用 $K$ 个等概率类别和固定速率 $\\{r_k\\}_{k=1}^K$ 来近似。总的位点似然是所有类别的平均值。\n\n您的任务是实现一个模拟和似然评估，以证明在某些情况下，一个在错误拓扑上的、具有位点间伽马分布速率异质性的通用时间可逆 (GTR) 模型 (GTR+$\\Gamma$)，可以比在正确拓扑上的 Jukes-Cantor $1969$ (JC$69$) 模型获得更高的对数似然。\n\n请遵循以下规范。\n\n1) 树拓扑和枝长。\n- 使用 $4$ 个标记为 $\\{A,B,C,D\\}$ 的分类单元，并将树表示为有根的严格二叉树，其根位于一个分裂成两个内部节点的内部节点上，每个内部节点再分裂成两个叶节点。在时间可逆性假设下，根的位置是任意的，但在模拟和似然计算中必须保持一致。\n- “正确”的拓扑是 $((A,B),(C,D))$。“错误”的拓扑是 $((A,C),(B,D))$。\n- 每棵有根树有 $6$ 个分支：根到左侧内部节点 ($\\ell_{RL}$)、根到右侧内部节点 ($\\ell_{RR}$) 和四个到叶节点的叶分支 ($\\ell_A,\\ell_B,\\ell_C,\\ell_D$)。枝长向量为 $\\mathbf{\\ell} = [\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_{RL},\\ell_{RR}]$。\n- 在评估错误拓扑上的似然时，通过将相同的叶分支长度分配给相同的叶节点，并将相同的两个根到内部节点的长度分配给两个内部边，来重用相同的枝长向量 $\\mathbf{\\ell}$。\n\n2) 替换模型。\n- 简单模型是 JC$69$，具有相等的碱基频率和相等的速率，经过缩放以使预期替换速率为每单位时间 $1$。也就是说，对于 $i \\neq j$，$Q_{ij} = \\frac{1}{3}$ 且 $Q_{ii} = -1$，平稳分布为 $\\boldsymbol{\\pi}_{JC} = [\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4}]$。\n- 复杂模型是 GTR+$\\Gamma$。使用一个通用时间可逆 (GTR) 速率矩阵，其平稳碱基频率为 $\\boldsymbol{\\pi} = [\\pi_A,\\pi_C,\\pi_G,\\pi_T] = [0.36, 0.14, 0.24, 0.26]$，以及针对无序核苷酸对的对称可交换性参数：\n  - $r_{AC} = 0.7$, $r_{AG} = 2.0$, $r_{AT} = 0.3$, $r_{CG} = 0.5$, $r_{CT} = 1.5$, $r_{GT} = 0.7$。\n  - 对于 $i \\neq j$，$Q_{ij} = r_{ij}\\,\\pi_j$，$Q_{ii} = -\\sum_{j\\neq i} Q_{ij}$。缩放 $Q$ 以使平均替换速率为 $1$，即 $-\\sum_i \\pi_i Q_{ii} = 1$。使用 $\\boldsymbol{\\pi}$ 作为根节点的平稳分布。\n- 对于位点间的伽马分布速率，使用 $K=4$ 个类别，形状参数为 $\\alpha$，均值为 $1$；通过 $\\mathrm{Gamma}(\\alpha,\\text{scale} = \\frac{1}{\\alpha})$ 分布的中位分位数来近似类别速率 $\\{r_k\\}$，并重新缩放它们以使 $\\frac{1}{K}\\sum_{k=1}^K r_k = 1$。\n\n3) 比对序列的模拟。\n- 对于每个测试用例，在正确的拓扑上，使用指定的 $\\boldsymbol{\\pi}$、可交换性参数 $\\{r_{ij}\\}$、形状参数 $\\alpha$ 和枝长向量 $\\mathbf{\\ell}$，在 GTR+$\\Gamma$ 模型下模拟一个包含 $L$ 个独立同分布位点的比对序列。对于每个位点，从 $K$ 个类别中均匀抽取一个速率类别，并通过从根节点（从平稳分布 $\\boldsymbol{\\pi}$ 中抽取）向下传播来生成叶节点处的核苷酸状态，该位点沿每个分支使用 $P(t) = \\exp(Q r_k t)$。\n\n4) 用于比较的似然评估。\n- 在正确的拓扑上，使用 JC$69$ 模型（无伽马），并使用与模拟相同的枝长向量 $\\mathbf{\\ell}$，评估对数似然。将此值表示为 $\\mathcal{L}_{\\text{JC},\\text{correct}}$。\n- 在错误的拓扑上，使用 GTR+$\\Gamma$ 模型，采用真实的 $\\boldsymbol{\\pi}$、$\\{r_{ij}\\}$、以及相同的 $\\alpha$ 和 $K=4$，并使用相同的枝长向量 $\\mathbf{\\ell}$，评估对数似然。将此值表示为 $\\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}}$。\n- 使用 Felsenstein 剪枝算法，并在内部节点进行数值稳定的缩放。对于伽马混合模型，通过对各类别进行平均来组合每个类别的位点似然。\n\n5) 输出和决策规则。\n- 对于每个测试用例，计算差值 $\\Delta = \\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}} - \\mathcal{L}_{\\text{JC},\\text{correct}}$，然后通过测试 $\\Delta > 0$ 将其转换为布尔值答案。\n- 程序应输出一行，其中包含一个布尔值列表，每个测试用例一个，按下方给出的顺序排列，格式为 Python 风格的列表，例如，“[True,False,True]”。\n\n6) 测试套件。\n使用以下三个测试用例，每个用例都由一个随机种子、比对长度 $L$、伽马形状参数 $\\alpha$ 和枝长向量 $\\mathbf{\\ell} = [\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_{RL},\\ell_{RR}]$ 完全指定：\n- 案例 $1$（短内部枝，强速率变异）：种子 $= 7$，$L = 500$，$\\alpha = 0.3$，$\\mathbf{\\ell} = [0.3, 0.3, 0.3, 0.3, 0.01, 0.01]$。\n- 案例 $2$（中等内部枝，强速率变异）：种子 $= 13$，$L = 1500$，$\\alpha = 0.3$，$\\mathbf{\\ell} = [0.3, 0.3, 0.3, 0.3, 0.2, 0.2]$。\n- 案例 $3$（异质叶分支长度，中等速率变异）：种子 $= 23$，$L = 2000$，$\\alpha = 1.5$，$\\mathbf{\\ell} = [0.5, 0.2, 0.5, 0.2, 0.4, 0.4]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的 Python 风格的布尔值列表，顺序与三个测试用例一致，例如，“[True,False,True]”。", "solution": "所提出的问题是对分子系统发育学中模型错误指定现象的有效探究，这是计算生物学中的一个核心重要主题。它探讨了一个复杂替换模型 (GTR+Γ) 在不正确的树拓扑上是否能比一个更简单的模型 (JC69) 在正确的拓扑上产生更高的数据似然。这是一个定义明确、有科学依据的问题，并且已经为计算实验提供了所有必要的参数和程序。因此，我们将着手进行正式的求解。\n\n该解决方案包括三个主要部分：定义系统发育模型和树结构，在指定情景下模拟序列数据，以及在两个竞争性假设下计算该数据的对数似然。\n\n**1. 系统发育模型和连续时间马尔可夫链**\n\n系统发育树上单个位点沿分支的核苷酸演化被建模为连续时间马尔可夫链 (CTMC)。该过程由一个 $4 \\times 4$ 的瞬时速率矩阵 $Q$ 定义，其中元素 $Q_{ij}$ (对于 $i \\neq j$) 代表从核苷酸 $i$ 替换为核苷酸 $j$ 的速率。对角线元素定义为 $Q_{ii} = -\\sum_{j\\neq i} Q_{ij}$。状态对应于四种核苷酸，我们将其索引为 $\\{A \\to 0, C \\to 1, G \\to 2, T \\to 3\\}$。\n\n在长度（持续时间）为 $t$ 的分支上，从状态 $i$ 变为状态 $j$ 的概率由矩阵指数 $P(t) = \\exp(Qt)$ 给出，其中 $P(t)_{ij}$ 是所需的转移概率。所有模型都假定为时间可逆的，这意味着对所有 $i,j$ 都有 $\\pi_i Q_{ij} = \\pi_j Q_{ji}$，其中 $\\boldsymbol{\\pi} = [\\pi_A, \\pi_C, \\pi_G, \\pi_T]$ 是该过程的平稳分布。\n\n**1.1. Jukes-Cantor 1969 (JC69) 模型**\n\n这是最简单的替换模型。它假设碱基频率相等，替换速率也相等。\n- 平稳分布是均匀的：$\\boldsymbol{\\pi}_{JC} = [\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}]$。\n- 速率矩阵的结构使得所有非对角线元素都相等。为确保每个位点每单位时间的预期替换数为 $1$，该矩阵被缩放为：\n$$\nQ_{JC} = \\begin{pmatrix} -1   1/3  1/3  1/3 \\\\ 1/3  -1   1/3  1/3 \\\\ 1/3  1/3  -1   1/3 \\\\ 1/3  1/3  1/3  -1 \\end{pmatrix}\n$$\n\n**1.2. 通用时间可逆 (GTR) 模型**\n\nGTR 模型是普适性最强的时间可逆模型。它由以下部分定义：\n- 一个平稳分布向量 $\\boldsymbol{\\pi} = [\\pi_A, \\pi_C, \\pi_G, \\pi_T]$。\n- 一个对称的可交换性速率矩阵 $R$，其中 $r_{ij} = r_{ji}$。\n未缩放的 GTR 速率矩阵 $Q'$ 的非对角线元素为 $Q'_{ij} = r_{ij}\\pi_j$。问题提供了 $\\boldsymbol{\\pi} = [0.36, 0.14, 0.24, 0.26]$ 和以下可交换性参数：$r_{AC}=0.7$, $r_{AG}=2.0$, $r_{AT}=0.3$, $r_{CG}=0.5$, $r_{CT}=1.5$, $r_{GT}=0.7$。\n矩阵 $Q$ 经过缩放，使得平均替换速率为 $1$：\n$$\n-\\sum_{i=0}^{3} \\pi_i Q_{ii} = 1\n$$\n这通过计算缩放因子 $\\beta = 1 / (-\\sum_i \\pi_i Q'_{ii})$ 并设置 $Q = \\beta Q'$ 来实现。\n\n**1.3. 位点间速率异质性 ($\\Gamma$ 模型)**\n\n为了解释比对序列中不同位点的不同演化速率，我们使用离散伽马分布。速率 $r$ 从一个均值为 $1$ 的 $\\mathrm{Gamma}(\\alpha, \\text{scale}=1/\\alpha)$ 分布中抽取。这个连续分布由 $K=4$ 个离散速率类别来近似。每个类别的速率 $r_k$ 由伽马分布的分位数中点确定。具体来说，对于 $k \\in \\{1, 2, 3, 4\\}$，我们找到累积概率为 $(2k-1)/(2K)$ 处的值 $r'_k$。然后对这些速率进行重新缩放，以确保其均值为 $1$：$r_k = r'_k / (\\frac{1}{K}\\sum_j r'_j)$。一个位点的似然是针对 $K$ 个速率类别计算的似然的平均值。\n\n**2. 序列数据的模拟**\n\n序列数据（比对序列）在“正确”的树拓扑 $((A,B),(C,D))$ 上，使用 GTR+$\\Gamma$ 模型进行模拟。这个有根二叉树有一个根节点、两个内部节点和四个标记为 $A, B, C, D$ 的叶（末端）节点。枝长由向量 $\\mathbf{\\ell} = [\\ell_A,\\ell_B,\\ell_C,\\ell_D,\\ell_{RL},\\ell_{RR}]$ 给出。\n\n对于比对序列中的 $L$ 个位点中的每一个：\n1.  从 $\\{1, 2, 3, 4\\}$ 中均匀随机选择一个速率类别 $k$。令相应的速率为 $r_k$。\n2.  从平稳分布 $\\boldsymbol{\\pi}$ 中为根节点抽取一个核苷酸状态。\n3.  该状态沿树向下传播。对于每个长度为 $t$ 的分支，子节点的状态基于转移概率 $P(t \\cdot r_k) = \\exp(Q_{GTR} \\cdot t \\cdot r_k)$ 进行抽取，条件是父节点的状态。\n4.  该过程持续进行，直到确定所有四个叶节点的状态，形成比对序列的一列。\n这个过程重复 $L$ 次以生成完整的比对序列。\n\n**3. 通过 Felsenstein 剪枝算法计算似然**\n\n模拟比对序列的对数似然在两种情景下进行计算。其核心是 Felsenstein 剪枝算法，该算法能高效地计算给定树上某个位点数据的似然。\n\n令 $\\mathbf{D}_s$ 为位点 $s$ 的数据（四个叶节点的核苷酸）。该算法为树中的每个节点 $u$ 计算条件似然向量 $\\mathbf{L}_u^{(s)}$。这是一个包含四个元素的向量，其中第 $i$ 个元素 $L_{u,i}^{(s)}$ 是在节点 $u$ 处于状态 $i$ 的条件下，观察到从 $u$ 向下延伸的子树数据的概率。\n\n-   **在叶节点处：** 对于一个观测到核苷酸状态为 $j$ 的叶节点 $u$，条件似然向量是一个“独热”向量：$L_{u,i}^{(s)} = \\delta_{ij}$，其中 $\\delta_{ij}$ 是克罗内克 δ。\n-   **在内部节点处：** 对于一个有子节点 $v$ 和 $w$ 的内部节点 $u$，它们通过长度为 $t_v$ 和 $t_w$ 的分支相连，递归公式为：\n    $$\n    L_{u,i}^{(s)} = \\left( \\sum_{j=0}^3 P(t_v)_{ij} L_{v,j}^{(s)} \\right) \\left( \\sum_{k=0}^3 P(t_w)_{ik} L_{w,k}^{(s)} \\right)\n    $$\n    这是一个从叶节点到根节点的后序遍历。\n\n给定一个具有平稳分布 $\\boldsymbol{\\pi}$ 的模型，位点 $s$ 的总似然在根节点（节点 $R$）处计算：\n$$\n\\mathcal{L}_s = P(\\mathbf{D}_s | \\text{Tree, Model}) = \\sum_{i=0}^3 \\pi_i L_{R,i}^{(s)}\n$$\n为防止长比对序列出现数值下溢，条件似然向量在每个内部节点处进行重新缩放。缩放因子的对数被累加，并加到最终的对数似然中。\n\n整个比对序列的总对数似然是各个位点对数似然的总和：\n$$\n\\mathcal{L} = \\sum_{s=1}^L \\log(\\mathcal{L}_s)\n$$\n\n对于 GTR+$\\Gamma$ 模型，一个位点的似然是 $K$ 个速率类别的平均值：\n$$\n\\mathcal{L}_s = \\frac{1}{K} \\sum_{k=1}^K P(\\mathbf{D}_s | \\text{Tree, Model with rate } r_k)\n$$\n\n**4. 比较和决策**\n\n我们对步骤 2 中模拟的比对序列进行两次似然评估：\n1.  $\\mathcal{L}_{\\text{JC},\\text{correct}}$：在正确拓扑 $((A,B),(C,D))$ 上，使用 JC$69$ 模型（无伽马速率，即 $K=1, r_1=1$）的对数似然。\n2.  $\\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}}$：在错误拓扑 $((A,C),(B,D))$ 上，使用 GTR+$\\Gamma$ 模型（具有真实的模拟参数）的对数似然。枝长向量 $\\mathbf{\\ell}$ 通过将叶分支长度映射到相应的叶节点，并将内部枝长映射到两个新的内部连接分支来重用。\n\n最终输出是一个布尔值，指示 $\\mathcal{L}_{\\text{GTR}+\\Gamma,\\text{wrong}} > \\mathcal{L}_{\\text{JC},\\text{correct}}$ 是否成立。当一个更复杂的模型 (GTR+$\\Gamma$) 在不正确的拓扑上能比一个更简单、灵活性较低的模型 (JC$69$) 即使在正确的拓扑上也能更好地拟合数据时，这种现象就会发生。这突显了系统发育模型选择中的一个潜在问题，即模型错误指定可能会误导推断。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\nfrom scipy.stats import gamma\nimport collections\n\n# Define nucleotide and taxa mapping\nNUCS = ['A', 'C', 'G', 'T']\nNUC_MAP = {n: i for i, n in enumerate(NUCS)}\nTAXA = ['A', 'B', 'C', 'D']\nTAXA_MAP = {t: i for i, t in enumerate(TAXA)}\n\ndef get_gtr_q_matrix(pi, r_params):\n    \"\"\"Constructs and scales the GTR rate matrix.\"\"\"\n    q_unscaled = np.zeros((4, 4))\n    r_mat = np.zeros((4, 4))\n    r_mat[0, 1] = r_mat[1, 0] = r_params['ac']\n    r_mat[0, 2] = r_mat[2, 0] = r_params['ag']\n    r_mat[0, 3] = r_mat[3, 0] = r_params['at']\n    r_mat[1, 2] = r_mat[2, 1] = r_params['cg']\n    r_mat[1, 3] = r_mat[3, 1] = r_params['ct']\n    r_mat[2, 3] = r_mat[3, 2] = r_params['gt']\n\n    for i in range(4):\n        for j in range(4):\n            if i != j:\n                q_unscaled[i, j] = r_mat[i, j] * pi[j]\n    \n    for i in range(4):\n        q_unscaled[i, i] = -np.sum(q_unscaled[i, :])\n\n    # Scale the matrix so the average rate is 1\n    mean_rate = -np.sum(pi * np.diag(q_unscaled))\n    if mean_rate == 0:\n        return q_unscaled \n    return q_unscaled / mean_rate\n\ndef get_jc69_q_matrix():\n    \"\"\"Constructs the JC69 rate matrix.\"\"\"\n    q = np.full((4, 4), 1/3)\n    np.fill_diagonal(q, -1)\n    return q\n\ndef get_gamma_rates(alpha, K):\n    \"\"\"Computes K discrete gamma rates with mean 1.\"\"\"\n    if alpha == np.inf: # Case of no rate variation\n        return np.ones(K)\n    \n    # Use mid-quantile approximation\n    probs = (np.arange(K) + 0.5) / K\n    rates = gamma.ppf(probs, a=alpha, scale=1/alpha)\n    \n    # Rescale to have a mean of 1\n    rates /= np.mean(rates)\n    return rates\n\ndef build_tree_from_case(case, topology_type):\n    \"\"\"Builds a tree dictionary based on topology and branch lengths.\"\"\"\n    l_A, l_B, l_C, l_D, l_RL, l_RR = case['l']\n    \n    leaf_A = {'name': 'A', 'bl': l_A, 'children': []}\n    leaf_B = {'name': 'B', 'bl': l_B, 'children': []}\n    leaf_C = {'name': 'C', 'bl': l_C, 'children': []}\n    leaf_D = {'name': 'D', 'bl': l_D, 'children': []}\n\n    if topology_type == 'correct':\n        # Topology: ((A,B),(C,D))\n        internal_1 = {'name': 'N1', 'bl': l_RL, 'children': [leaf_A, leaf_B]}\n        internal_2 = {'name': 'N2', 'bl': l_RR, 'children': [leaf_C, leaf_D]}\n        root = {'name': 'root', 'bl': 0.0, 'children': [internal_1, internal_2]}\n    elif topology_type == 'wrong':\n        # Topology: ((A,C),(B,D))\n        internal_1 = {'name': 'N1', 'bl': l_RL, 'children': [leaf_A, leaf_C]}\n        internal_2 = {'name': 'N2', 'bl': l_RR, 'children': [leaf_B, leaf_D]}\n        root = {'name': 'root', 'bl': 0.0, 'children': [internal_1, internal_2]}\n    else:\n        raise ValueError(\"Unknown topology type\")\n        \n    return root\n\ndef simulate_alignment(tree, q_matrix, pi, gamma_rates, L, rng):\n    \"\"\"Simulates a sequence alignment on the given tree.\"\"\"\n    K = len(gamma_rates)\n    alignment = np.zeros((L, 4), dtype=int)\n    \n    # Pre-calculate transition matrices for each branch and rate category\n    memo_p = {}\n    \n    for site_idx in range(L):\n        rate_idx = rng.choice(K)\n        rate = gamma_rates[rate_idx]\n        \n        # Simulate one column of the alignment top-down\n        q = collections.deque()\n        root_state = rng.choice(4, p=pi)\n        q.append((tree, root_state))\n        leaf_states = {}\n        \n        while q:\n            node, parent_state = q.popleft()\n            for child in node['children']:\n                bl = child['bl']\n                # Memoize P(t) computation\n                key = (bl, rate_idx)\n                if key not in memo_p:\n                    memo_p[key] = expm(q_matrix * bl * rate)\n                P = memo_p[key]\n                \n                child_state = rng.choice(4, p=P[parent_state, :])\n                if not child['children']:  # Leaf node\n                    leaf_states[child['name']] = child_state\n                else:  # Internal node\n                    q.append((child, child_state))\n    \n        for i, taxon_name in enumerate(TAXA):\n            alignment[site_idx, i] = leaf_states[taxon_name]\n            \n    return alignment\n\ndef calculate_log_likelihood(alignment, tree, q_matrix, pi, gamma_rates):\n    \"\"\"Calculates the log-likelihood of an alignment given a tree and model.\"\"\"\n    L = alignment.shape[0]\n    K = len(gamma_rates)\n    total_log_likelihood = 0.0\n    \n    # Memoize P(t) matrices\n    memo_p_lik = {}\n    \n    for site_idx in range(L):\n        site_data = {taxon: alignment[site_idx, tax_idx] for tax_idx, taxon in enumerate(TAXA)}\n        \n        per_category_likelihoods = np.zeros(K)\n        for k in range(K):\n            rate = gamma_rates[k]\n            \n            # Recursive computation of conditional likelihoods\n            log_scaler_sum, root_likelihood_vec = _pruning_recursion(tree, site_data, q_matrix, rate, memo_p_lik)\n            \n            site_lik_for_category = np.dot(pi, root_likelihood_vec)\n            \n            # Add log of site likelihood for this category\n            if site_lik_for_category > 0:\n                log_lik_for_category = np.log(site_lik_for_category) + log_scaler_sum\n                per_category_likelihoods[k] = np.exp(log_lik_for_category)\n\n        # Average likelihoods over categories\n        site_avg_likelihood = np.mean(per_category_likelihoods)\n        \n        if site_avg_likelihood > 0:\n            total_log_likelihood += np.log(site_avg_likelihood)\n            \n    return total_log_likelihood\n\ndef _pruning_recursion(node, site_data, q_matrix, rate, memo):\n    \"\"\"Recursive helper for Felsenstein's pruning algorithm.\"\"\"\n    if not node['children']: # Leaf node\n        likelihood_vec = np.zeros(4)\n        likelihood_vec[site_data[node['name']]] = 1.0\n        return 0.0, likelihood_vec\n\n    child_results = []\n    total_log_scaler = 0.0\n    \n    for child in node['children']:\n        log_scaler, child_likelihood_vec = _pruning_recursion(child, site_data, q_matrix, rate, memo)\n        total_log_scaler += log_scaler\n        \n        bl = child['bl']\n        key = (bl, rate)\n        if key not in memo:\n            memo[key] = expm(q_matrix * bl * rate)\n        P = memo[key]\n        \n        transformed_vec = P @ child_likelihood_vec\n        child_results.append(transformed_vec)\n    \n    # Element-wise product of children's likelihood vectors\n    node_likelihood_vec = child_results[0]\n    for i in range(1, len(child_results)):\n        node_likelihood_vec *= child_results[i]\n        \n    # Numerical scaling\n    scaler = np.sum(node_likelihood_vec)\n    if scaler > 1e-300: # Avoid log(0)\n        node_likelihood_vec /= scaler\n        total_log_scaler += np.log(scaler)\n        \n    return total_log_scaler, node_likelihood_vec\n\n\ndef solve():\n    test_cases = [\n        {'seed': 7, 'L': 500, 'alpha': 0.3, 'l': [0.3, 0.3, 0.3, 0.3, 0.01, 0.01]},\n        {'seed': 13, 'L': 1500, 'alpha': 0.3, 'l': [0.3, 0.3, 0.3, 0.3, 0.2, 0.2]},\n        {'seed': 23, 'L': 2000, 'alpha': 1.5, 'l': [0.5, 0.2, 0.5, 0.2, 0.4, 0.4]},\n    ]\n    \n    gtr_pi = np.array([0.36, 0.14, 0.24, 0.26])\n    gtr_r = {'ac': 0.7, 'ag': 2.0, 'at': 0.3, 'cg': 0.5, 'ct': 1.5, 'gt': 0.7}\n    \n    results = []\n    \n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        \n        # --- 1. Simulation Setup ---\n        # GTR+Gamma model for simulation\n        q_gtr = get_gtr_q_matrix(gtr_pi, gtr_r)\n        gamma_rates_sim = get_gamma_rates(case['alpha'], K=4)\n        correct_tree = build_tree_from_case(case, 'correct')\n        \n        # Simulate alignment on CORRECT topology with GTR+Gamma\n        alignment = simulate_alignment(correct_tree, q_gtr, gtr_pi, gamma_rates_sim, case['L'], rng)\n        \n        # --- 2. Likelihood Calculation ---\n        \n        # Likelihood on CORRECT topology with JC69 model\n        q_jc69 = get_jc69_q_matrix()\n        pi_jc69 = np.array([0.25, 0.25, 0.25, 0.25])\n        # JC69 has no rate variation, so K=1, rate=1.0 is equivalent\n        gamma_rates_jc = np.array([1.0])\n        \n        log_lik_jc_correct = calculate_log_likelihood(alignment, correct_tree, q_jc69, pi_jc69, gamma_rates_jc)\n\n        # Likelihood on WRONG topology with GTR+Gamma model\n        wrong_tree = build_tree_from_case(case, 'wrong')\n        gamma_rates_gtr = get_gamma_rates(case['alpha'], K=4) # Use case alpha\n        \n        log_lik_gtr_wrong = calculate_log_likelihood(alignment, wrong_tree, q_gtr, gtr_pi, gamma_rates_gtr)\n        \n        # --- 3. Comparison ---\n        delta = log_lik_gtr_wrong - log_lik_jc_correct\n        results.append(delta > 0)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406821"}]}