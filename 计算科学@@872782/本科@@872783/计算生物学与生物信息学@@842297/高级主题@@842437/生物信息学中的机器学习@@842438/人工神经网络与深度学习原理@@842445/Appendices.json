{"hands_on_practices": [{"introduction": "卷积神经网络（CNNs）在识别DNA或RNA等序列中的模式方面表现出色。其核心是卷积滤波器，它本质上是一个可训练的模式检测器。通过亲手构建一个理想的“匹配滤波器”来识别原核生物中一个关键的生物信号——夏因-达尔加诺序列，这个练习将揭示CNNs如何学习检测特定基序的奥秘。这项实践将为你提供一个基础直觉，让你理解神经网络在自动学习过程中究竟在做什么 [@problem_id:2373361]。", "problem": "一个原核生物的信使核糖核酸序列被编码为一个独热张量，作为卷积神经网络（CNN）的输入，其字母表为 $\\{A,C,G,U\\}$。在每个核苷酸位置 $i$，编码是一个列向量 $x_i \\in \\{0,1\\}^{4}$，通道顺序为 $(A,C,G,U)$，其中恰好有一个条目等于 $1$，其余条目等于 $0$。考虑一个一维卷积滤波器，其核大小为 $k=6$，输入通道为 $4$，单个输出通道，权重矩阵为 $W \\in \\mathbb{R}^{4 \\times 6}$，标量偏置为 $b \\in \\mathbb{R}$。当应用于长度为 $6$ 的窗口时，该滤波器计算线性得分\n$$\ns \\;=\\; \\sum_{i=0}^{5} w_i^{\\top} x_i \\;+\\; b,\n$$\n其中 $w_i \\in \\mathbb{R}^{4}$ 表示 $W$ 的第 $i$ 列。\n\n设待检测的夏因-达尔加诺基序为精确的信使核糖核酸序列 AGGAGG。设计 $W$ 和 $b$，使得对于任何长度为 $6$ 的窗口，得分满足 $s=6$ 当且仅当该窗口等于 AGGAGG（按此顺序），否则 $s  6$。通过按列展平 $W$（顺序为 $[A_0, C_0, G_0, U_0, A_1, C_1, G_1, U_1, \\dots, A_5, C_5, G_5, U_5]$），然后将 $b$ 作为最后一个条目附加，以单个行向量的形式提供你的答案，得到一个长度为 $25$ 的向量。将所有条目表示为精确整数。无需四舍五入。", "solution": "问题陈述已经过验证，被认为是有效的。它具有科学依据、问题明确且客观。它提出了一个为特定生物信息学应用设计卷积神经网络组件的清晰任务。\n\n任务是设计一个一维卷积滤波器，由其权重矩阵 $W \\in \\mathbb{R}^{4 \\times 6}$ 和标量偏置 $b \\in \\mathbb{R}$ 定义，该滤波器可作为信使核糖核酸（mRNA）序列 `AGGAGG` 的完美检测器。如果长度为 $6$ 的输入窗口恰好是 `AGGAGG`，则滤波器的得分 $s$ 必须等于 $6$，而对于任何其他序列，得分必须严格小于 $6$。\n\n在每个位置 $i$ 的输入是一个独热编码向量 $x_i \\in \\{0, 1\\}^4$，对应于字母表 $\\{A, C, G, U\\}$ 中的四个核苷酸之一，通道顺序为 $(A, C, G, U)$。对于给定的核苷酸，其向量表示中只有对应的条目为 $1$，而所有其他条目都为 $0$。设位置 $i$ 的权重向量为 $W$ 的第 $i$ 列，记为 $w_i \\in \\mathbb{R}^4$。$w_i$ 的分量为 $(w_{i,A}, w_{i,C}, w_{i,G}, w_{i,U})^{\\top}$。对于窗口 $(x_0, x_1, \\dots, x_5)$，滤波器的得分由以下公式给出：\n$$s = \\sum_{i=0}^{5} w_i^{\\top} x_i + b$$\n由于是独热编码，点积 $w_i^{\\top} x_i$ 的作用是从向量 $w_i$ 中精确选择一个权重。如果位置 $i$ 的核苷酸是 $N_i$，那么 $w_i^{\\top} x_i = w_{i, N_i}$。因此，得分可以重写为：\n$$s = \\sum_{i=0}^{5} w_{i, N_i} + b$$\n其中 $N_i$ 是输入窗口中位置 $i$ 的核苷酸。\n\n目标序列是 $S^* = \\text{AGGAGG}$。相应的核苷酸序列是 $(N^*_0, N^*_1, N^*_2, N^*_3, N^*_4, N^*_5) = (A, G, G, A, G, G)$。\n我们的目标是创建一个匹配滤波器。对于每个位置 $i$，如果核苷酸 $N_i$ 与目标 $N^*_i$ 匹配，滤波器应分配最高可能的分数贡献，否则分配较低的贡献。为了最大化目标序列的得分并惩罚任何偏差，我们将为每个位置的匹配分配权重 $1$，为不匹配分配一个较小的值。我们将采用最直接的设计，即将不匹配的权重设置为 $0$。\n\n形式上，对于每个位置 $i \\in \\{0, 1, ..., 5\\}$，我们为每个核苷酸 $N \\in \\{A, C, G, U\\}$ 定义权重 $w_{i,N}$ 如下：\n$$w_{i,N} = \\begin{cases} 1  \\text{if } N = N^*_i \\\\ 0  \\text{if } N \\neq N^*_i \\end{cases}$$\n\n将此规则应用于目标序列 `AGGAGG`：\n-   对于 $i=0$， $N^*_0 = A$。因此，$w_{0,A}=1$ 且 $w_{0,C}=w_{0,G}=w_{0,U}=0$。于是，$w_0 = [1, 0, 0, 0]^{\\top}$。\n-   对于 $i=1$， $N^*_1 = G$。因此，$w_{1,G}=1$ 且 $w_{1,A}=w_{1,C}=w_{1,U}=0$。于是，$w_1 = [0, 0, 1, 0]^{\\top}$。\n-   对于 $i=2$， $N^*_2 = G$。因此，$w_{2,G}=1$ 且 $w_{2,A}=w_{2,C}=w_{2,U}=0$。于是，$w_2 = [0, 0, 1, 0]^{\\top}$。\n-   对于 $i=3$， $N^*_3 = A$。因此，$w_{3,A}=1$ 且 $w_{3,C}=w_{3,G}=w_{3,U}=0$。于是，$w_3 = [1, 0, 0, 0]^{\\top}$。\n-   对于 $i=4$， $N^*_4 = G$。因此，$w_{4,G}=1$ 且 $w_{4,A}=w_{4,C}=w_{4,U}=0$。于是，$w_4 = [0, 0, 1, 0]^{\\top}$。\n-   对于 $i=5$， $N^*_5 = G$。因此，$w_{5,G}=1$ 且 $w_{5,A}=w_{5,C}=w_{5,U}=0$。于是，$w_5 = [0, 0, 1, 0]^{\\top}$。\n\n现在我们确定偏置 $b$。第一个条件指出，如果输入是目标序列 `AGGAGG`，得分必须为 $s=6$。对于此序列，每个核苷酸 $N_i$ 都等于目标核苷酸 $N^*_i$。因此，每个位置的得分贡献为 $w_{i,N^*_i} = 1$。\n总得分 $s^*$ 为：\n$$s^* = \\sum_{i=0}^{5} w_{i,N^*_i} + b = (1+1+1+1+1+1) + b = 6 + b$$\n为了满足条件 $s^*=6$，我们必须有 $6+b=6$，这意味着 $b=0$。\n\n现在我们必须验证第二个条件：对于除 `AGGAGG` 之外的任何输入序列，得分必须严格小于 $6$。\n设任意输入序列为 $(N_0, N_1, \\dots, N_5)$。得分为 $s = \\sum_{i=0}^5 w_{i,N_i} + 0$。\n每个位置的贡献 $w_{i,N_i}$ 在 $N_i = N^*_i$（匹配）时为 $1$，在 $N_i \\neq N^*_i$（不匹配）时为 $0$。\n因此，总得分 $s$ 等于输入序列与目标序列 `AGGAGG` 匹配的位置数。设匹配数为 $m$。则 $s=m$。\n如果输入序列不是 `AGGAGG`，则必须至少有一个不匹配。因此，匹配数 $m$ 必须小于 $6$，即 $m \\in \\{0, 1, 2, 3, 4, 5\\}$。\n这意味着得分为 $s = m \\le 5$，满足条件 $s  6$。因此，该设计是正确的。\n\n权重矩阵 $W$ 由这些列向量构成：\n$$W = \\begin{pmatrix} w_0  w_1  w_2  w_3  w_4  w_5 \\end{pmatrix} = \\begin{pmatrix} 1  0  0  1  0  0 \\\\ 0  0  0  0  0  0 \\\\ 0  1  1  0  1  1 \\\\ 0  0  0  0  0  0 \\end{pmatrix}$$\n偏置为 $b=0$。\n\n最终答案必须是通过按列展平 $W$ 并附加 $b$ 得到的单个行向量。\n从 $W$ 展平得到的向量是 $[w_{0,A}, w_{0,C}, w_{0,G}, w_{0,U}, w_{1,A}, \\dots, w_{5,U}]$。\n这对应于连接列向量 $w_0, w_1, \\dots, w_5$ 的转置。\n展平的 $W$：$[1, 0, 0, 0, \\quad 0, 0, 1, 0, \\quad 0, 0, 1, 0, \\quad 1, 0, 0, 0, \\quad 0, 0, 1, 0, \\quad 0, 0, 1, 0]$。\n在末尾附加偏置 $b=0$ 得到最终长度为 $25$ 的向量。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0  0  0  0  0  1  0  0  0  1  0  1  0  0  0  0  0  1  0  0  0  1  0  0\n\\end{pmatrix}\n}\n$$", "id": "2373361"}, {"introduction": "训练深度学习模型需要一个损失函数来评估模型的表现。对于复杂的生物学任务，标准的损失函数（如交叉熵）往往不足够。我们可以设计包含领域特定知识和惩罚项的自定义损失函数。在这个问题中，你将为一个DNA到蛋白质的翻译任务计算一个独特的损失函数，它不仅衡量了翻译的准确性，还对可能导致严重后果的移码错误施加了惩罚。这项练习将教会你如何引导模型的学习过程以遵循关键的生物学约束 [@problem_id:2373364]。", "problem": "一个序列到序列 (Seq2Seq) 模型被训练用于在一个从第一个核苷酸开始的固定阅读框下，将一个脱氧核糖核酸 (DNA) 编码区翻译成其氨基酸序列。对于一个训练样本，其 DNA 编码序列的长度为 $6$ 个核苷酸（两个密码子），真实氨基酸序列的长度为 $2$，标记为 $y_{1} = \\mathrm{M}$ 和 $y_{2} = \\mathrm{K}$，其中 $\\mathrm{M}$ 和 $\\mathrm{K}$ 分别表示氨基酸甲硫氨酸和赖氨酸。\n\n在每个解码步骤 $t \\in \\{1,2\\}$，模型输出：\n- 一个在氨基酸词汇表 $\\mathcal{A} = \\{\\mathrm{M}, \\mathrm{K}, \\mathrm{L}\\}$ 上的分类分布 $p_{t}$。\n- 一个关于移码偏移量随机变量 $s_{t} \\in \\{-1, 0, +1\\}$ 的分类分布 $r_{t}$，它模拟了在生成第 $t$ 个氨基酸之前发生的单核苷酸缺失（$-1$）、无移码（$0$）或单核苷酸插入（$+1$）。\n\n将单个样本的标量损失定义为：从 $t=1$ 到 $t=2$ 的各项之和，每一项是 $p_{t}$ 赋予正确氨基酸的概率的负自然对数，再加上一个移码惩罚，该惩罚等于一个非负系数 $\\lambda$ 乘以在 $r_{t}$ 分布下的期望绝对移码幅度。自然对数用 $\\ln$ 表示。\n\n对于此样本，模型的输出是：\n- $p_{1}(\\mathrm{M}) = 0.7$, $p_{1}(\\mathrm{K}) = 0.2$, $p_{1}(\\mathrm{L}) = 0.1$;\n- $p_{2}(\\mathrm{M}) = 0.1$, $p_{2}(\\mathrm{K}) = 0.6$, $p_{2}(\\mathrm{L}) = 0.3$;\n- $r_{1}(-1) = 0.05$, $r_{1}(0) = 0.9$, $r_{1}(+1) = 0.05$;\n- $r_{2}(-1) = 0.2$, $r_{2}(0) = 0.7$, $r_{2}(+1) = 0.1$.\n\n设 $\\lambda = 2$。使用上述定义，计算此训练样本的总损失。使用自然对数 $\\ln$，并将最终答案四舍五入到四位有效数字。", "solution": "该问题陈述在科学上是合理的、定义明确、客观，并包含了得出唯一且有意义解所需的所有信息。因此，该问题是有效的。\n\n训练样本的总损失（记为 $L$）定义为两个解码步骤（$t=1$ 和 $t=2$）的损失之和。每个步骤 $t$ 的损失（记为 $L_t$）由两部分组成：用于氨基酸预测的交叉熵项和用于移码事件的惩罚项。\n\n总损失 $L$ 的通用公式如下：\n$$L = \\sum_{t=1}^{2} L_t = \\sum_{t=1}^{2} \\left( -\\ln(p_t(y_t)) + \\lambda \\mathbb{E}_{s_t \\sim r_t}[|s_t|] \\right)$$\n其中 $y_t$ 是步骤 $t$ 的真实氨基酸，$p_t(y_t)$ 是模型赋予正确氨基酸的概率，$\\lambda$ 是移码惩罚系数，而 $\\mathbb{E}_{s_t \\sim r_t}[|s_t|]$ 是在分布 $r_t$ 下的期望绝对移码幅度。移码偏移量的随机变量是 $s_t$，其取值范围为 $\\{-1, 0, +1\\}$。\n\n首先，我们计算第一个解码步骤 $t=1$ 的损失。\n真实氨基酸是 $y_1 = \\mathrm{M}$。模型对该氨基酸的预测概率是 $p_1(\\mathrm{M}) = 0.7$。\n损失的交叉熵部分是：\n$$-\\ln(p_1(y_1)) = -\\ln(0.7)$$\n接下来，我们计算 $t=1$ 时的期望绝对移码幅度。移码偏移量 $s_1$ 的概率分布为 $r_1(-1) = 0.05$，$r_1(0) = 0.9$ 和 $r_1(+1) = 0.05$。\n期望值计算如下：\n$$\\mathbb{E}[|s_1|] = \\sum_{s \\in \\{-1,0,1\\}} |s| \\cdot r_1(s)$$\n$$\\mathbb{E}[|s_1|] = |-1| \\cdot r_1(-1) + |0| \\cdot r_1(0) + |+1| \\cdot r_1(+1)$$\n$$\\mathbb{E}[|s_1|] = (1)(0.05) + (0)(0.9) + (1)(0.05) = 0.05 + 0 + 0.05 = 0.1$$\n$t=1$ 时的移码惩罚是 $\\lambda \\mathbb{E}[|s_1|]$。给定 $\\lambda = 2$：\n$$\\lambda \\mathbb{E}[|s_1|] = 2 \\times 0.1 = 0.2$$\n所以，步骤 $t=1$ 的总损失是：\n$$L_1 = -\\ln(0.7) + 0.2$$\n\n其次，我们计算第二个解码步骤 $t=2$ 的损失。\n真实氨基酸是 $y_2 = \\mathrm{K}$。模型对此的预测概率是 $p_2(\\mathrm{K}) = 0.6$。\n损失的交叉熵部分是：\n$$-\\ln(p_2(y_2)) = -\\ln(0.6)$$\n接下来，我们计算 $t=2$ 时的期望绝对移码幅度。其分布为 $r_2(-1) = 0.2$，$r_2(0) = 0.7$ 和 $r_2(+1) = 0.1$。\n$$\\mathbb{E}[|s_2|] = \\sum_{s \\in \\{-1,0,1\\}} |s| \\cdot r_2(s)$$\n$$\\mathbb{E}[|s_2|] = |-1| \\cdot r_2(-1) + |0| \\cdot r_2(0) + |+1| \\cdot r_2(+1)$$\n$$\\mathbb{E}[|s_2|] = (1)(0.2) + (0)(0.7) + (1)(0.1) = 0.2 + 0 + 0.1 = 0.3$$\n当 $\\lambda=2$ 时，$t=2$ 时的移码惩罚是：\n$$\\lambda \\mathbb{E}[|s_2|] = 2 \\times 0.3 = 0.6$$\n所以，步骤 $t=2$ 的总损失是：\n$$L_2 = -\\ln(0.6) + 0.6$$\n\n最后，整个样本的总损失 $L$ 是每个步骤损失的和：\n$$L = L_1 + L_2 = (-\\ln(0.7) + 0.2) + (-\\ln(0.6) + 0.6)$$\n$$L = -\\ln(0.7) - \\ln(0.6) + 0.8$$\n使用对数性质 $\\ln(a) + \\ln(b) = \\ln(ab)$：\n$$L = -(\\ln(0.7) + \\ln(0.6)) + 0.8 = -\\ln(0.7 \\times 0.6) + 0.8 = -\\ln(0.42) + 0.8$$\n现在我们计算数值：\n$\\ln(0.7) \\approx -0.35667$\n$\\ln(0.6) \\approx -0.51083$\n$$L \\approx -(-0.35667) - (-0.51083) + 0.8$$\n$$L \\approx 0.35667 + 0.51083 + 0.8 = 0.86750 + 0.8 = 1.66750$$\n或者使用合并后的形式：\n$\\ln(0.42) \\approx -0.86750$\n$$L \\approx -(-0.86750) + 0.8 = 0.86750 + 0.8 = 1.66750$$\n问题要求将最终答案四舍五入到四位有效数字。\n值为 $1.66750$。第五位有效数字是 $5$，因此我们将第四位数字向上取整。\n$$L \\approx 1.668$$", "answer": "$$\\boxed{1.668}$$", "id": "2373364"}, {"introduction": "成功构建一个深度学习模型很少是一个一帆风顺的过程；通常，初步模型的表现并不理想。有效的模型开发需要一套系统且科学的调试方法，通过受控实验来分离和定位失败的根源。这项实践将挑战你像一名机器学习科学家一样思考。通过评估一个为蛋白质功能预测而设计的图神经网络（GNN）的调试流程，你将学会如何区分问题是源于数据（节点特征或图结构）还是模型本身（消息传递机制）[@problem_id:2373344]。", "problem": "你正在一个蛋白质-蛋白质相互作用图上训练一个图神经网络（GNN），用于多标签蛋白质功能预测。数据包含一个图 $G=(V,E)$，其中节点 $v \\in V$ 是蛋白质，边 $e \\in E$ 表示已报道的相互作用；节点特征 $X \\in \\mathbb{R}^{|V| \\times d}$ 来自氨基酸序列；以及针对 $C$ 个基因本体论术语的二进制标签矩阵 $Y \\in \\{0,1\\}^{|V| \\times C}$。模型 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 是一个具有 $L$ 层的消息传递网络，它聚合邻居信息并输出 $\\hat{Y} \\in [0,1]^{|V| \\times C}$。尽管进行了超参数调整，验证集上的 micro-$F_{1}$ 分数仍然很低且不稳定。你必须确定主要的失败模式是源于图结构 $E$、节点特征 $X$ 还是消息传递机制本身。\n\n在上述约束条件下，仅使用 $G$、$X$ 和消息传递组件的受控变量，同时保持训练协议和评估指标不变，以下哪个实验工作流最能可靠地隔离和诊断错误来源？\n\nA. 训练一个忽略 $E$ 的逐节点基线模型 $f_{\\mathrm{MLP}}(X;\\phi)$，并将其验证集 micro-$F_{1}$ 分数与 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 的分数进行比较。然后，执行度保持的边重连以获得 $G'=(V,E')$，并重新训练 $f_{\\mathrm{GNN}}(G',X;\\theta)$；接下来，通过对 $X$ 的行应用随机置换 $\\pi$ 来独立地置换节点间的特征，以获得 $X'=P_{\\pi}X$，并重新训练 $f_{\\mathrm{GNN}}(G,X';\\theta)$。最后，通过将邻居聚合替换为恒等映射或将所有邻居权重设置为零的均值聚合器来消融消息传递，并扫描 $L$ 的值，同时测量验证集 micro-$F_{1}$ 分数的变化。如果 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 的性能不优于在 $G'$ 上的性能，则将主要失败归因于 $E$；如果在 $(G,X')$ 上的性能没有变化，则归因于 $X$；如果 $f_{\\mathrm{MLP}}$ 性能强大，但 GNN 性能随着 $L$ 的增加而下降或对 $L$ 和聚合消融不敏感，则归因于消息传递。\n\nB. 通过增加层数至 $L+2$ 来增加模型容量，添加率为 $p$ 的 dropout，并在 $E$ 中包含自环；根据训练损失选择最佳模型。如果训练损失下降而验证集 micro-$F_{1}$ 分数仍然很低，则断定节点特征 $X$ 是问题所在；如果两者都下降，则断定图 $E$ 是问题所在；否则，断定消息传递是问题所在。\n\nC. 使用 $k$ 折交叉验证来选择学习率和权重衰减，对验证集损失应用早停，并使用 t-分布随机邻居嵌入（t-SNE）可视化最后一层的嵌入。如果可视化显示功能类别之间存在重叠的簇，则断定消息传递不足；如果簇是分开的，则断定图 $E$ 提供了有用的信息；如果没有清晰的结构，则断定特征 $X$ 有问题。\n\nD. 在训练过程中对原始的 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 应用概率为 $p$ 的边 dropout，并记录验证集 micro-$F_{1}$ 分数作为 $p$ 的函数。如果随着 $p$ 增加到 $1$ 性能没有下降，则断定图 $E$ 没有携带有用信息；如果在较小的 $p$ 值时性能急剧下降，则断定消息传递很强。不需要与仅使用特征的基线模型或特征随机化进行比较。\n\n选择一个能提供有原则的、混淆因素最少的诊断工作流的选项，以区分 $E$、$X$ 和消息传递机制中的问题。", "solution": "该问题要求确定最可靠的实验工作流，以诊断图神经网络（GNN）在多标签蛋白质功能预测任务中的失败原因。潜在的失败来源被指定为图结构 $E$、节点特征 $X$ 和 GNN 的消息传递机制。一个可靠的诊断工作流必须采用受控实验来分离每个组件对模型性能的贡献。评估指标是验证集 micro-$F_{1}$ 分数。\n\n科学诊断的核心原则是变量分离。GNN 的预测能力 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 是三个相互交织的组件的函数：每个节点的初始信息 ($X$)、信息传播所依据的关系结构 ($E$)，以及构成消息传递的转换/聚合函数（$f_{\\mathrm{GNN}}$ 的架构）。严谨的诊断程序必须系统地评估每个组件的价值。\n\n让我们分析每个提议的工作流。\n\n**A. 训练一个忽略 $E$ 的逐节点基线模型 $f_{\\mathrm{MLP}}(X;\\phi)$，并将其验证集 micro-$F_{1}$ 分数与 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 的分数进行比较。然后，执行度保持的边重连以获得 $G'=(V,E')$，并重新训练 $f_{\\mathrm{GNN}}(G',X;\\theta)$；接下来，通过对 $X$ 的行应用随机置换 $\\pi$ 来独立地置换节点间的特征，以获得 $X'=P_{\\pi}X$，并重新训练 $f_{\\mathrm{GNN}}(G,X';\\theta)$。最后，通过将邻居聚合替换为恒等映射或将所有邻居权重设置为零的均值聚合器来消融消息传递，并扫描 $L$ 的值，同时测量验证集 micro-$F_{1}$ 分数的变化。如果 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 的性能不优于在 $G'$ 上的性能，则将主要失败归因于 $E$；如果在 $(G,X')$ 上的性能没有变化，则归因于 $X$；如果 $f_{\\mathrm{MLP}}$ 性能强大，但 GNN 性能随着 $L$ 的增加而下降或对 $L$ 和聚合消融不敏感，则归因于消息传递。**\n\n这个工作流在方法论上是合理的，并遵循了受控实验的原则。\n1.  **分离特征贡献 ($X$)**：仅在节点特征上训练一个多层感知机（MLP）$f_{\\mathrm{MLP}}(X;\\phi)$，可以建立一个关键的基线。它衡量了仅包含在 $X$ 中而没有任何图信息的预测能力。如果这个基线已经很低，这强烈表明特征 $X$ 是一个主要问题。\n2.  **分离图结构贡献 ($E$)**：\n    *   $f_{\\mathrm{GNN}}(G,X;\\theta)$ 与 $f_{\\mathrm{MLP}}(X;\\phi)$ 基线之间的比较直接量化了使用图结构 $E$ 的边际效益。如果 GNN 的性能没有显著优于 MLP，那么在 $E$ 上的消息传递没有提供价值。\n    *   度保持的边重连实验创建了一个具有相同节点度的随机图 $G'$。比较 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 与 $f_{\\mathrm{GNN}}(G',X;\\theta)$ 的性能，可以测试蛋白质-蛋白质相互作用网络的*特定*连接性是否重要，或者模型是否仅仅从节点度中学习。如果性能相似，那么 $E$ 中的特定边信息没有被有效利用。\n3.  **分离特征-节点关联**：创建 $X' = P_{\\pi}X$ 的特征置换实验，打破了节点与其特定特征之间的联系。如果 $f_{\\mathrm{GNN}}(G,X';\\theta)$ 的性能与原始模型相似，这意味着模型正在忽略节点特征，这是一个严重的失败。这是在 GNN 框架内检验 $X$ 效用性的一个稳健测试。\n4.  **分离消息传递机制**：消融聚合步骤（例如，使用恒等映射）能有效地将 GNN 变为 MLP。扫描层数 $L$ 的值可以诊断诸如过平滑（性能随 $L$ 增加而下降）或传播不足等问题。这些测试直接探究了消息传递组件的行为。\n\n所提出的诊断逻辑清晰，并与这些受控实验的结果直接相关。这个工作流系统地、独立地评估了每个组件。\n\n**结论**：**正确**。该选项描述了一个全面且有原则的诊断程序。\n\n**B. 通过增加层数至 $L+2$ 来增加模型容量，添加率为 $p$ 的 dropout，并在 $E$ 中包含自环；根据训练损失选择最佳模型。如果训练损失下降而验证集 micro-$F_{1}$ 分数仍然很低，则断定节点特征 $X$ 是问题所在；如果两者都下降，则断定图 $E$ 是问题所在；否则，断定消息传递是问题所在。**\n\n这个工作流存在根本性缺陷。\n1.  **混淆变量**：它建议同时改变多个超参数和架构元素（层数 $L$、dropout 率 $p$ 和 $E$ 中的自环）。这使得无法将观察到的任何性能变化归因于单一原因。它违反了受控实验的核心原则。\n2.  **不正确的模型选择标准**：根据最小*训练损失*来选择模型是不正确的做法。这会鼓励过拟合，并且不能反映模型的泛化能力，泛化能力应由验证指标来衡量。\n3.  **虚假的诊断逻辑**：所提供的推理是武断的，并非基于已建立的机器学习原则。例如，“训练损失下降而验证集 micro-$F_{1}$ 仍然很低”是过拟合的经典定义。过拟合可能由噪声特征、鼓励记忆的噪声图或对于数据而言过于复杂的模型引起。将其完全归因于 $X$ 是一个毫无根据的跳跃。其他规则也同样没有依据。\n\n**结论**：**不正确**。这种方法不科学，很可能导致错误的结论。\n\n**C. 使用 $k$ 折交叉验证来选择学习率和权重衰减，对验证集损失应用早停，并使用 t-分布随机邻居嵌入（t-SNE）可视化最后一层的嵌入。如果可视化显示功能类别之间存在重叠的簇，则断定消息传递不足；如果簇是分开的，则断定图 $E$ 提供了有用的信息；如果没有清晰的结构，则断定特征 $X$ 有问题。**\n\n这个工作流有显著的弱点。\n1.  **冗余**：问题陈述中已说明超参数调整已经执行。建议一个标准的调优协议并不能解决诊断的需求。\n2.  **可视化的主观性**：t-SNE 是一种用于可视化和探索性数据分析的工具，不适用于严谨的定量诊断。其产生的二维投影对其自身的超参数（例如，困惑度）很敏感，并且可以从相同的数据中产生视觉上不同的输出。它不保证能保留真实的高维结构。\n3.  **模棱两可且不可靠的逻辑**：诊断规则基于主观的视觉解释，并且在逻辑上不成立。\n    *   “重叠的簇”是模型整体性能不佳的一个症状；它可能是由于糟糕的特征 $X$、一个带噪声的图 $E$ 或一个有缺陷的模型架构所致。它并不能具体地指向消息传递的问题。\n    *   “分开的簇”表明模型学到了一些可分的表示，但这并没有分离出图 $E$ 的贡献。一个在信息量丰富的特征 $X$ 上训练的简单 MLP 也可以产生分离良好的簇。\n    *   “没有清晰的结构”是一个失败模型的预期视觉结果，但它没有提供关于失败原因的任何信息。\n\n**结论**：**不正确**。这个工作流依赖于主观和不可靠的方法，并且未能分离潜在的错误来源。\n\n**D. 在训练过程中对原始的 $f_{\\mathrm{GNN}}(G,X;\\theta)$ 应用概率为 $p$ 的边 dropout，并记录验证集 micro-$F_{1}$ 分数作为 $p$ 的函数。如果随着 $p$ 增加到 $1$ 性能没有下降，则断定图 $E$ 没有携带有用信息；如果在较小的 $p$ 值时性能急剧下降，则断定消息传递很强。不需要与仅使用特征的基线模型或特征随机化进行比较。**\n\n这个工作流是不完整的。\n1.  **不完整的诊断**：这个实验，被称为模型对边移除的敏感性分析，主要探究图结构 $E$ 的重要性。它没有提供诊断节点特征 $X$ 问题的机制。问题明确要求一个工作流来区分 $E$、$X$ 和消息传递机制中的问题。这个选项未能解决 $X$ 的作用。\n2.  **薄弱的诊断逻辑**：虽然如果性能对边 dropout 不敏感，则断定 $E$ 无用是一个合理的推断，但第二个结论是薄弱的。性能急剧下降只表明模型对图结构*敏感*。这并不一定意味着“消息传递很强”或正在学习有用的信息；模型可能只是在对边中的噪声进行过拟合。\n3.  **缺乏基线**：声明“不需要与仅使用特征的基线模型……进行比较”是一个关键错误。没有 MLP 基线，就无法知道仅由特征提供的性能下限。例如，如果性能很差且随着 $p \\to 1$ 而没有变化，这可能是因为图是无用的，但也可能是因为特征是无用的，所以无论是 GNN 还是等效的 MLP（在 $p \\approx 1$ 时）都学不到任何东西。这种方法无法解开这两种可能性。\n\n**结论**：**不正确**。这是一个不完整的诊断工具，不能满足问题的要求。\n\n总而言之，选项 A 是唯一一个通过受控实验和逻辑推断来分离每个关键组件，从而为诊断模型失败提供了一个严谨、系统和全面计划的选项。", "answer": "$$\\boxed{A}$$", "id": "2373344"}]}