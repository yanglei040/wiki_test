{"hands_on_practices": [{"introduction": "我们的第一个实践是一项奠基石式的练习。我们将从头开始构建一个简单的 VAE，用于分析合成的单细胞 RNA 测序（scRNA-seq）数据。我们的目标是亲眼见证 VAE 如何在其低维潜空间中自动发现并表示一个基本的生物学过程，例如细胞周期。这个练习将帮助你建立关于 VAE 如何实现降维和特征提取的直观认识。[@problem_id:2439780]", "problem": "您将实现一个完整的、可运行的程序，该程序在一个代表主导性细胞周期样因子的合成单细胞RNA测序（scRNA-seq）数据集上训练一个一维变分自编码器（VAE）。然后，您将通过分析两个典型细胞周期标记基因的重构轨迹，证明遍历单一潜在维度对应于可解释的细胞周期变异。\n\n基本基础。仅使用以下基本定义和事实来推导和设计您的解决方案：\n- 变分自编码器（VAE）基于最大化证据下界（ELBO）。对于数据 $x$、潜在变量 $z$、先验 $p(z)$、似然 $p_\\theta(x \\mid z)$ 和变分后验 $q_\\phi(z \\mid x)$，每个样本的ELBO为\n$$\n\\mathcal{L}(\\theta,\\phi;x) \\;=\\; \\mathbb{E}_{q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x \\mid z)\\right] \\;-\\; \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\n其中 $\\mathrm{KL}$ 表示库尔贝克-莱布勒散度。\n- 重参数化技巧将 $z$ 写为 $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\,\\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0,1)$，以实现对 $\\mathbb{E}_{q_\\phi}$ 的基于梯度的优化。\n- 对于高斯先验 $p(z)=\\mathcal{N}(0,1)$ 和高斯变分后验 $q_\\phi(z \\mid x)=\\mathcal{N}(\\mu_\\phi(x), \\sigma_\\phi^2(x))$，每个样本的库尔贝克-莱布勒散度等于\n$$\n\\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) \\;=\\; \\tfrac{1}{2}\\left(\\mu_\\phi(x)^2 + \\sigma_\\phi(x)^2 - \\log \\sigma_\\phi(x)^2 - 1\\right).\n$$\n- 对于高斯解码器 $p_\\theta(x \\mid z) = \\mathcal{N}(f_\\theta(z), \\sigma_x^2 I)$，最大化 $\\mathbb{E}_{q_\\phi}[\\log p_\\theta(x \\mid z)]$ 等价于（相差一个加性常数）最小化期望平方重构误差 $\\tfrac{1}{2\\sigma_x^2}\\,\\mathbb{E}_{q_\\phi}\\!\\left[\\lVert x - f_\\theta(z)\\rVert_2^2\\right]$。\n\n要实现的模型和数据集规范。您必须构建一个具有线性编码器和线性解码器的一维VAE：\n- 先验：$p(z)=\\mathcal{N}(0,1)$。\n- 解码器：$p_\\theta(x\\mid z)=\\mathcal{N}(W z + b,\\; \\sigma_x^2 I)$，其中 $\\sigma_x^2 = 1$ 固定， $W \\in \\mathbb{R}^{D \\times 1}$ 和 $b \\in \\mathbb{R}^{D}$ 是可训练的。\n- 编码器：$q_\\phi(z \\mid x)=\\mathcal{N}(a^\\top x + c,\\; \\exp(\\ell))$，其中 $a \\in \\mathbb{R}^{D}$，$c \\in \\mathbb{R}$ 和 $\\ell \\in \\mathbb{R}$（一个标量对数方差）是可训练的。\n- 训练目标：最小化负ELBO，并在库尔贝克-莱布勒散度项上带一个可选的 $\\beta$ 权重（即 $\\beta$-VAE 目标），即\n$$\n\\mathcal{J}(\\theta,\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\left\\{\\tfrac{1}{2}\\left\\lVert x_i - (W z_i + b)\\right\\rVert_2^2 \\;+\\; \\beta \\cdot \\tfrac{1}{2}\\left(\\mu_i^2 + \\sigma^2 - \\ell - 1\\right)\\right\\},\n$$\n其中 $z_i = \\mu_i + \\sigma \\epsilon_i$，$\\mu_i = a^\\top x_i + c$，$\\sigma = \\exp(\\tfrac{1}{2}\\ell)$，以及 $\\epsilon_i \\sim \\mathcal{N}(0,1)$。\n- 优化：使用随机梯度下降或全批量梯度下降与重参数化技巧。需要优化的参数是 $W$、$b$、$a$、$c$ 和 $\\ell$。\n\n要实现的合成scRNA-seq数据生成器。对于每个测试用例，生成 $N$ 个细胞，包含 $D$ 个基因，遵循一个代表细胞周期分数的主导线性因子：\n- 选择 $D = 5$。令基因索引 $0$ 表示一个典型的S期标记（例如，增殖细胞核抗原），基因索引 $1$ 表示一个典型的G2/M期标记（例如，细胞周期蛋白B1）。为简单起见，我们将它们称为 “PCNA”（索引 $0$）和 “CCNB1”（索引 $1$）。\n- 对 $i = 1,\\dots,N$ 采样潜在分数 $s_i \\sim \\mathcal{N}(0,1)$。\n- 设真实载荷向量为 $u \\in \\mathbb{R}^D$，其中 $u_0 = 1.0$, $u_1 = -0.8$, $u_2 = 0.25$, $u_3 = 0.1$, $u_4 = -0.05$。\n- 设基线向量为 $m \\in \\mathbb{R}^D$，其中 $m_0 = 2.0$, $m_1 = 1.5$, $m_2 = 0.5$, $m_3 = 0.1$, $m_4 = -0.2$。\n- 观测值生成如下\n$$\nx_i \\;=\\; m \\;+\\; u\\, s_i \\;+\\; \\varepsilon_i,\n\\quad\\text{with}\\quad\n\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\mathrm{noise}^2 I_D),\n$$\n其中 $\\sigma_\\mathrm{noise}$ 按每个测试用例指定。这种构造确保了主导变异是增加“PCNA”并减少“CCNB1”的单一因子，从而捕捉到一个细胞周期样的轴。\n\n潜在遍历与对齐准则。训练后，定义一个潜在遍历网格 $z_j$，其中 $j=1,\\dots,K$，$K=41$ 个点在 $-2$ 到 $2$ 之间等距分布。对于每个 $z_j$，解码 $\\hat{x}(z_j) = W z_j + b$。由于潜在轴的方向是任意的，通过翻转 $W$ 的符号（如果内部维护编码器方向，则相应地翻转）来强制使用标准方向，使得重构的“PCNA”随 $z$ 增加。定义细胞周期对齐准则如下：\n- $z$ 与重构的“PCNA”轨迹之间的斯皮尔曼等级相关性的非负，且其绝对值至少为 $\\tau$。\n- $z$ 与重构的“CCNB1”轨迹之间的斯皮尔曼等级相关性的非正，且其绝对值至少为 $\\tau$。\n使用 $\\tau = 0.95$，并使用标准的斯皮尔曼等级相关性定义计算相关性。\n\n测试套件。您的程序必须运行以下三个测试用例，并为每个用例返回一个布尔值，指示对齐准则是否成立：\n- 用例A（理想路径）：$N=300$, $\\sigma_\\mathrm{noise}=0.30$, $\\beta=1.0$, seed $=42$。\n- 用例B（中度噪声，欠正则化）：$N=120$, $\\sigma_\\mathrm{noise}=0.60$, $\\beta=0.5$, seed $=123$。\n- 用例C（小样本，过正则化）：$N=40$, $\\sigma_\\mathrm{noise}=0.80$, $\\beta=4.0$, seed $=2024$。\n\n最终输出格式。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，“[resultA,resultB,resultC]”），其中每个结果是对应于用例A、B和C的布尔字面量 True 或 False。不应打印其他任何文本。", "solution": "问题陈述已经过验证。\n\n### 步骤 1：提取已知条件\n\n**变分自编码器 (VAE) 公式：**\n- 证据下界 (ELBO): $\\mathcal{L}(\\theta,\\phi;x) \\;=\\; \\mathbb{E}_{q_\\phi(z \\mid x)}\\left[\\log p_\\theta(x \\mid z)\\right] \\;-\\; \\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right)$\n- 重参数化技巧: $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\,\\epsilon$ 其中 $\\epsilon \\sim \\mathcal{N}(0,1)$\n- 高斯后验和先验的库尔贝克-莱布勒 (KL) 散度: $\\mathrm{KL}\\!\\left(q_\\phi(z \\mid x)\\,\\|\\,p(z)\\right) \\;=\\; \\tfrac{1}{2}\\left(\\mu_\\phi(x)^2 + \\sigma_\\phi(x)^2 - \\log \\sigma_\\phi(x)^2 - 1\\right)$\n- 高斯解码器重构项: 最小化 $\\tfrac{1}{2\\sigma_x^2}\\,\\mathbb{E}_{q_\\phi}\\!\\left[\\lVert x - f_\\theta(z)\\rVert_2^2\\right]$\n\n**具体的模型和数据集实现：**\n- **模型架构 (一维线性 VAE):**\n    - 先验: $p(z)=\\mathcal{N}(0,1)$。\n    - 解码器: $p_\\theta(x\\mid z)=\\mathcal{N}(W z + b,\\; \\sigma_x^2 I)$ 其中固定 $\\sigma_x^2 = 1$。可训练参数为 $W \\in \\mathbb{R}^{D \\times 1}$ 和 $b \\in \\mathbb{R}^{D}$。\n    - 编码器: $q_\\phi(z \\mid x)=\\mathcal{N}(a^\\top x + c,\\; \\exp(\\ell))$。可训练参数为 $a \\in \\mathbb{R}^{D}$、$c \\in \\mathbb{R}$ 和 $\\ell \\in \\mathbb{R}$。\n- **训练目标 ($\\beta$-VAE):**\n    - 最小化: $\\mathcal{J}(\\theta,\\phi) \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N}\\left\\{\\tfrac{1}{2}\\left\\lVert x_i - (W z_i + b)\\right\\rVert_2^2 \\;+\\; \\beta \\cdot \\tfrac{1}{2}\\left(\\mu_i^2 + \\sigma^2 - \\ell - 1\\right)\\right\\}$\n    - 定义: $z_i = \\mu_i + \\sigma \\epsilon_i$, $\\mu_i = a^\\top x_i + c$, $\\sigma = \\exp(\\tfrac{1}{2}\\ell)$, $\\epsilon_i \\sim \\mathcal{N}(0,1)$。\n- **合成数据生成器：**\n    - 基因数量: $D = 5$。\n    - 基因标识: 索引 $0$ 是 \"PCNA\"，索引 $1$ 是 \"CCNB1\"。\n    - 潜在分数: $s_i \\sim \\mathcal{N}(0,1)$ for $i=1,\\dots,N$。\n    - 真实载荷向量: $u = [1.0, -0.8, 0.25, 0.1, -0.05]^\\top$。\n    - 基线向量: $m = [2.0, 1.5, 0.5, 0.1, -0.2]^\\top$。\n    - 观测模型: $x_i \\;=\\; m \\;+\\; u\\, s_i \\;+\\; \\varepsilon_i$, 其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\mathrm{noise}^2 I_D)$。\n- **潜在遍历与对齐准则：**\n    - 遍历网格: $K=41$ 个从 $-2$ 到 $2$ 的等距点 $z_j$。\n    - 重构轨迹: $\\hat{x}(z_j) = W z_j + b$。\n    - 方向: 如有必要，翻转 $W$ 的符号，以确保重构的 \"PCNA\" 随 $z$ 增加。\n    - 对齐阈值: $\\tau = 0.95$。\n    - 条件: 斯皮尔曼等级相关性 $\\rho(z, \\text{\"PCNA\"}) \\ge \\tau$ 且 $\\rho(z, \\text{\"CCNB1\"}) \\le -\\tau$。\n\n**测试套件：**\n- 用例 A: $N=300$, $\\sigma_\\mathrm{noise}=0.30$, $\\beta=1.0$, seed $=42$。\n- 用例 B: $N=120$, $\\sigma_\\mathrm{noise}=0.60$, $\\beta=0.5$, seed $=123$。\n- 用例 C: $N=40$, $\\sigma_\\mathrm{noise}=0.80$, $\\beta=4.0$, seed $=2024$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n根据验证标准对问题进行评估。\n- **科学依据：** 问题是合理的。它使用标准的机器学习模型（变分自编码器）和经典的统计模型（线性因子分析）来解决计算生物学中的一个问题（识别单细胞数据中的潜在结构）。合成数据生成过程是一个明确定义的模拟，这是测试算法常用且有效的方法。\n- **良态的：** 问题是良态的。所有模型组件、目标函数、数据生成参数和评估标准都得到了明确和数学化的定义。没有缺失或矛盾的规范。\n- **客观性：** 问题以客观、正式的语言陈述。所有标准都是定量的和可验证的。\n\n### 步骤 3：结论与行动\n\n该问题是 **有效的**。它是一个定义明确、有科学依据的计算任务。现在将构建一个完整的解决方案。\n\n### 解决方案\n\n任务是构建并训练一个一维线性变分自编码器（$\\beta$-VAE），用于处理合成的单细胞数据，并验证学习到的潜在维度是否正确捕捉了潜在的生物过程，即本例中的细胞周期样因子。\n\n**1. 合成数据生成**\n我们首先生成一个代表 $N$ 个细胞和 $D=5$ 个基因的合成数据集。细胞 $i$ 的表达谱 $x_i \\in \\mathbb{R}^D$ 由一个线性因子模型生成：\n$$\nx_i = m + u s_i + \\varepsilon_i\n$$\n这里，$s_i \\sim \\mathcal{N}(0,1)$ 是一个潜在分数，代表细胞在细胞周期轴上的位置。向量 $m \\in \\mathbb{R}^D$ 是一个基线平均表达水平，而 $u \\in \\mathbb{R}^D$ 是定义变异方向的载荷向量。$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I_D)$ 项代表每个基因独立的生物学和技术噪声。指定的载荷，$u_0 = 1.0$ (\"PCNA\") 和 $u_1 = -0.8$ (\"CCNB1\")，建立了从S期到G2/M期转换所特有的负相关性。我们的VAE必须从数据 $x_i$ 中恢复这个变异轴。\n\n**2. VAE模型和目标函数**\n我们采用一个具有一维潜在空间 $z \\in \\mathbb{R}$ 的线性VAE。\n- **编码器** $q_\\phi(z|x)$ 将数据点 $x \\in \\mathbb{R}^D$ 映射到潜在空间上的一个分布。我们使用一个高斯后验，其均值通过线性映射得到：\n$$\nq_\\phi(z \\mid x) = \\mathcal{N}(z \\mid \\mu(x), \\sigma^2)\n\\quad \\text{其中} \\quad\n\\mu(x) = a^\\top x + c \\quad \\text{且} \\quad \\sigma^2 = \\exp(\\ell)\n$$\n需要学习的参数是权重向量 $a \\in \\mathbb{R}^D$、偏置 $c \\in \\mathbb{R}$ 和对数方差 $\\ell \\in \\mathbb{R}$。\n\n- **解码器** $p_\\theta(x|z)$ 将潜在空间中的一个点 $z$ 映射回数据空间中的一个分布。我们使用一个具有固定单位方差的线性高斯解码器：\n$$\np_\\theta(x \\mid z) = \\mathcal{N}(x \\mid W z + b, I_D)\n$$\n需要学习的参数是权重矩阵（这里是一个向量）$W \\in \\mathbb{R}^{D \\times 1}$ 和偏置向量 $b \\in \\mathbb{R}^D$。\n\n训练目标是最小化负证据下界（ELBO），其中KL项由参数 $\\beta$ 加权（即 $\\beta$-VAE 目标）。对于一批 $N$ 个样本，目标函数为：\n$$\n\\mathcal{J} = \\frac{1}{N} \\sum_{i=1}^N \\left( \\underbrace{ \\frac{1}{2} \\|x_i - \\hat{x}_i\\|^2_2 }_{\\text{重构误差}} + \\beta \\cdot \\underbrace{ \\frac{1}{2} \\left( \\mu_i^2 + \\sigma^2 - \\log(\\sigma^2) - 1 \\right) }_{\\text{KL 散度}} \\right)\n$$\n其中 $\\hat{x}_i = W z_i + b$，而 $z_i$ 是使用重参数化技巧从 $q_\\phi(z|x_i)$ 中采样的：$z_i = \\mu_i + \\sigma \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,1)$。代入 $\\log(\\sigma^2) = \\ell$，我们得到问题陈述中精确的目标函数。\n\n**3. 通过梯度下降进行优化**\n我们使用全批量梯度下降法，对参数 $\\phi = \\{a, c, \\ell\\}$ 和 $\\theta = \\{W, b\\}$ 最小化 $\\mathcal{J}$。梯度是针对整个数据集的总损失计算的，然后取平均。对于单个样本 $i$，设残差为 $r_i = x_i - (W z_i + b)$。损失 $L_i$ 的梯度通过链式法则推导。\n\n- **解码器参数梯度：**\n$$ \\nabla_W L_i = -r_i z_i^\\top \\quad\\quad \\nabla_b L_i = -r_i $$\n- **编码器参数梯度：**\n$$ \\nabla_a L_i = \\left( -W^\\top r_i + \\beta \\mu_i \\right) x_i^\\top $$\n$$ \\nabla_c L_i = -W^\\top r_i + \\beta \\mu_i $$\n$$ \\nabla_\\ell L_i = \\frac{1}{2} \\left[ (-W^\\top r_i) \\sigma \\epsilon_i + \\beta (\\sigma^2 - 1) \\right] $$\n其中 $\\sigma = \\exp(\\ell/2)$。参数通过迭代更新：$p \\leftarrow p - \\eta \\nabla_p \\mathcal{J}$，其中 $\\eta$ 是学习率。\n\n**4. 潜在空间遍历与验证**\n训练收敛后，我们评估潜在变量 $z$ 是否捕捉到了主要的变异轴。\n首先，我们建立一个规范方向。学习到的潜在轴的方向是任意的；模型可能会学到增加 $z$ 对应于“PCNA”表达的减少，这与真实的生成因子 $s$ 相反。我们通过检查潜在变量遍历与重构的“PCNA”表达之间的相关性来强制使用一个标准方向。我们创建一个包含 $K=41$ 个值的网格 $z_j$，范围从 $-2$ 到 $2$。这些值被解码以获得轨迹 $\\hat{x}(z_j) = W z_j + b$。我们计算斯皮尔曼等级相关性 $\\rho(z, \\hat{x}_0(z))$。如果 $\\rho  0$，我们通过翻转解码器权重的符号来反转潜在轴：$W \\leftarrow -W$。此操作确保增加 $z$ 对应于增加“PCNA”表达，使我们的潜在空间与生物学惯例对齐。如果我们要继续使用编码器，其参数也需要翻转（$a \\leftarrow -a$, $c \\leftarrow -c$），但对于解码器的分析，只有 $W$ 是重要的。\n\n最后，我们测试对齐准则。使用正确定向的 $W$，我们计算“PCNA”（基因索引0）和“CCNB1”（基因索引1）轨迹的斯皮尔曼相关性。如果满足以下条件，则准则成立：\n$$\n\\rho(z, \\hat{x}_0(z)) \\ge 0.95 \\quad \\text{和} \\quad \\rho(z, \\hat{x}_1(z)) \\le -0.95\n$$\n这证实了模型在其单一潜在维度上学习到了两个关键标记基因的负相关行为。对所有三个测试用例重复此过程，以得出最终的布尔结果。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import spearmanr\n\ndef solve():\n    \"\"\"\n    Main function to run the VAE experiment for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A: Happy path\n        {'N': 300, 'sigma_noise': 0.30, 'beta': 1.0, 'seed': 42},\n        # Case B: Moderate noise, under-regularized\n        {'N': 120, 'sigma_noise': 0.60, 'beta': 0.5, 'seed': 123},\n        # Case C: Small sample, over-regularized\n        {'N': 40, 'sigma_noise': 0.80, 'beta': 4.0, 'seed': 2024},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_single_case(**params)\n        results.append(result)\n\n    # Format the final output as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(N, sigma_noise, seed):\n    \"\"\"\n    Generates synthetic scRNA-seq data based on a linear factor model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    D = 5\n    \n    # True generating parameters\n    u = np.array([1.0, -0.8, 0.25, 0.1, -0.05]).reshape(D, 1)\n    m = np.array([2.0, 1.5, 0.5, 0.1, -0.2])\n\n    # Latent scores and noise\n    s = rng.normal(loc=0.0, scale=1.0, size=(N, 1))\n    noise = rng.normal(loc=0.0, scale=sigma_noise, size=(N, D))\n\n    # Generate data\n    X = m + s @ u.T + noise\n    return X\n\ndef train_vae(X, beta, seed, learning_rate=0.01, epochs=1000):\n    \"\"\"\n    Trains a 1D linear Variational Autoencoder.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    N, D = X.shape\n\n    # Initialize VAE parameters\n    # Decoder params\n    W = rng.normal(loc=0.0, scale=0.1, size=(D, 1))\n    b = np.zeros(D)\n    # Encoder params\n    a = rng.normal(loc=0.0, scale=0.1, size=(D, 1))\n    c = 0.0\n    ell = 0.0 # log-variance\n\n    for epoch in range(epochs):\n        # --- Forward pass ---\n        # Encoder\n        mu = X @ a + c  # Shape (N, 1)\n        sigma = np.exp(0.5 * ell)\n        \n        # Reparameterization trick\n        epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, 1))\n        z = mu + sigma * epsilon  # Shape (N, 1)\n        \n        # Decoder\n        X_recon = z @ W.T + b  # Shape (N, D)\n        \n        # --- Loss calculation (for monitoring, not strictly needed for gradients) ---\n        # recon_loss = 0.5 * np.mean(np.sum((X - X_recon)**2, axis=1))\n        # kl_loss = 0.5 * np.mean(mu**2 + sigma**2 - ell - 1)\n        # loss = recon_loss + beta * kl_loss\n        \n        # --- Gradient calculation (full-batch) ---\n        # Residual term\n        res = X - X_recon  # Shape (N, D)\n\n        # Gradients for decoder parameters (W, b)\n        grad_W = - (res.T @ z) / N\n        grad_b = - np.mean(res, axis=0)\n\n        # Gradients for encoder parameters (a, c, ell)\n        # Chain rule term from reconstruction loss back to z\n        d_recon_loss_d_z = - (res @ W) # Shape (N, 1)\n        \n        # Gradient for 'a'\n        d_kl_loss_d_mu = beta * mu\n        d_loss_d_mu = d_recon_loss_d_z + d_kl_loss_d_mu # Shape (N, 1)\n        grad_a = (X.T @ d_loss_d_mu) / N\n        \n        # Gradient for 'c'\n        grad_c = np.mean(d_loss_d_mu)\n\n        # Gradient for 'ell'\n        d_loss_d_z = d_recon_loss_d_z\n        d_z_d_ell = 0.5 * sigma * epsilon\n        d_kl_loss_d_ell = 0.5 * beta * (sigma**2 - 1)\n        grad_ell = np.mean(d_loss_d_z * d_z_d_ell) + d_kl_loss_d_ell\n\n        # --- Parameter update ---\n        W -= learning_rate * grad_W\n        b -= learning_rate * grad_b\n        a -= learning_rate * grad_a\n        c -= learning_rate * grad_c\n        ell -= learning_rate * grad_ell\n\n    return W, b\n\ndef analyze_model(W, b):\n    \"\"\"\n    Performs latent traversal and checks the alignment criterion.\n    \"\"\"\n    tau = 0.95\n    K = 41\n    z_grid = np.linspace(-2.0, 2.0, K)\n\n    # Decode the latent traversal grid\n    X_recon_traj = z_grid.reshape(-1, 1) @ W.T + b.reshape(1, -1)\n\n    # Extract trajectories for \"PCNA\" (gene 0) and \"CCNB1\" (gene 1)\n    pcna_traj = X_recon_traj[:, 0]\n    \n    # Check orientation and flip if necessary\n    rho_pcna, _ = spearmanr(z_grid, pcna_traj)\n\n    if rho_pcna  0:\n        W = -W\n        # Recalculate trajectories with flipped W\n        X_recon_traj = z_grid.reshape(-1, 1) @ W.T + b.reshape(1, -1)\n        pcna_traj = X_recon_traj[:, 0]\n        # Recalculate correlation for the check\n        rho_pcna, _ = spearmanr(z_grid, pcna_traj)\n        \n    ccnb1_traj = X_recon_traj[:, 1]\n    rho_ccnb1, _ = spearmanr(z_grid, ccnb1_traj)\n\n    # Verify the alignment criterion\n    is_aligned = (rho_pcna >= tau) and (rho_ccnb1 = -tau)\n    \n    return is_aligned\n\ndef run_single_case(N, sigma_noise, beta, seed):\n    \"\"\"\n    Executes one full test case from data generation to analysis.\n    \"\"\"\n    # Generate data with the case-specific seed for the entire process.\n    X = generate_data(N=N, sigma_noise=sigma_noise, seed=seed)\n    \n    # Train the VAE. The same seed ensures reproducible parameter initialization.\n    W, b = train_vae(X=X, beta=beta, seed=seed)\n\n    # Analyze the trained model\n    is_aligned = analyze_model(W, b)\n    \n    return is_aligned\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2439780"}, {"introduction": "VAE 非常强大，但其无监督的特性是一把双刃剑。这个练习提出了一个在生物信息学中常见且具有挑战性的场景：一个模型成功地完成了训练（即重建误差很低），但其潜空间却未能根据一个关键的生物学标签（如健康状况）将数据分离开。这个思想实验将帮助你理解这种情况发生的原因，以及它对你的数据和模型选择意味着什么，并强调了重建保真度与外部变量对齐之间的区别。[@problem_id:2439785]", "problem": "给定一个包含$n$个独立样本的微生物组分析数据集。对于每个样本$i \\in \\{1,\\dots,n\\}$，令$x_i \\in \\Delta^{D-1}$表示$D$个分类单元的相对丰度向量，其中$\\Delta^{D-1} = \\{x \\in \\mathbb{R}^D_{\\ge 0} : \\sum_{j=1}^D x_j = 1\\}$是概率单纯形。每个样本还有一个二元健康状态标签$y_i \\in \\{0,1\\}$。一个变分自编码器（VAE）在变换后的特征$\\tilde{x}_i$上以无监督方式进行训练。这些特征是通过对$x_i$进行标准的伪计数预处理后，应用中心对数比变换得到的。该VAE使用一个标准正态先验$p(z) = \\mathcal{N}(0, I)$，一个编码器$q_\\phi(z \\mid \\tilde{x}) = \\mathcal{N}(\\mu_\\phi(\\tilde{x}), \\mathrm{diag}(\\sigma^2_\\phi(\\tilde{x})))$，以及一个解码器$p_\\theta(\\tilde{x} \\mid z)$，该解码器被建模为具有对角协方差的因子化高斯分布。训练过程最大化数据集上的证据下界（ELBO）。训练后，编码器分布的均值$m_i = \\mu_\\phi(\\tilde{x}_i)$被用作样本$i$在维度为$d_z$的潜在空间中的嵌入。当可视化$\\{m_i\\}_{i=1}^n$的前$2$个主成分并按$y_i$着色时，$y=0$和$y=1$两组之间没有明显的分离。此外，一个训练用于从$m_i$预测$y$的线性分类器，在留出数据上达到了约$0.5$的平衡准确率，而$\\tilde{x}$的重构误差很低。\n\n选择所有最能被这些观察结果支持的陈述。\n\nA. 该观察结果意味着健康状态$y$与人群中的微生物组构成$x$在统计上是独立的，即$I(X;Y)=0$。\n\nB. 无监督的VAE目标函数没有明确鼓励按$y$进行分离，因此潜在变量$z$可能捕获了与$y$无关的变异来源（例如，饮食、批次或受试者特异性效应）。这表明与$y$相关的信号可能相对于其他变异较弱或被混淆，并且仅仅缺乏分离并不表示训练失败。\n\nC. 似然或预处理的不匹配可能会掩盖与标签相关的结构；对于成分性和计数类的微生物组数据，一个与计数或成分对齐的解码器（例如，基于计数的多项式、狄利克雷-多项式或负二项分布，或一个明确感知单纯形的模型）可能比在变换数据上使用因子化高斯分布更合适。\n\nD. 简单地增加潜在维度$d_z$而不改变学习目标，就保证了如果提供足够的容量，潜在空间将会按$y$分离样本。\n\nE. 缺乏分离是过拟合的直接证据，因为一个良好正则化的模型总是会将其潜在轴与任何可用的二元标签对齐。\n\nF. 在目标函数中没有标签信息的情况下，没有要求$z$必须与$y$对齐；如果期望按$y$进行分离，则有必要将$y$整合到模型或目标函数中（例如，使用条件或半监督VAE），以激励$I(Z;Y)0$。", "solution": "问题陈述需经过验证。\n\n### 第1步：提取已知信息\n- **数据集**：来自一项微生物组分析研究的$n$个独立样本。\n- **输入数据**：对于每个样本$i$，数据是相对丰度向量$x_i \\in \\Delta^{D-1}$，其中$\\Delta^{D-1} = \\{x \\in \\mathbb{R}^D_{\\ge 0} : \\sum_{j=1}^D x_j = 1\\}$是$(D-1)$维概率单纯形。\n- **标签**：每个样本都有一个二元健康状态标签$y_i \\in \\{0,1\\}$。\n- **预处理**：原始丰度向量$x_i$经过伪计数预处理，然后使用中心对数比（CLR）变换，得到特征$\\tilde{x}_i$。\n- **模型**：在变换后的特征集$\\{\\tilde{x}_i\\}_{i=1}^n$上训练一个变分自编码器（VAE）。训练是无监督的。\n- **VAE组件**：\n    - 潜在变量的先验分布：$p(z) = \\mathcal{N}(0, I)$。\n    - 编码器（概率性）：$q_\\phi(z \\mid \\tilde{x}) = \\mathcal{N}(\\mu_\\phi(\\tilde{x}), \\mathrm{diag}(\\sigma^2_\\phi(\\tilde{x})))$，其中$\\phi$是编码器参数。\n    - 解码器（概率性）：$p_\\theta(\\tilde{x} \\mid z)$是一个具有对角协方差矩阵的因子化高斯分布，其中$\\theta$是解码器参数。\n- **训练**：通过最大化数据集上的证据下界（ELBO）来训练模型。单个样本$\\tilde{x}$的ELBO定义为 $\\mathcal{L}(\\phi, \\theta; \\tilde{x}) = \\mathbb{E}_{z \\sim q_\\phi(z|\\tilde{x})} [\\log p_\\theta(\\tilde{x} \\mid z)] - D_{KL}(q_\\phi(z \\mid \\tilde{x}) \\| p(z))$。\n- **潜在嵌入**：训练后，样本$i$的嵌入取自维度为$d_z$的潜在空间中近似后验的均值，$m_i = \\mu_\\phi(\\tilde{x}_i)$。\n- **观察结果**：\n    1.  $\\tilde{x}$的重构误差很低。\n    2.  对嵌入$\\{m_i\\}_{i=1}^n$进行主成分分析（PCA）后，在$y=0$和$y=1$的样本之间没有显示出分离。\n    3.  一个训练用于从$m_i$预测$y_i$的线性分类器，在留出测试集上取得了约$0.5$的平衡准确率，这对于一个二元问题来说相当于随机猜测。\n\n### 第2步：使用提取的已知信息进行验证\n该问题描述了VAE在高维生物数据上的一个标准应用，这是计算生物学和生物信息学中的一个常见任务。\n- **科学依据**：使用VAE对微生物组数据进行降维是一种成熟的技术。数据类型（成分丰度）、预处理（CLR变换）、模型架构（带高斯组件的标准VAE）和训练目标（ELBO）都与该领域的实践相符。观察到的结果——重构效果好，但在无监督设置下未能按外部标签分离样本——是一个现实中经常遇到的结果。\n- **问题定义明确**：问题陈述清晰，为评估给定陈述提供了所有必要的背景。它描述了一个计算实验及其结果，并要求进行逻辑解释。\n- **客观性**：描述是事实性和定量性的，避免了主观或模糊的语言。\n\n问题陈述内部一致，科学上合理，且定义明确。它没有违反任何验证标准。\n\n### 第3步：结论与行动\n问题陈述有效。将进行全面分析。\n\n### 解题推导\n\nVAE的训练目标是最大化ELBO，它平衡了两个项：重构对数似然（$\\mathbb{E}_{z \\sim q_\\phi(z|\\tilde{x})} [\\log p_\\theta(\\tilde{x} \\mid z)]$）和一个正则化项（$- D_{KL}(q_\\phi(z \\mid \\tilde{x}) \\| p(z))$）。训练是无监督的，意味着标签$y_i$没有被用在目标函数中。VAE的目标是学习一个压缩表示$z$，从中可以忠实地重构出原始输入$\\tilde{x}$。因此，潜在变量$z$被优化以捕获数据分布$p(\\tilde{x})$中存在的主要变异来源。\n\n重构误差低的观察结果表明，VAE已经成功地为变换后的数据$\\tilde{x}$学习到了一个好的生成模型。也就是说，潜在空间$Z$有效地捕获了重构$\\tilde{x}$所需的信息。\n\n线性分类器在从潜在嵌入$m_i = \\mu_\\phi(\\tilde{x}_i)$预测$y$时表现出随机猜测水平，这一观察结果意味着关于健康状态$y$的信息没有线性地编码在潜在空间中。结合PCA中缺乏视觉分离的现象，这强烈表明潜在维度没有与$y$所代表的生物学因素对齐。当数据中可归因于$y$的变异与其他变异来源（例如，个体间差异、饮食、技术批次效应）相比很小，或者是高度复杂和非线性的时，就会发生这种情况。无监督的目标函数激励模型去解释最显著的方差，因此它会优先考虑这些其他因素，而不是来自$y$的可能很微弱的信号。\n\n### 逐项分析\n\n**A. 该观察结果意味着健康状态$y$与人群中的微生物组构成$x$在统计上是独立的，即$I(X;Y)=0$。**\n这个陈述对原始数据$X$和标签$Y$之间真实的潜在统计关系做出了一个强有力的断言。实验表明，一个特定的模型——一个具有特定架构和预处理流程的VAE——未能找到其潜在表示$Z$和$Y$之间的关系。数据处理不等式指出，对于随机变量的任何处理，信息只能减少或保持不变。在这里，$Z$是$X$的一个随机函数（通过CLR变换和编码器）。因此，$I(X; Y) \\geq I(Z; Y)$。$I(Z; Y) \\approx 0$（从分类器性能差推断）的观察结果并不意味着$I(X;Y)=0$。信息可能在预处理过程中丢失了，或者仅仅因为不是一个主导信号而没有被无监督模型捕获。没有证据不等于没有存在的证据。\n**结论：错误。**\n\n**B. 无监督的VAE目标函数没有明确鼓励按$y$进行分离，因此潜在变量$z$可能捕获了与$y$无关的变异来源（例如，饮食、批次或受试者特异性效应）。这表明与$y$相关的信号可能相对于其他变异较弱或被混淆，并且仅仅缺乏分离并不表示训练失败。**\n这个陈述准确地诊断了情况。VAE的目标函数对$y$是“无知”的。它会配置其潜在空间以有效地表示$\\tilde{x}$中最显著的变异模式，从而最小化重构损失。如果这些模式是由健康状态以外的因素驱动的，那么潜在空间将反映这些因素。VAE实现了低重构误差这一事实表明它成功完成了其指定的任务。未能按$y$分离是实验设计未能达到特定分析目标的失败，而不是模型未能优化其目标的失败。这是一个正确且有见地的解释。\n**结论：正确。**\n\n**C. 似然或预处理的不匹配可能会掩盖与标签相关的结构；对于成分性和计数类的微生物组数据，一个与计数或成分对齐的解码器（例如，基于计数的多项式、狄利克雷-多项式或负二项分布，或一个明确感知单纯形的模型）可能比在变换数据上使用因子化高斯分布更合适。**\n这个陈述为模型未能捕获与$y$相关的信号提出了一个有效的假设。选择用因子化高斯模型来建模CLR变换后的数据是一个很强的建模假设。CLR变换本身对于稀疏数据可能存在问题，而高斯似然对于源自计数的数据可能不是最佳选择。这些选择很可能减弱了与$y$相关的信号，或者使其在变换空间中成为一个不那么主导的方差分量。提出更合适的替代似然（如基于计数的模型），这些似然在原始数据尺度上工作，是解决此类问题的标准且科学合理的方法。这代表了对所选方法论的有效批评，并可作为一种潜在的解释。\n**结论：正确。**\n\n**D. 简单地增加潜在维度$d_z$而不改变学习目标，就保证了如果提供足够的容量，潜在空间将会按$y$分离样本。**\n这个陈述是错误的。通过增加$d_z$来增加模型容量，可以让模型捕获关于数据分布的更多细节。然而，无监督学习目标保持不变。模型将使用额外的容量来更好地建模它已经关注的相同主导变异来源。没有“保证”它会突然发现一个与$y$相关的微弱的次要信号。事实上，如果没有适当的正则化，一个非常高的$d_z$可能导致VAE学习到一个平凡的、无信息的恒等函数。容量不意味着方向；目标函数提供了方向。\n**结论：错误。**\n\n**E. 缺乏分离是过拟合的直接证据，因为一个良好正则化的模型总是会将其潜在轴与任何可用的二元标签对齐。**\n这个陈述从根本上误解了过拟合和正则化。在VAE的背景下，过拟合通常意味着模型在训练数据上实现了非常低的重构误差，但在泛化上失败，导致在留出数据上重构误差很高。问题没有提供足够的信息来诊断过拟合，但总的来说，低重构误差表明在给定任务上表现良好。更重要的是，正则化（通过KL散度项）鼓励潜在空间平滑且结构良好（接近先验），这有助于泛化。它本身并不会强迫潜在轴与任何不属于目标函数的外部标签对齐。除非明确提供，否则模型对“可用”标签一无所知。\n**结论：错误。**\n\n**F. 在目标函数中没有标签信息的情况下，没有要求$z$必须与$y$对齐；如果期望按$y$进行分离，则有必要将$y$整合到模型或目标函数中（例如，使用条件或半监督VAE），以激励$I(Z;Y)0$。**\n这个陈述抓住了机器学习的一个基本原则，该问题的情景完美地说明了这一点。无监督模型没有动机或义务去学习对外部变量（如$y$）具有信息量的表示。观察到的分离失败是这一点的直接后果。该陈述正确地指出了解决方案：为了确保潜在空间对$y$具有信息量，必须修改模型和/或目标函数以包含$y$。这引出了像条件VAE（cVAE）这样的方法，其中$y$是编码器和解码器的输入，或者半监督VAE，其中关于$y$的分类损失被添加到ELBO中。这迫使模型使互信息$I(Z;Y)$非零。\n**结论：正确。**", "answer": "$$\\boxed{BCF}$$", "id": "2439785"}, {"introduction": "构建更强大、更具生物学意义的模型的关键之一是将领域知识直接注入其中。本实践将演示如何修改标准的 VAE 重建损失，以整合来自蛋白质-蛋白质相互作用（PPI）网络的信息。你将推导出一个计算这种网络感知损失的高效方法，学习如何对已知相互作用的基因的重建误差施加更重的惩罚，从而引导模型学习一个更具生物学意义的表示。[@problem_id:2439822]", "problem": "给定一个场景，其中一个变分自编码器 (VAE) (Variational Autoencoder) 在基因表达谱上进行训练，每个样本是一个表示 $G$ 个基因表达水平的实值向量。对于单个样本，我们将真实表达向量表示为 $\\mathbf{x} \\in \\mathbb{R}^G$，解码器重构的向量表示为 $\\hat{\\mathbf{x}} \\in \\mathbb{R}^G$。对于具有单位方差的高斯似然，证据下界 (ELBO) (Evidence Lower Bound) 的标准重构项贡献与平方误差 $\\sum_{i=1}^{G} (x_i - \\hat{x}_i)^2$ 成正比。此外，还给定一个蛋白质-蛋白质相互作用 (PPI) (Protein-Protein Interaction) 网络，它由一个对称、非负且对角线为零的邻接矩阵 $\\mathbf{A} \\in \\mathbb{R}_{\\ge 0}^{G \\times G}$ 表示，该矩阵编码了相互作用的基因对。\n\n您的任务是修改重构项，以便对相互作用的基因对的重构误差施加更重的惩罚。从以下针对单个样本的成对增广重构目标开始：\n$$\n\\mathcal{L}_{\\text{mod}}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\mathbf{A}, \\alpha) \\;=\\; \\sum_{i=1}^{G} \\big(x_i - \\hat{x}_i\\big)^2 \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} \\Big(\\big(x_i - \\hat{x}_i\\big)^2 + \\big(x_j - \\hat{x}_j\\big)^2\\Big),\n$$\n其中 $\\alpha \\in \\mathbb{R}_{\\ge 0}$ 是一个可调标量，用于控制成对惩罚的强度。仅根据此定义和图上求和的标准性质，推导出一个 $\\mathcal{L}_{\\text{mod}}$ 的等价表达式，该表达式仅使用每个基因的平方误差和 PPI 图的节点度，即可在 $\\mathcal{O}(G + \\lvert E \\rvert)$ 时间内计算（其中 $\\lvert E \\rvert$ 是 $\\mathbf{A}$ 中非零非对角线条目的数量，每条无向边计数一次）。使用代数操作和图论恒等式来证明您推导的每一步。\n\n然后，实现一个程序，为几个指定的测试用例计算 $\\mathcal{L}_{\\text{mod}}$。在每个测试用例中，都给定了 $\\mathbf{x}$、$\\hat{\\mathbf{x}}$、$\\mathbf{A}$ 和 $\\alpha$。所有数组和矩阵都很小，并在下面明确提供。不涉及随机性。没有需要报告的物理单位。\n\n实现约束：\n- 使用实值算术。\n- 假设 $\\mathbf{A}$ 是对称的且对角线为零；如果存在微小的不对称，您可以通过 $\\tfrac{1}{2}\\big(\\mathbf{A} + \\mathbf{A}^\\top\\big)$ 对其进行对称化，然后将对角线置零。\n- 您的程序必须为每个测试用例计算一个标量 $\\mathcal{L}_{\\text{mod}}$。\n\n测试套件（每个用例都是独立的）：\n- 用例 $1$（正常路径，混合连通性）：\n  - $G = 4$，\n  - $\\mathbf{x} = [\\,10.0,\\, 7.5,\\, 3.0,\\, 0.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,9.5,\\, 7.0,\\, 2.5,\\, 0.5\\,]$,\n  - $\\mathbf{A}$ 有边 $(0,1)$、$(1,2)$、$(2,3)$，权重各为 $1.0$，其余为零；明确地，\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  0  0 \\\\\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    0  0  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.3$。\n- 用例 $2$（边界情况，$\\alpha = 0$）：\n  - $G = 3$，\n  - $\\mathbf{x} = [\\,1.0,\\, 2.0,\\, 3.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.0,\\, 1.0,\\, 3.0\\,]$,\n  - $\\mathbf{A}$ 全连接且无自环；明确地，\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  1 \\\\\n    1  0  1 \\\\\n    1  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.0$。\n- 用例 $3$（无边，$\\mathbf{A}=\\mathbf{0}$）：\n  - $G = 3$，\n  - $\\mathbf{x} = [\\,4.0,\\, 0.0,\\, 5.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,2.0,\\, 1.0,\\, 7.0\\,]$,\n  - $\\mathbf{A} = \\mathbf{0}_{3 \\times 3}$，\n  - $\\alpha = 0.8$。\n- 用例 $4$（全连接，更强的放大作用）：\n  - $G = 3$，\n  - $\\mathbf{x} = [\\,5.0,\\, 5.0,\\, 5.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,4.0,\\, 7.0,\\, 2.0\\,]$,\n  - $\\mathbf{A}$ 全连接且无自环；明确地，\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1  1 \\\\\n    1  0  1 \\\\\n    1  1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 0.5$。\n- 用例 $5$（精确重构）：\n  - $G = 2$，\n  - $\\mathbf{x} = [\\,1.2,\\, 3.4\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.2,\\, 3.4\\,]$,\n  - $\\mathbf{A}$ 是一条单独的边；明确地，\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  1 \\\\\n    1  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 2.0$。\n- 用例 $6$（加权 PPI 边）：\n  - $G = 3$，\n  - $\\mathbf{x} = [\\,2.0,\\, 0.0,\\, 1.0\\,]$,\n  - $\\hat{\\mathbf{x}} = [\\,1.0,\\, 1.0,\\, 0.0\\,]$,\n  - $\\mathbf{A}$ 的边权重为 $A_{01} = A_{10} = 0.5$，$A_{12} = A_{21} = 0.2$，其余为零；明确地，\n    $$\n    \\mathbf{A} \\;=\\; \\begin{bmatrix}\n    0  0.5  0 \\\\\n    0.5  0  0.2 \\\\\n    0  0.2  0\n    \\end{bmatrix},\n    $$\n  - $\\alpha = 1.25$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个标量 $\\mathcal{L}_{\\text{mod}}$ 按顺序对应于用例 1 到 6，每个值四舍五入到恰好 $6$ 位小数（例如，$[1.234000,2.000000]$）。", "solution": "所述问题在科学上是合理的，在数学上是适定的，并且是完整的。它提出了计算生物学中的一个标准任务：修改机器学习目标函数以融入先验生物学知识（在本例中是蛋白质-蛋白质相互作用网络），然后推导出一种高效的计算方法。其前提建立在变分自编码器和图论的既定原则之上。唯一解所需的所有数据均已提供。因此，我们可以进行推导和实现。\n\n任务是为修改后的重构损失 $\\mathcal{L}_{\\text{mod}}$ 推导出一个等价但计算效率更高的表达式。给定的定义是：\n$$\n\\mathcal{L}_{\\text{mod}}(\\mathbf{x}, \\hat{\\mathbf{x}}; \\mathbf{A}, \\alpha) \\;=\\; \\sum_{i=1}^{G} \\big(x_i - \\hat{x}_i\\big)^2 \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} \\Big(\\big(x_i - \\hat{x}_i\\big)^2 + \\big(x_j - \\hat{x}_j\\big)^2\\Big)\n$$\n在这里，$G$ 是基因的数量，$\\mathbf{x}$ 是真实的表达向量，$\\hat{\\mathbf{x}}$ 是其重构，$\\mathbf{A}$ 是 PPI 网络的对称邻接矩阵，且 $A_{ii}=0$，$\\alpha$ 是一个非负标量权重。\n\n为简化分析，我们定义每个基因的平方误差为 $e_i = (x_i - \\hat{x}_i)^2$。$\\mathcal{L}_{\\text{mod}}$ 的表达式变为：\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i \\;+\\; \\alpha \\sum_{1 \\le i  j \\le G} A_{ij} (e_i + e_j)\n$$\n第一项 $\\sum_{i=1}^{G} e_i$ 是标准的平方误差和，可在 $\\mathcal{O}(G)$ 时间内计算。第二项，我们将其表示为 $S_{_P}$，代表成对惩罚。对 $S_{_P}$ 的朴素计算涉及对所有对 $(i, j)$ 的双重循环，导致 $\\mathcal{O}(G^2)$ 的复杂度。我们的目标是为此项找到一个可以更高效计算的表达式。\n$$\nS_{_P} = \\sum_{1 \\le i  j \\le G} A_{ij} (e_i + e_j)\n$$\n我们可以将这个和分成两部分：\n$$\nS_{_P} = \\sum_{1 \\le i  j \\le G} A_{ij} e_i + \\sum_{1 \\le i  j \\le G} A_{ij} e_j\n$$\n这种形式没有直接的帮助。一种更有效的方法是通过关注每个单独节点误差 $e_k$ 对总和 $S_{_P}$ 的贡献来重新排列求和。求和 $\\sum_{1 \\le i  j \\le G}$ 遍历了由 $\\mathbf{A}$ 表示的图中所有唯一的无向边。\n\n对于任何给定节点 $k$，只要考虑到与 $k$ 相关联的边，其误差项 $e_k$ 就会出现在和中。\n\n对于一个特定节点 $k$，当满足以下条件时，项 $e_k$ 被包含在内：\n1.  $i = k$，对于所有 $j  k$。对和的贡献是 $\\sum_{j  k} A_{kj} e_k$。\n2.  $j = k$，对于所有 $i  k$。对和的贡献是 $\\sum_{i  k} A_{ik} e_k$。\n\n因此，在 $S_{_P}$ 的表达式中，乘以 $e_k$ 的总系数是这两部分之和：\n$$\n\\text{Coefficient of } e_k = \\sum_{j  k} A_{kj} + \\sum_{i  k} A_{ik}\n$$\n问题指出邻接矩阵 $\\mathbf{A}$ 是对称的（$A_{ij} = A_{ji}$）并且对角线为零（$A_{ii} = 0$）。利用对称性，我们可以重写第二个求和：$\\sum_{i  k} A_{ik} = \\sum_{i  k} A_{ki}$。\n$e_k$ 的系数则为：\n$$\n\\sum_{j  k} A_{kj} + \\sum_{i  k} A_{ki} = \\sum_{j \\neq k} A_{kj}\n$$\n这个和 $\\sum_{j \\neq k} A_{kj}$ 正是节点 $k$ 的加权度，我们将其表示为 $d_k$。由于 $A_{kk} = 0$，加权度就是邻接矩阵第 $k$ 行（或列）的和：$d_k = \\sum_{j=1}^{G} A_{kj}$。\n\n通过对所有节点 $k = 1, \\dots, G$ 的贡献求和，我们重构了整个成对和 $S_{_P}$：\n$$\nS_{_P} = \\sum_{k=1}^{G} e_k d_k\n$$\n这个推导是正确的，因为原始和中的每一项 $A_{ij}(e_i+e_j)$ 都被完美地计算在内：$A_{ij}e_i$ 被计入 $e_i d_i$ 项，而 $A_{ij}e_j$（即 $A_{ji}e_j$）被计入 $e_j d_j$ 项。\n\n将这个简化的 $S_{_P}$ 表达式代回到 $\\mathcal{L}_{\\text{mod}}$ 的公式中，我们得到：\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i + \\alpha \\sum_{i=1}^{G} e_i d_i\n$$\n我们可以提出因子 $e_i$，得到最终的、计算上高效的表达式：\n$$\n\\mathcal{L}_{\\text{mod}} = \\sum_{i=1}^{G} e_i (1 + \\alpha d_i)\n$$\n让我们分析这个最终形式的计算复杂度。\n1.  所有基因的平方误差 $e_i = (x_i - \\hat{x}_i)^2$（$i=1, \\dots, G$）可以在 $\\mathcal{O}(G)$ 时间内计算。\n2.  所有节点的加权度 $d_i = \\sum_{j=1}^{G} A_{ij}$（$i=1, \\dots, G$）可以被计算。如果 $\\mathbf{A}$ 是一个稠密矩阵，这需要 $\\mathcal{O}(G^2)$ 的时间。然而，对于一个有 $|E|$ 条边的稀疏图（其中 $|E|$ 是上三角矩阵中非零条目的数量），我们可以通过遍历邻接表表示，在 $\\mathcal{O}(G+|E|)$ 时间内计算所有度。\n3.  最终的求和 $\\sum_{i=1}^{G} e_i (1 + \\alpha d_i)$ 需要对 $G$ 个基因进行单次遍历，耗时 $\\mathcal{O}(G)$。\n\n因此，如果从稀疏表示计算度，使用此推导公式的总时间复杂度为 $\\mathcal{O}(G + |E|)$；如果从稠密矩阵计算，则为 $\\mathcal{O}(G^2)$。由于问题要求一个*可以*在 $\\mathcal{O}(G+|E|)$ 时间内计算的公式，我们推导的表达式满足要求。这种形式避免了对基因对的显式 $\\mathcal{O}(G^2)$ 迭代，这对于大的 $G$ 来说是原始定义的主要瓶颈。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the modified reconstruction loss L_mod for a VAE\n    using an efficient, derived formula for several test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1\n        {\n            \"x\": np.array([10.0, 7.5, 3.0, 0.0]),\n            \"x_hat\": np.array([9.5, 7.0, 2.5, 0.5]),\n            \"A\": np.array([\n                [0.0, 1.0, 0.0, 0.0],\n                [1.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 1.0],\n                [0.0, 0.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.3\n        },\n        # Case 2\n        {\n            \"x\": np.array([1.0, 2.0, 3.0]),\n            \"x_hat\": np.array([1.0, 1.0, 3.0]),\n            \"A\": np.array([\n                [0.0, 1.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [1.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.0\n        },\n        # Case 3\n        {\n            \"x\": np.array([4.0, 0.0, 5.0]),\n            \"x_hat\": np.array([2.0, 1.0, 7.0]),\n            \"A\": np.zeros((3, 3)),\n            \"alpha\": 0.8\n        },\n        # Case 4\n        {\n            \"x\": np.array([5.0, 5.0, 5.0]),\n            \"x_hat\": np.array([4.0, 7.0, 2.0]),\n            \"A\": np.array([\n                [0.0, 1.0, 1.0],\n                [1.0, 0.0, 1.0],\n                [1.0, 1.0, 0.0]\n            ]),\n            \"alpha\": 0.5\n        },\n        # Case 5\n        {\n            \"x\": np.array([1.2, 3.4]),\n            \"x_hat\": np.array([1.2, 3.4]),\n            \"A\": np.array([\n                [0.0, 1.0],\n                [1.0, 0.0]\n            ]),\n            \"alpha\": 2.0\n        },\n        # Case 6\n        {\n            \"x\": np.array([2.0, 0.0, 1.0]),\n            \"x_hat\": np.array([1.0, 1.0, 0.0]),\n            \"A\": np.array([\n                [0.0, 0.5, 0.0],\n                [0.5, 0.0, 0.2],\n                [0.0, 0.2, 0.0]\n            ]),\n            \"alpha\": 1.25\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x = case[\"x\"]\n        x_hat = case[\"x_hat\"]\n        A = case[\"A\"]\n        alpha = case[\"alpha\"]\n\n        # Per problem specification, ensure A is symmetric with a zero diagonal.\n        # This is a robust preprocessing step.\n        A_sym = 0.5 * (A + A.T)\n        np.fill_diagonal(A_sym, 0)\n\n        # Calculate per-gene squared errors: e_i = (x_i - x_hat_i)^2\n        e = (x - x_hat)**2\n\n        # Calculate weighted node degrees: d_i = sum_j A_ij\n        d = A_sym.sum(axis=1)\n\n        # Compute L_mod using the derived efficient formula:\n        # L_mod = sum_i e_i * (1 + alpha * d_i)\n        l_mod = np.sum(e * (1.0 + alpha * d))\n        \n        results.append(l_mod)\n\n    # Format the output as a comma-separated list with 6 decimal places.\n    output_str = \"[\" + \",\".join([f\"{res:.6f}\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "2439822"}]}