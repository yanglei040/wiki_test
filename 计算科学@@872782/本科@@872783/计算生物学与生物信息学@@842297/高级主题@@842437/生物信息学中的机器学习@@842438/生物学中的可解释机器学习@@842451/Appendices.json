{"hands_on_practices": [{"introduction": "在模型解释中，一个基本任务是量化不同特征的重要性。这个练习将通过一个简化但功能强大的线性模型，让你亲手实践组特征重要性的概念。你将学习如何通过将输出变量的方差归因于不同的生物输入组（在本例中是DNA甲基化与组蛋白修饰），来剖析模型的预测。这项实践为你理解更复杂的特征归属方法奠定了坚实的基础 [@problem_id:2400021]。", "problem": "您将获得一个形式化设定，用于比较两个分子特征组——脱氧核糖核酸（DNA）甲基化和组蛋白修饰——对某一细胞系中基因表达的预测贡献。考虑一个为表达变量 $y$ 定义的生成线性模型\n$$\ny \\;=\\; X^{(M)} w^{(M)} \\;+\\; X^{(H)} w^{(H)} \\;+\\; \\varepsilon,\n$$\n其中 $X^{(M)} \\in \\mathbb{R}^{n \\times m}$ 表示 DNA 甲基化特征，$X^{(H)} \\in \\mathbb{R}^{n \\times h}$ 表示组蛋白修饰特征，$w^{(M)} \\in \\mathbb{R}^{m}$ 和 $w^{(H)} \\in \\mathbb{R}^{h}$ 是固定的系数向量，而 $\\varepsilon \\in \\mathbb{R}^{n}$ 是一个零均值噪声向量，每个样本的方差为 $\\sigma^2$。假设 $X^{(M)}$ 和 $X^{(H)}$ 的每一列都是方差为 $1$ 的独立标准化随机变量，并且 $X^{(M)}$、$X^{(H)}$ 和 $\\varepsilon$ 相互独立，$n$ 足够大以至于总体方差表达式适用。\n\n在模型假设下，将 DNA 甲基化的组特征重要性定义为 $y$ 中可归因于 DNA 甲基化的方差比例：\n$$\nI_M \\;=\\; \\frac{\\mathrm{Var}\\!\\left(X^{(M)} w^{(M)}\\right)}{\\mathrm{Var}(y)}.\n$$\n类似地，将组蛋白修饰的组特征重要性定义为\n$$\nI_H \\;=\\; \\frac{\\mathrm{Var}\\!\\left(X^{(H)} w^{(H)}\\right)}{\\mathrm{Var}(y)}.\n$$\n在所述假设下，这些量可以简化为以下用系数向量和噪声方差表示的表达式：\n$$\n\\mathrm{Var}\\!\\left(X^{(M)} w^{(M)}\\right) \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2,\\quad\n\\mathrm{Var}\\!\\left(X^{(H)} w^{(H)}\\right) \\;=\\; \\left\\| w^{(H)} \\right\\|_2^2,\\quad\n\\mathrm{Var}(y) \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2.\n$$\n您的任务是编写一个程序，对于下方测试套件中的每个参数集，使用上述定义计算 $I_M$ 和 $I_H$，并根据以下规则（容差 $\\delta = 10^{-9}$）为每种情况输出一个整数决策：\n- 如果 $I_M - I_H \\ge \\delta$，输出 $1$。\n- 如果 $I_H - I_M \\ge \\delta$，输出 $-1$。\n- 否则输出 $0$。\n\n测试套件（每种情况指定了 $w^{(M)}$、$w^{(H)}$ 和 $\\sigma$）：\n- 情况 1：$w^{(M)} = (1.0,\\, 0.8,\\, 0.0,\\, 0.0,\\, 0.0)$，$w^{(H)} = (0.3,\\, 0.0,\\, 0.0,\\, 0.0)$，$\\sigma = 0.5$。\n- 情况 2：$w^{(M)} = (0.2,\\, 0.0,\\, 0.0)$，$w^{(H)} = (0.9,\\, 0.7,\\, 0.0,\\, 0.0)$，$\\sigma = 0.4$。\n- 情况 3：$w^{(M)} = (0.5,\\, 0.5)$，$w^{(H)} = (0.7071067811865476,\\, 0.0)$，$\\sigma = 0.0$。\n- 情况 4：$w^{(M)} = (0.0,\\, 0.0,\\, 0.0)$，$w^{(H)} = (0.2,\\, 0.2,\\, 0.2)$，$\\sigma = 1.0$。\n- 情况 5：$w^{(M)} = (0.0,\\, 0.0)$，$w^{(H)} = (0.0,\\, 0.0)$，$\\sigma = 0.3$。\n\n您的程序应生成单行输出，其中包含五个情况的结果，形式为用方括号括起来的逗号分隔的整数列表（例如，$[1,-1,0,-1,0]$）。不允许有任何额外的输出或空白字符。不涉及角度。不涉及物理单位。所有数值计算必须使用标准实数算术进行。", "solution": "所提出的问题是有效的，因为它具有科学依据、提法明确且客观。它基于一个简化但合理的基因调控统计模型提出了一个清晰的计算任务，这在计算生物学中是一种标准方法。定义、假设和测试用例是完备且内部一致的。我们将继续提供解决方案。\n\n目标是比较两个特征组，即 DNA 甲基化 ($M$) 和组蛋白修饰 ($H$)，对生物响应变量基因表达 ($y$) 的预测贡献。这是通过量化每个特征组所能解释的 $y$ 的方差比例来实现的。这是模型解释中的一种基本技术。\n\n问题为基因表达 $y$ 定义了一个线性模型：\n$$\ny \\;=\\; X^{(M)} w^{(M)} \\;+\\; X^{(H)} w^{(H)} \\;+\\; \\varepsilon\n$$\n这里，$y$ 是中心化的响应向量。项 $X^{(M)} w^{(M)}$ 和 $X^{(H)} w^{(H)}$ 分别代表来自 DNA 甲基化和组蛋白修饰特征的贡献，由固定的系数向量 $w^{(M)}$ 和 $w^{(H)}$ 加权。项 $\\varepsilon$ 代表随机噪声，假设每个样本的噪声是独立同分布的，均值为 $0$，方差为 $\\sigma^2$。\n\n一个关键假设是特征矩阵 $X^{(M)}$ 和 $X^{(H)}$ 的列是独立的、标准化的随机变量（均值为 $0$，方差为 $1$）。此外，特征组和噪声是相互独立的。在这些条件下，响应 $y$ 的总方差可以进行可加性分解，这是源于 Bienaymé 公式（用于不相关随机变量和的方差）的一个性质。\n\n对于单个样本 $y_i$，其方差为：\n$$\n\\mathrm{Var}(y_i) \\;=\\; \\mathrm{Var}\\left(\\sum_{j} X_{ij}^{(M)} w_{j}^{(M)}\\right) \\;+\\; \\mathrm{Var}\\left(\\sum_{k} X_{ik}^{(H)} w_{k}^{(H)}\\right) \\;+\\; \\mathrm{Var}(\\varepsilon_i)\n$$\n由于特征 $X_{ij}$ 的独立性和单位方差：\n$$\n\\mathrm{Var}\\left(\\sum_{j} X_{ij}^{(M)} w_{j}^{(M)}\\right) \\;=\\; \\sum_{j} (w_{j}^{(M)})^2 \\mathrm{Var}(X_{ij}^{(M)}) \\;=\\; \\sum_{j} (w_{j}^{(M)})^2 \\cdot 1 \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2\n$$\n类似地，$\\mathrm{Var}\\left(\\sum_k X_{ik}^{(H)} w_k^{(H)}\\right) = \\left\\| w^{(H)} \\right\\|_2^2$。噪声方差给定为 $\\mathrm{Var}(\\varepsilon_i) = \\sigma^2$。\n因此，$y$ 的总方差正确地给出为：\n$$\n\\mathrm{Var}(y) \\;=\\; \\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2\n$$\n$L_2$范数的平方 $\\left\\| w \\right\\|_2^2$ 计算为向量 $w$ 的元素平方和。\n\n然后计算组特征重要性，它被定义为总方差的比例。对于 DNA 甲基化：\n$$\nI_M \\;=\\; \\frac{\\mathrm{Var}(X^{(M)} w^{(M)})}{\\mathrm{Var}(y)} \\;=\\; \\frac{\\left\\| w^{(M)} \\right\\|_2^2}{\\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2}\n$$\n对于组蛋白修饰：\n$$\nI_H \\;=\\; \\frac{\\mathrm{Var}(X^{(H)} w^{(H)})}{\\mathrm{Var}(y)} \\;=\\; \\frac{\\left\\| w^{(H)} \\right\\|_2^2}{\\left\\| w^{(M)} \\right\\|_2^2 \\;+\\; \\left\\| w^{(H)} \\right\\|_2^2 \\;+\\; \\sigma^2}\n$$\n计算任务是为每个提供的参数集评估这些量，并应用决策规则。对于每种情况，我们将计算 $\\left\\| w^{(M)} \\right\\|_2^2$、$\\left\\| w^{(H)} \\right\\|_2^2$ 和 $\\sigma^2$ 的值。让它们分别为 $V_M$、$V_H$ 和 $V_{\\text{noise}}$。总方差为 $V_{\\text{total}} = V_M + V_H + V_{\\text{noise}}$。那么，$I_M = V_M / V_{\\text{total}}$ 且 $I_H = V_H / V_{\\text{total}}$。注意，如果 $V_{\\text{total}} = 0$（仅当所有权重和 $\\sigma$ 都为零时发生），这些重要性是未定义的。这种情况在给定的测试套件中不会出现。\n\n最终决策基于差值 $D = I_M - I_H$，容差为 $\\delta = 10^{-9}$：\n1. 如果 $D \\ge \\delta$，则判定甲基化的贡献更大。输出为 $1$。\n2. 如果 $-D \\ge \\delta$（等价于 $D \\le -\\delta$），则判定组蛋白的贡献更大。输出为 $-1$。\n3. 否则，如果 $|D|  \\delta$，则认为在给定容差内它们的贡献无法区分。输出为 $0$。\n\n将对每个测试用例实施此程序，以生成最终的决策列表。例如，对于情况1：$w^{(M)} = (1.0, 0.8, \\dots)$，$w^{(H)} = (0.3, \\dots)$，$\\sigma = 0.5$。\n$V_M = 1.0^2 + 0.8^2 = 1.64$。\n$V_H = 0.3^2 = 0.09$。\n$V_{\\text{noise}} = 0.5^2 = 0.25$。\n$V_{\\text{total}} = 1.64 + 0.09 + 0.25 = 1.98$。\n$I_M = 1.64 / 1.98$，$I_H = 0.09 / 1.98$。\n$I_M - I_H = (1.64 - 0.09) / 1.98 = 1.55 / 1.98 \\approx 0.7828$。由于 $0.7828 \\ge 10^{-9}$，输出为 $1$。该算法对所有情况都以相同方式进行。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares group feature importances for DNA methylation and histone modifications\n    based on a generative linear model.\n    \"\"\"\n    # Test suite with parameter sets (w_M, w_H, sigma).\n    test_cases = [\n        # Case 1\n        (np.array([1.0, 0.8, 0.0, 0.0, 0.0]), np.array([0.3, 0.0, 0.0, 0.0]), 0.5),\n        # Case 2\n        (np.array([0.2, 0.0, 0.0]), np.array([0.9, 0.7, 0.0, 0.0]), 0.4),\n        # Case 3\n        (np.array([0.5, 0.5]), np.array([0.7071067811865476, 0.0]), 0.0),\n        # Case 4\n        (np.array([0.0, 0.0, 0.0]), np.array([0.2, 0.2, 0.2]), 1.0),\n        # Case 5\n        (np.array([0.0, 0.0]), np.array([0.0, 0.0]), 0.3),\n    ]\n\n    # Tolerance for comparison.\n    delta = 1e-9\n\n    results = []\n    \n    for w_M, w_H, sigma in test_cases:\n        # Calculate the variance component for DNA methylation: ||w^(M)||_2^2\n        v_M = np.dot(w_M, w_M)\n        \n        # Calculate the variance component for histone modifications: ||w^(H)||_2^2\n        v_H = np.dot(w_H, w_H)\n        \n        # Calculate the variance component for noise: sigma^2\n        v_noise = sigma**2\n        \n        # Calculate the total variance of y.\n        v_total = v_M + v_H + v_noise\n        \n        # Calculate group feature importances I_M and I_H.\n        # Handle the edge case where total variance is zero, though not in the test data.\n        if v_total == 0.0:\n            i_M = 0.0\n            i_H = 0.0\n        else:\n            i_M = v_M / v_total\n            i_H = v_H / v_total\n            \n        # Calculate the difference in importances.\n        diff = i_M - i_H\n        \n        # Apply the decision rule.\n        if diff >= delta:\n            # Importance of methylation is significantly greater.\n            decision = 1\n        elif diff = -delta:\n            # Importance of histone modifications is significantly greater.\n            decision = -1\n        else:\n            # Importances are not significantly different.\n            decision = 0\n            \n        results.append(decision)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2400021"}, {"introduction": "在特征重要性的概念之上，本次实践将解决一个更具挑战性和现实意义的问题：检测模型何时依赖于虚假的伪影而非真实的生物信号，这种现象有时被称为“聪明的汉斯”效应。你将创建一个带有内置混杂因素（实验批次效应）的合成数据集，并结合使用性能评估和排列特征重要性来诊断这一关键问题。这项练习对于培养从真实世界生物数据中构建稳健且可信赖模型的能力至关重要 [@problem_id:2400032]。", "problem": "您的任务是设计一个程序，用于揭示一种“聪明的汉斯”效应，在这种效应中，预测模型使用实验批次的人为因素（artifacts）而非生物信号进行预测。您的实现必须使用基于第一性原理的可解释性工具来检测这种行为。目标是设计一个能够反映合理生物数据生成过程的合成场景，然后从第一性原理出发，检测一个在经验风险最小化下训练的线性分类器是否主要依赖于批次伪影而非生物信号。\n\n从以下基础出发：\n- 经验风险最小化通过优化模型参数矢量来最小化经验损失。对于二元分类，一个常见的选择是逻辑斯谛损失。对于样本 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{0,1\\}$ 且特征 $x_i \\in \\mathbb{R}^d$，一个权重为 $w \\in \\mathbb{R}^d$、偏置为 $b \\in \\mathbb{R}$ 的逻辑斯谛回归旨在最小化\n$$\n\\mathcal{L}(w,b) \\;=\\; \\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log\\left(1-\\sigma(w^\\top x_i + b)\\right)\\right] \\;+\\; \\frac{\\lambda}{2}\\|w\\|_2^2,\n$$\n其中 $\\sigma(z) = \\frac{1}{1+e^{-z}}$，$\\lambda \\ge 0$ 是正则化强度。\n- 批次伪影是一种非生物来源的变异，此处建模为一个可以与标签产生伪相关的二元批次指示符。设 $y \\in \\{0,1\\}$ 为生物表型。设 $b \\in \\{0,1\\}$ 为批次分配。定义一个混杂参数 $c \\in [0,1]$，使得\n$$\n\\mathbb{P}(b = y) = c, \\quad \\mathbb{P}(b \\ne y) = 1-c.\n$$\n因此，当 $c = 1$ 时，批次与标签完全混杂；当 $c = \\tfrac{1}{2}$ 时，批次与标签无关。\n- 生物特征和批次特征按如下方式生成。假设有 $p_b$ 个生物特征和 $p_a$ 个批次特征。对于每个样本 $i$：\n    - 抽取 $y_i \\sim \\mathrm{Bernoulli}(0.5)$。\n    - 抽取 $b_i$，使得 $b_i = y_i$ 的概率为 $c$，$b_i = 1-y_i$ 的概率为 $1-c$。\n    - 生物特征向量 $x^{(bio)}_i \\in \\mathbb{R}^{p_b}$ 从一个类别相关的均值分布中抽取，$x^{(bio)}_i \\sim \\mathcal{N}(\\mu^{(bio)}(y_i), \\sigma^2 I)$，其中 $\\mu^{(bio)}(1)=+\\mu_b \\cdot \\mathbf{1}$ 且 $\\mu^{(bio)}(0)=-\\mu_b \\cdot \\mathbf{1}$。\n    - 批次特征向量 $x^{(art)}_i \\in \\mathbb{R}^{p_a}$ 从一个批次相关的均值分布中抽取，$x^{(art)}_i \\sim \\mathcal{N}(\\mu^{(art)}(b_i), \\sigma^2 I)$，其中 $\\mu^{(art)}(1)=+\\mu_a \\cdot \\mathbf{1}$ 且 $\\mu^{(art)}(0)=-\\mu_a \\cdot \\mathbf{1}$。\n    - 拼接形成 $x_i = [x^{(bio)}_i, x^{(art)}_i] \\in \\mathbb{R}^{p_b + p_a}$。\n- 对于验证集 $\\mathcal{D}_{val}$ 上的一组特征 $G$，其置换特征重要性定义为当 $G$ 中的特征在样本间进行置换（从而打破它们与目标之间的依赖结构）时，所选性能指标的下降值。以准确率作为指标，组重要性为\n$$\nI(G) = \\mathrm{Acc}(\\mathcal{D}_{val}) - \\mathrm{Acc}(\\pi_G(\\mathcal{D}_{val})),\n$$\n其中 $\\pi_G(\\cdot)$ 表示对验证集中由 $G$ 索引的列在样本间进行独立的随机置换。\n\n您的程序必须：\n1. 根据上述过程生成数据，包括一个来自参数为 $c_{train} \\in [0,1]$ 的混杂分布的训练集和验证集，以及一个单独的 $c_{test} = \\tfrac{1}{2}$ 的去混杂测试集。\n2. 通过在训练集上最小化带 $\\ell_2$ 惩罚（强度为 $\\lambda  0$）的正则化逻辑斯谛损失 $\\mathcal{L}(w,b)$，来训练一个正则化逻辑斯谛回归分类器。\n3. 计算在来自混杂分布的验证集上的验证准确率，以及在去混杂测试集上的测试准确率。\n4. 在验证集上计算生物特征组 $G_{bio}$ 和批次伪影组 $G_{art}$ 的组置换特征重要性 $I(G_{bio})$ 和 $I(G_{art})$。\n5. 如果以下两个条件同时成立，则检测到“聪明的汉斯”效应：\n   - 批次伪影组的重要性严格大于生物特征组的重要性，且差值至少为一个小的边际值 $\\epsilon  0$，即 $I(G_{art}) \\ge I(G_{bio}) + \\epsilon$。\n   - 从混杂验证集到去混杂测试集的泛化差距超过一个阈值 $\\Delta  0$，即 $\\mathrm{Acc}_{val} - \\mathrm{Acc}_{test} \\ge \\Delta$。\n\n使用固定值 $\\lambda = 1$，$\\epsilon = 0.01$，$\\Delta = 0.2$ 和高斯噪声标准差 $\\sigma = 1$。使用确定性分割，训练集比例为 0.6，验证集比例为 0.4。对于每个数据集，使用指定的整数随机种子来构建一个 NumPy 随机数生成器。\n\n用 Python 实现您的解决方案，并在以下测试套件上进行评估，其中每个元组编码为 $(N, p_b, p_a, \\mu_b, \\mu_a, c_{train}, seed)$：\n- 情况 A (强混杂，强伪影，弱生物信号)：$(4000, 50, 2, 0.3, 3.0, 0.95, 42)$。\n- 情况 B (无混杂，强生物信号)：$(4000, 50, 2, 1.5, 3.0, 0.5, 43)$。\n- 情况 C (部分混杂，生物信号占主导)：$(4000, 50, 2, 1.5, 1.0, 0.7, 44)$。\n- 情况 D (完全混杂，无生物信号)：$(3000, 10, 10, 0.0, 2.0, 1.0, 45)$。\n- 情况 E (小样本，强混杂，强伪影)：$(800, 20, 1, 0.3, 2.5, 0.95, 46)$。\n\n您的程序必须产生单行输出，其中包含一个用方括号括起来的、逗号分隔的布尔值列表，表示每种情况下是否检测到“聪明的汉斯”效应。例如，输出格式必须与以下完全一样：\n\"[True,False,True,False,True]\"\n\n不涉及物理单位。不涉及角度。当描述分数时，它们在代码中必须作为小数处理。\n\n约束条件：\n- 您必须通过对上述定义的正则化逻辑斯谛损失进行数值优化来实现训练。\n- 置换重要性必须通过在验证样本中联合置换组内所有特征来计算。\n- 去混杂测试集必须使用 $c_{test} = 0.5$ 并且大小与验证集相同。\n- 仅可根据优化需要使用 Python 标准库、NumPy 和 SciPy。最终代码必须无需用户输入即可运行，并且只打印所需的单行输出。", "solution": "该问题要求设计并实现一个程序，用以在一个模拟的生物分类场景中检测“聪明的汉斯”效应。当预测模型利用虚假相关性（例如实验批次伪影）而不是真实的生物信号来进行预测时，就会出现这种效应。解决方案涉及一个三阶段过程：首先，生成模拟此现象的合成数据；其次，在该数据上训练一个分类器；第三，应用基于第一性原理的可解释性方法来诊断模型的行为。\n\n首先，我们形式化数据生成过程。我们考虑一个二元分类任务，其中每个样本 $i$ 都有一个真实的生物学标签 $y_i \\in \\{0, 1\\}$，该标签从参数为 0.5 的伯努利分布中抽取。每个样本还关联一个批次分配 $b_i \\in \\{0, 1\\}$。批次分配与生物学标签之间的相关性由一个混杂参数 $c \\in [0, 1]$ 控制，使得批次与标签匹配的概率为 $\\mathbb{P}(b_i = y_i) = c$。$c=0.5$ 的值表示没有混杂（独立），而 $c=1.0$ 的值表示完全混杂。每个样本的特征向量 $x_i \\in \\mathbb{R}^{p_b + p_a}$ 由两部分组成：一个生物学特征向量 $x^{(bio)}_i \\in \\mathbb{R}^{p_b}$ 和一个批次伪影特征向量 $x^{(art)}_i \\in \\mathbb{R}^{p_a}$。生物学特征从一个类别条件的高斯分布中抽取：$x^{(bio)}_i \\sim \\mathcal{N}(\\mu^{(bio)}(y_i), \\sigma^2 I)$，其中均值向量 $\\mu^{(bio)}(y_i)$ 取决于真实标签。具体来说，$\\mu^{(bio)}(1) = +\\mu_b \\cdot \\mathbf{1}$ 和 $\\mu^{(bio)}(0) = -\\mu_b \\cdot \\mathbf{1}$，其中 $\\mu_b$ 控制生物信号的强度。类似地，伪影特征从一个批次条件的高斯分布中抽取：$x^{(art)}_i \\sim \\mathcal{N}(\\mu^{(art)}(b_i), \\sigma^2 I)$，其中均值 $\\mu^{(art)}(b_i)$ 取决于批次分配。此处，$\\mu^{(art)}(1) = +\\mu_a \\cdot \\mathbf{1}$ 和 $\\mu^{(art)}(0) = -\\mu_a \\cdot \\mathbf{1}$，其中 $\\mu_a$ 控制伪影信号的强度。最终的特征向量是两者的拼接 $x_i = [x^{(bio)}_i, x^{(art)}_i]$。\n\n其次，我们在生成的训练数据上训练一个逻辑斯谛回归分类器。该模型通过最小化正则化的负对数似然（一种经验风险最小化的形式）来学习一个权重向量 $w \\in \\mathbb{R}^{p_b+p_a}$ 和一个偏置项 $b \\in \\mathbb{R}$。目标函数 $\\mathcal{L}(w, b)$，也称为带 $\\ell_2$ 惩罚的逻辑斯谛损失或交叉熵损失，由以下公式给出：\n$$\n\\mathcal{L}(w,b) \\;=\\; \\sum_{i=1}^n \\left[-y_i \\log \\sigma(w^\\top x_i + b) - (1-y_i)\\log\\left(1-\\sigma(w^\\top x_i + b)\\right)\\right] \\;+\\; \\frac{\\lambda}{2}\\|w\\|_2^2\n$$\n这里，$\\sigma(z) = (1+e^{-z})^{-1}$ 是 sigmoid 函数，$\\lambda  0$ 是正则化参数，用于惩罚过大的权重以防止过拟合。这个优化问题是凸的，其最小值可以使用诸如 L-BFGS 之类的拟牛顿法高效地找到。为此，我们必须提供目标函数相对于参数的梯度。对于单个样本 $(x_i, y_i)$，损失项关于权重 $w$ 的梯度是 $(\\sigma(w^\\top x_i + b) - y_i)x_i$，关于偏置 $b$ 的梯度是 $(\\sigma(w^\\top x_i + b) - y_i)$。正则化项的梯度是 $\\lambda w$。对所有样本求和得到用于优化的完整梯度。\n\n第三，我们设计了一个由两部分组成的标准来检测“聪明的汉斯”行为。\n第一部分衡量模型从混杂环境泛化到去混杂环境的失败程度。我们在两个集合上评估训练好的模型的准确率：一个是验证集，它与训练数据一样从相同的混杂分布中抽取（混杂参数为 $c_{train}$）；另一个是测试集，它从一个去混杂的分布中抽取，其中批次伪影与标签无关（$c_{test} = 0.5$）。准确率的显著下降，即 $\\mathrm{Acc}_{val} - \\mathrm{Acc}_{test} \\ge \\Delta$（其中 $\\Delta$ 是一个预定义阈值），表明模型学到了一个仅在混杂环境中才有效的捷径。\n\n第二部分使用一种可解释性技术——置换特征重要性，来验证伪影特征确实是这种依赖性的来源。一组特征 $G$ 的重要性，记为 $I(G)$，是当这些特征与目标标签之间的关联被打破时，模型在验证集上准确率的下降值。这是通过在验证集的所有样本中置换 $G$ 组特征的值来实现的。其重要性为 $I(G) = \\mathrm{Acc}_{val} - \\mathrm{Acc}_{\\pi_G(val)}$，其中 $\\mathrm{Acc}_{\\pi_G(val)}$ 是在特征被置换后的数据上的准确率。我们计算生物特征组的重要性 $I(G_{bio})$ 和伪影特征组的重要性 $I(G_{art})$。如果模型是一个“聪明的汉斯”，它将更严重地依赖于伪影特征。这种依赖性由条件 $I(G_{art}) \\ge I(G_{bio}) + \\epsilon$ 捕捉，其中 $\\epsilon  0$ 是一个小的边际值，以确保差异不容忽视。\n\n当且仅当这两个条件都满足时，才正式检测到“聪明的汉斯”效应：模型显示出显著的泛化差距，并且其性能被证明更依赖于伪影特征而非生物学特征。这种基于原则的方法将性能评估与对模型内部逻辑的直接探询相结合，为诊断虚假学习提供了一种稳健的方法。该实现将系统地将此过程应用于每个指定的测试用例，并使用所提供的参数和随机种子以确保可复现性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the \"Clever Hans\" detection analysis on a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (N, p_b, p_a, mu_b, mu_a, c_train, seed)\n        (4000, 50, 2, 0.3, 3.0, 0.95, 42),  # Case A\n        (4000, 50, 2, 1.5, 3.0, 0.5, 43),   # Case B\n        (4000, 50, 2, 1.5, 1.0, 0.7, 44),   # Case C\n        (3000, 10, 10, 0.0, 2.0, 1.0, 45),  # Case D\n        (800, 20, 1, 0.3, 2.5, 0.95, 46),    # Case E\n    ]\n\n    # Fixed parameters from the problem statement\n    LAMBDA = 1.0\n    EPSILON = 0.01\n    DELTA = 0.2\n    SIGMA = 1.0\n    TRAIN_FRAC = 0.6\n    C_TEST = 0.5\n\n    results = []\n    for params in test_cases:\n        result = _run_case(params, TRAIN_FRAC, C_TEST, SIGMA, LAMBDA, EPSILON, DELTA)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _generate_data(N, p_b, p_a, mu_b, mu_a, c, sigma, rng):\n    \"\"\"\n    Generates synthetic data according to the specified model.\n    \"\"\"\n    # 1. Draw biological labels\n    y = rng.binomial(1, 0.5, size=N)\n\n    # 2. Draw batch assignments based on confounding parameter c\n    u = rng.random(size=N)\n    b = np.where(u  c, y, 1 - y)\n\n    # 3. Generate biological features\n    mean_bio = np.where(y[:, np.newaxis] == 1, mu_b, -mu_b)\n    X_bio = mean_bio + rng.normal(0, sigma, size=(N, p_b))\n\n    # 4. Generate artifact features\n    mean_art = np.where(b[:, np.newaxis] == 1, mu_a, -mu_a)\n    X_art = mean_art + rng.normal(0, sigma, size=(N, p_a))\n    \n    # 5. Concatenate features\n    X = np.hstack([X_bio, X_art])\n    \n    return X, y\n\ndef _cost_function(theta, X, y, lambda_reg):\n    \"\"\"\n    Computes the regularized logistic loss.\n    \"\"\"\n    m = X.shape[0]\n    X_aug = np.hstack([X, np.ones((m, 1))])\n    z = X_aug @ theta\n    \n    # Sigmoid function, clipped for numerical stability\n    h = 1 / (1 + np.exp(-z))\n    h = np.clip(h, 1e-10, 1 - 1e-10)\n\n    log_loss = -np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n    reg_term = (lambda_reg / 2) * np.sum(theta[:-1]**2)\n\n    return log_loss + reg_term\n\ndef _gradient_function(theta, X, y, lambda_reg):\n    \"\"\"\n    Computes the gradient of the regularized logistic loss.\n    \"\"\"\n    m = X.shape[0]\n    X_aug = np.hstack([X, np.ones((m, 1))])\n    z = X_aug @ theta\n    h = 1 / (1 + np.exp(-z))\n    \n    error = h - y\n    grad = X_aug.T @ error\n    \n    # Add gradient of regularization term (bias is not regularized)\n    grad[:-1] += lambda_reg * theta[:-1]\n    \n    return grad\n\ndef _train_logistic_regression(X_train, y_train, lambda_reg):\n    \"\"\"\n    Trains a logistic regression model using L-BFGS optimization.\n    \"\"\"\n    d = X_train.shape[1]\n    theta_initial = np.zeros(d + 1)\n    \n    res = minimize(\n        _cost_function,\n        theta_initial,\n        args=(X_train, y_train, lambda_reg),\n        jac=_gradient_function,\n        method='L-BFGS-B'\n    )\n    return res.x\n\ndef _predict(X, theta):\n    \"\"\"\n    Makes predictions using the trained logistic regression model.\n    \"\"\"\n    m = X.shape[0]\n    X_aug = np.hstack([X, np.ones((m, 1))])\n    scores = X_aug @ theta\n    return (scores > 0).astype(int)\n\ndef _calculate_accuracy(y_true, y_pred):\n    \"\"\"\n    Calculates prediction accuracy.\n    \"\"\"\n    return np.mean(y_true == y_pred)\n\ndef _calculate_permutation_importance(X_val, y_val, theta, group_indices, rng):\n    \"\"\"\n    Calculates permutation importance for a given feature group.\n    \"\"\"\n    # Baseline accuracy\n    y_pred_val = _predict(X_val, theta)\n    acc_val = _calculate_accuracy(y_val, y_pred_val)\n\n    # Permuted accuracy\n    X_perm = X_val.copy()\n    n_val = X_val.shape[0]\n    \n    # Jointly permute the feature group columns across samples\n    perm_indices = rng.permutation(n_val)\n    X_perm[:, group_indices] = X_perm[perm_indices, :][:, group_indices]\n    \n    y_pred_perm = _predict(X_perm, theta)\n    acc_perm = _calculate_accuracy(y_val, y_pred_perm)\n    \n    return acc_val - acc_perm\n\ndef _run_case(params, train_frac, c_test, sigma, lambda_reg, epsilon, delta):\n    \"\"\"\n    Executes one full test case for \"Clever Hans\" detection.\n    \"\"\"\n    N, p_b, p_a, mu_b, mu_a, c_train, seed = params\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate confounded data and split\n    X, y = _generate_data(N, p_b, p_a, mu_b, mu_a, c_train, sigma, rng)\n    n_train = int(N * train_frac)\n    X_train, y_train = X[:n_train], y[:n_train]\n    X_val, y_val = X[n_train:], y[n_train:]\n    n_val = X_val.shape[0]\n\n    # 2. Train the model\n    theta = _train_logistic_regression(X_train, y_train, lambda_reg)\n\n    # 3. Compute validation and test accuracies\n    y_pred_val = _predict(X_val, theta)\n    acc_val = _calculate_accuracy(y_val, y_pred_val)\n    \n    X_test, y_test = _generate_data(n_val, p_b, p_a, mu_b, mu_a, c_test, sigma, rng)\n    y_pred_test = _predict(X_test, theta)\n    acc_test = _calculate_accuracy(y_test, y_pred_test)\n\n    # 4. Compute permutation feature importances\n    bio_indices = list(range(p_b))\n    art_indices = list(range(p_b, p_b + p_a))\n    \n    imp_bio = _calculate_permutation_importance(X_val, y_val, theta, bio_indices, rng)\n    imp_art = _calculate_permutation_importance(X_val, y_val, theta, art_indices, rng)\n\n    # 5. Check for \"Clever Hans\" effect\n    cond1 = imp_art >= imp_bio + epsilon\n    cond2 = acc_val - acc_test >= delta\n    \n    is_clever_hans = cond1 and cond2\n    return is_clever_hans\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2400032"}, {"introduction": "超越了分析现有预测的范畴，这个练习引入了强大的反事实解释概念，它允许我们提出“如果……会怎样”的问题。你将使用一个简单的肽结合模型，任务是找出一个非结合肽需要进行哪些最小化的改动才能实现结合。这项实践展示了可解释模型不仅可以用于理解，还可以用于生成新的科学假设和指导新的生物序列设计 [@problem_id:2399963]。", "problem": "给定一个基于位置权重矩阵的、简单且完全可解释的肽结合线性模型。肽字母表由整数索引，一个肽表示为一个定长的整数向量。对于一个长度为 $L$、字母表大小为 $M$ 的肽，其模型分数定义为\n$$\n\\text{score}(\\mathbf{s}) \\;=\\; b \\;+\\; \\sum_{i=0}^{L-1} W_{i,\\, s_i},\n$$\n其中 $\\mathbf{s} = [s_0, s_1, \\dots, s_{L-1}]$ 且每个 $s_i \\in \\{0,1,\\dots,M-1\\}$，$W \\in \\mathbb{R}^{L \\times M}$ 是一个位置权重矩阵，$b \\in \\mathbb{R}$ 是一个标量偏置。当且仅当 $\\text{score}(\\mathbf{s}) \\ge T$ 时，一个肽被认为会结合，其中 $T \\in \\mathbb{R}$ 是一个阈值。两个等长肽 $\\mathbf{s}$ 和 $\\mathbf{t}$ 之间的汉明距离是\n$$\nd_H(\\mathbf{s}, \\mathbf{t}) \\;=\\; \\left|\\left\\{\\, i \\in \\{0,\\dots,L-1\\} \\;:\\; s_i \\ne t_i \\,\\right\\}\\right|.\n$$\n对于给定的非结合肽 $\\mathbf{s}$，其反事实肽是任何满足 $\\text{score}(\\mathbf{s}^\\star) \\ge T$ 同时最小化 $d_H(\\mathbf{s}, \\mathbf{s}^\\star)$ 的肽 $\\mathbf{s}^\\star$。如果存在多个这样的最小化子，必须选择字典序最小的整数向量（使用整数元组的常规顺序）。\n\n您的任务是为下面的每个测试用例计算出根据给定模型能够结合的、具有最小汉明距离且字典序最小的反事实肽 $\\mathbf{s}^\\star$。如果给定的肽已经结合，则返回原始肽。如果对于给定的模型和阈值，没有任何肽能够结合，则返回整数 $-1$。为了可视化的目的，将每个肽表示为其整数编码的序列。\n\n所有测试用例共享相同的模型参数 $L$、 $M$、 $W$ 和 $b$：\n\n- 长度 $L$：$5$。\n- 字母表大小 $M$：$5$。\n- 权重矩阵 $W$ 按位置 $i \\in \\{0,1,2,3,4\\}$ 逐行给出，列出 $W_{i,0}, W_{i,1}, W_{i,2}, W_{i,3}, W_{i,4}$：\n  - 位置 $0$：$(1.0,\\,-0.5,\\,0.2,\\,0.0,\\,-0.2)$。\n  - 位置 $1$：$(0.3,\\,0.1,\\,-0.4,\\,0.8,\\,-0.1)$。\n  - 位置 $2$：$(0.0,\\,0.5,\\,-0.6,\\,0.9,\\,0.2)$。\n  - 位置 $3$：$(-0.2,\\,0.4,\\,0.7,\\,-0.5,\\,0.3)$。\n  - 位置 $4$：$(0.5,\\,-0.2,\\,0.0,\\,0.4,\\,-0.3)$。\n- 偏置 $b$：$-0.5$。\n\n测试套件（每个测试用例指定阈值 $T$ 和起始肽 $\\mathbf{s}$）：\n1. $T = 1.5$，$\\mathbf{s} = [0,3,3,2,0]$。\n2. $T = -1.5$，$\\mathbf{s} = [1,1,2,3,1]$。\n3. $T = 1.0$，$\\mathbf{s} = [4,4,4,4,4]$。\n4. $T = 3.6$，$\\mathbf{s} = [2,2,2,2,2]$。\n\n您的程序必须为每个测试用例计算以下任一结果：\n- 整数编码的反事实肽 $\\mathbf{s}^\\star$，表示为一个包含 $L$ 个整数的列表，或\n- 整数 $-1$，如果对于给定模型没有任何肽能满足 $\\text{score}(\\mathbf{s}) \\ge T$。\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，且无空格。每个结果必须是方括号中的 $L$ 个整数的列表，或者是整数 $-1$。例如，一个包含两个列表结果和一个不可能情况的有效输出看起来像这样：$[[0,1,2,3,4],[4,3,2,1,0],-1]$。", "solution": "所提出的问题是有效的。这是一个定义明确的计算任务，其基础是生物信息学和可解释机器学习中已建立的概念，特别是使用位置权重矩阵（PWM）模型并寻求反事实解释。所有参数和条件都已明确指定，没有歧义或矛盾。\n\n目标是为给定的输入肽 $\\mathbf{s}$ 找到一个反事实肽 $\\mathbf{s}^\\star$。肽是一个整数向量 $\\mathbf{s} = [s_0, s_1, \\dots, s_{L-1}]$，其中 $L$ 是长度，每个 $s_i$ 属于一个大小为 $M$ 的字母表。结合分数由一个线性模型给出：\n$$\n\\text{score}(\\mathbf{s}) \\;=\\; b \\;+\\; \\sum_{i=0}^{L-1} W_{i,\\, s_i}\n$$\n如果一个肽的分数达到或超过阈值 $T$，即 $\\text{score}(\\mathbf{s}) \\ge T$，则它会结合。对于非结合的 $\\mathbf{s}$，其反事实肽 $\\mathbf{s}^\\star$ 必须满足两个层级标准：\n1. 汉明距离 $d_H(\\mathbf{s}, \\mathbf{s}^\\star)$ 必须最小化。\n2. 在所有满足第一个标准的肽中，$\\mathbf{s}^\\star$ 必须是字典序最小的。\n\n如果输入肽 $\\mathbf{s}$ 已经结合，则解就是 $\\mathbf{s}$ 本身。如果没有肽能达到阈值分数 $T$，则解用整数 $-1$ 表示。\n\n目标的分层性质决定了需要采用一种结构化的迭代搜索策略。主要目标是最小化汉明距离 $d = d_H(\\mathbf{s}, \\mathbf{s}^\\star)$。因此，我们必须通过系统地考虑与 $\\mathbf{s}$ 的汉明距离递增的所有肽来搜索有效的反事实，从 $d=0$ 开始，然后是 $d=1, 2, \\dots, L$。根据定义，第一个存在至少一个有效反事实的距离 $d$ 就是最小距离。\n\n整体算法流程如下：\n\n首先，对于给定的测试用例 $(\\mathbf{s}, T)$，我们计算原始肽的分数 $\\text{score}(\\mathbf{s})$。如果 $\\text{score}(\\mathbf{s}) \\ge T$，则该肽已经结合，$\\mathbf{s}$ 就是解。\n\n其次，我们检查问题的一般可行性。我们通过将偏置 $b$ 与每个位置的最大权重相加来计算最大可能分数 $\\text{score}_{\\text{max}}$：\n$$\n\\text{score}_{\\text{max}} = b + \\sum_{i=0}^{L-1} \\max_{j \\in \\{0, \\dots, M-1\\}} W_{i,j}\n$$\n如果 $\\text{score}_{\\text{max}}  T$，则没有任何肽能达到结合阈值，问题无解。在这种情况下，解为 $-1$。\n\n第三，如果原始肽不结合但存在解的可能性，我们从 $d=1$ 开始，对汉明距离 $d$ 进行迭代搜索。对于每个 $d$ 值：\n1. 我们生成所有满足 $d_H(\\mathbf{s}, \\mathbf{s}') = d$ 的候选肽 $\\mathbf{s}'$ 的完整集合。这个生成过程是组合式的。我们首先从 $L$ 个可用位置中选择所有 $d$ 个位置的组合进行突变。对于每个这样的位置组合，我们接着考虑这些特定位点的所有可能的新残基分配。每一种位置选择和相应的新残基都定义了一个候选肽 $\\mathbf{s}'$。\n2. 对于每个候选肽 $\\mathbf{s}'$，我们计算其分数。\n3. 我们将此步骤中所有满足结合条件 $\\text{score}(\\mathbf{s}') \\ge T$ 的候选肽组成一个列表。\n\n如果在检查完给定 $d$ 的所有候选肽后，这个有效反事实的列表非空，那么我们对最小距离的搜索就完成了。问题接着要求我们从这个列表中选择字典序最小的向量。这个向量就是最终答案 $\\mathbf{s}^\\star$，然后我们可以终止搜索。由于我们按 $d$ 的递增顺序进行迭代，这个过程保证了最小汉明距离，并在此基础上保证了字典序最小。对每个测试用例重复此过程。", "answer": "```python\nimport numpy as np\nfrom itertools import combinations, product\n\ndef solve():\n    \"\"\"\n    Computes the lexicographically smallest counterfactual peptide of minimal\n    Hamming distance for a set of test cases.\n    \"\"\"\n    # Shared model parameters\n    L = 5  # Peptide length\n    M = 5  # Alphabet size\n    W = np.array([\n        [1.0, -0.5, 0.2, 0.0, -0.2],  # Position 0\n        [0.3, 0.1, -0.4, 0.8, -0.1],  # Position 1\n        [0.0, 0.5, -0.6, 0.9, 0.2],  # Position 2\n        [-0.2, 0.4, 0.7, -0.5, 0.3],  # Position 3\n        [0.5, -0.2, 0.0, 0.4, -0.3],  # Position 4\n    ])\n    b = -0.5  # Bias\n\n    # Test suite\n    test_cases = [\n        (1.5, [0, 3, 3, 2, 0]),\n        (-1.5, [1, 1, 2, 3, 1]),\n        (1.0, [4, 4, 4, 4, 4]),\n        (3.6, [2, 2, 2, 2, 2]),\n    ]\n\n    def calculate_score(peptide):\n        \"\"\"Calculates the binding score of a peptide.\"\"\"\n        score = b\n        for i in range(L):\n            score += W[i, peptide[i]]\n        return score\n\n    results = []\n    for T, s in test_cases:\n        # Check if the original peptide already binds\n        if calculate_score(s) >= T:\n            results.append(s)\n            continue\n\n        # Check if any peptide can possibly bind\n        max_score = b + np.sum(np.max(W, axis=1))\n        if max_score  T:\n            results.append(-1)\n            continue\n\n        found_solution = False\n        # Iterate through Hamming distances d = 1, 2, ..., L\n        for d in range(1, L + 1):\n            solutions_at_d = []\n            \n            # Generate all combinations of d indices to mutate\n            indices_to_mutate_iter = combinations(range(L), d)\n            \n            for indices in indices_to_mutate_iter:\n                # For each set of indices, generate all possible new characters\n                char_options = []\n                for i in indices:\n                    char_options.append([j for j in range(M) if j != s[i]])\n                \n                new_chars_iter = product(*char_options)\n                \n                for new_chars in new_chars_iter:\n                    s_candidate = list(s)\n                    for k in range(d):\n                        s_candidate[indices[k]] = new_chars[k]\n                    \n                    if calculate_score(s_candidate) >= T:\n                        solutions_at_d.append(s_candidate)\n            \n            if solutions_at_d:\n                # If solutions exist at this distance, find the lexicographically smallest\n                best_solution = min(solutions_at_d)\n                results.append(best_solution)\n                found_solution = True\n                break\n        \n        if not found_solution:\n            # This case should not be reached due to the max_score check\n            results.append(-1)\n            \n    # Format the output as specified\n    def format_result(item):\n        if isinstance(item, int):\n            return str(item)\n        elif isinstance(item, list):\n            return f\"[{','.join(map(str, item))}]\"\n        return \"\"\n\n    print(f\"[{','.join(map(format_result, results))}]\")\n\nsolve()\n```", "id": "2399963"}]}