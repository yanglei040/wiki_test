{"hands_on_practices": [{"introduction": "在免疫组库测序中，一个核心问题是评估测序深度是否足以捕捉样本的真实多样性。由于取样限制，我们观测到的克隆型数量通常只是总数的一部分。稀疏曲线（Rarefaction curve）是一种重要的工具，它通过对现有数据进行二次抽样来预测增加测序深度能发现多少新克隆型，从而帮助我们判断是否需要进一步测序。这项练习将指导你从第一性原理出发，通过计算超几何分布下的期望值来构建稀疏曲线，并应用它来做出科学决策[@problem_id:2399358]。", "problem": "一个实验室对一个淋巴细胞群的免疫受体库进行了测序，并获得了一个包含观测到的克隆型及其读数计数的列表。将测序读数建模为从一个大小为 $N$ 的有限多重集中的无放回抽样，其中 $N$ 是观测到的总读数，每个克隆型 $i$ 出现 $c_i$ 次，且 $c_i \\ge 1$ 和 $\\sum_i c_i = N$。将稀疏化函数定义为当无放回地二次抽样 $n$ 个读数时观测到的不同克隆型数量的期望值，记为 $E[S(n)]$。\n\n你的任务是编写一个程序，对于每个指定的测试用例，根据当前测序深度附近稀疏化曲线的斜率，来判断是否建议进行额外的测序。使用以下科学依据：\n\n- 从有限总体中进行无放回抽样服从超几何分布。\n- 对于每个计数为 $c_i$ 的克隆型 $i$，当 $0 \\le n \\le N$ 时，在一个大小为 $n$ 的二次样本中未观测到其 $c_i$ 个分子中任何一个的概率是二项式系数的比值 $\\dfrac{\\binom{N - c_i}{n}}{\\binom{N}{n}}$。\n- 根据期望的线性性质，$E[S(n)]$ 等于所有克隆型在二次样本中至少被观测到一次的概率之和。\n\n决策规则：给定一个正整数窗口大小 $m$（满足 $1 \\le m \\le N$）和一个非负阈值 $\\alpha$（单位为每个额外读数预期产生的新克隆型数量），计算在最后 $m$ 个读数上预期的克隆丰富度的平均边际增益，\n$$\ng = \\frac{E[S(N)] - E[S(N - m)]}{m}.\n$$\n如果 $g > \\alpha$，则返回布尔值 True（建议测序到更深的深度）；否则返回 False。\n\n仅使用上述假设和标准数学函数来计算 $E[S(n)]$。为确保对于大的 $N$ 值的数值稳定性，请通过阶乘的对数（例如，使用伽马函数的对数）来计算二项式系数的比值。\n\n每个测试用例的输入是：\n- 一个正整数列表 $[c_1,c_2,\\dots,c_R]$，满足 $\\sum_i c_i = N$，\n- 一个整数窗口大小 $m$，满足 $1 \\le m \\le N$，\n- 一个非负实数阈值 $\\alpha$。\n\n你的程序必须处理以下测试套件：\n\n- 测试用例 1：计数 $[25,20,15,10,8,7,5,5,3,2]$，$m = 10$，$\\alpha = 0.05$。\n- 测试用例 2：计数 $[50,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]$（一个有 $50$ 个读数的克隆型和 $50$ 个单例），$m = 10$，$\\alpha = 0.15$。\n- 测试用例 3：计数由 $20$ 个各有 $5$ 个读数的克隆型组成，即 $[5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5]$，$m = 10$，$\\alpha = 0.05$。\n- 测试用例 4：计数 $[100]$，$m = 10$，$\\alpha = 0.001$。\n- 测试用例 5：计数由 $100$ 个单例组成，即 $[1,1,1,\\dots,1]$（含 $100$ 个条目），$m = 10$，$\\alpha = 0.5$。\n\n你的程序应产生单行输出，其中包含测试用例的布尔决策结果，格式为方括号括起来的逗号分隔列表（例如，`[True,False,True]`）。不应打印任何其他文本。", "solution": "问题陈述已经过严格验证，并被认为是有效的。它基于已确立的概率论原理及其在计算生物学中的标准应用，特别是在免疫受体库测序数据的分析中，具有科学依据。该问题是良构的，所有术语、条件和数据都以清晰、一致和客观的方式提供。不存在逻辑矛盾、科学不准确性或歧义。因此，我们可以进行正式的求解。\n\n核心任务是基于克隆型稀疏化曲线的局部斜率来评估一个决定是否继续测序的决策规则。如果平均边际增益 $g$ 超过阈值 $\\alpha$，则决策为 `True`，否则为 `False`。边际增益定义为：\n$$\ng = \\frac{E[S(N)] - E[S(N - m)]}{m}\n$$\n其中 $N$ 是总读数，$m$ 是指定的窗口大小，$E[S(n)]$ 是在无放回随机抽取的 $n$ 个读数的二次样本中观测到的不同克隆型数量的期望值。输入是克隆型计数列表 $[c_1, c_2, \\dots, c_R]$、窗口大小 $m$ 和阈值 $\\alpha$。\n\n我们的推导分两部分进行：首先，评估 $E[S(N)]$；其次，评估 $E[S(N-m)]$。\n\n**1. $E[S(N)]$ 的评估**\n\n$E[S(N)]$ 项代表当从总体中抽样全部 $N$ 个读数时观测到的不同克隆型数量的期望值。根据定义，如果抽样了所有读数，则保证能观测到原始库中存在的每一个克隆型。因此，观测到的克隆型数量确定性地等于 $R$，即不同克隆型的总数。\n$$\nE[S(N)] = R\n$$\n其中 $R$ 就是输入计数列表的长度。这简化了 $g$ 的表达式：\n$$\ng = \\frac{R - E[S(N - m)]}{m}\n$$\n\n**2. 对于通用二次样本大小 $n$ 的 $E[S(n)]$ 评估**\n\n不同克隆型数量的期望值 $E[S(n)]$ 源于期望的线性性质。设 $I_i(n)$ 为一个指示随机变量，如果在大小为 $n$ 的二次样本中观测到克隆型 $i$，则 $I_i(n) = 1$，否则 $I_i(n) = 0$。观测到的克隆型总数为 $S(n) = \\sum_{i=1}^{R} I_i(n)$。其期望为：\n$$\nE[S(n)] = E\\left[\\sum_{i=1}^{R} I_i(n)\\right] = \\sum_{i=1}^{R} E[I_i(n)]\n$$\n指示变量的期望是它所指示事件的概率。\n$$\nE[I_i(n)] = P(\\text{克隆型 } i \\text{ 被观测到}) = 1 - P(\\text{克隆型 } i \\text{ 未被观测到})\n$$\n问题正确地指出抽样是无放回的，这由超几何分布决定。在大小为 $n$ 的二次样本中未观测到克隆型 $i$（其有 $c_i$ 个读数）的概率，是从不属于克隆型 $i$ 的 $N-c_i$ 个读数中选取全部 $n$ 个读数的概率。从 $N$ 个读数中选择 $n$ 个读数的总方式数为 $\\binom{N}{n}$。从不属于 $i$ 的读数中选择 $n$ 个读数的方式数为 $\\binom{N-c_i}{n}$。因此，\n$$\nP(\\text{克隆型 } i \\text{ 未被观测到}) = \\frac{\\binom{N - c_i}{n}}{\\binom{N}{n}}\n$$\n这在 $0 \\le n \\le N-c_i$ 时有效。如果 $n > N-c_i$，则不可能避免从克隆型 $i$ 中至少选择一个读数，所以未观测到它的概率为 $0$。\n\n综合这些结果，我们得到稀疏化曲线的公式：\n$$\nE[S(n)] = \\sum_{i=1}^{R} \\left(1 - \\frac{\\binom{N-c_i}{n}}{\\binom{N}{n}}\\right) = R - \\sum_{i=1}^{R} \\frac{\\binom{N-c_i}{n}}{\\binom{N}{n}}\n$$\n\n**3. 数值计算与最终算法**\n\n为了计算 $g$，我们必须计算 $E[S(N-m)]$。设 $n' = N-m$。\n$$\nE[S(n')] = R - \\sum_{i=1}^{R} \\frac{\\binom{N-c_i}{n'}}{\\binom{N}{n'}}\n$$\n直接计算具有大参数的二项式系数会导致数值溢出。按规定，我们必须使用伽马函数的对数 $\\log \\Gamma(z+1) = \\log(z!)$ 来计算该比率。未命中克隆型 $i$ 的概率的对数是：\n$$\n\\log P_i(\\text{miss}) = \\log\\left(\\frac{\\binom{N-c_i}{n'}}{\\binom{N}{n'}}\\right) = \\log\\left(\\frac{(N-c_i)!(N-n')!}{N!(N-c_i-n')!}\\right)\n$$\n使用对数伽马函数，这变为：\n$$\n\\log P_i(\\text{miss}) = \\log\\Gamma(N-c_i+1) + \\log\\Gamma(N-n'+1) - \\log\\Gamma(N+1) - \\log\\Gamma(N-c_i-n'+1)\n$$\n那么概率为 $P_i(\\text{miss}) = \\exp(\\log P_i(\\text{miss}))$。`scipy.special.gammaln` 函数通过返回无穷大来正确处理参数为非正整数的情况，这在取指数后，当 $n' > N-c_i$ 时正确地得到概率 $0$。\n\n完整的算法如下：\n1.  给定计数列表 $[c_1, \\dots, c_R]$、$m$ 和 $\\alpha$。\n2.  计算总读数 $N = \\sum_{i=1}^{R} c_i$ 和不同克隆型的数量 $R$。\n3.  将用于计算的二次样本大小设置为 $n' = N-m$。\n4.  初始化未命中概率的总和 $\\Sigma_P = 0$。\n5.  对于列表中的每个计数 $c_i$：\n    a. 如果 $n' \\le N-c_i$，使用上述对数伽马公式计算 $\\log P_i(\\text{miss})$。\n    b. 将 $\\exp(\\log P_i(\\text{miss}))$ 加到 $\\Sigma_P$ 上。\n6.  计算深度 $n'$ 处的预期克隆型数量：$E[S(n')] = R - \\Sigma_P$。\n7.  计算边际增益：$g = (R - E[S(n')]) / m$。\n8.  返回比较 $g > \\alpha$ 的布尔结果。\n这个过程是稳健的，并直接实现了经过验证的科学模型。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Solves the immune repertoire sequencing analysis problem for a suite of test cases.\n    \"\"\"\n\n    def compute_decision(counts, m, alpha):\n        \"\"\"\n        Computes the decision to continue sequencing based on the slope of the rarefaction curve.\n\n        Args:\n            counts (list): A list of positive integers representing clonotype read counts.\n            m (int): A positive integer window size.\n            alpha (float): A non-negative real threshold for the marginal gain.\n\n        Returns:\n            bool: True if further sequencing is advisable, False otherwise.\n        \"\"\"\n        # Total number of reads (N) and distinct clonotypes (R)\n        N = sum(counts)\n        R = len(counts)\n\n        # The expected number of clonotypes at full depth E[S(N)] is exactly R.\n        E_S_N = float(R)\n\n        # We need to compute E[S(N-m)], the expected number of clonotypes when\n        # subsampling n_prime = N - m reads.\n        n_prime = N - m\n\n        # E[S(n')] = sum_{i=1 to R} (1 - P(clonotype i is missed))\n        # This is equivalent to R - sum_{i=1 to R} P(clonotype i is missed).\n        # We calculate the sum of miss probabilities.\n        sum_prob_miss = 0.0\n\n        for c_i in counts:\n            # The probability of missing a clonotype is non-zero only if\n            # it's possible to draw n_prime reads without picking any of its c_i members.\n            # This requires the number of other reads (N - c_i) to be at least n_prime.\n            if N - c_i >= n_prime:\n                # To avoid numerical overflow with large factorials, we compute the\n                # log of the ratio of binomial coefficients C(N-c_i, n') / C(N, n').\n                # log(P_miss) = log( (N-c_i)! * (N-n')! / (N! * (N-c_i-n')!) )\n                # This is computed using the log-gamma function, where lgamma(x+1) = log(x!).\n                log_prob_miss = (gammaln(N - c_i + 1) +\n                                 gammaln(N - n_prime + 1) -\n                                 gammaln(N + 1) -\n                                 gammaln(N - c_i - n_prime + 1))\n                \n                sum_prob_miss += np.exp(log_prob_miss)\n\n        # The expected number of clonotypes at depth n_prime\n        E_S_N_minus_m = R - sum_prob_miss\n\n        # The average marginal gain 'g' is the slope of the rarefaction curve\n        # approximated over the last m reads.\n        # The problem statement guarantees 1 = m = N, so m is never zero.\n        g = (E_S_N - E_S_N_minus_m) / m\n\n        return g > alpha\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # counts, m, alpha\n        ([25, 20, 15, 10, 8, 7, 5, 5, 3, 2], 10, 0.05),\n        (([50] + [1] * 50), 10, 0.15),\n        ([5] * 20, 10, 0.05),\n        ([100], 10, 0.001),\n        ([1] * 100, 10, 0.5),\n    ]\n\n    results = []\n    for counts, m, alpha in test_cases:\n        decision = compute_decision(counts, m, alpha)\n        # Format boolean as specified (True, False)\n        results.append(str(decision))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2399358"}, {"introduction": "高通量测序数据中不可避免地会包含错误，这给免疫组库分析带来了巨大挑战，尤其是在区分真实的低频克隆型和由高频克隆型产生的测序错误时。一个错误的碱基识别就可能产生一个看似全新的克隆型序列。本练习将引导你建立一个基于二项分布的统计假设检验模型，以量化地判断一个观测到的低频克隆型是真实存在的生物信号，还是仅仅是测序过程中的随机噪声[@problem_id:2399399]。", "problem": "这是一个关于免疫组库测序实验的独立读段抽样模型。在该实验中，一个具有相对频率 $f$ 的已知克隆型存在于样本中。设读段总数为 $N$。在测序数据中，一个候选克隆型被观察到 $k$ 次。来自已知克隆型的每个读段都有概率 $p_{error}$ 被独立地错误测序为候选克隆型，其中 $p_{error}$ 是针对该特定错误序列的有效单位读段错判概率。假设读段是独立的，并且与其他克隆型通过测序错误对候选克隆型产生的任何贡献，与来自已知克隆型的贡献相比是可以忽略的。在原假设（即候选克隆型完全是已知克隆型的错误衍生品）下，候选克隆型的观测读段数 $K$ 服从参数为 $N$ 和 $f \\cdot p_{error}$ 的二项分布。在备择假设下，除了来自已知克隆型的错误衍生贡献外，候选克隆型还有一个真实的潜在频率 $\\theta$。对于每种情况，使用显著性水平为 $\\alpha$ 的右尾检验准则来判断观测结果是否与原假设不一致。\n\n你的任务是编写一个完整的程序，针对每个测试用例，根据在原假设模型下观测到至少 $k$ 次的概率以及阈值 $\\alpha$，输出一个布尔值，指示是否应将候选克隆型判定为真实克隆型（即拒绝原假设）。程序必须硬编码以下测试套件，并以要求的格式生成结果。\n\n测试套件（每个用例是一个 $(N, f, p_{error}, k, \\alpha)$ 的元组）：\n- 用例 $1$：$(N=100000,\\, f=0.01,\\, p_{error}=0.001,\\, k=6,\\, \\alpha=0.01)$\n- 用例 $2$：$(N=100000,\\, f=0.01,\\, p_{error}=0.001,\\, k=1,\\, \\alpha=0.05)$\n- 用例 $3$：$(N=20000,\\, f=0.005,\\, p_{error}=0.001,\\, k=1,\\, \\alpha=0.1)$\n- 用例 $4$：$(N=50000,\\, f=0.0,\\, p_{error}=0.001,\\, k=1,\\, \\alpha=0.05)$\n- 用例 $5$：$(N=1000000,\\, f=0.01,\\, p_{error}=0.001,\\, k=8,\\, \\alpha=0.01)$\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目按测试用例的顺序为 True 或 False。例如，输出必须如下所示：\"[True,False,True,True,False]\"。", "solution": "所提出的问题是统计假设检验中的一个标准练习，应用于计算生物学领域，特别是免疫组库测序数据的分析。该问题定义明确，有科学依据，并包含了获得唯一解所需的所有信息。我们将进行形式化分析。\n\n核心任务是确定一个观察到的候选克隆型是一个真实的生物实体，还是仅仅是一个已知的、高丰度克隆型在测序过程中产生的错误产物。这被表述为一个假设检验。\n\n原假设 $H_0$ 是候选克隆型完全是一个错误产物。在此假设下，其观察到的读段数（我们用随机变量 $K$ 表示）源于属于已知克隆型的读段的错误测序事件。在一个总共测序了 $N$ 个读段的样本中，已知克隆型的真实相对频率为 $f$。任何单个来自已知克隆型的读段被错误测序为候选克隆型的概率为 $p_{error}$。由于我们假设来自任何其他来源的贡献可以忽略不计，那么在整个 $N$ 个读段的测序运行中，任何给定读段因这一特定错误过程而被观察为候选克隆型的概率，是抽样到已知克隆型的概率与后续发生错误的概率的乘积。这给出了单位读段的成功概率为 $p = f \\cdot p_{error}$。\n\n如问题所述，读段是独立抽样的。因此，候选克隆型的观测读段数 $K$ 服从具有 $N$ 次试验和成功概率 $p$ 的二项分布。\n$$K \\sim \\text{Binomial}(n, p)$$\n其中 $n = N$ 且 $p = f \\cdot p_{error}$。\n\n给定候选克隆型的观测计数为 $k$ 个读段。为了检验 $H_0$ 的有效性，我们采用右尾检验。我们计算在假设 $H_0$ 为真的情况下，观察到至少与我们测量到的结果一样极端的结果的概率。这个概率就是p值。对于右尾检验，p值为：\n$$p\\text{-value} = P(K \\ge k | H_0)$$\n\n该概率由二项分布的概率质量函数 (PMF) $P(K=i) = \\binom{n}{i} p^i (1-p)^{n-i}$ 计算如下：\n$$p\\text{-value} = \\sum_{i=k}^{n} \\binom{n}{i} p^i (1-p)^{n-i}$$\n\n出于计算目的，可以使用累积分布函数 (CDF) $F(k) = P(K \\le k)$ 的补集更高效、更准确地计算这个和。关系如下：\n$$P(K \\ge k) = 1 - P(K  k) = 1 - P(K \\le k-1) = 1 - F(k-1; n, p)$$\n这等价于在 $k-1$ 处求值的生存函数 (SF)。\n\n决策规则是将计算出的p值与预定的显著性水平 $\\alpha$ 进行比较。如果p值小于或等于 $\\alpha$，则认为观测结果在原假设下因偶然发生的可能性足够小。\n$$\\text{拒绝 } H_0 \\text{ 如果 } p\\text{-value} \\le \\alpha$$\n\n拒绝 $H_0$ 意味着我们有统计证据得出结论，即候选克隆型不仅仅是一个错误产物，应被视为一个“真实”的克隆型。如果 $p\\text{-value} > \\alpha$，我们不能拒绝 $H_0$，意味着观测结果与错误模型一致。\n\n对于每个具有参数 $(N, f, p_{error}, k, \\alpha)$ 的测试用例，我们将：\n1.  计算二项分布的概率参数 $p = f \\cdot p_{error}$。\n2.  对于 $K \\sim \\text{Binomial}(N, p)$，计算p值 $P(K \\ge k)$。\n3.  将p值与 $\\alpha$ 进行比较，并断定是否拒绝 $H_0$。拒绝对应于 `True` 结果，不拒绝对应于 `False` 结果。\n\n让我们简要分析一下当 $f=0$ 时的特殊情况，如用例4所示。在这种情况下，概率参数 $p = 0 \\cdot p_{error} = 0$。分布为 $\\text{Binomial}(N, 0)$，这意味着 $P(K=0)=1$ 且 $P(K>0)=0$。如果观察到任何非零计数 $k \\ge 1$，则p值 $P(K \\ge k | p=0)$ 恰好为 $0$。由于对于任何常规的显著性水平都有 $0 \\le \\alpha$，所以在原假设模型下不可能发生的情况下，即使观察到一个读段也会立即导致拒绝 $H_0$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Solves the immune repertoire clonotype calling problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple of (N, f, p_error, k, alpha).\n    test_cases = [\n        (100000, 0.01, 0.001, 6, 0.01),\n        (100000, 0.01, 0.001, 1, 0.05),\n        (20000, 0.005, 0.001, 1, 0.1),\n        (50000, 0.0, 0.001, 1, 0.05),\n        (1000000, 0.01, 0.001, 8, 0.01),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        N, f, p_error, k, alpha = case\n\n        # Number of trials for the Binomial distribution\n        n = int(N)\n        \n        # Probability of success for each trial under the null hypothesis\n        p = f * p_error\n\n        # Under the null hypothesis, the number of observed reads K for the candidate\n        # clonotype follows a Binomial distribution B(n, p).\n        # We perform a right-tail test. The p-value is the probability of\n        # observing a count of at least k.\n        # p-value = P(K >= k)\n\n        # Handle the edge case where k=0. P(K>=0) is always 1.\n        # The problem implies k>=1 since the clonotype is \"observed k times\".\n        # However, for completeness:\n        if k == 0:\n            p_value = 1.0\n        else:\n            # P(K >= k) is calculated using the survival function (SF), which is 1 - CDF.\n            # sf(x) = P(X > x). Therefore, P(K >= k) = P(K > k-1) = sf(k-1).\n            # This is numerically more stable than 1 - cdf(k-1) or summing the pmf.\n            p_value = binom.sf(k - 1, n, p)\n\n        # Reject the null hypothesis if the p-value is less than or equal to the\n        # significance level alpha. Rejecting H0 means we call it a true clonotype.\n        is_true_clonotype = p_value = alpha\n        results.append(is_true_clonotype)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2399399"}, {"introduction": "识别针对同一抗原发生响应的T细胞克隆群是免疫组库分析的核心目标之一。理论上，这些功能相关的克隆通常具有相似的T细胞受体（TCR）序列。这项练习介绍了一种强大的、基于图论的方法：将序列相似性网络化，然后应用社区发现算法来识别这些功能相关的“克隆家族”。你将学习如何根据序列间的编辑距离构建网络，并应用模块度最优化来划分社区，从而揭示免疫应答的潜在功能结构[@problem_id:2399318]。", "problem": "给定一个有限的氨基酸序列集合，代表来自T细胞受体的互补决定区3（CDR3）的氨基酸序列。设该序列集索引为 $\\{s_1,\\dots,s_n\\}$。定义任意两个序列 $s_i$ 和 $s_j$ 之间的编辑（Levenshtein）距离 $d(s_i,s_j)$ 为将 $s_i$ 转换为 $s_j$ 所需的单字符插入、删除或替换的最小次数。对于一个固定的非负整数阈值 $\\tau$，构建一个无向简单图 $G=(V,E)$，其中 $V=\\{1,\\dots,n\\}$，并且当且仅当 $d(s_i,s_j)\\le \\tau$ 时，存在一条边 $\\{i,j\\}\\in E$。设 $A\\in\\{0,1\\}^{n\\times n}$ 为 $G$ 的邻接矩阵，其中对所有 $i$ 都有 $A_{ii}=0$。设 $k_i=\\sum_{j=1}^n A_{ij}$ 表示节点 $i$ 的度。设 $m=\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n A_{ij}$ 表示边的总数。\n\n一个划分（社群分配）$c$ 为每个节点 $i$ 分配一个社群标签 $c_i\\in\\{1,\\dots,C\\}$，其中 $C$ 为某个正整数。在图 $G$ 上，一个划分 $c$ 的 Newman–Girvan 模块度 $Q(c)$ 定义为\n$$\nQ(c)=\\frac{1}{2m}\\sum_{i=1}^n\\sum_{j=1}^n\\left(A_{ij}-\\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{c_i=c_j\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。当 $m=0$（没有边）时，按照惯例，对所有划分 $c$ 定义 $Q(c)=0$。\n\n您的任务是，对于每个指定的测试用例，计算一个划分 $c^\\star$，使得 $Q(c)$ 在所有可能的划分 $c$ 中最大化。如果存在多个最大化划分，请按顺序应用以下确定性平局打破规则：\n- 如果 $m=0$，选择每个节点自成一个社群的划分（即 $C=n$，且每个社群大小为 $1$）。\n- 否则，在具有最大模块度的划分中，选择社群数 $C$ 最少的一个。\n- 如果仍然平局，在剩余的划分中，选择其社群大小的多重集（按非递增顺序排序后）字典序最小的一个。\n- 如果仍然平局，选择其社群标签向量 $(c_1,\\dots,c_n)$（按照首次出现的顺序重新标记为连续整数后）字典序最小的一个。\n\n对于每个测试用例，报告 $c^\\star$ 的社群大小列表，按非递增顺序排序。最终的程序输出必须将所有测试用例的结果汇总到单行中，该行包含一个列表，其元素是每个测试用例的社群大小列表，且不含空格。\n\n测试套件：\n- 测试用例 1：$\\tau=1$，序列\n  - $s_1=$ \"CASSLGQETQYF\"\n  - $s_2=$ \"CASSLGQETQFF\"\n  - $s_3=$ \"CASSLGQDTQYF\"\n  - $s_4=$ \"CASSIRSSYEQYF\"\n  - $s_5=$ \"CASSIGSSYEQYF\"\n  - $s_6=$ \"CASSIRSSYEQFF\"\n- 测试用例 2：$\\tau=0$，序列\n  - $s_1=$ \"CASSLAPGNTIYF\"\n  - $s_2=$ \"CATSQRGQLNTQF\"\n  - $s_3=$ \"CATSASGQGNNEQF\"\n  - $s_4=$ \"CASSYNEGYTF\"\n- 测试用例 3：$\\tau=2$，序列\n  - $s_1=$ \"CASSLGQETQYF\"\n  - $s_2=$ \"CASSLGQETQFF\"\n  - $s_3=$ \"CASSLGQDTQYF\"\n  - $s_4=$ \"CASSLGRETQYF\"\n- 测试用例 4：$\\tau=1$，序列\n  - $s_1=$ \"CASSVGQETQYF\"\n  - $s_2=$ \"CASSVGQETQFF\"\n  - $s_3=$ \"CASSVGRETQYF\"\n  - $s_4=$ \"CASSIGQETQYF\"\n  - $s_5=$ \"CASSIRSSYEQYF\"\n  - $s_6=$ \"CASSIRSSYEQFF\"\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含按给定顺序排列的测试用例结果列表。每个结果必须是该测试用例的社群大小列表，按非递增顺序排序。外层列表和所有内层列表都必须在没有空格的情况下呈现，例如 `[[3,3],[4],[4],[3,2,1]]`，其中列表内的元素是整数。", "solution": "问题陈述已经过严格验证，被确定为具有科学依据、定义明确且客观。它提出了一个植根于网络科学和生物信息学既定原则的正式计算任务。因此，该问题是有效的，下面提供了解决方案。\n\n问题的核心是找到节点集 $V=\\{1, \\dots, n\\}$ 的一个划分 $c^\\star$，以最大化 Newman-Girvan 模块度函数 $Q(c)$。节点代表T细胞受体CDR3序列，图结构由它们的相似性决定，该相似性使用 Levenshtein 距离进行量化。\n\n一个划分 $c$ 的模块度由以下公式给出：\n$$\nQ(c)=\\frac{1}{2m}\\sum_{i=1}^n\\sum_{j=1}^n\\left(A_{ij}-\\frac{k_i k_j}{2m}\\right)\\,\\mathbf{1}\\{c_i=c_j\\},\n$$\n其中 $A$ 是邻接矩阵，$k_i$ 是节点 $i$ 的度，$m$ 是边的总数，$\\mathbf{1}\\{c_i=c_j\\}$ 是一个指示函数。对于 $m=0$ 的退化情况，$Q(c)$ 被定义为 $0$。模块度 $Q$ 通过比较社群内部的边密度与具有相同度分布的随机图中预期的密度来量化划分的质量。较高的 $Q$ 值表示更显著的社群结构。\n\n最大化 $Q(c)$ 的任务对于一般图来说是NP难问题。然而，在所提供的测试用例中，节点数 $n$ 很小（最多 $n=6$）。一个 $n$ 元集合的全部不同划分的数量由贝尔数 $B_n$ 给出。对于 $n=6$，$B_6=203$，这是一个计算上可行的数字。因此，该问题可以通过对节点集所有可能的划分进行穷举搜索来解决。\n\n总体算法如下：\n\n1.  **图的构建**：对于每个测试用例，它由一组序列 $\\{s_1, \\dots, s_n\\}$ 和一个阈值 $\\tau$ 组成，构建一个无向图 $G=(V, E)$。当且仅当 Levenshtein 距离 $d(s_i, s_j) \\le \\tau$ 时，存在一条边 $\\{i,j\\}$。由此计算出邻接矩阵 $A$、节点度 $k_i$ 和边的总数 $m$。Levenshtein 距离使用标准的动态规划算法计算。\n\n2.  **划分生成**：生成顶点集 $V=\\{1, \\dots, n\\}$ 的所有可能划分。这可以通过递归算法完成。每个划分是 $V$ 的一组非空、不相交的子集，其并集为 $V$。\n\n3.  **模块度计算与优化**：对每个生成的划分 $c$，计算其模块度 $Q(c)$。为了计算方便，该公式可以更方便地表示为对社群求和的形式：\n    $$\n    Q(c) = \\sum_{l=1}^C \\left( \\frac{e_l}{m} - \\left(\\frac{d_l}{2m}\\right)^2 \\right),\n    $$\n    其中求和是针对划分中的 $C$ 个社群，$e_l$ 是社群 $l$ 内部的边数，$d_l$ 是社群 $l$ 中节点度的总和。\n\n4.  **平局打破**：问题指定了一套严格、确定性的平局打破规则。为了找到唯一的最优划分 $c^\\star$，我们寻求在所有可能划分的集合中找到字典序最小的元组，其中一个划分 $c$ 的元组构造如下：\n    $$\n    \\left( -Q(c), C(c), S(c), V(c) \\right)\n    $$\n    - $-Q(c)$：模块度的负值。最小化此值即最大化 $Q(c)$。\n    - $C(c)$：划分 $c$ 中的社群数量。\n    - $S(c)$：社群大小的元组，按非递增顺序排序。\n    - $V(c)$：规范的社群标签向量 $(c_1, \\dots, c_n)$，按照首次出现的顺序重新标记为连续整数（例如，包含节点 $1$ 的社群标记为 $1$，扫描节点 $2, 3, \\dots$ 时遇到的第一个新社群标记为 $2$，依此类推）。这为每个划分提供了唯一的表示。\n\n    搜索算法遍历所有划分，为每个划分计算此元组，并维护与找到的字典序最小元组相对应的划分。对于 $m=0$ 的特殊情况，按照问题陈述中的定义单独处理。\n\n5.  **对不连通图的分解**：一个关键的观察是，最优划分绝不会将来自图的不同连通分量的节点分在同一组。可以证明，合并来自不同分量的社群会严格降低模块度。这意味着问题可以分解：可以为每个连通分量独立地找到最优划分，这些划分的并集构成了整个图的最优划分。全局划分的模块度不是各分量模块度的简单加和，但最优性分解原理成立。对于测试套件中的小 $n$ 值，这种简化并非绝对必要，但它是模块度的一个基本性质。对于给定的测试用例，这一洞见简化了手动分析。例如，在测试用例1中，图分解为两个不相交的星形图，从而可以进行分开分析。\n\n通过为每个测试用例系统地应用此过程，可以确定一个唯一的最优划分 $c^\\star$。每个用例的最终输出是 $c^\\star$ 中社群大小的列表，按非递增顺序排序。", "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\n\ndef levenshtein_distance(s1: str, s2: str) -> int:\n    \"\"\"Calculates the Levenshtein distance between two strings.\"\"\"\n    m, n = len(s1), len(s2)\n    if m  n:\n        s1, s2 = s2, s1\n        m, n = n, m\n    \n    if n == 0:\n        return m\n\n    dp_row = np.arange(n + 1, dtype=int)\n\n    for i in range(1, m + 1):\n        prev_row_val = dp_row[0]\n        dp_row[0] = i\n        for j in range(1, n + 1):\n            temp = dp_row[j]\n            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n            dp_row[j] = min(dp_row[j] + 1,          # Deletion\n                            dp_row[j - 1] + 1,      # Insertion\n                            prev_row_val + cost) # Substitution\n            prev_row_val = temp\n            \n    return dp_row[n]\n\ndef generate_partitions(n: int):\n    \"\"\"Generates all partitions of the set {0, 1, ..., n-1}.\"\"\"\n    elements = list(range(n))\n    def _generate(index, partition):\n        if index == n:\n            yield [list(p) for p in partition]\n            return\n        \n        # Add to an existing community\n        for p_set in partition:\n            p_set.add(elements[index])\n            yield from _generate(index + 1, partition)\n            p_set.remove(elements[index])\n            \n        # Add to a new community\n        partition.append({elements[index]})\n        yield from _generate(index + 1, partition)\n        partition.pop()\n\n    if n == 0:\n        yield []\n        return\n        \n    yield from _generate(1, [{elements[0]}])\n\ndef get_canonical_vector(partition, n: int) -> tuple[int, ...]:\n    \"\"\"Computes the canonical label vector for a partition.\"\"\"\n    # Create an arbitrary labeling first\n    temp_c = [0] * n\n    label = 1\n    for community in partition:\n        for node_idx in community:\n            temp_c[node_idx] = label\n        label += 1\n    \n    # Relabel to be consecutive in order of first appearance\n    c_canon = [0] * n\n    mapping = {}\n    next_canon_label = 1\n    for i in range(n):\n        original_label = temp_c[i]\n        if original_label not in mapping:\n            mapping[original_label] = next_canon_label\n            next_canon_label += 1\n        c_canon[i] = mapping[original_label]\n    \n    return tuple(c_canon)\n\n\ndef solve():\n    \"\"\"\n    Solves the modularity maximization problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"tau\": 1,\n            \"sequences\": [\n                \"CASSLGQETQYF\", \"CASSLGQETQFF\", \"CASSLGQDTQYF\",\n                \"CASSIRSSYEQYF\", \"CASSIGSSYEQYF\", \"CASSIRSSYEQFF\"\n            ]\n        },\n        {\n            \"tau\": 0,\n            \"sequences\": [\n                \"CASSLAPGNTIYF\", \"CATSQRGQLNTQF\",\n                \"CATSASGQGNNEQF\", \"CASSYNEGYTF\"\n            ]\n        },\n        {\n            \"tau\": 2,\n            \"sequences\": [\n                \"CASSLGQETQYF\", \"CASSLGQETQFF\",\n                \"CASSLGQDTQYF\", \"CASSLGRETQYF\"\n            ]\n        },\n        {\n            \"tau\": 1,\n            \"sequences\": [\n                \"CASSVGQETQYF\", \"CASSVGQETQFF\", \"CASSVGRETQYF\",\n                \"CASSIGQETQYF\", \"CASSIRSSYEQYF\", \"CASSIRSSYEQFF\"\n            ]\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        tau, sequences = case[\"tau\"], case[\"sequences\"]\n        n = len(sequences)\n\n        # Step 1: Construct graph\n        adj = np.zeros((n, n), dtype=int)\n        for i in range(n):\n            for j in range(i + 1, n):\n                if levenshtein_distance(sequences[i], sequences[j]) = tau:\n                    adj[i, j] = adj[j, i] = 1\n\n        degrees = np.sum(adj, axis=1)\n        m_total_edges = int(np.sum(degrees) / 2)\n\n        # Handle m=0 case as per tie-breaking rule 1\n        if m_total_edges == 0:\n            results.append(sorted([1] * n, reverse=True))\n            continue\n\n        best_key = (float('inf'),)\n        best_partition_sizes = []\n\n        two_m = 2 * m_total_edges\n\n        # Iterate through all partitions of the nodes\n        for partition in generate_partitions(n):\n            \n            # Calculate modularity\n            q_val = 0.0\n            for community in partition:\n                community_nodes = list(community)\n                d_l = np.sum(degrees[community_nodes])\n                e_l = 0\n                for i in range(len(community_nodes)):\n                    for j in range(i + 1, len(community_nodes)):\n                        u, v = community_nodes[i], community_nodes[j]\n                        e_l += adj[u, v]\n                \n                term_1 = e_l / m_total_edges if m_total_edges > 0 else 0\n                term_2 = (d_l / two_m)**2 if two_m > 0 else 0\n                q_val += (term_1 - term_2)\n            \n            # Get tie-breaker values\n            num_communities = len(partition)\n            sorted_sizes = tuple(sorted([len(c) for c in partition], reverse=True))\n            canonical_vec = get_canonical_vector(partition, n)\n            \n            current_key = (-q_val, num_communities, sorted_sizes, canonical_vec)\n\n            if best_key[0] == float('inf') or current_key  best_key:\n                best_key = current_key\n                best_partition_sizes = list(sorted_sizes)\n        \n        results.append(best_partition_sizes)\n\n    # Final output formatting\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "2399318"}]}