{"hands_on_practices": [{"introduction": "我们以一个基础练习开始，从头开始构建经典的 Jacobi 和 Gauss-Seidel 求解器。这个练习旨在通过直接比较强对角占优和弱对角占优矩阵，来探索矩阵的属性（如对角占优性）与迭代收敛速度之间的基本联系。通过计算迭代矩阵的谱半径，你将亲手验证理论预测，即谱半径越小，收敛越快。[@problem_id:2406932]", "problem": "您必须编写一个完整的、可运行的程序，用于评估指定线性系统的线性定常迭代。考虑大小为 $n \\times n$ 的方形实对称矩阵，其右端向量为 $\\mathbf{b} \\in \\mathbb{R}^n$。对于每个系统，从零向量 $\\mathbf{x}^{(0)} = \\mathbf{0}$ 开始，迭代直到残差的无穷范数 $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A \\mathbf{x}^{(k)}\\|_{\\infty}$ 小于或等于容差 $\\tau$，或者达到最大迭代次数 $k_{\\max}$。使用 Jacobi 方法和 Gauss–Seidel 方法的标准定义。对于每个指定的矩阵，报告 Jacobi 和 Gauss–Seidel 方法满足残差容差所需的最小迭代次数，以及 Jacobi 迭代矩阵的谱半径。所有计算均为纯数值计算，不涉及物理单位。\n\n使用以下构成测试套件的参数：\n\n- 维度 $n = 6$。\n- 右端向量 $\\mathbf{b} = [1,2,3,4,5,6]^T$。\n- 初始猜测值 $\\mathbf{x}^{(0)} = \\mathbf{0}$。\n- 残差容差 $\\tau = 10^{-8}$。\n- 最大迭代次数 $k_{\\max} = 20000$。\n\n定义三个矩阵 $A \\in \\mathbb{R}^{6 \\times 6}$ 如下：\n\n1) 强对角占优的稠密矩阵，具有常数非对角元：\n- 参数 $\\alpha_{\\mathrm{s}} = 20.0$, $\\gamma = -1.0$。\n- 元素：\n  - 对所有 $i \\in \\{1,\\dots,6\\}$，$A_{ii} = \\alpha_{\\mathrm{s}}$。\n  - 对所有 $i \\neq j$，$A_{ij} = \\gamma$。\n\n2) 勉强对角占优的稠密矩阵，具有常数非对角元：\n- 参数 $\\alpha_{\\mathrm{w}} = 5.1$, $\\gamma = -1.0$。\n- 元素：\n  - 对所有 $i \\in \\{1,\\dots,6\\}$，$A_{ii} = \\alpha_{\\mathrm{w}}$。\n  - 对所有 $i \\neq j$，$A_{ij} = \\gamma$。\n\n3) 边界情况，对称三对角矩阵（一维离散拉普拉斯形式）：\n- 元素：\n  - 对所有 $i \\in \\{1,\\dots,6\\}$，$A_{ii} = 2.0$。\n  - 对所有 $i \\in \\{1,\\dots,5\\}$，$A_{i,i+1} = A_{i+1,i} = -1.0$。\n  - 所有其他非对角元素均为 $0.0$。\n\n对于这三个矩阵中的每一个，执行以下操作：\n- 使用 Jacobi 方法，确定最小迭代次数 $k_{\\mathrm{J}}$，使得 $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{J}})}\\|_{\\infty} \\le \\tau$，如果未能在 $k_{\\max}$ 次迭代内达到，则返回 $k_{\\max}$。\n- 使用 Gauss–Seidel 方法，确定最小迭代次数 $k_{\\mathrm{GS}}$，使得 $\\|\\mathbf{b} - A \\mathbf{x}^{(k_{\\mathrm{GS}})}\\|_{\\infty} \\le \\tau$，如果未能在 $k_{\\max}$ 次迭代内达到，则返回 $k_{\\max}$。\n- 对于 Jacobi 方法，计算其迭代矩阵的谱半径 $\\rho_{\\mathrm{J}}$。报告四舍五入到六位小数的 $\\rho_{\\mathrm{J}}$。\n\n您的程序应生成单行输出，包含按以下顺序排列的九个结果：\n- 对于强对角占优矩阵：$k_{\\mathrm{J}}$、$k_{\\mathrm{GS}}$、$\\rho_{\\mathrm{J}}$（四舍五入到六位小数）。\n- 对于勉强对角占优矩阵：$k_{\\mathrm{J}}$、$k_{\\mathrm{GS}}$、$\\rho_{\\mathrm{J}}$（四舍五入到六位小数）。\n- 对于三对角边界情况矩阵：$k_{\\mathrm{J}}$、$k_{\\mathrm{GS}}$、$\\rho_{\\mathrm{J}}$（四舍五入到六位小数）。\n\n最终输出格式必须是单行，形式为用方括号括起来的逗号分隔列表，例如\n$[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$，\n其中 $k_{\\mathrm{J}}$ 和 $k_{\\mathrm{GS}}$ 是整数，每个 $\\rho_{\\mathrm{J}}$ 是四舍五入到六位小数的浮点数。", "solution": "该问题经过验证并被确定为有效。它在数值线性代数领域有科学依据，定义了所有必要参数，问题本身是适定的，并且其表述是客观的。该问题要求实现和评估两种基本的线性定常迭代方法——Jacobi 法和 Gauss–Seidel 法，用以求解线性方程组 $A\\mathbf{x} = \\mathbf{b}$。\n\n这些方法基于将矩阵 $A$ 分裂为其组成部分。一个方阵 $A$ 可以分解为 $A = D + L + U$，其中 $D$ 是包含 $A$ 的对角元素的对角矩阵，$L$ 是严格下三角矩阵，$U$ 是严格上三角矩阵。因此，系统 $A\\mathbf{x} = \\mathbf{b}$ 可以写成 $(D+L+U)\\mathbf{x} = \\mathbf{b}$。\n\n**Jacobi 方法**\n\nJacobi 方法将系统重新排列为 $D\\mathbf{x} = \\mathbf{b} - (L+U)\\mathbf{x}$。这导出了迭代格式：\n$$ D\\mathbf{x}^{(k+1)} = \\mathbf{b} - (L+U)\\mathbf{x}^{(k)} $$\n假设 $D$ 是可逆的（即没有零对角元素，本问题中的所有矩阵都满足此条件），我们得到迭代公式：\n$$ \\mathbf{x}^{(k+1)} = D^{-1}(\\mathbf{b} - (L+U)\\mathbf{x}^{(k)}) $$\n这可以对向量 $\\mathbf{x}^{(k+1)}$ 的每个元素 $i$ 进行分量计算：\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1, j \\neq i}^{n} A_{ij} x_j^{(k)} \\right) $$\nJacobi 方法的一个关键特性是，每个分量 $x_i^{(k+1)}$ 的计算仅依赖于前一次迭代的向量 $\\mathbf{x}^{(k)}$ 的分量。这使得新向量分量的计算可以并行进行。\n\n**Gauss–Seidel 方法**\n\nGauss–Seidel 方法旨在通过使用可用的最新信息来提高收敛速度。它将系统重新排列为 $(D+L)\\mathbf{x} = \\mathbf{b} - U\\mathbf{x}$，从而得到迭代格式：\n$$ (D+L)\\mathbf{x}^{(k+1)} = \\mathbf{b} - U\\mathbf{x}^{(k)} $$\n这产生了迭代公式：\n$$ \\mathbf{x}^{(k+1)} = (D+L)^{-1}(\\mathbf{b} - U\\mathbf{x}^{(k)}) $$\n在实践中，这是通过前向代入实现的。其分量公式为：\n$$ x_i^{(k+1)} = \\frac{1}{A_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} A_{ij} x_j^{(k)} \\right) $$\n请注意，在计算 $x_i^{(k+1)}$ 时，我们使用了当前迭代 $k+1$ 中新计算出的分量 $x_j^{(k+1)}$（对于 $j  i$），以及前一次迭代 $k$ 中的旧分量 $x_j^{(k)}$（对于 $j > i$）。这种顺序依赖性意味着必须按顺序更新分量。\n\n**收敛性与谱半径**\n\n任何线性定常迭代都可以写成 $\\mathbf{x}^{(k+1)} = T \\mathbf{x}^{(k)} + \\mathbf{c}$ 的形式，其中 $T$ 是迭代矩阵。对于任何初始猜测值 $\\mathbf{x}^{(0)}$，该方法保证收敛的充分必要条件是迭代矩阵的谱半径 $\\rho(T)$ 严格小于 1。谱半径定义为 $T$ 的特征值的最大绝对值，即 $\\rho(T) = \\max_i |\\lambda_i(T)|$。\n\n对于 Jacobi 方法，迭代矩阵 $T_J$ 由下式给出：\n$$ T_J = -D^{-1}(L+U) = I - D^{-1}A $$\n谱半径 $\\rho(T_J)$ 决定了 Jacobi 方法的收敛性。较小的谱半径意味着更快的渐近收敛率。本问题要求计算此值。\n\n**实现策略**\n\n解决方案将使用 Python 的 `numpy` 库来实现。\n1.  **矩阵构建**：将三个指定的矩阵，$A_1$（强对角占优）、$A_2$（勉强对角占优）和 $A_3$（三对角），构建为 `numpy` 数组。定义问题参数 $n=6$、$\\mathbf{b}=[1,2,3,4,5,6]^T$、$\\mathbf{x}^{(0)}=\\mathbf{0}$、$\\tau=10^{-8}$ 和 $k_{\\max}=20000$。\n2.  **迭代求解器**：将实现 Jacobi 和 Gauss–Seidel 方法的函数。每个函数将接受矩阵 $A$、向量 $\\mathbf{b}$、初始猜测值 $\\mathbf{x}^{(0)}$、容差 $\\tau$ 和最大迭代次数 $k_{\\max}$ 作为输入。循环将从 $k=1$ 到 $k_{\\max}$ 运行，在每一步更新解向量 $\\mathbf{x}$。每次更新后，将检查残差的无穷范数 $\\|\\mathbf{r}^{(k)}\\|_{\\infty} = \\|\\mathbf{b} - A\\mathbf{x}^{(k)}\\|_{\\infty}$ 是否满足容差 $\\tau$。如果条件满足，则返回当前迭代次数 $k$。如果循环完成而未收敛，则返回 $k_{\\max}$。还会对 $k=0$ 进行初始检查。\n3.  **谱半径计算**：一个函数将计算 Jacobi 迭代矩阵 $T_J = I - D^{-1}A$。将使用 `numpy.linalg.eigvals` 找到 $T_J$ 的特征值，谱半径将是它们绝对值的最大值。\n4.  **执行与输出**：程序的主要部分将遍历三个测试用例（矩阵）。对于每个用例，它将调用求解器函数以获取迭代次数 $k_J$ 和 $k_{GS}$，并调用谱半径函数获取 $\\rho_J$。结果将被收集并按问题陈述中指定的格式格式化为单个字符串。", "answer": "```python\nimport numpy as np\n\ndef jacobi(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Jacobi method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    D = np.diag(A)\n    R = A - np.diag(D)  # R = L + U\n\n    for k in range(1, k_max + 1):\n        x_new = (b - R @ x) / D\n        x = x_new\n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n    \n    return k_max\n\ndef gauss_seidel(A: np.ndarray, b: np.ndarray, x0: np.ndarray, tol: float, k_max: int) -> int:\n    \"\"\"\n    Solves the system Ax=b using the Gauss-Seidel method.\n\n    Args:\n        A: The n x n coefficient matrix.\n        b: The n x 1 right-hand side vector.\n        x0: The initial guess vector.\n        tol: The residual tolerance.\n        k_max: The maximum number of iterations.\n\n    Returns:\n        The number of iterations required for convergence.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Check for k=0\n    residual_norm = np.linalg.norm(b - A @ x, np.inf)\n    if residual_norm = tol:\n        return 0\n\n    for k in range(1, k_max + 1):\n        x_old = x.copy()\n        for i in range(n):\n            sum1 = np.dot(A[i, :i], x[:i])\n            sum2 = np.dot(A[i, i + 1:], x_old[i + 1:])\n            x[i] = (b[i] - sum1 - sum2) / A[i, i]\n        \n        residual_norm = np.linalg.norm(b - A @ x, np.inf)\n        if residual_norm = tol:\n            return k\n            \n    return k_max\n\ndef get_spectral_radius_J(A: np.ndarray) -> float:\n    \"\"\"\n    Computes the spectral radius of the Jacobi iteration matrix.\n\n    Args:\n        A: The n x n coefficient matrix.\n\n    Returns:\n        The spectral radius of the Jacobi matrix T_J.\n    \"\"\"\n    D = np.diag(np.diag(A))\n    D_inv = np.linalg.inv(D)\n    T_J = np.eye(A.shape[0]) - D_inv @ A\n    eigenvalues = np.linalg.eigvals(T_J)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Global parameters\n    n = 6\n    b = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    x0 = np.zeros(n)\n    tol = 1e-8\n    k_max = 20000\n\n    # Define the three matrices\n    test_cases = []\n\n    # Case 1: Strongly diagonally dominant\n    alpha_s = 20.0\n    gamma1 = -1.0\n    A1 = np.full((n, n), gamma1)\n    np.fill_diagonal(A1, alpha_s)\n    test_cases.append(A1)\n\n    # Case 2: Just-barely diagonally dominant\n    alpha_w = 5.1\n    gamma2 = -1.0\n    A2 = np.full((n, n), gamma2)\n    np.fill_diagonal(A2, alpha_w)\n    test_cases.append(A2)\n\n    # Case 3: Tridiagonal edge case\n    A3 = 2.0 * np.eye(n) - np.eye(n, k=1) - np.eye(n, k=-1)\n    test_cases.append(A3)\n\n    results = []\n    for A in test_cases:\n        k_J = jacobi(A, b, x0, tol, k_max)\n        k_GS = gauss_seidel(A, b, x0, tol, k_max)\n        rho_J = get_spectral_radius_J(A)\n\n        results.append(k_J)\n        results.append(k_GS)\n        results.append(round(rho_J, 6))\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2406932"}, {"introduction": "在掌握了基本实现后，我们将把这些方法应用于一个经典的物理问题：求解二维拉普拉斯方程。这个练习将引导你通过有限差分法将一个偏微分方程离散化，并对由此产生的大型稀疏线性系统应用迭代法。你将有机会直接比较 Jacobi、Gauss-Seidel 和逐次超松弛（SOR）方法的性能，并见证 SOR 方法在收敛速度上的显著优势。[@problem_id:2406769]", "problem": "考虑方形域 $\\Omega = (0,1)\\times(0,1)$ 上的二维拉普拉斯方程 $\\nabla^2 u = 0$，其带有狄利克雷边界条件。设边界数据为 $u(x,0) = \\sin(\\pi x)$，$u(x,1) = 0$，$u(0,y)=0$ 以及 $u(1,y)=0$，其中角度以弧度为单位。使用均匀网格对该域进行离散化，每个空间方向上有 $N$ 个内部点（因此网格间距为 $h = \\frac{1}{N+1}$，包括边界在内共有 $(N+2)\\times(N+2)$ 个网格点）。使用标准的二阶中心有限差分近似来逼近拉普拉斯算子，从而获得关于内部未知数的线性系统。从由连续方程和中心差分推导出的离散拉普拉斯算子定义出发，实现三种定常迭代法来求解离散方程：Jacobi方法、Gauss–Seidel方法以及逐次超松弛（SOR）方法，其中松弛参数 $\\omega$ 满足 $0  \\omega  2$。对于每种方法，请使用以下基本依据和定义：\n\n- 在内部网格点 $(i,j)$ 处的离散拉普拉斯方程由 $\\nabla^2 u = 0$ 的中心差分近似得到：\n$$\\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} = 0,$$\n该式在代数上等价于模板方程\n$$-u_{i+1,j} - u_{i-1,j} - u_{i,j+1} - u_{i,j-1} + 4 u_{i,j} = 0.$$\n- 由此，将内部的离散残差定义为\n$$r_{i,j} = 4u_{i,j} - \\left(u_{i+1,j}+u_{i-1,j}+u_{i,j+1}+u_{i,j-1}\\right),$$\n其无穷范数定义为 $\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$。\n- 使用停止准则 $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol}\\cdot \\|r^{(0)}\\|_{\\infty}$，其中 $\\text{tol}$ 是给定的容差，$k$ 是迭代指数，初始猜测 $u^{(0)}$ 在所有内部点上为零，并在边界点上取固定的边界值。\n\n您的程序必须：\n- 实现 Jacobi 迭代，该迭代仅使用前一次迭代的值来更新所有内部值。\n- 实现 Gauss–Seidel 迭代，其方式应使用最新可用的相邻值（您可以使用红黑排序来实现）。\n- 使用红黑排序和松弛参数 $\\omega$ 实现 SOR 迭代，公式为\n$$u^{(k+1)}_{i,j} = u^{(k)}_{i,j} + \\omega\\left(\\frac{1}{4}\\left(u^{(*)}_{i+1,j}+u^{(*)}_{i-1,j}+u^{(*)}_{i,j+1}+u^{(*)}_{i,j-1}\\right) - u^{(k)}_{i,j}\\right),$$\n其中 $u^{(*)}$ 表示与每种颜色上的 Gauss–Seidel 排序一致的最新值。取 $\\omega = 1$ 可恢复为 Gauss–Seidel 方法。\n\n您的任务是通过报告每种方法在各测试用例中满足停止准则所需的迭代次数，来定量比较这三种方法的收敛速度。对所有方法使用相同的离散化、边界条件和停止准则，并将迭代次数报告为整数。\n\n测试套件。在以下参数集上运行您的程序，其中每个测试用例是一个三元组 $(N, \\text{tol}, \\omega)$：\n- 测试 $1$：$(20, 10^{-5}, 1.5)$。\n- 测试 $2$：$(20, 10^{-5}, 1.0)$。\n- 测试 $3$：$(10, 10^{-8}, 1.8)$。\n- 测试 $4$：$(40, 10^{-4}, 1.9)$。\n\n对于每个测试用例，您的程序必须生成一个列表 $[n_J, n_{GS}, n_{SOR}]$，其中分别包含 Jacobi、Gauss–Seidel 和 SOR 方法满足停止准则所需的迭代次数。将所有测试的结果汇总到一行中，形式为这些列表的逗号分隔列表，不含空格，并用方括号括起来。例如，您的输出必须与以下形式完全一致\n$[[n_J^{(1)},n_{GS}^{(1)},n_{SOR}^{(1)}],[n_J^{(2)},n_{GS}^{(2)},n_{SOR}^{(2)}],[n_J^{(3)},n_{GS}^{(3)},n_{SOR}^{(3)}],[n_J^{(4)},n_{GS}^{(4)},n_{SOR}^{(4)}]]$，\n并作为单行打印。不涉及物理单位。正弦函数中的角度必须以弧度解释。", "solution": "所述问题是求解椭圆型偏微分方程的数值解中的一个标准的、适定的练习。该问题在科学上是合理的，内容是自洽的，且算法上是明确的。不存在矛盾、歧义或事实错误。因此，我们直接进行求解。\n\n该问题要求在单位正方形域 $\\Omega = (0,1)\\times(0,1)$ 上求解二维拉普拉斯方程 $\\nabla^2 u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0$。势 $u(x,y)$ 满足狄利克雷边界条件：$u(x,0) = \\sin(\\pi x)$，在其他三个边界上 $u=0$。\n\n第一步是离散化连续问题。该域被一个具有 $(N+2) \\times (N+2)$ 个点的均匀网格覆盖，其中 $N$ 是每个方向上的内部点数。网格间距为 $h = \\frac{1}{N+1}$。对于索引 $i,j \\in \\{0, 1, \\dots, N+1\\}$，网格点表示为 $(x_i, y_j) = (ih, jh)$。该点处的势值为 $u_{i,j} \\approx u(x_i, y_j)$。\n\n在每个内部网格点 $(i,j)$ 处，拉普拉斯算子 $\\nabla^2$ 使用二阶中心差分公式进行近似：\n$$ \\nabla^2 u \\bigg|_{(x_i, y_j)} \\approx \\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1,j}}{h^2} + \\frac{u_{i,j+1} - 2u_{i,j} + u_{i,j-1}}{h^2} $$\n将此近似值设为 $0$ 可得到内部点（$1 \\le i,j \\le N$）的离散拉普拉斯方程：\n$$ u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4u_{i,j} = 0 $$\n这个五点模板方程可以重新排列，将 $u_{i,j}$ 表示为其四个邻点的平均值：\n$$ u_{i,j} = \\frac{1}{4} \\left( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\right) $$\n这组针对 $N^2$ 个内部未知数值的 $N^2$ 个线性方程构成一个大型稀疏线性系统，形式为 $A\\mathbf{u} = \\mathbf{b}$，其中 $\\mathbf{u}$ 是未知数 $u_{i,j}$ 的向量，矩阵 $A$ 代表离散拉普拉斯算子。右侧向量 $\\mathbf{b}$ 包含了固定的边界值。此类系统非常适合用迭代法求解。\n\n我们的任务是实现三种经典的定常迭代法：Jacobi、Gauss-Seidel 和逐次超松弛（SOR）。这些方法从一个初始猜测 $u^{(0)}$ 开始，生成一系列收敛到真实解的近似值 $u^{(k)}$。\n\nJacobi 方法是最简单的迭代格式。对于每个点 $(i,j)$，新值 $u_{i,j}^{(k+1)}$ 仅使用前一次迭代的值 $u^{(k)}$ 来计算。更新规则是平均值公式的直接应用：\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)} \\right) $$\n此更新可以同时（或以任何顺序）对所有内部点执行，因为在同一次迭代中，每个点的计算与其他点无关。在向量化实现中，需要第 $k$ 次迭代的网格的完整副本来计算第 $k+1$ 次迭代的网格。\n\nGauss-Seidel 方法通过在当前迭代中使用最新计算出的值来改进 Jacobi 方法。更新规则是：\n$$ u_{i,j}^{(k+1)} = \\frac{1}{4} \\left( u_{i+1,j}^{(*)} + u_{i-1,j}^{(*)} + u_{i,j+1}^{(*)} + u_{i,j-1}^{(*)} \\right) $$\n其中 $u^{(*)}$ 表示可用的最新值。例如，在字典序（逐行、逐列）中，计算 $u_{i,j}^{(k+1)}$ 会使用来自当前迭代 $k+1$ 的 $u_{i-1,j}^{(k+1)}$ 和 $u_{i,j-1}^{(k+1)}$，以及来自前一次迭代 $k$ 的 $u_{i+1,j}^{(k)}$ 和 $u_{i,j+1}^{(k)}$。这种对更新顺序的依赖性使并行化变得复杂。红黑排序方案可以规避这个问题。网格点像棋盘一样被着色。所有“红”点首先被更新，使用其“黑”邻点的值（来自前一次迭代）。然后，所有“黑”点被更新，使用其“红”邻点的新计算值。这两个阶段（红点更新、黑点更新）中的每一个都可以完全向量化。\n\n逐次超松弛（SOR）方法是 Gauss-Seidel 方法的一种外推，旨在加速收敛。它计算 Gauss-Seidel 更新，然后由松弛参数 $\\omega$ 控制，将解朝该方向进一步推进。更新公式为：\n$$ u_{i,j}^{(k+1)} = u_{i,j}^{(k)} + \\omega \\left( u_{i,j}^{\\text{GS}} - u_{i,j}^{(k)} \\right) = (1-\\omega)u_{i,j}^{(k)} + \\omega u_{i,j}^{\\text{GS}} $$\n其中 $u_{i,j}^{\\text{GS}}$ 是 Gauss-Seidel 步骤在该点计算出的值。与 Gauss-Seidel 一样，SOR 也使用红黑排序来实现，以有效地使用最新值。当 $\\omega=1$ 时，SOR 方法完全简化为 Gauss-Seidel 方法。对于拉普拉斯型问题，选择范围在 $1  \\omega  2$（超松弛）内的最优 $\\omega$ 通常会显著加快收敛速度。\n\n停止准则基于离散残差的无穷范数，定义为 $\\|r^{(k)}\\|_{\\infty} = \\max_{i,j} |r_{i,j}^{(k)}|$，其中 $r_{i,j}^{(k)} = 4u_{i,j}^{(k)} - (u_{i+1,j}^{(k)} + u_{i-1,j}^{(k)} + u_{i,j+1}^{(k)} + u_{i,j-1}^{(k)})$。当 $\\|r^{(k)}\\|_{\\infty} \\le \\text{tol} \\cdot \\|r^{(0)}\\|_{\\infty}$ 时，迭代停止，其中 $\\text{tol}$ 是给定的容差，$\\|r^{(0)}\\|_{\\infty}$ 是初始猜测（内部点 $u^{(0)}=0$）的残差范数。这种相对准则确保了不同问题设置之间的公平比较。\n\n该实现包含三个主要函数。一个函数用于设置 $(N+2) \\times (N+2)$ 网格，将内部初始化为 $0$ 并设置边界条件。第二个函数实现 Jacobi 迭代。第三个函数实现带有红黑排序的 SOR 迭代，通过设置 $\\omega=1$ 也可用于 Gauss-Seidel 方法。一个辅助函数在每一步计算残差范数。主程序遍历测试用例，调用相应的求解器函数，并记录收敛所需的迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_initial_state(N):\n    \"\"\"\n    Initializes the grid with boundary conditions and zero interior.\n\n    Args:\n        N (int): Number of interior points in each direction.\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The (N+2)x(N+2) grid `u`.\n            - float: The grid spacing `h`.\n    \"\"\"\n    h = 1.0 / (N + 1)\n    u = np.zeros((N + 2, N + 2))\n    \n    # Set boundary condition u(x,0) = sin(pi*x)\n    # The j=0 row corresponds to y=0.\n    x_coords = np.linspace(0, 1, N + 2)\n    u[0, :] = np.sin(np.pi * x_coords)\n    \n    # Other boundaries u(x,1)=0, u(0,y)=0, u(1,y)=0 are already zero.\n    return u, h\n\ndef calculate_residual_norm(u, N):\n    \"\"\"\n    Calculates the infinity norm of the residual on the interior grid.\n\n    Args:\n        u (np.ndarray): The full (N+2)x(N+2) grid.\n        N (int): Number of interior grid points.\n\n    Returns:\n        float: The infinity norm of the residual.\n    \"\"\"\n    interior = u[1:N + 1, 1:N + 1]\n    neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                     u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n    residual = 4 * interior - neighbors_sum\n    return np.max(np.abs(residual))\n\ndef solve_jacobi(N, tol):\n    \"\"\"\n    Solves the Laplace equation using the Jacobi method.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n    \n    threshold = tol * r0_norm\n    \n    k = 0\n    while True:\n        k += 1\n        \n        u_old = u.copy()\n        \n        neighbors_sum = (u_old[1:N + 1, 2:N + 2] + u_old[1:N + 1, 0:N] +\n                         u_old[2:N + 2, 1:N + 1] + u_old[0:N, 1:N + 1])\n        u[1:N + 1, 1:N + 1] = 0.25 * neighbors_sum\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve_sor(N, tol, omega):\n    \"\"\"\n    Solves the Laplace equation using SOR with red-black ordering.\n    Recovers Gauss-Seidel for omega=1.0.\n\n    Args:\n        N (int): Number of interior grid points.\n        tol (float): Convergence tolerance.\n        omega (float): Relaxation parameter.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    u, h = setup_initial_state(N)\n    \n    r0_norm = calculate_residual_norm(u, N)\n    if r0_norm == 0:\n        return 0\n        \n    threshold = tol * r0_norm\n    \n    # Create red-black masks for the interior (N x N) grid.\n    # (j, i) indices for the interior part start from 0.\n    # Grid point (j_grid, i_grid) where j_grid, i_grid in [1,N]\n    # corresponds to mask point (j_grid-1, i_grid-1).\n    # Color depends on (j_grid + i_grid). (j_grid-1) + (i_grid-1) has same parity.\n    I, J = np.meshgrid(np.arange(N), np.arange(N))\n    red_mask = (I + J) % 2 == 0\n    black_mask = ~red_mask\n    \n    k = 0\n    while True:\n        k += 1\n        \n        # Keep a copy of the interior from the start of the iteration\n        # for the (1-omega) term.\n        u_old_interior = u[1:N + 1, 1:N + 1].copy()\n\n        # Update red points. Neighbors are black, use values from start of iteration.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][red_mask] = (1 - omega) * u_old_interior[red_mask] + \\\n                                      omega * gs_update[red_mask]\n\n        # Update black points. Neighbors are red, use newly updated values.\n        neighbors_sum = (u[1:N + 1, 2:N + 2] + u[1:N + 1, 0:N] +\n                         u[2:N + 2, 1:N + 1] + u[0:N, 1:N + 1])\n        gs_update = 0.25 * neighbors_sum\n        u[1:N + 1, 1:N + 1][black_mask] = (1 - omega) * u_old_interior[black_mask] + \\\n                                        omega * gs_update[black_mask]\n        \n        r_norm = calculate_residual_norm(u, N)\n        if r_norm = threshold:\n            return k\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (20, 1e-5, 1.5),\n        (20, 1e-5, 1.0),\n        (10, 1e-8, 1.8),\n        (40, 1e-4, 1.9),\n    ]\n\n    results = []\n    for N, tol, omega_sor in test_cases:\n        # Calculate iterations for Jacobi\n        n_J = solve_jacobi(N, tol)\n        \n        # Calculate iterations for Gauss-Seidel (SOR with omega=1.0)\n        n_GS = solve_sor(N, tol, 1.0)\n        \n        # Calculate iterations for SOR with the specified omega\n        n_SOR = solve_sor(N, tol, omega_sor)\n        \n        results.append([n_J, n_GS, n_SOR])\n\n    # Format the output string as specified: [[r1,r2,r3],[...],...]\n    formatted_results = [f'[{\",\".join(map(str, r))}]' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2406769"}, {"introduction": "上一个练习展示了 SOR 方法的强大威力，但其效率高度依赖于一个关键参数——松弛因子 $\\omega$。这个最终练习将深入探讨如何通过寻找最优的 $\\omega$ 值来最大化 SOR 方法的性能。你将通过数值搜索的方法，通过最小化 SOR 迭代矩阵的谱半径来确定最优参数，从而加深对谱特性与收敛速度之间根本联系的理解。[@problem_id:2442097]", "problem": "要求您编写一个完整、可运行的程序，该程序通过数值方法估计求解二维泊松方程的有限差分格式（带有齐次狄利克雷边界条件）的最佳逐次超松弛（SOR）参数。目标是针对几种不同的网格尺寸，确定能够最小化 SOR 迭代矩阵谱半径的超松弛参数值。\n\n考虑在一个方形网格内部，对拉普拉斯算子使用标准五点有限差分格式，并施加齐次狄利克雷边界条件所产生的线性系统。设每个空间维度上有 $N$ 个内部点，未知数按字典序排列。该离散算子将产生一个稀疏对称正定矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其中 $n = N^2$。该矩阵的对角线元素为 $4$，对于四个基本方向上的直接邻居，其非对角线元素为 $-1$，其他位置为 $0$。\n\n求解 $A x = b$ 的一个定常迭代法通过将 $A$ 分解为 $A = D - L - U$ 来进行，其中 $D$ 是 $A$ 的对角部分，而 $L$ 和 $U$ 分别是 $-A$ 的严格下三角和严格上三角部分，因此 $L$ 和 $U$ 的元素均为非负。带有松弛参数 $\\omega \\in (0,2)$ 的逐次超松弛（SOR）迭代定义为：\n$$\nx^{(k+1)} = (D - \\omega L)^{-1} \\big( (1 - \\omega) D + \\omega U \\big) x^{(k)} + \\omega (D - \\omega L)^{-1} b .\n$$\n相应的迭代矩阵为：\n$$\nT_{\\mathrm{SOR}}(\\omega) = (D - \\omega L)^{-1} \\big( (1 - \\omega) D + \\omega U \\big) .\n$$\n对于任意初始猜测 $x^{(0)}$，迭代的收敛性由迭代矩阵的谱半径决定，记为 $\\rho(T_{\\mathrm{SOR}}(\\omega))$，其中\n$$\n\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\text{ 是 } M \\text{ 的一个特征值} \\} .\n$$\n$\\rho(T_{\\mathrm{SOR}}(\\omega))$ 的值越小，线性迭代的收敛速度越快。\n\n您的任务是：\n- 构建在 $N \\times N$ 内部网格上使用五点差分格式和齐次狄利克雷边界条件，并采用未知量字典序的矩阵 $A$。\n- 将 $A$ 分解为 $A = D - L - U$，其中 $D$ 是对角矩阵，$L, U$ 分别是 $-A$ 的严格下三角和严格上三角部分，使得 $A = D - L - U$ 精确成立。\n- 对于给定的 $\\omega \\in (0,2)$，计算 SOR 迭代矩阵 $T_{\\mathrm{SOR}}(\\omega)$ 及其谱半径 $\\rho(T_{\\mathrm{SOR}}(\\omega))$，方法是计算所有特征值并取其最大模。\n- 对于下面列出的每个测试用例，对 $\\omega$ 进行网格搜索，以近似找到 $\\rho(T_{\\mathrm{SOR}}(\\omega))$ 的最小值点。使用两阶段搜索：首先在 $\\omega \\in [0.2,1.95]$ 上以步长 $\\Delta \\omega = 0.05$ 进行粗略扫描，然后以粗略搜索得到的最佳值为中心，在一个宽度为 $0.10$ 的区间内进行精细化扫描，步长为 $\\Delta \\omega = 0.01$。该区间将被截断以保持在 $(0.05,1.95)$ 范围内。\n\n测试套件：\n- 使用 $N \\in \\{ 3, 6, 8, 10 \\}$。\n\n对于测试套件中的每个 $N$，您的程序必须输出通过上述搜索找到的 $\\rho(T_{\\mathrm{SOR}}(\\omega))$ 的近似最小值点 $\\omega_{\\mathrm{opt}}$。报告每个 $\\omega_{\\mathrm{opt}}$，并四舍五入到恰好四位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，结果的顺序与测试套件的顺序相同。例如，如果有四个结果 $w_1, w_2, w_3, w_4$，则打印单行“[w1,w2,w3,w4]”，其中每个 $w_i$ 是一个四舍五入到恰好四位小数的十进制字符串。此问题不涉及任何物理单位或角度；所有报告的值都是无量纲的。", "solution": "该问题是数值线性代数和计算物理中的一个标准练习。它要求数值确定应用于二维泊松方程 $-\\nabla^2 u = f$（在方形域上，带有齐次狄利克雷边界条件）的有限差分格式所导出线性系统的最佳超松弛参数 $\\omega_{\\mathrm{opt}}$。该问题在科学上是合理的、适定的，并且解决它所需的所有参数都已提供。因此，我们着手求解。\n\n问题的核心在于理解定常迭代法的收敛速度由其迭代矩阵的谱半径决定。对于 SOR 方法，这个矩阵表示为 $T_{\\mathrm{SOR}}(\\omega)$，我们的目标是找到在区间 $(0, 2)$ 内的松弛参数 $\\omega$ 值，以最小化其谱半径 $\\rho(T_{\\mathrm{SOR}}(\\omega))$。\n\n首先，我们必须为线性系统 $Ax=b$ 构建系数矩阵 $A$。在 $N \\times N$ 内部网格上的五点差分格式会产生一个包含 $n = N^2$ 个线性方程的系统。采用字典序对网格点进行排序，其中网格坐标为 $(i, j)$（$1 \\le i, j \\le N$）的点被映射到单一索引 $k = (i-1)N + j-1$，该点处的离散拉普拉斯算子为：\n$$\n4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = h^2 f_{i,j}\n$$\n其中 $h$ 是网格间距。这个结构定义了矩阵 $A \\in \\mathbb{R}^{n \\times n}$。$A$ 是一个块三对角矩阵：\n$$\nA = \\begin{pmatrix}\nB  -I  0  \\cdots  0 \\\\\n-I  B  -I  \\ddots  \\vdots \\\\\n0  \\ddots  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  -I  B  -I \\\\\n0  \\cdots  0  -I  B\n\\end{pmatrix}\n$$\n其中每个块都是一个 $N \\times N$ 的矩阵。块 $I$ 是单位矩阵，块 $B$ 是一个三对角矩阵，代表单个网格行内的连接：\n$$\nB = \\begin{pmatrix}\n4  -1  0  \\cdots  0 \\\\\n-1  4  -1  \\ddots  \\vdots \\\\\n0  \\ddots  \\ddots  \\ddots  0 \\\\\n\\vdots  \\ddots  -1  4  -1 \\\\\n0  \\cdots  0  -1  4\n\\end{pmatrix}\n$$\n矩阵 $A$ 是稀疏、对称且正定的。\n\nSOR 方法基于将矩阵 $A$ 分解为 $A = D - L - U$，其中 $D$ 是 $A$ 的对角部分，$-L$ 是 $A$ 的严格下三角部分，$-U$ 是 $A$ 的严格上三角部分。根据 $A$ 的结构，$D$ 是一个纯量矩阵 $4I_n$，其中 $I_n$ 是 $n \\times n$ 的单位矩阵。矩阵 $L$ 和 $U$ 分别是严格下三角和上三角矩阵，由元素 $0$ 和 $1$ 组成。\n\nSOR 迭代由以下递推关系定义：\n$$\nx^{(k+1)} = T_{\\mathrm{SOR}}(\\omega) x^{(k)} + c\n$$\n其中 SOR 迭代矩阵 $T_{\\mathrm{SOR}}(\\omega)$ 由下式给出：\n$$\nT_{\\mathrm{SOR}}(\\omega) = (D - \\omega L)^{-1} \\big( (1 - \\omega) D + \\omega U \\big)\n$$\n收敛速度由谱半径 $\\rho(T_{\\mathrm{SOR}}(\\omega)) = \\max_i |\\lambda_i|$ 决定，其中 $\\{\\lambda_i\\}$ 是 $T_{\\mathrm{SOR}}(\\omega)$ 的特征值。我们必须找到 $\\omega_{\\mathrm{opt}} = \\arg\\min_{\\omega \\in (0,2)} \\rho(T_{\\mathrm{SOR}}(\\omega))$。\n\n对于给定的网格尺寸 $N$，计算步骤如下：\n1.  根据上述块三对角结构，构建 $n \\times n$ 矩阵 $A$，其中 $n=N^2$。\n2.  将 $A$ 分解为其对角部分 $D$、严格下三角部分 $-L$ 和严格上三角部分 $-U$。注意，$D = \\text{diag}(A)$，$L = -\\text{tril}(A, -1)$，以及 $U = -\\text{triu}(A, 1)$。\n3.  实现一个函数，对于给定的 $\\omega$ 值，计算谱半径 $\\rho(T_{\\mathrm{SOR}}(\\omega))$。这包括：\n    a. 构造矩阵 $M = D - \\omega L$ 和 $P = (1 - \\omega) D + \\omega U$。\n    b. 计算 SOR 矩阵 $T_{\\mathrm{SOR}}(\\omega) = M^{-1} P$。由于 $M$ 是一个下三角矩阵，其逆矩阵是良定义的，可以用数值方法计算。\n    c. 计算 $T_{\\mathrm{SOR}}(\\omega)$ 的所有特征值。\n    d. 找出特征值绝对值的最大值，即为谱半径。\n\n4.  执行两阶段网格搜索以找到最佳的 $\\omega$。\n    a.  **粗略搜索：** 以步长 $\\Delta\\omega = 0.05$ 在 $\\omega$ 从 $0.20$ 到 $1.95$ 的范围内迭代。对每个 $\\omega$ 计算 $\\rho(T_{\\mathrm{SOR}}(\\omega))$，并找出产生最小谱半径的值 $\\omega_{\\text{coarse\\_opt}}$。\n    b.  **精细化搜索：** 定义一个新的搜索区间 $[\\omega_{\\text{coarse\\_opt}} - 0.05, \\omega_{\\text{coarse\\_opt}} + 0.05]$，并截断使其位于 $(0.05, 1.95)$ 内部。在此新区间内以更精细的步长 $\\Delta\\omega = 0.01$ 进行搜索。在此精细化搜索中使谱半径最小化的 $\\omega$ 值即为我们对 $\\omega_{\\mathrm{opt}}$ 的估计。\n\n对于测试套件中的每个 $N$ 值（即 $N \\in \\{ 3, 6, 8, 10 \\}$），重复整个过程。最终结果是每个网格尺寸的估计最优参数 $\\omega_{\\mathrm{opt}}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef construct_poisson_matrix(N):\n    \"\"\"\n    Constructs the matrix for the 2D Poisson problem on an N x N grid.\n    \n    Args:\n        N (int): The number of interior grid points in one dimension.\n        \n    Returns:\n        numpy.ndarray: The (N*N) x (N*N) matrix A.\n    \"\"\"\n    n = N * N\n    \n    # Create the main diagonal block B\n    B = np.diag([4] * N) + np.diag([-1] * (N - 1), k=1) + np.diag([-1] * (N - 1), k=-1)\n    \n    # Create the full matrix A using Kronecker products\n    A = np.kron(np.eye(N), B)\n    \n    # Create identity matrices for off-diagonal blocks\n    off_diag_block = -np.eye(N)\n    \n    # Add the off-diagonal blocks\n    A += np.kron(np.diag(np.ones(N - 1), k=1), off_diag_block)\n    A += np.kron(np.diag(np.ones(N - 1), k=-1), off_diag_block)\n    \n    return A\n\ndef calculate_sor_spectral_radius(omega, D, L, U):\n    \"\"\"\n    Calculates the spectral radius of the SOR iteration matrix for a given omega.\n    \n    Args:\n        omega (float): The relaxation parameter.\n        D (numpy.ndarray): The diagonal part of matrix A.\n        L (numpy.ndarray): The strictly lower triangular part of -A.\n        U (numpy.ndarray): The strictly upper triangular part of -A.\n        \n    Returns:\n        float: The spectral radius of the SOR iteration matrix.\n    \"\"\"\n    # Form the matrix (D - omega*L)\n    D_minus_omega_L = D - omega * L\n    \n    # Form the matrix ((1-omega)*D + omega*U)\n    rhs_matrix = (1 - omega) * D + omega * U\n    \n    # Compute the SOR iteration matrix T_SOR = (D - omega*L)^-1 * (...)\n    # Since D_minus_omega_L is triangular, linalg.solve is more stable and efficient\n    # than computing the inverse explicitly, but for small dense matrices, inv is fine.\n    inv_D_minus_omega_L = np.linalg.inv(D_minus_omega_L)\n    T_sor = inv_D_minus_omega_L @ rhs_matrix\n    \n    # Calculate eigenvalues and spectral radius\n    eigenvalues = np.linalg.eigvals(T_sor)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    \n    return spectral_radius\n\ndef find_optimal_omega(N):\n    \"\"\"\n    Finds the optimal SOR parameter for a given grid size N using a two-stage search.\n    \n    Args:\n        N (int): The number of interior grid points in one dimension.\n        \n    Returns:\n        float: The estimated optimal omega.\n    \"\"\"\n    A = construct_poisson_matrix(N)\n    D = np.diag(np.diag(A))\n    L = -np.tril(A, k=-1)\n    U = -np.triu(A, k=1)\n    \n    # Stage 1: Coarse search\n    coarse_omegas = np.linspace(0.2, 1.95, int(round((1.95 - 0.2) / 0.05)) + 1)\n    min_rho_coarse = float('inf')\n    omega_coarse_opt = 0.0\n    \n    for omega in coarse_omegas:\n        rho = calculate_sor_spectral_radius(omega, D, L, U)\n        if rho  min_rho_coarse:\n            min_rho_coarse = rho\n            omega_coarse_opt = omega\n            \n    # Stage 2: Refined search\n    # Define search interval, truncated to (0.05, 1.95)\n    fine_start = max(0.051, omega_coarse_opt - 0.05) # Use 0.051 to avoid floating point issues at boundary\n    fine_end = min(1.949, omega_coarse_opt + 0.05)   # Use 1.949 for the same reason\n    \n    num_points = int(round((fine_end - fine_start) / 0.01)) + 1\n    fine_omegas = np.linspace(fine_start, fine_end, num_points)\n    \n    min_rho_fine = float('inf')\n    omega_opt = 0.0\n    \n    for omega in fine_omegas:\n        rho = calculate_sor_spectral_radius(omega, D, L, U)\n        if rho  min_rho_fine:\n            min_rho_fine = rho\n            omega_opt = omega\n            \n    return omega_opt\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [3, 6, 8, 10]\n\n    results = []\n    for N in test_cases:\n        optimal_omega = find_optimal_omega(N)\n        results.append(optimal_omega)\n\n    # Format the final output as specified\n    formatted_results = [f\"{r:.4f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2442097"}]}