{"hands_on_practices": [{"introduction": "逐次超松弛（SOR）方法的效率在很大程度上取决于松弛参数 $\\omega$ 的选择。一个经过精心优化的 $\\omega$ 值可以显著减少收敛所需的迭代次数。本练习 [@problem_id:2397060] 将指导你通过编写代码，对给定的几何形状凭经验确定最优的 $\\omega_{\\mathrm{opt}}$，让你亲身体验如何通过实验手段来校准和优化数值算法。", "problem": "您需要实现一个二维拉普拉斯方程的数值求解器，使用逐次超松弛（SOR）方法来加速高斯–赛德尔迭代。您的实现必须通过经验方法确定在具有狄利克雷边界条件的特定矩形几何区域下的最优松弛参数 $\\,\\omega_{\\mathrm{opt}}\\,\\in(0,2)\\,$。此任务必须从第一性原理出发，即从拉普拉斯方程的定义和在均匀笛卡尔网格上的标准五点有限差分离散格式开始。\n\n您必须遵守以下要求。\n\n- 从二维空间中的拉普拉斯方程定义 $\\,\\nabla^2 u = 0\\,$ 和在矩形网格上的五点均匀网格有限差分离散格式出发。使用通过逐次超松弛（SOR）增强的高斯–赛德尔迭代。除了这些基础知识外，不要假定任何预先推导的公式。\n- 使用红黑排序（棋盘式更新），以便在每次迭代中，首先更新所有红色内部点，然后更新所有黑色内部点，并进行原地更新。\n- 采用以下收敛准则。将一次完整的红黑扫描定义为一次“迭代”。每次扫描后，将迭代间的变化量定义为在所有已更新的内部点（不包括狄利克雷固定的边界点和任何内部狄利克雷障碍物）上，更新后的值与先前值之间的最大绝对差。当更新量的无穷范数低于容差 $\\,\\tau = 10^{-5}\\,$ 时，或当迭代次数达到最大值 $\\,I_{\\max} = 5000\\,$ 时停止，以先到者为准。\n- 对所有内部未知数使用零初始猜测，即在每次求解开始时，内部点的 $\\,u_{i,j}=0\\,$。在迭代开始前应用狄利克雷边界值和任何内部狄利克雷障碍物的值，并在迭代过程中保持它们固定不变。\n- 将 $\\,\\omega_{\\mathrm{opt}}\\,$ 的经验性搜索定义为一个两阶段搜索：\n  - 粗略阶段：在网格 $\\,\\{1.0, 1.1, 1.2, \\dots, 1.9\\}\\,$ 上测试 $\\,\\omega\\,$。\n  - 精细阶段：设 $\\,\\omega_c\\,$ 为粗略阶段中使收敛迭代次数最小的 $\\,\\omega\\,$ 值（若有多个，则选择最小的 $\\,\\omega\\,$）。然后在均匀网格 $\\,\\max(1.0,\\omega_c-0.05),\\,\\max(1.0,\\omega_c-0.05)+0.01,\\,\\dots,\\,\\min(1.95,\\omega_c+0.05)\\,$ 上测试 $\\,\\omega\\,$。选择 $\\,\\omega_{\\mathrm{opt}}\\,$ 作为精细网格上使迭代次数最小的 $\\,\\omega\\,$ 值（同样，若有多个则选择最小的 $\\,\\omega\\,$）。报告四舍五入到两位小数的 $\\,\\omega_{\\mathrm{opt}}\\,$，并将相应的最小迭代次数作为整数报告。仅在选定最小值点后进行四舍五入。\n- 在两个方向上均使用均匀的网格间距。您不需要报告物理单位，也不应使用角度量。\n\n测试套件。实现您的程序以计算并报告以下三（3）个测试用例的结果。\n\n- 用例 A（带有热顶边的正方形）：\n  - 网格：$\\,N_x = 50\\,$, $\\,N_y = 50\\,$。\n  - 狄利克雷边界值：顶边 $\\,u=1.0\\,$，底边、左边、右边 $\\,u=0.0\\,$。\n  - 无内部障碍物。\n- 用例 B（带有热左边的矩形）：\n  - 网格：$\\,N_x = 60\\,$, $\\,N_y = 30\\,$。\n  - 狄利克雷边界值：左边 $\\,u=1.0\\,$，顶边、底边、右边 $\\,u=0.0\\,$。\n  - 无内部障碍物。\n- 用例 C（带有温热内部障碍物的正方形）：\n  - 网格：$\\,N_x = 40\\,$, $\\,N_y = 40\\,$。\n  - 狄利克雷边界值：所有四个外边界 $\\,u=0.0\\,$。\n  - 内部狄利克雷障碍物：一个边长为 $\\,8\\,$ 个内部网格点的中心正方形（即，在从零开始的索引中，索引 $\\,x\\in\\{16,17,\\dots,23\\}\\,$ 和 $\\,y\\in\\{16,17,\\dots,23\\}\\,$），其值固定为 $\\,u=0.5\\,$。\n\n最终输出格式。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是一个双元素列表 $[\\omega_{\\mathrm{opt}}, I_{\\min}]$，其中 $\\,\\omega_{\\mathrm{opt}}\\,$ 四舍五入到两位小数，$\\,I_{\\min}\\,$ 是作为整数的最小迭代次数。因此，最后一行必须类似于\n$\\,\\big[[\\omega^{(A)}_{\\mathrm{opt}}, I^{(A)}_{\\min}], [\\omega^{(B)}_{\\mathrm{opt}}, I^{(B)}_{\\min}], [\\omega^{(C)}_{\\mathrm{opt}}, I^{(C)}_{\\min}]\\big]\\,$\n，不含任何额外文本。例如，一个语法正确的输出将是\n$\\,\\big[[1.80, 250], [1.90, 180], [1.75, 310]\\big]\\,$\n，尽管这些不是此问题的预期数值。", "solution": "我们从二维空间中的拉普拉斯方程开始，\n$$\n\\nabla^2 u \\;=\\; \\frac{\\partial^2 u}{\\partial x^2} \\;+\\; \\frac{\\partial^2 u}{\\partial y^2} \\;=\\; 0,\n$$\n该方程设定在一个具有狄利克雷边界数据的矩形上。在一个两个方向间距均为 $\\,h\\,$ 的均匀笛卡尔网格上，内部索引为 $\\,i=1,\\dots,N_x-2\\,$ 和 $\\,j=1,\\dots,N_y-2\\,$（最外层索引为狄利克雷边界保留），经典的五点有限差分（FD）近似得到离散拉普拉斯算子\n$$\n\\frac{u_{i+1,j} - 2\\,u_{i,j} + u_{i-1,j}}{h^2} \\;+\\; \\frac{u_{i,j+1} - 2\\,u_{i,j} + u_{i,j-1}}{h^2} \\;=\\; 0,\n$$\n对于均匀的 $\\,h\\,$ 和纯拉普拉斯问题（无源项），该式简化为不动点关系式：\n$$\nu_{i,j} \\;=\\; \\frac{1}{4}\\,\\big( u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} \\big).\n$$\n这个恒等式提示了一种迭代方法：高斯–赛德尔（GS）迭代使用最新可用的邻居值来原地更新每个内部点 $\\,u_{i,j}\\,$。当某些网格点是狄利克雷固定的（边界或内部障碍物），它们将不被更新并作为常数保留，因此它们的值会参与邻居值的求和但自身保持不变。\n\n逐次超松弛（SOR）通过将 GS 更新量朝不动点方向进行凸外插来加速 GS 迭代。将在 $\\,\\{i,j\\}\\,$ 点的高斯–赛德尔更新量表示为\n$$\nu^{\\mathrm{GS}}_{i,j} \\;=\\; \\frac{1}{4}\\,\\big( u_{i+1,j}^{\\star} + u_{i-1,j}^{\\star} + u_{i,j+1}^{\\star} + u_{i,j-1}^{\\star} \\big),\n$$\n其中每个邻居值 $\\,u^{\\star}\\,$ 是当前的原地值（一些邻居在同一次扫描中已被更新，另一些则尚未更新）。使用松弛参数 $\\,\\omega\\in(0,2)\\,$ 的 SOR 更新公式为\n$$\nu^{\\mathrm{new}}_{i,j} \\;=\\; (1-\\omega)\\,u^{\\mathrm{old}}_{i,j} \\;+\\; \\omega\\,u^{\\mathrm{GS}}_{i,j}.\n$$\n对于红黑排序，网格根据 $\\,i+j\\,$ 的奇偶性被划分为两个交错的集合。首先使用黑色邻居点更新所有红色点（因为每个红色点的最近邻都是黑色的），然后使用新更新的红色邻居点更新所有黑色点，从而完成一次迭代（一次扫描）。这种排序方式可以实现向量化更新，并保留了高斯–赛德尔的依赖结构。\n\n收敛性通过每次迭代变化量的无穷范数来监控。设 $\\,\\Delta^{(k)}\\,$ 为第 $\\,k\\,$ 次迭代期间所有已更新内部点上更新后值与先前值之间的最大绝对差。当\n$$\n\\Delta^{(k)}  \\tau \\;=\\; 10^{-5},\n$$\n或当迭代次数达到上限 $\\,I_{\\max}=5000\\,$ 时，我们终止迭代。所有内部点的初始猜测值设为 $\\,u_{i,j}=0\\,$，狄利克雷边界和内部障碍物的值被施加并保持固定。\n\n为了通过经验方法确定 $\\,\\omega_{\\mathrm{opt}}\\,$，我们在一个预设的 $\\,\\omega\\,$ 值集合上，最小化满足停止准则所需的迭代次数。我们采用两阶段搜索来平衡鲁棒性和计算成本：\n\n- 粗略阶段：评估 $\\,\\omega \\in \\{1.0, 1.1, \\dots, 1.9\\}\\,$，并选择使迭代次数最小的 $\\,\\omega_c\\,$（若有多个，则选择较小的 $\\,\\omega\\,$）。\n- 精细阶段：在步长为 $\\,0.01\\,$ 的均匀网格上，评估邻域 $\\,\\omega \\in [\\max(1.0,\\omega_c-0.05), \\min(1.95,\\omega_c+0.05)]\\,$，并选择最小值点 $\\,\\omega_{\\mathrm{opt}}\\,$（同样，若有多个则选择较小的 $\\,\\omega\\,$）。仅在选定最小值点后，我们才将 $\\,\\omega_{\\mathrm{opt}}\\,$ 四舍五入到两位小数以供报告。\n\n我们实现三种几何构型。\n\n- 用例 A：$\\,N_x=50\\,$, $\\,N_y=50\\,$。顶边固定为 $\\,u=1.0\\,$；其他边界固定为 $\\,u=0.0\\,$；无障碍物。\n- 用例 B：$\\,N_x=60\\,$, $\\,N_y=30\\,$。左边固定为 $\\,u=1.0\\,$；其他边界固定为 $\\,u=0.0\\,$；无障碍物。\n- 用例 C：$\\,N_x=40\\,$, $\\,N_y=40\\,$。所有外边界固定为 $\\,u=0.0\\,$。一个边长为 $\\,8\\,$ 个内部网格点的中心内部狄利克雷障碍物，其索引为 $\\,x\\in\\{16,\\dots,23\\},\\,y\\in\\{16,\\dots,23\\}\\,$，固定为 $\\,u=0.5\\,$。\n\n为保证效率和正确性的算法细节：\n\n- 在内部点上构建布尔掩码，以识别红色和黑色的更新位置，并可选择性地将障碍物单元从更新中排除。\n- 在每个红色（或黑色）半扫描中，计算整个内部区域的邻居和 $\\,u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1}\\,$，然后仅在红色（或黑色）掩码上应用 SOR 更新。对于红色半扫描，邻居和使用的值中，所有红色点的邻居实际上都是黑色单元；因此，求和正确地使用了最新的黑色值。在红色点更新后，重新计算邻居和，以便黑色点的更新能看到已更新的红色邻居。\n- 将该次扫描中所有更新的最大绝对变化量追踪为 $\\,\\Delta^{(k)}\\,$。\n\n然后，程序对每个测试用例执行两阶段搜索，记录最佳的 $\\,\\omega_{\\mathrm{opt}}\\,$ 及其最小迭代次数 $\\,I_{\\min}\\,$，将 $\\,\\omega_{\\mathrm{opt}}\\,$ 四舍五入到两位小数，并打印包含列表的单行：\n$$\n\\big[[\\omega^{(A)}_{\\mathrm{opt}}, I^{(A)}_{\\min}], [\\omega^{(B)}_{\\mathrm{opt}}, I^{(B)}_{\\min}], [\\omega^{(C)}_{\\mathrm{opt}}, I^{(C)}_{\\min}]\\big].\n$$\n此过程对于具有狄利克雷数据的均匀网格上的拉普拉斯类型问题是通用的，并演示了逐次超松弛（SOR）如何加速高斯–赛德尔迭代，以及如何为给定的离散几何体通过经验方法调整松弛参数 $\\,\\omega\\,$。", "answer": "```python\nimport numpy as np\n\ndef sor_red_black(\n    nx: int,\n    ny: int,\n    boundary_values: dict,\n    obstacle_mask: np.ndarray | None = None,\n    obstacle_value: float | None = None,\n    omega: float = 1.5,\n    tol: float = 1e-5,\n    max_iters: int = 5000,\n) - int:\n    \"\"\"\n    Perform red-black SOR for Laplace's equation on an nx-by-ny grid.\n    - boundary_values: dict with any of 'top','bottom','left','right' mapping to floats.\n      Edges not specified default to 0.0.\n    - obstacle_mask: full-size boolean array (ny, nx), True where Dirichlet-fixed internal obstacle exists.\n    - obstacle_value: float value assigned to obstacle cells (required if obstacle_mask is provided).\n    - omega: relaxation parameter in (0,2).\n    - tol: stopping tolerance on infinity norm of per-iteration change.\n    - max_iters: maximum number of red-black sweeps.\n    Returns: number of iterations (sweeps) taken to reach tol (or max_iters if not reached).\n    \"\"\"\n    # Initialize potential field\n    u = np.zeros((ny, nx), dtype=np.float64)\n\n    # Apply boundary conditions (unspecified edges are zero)\n    if 'bottom' in boundary_values:\n        u[0, :] = boundary_values['bottom']\n    if 'top' in boundary_values:\n        u[-1, :] = boundary_values['top']\n    if 'left' in boundary_values:\n        u[:, 0] = boundary_values['left']\n    if 'right' in boundary_values:\n        u[:, -1] = boundary_values['right']\n\n    # Internal Dirichlet obstacle\n    if obstacle_mask is not None:\n        if obstacle_value is None:\n            raise ValueError(\"obstacle_value must be provided when obstacle_mask is given.\")\n        u[obstacle_mask] = obstacle_value\n\n    # Interior masks (exclude outer boundary)\n    # Boolean mask of interior points that are updatable (not obstacle)\n    interior_shape = (ny - 2, nx - 2)\n    if interior_shape[0] = 0 or interior_shape[1] = 0:\n        return 0  # no interior to update\n\n    # Build interior obstacle mask\n    if obstacle_mask is not None:\n        obsI = obstacle_mask[1:-1, 1:-1].copy()\n    else:\n        obsI = np.zeros(interior_shape, dtype=bool)\n\n    update_mask = ~obsI  # True where we update\n\n    # Red-black masks over the interior\n    ii = np.arange(1, ny - 1)[:, None]\n    jj = np.arange(1, nx - 1)[None, :]\n    parity = (ii + jj) % 2  # 0 for even (red), 1 for odd (black)\n    red_mask = (parity == 0)\n    black_mask = ~red_mask\n    red_mask = red_mask  update_mask\n    black_mask = black_mask  update_mask\n\n    # Helper to compute neighbor sum over interior\n    def neighbor_sum(U):\n        return (U[2:, 1:-1] + U[:-2, 1:-1] + U[1:-1, 2:] + U[1:-1, :-2])\n\n    # Iterate\n    iters = 0\n    for k in range(1, max_iters + 1):\n        max_change = 0.0\n\n        # Red half-sweep\n        s = neighbor_sum(u)\n        uI = u[1:-1, 1:-1]\n        if np.any(red_mask):\n            old_vals = uI[red_mask]\n            gs_vals = 0.25 * s[red_mask]\n            new_vals = (1.0 - omega) * old_vals + omega * gs_vals\n            change = np.max(np.abs(new_vals - old_vals)) if new_vals.size else 0.0\n            max_change = max(max_change, float(change))\n            uI[red_mask] = new_vals\n\n        # Black half-sweep\n        s = neighbor_sum(u)  # recompute after red updates\n        if np.any(black_mask):\n            old_vals = uI[black_mask]\n            gs_vals = 0.25 * s[black_mask]\n            new_vals = (1.0 - omega) * old_vals + omega * gs_vals\n            change = np.max(np.abs(new_vals - old_vals)) if new_vals.size else 0.0\n            max_change = max(max_change, float(change))\n            uI[black_mask] = new_vals\n\n        iters = k\n        if max_change  tol:\n            break\n\n    return iters\n\n\ndef empirical_omega_opt(\n    nx: int,\n    ny: int,\n    boundary_values: dict,\n    obstacle_mask: np.ndarray | None,\n    obstacle_value: float | None,\n    tol: float = 1e-5,\n    max_iters: int = 5000,\n):\n    \"\"\"\n    Two-stage empirical search for omega_opt minimizing iteration count.\n    Returns (omega_opt_rounded_to_2_decimals, min_iterations).\n    \"\"\"\n    # Coarse grid: 1.0 to 1.9 step 0.1\n    coarse_candidates = [round(1.0 + 0.1 * i, 10) for i in range(0, 10)]  # [1.0, ..., 1.9]\n    coarse_results = []\n    for w in coarse_candidates:\n        it = sor_red_black(\n            nx, ny, boundary_values,\n            obstacle_mask=obstacle_mask,\n            obstacle_value=obstacle_value,\n            omega=w, tol=tol, max_iters=max_iters\n        )\n        coarse_results.append((w, it))\n    # Select best coarse (min iters, tie-break by smallest omega)\n    min_it_coarse = min(it for _, it in coarse_results)\n    best_coarse = min([w for (w, it) in coarse_results if it == min_it_coarse])\n\n    # Fine grid around best_coarse: step 0.01 within [1.0, 1.95]\n    start = max(1.0, best_coarse - 0.05)\n    end = min(1.95, best_coarse + 0.05)\n    n_steps = int(round((end - start) / 0.01))  # inclusive range\n    fine_candidates = [round(start + 0.01 * i, 10) for i in range(n_steps + 1)]\n    fine_results = []\n    for w in fine_candidates:\n        it = sor_red_black(\n            nx, ny, boundary_values,\n            obstacle_mask=obstacle_mask,\n            obstacle_value=obstacle_value,\n            omega=w, tol=tol, max_iters=max_iters\n        )\n        fine_results.append((w, it))\n    min_it_fine = min(it for _, it in fine_results)\n    best_fine = min([w for (w, it) in fine_results if it == min_it_fine])\n\n    return round(best_fine, 2), int(min_it_fine)\n\n\ndef build_obstacle_mask(ny: int, nx: int, y0: int, y1: int, x0: int, x1: int) - np.ndarray:\n    \"\"\"\n    Build a full-size boolean mask with True on [y0:y1, x0:x1] inclusive of endpoints if using slice semantics.\n    Here, we assume x1, y1 are exclusive upper bounds for numpy slicing.\n    \"\"\"\n    mask = np.zeros((ny, nx), dtype=bool)\n    mask[y0:y1, x0:x1] = True\n    return mask\n\n\ndef solve():\n    results = []\n\n    # Common solver settings\n    tol = 1e-5\n    max_iters = 5000\n\n    # Case A: Nx=50, Ny=50, top=1.0, others=0.0, no obstacle\n    nx_A, ny_A = 50, 50\n    bvals_A = {'top': 1.0, 'bottom': 0.0, 'left': 0.0, 'right': 0.0}\n    omega_A, it_A = empirical_omega_opt(\n        nx_A, ny_A, bvals_A,\n        obstacle_mask=None, obstacle_value=None,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_A, it_A])\n\n    # Case B: Nx=60, Ny=30, left=1.0, others=0.0, no obstacle\n    nx_B, ny_B = 60, 30\n    bvals_B = {'left': 1.0, 'top': 0.0, 'bottom': 0.0, 'right': 0.0}\n    omega_B, it_B = empirical_omega_opt(\n        nx_B, ny_B, bvals_B,\n        obstacle_mask=None, obstacle_value=None,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_B, it_B])\n\n    # Case C: Nx=40, Ny=40, all edges 0.0, centered 8x8 obstacle at 0.5\n    nx_C, ny_C = 40, 40\n    bvals_C = {'top': 0.0, 'bottom': 0.0, 'left': 0.0, 'right': 0.0}\n    # Center indices for obstacle: 8x8 block\n    side = 8\n    x0 = nx_C // 2 - side // 2\n    x1 = x0 + side\n    y0 = ny_C // 2 - side // 2\n    y1 = y0 + side\n    obstacle_C = build_obstacle_mask(ny_C, nx_C, y0, y1, x0, x1)\n    omega_C, it_C = empirical_omega_opt(\n        nx_C, ny_C, bvals_C,\n        obstacle_mask=obstacle_C, obstacle_value=0.5,\n        tol=tol, max_iters=max_iters\n    )\n    results.append([omega_C, it_C])\n\n    # Final output in the exact required format\n    # Print a single line: [[omegaA,iterA],[omegaB,iterB],[omegaC,iterC]]\n    print(str(results).replace(' ', ''))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2397060"}, {"introduction": "在凭经验找到了最优松弛参数后，一个自然的问题是：我们能否从理论上预测这个值？对于一些定义明确的问题，例如矩形域上的拉普拉斯方程，存在一个优美的解析理论。本练习 [@problem_id:2444079] 将引导你探索最优松弛参数 $\\omega_{\\mathrm{opt}}$ 与雅可比（Jacobi）迭代矩阵谱半径之间的深刻联系，从而从理论层面理解算法性能如何随问题几何形状而变化。", "problem": "考虑在尺寸为 $N \\times M$ 的均匀矩形内部点网格上，使用标准五点差分格式求解带有狄利克雷边界条件的二维拉普拉斯方程。令 $u_{i,j}$ 表示内部网格点 $(i,j)$ 处的网格函数，其中 $i \\in \\{1,\\dots,N\\}$ 且 $j \\in \\{1,\\dots,M\\}$。离散拉普拉斯算子由标准的二阶中心差分近似定义。通过将线性系统的系数矩阵分裂为对角、严格下三角和严格上三角部分，可以构造一个定常线性迭代法。特别地，逐次超松弛（SOR）方法是通过一个标量松弛因子 $\\omega \\in (0,2)$ 对高斯-赛德尔（GS）更新进行松弛得到的。\n\n你的任务是，从第一性原理出发，确定最优松弛因子 $\\omega_{\\mathrm{opt}}$ 如何依赖于网格维度 $N \\times M$，特别是在网格高度各向异性（例如，一个维度远大于另一个维度的长而窄的通道）的情况下。请按以下步骤进行。\n\n1) 从两个方向上具有均匀网格间距的矩形网格上的拉普拉斯方程的离散形式开始。根据离散系统的矩阵分裂来定义雅可比（Jacobi）和高斯-赛德尔（Gauss–Seidel）迭代。使用分离变量法分析雅可比迭代算子的本征模，将其特征值用模态指数和网格尺寸的三角函数表示。目标是将雅可比迭代的谱半径（记为 $\\rho_J$）表示为 $N$ 和 $M$ 的函数。\n\n2) 利用对称正定矩阵的定常迭代谱分析的标准结果，将最优SOR参数 $\\omega_{\\mathrm{opt}}$ 与 $\\rho_J$ 联系起来。使用基于线性迭代理论和五点离散拉普拉斯算子结构的严格论证来证明该关系。所有三角函数必须使用以弧度为单位的角度。\n\n3) 实现一个程序，给定 $(N,M)$，根据步骤1中的特征值分析计算 $\\rho_J$，然后根据步骤2中的关系计算 $\\omega_{\\mathrm{opt}}$。你的程序应计算并报告以下每个测试用例的 $\\omega_{\\mathrm{opt}}$：\n- $(N,M) = (\\,32,\\,32\\,)$\n- $(N,M) = (\\,64,\\,8\\,)$\n- $(N,M) = (\\,8,\\,64\\,)$\n- $(N,M) = (\\,3,\\,3\\,)$\n- $(N,M) = (\\,3,\\,50\\,)$\n\n4) 为了数值再现性，你的程序必须将每个 $\\omega_{\\mathrm{opt}}$ 输出为精确到6位小数的浮点数。\n\n最终输出格式：你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，且没有空格。例如，如果有三个结果 $a$、$b$ 和 $c$，输出格式将是“[a,b,c]”。每个列表元素必须是精确到6位小数的浮点数。", "solution": "我们从一个在尺寸为 $N \\times M$、两个方向上具有均匀网格间距的矩形内部节点网格上的二维拉普拉斯方程开始。标准的五点差分格式导出一个线性系统 $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$，其中 $\\mathbf{A}$ 是一个与狄利克雷边界条件下的离散拉普拉斯算子相关的稀疏、对称正定矩阵。设矩阵分裂为 $\\mathbf{A}=\\mathbf{D}-\\mathbf{L}-\\mathbf{U}$，其中 $\\mathbf{D}$ 是 $\\mathbf{A}$ 的对角部分，$\\mathbf{L}$ 是严格下三角部分，$\\mathbf{U}$ 是严格上三角部分。\n\n雅可比方法通过以下方式更新\n$$\n\\mathbf{u}^{(k+1)}=\\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U})\\mathbf{u}^{(k)}+\\mathbf{D}^{-1}\\mathbf{b},\n$$\n迭代矩阵为\n$$\n\\mathbf{T}_J = \\mathbf{D}^{-1}(\\mathbf{L}+\\mathbf{U}) = \\mathbf{I} - \\mathbf{D}^{-1}\\mathbf{A}.\n$$\n高斯-赛德尔（GS）方法通过以下方式更新\n$$\n\\mathbf{u}^{(k+1)}=(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{U}\\,\\mathbf{u}^{(k)}+(\\mathbf{D}-\\mathbf{L})^{-1}\\mathbf{b}.\n$$\n逐次超松弛（SOR）方法通过以下方式更新\n$$\n\\mathbf{u}^{(k+1)}=(\\mathbf{D}-\\omega \\mathbf{L})^{-1}\\left[(1-\\omega)\\mathbf{D}+\\omega \\mathbf{U}\\right]\\mathbf{u}^{(k)}+(\\mathbf{D}-\\omega \\mathbf{L})^{-1}\\,\\omega \\mathbf{b},\n$$\n松弛因子为 $\\omega \\in (0,2)$。\n\n我们现在分析矩形网格上五点离散拉普拉斯算子的雅可比迭代的特征结构。假设单位间距并将狄利克雷边界条件并入 $\\mathbf{b}$，内部点 $(i,j)$ 上的离散拉普拉斯方程为\n$$\n- u_{i-1,j}-u_{i+1,j}-u_{i,j-1}-u_{i,j+1}+4u_{i,j}=0.\n$$\n雅可比迭代通过对其四个邻居点的值进行平均来更新 $u_{i,j}$，并且齐次迭代矩阵 $\\mathbf{T}_J$ 是可分离的。使用分离变量法和与狄利克雷边界条件一致的离散正弦基，$\\mathbf{T}_J$ 的特征向量的分量为\n$$\nv_{i,j}^{(k,\\ell)} = \\sin\\!\\left(\\frac{\\pi k i}{N+1}\\right)\\sin\\!\\left(\\frac{\\pi \\ell j}{M+1}\\right), \\quad k\\in\\{1,\\dots,N\\},\\ \\ell\\in\\{1,\\dots,M\\}.\n$$\n对于这些模态，$\\mathbf{T}_J$ 对应的特征值为\n$$\n\\lambda_{k,\\ell} = \\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi k}{N+1}\\right) + \\cos\\!\\left(\\frac{\\pi \\ell}{M+1}\\right)\\right].\n$$\n该公式是通过将分离的本征模代入雅可比更新公式，并使用离散二阶差分的三角恒等式得到的。雅可比迭代的谱半径，记为 $\\rho_J$，是 $\\lambda_{k,\\ell}$ 的最大绝对值：\n$$\n\\rho_J = \\max_{1\\le k\\le N,\\ 1\\le \\ell\\le M} \\left|\\lambda_{k,\\ell}\\right|.\n$$\n因为 $\\cos(x)$ 在 $x\\in[0,\\pi]$ 上是递减的，并且在此处所选的排序下所有 $\\lambda_{k,\\ell}$ 都是非负的，所以在 $k=1$ 和 $\\ell=1$ 时达到最大值，得到\n$$\n\\rho_J = \\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)+\\cos\\!\\left(\\frac{\\pi}{M+1}\\right)\\right].\n$$\n\n为了将最优SOR松弛因子与 $\\rho_J$ 联系起来，我们回顾一个在分析矩形网格上五点拉普拉斯型对称正定系统的定常迭代中的经典结果（Young的超松弛分析）：使SOR迭代矩阵谱半径最小化的最优SOR松弛因子，可以用雅可比谱半径来表示。具体来说，\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1+\\sqrt{1-\\rho_J^2}}.\n$$\n该关系可以通过考虑将切比雪夫（Chebyshev）半迭代多项式加速应用于GS分裂来证明，并利用SOR对应于加权GS步长这一事实；最优权重来自于在包含GS特征值的区间上最小化谱半径，对于五点拉普拉斯算子，该谱半径可以用雅可比谱半径 $\\rho_J$ 表示。对于特殊的正方形情况 $N=M$，有 $\\rho_J=\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)$，因此\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{1+\\sin\\!\\left(\\frac{\\pi}{N+1}\\right)},\n$$\n因为当 $\\theta=\\frac{\\pi}{N+1}$ 时，有 $\\sqrt{1-\\cos^2(\\theta)}=\\sin(\\theta)$。对于 $N\\neq M$ 的矩形网格，这种依赖关系通过上面导出的 $\\rho_J$ 保持不变：\n$$\n\\rho_J = \\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)+\\cos\\!\\left(\\frac{\\pi}{M+1}\\right)\\right], \\quad\n\\omega_{\\mathrm{opt}} = \\frac{2}{1+\\sqrt{1-\\rho_J^2}}.\n$$\n\n算法设计：\n- 输入：内部网格尺寸对 $(N,M)$。\n- 步骤1：使用\n$$\n\\rho_J(N,M)=\\frac{1}{2}\\left[\\cos\\!\\left(\\frac{\\pi}{N+1}\\right)+\\cos\\!\\left(\\frac{\\pi}{M+1}\\right)\\right],\n$$\n计算 $\\rho_J$，其中所有角度均以弧度为单位。\n- 步骤2：计算\n$$\n\\omega_{\\mathrm{opt}}(N,M)=\\frac{2}{1+\\sqrt{1-\\rho_J(N,M)^2}}.\n$$\n为保证数值稳定性，先计算 $1-\\rho_J^2$，并且为了抵消大 $N$ 或 $M$ 时的舍入误差，在开方前将非常接近零的负值钳位到零。\n- 步骤3：将每个 $\\omega_{\\mathrm{opt}}$ 精确到6位小数，并以指定的单行格式输出列表。\n\n定性行为：\n- 对于大的 $N$ 和 $M$ 的方形网格，$\\frac{\\pi}{N+1}$ 和 $\\frac{\\pi}{M+1}$ 很小，$\\rho_J$ 趋近于1，$\\sqrt{1-\\rho_J^2}$ 变小，而 $\\omega_{\\mathrm{opt}}$ 从下方趋近于2。\n- 对于高度各向异性的网格（长而窄的通道），$\\frac{\\pi}{N+1}$ 或 $\\frac{\\pi}{M+1}$ 中有一个相对较大，这会减小 $\\rho_J$ 并增大 $\\sqrt{1-\\rho_J^2}$，从而减小 $\\omega_{\\mathrm{opt}}$。因此，各向异性使得 $\\omega_{\\mathrm{opt}}$ 偏离2。\n\n将算法应用于测试套件：\n- $(N,M)=(\\,32,\\,32\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.827963$。\n- $(N,M)=(\\,64,\\,8\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.605816$。\n- $(N,M)=(\\,8,\\,64\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.605816$。\n- $(N,M)=(\\,3,\\,3\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.171573$。\n- $(N,M)=(\\,3,\\,50\\,)$ 得到 $\\omega_{\\mathrm{opt}}\\approx 1.313552$。\n\n这些值是由程序使用弧度角并四舍五入到6位小数产生的，它们说明了随着网格各向异性增强，$\\omega_{\\mathrm{opt}}$ 是如何减小的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef omega_opt(N: int, M: int) - float:\n    \"\"\"\n    Compute the optimal SOR relaxation parameter for the 2D five-point Laplacian\n    on an N x M interior grid (Dirichlet BCs), using the classical relation\n    with the Jacobi spectral radius.\n    \"\"\"\n    # Angles in radians\n    theta_x = np.pi / (N + 1)\n    theta_y = np.pi / (M + 1)\n    rho_J = 0.5 * (np.cos(theta_x) + np.cos(theta_y))\n    # Numerical guard for round-off: ensure the argument of sqrt is non-negative\n    arg = 1.0 - rho_J * rho_J\n    if arg  0.0 and arg  -1e-16:\n        arg = 0.0\n    # Compute omega_opt\n    denom = 1.0 + np.sqrt(arg)\n    # Avoid division by zero (should not occur for valid N, M = 1)\n    if denom == 0.0:\n        return 2.0\n    return 2.0 / denom\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple is (N, M), numbers of interior points in x and y directions.\n    test_cases = [\n        (32, 32),\n        (64, 8),\n        (8, 64),\n        (3, 3),\n        (3, 50),\n    ]\n\n    results = []\n    for N, M in test_cases:\n        w = omega_opt(N, M)\n        # Round to exactly 6 decimal places\n        results.append(f\"{w:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2444079"}, {"introduction": "优化算法的收敛速度固然重要，但确保其正确收敛到真实的解则更为关键。在实践中，一个常见的陷阱是所谓的“假性收敛”，即迭代更新量已经变得微不足道，但解的误差（由残差衡量）仍然很大。本练习 [@problem_id:2397044] 将通过一个具体的编程实例，让你亲手揭示这一现象，并学会如何通过监控残差来审慎地评估迭代过程的收敛性。", "problem": "考虑方形网格上的二维拉普拉斯方程及狄利克雷边界条件。连续问题由在具有指定边界值的域中的 $-\\nabla^2 \\phi(\\mathbf{x}) = 0$ 给出。使用标准五点有限差分格式，在尺寸为 $N \\times N$、单位网格间距 $h = 1$ 的均匀方形网格上离散化该方程。令内部未知数通过加权高斯-赛德尔扫描进行更新（当松弛参数不为 $1$ 时，也称为逐次超松弛法（SOR）），其逐点更新公式定义为：\n$$\n\\phi_{i,j}^{\\text{new}} = \\phi_{i,j}^{\\text{old}} + \\omega \\left(\\frac{1}{4}\\left(\\phi_{i+1,j}^{\\text{current}} + \\phi_{i-1,j}^{\\text{new}} + \\phi_{i,j+1}^{\\text{current}} + \\phi_{i,j-1}^{\\text{new}}\\right) - \\phi_{i,j}^{\\text{old}} \\right),\n$$\n其中 $\\omega \\in [0,2)$ 是松弛参数，该表示法强调了如高斯-赛德尔方法中那样，按字典序使用新更新的邻居点的值。假设右侧项为齐次的 $f_{i,j} = 0$（拉普拉斯方程），因此内部点的离散方程可重排为：\n$$\n\\phi_{i,j} = \\frac{1}{4}\\left(\\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1}\\right).\n$$\n在方形区域上施加狄利克雷边界条件：\n- 左边界 $x=0$：$\\phi=1$，\n- 右边界 $x=1$：$\\phi=0$，\n- 上边界 $y=1$：$\\phi=0$，\n- 下边界 $y=0$：$\\phi=0$。\n将所有内部值初始化为 $\\phi=0$。设网格间距在任意单位下为 $h=1$，并令内部点的离散残差为：\n$$\nr_{i,j} = \\left(\\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} - 4\\phi_{i,j}\\right),\n$$\n该残差与代数残差 $(A\\phi-b)$ 成正比，比例常数为 $1/h^2$。将一次扫描中更新量的无穷范数定义为：\n$$\n\\|\\Delta \\phi\\|_{\\infty} = \\max_{i,j} \\left| \\phi_{i,j}^{\\text{new}} - \\phi_{i,j}^{\\text{old}} \\right|,\n$$\n并将残差的无穷范数定义为：\n$$\n\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|.\n$$\n你的任务是编写一个完整、可运行的程序，该程序：\n- 对于每个测试用例，在上述问题设置上，使用松弛参数 $\\omega$ 执行恰好 $K$ 次高斯-赛德尔扫描。\n- 在最后一次扫描之后，使用最后一次扫描的更新量计算 $\\|\\Delta \\phi\\|_{\\infty}$，并在得到的结果场上计算 $\\|r\\|_{\\infty}$。\n- 对于每个测试用例，输出一个布尔值，指示是否发生了“伪收敛”，此处定义为：\n$$\n\\|\\Delta \\phi\\|_{\\infty} \\le \\tau_u \\quad \\text{and} \\quad \\|r\\|_{\\infty} \\ge \\tau_r.\n$$\n\n使用以下参数值测试套件，该套件旨在探查典型行为和边界情况行为：\n- 案例 1（正常收敛路径）：$(N,K,\\omega,\\tau_u,\\tau_r) = (33,8000,1.0,10^{-6},10^{-6})$。\n- 案例 2（极端欠松弛边界情况）：$(N,K,\\omega,\\tau_u,\\tau_r) = (33,1,0.0,10^{-12},10^{-1})$。\n- 案例 3（轻度欠松弛，说明一次扫描后更新量小而残差仍然很大）：$(N,K,\\omega,\\tau_u,\\tau_r) = (33,1,0.1,3\\times 10^{-2},5\\times 10^{-1})$。\n\n所有角度（如果存在）都应以弧度为单位，但此处未使用角度。除了为网格间距 $h=1$ 指定的任意单位外，无需进行其他物理单位转换。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。例如，输出应如下所示：\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3]\n$$\n其中每个 $\\text{result}_i$ 对于按上述顺序排列的相应测试用例，其值为 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题要求分析使用逐次超松弛（SOR）方法求解方形网格上二维拉普拉斯方程的收敛行为。这是计算物理学和数值分析中的一个基本问题。在尝试任何解决方案之前，必须先验证问题陈述的有效性。\n\n该问题有科学依据、是适定的且客观的。它描述了在单位间距 $h=1$ 的 $N \\times N$ 网格上，使用标准五点有限差分格式对拉普拉斯方程 $-\\nabla^2 \\phi(\\mathbf{x}) = 0$ 进行离散化。这会得到一个关于内部网格点位势 $\\phi_{i,j}$ 的线性方程组。每个内部点 $(i,j)$ 的离散方程为：\n$$\n\\phi_{i,j} = \\frac{1}{4} \\left( \\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} \\right).\n$$\n该方程组将使用迭代法求解，具体是 SOR 方法，其中当松弛参数 $\\omega=1$ 时，高斯-赛德尔方法是其一个特例。更新规则如下：\n$$\n\\phi_{i,j}^{\\text{new}} = \\phi_{i,j}^{\\text{old}} + \\omega \\left(\\frac{1}{4}\\left(\\phi_{i+1,j}^{\\text{current}} + \\phi_{i-1,j}^{\\text{new}} + \\phi_{i,j+1}^{\\text{current}} + \\phi_{i,j-1}^{\\text{new}}\\right) - \\phi_{i,j}^{\\text{old}} \\right).\n$$\n这可以重写为：\n$$\n\\phi_{i,j}^{\\text{new}} = (1-\\omega)\\phi_{i,j}^{\\text{old}} + \\frac{\\omega}{4}\\left(\\phi_{i+1,j}^{\\text{current}} + \\phi_{i-1,j}^{\\text{new}} + \\phi_{i,j+1}^{\\text{current}} + \\phi_{i,j-1}^{\\text{new}}\\right).\n$$\n上标表示来自上一次迭代的值（'old', 'current'）和在当前迭代中已更新的值（'new'），假设对网格点进行字典序扫描。这对应于位势数组 $\\phi$ 的原地更新。所有定义，包括狄利克雷边界条件（左边界 $\\phi=1$，其他边界 $\\phi=0$）、初始条件（所有内部点 $\\phi=0$）以及迭代次数 $K$，都已明确指定。\n\n任务是针对三个不同的测试用例，评估一个被称为“伪收敛”的特定条件。该条件定义为：当更新量的无穷范数 $\\|\\Delta \\phi\\|_{\\infty}$ 很小，而残差的无穷范数 $\\|r\\|_{\\infty}$ 仍然很大时发生。具体定义如下：\n- 更新范数：$\\|\\Delta \\phi\\|_{\\infty} = \\max_{i,j} \\left| \\phi_{i,j}^{\\text{new}} - \\phi_{i,j}^{\\text{old}} \\right|$，在最后一次（第 $K$ 次）扫描期间计算。\n- 残差范数：$\\|r\\|_{\\infty} = \\max_{i,j} |r_{i,j}|$，其中 $r_{i,j} = \\phi_{i+1,j} + \\phi_{i-1,j} + \\phi_{i,j+1} + \\phi_{i,j-1} - 4\\phi_{i,j}$，在最后一次扫描后计算。\n\n该问题是有效的，我们继续进行求解。解决方案的核心是一个实现 SOR 算法并计算所需度量指标的程序。\n\n对于每个测试用例 $(N, K, \\omega, \\tau_u, \\tau_r)$，算法按以下步骤进行：\n\n1.  **初始化**：创建一个表示 $\\phi$ 的 $N \\times N$ 数组，并将其初始化为零。然后施加狄利克雷边界条件：将对应于左边界（$j=0$）的列设置为 $1.0$，而其他边界保持为 $0.0$。\n\n2.  **迭代**：程序执行恰好 $K$ 次 SOR 扫描。一次扫描包括按字典序遍历所有内部网格点 $(i,j)$（其中 $i,j \\in [1, N-2]$）。在每个点，根据 SOR 公式原地更新 $\\phi_{i,j}$ 的值。在最后一次（第 $K$ 次）扫描期间，跟踪新旧值之间的最大绝对差 $|\\phi_{i,j}^{\\text{new}} - \\phi_{i,j}^{\\text{old}}|$，以确定 $\\|\\Delta \\phi\\|_{\\infty}$。\n\n3.  **残差计算**：在 $K$ 次扫描之后，使用最终的位势场 $\\phi$ 计算所有内部点的残差 $r_{i,j}$。然后求出该残差场的无穷范数 $\\|r\\|_{\\infty}$。这可以使用数组切片高效地计算。\n\n4.  **条件评估**：最后，评估“伪收敛”的布尔条件：\n    $$\n    \\|\\Delta \\phi\\|_{\\infty} \\le \\tau_u \\quad \\text{and} \\quad \\|r\\|_{\\infty} \\ge \\tau_r.\n    $$\n    得到的布尔值（`True` 或 `False`）即为该测试用例的输出。\n\n选择这些测试用例是为了探索不同的行为：\n- **案例 1**：$(N, K, \\omega, \\tau_u, \\tau_r) = (33, 8000, 1.0, 10^{-6}, 10^{-6})$。这代表一个标准的、经过多次迭代的高斯-赛德尔运行。我们预计该方法会良好收敛，这意味着 $\\|\\Delta \\phi\\|_{\\infty}$ 和 $\\|r\\|_{\\infty}$ 都会很小。该条件应评估为 `False`。\n- **案例 2**：$(N, K, \\omega, \\tau_u, \\tau_r) = (33, 1, 0.0, 10^{-12}, 10^{-1})$。当 $\\omega=0$ 时，不发生更新，因此 $\\phi$ 从其初始状态保持不变。因此 $\\|\\Delta \\phi\\|_{\\infty} = 0$。然而，初始状态远离真实解，因此残差，尤其是在 $\\phi=1$ 边界附近，将会很大（量级为 $1$）。这是一个“伪收敛”的平凡案例，其中更新量为零但解是错误的。该条件应评估为 `True`。\n- **案例 3**：$(N, K, \\omega, \\tau_u, \\tau_r) = (33, 1, 0.1, 3 \\times 10^{-2}, 5 \\times 10^{-1})$。一次强欠松弛（$\\omega=0.1$）的扫描。更新量的大小与 $\\omega$ 成正比，导致 $\\|\\Delta \\phi\\|_{\\infty}$ 很小。然而，一个小的更新步骤不足以显著减小巨大的初始残差。因此，该案例也应触发“伪收敛”条件，评估为 `True`。\n\n这种系统化的方法，直接实现所提供的数学公式，将产生所需的结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case_simulation(N, K, omega, tau_u, tau_r):\n    \"\"\"\n    Solves the Laplace equation on a square grid using SOR for one test case.\n\n    Args:\n        N (int): The grid size (N x N).\n        K (int): The number of sweeps to perform.\n        omega (float): The relaxation parameter.\n        tau_u (float): The threshold for the update norm.\n        tau_r (float): The threshold for the residual norm.\n\n    Returns:\n        bool: True if false convergence occurred, False otherwise.\n    \"\"\"\n    # 1. Initialize the grid and boundary conditions\n    phi = np.zeros((N, N), dtype=np.float64)\n    # Dirichlet boundary conditions:\n    # phi=0 on top, bottom, right boundaries is default from np.zeros.\n    # phi=1 on left boundary.\n    phi[:, 0] = 1.0\n\n    # 2. Perform K sweeps\n    max_update = 0.0\n    \n    if K == 0:\n        # No sweeps, so no update.\n        delta_phi_inf = 0.0\n    else:\n        for k in range(K):\n            is_final_sweep = (k == K - 1)\n            if is_final_sweep:\n                max_update = 0.0\n\n            for i in range(1, N - 1):\n                for j in range(1, N - 1):\n                    old_val = phi[i, j]\n                    \n                    # Five-point stencil average\n                    term = 0.25 * (phi[i+1, j] + phi[i-1, j] + phi[i, j+1] + phi[i, j-1])\n                    \n                    # SOR update formula\n                    new_val = (1.0 - omega) * old_val + omega * term\n                    phi[i, j] = new_val\n\n                    if is_final_sweep:\n                        update = abs(new_val - old_val)\n                        if update > max_update:\n                            max_update = update\n        delta_phi_inf = max_update\n\n    # 3. Calculate the infinity norm of the residual after all sweeps\n    if N > 2:\n        # Calculate residual r_ij = (phi_{i+1,j} + ...) - 4*phi_{i,j} for interior points\n        residual = (phi[2:, 1:-1] + phi[:-2, 1:-1] +\n                    phi[1:-1, 2:] + phi[1:-1, :-2] -\n                    4.0 * phi[1:-1, 1:-1])\n        r_inf = np.max(np.abs(residual))\n    else:\n        # No interior points, residual is trivially zero.\n        r_inf = 0.0\n\n    # 4. Check for \"false convergence\"\n    is_false_converged = (delta_phi_inf = tau_u) and (r_inf >= tau_r)\n    return is_false_converged\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path convergence)\n        (33, 8000, 1.0, 10**-6, 10**-6),\n        # Case 2 (extreme under-relaxation edge case)\n        (33, 1, 0.0, 10**-12, 10**-1),\n        # Case 3 (mild under-relaxation with small updates but large residual)\n        (33, 1, 0.1, 3 * 10**-2, 5 * 10**-1),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, K, omega, tau_u, tau_r = case\n        result = run_case_simulation(N, K, omega, tau_u, tau_r)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) converts boolean True/False to \"True\"/\"False\" which is the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2397044"}]}