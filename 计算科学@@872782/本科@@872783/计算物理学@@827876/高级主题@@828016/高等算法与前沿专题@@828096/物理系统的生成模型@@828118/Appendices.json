{"hands_on_practices": [{"introduction": "本次实践将从一个基础概念开始：表示物理系统的平衡分布。我们将构建一个简单的生成模型——归一化流，来精确地再现一个谐振子系统的玻尔兹曼分布，从而清晰、可解析地介绍如何映射概率分布。这个练习旨在展示生成模型如何作为强大工具，用于从统计力学中遇到的复杂概率分布中进行采样 [@problem_id:2398415]。", "problem": "实现一个完整、可运行的程序，构建一个归一化流，将一个简单的高斯基分布转换为一个一维双粒子谐波相互作用系统的玻尔兹曼分布。系统使用约化单位，其中玻尔兹曼常数等于1。考虑一维空间中的两个粒子，其位置分别为 $x_1$ 和 $x_2$。其势能由下式给出\n$$\nU(x_1,x_2) = \\frac{1}{2} k \\, (x_1 - x_2)^2 + \\frac{1}{2} k_0 \\, (x_1^2 + x_2^2),\n$$\n其中 $k$ 和 $k_0$ 是刚度常数。在温度 $T$ 下，状态向量 $x = (x_1,x_2)^\\top \\in \\mathbb{R}^2$ 上的玻尔兹曼分布为\n$$\np(x) = \\frac{1}{Z} \\exp\\!\\left(-\\frac{U(x)}{T}\\right),\n$$\n其中 $Z$ 是配分函数，所有量均采用约化无量纲单位，玻尔兹曼常数等于 $1$。基分布是标准正态分布\n$$\nq_0(z) = \\mathcal{N}(0, I_2)\n$$\n对于 $z \\in \\mathbb{R}^2$。实现一个归一化流，它是一个双射、可微的映射 $f_\\theta : \\mathbb{R}^2 \\to \\mathbb{R}^2$，将 $z \\sim q_0$ 转换为 $x = f_\\theta(z)$，其模型密度为\n$$\nq_\\theta(x) = q_0\\!\\big(f_\\theta^{-1}(x)\\big)\\,\\left|\\det J_{f_\\theta^{-1}}(x)\\right|,\n$$\n其中 $J_{f_\\theta^{-1}}(x)$ 是逆映射的雅可比矩阵。将流约束为线性和可逆的，$x = L z + b$，其中 $L \\in \\mathbb{R}^{2\\times 2}$ 是可逆矩阵，$b \\in \\mathbb{R}^2$。\n\n从第一性原理出发，将指定势能的玻尔兹曼分布表示为具有显式均值和协方差的多元正态分布，并选择 $L$ 和 $b$ 使得前推分布 $q_\\theta$ 精确等于玻尔兹曼分布。然后，对于下面的每个测试用例，以闭合形式计算库尔贝克-莱布勒散度，\n$$\nD_{\\mathrm{KL}}\\!\\big(q_\\theta \\,\\|\\, p\\big) = \\frac{1}{2}\\left(\\operatorname{tr}\\!\\big(\\Sigma_p^{-1} \\Sigma_\\theta\\big) + (\\mu_p - \\mu_\\theta)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_\\theta) - d + \\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_\\theta}\\right),\n$$\n其中 $d = 2$，$\\mu_\\theta$、$\\Sigma_\\theta$ 分别是 $q_\\theta$ 的均值和协方差，而 $\\mu_p$、$\\Sigma_p$ 分别是 $p$ 的均值和协方差。为每个测试用例报告一个实数，即计算出的 $D_{\\mathrm{KL}}$ 的十进制值。\n\n测试套件（每个元组为 $(k, k_0, T)$）：\n- 用例1：$(1.0, 1.0, 1.0)$\n- 用例2：$(0.0, 1.0, 0.5)$\n- 用例3：$(0.5, 5.0, 2.0)$\n- 用例4：$(5.0, 0.1, 1.5)$\n\n最终输出格式：你的程序应生成一行输出，其中包含一个由方括号括起来的、逗号分隔的四个 $D_{\\mathrm{KL}}$ 值列表。例如，包含四个值的输出应如下所示\n$$\n[\\alpha_1,\\alpha_2,\\alpha_3,\\alpha_4],\n$$\n其中每个 $\\alpha_i$ 是一个十进制数。不允许有其他输出或文本。", "solution": "该问题要求构建一个线性归一化流，将一个标准的二元高斯基分布转换为一个双粒子系统的目标玻尔兹曼分布。首先必须确定问题陈述的有效性。\n\n**问题验证**\n\n**步骤1：提取已知条件**\n- 势能：$U(x_1,x_2) = \\frac{1}{2} k \\, (x_1 - x_2)^2 + \\frac{1}{2} k_0 \\, (x_1^2 + x_2^2)$\n- 状态向量：$x = (x_1, x_2)^\\top \\in \\mathbb{R}^2$\n- 玻尔兹曼分布（目标）：$p(x) = \\frac{1}{Z} \\exp(-\\frac{U(x)}{T})$\n- 基分布：$q_0(z) = \\mathcal{N}(0, I_2)$\n- 归一化流映射：$x = f_\\theta(z) = L z + b$，其中 $L \\in \\mathbb{R}^{2\\times 2}$ 是可逆矩阵，$b \\in \\mathbb{R}^2$。\n- 模型分布：$q_\\theta(x) = q_0(f_\\theta^{-1}(x))\\,|\\det J_{f_\\theta^{-1}}(x)|$\n- 约束：选择 $L$ 和 $b$ 使得 $q_\\theta(x) = p(x)$ 精确成立。\n- 库尔贝克-莱布勒散度公式：$D_{\\mathrm{KL}}(q_\\theta \\,\\|\\, p) = \\frac{1}{2}\\left(\\operatorname{tr}(\\Sigma_p^{-1} \\Sigma_\\theta) + (\\mu_p - \\mu_\\theta)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_\\theta) - d + \\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_\\theta}\\right)$，其中 $d = 2$。\n- 测试用例 $(k, k_0, T)$：$(1.0, 1.0, 1.0)$、$(0.0, 1.0, 0.5)$、$(0.5, 5.0, 2.0)$、$(5.0, 0.1, 1.5)$。\n\n**步骤2：使用提取的已知条件进行验证**\n- **科学基础：** 该问题牢固地植根于统计力学（玻尔兹曼分布）、线性代数和概率论（多元正态分布、概率密度变量替换公式、库尔贝克-莱布勒散度）。应用归一化流来模拟物理分布是计算物理学中的一种标准技术。所有给定的公式都是正确和标准的。\n- **适定性：** 该问题是适定的。它要求证明目标分布是一个多元高斯分布，然后找到一个线性变换的参数，该变换将标准高斯分布映射到此目标分布。这是一个可解问题，其结果分布的参数有唯一解。计算KL散度的要求也定义明确。\n- **客观性：** 该问题使用精确、客观的数学语言陈述，没有主观或含糊的术语。\n\n**步骤3：结论与行动**\n该问题是有效的。它在科学上是合理的、适定的和客观的。将提供一个完整的解决方案。\n\n**基于原理的解决方案**\n\n目标是构建一个线性归一化流 $x = Lz + b$，它将基分布 $q_0(z) = \\mathcal{N}(z | 0, I_2)$ 转换为一个模型分布 $q_\\theta(x)$，该模型分布与目标玻尔兹曼分布 $p(x)$ 完全相同。我们将首先将 $p(x)$ 和 $q_\\theta(x)$ 表征为多元正态分布，然后匹配它们的参数。\n\n**1. 目标分布 $p(x)$ 的表征**\n\n势能 $U(x)$ 是坐标 $x_1$ 和 $x_2$ 的二次型。我们将其表示为矩阵形式。\n$$\nU(x_1,x_2) = \\frac{1}{2} k (x_1^2 - 2x_1x_2 + x_2^2) + \\frac{1}{2} k_0 (x_1^2 + x_2^2) = \\frac{1}{2} (k+k_0)x_1^2 - k x_1 x_2 + \\frac{1}{2} (k+k_0)x_2^2\n$$\n这可以写成 $U(x) = \\frac{1}{2} x^\\top H x$，其中 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ 且 $H$ 是势能的海森矩阵，由下式给出：\n$$\nH = \\begin{pmatrix} k+k_0 & -k \\\\ -k & k+k_0 \\end{pmatrix}\n$$\n目标玻尔兹曼分布为 $p(x) = \\frac{1}{Z} \\exp(-\\frac{U(x)}{T}) = \\frac{1}{Z} \\exp(-\\frac{1}{2T} x^\\top H x)$。这是一个多元正态分布 $\\mathcal{N}(\\mu_p, \\Sigma_p)$ 的概率密度函数，其通用形式为：\n$$\np(x) = \\frac{1}{\\sqrt{(2\\pi)^d \\det \\Sigma_p}} \\exp\\left(-\\frac{1}{2} (x - \\mu_p)^\\top \\Sigma_p^{-1} (x - \\mu_p)\\right)\n$$\n通过比较指数部分，我们可以确定均值 $\\mu_p$ 和逆协方差（精度）矩阵 $\\Sigma_p^{-1}$。指数中缺少关于 $x$ 的线性项意味着均值向量为零：\n$$\n\\mu_p = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n精度矩阵被确定为：\n$$\n\\Sigma_p^{-1} = \\frac{1}{T} H = \\frac{1}{T} \\begin{pmatrix} k+k_0 & -k \\\\ -k & k+k_0 \\end{pmatrix}\n$$\n协方差矩阵是精度矩阵的逆，$\\Sigma_p = ( \\Sigma_p^{-1} )^{-1} = T H^{-1}$。对于给定的测试用例，$H$ 是可逆的。因此，目标分布 $p(x)$ 是一个零均值的多元正态分布，$p(x) = \\mathcal{N}(x | 0, \\Sigma_p)$。\n\n**2. 模型分布 $q_\\theta(x)$ 的表征**\n\n基随机变量 $z$ 服从标准正态分布，$z \\sim \\mathcal{N}(0, I_2)$。归一化流定义了一个新的随机变量 $x = Lz + b$。这是一个高斯变量的仿射变换，因此 $x$ 也服从高斯分布。\n模型分布 $q_\\theta(x)$ 的均值为：\n$$\n\\mu_\\theta = \\mathbb{E}[x] = \\mathbb{E}[Lz + b] = L\\mathbb{E}[z] + b = L \\cdot 0 + b = b\n$$\n模型分布的协方差为：\n$$\n\\Sigma_\\theta = \\text{Cov}(x) = \\text{Cov}(Lz + b) = L \\, \\text{Cov}(z) \\, L^\\top = L I_2 L^\\top = LL^\\top\n$$\n因此，模型分布为 $q_\\theta(x) = \\mathcal{N}(x | b, LL^\\top)$。\n\n**3. 匹配分布并计算KL散度**\n\n问题要求我们选择流参数 $L$ 和 $b$ 使得 $q_\\theta(x)$ 与 $p(x)$ 完全相同。这需要匹配它们各自的均值和协方差矩阵：\n$$\n\\mu_\\theta = \\mu_p \\implies b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\Sigma_\\theta = \\Sigma_p \\implies LL^\\top = \\Sigma_p\n$$\n由于对于给定的参数（因为 $T>0$ 且 $H$ 是正定的），$\\Sigma_p = T H^{-1}$ 是一个对称正定矩阵，所以总能找到一个合适的矩阵 $L$，例如，通过 Cholesky 分解。\n\n通过选择这些参数，模型分布与目标分布完全相同：$q_\\theta(x) = p(x)$。库尔贝克-莱布勒散度衡量两个分布之间的不相似性。对于任何分布 $P$，其与自身的KL散度为零：$D_{\\mathrm{KL}}(P \\,\\|\\, P) = \\int P(x) \\log\\frac{P(x)}{P(x)} dx = \\int P(x) \\log(1) dx = 0$。\n\n我们可以使用为两个多元正态分布 $q_\\theta = \\mathcal{N}(\\mu_\\theta, \\Sigma_\\theta)$ 和 $p = \\mathcal{N}(\\mu_p, \\Sigma_p)$ 提供的公式来验证这一点：\n$$\nD_{\\mathrm{KL}}(q_\\theta \\,\\|\\, p) = \\frac{1}{2}\\left(\\operatorname{tr}(\\Sigma_p^{-1} \\Sigma_\\theta) + (\\mu_p - \\mu_\\theta)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_\\theta) - d + \\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_\\theta}\\right)\n$$\n代入我们匹配的参数 $\\mu_\\theta = \\mu_p$ 和 $\\Sigma_\\theta = \\Sigma_p$：\n- 迹项变为 $\\operatorname{tr}(\\Sigma_p^{-1} \\Sigma_p) = \\operatorname{tr}(I_d) = d$。\n- 均值差项变为 $(\\mu_p - \\mu_p)^\\top \\Sigma_p^{-1} (\\mu_p - \\mu_p) = 0$。\n- 对数行列式项变为 $\\ln\\frac{\\det \\Sigma_p}{\\det \\Sigma_p} = \\ln(1) = 0$。\n\n将这些代入 $d=2$ 的公式中：\n$$\nD_{\\mathrm{KL}}(q_\\theta \\,\\|\\, p) = \\frac{1}{2}(d + 0 - d + 0) = \\frac{1}{2}(2 - 2) = 0\n$$\n这个结果与 $k$、$k_0$ 和 $T$ 的具体值无关，只要它们定义了一个有效的（非退化的）高斯分布。由于问题明确指出要构建流使得分布完全匹配，因此所有测试用例的KL散度必须为 $0$。实现将进行数值计算以确认这一分析结论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the KL divergence for each test case.\n\n    The analytical derivation shows that if the normalizing flow parameters are\n    chosen to perfectly match the target Boltzmann distribution, the KL divergence\n    must be exactly zero. This program confirms this result numerically for\n    the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 1.0, 1.0),  # Case 1: (k, k_0, T)\n        (0.0, 1.0, 0.5),  # Case 2\n        (0.5, 5.0, 2.0),  # Case 3\n        (5.0, 0.1, 1.5),  # Case 4\n    ]\n\n    results = []\n    d = 2  # Dimensionality of the system\n\n    for case in test_cases:\n        k, k0, T = case\n\n        # 1. Define the parameters of the target Boltzmann distribution p(x).\n        # The potential is U(x) = 1/2 * x^T H x.\n        # The distribution p(x) is a multivariate normal N(mu_p, Sigma_p).\n        \n        # The mean is zero due to the form of the potential.\n        mu_p = np.array([0.0, 0.0])\n\n        # The inverse covariance matrix is Sigma_p^-1 = (1/T) * H, where H is the Hessian.\n        H = np.array([[k + k0, -k], \n                      [-k, k + k0]])\n        \n        # Check for non-invertible Hessian (though not expected for test cases)\n        if np.linalg.det(H) == 0:\n            # This would indicate a non-physical or degenerate system.\n            # For the given cases, det(H) = k0 * (2k + k0), which is > 0.\n            # Handle as an error, though we expect it not to occur.\n            results.append(np.nan) \n            continue\n\n        Sigma_p_inv = (1.0 / T) * H\n        \n        # The covariance matrix is the inverse of the precision matrix.\n        Sigma_p = np.linalg.inv(Sigma_p_inv)\n\n        # 2. Define the parameters of the model distribution q_theta(x).\n        # The problem states to choose the linear flow x = Lz + b such that\n        # the model distribution q_theta(x) exactly equals the target p(x).\n        # This means we must match their means and covariances.\n        mu_theta = mu_p\n        Sigma_theta = Sigma_p\n\n        # 3. Compute the KL divergence using the provided formula.\n        # D_KL(q_theta || p) = 0.5 * ( tr(Sigma_p^-1 * Sigma_theta) + \n        #                              (mu_p - mu_theta)^T * Sigma_p^-1 * (mu_p - mu_theta) -\n        #                              d + log(det(Sigma_p) / det(Sigma_theta)) )\n\n        # Since mu_theta = mu_p and Sigma_theta = Sigma_p, the terms simplify:\n        # trace_term = tr(Sigma_p^-1 * Sigma_p) = tr(I) = d\n        # mean_term = (0)^T * ... * (0) = 0\n        # log_det_term = log(det(Sigma_p) / det(Sigma_p)) = log(1) = 0\n        # D_KL = 0.5 * (d + 0 - d + 0) = 0\n        \n        # We perform the full numerical computation to verify.\n        \n        trace_term = np.trace(Sigma_p_inv @ Sigma_theta)\n        \n        mu_diff = mu_p - mu_theta\n        mean_term = mu_diff.T @ Sigma_p_inv @ mu_diff\n        \n        det_p = np.linalg.det(Sigma_p)\n        det_theta = np.linalg.det(Sigma_theta)\n        \n        # Handle potential division by zero or log of non-positive, though not expected here.\n        if det_theta == 0 or det_p == 0:\n            log_det_term = np.nan\n        else:\n            log_det_term = np.log(det_p / det_theta)\n        \n        dkl = 0.5 * (trace_term + mean_term - d + log_det_term)\n        \n        # The result should be numerically very close to 0.0.\n        results.append(dkl)\n\n    # Final print statement in the exact required format.\n    # We format the numbers to represent them as standard decimals.\n    print(f\"[{','.join(map(lambda x: f'{x:.1f}', results))}]\")\n\nsolve()\n\n```", "id": "2398415"}, {"introduction": "从静态分布转向动态系统，本练习将探讨一个关键问题：一个纯数据驱动的模型能否隐式地学习到基本的物理守恒定律？你将使用 N 体模拟数据训练一个简单的线性模型，并测试它是否能保持角动量守恒——这是真实物理定律的一种对称性。这项实践突出了在动力学系统的机器学习模型中，融入物理原理的挑战和重要性 [@problem_id:2398389]。", "problem": "您的任务是评估一个仅在牛顿引力 $N$ 体动力学快照上训练的数据驱动生成模型，是否能学会保持总角动量矢量 $\\vec{L}$ 的守恒性。您必须编写一个完整的程序，该程序 (i) 使用辛积分器生成物理上真实的 $N$ 体训练数据，(ii) 从快照中学习一个线性时间推进生成模型，(iii) 在预留的初始条件下对学习到的模型进行演化推算，以及 (iv) 根据指定的容差测试生成的推算结果中 $\\vec{L}$ 的守恒性。\n\n请使用以下基本原理。牛顿第二定律指出，对于质量为 $m_i$ 的粒子 $i$，其运动方程为 $m_i \\,\\mathrm{d}^2\\vec{r}_i/\\mathrm{d}t^2=\\vec{F}_i$，其中 $\\vec{F}_i$ 是合力。对于引力常数为 $G$ 的牛顿引力，粒子 $j$ 对粒子 $i$ 的作用力为 $\\vec{F}_{ij}=-G\\,m_i m_j\\,(\\vec{r}_i-\\vec{r}_j)/\\lVert \\vec{r}_i-\\vec{r}_j\\rVert^3$。根据牛顿第三定律，$\\vec{F}_{ij}=-\\vec{F}_{ji}$，并且由于这些力是中心力，总内力矩为零，这意味着在没有外力矩的情况下 $\\mathrm{d}\\vec{L}/\\mathrm{d}t=\\vec{0}$。总角动量定义为 $\\vec{L}=\\sum_{i=1}^{N} m_i\\,\\vec{r}_i\\times\\vec{v}_i$，其中 $\\vec{v}_i=\\mathrm{d}\\vec{r}_i/\\mathrm{d}t$。\n\n您编写的程序必须实现以下任务。\n\n- 数据生成：对每个测试用例，使用速度-Verlet方法（一种辛积分器）模拟真实的引力 $N$ 体系统，并采用引力软化以避免奇异点。在无量纲单位下使用引力常数 $G=1$。在给定时间 $t$ 的质量 $\\{m_i\\}_{i=1}^N$、位置 $\\{\\vec{r}_i\\}_{i=1}^N$ 和速度 $\\{\\vec{v}_i\\}_{i=1}^N$ 的情况下，使用大于零的软化长度平方 $\\epsilon^2>0$ 计算具有软化成对相互作用的引力加速度 $\\{\\vec{a}_i\\}_{i=1}^N$。使用速度-Verlet方法将位置和速度推进一个时间步长 $\\Delta t$。在每个时间步 $t$，通过将所有位置和速度按固定顺序堆叠成一个单一矢量，来构建快照矢量 $\\vec{s}_t$。\n\n- 生成模型训练：通过在训练集上最小化均方误差 $\\sum_t \\lVert \\vec{s}_{t+1}-\\mathbf{A}\\vec{s}_t\\rVert^2$，从快照对中学习一个线性时间推进模型 $\\mathbf{A}$。不要在模型中引入任何外部物理约束；它必须只从数据中学习。\n\n- 演化推算生成：从每个测试用例的初始快照 $\\vec{s}_0$ 开始，通过 $\\vec{s}_{t+1}=\\mathbf{A}\\vec{s}_t$ 使用学习到的模型生成长度为 $T_{\\mathrm{eval}}$ 的演化推算。在每个生成的步骤中，计算生成状态的总角动量 $\\vec{L}(t)$。\n\n- 守恒性测试：定义初始角动量大小为 $L_0=\\lVert \\vec{L}(0)\\rVert_2$。对于给定的容差对 $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})$，如果在生成的演化推算中，满足 $L_0>\\tau_0$ 且 $\\max_{0\\le t\\le T_{\\mathrm{eval}}}\\lVert \\vec{L}(t)-\\vec{L}(0)\\rVert_2/L_0\\le\\varepsilon_{\\mathrm{rel}}$，或者满足 $L_0\\le\\tau_0$ 且 $\\max_{0\\le t\\le T_{\\mathrm{eval}}}\\lVert \\vec{L}(t)\\rVert_2\\le\\varepsilon_{\\mathrm{abs}}$，则声明角动量是守恒的。其中 $\\tau_0$ 是一个很小的阈值。所有比较都是无量纲的，因为它们使用的是 $\\vec{L}$ 的相对或绝对范数。\n\n实现上述步骤，并为以下每个测试用例（“测试套件”）评估布尔结果。在所有用例中，使用 $G=1$ 和带软化的速度-Verlet积分器。初始条件和参数如下；所有矢量都是三维的，第三个分量 $z$ 初始化为 $0$。\n\n- 用例 1（双体圆形轨道，等质量）：\n  - $N=2$，质量 $(m_1,m_2)=(1,1)$。\n  - 初始位置 $(\\vec{r}_1,\\vec{r}_2)=((-1/2,0,0),(+1/2,0,0))$。\n  - 对于间距 $a=1$、总质量 $M=2$ 的圆形轨道，角频率满足 $\\omega^2=GM/a^3=2$。使用初始速度 $(\\vec{v}_1,\\vec{v}_2)=((0,+\\tfrac{1}{2}\\sqrt{2},0),(0,-\\tfrac{1}{2}\\sqrt{2},0))$。\n  - 时间步长 $\\Delta t=10^{-2}$，软化长度平方 $\\epsilon^2=10^{-6}$。\n  - 训练步数 $T_{\\mathrm{train}}=400$，评估步数 $T_{\\mathrm{eval}}=200$。\n  - 容差 $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.15,10^{-3})$，其中 $\\tau_0=10^{-6}$。\n\n- 用例 2（双体迎面接近，零角动量）：\n  - $N=2$，质量 $(m_1,m_2)=(1,1)$。\n  - 初始位置 $(\\vec{r}_1,\\vec{r}_2)=((0,-1/2,0),(0,+1/2,0))$。\n  - 初始速度沿中心连线且方向相反：$(\\vec{v}_1,\\vec{v}_2)=((0,+0.5,0),(0,-0.5,0))$。\n  - $\\Delta t=10^{-2}$，$\\epsilon^2=10^{-4}$。\n  - $T_{\\mathrm{train}}=400$，$T_{\\mathrm{eval}}=200$。\n  - $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.15,5\\times 10^{-3})$，其中 $\\tau_0=10^{-6}$。\n\n- 用例 3（三体等质量等边三角形旋转构型）：\n  - $N=3$，质量 $(1,1,1)$。\n  - 将粒子放置在外接圆半径 $R=1$ 的等边三角形的顶点上：$\\vec{r}_1=(1,0,0)$，$\\vec{r}_2=(-1/2,\\sqrt{3}/2,0)$，$\\vec{r}_3=(-1/2,-\\sqrt{3}/2,0)$。\n  - 对于角速度为 $\\omega$ 且满足 $\\omega^2=1/(\\sqrt{3} R^3)$ 的匀速旋转，初始化切向速度 $\\vec{v}_i=\\omega\\,\\hat{\\boldsymbol{t}}_i\\,R$，其中 $\\hat{\\boldsymbol{t}}_i$ 在逆时针方向上垂直于 $\\vec{r}_i$。在数值上，使用 $\\omega=\\sqrt{1/\\sqrt{3}}$ 和 $R=1$。\n  - $\\Delta t=10^{-2}$，$\\epsilon^2=10^{-6}$。\n  - $T_{\\mathrm{train}}=600$，$T_{\\mathrm{eval}}=300$。\n  - $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.20,10^{-3})$，其中 $\\tau_0=10^{-6}$。\n\n- 用例 4（三体，不等质量，非圆形，近距离相遇）：\n  - $N=3$，质量 $(1.5,1.0,0.1)$。\n  - 初始位置 $\\vec{r}_1=(-0.7,0,0)$，$\\vec{r}_2=(+0.7,0,0)$，$\\vec{r}_3=(0,0.2,0)$。\n  - 初始速度选择为近似束缚态，并调整以使总线动量为零：从 $\\vec{v}_1=(0,0.4,0)$，$\\vec{v}_2=(0,-0.6,0)$，$\\vec{v}_3=(0.2,0,0)$ 开始，然后从每个速度中减去质量加权平均速度，以强制满足 $\\sum_i m_i \\vec{v}_i=\\vec{0}$。\n  - $\\Delta t=5\\times 10^{-3}$，$\\epsilon^2=10^{-4}$。\n  - $T_{\\mathrm{train}}=800$，$T_{\\mathrm{eval}}=300$。\n  - $(\\varepsilon_{\\mathrm{rel}},\\varepsilon_{\\mathrm{abs}})=(0.15,10^{-3})$，其中 $\\tau_0=10^{-6}$。\n\n您的程序必须对每个用例，在训练快照上训练线性生成器，使用学习到的生成器从初始快照生成评估演化推算，计算如上定义的角动量偏差，并生成一个布尔值，指示在该用例的学习模型下守恒性是否得到满足。所有计算均采用无量纲单位；最终的布尔输出是无量纲的。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[True,False,True,False]”），按顺序对应于用例 1 到 4 的结果。", "solution": "The user has provided a problem that is scientifically grounded, well-posed, and objective. It outlines a computational experiment to evaluate whether a data-driven linear generative model can learn to preserve the physical conservation of angular momentum in an $N$-body gravitational system. The problem supplies all necessary physical laws, numerical methods, model specifications, evaluation criteria, and concrete test cases. It is a valid problem in the field of computational physics and scientific machine learning. Therefore, I will proceed with a full solution.\n\nThe fundamental task is to assess a linear model's ability to respect a nonlinear conservation law. The true dynamics of an $N$-body system are governed by a set of coupled, nonlinear ordinary differential equations. The time-evolution operator, or flow map $\\Phi_{\\Delta t}$, that advances the state of the system $\\vec{s}(t)$ to $\\vec{s}(t+\\Delta t)$ is a complex nonlinear function. The problem proposes to approximate this map with a single linear operator $\\mathbf{A}$, such that $\\vec{s}_{t+1} \\approx \\mathbf{A}\\vec{s}_t$. This matrix $\\mathbf{A}$ is learned from data by minimizing the least-squares error, with no explicit physical constraints imposed. The central question is whether the symmetries of the true dynamics that lead to conservation laws are implicitly captured by this linear approximation.\n\nThe total angular momentum of an isolated $N$-body system, $\\vec{L} = \\sum_{i=1}^{N} m_i \\vec{r}_i \\times \\vec{v}_i$, is a conserved quantity. This conservation arises from the rotational symmetry of the Hamiltonian. A linear model $\\mathbf{A}$ will preserve $\\vec{L}$ if and only if the structure of $\\mathbf{A}$ respects this symmetry for all possible states. For a general time-varying state, this is highly unlikely. The matrix $\\mathbf{A}$ is not guaranteed to be symplectic, which would be a necessary condition to preserve the geometric structure of Hamiltonian dynamics. However, for a sufficiently small time step $\\Delta t$ and for trajectories that are regular (e.g., periodic or quasi-periodic), the nonlinear flow map $\\Phi_{\\Delta t}$ may be well-approximated by a linear map in the local region of phase space occupied by the trajectory. In such cases, the learned model might exhibit approximate conservation. For chaotic trajectories, this approximation is expected to fail rapidly.\n\nThe solution is implemented through a series of computational steps for each test case.\n\n1.  **Data Generation**:\n    The state of the system at any time $t$ is represented by a snapshot vector $\\vec{s}_t \\in \\mathbb{R}^{6N}$, which is a flattened concatenation of the position vectors $\\{\\vec{r}_i \\in \\mathbb{R}^3\\}_{i=1}^N$ and velocity vectors $\\{\\vec{v}_i \\in \\mathbb{R}^3\\}_{i=1}^N$. We generate a time series of these snapshots, $\\vec{s}_0, \\vec{s}_1, \\ldots, \\vec{s}_{T_{\\mathrm{train}}}$, by numerically integrating the equations of motion using the velocity-Verlet method. This is a second-order symplectic integrator known for its good long-term stability and conservation properties in Hamiltonian systems. The gravitational acceleration on particle $i$ is computed with a softening term $\\epsilon^2$ to prevent singularities during close encounters:\n    $$ \\vec{a}_i = \\sum_{j \\neq i} \\frac{G m_j (\\vec{r}_j - \\vec{r}_i)}{(\\lVert \\vec{r}_i - \\vec{r}_j \\rVert_2^2 + \\epsilon^2)^{3/2}} $$\n    The velocity-Verlet algorithm advances the state over a time step $\\Delta t$ as follows:\n    \\begin{enumerate}\n        \\item $\\vec{v}_i(t + \\frac{1}{2}\\Delta t) = \\vec{v}_i(t) + \\frac{1}{2}\\vec{a}_i(t)\\Delta t$\n        \\item $\\vec{r}_i(t + \\Delta t) = \\vec{r}_i(t) + \\vec{v}_i(t + \\frac{1}{2}\\Delta t)\\Delta t$\n        \\item Compute $\\vec{a}_i(t + \\Delta t)$ using the new positions $\\vec{r}_i(t + \\Delta t)$.\n        \\item $\\vec{v}_i(t + \\Delta t) = \\vec{v}_i(t + \\frac{1}{2}\\Delta t) + \\frac{1}{2}\\vec{a}_i(t + \\Delta t)\\Delta t$\n    \\end{enumerate}\n    This process is repeated for $T_{\\mathrm{train}}$ steps to create the training dataset.\n\n2.  **Generative Model Training**:\n    The purely data-driven model is a linear map $\\mathbf{A}$ of size $6N \\times 6N$. We seek the matrix $\\mathbf{A}$ that best approximates the relationship $\\vec{s}_{t+1} = \\mathbf{A}\\vec{s}_t$. We form two matrices from the training data: $S_{\\mathrm{in}}$, whose rows are the snapshots $\\vec{s}_0, \\ldots, \\vec{s}_{T_{\\mathrm{train}}-1}$, and $S_{\\mathrm{out}}$, whose rows are $\\vec{s}_1, \\ldots, \\vec{s}_{T_{\\mathrm{train}}}$. We wish to find a matrix $\\mathbf{M}$ such that $S_{\\mathrm{out}} \\approx S_{\\mathrm{in}}\\mathbf{M}$. This is a standard linear least-squares problem, solved for $\\mathbf{M}$. If we define state vectors as row vectors, then this matrix $\\mathbf{M}$ is the transpose of the desired operator $\\mathbf{A}$. The solution is found efficiently using numerical linear algebra routines, such as those provided by `numpy.linalg.lstsq`. The learned matrix, which we will call $\\mathbf{A}_{\\text{op}}$, defines the update rule $\\vec{s}_{t+1}^T = \\vec{s}_t^T \\mathbf{A}_{\\text{op}}$.\n\n3.  **Rollout and Conservation Test**:\n    Using the learned operator $\\mathbf{A}_{\\text{op}}$, we generate a new sequence of states (a \"rollout\") starting from the initial condition $\\vec{s}_0$ of the test case. The sequence is generated recursively for $T_{\\mathrm{eval}}$ steps:\n    $$ \\vec{s}_{t+1}^{\\text{gen},T} = \\vec{s}_{t}^{\\text{gen},T} \\mathbf{A}_{\\text{op}} $$\n    For each generated snapshot $\\vec{s}_t^{\\text{gen}}$, we reconstruct the positions $\\{\\vec{r}_i(t)\\}$ and velocities $\\{\\vec{v}_i(t)\\}$ and compute the total angular momentum vector $\\vec{L}(t) = \\sum_{i=1}^{N} m_i (\\vec{r}_i(t) \\times \\vec{v}_i(t))$. We then compare the evolution of $\\vec{L}(t)$ to its initial value $\\vec{L}(0)$.\n    The conservation test is performed according to the specified criteria. Let $L_0 = \\lVert \\vec{L}(0) \\rVert_2$.\n    \\begin{itemize}\n        \\item If $L_0 > \\tau_0$, we calculate the maximum relative deviation: $\\max_{0 \\le t \\le T_{\\mathrm{eval}}} \\frac{\\lVert \\vec{L}(t) - \\vec{L}(0) \\rVert_2}{L_0}$. The test passes if this value is less than or equal to the relative tolerance $\\varepsilon_{\\mathrm{rel}}$.\n        \\item If $L_0 \\le \\tau_0$, we calculate the maximum absolute magnitude: $\\max_{0 \\le t \\le T_{\\mathrm{eval}}} \\lVert \\vec{L}(t) \\rVert_2$. The test passes if this value is less than or equal to the absolute tolerance $\\varepsilon_{\\mathrm{abs}}$.\n    \\end{itemize}\n    This procedure is carried out for each of the four specified test cases, and a boolean result is recorded for each.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    # Gravitational constant G=1 everywhere.\n    G = 1.0\n    \n    # Threshold for L0 magnitude in conservation test\n    tau0 = 1e-6\n\n    # Test cases defined as a list of dictionaries.\n    test_cases = [\n        {\n            \"name\": \"Case 1: Two-body circular orbit\",\n            \"N\": 2,\n            \"m\": np.array([1.0, 1.0]),\n            \"r0\": np.array([[-0.5, 0.0, 0.0], [0.5, 0.0, 0.0]]),\n            \"v0\": np.array([[0.0, 0.5 * np.sqrt(2), 0.0], [0.0, -0.5 * np.sqrt(2), 0.0]]),\n            \"dt\": 1e-2,\n            \"eps2\": 1e-6,\n            \"T_train\": 400,\n            \"T_eval\": 200,\n            \"eps_rel\": 0.15,\n            \"eps_abs\": 1e-3,\n        },\n        {\n            \"name\": \"Case 2: Two-body head-on\",\n            \"N\": 2,\n            \"m\": np.array([1.0, 1.0]),\n            \"r0\": np.array([[0.0, -0.5, 0.0], [0.0, 0.5, 0.0]]),\n            \"v0\": np.array([[0.0, 0.5, 0.0], [0.0, -0.5, 0.0]]),\n            \"dt\": 1e-2,\n            \"eps2\": 1e-4,\n            \"T_train\": 400,\n            \"T_eval\": 200,\n            \"eps_rel\": 0.15,\n            \"eps_abs\": 5e-3,\n        },\n        {\n            \"name\": \"Case 3: Three-body equilateral\",\n            \"N\": 3,\n            \"m\": np.array([1.0, 1.0, 1.0]),\n            \"r0\": np.array([\n                [1.0, 0.0, 0.0],\n                [-0.5, np.sqrt(3)/2.0, 0.0],\n                [-0.5, -np.sqrt(3)/2.0, 0.0]\n            ]),\n            \"v0\": None, # Will be computed\n            \"dt\": 1e-2,\n            \"eps2\": 1e-6,\n            \"T_train\": 600,\n            \"T_eval\": 300,\n            \"eps_rel\": 0.20,\n            \"eps_abs\": 1e-3,\n        },\n        {\n            \"name\": \"Case 4: Three-body near-encounter\",\n            \"N\": 3,\n            \"m\": np.array([1.5, 1.0, 0.1]),\n            \"r0\": np.array([[-0.7, 0.0, 0.0], [0.7, 0.0, 0.0], [0.0, 0.2, 0.0]]),\n            \"v0\": None, # Will be computed\n            \"dt\": 5e-3,\n            \"eps2\": 1e-4,\n            \"T_train\": 800,\n            \"T_eval\": 300,\n            \"eps_rel\": 0.15,\n            \"eps_abs\": 1e-3,\n        },\n    ]\n\n    # Special initial condition calculations\n    # Case 3 ICs\n    R_c3 = 1.0\n    # From problem statement: omega^2 = 1/(sqrt(3) * R^3) for G=1, m=1.\n    omega_c3_from_prob = np.sqrt(1.0 / (np.sqrt(3.0) * R_c3**3))\n    r0_c3 = test_cases[2]['r0']\n    v0_c3 = np.zeros_like(r0_c3)\n    for i in range(3):\n        # Tangential vector is perpendicular to position vector in xy-plane\n        v0_c3[i, :] = np.cross([0, 0, 1], r0_c3[i, :])\n    v0_c3 = v0_c3 * omega_c3_from_prob\n    test_cases[2]['v0'] = v0_c3\n\n    # Case 4 ICs\n    m_c4 = test_cases[3]['m']\n    v0_prime_c4 = np.array([[0.0, 0.4, 0.0], [0.0, -0.6, 0.0], [0.2, 0.0, 0.0]])\n    total_momentum = (m_c4[:, np.newaxis] * v0_prime_c4).sum(axis=0)\n    total_mass = m_c4.sum()\n    v_com = total_momentum / total_mass\n    test_cases[3]['v0'] = v0_prime_c4 - v_com\n\n    results = []\n    for case in test_cases:\n        result = run_case(G=G, tau0=tau0, **case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_accel(r, m, G, eps2):\n    \"\"\"Calculates gravitational acceleration for all particles.\"\"\"\n    N = r.shape[0]\n    accel = np.zeros_like(r)\n    for i in range(N):\n        for j in range(N):\n            if i == j:\n                continue\n            r_ij = r[j] - r[i]\n            r_ij_norm_sq = np.sum(r_ij**2)\n            denominator = (r_ij_norm_sq + eps2)**1.5\n            accel[i] += G * m[j] * r_ij / denominator\n    return accel\n\ndef verlet_step(r, v, m, G, dt, eps2):\n    \"\"\"Performs a single Velocity-Verlet integration step.\"\"\"\n    a_t = get_accel(r, m, G, eps2)\n    v_half = v + 0.5 * dt * a_t\n    r_new = r + dt * v_half\n    a_new = get_accel(r_new, m, G, eps2)\n    v_new = v_half + 0.5 * dt * a_new\n    return r_new, v_new\n\ndef get_angular_momentum(r, v, m):\n    \"\"\"Calculates the total angular momentum of the system.\"\"\"\n    L_per_particle = np.cross(r, v) * m[:, np.newaxis]\n    return np.sum(L_per_particle, axis=0)\n\ndef run_case(name, N, m, r0, v0, dt, eps2, T_train, T_eval, eps_rel, eps_abs, G, tau0):\n    \"\"\"Runs a single test case from simulation to evaluation.\"\"\"\n    \n    # 1. Data Generation\n    dim = 3 * N\n    snapshots = np.zeros((T_train + 1, 2 * dim))\n    r, v = np.copy(r0), np.copy(v0)\n    \n    snapshots[0] = np.concatenate((r.flatten(), v.flatten()))\n    for t_idx in range(T_train):\n        r, v = verlet_step(r, v, m, G, dt, eps2)\n        snapshots[t_idx + 1] = np.concatenate((r.flatten(), v.flatten()))\n\n    # 2. Generative Model Training\n    S_in = snapshots[:-1]\n    S_out = snapshots[1:]\n    \n    # Solve S_in @ A_op = S_out for A_op. We model s_new.T = A @ s_old.T\n    # So we want to solve A @ S_in.T = S_out.T\n    A = np.linalg.lstsq(S_in.T, S_out.T, rcond=None)[0]\n\n    # 3. Rollout Generation\n    rollout_snapshots = np.zeros((T_eval + 1, 2 * dim))\n    current_s = snapshots[0].reshape(-1, 1) # column vector\n    rollout_snapshots[0] = current_s.flatten()\n\n    for t_idx in range(T_eval):\n        # Update using s_new = A @ s_old\n        current_s = A @ current_s\n        rollout_snapshots[t_idx + 1] = current_s.flatten()\n        \n    # 4. Conservation Test\n    r_init = rollout_snapshots[0, :dim].reshape((N, 3))\n    v_init = rollout_snapshots[0, dim:].reshape((N, 3))\n    L0_vec = get_angular_momentum(r_init, v_init, m)\n    L0_mag = np.linalg.norm(L0_vec)\n\n    deviations = []\n    L_mags = []\n\n    for t_idx in range(T_eval + 1):\n        s_t = rollout_snapshots[t_idx]\n        r_t = s_t[:dim].reshape((N, 3))\n        v_t = s_t[dim:].reshape((N, 3))\n        L_t_vec = get_angular_momentum(r_t, v_t, m)\n        \n        if L0_mag > tau0:\n            dev = np.linalg.norm(L_t_vec - L0_vec)\n            deviations.append(dev)\n        else: # L0 is effectively zero\n            mag = np.linalg.norm(L_t_vec)\n            L_mags.append(mag)\n            \n    is_conserved = False\n    if L0_mag > tau0:\n        max_rel_dev = np.max(deviations) / L0_mag\n        if max_rel_dev = eps_rel:\n            is_conserved = True\n    else:\n        if L_mags: # ensure list is not empty\n            max_abs_mag = np.max(L_mags)\n            if max_abs_mag = eps_abs:\n                is_conserved = True\n        else: # If L0 is zero and T_eval is 0, L_mags is empty.\n            is_conserved = True # No deviation occurred\n            \n    return is_conserved\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2398389"}, {"introduction": "本次实践将进入一个更复杂的任务：构建一个条件生成模型来预测物理过程的结果。你将训练一个条件变分自编码器 (cVAE)，以学习从初始散射条件到最终粒子位置的映射关系。这项练习将介绍如何处理条件输入，并使用更强大的非线性模型来捕捉物理系统中复杂的函数关系 [@problem_id:2398395]。", "problem": "您将构建、训练并评估一个条件生成模型，用于模拟带电粒子被固定排斥性库仑中心所进行的二维经典散射。目标是使用条件变分自编码器 (Conditional Variational Autoencoder, cVAE) 对给定初始束流能量和碰撞参数下的最终探测器平面位置的条件分布进行建模。所有计算都必须使用一套一致的物理单位，最终程序必须输出一个机器可检查的单行文本，汇总固定测试集的结果。\n\n任务概述：\n- 物理学：一个质量为 $m$、电荷为 $q$ 的经典点粒子，以初始动能 $E$ 和 $y$ 方向的碰撞参数 $b$，从 $x=-\\infty$ 处沿 $+x$ 方向入射。它被一个位于原点、带电荷 $Q$ 的固定靶散射，两者之间存在排斥性平方反比库仑力。一个以原点为中心、半径为 $R$ 的圆形探测器记录其渐近出射方向，表现为圆上的一个位置。\n- 数据生成：通过采样 $(E,b)$ 对并将其映射到探测器上带有噪声的观测最终位置 $(x_{\\text{det}},y_{\\text{det}})$ 来合成训练数据。您必须基于牛顿第二定律、能量守恒和角动量守恒推导出的平方反比排斥散射的基本力学原理来建立此映射。在 $(x_{\\text{det}},y_{\\text{det}})$ 上加入一个小的、物理上合理的加性高斯测量噪声，以表示探测器不确定性。\n- 模型：训练一个条件变分自编码器 (cVAE) 来学习 $p_\\theta(\\mathbf{x}\\mid \\mathbf{c})$，其中 $\\mathbf{x}=\\left[x_{\\text{det}},y_{\\text{det}}\\right]$ 且 $\\mathbf{c}=\\left[E,b\\right]$，使用重参数化技巧和证据下界目标函数。使用一个低维潜变量 $\\mathbf{z}$，其先验分布为标准正态分布 $p(\\mathbf{z})=\\mathcal{N}(\\mathbf{0},\\mathbf{I})$。编码器和解码器都以 $\\mathbf{c}$ 为条件。重构项必须是高斯负对数似然或其均方误差代理，并且库尔贝克-莱布勒散度必须将近似后验正则化至标准正态先验。\n- 生成：训练后，以测试 $(E,b)$ 为条件，并通过使用 $\\mathbf{z}=\\mathbf{0}$（对应于先验均值）进行解码来生成平均最终位置，从而为评估产生一个确定性预测。\n- 评估：对于每个指定的测试用例，计算生成位置与探测器圆上物理正确的基准真相位置之间的欧几里得距离（单位为米）。\n\n您必须使用的基本原理：\n- 中心场中的牛顿第二定律、角动量守恒，以及库仑中心势 $U(r)=\\frac{\\kappa qQ}{r}$（其中 $\\kappa=\\frac{1}{4\\pi\\varepsilon_0}$）下的机械能守恒。\n- 由上述原理推导出的经典双曲线散射运动学，不使用任何未经推导的散射简化公式。\n\n物理和数值设置：\n- 质量 $m = 1.67262192369\\times 10^{-27}\\,\\mathrm{kg}$。\n- 基本电荷量 $e = 1.602176634\\times 10^{-19}\\,\\mathrm{C}$。\n- 固定电荷 $q=+e$, $Q=+e$（排斥性）。\n- 库仑常数 $\\kappa = 8.9875517923\\times 10^{9}\\,\\mathrm{N\\,m^2/C^2}$，因此 $\\alpha \\equiv \\kappa qQ$ 在与 $\\mathrm{C^2}$ 结合时单位为 $\\mathrm{J\\,m}$。\n- 探测器半径 $R = 1.0\\times 10^{-9}\\,\\mathrm{m}$。\n- 所有计算中的能量都必须以焦耳表示；如果在内部以千电子伏特生成能量，请确保使用 $1\\,\\mathrm{eV}=1.602176634\\times 10^{-19}\\,\\mathrm{J}$ 和 $1\\,\\mathrm{keV}=10^{3}\\,\\mathrm{eV}$ 将其转换为焦耳。\n- 碰撞参数 $b$ 以米为单位。\n- 在 $(x_{\\text{det}},y_{\\text{det}})$ 上施加的加性各向同性高斯探测器噪声为 $\\mathcal{N}(\\mathbf{0},\\sigma_{\\text{det}}^2\\mathbf{I})$，其中 $\\sigma_{\\text{det}}=1.0\\times 10^{-11}\\,\\mathrm{m}$。\n\n程序要求：\n- 通过在物理上合理的范围内（例如，能量 $E$ 在千电子伏特尺度并转换为焦耳，$|b|$ 最大为皮米量级）采样 $(E,b)$ 对，并通过物理推导的散射关系将其映射到带噪声的探测器位置 $(x_{\\text{det}},y_{\\text{det}})$，从而合成一个训练集。对输入和输出进行归一化以稳定训练，但评估结果需以物理单位报告。\n- 实现一个条件变分自编码器 (cVAE)，具备：\n  - 潜变量 $\\mathbf{z}$ 的标准正态先验。\n  - 一个编码器 $q_\\phi(\\mathbf{z}\\mid \\mathbf{x},\\mathbf{c})$ 和一个解码器 $p_\\theta(\\mathbf{x}\\mid \\mathbf{z},\\mathbf{c})$，它们是使用基本全连接层实现的可微函数。使用重参数化技巧 $\\mathbf{z}=\\boldsymbol{\\mu}_z+\\boldsymbol{\\sigma}_z\\odot \\boldsymbol{\\epsilon}$，其中 $\\boldsymbol{\\epsilon}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})$。\n  - 一个训练目标函数，包含一个重构项和一个介于 $q_\\phi(\\mathbf{z}\\mid \\mathbf{x},\\mathbf{c})$ 与先验之间的库尔贝克-莱布勒项。\n- 训练直至收敛，以在测试集上获得较小的欧几里得误差。保持模型大小和数据集大小适中，以使程序快速运行。\n- 对于测试集上的生成任务，使用 $\\mathbf{z}=\\mathbf{0}$ 进行解码，以获得单一的确定性预测位置。\n\n测试集：\n使用以下五个测试用例，能量单位为千电子伏特（需在程序中转换为焦耳），碰撞参数单位为米。对于每一对，使用物理推导的关系计算探测器上的基准真相最终位置，并与上述指定的 cVAE 预测进行比较。最终需要打印的输出是欧几里得误差（单位为米，浮点数）。\n\n- 用例 1：$E = 1.0\\,\\mathrm{keV}$, $b = 0.0\\,\\mathrm{m}$。\n- 用例 2：$E = 2.0\\,\\mathrm{keV}$, $b = 1.0\\times 10^{-12}\\,\\mathrm{m}$。\n- 用例 3：$E = 0.8\\,\\mathrm{keV}$, $b = 2.0\\times 10^{-12}\\,\\mathrm{m}$。\n- 用例 4：$E = 3.0\\,\\mathrm{keV}$, $b = -1.5\\times 10^{-12}\\,\\mathrm{m}$。\n- 用例 5：$E = 1.5\\,\\mathrm{keV}$, $b = 2.0\\times 10^{-12}\\,\\mathrm{m}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，\"[r1,r2,r3,r4,r5]\"），其中每个 $r_i$ 是第 $i$ 个用例的欧几里得误差（单位为米，浮点数），顺序与上文一致。\n\n角度单位：\n- 任何内部角度计算都必须使用弧度。\n\n物理单位：\n- 所有最终距离必须以米报告，物理计算中的能量必须为焦耳，任何内部指定的千电子伏特都必须正确转换为焦耳。\n\n约束条件：\n- 最终答案必须是一个完整的可运行程序，执行所有步骤：数据合成、cVAE 训练、条件生成和测试集评估，然后打印所需的单行输出。程序不得要求任何用户输入。", "solution": "在尝试任何解决方案之前，问题陈述都经过了严格的验证过程。\n\n### 步骤 1：提取已知条件\n- **物理系统**：一个质量为 $m=1.67262192369\\times 10^{-27}\\,\\mathrm{kg}$、电荷为 $q=+e$ 的点粒子被位于原点、电荷为 $Q=+e$ 的固定电荷散射。基本电荷为 $e = 1.602176634\\times 10^{-19}\\,\\mathrm{C}$。\n- **相互作用**：排斥性库仑势 $U(r) = \\frac{\\alpha}{r}$，其中 $\\alpha = \\kappa q Q$ 且 $\\kappa = 8.9875517923\\times 10^{9}\\,\\mathrm{N\\,m^2/C^2}$。\n- **初始条件**：粒子具有初始动能 $E$ 和碰撞参数 $b$。\n- **探测器**：一个以原点为中心、半径为 $R = 1.0\\times 10^{-9}\\,\\mathrm{m}$ 的圆形探测器记录最终位置 $(x_{\\text{det}}, y_{\\text{det}})$。\n- **测量噪声**：为训练数据，在探测器位置上添加了加性各向同性高斯噪声 $\\mathcal{N}(\\mathbf{0}, \\sigma_{\\text{det}}^2\\mathbf{I})$，其中 $\\sigma_{\\text{det}} = 1.0\\times 10^{-11}\\,\\mathrm{m}$。\n- **模型**：将训练一个条件变分自编码器 (cVAE) 来学习条件概率分布 $p_{\\theta}(\\mathbf{x}|\\mathbf{c})$，其中数据为 $\\mathbf{x} = [x_{\\text{det}}, y_{\\text{det}}]$，条件为 $\\mathbf{c} = [E, b]$。\n- **cVAE 规范**：该模型对潜变量 $\\mathbf{z}$ 使用标准正态先验 $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$。训练目标是使用重参数化技巧的证据下界 (ELBO)。\n- **生成**：对于给定的条件 $\\mathbf{c}$，通过解码先验均值（即使用 $\\mathbf{z}=\\mathbf{0}$）来生成预测。\n- **评估**：性能通过模型预测与探测器上无噪声的基准真相位置之间的欧几里得距离来衡量。\n- **能量转换**：$1\\,\\mathrm{keV} = 10^3\\,\\mathrm{eV} = 1.602176634\\times 10^{-16}\\,\\mathrm{J}$。\n- **测试集**：提供五对 $(E, b)$ 用于最终评估。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行严格评估。\n\n- **科学基础**：该问题建立在经典力学和电磁学原理之上，特别是卢瑟福（或库仑）散射这一已被充分理解的现象。所有提供的物理常数都是准确的，该场景是一个标准的教科书示例。此标准已满足。\n- **适定性**：对于任何 $E0$ 的非病态初始条件 $(E,b)$，都存在唯一、稳定的散射轨迹。初始条件与最终散射角之间的关系是一个连续函数。使用生成模型学习此函数（包括噪声效应）的任务在机器学习中是一个适定问题。此标准已满足。\n- **客观性**：问题以定量的精度进行了规定。所有参数、常数、模型要求和评估指标都得到了明确定义。没有主观或推测性内容。此标准已满足。\n\n所有其他标准（完整性、无矛盾性、可行性）也得到满足。该问题是计算物理学中的一个标准练习，它将经典理论与现代机器学习技术相结合。\n\n### 步骤 3：结论与行动\n该问题被判定为**有效**。将根据要求构建解决方案。\n\n### 解决方案推导\n\n解决方案的开发过程是：首先建立物理基准真相，然后合成数据集，最后构建并训练 cVAE 模型以学习潜在的物理过程。\n\n**1. 源于经典散射理论的基准真相**\n初始条件 $(E, b)$ 与最终状态之间的关系由排斥性库仑势 $U(r) = \\alpha/r$（其中 $\\alpha = \\kappa q Q$）下的能量守恒和角动量守恒推导得出。\n\n总能量 $E$ 和角动量 $L$ 是运动常数：\n$$ E = \\frac{1}{2}mv_0^2 $$\n$$ L = v_0 b m $$\n其中 $v_0$ 是粒子在无穷远处的初始速率。\n\n散射角 $\\theta_s$（初始和最终速度矢量之间的夹角）可以通过求解轨道方程找到。在 $1/r^2$ 力作用下的双曲线轨迹的标准结果将碰撞参数 $b$ 和散射角 $\\theta_s$ 联系起来，如下所示：\n$$ \\cot\\left(\\frac{\\theta_s}{2}\\right) = \\frac{2 E b}{\\alpha} $$\n这个方程可以解出 $\\theta_s$。然而，为了正确捕捉正负碰撞参数下的偏转方向，更方便的做法是定义粒子轨迹的最终渐近极角 $\\phi_{\\text{final}}$。该角度取决于 $b$ 的符号：\n$$ \\tan\\left(\\frac{\\phi_{\\text{final}}}{2}\\right) = \\frac{\\alpha}{2 E b} $$\n这个方程可以解出 $\\phi_{\\text{final}}$：\n$$ \\phi_{\\text{final}} = 2 \\arctan\\left(\\frac{\\alpha}{2 E b}\\right) $$\n该表达式正确地给出，当 $b \\to 0^+$ 时 $\\phi_{\\text{final}} \\to \\pi$（正向对心碰撞，粒子向后散射），当 $b \\to 0^-$ 时 $\\phi_{\\text{final}} \\to -\\pi$。对于 $b=0$ 的特殊情况，偏转恰好是向后，因此 $\\phi_{\\text{final}} = \\pi$。当 $|b| \\to \\infty$ 时，$\\phi_{\\text{final}} \\to 0$，表示没有偏转。\n\n然后，通过将这个最终角度投影到半径为 $R$ 的探测器圆上，可以得到基准真相位置：\n$$ \\mathbf{x}_{\\text{true}} = (x_{\\text{det}}, y_{\\text{det}}) = (R \\cos(\\phi_{\\text{final}}), R \\sin(\\phi_{\\text{final}})) $$\n这个物理模型作为生成训练数据和评估最终训练好的 cVAE 的“神谕”（oracle）。\n\n**2. 数据合成与预处理**\n生成一个合成数据集来训练 cVAE。从物理上合理的范围（$E \\in [0.5, 3.5]\\,\\mathrm{keV}$，$b \\in [-3 \\times 10^{-12}, 3 \\times 10^{-12}]\\,\\mathrm{m}$）的均匀分布中采样条件对 $\\mathbf{c} = (E, b)$。对于每个采样对，使用上述公式计算基准真相探测器位置 $\\mathbf{x}_{\\text{true}}$。为了模拟实验不确定性，引入了加性高斯噪声：\n$$ \\mathbf{x}_{\\text{noisy}} = \\mathbf{x}_{\\text{true}} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\text{det}}^2\\mathbf{I}) $$\n训练集由 $(\\mathbf{c}, \\mathbf{x}_{\\text{noisy}})$ 对组成。为确保神经网络的稳定训练，条件 $\\mathbf{c}$ 和位置 $\\mathbf{x}$ 都需要进行归一化。条件根据训练集的统计数据进行标准化，使其均值为零，方差为一。位置 $\\mathbf{x}$ 通过除以探测器半径 $R$ 进行归一化，将其映射到一个半径为 1 的圆上。\n\n**3. 条件变分自编码器 (cVAE)**\ncVAE 旨在学习条件分布 $p(\\mathbf{x}|\\mathbf{c})$。它由两个主要部分组成：一个编码器和一个解码器，两者都实现为多层感知器 (MLP)。\n\n- **编码器 $q_{\\phi}(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})$**：该网络将连接后的归一化位置 $\\mathbf{x}_{\\text{norm}}$ 和条件 $\\mathbf{c}_{\\text{norm}}$ 作为输入。它输出潜变量 $\\mathbf{z}$ 的近似后验分布的参数：均值向量 $\\boldsymbol{\\mu}_z$ 和对数方差向量 $\\log(\\boldsymbol{\\sigma}_z^2)$。该后验是一个对角高斯分布。\n\n- **解码器 $p_{\\theta}(\\mathbf{x}|\\mathbf{z}, \\mathbf{c})$**：该网络将连接后的潜向量 $\\mathbf{z}$ 和条件 $\\mathbf{c}_{\\text{norm}}$ 作为输入。它输出重构的归一化位置 $\\hat{\\mathbf{x}}_{\\text{norm}}$。\n\n**4. 训练目标与过程**\ncVAE 通过最大化证据下界 (ELBO) 进行训练，这等同于最小化损失函数 $L$：\n$$ L = L_{\\text{recon}} + L_{\\text{KL}} $$\n- **重构损失 $L_{\\text{recon}}$**：此项鼓励解码器准确地重构输入数据。我们使用原始归一化位置 $\\mathbf{x}_{\\text{norm}}$ 和重构位置 $\\hat{\\mathbf{x}}_{\\text{norm}}$ 之间的均方误差 (MSE)：\n$$ L_{\\text{recon}} = ||\\mathbf{x}_{\\text{norm}} - \\hat{\\mathbf{x}}_{\\text{norm}}||^2 $$\n- **库尔贝克-莱布勒 (KL) 散度 $L_{\\text{KL}}$**：此项作为正则化器，迫使近似后验 $q_{\\phi}(\\mathbf{z}|\\mathbf{x}, \\mathbf{c})$ 接近标准正态先验 $p(\\mathbf{z})$。对于对角高斯分布，它有一个封闭形式的表达式：\n$$ L_{\\text{KL}} = D_{KL}(q_{\\phi} || p) = \\frac{1}{2} \\sum_{i=1}^{d_z} \\left(\\sigma_{z,i}^2 + \\mu_{z,i}^2 - \\log(\\sigma_{z,i}^2) - 1\\right) $$\n其中 $d_z$ 是潜空间的维度。\n\n训练使用 Adam 优化器，通过小批量随机梯度下降进行。使用重参数化技巧 $\\mathbf{z} = \\boldsymbol{\\mu}_z + \\boldsymbol{\\sigma}_z \\odot \\boldsymbol{\\epsilon}$（其中 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$），以允许梯度从重构损失通过采样过程反向传播到编码器参数。\n\n**5. 生成与评估**\n训练后，模型可以为任何新的条件 $\\mathbf{c}_{\\text{test}}$ 生成预测的探测器位置。按照规定，这是通过将潜变量设置为其先验均值 $\\mathbf{z}=\\mathbf{0}$，并将其与归一化的条件 $\\mathbf{c}_{\\text{test,norm}}$ 一起传递给解码器来完成的。这会产生一个确定性的预测 $\\hat{\\mathbf{x}}_{\\text{norm,pred}}$。该预测通过乘以 $R$ 来去归一化。最终评估是对于测试集中的每个案例，此预测位置 $\\hat{\\mathbf{x}}_{\\text{pred}}$ 与无噪声的基准真相位置 $\\mathbf{x}_{\\text{true}}$ 之间的欧几里得距离（单位为米）。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the CVAE-based modeling of Coulomb scattering.\n    It performs data synthesis, model training, and evaluation on the test suite.\n    \"\"\"\n    # For reproducibility of the neural network training\n    np.random.seed(42)\n\n    # --- Physical Constants ---\n    M_P = 1.67262192369e-27  # kg (proton mass)\n    E_CHARGE = 1.602176634e-19  # C (elementary charge)\n    K_COULOMB = 8.9875517923e9  # N m^2 / C^2\n    ALPHA = K_COULOMB * E_CHARGE**2  # J*m\n    R_DET = 1.0e-9  # m (detector radius)\n    SIGMA_DET = 1.0e-11  # m (detector noise standard deviation)\n    EV_TO_J = 1.602176634e-19  # J/eV\n\n    # --- Ground Truth Calculation ---\n    def get_ground_truth(E_J, b_m):\n        \"\"\"\n        Calculates the ground-truth detector position for given energy and impact parameter.\n        E_J: Energy in Joules.\n        b_m: Impact parameter in meters.\n        \"\"\"\n        positions = np.zeros((len(E_J), 2))\n        \n        # Handle non-zero impact parameter case\n        nonzero_b = b_m != 0\n        if np.any(nonzero_b):\n            arg = ALPHA / (2 * E_J[nonzero_b] * b_m[nonzero_b])\n            phi_final = 2 * np.arctan(arg)\n            positions[nonzero_b, 0] = R_DET * np.cos(phi_final)\n            positions[nonzero_b, 1] = R_DET * np.sin(phi_final)\n        \n        # Handle zero impact parameter (head-on collision)\n        zero_b = ~nonzero_b\n        if np.any(zero_b):\n            positions[zero_b, 0] = -R_DET\n            positions[zero_b, 1] = 0.0\n\n        return positions\n\n    # --- Data Synthesis ---\n    def generate_dataset(n_samples):\n        # Sample initial conditions\n        E_keV = np.random.uniform(0.5, 3.5, n_samples)\n        E_J = E_keV * 1000 * EV_TO_J\n        b_m = np.random.uniform(-3e-12, 3e-12, n_samples)\n        \n        # Conditions c = (E, b)\n        c = np.vstack([E_J, b_m]).T\n        \n        # Get ground truth positions\n        x_true = get_ground_truth(E_J, b_m)\n        \n        # Add detector noise\n        noise = np.random.normal(0, SIGMA_DET, x_true.shape)\n        x_noisy = x_true + noise\n        \n        return c, x_noisy\n\n    # --- Normalization ---\n    class Normalizer:\n        def __init__(self):\n            self.mean = None\n            self.std = None\n\n        def fit(self, data):\n            self.mean = np.mean(data, axis=0)\n            self.std = np.std(data, axis=0)\n            # Prevent division by zero if a feature is constant\n            self.std[self.std == 0] = 1.0\n\n        def transform(self, data):\n            return (data - self.mean) / self.std\n\n        def inverse_transform(self, data):\n            return data * self.std + self.mean\n\n    # --- Neural Network Implementation ---\n    def relu(x):\n        return np.maximum(0, x)\n\n    def relu_backward(dA, Z):\n        # dA is gradient of loss wrt output of relu\n        # Z is input to relu\n        dZ = dA.copy()\n        dZ[Z = 0] = 0\n        return dZ\n\n    class CVAE:\n        def __init__(self, input_dim, cond_dim, latent_dim, hidden_dim, learning_rate=1e-3):\n            self.input_dim = input_dim\n            self.cond_dim = cond_dim\n            self.latent_dim = latent_dim\n            self.hidden_dim = hidden_dim\n            self.lr = learning_rate\n            self.params = {}\n            self.caches = {}\n            self.grads = {}\n\n            # Xavier initialization\n            # Encoder\n            self.params['W_e1'] = np.random.randn(input_dim + cond_dim, hidden_dim) * np.sqrt(2 / (input_dim + cond_dim))\n            self.params['b_e1'] = np.zeros((1, hidden_dim))\n            self.params['W_e2'] = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_e2'] = np.zeros((1, hidden_dim))\n            self.params['W_mu'] = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_mu'] = np.zeros((1, latent_dim))\n            self.params['W_lv'] = np.random.randn(hidden_dim, latent_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_lv'] = np.zeros((1, latent_dim))\n\n            # Decoder\n            self.params['W_d1'] = np.random.randn(latent_dim + cond_dim, hidden_dim) * np.sqrt(2 / (latent_dim + cond_dim))\n            self.params['b_d1'] = np.zeros((1, hidden_dim))\n            self.params['W_d2'] = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_d2'] = np.zeros((1, hidden_dim))\n            self.params['W_out'] = np.random.randn(hidden_dim, input_dim) * np.sqrt(2 / hidden_dim)\n            self.params['b_out'] = np.zeros((1, input_dim))\n            \n            # Adam optimizer parameters\n            self.m = {k: np.zeros_like(v) for k, v in self.params.items()}\n            self.v = {k: np.zeros_like(v) for k, v in self.params.items()}\n            self.t = 0\n            self.beta1 = 0.9\n            self.beta2 = 0.999\n            self.epsilon = 1e-8\n\n\n        def forward(self, x, c):\n            # --- Encoder ---\n            enc_input = np.hstack([x, c])\n            self.caches['Z_e1'] = enc_input @ self.params['W_e1'] + self.params['b_e1']\n            self.caches['A_e1'] = relu(self.caches['Z_e1'])\n            self.caches['Z_e2'] = self.caches['A_e1'] @ self.params['W_e2'] + self.params['b_e2']\n            self.caches['A_e2'] = relu(self.caches['Z_e2'])\n            \n            mu = self.caches['A_e2'] @ self.params['W_mu'] + self.params['b_mu']\n            log_var = self.caches['A_e2'] @ self.params['W_lv'] + self.params['b_lv']\n            \n            # --- Reparameterization ---\n            std = np.exp(0.5 * log_var)\n            epsilon = np.random.randn(*mu.shape)\n            z = mu + std * epsilon\n            \n            self.caches['x'] = x\n            self.caches['c'] = c\n            self.caches['enc_input'] = enc_input\n            self.caches['mu'] = mu\n            self.caches['log_var'] = log_var\n            self.caches['std'] = std\n            self.caches['epsilon'] = epsilon\n\n            # --- Decoder ---\n            dec_input = np.hstack([z, c])\n            self.caches['Z_d1'] = dec_input @ self.params['W_d1'] + self.params['b_d1']\n            self.caches['A_d1'] = relu(self.caches['Z_d1'])\n            self.caches['Z_d2'] = self.caches['A_d1'] @ self.params['W_d2'] + self.params['b_d2']\n            self.caches['A_d2'] = relu(self.caches['Z_d2'])\n            \n            x_hat = self.caches['A_d2'] @ self.params['W_out'] + self.params['b_out']\n            self.caches['dec_input'] = dec_input\n            \n            return x_hat\n\n        def loss(self, x_hat):\n            batch_size = x_hat.shape[0]\n            # Reconstruction Loss (MSE)\n            recon_loss = np.mean((self.caches['x'] - x_hat)**2)\n            \n            # KL Divergence\n            kl_loss = -0.5 * np.sum(1 + self.caches['log_var'] - self.caches['mu']**2 - np.exp(self.caches['log_var']))\n            kl_loss /= batch_size\n            \n            return recon_loss + kl_loss\n\n        def backward(self, x_hat):\n            batch_size = x_hat.shape[0]\n            \n            # --- Gradient of Loss ---\n            d_recon_loss = (2 / batch_size) * (x_hat - self.caches['x'])\n            d_kl_mu = self.caches['mu'] / batch_size\n            d_kl_log_var = 0.5 * (np.exp(self.caches['log_var']) - 1) / batch_size\n            \n            # --- Decoder Backward ---\n            d_x_hat = d_recon_loss\n            dZ_out = d_x_hat\n            self.grads['W_out'] = self.caches['A_d2'].T @ dZ_out\n            self.grads['b_out'] = np.sum(dZ_out, axis=0, keepdims=True)\n            dA_d2 = dZ_out @ self.params['W_out'].T\n            \n            dZ_d2 = relu_backward(dA_d2, self.caches['Z_d2'])\n            self.grads['W_d2'] = self.caches['A_d1'].T @ dZ_d2\n            self.grads['b_d2'] = np.sum(dZ_d2, axis=0, keepdims=True)\n            dA_d1= dZ_d2 @ self.params['W_d2'].T\n            \n            dZ_d1 = relu_backward(dA_d1, self.caches['Z_d1'])\n            self.grads['W_d1'] = self.caches['dec_input'].T @ dZ_d1\n            self.grads['b_d1'] = np.sum(dZ_d1, axis=0, keepdims=True)\n            d_dec_input = dZ_d1 @ self.params['W_d1'].T\n\n            # --- Reparameterization Backward ---\n            d_z = d_dec_input[:, :self.latent_dim]\n            d_mu_from_recon = d_z\n            d_std = d_z * self.caches['epsilon']\n            d_log_var_from_recon = d_std * 0.5 * self.caches['std']\n            \n            # --- Encoder Backward ---\n            d_mu = d_mu_from_recon + d_kl_mu\n            d_log_var = d_log_var_from_recon + d_kl_log_var\n            \n            dZ_mu = d_mu\n            self.grads['W_mu'] = self.caches['A_e2'].T @ dZ_mu\n            self.grads['b_mu'] = np.sum(dZ_mu, axis=0, keepdims=True)\n            \n            dZ_lv = d_log_var\n            self.grads['W_lv'] = self.caches['A_e2'].T @ dZ_lv\n            self.grads['b_lv'] = np.sum(dZ_lv, axis=0, keepdims=True)\n            \n            dA_e2 = (dZ_mu @ self.params['W_mu'].T) + (dZ_lv @ self.params['W_lv'].T)\n            \n            dZ_e2 = relu_backward(dA_e2, self.caches['Z_e2'])\n            self.grads['W_e2'] = self.caches['A_e1'].T @ dZ_e2\n            self.grads['b_e2'] = np.sum(dZ_e2, axis=0, keepdims=True)\n            dA_e1 = dZ_e2 @ self.params['W_e2'].T\n            \n            dZ_e1 = relu_backward(dA_e1, self.caches['Z_e1'])\n            self.grads['W_e1'] = self.caches['enc_input'].T @ dZ_e1\n            self.grads['b_e1'] = np.sum(dZ_e1, axis=0, keepdims=True)\n        \n        def update_params(self):\n            self.t += 1\n            for key in self.params:\n                # Adam update\n                self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * self.grads[key]\n                self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (self.grads[key]**2)\n                m_hat = self.m[key] / (1 - self.beta1**self.t)\n                v_hat = self.v[key] / (1 - self.beta2**self.t)\n                self.params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n\n        def predict(self, c_norm):\n            z = np.zeros((c_norm.shape[0], self.latent_dim))\n            dec_input = np.hstack([z, c_norm])\n            \n            Z_d1 = dec_input @ self.params['W_d1'] + self.params['b_d1']\n            A_d1 = relu(Z_d1)\n            Z_d2 = A_d1 @ self.params['W_d2'] + self.params['b_d2']\n            A_d2 = relu(Z_d2)\n            x_hat_norm = A_d2 @ self.params['W_out'] + self.params['b_out']\n            \n            return x_hat_norm * R_DET\n\n    # --- Training Configuration ---\n    N_SAMPLES = 10000\n    EPOCHS = 100\n    BATCH_SIZE = 64\n    LEARNING_RATE = 1e-3\n    LATENT_DIM = 2\n    HIDDEN_DIM = 32\n\n    # --- Main Execution ---\n    \n    # 1. Prepare Data\n    c_train, x_train = generate_dataset(N_SAMPLES)\n    \n    c_normalizer = Normalizer()\n    c_normalizer.fit(c_train)\n    c_train_norm = c_normalizer.transform(c_train)\n    \n    x_train_norm = x_train / R_DET\n    \n    # 2. Initialize Model\n    model = CVAE(input_dim=2, cond_dim=2, latent_dim=LATENT_DIM, \n                 hidden_dim=HIDDEN_DIM, learning_rate=LEARNING_RATE)\n\n    # 3. Train Model\n    n_batches = N_SAMPLES // BATCH_SIZE\n    for epoch in range(EPOCHS):\n        permutation = np.random.permutation(N_SAMPLES)\n        c_train_norm_shuffled = c_train_norm[permutation]\n        x_train_norm_shuffled = x_train_norm[permutation]\n        \n        for i in range(n_batches):\n            start = i * BATCH_SIZE\n            end = start + BATCH_SIZE\n            c_batch = c_train_norm_shuffled[start:end]\n            x_batch = x_train_norm_shuffled[start:end]\n            \n            # Forward pass\n            x_hat_batch = model.forward(x_batch, c_batch)\n            \n            # Loss calculation\n            loss = model.loss(x_hat_batch)\n            \n            # Backward pass\n            model.backward(x_hat_batch)\n            \n            # Update parameters\n            model.update_params()\n\n    # 4. Evaluation\n    test_cases = [\n        (1.0, 0.0),                      # E in keV, b in m\n        (2.0, 1.0e-12),\n        (0.8, 2.0e-12),\n        (3.0, -1.5e-12),\n        (1.5, 2.0e-12),\n    ]\n\n    E_test_keV = np.array([c[0] for c in test_cases])\n    b_test_m = np.array([c[1] for c in test_cases])\n    \n    E_test_J = E_test_keV * 1000 * EV_TO_J\n    c_test = np.vstack([E_test_J, b_test_m]).T\n    \n    # Get ground truth for test cases\n    x_true_test = get_ground_truth(E_test_J, b_test_m)\n    \n    # Get model predictions for test cases\n    c_test_norm = c_normalizer.transform(c_test)\n    x_pred_test = model.predict(c_test_norm)\n    \n    # Calculate Euclidean errors\n    errors = np.linalg.norm(x_true_test - x_pred_test, axis=1)\n    \n    # 5. Print Final Output\n    print(f\"[{','.join(map(str, errors))}]\")\n\n\nsolve()\n```", "id": "2398395"}]}