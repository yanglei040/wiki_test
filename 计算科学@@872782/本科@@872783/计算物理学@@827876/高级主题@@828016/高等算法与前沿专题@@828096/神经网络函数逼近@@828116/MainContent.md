## 引言
近年来，[神经网](@entry_id:276355)络已成为[科学计算](@entry_id:143987)和数据分析中不可或缺的工具。其核心能力之一，也是其强大功能的基础，便是作为一种通用的**函数逼近器**。从拟合实验数据到模拟复杂的物理系统，[神经网](@entry_id:276355)络能够学习并表示任意复杂的输入-输出关系，为传统数值方法难以解决的问题开辟了新的途径。然而，如何有效地将这种强大的数据驱动能力与物理学等领域中根深蒂固的第一性原理相结合，仍然是一个充满挑战和机遇的研究前沿。本文旨在弥合这一差距，系统性地介绍[神经网](@entry_id:276355)络作为[函数逼近](@entry_id:141329)器的理论、应用与实践。

在接下来的内容中，我们将分三个章节展开探讨。首先，在“**原理与机制**”一章，我们将深入[神经网](@entry_id:276355)络的核心，揭示其作为[通用函数逼近器](@entry_id:637737)的数学基础，并探讨如何通过架构设计和损失函数将物理知识（如对称性和守恒律）融入模型。接着，在“**应用与交叉学科联系**”一章，我们将展示这些原理在物理、化学、生物学等多个学科中的实际应用，看[神经网](@entry_id:276355)络如何被用作动力学发现工具、[量子态](@entry_id:146142)的拟设以及[偏微分方程](@entry_id:141332)的求解器。最后，在“**动手实践**”部分，您将有机会通过具体的编程练习，亲身体验[维度灾难](@entry_id:143920)、[归纳偏置](@entry_id:137419)的力量以及物理知识驱动[神经网](@entry_id:276355)络的构建，从而将理论知识转化为实践技能。通过这趟学习之旅，您将掌握利用[神经网](@entry_id:276355)络解决复杂科学问题的基本思想和方法。

## 原理与机制

在介绍性章节之后，我们现在深入探讨[神经网](@entry_id:276355)络作为[函数逼近](@entry_id:141329)器的核心原理和工作机制。本章旨在揭示[神经网](@entry_id:276355)络何以能够学习复杂的函数关系，它们是如何通过训练来优化自身的，以及如何通过精心设计其架构和学习过程来融入基本的物理原理。我们将从基本概念出发，逐步过渡到在[科学计算](@entry_id:143987)中应用[神经网](@entry_id:276355)络所面临的前沿挑战和解决方案。

### 作为[通用函数逼近器](@entry_id:637737)的[神经网](@entry_id:276355)络

从根本上说，一个[神经网](@entry_id:276355)络定义了一个[参数化](@entry_id:272587)的函数。理解其作为函数逼近器的能力，最清晰的途径是考察一种相对简单的结构：带有一个隐藏层的[多层感知器](@entry_id:636847)（MLP）。给定一个 $d$ 维输入向量 $\boldsymbol{x} \in \mathbb{R}^d$，该网络的输出 $f(\boldsymbol{x}; \theta)$ 可以表示为：

$$
f(\boldsymbol{x};\theta) = \boldsymbol{v}^{\top}\sigma(\boldsymbol{W}^{\top}\boldsymbol{x} + \boldsymbol{b}) + c
$$

其中，$\theta = (\boldsymbol{W}, \boldsymbol{b}, \boldsymbol{v}, c)$ 是网络的所有可训练参数的集合。$\boldsymbol{W} \in \mathbb{R}^{d \times m}$ 和 $\boldsymbol{b} \in \mathbb{R}^m$ 分别是隐藏层的权重矩阵和偏置向量，$\boldsymbol{v} \in \mathbb{R}^m$ 和 $c \in \mathbb{R}$ 是输出层的权重和偏置。$m$ 是隐藏单元（或神经元）的数量。函数 $\sigma(\cdot)$ 是一个[非线性](@entry_id:637147)**激活函数**，例如 Sigmoid 函数 $\sigma(t) = \frac{1}{1+\exp(-t)}$，它按元素方式作用于其向量输入。

这个公式揭示了[神经网](@entry_id:276355)络的一个核心思想。隐藏层首先对输入 $\boldsymbol{x}$ 进行一系列仿射变换（$\boldsymbol{w}_j^{\top}\boldsymbol{x} + b_j$，其中 $\boldsymbol{w}_j$ 是 $\boldsymbol{W}$ 的第 $j$ 列），然后通过[非线性激活函数](@entry_id:635291) $\sigma$ 进行处理。这一过程可以被看作是创建了一组 $m$ 个新的、[非线性](@entry_id:637147)的**[基函数](@entry_id:170178)** $z_j(\boldsymbol{x}) = \sigma(\boldsymbol{w}_j^{\top}\boldsymbol{x} + b_j)$。网络的最终输出则是这些[基函数](@entry_id:170178)的线性组合，权重为 $\boldsymbol{v}$，偏置为 $c$。

因此，这个神经[网络模型](@entry_id:136956)可以被精确地重新表述为一种**[非线性](@entry_id:637147)[基函数](@entry_id:170178)回归**模型。与传统的[基函数](@entry_id:170178)方法（如[径向基函数](@entry_id:754004)网络或[傅里叶级数](@entry_id:139455)）不同，[神经网](@entry_id:276355)络的独特之处在于它不仅仅学习[基函数](@entry_id:170178)的[线性组合](@entry_id:154743)系数（$\boldsymbol{v}$ 和 $c$），它还通过调整隐藏层参数（$\boldsymbol{W}$ 和 $\boldsymbol{b}$）来**学习[基函数](@entry_id:170178)本身**。这种自适应学习[基函数](@entry_id:170178)的能力赋予了[神经网](@entry_id:276355)络巨大的灵活性 [@problem_id:2425193]。

这种灵活性得到了**[通用近似定理](@entry_id:146978)** (Universal Approximation Theorem) 的理论支持。该定理指出，只要隐藏单元的数量 $m$ 足够大，一个具有单个隐藏层和某种非多项式激活函数（如 Sigmoid）的[神经网](@entry_id:276355)络，可以以任意精度近似定义在 $\mathbb{R}^d$ 的紧凑[子集](@entry_id:261956)上的任何[连续函数](@entry_id:137361)。这一定理保证了该模型家族具有足够的**表达能力**，能够捕捉任意复杂的非线性关系，使其成为一种强大而灵活的[非线性回归](@entry_id:178880)工具 [@problem_id:2425193]。

然而，强大的[表达能力](@entry_id:149863)也伴随着挑战。其中最主要的就是**[维度灾难](@entry_id:143920)** (curse of dimensionality)。一般而言，为了在 $D$ 维空间中达到固定的逼近精度，所需的样本点数量会随着维度 $D$ 的增加而呈指数级增长。例如，在一项经验研究中，我们尝试使用一个具有固定容量（例如，固定数量的隐藏单元）的[神经网](@entry_id:276355)络去逼近一个[目标函数](@entry_id:267263) $f_D(\mathbf{x}) = \prod_{i=1}^{D} \sqrt{2}\sin(2\pi x_i)$。研究发现，为了达到固定的[均方根误差](@entry_id:170440)阈值，随着维度 $D$ 从 1 增加到 8，所需的最小训练样本数量急剧增加。当维度较高时（如 $D=8$），在合理的样本量范围内甚至可能无法达到预设的精度目标 [@problem_id:2417291]。尽管存在维度灾难，[神经网](@entry_id:276355)络在实践中仍然是处理高维问题的首选工具之一，这部分归功于它们能够发现并利用数据中潜在的低维结构。

### 核心机制：[基于梯度的优化](@entry_id:169228)

一旦定义了网络架构，接下来的任务就是**训练**网络，即寻找一组最优参数 $\theta^*$，使得网络的预测值 $\hat{y} = f(\boldsymbol{x}; \theta)$ 与真实目标值 $y$ 尽可能接近。这通常被构建为一个[优化问题](@entry_id:266749)，目标是最小化一个**损失函数** $L$。对于回归问题，最常用的[损失函数](@entry_id:634569)是**[均方误差](@entry_id:175403)** (Mean Squared Error, MSE)：

$$
L(\theta) = \frac{1}{N} \sum_{i=1}^{N} (y_i - f(\boldsymbol{x}_i; \theta))^2
$$

其中 $\\{(\boldsymbol{x}_i, y_i)\\}_{i=1}^{N}$ 是训练数据集。选择 MSE 作为损失函数有着深刻的概率论依据。如果我们假设数据是由一个确定性函数 $f(\boldsymbol{x}_i; \theta)$ 加上独立同分布的零均值[高斯噪声](@entry_id:260752) $\varepsilon_i \sim \mathcal{N}(0, \sigma^2)$ 生成的，即 $y_i = f(\boldsymbol{x}_i; \theta) + \varepsilon_i$，那么对参数 $\theta$ 进行**最大似然估计** (Maximum Likelihood Estimation, MLE) 等价于最小化 MSE [@problem_id:2425193]。

为了最小化损失函数 $L(\theta)$，最常用的方法是**[梯度下降](@entry_id:145942)** (Gradient Descent) 及其变体（如[随机梯度下降](@entry_id:139134), SGD）。其核心思想是迭代地更新参数，使之朝着损失函数梯度的反方向移动：

$$
\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta_t)
$$

其中 $\eta$ 是**[学习率](@entry_id:140210)**，它控制着每一步更新的幅度。

计算梯度 $\nabla_{\theta} L$ 是训练过程中的关键步骤，而**[反向传播](@entry_id:199535)** (backpropagation) 算法为此提供了一种高效的计算方案。反向传播本质上是应用**链式法则**来计算损失函数相对于网络中每一个参数的偏导数，它属于**[逆向模式自动微分](@entry_id:634526)** (reverse-mode automatic differentiation) 的一种实现。

让我们通过一个简单的两层网络来具体看这个过程。假设[网络结构](@entry_id:265673)如 [@problem_id:2154654] 中所述：输入为 $x$，经过 $z_1 = w_1 x + b_1$ 和 $a_1 = \sigma(z_1)$，最终输出 $\hat{y} = w_2 a_1 + b_2$。[损失函数](@entry_id:634569)为 $L = \frac{1}{2}(\hat{y} - y)^2$。我们想计算损失 $L$ 相对于权重 $w_1$ 的梯度 $\frac{\partial L}{\partial w_1}$。根据[链式法则](@entry_id:190743)，我们可以将这个梯度分解为一系列局部梯度的乘积，从输出层[反向传播](@entry_id:199535)至输入层：

$$
\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}
$$

每个局部梯度都可以根据其定义直接计算：
- $\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$
- $\frac{\partial \hat{y}}{\partial a_1} = w_2$
- $\frac{\partial a_1}{\partial z_1} = \sigma'(z_1) = \sigma(z_1)(1 - \sigma(z_1)) = a_1(1-a_1)$
- $\frac{\partial z_1}{\partial w_1} = x$

在一次**[前向传播](@entry_id:193086)** (forward pass) 中，我们给定输入 $x$ 和当前参数，计算出所有中间变量 ($z_1, a_1$) 和最终输出 $\hat{y}$。然后，在一次**[反向传播](@entry_id:199535)** (backward pass) 中，我们利用这些已计算出的值，从后向前依次计算并累积梯度。这个过程可以高效地计算出损失函数相对于网络所有参数的梯度 [@problem_id:2154654]。

值得注意的是，由于激活函数的[非线性](@entry_id:637147)，[神经网](@entry_id:276355)络的[损失函数](@entry_id:634569) $L(\theta)$ 通常是关于参数 $\theta$ 的**非[凸函数](@entry_id:143075)**。例如，在上述模型中，$w_1$ 和 $b_1$ 位于[非线性](@entry_id:637147)函数 $\sigma$ 的内部，导致 $f$ 对它们呈[非线性依赖](@entry_id:265776)。这意味着梯度下降法只能保证收敛到**局部最小值**，而不能保证找到全局最优解。这也是[神经网](@entry_id:276355)络训练充满挑战且结果依赖于参数初始化和优化器选择的原因之一 [@problem_id:2425193]。

### 架构设计：[归纳偏置](@entry_id:137419)的作用

[通用近似定理](@entry_id:146978)虽然保证了[神经网](@entry_id:276355)络的潜力，但在实践中，我们不可能使用无限大的网络。为了让模型在有限的数据上学习并**泛化**到未见过的数据，它必须做出一些关于函数真[实形式](@entry_id:193866)的假设。这些内置于模型架构或训练过程中的假设被称为**[归纳偏置](@entry_id:137419)** (inductive bias)。为特定问题选择正确的[归纳偏置](@entry_id:137419)是成功应用[神经网](@entry_id:276355)络的关键。

#### 激活函数与内在属性

[归纳偏置](@entry_id:137419)的一个基本来源是[激活函数](@entry_id:141784)的选择。不同的[激活函数](@entry_id:141784)赋予网络不同的内在属性，使其更善于学习特定类型的函数。

一个典型的例子是比较**[修正线性单元](@entry_id:636721)** (Rectified Linear Unit, **ReLU**) $\sigma(x) = \max\{0, x\}$ 和**[双曲正切函数](@entry_id:634307)** (**[tanh](@entry_id:636446)**) $\sigma(x) = \tanh(x)$。ReLU 是分段线性的，在 $x=0$ 处有一个“[拐点](@entry_id:144929)”（不可导），而 [tanh](@entry_id:636446) 是无限次可导的平滑函数 ($C^\infty$)。

考虑一个在经济学中常见的函数逼近问题：[消费-储蓄模型](@entry_id:141080)中的价值函数 $V(a)$，其中 $a$ 是资产。由于[借贷约束](@entry_id:137839)（例如 $a \ge 0$），价值函数在约束边界（$a=0$）处通常会表现出一个**拐折** (kink)，即[一阶导数](@entry_id:749425)不连续。如果我们的任务是逼近这个函数，ReLU 网络的[归纳偏置](@entry_id:137419)就与问题结构高度匹配。由 ReLU 单元构成的网络本身就是连续的[分段线性函数](@entry_id:273766)，它能够非常高效地、甚至精确地表示出拐折。例如，函数 $|x|$ 可以用两个 ReLU 单元精确表示为 $\max\{0, x\} + \max\{0, -x\}$。相比之下，由 [tanh](@entry_id:636446) 单元构成的网络必然是一个平滑函数，它无法产生真正的拐折。为了模仿一个尖锐的拐折，[tanh](@entry_id:636446) 网络必须使用大量的神经元来创造一个曲率极大的区域，这不仅效率低下，而且会“抹平”拐折，导致在约束点附近的边际价值（即导数）出现严重偏差 [@problem_id:2399859]。

因此，当待逼近的函数已知包含非光滑特征时，选择具有相应非光滑特性的[激活函数](@entry_id:141784)（如 ReLU）会提供更合适的[归纳偏置](@entry_id:137419)，从而以更少的参数获得更高的精度 [@problem_id:2399859]。

#### 架构中的对称性

在物理和工程问题中，一个更强大的[归纳偏置](@entry_id:137419)来源是系统的**对称性**。如果一个物理定律在某种变换（如平移、旋转、[置换](@entry_id:136432)）下保持不变，那么描述该系统的函数也必须遵守相应的对称性。将这些对称性直接构建到[神经网络架构](@entry_id:637524)中，可以极大地提高数据效率和模型的物理真实性。

**[置换不变性](@entry_id:753356) (Permutation Invariance)**：考虑一个由 $N$ 个相同原子组成的系统。根据量子力学的基本原理，这些全同粒子是不可区分的。这意味着系统的总势能 $E(\{\mathbf{R}_i\})$ 在交换任意两个原子 $i$ 和 $j$ 的标签（即交换它们的位置坐标 $\mathbf{R}_i$ 和 $\mathbf{R}_j$）后必须保持不变。一个物理上正确的**[神经网络势](@entry_id:752446)函数** (NNP) 必须满足这个[置换不变性](@entry_id:753356)。如果一个 NNP 的设计未能保证这一点（例如，通过使用不能保证置換不变性的原子描述符作为输入），那么它预测的能量和力将依赖于对原子的人为编号，这是完全不符合物理规律的。即使对于一个固定的原子编号，它所预测的[力场](@entry_id:147325)在数学上仍然是**保守的**（即可以写为某个[势能的梯度](@entry_id:173126)），但这个[势能面](@entry_id:147441)本身是错误的。这种模型在[分子动力学模拟](@entry_id:160737)中会导致荒谬的结果，比如两个处于对称位置的相同原子会感受到不同的力 [@problem_id:2456264]。

**[平移等变性](@entry_id:636340) (Translation Equivariance)**：另一个例子是[平移对称性](@entry_id:171614)。许多[偏微分方程](@entry_id:141332)（PDE）的解算子是平移不变的。例如，对于一个一维的平移不变[线性算子](@entry_id:149003) $L$，它对输入 $f(x)$ 的响应是 $u(x) = (G * f)(x)$，其中 $G$ 是系统的[格林函数](@entry_id:147802)（或脉冲响应），$*$ 代表卷积。这意味着，如果输入 $f(x)$ 被平移，输出 $u(x)$ 也会相应地平移。

**[卷积神经网络](@entry_id:178973)** (**CNN**) 的架构天生就具有平移**[等变性](@entry_id:636671)**。这是因为它使用**[卷积核](@entry_id:635097)**和**[权重共享](@entry_id:633885)**。一个卷积层将其输入与一个小的、可学习的核进行卷积，这个核在整个输入域上是共享的。无论一个特征出现在输入的哪个位置，都会被同一个核检测到，并在输出的相应位置产生响应。

我们可以通过一个求解一维椭圆型 PDE 的例子来生动地展示这种[归纳偏置](@entry_id:137419)的力量。考虑用[神经网](@entry_id:276355)络学习从[源项](@entry_id:269111) $f$到解 $u$ 的映射 $T: f \mapsto u$。如果我们只用一个训练样本——一个在 $x=0$ 处的[单位脉冲](@entry_id:272155)输入及其精确解——来训练网络。一个标准的 MLP（全连接网络）只会学会如何处理 $x=0$ 处的输入，当面对一个平移后的脉冲输入时，它会完全失效，因为它没有[平移不变性](@entry_id:195885)的[归纳偏置](@entry_id:137419)。相反，一个一维 CNN，当用同样的单个脉冲响应样本训练时，实际上是在学习这个线性平移不变系统的[格林函数](@entry_id:147802)（卷积核）。一旦学到了这个核，它就可以通过卷积运算，正确地预测系统对**任何**输入（包括平移后的脉冲或不同频率的[正弦波](@entry_id:274998)）的响应。CNN 凭借其正确的[归纳偏置](@entry_id:137419)，实现了从“一个样本”到“完全泛化”的飞跃 [@problem_id:2417315]。

#### 物理系统的局域性

对于大型物理系统，如[凝聚态物质](@entry_id:747660)或[大分子](@entry_id:150543)，另一个关键的物理原理是**局域性** (locality) 或 Walter Kohn 所提出的**“电子物质的短视性”** (nearsightedness of electronic matter)。在许多非金属系统中，一个原子的电子结构和它所受的力主要由其近邻环境决定，而与远处的原子关系不大。这种影响随距离呈指数衰减。

这个物理原理为高效的 NNP 架构提供了基本 justification。现代 NNP 设计，如 [Behler-Parrinello](@entry_id:177243) 网络，将总[能量分解](@entry_id:193582)为各个原子能量贡献的总和：

$$
\hat{E}_\theta(\mathbf{R}_{1:N})=\sum_{I=1}^{N}\varepsilon_\theta(\mathcal{D}_I)
$$

其中，原子能量 $\varepsilon_\theta$ 是一个[神经网](@entry_id:276355)络的输出，其输入 $\mathcal{D}_I$ 是一个**描述符**，该描述符仅编码了原子 $I$ 在一个固定的**[截断半径](@entry_id:136708)** $r_c$ 内的局部化学环境。由于[局域性原理](@entry_id:753741)，忽略 $r_c$ 之外的原子所引入的误差是可控的，并且会随着 $r_c$ 的增大而指数级减小 [@problem_id:2908380]。

这种基于局域性的分解不仅编码了正确的物理，还带来了巨大的计算优势。由于每个原子的能量计算只依赖于其邻近的、数量近似恒定的原子，总能量和力的计算复杂度与系统中的原子总数 $N$呈线性关系，即 $\mathcal{O}(N)$。这使得使用 NNP 对包含数百万个原子的系统进行大规模[分子动力学模拟](@entry_id:160737)成为可能 [@problem_id:2908380]。

在实际实现中，为了保证能量面对原子坐标的[可微性](@entry_id:140863)，从而得到连续的力，截断不能是突兀的。通常会采用一个**平滑截断函数** $f_c(r)$，它在接近 $r_c$ 时平滑地将相互作用过渡到零，并保证其导数在 $r_c$ 处也为零。这可以避免原子穿越截断边界时[力场](@entry_id:147325)中出现不合物理的突变 [@problem_id:2908380]。

### 物理知识驱动的[神经网](@entry_id:276355)络

除了在架构中嵌入[归纳偏置](@entry_id:137419)，我们还可以通过设计[损失函数](@entry_id:634569)来将物理知识注入[神经网](@entry_id:276355)络，这一思想催生了**物理知识驱动的[神经网](@entry_id:276355)络** (Physics-Informed Neural Networks, **[PINNs](@entry_id:145229)**)。

#### 在[损失函数](@entry_id:634569)中编码物理定律

PINNs 的核心思想是训练一个[神经网](@entry_id:276355)络 $u_\theta(x,t)$，使其不仅拟合已知的观测数据，还要满足一个给定的[偏微分方程](@entry_id:141332)。这是通过在损失函数中加入一个惩罚项来实现的，该惩罚项度量了网络输出在多大程度上违反了该 PDE。

以一维粘性 Burgers 方程为例：
$$
\mathcal{N}[u] \equiv u_t + u u_x - \nu u_{xx} = 0
$$
PINN 的总损失函数通常由几部分组成：
$$
\mathcal{L}_{\text{total}} = w_{\text{data}}\mathcal{L}_{\text{data}} + w_{\text{pde}}\mathcal{L}_{\text{pde}} + w_{\text{bc}}\mathcal{L}_{\text{bc}} + w_{\text{ic}}\mathcal{L}_{\text{ic}}
$$
- $\mathcal{L}_{\text{data}}$ 度量网络在已知数据点上的拟合误差。
- $\mathcal{L}_{\text{pde}}$ 是 PDE 残差的范数，在时空域内的一组**[配置点](@entry_id:169000)** (collocation points) 上计算，例如 $\mathcal{L}_{\text{pde}} = \frac{1}{N_p}\sum_i (\mathcal{N}[u_\theta(x_i, t_i)])^2$。计算这个残差需要通过[自动微分](@entry_id:144512)精确求得 $u_\theta$ 对其输入（$x$ 和 $t$）的导数。
- $\mathcal{L}_{\text{bc}}$ 和 $\mathcal{L}_{\text{ic}}$ 分别惩罚网络对边界条件和[初始条件](@entry_id:152863)的违反。

通过最小化这个总[损失函数](@entry_id:634569)，PINN 被引导去寻找一个同时满足数据约束和物理定律的函数。

我们甚至可以更进一步，将更精细的物理知识（如对称性）编码到损失函数中。例如，Burgers 方程具有伽利略变换不变性。我们可以为此添加一个**对称性损失项** $\mathcal{L}_{\text{sym}}$，它惩罚网络对该对称性的违反程度 [@problem_id:2417275]。例如，对于一个变换后的解 $u'(x,t) = u(x-ct, t) + c$，我们可以定义 $\mathcal{L}_{\text{sym}}(c) = \sum_i (u_\theta(x_i, t_i) - u'_\theta(x_i, t_i))^2$。这鼓励网络学习到的解空间本身也具有这种对称性，从而可能改善其泛化能力。

#### 挑战与前沿：谱偏置与训练动力学

尽管 [PINNs](@entry_id:145229) 功能强大，但其训练过程并非一帆风顺，其中一个主要障碍是**谱偏置** (spectral bias)。标准的、使用 [tanh](@entry_id:636446) 或 sigmoid 等激活函数的[神经网](@entry_id:276355)络，在通过[梯度下降](@entry_id:145942)训练时，会优先学习目标函数的低频分量，而学习高频分量则困难得多。

当试图求解某些 PDE（如高波数下的[亥姆霍兹方程](@entry_id:149977) $u'' + k^2 u = 0$）时，谱偏置会成为一个严重问题。这[类方程](@entry_id:144428)的解是高频[振荡](@entry_id:267781)的正弦函数。然而，对于[神经网络优化](@entry_id:633904)器来说，平凡解 $u(x)=0$ 是一个非常吸引人的（也是有效的）零频解，它完美地满足了方程和边界条件，其损失值为零。因此，训练过程很容易陷入这个[平凡解](@entry_id:155162)，而无法发现正确的高频[振荡](@entry_id:267781)解 [@problem_id:2411070]。

克服谱偏置是当前研究的一个活跃领域。有几种策略被证明是有效的：
1.  **提高采样密度**：根据奈奎斯特-香non采样定理，为了让优化器“看到”高频[振荡](@entry_id:267781)，[配置点](@entry_id:169000)的采样密度必须足够高，即每个波长内至少要有两个以上的点。否则，高频残差可能会在稀疏的[配置点](@entry_id:169000)上发生**[混叠](@entry_id:146322)**，表现为零或低频信号，从而误导优化器 [@problem_id:2411070]。
2.  **改进网络架构**：可以使用能更好地表示高频函数的架构。例如，在将输入 $x$ 馈入网络之前，先通过一组固定的傅里葉特征（如 $[\sin(kx), \cos(kx)]$）进行映射。或者，使用本身就具有周期性的[激活函数](@entry_id:141784)，如正弦函数（例如在 SIRENs 网络中），这为模型提供了更适合学习[振荡](@entry_id:267781)函数的[归纳偏置](@entry_id:137419) [@problem_id:2411070]。

除了谱偏置，PINN 训练的另一个 subtleties 在于[损失函数](@entry_id:634569)的具体形式。PDE 可以用**强形式**（直接写出微分算子）或**[弱形式](@entry_id:142897)**（[变分形式](@entry_id:166033)，通过[分部积分](@entry_id:136350)降低[微分](@entry_id:158718)阶数）来表述。例如，对于一维弹性杆问题，强形式涉及[位移场](@entry_id:141476) $u(x)$ 的[二阶导数](@entry_id:144508)，而[弱形式](@entry_id:142897)只涉及一阶导数。

当使用[随机梯度下降](@entry_id:139134)进行训练时，基于强形式的**[配置点](@entry_id:169000)法**（在随机点上评估高阶导数残差）所产生的随机梯度，其[方差](@entry_id:200758)通常远高于基于弱形式的**积分法**（在随机单元上通过[数值积分](@entry_id:136578)评估低阶导数）。这是因为[神经网](@entry_id:276355)络的高阶导数往往比低阶导数更具[振荡](@entry_id:267781)性且数值更大。[弱形式](@entry_id:142897)通[过积分](@entry_id:753033)和降低[微分](@entry_id:158718)阶数，有效地平滑了被采样的量，从而降低了梯度的[方差](@entry_id:200758)。梯度[方差](@entry_id:200758)的降低使得训练过程更加稳定，允许使用更大的[学习率](@entry_id:140210)，从而实现更快的收敛和更低的最终误差 [@problem_id:2668916]。这揭示了将经典[数值分析](@entry_id:142637)（如有限元法）的思想与[神经网](@entry_id:276355)络训练相结合的深刻价值。