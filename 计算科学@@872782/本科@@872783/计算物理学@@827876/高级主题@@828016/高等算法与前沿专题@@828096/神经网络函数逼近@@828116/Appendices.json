{"hands_on_practices": [{"introduction": "神经网络的架构可以编码关于问题的先验知识，这被称为归纳偏置。物理系统中一个常见的对称性是平移不变性，而卷积神经网络（CNN）中的卷积层天生就适合处理这种对称性。本练习将通过解决一个具有平移不变性的偏微分方程，来展示使用 CNN 相对于通用多层感知机（MLP）在泛化能力上的显著优势 [@problem_id:2417315]。", "problem": "考虑一个在周期性域上的一维、平移不变的椭圆偏微分方程（PDE），其形式为 $-u''(x) + a\\,u(x) = f(x)$，边界条件为在 $[0,1)$ 上的周期性边界条件。在 $N$ 个点的均匀网格上对问题进行离散化，点间距为 $h = 1/N$。标准的中心二阶差分方法为离散解向量 $u \\in \\mathbb{R}^N$ 和强迫向量 $f \\in \\mathbb{R}^N$ 产生一个循环线性系统。该离散算子可由离散傅里叶变换对角化：每个由整数 $k \\in \\{0,1,\\dots,N-1\\}$ 索引的模态都有一个特征值 $\\lambda_k = a + \\dfrac{4}{h^2}\\sin^2\\!\\left(\\dfrac{\\pi k}{N}\\right)$。因此，通过将 $f$ 的傅里叶系数除以 $\\lambda_k$ 并变换回物理空间，可以获得精确的离散解。\n\n在此问题中，您将从基本原理出发，实现两个解算子 $T: f \\mapsto u$ 的神经网络代理模型。这两个模型仅使用单个输入输出样本进行训练，并比较它们的性能，以展示归纳偏置对平移不变算子的影响：\n\n- 一个没有隐藏层的全连接多层感知器（MLP）（即单个线性映射），由一个稠密矩阵 $W \\in \\mathbb{R}^{N \\times N}$ 表示，其作用方式为 $u_{\\text{MLP}} = W f$。\n\n- 一个一维循环卷积神经网络（CNN），带有一个线性卷积层，其核的长度为奇数 $K$。该网络由一个核向量 $w \\in \\mathbb{R}^{K}$ 表示，通过循环卷积作用：$u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N]$，其中 $m = (K-1)/2$。\n\n两个模型都将通过最小化模型输出与上述傅里叶对角化方法产生的精确离散解之间的均方误差损失进行训练。使用固定学习率和固定迭代次数的简单梯度下降法。将所有可训练参数初始化为零。任何三角表达式中的角度都必须是弧度。\n\n您的实现必须遵循以下固定的、科学上一致的参数选择和数据集定义：\n\n- 网格尺寸：$N = 64$，间距 $h = 1/N$。\n\n- PDE 参数：$a = 1$。\n\n- CNN 核尺寸：$K = 31$（因此 $m = 15$）。\n\n- 训练集：单个训练样本对 $(f^{\\text{train}}, u^{\\text{train}})$，其中 $f^{\\text{train}}$ 是在索引 $0$ 处的单位脉冲（即，$f^{\\text{train}}[0] = 1$ 且对于所有 $i \\neq 0$，$f^{\\text{train}}[i] = 0$），$u^{\\text{train}}$ 是通过傅里叶对角化计算出的对应于该强迫项的精确离散解。\n\n- 优化细节：对于 MLP，使用固定步长的梯度下降法，迭代次数 $T_{\\text{MLP}} = 2000$，学习率 $\\eta_{\\text{MLP}} = 0.2$；对于 CNN，迭代次数 $T_{\\text{CNN}} = 2000$，学习率 $\\eta_{\\text{CNN}} = 0.05$。对于损失为 $L = \\dfrac{1}{N} \\|W f - u\\|_2^2$ 的 MLP，更新规则为 $W \\leftarrow W - \\eta_{\\text{MLP}} \\dfrac{2}{N} (W f - u) f^\\top$。对于损失为 $L = \\dfrac{1}{N} \\|w \\circledast f - u\\|_2^2$ 的 CNN，其中 $\\circledast$ 表示如上定义的循环卷积，更新规则为 $w[s+m] \\leftarrow w[s+m] - \\eta_{\\text{CNN}} \\dfrac{2}{N} \\sum_{i=0}^{N-1} \\big( (w \\circledast f)[i] - u[i] \\big)\\, f[(i - s) \\bmod N]$，适用于所有 $s \\in \\{-m,\\dots,m\\}$。\n\n设计一个测试套件，使用四个固定的强迫项 $f^{(j)}$ 及其精确离散解 $u^{(j)}$ 来评估平移泛化能力和频率响应：\n\n- 情况 1：$f^{(1)}$ 是在索引 $0$ 处的单位脉冲。\n\n- 情况 2：$f^{(2)}$ 是在索引 $16$ 处的单位脉冲。\n\n- 情况 3：对于所有 $i \\in \\{0,\\dots,N-1\\}$，$f^{(3)}[i] = \\cos\\!\\left(2 \\pi \\cdot 4 \\cdot \\dfrac{i}{N}\\right)$（角度为弧度）。\n\n- 情况 4：对于所有 $i \\in \\{0,\\dots,N-1\\}$，$f^{(4)}[i] = 1$。\n\n对于每种情况，计算 MLP 和 CNN 各自模型相对于精确解的相对 $\\ell_2$ 误差，定义为 $\\varepsilon_{\\text{model}} = \\dfrac{\\|u_{\\text{model}} - u^{(j)}\\|_2}{\\|u^{(j)}\\|_2}$。\n\n您的程序必须实现以上所有内容，使用指定的超参数，并产生单行输出，包含以下顺序的八个浮点数结果：$[\\varepsilon_{\\text{MLP}}^{(1)}, \\varepsilon_{\\text{CNN}}^{(1)}, \\varepsilon_{\\text{MLP}}^{(2)}, \\varepsilon_{\\text{CNN}}^{(2)}, \\varepsilon_{\\text{MLP}}^{(3)}, \\varepsilon_{\\text{CNN}}^{(3)}, \\varepsilon_{\\text{MLP}}^{(4)}, \\varepsilon_{\\text{CNN}}^{(4)}]$。该行必须以方括号括起来的逗号分隔列表形式打印，不含任何额外文本。\n\n在所选的 PDE 无量纲化方案下，所有量都是无量纲的。确保您的代码是一个完整的、可运行的程序，不需要任何输入，并且只使用允许的库。最终答案必须以指定的单行格式表示为浮点数。", "solution": "所给问题是有效的。它在科学上基于数值分析和计算物理，具体涉及偏微分方程的数值解以及机器学习方法作为代理模型的现代应用。该问题是适定的，所有参数、方法和目标都得到了清晰明确的定义。它构成了一个标准的、尽管简化的科学机器学习练习，旨在展示神经网络架构中归纳偏置的概念。因此，我将继续提供完整的解决方案。\n\n该问题要求比较两种神经网络模型——多层感知器（MLP）和卷积神经网络（CNN）——在学习一维椭圆偏微分方程解算子方面的表现。分析将分四个阶段进行：首先，建立 PDE 的精确数值解作为真实值；其次，构建和训练 MLP 模型；第三，构建和训练 CNN 模型；最后，在一系列测试案例上评估两个训练好的模型，以评估它们的泛化能力。\n\n**1. 控制方程及其精确离散解**\n\n物理系统由定义在域 $x \\in [0,1)$ 上的一维二阶椭圆偏微分方程描述，带有周期性边界条件：\n$$ -u''(x) + a\\,u(x) = f(x) $$\n这里，$u(x)$ 是解场，$f(x)$ 是源项或强迫项，$a$ 是一个正常数，给定为 $a=1$。在由 $N=64$ 个点 $x_i = i h$（$i=0, \\dots, N-1$）组成的均匀网格上离散化此方程，间距为 $h=1/N$，我们使用标准的二阶精度中心有限差分近似来处理二阶导数：\n$$ -u''(x_i) \\approx \\frac{-u_{i-1} + 2u_i - u_{i+1}}{h^2} $$\n其中 $u_i = u(x_i)$。将此应用于 PDE，得到关于离散解向量 $\\mathbf{u} \\in \\mathbb{R}^N$ 的线性方程组。由于周期性边界条件，所得的系统矩阵是一个循环矩阵。循环矩阵的一个基本性质是它们可以被离散傅里叶变换（DFT）对角化。这意味着如果我们将方程转换到傅里叶域，微分算子就变成一个简单的乘法。\n\n令 $\\hat{\\mathbf{u}}$ 和 $\\hat{\\mathbf{f}}$ 分别为 $\\mathbf{u}$ 和 $\\mathbf{f}$ 的 DFT。在傅里叶域中的线性系统为：\n$$ \\lambda_k \\hat{u}_k = \\hat{f}_k $$\n对于每个傅里叶模态 $k \\in \\{0, 1, \\dots, N-1\\}$。离散算子的特征值 $\\lambda_k$ 由问题陈述给出：\n$$ \\lambda_k = a + \\frac{4}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right) $$\n当 $a=1$ 且 $N=64$ 时，所有特征值 $\\lambda_k$ 都严格为正，这保证了算子是可逆的，并且对于任何强迫项 $\\mathbf{f}$ 都存在唯一解。精确的离散解 $\\mathbf{u}$ 通过在傅里叶域中求解 $\\hat{u}_k = \\hat{f}_k / \\lambda_k$ 然后应用逆离散傅里叶变换返回到物理域来找到：\n$$ \\mathbf{u} = \\text{IDFT}\\left(\\frac{\\text{DFT}(\\mathbf{f})}{\\mathbf{\\lambda}}\\right) $$\n该方法为我们的神经网络代理模型提供了用于训练和测试的真实值数据。\n\n**2. 多层感知器（MLP）模型**\n\n第一个模型是一个没有隐藏层的全连接网络，这等同于由一个稠密矩阵 $W \\in \\mathbb{R}^{N \\times N}$ 表示的单一线性变换。该模型的预测是：\n$$ \\mathbf{u}_{\\text{MLP}} = W \\mathbf{f} $$\n该模型是完全通用的，没有对解算子预设任何结构。它有 $N^2 = 64^2 = 4096$ 个可训练参数。\n\n该模型在单个数据对 $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$ 上进行训练，其中 $\\mathbf{f}^{\\text{train}}$ 是索引为 $0$ 的单位脉冲（即 $\\mathbf{f}^{\\text{train}} = \\mathbf{e}_0$，第一个标准基向量）。训练使用梯度下降法来最小化均方误差损失 $L = \\frac{1}{N} \\|\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}\\|_2^2$。损失函数关于权重矩阵 $W$ 的梯度是 $\\nabla_W L = \\frac{2}{N} (\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}) (\\mathbf{f}^{\\text{train}})^\\top$。给定 $\\mathbf{f}^{\\text{train}} = \\mathbf{e}_0$，外积 $(\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}}) \\mathbf{e}_0^\\top$ 是一个矩阵，其第一列是误差向量 $(\\mathbf{u}_{\\text{MLP}} - \\mathbf{u}^{\\text{train}})$，所有其他列均为零。\n\n至关重要的是，由于权重初始化为 $W=0$ 且训练输入始终为 $\\mathbf{e}_0$，梯度仅对 $W$ 的第一列非零。因此，在训练过程中只有 $W$ 的第一列会被更新。MLP 实际上学会了将输入向量的第一个分量 $f_0$ 映射到一个输出向量，并忽略所有其他输入分量 $f_i$（对于 $i>0$）。这种架构缺乏对平移不变性的*归纳偏置*，而这是底层物理系统的一个关键属性。因此，我们预测它将无法泛化到平移的或具有不同频率内容的输入。\n\n**3. 卷积神经网络（CNN）模型**\n\n第二个模型是一个一维循环卷积层。其预测由输入 $\\mathbf{f}$ 与一个可学习的核 $\\mathbf{w} \\in \\mathbb{R}^K$ 的循环卷积给出：\n$$ \\mathbf{u}_{\\text{CNN}} = \\mathbf{w} \\circledast \\mathbf{f} $$\n核尺寸为 $K=31$。此操作定义为：\n$$ u_{\\text{CNN}}[i] = \\sum_{s=-m}^{m} w[s+m]\\; f[(i - s) \\bmod N] $$\n其中 $m = (K-1)/2 = 15$。CNN 模型只有 $K=31$ 个可训练参数。这种架构明确地内置了平移不变性的假设，因为卷积是对平移等变的运算。对于给定的线性平移不变（LTI）物理算子，这是正确的归纳偏置。LTI 系统对任意输入的响应是输入与系统的脉冲响应（格林函数）的卷积。\n\nCNN 在相同的脉冲响应对 $(\\mathbf{f}^{\\text{train}}, \\mathbf{u}^{\\text{train}})$ 上进行训练。由于用核与一个 δ 脉冲进行卷积会得到核本身（经过适当的填充或截断），训练过程实际上是教会核 $\\mathbf{w}$ 成为系统真实脉冲响应 $\\mathbf{u}^{\\text{train}}$ 的一个近似。因为 CNN 已经学习了底层 LTI 算子核的一个近似，预计它能够正确地泛化到任何输入，包括平移的脉冲和不同频率的输入。\n\n**4. 模型评估与预期结果**\n\n两种模型都在四个测试案例上进行评估：\n1.  索引为 0 的训练脉冲：测试拟合训练数据的能力。\n2.  索引为 16 的平移脉冲：测试对平移的泛化能力。\n3.  低频余弦波（$\\cos(2\\pi \\cdot 4x)$）：测试对不同频率的泛化能力。\n4.  常数输入（零频波）：测试对最低频率模态的泛化能力。\n\n预期的结果是清晰地展示归纳偏置的力量。缺乏此偏置的 MLP 应该只在第一个测试案例（其训练数据）上表现良好，而在所有其他案例上失败。拥有正确的平移不变性偏置的 CNN 应该在所有四个测试案例上都表现良好，展示出从单个训练样本中获得的鲁棒泛化能力。数值结果将通过相对 $\\ell_2$ 误差来量化这种性能差异。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.fft\n\ndef solve():\n    \"\"\"\n    Implements and compares MLP and CNN surrogates for a 1D elliptic PDE.\n    \"\"\"\n    # -- 1. Define constants and problem parameters --\n    N = 64\n    h = 1.0 / N\n    a = 1.0\n    K = 31\n    m = (K - 1) // 2\n    T_MLP = 2000\n    eta_MLP = 0.2\n    T_CNN = 2000\n    eta_CNN = 0.05\n\n    # -- 2. Define helper functions --\n    def exact_solver(f_vec):\n        \"\"\"\n        Computes the exact discrete solution using Fourier diagonalization.\n        \"\"\"\n        k = np.arange(N)\n        lambda_k = a + (4 / h**2) * np.sin(np.pi * k / N)**2\n        # Handle the case where a mode might be numerically zero, though not here.\n        # Add a small epsilon to the denominator for stability if needed.\n        f_hat = scipy.fft.fft(f_vec)\n        u_hat = f_hat / lambda_k\n        u_vec = scipy.fft.ifft(u_hat)\n        # The solution for a real forcing must be real.\n        return u_vec.real\n\n    def circular_conv(w_kernel, f_vec):\n        \"\"\"\n        Performs 1D circular convolution.\n        w_kernel has length K, f_vec has length N.\n        \"\"\"\n        y = np.zeros_like(f_vec, dtype=np.float64)\n        s_vals = np.arange(-m, m + 1)\n        for s_idx, s in enumerate(s_vals):\n            y += w_kernel[s_idx] * np.roll(f_vec, s)\n        return y\n\n    # -- 3. Generate training data --\n    f_train = np.zeros(N)\n    f_train[0] = 1.0\n    u_train = exact_solver(f_train)\n\n    # -- 4. Train the MLP model --\n    W = np.zeros((N, N))\n    for _ in range(T_MLP):\n        u_pred_mlp = W @ f_train\n        error_mlp = u_pred_mlp - u_train\n        # Gradient update for L = (1/N) * ||Wf - u||^2\n        grad_W = (2.0 / N) * np.outer(error_mlp, f_train)\n        W -= eta_MLP * grad_W\n\n    # -- 5. Train the CNN model --\n    w = np.zeros(K)\n    for _ in range(T_CNN):\n        u_pred_cnn = circular_conv(w, f_train)\n        error_cnn = u_pred_cnn - u_train\n        grad_w = np.zeros(K)\n        s_vals = np.arange(-m, m + 1)\n        \n        # Gradient for L = (1/N) * ||w*f - u||^2\n        # dL/dw_j = (2/N) * sum_i ( (w*f)_i - u_i ) * f_{i-j_s}\n        # where j_s is the shift corresponding to kernel element j.\n        for s_idx, s in enumerate(s_vals):\n            # The sum is the cross-correlation of the error and the input signal\n            grad_w[s_idx] = np.dot(error_cnn, np.roll(f_train, s))\n        \n        grad_w *= (2.0 / N)\n        w -= eta_CNN * grad_w\n\n    # -- 6. Define test cases and evaluate models --\n    # Case 1: Unit impulse at index 0 (training case)\n    f1 = np.zeros(N); f1[0] = 1.0\n    # Case 2: Unit impulse at index 16\n    f2 = np.zeros(N); f2[16] = 1.0\n    # Case 3: Cosine wave\n    i_indices = np.arange(N)\n    f3 = np.cos(2 * np.pi * 4 * i_indices / N)\n    # Case 4: Constant function (zero-frequency wave)\n    f4 = np.ones(N)\n\n    test_forcings = [f1, f2, f3, f4]\n    test_solutions = [exact_solver(f) for f in test_forcings]\n    \n    results = []\n    for f_test, u_exact in zip(test_forcings, test_solutions):\n        # Prevent division by zero, although not expected here\n        norm_u_exact = np.linalg.norm(u_exact)\n        if norm_u_exact == 0:\n            norm_u_exact = 1.0\n\n        # MLP prediction and error\n        u_mlp = W @ f_test\n        err_mlp = np.linalg.norm(u_mlp - u_exact) / norm_u_exact\n        results.append(err_mlp)\n        \n        # CNN prediction and error\n        u_cnn = circular_conv(w, f_test)\n        err_cnn = np.linalg.norm(u_cnn - u_exact) / norm_u_exact\n        results.append(err_cnn)\n\n    # -- 7. Print final results in the specified format --\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2417315"}, {"introduction": "除了在架构中编码先验知识，我们还可以通过在损失函数中惩罚对物理定律的违背来直接强制执行这些定律，这正是物理信息神经网络（PINN）的核心思想。本练习将指导您为粘性伯格斯方程（viscous Burgers' equation）构建并计算一个损失函数，该损失函数不仅包括对偏微分方程（PDE）残差的惩罚，还包含一个针对其已知李对称性（伽利略变换）的项 [@problem_id:2417275]。", "problem": "要求您形式化并计算一个一维粘性 Burgers 方程的物理信息神经网络损失，该损失显式地包含一个项，以促进方程在已知李对称性下的等变性。您的程序必须实现下文给出的精确定义，并为指定的测试套件返回总损失值。\n\n目标偏微分方程是时空域 $\\{(x,t) \\mid x \\in [-1,1],\\ t \\in [0,1]\\}$ 上的一维粘性 Burgers 方程：\n$$\nu_t(x,t) + u(x,t)\\,u_x(x,t) - \\nu\\,u_{xx}(x,t) = 0,\n$$\n其中 $\\nu > 0$ 是运动粘度。考虑由该方程与初始条件\n$$\nu(x,0) = 0 \\quad \\text{for all } x \\in [-1,1],\n$$\n以及狄利克雷边界条件\n$$\nu(-1,t) = 0, \\quad u(1,t) = 0 \\quad \\text{for all } t \\in [0,1]\n$$\n组成的柯西问题。\n\n粘性 Burgers 方程的一个已知李对称性是伽利略变换：对于任意常数 $c \\in \\mathbb{R}$，如果 $u(x,t)$ 是一个解，那么\n$$\nu'(x,t) = u(x - c\\,t, t) + c\n$$\n也是一个解。为了促使神经网络近似尊重这种对称性，引入一个对称性一致性惩罚项，该惩罚项通过在一组预设点 $(x,t)$ 上对此等变关系的平方偏差进行平均来定义。\n\n您将使用一个固定的单隐层神经网络近似 $u_\\theta(x,t)$，其激活函数为双曲正切函数（其中 $\\tanh$ 表示双曲正切）：\n$$\nu_\\theta(x,t) = W_2 \\,\\tanh\\!\\bigl(W_1 \\,[x,\\ t]^\\top + b_1 \\bigr) + b_2,\n$$\n其中 $W_1 \\in \\mathbb{R}^{3 \\times 2}$，$b_1 \\in \\mathbb{R}^3$，$W_2 \\in \\mathbb{R}^{1 \\times 3}$，以及 $b_2 \\in \\mathbb{R}$。所有导数 $u_t(x,t)$、$u_x(x,t)$ 和 $u_{xx}(x,t)$ 都应通过对该参数化函数对 $x$ 和 $t$ 求导来精确计算。\n\n对神经网络使用以下固定的参数值：\n- $W_1 = \\begin{bmatrix} 1.2  -0.7 \\\\ -0.3  0.9 \\\\ 0.5  1.1 \\end{bmatrix}$,\n- $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$,\n- $W_2 = \\begin{bmatrix} 0.8  -0.5  0.3 \\end{bmatrix}$,\n- $b_2 = -0.1$.\n\n定义以下均方损失分量：\n- 偏微分方程残差损失\n$$\n\\mathcal{L}_{\\mathrm{pde}} = \\frac{1}{N_{\\mathrm{p}}} \\sum_{(x,t) \\in \\mathcal{P}} \\bigl( u_t(x,t) + u(x,t)\\,u_x(x,t) - \\nu\\,u_{xx}(x,t) \\bigr)^2,\n$$\n其中偏微分方程的配置点集为\n$$\n\\mathcal{P} = \\{(-0.8,\\,0.2),\\ (0,\\,0.5),\\ (0.9,\\,0.9),\\ (0.5,\\,0.1)\\},\n$$\n且 $N_{\\mathrm{p}} = 4$。\n- 初始条件损失\n$$\n\\mathcal{L}_{\\mathrm{ic}} = \\frac{1}{N_{\\mathrm{ic}}} \\sum_{x \\in \\mathcal{X}_{\\mathrm{ic}}} \\bigl(u(x,0)\\bigr)^2,\n$$\n其中\n$$\n\\mathcal{X}_{\\mathrm{ic}} = \\{-1,\\ -0.3,\\ 0.2,\\ 1\\},\n$$\n且 $N_{\\mathrm{ic}} = 4$。\n- 边界条件损失\n$$\n\\mathcal{L}_{\\mathrm{bc}} = \\frac{1}{N_{\\mathrm{b}}} \\sum_{t \\in \\mathcal{T}_{\\mathrm{bc}}} \\Bigl( \\bigl(u(-1,t)\\bigr)^2 + \\bigl(u(1,t)\\bigr)^2 \\Bigr),\n$$\n其中\n$$\n\\mathcal{T}_{\\mathrm{bc}} = \\{0,\\ 0.4,\\ 1\\},\n$$\n且 $N_{\\mathrm{b}} = 6$。\n- 对于给定的变换参数 $c \\in \\mathbb{R}$ 的对称性一致性损失\n$$\n\\mathcal{L}_{\\mathrm{sym}}(c) = \\frac{1}{N_{\\mathrm{s}}} \\sum_{(x,t) \\in \\mathcal{S}} \\Bigl( u(x,t) - \\bigl(u(x - c\\,t, t) + c\\bigr) \\Bigr)^2,\n$$\n其中\n$$\n\\mathcal{S} = \\{(-0.6,\\,0.3),\\ (0.2,\\,0.7),\\ (0.95,\\,0.5),\\ (-1,\\,1)\\},\n$$\n且 $N_{\\mathrm{s}} = 4$。\n\n使用正常数权重\n$$\nw_{\\mathrm{pde}} = 1,\\quad w_{\\mathrm{ic}} = 10,\\quad w_{\\mathrm{bc}} = 10,\\quad w_{\\mathrm{sym}} = 1,\n$$\n定义总损失\n$$\n\\mathcal{L}_{\\mathrm{total}}(\\nu,c) = w_{\\mathrm{pde}}\\,\\mathcal{L}_{\\mathrm{pde}} + w_{\\mathrm{ic}}\\,\\mathcal{L}_{\\mathrm{ic}} + w_{\\mathrm{bc}}\\,\\mathcal{L}_{\\mathrm{bc}} + w_{\\mathrm{sym}}\\,\\mathcal{L}_{\\mathrm{sym}}(c).\n$$\n\n您的程序必须使用相同的神经网络和配置点集，为以下测试套件中的每一项计算并报告 $\\mathcal{L}_{\\mathrm{total}}(\\nu,c)$：\n- 测试 $1$：$(\\nu,c) = (0.1,\\,0)$，\n- 测试 $2$：$(\\nu,c) = (0.1,\\,0.5)$，\n- 测试 $3$：$(\\nu,c) = (0.01,\\,-0.75)$。\n\n最终输出格式：您的程序应生成单行输出，其中包含测试 $1$–$3$ 的三个总损失值，形式为用方括号括起来的逗号分隔列表，每个值四舍五入到六位小数（例如，$[0.123456,0.234567,0.345678]$）。本问题不涉及角度。本问题不涉及物理单位；所有量均为无量纲量。", "solution": "对问题陈述进行验证。\n\n### 步骤1：提取给定条件\n\n- **控制方程**：一维粘性 Burgers 方程，\n  $u_t(x,t) + u(x,t)\\,u_x(x,t) - \\nu\\,u_{xx}(x,t) = 0$，对于 $(x,t) \\in [-1,1] \\times [0,1]$ 和粘度 $\\nu > 0$。\n- **初始条件**：对于 $x \\in [-1,1]$，$u(x,0) = 0$。\n- **边界条件**：对于 $t \\in [0,1]$，$u(-1,t) = 0$ 且 $u(1,t) = 0$。\n- **李对称性（伽利略变换）**：如果 $u(x,t)$ 是一个解，那么对于任意常数 $c \\in \\mathbb{R}$，$u'(x,t) = u(x - c\\,t, t) + c$ 也是一个解。\n- **神经网络近似**：$u_\\theta(x,t) = W_2 \\,\\tanh\\!\\bigl(W_1 \\,[x,\\ t]^\\top + b_1 \\bigr) + b_2$。\n- **网络参数**：\n  - $W_1 = \\begin{bmatrix} 1.2  -0.7 \\\\ -0.3  0.9 \\\\ 0.5  1.1 \\end{bmatrix}$\n  - $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$\n  - $W_2 = \\begin{bmatrix} 0.8  -0.5  0.3 \\end{bmatrix}$\n  - $b_2 = -0.1$\n- **损失函数**：\n  - $\\mathcal{L}_{\\mathrm{pde}} = \\frac{1}{N_{\\mathrm{p}}} \\sum_{(x,t) \\in \\mathcal{P}} \\bigl( u_t + u u_x - \\nu u_{xx} \\bigr)^2$，其中 $N_{\\mathrm{p}} = 4$ 且 $\\mathcal{P} = \\{(-0.8,\\,0.2),\\ (0,\\,0.5),\\ (0.9,\\,0.9),\\ (0.5,\\,0.1)\\}$。\n  - $\\mathcal{L}_{\\mathrm{ic}} = \\frac{1}{N_{\\mathrm{ic}}} \\sum_{x \\in \\mathcal{X}_{\\mathrm{ic}}} \\bigl(u(x,0)\\bigr)^2$，其中 $N_{\\mathrm{ic}} = 4$ 且 $\\mathcal{X}_{\\mathrm{ic}} = \\{-1,\\ -0.3,\\ 0.2,\\ 1\\}$。\n  - $\\mathcal{L}_{\\mathrm{bc}} = \\frac{1}{N_{\\mathrm{b}}} \\sum_{t \\in \\mathcal{T}_{\\mathrm{bc}}} \\left( \\bigl(u(-1,t)\\bigr)^2 + \\bigl(u(1,t)\\bigr)^2 \\right)$，其中 $N_{\\mathrm{b}} = 6$ 且 $\\mathcal{T}_{\\mathrm{bc}} = \\{0,\\ 0.4,\\ 1\\}$。\n  - $\\mathcal{L}_{\\mathrm{sym}}(c) = \\frac{1}{N_{\\mathrm{s}}} \\sum_{(x,t) \\in \\mathcal{S}} \\Bigl( u(x,t) - \\bigl(u(x - c\\,t, t) + c\\bigr) \\Bigr)^2$，其中 $N_{\\mathrm{s}} = 4$ 且 $\\mathcal{S} = \\{(-0.6,\\,0.3),\\ (0.2,\\,0.7),\\ (0.95,\\,0.5),\\ (-1,\\,1)\\}$。\n- **总损失**：$\\mathcal{L}_{\\mathrm{total}}(\\nu,c) = w_{\\mathrm{pde}}\\,\\mathcal{L}_{\\mathrm{pde}} + w_{\\mathrm{ic}}\\,\\mathcal{L}_{\\mathrm{ic}} + w_{\\mathrm{bc}}\\,\\mathcal{L}_{\\mathrm{bc}} + w_{\\mathrm{sym}}\\,\\mathcal{L}_{\\mathrm{sym}}(c)$。\n- **损失权重**：$w_{\\mathrm{pde}} = 1$，$w_{\\mathrm{ic}} = 10$，$w_{\\mathrm{bc}} = 10$，$w_{\\mathrm{sym}} = 1$。\n- **测试用例**：$(\\nu,c) \\in \\{(0.1,\\,0), (0.1,\\,0.5), (0.01,\\,-0.75)\\}$。\n\n### 步骤2：使用提取的给定条件进行验证\n\n检查问题的有效性。\n1.  **科学性或事实性**：问题是科学上合理的。粘性 Burgers 方程是一个典型的非线性偏微分方程。物理信息神经网络（PINNs）是当前计算科学中的一个有效课题。伽利略变换是 Burgers 方程的一个已知李对称性。所有的数学和物理前提都是正确的。\n2.  **无法形式化或不相关**：问题与*计算物理*中的*神经网络函数近似*这一主题相关。它以完整的数学形式体系进行了规定。\n3.  **不完整或矛盾的设置**：问题是自洽的。计算所需的所有参数、函数、点和权重都已明确提供。没有矛盾之处。在点 $(x-ct, t)$ 处对函数 $u_\\theta$ 进行求值，其中 $x-ct$ 可能在 $[-1, 1]$ 之外，对于给定的神经网络函数来说在数学上是良定义的，不构成缺陷。\n4.  **不切实际或不可行**：参数和条件在计算上可行且数学上一致。\n5.  **不适定或结构不良**：任务是为一组给定的参数计算一个特定的数值（一个损失函数）。这是一个具有唯一解的良定义计算问题。\n6.  **超出科学可验证性范围**：计算是确定性的，可以独立验证。\n\n### 步骤3：结论与行动\n\n问题陈述有效。将提供解决方案。\n\n任务是为三个指定的 $(\\nu,c)$ 对计算总损失 $\\mathcal{L}_{\\mathrm{total}}(\\nu,c)$。这需要对神经网络近似函数 $u_\\theta(x,t)$ 进行精确的解析微分，并随后评估四个不同的损失分量。\n\n设输入向量为 $p = [x, t]^\\top \\in \\mathbb{R}^{2}$。神经网络是一个函数 $u_\\theta: \\mathbb{R}^2 \\to \\mathbb{R}$。\n隐藏层的预激活为 $z(p) = W_1 p + b_1$，其中 $z \\in \\mathbb{R}^3$。\n激活为 $a(z) = \\tanh(z)$，其中函数是逐元素应用的。\n输出为 $u_\\theta(p) = W_2 a(z) + b_2$。\n\n为了计算 PDE 残差，我们必须找到偏导数 $u_t$、$u_x$ 和 $u_{xx}$。这通过应用链式法则实现。激活函数的导数是 $\\frac{d}{ds}\\tanh(s) = 1 - \\tanh^2(s)$。\n\n一阶偏导数由梯度 $\\nabla u_\\theta = [\\frac{\\partial u_\\theta}{\\partial x}, \\frac{\\partial u_\\theta}{\\partial t}]$ 给出。\n$$\n\\nabla u_\\theta(p) = \\frac{\\partial u_\\theta}{\\partial a^\\top} \\frac{\\partial a}{\\partial z^\\top} \\frac{\\partial z}{\\partial p^\\top} = W_2 \\cdot \\text{diag}(1 - \\tanh^2(z_j)) \\cdot W_1\n$$\n这里，$\\text{diag}(1 - \\tanh^2(z_j))$ 是一个 $3 \\times 3$ 的对角矩阵。\n设 $W_1 = [W_{1x}, W_{1t}]$，其中 $W_{1x}, W_{1t} \\in \\mathbb{R}^3$ 是 $W_1$ 的列向量。\n偏导数可以写成：\n$$\nu_x(x,t) = \\frac{\\partial u_\\theta}{\\partial x} = W_2 \\cdot \\left( (1 - a^2) \\odot W_{1x} \\right) = \\sum_{j=1}^3 (W_2)_{1j} (1-\\tanh^2(z_j)) (W_1)_{j1}\n$$\n$$\nu_t(x,t) = \\frac{\\partial u_\\theta}{\\partial t} = W_2 \\cdot \\left( (1 - a^2) \\odot W_{1t} \\right) = \\sum_{j=1}^3 (W_2)_{1j} (1-\\tanh^2(z_j)) (W_1)_{j2}\n$$\n其中 $\\odot$ 表示逐元素乘法。\n\n二阶偏导数 $u_{xx}$ 通过对 $u_x$ 关于 $x$ 求导得到：\n$$\nu_{xx}(x,t) = \\frac{\\partial u_x}{\\partial x} = \\sum_{j=1}^3 (W_2)_{1j} (W_1)_{j1} \\frac{\\partial}{\\partial x}(1 - \\tanh^2(z_j))\n$$\n导数项为 $\\frac{\\partial}{\\partial x}(1 - \\tanh^2(z_j)) = -2 \\tanh(z_j) (1 - \\tanh^2(z_j)) \\frac{\\partial z_j}{\\partial x} = -2 a_j (1 - a_j^2) (W_1)_{j1}$。\n将其代回得到：\n$$\nu_{xx}(x,t) = -2 \\sum_{j=1}^3 (W_2)_{1j} (W_1)_{j1}^2 a_j (1 - a_j^2)\n$$\n\n步骤如下：\n1.  实现一个函数，该函数使用固定的网络参数，为任意给定的点 $(x,t)$ 计算 $u_\\theta(x,t)$ 及其导数 $u_t, u_x, u_{xx}$。\n2.  实现函数来计算四个损失分量（$\\mathcal{L}_{\\mathrm{pde}}$, $\\mathcal{L}_{\\mathrm{ic}}$, $\\mathcal{L}_{\\mathrm{bc}}$, $\\mathcal{L}_{\\mathrm{sym}}$）中的每一个，方法是将其各自配置点集上的残差平方相加，并进行正确的归一化。\n    - 对于 $\\mathcal{L}_{\\mathrm{pde}}$，一个点 $(x,t)$ 和参数 $\\nu$ 的残差是 $R_{pde} = u_t + u_\\theta u_x - \\nu u_{xx}$。\n    - 对于 $\\mathcal{L}_{\\mathrm{ic}}$，一个点 $x$ 的残差是 $R_{ic} = u_\\theta(x,0) - 0$。\n    - 对于 $\\mathcal{L}_{\\mathrm{bc}}$，一个时间 $t$ 的残差是 $R_{bc,1} = u_\\theta(-1,t) - 0$ 和 $R_{bc,2} = u_\\theta(1,t) - 0$。\n    - 对于 $\\mathcal{L}_{\\mathrm{sym}}$，一个点 $(x,t)$ 和参数 $c$ 的残差是 $R_{sym} = u_\\theta(x,t) - (u_\\theta(x - ct, t) + c)$。\n3.  对于每个测试用例 $(\\nu, c)$，使用提供的权重计算总损失 $\\mathcal{L}_{\\mathrm{total}}$。\n4.  按规定收集并格式化结果。\n\n这种基于所提供解析形式的严谨、分步计算确保了正确性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    # Define fixed network parameters\n    W1 = np.array([[1.2, -0.7], [-0.3, 0.9], [0.5, 1.1]])\n    b1 = np.array([0.1, -0.2, 0.3]).reshape(3, 1)\n    W2 = np.array([0.8, -0.5, 0.3]).reshape(1, 3)\n    b2 = np.array([-0.1])\n\n    # Define collocation point sets\n    P_points = [(-0.8, 0.2), (0.0, 0.5), (0.9, 0.9), (0.5, 0.1)]\n    X_ic_points = [-1.0, -0.3, 0.2, 1.0]\n    T_bc_points = [0.0, 0.4, 1.0]\n    S_points = [(-0.6, 0.3), (0.2, 0.7), (0.95, 0.5), (-1.0, 1.0)]\n\n    # Define loss component weights\n    w_pde = 1.0\n    w_ic = 10.0\n    w_bc = 10.0\n    w_sym = 1.0\n\n    # Test cases to evaluate\n    test_cases = [\n        (0.1, 0.0),    # Test 1\n        (0.1, 0.5),    # Test 2\n        (0.01, -0.75)  # Test 3\n    ]\n\n    memo_u = {}\n    memo_derivs = {}\n\n    def get_u_and_derivatives(x, t):\n        \"\"\"Computes u and its partial derivatives at (x, t) using automatic differentiation.\"\"\"\n        if (x, t) in memo_derivs:\n            return memo_derivs[(x, t)]\n\n        p = np.array([x, t]).reshape(2, 1)\n        \n        # Forward pass\n        z = W1 @ p + b1\n        a = np.tanh(z)\n        u_val = (W2 @ a + b2).item()\n        \n        # Derivatives calculation\n        d_tanh_z = 1 - a**2  # Derivative of tanh(z) w.r.t. z\n        \n        # First-order derivatives\n        W1_x = W1[:, 0].reshape(3, 1)\n        W1_t = W1[:, 1].reshape(3, 1)\n        \n        u_x_val = (W2 @ (d_tanh_z * W1_x)).item()\n        u_t_val = (W2 @ (d_tanh_z * W1_t)).item()\n        \n        # Second-order derivative u_xx\n        u_xx_vec_contrib = -2 * a * d_tanh_z * (W1_x**2)\n        u_xx_val = (W2 @ u_xx_vec_contrib).item()\n        \n        result = (u_val, u_t_val, u_x_val, u_xx_val)\n        memo_derivs[(x, t)] = result\n        memo_u[(x, t)] = u_val\n        return result\n\n    def get_u(x, t):\n        \"\"\"Computes u(x, t) using memoization.\"\"\"\n        if (x, t) in memo_u:\n            return memo_u[(x, t)]\n        \n        u_val, _, _, _ = get_u_and_derivatives(x, t)\n        return u_val\n\n    def calculate_loss_pde(nu):\n        \"\"\"Calculates the PDE residual loss.\"\"\"\n        loss_sum = 0.0\n        for x, t in P_points:\n            u, u_t, u_x, u_xx = get_u_and_derivatives(x, t)\n            residual = u_t + u * u_x - nu * u_xx\n            loss_sum += residual**2\n        return loss_sum / len(P_points)\n\n    def calculate_loss_ic():\n        \"\"\"Calculates the initial condition loss.\"\"\"\n        loss_sum = 0.0\n        for x in X_ic_points:\n            u_val = get_u(x, 0.0)\n            loss_sum += u_val**2\n        return loss_sum / len(X_ic_points)\n\n    def calculate_loss_bc():\n        \"\"\"Calculates the boundary condition loss.\"\"\"\n        loss_sum = 0.0\n        for t in T_bc_points:\n            u_neg1 = get_u(-1.0, t)\n            u_pos1 = get_u(1.0, t)\n            loss_sum += u_neg1**2 + u_pos1**2\n        return loss_sum / (2 * len(T_bc_points))\n\n    def calculate_loss_sym(c):\n        \"\"\"Calculates the symmetry-consistency loss.\"\"\"\n        loss_sum = 0.0\n        for x, t in S_points:\n            u_original = get_u(x, t)\n            x_prime = x - c * t\n            u_transformed = get_u(x_prime, t)\n            u_prime = u_transformed + c\n            residual = u_original - u_prime\n            loss_sum += residual**2\n        return loss_sum / len(S_points)\n\n    results = []\n    for nu, c in test_cases:\n        # Clear memoization for each test case as nu/c change dependencies\n        memo_derivs.clear()\n        memo_u.clear()\n        \n        l_pde = calculate_loss_pde(nu)\n        l_ic = calculate_loss_ic()\n        l_bc = calculate_loss_bc()\n        l_sym = calculate_loss_sym(c)\n        \n        total_loss = w_pde * l_pde + w_ic * l_ic + w_bc * l_bc + w_sym * l_sym\n        results.append(f\"{total_loss:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2417275"}, {"introduction": "在了解了如何构建更智能的模型之后，理解它们的根本局限性也至关重要。“维度灾难”描述了在高维空间中近似函数所需数据量呈指数增长的现象，这是一个基本挑战。这项动手实践将让您亲身体验这一现象，通过经验性地测量，展示对于一个固定的神经网络模型，要达到相同的近似精度，所需的训练样本数量是如何随着输入维度的增加而爆炸性增长的 [@problem_id:2417291]。", "problem": "要求您通过测量为达到固定的逼近精度，所需训练样本数量如何随输入维度增长，来经验性地展示神经网络函数逼近中的维度灾难。请实现一个完整、可运行的程序，完成以下任务。\n\n从第一性原理出发，考虑以下设置。\n\n- 定义域和目标函数：\n  - 对于每个输入维度 $D \\in \\{\\,1,2,4,8\\,\\}$，将定义域定义为单位超立方体 $[0,1]^D$。\n  - 定义目标函数 $f_D:[0,1]^D \\to \\mathbb{R}$ 为\n    $$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big),$$\n    其中角度以弧度为单位。乘法因子 $\\sqrt{2}$ 确保了在均匀分布下，$f_D$ 的均方幅值不会随 $D$ 的变化而消失或爆炸。\n\n- 神经网络模型和训练协议：\n  - 使用带有整流线性单元 (ReLU) 激活函数的单隐藏层前馈神经网络。隐藏层有 $M$ 个固定的随机特征，且仅通过岭正则化线性最小二乘法训练输出层。这是一种有效的神经网络函数逼近器，也是一种使用随机特征对核机器进行逼近的著名方法。\n  - 令 $\\phi:\\mathbb{R}^D \\to \\mathbb{R}^M$ 为隐藏特征映射，其定义为\n    $$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j\\in\\{1,\\dots,M\\},$$\n    其中每个 $\\mathbf{w}_j \\in \\mathbb{R}^D$ 和 $b_j \\in \\mathbb{R}$ 都独立地从标准正态分布中抽取。总体模型为\n    $$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c,$$\n    其中 $\\mathbf{a}\\in\\mathbb{R}^M$ 和 $c\\in\\mathbb{R}$ 是通过在大小为 $N$ 的训练集上最小化岭正则化经验均方误差来训练的。\n  - 训练过程归结为求解岭正则化正规方程\n    $$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\n    其中 $\\Phi\\in\\mathbb{R}^{N\\times(M+1)}$ 是设计矩阵，其第 $n$ 行为 $[\\phi(\\mathbf{x}^{(n)})^\\top,\\;1]$，$\\mathbf{y}\\in\\mathbb{R}^N$ 包含目标值 $f_D(\\mathbf{x}^{(n)})$，$\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ 连接了 $\\mathbf{a}$ 和 $c$，$\\lambda>0$ 是岭正则化强度。对所有实验使用固定的 $\\lambda$。\n  - 使用从 $[0,1]^D$ 上的均匀分布中抽取的独立同分布 (i.i.d.) 样本进行训练和验证。\n\n- 精度标准：\n  - 在一个独立同分布的验证集上测量均方根误差 (RMSE)：\n    $$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})-f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\n  - 如果在验证集上 $\\mathrm{RMSE} \\le \\varepsilon$，则宣布给定的训练集大小 $N$ 是足够的。\n\n- 固定的超参数和可复现性：\n  - 使用 $M=256$ 个随机隐藏单元。\n  - 使用岭参数 $\\lambda=10^{-3}$。\n  - 使用验证集大小 $N_{\\mathrm{val}}=2000$。\n  - 使用候选训练集大小 $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$，并选择满足精度标准的最小 $N$。\n  - 使用精度阈值 $\\varepsilon=0.20$。\n  - 使用带有固定种子的独立随机数生成器，以确保结果可以精确复现。正弦函数中的角度以弧度为单位。\n\n- 测试套件和输出规范：\n  - 对于每个 $D \\in \\{\\,1,2,4,8\\,\\}$，在固定的 $M$、$\\lambda$、 $N_{\\mathrm{val}}$ 和 $\\varepsilon$（如上所述）下，从候选列表中确定达到 $\\mathrm{RMSE}\\le\\varepsilon$ 的最小 $N$。如果对于给定的 $D$，没有候选的 $N$ 达到该阈值，则为该 $D$ 返回 $-1$。\n  - 您的程序应生成单行输出，其中包含按 $D=\\{\\,1,2,4,8\\,\\}$ 顺序排列的结果，格式为方括号内由逗号分隔的整数列表，例如：\n    $$[N_1,N_2,N_3,N_4],$$\n    其中每个 $N_k$ 是该维度的最小足够训练集大小，如果没有满足条件的，则为 $-1$。\n\n科学真实性和基础依据：您必须从均方逼近误差的定义、带有 ReLU 激活的单隐藏层神经网络的结构，以及随机特征和岭正则化最小二乘法的性质出发进行推理。除指定内容外，请勿使用任何数据依赖的启发式方法。所有角度必须以弧度为单位。此问题不涉及任何物理单位。如果出现任何内部百分比，请以小数或分数表示，不要使用百分号。", "solution": "我们设计一项经验性研究，通过量化在使用固定容量的神经网络逼近器时，为达到固定的精度目标，所需训练集大小如何随输入维度增长，来揭示维度灾难。其基本依据是均方误差的定义、单隐藏层神经网络的结构以及岭正则化最小二乘法。\n\n1. 函数和定义域。对于每个维度 $D \\in \\{\\,1,2,4,8\\,\\}$，我们考虑目标函数\n$$f_D(\\mathbf{x}) \\;=\\; \\prod_{i=1}^{D} \\sqrt{2}\\,\\sin\\!\\big(2\\pi x_i\\big), \\quad \\mathbf{x}\\in[0,1]^D.$$\n角度以弧度为单位。因子 $\\sqrt{2}$ 确保在均匀测度下，期望平方幅值不随 $D$ 变化：\n$$\\mathbb{E}\\!\\left[f_D(\\mathbf{x})^2\\right] \\;=\\; \\prod_{i=1}^{D} \\mathbb{E}\\!\\left[2\\sin^2(2\\pi X_i)\\right] \\;=\\; \\prod_{i=1}^{D} 1 \\;=\\; 1,$$\n其中 $X_i \\sim \\mathrm{Uniform}[0,1]$ 独立分布，并且我们使用了 $\\mathbb{E}[\\sin^2(2\\pi X_i)] = \\tfrac{1}{2}$。这种归一化消除了随 $D$ 产生的平凡缩放效应，并专注于表示和采样的难度。\n\n2. 带随机特征的神经网络逼近器。考虑一个带有整流线性单元 (ReLU) 激活函数的单隐藏层网络，其定义为\n$$\\phi_j(\\mathbf{x}) \\;=\\; \\max\\!\\big(0,\\,\\mathbf{w}_j^\\top \\mathbf{x} + b_j\\big), \\quad j=1,\\dots,M,$$\n其中 $\\mathbf{w}_j \\in \\mathbb{R}^D$ 且 $b_j\\in\\mathbb{R}$。该模型对这些特征是线性的：\n$$\\hat{f}_D(\\mathbf{x}) \\;=\\; \\mathbf{a}^\\top \\phi(\\mathbf{x}) + c.$$\n我们通过从标准正态分布中进行独立同分布采样来固定隐藏参数 $\\{\\mathbf{w}_j,b_j\\}_{j=1}^{M}$。这产生了一个随机特征模型，该模型在 $M$ 增长时保留了通用逼近属性，并允许对输出层进行快速、凸的训练。\n\n3. 通过岭正则化最小二乘法进行训练。给定一个训练集 $\\{(\\mathbf{x}^{(n)}, y^{(n)})\\}_{n=1}^{N}$，其中 $y^{(n)} = f_D(\\mathbf{x}^{(n)})$，通过用一列1来增广隐藏特征，定义设计矩阵 $\\Phi \\in \\mathbb{R}^{N\\times(M+1)}$：\n$$\\Phi_{n,:} \\;=\\; \\big[\\phi(\\mathbf{x}^{(n)})^\\top,\\,1\\big].$$\n令 $\\boldsymbol{\\theta} \\in \\mathbb{R}^{M+1}$ 为输出权重和偏置的拼接，令 $\\mathbf{y} \\in \\mathbb{R}^{N}$ 为目标向量。岭正则化经验风险最小化问题\n$$\\min_{\\boldsymbol{\\theta}} \\;\\frac{1}{N}\\|\\Phi \\boldsymbol{\\theta} - \\mathbf{y}\\|_2^2 \\;+\\; \\lambda \\|\\boldsymbol{\\theta}\\|_2^2$$\n其唯一解由正规方程给出\n$$(\\Phi^\\top \\Phi + \\lambda I)\\,\\boldsymbol{\\theta} \\;=\\; \\Phi^\\top \\mathbf{y},$$\n其中 $I$ 是 $(M+1)\\times(M+1)$ 的单位矩阵，$\\lambda>0$ 是岭参数。求解这个线性系统可以得到训练好的输出层参数。\n\n4. 精度和验证。为了量化逼近质量，我们使用在大小为 $N_{\\mathrm{val}}$ 的独立验证集上的均方根误差 (RMSE)：\n$$\\mathrm{RMSE} \\;=\\; \\sqrt{\\frac{1}{N_{\\mathrm{val}}}\\sum_{n=1}^{N_{\\mathrm{val}}}\\big(\\hat{f}_D(\\mathbf{x}^{(n)}_{\\mathrm{val}}) - f_D(\\mathbf{x}^{(n)}_{\\mathrm{val}})\\big)^2}.$$\n如果 $\\mathrm{RMSE} \\le \\varepsilon$，我们则宣布对于给定的 $N$ 实验成功。\n\n5. 实验协议。对于每个 $D \\in \\{\\,1,2,4,8\\,\\}$：\n   - 将隐藏单元数固定为 $M = 256$，岭参数固定为 $\\lambda = 10^{-3}$。\n   - 使用从 $[0,1]^D$ 上均匀独立同分布抽取的 $N_{\\mathrm{val}} = 2000$ 个验证样本。\n   - 考虑候选训练集大小 $N \\in \\{\\,32,64,128,256,512,1024,2048\\,\\}$。\n   - 对于每个 $N$（按递增顺序）：\n     - 从 $[0,1]^D$ 中独立同分布地抽取 $N$ 个训练输入，使用 $f_D$ 计算目标值。\n     - 构建特征矩阵 $\\Phi$ 并求解 $(\\Phi^\\top \\Phi + \\lambda I)\\boldsymbol{\\theta} = \\Phi^\\top \\mathbf{y}$。\n     - 计算验证集上的 $\\mathrm{RMSE}$。\n     - 如果 $\\mathrm{RMSE} \\le \\varepsilon$ 且 $\\varepsilon = 0.20$，则记录此 $N$ 为足够值，并停止为该 $D$ 增加 $N$。\n   - 如果没有候选的 $N$ 满足该标准，则为该 $D$ 记录 $-1$。\n   - 对特征初始化和数据采样使用固定的随机种子，以确保不同运行之间的可复现性。\n\n6. 关于维度灾难的原理说明。维度灾难的体现是，输入空间的体积随 $D$ 指数级增长，因此以固定的样本密度覆盖定义域需要指数级数量的点。此外，诸如振荡因子乘积之类的函数，随着 $D$ 的增加，在 $[0,1]^D$ 上会表现出日益复杂的变化。在模型容量 $M$ 固定的情况下，达到目标 $\\mathrm{RMSE}$ 所需的样本量 $N$ 通常随 $D$ 的增加而增加，并且可能存在某些维度，在实际范围内没有 $N$ 能达到阈值，因为逼近受限于 $M$ 的表示能力。\n\n7. 输出。程序将每个 $D$（按 $D=\\{\\,1,2,4,8\\,\\}$ 的顺序）的最小足够 $N$ 汇总到单行中，格式为方括号内由逗号分隔的整数列表：\n$$[N_1,N_2,N_3,N_4].$$\n如果对于特定的 $D$ 没有足够的候选 $N$，则对应的条目为 $-1$。\n\n该设计从均方误差的定义和岭正则化最小二乘法的线性代数解出发，通过随机 ReLU 特征指定了一个具体的神经网络函数逼近器，并实现了一个经验性协议来测量所需数据大小如何随维度变化，从而展示维度灾难。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef target_function(X):\n    \"\"\"\n    Compute f_D(x) = prod_i sqrt(2) * sin(2*pi*x_i) for a batch of inputs X.\n    X: array of shape (N, D)\n    Returns: array of shape (N,)\n    \"\"\"\n    return np.prod(np.sqrt(2.0) * np.sin(2.0 * np.pi * X), axis=1)\n\ndef generate_random_features(D, M, seed):\n    \"\"\"\n    Generate random ReLU feature parameters (W, b) for dimension D and M features.\n    W: shape (M, D), entries ~ N(0,1)\n    b: shape (M,), entries ~ N(0,1)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(M, D))\n    b = rng.standard_normal(size=(M,))\n    return W, b\n\ndef relu_features(X, W, b):\n    \"\"\"\n    Compute ReLU features Phi = max(0, X @ W^T + b) for batch X.\n    X: (N, D), W: (M, D), b: (M,)\n    Returns: Phi of shape (N, M)\n    \"\"\"\n    Z = X @ W.T + b  # broadcasting b over rows\n    return np.maximum(0.0, Z)\n\ndef ridge_solve(Phi, y, lam):\n    \"\"\"\n    Solve (Phi^T Phi + lam I) theta = Phi^T y for theta.\n    Phi should already include a bias column (i.e., augmented with ones).\n    \"\"\"\n    # Normal equations with Tikhonov regularization\n    A = Phi.T @ Phi\n    # Add ridge on all parameters (including bias for simplicity)\n    A.flat[::A.shape[0]+1] += lam  # add lam to diagonal\n    b = Phi.T @ y\n    theta = np.linalg.solve(A, b)\n    return theta\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n\ndef minimal_training_size_for_dimension(D, M, lam, eps, N_candidates, N_val,\n                                        seed_features, seed_train_base, seed_val_base):\n    \"\"\"\n    For a given input dimension D, find the minimal N in N_candidates such that\n    RMSE = eps on a validation set, using fixed random features and ridge training.\n    Returns the minimal N, or -1 if none suffices.\n    \"\"\"\n    # Fixed random features for this D\n    W, b = generate_random_features(D, M, seed_features + D)\n\n    # Fixed validation set for this D\n    rng_val = np.random.default_rng(seed_val_base + D)\n    X_val = rng_val.random(size=(N_val, D))\n    y_val = target_function(X_val)\n    Phi_val = relu_features(X_val, W, b)\n    Phi_val_aug = np.hstack([Phi_val, np.ones((N_val, 1))])\n\n    for N in N_candidates:\n        rng_train = np.random.default_rng(seed_train_base + D + N)\n        X_train = rng_train.random(size=(N, D))\n        y_train = target_function(X_train)\n        Phi_train = relu_features(X_train, W, b)\n        Phi_train_aug = np.hstack([Phi_train, np.ones((N, 1))])\n\n        theta = ridge_solve(Phi_train_aug, y_train, lam)\n        y_pred_val = Phi_val_aug @ theta\n        e = rmse(y_val, y_pred_val)\n        if e = eps:\n            return N\n    return -1\n\ndef solve():\n    # Experimental settings as specified\n    dims = [1, 2, 4, 8]              # D values\n    M = 256                          # number of random hidden units\n    lam = 1e-3                       # ridge regularization strength\n    eps = 0.20                       # RMSE threshold\n    N_candidates = [32, 64, 128, 256, 512, 1024, 2048]\n    N_val = 2000\n\n    # Fixed seeds for reproducibility\n    seed_features = 12345\n    seed_train_base = 54321\n    seed_val_base = 99999\n\n    results = []\n    for D in dims:\n        N_min = minimal_training_size_for_dimension(\n            D=D, M=M, lam=lam, eps=eps, N_candidates=N_candidates, N_val=N_val,\n            seed_features=seed_features, seed_train_base=seed_train_base, seed_val_base=seed_val_base\n        )\n        results.append(N_min)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2417291"}]}