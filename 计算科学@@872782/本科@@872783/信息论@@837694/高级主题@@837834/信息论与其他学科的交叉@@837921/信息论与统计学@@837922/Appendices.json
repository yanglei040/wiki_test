{"hands_on_practices": [{"introduction": "理解一个系统的不确定性是信息论的核心任务。第一个练习将让你实践计算香农熵——离散变量信息的基本度量——通过分析一个简单的模运算如何改变一个均匀概率分布，并因此改变其熵。这个基础练习有助于巩固对熵定义的直接应用。[@problem_id:1631970]", "problem": "一个离散随机变量 $X$ 定义在结果集合 $\\{0, 1, 2, 3, 4, 5, 6, 7\\}$ 上。$X$ 的概率质量函数在此集合上是均匀分布的。第二个随机变量 $Y$ 是通过变换 $Y = X \\pmod 4$ 从 $X$ 导出的。\n\n计算随机变量 $Y$ 的香农熵。请以比特为单位，将您的答案表示为封闭形式的解析表达式。", "solution": "设 $X$ 在 $\\{0,1,2,3,4,5,6,7\\}$ 上均匀分布，因此对于每一个 $x \\in \\{0,1,2,3,4,5,6,7\\}$，\n$$\n\\Pr(X=x)=\\frac{1}{8}.\n$$\n定义 $Y = X \\bmod 4$。那么 $Y$ 在 $\\{0,1,2,3\\}$ 中取值。对于任意 $y \\in \\{0,1,2,3\\}$，\n$$\n\\Pr(Y=y)=\\Pr(X \\in \\{y, y+4\\})=\\Pr(X=y)+\\Pr(X=y+4)=\\frac{1}{8}+\\frac{1}{8}=\\frac{1}{4}.\n$$\n因此 $Y$ 在 $\\{0,1,2,3\\}$ 上是均匀分布的。\n\n$Y$ 的香农熵（以比特为单位）为\n$$\nH(Y)=-\\sum_{y=0}^{3} \\Pr(Y=y)\\,\\log_{2}\\bigl(\\Pr(Y=y)\\bigr)\n= -4 \\cdot \\frac{1}{4}\\, \\log_{2}\\!\\left(\\frac{1}{4}\\right)\n= - \\log_{2}\\!\\left(\\frac{1}{4}\\right).\n$$\n使用 $\\log_{2}\\!\\left(\\frac{1}{4}\\right)=\\log_{2}\\!\\left(2^{-2}\\right)=-2$，我们得到\n$$\nH(Y)=2.\n$$\n因此熵为 $2$ 比特。", "answer": "$$\\boxed{2}$$", "id": "1631970"}, {"introduction": "虽然离散变量是基础，但许多现实世界的过程是连续的，例如电子元件的寿命或放射性衰变的时间。本题将介绍微分熵，即香农熵在连续随机变量上的对应概念，并将其应用于指数分布——一个在工程和物理学中常见的寿命模型。通过这个练习，你将学会如何从概率密度函数计算不确定性。[@problem_id:1631980]", "problem": "在可靠性工程中，某些电子元件的寿命通常使用连续概率分布进行建模。考虑一个元件，其寿命由随机变量 $T$（单位为小时）表示，服从指数分布。$T$ 的概率密度函数 (PDF) 由下式给出：\n$$\nf(t) = \\begin{cases} \\lambda \\exp(-\\lambda t)  \\text{for } t \\ge 0 \\\\ 0  \\text{for } t  0 \\end{cases}\n$$\n其中 $\\lambda  0$ 是恒定的率参数，表示元件的失效率。\n\n在信息论中，与连续随机变量相关的不确定性由其微分熵来量化。对于具有概率密度函数 $f(x)$ 的随机变量 $X$，其微分熵 $h(X)$ 定义为：\n$$\nh(X) = -\\int_{-\\infty}^{\\infty} f(x) \\ln(f(x)) \\, dx\n$$\n使用这些定义，确定该元件寿命的微分熵 $h(T)$。请用率参数 $\\lambda$ 的闭式解析表达式表示您的答案。", "solution": "具有概率密度函数 $f(x)$ 的连续随机变量的微分熵定义为\n$$\nh(X)=-\\int_{-\\infty}^{\\infty} f(x)\\ln\\big(f(x)\\big)\\,dx.\n$$\n对于率参数为 $\\lambda0$ 的指数寿命 $T$，其概率密度函数在 $t\\ge 0$ 时为 $f(t)=\\lambda\\exp(-\\lambda t)$，在 $t0$ 时为 $f(t)=0$。因此，熵积分简化为\n$$\nh(T)=-\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda t)\\,\\ln\\big(\\lambda \\exp(-\\lambda t)\\big)\\,dt.\n$$\n使用 $\\ln\\big(\\lambda \\exp(-\\lambda t)\\big)=\\ln(\\lambda)-\\lambda t$，我们得到\n$$\nh(T)=-\\int_{0}^{\\infty} \\lambda \\exp(-\\lambda t)\\big(\\ln(\\lambda)-\\lambda t\\big)\\,dt\n= -\\left[\\ln(\\lambda)\\int_{0}^{\\infty}\\lambda \\exp(-\\lambda t)\\,dt-\\int_{0}^{\\infty}\\lambda^{2} t \\exp(-\\lambda t)\\,dt\\right].\n$$\n对于第一个积分，使用换元法 $u=\\lambda t$, $dt=du/\\lambda$，得到\n$$\n\\int_{0}^{\\infty}\\lambda \\exp(-\\lambda t)\\,dt=\\int_{0}^{\\infty}\\exp(-u)\\,du=\\left[-\\exp(-u)\\right]_{0}^{\\infty}=1.\n$$\n对于第二个积分，使用相同的换元法 $u=\\lambda t$, $dt=du/\\lambda$：\n$$\n\\int_{0}^{\\infty}\\lambda^{2} t \\exp(-\\lambda t)\\,dt=\\int_{0}^{\\infty}u \\exp(-u)\\,du.\n$$\n通过分部积分法计算该积分，令 $a=u$，$db=\\exp(-u)\\,du$，则 $da=du$，$b=-\\exp(-u)$：\n$$\n\\int_{0}^{\\infty}u \\exp(-u)\\,du=\\left[-u\\exp(-u)\\right]_{0}^{\\infty}+\\int_{0}^{\\infty}\\exp(-u)\\,du=0+1=1.\n$$\n将这些结果代回，得到\n$$\nh(T)=-\\big(\\ln(\\lambda)\\cdot 1-1\\big)=1-\\ln(\\lambda).\n$$\n因此，指数寿命 $T$ 的微分熵为 $1-\\ln(\\lambda)$。", "answer": "$$\\boxed{1-\\ln(\\lambda)}$$", "id": "1631980"}, {"introduction": "信息论为统计推断提供了强大的工具，帮助我们量化能从数据中学到多少知识。本练习将探讨费雪信息，这个核心概念量化了一项实验能够告诉我们多少关于一个未知参数的信息。通过为一系列伯努利试验（例如在通信系统中重复传输比特）计算费雪信息，你将深入了解参数估计的根本极限。[@problem_id:1632005]", "problem": "在一个数字通信系统中，一个包含 $n$ 个相同比特的序列通过一个有噪声的信道传输，以建立其可靠性的基准。每个传输的比特都是 '1'。由于噪声，每个比特的接收都是一个独立的随机事件。正确接收到 '1' 的概率用 $p$ 表示，其中 $0  p  1$。相应地，发生错误（接收到 '0'）的概率是 $1-p$。因此，接收到的 $n$ 个比特的序列可以建模为一组 $n$ 次独立的伯努利试验。\n\n费雪信息 (Fisher Information) 是信息论和统计学中的一个基本概念，它量化了一个随机变量所携带的、关于其概率所依赖的未知参数的信息量。对于这个通信系统，我们感兴趣的参数是成功概率 $p$。\n\n基于接收到的 $n$ 个比特的序列，确定参数 $p$ 的费雪信息 $I(p)$。将答案表示为一个关于 $n$ 和 $p$ 的单一闭式解析表达式。", "solution": "问题要求解 $n$ 次独立伯努利试验的成功概率参数 $p$ 的费雪信息 $I(p)$。\n\n设接收到的比特序列由随机变量 $X_1, X_2, \\ldots, X_n$ 表示。每个 $X_i$ 是一个伯努利随机变量，如果比特被正确接收（成功），则 $X_i=1$；如果接收错误（失败），则 $X_i=0$。单次试验 $X_i$ 的概率质量函数（PMF）由下式给出：\n$$P(X_i=x | p) = p^x (1-p)^{1-x} \\quad \\text{for } x \\in \\{0, 1\\}$$\n\n由于各次试验是独立的，对于一个特定的观测序列 $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$，其联合概率质量函数，或称为似然函数 $L(p)$, 是各个独立 PMF 的乘积：\n$$L(p | \\mathbf{x}) = \\prod_{i=1}^{n} P(X_i=x_i | p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i}$$\n这可以通过合并指数来简化：\n$$L(p | \\mathbf{x}) = p^{\\sum_{i=1}^{n} x_i} (1-p)^{\\sum_{i=1}^{n} (1-x_i)} = p^{\\sum x_i} (1-p)^{n - \\sum x_i}$$\n设 $k = \\sum_{i=1}^{n} x_i$ 为 $n$ 次试验序列中成功的总次数（正确接收的比特数）。似然函数变为：\n$$L(p | k) = p^k (1-p)^{n-k}$$\n\n费雪信息通常使用对数似然函数（记为 $\\ell(p)$）来计算。对数似然函数是似然函数的自然对数：\n$$\\ell(p | k) = \\ln(L(p | k)) = \\ln(p^k (1-p)^{n-k}) = k \\ln(p) + (n-k) \\ln(1-p)$$\n\n费雪信息 $I(p)$ 的一个定义是对数似然函数关于参数 $p$ 的二阶导数的期望值的负数：\n$$I(p) = -E\\left[\\frac{\\partial^2 \\ell(p|k)}{\\partial p^2}\\right]$$\n\n首先，我们求对数似然函数关于 $p$ 的一阶导数。这被称为得分函数。\n$$\\frac{\\partial \\ell}{\\partial p} = \\frac{\\partial}{\\partial p} [k \\ln(p) + (n-k) \\ln(1-p)] = \\frac{k}{p} - \\frac{n-k}{1-p}$$\n\n接下来，我们求对数似然函数关于 $p$ 的二阶导数：\n$$\\frac{\\partial^2 \\ell}{\\partial p^2} = \\frac{\\partial}{\\partial p} \\left[\\frac{k}{p} - \\frac{n-k}{1-p}\\right] = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}(-1) = -\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}$$\n\n现在，我们计算这个二阶导数的期望值。这里的随机变量是 $k$，即成功的次数。由于 $k$ 是 $n$ 次成功概率为 $p$ 的独立同分布伯努利试验之和，所以 $k$ 服从二项分布，$k \\sim \\text{Bin}(n, p)$。$k$ 的期望值是 $E[k] = np$。\n\n我们将此代入 $I(p)$ 的表达式中：\n$$I(p) = -E\\left[-\\frac{k}{p^2} - \\frac{n-k}{(1-p)^2}\\right] = E\\left[\\frac{k}{p^2} + \\frac{n-k}{(1-p)^2}\\right]$$\n根据期望的线性性质：\n$$I(p) = \\frac{E[k]}{p^2} + \\frac{E[n-k]}{(1-p)^2}$$\n我们知道 $E[k] = np$。并且，$E[n-k] = E[n] - E[k] = n - np = n(1-p)$。\n代入这些期望值：\n$$I(p) = \\frac{np}{p^2} + \\frac{n(1-p)}{(1-p)^2}$$\n简化各项可得：\n$$I(p) = \\frac{n}{p} + \\frac{n}{1-p}$$\n将各项通分合并：\n$$I(p) = n \\left(\\frac{1-p+p}{p(1-p)}\\right) = \\frac{n}{p(1-p)}$$\n\n因此，来自 $n$ 次伯努利试验的参数 $p$ 的费雪信息是 $\\frac{n}{p(1-p)}$。这个结果表示来自所有 $n$ 次试验的总信息量，它就是单次试验的费雪信息 $I_1(p) = \\frac{1}{p(1-p)}$ 的 $n$ 倍。", "answer": "$$\\boxed{\\frac{n}{p(1-p)}}$$", "id": "1632005"}]}