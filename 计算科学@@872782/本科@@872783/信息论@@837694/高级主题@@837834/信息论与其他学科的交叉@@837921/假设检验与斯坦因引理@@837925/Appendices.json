{"hands_on_practices": [{"introduction": "在进行假设检验时，一个核心问题是评估哪一个假设更能解释我们观察到的数据。似然比是解决这个问题的基本工具，它直接比较了在两种不同假设下，观测到特定数据序列的概率。通过计算这个比率 [@problem_id:1630542]，我们可以量化证据支持某个假设的强度。", "problem": "一个数字通信系统被监控以检测故障。该系统设计用于传输由‘0’和‘1’组成的二进制序列。在正常运行（假设$H_0$）下，信源是一个无记忆过程，以相等的概率生成‘0’和‘1’。一种特定的故障模式（假设$H_1$）导致信源产生偏差，增加了传输‘1’的概率。\n\n我们有两个相互竞争的模型来描述信源的行为：\n- **标称模型（$H_0$）：** 传输‘1’的概率为 $p_0 = 0.5$。\n- **故障模型（$H_1$）：** 传输‘1’的概率为 $p_1 = 0.75$。\n\n一个监控工具捕获了一个由四个传输比特组成的短序列，观测结果为`1011`。为了帮助判断哪个模型能更好地解释这一观测结果，我们想要计算在故障模型下观测到此序列的概率与在标称模型下观测到此序列的概率之比。\n\n计算该比率的值。答案以四舍五入到三位有效数字的数字表示。", "solution": "在每种假设下，信源发出的是独立的伯努利试验。对于一个长度为$n$，包含$k$个1和$n-k$个0的特定二进制序列，在参数$p$下的概率是$p^{k}(1-p)^{n-k}$。对于观测到的序列$1011$，我们有$n=4$和$k=3$，所以\n$$\nP(1011 \\mid p)=p^{3}(1-p).\n$$\n故障模型$H_{1}$与标称模型$H_{0}$的似然比为\n$$\nR=\\frac{P(1011 \\mid H_{1})}{P(1011 \\mid H_{0})}=\\frac{p_{1}^{3}(1-p_{1})}{p_{0}^{3}(1-p_{0})}.\n$$\n代入$p_{1}=\\frac{3}{4}$和$p_{0}=\\frac{1}{2}$，\n$$\nR=\\frac{\\left(\\frac{3}{4}\\right)^{3}\\left(1-\\frac{3}{4}\\right)}{\\left(\\frac{1}{2}\\right)^{3}\\left(1-\\frac{1}{2}\\right)}=\\frac{\\frac{27}{64}\\cdot \\frac{1}{4}}{\\frac{1}{8}\\cdot \\frac{1}{2}}=\\frac{\\frac{27}{256}}{\\frac{1}{16}}=\\frac{27}{16}=1.6875.\n$$\n四舍五入到三位有效数字，结果是$1.69$。", "answer": "$$\\boxed{1.69}$$", "id": "1630542"}, {"introduction": "虽然似然比对单次观测很有用，但我们通常更关心在收集大量数据时，区分两种假设的能力。KL散度（Kullback-Leibler divergence）正是衡量这种长期可区分性的关键指标。计算两个概率分布之间的KL散度 [@problem_id:1630521]，可以为我们理解斯坦因引理（Stein's Lemma）中描述的指数级错误衰减率奠定基础。", "problem": "在一个工厂里，一台机器制造一种特定类型的电子元件。每个元件都会经过测试，并被分类为功能正常（状态1）或有缺陷（状态0）。这台机器的性能可以用伯努利分布来建模。\n\n关于这台机器的运行状态，有两个假设：\n- 假设 $H_0$：机器在正常条件下运行。生产一个功能正常元件的概率是 $p_0 = 1/3$。设此分布为 $P_0$。\n- 假设 $H_1$：机器需要维护。生产一个功能正常元件的概率是 $p_1 = 2/3$。设此分布为 $P_1$。\n\n为了量化“正常”状态与“需要维护”状态之间的统计可区分性，一位工程师决定计算库尔贝克-莱布勒（KL）散度，也称为相对熵。\n\n计算KL散度 $D(P_0 || P_1)$。请将您的答案以数值形式给出，并四舍五入到三位有效数字。", "solution": "我们将机器的输出建模为一个伯努利随机变量。对于伯努利分布，从参数为 $p_{0}$ 的分布 $P_{0}$ 到参数为 $p_{1}$ 的分布 $P_{1}$ 的库尔贝克-莱布勒散度为\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; p_{0}\\,\\ln\\!\\left(\\frac{p_{0}}{p_{1}}\\right) \\;+\\; \\left(1-p_{0}\\right)\\,\\ln\\!\\left(\\frac{1-p_{0}}{1-p_{1}}\\right).\n$$\n代入 $p_{0}=\\frac{1}{3}$ 和 $p_{1}=\\frac{2}{3}$：\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{\\frac{1}{3}}{\\frac{2}{3}}\\right) \\;+\\; \\frac{2}{3}\\,\\ln\\!\\left(\\frac{\\frac{2}{3}}{\\frac{1}{3}}\\right).\n$$\n化简比率：\n$$\n\\frac{\\frac{1}{3}}{\\frac{2}{3}} \\;=\\; \\frac{1}{2}, \\qquad \\frac{\\frac{2}{3}}{\\frac{1}{3}} \\;=\\; 2,\n$$\n所以\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; \\frac{1}{3}\\,\\ln\\!\\left(\\frac{1}{2}\\right) \\;+\\; \\frac{2}{3}\\,\\ln(2).\n$$\n使用 $\\ln\\!\\left(\\frac{1}{2}\\right)=-\\ln(2)$：\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\;=\\; -\\frac{1}{3}\\,\\ln(2) \\;+\\; \\frac{2}{3}\\,\\ln(2) \\;=\\; \\frac{1}{3}\\,\\ln(2).\n$$\n数值上，使用 $\\ln(2)\\approx 0.693147$，我们得到\n$$\nD(P_{0}\\,\\|\\,P_{1}) \\approx \\frac{1}{3}\\times 0.693147 \\approx 0.231049,\n$$\n四舍五入到三位有效数字为 $0.231$。", "answer": "$$\\boxed{0.231}$$", "id": "1630521"}, {"introduction": "斯坦因引理揭示了第二类错误概率随样本数量呈指数级下降，其衰减率由KL散度决定。这个深刻的理论结果具有重要的现实意义，因为它使我们能够进行实际的工程设计。本练习 [@problem_id:1630537] 将向您展示如何利用这个原理，来估算为了达到特定的错误概率目标，需要采集多少样本数据。", "problem": "一位数据科学家正在进行一项二元假设检验，以区分两种数据源。这些数据源被建模为独立同分布的伯努利随机变量序列。原假设 $H_0$ 假定数据来自参数为 $p_0 = 1/2$ 的伯努利分布。备择假设 $H_1$ 假定数据源是参数为 $p_1 \\neq p_0$ 的伯努利分布。\n\n对于一个包含 $n$ 个样本序列的假设检验，我们将第一类错误概率定义为 $\\alpha_n = P(\\text{decide } H_1 | H_0 \\text{ is true})$，第二类错误概率定义为 $\\beta_n = P(\\text{decide } H_0 | H_1 \\text{ is true})$。斯坦因引理指出，对于固定的第一类错误概率约束 $\\alpha_n \\leq \\epsilon$（其中 $\\epsilon \\in (0, 1/2)$），可实现的最小第二类错误概率 $\\beta_n^*$ 会随着样本数量 $n$ 的增加而指数级衰减。该衰减的速率由两个分布之间的 Kullback-Leibler 散度给出。\n\n该数据科学家正在使用一种检验程序，其中使用自然对数计算出的最优错误指数为 $C = 0.0872$。公司的协议要求将第一类错误概率固定在 $\\epsilon = 0.05$。目标是确定为达到一个非常低的第二类错误概率所需的样本量。\n\n使用从斯坦因引理导出的大样本近似，计算所需的最小整数样本量 $n$，以确保第二类错误概率最多为 $\\beta^* = 1 \\times 10^{-5}$。", "solution": "本题要求计算，在给定二元假设检验的最优错误指数 $C$ 的情况下，为达到特定的第二类错误概率 $\\beta^*$ 所需的最小样本量 $n$。\n\n斯坦因引理给出了在固定第一类错误概率 $\\epsilon$ 的条件下，最小第二类错误概率 $\\beta_n^*$ 的渐进行为。对于大样本量 $n$，该关系由下式给出：\n$$ \\beta_n^* \\approx \\exp(-n \\cdot D(P_1 || P_0)) $$\n其中 $D(P_1 || P_0)$ 是假设 $H_1$ 下的概率分布（记为 $P_1$）与假设 $H_0$ 下的概率分布（记为 $P_0$）之间的 Kullback-Leibler (KL) 散度。此 KL 散度即为第二类错误的最优错误指数。\n\n题目指出，最优错误指数为 $C = 0.0872$。因此，我们有：\n$$ C = D(P_1 || P_0) = 0.0872 $$\n需要注意的是，根据斯坦因引理，在大样本 $n$ 的极限情况下，此结果对任何固定的第一类错误约束 $\\epsilon \\in (0, 1/2)$ 都成立。因此，具体值 $\\epsilon = 0.05$ 是问题设定的一部分，但在基于此近似计算样本量 $n$ 时并不需要。\n\n我们已知第二类错误概率的目标值为 $\\beta^* = 1 \\times 10^{-5}$。我们需要找到最小的整数 $n$ 使得 $\\beta_n^* \\leq \\beta^*$。使用该近似，我们设定：\n$$ \\beta^* = \\exp(-nC) $$\n现在，我们求解该方程以得到 $n$：\n$$ \\ln(\\beta^*) = \\ln(\\exp(-nC)) $$\n$$ \\ln(\\beta^*) = -nC $$\n$$ n = -\\frac{\\ln(\\beta^*)}{C} $$\n这也可以写作：\n$$ n = \\frac{\\ln(1/\\beta^*)}{C} $$\n\n现在，我们将给定的数值代入此表达式：\n- $C = 0.0872$\n- $\\beta^* = 1 \\times 10^{-5}$\n\n所以，\n$$ n = \\frac{\\ln(1/(1 \\times 10^{-5}))}{0.0872} $$\n$$ n = \\frac{\\ln(10^5)}{0.0872} $$\n使用对数性质 $\\ln(x^y) = y \\ln(x)$，我们得到：\n$$ n = \\frac{5 \\ln(10)}{0.0872} $$\n使用 10 的自然对数的近似值 $\\ln(10) \\approx 2.302585$：\n$$ n \\approx \\frac{5 \\times 2.302585}{0.0872} $$\n$$ n \\approx \\frac{11.512925}{0.0872} $$\n$$ n \\approx 132.028956 $$\n\n样本量 $n$ 必须是整数。题目要求的是确保第二类错误*最多*为 $\\beta^*$ 的最小整数样本量。关系式 $\\beta_n^* \\approx \\exp(-nC)$ 表明 $\\beta_n^*$ 是 $n$ 的递减函数。因此，为确保错误概率小于或等于目标值 $\\beta^*$，我们必须将计算出的 $n$ 值向上取整到下一个整数。这个数学运算称为向上取整函数 (ceiling function)。\n\n$$ n_{\\text{min}} = \\lceil 132.028956 \\rceil = 133 $$\n因此，需要最少 133 个样本才能满足指定的错误概率要求。", "answer": "$$\\boxed{133}$$", "id": "1630537"}]}