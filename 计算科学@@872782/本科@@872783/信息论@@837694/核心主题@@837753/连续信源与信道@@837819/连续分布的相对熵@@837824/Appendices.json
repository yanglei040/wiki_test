{"hands_on_practices": [{"introduction": "掌握一个新概念的最佳方式之一就是从最简单的例子开始。这个练习将通过计算两个均匀分布之间的相对熵来帮助你建立基本直觉。我们将探讨当一个概率模型被一个覆盖范围更广的新模型所取代时，所产生的信息散度。[@problem_id:1655218]", "problem": "假设一个关于连续随机变量 $X$ 的初始概率模型，记为 $P$，假定 $X$ 在区间 $[a, b]$ 上均匀分布。随后，一个修正后的模型，记为 $Q$，提出 $X$ 转而在一个更大的区间 $[c, d]$ 上均匀分布。已知初始区间完全包含在修正后的区间内，即 $c \\le a  b \\le d$。\n\n计算从初始模型 $P$ 到修正模型 $Q$ 的 KL 散度 $D_{KL}(P || Q)$。\n\n请用 $a, b, c$ 和 $d$ 将您的答案表示为一个符号表达式。", "solution": "令 $P$ 为 $\\operatorname{Unif}[a,b]$ 分布，$Q$ 为 $\\operatorname{Unif}[c,d]$ 分布，且 $c \\le a  b \\le d$。相应的概率密度函数为\n$$\np(x)=\\begin{cases}\n\\frac{1}{b-a},  x\\in[a,b],\\\\\n0,  \\text{otherwise},\n\\end{cases}\n\\quad\nq(x)=\\begin{cases}\n\\frac{1}{d-c},  x\\in[c,d],\\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n对于连续分布，库尔贝克-莱布勒散度为\n$$\nD_{KL}(P\\|Q)=\\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx.\n$$\n因为 $[a,b]\\subseteq[c,d]$，我们有对于所有 $x\\in[a,b]$，$q(x)=\\frac{1}{d-c}$，并且在 $[a,b]$ 之外 $p(x)=0$。因此，\n$$\nD_{KL}(P\\|Q)=\\int_{a}^{b}\\frac{1}{b-a}\\,\\ln\\!\\left(\\frac{\\frac{1}{b-a}}{\\frac{1}{d-c}}\\right)\\,dx\n=\\int_{a}^{b}\\frac{1}{b-a}\\,\\ln\\!\\left(\\frac{d-c}{b-a}\\right)\\,dx.\n$$\n被积函数是常数，所以\n$$\nD_{KL}(P\\|Q)=\\ln\\!\\left(\\frac{d-c}{b-a}\\right)\\int_{a}^{b}\\frac{1}{b-a}\\,dx\n=\\ln\\!\\left(\\frac{d-c}{b-a}\\right)\\cdot\\frac{b-a}{b-a}\n=\\ln\\!\\left(\\frac{d-c}{b-a}\\right).\n$$", "answer": "$$\\boxed{\\ln\\!\\left(\\frac{d-c}{b-a}\\right)}$$", "id": "1655218"}, {"introduction": "在实际应用中，我们经常用一个分布来近似另一个分布，例如用计算上更简单的分布来替代复杂的目标分布。这个练习将计算当用标准拉普拉斯分布来近似标准正态分布时所损失的信息量。通过这个计算，你将体会到相对熵如何作为一种量化工具，来衡量模型近似的优劣。[@problem_id:1655225]", "problem": "在信息论中，Kullback-Leibler (KL) 散度，也称为相对熵，衡量一个概率分布与第二个参考概率分布的差异。对于定义在实数上的两个连续概率分布 $P$ 和 $Q$，其对应的概率密度函数 (PDFs) 分别为 $p(x)$ 和 $q(x)$，从 $P$ 到 $Q$ 的 KL 散度定义为：\n$$D_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\n考虑一个随机变量，其真实分布是标准正态分布，我们记为 $P$。其概率密度函数由下式给出：\n$$p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)$$\n假设我们使用标准拉普拉斯分布来近似这个分布，我们记为 $Q$。其概率密度函数由下式给出：\n$$q(x) = \\frac{1}{2} \\exp(-|x|)$$\n计算 KL 散度 $D_{\\text{KL}}(P \\| Q)$，它量化了使用 $Q$ 近似 $P$ 时丢失的信息。请将您的答案表示为单个闭式解析表达式。", "solution": "我们从连续分布的 KL 散度定义开始：\n$$\nD_{\\text{KL}}(P \\| Q) = \\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx\n= \\mathbb{E}_{P}[\\ln p(X)] - \\mathbb{E}_{P}[\\ln q(X)].\n$$\n对于标准正态分布 $P$，其 $p(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{x^{2}}{2}\\right)$，计算\n$$\n\\ln p(x) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{x^{2}}{2},\n$$\n所以\n$$\n\\mathbb{E}_{P}[\\ln p(X)] = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\mathbb{E}_{P}[X^{2}] = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2},\n$$\n因为对于标准正态分布，$\\mathbb{E}_{P}[X^{2}] = 1$。\n\n对于标准拉普拉斯分布 $Q$，其 $q(x) = \\frac{1}{2}\\exp(-|x|)$，我们有\n$$\n\\ln q(x) = -\\ln 2 - |x|,\n$$\n因此\n$$\n\\mathbb{E}_{P}[\\ln q(X)] = -\\ln 2 - \\mathbb{E}_{P}[|X|].\n$$\n为了计算 $X \\sim \\mathcal{N}(0,1)$ 时的 $\\mathbb{E}_{P}[|X|]$，利用对称性：\n$$\n\\mathbb{E}_{P}[|X|] = 2\\int_{0}^{\\infty} x\\,\\frac{1}{\\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{x^{2}}{2}\\right)\\,dx.\n$$\n令 $u = \\frac{x^{2}}{2}$，则 $du = x\\,dx$。那么\n$$\n\\mathbb{E}_{P}[|X|] = \\frac{2}{\\sqrt{2\\pi}}\\int_{0}^{\\infty} \\exp(-u)\\,du = \\frac{2}{\\sqrt{2\\pi}}\\cdot 1 = \\sqrt{\\frac{2}{\\pi}}.\n$$\n因此，\n$$\nD_{\\text{KL}}(P \\| Q) = \\left(-\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\right) - \\left(-\\ln 2 - \\sqrt{\\frac{2}{\\pi}}\\right)\n= \\sqrt{\\frac{2}{\\pi}} + \\ln 2 - \\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}.\n$$\n化简对数项，\n$$\n\\ln 2 - \\frac{1}{2}\\ln(2\\pi) = \\frac{1}{2}\\ln 4 - \\frac{1}{2}\\ln(2\\pi) = \\frac{1}{2}\\ln\\left(\\frac{4}{2\\pi}\\right) = \\frac{1}{2}\\ln\\left(\\frac{2}{\\pi}\\right) = -\\frac{1}{2}\\ln\\left(\\frac{\\pi}{2}\\right),\n$$\n这给出了闭式表达式\n$$\nD_{\\text{KL}}(P \\| Q) = \\sqrt{\\frac{2}{\\pi}} - \\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right) - \\frac{1}{2}.\n$$", "answer": "$$\\boxed{\\sqrt{\\frac{2}{\\pi}}-\\frac{1}{2}\\ln\\!\\left(\\frac{\\pi}{2}\\right)-\\frac{1}{2}}$$", "id": "1655225"}, {"introduction": "相对熵的一个关键特性是其不对称性，即 $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$，这意味着它不能直接作为“距离”度量。为了克服这一点，我们可以构造对称的度量，例如杰弗里斯散度。本练习将计算两个仅均值不同而方差相同的高斯分布之间的杰弗里斯散度，从而揭示信息散度与统计距离之间深刻而优雅的联系。[@problem_id:1655241]", "problem": "在信息论中，衡量两个概率分布之间的“距离”或不相似性的一种方法是使用散度度量。考虑定义在实数轴上的随机变量 $x$ 的两个连续概率密度函数 $p(x)$ 和 $q(x)$。从 $q$ 到 $p$ 的 Kullback-Leibler (KL) 散度，或称相对熵，定义为：\n$$D_{KL}(p||q) = \\int_{-\\infty}^{\\infty} p(x) \\ln\\left(\\frac{p(x)}{q(x)}\\right) dx$$\nKL 散度不是对称的，也就是说，通常情况下 $D_{KL}(p||q) \\neq D_{KL}(q||p)$。为了创建一个对称的度量，Jeffreys 散度 $J(p, q)$ 被定义为两个 KL 散度之和：\n$$J(p, q) = D_{KL}(p||q) + D_{KL}(q||p)$$\n设 $p(x)$ 是均值为 $\\mu_A$、方差为 $\\sigma^2$ 的单变量正态分布的概率密度函数。设 $q(x)$ 是另一个均值为 $\\mu_B$ 但方差同为 $\\sigma^2$ 的单变量正态分布的概率密度函数。\n\n确定这两个正态分布的 Jeffreys 散度 $J(p, q)$。请用 $\\mu_A$、$\\mu_B$ 和 $\\sigma$ 的符号表达式表示你的答案。", "solution": "我们从 Kullback-Leibler 散度的定义开始：\n$$\nD_{KL}(p\\|q)=\\int_{-\\infty}^{\\infty} p(x)\\,\\ln\\!\\left(\\frac{p(x)}{q(x)}\\right)\\,dx=\\mathbb{E}_{p}\\!\\left[\\ln p(X)-\\ln q(X)\\right].\n$$\n设 $p(x)$ 和 $q(x)$ 分别是方差相同为 $\\sigma^{2}$，均值分别为 $\\mu_{A}$ 和 $\\mu_{B}$ 的单变量正态分布的密度函数：\n$$\np(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu_{A})^{2}}{2\\sigma^{2}}\\right),\\quad\nq(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu_{B})^{2}}{2\\sigma^{2}}\\right).\n$$\n取对数，\n$$\n\\ln p(x)=-\\ln(\\sqrt{2\\pi}\\,\\sigma)-\\frac{(x-\\mu_{A})^{2}}{2\\sigma^{2}},\\quad\n\\ln q(x)=-\\ln(\\sqrt{2\\pi}\\,\\sigma)-\\frac{(x-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\n因此，\n$$\n\\ln p(x)-\\ln q(x)=-\\frac{(x-\\mu_{A})^{2}-(x-\\mu_{B})^{2}}{2\\sigma^{2}}=\\frac{(x-\\mu_{B})^{2}-(x-\\mu_{A})^{2}}{2\\sigma^{2}}.\n$$\n在 $p$ 下取期望，\n$$\nD_{KL}(p\\|q)=\\mathbb{E}_{p}\\!\\left[\\frac{(X-\\mu_{B})^{2}-(X-\\mu_{A})^{2}}{2\\sigma^{2}}\\right]\n=\\frac{1}{2\\sigma^{2}}\\left(\\mathbb{E}_{p}[(X-\\mu_{B})^{2}]-\\mathbb{E}_{p}[(X-\\mu_{A})^{2}]\\right).\n$$\n使用 $\\mathbb{E}_{p}[X]=\\mu_{A}$ 和 $\\operatorname{Var}_{p}(X)=\\sigma^{2}$，以及恒等式 $\\mathbb{E}[(X-a)^{2}]=\\operatorname{Var}(X)+(\\mathbb{E}[X]-a)^{2}$，我们得到\n$$\n\\mathbb{E}_{p}[(X-\\mu_{A})^{2}]=\\sigma^{2},\\quad\n\\mathbb{E}_{p}[(X-\\mu_{B})^{2}]=\\sigma^{2}+(\\mu_{A}-\\mu_{B})^{2}.\n$$\n因此，\n$$\nD_{KL}(p\\|q)=\\frac{1}{2\\sigma^{2}}\\left(\\sigma^{2}+(\\mu_{A}-\\mu_{B})^{2}-\\sigma^{2}\\right)\n=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\n根据上述推导中 $p$ 和 $q$ 角色的对称性（交换 $\\mu_{A}$ 和 $\\mu_{B}$ 不改变表达式），我们同样有\n$$\nD_{KL}(q\\|p)=\\frac{(\\mu_{B}-\\mu_{A})^{2}}{2\\sigma^{2}}=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}.\n$$\n所以，定义为 $J(p,q)=D_{KL}(p\\|q)+D_{KL}(q\\|p)$ 的 Jeffreys 散度为\n$$\nJ(p,q)=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}+\\frac{(\\mu_{A}-\\mu_{B})^{2}}{2\\sigma^{2}}=\\frac{(\\mu_{A}-\\mu_{B})^{2}}{\\sigma^{2}}.\n$$", "answer": "$$\\boxed{\\frac{(\\mu_{A}-\\mu_{B})^{2}}{\\sigma^{2}}}$$", "id": "1655241"}]}