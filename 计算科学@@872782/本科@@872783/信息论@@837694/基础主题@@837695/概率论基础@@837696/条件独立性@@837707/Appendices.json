{"hands_on_practices": [{"introduction": "这个练习探讨了一个“共因”结构，这是统计推断中的一个基本模型。两个看似相关的观测结果，其关联可能源于一个共同的、未被观测到的原因。通过对这个共因进行条件化，我们可以揭示出它们之间内在的条件独立性，即$I(X;Y|Z)=0$。这个练习将让你计算两个观测量$X$和$Y$之间的互信息$I(X;Y)$，以量化它们在不知道共因$Z$的情况下的依赖程度。[@problem_id:1612658]", "problem": "考虑一个简单的通信系统，其中信源状态 $Z$ 可以是“低”（由二进制值0表示）或“高”（由1表示）。信源以相等的概率选择一个状态，即 $p(Z=0) = p(Z=1) = 0.5$。状态 $Z$ 被同时传输到两个物理上分离且独立的接收器A和B。\n\n到两个接收器的传输信道都是有噪声的。在接收器A处的观测值用二进制变量 $X$ 表示，在接收器B处的观测值用 $Y$ 表示。对于每个接收器，存在一个恒定的比特翻转错误概率，$\\alpha = 1/4$。也就是说，接收器记录的状态与传输状态不同的概率是 $p(X \\neq Z) = \\alpha$ 和 $p(Y \\neq Z) = \\alpha$。在给定信源状态 $Z$ 的条件下，两个信道中的错误是相互独立的。\n\n计算两个接收器观测值之间的互信息 $I(X;Y)$。请用一个以比特为单位的闭式解析表达式来表示你的最终答案。", "solution": "设 $Z \\in \\{0,1\\}$，且 $p(Z=0)=p(Z=1)=\\frac{1}{2}$。观测值 $X$ 和 $Y$ 是由独立的二进制对称信道（BSC）产生的，其交叉概率为 $\\alpha=\\frac{1}{4}$，因此 $p(X \\neq Z \\mid Z)=\\alpha$ 且 $p(Y \\neq Z \\mid Z)=\\alpha$，并且 $X \\perp Y \\mid Z$。\n\n首先，计算 $X$ 和 $Y$ 的边缘分布。对于 $X$，\n$$\np(X=1) = \\sum_{z \\in \\{0,1\\}} p(Z=z)\\,p(X=1 \\mid Z=z) = \\frac{1}{2}\\big[\\alpha+(1-\\alpha)\\big] = \\frac{1}{2},\n$$\n类似地，$p(X=0)=\\frac{1}{2}$，所以 $H(X)=1$ 比特；同理 $H(Y)=1$ 比特。\n\n接下来，计算 $(X,Y)$ 的联合分布。首先计算 $X$ 和 $Y$ 相同的概率 $q$ 和不同的概率 $r$。利用在给定 $Z$ 下的条件独立性：\n$q = p(X=Y) = \\sum_{z} p(X=Y|Z=z)p(Z=z)$。由于 $p(X=Y|Z=z) = p(X=z, Y=z|Z=z) + p(X \\neq z, Y \\neq z|Z=z) = (1-\\alpha)^2 + \\alpha^2$ 对于所有 $z$ 都成立，因此 $q = (1-\\alpha)^2 + \\alpha^2$。\n$r = p(X \\neq Y) = 1-q = 2\\alpha(1-\\alpha)$。\n根据对称性，四个联合概率为\n$$\np(0,0) = p(1,1) = \\frac{q}{2}, \\quad p(0,1) = p(1,0) = \\frac{r}{2}.\n$$\n\n联合熵为\n$$\nH(X,Y) = -2 \\cdot \\frac{q}{2} \\log_{2}\\!\\left(\\frac{q}{2}\\right) - 2 \\cdot \\frac{r}{2} \\log_{2}\\!\\left(\\frac{r}{2}\\right)\n= - q \\log_{2}\\!\\left(\\frac{q}{2}\\right) - r \\log_{2}\\!\\left(\\frac{r}{2}\\right).\n$$\n使用 $\\log_{2}\\!\\left(\\frac{q}{2}\\right) = \\log_{2} q - 1$ 和 $\\log_{2}\\!\\left(\\frac{r}{2}\\right) = \\log_{2} r - 1$，以及 $q+r=1$，可将其简化为\n$$\nH(X,Y) = 1 - \\big(q \\log_{2} q + r \\log_{2} r\\big).\n$$\n因此，互信息为\n$$\nI(X;Y) = H(X) + H(Y) - H(X,Y) = 2 - \\Big[1 - \\big(q \\log_{2} q + r \\log_{2} r\\big)\\Big]\n= 1 + q \\log_{2} q + r \\log_{2} r.\n$$\n\n代入 $\\alpha=\\frac{1}{4}$。那么\n$$\nq = (1-\\frac{1}{4})^2 + (\\frac{1}{4})^2 = (\\frac{3}{4})^2 + (\\frac{1}{4})^2 = \\frac{9}{16} + \\frac{1}{16} = \\frac{10}{16} = \\frac{5}{8}.\n$$\n$$\nr = 1 - q = \\frac{3}{8}.\n$$\n因此\n$$\nI(X;Y) = 1 + \\frac{5}{8} \\log_{2}\\!\\left(\\frac{5}{8}\\right) + \\frac{3}{8} \\log_{2}\\!\\left(\\frac{3}{8}\\right),\n$$\n这是一个以比特为单位的精确闭式表达式。等价地，$I(X;Y) = 1 - h_{2}\\!\\left(\\frac{3}{8}\\right)$，其中 $h_{2}$ 表示以比特为单位的二进制熵函数。", "answer": "$$\\boxed{1+\\frac{5}{8}\\log_{2}\\left(\\frac{5}{8}\\right)+\\frac{3}{8}\\log_{2}\\left(\\frac{3}{8}\\right)}$$", "id": "1612658"}, {"introduction": "与“共因”结构相对的是“对撞”(collider)结构，它会产生一种被称为“解释消除”效应的有趣现象。在这个练习中，两个独立的变量$X$和$Y$共同决定了第三个变量$Z$。我们将看到，虽然$X$和$Y$本身是独立的（$I(X;Y)=0$），但一旦我们观测到它们的共同结果$Z$，它们之间就会产生依赖关系。计算条件互信息$I(X;Y|Z)$将清晰地展示这一现象。[@problem_id:1612630]", "problem": "在许多现代数据系统中，信息是通过并行流来处理的。考虑这样一个系统的简化模型，它有两个独立的二进制数据流，由随机变量 $X$ 和 $Y$ 表示。这两个流都被设计为统计上无偏的，这意味着比特为 0 或 1 的概率是相等的。因此，$P(X=0) = P(X=1) = 1/2$，且 $P(Y=0) = P(Y=1) = 1/2$。一个监控过程并不直接观察 $X$ 和 $Y$，而是观察第三个二进制变量 $Z$，它是由两个流中相应比特进行异或（XOR）运算生成的：$Z = X \\oplus Y$。\n\n一位工程师正在分析该系统中的信息流。一个关键问题是，在给定观测输出的条件下，两个初始流共享多少信息。具体来说，计算在观测到组合流 $Z$ 的情况下，两个流 $X$ 和 $Y$ 之间的条件互信息 $I(X;Y \\mid Z)$。\n\n用比特（bits）表示你的答案。", "solution": "我们已知两个独立的无偏二进制随机变量 $X$ 和 $Y$，满足 $P(X=0)=P(X=1)=\\frac{1}{2}$ 和 $P(Y=0)=P(Y=1)=\\frac{1}{2}$，且 $Z=X\\oplus Y$。我们要计算以比特为单位的条件互信息 $I(X;Y\\mid Z)$。\n\n根据定义，\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z).\n$$\n首先计算 $H(X\\mid Z)$。我们有\n$$\nP(Z=0)=P(X=0,Y=0)+P(X=1,Y=1)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2},\n$$\n$$\nP(Z=1)=P(X=0,Y=1)+P(X=1,Y=0)=\\frac{1}{4}+\\frac{1}{4}=\\frac{1}{2}.\n$$\n对于 $z=0$，\n$$\nP(X=0\\mid Z=0)=\\frac{P(X=0,Z=0)}{P(Z=0)}=\\frac{P(X=0,Y=0)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\n类似地，$P(X=1\\mid Z=0)=\\frac{1}{2}$。因此 $H(X\\mid Z=0)=1$ 比特。\n\n对于 $z=1$，\n$$\nP(X=0\\mid Z=1)=\\frac{P(X=0,Z=1)}{P(Z=1)}=\\frac{P(X=0,Y=1)}{\\frac{1}{2}}=\\frac{\\frac{1}{4}}{\\frac{1}{2}}=\\frac{1}{2},\n$$\n类似地，$P(X=1\\mid Z=1)=\\frac{1}{2}$。因此 $H(X\\mid Z=1)=1$ 比特。\n\n所以，\n$$\nH(X\\mid Z)=\\sum_{z\\in\\{0,1\\}}P(Z=z)\\,H(X\\mid Z=z)=\\frac{1}{2}\\cdot 1+\\frac{1}{2}\\cdot 1=1.\n$$\n\n接下来计算 $H(X\\mid Y,Z)$。因为 $Z=X\\oplus Y$，所以 $X$ 的值由 $Y$ 和 $Z$ 通过 $X=Y\\oplus Z$ 确定。因此\n$$\nH(X\\mid Y,Z)=0.\n$$\n\n将这些结果放在一起，\n$$\nI(X;Y\\mid Z)=H(X\\mid Z)-H(X\\mid Y,Z)=1-0=1.\n$$\n因此条件互信息是 $1$ 比特。", "answer": "$$\\boxed{1}$$", "id": "1612630"}, {"introduction": "条件独立性不仅仅取决于变量间的结构关系，有时还与系统底层的概率参数密切相关。这个更深入的练习将挑战你对条件独立性的理解。它提出了一个情景：对于三个独立的随机变量，当我们知道它们的和是偶数时，其中两个变量是否还保持独立？通过解决这个问题，你将发现条件独立性有时是一个微妙的属性，它可能只在特定的概率条件下才成立。[@problem_id:1612639]", "problem": "考虑三个随机变量 $X, Y, Z$，它们代表三次独立同分布的伯努利试验的结果。每次试验成功（即变量取值为1）的概率为 $p$，其中 $P(\\text{试验}=1) = p$，失败（取值为0）的概率为 $P(\\text{试验}=0) = 1-p$。参数 $p$ 被限制在开区间 $(0, 1)$ 内。\n\n设 $E$ 为结果之和为偶数的事件，即 $E$ 是 $X+Y+Z$ 为偶整数的事件。\n\n在什么条件下，随机变量 $X$ 和 $Y$ 在给定事件 $E$ 的情况下是条件独立的？\n\nA. 对于任何 $p \\in (0,1)$ 的值，给定 $E$，$X$ 和 $Y$ 总是条件独立的。\n\nB. 对于任何 $p \\in (0,1)$ 的值，给定 $E$，$X$ 和 $Y$ 永远不是条件独立的。\n\nC. 给定 $E$，$X$ 和 $Y$ 条件独立当且仅当 $p = 1/2$。\n\nD. 给定 $E$，$X$ 和 $Y$ 条件独立当且仅当 $p$ 是一个有理数。\n\nE. 在不知道 $Z$ 的具体结果的情况下，无法确定给定 $E$ 时 $X$ 和 $Y$ 的条件独立性。", "solution": "设 $X,Y,Z$ 是独立的 $\\operatorname{Bernoulli}(p)$ 随机变量，其中 $p \\in (0,1)$。定义 $E=\\{X+Y+Z \\text{ 是偶数}\\}$。事件 $E$ 发生当且仅当变量中恰好有0个或恰好有2个为1，所以\n$$\nP(E)=P(X=0,Y=0,Z=0)+P(X,Y,Z\\text{中恰好有两个为}1)\n=(1-p)^{3}+3p^{2}(1-p).\n$$\n等价地，\n$$\nP(E)=(1-p)\\big((1-p)^{2}+3p^{2}\\big).\n$$\n\n计算以 $E$ 为条件的联合概率：\n- 如果 $X=1$ 且 $Y=1$，那么为了使 $E$ 发生，$Z$ 必须为 $0$，所以\n$$\nP(X=1,Y=1,E)=P(X=1,Y=1,Z=0)=p^{2}(1-p).\n$$\n- 如果 $X=1$ 且 $Y=0$，那么 $Z$ 必须为 $1$，所以\n$$\nP(X=1,Y=0,E)=P(X=1,Y=0,Z=1)=p(1-p)p=p^{2}(1-p).\n$$\n- 根据对称性，\n$$\nP(X=0,Y=1,E)=p^{2}(1-p), \\quad P(X=0,Y=0,E)=(1-p)^{3}.\n$$\n因此，\n$$\nP(X=1,Y=1 \\mid E)=\\frac{p^{2}(1-p)}{P(E)}, \\quad\nP(X=1,Y=0 \\mid E)=\\frac{p^{2}(1-p)}{P(E)},\n$$\n$$\nP(X=0,Y=1 \\mid E)=\\frac{p^{2}(1-p)}{P(E)}, \\quad\nP(X=0,Y=0 \\mid E)=\\frac{(1-p)^{3}}{P(E)}.\n$$\n\n在条件 $E$ 下的边缘概率是\n$$\nP(X=1 \\mid E)=P(X=1,Y=1 \\mid E)+P(X=1,Y=0 \\mid E)=\\frac{2p^{2}(1-p)}{P(E)},\n$$\n并且根据对称性，$P(Y=1 \\mid E)=\\frac{2p^{2}(1-p)}{P(E)}$。\n\n为了条件独立性，我们需要（例如）\n$$\nP(X=1,Y=1 \\mid E)=P(X=1 \\mid E)\\,P(Y=1 \\mid E).\n$$\n代入上述表达式可得\n$$\n\\frac{p^{2}(1-p)}{P(E)}=\\left(\\frac{2p^{2}(1-p)}{P(E)}\\right)^{2}.\n$$\n两边同乘以 $P(E)^{2}$ 并注意到 $p \\in (0,1)$，所以我们可以除以 $p^{2}(1-p)>0$，得到\n$$\nP(E)=4p^{2}(1-p).\n$$\n使用 $P(E)=(1-p)\\big((1-p)^{2}+3p^{2}\\big)$ 并除以 $(1-p)>0$ 得到\n$$\n(1-p)^{2}+3p^{2}=4p^{2} \\quad \\Longrightarrow \\quad 1-2p+p^2+3p^{2}=4p^{2} \\quad \\Longrightarrow \\quad 1-2p=0,\n$$\n所以\n$$\np=\\frac{1}{2}.\n$$\n\n充分性成立：当 $p=\\frac{1}{2}$ 时，上述四个联合条件概率均相等，每个都为 $\\frac{1}{4}$，并且边缘概率为 $\\frac{1}{2}$，所以对于所有 $a,b \\in \\{0,1\\}$，$P(X=a,Y=b \\mid E)=P(X=a \\mid E)P(Y=b \\mid E)$。\n\n因此，$X$ 和 $Y$ 在给定事件 $E$ 的情况下条件独立当且仅当 $p=\\frac{1}{2}$，这对应于选项 C。", "answer": "$$\\boxed{C}$$", "id": "1612639"}]}