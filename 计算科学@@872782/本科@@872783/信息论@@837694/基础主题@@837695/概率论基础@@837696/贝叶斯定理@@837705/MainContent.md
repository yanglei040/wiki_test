## 引言
在科学与工程的众多领域中，处理不确定性是推动发现与创新的核心挑战。我们如何量化并更新我们对世界的信念？当新的数据或证据出现时，我们应如何理性地调整我们的判断？贝叶斯定理为这些基本问题提供了一个优雅而强大的数学框架。它不仅仅是概率论中的一个简单公式，更是一种系统性的推理哲学，使我们能够将先前的知识与新的观测相结合，从而在不确定性的迷雾中做出更明智的推断。本文旨在揭开[贝叶斯定理](@entry_id:151040)的神秘面纱，阐明其在信息理论和现代数据科学中的核心地位。

本文将引导读者踏上一段从理论到实践的旅程。在“**原理与机制**”一章中，我们将深入剖析贝叶斯公式的每一个组成部分，并通过实例说明其在离散与连续场景下的推断过程，探索证据累积、结构化[模型推断](@entry_id:636556)以及参数学习的内在逻辑。接着，在“**应用与跨学科联系**”一章中，我们将视野拓宽至现实世界，展示贝叶斯思想如何驱动[医学诊断](@entry_id:169766)、[遗传分析](@entry_id:167901)、信号处理和机器学习等多个领域的进步。最后，在“**动手实践**”部分，你将有机会通过解决具体问题来巩固所学知识。通过这三个章节的学习，你将不仅掌握[贝叶斯定理](@entry_id:151040)的计算方法，更能领会其作为一种通用推理工具的深刻内涵。

## 原理与机制

在本章中，我们将深入探讨贝叶斯定理的原理及其在信息理论和相关领域中的广泛应用机制。贝叶斯定理不仅是概率论中的一个基本恒等式，更是一种强大的推断框架，它使我们能够基于新证据系统地更新我们的信念。我们将从其基本构成开始，逐步扩展到更复杂的模型和应用场景中，揭示其作为一种通用推理工具的内在力量。

### [贝叶斯推断](@entry_id:146958)的剖析

在不确定性下进行推理是科学与工程的核心挑战。[贝叶斯定理](@entry_id:151040)为此提供了一个规范性的数学框架。假设我们有一个我们关心的假设或未知的[隐变量](@entry_id:150146) $H$（例如，一个未观测到的信号源状态），以及我们观测到的证据或数据 $E$（例如，一个接收到的信号）。[贝叶斯定理](@entry_id:151040)将这两者联系起来：

$$ P(H|E) = \frac{P(E|H)P(H)}{P(E)} $$

这个公式的每一个组成部分都有一个直观的名称和意义：

*   **后验概率 (Posterior Probability)** $P(H|E)$：这是我们在观测到证据 $E$ 之后，关于假设 $H$ 为真的更新后的信念。这是推断的目标。
*   **[先验概率](@entry_id:275634) (Prior Probability)** $P(H)$：这是我们在获得任何证据之前，关于假设 $H$ 为真的初始信念。它代表了我们的背景知识或初步假设。
*   **似然 (Likelihood)** $P(E|H)$：这描述了在假设 $H$ 为真的前提下，我们观测到证据 $E$ 的概率。它将我们的假设与数据联系起来。值得注意的是，似然是关于证据 $E$ 的函数，而不是关于假设 $H$ 的[概率分布](@entry_id:146404)。
*   **证据 (Evidence)** $P(E)$：也称为**[边际似然](@entry_id:636856) (Marginal Likelihood)**，这是观测到证据 $E$ 的总概率，无论假设 $H$ 是否为真。它通常通过对所有可能的假设进行求和或积分来计算，使用[全概率公式](@entry_id:194231)：$P(E) = \sum_{H'} P(E|H')P(H')$。证据项起到归一化常数的作用，确保后验概率的总和为1。

为了具体理解这些概念，让我们考虑一个数字存储设备，它存储二进制比特位 $X$（0 或 1）。假设根据历史数据，我们知道一个比特位为 0 的先验概率是 $P(X=0) = \alpha$。当读取该比特时，可能会发生“擦除”现象，我们用 '?' 表示。我们知道，当存储的比特为 0 时发生擦除的概率是 $P(Y='?'|X=0) = p_0$，而当存储的比特为 1 时发生擦除的概率是 $P(Y='?'|X=1) = p_1$。现在，如果我们观测到了一次擦除，即证据 $E$ 是 $Y='?'$，我们希望更新我们关于原始比特是 0 的信念，即计算后验概率 $P(X=0|Y='?')$ [@problem_id:1603705]。

根据[贝叶斯定理](@entry_id:151040)：

$$ P(X=0|Y='?') = \frac{P(Y='?'|X=0) P(X=0)}{P(Y='?')} $$

这里的各个部分对应如下：
*   先验概率: $P(X=0) = \alpha$
*   似然: $P(Y='?'|X=0) = p_0$

证据项 $P(Y='?')$ 是观察到擦除的总概率，它可以通过对所有可能的原始比特状态进行加权求和来得到：
$$ P(Y='?') = P(Y='?'|X=0)P(X=0) + P(Y='?'|X=1)P(X=1) = p_0\alpha + p_1(1-\alpha) $$

将这些部分组合起来，我们得到[后验概率](@entry_id:153467)：
$$ P(X=0|Y='?') = \frac{\alpha p_0}{\alpha p_0 + p_1(1-\alpha)} $$
这个表达式精确地量化了在观察到“擦除”这一不确定性证据后，我们对原始比特为0的信念应该如何调整。如果当比特为0时更容易发生擦除（即 $p_0$ 较大），那么观察到擦除会增强我们认为原始比特是0的信念。

### 证据累积的逻辑

贝叶斯推断的一个关键优势是它能够自然地处理多个证据。当新数据到来时，我们可以将上一次推断的[后验概率](@entry_id:153467)作为下一次推断的[先验概率](@entry_id:275634)，从而实现信念的顺序更新。当多个证据是**条件独立**的时，这个过程尤为简洁。条件独立意味着，给定一个假设 $H$，证据 $E_1, E_2, \dots, E_n$ 的出现是[相互独立](@entry_id:273670)的。数学上，这表示：

$$ P(E_1, E_2, \dots, E_n | H) = \prod_{i=1}^{n} P(E_i|H) $$

在这种情况下，整个证据序列的[联合似然](@entry_id:750952)就是各个证据似然的乘积。

考虑一个深空探测器向地球发送其子系统状态 $X$ 的例子，其中 $X=0$ 代表“标称操作”，$X=1$ 代表“警报条件”。根据长期运行数据，我们有先验概率 $P(X=0)=0.95$。为了提高通信可靠性，探测器将同一状态位 $X$ 连续发送两次。通信信道是一个[二进制对称信道 (BSC)](@entry_id:274227)，每个比特有 $\epsilon=0.10$ 的概率被翻转。假设地球上的任务控制中心收到了序列 $(Y_1, Y_2)=(0,0)$ [@problem_id:1603696]。我们想计算在收到这个证据后，子系统处于“标称操作”状态的后验概率 $P(X=0 | Y_1=0, Y_2=0)$。

由于两次传输是独立的事件，我们可以使用[条件独立性](@entry_id:262650)。给定 $X=0$，收到 $(0,0)$ 的[似然](@entry_id:167119)是：
$$ P(Y_1=0, Y_2=0 | X=0) = P(Y_1=0|X=0) P(Y_2=0|X=0) = (1-\epsilon)^2 $$
而给定 $X=1$，收到 $(0,0)$ 的[似然](@entry_id:167119)是：
$$ P(Y_1=0, Y_2=0 | X=1) = P(Y_1=0|X=1) P(Y_2=0|X=1) = \epsilon^2 $$

应用[贝叶斯定理](@entry_id:151040)，[后验概率](@entry_id:153467)为：
$$ P(X=0 | (0,0)) = \frac{P((0,0)|X=0)P(X=0)}{P((0,0)|X=0)P(X=0) + P((0,0)|X=1)P(X=1)} $$
代入数值 $P(X=0)=0.95$ 和 $\epsilon=0.10$，我们得到：
$$ P(X=0 | (0,0)) = \frac{(0.90)^2 \times 0.95}{(0.90)^2 \times 0.95 + (0.10)^2 \times 0.05} \approx 0.9994 $$
初始时我们有 $95\%$ 的把握认为系统正常，在连续两次收到确认“正常”的信号后，我们的信心提升到了 $99.94\%$。这个例子生动地展示了重复的、一致的证据如何极大地增强我们的信念强度。

### 结构化模型中的推断

[贝叶斯推断](@entry_id:146958)的框架可以优雅地扩展到更复杂的、具有内部结构的模型中，例如涉及多个阶段的系统或具有不确定参数的模型。

#### 级联推断

在许多现实世界的系统中，信息是通过一系列阶段传递的，形成一个**[马尔可夫链](@entry_id:150828) (Markov Chain)**。例如，一个信号 $X$ 经过第一个中继站变为 $Y$，再经过第二个中继站变为 $Z$，即 $X \to Y \to Z$。这意味着 $Z$ 的状态仅依赖于 $Y$，而与 $X$ 无关（给定 $Y$ 的情况下）。如果我们只观测到最终的输出 $Z$，并希望推断原始信号 $X$，我们需要在贝叶斯框架内处理这个中间层。

假设一个信源生成三种信号类型 $X \in \{1, 2, 3\}$，具有先验分布 $P(X)$。信号经过两个噪声中继，其特性由转移[概率矩阵](@entry_id:274812) $T_{Y|X}$ 和 $T_{Z|Y}$ 描述。如果我们观测到 $Z=3$，并想计算关于 $X$ 的[后验分布](@entry_id:145605) $P(X=j|Z=3)$ for $j=1,2,3$ [@problem_id:1603695]，我们需要计算[似然](@entry_id:167119) $P(Z=3|X=j)$。

由于 $X$ 和 $Z$ 之间由 $Y$ 隔开，我们必须通过对所有可能的中间状态 $Y$ 进行求和（边际化）来建立它们之间的联系：
$$ P(Z=3|X=j) = \sum_{y=1}^{3} P(Z=3, Y=y | X=j) $$
利用马尔可夫性质 $P(Z=3|Y=y, X=j) = P(Z=3|Y=y)$，我们得到：
$$ P(Z=3|X=j) = \sum_{y=1}^{3} P(Z=3|Y=y) P(Y=y|X=j) $$
这个计算本质上是[矩阵乘法](@entry_id:156035)的一个元素。一旦我们为每个 $j$ 计算出[似然](@entry_id:167119) $P(Z=3|X=j)$，我们就可以像之前一样应用贝叶斯定理来得到最终的[后验分布](@entry_id:145605) $P(X|Z=3)$。这个过程说明了如何在存在[潜变量](@entry_id:143771)或中间步骤的系统中进行推理：通过对我们未观测到的所有可能性进行积分或求和来构建有效的[似然](@entry_id:167119)。

#### [模型不确定性](@entry_id:265539)下的推断

有时，我们甚至不确定我们正在使用的模型本身的参数。例如，一个通信信道的噪声水平可能是变化的。假设一个信道可能处于两种状态之一：“良好”（概率为 $\alpha$，翻转率为 $\epsilon_g$）或“差”（概率为 $1-\alpha$，翻转率为 $\epsilon_b$）[@problem_id:1603709]。在这种情况下，信道状态本身就是一个我们无法直接观测的[隐变量](@entry_id:150146)。

如果我们发送一个比特 $X=0$ 并接收到 $Y=1$，我们想要计算原始比特为0的后验概率 $P(X=0|Y=1)$。这里的关键是，似然 $P(Y=1|X=0)$ 并不是一个固定的值，它取决于信道的潜在状态。为了得到一个总体的、有效的[似然](@entry_id:167119)，我们需要再次使用[全概率公式](@entry_id:194231)，这次是针对模型参数（信道状态 $S$）进行边际化：
$$ P(Y=1|X=0) = P(Y=1|X=0, S=\text{良好})P(S=\text{良好}) + P(Y=1|X=0, S=\text{差})P(S=\text{差}) $$
$$ P(Y=1|X=0) = \epsilon_g \alpha + \epsilon_b (1-\alpha) $$
这实际上是一个**混合模型 (Mixture Model)** 的例子，其中观测数据的生成过程是多个子模型的加权平均。计算出这个有效的[似然](@entry_id:167119)（以及相应的证据项 $P(Y=1)$）后，我们就可以应用贝叶斯定理来更新我们对发送比特 $X$ 的信念。这种方法让我们能够在模型参数本身存在不确定性的情况下进行稳健的推断。

### 连续变量的贝叶斯推断

[贝叶斯定理](@entry_id:151040)同样适用于连续的[随机变量](@entry_id:195330)，只需将[概率质量函数](@entry_id:265484) $P(\cdot)$ 替换为概率密度函数 (PDF) $p(\cdot)$，并将求和替换为积分。
$$ p(h|y) = \frac{p(y|h)p(h)}{p(y)} = \frac{p(y|h)p(h)}{\int p(y|h')p(h')dh'} $$
一个在工程和科学中极其重要的例子是**[线性高斯模型](@entry_id:268963) (Linear-Gaussian Model)**。考虑一个[无线通信](@entry_id:266253)系统，其中接收信号 $y$、信道增益 $h$、发送信号 $x$ 和噪声 $n$ 之间的关系为 $y = hx + n$ [@problem_id:1603703]。

假设我们对信道增益 $h$ 有一个先验信念，它服从均值为0、[方差](@entry_id:200758)为 $\sigma_h^2$ 的高斯分布，即 $h \sim \mathcal{N}(0, \sigma_h^2)$。噪声 $n$ 也服从均值为0、[方差](@entry_id:200758)为 $\sigma_n^2$ 的[高斯分布](@entry_id:154414)，即 $n \sim \mathcal{N}(0, \sigma_n^2)$，且与 $h$ 独立。给定一个已知的导频符号 $x$ 和观测到的接收信号 $y$，我们希望找到增益 $h$ 的后验分布 $p(h|y)$。

*   **先验**: $p(h) \propto \exp\left(-\frac{h^2}{2\sigma_h^2}\right)$
*   **似然**: 给定 $h$，则 $y$ 是一个均值为 $hx$，[方差](@entry_id:200758)为 $\sigma_n^2$ 的高斯[随机变量](@entry_id:195330)。因此，似然函数 $p(y|h)$ 为：
    $p(y|h) \propto \exp\left(-\frac{(y-hx)^2}{2\sigma_n^2}\right)$

后验分布 $p(h|y)$ 与先验和似然的乘积成正比：
$$ p(h|y) \propto \exp\left(-\frac{h^2}{2\sigma_h^2} - \frac{(y-hx)^2}{2\sigma_n^2}\right) $$
指数部分是 $h$ 的二次函数。通过**[配方法](@entry_id:265480) (completing the square)**，我们可以将这个指数重新整理成一个标准的高斯分布指数形式 $-\frac{(h-\mu_{\text{post}})^2}{2\sigma_{\text{post}}^2}$。这个代数过程揭示了后验分布也是一个高斯分布，其均值 $\mu_{\text{post}}$ 和[方差](@entry_id:200758) $\sigma_{\text{post}}^2$ 分别为：
$$ \mu_{\text{post}} = \frac{\sigma_h^2 x y}{x^2 \sigma_h^2 + \sigma_n^2}, \quad \sigma_{\text{post}}^2 = \frac{\sigma_h^2 \sigma_n^2}{x^2 \sigma_h^2 + \sigma_n^2} $$
这个结果非常深刻：[高斯先验](@entry_id:749752)和高斯似然的结合产生了高斯的后验。这被称为**共轭性 (conjugacy)**，我们将在下一节详细讨论。[后验均值](@entry_id:173826) $\mu_{\text{post}}$ 是先验均值（0）和数据驱动的估计（$y/x$）的加权平均，权重取决于先验的不确定性（$\sigma_h^2$）和数据的不确定性（$\sigma_n^2$）。后验[方差](@entry_id:200758) $\sigma_{\text{post}}^2$ 小于先验[方差](@entry_id:200758) $\sigma_h^2$，这表明观测数据减少了我们对信道增益的不确定性。

### 作为贝叶斯推断的学习

贝叶斯框架的威力远不止于对单个[隐藏状态](@entry_id:634361)的推断；它还可以被用来**学习**模型的参数。在这种[范式](@entry_id:161181)下，我们感兴趣的未知量就是模型参数本身，比如一个二进制信源产生'1'的概率 $p$。我们收集数据 $D$，并使用贝叶斯定理来更新我们关于参数的信念[分布](@entry_id:182848) $p(p|D)$。

#### [共轭先验](@entry_id:262304)

在处理参数学习问题时，如果先验分布和[后验分布](@entry_id:145605)属于同一个[概率分布](@entry_id:146404)族，那么计算会大大简化。这样的[先验分布](@entry_id:141376)被称为[似然函数](@entry_id:141927)的**[共轭先验](@entry_id:262304) (Conjugate Prior)**。

一个经典的例子是**Beta-二项 (Beta-Binomial)** 模型 [@problem_id:1603712]。假设我们正在表征一个二[进制](@entry_id:634389)信源，其产生'1'的未知概率为 $p$。由于制造差异，我们对 $p$ 的初始信念可以用一个**Beta[分布](@entry_id:182848)**来描述，$p \sim \operatorname{Beta}(\alpha, \beta)$。Beta[分布](@entry_id:182848)的PDF为 $f(p) \propto p^{\alpha-1}(1-p)^{\beta-1}$，它非常适合为定义在 $(0,1)$ 区间内的[概率建模](@entry_id:168598)。参数 $\alpha$ 和 $\beta$ 可以被直观地理解为“伪计数”(pseudo-counts)，即在看到任何真实数据之前，我们想象已经看到了 $\alpha-1$ 个'1'和 $\beta-1$ 个'0'。

现在，我们观测到一个包含 $S$ 个'1'和 $F$ 个'0'的序列。假设每次生成都是独立的伯努利试验，那么观测到这个序列的[似然函数](@entry_id:141927)是 $L(p|D) \propto p^S(1-p)^F$。

根据贝叶斯定理，后验分布为：
$$ p(p|D) \propto L(p|D) f(p) \propto \left(p^S (1-p)^F\right) \left(p^{\alpha-1} (1-p)^{\beta-1}\right) = p^{\alpha+S-1}(1-p)^{\beta+F-1} $$
我们立即认出，这个后验分布的形式正是一个新的Beta[分布](@entry_id:182848)，其参数为 $\alpha_{\text{post}} = \alpha+S$ 和 $\beta_{\text{post}} = \beta+F$。学习过程变得异常简单：只需将观测到的'1'和'0'的数量分别加到先验的参数上即可。例如，如果我们从 $\operatorname{Beta}(2,2)$ 的先验开始，观测到70个'1'和30个'0'，我们的[后验分布](@entry_id:145605)就是 $\operatorname{Beta}(72,32)$。这个新[分布](@entry_id:182848)的[期望值](@entry_id:153208)，即 $p$ 的后验期望，是 $\frac{\alpha_{\text{post}}}{\alpha_{\text{post}}+\beta_{\text{post}}} = \frac{72}{72+32} \approx 0.692$。

#### [预测分布](@entry_id:165741)

一旦我们通过数据学习到了参数的后验分布 $p(\theta|D)$，我们就可以对未来的观测进行预测。对于下一个符号 $X_{n+1}$，其预测概率并不是简单地使用某个单一的参数估计值（如[后验均值](@entry_id:173826)），而是通过对所有可能的参数值进行加权平均，权重就是参数的后验概率密度。这被称为**[贝叶斯预测](@entry_id:746731)[分布](@entry_id:182848) (Bayesian Predictive Distribution)**：

$$ P(X_{n+1}=j | D) = \int P(X_{n+1}=j|\theta) p(\theta|D) d\theta = \int \theta_j p(\theta|D) d\theta $$
这本质上是参数 $\theta_j$ 在[后验分布](@entry_id:145605)下的[期望值](@entry_id:153208)。

这个概念可以从二元情况推广到多元情况。对于一个从 $K$ 个符号的字母表中生成符号的信源，其[概率向量](@entry_id:200434) $\theta = (\theta_1, \dots, \theta_K)$ 的[共轭先验](@entry_id:262304)是**[狄利克雷分布](@entry_id:274669) (Dirichlet Distribution)**，$\theta \sim \operatorname{Dir}(\alpha)$，其中 $\alpha=(\alpha_1, \dots, \alpha_K)$ 是浓度参数。在观测到每个符号 $k$ 出现了 $n_k$ 次之后，后验分布是 $\operatorname{Dir}(\alpha_1+n_1, \dots, \alpha_K+n_K)$ [@problem_id:1603701]。

在这种情况下，下一个符号为 $j$ 的预测概率被证明为：
$$ P(X_{n+1}=j | X^n) = \frac{\alpha_j + n_j}{\sum_{k=1}^K (\alpha_k + n_k)} = \frac{\alpha_j + n_j}{\alpha_0 + n} $$
其中 $\alpha_0 = \sum_k \alpha_k$ 且 $n = \sum_k n_k$。这个优雅的公式被称为**[拉普拉斯继承规则](@entry_id:177306) (Laplace's rule of succession)** 的推广。它直观地表示，对符号 $j$ 的预测概率是其历史频率 $\frac{n_j}{n}$ 和其先验期望 $\frac{\alpha_j}{\alpha_0}$ 的一种平滑或加权平均。

### [贝叶斯推理](@entry_id:165613)的前沿应用

贝叶斯方法的普遍性使其能够应用于更广泛和更复杂的推理任务，包括在多个竞争模型之间做出选择，以及与[密码学](@entry_id:139166)等领域的[交叉](@entry_id:147634)。

#### [贝叶斯模型选择](@entry_id:147207)

当面临多个可以解释相同数据的竞争性假设或模型时，贝叶斯框架可以用来量化对每个模型的支持程度。在这种情况下，我们推断的“[隐变量](@entry_id:150146)”是模型本身。

假设我们不确定一个通信信道是静态噪声信道 $C_S$ (一个简单的BSC) 还是有状态噪声信道 $C_M$ (其翻转概率依赖于前一个发送的比特) [@problem_id:1603711]。我们有关于这两个模型的先验概率 $P(C_S)$ 和 $P(C_M)$。在观测到一段数据 $D$（例如，一个特定的输入输出序列）后，我们可以计算每个模型的[后验概率](@entry_id:153467)：

$$ P(C_S | D) = \frac{P(D|C_S)P(C_S)}{P(D)} $$
这里的 $P(D|C_S)$ 是模型 $C_S$ 的**[边际似然](@entry_id:636856)**或**证据**。它是指在给定模型 $C_S$ 的情况下，观测到数据 $D$ 的总概率（可能需要对模型内部的参数进行积分或求和）。

比较两个模型的后验概率的比值：
$$ \frac{P(C_S|D)}{P(C_M|D)} = \frac{P(D|C_S)}{P(D|C_M)} \times \frac{P(C_S)}{P(C_M)} $$
这个等式表明，后验概率比值等于**[贝叶斯因子](@entry_id:143567) (Bayes Factor)** 乘以先验概率比值。[贝叶斯因子](@entry_id:143567) $K = \frac{P(D|C_S)}{P(D|C_M)}$ 完全由数据驱动，它衡量了数据在多大程度上更有利于模型 $C_S$ 而不是模型 $C_M$。一个大于1的[贝叶斯因子](@entry_id:143567)意味着证据支持 $C_S$。

#### 作为[逻辑约束](@entry_id:635151)传播的推断

在某些情况下，证据的作用不是轻微地调整概率，而是像逻辑推理一样，直接排除掉一整类的可能性。这发生在[似然函数](@entry_id:141927)是确定性的时候，即对于某些假设，证据的概率为1，而对于其他所有假设，概率为0。

考虑一个[密码分析](@entry_id:196791)的场景，其中我们截获了一段密文，并且知道了对应的明文 [@problem_id:1603710]。加密算法（例如[仿射密码](@entry_id:152534)）是已知的，但密钥 $K$ 未知。在这里，观测到的明文-密文对构成了对密钥的硬约束。

*   **似然**: $P(\text{证据}|K)$ 对于任何能够将该明文加密成该密文的密钥 $K$ 都是1，而对于所有其他密钥都是0。
*   **先验**: [密码分析](@entry_id:196791)员可能对密钥的选取有一个非均匀的先验分布 $P(K)$。
*   **后验**: 根据[贝叶斯定理](@entry_id:151040)，后验概率 $P(K|\text{证据}) \propto P(\text{证据}|K)P(K)$。由于似然函数是0或1，[后验分布](@entry_id:145605)将只在满足约束的密钥[子集](@entry_id:261956)上有非零概率。在这个[子集](@entry_id:261956)上，后验概率的[分布](@entry_id:182848)形状与先验概率的[分布](@entry_id:182848)形状成正比，只是被重新归一化以确保总和为1。

这个过程精确地模拟了[密码分析](@entry_id:196791)中的“排除法”：证据排除了所有不兼容的密钥，我们根据剩余密钥的先验可能性来重新分配我们的信念。此外，我们可以通过计算密钥的后验熵 $H(K|\text{证据})$ 来量化在获得证据后我们对密钥的剩余不确定性。这个熵值告诉我们，为了完全确定密钥，平均还需要多少比特的信息。这巧妙地将[贝叶斯推断](@entry_id:146958)、[逻辑约束](@entry_id:635151)和信息论联系在了一起。