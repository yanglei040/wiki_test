{"hands_on_practices": [{"introduction": "拟牛顿法是强大的优化工具，其效率在很大程度上取决于更新Hessian矩阵近似的特定公式。本练习 ([@problem_id:2461204]) 提供了一个动手实践的机会，让你实现并比较两个里程碑式的更新方法：Davidon–Fletcher–Powell (DFP) 和 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 公式。通过将它们应用于一个具有挑战性的势能面，你将直接洞察它们的相对稳健性，并理解为何BFGS已成为现代计算化学软件中的首选方法。", "problem": "您必须编写一个完整且可运行的程序，在一个二维光滑势能面上比较两种拟牛顿优化方法。该势能面用于模拟无量纲单位下耦合的键伸缩和角弯曲。设决策变量为 $u$（无量纲的键长位移）和 $v$（无量纲的角位移，单位为弧度）。定义势能函数\n$$\nV(u,v) \\;=\\; \\tfrac{1}{2}\\,k_r\\,(u - r_0)^2 \\;+\\; \\tfrac{1}{2}\\,k_{\\theta}\\,(v - \\theta_0)^2 \\;+\\; k_c\\,(u - r_0)^2\\,(v - \\theta_0) \\;+\\; \\gamma\\,\\big(u - r_0 - \\alpha\\,(v - \\theta_0)^2\\big)^2,\n$$\n其中 $k_r$、$k_{\\theta}$、$k_c$、$\\gamma$、$\\alpha$、$r_0$ 和 $\\theta_0$ 是实数参数。所有量均被视为无量纲的约化单位，角位移 $v$ 以弧度为单位。\n\n您的程序必须在每次迭代 $k$ 时，使用两种不同的逆海森矩阵更新公式，从给定的起始点在 $\\mathbb{R}^2$ 中最小化 $V(u,v)$：\n- Davidon–Fletcher–Powell (DFP) 更新公式，用于更新逆海森矩阵的近似 $H_k \\in \\mathbb{R}^{2 \\times 2}$，\n$$\nH_{k+1} \\;=\\; H_k \\;+\\; \\frac{s_k s_k^{\\mathsf{T}}}{s_k^{\\mathsf{T}} y_k} \\;-\\; \\frac{H_k y_k y_k^{\\mathsf{T}} H_k}{y_k^{\\mathsf{T}} H_k y_k},\n$$\n- Broyden–Fletcher–Goldfarb–Shanno (BFGS) 更新公式，用于更新逆海森矩阵的近似 $H_k$，\n$$\nH_{k+1} \\;=\\; \\big(I - \\rho_k s_k y_k^{\\mathsf{T}}\\big)\\,H_k\\,\\big(I - \\rho_k y_k s_k^{\\mathsf{T}}\\big) \\;+\\; \\rho_k\\, s_k s_k^{\\mathsf{T}}, \\quad \\rho_k \\;=\\; \\frac{1}{y_k^{\\mathsf{T}} s_k},\n$$\n其中 $s_k = x_{k+1} - x_k$ 且 $y_k = \\nabla V(x_{k+1}) - \\nabla V(x_k)$，这里 $x_k = \\begin{bmatrix}u_k \\\\ v_k\\end{bmatrix}$，$I$ 是 $2 \\times 2$ 的单位矩阵。在每次迭代中，搜索方向必须是 $p_k = - H_k \\nabla V(x_k)$，步长为 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $\\alpha_k > 0$ 是某个标量步长。如果对于指定的容差 $\\varepsilon$ 满足 $\\lVert \\nabla V(x_k)\\rVert_2 \\leq \\varepsilon$，则算法必须成功终止；否则，则认为算法在指定的最大迭代次数内未能收敛。\n\n您必须将这两种更新公式应用于以下三个测试用例。在所有情况下，均使用相同的终止容差 $\\varepsilon = 10^{-5}$ 和欧几里得范数。必须使用单位矩阵作为初始逆海森矩阵近似 $H_0$。\n\n测试套件：\n1. 用例 A（良态谷形，常规顺利路径）：参数 $k_r = 1$，$k_{\\theta} = 1$，$k_c = 0.1$，$\\gamma = 100$，$\\alpha = 1$，$r_0 = 0$，$\\theta_0 = 0$；起始点 $x_0 = \\begin{bmatrix}-1.5 \\\\ 1.0\\end{bmatrix}$；最大迭代次数 $N_{\\max} = 200$。\n2. 用例 B（高曲率、各向异性谷形，旨在对更新公式进行压力测试）：参数 $k_r = 1$，$k_{\\theta} = 1$，$k_c = 0.2$，$\\gamma = 10^6$，$\\alpha = 10$，$r_0 = 0$，$\\theta_0 = 0$；起始点 $x_0 = \\begin{bmatrix}3.0 \\\\ -2.0\\end{bmatrix}$；最大迭代次数 $N_{\\max} = 60$。\n3. 用例 C（最小化点的边界条件）：参数 $k_r = 1$，$k_{\\theta} = 1$，$k_c = 0$，$\\gamma = 50$，$\\alpha = 0.5$，$r_0 = 0$，$\\theta_0 = 0$；起始点 $x_0 = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix}$；最大迭代次数 $N_{\\max} = 1$。\n\n对于每个用例，您的程序必须分别返回两个布尔值，根据停止准则 $\\lVert \\nabla V(x_k)\\rVert_2 \\leq \\varepsilon$ 和迭代次数上限，指示基于 DFP 的优化是否收敛以及基于 BFGS 的优化是否收敛。将用例 A、B 和 C 的六个布尔结果按顺序汇总到单行输出中，格式为 Python 风格的列表字面量，元素之间用逗号分隔，不含空格，即\n- 输出格式：单行包含 [bA_DFP,bA_BFGS,bB_DFP,bB_BFGS,bC_DFP,bC_BFGS]，\n其中每个 b... 的值为 True 或 False。\n\n角度必须以弧度为单位进行解释。输出中不需要其他物理单位。您必须确保程序的输出严格符合指定格式，并以单条 print 语句输出。", "solution": "所述问题构成了数值优化领域一个有效且定义明确的任务，特别是在计算化学中用于势能面最小化的背景下。在进行求解之前，有必要进行一项关键的验证。\n\n已知条件如下：\n- 一个势能函数 $V(u,v) = \\tfrac{1}{2}\\,k_r\\,(u - r_0)^2 + \\tfrac{1}{2}\\,k_{\\theta}\\,(v - \\theta_0)^2 + k_c\\,(u - r_0)^2\\,(v - \\theta_0) + \\gamma\\,\\big(u - r_0 - \\alpha\\,(v - \\theta_0)^2\\big)^2$。\n- 两种用于逆海森矩阵近似 $H_k$ 的拟牛顿更新公式：Davidon–Fletcher–Powell (DFP) 和 Broyden–Fletcher–Goldfarb–Shanno (BFGS)。\n- 一个迭代方案：$x_{k+1} = x_k + \\alpha_k p_k$，搜索方向为 $p_k = - H_k \\nabla V(x_k)$。\n- 初始逆海森矩阵：$H_0 = I$。\n- 终止准则：$\\lVert \\nabla V(x_k)\\rVert_2 \\leq \\varepsilon$，其中 $\\varepsilon = 10^{-5}$。\n- 三个测试用例（A、B、C），包含指定的参数（$k_r, k_{\\theta}, k_c, \\gamma, \\alpha, r_0, \\theta_0$）、起始点 $x_0$ 和最大迭代次数 $N_{\\max}$。\n\n该问题具有科学依据，采用了计算科学中的标准模型和算法。然而，陈述“步长为 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $\\alpha_k > 0$ 是某个标量步长”是不完整的。确定 $\\alpha_k$ 的方法并未指明。为了使拟牛顿法具有鲁棒性，并使 DFP 和 BFGS 之间的比较有意义，必须通过线搜索过程系统地选择 $\\alpha_k$。一个简单的选择（例如，固定的 $\\alpha_k=1$）会导致性能不佳或发散，从而使比较无效。因此，假设必须使用标准的线搜索算法来找到满足强 Wolfe 条件的步长 $\\alpha_k$。这既能确保势能充分下降，又能保证曲率条件 $y_k^{\\mathsf{T}} s_k > 0$ 成立，这对于 BFGS 更新的稳定性和正定性至关重要。这个假设使得问题是适定且可解的。所提供的测试用例是客观且可验证的。用例 C 的起始点即为最小值点，它正确地测试了在迭代 $k=0$ 时的终止逻辑。因此，在这个必要且标准的假设下，该问题被验证为可解。\n\n解决方案需要实现一个拟牛顿优化框架。该算法的核心是迭代地构建海森矩阵逆矩阵的近似 $H_k \\approx (\\nabla^2 V(x_k))^{-1}$，以指导搜索势能函数 $V(x)$ 的最小值，其中 $x = [u, v]^{\\mathsf{T}}$。\n\n首先，我们必须推导势能函数 $V(u,v)$ 的解析梯度。令 $\\Delta u = u - r_0$ 和 $\\Delta v = v - \\theta_0$。该函数为：\n$$V(u,v) = \\tfrac{1}{2} k_r (\\Delta u)^2 + \\tfrac{1}{2} k_\\theta (\\Delta v)^2 + k_c (\\Delta u)^2 (\\Delta v) + \\gamma (\\Delta u - \\alpha (\\Delta v)^2)^2$$\n其梯度 $\\nabla V(u,v) = \\begin{bmatrix} \\partial V / \\partial u \\\\ \\partial V / \\partial v \\end{bmatrix}$ 为：\n$$ \\frac{\\partial V}{\\partial u} = k_r (\\Delta u) + 2 k_c (\\Delta u)(\\Delta v) + 2\\gamma\\big(\\Delta u - \\alpha (\\Delta v)^2\\big) $$\n$$ \\frac{\\partial V}{\\partial v} = k_\\theta (\\Delta v) + k_c (\\Delta u)^2 - 4\\alpha\\gamma (\\Delta v)\\big(\\Delta u - \\alpha (\\Delta v)^2\\big) $$\n\n每种方法（DFP 和 BFGS）的优化算法流程如下：\n1.  初始化迭代计数器 $k = 0$、位置向量 $x_0$ 和逆海森矩阵近似 $H_0 = I$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n2.  计算初始梯度 $\\nabla V(x_0)$。如果其欧几里得范数 $\\lVert \\nabla V(x_0) \\rVert_2 \\leq \\varepsilon = 10^{-5}$，则过程成功终止。测试用例 C 即属于这种情况。\n3.  对于 $k = 0, 1, \\dots, N_{\\max}-1$：\n    a. 计算搜索方向：$p_k = -H_k \\nabla V(x_k)$。\n    b. 执行线搜索以找到合适的步长 $\\alpha_k > 0$。搜索必须找到一个满足强 Wolfe 条件的 $\\alpha_k$，这保证了 $V$ 的充分下降和曲率条件的满足。如果线搜索未能找到这样的步长，则优化失败。\n    c. 更新位置：$x_{k+1} = x_k + \\alpha_k p_k$。\n    d. 定义位置变化量 $s_k = x_{k+1} - x_k$ 和梯度变化量 $y_k = \\nabla V(x_{k+1}) - \\nabla V(x_k)$。\n    e. 成功的线搜索保证了曲率条件 $y_k^{\\mathsf{T}} s_k > 0$。该量用于两个更新公式的分母。\n    f. 使用 DFP 或 BFGS 公式将逆海森矩阵近似 $H_k$ 更新为 $H_{k+1}$。\n        -   **DFP 更新：**\n            $$ H_{k+1} = H_k + \\frac{s_k s_k^{\\mathsf{T}}}{s_k^{\\mathsf{T}} y_k} - \\frac{H_k y_k y_k^{\\mathsf{T}} H_k}{y_k^{\\mathsf{T}} H_k y_k} $$\n            DFP 更新在数值上很敏感，可能会失去正定性。我们实现了一个保障措施：如果分母 $y_k^{\\mathsf{T}} H_k y_k$ 接近于零或为负，则跳过更新，并设置 $H_{k+1} = H_k$。\n        -   **BFGS 更新：**\n            $$ H_{k+1} = \\big(I - \\rho_k s_k y_k^{\\mathsf{T}}\\big)\\,H_k\\,\\big(I - \\rho_k y_k s_k^{\\mathsf{T}}\\big) + \\rho_k\\, s_k s_k^{\\mathsf{T}}, \\quad \\text{其中 } \\rho_k = \\frac{1}{y_k^{\\mathsf{T}} s_k} $$\n            BFGS 更新被认为更具鲁棒性，并且如果 $H_0$ 是正定的且曲率条件成立，它能保持 $H_k$ 的正定性。\n    g. 检查终止条件：如果 $\\lVert \\nabla V(x_{k+1}) \\rVert_2 \\leq \\varepsilon$，则过程成功终止。请注意，我们是在开始下一次迭代之前检查*新*点的梯度。我的实现是在循环开始时检查点 $x_k$ 的梯度，这与前述方式等效。\n4.  如果循环完成而未满足收敛准则，则优化在 $N_{\\max}$ 次迭代内未能收敛。\n\n程序将对三个测试用例分别使用 DFP 和 BFGS 更新来执行此算法，并为六次运行中的每一次报告一个布尔值，以指示成功（True）或失败（False）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import line_search\n\ndef get_potential_and_gradient(params):\n    \"\"\"\n    Creates the potential energy function V and its gradient grad_V for a given set of parameters.\n    \"\"\"\n    kr, ktheta, kc, gamma, alpha, r0, theta0 = params\n\n    def V(x):\n        u, v = x\n        du = u - r0\n        dv = v - theta0\n        term1 = 0.5 * kr * du**2\n        term2 = 0.5 * ktheta * dv**2\n        term3 = kc * du**2 * dv\n        term4 = gamma * (du - alpha * dv**2)**2\n        return term1 + term2 + term3 + term4\n\n    def grad_V(x):\n        u, v = x\n        du = u - r0\n        dv = v - theta0\n        \n        common_term = du - alpha * dv**2\n        \n        # dV/du\n        grad_u = kr * du + 2.0 * kc * du * dv + 2.0 * gamma * common_term\n        \n        # dV/dv\n        grad_v = ktheta * dv + kc * du**2 - 4.0 * alpha * gamma * dv * common_term\n        \n        return np.array([grad_u, grad_v], dtype=float)\n        \n    return V, grad_V\n\ndef quasi_newton_optimizer(potential_params, x0, n_max, epsilon, update_formula):\n    \"\"\"\n    Performs minimization of a potential energy surface using a quasi-Newton method.\n\n    Args:\n        potential_params: Tuple of parameters for the potential function.\n        x0: Initial guess for the coordinates [u, v].\n        n_max: Maximum number of iterations.\n        epsilon: Convergence tolerance for the gradient norm.\n        update_formula: String specifying the update formula ('DFP' or 'BFGS').\n\n    Returns:\n        bool: True if converged, False otherwise.\n    \"\"\"\n    V, grad_V = get_potential_and_gradient(potential_params)\n    \n    x_k = np.array(x0, dtype=float)\n    H_k = np.identity(2, dtype=float)\n    \n    # Check for convergence at the starting point\n    grad_k = grad_V(x_k)\n    if np.linalg.norm(grad_k) = epsilon:\n        return True\n\n    for k in range(n_max):\n        # Compute search direction\n        p_k = -np.dot(H_k, grad_k)\n        \n        # Perform line search to find step length alpha_k\n        # scipy.optimize.line_search finds a step satisfying the Strong Wolfe conditions\n        alpha_k, _, _, _, _, _ = line_search(V, grad_V, x_k, p_k, gfk=grad_k, maxiter=100)\n        \n        # If line search fails, optimization has failed\n        if alpha_k is None:\n            return False\n            \n        # Update position\n        s_k = alpha_k * p_k\n        x_k_plus_1 = x_k + s_k\n        \n        # Calculate new gradient and check for convergence\n        grad_k_plus_1 = grad_V(x_k_plus_1)\n        if np.linalg.norm(grad_k_plus_1) = epsilon:\n            return True\n            \n        # Define y_k for the Hessian update\n        y_k = grad_k_plus_1 - grad_k\n        \n        # The curvature condition y_k.T @ s_k > 0 is guaranteed by the Wolfe conditions\n        # from a successful line search. The denominator will be positive.\n        denom_sy = np.dot(y_k.T, s_k)\n\n        # Avoid updating if curvature condition is not sufficiently positive\n        if denom_sy = 1e-9:\n             x_k = x_k_plus_1\n             grad_k = grad_k_plus_1\n             continue\n\n        # Update inverse Hessian approximation H_k\n        if update_formula == 'DFP':\n            term1_numerator = np.outer(s_k, s_k)\n            term1 = term1_numerator / denom_sy\n            \n            Hy = np.dot(H_k, y_k)\n            yHy = np.dot(y_k.T, Hy)\n            \n            # Safeguard for DFP: skip update if H_k is not positive definite\n            # along the y_k direction.\n            if yHy = 1e-9:\n                H_k_plus_1 = H_k\n            else:\n                term2_numerator = np.outer(Hy, Hy)\n                term2 = term2_numerator / yHy\n                H_k_plus_1 = H_k + term1 - term2\n        \n        elif update_formula == 'BFGS':\n            rho_k = 1.0 / denom_sy\n            I = np.identity(2, dtype=float)\n            \n            # Use the more stable implementation for BFGS update\n            term_mat = I - rho_k * np.outer(s_k, y_k)\n            H_k_plus_1 = np.dot(term_mat, np.dot(H_k, term_mat.T)) + rho_k * np.outer(s_k, s_k)\n        else:\n            raise ValueError(\"Invalid update formula specified.\")\n            \n        # Prepare for next iteration\n        x_k = x_k_plus_1\n        grad_k = grad_k_plus_1\n        H_k = H_k_plus_1\n\n    # If loop finishes, convergence was not reached within n_max iterations\n    return False\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {'params': (1.0, 1.0, 0.1, 100.0, 1.0, 0.0, 0.0), 'x0': [-1.5, 1.0], 'N_max': 200},\n        # Case B\n        {'params': (1.0, 1.0, 0.2, 1e6, 10.0, 0.0, 0.0), 'x0': [3.0, -2.0], 'N_max': 60},\n        # Case C\n        {'params': (1.0, 1.0, 0.0, 50.0, 0.5, 0.0, 0.0), 'x0': [0.0, 0.0], 'N_max': 1}\n    ]\n    \n    epsilon = 1e-5\n    results = []\n\n    for case in test_cases:\n        # Run DFP optimizer\n        converged_dfp = quasi_newton_optimizer(case['params'], case['x0'], case['N_max'], epsilon, 'DFP')\n        results.append(converged_dfp)\n        \n        # Run BFGS optimizer\n        converged_bfgs = quasi_newton_optimizer(case['params'], case['x0'], case['N_max'], epsilon, 'BFGS')\n        results.append(converged_bfgs)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2461204"}, {"introduction": "虽然线搜索方法很直观，但在势能面的复杂区域（例如最大值或鞍点附近），它们可能会遇到困难甚至失败。信赖域方法提供了一个更强大的框架来驾驭这些具有挑战性的地形。在本练习 ([@problem_id:2461247]) 中，你将从头开始构建一个信赖域优化器，以探索一个一维双阱势。这个练习将生动地展示信赖域机制如何内在地避免纯牛顿法等方法的陷阱，特别是通过正确处理负曲率区域来可靠地找到最小值。", "problem": "考虑以无量纲单位给出的单变量目标函数 $f(x)=x^4-x^2$，其一阶导数为 $f'(x)=4x^3-2x$，二阶导数为 $f''(x)=12x^2-2$。其驻点为位于 $x=0$ 处的局部极大值点和位于 $x_{\\pm}=\\pm 1/\\sqrt{2}$ 处的局部极小值点。对于一个迭代点 $x_k\\in\\mathbb{R}$，定义二次模型 $m_k(s)=f(x_k)+f'(x_k)s+\\tfrac{1}{2}f''(x_k)s^2$ 和一个信赖域半径 $\\Delta_k>0$。信赖域子问题是寻找一个步长 $s_k\\in\\mathbb{R}$，它在约束 $\\lvert s\\rvert\\le \\Delta_k$ 下使 $m_k(s)$ 最小化。令预测下降量为 $\\operatorname{pred}_k=-(m_k(s_k)-m_k(0))=-(f'(x_k)s_k+\\tfrac{1}{2}f''(x_k)s_k^2)$，实际下降量为 $\\operatorname{ared}_k=f(x_k)-f(x_k+s_k)$。定义接受比率 $\\rho_k=\\operatorname{ared}_k/\\operatorname{pred}_k$（当 $\\operatorname{pred}_k>0$ 时）；如果 $\\operatorname{pred}_k\\le 0$，则设 $\\rho_k=-\\infty$。如果 $\\rho_k\\ge \\eta_1$，则步长被接受，否则被拒绝。信赖域半径根据以下规则更新，其中 $\\eta_1\\in(0,1)$，$\\eta_2\\in(\\eta_1,1)$，$\\gamma_1\\in(0,1)$ 和 $\\gamma_2>1$ 为固定参数：\n- 如果 $\\rho_k  \\eta_1$，设 $\\Delta_{k+1}=\\gamma_1\\Delta_k$。\n- 如果 $\\rho_k\\ge \\eta_2$ 且 $\\lvert s_k\\rvert\\ge 0.8\\,\\Delta_k$，设 $\\Delta_{k+1}=\\min\\{\\gamma_2\\Delta_k,\\Delta_{\\max}\\}$。\n- 否则，设 $\\Delta_{k+1}=\\Delta_k$。\n如果步长被拒绝，则保持 $x_{k+1}=x_k$；如果被接受，则设 $x_{k+1}=x_k+s_k$。当 $\\lvert f'(x_k)\\rvert\\le \\varepsilon_g$、对于一个接受的步长有 $\\lvert s_k\\rvert\\le \\varepsilon_s$、$\\Delta_k\\le \\varepsilon_s$ 或达到固定的迭代次数上限时，迭代终止。\n\n在一维空间中，受约束 $\\lvert s\\rvert\\le \\Delta$ 的二次模型的唯一全局极小化子可被描述如下。对于给定的 $g\\in\\mathbb{R}$ 和 $h\\in\\mathbb{R}$，其中 $g=f'(x)$ 且 $h=f''(x)$，\n- 如果 $h>0$ 且无约束极小化子 $s_N=-g/h$ 满足 $\\lvert s_N\\rvert\\le \\Delta$，则 $s^\\star=s_N$；否则 $s^\\star=-\\operatorname{sign}(g)\\,\\Delta$。\n- 如果 $h\\le 0$，则当 $g0$ 时 $s^\\star=\\Delta$，当 $g>0$ 时 $s^\\star=-\\Delta$，当 $g=0$ 时 $s^\\star=\\Delta$。\n\n固定数值参数 $\\eta_1=0.1$，$\\eta_2=0.9$，$\\gamma_1=0.25$，$\\gamma_2=2$，$\\Delta_{\\max}=10$，梯度容差 $\\varepsilon_g=10^{-8}$，步长容差 $\\varepsilon_s=10^{-10}$，以及最大迭代次数 $100$ 次。对于下方的每个测试用例，从给定的初始点 $x_0$ 和初始半径 $\\Delta_0$ 开始，并在每一步应用上述迭代，使用一维模型极小化子 $s_k$。\n\n为每个测试用例定义三个标量输出：\n- $b_1$：一个逻辑值，其为真的条件是当且仅当没有任何被接受的步长会增加目标函数值，即对于所有被接受的步长 k，都有 $f(x_{k+1})\\le f(x_k)$。\n- $b_2$：一个逻辑值，其为真的条件是当且仅当在第一个迭代点的完整无约束牛顿步 $s_N=-f'(x_0)/f''(x_0)$（当 $f''(x_0)\\ne 0$ 时；如果 $f''(x_0)=0$，则定义 $b_2$ 为假）产生了一个更高的目标函数值，即 $f(x_0+s_N)>f(x_0)$。\n- $d$：最终接受的迭代点 $x_{\\mathrm{final}}$ 与最近的局部极小值点之间的绝对距离，即 $d=\\min\\{\\lvert x_{\\mathrm{final}}-1/\\sqrt{2}\\rvert,\\lvert x_{\\mathrm{final}}+1/\\sqrt{2}\\rvert\\}$。\n\n待评估的测试套件参数：\n1. $(x_0,\\Delta_0)=(0.1,0.05)$\n2. $(x_0,\\Delta_0)=(0.1,2.0)$\n3. $(x_0,\\Delta_0)=(0.0,0.5)$\n4. $(x_0,\\Delta_0)=(0.1,0.001)$\n\n你的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，输出一个列表 $[b_1,b_2,d]$，其中 $b_1$ 和 $b_2$ 是小写字符串 \"true\" 或 \"false\"，$d$ 是一个小数点后恰好有六位数字的十进制浮点数。因此，最终输出必须是形如\n$[[b_{1,1},b_{1,2},d_1],[b_{2,1},b_{2,2},d_2],[b_{3,1},b_{3,2},d_3],[b_{4,1},b_{4,2},d_4]]$ 的单行文本，\n该行中任何地方都不能有空格。", "solution": "该问题是有效的。它提出了一个适定、自洽且科学上合理的数值优化练习，具体来说是关于将信赖域算法应用于一维势能函数。所有必要的参数、算法规则、函数和终止准则都以数学精度给出。我们将继续提供一个完整的解。\n\n此问题的核心是实现并分析一个信赖域优化算法。这类方法在计算科学中是基础性的，特别是在计算化学中用于定位稳定的分子几何构型，这些构型对应于势能面上的极小值点。目标函数 $f(x) = x^4 - x^2$ 是一个典型的一维双阱势，代表一个在 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 处有两个稳定态（极小值）和在 $x=0$ 处有一个不稳定过渡态（极大值）的系统。\n\n信赖域算法通过构造目标函数的一个简化模型来迭代地寻找最小值，该模型仅在当前迭代点 $x_k$ 的一个邻域内是可信的。这个邻域是一个半径为 $\\Delta_k$ 的“信赖域”。该模型是一个二次函数 $m_k(s)$，它由 $f(x)$ 在 $x_k$ 附近的二阶泰勒展开式导出：\n$$m_k(s) = f(x_k) + f'(x_k)s + \\frac{1}{2}f''(x_k)s^2$$\n其中 $s$ 是从 $x_k$ 开始的步长。该模型在约束 $\\lvert s \\rvert \\le \\Delta_k$ 下相对于 $s$ 进行最小化。这个约束最小化问题被称为信赖域子问题。\n\n如题目所给，一维子问题的解取决于模型的曲率，该曲率由二阶导数 $h = f''(x_k)$ 给出。\n1.  如果 $h > 0$，模型是凸的（一个开口向上的抛物线）。无约束极小化子是牛顿步 $s_N = -g/h$，其中 $g = f'(x_k)$。如果此步长位于信赖域内，即 $\\lvert s_N \\rvert \\le \\Delta_k$，它就是最优步长 $s_k$。否则，模型在信赖域的边界处达到最小值，$s_k = -\\operatorname{sign}(g)\\Delta_k$，即在最速下降方向上尽可能远地移动。\n2.  如果 $h \\le 0$，模型是局部凹的或线性的。它在区间 $[-\\Delta_k, \\Delta_k]$ 上的最小值必定位于其中一个边界上。在 $s = \\Delta_k$ 和 $s = -\\Delta_k$ 之间的选择由梯度 $g$ 的符号决定，它指明了下降方向。\n\n一旦计算出试探步长 $s_k$，就通过比较目标函数的*实际下降量* $\\operatorname{ared}_k = f(x_k) - f(x_k + s_k)$ 和模型预测的*预测下降量* $\\operatorname{pred}_k = m_k(0) - m_k(s_k)$ 来评估其质量。它们的比率 $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$ 衡量了模型的保真度。\n\n-   如果 $\\rho_k$ 接近 1，说明模型是一个出色的预测器。步长被接受，并且我们可以扩大信赖域（$\\Delta_{k+1} = \\gamma_2 \\Delta_k$）以允许更激进的步长，前提是当前步长已经接近信赖域边界。\n-   如果 $\\rho_k$ 为正但不大，说明模型是足够的。步长被接受，但信赖域大小保持不变（$\\Delta_{k+1} = \\Delta_k$）。\n-   如果 $\\rho_k$ 很小或为负，说明模型很差。步长被拒绝（$x_{k+1} = x_k$），并且信赖域被收缩（$\\Delta_{k+1} = \\gamma_1 \\Delta_k$），以在后续迭代中提高模型准确性。\n\n步长接受规则是 $\\rho_k \\ge \\eta_1$。由于 $\\eta_1 = 0.1 > 0$ 并且子问题的解确保了 $\\operatorname{pred}_k \\ge 0$，因此任何被接受的步长都必须满足 $\\operatorname{ared}_k \\ge \\eta_1 \\operatorname{pred}_k \\ge 0$。如果 $\\operatorname{pred}_k > 0$，则 $\\operatorname{ared}_k > 0$，这保证了 $f(x_{k+1})  f(x_k)$。只有当模型预测函数值下降（$\\operatorname{pred}_k > 0$）时，步长才可能被接受。因此，对于 $b_1$ 的条件——即没有任何被接受的步长会增加目标函数值——根据该算法的构造本身就保证为真。任何偏差都将表明实现存在缺陷。\n\n输出 $b_2$ 探究了纯牛顿-拉弗森方法的局限性。无约束牛顿步 $s_N = -f'(x_0)/f''(x_0)$ 找到了二次模型的极值点。在极大值点附近，例如在 $x_0=0.0$ 或 $x_0=0.1$ 处，Hessian矩阵 $f''(x_0)$ 是负的。因此，牛顿步寻求的是局部二次模型的*极大值*，这对于最小化全局函数 $f(x)$ 来说是一个糟糕的策略，并且很可能导致一个上升步，即 $f(x_0+s_N) > f(x_0)$。信赖域框架通过约束步长大小来纠正这个缺陷。\n\n最终输出 $d$ 衡量了收敛到其中一个真实极小值点 $x_{\\pm} = \\pm 1/\\sqrt{2}$ 的准确性。鉴于所有初始点都是非负的，该算法预期将收敛到正的极小值点 $x_+ = 1/\\sqrt{2}$。\n\n实现将首先定义目标函数及其导数。然后，对每个测试用例，计算 $b_2$ 的值。接着执行主迭代循环，在每一步 $k$ 中包括：检查终止条件，求解信赖域子问题以得到 $s_k$，通过 $\\rho_k$ 评估步长质量，并根据指定规则更新状态变量 $x_k$ 和 $\\Delta_k$。在整个迭代过程中跟踪 $b_1$ 的值。终止时，计算最终距离 $d$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region optimization problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n    def f(x):\n        return x**4 - x**2\n\n    def f_prime(x):\n        return 4 * x**3 - 2 * x\n\n    def f_double_prime(x):\n        return 12 * x**2 - 2\n\n    # --- Algorithm Parameters ---\n    eta1 = 0.1\n    eta2 = 0.9\n    gamma1 = 0.25\n    gamma2 = 2.0\n    delta_max = 10.0\n    eps_g = 1e-8\n    eps_s = 1e-10\n    max_iter = 100\n    \n    minimizers = [-1/np.sqrt(2), 1/np.sqrt(2)]\n\n    # --- Test Cases ---\n    test_cases = [\n        (0.1, 0.05),\n        (0.1, 2.0),\n        (0.0, 0.5),\n        (0.1, 0.001)\n    ]\n\n    results = []\n\n    for x0, delta0 in test_cases:\n        # --- Output variable initialization ---\n        b1_flag = True\n        \n        # --- Calculate b2 before starting iterations ---\n        g0 = f_prime(x0)\n        h0 = f_double_prime(x0)\n        \n        if h0 == 0:\n            b2 = False\n        else:\n            s_N = -g0 / h0\n            b2 = f(x0 + s_N) > f(x0)\n            \n        # --- Main Trust-Region Loop ---\n        x_k = x0\n        delta_k = delta0\n        final_x = x0 # Will hold the last accepted iterate value\n        \n        for _ in range(max_iter):\n            g_k = f_prime(x_k)\n            \n            # --- Termination check 1: Gradient ---\n            if abs(g_k) = eps_g:\n                final_x = x_k\n                break\n\n            # --- Solve the trust-region subproblem ---\n            h_k = f_double_prime(x_k)\n            s_k = 0.0\n            \n            if h_k > 0:\n                s_N = -g_k / h_k\n                if abs(s_N) = delta_k:\n                    s_k = s_N\n                else:\n                    s_k = -np.sign(g_k) * delta_k\n            else: # h_k = 0\n                if g_k  0:\n                    s_k = delta_k\n                elif g_k > 0:\n                    s_k = -delta_k\n                else: # g_k == 0\n                    s_k = delta_k\n\n            # --- Evaluate step quality ---\n            pred_k = -(g_k * s_k + 0.5 * h_k * s_k**2)\n            ared_k = f(x_k) - f(x_k + s_k)\n            \n            rho_k = 0.0\n            # Use small tolerance for pred_k to avoid division by zero instability\n            if pred_k > 1e-12: # Check for meaningful predicted reduction\n                rho_k = ared_k / pred_k\n            else:\n                rho_k = -np.inf # Model predicts no improvement or a trivial step\n\n            delta_kp1 = delta_k\n            \n            # --- Step acceptance/rejection and state update ---\n            if rho_k >= eta1: # Accept step\n                if f(x_k + s_k) > f(x_k):\n                    b1_flag = False\n                \n                x_k += s_k\n                final_x = x_k\n                \n                # --- Termination check 2: Step size for an accepted step ---\n                if abs(s_k) = eps_s:\n                    break\n                    \n                # --- Update trust radius (for accepted step) ---\n                if rho_k >= eta2 and abs(s_k) >= 0.8 * delta_k:\n                    delta_kp1 = min(gamma2 * delta_k, delta_max)\n                # else: delta_kp1 remains delta_k\n                \n            else: # Reject step\n                # x_k remains the same\n                delta_kp1 = gamma1 * delta_k\n            \n            delta_k = delta_kp1\n            \n            # --- Termination check 3: Trust radius size ---\n            if delta_k = eps_s:\n                break\n        \n        # --- Calculate final distance d ---\n        d = min(abs(final_x - m) for m in minimizers)\n        \n        # Format results for the current test case\n        b1_str = \"true\" if b1_flag else \"false\"\n        b2_str = \"true\" if b2 else \"false\"\n        d_str = \"{:.6f}\".format(d)\n        results.append(f'[{b1_str},{b2_str},{d_str}]')\n        \n    # --- Final Print ---\n    # The final output is a single line, formatted as a list of lists.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2461247"}, {"introduction": "信赖域方法的一个关键特征是其自我校正能力，它通过评估局部二次模型对真实函数的代表程度来实现。这一评估由一致性比率 $\\rho_k$ 来量化。这个思想实验 ([@problem_id:2461234]) 挑战你将这个抽象的数学概念与分子建模中的具体情景联系起来。通过分析不同的情况，你将对模型可能失败的原因和方式形成更深的直觉，并理解势能中的物理不连续性（例如力场中的截断）如何导致较差的一致性比率，从而促使算法减小信赖域半径。", "problem": "您正在使用信赖域拟牛顿法来最小化一个分子势能函数 $f(\\mathbf{x})$。在第 $k$ 次迭代时，您构建了二次模型\n$$\nm_k(\\mathbf{s}) \\;=\\; f(\\mathbf{x}_k)\\;+\\;\\mathbf{g}_k^{\\top}\\mathbf{s}\\;+\\;\\tfrac{1}{2}\\,\\mathbf{s}^{\\top}\\mathbf{B}_k\\,\\mathbf{s}\n$$\n并接受一个步长 $\\mathbf{s}_k$，该步长在球 $\\{\\mathbf{s}:\\,\\lVert \\mathbf{s}\\rVert \\le \\Delta_k\\}$ 内近似求解了信赖域子问题。一致性比率定义为\n$$\n\\rho_k \\;=\\; \\dfrac{f(\\mathbf{x}_k)\\;-\\;f(\\mathbf{x}_k+\\mathbf{s}_k)}{\\,m_k(\\mathbf{0})\\;-\\;m_k(\\mathbf{s}_k)\\,}.\n$$\n假设 $m_k(\\mathbf{0})-m_k(\\mathbf{s}_k) > 0$。在下列哪种计算化学情景下，最有可能导致此步的一致性比率非常差，即 $\\rho_k \\approx 0$？\n\nA. 经典力场计算，在距离 $r_c$ 处使用能量平移但非力平移的伦纳德-琼斯截断。在 $\\mathbf{x}_k$ 处，有几个非键对的间距 $r \\approx r_c^{-}$，因此它们的对能量已接近零，而力仍然是有限的。信赖域步长 $\\mathbf{s}_k$ 将这些间距增加到 $r \\approx r_c^{+}$（无相互作用），使得实际能量下降非常微小，尽管根据在 $r \\approx r_c^{-}$ 处的非零梯度构建的二次模型预测了可观的能量减少。\n\nB. 一个纯谐振分子内势 $f(\\mathbf{x}) \\;=\\; \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_\\star)^{\\top}\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_\\star)$，具有精确的海森矩阵 $\\mathbf{B}_k \\;=\\; \\mathbf{H}$ 和精确的梯度 $\\mathbf{g}_k \\;=\\; \\nabla f(\\mathbf{x}_k)$；步长 $\\mathbf{s}_k$ 在信赖域内被精确计算。\n\nC. 密度泛函理论（DFT）自洽场（SCF）计算，其能量收敛标准极其宽松，以至于能量噪声的标准差约为 $10^{-5}$ Hartree，而信赖域模型预测的能量减少量约为 $10^{-6}$ Hartree。\n\nD. 一个光滑的势能面，具有精确的能量和梯度，但在运行初期，拟牛顿矩阵 $\\mathbf{B}_k$ 是一个较差的曲率近似；信赖域步长很小，并保持在泰勒展开有效的区域内。", "solution": "从定义开始。一致性比率是\n$$\n\\rho_k \\;=\\; \\dfrac{\\text{实际减少量}}{\\text{预测减少量}}\n\\;=\\; \\dfrac{f(\\mathbf{x}_k)\\;-\\;f(\\mathbf{x}_k+\\mathbf{s}_k)}{\\,m_k(\\mathbf{0})\\;-\\;m_k(\\mathbf{s}_k)\\,}.\n$$\n根据假设，$m_k(\\mathbf{0})-m_k(\\mathbf{s}_k) > 0$。为了得到 $\\rho_k \\approx 0$，分子必须远小于分母；也就是说，实际能量下降 $f(\\mathbf{x}_k)-f(\\mathbf{x}_k+\\mathbf{s}_k)$ 必须接近于零，而二次模型预测了一个显著的减少。我们从基本原理出发分析每个选项。\n\n选项A。考虑一个在 $r_c$ 处具有能量平移截断的单一伦纳德-琼斯对势 $U(r)$，即\n$$\n\\tilde{U}(r) \\;=\\; \\begin{cases}\nU(r)\\;-\\;U(r_c),  r \\le r_c \\\\\n0,  r > r_c\n\\end{cases}\n$$\n因此 $\\tilde{U}(r_c) = 0$，但力在 $r_c$ 处是不连续的，因为 $\\dfrac{d\\tilde{U}}{dr}(r_c^{-}) = U'(r_c)$ 通常是非零的，而 $\\dfrac{d\\tilde{U}}{dr}(r_c^{+}) = 0$。假设在 $\\mathbf{x}_k$ 处，一对原子位于 $r = r_c-\\varepsilon$，其中 $\\varepsilon > 0$ 是一个小数。该对原子对模型梯度的贡献是有限的，因此如果步长将 $r$ 增加 $2\\varepsilon$ 并越过 $r_c$，二次模型会预测一个大小约为 $\\approx -\\,\\mathbf{g}_k^{\\top}\\mathbf{s}_k \\sim \\bigl|\\tfrac{d\\tilde{U}}{dr}(r_c^{-})\\bigr|\\,(2\\varepsilon)$ 的一阶下降。然而，这对原子对实际能量下降的贡献不会超过微小的残余值 $\\tilde{U}(r_c-\\varepsilon)$，因为一旦 $r \\ge r_c$，相互作用能就精确为0。当 $\\varepsilon \\to 0^{+}$ 时，我们有\n$$\n\\underbrace{f(\\mathbf{x}_k)-f(\\mathbf{x}_k+\\mathbf{s}_k)}_{\\text{实际减少量}} \\;\\to\\; 0,\n\\qquad\n\\underbrace{m_k(\\mathbf{0})-m_k(\\mathbf{s}_k)}_{\\text{预测减少量}} \\;\\not\\to\\; 0,\n$$\n因为预测的减少量反映了截断前的非零力。因此，\n$$\n\\rho_k \\;=\\; \\dfrac{\\text{实际减少量}}{\\text{预测减少量}} \\;\\to\\; 0.\n$$\n这精确地匹配了所要求的情景：二次模型将非零斜率外推经过一个真实能量变为平坦（为零）的点，从而产生 $\\rho_k \\approx 0$。结论 — 正确。\n\n选项B。对于一个纯二次势 $f(\\mathbf{x}) = \\tfrac{1}{2}(\\mathbf{x}-\\mathbf{x}_\\star)^{\\top}\\mathbf{H}(\\mathbf{x}-\\mathbf{x}_\\star)$，具有精确的梯度 $\\mathbf{g}_k$ 和精确的海森矩阵 $\\mathbf{B}_k = \\mathbf{H}$，二次模型 $m_k$ 等于真实的二阶泰勒展开，并且是全局精确的。信赖域内的任何步长 $\\mathbf{s}_k$ 都会得到\n$$\nf(\\mathbf{x}_k)-f(\\mathbf{x}_k+\\mathbf{s}_k) = m_k(\\mathbf{0})-m_k(\\mathbf{s}_k),\n$$\n所以 $\\rho_k = 1$。这不等于 $\\rho_k \\approx 0$。结论 — 错误。\n\n选项C。设预测的减少量为 $m_k(\\mathbf{0})-m_k(\\mathbf{s}_k) \\approx 10^{-6}$ Hartree。如果在密度泛函理论（DFT）计算中，自洽场（SCF）的能量噪声具有约 $10^{-5}$ Hartree 的标准差，那么测得的实际减少量 $f(\\mathbf{x}_k)-f(\\mathbf{x}_k+\\mathbf{s}_k)$ 将主要由噪声决定，并且不会系统性地接近于0；其大小通常与预测值相当或更大，其符号甚至可能翻转，导致 $|\\rho_k|$ 的值可能远大于1或为负。虽然这导致了较差的一致性，但它并不会特异性地产生 $\\rho_k \\approx 0$；相反，它会产生不稳定的 $\\rho_k$ 值，其绝对值通常远不为零。结论 — 错误。\n\n选项D。在一个光滑的势能面上，具有精确的能量和梯度，一个仅中度不准确的拟牛顿矩阵 $\\mathbf{B}_k$ 通常会产生一个高估或低估真实减少量的模型，但对于足够小的信赖域步长，该模型仍然与真实减少量相关。通常会观察到 $0  \\rho_k  1$（或偶尔略微超出此范围），而不是 $\\rho_k \\approx 0$。要实现 $\\rho_k \\approx 0$，需要实际减少量几乎为零，而预测值是明确的正数，这在函数光滑且步长遵守信赖域的情况下不是典型结果。结论 — 错误。", "answer": "$$\\boxed{A}$$", "id": "2461234"}]}