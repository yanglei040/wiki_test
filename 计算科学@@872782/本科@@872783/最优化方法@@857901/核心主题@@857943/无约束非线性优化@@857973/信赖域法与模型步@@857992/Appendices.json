{"hands_on_practices": [{"introduction": "信赖域方法的效果取决于二次模型在局部邻域内能否忠实地表示目标函数。这个练习提供了一个动手实践的机会来研究这一核心原则。通过系统地改变一个样本函数的非凸性，你将计算模型预测与实际函数改进之间的差异，从而定量地理解局部曲率如何影响模型的保真度。", "problem": "考虑一个用于无约束最小化的一维信赖域方法。在迭代点 $x_k$ 处，一个二次连续可微函数 $f$ 在由半径 $\\Delta$ 定义的信赖域内，被其二阶泰勒多项式（称为二次模型）所近似。关于 $x_k$ 的二次模型 $m(s)$ 是使用 $f$ 在 $x_k$ 处的梯度 $g$ 和海森矩阵 $B$ 构建的。信赖域子问题旨在寻找一个步长 $s$，在步长范数的约束下使模型下降。实际下降量由真实函数值的变化来衡量，而预测下降量则由二次模型的减少量来衡量。实际下降量与预测下降量之间的差异，量化了二次模型在信赖域内捕捉真实函数行为的优劣程度。\n\n从这些基本定义出发，考虑以下具体设置：\n\n- 函数族为一个参数化的四次多项式，由 $f_{\\alpha}(x) = \\frac{1}{4}x^4 - \\frac{\\alpha}{2} x^2 + \\beta x$ 给出，其中 $\\alpha$ 控制非凸性，$\\beta$ 是一个固定的微小线性扰动。\n- 使用固定扰动 $\\beta = 0.1$。\n- 使用固定迭代点 $x_k = 0.5$。\n- 在一维情况下，在 $x_k$ 处的二次模型可以表示为 $m(s) = f_{\\alpha}(x_k) + g_{\\alpha} s + \\frac{1}{2} B_{\\alpha} s^2$，其中 $g_{\\alpha} = f_{\\alpha}'(x_k)$ 且 $B_{\\alpha} = f_{\\alpha}''(x_k)$。\n- 将一维信赖域子问题定义为在约束 $|s| \\le \\Delta$ 下最小化 $m(s)$。\n\n对于一维情况，最速下降方向是负梯度方向。本问题中使用的步长应是在最速下降方向上，满足信赖域边界约束的 $m(s)$ 的最小化子，即在方向 $-g_{\\alpha}$ 的射线上，满足 $|s| \\le \\Delta$ 且最小化 $m(s)$ 的步长 $s(\\Delta)$。\n\n你的任务是：\n\n- 对于每个 $\\alpha$ 的选择，构造 $f_{\\alpha}$ 并计算在 $x_k$ 处的梯度 $g_{\\alpha}$ 和海森矩阵 $B_{\\alpha}$。\n- 确定在最速下降方向上，满足约束 $|s| \\le \\Delta$ 且最小化二次模型 $m(s)$ 的步长 $s(\\Delta)$。\n- 计算实际下降量 $AR(\\Delta) = f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))$ 和预测下降量 $PR(\\Delta) = m(0) - m(s(\\Delta))$。\n- 为每个测试用例计算差值 $D(\\Delta) = AR(\\Delta) - PR(\\Delta)$。\n\n测试套件规范：\n\n- 使用非凸性水平参数集 $\\alpha \\in \\{0.0, 0.5, 0.75, 1.0, 2.0\\}$。\n- 使用信赖域半径 $\\Delta \\in \\{0.01, 0.1, 0.3, 0.5\\}$。\n- 固定迭代点为 $x_k = 0.5$。\n- 固定扰动为 $\\beta = 0.1$。\n\n覆盖性设计：\n\n- $\\alpha$ 值的集合探查了从 $x_k$ 附近的凸区域到 $x_k$ 附近的强非凸区域。\n- $\\Delta$ 值的集合探查了小半径（模型局部准确）、边界限制步长和中等大半径的情况。\n- 包括在 $\\alpha = 0.75$ 时，$x_k$ 处曲率为零的边界情况。\n\n最终输出规范：\n\n- 您的程序应生成单行输出，其中包含一个由逗号分隔的列表的列表，并用方括号括起来。每个内部列表对应一个按升序排列的 $\\alpha$ 值，并包含按升序排列的 $\\Delta$ 对应的 $D(\\Delta)$ 值。例如，输出格式应如下所示：$[[d_{0,0},d_{0,1},d_{0,2},d_{0,3}],[d_{1,0},\\dots],\\dots]$，其中 $d_{i,j}$ 是第 $i$ 个 $\\alpha$ 和第 $j$ 个 $\\Delta$ 的差值。\n- 每个 $d_{i,j}$ 必须是一个实数（浮点数），不带任何物理单位，并直接作为数字字面量打印。\n\n不需要用户输入；程序必须是自包含的，并使用给定的测试套件计算指定的值。角度单位不适用。此问题不涉及任何物理单位或百分比。确保科学上的一致性，并且只使用信赖域方法中源自二阶泰勒模型的定义和经过充分检验的公式。", "solution": "用户请求了一个关于信赖域优化方法问题的解决方案。该任务涉及为给定的一维四次多项式族，计算模型函数的实际下降量和预测下降量之间的差异。\n\n### 步骤 1：初步分析与公式化\n\n问题围绕函数 $f_{\\alpha}(x) = \\frac{1}{4}x^4 - \\frac{\\alpha}{2} x^2 + \\beta x$ 展开，其固定参数为 $x_k=0.5$ 和 $\\beta=0.1$。问题的核心是确定使二次模型 $m(s)$ 最小化的步长 $s(\\Delta)$，然后用这个步长计算量 $D(\\Delta) = AR(\\Delta) - PR(\\Delta)$。\n\n首先，我们确定二次模型的关键组成部分，$m(s) = f_{\\alpha}(x_k) + g_{\\alpha} s + \\frac{1}{2} B_{\\alpha} s^2$。在迭代点 $x_k$ 处的梯度 $g_{\\alpha}$ 和海森矩阵 $B_{\\alpha}$ 是 $f_{\\alpha}(x)$ 在 $x_k$ 处求值的一阶和二阶导数。\n\n$f_{\\alpha}(x)$ 的导数是：\n$f'_{\\alpha}(x) = x^3 - \\alpha x + \\beta$\n$f''_{\\alpha}(x) = 3x^2 - \\alpha$\n$f'''_{\\alpha}(x) = 6x$\n$f''''_{\\alpha}(x) = 6$\n\n在固定迭代点 $x_k = 0.5$ 和 $\\beta = 0.1$ 处对这些导数求值：\n梯度为 $g_{\\alpha} = f'_{\\alpha}(0.5) = (0.5)^3 - \\alpha(0.5) + 0.1 = 0.125 - 0.5\\alpha + 0.1 = 0.225 - 0.5\\alpha$。\n海森矩阵（或一维情况下的曲率）为 $B_{\\alpha} = f''_{\\alpha}(0.5) = 3(0.5)^2 - \\alpha = 0.75 - \\alpha$。\n\n### 步骤 2：简化目标量 $D(\\Delta)$\n\n目标量是实际下降量 $AR(\\Delta)$ 和预测下降量 $PR(\\Delta)$ 之间的差值。\n$AR(\\Delta) = f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))$\n$PR(\\Delta) = m(0) - m(s(\\Delta)) = f_{\\alpha}(x_k) - m(s(\\Delta))$\n\n因此，差值为：\n$D(\\Delta) = AR(\\Delta) - PR(\\Delta) = (f_{\\alpha}(x_k) - f_{\\alpha}(x_k + s(\\Delta))) - (f_{\\alpha}(x_k) - m(s(\\Delta)))$\n$D(\\Delta) = m(s(\\Delta)) - f_{\\alpha}(x_k + s(\\Delta))$\n\n这个量表示二次模型与真实函数在点 $x_k + s(\\Delta)$ 处的误差。我们可以使用 $f_{\\alpha}$ 在 $x_k$ 附近的泰勒展开的高阶项来表示这个误差。由于 $f_{\\alpha}$ 是一个四次多项式，其泰勒级数是有限且精确的：\n$f_{\\alpha}(x_k+s) = f_{\\alpha}(x_k) + f'_{\\alpha}(x_k)s + \\frac{1}{2}f''_{\\alpha}(x_k)s^2 + \\frac{1}{6}f'''_{\\alpha}(x_k)s^3 + \\frac{1}{24}f''''_{\\alpha}(x_k)s^4$\n根据定义，$m(s) = f_{\\alpha}(x_k) + f'_{\\alpha}(x_k)s + \\frac{1}{2}f''_{\\alpha}(x_k)s^2$。\n所以，$f_{\\alpha}(x_k+s) = m(s) + \\frac{1}{6}f'''_{\\alpha}(x_k)s^3 + \\frac{1}{24}f''''_{\\alpha}(x_k)s^4$。\n\n代入 $f'''_{\\alpha}(0.5) = 6(0.5) = 3$ 和 $f''''_{\\alpha}(0.5) = 6$：\n$f_{\\alpha}(0.5+s) = m(s) + \\frac{1}{6}(3)s^3 + \\frac{1}{24}(6)s^4 = m(s) + \\frac{1}{2}s^3 + \\frac{1}{4}s^4$。\n\n因此，对于任何步长 $s$，差值为：\n$m(s) - f_{\\alpha}(0.5+s) = -(\\frac{1}{2}s^3 + \\frac{1}{4}s^4)$。\n这意味着 $D(\\Delta) = -\\frac{1}{2} s(\\Delta)^3 - \\frac{1}{4} s(\\Delta)^4$，这极大地简化了计算。问题简化为求解步长 $s(\\Delta)$。\n\n### 步骤 3：确定步长 $s(\\Delta)$\n\n步长 $s(\\Delta)$ 被定义为在最速下降方向 $-g_{\\alpha}$ 上，受限于信赖域约束 $|s| \\le \\Delta$ 的模型 $m(s)$ 的最小化子。\n设步长形式为 $s = \\tau \\cdot \\operatorname{dir}$，其中 $\\tau \\ge 0$ 是步长大小，$\\operatorname{dir} = -\\frac{g_{\\alpha}}{|g_{\\alpha}|} = -\\operatorname{sgn}(g_{\\alpha})$ 是单位方向。约束变为 $\\tau \\le \\Delta$。\n\n我们将 $s = -\\tau \\operatorname{sgn}(g_{\\alpha})$ 代入模型中。由于 $g_{\\alpha} s = -\\tau |g_{\\alpha}|$，模型成为 $\\tau$ 的函数：\n$m(\\tau) = f_{\\alpha}(x_k) - |g_{\\alpha}|\\tau + \\frac{1}{2} B_{\\alpha} \\tau^2$。\n\n我们需要在区间 $[0, \\Delta]$ 上最小化这个关于 $\\tau$ 的二次函数。我们根据曲率 $B_{\\alpha}$ 的符号分析两种情况。\n\n**情况 1：凸模型 ($B_{\\alpha}  0$)**\n该模型是一个开口向上的凸抛物线。通过将关于 $\\tau$ 的导数设为零，可以找到无约束最小值：\n$m'(\\tau) = -|g_{\\alpha}| + B_{\\alpha}\\tau = 0 \\implies \\tau_{unc} = \\frac{|g_{\\alpha}|}{B_{\\alpha}}$。\n信赖域内的最小化子是通过将该点投影到可行区间 $[0, \\Delta]$ 上找到的：\n$\\tau_{opt} = \\min(\\tau_{unc}, \\Delta) = \\min\\left(\\frac{|g_{\\alpha}|}{B_{\\alpha}}, \\Delta\\right)$。\n\n**情况 2：凹模型或线性模型 ($B_{\\alpha} \\le 0$)**\n函数 $m(\\tau)$ 要么是开口向下的抛物线，要么是斜率为负的直线（因为在所有测试用例中 $g_{\\alpha}$ 都不为零）。无论哪种情况，$m(\\tau)$ 在 $[0, \\infty)$ 上都是一个递减函数。要在 $[0, \\Delta]$ 上最小化 $m(\\tau)$，我们必须选择 $\\tau$ 的最大可能值。\n$\\tau_{opt} = \\Delta$。\n\n一旦确定了 $\\tau_{opt}$，步长就计算为 $s(\\Delta) = \\tau_{opt} \\cdot (-\\operatorname{sgn}(g_{\\alpha}))$。\n\n### 步骤 4：最终计算算法\n\n对于指定测试套件中的每一对 $(\\alpha, \\Delta)$，我们执行以下计算：\n1.  计算 $g_{\\alpha} = 0.225 - 0.5\\alpha$。\n2.  计算 $B_{\\alpha} = 0.75 - \\alpha$。\n3.  确定最优步长大小 $\\tau_{opt}$：\n    - 如果 $B_{\\alpha}  0$，计算 $\\tau_{opt} = \\min(|g_{\\alpha}|/B_{\\alpha}, \\Delta)$。\n    - 如果 $B_{\\alpha} \\le 0$，设置 $\\tau_{opt} = \\Delta$。\n4.  计算步长 $s(\\Delta) = \\tau_{opt} \\cdot (-\\operatorname{sgn}(g_{\\alpha}))$。请注意，对于 $g_{\\alpha}=0$ 的情况，这将正确地得到 $s=0$。\n5.  计算差值 $D(\\Delta) = -0.5 s(\\Delta)^3 - 0.25 s(\\Delta)^4$。\n6.  将当前 $\\alpha$ 的结果 $D(\\Delta)$ 收集到一个列表中。\n7.  在遍历所有 $\\Delta$ 值之后，将此列表附加到最终的列表的列表中。\n然后对所有指定的 $\\alpha$ 值实施此过程，以生成最终输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the trust-region subproblem for a parameterized quartic polynomial\n    and computes the difference between actual and predicted reduction.\n    \"\"\"\n    \n    # Test Suite Specification\n    alphas = [0.0, 0.5, 0.75, 1.0, 2.0]\n    deltas = [0.01, 0.1, 0.3, 0.5]\n    \n    # Fixed parameters\n    x_k = 0.5\n    beta = 0.1\n    \n    # This list will hold lists of results, one for each alpha.\n    all_results = []\n    \n    for alpha in alphas:\n        # This list will hold the results for the current alpha over all deltas.\n        results_for_alpha = []\n        \n        # Step 1: Compute gradient g_alpha and Hessian B_alpha at x_k\n        # g_alpha = f'(x_k) = x_k^3 - alpha*x_k + beta\n        g_alpha = x_k**3 - alpha * x_k + beta\n        # B_alpha = f''(x_k) = 3*x_k^2 - alpha\n        B_alpha = 3 * x_k**2 - alpha\n        \n        for delta in deltas:\n            # Step 2: Determine the optimal step s(delta)\n            \n            # The minimization is along the steepest descent direction -g_alpha.\n            # We solve for the step length tau_opt along this direction.\n            # The model along this direction is m(tau) = c - |g_alpha|*tau + 0.5*B_alpha*tau^2.\n            \n            tau_opt = 0.0\n\n            if B_alpha  0:\n                # Convex case: The unconstrained minimizer is at tau_unc = |g_alpha| / B_alpha.\n                # The constrained solution is the minimum of this and the trust-region radius.\n                if g_alpha != 0:\n                    tau_unc = abs(g_alpha) / B_alpha\n                    tau_opt = min(tau_unc, delta)\n                else:\n                    # If gradient is zero, step is zero.\n                    tau_opt = 0.0\n\n            else: # B_alpha = 0\n                # Concave or linear case: The model decreases along the ray.\n                # The minimum is at the trust-region boundary.\n                tau_opt = delta\n                \n            # Compute the step vector s. The direction is -g_alpha.\n            # The sign of the step is -sign(g_alpha).\n            # np.sign(0) is 0, correctly handling the g_alpha=0 case.\n            step_s = tau_opt * (-np.sign(g_alpha))\n            \n            # Step 3: Compute the difference D(delta) = AR - PR = m(s) - f(x_k + s)\n            # As derived, D(delta) = -0.5 * s^3 - 0.25 * s^4.\n            diff_D = -0.5 * step_s**3 - 0.25 * step_s**4\n            results_for_alpha.append(diff_D)\n            \n        all_results.append(results_for_alpha)\n\n    # Final Output Specification\n    # Format: [[d_0,0, d_0,1, ...], [d_1,0, ...], ...]\n    # Using a list comprehension to format numbers without trailing '.0'\n    formatted_results = [\n        \"[\" + \",\".join(map(str, inner_list)) + \"]\"\n        for inner_list in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3193707"}, {"introduction": "每次信赖域迭代的核心任务是求解信赖域子问题：一个约束下的二次优化问题。这个练习将引导你使用强大的 Karush-Kuhn-Tucker (KKT) 框架来推导其精确的解析解。通过推导和分析由此产生的“长期方程”(secular equation)，你将揭示最优步长、信赖域半径 $\\Delta$ 及其相关的拉格朗日乘子 $\\lambda$ 之间的基本关系。", "problem": "设 $m(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s}$ 是一个二次连续可微目标函数在给定迭代点处的二次模型，其中 $\\mathbf{s} \\in \\mathbb{R}^{2}$ 是步长，$g \\in \\mathbb{R}^{2}$ 是梯度向量，$Q \\in \\mathbb{R}^{2 \\times 2}$ 是一个对称正定矩阵。信赖域子问题旨在寻找使 $m(\\mathbf{s})$ 最小化的步长，其约束条件为欧几里得范数约束 $\\|\\mathbf{s}\\| \\leq \\Delta$，其中 $\\Delta  0$ 是信赖域半径。\n\n考虑 $Q = 3 I_{2}$ 和 $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$ 的特定情况，其中 $I_{2}$ 是 $2 \\times 2$ 的单位矩阵。将 $\\Delta$ 视为一个正参数，并假设对于足够小的 $\\Delta$，最小化子位于信赖域的边界上。\n\n仅从约束最小化的一阶最优性条件（Karush-Kuhn-Tucker (KKT) 条件）出发，推导决定与信赖域约束相关的拉格朗日乘子 $\\lambda \\geq 0$ 的特征方程，精确求解该方程得到 $\\lambda$ 关于 $\\Delta$ 的表达式，然后获得精确的信赖域步长 $\\mathbf{s}^{\\star}(\\Delta)$。最后，通过计算 $\\frac{d\\lambda}{d\\Delta}$ 在 $\\Delta = 1$ 处的值，确定拉格朗日乘子对信赖域半径的灵敏度。\n\n你的最终答案必须是 $\\frac{d\\lambda}{d\\Delta}$ 在 $\\Delta = 1$ 处的单个实数值。无需四舍五入。", "solution": "问题是要求解二次模型 $m(\\mathbf{s})$ 在信赖域约束下的最小化子。该优化问题表述为：\n$$ \\min_{\\mathbf{s} \\in \\mathbb{R}^{2}} m(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s} $$\n$$ \\text{约束条件为} \\quad \\|\\mathbf{s}\\| \\leq \\Delta $$\n其中使用的是欧几里得范数。我们已知具体数值 $Q = 3 I_{2}$ 和 $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$。问题还指明了解（记为 $\\mathbf{s}^{\\star}$）位于可行集的边界上，即 $\\|\\mathbf{s}^{\\star}\\| = \\Delta$。\n\n为了解决这个约束优化问题，我们使用 Karush-Kuhn-Tucker (KKT) 条件。约束可以写成 $c(\\mathbf{s}) = \\frac{1}{2}(\\mathbf{s}^{\\top}\\mathbf{s} - \\Delta^2) \\leq 0$。拉格朗日函数 $\\mathcal{L}(\\mathbf{s}, \\lambda)$ 是：\n$$ \\mathcal{L}(\\mathbf{s}, \\lambda) = m(\\mathbf{s}) + \\lambda c(\\mathbf{s}) = g^{\\top} \\mathbf{s} + \\frac{1}{2} \\mathbf{s}^{\\top} Q \\mathbf{s} + \\frac{\\lambda}{2} (\\mathbf{s}^{\\top}\\mathbf{s} - \\Delta^2) $$\n其中 $\\lambda$ 是与范数约束相关的拉格朗日乘子。\n\n一阶必要最优性条件（KKT 条件）是：\n1.  **驻定性 (Stationarity):** 拉格朗日函数关于 $\\mathbf{s}$ 的梯度必须为零向量：$\\nabla_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}^{\\star}, \\lambda) = \\mathbf{0}$。\n2.  **原始可行性 (Primal Feasibility):** 解必须满足约束条件：$(\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2 \\leq 0$。\n3.  **对偶可行性 (Dual Feasibility):** 拉格朗日乘子必须为非负：$\\lambda \\geq 0$。\n4.  **互补松弛性 (Complementary Slackness):** $\\lambda ((\\mathbf{s}^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2) = 0$。\n\n首先，我们计算拉格朗日函数的梯度：\n$$ \\nabla_{\\mathbf{s}} \\mathcal{L}(\\mathbf{s}, \\lambda) = g + Q\\mathbf{s} + \\lambda\\mathbf{s} = (Q + \\lambda I)\\mathbf{s} + g $$\n因此，驻定性条件为：\n$$ (Q + \\lambda I)\\mathbf{s}^{\\star} = -g $$\n\n我们已知解位于边界上，所以 $\\|\\mathbf{s}^{\\star}\\| = \\Delta$。这意味着约束是激活的，即 $(\\mathbfs^{\\star})^{\\top}\\mathbf{s}^{\\star} - \\Delta^2 = 0$。根据互补松弛性条件，这允许 $\\lambda  0$。对于一个凸问题，解位于边界上的条件意味着 $\\lambda$ 将严格为正。\n\n将给定的矩阵 $Q = 3 I_{2}$ 代入驻定性方程：\n$$ (3I_{2} + \\lambda I_{2})\\mathbf{s}^{\\star} = -g $$\n$$ (3 + \\lambda)\\mathbf{s}^{\\star} = -g $$\n由于 $Q$ 是正定的（其特征值均为 $3$），并且 $\\lambda \\geq 0$，矩阵 $Q + \\lambda I_2$ 是对称正定的（其特征值为 $3+\\lambda  0$）。因此，它是可逆的。我们可以解出 $\\mathbf{s}^{\\star}$：\n$$ \\mathbf{s}^{\\star} = -\\frac{1}{3+\\lambda} g $$\n\n现在，我们应用边界条件 $\\|\\mathbf{s}^{\\star}\\| = \\Delta$：\n$$ \\|\\mathbf{s}^{\\star}\\| = \\left\\| -\\frac{1}{3+\\lambda} g \\right\\| = \\frac{1}{3+\\lambda} \\|g\\| $$\n这导出了特征方程，它将 $\\lambda$ 与 $\\Delta$ 联系起来：\n$$ \\frac{\\|g\\|}{3+\\lambda} = \\Delta $$\n我们已知 $g = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$。$g$ 的欧几里得范数是：\n$$ \\|g\\| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$\n将此值代入特征方程得到：\n$$ \\frac{5}{3+\\lambda} = \\Delta $$\n这个方程确定了对于给定的信赖域半径 $\\Delta$，拉格朗日乘子 $\\lambda$ 的值。\n\n下一步是求解此方程，得到 $\\lambda$ 作为 $\\Delta$ 的函数：\n$$ 5 = \\Delta (3+\\lambda) $$\n$$ \\frac{5}{\\Delta} = 3+\\lambda $$\n$$ \\lambda(\\Delta) = \\frac{5}{\\Delta} - 3 $$\n这个表达式在 $\\lambda \\geq 0$ 时有效，这对应于 $\\frac{5}{\\Delta} - 3 \\geq 0$，即 $\\Delta \\leq \\frac{5}{3}$。问题考虑 $\\Delta=1$ 的情况，这满足此条件。\n\n使用这个结果，我们可以找到精确的信赖域步长 $\\mathbf{s}^{\\star}(\\Delta)$：\n$$ 3 + \\lambda(\\Delta) = 3 + \\left(\\frac{5}{\\Delta} - 3\\right) = \\frac{5}{\\Delta} $$\n所以，\n$$ \\mathbf{s}^{\\star}(\\Delta) = -\\frac{1}{3+\\lambda(\\Delta)} g = -\\frac{1}{5/\\Delta} g = -\\frac{\\Delta}{5} g = -\\frac{\\Delta}{5} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} $$\n\n最后，我们需要计算拉格朗日乘子对信赖域半径的灵敏度 $\\frac{d\\lambda}{d\\Delta}$ 在 $\\Delta = 1$ 处的值。我们有 $\\lambda(\\Delta)$ 的表达式：\n$$ \\lambda(\\Delta) = 5\\Delta^{-1} - 3 $$\n我们对 $\\lambda(\\Delta)$ 关于 $\\Delta$ 求导：\n$$ \\frac{d\\lambda}{d\\Delta} = \\frac{d}{d\\Delta} (5\\Delta^{-1} - 3) = -5\\Delta^{-2} = -\\frac{5}{\\Delta^2} $$\n计算该导数在 $\\Delta = 1$ 处的值：\n$$ \\left. \\frac{d\\lambda}{d\\Delta} \\right|_{\\Delta=1} = -\\frac{5}{1^2} = -5 $$\n拉格朗日乘子对信赖域半径在 $\\Delta=1$ 处的灵敏度是 $-5$。", "answer": "$$\n\\boxed{-5}\n$$", "id": "3193674"}, {"introduction": "虽然长期方程为信赖域子问题提供了精确解，但在实际的大规模应用中，我们通常依赖于高效的迭代近似方法。这个练习邀请你实现并比较两种著名的策略：狗腿法 (dogleg method) 和截断共轭梯度法 (truncated Conjugate Gradient method)。通过这种比较分析，你将体会到它们在平衡梯度下降与类牛顿步长以及处理不定黑塞矩阵等挑战方面的不同方法。", "problem": "您的任务是实现并比较两种信赖域步长策略，以最小化一个光滑非线性目标函数。在给定点 $x \\in \\mathbb{R}^n$ 处，考虑一个二阶连续可微函数 $f(x)$ 以及基于其在 $x$ 点的二阶泰勒展开的二次模型：\n$$\nm(s) = f(x) + \\nabla f(x)^\\top s + \\tfrac{1}{2} s^\\top \\nabla^2 f(x) s,\n$$\n其中 $s \\in \\mathbb{R}^n$ 是步长，$\\nabla f(x)$ 是梯度，$\\nabla^2 f(x)$ 是 Hessian 矩阵。信赖域框架通过球形约束 $\\|s\\| \\le \\Delta$ 来限制步长，其中 $\\Delta  0$ 是信赖域半径。\n\n您必须从上述基本定义出发，并且不调用任何现成公式，推导并实现以下两种策略：\n\n- 基于模型的两阶段步长：\n  1. 通过在信赖域约束 $\\|s\\| \\le \\Delta$ 内，沿着负梯度方向最小化二次模型 $m(s)$，计算一个柯西型点。\n  2. 从柯西型点朝向牛顿步（如果存在，即为 $\\nabla^2 f(x) s = -\\nabla f(x)$ 的解）沿多边形狗腿路径移动，并进行截断以满足 $\\|s\\| \\le \\Delta$，从而应用二次校正。最终的两阶段步长即为得到的狗腿步长。\n- 直接二次模型步长：\n  直接求解二次模型的信赖域子问题，即找到最小化 $m(s)$ 且满足 $\\|s\\| \\le \\Delta$ 的步长 $s$。此过程需从基本原理出发，使用截断共轭梯度法（CG）：从 $s=0$ 开始，使用最速下降方向，检测负曲率，并在必要时处理边界截断。\n\n对于这两种策略，使用信赖域接受比率来量化模型与实际目标函数之间的一致性：\n$$\n\\rho = \\frac{f(x) - f(x + s)}{m(0) - m(s)}.\n$$\n如果 $\\rho \\ge \\eta$ 且预测下降量 $m(0) - m(s)$ 严格为正，则认为步长被接受；否则，步长被拒绝。\n\n为以下目标函数实现这两种策略\n$$\nf(x) = \\tfrac{1}{2} x^\\top A x + \\alpha \\left( \\sin(x_1) + \\sin(x_2) \\right) + b^\\top x,\n$$\n其梯度和 Hessian 矩阵为\n$$\n\\nabla f(x) = A x + \\alpha \\begin{bmatrix}\\cos(x_1) \\\\ \\cos(x_2)\\end{bmatrix} + b,\\quad\n\\nabla^2 f(x) = A - \\alpha \\begin{bmatrix}\\sin(x_1)  0 \\\\ 0  \\sin(x_2)\\end{bmatrix}.\n$$\n您必须根据这些基本定义来实现全部逻辑。不要使用任何未从这些基础推导出的函数或公式。\n\n使用以下四个测试用例，每个用例由元组 $(A, \\alpha, b, x, \\Delta, \\eta)$ 指定：\n\n1. 理想情况：\n   - $A = \\begin{bmatrix}3  1 \\\\ 1  2\\end{bmatrix}$，$\\alpha = 0.2$，$b = \\begin{bmatrix}-0.5 \\\\ 0.3\\end{bmatrix}$，$x = \\begin{bmatrix}0.8 \\\\ -0.5\\end{bmatrix}$，$\\Delta = 0.5$，$\\eta = 0.1$。\n2. 小信赖域：\n   - $A = \\begin{bmatrix}4  0 \\\\ 0  1\\end{bmatrix}$，$\\alpha = 0.1$，$b = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$x = \\begin{bmatrix}2.0 \\\\ 2.0\\end{bmatrix}$，$\\Delta = 0.1$，$\\eta = 0.2$。\n3. 不定曲率：\n   - $A = \\begin{bmatrix}-1  0 \\\\ 0  3\\end{bmatrix}$，$\\alpha = 0.5$，$b = \\begin{bmatrix}0.2 \\\\ -0.1\\end{bmatrix}$，$x = \\begin{bmatrix}0.2 \\\\ 2.0\\end{bmatrix}$，$\\Delta = 0.7$，$\\eta = 0.05$。\n4. 近乎平坦的梯度：\n   - $A = \\begin{bmatrix}0.01  0 \\\\ 0  0.02\\end{bmatrix}$，$\\alpha = 0.01$，$b = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$x = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$，$\\Delta = 0.05$，$\\eta = 0.5$。\n\n对于每个用例，使用所述的接受标准，计算两阶段步长是否被接受以及直接二次模型步长是否被接受。然后，根据以下定义为每个测试用例生成一个整数结果：\n- $2$ 如果两种策略都产生可接受的步长，\n- $1$ 如果只有两阶段步长被接受，\n- $-1$ 如果只有直接二次模型步长被接受，\n- $0$ 如果两种策略都没有产生可接受的步长。\n\n您的程序应生成一行输出，其中包含四个测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，例如 $[r_1,r_2,r_3,r_4]$。所有角度必须以弧度为单位，数值输出为无单位整数。", "solution": "该问题要求在一个信赖域优化框架中，实现并比较两种用于计算试探步长的不同策略。我们给定了一个具体的目标函数 $f(x)$、其梯度 $\\nabla f(x)$ 和 Hessian 矩阵 $\\nabla^2 f(x)$。在点 $x$ 处，我们构建该函数的二次模型：\n$$\nm(s) = f(x) + g^\\top s + \\tfrac{1}{2} s^\\top H s\n$$\n其中 $s$ 是步长，$g = \\nabla f(x)$ 是梯度，$H = \\nabla^2 f(x)$ 是在 $x$ 点的 Hessian 矩阵。步长 $s$ 被约束在信赖域内，由不等式 $\\|s\\| \\le \\Delta$ 定义，其中半径 $\\Delta  0$。范数 $\\|\\cdot\\|$ 是标准欧几里得范数。\n\n在使用任一策略计算出步长 $s$ 后，通过实际下降量与预测下降量的比率 $\\rho$ 来评估其质量：\n$$\n\\rho = \\frac{f(x) - f(x+s)}{m(0) - m(s)}\n$$\n预测下降量由 $pred(s) = m(0) - m(s) = -g^\\top s - \\tfrac{1}{2}s^\\top H s$ 给出。如果预测下降量严格为正，即 $pred(s)  0$，并且比率满足 $\\rho \\ge \\eta$（其中 $\\eta$ 为给定的阈值），则该步长被视为“接受”。\n\n该问题定义明确，科学上基于数值优化理论，并提供了所有必要的信息。我们接下来从给定的定义出发，推导并实现这两种策略。\n\n### 策略1：基于模型的两阶段（类狗腿）步长\n\n该策略被描述为一个两阶段过程。首先，计算一个柯西型点。其次，通过从该点向牛顿步移动来应用校正。\n\n**阶段1：柯西点, $s^C$**\n柯西点是通过在信赖域内沿着最速下降方向 $d = -g$ 最小化二次模型 $m(s)$ 来找到的。令步长为 $s(\\tau) = \\tau d = -\\tau g$，其中 $\\tau \\ge 0$。信赖域约束 $\\|s(\\tau)\\| \\le \\Delta$ 转化为 $\\tau\\|g\\| \\le \\Delta$，即 $\\tau \\in [0, \\Delta/\\|g\\|]$。\n我们将 $s(\\tau)$ 代入模型的变化量 $m(s) - m(0)$ 中：\n$$\nm(-\\tau g) - m(0) = -\\tau g^\\top g + \\tfrac{1}{2} \\tau^2 g^\\top H g\n$$\n这是一个关于 $\\tau$ 的一维二次函数。令 $q(\\tau) = m(-\\tau g)$。为了找到最小值：\n1.  如果 $g^\\top H g \\le 0$，则二次函数沿该射线不增加（或为下凹）。因此，在 $\\tau$ 的允许区间内的最小值在边界处取到。我们取可能的最大步长：$\\tau_C = \\Delta / \\|g\\|$。\n2.  如果 $g^\\top H g  0$，通过将其导数设为零来找到二次函数的无约束最小化子：$q'(\\tau) = -g^\\top g + \\tau (g^\\top H g) = 0$，得到 $\\tau^* = \\frac{g^\\top g}{g^\\top H g}$。我们必须遵守信赖域边界，因此最优的 $\\tau$ 是 $\\tau_C = \\min(\\tau^*, \\Delta / \\|g\\|)$。\n\n在这两种情况下，柯西点均为 $s^C = -\\tau_C g$。这一步保证了模型值的下降（除非 $g=0$）。\n\n**阶段2：向牛顿点 $s^N$ 校正**\n牛顿步 $s^N$ 是二次模型的无约束最小化子（如果 $H$ 是正定的），通过求解线性系统 $H s = -g$ 得到。我们通过尝试求解该系统来计算 $s^N$。如果 Hessian 矩阵 $H$ 是奇异的，唯一解可能不存在，此时我们将柯西点 $s^C$ 作为最终步长。\n\n如果成功计算出 $s^N$，我们遵循问题的描述来应用校正：“从柯西型点朝向牛顿步沿多边形狗腿路径移动，并进行截断以满足 $\\|s\\| \\le \\Delta$。”我们将其解释为定义一条沿着连接 $s^C$ 和 $s^N$ 的线段的路径。设该线段上的一点为 $s_{path}(\\beta) = s^C + \\beta(s^N - s^C)$，其中 $\\beta \\in [0, 1]$。最终步长 $s$ 是这个路径向量，但它必须满足 $\\|s\\| \\le \\Delta$。这意味着我们要找到最大的 $\\beta \\in [0, 1]$ 使得 $\\|s^C + \\beta(s^N-s^C)\\| \\le \\Delta$。这等价于求解二次不等式 $\\|v\\|^2\\beta^2 + 2(s^C \\cdot v)\\beta + (\\|s^C\\|^2 - \\Delta^2) \\le 0$，其中 $v = s^N - s^C$。由于 $\\|s^C\\| \\le \\Delta$，常数项为非正数，保证了 $\\beta$ 存在实数解。我们求解相应方程的正根，称之为 $\\beta_{root}$。那么最终步长为 $s = s^C + \\min(1, \\beta_{root})(s^N - s^C)$。\n\n### 策略2：通过截断共轭梯度（CG）法的直接二次模型步长\n\n该策略，也称为 Steihaug-Toint 方法，使用迭代共轭梯度方法直接逼近信赖域子问题 $\\min_s \\{m(s) : \\|s\\| \\le \\Delta\\}$ 的解。标准的 CG 算法经过调整，以处理信赖域约束和 Hessian 矩阵潜在的负曲率。\n\n该算法从初始猜测 $s_0 = 0$ 开始。相应的残差为 $r_0 = \\nabla m(s_0) = g + H s_0 = g$。初始搜索方向为最速下降方向，$p_0 = -r_0 = -g$。\n\n在每次迭代 $k$ 中，算法执行以下检查：\n1.  **负曲率检测**：检查沿当前搜索方向 $p_k$ 的曲率。如果 $p_k^\\top H p_k \\le 0$，则模型 $m(s)$ 在此方向上不受正定二次函数的下界约束。然后将当前步长 $s_k$ 沿 $p_k$ 方向延伸，直到碰到信赖域边界。这需要通过求解一个二次方程找到 $\\tau  0$ 使得 $\\|s_k + \\tau p_k\\| = \\Delta$。得到的步长 $s_k + \\tau p_k$ 即为最终解，算法终止。\n2.  **步长计算**：如果曲率为正，则沿 $p_k$ 方向在子空间中最小化模型的最优步长为 $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top H p_k}$。\n3.  **边界截断**：新的候选步长为 $s_{k+1}' = s_k + \\alpha_k p_k$。如果 $\\|s_{k+1}'\\| \\ge \\Delta$，则步长已超出信赖域。最终解必须位于边界上。我们找到 $\\tau  0$ 使得 $\\|s_k + \\tau p_k\\| = \\Delta$（同样是求解一个二次方程），并以步长 $s_k + \\tau p_k$ 终止。\n4.  **迭代更新**：如果算法尚未终止，则将步长更新为 $s_{k+1} = s_{k+1}'$。残差更新为 $r_{k+1} = r_k + \\alpha_k H p_k$。计算新的共轭搜索方向：$p_{k+1} = -r_{k+1} + \\beta_{k+1} p_k$，其中 $\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$。\n\n该过程持续进行，直到终止或达到最多 $n$ 次迭代（空间的维度），因为在精确算术中，CG 保证最多在 $n$ 步内找到二次函数的精确最小化子。\n\n两种策略都是根据这些基本原理为所提供的目标函数和测试用例实现的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        (np.array([[3, 1], [1, 2]]), 0.2, np.array([-0.5, 0.3]), np.array([0.8, -0.5]), 0.5, 0.1),\n        (np.array([[4, 0], [0, 1]]), 0.1, np.array([0, 0]), np.array([2.0, 2.0]), 0.1, 0.2),\n        (np.array([[-1, 0], [0, 3]]), 0.5, np.array([0.2, -0.1]), np.array([0.2, 2.0]), 0.7, 0.05),\n        (np.array([[0.01, 0], [0, 0.02]]), 0.01, np.array([0, 0]), np.array([0, 0]), 0.05, 0.5),\n    ]\n\n    results = []\n    for A, alpha, b, x, delta, eta in test_cases:\n        # Define objective function and its derivatives\n        def f(vec, A_mat, a_val, b_vec):\n            return 0.5 * vec.T @ A_mat @ vec + a_val * (np.sin(vec[0]) + np.sin(vec[1])) + b_vec.T @ vec\n\n        def grad_f(vec, A_mat, a_val, b_vec):\n            return A_mat @ vec + a_val * np.array([np.cos(vec[0]), np.cos(vec[1])]) + b_vec\n\n        def hess_f(vec, A_mat, a_val):\n            return A_mat - a_val * np.diag([np.sin(vec[0]), np.sin(vec[1])])\n\n        # Calculate values at current point x\n        f_val = f(x, A, alpha, b)\n        g = grad_f(x, A, alpha, b)\n        H = hess_f(x, A, alpha)\n\n        # Common helper functions\n        def predicted_reduction(s, g_vec, H_mat):\n            return -np.dot(g_vec, s) - 0.5 * np.dot(s, H_mat @ s)\n\n        def solve_boundary_intersection(s_vec, p_vec, delta_val):\n            a = np.dot(p_vec, p_vec)\n            b_term = 2 * np.dot(s_vec, p_vec)\n            c = np.dot(s_vec, s_vec) - delta_val**2\n            if a  1e-12: # p is nearly zero vector, no unique intersection\n                return np.inf\n\n            discriminant = b_term**2 - 4 * a * c\n            if discriminant  0:\n                return np.inf # No real intersection\n            \n            # Since s is inside the trust region, c = 0, guaranteeing one positive\n            # and one negative root. We want the positive root.\n            tau = (-b_term + np.sqrt(discriminant)) / (2 * a)\n            return tau\n\n        # --- Strategy 1: Two-Stage (Dogleg-like) Step ---\n        def two_stage_step(g_vec, H_mat, delta_val):\n            g_norm = np.linalg.norm(g_vec)\n            if g_norm  1e-12:\n                return np.zeros_like(g_vec)\n\n            # Stage 1: Cauchy Point\n            d = -g_vec\n            gHg = d.T @ H_mat @ d\n            if gHg = 0:\n                tau = delta_val / g_norm\n            else:\n                tau = min(g_norm**2 / gHg, delta_val / g_norm)\n            s_c = tau * d\n\n            # Stage 2: Correction towards Newton Point\n            try:\n                s_n = np.linalg.solve(H_mat, -g_vec)\n            except np.linalg.LinAlgError:\n                return s_c # Hessian is singular, return Cauchy point\n            \n            v = s_n - s_c\n            beta_root = solve_boundary_intersection(s_c, v, delta_val)\n            \n            beta = min(1.0, beta_root)\n            return s_c + beta * v\n\n        # --- Strategy 2: Direct Quadratic (Truncated CG) Step ---\n        def direct_quadratic_step(g_vec, H_mat, delta_val):\n            s = np.zeros_like(g_vec)\n            r = g_vec\n            p = -g_vec\n            \n            g_norm = np.linalg.norm(g_vec)\n            if g_norm  1e-12:\n                return s\n\n            for j in range(len(g_vec)): # Max n iterations for R^n\n                pHp = p.T @ H_mat @ p\n                if pHp = 0: # Negative curvature detected\n                    tau = solve_boundary_intersection(s, p, delta_val)\n                    return s + tau * p\n                \n                alpha_k = np.dot(r, r) / pHp\n                s_next = s + alpha_k * p\n\n                if np.linalg.norm(s_next) = delta_val:\n                    tau = solve_boundary_intersection(s, p, delta_val)\n                    return s + tau * p\n                \n                s = s_next\n                r_next = r + alpha_k * (H_mat @ p)\n\n                if np.linalg.norm(r_next)  1e-9 * g_norm:\n                    break\n                \n                beta = np.dot(r_next, r_next) / np.dot(r, r)\n                p = -r_next + beta * p\n                r = r_next\n            return s\n        \n        # --- Evaluate both strategies ---\n        accepted1 = False\n        s1 = two_stage_step(g, H, delta)\n        pred1 = predicted_reduction(s1, g, H)\n        if pred1  1e-12:\n            ared1 = f_val - f(x + s1, A, alpha, b)\n            rho1 = ared1 / pred1\n            if rho1 = eta:\n                accepted1 = True\n        \n        accepted2 = False\n        s2 = direct_quadratic_step(g, H, delta)\n        pred2 = predicted_reduction(s2, g, H)\n        if pred2  1e-12:\n            ared2 = f_val - f(x + s2, A, alpha, b)\n            rho2 = ared2 / pred2\n            if rho2 = eta:\n                accepted2 = True\n\n        # Determine outcome\n        if accepted1 and accepted2:\n            outcome = 2\n        elif accepted1 and not accepted2:\n            outcome = 1\n        elif not accepted1 and accepted2:\n            outcome = -1\n        else:\n            outcome = 0\n        results.append(outcome)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3193710"}]}