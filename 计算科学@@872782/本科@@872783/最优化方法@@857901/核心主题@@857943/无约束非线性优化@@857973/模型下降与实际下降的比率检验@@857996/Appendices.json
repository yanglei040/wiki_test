{"hands_on_practices": [{"introduction": "要真正掌握模型与实际下降量的比率检验，最有效的方法之一就是亲手进行一次完整的计算。这个练习将引导你完成这个基本过程。我们将从一个具体的一维函数出发，计算在阻尼牛顿法（Levenberg-Marquardt方法）的某一步迭代中，实际下降量与模型预测下降量，从而得出比率 $\\rho_k$，并根据这个比率的值来更新阻尼参数。这个练习旨在夯实你的计算基础，让你清晰地看到比率检验如何为算法提供反馈。[@problem_id:3152688]", "problem": "考虑使用通过 Levenberg–Marquardt (LM) 修正计算的阻尼牛顿步来最小化一个二次连续可微的标量函数 $f(x)$。在迭代点 $x_k$ 处，带有参数 $\\lambda_k$ 的 LM 步长 $s_k$ 由线性系统 $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$ 隐式定义，其中 $I$ 是单位算子，$\\nabla f(x)$ 和 $\\nabla^{2} f(x)$ 分别表示梯度和 Hessian 矩阵。$f$ 在 $x_k$ 附近的局部二次泰勒模型是 $m_k(s) = f(x_k) + \\nabla f(x_k)^{\\top} s + \\tfrac{1}{2}s^{\\top} \\nabla^{2} f(x_k) s$。实际下降量为 $A_k(s) = f(x_k) - f(x_k + s)$，预测下降量为 $P_k(s) = f(x_k) - m_k(s)$。比率检验使用量 $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)}$ 来判断模型相对于实际下降量的质量。\n\n在一维情况下，使用函数 $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$。在当前迭代点 $x_k = \\dfrac{1}{2}$，设置 LM 参数为 $\\lambda_k = \\dfrac{1}{20}$。使用二次泰勒模型的基本定义（将 $f$ 在 $x_k$ 处的泰勒级数截断至二阶）和 LM 步长定义，计算由 $(\\nabla^{2} f(x_k) + \\lambda_k I)s_k = -\\nabla f(x_k)$ 得到的步长 $s_k$ 的比率 $\\rho_k$。然后应用以下基于比率的 LM 更新策略：\n- 如果 $\\rho_k  \\underline{\\eta}$，其中 $\\underline{\\eta} = \\dfrac{1}{4}$，则设置 $\\lambda_{k+1} = 8 \\lambda_k$。\n- 如果 $\\underline{\\eta} \\le \\rho_k \\le \\overline{\\eta}$，其中 $\\overline{\\eta} = \\dfrac{3}{4}$，则设置 $\\lambda_{k+1} = \\lambda_k$。\n- 如果 $\\rho_k > \\overline{\\eta}$，则设置 $\\lambda_{k+1} = \\dfrac{\\lambda_k}{2}$。\n\n你的最终答案必须是 $\\lambda_{k+1}$ 的单一值，以精确分数形式表示。无需四舍五入。该场景的设定旨在使得过度激进的阻尼（即 $\\lambda_k$ 太小）产生一个步长，其二次模型预测的下降量相对于实际下降量而言效果很差，从而触发 $\\lambda_k$ 的增加。", "solution": "问题是要求根据实际下降量与预测下降量的比率 $\\rho_k$，为一个特定的一维函数 $f(x)$ 在给定点 $x_k$ 和给定的初始 LM 参数 $\\lambda_k$ 下，确定下一个 Levenberg-Marquardt (LM) 参数 $\\lambda_{k+1}$。\n\n首先，我们定义函数并计算其一阶和二阶导数：\n函数为 $f(x) = \\dfrac{1}{4}x^{4} - x^{3} + \\dfrac{3}{2}x^{2}$。\n一阶导数（一维中的梯度）为 $f'(x) = \\dfrac{d f}{dx} = x^{3} - 3x^{2} + 3x$。\n二阶导数（一维中的 Hessian 矩阵）为 $f''(x) = \\dfrac{d^{2} f}{dx^{2}} = 3x^{2} - 6x + 3 = 3(x-1)^{2}$。\n\n接下来，我们在当前迭代点 $x_k = \\dfrac{1}{2}$ 处计算函数及其导数的值：\n$f(x_k) = f\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{4}\\left(\\dfrac{1}{2}\\right)^{4} - \\left(\\dfrac{1}{2}\\right)^{3} + \\dfrac{3}{2}\\left(\\dfrac{1}{2}\\right)^{2} = \\dfrac{1}{4}\\left(\\dfrac{1}{16}\\right) - \\dfrac{1}{8} + \\dfrac{3}{2}\\left(\\dfrac{1}{4}\\right) = \\dfrac{1}{64} - \\dfrac{8}{64} + \\dfrac{24}{64} = \\dfrac{17}{64}$。\n$f'(x_k) = f'\\left(\\dfrac{1}{2}\\right) = \\left(\\dfrac{1}{2}\\right)^{3} - 3\\left(\\dfrac{1}{2}\\right)^{2} + 3\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{8} - \\dfrac{3}{4} + \\dfrac{3}{2} = \\dfrac{1}{8} - \\dfrac{6}{8} + \\dfrac{12}{8} = \\dfrac{7}{8}$。\n$f''(x_k) = f''\\left(\\dfrac{1}{2}\\right) = 3\\left(\\dfrac{1}{2} - 1\\right)^{2} = 3\\left(-\\dfrac{1}{2}\\right)^{2} = 3\\left(\\dfrac{1}{4}\\right) = \\dfrac{3}{4}$。\n\nLM 步长 $s_k$ 使用给定参数 $\\lambda_k = \\dfrac{1}{20}$ 计算。一维中的 LM 系统为：\n$$ (f''(x_k) + \\lambda_k) s_k = -f'(x_k) $$\n代入计算出的值：\n$$ \\left(\\dfrac{3}{4} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} $$\n$$ \\left(\\dfrac{15}{20} + \\dfrac{1}{20}\\right) s_k = -\\dfrac{7}{8} $$\n$$ \\dfrac{16}{20} s_k = -\\dfrac{7}{8} $$\n$$ \\dfrac{4}{5} s_k = -\\dfrac{7}{8} $$\n解出 $s_k$：\n$$ s_k = -\\dfrac{7}{8} \\cdot \\dfrac{5}{4} = -\\dfrac{35}{32} $$\n\n现在，我们计算比率 $\\rho_k = \\dfrac{A_k(s_k)}{P_k(s_k)}$。\n预测下降量为 $P_k(s_k) = -f'(x_k)s_k - \\dfrac{1}{2}f''(x_k)s_k^2$。\n实际下降量为 $A_k(s_k) = f(x_k) - f(x_k + s_k)$。\n\n由于 $f(x)$ 是一个四阶多项式，我们可以用泰勒余项精确地表示实际下降量和预测下降量之间的关系。\n$A_k(s_k) = P_k(s_k) - \\left( \\dfrac{1}{6}f'''(x_k)s_k^3 + \\dfrac{1}{24}f^{(4)}(x_k)s_k^4 \\right)$。\n令余项为 $R_k(s_k) = \\dfrac{1}{6}f'''(x_k)s_k^3 + \\dfrac{1}{24}f^{(4)}(x_k)s_k^4$。\n则比率为 $\\rho_k = \\dfrac{P_k(s_k) - R_k(s_k)}{P_k(s_k)} = 1 - \\dfrac{R_k(s_k)}{P_k(s_k)}$。\n\n我们计算 $P_k(s_k)$ 和 $R_k(s_k)$。\n$P_k(s_k) = -\\left(\\dfrac{7}{8}\\right)\\left(-\\dfrac{35}{32}\\right) - \\dfrac{1}{2}\\left(\\dfrac{3}{4}\\right)\\left(-\\dfrac{35}{32}\\right)^2 = \\dfrac{245}{256} - \\dfrac{3}{8}\\left(\\dfrac{1225}{1024}\\right) = \\dfrac{7840 - 3675}{8192} = \\dfrac{4165}{8192}$。\n三阶和四阶导数是：\n$f'''(x) = 6x-6$。在 $x_k = \\dfrac{1}{2}$ 处，$f'''(\\frac{1}{2}) = 6(\\frac{1}{2}) - 6 = -3$。\n$f^{(4)}(x) = 6$。在 $x_k = \\dfrac{1}{2}$ 处，$f^{(4)}(\\frac{1}{2}) = 6$。\n代入 $R_k(s_k)$：\n$R_k(s_k) = \\dfrac{1}{6}(-3)s_k^3 + \\dfrac{1}{24}(6)s_k^4 = -\\dfrac{1}{2}s_k^3 + \\dfrac{1}{4}s_k^4 = s_k^3\\left(-\\dfrac{1}{2} + \\dfrac{1}{4}s_k\\right)$\n$R_k(s_k) = \\left(-\\dfrac{35}{32}\\right)^3 \\left(-\\dfrac{1}{2} + \\dfrac{1}{4}\\left(-\\dfrac{35}{32}\\right)\\right) = \\left(-\\dfrac{42875}{32768}\\right) \\left(-\\dfrac{1}{2} - \\dfrac{35}{128}\\right)$\n$R_k(s_k) = \\left(-\\dfrac{42875}{32768}\\right) \\left(\\dfrac{-64-35}{128}\\right) = \\left(-\\dfrac{42875}{32768}\\right) \\left(-\\dfrac{99}{128}\\right) = \\dfrac{42875 \\cdot 99}{32768 \\cdot 128} = \\dfrac{4244625}{4194304}$。\n\n现在我们可以计算分数 $\\dfrac{R_k(s_k)}{P_k(s_k)}$：\n$$ \\frac{R_k(s_k)}{P_k(s_k)} = \\frac{4244625/4194304}{4165/8192} = \\frac{4244625}{4194304} \\cdot \\frac{8192}{4165} = \\frac{4244625}{512 \\cdot 4165} $$\n为简化此分数，我们进行质因数分解：\n$4165 = 5 \\cdot 7^2 \\cdot 17$\n$4244625 = 42875 \\cdot 99 = (5^3 \\cdot 7^3) \\cdot (3^2 \\cdot 11)$\n$512 = 2^9$\n将这些代入分数中：\n$$ \\frac{R_k(s_k)}{P_k(s_k)} = \\frac{5^3 \\cdot 7^3 \\cdot 3^2 \\cdot 11}{2^9 \\cdot (5 \\cdot 7^2 \\cdot 17)} = \\frac{5^2 \\cdot 7 \\cdot 3^2 \\cdot 11}{2^9 \\cdot 17} = \\frac{25 \\cdot 7 \\cdot 9 \\cdot 11}{512 \\cdot 17} = \\frac{17325}{8704} $$\n因此，$\\rho_k = 1 - \\dfrac{17325}{8704} = \\dfrac{8704 - 17325}{8704} = -\\dfrac{8621}{8704}$。\n\n最后，我们应用 $\\lambda_{k+1}$ 的更新规则。阈值为 $\\underline{\\eta} = \\dfrac{1}{4}$ 和 $\\overline{\\eta} = \\dfrac{3}{4}$。\n由于 $\\rho_k = -\\dfrac{8621}{8704}$ 是一个负数，它显然小于 $\\underline{\\eta} = \\dfrac{1}{4}$。\n规则规定：如果 $\\rho_k  \\underline{\\eta}$，则设置 $\\lambda_{k+1} = 8 \\lambda_k$。\n给定 $\\lambda_k = \\dfrac{1}{20}$，新的参数是：\n$$ \\lambda_{k+1} = 8 \\cdot \\lambda_k = 8 \\cdot \\dfrac{1}{20} = \\dfrac{8}{20} = \\dfrac{2}{5} $$\n二次模型的质量不佳（由负的 $\\rho_k$ 表明）正确地触发了阻尼参数 $\\lambda$ 的增加，这将在下一次迭代中导致一个更小、更保守的步长。", "answer": "$$\\boxed{\\frac{2}{5}}$$", "id": "3152688"}, {"introduction": "比率检验并非总是能简单地通过几个阈值来诠释。这个练习将带你进入一个更微妙的场景，挑战你对 $\\rho_k$ 的直观理解。我们将探索一个目标函数，该函数在某处存在“扭折”（即Hessian矩阵发生跳变），而算法的试探步恰好跨越了这个扭折点。你会发现这导致了一个异常大的 $\\rho_k$ 值，练习的核心在于分析模型预测为何会严重失效，并设计一种更智能的信赖域半径更新策略，以避免算法在扭折点附近反复振荡，从而展示深刻理解比率检验对于构建稳健算法的重要性。[@problem_id:3152655]", "problem": "考虑一个用于无约束优化的信赖域方法，其模型为 $m_k(s) = f(x_k) + \\nabla f(x_k)^\\top s + \\tfrac{1}{2} s^\\top B_k s$，其中 $B_k$ 近似于 $\\nabla^2 f(x_k)$。比率测试使用实际下降量和模型预测下降量，其定义为\n$$\\text{ared}_k = f(x_k) - f(x_k + s_k), \\quad \\text{pred}_k = m_k(0) - m_k(s_k) = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k,$$\n以及比率\n$$\\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k}。$$\n假设目标函数 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 是一个分段三次函数，在 $x_1 = 0$ 处有一个扭结，并且海森矩阵在该扭结处有跳跃：\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1 \\ge 0, \\\\[6pt]\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1  0,\n\\end{cases}\n$$\n参数为 $\\alpha = 3$，$\\beta_+ = 4$，$\\beta_- = 1$，$\\gamma = -3$ 和 $\\delta = 2$。设当前迭代点为 $x_k = (0.05,\\, 0.2)$，信赖域半径为 $\\Delta_k = 1.0$。算法返回一个步长 $s_k = (-0.50,\\, -0.73)$，该步长穿过了扭结（因为 $x_{k,1} + s_{k,1} = -0.45  0$）。取 $B_k = \\nabla^2 f(x_k)$，接受阈值为 $\\eta_1 = 0.1$ 和 $\\eta_2 = 0.75$。\n\n任务：\n- 使用 $f$ 在 $x_1 \\ge 0$ 一侧的定义计算 $\\nabla f(x_k)$ 和 $B_k$。\n- 计算 $\\text{pred}_k$ 和 $\\text{ared}_k$。\n- 计算 $\\rho_k$ 并使用比率测试对其进行解释。\n- 在接受步长 $s_k$ 后，设计一个信赖域半径 $\\Delta_{k+1}$ 的更新规则，以避免重复穿越扭结 $x_1 = 0$。\n\n哪个选项正确报告了 $\\rho_k$ 的近似值，并给出了一个避免重复穿越 $x_1 = 0$ 的半径更新规则？\n\nA. $\\rho_k \\approx 0.24$。拒绝该步长并设置 $\\Delta_{k+1} = 0.5\\,\\Delta_k$。\n\nB. $\\rho_k \\approx 1.2$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$。\n\nC. $\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ 以避免重复穿越 $x_1 = 0$。\n\nD. $\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$，因为高 $\\rho_k$ 值总是保证需要扩大半径。", "solution": "该问题陈述是数值优化中的一个有效练习。它提出了一个定义明确、自成一体的场景，旨在测试信赖域算法在非光滑目标函数上的行为。所有参数和条件都已明确指定，问题是客观的且可以用数学形式化的。\n\n我们将按概述的任务进行。\n\n目标函数 $f(x)$ 由下式给出：\n$$\nf(x) = \\begin{cases}\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_+ x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1 \\ge 0 \\\\\n\\tfrac{1}{3}\\alpha x_1^3 + \\tfrac{1}{2}\\beta_- x_1^2 + \\gamma x_1 x_2 + \\tfrac{1}{2}\\delta x_2^2,  x_1  0\n\\end{cases}\n$$\n当参数 $\\alpha = 3$，$\\beta_+ = 4$，$\\beta_- = 1$，$\\gamma = -3$ 和 $\\delta = 2$ 时，函数变为：\n$$\nf(x) = \\begin{cases}\nx_1^3 + 2 x_1^2 - 3 x_1 x_2 + x_2^2,  x_1 \\ge 0 \\\\\nx_1^3 + \\tfrac{1}{2} x_1^2 - 3 x_1 x_2 + x_2^2,  x_1  0\n\\end{cases}\n$$\n当前迭代点为 $x_k = (0.05, 0.2)$。建议的步长为 $s_k = (-0.50, -0.73)$。\n下一个点将是 $x_{k+1} = x_k + s_k = (0.05 - 0.50, 0.2 - 0.73) = (-0.45, -0.53)$。\n\n**1. 计算 $\\nabla f(x_k)$ 和 $B_k$。**\n\n由于 $x_{k,1} = 0.05 \\ge 0$，我们使用 $f(x)$ 在 $x_1 \\ge 0$ 时的定义来计算 $x_k$ 处的梯度和海森矩阵。\n对于 $x_1  0$，梯度为：\n$$ \\nabla f(x) = \\begin{pmatrix} 3x_1^2 + 4x_1 - 3x_2 \\\\ -3x_1 + 2x_2 \\end{pmatrix} $$\n在 $x_k = (0.05, 0.2)$ 处求值：\n$$ \\nabla f(x_k) = \\begin{pmatrix} 3(0.05)^2 + 4(0.05) - 3(0.2) \\\\ -3(0.05) + 2(0.2) \\end{pmatrix} = \\begin{pmatrix} 3(0.0025) + 0.2 - 0.6 \\\\ -0.15 + 0.4 \\end{pmatrix} = \\begin{pmatrix} 0.0075 - 0.4 \\\\ 0.25 \\end{pmatrix} = \\begin{pmatrix} -0.3925 \\\\ 0.25 \\end{pmatrix} $$\n模型海森矩阵 $B_k$ 是真实的海森矩阵 $\\nabla^2 f(x_k)$。对于 $x_1  0$，海森矩阵为：\n$$ \\nabla^2 f(x) = \\begin{pmatrix} 6x_1 + 4  -3 \\\\ -3  2 \\end{pmatrix} $$\n在 $x_k = (0.05, 0.2)$ 处求值：\n$$ B_k = \\nabla^2 f(x_k) = \\begin{pmatrix} 6(0.05) + 4  -3 \\\\ -3  2 \\end{pmatrix} = \\begin{pmatrix} 0.3 + 4  -3 \\\\ -3  2 \\end{pmatrix} = \\begin{pmatrix} 4.3  -3 \\\\ -3  2 \\end{pmatrix} $$\n\n**2. 计算 $\\text{pred}_k$ 和 $\\text{ared}_k$。**\n\n模型预测下降量为 $\\text{pred}_k = -\\nabla f(x_k)^\\top s_k - \\tfrac{1}{2} s_k^\\top B_k s_k$。\n第一项是：\n$$ -\\nabla f(x_k)^\\top s_k = - \\begin{pmatrix} -0.3925  0.25 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = - ((-0.3925)(-0.50) + (0.25)(-0.73)) = - (0.19625 - 0.1825) = -0.01375 $$\n第二项涉及二次型 $s_k^\\top B_k s_k$：\n$$ s_k^\\top B_k s_k = \\begin{pmatrix} -0.50  -0.73 \\end{pmatrix} \\begin{pmatrix} 4.3  -3 \\\\ -3  2 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} (-0.50)(4.3) + (-0.73)(-3)  (-0.50)(-3) + (-0.73)(2) \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = \\begin{pmatrix} -2.15 + 2.19  1.5 - 1.46 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} = \\begin{pmatrix} 0.04  0.04 \\end{pmatrix} \\begin{pmatrix} -0.50 \\\\ -0.73 \\end{pmatrix} $$\n$$ = (0.04)(-0.50) + (0.04)(-0.73) = -0.02 - 0.0292 = -0.0492 $$\n所以，$\\text{pred}_k$ 的第二项是 $-\\tfrac{1}{2} s_k^\\top B_k s_k = -\\tfrac{1}{2}(-0.0492) = 0.0246$。\n因此，模型预测下降量为：\n$$ \\text{pred}_k = -0.01375 + 0.0246 = 0.01085 $$\n实际下降量为 $\\text{ared}_k = f(x_k) - f(x_k + s_k)$。\n首先，我们使用 $x_1 \\ge 0$ 的公式计算 $f(x_k) = f(0.05, 0.2)$：\n$$ f(0.05, 0.2) = (0.05)^3 + 2(0.05)^2 - 3(0.05)(0.2) + (0.2)^2 = 0.000125 + 2(0.0025) - 0.03 + 0.04 = 0.015125 $$\n接下来，我们使用 $x_1  0$ 的公式计算 $f(x_k + s_k) = f(-0.45, -0.53)$：\n$$ f(-0.45, -0.53) = (-0.45)^3 + \\tfrac{1}{2}(-0.45)^2 - 3(-0.45)(-0.53) + (-0.53)^2 $$\n$$ = -0.091125 + \\tfrac{1}{2}(0.2025) - 3(0.2385) + 0.2809 $$\n$$ = -0.091125 + 0.10125 - 0.7155 + 0.2809 = 0.010125 - 0.4346 = -0.424475 $$\n实际下降量为：\n$$ \\text{ared}_k = 0.015125 - (-0.424475) = 0.4396 $$\n\n**3. 计算 $\\rho_k$ 并对其进行解释。**\n\n比率为：\n$$ \\rho_k = \\frac{\\text{ared}_k}{\\text{pred}_k} = \\frac{0.4396}{0.01085} \\approx 40.516 $$\n该值约等于 $40$。\n接受阈值为 $\\eta_1 = 0.1$ 和 $\\eta_2 = 0.75$。由于 $\\rho_k \\approx 40  \\eta_2$，该步长被认为非常成功，应该被接受：$x_{k+1} = x_k + s_k$。\n\n**4. 设计信赖域半径 $\\Delta_{k+1}$ 的更新规则。**\n\n$\\rho_k$ 的极大值表明模型 $m_k(s)$ 与真实函数 $f(x)$ 在步长 $s_k$ 方向上存在严重不匹配。这种不匹配的发生是因为模型是在 $x_k$（在 $x_1  0$ 区域）处构建的，而步长却进入了 $x_1  0$ 区域，在该区域函数有不同的定义（以及不同的海森矩阵）。标准的信赖域算法可能会因为这一非常成功的步长而扩大半径（例如 $\\Delta_{k+1} = 2\\Delta_k$）。然而，这将是一个天真的反应。在新点 $x_{k+1}$（其中 $x_{k+1,1} = -0.45  0$）处设置一个大的信赖域，很可能导致新的一步再次穿越扭结回到 $x_1  0$ 区域，从而导致又一次糟糕的模型预测和可能在扭结周围的振荡。\n\n一个更精巧的策略是接受该步长，然后为下一次迭代选择一个足够小的信赖域半径，以防止立即穿越回扭结。这使得算法能够构建一个新模型 $m_{k+1}(s)$，该模型在 $x_{k+1}$ 周围的局部区域是准确的。一个合理的启发式策略是将新半径 $\\Delta_{k+1}$ 设置为不大于新点到扭结的距离，并带有一个安全系数。从 $x_{k+1}$ 到直线 $x_1 = 0$ 的距离是 $|x_{k+1,1}| = |-0.45| = 0.45$。因此，我们应该要求 $\\Delta_{k+1} \\le c |x_{k+1,1}|$，其中某个 $c  1$（例如，$c = 0.8$）。\n\n现在我们根据这些发现来评估各个选项。\n\n**选项 A：$\\rho_k \\approx 0.24$。拒绝该步长并设置 $\\Delta_{k+1} = 0.5\\,\\Delta_k$。**\n$\\rho_k \\approx 0.24$ 这个值是错误的。我们的计算得出 $\\rho_k \\approx 40$。由于 $\\rho_k  \\eta_1$，该步长应该被接受，而不是拒绝。\n**结论：错误。**\n\n**选项 B：$\\rho_k \\approx 1.2$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$。**\n$\\rho_k \\approx 1.2$ 这个值是错误的。我们的计算得出 $\\rho_k \\approx 40$。\n**结论：错误。**\n\n**选项 C：$\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ 以避免重复穿越 $x_1 = 0$。**\n$\\rho_k \\approx 40$ 的值与我们的计算相符。“接受该步长”是正确的操作，因为 $\\rho_k  \\eta_2$。建议的半径更新规则 $\\Delta_{k+1} := \\min\\{1.5\\,\\Delta_k,\\, 0.8\\,|x_{k+1,1}|\\}$ 实施了上述的精巧启发式策略。它将成功步长的标准半径扩大（$1.5\\,\\Delta_k = 1.5$）与防止穿越扭结的约束（$0.8\\,|x_{k+1,1}| = 0.8 \\cdot 0.45 = 0.36$）相结合。新半径将是 $\\Delta_{k+1} = \\min\\{1.5, 0.36\\} = 0.36$。对于这个特定问题，这是一个出色的算法选择。\n**结论：正确。**\n\n**选项 D：$\\rho_k \\approx 40$。接受该步长并设置 $\\Delta_{k+1} = 2\\,\\Delta_k$，因为高 $\\rho_k$ 值总是保证需要扩大半径。**\n$\\rho_k \\approx 40$ 的值和“接受该步长”的操作是正确的。然而，其推理和建议的半径更新规则是有缺陷的。非常高的 $\\rho_k$ 值表明模型非常差，盲目扩大信赖域很可能会使穿越不连续性的问题持续存在。认为高 $\\rho_k$ 值“总是”保证需要扩大半径的理由是一个错误且天真的启发式策略。选项 C 提供了一个更为稳健的策略。\n**结论：错误。**", "answer": "$$\\boxed{C}$$", "id": "3152655"}, {"introduction": "比率检验的威力远不止于决定是否接受一个步长。在最后的这个实践中，我们将展示它如何作为一个通用的反馈机制，用于优化算法的“自我调节”。我们将实现一个方案，其中 $\\rho_k$ 不仅用于步长控制，还用于动态调整构建模型时所用的有限差分步长 $h_k$。这个练习的目标是通过代码实践，理解如何利用比率检验来提升模型本身的精度，从而使 $\\rho_k$ 稳定在理想值 1 附近，这揭示了比率检验在自适应算法设计中的核心作用。[@problem_id:3152595]", "problem": "要求您在一个信赖域框架中实现并研究一个一维比率测试，其中，实际下降量与模型下降量之比（记为 $\\rho_k$）不仅用于决定是否接受一个试探步，还用于调整构建局部模型时所用的有限差分步长 $h_k$。您的任务是设计一个程序，在一个小型测试集上执行此过程，并报告一个基于 $\\rho_k$ 接近 $1$ 的程度的定量稳定性度量。\n\n从以下基本数学原理开始。\n\n- 一个充分光滑的标量函数 $f:\\mathbb{R}\\to\\mathbb{R}$ 在点 $x_k$ 附近的局部行为可用二阶泰勒展开表示，\n  $$f(x_k+s) \\approx f(x_k) + \\nabla f(x_k)\\,s + \\tfrac{1}{2}\\,\\nabla^2 f(x_k)\\,s^2,$$\n  这启发了我们使用二次模型\n  $$m_k(s) = f(x_k) + g_k\\,s + \\tfrac{1}{2}\\,B_k\\,s^2,$$\n  其中 $g_k$ 和 $B_k$ 分别是 $\\nabla f(x_k)$ 和 $\\nabla^2 f(x_k)$ 的近似值。\n\n- 当精确导数不可用时，使用一个正步长 $h_k$ 通过中心有限差分来近似 $g_k$ 和 $B_k$：\n  $$g_k = \\frac{f(x_k+h_k) - f(x_k-h_k)}{2 h_k},\\qquad B_k = \\frac{f(x_k+h_k) - 2 f(x_k) + f(x_k-h_k)}{h_k^2}。$$\n\n- 给定一个信赖域半径 $\\Delta_k0$，通过最小化 $m_k(s)$ （约束条件为 $|s|\\le \\Delta_k$）来构建试探步 $s_k$。在一维情况下，当 $B_k0$ 时，无约束最小化子为 $s^\\star_k = -g_k/B_k$，而信赖域步是其投影\n  $s_k = \\operatorname{clip}\\!\\big(s^\\star_k, -\\Delta_k, \\Delta_k\\big),$\n  其中 $\\operatorname{clip}(u,a,b)$ 返回 $\\min(\\max(u,a),b)$。如果 $B_k \\le 0$，则取 $s_k = -\\operatorname{sign}(g_k)\\,\\Delta_k$。\n\n- 定义预测下降量和实际下降量，\n  $$\\operatorname{pred}_k = m_k(0) - m_k(s_k) = -g_k s_k - \\tfrac{1}{2} B_k s_k^2,\\qquad \\operatorname{ared}_k = f(x_k) - f(x_k+s_k),$$\n  以及比率\n  $$\\rho_k = \\begin{cases}\n  \\dfrac{\\operatorname{ared}_k}{\\operatorname{pred}_k},  \\text{if } \\operatorname{pred}_k0,\\\\\n  0,  \\text{otherwise.}\n  \\end{cases}$$\n\n- 如果 $\\rho_k \\ge \\eta_{\\mathrm{acc}}$（给定阈值 $\\eta_{\\mathrm{acc}}\\in (0,1)$），则接受该步，并设置 $x_{k+1}=x_k+s_k$；否则拒绝该步，并设置 $x_{k+1}=x_k$。\n\n您的实现还必须利用 $\\rho_k$ 中的信息来调整有限差分步长 $h_k$，以使 $\\rho_k$ 稳定在 $1$ 附近。使用以下带限位的确定性更新规则：\n- 如果 $\\rho_k  \\eta_{\\mathrm{low}}$，设置 $h_{k+1} = \\min\\{\\gamma_{\\uparrow} h_k, h_{\\max}\\}$。\n- 否则如果 $\\rho_k > \\eta_{\\mathrm{high}}$，设置 $h_{k+1} = \\max\\{\\gamma_{\\downarrow} h_k, h_{\\min}\\}$。\n- 否则，设置 $h_{k+1} = h_k$。\n\n所有测试案例的常数固定如下：\n- 信赖域半径 $\\Delta_k \\equiv \\Delta = 0.5$，\n- 接受阈值 $\\eta_{\\mathrm{acc}} = 0.1$，\n- 有限差分步长边界 $h_{\\min} = 10^{-6}$ 和 $h_{\\max} = 10^{-1}$，\n- 调整阈值 $\\eta_{\\mathrm{low}} = 0.5$ 和 $\\eta_{\\mathrm{high}} = 1.5$，\n- 调整因子 $\\gamma_{\\uparrow} = 1.5$ 和 $\\gamma_{\\downarrow} = 2/3$，\n- 总迭代次数 $K=100$。\n\n使用测试函数族\n$$f(x) = \\big(x - c\\big)^4 + \\alpha \\,\\sin(\\omega x),\\quad \\text{with } c = 1.5,$$\n其中振荡项是一个确定性扰动，它使得有限差分对 $h_k$ 的敏感性变得有意义。对于每个测试案例，参数 $(\\alpha,\\omega,x_0,h_0)$ 在下方指定。按照给定值初始化 $x_0$ 和 $h_0$，并应用上述迭代方案进行 $K$ 次迭代。\n\n运行 $K$ 次迭代后，为每个测试案例计算一个稳定性度量，其定义为最后 $J$ 个被接受的步的 $\\big|1-\\rho_k\\big|$ 的算术平均值，其中 $J=10$。如果被接受的步数少于 $J$ 步，则对所有被接受的步求平均；如果没有步被接受，则将该度量定义为浮点非数值（not-a-number）。将报告的度量四舍五入到 $6$ 位小数。\n\n测试集：\n- 案例 A（初始有限差分步长过小）：$(\\alpha,\\omega,x_0,h_0) = \\big(10^{-3},\\,10^{3},\\,0,\\,10^{-6}\\big)$。\n- 案例 B（初始有限差分步长过大）：$(\\alpha,\\omega,x_0,h_0) = \\big(10^{-3},\\,10^{3},\\,0,\\,10^{-1}\\big)$。\n- 案例 C（扰动较温和的理想情况）：$(\\alpha,\\omega,x_0,h_0) = \\big(10^{-4},\\,500,\\,3,\\,10^{-4}\\big)$。\n- 案例 D（无振荡基线）：$(\\alpha,\\omega,x_0,h_0) = \\big(0,\\,10^{3},\\,-2,\\,10^{-6}\\big)$。\n\n您的程序必须生成单行输出，其中包含案例 A–D 的度量，按顺序排列，形式为用方括号括起来的逗号分隔列表。例如，要求的格式是\n\"[mA,mB,mC,mD]\"\n其中每个 $m\\cdot$ 是对应的四舍五入后的浮点值，表示为小数点后恰好有 $6$ 位数字的十进制数。本问题不涉及任何物理单位或角度单位。", "solution": "问题陈述是有效的。它在科学上是合理的，问题是适定的，并为优化理论中的一个数值实验提供了完整、明确的规范。所有常数、初始条件和过程规则都已定义，且没有内部矛盾。因此，我们可以着手求解。\n\n任务是实现并分析一个一维信赖域优化算法。该算法的一个关键特征是它使用自适应的有限差分步长 $h_k$，该步长根据目标函数的二次模型的质量进行调整。质量由目标函数的实际下降量与模型预测的下降量之比 $\\rho_k$ 来衡量。总体目标是为一个给定的测试集实现此过程，并报告一个稳定性度量，该度量用于量化算法将比率 $\\rho_k$ 稳定在理想值 $1$ 附近的表现有多好。\n\n该算法总共迭代 $K=100$ 步。在每次迭代 $k$ 中，从当前点 $x_k$ 和有限差分步长 $h_k$ 开始，执行以下操作序列。\n\n首先，在 $x_k$ 周围构建目标函数 $f(x)$ 的一个局部二次模型 $m_k(s)$。目标函数属于函数族 $f(x) = (x - c)^4 + \\alpha \\sin(\\omega x)$，其中常数 $c=1.5$。该模型采用二阶泰勒展开的形式：\n$$m_k(s) = f(x_k) + g_k s + \\frac{1}{2} B_k s^2$$\n由于假设精确导数不可用，梯度近似 $g_k \\approx \\nabla f(x_k)$ 和黑塞矩阵近似 $B_k \\approx \\nabla^2 f(x_k)$ 使用中心有限差分公式和步长 $h_k$ 计算：\n$$g_k = \\frac{f(x_k+h_k) - f(x_k-h_k)}{2 h_k}$$\n$$B_k = \\frac{f(x_k+h_k) - 2 f(x_k) + f(x_k-h_k)}{h_k^2}$$\n\n其次，通过在由 $|s| \\le \\Delta_k$ 定义的信赖域内最小化模型 $m_k(s)$ 来计算试探步 $s_k$。信赖域半径保持恒定为 $\\Delta_k \\equiv \\Delta = 0.5$。这个一维子问题的解取决于模型曲率 $B_k$ 的符号。\n- 如果 $B_k  0$，模型是一个凸抛物线，其无约束最小化子位于 $s^\\star_k = -g_k/B_k$。通过将此最小化子投影到信赖域区间 $[-\\Delta, \\Delta]$ 上找到试探步 $s_k$，这通过裁剪操作 $s_k = \\operatorname{clip}(s^\\star_k, -\\Delta, \\Delta)$ 实现。\n- 如果 $B_k \\le 0$，模型是凹的或线性的，其在区间 $[-\\Delta, \\Delta]$ 上的最小值必定位于边界之一。步长沿最速下降方向取值，即 $s_k = -\\operatorname{sign}(g_k)\\Delta$。\n\n第三，通过比较目标函数的实际下降量 $\\operatorname{ared}_k$ 与模型预测的下降量 $\\operatorname{pred}_k$ 来评估模型的保真度。它们的定义如下：\n$$\\operatorname{ared}_k = f(x_k) - f(x_k+s_k)$$\n$$\\operatorname{pred}_k = m_k(0) - m_k(s_k) = -g_k s_k - \\frac{1}{2} B_k s_k^2$$\n这两个量之比 $\\rho_k = \\operatorname{ared}_k / \\operatorname{pred}_k$ 用于衡量模型的准确性。如果 $\\operatorname{pred}_k$ 不为正，表示该步未能预测到改善，则将 $\\rho_k$ 设为 $0$。$\\rho_k$ 的值接近 $1$ 意味着模型非常准确。\n\n第四，根据 $\\rho_k$ 的值，算法决定是否接受该试探步以及如何为下一次迭代调整有限差分步长。\n- 位置更新：如果 $\\rho_k \\ge \\eta_{\\mathrm{acc}}$，其中接受阈值为 $\\eta_{\\mathrm{acc}} = 0.1$，则接受该步，且 $x_{k+1} = x_k + s_k$。否则，拒绝该步，且 $x_{k+1} = x_k$。\n- 调整有限差分步长以改善后续迭代中的模型质量。该逻辑旨在将 $\\rho_k$ 推向 $1$ 附近的理想范围：\n  - 如果 $\\rho_k  \\eta_{\\mathrm{low}} = 0.5$，则认为模型不够准确，这可能是由于较大的 $h_k$ 导致的截断误差或模型形式的问题。问题陈述提供了一个增加 $h_k$ 的规则：$h_{k+1} = \\min\\{\\gamma_{\\uparrow} h_k, h_{\\max}\\}$。常数为 $\\gamma_{\\uparrow} = 1.5$ 和 $h_{\\max} = 10^{-1}$。\n  - 如果 $\\rho_k > \\eta_{\\mathrm{high}} = 1.5$，模型也被认为是拟合不佳。例如，如果一个小的 $h_k$ 导致有限差分公式中出现相减抵消误差，就可能发生这种情况。减小步长：$h_{k+1} = \\max\\{\\gamma_{\\downarrow} h_k, h_{\\min}\\}$。常数为 $\\gamma_{\\downarrow} = 2/3$ 和 $h_{\\min} = 10^{-6}$。\n  - 如果 $\\eta_{\\mathrm{low}} \\le \\rho_k \\le \\eta_{\\mathrm{high}}$，则认为模型是令人满意的，步长保持不变：$h_{k+1} = h_k$。\n\n最后，在 $K=100$ 次迭代后，为每个测试案例计算一个稳定性度量。该度量是在最后 $J=10$ 个被接受的步上计算的 $|1 - \\rho_k|$ 的算术平均值。如果总共被接受的步数少于 $J$ 步，则对所有可用的被接受步求平均。如果没有步被接受，则该度量被定义为非数值（not-a-number）。该度量提供了一个定量衡量，说明自适应过程在模拟的后期将 $\\rho_k$ 引导至 $1$ 的成功程度。\n\n实现包含一个主循环，为由参数 $(\\alpha, \\omega, x_0, h_0)$ 定义的四个测试案例中的每一个执行这些步骤。计算出的度量被收集起来，四舍五入到 $6$ 位小数，并格式化为指定的输出字符串。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a 1D trust-region algorithm with an adaptive\n    finite-difference step size on a suite of test problems.\n    \"\"\"\n    \n    # Define constants from the problem statement.\n    C = 1.5\n    DELTA = 0.5\n    ETA_ACC = 0.1\n    H_MIN = 1e-6\n    H_MAX = 1e-1\n    ETA_LOW = 0.5\n    ETA_HIGH = 1.5\n    GAMMA_UP = 1.5\n    GAMMA_DOWN = 2/3.0\n    K = 100\n    J = 10\n\n    def objective_function(x, alpha, omega):\n        \"\"\"The test function f(x) = (x - c)^4 + alpha * sin(omega * x).\"\"\"\n        return (x - C)**4 + alpha * np.sin(omega * x)\n\n    def run_simulation(params):\n        \"\"\"\n        Runs the trust-region simulation for a single test case.\n        \n        Args:\n            params (tuple): A tuple containing (alpha, omega, x0, h0).\n\n        Returns:\n            float: The computed stability metric.\n        \"\"\"\n        alpha, omega, x0, h0 = params\n        x_k = float(x0)\n        h_k = float(h0)\n        \n        accepted_rhos = []\n        \n        for _ in range(K):\n            # 1. Build the quadratic model m_k(s)\n            f_k = objective_function(x_k, alpha, omega)\n            f_plus_h = objective_function(x_k + h_k, alpha, omega)\n            f_minus_h = objective_function(x_k - h_k, alpha, omega)\n            \n            g_k = (f_plus_h - f_minus_h) / (2.0 * h_k)\n            B_k = (f_plus_h - 2.0 * f_k + f_minus_h) / (h_k**2)\n            \n            # 2. Solve the trust-region subproblem for s_k\n            if B_k > 0:\n                s_star = -g_k / B_k\n                s_k = np.clip(s_star, -DELTA, DELTA)\n            else:\n                s_k = -np.sign(g_k) * DELTA\n                \n            # 3. Evaluate step quality with rho_k\n            if s_k == 0.0:\n                 pred_k = 0.0\n                 ared_k = 0.0\n            else:\n                pred_k = -g_k * s_k - 0.5 * B_k * s_k**2\n                ared_k = f_k - objective_function(x_k + s_k, alpha, omega)\n\n            if pred_k > 0:\n                rho_k = ared_k / pred_k\n            else:\n                rho_k = 0.0\n                \n            # 4. Update position x_k\n            if rho_k >= ETA_ACC:\n                x_k = x_k + s_k\n                accepted_rhos.append(rho_k)\n            # else: x_{k+1} = x_k (no change)\n\n            # 5. Adapt finite-difference step size h_k\n            if rho_k  ETA_LOW:\n                h_k = min(GAMMA_UP * h_k, H_MAX)\n            elif rho_k > ETA_HIGH:\n                h_k = max(GAMMA_DOWN * h_k, H_MIN)\n            # else: h_{k+1} = h_k (no change)\n                \n        # After K iterations, compute the stability metric\n        if not accepted_rhos:\n            metric = np.nan\n        else:\n            # Use last J accepted steps, or all if fewer than J\n            rhos_to_average = accepted_rhos[-J:]\n            # Calculate mean of |1 - rho|\n            metric = np.mean(np.abs(1.0 - np.array(rhos_to_average)))\n            \n        return metric\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (too small initial h): (alpha, omega, x0, h0)\n        (1e-3, 1e3, 0.0, 1e-6),\n        # Case B (too large initial h): (alpha, omega, x0, h0)\n        (1e-3, 1e3, 0.0, 1e-1),\n        # Case C (happy path): (alpha, omega, x0, h0)\n        (1e-4, 500.0, 3.0, 1e-4),\n        # Case D (no oscillation baseline): (alpha, omega, x0, h0)\n        (0.0, 1e3, -2.0, 1e-6),\n    ]\n\n    results = []\n    for case in test_cases:\n        metric = run_simulation(case)\n        if np.isnan(metric):\n            results.append('nan')\n        else:\n            # Format to 6 decimal places per problem requirement.\n            results.append(f\"{metric:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3152595"}]}