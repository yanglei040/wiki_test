{"hands_on_practices": [{"introduction": "理论知识是基础，但真正的理解来自于实践。在这个练习中，你将从零开始构建一个完整的非线性共轭梯度（NCG）求解器。通过解决一系列具有不同特性的典型优化问题，你将把算法的各个组成部分——梯度计算、搜索方向更新和线搜索——结合在一起，亲手实现一个强大的优化工具。[@problem_id:2418452]", "problem": "给定欧几里得空间上的几个可微目标函数以及初始点。您的任务是编写一个完整的程序，在每种情况下，仅使用函数值和精确的一阶导数来计算近似极小值点。您的程序所使用的梯度对于指定的目标函数必须在机器精度内是精确的；请勿使用任何有限差分近似。请勿使用一阶导数之外的任何信息。使用基于梯度欧几里得范数的终止准则。\n\n数学设定：\n\n- 设 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 为一个连续可微函数，并用 $\\nabla f(\\mathbf{x})$ 表示其梯度。从给定的初始点 $\\mathbf{x}_0 \\in \\mathbb{R}^n$ 开始，计算一个序列 $\\{\\mathbf{x}_k\\}$，该序列仅使用 $f$ 和 $\\nabla f$ 的求值来尝试最小化 $f$。\n- 当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或迭代次数达到指定的最大值时终止。对 $\\|\\cdot\\|_2$ 使用欧几里得范数。\n- 算法不得使用任何二阶信息（无 Hessian 矩阵或 Hessian-向量积），也不得使用有限差分导数近似。程序必须在机器精度内计算指定函数的精确梯度。\n\n测试套件：\n\n对于下面的每种情况，都给定了函数 $f(\\mathbf{x})$、维度 $n$ 和初始点 $\\mathbf{x}_0$。\n\n- 情况 A (非凸，窄谷，双变量)：\n  - 维度：$n=2$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = 100\\,(x_2 - x_1^2)^2 + (1 - x_1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (-1.2,\\; 1.0)$。\n\n- 情况 B (病态可分二次函数，五变量)：\n  - 维度：$n=5$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = \\tfrac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2,\\quad \\lambda_i = 10^{\\,i-1}.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; -1,\\; 1,\\; -1,\\; 1)$。\n\n- 情况 C (平滑、耦合、非凸，三变量)：\n  - 维度：$n=3$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = 0.1\\,(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3.$$\n  - 初始点：$\\mathbf{x}_0 = (0.5,\\; -0.5,\\; 0.0)$。\n\n- 情况 D (已在极小值点，四变量)：\n  - 维度：$n=4$。\n  - 目标函数：\n    $$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2.$$\n  - 初始点：$\\mathbf{x}_0 = (1,\\; 1,\\; 1,\\; 1).$\n\n要求：\n\n- 停止容差：梯度范数使用 $\\varepsilon = 10^{-6}$。\n- 最大迭代次数：每个情况使用 $N_{\\max} = 10000$。\n- 程序所使用的梯度对于指定的目标函数必须在机器精度内是精确的。\n- 程序不得读取任何输入，并且除了下面描述的最后一行外，不得写入任何输出。\n\n输出规格：\n\n- 对于每个情况，报告终止时的最终目标函数值 $f(\\mathbf{x}_\\star)$，四舍五入到小数点后恰好六位。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含情况 A、情况 B、情况 C 和情况 D 的四个四舍五入后的目标函数值。例如，一个有效的输出格式是：\n  - \"[0.000123,0.000000,1.234567,0.000000]\"\n\n不涉及物理单位。根据数学惯例，三角函数中使用的角度以弧度为单位。输出必须是实数，并且必须遵循上述确切格式。", "solution": "所提出的问题是一个标准的数值优化任务，要求最小化几个定义明确的可微函数。该方法被限制为一阶方法，这意味着它只能使用函数值 $f(\\mathbf{x})$ 和梯度值 $\\nabla f(\\mathbf{x})$。禁止使用二阶信息，如 Hessian 矩阵。鉴于这些限制，一个非常合适且高效的算法是非线性共轭梯度 (CG) 法。虽然更简单的最速下降法也满足这些限制，但其收敛速度对于具有高曲率或病态条件的问题（例如 Rosenbrock 函数（情况 A）和给定的具有悬殊特征值的二次函数（情况 B））来说是出了名的差。CG 方法通过构建作为梯度的类共轭扩展的搜索方向来加速收敛，从而有效地整合了先前步骤的信息。\n\n非线性 CG 算法的迭代过程从初始点 $\\mathbf{x}_0$ 开始，对 $k=0, 1, 2, \\dots$ 定义如下：\n1. 计算当前迭代点的梯度：$\\mathbf{g}_k = \\nabla f(\\mathbf{x}_k)$。\n2. 检查收敛性：如果梯度的欧几里得范数 $\\|\\mathbf{g}_k\\|_2$ 低于指定的容差 $\\varepsilon$，则算法终止。\n3. 计算搜索方向 $\\mathbf{p}_k$。对于第一次迭代 ($k=0$)，方向是最速下降方向，$\\mathbf{p}_0 = -\\mathbf{g}_0$。对于后续迭代 ($k  0$)，方向是当前负梯度和前一个搜索方向的线性组合：\n   $$\n   \\mathbf{p}_k = -\\mathbf{g}_k + \\beta_k \\mathbf{p}_{k-1}\n   $$\n   标量 $\\beta_k$ 决定了 CG 方法的具体变体。\n4. 执行线搜索以确定沿方向 $\\mathbf{p}_k$ 的合适步长 $\\alpha_k  0$。\n5. 更新迭代点：$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$。\n\n对于此实现，选择 Polak–Ribière–Polyak (PRP) 公式来计算 $\\beta_k$，因为与其他公式（如 Fletcher-Reeves）相比，它通常具有更优的经验性能。PRP 公式是：\n$$\n\\beta_k^{\\text{PRP}} = \\frac{\\mathbf{g}_k^T (\\mathbf{g}_k - \\mathbf{g}_{k-1})}{\\mathbf{g}_{k-1}^T \\mathbf{g}_{k-1}}\n$$\n为了提高稳健性并保证全局收敛性质，此公式被增强为 PRP+ 方法，其中 $\\beta_k = \\max(0, \\beta_k^{\\text{PRP}})$。如果 $\\beta_k^{\\text{PRP}}$ 变为负值（这在远离局部极小值点时可能发生），此修改可以防止算法采取不良步骤。此外，作为一种保险措施，如果搜索方向 $\\mathbf{p}_k$ 不再是下降方向（即，如果 $\\mathbf{p}_k^T \\mathbf{g}_k \\ge 0$），它将被强制重置为最速下降方向 $-\\mathbf{g}_k$。\n\n步长 $\\alpha_k$ 是通过满足强 Wolfe 条件的线搜索找到的：\n1. 充分下降 (Armijo) 条件：$f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha_k \\mathbf{g}_k^T \\mathbf{p}_k$\n2. 曲率条件：$|\\nabla f(\\mathbf{x}_k + \\alpha_k \\mathbf{p}_k)^T \\mathbf{p}_k| \\le c_2 |\\mathbf{g}_k^T \\mathbf{p}_k|$\n常数选择标准值 $c_1 = 10^{-4}$ 和 $c_2 = 0.9$。这些条件确保每一步都能在目标函数值上实现有意义的减少。使用 `scipy.optimize.line_search` 函数来实现此步骤。\n\n解析梯度必须是精确的。四个测试案例的梯度推导如下：\n\n情况 A：Rosenbrock 函数，$f(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$ 对于 $\\mathbf{x} \\in \\mathbb{R}^2$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} -400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\ 200(x_2 - x_1^2) \\end{pmatrix}\n$$\n\n情况 B：病态可分二次函数，$f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^{5} \\lambda_i x_i^2$ 且 $\\lambda_i = 10^{i-1}$ 对于 $\\mathbf{x} \\in \\mathbb{R}^5$。\n每个 $x_j$ 的梯度分量是 $\\frac{\\partial f}{\\partial x_j} = \\lambda_j x_j$。\n$$\n\\nabla f(\\mathbf{x})_j = 10^{j-1} x_j, \\quad \\text{for } j=1, \\dots, 5\n$$\n\n情况 C：平滑、耦合、非凸函数，$f(\\mathbf{x}) = 0.1(x_1^2 + x_2^2 + x_3^2) + \\sin(x_1)\\cos(x_2) + e^{x_3} - x_3$ 对于 $\\mathbf{x} \\in \\mathbb{R}^3$。\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 0.2x_1 + \\cos(x_1)\\cos(x_2) \\\\ 0.2x_2 - \\sin(x_1)\\sin(x_2) \\\\ 0.2x_3 + e^{x_3} - 1 \\end{pmatrix}\n$$\n\n情况 D：简单二次函数，$f(\\mathbf{x}) = \\sum_{i=1}^{4} (x_i - 1)^2$ 对于 $\\mathbf{x} \\in \\mathbb{R}^4$。\n每个 $x_j$ 的梯度分量是 $\\frac{\\partial f}{\\partial x_j} = 2(x_j-1)$。\n$$\n\\nabla f(\\mathbf{x})_j = 2(x_j - 1), \\quad \\text{for } j=1, \\dots, 4\n$$\n对于这种情况，初始点 $\\mathbf{x}_0 = (1, 1, 1, 1)$ 是该函数的唯一全局极小值点。因此，$\\nabla f(\\mathbf{x}_0) = \\mathbf{0}$，算法在迭代 $k=0$ 时立即终止，目标函数值为 $f(\\mathbf{x}_0)=0$。\n\n该实现将这些元素组合成一个单一的程序。一个通用的求解器函数封装了 CG 逻辑，并为每个测试案例调用，传入各自的目标函数、梯度和初始点。当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le 10^{-6}$ 或达到 $10000$ 次迭代后，程序终止。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\ndef conjugate_gradient_solver(f, grad_f, x0, tol=1e-6, max_iter=10000):\n    \"\"\"\n    Minimizes a function using the Nonlinear Conjugate Gradient method (Polak-Ribière-Polyak+).\n    \"\"\"\n    x_k = np.array(x0, dtype=float)\n    f_k = f(x_k)\n    g_k = grad_f(x_k)\n    grad_norm = np.linalg.norm(g_k)\n\n    if grad_norm = tol:\n        return f(x_k)\n\n    p_k = -g_k\n    \n    k = 0\n    while k  max_iter and grad_norm  tol:\n        # Perform line search to find alpha_k satisfying strong Wolfe conditions.\n        # c1=1e-4 and c2=0.9 are standard for CG.\n        try:\n            line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n            alpha_k = line_search_result[0]\n        except Exception:\n            # line_search can sometimes raise errors for extreme values\n            alpha_k = None\n\n        # If line search fails, restart with steepest descent.\n        if alpha_k is None:\n            p_k = -g_k\n            try:\n                line_search_result = optimize.line_search(f, grad_f, x_k, p_k, gfk=g_k, old_fval=f_k, c1=1e-4, c2=0.9)\n                alpha_k = line_search_result[0]\n            except Exception:\n                alpha_k = None\n            \n            if alpha_k is None:\n                # If it still fails, terminate. Could be due to precision limits.\n                break\n\n        x_k_plus_1 = x_k + alpha_k * p_k\n        g_k_plus_1 = grad_f(x_k_plus_1)\n\n        # Polak-Ribière-Polyak+ update for beta\n        g_k_dot_g_k = np.dot(g_k, g_k)\n        if g_k_dot_g_k == 0:\n            beta_k_plus_1 = 0.0\n        else:\n            beta_numerator = np.dot(g_k_plus_1, g_k_plus_1 - g_k)\n            beta_k_plus_1 = max(0, beta_numerator / g_k_dot_g_k)\n        \n        # New search direction\n        p_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * p_k\n\n        # Check for descent direction. If not, reset to steepest descent.\n        if np.dot(p_k_plus_1, g_k_plus_1) = 0:\n            p_k_plus_1 = -g_k_plus_1\n\n        # Update variables for the next iteration\n        x_k = x_k_plus_1\n        g_k = g_k_plus_1\n        p_k = p_k_plus_1\n        f_k = f(x_k) # Can be taken from line_search output, but re-evaluating is simple.\n        \n        grad_norm = np.linalg.norm(g_k)\n        k += 1\n\n    return f(x_k)\n\ndef solve():\n    # Final print statement in the exact required format.\n    \n    # Case A: Rosenbrock function\n    def f_A(x):\n        return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\n    def grad_f_A(x):\n        df_dx1 = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 200.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n\n    # Case B: Ill-conditioned separable quadratic\n    def f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return 0.5 * np.sum(lambdas * x**2)\n\n    def grad_f_B(x):\n        lambdas = 10.**np.arange(len(x))\n        return lambdas * x\n\n    # Case C: Smooth, coupled, nonconvex\n    def f_C(x):\n        term1 = 0.1 * np.sum(x**2)\n        term2 = np.sin(x[0]) * np.cos(x[1])\n        term3 = np.exp(x[2]) - x[2]\n        return term1 + term2 + term3\n\n    def grad_f_C(x):\n        df_dx1 = 0.2 * x[0] + np.cos(x[0]) * np.cos(x[1])\n        df_dx2 = 0.2 * x[1] - np.sin(x[0]) * np.sin(x[1])\n        df_dx3 = 0.2 * x[2] + np.exp(x[2]) - 1.0\n        return np.array([df_dx1, df_dx2, df_dx3])\n    \n    # Case D: Simple quadratic\n    def f_D(x):\n        return np.sum((x - 1.0)**2)\n\n    def grad_f_D(x):\n        return 2.0 * (x - 1.0)\n    \n    test_cases = [\n        {'f': f_A, 'grad_f': grad_f_A, 'x0': [-1.2, 1.0]},\n        {'f': f_B, 'grad_f': grad_f_B, 'x0': [1.0, -1.0, 1.0, -1.0, 1.0]},\n        {'f': f_C, 'grad_f': grad_f_C, 'x0': [0.5, -0.5, 0.0]},\n        {'f': f_D, 'grad_f': grad_f_D, 'x0': [1.0, 1.0, 1.0, 1.0]}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_f_val = conjugate_gradient_solver(\n            f=case['f'],\n            grad_f=case['grad_f'],\n            x0=case['x0'],\n            tol=1e-6,\n            max_iter=10000\n        )\n        results.append(final_f_val)\n\n    # Format output as specified\n    formatted_results = [\"{:.6f}\".format(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2418452"}, {"introduction": "一个稳健的算法不仅在于其核心逻辑，更在于其内置的“安全网”。上一个练习构建了求解器，而这个练习将通过一个对比实验，揭示线搜索中 Armijo 条件的至关重要性。你将比较一个遵循该条件的正确实现和一个采用固定步长的“错误”实现，从而直观地理解为什么充分下降条件是保证算法收敛性的关键。[@problem_id:2418455]", "problem": "要求您实现并比较非线性共轭梯度法的两种变体，用于无约束最小化一个连续可微函数。第一种变体是标准的非线性共轭梯度算法，它通过回溯线搜索强制执行第一个 Wolfe 条件（Armijo 充分下降条件）。第二种变体是故意设置的错误版本：它不执行 Armijo 条件，而是在每次迭代中使用固定的单位步长。您的任务是通过精心选择的测试函数来证明，即使在标准方法能够收敛的情况下，省略 Armijo 条件也可能导致不收敛或发散。\n\n从以下基础开始：\n- 对于一个连续可微的目标函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$，下降方法生成迭代点 $x_{k+1} = x_k + \\alpha_k d_k$，其中搜索方向 $d_k$ 满足 $g_k^\\top d_k  0$，这里 $g_k = \\nabla f(x_k)$ 且 $\\alpha_k  0$ 是步长。\n- 第一个 Wolfe (Armijo) 充分下降条件要求，对于常数 $c_1 \\in (0,1)$，步长满足\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k.\n$$\n- 在非线性共轭梯度法中，搜索方向的构造方式如下\n$$\nd_0 = -g_0,\\quad d_k = -g_k + \\beta_k d_{k-1}\\ \\text{for}\\ k \\ge 1,\n$$\n其中 $\\beta_k$ 的经典选择如 Polak–Ribiere–Plus，并带有一个安全措施，即如果 $g_k^\\top d_k \\ge 0$，则重置 $d_k=-g_k$ 以保持下降性。\n- 对于形如 $f(x) = \\tfrac{1}{2} x^\\top Q x$ 的光滑凸二次目标函数，其中 $Q$ 是对称正定矩阵，采用固定步长 $\\alpha$ 的梯度下降法会产生线性迭代 $x_{k+1} = (I - \\alpha Q) x_k$。当且仅当谱半径满足 $\\rho(I - \\alpha Q)  1$（等价于 $0  \\alpha  2/\\lambda_{\\max}(Q)$）时，迭代才会收敛到最小值点 $x^\\star=0$。\n\n您的程序必须实现：\n- 一个标准的非线性共轭梯度求解器，它使用回溯线搜索来强制执行 Armijo 条件，并使用用户选择的常数 $c_1 \\in (0,1)$ 和回溯比率 $\\tau \\in (0,1)$。\n- 一个错误的非线性共轭梯度求解器，它对所有 $k$ 都使用 $\\alpha_k \\equiv 1$（不进行充分下降检查）。\n\n您必须遵守的设计细节：\n- 共轭参数 $\\beta_k$ 使用 Polak–Ribiere–Plus 选择，并且如果计算出的方向不是下降方向，则使用重置安全措施。\n- 当 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（其中 $\\varepsilon  0$ 为给定容差）时，或当达到固定的迭代次数上限时终止。\n- 如果在达到迭代次数上限时仍未满足梯度容差，则声明为不收敛。如果目标函数值变为非数值、超过一个很大的阈值，或迭代点范数超过一个很大的阈值，则声明为发散。\n\n构建一个测试套件以展示不同的行为：\n- 测试 A（发散见证）：一个凸二次函数 $f(x) = \\tfrac{1}{2} x^\\top Q x$，其中 $Q = \\mathrm{diag}(10.0, 0.1)$，初始点为 $x_0 = [1.0, 1.0]$。根据谱半径准则，固定步长 $\\alpha = 1$ 违反了 $0  \\alpha  2/\\lambda_{\\max}(Q)$，因为 $\\lambda_{\\max}(Q) = 10.0$，因此错误方法预期会发散，而强制执行 Armijo 条件的方法应该收敛。\n- 测试 B（理想路径）：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$，初始点为 $x_0 = [2.0, -3.0]$。这里 $\\lambda_{\\max}(Q) = 0.5$，所以固定步长 $\\alpha = 1$ 满足 $0  \\alpha  2/\\lambda_{\\max}(Q)$，两种方法都应该收敛。\n- 测试 C（边界条件）：同样的凸二次函数，$Q = \\mathrm{diag}(1.0, 1.0)$，初始点为 $x_0 = [0.0, 0.0]$，这已经是一个最小值点。两种方法都应能立即检测到收敛。\n- 测试 D（非凸压力测试）：一个缩放的 Rosenbrock 函数 $f(x_1,x_2) = 10\\,(x_2 - x_1^2)^2 + (1 - x_1)^2$，初始点为 $x_0 = [-1.2, 1.0]$。错误方法的单位步长可能在这个弯曲的谷底导致数值爆炸，而带有 Armijo 回溯的标准方法应该能收敛到 $[1,1]$ 附近的最小值点。\n\n在您的程序中使用的数值参数：\n- 梯度范数容差 $\\varepsilon = 10^{-6}$，最大迭代次数 $N_{\\max} = 5000$，Armijo 常数 $c_1 = 10^{-4}$，回溯比率 $\\tau = 0.5$。\n- 发散阈值：如果 $\\|x_k\\|_2  10^{12}$ 或 $f(x_k)  10^{50}$ 或 $f(x_k)$ 为非数值，则声明为发散。\n\n您的程序必须：\n- 实现两种求解器，在所有四个测试上运行它们，并根据观察到的行为为每个测试确定一个整数代码：\n    - 如果标准方法收敛而错误方法不收敛（不收敛或发散），则输出 $1$。\n    - 如果两者都收敛，则输出 $0$。\n    - 如果两者都不收敛，则输出 $-1$。\n    - 如果错误方法收敛而标准方法不收敛，则输出 $2$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。例如，输出格式必须类似于单个 Python 风格的列表字面量，如 [r1,r2,r3,r4]，其中每个条目是指定的整数代码之一。\n\n不涉及角度单位。此问题中没有物理单位。所有量纲均为无量纲。输出必须严格遵循指定的单行格式。", "solution": "所提出的问题是实现并对比分析用于无约束优化的非线性共轭梯度（NCG）法的两种变体。其中一种变体被正确实现，遵循了线搜索方法的基本原则，强制执行了 Armijo 充分下降条件。第二种变体是故意设计的有缺陷版本，它采用固定的单位步长，从而省略了线搜索这一关键的保障措施。其目标是通过计算来证明，在正确实现的算法能够成功的那些问题上，省略 Armijo 条件可能导致失败，具体表现为不收敛或发散。该问题定义明确，科学上合理，并为算法实现与验证提供了清晰的基础。\n\n我们考虑一个一般的无约束最小化问题，旨在寻找一个连续可微目标函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的局部最小值点。NCG 方法是一种迭代算法，它使用以下更新规则生成一系列点 $\\{x_k\\}_{k \\ge 0}$：\n$$\nx_{k+1} = x_k + \\alpha_k d_k\n$$\n这里，$x_k \\in \\mathbb{R}^n$ 是当前迭代点，$d_k \\in \\mathbb{R}^n$ 是搜索方向，$\\alpha_k  0$ 是步长。目标函数在 $x_k$ 处的梯度记为 $g_k = \\nabla f(x_k)$。\n\n搜索方向 $d_k$ 被构造成一个下降方向，即 $g_k^\\top d_k  0$。NCG 的方向是递归定义的。初始方向是最速下降方向，$d_0 = -g_0$。对于后续的迭代 $k \\ge 1$，方向是当前负梯度和前一个方向的线性组合：\n$$\nd_k = -g_k + \\beta_k d_{k-1}\n$$\n标量 $\\beta_k$ 是共轭参数。问题指定了 Polak–Ribière–Plus 变体，该变体以其强大的数值性能而闻名。其定义如下：\n$$\n\\beta_k = \\max \\left\\{ 0, \\frac{g_k^\\top(g_k - g_{k-1})}{\\|g_{k-1}\\|_2^2} \\right\\}\n$$\n这个选择包含了一个非负性约束，这有助于在特定条件下确保全局收敛。一个至关重要的保障措施是重置条件：如果计算出的方向 $d_k$ 未能成为一个下降方向（即，如果 $g_k^\\top d_k \\ge 0$），则通过将搜索方向设为最速下降方向 $d_k = -g_k$ 来重置该方法。\n\n本次研究的核心在于步长 $\\alpha_k$ 的确定。\n\n**标准的 NCG 方法**采用回溯线搜索来寻找一个满足 Armijo 充分下降条件的步长 $\\alpha_k$。对于给定的常数 $c_1 \\in (0, 1)$，该条件为：\n$$\nf(x_k + \\alpha_k d_k) \\le f(x_k) + c_1 \\alpha_k g_k^\\top d_k\n$$\n这个不等式确保了目标函数的减少量至少是在 $x_k$ 处 $f$ 的线性近似所预测的减少量的一个分数。回溯过程从一个初始试探步长（通常为 $\\alpha = 1$）开始，并以一个因子 $\\tau \\in (0, 1)$（例如 $\\alpha \\leftarrow \\tau \\alpha$）迭代地减小它，直到满足该条件。指定的参数是 $c_1 = 10^{-4}$ 和 $\\tau = 0.5$。\n\n**错误的 NCG 方法**绕过了这个关键检查，并简单地对所有迭代 $k \\ge 0$ 设置 $\\alpha_k = 1$。虽然对于某些表现良好的函数或者初始迭代点接近解时这可能是可接受的，但通常这是一种不可靠的策略，可能导致失败。\n\n算法的终止由梯度的范数决定。如果 $\\|\\nabla f(x_k)\\|_2 \\le \\varepsilon$（容差 $\\varepsilon = 10^{-6}$），则认为迭代收敛。如果迭代次数超过上限 $N_{\\max} = 5000$，则中止过程，并归类为不收敛。如果迭代点范数 $\\|x_k\\|_2$ 超过 $10^{12}$、函数值 $f(x_k)$ 超过 $10^{50}$，或者 $f(x_k)$ 变为非数值 (NaN)，则声明为发散。\n\n分析是在一套旨在揭示两种方法不同行为的四个测试用例上进行的。\n\n**测试 A**：一个凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x$，其 Hessian 矩阵 $Q = \\mathrm{diag}(10.0, 0.1)$ 条件很差。对于二次函数，采用固定步长 $\\alpha$ 的 NCG 方法等价于线性迭代系统 $x_{k+1} = (I - \\alpha Q) x_k$。该系统收敛当且仅当迭代矩阵的谱半径 $\\rho(I - \\alpha Q)$ 小于 $1$。当 $\\alpha=1$ 时，$I-Q$ 的特征值为 $1-10.0 = -9.0$ 和 $1-0.1 = 0.9$。谱半径为 $\\rho(I - Q) = \\max\\{|-9.0|, |0.9|\\} = 9.0$，大于 $1$。错误方法因此保证会发散。而标准方法，由于其来自 Armijo 条件的自适应步长，预期会收敛。\n\n**测试 B**：一个良态的凸二次函数，其中 $Q = \\mathrm{diag}(0.5, 0.25)$。这里，对于采用 $\\alpha=1$ 的错误方法，$I-Q$ 的特征值为 $1-0.5 = 0.5$ 和 $1-0.25=0.75$。谱半径为 $\\rho(I - Q) = 0.75  1$，满足收敛条件。因此，错误方法和标准方法都预期会收敛。\n\n**测试 C**：一个凸二次函数，初始点 $x_0 = [0.0, 0.0]$ 是全局最小值点。初始梯度为 $\\nabla f(x_0) = 0$。两个算法都必须在第一次迭代前检查终止条件并立即声明收敛。\n\n**测试 D**：非凸 Rosenbrock 函数，$f(x_1,x_2) = 10(x_2 - x_1^2)^2 + (1-x_1)^2$，从起始点 $x_0 = [-1.2, 1.0]$ 开始。这是一个以狭窄、弯曲的谷底为特征的经典基准测试函数。错误方法的固定单位步长很可能导致迭代点“跳过”谷底，从而引起函数值增加和不稳定的行为，可能导致发散或不收敛。相比之下，标准方法的回溯线搜索将系统地减小步长以确保充分下降，从而使迭代点能够沿着谷底走向位于 $[1,1]$ 附近的最小值点。\n\n每个测试的结果是一个整数代码：如果标准方法收敛而错误方法不收敛，则为 $1$；如果两者都收敛，则为 $0$；如果两者都不收敛，则为 $-1$；如果错误方法收敛而标准方法不收敛，则为 $2$。这种系统性的比较为线搜索机制在确保基于下降的优化算法的鲁棒性方面所起的不可或缺的作用提供了明确的证据。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares a proper and a faulty nonlinear conjugate gradient method.\n    \"\"\"\n\n    # --- Numerical Parameters ---\n    EPSILON = 1e-6\n    MAX_ITER = 5000\n    C1 = 1e-4\n    TAU = 0.5\n    DIV_NORM_THRESHOLD = 1e12\n    DIV_F_THRESHOLD = 1e50\n\n    def nonlinear_cg(f, grad_f, x0, use_armijo):\n        \"\"\"\n        Nonlinear Conjugate Gradient (NCG) solver.\n\n        Args:\n            f: Objective function.\n            grad_f: Gradient of the objective function.\n            x0: Initial point.\n            use_armijo: Boolean flag to use Armijo line search.\n\n        Returns:\n            A string indicating the outcome: \"converged\", \"nonconverged\", \"diverged\".\n        \"\"\"\n        x_k = np.copy(x0).astype(np.float64)\n        g_k = grad_f(x_k)\n        \n        # Initial check for convergence at x0\n        norm_g_k = np.linalg.norm(g_k)\n        if norm_g_k = EPSILON:\n            return \"converged\"\n\n        d_k = -g_k\n        k = 0\n\n        while k  MAX_ITER:\n            # Line Search\n            if use_armijo:\n                alpha_k = 1.0\n                descent_condition_val = C1 * np.dot(g_k, d_k)\n                # The dot product g_k.T @ d_k should be  0 due to safeguard\n                try:\n                    f_k = f(x_k)\n                    while f(x_k + alpha_k * d_k)  f_k + alpha_k * descent_condition_val:\n                        alpha_k *= TAU\n                        if alpha_k  1e-15: # Prevent infinite loop if step size becomes too small\n                           return \"nonconverged\"\n                except (OverflowError, ValueError):\n                    return \"diverged\" # f() evaluation might fail\n            else:\n                alpha_k = 1.0\n\n            # Update position\n            x_k_plus_1 = x_k + alpha_k * d_k\n\n            # Check for divergence\n            try:\n                f_next = f(x_k_plus_1)\n                if np.linalg.norm(x_k_plus_1)  DIV_NORM_THRESHOLD or f_next  DIV_F_THRESHOLD or np.isnan(f_next):\n                    return \"diverged\"\n            except (OverflowError, ValueError):\n                return \"diverged\"\n\n            g_k_plus_1 = grad_f(x_k_plus_1)\n            norm_g_k_plus_1 = np.linalg.norm(g_k_plus_1)\n\n            # Check for convergence\n            if norm_g_k_plus_1 = EPSILON:\n                return \"converged\"\n\n            # Polak-Ribiere-Plus (PR+) for beta\n            norm_g_k_sq = norm_g_k**2\n            if norm_g_k_sq  1e-14: # Safeguard against division by zero\n                beta_k_plus_1 = max(0, np.dot(g_k_plus_1, g_k_plus_1 - g_k) / norm_g_k_sq)\n            else:\n                beta_k_plus_1 = 0.0\n\n            # Update search direction\n            d_k_plus_1 = -g_k_plus_1 + beta_k_plus_1 * d_k\n\n            # Restart if not a descent direction\n            if np.dot(g_k_plus_1, d_k_plus_1) = 0:\n                d_k_plus_1 = -g_k_plus_1\n\n            # Prepare for next iteration\n            x_k = x_k_plus_1\n            g_k = g_k_plus_1\n            norm_g_k = norm_g_k_plus_1\n            d_k = d_k_plus_1\n            k += 1\n\n        return \"nonconverged\"\n    \n    # --- Test Case Definitions ---\n\n    # Test A: Divergence Witness\n    Q_A = np.diag([10.0, 0.1])\n    def f_A(x): return 0.5 * x.T @ Q_A @ x\n    def grad_f_A(x): return Q_A @ x\n    x0_A = np.array([1.0, 1.0])\n\n    # Test B: Happy Path\n    Q_B = np.diag([0.5, 0.25])\n    def f_B(x): return 0.5 * x.T @ Q_B @ x\n    def grad_f_B(x): return Q_B @ x\n    x0_B = np.array([2.0, -3.0])\n\n    # Test C: Boundary Condition\n    Q_C = np.diag([1.0, 1.0])\n    def f_C(x): return 0.5 * x.T @ Q_C @ x\n    def grad_f_C(x): return Q_C @ x\n    x0_C = np.array([0.0, 0.0])\n\n    # Test D: Nonconvex Stress Test (Scaled Rosenbrock)\n    def f_D(x): return 10.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n    def grad_f_D(x):\n        df_dx1 = -40.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n        df_dx2 = 20.0 * (x[1] - x[0]**2)\n        return np.array([df_dx1, df_dx2])\n    x0_D = np.array([-1.2, 1.0])\n\n    test_cases = [\n        (f_A, grad_f_A, x0_A),\n        (f_B, grad_f_B, x0_B),\n        (f_C, grad_f_C, x0_C),\n        (f_D, grad_f_D, x0_D),\n    ]\n\n    results = []\n    for f, grad_f, x0 in test_cases:\n        proper_status = nonlinear_cg(f, grad_f, x0, use_armijo=True)\n        faulty_status = nonlinear_cg(f, grad_f, x0, use_armijo=False)\n\n        proper_converged = (proper_status == \"converged\")\n        faulty_converged = (faulty_status == \"converged\")\n\n        if proper_converged and not faulty_converged:\n            results.append(1)\n        elif proper_converged and faulty_converged:\n            results.append(0)\n        elif not proper_converged and not faulty_converged:\n            results.append(-1)\n        elif not proper_converged and faulty_converged:\n            results.append(2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2418455"}, {"introduction": "与步长选择同样重要的是搜索方向的计算。这个练习将焦点放在了共轭梯度法的核心参数 $\\beta_k$ 上，特别是 Polak–Ribière–Polyak (PRP) 公式及其改进版 PRP+。通过在一个精心设计的非凸问题上进行测试，你将发现标准 PRP 方法为何可能生成非下降方向，并理解 PRP+ 中小小的 $\\max\\{0, \\cdot\\}$ 操作是如何成为一个至关重要的“重启”机制，从而大大增强算法的稳定性和可靠性。[@problem_id:2418475]", "problem": "您将编写一个完整、可运行的程序，在一个非凸、二次连续可微的目标函数上，比较使用 Polak–Ribière–Polyak (PRP) 更新的非线性共轭梯度法与其修改变体 PRP-plus (PRP+)。在该目标函数上，标准 PRP 方法可能会无法生成下降方向。您必须仅使用无约束光滑最小化的基本定义以及 PRP 和 PRP-plus 更新的定义来构建该算法。\n\n无约束最小化问题是最小化函数\n$$\nf(\\mathbf{x}) = \\tfrac{1}{4} x_1^4 - \\tfrac{1}{2} x_1^2 + \\tfrac{1}{2} x_2^2,\n$$\n其梯度为\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} x_1^3 - x_1 \\\\ x_2 \\end{bmatrix}.\n$$\n该函数是非凸的，因为其 Hessian 矩阵的特征值为 $3 x_1^2 - 1$ 和 $1$，因此对于 $|x_1|  1$ 的情况，在 $x_1$ 方向上存在负曲率。\n\n实现两个非线性共轭梯度求解器，除了更新系数外，它们共享所有其他组件：\n- 搜索方向初始化：$\\mathbf{d}_0 = -\\nabla f(\\mathbf{x}_0)$。\n- 在迭代 $k \\ge 1$ 时，计算\n$$\n\\beta_k^{PRP} = \\frac{\\nabla f(\\mathbf{x}_k)^\\top\\big(\\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1})\\big)}{\\|\\nabla f(\\mathbf{x}_{k-1})\\|_2^2},\n$$\n并令方向更新为 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k) + \\beta_k \\mathbf{d}_{k-1}$，其中 $\\beta_k$ 的选择如下：\n- PRP: $\\beta_k = \\beta_k^{PRP}$，\n- PRP-plus (PRP+): $\\beta_k = \\max\\{0, \\beta_k^{PRP}\\}$。\n\n使用回溯 Armijo 线搜索来选择沿方向 $\\mathbf{d}_k$ 的步长 $\\alpha_k$。从一个初始试探步长 $\\alpha_0$ 开始，通过一个固定因子重复缩减，直到 Armijo 充分下降条件成立。具体而言，对于给定的参数 $c_1 \\in (0, 1)$ 和 $\\rho \\in (0, 1)$，找到最小的整数 $m \\ge 0$ 使得当 $\\alpha = \\alpha_0 \\rho^m$ 时，下式成立：\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k.\n$$\n\n停止准则：当 $\\|\\nabla f(\\mathbf{x}_k)\\|_2 \\le \\varepsilon$ 或达到最大迭代次数时停止。\n\n下降测试：在每次迭代 $k$ 采取步长之前，记录 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k  0$ 是否成立（这表示一个下降方向）。对于每次运行，如果在此次运行期间的任何迭代 $k$ 中出现非下降方向（即 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k \\ge 0$），则生成一个为真的布尔标志。\n\n所有运行中使用的参数：\n- 初始试探步长 $ \\alpha_0 = 5.0$，\n- 充分下降常数 $ c_1 = 10^{-4}$，\n- 回溯因子 $ \\rho = 0.5$，\n- 容差 $ \\varepsilon = 10^{-8}$，\n- 最大迭代次数 $ N_{\\max} = 50$，\n- 每次线搜索的最大回溯缩减次数 $ M_{\\max} = 40$。\n\n您必须从以下初始点（此为测试套件）开始，运行 PRP 和 PRP+ 两种方法：\n1. $\\mathbf{x}_0^{(1)} = \\begin{bmatrix} 0.2 \\\\ 0.0 \\end{bmatrix}$，沿 $x_1$ 方向鞍点附近的一个点，该点的负曲率可能导致 PRP 方法在后续迭代中无法生成下降方向。\n2. $\\mathbf{x}_0^{(2)} = \\begin{bmatrix} 0.0 \\\\ 2.0 \\end{bmatrix}$，在 $x_2$ 方向梯度较大但 $x_1$ 分量为零的点，用于探测可分性以及沿 $x_2$ 方向的快速下降。\n3. $\\mathbf{x}_0^{(3)} = \\begin{bmatrix} -1.5 \\\\ 0.5 \\end{bmatrix}$，左侧极小值点左侧的一个点，其 $x_2$ 分量非零。\n4. $\\mathbf{x}_0^{(4)} = \\begin{bmatrix} 1.2 \\\\ -1.0 \\end{bmatrix}$，右侧极小值点右侧的一个点，其 $x_2$ 分量非零。\n\n对于每个初始点，运行 PRP 和 PRP+ 两种方法，并返回包含以下四个值的列表：\n- PRP 终止后的最终目标函数值，四舍五入到六位小数，\n- PRP+ 终止后的最终目标函数值，四舍五入到六位小数，\n- 一个布尔值，指示 PRP 是否至少生成过一次非下降方向，\n- 一个布尔值，指示 PRP+ 是否至少生成过一次非下降方向。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个由四个列表组成的列表，每个内部列表对应一个初始点，顺序与上文一致。每个内部列表必须是 $[f_{\\text{PRP}}, f_{\\text{PRP+}}, b_{\\text{PRP}}, b_{\\text{PRP+}}]$ 的形式，其中 $f_{\\text{PRP}}$ 和 $f_{\\text{PRP+}}$ 是四舍五入到六位小数的浮点数，$b_{\\text{PRP}}$ 和 $b_{\\text{PRP+}}$ 是布尔值。例如，一个语法正确的输出形状是\n$$\n[[0.0,0.0,False,True],[\\dots],[\\dots],[\\dots]].\n$$\n作为一个重要的边界情况，请注意在 $\\mathbf{x}_0^{(1)} = [0.2, 0.0]^\\top$ 处，使用 $\\alpha_0 = 5.0$ 和所述的 Armijo 规则，第一步会到达 $\\mathbf{x}_1 \\approx [1.16, 0.0]^\\top$，对此 PRP 更新会产生一个正的 $\\beta_1^{PRP}$，因此下一个搜索方向 $\\mathbf{d}_1$ 满足 $\\nabla f(\\mathbf{x}_1)^\\top \\mathbf{d}_1  0$，即一个非下降方向。在这种情况下，PRP+ 更新与 PRP 一致，因为 $\\beta_1^{PRP}  0$，所以在此次迭代中也表现出非下降方向。您的程序必须通过下降测试来检测此类事件。", "solution": "用户在计算工程领域提供了一个有效且定义明确的问题，具体涉及无约束非线性优化。任务是实现并比较非线性共轭梯度 (NCG) 方法的两种变体——Polak–Ribière–Polyak (PRP) 及其修改版 PRP-plus (PRP+)——在一个指定的非凸目标函数上的表现。所有参数、初始条件和评估标准都已明确定义。该问题在科学上是合理的，并且需要实现已有的数值算法。因此，将开发一个完整的解决方案。\n\n问题的核心是迭代最小化目标函数 $f(\\mathbf{x}): \\mathbb{R}^2 \\to \\mathbb{R}$，其定义如下：\n$$\nf(\\mathbf{x}) = \\frac{1}{4} x_1^4 - \\frac{1}{2} x_1^2 + \\frac{1}{2} x_2^2\n$$\n该函数的梯度 $\\nabla f(\\mathbf{x})$ 对于 NCG 方法至关重要，其表达式为：\n$$\n\\nabla f(\\mathbf{x}) = \\begin{bmatrix} x_1^3 - x_1 \\\\ x_2 \\end{bmatrix}\n$$\nNCG 方法是一种迭代算法，它生成一个点序列 $\\mathbf{x}_k$，旨在收敛到 $f(\\mathbf{x})$ 的一个局部最小值。从 $\\mathbf{x}_k$ 到 $\\mathbf{x}_{k+1}$ 的更新是通过以下规则执行的：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{d}_k\n$$\n其中 $\\mathbf{d}_k$ 是搜索方向，$\\alpha_k  0$ 是步长。\n\n对于每次迭代 $k = 0, 1, 2, \\dots$，算法按以下步骤进行：\n\n1.  **搜索方向计算**：初始搜索方向是最速下降方向：$\\mathbf{d}_0 = -\\nabla f(\\mathbf{x}_0)$。对于后续迭代（$k \\ge 1$），搜索方向是当前负梯度和前一个搜索方向的线性组合：\n    $$\n    \\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k) + \\beta_k \\mathbf{d}_{k-1}\n    $$\n    系数 $\\beta_k$ 区分了不同的 NCG 方法。本问题考虑 PRP 和 PRP+ 变体。标准的 PRP 更新公式为：\n    $$\n    \\beta_k^{\\text{PRP}} = \\frac{\\nabla f(\\mathbf{x}_k)^\\top \\left( \\nabla f(\\mathbf{x}_k) - \\nabla f(\\mathbf{x}_{k-1}) \\right)}{\\|\\nabla f(\\mathbf{x}_{k-1})\\|_2^2}\n    $$\n    这两种方法由它们对 $\\beta_k$ 的选择来定义：\n    -   **PRP 方法**：$\\beta_k = \\beta_k^{\\text{PRP}}$。该方法可以表现出良好的性能，但在非凸问题上可能无法生成下降方向，可能导致算法失败。\n    -   **PRP+ 方法**：$\\beta_k = \\max\\{0, \\beta_k^{\\text{PRP}}\\}$。这一修改确保了 $\\beta_k$ 是非负的。如果 $\\beta_k^{PRP}  0$，$\\beta_k$ 将被重置为 $0$，此时搜索方向变为最速下降方向 $\\mathbf{d}_k = -\\nabla f(\\mathbf{x}_k)$，这保证了下降（在 $\\nabla f(\\mathbf{x}_k) \\neq \\mathbf{0}$ 的条件下）。这使得 PRP+ 方法更具鲁棒性。\n\n2.  **下降方向测试**：在确定步长之前，验证 $\\mathbf{d}_k$ 是否为下降方向是至关重要的。如果 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k  0$，则该条件满足。我们将为每次运行记录是否曾生成过非下降方向（即 $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k \\ge 0$）。\n\n3.  **步长计算**：步长 $\\alpha_k$ 由满足 Armijo 充分下降条件的回溯线搜索确定。从一个初始试探步长 $\\alpha = \\alpha_0$ 开始，我们通过一个因子 $\\rho$ 迭代地减小它，直到对于最小的整数 $m \\ge 0$ 满足以下条件：\n    $$\n    f(\\mathbf{x}_k + \\alpha \\mathbf{d}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{d}_k\n    $$\n    其中 $\\alpha = \\alpha_0 \\rho^m$。提供的参数为 $c_1 = 10^{-4}$，$\\rho = 0.5$，以及初始试探步长 $\\alpha_0 = 5.0$。线搜索将在最多 $M_{\\max} = 40$ 次回溯后终止。\n\n4.  **终止**：迭代过程持续进行，直到梯度的范数低于指定的容差 $\\varepsilon = 10^{-8}$，或达到最大迭代次数 $N_{\\max} = 50$。\n\n实现将包含一个封装了这一逻辑的主求解器函数。该函数将针对四种指定的初始点 $\\mathbf{x}_0$ 中的每一种，分别在两种方法（PRP 和 PRP+）上被调用。对于每次运行，将记录最终的目标函数值以及一个指示是否遇到非下降方向的布尔标志。最终结果将被汇总并按指定格式化。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the NCG comparison and print results.\n    \"\"\"\n    params = {\n        'alpha0_init': 5.0,\n        'c1': 1e-4,\n        'rho': 0.5,\n        'epsilon': 1e-8,\n        'N_max': 50,\n        'M_max': 40,\n    }\n\n    test_cases = [\n        np.array([0.2, 0.0]),\n        np.array([0.0, 2.0]),\n        np.array([-1.5, 0.5]),\n        np.array([1.2, -1.0])\n    ]\n\n    all_results = []\n    for x0 in test_cases:\n        f_prp, b_prp = run_cg_solver(x0, 'prp', params)\n        f_prp_plus, b_prp_plus = run_cg_solver(x0, 'prp+', params)\n        \n        inner_result = [\n            round(f_prp, 6),\n            round(f_prp_plus, 6),\n            b_prp,\n            b_prp_plus\n        ]\n        all_results.append(inner_result)\n\n    # Format the final output string as specified, without spaces in lists.\n    string_inner_lists = []\n    for res_list in all_results:\n        parts = [str(item) for item in res_list]\n        string_inner_lists.append(f\"[{','.join(parts)}]\")\n    \n    final_output = f\"[{','.join(string_inner_lists)}]\"\n    print(final_output)\n\ndef f(x):\n    \"\"\"Objective function.\"\"\"\n    return 0.25 * x[0]**4 - 0.5 * x[0]**2 + 0.5 * x[1]**2\n\ndef grad_f(x):\n    \"\"\"Gradient of the objective function.\"\"\"\n    return np.array([x[0]**3 - x[0], x[1]])\n\ndef line_search(xk, dk, fk, grad_fk_dot_dk, params):\n    \"\"\"Backtracking Armijo line search.\"\"\"\n    alpha = params['alpha0_init']\n    c1 = params['c1']\n    rho = params['rho']\n    M_max = params['M_max']\n\n    for _ in range(M_max):\n        # A very small step might result from a non-descent direction.\n        # This is expected behavior demonstrating the algorithm's failure.\n        if grad_fk_dot_dk = 0:\n            pass # Armijo condition check below will handle this\n\n        if f(xk + alpha * dk) = fk + c1 * alpha * grad_fk_dot_dk:\n            return alpha\n        alpha *= rho\n    \n    return alpha # Return the last (smallest) alpha if M_max is reached.\n\ndef run_cg_solver(x0, method_type, params):\n    \"\"\"\n    Runs the Nonlinear Conjugate Gradient solver.\n    \"\"\"\n    x_k = np.copy(x0).astype(float)\n    k = 0\n    non_descent_occurred = False\n\n    N_max = params['N_max']\n    epsilon = params['epsilon']\n\n    grad_k = grad_f(x_k)\n    d_k = -grad_k\n    \n    while k  N_max:\n        grad_norm_k = np.linalg.norm(grad_k)\n        if grad_norm_k = epsilon:\n            break\n        \n        # Descent direction test for the current direction d_k\n        grad_fk_dot_dk = np.dot(grad_k, d_k)\n        if grad_fk_dot_dk = 0:\n            non_descent_occurred = True\n\n        # Perform line search to find an appropriate step length alpha_k\n        fk = f(x_k)\n        alpha_k = line_search(x_k, d_k, fk, grad_fk_dot_dk, params)\n        \n        # Update iterate\n        x_k_plus_1 = x_k + alpha_k * d_k\n        \n        # Compute gradient for the next iteration\n        grad_k_plus_1 = grad_f(x_k_plus_1)\n        \n        # Compute beta for the next direction\n        # Denominator for beta calculation\n        grad_k_norm_sq = np.dot(grad_k, grad_k)\n        \n        if grad_k_norm_sq  1e-16: # Safety for division by zero\n            beta_k_plus_1 = 0.0\n        else:\n            # Numerator for PRP beta\n            beta_prp_num = np.dot(grad_k_plus_1, grad_k_plus_1 - grad_k)\n            beta_prp = beta_prp_num / grad_k_norm_sq\n            \n            if method_type == 'prp':\n                beta_k_plus_1 = beta_prp\n            elif method_type == 'prp+':\n                beta_k_plus_1 = max(0.0, beta_prp)\n            else:\n                raise ValueError(\"Unknown method type\")\n\n        # Update direction for the next iteration\n        d_k_plus_1 = -grad_k_plus_1 + beta_k_plus_1 * d_k\n        \n        # Prepare for next iteration\n        x_k = x_k_plus_1\n        grad_k = grad_k_plus_1\n        d_k = d_k_plus_1\n        k += 1\n\n    final_f = f(x_k)\n    return final_f, non_descent_occurred\n\nsolve()\n```", "id": "2418475"}]}