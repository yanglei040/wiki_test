## 引言
在[数值优化](@entry_id:138060)的世界里，迭代算法是解决复杂问题的核心工具。它们从一个初始点出发，通过一系列步骤逐步逼近最优解。然而，一个根本性的实践问题随之而来：我们应该在何时停止迭代？过[早停](@entry_id:633908)止会牺牲解的精度，而过晚停止则浪费宝贵的计算资源。在解的质量与计算成本之间取得理想的平衡，正是“[停止准则](@entry_id:136282)与容差”这一主题的核心。本文旨在填补理论与实践之间的鸿沟，系统性地解决如何设计和选择有效且稳健的终止条件这一关键问题。在接下来的内容中，我们将首先深入“原理与机制”部分，剖析从基于迭代变化到基于[最优性条件](@entry_id:634091)的各种准则的内在逻辑与潜在陷阱。随后，在“应用与[交叉](@entry_id:147634)学科联系”一章中，我们将展示这些原理如何应用于机器学习、工程等前沿领域，并与特定算法（如[ADMM](@entry_id:163024)、[信赖域方法](@entry_id:138393)）相结合。最后，通过“动手实践”环节，您将有机会亲手构建和诊断[停止准则](@entry_id:136282)，将理论知识转化为可靠的编程技能。

## 原理与机制

在前一章中，我们了解到优化算法，尤其是用于大规模或复杂问题的算法，本质上是迭代的。它们从一个初始猜测点开始，生成一个序列 $\{x_k\}$，希望这个序列能够收敛到目标函数的一个解（例如，一个局部最小值点）。然而，在实践中，我们几乎永远无法让算法运行至达到理论上的精确解。计算资源是有限的，并且在达到一定精度后，继续迭代带来的收益会急剧减少。这就引出了一个至关重要的问题：我们应该在何时停止迭代？

这个决定依赖于**[停止准则](@entry_id:136282) (stopping criteria)**，有时也称为**[终止准则](@entry_id:136282) (termination criteria)**。一个好的[停止准则](@entry_id:136282)应该在计算成本和解的质量之间取得平衡。它应该能在迭代点 $x_k$ “足够接近”真实解 $x^*$ 时终止算法，同时要避免过[早停](@entry_id:633908)止（导致解的精度不足）或过晚停止（浪费计算资源）。

由于真实解 $x^*$ 通常是未知的，我们无法直接计算真实误差（例如 $\|x_k - x^*\|$ 或 $f(x_k) - f(x^*)$）。因此，我们必须依赖于可计算的**代理指标 (proxy metrics)** 来间接判断收敛性。本章将系统地探讨设计和应用[停止准则](@entry_id:136282)的核心原理与关键机制，涵盖从简单到复杂的各种优化场景。

### 基于迭代变化的准则

最直观的一类[停止准则](@entry_id:136282)基于对迭代序列 $\{x_k\}$ 自身变化的观察。其基本思想是，当算法收敛时，后续迭代点之间的变化会越来越小。

#### 绝对与相对变化

我们可以监测连续两次迭代 $x_k$ 和 $x_{k+1}$ 之间的**绝对变化 (absolute change)**。当这个变化量小于某个预设的**容差 (tolerance)** $\epsilon$ 时，我们就停止迭代。这个准则可以表示为：

$$
\|x_{k+1} - x_k\| \le \epsilon
$$

这里的范数 $\|\cdot\|$ 可以是任何[向量范数](@entry_id:140649)，例如欧几里得范数（$L_2$ 范数）或[无穷范数](@entry_id:637586)（$L_\infty$ 范数）。

然而，绝对变化准则对解的尺度很敏感。如果解向量的分量本身数值很大，那么即使[相对误差](@entry_id:147538)很小，其绝对变化也可能很大。为了克服这个问题，使用**相对变化 (relative change)** 通常是更好的选择：

$$
\frac{\|x_{k+1} - x_k\|}{\|x_{k+1}\|} \le \epsilon
$$

这个准则将变化量与当前迭代点的大小进行了归一化，使其对解的尺度不那么敏感。分母有时也使用 $\|x_k\|$ 或 $\max\{\|x_k\|, \|x_{k+1}\|\}$ 以增加稳定性。

**示例**：假设一个迭代方法用于求解一个线性方程组，在第 $k$ 步和第 $k+1$ 步得到的解向量分别是：
$$
\mathbf{x}^{(k)} = \begin{pmatrix} 3.451 \\ -1.234 \\ 5.802 \end{pmatrix} \quad \text{和} \quad \mathbf{x}^{(k+1)} = \begin{pmatrix} 3.486 \\ -1.201 \\ 5.845 \end{pmatrix}
$$
我们使用的[停止准则](@entry_id:136282)是基于[无穷范数](@entry_id:637586)的相对变化，容差为 $\epsilon = 7.5 \times 10^{-3}$。首先计算差向量的[无穷范数](@entry_id:637586)：
$$
\|\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\|_{\infty} = \left\| \begin{pmatrix} 0.035 \\ 0.033 \\ 0.043 \end{pmatrix} \right\|_{\infty} = \max\{|0.035|, |0.033|, |0.043|\} = 0.043
$$
然后计算新迭代点的[无穷范数](@entry_id:637586)：
$$
\|\mathbf{x}^{(k+1)}\|_{\infty} = \max\{|3.486|, |-1.201|, |5.845|\} = 5.845
$$
相对变化为：
$$
\frac{\|\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\|_{\infty}}{\|\mathbf{x}^{(k+1)}\|_{\infty}} = \frac{0.043}{5.845} \approx 7.36 \times 10^{-3}
$$
由于 $7.36 \times 10^{-3} \le 7.5 \times 10^{-3}$，该[停止准则](@entry_id:136282)被满足，算法可以终止 [@problem_id:2182360]。

#### 慢收敛的陷阱

尽管基于迭代变化的准则很直观，但它有一个严重的缺陷：**它可能在算法收敛缓慢时过早终止**。当算法在一个平坦的区域（[目标函数](@entry_id:267263)梯度很小）或者沿着一个狭长的山谷下降时，每一步的移动可能非常小，即使当前点距离真正的最小值点还很远。

**示例**：考虑一个算法，其生成的序列 $x_k$ 收敛到真实解 $x^* = 2$。假设我们有两个[停止准则](@entry_id:136282)，容差都为 $\epsilon = 0.05$：
- 准则A（基于真实误差）：$|x_k - x^*| \le \epsilon$
- 准则B（基于迭代变化）：$|x_k - x_{k-1}| \le \epsilon$

假设算法的收敛过程非常缓慢，例如 $x_k = 2 + 0.8^k$。我们可以计算出，基于迭代变化的准则B在第8次迭代时就会满足（$|x_8 - x_7| \approx 0.042  0.05$），但此时的真实[绝对误差](@entry_id:139354)仍然很大（$|x_8 - 2| = 0.8^8 \approx 0.168 \gg 0.05$）。而基于真实误差的准则A直到第14次迭代才被满足 [@problem_id:2206870]。这个例子清楚地表明，仅仅因为算法“走得慢”就判断它“已到达”是不可靠的。

### 基于[最优性条件](@entry_id:634091)的准则

为了克服上述缺陷，更可靠的[停止准则](@entry_id:136282)应该基于问题本身的**[最优性条件](@entry_id:634091) (optimality conditions)**。其核心思想是检查当前迭代点 $x_k$ 在多大程度上满足了成为一个解所必须满足的数学条件。

#### 无约束光滑优化

对于无约束光滑[优化问题](@entry_id:266749) $\min_{x \in \mathbb{R}^n} f(x)$，[一阶必要条件](@entry_id:170730)是解点 $x^*$ 的梯度为零，即 $\nabla f(x^*) = 0$。因此，一个自然且广泛使用的[停止准则](@entry_id:136282)是检查梯度范数的大小：

$$
\|\nabla f(x_k)\| \le \varepsilon
$$

当梯度范数足够小时，我们可以认为 $x_k$ 接近一个**稳定点 (stationary point)**。

为了增加鲁棒性，特别是处理不同尺度的问题，一种更先进的准则是**混合绝对-相对梯度范数测试 (mixed absolute-relative gradient-norm test)**：

$$
\|\nabla f(x_k)\| \le \varepsilon_{\mathrm{abs}} + \varepsilon_{\mathrm{rel}} \|\nabla f(x_0)\|
$$

其中 $\varepsilon_{\mathrm{abs}}$ 是一个绝对容差，确保即使初始梯度 $\|\nabla f(x_0)\|$ 很小，算法最终也能收敛到一个梯度足够小的点。而 $\varepsilon_{\mathrm{rel}}$ 是一个相对容差，它将收敛要求与问题的初始状态（即初始梯度的大小）联系起来，使得该准则对于[目标函数](@entry_id:267263) $f(x)$ 的尺度变化不那么敏感 [@problem_id:3187951]。例如，如果我们将[目标函数](@entry_id:267263)放大一倍 $g(x) = 2f(x)$，那么所有梯度值都会变为原来的两倍。一个纯粹的绝对容差准则 $\|\nabla g(x_k)\| \le \varepsilon_{\mathrm{abs}}$ 会变得更难满足，而混合准则的相对部分则能适应这种变化。当且仅当 $\varepsilon_{\mathrm{abs}}=0$ 时，该准则对于任意正因子 $c0$ 的缩放 $g(x)=cf(x)$ 是完全不变的 [@problem_id:3187951]。

#### 尺度问题与病态条件

然而，即使是基于梯度的准则也并非万无一失。它最大的弱点在于对**病态条件 (ill-conditioning)** 的敏感性。当一个问题的海森矩阵 $H = \nabla^2 f(x)$ 的条件数 $\kappa(H) = \frac{\lambda_{\max}(H)}{\lambda_{\min}(H)}$ 非常大时，我们称该问题是病态的。这通常对应于目标函数的[等高线](@entry_id:268504)呈狭长的椭球状。

在这种狭长的“山谷”中，即使梯度范数 $\|\nabla f(x_k)\|$ 已经很小，当前点 $x_k$ 离谷底（即最优解 $x^*$）的距离以及函数值的次优性 $f(x_k) - f(x^*)$ 仍可能非常大。这是因为次优性与梯度的关系为：

$$
f(x_k) - f(x^*) = \frac{1}{2} (\nabla f(x_k))^{\top} (\nabla^2 f(\bar{x}))^{-1} (\nabla f(x_k))
$$

其中 $\bar{x}$ 是 $x_k$ 和 $x^*$ 之间的某个点。如果[海森矩阵](@entry_id:139140)的逆 $(\nabla^2 f)^{-1}$ 有一个非常大的[特征值](@entry_id:154894)（对应于 $\nabla^2 f$ 有一个非常小的[特征值](@entry_id:154894)，即“平坦”方向），那么即使梯度 $\nabla f(x_k)$ 很小，其二次型的值也可能很大。

**示例**：考虑一个二次函数 $f(x) = \frac{1}{2} x^{\top} Q x$，其中 $Q = \mathrm{diag}(10^{-6}, 10^{2})$。该问题的条件数极大。假设算法在某点 $x_k$ 停止，满足 $\|\nabla f(x_k)\|_2 \le 10^{-3}$。通过分析可以发现，最坏情况下的次优性 $f(x_k) - f(x^*)$ 可以高达 $0.5$。这是因为如果梯度 $g = \nabla f(x_k)$ 恰好指向 $Q$ 最平坦的方向（对应于最小特征值 $10^{-6}$ 的方向），那么次优性 $f(x_k)-f(x^*) = \frac{1}{2}g^\top Q^{-1} g$ 将被 $Q^{-1}$ 的最大[特征值](@entry_id:154894) $10^6$ 放大，导致 $\frac{1}{2} (10^{-3})^2 (10^6) = 0.5$。这个例子戏剧性地说明，对于病态问题，一个未缩放的梯度范数准则是不可靠的 [@problem_id:3187908]。

#### 尺度不变与预处理准则

解决上述尺度问题的根本方法是设计**尺度不变 (scale-invariant)** 的[停止准则](@entry_id:136282)。这意味着准则的判断结果不应因变量或目标函数的单位或尺度的改变而改变。

一种方法是使用[对角矩阵](@entry_id:637782)对梯度进行缩放。例如，一个**尺度感知 (scaling-aware)** 的准则可以定义为：
$$
\|D(x_k)^{-1}\nabla f(x_k)\| \le \varepsilon
$$
其中 $D(x_k)$ 是一个[对角缩放](@entry_id:748382)矩阵，例如 $D(x_k) = \operatorname{diag}(\max\{1, |x_k|\})$。这个准则对每个梯度分量进行了与其对应变量大小相关的归一化，从而对变量尺度的差异不那么敏感 [@problem_id:3187890]。

一个更通用的概念是**[预处理](@entry_id:141204) (preconditioning)**。许多高级优化算法（如[预处理共轭梯度法](@entry_id:753674)或[拟牛顿法](@entry_id:138962)）在内部使用一个对称正定矩阵 $M$（称为**[预处理器](@entry_id:753679)**）来改善问题的几何性质。这个矩阵 $M$ 通常是[海森矩阵](@entry_id:139140) $\nabla^2 f(x)$ 的一个近似。这类算法的迭代步长形式为 $x_{k+1} = x_k - \alpha_k M^{-1} \nabla f(x_k)$。

对于这类算法，最自然的[停止准则](@entry_id:136282)不是标准的[欧几里得范数](@entry_id:172687)，而是与预处理器 $M$ 相关的**能量范数 (energy norm)**：

$$
\|M^{-1/2} \nabla f(x_k)\| \le \varepsilon_M
$$

这个准则有深刻的几何意义。它等价于在经过变量代换 $y = M^{1/2} x$ 后的新[坐标系](@entry_id:156346)下计算的普通梯度范数。如果 $M$ 是一个好的[预处理器](@entry_id:753679)（例如，$M \approx \nabla^2 f(x)$），那么新[坐标系](@entry_id:156346)下的问题是良态的，梯度范数也就成了可靠的收敛指标。因此，对于使用[预处理](@entry_id:141204)的算法，[预处理](@entry_id:141204)范数准则通常是首选，因为它与算法的内在几何结构一致，并且具有尺度不变性。相比之下，标准的[欧几里得范数](@entry_id:172687) $\|\nabla f(x_k)\|$ 虽然失去了这种几何一致性，但它的优点是提供了一个与算法或预处理器选择无关的通用衡量标准，便于在不同算法之间进行比较 [@problem_id:3187949]。

此外，当函数具有**强[凸性](@entry_id:138568) (strong convexity)** 时，我们可以从梯度范数的大小直接推断出解的误差界。如果函数是 $\mu$-强凸的，则有以下重要关系：
- $\|x_k - x^*\| \le \frac{1}{\mu} \|\nabla f(x_k)\|$
- $f(x_k) - f(x^*) \le \frac{1}{2\mu} \|\nabla f(x_k)\|^2$

这意味着，一旦梯度范数 $\|\nabla f(x_k)\|$ 被控制在某个范围内（例如，通过混合绝对-相对准则），我们就可以保证次优性和到解的距离也被控制在某个可以计算的界限内 [@problem_id:3187951]。

### 约束与[复合优化](@entry_id:165215)问题的[停止准则](@entry_id:136282)

当[优化问题](@entry_id:266749)包含约束或非光滑项时，[最优性条件](@entry_id:634091)会变得更加复杂，[停止准则](@entry_id:136282)也必须相应地进行调整。

#### [复合优化](@entry_id:165215)

许多现代[优化问题](@entry_id:266749)（例如在统计和机器学习中）具有**复合结构 (composite structure)**，其形式为 $\min F(x) = f(x) + g(x)$，其中 $f(x)$ 是光滑的，而 $g(x)$ 是凸但可能非光滑的（例如 $L_1$ 范数）。

对于这类问题，[最优性条件](@entry_id:634091)不再是 $\nabla F(x^*) = 0$（因为 $g(x)$ 可能不可微），而是 $0 \in \nabla f(x^*) + \partial g(x^*)$，其中 $\partial g$ 是 $g$ 的**[次微分](@entry_id:175641) (subdifferential)**。因此，仅仅检查 $\|\nabla f(x_k)\|$ 是否足够小是错误的。

正确的做法是使用一个能反映完整[最优性条件](@entry_id:634091)的代理指标。对于[近端梯度法](@entry_id:634891) (proximal gradient methods)，这个指标是**近端梯度映射 (proximal gradient mapping)** 的范数：
$$
G_\alpha(x) := \frac{1}{\alpha} \left( x - \operatorname{prox}_{\alpha g}\big(x - \alpha \nabla f(x)\big) \right)
$$
其中 $\operatorname{prox}_{\alpha g}$ 是函数 $g$ 的**[近端算子](@entry_id:635396) (proximal operator)**。$G_\alpha(x) = 0$ 当且仅当 $x$ 满足复合问题的[最优性条件](@entry_id:634091)。因此，一个可靠的[停止准则](@entry_id:136282)是检查：
$$
\|G_\alpha(x_k)\| \le \varepsilon
$$
当 $g(x) \equiv 0$ 时，[近端算子](@entry_id:635396)是恒等映射，此时 $G_\alpha(x)$ 精确地简化为 $\nabla f(x)$，该准则退化为标准的光滑优化准则 [@problem_id:3187901]。

#### 约束优化

对于约束优化问题，[停止准则](@entry_id:136282)必须同时衡量**可行性 (feasibility)**（即满足约束的程度）和**最优性 (optimality)**（即满足拉格朗日相关的[最优性条件](@entry_id:634091)）。

**1. [盒子约束](@entry_id:746959)**

对于简单的[盒子约束](@entry_id:746959)问题 $\min f(x)$ s.t. $l \le x \le u$，[最优性条件](@entry_id:634091)表明，在解点 $x^*$ 处，梯度 $\nabla f(x^*)$ 的分量必须指向盒子内部或在边界上被“挡住”。这个条件可以通过**投影梯度 (projected gradient)** 来刻画。一个点 $x$ 是[稳定点](@entry_id:136617)的充要条件是它成为一个定点：$x = \Pi_{[l,u]}(x - \alpha \nabla f(x))$，其中 $\Pi_{[l,u]}$ 是到盒子 $[l, u]$ 上的[投影算子](@entry_id:154142)。

因此，一个自然的[停止准则](@entry_id:136282)是检查投影梯度映射的范数：
$$
\|\mathbf{x}_k - \Pi_{[l,u]}\big(\mathbf{x}_k - \nabla f(\mathbf{x}_k)\big)\| \le \varepsilon
$$
这个量在稳定点处为零，为衡量偏离最优性提供了一个很好的指标 [@problem_id:3187933]。可行性检查则很简单，只需确认 $x_k$ 在盒子内即可，这通常由算法（如[投影梯度法](@entry_id:169354)）的每一步来保证。

**2. 一般[等式约束](@entry_id:175290)与单位问题**

对于一般的[等式约束](@entry_id:175290)问题 $\min f(x)$ s.t. $c(x) = 0$，[KKT条件](@entry_id:185881)要求解点 $(x^*, \lambda^*)$ 同时满足：
- **原始可行性 (Primal feasibility)**: $c(x^*) = 0$
- **对偶可行性 (Dual feasibility)**: $\nabla_x \mathcal{L}(x^*, \lambda^*) = \nabla f(x^*) + (\nabla c(x^*))^\top \lambda^* = 0$

因此，[停止准则](@entry_id:136282)必须包含两个部分：
- $\|c(x_k)\| \le \varepsilon_c$
- $\|\nabla_x \mathcal{L}(x_k, \lambda_k)\| \le \varepsilon_g$

一个极其重要但在实践中常被忽视的问题是**物理单位 (physical units)**。如果变量 $x_i$ 或约束 $c_j(x)$ 具有不同的物理单位（例如，米和毫米），那么直接计算范数（如 $\|c(x_k)\|_2$）在物理上是无意义的，因为它涉及将不同单位的量相加。这种做法会导致准则不具有单位不变性——仅仅因为你将毫米换算成米，算法的停止决策就可能改变。

正确的做法是在计算范数或进行比较之前，先对所有量进行**无量纲化 (non-dimensionalization)**。这通常通过将每个分量除以一个具有相同单位的“特征尺度”值来实现。

**示例**：假设一个问题中，$c_1(x_k) = 3 \times 10^{-4}$ 米，$c_2(x_k) = 0.2$ 毫米。直接计算范数是错误的。一个稳健的策略是选择一个统一的长度尺度（例如1米），然后构建一个缩放向量或矩阵 $W_c$ 来进行无量纲化。例如，令缩放后的向量为 $c'_{\text{scaled}} = (|c_1|/(1 \text{ m}), |c_2|/(1 \text{ mm})) = (3 \times 10^{-4}, 0.2)$，然后比较 $\max(c'_{\text{scaled}})$ 与一个无量纲容差。一个更好的方法是将所有单位统一，例如，将 $0.2$ 毫米转换为 $2 \times 10^{-4}$ 米，然后对[无量纲化](@entry_id:136704)的向量 $(3 \times 10^{-4}, 2 \times 10^{-4})$ 进行比较。对拉格朗日梯度的处理也遵循同样的原则 [@problem_id:3187859]。

**3. [内点法](@entry_id:169727)**

对于[内点法](@entry_id:169727) (interior-point methods)，特别是在[线性规划](@entry_id:138188)或二次规划中，[停止准则](@entry_id:136282)与算法的结构紧密相连。这些方法通过引入一个**障碍参数 (barrier parameter)** $\mu  0$ 来将约束问题转化为一系列无约束问题。[KKT条件](@entry_id:185881)被扰动，例如互补松弛条件 $x_i s_i = 0$ 变为 $x_i s_i = \mu$。

算法的目标是同时驱动原始可行性残差 $r_{\text{pri}}$、对偶可行性残差 $r_{\text{dual}}$ 和障碍参数 $\mu$ 趋于零。因此，一个典型的[内点法](@entry_id:169727)[停止准则](@entry_id:136282)包含三个部分：
- $\|r_{\text{pri}}\| \le \varepsilon_{\text{pri}}$
- $\|r_{\text{dual}}\| \le \varepsilon_{\text{dual}}$
- $\mu_{\text{comp}} := \frac{x^\top s}{n} \le \varepsilon_{\mu}$

其中 $\mu_{\text{comp}}$ 是平均互补间隙。算法的逻辑，包括何时减小障碍参数 $\mu$，通常也与这些残差的大小相关联。例如，只有当可行性残差相对于当前的互补间隙足够小时，算法才会减小 $\mu$，以确保迭代点保持在[中心路径](@entry_id:147754)附近 [@problem_id:3187939]。

### 结论

选择和实施[停止准则](@entry_id:136282)是[优化算法](@entry_id:147840)设计中一个微妙而关键的环节。没有一个“万能”的准则。一个优秀的实践者必须根据问题的具体特性来做出明智的选择：
- 对于简单或初步的分析，基于迭代变化的准则可能就足够了。
- 对于标准的无约束光滑优化，基于梯度范数的准则是黄金标准。
- 当问题是病态的或变量尺度差异很大时，必须使用[尺度不变的](@entry_id:178566)或预处理的准则来避免误判。
- 对于有约束或非光滑的问题，必须使用能正确反映相应[最优性条件](@entry_id:634091)的准则，例如基于投影梯度、近端梯度映射或KKT残差的准则。
- 在涉及物理单位的工程或科学问题中，必须通过无量纲化来确保准则的物理意义和单位[不变性](@entry_id:140168)。

深刻理解这些原理，是开发稳健、高效和可靠的优化软件的基石。