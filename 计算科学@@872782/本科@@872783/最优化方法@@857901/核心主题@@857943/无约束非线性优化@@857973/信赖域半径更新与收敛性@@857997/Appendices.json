{"hands_on_practices": [{"introduction": "我们将从一个简化的理想场景开始，为后续更复杂的实践打下坚实的理论基础。通过分析一个凸二次目标函数，我们可以假设模型是完美的（即实际下降与预测下降的比率 $\\rho_k = 1$），从而精确推导出信赖域半径和梯度范数的演化序列。这项练习 [@problem_id:3193997] 将揭示梯度大小、信赖域尺寸以及计算步长（内部步或边界步）之间的根本相互作用，帮助您从第一性原理层面理解半径更新的核心逻辑。", "problem": "考虑一个信赖域方法，应用于最小化一个严格凸二次目标函数 $f(x) = \\frac{1}{2} x^{\\top} Q x + b^{\\top} x$，其中 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是一个已知的对称正定矩阵。假设 $Q = 2 I$，其中 $I$ 是单位矩阵，初始点 $x_0$ 使得初始梯度为 $g_0 = \\nabla f(x_0) = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$，因此 $\\|g_0\\| = 1$。在第 $k$ 次迭代时的信赖域子问题是最小化二次模型 $m_k(s) = f(x_k) + g_k^{\\top} s + \\frac{1}{2} s^{\\top} Q s$，约束条件为 $\\|s\\| \\leq \\Delta_k$。假设在理想情况下，对所有 $k$，实际减少量与预测减少量之比满足 $\\rho_k = 1$，并且在每次迭代中都计算出信赖域子问题的精确全局最小值点 $s_k$。\n\n使用以下半径更新规则：\n- 如果计算出的步长满足 $\\|s_k\\| = \\Delta_k$（即步长位于信赖域的边界上），则设置 $\\Delta_{k+1} = \\tau \\Delta_k$，其中 $\\tau = 1.5$。\n- 如果计算出的步长满足 $\\|s_k\\|  \\Delta_k$（即步长严格位于信赖域内部），则保持半径不变：$\\Delta_{k+1} = \\Delta_k$。\n\n设初始信赖域半径为 $\\Delta_0 = 0.1$。在这些假设和更新规则下：\n1. 从第一性原理推导二次模型（其中 $Q = 2 I$）的信赖域子问题的精确解 $s_k$，区分边界和内部两种情况，并说明梯度 $\\|g_k\\|$ 如何演化。\n2. 使用推导出的关系，计算精确序列 $\\Delta_0, \\Delta_1, \\Delta_2, \\ldots$，直到并包括第一个使步长为内部步长（即 $\\|s_K\\|  \\Delta_K$）的迭代索引 $K$，这将导致在下一次迭代中收敛。\n3. 在二次函数的信赖域方法背景下，简要解释激进的信赖域半径增长（较大的 $\\tau$）与数值稳定性之间的权衡。\n\n将最终答案以包含 $(\\Delta_0, \\Delta_1, \\ldots, \\Delta_K, K)$ 的行向量形式给出。无需四舍五入。", "solution": "该问题是有效的，因为它在数值优化理论中有科学依据，提法得当，并且呈现客观。它要求对一个严格凸二次函数上的信赖域方法进行分析，这是优化文献中的一个标准且具有说明性的案例。所有必需的参数和规则都已给出。\n\n此问题将按要求分三部分解决。\n\n第一部分：子问题解和梯度演化的推导。\n\n第 $k$ 次迭代的信赖域子问题是找到一个步长 $s_k$ 来求解：\n$$ \\min_{s} \\quad m_k(s) = f(x_k) + g_k^{\\top} s + \\frac{1}{2} s^{\\top} Q s $$\n$$ \\text{subject to} \\quad \\|s\\| \\leq \\Delta_k $$\n其中 $g_k = \\nabla f(x_k)$ 且 $Q = 2I$。该模型为 $m_k(s) = f(x_k) + g_k^{\\top} s + s^{\\top}s$。\n\n这是一个凸优化问题，因此 Karush-Kuhn-Tucker (KKT) 条件是最优性的充分必要条件。拉格朗日函数是：\n$$ \\mathcal{L}(s, \\lambda) = m_k(s) + \\frac{\\lambda}{2} (s^{\\top}s - \\Delta_k^2) = f(x_k) + g_k^{\\top} s + s^{\\top}s + \\frac{\\lambda}{2} (s^{\\top}s - \\Delta_k^2) $$\nKKT 条件是：\n1. 驻点条件：$\\nabla_s \\mathcal{L}(s, \\lambda) = g_k + 2s + \\lambda s = g_k + (2+\\lambda)s = 0$\n2. 原始可行性：$\\|s\\|^2 \\leq \\Delta_k^2$\n3. 对偶可行性：$\\lambda \\geq 0$\n4. 互补松弛性：$\\lambda (\\|s\\|^2 - \\Delta_k^2) = 0$\n\n根据驻点条件，我们有 $(2+\\lambda)s = -g_k$。因为 $Q=2I$ 是正定的，所以对于任何 $\\lambda \\ge 0$ 都有 $2+\\lambda > 0$。因此，我们可以将解 $s$ 写为：\n$$ s_k(\\lambda) = -\\frac{1}{2+\\lambda} g_k $$\n\n我们根据互补松弛性条件分析两种情况。\n\n情况A：内部解（$\\|s_k\\|  \\Delta_k$）。\n约束不起作用，这意味着 $\\lambda=0$。步长是模型 $m_k(s)$ 的无约束最小化点：\n$$ s_k = -\\frac{1}{2} g_k $$\n当无约束步长的范数在信赖域半径内时，会发生这种情况：\n$$ \\|s_k\\| = \\|-\\frac{1}{2} g_k\\| = \\frac{1}{2} \\|g_k\\|  \\Delta_k $$\n这等价于条件 $\\|g_k\\|  2\\Delta_k$。\n\n情况B：边界解（$\\|s_k\\| = \\Delta_k$）。\n约束是激活的，这意味着 $\\lambda \\ge 0$。我们必须求解 $\\lambda$ 使得 $\\|s_k(\\lambda)\\| = \\Delta_k$。\n$$ \\|s_k(\\lambda)\\| = \\left\\|-\\frac{1}{2+\\lambda} g_k\\right\\| = \\frac{1}{2+\\lambda} \\|g_k\\| = \\Delta_k $$\n求解 $2+\\lambda$ 可得 $2+\\lambda = \\frac{\\|g_k\\|}{\\Delta_k}$。\n将其代回到 $s_k$ 的表达式中：\n$$ s_k = -\\frac{1}{(\\|g_k\\|/\\Delta_k)} g_k = -\\Delta_k \\frac{g_k}{\\|g_k\\|} $$\n这是一个在最速下降方向上的步长，其长度等于信赖域半径 $\\Delta_k$。当无约束步长将位于信赖域的外部或边界上时，即 $\\frac{1}{2}\\|g_k\\| \\ge \\Delta_k$，或 $\\|g_k\\| \\ge 2\\Delta_k$，会发生这种情况。\n\n接下来，我们推导梯度的演化。目标函数为 $f(x) = \\frac{1}{2}x^\\top Q x + b^\\top x$，所以其梯度为 $\\nabla f(x) = Qx+b$。\n新的迭代点是 $x_{k+1} = x_k + s_k$。新的梯度是：\n$$ g_{k+1} = \\nabla f(x_{k+1}) = Q(x_k + s_k) + b = (Qx_k+b) + Qs_k = g_k + Qs_k $$\n当 $Q=2I$ 时，这变成 $g_{k+1} = g_k + 2s_k$。\n\n我们来研究每种情况下的梯度更新：\n- 对于内部解（$s_k = -\\frac{1}{2} g_k$）：\n  $$ g_{k+1} = g_k + 2(-\\frac{1}{2} g_k) = g_k - g_k = 0 $$\n  算法在这一步找到了精确的最小值点。\n- 对于边界解（$s_k = -\\Delta_k \\frac{g_k}{\\|g_k\\|}$）：\n  $$ g_{k+1} = g_k + 2\\left(-\\Delta_k \\frac{g_k}{\\|g_k\\|}\\right) = \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) g_k $$\n  新梯度 $g_{k+1}$ 与 $g_k$ 共线。新梯度的范数是：\n  $$ \\|g_{k+1}\\| = \\left\\| \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) g_k \\right\\| = \\left|1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right| \\|g_k\\| $$\n  因为这种情况发生在 $\\|g_k\\| \\ge 2\\Delta_k$ 时，所以项 $1 - \\frac{2\\Delta_k}{\\|g_k\\|}$ 是非负的。\n  $$ \\|g_{k+1}\\| = \\left(1 - \\frac{2\\Delta_k}{\\|g_k\\|}\\right) \\|g_k\\| = \\|g_k\\| - 2\\Delta_k $$\n\n第二部分：序列 $\\Delta_0, \\Delta_1, \\ldots, \\Delta_K$ 的计算。\n\n给定 $\\|g_0\\| = 1$ 和 $\\Delta_0 = 0.1$。对于边界步长，半径更新为 $\\Delta_{k+1} = 1.5 \\Delta_k$；对于内部步长，半径更新为 $\\Delta_{k+1} = \\Delta_k$。我们寻找第一个使步长成为内部步长的索引 $K$。\n\n迭代 $k=0$：\n- 我们有 $\\|g_0\\| = 1$ 和 $\\Delta_0 = 0.1$。\n- 我们检查条件：$\\|g_0\\| = 1$ 对比 $2\\Delta_0 = 2(0.1) = 0.2$。\n- 由于 $1 \\ge 0.2$，我们处于边界情况。步长为 $\\|s_0\\|=\\Delta_0$。\n- 新的梯度范数是 $\\|g_1\\| = \\|g_0\\| - 2\\Delta_0 = 1 - 0.2 = 0.8$。\n- 新的半径是 $\\Delta_1 = 1.5 \\Delta_0 = 1.5 \\times 0.1 = 0.15$。\n\n迭代 $k=1$：\n- 我们有 $\\|g_1\\| = 0.8$ 和 $\\Delta_1 = 0.15$。\n- 我们检查条件：$\\|g_1\\| = 0.8$ 对比 $2\\Delta_1 = 2(0.15) = 0.3$。\n- 由于 $0.8 \\ge 0.3$，我们处于边界情况。步长为 $\\|s_1\\|=\\Delta_1$。\n- 新的梯度范数是 $\\|g_2\\| = \\|g_1\\| - 2\\Delta_1 = 0.8 - 0.3 = 0.5$。\n- 新的半径是 $\\Delta_2 = 1.5 \\Delta_1 = 1.5 \\times 0.15 = 0.225$。\n\n迭代 $k=2$：\n- 我们有 $\\|g_2\\| = 0.5$ 和 $\\Delta_2 = 0.225$。\n- 我们检查条件：$\\|g_2\\| = 0.5$ 对比 $2\\Delta_2 = 2(0.225) = 0.45$。\n- 由于 $0.5 \\ge 0.45$，我们处于边界情况。步长为 $\\|s_2\\|=\\Delta_2$。\n- 新的梯度范数是 $\\|g_3\\| = \\|g_2\\| - 2\\Delta_2 = 0.5 - 0.45 = 0.05$。\n- 新的半径是 $\\Delta_3 = 1.5 \\Delta_2 = 1.5 \\times 0.225 = 0.3375$。\n\n迭代 $k=3$：\n- 我们有 $\\|g_3\\| = 0.05$ 和 $\\Delta_3 = 0.3375$。\n- 我们检查条件：$\\|g_3\\| = 0.05$ 对比 $2\\Delta_3 = 2(0.3375) = 0.675$。\n- 由于 $0.05  0.675$，我们处于内部情况。这是第一个内部步长。\n- 因此，索引为 $K=3$。\n- 所取的步长是 $s_3 = -\\frac{1}{2}g_3$，其范数为 $\\|s_3\\| = \\frac{1}{2}\\|g_3\\| = \\frac{0.05}{2} = 0.025$。这确实小于 $\\Delta_3=0.3375$。根据规则，半径不会改变，所以 $\\Delta_4 = \\Delta_3$。\n- 算法在下一步终止，因为 $g_4=0$。\n\n直到 $\\Delta_K$ 的半径序列是：\n$\\Delta_0 = 0.1$\n$\\Delta_1 = 0.15$\n$\\Delta_2 = 0.225$\n$\\Delta_3 = 0.3375$\n索引为 $K=3$。\n所求的行向量是 $(\\Delta_0, \\Delta_1, \\Delta_2, \\Delta_3, K) = (0.1, 0.15, 0.225, 0.3375, 3)$。\n\n第三部分：权衡解释。\n\n信赖域半径更新因子 $\\tau$ 控制信赖域大小的调整速度。$\\tau$ 的选择代表了收敛速度和鲁棒性之间的基本权衡，特别是对于一般的非线性函数。\n\n激进的增长策略（一个大的 $\\tau$，例如 $\\tau > 2$）允许当模型是目标函数的一个良好预测器时（即当实际减少量与预测减少量之比 $\\rho_k$ 很高时），信赖域半径 $\\Delta_k$ 能够迅速增加。对于二次函数，如此问题所示，模型是函数的完美表示。因此，采取最优牛顿步（$s = -H^{-1}g$）的唯一障碍是信赖域约束。一个大的 $\\tau$ 允许信赖域快速扩张到一个可以包含牛顿步的大小，从而用更少的迭代次数实现收敛。在这种理想化设置下，更激进的 $\\tau$ 对性能来说是严格更优的。\n\n然而，在更一般的非二次目标函数的情况下，二次模型 $m_k(s)$ 只是一个局部近似。过于激进的半径扩张可能导致信赖域变得过大，使得模型在区域边界不再是目标函数的忠实表示。这可能导致计算出的步长 $s_k$ 很差，从而使得 $\\rho_k$ 的值很小甚至为负。结果是，该步长被拒绝，信赖域必须缩小。这可能导致算法在半径的激进扩张和急剧收缩之间振荡，这是低效且数值不稳定的。\n\n保守的增长策略（一个较小的 $\\tau$，例如 $\\tau \\in (1, 2]$）促进了数值稳定性和鲁棒性。通过更谨慎地扩张信赖域，模型在信赖域内保持良好近似的可能性更大。这导致接受步长和取得稳定进展的概率更高，这在处理高度非线性函数或远离局部最小值点时尤其重要。这种稳定性的代价是可能更慢的收敛速度，因为它可能需要更多次迭代才能使信赖域变得足够大，以允许更长、更富有成效的步长。\n\n总之，这种权衡是在表现良好（接近二次）的函数上实现快速收敛与在更困难、高度非线性的问题上确保稳定、可靠的进展之间进行的。", "answer": "$$ \\boxed{ \\begin{pmatrix} 0.1  0.15  0.225  0.3375  3 \\end{pmatrix} } $$", "id": "3193997"}, {"introduction": "接下来，我们将从理想模型走向现实世界，挑战著名的 Rosenbrock 函数，它以其狭窄的弯曲谷底而闻名。实践表明，简单的半径更新规则在处理此类问题时常常会失效，导致算法在峡谷中振荡或信赖域半径过早崩溃。这项练习 [@problem_id:3193957] 要求您实现一个包含实用子问题求解器（截断共轭梯度法）的信赖域方法，并通过调试关键更新参数，学会如何引导算法稳定高效地收敛。", "problem": "您需要实现并研究一种带有显式信赖域半径更新的信赖域方法，该方法应用于一个具有狭窄弯曲谷的非凸函数。目标是诊断信赖域半径的朴素更新规则何时会导致信赖域半径的振荡或过早崩溃，并展示能够保持进展且仅在驻点附近才将信赖域半径趋于零的参数选择。\n\n考虑具有狭窄弯曲谷的双变量Rosenbrock函数，\n$$\nf(x,y) \\;=\\; 100\\,(y - x^2)^2 + (1 - x)^2,\n$$\n其定义于所有 $(x,y)\\in \\mathbb{R}^2$。令 $x_k = (x_k^{(1)}, x_k^{(2)})^\\top \\in \\mathbb{R}^2$，并定义梯度 $g_k = \\nabla f(x_k)$ 和Hessian矩阵 $B_k = \\nabla^2 f(x_k)$。在第 $k$ 次迭代时，信赖域模型是二阶泰勒近似\n$$\nm_k(s) \\;=\\; f(x_k) + g_k^\\top s + \\tfrac{1}{2} s^\\top B_k s,\n$$\n带有一个信赖域约束 $\\|s\\|_2 \\le \\Delta_k$，其中 $\\Delta_k > 0$ 是信赖域半径，$\\|\\cdot\\|_2$ 表示欧几里得范数。在每次迭代中，您需要求解信赖域子问题的近似解，即在 $\\|s\\|_2 \\le \\Delta_k$ 的约束下最小化 $m_k(s)$。\n\n使用以下基本定义和事实作为您推导和实现的基础：\n- 一个二次连续可微函数的梯度和Hessian矩阵被定义为表征一阶和二阶泰勒模型的一阶和二阶导数。\n- 信赖域比率定义为\n$$\n\\rho_k \\;=\\; \\frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} \\;=\\; \\frac{\\text{实际下降量}}{\\text{预测下降量}}。\n$$\n- 一个经过充分测试的基本接受准则是，如果 $\\rho_k \\ge \\eta$，则接受步长 $s_k$，其中 $\\eta \\in (0,1)$。\n- 标准的信赖域半径更新使用参数 $\\gamma_{\\text{dec}} \\in (0,1)$ 和 $\\gamma_{\\text{inc}} > 1$，并可选地使用第二个阈值 $\\eta_{\\text{inc}} \\in (\\eta,1)$ 来决定何时扩大信赖域。\n\n您的任务：\n1) 使用截断共轭梯度法（Steihaug型）实现一个信赖域方法来近似求解子问题。该方法利用局部曲率信息构建搜索方向，并在遇到负曲率或触及信赖域边界时停止，无需显式地对 $B_k$ 求逆。\n2) 使用参数 $\\eta$、$\\eta_{\\text{inc}}$、$\\gamma_{\\text{dec}}$、$\\gamma_{\\text{inc}}$ 实现步长接受和信赖域半径更新规则，逻辑如下：\n   - 如果 $\\rho_k  \\eta$，拒绝该步长，并设置 $\\Delta_{k+1} = \\max(\\gamma_{\\text{dec}} \\Delta_k, \\Delta_{\\min})$。\n   - 如果 $\\rho_k \\ge \\eta$，接受该步长。如果此外还有 $\\rho_k > \\eta_{\\text{inc}}$ 且 $\\|s_k\\|_2$ 位于边界上（即在数值容差范围内 $\\|s_k\\|_2$ 等于 $\\Delta_k$），则设置 $\\Delta_{k+1} = \\min(\\gamma_{\\text{inc}} \\Delta_k, \\Delta_{\\max})$。否则，保持 $\\Delta_{k+1} = \\Delta_k$。\n   - 使用固定边界 $\\Delta_{\\min}$ 和 $\\Delta_{\\max}$，满足 $0  \\Delta_{\\min} \\ll 1 \\ll \\Delta_{\\max}$。\n3) 通过追踪在梯度范数仍然很大时 $\\Delta_k$ 是否在许多连续迭代中变得非常小，来诊断 $\\Delta_k$ 的振荡和过早崩溃。具体来说，定义：\n   - 一个小半径阈值 $\\Delta_{\\text{small}}$，\n   - 一个关于梯度的“远离驻点”阈值 $\\tau_g$，\n   - 一个连续迭代窗口长度 $M$，\n   并且，如果存在一个长度为 $M$ 的索引窗口（忽略最初 $W$ 次迭代的预热期），在该窗口的每次迭代中同时满足 $\\Delta_k  \\Delta_{\\text{small}}$ 和 $\\|g_k\\|_2 > \\tau_g$，则声明发生了“远离驻点的过早崩溃”。\n4) 为了进行数值评估，使用初始点 $x_0 = (-1.2, 1.0)^\\top$，最大迭代次数上限 $K_{\\max}$，以及以下停止和评估阈值：\n   - 用于通过 $\\|g_k\\|_2 \\le \\varepsilon_g$ 声明收敛的驻点容差 $\\varepsilon_g$，\n   - 用于断言函数值有明显下降的目标函数阈值 $f_\\text{target}$，\n   - 如上所述的参数 $\\Delta_{\\text{small}}$、$\\tau_g$、$M$、$W$。\n\n测试套件：\n提供以下五个测试用例，每个用例都是一个包含参数 $(\\eta, \\eta_{\\text{inc}}, \\gamma_{\\text{dec}}, \\gamma_{\\text{inc}}, \\Delta_0, \\Delta_{\\max}, K_{\\max})$ 的元组：\n- 用例 A（朴素、激进收缩、严格接受）：$(0.95, 0.98, 0.1, 4.0, 1.0, 100.0, 1000)$。\n- 用例 B（调优、保守接受、适度更新）：$(0.1, 0.75, 0.5, 2.0, 1.0, 100.0, 1000)$。\n- 用例 C（调优、大初始半径）：$(0.1, 0.75, 0.5, 2.0, 10.0, 100.0, 1000)$。\n- 用例 D（调优、极小初始半径）：$(0.1, 0.75, 0.5, 2.0, 10^{-4}, 100.0, 1000)$。\n- 用例 E（朴素、略不那么激进但仍严格）：$(0.9, 0.9, 0.2, 3.0, 1.0, 100.0, 1000)$。\n\n所有用例中使用的固定评估阈值：\n- $\\Delta_{\\min} = 10^{-12}$,\n- $\\Delta_{\\text{small}} = 10^{-8}$,\n- $\\tau_g = 10^{-2}$,\n- $M = 15$,\n- $W = 20$,\n- $\\varepsilon_g = 10^{-5}$,\n- $f_\\text{target} = 10^{-4}$.\n\n您的程序应为每个测试用例运行信赖域方法，并返回一个包含三个整数的列表，这些整数将布尔值编码为 $1$ (true) 和 $0$ (false)：\n- $b_1$：是否取得了显著进展，定义为 $f(x_{\\text{final}}) \\le f_\\text{target}$，\n- $b_2$：是否发生了远离驻点的过早崩溃（如第 $3$ 项所定义），\n- $b_3$：是否收敛到驻点，定义为 $\\|g_{\\text{final}}\\|_2 \\le \\varepsilon_g$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有五个用例的结果，格式为一个由方括号括起来的逗号分隔列表，其中每个元素本身是一个列表 $[b_1,b_2,b_3]$。例如：$[[1,0,1],[\\dots],\\dots]$。不应打印任何额外文本。由于不涉及物理单位、角度或百分比，因此无需进行单位转换。以自包含的方式实现所有内容，无需外部输入文件或用户交互。", "solution": "该问题要求实现并分析一个应用于非凸Rosenbrock函数的信赖域优化算法。分析的重点是信赖域半径的行为，特别是诊断导致其过早崩溃的条件。解决方案将通过以下步骤展开：首先定义目标函数及其导数，然后详细说明信赖域方法的组成部分，包括子问题求解器和半径更新逻辑，最后解释评估标准。\n\n目标函数是双变量Rosenbrock函数，定义为：\n$$\nf(x, y) = 100(y - x^2)^2 + (1 - x)^2\n$$\n令变量向量为 $x = (x^{(1)}, x^{(2)})^\\top$。则函数为 $f(x^{(1)}, x^{(2)}) = 100(x^{(2)} - (x^{(1)})^2)^2 + (1 - x^{(1)})^2$。该函数是优化算法的标准基准测试函数，因为它有一个狭窄的弯曲谷，通向全局最小值点 $(1, 1)^\\top$，在该点处 $f(1, 1) = 0$。\n\n对于二阶信赖域方法，我们需要梯度向量 $g(x) = \\nabla f(x)$ 和Hessian矩阵 $B(x) = \\nabla^2 f(x)$。\n偏导数是：\n$$\n\\frac{\\partial f}{\\partial x^{(1)}} = 200(y - x^2)(-2x) - 2(1 - x) = -400x(y - x^2) - 2(1 - x)\n$$\n$$\n\\frac{\\partial f}{\\partial x^{(2)}} = 200(y - x^2)\n$$\n因此，梯度为：\n$$\ng(x, y) = \\begin{pmatrix} -400x(y - x^2) - 2(1 - x) \\\\ 200(y - x^2) \\end{pmatrix}\n$$\n二阶偏导数是：\n$$\n\\frac{\\partial^2 f}{(\\partial x^{(1)})^2} = -400(y - x^2) - 400x(-2x) + 2 = -400y + 1200x^2 + 2\n$$\n$$\n\\frac{\\partial^2 f}{(\\partial x^{(2)})^2} = 200\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x^{(2)} \\partial x^{(1)}} = \\frac{\\partial^2 f}{\\partial x^{(1)} \\partial x^{(2)}} = -400x\n$$\n这给出了Hessian矩阵：\n$$\nB(x, y) = \\begin{pmatrix} 1200x^2 - 400y + 2  -400x \\\\ -400x  200 \\end{pmatrix}\n$$\n信赖域方法通过迭代来寻求最小化目标函数。在每次迭代 $k$ 中，从点 $x_k$ 开始，我们在 $x_k$ 周围构建函数 $f$ 的一个二次模型 $m_k(s)$：\n$$\nm_k(s) = f(x_k) + g_k^\\top s + \\frac{1}{2} s^\\top B_k s\n$$\n其中 $g_k = g(x_k)$ 且 $B_k = B(x_k)$。然后我们找到一个步长 $s_k$，它近似地解决了信赖域子问题：\n$$\n\\min_{s \\in \\mathbb{R}^2} m_k(s) \\quad \\text{约束条件为} \\quad \\|s\\|_2 \\le \\Delta_k\n$$\n这里，$\\Delta_k > 0$ 是信赖域半径，它定义了 $x_k$ 周围的一个区域，我们“相信”在该区域内模型是 $f$ 的一个良好近似。\n\n为求解此子问题，我们使用截断共轭梯度（CG）法，通常称为Steihaug-Toint算法。这种迭代方法非常适用，因为它可以处理不定的Hessian矩阵 $B_k$，并能自然地结合信赖域边界。该方法应用CG来求解线性系统 $B_k s = -g_k$，但有两个关键的修改：\n1.  **负曲率：** 如果遇到一个方向 $d_j$ 使得 $d_j^\\top B_k d_j \\le 0$，则二次模型 $m_k(s)$ 在该方向上不是凸的。CG过程将终止，步长 $s_k$ 通过从当前CG迭代点沿 $d_j$ 方向移动直到触及信赖域边界 $\\|s\\|_2 = \\Delta_k$ 来确定。\n2.  **边界相交：** 如果一个CG步长会导致迭代点 $s_{j+1}$ 超出信赖域（即 $\\|s_{j+1}\\|_2 > \\Delta_k$），则该步长被截断，使其正好落在边界上。过程终止。\n\n在这两种触及边界的情况下，我们通过求解二次方程 $\\|s_j + \\tau d_j\\|_2^2 = \\Delta_k^2$ 来找到一个标量 $\\tau > 0$，并取其正根。最终步长则为 $s_k = s_j + \\tau d_j$。\n\n一旦找到子问题的近似解 $s_k$，我们就通过比较目标函数的实际下降量与模型预测的下降量来评估其质量。比率 $\\rho_k$ 定义为：\n$$\n\\rho_k = \\frac{\\text{实际下降量}}{\\text{预测下降量}} = \\frac{f(x_k) - f(x_k + s_k)}{m_k(0) - m_k(s_k)} = \\frac{f(x_k) - f(x_k + s_k)}{-g_k^\\top s_k - \\frac{1}{2} s_k^\\top B_k s_k}\n$$\n对于一个有效的步长，预测下降量应为正。如果不是，则模型很差，我们通过将 $\\rho_k$ 设置为一个能确保步长被拒绝的值（例如 $\\rho_k=0$）来处理这种情况。根据 $\\rho_k$ 的值，我们接受或拒绝该步长，并更新信赖域半径：\n- 如果 $\\rho_k  \\eta$：模型拟合不佳。拒绝该步长（$x_{k+1} = x_k$），并收缩信赖域：$\\Delta_{k+1} = \\max(\\gamma_{\\text{dec}} \\Delta_k, \\Delta_{\\min})$。\n- 如果 $\\rho_k \\ge \\eta$：模型足够好。接受该步长：$x_{k+1} = x_k + s_k$。然后调整半径。如果模型契合度非常好（$\\rho_k > \\eta_{\\text{inc}}$）且步长受边界约束（$\\|s_k\\|_2$ 接近 $\\Delta_k$），我们扩大信赖域：$\\Delta_{k+1} = \\min(\\gamma_{\\text{inc}} \\Delta_k, \\Delta_{\\max})$。否则，半径保持不变：$\\Delta_{k+1} = \\Delta_k$。\n\n整个算法通过迭代这些步骤进行，直到梯度范数 $\\|g_k\\|_2$ 小于容差 $\\varepsilon_g$，或达到最大迭代次数 $K_{\\max}$。\n\n分析涉及诊断“远离驻点的过早崩溃”。当信赖域半径 $\\Delta_k$ 变得非常小，阻碍了进展，而迭代点仍远离驻点（即 $\\|g_k\\|_2$ 很大）时，就会发生这种现象。如果在初始 $W$ 次迭代的预热期后，连续 $M$ 次迭代中，条件 $\\Delta_k  \\Delta_{\\text{small}}$ 和 $\\|g_k\\|_2 > \\tau_g$ 同时成立，则检测到此现象。\n\n最后，对于每个测试用例，我们基于三个标准来评估其性能：\n1.  $b_1$：是否取得了显著进展（$f(x_{\\text{final}}) \\le f_\\text{target}$）。\n2.  $b_2$：是否发生了如上定义的过早崩溃。\n3.  $b_3$：是否收敛到了驻点（$\\|g_{\\text{final}}\\|_2 \\le \\varepsilon_g$）。\n\n参数 $(\\eta, \\eta_{\\text{inc}}, \\gamma_{\\text{dec}}, \\gamma_{\\text{inc}})$ 的选择对性能有关键影响。严格的接受标准（高 $\\eta$）与激进的半径收缩（低 $\\gamma_{\\text{dec}}$）相结合，很容易导致过早崩溃，因为算法可能无法在Rosenbrock函数的狭窄谷中前进。相反，更宽松和保守的参数通常会带来稳健的收敛。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rosenbrock(x):\n    \"\"\"Computes the Rosenbrock function value.\"\"\"\n    return 100.0 * (x[1] - x[0]**2)**2 + (1.0 - x[0])**2\n\ndef rosenbrock_grad(x):\n    \"\"\"Computes the gradient of the Rosenbrock function.\"\"\"\n    grad = np.zeros(2)\n    grad[0] = -400.0 * x[0] * (x[1] - x[0]**2) - 2.0 * (1.0 - x[0])\n    grad[1] = 200.0 * (x[1] - x[0]**2)\n    return grad\n\ndef rosenbrock_hessian(x):\n    \"\"\"Computes the Hessian of the Rosenbrock function.\"\"\"\n    hess = np.zeros((2, 2))\n    hess[0, 0] = 1200.0 * x[0]**2 - 400.0 * x[1] + 2.0\n    hess[0, 1] = -400.0 * x[0]\n    hess[1, 0] = -400.0 * x[0]\n    hess[1, 1] = 200.0\n    return hess\n\ndef truncated_cg(g, B, delta):\n    \"\"\"\n    Solves the trust-region subproblem using the truncated conjugate-gradient\n    (Steihaug-Toint) method.\n    \"\"\"\n    s = np.zeros_like(g)\n    r = g.copy()\n    d = -r.copy()\n    hit_boundary = False\n\n    if np.linalg.norm(r) == 0:\n        return s, hit_boundary\n\n    max_cg_iter = len(g)\n    for j in range(max_cg_iter):\n        dBd = d.T @ B @ d\n\n        if dBd = 0:\n            # Negative curvature detected. Find tau to hit the boundary.\n            a = d.T @ d\n            b = 2 * (s.T @ d)\n            c = s.T @ s - delta**2\n            # We want the positive root of a*tau^2 + b*tau + c = 0\n            tau = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s += tau * d\n            hit_boundary = True\n            break\n        \n        alpha = (r.T @ r) / dBd\n        s_new = s + alpha * d\n\n        if np.linalg.norm(s_new) >= delta:\n            # Step hits or exceeds boundary. Find tau to be exactly on boundary.\n            a = d.T @ d\n            b = 2 * (s.T @ d)\n            c = s.T @ s - delta**2\n            tau = (-b + np.sqrt(b**2 - 4 * a * c)) / (2 * a)\n            s += tau * d\n            hit_boundary = True\n            break\n\n        s = s_new\n        r_new = r + alpha * (B @ d)\n        \n        # CG convergence check\n        if np.linalg.norm(r_new)  1e-6 * np.linalg.norm(g):\n            break\n\n        beta = (r_new.T @ r_new) / (r.T @ r)\n        r = r_new\n        d = -r + beta * d\n        \n    return s, hit_boundary\n\ndef run_trust_region(params, fixed_params):\n    \"\"\"Runs the trust-region algorithm for a given set of parameters.\"\"\"\n    eta, eta_inc, gamma_dec, gamma_inc, delta_0, delta_max, k_max = params\n    delta_min = fixed_params['delta_min']\n    delta_small = fixed_params['delta_small']\n    tau_g = fixed_params['tau_g']\n    M = fixed_params['M']\n    W = fixed_params['W']\n    eps_g = fixed_params['eps_g']\n    f_target = fixed_params['f_target']\n\n    x = np.array([-1.2, 1.0])\n    delta = delta_0\n\n    delta_history = []\n    gnorm_history = []\n\n    for k in range(k_max):\n        f_k = rosenbrock(x)\n        g_k = rosenbrock_grad(x)\n        B_k = rosenbrock_hessian(x)\n        \n        g_norm = np.linalg.norm(g_k)\n        delta_history.append(delta)\n        gnorm_history.append(g_norm)\n\n        if g_norm = eps_g:\n            break\n\n        s_k, hit_boundary = truncated_cg(g_k, B_k, delta)\n\n        pred_reduction = -(g_k.T @ s_k + 0.5 * s_k.T @ B_k @ s_k)\n        \n        x_new = x + s_k\n        actual_reduction = f_k - rosenbrock(x_new)\n\n        if pred_reduction = 0:\n            rho_k = -1.0 # Will force rejection\n        else:\n            rho_k = actual_reduction / pred_reduction\n\n        if rho_k  eta:\n            # Reject step, shrink radius\n            delta = max(gamma_dec * delta, delta_min)\n        else:\n            # Accept step\n            x = x_new\n            # Update radius\n            if rho_k > eta_inc and hit_boundary:\n                delta = min(gamma_inc * delta, delta_max)\n            # else delta remains the same\n\n    x_final = x\n    f_final = rosenbrock(x_final)\n    g_final_norm = np.linalg.norm(rosenbrock_grad(x_final))\n\n    # Assessment\n    b1 = 1 if f_final = f_target else 0\n    b3 = 1 if g_final_norm = eps_g else 0\n\n    premature_collapse = False\n    collapse_counter = 0\n    if len(delta_history) > W + M:\n        for i in range(W, len(delta_history)):\n            if delta_history[i]  delta_small and gnorm_history[i] > tau_g:\n                collapse_counter += 1\n            else:\n                collapse_counter = 0\n            \n            if collapse_counter >= M:\n                premature_collapse = True\n                break\n    b2 = 1 if premature_collapse else 0\n    \n    return [b1, b2, b3]\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (eta, eta_inc, gamma_dec, gamma_inc, delta_0, delta_max, k_max)\n        (0.95, 0.98, 0.1, 4.0, 1.0, 100.0, 1000), # Case A\n        (0.1, 0.75, 0.5, 2.0, 1.0, 100.0, 1000),  # Case B\n        (0.1, 0.75, 0.5, 2.0, 10.0, 100.0, 1000), # Case C\n        (0.1, 0.75, 0.5, 2.0, 1e-4, 100.0, 1000), # Case D\n        (0.9, 0.9, 0.2, 3.0, 1.0, 100.0, 1000),   # Case E\n    ]\n\n    fixed_params = {\n        'delta_min': 1e-12,\n        'delta_small': 1e-8,\n        'tau_g': 1e-2,\n        'M': 15,\n        'W': 20,\n        'eps_g': 1e-5,\n        'f_target': 1e-4,\n    }\n\n    results = []\n    for case in test_cases:\n        result = run_trust_region(case, fixed_params)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3193957"}, {"introduction": "最后，我们来解决一个特定但常见的算法失败模式：在目标函数的平坦区域停滞不前。标准的更新规则通常依赖于较大的预测下降量或触及边界的步长来扩大信赖域，因此在平坦区域可能失效。这项练习 [@problem_id:3193978] 将挑战您设计并实现一种更“智能”的更新规则，它能够识别这种停滞情况，并主动增加信赖域半径，从而帮助算法快速逃离“高原区”。", "problem": "要求您设计并实现一个程序，用于研究在具有近乎平坦脊线的二阶连续可微目标函数上，信赖域半径的更新规则，并提出一种有原则的修正方法，在模型下降量很小但模型高度可靠的情况下，也能增大信赖域半径。核心重点在于，当模型与真实目标函数吻合度很高但步长仍然很小时，如何、为何以及何时更新信赖域半径，以确保算法能从平坦区域收敛出来。\n\n使用的基本方法是标准的无约束优化信赖域方法。在迭代点 $x_k \\in \\mathbb{R}^n$ 处，二次模型为\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\tfrac{1}{2} p^\\top B_k p,\n$$\n其中 $g_k = \\nabla f(x_k)$，$B_k = \\nabla^2 f(x_k)$ 是 Hessian 矩阵。信赖域子问题旨在求解\n$$\n\\min_{p \\in \\mathbb{R}^n} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\| \\le \\Delta_k,\n$$\n其中 $\\Delta_k > 0$ 是当前的信赖域半径。候选步 $p_k$ 通过比率\n$$\n\\rho_k = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)},\n$$\n来评估，该比率比较了实际下降量与预测下降量。标准更新规则在 $\\rho_k$ 很小时缩小 $\\Delta_k$，在 $\\rho_k$ 很大且步长位于或接近边界时增大 $\\Delta_k$。\n\n构建一个具有近乎平坦脊线的光滑目标函数：\n$$\nf(x_1, x_2) = \\sqrt{\\varepsilon + x_1^2} + \\tfrac{\\kappa}{2} x_2^2,\n$$\n其中 $\\varepsilon > 0$ 和 $\\kappa > 0$ 为固定的正标量。其梯度和 Hessian 矩阵为\n$$\ng(x) = \\begin{bmatrix} \\dfrac{x_1}{\\sqrt{\\varepsilon + x_1^2}} \\\\ \\kappa x_2 \\end{bmatrix}, \\qquad\nB(x) = \\begin{bmatrix} \\dfrac{\\varepsilon}{(\\varepsilon + x_1^2)^{3/2}}  0 \\\\ 0  \\kappa \\end{bmatrix}.\n$$\n注意，在 $x_1 \\approx 0$ 附近的脊线上，$x_1$ 方向的曲率很大，而函数值沿 $x_1$ 变化非常缓慢，并且分量 $g_1$ 可能非常小；这可能导致步长 $p_k$ 满足 $\\rho_k \\approx 1$ 但 $\\|p_k\\|$ 非常小，因此标准规则可能不会增大 $\\Delta_k$。\n\n您的任务：\n- 使用拉格朗日乘子最优性条件，为维度 $n=2$ 的信赖域子问题实现一个精确求解器：找到 $\\lambda \\ge 0$ 使得\n$$\n(B(x_k) + \\lambda I) p_k = -g(x_k), \\quad \\text{with} \\quad \\|p_k\\| \\le \\Delta_k,\n$$\n并且如果当 $\\lambda=0$ 时 $\\|p_k\\| > \\Delta_k$，则增加 $\\lambda$ 直到 $\\|p_k\\| = \\Delta_k$（对 $\\lambda$ 使用稳健的二分法）。\n- 计算实际下降量 $f(x_k) - f(x_k + p_k)$ 和预测下降量 $m_k(0) - m_k(p_k) = -g_k^\\top p_k - \\tfrac{1}{2} p_k^\\top B_k p_k$，然后计算 $\\rho_k$。\n- 实现标准的信赖域半径更新决策规则：\n  - 如果 $\\rho_k  0.25$，将决策标记为“缩小”（-1）。\n  - 否则，如果 $\\rho_k > 0.75$ 且 $\\|p_k\\| \\ge 0.8 \\Delta_k$，标记为“增大”（+1）。\n  - 否则标记为“不变”（0）。\n- 提出并实现一个感知停滞的扩张规则，以便在模型下降量很小的情况下也能逃离平坦区域：\n  - 如果 $\\rho_k \\ge \\eta_{\\text{high}}$ 且 $\\|p_k\\| \\le \\chi \\Delta_k$ 且 $m_k(0) - m_k(p_k) \\le c_{\\text{pred}} \\|g_k\\| \\Delta_k$ 且 $\\|g_k\\| > \\tau_g$，标记为“增大”（+1）。\n  - 否则，如果 $\\rho_k  0.25$，将决策标记为“缩小”（-1）。\n  - 否则，如果 $\\rho_k > 0.75$ 且 $\\|p_k\\| \\ge 0.8 \\Delta_k$，标记为“增大”（+1）。\n  - 否则标记为“不变”（0）。\n此处的 $\\eta_{\\text{high}}$、$\\chi$、$c_{\\text{pred}}$ 和 $\\tau_g$ 是固定的阈值，您必须选择在此尺度下合理的值；使用 $\\eta_{\\text{high}} = 0.9$、$\\chi = 0.2$、$c_{\\text{pred}} = 0.2$ 和 $\\tau_g = 10^{-8}$。\n\n测试套件规范：\n使用 $\\varepsilon = 10^{-6}$ 和 $\\kappa = 20$。对于以下每种情况，程序必须计算 $p_k$、$\\rho_k$，并返回决策对 $[d_{\\text{std}}, d_{\\text{prop}}]$，其中 $d_{\\text{std}}, d_{\\text{prop}} \\in \\{-1, 0, +1\\}$：\n1. $x_k = (10^{-8}, 10^{-3})$, $\\Delta_k = 10^{-2}$：近乎平坦的脊线，具有微小的内部步长且 $\\rho_k \\approx 1$；感知停滞的规则应增大半径，而标准规则不应。\n2. $x_k = (10^{-1}, 0)$, $\\Delta_k = 5 \\cdot 10^{-2}$：在边界附近模型吻合度好；两个规则都应增大半径。\n3. $x_k = (2, 0)$, $\\Delta_k = 10$：模型吻合度差，候选步长非常大；两个规则都应缩小半径。\n4. $x_k = (0, 0)$, $\\Delta_k = 10^{-1}$：梯度为零的边界情况；两个规则都应返回不变。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含测试套件的结果，格式为一个逗号分隔的 Python 风格列表的列表，按顺序包含每种情况的决策，例如 $[[d_1^{\\text{std}}, d_1^{\\text{prop}}],[d_2^{\\text{std}}, d_2^{\\text{prop}}],\\dots]$。本问题不涉及任何物理单位或角度单位。输出中的所有数值答案都是 $\\{-1,0,1\\}$ 中的整数，聚合输出是这些整数对的列表。", "solution": "用户提出了一个有效的数值优化问题。任务是针对一个已知会导致缓慢收敛的特定目标函数，分析和比较两种不同的信赖域半径更新策略。该问题具有科学依据，是适定的，并包含获得唯一、可验证解所需的所有信息。\n\n该问题围绕使用信赖域方法对一个二阶连续可微目标函数 $f: \\mathbb{R}^2 \\to \\mathbb{R}$ 进行无约束优化。特定函数被设计为具有近乎平坦的脊线，这对标准算法构成了挑战。该函数由下式给出：\n$$\nf(x_1, x_2) = \\sqrt{\\varepsilon + x_1^2} + \\frac{\\kappa}{2} x_2^2\n$$\n其中 $\\varepsilon > 0$ 和 $\\kappa > 0$ 是固定的正标量。在我们的测试中，$\\varepsilon = 10^{-6}$ 且 $\\kappa = 20$。梯度 $g(x) = \\nabla f(x)$ 和 Hessian 矩阵 $B(x) = \\nabla^2 f(x)$ 分别为：\n$$\ng(x) = \\begin{bmatrix} \\frac{x_1}{\\sqrt{\\varepsilon + x_1^2}} \\\\ \\kappa x_2 \\end{bmatrix}, \\qquad\nB(x) = \\begin{bmatrix} \\frac{\\varepsilon}{(\\varepsilon + x_1^2)^{3/2}}  0 \\\\ 0  \\kappa \\end{bmatrix}\n$$\n在每次迭代 $k$ 中，给定一个点 $x_k$，我们构建目标函数的一个二次模型：\n$$\nm_k(p) = f(x_k) + g_k^\\top p + \\frac{1}{2} p^\\top B_k p\n$$\n其中 $g_k = g(x_k)$ 且 $B_k = B(x_k)$。信赖域方法的核心是找到一个步长 $p_k$，近似求解信赖域子问题：\n$$\n\\min_{p \\in \\mathbb{R}^2} \\; m_k(p) \\quad \\text{subject to} \\quad \\|p\\|_2 \\le \\Delta_k\n$$\n其中 $\\Delta_k > 0$ 是信赖域半径。\n\n为了求解这个子问题，我们使用 Karush-Kuhn-Tucker (KKT) 条件。一个解 $p_k$ 必须满足：\n$$\n(B_k + \\lambda I) p_k = -g_k\n$$\n对于某个拉格朗日乘子 $\\lambda \\ge 0$。这里，$I$ 是 $2 \\times 2$ 的单位矩阵。条件还要求 $\\lambda(\\Delta_k - \\|p_k\\|_2) = 0$。由于 $\\varepsilon > 0$ 和 $\\kappa > 0$，Hessian 矩阵 $B_k$ 是一个对角线上元素为正的对角矩阵，这意味着它是正定的。这显著简化了子问题的求解。找到 $p_k$ 的算法如下：\n1. 通过设置 $\\lambda=0$ 来计算完全牛顿步（$m_k(p)$ 的无约束最小化子）：$p_k(0) = -B_k^{-1} g_k$。\n2. 如果 $\\|p_k(0)\\|_2 \\le \\Delta_k$，牛顿步位于信赖域内，是该子问题的最优解。因此，$p_k = p_k(0)$。这是内部解。\n3. 如果 $\\|p_k(0)\\|_2 > \\Delta_k$，解必须位于信赖域的边界上，即 $\\|p_k\\|_2 = \\Delta_k$。我们必须找到一个 $\\lambda > 0$ 来满足这个条件。步长作为 $\\lambda$ 的函数是 $p_k(\\lambda) = -(B_k + \\lambda I)^{-1} g_k$。我们必须求解标量非线性久期方程 $\\|p_k(\\lambda)\\|_2 - \\Delta_k = 0$ 来得到 $\\lambda$。由于 $B_k$ 是对角矩阵，其对角元素为 $b_{11}$ 和 $b_{22}$，该方程变为：\n$$\n\\sqrt{\\left(\\frac{-g_1}{b_{11} + \\lambda}\\right)^2 + \\left(\\frac{-g_2}{b_{22} + \\lambda}\\right)^2} = \\Delta_k\n$$\n使用数值求根方法求解这个关于 $\\lambda > 0$ 的方程，特别是二分法，该方法是稳健的，因为函数 $\\|p_k(\\lambda)\\|_2$ 对于 $\\lambda > 0$ 是单调递减的。\n\n一旦计算出步长 $p_k$，其质量就通过目标函数实际下降量与二次模型预测下降量的比率 $\\rho_k$ 来评估：\n$$\n\\rho_k = \\frac{\\text{Ared}}{\\text{Pred}} = \\frac{f(x_k) - f(x_k + p_k)}{m_k(0) - m_k(p_k)}\n$$\n预测下降量由 $m_k(0) - m_k(p_k) = -g_k^\\top p_k - \\frac{1}{2} p_k^\\top B_k p_k$ 给出。如果 $Pred = 0$，这当且仅当 $g_k = 0$ 时发生（因为 $B_k$ 是正定的），那么 $p_k=0$ 且 $Ared=0$。在这种情况下，$\\rho_k$ 是未定义的，信赖域半径应保持不变。\n\n研究的核心在于为下一次迭代更新 $\\Delta_k$ 的规则，即 $\\Delta_{k+1}$。\n标准更新规则基于 $\\rho_k$ 和步长相对于半径的长度：\n- 如果 $\\rho_k  0.25$：模型预测效果差。缩小信赖域。决策是“缩小”（-1）。\n- 否则，如果 $\\rho_k > 0.75$ 且 $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$：模型预测效果好且步长受边界约束。增大信赖域。决策是“增大”（+1）。\n- 否则：步长可接受，但没有充分理由改变半径。决策是“不变”（0）。\n\n所提出的感知停滞的扩张规则旨在处理模型准确（$\\rho_k$ 很高），但步长 $p_k$ 和预测下降量都非常小的情况。这发生在平坦区域。该规则在这些条件下优先进行扩张，以便更快地逃离平坦区域。其逻辑是顺序的：\n1. 如果 $\\rho_k \\ge \\eta_{\\text{high}}$ ($0.9$)、$\\|p_k\\|_2 \\le \\chi \\Delta_k$ ($0.2$)、$Pred \\le c_{\\text{pred}} \\|g_k\\|_2 \\Delta_k$ ($0.2$) 且 $\\|g_k\\|_2 > \\tau_g$ ($10^{-8}$)，则：这些条件识别出一个在一个非驻点区域的微小但高度准确的步长，其中由线性项预测的下降量很小。增大信赖域。决策是“增大”（+1）。\n2. 否则，如果 $\\rho_k  0.25$：缩小信赖域。决策是“缩小”（-1）。\n3. 否则，如果 $\\rho_k > 0.75$ 且 $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$：标准增大条件。决策是“增大”（+1）。\n4. 否则：决策是“不变”（0）。\n\n将此框架应用于测试用例，得到以下分析：\n\n情况 1：$x_k = (10^{-8}, 10^{-3})$, $\\Delta_k = 10^{-2}$。\n在此点，$x_1$ 非常接近 $0$。梯度分量 $g_1$ 非常小（$\\approx 10^{-5}$），而 Hessian 分量 $b_{11}$ 很大（$\\approx 1000$）。得到的牛顿步是一个内部解（$p_k \\approx (-10^{-8}, -10^{-3})$），其范数 $\\|p_k\\|_2 \\approx 10^{-3}$ 非常小。模型高度准确，导致 $\\rho_k \\approx 1$。\n- 标准规则：$\\rho_k > 0.75$，但 $\\|p_k\\|_2 \\approx 10^{-3}$ 远小于 $0.8 \\Delta_k = 8 \\cdot 10^{-3}$。因此，决策是“不变”（0）。\n- 所提规则：停滞条件被触发。$\\rho_k \\approx 1 \\ge 0.9$，$\\|p_k\\|_2 \\approx 10^{-3} \\le 0.2 \\Delta_k = 2 \\cdot 10^{-3}$，预测下降量很小，满足 $c_{pred}$ 准则，且 $\\|g_k\\|_2 \\approx 2 \\cdot 10^{-2} > 10^{-8}$。所有条件都满足，导致“增大”（+1）的决策。这展示了所提规则的优势。\n\n情况 2：$x_k = (10^{-1}, 0)$, $\\Delta_k = 5 \\cdot 10^{-2}$。\n牛顿步非常大，因此解 $p_k$ 位于信赖域边界上，且 $\\|p_k\\|_2 = \\Delta_k$。模型吻合度极好，$\\rho_k \\approx 1$。\n- 两种规则：条件 $\\rho_k > 0.75$ 和 $\\|p_k\\|_2 \\ge 0.8 \\Delta_k$ 得到满足。所提规则的停滞条件不满足，因为 $\\|p_k\\|_2 = \\Delta_k$ 违反了 $\\|p_k\\|_2 \\le \\chi \\Delta_k$。两种规则都正确地决定“增大”（+1）。\n\n情况 3：$x_k = (2, 0)$, $\\Delta_k = 10$。\n远离原点时，函数在 $x_1$ 方向上接近线性，因此二阶模型在长距离上是一个较差的近似。步长 $p_k$ 位于边界上，且 $\\|p_k\\|_2 = 10$。步进后，实际函数值增加，导致实际下降量为负（$Ared  0$），因此 $\\rho_k  0$。\n- 两种规则：由于 $\\rho_k  0.25$，两种规则都正确地决定“缩小”（-1）。\n\n情况 4：$x_k = (0, 0)$, $\\Delta_k = 10^{-1}$。\n这是一个驻点，因为 $g_k = (0,0)$。最优步长是 $p_k = 0$。因此，$Ared=0$ 且 $Pred=0$。在这种情况下，$\\rho_k$ 是未定义的。没有理由改变信赖域半径。\n- 两种规则：逻辑通过默认为“不变”（0）来处理这种情况。所提规则中的特定条件 $\\|g_k\\|_2 > \\tau_g$ 明确防止了在驻点处进行扩张。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root_scalar\n\n# --- Problem Definition ---\n\n# Fixed parameters for the objective function\nEPSILON = 1e-6\nKAPPA = 20.0\n\n# Thresholds for the proposed stagnation-aware rule\nETA_HIGH = 0.9\nCHI = 0.2\nC_PRED = 0.2\nTAU_G = 1e-8\n\ndef f_obj(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Objective function f(x_1, x_2).\"\"\"\n    return np.sqrt(eps + x[0]**2) + (kap / 2.0) * x[1]**2\n\ndef g_grad(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Gradient of the objective function.\"\"\"\n    g1 = x[0] / np.sqrt(eps + x[0]**2)\n    g2 = kap * x[1]\n    return np.array([g1, g2])\n\ndef B_hess(x, eps=EPSILON, kap=KAPPA):\n    \"\"\"Hessian of the objective function.\"\"\"\n    b11 = eps / (eps + x[0]**2)**1.5\n    b22 = kap\n    # The Hessian is diagonal\n    return np.diag([b11, b22])\n\ndef solve_tr_subproblem(gk, Bk, delta_k):\n    \"\"\"\n    Solves the 2D trust-region subproblem min m(p) s.t. ||p|| = delta_k\n    using the KKT conditions and bisection for the boundary case.\n    \"\"\"\n    if np.linalg.norm(gk) == 0:\n        return np.zeros(2)\n\n    # Since B_k is positive definite, we can compute the Newton step.\n    try:\n        p_newton = -np.linalg.solve(Bk, gk)\n    except np.linalg.LinAlgError:\n        # This case is not expected here as Bk is positive definite\n        p_newton = -np.linalg.pinv(Bk) @ gk\n    \n    if np.linalg.norm(p_newton) = delta_k:\n        # Interior solution\n        return p_newton\n\n    # Boundary solution: find lambda > 0 such that ||p(lambda)|| = delta_k\n    b_diag = np.diag(Bk)\n    \n    def secular_eq(lam):\n        # Using the diagonal structure of Bk for p(lambda)\n        p_lam = -gk / (b_diag + lam)\n        return np.linalg.norm(p_lam) - delta_k\n    \n    # Establish a safe search bracket [lambda_low, lambda_high] for bisection.\n    lambda_low = 0.0\n    # An upper bound can be derived from ||g||/delta_k. Start there and increase if needed.\n    lambda_high = np.linalg.norm(gk) / delta_k\n    while secular_eq(lambda_high) > 0:\n        lambda_high *= 2.0\n\n    # Use a robust root-finding method (bisection) to find lambda\n    sol = root_scalar(secular_eq, bracket=[lambda_low, lambda_high], method='bisect')\n    lam_star = sol.root\n    \n    p_star = -gk / (b_diag + lam_star)\n    return p_star\n\ndef compute_decisions(xk, pk, delta_k):\n    \"\"\"\n    Computes rho_k and returns the decisions for both standard and proposed rules.\n    \"\"\"\n    gk = g_grad(xk)\n    Bk = B_hess(xk)\n\n    actual_reduction = f_obj(xk) - f_obj(xk + pk)\n    predicted_reduction = -gk.T @ pk - 0.5 * pk.T @ Bk @ pk\n\n    if abs(predicted_reduction)  1e-15:\n        # If predicted reduction is zero (e.g. g_k=0), p_k=0, no change.\n        return [0, 0]\n\n    rho_k = actual_reduction / predicted_reduction\n    norm_pk = np.linalg.norm(pk)\n    norm_gk = np.linalg.norm(gk)\n\n    # Standard rule\n    d_std = 0\n    if rho_k  0.25:\n        d_std = -1\n    elif rho_k > 0.75 and norm_pk >= 0.8 * delta_k:\n        d_std = 1\n\n    # Proposed stagnation-aware rule\n    d_prop = 0\n    # The conditions must be checked in the specified order (if-elif-else)\n    stagnation_increase = (\n        rho_k >= ETA_HIGH and\n        norm_pk = CHI * delta_k and\n        predicted_reduction = C_PRED * norm_gk * delta_k and\n        norm_gk > TAU_G\n    )\n    \n    if stagnation_increase:\n        d_prop = 1\n    elif rho_k  0.25:\n        d_prop = -1\n    elif rho_k > 0.75 and norm_pk >= 0.8 * delta_k:\n        d_prop = 1\n    \n    return [d_std, d_prop]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute decisions.\n    \"\"\"\n    # Test cases: (x_k_tuple, delta_k)\n    test_cases = [\n        ((1e-8, 1e-3), 1e-2),\n        ((1e-1, 0.0), 5e-2),\n        ((2.0, 0.0), 10.0),\n        ((0.0, 0.0), 1e-1),\n    ]\n\n    results = []\n    for case in test_cases:\n        x_k_tuple, delta_k = case\n        x_k = np.array(x_k_tuple)\n        \n        # Calculate g_k and B_k at the current point\n        g_k = g_grad(x_k)\n        B_k = B_hess(x_k)\n        \n        # Solve the trust-region subproblem to get the step p_k\n        p_k = solve_tr_subproblem(g_k, B_k, delta_k)\n\n        # Compute the decisions based on the two different update rules\n        decisions = compute_decisions(x_k, p_k, delta_k)\n        results.append(decisions)\n\n    # Format the final output string exactly as specified\n    result_strs = [f\"[{r[0]},{r[1]}]\" for r in results]\n    final_output = f\"[{','.join(result_strs)}]\"\n    print(final_output)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3193978"}]}