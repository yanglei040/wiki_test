{"hands_on_practices": [{"introduction": "最速下降法的收敛性能在很大程度上受目标函数Hessian矩阵条件数的制约，病态问题会导致收敛极其缓慢。本练习旨在通过分析构建一个二维二次函数的最坏情况，从第一性原理出发推导出其单步收敛因子，从而深刻理解导致算法呈现“之字形”下降并几乎停滞的几何与代数根源 [@problem_id:3149658]。", "problem": "考虑一个由二次模型定义的二元、二次连续可微目标函数 $$f(x) = \\frac{1}{2} x^{\\top} Q x,$$ 其中 $Q \\in \\mathbb{R}^{2 \\times 2}$ 是对称正定 (SPD) 矩阵。设 $Q$ 的特征分解为 $Q = V \\Lambda V^{\\top}$，其中 $v_{1}$ 和 $v_{2}$ 是标准正交特征向量，对应的特征值为 $0  m  L$，并定义条件数 $$\\kappa = \\frac{L}{m}.$$ 唯一极小值点是 $x^{\\star} = 0$。使用精确线搜索的最速下降法生成迭代序列 $$x_{k+1} = x_{k} - \\alpha_{k} \\nabla f(x_{k}),$$ 其中 $\\alpha_{k}$ 是使 $f(x_{k} - \\alpha \\nabla f(x_{k}))$ 在实数 $\\alpha$ 上最小化的值。\n\n你的任务是：\n- 仅使用基本定义（梯度、精确线搜索、SPD 矩阵及其特征分解的性质），通过选择系数 $a$ 和 $b$ 构造一个最坏情况下的初始迭代点 $x_{0}$，使得 $$x_{0} = a v_{1} + b v_{2}$$ 在精确线搜索的最速下降法下导致能量 $f(x)$ 的单步下降最慢。从几何上解释这个构造，说明为什么对于大的 $\\kappa$，负梯度方向 $-\\nabla f(x_{0})$ 会变得几乎与朝向 $x^{\\star}$ 的方向正交，从而导致“之”字形和停滞模式。\n- 从第一性原理出发，推导以 $Q$-范数度量的平方误差的最坏情况单步缩减因子，$$\\frac{\\|x_{1}\\|_{Q}^{2}}{\\|x_{0}\\|_{Q}^{2}}, \\quad \\text{其中 } \\|x\\|_{Q}^{2} = x^{\\top} Q x,$$ 并将其仅表示为 $\\kappa$ 的函数。\n\n以 $\\kappa$ 的单个闭式解析表达式形式给出最终答案。不需要进行数值舍入。", "solution": "该问题要求为二维二次目标函数上的最速下降法构造一个最坏情况下的初始迭代点，并推导相应的单步误差缩减因子。\n\n设目标函数为 $f(x) = \\frac{1}{2} x^{\\top} Q x$，其中 $x \\in \\mathbb{R}^{2}$，Q 是一个对称正定 (SPD) 矩阵。其梯度为 $\\nabla f(x) = Qx$。唯一极小值点是 $x^{\\star} = 0$。矩阵 $Q$ 有标准正交特征向量 $v_1$ 和 $v_2$，对应的特征值为 $0  m  L$。我们假设 $Qv_1 = mv_1$ 和 $Qv_2 = Lv_2$。任何向量 $x \\in \\mathbb{R}^2$ 都可以用这个特征基表示为 $x = c_1 v_1 + c_2 v_2$。\n\n最速下降法的迭代公式为 $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$，其中 $\\nabla f(x_k) = g_k = Qx_k$。步长 $\\alpha_k$ 通过精确线搜索来选择，以最小化 $\\phi(\\alpha) = f(x_k - \\alpha g_k)$。\n$$ \\phi(\\alpha) = \\frac{1}{2} (x_k - \\alpha g_k)^{\\top} Q (x_k - \\alpha g_k) = \\frac{1}{2} (x_k^{\\top}Qx_k - 2\\alpha g_k^{\\top}Qx_k + \\alpha^2 g_k^{\\top}Qg_k) $$\n为了找到最小值，我们将关于 $\\alpha$ 的导数设为零：\n$$ \\frac{d\\phi}{d\\alpha} = -g_k^{\\top}Qx_k + \\alpha g_k^{\\top}Qg_k = 0 $$\n由于 $g_k = Qx_k$，这给出了最优步长：\n$$ \\alpha_k = \\frac{g_k^{\\top}Qx_k}{g_k^{\\top}Qg_k} = \\frac{g_k^{\\top}g_k}{g_k^{\\top}Qg_k} $$\n误差以 $Q$-范数度量，定义为 $\\|x\\|_Q^2 = x^{\\top}Qx = 2f(x)$。此误差度量的单步缩减为：\n$$ \\|x_{k+1}\\|_Q^2 = (x_k - \\alpha_k g_k)^{\\top} Q (x_k - \\alpha_k g_k) = \\|x_k\\|_Q^2 - 2\\alpha_k g_k^{\\top}Qx_k + \\alpha_k^2 g_k^{\\top}Qg_k $$\n代入 $\\alpha_k = \\frac{g_k^{\\top}g_k}{g_k^{\\top}Qg_k}$ 和 $g_k=Qx_k$：\n$$ \\|x_{k+1}\\|_Q^2 = \\|x_k\\|_Q^2 - 2\\frac{(g_k^{\\top}g_k)}{(g_k^{\\top}Qg_k)}g_k^{\\top}g_k + \\left(\\frac{g_k^{\\top}g_k}{g_k^{\\top}Qg_k}\\right)^2 g_k^{\\top}Qg_k = \\|x_k\\|_Q^2 - \\frac{(g_k^{\\top}g_k)^2}{g_k^{\\top}Qg_k} $$\n因此，缩减因子为：\n$$ \\frac{\\|x_{k+1}\\|_Q^2}{\\|x_k\\|_Q^2} = 1 - \\frac{(g_k^{\\top}g_k)^2}{\\|x_k\\|_Q^2 (g_k^{\\top}Qg_k)} = 1 - \\frac{(x_k^{\\top}Q^2x_k)^2}{(x_k^{\\top}Qx_k)(x_k^{\\top}Q^3x_k)} $$\n为了找到最坏情况的单步缩减，我们必须最大化这个比率，这等同于最小化被减去的项。设初始迭代点为 $x_0 = a v_1 + b v_2$，其中 $a, b$ 是不全为零的实系数。\n使用特征分解，我们可以将这些项表示为：\n$x_0^{\\top}Q^p x_0 = (av_1+bv_2)^{\\top}Q^p(av_1+bv_2) = (av_1+bv_2)^{\\top}(a m^p v_1 + b L^p v_2) = a^2 m^p + b^2 L^p$。\n需要最小化的项是：\n$$ H(a,b) = \\frac{(a^2 m^2 + b^2 L^2)^2}{(a^2 m + b^2 L)(a^2 m^3 + b^2 L^3)} $$\n由于该表达式在 $a^2$ 和 $b^2$ 上是齐次的，其值仅取决于比率 $t = b^2/a^2$ (对于 $a \\neq 0$)。我们来分析这个函数：\n$$ H(t) = \\frac{(m^2 + t L^2)^2}{(m + t L)(m^3 + t L^3)} $$\n为了找到最小值，我们将其关于 $t$ 的导数设为零。使用对数导数会更简单：$\\frac{d}{dt} \\ln H(t) = 0$。\n$$ \\frac{2L^2}{m^2+tL^2} - \\frac{L}{m+tL} - \\frac{L^3}{m^3+tL^3} = 0 $$\n乘以公分母并关注分子，得到：\n$$ 2L^2(m+tL)(m^3+tL^3) - L(m^2+tL^2)(m^3+tL^3) - L^3(m^2+tL^2)(m+tL) = 0 $$\n展开并根据 $t$ 的幂次合并同类项：\n$t^2$ 的系数是 $2L^2(L \\cdot L^3) - L(L^2 \\cdot L^3) - L^3(L^2 \\cdot L) = 2L^6 - L^6 - L^6 = 0$。\n该方程在 $t$ 上是线性的。常数项 (t=0) 是 $2L^2(m \\cdot m^3) - L(m^2 \\cdot m^3) - L^3(m^2 \\cdot m) = 2L^2m^4 - Lm^5 - L^3m^3 = -Lm^3(L-m)^2$。\n$t$ 的系数是 $2L^2(m L^3 + L m^3) - L(m^2 L^3 + L^2 m^3) - L^3(m^2 L + L^2 m) = (2L^5m + 2L^3m^3) - (L^4m^2 + L^3m^3) - (L^5m + L^4m^2) = L^5m + L^3m^3 - 2L^4m^2 = L^3m(L^2+m^2-2Lm)=L^3m(L-m)^2$。\n方程变为：$-Lm^3(L-m)^2 + t L^3m(L-m)^2 = 0$。\n假设 $L \\ne m$，我们可以除以 $Lm(L-m)^2$，得到 $-m^2 + tL^2 = 0$，所以 $t = m^2/L^2$。\n因此，当 $b^2/a^2 = m^2/L^2$ 或 $|b/a| = m/L$ 时，发生最慢的单步下降。\n最坏情况下的初始迭代点 $x_0$ 是形如 $x_0 = c(v_1 \\pm \\frac{m}{L} v_2)$ 或 $x_0=c(\\frac{L}{m}v_1 \\pm v_2)$ 的任意向量，其中 $c$ 为任意非零标量。为简单起见，我们可以选择 $x_0 = L v_1 + m v_2$。\n\n对于几何解释，考虑一个大的条件数 $\\kappa = L/m \\gg 1$。$f(x)$ 的水平集是由 $m c_1^2 + L c_2^2 = \\text{const}$ 定义的椭圆，这些椭圆沿着 $v_1$ 轴（“慢”方向）被高度拉长。最坏情况的起始点 $x_0 = L v_1 + m v_2$ 在慢方向 $v_1$ 上有很大的分量，而在快方向 $v_2$ 上有很小的分量。朝向极小值点 $x^\\star=0$ 的方向是 $-x_0 = -L v_1 - m v_2$。对于大的 $\\kappa$，这个方向几乎与 $-v_1$ 平行。\n梯度是 $g_0 = Qx_0 = Q(Lv_1+mv_2) = Lmv_1 + Lmv_2 = Lm(v_1+v_2)$。\n最速下降方向是 $-g_0 = -Lm(v_1+v_2)$。这个方向与 $-v_1$ 和 $-v_2$ 都成 $45^\\circ$ 角。\n对于大的 $\\kappa$，到极小值点的方向 ($-x_0$) 几乎是 $-v_1$，而搜索方向 ($-g_0$) 是 $- (v_1+v_2)$。这两个方向之间的夹角是 $\\arccos\\left(\\frac{L+m}{\\sqrt{2(L^2+m^2)}}\\right)$，当 $\\kappa \\to \\infty$ 时，该角度趋近于 $45^\\circ$。搜索方向与通往极小值点的最优路径之间的这种显著不匹配，导致算法迈出的一步会越过“谷底”（$v_1$-轴）。下一个迭代点 $x_1$ 位于山谷的另一侧，呈对称配置。这导致了经典的“之”字形行为，其中朝向极小值点的进展非常缓慢。\n\n为了推导最坏情况下的缩减因子，我们将 $t=m^2/L^2$ 代入比率 $\\frac{\\|x_{1}\\|_Q^2}{\\|x_{0}\\|_Q^2}$ 的表达式中：\n$$ \\frac{\\|x_{1}\\|_Q^2}{\\|x_{0}\\|_Q^2} = 1 - H(m^2/L^2) = 1 - \\frac{(m^2 + (m^2/L^2) L^2)^2}{(m + (m^2/L^2) L)(m^3 + (m^2/L^2) L^3)} $$\n$$ = 1 - \\frac{(m^2 + m^2)^2}{(m + m^2/L)(m^3 + m^2L)} = 1 - \\frac{(2m^2)^2}{m(1+m/L)m^2(m+L)} $$\n$$ = 1 - \\frac{4m^4}{m^3 \\frac{L+m}{L} (m+L)} = 1 - \\frac{4mL}{(L+m)^2} $$\n$$ = \\frac{(L+m)^2 - 4mL}{(L+m)^2} = \\frac{L^2+2mL+m^2-4mL}{(L+m)^2} = \\frac{L^2-2mL+m^2}{(L+m)^2} = \\left(\\frac{L-m}{L+m}\\right)^2 $$\n最后，用条件数 $\\kappa = L/m$ 来表示这个结果：\n$$ \\left(\\frac{L/m - 1}{L/m + 1}\\right)^2 = \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2 $$\n这就是平方 $Q$-范数下的精确最坏情况单步缩减因子。", "answer": "$$\n\\boxed{\\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^{2}}\n$$", "id": "3149658"}, {"introduction": "超越经典的精确线搜索，更智能的步长选择策略能够显著提升最速下降法的效率。本练习将引导你实现并分析Barzilai-Borwein (BB)步长，这是一种巧妙利用历史迭代信息以近似二阶信息的先进技术。你将通过编码实践，验证其步长与问题谱特性（特征值）之间的深刻联系，并将其收敛性能与理论界进行对比 [@problem_id:3149736]。", "problem": "考虑对严格凸二次函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ 进行无约束最小化，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。其梯度为 $\\nabla f(x) = Q x$。最速下降法通过迭代 $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$ 生成迭代点，步长 $\\alpha_k  0$。Barzilai–Borwein 步长利用了连续迭代点之间的曲率信息。令 $s_{k-1} = x_k - x_{k-1}$ 以及 $y_{k-1} = \\nabla f(x_k) - \\nabla f(x_{k-1})$。对于二次函数的情况，$y_{k-1} = Q s_{k-1}$。两种 Barzilai–Borwein 变体是：\n- BB1: $\\alpha_k = \\dfrac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} y_{k-1}}$，\n- BB2: $\\alpha_k = \\dfrac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$。\n从 $x_0$ 开始迭代，并使用沿最速下降方向进行精确线搜索得到的初始步长，即 $\\alpha_0 = \\dfrac{\\nabla f(x_0)^{\\top} \\nabla f(x_0)}{\\nabla f(x_0)^{\\top} Q \\nabla f(x_0)}$。\n\n需要使用的基本事实：\n- 如果 $Q$ 是对称正定矩阵，那么其特征值满足 $0  \\lambda_{\\min} \\le \\lambda_{\\max}$，且对于任何非零向量 $z$，瑞利商 $R_Q(z) = \\dfrac{z^{\\top} Q z}{z^{\\top} z}$ 位于 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 区间内。\n- 梯度和误差的递推关系为 $g_{k+1} = (I - \\alpha_k Q) g_k$ 和 $e_{k+1} = (I - \\alpha_k Q) e_k$，其中 $g_k = \\nabla f(x_k)$ 和 $e_k = x_k - x^\\star$，且 $x^\\star = 0$。\n- 因此，经过 $m$ 步后，$g_m = P_m(Q) g_0$，其中 $P_m(t) = \\prod_{k=0}^{m-1} (1 - \\alpha_k t)$。根据谱定理，$\\|g_m\\|_2 \\le \\left(\\max_{i} |P_m(\\lambda_i(Q))|\\right) \\|g_0\\|_2$。\n\n任务：\n1) 在 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ 上实现最速下降法，使用两种 Barzilai–Borwein 步长选择（BB1 和 BB2），并对第一次更新使用上面给出的精确线搜索步长 $\\alpha_0$。最多运行 $m$ 次迭代，或直到 $\\|\\nabla f(x_k)\\|_2$ 在数值上为零（低于 $10^{-14}$）。对于每次运行，记录实际产生的步长序列 $\\{\\alpha_k\\}_{k=0}^{K-1}$，其中 $K \\le m$ 是实际执行的步数。\n2) 对于每次运行，验证步长的包含性质：对于特征值在 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 区间内的对称正定矩阵 $Q$，每个 $\\alpha_k$ 必须位于 $\\left[\\dfrac{1}{\\lambda_{\\max}}, \\dfrac{1}{\\lambda_{\\min}}\\right]$ 区间内。报告一个布尔值，指示所有记录的步长是否在 $10^{-12}$ 的容差内满足此条件。\n3) 对于每次运行，计算 $K$ 步后的经验收缩比 $r_{\\text{emp}} = \\dfrac{\\|g_K\\|_2}{\\|g_0\\|_2}$。\n4) 通过多项式谱界将经验收缩比映射到 $Q$ 的谱上。使用 $Q$ 的真实特征值计算精确的谱界 $B = \\max_{i} \\left| \\prod_{k=0}^{K-1} (1 - \\alpha_k \\lambda_i(Q)) \\right|$，并报告 $B$。理论保证 $r_{\\text{emp}} \\le B$。\n\n测试套件：\n- 情况 A（理想情况，对角矩阵）：$Q = \\operatorname{diag}(1, 3, 10)$，$x_0 = [1, -2, 3]^{\\top}$，$m = 30$。\n- 情况 B（非对角对称正定矩阵）：$Q = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0.5 \\\\ 0  0.5  2 \\end{bmatrix}$，$x_0 = [2, 1, -1]^{\\top}$，$m = 30$。\n- 情况 C（类边界情况，缩放单位矩阵）：$Q = 5 I_3$，$x_0 = [1, 2, 3]^{\\top}$，$m = 30$。\n- 情况 D（病态对角矩阵）：$Q = \\operatorname{diag}(0.01, 1, 100)$，$x_0 = [1, 1, 1]^{\\top}$，$m = 50$。\n\n输出要求：\n- 对于每个测试用例，分别运行 BB1 和 BB2，从相同的 $x_0$ 开始，并使用相同的 $\\alpha_0$ 公式。对于每次运行，生成：\n  - 项目 2) 中包含性质的布尔值，\n  - 项目 3) 中的经验收缩比 $r_{\\text{emp}}$（浮点数），\n  - 项目 4) 中的谱界 $B$（浮点数）。\n- 按以下顺序汇总每个测试用例的结果：$[\\text{BB1\\_inclusion}, \\text{BB2\\_inclusion}, r_{\\text{emp}}^{\\text{BB1}}, B^{\\text{BB1}}, r_{\\text{emp}}^{\\text{BB2}}, B^{\\text{BB2}}]$。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果列表，该列表以逗号分隔并用方括号括起，其中每个测试用例的结果本身也是一个列表，例如：$[[\\dots],[\\dots],[\\dots],[\\dots]]$。\n- 不涉及物理单位。所有数值应按规定报告为浮点数或布尔值。", "solution": "所提出的问题是数值优化领域一个有效且适定的练习。它要求实现和分析使用 Barzilai–Borwein (BB) 步长的最速下降法，以最小化一个严格凸二次函数。所有给定的定义、初始条件和理论论断在优化和线性代数领域都是标准的，并且在事实上是正确的。测试用例涉及对称正定矩阵，这确保了函数具有唯一的最小值，并且理论框架是适用的。该问题是自洽的，并为得出唯一且有意义的解提供了所有必要信息。\n\n问题的核心是对函数 $f(x) = \\frac{1}{2} x^{\\top} Q x$ 进行无约束最小化，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定（SPD）矩阵。$f(x)$ 的严格凸性保证了在 $x^{\\star} = 0$ 处存在唯一的全局最小值。该函数的梯度由 $\\nabla f(x) = Qx$ 给出。\n\n主要算法是最速下降法，它通过以下更新规则生成一个迭代序列 $\\{x_k\\}$：\n$$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$$\n其中 $\\nabla f(x_k)$ 是在 $x_k$ 处的最速下降方向，而 $\\alpha_k > 0$ 是步长。该方法的性能关键取决于步长序列 $\\{\\alpha_k\\}$ 的选择。\n\n问题指定了一种选择 $\\alpha_k$ 的混合策略：\n1.  **初始步长 ($\\alpha_0$)**: 第一个步长 $\\alpha_0$ 通过精确线搜索确定，该搜索沿着初始下降方向 $-\\nabla f(x_0)$ 最小化 $f(x)$。其公式为：\n    $$\\alpha_0 = \\frac{\\nabla f(x_0)^{\\top} \\nabla f(x_0)}{\\nabla f(x_0)^{\\top} Q \\nabla f(x_0)}$$\n    令 $g_k = \\nabla f(x_k)$，则有 $\\alpha_0 = \\frac{g_0^{\\top} g_0}{g_0^{\\top} Q g_0}$。\n\n2.  **后续步长 ($\\alpha_k$，对于 $k \\ge 1$)**: 对于后续迭代，使用两种 Barzilai–Borwein 步长之一。这些方法利用最近两次迭代的曲率信息。我们定义位移向量 $s_{k-1} = x_k - x_{k-1}$ 和梯度差分向量 $y_{k-1} = g_k - g_{k-1}$。对于二次目标函数，我们有精确关系 $y_{k-1} = Q s_{k-1}$。两种 BB 步长是：\n    -   **BB1**: 此步长源于对海森矩阵 $Q$ 的割线近似。\n        $$\\alpha_k = \\frac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} y_{k-1}} = \\frac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} Q s_{k-1}}$$\n    -   **BB2**: 此步长源于对逆海森矩阵 $Q^{-1}$ 的割线近似。\n        $$\\alpha_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}} = \\frac{s_{k-1}^{\\top} Q s_{k-1}}{s_{k-1}^{\\top} Q^2 s_{k-1}}$$\n\n这些步长的一个重要理论性质将其与海森矩阵 $Q$ 的谱联系起来。设 $Q$ 的特征值为 $0  \\lambda_{\\min} \\le \\lambda_2 \\le \\dots \\le \\lambda_{\\max}$。\n- BB1 步长和初始步长 $\\alpha_0$ 可以用瑞利商 $R_Q(z) = \\frac{z^{\\top} Q z}{z^{\\top} z}$ 表示。具体来说，$\\alpha_k = 1 / R_Q(s_{k-1})$ 且 $\\alpha_0 = 1 / R_Q(g_0)$。由于对称正定矩阵的瑞利商受其最小和最大特征值界定（对于任何非零 $z$，有 $\\lambda_{\\min} \\le R_Q(z) \\le \\lambda_{\\max}$），因此这些步长必须位于区间 $\\left[\\frac{1}{\\lambda_{\\max}}, \\frac{1}{\\lambda_{\\min}}\\right]$ 内。\n- 对于 BB2 步长，使用 $s_{k-1}$ 相对于 $Q$ 的特征向量的谱分解，可以证明 $1/\\alpha_k$ 是 $Q$ 特征值的加权平均值。因此，$\\lambda_{\\min} \\le 1/\\alpha_k \\le \\lambda_{\\max}$，这同样意味着 $\\alpha_k \\in \\left[\\frac{1}{\\lambda_{\\max}}, \\frac{1}{\\lambda_{\\min}}\\right]$。\n任务 2 要求在 $10^{-12}$ 的数值容差内，为所有计算出的步长验证此包含性质。\n\n该方法的收敛性通过检验梯度范数 $\\|g_k\\|_2$ 来分析。梯度向量遵循线性递推关系：\n$$g_{k+1} = Qx_{k+1} = Q(x_k - \\alpha_k g_k) = g_k - \\alpha_k Q g_k = (I - \\alpha_k Q) g_k$$\n将此递推关系展开 $K$ 步，得到：\n$$g_K = \\left( \\prod_{k=0}^{K-1} (I - \\alpha_k Q) \\right) g_0 = P_K(Q) g_0$$\n其中 $P_K(t) = \\prod_{k=0}^{K-1} (1 - \\alpha_k t)$ 是一个 $K$ 次多项式。最终梯度的范数受矩阵多项式 $P_K(Q)$ 的谱半径限制：\n$$\\|g_K\\|_2 \\le \\|P_K(Q)\\|_2 \\|g_0\\|_2 = \\left( \\max_{i} |P_K(\\lambda_i(Q))| \\right) \\|g_0\\|_2$$\n这导出了关系 $r_{\\text{emp}} \\le B$，其中：\n-   $r_{\\text{emp}} = \\frac{\\|g_K\\|_2}{\\|g_0\\|_2}$ 是经验收缩比（任务 3）。\n-   $B = \\max_{i} |P_K(\\lambda_i(Q))|$ 是理论谱界（任务 4），使用实际产生的步长序列 $\\{\\alpha_k\\}_{k=0}^{K-1}$ 和 $Q$ 的特征值计算得出。\n\n实现过程将为每个测试用例和每种 BB 方法（BB1 和 BB2）执行最速下降算法。对于每次运行，算法将从 $x_0$ 开始，计算 $\\alpha_0$，然后迭代，使用指定的 BB 规则计算 $k \\ge 1$ 的 $\\alpha_k$。当达到最大迭代次数 $m$ 或梯度范数低于 $10^{-14}$ 时，循环将终止。终止后，收集到的步长序列 $\\{\\alpha_k\\}$ 以及初始和最终梯度范数将用于计算所需的输出：包含性质的布尔值、经验收缩比 $r_{\\text{emp}}$ 和谱界 $B$。然后将这些结果汇总成指定的输出格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases for steepest descent\n    on a nonconvex function and report convergence to the saddle point.\n    \"\"\"\n\n    def run_steepest_descent(x0, y0, alpha, max_iter, use_perturbation, pert_params=None):\n        \"\"\"\n        Executes the steepest descent algorithm for a single test case.\n\n        Args:\n            x0 (float): Initial x-coordinate.\n            y0 (float): Initial y-coordinate.\n            alpha (float): The fixed step size.\n            max_iter (int): The maximum number of iterations.\n            use_perturbation (bool): Flag to enable/disable the perturbation mechanism.\n            pert_params (dict, optional): Parameters for the perturbation.\n\n        Returns:\n            bool: True if the final iterate is within the saddle tolerance, False otherwise.\n        \"\"\"\n\n        def grad_f(p):\n            \"\"\"Computes the gradient of f(x,y) = x^2 - y^2 + y^4.\"\"\"\n            x, y = p\n            gx = 2.0 * x\n            gy = 4.0 * y**3 - 2.0 * y\n            return np.array([gx, gy])\n\n        rng = None\n        if use_perturbation and pert_params:\n            rng = np.random.default_rng(pert_params['seed'])\n\n        p = np.array([float(x0), float(y0)])\n        perturbation_applied = False\n\n        for _ in range(max_iter):\n            # Check for and apply perturbation if conditions are met\n            if use_perturbation and not perturbation_applied and pert_params:\n                grad_norm = np.linalg.norm(grad_f(p))\n                saddle_dist = np.linalg.norm(p)\n\n                if (grad_norm  pert_params['grad_norm_thresh'] and\n                        saddle_dist  pert_params['saddle_radius']):\n                    \n                    perturbation = rng.normal(loc=0.0, scale=pert_params['std_dev'], size=2)\n                    p += perturbation\n                    perturbation_applied = True\n            \n            # Calculate gradient and perform update step\n            grad = grad_f(p)\n            p -= alpha * grad\n\n        final_norm = np.linalg.norm(p)\n        saddle_tolerance = 1e-8\n        \n        return final_norm  saddle_tolerance\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Start on stable manifold, no perturbation. Expect convergence to saddle.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 200, 'use_perturbation': False},\n        \n        # Case 2: Start near stable manifold, no perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.01, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': False},\n\n        # Case 3: Start on stable manifold, with perturbation. Expect escape.\n        {'x0': 1.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}},\n        \n        # Case 4: Start at the saddle, with perturbation. Expect escape.\n        {'x0': 0.0, 'y0': 0.0, 'alpha': 0.3, 'max_iter': 400, 'use_perturbation': True,\n         'pert_params': {'grad_norm_thresh': 1e-12, 'saddle_radius': 1e-3, 'std_dev': 1e-3, 'seed': 12345}}\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_steepest_descent(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3149710"}, {"introduction": "超越经典的精确线搜索，更智能的步长选择策略能够显著提升最速下降法的效率。本练习将引导你实现并分析Barzilai-Borwein (BB)步长，这是一种巧妙利用历史迭代信息以近似二阶信息的先进技术。你将通过编码实践，验证其步长与问题谱特性（特征值）之间的深刻联系，并将其收敛性能与理论界进行对比 [@problem_id:3149736]。", "problem": "考虑对严格凸二次函数 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ 进行无约束最小化，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。其梯度为 $\\nabla f(x) = Q x$。最速下降法通过迭代 $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$ 生成迭代点，步长 $\\alpha_k > 0$。Barzilai–Borwein 步长利用了连续迭代点之间的曲率信息。令 $s_{k-1} = x_k - x_{k-1}$ 以及 $y_{k-1} = \\nabla f(x_k) - \\nabla f(x_{k-1})$。对于二次函数的情况，$y_{k-1} = Q s_{k-1}$。两种 Barzilai–Borwein 变体是：\n- BB1: $\\alpha_k = \\dfrac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} y_{k-1}}$，\n- BB2: $\\alpha_k = \\dfrac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}}$。\n从 $x_0$ 开始迭代，并使用沿最速下降方向进行精确线搜索得到的初始步长，即 $\\alpha_0 = \\dfrac{\\nabla f(x_0)^{\\top} \\nabla f(x_0)}{\\nabla f(x_0)^{\\top} Q \\nabla f(x_0)}$。\n\n需要使用的基本事实：\n- 如果 $Q$ 是对称正定矩阵，那么其特征值满足 $0  \\lambda_{\\min} \\le \\lambda_{\\max}$，且对于任何非零向量 $z$，瑞利商 $R_Q(z) = \\dfrac{z^{\\top} Q z}{z^{\\top} z}$ 位于 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 区间内。\n- 梯度和误差的递推关系为 $g_{k+1} = (I - \\alpha_k Q) g_k$ 和 $e_{k+1} = (I - \\alpha_k Q) e_k$，其中 $g_k = \\nabla f(x_k)$ 和 $e_k = x_k - x^\\star$，且 $x^\\star = 0$。\n- 因此，经过 $m$ 步后，$g_m = P_m(Q) g_0$，其中 $P_m(t) = \\prod_{k=0}^{m-1} (1 - \\alpha_k t)$。根据谱定理，$\\|g_m\\|_2 \\le \\left(\\max_{i} |P_m(\\lambda_i(Q))|\\right) \\|g_0\\|_2$。\n\n任务：\n1) 在 $f(x) = \\tfrac{1}{2} x^{\\top} Q x$ 上实现最速下降法，使用两种 Barzilai–Borwein 步长选择（BB1 和 BB2），并对第一次更新使用上面给出的精确线搜索步长 $\\alpha_0$。最多运行 $m$ 次迭代，或直到 $\\|\\nabla f(x_k)\\|_2$ 在数值上为零（低于 $10^{-14}$）。对于每次运行，记录实际产生的步长序列 $\\{\\alpha_k\\}_{k=0}^{K-1}$，其中 $K \\le m$ 是实际执行的步数。\n2) 对于每次运行，验证步长的包含性质：对于特征值在 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 区间内的对称正定矩阵 $Q$，每个 $\\alpha_k$ 必须位于 $\\left[\\dfrac{1}{\\lambda_{\\max}}, \\dfrac{1}{\\lambda_{\\min}}\\right]$ 区间内。报告一个布尔值，指示所有记录的步长是否在 $10^{-12}$ 的容差内满足此条件。\n3) 对于每次运行，计算 $K$ 步后的经验收缩比 $r_{\\text{emp}} = \\dfrac{\\|g_K\\|_2}{\\|g_0\\|_2}$。\n4) 通过多项式谱界将经验收缩比映射到 $Q$ 的谱上。使用 $Q$ 的真实特征值计算精确的谱界 $B = \\max_{i} \\left| \\prod_{k=0}^{K-1} (1 - \\alpha_k \\lambda_i(Q)) \\right|$，并报告 $B$。理论保证 $r_{\\text{emp}} \\le B$。\n\n测试套件：\n- 情况 A（理想情况，对角矩阵）：$Q = \\operatorname{diag}(1, 3, 10)$，$x_0 = [1, -2, 3]^{\\top}$，$m = 30$。\n- 情况 B（非对角对称正定矩阵）：$Q = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0.5 \\\\ 0  0.5  2 \\end{bmatrix}$，$x_0 = [2, 1, -1]^{\\top}$，$m = 30$。\n- 情况 C（类边界情况，缩放单位矩阵）：$Q = 5 I_3$，$x_0 = [1, 2, 3]^{\\top}$，$m = 30$。\n- 情况 D（病态对角矩阵）：$Q = \\operatorname{diag}(0.01, 1, 100)$，$x_0 = [1, 1, 1]^{\\top}$，$m = 50$。\n\n输出要求：\n- 对于每个测试用例，分别运行 BB1 和 BB2，从相同的 $x_0$ 开始，并使用相同的 $\\alpha_0$ 公式。对于每次运行，生成：\n  - 项目 2) 中包含性质的布尔值，\n  - 项目 3) 中的经验收缩比 $r_{\\text{emp}}$（浮点数），\n  - 项目 4) 中的谱界 $B$（浮点数）。\n- 按以下顺序汇总每个测试用例的结果：$[\\text{BB1\\_inclusion}, \\text{BB2\\_inclusion}, r_{\\text{emp}}^{\\text{BB1}}, B^{\\text{BB1}}, r_{\\text{emp}}^{\\text{BB2}}, B^{\\text{BB2}}]$。\n- 您的程序应生成单行输出，其中包含所有测试用例的结果列表，该列表以逗号分隔并用方括号括起，其中每个测试用例的结果本身也是一个列表，例如：$[[\\dots],[\\dots],[\\dots],[\\dots]]$。\n- 不涉及物理单位。所有数值应按规定报告为浮点数或布尔值。", "solution": "所提出的问题是数值优化领域一个有效且适定的练习。它要求实现和分析使用 Barzilai–Borwein (BB) 步长的最速下降法，以最小化一个严格凸二次函数。所有给定的定义、初始条件和理论论断在优化和线性代数领域都是标准的，并且在事实上是正确的。测试用例涉及对称正定矩阵，这确保了函数具有唯一的最小值，并且理论框架是适用的。该问题是自洽的，并为得出唯一且有意义的解提供了所有必要信息。\n\n问题的核心是对函数 $f(x) = \\frac{1}{2} x^{\\top} Q x$ 进行无约束最小化，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定（SPD）矩阵。$f(x)$ 的严格凸性保证了在 $x^{\\star} = 0$ 处存在唯一的全局最小值。该函数的梯度由 $\\nabla f(x) = Qx$ 给出。\n\n主要算法是最速下降法，它通过以下更新规则生成一个迭代序列 $\\{x_k\\}$：\n$$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$$\n其中 $\\nabla f(x_k)$ 是在 $x_k$ 处的最速下降方向，而 $\\alpha_k > 0$ 是步长。该方法的性能关键取决于步长序列 $\\{\\alpha_k\\}$ 的选择。\n\n问题指定了一种选择 $\\alpha_k$ 的混合策略：\n1.  **初始步长 ($\\alpha_0$)**: 第一个步长 $\\alpha_0$ 通过精确线搜索确定，该搜索沿着初始下降方向 $-\\nabla f(x_0)$ 最小化 $f(x)$。其公式为：\n    $$\\alpha_0 = \\frac{\\nabla f(x_0)^{\\top} \\nabla f(x_0)}{\\nabla f(x_0)^{\\top} Q \\nabla f(x_0)}$$\n    令 $g_k = \\nabla f(x_k)$，则有 $\\alpha_0 = \\frac{g_0^{\\top} g_0}{g_0^{\\top} Q g_0}$。\n\n2.  **后续步长 ($\\alpha_k$，对于 $k \\ge 1$)**: 对于后续迭代，使用两种 Barzilai–Borwein 步长之一。这些方法利用最近两次迭代的曲率信息。我们定义位移向量 $s_{k-1} = x_k - x_{k-1}$ 和梯度差分向量 $y_{k-1} = g_k - g_{k-1}$。对于二次目标函数，我们有精确关系 $y_{k-1} = Q s_{k-1}$。两种 BB 步长是：\n    -   **BB1**: 此步长源于对海森矩阵 $Q$ 的割线近似。\n        $$\\alpha_k = \\frac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} y_{k-1}} = \\frac{s_{k-1}^{\\top} s_{k-1}}{s_{k-1}^{\\top} Q s_{k-1}}$$\n    -   **BB2**: 此步长源于对逆海森矩阵 $Q^{-1}$ 的割线近似。\n        $$\\alpha_k = \\frac{s_{k-1}^{\\top} y_{k-1}}{y_{k-1}^{\\top} y_{k-1}} = \\frac{s_{k-1}^{\\top} Q s_{k-1}}{s_{k-1}^{\\top} Q^2 s_{k-1}}$$\n\n这些步长的一个重要理论性质将其与海森矩阵 $Q$ 的谱联系起来。设 $Q$ 的特征值为 $0  \\lambda_{\\min} \\le \\lambda_2 \\le \\dots \\le \\lambda_{\\max}$。\n- BB1 步长和初始步长 $\\alpha_0$ 可以用瑞利商 $R_Q(z) = \\frac{z^{\\top} Q z}{z^{\\top} z}$ 表示。具体来说，$\\alpha_k = 1 / R_Q(s_{k-1})$ 且 $\\alpha_0 = 1 / R_Q(g_0)$。由于对称正定矩阵的瑞利商受其最小和最大特征值界定（对于任何非零 $z$，有 $\\lambda_{\\min} \\le R_Q(z) \\le \\lambda_{\\max}$），因此这些步长必须位于区间 $\\left[\\frac{1}{\\lambda_{\\max}}, \\frac{1}{\\lambda_{\\min}}\\right]$ 内。\n- 对于 BB2 步长，使用 $s_{k-1}$ 相对于 $Q$ 的特征向量的谱分解，可以证明 $1/\\alpha_k$ 是 $Q$ 特征值的加权平均值。因此，$\\lambda_{\\min} \\le 1/\\alpha_k \\le \\lambda_{\\max}$，这同样意味着 $\\alpha_k \\in \\left[\\frac{1}{\\lambda_{\\max}}, \\frac{1}{\\lambda_{\\min}}\\right]$。\n任务 2 要求在 $10^{-12}$ 的数值容差内，为所有计算出的步长验证此包含性质。\n\n该方法的收敛性通过检验梯度范数 $\\|g_k\\|_2$ 来分析。梯度向量遵循线性递推关系：\n$$g_{k+1} = Qx_{k+1} = Q(x_k - \\alpha_k g_k) = g_k - \\alpha_k Q g_k = (I - \\alpha_k Q) g_k$$\n将此递推关系展开 $K$ 步，得到：\n$$g_K = \\left( \\prod_{k=0}^{K-1} (I - \\alpha_k Q) \\right) g_0 = P_K(Q) g_0$$\n其中 $P_K(t) = \\prod_{k=0}^{K-1} (1 - \\alpha_k t)$ 是一个 $K$ 次多项式。最终梯度的范数受矩阵多项式 $P_K(Q)$ 的谱半径限制：\n$$\\|g_K\\|_2 \\le \\|P_K(Q)\\|_2 \\|g_0\\|_2 = \\left( \\max_{i} |P_K(\\lambda_i(Q))| \\right) \\|g_0\\|_2$$\n这导出了关系 $r_{\\text{emp}} \\le B$，其中：\n-   $r_{\\text{emp}} = \\frac{\\|g_K\\|_2}{\\|g_0\\|_2}$ 是经验收缩比（任务 3）。\n-   $B = \\max_{i} |P_K(\\lambda_i(Q))|$ 是理论谱界（任务 4），使用实际产生的步长序列 $\\{\\alpha_k\\}_{k=0}^{K-1}$ 和 $Q$ 的特征值计算得出。\n\n实现过程将为每个测试用例和每种 BB 方法（BB1 和 BB2）执行最速下降算法。对于每次运行，算法将从 $x_0$ 开始，计算 $\\alpha_0$，然后迭代，使用指定的 BB 规则计算 $k \\ge 1$ 的 $\\alpha_k$。当达到最大迭代次数 $m$ 或梯度范数低于 $10^{-14}$ 时，循环将终止。终止后，收集到的步长序列 $\\{\\alpha_k\\}$ 以及初始和最终梯度范数将用于计算所需的输出：包含性质的布尔值、经验收缩比 $r_{\\text{emp}}$ 和谱界 $B$。然后将这些结果汇总成指定的输出格式。", "answer": "```python\nimport numpy as np\n\ndef run_bb(Q, x0, m, bb_type):\n    \"\"\"\n    Runs the steepest descent method with Barzilai-Borwein steps.\n\n    Args:\n        Q (np.ndarray): The symmetric positive definite matrix.\n        x0 (np.ndarray): The starting vector.\n        m (int): The maximum number of iterations.\n        bb_type (str): The type of BB step to use ('BB1' or 'BB2').\n\n    Returns:\n        tuple: A tuple containing:\n            - bool: True if all steps satisfy the inclusion property.\n            - float: The empirical contraction ratio r_emp.\n            - float: The spectral bound B.\n    \"\"\"\n    g = Q @ x0\n    g0_norm = np.linalg.norm(g)\n    \n    if g0_norm  1e-14:\n        return True, 1.0, 1.0\n\n    x = x0.copy()\n    alphas = []\n    \n    # --- Step k=0: Exact line search step ---\n    alpha0_num = g @ g\n    alpha0_den = g @ Q @ g\n    if alpha0_den == 0: # Should not happen for SPD Q and g != 0\n        return False, np.inf, np.inf\n    alpha = alpha0_num / alpha0_den\n    alphas.append(alpha)\n    \n    x_prev = x\n    g_prev = g\n    x = x - alpha * g\n    g = Q @ x\n    \n    # --- Main Loop: BB steps for k >= 1 ---\n    for k in range(1, m):\n        if np.linalg.norm(g)  1e-14:\n            break\n            \n        s = x - x_prev\n        y = g - g_prev\n        \n        s_dot_s = s @ s\n        s_dot_y = s @ y\n        y_dot_y = y @ y\n        \n        if bb_type == 'BB1':\n            if abs(s_dot_y)  1e-15: break # Avoid division by zero\n            alpha = s_dot_s / s_dot_y\n        elif bb_type == 'BB2':\n            if abs(y_dot_y)  1e-15: break # Avoid division by zero\n            alpha = s_dot_y / y_dot_y\n        else:\n            raise ValueError(\"Invalid bb_type specified.\")\n            \n        alphas.append(alpha)\n        \n        x_prev = x\n        g_prev = g\n        x = x - alpha * g\n        g = Q @ x\n\n    K = len(alphas)\n    gK_norm = np.linalg.norm(g)\n    \n    # Task 2: Verify inclusion property\n    eigvals = np.linalg.eigvalsh(Q)\n    lambda_min = eigvals[0]\n    lambda_max = eigvals[-1]\n    \n    lower_bound = 1 / lambda_max - 1e-12\n    upper_bound = 1 / lambda_min + 1e-12\n    inclusion_holds = all(lower_bound = a = upper_bound for a in alphas)\n\n    # Task 3: Compute empirical contraction ratio\n    r_emp = gK_norm / g0_norm if g0_norm > 0 else 0.0\n\n    # Task 4: Compute spectral bound\n    poly_vals_abs = []\n    for lam in eigvals:\n        # P_K(lambda) = product of (1 - alpha_k * lambda)\n        p = np.prod([(1 - a * lam) for a in alphas])\n        poly_vals_abs.append(abs(p))\n    B = np.max(poly_vals_abs) if poly_vals_abs else 1.0\n\n    return inclusion_holds, r_emp, B\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (np.diag([1., 3., 10.]), np.array([1., -2., 3.]), 30),\n        (np.array([[4., 1., 0.], [1., 3., 0.5], [0., 0.5, 2.]]), np.array([2., 1., -1.]), 30),\n        (5 * np.identity(3), np.array([1., 2., 3.]), 30),\n        (np.diag([0.01, 1., 100.]), np.array([1., 1., 1.]), 50),\n    ]\n\n    all_results = []\n    for Q, x0, m in test_cases:\n        incl_bb1, r_emp_bb1, B_bb1 = run_bb(Q, x0, m, 'BB1')\n        incl_bb2, r_emp_bb2, B_bb2 = run_bb(Q, x0, m, 'BB2')\n        \n        case_results = [\n            incl_bb1, incl_bb2,\n            r_emp_bb1, B_bb1,\n            r_emp_bb2, B_bb2\n        ]\n        all_results.append(case_results)\n\n    # Produce the final output string in the specified format without spaces.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "3149736"}]}