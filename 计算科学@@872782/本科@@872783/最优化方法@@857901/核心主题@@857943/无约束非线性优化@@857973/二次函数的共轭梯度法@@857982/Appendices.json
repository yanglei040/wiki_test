{"hands_on_practices": [{"introduction": "我们从卷起袖子、一步步地手动演练共轭梯度法开始。这个练习旨在让你通过为一个简单的 $3 \\times 3$ 问题手动计算每个组成部分——残差、搜索方向和步长——来揭开该算法的神秘面纱。通过这样做，你不仅能看到公式在实践中的应用，还能亲眼见证该方法对于 $n$ 维二次问题在至多 $n$ 步内保证收敛的理论特性[@problem_id:3111640]。", "problem": "考虑目标函数为 $f(x) = \\tfrac{1}{2} x^{\\top} A x - b^{\\top} x$ 的无约束二次最小化问题，其中 $A \\in \\mathbb{R}^{3 \\times 3}$ 是具有不同特征值的对称正定 (SPD) 矩阵。设\n$$\nA = \\begin{pmatrix}\n2  0  0 \\\\\n0  3  0 \\\\\n0  0  5\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\ 1 \\\\ 1\n\\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\n注意到 $A$ 是对称正定矩阵，且具有不同的特征值 $2$，$3$ 和 $5$。从 $x_{0}$ 开始，应用共轭梯度法 (CG) 来最小化 $f(x)$，方法是构造两两 A-共轭的搜索方向，并在每一步沿着当前搜索方向执行精确线搜索。请按以下基本原理进行：\n- 使用 $f(x)$ 的定义和欧几里得内积，通过最小化关于 $\\alpha$ 的 $f(x_{k} + \\alpha p_{k})$ 来推导每次迭代的步长。\n- 对连续的搜索方向施加 A-共轭性以确定新搜索方向的递推关系，并证明残差与搜索方向之间出现的正交关系。\n- 手动执行整整 $3$ 次共轭梯度迭代 $(k = 0, 1, 2)$，计算 $x_{1}$，$x_{2}$ 和 $x_{3}$，并通过证明 $x_{3}$ 处的残差是零向量来验证终止。\n\n$f(x_{3})$ 的精确值是多少？请以单一简化分数的形式提供您的答案。不需要四舍五入。", "solution": "问题是使用共轭梯度法 (CG) 最小化二次函数 $f(x) = \\frac{1}{2} x^{\\top} A x - b^{\\top} x$。该函数的梯度为 $\\nabla f(x) = A x - b$。当 $\\nabla f(x) = 0$ 时达到最小值，这对应于求解线性系统 $A x = b$。在迭代点 $x_k$ 处的残差定义为 $r_k = b - A x_k = -\\nabla f(x_k)$。CG 方法生成一系列迭代点 $x_{k+1} = x_k + \\alpha_k p_k$，其中 $p_k$ 是搜索方向，$\\alpha_k$ 是步长。\n\n首先，我们通过执行精确线搜索来推导步长 $\\alpha_k$。我们最小化关于 $\\alpha$ 的 $\\phi(\\alpha) = f(x_k + \\alpha p_k)$：\n$$\n\\phi(\\alpha) = \\frac{1}{2} (x_k + \\alpha p_k)^{\\top} A (x_k + \\alpha p_k) - b^{\\top} (x_k + \\alpha p_k)\n$$\n展开此表达式并利用 $A$ 的对称性 ($A=A^{\\top}$)，我们得到：\n$$\n\\phi(\\alpha) = \\frac{1}{2} x_k^{\\top} A x_k - b^{\\top} x_k + \\alpha (p_k^{\\top} A x_k - p_k^{\\top} b) + \\frac{1}{2} \\alpha^2 p_k^{\\top} A p_k\n$$\n为了找到最小值，我们将关于 $\\alpha$ 的导数设为零：\n$$\n\\frac{d\\phi}{d\\alpha} = p_k^{\\top} A x_k - p_k^{\\top} b + \\alpha p_k^{\\top} A p_k = p_k^{\\top} (A x_k - b) + \\alpha p_k^{\\top} A p_k = -p_k^{\\top} r_k + \\alpha p_k^{\\top} A p_k = 0\n$$\n解出 $\\alpha$，我们得到步长：\n$$\n\\alpha_k = \\frac{p_k^{\\top} r_k}{p_k^{\\top} A p_k}\n$$\n\n接下来，我们确定搜索方向 $p_k$ 的递推关系。初始搜索方向是最速下降方向，$p_0 = r_0$。对于后续步骤，新的搜索方向 $p_{k+1}$ 被构造为新残差 $r_{k+1}$ 和前一个搜索方向 $p_k$ 的线性组合：\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\n选择系数 $\\beta_k$ 以强制连续搜索方向之间的 A-共轭性，即 $p_{k+1}^{\\top} A p_k = 0$。\n$$\n(r_{k+1} + \\beta_k p_k)^{\\top} A p_k = 0 \\implies r_{k+1}^{\\top} A p_k + \\beta_k p_k^{\\top} A p_k = 0\n$$\n这给出了 $\\beta_k$ 的 Hestenes-Stiefel 公式：\n$$\n\\beta_k = -\\frac{r_{k+1}^{\\top} A p_k}{p_k^{\\top} A p_k}\n$$\n残差更新为 $r_{k+1} = r_k - \\alpha_k A p_k$，由此我们得到 $A p_k = \\frac{1}{\\alpha_k} (r_k - r_{k+1})$。将此代入 $\\beta_k$ 的分子中：\n$$\nr_{k+1}^{\\top} A p_k = \\frac{1}{\\alpha_k} r_{k+1}^{\\top} (r_k - r_{k+1}) = -\\frac{1}{\\alpha_k} r_{k+1}^{\\top} r_{k+1}\n$$\n这里我们使用了连续残差是正交的这一事实，$r_{k+1}^{\\top} r_k = 0$。此外，搜索方向更新 $p_k = r_k + \\beta_{k-1} p_{k-1}$ 和残差的正交性 $r_k^{\\top} r_j=0$（对于 $j  k$）意味着 $p_k^{\\top} r_k = (r_k + \\beta_{k-1} p_{k-1})^{\\top} r_k = r_k^{\\top} r_k$。这将步长公式简化为 $\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}$。\n将此代入 $\\beta_k$ 的表达式中：\n$$\n\\beta_k = -\\frac{- (r_{k+1}^{\\top} r_{k+1}) / \\alpha_k}{p_k^{\\top} A p_k} = \\frac{r_{k+1}^{\\top} r_{k+1}}{\\alpha_k (p_k^{\\top} A p_k)} = \\frac{r_{k+1}^{\\top} r_{k+1}}{(\\frac{r_k^{\\top} r_k}{p_k^{\\top} A p_k}) (p_k^{\\top} A p_k)} = \\frac{r_{k+1}^{\\top} r_{k+1}}{r_k^{\\top} r_k}\n$$\n这就是 $\\beta_k$ 的 Fletcher-Reeves 公式，我们将在计算中使用它。\n\n给定 $A = \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix}$，$b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$ 和 $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n**迭代 $k=0$:**\n1.  初始残差：$r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n2.  初始搜索方向：$p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$。\n3.  计算 $r_0^{\\top} r_0 = 1^2 + 1^2 + 1^2 = 3$。\n4.  计算 $A p_0 = \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix}$。\n5.  计算 $p_0^{\\top} A p_0 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = 2+3+5 = 10$。\n6.  步长：$\\alpha_0 = \\frac{r_0^{\\top} r_0}{p_0^{\\top} A p_0} = \\frac{3}{10}$。\n7.  更新解：$x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{3}{10} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix}$。\n8.  更新残差：$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{3}{10} \\begin{pmatrix} 2 \\\\ 3 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 1 - 6/10 \\\\ 1 - 9/10 \\\\ 1 - 15/10 \\end{pmatrix} = \\begin{pmatrix} 4/10 \\\\ 1/10 \\\\ -5/10 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix}$。\n\n**迭代 $k=1$:**\n1.  计算 $r_1^{\\top} r_1 = (\\frac{1}{10})^2 (4^2 + 1^2 + (-5)^2) = \\frac{1}{100}(16+1+25) = \\frac{42}{100} = \\frac{21}{50}$。\n2.  计算 $\\beta_0 = \\frac{r_1^{\\top} r_1}{r_0^{\\top} r_0} = \\frac{21/50}{3} = \\frac{7}{50}$。\n3.  更新搜索方向：$p_1 = r_1 + \\beta_0 p_0 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} + \\frac{7}{50} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 20 \\\\ 5 \\\\ -25 \\end{pmatrix} + \\frac{1}{50} \\begin{pmatrix} 7 \\\\ 7 \\\\ 7 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix}$。\n4.  计算 $A p_1 = \\frac{1}{50} \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix}$。\n5.  计算 $p_1^{\\top} A p_1 = \\frac{1}{50} \\begin{pmatrix} 27  12  -18 \\end{pmatrix} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{2500}(27 \\cdot 54 + 12 \\cdot 36 - 18 \\cdot (-90)) = \\frac{1458+432+1620}{2500} = \\frac{3510}{2500} = \\frac{351}{250}$。\n6.  步长：$\\alpha_1 = \\frac{r_1^{\\top} r_1}{p_1^{\\top} A p_1} = \\frac{21/50}{351/250} = \\frac{21}{50} \\cdot \\frac{250}{351} = \\frac{21 \\cdot 5}{351} = \\frac{105}{351} = \\frac{35}{117}$。\n7.  更新解：$x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 3/10 \\\\ 3/10 \\\\ 3/10 \\end{pmatrix} + \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{10}\\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{7}{1170} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{117}{1170} \\begin{pmatrix} 3 \\\\ 3 \\\\ 3 \\end{pmatrix} + \\frac{1}{1170} \\begin{pmatrix} 189 \\\\ 84 \\\\ -126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 351+189 \\\\ 351+84 \\\\ 351-126 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 540 \\\\ 435 \\\\ 225 \\end{pmatrix} = \\frac{15}{1170} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix}$。\n8.  更新残差：$r_2 = r_1 - \\alpha_1 A p_1 = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{35}{117} \\frac{1}{50} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4 \\\\ 1 \\\\ -5 \\end{pmatrix} - \\frac{7}{1170} \\begin{pmatrix} 54 \\\\ 36 \\\\ -90 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 468 - 378 \\\\ 117 - 252 \\\\ -585 + 630 \\end{pmatrix} = \\frac{1}{1170} \\begin{pmatrix} 90 \\\\ -135 \\\\ 45 \\end{pmatrix} = \\frac{45}{1170} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$。\n\n**迭代 $k=2$:**\n1.  计算 $r_2^{\\top} r_2 = (\\frac{1}{26})^2 (2^2 + (-3)^2 + 1^2) = \\frac{1}{676}(4+9+1) = \\frac{14}{676} = \\frac{7}{338}$。\n2.  计算 $\\beta_1 = \\frac{r_2^{\\top} r_2}{r_1^{\\top} r_1} = \\frac{7/338}{21/50} = \\frac{7}{338} \\cdot \\frac{50}{21} = \\frac{1}{338} \\frac{50}{3} = \\frac{25}{507}$。\n3.  更新搜索方向：$p_2 = r_2 + \\beta_1 p_1 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{25}{507} \\frac{1}{50} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{39}{1014} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\frac{1}{1014} \\begin{pmatrix} 27 \\\\ 12 \\\\ -18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 78+27 \\\\ -117+12 \\\\ 39-18 \\end{pmatrix} = \\frac{1}{1014} \\begin{pmatrix} 105 \\\\ -105 \\\\ 21 \\end{pmatrix} = \\frac{21}{1014} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix}$。\n4.  计算 $A p_2 = \\frac{7}{338} \\begin{pmatrix} 2  0  0 \\\\ 0  3  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{7}{338} \\begin{pmatrix} 10 \\\\ -15 \\\\ 5 \\end{pmatrix} = \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix}$。\n5.  计算 $p_2^{\\top} A p_2 = \\frac{7}{338} \\begin{pmatrix} 5  -5  1 \\end{pmatrix} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{245}{338^2}(10+15+1) = \\frac{245 \\cdot 26}{338^2} = \\frac{245}{338 \\cdot 13} = \\frac{245}{4394}$。\n6.  步长：$\\alpha_2 = \\frac{r_2^{\\top} r_2}{p_2^{\\top} A p_2} = \\frac{7/338}{245/4394} = \\frac{7}{338} \\frac{4394}{245} = \\frac{7 \\cdot 13}{245} = \\frac{91}{245} = \\frac{13}{35}$。\n7.  更新解：$x_3 = x_2 + \\alpha_2 p_2 = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{13}{35} \\frac{7}{338} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{5 \\cdot 26} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{78} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{1}{130} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{5}{390} \\begin{pmatrix} 36 \\\\ 29 \\\\ 15 \\end{pmatrix} + \\frac{3}{390} \\begin{pmatrix} 5 \\\\ -5 \\\\ 1 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 180+15 \\\\ 145-15 \\\\ 75+3 \\end{pmatrix} = \\frac{1}{390} \\begin{pmatrix} 195 \\\\ 130 \\\\ 78 \\end{pmatrix} = \\begin{pmatrix} 195/390 \\\\ 130/390 \\\\ 78/390 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$。\n8.  更新残差：$r_3 = r_2 - \\alpha_2 A p_2 = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{35} \\frac{35}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{13}{338} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} - \\frac{1}{26} \\begin{pmatrix} 2 \\\\ -3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$。\n\n残差 $r_3$ 是零向量，这验证了该方法在 $3$ 次迭代后收敛到精确解，这对于一个 $3 \\times 3$ 的系统是符合预期的。\n最终解为 $x_3 = x^* = A^{-1} b = \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix}$。\n\n我们被要求计算 $f(x_3)$。\n$$\nf(x_3) = \\frac{1}{2} x_3^{\\top} A x_3 - b^{\\top} x_3\n$$\n由于 $r_3 = b - Ax_3 = 0$，我们有 $Ax_3 = b$。将此代入 $f(x_3)$ 的表达式中：\n$$\nf(x_3) = \\frac{1}{2} x_3^{\\top} b - b^{\\top} x_3 = -\\frac{1}{2} b^{\\top} x_3\n$$\n现在我们计算这个值：\n$$\nb^{\\top} x_3 = \\begin{pmatrix} 1  1  1 \\end{pmatrix} \\begin{pmatrix} 1/2 \\\\ 1/3 \\\\ 1/5 \\end{pmatrix} = \\frac{1}{2} + \\frac{1}{3} + \\frac{1}{5} = \\frac{15}{30} + \\frac{10}{30} + \\frac{6}{30} = \\frac{31}{30}\n$$\n因此，函数的最小值为：\n$$\nf(x_3) = -\\frac{1}{2} \\left( \\frac{31}{30} \\right) = -\\frac{31}{60}\n$$", "answer": "$$\\boxed{-\\frac{31}{60}}$$", "id": "3111640"}, {"introduction": "为什么共轭梯度法对某些问题收敛快，而对另一些问题收敛慢？这个实践将从算法的“如何”运作，转向“为何”如此运作。通过编程实现共轭梯度法，并将误差投影到矩阵的特征向量上，你将通过实验发现一个关键原理：共轭梯度法擅长快速消除与极端（极大和极小）特征值相关的误差分量[@problem_id:3111694]。这将为你提供一个关于该方法收敛特性的深刻、可视化的见解。", "problem": "考虑严格凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$ 的无约束最小化问题，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）矩阵，$\\mathbf{b} \\in \\mathbb{R}^{n}$。共轭梯度（CG）方法通过使用 $A$-共轭的搜索方向迭代更新 $\\mathbf{x}_{k}$ 来解决此问题，而无需构造 $A^{-1}$。从第一性原理出发，可以通过误差 $\\mathbf{e}_{k} = \\mathbf{x}_{k} - \\mathbf{x}^{\\star}$ 来刻画 CG 的迭代量 $\\mathbf{x}_{k}$，其中 $\\mathbf{x}^{\\star}$ 是满足 $A \\mathbf{x}^{\\star} = \\mathbf{b}$ 的唯一最小化子。当 $A$ 具有特征分解 $A = U \\Lambda U^{\\top}$，其中 $U = [\\mathbf{u}_{1}, \\mathbf{u}_{2}, \\ldots, \\mathbf{u}_{n}]$ 是正交特征向量，$ \\Lambda = \\operatorname{diag}(\\lambda_{1}, \\ldots, \\lambda_{n})$ 是特征值时，沿特征基的误差分量由投影 $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} \\mathbf{e}_{k}$ 给出。根据经验和理论观察，CG 迭代会以特有的模式逐步抑制与 $A$ 的极端特征值（非常大和非常小）相关的误差分量。\n\n您的任务是编写一个完整的、可运行的程序，该程序能够：\n- 为 SPD 矩阵 $A$ 实现共轭梯度（CG）方法，以在 $\\mathbb{R}^{3}$ 中最小化 $f(\\mathbf{x})$。\n- 对于给定的测试用例，构造具有已知特征向量和特征值的 $A$，设置 $\\mathbf{x}^{\\star}$ 和 $\\mathbf{x}_{0}$，运行 CG 恰好 $n$ 次迭代（$n=3$），并记录在每次迭代 $k \\in \\{0,1,2,3\\}$ 时，每个特征向量 $\\mathbf{u}_{i}$ 对应的误差投影 $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} (\\mathbf{x}_{k} - \\mathbf{x}^{\\star})$。\n- 对于每个特征分量 $i$，定义“湮灭迭代” $t_{i}$ 为满足 $\\left|\\alpha_{i}^{(k)}\\right| \\leq \\tau \\left|\\alpha_{i}^{(0)}\\right|$ 的最小 $k \\in \\{0,1,2,3\\}$，其中 $\\tau \\in (0,1)$ 是一个给定的阈值分数。\n- 确定 CG 是否表现出以下湮灭时间顺序：与最大特征值对齐的分量比与中间特征值对齐的分量更早被湮灭，而后者又比与最小特征值对齐的分量更早被湮灭。形式上，当特征值排序为 $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$ 时，验证 $t_{\\max}  t_{\\text{mid}}  t_{\\min}$ 是否成立。\n\n使用以下涵盖不同行为的测试套件：\n- 测试用例 1（具有强谱分离的理想情况）：$A = \\operatorname{diag}([1, 10, 100])$，$\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$，$\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$，阈值 $\\tau = 0.05$。\n- 测试用例 2（相同谱但具有非平凡特征基）：设 $U = R_{z}(\\theta) R_{x}(\\phi)$，其中 $R_{z}(\\theta) = \\begin{bmatrix}\\cos \\theta  -\\sin \\theta  0 \\\\ \\sin \\theta  \\cos \\theta  0 \\\\ 0  0  1 \\end{bmatrix}$ 且 $R_{x}(\\phi) = \\begin{bmatrix} 1  0  0 \\\\ 0  \\cos \\phi  -\\sin \\phi \\\\ 0  \\sin \\phi  \\cos \\phi \\end{bmatrix}$，角度以弧度为单位。取 $\\theta = \\pi/4$，$\\phi = \\pi/3$，设 $\\Lambda = \\operatorname{diag}([1, 10, 100])$，且 $A = U \\Lambda U^{\\top}$。使用 $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$，$\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$，阈值 $\\tau = 0.05$。\n- 测试用例 3（特征值相近）：$A = \\operatorname{diag}([10, 11, 12])$，$\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$，$\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$，阈值 $\\tau = 0.05$。\n- 测试用例 4（病态谱）：$A = \\operatorname{diag}([10^{-3}, 1, 10^{3}])$，$\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$，$\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$，阈值 $\\tau = 0.05$。\n\n对于每个测试用例：\n1. 计算特征分解 $A = U \\Lambda U^{\\top}$ 并对特征值进行排序，使得 $\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$，以及对应的特征向量 $\\mathbf{u}_{\\min}$、$\\mathbf{u}_{\\text{mid}}$ 和 $\\mathbf{u}_{\\max}$。\n2. 从 $\\mathbf{x}_{0}$ 开始运行共轭梯度法，恰好进行 $n = 3$ 次迭代，其中 $\\mathbf{b} = A \\mathbf{x}^{\\star}$，并记录 $k = 0, 1, 2, 3$ 时的 $\\mathbf{x}_{k}$。\n3. 使用阈值分数 $\\tau$ 计算如上定义的 $t_{\\min}$、$t_{\\text{mid}}$ 和 $t_{\\max}$。\n4. 输出一个布尔值，指示 $t_{\\max}  t_{\\text{mid}}  t_{\\min}$ 是否满足。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，例如 $[r_{1}, r_{2}, r_{3}, r_{4}]$，其中每个 $r_{i}$ 是测试用例 $i$ 的布尔值。角度只允许使用弧度。不涉及任何物理单位。最终输出必须是布尔值。该单行打印是您的程序应产生的唯一输出。", "solution": "用户提供了一个有效的问题。\n该问题要求研究共轭梯度（CG）方法在最小化严格凸二次函数 $f(\\mathbf{x}) = \\tfrac{1}{2}\\mathbf{x}^{\\top} A \\mathbf{x} - \\mathbf{b}^{\\top} \\mathbf{x}$ 时的收敛行为。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是对称正定（SPD）的，维度指定为 $n=3$。$f(\\mathbf{x})$ 的唯一最小化子 $\\mathbf{x}^{\\star}$ 是线性系统 $A \\mathbf{x}^{\\star} = \\mathbf{b}$ 的解。问题的核心是验证一个关于 CG 方法降低与 $A$ 的特征向量对齐的误差分量速率的假设。具体来说，我们要测试与最大特征值对应的误差分量是否比与中间特征值对应的误差分量被抑制得更快，而后者又是否比与最小特征值对应的误差分量被抑制得更快。\n\n**共轭梯度（CG）算法**\nCG 方法是一种迭代算法，它从一个初始猜测 $\\mathbf{x}_{0}$ 开始，生成一个收敛于 $\\mathbf{x}^{\\star}$ 的近似序列 $\\{\\mathbf{x}_{k}\\}$。给定迭代 $k$ 的更新规则如下：\n\n初始化：\n$\\mathbf{r}_{0} = \\mathbf{b} - A \\mathbf{x}_{0}$\n$\\mathbf{p}_{0} = \\mathbf{r}_{0}$\n\n对于 $k = 0, 1, 2, \\ldots$：\n步长：$\\alpha_{k} = \\frac{\\mathbf{r}_{k}^{\\top} \\mathbf{r}_{k}}{\\mathbf{p}_{k}^{\\top} A \\mathbf{p}_{k}}$\n更新解：$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} + \\alpha_{k} \\mathbf{p}_{k}$\n更新残差：$\\mathbf{r}_{k+1} = \\mathbf{r}_{k} - \\alpha_{k} A \\mathbf{p}_{k}$\n更新方向：$\\beta_{k} = \\frac{\\mathbf{r}_{k+1}^{\\top} \\mathbf{r}_{k+1}}{\\mathbf{r}_{k}^{\\top} \\mathbf{r}_{k}}$，然后 $\\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_{k} \\mathbf{p}_{k}$\n\n搜索方向 $\\{\\mathbf{p}_k\\}$ 被构造成 $A$-共轭的，即对于 $i \\neq j$，有 $\\mathbf{p}_i^\\top A \\mathbf{p}_j = 0$。\n\n**特征基中的误差分析**\n第 $k$ 次迭代的误差是向量 $\\mathbf{e}_{k} = \\mathbf{x}_{k} - \\mathbf{x}^{\\star}$。由于 $A$ 是 SPD 矩阵，它有一套完备的正交特征向量 $\\mathbf{u}_{1}, \\ldots, \\mathbf{u}_{n}$，以及对应的正特征值 $\\lambda_{1}, \\ldots, \\lambda_{n}$。我们可以在这个特征基中表示误差 $\\mathbf{e}_{k}$。误差沿着每个特征向量 $\\mathbf{u}_{i}$ 的分量由投影 $\\alpha_{i}^{(k)} = \\mathbf{u}_{i}^{\\top} \\mathbf{e}_{k}$ 给出。本问题研究的是随着 $k$ 的增加，量级 $|\\alpha_{i}^{(k)}|$ 的行为。\n\n**验证过程**\n该解决方案以一个程序实现，对每个测试用例遵循以下步骤：\n1.  **系统设置**：按规定构造矩阵 $A \\in \\mathbb{R}^{3 \\times 3}$。对于给定的最小化子 $\\mathbf{x}^{\\star} = [1, 1, 1]^{\\top}$，向量 $\\mathbf{b}$ 计算为 $\\mathbf{b} = A \\mathbf{x}^{\\star}$。\n2.  **特征分解**：程序计算 $A$ 的特征分解，以找到其特征值和特征向量。特征值按升序排序，$\\lambda_{\\min} \\leq \\lambda_{\\text{mid}} \\leq \\lambda_{\\max}$，其对应的特征向量被识别为 $\\mathbf{u}_{\\min}$、$\\mathbf{u}_{\\text{mid}}$ 和 $\\mathbf{u}_{\\max}$。\n3.  **CG 迭代**：从初始猜测 $\\mathbf{x}_{0} = [0, 0, 0]^{\\top}$ 开始，执行 CG 算法恰好 $n=3$ 次迭代。这将生成迭代序列 $\\mathbf{x}_{0}, \\mathbf{x}_{1}, \\mathbf{x}_{2}, \\mathbf{x}_{3}$。\n4.  **湮灭时间计算**：对于每个特征方向 $i \\in \\{\\min, \\text{mid}, \\max\\}$，确定“湮灭迭代” $t_{i}$。这被定义为使相对误差投影量级降至给定阈值 $\\tau = 0.05$ 以下的最小迭代指数 $k \\in \\{0, 1, 2, 3\\}$。形式上，$t_{i} = \\min \\left\\{ k \\in \\{0,1,2,3\\} \\, \\Big| \\, \\left|\\alpha_{i}^{(k)}\\right| \\leq \\tau \\left|\\alpha_{i}^{(0)}\\right| \\right\\}$。在精确算术中，CG 最多在 $n$ 步内收敛，因此 $\\mathbf{e}_{3} = \\mathbf{0}$，保证了 $\\alpha_{i}^{(3)} = 0$。这确保了湮灭条件最迟在 $k=3$ 时总能满足，因此 $t_i$ 在集合 $\\{0, 1, 2, 3\\}$ 内总是有明确定义的。\n5.  **假设检验**：程序评估布尔条件 $t_{\\max}  t_{\\text{mid}}  t_{\\min}$。此关系形式化了与较大特征值相关的误差分量被更快抑制的假设。如果条件满足，程序输出 `True`，否则输出 `False`。对所有四个提供的测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef conjugate_gradient(A, b, x0, n_iter):\n    \"\"\"\n    Implements the Conjugate Gradient method for solving Ax=b.\n    Runs for exactly n_iter iterations and returns all intermediate solutions.\n    \"\"\"\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    \n    # Check for the trivial case where the initial guess is already the solution.\n    if np.linalg.norm(r) = 1e-15:\n        return [x0.copy()] * (n_iter + 1)\n        \n    rs_old = r.T @ r\n    iterates = [x0.copy()]\n    \n    for _ in range(n_iter):\n        Ap = A @ p\n        \n        # Denominator of alpha can be zero if p is in the null space of A.\n        # For an SPD matrix A, this only happens if p=0.\n        # p=0 implies r=0, so the algorithm would have converged.\n        denom = p.T @ Ap\n        if np.isclose(denom, 0):\n            # This indicates convergence has been reached.\n            # Continue filling iterates list with the current solution.\n            for _ in range(n_iter - len(iterates) + 1):\n                iterates.append(x.copy())\n            return iterates\n\n        alpha = rs_old / denom\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new = r.T @ r\n        \n        # In exact arithmetic, p cannot be zero unless r is, so rs_old won't be zero.\n        # Floating point arithmetic can lead to issues, though unlikely in this controlled problem.\n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n        \n        iterates.append(x.copy())\n        \n    return iterates\n\ndef analyze_case(A, x_star, x0, tau):\n    \"\"\"\n    Analyzes a single test case according to the problem description.\n    \"\"\"\n    n = A.shape[0]\n    b = A @ x_star\n    \n    # Step 1: Compute eigendecomposition.\n    # np.linalg.eigh returns eigenvalues in ascending order, which is required.\n    lambdas, U = np.linalg.eigh(A)\n    \n    # Eigenvectors corresponding to lambda_min, lambda_mid, lambda_max.\n    eigenvectors = {'min': U[:, 0], 'mid': U[:, 1], 'max': U[:, 2]}\n\n    # Step 2: Run Conjugate Gradient for n=3 iterations.\n    # The list will contain x_0, x_1, x_2, x_3.\n    x_k_list = conjugate_gradient(A, b, x0, n)\n    \n    # Step 3: Compute error projections and annihilation times.\n    annihilation_times = {}\n    \n    # Compute initial error and its projections.\n    e0 = x_k_list[0] - x_star\n    alpha_i_0 = {\n        'min': eigenvectors['min'].T @ e0,\n        'mid': eigenvectors['mid'].T @ e0,\n        'max': eigenvectors['max'].T @ e0\n    }\n    \n    for i_key in ['min', 'mid', 'max']:\n        initial_proj_mag = abs(alpha_i_0[i_key])\n        \n        # Handle case where initial projection is already zero.\n        if np.isclose(initial_proj_mag, 0):\n            annihilation_times[i_key] = 0\n            continue\n\n        # Find the smallest k where the annihilation condition is met.\n        found = False\n        for k in range(n + 1):  # k from 0 to 3\n            ek = x_k_list[k] - x_star\n            alpha_i_k = eigenvectors[i_key].T @ ek\n            \n            if abs(alpha_i_k) = tau * initial_proj_mag:\n                annihilation_times[i_key] = k\n                found = True\n                break\n        \n        # This fallback is for safety but should not be reached in exact arithmetic,\n        # as CG converges in n steps, making e_n = 0.\n        if not found:\n            annihilation_times[i_key] = n + 1 \n\n    t_min = annihilation_times['min']\n    t_mid = annihilation_times['mid']\n    t_max = annihilation_times['max']\n\n    # Step 4: Output boolean indicating if t_max  t_mid  t_min is satisfied.\n    return t_max  t_mid  t_min\n\ndef solve():\n    # Define common parameters for all test cases.\n    x_star = np.array([1.0, 1.0, 1.0])\n    x0 = np.array([0.0, 0.0, 0.0])\n    tau = 0.05\n\n    # Test Case 1: Happy path with strong spectral separation\n    A1 = np.diag([1.0, 10.0, 100.0])\n    \n    # Test Case 2: Same spectrum with a nontrivial eigenbasis\n    theta = np.pi / 4.0\n    phi = np.pi / 3.0\n    cos_t, sin_t = np.cos(theta), np.sin(theta)\n    cos_p, sin_p = np.cos(phi), np.sin(phi)\n    \n    Rz = np.array([[cos_t, -sin_t, 0.0], \n                   [sin_t, cos_t, 0.0], \n                   [0.0, 0.0, 1.0]])\n    Rx = np.array([[1.0, 0.0, 0.0], \n                   [0.0, cos_p, -sin_p], \n                   [0.0, sin_p, cos_p]])\n                   \n    U = Rz @ Rx\n    # Reorder eigenvalues to match the U from problem description, then let eigh sort it.\n    # The problem description for case 2 is a bit ambiguous if we need to sort or not.\n    # We will let eigh sort them, so the logic is consistent.\n    # `eigh` will sort lambdas as 1, 10, 100. The columns of U will correspond to this order.\n    Lambda = np.diag([1.0, 10.0, 100.0]) \n    A2 = U @ Lambda @ U.T\n\n    # Test Case 3: Near-equal eigenvalues\n    A3 = np.diag([10.0, 11.0, 12.0])\n\n    # Test Case 4: Ill-conditioned spectrum\n    A4 = np.diag([1e-3, 1.0, 1e3])\n    \n    test_cases_matrices = [A1, A2, A3, A4]\n    \n    results = []\n    for A in test_cases_matrices:\n        result = analyze_case(A, x_star, x0, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3111694"}, {"introduction": "上一个实践表明，特征值的分布决定了共轭梯度法的性能。对于特征值分布广泛的病态系统，共轭梯度法可能会收敛得很慢，这正是预处理技术发挥作用的地方。在这个练习中，你将为一个来自物理和工程领域的经典问题——泊松方程——实现预处理共轭梯度法（PCG），并将其性能与标准共轭梯度法进行比较[@problem_id:3111608]。这将突显预处理在使共轭梯度法成为一个真正稳健高效的求解器方面的强大作用。", "problem": "给定一个由二次目标函数最小化问题产生的对称正定线性系统。具体而言，考虑二次函数 $q(x) = \\frac{1}{2} x^\\top A x - b^\\top x$，其最小化解即为线性系统 $A x = b$ 的解。矩阵 $A$ 是在单位正方形上对负拉普拉斯算子采用标准二维二阶有限差分（FD）离散化的结果，其中使用齐次狄利克雷边界条件和 $5$-点模板。对于每个空间维度有 $N$ 个内部点的网格，此构造会产生一个 $N^2 \\times N^2$ 的稀疏矩阵；相应的向量 $b$ 取为元素全为 1 的 $N^2$ 维向量。\n\n你必须严格从基本原理出发——即二次函数 $q(x)$ 的定义、对于对称正定（SPD）矩阵 $A$，其最小化解满足 $A x = b$ 的性质，以及搜索方向关于 $A$-内积的共轭性——推导、实现并比较共轭梯度（CG）法及其左预处理变体。你的预处理器必须基于对 $A$ 结构的有原则的建模，在本研究中，你应使用由 $M = \\mathrm{diag}(A)$ 给出的 Jacobi 预处理器，该预处理器是对称正定的，且计算应用成本低廉。因此，你将通过预处理共轭梯度（PCG）法求解预处理后的系统 $M^{-1} A x = M^{-1} b$，并将其迭代次数与应用于 $A x = b$ 的无预处理的 CG 方法进行比较。\n\n实现约束和指标：\n- 使用 $x_0 = 0$ 进行初始化。\n- 使用相对残差范数 $\\|r_k\\|_2 / \\|b\\|_2$ 作为停止准则，其中 $r_k = b - A x_k$，容差为 $\\varepsilon = 10^{-8}$。\n- 对 CG 和 PCG，统计并报告达到 $\\|r_k\\|_2 / \\|b\\|_2 \\le \\varepsilon$ 所需的最小迭代次数。\n- 对于预处理，通过与 $A$ 的对角线进行逐元素除法来应用 $M^{-1}$。\n- 确保所有计算都以对 SPD 矩阵数学上一致的方式执行，不依赖于隐藏 CG 或 PCG 步骤的黑箱求解器。\n\n测试套件：\n- 使用内部网格尺寸 $N \\in \\{\\,2,\\,8,\\,16,\\,32\\,\\}$（这是每个空间维度的内部点数），分别产生大小为 $N^2 \\in \\{\\,4,\\,64,\\,256,\\,1024\\,\\}$ 的问题。\n- 对于每个 $N$，将 $b$ 设置为元素全为 1 的 $N^2$ 维向量，并根据所述的有限差分离散化方法构造 $A$。\n\n可量化的输出：\n- 对于每个测试用例，产生三个值：CG 的整数迭代次数，PCG 的整数迭代次数，以及定义为 CG 迭代次数与 PCG 迭代次数之比的浮点改进因子。\n- 最终输出必须将所有测试用例的结果聚合成单行。你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3]\"）。对于 $N=2, N=8, N=16, N=32$，顺序必须是：$[\\mathrm{it}_{\\mathrm{CG}}(2), \\mathrm{it}_{\\mathrm{PCG}}(2), I(2), \\mathrm{it}_{\\mathrm{CG}}(8), \\mathrm{it}_{\\mathrm{PCG}}(8), I(8), \\mathrm{it}_{\\mathrm{CG}}(16), \\mathrm{it}_{\\mathrm{PCG}}(16), I(16), \\mathrm{it}_{\\mathrm{CG}}(32), \\mathrm{it}_{\\mathrm{PCG}}(32), I(32)]$，其中每个 $\\mathrm{it}$ 是一个整数，每个 $I$ 是一个浮点数。\n\n你的任务：\n- 从最小化具有 SPD 矩阵 $A$ 的 $q(x)$ 的原理出发，使用 $A$-共轭方向，以及对于预处理，使用一个残差通过 $M^{-1}$ 映射的等价左预处理系统，来推导 CG 和 PCG 算法。\n- 实现这两种方法，构造有限差分矩阵 $A$ 和 Jacobi 预处理器 $M = \\mathrm{diag}(A)$，运行测试套件，并以上述确切格式输出指定的聚合结果。\n\n此问题不涉及物理单位或角度；因此，不需要进行单位转换。所有数值阈值和计数都是无单位的纯数字。", "solution": "该问题要求推导、实现并比较共轭梯度（CG）法和预处理共轭梯度（PCG）法，以求解一类特定的线性系统 $A x = b$。矩阵 $A$ 源于单位正方形上负拉普拉斯算子的有限差分离散化，而 PCG 的预处理器是 Jacobi 预处理器，即 $M = \\mathrm{diag}(A)$。\n\n### 第一部分：共轭梯度（CG）法的推导\n\n问题是找到二次函数 $q(x) = \\frac{1}{2} x^\\top A x - b^\\top x$ 的最小化解，其中 $A$ 是一个对称正定（SPD）矩阵。$q(x)$ 的梯度为 $\\nabla q(x) = A x - b$。在最小值点 $x^*$ 处，梯度为零，因此 $A x^* = b$。这就建立了最小化二次型与求解线性系统之间的联系。\n\n共轭梯度法是一种迭代算法，它生成一个收敛于 $x^*$ 的近似序列 $x_k$。从一个初始猜测 $x_0$ 开始，后续的迭代解按 $x_{k+1} = x_k + \\alpha_k p_k$ 计算，其中 $p_k$ 是一个搜索方向，$\\alpha_k$ 是为最小化 $q(x_{k+1})$ 而选择的步长。\n\n为了找到最优步长 $\\alpha_k$，我们对 $q(x_k + \\alpha p_k)$ 关于 $\\alpha$ 进行最小化：\n$$ \\frac{d}{d\\alpha} q(x_k + \\alpha p_k) = \\nabla q(x_k + \\alpha p_k)^\\top p_k = (A(x_k + \\alpha p_k) - b)^\\top p_k = 0 $$\n$$ (A x_k - b)^\\top p_k + \\alpha_k (A p_k)^\\top p_k = 0 $$\n利用残差的定义 $r_k = b - A x_k$，我们得到：\n$$ -r_k^\\top p_k + \\alpha_k p_k^\\top A p_k = 0 \\implies \\alpha_k = \\frac{r_k^\\top p_k}{p_k^\\top A p_k} $$\n\nCG 方法的核心是搜索方向 $\\{p_0, p_1, \\dots\\}$ 的选择。这些方向被选择为 $A$-共轭（或 $A$-正交）的，即对所有 $i \\neq j$ 都有 $p_i^\\top A p_j = 0$。此性质确保在一个 $n$ 维空间中，沿每个新方向 $p_k$ 的最小化不会破坏在先前方向 $p_0, \\dots, p_{k-1}$ 上已实现的最小化。理论上，这保证了在精确算术中最多 $n$ 步收敛。\n\n搜索方向是迭代构造的。一种计算上高效的方法是从当前残差 $r_{k+1}$ 和前一个搜索方向 $p_k$ 生成 $p_{k+1}$。我们以最速下降方向开始，即 $p_0 = r_0 = b - A x_0$。后续方向通过将新残差 $r_{k+1}$ 对前一个方向 $p_k$ 进行 $A$-正交化来形成：\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\n为了强制实现 $A$-共轭性，$p_{k+1}^\\top A p_k = 0$：\n$$ (r_{k+1} + \\beta_k p_k)^\\top A p_k = 0 \\implies \\beta_k = -\\frac{r_{k+1}^\\top A p_k}{p_k^\\top A p_k} $$\n通过利用 CG 方法的性质（例如 $r_{k+1} = r_k - \\alpha_k A p_k$ 和残差的正交性）进行代数操作，可以推导出更高效的 $\\alpha_k$ 和 $\\beta_k$ 计算公式：\n$$ \\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k} $$\n$$ \\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k} $$\n\n完整的 CG 算法如下：\n1. 初始化：$x_0 = 0$, $r_0 = b - A x_0 = b$, $p_0 = r_0$。\n2. 对于 $k = 0, 1, 2, \\dots$：\n   a. $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$\n   b. $x_{k+1} = x_k + \\alpha_k p_k$\n   c. $r_{k+1} = r_k - \\alpha_k A p_k$\n   d. 检查收敛性（例如，$\\|r_{k+1}\\|_2$ 是否足够小）。\n   e. $\\beta_k = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$\n   f. $p_{k+1} = r_{k+1} + \\beta_k p_k$\n\n### 第二部分：预处理共轭梯度（PCG）法的推导\n\nCG 的收敛速度取决于 $A$ 的条件数 $\\kappa(A)$。预处理旨在将系统转换为一个具有更小条件数的等价系统。设 $M$ 是一个近似于 $A$ 的 SPD 矩阵，称为预处理器，且 $M^{-1}$ 易于计算。\n\n我们将系统 $A x = b$ 转换为一种适合 CG 的形式。我们要求新的系统矩阵是 SPD 的。设 $M = C C^\\top$ 是预处理器的一个分解（例如 Cholesky 分解）。我们可以写出：\n$$ C^{-1} A (C^\\top)^{-1} C^\\top x = C^{-1} b $$\n令 $\\hat{A} = C^{-1} A (C^\\top)^{-1}$，$\\hat{x} = C^\\top x$，以及 $\\hat{b} = C^{-1} b$。系统变为 $\\hat{A} \\hat{x} = \\hat{b}$。新矩阵 $\\hat{A}$ 是 SPD 的，因此我们可以对其应用 CG 算法。\n\n对带帽系统应用 CG 得到：\n- $\\hat{r}_k = \\hat{b} - \\hat{A} \\hat{x}_k$\n- $\\hat{\\alpha}_k = \\frac{\\hat{r}_k^\\top \\hat{r}_k}{\\hat{p}_k^\\top \\hat{A} \\hat{p}_k}$\n- $\\hat{x}_{k+1} = \\hat{x}_k + \\hat{\\alpha}_k \\hat{p}_k$\n- $\\hat{r}_{k+1} = \\hat{r}_k - \\hat{\\alpha}_k \\hat{A} \\hat{p}_k$\n- $\\hat{\\beta}_k = \\frac{\\hat{r}_{k+1}^\\top \\hat{r}_{k+1}}{\\hat{r}_k^\\top \\hat{r}_k}$\n- $\\hat{p}_{k+1} = \\hat{r}_{k+1} + \\hat{\\beta}_k \\hat{p}_k$\n\n为了得到一个实用的算法，我们将这些更新转换回原始变量 $x_k$。我们定义 $r_k = b - A x_k$ 和一个辅助向量 $z_k = M^{-1} r_k$。它们之间的关系是：\n- $\\hat{r}_k = C^{-1} r_k$\n- $\\hat{x}_k = C^\\top x_k$\n- 我们将原始空间中的搜索方向定义为 $p_k = (C^\\top)^{-1} \\hat{p}_k$。\n\nCG 算法中的各项变为：\n- $\\hat{\\alpha}_k$ 的分子：$\\hat{r}_k^\\top \\hat{r}_k = (C^{-1} r_k)^\\top (C^{-1} r_k) = r_k^\\top (C C^\\top)^{-1} r_k = r_k^\\top M^{-1} r_k = r_k^\\top z_k$。\n- $\\hat{\\alpha}_k$ 的分母：$\\hat{p}_k^\\top \\hat{A} \\hat{p}_k = (C^\\top p_k)^\\top (C^{-1} A (C^\\top)^{-1}) (C^\\top p_k) = p_k^\\top A p_k$。\n- 所以，$\\alpha_k = \\frac{r_k^\\top z_k}{p_k^\\top A p_k}$。\n- $\\hat{\\beta}_k$ 的分子：$\\hat{r}_{k+1}^\\top \\hat{r}_{k+1} = r_{k+1}^\\top z_{k+1}$。\n- $\\hat{\\beta}_k$ 的分母：$\\hat{r}_k^\\top \\hat{r}_k = r_k^\\top z_k$。\n- 所以，$\\beta_k = \\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k}$。\n- $p_{k+1}$ 的更新：$(C^\\top p_{k+1}) = \\hat{p}_{k+1} = \\hat{r}_{k+1} + \\beta_k \\hat{p}_k = C^{-1} r_{k+1} + \\beta_k (C^\\top p_k)$。两边乘以 $(C^\\top)^{-1}$ 得到 $p_{k+1} = M^{-1} r_{k+1} + \\beta_k p_k = z_{k+1} + \\beta_k p_k$。\n\n完整的 PCG 算法如下：\n1. 初始化：$x_0 = 0$, $r_0 = b - A x_0 = b$。\n2. 求解 $M z_0 = r_0$。\n3. 设置 $p_0 = z_0$。\n4. 对于 $k = 0, 1, 2, \\dots$：\n   a. $\\alpha_k = \\frac{r_k^\\top z_k}{p_k^\\top A p_k}$\n   b. $x_{k+1} = x_k + \\alpha_k p_k$\n   c. $r_{k+1} = r_k - \\alpha_k A p_k$\n   d. 检查收敛性。\n   e. 求解 $M z_{k+1} = r_{k+1}$。\n   f. $\\beta_k = \\frac{r_{k+1}^\\top z_{k+1}}{r_k^\\top z_k}$\n   g. $p_{k+1} = z_{k+1} + \\beta_k p_k$\n\n### 第三部分：系统特定分析与实现策略\n\n矩阵 $A$ 是在具有 $N \\times N$ 个内部点的网格上，对负拉普拉斯算子进行 5 点有限差分离散化得到的 $N^2 \\times N^2$ 矩阵。网格间距为 $h=1/(N+1)$。在内部点 $(i,j)$ 处的方程是：\n$$ \\frac{1}{h^2} (4 u_{ij} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}) = b_{ij} $$\n这种结构意味着矩阵 $A$ 的对角线元素都是相同的：$A_{kk} = 4/h^2 = 4(N+1)^2$。\n\nJacobi 预处理器定义为 $M = \\mathrm{diag}(A)$。对于这个特定的矩阵 $A$，预处理器是单位矩阵的一个标量倍数：\n$$ M = 4(N+1)^2 I = cI $$\n其中 $c=4(N+1)^2$ 是一个常数，$I$ 是单位矩阵。\n\n我们来分析这个预处理器的效果。PCG 算法等价于对一个矩阵为 $\\hat{A} = C^{-1} A (C^\\top)^{-1}$ 的系统应用 CG。当 $M=cI$ 时，其 Cholesky 因子为 $C=\\sqrt{c}I$。因此，\n$$ \\hat{A} = (\\sqrt{c}I)^{-1} A ((\\sqrt{c}I)^\\top)^{-1} = \\frac{1}{\\sqrt{c}}I \\cdot A \\cdot \\frac{1}{\\sqrt{c}}I = \\frac{1}{c}A $$\n预处理后矩阵的条件数为 $\\kappa(\\hat{A}) = \\kappa(\\frac{1}{c}A) = \\kappa(A)$。该预处理器并没有改变条件数。\n\n此外，由 CG 生成的迭代序列 $x_k$ 在整个系统进行缩放时保持不变。详细分析表明，在精确算术中，使用 $M=cI$ 的 PCG 算法生成的迭代序列 $\\{x_k\\}$ 和残差序列 $\\{r_k\\}$ 与标准 CG 算法生成的序列是相同的。因此，两种算法应该在相同的迭代次数内收敛。理论上，改进因子恰好为 $1$。\n\n实现将构造稀疏矩阵 $A$，然后实现推导出的 CG 和 PCG 算法。PCG 的实现将通过除以 $A$ 的对角线来应用 $M^{-1}$。通过在浮点算术中运行代码获得的数值结果预计将证实这一理论发现，即 CG 和 PCG 的迭代次数将相同或极其接近。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\n\ndef solve():\n    \"\"\"\n    Derives, implements, and compares the Conjugate Gradient (CG) and Preconditioned\n    Conjugate Gradient (PCG) methods for a 2D finite difference problem.\n    \"\"\"\n\n    test_cases = [2, 8, 16, 32]\n    tolerance = 1e-8\n    results = []\n\n    def create_laplace_matrix(N):\n        \"\"\"\n        Constructs the sparse matrix for the 2D negative Laplacian using a 5-point stencil.\n        -N: Number of interior grid points in one dimension.\n        \"\"\"\n        n_sq = N * N\n        h = 1.0 / (N + 1)\n        \n        # Use LIL format for efficient construction\n        A = sparse.lil_matrix((n_sq, n_sq))\n        \n        for k in range(n_sq):\n            A[k, k] = 4.0\n            # West neighbor\n            if k % N > 0:\n                A[k, k - 1] = -1.0\n            # East neighbor\n            if k % N  N - 1:\n                A[k, k + 1] = -1.0\n            # South neighbor\n            if k >= N:\n                A[k, k - N] = -1.0\n            # North neighbor\n            if k  n_sq - N:\n                A[k, k + N] = -1.0\n        \n        A = A / (h * h)\n        # Convert to CSR for fast matrix-vector products\n        return A.tocsr()\n\n    def cg_solver(A, b, tol):\n        \"\"\"\n        Solves Ax=b using the Conjugate Gradient method.\n        - A: SPD sparse matrix.\n        - b: Right-hand side vector.\n        - tol: Relative residual tolerance for stopping.\n        Returns the number of iterations.\n        \"\"\"\n        n_sq = b.shape[0]\n        x = np.zeros(n_sq)\n        r = b.copy()  # Since x_0=0, r_0 = b - Ax_0 = b\n        p = r.copy()\n        rs_old = np.dot(r, r)\n        b_norm = np.linalg.norm(b)\n\n        if b_norm == 0.0:\n            return 0\n\n        # Maximum iterations to prevent infinite loops, e.g., 2*n_sq\n        max_iter = 2 * n_sq\n        for i in range(max_iter):\n            Ap = A.dot(p)\n            alpha = rs_old / np.dot(p, Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new = np.dot(r, r)\n            \n            if np.sqrt(rs_new) / b_norm = tol:\n                return i + 1\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n        return max_iter\n\n    def pcg_solver(A, b, M_diag, tol):\n        \"\"\"\n        Solves Ax=b using the Preconditioned Conjugate Gradient method.\n        - A: SPD sparse matrix.\n        - b: Right-hand side vector.\n        - M_diag: The diagonal of the Jacobi preconditioner M=diag(A).\n        - tol: Relative residual tolerance for stopping.\n        Returns the number of iterations.\n        \"\"\"\n        n_sq = b.shape[0]\n        x = np.zeros(n_sq)\n        r = b.copy()\n        b_norm = np.linalg.norm(b)\n\n        if b_norm == 0.0:\n            return 0\n            \n        z = r / M_diag\n        p = z.copy()\n        rz_old = np.dot(r, z)\n\n        max_iter = 2 * n_sq\n        for i in range(max_iter):\n            Ap = A.dot(p)\n            alpha = rz_old / np.dot(p, Ap)\n            x += alpha * p\n            r -= alpha * Ap\n\n            if np.linalg.norm(r) / b_norm = tol:\n                return i + 1\n\n            z = r / M_diag\n            rz_new = np.dot(r, z)\n            \n            beta = rz_new / rz_old\n            p = z + beta * p\n            rz_old = rz_new\n            \n        return max_iter\n\n    for N in test_cases:\n        n_sq = N * N\n        A = create_laplace_matrix(N)\n        b = np.ones(n_sq)\n\n        # Solve with CG\n        it_cg = cg_solver(A, b, tolerance)\n\n        # Solve with PCG (Jacobi)\n        M_diag = A.diagonal()\n        it_pcg = pcg_solver(A, b, M_diag, tolerance)\n\n        # Calculate improvement factor\n        if it_pcg > 0:\n            improvement = float(it_cg) / it_pcg\n        else: # Should not happen if it_cg > 0\n            improvement = 1.0\n\n        results.extend([it_cg, it_pcg, improvement])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' if isinstance(x, float) else str(x) for x in results)}]\")\n\nsolve()\n```", "id": "3111608"}]}