{"hands_on_practices": [{"introduction": "在深入探讨如何选择合适的步长之前，我们首先需要理解一个基本前提：搜索方向必须是下降方向。本练习将通过一个简单的二次函数示例 [@problem_id:3189981]，揭示当选择一个非下降方向时，Armijo 条件为何会失效。这个练习旨在强调在执行线搜索前检验方向有效性的重要性，为你后续的学习打下坚实的基础。", "problem": "考虑在点 $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 和搜索方向 $p=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 处，对二次连续可微函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$（定义为 $f(x)=\\tfrac{1}{2}\\|x\\|^{2}$）进行计算。充分下降（Armijo）条件规定，对于给定的常数 $c_{1}\\in(0,1)$，一个可接受的步长 $\\alpha0$ 必须满足\n$$\nf(x_{0}+\\alpha p)\\le f(x_{0})+c_{1}\\,\\alpha\\,\\nabla f(x_{0})^{\\top}p.\n$$\n仅使用 Armijo 条件的定义和基础微积分，分析函数 $f$ 沿直线 $x(\\alpha)=x_{0}+\\alpha p$ 的行为以及由 $p^{\\top}\\nabla^{2}f(x_{0})p$ 量化的沿 $p$ 方向的曲率的作用。证明沿此方向 $p$ 的曲率是非负的，并且 $p$ 不是一个下降方向。然后，计算阈值\n$$\nc_{1}^{\\star}=\\inf_{\\alpha0}\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p},\n$$\n这是原则上能使 Armijo 不等式对某个 $\\alpha0$ 成立的最小常数。判断在此设置下，对于任何 $c_{1}\\in(0,1)$，是否存在 $\\alpha0$ 能满足 Armijo 条件，并报告 $c_{1}^{\\star}$ 的单一数值。你的最终答案必须是一个无单位的实数。无需四舍五入。", "solution": "该问题是有效的，因为它科学地基于数值优化的原理，是适定的，具有明确的目标，并为得出唯一解提供了所有必要的信息。\n\n首先，我们确定函数 $f(x)=\\frac{1}{2}\\|x\\|^{2}$ 在点 $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$ 处的性质。该函数为 $f(x_1, x_2) = \\frac{1}{2}(x_1^2 + x_2^2)$。\n$f$ 的梯度由 $\\nabla f(x) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = x$ 给出。\n$f$ 的海森矩阵是 $\\nabla^{2}f(x) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。对于所有 $x \\in \\mathbb{R}^2$，海森矩阵是常数。\n\n在特定点 $x_{0}=\\begin{pmatrix}1\\\\0\\end{pmatrix}$，我们有：\n函数值为 $f(x_{0}) = \\frac{1}{2}(1^2 + 0^2) = \\frac{1}{2}$。\n梯度是 $\\nabla f(x_{0}) = x_{0} = \\begin{pmatrix}1\\\\0\\end{pmatrix}$。\n海森矩阵是 $\\nabla^{2}f(x_{0}) = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$。\n\n问题要求我们考虑搜索方向 $p=\\begin{pmatrix}1\\\\0\\end{pmatrix}$。如果方向 $p$ 满足 $\\nabla f(x_{0})^{\\top}p  0$，则它是一个下降方向。我们来计算这个量：\n$$\n\\nabla f(x_{0})^{\\top}p = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = (1)(1) + (0)(0) = 1.\n$$\n由于 $\\nabla f(x_{0})^{\\top}p = 1  0$，方向 $p$ 不是一个下降方向。它是一个上升方向，意味着函数值从 $x_{0}$ 开始沿此方向初始是增加的。\n\n接下来，我们分析 $f$ 在 $x_{0}$ 处沿方向 $p$ 的曲率。曲率由二次型 $p^{\\top}\\nabla^{2}f(x_{0})p$ 给出。\n$$\np^{\\top}\\nabla^{2}f(x_{0})p = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}1  0\\end{pmatrix} \\begin{pmatrix}1\\\\0\\end{pmatrix} = 1.\n$$\n曲率为 $1$，这是一个正值，因此证实了曲率是非负的。\n\n现在，我们计算阈值 $c_{1}^{\\star}$，其定义为：\n$$\nc_{1}^{\\star}=\\inf_{\\alpha0}\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p}.\n$$\n我们来计算这个表达式中的各项。沿搜索方向的点是 $x(\\alpha) = x_{0}+\\alpha p = \\begin{pmatrix}1\\\\0\\end{pmatrix} + \\alpha\\begin{pmatrix}1\\\\0\\end{pmatrix} = \\begin{pmatrix}1+\\alpha\\\\0\\end{pmatrix}$。\n在此点处的函数值为：\n$$\nf(x_{0}+\\alpha p) = \\frac{1}{2}\\|x_{0}+\\alpha p\\|^2 = \\frac{1}{2}((1+\\alpha)^2 + 0^2) = \\frac{1}{2}(1+\\alpha)^2 = \\frac{1}{2}(1+2\\alpha+\\alpha^2).\n$$\n定义 $c_{1}^{\\star}$ 的分式的分子是：\n$$\nf(x_{0}+\\alpha p)-f(x_{0}) = \\frac{1}{2}(1+2\\alpha+\\alpha^2) - \\frac{1}{2} = \\alpha + \\frac{1}{2}\\alpha^2.\n$$\n分母是：\n$$\n\\alpha\\,\\nabla f(x_{0})^{\\top}p = \\alpha(1) = \\alpha.\n$$\n将这些代入 $c_{1}^{\\star}$ 的表达式中，我们得到下确界内的比值为：\n$$\n\\frac{\\alpha + \\frac{1}{2}\\alpha^2}{\\alpha}.\n$$\n由于我们考虑的是 $\\alpha  0$，我们可以通过分子分母同除以 $\\alpha$ 来化简此分式：\n$$\n\\frac{\\alpha(1 + \\frac{1}{2}\\alpha)}{\\alpha} = 1 + \\frac{1}{2}\\alpha.\n$$\n现在我们必须求出此表达式在所有 $\\alpha  0$ 上的下确界：\n$$\nc_{1}^{\\star} = \\inf_{\\alpha0} \\left(1 + \\frac{1}{2}\\alpha\\right).\n$$\n函数 $g(\\alpha) = 1 + \\frac{1}{2}\\alpha$ 是一个关于 $\\alpha$ 的严格递增函数。它在区间 $(0, \\infty)$ 上的下确界是当 $\\alpha$ 从右侧趋近于 $0$ 时的极限：\n$$\nc_{1}^{\\star} = \\lim_{\\alpha \\to 0^+} \\left(1 + \\frac{1}{2}\\alpha\\right) = 1.\n$$\n阈值为 $c_{1}^{\\star}=1$。\n\n最后，我们必须判断对于任意 $c_{1} \\in (0,1)$，是否存在 $\\alpha  0$ 能满足 Armijo 条件。Armijo 条件是：\n$$\nf(x_{0}+\\alpha p) \\le f(x_{0})+c_{1}\\,\\alpha\\,\\nabla f(x_{0})^{\\top}p.\n$$\n对于 $\\alpha  0$ 且 $\\nabla f(x_0)^\\top p \\ne 0$，我们可以将其重新整理为：\n$$\n\\frac{f(x_{0}+\\alpha p)-f(x_{0})}{\\alpha\\,\\nabla f(x_{0})^{\\top}p} \\le c_1.\n$$\n我们已经计算出左边等于 $1 + \\frac{1}{2}\\alpha$。所以该条件变为：\n$$\n1 + \\frac{1}{2}\\alpha \\le c_1.\n$$\n我们已知 $\\alpha  0$，这意味着 $\\frac{1}{2}\\alpha  0$，因此 $1 + \\frac{1}{2}\\alpha  1$。\nArmijo 条件要求参数 $c_1$ 在区间 $(0,1)$ 内，即 $c_1  1$。\n因此，不等式 $1 + \\frac{1}{2}\\alpha \\le c_1$ 要求一个严格大于 $1$ 的数小于或等于一个严格小于 $1$ 的数。这是一个矛盾。\n因此，对于任何 $c_1 \\in (0,1)$，不存在能够满足 Armijo 条件的步长 $\\alpha  0$。这是 $p$ 不是下降方向的直接结果。所要求的数值是 $c_{1}^{\\star}$。", "answer": "$$\\boxed{1}$$", "id": "3189981"}, {"introduction": "确认了下降方向后，下一个关键问题是如何确保我们能找到一个满足充分下降条件的步长 $\\alpha$。本练习 [@problem_id:3189999] 将引导你从函数的全局属性（即梯度的 Lipschitz 连续性）出发，推导出一个理论上保证满足 Armijo 条件的步长选择。这个过程不仅展示了理论分析在算法设计中的力量，也让我们思考这种基于“最坏情况”的保守选择在实践中意味着什么。", "problem": "设 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是一个连续可微函数，其梯度是 $L$-利普希茨连续的，即存在 $L0$ 使得对于所有 $x,y\\in\\mathbb{R}^{n}$ 都有 $\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|$。考虑单步最速下降 $x^{+}=x-\\alpha \\nabla f(x)$，其中步长 $\\alpha0$。参数为 $c_{1}\\in(0,1)$ 的Armijo充分下降条件要求，沿着下降方向 $d$ 必须满足 $f(x+\\alpha d)\\le f(x)+c_{1}\\alpha \\nabla f(x)^{\\top}d$。假设方向为最速下降方向 $d=-\\nabla f(x)$，并回想梯度的 $L$-利普希茨连续性意味着所谓的下降引理。\n\n仅使用这些基本事实，推导出一个关于 $\\alpha$ 的显式上界，以保证Armijo充分下降条件对于最速下降步成立。然后，将此界特殊化，得出一个仅依赖于 $L$ 的 $\\alpha$ 的单一选择，该选择能保证对于任意 $c_{1}\\in(0,\\tfrac{1}{2}]$、任意 $x$ 和 $\\nabla f(x)$，Armijo条件都成立。用包含 $L$ 的闭式解析表达式表示你的最终答案。最后，简要解释与实践中Armijo条件可能允许的步长相比，这个固定的选择是否保守，并将你的解释与诸如Wolfe和Goldstein条件等曲率条件的作用联系起来。最终答案无需进行数值四舍五入。", "solution": "该问题要求，在给定一个梯度为$L$-利普希茨连续的函数$f$的条件下，为最速下降法推导步长$\\alpha$的一个上界，以保证Armijo充分下降条件成立。然后，要求基于此界给出一个特定的固定步长，并简要讨论其實际意义。\n\n首先，我们将给定信息形式化。\n函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ 是连续可微的。\n其梯度 $\\nabla f$ 是 $L$-利普希茨连续的，即存在常数 $L0$，使得对于所有 $x, y \\in \\mathbb{R}^{n}$：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\| \\le L \\|x-y\\|\n$$\n此性质蕴含了下降引理（也称为二次上界引理），其表述为：\n$$\nf(y) \\le f(x) + \\nabla f(x)^{\\top}(y-x) + \\frac{L}{2} \\|y-x\\|^2\n$$\n优化步骤为最速下降步：\n$$\nx^{+} = x - \\alpha \\nabla f(x)\n$$\n其中 $\\alpha  0$ 是步长。这对应于在方向 $d = -\\nabla f(x)$ 上的步长 $\\alpha d$。\n\n对于沿下降方向 $d$ 的步长 $\\alpha$，Armijo充分下降条件为：\n$$\nf(x+\\alpha d) \\le f(x) + c_1 \\alpha \\nabla f(x)^{\\top} d\n$$\n其中 $c_1 \\in (0, 1)$。将最速下降方向 $d = -\\nabla f(x)$ 代入此条件，得到：\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + c_1 \\alpha \\nabla f(x)^{\\top} (-\\nabla f(x))\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) - c_1 \\alpha \\|\\nabla f(x)\\|^2\n$$\n我们的目标是找到 $\\alpha$ 的一个上界，以保证该不等式成立。我们使用下降引理。令 $y = x^{+} = x - \\alpha \\nabla f(x)$。应用引理可得：\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + \\nabla f(x)^{\\top}((x - \\alpha \\nabla f(x)) - x) + \\frac{L}{2} \\|(x - \\alpha \\nabla f(x)) - x\\|^2\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) + \\nabla f(x)^{\\top}(-\\alpha \\nabla f(x)) + \\frac{L}{2} \\|-\\alpha \\nabla f(x)\\|^2\n$$\n$$\nf(x - \\alpha \\nabla f(x)) \\le f(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2\n$$\n此不等式为下一次迭代中 $f$ 的值提供了一个上界。如果该上界小于或等于Armijo不等式的右侧，则Armijo条件将得到满足。也就是说，我们要求：\n$$\nf(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2 \\le f(x) - c_1 \\alpha \\|\\nabla f(x)\\|^2\n$$\n假设我们不在驻点（即 $\\nabla f(x) \\neq 0$），我们可以从两边减去 $f(x)$，然后除以正标量 $\\|\\nabla f(x)\\|^2$：\n$$\n-\\alpha + \\frac{L\\alpha^2}{2} \\le -c_1 \\alpha\n$$\n由于根据定义 $\\alpha  0$，我们可以两边同除以 $\\alpha$：\n$$\n-1 + \\frac{L\\alpha}{2} \\le -c_1\n$$\n现在，我们求解 $\\alpha$：\n$$\n\\frac{L\\alpha}{2} \\le 1 - c_1\n$$\n$$\n\\alpha \\le \\frac{2(1-c_1)}{L}\n$$\n这就是保证对于给定 $c_1 \\in (0, 1)$，Armijo条件成立的 $\\alpha$ 的显式上界。\n\n接下来，我们必须将此界特殊化，找到一个仅依赖于 $L$ 的 $\\alpha$ 的单一选择，以保证对于任意 $c_1 \\in (0, \\frac{1}{2}]$ 该条件都成立。为确保不等式 $\\alpha \\le \\frac{2(1-c_1)}{L}$ 对该区间内所有的 $c_1$ 都成立，$\\alpha$ 必须小于或等于该区间上右侧表达式的最小值。令 $g(c_1) = \\frac{2(1-c_1)}{L}$。关于 $c_1$ 的导数是 $g'(c_1) = -\\frac{2}{L}$，由于 $L0$ 该导数为负。因此，$g(c_1)$ 是关于 $c_1$ 的递减函数。在区间 $(0, \\frac{1}{2}]$ 上，$g(c_1)$ 的最小值将在 $c_1$ 的最大值处取得，即 $c_1 = \\frac{1}{2}$。该上界的最小值为：\n$$\n\\min_{c_1 \\in (0, 1/2]} \\frac{2(1-c_1)}{L} = \\frac{2(1 - \\frac{1}{2})}{L} = \\frac{2(\\frac{1}{2})}{L} = \\frac{1}{L}\n$$\n因此，任何满足 $0  \\alpha \\le \\frac{1}{L}$ 的 $\\alpha$ 都将对任意 $c_1 \\in (0, \\frac{1}{2}]$ 的选择满足Armijo条件。问题要求的是一个单一的 $\\alpha$ 选择；对于任何函数 $f$（具有$L$-利普希茨梯度）和任何 $x$ 都适用的最通用且限制最少的固定选择是此范围的上限，即 $\\alpha = \\frac{1}{L}$。\n\n最后，我们被要求评论这个固定的选择是否保守。步长 $\\alpha = \\frac{1}{L}$ 确实非常保守。这个值是根据全局利普希茨常数 $L$ 推导出的最坏情况保证，它反映了函数 $f$ 在其整个定义域上的最大曲率。在许多区域，局部曲率可能远小于 $L$，这意味着在Armijo条件下可能允许大得多的步长，从而导致更快的收敛。实际的线搜索方法，例如回溯法，利用了这一点，它们从一个较大的试探步长开始，然后迭代地减小它，直到满足Armijo条件为止，从而在每次迭代中使步长适应函数的局部性质。\n\n这与曲率条件（如Wolfe或Goldstein条件）的作用有关。Armijo条件（一个充分下降条件）本身只提供了可接受步长的上界。它并不能防止步长 $\\alpha$ 过小。一个算法可能用一个极小的 $\\alpha$ 滿足Armijo条件，但在朝向最小值方向上取得的进展微不足道。为了确保有意义的进展，Armijo条件通常与一个曲率条件配对使用。第二个Wolfe条件，$\\nabla f(x+\\alpha d)^{\\top}d \\ge c_2 \\nabla f(x)^{\\top}d$（其中 $c_2 \\in (c_1, 1)$），强制要求新点的斜率比初始斜率平缓，这有效地为可接受步长设置了一个下界，并排除了病态的小步长。Goldstein条件同样从上下两方面限定了可接受的步长。因此，尽管我们固定的步长 $\\alpha=\\frac{1}{L}$ 保证了函数值的下降，但它可能是一个过于谨慎的步长，一个采用曲率条件的更复杂的线搜索方法可能会拒绝它，而选择一个更大、更有效的步长。", "answer": "$$\n\\boxed{\\frac{1}{L}}\n$$", "id": "3189999"}, {"introduction": "理论为我们提供了指导，但在实际的数值计算中，我们往往会遇到理论未曾详述的陷阱。本练习 [@problem_id:3143424] 将带你从理论走向实践，通过编写代码来实现包含 Armijo 条件和曲率条件的强 Wolfe 条件线搜索。你将亲手构造一个由于曲率参数 $c_2$ 选择不当而导致算法停滞的场景，并实现一种自适应策略来解决这个问题，从而深刻理解鲁棒线搜索算法的设计精髓。", "problem": "考虑一个二次连续可微函数 $f:\\mathbb{R}^n\\rightarrow\\mathbb{R}$，以及在点 $x\\in\\mathbb{R}^n$ 处的一个下降方向 $p\\in\\mathbb{R}^n$。线搜索选择一个步长 $\\alpha0$ 以沿直线 $x+\\alpha p$ 减小 $f$ 的值。被称为强 Wolfe 条件的非精确线搜索包含两个不等式，由常数 $c_1$ 和 $c_2$（满足 $0c_1c_21$）参数化：充分下降（Armijo）条件 $f(x+\\alpha p)\\le f(x)+c_1\\,\\alpha\\,\\nabla f(x)^\\top p$，以及曲率条件 $\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert \\le c_2\\,\\lvert \\nabla f(x)^\\top p\\rvert$。许多实际实现通过将 $\\alpha$ 乘以一个在 $(0,1)$ 之间的因子来进行回溯，直到满足这些条件。然而，如果 $c_2$ 选择不当，且算法只是一味减小 $\\alpha$，曲率条件可能迫使步长停滞在一个最小阈值上。\n\n你的任务是：\n\n1. 使用可微性和梯度的基本定义，结合强 Wolfe 充分下降和曲率条件，实现一个测试这两个条件的回溯线搜索。回溯必须从一个初始值 $\\alpha_00$ 开始，并在每次条件不满足时乘以一个因子 $\\beta\\in(0,1)$。为保证终止，如果 $\\alpha$ 低于预设的最小值 $\\alpha_{\\min}0$，则程序必须返回 $\\alpha_{\\min}$。\n\n2. 通过选择一个正定二次函数 $f(x)=\\tfrac{1}{2}\\,x^\\top Q x$（其中 $Q\\in\\mathbb{R}^{2\\times 2}$ 是对称正定矩阵，因此 $\\nabla f(x)=Qx$）、一个初始点 $x_0\\in\\mathbb{R}^2$ 以及最速下降方向 $p_0=-\\nabla f(x_0)$ 来构建一个具体示例。此设置使得所有相关量都可以从第一性原理计算得出，并确保了科学真实性。在此设置中，定义 $Q=\\mathrm{diag}(100,1)$ 和 $x_0=(1,1)^\\top$。使用常数 $c_1=10^{-4}$，$\\alpha_0=1$，$\\beta=0.5$ 和 $\\alpha_{\\min}=10^{-6}$。\n\n3. 用此示例证明，不当的 $c_2$ 选择会导致纯回溯法下的步长停滞。具体来说，数值上表明，对于较小的 $c_2$ 值，算法会返回 $\\alpha_{\\min}$，因为仅减小 $\\alpha$ 的回溯法无法满足曲率条件。\n\n4. 提出并实现一种基于观测到的曲率比率 $r(\\alpha)=\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert / \\lvert \\nabla f(x)^\\top p\\rvert$ 的 $c_2$ 自动调整策略。当充分下降条件满足但曲率条件失败时，将 $c_2$ 更新为一个依赖于观测比率的宽松值 $c_2^{\\mathrm{new}}$。使用规则 $c_2^{\\mathrm{new}}=\\min\\{0.99,\\max\\{c_1+10^{-12},\\,r(\\alpha)+0.05\\}\\}$，然后用更新后的 $c_2$ 在当前的 $\\alpha$ 处重新检查曲率条件。这种简单的自动调整能保持 $c_2\\in(c_1,1)$，并利用观测到的曲率比率来避免停滞。\n\n5. 作为参考，还需计算二次函数沿最速下降方向的精确线搜索步长，这可以通过使用 $f$、$\\nabla f$ 的定义以及 $Q$ 是对称正定的事实，最小化 $f(x+\\alpha p)$ 关于 $\\alpha$ 的函数来得到。\n\n在一个程序中实现所有计算，并评估以下参数值的测试套件，每个测试用例产生一个等于所接受步长的浮点数输出：\n- 测试用例 1（理想路径）：$c_2=0.9$，无自动调整。\n- 测试用例 2（停滞）：$c_2=0.1$，无自动调整。\n- 测试用例 3（自动调整恢复）：$c_2=0.1$，启用自动调整。\n- 测试用例 4（边缘情况，更严重）：$c_2=0.001$，无自动调整。\n- 测试用例 5（冗余调整）：$c_2=0.9$，启用自动调整。\n- 测试用例 6（基准精确值）：计算二次函数 $f$ 沿 $p_0$ 的精确线搜索步长。\n\n你的程序应生成一行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[result_1,result_2,\\dots]$）。此问题不涉及任何物理单位、角度或百分比；所有输出均为原始实数。", "solution": "该问题要求实现并分析一种基于强 Wolfe 条件的回溯线搜索算法，展示一种常见的失败模式及其纠正策略。解决方案是基于多元微积分和数值优化的基本原理来制定的。\n\n### 线搜索原理与强 Wolfe 条件\n\n在梯度下降或拟牛顿法等迭代优化方法中，从当前点 $x$ 沿下降方向 $p$（即函数 $f$ 初始下降的方向，$\\nabla f(x)^\\top p  0$）进行更新。新点为 $x_{\\text{new}} = x + \\alpha p$，其中 $\\alpha  0$ 是步长。线搜索的目标是找到一个合适的 $\\alpha$，使 $f$ 的值得到充分的减小。\n\n**强 Wolfe 条件**为可接受的步长 $\\alpha$ 提供了一套标准准则。它们包括两个不等式：\n1.  **充分下降（Armijo）条件**：此条件确保步长 $\\alpha$ 能使函数值实现明显的下降，该下降量与步长和方向导数成比例。\n    $$f(x+\\alpha p)\\le f(x)+c_1\\,\\alpha\\,\\nabla f(x)^\\top p$$\n    此处，$c_1 \\in (0, 1)$ 是一个常数，通常很小（例如 $10^{-4}$）。由于 $p$ 是下降方向，$\\nabla f(x)^\\top p  0$，因此右侧构成了 $f(x+\\alpha p)$ 的一个上界，这个界比单纯的 $f(x+\\alpha p)  f(x)$ 更为严格。\n\n2.  **曲率条件**：此条件确保步长不会过短。它要求在新点 $x+\\alpha p$ 处函数沿方向 $p$ 的斜率比在原点 $x$ 处的斜率更平缓，从而保证取得进展。强 Wolfe 版本的形式是：\n    $$\\lvert \\nabla f(x+\\alpha p)^\\top p\\rvert \\le c_2\\,\\lvert \\nabla f(x)^\\top p\\rvert$$\n    此处，$c_2 \\in (c_1, 1)$ 是一个常数。此条件防止算法在梯度仍然很大的区域采取过小的步长。\n\n### 二次模型问题\n\n为了分析该算法，我们使用一个标准的测试函数：一个正定二次型 $f(x)=\\tfrac{1}{2}\\,x^\\top Q x$。对于这类函数，其梯度是线性的：$\\nabla f(x) = Qx$。这种解析上的简单性使得所有相关量都能被精确计算。\n具体参数如下：\n-   $Q = \\mathrm{diag}(100, 1)$，一个 $2 \\times 2$ 对称正定矩阵。\n-   $x_0 = (1, 1)^\\top$。\n-   方向是最速下降方向，$p_0 = -\\nabla f(x_0) = -Qx_0$。\n\n我们来计算初始值：\n-   $\\nabla f(x_0) = Qx_0 = \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 100 \\\\ 1 \\end{pmatrix}$。\n-   $p_0 = -\\nabla f(x_0) = \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix}$。\n-   在 $x_0$ 处的方向导数为 $\\nabla f(x_0)^\\top p_0 = \\begin{pmatrix} 100  1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = -10000 - 1 = -10001$。\n\n在新点 $x_0+\\alpha p_0$ 处的梯度是 $\\nabla f(x_0+\\alpha p_0) = Q(x_0+\\alpha p_0)$。此新点处的方向导数为 $\\nabla f(x_0+\\alpha p_0)^\\top p_0 = (Q(x_0+\\alpha p_0))^\\top p_0 = (Qx_0 + \\alpha Qp_0)^\\top p_0 = \\nabla f(x_0)^\\top p_0 + \\alpha(Qp_0)^\\top p_0$。使用 $p_0 = -Qx_0$，这可以简化为 $\\nabla f(x_0)^\\top p_0 - \\alpha(Q(Qx_0))^\\top p_0 = \\nabla f(x_0)^\\top p_0 - \\alpha(Q^2x_0)^\\top p_0$。表达式 $\\nabla f(x_0)^\\top p_0 + \\alpha p_0^\\top Q p_0$ 更为直接。\n数值上：\n$p_0^\\top Q p_0 = \\begin{pmatrix} -100  -1 \\end{pmatrix} \\begin{pmatrix} 100  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -10000  -1 \\end{pmatrix} \\begin{pmatrix} -100 \\\\ -1 \\end{pmatrix} = 1000000 + 1 = 1000001$。\n因此，$\\nabla f(x_0+\\alpha p_0)^\\top p_0 = -10001 + \\alpha (1000001)$。\n\n### 精确线搜索（基准）\n\n对于二次函数，最小化 $\\phi(\\alpha) = f(x_0 + \\alpha p_0)$ 的精确步长 $\\alpha_{\\text{exact}}$ 可以通过设置 $\\frac{d\\phi}{d\\alpha}=0$ 来找到。\n$\\frac{d\\phi}{d\\alpha} = \\nabla f(x_0+\\alpha p_0)^\\top p_0 = 0$。\n使用上面的表达式：$-10001 + \\alpha_{\\text{exact}} (1000001) = 0$。\n这得到 $\\alpha_{\\text{exact}} = \\frac{10001}{1000001} \\approx 0.01$。这个值是理论上的最优值，作为我们的参考基准。\n\n### 回溯算法与停滞\n\n回溯线搜索从一个初始猜测 $\\alpha_0$ 开始，并以一个因子 $\\beta \\in (0, 1)$ 迭代地减小它，直到满足 Wolfe 条件。如果 $\\alpha$ 小于一个阈值 $\\alpha_{\\min}$，算法则终止。\n\n曲率条件是 $|\\nabla f(x_0+\\alpha p_0)^\\top p_0| \\le c_2 |\\nabla f(x_0)^\\top p_0|$，可以转化为：\n$$|-10001 + \\alpha (1000001)| \\le c_2 |-10001|$$\n$$|1 - \\alpha \\frac{1000001}{10001}| \\le c_2 \\implies |1 - \\alpha/\\alpha_{\\text{exact}}| \\le c_2$$\n这个不等式定义了在 $\\alpha_{\\text{exact}}$ 周围一个可接受的 $\\alpha$ 值区间：\n$$\\alpha \\in [(1-c_2)\\alpha_{\\text{exact}}, (1+c_2)\\alpha_{\\text{exact}}]$$\n\n采用从 $\\alpha_0=1$ 和 $\\beta=0.5$ 开始的纯回溯法，尝试的步长序列是 $1, 0.5, 0.25, ..., \\alpha_k = (\\frac{1}{2})^k$。如果选择的 $c_2$ 过小，可接受的 $\\alpha$ 值区间可能变得非常窄，以至于没有一个尝试步长 $\\alpha_k$ 能落入其中。例如，当 $c_2=0.1$ 且 $\\alpha_{\\text{exact}} \\approx 0.01$ 时，可接受的区间大约是 $[0.009, 0.011]$。回溯序列包括 $\\alpha=0.015625$（过高），而下一步是 $\\alpha=0.0078125$（过低）。回溯法“跳过”了有效区域。当 $\\alpha \\to 0$ 时，我们有 $|1-\\alpha/\\alpha_{\\text{exact}}| \\to 1$。曲率条件要求 $1 \\le c_2$，这是不可能的，因为 $c_2  1$。因此，一旦 $\\alpha$ 足够小，曲率条件就永远无法满足。算法将继续回溯直到 $\\alpha  \\alpha_{\\min}$，导致停滞。这就是测试用例 2 和 4 所展示的情况。\n\n### 用于恢复的自动调整策略\n\n为了防止这种停滞，引入了一种针对 $c_2$ 的自适应策略。如果充分下降条件成立但曲率条件失败，这意味着步长位于一个“好”的区域，但曲率约束过于严格。自动调整机制根据观测到的曲率比 $r(\\alpha) = |\\nabla f(x+\\alpha p)^\\top p| / |\\nabla f(x)^\\top p|$ 来放宽 $c_2$。\n更新规则是：\n$$c_2^{\\mathrm{new}} = \\min\\{0.99, \\max\\{c_1+10^{-12}, r(\\alpha)+0.05\\}\\}$$\n新的 $c_2$ 保证在 $(c_1, 1)$ 范围内，并且比当前观测到的比率 $r(\\alpha)$ 稍大。当使用 $c_2^{\\mathrm{new}}$ 重新检查曲率条件时，测试 $r(\\alpha) \\le c_2^{\\mathrm{new}}$ 极有可能通过，因为 $r(\\alpha) \\le r(\\alpha)+0.05$。这使得算法可以接受当前的步长 $\\alpha$ 并继续进行，从而有效克服停滞。测试用例 3 展示了这种恢复。测试用例 5 表明该机制在不需要时不会产生干扰。\n\n下面的 Python 程序实现了这整个逻辑，计算了所有测试用例的步长，并计算了精确步长以供比较。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a backtracking line search with strong Wolfe conditions,\n    demonstrating stagnation due to a poor c2 choice and recovery via auto-tuning.\n    \"\"\"\n    \n    # 2. Construct the concrete example\n    Q = np.diag([100.0, 1.0])\n    x0 = np.array([1.0, 1.0])\n    \n    c1 = 1e-4\n    alpha0 = 1.0\n    beta = 0.5\n    alpha_min = 1e-6\n\n    # Define the function and its gradient\n    def f(x):\n        return 0.5 * x.T @ Q @ x\n\n    def grad_f(x):\n        return Q @ x\n\n    # Calculate initial direction and constant terms\n    p0 = -grad_f(x0)\n    f_x0 = f(x0)\n    grad_f_x0_T_p0 = grad_f(x0).T @ p0\n    abs_grad_p_term = np.abs(grad_f_x0_T_p0)\n\n    # 1. Implement backtracking line search with strong Wolfe conditions\n    def backtracking_wolfe(c2_initial, auto_tune):\n        \"\"\"\n        Performs backtracking line search to find a step size alpha\n        satisfying the strong Wolfe conditions.\n        \"\"\"\n        alpha = alpha0\n        c2 = c2_initial\n\n        while alpha >= alpha_min:\n            # Test sufficient decrease (Armijo) condition\n            armijo_cond_satisfied = f(x0 + alpha * p0) = f_x0 + c1 * alpha * grad_f_x0_T_p0\n            \n            if armijo_cond_satisfied:\n                # Test curvature condition\n                grad_f_x_alpha_p_T_p = grad_f(x0 + alpha * p0).T @ p0\n                abs_grad_p_alpha_term = np.abs(grad_f_x_alpha_p_T_p)\n                \n                curvature_cond_satisfied = abs_grad_p_alpha_term = c2 * abs_grad_p_term\n                \n                if curvature_cond_satisfied:\n                    return alpha # Both conditions met\n\n                # 4. Implement auto-tuning for c2\n                elif auto_tune:\n                    r_alpha = abs_grad_p_alpha_term / abs_grad_p_term\n                    c2_new = min(0.99, max(c1 + 1e-12, r_alpha + 0.05))\n                    \n                    # Re-check with the relaxed c2\n                    if abs_grad_p_alpha_term = c2_new * abs_grad_p_term:\n                        return alpha # Recovered with new c2\n\n            # If conditions fail, backtrack\n            alpha *= beta\n\n        # If loop finishes, alpha has dropped below alpha_min\n        return alpha_min\n\n    # 5. Compute the exact line search step size\n    def exact_line_search():\n        \"\"\"\n        Computes the exact optimal step size for the quadratic function.\n        \"\"\"\n        grad_fx0 = grad_f(x0)\n        # Using the formula alpha = (g'g) / (g'Hg) for steepest descent on quadratics\n        # where g = grad_f(x0) and H = Q here.\n        # This is equivalent to alpha = - (p'g) / (p'Hp) with p=-g\n        numerator = grad_fx0.T @ grad_fx0\n        denominator = grad_fx0.T @ Q @ grad_fx0\n        return numerator / denominator\n\n    # 6. Evaluate all test cases\n    test_cases = [\n        # (c2, auto_tune_enabled)\n        (0.9, False),    # Case 1: Happy path\n        (0.1, False),    # Case 2: Stagnation\n        (0.1, True),     # Case 3: Auto-tuned recovery\n        (0.001, False),  # Case 4: Severe stagnation\n        (0.9, True),     # Case 5: Redundant tuning\n    ]\n\n    results = []\n    for c2_val, tune_flag in test_cases:\n        result = backtracking_wolfe(c2_val, tune_flag)\n        results.append(result)\n\n    # Add the exact line search result\n    results.append(exact_line_search())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3143424"}]}