{"hands_on_practices": [{"introduction": "在我们了解了如何找到“正确”的罚参数之后，理解当我们选择“错误”的参数时会发生什么是至关重要的。这个思想实验[@problem_id:3126635]利用一个特殊设计的函数，来展示一个不够大的罚参数会如何误导优化过程，从而在可行域之外产生错误的解（伪最小值）。这个实践强调了罚参数必须“足够大”这一条件的重要性。", "problem": "考虑一个一维约束优化问题：最小化成本函数 $f(x)$，服从单个不等式约束 $g(x) \\leq 0$。可行集为 $\\mathcal{F} = \\{x \\in \\mathbb{R} : x \\leq 0\\}$。定义成本函数 $f$ 在一个不可行区间上有一个平台，具体如下\n$$\nf(x) =\n\\begin{cases}\n5,  x \\leq 0, \\\\\n11,  0  x  1, \\\\\n1,  1 \\leq x \\leq 2, \\\\\n10 + (x-2)^2,  x > 2,\n\\end{cases}\n$$\n并定义不等式约束为 $g(x) = x$，因此 $\\mathcal{F} = (-\\infty, 0]$。考虑标准的 $L^1$ 型精确罚函数 $P_\\mu(x) = f(x) + \\mu \\max\\{0, g(x)\\} = f(x) + \\mu \\max\\{0, x\\}$，其中罚参数 $\\mu > 0$。\n\n从可行性和精确惩罚的定义出发，分析当 $\\mu$ 不够大时，惩罚目标函数 $P_\\mu$ 是否会在可行集 $\\mathcal{F}$ 之外引入伪局部极小值点，并确定在上述情形下发生这种情况时 $\\mu$ 需满足的条件。然后选择下面最准确的陈述。\n\nA. 在此情形下，$x=1$ 是 $P_\\mu$ 的一个严格局部极小值点，位于 $\\mathcal{F}$ 之外。如果 $\\mu  4$，$x=1$ 也是 $P_\\mu$ 的一个全局极小值点，因此不够大的 $\\mu$ 会引入一个不可行的极小值点；对于 $\\mu \\geq 4$，所有全局极小值点都是可行的，尽管在 $x=1$ 处的不可行局部极小值仍然高于可行最小值。\n\nB. 对于任何 $\\mu > 0$，$P_\\mu$ 在 $\\mathcal{F}$ 之外不能有局部极小值点，因为惩罚项总是迫使极小值点进入 $\\mathcal{F}$。\n\nC. 伪局部极小值点只在 $f$ 在不可行区域上是严格凸函数时才会出现；$f$ 中的平台会阻止任何 $\\mu$ 的不可行局部极小值点的出现。\n\nD. 精确罚函数对于所有 $\\mu > 0$ 都是精确的；因此，$P_\\mu$ 的极小值点集总是与约束问题的解集一致，无论 $f$ 的形状如何。", "solution": "用户希望我扮演一位严谨的STEM教授来解决一个优化问题。\n\n### 步骤1：提取已知条件\n- 优化问题是最小化成本函数 $f(x)$，服从不等式约束 $g(x) \\leq 0$。\n- 可行集为 $\\mathcal{F} = \\{x \\in \\mathbb{R} : x \\leq 0\\}$。\n- 成本函数定义为：\n$$\nf(x) =\n\\begin{cases}\n5,  x \\leq 0, \\\\\n11,  0  x  1, \\\\\n1,  1 \\leq x \\leq 2, \\\\\n10 + (x-2)^2,  x > 2,\n\\end{cases}\n$$\n- 不等式约束函数为 $g(x) = x$。\n- $L^1$ 型精确罚函数为 $P_\\mu(x) = f(x) + \\mu \\max\\{0, g(x)\\} = f(x) + \\mu \\max\\{0, x\\}$，其中罚参数 $\\mu > 0$。\n- 问题要求分析在何种条件下，惩罚目标函数 $P_\\mu(x)$ 会在可行集 $\\mathcal{F}$ 之外引入伪局部极小值点，并从选项中选择最准确的陈述。\n\n### 步骤2：使用提取的已知条件进行验证\n1.", "answer": "$$\\boxed{A}$$", "id": "3126635"}, {"introduction": "理论理解固然强大，但将其应用于数值求解问题才是最终目标。这个练习[@problem_id:3126615]将指导你把精确罚函数的概念转化为一个可以工作的算法——近端梯度法。你将亲手实现这个求解器并测试其行为，亲眼观察当罚参数足够大时算法如何正确找到约束解，而在参数不足时又是如何失效的，从而搭建起从抽象理论到计算实践的桥梁。", "problem": "考虑 $\\mathbb{R}^n$中的约束优化问题：\n最小化 $f(x)$，约束条件为 $x \\ge 0$（坐标方式），其中对于给定的向量 $c \\in \\mathbb{R}^n$，$f(x) = \\frac{1}{2}\\lVert x - c \\rVert_2^2$。精确罚函数方法用非光滑罚函数 $\\psi_\\mu(x) = \\mu \\sum_{i=1}^n \\max(0,-x_i)$ 替代约束，并对罚参数 $\\mu > 0$ 最小化带罚函数的目标 $P_\\mu(x) = f(x) + \\psi_\\mu(x)$。用于最小化 $P_\\mu$ 的近端梯度法（Proximal Gradient (PG)），使用常数步长 $s > 0$，交替执行对 $f$ 的梯度步和对 $\\psi_\\mu$ 的近端步。仅使用以下基本依据：近端算子的定义、梯度的定义、梯度利普希茨连续性的概念以及卡鲁什-库恩-塔克（KKT）最优性条件。\n\n任务：\n1. 从第一性原理出发，仅使用其定义和基本微积分，按坐标推导函数 $\\psi_\\mu(x)$ 的近端算子。不得使用任何预先推导的“快捷”公式。函数 $\\phi$ 在点 $y$ 处，参数为 $t > 0$ 的近端算子定义为 $\\operatorname{prox}_{t\\phi}(y) = \\arg\\min_{x} \\left\\{ \\frac{1}{2}\\lVert x - y \\rVert_2^2 + t \\phi(x) \\right\\}$。\n2. 使用您的推导，实现复合函数 $P_\\mu(x) = f(x) + \\psi_\\mu(x)$ 的近端梯度迭代，从 $x^{(0)} = 0$ 开始，使用满足 $0  s \\le 1$ 的常数步长 $s$。$f$ 的梯度必须从其定义推导得出。\n3. 对于给定的 $c$ 和 $\\mu$，确定精确罚函数是否“精确”，即 $P_\\mu$ 的最小化子是否与约束问题 $\\min_{x \\ge 0} f(x)$ 的解一致。使用卡鲁什-库恩-塔克（KKT）条件来确定保证精确性的 $\\mu$ 的阈值（用最优乘子表示）。在这个特殊情况下，该阈值可以用数据 $c$ 来表示。\n4. 对下面的每个测试用例，运行近端梯度法直到收敛（当连续迭代的 $\\ell_2$ 范数差小于 $10^{-12}$ 或达到最大 2000 次迭代时，以先到者为准，即可停止）。将最终迭代结果与以下目标进行比较：\n   - 在罚函数精确的情况下（即 $\\mu$ 等于或高于阈值），与约束解 $x^\\star_{\\text{con}} = \\arg\\min_{x \\ge 0} f(x)$ 比较，以及\n   - 在罚函数不精确的情况下（即 $\\mu$ 低于阈值），与 $P_\\mu$ 的唯一最小化子比较。在这种可分离的情况下，从第一性原理按坐标推导此最小化子。\n5. 您的程序必须输出单行，其中包含一个逗号分隔的列表，列表内容为每个测试用例的最终迭代与相应目标解之间的 $\\ell_2$ 范数差，并用方括号括起来。每个数字必须四舍五入到八位小数，且不含空格。\n\n用作基本依据的定义和事实：\n- 近端算子定义：$\\operatorname{prox}_{t\\phi}(y) = \\arg\\min_{x} \\left\\{ \\frac{1}{2}\\lVert x - y \\rVert_2^2 + t \\phi(x) \\right\\}$。\n- $f(x) = \\frac{1}{2}\\lVert x - c \\rVert_2^2$ 的梯度为 $\\nabla f(x) = x - c$，从梯度定义推导得出。\n- 如果 $\\nabla f$ 是 $L$-利普希茨连续的，那么在标准的凸性假设下，满足 $0  s \\le \\frac{1}{L}$ 的常数步长 $s$ 能确保近端梯度法收敛到最小化子。在本问题中，$L = 1$。\n- $\\ell_1$ 型罚函数的精确性取决于选择的 $\\mu$ 不小于约束问题的卡鲁什-库恩-塔克（KKT）条件中的最大最优拉格朗日乘子。\n\n测试套件：\n- 测试用例 1：$c = [-2.0, 1.5, -0.3]$，$\\mu = 2.0$，$s = 1.0$。使用约束解作为目标。\n- 测试用例 2：$c = [-2.0, 1.5, -0.3]$，$\\mu = 2.5$，$s = 0.25$。使用约束解作为目标。\n- 测试用例 3：$c = [-2.0, -0.5]$，$\\mu = 0.7$，$s = 0.5$。使用带罚函数的最小化子作为目标。\n- 测试用例 4：$c = [0.0, -10^{-8}, 3\\cdot 10^{-9}]$，$\\mu = 10^{-8}$，$s = 1.0$。使用约束解作为目标。\n\n最终输出格式：\n您的程序应产生单行输出，其中包含一个逗号分隔的列表形式的结果，用方括号括起来，每个数字四舍五入到八位小数，不含空格，例如 `\"[0.00000000,0.12345678,0.00000001]\"`。", "solution": "用户的问题陈述已经过分析，并被认为是有效的。这是一个凸优化中的适定问题，基于已建立的数学原理，并包含了获得唯一解所需的所有信息。\n\n该问题要求进行几项推导和数值实现，涉及使用精确罚函数方法最小化一个受非负约束的二次函数。带罚函数的问题使用近端梯度算法求解。\n\n主要的约束优化问题是：\n$$ \\underset{x \\in \\mathbb{R}^n}{\\text{minimize}} \\quad f(x) = \\frac{1}{2}\\lVert x - c \\rVert_2^2 \\quad \\text{subject to} \\quad x \\ge 0 $$\n精确罚函数方法通过最小化复合函数 $P_\\mu(x) = f(x) + \\psi_\\mu(x)$ 将此问题转化为无约束问题，其中 $f(x)$ 是光滑部分，$\\psi_\\mu(x)$ 是非光滑罚项。\n光滑部分是 $f(x) = \\frac{1}{2}\\lVert x - c \\rVert_2^2 = \\frac{1}{2}\\sum_{i=1}^n(x_i - c_i)^2$。\n非光滑罚项是 $\\psi_\\mu(x) = \\mu \\sum_{i=1}^n \\max(0, -x_i)$，罚参数 $\\mu > 0$。\n\n首先，我们处理问题所要求的分析任务。\n\n**1. 推导 $\\psi_\\mu(x)$ 的近端算子**\n\n参数为 $t > 0$ 的函数 $\\phi(x)$ 的近端算子定义为：\n$$ \\operatorname{prox}_{t\\phi}(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\lVert x - y \\rVert_2^2 + t \\phi(x) \\right\\} $$\n在我们的情况下，函数是 $t\\phi(x) = s\\psi_\\mu(x)$，其中 $s$ 是步长。所以我们需要计算：\n$$ \\operatorname{prox}_{s\\psi_\\mu}(y) = \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\lVert x - y \\rVert_2^2 + s \\mu \\sum_{i=1}^n \\max(0, -x_i) \\right\\} $$\n$\\arg\\min$ 内的目标函数关于 $x$ 的坐标是可分离的。这意味着我们可以通过独立地对每个坐标 $x_i$ 进行最小化来求解。对于第 $i$ 个坐标，问题是：\n$$ [\\operatorname{prox}_{s\\psi_\\mu}(y)]_i = \\arg\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\frac{1}{2}(x_i - y_i)^2 + s \\mu \\max(0, -x_i) \\right\\} $$\n令 $g(x_i) = \\frac{1}{2}(x_i - y_i)^2 + s \\mu \\max(0, -x_i)$。我们通过考虑 $x_i$ 的两种情况来分析这个问题：\n\n情况 1：$x_i \\ge 0$。\n在这种情况下，$\\max(0, -x_i) = 0$。目标变为 $g_1(x_i) = \\frac{1}{2}(x_i - y_i)^2$。这是一个简单的二次函数，其无约束最小化子是 $x_i = y_i$。为了使此解在当前情况下有效，我们要求 $y_i \\ge 0$。如果 $y_i \\ge 0$，则 $g_1(x_i)$ 在 $[0, \\infty)$ 上的最小值在 $x_i = y_i$ 处。\n\n情况 2：$x_i  0$。\n在这种情况下，$\\max(0, -x_i) = -x_i$。目标变为 $g_2(x_i) = \\frac{1}{2}(x_i - y_i)^2 - s \\mu x_i$。这是一个凸二次函数。我们通过将其导数设为零来找到其最小化子：\n$$ \\frac{dg_2}{dx_i} = (x_i - y_i) - s\\mu = 0 \\implies x_i = y_i + s\\mu $$\n为了使此解在当前情况下有效，我们要求 $y_i + s\\mu  0$，这等价于 $y_i  -s\\mu$。\n\n情况 3：合并各种情况。\n我们已经确定了 $y_i \\ge 0$ 时的解（即 $x_i = y_i$）和 $y_i  -s\\mu$ 时的解（即 $x_i = y_i + s\\mu$）。我们现在必须考虑区间 $-s\\mu \\le y_i  0$。\n如果 $-s\\mu \\le y_i  0$：\n- $g_1(x_i)$ 的最小化子是 $x_i=y_i  0$，这超出了定义域 $x_i \\ge 0$。由于当 $x_i  y_i$ 时 $g_1(x_i)$ 是递减的，它在 $[0, \\infty)$ 上的最小值出现在边界点 $x_i=0$。\n- $g_2(x_i)$ 的最小化子是 $x_i=y_i+s\\mu \\ge 0$，这超出了定义域 $x_i  0$。由于当 $x_i > y_i+s\\mu$ 时 $g_2(x_i)$ 是递增的，它在 $(-\\infty, 0)$ 上的最小值出现在边界点 $x_i=0$。\n由于函数的两个部分都指向 $x_i = 0$ 作为最小化子，全局最小值在 $x_i = 0$ 处。\n\n总结最优 $x_i$ 的三种可能性：\n$$ [\\operatorname{prox}_{s\\psi_\\mu}(y)]_i = \\begin{cases} y_i  \\text{if } y_i \\ge 0 \\\\ 0  \\text{if } -s\\mu  y_i  0 \\\\ y_i + s\\mu  \\text{if } y_i \\le -s\\mu \\end{cases} $$\n这是近端算子的坐标方式公式。\n\n**2. 近端梯度迭代**\n\n用于最小化 $P_\\mu(x) = f(x) + \\psi_\\mu(x)$ 的近端梯度（PG）法由以下迭代给出：\n$$ x^{(k+1)} = \\operatorname{prox}_{s\\psi_\\mu}(x^{(k)} - s \\nabla f(x^{(k)})) $$\n首先，我们推导 $f(x)$ 的梯度。给定 $f(x) = \\frac{1}{2}\\sum_{i=1}^n(x_i - c_i)^2$，关于 $x_j$ 的偏导数是 $\\frac{\\partial f}{\\partial x_j} = \\frac{1}{2} \\cdot 2(x_j - c_j) = x_j - c_j$。因此，梯度是 $\\nabla f(x) = x - c$。海森矩阵是单位矩阵，所以其最大特征值为 $1$，这意味着梯度是 $L$-利普希茨连续的，且 $L=1$。步长 $s$ 必须满足 $0  s \\le 1/L = 1$，这与问题陈述一致。\n\nPG迭代中近端算子的参数是：\n$$ y^{(k)} = x^{(k)} - s \\nabla f(x^{(k)}) = x^{(k)} - s(x^{(k)} - c) = (1-s)x^{(k)} + sc $$\n所以，迭代 $k+1$ 的完整更新规则是 $x^{(k+1)} = \\operatorname{prox}_{s\\psi_\\mu}(y^{(k)})$，使用上一节推导的公式按坐标计算。\n\n**3. 精确性分析和目标解**\n\n我们需要确定带罚函数问题 $P_\\mu(x)$ 的最小化子何时与原始约束问题的最小化子一致。\n\n**a) 约束问题的解**\n问题是 $\\min_{x \\ge 0} f(x)$。卡鲁什-库恩-塔克（KKT）条件为最优性提供了必要和充分条件。拉格朗日函数为 $\\mathcal{L}(x, \\lambda) = f(x) - \\lambda^T x = \\frac{1}{2}\\lVert x-c \\rVert_2^2 - \\sum_{i=1}^n \\lambda_i x_i$，其中 $\\lambda \\ge 0$ 是拉格朗日乘子。\n对于解 $x^\\star_{\\text{con}}$ 和乘子 $\\lambda^\\star$ 的KKT条件是：\n1. 平稳性：$\\nabla_x \\mathcal{L}(x^\\star_{\\text{con}}, \\lambda^\\star) = x^\\star_{\\text{con}} - c - \\lambda^\\star = 0$。\n2. 原始可行性：$x^\\star_{\\text{con}} \\ge 0$。\n3. 对偶可行性：$\\lambda^\\star \\ge 0$。\n4. 互补松弛性：$\\lambda_i^\\star x^\\star_{\\text{con}, i} = 0$ 对所有 $i=1, \\dots, n$。\n\n从平稳性条件，$\\lambda^\\star = x^\\star_{\\text{con}} - c$。将其代入互补松弛性条件：$(x^\\star_{\\text{con}, i} - c_i)x^\\star_{\\text{con}, i} = 0$。\n对于每个坐标 $i$：\n- 如果 $x^\\star_{\\text{con}, i} > 0$，则 $x^\\star_{\\text{con}, i} - c_i = 0$，所以 $x^\\star_{\\text{con}, i} = c_i$。这要求 $c_i > 0$。对应的乘子是 $\\lambda_i^\\star = 0$。\n- 如果 $x^\\star_{\\text{con}, i} = 0$，则从平稳性条件，$\\lambda_i^\\star = -c_i$。对偶可行性要求 $\\lambda_i^\\star \\ge 0$，所以这种情况适用于 $c_i \\le 0$。\n结合这些，约束问题的解是 $x^\\star_{\\text{con}, i} = \\max(0, c_i)$。最优拉格朗日乘子是 $\\lambda_i^\\star = \\max(0, -c_i)$。该解是 $c$ 在非负象限上的投影。\n\n**b) 带罚函数问题的解**\n$P_\\mu(x) = f(x) + \\psi_\\mu(x)$ 的最小化子 $x^\\star_{\\text{pen}}$ 必须满足次微分最优性条件 $0 \\in \\partial P_\\mu(x^\\star_{\\text{pen}})$。\n由于 $f$ 是光滑的，$\\partial P_\\mu(x) = \\nabla f(x) + \\partial \\psi_\\mu(x) = x-c + \\mu \\partial \\sum_{i=1}^n\\max(0, -x_i)$。这个条件是可分离的。对于每个坐标 $i$：\n$$ 0 \\in x^\\star_{\\text{pen}, i} - c_i + \\mu \\partial(\\max(0, -x_i)) $$\n$\\phi(z) = \\max(0, -z)$ 的次微分是 $\\partial\\phi(z) = \\{0\\}$（如果 $z>0$），$[-1, 0]$（如果 $z=0$），和 $\\{-1\\}$（如果 $z0$）。\n- 如果 $x^\\star_{\\text{pen}, i} > 0$：$x^\\star_{\\text{pen}, i} - c_i + \\mu\\{0\\} = 0 \\implies x^\\star_{\\text{pen}, i} = c_i$。这个选择仅当 $c_i > 0$ 时一致。\n- 如果 $x^\\star_{\\text{pen}, i}  0$：$x^\\star_{\\text{pen}, i} - c_i + \\mu\\{-1\\} = 0 \\implies x^\\star_{\\text{pen}, i} = c_i + \\mu$。这个选择仅当 $c_i + \\mu  0 \\implies c_i  -\\mu$ 时一致。\n- 如果 $x^\\star_{\\text{pen}, i} = 0$：$0 - c_i + \\mu[-1, 0] \\ni 0 \\implies c_i \\in [-\\mu, 0]$。\n总结起来，带罚函数问题的最小化子是：\n$$ x^\\star_{\\text{pen}, i} = \\begin{cases} c_i  \\text{if } c_i > 0 \\\\ 0  \\text{if } -\\mu \\le c_i \\le 0 \\\\ c_i + \\mu  \\text{if } c_i  -\\mu \\end{cases} $$\n\n**c) 精确性条件**\n如果 $x^\\star_{\\text{pen}} = x^\\star_{\\text{con}}$，则罚函数是精确的。我们比较它们的分量。\n- 如果 $c_i \\ge 0$：$x^\\star_{\\text{con}, i} = c_i$。带罚函数的解是 $x^\\star_{\\text{pen}, i} = c_i$（如果 $c_i > 0$）和 $x^\\star_{\\text{pen}, i} = 0$（如果 $c_i=0$，因为 $0 \\in [-\\mu, 0]$）。在这两种情况下，$x^\\star_{\\text{con}, i} = x^\\star_{\\text{pen}, i}$。\n- 如果 $c_i  0$：$x^\\star_{\\text{con}, i} = 0$。带罚函数的解是 $x^\\star_{\\text{pen}, i} = 0$（如果 $-\\mu \\le c_i  0$），但 $x^\\star_{\\text{pen}, i} = c_i + \\mu \\ne 0$（如果 $c_i  -\\mu$）。\n为保证精确性（$x^\\star_{\\text{pen}} = x^\\star_{\\text{con}}$），对于所有 $c_i  0$ 的 $i$，我们必须有 $x^\\star_{\\text{pen}, i} = 0$。这要求对于所有 $i$，我们避免 $c_i  -\\mu$ 的情况。这等价于要求对于所有 $c_i  0$ 的 $i$，$\\mu \\ge -c_i$。这个条件可以紧凑地写成 $\\mu \\ge \\max_{i: c_i0} (-c_i)$。\n这个阈值正是最大的最优拉格朗日乘子 $\\mu_{\\text{crit}} = \\max_i \\lambda_i^\\star = \\max_i \\max(0, -c_i)$。因此，当且仅当 $\\mu \\ge \\mu_{\\text{crit}}$ 时，罚函数是精确的。\n\n**4. 测试用例分析**\n\n- **测试用例 1：** $c = [-2.0, 1.5, -0.3]$，$\\mu = 2.0$，$s = 1.0$。\n  $\\mu_{\\text{crit}} = \\max(\\max(0, 2.0), \\max(0, -1.5), \\max(0, 0.3)) = \\max(2.0, 0, 0.3) = 2.0$。\n  由于 $\\mu = 2.0 \\ge \\mu_{\\text{crit}}$，罚函数是精确的。\n  目标解是 $x^\\star_{\\text{con}} = (\\max(0, -2.0), \\max(0, 1.5), \\max(0, -0.3)) = [0, 1.5, 0]$。\n\n- **测试用例 2：** $c = [-2.0, 1.5, -0.3]$，$\\mu = 2.5$，$s = 0.25$。\n  $\\mu_{\\text{crit}} = 2.0$。\n  由于 $\\mu = 2.5 > \\mu_{\\text{crit}}$，罚函数是精确的。\n  目标解是 $x^\\star_{\\text{con}} = [0, 1.5, 0]$。\n\n- **测试用例 3：** $c = [-2.0, -0.5]$，$\\mu = 0.7$，$s = 0.5$。\n  $\\mu_{\\text{crit}} = \\max(\\max(0, 2.0), \\max(0, 0.5)) = \\max(2.0, 0.5) = 2.0$。\n  由于 $\\mu = 0.7  \\mu_{\\text{crit}}$，罚函数不精确。\n  目标解是带罚函数的最小化子 $x^\\star_{\\text{pen}}$：\n  $c_1 = -2.0  -\\mu = -0.7 \\implies x^\\star_{\\text{pen}, 1} = c_1 + \\mu = -2.0 + 0.7 = -1.3$。\n  $-\\mu = -0.7 \\le c_2 = -0.5 \\le 0 \\implies x^\\star_{\\text{pen}, 2} = 0$。\n  目标解是 $x^\\star_{\\text{pen}} = [-1.3, 0]$。\n\n- **测试用例 4：** $c = [0.0, -10^{-8}, 3\\cdot 10^{-9}]$，$\\mu = 10^{-8}$，$s = 1.0$。\n  $\\mu_{\\text{crit}} = \\max(\\max(0, 0), \\max(0, 10^{-8}), \\max(0, -3\\cdot 10^{-9})) = \\max(0, 10^{-8}, 0) = 10^{-8}$。\n  由于 $\\mu = 10^{-8} \\ge \\mu_{\\text{crit}}$，罚函数是精确的。\n  目标解是 $x^\\star_{\\text{con}} = (\\max(0, 0), \\max(0, -10^{-8}), \\max(0, 3\\cdot 10^{-9})) = [0, 0, 3\\cdot 10^{-9}]$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for a suite of test cases using the\n    Proximal Gradient method with an exact penalty function.\n    \"\"\"\n\n    test_cases = [\n        {'c': np.array([-2.0, 1.5, -0.3]), 'mu': 2.0, 's': 1.0},\n        {'c': np.array([-2.0, 1.5, -0.3]), 'mu': 2.5, 's': 0.25},\n        {'c': np.array([-2.0, -0.5]), 'mu': 0.7, 's': 0.5},\n        {'c': np.array([0.0, -1e-8, 3e-9]), 'mu': 1e-8, 's': 1.0},\n    ]\n\n    results = []\n    \n    # Convergence parameters\n    max_iter = 2000\n    tol = 1e-12\n\n    for case in test_cases:\n        c, mu, s = case['c'], case['mu'], case['s']\n        \n        # --- Determine the target solution ---\n        # The critical penalty parameter is the max of the optimal Lagrange multipliers\n        mu_crit = np.max(np.maximum(0, -c))\n        \n        target_solution = np.zeros_like(c)\n        if mu >= mu_crit:\n            # Exact penalty: target is the solution to the constrained problem\n            # x*_con = max(0, c_i) for each component\n            target_solution = np.maximum(0, c)\n        else:\n            # Not exact: target is the minimizer of the penalized problem\n            # x*_pen,i = c_i if c_i > 0\n            # x*_pen,i = 0 if -mu = c_i = 0\n            # x*_pen,i = c_i + mu if c_i  -mu\n            x_pen = np.zeros_like(c)\n            mask_pos = c > 0\n            mask_neg_small = (-mu = c)  (c = 0)\n            mask_neg_large = c  -mu\n            \n            x_pen[mask_pos] = c[mask_pos]\n            x_pen[mask_neg_small] = 0.0\n            x_pen[mask_neg_large] = c[mask_neg_large] + mu\n            target_solution = x_pen\n\n        # --- Proximal Gradient Method ---\n        x = np.zeros_like(c)\n        for _ in range(max_iter):\n            x_prev = x.copy()\n            \n            # Gradient step for the smooth part f(x)\n            # y = x - s * grad(f(x)) = x - s * (x - c)\n            y = (1 - s) * x + s * c\n            \n            # Proximal step for the non-smooth part psi_mu(x)\n            # prox(y)_i = y_i if y_i >= 0\n            # prox(y)_i = 0 if -s*mu  y_i  0\n            # prox(y)_i = y_i + s*mu if y_i = -s*mu\n            x = np.zeros_like(y)\n            mask_pos = y >= 0\n            mask_neg = y = -s * mu\n            \n            x[mask_pos] = y[mask_pos]\n            x[mask_neg] = y[mask_neg] + s * mu\n            \n            # Check for convergence\n            if np.linalg.norm(x - x_prev)  tol:\n                break\n        \n        # Calculate the L2 norm of the difference\n        diff = np.linalg.norm(x - target_solution)\n        results.append(f\"{diff:.8f}\")\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3126615"}]}