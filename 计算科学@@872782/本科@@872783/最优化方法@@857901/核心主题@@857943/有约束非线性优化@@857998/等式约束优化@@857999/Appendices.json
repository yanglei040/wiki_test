{"hands_on_practices": [{"introduction": "本练习将拉格朗日乘子法的抽象概念与一个具体的工程问题联系起来。我们将探讨如何在一个材料内部找到最大和最小法向应力，即主应力。这个练习 [@problem_id:2183834] 不仅为约束优化提供了一个实际应用，而且还巧妙地揭示了拉格朗日乘子法与线性代数中特征值问题之间的深刻联系。", "problem": "在连续介质力学领域，材料内某一点的应力状态由一个应力张量来表征。对于一个二维（平面应力）问题，薄板上某特定点的应力张量分量为 $\\sigma_{xx} = 80.0$ 兆帕（MPa），$\\sigma_{yy} = 20.0$ MPa，以及 $\\sigma_{xy} = -30.0$ MPa。\n\n通过该点的平面上的正应力 $\\sigma_n$ 取决于该平面的方向，该方向可由分量为 $(n_x, n_y)$ 的单位法向量 $\\mathbf{n}$ 定义。正应力的大小由以下公式给出：\n$$ \\sigma_n = \\sigma_{xx} n_x^2 + \\sigma_{yy} n_y^2 + 2 \\sigma_{xy} n_x n_y $$\n由于 $\\mathbf{n}$ 是一个单位向量，其分量必须满足条件 $n_x^2 + n_y^2 = 1$。\n\n在所有可能方向上 $\\sigma_n$ 的极值被称为主应力。确定最大主应力 $\\sigma_\\text{max}$ 和最小主应力 $\\sigma_\\text{min}$。\n\n答案以 MPa 为单位表示，并四舍五入到三位有效数字。将 $\\sigma_\\text{max}$ 和 $\\sigma_\\text{min}$ 这两个值按此顺序以行矩阵的形式呈现。", "solution": "通过某点的平面上的正应力极值即为主应力，它们是平面内应力张量的特征值。对于一个单位法向量 $\\mathbf{n}$，正应力为 $\\sigma_{n}=\\mathbf{n}^{T}\\boldsymbol{\\sigma}\\mathbf{n}$，并带有约束条件 $\\mathbf{n}^{T}\\mathbf{n}=1$。为此约束引入一个拉格朗日乘子 $\\lambda$，定义 $L(\\mathbf{n},\\lambda)=\\mathbf{n}^{T}\\boldsymbol{\\sigma}\\mathbf{n}-\\lambda(\\mathbf{n}^{T}\\mathbf{n}-1)$。关于 $\\mathbf{n}$ 的驻定条件给出\n$$\n\\frac{\\partial L}{\\partial \\mathbf{n}}=2\\boldsymbol{\\sigma}\\mathbf{n}-2\\lambda \\mathbf{n}=\\mathbf{0}\\quad\\Rightarrow\\quad (\\boldsymbol{\\sigma}-\\lambda \\mathbf{I})\\mathbf{n}=\\mathbf{0},\n$$\n因此，非平凡解需要满足\n$$\n\\det(\\boldsymbol{\\sigma}-\\lambda \\mathbf{I})=0.\n$$\n对于给定的具有分量 $\\sigma_{xx}$、$\\sigma_{yy}$ 和 $\\sigma_{xy}$ 的平面应力张量，\n$$\n\\det\\begin{pmatrix}\\sigma_{xx}-\\lambda  & \\sigma_{xy}\\\\ \\sigma_{xy} & \\sigma_{yy}-\\lambda\\end{pmatrix}=(\\sigma_{xx}-\\lambda)(\\sigma_{yy}-\\lambda)-\\sigma_{xy}^{2}=0,\n$$\n展开后得到\n$$\n\\lambda^{2}-(\\sigma_{xx}+\\sigma_{yy})\\lambda+(\\sigma_{xx}\\sigma_{yy}-\\sigma_{xy}^{2})=0.\n$$\n方程的根（即主应力）为\n$$\n\\lambda=\\frac{\\sigma_{xx}+\\sigma_{yy}}{2}\\pm \\sqrt{\\left(\\frac{\\sigma_{xx}-\\sigma_{yy}}{2}\\right)^{2}+\\sigma_{xy}^{2}}.\n$$\n代入 $\\sigma_{xx}=80.0$、$\\sigma_{yy}=20.0$ 和 $\\sigma_{xy}=-30.0$（单位为 MPa），\n$$\n\\frac{\\sigma_{xx}+\\sigma_{yy}}{2}=50.0,\\qquad \\left(\\frac{\\sigma_{xx}-\\sigma_{yy}}{2}\\right)^{2}=30^{2}=900,\\qquad \\sigma_{xy}^{2}=900,\n$$\n所以\n$$\n\\sqrt{\\left(\\frac{\\sigma_{xx}-\\sigma_{yy}}{2}\\right)^{2}+\\sigma_{xy}^{2}}=\\sqrt{1800}=30\\sqrt{2}.\n$$\n因此，\n$$\n\\sigma_{\\max}=50+30\\sqrt{2},\\qquad \\sigma_{\\min}=50-30\\sqrt{2}.\n$$\n数值上（单位为 MPa），保留三位有效数字，\n$$\n\\sigma_{\\max}\\approx 92.4,\\qquad \\sigma_{\\min}\\approx 7.57.\n$$", "answer": "$$\\boxed{\\begin{pmatrix} 92.4 & 7.57 \\end{pmatrix}}$$", "id": "2183834"}, {"introduction": "驻点并不总是最小值点或最大值点。这个思想实验 [@problem_id:3126154] 通过一个场景挑战了我们的直觉，在该场景中，一个点在整个空间中是鞍点，但当被限制在可行流形上时，它却变成了一个严格的局部最小值点。掌握这个概念对于正确分类约束最优点至关重要，并突出了为什么我们必须仅沿着可行方向分析函数的曲率。", "problem": "考虑一个等式约束优化问题，其目标函数为 $f:\\mathbb{R}^2 \\to \\mathbb{R}$，由 $f(x,y) = x^2 - y^2$ 给出，以及一个等式约束 $g(x,y) = y = 0$。设 $(x^\\star,y^\\star) = (0,0)$ 为我们关注的点。使用通过拉格朗日函数定义的约束稳定点的基本定义以及可行方向的切空间刻画。拉格朗日函数定义为 $\\mathcal{L}(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)$，并且在一个可行点处的切空间是约束雅可比矩阵的零空间。Karush-Kuhn-Tucker (KKT) 条件规定了约束最优性的一阶必要条件，而二阶条件则评估拉格朗日函数的海森矩阵在切空间上的二次型的定性。\n\n关于点 $(0,0)$，以下哪些陈述是正确的？\n\nA. $(0,0)$ 是一个约束稳定点，其拉格朗日乘子 $\\lambda = 0$ 满足一阶必要条件。\n\nB. 在整个环境空间 $\\mathbb{R}^2$ 中，$(0,0)$ 是 $f$ 的一个严格鞍点，因为存在使 $f$ 增加的方向和使 $f$ 减少的方向。\n\nC. 当限制在约束流形 $\\{(x,y)\\in\\mathbb{R}^2 : y = 0\\}$ 上时，$(0,0)$ 是 $f$ 的一个严格局部最小值点。\n\nD. 在 $(0,0)$ 点，通过将海森矩阵限制在约束的切空间方向上形成的拉格朗日函数的约化海森矩阵是正定的。\n\nE. 约束局部最优性的二阶必要条件在 $(0,0)$ 点不成立，因此 $(0,0)$ 不可能是一个约束局部最小值点。\n\n选择所有正确的选项。", "solution": "该问题陈述是等式约束优化中的一个有效练习。我们被给予目标函数 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 为 $f(x,y) = x^2 - y^2$ 以及等式约束 $g(x,y) = y=0$。我们需要分析点 $(x^\\star,y^\\star) = (0,0)$ 的性质。\n\n首先，我们验证点 $(x^\\star,y^\\star) = (0,0)$ 是一个可行点。代入约束方程，我们得到 $g(0,0) = 0$，满足该条件。\n\n该问题的拉格朗日函数定义为 $\\mathcal{L}(x,y,\\lambda) = f(x,y) + \\lambda g(x,y)$。\n代入给定的函数，我们得到：\n$$ \\mathcal{L}(x,y,\\lambda) = (x^2 - y^2) + \\lambda y $$\n\n我们现在将评估每个陈述。\n\n**A. $(0,0)$ 是一个约束稳定点，其拉格朗日乘子 $\\lambda = 0$ 满足一阶必要条件。**\n\n如果存在一个拉格朗日乘子 $\\lambda^\\star$，使得拉格朗日函数关于变量 $(x,y)$ 的梯度在该点为零，那么点 $(x^\\star, y^\\star)$ 就是一个约束稳定点。一阶必要条件（等式约束的 KKT 条件）是：\n$$ \\nabla_{x,y} \\mathcal{L}(x^\\star, y^\\star, \\lambda^\\star) = 0 $$\n$$ g(x^\\star, y^\\star) = 0 $$\n我们已经确认了第二个条件（可行性）。现在我们计算拉格朗日函数的梯度：\n$$ \\nabla_{x,y} \\mathcal{L}(x,y,\\lambda) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} 2x \\\\ -2y + \\lambda \\end{pmatrix} $$\n我们在点 $(x^\\star, y^\\star) = (0,0)$ 处评估此梯度：\n$$ \\nabla_{x,y} \\mathcal{L}(0,0,\\lambda) = \\begin{pmatrix} 2(0) \\\\ -2(0) + \\lambda \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\lambda \\end{pmatrix} $$\n要使此梯度为零向量，我们必须有 $\\lambda = 0$。\n由于我们找到了一个值 $\\lambda^\\star = 0$ 满足一阶必要条件 $\\nabla_{x,y} \\mathcal{L}(0,0,0) = (0,0)^T$，点 $(0,0)$ 确实是一个约束稳定点。\n因此，陈述 A 是 **正确的**。\n\n**B. 在整个环境空间 $\\mathbb{R}^2$ 中，$(0,0)$ 是 $f$ 的一个严格鞍点，因为存在使 $f$ 增加的方向和使 $f$ 减少的方向。**\n\n我们在 $(0,0)$ 的邻域内分析无约束函数 $f(x,y) = x^2 - y^2$。首先，我们通过将 $f$ 的梯度设为零来找到临界点：\n$$ \\nabla f(x,y) = \\begin{pmatrix} 2x \\\\ -2y \\end{pmatrix} $$\n设 $\\nabla f(x,y) = (0,0)^T$ 得到 $x=0$ 和 $y=0$。所以，$(0,0)$ 是无约束函数 $f$ 的一个临界点。\n接下来，我们使用 $f$ 的海森矩阵来分类这个临界点：\n$$ H_f(x,y) = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x^2} & \\frac{\\partial^2 f}{\\partial x \\partial y} \\\\ \\frac{\\partial^2 f}{\\partial y \\partial x} & \\frac{\\partial^2 f}{\\partial y^2} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} $$\n海森矩阵是常数。其特征值为 $\\lambda_1 = 2$ 和 $\\lambda_2 = -2$。由于特征值符号相反，海森矩阵是不定的，这是鞍点的条件。该点是一个严格（或非退化）的鞍点，因为没有一个特征值为零。\n正如陈述正确指出的那样，在第一个特征向量 $d_1 = (1,0)^T$ 的方向上，函数表现为 $f(t,0) = t^2$，是增加的。在第二个特征向量 $d_2 = (0,1)^T$ 的方向上，函数表现为 $f(0,t) = -t^2$，是减少的。\n因此，陈述 B 是 **正确的**。\n\n**C. 当限制在约束流形 $\\{(x,y)\\in\\mathbb{R}^2 : y = 0\\}$ 上时，$(0,0)$ 是 $f$ 的一个严格局部最小值点。**\n\n约束流形是所有满足 $y=0$ 的点的集合。这是 $\\mathbb{R}^2$ 平面中的 x 轴。\n为了分析限制在该流形上的函数 $f$，我们将 $y=0$ 代入 $f(x,y)$ 的表达式中：\n$$ f_{constrained}(x) = f(x,0) = x^2 - (0)^2 = x^2 $$\n我们关心这个一维函数在点 $(0,0)$ 处的行为，这对应于 $x=0$。函数 $h(x) = x^2$ 的导数为 $h'(x) = 2x$，在 $x=0$ 处为零。二阶导数为 $h''(x) = 2$，是正数。这表明在 $x=0$ 处有一个严格局部最小值。更直接地，对于 $0$ 的任何邻域内的任意 $x \\neq 0$，都有 $h(x) = x^2 > 0 = h(0)$。\n因此，当限制在约束流形上时，点 $(0,0)$ 是 $f$ 的一个严格局部最小值点。\n因此，陈述 C 是 **正确的**。\n\n**D. 在 $(0,0)$ 点，通过将海森矩阵限制在约束的切空间方向上形成的拉格朗日函数的约化海森矩阵是正定的。**\n\n这个陈述涉及最优性的二阶充分条件。拉格朗日函数关于 $(x,y)$ 的海森矩阵是：\n$$ H_{\\mathcal{L}}(x,y,\\lambda) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} $$\n在稳定点 $(x^\\star,y^\\star,\\lambda^\\star) = (0,0,0)$ 处，海森矩阵是 $H_{\\mathcal{L}}(0,0,0) = \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix}$。\n在可行点处的切空间是活动约束的雅可比矩阵的零空间。约束是 $g(x,y)=y=0$。$g$ 的雅可比矩阵是：\n$$ J_g(x,y) = \\nabla g(x,y)^T = \\begin{pmatrix} 0 & 1 \\end{pmatrix} $$\n在 $(0,0)$ 处的切空间 $T$ 是所有向量 $z = (z_1, z_2)^T \\in \\mathbb{R}^2$ 的集合，使得 $J_g(0,0) z = 0$：\n$$ \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix} = 0 \\implies z_2 = 0 $$\n所以，切空间是形如 $(z_1, 0)^T$ 的向量集合，也就是 x 轴。该空间的一组基是向量 $Z = (1,0)^T$。\n为了确定海森矩阵在该切空间上的定性，我们对任意非零向量 $z \\in T$ 计算二次型 $z^T H_{\\mathcal{L}} z$。设 $z = (z_1, 0)^T$ 且 $z_1 \\neq 0$。\n$$ z^T H_{\\mathcal{L}} z = \\begin{pmatrix} z_1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2z_1 & 0 \\end{pmatrix} \\begin{pmatrix} z_1 \\\\ 0 \\end{pmatrix} = 2z_1^2 $$\n对于任意 $z_1 \\neq 0$，都有 $2z_1^2 > 0$。这意味着当限制在切空间上时，拉格朗日函数的海森矩阵是正定的。约化海森矩阵是 $Z^T H_{\\mathcal{L}} Z = (1,0) \\begin{pmatrix} 2 & 0 \\\\ 0 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = [2]$，这是一个正定的 $1 \\times 1$ 矩阵。\n因此，陈述 D 是 **正确的**。\n\n**E. 约束局部最优性的二阶必要条件在 $(0,0)$ 点不成立，因此 $(0,0)$ 不可能是一个约束局部最小值点。**\n\n一个点 $(x^\\star, y^\\star)$ 成为约束局部最小值点的二阶必要条件 (SONC) 是，拉格朗日函数的海森矩阵 $H_{\\mathcal{L}}(x^\\star, y^\\star, \\lambda^\\star)$ 在切空间 $T(x^\\star, y^\\star)$ 上必须是半正定的。这意味着对于所有 $z \\in T(x^\\star, y^\\star)$，都有 $z^T H_{\\mathcal{L}} z \\ge 0$。\n根据我们对选项 D 的分析，我们发现对于任意非零 $z \\in T(0,0)$，都有 $z^T H_{\\mathcal{L}}(0,0,0) z = 2z_1^2 > 0$。这是一个严格正值，当然满足非负（$\\ge 0$）的条件。实际上，更强的二阶充分条件（对于严格局部最小值）也得到了满足。\n由于二阶必要条件得到满足，所以说它们不成立的陈述是错误的。\n因此，陈述 E 是 **不正确的**。\n\n总而言之，陈述 A、B、C 和 D 是正确的，而陈述 E 是不正确的。", "answer": "$$\\boxed{ABCD}$$", "id": "3126154"}, {"introduction": "当我们把理论知识转化为一个有效的算法时，这些知识才真正焕发出生命力。这个动手编程任务 [@problem_id:3126067] 将指导你从基本原理出发，实现一个求解等式约束优化问题的数值方法。你将构建一个算法，它巧妙地将问题分解为两部分：恢复可行性和优化目标函数，这是许多高级求解器背后的核心思想。", "problem": "考虑一个等式约束优化问题，该问题旨在最小化目标函数 $f(x)$，并满足等式约束 $c(x)=0$。设 $x\\in\\mathbb{R}^n$ 且 $c(x)\\in\\mathbb{R}^m$，其中 $m\\leq n$。将约束的雅可比矩阵记为 $A(x)=\\nabla c(x)\\in\\mathbb{R}^{m\\times n}$，目标函数的梯度记为 $g(x)=\\nabla f(x)\\in\\mathbb{R}^n$。在迭代方法中，处理不可行性的一种标准方法是，通过求解子问题 $\\min_{p}\\|p\\|$（约束条件为线性化约束 $A(x)p=-c(x)$）来计算一个可行性恢复步 $p$，然后将其与一个保持在 $A(x)$ 零空间内的切向步 $t$ 相结合，以在保持一阶可行性的同时减小目标函数值。您的任务是从基本原理出发实现此方法，并定量分析其对收敛性的影响。\n\n从基本定义出发，实现一个迭代方法，该方法在以下两个步骤之间交替进行：\n- 在当前迭代点 $x_k$ 处，通过求解 $\\min_{p}\\|p\\|$（约束条件为 $A(x_k)p=-c(x_k)$）计算一个可行性恢复步 $p_k$，并更新 $x_{k+\\frac{1}{2}}=x_k+p_k$。\n- 在 $x_{k+\\frac{1}{2}}$ 处计算一个切向步 $t_k$，该步满足 $A(x_{k+\\frac{1}{2}})t_k=0$ 并局部减小 $f$，然后更新 $x_{k+1}=x_{k+\\frac{1}{2}}+t_k$。\n\n使用带有充分下降条件的回溯法来选择切向步长。任何三角函数中的角度都必须解释为弧度。当约束违反范数 $\\|c(x)\\|$ 小于可行性容差时，停止仅法向步方法。对于耦合方法，运行固定的迭代次数，记录首次达到可行性时的迭代次数以及最终的目标函数值。所有浮点数输出必须四舍五入到 $6$ 位小数。\n\n在您的推导和实现中应使用的基本依据：\n- 等式约束优化的 Karush–Kuhn–Tucker (KKT) 条件指出，在局部极小值点 $(x^\\star,\\lambda^\\star)$ 处，平稳性要求 $g(x^\\star)+A(x^\\star)^\\top\\lambda^\\star=0$，可行性要求 $c(x^\\star)=0$。\n- 在 $x$ 附近约束的线性化为 $c(x+p)\\approx c(x)+A(x)p$。\n- 欧几里得空间中的正交投影，以及矩阵零空间的概念。\n\n为每个测试案例实现两种模式：\n- 仅法向步：仅应用可行性恢复步 $p_k$，直到 $\\|c(x)\\|\\leq\\varepsilon_{\\text{feas}}$ 或达到最大迭代次数 $N_{\\max}$。报告达到可行性所需的迭代次数和终止时的目标函数值。\n- 耦合：在每次迭代中，计算一个可行性恢复步，然后是一个带有回溯的切向步，以减小 $f$ 的同时保持切向性。运行 $N_{\\text{run}}$ 次迭代，记录首次满足 $\\|c(x)\\|\\leq\\varepsilon_{\\text{feas}}$ 时的迭代次数，并报告最终的目标函数值。\n\n使用以下具体的目标和测试套件。所有测试案例的目标函数相同：\n- 目标函数: $f(x)=\\left(x_1-1\\right)^2+\\left(x_2+2\\right)^2+\\frac{1}{2}x_3^2$。\n- 目标函数梯度: $g(x)=\\left[2\\left(x_1-1\\right),\\,2\\left(x_2+2\\right),\\,x_3\\right]^\\top$。\n\n包含三个案例的测试套件：\n- 案例 1（理想情况，具有满行秩的线性约束）：\n  - 约束: $c(x)=\\begin{bmatrix}x_1+x_2+x_3-1\\\\ x_1-x_2\\end{bmatrix}$。\n  - 雅可比矩阵: $A(x)=\\begin{bmatrix}1 & 1 & 1\\\\ 1 & -1 & 0\\end{bmatrix}$。\n  - 初始点: $x_0=\\begin{bmatrix}3\\\\ -4\\\\ 2\\end{bmatrix}$。\n- 案例 2（带有曲率的非线性约束，角度单位为弧度）：\n  - 约束: $c(x)=\\begin{bmatrix}x_1^2+x_2-1\\\\ x_1+\\sin(x_3)-\\frac{1}{2}\\end{bmatrix}$。\n  - 雅可比矩阵: $A(x)=\\begin{bmatrix}2x_1 & 1 & 0\\\\ 1 & 0 & \\cos(x_3)\\end{bmatrix}$。\n  - 初始点: $x_0=\\begin{bmatrix}2\\\\ -1\\\\ 1\\end{bmatrix}$。\n- 案例 3（冗余线性约束，秩亏）：\n  - 约束: $c(x)=\\begin{bmatrix}x_1+x_2+x_3-1\\\\ 2x_1+2x_2+2x_3-2\\end{bmatrix}$。\n  - 雅可比矩阵: $A(x)=\\begin{bmatrix}1 & 1 & 1\\\\ 2 & 2 & 2\\end{bmatrix}$。\n  - 初始点: $x_0=\\begin{bmatrix}0\\\\ 0\\\\ 0\\end{bmatrix}$。\n\n算法参数：\n- 可行性容差: $\\varepsilon_{\\text{feas}}=10^{-8}$。\n- 仅法向步模式的最大迭代次数: $N_{\\max}=50$。\n- 耦合迭代次数: $N_{\\text{run}}=30$。\n- 初始切向步长: $\\alpha_0=1$。\n- 回溯缩减因子: $\\tau=0.5$。\n- 充分下降参数: $\\sigma=10^{-4}$。\n\n对于每个测试案例，您的程序必须计算并返回一个列表，其中包含：\n- 仅法向步模式达到可行性所需的整数迭代次数（如果未达到，则为 $N_{\\max}$）。\n- 耦合模式中首次达到可行性时的整数迭代次数（如果未达到，则为 $N_{\\text{run}}$）。\n- 仅法向步模式终止时的浮点目标函数值，四舍五入到 $6$ 位小数。\n- 耦合模式在 $N_{\\text{run}}$ 次迭代后终止时的浮点目标函数值，四舍五入到 $6$ 位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表由三个用方括号括起来的、针对每个测试案例的列表组成，例如 $\\left[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3]\\right]$，其中每个 $a_i$ 和 $b_i$ 是整数，每个 $c_i$ 和 $d_i$ 是四舍五入到 $6$ 位小数的浮点数。", "solution": "### **1. 基于原理的推导**\n\n该迭代方法包括两个不同的步骤：一个用于恢复可行性的法向步，以及一个在保持一阶可行性的同时优化目标函数的切向步。我们将从基本原理出发推导每一步。\n\n#### **可行性恢复步（法向步）**\n\n在某个迭代点 $x_k$ 处，如果 $c(x_k) \\neq 0$，我们寻求一个修正步 $p_k$ 来向满足约束的方向移动。约束在 $x_k$ 周围被线性化：$c(x_k+p) \\approx c(x_k) + A(x_k)p$。为达到可行性（$c(x_k+p) = 0$），我们必须满足线性系统 $A(x_k)p = -c(x_k)$。\n\n该系统通常是欠定的（$m \\le n$），允许无限个 $p$ 的解。一个标准的选择是具有最小欧几里得范数的解，这可以避免过大的步长。这引出了以下优化子问题：\n$$\n\\min_{p} \\frac{1}{2} \\|p\\|^2_2 \\quad \\text{subject to} \\quad A(x_k)p = -c(x_k)\n$$\n因子 $\\frac{1}{2}$ 和范数的平方是为了方便，不改变极小值点。我们构造拉格朗日函数：\n$$\n\\mathcal{L}(p, \\nu) = \\frac{1}{2}p^\\top p + \\nu^\\top(A(x_k)p + c(x_k))\n$$\n其中 $\\nu \\in \\mathbb{R}^m$ 是拉格朗日乘子向量。一阶最优性KKT条件为：\n$$\n\\nabla_p \\mathcal{L} = p + A(x_k)^\\top \\nu = 0 \\implies p = -A(x_k)^\\top \\nu\n$$\n$$\n\\nabla_\\nu \\mathcal{L} = A(x_k)p + c(x_k) = 0\n$$\n将 $p$ 的表达式代入第二个方程得到：\n$$\nA(x_k)(-A(x_k)^\\top \\nu) + c(x_k) = 0 \\implies (A(x_k)A(x_k)^\\top)\\nu = c(x_k)\n$$\n如果雅可比矩阵 $A(x_k)$ 具有满行秩，则矩阵 $A(x_k)A(x_k)^\\top$ 是可逆的，我们可以解出 $\\nu$。然而，为处理秩亏情况（如案例3），我们使用 Moore-Penrose 伪逆，记为 $A(x_k)^+$。$A(x_k)p = -c(x_k)$ 的最小范数解由以下公式给出：\n$$\np_k = -A(x_k)^+ c(x_k)\n$$\n可行性恢复的更新步骤即为 $x_{k+\\frac{1}{2}} = x_k + p_k$。\n\n#### **切向步**\n\n在法向步之后，新点 $x_{k+\\frac{1}{2}}$ 更接近可行集。切向步 $t_k$ 旨在减小目标函数 $f$ 而不立即偏离可行集。这是通过沿着在 $x_{k+\\frac{1}{2}}$ 处与约束相切的方向移动来实现的。如果一个方向 $t$ 位于约束雅可比矩阵的零空间内，即 $A(x_{k+\\frac{1}{2}})t = 0$，则该方向是切向的。\n\n为了找到一个合适的切向方向，我们可以将目标函数的负梯度 $-g(x_{k+\\frac{1}{2}})$ 投影到 $A(x_{k+\\frac{1}{2}})$ 的零空间上。这个投影梯度是切向子空间内的最速下降方向。\n\n投影到矩阵 $A$ 的零空间上的投影矩阵由 $P_N = I - A^+A$ 给出。因此，切向搜索方向 $t_{\\text{dir}}$ 为：\n$$\nt_{\\text{dir}} = -P_N g(x_{k+\\frac{1}{2}}) = -(I - A(x_{k+\\frac{1}{2}})^+A(x_{k+\\frac{1}{2}}))g(x_{k+\\frac{1}{2}})\n$$\n该方向是 $f$ 的一个下降方向，因为 $g^\\top t_{\\text{dir}} = -g^\\top P_N^\\top P_N g = -\\|P_N g\\|^2 = -\\|t_{\\text{dir}}\\|^2 \\le 0$。除非投影梯度为零（这是一个KKT最优性条件），否则下降是严格的。\n\n完整的切向步是 $t_k = \\alpha_k t_{\\text{dir}}$，其中步长 $\\alpha_k > 0$ 由回溯线搜索确定。从初始猜测 $\\alpha = \\alpha_0$ 开始，我们通过因子 $\\tau$ 迭代减小 $\\alpha$，直到满足 Armijo-Goldstein 充分下降条件：\n$$\nf(x_{k+\\frac{1}{2}} + \\alpha t_{\\text{dir}}) \\le f(x_{k+\\frac{1}{2}}) + \\sigma \\alpha g(x_{k+\\frac{1}{2}})^\\top t_{\\text{dir}}\n$$\n其中 $\\sigma \\in (0,1)$ 是一个小的常数。迭代的最终更新为 $x_{k+1} = x_{k+\\frac{1}{2}} + t_k$。\n\n### **2. 算法实现**\n\n```python\nimport numpy as np\n\n# Global problem parameters from the problem statement.\nE_FEAS = 1e-8\nN_MAX = 50\nN_RUN = 30\nALPHA0 = 1.0\nTAU = 0.5\nSIGMA = 1e-4\n\n# Objective function and its gradient.\ndef f(x):\n    \"\"\"Objective function f(x).\"\"\"\n    return (x[0] - 1)**2 + (x[1] + 2)**2 + 0.5 * x[2]**2\n\ndef g(x):\n    \"\"\"Gradient of the objective function, g(x).\"\"\"\n    return np.array([2 * (x[0] - 1), 2 * (x[1] + 2), x[2]])\n\n# Case 1: Linear constraints.\ndef c1(x):\n    \"\"\"Constraint function for Case 1.\"\"\"\n    return np.array([x[0] + x[1] + x[2] - 1, x[0] - x[1]])\n\ndef A1(x):\n    \"\"\"Jacobian of the constraints for Case 1.\"\"\"\n    return np.array([[1, 1, 1], [1, -1, 0]])\n\n# Case 2: Nonlinear constraints.\ndef c2(x):\n    \"\"\"Constraint function for Case 2.\"\"\"\n    return np.array([x[0]**2 + x[1] - 1, x[0] + np.sin(x[2]) - 0.5])\n\ndef A2(x):\n    \"\"\"Jacobian of the constraints for Case 2.\"\"\"\n    return np.array([[2 * x[0], 1, 0], [1, 0, np.cos(x[2])]])\n\n# Case 3: Redundant linear constraints.\ndef c3(x):\n    \"\"\"Constraint function for Case 3.\"\"\"\n    return np.array([x[0] + x[1] + x[2] - 1, 2 * x[0] + 2 * x[1] + 2 * x[2] - 2])\n\ndef A3(x):\n    \"\"\"Jacobian of the constraints for Case 3.\"\"\"\n    return np.array([[1, 1, 1], [2, 2, 2]])\n\ndef run_optimization(c_func, A_func, x0):\n    \"\"\"\n    Executes both the normal-only and coupled optimization regimes for a given test case.\n    \"\"\"\n    # --- Normal-only regime ---\n    x_n = np.copy(x0)\n    iter_normal = N_MAX\n    # Loop to check feasibility at the start of iteration k (0-indexed).\n    # k represents the number of iterations performed.\n    for k in range(N_MAX + 1):\n        if np.linalg.norm(c_func(x_n)) = E_FEAS:\n            iter_normal = k\n            break\n        if k == N_MAX:\n            break\n        # Feasibility restoration step\n        ck = c_func(x_n)\n        Ak = A_func(x_n)\n        # Use Moore-Penrose pseudo-inverse for robustness (handles rank-deficiency).\n        p = -np.linalg.pinv(Ak) @ ck\n        x_n += p\n    obj_normal = f(x_n)\n\n    # --- Coupled regime ---\n    x_c = np.copy(x0)\n    iter_feas_coupled = N_RUN\n    \n    # Check if the initial point is already feasible.\n    if np.linalg.norm(c_func(x_c)) = E_FEAS:\n        iter_feas_coupled = 0\n\n    for k in range(N_RUN):\n        # Normal (feasibility restoration) step\n        ck = c_func(x_c)\n        Ak = A_func(x_c)\n        p = -np.linalg.pinv(Ak) @ ck\n        x_half = x_c + p\n\n        # Tangential step\n        g_half = g(x_half)\n        A_half = A_func(x_half)\n        # Projector onto the null space of A_half\n        P_N = np.eye(x_c.shape[0]) - np.linalg.pinv(A_half) @ A_half\n        t_dir = -P_N @ g_half\n\n        t = np.zeros_like(x_c)\n        if np.linalg.norm(t_dir) > 1e-12:\n            alpha = ALPHA0\n            f_half = f(x_half)\n            g_dot_t = g_half @ t_dir\n            \n            # Backtracking line search with Armijo condition\n            max_backtrack_iter = 30 # Prevents excessive backtracking\n            for _ in range(max_backtrack_iter):\n                if f(x_half + alpha * t_dir) = f_half + SIGMA * alpha * g_dot_t:\n                    break\n                alpha *= TAU\n            else: # If loop finishes, backtracking failed.\n                 alpha = 0.0\n            \n            t = alpha * t_dir\n        \n        x_c = x_half + t\n\n        # Check for feasibility if not yet achieved\n        if iter_feas_coupled == N_RUN and np.linalg.norm(c_func(x_c)) = E_FEAS:\n            iter_feas_coupled = k + 1\n    \n    obj_coupled = f(x_c)\n\n    return [iter_normal, iter_feas_coupled, round(obj_normal, 6), round(obj_coupled, 6)]\n\ndef solve_for_internal_use():\n    \"\"\"\n    Defines and runs the test cases, then returns the results.\n    \"\"\"\n    test_cases = [\n        {\"c_func\": c1, \"A_func\": A1, \"x0\": np.array([3.0, -4.0, 2.0])},\n        {\"c_func\": c2, \"A_func\": A2, \"x0\": np.array([2.0, -1.0, 1.0])},\n        {\"c_func\": c3, \"A_func\": A3, \"x0\": np.array([0.0, 0.0, 0.0])}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_optimization(case[\"c_func\"], case[\"A_func\"], case[\"x0\"])\n        results.append(result)\n    \n    # Format the output as a single string as specified.\n    return f\"[{','.join(map(str, results))}]\"\n\n# The solve() function is for generating the final output string, not for direct execution in this context.\n# print(solve_for_internal_use())\n```", "answer": "$$\\boxed{\\text{需要运行解决方案中的代码以生成数值结果}}$$", "id": "3126067"}]}