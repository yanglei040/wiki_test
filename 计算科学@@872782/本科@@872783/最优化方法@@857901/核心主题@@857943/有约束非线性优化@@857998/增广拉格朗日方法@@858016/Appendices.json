{"hands_on_practices": [{"introduction": "要真正掌握一个算法，最好的方法莫过于亲手实践一次。这个练习将引导你手动完成一轮完整的增广拉格朗日方法迭代。通过在一个简单的一维问题上求解，你将清晰地看到该方法的两个核心步骤：如何通过求解一个无约束子问题来更新变量$x$，以及如何利用约束违反的程度来更新拉格朗日乘子$\\lambda$。这个基础练习旨在揭开算法的神秘面纱，让你对它的运作机制建立起直观的认识。[@problem_id:2208360]", "problem": "考虑约束优化问题，最小化目标函数 $f(x) = x^2$，约束条件为等式约束 $h(x) = x - 3 = 0$。\n\n增广拉格朗日方法是解决此类问题的一种迭代算法。增广拉格朗日函数定义为：\n$$L_A(x, \\lambda; \\rho) = f(x) - \\lambda h(x) + \\frac{\\rho}{2} [h(x)]^2$$\n其中 $\\lambda$ 是拉格朗日乘子估计值，$\\rho > 0$ 是惩罚参数。\n\n该方法从一个估计值 $\\lambda_k$ 开始，单次迭代包括以下两个主要步骤：\n1.  通过求解无约束最小化问题来找到下一个迭代点 $x_{k+1}$：\n    $$x_{k+1} = \\arg\\min_{x} L_A(x, \\lambda_k; \\rho)$$\n2.  使用以下公式更新拉格朗日乘子：\n    $$\\lambda_{k+1} = \\lambda_k - \\rho h(x_{k+1})$$\n\n从初始乘子估计值 $\\lambda_0 = 1$ 和惩罚参数 $\\rho = 2$ 开始，执行一次完整的增广拉格朗日方法迭代。确定得到的新迭代点 $x_1$ 和更新后的乘子 $\\lambda_1$ 的值。\n\n请用精确分数将你的答案表示为行矩阵 $\\begin{pmatrix} x_1  \\lambda_1 \\end{pmatrix}$。", "solution": "给定 $f(x) = x^{2}$ 和 $h(x) = x - 3$，增广拉格朗日函数为\n$$L_{A}(x,\\lambda;\\rho) = f(x) - \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^{2}.$$\n当 $\\lambda_{0} = 1$ 且 $\\rho = 2$ 时，该函数变为\n$$L_{A}(x,1;2) = x^{2} - 1\\cdot(x-3) + \\frac{2}{2}(x-3)^{2} = x^{2} - x + 3 + (x-3)^{2}.$$\n为了得到 $x_{1}$，求解无约束最小化问题：\n$$x_{1} = \\arg\\min_{x} L_{A}(x,1;2).$$\n求导并令导数为零：\n$$\\frac{d}{dx}L_{A}(x,1;2) = 2x - 1 + 2(x-3) = 4x - 7,$$\n$$4x - 7 = 0 \\implies x_{1} = \\frac{7}{4}.$$\n二阶导数为\n$$\\frac{d^{2}}{dx^{2}}L_{A}(x,1;2) = 4 > 0,$$\n因此 $x_{1} = \\frac{7}{4}$ 是唯一的最小化点。\n\n使用 $\\lambda_{1} = \\lambda_{0} - \\rho h(x_{1})$ 更新乘子：\n$$h(x_{1}) = \\frac{7}{4} - 3 = -\\frac{5}{4},$$\n$$\\lambda_{1} = 1 - 2\\left(-\\frac{5}{4}\\right) = 1 + \\frac{10}{4} = \\frac{7}{2}.$$\n\n因此，得到的值为 $x_{1} = \\frac{7}{4}$ 和 $\\lambda_{1} = \\frac{7}{2}$。", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{7}{4}  \\frac{7}{2} \\end{pmatrix}}$$", "id": "2208360"}, {"introduction": "在熟悉了单次迭代的完整流程后，让我们聚焦于其中最关键的一步：求解关于$x$的无约束最小化子问题。这个练习将问题扩展到二维空间，并让你深入探究惩罚参数$\\rho$如何影响子问题的解。通过分析解与$\\rho$的关系，你将理解惩罚项是如何将迭代点“拉向”可行域的，这对于理解算法如何平衡目标函数最小化和约束满足至关重要。[@problem_id:2208379]", "problem": "考虑最小化函数 $f(x_1, x_2) = x_1^2 + x_2^2$ 的优化问题，其约束条件为线性等式 $x_1 + x_2 - 2 = 0$。\n\n该问题可以使用增广拉格朗日方法来解决。对于一个目标函数为 $f(x)$ 且带有一个等式约束 $h(x) = 0$ 的一般优化问题，其增广拉格朗日函数定义为：\n$$L_\\rho(x, \\lambda) = f(x) - \\lambda h(x) + \\frac{\\rho}{2}[h(x)]^2$$\n其中 $\\lambda$ 是拉格朗日乘子的估计值，$\\rho > 0$ 是一个正的惩罚参数。\n\n该方法涉及一系列无约束最小化子问题。从一个初始乘子估计值 $\\lambda_0$ 开始，我们求解最小化 $L_\\rho(x, \\lambda_0)$ 的向量 $x^{(1)}$。\n\n你的任务是解决这第一个子问题。给定初始拉格朗日乘子估计值 $\\lambda_0 = 0$，找到最小化相应增广拉格朗日函数的向量 $x^{(1)} = (x_1^{(1)}, x_2^{(1)})$。请用惩罚参数 $\\rho$ 将你的答案表示为一个行向量。", "solution": "我们已知 $f(x_{1}, x_{2}) = x_{1}^{2} + x_{2}^{2}$ 和等式约束 $h(x) = x_{1} + x_{2} - 2 = 0$。带有参数 $\\rho > 0$ 的增广拉格朗日函数为\n$$\nL_{\\rho}(x, \\lambda) = f(x) - \\lambda h(x) + \\frac{\\rho}{2}\\left[h(x)\\right]^{2}.\n$$\n当初始乘子估计值为 $\\lambda_{0} = 0$ 时，第一个子问题是无约束最小化以下函数\n$$\nL_{\\rho}(x, 0) = x_{1}^{2} + x_{2}^{2} + \\frac{\\rho}{2}\\left(x_{1} + x_{2} - 2\\right)^{2}.\n$$\n为求其最小值点，我们将梯度设为零。令 $s = x_{1} + x_{2} - 2$。那么\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{1}} = 2x_{1} + \\rho s = 2x_{1} + \\rho(x_{1} + x_{2} - 2) = 0,\n$$\n$$\n\\frac{\\partial L_{\\rho}}{\\partial x_{2}} = 2x_{2} + \\rho s = 2x_{2} + \\rho(x_{1} + x_{2} - 2) = 0.\n$$\n这就得到了如下线性方程组\n$$\n(2+\\rho)x_{1} + \\rho x_{2} = 2\\rho, \\qquad \\rho x_{1} + (2+\\rho)x_{2} = 2\\rho.\n$$\n根据对称性，解满足 $x_{1} = x_{2} = t$。代入可得\n$$\n2t + \\rho(2t - 2) = 0 \\;\\;\\Longrightarrow\\;\\; (2 + 2\\rho)t = 2\\rho \\;\\;\\Longrightarrow\\;\\; t = \\frac{\\rho}{1+\\rho}.\n$$\n因此 $x_{1}^{(1)} = x_{2}^{(1)} = \\frac{\\rho}{1+\\rho}$。$L_{\\rho}(x,0)$ 的海森矩阵为 $2I + \\rho\\begin{pmatrix}1  1 \\\\ 1  1\\end{pmatrix}$，当 $\\rho > 0$ 时，该矩阵是正定的，因此该驻点是唯一的全局最小值点。\n\n因此，\n$$\nx^{(1)} = \\begin{pmatrix} \\frac{\\rho}{1+\\rho}  \\frac{\\rho}{1+\\rho} \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{\\rho}{1+\\rho}  \\frac{\\rho}{1+\\rho}\\end{pmatrix}}$$", "id": "2208379"}, {"introduction": "从手动计算到编写代码，我们现在进入一个更高级的实践，它揭示了增广拉格朗日方法在处理非凸问题时的精妙之处。这个练习要求你实现完整的乘子法，并将其应用于一个具有两个不同局部最优解（KKT点）的非凸问题。你将通过实验发现，拉格朗日乘子的初始估计值$\\lambda^0$并非无足轻重，它能像“导航仪”一样，引导算法收敛到不同的最优点。这项实践不仅能锻炼你的编程实现能力，更能让你深刻理解乘子在复杂优化景观中的引导作用。[@problem_id:3099666]", "problem": "构造一个完整、可运行的程序，该程序为一个小型非凸等式约束优化问题实现乘子法（也称为增广拉格朗日方法），并用它来演示不同的初始拉格朗日乘子如何引导算法收敛到不同的局部 Karush–Kuhn–Tucker (KKT) 点。\n\n您必须使用以下设置。\n\n- 决策变量：$x \\in \\mathbb{R}^2$，其分量为 $x = (x_1, x_2)$。\n- 目标函数：$f(x) = \\tfrac{1}{2}(x_1^2 + x_2^2) + \\alpha x_1$，其中 $\\alpha = 0.02$。\n- 等式约束 $h_1(x) = 0$ 和 $h_2(x) = 0$ 定义为：\n  - $h_1(x) = x_1^2 + x_2^2 - 1$，\n  - $h_2(x) = x_2 - m x_1$，其中 $m = 0.3$。\n\n使用的基本原理和定义：\n\n- 乘子法基于等式约束的增广拉格朗日函数，对于乘子 $\\lambda = (\\lambda_1,\\lambda_2)$ 和罚参数 $\\rho > 0$，定义为\n  $$\n  \\mathcal{L}_\\rho(x,\\lambda) \\;=\\; f(x) \\;+\\; \\sum_{i=1}^2 \\lambda_i h_i(x) \\;+\\; \\frac{\\rho}{2} \\sum_{i=1}^2 h_i(x)^2.\n  $$\n- 乘子法通过以下方式生成序列 $\\{x^k\\}$ 和 $\\{\\lambda^k\\}$：\n  1. $x^{k+1}$ 近似地最小化 $x \\mapsto \\mathcal{L}_\\rho(x,\\lambda^k)$，\n  2. $\\lambda^{k+1} = \\lambda^k + \\rho \\, h(x^{k+1})$，其中 $h(x) = (h_1(x),h_2(x))$。\n- 等式约束的 Karush–Kuhn–Tucker (KKT) 条件为：存在 $\\lambda^\\star$ 使得\n  $$\n  \\nabla f(x^\\star) + \\sum_{i=1}^2 \\lambda_i^\\star \\nabla h_i(x^\\star) = 0,\\quad h_1(x^\\star)=0,\\quad h_2(x^\\star)=0.\n  $$\n\n问题任务：\n\n1) 仅使用上述基本原理和定义，分析可行集 $\\{x \\in \\mathbb{R}^2 \\mid h_1(x)=0,\\, h_2(x)=0\\}$ 以确定其结构。证明约束恰好在两个点相交，并用 $m$ 显式地给出这些点，并计算出 $m = 0.3$ 时的数值。\n\n2) 使用 KKT 条件，论证两个交点都是给定函数 $f$ 的等式约束问题的 KKT 点，并写出确定每个点关联乘子 $(\\lambda_1^\\star,\\lambda_2^\\star)$ 的线性系统。您无需计算乘子的数值，但必须证明存在有限解。\n\n3) 从基本原理出发，解释为什么第二个乘子的初始值 $\\lambda_2^0$ 会对内部最小化问题 $x \\mapsto \\mathcal{L}_\\rho(x,\\lambda^k)$ 在对称初始点 $x^0=(0,0)$ 附近的下降方向产生偏置，从而将乘子法引向两个可行 KKT 点中的一个。您的推理必须从 $\\mathcal{L}_\\rho$ 的梯度公式和 $h_2$ 的性质开始。\n\n4) 实现乘子法，使用固定的罚参数 $\\rho$，并在每次外层迭代中使用一个标准的无约束求解器对 $\\mathcal{L}_\\rho(\\cdot,\\lambda^k)$ 进行内部无约束最小化。使用 $f$、$h_1$ 和 $h_2$ 的梯度，为求解器提供 $\\mathcal{L}_\\rho$ 的精确梯度。使用停止准则：约束残差范数 $\\|h(x^{k+1})\\|_2$ 小于 $10^{-8}$ 或达到最大外层迭代次数。使用初始点 $x^0=(0,0)$ 和固定的 $\\rho$。\n\n5) 测试套件。对以下四个测试用例运行您的实现，其中 $x^0=(0,0)$ 且 $\\rho=40$：\n   - 情况 A (理想路径，强正偏置): $\\lambda^0 = (0, +3)$。\n   - 情况 B (理想路径，强负偏置): $\\lambda^0 = (0, -3)$。\n   - 情况 C (中性乘子基线): $\\lambda^0 = (0, 0)$。\n   - 情况 D (边界条件：仅第一个乘子有偏置): $\\lambda^0 = (+5, 0)$。\n\n对于每种情况，在收敛后，通过第一个坐标 $x_1^\\star$ 的符号来确定算法达到了哪个 KKT 点：如果 $x_1^\\star \\ge 0$ 返回 $+1$，如果 $x_1^\\star  0$ 返回 $-1$。\n\n最终输出格式：\n\n- 您的程序应产生单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3,result4]”），每个结果是对应于测试用例 A, B, C, D 的整数 $+1$ 或 $-1$。", "solution": "该问题陈述是数值优化领域一个有效的练习。它具有科学依据、问题适定、目标明确，并包含了进行求解所需的所有必要信息。我们将按顺序解决这些问题任务。\n\n问题是求解等式约束优化问题\n$$\n\\min_{x \\in \\mathbb{R}^2} f(x) \\quad \\text{subject to} \\quad h_1(x)=0, \\; h_2(x)=0,\n$$\n其中决策变量是 $x=(x_1, x_2)$，目标函数是 $f(x) = \\frac{1}{2}(x_1^2 + x_2^2) + \\alpha x_1$ (其中 $\\alpha = 0.02$)，约束条件是 $h_1(x) = x_1^2 + x_2^2 - 1$ 和 $h_2(x) = x_2 - m x_1$ (其中 $m = 0.3$)。将使用乘子法找到解。\n\n**1. 可行集分析**\n\n可行集是所有同时满足两个约束方程的点 $x \\in \\mathbb{R}^2$ 的集合：\n$$\n\\begin{cases}\nh_1(x) = x_1^2 + x_2^2 - 1 = 0 \\\\\nh_2(x) = x_2 - m x_1 = 0\n\\end{cases}\n$$\n第一个方程 $x_1^2 + x_2^2 = 1$ 描述了以原点为中心的单位圆。第二个方程 $x_2 = m x_1$ 描述了穿过原点且斜率为 $m$ 的直线。可行集是这个圆和这条线的交集。\n\n为了找到交点，我们将第二个方程中 $x_2$ 的表达式代入第一个方程：\n$$\nx_1^2 + (m x_1)^2 = 1\n$$\n提出 $x_1^2$ 因子可得：\n$$\nx_1^2 (1 + m^2) = 1\n$$\n解出 $x_1^2$，我们得到 $x_1^2 = \\frac{1}{1 + m^2}$，这给出了 $x_1$ 的两个解：\n$$\nx_1 = \\pm \\frac{1}{\\sqrt{1+m^2}}\n$$\n我们使用 $x_2 = m x_1$ 来找到每个解对应的 $x_2$ 坐标：\n$$\nx_2 = \\pm \\frac{m}{\\sqrt{1+m^2}}\n$$\n因此，恰好有两个可行点，我们将其表示为 $x_A^\\star$ 和 $x_B^\\star$：\n$$\nx_A^\\star = \\left( \\frac{1}{\\sqrt{1+m^2}}, \\frac{m}{\\sqrt{1+m^2}} \\right) \\quad \\text{and} \\quad x_B^\\star = \\left( -\\frac{1}{\\sqrt{1+m^2}}, -\\frac{m}{\\sqrt{1+m^2}} \\right)\n$$\n对于给定值 $m = 0.3$，我们有 $1+m^2 = 1+(0.3)^2 = 1.09$。这两个可行点的数值为：\n$$\nx_A^\\star \\approx \\left( \\frac{1}{\\sqrt{1.09}}, \\frac{0.3}{\\sqrt{1.09}} \\right) \\approx (0.957826, 0.287348)\n$$\n$$\nx_B^\\star \\approx \\left( -\\frac{1}{\\sqrt{1.09}}, -\\frac{0.3}{\\sqrt{1.09}} \\right) \\approx (-0.957826, -0.287348)\n$$\n\n**2. Karush–Kuhn–Tucker (KKT) 点验证**\n\n对于一个等式约束问题，如果一个点 $x^\\star$ 是可行的，并且存在拉格朗日乘子 $\\lambda^\\star = (\\lambda_1^\\star, \\lambda_2^\\star)$ 使得拉格朗日函数的梯度为零，那么该点就是一个 KKT 点：\n$$\n\\nabla f(x^\\star) + \\sum_{i=1}^2 \\lambda_i^\\star \\nabla h_i(x^\\star) = 0\n$$\n我们已经证明了 $x_A^\\star$ 和 $x_B^\\star$ 是可行的。现在我们计算必要的梯度：\n- $\\nabla f(x) = \\begin{pmatrix} x_1 + \\alpha \\\\ x_2 \\end{pmatrix}$\n- $\\nabla h_1(x) = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix}$\n- $\\nabla h_2(x) = \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix}$\n\nKKT 平稳性条件可以写成关于乘子 $\\lambda^\\star$ 的线性系统：\n$$\n\\nabla h_1(x^\\star) \\lambda_1^\\star + \\nabla h_2(x^\\star) \\lambda_2^\\star = - \\nabla f(x^\\star)\n$$\n用矩阵形式表示，即为 $J_h(x^\\star)^T \\lambda^\\star = -\\nabla f(x^\\star)$，其中 $J_h(x^\\star)$ 是约束函数的雅可比矩阵：\n$$\n\\begin{pmatrix} 2x_1^\\star  -m \\\\ 2x_2^\\star  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1^\\star \\\\ \\lambda_2^\\star \\end{pmatrix} = -\\begin{pmatrix} x_1^\\star + \\alpha \\\\ x_2^\\star \\end{pmatrix}\n$$\n如果左侧的矩阵是可逆的，则存在 $(\\lambda_1^\\star, \\lambda_2^\\star)$ 的唯一解，而矩阵可逆的条件是其行列式不为零。行列式为：\n$$\n\\det\\begin{pmatrix} 2x_1^\\star  -m \\\\ 2x_2^\\star  1 \\end{pmatrix} = (2x_1^\\star)(1) - (-m)(2x_2^\\star) = 2x_1^\\star + 2mx_2^\\star\n$$\n在任意可行点，我们知道 $x_2^\\star = m x_1^\\star$。将此代入行列式表达式：\n$$\n2x_1^\\star + 2m(m x_1^\\star) = 2x_1^\\star (1+m^2)\n$$\n从第1部分我们知道 $x_1^\\star = \\pm 1/\\sqrt{1+m^2}$。由于 $m=0.3$，$1+m^2 \\neq 0$，因此对于两个可行点 $x_A^\\star$ 和 $x_B^\\star$，$x_1^\\star \\neq 0$。因此，行列式 $2x_1^\\star(1+m^2)$ 在这两个点上都不为零。这保证了对于这两个可行点中的每一个，都存在一对唯一的拉格朗日乘子满足 KKT 条件。因此，$x_A^\\star$ 和 $x_B^\\star$ 都是 KKT 点。\n\n**3. 来自初始乘子的偏置**\n\n乘子法迭代地求解一个形式为 $\\min_x \\mathcal{L}_\\rho(x, \\lambda^k)$ 的无约束子问题。算法的初始行为由第一个子问题的搜索方向决定，该搜索从 $x^0=(0,0)$ 和初始乘子向量 $\\lambda^0=(\\lambda_1^0, \\lambda_2^0)$ 开始。这个方向是增广拉格朗日函数的负梯度，即 $-\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0)$。\n\n增广拉格朗日函数 $\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda^T h(x) + \\frac{\\rho}{2} \\|h(x)\\|_2^2$ 的梯度是：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x, \\lambda) = \\nabla f(x) + \\sum_{i=1}^2 \\lambda_i \\nabla h_i(x) + \\rho \\sum_{i=1}^2 h_i(x) \\nabla h_i(x)\n$$\n我们在初始点 $x^0 = (0,0)$ 计算这些分量：\n- $\\nabla f(x^0) = (\\alpha, 0)^T = (0.02, 0)^T$\n- $h_1(x^0) = 0^2 + 0^2 - 1 = -1$\n- $h_2(x^0) = 0 - m(0) = 0$\n- $\\nabla h_1(x^0) = (2(0), 2(0))^T = (0, 0)^T$\n- $\\nabla h_2(x^0) = (-m, 1)^T = (-0.3, 1)^T$\n\n将这些代入梯度公式：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\nabla f(x^0) + \\lambda_1^0 \\nabla h_1(x^0) + \\lambda_2^0 \\nabla h_2(x^0) + \\rho h_1(x^0) \\nabla h_1(x^0) + \\rho h_2(x^0) \\nabla h_2(x^0)\n$$\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} + \\lambda_1^0 \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\lambda_2^0 \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix} + \\rho(-1) \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\rho(0) \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix}\n$$\n涉及 $\\nabla h_1(x^0)$ 的项为零。化简后，我们得到：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = \\begin{pmatrix} \\alpha \\\\ 0 \\end{pmatrix} + \\lambda_2^0 \\begin{pmatrix} -m \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha - m\\lambda_2^0 \\\\ \\lambda_2^0 \\end{pmatrix}\n$$\n初始搜索方向为 $d^0 = -\\nabla_x \\mathcal{L}_\\rho(x^0, \\lambda^0) = (m\\lambda_2^0 - \\alpha, -\\lambda_2^0)^T$。两个 KKT 点位于由 $x_1=0$ 定义的相反半平面中。$d^0$ 的第一个分量 $m\\lambda_2^0 - \\alpha$ 的符号决定了沿 $x_1$ 轴的初始移动方向。\n- 如果 $\\lambda_2^0$ 是一个较大的正数 (例如 $\\lambda_2^0 = 3$)，方向的第一个分量 $m\\lambda_2^0 - \\alpha = 0.3 \\times 3 - 0.02 = 0.88$ 是正的。算法初始时向正 $x_1$ 方向移动，使其偏向 $x_1 > 0$ 的 KKT 点 $x_A^\\star$。\n- 如果 $\\lambda_2^0$ 是一个较大的负数 (例如 $\\lambda_2^0 = -3$)，第一个分量 $m\\lambda_2^0 - \\alpha = 0.3 \\times (-3) - 0.02 = -0.92$ 是负的。算法初始时向负 $x_1$ 方向移动，使其偏向 $x_1  0$ 的 KKT 点 $x_B^\\star$。\n- 如果 $\\lambda_2^0 = 0$，第一个分量就是 $-\\alpha = -0.02$，这是负的。这是因为目标函数 $f(x)$ 本身就偏向于负的 $x_1$。这会使搜索偏向 $x_B^\\star$。值得注意的是，初始乘子 $\\lambda_1^0$ 对初始方向没有影响，因为 $\\nabla h_1(x^0) = 0$。\n\n这个分析表明，$\\lambda_2^0$ 的初始选择直接控制了从原点开始的初始搜索方向，从而将算法引导到两个不同局部解中的一个。\n\n**4. 算法实现与测试**\n\n乘子法被实现为一个更新拉格朗日乘子 $\\lambda^k$ 的外循环。在这个循环内部，使用拟牛顿法（来自 `scipy.optimize.minimize` 的 `BFGS`），求解关于 $x^{k+1}$ 的无约束最小化子问题，该方法非常适合光滑的无约束问题。为了效率和准确性，我们向求解器提供了增广拉格朗日函数的精确解析梯度 $\\nabla_x \\mathcal{L}_\\rho(x, \\lambda^k)$。对于每个测试用例，外循环从指定的初始值 $x^0=(0,0)$ 和 $\\lambda^0$ 以及固定的罚参数 $\\rho=40$ 开始。当由 $\\|h(x^{k+1})\\|_2$ 度量的约束违反量低于 $10^{-8}$ 的容差时，循环终止。然后根据最终收敛点 $x^\\star$ 的第一个分量 $x_1^\\star$ 的符号对其进行分类。实现此逻辑的代码在最终答案中提供。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements the method of multipliers for a nonconvex equality-constrained problem\n    and demonstrates how different initial Lagrange multipliers steer the algorithm\n    to different local KKT points.\n    \"\"\"\n\n    # --- Problem Definition ---\n    ALPHA = 0.02\n    M = 0.3\n    \n    def f(x):\n        \"\"\"Objective function.\"\"\"\n        return 0.5 * (x[0]**2 + x[1]**2) + ALPHA * x[0]\n\n    def grad_f(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return np.array([x[0] + ALPHA, x[1]])\n\n    def h(x):\n        \"\"\"Constraint functions vector h(x).\"\"\"\n        h1 = x[0]**2 + x[1]**2 - 1.0\n        h2 = x[1] - M * x[0]\n        return np.array([h1, h2])\n\n    def grad_h(x):\n        \"\"\"Jacobian of the constraint functions, returned as a list of gradients.\"\"\"\n        grad_h1 = np.array([2.0 * x[0], 2.0 * x[1]])\n        grad_h2 = np.array([-M, 1.0])\n        return [grad_h1, grad_h2]\n\n    # --- Augmented Lagrangian Method Implementation ---\n    def augmented_lagrangian_solver(x0, lambda0, rho, max_outer_iter=100, tol=1e-8):\n        \"\"\"\n        Solves the constrained optimization problem using the method of multipliers.\n\n        Args:\n            x0 (np.ndarray): Initial guess for the decision variables x.\n            lambda0 (np.ndarray): Initial guess for the Lagrange multipliers.\n            rho (float): Penalty parameter.\n            max_outer_iter (int): Maximum number of outer loop iterations.\n            tol (float): Tolerance for constraint violation norm.\n\n        Returns:\n            np.ndarray: The solution vector x.\n        \"\"\"\n        x_k = np.copy(x0)\n        lambda_k = np.copy(lambda0)\n\n        for k in range(max_outer_iter):\n            # Define the augmented Lagrangian and its gradient for the current lambda_k\n            def L_rho(x):\n                h_x = h(x)\n                return f(x) + np.dot(lambda_k, h_x) + (rho / 2.0) * np.dot(h_x, h_x)\n\n            def grad_L_rho(x):\n                h_x = h(x)\n                grads_h_x = grad_h(x)\n                \n                # Gradient of penalty term section\n                penalty_grad_term = np.zeros(2)\n                for i in range(2):\n                    penalty_grad_term += h_x[i] * grads_h_x[i]\n                \n                # Gradient of lambda term section\n                lambda_grad_term = np.zeros(2)\n                for i in range(2):\n                    lambda_grad_term += lambda_k[i] * grads_h_x[i]\n\n                return grad_f(x) + lambda_grad_term + rho * penalty_grad_term\n\n            # Solve the unconstrained subproblem\n            # Start the minimization from the previous iterate x_k\n            res = minimize(L_rho, x_k, method='BFGS', jac=grad_L_rho)\n            x_k_plus_1 = res.x\n\n            # Check for convergence\n            h_next = h(x_k_plus_1)\n            constraint_residual = np.linalg.norm(h_next)\n            \n            if constraint_residual  tol:\n                return x_k_plus_1\n            \n            # Update multipliers\n            lambda_k = lambda_k + rho * h_next\n            \n            # Update x for the next iteration\n            x_k = x_k_plus_1\n            \n        return x_k\n\n    # --- Test Suite ---\n    x0 = np.array([0.0, 0.0])\n    rho = 40.0\n\n    test_cases = [\n        # Case A: Strong positive bias\n        np.array([0.0, 3.0]),\n        # Case B: Strong negative bias\n        np.array([0.0, -3.0]),\n        # Case C: Neutral multiplier baseline\n        np.array([0.0, 0.0]),\n        # Case D: Bias in the first multiplier only\n        np.array([5.0, 0.0]),\n    ]\n\n    results = []\n    for lambda0_case in test_cases:\n        x_star = augmented_lagrangian_solver(x0, lambda0_case, rho)\n        \n        # Identify which KKT point was reached\n        if x_star[0] >= 0:\n            results.append(1)\n        else:\n            results.append(-1)\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3099666"}]}