{"hands_on_practices": [{"introduction": "预测-校正方法的核心思想是先做一个初步的预测，然后再修正这个预测以满足某些条件。为了建立对这一思想的直观理解，我们从一个简单的单变量优化问题开始。在这个练习中，我们将把一个无约束最优解（预测值）通过罚函数法拉回到可行域内（校正），并精确地量化校正的强度与罚参数之间的关系 [@problem_id:3163753]，从而揭示校正步骤的本质。", "problem": "考虑一个一维约束优化问题，其目标函数为 $f(x) = \\frac{1}{2}(x - 1)^{2}$，非线性不等式约束为 $g(x) = x - \\frac{1}{2} \\leq 0$。该问题采用一种预测-校正方案：预测步通过求解无约束问题（忽略任何非激活约束）获得，校正步则通过最小化惩罚目标函数 $F_{\\rho}(x) = f(x) + \\rho \\,\\max\\!\\big(0, g(x)\\big)^{2}$ 获得，其中 $\\rho > 0$ 是惩罚参数。\n\n从无约束最小化（目标函数的一阶平稳性）的基本定义和惩罚函数（其中 max 算子定义了一个分段平滑校正）的定义出发，推导由惩罚目标产生的校正后极小值点 $x^{c}(\\rho)$。将校正强度 $I(\\rho)$ 定义为从预测值到校正值的绝对位移，即 $I(\\rho) = \\big|x^{p} - x^{c}(\\rho)\\big|$，其中 $x^{p}$ 表示预测值。请给出 $I(\\rho)$ 作为 $\\rho$ 函数的闭式解析表达式。\n\n你的最终答案必须是 $I(\\rho)$ 的单一解析表达式，且不得包含任何单位或其他评论。", "solution": "本问题要求推导应用于一个一维约束优化问题的特定预测-校正方案的校正强度 $I(\\rho)$。\n\n目标函数为 $f(x) = \\frac{1}{2}(x - 1)^{2}$，不等式约束为 $g(x) = x - \\frac{1}{2} \\leq 0$。\n\n首先，我们确定预测解 $x^{p}$。问题指出，预测步通过求解无约束问题获得。这意味着我们必须找到使 $f(x)$ 最小化的 $x$ 值，而不考虑约束 $g(x) \\leq 0$。\n\n函数 $f(x)$ 是一个严格凸二次函数。其最小值可以通过找到其一阶导数为零的点来求得。\n$$f'(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(x - 1)^{2} \\right] = x - 1$$\n令导数为零可得：\n$$x - 1 = 0 \\implies x = 1$$\n二阶导数为 $f''(x) = 1 > 0$，这证实了 $x=1$ 是一个极小值点。\n因此，预测解为 $x^{p} = 1$。\n我们可以检查该预测值是否满足约束条件：$g(x^p) = g(1) = 1 - \\frac{1}{2} = \\frac{1}{2}$。由于 $\\frac{1}{2} \\not\\leq 0$，预测值违反了约束条件。\n\n接下来，我们确定校正解 $x^{c}(\\rho)$。它是通过最小化惩罚目标函数 $F_{\\rho}(x)$ 获得的，定义如下：\n$$F_{\\rho}(x) = f(x) + \\rho \\,\\max\\!\\big(0, g(x)\\big)^{2}$$\n代入 $f(x)$ 和 $g(x)$ 的表达式：\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} + \\rho \\,\\max\\!\\big(0, x - \\frac{1}{2}\\big)^{2}$$\n$\\max$ 算子的存在使其成为一个分段函数。我们根据 $g(x) = x - \\frac{1}{2}$ 的符号对其进行分析。\n\n情况 1：$x \\leq \\frac{1}{2}$。\n在此区域内，$x - \\frac{1}{2} \\leq 0$，因此 $\\max(0, x - \\frac{1}{2}) = 0$。惩罚函数简化为：\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} \\quad \\text{for } x \\leq \\frac{1}{2}$$\n我们需要在区间 $(-\\infty, \\frac{1}{2}]$ 上找到该函数的最小值。无约束情况下的极小值点在 $x=1$，它不在此区间内。由于函数 $f(x)$ 在 $x < 1$ 时是严格递减的，因此它在区间 $(-\\infty, \\frac{1}{2}]$ 上的最小值出现在边界点 $x = \\frac{1}{2}$。\n\n情况 2：$x > \\frac{1}{2}$。\n在此区域内，$x - \\frac{1}{2} > 0$，因此 $\\max(0, x - \\frac{1}{2}) = x - \\frac{1}{2}$。惩罚函数为：\n$$F_{\\rho}(x) = \\frac{1}{2}(x - 1)^{2} + \\rho \\left(x - \\frac{1}{2}\\right)^{2} \\quad \\text{for } x > \\frac{1}{2}$$\n这是一个可微函数。为求其最小值，我们计算其一阶导数并令其为零。\n$$F'_{\\rho}(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(x - 1)^{2} + \\rho \\left(x - \\frac{1}{2}\\right)^{2} \\right]$$\n$$F'_{\\rho}(x) = (x - 1) + 2\\rho \\left(x - \\frac{1}{2}\\right) = x - 1 + 2\\rho x - \\rho$$\n$$F'_{\\rho}(x) = (1 + 2\\rho)x - (1 + \\rho)$$\n令导数为零以找到驻点：\n$$(1 + 2\\rho)x - (1 + \\rho) = 0$$\n$$(1 + 2\\rho)x = 1 + \\rho$$\n$$x = \\frac{1 + \\rho}{1 + 2\\rho}$$\n我们必须验证该驻点位于此情况的区域内，即 $x > \\frac{1}{2}$。我们检验该不等式：\n$$\\frac{1 + \\rho}{1 + 2\\rho} > \\frac{1}{2}$$\n由于 $\\rho > 0$，分母 $1 + 2\\rho$ 为正，因此我们可以将两边同乘以 $2(1+2\\rho)$ 而不改变不等号方向：\n$$2(1 + \\rho) > 1(1 + 2\\rho)$$\n$$2 + 2\\rho > 1 + 2\\rho$$\n$$2 > 1$$\n对于任何 $\\rho > 0$，此不等式恒成立。因此，该驻点总是在区域 $x > \\frac{1}{2}$ 内。二阶导数为 $F''_{\\rho}(x) = 1 + 2\\rho$，对于 $\\rho > 0$ 其值为正，证实了这是一个极小值点。\n\n函数 $F_{\\rho}(x)$ 在 $\\mathbb{R}$ 上是连续且可微的。在边界 $x = \\frac{1}{2}$ 处的左右导数相匹配。因此，$F_{\\rho}(x)$ 的全局最小值是其唯一的驻点。该极小值点即为校正解：\n$$x^{c}(\\rho) = \\frac{1 + \\rho}{1 + 2\\rho}$$\n\n最后，我们计算校正强度 $I(\\rho)$，它被定义为从预测值到校正值的绝对位移：\n$$I(\\rho) = |x^{p} - x^{c}(\\rho)|$$\n代入 $x^p$ 和 $x^c(\\rho)$ 的表达式：\n$$I(\\rho) = \\left| 1 - \\frac{1 + \\rho}{1 + 2\\rho} \\right|$$\n为简化绝对值内的表达式，我们通分：\n$$I(\\rho) = \\left| \\frac{1 + 2\\rho}{1 + 2\\rho} - \\frac{1 + \\rho}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{(1 + 2\\rho) - (1 + \\rho)}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{1 + 2\\rho - 1 - \\rho}{1 + 2\\rho} \\right|$$\n$$I(\\rho) = \\left| \\frac{\\rho}{1 + 2\\rho} \\right|$$\n由于题目规定 $\\rho > 0$，分子 $\\rho$ 和分母 $1 + 2\\rho$ 均为严格正数。因此，绝对值是多余的。\n$$I(\\rho) = \\frac{\\rho}{1 + 2\\rho}$$\n这就是校正强度作为惩罚参数 $\\rho$ 的函数的闭式解析表达式。", "answer": "$$\n\\boxed{\\frac{\\rho}{1 + 2\\rho}}\n$$", "id": "3163753"}, {"introduction": "在掌握了基本的预测-校正概念后，我们转向一个更实际的多维约束优化问题：在概率单纯形上进行最小化。这个练习将对比两种重要的校正策略：欧几里得投影和熵正则化。通过亲手实现并比较这两种方法 [@problem_id:3163727]，你将深入理解校正机制的设计选择如何影响算法的行为和性能，特别是在处理具有特定几何结构（如概率分布）的约束时。", "problem": "考虑在概率单纯形上最小化一个光滑凸函数。设单纯形定义为 $\\Delta = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, \\; x_i \\ge 0\\}$。设目标函数为 $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$，其中 $Q \\in \\mathbb{R}^{n \\times n}$ 是对称半正定矩阵，$c \\in \\mathbb{R}^n$。其梯度为 $\\nabla f(x) = Q x + c$。从投影梯度法和使用熵邻近函数的镜像下降法的基本定义出发，为这个约束最小化问题构建一个预测-校正方案，该方案包含两种变体：\n\n- 变体 A (欧几里得投影)：通过一个梯度步进行预测，并通过投影到定义 $\\Delta$ 的仿射约束和不等式约束上来进行校正。\n- 变体 B (熵校正)：通过一个梯度步进行预测，并使用熵邻近（Kullback-Leibler 散度）进行校正，以保持在 $\\Delta$ 上。\n\n你的任务是：\n- 基于无约束梯度下降原理，构建预测步。\n- 使用必要最优性条件，从第一性原理推导到 $\\Delta$ 上的欧几里得投影校正器。\n- 通过最小化一个由 Kullback-Leibler 散度正则化的线性化目标，从第一性原理推导熵校正更新。\n- 将两种变体实现为具有固定步长 $\\alpha$ 和固定迭代次数 $T$ 的迭代方案。\n- 对于每个测试用例，从 $\\Delta$ 中的相同初始点开始运行两种变体 $T$ 步，并比较它们的最终目标值。\n\n最终程序必须：\n- 根据推导忠实地实现两种方案。\n- 对于每个测试用例，计算一个布尔输出，表明变体 B 是否比变体 A 获得了严格更低的最终目标值。\n\n使用以下测试套件，其中每个用例指定了 $(n, Q, c, \\alpha, T, x^{(0)})$：\n1. 顺利路径: $n=3$, $Q=\\begin{bmatrix}2  &-1  &0 \\\\ -1  &2  &-1 \\\\ 0  &-1  &2\\end{bmatrix}$, $c=\\begin{bmatrix}0.5 \\\\ -0.2 \\\\ 0.1\\end{bmatrix}$, $\\alpha=0.2$, $T=200$, $x^{(0)}=\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3}\\end{bmatrix}$。\n2. 边界起始: $n=2$, $Q=\\begin{bmatrix}1  &0.2 \\\\ 0.2  &1\\end{bmatrix}$, $c=\\begin{bmatrix}0.0 \\\\ 0.4\\end{bmatrix}$, $\\alpha=0.5$, $T=100$, $x^{(0)}=\\begin{bmatrix}1.0 \\\\ 0.0\\end{bmatrix}$。\n3. 大步长: $n=4$, $Q=\\operatorname{diag}(1.5, 2.0, 0.7, 0.3)$, $c=\\begin{bmatrix}-0.3 \\\\ 0.1 \\\\ 0.0 \\\\ 0.2\\end{bmatrix}$, $\\alpha=1.0$, $T=60$, $x^{(0)}=\\begin{bmatrix}0.25 \\\\ 0.25 \\\\ 0.25 \\\\ 0.25\\end{bmatrix}$。\n4. 平坦二次型（线性目标）: $n=3$, $Q=\\begin{bmatrix}0  &0  &0 \\\\ 0  &0  &0 \\\\ 0  &0  &0\\end{bmatrix}$, $c=\\begin{bmatrix}0.1 \\\\ -0.05 \\\\ 0.0\\end{bmatrix}$, $\\alpha=0.3$, $T=100$, $x^{(0)}=\\begin{bmatrix}\\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3}\\end{bmatrix}$。\n5. 更高维度: $n=5$, $Q=\\operatorname{diag}(0.5, 1.0, 1.5, 0.8, 1.2)$, $c=\\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.05 \\\\ 0.0 \\\\ -0.2\\end{bmatrix}$, $\\alpha=0.15$, $T=150$, $x^{(0)}=\\begin{bmatrix}0.5 \\\\ 0.2 \\\\ 0.1 \\\\ 0.1 \\\\ 0.1\\end{bmatrix}$。\n\n你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[result_1,result_2,result_3,result_4,result_5]$），其中每个 $result_i$ 是测试用例 $i$ 的布尔值，表示变体 B 是否严格优于变体 A（即，其最终目标值是否严格更低）。", "solution": "该问题要求构建、实现和比较两种预测-校正方案，用于在概率单纯形 $\\Delta = \\{x \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n x_i = 1, \\; x_i \\ge 0\\}$ 上最小化光滑凸二次函数 $f(x) = \\frac{1}{2} x^\\top Q x + c^\\top x$。矩阵 $Q$ 是对称半正定的，这保证了 $f(x)$ 的凸性。\n\n用于约束优化的预测-校正方法的一般迭代结构如下：\n1.  **预测步**：从当前迭代点 $x^{(k)}$ 出发，基于目标函数的某些局部信息（通常忽略约束）走一步。这会产生一个候选点 $y^{(k+1)}$。\n2.  **校正步**：调整候选点 $y^{(k+1)}$ 以产生满足问题约束的下一个迭代点 $x^{(k+1)}$。\n\n我们将推导此方案的两种变体，它们的区别在于校正步。预测步对两者是共同的，并且基于标准的梯度下降更新。\n\n**预测步**\n\n给定当前迭代点 $x^{(k)} \\in \\Delta$，预测步执行一次无约束梯度下降更新。目标函数的梯度为 $\\nabla f(x) = Qx + c$。预测步以固定步长 $\\alpha > 0$ 沿负梯度方向移动：\n$$\ny^{(k+1)} = x^{(k)} - \\alpha \\nabla f(x^{(k)})\n$$\n点 $y^{(k+1)}$ 通常在单纯形 $\\Delta$ 之外，因为它不一定满足 $\\sum_{i=1}^n y_i^{(k+1)} = 1$ 或 $y_i^{(k+1)} \\ge 0$。\n\n**变体 A：通过欧几里得投影进行校正**\n\n这个变体被称为投影梯度下降法，它通过在 $\\Delta$ 中找到距预测点 $y^{(k+1)}$ 欧几里得距离最近的点来对其进行校正。这个校正是以下二次规划子问题的解：\n$$\nx^{(k+1)} = \\operatorname{proj}_{\\Delta}(y^{(k+1)}) = \\arg\\min_{z \\in \\Delta} \\frac{1}{2} \\|z - y^{(k+1)}\\|_2^2\n$$\n为求解此问题，我们为带有约束 $z \\ge 0$ 和 $\\mathbf{1}^\\top z = 1$ 的最小化问题构建拉格朗日函数：\n$$\nL(z, \\lambda, \\nu) = \\frac{1}{2} \\sum_{i=1}^n (z_i - y_i^{(k+1)})^2 - \\sum_{i=1}^n \\lambda_i z_i + \\nu \\left(\\sum_{i=1}^n z_i - 1\\right)\n$$\n其中 $\\lambda_i \\ge 0$ 是非负约束的拉格朗日乘子，$\\nu$ 是等式约束的乘子。最优性的 Karush-Kuhn-Tucker (KKT) 条件是：\n1.  **平稳性**: $\\frac{\\partial L}{\\partial z_i} = z_i - y_i^{(k+1)} - \\lambda_i + \\nu = 0$，对于 $i=1, \\dots, n$。\n2.  **原始可行性**: $\\sum_{i=1}^n z_i = 1$ 和 $z_i \\ge 0$。\n3.  **对偶可行性**: $\\lambda_i \\ge 0$。\n4.  **互补松弛性**: $\\lambda_i z_i = 0$。\n\n从平稳性条件，我们得到 $z_i = y_i^{(k+1)} + \\lambda_i - \\nu$。\n从互补松弛性，如果 $z_i > 0$，则 $\\lambda_i = 0$，这意味着 $z_i = y_i^{(k+1)} - \\nu$。\n如果 $z_i = 0$，则 $\\lambda_i \\ge 0$。从平稳性条件，$\\lambda_i = \\nu - y_i^{(k+1)} \\ge 0$，这意味着 $y_i^{(k+1)} \\le \\nu$。\n综合这些观察， $z_i$ 的解可以紧凑地用单个乘子 $\\nu$ 表示：\n$$\nz_i = \\max(0, y_i^{(k+1)} - \\nu)\n$$\n$\\nu$ 的值由等式约束 $\\sum_{i=1}^n z_i = 1$ 确定：\n$$\n\\sum_{i=1}^n \\max(0, y_i^{(k+1)} - \\nu) = 1\n$$\n函数 $g(\\nu) = \\sum_{i=1}^n \\max(0, y_i^{(k+1)} - \\nu)$ 是一个关于 $\\nu$ 的连续、分段线性和非增函数。求解 $g(\\nu)=1$ 可以高效完成。一个标准算法包括将 $y^{(k+1)}$ 的分量按降序排序，即 $y_{(1)} \\ge y_{(2)} \\ge \\dots \\ge y_{(n)}$。然后找到一个索引 $\\rho \\in \\{1, \\dots, n\\}$，使得最优的 $\\nu$ 位于区间 $[y_{(\\rho+1)}, y_{(\\rho)}]$ 内。这会得到一个基于已排序向量的前 $\\rho$ 个分量的 $\\nu$ 的闭式解。一旦找到 $\\nu$，就可以计算出投影 $x^{(k+1)}$。\n\n**变体 B：通过熵正则化进行校正**\n\n这个变体是镜像下降算法的一个实例，它将预测步和校正步合并为单次更新。它通过最小化在 $x^{(k)}$ 处的目标函数的线性近似（由 Kullback-Leibler (KL) 散度正则化）来找到下一个迭代点 $x^{(k+1)}$。KL 散度在单纯形上作为一种邻近性度量（Bregman 散度）。KL 散度定义为 $D_{KL}(z \\| x) = \\sum_{i=1}^n z_i \\log\\left(\\frac{z_i}{x_i^{(k)}}\\right)$。更新规则由下式给出：\n$$\nx^{(k+1)} = \\arg\\min_{z \\in \\Delta} \\left\\{ \\langle \\nabla f(x^{(k)}), z \\rangle + \\frac{1}{\\alpha} D_{KL}(z \\| x^{(k)}) \\right\\}\n$$\n该子问题的目标可以写为：\n$$\nJ(z) = \\alpha \\sum_{i=1}^n (\\nabla_i f(x^{(k)})) z_i + \\sum_{i=1}^n z_i \\log z_i - \\sum_{i=1}^n z_i \\log x_i^{(k)}\n$$\n我们在约束 $\\sum_{i=1}^n z_i = 1$ 和 $z_i \\ge 0$ 下最小化 $J(z)$。假设搜索空间为单纯形的内部（以保证可微性），我们建立拉格朗日函数：\n$$\nL(z, \\mu) = J(z) + \\mu \\left(\\sum_{i=1}^n z_i - 1\\right)\n$$\n对 $z_i$ 求导并令其为零，得到：\n$$\n\\frac{\\partial L}{\\partial z_i} = \\alpha \\nabla_i f(x^{(k)}) + (\\log z_i + 1) - \\log x_i^{(k)} + \\mu = 0\n$$\n解出 $z_i$：\n$$\n\\log z_i = \\log x_i^{(k)} - \\alpha \\nabla_i f(x^{(k)}) - \\mu - 1\n$$\n$$\nz_i = \\exp(\\log x_i^{(k)} - \\alpha \\nabla_i f(x^{(k)}) - \\mu - 1) = x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)})) \\exp(-\\mu - 1)\n$$\n项 $\\exp(-\\mu - 1)$ 是一个归一化常数，我们称之为 $C$。因此，$z_i = C \\cdot x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))$。我们使用约束 $\\sum_i z_i = 1$ 来找到 $C$：\n$$\n1 = \\sum_{i=1}^n z_i = C \\sum_{i=1}^n x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))\n$$\n这给出了 $x^{(k+1)} = z$ 的更新规则：\n$$\nx_i^{(k+1)} = \\frac{x_i^{(k)} \\exp(-\\alpha \\nabla_i f(x^{(k)}))}{\\sum_{j=1}^n x_j^{(k)} \\exp(-\\alpha \\nabla_j f(x^{(k)}))}\n$$\n这就是指数化梯度更新。它巧妙地保证了如果 $x^{(k)} \\in \\Delta$，那么 $x^{(k+1)}$ 也自动属于 $\\Delta$（所有分量都非负且总和为 1）。如果某个分量 $x_i^{(k)}$ 为 0，它在所有后续迭代中都将保持为 0。\n\n**总结与比较**\n\n-   **变体 A (投影梯度下降)**：在每一步中，它执行一个标准的欧几里得梯度步，然后将结果投影回单纯形上。投影操作可能计算量很大，但能确保迭代点是可行集中离无约束更新点最近的点。\n-   **变体 B (镜像下降/指数化梯度)**：它使用由熵函数导出的非欧几何。其更新是乘性的，并且比欧几里得投影在计算上更简单。它内在地尊重单纯形边界，特别适用于概率分布。\n\n实现部分将对两种算法执行固定的迭代次数 $T$，并比较最终的目标函数值，以确定在每个测试用例中哪个变体表现更好。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path\n        {'n': 3, 'Q': np.array([[2, -1, 0], [-1, 2, -1], [0, -1, 2]]), \n         'c': np.array([0.5, -0.2, 0.1]), 'alpha': 0.2, 'T': 200, \n         'x0': np.array([1/3, 1/3, 1/3])},\n        # Case 2: Boundary start\n        {'n': 2, 'Q': np.array([[1, 0.2], [0.2, 1]]), \n         'c': np.array([0.0, 0.4]), 'alpha': 0.5, 'T': 100, \n         'x0': np.array([1.0, 0.0])},\n        # Case 3: Large step size\n        {'n': 4, 'Q': np.diag([1.5, 2.0, 0.7, 0.3]), \n         'c': np.array([-0.3, 0.1, 0.0, 0.2]), 'alpha': 1.0, 'T': 60, \n         'x0': np.array([0.25, 0.25, 0.25, 0.25])},\n        # Case 4: Flat quadratic (linear objective)\n        {'n': 3, 'Q': np.zeros((3, 3)), \n         'c': np.array([0.1, -0.05, 0.0]), 'alpha': 0.3, 'T': 100, \n         'x0': np.array([1/3, 1/3, 1/3])},\n        # Case 5: Higher dimension\n        {'n': 5, 'Q': np.diag([0.5, 1.0, 1.5, 0.8, 1.2]), \n         'c': np.array([0.2, -0.1, 0.05, 0.0, -0.2]), 'alpha': 0.15, 'T': 150, \n         'x0': np.array([0.5, 0.2, 0.1, 0.1, 0.1])}\n    ]\n\n    results = []\n    for case in test_cases:\n        Q, c, alpha, T, x0 = case['Q'], case['c'], case['alpha'], case['T'], case['x0']\n\n        # Run Variant A\n        x_final_A = run_variant_A(Q, c, alpha, T, x0)\n        \n        # Run Variant B\n        x_final_B = run_variant_B(Q, c, alpha, T, x0)\n\n        # Calculate final objective values\n        f_A = objective_function(x_final_A, Q, c)\n        f_B = objective_function(x_final_B, Q, c)\n        \n        # Compare and store the boolean result\n        results.append(f_B < f_A)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef objective_function(x, Q, c):\n    \"\"\"Computes the objective function f(x) = 0.5 * x.T @ Q @ x + c.T @ x.\"\"\"\n    return 0.5 * x.T @ Q @ x + c.T @ x\n\ndef gradient(x, Q, c):\n    \"\"\"Computes the gradient of the objective function.\"\"\"\n    return Q @ x + c\n\ndef project_onto_simplex(y):\n    \"\"\"Projects a vector y onto the probability simplex.\"\"\"\n    n = len(y)\n    y_sorted = np.sort(y)[::-1]\n    y_cumsum = np.cumsum(y_sorted)\n    \n    # Find rho: the number of positive components in the projection\n    candidates = y_sorted - (y_cumsum - 1) / np.arange(1, n + 1)\n    rho_candidates = np.where(candidates > 0)[0]\n    \n    if len(rho_candidates) == 0:\n        # This case should not be hit with valid inputs but is here for robustness.\n        # It implies all y_i are very small, and we should find a corner.\n        # The logic will default to picking the largest y_i and setting its projection to 1.\n        rho = 1\n    else:    \n        rho = rho_candidates[-1] + 1\n    \n    theta = (y_cumsum[rho - 1] - 1) / rho\n    \n    x_proj = np.maximum(0, y - theta)\n    return x_proj\n\ndef run_variant_A(Q, c, alpha, T, x0):\n    \"\"\"Implements Variant A: Predictor-corrector with Euclidean projection.\"\"\"\n    x = np.copy(x0)\n    for _ in range(T):\n        grad = gradient(x, Q, c)\n        y = x - alpha * grad\n        x = project_onto_simplex(y)\n    return x\n\ndef run_variant_B(Q, c, alpha, T, x0):\n    \"\"\"Implements Variant B: Predictor-corrector with entropic correction.\"\"\"\n    x = np.copy(x0)\n    for _ in range(T):\n        grad = gradient(x, Q, c)\n        # Using a guard for exp to prevent overflow/underflow on extreme grad values.\n        # Clip the argument of exp to a reasonable range.\n        exp_arg = -alpha * grad\n        # A large positive value in exp_arg can cause overflow.\n        # A large negative value can cause underflow to zero.\n        # Clipping helps maintain numerical stability.\n        exp_arg = np.clip(exp_arg, -700, 700)\n        x_unnormalized = x * np.exp(exp_arg)\n        # Normalize to stay on the simplex\n        x = x_unnormalized / np.sum(x_unnormalized)\n    return x\n\nsolve()\n```", "id": "3163727"}]}