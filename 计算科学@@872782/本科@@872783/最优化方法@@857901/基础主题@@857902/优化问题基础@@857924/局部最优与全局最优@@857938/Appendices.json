{"hands_on_practices": [{"introduction": "理论知识是基础，但真正的理解来自于实践。本节将通过一系列动手实践，加深你对局部最优和全局最优概念的理解。我们将从一个简单的一维非凸函数入手，比较最基本的一阶方法（梯度下降法）和二阶方法（牛顿法）的行为。通过这个练习[@problem_id:3145146]，你将亲眼看到初始点的选择如何决定算法收敛到哪个局部最小值，并理解为何牛顿法虽快，却对函数的局部曲率异常敏感。", "problem": "考虑使用两种方法来最小化一个非凸标量函数：带回溯线搜索的梯度下降法和带回溯线搜索的牛顿法。您必须使用的基本概念和定义是：局部最小值和全局最小值的定义、可微函数的梯度和 Hessian 矩阵的概念，以及一阶和二阶泰勒近似。如果在一个点 $x^{\\star}$ 周围存在一个邻域，对于该邻域中的所有 $x$ 都满足 $f(x^{\\star}) \\leq f(x)$，则称 $x^{\\star}$ 是一个局部最小值。一个全局最小值 $x^{\\mathrm{glob}}$ 满足对于定义域中的所有 $x$，都有 $f(x^{\\mathrm{glob}}) \\leq f(x)$。梯度下降法源于一阶泰勒近似，而牛顿法源于二阶泰勒近似。\n\n您必须在标量函数\n$$\nf(x) = (x^2 - 1)^2 + 0.3\\,x,\n$$\n上实现这两种方法，其梯度为\n$$\n\\nabla f(x) = 4x^3 - 4x + 0.3,\n$$\n其 Hessian 矩阵为\n$$\n\\nabla^2 f(x) = 12x^2 - 4.\n$$\n回溯线搜索必须强制执行 Armijo 充分下降条件：对于一个试探步 $x_{k+1} = x_k + \\alpha p_k$，如果满足\n$$\nf(x_{k+1}) \\leq f(x_k) + c\\,\\alpha\\,\\nabla f(x_k)\\,p_k,\n$$\n则接受 $x_{k+1}$，其中 $c \\in (0,1)$ 为固定值。在回溯线搜索中，使用 $c = 10^{-4}$ 和收缩因子 $\\rho = 0.5$。对于梯度下降法，使用搜索方向 $p_k = -\\nabla f(x_k)$。对于牛顿法，当 $\\nabla^2 f(x_k) \\neq 0$ 时，使用一维牛顿方向 $p_k = -\\nabla f(x_k)/\\nabla^2 f(x_k)$。如果牛顿方向不是下降方向，即 $\\nabla f(x_k)\\,p_k \\geq 0$，则声明牛顿法在 $x_k$ 处失败，原因是局部曲率引导该方法走向了错误的盆地，并且不再从该初始条件继续进行。\n\n您的程序必须：\n- 精确地按照规定实现这两种方法，最大迭代次数为 $100$ 次，梯度范数的终止容差为 $\\varepsilon = 10^{-8}$，即当 $|\\nabla f(x_k)| \\leq \\varepsilon$ 时停止。\n- 通过对三次方程 $4x^3 - 4x + 0.3 = 0$ 进行多项式求根来精确求解 $\\nabla f(x) = 0$，从而确定驻点集合；使用 $\\nabla^2 f(x)$ 的符号对最小值点进行分类，并通过比较它们的函数值来确定最小值点中的全局最小值。\n- 对于每次运行，确定该方法是否收敛、使用的迭代次数、最终点 $x_{\\mathrm{final}}$，以及最终值 $f(x_{\\mathrm{final}})$ 是否在 $10^{-8}$ 的容差内与全局最小值匹配。\n\n测试套件：\n从以下初始点运行这两种方法：\n- $x_0 = -0.2$ (测试原点附近的非凸曲率)，\n- $x_0 = 0.8$ (测试向右侧较浅的局部盆地的吸引)，\n- $x_0 = -0.8$ (测试向左侧更深的全局盆地的吸引)。\n\n输出规格：\n对于每个初始点，生成一个包含以下五个条目的列表：\n- 一个布尔值，表示牛顿法是否达到全局最小值，\n- 一个布尔值，表示梯度下降法是否达到全局最小值，\n- 一个布尔值，表示牛顿法所需的迭代次数是否严格少于梯度下降法（如果牛顿法未能产生一个步长，则将此布尔值声明为假），\n- 一个整数，表示牛顿法使用的迭代次数（如果牛顿法立即失败，则使用 $0$），\n- 一个整数，表示梯度下降法使用的迭代次数。\n\n您的程序应生成单行输出，其中包含三个初始点的结果，格式为一个逗号分隔的列表，并用方括号括起来，不含空格，其中每个元素是如上所述的列表，例如，$[\\,[\\cdots],\\,[\\cdots],\\,[\\cdots]\\,]$，格式化且无空格。", "solution": "该问题是有效的。这是一个适定且具有科学依据的数值优化练习，旨在比较梯度下降法和牛顿法在一维非凸函数上的行为。所有参数和条件都已明确指定。\n\n问题的核心是在最小化函数 $f(x) = (x^2 - 1)^2 + 0.3x$ 时，分析一阶方法（梯度下降法）与二阶方法（牛顿法）的性能。该函数的非凸性，其特点是同时存在正曲率和负曲率区域，旨在突出这两种方法之间的根本差异。\n\n首先，我们必须通过识别函数的驻点并对其进行分类来刻画函数 $f(x)$。驻点是梯度 $\\nabla f(x) = 4x^3 - 4x + 0.3 = 0$ 的根。对这个三次多项式使用数值求根算法，我们找到三个实根：\n$x_1^* \\approx -1.03653191$\n$x_2^* \\approx 0.07530320$\n$x_3^* \\approx 0.96122871$\n\n为了对这些驻点进行分类，我们使用二阶充分条件，即在每个点上评估 Hessian 矩阵 $\\nabla^2 f(x) = 12x^2 - 4$ 的符号。\n- 对于 $x_1^* \\approx -1.0365$：$\\nabla^2 f(x_1^*) = 12(-1.0365)^2 - 4 \\approx 8.89  0$。这表明 $x_1^*$ 是一个局部最小值。\n- 对于 $x_2^* \\approx 0.0753$：$\\nabla^2 f(x_2^*) = 12(0.0753)^2 - 4 \\approx -3.93  0$。这表明 $x_2^*$ 是一个局部最大值。在这个负曲率区域，牛顿法预计会失败。\n- 对于 $x_3^* \\approx 0.9612$：$\\nabla^2 f(x_3^*) = 12(0.9612)^2 - 4 \\approx 7.09  0$。这表明 $x_3^*$ 是另一个局部最小值。\n\n为了找到全局最小值，我们比较两个局部最小值点的函数值：\n- $f(x_1^*) \\approx f(-1.0365) = ((-1.0365)^2 - 1)^2 + 0.3(-1.0365) \\approx -0.30546$\n- $f(x_3^*) \\approx f(0.9612) = ((0.9612)^2 - 1)^2 + 0.3(0.9612) \\approx 0.29416$\n由于 $f(x_1^*)  f(x_3^*)$，点 $x_1^*$ 是全局最小值，全局最小值为 $f_{\\mathrm{glob}} \\approx -0.30546193$。如果任何算法运行的最终点的函数值与此值的差在 $10^{-8}$ 的容差内，则认为该算法找到了全局最小值。\n\n优化算法实现如下。两种方法都利用回溯线搜索来确保每一步都有充分的下降，满足 Armijo 条件 $f(x_{k+1}) \\leq f(x_k) + c\\,\\alpha\\,\\nabla f(x_k)\\,p_k$，其中 $c = 10^{-4}$，步长缩减因子 $\\rho = 0.5$。初始步长取为 $\\alpha=1.0$。\n\n**梯度下降法 (GD)：** 这种一阶方法使用负梯度作为其搜索方向，即 $p_k = -\\nabla f(x_k)$。只要 $\\nabla f(x_k) \\neq 0$，这就保证了下降方向，因为 $\\nabla f(x_k) p_k = -(\\nabla f(x_k))^2 \\le 0$。梯度下降法是稳健的，总能朝局部最小值取得进展，但其收敛速度可能很慢。\n\n**牛顿法 (NM)：** 这种二阶方法使用搜索方向 $p_k = -(\\nabla^2 f(x_k))^{-1} \\nabla f(x_k)$。它源于函数的二次模型。当 Hessian 矩阵 $\\nabla^2 f(x_k)$ 是正定时，牛顿方向是一个下降方向，该方法表现出快速的（二次）局部收敛性。然而，如果 Hessian 矩阵非正定（即，在此一维情况下 $\\nabla^2 f(x_k) \\leq 0$），该方向可能不是下降方向，方法可能被吸引到鞍点或最大值点。根据问题规定，如果在任何迭代 $k$ 中 $\\nabla^2 f(x_k) \\leq 0$，则声明该方法对于该初始条件失败，并终止该次运行。报告的迭代次数为 $k$。\n\n程序将从每个指定的初始点 ($x_0 \\in \\{-0.2, 0.8, -0.8\\}$) 执行这两种算法，跟踪它们的性能指标（收敛状态、迭代次数、最终点），并根据指定的输出格式报告结果。这包括将结果与预先计算的全局最小值进行比较，以及比较两种方法之间的迭代次数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by implementing and comparing\n    gradient descent and Newton's method on a given nonconvex function.\n    \"\"\"\n\n    # --- Problem Definition ---\n    C = 1e-4\n    RHO = 0.5\n    TOL = 1e-8\n    MAX_ITER = 100\n\n    def f(x):\n        return (x**2 - 1)**2 + 0.3 * x\n\n    def grad_f(x):\n        return 4 * x**3 - 4 * x + 0.3\n\n    def hess_f(x):\n        return 12 * x**2 - 4\n\n    # --- Stationary Points and Global Minimum Analysis ---\n    # Solve grad_f(x) = 4x^3 - 4x + 0.3 = 0\n    roots = np.roots([4, 0, -4, 0.3])\n    \n    # Classify roots and find local minima\n    local_minima_x = []\n    for r in roots:\n        # We are only interested in real roots for this 1D problem.\n        if np.isreal(r):\n            if hess_f(np.real(r)) > 0:\n                local_minima_x.append(np.real(r))\n\n    # Find global minimum value by comparing function values at local minima\n    f_values_at_minima = [f(x) for x in local_minima_x]\n    global_min_val = min(f_values_at_minima)\n\n    # --- Core Algorithm Implementations ---\n    def backtracking_line_search(x, p, f, grad_f):\n        alpha = 1.0\n        fx = f(x)\n        grad_fx_p = grad_f(x) * p\n        while f(x + alpha * p) > fx + C * alpha * grad_fx_p:\n            alpha *= RHO\n            if alpha  1e-16: # Prevent infinite loop\n                break\n        return alpha\n\n    def gradient_descent(x0):\n        x = float(x0)\n        for k in range(MAX_ITER):\n            grad = grad_f(x)\n            if abs(grad) = TOL:\n                return x, k, 'converged'\n            \n            p = -grad\n            alpha = backtracking_line_search(x, p, f, grad_f)\n            x = x + alpha * p\n        \n        # Final check after max iterations\n        status = 'converged' if abs(grad_f(x)) = TOL else 'max_iter_reached'\n        return x, MAX_ITER, status\n\n    def newton_method(x0):\n        x = float(x0)\n        for k in range(MAX_ITER):\n            # Check for failure condition (non-positive definite Hessian)\n            hess = hess_f(x)\n            if hess = 0:\n                return x, k, 'failed'\n            \n            grad = grad_f(x)\n            if abs(grad) = TOL:\n                return x, k, 'converged'\n            \n            p = -grad / hess\n            alpha = backtracking_line_search(x, p, f, grad_f)\n            x = x + alpha * p\n            \n        # Final check after max iterations\n        status = 'converged' if abs(grad_f(x)) = TOL else 'max_iter_reached'\n        return x, MAX_ITER, status\n\n    # --- Test Suite Execution ---\n    test_cases = [-0.2, 0.8, -0.8]\n    results = []\n\n    for x0 in test_cases:\n        # Run Gradient Descent\n        x_final_gd, iters_gd, status_gd = gradient_descent(x0)\n        \n        # Run Newton's Method\n        x_final_nm, iters_nm, status_nm = newton_method(x0)\n\n        # --- Evaluate Results ---\n        # 1. Did Newton reach the global minimum?\n        nm_reaches_glob_min = False\n        if status_nm == 'converged' or status_nm == 'max_iter_reached':\n             if abs(f(x_final_nm) - global_min_val) = TOL:\n                 nm_reaches_glob_min = True\n\n        # 2. Did Gradient Descent reach the global minimum?\n        gd_reaches_glob_min = False\n        if status_gd == 'converged' or status_gd == 'max_iter_reached':\n            if abs(f(x_final_gd) - global_min_val) = TOL:\n                gd_reaches_glob_min = True\n        \n        # 3. Did Newton use strictly fewer iterations?\n        nm_fewer_iters = False\n        if status_nm != 'failed':\n            nm_fewer_iters = iters_nm  iters_gd\n        \n        # 4. Number of iterations for Newton\n        iters_nm_out = iters_nm\n        \n        # 5. Number of iterations for Gradient Descent\n        iters_gd_out = iters_gd\n        \n        case_result = [\n            str(nm_reaches_glob_min),\n            str(gd_reaches_glob_min),\n            str(nm_fewer_iters),\n            str(iters_nm_out),\n            str(iters_gd_out)\n        ]\n        results.append(f\"[{','.join(case_result)}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3145146"}, {"introduction": "在了解了基础算法的收敛特性后，我们来探讨更具挑战性的情况。现实世界中的优化问题常常具有复杂的几何形状，例如狭窄的“山谷”。这个练习[@problem_id:3145074]将指导你构建一个特殊的函数，以展示梯度下降法在狭窄山谷中低效的“之”字形振荡行为。通过亲手实现并与一个利用曲率信息的改进方法进行对比，你将深刻体会到二阶信息对于提升优化效率的重要性。", "problem": "给定一项任务，要求构建并分析一个具体的、光滑的二维函数。当使用带Armijo回溯线搜索的梯度下降法在非凸曲面中的狭窄山谷中导航时，该函数会表现出锯齿形行为。重点在于识别局部和全局最优解，展示朴素的一阶方法如何在山谷壁之间循环（即第一坐标的符号反复交替），并提出能够缓解此行为的感知曲率的更新方法。推导和实现必须基于优化方法的核心定义。\n\n基本原理：\n- 如果存在一个半径$r0$，使得对于所有满足$\\lVert x-x^\\star\\rVert\\le r$的$x$，都有$f(x^\\star)\\le f(x)$，那么点$x^\\star$是可微函数$f:\\mathbb{R}^n\\to\\mathbb{R}$的一个局部极小值点。如果对于所有$x\\in\\mathbb{R}^n$，都有$f(x^\\star)\\le f(x)$，那么点$x^\\star$是一个全局极小值点。\n- 对于一个可微函数$f$，局部最优性的一阶必要条件是$\\nabla f(x^\\star)=0$。\n- 带Armijo回溯的梯度下降法选择方向$d_k=-\\nabla f(x_k)$和步长$t_k$，使得$f(x_k+t_k d_k)\\le f(x_k)+c\\,t_k\\,\\nabla f(x_k)^\\top d_k$，其中$c\\in(0,1)$是固定常数。标准的做法是通过回溯收缩因子$\\tau\\in(0,1)$来减小步长$t_k\\leftarrow \\tau t_k$，直到满足Armijo条件。\n- Hessian矩阵$H(x)=\\nabla^2 f(x)$捕捉了局部曲率。感知曲率的更新方法通过$H(x)^{-1}$的近似来对梯度进行预处理，通常会加入正则化以确保正定性。\n\n问题设定：\n1. 考虑由下式定义的光滑函数$f:\\mathbb{R}^2\\to\\mathbb{R}$\n$$\nf(x,y)=\\left(x^2-1\\right)^2+\\alpha\\,y^2+\\beta\\,\\sin(3y)\\,x+\\gamma\\,\\sin^2(3y),\n$$\n其中$\\alpha0$，$\\beta\\in\\mathbb{R}$，$\\gamma\\ge 0$是固定参数。该函数在$x$方向上具有四次截面，形成一个在$x=\\pm 1$附近有名义上壁的山谷；在$y$方向上是一个浅二次型；以及一个在$y$方向上的正弦耦合项，它调制山谷并能在$y$演变时产生交替的侧向力，将$x$推向相对的壁。\n\n2. 仅使用微分法则，从上述定义中推导出梯度$\\nabla f(x,y)$和Hessian矩阵$H(x,y)$：\n- 梯度分量为\n$$\n\\frac{\\partial f}{\\partial x}(x,y)=4x\\left(x^2-1\\right)+\\beta\\,\\sin(3y),\\quad\n\\frac{\\partial f}{\\partial y}(x,y)=2\\alpha\\,y+3\\beta\\,\\cos(3y)\\,x+6\\gamma\\,\\sin(3y)\\cos(3y).\n$$\n- Hessian矩阵的元素为\n$$\n\\frac{\\partial^2 f}{\\partial x^2}(x,y)=12x^2-4,\\quad\n\\frac{\\partial^2 f}{\\partial y^2}(x,y)=2\\alpha-9\\beta\\,\\sin(3y)\\,x+18\\gamma\\,\\cos(6y),\n$$\n$$\n\\frac{\\partial^2 f}{\\partial x\\partial y}(x,y)=\\frac{\\partial^2 f}{\\partial y\\partial x}(x,y)=3\\beta\\,\\cos(3y).\n$$\n\n3. 仅使用上述定义实现两种下降方法：\n- 方法A（一阶）：带Armijo回溯的梯度下降法，使用方向$d_k=-\\nabla f(x_k,y_k)$，Armijo参数$c$固定为$10^{-4}$，回溯因子$\\tau$固定为$0.5$，初始步长$t_0=1$。\n- 方法B（感知曲率）：使用阻尼牛顿预处理步，方向为$d_k=-\\left(H(x_k,y_k)+\\lambda_k I\\right)^{-1}\\nabla f(x_k,y_k)$，其中$\\lambda_k$选择为$\\lambda_k=\\max\\left\\{0,-\\lambda_{\\min}(H(x_k,y_k))+\\varepsilon\\right\\}$，$\\lambda_{\\min}(H)$是$H$的最小特征值，$I$是单位矩阵，$\\varepsilon$是一个小的正常数（取$\\varepsilon=10^{-3}$）。对该方向使用与之前相同的Armijo参数进行回溯。\n\n4. 检测锯齿形循环：定义一次“壁翻转”为连续两次迭代中$x$坐标的符号发生变化，即$\\operatorname{sign}(x_{k+1})\\neq \\operatorname{sign}(x_k)$，且$x_{k+1}$和$x_k$均不为零。如果壁翻转次数至少为$K$（其中$K=8$），则声明出现“循环”。在固定的迭代预算内，为每种方法计算壁翻转次数。\n\n5. 收敛性与最优解：在测试所用的参数范围内，确定$f$的全局极小值点。报告最终迭代点是否接近一个全局极小值点，该判断需同时满足两个标准：与候选全局极小值点$(1,0)$或$(-1,0)$中较近者的距离小于距离阈值$\\delta=10^{-2}$，并且函数值小于绝对阈值$\\eta=10^{-6}$。\n\n6. 测试套件和输出规范：\n使用以下测试用例，每个用例由元组$(\\alpha,\\beta,\\gamma,x_0,y_0,\\text{max\\_iters})$指定，所有数字均明确写出：\n- 用例1：$(0.005,4.0,0.1,0.0,3.0,400)$，预期方法A会显示显著的锯齿形行为，而方法B的翻转次数较少。\n- 用例2：$(0.005,4.0,0.1,0.0,0.2,200)$，从靠近谷底处开始，预期锯齿形行为最少。\n- 用例3：$(0.08,1.0,0.05,0.0,2.4,400)$，耦合较弱，预期锯齿形行为减少。\n- 用例4：$(0.5,4.0,0.1,0.0,3.0,400)$，$y$方向曲率更陡峭，预期能抑制振荡。\n\n对于每个用例，从相同的初始点运行方法A和方法B，并按顺序记录以下六个值：\n- 方法A的壁翻转次数（整数）。\n- 方法B的壁翻转次数（整数）。\n- 方法A的最终函数值（浮点数）。\n- 方法B的最终函数值（浮点数）。\n- 一个布尔值，指示方法A是否根据上述标准位于全局最小值邻域内。\n- 一个布尔值，指示方法B是否根据上述标准位于全局最小值邻域内。\n\n最终输出格式：\n你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素对应一个测试用例，并且本身是按上述顺序排列的六个值的列表。例如，输出应如下所示：\n$[[\\text{case1\\_vals}],[\\text{case2\\_vals}],[\\text{case3\\_vals}],[\\text{case4\\_vals}]]$.\n不涉及物理单位，也不需要角度或百分比。所有计算必须在$\\mathbb{R}^2$中使用实数算术进行。", "solution": "该问题要求构建并分析一个光滑的二维函数，该函数被设计用于在一阶和感知曲率的优化方法下表现出特定的行为。目标是实现并比较两种算法——带Armijo回溯的梯度下降法和一种阻尼牛顿预处理方法——并量化它们在穿越狭窄非凸山谷时的性能。此分析涉及跟踪一个“锯齿形”度量，评估目标函数，并检查是否收敛到预定义的目标区域。该问题定义明确，数学上合理，并为完整解决方案提供了所有必要的参数和规范。\n\n所考虑的函数是$f:\\mathbb{R}^2\\to\\mathbb{R}$，定义为：\n$$\nf(x,y)=\\left(x^2-1\\right)^2+\\alpha\\,y^2+\\beta\\,\\sin(3y)\\,x+\\gamma\\,\\sin^2(3y)\n$$\n参数为$\\alpha  0$，$\\beta \\in \\mathbb{R}$，以及$\\gamma \\ge 0$。该函数被设计成具有类似山谷的结构。主导项$\\left(x^2-1\\right)^2$创建了一个四次势，其极小值点在$x=\\pm 1$处，形成了沿y轴对齐的山谷的壁。项$\\alpha y^2$为谷底引入了一个简单的二次斜率。项$\\beta \\sin(3y)x$提供了两个坐标之间的关键耦合，引入了一个依赖于位置$y$的$x$方向上的振荡力。最后，项$\\gamma \\sin^2(3y)$为谷底增加了进一步的波动。对于$\\beta \\ne 0$的情况，真正的全局极小值点并不位于$(\\pm 1, 0)$，因为在这些点上梯度不为零。然而，这些点作为主山谷结构底部的参考位置，并且问题基于它们定义了一个特定的收敛标准。\n\n分析基于两种优化算法：\n\n**方法A：带Armijo回溯的梯度下降法**\n\n这是一种一阶方法，其中每次迭代$k$的搜索方向是负梯度，$d_k = -\\nabla f(x_k, y_k)$。梯度由下式给出：\n$$\n\\nabla f(x,y) = \\begin{pmatrix} 4x(x^2-1) + \\beta\\sin(3y) \\\\ 2\\alpha y + 3\\beta x\\cos(3y) + 3\\gamma\\sin(6y) \\end{pmatrix}\n$$\n步长$t_k$由Armijo回溯线搜索确定，从初始猜测$t_{k,0}=1$开始，并以因子$\\tau=0.5$递减，直到满足条件$f(x_k+t_k d_k) \\le f(x_k) + c\\,t_k\\,\\nabla f(x_k)^\\top d_k$，其中使用常数$c=10^{-4}$。在水平集是细长且弯曲的狭窄山谷中，梯度向量通常几乎与谷底方向正交。因此，梯度下降法倾向于采取大的步长横跨山谷，在其壁之间反弹，而沿山谷方向的进展缓慢。这表现为“锯齿形”行为，通过计算“壁翻转”来量化，定义为连续迭代点$x_k$和$x_{k+1}$具有相反的符号。\n\n**方法B：阻尼牛顿预处理下降法**\n\n这是一种感知曲率的方法，它使用来自Hessian矩阵$H(x,y) = \\nabla^2 f(x,y)$的信息来找到更有效的搜索方向。Hessian矩阵由下式给出：\n$$\nH(x,y) = \\begin{pmatrix}\n12x^2-4  3\\beta\\cos(3y) \\\\\n3\\beta\\cos(3y)  2\\alpha - 9\\beta x\\sin(3y) + 18\\gamma\\cos(6y)\n\\end{pmatrix}\n$$\n搜索方向计算为$d_k = - (H(x_k,y_k) + \\lambda_k I)^{-1} \\nabla f(x_k,y_k)$，其中$I$是$2 \\times 2$的单位矩阵。项$\\lambda_k$是一个阻尼参数，用于确保修正后的Hessian矩阵$H(x_k,y_k) + \\lambda_k I$是正定的，这保证了$d_k$是一个下降方向。它被选择为$\\lambda_k = \\max\\{0, -\\lambda_{\\min}(H(x_k,y_k)) + \\varepsilon\\}$，其中$\\lambda_{\\min}(H)$是Hessian矩阵的最小特征值，$\\varepsilon=10^{-3}$是一个小的正常数。通过整合曲率信息，该方法有效地重新调整了问题的几何结构，使搜索方向能够更直接地指向沿谷底的极小值点，从而减轻锯齿形效应。步长使用与方法A中相同的Armijo回溯过程来确定。\n\n该实现将为四个不同的测试用例模拟这两种方法，每个用例由一组参数$(\\alpha, \\beta, \\gamma)$和初始点$(x_0, y_0)$定义。对于每次运行，我们将记录总的壁翻转次数。在指定的迭代次数结束时，我们将评估最终函数值并检查收敛性。收敛性由同时满足两个条件定义：最终迭代点到点$(1,0)$和$(-1,0)$中较近者的欧几里得距离必须小于$\\delta=10^{-2}$，并且最终函数值必须小于$\\eta=10^{-6}$。最终输出将是一个结构化列表，其中包含每个测试用例的这六个度量。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases.\n    Implements and compares gradient descent and a damped Newton method\n    on a specified non-convex function.\n    \"\"\"\n\n    test_cases = [\n        # (alpha, beta, gamma, x0, y0, max_iters)\n        (0.005, 4.0, 0.1, 0.0, 3.0, 400),\n        (0.005, 4.0, 0.1, 0.0, 0.2, 200),\n        (0.08, 1.0, 0.05, 0.0, 2.4, 400),\n        (0.5, 4.0, 0.1, 0.0, 3.0, 400),\n    ]\n\n    # Armijo and convergence parameters\n    C_ARMIJO = 1e-4\n    TAU = 0.5\n    EPSILON = 1e-3\n    DIST_THRESHOLD = 1e-2\n    FUNC_THRESHOLD = 1e-6\n    MINIMIZER_CANDIDATES = [np.array([1.0, 0.0]), np.array([-1.0, 0.0])]\n\n    def f(p, alpha, beta, gamma):\n        x, y = p\n        return (x**2 - 1)**2 + alpha * y**2 + beta * np.sin(3*y) * x + gamma * np.sin(3*y)**2\n\n    def grad_f(p, alpha, beta, gamma):\n        x, y = p\n        df_dx = 4 * x * (x**2 - 1) + beta * np.sin(3*y)\n        df_dy = 2 * alpha * y + 3 * beta * np.cos(3*y) * x + 6 * gamma * np.sin(3*y) * np.cos(3*y)\n        return np.array([df_dx, df_dy])\n\n    def hess_f(p, alpha, beta, gamma):\n        x, y = p\n        d2f_dx2 = 12 * x**2 - 4\n        d2f_dy2 = 2 * alpha - 9 * beta * np.sin(3*y) * x + 18 * gamma * np.cos(6*y)\n        d2f_dxdy = 3 * beta * np.cos(3*y)\n        return np.array([[d2f_dx2, d2f_dxdy], [d2f_dxdy, d2f_dy2]])\n\n    def armijo_backtracking(p, d, grad_p, f_p, alpha, beta, gamma):\n        t = 1.0\n        while True:\n            p_new = p + t * d\n            f_new = f(p_new, alpha, beta, gamma)\n            if f_new = f_p + C_ARMIJO * t * np.dot(grad_p, d):\n                return t\n            t = t * TAU\n            if t  1e-16: # Prevent infinite loop\n                return t\n\n\n    def run_method_a(alpha, beta, gamma, x0, y0, max_iters):\n        p = np.array([x0, y0])\n        flips = 0\n        for k in range(max_iters):\n            grad_p = grad_f(p, alpha, beta, gamma)\n            d = -grad_p\n            f_p = f(p, alpha, beta, gamma)\n            \n            t = armijo_backtracking(p, d, grad_p, f_p, alpha, beta, gamma)\n            \n            p_prev = p\n            p = p + t * d\n\n            if np.sign(p_prev[0]) != np.sign(p[0]) and p_prev[0] != 0 and p[0] != 0:\n                flips += 1\n        \n        return p, flips\n\n    def run_method_b(alpha, beta, gamma, x0, y0, max_iters):\n        p = np.array([x0, y0])\n        flips = 0\n        for k in range(max_iters):\n            grad_p = grad_f(p, alpha, beta, gamma)\n            hess_p = hess_f(p, alpha, beta, gamma)\n\n            try:\n                min_eigval = np.linalg.eigvalsh(hess_p).min()\n            except np.linalg.LinAlgError:\n                min_eigval = -1.0 # If something fails, assume non-PD\n\n            lambda_k = max(0, -min_eigval + EPSILON)\n            \n            H_reg = hess_p + lambda_k * np.eye(2)\n            \n            try:\n                d = np.linalg.solve(H_reg, -grad_p)\n            except np.linalg.LinAlgError:\n                # Fallback to gradient descent if regularized Hessian is singular\n                d = -grad_p\n            \n            f_p = f(p, alpha, beta, gamma)\n            t = armijo_backtracking(p, d, grad_p, f_p, alpha, beta, gamma)\n            \n            p_prev = p\n            p = p + t * d\n\n            if np.sign(p_prev[0]) != np.sign(p[0]) and p_prev[0] != 0 and p[0] != 0:\n                flips += 1\n        \n        return p, flips\n\n    results = []\n    for case in test_cases:\n        alpha, beta, gamma, x0, y0, max_iters = case\n        \n        # Method A\n        p_final_a, flips_a = run_method_a(alpha, beta, gamma, x0, y0, max_iters)\n        f_val_a = f(p_final_a, alpha, beta, gamma)\n        dist_a = min(np.linalg.norm(p_final_a - m) for m in MINIMIZER_CANDIDATES)\n        converged_a = dist_a  DIST_THRESHOLD and f_val_a  FUNC_THRESHOLD\n        \n        # Method B\n        p_final_b, flips_b = run_method_b(alpha, beta, gamma, x0, y0, max_iters)\n        f_val_b = f(p_final_b, alpha, beta, gamma)\n        dist_b = min(np.linalg.norm(p_final_b - m) for m in MINIMIZER_CANDIDATES)\n        converged_b = dist_b  DIST_THRESHOLD and f_val_b  FUNC_THRESHOLD\n        \n        case_results = [\n            flips_a, flips_b, f_val_a, f_val_b, converged_a, converged_b\n        ]\n        results.append(case_results)\n\n    # Format output as specified\n    results_str = [\n        f\"[{r[0]},{r[1]},{r[2]},{r[3]},{str(r[4]).lower()},{str(r[5]).lower()}]\"\n        for r in results\n    ]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3145074"}, {"introduction": "我们已经探讨了如何改进单次优化运行的效率，但最终目标往往是找到全局最优解。对于拥有多个极小值的复杂函数，单次运行局部优化算法很可能无法找到全局最优。这个练习[@problem_id:3145095]介绍了一种常见且直观的全局优化策略——“多起点”法。通过将该策略应用于著名的Himmelblau函数，你将通过实验探索“吸引盆”的概念，并学习如何量化评估找到全局最优解的概率。", "problem": "考虑由 Himmelblau 函数定义的函数 $f:\\mathbb{R}^2\\to\\mathbb{R}$：$f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$。您将设计并分析一个多起点局部优化方案，以根据随机初始化凭经验量化达到全局最小值的概率。\n\n基本基础和定义：\n- 如果存在点 $(x^\\star,y^\\star)$ 的一个邻域 $\\mathcal{N}$，使得对于所有 $(x,y)\\in\\mathcal{N}$ 都有 $f(x^\\star,y^\\star)\\le f(x,y)$，则该点是一个局部极小值点。\n- 如果对于所有 $(x,y)\\in\\mathbb{R}^2$ 都有 $f(x^\\star,y^\\star)\\le f(x,y)$，则点 $(x^\\star,y^\\star)$ 是一个全局极小值点。\n- 因为 $f(x,y)$ 是平方和，它对所有 $(x,y)\\in\\mathbb{R}^2$ 都满足 $f(x,y)\\ge 0$，并且任何使两个平方项同时为零的点都会得到值为 $0$ 的全局最小值。\n\n任务：\n- 实现一个执行以下步骤的多起点方案：\n  1. 从给定的轴对齐矩形 $[a_x,b_x]\\times[a_y,b_y]$ 上的均匀分布中抽取 $N$ 个独立的起始点 $(x_0,y_0)$。\n  2. 从每个起点 $(x_0,y_0)$ 开始，使用从微分的乘积法则和链式法则推导出的 $f(x,y)$ 的解析梯度，运行一个基于一阶微积分的确定性局部求解器（例如，Broyden–Fletcher–Goldfarb–Shanno (BFGS) 方法）。\n  3. 对于一个数值容差 $\\varepsilon_f0$，如果局部求解器产生的终点 $(\\hat x,\\hat y)$ 满足 $f(\\hat x,\\hat y)\\le \\varepsilon_f$，则声明为“成功”。否则，声明为“失败”。\n  4. 通过蒙特卡洛估计量 $\\hat p = \\frac{1}{N}\\sum_{i=1}^{N} I_i$ 来估计在指定的随机初始化下达到全局最小值的概率 $p$，其中 $I_i$ 是第 $i$ 次运行成功的指示变量。\n- 在同一测试用例的所有运行中使用相同的求解器超参数，包括最大迭代次数。\n\n不涉及角度单位。不涉及物理单位。所有概率必须以 $[0,1]$ 范围内的十进制数报告。\n\n测试套件（所有随机抽取必须是均匀的，并可通过给定的种子进行复现）：\n- 案例 $1$（理想路径，宽域，足够迭代次数）：$N=200$，$\\text{seed}=7$，$[a_x,b_x]=[-6,6]$，$[a_y,b_y]=[-6,6]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=200$。\n- 案例 $2$（小样本边界）：$N=10$，$\\text{seed}=11$，$[a_x,b_x]=[-6,6]$，$[a_y,b_y]=[-6,6]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=200$。\n- 案例 $3$（域集中于单个盆地）：$N=100$，$\\text{seed}=23$，$[a_x,b_x]=[2.5,3.5]$，$[a_y,b_y]=[1.5,2.5]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=50$。\n- 案例 $4$（迭代次数极少的压力测试）：$N=200$，$\\text{seed}=31$，$[a_x,b_x]=[-6,6]$，$[a_y,b_y]=[-6,6]$，$\\varepsilon_f=10^{-6}$，最大迭代次数 $=5$。\n\n要求输出：\n- 您的程序必须使用 $f(x,y)$ 的解析梯度、Broyden–Fletcher–Goldfarb–Shanno 方法以及为每个案例指定的参数来实现上述方案。\n- 对于每个案例，将估计概率 $\\hat p$ 作为浮点数输出。最终输出必须是单行，包含按案例 1 到 4 顺序排列的四个概率列表，形式为用方括号括起来的逗号分隔列表（例如，$[\\hat p_1,\\hat p_2,\\hat p_3,\\hat p_4]$）。这些值可以以任何合理的固定精度打印。", "solution": "该问题要求实现并分析一个多起点优化方案，以估计找到 Himmelblau 函数 $f(x,y)$ 全局最小值的概率。对问题陈述的验证确认了其科学性、适定性、客观性，并包含了计算求解所需的所有必要信息。该问题是有效的。\n\n待最小化的函数是 Himmelblau 函数，定义为 $f:\\mathbb{R}^2 \\to \\mathbb{R}$：\n$$f(x,y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2$$\n该函数是两个平方项的和。其值始终为非负，即对所有 $(x,y) \\in \\mathbb{R}^2$ 都有 $f(x,y) \\ge 0$。$f(x,y)$ 的全局最小值是使得 $f(x^\\star, y^\\star) = 0$ 的点 $(x^\\star, y^\\star)$。这当且仅当两个平方项同时为零时发生：\n\\begin{align*}\nx^2 + y - 11 = 0 \\\\\nx + y^2 - 7 = 0\n\\end{align*}\n求解这个非线性方程组可得到四个不同的解，它们是该函数的四个全局极小值点：\n\\begin{itemize}\n    \\item $(3, 2)$\n    \\item $(-2.805118..., 3.131312...)$\n    \\item $(-3.779310..., -3.283186...)$\n    \\item $(3.584428..., -1.848126...)$\n\\end{itemize}\n该多起点方案采用一种局部优化算法，特别是 Broyden–Fletcher–Goldfarb–Shanno (BFGS) 方法。BFGS 是一种拟牛顿法，它需要目标函数的梯度来迭代搜索局部最小值。该问题强制要求使用解析梯度，我们将在下面推导它。\n\n设 $u(x,y) = x^2 + y - 11$ 且 $v(x,y) = x + y^2 - 7$，因此 $f = u^2 + v^2$。$f$ 的梯度，记为 $\\nabla f(x,y)$，是其偏导数向量 $(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y})$。使用微分的链式法则和求和法则：\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x}(u^2 + v^2) = 2u \\frac{\\partial u}{\\partial x} + 2v \\frac{\\partial v}{\\partial x} $$\n$$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y}(u^2 + v^2) = 2u \\frac{\\partial u}{\\partial y} + 2v \\frac{\\partial v}{\\partial y} $$\n$u(x,y)$ 和 $v(x,y)$ 的偏导数是：\n\\begin{align*}\n\\frac{\\partial u}{\\partial x} = 2x, \\quad \\frac{\\partial u}{\\partial y} = 1 \\\\\n\\frac{\\partial v}{\\partial x} = 1, \\quad \\frac{\\partial v}{\\partial y} = 2y\n\\end{align*}\n将这些代入 $f(x,y)$ 偏导数的表达式中：\n\\begin{align*}\n\\frac{\\partial f}{\\partial x} = 2(x^2 + y - 11)(2x) + 2(x + y^2 - 7)(1) \\\\\n= 4x(x^2 + y - 11) + 2(x + y^2 - 7) \\\\\n\\frac{\\partial f}{\\partial y} = 2(x^2 + y - 11)(1) + 2(x + y^2 - 7)(2y) \\\\\n= 2(x^2 + y - 11) + 4y(x + y^2 - 7)\n\\end{align*}\n因此，解析梯度向量为：\n$$ \\nabla f(x,y) = \\begin{pmatrix} 4x(x^2 + y - 11) + 2(x + y^2 - 7) \\\\ 2(x^2 + y - 11) + 4y(x + y^2 - 7) \\end{pmatrix} $$\n该梯度函数将提供给 BFGS 求解器以实现高效收敛。\n\n任务的核心是实现一个蒙特卡洛模拟。每个测试用例的步骤如下：\n$1$. 为保证可复现性，设置随机数生成器种子。指定运行参数：试验次数 $N$、采样域 $[a_x,b_x]\\times[a_y,b_y]$、成功容差 $\\varepsilon_f$ 以及求解器的最大迭代次数。\n$2$. 初始化成功计数器 $S$ 为 $0$。\n$3$. 对于 $i$ 从 $1$ 到 $N$：\n    a. 从指定的矩形域上的均匀分布中抽取一个随机起始点 $(x_0, y_0)$，即 $x_0 \\sim U(a_x, b_x)$ 且 $y_0 \\sim U(a_y, b_y)$。\n    b. 从 $(x_0, y_0)$ 开始，使用解析推导出的梯度 $\\nabla f$ 运行 BFGS 算法。求解器在收敛或达到最大迭代次数后终止。\n    c. 设求解器找到的终点为 $(\\hat{x}, \\hat{y})$。计算函数值 $f(\\hat{x}, \\hat{y})$。\n    d. 如果 $f(\\hat{x}, \\hat{y}) \\le \\varepsilon_f$，则该次运行被视为“成功”，计数器 $S$ 增加 $1$。容差 $\\varepsilon_f = 10^{-6}$ 足够小，可以确保一次成功的运行已经终止在四个全局最小值（其中 $f=0$）之一的极近位置。\n$4$. 在所有 $N$ 次试验完成后，成功概率 $p$ 通过样本均值 $\\hat{p} = S/N$ 来估计。\n\n对提供的四个测试用例中的每一个都执行此过程，从而产生四个概率估计值：$\\hat{p}_1, \\hat{p}_2, \\hat{p}_3, \\hat{p}_4$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements a multistart optimization scheme to estimate the probability of\n    finding a global minimum of the Himmelblau function for four test cases.\n    \"\"\"\n\n    # Define the Himmelblau function f(p) where p = [x, y]\n    def himmelblau_func(p):\n        x, y = p\n        term1 = x**2 + y - 11\n        term2 = x + y**2 - 7\n        return term1**2 + term2**2\n\n    # Define the analytic gradient of the Himmelblau function\n    def himmelblau_grad(p):\n        x, y = p\n        # Partial derivative with respect to x\n        df_dx = 4 * x * (x**2 + y - 11) + 2 * (x + y**2 - 7)\n        # Partial derivative with respect to y\n        df_dy = 2 * (x**2 + y - 11) + 4 * y * (x + y**2 - 7)\n        return np.array([df_dx, df_dy])\n\n    # Test suite parameters\n    test_cases = [\n        # Case 1: N=200, seed=7, [-6,6]x[-6,6], eps=1e-6, max_iter=200\n        {'N': 200, 'seed': 7, 'domain_x': [-6, 6], 'domain_y': [-6, 6], 'eps_f': 1e-6, 'max_iter': 200},\n        # Case 2: N=10, seed=11, [-6,6]x[-6,6], eps=1e-6, max_iter=200\n        {'N': 10, 'seed': 11, 'domain_x': [-6, 6], 'domain_y': [-6, 6], 'eps_f': 1e-6, 'max_iter': 200},\n        # Case 3: N=100, seed=23, [2.5,3.5]x[1.5,2.5], eps=1e-6, max_iter=50\n        {'N': 100, 'seed': 23, 'domain_x': [2.5, 3.5], 'domain_y': [1.5, 2.5], 'eps_f': 1e-6, 'max_iter': 50},\n        # Case 4: N=200, seed=31, [-6,6]x[-6,6], eps=1e-6, max_iter=5\n        {'N': 200, 'seed': 31, 'domain_x': [-6, 6], 'domain_y': [-6, 6], 'eps_f': 1e-6, 'max_iter': 5},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case['N']\n        seed = case['seed']\n        ax, bx = case['domain_x']\n        ay, by = case['domain_y']\n        eps_f = case['eps_f']\n        max_iter = case['max_iter']\n\n        # Set up a random number generator with the specified seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        success_count = 0\n        for _ in range(N):\n            # 1. Draw a random starting point from the uniform distribution\n            x0 = rng.uniform(ax, bx)\n            y0 = rng.uniform(ay, by)\n            initial_guess = np.array([x0, y0])\n\n            # 2. Run the BFGS local solver\n            res = minimize(\n                himmelblau_func,\n                initial_guess,\n                method='BFGS',\n                jac=himmelblau_grad,\n                options={'maxiter': max_iter, 'gtol': 1e-7} # Add gtol for robustness\n            )\n\n            # 3. Check for success\n            if res.fun = eps_f:\n                success_count += 1\n        \n        # 4. Estimate the probability\n        p_hat = success_count / N\n        results.append(p_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3145095"}]}