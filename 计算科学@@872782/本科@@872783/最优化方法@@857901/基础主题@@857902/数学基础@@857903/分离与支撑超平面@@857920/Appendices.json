{"hands_on_practices": [{"introduction": "构建分离超平面最基本的方法是利用投影的几何直观。本练习将引导你从凸优化的第一性原理出发，推导出这种构造方法，从而巩固最近点问题与分离超平面存在性之间的联系。[@problem_id:3179785]", "problem": "设 $C \\subset \\mathbb{R}^{3}$ 是一个非空闭凸集，且 $x_{0} \\in \\mathbb{R}^{3}$ 是一个满足 $x_{0} \\notin C$ 的点。记 $p = \\operatorname{proj}_{C}(x_{0})$ 为 $x_{0}$ 到 $C$ 上的欧几里得投影，其定义为 $C$ 中唯一一个使与 $x_{0}$ 的欧几里得距离平方最小化的点。从凸优化的基本原理和投影的定义出发，使用向量 $x_{0} - p$ 推导出一个在点 $p$ 处支撑 $C$ 且将 $x_{0}$ 与 $C$ 严格分离的超平面。你的推导必须从投影作为凸最小化问题解的定义开始，并通过建立刻画 $p$ 的必要最优性条件来进行。\n\n然后，对特定的闭凸集\n$$\nC = \\{ x \\in \\mathbb{R}^{3} : 0 \\leq x_{1} \\leq 1,\\; 0 \\leq x_{2} \\leq 1,\\; 0 \\leq x_{3} \\leq 1 \\}\n$$\n和点\n$$\nx_{0} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}.\n$$\n实例化你的构造。\n计算投影 $p = \\operatorname{proj}_{C}(x_{0})$，并根据你的推导，计算形式为 $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x = b \\}$ 的分离超平面的法向量 $w \\in \\mathbb{R}^{3}$ 和偏移量 $b \\in \\mathbb{R}$。\n\n使用 LaTeX 的 $\\mathrm{pmatrix}$ 环境，将你的最终答案表示为行矩阵 $(w_{1}\\; w_{2}\\; w_{3}\\; b)$。无需四舍五入。", "solution": "该问题是有效的。这是一个凸分析中提法恰当的问题，具有科学依据且陈述客观。它要求推导一个关于凸集投影的标准结果，并将其应用于一个具体案例。所有必要信息都已提供。\n\n**第一部分：一般性推导**\n\n设 $C \\subset \\mathbb{R}^{3}$ 是一个非空闭凸集，且 $x_{0} \\in \\mathbb{R}^{3}$ 是一个满足 $x_{0} \\notin C$ 的点。$x_{0}$ 到 $C$ 上的欧几里得投影，记为 $p = \\operatorname{proj}_{C}(x_{0})$，定义为 $C$ 中唯一一个使与 $x_{0}$ 的欧几里得距离平方最小化的点。这可以表述为一个凸优化问题：\n$$\np = \\arg\\min_{z \\in C} f(z)\n$$\n其中目标函数为 $f(z) = \\frac{1}{2} \\|z - x_{0}\\|^{2}$。函数 $f(z)$ 是严格凸的，约束集 $C$ 是凸的。因此，存在唯一的最小化点 $p$。\n\n对于一个点 $p \\in C$，其成为可微凸函数 $f(z)$ 在凸集 $C$ 上的最小化点的充分必要一阶条件由变分不等式给出：\n$$\n\\nabla f(p)^{\\top} (z - p) \\geq 0, \\quad \\forall z \\in C\n$$\n该条件表明，在最优点 $p$ 处的负梯度 $-\\nabla f(p)$ 定义了可行集 $C$ 在点 $p$ 处的一个支撑超平面。\n\n对于我们的具体目标函数 $f(z) = \\frac{1}{2} \\|z - x_{0}\\|^{2} = \\frac{1}{2} (z - x_{0})^{\\top}(z - x_{0})$，其关于 $z$ 的梯度为：\n$$\n\\nabla f(z) = z - x_{0}\n$$\n在最优点 $p$ 处计算梯度，我们得到 $\\nabla f(p) = p - x_{0}$。将此代入最优性条件，得到：\n$$\n(p - x_{0})^{\\top} (z - p) \\geq 0, \\quad \\forall z \\in C\n$$\n这个不等式是投影 $p$ 的基本特征。我们可以通过乘以 $-1$ 来重新整理它：\n$$\n(x_{0} - p)^{\\top} (z - p) \\leq 0\n$$\n$$\n(x_{0} - p)^{\\top} z - (x_{0} - p)^{\\top} p \\leq 0\n$$\n$$\n(x_{0} - p)^{\\top} z \\leq (x_{0} - p)^{\\top} p, \\quad \\forall z \\in C\n$$\n现在，我们使用向量 $w = x_{0} - p$ 定义一个超平面。由于 $x_{0} \\notin C$ 且 $p \\in C$，我们有 $x_{0} \\neq p$，所以 $w \\neq 0$。设超平面为 $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x = b \\}$。我们将偏移量 $b$ 设为 $b = w^{\\top} p$。\n根据这些定义，不等式变为：\n$$\nw^{\\top} z \\leq b, \\quad \\forall z \\in C\n$$\n这表明整个集合 $C$ 都包含在闭半空间 $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x \\leq b \\}$ 中。根据定义，$w^{\\top} p = b$，所以点 $p$ 位于超平面上。由于 $p \\in C$，这证实了超平面 $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x = b \\}$ 在点 $p$ 处支撑集合 $C$。\n\n接下来，我们必须证明这个超平面将 $x_{0}$ 与 $C$ 严格分离。我们已经证明了对于所有 $z \\in C$，都有 $w^\\top z \\leq b$。我们需要证明 $w^\\top x_0  b$。让我们计算 $w^{\\top} x_{0}$：\n$$\nw^{\\top} x_{0} = (x_{0} - p)^{\\top} x_{0}\n$$\n这个值与 $b$ 的差为：\n$$\nw^{\\top} x_{0} - b = (x_{0} - p)^{\\top} x_{0} - (x_{0} - p)^{\\top} p = (x_{0} - p)^{\\top} (x_{0} - p) = \\|x_{0} - p\\|^{2}\n$$\n由于 $x_{0} \\notin C$，我们知道 $x_{0} \\neq p$，因此欧几里得距离的平方 $\\|x_{0} - p\\|^{2}$ 是严格为正的。因此：\n$$\nw^{\\top} x_{0} - b  0 \\implies w^{\\top} x_{0}  b\n$$\n这表明 $x_{0}$ 位于开半空间 $\\{ x \\in \\mathbb{R}^{3} : w^{\\top} x  b \\}$ 中，该半空间与集合 $C$ 严格分离。\n\n**第二部分：具体实例**\n\n我们给定的闭凸集\n$$\nC = \\{ x \\in \\mathbb{R}^{3} : 0 \\leq x_{1} \\leq 1,\\; 0 \\leq x_{2} \\leq 1,\\; 0 \\leq x_{3} \\leq 1 \\} = [0, 1]^{3}\n$$\n是 $\\mathbb{R}^{3}$ 中的单位立方体。给定的点为\n$$\nx_{0} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}.\n$$\n首先，我们计算投影 $p = \\operatorname{proj}_{C}(x_{0})$。对于像 $C$ 这样的箱型集合，投影是可分的，可以按分量计算。对于每个分量 $i \\in \\{1, 2, 3\\}$，$(x_{0})_{i}$ 到区间 $[0, 1]$ 上的投影由下式给出：\n$$\np_{i} = \\operatorname{proj}_{[0,1]}((x_{0})_{i}) = \\max(0, \\min(1, (x_{0})_{i}))\n$$\n对于 $i=1$：$p_{1} = \\operatorname{proj}_{[0,1]}(2) = 1$。\n对于 $i=2$：$p_{2} = \\operatorname{proj}_{[0,1]}(-1) = 0$。\n对于 $i=3$：$p_{3} = \\operatorname{proj}_{[0,1]}(3) = 1$。\n因此，投影点为：\n$$\np = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n接下来，我们使用在一般性部分推导出的公式 $w = x_{0} - p$ 来计算分离超平面的法向量 $w$。\n$$\nw = x_{0} - p = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n$$\n所以，法向量的分量为 $w_{1} = 1$，$w_{2} = -1$，$w_{3} = 2$。\n\n最后，我们使用公式 $b = w^{\\top} p$ 计算超平面的偏移量 $b$。\n$$\nb = w^{\\top} p = \\begin{pmatrix} 1  -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = (1)(1) + (-1)(0) + (2)(1) = 1 + 0 + 2 = 3\n$$\n因此，该超平面由方程 $w^{\\top} x = b$ 描述，即 $x_{1} - x_{2} + 2x_{3} = 3$。\n\n最终答案由 $w$ 的分量和 $b$ 的值组成。\n$w_{1} = 1$，$w_{2} = -1$，$w_{3} = 2$，$b = 3$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  -1  2  3\n\\end{pmatrix}\n}\n$$", "id": "3179785"}, {"introduction": "在光滑边界之外，我们来探索更复杂也更常见的情形：不可微点，即“角点”。本练习将揭示，在这些点上不存在唯一的支撑超平面，而是一个超平面族。你将学习到这一几何特性如何被次微分这一代数概念精确地刻画。[@problem_id:3179819]", "problem": "考虑由 $f(x_{1},x_{2})=\\max\\{|x_{1}|,|x_{2}|\\}$ 定义的凸函数 $f:\\mathbb{R}^{2}\\to\\mathbb{R}$。令 $\\operatorname{epi}(f)=\\{(x_{1},x_{2},t)\\in\\mathbb{R}^{3}:t\\geq f(x_{1},x_{2})\\}$ 表示 $f$ 的上镜图。在点 $(x_{1},x_{2})=(0,0)$ 处，仅使用凸性、上镜图、次梯度和支撑超平面的定义，完成以下任务：\n- 证明 $f$ 在 $(0,0)$ 处是不可微的，并通过次梯度不等式刻画其次梯度集合 $\\partial f(0,0)$。\n- 解释每个次梯度 $g\\in\\partial f(0,0)$ 如何在点 $(0,0,f(0,0))$ 处产生一个 $\\operatorname{epi}(f)$ 的支撑超平面，并描述所有这些支撑超平面的外法向量集合。\n- 在此集合中，找出对应于证明不可微性的不同次梯度的两个极端外法线方向，并计算这两条极端外法线射线之间的夹角。\n\n将最终角度以弧度表示为一个精确值。你的最终答案必须是单一的闭式解析表达式。", "solution": "问题要求在点 $(x_{1},x_{2})=(0,0)$ 处分析凸函数 $f(x_{1},x_{2})=\\max\\{|x_{1}|,|x_{2}|\\}$，重点关注其不可微性、次梯度以及其上镜图的支撑超平面的几何性质。\n\n令 $x = (x_{1}, x_{2}) \\in \\mathbb{R}^{2}$。函数为 $f(x) = \\max\\{|x_{1}|, |x_{2}|\\}$。分析点为 $x_0 = (0,0)$。在此点，$f(x_0) = f(0,0) = \\max\\{|0|,|0|\\} = 0$。\n\n**第一部分：不可微性与次梯度集合**\n\n首先，我们证明 $f$ 在 $x_0 = (0,0)$ 处是不可微的。一个函数在某点可微，如果其方向导数存在且对于方向向量是线性的。$f$ 在 $x_0$ 处沿方向 $v=(v_{1},v_{2})$ 的方向导数由以下公式给出：\n$$ f'(x_0; v) = \\lim_{h\\to 0^{+}} \\frac{f(x_0+hv) - f(x_0)}{h} $$\n代入 $x_0=(0,0)$ 和 $f(0,0)=0$：\n$$ f'(0; v) = \\lim_{h\\to 0^{+}} \\frac{f(hv) - 0}{h} = \\lim_{h\\to 0^{+}} \\frac{\\max\\{|hv_{1}|, |hv_{2}|\\}}{h} $$\n因为 $h0$，我们有 $|h|=h$，所以可以将其从最大值函数中提取出来：\n$$ f'(0; v) = \\lim_{h\\to 0^{+}} \\frac{h\\max\\{|v_{1}|, |v_{2}|\\}}{h} = \\max\\{|v_{1}|, |v_{2}|\\} $$\n为了使 $f$ 在 $(0,0)$ 处可微，必须存在一个梯度向量 $\\nabla f(0,0) = g \\in \\mathbb{R}^2$ 使得对所有 $v \\in \\mathbb{R}^2$ 都有 $f'(0; v) = g \\cdot v$。这将要求对于所有的 $v_{1}, v_{2}$，都有 $\\max\\{|v_{1}|, |v_{2}|\\} = g_{1}v_{1} + g_{2}v_{2}$。我们测试两个不同的方向：\n- 对于 $v=(1,0)$，我们得到 $\\max\\{|1|,|0|\\} = 1 = g_1(1) + g_2(0)$，所以 $g_1 = 1$。\n- 对于 $v=(-1,0)$，我们得到 $\\max\\{|-1|,|0|\\} = 1 = g_1(-1) + g_2(0)$，所以 $g_1 = -1$。\n因为 $g_{1}$ 不能同时为 $1$ 和 $-1$，所以不存在这样的梯度向量 $g$。因此，$f$ 在 $(0,0)$ 处是不可微的。\n\n接下来，我们刻画次梯度集合 $\\partial f(0,0)$。如果对于所有 $x=(x_1, x_2) \\in \\mathbb{R}^2$，以下次梯度不等式成立，那么向量 $g=(g_1, g_2) \\in \\mathbb{R}^2$ 就是 $f$ 在 $x_0=(0,0)$ 处的次梯度：\n$$ f(x) \\geq f(x_0) + g \\cdot (x - x_0) $$\n代入 $f(x)$、$x_0$ 和 $f(x_0)$：\n$$ \\max\\{|x_1|, |x_2|\\} \\geq g_1 x_1 + g_2 x_2 $$\n为了找到对 $g$ 的条件，我们必须确保这个不等式对所有 $x$ 都成立。\n我们选择特定的 $x$ 值来推导 $g_1$ 和 $g_2$ 的必要条件。考虑 $x = (\\text{sgn}(g_1), \\text{sgn}(g_2))$。注意 $|\\text{sgn}(g_1)|$ 要么是 $0$ 要么是 $1$，对 $g_2$ 也类似。\n$$ \\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\} \\geq g_1 \\text{sgn}(g_1) + g_2 \\text{sgn}(g_2) = |g_1| + |g_2| $$\n左边是 $\\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\}$，如果 $g_1, g_2$ 中至少有一个非零，则为 $1$，如果两者都为零，则为 $0$。在所有情况下，$\\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\} \\le 1$。不等式要求 $|g_1| + |g_2| \\leq \\max\\{|\\text{sgn}(g_1)|, |\\text{sgn}(g_2)|\\}$。如果我们选择 $x_1 = \\text{sgn}(g_1)$ 和 $x_2=0$，不等式给出 $|x_1| \\ge g_1 x_1 \\implies 1 \\ge |g_1|$（如果 $g_1 \\ne 0$）。一个更好的 $x$ 的选择是 $x_1 = s \\cdot \\text{sgn}(g_1)$ 和 $x_2 = s \\cdot \\text{sgn}(g_2)$，对于某个 $s0$。那么不等式变为 $s \\ge s(|g_1| + |g_2|)$，这意味着 $1 \\ge |g_1|+|g_2|$。这是一个必要条件。\n\n现在我们证明这个条件也是充分的。假设 $|g_1| + |g_2| \\le 1$。对于任何 $x=(x_1, x_2)$，我们有：\n$$ g_1 x_1 + g_2 x_2 \\le |g_1 x_1| + |g_2 x_2| = |g_1||x_1| + |g_2||x_2| $$\n令 $M = \\max\\{|x_1|, |x_2|\\}$。那么 $|x_1| \\le M$ 且 $|x_2| \\le M$。\n$$ |g_1||x_1| + |g_2||x_2| \\le |g_1|M + |g_2|M = (|g_1| + |g_2|)M $$\n因为我们假设了 $|g_1| + |g_2| \\le 1$，我们有 $(|g_1| + |g_2|)M \\le 1 \\cdot M = M$。\n因此，$g_1 x_1 + g_2 x_2 \\le M = \\max\\{|x_1|, |x_2|\\}$，满足次梯度不等式。\n所以，次梯度集合为 $\\partial f(0,0) = \\{ g=(g_1,g_2) \\in \\mathbb{R}^2 : |g_1| + |g_2| \\le 1 \\}$。这个集合是一个旋转了 $45^\\circ$ 的正方形，其顶点位于 $(1,0)$, $(0,1)$, $(-1,0)$ 和 $(0,-1)$。由于这个集合不是单点集，这证实了 $f$ 在 $(0,0)$ 处是不可微的。\n\n**第二部分：上镜图的支撑超平面**\n\n$f$ 的上镜图是集合 $\\operatorname{epi}(f) = \\{ (x,t) \\in \\mathbb{R}^3 : t \\ge f(x) \\}$。对应于 $x_0=(0,0)$ 的上镜图边界上的点是 $(x_0, f(x_0)) = (0,0,0)$。\n在 $(0,0,0)$ 处 $\\operatorname{epi}(f)$ 的一个支撑超平面是穿过该点的一个超平面，使得整个集合 $\\operatorname{epi}(f)$ 位于由该超平面定义的某个闭半空间内。设超平面的法向量为 $a=(n_1, n_2, n_3) \\in \\mathbb{R}^3$，其中 $n=(n_1,n_2)$。超平面方程为 $n\\cdot x + n_3 t = 0$。支撑条件是对于所有 $(x,t) \\in \\operatorname{epi}(f)$，我们必须有 $n\\cdot x + n_3 t \\le 0$（对于一个向外的法向量）。\n\n对于任何 $(x,t) \\in \\operatorname{epi}(f)$，我们有 $t \\ge f(x)$。考虑形式为 $(0,0,t)$ 且 $t \\ge f(0,0)=0$ 的点。对于这些点，不等式变为 $n_3 t \\le 0$。因为这必须对所有 $t \\ge 0$ 成立，我们必须有 $n_3 \\le 0$。一个非平凡的超平面需要一个非零的法向量。如果 $n_3 = 0$，那么对所有 $x \\in \\mathbb{R}^2$ 都有 $n \\cdot x \\le 0$，这意味着 $n=0$。因此，对于一个非平凡的支撑超平面，$n_3  0$。\n\n让我们通过设置 $n_3 = -1$ 来归一化法向量。支撑条件变为 $n \\cdot x - t \\le 0$，即 $t \\ge n \\cdot x$。这必须对所有满足 $t \\ge f(x)$ 的 $(x,t)$ 成立。这当且仅当它对 $t$ 的最小可能值 $t=f(x)$ 成立时才为真。所以我们要求对所有 $x \\in \\mathbb{R}^2$ 都有 $f(x) \\ge n \\cdot x$。\n这恰好是在 $x_0=(0,0)$ 处的次梯度不等式，其中 $n=g$。\n因此，对于每个次梯度 $g \\in \\partial f(0,0)$，向量 $(g, -1)$ 是 $\\operatorname{epi}(f)$ 在 $(0,0,0)$ 处的一个支撑超平面的外法向量。所有这些外法向量的集合是：\n$$ N = \\{ (g_1, g_2, -1) \\in \\mathbb{R}^3 : |g_1| + |g_2| \\le 1 \\} $$\n\n**第三部分：极端法向量与夹角**\n\n次梯度集合 $\\partial f(0,0)$ 是一个凸多边形。其“极端”点是其顶点。这些点产生了极端的支撑超平面。次微分 $\\partial f(0,0)$ 的顶点是 $g_A=(1,0)$, $g_B=(0,1)$, $g_C=(-1,0)$ 和 $g_D=(0,-1)$。\n\n问题要求找出两个证明不可微性的极端外法线方向。$f$ 在 $(0,0)$ 处的不可微性可以通过证明方向导数 $f'(0;v)$ 的线性性失效来展示。一个标准的演示使用标准基向量 $v=e_1=(1,0)$ 和 $w=e_2=(0,1)$：\n$f'(0; e_1) = \\max\\{|1|,|0|\\} = 1$。\n$f'(0; e_2) = \\max\\{|0|,|1|\\} = 1$。\n$f'(0; e_1+e_2) = f'(0; (1,1)) = \\max\\{|1|,|1|\\} = 1$。\n如果 $f$ 是可微的，我们应有 $f'(0; e_1+e_2) = f'(0; e_1)+f'(0; e_2) = 1+1=2$。因为 $1 \\neq 2$，所以 $f$ 是不可微的。\n方向导数也由 $f'(x; v) = \\max_{g \\in \\partial f(x)} g \\cdot v$ 给出。对于 $v=e_1$ 和 $v=e_2$，达到最大值的次梯度分别是 $g_A=(1,0)$ 和 $g_B=(0,1)$。因此，这两个极端次梯度是不可微性的天然“见证者”。\n\n因此，我们将两个极端次梯度确定为 $g_A=(1,0)$ 和 $g_B=(0,1)$。相应的极端外法向量是：\n$n_A = (g_A, -1) = (1, 0, -1)$\n$n_B = (g_B, -1) = (0, 1, -1)$\n\n这两个向量之间的夹角 $\\theta$ 可以使用点积公式 $\\cos\\theta = \\frac{n_A \\cdot n_B}{\\|n_A\\| \\|n_B\\|}$ 求得。\n点积是：\n$$ n_A \\cdot n_B = (1)(0) + (0)(1) + (-1)(-1) = 0 + 0 + 1 = 1 $$\n向量的模是：\n$$ \\|n_A\\| = \\sqrt{1^2 + 0^2 + (-1)^2} = \\sqrt{2} $$\n$$ \\|n_B\\| = \\sqrt{0^2 + 1^2 + (-1)^2} = \\sqrt{2} $$\n夹角的余弦是：\n$$ \\cos\\theta = \\frac{1}{\\sqrt{2} \\cdot \\sqrt{2}} = \\frac{1}{2} $$\n角度 $\\theta$ 以弧度表示为：\n$$ \\theta = \\arccos\\left(\\frac{1}{2}\\right) = \\frac{\\pi}{3} $$", "answer": "$$\\boxed{\\frac{\\pi}{3}}$$", "id": "3179819"}, {"introduction": "现在，让我们从理论走向一个强大的现实世界应用。本练习将展示，分离凸包这一抽象几何概念如何为支持向量机（SVM）——现代机器学习的支柱——提供理论基础。通过将最大间隔分类器与凸包间的最小距离问题联系起来，你将看到优化理论如何解决实际的数据分类问题。[@problem_id:3114075]", "problem": "设 $A,B \\subset \\mathbb{R}^n$ 是两个有限不相交的点云。考虑寻找一个线性分类器以最大可能间隔分离 $A$ 和 $B$ 的问题。从以下基本定义和事实开始：\n\n- 一个集合 $S \\subset \\mathbb{R}^n$ 的凸包，记作 $\\operatorname{conv}(S)$，是 $S$ 中所有点的凸组合的集合，即 $\\operatorname{conv}(S) = \\left\\{ \\sum_{i=1}^m \\alpha_i s_i \\,\\middle|\\, s_i \\in S, \\alpha_i \\ge 0, \\sum_{i=1}^m \\alpha_i = 1 \\right\\}$。\n- $\\mathbb{R}^n$ 中的一个超平面是集合 $\\{ x \\in \\mathbb{R}^n \\mid \\langle w, x \\rangle + b = 0 \\}$，其中 $w \\in \\mathbb{R}^n$ 且 $b \\in \\mathbb{R}$，$\\langle \\cdot, \\cdot \\rangle$ 表示标准欧几里得内积。$w$ 的欧几里得范数为 $\\|w\\| = \\sqrt{\\langle w, w \\rangle}$。\n- 分离带标签点 $\\{(x_i, y_i)\\}$（其中 $y_i \\in \\{+1,-1\\}$）的线性分类器 $(w,b)$ 的间隔，是指从超平面 $\\{x \\mid \\langle w, x \\rangle + b = 0\\}$ 到任意训练点的最小距离，以欧几里得度量单位表示。对于硬间隔分离，可以对 $(w,b)$ 进行缩放，使得对所有 $i$ 都有 $y_i (\\langle w, x_i \\rangle + b) \\ge 1$；在此缩放下，间隔等于 $1/\\|w\\|$。\n- 给定两个非空、不相交、闭合的凸集 $C,D \\subset \\mathbb{R}^n$，超平面分离定理保证存在一个分离超平面，并且当 $C$ 和 $D$ 是紧集时，可以取到最小欧几里得距离 $\\operatorname{dist}(C,D) = \\inf\\{ \\|x - y\\| \\mid x \\in C, y \\in D \\}$。\n\n任务1（推导）：仅使用上述基本定义和事实，推导为何对于两个不相交的有限集 $A$ 和 $B$，硬间隔最大间隔线性分类器等价于最大化 $\\operatorname{conv}(A)$ 和 $\\operatorname{conv}(B)$ 之间的欧几里得距离。具体来说，证明实现最大间隔的超平面，其法向量与连接 $\\operatorname{conv}(A)$ 和 $\\operatorname{conv}(B)$ 之间最近点对的线段对齐，并且间隔等于这两个凸包之间最小距离的一半。你的推导必须明确建立几何间隔与凸包分离之间的关系，从凸包、超平面、间隔和欧几里得距离的定义出发，并依赖于诸如超平面分离等经过充分验证的事实。\n\n任务2（算法设计）：设计两个基于凸优化原理的、以优化为基础的计算方法：\n\n- 通过最小化平方欧几里得范数 $\\frac{1}{2}\\|w\\|^2$ 来计算硬间隔最大间隔分类器，其约束条件为对于所有训练点 $x_i \\in A \\cup B$ 都有 $y_i(\\langle w, x_i \\rangle + b) \\ge 1$，其中对于 $x_i \\in A$，$y_i = +1$，对于 $x_i \\in B$，$y_i = -1$。将间隔报告为 $1/\\|w\\|$。\n- 通过求解凸优化问题来计算 $\\operatorname{conv}(A)$ 和 $\\operatorname{conv}(B)$ 之间的最小欧几里得距离\n$$\n\\min_{\\alpha \\in \\mathbb{R}^{|A|},\\, \\beta \\in \\mathbb{R}^{|B|}} \\left\\| \\sum_{i=1}^{|A|} \\alpha_i a_i - \\sum_{j=1}^{|B|} \\beta_j b_j \\right\\|^2 \\quad \\text{subject to} \\quad \\alpha_i \\ge 0, \\sum_{i=1}^{|A|} \\alpha_i = 1, \\ \\beta_j \\ge 0, \\sum_{j=1}^{|B|} \\beta_j = 1,\n$$\n其中 $\\{a_i\\}$ 是 $A$ 中的点，$\\{b_j\\}$ 是 $B$ 中的点。将欧几里得距离报告为最优目标值的平方根。\n\n任务3（数值实验与测试套件）：实现一个单一的程序，对于以下每个测试用例，该程序计算：\n- 根据上述描述，从硬间隔分类器得到的间隔 $m$，以及\n- 根据上述描述，$\\operatorname{conv}(A)$ 和 $\\operatorname{conv}(B)$ 之间的最小距离 $d$，\n然后，对每个用例返回绝对差 $|d - 2m|$ 和一个布尔值，该布尔值指示 $|d - 2m|$ 是否小于或等于容差 $\\tau = 10^{-4}$。\n\n使用以下点云测试套件（所有坐标均无单位）：\n\n- 测试用例1（二维，良好分离）：\n$A = \\{(0.0,0.0), (0.0,1.0), (0.8,0.2), (0.5,0.9)\\}$ 且 $B = \\{(3.0,0.1), (3.2,1.1), (3.8,0.4), (4.0,0.9)\\}$。\n- 测试用例2（三维，小间距分离）：\n$A = \\{(-0.2,-0.1,0.0), (0.0,0.2,-0.1), (0.1,-0.2,0.1), (-0.1,0.0,0.2)\\}$ 且 $B = \\{(0.6,0.1,0.0), (0.7,-0.2,0.1), (0.8,0.2,-0.1), (0.9,0.0,0.2)\\}$。\n- 测试用例3（二维，多个支持向量）：\n$A = \\{(0.0,-0.2), (0.0,0.2), (1.0,-0.2), (1.0,0.2), (0.5,-0.2), (0.5,0.2), (0.25,0.2), (0.75,-0.2)\\}$ 且 $B = \\{(2.0,-0.25), (2.0,0.25), (3.0,-0.25), (3.0,0.25), (2.5,-0.25), (2.5,0.25), (2.25,0.25), (2.75,-0.25)\\}$。\n\n最终输出格式：你的程序应生成单行输出，其中包含以下精确格式的汇总结果：\n- 一个包含两个元素的嵌套列表：\n1. 一个包含三个浮点数的列表，按给定顺序表示三个测试用例的绝对差 $|d - 2m|$。\n2. 一个包含三个布尔值的列表，指示每个绝对差是否小于或等于 $\\tau$。\n例如，输出必须形如\n$[[e_1,e_2,e_3],[p_1,p_2,p_3]]$\n不得包含任何额外文本。\n\n角度、物理单位或百分比不适用于此问题；所有量均为无单位实数。程序必须是自包含的，无需输入，并遵循上述精确的输出格式。", "solution": "我们从核心定义和事实开始。对于有限不相交的点云 $A,B \\subset \\mathbb{R}^n$，其凸包 $\\operatorname{conv}(A)$ 和 $\\operatorname{conv}(B)$ 是 $\\mathbb{R}^n$ 中的紧凸子集。由于它们不相交，根据超平面分离定理，存在分离这两个集合的平行支撑超平面。两个紧凸集之间的最小欧几里得距离可由至少一对点 $(p^\\star, q^\\star) \\in \\operatorname{conv}(A) \\times \\operatorname{conv}(B)$ 达到，并且连接 $p^\\star$ 和 $q^\\star$ 的线段与在这些点上的一对平行支撑超平面正交。\n\n我们通过 $(w,b)$ 定义一个线性分类器，其决策边界为 $\\{x \\mid \\langle w, x \\rangle + b = 0\\}$。对于两个带标签集合的硬间隔分离，我们可以缩放 $(w,b)$ 使得对所有 $i$（其中 $y_i \\in \\{+1,-1\\}$）都有 $y_i(\\langle w, x_i \\rangle + b) \\ge 1$，这里 $x_i \\in A$ 时 $y_i=+1$，$x_i \\in B$ 时 $y_i=-1$。在此缩放下，几何间隔等于 $1/\\|w\\|$，即从超平面到任一类别中最近训练点的欧几里得距离。最大间隔分类器在这些分离约束下最小化 $\\frac{1}{2}\\|w\\|^2$。\n\n我们现在将超平面的间隔与凸包之间的距离联系起来。考虑定义凸包间最小距离的优化问题：\n$$\n\\min_{\\alpha, \\beta} \\left\\| \\sum_{i=1}^{|A|} \\alpha_i a_i - \\sum_{j=1}^{|B|} \\beta_j b_j \\right\\|^2 \\quad \\text{subject to} \\quad \\alpha \\ge 0, \\sum_i \\alpha_i = 1, \\ \\beta \\ge 0, \\sum_j \\beta_j = 1,\n$$\n其最优解为 $(\\alpha^\\star,\\beta^\\star)$，对应的点为 $p^\\star = \\sum_i \\alpha_i^\\star a_i \\in \\operatorname{conv}(A)$ 和 $q^\\star = \\sum_j \\beta_j^\\star b_j \\in \\operatorname{conv}(B)$。向量 $v^\\star = p^\\star - q^\\star$ 实现了最小距离 $d = \\|v^\\star\\|$。由于集合是凸的，该凸问题的一阶最优性条件意味着存在与 $v^\\star$ 正交的平行支撑超平面，分别在 $p^\\star$ 和 $q^\\star$ 处与集合相切。具体来说，考虑一对与 $v^\\star$ 正交的平行超平面，使其分别通过 $p^\\star$ 和 $q^\\star$。与 $v^\\star$ 正交并平分线段 $[p^\\star,q^\\star]$ 的中点超平面分离了 $\\operatorname{conv}(A)$ 和 $\\operatorname{conv}(B)$，因此也分离了 $A$ 和 $B$，因为凸包是不相交的。\n\n选择与 $v^\\star$ 共线的 $w$，即 $w = \\lambda v^\\star$（对于某个 $\\lambda  0$），并选择 $b$ 使得两个平行平面 $\\langle w, x \\rangle + b = \\pm 1$ 分别通过 $p^\\star$ 和 $q^\\star$。沿法线方向测量的这两个平面之间的距离等于 $\\frac{2}{\\|w\\|}$，因为在 $y_i(\\langle w, x_i \\rangle + b) \\ge 1$ 的缩放下，从超平面 $\\langle w, x \\rangle + b = 0$ 到任一平面的有向距离为 $\\frac{1}{\\|w\\|}$。点 $p^\\star$ 和 $q^\\star$ 到中心超平面的距离大小相等，符号相反。根据构造，连接 $p^\\star$ 和 $q^\\star$ 的线段与这些超平面正交，所以欧几里得距离 $\\|p^\\star - q^\\star\\|$ 等于两个定义间隔的平面之间的间距，即 $2/\\|w\\|$。因此，对于法向量与 $v^\\star$ 对齐并经缩放以满足硬间隔约束的超平面，其间隔为\n$$\nm = \\frac{1}{\\|w\\|} = \\frac{1}{2} \\|p^\\star - q^\\star\\| = \\frac{1}{2} \\operatorname{dist}(\\operatorname{conv}(A), \\operatorname{conv}(B)).\n$$\n如果我们考虑任何其他分离超平面 $(\\tilde{w},\\tilde{b})$，其间隔不能超过凸包间距的一半，因为最接近的可行间隔平面之间的距离至少与凸包之间最近点对所实现的最小间距一样大。因此，最大化间隔等价于最大化凸包之间的间距，但由于凸包是固定的，最大间隔等于其最小欧几里得间距的一半。这就建立了等价关系。\n\n从算法上讲，我们通过求解硬间隔分类器的凸二次规划问题来计算间隔：\n$$\n\\min_{w \\in \\mathbb{R}^n,\\, b \\in \\mathbb{R}} \\ \\frac{1}{2}\\|w\\|^2 \\quad \\text{subject to} \\quad y_i(\\langle w, x_i \\rangle + b) \\ge 1 \\ \\text{for all } i,\n$$\n该问题是凸的，因为目标是严格凸的二次函数，而约束是仿射不等式。间隔为 $m = 1/\\|w^\\star\\|$，其中 $(w^\\star,b^\\star)$ 是最优解。\n\n我们通过求解\n$$\n\\min_{\\alpha \\in \\mathbb{R}^{|A|},\\, \\beta \\in \\mathbb{R}^{|B|}} \\left\\| \\sum_{i=1}^{|A|} \\alpha_i a_i - \\sum_{j=1}^{|B|} \\beta_j b_j \\right\\|^2\n$$\n来计算凸包之间的最小距离，其约束条件为 $\\alpha$ 和 $\\beta$ 是非负的并且各自求和为1（单纯形约束）。这是一个在单纯形乘积上的凸二次规划问题，因此当二次型在可行集上是严格凸时，它有唯一的最小化子。欧几里得距离是 $d = \\sqrt{\\text{最优目标值}}$。\n\n对于每个测试用例，我们报告绝对差 $|d - 2m|$ 和一个布尔值，用于检查此差异是否在容差 $\\tau = 10^{-4}$ 之内。构建此数值实验是为了在不同配置下验证所推导的等价性：二维良好分离的簇（一般情况）、三维小间距分离的簇（具有小间隔的近边界情况），以及在支撑面上有多个点的二维矩形（具有多个支持向量的边界情况）。所有量都是无单位的，不涉及角度或物理单位。\n\n最终的程序使用序列最小二乘二次规划（Sequential Least Squares Quadratic Programming）实现这两个凸优化问题，计算 $m$ 和 $d$，并按规定打印一个嵌套列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef hard_margin_svm_margin(A, B):\n    \"\"\"\n    Compute the hard-margin SVM margin for two disjoint point sets A (label +1) and B (label -1).\n    Solves: minimize 0.5 * ||w||^2 subject to y_i * (w^T x_i + b) = 1 for all i.\n    Returns: margin m = 1 / ||w||.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    n = A.shape[1]\n    # Build dataset\n    X_pos = A\n    X_neg = B\n    X = np.vstack([X_pos, X_neg])\n    y = np.hstack([np.ones(len(X_pos)), -np.ones(len(X_neg))])\n\n    # Objective: 0.5 * ||w||^2\n    def objective(vars_):\n        w = vars_[:-1]\n        return 0.5 * np.dot(w, w)\n\n    # Gradient of objective\n    def objective_jac(vars_):\n        w = vars_[:-1]\n        grad_w = w\n        grad_b = 0.0\n        return np.concatenate([grad_w, [grad_b]])\n\n    # Inequality constraints: y_i * (w^T x_i + b) - 1 = 0\n    constraints = []\n    for i in range(len(X)):\n        xi = X[i]\n        yi = y[i]\n        def fun_factory(xi, yi):\n            def fun(vars_):\n                w = vars_[:-1]\n                b = vars_[-1]\n                return yi * (np.dot(w, xi) + b) - 1.0\n            return fun\n        def jac_factory(xi, yi):\n            def jac(vars_):\n                # Gradient w.r.t. w is yi * xi, w.r.t. b is yi\n                return np.concatenate([yi * xi, [yi]])\n            return jac\n        constraints.append({'type': 'ineq', 'fun': fun_factory(xi, yi), 'jac': jac_factory(xi, yi)})\n\n    # Initial guess: normal pointing from class B mean to class A mean; b at mid-plane\n    muA = np.mean(A, axis=0)\n    muB = np.mean(B, axis=0)\n    w0 = muA - muB\n    norm_w0 = np.linalg.norm(w0)\n    if norm_w0  1e-8:\n        # Fallback to a canonical direction if means coincide\n        w0 = np.zeros(n)\n        w0[0] = 1.0\n    b0 = -0.5 * np.dot(w0, muA + muB)\n    x0 = np.concatenate([w0, [b0]])\n\n    res = minimize(objective, x0, method='SLSQP', jac=objective_jac,\n                   constraints=constraints, options={'ftol': 1e-12, 'maxiter': 1000, 'disp': False})\n    if not res.success:\n        raise RuntimeError(f\"SVM optimization failed: {res.message}\")\n    w_opt = res.x[:-1]\n    norm_w = np.linalg.norm(w_opt)\n    if norm_w == 0:\n        raise RuntimeError(\"Optimized w has zero norm, margin undefined.\")\n    margin = 1.0 / norm_w\n    return margin\n\ndef convex_hull_distance(A, B):\n    \"\"\"\n    Compute the minimal Euclidean distance between conv(A) and conv(B).\n    Solve QP over simplices:\n        min ||sum_i alpha_i a_i - sum_j beta_j b_j||^2\n        s.t. alpha = 0, sum(alpha) = 1; beta = 0, sum(beta) = 1.\n    Returns: distance d = 0.\n    \"\"\"\n    A = np.asarray(A, dtype=float)\n    B = np.asarray(B, dtype=float)\n    mA, nA = A.shape\n    mB, nB = B.shape\n    assert nA == nB, \"A and B must have same dimension.\"\n\n    # Precompute Gram matrices\n    AA = A @ A.T          # shape (mA, mA)\n    BB = B @ B.T          # shape (mB, mB)\n    AB = A @ B.T          # shape (mA, mB)\n\n    # Objective in terms of z = [alpha, beta]\n    def obj(z):\n        alpha = z[:mA]\n        beta = z[mA:]\n        # Quadratic form: ||A^T alpha - B^T beta||^2\n        term = alpha @ (AA @ alpha) - 2.0 * alpha @ (AB @ beta) + beta @ (BB @ beta)\n        return term\n\n    def obj_jac(z):\n        alpha = z[:mA]\n        beta = z[mA:]\n        grad_alpha = 2.0 * (AA @ alpha) - 2.0 * (AB @ beta)\n        grad_beta = 2.0 * (BB @ beta) - 2.0 * (AB.T @ alpha)\n        return np.concatenate([grad_alpha, grad_beta])\n\n    # Equality constraints: sum(alpha)=1, sum(beta)=1\n    def con_alpha_fun(z):\n        alpha = z[:mA]\n        return np.sum(alpha) - 1.0\n\n    def con_alpha_jac(z):\n        jac = np.zeros(mA + mB)\n        jac[:mA] = 1.0\n        return jac\n\n    def con_beta_fun(z):\n        beta = z[mA:]\n        return np.sum(beta) - 1.0\n\n    def con_beta_jac(z):\n        jac = np.zeros(mA + mB)\n        jac[mA:] = 1.0\n        return jac\n\n    constraints = [\n        {'type': 'eq', 'fun': con_alpha_fun, 'jac': con_alpha_jac},\n        {'type': 'eq', 'fun': con_beta_fun, 'jac': con_beta_jac},\n    ]\n    # Bounds: alpha_i = 0, beta_j = 0\n    bounds = [(0.0, None)] * (mA + mB)\n\n    # Initial guess: uniform weights on each simplex\n    alpha0 = np.full(mA, 1.0 / mA)\n    beta0 = np.full(mB, 1.0 / mB)\n    z0 = np.concatenate([alpha0, beta0])\n\n    res = minimize(obj, z0, method='SLSQP', jac=obj_jac,\n                   bounds=bounds, constraints=constraints,\n                   options={'ftol': 1e-12, 'maxiter': 2000, 'disp': False})\n    if not res.success:\n        raise RuntimeError(f\"Convex hull distance optimization failed: {res.message}\")\n    val = res.fun\n    if val  0:\n        # numerical guard\n        val = max(val, 0.0)\n    d = np.sqrt(val)\n    return d\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1: 2D well separated\n        (\n            np.array([[0.0, 0.0],\n                      [0.0, 1.0],\n                      [0.8, 0.2],\n                      [0.5, 0.9]]),\n            np.array([[3.0, 0.1],\n                      [3.2, 1.1],\n                      [3.8, 0.4],\n                      [4.0, 0.9]])\n        ),\n        # Test Case 2: 3D small separation\n        (\n            np.array([[-0.2, -0.1, 0.0],\n                      [ 0.0,  0.2, -0.1],\n                      [ 0.1, -0.2,  0.1],\n                      [-0.1,  0.0,  0.2]]),\n            np.array([[0.6,  0.1,  0.0],\n                      [0.7, -0.2,  0.1],\n                      [0.8,  0.2, -0.1],\n                      [0.9,  0.0,  0.2]])\n        ),\n        # Test Case 3: 2D rectangles with multiple support vectors\n        (\n            np.array([[0.0, -0.2],\n                      [0.0,  0.2],\n                      [1.0, -0.2],\n                      [1.0,  0.2],\n                      [0.5, -0.2],\n                      [0.5,  0.2],\n                      [0.25, 0.2],\n                      [0.75, -0.2]]),\n            np.array([[2.0, -0.25],\n                      [2.0,  0.25],\n                      [3.0, -0.25],\n                      [3.0,  0.25],\n                      [2.5, -0.25],\n                      [2.5,  0.25],\n                      [2.25, 0.25],\n                      [2.75, -0.25]])\n        ),\n    ]\n\n    tau = 1e-4\n    errors = []\n    passes = []\n    for A, B in test_cases:\n        # Compute margin via hard-margin SVM\n        m = hard_margin_svm_margin(A, B)\n        # Compute distance between convex hulls\n        d = convex_hull_distance(A, B)\n        # Compare d and 2*m\n        err = abs(d - 2.0 * m)\n        errors.append(err)\n        passes.append(err = tau)\n\n    # Final print statement in the exact required format.\n    # Nested list: [[e1,e2,e3],[p1,p2,p3]]\n    # Use repr for booleans and floats consistency in join\n    errors_str = \",\".join(f\"{e:.10f}\" for e in errors)\n    passes_str = \",\".join(\"True\" if p else \"False\" for p in passes)\n    print(f\"[[{errors_str}],[{passes_str}]]\")\n\nsolve()\n```", "id": "3114075"}]}