{"hands_on_practices": [{"introduction": "理解梯度的利普希茨常数，始于掌握从第一性原理出发计算它的能力。这项练习将引导你通过分析一个非二次函数的海森矩阵，来找到其全局利普希茨常数。通过解决这个问题，你将具体地理解一个函数的曲率，特别是它在不同尺度下的行为，是如何直接决定其平滑度参数 $L$ 的 [@problem_id:3144663]。", "problem": "考虑函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$，定义为 $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$，其中 $\\|x\\|_{2}$ 表示欧几里得范数。从梯度关于欧几里得范数是利普希茨连续的定义出发，使用多元微积分的核心知识来确定一个全局利普希茨常数 $L$，使得梯度 $\\nabla f$ 对所有 $x,y\\in\\mathbb{R}^{n}$ 满足 $ \\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}$。你的推导必须基于第一性原理，并且在科学上是一致的。作为你推理的一部分，解释当 $\\|x\\|_{2}$ 很大时，$f$ 的饱和行为如何影响你获得的 $L$ 的界。将 $L$ 的最终值以单个精确数的形式给出。无需四舍五入，不涉及单位。", "solution": "我们从定义开始：如果存在一个常数 $L\\geq 0$，使得对于所有 $x,y\\in\\mathbb{R}^{n}$，函数具有关于欧几里得范数的利普希茨连续梯度，则\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}。\n$$\n根据多元微积分和向量值函数的中值定理，一个经过充分检验的结论是，当 $f$ 是二次连续可微且其海森矩阵 $\\nabla^{2}f(x)$ 对所有 $x$ 都存在时，一个充分的（且在许多光滑情况下是紧的）全局利普希茨常数可以通过海森矩阵谱范数的上确界给出：\n$$\nL=\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2}，\n$$\n其中 $\\|\\cdot\\|_{2}$ 表示由欧几里得范数诱导的算子范数。为了计算这个界，我们将推导 $f$ 的梯度和海森矩阵，然后一致地对 $x$ 计算 $\\nabla^{2}f(x)$ 的最大绝对值特征值。\n\n令 $s=\\|x\\|_{2}^{2}=x^{\\top}x$。定义 $g(s)=\\frac{s}{1+s}$。那么 $f(x)=g(s)$ 是 $g$ 与 $s(x)$ 的复合函数。根据链式法则和已知的梯度 $\\nabla s(x)=2x$，我们有\n$$\ng'(s)=\\frac{(1+s)-s}{(1+s)^{2}}=\\frac{1}{(1+s)^{2}},\n\\quad\\text{所以}\\quad\n\\nabla f(x)=g'(s)\\,\\nabla s(x)=\\frac{2x}{(1+s)^{2}}。\n$$\n为了求海森矩阵，将 $\\nabla f(x)$ 写成 $a(x)\\,x$ 的形式，其中 $a(x)=\\frac{2}{(1+s)^{2}}$。$a(x)\\,x$ 的雅可比矩阵是\n$$\n\\nabla^{2}f(x)=a(x)\\,I + x\\,(\\nabla a(x))^{\\top}，\n$$\n其中 $I$ 是单位矩阵，$\\nabla a(x)$ 是标量场 $a(x)$ 的梯度。通过 $s$ 对 $a(x)$ 关于 $x$ 求导：\n$$\na(x)=2(1+s)^{-2},\\quad \\frac{\\mathrm{d}a}{\\mathrm{d}s} = -4(1+s)^{-3},\n\\quad \\nabla a(x)=\\frac{\\mathrm{d}a}{\\mathrm{d}s}\\,\\nabla s(x) = -4(1+s)^{-3}\\cdot 2x = -\\frac{8x}{(1+s)^{3}}。\n$$\n因此，\n$$\n\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}\\,I \\;-\\; \\frac{8}{(1+s)^{3}}\\,x x^{\\top}。\n$$\n这是一个形式为 $\\alpha\\,I - \\beta\\,u u^{\\top}$ 的对称矩阵，其中 $\\alpha=\\frac{2}{(1+s)^{2}}$，$\\beta=\\frac{8s}{(1+s)^{3}}$，并且当 $x\\neq 0$ 时，$u$ 是与 $x$同向的任意单位向量（如果 $x=0$，第二项为 $0$）。其谱可以通过秩一更新的特征结构来刻画。具体来说：\n- 在任何与 $u$ 正交的方向上，特征值为 $\\lambda_{\\perp}=\\alpha=\\frac{2}{(1+s)^{2}}$（重数为 $n-1$）。\n- 在 $u$ 方向上，由于 $x x^{\\top}$ 的作用相当于 $s\\,u u^{\\top}$，特征值为\n$$\n\\lambda_{\\parallel}=\\alpha - \\beta = \\frac{2}{(1+s)^{2}} - \\frac{8s}{(1+s)^{3}} = \\frac{2 - 6s}{(1+s)^{3}}。\n$$\n对于对称矩阵，谱范数 $\\|\\nabla^{2}f(x)\\|_{2}$ 等于特征值中的最大绝对值。因此，\n$$\n\\|\\nabla^{2}f(x)\\|_{2}=\\max\\!\\left\\{\\left|\\lambda_{\\perp}\\right|,\\left|\\lambda_{\\parallel}\\right|\\right\\}=\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}, \\quad s=\\|x\\|_{2}^{2}\\ge 0。\n$$\n我们现在对 $s\\ge 0$ 最大化这个表达式。\n\n首先，观察到 $\\lambda_{\\perp}(s)=\\frac{2}{(1+s)^{2}}$ 在 $s$ 上是严格递减的，并在 $s=0$ 处达到其最大值：\n$$\n\\max_{s\\ge 0}\\lambda_{\\perp}(s)=\\lambda_{\\perp}(0)=\\frac{2}{(1+0)^{2}}=2。\n$$\n接下来，考虑 $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{|2-6s|}{(1+s)^{3}}$。分为两个区域：\n- 对于 $0\\le s\\le \\frac{1}{3}$，我们有 $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{2-6s}{(1+s)^{3}}$。求导\n$$\nh(s)=\\frac{2-6s}{(1+s)^{3}},\\quad h'(s)=\\frac{12(s-1)}{(1+s)^{4}},\n$$\n在 $[0,\\frac{1}{3}]$ 上为负。因此 $h$ 在该区间上递减，其在此区间的最大值在 $s=0$ 处取得：\n$$\n\\max_{0\\le s\\le \\frac{1}{3}}\\left|\\lambda_{\\parallel}(s)\\right|=h(0)=\\frac{2}{1^{3}}=2。\n$$\n- 对于 $s\\ge \\frac{1}{3}$，我们有 $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{6s-2}{(1+s)^{3}}$。求导\n$$\nk(s)=\\frac{6s-2}{(1+s)^{3}},\\quad k'(s)=\\frac{12(1-s)}{(1+s)^{4}},\n$$\n在 $\\left[\\frac{1}{3},1\\right]$ 上为正，在 $s1$ 时为负。因此，在 $s\\ge \\frac{1}{3}$ 上的最大值出现在 $s=1$ 处：\n$$\n\\max_{s\\ge \\frac{1}{3}}\\left|\\lambda_{\\parallel}(s)\\right|=k(1)=\\frac{6\\cdot 1-2}{(1+1)^{3}}=\\frac{4}{8}=\\frac{1}{2}。\n$$\n结合两个区域，$\\left|\\lambda_{\\parallel}(s)\\right|$ 的全局最大值是 $\\max\\{2,\\frac{1}{2}\\}=2$。因此，\n$$\n\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2}=\\sup_{s\\ge 0}\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}=2。\n$$\n根据前面的原理，这个上确界为梯度提供了一个有效的全局利普希茨常数：\n$$\nL=2。\n$$\n\n最后，我们讨论饱和行为的影响。当 $\\|x\\|_{2}\\to\\infty$ 时，函数 $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$ 趋近于饱和值 $1$，并且梯度 $\\nabla f(x)=\\frac{2x}{(1+\\|x\\|_{2}^{2})^{2}}$ 趋于 $0$。海森矩阵 $\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}I-\\frac{8}{(1+s)^{3}}x x^{\\top}$ 的算子范数也随着 $s\\to\\infty$ 而衰减到零矩阵。这意味着曲率在原点附近最大，并随着函数的饱和而减小，因此全局利普希茨常数 $L$ 由 $\\|x\\|_{2}$ 较小值的行为决定，具体来说，在 $x=0$ 处达到其最大值，此时 $\\nabla^{2}f(0)=2I$ 且 $\\|\\nabla^{2}f(0)\\|_{2}=2$。", "answer": "$$\\boxed{2}$$", "id": "3144663"}, {"introduction": "在许多机器学习应用中，目标函数是大量单个损失函数的平均值。本练习探讨了一个引人入胜且至关重要的特性：多个函数的平均值可能比任何单个分量都要“平滑”得多。这项练习使用一个简单的二次函数例子，揭示了为何平均化能带来一个更小的利普希茨常数，这是提升许多分布式优化算法效率的关键原理 [@problem_id:3144694]。", "problem": "考虑 $L \\in \\mathbb{R}$ 且 $L0$，定义函数 $\\phi_1,\\phi_2:\\mathbb{R}^2 \\to \\mathbb{R}$ 为\n$$\n\\phi_1(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_1 x\n\\quad\\text{和}\\quad\n\\phi_2(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_2 x,\n$$\n其中\n$$\nA_1 \\;=\\; \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}\n\\quad\\text{和}\\quad\nA_2 \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}.\n$$\n令 $f:\\mathbb{R}^2 \\to \\mathbb{R}$ 为经验平均值\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\big(\\phi_1(x)+\\phi_2(x)\\big).\n$$\n仅从梯度利普希茨连续性的定义和关于对称矩阵的基本线性代数知识出发，首先证明每个 $\\phi_i$ 的梯度相对于欧几里得范数是 $L$-利普希茨的，然后确定满足以下条件的最小常数 $L_f$：\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_2 \\;\\le\\; L_f \\,\\|x-y\\|_2\n\\quad\\text{对所有}\\quad x,y \\in \\mathbb{R}^2.\n$$\n请以 $L$ 的闭式表达式形式给出最终答案。无需四舍五入，也不涉及单位。答案必须是一个单独的表达式。", "solution": "该问题是适定的，有科学依据，并包含确定唯一解所需的所有信息。我们可以开始求解。\n\n问题要求做两件事：首先，证明 $\\phi_1$ 和 $\\phi_2$ 的梯度是 $L$-利普希茨连续的；其次，求出它们的经验平均值 $f$ 的梯度的最小利普希茨常数。\n\n令 $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$ 和 $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\in \\mathbb{R}^2$ 为任意向量。范数为欧几里得范数 $\\|\\cdot\\|_2$。\n\n如果存在一个常数 $K \\ge 0$，使得对于所有 $x, y \\in \\mathbb{R}^n$，都有 $\\|g(x) - g(y)\\|_2 \\le K \\|x-y\\|_2$，则函数 $g: \\mathbb{R}^n \\to \\mathbb{R}^m$ 是 $K$-利普希茨连续的。满足条件的最小 $K$ 即为利普希茨常数。\n\n对于一个定义域为整个 $\\mathbb{R}^n$ 的可微函数，如果其海森矩阵有界，即对所有 $x$ 都有 $\\|\\nabla^2 f(x)\\|_2 \\le L$，则其梯度是 $L$-利普希茨连续的。对于形如 $\\phi(x) = \\frac{1}{2} x^\\top A x$ 的二次函数，其中 $A$ 为对称矩阵，梯度为 $\\nabla \\phi(x) = Ax$，海森矩阵为 $\\nabla^2 \\phi(x) = A$。梯度的利普希茨常数是常数海森矩阵的谱范数，即 $\\|A\\|_2$。对于对称矩阵，谱范数是其最大特征值的绝对值，$\\lambda_{\\max}(|A|)$。\n\n**第1部分：$\\phi_1$ 和 $\\phi_2$ 梯度的利普希茨连续性**\n\n首先，考虑函数 $\\phi_1(x) = \\frac{1}{2} x^\\top A_1 x$，其中 $A_1 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}$。矩阵 $A_1$ 是对称的。\n$\\phi_1$ 的梯度由下式给出：\n$$\n\\nabla \\phi_1(x) = A_1 x = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} Lx_1 \\\\ 0 \\end{pmatrix}.\n$$\n为了检验梯度的利普希茨连续性，我们考察其差值：\n$$\n\\nabla \\phi_1(x) - \\nabla \\phi_1(y) = A_1 x - A_1 y = A_1(x-y).\n$$\n取欧几里得范数：\n$$\n\\|\\nabla \\phi_1(x) - \\nabla \\phi_1(y)\\|_2 = \\|A_1(x-y)\\|_2.\n$$\n根据诱导矩阵范数（谱范数）的定义，我们有：\n$$\n\\|A_1(x-y)\\|_2 \\le \\|A_1\\|_2 \\|x-y\\|_2.\n$$\n满足此不等式的最小常数是 $\\|A_1\\|_2$。对于对称矩阵，该范数是其特征值绝对值的最大值。$A_1$ 的特征值是其对角线元素，即 $\\lambda_1 = L$ 和 $\\lambda_2 = 0$。由于给定 $L0$，特征值是非负的。\n$$\n\\|A_1\\|_2 = \\max(|L|, |0|) = L.\n$$\n因此，$\\|\\nabla \\phi_1(x) - \\nabla \\phi_1(y)\\|_2 \\le L \\|x-y\\|_2$ 对所有 $x, y \\in \\mathbb{R}^2$ 成立。这表明 $\\nabla \\phi_1(x)$ 是 $L$-利普希茨连续的。\n\n类似地，对于函数 $\\phi_2(x) = \\frac{1}{2} x^\\top A_2 x$，其中 $A_2 = \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}$。矩阵 $A_2$ 是对称的。\n$\\phi_2$ 的梯度为：\n$$\n\\nabla \\phi_2(x) = A_2 x = \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ Lx_2 \\end{pmatrix}.\n$$\n梯度的差值为 $\\nabla \\phi_2(x) - \\nabla \\phi_2(y) = A_2(x-y)$。$\\nabla \\phi_2$ 的利普希茨常数是 $\\|A_2\\|_2$。$A_2$ 的特征值是 $\\lambda_1=0$ 和 $\\lambda_2=L$。谱范数为：\n$$\n\\|A_2\\|_2 = \\max(|0|, |L|) = L.\n$$\n因此，$\\|\\nabla \\phi_2(x) - \\nabla \\phi_2(y)\\|_2 \\le L \\|x-y\\|_2$ 对所有 $x, y \\in \\mathbb{R}^2$ 成立。这表明 $\\nabla \\phi_2(x)$ 也是 $L$-利普希茨连续的。\n\n**第2部分：$f$ 的梯度的最小利普希茨常数 $L_f$**\n\n函数 $f(x)$ 定义为 $\\phi_1(x)$ 和 $\\phi_2(x)$ 的经验平均值：\n$$\nf(x) = \\frac{1}{2} (\\phi_1(x) + \\phi_2(x)).\n$$\n代入 $\\phi_1$ 和 $\\phi_2$ 的定义：\n$$\nf(x) = \\frac{1}{2} \\left( \\frac{1}{2} x^\\top A_1 x + \\frac{1}{2} x^\\top A_2 x \\right) = \\frac{1}{4} x^\\top (A_1 + A_2) x\n$$\n梯度算子是线性的，所以我们可以写出：\n$$\n\\nabla f(x) = \\nabla \\left( \\frac{1}{2} (\\phi_1(x) + \\phi_2(x)) \\right) = \\frac{1}{2} (\\nabla \\phi_1(x) + \\nabla \\phi_2(x)).\n$$\n代入梯度的表达式：\n$$\n\\nabla f(x) = \\frac{1}{2} (A_1 x + A_2 x) = \\frac{1}{2} (A_1 + A_2) x.\n$$\n我们来计算矩阵和 $A_1+A_2$：\n$$\nA_1 + A_2 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix} = \\begin{pmatrix} L  0 \\\\ 0  L \\end{pmatrix} = L I,\n$$\n其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n所以，$f$ 的梯度是：\n$$\n\\nabla f(x) = \\frac{1}{2} (L I) x = \\frac{L}{2} x.\n$$\n我们想要求满足 $\\|\\nabla f(x) - \\nabla f(y)\\|_2 \\le L_f \\|x-y\\|_2$ 的最小常数 $L_f$。\n我们来计算不等式的左边：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_2 = \\left\\| \\frac{L}{2} x - \\frac{L}{2} y \\right\\|_2 = \\left\\| \\frac{L}{2} (x - y) \\right\\|_2.\n$$\n利用范数的性质，并且已知 $L0$：\n$$\n\\left\\| \\frac{L}{2} (x - y) \\right\\|_2 = \\left| \\frac{L}{2} \\right| \\|x-y\\|_2 = \\frac{L}{2} \\|x-y\\|_2.\n$$\n在这种情况下，不等式变成了等式：\n$$\n\\|\\nabla f(x) - \\nabla f(y)\\|_2 = \\frac{L}{2} \\|x-y\\|_2.\n$$\n我们需要找到对所有 $x, y \\in \\mathbb{R}^2$ 都满足 $\\frac{L}{2} \\|x-y\\|_2 \\le L_f \\|x-y\\|_2$ 的最小 $L_f$。对于任意 $x \\ne y$，我们可以用 $\\|x-y\\|_2  0$ 去除，得到：\n$$\n\\frac{L}{2} \\le L_f.\n$$\n因此 $L_f$ 能取的最小值是 $\\frac{L}{2}$。\n\n另外，我们可以求 $f(x)$ 的海森矩阵。$f(x)$ 的海森矩阵是 $\\nabla f(x) = \\frac{L}{2}x$ 的梯度：\n$$\n\\nabla^2 f(x) = \\frac{d}{dx} \\left(\\frac{L}{2} x\\right) = \\frac{L}{2} I = \\begin{pmatrix} L/2  0 \\\\ 0  L/2 \\end{pmatrix}.\n$$\n梯度的利普希茨常数 $L_f$ 是海森矩阵的谱范数。\n$$\nL_f = \\|\\nabla^2 f(x)\\|_2 = \\left\\| \\frac{L}{2} I \\right\\|_2.\n$$\n这个对角矩阵的特征值都是 $\\frac{L}{2}$。谱范数是最大的特征值绝对值。\n$$\nL_f = \\max\\left(\\left|\\frac{L}{2}\\right|, \\left|\\frac{L}{2}\\right|\\right) = \\frac{L}{2}.\n$$\n两种方法得到相同的结果。最小常数 $L_f$ 是 $\\frac{L}{2}$。", "answer": "$$\n\\boxed{\\frac{L}{2}}\n$$", "id": "3144694"}, {"introduction": "对于复杂的非凸函数，单一的全局利普希茨常数可能过于保守，导致基于梯度的优化收敛缓慢。这个动手编程练习将挑战你突破这一限制，实现一种自适应的梯度下降算法。你将比较一个固定步长策略与一个根据局部平滑度动态调整步长的策略，从而获得关于如何利用理论概念设计更智能、更快速的优化方法的实践洞见 [@problem_id:3144675]。", "problem": "考虑二次连续可微函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$，其定义为\n$$\nf(\\mathbf{x}) \\;=\\; \\sum_{i=1}^n \\left( \\sin(x_i) + 0.1\\,x_i^2 \\right),\n$$\n其中 $\\mathbf{x} = (x_1,\\dots,x_n) \\in \\mathbb{R}^n$。所有三角函数量必须以弧度处理。梯度 $\\nabla f$ 和海森矩阵 $\\nabla^2 f$ 处处存在，且梯度在 $\\mathbb{R}^n$ 上是全局利普希茨连续的。\n\n您的任务是，对于非凸函数 $f$，沿着梯度下降 (GD) 轨迹，凭经验比较全局利普希茨常数 $L_{\\mathrm{global}}$ 与局部利普希茨常数 $L_{\\mathrm{local}}(\\mathbf{x}, r)$，并实现一个随时间增加但保持局部安全的步长方案。\n\n该比较和方案必须基于以下有数学依据的构造：\n\n- 全局利普希茨常数 $L_{\\mathrm{global}}$ 是任何可证明的上界，确保对于所有 $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$，都有 $\\|\\nabla f(\\mathbf{x}) - \\nabla f(\\mathbf{y})\\|_2 \\le L_{\\mathrm{global}} \\|\\mathbf{x} - \\mathbf{y}\\|_2$。\n- 对于给定的中心 $\\mathbf{x} \\in \\mathbb{R}^n$ 和半径 $r  0$，局部利普希茨常数 $L_{\\mathrm{local}}(\\mathbf{x}, r)$ 是任何可证明的上界，确保对于所有满足 $\\|\\mathbf{y} - \\mathbf{x}\\|_2 \\le r$ 的 $\\mathbf{y}$，都有 $\\|\\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\\|_2 \\le L_{\\mathrm{local}}(\\mathbf{x}, r) \\|\\mathbf{y} - \\mathbf{x}\\|_2$。在本问题中，$f$ 的海森矩阵是对角的，这使您可以通过取 $\\nabla^2 f$ 的对角元素绝对值在轴对齐区间 $[x_i - r, x_i + r]$（对于 $i=1,\\dots,n$）上的上确界来高效地计算 $L_{\\mathrm{local}}(\\mathbf{x}, r)$。您必须通过在每个坐标区间内使用 $m$ 个点进行均匀采样来数值逼近此上确界，然后取各坐标上的最大值。\n\n您必须实现两种从相同初始点开始并运行指定迭代次数 $T$ 的 GD 程序：\n\n1. 基准恒定步长规则：\n   - 使用 $\\alpha_{\\mathrm{base}} = \\theta / L_{\\mathrm{global}}$，其中给定一个标量 $\\theta \\in (0,1)$。\n   - 更新规则：$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha_{\\mathrm{base}} \\nabla f(\\mathbf{x}_t)$，对于 $t = 0,1,\\dots,T-1$。\n\n2. 保持局部安全的递增方案：\n   - 定义一个递增因子 $\\eta_t = 1 - e^{-\\kappa t}$（其中 $\\kappa  0$）和一个半径方案 $r_t = r_0 \\,\\rho^t$（其中 $r_0  0$ 且 $\\rho \\in (0,1]$）。\n   - 在迭代 $t$ 时，按上述方法计算 $L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)$ 的数值近似值。\n   - 选择步长\n     $$\n     \\alpha_t \\;=\\; \\min\\!\\left( \\frac{\\eta_t}{L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)} \\,,\\, \\frac{r_t}{\\|\\nabla f(\\mathbf{x}_t)\\|_2} \\right),\n     $$\n     其中第二项强制施加一个信赖域上限，以将步长保持在半径为 $r_t$ 的球内，从而使局部利普希茨界适用。\n   - 更新规则：$\\mathbf{x}_{t+1} = \\mathbf{x}_t - \\alpha_t \\nabla f(\\mathbf{x}_t)$，对于 $t = 0,1,\\dots,T-1$。\n\n对于每次运行，记录最终的函数值 $f(\\mathbf{x}_T)$。对于所提出的方案，还需检查序列 $(\\alpha_t)_{t=0}^{T-1}$ 是否单调非递减。\n\n角度单位要求：所有三角函数求值和区间都必须以弧度为单位。\n\n为以下测试套件实现上述内容。每个测试用例都指定为一个元组 $(n, \\mathbf{x}_0, T, r_0, \\rho, \\kappa, \\theta, m)$:\n\n- 用例 A (一般情况): $(3, (2.0, -1.0, 0.5), 60, 0.8, 0.95, 0.12, 0.8, 300)$。\n- 用例 B (靠近高曲率区域): $(3, (-\\tfrac{\\pi}{2}, -\\tfrac{\\pi}{2}, -\\tfrac{\\pi}{2}), 60, 0.4, 0.90, 0.10, 0.8, 300)$。\n- 用例 C (大半径，局部行为等同于全局行为): $(3, (10.0, -10.0, 3.0), 60, 5.0, 1.00, 0.15, 0.8, 300)$。\n\n您的程序必须：\n- 为给定的 $f$ 计算一个有数学依据的 $L_{\\mathrm{global}}$。\n- 对于每个用例，运行两种 GD 变体并计算三个输出：\n  1. 在提出的递增方案下的最终函数值 $f(\\mathbf{x}_T^{\\mathrm{inc}})$，作为浮点数。\n  2. 在基准恒定步长下的最终函数值 $f(\\mathbf{x}_T^{\\mathrm{base}})$，作为浮点数。\n  3. 一个布尔值，指示所提出的方案的 $(\\alpha_t)$ 是否为单调非递减。\n- 生成单行输出，其中包含结果，格式为一个逗号分隔的列表，包含三个子列表，每个子列表对应一个用例，形式为 $[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$，并用方括号括起来，例如：$[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3]]$。\n\n不允许外部输入。程序必须是自包含的并确定性地运行。", "solution": "问题陈述已经过验证，被认为是合理的。这是一个在数值优化领域中提法得当、有科学依据的问题，没有矛盾、歧义或故作高深的论断。我们可以着手提供一个正式的解法。\n\n核心任务是实现并比较两种用于最小化给定非凸函数 $f(\\mathbf{x})$ 的梯度下降 (GD) 算法。第一种算法使用基于梯度全局利普希茨常数的保守恒定步长。第二种算法采用一种更积极的、局部自适应的步长方案，该方案设计为随时间增加，同时通过遵循局部曲率估计来保持安全。\n\n首先，我们将问题的数学组成部分形式化。\n\n目标函数是 $f : \\mathbb{R}^n \\to \\mathbb{R}$，由下式给出：\n$$f(\\mathbf{x}) = \\sum_{i=1}^n \\left( \\sin(x_i) + 0.1\\,x_i^2 \\right)$$\n由于该函数是每个坐标 $x_i$ 的可分离函数的总和，其梯度 $\\nabla f(\\mathbf{x})$ 和海森矩阵 $\\nabla^2 f(\\mathbf{x})$ 的计算非常直接。梯度的第 $j$ 个分量是：\n$$(\\nabla f(\\mathbf{x}))_j = \\frac{\\partial f}{\\partial x_j} = \\cos(x_j) + 0.2\\,x_j$$\n海森矩阵是对角的，因为混合偏导数为零：对于 $i \\neq j$，有 $\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = 0$。对角线元素是：\n$$(\\nabla^2 f(\\mathbf{x}))_{ii} = \\frac{\\partial^2 f}{\\partial x_i^2} = -\\sin(x_i) + 0.2$$\n因此，海森矩阵是：\n$$\\nabla^2 f(\\mathbf{x}) = \\mathrm{diag}(-\\sin(x_1) + 0.2, \\dots, -\\sin(x_n) + 0.2)$$\n\n为了实现基准 GD 算法，我们需要梯度 $\\nabla f$ 的一个全局利普希茨常数 $L_{\\mathrm{global}}$。对于一个二次连续可微函数，可以通过取海森矩阵谱范数在整个定义域 $\\mathbb{R}^n$ 上的上确界来找到这样一个常数。对角矩阵的谱范数（或诱导 $2$-范数）是其对角线元素绝对值的最大值。\n$$L_{\\mathrm{global}} = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n} \\|\\nabla^2 f(\\mathbf{x})\\|_2 = \\sup_{\\mathbf{x} \\in \\mathbb{R}^n} \\max_{i=1, \\dots, n} |-\\sin(x_i) + 0.2|$$\n当 $\\sin(x_i)$ 取其极值 $-1$ 和 $1$ 时，表达式 $|-\\sin(x_i) + 0.2|$ 达到最大值。当 $\\sin(x_i) = -1$ 时，值为 $|-(-1) + 0.2| = |1.2| = 1.2$。当 $\\sin(x_i) = 1$ 时，值为 $|-1 + 0.2| = |-0.8| = 0.8$。这些值中的最大值为 $1.2$。因此，一个有效的全局利普希茨常数是 $L_{\\mathrm{global}} = 1.2$。\n\n接下来，我们定义局部利普希茨常数 $L_{\\mathrm{local}}(\\mathbf{x}, r)$。此常数必须对 $\\mathbf{x}$ 邻域内的所有 $\\mathbf{y}$ 满足 $\\|\\nabla f(\\mathbf{y}) - \\nabla f(\\mathbf{x})\\|_2 \\le L_{\\mathrm{local}}(\\mathbf{x}, r) \\|\\mathbf{y} - \\mathbf{x}\\|_2$。问题指定使用一个可证明的上界。一个合适的界是海森矩阵谱范数在包含该邻域的一个区域上的上确界。按照指示，我们使用由区间 $[x_i - r, x_i + r]$ 定义的轴对齐超矩形。\n$$L_{\\mathrm{local}}(\\mathbf{x}, r) = \\max_{i=1,\\dots,n} \\left( \\sup_{y_i \\in [x_i - r, x_i + r]} |-\\sin(y_i) + 0.2| \\right)$$\n问题要求通过在每个区间 $[x_i - r, x_i + r]$ 内均匀采样 $m$ 个点，在这些点上计算 $|-\\sin(y_i) + 0.2|$ 的值，找出每个坐标 $i$ 的最大值，然后取所有坐标上的最大值来数值逼近该值。\n\n在定义了这些组件之后，我们来具体说明这两种 GD 算法。两者都从一个初始点 $\\mathbf{x}_0$ 开始，运行 $T$ 次迭代。\n\n1.  **基准恒定步长 GD：**\n    步长是恒定的：$\\alpha_{\\mathrm{base}} = \\theta / L_{\\mathrm{global}}$，其中 $\\theta \\in (0,1)$ 是一个给定参数。\n    对于 $t = 0, 1, \\dots, T-1$ 的更新规则是：\n    $$\\mathbf{x}_{t+1}^{\\mathrm{base}} = \\mathbf{x}_t^{\\mathrm{base}} - \\alpha_{\\mathrm{base}} \\nabla f(\\mathbf{x}_t^{\\mathrm{base}})$$\n\n2.  **提出的递增步长 GD：**\n    该算法使用一个随时间变化的步长 $\\alpha_t$。在每次迭代 $t$ 时：\n    - 计算一个递增因子 $\\eta_t = 1 - e^{-\\kappa t}$，其中 $\\kappa  0$。当 $t \\to \\infty$ 时，该因子趋近于 $1$。\n    - 计算一个局部半径 $r_t = r_0 \\rho^t$，其中 $r_0  0$ 且 $\\rho \\in (0,1]$。该半径定义了进行局部分析的区域。\n    - 局部利普希茨常数 $L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)$ 按前述方法进行数值逼近。\n    - 步长 $\\alpha_t$ 选择为：\n      $$\\alpha_t = \\min\\!\\left( \\frac{\\eta_t}{L_{\\mathrm{local}}(\\mathbf{x}_t, r_t)} \\,,\\, \\frac{r_t}{\\|\\nabla f(\\mathbf{x}_t)\\|_2} \\right)$$\n      第一项根据局部曲率调整步长，并由递增因子 $\\eta_t$ 进行缩放。第二项作为一个信赖域约束，确保更新步长 $\\alpha_t \\nabla f(\\mathbf{x}_t)$ 的范数不大于 $r_t$。这将下一次迭代保持在当前迭代点周围半径为 $r_t$ 的球内，在该区域内局部利普希茨估计是适用的。这也避免了当 $\\|\\nabla f(\\mathbf{x}_t)\\|_2=0$ 时出现除以零的情况。\n    - 对于 $t = 0, 1, \\dots, T-1$ 的更新规则是：\n      $$\\mathbf{x}_{t+1}^{\\mathrm{inc}} = \\mathbf{x}_t^{\\mathrm{inc}} - \\alpha_t \\nabla f(\\mathbf{x}_t^{\\mathrm{inc}})$$\n\n实现将对提供的每个测试用例执行这两种算法。对于每个用例，它将报告最终函数值 $f(\\mathbf{x}_T^{\\mathrm{inc}})$、最终函数值 $f(\\mathbf{x}_T^{\\mathrm{base}})$，以及一个布尔值，用以指示所提出方案的步长序列 $(\\alpha_t)_{t=0}^{T-1}$ 是否单调非递减。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by comparing two gradient descent schedules.\n    \"\"\"\n\n    # --- Mathematical function definitions ---\n\n    def f(x: np.ndarray) -> float:\n        \"\"\"Computes the value of the objective function f(x).\"\"\"\n        return np.sum(np.sin(x) + 0.1 * x**2)\n\n    def grad_f(x: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the gradient of the objective function f(x).\"\"\"\n        return np.cos(x) + 0.2 * x\n\n    def hessian_diag_abs(y_coords: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the absolute values of the diagonal entries of the Hessian.\"\"\"\n        return np.abs(-np.sin(y_coords) + 0.2)\n\n    # --- Lipschitz constant calculations ---\n\n    L_global = 1.2  # Derived analytically as sup ||nabla^2 f(x)||_2\n\n    def compute_L_local(x: np.ndarray, r: float, m: int) -> float:\n        \"\"\"\n        Numerically approximates the local Lipschitz constant L_local(x, r).\n        \"\"\"\n        n = x.shape[0]\n        max_eigenvalues = np.zeros(n)\n        for i in range(n):\n            # Sample m points in the interval [x_i - r, x_i + r]\n            sample_points = np.linspace(x[i] - r, x[i] + r, m)\n            # Evaluate the absolute Hessian diagonal entry at these points\n            hessian_values = hessian_diag_abs(sample_points)\n            # Find the max over the samples for this coordinate\n            max_eigenvalues[i] = np.max(hessian_values)\n        \n        # L_local is the max over all coordinates\n        return np.max(max_eigenvalues)\n\n    # --- Gradient Descent algorithms ---\n\n    def run_gd_baseline(x0: np.ndarray, T: int, theta: float) -> float:\n        \"\"\"Runs the baseline GD with a constant step-size.\"\"\"\n        alpha_base = theta / L_global\n        x = np.copy(x0)\n        for _ in range(T):\n            grad = grad_f(x)\n            x = x - alpha_base * grad\n        return f(x)\n\n    def run_gd_proposed(x0: np.ndarray, T: int, r0: float, rho: float, kappa: float, m: int):\n        \"\"\"Runs the proposed GD with an increasing, locally safe step-size.\"\"\"\n        x = np.copy(x0)\n        alpha_history = []\n        \n        for t in range(T):\n            eta_t = 1.0 - np.exp(-kappa * t)\n            r_t = r0 * (rho ** t)\n            \n            L_local_t = compute_L_local(x, r_t, m)\n            \n            grad = grad_f(x)\n            grad_norm = np.linalg.norm(grad)\n\n            # Term 1 of the min expression for alpha_t\n            alpha_term1 = eta_t / L_local_t\n            \n            # Term 2 (trust-region cap), with a safeguard for grad_norm == 0\n            if grad_norm  1e-12:\n                # If gradient is zero, the step is zero. alpha's value is irrelevant for the update.\n                # However, to avoid division by zero, we take the other term.\n                alpha_term2 = np.inf\n            else:\n                alpha_term2 = r_t / grad_norm\n            \n            alpha_t = min(alpha_term1, alpha_term2)\n            alpha_history.append(alpha_t)\n            \n            x = x - alpha_t * grad\n            \n        final_f_val = f(x)\n        \n        # Check for monotonic non-decreasing property of the alpha sequence\n        is_monotone = all(alpha_history[i] >= alpha_history[i-1] for i in range(1, len(alpha_history)))\n        \n        return final_f_val, is_monotone\n\n    # --- Test suite execution ---\n\n    test_cases = [\n        # Case A: (n, x0, T, r0, rho, kappa, theta, m)\n        (3, np.array([2.0, -1.0, 0.5]), 60, 0.8, 0.95, 0.12, 0.8, 300),\n        # Case B:\n        (3, np.array([-np.pi/2, -np.pi/2, -np.pi/2]), 60, 0.4, 0.90, 0.10, 0.8, 300),\n        # Case C:\n        (3, np.array([10.0, -10.0, 3.0]), 60, 5.0, 1.00, 0.15, 0.8, 300),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, x0, T, r0, rho, kappa, theta, m = case\n        \n        # Run baseline GD\n        f_final_base = run_gd_baseline(x0, T, theta)\n        \n        # Run proposed GD\n        f_final_inc, is_monotone = run_gd_proposed(x0, T, r0, rho, kappa, m)\n        \n        # The boolean needs to be lowercase for the final string representation\n        is_monotone_str = str(is_monotone).lower()\n        \n        results.append(f\"[{f_final_inc},{f_final_base},{is_monotone_str}]\")\n\n    # Format the final output string as specified\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3144675"}]}