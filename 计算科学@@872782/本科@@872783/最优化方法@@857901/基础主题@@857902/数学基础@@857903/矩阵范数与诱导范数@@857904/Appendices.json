{"hands_on_practices": [{"introduction": "理论学习的最佳方式是将其付诸实践。我们将从诱导 $1$-范数的基本定义出发，推导出其常用的计算公式，即最大绝对列和。这个练习不仅能加深你对诱导范数作为“最大拉伸因子”这一核心概念的理解，还将通过一个机器学习中的特征缩放场景，展示该范数在评估模型稳定性方面的实际应用价值 [@problem_id:3148401]。", "problem": "考虑一个用于机器学习优化的线性模型，其中一个数据集由矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 表示。$A$ 的每一列对应一个特征，每一行对应一个样本。假设该模型使用坐标级更新，其稳定性受线性映射 $x \\mapsto A x$ 如何放大以向量 1-范数衡量的扰动的影响。仅使用以下基本定义：对于向量 $x \\in \\mathbb{R}^{n}$，其 1-范数定义为 $\\|x\\|_{1} = \\sum_{j=1}^{n} |x_{j}|$，而 $A$ 的诱导矩阵 1-范数定义为\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}.\n$$\n从这些定义以及绝对值和不等式的标准性质出发，推导 $\\|A\\|_{1}$ 的一个以 $A$ 的元素表示的闭式表达式，然后对以下数据集矩阵求值（该矩阵的构造旨在反映一个特征的尺度比其他特征大得多的情况）：\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}.\n$$\n用文字解释所推导的表达式如何与优化中特征级缩放的敏感性相关联，特别是为什么不平衡的特征尺度会主导诱导矩阵 1-范数，从而主导映射 $x \\mapsto A x$ 下扰动的最坏情况放大。最后，报告给定矩阵 $A$ 的 $\\|A\\|_{1}$ 的精确数值。无需四舍五入；请提供精确值。", "solution": "该问题要求推导诱导矩阵 1-范数的闭式表达式，对给定矩阵 $A$ 进行求值，并从概念上解释其与机器学习模型中特征缩放的相关性。这个问题是适定的，有科学依据，并为得出完整解提供了所有必要信息。\n\n首先，我们来推导矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 的诱导矩阵 1-范数（记为 $\\|A\\|_1$）的公式。给出的定义是：\n$$\n\\|A\\|_{1} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{1}}{\\|x\\|_{1}}\n$$\n其中 $x \\in \\mathbb{R}^{n}$ 且向量 1-范数为 $\\|x\\|_1 = \\sum_{j=1}^{n} |x_j|$。\n\n令 $y = Ax$。向量 $y \\in \\mathbb{R}^m$ 的第 $i$ 个分量由 $y_i = \\sum_{j=1}^{n} a_{ij} x_j$ 给出，其中 $a_{ij}$ 是 $A$ 的元素。那么 $Ax$ 的 1-范数是：\n$$\n\\|Ax\\|_1 = \\|y\\|_1 = \\sum_{i=1}^{m} |y_i| = \\sum_{i=1}^{m} \\left| \\sum_{j=1}^{n} a_{ij} x_j \\right|\n$$\n对内部的和应用绝对值的三角不等式（即 $|\\sum_k z_k| \\le \\sum_k |z_k|$），我们得到：\n$$\n\\left| \\sum_{j=1}^{n} a_{ij} x_j \\right| \\le \\sum_{j=1}^{n} |a_{ij} x_j| = \\sum_{j=1}^{n} |a_{ij}| |x_j|\n$$\n将这个不等式代回 $\\|Ax\\|_1$ 的表达式中：\n$$\n\\|Ax\\|_1 \\le \\sum_{i=1}^{m} \\left( \\sum_{j=1}^{n} |a_{ij}| |x_j| \\right)\n$$\n由于所有项都是非负的，我们可以交换求和顺序：\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} \\sum_{i=1}^{m} |a_{ij}| |x_j| = \\sum_{j=1}^{n} \\left( |x_j| \\sum_{i=1}^{m} |a_{ij}| \\right)\n$$\n让我们定义 $C_j = \\sum_{i=1}^{m} |a_{ij}|$ 为 $A$ 的第 $j$ 列中各元素绝对值之和。不等式变为：\n$$\n\\|Ax\\|_1 \\le \\sum_{j=1}^{n} C_j |x_j|\n$$\n现在，令 $C_{\\max} = \\max_{1 \\le j \\le n} C_j = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|$。由于对所有 $j$ 都有 $C_j \\le C_{\\max}$，我们可以写出：\n$$\n\\sum_{j=1}^{n} C_j |x_j| \\le \\sum_{j=1}^{n} C_{\\max} |x_j| = C_{\\max} \\sum_{j=1}^{n} |x_j| = C_{\\max} \\|x\\|_1\n$$\n结合这些不等式，我们为比率 $\\frac{\\|Ax\\|_1}{\\|x\\|_1}$ 建立了一个上界：\n$$\n\\|Ax\\|_1 \\le C_{\\max} \\|x\\|_1 \\implies \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max} \\quad \\text{对所有 } x \\neq 0\n$$\n这意味着该比率的上确界也小于或等于 $C_{\\max}$：\n$$\n\\|A\\|_1 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le C_{\\max}\n$$\n为了完成推导，我们必须证明这个上界是可以达到的。也就是说，我们必须找到一个特定的非零向量 $x_0$，使得等式 $\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = C_{\\max}$ 成立。\n\n令 $k$ 为达到最大绝对值列和的列的索引，使得 $C_k = C_{\\max}$。考虑向量 $x_0 = e_k$，其中 $e_k$ 是 $\\mathbb{R}^n$ 中的第 $k$ 个标准基向量。这个向量在第 $k$ 个位置为 1，其他位置均为 0。\n该向量的 1-范数为 $\\|x_0\\|_1 = \\|e_k\\|_1 = 1$。\n乘积 $Ax_0 = Ae_k$ 是矩阵 $A$ 的第 $k$ 列。我们把这个列向量记为 $a_k$。因此，$Ax_0$ 的 1-范数为：\n$$\n\\|Ax_0\\|_1 = \\|a_k\\|_1 = \\sum_{i=1}^{m} |a_{ik}|\n$$\n根据我们对索引 $k$ 的定义，这个和恰好是 $C_k = C_{\\max}$。\n对于 $x_0 = e_k$ 这个特定选择，该比率变为：\n$$\n\\frac{\\|Ax_0\\|_1}{\\|x_0\\|_1} = \\frac{C_{\\max}}{1} = C_{\\max}\n$$\n既然我们找到了一个向量 $x_0$ 使得比率等于 $C_{\\max}$，并且我们也证明了该比率永远不会超过 $C_{\\max}$，我们可以得出结论，上确界必须恰好是 $C_{\\max}$。因此，诱导矩阵 1-范数的闭式表达式是最大绝对值列和：\n$$\n\\|A\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{m} |a_{ij}|\n$$\n\n接下来，我们对给定的矩阵计算这个表达式的值：\n$$\nA = \\begin{pmatrix}\n7.5  0.2  -0.1 \\\\\n8.3  -0.15  0.05 \\\\\n6.9  0.1  0.2 \\\\\n9.1  -0.05  -0.15 \\\\\n8.7  0.25  0.05\n\\end{pmatrix}\n$$\n这里，$m=5$ 且 $n=3$。我们计算绝对值列和 $C_1$、$C_2$ 和 $C_3$。\n对第一列（$j=1$）：\n$$\nC_1 = |7.5| + |8.3| + |6.9| + |9.1| + |8.7| = 7.5 + 8.3 + 6.9 + 9.1 + 8.7 = 40.5\n$$\n对第二列（$j=2$）：\n$$\nC_2 = |0.2| + |-0.15| + |0.1| + |-0.05| + |0.25| = 0.2 + 0.15 + 0.1 + 0.05 + 0.25 = 0.75\n$$\n对第三列（$j=3$）：\n$$\nC_3 = |-0.1| + |0.05| + |0.2| + |-0.15| + |0.05| = 0.1 + 0.05 + 0.2 + 0.15 + 0.05 = 0.55\n$$\n诱导矩阵 1-范数是这些和的最大值：\n$$\n\\|A\\|_{1} = \\max(C_1, C_2, C_3) = \\max(40.5, 0.75, 0.55) = 40.5\n$$\n\n最后，我们解释其与特征缩放的联系。推导出的公式 $\\|A\\|_1 = \\max_j \\sum_i |a_{ij}|$ 揭示了诱导 1-范数仅由具有最大绝对值和的列决定。在给定的背景下，$A$ 的每一列代表一个特征，其元素是该特征在不同样本上的值。和 $\\sum_i |a_{ij}|$可以被解释为数据集中第 $j$ 个特征的总量级或尺度的一种度量。\n\n诱导范数 $\\|A\\|_1$ 量化了在线性变换 $x \\mapsto Ax$ 下向量 1-范数的最大放大率。它代表了一个最坏情况下的敏感性度量：对向量 $x$ 的任何输入扰动 $\\delta x$ 将在输出中产生一个扰动 $\\delta y = A\\delta x$，其界限为 $\\|\\delta y\\|_1 \\le \\|A\\|_1 \\|\\delta x\\|_1$。\n\n给定的矩阵 $A$ 被构造成使其一个特征（第一列）的尺度远大于其他特征。如我们的计算所示，这种不平衡的缩放导致第一列的绝对值和（$C_1=40.5$）完全主导了其他列的和（$C_2=0.75$, $C_3=0.55$）。因此，整体范数 $\\|A\\|_1$ 完全由这个单一的大尺度特征决定。这意味着任何对此范数敏感的优化算法（例如某些坐标级方法）的稳定性和行为将由尺度最差的特征所决定。与该特征对齐的扰动（即，集中在输入向量 $x$ 的第一个分量中的扰动）被放大了 40.5 倍，而沿着其他特征方向的扰动将被放大得小得多。如此大的、不平衡的放大因子会导致数值方法出现病态条件和不稳定性，这就是为什么特征缩放（例如，将列标准化以具有相似的尺度）是许多优化和机器学习算法的关键预处理步骤。\n\n$\\|A\\|_1$ 的精确数值是 $40.5$。", "answer": "$$\n\\boxed{40.5}\n$$", "id": "3148401"}, {"introduction": "在掌握了单一范数的计算后，我们将探讨一个更深刻的问题：不同范数的选择如何影响我们对系统特性的判断。本练习将通过构造一个特殊的矩阵，揭示一个系统在 $1$-范数下表现良好，但在 $\\infty$-范数下却可能极其敏感 [@problem_id:3148445]。这突显了在进行误差分析和评估线性系统条件数 $\\kappa(A)$ 时，谨慎选择度量标准的重要性。", "problem": "考虑一个维度为 $n \\geq 2$ 的方阵。定义矩阵 $S \\in \\mathbb{R}^{n \\times n}$，其元素为：当 $j = 2, 3, \\dots, n$ 时，$s_{1j} = 1$；其他情况下 $s_{ij} = 0$。构造矩阵 $A = I - S$，其中 $I$ 是单位矩阵。使用这种显式构造来分析在线性系统 $A x = b$ 的数据向量 $b$ 存在扰动时，求解该系统的敏感性。\n\n任务：\n- 仅使用矩阵和幂零算子的基本性质，证明 $A$ 是可逆的，并求出 $A^{-1}$。\n- 计算此 $A$ 的 $\\|A^{-1}\\|_{1}$ 和 $\\|A^{-1}\\|_{\\infty}$。\n- 计算 $\\|A\\|_{1}$ 和 $\\|A\\|_{\\infty}$，然后计算相应的条件数 $\\kappa_{1}(A)$ 和 $\\kappa_{\\infty}(A)$，其中 $\\kappa_{p}(A) = \\|A\\|_{p} \\|A^{-1}\\|_{p}$ 是针对一个诱导矩阵范数。\n- 对于扰动 $b \\mapsto b + \\delta b$，仅使用诱导范数的定义和次可乘性，推导当 $p \\in \\{1, \\infty\\}$ 时相对解误差 $\\|x - \\hat{x}\\|_{p} / \\|x\\|_{p}$ 的最坏情况上界。这里，$x$ 是 $A x = b$ 的精确解，$\\hat{x}$ 是 $A \\hat{x} = b + \\delta b$ 的解。\n- 在附加假设下，即两种范数下的相对数据误差相同，即 $\\|\\delta b\\|_{1} / \\|b\\|_{1} = \\|\\delta b\\|_{\\infty} / \\|b\\|_{\\infty}$，确定两种最坏情况相对误差界限（$\\infty$-范数与$1$-范数）之比，作为 $n$ 的函数。\n\n以关于 $n$ 的单个解析表达式的形式提供您的最终答案。", "solution": "该问题陈述经评估具有科学依据、问题适定、客观且完整。它代表了数值线性代数中的一个标准练习。该问题是有效的，将提供一个解答。\n\n该问题要求对一个特定矩阵 $A = I - S$ 进行多步分析。让我们逐一完成每个任务。\n\n首先，我们定义当 $n \\ge 2$ 时矩阵 $S \\in \\mathbb{R}^{n \\times n}$。其元素由 $s_{1j} = 1$（对于 $j \\in \\{2, 3, \\dots, n\\}$）和 $s_{ij} = 0$（对于所有其他点对 $(i, j)$）给出。这意味着 $S$ 的唯一非零项位于第一行，从第二列开始。\n矩阵 $S$ 的形式如下：\n$$S = \\begin{pmatrix}\n0  1  1  \\dots  1 \\\\\n0  0  0  \\dots  0 \\\\\n0  0  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  0\n\\end{pmatrix}$$\n\n**任务 1：证明 $A$ 是可逆的，并求出 $A^{-1}$。**\n\n矩阵 $A$ 定义为 $A = I - S$。为了证明其可逆性并求出其逆矩阵，我们首先分析 $S$ 的性质。让我们计算 $S^2$。$S^2$ 的元素 $(i, k)$ 由 $(S^2)_{ik} = \\sum_{j=1}^{n} s_{ij} s_{jk}$ 给出。\n- 如果 $i  1$，则对所有 $j$ 都有 $s_{ij} = 0$，所以 $(S^2)_{ik} = 0$。\n- 如果 $i = 1$，则 $(S^2)_{1k} = \\sum_{j=1}^{n} s_{1j} s_{jk}$。唯一非零的 $s_{1j}$ 项是当 $j \\in \\{2, 3, \\dots, n\\}$ 时。因此，求和变为 $(S^2)_{1k} = \\sum_{j=2}^{n} s_{1j} s_{jk} = \\sum_{j=2}^{n} (1) s_{jk}$。\n要使 $s_{jk}$ 非零，必须有 $j=1$。然而，求和的范围是从 $j=2$ 到 $n$。因此，对于求和中的每个 $j$，$s_{jk} = 0$。\n因此，对于所有 $k$，$(S^2)_{1k} = \\sum_{j=2}^{n} 0 = 0$。\n由于 $S^2$ 的所有元素都为零，所以 $S^2 = O$，其中 $O$ 是 $n \\times n$ 的零矩阵。这表明 $S$ 是一个2阶幂零矩阵。\n\n矩阵 $A$ 由 $A = I - S$ 给出。因为 $S$ 是幂零的，所以 $A$ 的逆的几何级数（诺依曼级数）是有限的：\n$$(I-S)^{-1} = I + S + S^2 + S^3 + \\dots$$\n由于对于所有 $k \\ge 2$ 都有 $S^k = O$，该级数终止。\n$$A^{-1} = (I - S)^{-1} = I + S$$\n逆矩阵 $A^{-1}$ 的存在证明了 $A$ 是可逆的。\n$A$ 和 $A^{-1}$ 的显式形式为：\n$$A = I - S = \\begin{pmatrix}\n1  -1  -1  \\dots  -1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n$$A^{-1} = I + S = \\begin{pmatrix}\n1  1  1  \\dots  1 \\\\\n0  1  0  \\dots  0 \\\\\n0  0  1  \\dots  0 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  0  \\dots  1\n\\end{pmatrix}$$\n\n**任务 2：计算 $\\|A^{-1}\\|_{1}$ 和 $\\|A^{-1}\\|_{\\infty}$。**\n\n矩阵的诱导 $1$-范数是最大绝对列和，$\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |m_{ij}|$。\n对于 $A^{-1} = I + S$：\n- 第一列（$j=1$）的绝对值之和为 $|1| = 1$。\n- 对于任何其他列 $j \\in \\{2, \\dots, n\\}$，非零项是 $(A^{-1})_{1j}=1$ 和 $(A^{-1})_{jj}=1$。绝对值之和为 $|1| + |1| = 2$。\n因此，$\\|A^{-1}\\|_1 = \\max(1, 2, \\dots, 2) = 2$。\n\n矩阵的诱导 $\\infty$-范数是最大绝对行和，$\\|M\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^{n} |m_{ij}|$。\n对于 $A^{-1} = I + S$：\n- 第一行（$i=1$）有 $n$ 个等于 $1$ 的项。绝对值之和为 $\\sum_{j=1}^{n} |1| = n$。\n- 对于任何其他行 $i \\in \\{2, \\dots, n\\}$，唯一的非零项是 $(A^{-1})_{ii}=1$。绝对值之和为 $|1| = 1$。\n因此，$\\|A^{-1}\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$。\n\n**任务 3：计算 $\\|A\\|_{1}$、$\\|A\\|_{\\infty}$ 和条件数。**\n\n我们首先计算 $A = I - S$ 的范数。\n对于 $A$ 的 $1$-范数：\n- 第一列（$j=1$）的绝对值之和为 $|1| = 1$。\n- 对于任何其他列 $j \\in \\{2, \\dots, n\\}$，非零项是 $a_{1j}=-1$ 和 $a_{jj}=1$。绝对值之和为 $|-1| + |1| = 2$。\n因此，$\\|A\\|_1 = \\max(1, 2, \\dots, 2) = 2$。\n\n对于 $A$ 的 $\\infty$-范数：\n- 第一行（$i=1$）的项为 $1, -1, \\dots, -1$。绝对值之和为 $|1| + \\sum_{j=2}^{n} |-1| = 1 + (n-1) = n$。\n- 对于任何其他行 $i \\in \\{2, \\dots, n\\}$，唯一的非零项是 $a_{ii}=1$。绝对值之和为 $|1|=1$。\n因此，$\\|A\\|_{\\infty} = \\max(n, 1, \\dots, 1) = n$。\n\n条件数 $\\kappa_p(A)$ 定义为 $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$。\n- 对于 $p=1$：$\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1 = 2 \\cdot 2 = 4$。\n- 对于 $p=\\infty$：$\\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = n \\cdot n = n^2$。\n\n**任务 4：推导相对解误差的最坏情况上界。**\n\n设 $x$ 是 $Ax=b$ 的解，$\\hat{x}$ 是扰动系统 $A\\hat{x} = b + \\delta b$ 的解。\n解的误差为 $\\delta x = \\hat{x} - x$。\n用第二个方程减去第一个方程得到：$A\\hat{x} - Ax = (b + \\delta b) - b \\implies A(\\hat{x} - x) = \\delta b \\implies A \\delta x = \\delta b$。\n由于 $A$ 是可逆的，我们可以写成 $\\delta x = A^{-1} \\delta b$。\n对两边取 $p$-范数，并使用诱导范数的性质 $\\|Mv\\|_p \\le \\|M\\|_p \\|v\\|_p$，我们得到：\n$\\|\\delta x\\|_p = \\|A^{-1} \\delta b\\|_p \\le \\|A^{-1}\\|_p \\|\\delta b\\|_p$。\n\n为了找到相对误差 $\\|\\delta x\\|_p / \\|x\\|_p$，我们需要一个涉及 $\\|x\\|_p$ 的界。从原始方程 $Ax=b$ 出发，我们有 $\\|b\\|_p = \\|Ax\\|_p \\le \\|A\\|_p \\|x\\|_p$。假设 $b \\ne 0$（因此 $x \\ne 0$），我们可以写成 $1/\\|x\\|_p \\le \\|A\\|_p/\\|b\\|_p$。\n\n结合这两个不等式：\n$$\\frac{\\|\\delta x\\|_p}{\\|x\\|_p} \\le \\frac{\\|A^{-1}\\|_p \\|\\delta b\\|_p}{\\|x\\|_p} \\le \\|A^{-1}\\|_p \\|\\delta b\\|_p \\left( \\frac{\\|A\\|_p}{\\|b\\|_p} \\right)$$\n重新整理各项，我们得到标准的最坏情况相对误差界：\n$$\\frac{\\|\\hat{x} - x\\|_p}{\\|x\\|_p} \\le \\|A\\|_p \\|A^{-1}\\|_p \\frac{\\|\\delta b\\|_p}{\\|b\\|_p} = \\kappa_p(A) \\frac{\\|\\delta b\\|_p}{\\|b\\|_p}$$\n这个界被认为是“最坏情况”，因为对于任何矩阵 $A$，都存在向量 $b$ 和 $\\delta b$ 使得该不等式成为等式。\n\n**任务 5：确定两个最坏情况相对误差界之比。**\n\n在 $\\infty$-范数下，相对误差的最坏情况上界是：\n$$E_{\\infty} = \\kappa_{\\infty}(A) \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}} = n^2 \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}}$$\n在 $1$-范数下，相对误差的最坏情况上界是：\n$$E_{1} = \\kappa_{1}(A) \\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}} = 4 \\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}}$$\n我们被给予一个附加假设，即两种范数下的相对数据误差是相同的：\n$$\\frac{\\|\\delta b\\|_{1}}{\\|b\\|_{1}} = \\frac{\\|\\delta b\\|_{\\infty}}{\\|b\\|_{\\infty}}$$\n设这个共同的值为 $\\epsilon$。误差界变为 $E_{\\infty} = n^2 \\epsilon$ 和 $E_1 = 4 \\epsilon$。\n问题要求两个最坏情况相对误差界之比，即 $\\infty$-范数与 $1$-范数之比。这个比值为：\n$$\\frac{E_{\\infty}}{E_{1}} = \\frac{n^2 \\epsilon}{4 \\epsilon} = \\frac{n^2}{4}$$\n这个结果对 $n \\ge 2$ 有效。", "answer": "$$\\boxed{\\frac{n^2}{4}}$$", "id": "3148445"}, {"introduction": "最后一个练习将我们带入现代优化的前沿，探讨范数在高维空间中一些违反直觉的特性。我们将构造一个在 $2$-范数下“小”但在 $\\infty$-范数下“大”的矩阵，并观察这种差异如何极大地影响梯度裁剪等算法的性能 [@problem_id:3148424]。这个例子清晰地表明，在机器学习等领域，为特定任务选择合适的范数是一个具有重要实践意义的决策。", "problem": "考虑诱导矩阵范数 $\\|A\\|_{p} = \\sup_{\\|x\\|_{p} = 1} \\|A x\\|_{p}$ (其中 $p \\in \\{2, \\infty\\}$) 以及梯度裁剪的梯度下降法，其定义如下：给定一个阈值 $\\tau  0$ 和一个范数索引 $p$，如果梯度向量 $g$ 满足 $\\|g\\|_{p}  \\tau$，则将其缩放为一个范数为 $\\|\\tilde{g}_{p}\\|_{p} = \\tau$ 的共线向量 $\\tilde{g}_{p}$；否则 $\\tilde{g}_{p} = g$。您将分析在一个凸二次目标函数中，不同的诱导范数如何与梯度裁剪相互作用。\n\n定义矩阵 $A \\in \\mathbb{R}^{100 \\times 100}$，使其仅第一行为非零。设该第一行为 $r^{\\top}$，其中 $r \\in \\mathbb{R}^{100}$ 的分量为 $r_{i} = 0.1$ (对所有 $i \\in \\{1, \\dots, 100\\}$)，且 $A$ 的其余各行均为零。考虑凸二次函数 $f(x) = \\frac{1}{2} x^{\\top} A^{\\top} A x$ 和起始点 $x_{0} = r$。设步长为 $\\alpha = 1$，裁剪阈值为 $\\tau = 0.2$。\n\n任务：\n- 基于诱导矩阵范数的定义，验证矩阵 $A$ 满足 $\\|A\\|_{2}$ 小而 $\\|A\\|_{\\infty}$ 大。\n- 使用上述裁剪规则，分别在 $p = \\infty$ 和 $p = 2$ 的情况下，计算从 $x_{0}$ 开始的一步梯度下降，并简要解释这两种裁剪选择如何影响最小化 $f$ 的单步进展。\n- 最后，计算 $f(x_{1}^{(2)})$ 的精确数值，其中 $x_{1}^{(2)}$ 表示如前述在 $2$-范数下进行裁剪后，经过一步梯度下降得到的点。最终答案以一个无单位的数字形式给出。无需四舍五入。", "solution": "用户提供了一个优化方法领域的问题。我将首先根据指定标准验证问题陈述，如果问题被认为是有效的，则继续提供完整解答。\n\n### 步骤 1: 提取已知条件\n- 诱导矩阵范数定义：$\\|A\\|_{p} = \\sup_{\\|x\\|_{p} = 1} \\|A x\\|_{p}$，其中 $p \\in \\{2, \\infty\\}$。\n- 梯度裁剪规则：给定一个阈值 $\\tau  0$ 和一个范数索引 $p$，如果梯度向量 $g$ 满足 $\\|g\\|_{p}  \\tau$，则将其缩放为 $\\tilde{g}_{p} = \\frac{\\tau}{\\|g\\|_{p}} g$。否则，$\\tilde{g}_{p} = g$。\n- 矩阵 $A \\in \\mathbb{R}^{100 \\times 100}$ 的第一行为 $r^{\\top}$，其余各行均为零。\n- 向量 $r \\in \\mathbb{R}^{100}$ 的分量为 $r_{i} = 0.1$ (对所有 $i \\in \\{1, \\dots, 100\\}$)。\n- 目标函数为 $f(x) = \\frac{1}{2} x^{\\top} A^{\\top} A x$。\n- 起始点为 $x_{0} = r$。\n- 梯度下降步长为 $\\alpha = 1$。\n- 裁剪阈值为 $\\tau = 0.2$。\n\n### 步骤 2: 使用提取的已知条件进行验证\n该问题在线性代数和数值优化的数学领域内是良定义的。\n- **科学上成立：** 诱导矩阵范数、凸二次函数以及带裁剪的梯度下降等概念是优化和机器学习中的标准和基础概念。该问题构成了一个标准的数学练习。\n- **良构性：** 所有必要的定义、常数和初始条件都已提供，足以计算任务各部分的唯一解。函数 $f(x)$ 是凸函数，因为其海森矩阵 $A^{\\top}A$ 是半正定的，这是分析梯度下降的标准设定。\n- **客观性：** 问题以精确、客观的数学语言陈述。\n- **缺陷分析：**\n    - 该问题不违反任何科学或数学原理。\n    - 这是一个可形式化的问题，与所述主题直接相关。\n    - 设置是完整的且内部一致。\n    - 所有条件都是数学上的，不涉及物理现实，因此并非不可行。\n    - 问题结构良好，能够为所需的计算导出唯一的、有意义的解。\n    - 问题并非无足轻重；它需要仔细应用定义来阐释高维空间中 $L_2$ 和 $L_\\infty$ 范数之间的一个关键区别。\n    - 所有结果都可以通过数学方法验证。\n\n### 步骤 3: 结论与行动\n问题有效。将提供详细解答。\n\n### 解答\n\n该问题要求针对一个特定的二次目标函数，分析在两种不同范数下的梯度裁剪。我们将依次处理每个任务。\n\n首先，我们来刻画给定的矩阵 $A$ 和向量 $r$。向量 $r \\in \\mathbb{R}^{100}$ 由其分量 $r_i = 0.1$ ($i=1, \\dots, 100$) 定义。矩阵 $A \\in \\mathbb{R}^{100 \\times 100}$ 的第一行等于 $r^{\\top}$，其余各行均为零。这可以紧凑地写为 $A = e_1 r^{\\top}$，其中 $e_1 = (1, 0, \\dots, 0)^{\\top}$ 是 $\\mathbb{R}^{100}$ 中的第一个标准基向量。\n\n**任务1：验证 $\\|A\\|_{2}$ 小而 $\\|A\\|_{\\infty}$ 大。**\n\n诱导$\\infty$-范数 $\\|A\\|_{\\infty}$ 定义为最大绝对行和。\n$$ \\|A\\|_{\\infty} = \\max_{i} \\sum_{j=1}^{100} |A_{ij}| $$\n对于矩阵 $A$，第一行为 $r^{\\top}$，因此其元素绝对值之和为 $\\sum_{j=1}^{100} |r_j| = \\sum_{j=1}^{100} 0.1 = 100 \\times 0.1 = 10$。$A$ 的其余各行均为零，因此它们的绝对行和为 $0$。这些值中的最大值为 $10$。\n$$ \\|A\\|_{\\infty} = 10 $$\n诱导$2$-范数 $\\|A\\|_{2}$ 是 $A$ 的最大奇异值，也就是矩阵 $A^{\\top}A$ 最大特征值的平方根。\n$$ A^{\\top}A = (e_1 r^{\\top})^{\\top} (e_1 r^{\\top}) = (r e_1^{\\top}) (e_1 r^{\\top}) = r (e_1^{\\top} e_1) r^{\\top} $$\n由于 $e_1^{\\top}e_1 = \\|e_1\\|_2^2 = 1$，我们有：\n$$ A^{\\top}A = r r^{\\top} $$\n这是一个秩为1的矩阵。矩阵 $uv^{\\top}$ 的特征值是 $v^{\\top}u$ (对应特征向量 $u$) 和 $0$ (重数为 $n-1$)。在这里，$u=v=r$。唯一的非零特征值是 $\\lambda = r^{\\top}r$。\n$$ \\lambda = r^{\\top}r = \\sum_{i=1}^{100} r_i^2 = \\sum_{i=1}^{100} (0.1)^2 = 100 \\times 0.01 = 1 $$\n$A^{\\top}A$ 的特征值是 $\\{1, 0, \\dots, 0\\}$。最大特征值是 $\\lambda_{\\max} = 1$。因此，诱导$2$-范数为：\n$$ \\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)} = \\sqrt{1} = 1 $$\n比较这两个范数，$\\|A\\|_{\\infty} = 10$ 确实远大于 $\\|A\\|_{2} = 1$。这验证了前提。\n\n**任务2：计算一步梯度下降并解释裁剪的效果。**\n\n目标函数为 $f(x) = \\frac{1}{2} x^{\\top} A^{\\top} A x$。其梯度为 $\\nabla f(x) = A^{\\top} A x$。我们在起始点 $x_0 = r$ 计算梯度 $g$。\n$$ g = \\nabla f(x_0) = A^{\\top} A x_0 = (r r^{\\top}) r = r(r^{\\top}r) $$\n由于我们已求得 $r^{\\top}r = 1$，梯度即为：\n$$ g = r $$\n梯度下降的更新规则是 $x_1 = x_0 - \\alpha \\tilde{g}$，其中 $\\tilde{g}$ 是裁剪后的梯度且 $\\alpha=1$。此处，$x_1 = x_0 - \\tilde{g} = r - \\tilde{g}$。我们分析在阈值 $\\tau=0.2$ 下，对于 $p=\\infty$ 和 $p=2$ 的裁剪情况。\n\n情况 $p=\\infty$：我们计算梯度 $g=r$ 的 $\\infty$-范数。\n$$ \\|g\\|_{\\infty} = \\|r\\|_{\\infty} = \\max_{i} |r_i| = 0.1 $$\n由于 $\\|g\\|_{\\infty} = 0.1 \\le \\tau = 0.2$，不发生裁剪。因此，$\\tilde{g}_{\\infty} = g = r$。更新后的点是：\n$$ x_{1}^{(\\infty)} = x_0 - \\tilde{g}_{\\infty} = r - r = 0 $$\n凸函数 $f(x) = \\frac{1}{2} x^{\\top} (A^{\\top}A) x$ 的最小值点在 $x=0$ 处。使用 $\\infty$-范数裁剪，梯度下降步骤在单次迭代中直接移动到全局最小值。这是因为 $\\infty$-范数很小，反映了梯度的没有任何单个分量是大的事实。\n\n情况 $p=2$：我们计算梯度 $g=r$ 的 $2$-范数。\n$$ \\|g\\|_{2} = \\|r\\|_{2} = \\sqrt{r^{\\top}r} = \\sqrt{1} = 1 $$\n由于 $\\|g\\|_{2} = 1  \\tau = 0.2$，应用了裁剪。裁剪后的梯度为：\n$$ \\tilde{g}_{2} = \\frac{\\tau}{\\|g\\|_{2}} g = \\frac{0.2}{1} r = 0.2 r $$\n更新后的点是：\n$$ x_{1}^{(2)} = x_0 - \\tilde{g}_{2} = r - 0.2 r = (1 - 0.2) r = 0.8 r $$\n使用 $2$-范数裁剪，梯度的大小从 $1$ 缩小到 $0.2$，导致朝向最小值的步长小得多。$2$-范数对所有分量的聚合大小很敏感，尽管每个分量都很小，但在这种高维情况下，这个聚合值很大。这种敏感性导致步长急剧减小，与未裁剪的步骤或使用 $\\infty$-范数裁剪的步骤相比，收敛速度减慢。\n\n**任务3：计算 $f(x_{1}^{(2)})$ 的精确数值。**\n\n我们需要在点 $x_{1}^{(2)} = 0.8 r$ 处求函数 $f$ 的值。\n$$ f(x_{1}^{(2)}) = \\frac{1}{2} (x_{1}^{(2)})^{\\top} A^{\\top}A x_{1}^{(2)} $$\n代入 $x_{1}^{(2)} = 0.8 r$ 和 $A^{\\top}A = r r^{\\top}$：\n$$ f(0.8r) = \\frac{1}{2} (0.8r)^{\\top} (rr^{\\top}) (0.8r) $$\n$$ f(0.8r) = \\frac{1}{2} (0.8)^2 r^{\\top} r r^{\\top} r $$\n利用 $r^{\\top}r = 1$ 这一事实：\n$$ f(0.8r) = \\frac{1}{2} (0.8)^2 (1) (1) = \\frac{1}{2} (0.64) = 0.32 $$\n使用 $2$-范数裁剪一步后目标函数的值是 $0.32$。作为对比，起始点的值是 $f(x_0)=f(r)=\\frac{1}{2}r^{\\top}(rr^{\\top})r = \\frac{1}{2}(r^{\\top}r)^2 = \\frac{1}{2}(1)^2=0.5$。使用 $\\infty$-范数裁剪找到的点的函数值是 $f(x_1^{(\\infty)}) = f(0) = 0$。这证实了使用 $2$-范数裁剪时进展更慢。", "answer": "$$ \\boxed{0.32} $$", "id": "3148424"}]}