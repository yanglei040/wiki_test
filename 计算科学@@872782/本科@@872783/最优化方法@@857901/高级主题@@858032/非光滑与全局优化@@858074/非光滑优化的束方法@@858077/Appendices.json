{"hands_on_practices": [{"introduction": "束方法的“智能”在于它如何整合历史信息来选择一个有希望的搜索方向，其核心部件是聚合次梯度（aggregate subgradient），它为下降方向提供了一个理论上可靠的“最佳”估计。本练习将让你对比这个精巧的预测器与一个更朴素的方法——简单地平均最近的次梯度——看看哪一个能更好地预测函数的实际行为。通过这种动手比较，你将对聚合策略的强大功能和设计意图有更具体的感受。[@problem_id:3105153]", "problem": "给定一个由线性函数的逐点最大值定义的非光滑目标函数。设 $A \\in \\mathbb{R}^{m \\times n}$ 的行向量为 $\\{a_i^\\top\\}_{i=1}^m$，并定义函数 $f : \\mathbb{R}^n \\to \\mathbb{R}$ 为 $f(x) = \\max_{1 \\le i \\le m} a_i^\\top x$。函数 $f$ 在点 $x$ 处的次微分 $\\partial f(x)$ 是激活行集合 $\\{a_i : i \\in I(x)\\}$ 的凸包，其中 $I(x)$ 是在点 $x$ 处达到最大值的索引集合。束方法中使用的聚合次梯度 $s_k$ 取为 $\\partial f(x_k)$ 中欧几里得范数最小的元素，即在 $x_k$ 处的激活行的凸组合中，在 $\\mathbb{R}^n$ 中范数最小的那个。形式上，如果 $I(x_k) = \\{i_1,\\dots,i_p\\}$，则 $s_k$ 通过在满足 $\\alpha_j \\ge 0$ 和 $\\sum_{j=1}^p \\alpha_j = 1$ 的条件下，最小化欧几里得范数的平方 $\\|s\\|_2^2$ 来获得，其中 $s = \\sum_{j=1}^p \\alpha_j a_{i_j}$。另外，定义一个平均近期窗口预测子 $\\bar{g}$，作为在近期迭代点 $\\{x_j\\}$ 计算出的近期次梯度 $g_j$ 的算术平均值，其中每个 $g_j$ 是 $x_j$ 处的任意有效次梯度（在本练习中，为确保确定性，取 $g_j$ 为 $x_j$ 处的一个激活行 $a_i$，选择 $I(x_j)$ 中最小的索引）。\n\n你的任务是实现一个程序，对每个指定的测试用例，在当前迭代点 $x_k$ 执行以下步骤：\n1. 计算聚合次梯度 $s_k$，其为 $x_k$ 处激活行的最小欧几里得范数凸组合（如果 $I(x_k)$ 是单元素集，则 $s_k$ 就是那个唯一的向量）。\n2. 使用上述确定性选择规则，从提供的近期迭代点计算平均近期窗口次梯度 $\\bar{g}$。\n3. 对每个预测子 $g_{\\text{hat}} \\in \\{s_k, \\bar{g}\\}$，构建最速下降方向 $d = - g_{\\text{hat}}$，沿该方向前进一小步，步长为 $t$（如下所给），并计算：\n   - 实际下降量 $\\Delta f = f(x_k) - f(x_k + t d)$，其中 $f$ 通过其最大值定义精确计算。\n   - 线性模型预测的下降量 $v_{\\text{hat}} = t \\, \\|g_{\\text{hat}}\\|_2^2$，该值来自使用 $g_{\\text{hat}}$ 作为 $x_k$ 处次梯度的预测子对 $f$ 进行的局部线性化。\n4. 定义对齐误差 $e(g_{\\text{hat}}) = |\\Delta f - v_{\\text{hat}}|$，并声明，如果 $e(s_k)  e(\\bar{g})$，则聚合次梯度对齐更优；否则，平均近期窗口预测子对齐更优或两者相当。\n\n你的程序必须为下面的一组测试用例输出单行结果，该行包含一个布尔值列表，指明对于每个用例，聚合次梯度 $s_k$ 是否比 $\\bar{g}$ 对齐更优。\n\n此问题不涉及任何物理单位。所有角度（若有）均默认为弧度，但此问题不使用角度。所有分数和小数已按其形式指定；请勿使用百分号。\n\n使用以下测试套件。在每个用例中，步长均使用 $t = 0.01$。对于每个测试用例，矩阵 $A$ 由其行向量给出，$x_k$ 是当前迭代点，近期窗口是用于计算近期次梯度 $g_j$ 并将其平均以形成 $\\bar{g}$ 的点列表。在近期点 $x_j$ 选择次梯度时，使用 $I(x_j)$ 中的最小索引来确定性地选择 $a_i$。\n\n- 测试用例 1 (在 $x_k$ 处有两个激活行):\n  - $A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.7  0.6 \\\\ 1.0  -0.5 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$,\n  - 近期窗口点: $\\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.8 \\\\ 0.2 \\end{bmatrix}, \\begin{bmatrix} 1.2 \\\\ -0.1 \\end{bmatrix} \\right\\}$。\n\n- 测试用例 2 (在 $x_k$ 处有唯一的激活行，包含负项):\n  - $A = \\begin{bmatrix} -1.0  0.0 \\\\ 0.0  -1.0 \\\\ -0.5  -0.2 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} -1.0 \\\\ -2.0 \\end{bmatrix}$,\n  - 近期窗口点: $\\left\\{ \\begin{bmatrix} -1.0 \\\\ -2.0 \\end{bmatrix}, \\begin{bmatrix} -0.8 \\\\ -2.2 \\end{bmatrix}, \\begin{bmatrix} -1.2 \\\\ -1.8 \\end{bmatrix}, \\begin{bmatrix} -1.1 \\\\ -2.5 \\end{bmatrix} \\right\\}$。\n\n- 测试用例 3 (窗口大小为一):\n  - $A = \\begin{bmatrix} 0.0  1.0 \\\\ 1.0  0.1 \\\\ 0.6  0.7 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} 0.2 \\\\ 0.5 \\end{bmatrix}$,\n  - 近期窗口点: $\\left\\{ \\begin{bmatrix} 0.1 \\\\ 0.4 \\end{bmatrix} \\right\\}$。\n\n- 测试用例 4 (三维空间，存在重复项导致的平局):\n  - $A = \\begin{bmatrix} 1.0  -0.5  0.2 \\\\ 0.8  0.9  -0.3 \\\\ 0.6  0.0  1.2 \\\\ 1.0  -0.5  0.2 \\end{bmatrix}$,\n  - $x_k = \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}$,\n  - 近期窗口点: $\\left\\{ \\begin{bmatrix} 0.2 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}, \\begin{bmatrix} 0.6 \\\\ -0.5 \\\\ 0.8 \\end{bmatrix} \\right\\}$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，\"[result1,result2,result3,result4]\"），每个结果是一个布尔值，表示对于相应的测试用例，$e(s_k)  e(\\bar{g})$ 是否成立。当需要选择单个次梯度且出现平局时，通过选择 $I(x)$ 中的最小索引来确保确定性选择。\n\n注意：聚合次梯度的计算需要解决一个凸二次规划（QP）问题，即在概率单纯形上最小化一个凸二次目标。你必须实现一个数值稳定且确定性的过程，为所有给定的测试用例生成有效解。", "solution": "所呈现的问题是非光滑优化领域一个有效的计算任务。它要求实现并比较一个定义为仿射函数逐点最大值的函数的两种不同次梯度预测子。该问题具有科学依据，是良定的，并且所有术语和数据都已明确无误地提供。我将逐步进行求解。\n\n问题的核心是比较两种次梯度估计在一个点 $x_k$ 对函数 $f(x) = \\max_{1 \\le i \\le m} a_i^\\top x$ 的局部预测能力。这两种预测子是聚合次梯度 $s_k$ 和一个平均近期窗口次梯度 $\\bar{g}$。比较基于对齐误差，该误差衡量实际函数下降量与基于次梯度预测子的线性模型所预测的下降量之间的差异。较小的对齐误差表明预测子更优。\n\n我们来概述一下对于一个给定的测试用例（包含矩阵 $A$、当前迭代点 $x_k$ 和一组近期窗口点）所需的计算步骤。下降步的步长给定为 $t = 0.01$。\n\n首先，我们定义两个辅助函数：一个用于评估 $f(x)$，另一个用于识别在点 $x$ 处的激活次梯度集合。\n函数 $f(x)$ 计算为 $f(x) = \\max_{i} (A x)_i$。\n在点 $x$ 的激活索引集合，记为 $I(x)$，是 $I(x) = \\{i \\mid a_i^\\top x = f(x)\\}$。由于浮点运算，我们将激活索引识别为那些满足 $f(x) - a_i^\\top x \\le \\epsilon$ 的索引，其中 $\\epsilon$ 是一个小的容差（例如，$\\epsilon=10^{-9}$）。相应的激活次梯度是行向量 $\\{a_i\\}_{i \\in I(x)}$。\n\n**步骤 1：计算聚合次梯度 $s_k$**\n\n聚合次梯度 $s_k$ 是次微分 $\\partial f(x_k) = \\text{conv}\\{a_i \\mid i \\in I(x_k)\\}$ 中欧几里得范数最小的元素。这等价于将原点投影到 $x_k$ 处激活次梯度的凸包上。\n\n设 $x_k$ 处的激活次梯度集合为 $G_{act} = \\{g_1, g_2, \\dots, g_p\\}$，其中每个 $g_j$ 是来自 $A$ 的行向量 $a_i^\\top$，对应某个 $i \\in I(x_k)$。我们需要找到 $s_k = \\sum_{j=1}^p \\alpha_j g_j$，使得 $\\|s_k\\|_2^2$ 最小，且满足约束条件 $\\sum_{j=1}^p \\alpha_j = 1$ 和对所有 $j$ 都有 $\\alpha_j \\ge 0$。\n\n这是一个凸二次规划（QP）问题。设 $G$ 是以向量 $g_j \\in G_{act}$ 为列的矩阵。我们寻求找到向量 $\\alpha = [\\alpha_1, \\dots, \\alpha_p]^\\top$ 来求解：\n$$\n\\min_{\\alpha} \\frac{1}{2} \\|G \\alpha\\|_2^2 = \\frac{1}{2} \\alpha^\\top (G^\\top G) \\alpha\n$$\n约束条件为：\n$$\n\\sum_{j=1}^p \\alpha_j = 1, \\quad \\alpha_j \\ge 0, \\quad j=1, \\dots, p\n$$\n这个标准的 QP 问题可以使用数值优化库来解决，例如 `scipy.optimize.minimize` 的 'SLSQP' 方法。目标函数是二次的，约束是线性的。\n\n如果 $|I(x_k)|=1$，意味着只有一个激活次梯度 $a_i$，其凸包是一个单点，因此 $s_k = a_i$。\n\n**步骤 2：计算平均近期窗口次梯度 $\\bar{g}$**\n\n预测子 $\\bar{g}$ 是在一系列“近期窗口”中提供的近期点上计算的次梯度的算术平均值。对于窗口中的每个点 $x_j$，我们必须首先确定一个单一的次梯度 $g_j \\in \\partial f(x_j)$。问题指定了一个确定性规则：识别激活索引集合 $I(x_j)$，并选择与 $I(x_j)$ 中最小索引 $i$ 对应的次梯度 $a_i$。\n为窗口中的每个 $x_j$ 计算出 $g_j$ 后，$\\bar{g}$ 计算如下：\n$$\n\\bar{g} = \\frac{1}{|W|} \\sum_{x_j \\in W} g_j\n$$\n其中 $W$ 是近期窗口点的集合。\n\n**步骤 3：计算对齐误差**\n\n对每个预测子 $g_{\\text{hat}} \\in \\{s_k, \\bar{g}\\}$，我们评估其质量。质量通过比较实际函数下降量与从函数线性模型导出的预测下降量来衡量。\n\n下降方向取为 $d = -g_{\\text{hat}}$。从 $x_k$ 沿该方向前进一个步长为 $t$ 的步，到达新点 $x_{\\text{new}} = x_k + t d$。\n\n实际函数下降量为：\n$$\n\\Delta f = f(x_k) - f(x_{\\text{new}}) = f(x_k) - f(x_k - t g_{\\text{hat}})\n$$\n\n基于一阶近似 $f(x_k+d) \\approx f(x_k) + g_{\\text{hat}}^\\top d$，预测的下降量为：\n$$\nv_{\\text{hat}} = -g_{\\text{hat}}^\\top d = -g_{\\text{hat}}^\\top (-t g_{\\text{hat}}) = t \\, \\|g_{\\text{hat}}\\|_2^2\n$$\n\n预测子 $g_{\\text{hat}}$ 的对齐误差定义为实际下降量和预测下降量之间的绝对差值：\n$$\ne(g_{\\text{hat}}) = |\\Delta f - v_{\\text{hat}}|\n$$\n我们为 $s_k$ 和 $\\bar{g}$ 分别计算此误差，得到 $e(s_k)$ 和 $e(\\bar{g})$。\n\n**步骤 4：比较预测子**\n\n最后一步是比较这两个误差。如果聚合次梯度 $s_k$ 的误差严格小于平均预测子 $\\bar{g}$ 的误差，则认为前者对齐更优。即，如果 $e(s_k)  e(\\bar{g})$，则该测试用例的结果为 `True`，否则为 `False`。对每个提供的测试用例都执行此比较。最终输出是这些布尔结果的列表。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize, Bounds\n\ndef solve():\n    \"\"\"\n    解决一组测试用例的非光滑优化预测子比较问题。\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.0], [0.7, 0.6], [1.0, -0.5]]),\n            \"x_k\": np.array([1.0, 0.0]),\n            \"window_points\": [np.array([1.0, 0.0]), np.array([0.8, 0.2]), np.array([1.2, -0.1])]\n        },\n        {\n            \"A\": np.array([[-1.0, 0.0], [0.0, -1.0], [-0.5, -0.2]]),\n            \"x_k\": np.array([-1.0, -2.0]),\n            \"window_points\": [np.array([-1.0, -2.0]), np.array([-0.8, -2.2]), np.array([-1.2, -1.8]), np.array([-1.1, -2.5])]\n        },\n        {\n            \"A\": np.array([[0.0, 1.0], [1.0, 0.1], [0.6, 0.7]]),\n            \"x_k\": np.array([0.2, 0.5]),\n            \"window_points\": [np.array([0.1, 0.4])]\n        },\n        {\n            \"A\": np.array([[1.0, -0.5, 0.2], [0.8, 0.9, -0.3], [0.6, 0.0, 1.2], [1.0, -0.5, 0.2]]),\n            \"x_k\": np.array([0.2, -1.0, 0.0]),\n            \"window_points\": [np.array([0.2, -1.0, 0.0]), np.array([0.0, -1.0, 0.5]), np.array([0.6, -0.5, 0.8])]\n        }\n    ]\n\n    t = 0.01  # Step size\n    epsilon = 1e-9 # Tolerance for floating point comparisons of active subgradients\n    results = []\n    \n    def f(x_vec, A_mat):\n        \"\"\"Computes the function value f(x) = max(A @ x).\"\"\"\n        return np.max(A_mat @ x_vec)\n\n    def get_active_indices(x_vec, A_mat):\n        \"\"\"Finds the indices of active rows in A at point x.\"\"\"\n        vals = A_mat @ x_vec\n        max_val = np.max(vals)\n        return np.where(max_val - vals = epsilon)[0]\n\n    for case in test_cases:\n        A, x_k, window_points = case[\"A\"], case[\"x_k\"], case[\"window_points\"]\n        \n        # 1. Compute the aggregate subgradient s_k\n        active_indices_k = get_active_indices(x_k, A)\n        \n        # We need to handle duplicate active subgradients carefully.\n        # The QP must be over unique vectors to form a valid basis.\n        unique_active_subgrads, unique_indices = np.unique(A[active_indices_k], axis=0, return_index=True)\n\n        if len(unique_active_subgrads) == 1:\n            s_k = unique_active_subgrads[0]\n        else:\n            # G has unique active subgradients as columns.\n            G = unique_active_subgrads.T\n            p = G.shape[1]\n            Q = G.T @ G\n            \n            def qp_objective(alpha, Q_mat):\n                return 0.5 * alpha.T @ Q_mat @ alpha\n\n            constraints = ({'type': 'eq', 'fun': lambda alpha: np.sum(alpha) - 1})\n            bounds = Bounds([0.] * p, [np.inf] * p)\n            alpha_0 = np.ones(p) / p\n            \n            res = minimize(fun=qp_objective, x0=alpha_0, args=(Q,), method='SLSQP', bounds=bounds, constraints=constraints)\n            optimal_alpha = res.x\n            s_k = G @ optimal_alpha\n\n        # 2. Compute the averaged recent-window subgradient g_bar\n        recent_subgrads = []\n        for x_j in window_points:\n            active_indices_j = get_active_indices(x_j, A)\n            smallest_idx = np.min(active_indices_j)\n            g_j = A[smallest_idx]\n            recent_subgrads.append(g_j)\n        g_bar = np.mean(recent_subgrads, axis=0)\n\n        # 3. Calculate alignment errors\n        f_k = f(x_k, A)\n        \n        # For s_k\n        d_s = -s_k\n        x_new_s = x_k + t * d_s\n        delta_f_s = f_k - f(x_new_s, A)\n        v_hat_s = t * np.linalg.norm(s_k)**2\n        e_s = np.abs(delta_f_s - v_hat_s)\n        \n        # For g_bar\n        d_g = -g_bar\n        x_new_g = x_k + t * d_g\n        delta_f_g = f_k - f(x_new_g, A)\n        v_hat_g = t * np.linalg.norm(g_bar)**2\n        e_g = np.abs(delta_f_g - v_hat_g)\n\n        # 4. Compare and append result\n        results.append(e_s  e_g)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results).lower()}]\")\n\nsolve()\n```", "id": "3105153"}, {"introduction": "一个真正稳健的算法必须能优雅地处理冗余数据等实际挑战。在束方法中，当从不同点生成了相同的切平面并加入到信息束中时，就会出现这种情况。本练习将引导你探究此场景，并验证一个值得注意的特性：算法的聚合机制对这种冗余是“免疫”的。通过在有重复和无重复切平面的情况下求解问题，你将看到其底层的对偶公式如何确保稳定性和效率，从而展示该方法设计中精妙的一面。[@problem_id:3105175]", "problem": "考虑一个非光滑凸函数，它被定义为有限个仿射函数（称为割平面）的逐点最大值。设该函数由一组割平面 $\\ell_i(x)$ 表示，其形式为 $\\ell_i(x) = g_i^\\top x + c_i$，其中 $g_i \\in \\mathbb{R}^n$ 且 $c_i \\in \\mathbb{R}$。在第 $k$ 次迭代时的割平面模型为 $m_k(x) = \\max_i \\ell_i(x)$。在丛方法中，正则化主问题是通过求解以下凸二次规划（QP）问题来计算近端中心更新 $x^\\star$：\n$$\n\\min_{x \\in \\mathbb{R}^n, \\, s \\in \\mathbb{R}} \\; s + \\frac{t}{2}\\|x - x_k\\|_2^2 \\quad \\text{subject to} \\quad s \\ge \\ell_i(x) \\;\\; \\text{for all}\\;\\; i,\n$$\n其中 $t > 0$ 是正则化参数，$x_k \\in \\mathbb{R}^n$ 是当前的近端中心。\n\n丛方法中的一种退化现象发生在多个割平面 $\\ell_i$ 重合（即完全相同）时，但它们源自不同的点 $x_i$ 和可能不同的次梯度评估 $g_i$，尽管它们定义了相同的仿射函数 $x \\mapsto g^\\top x + c$。在这种情况下，割平面的聚合（通过与主问题的活动约束相关的对偶乘子）应能处理冗余，而不会使步长 $x^\\star$ 或模型预测不稳定，并且聚合后的割平面在相同割平面的重复下应保持不变。\n\n您的任务是实现一个程序，针对下面描述的每种情景，使用给定的割平面求解正则化主问题，计算聚合次梯度 $g_{\\text{agg}}$ 和相应的更新 $x^\\star$，然后评估重复相同割平面对解和模型值的影响。聚合割平面可以表示为 $\\ell_{\\text{agg}}(x) = g_{\\text{agg}}^\\top x + c_{\\text{agg}}$，其中 $c_{\\text{agg}}$ 是偏移量 $c_i$ 的凸组合，其系数自然地从主问题的最优性条件中产生。您的实现应基于凸优化的第一性原理，从主问题出发，并且只使用公认的定义和事实；不要使用任何跳过基本推导路径的快捷公式或预先推导的表达式。\n\n使用以下情景（维度为 $n=2$）。在每个情景中，构建指定的割平面集，求解主问题两次（一次使用重复的割平面，另一次将每个相同割平面的重复项合并为单个代表），并计算所要求的输出。下面提到的所有数值公差均为绝对公差。\n\n情景 $\\mathsf{S1}$ (一般情况):\n- 正则化参数: $t = 2.0$。\n- 近端中心: $x_k = [1.3, -2.1]$。\n- 相同割平面族 $\\mathsf{A}$: $g_{\\mathsf{A}} = [2, -1]$, $c_{\\mathsf{A}} = 0.0$，在三个不同点重复（因此有三个相同的割平面）。\n- 额外的不同割平面 $\\mathsf{B}$: $g_{\\mathsf{B}} = [-1, 2]$, $c_{\\mathsf{B}} = -0.5$，包含一次。\n- 构建“duplicates”（重复）集，包含三个 $\\mathsf{A}$ 的副本和一个 $\\mathsf{B}$ 的副本；以及“unique”（唯一）集，包含一个 $\\mathsf{A}$ 的副本和一个 $\\mathsf{B}$ 的副本。\n- 计算:\n  1. $b_1$: 一个布尔值，指示是否 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2 \\le 10^{-9}$。\n  2. $b_2$: 一个布尔值，指示是否 $\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2 \\le 10^{-9}$。\n  3. $d_1$: 一个浮点数，等于 $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\|x^\\star_{\\text{dup}} - x_k\\|_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\|x^\\star_{\\text{uniq}} - x_k\\|_2^2 \\right) \\right|$，其中 $s$ 是主问题中的最优上镜图变量。\n\n情景 $\\mathsf{S2}$ (边缘情况：只有相同的割平面):\n- 正则化参数: $t = 1.5$。\n- 近端中心: $x_k = [-3.7, 4.2]$。\n- 仅相同割平面族 $\\mathsf{A}$: $g_{\\mathsf{A}} = [2, -1]$, $c_{\\mathsf{A}} = 0.0$，重复五次。\n- 构建“duplicates”（重复）集，包含五个 $\\mathsf{A}$ 的副本；以及“unique”（唯一）集，包含一个 $\\mathsf{A}$ 的副本。\n- 计算:\n  1. $b_3$: 一个布尔值，指示是否 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2 \\le 10^{-9}$。\n  2. $b_4$: 一个布尔值，指示是否 $\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2 \\le 10^{-9}$。\n  3. $d_2$: 一个浮点数，等于 $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\|x^\\star_{\\text{dup}} - x_k\\|_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\|x^\\star_{\\text{uniq}} - x_k\\|_2^2 \\right) \\right|$。\n\n情景 $\\mathsf{S3}$ (边界情况：非常强的正则化):\n- 正则化参数: $t = 1000.0$。\n- 近端中心: $x_k = [10.0, -10.0]$。\n- 相同割平面族 $\\mathsf{A}$: $g_{\\mathsf{A}} = [2, -1]$, $c_{\\mathsf{A}} = 0.0$，重复四次。\n- 相同割平面族 $\\mathsf{B}$: $g_{\\mathsf{B}} = [-1, 2]$, $c_{\\mathsf{B}} = -0.5$，重复四次。\n- 构建“duplicates”（重复）集，包含四个 $\\mathsf{A}$ 的副本和四个 $\\mathsf{B}$ 的副本；以及“unique”（唯一）集，包含一个 $\\mathsf{A}$ 的副本和一个 $\\mathsf{B}$ 的副本。\n- 计算:\n  1. $b_5$: 一个布尔值，指示是否 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2 \\le 10^{-9}$。\n  2. $b_6$: 一个布尔值，指示是否 $\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2 \\le 10^{-9}$。\n  3. $d_3$: 一个浮点数，等于 $\\left| \\left( s_{\\text{dup}} + \\frac{t}{2}\\|x^\\star_{\\text{dup}} - x_k\\|_2^2 \\right) - \\left( s_{\\text{uniq}} + \\frac{t}{2}\\|x^\\star_{\\text{uniq}} - x_k\\|_2^2 \\right) \\right|$。\n\n实现要求:\n- 从主问题和凸优化的基本事实出发，推导并使用一个有原则的算法。您可以与活动割平面的凸组合系数（对偶乘子）相对应的变量进行等价公式化，前提是您的推导基于第一性原理。\n- 对于每个情景，您必须为“duplicates”和“unique”集计算 $x^\\star$、$s$ 和 $g_{\\text{agg}}$。\n- 最终输出必须使用 $10^{-9}$ 的绝对公差计算布尔值，并使用为浮点数实现的精确算术。\n\n最终输出格式:\n- 您的程序应生成单行输出，其中包含按顺序聚合的九个结果，形式为方括号内以逗号分隔的列表：`[b1,b2,d1,b3,b4,d2,b5,b6,d3]`。\n- 布尔值必须打印为 `True` 或 `False`，浮点数必须打印为标准十进制数。\n\n此问题不涉及物理单位或角度单位。", "solution": "该问题要求分析丛方法主问题中冗余割平面的影响。我们将通过推导正则化主问题的对偶问题，证明其对于重复割平面的不变性，并针对三个指定的情景对该不变性进行数值验证来完成此任务。\n\n### 步骤 1：问题验证\n\n所提供的问题陈述是凸优化领域中一个适定且有科学依据的练习，特别关注非光滑函数的丛方法理论。\n\n**1. 提取给定条件：**\n为三个不同的情景（$\\mathsf{S1}$, $\\mathsf{S2}$, $\\mathsf{S3}$）提供了所有必需的参数。对于每个情景，给定条件包括：\n- 正则化参数 $t$。\n- 近端中心 $x_k$。\n- 一组由梯度 $g_i$ 和偏移量 $c_i$ 定义的仿射割平面，包括若干族相同的割平面。\n- “duplicates”（重复）和“unique”（唯一）割平面集的定义。\n- 需要精确计算的量：解向量（$x^\\star$, $g_{\\text{agg}}$）的布尔比较 $b_1, \\dots, b_6$ 和主问题目标值的浮点数差异 $d_1, d_2, d_3$。\n- 向量比较的绝对容差为 $10^{-9}$。\n\n**2. 使用提取的给定条件进行验证：**\n- **科学依据：** 该问题基于近端丛方法的标准公式化，这是非光滑凸优化中一种成熟的技术。主问题是一个凸二次规划（QP），其分析使用凸分析和对偶理论的基本原理。所有概念都是标准的并且在事实上是正确的。\n- **适定性：** 主问题是一个严格凸的QP（由于项 $\\frac{t}{2}\\|x - x_k\\|_2^2$ 且 $t>0$），这保证了唯一解 $(x^\\star, s^\\star)$ 的存在。问题是自包含的，并提供了求解此解所需的所有数据。\n- **客观性：** 问题陈述使用精确、无歧义的数学语言表述。任务是客观的，可通过计算进行验证。\n\n**3. 结论与行动：**\n该问题是有效的。它具有科学合理性、适定性和客观性。将根据要求基于第一性原理来开发解决方案。\n\n### 步骤 2：推导与求解\n\n任务的核心是求解正则化主问题，这是一个凸二次规划（QP）：\n$$\n\\min_{x \\in \\mathbb{R}^n, \\, s \\in \\mathbb{R}} \\; s + \\frac{t}{2}\\|x - x_k\\|_2^2 \\quad \\text{subject to} \\quad g_i^\\top x + c_i - s \\le 0 \\;\\; \\text{for all cuts}\\;\\; i=1, \\dots, m.\n$$\n为了从第一性原理出发解决这个问题，我们推导并求解其对偶问题。\n\n**1. 对偶问题的推导**\n设 $\\lambda_i \\ge 0$ 是与第 $i$ 个约束相关的拉格朗日乘子。拉格朗日函数为：\n$$\nL(x, s, \\lambda) = s + \\frac{t}{2}\\|x - x_k\\|_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x + c_i - s)\n$$\n对偶函数 $q(\\lambda)$ 是拉格朗日函数关于原变量 $x$ 和 $s$ 的下确界。我们通过将 $L$ 关于 $x$ 和 $s$ 的偏导数设为零（平稳性条件）来找到这个下确界。\n\n关于 $s$ 的导数：\n$$\n\\frac{\\partial L}{\\partial s} = 1 - \\sum_{i=1}^m \\lambda_i = 0 \\implies \\sum_{i=1}^m \\lambda_i = 1\n$$\n这个条件表明最优乘子 $\\lambda_i^\\star$ 必须形成一个凸组合。\n\n关于 $x$ 的导数：\n$$\n\\nabla_x L = t(x - x_k) + \\sum_{i=1}^m \\lambda_i g_i = 0\n$$\n这给出了最优原解 $x^\\star$ 关于最优乘子 $\\lambda^\\star$ 的表达式：\n$$\nx^\\star = x_k - \\frac{1}{t} \\sum_{i=1}^m \\lambda_i g_i\n$$\n将这些条件代回拉格朗日函数以消去 $x$ 和 $s$：\n$$\n\\begin{align*}\nq(\\lambda) = s\\left(1 - \\sum_{i=1}^m \\lambda_i\\right) + \\frac{t}{2}\\left\\|-\\frac{1}{t}\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i c_i + \\sum_{i=1}^m \\lambda_i g_i^\\top \\left(x_k - \\frac{1}{t}\\sum_{j=1}^m \\lambda_j g_j\\right) \\\\\n= 0 + \\frac{1}{2t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i c_i + \\left(\\sum_{i=1}^m \\lambda_i g_i\\right)^\\top x_k - \\frac{1}{t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 \\\\\n= -\\frac{1}{2t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x_k + c_i)\n\\end{align*}\n$$\n对偶问题是最大化 $q(\\lambda)$，受限于对 $\\lambda$ 的约束：\n$$\n\\max_{\\lambda \\in \\mathbb{R}^m} \\quad -\\frac{1}{2t}\\left\\|\\sum_{i=1}^m \\lambda_i g_i\\right\\|_2^2 + \\sum_{i=1}^m \\lambda_i (g_i^\\top x_k + c_i) \\quad \\text{s.t.} \\quad \\sum_{i=1}^m \\lambda_i = 1, \\;\\; \\lambda_i \\ge 0.\n$$\n这等价于最小化 $-q(\\lambda)$，这是一个标准形式的凸 QP。设 $G$ 是一个 $n \\times m$ 矩阵，其列是次梯度 $g_i$，并设 $\\alpha$ 是一个 $m$ 维向量，其中 $\\alpha_i = g_i^\\top x_k + c_i$。对偶问题变为：\n$$\n\\min_{\\lambda \\in \\mathbb{R}^m} \\quad \\frac{1}{2t}\\|G\\lambda\\|_2^2 - \\alpha^\\top \\lambda \\quad \\text{s.t.} \\quad \\mathbf{1}^\\top \\lambda = 1, \\;\\; \\lambda \\ge 0.\n$$\n这可以写成 $\\min_{\\lambda} \\frac{1}{2}\\lambda^\\top H \\lambda + f^\\top \\lambda$，其中 $H = \\frac{1}{t}G^\\top G$ 和 $f = -\\alpha$。\n\n**2. 恢复原解和不变性**\n一旦通过求解这个 QP 找到了最优对偶变量 $\\lambda^\\star$，我们就可以计算所需的量：\n- **聚合次梯度：** $g_{\\text{agg}} = \\sum_{i=1}^m \\lambda_i^\\star g_i = G\\lambda^\\star$。\n- **近端中心更新：** $x^\\star = x_k - \\frac{1}{t} g_{\\text{agg}}$。\n- **上镜图变量：** 在最优点， $s$ 被推到仿射函数的上包络，所以 $s^\\star = \\max_i(g_i^\\top x^\\star + c_i)$。\n\n现在，考虑重复割平面的情况。设唯一割平面的集合由 $u \\in U$ 索引。设 $I_u$ 是原始（重复）列表中对应于唯一割平面 $u$ 的索引集。那么对于任何 $i \\in I_u$，我们有 $g_i = g_u$ 和 $c_i = c_u$，这意味着 $\\alpha_i = \\alpha_u$。\n设 $\\hat{\\lambda}_u = \\sum_{i \\in I_u} \\lambda_i$。对偶目标中的项可以分组：\n$$\n\\sum_{i=1}^m \\lambda_i g_i = \\sum_{u \\in U} \\sum_{i \\in I_u} \\lambda_i g_u = \\sum_{u \\in U} \\left(\\sum_{i \\in I_u} \\lambda_i\\right) g_u = \\sum_{u \\in U} \\hat{\\lambda}_u g_u\n$$\n$$\n\\sum_{i=1}^m \\lambda_i \\alpha_i = \\sum_{u \\in U} \\sum_{i \\in I_u} \\lambda_i \\alpha_u = \\sum_{u \\in U} \\left(\\sum_{i \\in I_u} \\lambda_i\\right) \\alpha_u = \\sum_{u \\in U} \\hat{\\lambda}_u \\alpha_u\n$$\n对 $\\lambda$ 的约束转化为 $\\sum_{u \\in U} \\hat{\\lambda}_u = 1$ 和 $\\hat{\\lambda}_u \\ge 0$。用 $\\hat{\\lambda}_u$ 表示的对偶问题与唯一割平面集的对偶问题是相同的。\n这证明了 $\\hat{\\lambda}_u^\\star$ 的最优值是唯一的，并且与割平面 $u$ 的重复次数无关。因此：\n- $g_{\\text{agg}} = \\sum_{u \\in U} \\hat{\\lambda}_u^\\star g_u$ 是不变的。\n- $x^\\star = x_k - \\frac{1}{t} g_{\\text{agg}}$ 是不变的。\n- $s^\\star = \\max_{u \\in U} (g_u^\\top x^\\star + c_u)$ 是不变的。\n- 最优目标值 $s^\\star + \\frac{t}{2}\\|x^\\star - x_k\\|_2^2$ 是不变的。\n\n因此，对于每个情景，我们预期 $\\|x^\\star_{\\text{dup}} - x^\\star_{\\text{uniq}}\\|_2$、$\\|g_{\\text{agg,dup}} - g_{\\text{agg,uniq}}\\|_2$ 以及目标值的差异在数值精度范围内应为零。\n\n**3. 算法实现**\n对于每个情景以及“duplicates”和“unique”割平面集，都实现了以下过程：\n1. 构建矩阵 $G$ 和向量 $\\alpha$。\n2. 构造对偶 QP 目标函数和约束。\n3. 使用数值优化例程（`scipy.optimize.minimize`）求解对偶 QP 以获得 $\\lambda^\\star$。\n4. 从 $\\lambda^\\star$ 计算 $g_{\\text{agg}}$、$x^\\star$ 和 $s^\\star$。\n5. 计算原问题的目标值。\n6. 比较“duplicates”和“unique”运行的结果，以计算所需的布尔值和浮点数差异。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve_master_problem(cuts, t, x_k):\n    \"\"\"\n    Solves the regularized bundle method master problem via its dual.\n\n    Args:\n        cuts (list of tuples): A list where each tuple is (g, c)\n                               defining a cut l(x) = g.T @ x + c.\n        t (float): The regularization parameter.\n        x_k (np.ndarray): The current proximal center.\n\n    Returns:\n        tuple: A tuple containing (x_star, g_agg, s_star).\n    \"\"\"\n    m = len(cuts)\n    n = len(x_k)\n\n    if m == 0:\n        g_agg = np.zeros(n)\n        x_star = x_k.copy()\n        s_star = -np.inf \n        return x_star, g_agg, s_star\n\n    G = np.array([cut[0] for cut in cuts]).T  # Shape (n, m)\n    c = np.array([cut[1] for cut in cuts])    # Shape (m,)\n    \n    # This is from the primal problem in the problem description, with penalty parameter t.\n    # The dual objective is to MINIMIZE: 0.5 * (1/t) * ||G*lambda||^2 - (G.T@x_k + c).T @ lambda\n    H = (1.0 / t) * (G.T @ G)\n    f = -(G.T @ x_k + c)\n\n    def dual_objective(lambda_vec):\n        return 0.5 * lambda_vec.T @ H @ lambda_vec + f.T @ lambda_vec\n\n    # Constraints: sum(lambda) = 1, lambda_i >= 0\n    constraints = ({'type': 'eq', 'fun': lambda l: np.sum(l) - 1.0})\n    bounds = tuple((0, None) for _ in range(m))\n\n    # Initial guess for the optimizer\n    lambda0 = np.ones(m) / m\n\n    res = minimize(dual_objective, lambda0, method='SLSQP', bounds=bounds, constraints=constraints)\n    \n    lambda_star = res.x\n\n    # Numerical stability: project lambda_star onto the simplex\n    lambda_star[lambda_star  0] = 0\n    if np.sum(lambda_star) > 0:\n      lambda_star /= np.sum(lambda_star)\n\n    # Recover primal solution\n    g_agg = G @ lambda_star\n    x_star = x_k - (1.0 / t) * g_agg\n    s_star = np.max(G.T @ x_star + c)\n    \n    return x_star, g_agg, s_star\n\ndef solve_scenario(t, x_k, cuts_dup, cuts_uniq):\n    \"\"\"\n    Solves a scenario for both duplicate and unique cuts and computes metrics.\n    \"\"\"\n    # Solve for duplicate cuts\n    x_star_dup, g_agg_dup, s_star_dup = solve_master_problem(cuts_dup, t, x_k)\n    obj_val_dup = s_star_dup + (t / 2.0) * np.linalg.norm(x_star_dup - x_k)**2\n\n    # Solve for unique cuts\n    x_star_uniq, g_agg_uniq, s_star_uniq = solve_master_problem(cuts_uniq, t, x_k)\n    obj_val_uniq = s_star_uniq + (t / 2.0) * np.linalg.norm(x_star_uniq - x_k)**2\n    \n    # Compute metrics\n    tol = 1e-9\n    b_x = np.linalg.norm(x_star_dup - x_star_uniq) = tol\n    b_g = np.linalg.norm(g_agg_dup - g_agg_uniq) = tol\n    d_obj = np.abs(obj_val_dup - obj_val_uniq)\n\n    return b_x, b_g, d_obj\n\ndef solve():\n    results = []\n\n    # Scenario S1\n    t1 = 2.0\n    x_k1 = np.array([1.3, -2.1])\n    g_A1, c_A1 = np.array([2.0, -1.0]), 0.0\n    g_B1, c_B1 = np.array([-1.0, 2.0]), -0.5\n    cuts_dup1 = [(g_A1, c_A1)] * 3 + [(g_B1, c_B1)]\n    cuts_uniq1 = [(g_A1, c_A1), (g_B1, c_B1)]\n    b1, b2, d1 = solve_scenario(t1, x_k1, cuts_dup1, cuts_uniq1)\n    results.extend([b1, b2, d1])\n\n    # Scenario S2\n    t2 = 1.5\n    x_k2 = np.array([-3.7, 4.2])\n    g_A2, c_A2 = np.array([2.0, -1.0]), 0.0\n    cuts_dup2 = [(g_A2, c_A2)] * 5\n    cuts_uniq2 = [(g_A2, c_A2)]\n    b3, b4, d2 = solve_scenario(t2, x_k2, cuts_dup2, cuts_uniq2)\n    results.extend([b3, b4, d2])\n    \n    # Scenario S3\n    t3 = 1000.0\n    x_k3 = np.array([10.0, -10.0])\n    g_A3, c_A3 = np.array([2.0, -1.0]), 0.0\n    g_B3, c_B3 = np.array([-1.0, 2.0]), -0.5\n    cuts_dup3 = [(g_A3, c_A3)] * 4 + [(g_B3, c_B3)] * 4\n    cuts_uniq3 = [(g_A3, c_A3), (g_B3, c_B3)]\n    b5, b6, d3 = solve_scenario(t3, x_k3, cuts_dup3, cuts_uniq3)\n    results.extend([b5, b6, d3])\n\n    # Format and print the final output\n    # Convert booleans to lowercase true/false strings as required.\n    formatted_results = []\n    for r in results:\n        if isinstance(r, bool) or isinstance(r, np.bool_):\n            formatted_results.append(str(r).lower())\n        else:\n            formatted_results.append(str(r))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3105175"}]}