{"hands_on_practices": [{"introduction": "本练习将指导你从头开始实现一个完整的差分进化求解器。通过整合诸如停滞重启策略和高级边界处理等功能，你将学习如何构建一个鲁棒的优化器，使其能够在复杂搜索空间中导航并逃离局部最优解，这是实际应用中的一项关键技能。[@problem_id:3120583]", "problem": "您需要实现一个完整的、确定性的程序，该程序使用带有重启策略和反弹边界处理的差分进化算法来最小化多峰目标函数。该程序在算法步骤的设计中必须体现第一性原理的推理，并为指定的测试套件生成数值结果。\n\n差分进化（Differential Evolution, DE）是一种基于种群的优化方法，定义于函数 $f : \\mathbb{R}^d \\to \\mathbb{R}$ 在边界约束 $[\\mathbf{L}, \\mathbf{U}]$ 上，其中 $\\mathbf{L}, \\mathbf{U} \\in \\mathbb{R}^d$ 且对于所有 $j \\in \\{1, \\dots, d\\}$ 都有 $\\mathbf{L}_j  \\mathbf{U}_j$。核心的DE算子由以下基本要素指定：\n\n1. 初始化：在 $[\\mathbf{L}, \\mathbf{U}]$ 内均匀随机采样 $N_p$ 个候选向量，其中 $N_p \\in \\mathbb{N}$ 是种群大小。\n\n2. 变异 (DE/rand/1)：对于每个目标索引 $i$，选择三个与 $i$ 不同的独立索引 $r_1, r_2, r_3$，并构建供体向量\n$$\n\\mathbf{v}_i = \\mathbf{x}_{r_1} + F \\cdot (\\mathbf{x}_{r_2} - \\mathbf{x}_{r_3}),\n$$\n其中 $F \\in \\mathbb{R}$ 是变异因子。\n\n3. 交叉（二项式，也称均匀交叉）：对于每个分量 $j \\in \\{1, \\dots, d\\}$，\n$$\nu_{i, j} = \n\\begin{cases}\nv_{i, j},  \\text{如果 } \\text{rand} \\le CR \\text{ 或 } j = j_{\\text{rand}}, \\\\\nx_{i, j},  \\text{其他情况},\n\\end{cases}\n$$\n其中 $CR \\in [0, 1]$ 是交叉率，$\\text{rand}$ 是从 $[0, 1]$ 上的均匀分布中新抽取的独立样本，而 $j_{\\text{rand}}$ 是从 $\\{1, \\dots, d\\}$ 中均匀选择的一个索引，以确保至少有一个分量来自供体向量。\n\n4. 选择（贪婪）：如果 $f(\\mathbf{u}_i) \\le f(\\mathbf{x}_i)$，则设置 $\\mathbf{x}_i \\leftarrow \\mathbf{u}_i$，否则保持 $\\mathbf{x}_i$ 不变。\n\n边界处理必须确保试验向量的可行性。请按如下方式实现反弹反射以保证可行性：对于任何具有下界 $L$ 和上界 $U$ 的标量分量 $y \\in \\mathbb{R}$，在违反的边界上反复反射，直到 $y \\in [L, U]$，\n$$\ny \\leftarrow \n\\begin{cases}\n2L - y,  \\text{如果 } y  L, \\\\\n2U - y,  \\text{如果 } y  U,\n\\end{cases}\n$$\n如果反射后的 $y$ 仍然违反边界，则重复此过程。当禁用反弹时，改用裁剪\n$$\ny \\leftarrow \\min(\\max(y, L), U).\n$$\n\n精确定义停滞和重启策略。跟踪每一代的最佳目标值，\n$$\nf^\\star(g) = \\min_{i \\in \\{1, \\dots, N_p\\}} f(\\mathbf{x}_i(g)),\n$$\n其中 $\\mathbf{x}_i(g)$ 是第 $g$ 代的个体 $i$。设 $\\epsilon = 10^{-12}$。如果 $f^\\star(g)  f^\\star(g-1) - \\epsilon$，则认为一代有所改进。停滞期是连续没有改进的代的序列。当停滞期长度达到 $S \\in \\mathbb{N}$ 时，触发重启策略，将变异因子和交叉率重置为探索性值 $(F_{\\text{explore}}, CR_{\\text{explore}})$，并持续整整 $g_{\\text{explore}} \\in \\mathbb{N}$ 代。在此探索窗口期间，使用 $(F, CR) = (F_{\\text{explore}}, CR_{\\text{explore}})$。窗口结束后，恢复为基础值 $(F_{\\text{base}}, CR_{\\text{base}})$。如果之后再次出现长度为 $S$ 的停滞期，则以相同方式触发另一个探索窗口。\n\n使用以下广泛使用且经过充分测试的多峰目标函数：\n\n- $d$ 维的 Rastrigin 函数，参数 $A = 10$，全局最小值在 $\\mathbf{x} = \\mathbf{0}$ 处：\n$$\nf_{\\text{Rastrigin}}(\\mathbf{x}) = A d + \\sum_{j=1}^{d} \\left( x_j^2 - A \\cos(2\\pi x_j) \\right).\n$$\n\n- $d$ 维的 Ackley 函数，参数 $a = 20$, $b = 0.2$, $c = 2\\pi$，全局最小值在 $\\mathbf{x} = \\mathbf{0}$ 处：\n$$\nf_{\\text{Ackley}}(\\mathbf{x}) = -a \\exp\\left( -b \\sqrt{ \\frac{1}{d} \\sum_{j=1}^d x_j^2 } \\right)\n- \\exp\\left( \\frac{1}{d} \\sum_{j=1}^d \\cos(c x_j) \\right)\n+ a + e.\n$$\n\n您的程序必须实现所述的带有反弹边界处理和重启策略的差分进化过程。它必须通过为每个测试用例使用固定的随机种子来确保确定性。\n\n测试套件：\n对于下面的每个测试用例，运行差分进化算法整整 $G_{\\max}$ 代，并返回最终的最佳目标值（浮点数）。\n\n- 用例 $1$ (理想情况，Rastrigin 反弹)：\n  - 函数: $f_{\\text{Rastrigin}}$.\n  - 维度: $d = 5$.\n  - 边界: 对所有 $j$， $L_j = -5.12$, $U_j = 5.12$.\n  - 种群大小: $N_p = 40$.\n  - 代数: $G_{\\max} = 300$.\n  - 基础参数: $F_{\\text{base}} = 0.5$, $CR_{\\text{base}} = 0.9$.\n  - 探索性参数: $F_{\\text{explore}} = 0.95$, $CR_{\\text{explore}} = 0.2$.\n  - 停滞阈值: $S = 25$.\n  - 探索窗口长度: $g_{\\text{explore}} = 10$.\n  - 反弹: 启用。\n  - 随机种子: $42$.\n\n- 用例 $2$ (Ackley 反弹，更高崎岖度)：\n  - 函数: $f_{\\text{Ackley}}$.\n  - 维度: $d = 5$.\n  - 边界: 对所有 $j$， $L_j = -32.768$, $U_j = 32.768$.\n  - 种群大小: $N_p = 30$.\n  - 代数: $G_{\\max} = 400$.\n  - 基础参数: $F_{\\text{base}} = 0.7$, $CR_{\\text{base}} = 0.6$.\n  - 探索性参数: $F_{\\text{explore}} = 1.0$, $CR_{\\text{explore}} = 0.1$.\n  - 停滞阈值: $S = 40$.\n  - 探索窗口长度: $g_{\\text{explore}} = 15$.\n  - 反弹: 启用。\n  - 随机种子: $123$.\n\n- 用例 $3$ (Rastrigin 反弹，频繁重启边缘情况)：\n  - 函数: $f_{\\text{Rastrigin}}$.\n  - 维度: $d = 2$.\n  - 边界: 对所有 $j$， $L_j = -5.12$, $U_j = 5.12$.\n  - 种群大小: $N_p = 10$.\n  - 代数: $G_{\\max} = 200$.\n  - 基础参数: $F_{\\text{base}} = 0.5$, $CR_{\\text{base}} = 0.9$.\n  - 探索性参数: $F_{\\text{explore}} = 1.2$, $CR_{\\text{explore}} = 0.1$.\n  - 停滞阈值: $S = 5$.\n  - 探索窗口长度: $g_{\\text{explore}} = 8$.\n  - 反弹: 启用。\n  - 随机种子: $7$.\n\n- 用例 $4$ (Ackley 使用裁剪而非反弹，用于对比分析)：\n  - 函数: $f_{\\text{Ackley}}$.\n  - 维度: $d = 5$.\n  - 边界: 对所有 $j$， $L_j = -32.768$, $U_j = 32.768$.\n  - 种群大小: $N_p = 30$.\n  - 代数: $G_{\\max} = 400$.\n  - 基础参数: $F_{\\text{base}} = 0.7$, $CR_{\\text{base}} = 0.6$.\n  - 探索性参数: $F_{\\text{explore}} = 1.0, CR_{\\text{explore}} = 0.1$.\n  - 停滞阈值: $S = 40$.\n  - 探索窗口长度: $g_{\\text{explore}} = 15$.\n  - 反弹: 禁用 (使用裁剪)。\n  - 随机种子: $123$.\n\n最终输出格式：\n您的程序应生成单行输出，其中包含来自测试套件的四个最终最佳目标值，按顺序以逗号分隔的列表形式放在方括号内，例如 $[$result1$,$result2$,$result3$,$result4$]$。每个结果都必须是浮点数，且打印的行不得包含任何其他文本。", "solution": "该问题要求实现差分进化（DE）算法，并整合一个针对停滞的特定重启策略和一个反弹边界处理机制。解决方案将通过一套在标准多峰基准函数上的测试用例进行验证。整个过程必须是确定性的，这通过为每个测试用例使用固定的随机种子来实现。\n\n解决方案的设计遵循了基于种群的随机优化的第一性原理以及问题陈述中定义的特定算子。\n\n首先，我们定义目标函数。$d$ 维 Rastrigin 函数的参数为 $A$，其公式为：\n$$\nf_{\\text{Rastrigin}}(\\mathbf{x}) = A d + \\sum_{j=1}^{d} \\left( x_j^2 - A \\cos(2\\pi x_j) \\right)\n$$\n$d$ 维 Ackley 函数的参数为 $a$、$b$ 和 $c$，其公式为：\n$$\nf_{\\text{Ackley}}(\\mathbf{x}) = -a \\exp\\left( -b \\sqrt{ \\frac{1}{d} \\sum_{j=1}^d x_j^2 } \\right)\n- \\exp\\left( \\frac{1}{d} \\sum_{j=1}^d \\cos(c x_j) \\right)\n+ a + e\n$$\n其中 $e$ 是自然对数的底数，$e \\approx 2.71828$。\n\n算法的核心是差分进化过程。\n1.  **初始化**：创建一个包含 $N_p$ 个候选向量的初始种群 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{N_p}\\}$。每个向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 都在指定的边界 $[\\mathbf{L}, \\mathbf{U}]$ 内从均匀分布中采样。也就是说，对于每个分量 $j \\in \\{1, \\dots, d\\}$，$x_{i,j}$ 从 $U(L_j, U_j)$ 中采样。该过程被播种以确保可复现性。\n\n2.  **代际循环**：算法最多迭代 $G_{\\max}$ 代。在每一代中，都会创建一个新的试验向量种群，并与当前种群进行比较。\n\n3.  **变异**：对于当前种群中的每个目标向量 $\\mathbf{x}_i$，使用 \"DE/rand/1\" 策略生成一个供体向量 $\\mathbf{v}_i$。从 $\\{1, \\dots, N_p\\}$ 中随机选择三个不重复的索引 $r_1, r_2, r_3$，并确保它们也不同于目标索引 $i$。然后按如下方式计算供体向量：\n    $$\n    \\mathbf{v}_i = \\mathbf{x}_{r_1} + F \\cdot (\\mathbf{x}_{r_2} - \\mathbf{x}_{r_3})\n    $$\n    这里，$F \\in \\mathbb{R}$ 是变异因子，是一个控制差分变异放大程度的关键参数。\n\n4.  **交叉**：通过组合目标向量 $\\mathbf{x}_i$ 和供体向量 $\\mathbf{v}_i$ 的分量来形成一个试验向量 $\\mathbf{u}_i$。采用指定的二项式交叉方案。对于每个分量 $j \\in \\{1, \\dots, d\\}$：\n    $$\n    u_{i, j} = \n    \\begin{cases}\n    v_{i, j},  \\text{如果 } \\text{rand}_j \\le CR \\text{ 或 } j = j_{\\text{rand}}, \\\\\n    x_{i, j},  \\text{其他情况},\n    \\end{cases}\n    $$\n    其中 $CR \\in [0, 1]$ 是交叉率，$\\text{rand}_j$ 是为每个分量从 $U(0, 1)$ 中抽取的新样本，$j_{\\text{rand}}$ 是从 $\\{1, \\dots, d\\}$ 中随机选择的一个索引。$j=j_{\\text{rand}}$ 条件保证了试验向量 $\\mathbf{u}_i$ 至少从供体向量 $\\mathbf{v}_i$ 接收一个分量。\n\n5.  **边界处理**：试验向量 $\\mathbf{u}_i$ 的分量可能超出可行边界 $[\\mathbf{L}, \\mathbf{U}]$。在函数评估前必须对其进行修正。\n    *   **裁剪**：当禁用反弹时使用的较简单方法，是将分量裁剪到边界：$u_{i,j} \\leftarrow \\min(\\max(u_{i,j}, L_j), U_j)$。\n    *   **反弹反射**：此方法将越界分量反射回可行范围内，跨过被违反的边界。对于一个分量 $y$ 和边界 $[L, U]$，规则是：\n        $$\n        y \\leftarrow \n        \\begin{cases}\n        2L - y,  \\text{如果 } y  L, \\\\\n        2U - y,  \\text{如果 } y  U.\n        \\end{cases}\n        $$\n        由于如果分量远在边界之外（例如，$y  L - (U-L)$），单次反射可能不足以使其回到界内，因此该过程必须在一个循环中迭代应用，直到 $y \\in [L, U]$。\n\n6.  **选择**：贪婪选择策略决定试验向量 $\\mathbf{u}_i$ 是否在下一代中替换目标向量 $\\mathbf{x}_i$。比较它们的目标函数值：\n    $$\n    \\mathbf{x}_i^{\\text{next}} = \n    \\begin{cases}\n    \\mathbf{u}_i,  \\text{如果 } f(\\mathbf{u}_i) \\le f(\\mathbf{x}_i), \\\\\n    \\mathbf{x}_i,  \\text{其他情况}.\n    \\end{cases}\n    $$\n    这确保了种群的适应度在代际间只能改善或保持不变。\n\n7.  **重启策略**：此机制旨在通过周期性地促进探索来逃离局部最优解。它由一个停滞计数器控制。\n    *   令 $f^\\star(g)$ 为第 $g$ 代种群中的最佳目标值。改进定义为 $f^\\star(g)  f^\\star(g-1) - \\epsilon$，其中 $\\epsilon = 10^{-12}$ 是一个小的容差，以考虑浮点数不精确性。\n    *   对于每个没有改进的连续代，`stagnation_streak` 计数器加一。一旦有改进，它就重置为 $0$。\n    *   如果 `stagnation_streak` 达到阈值 $S$，则触发重启。DE 参数 $(F, CR)$ 被设置为其探索性值 $(F_{\\text{explore}}, CR_{\\text{explore}})$，持续时间为 $g_{\\text{explore}}$ 代。`stagnation_streak` 计数器重置为 $0$，以防止立即再次触发。\n    *   一个 `exploratory_counter` 管理此阶段的持续时间。它初始化为 $g_{\\text{explore}}$ 并每代递减。当它达到 $0$ 时，参数 $(F, CR)$ 恢复到它们的基础值 $(F_{\\text{base}}, CR_{\\text{base}})$。\n\n通过将这些有原则的步骤组合到一个确定性程序中（通过固定的种子），我们可以系统地评估算法在指定测试套件上的性能。每个用例的最终输出将是经过 $G_{\\max}$ 代后找到的最佳目标函数值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the execution of all test cases.\n    \"\"\"\n    \n    # Define objective functions\n    def rastrigin(x, A=10):\n        d = len(x)\n        return A * d + np.sum(x**2 - A * np.cos(2 * np.pi * x))\n\n    def ackley(x, a=20, b=0.2, c=2*np.pi):\n        d = len(x)\n        sum_sq_term = -b * np.sqrt(np.sum(x**2) / d)\n        sum_cos_term = np.sum(np.cos(c * x)) / d\n        return -a * np.exp(sum_sq_term) - np.exp(sum_cos_term) + a + np.e\n\n    # Define test cases as a list of dictionaries\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"function\": rastrigin,\n            \"d\": 5,\n            \"bounds\": (-5.12, 5.12),\n            \"Np\": 40,\n            \"G_max\": 300,\n            \"F_base\": 0.5, \"CR_base\": 0.9,\n            \"F_explore\": 0.95, \"CR_explore\": 0.2,\n            \"S\": 25,\n            \"g_explore\": 10,\n            \"bounce_back\": True,\n            \"seed\": 42,\n        },\n        {\n            \"name\": \"Case 2\",\n            \"function\": ackley,\n            \"d\": 5,\n            \"bounds\": (-32.768, 32.768),\n            \"Np\": 30,\n            \"G_max\": 400,\n            \"F_base\": 0.7, \"CR_base\": 0.6,\n            \"F_explore\": 1.0, \"CR_explore\": 0.1,\n            \"S\": 40,\n            \"g_explore\": 15,\n            \"bounce_back\": True,\n            \"seed\": 123,\n        },\n        {\n            \"name\": \"Case 3\",\n            \"function\": rastrigin,\n            \"d\": 2,\n            \"bounds\": (-5.12, 5.12),\n            \"Np\": 10,\n            \"G_max\": 200,\n            \"F_base\": 0.5, \"CR_base\": 0.9,\n            \"F_explore\": 1.2, \"CR_explore\": 0.1,\n            \"S\": 5,\n            \"g_explore\": 8,\n            \"bounce_back\": True,\n            \"seed\": 7,\n        },\n        {\n            \"name\": \"Case 4\",\n            \"function\": ackley,\n            \"d\": 5,\n            \"bounds\": (-32.768, 32.768),\n            \"Np\": 30,\n            \"G_max\": 400,\n            \"F_base\": 0.7, \"CR_base\": 0.6,\n            \"F_explore\": 1.0, \"CR_explore\": 0.1,\n            \"S\": 40,\n            \"g_explore\": 15,\n            \"bounce_back\": False, # Use clipping\n            \"seed\": 123,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        final_best_val = run_de(case)\n        results.append(f\"{final_best_val:.12f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef run_de(params):\n    \"\"\"\n    Executes the Differential Evolution algorithm for a single test case.\n    \"\"\"\n    # Unpack parameters\n    obj_func = params[\"function\"]\n    d = params[\"d\"]\n    L, U = params[\"bounds\"]\n    bounds = np.array([L, U] * d).reshape(d, 2)\n    Np = params[\"Np\"]\n    G_max = params[\"G_max\"]\n    F_base, CR_base = params[\"F_base\"], params[\"CR_base\"]\n    F_explore, CR_explore = params[\"F_explore\"], params[\"CR_explore\"]\n    S = params[\"S\"]\n    g_explore = params[\"g_explore\"]\n    use_bounce_back = params[\"bounce_back\"]\n    seed = params[\"seed\"]\n    epsilon = 1e-12\n\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Initialization\n    population = rng.uniform(L, U, size=(Np, d))\n    fitness = np.array([obj_func(ind) for ind in population])\n    \n    # Restart schedule state variables\n    stagnation_streak = 0\n    exploratory_counter = 0\n    F, CR = F_base, CR_base\n    best_obj_val = np.min(fitness)\n\n    population_indices = list(range(Np))\n\n    # 2. Main generational loop\n    for _ in range(G_max):\n        new_population = population.copy()\n        new_fitness = fitness.copy()\n\n        for i in range(Np):\n            # 3. Mutation (DE/rand/1)\n            choices = population_indices[:i] + population_indices[i+1:]\n            r1, r2, r3 = rng.choice(choices, size=3, replace=False)\n            \n            x_r1 = population[r1]\n            x_r2 = population[r2]\n            x_r3 = population[r3]\n\n            donor_vector = x_r1 + F * (x_r2 - x_r3)\n\n            # 4. Crossover (Binomial)\n            target_vector = population[i]\n            trial_vector = target_vector.copy()\n            \n            rand_vals = rng.random(size=d)\n            j_rand = rng.integers(0, d)\n            \n            crossover_mask = (rand_vals = CR) | (np.arange(d) == j_rand)\n            trial_vector[crossover_mask] = donor_vector[crossover_mask]\n            \n            # 5. Boundary Handling\n            if use_bounce_back:\n                # Iterative bounce-back\n                for j in range(d):\n                    while trial_vector[j]  bounds[j, 0] or trial_vector[j] > bounds[j, 1]:\n                        if trial_vector[j]  bounds[j, 0]:\n                            trial_vector[j] = 2 * bounds[j, 0] - trial_vector[j]\n                        if trial_vector[j] > bounds[j, 1]:\n                            trial_vector[j] = 2 * bounds[j, 1] - trial_vector[j]\n            else:\n                # Clipping\n                np.clip(trial_vector, bounds[:, 0], bounds[:, 1], out=trial_vector)\n\n            # 6. Selection (Greedy)\n            trial_fitness = obj_func(trial_vector)\n            if trial_fitness = fitness[i]:\n                new_population[i] = trial_vector\n                new_fitness[i] = trial_fitness\n        \n        population = new_population\n        fitness = new_fitness\n\n        # 7. Restart Schedule Logic\n        current_best_gen_val = np.min(fitness)\n        \n        if current_best_gen_val  best_obj_val - epsilon:\n            stagnation_streak = 0\n        else:\n            stagnation_streak += 1\n        \n        best_obj_val = min(best_obj_val, current_best_gen_val)\n        \n        # Manage state for the *next* generation's parameters\n        if exploratory_counter > 0:\n            exploratory_counter -= 1\n            if exploratory_counter == 0:\n                F, CR = F_base, CR_base\n        \n        if stagnation_streak >= S:\n            F, CR = F_explore, CR_explore\n            exploratory_counter = g_explore\n            stagnation_streak = 0\n\n    return np.min(fitness)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3120583"}, {"introduction": "包括差分进化在内的任何演化算法的性能都对其初始种群的质量很敏感。本练习介绍了一种名为“对立学习”（Opposition-Based Learning, OBL）的智能初始化策略。你将通过实验，在固定的计算预算下，将其与标准均匀采样进行比较，从而深入了解如何有效地“热启动”一个优化过程。[@problem_id:3120621]", "problem": "要求您实现并比较两种初始化策略，这两种策略可用于热启动差分进化算法，以在超矩形域上最小化函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$。您的比较必须在固定的、有限的目标函数评估预算下进行。\n\n从以下基本概念开始：\n- 向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 的欧几里得范数定义为 $\\|\\mathbf{x}\\| = \\sqrt{\\sum_{i=1}^d x_i^2}$。因此，目标函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$ 即为 $f(\\mathbf{x}) = \\sum_{i=1}^d x_i^2$，当 $\\mathbf{0}$ 位于定义域内时，在 $\\mathbf{x}=\\mathbf{0}$ 处取得其全局最小值 $0$。\n- 超矩形域由分量级边界 $\\mathbf{l} \\le \\mathbf{x} \\le \\mathbf{u}$ 描述，其中 $\\mathbf{l} = (l,\\dots,l)$ 和 $\\mathbf{u} = (u,\\dots,u)$ 对应标量边界 $l  u$。\n- 基于对立的学习 (Opposition-Based Learning, OBL) 定义了 $\\mathbf{x}$ 相对于边界 $\\mathbf{l}$ 和 $\\mathbf{u}$ 的逐点对立点 $\\tilde{\\mathbf{x}}$，其分量为 $\\tilde{x}_i = l_i + u_i - x_i$。当 $l=-u$ 时，这是关于原点的反射，即 $\\tilde{\\mathbf{x}} = -\\mathbf{x}$。\n\n您的任务：\n- 在固定的评估预算 $M$ 下，实现两种热启动采样策略：\n  1. 纯均匀采样 (Pure Uniform Sampling, URS)：从 $[l,u]^d$ 中均匀抽取 $M$ 个独立样本 $\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(M)}$，对所有样本评估 $f$ 函数，并将热启动质量定义为 $q_{\\mathrm{URS}}=\\min_{1\\le k\\le M} f(\\mathbf{x}^{(k)})$。\n  2. 使用基于对立的学习 (OBL) 的对立初始化 (Opposition-Based Initialization, OBI)：从 $[l,u]^d$ 中均匀抽取 $M/2$ 个独立基点 $\\mathbf{x}^{(1)},\\dots,\\mathbf{x}^{(M/2)}$，通过 $\\tilde{x}^{(k)}_i = l_i + u_i - x^{(k)}_i$ 构建它们的逐点对立点 $\\tilde{\\mathbf{x}}^{(k)}$，对每个基点及其对立点都评估 $f$ 函数，并将热启动质量定义为 $q_{\\mathrm{OBI}}=\\min\\{ f(\\mathbf{x}^{(k)}), f(\\tilde{\\mathbf{x}}^{(k)}) : 1\\le k\\le M/2 \\}$。假设 $M$ 为偶数，因此 $M/2$ 是一个整数。\n- 对于每种策略，确保精确计算 $M$ 次函数评估，以保证在有限的评估预算下比较是公平的。\n\n比较指标：\n- 对于每个测试用例，计算标量 $c = q_{\\mathrm{URS}} - q_{\\mathrm{OBI}}$。$c$ 的正值表示在相同预算下，基于对立的初始化策略得到了一个严格更优（更小）的热启动目标值。\n\n实现细节：\n- 使用来自 $[l,u]^d$ 上均匀分布的独立同分布 (i.i.d.) 采样。\n- 所有计算必须使用双精度浮点数进行。\n- 必须通过伪随机种子控制随机性，以确保结果是完全可复现的。\n\n测试套件：\n- 在以下五个测试用例上评估您的实现，每个测试用例指定为一个元组 $(d,l,u,M,s)$：\n  1. $(d,l,u,M,s) = (2, -5, 5, 100, 42)$。\n  2. $(d,l,u,M,s) = (10, -1, 1, 200, 7)$。\n  3. $(d,l,u,M,s) = (30, -100, 100, 60, 123)$。\n  4. $(d,l,u,M,s) = (5, 0, 1, 10, 999)$。\n  5. $(d,l,u,M,s) = (1, -10, 10, 2, 2024)$。\n\n要求的最终输出：\n- 您的程序必须输出一行，其中包含一个 Python 风格的列表，该列表包含上述五个测试用例的标量 $c$ 值，顺序与列表顺序相同，每个值使用标准舍入规则四舍五入到恰好六位小数。例如：\"[0.123456,-0.000001,0.500000,0.000000,1.234568]\"。\n- 角度和物理单位不适用。所有输出都是无单位的实数。\n\n科学真实性和约束：\n- 确保实现完全遵循上述定义，并按规定计算目标函数评估次数。\n- 不要在程序输出中提供任何额外的注释或诊断信息。", "solution": "我们使用优化和概率论的核心定义来形式化此比较。目标函数为 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 = \\sum_{i=1}^d x_i^2$，它是一个严格凸、径向对称的函数，当可行时，在 $\\mathbf{0}$ 处取得最小值。定义域是超矩形 $[l,u]^d$，其中 $l  u$。热启动的目标是为后续的优化算法（如差分进化）提供具有低目标值的初始点，但在这里我们只关注在有限评估预算下的初始化阶段。\n\n两种策略如下：\n\n- 纯均匀采样 (URS)：从 $[l,u]^d$ 上的均匀分布中抽取 $M$ 个独立同分布样本，并取找到的最佳目标值。这恰好使用 $M$ 次评估。由于样本是独立的，对 $M$ 个独立同分布抽样的最小值会随着 $M$ 的增长而趋于减小。\n\n- 使用基于对立的学习 (OBL) 的对立初始化 (OBI)：对于每个均匀抽取的基点 $\\mathbf{x}^{(k)}$，构建其由 $\\tilde{x}^{(k)}_i = l_i + u_i - x^{(k)}_i$ 给出的逐点对立点 $\\tilde{\\mathbf{x}}^{(k)}$。这是关于每个坐标区间中点 $\\frac{l_i + u_i}{2}$ 的反射。通过抽取 $M/2$ 个基点并同时评估每个基点及其对立点来强制执行预算，总共进行 $M$ 次函数评估。这 $M$ 次评估中的最佳目标值即为热启动质量。\n\n正确性与公平性：\n- 两种策略都精确消耗 $M$ 次评估，确保了比较的公平性。\n- OBL 映射保持可行性：如果 $x^{(k)}_i \\in [l_i,u_i]$，那么 $\\tilde{x}^{(k)}_i = l_i + u_i - x^{(k)}_i \\in [l_i,u_i]$。\n\n基于目标函数结构的定性分析：\n- 当定义域关于原点对称时，即 $l = -u$，对立映射简化为 $\\tilde{\\mathbf{x}} = -\\mathbf{x}$。因为 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 = \\|-\\mathbf{x}\\|^2$，所以每个评估对 $(\\mathbf{x}, -\\mathbf{x})$ 产生相同的目标值。在固定的预算 $M$ 下，OBI 产生 $M/2$ 个不同的基点位置（仅通过对称性产生重复）但对两种符号都进行评估；相比之下，URS 产生 $M$ 个独立的抽样点。因此，当 $l=-u$ 时，OBI 中每对的最佳值与其中任一元素相同，每对没有提供任何改进，同时将独立位置的总数减少了 2 倍。因此，$M$ 个独立 URS 抽样的最小值可能小于 OBI 中 $M/2$ 个独立基点抽样的最小值，导致 $c = q_{\\mathrm{URS}} - q_{\\mathrm{OBI}}$ 平均为非正值或负值。\n\n- 当定义域相对于最小化点不对称时，例如 $[0,1]^d$，其中最小化点 $\\mathbf{0}$ 位于下界，对立映射 $\\tilde{x}_i = 1 - x_i$ 将 $x_i$ 关于 $0.5$ 进行反射。对于接近 $1$ 的 $x_i$，其对立点会更接近 $0$；因此 $f(\\tilde{\\mathbf{x}})$ 可能显著小于 $f(\\mathbf{x})$。在这种情况下，同时评估 $\\mathbf{x}$ 和 $\\tilde{\\mathbf{x}}$ 增加了这对点中至少有一个更接近最小化点的机会。尽管 OBI 只评估 $M/2$ 个基点，但在相同的预算 $M$ 下，这种“对中取优”效应可能产生比 URS 更小的最小值，从而可能产生正的 $c$ 值。\n\n算法设计：\n1. 对于给定的 $(d,l,u,M,s)$ 的 URS：\n   - 使用种子 $s$ 初始化一个伪随机数生成器。\n   - 抽取一个由 $M$ 个向量组成的数组，其分量从 $[l,u]$ 上的均匀分布中独立同分布地采样。\n   - 对每个向量，将其分量的平方和作为 $f$ 的值进行计算。\n   - 将这 $M$ 个值中的最小值记录为 $q_{\\mathrm{URS}}$。\n\n2. 对于相同的 $(d,l,u,M,s)$ 的 OBI：\n   - 使用相同的种子 $s$ 初始化一个独立的伪随机数生成器，以确保在每个测试用例的底层随机性方面进行公平且可复现的比较，同时通过分开抽取或使用不同流（如果需要）的方式，在实现中保持策略间样本的独立性。为每个策略自身的生成器使用相同的种子可确保可复现性，并且由于我们是根据指定的分布进行抽样，策略间的任何相关性都不会影响单个策略分布的正确性。\n   - 从 $[l,u]^d$ 中均匀抽取 $M/2$ 个基向量。\n   - 通过 $\\tilde{x}_i = l + u - x_i$ 计算它们的对立点 $\\tilde{\\mathbf{x}}$。\n   - 对两组点集都评估 $f$ 函数，并取这 $M$ 个目标值中的最小值为 $q_{\\mathrm{OBI}}$。\n\n3. 计算 $c = q_{\\mathrm{URS}} - q_{\\mathrm{OBI}}$。\n\n测试套件中的边界情况：\n- $(1, -10, 10, 2, 2024)$ 是一个边界情况，具有最小的偶数预算和一维空间。由于 $l = -u$，OBI 评估一对具有相等目标值的点 $(x,-x)$，而 URS 评估两个独立同分布的抽样点；由于独立性，比较可能略微有利于 URS。\n- $(30, -100, 100, 60, 123)$ 具有宽边界和有限预算，用于测试高维度的影响。在高维度下，样本点到原点的典型距离更大，策略间的差异更多地由样本数量驱动，而不是由每对点的对立优势驱动。\n- $(5, 0, 1, 10, 999)$ 可以展示 OBI 的优势，因为关于中点的反射可以将点移动到更靠近原点处的最小化点。\n\n输出：\n- 对五个测试用例中的每一个，计算 $c$ 并四舍五入到六位小数。\n- 按顺序输出一行包含这五个值的 Python 风格列表。不得打印任何额外文本。\n\n该方法遵循目标函数、定义域和基于对立的学习的核心定义，并确保在有限评估预算下的比较是有原则且可复现的。采样的随机性完全由固定的种子控制，保证了每次运行的输出完全相同。结论与函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$ 的几何特性以及定义域边界的对称性或非对称性相符。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_obj(x: np.ndarray) - np.ndarray:\n    # Computes f(x) = ||x||^2 for a batch of points (rows of x)\n    # Returns a 1D array of objective values.\n    return np.sum(x * x, axis=1)\n\ndef uniform_best_min(d: int, l: float, u: float, M: int, seed: int) - float:\n    rng = np.random.default_rng(seed)\n    # Sample M points uniformly in [l,u]^d\n    X = rng.uniform(l, u, size=(M, d))\n    vals = f_obj(X)\n    return float(np.min(vals))\n\ndef opposition_best_min(d: int, l: float, u: float, M: int, seed: int) - float:\n    assert M % 2 == 0, \"M must be even for opposition-based evaluation budget.\"\n    rng = np.random.default_rng(seed)\n    base_count = M // 2\n    X = rng.uniform(l, u, size=(base_count, d))\n    X_opp = (l + u) - X\n    vals_base = f_obj(X)\n    vals_opp = f_obj(X_opp)\n    # Combine both sets of evaluations (total M evaluations)\n    min_val = float(np.min(np.concatenate([vals_base, vals_opp], axis=0)))\n    return min_val\n\ndef run_case(d: int, l: float, u: float, M: int, seed: int) - float:\n    q_urs = uniform_best_min(d, l, u, M, seed)\n    q_obi = opposition_best_min(d, l, u, M, seed)\n    return q_urs - q_obi  # Positive means OBI better\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (d, l, u, M, seed)\n    test_cases = [\n        (2, -5.0, 5.0, 100, 42),\n        (10, -1.0, 1.0, 200, 7),\n        (30, -100.0, 100.0, 60, 123),\n        (5, 0.0, 1.0, 10, 999),\n        (1, -10.0, 10.0, 2, 2024),\n    ]\n\n    results = []\n    for d, l, u, M, seed in test_cases:\n        c = run_case(d, l, u, M, seed)\n        results.append(c)\n\n    # Round to exactly six decimal places and format as required.\n    formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3120621"}, {"introduction": "虽然差分进化是为连续域设计的，但许多实际优化问题涉及整数变量。本练习演示了一种解决此类离散问题的常用且强大的技术：用差分进化求解其连续松弛问题，然后将结果映射回整数域。通过计算“舍入偏差”，你将更深刻地理解这种混合方法的优势与局限。[@problem_id:3120707]", "problem": "考虑一个在边界约束下的整数向量 $\\mathbf{k}\\in\\mathbb{Z}^n$ 的离散优化问题。设目标函数为 $f(\\mathbf{k})=\\sum_{i=1}^n \\lvert k_i - 3\\rvert$，其中每个坐标 $k_i$ 受限于 $\\ell_i \\le k_i \\le u_i$，且 $\\ell_i,u_i\\in\\mathbb{Z}$ 及 $\\ell_i \\le u_i$。您将使用一个连续代理函数来近似这个离散问题，然后将连续解映射回整数域。具体来说，定义连续代理函数为 $g(\\mathbf{x})=\\sum_{i=1}^n \\lvert x_i - 3\\rvert$，其中 $\\mathbf{x}\\in\\mathbb{R}^n$ 且受箱形约束 $\\ell_i \\le x_i \\le u_i$。在 $\\mathbb{R}^n$ 上使用差分进化算法 (Differential Evolution, DE) 在箱形约束下最小化 $g(\\mathbf{x})$，以获得一个连续最小化子 $\\mathbf{x}^\\star$。通过将每个坐标四舍五入到最近的整数（平局时向上取整）来将 $\\mathbf{x}^\\star$ 映射到整数域，即对于每个 $i$，计算 $k^{\\mathrm{DE}}_i = \\min\\big(\\max\\big(\\lfloor x_i^\\star + 0.5 \\rfloor,\\ell_i\\big),u_i\\big)$ 以强制满足可行性。然后计算离散目标函数值 $f(\\mathbf{k}^{\\mathrm{DE}})$。独立地，计算精确的离散最优值 $f(\\mathbf{k}^\\star)$，其中 $\\mathbf{k}^\\star\\in\\mathbb{Z}^n$ 在相同边界下最小化 $f(\\mathbf{k})$。将给定实例的舍入偏差 (rounding bias) 定义为 $b = f(\\mathbf{k}^{\\mathrm{DE}}) - f(\\mathbf{k}^\\star)$。您的任务是实现一个程序，对每个指定的测试用例执行上述步骤，并返回作为浮点数的舍入偏差 $b$。\n\n使用以下边界约束实例的测试套件，表示为定义每个坐标的箱形的 $(\\ell_i,u_i)$ 对的列表：\n- 测试用例 $1$：$n=5$，对所有 $i\\in\\{1,2,3,4,5\\}$，边界为 $(\\ell_i,u_i)=(0,10)$。\n- 测试用例 $2$：$n=4$，对所有 $i\\in\\{1,2,3,4\\}$，边界为 $(\\ell_i,u_i)=(4,8)$。\n- 测试用例 $3$：$n=3$，边界为 $(\\ell_1,u_1)=(2,2)$，$(\\ell_2,u_2)=(3,3)$，$(\\ell_3,u_3)=(5,5)$。\n- 测试用例 $4$：$n=6$，边界为 $(\\ell_1,u_1)=(0,5)$，$(\\ell_2,u_2)=(1,1)$，$(\\ell_3,u_3)=(10,10)$，$(\\ell_4,u_4)=(2,7)$，$(\\ell_5,u_5)=(4,4)$，$(\\ell_6,u_6)=(3,9)$。\n\n程序应在给定的边界上对连续代理函数 $g(\\mathbf{x})$ 使用差分进化算法 (DE)，然后应用上述的舍入和钳位规则以获得 $\\mathbf{k}^{\\mathrm{DE}}$ 并计算 $f(\\mathbf{k}^{\\mathrm{DE}})$。它还必须精确计算真实的离散最优值 $f(\\mathbf{k}^\\star)$。对于每个测试用例，输出如上定义的舍入偏差 $b$。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个条目对应一个测试用例，顺序为 $[b_1,b_2,b_3,b_4]$，分别对应测试用例 $1$ 到 $4$。此问题不涉及任何物理单位或角度，所有输出均为实数（浮点数）。", "solution": "对用户提供的问题陈述进行了严格分析，并发现其是有效的。这是一个在数值优化领域内定义明确的问题，具有科学依据和正式规范。\n\n该问题要求为离散优化问题的几个实例计算“舍入偏差”(rounding bias)，记为 $b$。此偏差定义为通过启发式方法获得的目标函数值与精确最优解的值之间的差。\n\n让我们将所需的计算分解为一个系统的、分步的过程。\n\n### 1. 离散优化问题\n\n核心任务是为一个整数向量 $\\mathbf{k} = (k_1, k_2, \\ldots, k_n) \\in \\mathbb{Z}^n$ 最小化目标函数 $f(\\mathbf{k}) = \\sum_{i=1}^n \\lvert k_i - 3\\rvert$。搜索空间受限于每个坐标的一组下界和上界：$\\ell_i \\le k_i \\le u_i$，其中 $\\ell_i, u_i \\in \\mathbb{Z}$。\n\n### 2. 寻找精确离散最优解，$f(\\mathbf{k}^\\star)$\n\n目标函数 $f(\\mathbf{k})$ 是可分的，这意味着它可以表示为单个变量函数的和：$f(\\mathbf{k}) = \\sum_{i=1}^n f_i(k_i)$，其中 $f_i(k_i) = \\lvert k_i - 3\\rvert$。为了最小化这个和，我们可以独立地在各自的域 $[\\ell_i, u_i]$ 上最小化每一项 $f_i(k_i)$。\n\n函数 $f_i(k_i) = \\lvert k_i - 3\\rvert$ 衡量 $k_i$ 到值 $3$ 的距离。当 $k_i$ 是区间 $[\\ell_i, u_i]$ 中最接近 $3$ 的整数时，达到最小值。设这个最优整数为 $k_i^\\star$。\n\n我们可以如下确定 $k_i^\\star$：\n- 如果值 $3$ 在区间 $[\\ell_i, u_i]$ 内，即 $\\ell_i \\le 3 \\le u_i$，那么最接近的整数就是 $3$ 本身。所以，$k_i^\\star = 3$。\n- 如果 $3  \\ell_i$，那么区间内所有的整数都大于 $3$。最接近 $3$ 的是最小的那个，即 $\\ell_i$。所以，$k_i^\\star = \\ell_i$。\n- 如果 $3 > u_i$，那么区间内所有的整数都小于 $3$。最接近 $3$ 的是最大的那个，即 $u_i$。所以，$k_i^\\star = u_i$。\n\n这个逻辑可以使用钳位操作 (clipping operation) 紧凑地表达。由于 $\\ell_i$ 和 $u_i$ 是整数，第 $i$ 个坐标的最优离散值 $k_i^\\star$ 可以通过将目标值 $3$ 钳位到区间 $[\\ell_i, u_i]$ 来得到：\n$$k_i^\\star = \\max(\\ell_i, \\min(u_i, 3))$$\n精确的最优整数向量是 $\\mathbf{k}^\\star = (k_1^\\star, k_2^\\star, \\ldots, k_n^\\star)$。那么，最小目标值为：\n$$f(\\mathbf{k}^\\star) = \\sum_{i=1}^n \\lvert k_i^\\star - 3\\rvert$$\n\n### 3. 连续松弛和舍入启发式方法\n\n问题指定了一种启发式方法来寻找近似的整数解。\n\n**步骤 3a：连续代理问题**\n通过允许变量为实数，将离散问题松弛为连续问题。代理目标函数为 $g(\\mathbf{x}) = \\sum_{i=1}^n \\lvert x_i - 3\\rvert$，其中 $\\mathbf{x} = (x_1, x_2, \\ldots, x_n) \\in \\mathbb{R}^n$ 受相同的箱形约束 $\\ell_i \\le x_i \\le u_i$。\n\n与离散情况类似，这个连续、可分函数的最小值出现在每个 $x_i$ 是 $[\\ell_i, u_i]$ 中最接近 $3$ 的实数时。这给出了精确的连续最小化子 $\\mathbf{x}^\\star$，其坐标为：\n$$x_i^\\star = \\max(\\ell_i, \\min(u_i, 3))$$\n值得注意的是，由于 $\\ell_i$ 和 $u_i$ 是整数，真实连续最小化子的坐标 $x_i^\\star$ 本身就是整数，事实上，$\\mathbf{x}^\\star = \\mathbf{k}^\\star$。\n\n**步骤 3b：使用差分进化 (DE) 进行数值优化**\n问题要求使用差分进化 (DE) 算法来寻找连续最小化子。DE 是一种随机的、基于种群的优化器。对于像 $g(\\mathbf{x})$ 这样的简单凸函数，DE 预期会找到一个解向量 $\\mathbf{x}^{\\mathrm{DE}}$，它是真实最小化子 $\\mathbf{x}^\\star$ 的一个非常接近的浮点近似。\n\n**步骤 3c：舍入和钳位**\n将数值上获得的连续解 $\\mathbf{x}^{\\mathrm{DE}}$ 映射回整数域，以得到一个近似的整数解 $\\mathbf{k}^{\\mathrm{DE}}$。每个坐标的规则是：\n$$k^{\\mathrm{DE}}_i = \\min\\big(\\max\\big(\\lfloor x^{\\mathrm{DE}}_i + 0.5 \\rfloor,\\ell_i\\big),u_i\\big)$$\n操作 $\\lfloor z + 0.5 \\rfloor$ 对应于将 $z$ 四舍五入到最近的整数，其中小数部分为一半时（例如 2.5）向上取整。外围的 $\\min$ 和 $\\max$ 函数确保得到的整数 $k^{\\mathrm{DE}}_i$ 位于可行范围 $[\\ell_i, u_i]$ 内。\n\n**步骤 3d：评估启发式解**\n然后在这个新的整数向量 $\\mathbf{k}^{\\mathrm{DE}}$ 处评估目标函数：\n$$f(\\mathbf{k}^{\\mathrm{DE}}) = \\sum_{i=1}^n \\lvert k_i^{\\mathrm{DE}} - 3\\rvert$$\n\n### 4. 计算舍入偏差\n\n最后一步是计算舍入偏差 $b$，定义为启发式解的目标值与精确最优值之间的差：\n$$b = f(\\mathbf{k}^{\\mathrm{DE}}) - f(\\mathbf{k}^\\star)$$\n\n### 预期结果\n由于真实的连续最小化子 $\\mathbf{x}^\\star$ 与真实的离散最小化子 $\\mathbf{k}^\\star$ 相同，并且 DE 预期会找到一个非常精确的近似值 $\\mathbf{x}^{\\mathrm{DE}} \\approx \\mathbf{x}^\\star$，因此舍入步骤极有可能恢复出精确的整数解。也就是说，对于每个坐标 $i$，预期误差 $|x^{\\mathrm{DE}}_i - x_i^\\star|$ 将远小于 $0.5$，从而导致 $\\lfloor x^{\\mathrm{DE}}_i + 0.5 \\rfloor = x_i^\\star = k_i^\\star$。在钳位之后（因为 $k_i^\\star$ 已经在边界内，所以钳位没有效果），我们会发现 $\\mathbf{k}^{\\mathrm{DE}} = \\mathbf{k}^\\star$。这将意味着 $f(\\mathbf{k}^{\\mathrm{DE}}) = f(\\mathbf{k}^\\star)$，因此舍入偏差 $b = 0$。实现将对每个测试用例进行计算验证。\n\n提供的 Python 代码将为每个测试用例忠实地执行这些步骤，包括运行 DE 优化。将为 DE 使用固定的随机种子以确保可复现性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import differential_evolution\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem for all test cases and prints the rounding bias.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: n=5, bounds (0,10) for all i\n        [(0, 10)] * 5,\n        # Test case 2: n=4, bounds (4,8) for all i\n        [(4, 8)] * 4,\n        # Test case 3: n=3, bounds (2,2), (3,3), (5,5)\n        [(2, 2), (3, 3), (5, 5)],\n        # Test case 4: n=6, bounds (0,5), (1,1), (10,10), (2,7), (4,4), (3,9)\n        [(0, 5), (1, 1), (10, 10), (2, 7), (4, 4), (3, 9)],\n    ]\n\n    results = []\n    # Use a fixed seed for the stochastic DE algorithm for reproducibility.\n    RANDOM_SEED = 42\n    \n    # The target value for the objective function.\n    TARGET_VALUE = 3\n\n    for bounds in test_cases:\n        # Extract lower and upper bounds as numpy arrays for vectorized operations.\n        lower_bounds = np.array([b[0] for b in bounds])\n        upper_bounds = np.array([b[1] for b in bounds])\n\n        # 1. Compute the exact discrete optimum f(k*)\n        # The optimal integer k_i* is the integer in [l_i, u_i] closest to 3.\n        # This is equivalent to clipping the target value 3 to the integer interval.\n        k_star = np.clip(TARGET_VALUE, lower_bounds, upper_bounds)\n        f_k_star = np.sum(np.abs(k_star - TARGET_VALUE))\n\n        # 2. Minimize the continuous surrogate g(x) using Differential Evolution\n        # Define the continuous objective function g(x).\n        def g(x):\n            return np.sum(np.abs(x - TARGET_VALUE))\n\n        # Run the DE optimization.\n        de_result = differential_evolution(\n            g, \n            bounds, \n            seed=RANDOM_SEED\n        )\n        x_star_de = de_result.x\n\n        # 3. Map the continuous solution x* back to the integer domain\n        # The rule is to round to the nearest integer (ties round up), then clamp.\n        # floor(x + 0.5) implements this rounding rule.\n        k_de_rounded = np.floor(x_star_de + 0.5)\n        \n        # Clamp the rounded vector to ensure it is within the feasible integer domain.\n        k_de = np.clip(k_de_rounded, lower_bounds, upper_bounds)\n\n        # Compute the objective value for the DE-based integer solution.\n        f_k_de = np.sum(np.abs(k_de - TARGET_VALUE))\n\n        # 4. Compute the rounding bias b\n        bias = f_k_de - f_k_star\n        results.append(float(bias))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3120707"}]}