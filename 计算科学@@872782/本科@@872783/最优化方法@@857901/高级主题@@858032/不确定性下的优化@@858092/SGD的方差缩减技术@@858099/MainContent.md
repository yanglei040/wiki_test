## 引言
[随机梯度下降](@entry_id:139134)（SGD）是现代[大规模机器学习](@entry_id:634451)的基石，它通过处理小批量数据实现了无与伦比的[计算效率](@entry_id:270255)。然而，这种效率的代价是引入了[梯度估计](@entry_id:164549)的随机性——即[方差](@entry_id:200758)。这种固有的“噪声”不仅会减慢模型的收敛速度，还会阻止算法精确地收敛到最优点，从而限制了模型的最终性能。因此，如何有效管理和减少这种梯度[方差](@entry_id:200758)，已成为优化理论与实践中的一个核心挑战。

本文旨在系统性地解决这一知识鸿沟，为读者提供一套关于SGD[方差缩减技术](@entry_id:141433)的完整框架。我们将深入探讨这些技术背后的“为什么”和“怎么样”，帮助你超越基础SGD，掌握更高级、更高效的优化策略。

在接下来的内容中，你将学习到：在 **“原则与机制”** 章节，我们将剖析梯度[方差](@entry_id:200758)的根源及其对收敛的影响，并详细介绍两大主流[方差缩减技术](@entry_id:141433)——控制变量法和[重要性采样](@entry_id:145704)法。随后，在 **“应用与跨学科联系”** 章节，我们将展示这些理论如何在深度学习、[联邦学习](@entry_id:637118)等前沿领域中解决实际问题，揭示其广泛的适用性。最后，通过 **“动手实践”** 部分，你将有机会通过具体的编程练习来巩固和应用所学知识。

让我们从理解[方差](@entry_id:200758)问题的本质开始，进入[方差缩减](@entry_id:145496)的世界。

## 原则与机制

在上一章中，我们介绍了[随机梯度下降](@entry_id:139134)（SGD）作为[大规模优化](@entry_id:168142)的核心引擎。其每次迭代仅使用一小部分数据（一个或多个样本）来估计梯度，从而极大地降低了计算成本。然而，这种效率是以引入噪声为代价的。随机梯度的[方差](@entry_id:200758)是理解 SGD 行为及其局限性的关键，也是本章将要探讨的[方差缩减技术](@entry_id:141433)的核心动机。本章将深入剖析随机梯度[方差](@entry_id:200758)的来源与影响，并系统地介绍两类主流的[方差缩减](@entry_id:145496)方法：控制变量法和[重要性采样](@entry_id:145704)法。

### SGD的问题：梯度[方差](@entry_id:200758)

标准的SGD算法在每次迭代 $k$ 时，通过以下方式更新模型参数 $x$：
$$
x_{k+1} = x_k - \eta g_k
$$
其中 $\eta$ 是步长（学习率），$g_k$ 是对真实梯度 $\nabla f(x_k)$ 的一个随机、[无偏估计](@entry_id:756289)。在典型的有限和问题 $f(x) = \frac{1}{n}\sum_{i=1}^{n} f_i(x)$ 中，随机梯度通常通过均匀随机采样一个索引 $i_k$ 并计算 $g_k = \nabla f_{i_k}(x_k)$ 来获得。由于 $\mathbb{E}_{i_k}[g_k] = \nabla f(x_k)$，这个估计是无偏的，保证了算法在期望上沿着正确的下降方向前进。

然而，梯度的[方差](@entry_id:200758)，即 $\mathbb{V}[g_k] = \mathbb{E}[\|g_k - \nabla f(x_k)\|^2]$，通常不为零。这个[方差](@entry_id:200758)使得 SGD 的收敛路径变得曲折，并且在使用固定步长时，会阻止算法精确收敛到最优点 $x^\star$。

为了更精确地理解这个问题，我们可以分析一个二次[目标函数](@entry_id:267263) $f(x)=\frac{1}{2}x^{\top}Qx-b^{\top}x$，其中 $Q$ 是一个[对称正定矩阵](@entry_id:136714)。在这种情况下，真实梯度为 $\nabla f(x) = Qx - b$。SGD 的迭代可以写成 $x_{k+1}=x_{k}-\eta(Qx_{k}-b+\zeta_{k})$，其中 $\zeta_k$ 是均值为零的[梯度噪声](@entry_id:165895)，其协[方差](@entry_id:200758)为 $\operatorname{Cov}(\zeta_{k})=S$。经过一系列推导，我们可以得到算法在达到平稳状态时，解的期望误差的精确表达式 [@problem_id:3197235]。假设 $Q$ 和 $S$ 可以被[同时对角化](@entry_id:196036)，其[特征值](@entry_id:154894)分别为 $\{\lambda_i\}$ 和 $\{s_i\}$，并且步长满足[收敛条件](@entry_id:166121) $0  \eta  2/\lambda_{\max}$，那么平稳状态下的期望误差为：
$$
\lim_{k\to\infty} \mathbb{E}[\|x_{k}-x^{\star}\|^2] = \sum_{i=1}^{d} \frac{\eta s_i}{\lambda_i(2 - \eta\lambda_i)}
$$
这个公式揭示了一个深刻的结论：只要[梯度噪声](@entry_id:165895)的[方差](@entry_id:200758)不为零（即存在 $s_i  0$），SGD 在使用固定步长时就不会收敛到最优点 $x^\star$（误差为零），而是会在其周围的一个“噪声球”内持续[振荡](@entry_id:267781)。这个噪声球的大小正比于步长 $\eta$ 和[梯度噪声](@entry_id:165895)[方差](@entry_id:200758) $s_i$。虽然减小步长 $\eta$ 可以缩小这个噪声球，但这同时也会减慢收敛速度。为了在不牺牲[收敛速度](@entry_id:636873)的情况下实现更精确的收敛，我们必须从根本上解决问题：减小[梯度估计](@entry_id:164549)的[方差](@entry_id:200758)。

### 控制变量法

控制变量法是统计学中一种经典的减少[估计量方差](@entry_id:263211)的技术，其思想可以被巧妙地应用于 SGD。

#### 一般原理

假设我们想要估计一个[随机变量](@entry_id:195330) $X$ 的期望 $\mathbb{E}[X]$。如果我们能找到另一个[随机变量](@entry_id:195330) $Y$，它的期望 $\mathbb{E}[Y]$ 是已知的，并且 $Y$ 与 $X$ 相关，那么我们可以构造一个新的估计量：
$$
\tilde{X} = X - \beta (Y - \mathbb{E}[Y])
$$
其中 $\beta$ 是一个标量系数。这个新的估计量 $\tilde{X}$ 总是无偏的，因为 $\mathbb{E}[\tilde{X}] = \mathbb{E}[X] - \beta (\mathbb{E}[Y] - \mathbb{E}[Y]) = \mathbb{E}[X]$。它的[方差](@entry_id:200758)为：
$$
\mathbb{V}[\tilde{X}] = \mathbb{V}[X] - 2\beta \mathrm{Cov}(X, Y) + \beta^2 \mathbb{V}[Y]
$$
这是一个关于 $\beta$ 的二次函数。通过求导并令其为零，我们可以找到最小化[方差](@entry_id:200758)的最优系数 $\beta^\star$ [@problem_id:3197213]：
$$
\beta^\star = \frac{\mathrm{Cov}(X, Y)}{\mathbb{V}[Y]}
$$
这个最优选择表明，我们应该选择一个与我们目标估计量 $X$ 高度相关的控制变量 $Y$。当 $X$ 和 $Y$ 强正相关时，如果观测到 $Y$ 的值高于其均值，我们就会从 $X$ 的观测值中减去一个正数，从而将估计[拉回](@entry_id:160816)，反之亦然。

#### 在SGD中的应用：SVRG与SAGA

将[控制变量](@entry_id:137239)的思想应用于 SGD 时，我们的目标[随机变量](@entry_id:195330)是随机梯度 $g_{i_k}(x_k) = \nabla f_{i_k}(x_k)$。一个常见的误解是，可以直接减去一个近似于真实梯度的基线（baseline）$b$ 来减少[方差](@entry_id:200758)。然而，这种做法会引入偏差，因为 $\mathbb{E}[\nabla f_{i_k}(x_k) - b] = \nabla f(x_k) - b$，除非 $b=0$，否则估计是有偏的 [@problem_id:3197183]。这与强化学习中的[策略梯度方法](@entry_id:634727)形成对比，在[策略梯度](@entry_id:635542)中，由于数学结构的特殊性，减去一个与动作无关的基线不会引入偏差。

正确的做法是遵循[控制变量](@entry_id:137239)的构造，即减去一个随机项，同时加上该项的期望以保持无偏性。随机[方差缩减](@entry_id:145496)梯度（**SVRG**）算法正是基于此思想。它在每个外循环（epoch）的开始，选择一个快照点 $\tilde{x}$（通常是上一个 epoch 的最后一个迭代点），并计算一次完整的梯度 $\nabla f(\tilde{x})$。在内循环的每次迭代中，SVRG 使用如下的[梯度估计](@entry_id:164549)量：
$$
g_k = \nabla f_{i_k}(x_k) - \nabla f_{i_k}(\tilde{x}) + \nabla f(\tilde{x})
$$
在这个构造中，$\nabla f_{i_k}(\tilde{x})$ 就是我们的[控制变量](@entry_id:137239) $Y$。它的期望 $\mathbb{E}_{i_k}[\nabla f_{i_k}(\tilde{x})] = \nabla f(\tilde{x})$ 是已知的（因为我们已经计算过一次）。因此，整个估计量 $g_k$ 是无偏的：$\mathbb{E}[g_k] = \nabla f(x_k) - \nabla f(\tilde{x}) + \nabla f(\tilde{x}) = \nabla f(x_k)$ [@problem_id:3197183]。

SVRG 的巧妙之处在于，当 $x_k$ 接近 $\tilde{x}$ 时，$\nabla f_{i_k}(x_k)$ 和 $\nabla f_{i_k}(\tilde{x})$ 会非常相似。因此，它们的差 $\nabla f_{i_k}(x_k) - \nabla f_{i_k}(\tilde{x})$ 的[方差](@entry_id:200758)会远小于原始随机梯度 $\nabla f_{i_k}(x_k)$ 的[方差](@entry_id:200758)。当 $x_k = \tilde{x}$ 时，这个梯度[估计量的[方](@entry_id:167223)差](@entry_id:200758)甚至为零，因为它变成了确定性的全梯度 $\nabla f(\tilde{x})$。

在 [@problem_id:3197235] 中分析的特殊二次问题 $f_i(x)=\frac{1}{2}x^{\top}Qx-b_{i}^{\top}x$ 中，SVRG 的威力得到了极致的体现。对于这种结构，SVRG [梯度估计](@entry_id:164549)量出人意料地简化为：
$$
g_k = (Qx_k - b_{i_k}) - (Q\tilde{x} - b_{i_k}) + (Q\tilde{x} - b) = Qx_k - b = \nabla f(x_k)
$$
这意味着，对于这类问题，SVRG 估计的梯度就是真实的全梯度！算法退化为确定性的[梯度下降法](@entry_id:637322)，其梯度[方差](@entry_id:200758)完全消除，因此平稳状态误差为零。

另一个相关的方法是随机平均梯度（**SAGA**）。SAGA 维护一个包含所有 $n$ 个样本最新梯度的表格。其[梯度估计](@entry_id:164549)量为：
$$
g_k = \nabla f_{i_k}(x_k) - \nabla f_{i_k}(\text{past}) + \frac{1}{n}\sum_{j=1}^n \nabla f_j(\text{past})
$$
其中 $\nabla f_{i_k}(\text{past})$ 是表格中存储的关于样本 $i_k$ 的旧梯度。SAGA 也可以看作一种[控制变量](@entry_id:137239)方法，但它需要一个大小为 $O(nd)$ 的额外内存来存储整个梯度表。

#### [递归估计](@entry_id:169954)器：SARAH

随机递归梯度算法（**SARAH**）提供了一种不同的、内存效率更高的控制变量形式。它不将当前梯度与一个固定的快照点进行比较，而是与上一步的梯度进行比较。其[梯度估计](@entry_id:164549)量 $v_k$ 通过递归方式定义：
$$
v_k = \nabla f_{i_k}(x_k) - \nabla f_{i_k}(x_{k-1}) + v_{k-1}
$$
在每个外循环开始时，它同样被初始化为全梯度 $v_0 = \nabla f(\tilde{x})$。SARAH 的[梯度估计](@entry_id:164549)量是有偏的（$\mathbb{E}[v_k | \mathcal{F}_{k-1}] \neq \nabla f(x_k)$），但其更新规则 $x_k = x_{k-1} - \eta v_{k-1}$ 经过精心设计，使得算法整体能够收敛。

SARAH 和 SVRG 的一个关键区别在于它们[方差](@entry_id:200758)的来源。在梯度具有 $L$-Lipschitz 连续性的假设下，可以推导出它们[条件方差](@entry_id:183803)的上界 [@problem_id:3197177]：
$$
\operatorname{Var}(g_k^{\text{SVRG}} \mid \mathcal{F}_{k-1}) \leq L^2 \|x_k - \tilde{x}\|^2
$$
$$
\operatorname{Var}(v_k^{\text{SARAH}} \mid \mathcal{F}_{k-1}) \leq L^2 \|x_k - x_{k-1}\|^2
$$
SVRG 的[方差](@entry_id:200758)与当前迭代点 $x_k$ 到固定快照点 $\tilde{x}$ 的距离成正比，而 SARAH 的[方差](@entry_id:200758)与连续两次迭代之间的距离（即步长大小）成正比。在内循环中，随着算法收敛，$x_k$ 会逐渐远离初始的 $\tilde{x}$，但步长 $\|x_k - x_{k-1}\|$ 会趋于零。因此，SARAH 的[方差](@entry_id:200758)上界会比 SVRG 的更快地“崩溃”，这通常能带来更快的[收敛速度](@entry_id:636873)。

#### 实践中的权衡：内存与[方差](@entry_id:200758)

选择哪种控制变量方法取决于具体问题中的[资源限制](@entry_id:192963)。SVRG 需要周期性地计算全梯度，这可能成为计算瓶颈。SAGA 通过维护一个梯度表避免了全梯度的重复计算，但代价是巨大的内存开销，对于样本量 $n$ 很大的问题可能不可行。SARAH 则在计算和内存之间取得了很好的平衡，它只需要存储一个额外的梯度向量 $v_{k-1}$。

我们可以通过一个量化的指标来比较它们的效率，例如“每字节存储所获得的[方差缩减](@entry_id:145496)量” [@problem_id:3197212]。在一个假设的场景中，对于一个拥有 $n=10000$ 个样本和 $d=300$ 维参数的问题，SAGA 可能需要大约 $12$MB 的内存来存储梯度表，而 SARAH 只需要 $1.2$KB。尽管 SAGA 可能实现了略大的[方差缩减](@entry_id:145496)，但 SARAH 在内存效率上高出几个[数量级](@entry_id:264888)。这个例子说明，像 SARAH 这样的低内存算法在资源受限的环境中极具吸[引力](@entry_id:175476)。

### 重要性采样

第二大类[方差缩减技术](@entry_id:141433)是[重要性采样](@entry_id:145704)。其核心思想是：与其均匀地从所有样本中抽样，不如“更智能地”抽样，即更频繁地选择那些对[梯度估计](@entry_id:164549)“更重要”的样本。

#### 重要性采样原理

为了在[非均匀采样](@entry_id:752610)下保持[梯度估计](@entry_id:164549)的无偏性，我们需要对采样的梯度进行加权。如果我们以[概率分布](@entry_id:146404) $p = (p_1, \dots, p_n)$ 采样索引 $i$，那么无偏的[梯度估计](@entry_id:164549)量为：
$$
g(x;i) = \frac{1}{n p_i} \nabla f_i(x)
$$
它的期望为 $\mathbb{E}_{i \sim p}[g(x;i)] = \sum_{i=1}^n p_i \frac{1}{n p_i} \nabla f_i(x) = \frac{1}{n}\sum_{i=1}^n \nabla f_i(x) = \nabla f(x)$。

我们的目标是选择一个[概率分布](@entry_id:146404) $p$ 来最小化这个[估计量的方差](@entry_id:167223)。最小化[方差](@entry_id:200758)等价于最小化其二阶矩 $\mathbb{E}[\|g(x;i)\|^2]$。这个二阶矩可以表示为：
$$
\mathbb{E}[\|g(x;i)\|^2] = \sum_{i=1}^n p_i \left\|\frac{1}{n p_i} \nabla f_i(x)\right\|^2 = \frac{1}{n^2} \sum_{i=1}^n \frac{\|\nabla f_i(x)\|^2}{p_i}
$$
因此，寻找最优[采样分布](@entry_id:269683)的问题转化为一个约束优化问题：最小化 $\sum_{i=1}^n \frac{\|\nabla f_i(x)\|^2}{p_i}$，约束条件为 $\sum p_i = 1$ 且 $p_i \ge 0$。使用拉格朗日乘子法，我们可以求解得到最优的采样概率 [@problem_id:3197149]：
$$
p_i^{\star} = \frac{\|\nabla f_i(x)\|}{\sum_{j=1}^{n} \|\nabla f_j(x)\|}
$$
这个结果非常直观：**一个样本的最优采样概率应该正比于其当前梯度范数的大小**。梯度范数越大的样本，对整体梯度的贡献越大，也引入了越大的不确定性，因此应该被更频繁地采样以减小[方差](@entry_id:200758)。

然而，这个理论上最优的[分布](@entry_id:182848)在实践中却面临一个致命的挑战：为了计算每个样本的采样概率 $p_i^\star$，我们需要知道所有样本的梯度范数 $\|\nabla f_i(x)\|$。而计算所有这些范数需要对整个数据集进行一次完整的遍历，其代价等同于计算一次全梯度，这完全违背了使用 SGD 的初衷 [@problem_id:3197149]。

#### 实用启发式方法及其陷阱

为了克服计算最优[分布](@entry_id:182848)的巨大开销，研究者们提出了一系列[启发式方法](@entry_id:637904)。

一个自然的想法是使用**过时的（stale）梯度范数**来近似计算采样概率。例如，我们可以像 SVRG 一样，在某个快照点 $\tilde{x}$ 计算一次所有梯度范数 $\|\nabla f_i(\tilde{x})\|$，然后在接下来的多次迭代中都使用基于这些过时范数得到的[分布](@entry_id:182848) $p_i \propto \|\nabla f_i(\tilde{x})\|$。然而，这种方法存在风险。如果当前迭代点 $x$ 已经远离快照点 $\tilde{x}$，那么过时的梯度范数可能与当前的梯度范数大相径庭，使用错误的[分布](@entry_id:182848)甚至可能导致[方差比](@entry_id:162608)均匀采样更大。

一个具体的数值例子可以说明这一点 [@problem_id:3197204]。假设在一个问题中，我们基于过时梯度范数计算出的[重要性采样](@entry_id:145704)概率，与当前梯度范数的真实[分布](@entry_id:182848)存在显著差异。通过精确计算，我们发现使用这个过时[分布](@entry_id:182848)得到的梯度[估计量[方](@entry_id:263211)差](@entry_id:200758)，反而比简单的均匀采样高出了约 $39\%$（[方差比](@entry_id:162608)为 $\frac{317}{228} \approx 1.39$）。这个例子警示我们，使用过时信息进行[重要性采样](@entry_id:145704)必须非常谨慎。

另一个更实用的[启发式方法](@entry_id:637904)是根据每个样本的**梯度 Lipschitz 常数 $L_i$** 进行采样，即 $p_i \propto L_i$。对于许多[损失函数](@entry_id:634569)，如线性回归和逻辑回归中的最小二乘损失，每个样本的 Lipschitz 常数 $L_i$ 是数据本身的属性（例如，对于 $f_i(x) = \frac{1}{2}(a_i^\top x - b_i)^2$，$L_i = \|a_i\|^2$），在整个训练过程中保持不变。这意味着我们可以在训练开始前一次性计算出所有 $L_i$ 并确定[采样分布](@entry_id:269683)，而无需在迭代中更新。

这种方法在 $L_i$ 值[分布](@entry_id:182848)广泛的数据集上尤其有效。当某些数据点的[特征向量](@entry_id:151813)范数（即 $L_i$）远大于其他数据点时，对这些高 $L_i$ 的点进行[过采样](@entry_id:270705)可以显著降低[方差](@entry_id:200758) [@problem_id:3197238]。然而，当所有 $L_i$ 值都差不多时，这种[采样策略](@entry_id:188482)就退化为均匀采样，几乎带不来任何好处。

需要强调的是，采样概率正比于 $L_i$ 只是一个启发式规则，它并不等同于理论最优的 $p_i^\star \propto \|\nabla f_i(x)\|$。事实上，我们可以构造一个例子，其中 $p_i \propto L_i$ 的策略表现得非常差 [@problem_id:3197237]。考虑一个一维问题，其中一个样本具有极高的 $L_i$ 值，但在当前点 $x=0$ 处的梯度值却非常小；而另一些样本 $L_i$ 值不高，但梯度值很大。在这种情况下，$p_i \propto L_i$ 的策略会浪费大量的采样机会在那个梯度很小的样本上，而理论最优的 $p_i^\star \propto \|\nabla f_i(x)\|$ 策略则会正确地关注梯度大的样本。在这个精心设计的例子中，基于 $L_i$ 采样的[方差](@entry_id:200758)可以是基于 $\|\nabla f_i(x)\|$ 采样的三万多倍。这深刻地揭示了实用[启发式方法](@entry_id:637904)与理论最优解之间的差距。

### 一个更精细的要点：有放回与无放回采样

在讨论小批量（mini-batch）SGD 时，我们通常默认采样是**有放回的（with replacement）**，即每个样本都是独立同分布地（i.i.d.）从整个数据集中抽取的。然而，在实践中，一种更常见的做法是**无放回采样（without replacement）**，通常以**随机重排（random reshuffling）**的方式实现：在一个 epoch 开始时，随机打乱整个数据集的顺序，然后按顺序依次取出小批量数据进行训练，直到遍历完所有数据。

这两种采样方式在统计特性上存在微妙但重要的差异。对于一个大小为 $N$ 的总体，其[方差](@entry_id:200758)为 $\sigma^2$。当我们从中抽取一个大小为 $b$ 的小批量时：
-   有放回采样的样本均值的[方差](@entry_id:200758)为 $\frac{\sigma^2}{b}$。
-   无放回采样的样本均值的[方差](@entry_id:200758)为 $\frac{\sigma^2}{b} \frac{N-b}{N-1}$。

这里的 $\frac{N-b}{N-1}$ 被称为**有限总体修正因子**。由于 $b \ge 1$，这个因子总是小于1，这意味着无放回采样的[方差](@entry_id:200758)总是小于有放回采样。直观上，无放回采样保证了每个小批量内的样本都是不同的，从而提供了更多样化的信息，减少了估计的波动性。

这个[方差缩减](@entry_id:145496)的好处可以与[控制变量](@entry_id:137239)等其他技术叠加。我们可以推导，在使用[最优控制变量](@entry_id:752974)系数的情况下，两种采样方案的最小化[方差](@entry_id:200758)之比为 [@problem_id:3197164]：
$$
\frac{\text{Var}^*_{\text{有放回}}}{\text{Var}^*_{\text{无放回}}} = \frac{N-1}{N-b}
$$
这个结果表明，仅仅通过从有放回采样切换到无放回采样（随机重排），我们就能免费获得一个额外的[方差缩减](@entry_id:145496)因子。因此，在实践中，随机重排是比标准有放回采样更受青睐的策略。

### 本章小结

本章深入探讨了 SGD 中梯度[方差](@entry_id:200758)的挑战，并系统介绍了应对这一挑战的两大类技术。

-   **控制变量法**通过引入一个与随机梯度相关且均值已知的辅助变量来构造新的低[方差](@entry_id:200758)[梯度估计](@entry_id:164549)量。SVRG、SAGA 和 SARAH 是该思想的杰出代表，它们在计算开销、内存占用和[收敛速度](@entry_id:636873)之间提供了不同的权衡。
-   **[重要性采样](@entry_id:145704)法**通过非均匀地采样数据来降低[方差](@entry_id:200758)，优先选择那些对梯度贡献更大的“重要”样本。理论上最优的[采样分布](@entry_id:269683)正比于样本的梯度范数，但在实践中计算成本高昂。因此，各种启发式方法（如基于 Lipschitz 常数采样）被广泛使用，但需注意其局限性和潜在风险。

此外，我们还分析了采样机制本身对梯度的影响，揭示了无放回采样（随机重排）相比有放回采样具有内在的[方差](@entry_id:200758)优势。

总而言之，[方差缩减](@entry_id:145496)是现代[大规模优化](@entry_id:168142)的一个核心主题。不存在一种“万能”的最优方法。算法的选择应综合考虑问题的具体结构（如损失函数类型）、数据集的特性（如样本量、特征[分布](@entry_id:182848)）、以及可用的计算和内存资源。理解这些技术背后的基本原理与权衡，对于设计和实现高效、稳健的[优化算法](@entry_id:147840)至关重要。