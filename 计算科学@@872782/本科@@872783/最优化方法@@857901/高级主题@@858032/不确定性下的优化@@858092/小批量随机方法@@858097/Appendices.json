{"hands_on_practices": [{"introduction": "小批量方法的核心思想是用一个小样本来估计全体数据的梯度。然而，并非所有样本都具有相同的信息量；有些样本可能比其他样本引入更多的噪声（即方差）。本练习将探讨一个基本问题：如果我们知道单个梯度估计的方差，应如何组合它们以形成最稳定（即方差最小）的小批量梯度？通过解决这个问题，你将推导出逆方差加权（inverse-variance weighting）的原理，这是最优数据融合的基石之一。[@problem_id:3150573]", "problem": "考虑单参数模型下的小批量（mini-batch）随机梯度下降（Stochastic Gradient Descent, SGD）。设一个小批量由索引集 $B=\\{1,2,3\\}$ 表示，对于每个 $i \\in B$，令 $X_i$ 是一个标量随机梯度估计，满足 $\\mathbb{E}[X_i]=\\mu$（无偏性）和 $\\operatorname{Var}(X_i)=\\sigma_i^{2}$。假设随机变量 $X_i$ 相互独立。构建一个加权小批量梯度估计量 $\\widehat{g}=\\sum_{i \\in B} w_i X_i$，其中每个样本的权重满足约束 $\\sum_{i \\in B} w_i=1$ 以保持无偏性，即 $\\mathbb{E}[\\widehat{g}]=\\mu$。\n\n从期望和方差的定义出发，仅使用独立性和线性的标准性质，推导在约束 $\\sum_{i \\in B} w_i=1$ 下能最小化方差 $\\operatorname{Var}(\\widehat{g})$ 的权重。然后，对于一个方差为 $\\sigma_1^2=1$，$\\sigma_2^2=4$ 和 $\\sigma_3^2=9$ 的小批量，计算最优权重。\n\n将你的最终答案表示为按 $(w_1, w_2, w_3)$ 顺序排列的三个最优权重的行向量。无需四舍五入。", "solution": "用户希望找到一个小批量梯度估计量的最优权重，这些权重能在估计量保持无偏的约束下，使其方差最小化。\n\n首先，我们来验证问题的有效性。\n\n### 步骤 1：提取已知条件\n-   设置是带小批量的随机梯度下降（SGD）。\n-   小批量索引集为 $B=\\{1, 2, 3\\}$。\n-   对于 $i \\in B$，$X_i$ 是标量随机梯度估计。\n-   估计是无偏的：对所有 $i \\in B$，$\\mathbb{E}[X_i]=\\mu$。\n-   每个估计的方差为 $\\operatorname{Var}(X_i)=\\sigma_i^{2}$。\n-   随机变量 $X_i$ 相互独立。\n-   加权小批量梯度估计量为 $\\widehat{g}=\\sum_{i \\in B} w_i X_i$。\n-   权重受约束：$\\sum_{i \\in B} w_i=1$。\n-   该约束的一个结果是估计量是无偏的：$\\mathbb{E}[\\widehat{g}]=\\mu$。\n-   任务是推导在约束条件下最小化 $\\operatorname{Var}(\\widehat{g})$ 的权重 $w_i$。\n-   给出了用于计算的具体方差：$\\sigma_1^2=1$, $\\sigma_2^2=4$ 和 $\\sigma_3^2=9$。\n-   最终答案应为一个权重行向量 $(w_1, w_2, w_3)$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，是机器学习优化方法领域中的一个标准分析。该问题是适定的，因为它描述了一个带有二次目标函数和线性约束的约束优化问题，这保证了唯一解的存在。问题陈述客观，数学定义精确，没有歧义。所有必要信息都已提供，且不存在矛盾。该问题是一个非平凡但可解的优化练习题。\n\n### 步骤 3：结论与行动\n问题是有效的。我们继续进行求解。\n\n问题要求找到权重 $(w_1, w_2, w_3)$，在确保估计量无偏的约束下，最小化加权小批量梯度估计量 $\\widehat{g}$ 的方差。\n\n该估计量被定义为各个随机梯度估计的线性组合：\n$$\n\\widehat{g} = \\sum_{i \\in B} w_i X_i = w_1 X_1 + w_2 X_2 + w_3 X_3\n$$\n权重的约束条件是：\n$$\n\\sum_{i \\in B} w_i = w_1 + w_2 + w_3 = 1\n$$\n首先，我们确认该约束确保了估计量 $\\widehat{g}$ 是无偏的。利用期望的线性性质以及已知条件 $\\mathbb{E}[X_i]=\\mu$：\n$$\n\\mathbb{E}[\\widehat{g}] = \\mathbb{E}\\left[\\sum_{i \\in B} w_i X_i\\right] = \\sum_{i \\in B} w_i \\mathbb{E}[X_i] = \\sum_{i \\in B} w_i \\mu = \\mu \\sum_{i \\in B} w_i\n$$\n在约束 $\\sum_{i \\in B} w_i=1$ 下，我们有 $\\mathbb{E}[\\widehat{g}]=\\mu$。这证实了该估计量保持无偏。\n\n接下来，我们推导 $\\widehat{g}$ 的方差表达式。独立随机变量之和的方差等于它们方差之和。利用性质 $\\operatorname{Var}(aY) = a^2 \\operatorname{Var}(Y)$ 以及 $X_i$ 的相互独立性：\n$$\n\\operatorname{Var}(\\widehat{g}) = \\operatorname{Var}\\left(\\sum_{i \\in B} w_i X_i\\right) = \\sum_{i \\in B} \\operatorname{Var}(w_i X_i) = \\sum_{i \\in B} w_i^2 \\operatorname{Var}(X_i)\n$$\n代入 $\\operatorname{Var}(X_i) = \\sigma_i^2$，我们得到需要最小化的目标函数：\n$$\nV(w_1, w_2, w_3) = w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_3^2 \\sigma_3^2\n$$\n这是一个约束优化问题。我们希望在约束 $g(w_1, w_2, w_3) = w_1 + w_2 + w_3 - 1 = 0$ 下最小化 $V(w_1, w_2, w_3)$。我们使用拉格朗日乘数法。拉格朗日函数 $\\mathcal{L}$ 为：\n$$\n\\mathcal{L}(w_1, w_2, w_3, \\lambda) = V(w_1, w_2, w_3) - \\lambda g(w_1, w_2, w_3)\n$$\n$$\n\\mathcal{L}(w_1, w_2, w_3, \\lambda) = (w_1^2 \\sigma_1^2 + w_2^2 \\sigma_2^2 + w_3^2 \\sigma_3^2) - \\lambda(w_1 + w_2 + w_3 - 1)\n$$\n为求最小值，我们将 $\\mathcal{L}$ 对每个 $w_i$ 和 $\\lambda$ 的偏导数设为零。对于 $i \\in \\{1, 2, 3\\}$：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial w_i} = 2 w_i \\sigma_i^2 - \\lambda = 0\n$$\n解出 $w_i$，我们得到：\n$$\nw_i = \\frac{\\lambda}{2 \\sigma_i^2}\n$$\n这表明每个样本的最优权重与其方差成反比。现在，我们使用约束来求解拉格朗日乘数 $\\lambda$ 的值。我们将 $w_i$ 的表达式代入约束方程 $\\sum_{i=1}^3 w_i = 1$ 中：\n$$\n\\sum_{i=1}^3 \\frac{\\lambda}{2 \\sigma_i^2} = 1\n$$\n$$\n\\frac{\\lambda}{2} \\left( \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2} + \\frac{1}{\\sigma_3^2} \\right) = 1\n$$\n解出 $\\lambda$：\n$$\n\\lambda = \\frac{2}{\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2}}\n$$\n将 $\\lambda$ 的表达式代回 $w_i$ 的方程中：\n$$\nw_i = \\frac{1}{2 \\sigma_i^2} \\left( \\frac{2}{\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2}} \\right) = \\frac{1/\\sigma_i^2}{\\sum_{j=1}^3 1/\\sigma_j^2}\n$$\n这就是最优权重的通用公式，称为反方差加权（inverse-variance weighting）。\n\n现在，我们用给定的方差 $\\sigma_1^2=1$, $\\sigma_2^2=4$ 和 $\\sigma_3^2=9$ 来计算一个具体案例。\n首先，计算反方差之和：\n$$\n\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2} = \\frac{1}{1} + \\frac{1}{4} + \\frac{1}{9}\n$$\n为了对这些分数求和，我们找到一个公分母，即 $36$：\n$$\n\\sum_{j=1}^3 \\frac{1}{\\sigma_j^2} = \\frac{36}{36} + \\frac{9}{36} + \\frac{4}{36} = \\frac{36 + 9 + 4}{36} = \\frac{49}{36}\n$$\n现在我们可以计算每个最优权重 $w_i$：\n对于 $w_1$：\n$$\nw_1 = \\frac{1/\\sigma_1^2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/1}{49/36} = \\frac{1}{49/36} = \\frac{36}{49}\n$$\n对于 $w_2$：\n$$\nw_2 = \\frac{1/\\sigma_2^2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/4}{49/36} = \\frac{1}{4} \\times \\frac{36}{49} = \\frac{9}{49}\n$$\n对于 $w_3$：\n$$\nw_3 = \\frac{1/\\sigma_3^2}{\\sum_{j=1}^3 1/\\sigma_j^2} = \\frac{1/9}{49/36} = \\frac{1}{9} \\times \\frac{36}{49} = \\frac{4}{49}\n$$\n最优权重为 $(w_1, w_2, w_3) = (\\frac{36}{49}, \\frac{9}{49}, \\frac{4}{49})$。作为检验，它们的和是 $\\frac{36}{49} + \\frac{9}{49} + \\frac{4}{49} = \\frac{49}{49} = 1$，满足约束条件。\n\n最终答案是这三个最优权重的行向量。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{36}{49}  \\frac{9}{49}  \\frac{4}{49}\n\\end{pmatrix}\n}\n$$", "id": "3150573"}, {"introduction": "虽然随机抽样通常能提供一个不错的梯度估计，但在最坏情况下会发生什么？本练习挑战你从“对抗者”的视角思考，构建一个能使估计误差最大化的小批量，从而揭示该方法的潜在弱点。此外，它还介绍了一种广泛应用且有效的技术——梯度裁剪（gradient clipping），以增强算法对此类最坏情况和异常值的鲁棒性。[@problem_id:3150590]", "problem": "考虑一个一维的逐样本梯度数据集，由多重集 $\\{-4,-1,0,0,1,5\\}$ 给出，并假设使用大小为 $b=3$ 的小批量来构建一个小批量梯度估计器。令完整数据梯度为所有逐样本梯度的算术平均值 $\\bar{g}$，小批量梯度估计器为所选小批量 $B$ 中梯度的算术平均值 $\\hat{g}(B)$。在标准的小批量随机方法中，当 $B$ 是随机采样时，$\\hat{g}(B)$ 旨在成为 $\\bar{g}$ 的一个低方差估计。在对抗性小批量构建中，对手选择一个固定大小为 $b=3$ 的小批量 $B$，以最大化估计器与真实梯度之间的最坏情况平方偏差，该偏差由均方误差（MSE）度量 $R(B)=(\\hat{g}(B)-\\bar{g})^{2}$ 定义。\n\n从算术平均值的基本定义和平均值的基本性质出发，推导最大化 $R(B)$ 的对抗性选择形式，并计算精确的最坏情况值\n$$\nR^{\\star}=\\max_{\\substack{B\\subset\\{-4,-1,0,0,1,5\\}\\\\|B|=3}}(\\hat{g}(B)-\\bar{g})^{2}.\n$$\n\n作为一种缓解策略，考虑对每个逐样本梯度进行标量梯度裁剪，将其裁剪到区间 $[-\\tau,\\tau]$ 内，阈值为 $\\tau=3$。定义裁剪后的梯度映射 $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$ 和裁剪后的数据集为 $\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}$。令 $\\bar{g}_{\\tau}$ 为裁剪后梯度的均值，$\\hat{g}_{\\tau}(B)$ 为所选小批量中裁剪后梯度的均值。计算精确的最坏情况裁剪值\n$$\nR^{\\star}_{\\tau}=\\max_{\\substack{B\\subset\\{\\phi(g):g\\in\\{-4,-1,0,0,1,5\\}\\}\\\\|B|=3}}(\\hat{g}_{\\tau}(B)-\\bar{g}_{\\tau})^{2}.\n$$\n\n将您的最终答案以包含 $R^{\\star}$ 和 $R^{\\star}_{\\tau}$ 的单行矩阵形式报告，使用精确形式。无需四舍五入。", "solution": "该问题要求计算两个量，它们表示小批量梯度估计器与真实完整数据梯度之间的最坏情况平方偏差。第一个量 $R^{\\star}$ 对应原始数据集，第二个量 $R^{\\star}_{\\tau}$ 对应每个梯度都经过标量裁剪的数据集。\n\n首先，我们分析未裁剪的情况以确定 $R^{\\star}$。\n给定的逐样本梯度数据集是多重集 $S = \\{-4, -1, 0, 0, 1, 5\\}$。样本总数为 $N=6$。\n完整数据梯度 $\\bar{g}$ 是这些值的算术平均值：\n$$\n\\bar{g} = \\frac{1}{N} \\sum_{g \\in S} g = \\frac{1}{6} (-4 + (-1) + 0 + 0 + 1 + 5) = \\frac{1}{6}\n$$\n对于大小为 $b=3$ 的小批量 $B$，其小批量梯度估计器 $\\hat{g}(B)$ 是该小批量中梯度的均值：\n$$\n\\hat{g}(B) = \\frac{1}{b} \\sum_{g \\in B} g = \\frac{1}{3} \\sum_{g \\in B} g\n$$\n我们的任务是找到在所有可能的大小为 3 的小批量 $B$ 上，平方偏差 $R(B) = (\\hat{g}(B) - \\bar{g})^2$ 的最大可能值。该值记为 $R^{\\star}$：\n$$\nR^{\\star} = \\max_{B \\subset S, |B|=3} (\\hat{g}(B) - \\bar{g})^2\n$$\n为了最大化此表达式，对手必须选择一个小批量 $B$，以最大化绝对差 $|\\hat{g}(B) - \\bar{g}|$。当小批量均值 $\\hat{g}(B)$ 尽可能大或尽可能小时，就会出现这种情况。$\\hat{g}(B)$ 的值与 $B$ 中元素的总和成正比。\n\n为了找到最大可能均值 $\\hat{g}_{max}$，我们从 $S$ 中选择 $b=3$ 个最大的梯度值。它们是 $\\{0, 1, 5\\}$。设这个批量为 $B_{max}$。其元素的总和是 $0 + 1 + 5 = 6$。对应的均值是：\n$$\n\\hat{g}(B_{max}) = \\frac{1}{3}(6) = 2\n$$\n为了找到最小可能均值 $\\hat{g}_{min}$，我们从 $S$ 中选择 $b=3$ 个最小的梯度值。它们是 $\\{-4, -1, 0\\}$。设这个批量为 $B_{min}$。其元素的总和是 $-4 + (-1) + 0 = -5$。对应的均值是：\n$$\n\\hat{g}(B_{min}) = \\frac{1}{3}(-5) = -\\frac{5}{3}\n$$\n现在我们评估这两种极端情况下与真实均值 $\\bar{g} = \\frac{1}{6}$ 的偏差：\n$\\hat{g}(B_{max})$ 的偏差是 $2 - \\frac{1}{6} = \\frac{12}{6} - \\frac{1}{6} = \\frac{11}{6}$。\n$\\hat{g}(B_{min})$ 的偏差是 $-\\frac{5}{3} - \\frac{1}{6} = -\\frac{10}{6} - \\frac{1}{6} = -\\frac{11}{6}$。\n因此，最大绝对偏差是 $\\left|\\frac{11}{6}\\right| = \\frac{11}{6}$。\n最坏情况平方偏差是该值的平方：\n$$\nR^{\\star} = \\left(\\frac{11}{6}\\right)^2 = \\frac{121}{36}\n$$\n\n接下来，我们分析裁剪后的情况以确定 $R^{\\star}_{\\tau}$。\n梯度裁剪使用阈值 $\\tau=3$ 和函数 $\\phi(g)=\\max\\{-\\tau,\\min\\{g,\\tau\\}\\}$ 执行。我们将此函数应用于原始数据集 $S$ 中的每个梯度：\n$\\phi(-4) = \\max\\{-3, \\min\\{-4, 3\\}\\} = -3$\n$\\phi(-1) = \\max\\{-3, \\min\\{-1, 3\\}\\} = -1$\n$\\phi(0) = 0$\n$\\phi(1) = 1$\n$\\phi(5) = \\max\\{-3, \\min\\{5, 3\\}\\} = 3$\n裁剪后的数据集是多重集 $S_{\\tau} = \\{-3, -1, 0, 0, 1, 3\\}$。\n裁剪后梯度的均值 $\\bar{g}_{\\tau}$ 是：\n$$\n\\bar{g}_{\\tau} = \\frac{1}{N} \\sum_{g' \\in S_{\\tau}} g' = \\frac{1}{6}(-3 - 1 + 0 + 0 + 1 + 3) = \\frac{0}{6} = 0\n$$\n最坏情况裁剪平方偏差 $R^{\\star}_{\\tau}$ 是在所有来自裁剪后集合 $S_{\\tau}$ 的大小为 3 的小批量 $B$ 上 $(\\hat{g}_{\\tau}(B) - \\bar{g}_{\\tau})^2$ 的最大值。由于 $\\bar{g}_{\\tau} = 0$，这简化为最大化 $(\\hat{g}_{\\tau}(B))^2$。这等价于最大化 $|\\hat{g}_{\\tau}(B)|$。\n和之前一样，我们从 $S_{\\tau}$ 中找到具有最大和最小总和的小批量。\n对于最大总和，我们从 $S_{\\tau}$ 中选择 $b=3$ 个最大的元素：$\\{0, 1, 3\\}$。总和是 $0 + 1 + 3 = 4$。均值是 $\\hat{g}_{\\tau,max} = \\frac{4}{3}$。\n对于最小总和，我们从 $S_{\\tau}$ 中选择 $b=3$ 个最小的元素：$\\{-3, -1, 0\\}$。总和是 $-3 - 1 + 0 = -4$。均值是 $\\hat{g}_{\\tau,min} = -\\frac{4}{3}$。\n极值均值为 $\\frac{4}{3}$ 和 $-\\frac{4}{3}$。最大绝对值为 $\\left|\\frac{4}{3}\\right| = \\frac{4}{3}$。\n最坏情况裁剪平方偏差是该值的平方：\n$$\nR^{\\star}_{\\tau} = \\left(\\frac{4}{3}\\right)^2 = \\frac{16}{9}\n$$\n问题要求以行矩阵形式报告 $R^{\\star}$ 和 $R^{\\star}_{\\tau}$。\n我们得到 $R^{\\star} = \\frac{121}{36}$ 和 $R^{\\star}_{\\tau} = \\frac{16}{9}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{121}{36}  \\frac{16}{9}\n\\end{pmatrix}\n}\n$$", "id": "3150590"}, {"introduction": "随机梯度下降的行为会根据数据属性发生显著变化。这个更深入的问题将带你进入“插值机制”（interpolating regime）——即存在一个完美解可以拟合所有数据点，这种情况在现代深度学习中很常见。你将分析在这种特殊情况下，梯度的随机噪声是如何变化的，并推导出最优学习率，从而更深刻地理解优化算法与数据结构之间的相互作用。[@problem_id:3150601]", "problem": "考虑标量线性回归，数据为 $\\{(a_{i}, b_{i})\\}_{i=1}^{n}$，其中 $a_{i} \\in \\mathbb{R}$ 且 $b_{i} \\in \\mathbb{R}$。定义单个样本损失为 $f_{i}(x) = \\frac{1}{2}\\left(a_{i} x - b_{i}\\right)^{2}$，经验风险为 $f(x) = \\frac{1}{n}\\sum_{i=1}^{n} f_{i}(x)$。假设插值机制成立：存在 $x^{\\ast} \\in \\mathbb{R}$，使得对于所有 $i \\in \\{1,\\dots,n\\}$ 都有 $a_{i} x^{\\ast} = b_{i}$。\n\n考虑使用小批量随机梯度下降 (SGD)，批量大小为 $m \\in \\{1,\\dots,n\\}$，学习率为 $\\eta  0$。在每次迭代 $k$ 中，从 $\\{1,\\dots,n\\}$ 中无放回均匀采样一个大小为 $m$ 的子集 $B_{k}$，更新规则为\n$$\nx_{k+1} \\;=\\; x_{k} \\;-\\; \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x_{k}).\n$$\n令误差为 $e_{k} = x_{k} - x^{\\ast}$。定义 $H = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}^{2}$ 以及集合 $\\{a_{i}^{2}\\}_{i=1}^{n}$ 的有限总体方差 $\\sigma^{2} = \\frac{1}{n}\\sum_{i=1}^{n}\\left(a_{i}^{2} - H\\right)^{2}$。\n\n任务：\n1) 推导 $e_{k+1}$ 关于 $e_{k}$ 和一个依赖于批量的标量的精确小批量动力学，并明确说明为什么在这种插值机制下，随机性在 $x^{\\ast}$ 处会消失。\n\n2) 仅使用关于无放回抽样下样本均值的期望和方差的基本定义和成熟结论，推导 $\\mathbb{E}\\!\\left[e_{k+1}^{2}\\,\\middle|\\, e_{k}\\right]$ 作为 $e_{k}$、$\\eta$、$H$、$\\sigma^{2}$、$n$ 和 $m$ 的函数的显式表达式。\n\n3) 对于固定的 $e_{k} \\neq 0$，确定使 $\\mathbb{E}\\!\\left[e_{k+1}^{2}\\,\\middle|\\, e_{k}\\right]$ 最小化的学习率 $\\eta$，并将其表示为关于 $H$、$\\sigma^{2}$、$n$ 和 $m$ 的封闭形式解析表达式。\n\n将任务3中得到的最小化学习率的表达式作为你的最终答案。不需要进行数值计算。如果得到多种代数等价形式，请以最简封闭形式给出其中任意一种。不要包含单位。", "solution": "该问题提出了在插值机制下的标量线性回归设置，并要求分析采用无放回抽样的小批量随机梯度下降（SGD）。任务涉及推导误差动力学、条件期望平方误差和最优学习率。\n\n首先对问题进行验证。\n**步骤1：提取已知条件**\n- 数据：$\\{(a_{i}, b_{i})\\}_{i=1}^{n}$，其中 $a_{i} \\in \\mathbb{R}$ 且 $b_{i} \\in \\mathbb{R}$。\n- 单个样本损失：$f_{i}(x) = \\frac{1}{2}(a_{i} x - b_{i})^{2}$。\n- 经验风险：$f(x) = \\frac{1}{n}\\sum_{i=1}^{n} f_{i}(x)$。\n- 插值机制：存在 $x^{\\ast} \\in \\mathbb{R}$，使得对于所有 $i \\in \\{1,\\dots,n\\}$ 都有 $a_{i} x^{\\ast} = b_{i}$。\n- 小批量SGD更新：$x_{k+1} = x_{k} - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x_{k})$。\n- 批量大小：$m \\in \\{1,\\dots,n\\}$。\n- 学习率：$\\eta  0$。\n- 采样方案：在每次迭代 $k$ 中，从 $\\{1,\\dots,n\\}$ 中无放回均匀采样一个大小为 $m$ 的批量 $B_{k}$。\n- 误差定义：$e_{k} = x_{k} - x^{\\ast}$。\n- 常数 $H$：$H = \\frac{1}{n}\\sum_{i=1}^{n} a_{i}^{2}$。\n- 有限总体方差 $\\sigma^{2}$：$\\sigma^{2} = \\frac{1}{n}\\sum_{i=1}^{n}(a_{i}^{2} - H)^{2}$。\n\n**步骤2：验证**\n该问题具有科学依据，提法明确且客观。它是优化和机器学习领域的一个标准理论练习，依赖于成熟的数学原理。所有术语都定义清晰，前提是自洽的。该问题并非易事，因为它需要仔细应用与无放回抽样相关的统计性质。未发现任何缺陷。\n\n**结论：该问题有效。**\n\n我们现在开始解答。\n\n**任务1：推导误差动力学并分析在 $x^{\\ast}$ 处的随机性**\n\n首先，我们计算单个样本损失函数 $f_{i}(x)$ 的梯度：\n$$\n\\nabla f_{i}(x) = \\frac{d}{dx} \\left[ \\frac{1}{2}(a_{i} x - b_{i})^{2} \\right] = (a_{i} x - b_{i}) \\cdot a_{i} = a_{i}^{2}x - a_{i}b_{i}.\n$$\n利用插值条件 $b_{i} = a_{i}x^{\\ast}$，我们可以用误差 $e_{k} = x_{k} - x^{\\ast}$ 来表示梯度：\n$$\n\\nabla f_{i}(x_{k}) = a_{i}^{2}x_{k} - a_{i}(a_{i}x^{\\ast}) = a_{i}^{2}(x_{k} - x^{\\ast}) = a_{i}^{2}e_{k}.\n$$\n现在，将此代入小批量SGD更新规则中：\n$$\nx_{k+1} = x_{k} - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x_{k}) = x_{k} - \\frac{\\eta}{m}\\sum_{i \\in B_{k}} (a_{i}^{2}e_{k}).\n$$\n为了找到误差 $e_{k+1} = x_{k+1} - x^{\\ast}$ 的动力学，我们从等式两边减去 $x^{\\ast}$：\n$$\nx_{k+1} - x^{\\ast} = (x_{k} - x^{\\ast}) - \\frac{\\eta}{m}e_{k}\\sum_{i \\in B_{k}} a_{i}^{2}.\n$$\n这就给出了误差的精确小批量动力学：\n$$\ne_{k+1} = e_{k} - \\left(\\frac{\\eta}{m}\\sum_{i \\in B_{k}} a_{i}^{2}\\right) e_{k} = \\left(1 - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} a_{i}^{2}\\right) e_{k}.\n$$\n这个表达式表明 $e_{k+1}$ 与 $e_{k}$ 成正比，比例常数依赖于随机抽样的批量 $B_{k}$。这个依赖于批量的标量是 $\\left(1 - \\eta \\cdot \\frac{1}{m}\\sum_{i \\in B_{k}} a_{i}^{2}\\right)$。\n\n为了说明随机性在 $x^{\\ast}$ 处消失，我们评估在 $x_{k} = x^{\\ast}$ 处的随机梯度。此时误差为 $e_{k} = x^{\\ast} - x^{\\ast} = 0$。\n在 $x^{\\ast}$ 处的单个样本梯度是：\n$$\n\\nabla f_{i}(x^{\\ast}) = a_{i}^{2}(x^{\\ast} - x^{\\ast}) = 0 \\quad \\text{for all } i \\in \\{1,\\dots,n\\}.\n$$\n因此，小批量梯度是：\n$$\n\\frac{1}{m}\\sum_{i \\in B_{k}} \\nabla f_{i}(x^{\\ast}) = \\frac{1}{m}\\sum_{i \\in B_{k}} 0 = 0.\n$$\n这个结果对任何批量 $B_{k}$ 都成立。由于随机梯度恒等于零，并且不依赖于批量的随机选择，因此在解 $x^{\\ast}$ 处随机性消失。在 $x_k = x^*$ 处的更新是 $x_{k+1} = x^* - \\eta \\cdot 0 = x^*$，这是确定性的。\n\n**任务2：推导条件期望平方误差 $\\mathbb{E}[e_{k+1}^{2} | e_{k}]$**\n\n从任务1得到的误差动力学开始：\n$$\ne_{k+1} = \\left(1 - \\eta H_{B_{k}}\\right)e_{k}, \\quad \\text{其中} \\quad H_{B_{k}} = \\frac{1}{m}\\sum_{i \\in B_{k}} a_{i}^{2}.\n$$\n两边平方得：\n$$\ne_{k+1}^{2} = \\left(1 - \\eta H_{B_{k}}\\right)^{2} e_{k}^{2}.\n$$\n我们对 $e_{k}$ 取条件期望。由于在迭代 $k$ 时 $e_{k}$ 是已知的，它被视为一个常数。随机性来自于批量 $B_{k}$。\n$$\n\\mathbb{E}[e_{k+1}^{2} | e_{k}] = \\mathbb{E}\\left[\\left(1 - \\eta H_{B_{k}}\\right)^{2} \\mid e_{k}\\right] e_{k}^{2}.\n$$\n展开平方项并利用期望的线性性质：\n$$\n\\mathbb{E}\\left[\\left(1 - \\eta H_{B_{k}}\\right)^{2}\\right] = \\mathbb{E}\\left[1 - 2\\eta H_{B_{k}} + \\eta^{2} H_{B_{k}}^{2}\\right] = 1 - 2\\eta\\mathbb{E}[H_{B_{k}}] + \\eta^{2}\\mathbb{E}[H_{B_{k}}^{2}].\n$$\n我们需要计算样本均值 $H_{B_{k}}$ 的前两阶矩。令值的总体为 $\\{Z_{i} = a_{i}^{2}\\}_{i=1}^{n}$。总体均值为 $\\mu_{Z} = \\frac{1}{n}\\sum_{i=1}^{n} Z_{i} = H$。总体方差为 $\\frac{1}{n}\\sum_{i=1}^{n}(Z_{i} - \\mu_{Z})^{2} = \\sigma^{2}$，如定义所示。\n\n无放回抽样的样本均值 $H_{B_{k}}$ 的期望是总体均值：\n$$\n\\mathbb{E}[H_{B_{k}}] = H.\n$$\n二阶矩与方差相关：$\\mathbb{E}[H_{B_{k}}^{2}] = \\text{Var}(H_{B_{k}}) + (\\mathbb{E}[H_{B_{k}}])^{2} = \\text{Var}(H_{B_{k}}) + H^{2}$。\n\n我们推导从有限总体中进行无放回抽样时样本均值的方差。令 $Z_{i}$ 为值 $a_i^2$。\n$$\n\\text{Var}(H_{B_{k}}) = \\text{Var}\\left(\\frac{1}{m}\\sum_{i \\in B_{k}} Z_{i}\\right) = \\frac{1}{m^{2}} \\text{Var}\\left(\\sum_{i \\in B_{k}} Z_{i}\\right).\n$$\n令 $I_i$ 为一个指示变量，如果第 $i$ 个元素在样本 $B_k$ 中，则 $I_i=1$，否则 $I_i=0$。我们有 $\\mathbb{E}[I_{i}] = \\frac{m}{n}$，并且对于 $i \\neq j$，有 $\\mathbb{E}[I_{i}I_{j}] = P(i,j \\in B_{k}) = \\frac{m}{n}\\frac{m-1}{n-1}$。\n方差为 $\\text{Var}(\\sum_{i=1}^{n} I_{i}Z_{i}) = \\sum_{i} Z_{i}^{2}\\text{Var}(I_{i}) + \\sum_{i \\neq j} Z_{i}Z_{j}\\text{Cov}(I_{i},I_{j})$。\n一个更直接的推导如下：\n$\\text{Var}(H_{B_k}) = \\mathbb{E}[(H_{B_k}-H)^2] = \\frac{1}{m^2}\\mathbb{E}[(\\sum_{i \\in B_k} (Z_i-H))^2] = \\frac{1}{m^2} \\mathbb{E}[\\sum_{i,j \\in B_k} (Z_i-H)(Z_j-H)]$.\n对于 $i \\neq j$，这等于 $\\frac{1}{m^2} [m \\mathbb{E}[(Z_i-H)^2] + m(m-1)\\mathbb{E}[(Z_i-H)(Z_j-H)]]$。\n样本上的 $\\mathbb{E}[(Z_i-H)^2]$ 与总体方差有关。中心化乘积之和为 $-\\sum (Z_k-H)^2$。\n从大小为 $n$、方差为 $\\sigma^{2}$ 的总体中无放回抽取大小为 $m$ 的样本，其样本均值的方差标准公式为：\n$$\n\\text{Var}(H_{B_{k}}) = \\frac{\\sigma^{2}}{m}\\left(\\frac{n-m}{n-1}\\right).\n$$\n其中 $\\sigma^2$ 是用 $1/n$ 因子定义的总体方差。该公式包含了有限总体修正因子 $\\frac{n-m}{n-1}$。\n\n现在我们可以计算 $\\mathbb{E}[H_{B_{k}}^{2}]$：\n$$\n\\mathbb{E}[H_{B_{k}}^{2}] = \\text{Var}(H_{B_{k}}) + H^{2} = \\frac{\\sigma^{2}}{m}\\left(\\frac{n-m}{n-1}\\right) + H^{2}.\n$$\n将矩代回期望平方误差的表达式中：\n$$\n\\mathbb{E}[e_{k+1}^{2} | e_{k}] = \\left[1 - 2\\eta\\mathbb{E}[H_{B_{k}}] + \\eta^{2}\\mathbb{E}[H_{B_{k}}^{2}]\\right]e_{k}^{2}\n$$\n$$\n\\mathbb{E}[e_{k+1}^{2} | e_{k}] = \\left[1 - 2\\eta H + \\eta^{2}\\left(H^{2} + \\frac{\\sigma^{2}}{m}\\frac{n-m}{n-1}\\right)\\right]e_{k}^{2}.\n$$\n这是条件期望平方误差的最终表达式。\n\n**任务3：找到最优学习率 $\\eta$**\n\n为了找到对于固定 $e_{k} \\neq 0$ 时能最小化 $\\mathbb{E}[e_{k+1}^{2} | e_{k}]$ 的学习率 $\\eta$，我们需要最小化以下关于 $\\eta$ 的函数：\n$$\ng(\\eta) = 1 - 2\\eta H + \\eta^{2}\\left(H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}\\right).\n$$\n这是一个关于 $\\eta$ 的二次函数。由于 $H^{2} \\ge 0$，$\\sigma^{2} \\ge 0$，$n \\ge m \\ge 1$，$\\eta^{2}$ 项的系数为正，因此抛物线开口向上，存在唯一的最小值。我们通过将其关于 $\\eta$ 的导数设为零来求最小值：\n$$\n\\frac{dg}{d\\eta} = -2H + 2\\eta\\left(H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}\\right) = 0.\n$$\n解出 $\\eta$：\n$$\n2\\eta\\left(H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}\\right) = 2H,\n$$\n$$\n\\eta_{\\text{opt}} = \\frac{H}{H^{2} + \\frac{\\sigma^{2}(n-m)}{m(n-1)}}.\n$$\n为了将其表示为分母中不含复合分数的单分数形式，我们可以将分子和分母同乘以 $m(n-1)$：\n$$\n\\eta_{\\text{opt}} = \\frac{m(n-1)H}{m(n-1)H^{2} + \\sigma^{2}(n-m)}.\n$$\n这就是最小化单步期望平方误差的学习率的封闭形式表达式。", "answer": "$$\\boxed{\\frac{m(n-1)H}{m(n-1)H^{2} + (n-m)\\sigma^{2}}}$$", "id": "3150601"}]}