## 引言
在现代机器学习，尤其是训练驱动人工智能发展的大规模深度学习模型中，小批量随机方法（Mini-batch Stochastic Methods）已成为不可或缺的基石。作为优化算法的“主力军”，它在几乎所有的模型训练过程中都扮演着核心角色。然而，其成功的背后隐藏着一个根本性的挑战：如何在纯[随机梯度下降](@entry_id:139134)（SGD）的快速迭代与全[批量梯度下降](@entry_id:634190)的精确梯度之间找到最佳[平衡点](@entry_id:272705)？简单地选择一个[批量大小](@entry_id:174288)往往不足以应对复杂的优化难题，深入理解其内在机制、权衡利弊以及与其它系统组件的复杂互动，是释放其全部潜能的关键。

本文旨在系统性地剖析小批量随机方法。我们首先将在“原理与机制”一章中，深入探讨控制梯度[方差](@entry_id:200758)与计算成本的核心权衡，揭示[批量大小](@entry_id:174288)、学习率以及样本相关性之间的数学关系，并介绍模拟退火、分层采样等高级策略。接着，在“应用与交叉学科联系”一章，我们将展示这些理论如何在[分布](@entry_id:182848)式训练、物理信息神经网络等前沿领域中发挥作用，并探索其与[统计物理学](@entry_id:142945)和[进化生物学](@entry_id:145480)等学科的深刻联系。最后，通过“动手实践”部分提供的一系列练习，您将有机会亲手应用并巩固所学知识。让我们从理解这一强大优化工具的根本原理开始。

## 原理与机制

在[梯度下降](@entry_id:145942)的背景下，小批量（mini-batch）随机方法是一种核心的折衷策略。它旨在平衡两种极端情况的优缺点：其一是使用整个数据集计算梯度的全批量（full-batch）[梯度下降](@entry_id:145942)，其二是每次仅使用单个样本的[随机梯度下降](@entry_id:139134)（Stochastic Gradient Descent, SGD）。全批量方法能提供精确的梯度方向，但每一步的计算成本高昂，尤其是在处理海量数据集时；而纯粹的SGD计算速度快，但其[梯度估计](@entry_id:164549)包含巨大的[方差](@entry_id:200758)，导致优化路径非常嘈杂。小批量方法通过在每一步中使用一个小[子集](@entry_id:261956)的数据来估计梯度，试图在计算效率和[梯度估计](@entry_id:164549)的准确性之间找到一个最佳[平衡点](@entry_id:272705)。本章将深入探讨控制这种平衡的根本原理，并揭示小批量方法背后的关键机制。

### 小批量方法的核心权衡：[方差](@entry_id:200758)与计算成本

小批量方法的基本思想是通过平均一组样本的梯度来降低估计的[方差](@entry_id:200758)。如果从小批量中抽取的$b$个样本是独立同分布（i.i.d.）的，并且每个样本梯度的[方差](@entry_id:200758)[协方差矩阵](@entry_id:139155)为$\Sigma$，那么小批量梯度[估计量的[方](@entry_id:167223)差](@entry_id:200758)就会减少$b$倍，即$\frac{1}{b}\Sigma$。这种[方差](@entry_id:200758)的减小是小批量方法相对于单样本SGD最主要的优势，它使得优化过程更稳定，并允许使用更大的[学习率](@entry_id:140210)。

然而，“独立同分布”的假设在实践中可能并不成立，特别是在处理具有结构化依赖性的数据时（例如，时间序列数据、视频帧或通过复杂[数据增强](@entry_id:266029)流程生成的数据）。当小批量内部的样本存在相关性时，[方差](@entry_id:200758)的减小效应会受到限制。

我们可以通过一个简单的模型来量化这种影响。假设一个小批量由$m$个相关的样本贡献$X_1, X_2, \dots, X_m$构成，它们共同用于估计某个梯度分量。设每个$X_i$具有相同的均值$\mu$和[方差](@entry_id:200758)$\sigma^2$。如果样本之间不是独立的，而是存在一个共同的成对[相关系数](@entry_id:147037)$\rho$（即对所有$i \neq j$，$\operatorname{Corr}(X_i, X_j) = \rho$），那么小批量均值估计$G = \frac{1}{m} \sum_{i=1}^{m} X_{i}$的[方差](@entry_id:200758)将不再是简单的$\frac{\sigma^2}{m}$。

通过[方差](@entry_id:200758)的基本定义，我们可以推导出其有效[方差](@entry_id:200758)$\sigma_{\text{eff}}^2$ [@problem_id:3150647]：
$$
\sigma_{\text{eff}}^2 = \operatorname{Var}(G) = \operatorname{Var}\left(\frac{1}{m}\sum_{i=1}^m X_i\right) = \frac{1}{m^2} \left( \sum_{i=1}^m \operatorname{Var}(X_i) + \sum_{i \neq j} \operatorname{Cov}(X_i, X_j) \right)
$$
代入$\operatorname{Var}(X_i) = \sigma^2$和$\operatorname{Cov}(X_i, X_j) = \rho\sigma^2$，我们得到：
$$
\sigma_{\text{eff}}^2 = \frac{1}{m^2} (m\sigma^2 + m(m-1)\rho\sigma^2) = \sigma^2 \frac{1+(m-1)\rho}{m}
$$
这个结果揭示了一个重要的现象。当样本独立时（$\rho=0$），我们回到熟悉的$\sigma_{\text{eff}}^2 = \frac{\sigma^2}{m}$。然而，当存在正相关性（$\rho > 0$）时，随着[批量大小](@entry_id:174288)$m$的增加，[方差](@entry_id:200758)并不会趋近于零，而是趋近于一个下限$\rho\sigma^2$。这意味着，如果批量内的样本高度相关（$\rho \to 1$），增加[批量大小](@entry_id:174288)带来的[方差](@entry_id:200758)减小收益会迅速饱和，整个小批量表现得就像一个单独的样本。因此，构建小批量时，确保样本的多样性和独立性是至关重要的。

### [批量大小](@entry_id:174288)的角色：何时“越大越好”？

选择合适的[批量大小](@entry_id:174288)是应用小批量方法的关键。直观上，更大的批量可以提供更准确的[梯度估计](@entry_id:164549)，但计算成本也更高。那么，增加[批量大小](@entry_id:174288)的收益是否存在一个[临界点](@entry_id:144653)？答案是肯定的，这个[临界点](@entry_id:144653)由所谓的**噪声尺度（noise scale）**决定。

噪声尺度$S$定义为单样本梯度[方差](@entry_id:200758)与当前真实梯度范数平方的比值，即$S = \frac{\sigma^2}{\|\nabla f(w)\|^2}$。这个无量纲的数衡量了[梯度噪声](@entry_id:165895)相对于梯度信号的强度。当噪声尺度很大时（例如，在优化的初始阶段，梯度信号很强，或者在平坦区域，梯度信号很弱），[梯度估计](@entry_id:164549)主要被噪声主导。反之，当噪声尺度很小时（例如，接近最小值时，梯度信号很弱），信号可能淹没在噪声中。

我们可以通过一个简单的单维[线性回归](@entry_id:142318)模型来精确地理解[批量大小](@entry_id:174288)$b$与噪声尺度$S$之间的关系[@problem_id:3150559]。考虑目标函数$F(u) = \frac{1}{2}u^2$，其中$u$是与最优参数的偏差。经过推导可以发现，在使用最优[学习率](@entry_id:140210)时，SGD单步更新带来的期望目标函数下降量为：
$$
(\Delta F)_{\text{opt}} = \frac{1}{2} \frac{u^2}{1 + S/b}
$$
这个公式清晰地揭示了[批量大小](@entry_id:174288)$b$的作用：
-   当[批量大小](@entry_id:174288)远小于噪声尺度时（$b \ll S$），分母中的$S/b$项占主导地位，此时$(\Delta F)_{\text{opt}} \approx \frac{1}{2}\frac{u^2}{S/b} = \frac{b}{2S}u^2$。在这种情况下，优化的进展与[批量大小](@entry_id:174288)$b$成正比。加倍[批量大小](@entry_id:174288)，优化的进展也几乎加倍。
-   当[批量大小](@entry_id:174288)远大于噪声尺度时（$b \gg S$），分母中的$1$占主导地位，此时$(\Delta F)_{\text{opt}} \approx \frac{1}{2}u^2$。这对应于一个无噪声的梯度下降步骤所能实现的最大进展。在这种情况下，继续增加[批量大小](@entry_id:174288)几乎不会带来任何额外收益。

因此，当[批量大小](@entry_id:174288)$b$达到并超过噪声尺度$S$时，增加[批量大小](@entry_id:174288)所带来的回报开始递减。这为选择[批量大小](@entry_id:174288)提供了一个重要的理论指导：一个有效的[批量大小](@entry_id:174288)应该与当前优化点的噪声尺度相匹配。例如，在一个具体的[线性回归](@entry_id:142318)问题中，如果计算出在某点的噪声尺度$S=3$，那么选择$b \approx 3$的[批量大小](@entry_id:174288)将达到一个效率和收益的“拐点”，更大的批量可能不会带来显著的加速[@problem_id:3150559]。

### 先进的批量构建与调度策略

除了简单地选择一个固定的[批量大小](@entry_id:174288)，我们还可以设计更复杂的策略来构建和调整批量，以进一步提升优化效率和效果。

#### 智能采样：通过分层降低[方差](@entry_id:200758)

标准的随机采样假设所有数据点对[梯度估计](@entry_id:164549)的贡献是同质的。但在许多实际问题中，数据集可以被划分为具有不同统计特性的[子集](@entry_id:261956)或**分层（strata）**。例如，一个图像[分类数据](@entry_id:202244)集可能包含一些容易分类的“干净”图像和一些难以分类的“模糊”图像。后者对应的梯度[方差](@entry_id:200758)可能远大于前者。

在这种情况下，我们可以使用**分层采样（stratified sampling）**来构建更有效的小批量。其核心思想是，与其在整个数据集上进行简单的[随机抽样](@entry_id:175193)，不如按比例从每个分层中抽样，并给予高[方差](@entry_id:200758)的分层更多的抽样名额，从而以相同的总[批量大小](@entry_id:174288)获得更精确的[梯度估计](@entry_id:164549)。

具体来说，假设我们将数据集划分为$K$个分层，每个分层的权重（占总数据集的比例）为$W_k$，层内梯度某个投影分量的[方差](@entry_id:200758)为$\sigma_k^2$。我们的目标是分配总[批量大小](@entry_id:174288)$b$到每个分层，即确定每个分层的样本数$n_k$（$\sum n_k = b$），以最小化分层加权[梯度估计](@entry_id:164549)量的总[方差](@entry_id:200758)$\operatorname{Var}(\widehat{\mu}_b) = \sum_{k=1}^K \frac{W_k^2 \sigma_k^2}{n_k}$。

通过拉格朗日乘子法可以求解这个[约束优化](@entry_id:635027)问题，得到最优的样本分配方案，即**[Neyman分配](@entry_id:634618)（Neyman allocation）**[@problem_id:3150558]：
$$
n_k^\star = b \frac{W_k \sigma_k}{\sum_{j=1}^K W_j \sigma_j}
$$
这个公式表明，分配给一个分层的样本数量应该正比于该分层的权重$W_k$与其[标准差](@entry_id:153618)$\sigma_k$的乘积。直观地说，我们应该更多地关注那些规模大（$W_k$大）且内部“意见不统一”（$\sigma_k$大）的[子群](@entry_id:146164)体。例如，如果一个数据集被分为三层，其权重和[标准差](@entry_id:153618)分别为$(W_1, W_2, W_3) = (0.5, 0.3, 0.2)$和$(\sigma_1, \sigma_2, \sigma_3) = (4, 2, 1)$，那么最优的采样比例将是$p_k = n_k^\star / b \propto W_k \sigma_k$。计算可得，最优比例为$(\frac{5}{7}, \frac{3}{14}, \frac{1}{14})$，这意味着我们应该将超过70%的批量预算分配给[方差](@entry_id:200758)最大的第一层，即使它只占数据集的一半[@problem_id:3150558]。

#### [自适应学习率](@entry_id:634918)与[批量大小](@entry_id:174288)：平方根缩放定律

在实践中，一个常见的问题是：当我们改变[批量大小](@entry_id:174288)时，是否应该以及如何调整学习率？一个广为流传的启发式法则是**[线性缩放](@entry_id:197235)规则（linear scaling rule）**：当[批量大小](@entry_id:174288)乘以$k$时，[学习率](@entry_id:140210)也乘以$k$。然而，这个规则有其局限性，更精细的分析指向了另一个重要的关系——**平方根缩放定律（square-root scaling law）**。

这个定律可以通过不同的角度来推导。一个直观的推导方式是要求由[梯度噪声](@entry_id:165895)引起的参数更新的“波动幅度”在不同[批量大小](@entry_id:174288)下保持恒定。小批量更新可以分解为一个确定性部分（沿真实梯度方向）和一个随机噪声部分。随机部分的[均方根](@entry_id:263605)（RMS）大小与$\frac{\eta(b)}{\sqrt{b}}$成正比，其中$\eta(b)$是与[批量大小](@entry_id:174288)$b$相关的学习率。为了使这个波动幅度与$b$无关，我们必须让$\eta(b) \propto \sqrt{b}$。如果我们定义$b=1$时的[学习率](@entry_id:140210)为$\eta_0$，那么这个关系就可以写成$\eta(b) = \eta_0 \sqrt{b}$ [@problem_id:3150663]。

一个更严谨的推导来自于分析SGD在二次[目标函数](@entry_id:267263)上的[稳态](@entry_id:182458)行为[@problem_id:3150574]。在这种模型下，我们可以精确地计算出算法收敛后参数的[稳态](@entry_id:182458)[均方误差](@entry_id:175403)（MSE）以及每一步更新的期望平方大小。分析表明，[稳态](@entry_id:182458)MSE是[学习率](@entry_id:140210)$\eta$的增函数。如果我们要求算法在满足一定的“最小进展”（即期望更新的平方大小不小于某个阈值$u$）的前提下，最小化[稳态](@entry_id:182458)MSE，我们可以解出一个最优的学习率$\eta(b)$。在噪声主导的场景下（这在[随机优化](@entry_id:178938)中很常见），这个最优[学习率](@entry_id:140210)的渐近形式恰好是$\eta(b) \propto \sqrt{b}$。这个结果为平方根缩放定律提供了坚实的理论基础，其精确形式为：
$$
\eta(b) = \frac{-\,u\,b\,h + \sqrt{\,u^{2}\,b^{2}\,h^{2} + 16\,\sigma^{2}\,u\,b\,}}{4\,\sigma^{2}}
$$
其中$h$是目标[函数的曲率](@entry_id:173664)，$u$是最小进展要求，$\sigma^2$是[梯度噪声](@entry_id:165895)[方差](@entry_id:200758)[@problem_id:3150574]。

#### 作为[退火](@entry_id:159359)参数的[批量大小](@entry_id:174288)：逃离局部最小值

小批量SGD的动态行为与物理学中的扩散过程有着深刻的类比。SGD的迭代路径可以被近似为一个在由目标函数$f(x)$定义的[势能面](@entry_id:147441)中运动的粒子，其运动轨迹由一个随机微分方程（SDE）描述。这个SDE包含一个向着[势能](@entry_id:748988)谷底移动的“漂移项”$(-\nabla f(x)dt)$和一个由[梯度噪声](@entry_id:165895)引起的随机“[扩散](@entry_id:141445)项”$(\sqrt{2T}dW_t)$。

通过匹配SGD的SDE近似与标准的[朗之万动力学](@entry_id:142305)（Langevin dynamics）方程，我们可以建立一个[批量大小](@entry_id:174288)$b$和系统**[有效温度](@entry_id:161960)（effective temperature）**$T$之间的关系。对于固定的学习率$\eta$，这个关系是[@problem_id:3150634]：
$$
T \propto \frac{\eta}{b}
$$
这个关系极具启发性。它告诉我们：
-   **小批量（small batch）对应高温**：系统具有很高的随机能量，能够轻松越过势垒，探索整个[损失函数](@entry_id:634569)地貌。这有利于在优化的早期阶段逃离糟糕的局部最小值，进行全局性的探索。
-   **大批量（large batch）对应低温**：系统的随机性减弱，倾向于滑向并稳定在它当前所在的势能谷底。这有利于在优化的[后期](@entry_id:165003)阶段进行精细的收敛。

这个类比启发了一种强大的策略：**通过动态调整[批量大小](@entry_id:174288)来实现[模拟退火](@entry_id:144939)（simulated annealing）**。我们可以从一个小批量开始（高温），让算法充分探索；然后逐渐增大小批量（降温），使算法稳定地收敛到一个好的（可能是全局的）最小值。经典的模拟退火理论指出，为了保证收敛到全局最优解，温度的下降必须非常缓慢。一个满足条件的经典降温方案是$T_k \propto \frac{1}{\log k}$，其中$k$是迭代次数。根据$T \propto 1/b$的关系，这对应于一个对数增长的[批量大小](@entry_id:174288)调度方案：$b_k = b_0 \log(k+1)$。这种策略将SGD从一个局部优化器转变为一个具有[全局收敛](@entry_id:635436)保证的算法[@problem_id:3150634]。

### 现代[深度学习](@entry_id:142022)中的小批量方法：复杂的相互作用

在训练复杂的深度神经网络时，小批量方法的行为会变得更加微妙，因为它与网络架构、数据特性和优化器本身产生了复杂的相互作用。

#### 数据[异质性](@entry_id:275678)的挑战

真实世界的数据集往往是异质的，不同样本可能对优化过程产生截然不同的影响。一个特别的挑战来自于**平滑度离群点（smoothness outliers）**，即某些样本的[损失函数](@entry_id:634569)具有异常大的曲率（或[Lipschitz常数](@entry_id:146583)$L_i$）。

考虑一个简单的二次模型$f_i(x) = \frac{1}{2}L_i x^2$，其中大部分样本具有较小的$L_L$，但有一个离群点具有非常大的$L_H$。当一个小批量不幸地恰好抽中了这个离群点时，该批量的平均平滑度会急剧升高。为了保证算法在任何可能的批量组合下都[稳定收敛](@entry_id:199422)（即保证更新是[压缩映射](@entry_id:139989)），[学习率](@entry_id:140210)$\eta$必须足够小，以适应这种“最坏情况”的批量。

通过分析[收敛条件](@entry_id:166121)$|1 - \frac{\eta}{b}\sum_{i \in B_t} L_i|  1$，我们可以推导出保证稳定性的最大[学习率](@entry_id:140210)。这个最大[学习率](@entry_id:140210)由使得$\sum_{i \in B_t} L_i$最大的批量决定，也就是包含那个$L_H$离群点和$b-1$个$L_L$样本的批量。最终，我们得到一个稳健的学习率[上界](@entry_id:274738)[@problem_id:3150655]：
$$
\eta_{\text{robust}} = \frac{2b}{L_H + (b-1)L_L}
$$
这个结果警示我们，单个“行为恶劣”的数据点就可能严重限制整个优化过程的学习率，即使它在数据集中非常罕见。这凸显了[数据预处理](@entry_id:197920)、[离群点检测](@entry_id:175858)以及设计对异质性不敏感的[优化算法](@entry_id:147840)的重要性。

#### [批量大小](@entry_id:174288)、噪声与曲率的相互作用

梯度的噪声并非总是各向同性（isotropic）的。在多维空间中，不同方向上的梯度[方差](@entry_id:200758)可能截然不同。这种**噪声各向异性（anisotropy）**与[损失函数](@entry_id:634569)地貌的**曲率各向异性**（即Hessian矩阵的[特征值分布](@entry_id:194746)不均）相互作用，共同决定了SGD的收敛动态。

我们可以通过将SGD的动态分解到Hessian矩阵的[特征向量基](@entry_id:163721)上进行分析。设$H$的第$i$个[特征向量](@entry_id:151813)为$u_i$，对应的[特征值](@entry_id:154894)（曲率）为$\lambda_i$，[梯度噪声](@entry_id:165895)在$u_i$方向上的[方差](@entry_id:200758)为$s_i$。在[稳态](@entry_id:182458)下，参数沿该方向的期望平方误差$e_i^\star$为[@problem_id:3150660]：
$$
e_i^\star = \frac{\eta s_i}{b\lambda_i(2 - \eta\lambda_i)}
$$
这个公式揭示了精细的相互作用：
-   稳态误差与方向噪声$s_i$成正比：噪声越大的方向，收敛越不精确。
-   稳态误差与曲率$\lambda_i$成反比：曲率越小（方向越平坦），收敛越不精确。这是因为在平坦方向上，梯度信号微弱，恢复力不足以抵抗噪声。
-   稳态误差与[批量大小](@entry_id:174288)$b$成反比：增加[批量大小](@entry_id:174288)可以按比例减小所有方向上的误差。

这种分析进一步引出了[批量大小](@entry_id:174288)与所发现的解的“质量”之间的联系。[深度学习](@entry_id:142022)中的一个普遍观察是，SGD倾向于收敛到**平坦最小值（flat minima）**，而不是**尖锐最小值（sharp minima）**。平坦最小值通常具有更好的泛化性能。平坦度可以通过Hessian[矩阵的迹](@entry_id:139694)$\tau = \operatorname{tr}(H)$来衡量，迹越小，最小值越平坦。

SGD在最小值附近并不会静止，而是在一个由噪声驱动的区域内持续“探索”。这个探索区域的大小（或半径）与[批量大小](@entry_id:174288)有关。通过一个简化的[Lyapunov方程](@entry_id:165178)分析，我们可以建立起[批量大小](@entry_id:174288)$b$、Hessian迹$\tau$和[稳态](@entry_id:182458)探索半径$r$之间的关系。在某些假设下，要维持一个固定的探索半径$r^2 = \operatorname{tr}(S)$（其中$S$是参数的[稳态](@entry_id:182458)协方差矩阵），所需的[批量大小](@entry_id:174288)为[@problem_id:3150555]：
$$
b = \frac{\eta \sigma^{2} d^{2}}{2 \tau r^{2}}
$$
这个关系表明，$b$与$\tau$成反比。这意味着，为了在更平坦的最小值（$\tau$更小）附近维持相同的探索压力（固定的$r$），需要更大的[批量大小](@entry_id:174288)$b$。反过来看，对于一个固定的[批量大小](@entry_id:174288)$b$，SGD在更尖锐的最小值（$\tau$更大）周围会进行更小范围的探索，而在更平坦的最小值周围探索范围更大，这或许可以解释为什么SGD更容易“发现”并稳定在平坦的区域。

#### 批量依赖计算的偏差：以[批量归一化](@entry_id:634986)为例

小批量方法的一个基本假设是，小批量梯度是全批量梯度的无偏估计。然而，在现代[深度学习](@entry_id:142022)网络中，这一假设常常被违反。一个典型的例子就是**[批量归一化](@entry_id:634986)（Batch Normalization, BN）**。

BN层通过减去批量均值和除以批量[标准差](@entry_id:153618)来归一化其输入，然后再进行[仿射变换](@entry_id:144885)。这意味着，对于批量中的任何一个样本$x_i$，其输出不仅依赖于$x_i$本身，还依赖于同一批量中的所有其他样本$x_j$（$j \neq i$）。当反向传播计算BN层参数（如缩放因子$\gamma$和偏移因子$\beta$）的梯度时，这种依赖性导致了一个问题：计算出的梯度是关于当前这个特定小批量的统计量（$\hat{\mu}_B, \hat{v}_B$）的梯度，而不是关于整个数据集的统计量（$\mu, \sigma^2$）的梯度。

因此，BN层引入的随机梯度是一个**有偏（biased）**估计。我们可以精确地计算出这个偏差。例如，对于一个只包含缩放因子$\gamma$的BN层和一个$\ell(\gamma) \propto (\gamma r_i)^4$的损失函数（其中$r_i$是[批量归一化](@entry_id:634986)的激活值），小批量梯度$g_{\text{mb}}$的[期望值](@entry_id:153208)与真实总体梯度之间的偏差为[@problem_id:3150658]：
$$
b(m, \gamma, \lambda) = \mathbb{E}[g_{\text{mb}}] - \frac{d\mathcal{L}_{\text{pop}}}{d\gamma} = -\frac{6\lambda\gamma^3}{m+1}
$$
其中$m$是[批量大小](@entry_id:174288)。这个偏差是负的，意味着朴素的小批量[梯度系统](@entry_id:275982)性地低估了真实梯度。偏差的大小随着[批量大小](@entry_id:174288)$m$的增加而减小，趋近于零。这个分析揭示了BN的一个微妙之处：[批量大小](@entry_id:174288)不仅影响梯度[方差](@entry_id:200758)，还直接影响其[期望值](@entry_id:153208)。幸运的是，对于这个特定的模型，我们可以导出一个修正因子$c(m) = \frac{m+1}{m-1}$，通过使用修正后的梯度$g_{\text{corr}} = c(m) g_{\text{mb}}$来消除这种偏差。这提醒我们在设计和分析包含批量依赖操作的复杂模型时，必须审慎考察[梯度估计](@entry_id:164549)的无偏性。