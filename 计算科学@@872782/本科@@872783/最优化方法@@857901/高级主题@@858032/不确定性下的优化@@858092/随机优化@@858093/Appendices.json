{"hands_on_practices": [{"introduction": "在现实世界中，我们如何在旅行时间充满不确定性的情况下，规划出最优路线？样本平均近似 (Sample Average Approximation, SAA) 方法为这类问题提供了一个实用的解决框架，它将随机优化问题转化为确定性优化问题。这项练习 [@problem_id:2182114] 将在一个相关的物流场景中指导你应用 SAA 方法，通过使用模拟数据来估计未知的期望旅行时间，从而将一个复杂的随机问题简化为一个标准的最短路径问题并求解。", "problem": "一家物流公司正在使用模拟来优化一个拥堵的小城区的送货路线。该城区的道路网络可以建模为由四个交叉口（标记为1、2、3和4）和五条双向路段组成的集合。这些路段是 (1,2)、(1,3)、(2,3)、(2,4) 和 (3,4)。\n\n为了考虑交通的可变性，该公司运行了五次模拟，每次模拟代表一种可能的交通情景。下表给出了在每种情景下，穿过每个路段所需的行程时间（以分钟为单位）。\n\n| 路段 | 情景 1 | 情景 2 | 情景 3 | 情景 4 | 情景 5 |\n|--------------|------------|------------|------------|------------|------------|\n| (1,2)        | 15         | 25         | 18         | 22         | 20         |\n| (1,3)        | 30         | 12         | 15         | 18         | 25         |\n| (2,3)        | 10         | 8          | 12         | 15         | 5          |\n| (2,4)        | 35         | 40         | 28         | 30         | 32         |\n| (3,4)        | 20         | 25         | 35         | 30         | 20         |\n\n您的任务是找到从交叉口1到交叉口4的最佳路线。您需要使用样本均值近似 (Sample Average Approximation, SAA) 方法。在SAA方法中，每个路段的真实（但未知）期望行程时间由其样本均值近似，该样本均值根据模拟数据计算得出。于是，问题就变成了寻找具有最小总近似行程时间的路径。\n\n根据这个SAA模型，从交叉口1到交叉口4的行程的估计最小平均行程时间是多少？请用分钟作答，并四舍五入到三位有效数字。", "solution": "我们应用样本均值近似 (SAA) 方法：对于每个路段 $e$，我们用5个情景下的样本均值来近似其期望行程时间，\n$$\n\\hat{t}_{e}=\\frac{1}{5}\\sum_{s=1}^{5}t_{e}^{(s)}.\n$$\n计算所有路段的样本均值：\n- 对于 $(1,2)$:\n$$\n\\hat{t}_{12}=\\frac{1}{5}(15+25+18+22+20)=\\frac{100}{5}=20.\n$$\n- 对于 $(1,3)$:\n$$\n\\hat{t}_{13}=\\frac{1}{5}(30+12+15+18+25)=\\frac{100}{5}=20.\n$$\n- 对于 $(2,3)$:\n$$\n\\hat{t}_{23}=\\frac{1}{5}(10+8+12+15+5)=\\frac{50}{5}=10.\n$$\n- 对于 $(2,4)$:\n$$\n\\hat{t}_{24}=\\frac{1}{5}(35+40+28+30+32)=\\frac{165}{5}=33.\n$$\n- 对于 $(3,4)$:\n$$\n\\hat{t}_{34}=\\frac{1}{5}(20+25+35+30+20)=\\frac{130}{5}=26.\n$$\n\n利用这些边的权重，计算从$1$到$4$的简单路径的总时间：\n- 路径 $1\\to 2\\to 4$：$20+33=53$。\n- 路径 $1\\to 3\\to 4$：$20+26=46$。\n- 路径 $1\\to 2\\to 3\\to 4$：$20+10+26=56$。\n- 路径 $1\\to 3\\to 2\\to 4$：$20+10+33=63$。\n\n最小总时间为$46$，通过路径 $1\\to 3\\to 4$。四舍五入到三位有效数字得到$46.0$。", "answer": "$$\\boxed{46.0}$$", "id": "2182114"}, {"introduction": "在使用随机梯度下降（SGD）时，选择合适的微批次大小（mini-batch size）对于高效训练至关重要，因为更大的批次虽能降低梯度估计的方差，但也会增加每次迭代的计算成本。这个练习 [@problem_id:3187488] 揭示了在减少方差和增加计算时间之间存在的优化权衡，目标是最大化单位时间内的学习进展。通过推导和实验，你将深入理解如何调整 SGD 的超参数，并体会统计效率与计算成本之间的微妙平衡。", "problem": "考虑在由 $f(x) = \\frac{\\lambda}{2} x^{2}$ 定义的强凸二次目标函数上进行单参数随机梯度下降（SGD），其中 $\\lambda > 0$ 是曲率。在第 $k$ 次迭代时，真实梯度为 $\\nabla f(x_{k}) = \\lambda x_{k}$。假设存在一个带加性噪声的无偏随机梯度估计量，使得单样本梯度观测值的形式为 $g(x_{k}) = \\lambda x_{k} + \\varepsilon$，其中 $\\mathbb{E}[\\varepsilon] = 0$ 且 $\\mathrm{Var}(\\varepsilon) = \\sigma^{2}$。对于大小为 $b$ 的小批量（mini-batch），由于独立性，平均噪声的方差为 $\\sigma^{2} / b$，小批量估计量为 $g_{b}(x_{k}) = \\lambda x_{k} + \\bar{\\varepsilon}_{b}$，其中 $\\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\sigma^{2} / b$。\n\n使用固定步长 $\\alpha > 0$ 的 SGD 更新为 $x_{k+1} = x_{k} - \\alpha g_{b}(x_{k})$。将每次更新的计算时间定义为 $T(b) = t_{0} + c b$，其中 $t_{0} > 0$ 是每次更新的固定开销时间， $c > 0$ 是每个样本的计算成本。$t_{0}$ 和 $c$ 的单位都是秒（s），而小批量大小 $b$ 是无单位的。\n\n将预期单步进展定义为目标值的预期减少量，\n$$\n\\Delta(b) = \\mathbb{E}\\left[f(x_{k}) - f(x_{k+1})\\right],\n$$\n并将单位时间的预期进展定义为\n$$\nR(b) = \\frac{\\Delta(b)}{T(b)}.\n$$\n\n从上述核心定义——凸二次目标函数 $f(x)$、带独立噪声的无偏梯度估计量、小批量平均以及 SGD 更新规则——出发，推导出 $R(b)$ 关于参数 $(\\lambda, \\alpha, x_{k}, \\sigma^{2}, t_{0}, c, b)$ 的表达式，不使用任何快捷公式或外部提示。然后，设计一个程序，对于下面列出的每个测试用例，在整数小批量大小 $b \\in \\{1,2,\\dots,B_{\\max}\\}$ 中搜索，并返回使 $R(b)$ 最大化的 $b$ 的整数值。在所有测试用例中，使用固定常数 $\\lambda = 1$、$\\alpha = 0.5$、 $x_{k} = 1$、 $t_{0} = 0.02$ (s) 和 $B_{\\max} = 512$。在每个测试用例中，$c$ 的单位必须是秒/样本 (s/sample)。不涉及角度单位，也不需要百分比。\n\n测试套件（每个测试用例是一对 $(\\sigma^{2}, c)$，其中 $\\sigma^{2}$ 无单位， $c$ 的单位是秒/样本）：\n- 案例 1：$(\\sigma^{2}, c) = (1.0, 0.002)$\n- 案例 2：$(\\sigma^{2}, c) = (10.0, 0.002)$\n- 案例 3：$(\\sigma^{2}, c) = (0.01, 0.002)$\n- 案例 4：$(\\sigma^{2}, c) = (1.0, 0.02)$\n- 案例 5：$(\\sigma^{2}, c) = (5.0, 0.0005)$\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[b_{1},b_{2},b_{3},b_{4},b_{5}]$），其中 $b_{i}$ 是案例 $i$ 的最优小批量大小。输出值必须是整数。", "solution": "主要目标是找到整数小批量大小 $b$，以最大化单位时间的预期进展 $R(b)$。该量定义为预期单步进展 $\\Delta(b)$ 与每次更新的计算时间 $T(b)$ 之比。我们必须首先从给定的定义中推导出 $R(b)$ 的显式表达式。\n\n所讨论的目标函数是一个强凸二次函数：\n$$f(x) = \\frac{\\lambda}{2} x^{2}$$\n其中曲率 $\\lambda > 0$。在点 $x_k$ 处的真实梯度是 $\\nabla f(x_k) = \\lambda x_k$。\n\n大小为 $b$ 的小批量随机梯度估计量由下式给出：\n$$g_{b}(x_{k}) = \\lambda x_{k} + \\bar{\\varepsilon}_{b}$$\n其中平均噪声项 $\\bar{\\varepsilon}_{b}$ 的均值为 $\\mathbb{E}[\\bar{\\varepsilon}_{b}] = 0$，方差为 $\\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\frac{\\sigma^{2}}{b}$。这里，$\\sigma^2$ 是单个样本噪声的方差。\n\n使用固定步长 $\\alpha > 0$ 的随机梯度下降（SGD）更新规则是：\n$$x_{k+1} = x_{k} - \\alpha g_{b}(x_{k})$$\n\n预期单步进展定义为 $\\Delta(b) = \\mathbb{E}\\left[f(x_{k}) - f(x_{k+1})\\right]$。我们首先用 $x_k$ 和随机梯度来表示 $f(x_{k+1})$：\n$$f(x_{k+1}) = \\frac{\\lambda}{2} x_{k+1}^{2} = \\frac{\\lambda}{2} \\left(x_{k} - \\alpha g_{b}(x_{k})\\right)^{2}$$\n展开平方项可得：\n$$f(x_{k+1}) = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} g_{b}(x_{k}) + \\alpha^{2} g_{b}(x_{k})^{2} \\right)$$\n为了求 $f(x_{k+1})$ 的期望值，我们对梯度估计量中的噪声取期望。在第 $k$ 次迭代中，$x_k$ 的值被视为固定的：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\mathbb{E}\\left[ x_{k}^{2} - 2\\alpha x_{k} g_{b}(x_{k}) + \\alpha^{2} g_{b}(x_{k})^{2} \\right]$$\n根据期望的线性性质：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} \\mathbb{E}[g_{b}(x_{k})] + \\alpha^{2} \\mathbb{E}[g_{b}(x_{k})^{2}] \\right)$$\n我们现在需要计算 $g_{b}(x_{k})$ 的一阶矩和二阶矩。\n一阶矩（期望）是：\n$$\\mathbb{E}[g_{b}(x_{k})] = \\mathbb{E}[\\lambda x_{k} + \\bar{\\varepsilon}_{b}] = \\lambda x_{k} + \\mathbb{E}[\\bar{\\varepsilon}_{b}] = \\lambda x_{k}$$\n二阶矩使用关系 $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2$ 来计算：\n$$\\mathbb{E}[g_{b}(x_{k})^{2}] = \\mathrm{Var}(g_{b}(x_{k})) + (\\mathbb{E}[g_{b}(x_{k})])^{2}$$\n估计量的方差是：\n$$\\mathrm{Var}(g_{b}(x_{k})) = \\mathrm{Var}(\\lambda x_{k} + \\bar{\\varepsilon}_{b}) = \\mathrm{Var}(\\bar{\\varepsilon}_{b}) = \\frac{\\sigma^{2}}{b}$$\n因此，二阶矩是：\n$$\\mathbb{E}[g_{b}(x_{k})^{2}] = \\frac{\\sigma^{2}}{b} + (\\lambda x_{k})^{2} = \\lambda^{2} x_{k}^{2} + \\frac{\\sigma^{2}}{b}$$\n将这些矩代入 $\\mathbb{E}[f(x_{k+1})]$ 的表达式中：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha x_{k} (\\lambda x_{k}) + \\alpha^{2} \\left(\\lambda^{2} x_{k}^{2} + \\frac{\\sigma^{2}}{b}\\right) \\right)$$\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} \\left( x_{k}^{2} - 2\\alpha\\lambda x_{k}^{2} + \\alpha^{2}\\lambda^{2} x_{k}^{2} + \\frac{\\alpha^{2}\\sigma^{2}}{b} \\right)$$\n对含 $x_k^2$ 的项进行分组：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} x_{k}^{2} (1 - 2\\alpha\\lambda + \\alpha^{2}\\lambda^{2}) + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n括号中的项是一个完全平方 $(1 - \\alpha\\lambda)^{2}$：\n$$\\mathbb{E}[f(x_{k+1})] = \\frac{\\lambda}{2} x_{k}^{2} (1 - \\alpha\\lambda)^{2} + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n现在我们可以计算预期进展 $\\Delta(b) = \\mathbb{E}[f(x_k)] - \\mathbb{E}[f(x_{k+1})]$。由于 $x_k$ 是固定的，$\\mathbb{E}[f(x_k)]=f(x_k)=\\frac{\\lambda}{2}x_k^2$。\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 - \\left( \\frac{\\lambda}{2} x_{k}^{2} (1 - \\alpha\\lambda)^{2} + \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b} \\right)$$\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 \\left[ 1 - (1 - \\alpha\\lambda)^{2} \\right] - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n展开项 $1 - (1 - \\alpha\\lambda)^{2} = 1 - (1 - 2\\alpha\\lambda + \\alpha^2\\lambda^2) = 2\\alpha\\lambda - \\alpha^2\\lambda^2 = \\alpha\\lambda(2 - \\alpha\\lambda)$。\n$$\\Delta(b) = \\frac{\\lambda}{2}x_k^2 \\left[ \\alpha\\lambda(2 - \\alpha\\lambda) \\right] - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n化简后得到预期单步进展的最终表达式：\n$$\\Delta(b) = \\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right) - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}$$\n这个表达式清楚地显示了两个组成部分：一个与批量大小无关的正进展项（对应于确定性梯度步），以及一个由梯度噪声引起的负项，该项随着批量大小 $b$ 的增加而减小。\n\n每次更新的计算时间由线性模型 $T(b) = t_{0} + c b$ 给出。\n\n进展速率 $R(b)$ 是这两个量的比值：\n$$R(b) = \\frac{\\Delta(b)}{T(b)} = \\frac{\\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right) - \\frac{\\lambda\\alpha^{2}\\sigma^{2}}{2b}}{t_{0} + c b}$$\n问题要求找到整数 $b \\in \\{1, 2, \\dots, B_{\\max}\\}$ 来最大化此函数 $R(b)$，对于给定的参数集。常数为 $\\lambda = 1$、$\\alpha = 0.5$、 $x_{k} = 1$、 $t_{0} = 0.02$ 和 $B_{\\max} = 512$。参数 $(\\sigma^2, c)$ 因测试用例而异。\n\n让我们将固定常数代入 $R(b)$ 的表达式中。\n项 $\\alpha\\lambda^2 x_k^2 \\left(1 - \\frac{\\alpha\\lambda}{2}\\right)$ 变为：\n$$C_1 = (0.5)(1)^2(1)^2 \\left(1 - \\frac{(0.5)(1)}{2}\\right) = 0.5 \\left(1 - 0.25\\right) = 0.5(0.75) = 0.375$$\n噪声项的系数 $\\frac{\\lambda\\alpha^{2}}{2}$ 变为：\n$$\\frac{(1)(0.5)^2}{2} = \\frac{0.25}{2} = 0.125$$\n因此，对于给定参数，$R(b)$ 的完整表达式为：\n$$R(b) = \\frac{0.375 - \\frac{0.125 \\sigma^2}{b}}{0.02 + c b}$$\n寻找最优整数 $b$ 的算法是执行直接搜索。我们将遍历 $b$ 从 $1$ 到 $B_{\\max} = 512$ 的所有可能的整数值，为每个值计算 $R(b)$，并找出产生最大 $R(b)$ 的 $b$ 值。对于一个小的搜索空间，这是一种简单而鲁棒的方法。对于给定的测试用例 $(\\sigma^2, c)$，程序将计算 $b=1, 2, ..., 512$ 的 $R(b)$，并记录产生最高值的 $b$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the optimal mini-batch size b for a series of test cases.\n    The optimization goal is to maximize the expected progress per unit time, R(b).\n    \"\"\"\n\n    # Define the fixed constants from the problem statement.\n    LAMBDA = 1.0  # Curvature\n    ALPHA = 0.5   # Step-size\n    XK = 1.0      # Current position\n    T0 = 0.02     # Fixed overhead time per update (s)\n    B_MAX = 512   # Maximum mini-batch size to search\n\n    # Test suite with pairs of (sigma^2, c)\n    # sigma^2 is the single-sample noise variance (unitless)\n    # c is the compute cost per sample (s/sample)\n    test_cases = [\n        (1.0, 0.002),\n        (10.0, 0.002),\n        (0.01, 0.002),\n        (1.0, 0.02),\n        (5.0, 0.0005),\n    ]\n\n    results = []\n\n    # Pre-calculate the constant part of the numerator of R(b), which is independent of sigma^2 and c.\n    # This term corresponds to alpha * lambda^2 * x_k^2 * (1 - (alpha * lambda) / 2)\n    progress_term_constant = ALPHA * LAMBDA**2 * XK**2 * (1 - (ALPHA * LAMBDA) / 2.)\n\n    for sigma_sq, c in test_cases:\n        best_b = -1\n        max_R = -np.inf\n\n        # Pre-calculate the coefficient of the noise term in the numerator.\n        # This term corresponds to (lambda * alpha^2 * sigma^2) / 2\n        noise_term_coeff = (LAMBDA * ALPHA**2 * sigma_sq) / 2.\n\n        # Search over all allowed integer mini-batch sizes\n        for b in range(1, B_MAX + 1):\n            # Calculate the expected one-step progress, Delta(b)\n            # Delta(b) = progress_term_constant - noise_term_coeff / b\n            delta_b = progress_term_constant - noise_term_coeff / b\n            \n            # The progress rate R(b) can only be maximal if progress Delta(b) is positive.\n            # While we could skip b values where delta_b = 0, the max search handles this naturally.\n\n            # Calculate the computational time per update, T(b)\n            T_b = T0 + c * b\n\n            # Calculate the expected progress per unit time, R(b)\n            # Avoid division by zero, although T_b > 0 is guaranteed by problem constraints.\n            if T_b > 0:\n                R_b = delta_b / T_b\n            else:\n                R_b = -np.inf\n\n            # Update the best batch size if the current one is better\n            if R_b > max_R:\n                max_R = R_b\n                best_b = b\n        \n        results.append(best_b)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187488"}, {"introduction": "像 Adam 这样的现代自适应优化器功能强大，但有时也可能在特定情况下表现出不稳定的行为。理解这些潜在的失效模式是稳健训练模型的关键。这项编码挑战 [@problem_id:3187493] 将引导你构建一个特定的梯度流场景，以揭示 Adam 优化器在梯度二阶矩估计值快速变化时的不稳定性。通过亲手实现并对比 Adam 与其改进版 AMSGrad 的表现，你将直观地理解算法设计细节的重要性，以及 AMSGrad 为何能提供更可靠的收敛性保障。", "problem": "你需要设计一个确定性的随机优化实验，以对比自适应矩估计（Adam）方法和带最大值的自适应矩估计（AMSGrad）方法在不同超参数选择下的行为。重点在于构建一个受控的梯度流，在该梯度流中，由于数值稳定性项和指数平均参数选择不当，Adam 会发散，而 AMSGrad 在相同数据上保持稳定。\n\n基本原理必须遵循以下内容，不能走捷径：随机优化使用从一系列带噪声的梯度计算出的、基于梯度的更新。自适应矩方法建立在观测梯度的一阶矩和二阶矩的指数移动平均之上，并结合了因零初始化而产生的偏差校正。稳定性受到指数平均的衰减参数、学习率以及分母中的加性稳定项之间相互作用的影响。\n\n在一维空间中完全构建该实验。使用 $T$ 次迭代的时间范围，由 $C$ 个长度为 $L$ 的相同周期生成，因此 $T = C L$。在每个周期内，使用单个正向爆发和随后的多个小幅负值来定义一个梯度流 $\\{g_t\\}_{t=1}^T$：\n- 令爆发幅度为 $S > 0$。\n- 对于每个周期，在周期的第一步设置 $g_{t} = S$。\n- 在该周期的其余 $L-1$ 步中，设置 $g_{t} = -s$，其中 $s = \\dfrac{S}{L-1}$，从而使得每个周期内的梯度总和恰好为 $0$。\n\n这种构造产生了一个非平稳的梯度二阶矩，其具有间歇性尖峰和长时间的小幅值平稳期，用以测试自适应方法的敏感性。将参数初始化为 $x_0 = 0$，并使用所选的自适应方法执行参数更新。两种自适应方法都必须根据指数移动平均和偏差校正的定义来实现，并使用以下超参数：\n- 学习率 $\\,\\alpha > 0\\,$。\n- 一阶矩衰减 $\\,\\beta_1 \\in (0,1)\\,$。\n- 二阶矩衰减 $\\,\\beta_2 \\in (0,1)\\,$。\n- 加性稳定项 $\\,\\varepsilon > 0\\,$。\n\n如果最终迭代值的绝对值超过一个固定的界限或变为非有限数，则认为一次运行已经发散。具体来说，定义发散阈值 $D$，如果 $|x_T| > D$ 或者 $x_T$ 不是一个有限实数，则称该方法发散。\n\n你的程序必须：\n- 从指数移动平均和偏差校正的基本原理出发，实现 Adam 和 AMSGrad 两种方法。\n- 按照描述生成梯度流，其中 $L = 200$， $S = 1$， $C = 8$，因此 $T = 1600$。\n- 对每个测试用例，在完全相同的梯度流上运行这两种方法。\n- 使用初始迭代值 $x_0 = 0$ 和发散阈值 $D = 10$。\n\n使用以下超参数集 $\\left(\\alpha, \\beta_1, \\beta_2, \\varepsilon\\right)$ 的测试套件：\n1. $\\left(10^{-2},\\, 0.99,\\, 0.1,\\, 10^{-20}\\right)$，这是一个 $\\,\\varepsilon\\,$ 非常小且 $\\,\\beta_2\\,$ 相对较小的情况，当二阶矩估计值迅速下降时，应会引发 Adam 的不稳定性。\n2. $\\left(10^{-2},\\, 0.9,\\, 0.999,\\, 10^{-8}\\right)$，这是一个典型的选择，预计对 Adam 和 AMSGrad 都是稳定的。\n3. $\\left(10^{-2},\\, 0.9,\\, 10^{-6},\\, 10^{-32}\\right)$，这是一个 $\\,\\beta_2\\,$ 和 $\\,\\varepsilon\\,$ 极小的情况，旨在展示 Adam 的敏感性。\n4. $\\left(2 \\times 10^{-4},\\, 0.99,\\, 0.1,\\, 10^{-20}\\right)$，这是一个边界情况，学习率非常小，即使在不利的 $\\,\\beta_2\\,$ 和 $\\,\\varepsilon\\,$ 值下，也有望避免发散。\n\n对于每个测试用例，按固定顺序返回两个布尔结果：\n- 第一个布尔值：Adam 在该用例上是否发散。\n- 第二个布尔值：AMSGrad 在该用例上是否发散。\n\n最终输出格式：你的程序应生成单行输出，其中包含所有八个布尔值，以逗号分隔列表的形式并用方括号括起来，按测试用例顺序排列为\n$[ \\text{Adam 用例 1}, \\text{AMSGrad 用例 1}, \\text{Adam 用例 2}, \\text{AMSGrad 用例 2}, \\text{Adam 用例 3}, \\text{AMSGrad 用例 3}, \\text{Adam 用例 4}, \\text{AMSGrad 用例 4} ]$。", "solution": "本实验的目标是展示一个场景，其中自适应矩估计（Adam）优化器发散，而其变体 AMSGrad 保持稳定。这是通过构建一个特定的非平稳梯度流并选择能够暴露 Adam 对梯度二阶矩估计值快速变化的敏感性的超参数来实现的。\n\n首先，我们定义梯度流 $\\{g_t\\}_{t=1}^T$。该流构建在总共 $T$ 个时间步上，由 $C$ 个相同的周期组成，每个周期长度为 $L$。问题指定了 $C=8$ 个周期和周期长度 $L=200$，总时间范围为 $T = C \\times L = 8 \\times 200 = 1600$ 步。在每个周期内，梯度是一个大的正向爆发 $g_t = S$，后跟 $L-1$ 个小的负向梯度。问题将爆发幅度设置为 $S=1$。为确保单个周期内梯度总和为零，小的负向梯度值 $s$ 必须是 $s = \\frac{S}{L-1} = \\frac{1}{200-1} = \\frac{1}{199}$。\n因此，在任何时间步 $t \\in \\{1, 2, \\dots, 1600\\}$ 的梯度由下式给出：\n$$\ng_t =\n\\begin{cases}\n    S = 1  \\text{if } (t-1) \\pmod{L} = 0 \\\\\n    -s = -\\frac{1}{199}  \\text{if } (t-1) \\pmod{L} \\neq 0\n\\end{cases}\n$$\n这种结构确保了梯度的二阶矩 $g_t^2$ 会经历大的尖峰（$S^2=1$），随后是长时间的极小值（$s^2 \\approx 2.5 \\times 10^{-5}$）。\n\n优化过程从初始参数值 $x_0 = 0$ 开始。我们将实现并比较两种优化算法：Adam 和 AMSGrad。两者都依赖于梯度一阶矩和二阶矩的指数移动平均。超参数包括学习率 $\\alpha$、一阶矩衰减率 $\\beta_1$、二阶矩衰减率 $\\beta_2$ 以及数值稳定性项 $\\varepsilon$。\n\nAdam 算法的更新如下。我们将一阶矩和二阶矩向量初始化为 $m_0 = 0$ 和 $v_0 = 0$。对于每个时间步 $t = 1, \\dots, T$：\n$1$. 更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$。\n$2$. 更新有偏二阶矩估计：$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$。\n$3$. 计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$。\n$4$. 计算偏差校正后的二阶矩估计：$\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$。\n$5$. 更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$。\n\nAMSGrad 算法修改了 Adam 的更新规则，以确保有效学习率非递增。我们初始化 $m_0 = 0$，$v_0 = 0$，并额外初始化 $v_{\\max, 0} = 0$。对于每个时间步 $t = 1, \\dots, T$：\n$1$. 更新有偏一阶矩估计：$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$。\n$2$. 更新有偏二阶矩估计：$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$。\n$3$. 计算偏差校正后的一阶矩估计：$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}$。\n$4$. 维护迄今为止所见二阶矩估计的最大值：$v_{\\max, t} = \\max(v_{\\max, t-1}, v_t)$。\n$5$. 使用这个最大值更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{v_{\\max, t}} + \\varepsilon}$。\n\n关键区别在于更新规则的分母。在 Adam 中，如果最近的梯度很小，$\\hat{v}_t$ 项可能会减小，特别是当 $\\beta_2$ 很小（即记忆周期短）时。如果 $\\hat{v}_t$ 变得非常小，有效学习率 $\\alpha / (\\sqrt{\\hat{v}_t} + \\varepsilon)$ 可能会激增，导致发散。AMSGrad 通过使用非递减的 $v_{\\max, t}$ 来防止这种情况，从而使有效学习率不会失控增长。\n\n如果最终参数值 $x_T$ 的绝对值大于阈值 $D=10$，或者 $x_T$ 是一个非有限数（例如，无穷大或 NaN），则认为一次运行已经发散。\n\n实验针对四个不同的超参数集 $(\\alpha, \\beta_1, \\beta_2, \\varepsilon)$ 运行：\n$1$. $(10^{-2}, 0.99, 0.1, 10^{-20})$：小的 $\\beta_2$ 和极小的 $\\varepsilon$ 预计会导致 Adam 发散，因为对大梯度尖峰的记忆会迅速消失，分母会急剧缩小。AMSGrad 应保持稳定。\n$2$. $(10^{-2}, 0.9, 0.999, 10^{-8})$：大的 $\\beta_2$ 给予二阶矩估计长期的记忆，这应该能保持 Adam 的稳定。预计两种方法都将是稳定的。\n$3$. $(10^{-2}, 0.9, 10^{-6}, 10^{-32})$：极小的 $\\beta_2$ 和 $\\varepsilon$ 提供了一个更严峻的测试用例，极大地突出了 AMSGrad 稳定性机制的优势。Adam 极有可能发散。\n$4$. $(2 \\times 10^{-4}, 0.99, 0.1, 10^{-20})$：非常小的学习率 $\\alpha$ 应该足以抑制更新步长，从而防止 Adam 发散，即使在其他参数 $\\beta_2$ 和 $\\varepsilon$ 的值有问题的情况下。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a deterministic stochastic optimization experiment\n    to contrast Adam and AMSGrad, implementing both from first principles.\n    \"\"\"\n    \n    # Define problem constants\n    L = 200  # Cycle length\n    S = 1.0  # Burst magnitude\n    C = 8    # Number of cycles\n    T = L * C  # Total iterations\n    D = 10.0 # Divergence threshold\n    \n    # Generate the gradient stream\n    s = S / (L - 1)\n    gradients = np.full(T, -s)\n    cycle_starts = np.arange(0, T, L)\n    gradients[cycle_starts] = S\n\n    # Define the test suite of hyperparameters (alpha, beta1, beta2, epsilon)\n    test_cases = [\n        (1e-2, 0.99, 0.1, 1e-20),\n        (1e-2, 0.9, 0.999, 1e-8),\n        (1e-2, 0.9, 1e-6, 1e-32),\n        (2e-4, 0.99, 0.1, 1e-20),\n    ]\n\n    results = []\n    \n    for params in test_cases:\n        alpha, beta1, beta2, epsilon = params\n\n        # --- Adam Simulation ---\n        x_adam = 0.0\n        m_adam = 0.0\n        v_adam = 0.0\n        beta1_power_t = 1.0\n        beta2_power_t = 1.0\n        \n        for i in range(T):\n            grad = gradients[i]\n            \n            # Update powers for bias correction\n            beta1_power_t *= beta1\n            beta2_power_t *= beta2\n            \n            # Update biased moment estimates\n            m_adam = beta1 * m_adam + (1.0 - beta1) * grad\n            v_adam = beta2 * v_adam + (1.0 - beta2) * (grad**2)\n            \n            # Compute bias-corrected estimates\n            m_hat = m_adam / (1.0 - beta1_power_t)\n            v_hat = v_adam / (1.0 - beta2_power_t)\n            \n            # Parameter update\n            # Precaution: ensure v_hat is non-negative before sqrt\n            if v_hat  0: v_hat = 0\n            x_adam -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n            \n            # Early exit on non-finite value\n            if not np.isfinite(x_adam):\n                break\n\n        adam_diverged = np.abs(x_adam) > D or not np.isfinite(x_adam)\n        results.append(adam_diverged)\n\n        # --- AMSGrad Simulation ---\n        x_ams = 0.0\n        m_ams = 0.0\n        v_ams = 0.0\n        v_max_ams = 0.0\n        beta1_power_t_ams = 1.0\n        \n        for i in range(T):\n            grad = gradients[i]\n            \n            # Update powers for bias correction\n            beta1_power_t_ams *= beta1\n\n            # Update biased moment estimates\n            m_ams = beta1 * m_ams + (1.0 - beta1) * grad\n            v_ams = beta2 * v_ams + (1.0 - beta2) * (grad**2)\n            \n            # Maintain the maximum of the second moment estimate\n            v_max_ams = max(v_max_ams, v_ams)\n\n            # Compute bias-corrected first moment estimate\n            m_hat = m_ams / (1.0 - beta1_power_t_ams)\n            \n            # Parameter update\n            # No need to check for negative v_max_ams as it's non-decreasing from 0.\n            x_ams -= alpha * m_hat / (np.sqrt(v_max_ams) + epsilon)\n            \n            # Early exit on non-finite value\n            if not np.isfinite(x_ams):\n                break\n\n        amsgrad_diverged = np.abs(x_ams) > D or not np.isfinite(x_ams)\n        results.append(amsgrad_diverged)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3187493"}]}