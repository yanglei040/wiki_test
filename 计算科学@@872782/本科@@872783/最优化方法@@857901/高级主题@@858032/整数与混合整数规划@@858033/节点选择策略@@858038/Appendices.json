{"hands_on_practices": [{"introduction": "此练习对两种基石般的搜索策略——深度优先搜索（DFS）和最佳优先搜索（Best-First Search）——进行了基础性比较。通过在简单、明确的搜索树上手动追踪它们的行为，并计算一个“遗憾”指标，您将对它们的操作差异以及如何导航搜索空间获得具体的理解。这种实践对于建立分析更复杂算法行为所需的初步直觉至关重要。[@problem_id:3157396]", "problem": "考虑一个确定性树搜索场景，在该场景中，搜索策略在每一步选择一个前沿节点进行扩展。扩展一个节点会揭示其子节点及其相关的评估值，并产生 $1$ 的单位成本。叶节点带有一个目标值 $\\mathrm{obj}(\\ell)$，该值是叶节点 $\\ell$ 处解的真实质量。最优叶节点是具有最小目标值的叶节点，表示为 $v^{\\star} = \\min_{\\ell} \\mathrm{obj}(\\ell)$。一旦最优叶节点被扩展，搜索即终止。对于任何策略 $\\pi$ 和树 $T$，将扩展计数 $E_{\\pi}(T)$ 定义为在最优叶节点被扩展之前执行的节点扩展总数。将神谕策略定义为一种理想策略，它预先知道最优叶节点的身份，并且只扩展从根节点到该叶节点的唯一路径上的节点，因此 $E_{\\mathrm{oracle}}(T)$ 等于该路径上的节点数。策略 $\\pi$ 在树 $T$ 上的悔憾为\n$$\nR_{\\pi}(T) = E_{\\pi}(T) - E_{\\mathrm{oracle}}(T).\n$$\n我们将比较两种节点选择策略：深度优先选择和最佳优先选择。深度优先选择在每个内部节点使用后进先出 (LIFO) 栈机制，并采用固定的子节点生成顺序。最佳优先选择会选择具有最小启发式值 $h(n)$ 的前沿节点；若出现平局，则按生成时间早晚决定（先生成，先选择）。对于叶节点 $\\ell$，设置 $h(\\ell) = \\mathrm{obj}(\\ell)$。\n\n给定两棵基准树。\n\n树 $T_1$：根节点 $r_1$ 有两个子节点，按 $a$ 然后 $b$ 的顺序生成。启发式值为 $h(a) = 5$ 和 $h(b) = 6$。节点 $a$ 有两个叶子节点 $a_1$ 和 $a_2$，其目标值分别为 $\\mathrm{obj}(a_1) = 5$ 和 $\\mathrm{obj}(a_2) = 9$；因此 $h(a_1) = 5$ 和 $h(a_2) = 9$。节点 $b$ 有两个叶子节点 $b_1$ 和 $b_2$，其目标值分别为 $\\mathrm{obj}(b_1) = 4$ 和 $\\mathrm{obj}(b_2) = 10$；因此 $h(b_1) = 4$ 和 $h(b_2) = 10$。$T_1$ 中的最优叶节点是 $b_1$，其目标值为 $4$。\n\n树 $T_2$：根节点 $r_2$ 有两个子节点，按 $d$ 然后 $c$ 的顺序生成。启发式值为 $h(d) = 2.0$ 和 $h(c) = 1.6$。节点 $d$ 有一个叶子节点 $d_1$，其目标值为 $\\mathrm{obj}(d_1) = 2.0$，所以 $h(d_1) = 2.0$。节点 $c$ 有一个内部子节点 $c_A$（其 $h(c_A) = 1.5$）和一个叶子节点 $c_B$（其 $\\mathrm{obj}(c_B) = 5$，所以 $h(c_B) = 5$）。节点 $c_A$ 有两个叶子节点 $c_{A1}$ 和 $c_{A2}$，其目标值分别为 $\\mathrm{obj}(c_{A1}) = 3.5$ 和 $\\mathrm{obj}(c_{A2}) = 4.1$，所以 $h(c_{A1}) = 3.5$ 和 $h(c_{A2}) = 4.1$。$T_2$ 中的最优叶节点是 $d_1$，其目标值为 $2.0$。\n\n仅使用上述定义和数据，计算比率\n$$\n\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)},\n$$\n其中 $\\mathrm{DFS}$ 表示深度优先选择策略，$\\mathrm{BF}$ 表示最佳优先选择策略。请用精确分数表示最终答案。无需四舍五入。无需单位。", "solution": "此问题有效，因其内容自洽，科学上基于算法分析原理，且陈述客观。所有必要的数据、定义和条件均已提供，且无内部矛盾。我们可以开始求解。\n\n目标是计算比率 $\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)}$。策略 $\\pi$ 在树 $T$ 上的悔憾定义为 $R_{\\pi}(T) = E_{\\pi}(T) - E_{\\mathrm{oracle}}(T)$，其中 $E_{\\pi}(T)$ 是节点扩展的总数。\n\n“节点扩展”是一个产生 $1$ 成本的动作。当最优叶节点被扩展时，搜索终止。问题陈述，神谕策略的扩展计数 $E_{\\mathrm{oracle}}(T)$ 等于从根节点到最优叶节点的路径上的节点数。这意味着叶节点以从前沿中被选择的方式被“扩展”，这会产生 $1$ 的成本，并且如果该叶节点是最优的，则结束搜索。我们将一致地应用此解释。\n\n首先，我们确定每棵树的神谕策略的扩展计数 $E_{\\mathrm{oracle}}(T)$。\n对于树 $T_1$，最优叶节点是 $b_1$。从根节点 $r_1$ 到 $b_1$ 的唯一路径是 $r_1 \\to b \\to b_1$。这条路径包含 $3$ 个节点。因此，$E_{\\mathrm{oracle}}(T_1) = 3$。被扩展的节点是 $r_1$、$b$ 和 $b_1$。\n对于树 $T_2$，最优叶节点是 $d_1$。从根节点 $r_2$ 到 $d_1$ 的唯一路径是 $r_2 \\to d \\to d_1$。这条路径包含 $3$ 个节点。因此，$E_{\\mathrm{oracle}}(T_2) = 3$。被扩展的节点是 $r_2$、$d$ 和 $d_1$。\n\n接下来，我们计算深度优先搜索（$\\mathrm{DFS}$）和最佳优先搜索（$\\mathrm{BF}$）策略在每棵树上的扩展计数 $E_{\\pi}(T)$。\n\n**树 $T_1$ 的分析**\n最优叶节点是 $b_1$，其 $\\mathrm{obj}(b_1) = 4$。\n\n**1. 在 $T_1$ 上进行 DFS：**\n$\\mathrm{DFS}$ 使用 LIFO 栈。$r_1$ 处的子节点生成顺序是 $a$ 然后 $b$。我们假设其他节点的子节点也遵循固定的、一致的顺序（例如，$a$ 的子节点是 $a_1$ 然后 $a_2$，$b$ 的子节点是 $b_1$ 然后 $b_2$）。\n- 第 1 步：扩展 $r_1$。已扩展节点：{$r_1$}。成本：$1$。生成子节点 $a, b$。栈（顶端在前）：[$a, b$]。\n- 第 2 步：扩展 $a$。已扩展节点：{$r_1, a$}。成本：$2$。生成子节点 $a_1, a_2$。栈：[$a_1, a_2, b$]。\n- 第 3 步：扩展 $a_1$。已扩展节点：{$r_1, a, a_1$}。成本：$3$。这是一个叶节点，但不是最优的（$\\mathrm{obj}(a_1)=5 \\neq 4$）。栈：[$a_2, b$]。\n- 第 4 步：扩展 $a_2$。已扩展节点：{$r_1, a, a_1, a_2$}。成本：$4$。这是一个叶节点，但不是最优的（$\\mathrm{obj}(a_2)=9 \\neq 4$）。栈：[$b$]。\n- 第 5 步：扩展 $b$。已扩展节点：{$r_1, a, a_1, a_2, b$}。成本：$5$。生成子节点 $b_1, b_2$。栈：[$b_1, b_2$]。\n- 第 6 步：扩展 $b_1$。已扩展节点：{$r_1, a, a_1, a_2, b, b_1$}。成本：$6$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{DFS}}(T_1) = 6$。\n悔憾为 $R_{\\mathrm{DFS}}(T_1) = E_{\\mathrm{DFS}}(T_1) - E_{\\mathrm{oracle}}(T_1) = 6 - 3 = 3$。\n\n**2. 在 $T_1$ 上进行 BF：**\n$\\mathrm{BF}$ 使用按启发式值 $h(n)$ 排序的优先队列。平局由较早的生成时间打破。\n- 第 1 步：扩展 $r_1$。已扩展节点：{$r_1$}。成本：$1$。前沿（优先队列）：{($a, h=5$), ($b, h=6$)}。节点 $a$ 的优先级更高。\n- 第 2 步：扩展 $a$（因为 $h(a)=5  h(b)=6$）。已扩展节点：{$r_1, a$}。成本：$2$。子节点 $a_1, a_2$ 被添加到前沿。前沿：{($a_1, h=5$), ($b, h=6$), ($a_2, h=9$)}。节点 $a_1$ 的 $h$ 值最小。\n- 第 3 步：扩展 $a_1$。已扩展节点：{$r_1, a, a_1$}。成本：$3$。此叶节点不是最优的。前沿：{($b, h=6$), ($a_2, h=9$)}。节点 $b$ 的优先级更高。\n- 第 4 步：扩展 $b$。已扩展节点：{$r_1, a, a_1, b$}。成本：$4$。子节点 $b_1, b_2$ 被添加。前沿：{($b_1, h=4$), ($a_2, h=9$), ($b_2, h=10$)}。节点 $b_1$ 现在的 $h$ 值最小。\n- 第 5 步：扩展 $b_1$。已扩展节点：{$r_1, a, a_1, b, b_1$}。成本：$5$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{BF}}(T_1) = 5$。\n悔憾为 $R_{\\mathrm{BF}}(T_1) = E_{\\mathrm{BF}}(T_1) - E_{\\mathrm{oracle}}(T_1) = 5 - 3 = 2$。\n\n**树 $T_2$ 的分析**\n最优叶节点是 $d_1$，其 $\\mathrm{obj}(d_1) = 2.0$。\n\n**1. 在 $T_2$ 上进行 DFS：**\n$r_2$ 处的子节点生成顺序是 $d$ 然后 $c$。\n- 第 1 步：扩展 $r_2$。已扩展节点：{$r_2$}。成本：$1$。生成子节点 $d, c$。栈：[$d, c$]。\n- 第 2 步：扩展 $d$。已扩展节点：{$r_2, d$}。成本：$2$。生成子节点 $d_1$。栈：[$d_1, c$]。\n- 第 3 步：扩展 $d_1$。已扩展节点：{$r_2, d, d_1$}。成本：$3$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{DFS}}(T_2) = 3$。\n悔憾为 $R_{\\mathrm{DFS}}(T_2) = E_{\\mathrm{DFS}}(T_2) - E_{\\mathrm{oracle}}(T_2) = 3 - 3 = 0$。\n\n**2. 在 $T_2$ 上进行 BF：**\n- 第 1 步：扩展 $r_2$。已扩展节点：{$r_2$}。成本：$1$。前沿：{($c, h=1.6$), ($d, h=2.0$)}。节点 $c$ 的优先级更高。\n- 第 2 步：扩展 $c$（因为 $h(c)=1.6  h(d)=2.0$）。已扩展节点：{$r_2, c$}。成本：$2$。子节点 $c_A, c_B$ 被添加。前沿：{($c_A, h=1.5$), ($d, h=2.0$), ($c_B, h=5.0$)}。节点 $c_A$ 的 $h$ 值最小。\n- 第 3 步：扩展 $c_A$。已扩展节点：{$r_2, c, c_A$}。成本：$3$。子节点 $c_{A1}, c_{A2}$ 被添加。前沿：{($d, h=2.0$), ($c_{A1}, h=3.5$), ($c_{A2}, h=4.1$), ($c_B, h=5.0$)}。节点 $d$ 现在的 $h$ 值最小。\n- 第 4 步：扩展 $d$。已扩展节点：{$r_2, c, c_A, d$}。成本：$4$。子节点 $d_1$ 被添加。前沿：{($d_1, h=2.0$), ($c_{A1}, h=3.5$), ($c_{A2}, h=4.1$), ($c_B, h=5.0$)}。在 $d$ 被扩展并移除后，$d_1$ 是具有最小 $h$ 值的新前沿节点。\n- 第 5 步：扩展 $d_1$。已扩展节点：{$r_2, c, c_A, d, d_1$}。成本：$5$。这是最优叶节点。搜索终止。\n扩展总数为 $E_{\\mathrm{BF}}(T_2) = 5$。\n悔憾为 $R_{\\mathrm{BF}}(T_2) = E_{\\mathrm{BF}}(T_2) - E_{\\mathrm{oracle}}(T_2) = 5 - 3 = 2$。\n\n**最终计算**\n我们得到以下悔憾值：\n$R_{\\mathrm{DFS}}(T_1) = 3$\n$R_{\\mathrm{DFS}}(T_2) = 0$\n$R_{\\mathrm{BF}}(T_1) = 2$\n$R_{\\mathrm{BF}}(T_2) = 2$\n\n现在我们计算比率 $\\rho$：\n$$\n\\rho = \\frac{R_{\\mathrm{DFS}}(T_1) + R_{\\mathrm{DFS}}(T_2)}{R_{\\mathrm{BF}}(T_1) + R_{\\mathrm{BF}}(T_2)} = \\frac{3 + 0}{2 + 2} = \\frac{3}{4}\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3157396"}, {"introduction": "虽然最佳优先搜索（Best-First Search）因其启发式引导而直观上似乎更优，但其有效性完全取决于该启发式信息的质量。本问题呈现了一个精心构建的反例，其中一个无信息的下界导致最佳优先搜索探索的树节点数量比深度优先搜索（DFS）多出指数级别。通过分析此场景，您将揭示一个关键的洞见：深度优先搜索快速找到一个高质量“ incumbent ”解的潜力，有时可能成为决定性的优势，尤其是在搜索引导较弱时。[@problem_id:3157470]", "problem": "考虑一个关于二进制决策变量的最小化问题，其中 $n \\in \\mathbb{N}$。对于每个完整赋值 $x \\in \\{0,1\\}^{n}$，其目标值由函数 $f(x)$ 给出，定义如下：若 $x = \\mathbf{0}$，则 $f(x) = 0$；否则 $f(x) = M$，其中 $M  0$。该问题使用一个分支定界（BB）算法，其中如果一个节点 $u$ 的下界 $L(u)$ 满足 $L(u) \\geq z^{*}$，则该节点被剪枝，其中 $z^{*}$ 是当前最优（已知最佳）目标值。分支方案按索引顺序 $i = 1,2,\\dots,n$ 分配变量，左子节点固定 $x_{i} = 0$，右子节点固定 $x_{i} = 1$。下界函数是对于每个节点 $u$ 的平凡界 $L(u) = 0$，由于目标值非负，该下界是有效的。\n\n考虑两种节点选择策略：\n- 深度优先搜索（DFS）：总是选择可用的最深节点，若深度相同，则优先选择左子节点而非右子节点。\n- 最佳优先搜索（BestFS）：总是选择具有最小下界 $L(u)$ 的节点，若下界相同，则优先选择深度较浅的节点，然后按部分赋值的字典序选择。\n\n假设搜索树是一个满二叉树，根节点深度为 $0$，叶节点深度为 $n$。将节点扩展定义为选择一个节点进行处理（如果它不是叶节点，则将其分支为其子节点；如果是叶节点，则评估 $f(x)$）。在这个构造的实例中，唯一的最优叶节点 $x = \\mathbf{0}$ 位于始终选择左子节点的路径上的最左侧叶节点。\n\n根据分支定界的核心定义和指定的节点选择规则，推导出 DFS 和 BestFS 在终止前执行的节点扩展的确切数量。然后，计算扩展次数之比，定义为 BestFS 下的扩展次数除以 DFS 下的扩展次数，给出一个关于 $n$ 的闭式解析表达式。请将您的最终答案表示为关于 $n$ 的单个闭式表达式。", "solution": "该问题陈述是算法分析领域一个有效且定义明确的练习，具体涉及分支定界（BB）过程，该过程提供了一套完整且一致的规则。目标函数、分支规则、下界函数和节点选择策略都已明确定义。该问题在科学上基于优化理论，其语言客观而正式。因此，我们可以开始求解。\n\n目标是确定两种不同节点选择策略——深度优先搜索（DFS）和最佳优先搜索（BestFS）——的节点扩展数量，然后计算这些数量的比率。一次扩展定义为选择一个节点进行处理（对于内部节点是分支，对于叶节点是评估目标函数）。如果一个节点的下界 $L(u)$ 大于或等于当前最优值 $z^*$，则该节点被剪枝。被剪枝的节点不会被扩展。\n\nBB 算法初始化时，开放节点列表中有根节点，当前最优值为 $z^* = \\infty$。搜索树中任何节点 $u$ 的下界都为 $L(u) = 0$。\n\n**1. 深度优先搜索（DFS）分析**\n\nDFS 策略选择可用的最深节点，并通过选择左子节点（对应于将变量固定为 $0$）而非右子节点来打破平局。\n\n1.  搜索从根节点（深度 $0$）开始。它被扩展，在深度 $1$ 处创建两个子节点：对应 $x_1=0$ 的左子节点和对应 $x_1=1$ 的右子节点。这是一次扩展。\n2.  根据 DFS 规则，选择更深的节点。由于两个子节点深度相同，应用平局规则，选择左子节点（$x_1=0$）进行扩展。这是第 $2$ 次扩展。\n3.  此过程继续。算法总是沿着树的最左侧路径向下进行，因为在每一步，它都会生成一个新的、更深的层级，并且该新层级的左子节点会立即被选中。此路径对应于为 $i=1, 2, \\dots, n$ 设置 $x_i=0$。\n4.  算法扩展了沿着这条最左侧路径的 $n$ 个内部节点：根节点（深度 $0$）、对应 $(x_1=0)$ 的节点（深度 $1$）、...、最后是对应 $(x_1=0, \\dots, x_{n-1}=0)$ 的节点（深度 $n-1$）。这构成了 $n$ 次扩展。\n5.  在扩展深度为 $n-1$ 的节点之后，下一个被选择的节点是它的左子节点，即深度为 $n$ 的叶节点，对应于完整赋值 $x = \\mathbf{0} = (0, 0, \\dots, 0)$。这个叶节点被扩展。这是第 $(n+1)$ 次扩展。\n6.  扩展此叶节点后，评估目标函数：$f(\\mathbf{0}) = 0$。由于此值小于当前的 $z^* = \\infty$，因此将当前最优值更新为 $z^* = 0$。\n7.  此时，剪枝规则是如果 $L(u) \\geq z^*=0$，则剪枝任何节点 $u$。由于每个节点的下界都为 $L(u) = 0$，条件 $0 \\geq 0$ 总是满足的。\n8.  因此，从开放列表中选择的任何后续节点都将立即被剪枝。根据问题的定义，被剪枝的节点不会被扩展。不会发生进一步的扩展。\n\nDFS 的总节点扩展数，记为 $N_{DFS}$，是最左侧路径上的 $n$ 个内部节点和最左侧的一个叶节点之和。\n$$N_{DFS} = n + 1$$\n\n**2. 最佳优先搜索（BestFS）分析**\n\nBestFS 策略选择具有最小下界 $L(u)$ 的节点。\n\n1.  在此问题中，树中的每个节点 $u$ 都具有相同的下界 $L(u) = 0$。因此，总是会调用平局打破规则。\n2.  第一个平局打破规则是优先选择深度较浅的节点。这迫使算法的行为类似于广度优先搜索（BFS），在移动到下一个更深的层级之前，扩展给定深度的所有节点。\n3.  算法首先扩展深度为 $0$ 的根节点（$1$ 次扩展）。\n4.  然后它扩展深度为 $1$ 的所有 $2^1 = 2$ 个节点。\n5.  接着它扩展深度为 $2$ 的所有 $2^2 = 4$ 个节点，依此类推。\n6.  这个过程一直持续到树的所有内部节点都被扩展。内部节点是深度为 $k = 0, 1, \\dots, n-1$ 的节点。这类节点的总数是一个几何级数的和：\n    $$ \\sum_{k=0}^{n-1} 2^k = \\frac{2^n - 1}{2-1} = 2^n - 1 $$\n    到此为止，还没有评估过任何叶节点，所以当前最优值仍然是 $z^* = \\infty$。\n7.  在所有 $2^{n-1}$ 个深度为 $n-1$ 的节点都被扩展后，开放列表包含所有 $2^n$ 个深度为 $n$ 的叶节点。\n8.  现在对这个叶节点列表应用 BestFS 规则。所有叶节点的 $L(u)=0$ 且都处于相同的深度 $n$。最后的平局打破规则是“按部分赋值的字典序”。\n9.  字典序最前的赋值是 $x = \\mathbf{0} = (0, 0, \\dots, 0)$。对应的叶节点被选择进行扩展。这是一次额外的扩展。\n10. 扩展此叶节点后，找到目标值 $f(\\mathbf{0}) = 0$，并将当前最优值更新为 $z^* = 0$。\n11. 与 DFS 的情况一样，剪枝条件 $L(u) \\geq z^*$ 现在变为 $0 \\geq 0$，这总是成立的。开放列表中剩下的所有其他 $2^n - 1$ 个叶节点在被选中时都将被剪枝，不会被扩展。\n\nBestFS 的总扩展数，记为 $N_{BestFS}$，是所有内部节点扩展和最优叶节点的单次扩展之和。\n$$N_{BestFS} = (2^n - 1) + 1 = 2^n$$\n\n**3. 扩展次数之比**\n\n问题要求 BestFS 下的扩展次数与 DFS 下的扩展次数之比。\n$$ \\text{比率} = \\frac{N_{BestFS}}{N_{DFS}} $$\n代入推导出的 $N_{BestFS}$ 和 $N_{DFS}$ 的表达式：\n$$ \\text{比率} = \\frac{2^n}{n+1} $$\n这就是关于 $n$ 的比率的闭式解析表达式。", "answer": "$$\n\\boxed{\\frac{2^n}{n+1}}\n$$", "id": "3157470"}, {"introduction": "现实世界的优化问题很少有单一的“最佳”搜索策略；通常，混合多种方法最为有效。本练习将从理论分析转向实践应用，要求您构建一个混合节点选择策略，该策略使用参数 $\\lambda$ 融合了深度优先和最佳优先的原则。接着，您将执行元优化，为不同类型的问题找到最佳的 $\\lambda$ 值，亲身体验如何根据问题结构来调整算法性能。[@problem_id:3157413]", "problem": "给定一个确定性的树搜索设置，要求对一个控制节点选择的混合参数进行元优化。其基础是节点选择策略的定义：深度优先搜索选择深度较大的节点，最佳优先搜索选择启发式效用较大的节点。我们定义一个单一的参数化优先级策略，该策略混合了这两种策略，然后寻找在训练集上最小化总节点扩展数的参数。目标是从这些核心定义中推导出如何以纯算法的方式实现和评估元优化，并编写一个程序，为几个不同的训练集计算最优参数。\n\n用作基础的核心定义：\n- 一个有限有根搜索树是一个有向无环图，带有一个指定的根节点。每个节点都有一个非负整数深度，定义为从根节点到该节点的路径上的边数。\n- 开放列表（open list）是已生成但尚未扩展的前沿节点的集合。当一个节点从开放列表中移除时，它被扩展；扩展会将其子节点生成到开放列表中。\n- 目标节点是指当它被选择进行扩展时，搜索即告终止的节点。\n- 最佳优先节点选择由一个效用函数 $U(n)$ 指导，其中较大的 $U(n)$ 表示更有希望。深度优先节点选择由较大的 $\\text{depth}(n)$ 指导。\n- 我们将节点 $n$ 的混合优先级 $p(n)$ 定义为\n$$\np(n) \\;=\\; \\lambda\\, U(n) \\;+\\; (1-\\lambda)\\,\\text{depth}(n),\n$$\n其中 $\\lambda \\in [0,1]$ 是混合参数。通过最大化 $p(n)$ 来选择要扩展的节点，并使用下面指定的确定性打破平局规则。\n\n您的任务：\n- 实现一个确定性的树搜索，在每一步从开放列表中选择使 $p(n)$ 最大化的节点 $n$。平局首先由较大的 $\\text{depth}(n)$ 打破，如果 $p(n)$ 和 $\\text{depth}(n)$ 都相等，则由较小的节点标识符打破。初始开放列表仅包含根节点。当一个节点从开放列表中移除时，将扩展计数器加 $1$，如果它是一个目标节点，则终止该任务的搜索。否则，将其子节点以正确的深度生成到开放列表中。不要重新插入已扩展的节点（无重复扩展）。如果开放列表在未达到目标的情况下变空，则扩展计数为已扩展的节点数。\n- 对于元优化，您必须评估一个离散的混合参数网格 $\\lambda \\in \\{0.00, 0.25, 0.50, 0.75, 1.00\\}$，并选择使训练集中所有任务的总扩展数之和最小化的 $\\lambda$。如果不同 $\\lambda$ 值的总扩展数出现平局，选择最小的 $\\lambda$。\n\n训练集和任务：\n- 训练集 $1$ (两个任务):\n  - 任务 $A1$:\n    - 节点：从 $0$ 到 $8$ 的整数，根为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8)\\}$。\n    - 目标节点：$\\{5\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.20$, $U(1)=0.10$, $U(2)=0.90$, $U(3)=0.05$, $U(4)=0.05$, $U(5)=1.00$, $U(6)=0.20$, $U(7)=0.01$, $U(8)=0.01$。\n  - 任务 $A2$:\n    - 节点：从 $0$ 到 $7$ 的整数，根为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(2,4),(2,5),(4,6),(5,7)\\}$。\n    - 目标节点：$\\{6\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.20$, $U(1)=0.10$, $U(2)=0.80$, $U(3)=0.05$, $U(4)=0.90$, $U(5)=0.40$, $U(6)=1.00$, $U(7)=0.30$。\n- 训练集 $2$ (两个任务，启发式函数具有误导性且目标节点较深):\n  - 任务 $M1$:\n    - 节点：从 $0$ 到 $10$ 的整数，根为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(3,4),(4,5),(2,6),(2,7),(6,8),(7,9),(7,10)\\}$。\n    - 目标节点：$\\{5\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.20$, $U(1)=0.10$, $U(2)=0.95$, $U(3)=0.20$, $U(4)=0.30$, $U(5)=1.00$, $U(6)=0.90$, $U(7)=0.85$, $U(8)=0.80$, $U(9)=0.75$, $U(10)=0.70$。\n  - 任务 $M2$:\n    - 节点：从 $0$ 到 $8$ 的整数，根为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(3,4),(2,5),(2,6),(5,7),(6,8)\\}$。\n    - 目标节点：$\\{4\\}$。\n    - 效用 $U(n)$:\n      $U(0)=0.10$, $U(1)=0.10$, $U(2)=0.90$, $U(3)=0.20$, $U(4)=1.00$, $U(5)=0.85$, $U(6)=0.80$, $U(7)=0.70$, $U(8)=0.60$。\n- 训练集 $3$ (效用一致，目标节点较深):\n  - 任务 $U1$:\n    - 节点：从 $0$ 到 $6$ 的整数，根为 $0$。\n    - 边：$\\{(0,1),(0,2),(1,3),(2,4),(4,5),(5,6)\\}$。\n    - 目标节点：$\\{6\\}$。\n    - 效用 $U(n)$:\n      对于所有节点 $n \\in \\{0,1,2,3,4,5,6\\}$，$U(n)=0.50$。\n\n不涉及角度单位。不涉及物理单位。所有结果均为纯数值。上述训练集作为测试套件，并涵盖：\n- 启发式函数提供有用信息的一般情况（训练集 $1$）。\n- 启发式函数具有误导性且目标节点较深的情况（训练集 $2$）。\n- 一个效用一致的边界情况（训练集 $3$），用于检验打破平局的规则以及 $\\lambda=0$ 和 $\\lambda=1$ 的极端情况。\n\n要求输出：\n- 您的程序必须为每个训练集分别计算最优的 $\\lambda$，从集合 $\\{0.00, 0.25, 0.50, 0.75, 1.00\\}$ 中选择，并使用所描述的确定性搜索过程。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按训练集 $1$、训练集 $2$、训练集 $3$ 的顺序列出结果，每个 $\\lambda$ 值四舍五入到两位小数（例如，$[0.75,0.00,0.25]$）。", "solution": "该问题要求我们为一个混合树搜索策略找到一个最优的混合参数 $\\lambda$。该策略在最佳优先搜索（优先考虑高效用值 $U(n)$ 的节点）和深度优先搜索（优先考虑更深层的节点）之间进行插值。优化是在一个离散的 $\\lambda$ 值集合上进行的，目标是最小化给定训练集中一系列搜索任务的总节点扩展数。\n\n我们的第一步是根据所提供的定义，将环境和搜索算法形式化。\n\n一个搜索任务由一个树结构、一个根节点、一组目标节点以及每个节点 $n$ 的效用值 $U(n)$ 定义。树被表示为一个邻接表，将每个父节点映射到其子节点。一个节点的深度 $\\text{depth}(n)$ 是从根节点到节点 $n$ 的路径上的边数。我们可以通过从深度为 $\\text{depth}(0) = 0$ 的根节点开始进行广度优先搜索 (BFS) 来预先计算给定树中所有节点的深度。\n\n问题的核心是参数化的搜索算法。在每一步，算法必须从 `open list`（开放列表，即已生成但尚未扩展的节点集合）中选择一个节点进行扩展。选择受混合优先级函数控制：\n$$\np(n) = \\lambda\\, U(n) + (1-\\lambda)\\,\\text{depth}(n)\n$$\n其中 $\\lambda \\in [0, 1]$ 是混合参数。具有最大优先级 $p(n)$ 的节点被选中。为了确保确定性，指定了严格的打破平局规则：\n1. 如果多个节点共享相同的最大优先级 $p(n)$，则选择具有更大 $\\text{depth}(n)$ 的节点。\n2. 如果仍然存在平局，则选择具有较小数值标识符的节点。\n\n这种分级选择规则可以通过对开放列表中的节点进行排序来实现。对于任意两个节点 $n_1$ 和 $n_2$，如果元组 $(p(n_1), \\text{depth}(n_1), -n_1\\text{.id})$ 在字典序上大于 $(p(n_2), \\text{depth}(n_2), -n_2\\text{.id})$，则 $n_1$ 优先于 $n_2$。节点标识符上的负号反转了比较，从而在平局时有效选择了较小的 ID。\n\n对于单个任务和给定的 $\\lambda$，搜索过程如下：\n1. 初始化一个只包含根节点的 `open_list`。\n2. 初始化一个空的 `expanded_nodes` 集合来跟踪已扩展的节点并防止重复扩展。\n3. 初始化一个扩展计数器为 $0$。\n4. 循环直到 `open_list` 为空或找到目标：\n    a. 根据优先级函数 $p(n)$ 和指定的打破平局规则，从 `open_list` 中选择最佳节点 $n_{best}$。\n    b. 从 `open_list` 中移除 $n_{best}$ 并将其添加到 `expanded_nodes` 集合。增加扩展计数器。\n    c. 如果 $n_{best}$ 是一个目标节点，则终止此任务的搜索并返回当前的扩展计数。\n    d. 否则，对于 $n_{best}$ 的每个未被扩展过的子节点，将其添加到 `open_list`。\n5. 如果循环因 `open_list` 为空而终止，则返回总扩展计数。\n\n元优化层封装了此搜索过程。对于三个训练集中的每一个，我们都必须从离散集合 $\\{0.00, 0.25, 0.50, 0.75, 1.00\\}$ 中找到最优的 $\\lambda$。\n对于单个训练集的过程是：\n1. 初始化一个列表，用于存储每个 $\\lambda$ 的结果。\n2. 对于测试集中的每个 $\\lambda$：\n    a. 为此 $\\lambda$ 初始化一个 `total_expansions` 计数器为 $0$。\n    b. 对于训练集中的每个任务：\n        i. 使用当前的 $\\lambda$ 运行搜索算法。\n        ii. 将得到的扩展数加到 `total_expansions` 中。\n    c. 存储配对 (`total_expansions`, $\\lambda$)。\n3. 在评估了所有 $\\lambda$ 值之后，选择具有最小 `total_expansions` 的配对。如果 `total_expansions` 出现平局，问题指定选择具有最小 $\\lambda$ 的那一个。这可以通过首先按总扩展数（升序）然后按 $\\lambda$（升序）对结果配对进行排序，并选择第一个元素来实现。\n\n整个过程是确定性的，并且在计算上是明确定义的。实现将涉及创建表示任务的数据结构，一个封装搜索逻辑的类或函数，以及一个执行每个训练集元优化的顶层循环。最终输出将是每个训练集的这些最优 $\\lambda$ 值的列表，并按要求格式化。\n\n例如，在训练集 $3$ 中，所有节点的效用 $U(n)$ 都是一个常数 $0.5$。优先级函数变为 $p(n) = \\lambda \\cdot 0.5 + (1-\\lambda)\\text{depth}(n)$。对于任何 $\\lambda \\in [0, 1)$，由于 $1-\\lambda  0$，最大化 $p(n)$ 等同于最大化 $\\text{depth}(n)$。如果 $\\lambda=1$，$p(n)$ 变为常数，打破平局的规则规定我们最大化 $\\text{depth}(n)$。因此，对于测试集中的任何 $\\lambda$，搜索行为都是相同的，导致相同的扩展数。根据元优化的打破平局规则，必须选择最小的 $\\lambda$，即 $0.00$。这为我们的实现提供了一个有用的一致性检查。", "answer": "```python\nimport numpy as np\n\n# Note: The problem statement lists scipy as a library, but it is not necessary\n# for this particular problem, so it is not imported.\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the meta-optimization process\n    for each training set.\n    \"\"\"\n\n    # --- Data Definition ---\n    TRAINING_SET_1 = [\n        {\n            \"id\": \"A1\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(1,4),(2,5),(2,6),(3,7),(4,8)],\n            \"goals\": {5},\n            \"utilities\": {0:0.20, 1:0.10, 2:0.90, 3:0.05, 4:0.05, 5:1.00, 6:0.20, 7:0.01, 8:0.01},\n        },\n        {\n            \"id\": \"A2\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(2,4),(2,5),(4,6),(5,7)],\n            \"goals\": {6},\n            \"utilities\": {0:0.20, 1:0.10, 2:0.80, 3:0.05, 4:0.90, 5:0.40, 6:1.00, 7:0.30},\n        },\n    ]\n\n    TRAINING_SET_2 = [\n        {\n            \"id\": \"M1\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(3,4),(4,5),(2,6),(2,7),(6,8),(7,9),(7,10)],\n            \"goals\": {5},\n            \"utilities\": {0:0.20, 1:0.10, 2:0.95, 3:0.20, 4:0.30, 5:1.00, 6:0.90, 7:0.85, 8:0.80, 9:0.75, 10:0.70},\n        },\n        {\n            \"id\": \"M2\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(3,4),(2,5),(2,6),(5,7),(6,8)],\n            \"goals\": {4},\n            \"utilities\": {0:0.10, 1:0.10, 2:0.90, 3:0.20, 4:1.00, 5:0.85, 6:0.80, 7:0.70, 8:0.60},\n        },\n    ]\n\n    TRAINING_SET_3 = [\n        {\n            \"id\": \"U1\",\n            \"root\": 0,\n            \"edges\": [(0,1),(0,2),(1,3),(2,4),(4,5),(5,6)],\n            \"goals\": {6},\n            \"utilities\": {n: 0.50 for n in range(7)},\n        },\n    ]\n\n    all_training_sets = [TRAINING_SET_1, TRAINING_SET_2, TRAINING_SET_3]\n    \n    # --- Class for Search Task ---\n    class SearchTask:\n        \"\"\"\n        Represents a single tree search task, including its structure,\n        goal, utilities, and pre-computed depths.\n        \"\"\"\n        def __init__(self, task_data):\n            self.root = task_data[\"root\"]\n            self.goals = task_data[\"goals\"]\n            self.utilities = task_data[\"utilities\"]\n            \n            # Build adjacency list\n            max_node = 0\n            if task_data[\"edges\"]:\n                max_node = max(max(p, c) for p, c in task_data[\"edges\"])\n            if self.utilities:\n                 max_node = max(max_node, max(self.utilities.keys()))\n\n            self.adj = {i: [] for i in range(max_node + 1)}\n            for parent, child in task_data[\"edges\"]:\n                self.adj[parent].append(child)\n            \n            # Pre-compute depths using BFS\n            self.depths = {}\n            if self.root not in self.adj and self.root not in self.utilities:\n                return # Empty tree\n            \n            queue = [(self.root, 0)]\n            visited = {self.root}\n            self.depths[self.root] = 0\n            head = 0\n            while head  len(queue):\n                curr, d = queue[head]\n                head += 1\n                for child in self.adj.get(curr, []):\n                    if child not in visited:\n                        visited.add(child)\n                        self.depths[child] = d + 1\n                        queue.append((child, d + 1))\n\n        def run_search(self, lambda_val):\n            \"\"\"\n            Performs the deterministic tree search for a given lambda.\n            Returns the number of nodes expanded.\n            \"\"\"\n            open_list = [(self.root)]\n            expanded_nodes = set()\n            expansion_count = 0\n            \n            while open_list:\n                # Sort open_list to find the best node to expand.\n                # The key is a tuple that implements the specified priority and tie-breaking rules.\n                # 1. Maximize p(n)\n                # 2. Maximize depth(n)\n                # 3. Minimize node ID\n                open_list.sort(\n                    key=lambda node_id: (\n                        lambda_val * self.utilities[node_id] + (1 - lambda_val) * self.depths[node_id],\n                        self.depths[node_id],\n                        -node_id\n                    ),\n                    reverse=True\n                )\n                \n                current_node = open_list.pop(0)\n\n                if current_node in expanded_nodes:\n                    continue\n                \n                expanded_nodes.add(current_node)\n                expansion_count += 1\n                \n                if current_node in self.goals:\n                    return expansion_count\n                \n                # Add children to open_list\n                for child in self.adj.get(current_node, []):\n                    if child not in expanded_nodes:\n                        open_list.append(child)\n            \n            return expansion_count\n\n    # --- Meta-optimization logic ---\n    def find_optimal_lambda(task_list_data):\n        \"\"\"\n        Finds the optimal lambda for a given training set.\n        \"\"\"\n        tasks = [SearchTask(data) for data in task_list_data]\n        lambdas_to_test = [0.00, 0.25, 0.50, 0.75, 1.00]\n        \n        results = []\n        for lam in lambdas_to_test:\n            total_expansions = 0\n            for task in tasks:\n                total_expansions += task.run_search(lam)\n            results.append((total_expansions, lam))\n            \n        # Tie-break by smallest lambda if expansions are equal\n        results.sort(key=lambda x: (x[0], x[1]))\n        \n        return results[0][1]\n\n    # --- Main Execution Loop ---\n    optimal_lambdas = []\n    for training_set in all_training_sets:\n        best_lambda = find_optimal_lambda(training_set)\n        optimal_lambdas.append(f\"{best_lambda:.2f}\")\n\n    print(f\"[{','.join(optimal_lambdas)}]\")\n\nsolve()\n```", "id": "3157413"}]}