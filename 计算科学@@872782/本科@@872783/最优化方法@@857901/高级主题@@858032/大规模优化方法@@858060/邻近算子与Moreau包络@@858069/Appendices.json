{"hands_on_practices": [{"introduction": "在掌握了邻近算子和Moreau包络的理论基础后，我们通过一个核心计算问题来巩固理解。本练习将引导你推导$\\ell_\\infty$范数的邻近算子，该范数在鲁棒优化和机器学习等前沿领域中扮演着关键角色。通过将复杂的最小化问题分解为一个几何投影和随后的一维搜索，你将掌握一种强大的问题简化策略。[@problem_id:3167987]", "problem": "考虑由 $f(x)=\\|x\\|_{\\infty}$ 定义的函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$，其中 $\\|x\\|_{\\infty}=\\max_{1\\leq i\\leq n}|x_{i}|$。在点 $x\\in\\mathbb{R}^{n}$ 处，缩放函数 $\\lambda f$ 的近端算子 (proximal operator) 定义为\n$$\n\\operatorname{prox}_{\\lambda f}(x)=\\underset{u\\in\\mathbb{R}^{n}}{\\arg\\min}\\;\\left\\{\\frac{1}{2}\\|u-x\\|_{2}^{2}+\\lambda\\|u\\|_{\\infty}\\right\\},\n$$\n而 $f$ 的 Moreau 包络 (Moreau envelope)，参数为 $\\lambda0$，定义为\n$$\ne_{\\lambda}f(x)=\\underset{u\\in\\mathbb{R}^{n}}{\\min}\\;\\left\\{f(u)+\\frac{1}{2\\lambda}\\|u-x\\|_{2}^{2}\\right\\}。\n$$\n\n从这些定义出发，根据第一性原理推导在 $n=1$ 和 $n=2$ 的情况下 $\\operatorname{prox}_{\\lambda f}(x)$ 和 $e_{\\lambda}f(x)$ 的表达式。在这两种情况下，通过一个半径参数 $s\\geq 0$ 来处理 $\\|u\\|_{\\infty}$，并证明对于固定的 $s$，内部的最小化子是通过将 $x$ 的每个分量“钳位”(clamp)到立方体 $[-s,s]^{n}$ 上得到的，即 $u_{i}=\\mathrm{sign}(x_{i})\\min\\{|x_{i}|,s\\}$，然后对 $s$ 进行优化。识别出 $\\lambda$ 相对于 $x$ 各分量大小的所有情况 (regime)，描述最优的 $s$ 和对应的最小化子 $u$，并在每种情况下给出 $e_{\\lambda}f(x)$ 的闭式表达式。\n\n最后，对于给定的输入 $x^{(1)}\\in\\mathbb{R}$ 和 $x^{(2)}\\in\\mathbb{R}^{2}$，其中 $x^{(1)}=-2$，$x^{(2)}=(3,-1)$，当 $\\lambda=1$ 时，计算您的公式，并报告由以下五项组成的五元组：\n1) $\\operatorname{prox}_{\\lambda f}(x^{(1)})$，\n2) $e_{\\lambda}f(x^{(1)})$，\n3) $\\operatorname{prox}_{\\lambda f}(x^{(2)})$ 的第一个分量，\n4) $\\operatorname{prox}_{\\lambda f}(x^{(2)})$ 的第二个分量，\n5) $e_{\\lambda}f(x^{(2)})$。\n\n无需四舍五入；报告精确值。最终答案必须是输出规则中指定的单行矩阵。", "solution": "我们从近端算子和 Moreau 包络的核心定义开始。对于 $\\lambda0$ 和 $x\\in\\mathbb{R}^{n}$，\n$$\n\\operatorname{prox}_{\\lambda f}(x)=\\underset{u}{\\arg\\min}\\left\\{\\frac{1}{2}\\|u-x\\|_{2}^{2}+\\lambda\\|u\\|_{\\infty}\\right\\},\n\\quad\ne_{\\lambda}f(x)=\\min_{u}\\left\\{\\|u\\|_{\\infty}+\\frac{1}{2\\lambda}\\|u-x\\|_{2}^{2}\\right\\}。\n$$\n这两个问题的内部结构在标量前置因子上是相同的：同一个最小化子 $u^{\\star}$ 同时求解这两个问题，而包络是在该最小化子处的目标函数值，并带有适当的缩放。\n\n我们利用表示 $\\|u\\|_{\\infty}=\\min\\{s\\geq 0:\\;|u_{i}|\\leq s\\;\\text{对所有}\\;i\\}$，将近端问题重写为在 $s\\geq 0$ 和受 $|u_{i}|\\leq s$ 约束的 $u$ 上的最小化问题：\n$$\n\\min_{s\\geq 0}\\left\\{\\lambda s+\\min_{u:\\;|u_{i}|\\leq s}\\frac{1}{2}\\sum_{i=1}^{n}(u_{i}-x_{i})^{2}\\right\\}。\n$$\n对于任何固定的 $s$，内部的最小化是 $x$ 到立方体 $[-s,s]^{n}$ 上的欧几里得投影，这是按分量进行的：\n$$\nu_{i}(s)=\\mathrm{sign}(x_{i})\\min\\{|x_{i}|,s\\},\\quad i=1,\\dots,n.\n$$\n定义钳位向量 $c(s)=u(s)$，其分量为 $c_{i}(s)=\\mathrm{sign}(x_{i})\\min\\{|x_{i}|,s\\}$。外部问题简化为一个凸的一维优化问题\n$$\n\\varphi(s)=\\frac{1}{2}\\|x-c(s)\\|_{2}^{2}+\\lambda s,\\quad s\\geq 0.\n$$\n那么，在相同的最优 $s^{\\star}$ 和 $u^{\\star}=c(s^{\\star})$ 处，Moreau 包络满足：\n$$\ne_{\\lambda}f(x)=\\|u^{\\star}\\|_{\\infty}+\\frac{1}{2\\lambda}\\|x-c(s^{\\star})\\|_{2}^{2}=s^{\\star}+\\frac{1}{2\\lambda}\\|x-c(s^{\\star})\\|_{2}^{2}。\n$$\n\n情况 $n=1$。这里 $f(u)=|u|$ 且 $c(s)=\\mathrm{sign}(x)\\min\\{|x|,s\\}$。一维近端问题是\n$$\n\\min_{u\\in\\mathbb{R}}\\left\\{\\frac{1}{2}(u-x)^{2}+\\lambda|u|\\right\\}。\n$$\n我们通过次梯度最优性来求解。如果 $u0$，目标函数可微，最优性条件为 $u-x+\\lambda=0$，得到 $u=x-\\lambda$；这仅在 $x-\\lambda0$ (即 $x\\lambda$) 时成立。如果 $u0$，条件为 $u-x-\\lambda=0$，得到 $u=x+\\lambda$，仅在 $x-\\lambda$ 时成立。如果 $u=0$， $|u|$ 的次梯度是区间 $[-1,1]$，所以 $0\\in x+\\lambda[-1,1]$，这在 $|x|\\leq \\lambda$ 时成立。这些情况组合成软阈值规则 (soft-thresholding rule)：\n$$\n\\operatorname{prox}_{\\lambda|\\cdot|}(x)=\\mathrm{sign}(x)\\max\\{|x|-\\lambda,0\\}。\n$$\n给定最小化子 $u^{\\star}=\\operatorname{prox}_{\\lambda|\\cdot|}(x)$，包络值为\n$$\ne_{\\lambda}f(x)=|u^{\\star}|+\\frac{1}{2\\lambda}(u^{\\star}-x)^{2}。\n$$\n在两种情况下计算它。如果 $|x|\\leq \\lambda$，那么 $u^{\\star}=0$ 并且\n$$\ne_{\\lambda}f(x)=0+\\frac{1}{2\\lambda}x^{2}=\\frac{x^{2}}{2\\lambda}。\n$$\n如果 $|x|\\lambda$，那么 $u^{\\star}=\\mathrm{sign}(x)(|x|-\\lambda)$，所以 $|u^{\\star}|=|x|-\\lambda$ 且 $(u^{\\star}-x)^{2}=\\lambda^{2}$，因此\n$$\ne_{\\lambda}f(x)=|x|-\\lambda+\\frac{1}{2\\lambda}\\lambda^{2}=|x|-\\frac{\\lambda}{2}。\n$$\n\n情况 $n=2$。令 $a=\\max\\{|x_{1}|,|x_{2}|\\}$ 且 $b=\\min\\{|x_{1}|,|x_{2}|\\}$，所以 $a\\geq b\\geq 0$。钳位向量是 $c(s)=(\\mathrm{sign}(x_{1})\\min\\{|x_{1}|,s\\},\\mathrm{sign}(x_{2})\\min\\{|x_{2}|,s\\})$。距离的平方可分解为\n$$\n\\|x-c(s)\\|_{2}^{2}=\\sum_{i=1}^{2}\\left(\\max\\{|x_{i}|-s,0\\}\\right)^{2}=(\\max\\{a-s,0\\})^{2}+(\\max\\{b-s,0\\})^{2}。\n$$\n因此\n$$\n\\varphi(s)=\\frac{1}{2}\\left[(\\max\\{a-s,0\\})^{2}+(\\max\\{b-s,0\\})^{2}\\right]+\\lambda s,\\quad s\\geq 0.\n$$\n这个凸的分段可微函数的导数为\n$$\n\\varphi'(s)=-\\sum_{i=1}^{2}\\max\\{|x_{i}|-s,0\\}+\\lambda=\n-\\max\\{a-s,0\\}-\\max\\{b-s,0\\}+\\lambda.\n$$\n我们考虑三种情况：\n\n1) 如果 $\\lambda\\geq a+b$，那么 $\\varphi'(0)=-a-b+\\lambda\\geq 0$ 且最小化子是 $s^{\\star}=0$。因此 $u^{\\star}=c(0)=(0,0)$ 并且\n$$\n\\operatorname{prox}_{\\lambda\\|\\cdot\\|_{\\infty}}(x)=0,\\quad\ne_{\\lambda}f(x)=0+\\frac{1}{2\\lambda}(a^{2}+b^{2})=\\frac{a^{2}+b^{2}}{2\\lambda}。\n$$\n\n2) 如果 $a-b \\le \\lambda  a+b$，那么 $s^{\\star}=(a+b-\\lambda)/2$。最小化子 $u^{\\star}=c(s^{\\star})$ 的分量为\n$$\nu_{i}^{\\star}=\\mathrm{sign}(x_{i})\\min\\{|x_{i}|,s^{\\star}\\}=\\mathrm{sign}(x_{i})s^{\\star}\n$$\n因为 $s^{\\star}\\leq b$（由于 $a+b-\\lambda \\leq 2b \\iff a-b \\leq \\lambda$）。Moreau 包络值为\n$$\ne_{\\lambda}f(x)=s^{\\star}+\\frac{1}{2\\lambda}\\left[(a-s^{\\star})^{2}+(b-s^{\\star})^{2}\\right] = \\frac{(a+b-\\lambda)^{2}+2\\lambda^{2}}{4\\lambda}。\n$$\n\n3) 如果 $0\\lambda\\le a-b$，那么 $s^{\\star}=a-\\lambda$。最小化子 $u^{\\star}=c(s^{\\star})$ 的分量为\n$$\nu_{i}^{\\star}=\\mathrm{sign}(x_{i})\\min\\{|x_{i}|,s^{\\star}\\}\n$$\n所以，其大小为 $a$ 的分量变为 $s^{\\star}=a-\\lambda$，而其大小为 $b$ 的分量保持为 $b$。Moreau 包络值为\n$$\ne_{\\lambda}f(x)=s^{\\star}+\\frac{1}{2\\lambda}\\left[(a-s^{\\star})^{2}+0^{2}\\right]=a-\\lambda+\\frac{\\lambda^{2}}{2\\lambda}=a-\\frac{\\lambda}{2}。\n$$\n对于给定的输入 $x^{(1)}=-2$ 和 $x^{(2)}=(3,-1)$，当 $\\lambda=1$ 时，我们有：\n\n对于 $x^{(1)}=-2$ ($n=1$)：$|x^{(1)}|=2\\lambda=1$。所以\n$$\n\\operatorname{prox}_{\\lambda f}(x^{(1)})=\\mathrm{sign}(-2)\\max\\{2-1,0\\}=-1,\n\\quad\ne_{\\lambda}f(x^{(1)})=|x^{(1)}|-\\frac{\\lambda}{2}=2-\\frac{1}{2}=\\frac{3}{2}。\n$$\n对于 $x^{(2)}=(3,-1)$ ($n=2$)：$a=3$, $b=1$。我们有 $a-b=2$, $a+b=4$。因为 $\\lambda=1$ 满足 $0\\lambda\\le a-b$，我们处于情况3。\n$s^{\\star}=a-\\lambda=3-1=2$。\n$$\nu_{1}^{\\star}=\\mathrm{sign}(3)\\min\\{3,2\\}=2,\n\\quad\nu_{2}^{\\star}=\\mathrm{sign}(-1)\\min\\{1,2\\}=-1。\n$$\n因此 $\\operatorname{prox}_{\\lambda f}(x^{(2)})=(2,-1)$。\nMoreau 包络为 $e_{\\lambda}f(x^{(2)})=a-\\lambda/2=3-1/2=5/2$。\n五元组为 $(-1, 3/2, 2, -1, 5/2)$。", "answer": "$$\\boxed{\\begin{pmatrix}-1  \\frac{3}{2}  2  -1  \\frac{5}{2}\\end{pmatrix}}$$", "id": "3167987"}, {"introduction": "超越静态的计算，下一个练习将揭示Moreau包络更深层次的物理和动力学意义。我们将探讨Moreau包络如何通过平滑梯度来“正则化”一个系统的动态演化过程。通过求解平滑后的梯度流（一个常微分方程），并将其解与原始非光滑函数的次梯度流进行对比，你将直观地理解Moreau平滑在避免“突变”和保证系统良好行为中的作用。[@problem_id:3167965]", "problem": "考虑由 $f(x)=|x|$ 给出的凸函数 $f:\\mathbb{R}\\to\\mathbb{R}$，并固定一个平滑参数 $\\lambda0$。Moreau 包络 $e_{\\lambda}f:\\mathbb{R}\\to\\mathbb{R}$ 定义为\n$$\ne_{\\lambda}f(x)=\\min_{y\\in\\mathbb{R}}\\left\\{f(y)+\\frac{1}{2\\lambda}\\left(y-x\\right)^{2}\\right\\}。\n$$\n从这个定义以及凸函数和极小化点的基本性质出发，推导 $e_{\\lambda}f(x)$ 及其梯度 $\\nabla e_{\\lambda}f(x)$ 的显式表达式。\n\n然后，考虑具有初始条件 $x(0)=x_{0}\\in\\mathbb{R}$ 的梯度流常微分方程 (ODE) $\\dot{x}(t)=-\\nabla e_{\\lambda}f\\left(x(t)\\right)$。对所有 $t\\geq 0$ 显式求解此 ODE，并将解表示为 $t$、$\\lambda$ 和 $x_{0}$ 的闭式函数。\n\n最后，将此解的行为与 $f$ 的次梯度流（即微分包含关系 $\\dot{x}(t)\\in-\\partial f\\left(x(t)\\right)$，初始条件为 $x(0)=x_{0}$）进行比较。基于推导出的表达式，讨论该流是在有限时间还是无限时间内达到 $x=0$，并分析轨迹的定性差异。\n\n提供求解 $\\dot{x}=-\\nabla e_{\\lambda}f(x)$ 的 $x(t)$ 的最终显式公式，用 $t$、$\\lambda$ 和 $x_{0}$ 表示。不需要数值近似；您的答案必须是精确的闭式表达式。", "solution": "该问题陈述在科学上是合理的、适定的和客观的，并为在凸优化领域内进行严格求解提供了所有必要组成部分。\n\n问题要求三个主要部分：推导 $f(x)=|x|$ 的 Moreau 包络及其梯度，求解基于此包络的梯度流 ODE，以及与原始函数 $f(x)$ 的次梯度流进行比较。\n\n首先，我们推导 $f(x)=|x|$ 的 Moreau 包络 $e_{\\lambda}f(x)$。其定义为\n$$\ne_{\\lambda}f(x)=\\min_{y\\in\\mathbb{R}}\\left\\{|y|+\\frac{1}{2\\lambda}(y-x)^{2}\\right\\}\n$$\n极小化点，记为 $y^* = \\text{prox}_{\\lambda f}(x)$，是通过将目标函数关于 $y$ 的次梯度设为零来找到的。该目标函数是严格凸的，保证了唯一的极小化点。次梯度条件是 $0 \\in \\partial|y^*| + \\frac{1}{\\lambda}(y^*-x)$，整理后得到 $x-y^* \\in \\lambda\\partial|y^*|$。\n$|y|$ 在 $y^*$ 处的次梯度为：如果 $y^*0$，则为 $\\{1\\}$；如果 $y^*0$，则为 $\\{-1\\}$；如果 $y^*=0$，则为 $[-1, 1]$。\n情况 1：$y^*0$。$x-y^* = \\lambda$，因此 $y^*=x-\\lambda$。此式在 $x-\\lambda0$ 时成立，即 $x  \\lambda$。\n情况 2：$y^*0$。$x-y^* = -\\lambda$，因此 $y^*=x+\\lambda$。此式在 $x+\\lambda  0$ 时成立，即 $x  -\\lambda$。\n情况 3：$y^*=0$。$x-0 \\in \\lambda[-1, 1]$，因此 $|x| \\le \\lambda$。\n极小化点是软阈值算子：\n$$\ny^*(x) = \\text{prox}_{\\lambda f}(x) = \\begin{cases} x-\\lambda  \\text{if } x  \\lambda \\\\ 0  \\text{if } |x| \\leq \\lambda \\\\ x+\\lambda  \\text{if } x  -\\lambda \\end{cases}\n$$\n将 $y^*$ 代入 $e_{\\lambda}f(x)$ 的定义中：\n如果 $x  \\lambda$：$e_{\\lambda}f(x) = |x-\\lambda| + \\frac{1}{2\\lambda}((x-\\lambda)-x)^2 = x-\\lambda + \\frac{\\lambda^2}{2\\lambda} = x - \\frac{\\lambda}{2}$。\n如果 $x  -\\lambda$：$e_{\\lambda}f(x) = |x+\\lambda| + \\frac{1}{2\\lambda}((x+\\lambda)-x)^2 = -(x+\\lambda) + \\frac{\\lambda^2}{2\\lambda} = -x - \\frac{\\lambda}{2}$。\n如果 $|x| \\le \\lambda$：$e_{\\lambda}f(x) = |0| + \\frac{1}{2\\lambda}(0-x)^2 = \\frac{x^2}{2\\lambda}$。\n这就得到了 Moreau 包络，一个被称为 Huber 损失的函数：\n$$\ne_{\\lambda}f(x) = \\begin{cases} x - \\frac{\\lambda}{2}  \\text{if } x  \\lambda \\\\ \\frac{x^2}{2\\lambda}  \\text{if } |x| \\leq \\lambda \\\\ -x - \\frac{\\lambda}{2}  \\text{if } x  -\\lambda \\end{cases}\n$$\n梯度 $\\nabla e_{\\lambda}f(x)$ 可以通过对 $e_{\\lambda}f(x)$ 求导或使用 Moreau 恒等式 $\\nabla e_{\\lambda}f(x) = \\frac{1}{\\lambda}(x - \\text{prox}_{\\lambda f}(x))$ 来得到：\n$$\n\\nabla e_{\\lambda}f(x) = \\begin{cases} 1  \\text{if } x  \\lambda \\\\ \\frac{x}{\\lambda}  \\text{if } |x| \\leq \\lambda \\\\ -1  \\text{if } x  -\\lambda \\end{cases}\n$$\n函数 $e_{\\lambda}f(x)$ 是连续可微的。\n\n第二，我们求解梯度流 ODE $\\dot{x}(t) = -\\nabla e_{\\lambda}f(x(t))$，初始条件为 $x(0)=x_0$。\n情况 1：$|x_0| \\le \\lambda$。ODE 为 $\\dot{x} = -x/\\lambda$。这是一个可分离的线性 ODE，其解为 $x(t) = x_0 \\exp(-t/\\lambda)$。由于 $|x(t)| = |x_0|\\exp(-t/\\lambda) \\le |x_0| \\le \\lambda$，对于所有 $t \\ge 0$，解都保持在该区域内。\n情况 2：$|x_0|  \\lambda$。我们假设 $x_0  \\lambda$。初始时，$x(t)  \\lambda$，因此 ODE 为 $\\dot{x}=-1$。其解为 $x(t) = x_0 - t$。这个解一直有效，直到 $x(t)$ 达到 $\\lambda$，这发生在时间 $t_1 = x_0 - \\lambda$。此时，$x(t_1) = \\lambda$。对于 $t  t_1$，动力学切换为 $\\dot{x} = -x/\\lambda$。对于 $t  t_1$ 的解是 $x(t) = C \\exp(-t/\\lambda)$。使用条件 $x(t_1)=\\lambda$：$\\lambda = C \\exp(-t_1/\\lambda) = C \\exp(-(x_0-\\lambda)/\\lambda)$。所以，$C = \\lambda \\exp((x_0-\\lambda)/\\lambda)$。对于 $t  t_1$ 的解是 $x(t) = \\lambda \\exp((x_0-\\lambda)/\\lambda) \\exp(-t/\\lambda) = \\lambda \\exp\\left(-\\frac{t-(x_0-\\lambda)}{\\lambda}\\right)$。\n根据对称性，如果 $x_0  -\\lambda$，解为 $x(t) = x_0+t$，直到时间 $t_1=-x_0-\\lambda$ 时 $x(t_1)=-\\lambda$，之后对于 $tt_1$，$x(t) = -\\lambda \\exp\\left(-\\frac{t-(-x_0-\\lambda)}{\\lambda}\\right)$。\n我们可以将这些结果合并为 $x(t)$ 的一个分段定义。\n\n第三，我们将此流与次梯度流 $\\dot{x}(t) \\in -\\partial f(x(t))$ 进行比较。\n$f(x)=|x|$ 的次梯度给出了微分包含关系：对于 $x0$，$\\dot{x} = -1$；对于 $x0$，$\\dot{x}=1$；对于 $x=0$，$\\dot{x} \\in [-1, 1]$。\n对于初始条件 $x_0  0$，解为 $x(t) = x_0-t$，在时间 $t=x_0$ 时达到 $x=0$。对于 $t \\ge x_0$，轨迹保持在 $x=0$ (通过选择 $\\dot{x}=0$)。通解为 $x(t) = \\text{sign}(x_0)\\max(0, |x_0|-t)$。\n比较：\n1. 收敛时间：次梯度流在有限时间 $T=|x_0|$ 内达到最小值 $x=0$。Moreau 包络的梯度流在 $t\\to\\infty$ 时渐近收敛到 $x=0$，但由于指数衰减阶段，它永远不会在有限时间内达到。\n2. 轨迹平滑度：次梯度流的解 $x(t)$ 是连续且分段线性的，但其导数 $\\dot{x}(t)$ 在 $t=|x_0|$ 处不连续。轨迹不是 $C^1$ 的。Moreau 平滑流的解 $x(t)$ 是连续可微的（$C^1$），无缝地连接了线性部分和指数部分。然而，其二阶导数 $\\ddot{x}(t)$ 在转换时间 $t=|x_0|-\\lambda$（如果 $|x_0|\\lambda$）处不连续，因此轨迹不是 $C^2$ 的。平滑参数 $\\lambda$ 在原点周围创建了一个区域，使得速度平滑地减小到零，避免了次梯度流的突然停止。\n\n$x(t)$ 的最终显式公式以分段形式给出，取决于初始状态 $x_0$ 相对于 $\\lambda$ 的值。", "answer": "$$\n\\boxed{\nx(t) = \\begin{cases} x_0 \\exp\\left(-\\frac{t}{\\lambda}\\right)  \\text{if } |x_0| \\le \\lambda \\\\ \\text{sign}(x_0)\\left(|x_0|-t\\right)  \\text{if } |x_0|  \\lambda \\text{ and } 0 \\le t \\le |x_0|-\\lambda \\\\ \\text{sign}(x_0)\\lambda \\exp\\left(-\\frac{t - (|x_0|-\\lambda)}{\\lambda}\\right)  \\text{if } |x_0|  \\lambda \\text{ and } t  |x_0|-\\lambda \\end{cases}\n}\n$$", "id": "3167965"}, {"introduction": "理论的最终价值在于其应用。作为本章的收尾，这个练习将引导你搭建从数学公式到可执行代码的桥梁。你将为同一个问题实现两种基本的迭代算法：直接作用于原函数的邻近点算法（PPA），以及作用于其Moreau包络的梯度下降（GD）。通过编程实现和对比它们的数值结果，你不仅能验证两者之间的理论联系，还能获得关于算法收敛行为的直观感受。[@problem_id:3168003]", "problem": "考虑一维凸函数 $f:\\mathbb{R}\\to\\mathbb{R}$，其定义为 $f(x)=|x|+\\dfrac{c}{2}x^{2}$，其中 $c\\ge 0$ 是一个参数。设 $\\lambda0$ 是一个平滑参数。$f$ 的 Moreau 包络（ME）定义为\n$$e_{\\lambda}f(x)=\\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\dfrac{1}{2\\lambda}(y-x)^{2}\\right\\}，$$\n$f$ 的近端算子（proximal operator）定义为\n$$\\operatorname{prox}_{\\lambda f}(x)=\\arg\\min_{y\\in\\mathbb{R}}\\left\\{f(y)+\\dfrac{1}{2\\lambda}(y-x)^{2}\\right\\}。$$\n\n任务：\n1) 从上述核心定义出发，推导给定 $f(x)=|x|+\\dfrac{c}{2}x^{2}$ 时 $\\operatorname{prox}_{\\lambda f}(x)$ 的闭式表达式。接下来，仅使用凸优化的基本性质和所提供的定义，推导 Moreau 包络的梯度 $\\nabla e_{\\lambda}f(x)$ 的显式表达式，该表达式应使用 $x$、$\\lambda$、$c$ 和 $\\operatorname{prox}_{\\lambda f}(x)$ 表示。\n\n2) 实现两种迭代方案：\n- Moreau 包络上的梯度下降（GD）：对于一个恒定步长 $\\alpha0$ 和一个初始点 $x_{0}\\in\\mathbb{R}$，定义 $x_{k+1}=x_{k}-\\alpha\\,\\nabla e_{\\lambda}f(x_{k})$，其中 $k=0,1,\\dots,N-1$。\n- 近端点算法（PPA）：对于相同的 $\\lambda$ 和初始点 $x_{0}$，定义 $x_{k+1}=\\operatorname{prox}_{\\lambda f}(x_{k})$，其中 $k=0,1,\\dots,N-1$。\n\n对于每种方法，运行恰好 $N$ 次迭代并报告最终的迭代结果。\n\n3) 使用您的推导，编写一个完整、可运行的程序，该程序：\n- 实现指定 $f$ 的闭式 $\\operatorname{prox}_{\\lambda f}(x)$。\n- 通过您推导的表达式实现 $\\nabla e_{\\lambda}f(x)$。\n- 对下面的测试套件运行 $e_{\\lambda}f$ 上的 GD 和 $f$ 上的 PPA。\n- 对于每个测试用例，计算单个比较数 $\\Delta=f(x^{N}_{\\mathrm{GD}})-f(x^{N}_{\\mathrm{PPA}})$，其中 $x^{N}_{\\mathrm{GD}}$ 和 $x^{N}_{\\mathrm{PPA}}$ 分别是 GD 和 PPA 经过 $N$ 次迭代后的最终迭代值。\n\n测试套件（每个元组指定 $(c,\\lambda,x_{0},\\alpha,N)$）：\n- 情况1（一般凸函数，中等平滑）：$(1.0,\\,0.5,\\,3.0,\\,0.4,\\,20)$。\n- 情况2（接近原点的非光滑扭结处）：$(1.0,\\,0.5,\\,0.2,\\,0.4,\\,10)$。\n- 情况3（小平滑参数）：$(1.0,\\,0.05,\\,3.0,\\,0.04,\\,200)$。\n- 情况4（纯非光滑，无二次项）：$(0.0,\\,0.5,\\,3.0,\\,0.4,\\,20)$。\n- 情况5（阈值集的边界）：$(1.0,\\,0.5,\\,0.5,\\,0.4,\\,1)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含测试套件的五个 $\\Delta$ 值，格式为逗号分隔的列表，并用方括号括起来，顺序与上述情况一致，例如 $[\\,\\Delta_{1},\\Delta_{2},\\Delta_{3},\\Delta_{4},\\Delta_{5}\\,]$。不涉及物理单位；所有输出均为数学单位下的实数。不使用角度。不使用百分比。输出为实数（浮点数）。", "solution": "该问题被评估为有效，因为它在数学上是合理的、自洽的、适定的，并且构成了凸优化领域的一个标准练习。给定的定义、参数和任务清晰明确。\n\n解决方案分为两部分。首先，我们推导近端算子和 Moreau 包络梯度的闭式表达式。其次，我们描述利用这些表达式的迭代算法。\n\n**第一部分：解析推导**\n\n设函数为 $f(x) = |x| + \\dfrac{c}{2}x^2$，其中 $x \\in \\mathbb{R}$，参数 $c \\ge 0$。参数 $\\lambda$ 严格为正，即 $\\lambda  0$。\n\n**1. 近端算子 $\\operatorname{prox}_{\\lambda f}(x)$ 的推导**\n\n$f$ 的近端算子被定义为以下目标函数的唯一最小化子：\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\arg\\min_{y\\in\\mathbb{R}} \\left\\{ g(y) = f(y) + \\dfrac{1}{2\\lambda}(y-x)^2 \\right\\} $$\n代入 $f(y)$ 的表达式，我们有：\n$$ g(y) = |y| + \\dfrac{c}{2}y^2 + \\dfrac{1}{2\\lambda}(y-x)^2 $$\n函数 $g(y)$ 是强凸的，因为它是凸函数 $|y|$ 和强凸二次函数 $\\frac{c}{2}y^2 + \\frac{1}{2\\lambda}(y-x)^2$ 的和（二次部分的 $y^2$ 项系数为 $\\frac{1}{2}(c + \\frac{1}{\\lambda})  0$）。一个强凸函数有唯一的最小化子。\n\n为了找到这个最小化子，记为 $y^*$，我们使用凸分析中的费马法则，该法则指出，当 $0$ 属于目标函数的次微分时，达到最小值，即 $0 \\in \\partial g(y^*)$。$g(y)$ 的次微分是其各组成部分函数次微分的和：\n$$ \\partial g(y) = \\partial|y| + \\nabla\\left(\\dfrac{c}{2}y^2 + \\dfrac{1}{2\\lambda}(y-x)^2\\right) $$\n光滑部分的导数是：\n$$ \\nabla\\left(\\dfrac{c}{2}y^2 + \\dfrac{1}{2\\lambda}(y-x)^2\\right) = cy + \\dfrac{1}{\\lambda}(y-x) = \\left(c + \\dfrac{1}{\\lambda}\\right)y - \\dfrac{x}{\\lambda} $$\n绝对值函数的次微分是：\n$$ \\partial|y| = \\begin{cases} \\{-1\\},  \\text{若 } y  0 \\\\ [-1, 1],  \\text{若 } y = 0 \\\\ \\{1\\},  \\text{若 } y  0 \\end{cases} $$\n最优性条件 $0 \\in \\partial g(y^*)$ 变为：\n$$ 0 \\in \\partial|y^*| + \\left(c + \\dfrac{1}{\\lambda}\\right)y^* - \\dfrac{x}{\\lambda} $$\n这可以改写为：\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)y^* \\in \\partial|y^*| $$\n我们根据 $y^*$ 的符号来分析这个包含关系：\n\n情况 I：$y^*  0$。次微分是 $\\partial|y^*| = \\{1\\}$。\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)y^* = 1 \\implies \\left(\\dfrac{1+c\\lambda}{\\lambda}\\right)y^* = \\dfrac{x}{\\lambda} - 1 $$\n$$ y^* = \\dfrac{\\lambda}{1+c\\lambda}\\left(\\dfrac{x-\\lambda}{\\lambda}\\right) = \\dfrac{x-\\lambda}{1+c\\lambda} $$\n该解仅在 $y^*  0$ 时有效，这要求 $x - \\lambda  0$，即 $x  \\lambda$。\n\n情况 II：$y^*  0$。次微分是 $\\partial|y^*| = \\{-1\\}$。\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)y^* = -1 \\implies \\left(\\dfrac{1+c\\lambda}{\\lambda}\\right)y^* = \\dfrac{x}{\\lambda} + 1 $$\n$$ y^* = \\dfrac{\\lambda}{1+c\\lambda}\\left(\\dfrac{x+\\lambda}{\\lambda}\\right) = \\dfrac{x+\\lambda}{1+c\\lambda} $$\n该解仅在 $y^*  0$ 时有效，这要求 $x + \\lambda  0$，即 $x  -\\lambda$。\n\n情况 III：$y^* = 0$。次微分是 $\\partial|y^*| = [-1, 1]$。\n$$ \\dfrac{x}{\\lambda} - \\left(c + \\dfrac{1}{\\lambda}\\right)(0) \\in [-1, 1] \\implies \\dfrac{x}{\\lambda} \\in [-1, 1] $$\n这等价于 $-\\lambda \\le x \\le \\lambda$。\n\n综合这三种情况，我们得到 $y^* = \\operatorname{prox}_{\\lambda f}(x)$ 的闭式表达式：\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\begin{cases} \\dfrac{x+\\lambda}{1+c\\lambda},  \\text{若 } x  -\\lambda \\\\ 0,  \\text{若 } -\\lambda \\le x \\le \\lambda \\\\ \\dfrac{x-\\lambda}{1+c\\lambda},  \\text{若 } x  \\lambda \\end{cases} $$\n这可以用软阈值算子 $S_T(x) = \\operatorname{sign}(x)\\max(0, |x|-T)$ 紧凑地写成：\n$$ \\operatorname{prox}_{\\lambda f}(x) = \\dfrac{S_{\\lambda}(x)}{1+c\\lambda} $$\n\n**2. Moreau 包络梯度 $\\nabla e_{\\lambda}f(x)$ 的推导**\n\nMoreau 包络定义为 $e_{\\lambda}f(x) = \\inf_{y\\in\\mathbb{R}}\\left\\{f(y)+\\dfrac{1}{2\\lambda}(y-x)^{2}\\right\\}$。\n凸分析中的一个基本结果（通常由 Danskin 定理推导得出）指出，对于一个正常的、下半连续的凸函数 $f$，其 Moreau 包络是连续可微的。其梯度由 Moreau 恒等式（或第二预解式恒等式）给出：\n$$ \\nabla e_{\\lambda}f(x) = \\dfrac{1}{\\lambda}(x - \\operatorname{prox}_{\\lambda f}(x)) $$\n为了按要求使用基本原理来证明这一点，令 $\\phi(x,y) = f(y) + \\frac{1}{2\\lambda}(y-x)^2$。Moreau 包络是 $e_{\\lambda}f(x) = \\min_y \\phi(x,y)$。函数 $\\phi(x,y)$ 在 $y$ 上是凸的，并且关于 $x$ 是连续可微的。Danskin 的最小函数微分定理适用。它指出 $e_{\\lambda}f(x)$ 关于 $x$ 的梯度是 $\\phi(x,y)$ 关于 $x$ 的梯度，在点 $y = y^*(x) = \\operatorname{prox}_{\\lambda f}(x)$ 处求值，这里 $y^*(x)$ 是唯一的最小化子。\n\n首先，我们计算 $\\phi(x,y)$ 关于 $x$ 的梯度：\n$$ \\nabla_x \\phi(x,y) = \\nabla_x \\left( f(y) + \\dfrac{1}{2\\lambda}(y-x)^2 \\right) = \\dfrac{1}{2\\lambda} \\cdot 2(y-x) \\cdot (-1) = \\dfrac{1}{\\lambda}(x-y) $$\n在 $y = \\operatorname{prox}_{\\lambda f}(x)$ 处对此求值，得到 Moreau 包络梯度的最终表达式：\n$$ \\nabla e_{\\lambda}f(x) = \\dfrac{1}{\\lambda}(x - \\operatorname{prox}_{\\lambda f}(x)) $$\n这个表达式将平滑函数 $e_{\\lambda}f$ 的梯度与原始函数 $f$ 的近端算子直接联系起来。\n\n**第二部分：迭代方案**\n\n推导出的表达式用于实现两种迭代优化算法。两种算法都旨在找到 $f(x)$ 的最小化子，即 $x^*=0$。\n\n**1. Moreau 包络上的梯度下降（GD）**\n\n该方法将标准梯度下降算法应用于平滑函数 $e_{\\lambda}f(x)$。给定一个恒定步长 $\\alpha  0$ 和一个初始点 $x_0$，迭代过程为：\n$$ x_{k+1} = x_k - \\alpha \\nabla e_{\\lambda}f(x_k) $$\n代入我们推导出的梯度表达式：\n$$ x_{k+1} = x_k - \\dfrac{\\alpha}{\\lambda}(x_k - \\operatorname{prox}_{\\lambda f}(x_k)) $$\n$$ x_{k+1} = \\left(1 - \\dfrac{\\alpha}{\\lambda}\\right)x_k + \\dfrac{\\alpha}{\\lambda}\\operatorname{prox}_{\\lambda f}(x_k) $$\n这表明，在 Moreau 包络上进行梯度下降是在当前迭代点 $x_k$ 和近端步骤的结果 $\\operatorname{prox}_{\\lambda f}(x_k)$ 之间的一个凸组合。它通常被称为前向-后向分裂方法或松弛的近端点算法。\n\n**2. 近端点算法（PPA）**\n\n该方法直接使用近端算子来生成下一个迭代点。对于相同的 $\\lambda$ 和初始点 $x_0$，迭代定义为：\n$$ x_{k+1} = \\operatorname{prox}_{\\lambda f}(x_k) $$\n如果在 GD 方案中选择步长 $\\alpha$ 等于 $\\lambda$，则 GD 方案将变得与 PPA 相同。在给定的测试用例中，$\\alpha  \\lambda$，这使得 GD 方法成为 PPA 的一个欠松弛版本，预计其收敛速度会更慢。\n\n这些推导和算法描述为编写所需程序以计算给定测试套件的数值结果奠定了基础。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimization problem by implementing and comparing two iterative schemes.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple specifies (c, lambda, x0, alpha, N).\n    test_cases = [\n        (1.0, 0.5, 3.0, 0.4, 20),    # Case 1\n        (1.0, 0.5, 0.2, 0.4, 10),    # Case 2\n        (1.0, 0.05, 3.0, 0.04, 200), # Case 3\n        (0.0, 0.5, 3.0, 0.4, 20),    # Case 4\n        (1.0, 0.5, 0.5, 0.4, 1),     # Case 5\n    ]\n\n    def f(x, c):\n        \"\"\"\n        Computes the value of the function f(x) = |x| + (c/2)*x^2.\n        \"\"\"\n        return np.abs(x) + (c / 2.0) * x**2\n\n    def prox_f(x, c, lam):\n        \"\"\"\n        Computes the proximal operator of f with parameter lambda.\n        prox_{\\lambda f}(x) = S_{\\lambda}(x) / (1 + c*\\lambda)\n        where S is the soft-thresholding operator.\n        \"\"\"\n        # Soft-thresholding operator S_lam(x)\n        soft_threshold = np.sign(x) * np.maximum(0.0, np.abs(x) - lam)\n        return soft_threshold / (1.0 + c * lam)\n\n    def grad_e(x, c, lam):\n        \"\"\"\n        Computes the gradient of the Moreau envelope e_{\\lambda}f(x).\n        \\nabla e_{\\lambda}f(x) = (x - prox_{\\lambda f}(x)) / \\lambda\n        \"\"\"\n        return (x - prox_f(x, c, lam)) / lam\n\n    def run_gd(c, lam, x0, alpha, N):\n        \"\"\"\n        Runs Gradient Descent on the Moreau envelope for N iterations.\n        \"\"\"\n        x_k = x0\n        for _ in range(N):\n            grad = grad_e(x_k, c, lam)\n            x_k = x_k - alpha * grad\n        return x_k\n\n    def run_ppa(c, lam, x0, N):\n        \"\"\"\n        Runs the Proximal Point Algorithm for N iterations.\n        \"\"\"\n        x_k = x0\n        for _ in range(N):\n            x_k = prox_f(x_k, c, lam)\n        return x_k\n\n    results = []\n    for case in test_cases:\n        c, lam, x0, alpha, N = case\n        \n        # Run Gradient Descent on the Moreau envelope\n        x_N_gd = run_gd(c, lam, x0, alpha, int(N))\n        \n        # Run Proximal Point Algorithm\n        x_N_ppa = run_ppa(c, lam, x0, int(N))\n        \n        # Compute the comparison number Delta\n        delta = f(x_N_gd, c) - f(x_N_ppa, c)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3168003"}]}