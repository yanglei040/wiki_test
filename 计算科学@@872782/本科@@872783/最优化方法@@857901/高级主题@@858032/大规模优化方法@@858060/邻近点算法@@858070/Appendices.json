{"hands_on_practices": [{"introduction": "L1范数是现代统计学和机器学习的基石，以其在解中诱导稀疏性的能力而闻名，例如在LASSO模型中。本练习 [@problem_id:3168299] 提供了一个基础实践：从一阶最优性条件直接推导L1范数的邻近算子，即著名的“软阈值”算子。掌握这一推导至关重要，因为它构成了众多优化算法的基本构建模块。", "problem": "设 $f:\\mathbb{R}^n\\to\\mathbb{R}$ 是一个闭真凸函数，回顾 $f$ 在 $y\\in\\mathbb{R}^n$ 处的邻近算子的定义，其是强凸问题 $\\min_{x\\in\\mathbb{R}^n}\\left\\{f(x)+\\frac{1}{2}\\|x-y\\|_2^2\\right\\}$ 的唯一最小化子 $x^{\\star}$。该问题的一阶最优性条件是单调包含 $0\\in \\partial f(x^{\\star})+x^{\\star}-y$，这与在次微分算子 $\\partial f$ 上应用邻近点算法（PPA）的单次隐式步相吻合。设 $f_{\\lambda}(x)=\\lambda\\|x\\|_1$，其中 $\\lambda>0$ 且 $\\|x\\|_1=\\sum_{i=1}^n |x_i|$。考虑通过求解包含关系 $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$ 来计算 $\\operatorname{prox}_{f_{\\lambda}}(y)$。\n\n首先，从绝对值的次微分刻画出发，显式推导包含关系 $0\\in \\lambda\\,\\partial\\|x\\|_1 + x - y$ 的按坐标求解的解，从而得到 $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ 关于 $y$ 和 $\\lambda$ 的闭式表达式。其次，考虑扰动函数 $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$，其中 $\\varepsilon>0$，再次应用PPA最优性条件，推导出 $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$ 的显式按坐标表达式。\n\n现在取具体数据 $y\\in\\mathbb{R}^3$，其中 $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$，参数 $\\lambda=\\frac{1}{2}$ 和 $\\varepsilon=\\frac{1}{4}$。计算 $\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$ 和 $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$，然后计算这两个邻近点之间的欧几里得距离的平方，即 $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2$ 的值。请以精确有理数形式给出最终答案。不需要四舍五入，也不涉及物理单位。", "solution": "该问题要求推导两个邻近算子并进行后续的数值计算。过程分为三个主要部分：首先，推导函数 $f_{\\lambda}(x)=\\lambda\\|x\\|_1$ 的邻近算子；其次，推导扰动函数 $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$ 的邻近算子；第三，对于给定的具体数据，计算这两个邻近点之间的欧几里得距离的平方。\n\n### 第1部分：推导 $\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y)$\n\n$f_{\\lambda}(x) = \\lambda\\|x\\|_1$ 的邻近算子定义为以下最小化问题的唯一解：\n$$ x^{(1)} = \\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = \\arg\\min_{x\\in\\mathbb{R}^n}\\left\\{ \\lambda\\|x\\|_1 + \\frac{1}{2}\\|x-y\\|_2^2 \\right\\} $$\n目标函数是可分的，意味着它可以写成关于 $x$ 的各个分量之和：\n$$ \\sum_{i=1}^n \\left( \\lambda|x_i| + \\frac{1}{2}(x_i - y_i)^2 \\right) $$\n因此，我们可以通过求解 $n$ 个独立的标量最小化问题来找到最小化子。对于第 $i$ 个分量 $x_i$ 的一阶最优性条件由以下包含关系给出：\n$$ 0 \\in \\lambda\\,\\partial|x_i| + x_i - y_i $$\n这可以重写为 $y_i - x_i \\in \\lambda\\,\\partial|x_i|$。绝对值函数 $g(t)=|t|$ 的次微分是：\n$$ \\partial|t| = \\begin{cases} \\{1\\} & \\text{如果 } t > 0 \\\\ \\{-1\\} & \\text{如果 } t < 0 \\\\ [-1, 1] & \\text{如果 } t = 0 \\end{cases} $$\n我们根据解 $x_i$ 的符号来分析这个包含关系：\n情况1：$x_i > 0$。次微分为 $\\partial|x_i| = \\{1\\}$。包含关系变为 $y_i - x_i = \\lambda$，这意味着 $x_i = y_i - \\lambda$。这仅在 $y_i - \\lambda > 0$ 时成立，即 $y_i > \\lambda$。\n情况2：$x_i < 0$。次微分为 $\\partial|x_i| = \\{-1\\}$。包含关系变为 $y_i - x_i = -\\lambda$，这意味着 $x_i = y_i + \\lambda$。这仅在 $y_i + \\lambda < 0$ 时成立，即 $y_i < -\\lambda$。\n情况3：$x_i = 0$。次微分为 $\\partial|x_i| = [-1, 1]$。包含关系变为 $y_i - 0 \\in \\lambda[-1, 1]$，简化为 $y_i \\in [-\\lambda, \\lambda]$，或 $|y_i| \\le \\lambda$。\n\n结合这三种情况，按坐标求解的解是：\n$$ x_i^{(1)} = \\begin{cases} y_i - \\lambda & \\text{如果 } y_i > \\lambda \\\\ 0 & \\text{如果 } |y_i| \\le \\lambda \\\\ y_i + \\lambda & \\text{如果 } y_i < -\\lambda \\end{cases} $$\n这个算子被称为软阈值算子，可以紧凑地写为 $x_i^{(1)} = S_{\\lambda}(y_i) = \\operatorname{sgn}(y_i) \\max(|y_i| - \\lambda, 0)$。因此，$\\operatorname{prox}_{\\lambda\\|\\cdot\\|_1}(y) = S_{\\lambda}(y)$，其中算子是按分量应用的。\n\n### 第2部分：推导 $\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$\n\n设 $x^{(2)} = \\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)$。函数是 $f_{\\lambda,\\varepsilon}(x)=\\lambda\\|x\\|_1 + \\frac{\\varepsilon}{2}\\|x\\|_2^2$。$x^{(2)}$ 的最优性条件是：\n$$ 0 \\in \\partial f_{\\lambda,\\varepsilon}(x^{(2)}) + x^{(2)} - y $$\n使用次微分的求和法则（因为 $\\frac{\\varepsilon}{2}\\|x\\|_2^2$ 是连续可微的），我们有 $\\partial f_{\\lambda,\\varepsilon}(x) = \\lambda\\,\\partial\\|x\\|_1 + \\varepsilon x$。包含关系变为：\n$$ 0 \\in \\lambda\\,\\partial\\|x^{(2)}\\|_1 + \\varepsilon x^{(2)} + x^{(2)} - y = \\lambda\\,\\partial\\|x^{(2)}\\|_1 + (1+\\varepsilon)x^{(2)} - y $$\n这个问题也是可分的。按坐标的包含关系是：\n$$ y_i - (1+\\varepsilon)x_i^{(2)} \\in \\lambda\\,\\partial|x_i^{(2)}| $$\n我们用与之前相同的方式分析这个包含关系：\n情况1：$x_i^{(2)} > 0$。则 $\\partial|x_i^{(2)}| = \\{1\\}$，所以 $y_i - (1+\\varepsilon)x_i^{(2)} = \\lambda$。解出 $x_i^{(2)}$ 得 $x_i^{(2)} = \\frac{y_i - \\lambda}{1+\\varepsilon}$。这在 $y_i - \\lambda > 0 \\iff y_i > \\lambda$ 时成立。\n情况2：$x_i^{(2)} < 0$。则 $\\partial|x_i^{(2)}| = \\{-1\\}$，所以 $y_i - (1+\\varepsilon)x_i^{(2)} = -\\lambda$。解出 $x_i^{(2)}$ 得 $x_i^{(2)} = \\frac{y_i + \\lambda}{1+\\varepsilon}$。这在 $y_i + \\lambda < 0 \\iff y_i < -\\lambda$ 时成立。\n情况3：$x_i^{(2)} = 0$。则 $y_i \\in \\lambda[-1, 1]$，这意味着 $|y_i| \\le \\lambda$。\n\n将 $x_i^{(2)}$ 的解与软阈值算子 $S_{\\lambda}(y_i)$ 进行比较，我们发现：\n$$ x_i^{(2)} = \\frac{1}{1+\\varepsilon} \\begin{cases} y_i - \\lambda & \\text{如果 } y_i > \\lambda \\\\ 0 & \\text{如果 } |y_i| \\le \\lambda \\\\ y_i + \\lambda & \\text{如果 } y_i < -\\lambda \\end{cases} = \\frac{1}{1+\\varepsilon} S_{\\lambda}(y_i) = \\frac{1}{1+\\varepsilon} x_i^{(1)} $$\n这个关系对所有分量 $i=1, \\dots, n$ 都成立。因此，向量解是 $x^{(2)} = \\frac{1}{1+\\varepsilon}x^{(1)}$。\n\n### 第3部分：数值计算\n\n我们给定的数据是 $y=\\big(\\frac{3}{2},-\\frac{1}{3},\\frac{4}{5}\\big)$，$\\lambda=\\frac{1}{2}$ 和 $\\varepsilon=\\frac{1}{4}$。我们需要计算 $\\|\\operatorname{prox}_{\\lambda\\|x\\|_1}(y)-\\operatorname{prox}_{f_{\\lambda,\\varepsilon}}(y)\\|_2^2 = \\|x^{(1)} - x^{(2)}\\|_2^2$。\n\n使用上面推导的关系，$x^{(1)} - x^{(2)} = x^{(1)} - \\frac{1}{1+\\varepsilon}x^{(1)} = \\left(1 - \\frac{1}{1+\\varepsilon}\\right)x^{(1)} = \\frac{\\varepsilon}{1+\\varepsilon}x^{(1)}$。\n那么，欧几里得距离的平方是：\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left\\| \\frac{\\varepsilon}{1+\\varepsilon}x^{(1)} \\right\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 $$\n首先，我们用 $\\varepsilon = \\frac{1}{4}$ 计算标量因子：\n$$ \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{1+\\frac{1}{4}}\\right)^2 = \\left(\\frac{\\frac{1}{4}}{\\frac{5}{4}}\\right)^2 = \\left(\\frac{1}{5}\\right)^2 = \\frac{1}{25} $$\n接下来，我们用 $\\lambda = \\frac{1}{2}$ 计算 $x^{(1)} = \\operatorname{prox}_{\\lambda\\|x\\|_1}(y)$：\n对于 $i=1$：$y_1 = \\frac{3}{2}$。因为 $y_1 = \\frac{3}{2} > \\lambda = \\frac{1}{2}$，我们有 $x_1^{(1)} = y_1 - \\lambda = \\frac{3}{2} - \\frac{1}{2} = 1$。\n对于 $i=2$：$y_2 = -\\frac{1}{3}$。因为 $|y_2| = \\frac{1}{3} \\le \\lambda = \\frac{1}{2}$，我们有 $x_2^{(1)} = 0$。\n对于 $i=3$：$y_3 = \\frac{4}{5}$。因为 $y_3 = \\frac{4}{5} > \\lambda = \\frac{1}{2}$（因为 $\\frac{4}{5}=0.8$ 且 $\\frac{1}{2}=0.5$），我们有 $x_3^{(1)} = y_3 - \\lambda = \\frac{4}{5} - \\frac{1}{2} = \\frac{8-5}{10} = \\frac{3}{10}$。\n所以，$x^{(1)} = \\left(1, 0, \\frac{3}{10}\\right)$。\n\n现在，我们计算 $x^{(1)}$ 的欧几里得范数的平方：\n$$ \\|x^{(1)}\\|_2^2 = 1^2 + 0^2 + \\left(\\frac{3}{10}\\right)^2 = 1 + \\frac{9}{100} = \\frac{100}{100} + \\frac{9}{100} = \\frac{109}{100} $$\n最后，我们计算所求的量：\n$$ \\|x^{(1)} - x^{(2)}\\|_2^2 = \\left(\\frac{\\varepsilon}{1+\\varepsilon}\\right)^2 \\|x^{(1)}\\|_2^2 = \\frac{1}{25} \\times \\frac{109}{100} = \\frac{109}{2500} $$\n结果是所要求的精确有理数。", "answer": "$$\\boxed{\\frac{109}{2500}}$$", "id": "3168299"}, {"introduction": "与L1范数不同，L-无穷范数正则化项鼓励所有分量大小一致有界的解。本练习 [@problem_id:3168264] 深入探讨其邻近算子的行为，揭示了一种与软阈值化不同的、有趣的“收缩”机制。通过解决这个问题，您将更深入地洞察不同正则化项如何塑造解的几何形状，并理解其与对偶空间中L1球投影的联系。", "problem": "设 $f:\\mathbb{R}^{4}\\to\\mathbb{R}$ 为凸函数 $f(x)=\\|x\\|_{\\infty}$，其中对于 $x=(x_{1},x_{2},x_{3},x_{4})$，$\\|x\\|_{\\infty}=\\max\\{|x_{1}|,|x_{2}|,|x_{3}|,|x_{4}|\\}$。考虑集值次微分算子的预解式，即 $(I+\\lambda\\,\\partial f)^{-1}$，该算子出现在用于最小化 $f$ 的邻近点算法中。\n\n1) 仅使用次微分的定义和关于对偶范数的标准事实，构造一个具体的点 $x\\in\\mathbb{R}^{4}$，在该点上次微分 $\\partial f(x)$ 是集值的（即，包含不止一个向量），并证明它在该点是集值的。你的构造应是明确且自洽的。\n\n2) 当 $\\lambda=2$ 且 $y=(3,\\,2,\\,-1,\\,0.5)$ 时，精确计算预解式的值 $(I+\\lambda\\,\\partial f)^{-1}(y)$。你的最终数值答案必须是一个 $1\\times 4$ 的行向量。无需四舍五入。\n\n3) 简要解释你在第2部分计算出的点，说明无穷范数的近端映射(proximal mapping)如何重新分配最大模坐标的影响。使用次梯度(subgradients)和凸组合(convex combinations)的语言使你的解释精确化。\n\n需要报告的最终答案必须仅为第2部分的向量，精确写出（无需四舍五入或单位）。", "solution": "### 第1部分：构造一个具有集值次微分的点\n\n凸函数 $f$ 在点 $x$ 处的次微分是在该点的所有次梯度 $g$ 的集合，由不等式 $f(z) \\ge f(x) + g^T(z-x)$ 对所有 $z$ 定义。\n对于特定函数 $f(x) = \\|x\\|_{\\infty}$，其次微分 $\\partial f(x)$ 由以下集合给出：\n$$ \\partial f(x) = \\{ g \\in \\mathbb{R}^4 \\mid \\|g\\|_1 \\le 1 \\text{ 且 } g^T x = \\|x\\|_{\\infty} \\} $$\n其中 $\\|g\\|_1 = \\sum_{i=1}^4 |g_i|$ 是L1范数，它是无穷范数的对偶范数。\n次微分 $\\partial f(x)$ 在 $f$ 不可微的点是集值的（包含多个向量）。对于无穷范数，这种情况发生在分量的绝对值的最大值由多个索引达到时。\n\n我们选择一个满足此条件的点 $x \\in \\mathbb{R}^4$。考虑点 $x = (5, -5, 1, 2)$。\n对于这个点，$\\|x\\|_{\\infty} = \\max\\{|5|, |-5|, |1|, |2|\\} = 5$。最大模由两个分量 $x_1$ 和 $x_2$ 达到。\n\n现在，我们必须找到至少两个不同的向量 $g^{(1)}$ 和 $g^{(2)}$，它们满足属于 $\\partial f(x)$ 的条件。这些条件是：\n1. $\\|g\\|_1 \\le 1$\n2. $g^T x = g_1(5) + g_2(-5) + g_3(1) + g_4(2) = 5$\n\n我们来测试向量 $g^{(1)} = (1, 0, 0, 0)$。\n1. $\\|g^{(1)}\\|_1 = |1| + |0| + |0| + |0| = 1$。条件 $\\|g^{(1)}\\|_1 \\le 1$ 得到满足。\n2. $(g^{(1)})^T x = (1)(5) + (0)(-5) + (0)(1) + (0)(2) = 5$。这与 $\\|x\\|_{\\infty} = 5$ 相匹配。\n所以，$g^{(1)} = (1, 0, 0, 0)$ 是 $\\partial f(x)$ 中的一个有效次梯度。\n\n接下来，我们测试向量 $g^{(2)} = (0, -1, 0, 0)$。\n1. $\\|g^{(2)}\\|_1 = |0| + |-1| + |0| + |0| = 1$。条件 $\\|g^{(2)}\\|_1 \\le 1$ 得到满足。\n2. $(g^{(2)})^T x = (0)(5) + (-1)(-5) + (0)(1) + (0)(2) = 5$。这也与 $\\|x\\|_{\\infty} = 5$ 相匹配。\n所以，$g^{(2)} = (0, -1, 0, 0)$ 是 $\\partial f(x)$ 中的另一个有效次梯度。\n\n由于 $g^{(1)} \\neq g^{(2)}$，我们已经证明在点 $x = (5, -5, 1, 2)$ 处的次微分 $\\partial f(x)$ 包含至少两个不同的向量。因此，它在该点是集值的。事实上，$\\partial f(x)$ 是这类“基本”次梯度的凸包。对于这个 $x$，$\\partial f(x) = \\text{conv}\\{(1,0,0,0), (0,-1,0,0)\\}$。\n\n### 第2部分：预解式的计算\n\n我们需要在 $\\lambda = 2$ 和 $y = (3, 2, -1, 0.5)$ 的条件下计算 $x_{\\text{out}} = (I + \\lambda \\partial f)^{-1}(y)$。\n这等价于找到唯一的点 $x_{\\text{out}}$ 满足关系式 $y \\in x_{\\text{out}} + \\lambda \\partial f(x_{\\text{out}})$。这可以重排为 $\\frac{y - x_{\\text{out}}}{\\lambda} \\in \\partial f(x_{\\text{out}})$。\n这个定义对应于邻近算子(proximal operator)。我们旨在找到：\n$$ x_{\\text{out}} = \\text{prox}_{\\lambda f}(y) = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ \\lambda f(x) + \\frac{1}{2} \\|x - y\\|_2^2 \\right\\} $$\n代入给定值和 $f$ 的定义：\n$$ x_{\\text{out}} = \\arg\\min_{x \\in \\mathbb{R}^4} \\left\\{ 2 \\|x\\|_{\\infty} + \\frac{1}{2} \\|x - (3, 2, -1, 0.5)\\|_2^2 \\right\\} $$\n令 $\\alpha = \\|x\\|_{\\infty}$。该优化问题可以表述为在约束 $|x_i| \\le \\alpha$ (对所有 $i=1, \\dots, 4$)下，最小化 $2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 (x_i - y_i)^2$。\n对于一个固定的值 $\\alpha \\ge 0$，我们首先对 $x$ 进行最小化。问题对每个分量 $x_i$ 是解耦的：\n$$ \\min_{x_i} \\frac{1}{2} (x_i - y_i)^2 \\quad \\text{subject to} \\quad -\\alpha \\le x_i \\le \\alpha $$\n解是 $y_i$ 在区间 $[-\\alpha, \\alpha]$ 上的投影，由 $x_i(\\alpha) = \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\}$ 给出。\n将此代回目标函数，得到一个仅关于 $\\alpha$ 的问题：\n$$ \\min_{\\alpha \\ge 0} H(\\alpha) = \\min_{\\alpha \\ge 0} \\left\\{ 2\\alpha + \\frac{1}{2} \\sum_{i=1}^4 \\left( \\text{sign}(y_i) \\min\\{|y_i|, \\alpha\\} - y_i \\right)^2 \\right\\} $$\n如果 $|y_i| \\le \\alpha$，求和号内的项为零；如果 $|y_i| > \\alpha$，则为 $(\\text{sign}(y_i)\\alpha - \\text{sign}(y_i)|y_i|)^2 = (\\alpha - |y_i|)^2$。因此，我们有：\n$$ H(\\alpha) = 2\\alpha + \\frac{1}{2} \\sum_{i \\text{ s.t. } |y_i| > \\alpha} (|y_i| - \\alpha)^2 $$\n为了找到这个凸函数 $H(\\alpha)$ 的最小值，我们可以将其关于 $\\alpha$ 的导数设为零。其导数为：\n$$ H'(\\alpha) = 2 + \\sum_{i \\text{ s.t. } |y_i| > \\alpha} \\frac{1}{2} \\cdot 2(|y_i| - \\alpha) \\cdot (-1) = 2 - \\sum_{i \\text{ s.t. } |y_i| > \\alpha} (|y_i| - \\alpha) $$\n$y=(3, 2, -1, 0.5)$ 的各分量的绝对值为 $\\{3, 2, 1, 0.5\\}$。我们来测试最优 $\\alpha^*$ 的区间。\n- 如果 $1 \\le \\alpha < 2$：模大于 $\\alpha$ 的分量是 $\\{3, 2\\}$。\n  $H'(\\alpha) = 2 - ((3-\\alpha) + (2-\\alpha)) = 2 - (5 - 2\\alpha) = 2\\alpha - 3$。\n  令 $H'(\\alpha) = 0$ 得到 $2\\alpha - 3 = 0$，解得 $\\alpha = \\frac{3}{2} = 1.5$。\n  由于 $1.5 \\in [1, 2)$，这是我们的最优值，$\\alpha^* = 1.5$。\n\n现在我们使用 $\\alpha^* = 1.5$ 计算解向量 $x_{\\text{out}}$ 的分量：\n$x_{\\text{out}, i} = \\text{sign}(y_i) \\min\\{|y_i|, 1.5\\}$。\n- $y_1=3$: $x_{\\text{out}, 1} = \\text{sign}(3) \\min\\{3, 1.5\\} = 1 \\cdot 1.5 = 1.5 = \\frac{3}{2}$。\n- $y_2=2$: $x_{\\text{out}, 2} = \\text{sign}(2) \\min\\{2, 1.5\\} = 1 \\cdot 1.5 = 1.5 = \\frac{3}{2}$。\n- $y_3=-1$: $x_{\\text{out}, 3} = \\text{sign}(-1) \\min\\{|-1|, 1.5\\} = -1 \\cdot 1 = -1$。\n- $y_4=0.5$: $x_{\\text{out}, 4} = \\text{sign}(0.5) \\min\\{0.5, 1.5\\} = 1 \\cdot 0.5 = 0.5 = \\frac{1}{2}$。\n因此，计算出的预解式值为 $x_{\\text{out}} = (1.5, 1.5, -1, 0.5) = (\\frac{3}{2}, \\frac{3}{2}, -1, \\frac{1}{2})$。\n\n### 第3部分：结果的解释\n\n邻近算子 $x_{\\text{out}} = \\text{prox}_{\\lambda f}(y)$ 找到一个点 $x$，该点平衡了两个目标：接近 $y$（最小化 $\\frac{1}{2}\\|x-y\\|_2^2$）和拥有一个小的无穷范数（最小化 $\\lambda \\|x\\|_\\infty$）。\n解 $x_{\\text{out}} = (1.5, 1.5, -1, 0.5)$ 揭示了无穷范数近端映射特有的一种“收缩”（shrinking）机制。\n令阈值为 $\\alpha^* = \\|x_{\\text{out}}\\|_{\\infty} = 1.5$。\n1. 对于 $y$ 中模小于或等于此阈值的分量（$|y_3|=1 < 1.5$ 和 $|y_4|=0.5 < 1.5$），$x_{\\text{out}}$ 的相应分量保持不变：$x_{\\text{out},3} = y_3$ 和 $x_{\\text{out},4} = y_4$。对于这些分量，最小化与 $y$ 的邻近度占主导地位，因为它们对最终的无穷范数没有贡献。\n2. 对于 $y$ 中模大于此阈值的分量（$|y_1|=3 > 1.5$ 和 $|y_2|=2 > 1.5$），$x_{\\text{out}}$ 的相应分量被“收缩”到阈值水平，同时保留其符号。因此，$x_{\\text{out},1} = 1.5$ 和 $x_{\\text{out},2} = 1.5$。这些分量现在构成了 $x_{\\text{out}}$ 的活动集（active set），即满足 $|x_i| = \\|x\\|_{\\infty}$ 的索引 $i$ 的集合。\n\n这种行为由次梯度条件决定。向量 $g = \\frac{y-x_{\\text{out}}}{\\lambda} = \\frac{(3-1.5, 2-1.5, -1-(-1), 0.5-0.5)}{2} = (0.75, 0.25, 0, 0)$ 必须是 $f$ 在 $x_{\\text{out}}$ 处的一个次梯度。\n次微分 $\\partial f(x_{\\text{out}})$ 是对应于活动集的基本次梯度（即 $e_1=(1,0,0,0)$ 和 $e_2=(0,1,0,0)$）的凸包。计算出的次梯度是 $g = 0.75 e_1 + 0.25 e_2$，这是一个凸组合。\n这表明原始最大分量（$y_1, y_2$）的影响被重新分配和拉平了。模的总减少量 $\\sum_{i \\in \\{1,2\\}} (|y_i| - \\alpha^*) = (3-1.5) + (2-1.5) = 1.5 + 0.5 = 2$ 正好等于参数 $\\lambda$。近端算子有效地使用了“预算” $\\lambda$ 来收缩最大的分量，直到总收缩成本等于 $\\lambda$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{3}{2} & \\frac{3}{2} & -1 & \\frac{1}{2} \\end{pmatrix}\n}\n$$", "id": "3168264"}, {"introduction": "最后的这项实践旨在通过解决一个带组套索（group Lasso）惩罚的稀疏逻辑回归问题，来弥合理论与实际应用之间的鸿沟。在本练习 [@problem_id:3168254] 中，您将实现邻近点算法，其中每一步本身就需要使用邻近梯度法来求解一个子问题。这将为您提供全面的动手经验，学习如何推导组别邻近算子，并构建嵌套算法来解决机器学习中常见的复杂复合优化问题。", "problem": "考虑一个带稀疏性诱导组惩罚项的二分类凸优化问题。设数据矩阵为 $X \\in \\mathbb{R}^{n \\times d}$，标签向量为 $y \\in \\{-1,+1\\}^n$。定义逻辑斯蒂损失\n$$\nf(w) \\equiv \\frac{1}{n} \\sum_{i=1}^{n} \\log\\bigl(1 + \\exp(-y_i \\, x_i^\\top w)\\bigr),\n$$\n其中 $w \\in \\mathbb{R}^d$，$x_i^\\top$ 是 $X$ 的第 $i$ 行。假设特征索引被划分为覆盖 $\\{1,2,\\dots,d\\}$ 的不相交组 $\\{G_1, G_2, \\dots, G_m\\}$，并定义组 Lasso 惩罚项\n$$\nR(w) \\equiv \\sum_{g=1}^m \\|w_{G_g}\\|_2,\n$$\n其中 $w_{G_g}$ 汇集了由组 $G_g$ 索引的 $w$ 的坐标。对于正则化参数 $\\lambda \\ge 0$，学习问题是\n$$\n\\min_{w \\in \\mathbb{R}^d} \\; F(w) \\equiv f(w) + \\lambda R(w).\n$$\n\n你的任务是：\n1) 从正常、闭、凸函数 $\\phi$ 的近端算子的定义出发，\n$$\n\\mathrm{prox}_{\\alpha \\phi}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\phi(u) + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\}, \\quad \\alpha > 0,\n$$\n推导出与分组惩罚项 $\\phi(w) = \\sum_{g=1}^m \\|w_{G_g}\\|_2$ 相关的近端映射的显式形式，该映射逐组地作用于任何 $v \\in \\mathbb{R}^d$。\n\n2) 将邻近点算法 (PPA) 应用于 $F(w)$。PPA 的迭代点为\n$$\nw^{k+1} \\in \\arg\\min_{w \\in \\mathbb{R}^d} \\left\\{ f(w) + \\lambda R(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 \\right\\},\n$$\n其中步长参数 $t > 0$。每个子问题都是强凸的。对于数值实现，通过在复合目标函数 $f(w) + \\frac{1}{2t}\\|w - w^k\\|_2^2 + \\lambda R(w)$ 上使用近端梯度法来求解每个 PPA 子问题，步长为常数，基于光滑部分梯度的 Lipschitz 常数的上界。利用逻辑斯蒂损失的 Hessian 矩阵满足 $\\nabla^2 f(w) \\preceq \\frac{1}{4n} X^\\top X$（对所有 $w$ 成立）这一事实，因此一个可接受的 Lipschitz 常数是 $L_f \\le \\frac{\\lambda_{\\max}(X^\\top X)}{4n}$，并且子问题的光滑部分具有 Lipschitz 常数 $L_{\\text{sub}} = L_f + \\frac{1}{t}$。使用步长 $\\frac{1}{L_{\\text{sub}}}$。\n\n3) 通过计算收敛时的活跃组数量来评估 $\\lambda$ 变化时的分组效应，其中如果 $\\|w_{G_g}\\|_2 > \\epsilon$（对于某个小阈值 $\\epsilon$），则组 $G_g$ 被称为活跃组。同时报告达到收敛所用的 PPA 外层迭代次数以及收敛解的最终目标函数值 $F(w)$。\n\n使用以下固定实例：\n- 维度：$n = 12$，$d = 6$，以及 $m = 3$ 个组，其中 $G_1 = \\{1,2\\}$, $G_2 = \\{3,4\\}$, $G_3 = \\{5,6\\}$。\n- 数据矩阵 $X$（行 $x_i^\\top$）：\n  - 第 1 行：$[0.50, -1.20, 0.30, 0.80, -0.50, 1.00]$\n  - 第 2 行：$[1.50, 0.20, -0.30, 0.40, 0.70, -1.20]$\n  - 第 3 行：$[-0.80, 0.90, 1.10, -1.30, 0.20, 0.50]$\n  - 第 4 行：$[0.00, 0.30, -0.70, 0.60, -1.00, 0.90]$\n  - 第 5 行：$[1.20, -0.50, 0.60, -0.20, 0.40, -0.80]$\n  - 第 6 行：$[-1.10, 1.40, -0.40, 0.50, -0.60, 0.30]$\n  - 第 7 行：$[0.70, -0.90, 0.20, -0.40, 1.30, -0.70]$\n  - 第 8 行：$[-0.60, 0.80, -1.20, 1.00, -0.30, 0.20]$\n  - 第 9 行：$[0.90, -0.40, 0.10, -0.90, 0.80, -0.60]$\n  - 第 10 行：$[-0.30, 1.10, -0.50, 0.20, -0.70, 1.20]$\n  - 第 11 行：$[0.40, -1.00, 0.90, -0.10, 0.60, -0.40]$\n  - 第 12 行：$[-0.90, 0.60, -0.80, 1.20, -0.20, 0.10]$\n- 标签 $y$：$[+1, +1, -1, -1, +1, -1, +1, -1, +1, -1, +1, -1]$。\n\n实现细节：\n- 初始化 $w^0 = 0$。\n- 使用常数 $t = 1.0$。\n- PPA 外循环停止规则：当 $\\|w^{k+1} - w^k\\|_2 \\le 10^{-6}$ 或达到 100 次外循环迭代时停止，以先到者为准。\n- 每个子问题的内层近端梯度循环：使用步长 $\\frac{1}{L_{\\text{sub}}}$，其中 $L_{\\text{sub}} = \\frac{\\lambda_{\\max}(X^\\top X)}{4n} + \\frac{1}{t}$，当迭代点变化的欧几里得范数 $\\le 10^{-8}$ 或达到 500 次内循环迭代时停止，以先到者为准。\n- 活跃组阈值：$\\epsilon = 10^{-6}$。\n\n测试套件：\n- 四个正则化水平 $\\lambda \\in \\{0.0, 0.1, 0.5, 2.0\\}$。\n- 对于每个 $\\lambda$，按规定运行 PPA 并计算：\n  - 收敛时的活跃组数量（一个整数），\n  - 使用的外层迭代次数（一个整数），\n  - 收敛后 $w$ 的最终目标函数值 $F(w)$，四舍五入到小数点后六位（一个浮点数）。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个条目按给定顺序对应一个 $\\lambda$，其本身是一个包含三个值的列表：$[\\text{active\\_groups}, \\text{outer\\_iterations}, \\text{final\\_objective}]$。例如，整体输出必须类似于\n$$\n[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]],\n$$\n不得包含任何额外文本。", "solution": "### 第 1 部分：组 Lasso 惩罚项的近端算子推导\n\n正常、闭、凸函数 $\\phi: \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ 的近端算子定义为：\n$$\n\\mathrm{prox}_{\\alpha \\phi}(v) \\equiv \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\phi(u) + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\}, \\quad \\alpha > 0\n$$\n在这个问题中，函数 $\\phi(w)$ 是组 Lasso 惩罚项 $R(w) = \\sum_{g=1}^m \\|w_{G_g}\\|_2$。我们需要求解 $\\mathrm{prox}_{\\alpha R}(v)$。\n最小化问题是：\n$$\n\\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\sum_{g=1}^m \\|u_{G_g}\\|_2 + \\frac{1}{2\\alpha}\\|u - v\\|_2^2 \\right\\}\n$$\n目标函数相对于不相交的坐标组 $\\{G_1, \\dots, G_m\\}$ 是可分的。欧几里得范数的平方项也可以分解：$\\|u - v\\|_2^2 = \\sum_{g=1}^m \\|u_{G_g} - v_{G_g}\\|_2^2$。因此，我们可以为每个组独立地求解最小化问题。对于每个组 $g \\in \\{1, \\dots, m\\}$，我们求解：\n$$\n(\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\arg\\min_{u_g \\in \\mathbb{R}^{|G_g|}} \\left\\{ \\|u_g\\|_2 + \\frac{1}{2\\alpha}\\|u_g - v_g\\|_2^2 \\right\\}\n$$\n其中我们使用简写 $u_g = u_{G_g}$ 和 $v_g = v_{G_g}$。\n\n该子问题的一阶最优性条件表明，零向量必须位于目标函数在最小值点 $u_g^*$ 处的次梯度中。次梯度为 $\\partial \\|u_g^*\\|_2 + \\frac{1}{\\alpha}(u_g^* - v_g)$。令其包含零可得：\n$$\n0 \\in \\partial \\|u_g^*\\|_2 + \\frac{1}{\\alpha}(u_g^* - v_g) \\implies v_g - u_g^* \\in \\alpha \\, \\partial \\|u_g^*\\|_2\n$$\n欧几里得范数的次梯度在 $z \\neq 0$ 时为 $\\partial \\|z\\|_2 = \\{z/\\|z\\|_2\\}$，在 $z = 0$ 时为闭单位球 $\\{z: \\|z\\|_2 \\le 1\\}$。我们对解 $u_g^*$ 分析两种情况：\n\n情况 1：$u_g^* \\neq 0$。最优性条件变为 $v_g - u_g^* = \\alpha \\frac{u_g^*}{\\|u_g^*\\|_2}$。对 $v_g$ 重新整理，我们得到 $v_g = u_g^* (1 + \\frac{\\alpha}{\\|u_g^*\\|_2})$。这表明 $u_g^*$ 必须与 $v_g$ 共线。对两边取欧几里得范数得出 $\\|v_g\\|_2 = \\|u_g^*\\|_2(1 + \\frac{\\alpha}{\\|u_g^*\\|_2}) = \\|u_g^*\\|_2 + \\alpha$。这意味着 $\\|u_g^*\\|_2 = \\|v_g\\|_2 - \\alpha$。为了这是一个非零解，我们必须有 $\\|v_g\\|_2 > \\alpha$。代回到整理后的方程中，我们找到解：\n$$\nu_g^* = v_g \\left( \\frac{1}{1 + \\alpha/\\|u_g^*\\|_2} \\right) = v_g \\left( \\frac{\\|u_g^*\\|_2}{\\|u_g^*\\|_2 + \\alpha} \\right) = v_g \\left( \\frac{\\|v_g\\|_2 - \\alpha}{\\|v_g\\|_2} \\right) = \\left(1 - \\frac{\\alpha}{\\|v_g\\|_2}\\right) v_g\n$$\n\n情况 2：$u_g^* = 0$。最优性条件变为 $v_g \\in \\alpha \\, \\partial \\|0\\|_2$，即集合 $\\{z : \\|z\\|_2 \\le \\alpha\\}$。因此，如果 $\\|v_g\\|_2 \\le \\alpha$，解为 $u_g^* = 0$。\n\n结合这两种情况，单个组的近端算子是：\n$$\n(\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\begin{cases} \\left(1 - \\frac{\\alpha}{\\|v_{G_g}\\|_2}\\right) v_{G_g} & \\text{if } \\|v_{G_g}\\|_2 > \\alpha \\\\ 0 & \\text{if } \\|v_{G_g}\\|_2 \\le \\alpha \\end{cases}\n$$\n这可以紧凑地写为分组软阈值算子：\n$$\n(\\mathrm{prox}_{\\alpha R}(v))_{G_g} = \\left(1 - \\frac{\\alpha}{\\|v_{G_g}\\|_2}\\right)_+ v_{G_g}\n$$\n其中 $(x)_+ = \\max(0, x)$。此公式应用于每个组 $G_g$（$g=1, \\dots, m$）以计算完整的近端映射 $\\mathrm{prox}_{\\alpha R}(v)$。\n\n### 代码实现\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the group-Lasso regularized logistic regression problem using a \n    Proximal Point Algorithm, with subproblems solved by a Proximal Gradient Method.\n    \"\"\"\n    \n    # ------------------ Problem Data and Fixed Parameters ------------------\n    X = np.array([\n        [0.50, -1.20, 0.30, 0.80, -0.50, 1.00],\n        [1.50, 0.20, -0.30, 0.40, 0.70, -1.20],\n        [-0.80, 0.90, 1.10, -1.30, 0.20, 0.50],\n        [0.00, 0.30, -0.70, 0.60, -1.00, 0.90],\n        [1.20, -0.50, 0.60, -0.20, 0.40, -0.80],\n        [-1.10, 1.40, -0.40, 0.50, -0.60, 0.30],\n        [0.70, -0.90, 0.20, -0.40, 1.30, -0.70],\n        [-0.60, 0.80, -1.20, 1.00, -0.30, 0.20],\n        [0.90, -0.40, 0.10, -0.90, 0.80, -0.60],\n        [-0.30, 1.10, -0.50, 0.20, -0.70, 1.20],\n        [0.40, -1.00, 0.90, -0.10, 0.60, -0.40],\n        [-0.90, 0.60, -0.80, 1.20, -0.20, 0.10],\n    ])\n    y = np.array([1, 1, -1, -1, 1, -1, 1, -1, 1, -1, 1, -1])\n    n, d, m = 12, 6, 3\n    groups = [(0, 2), (2, 4), (4, 6)]\n\n    # Algorithmic parameters\n    t = 1.0\n    outer_tol = 1e-6\n    max_outer_iter = 100\n    inner_tol = 1e-8\n    max_inner_iter = 500\n    epsilon = 1e-6\n    \n    # Test suite\n    lambdas = [0.0, 0.1, 0.5, 2.0]\n\n    # ------------------ Pre-computation for PGM Step Size ------------------\n    X_T_X = X.T @ X\n    lambda_max_XTX = np.linalg.eigvalsh(X_T_X)[-1]\n    L_f = lambda_max_XTX / (4 * n)\n    L_sub = L_f + 1.0 / t\n    eta = 1.0 / L_sub\n\n    # ------------------ Helper Functions ------------------\n    def logistic_loss(w):\n        y_scores = y * (X @ w)\n        return np.mean(np.logaddexp(0, -y_scores))\n\n    def grad_logistic_loss(w):\n        y_scores = y * (X @ w)\n        sigmoid_vals = 1.0 / (1.0 + np.exp(y_scores))\n        coeffs = -y * sigmoid_vals\n        return (X.T @ coeffs) / n\n\n    def group_lasso_penalty(w):\n        penalty = 0.0\n        for start, end in groups:\n            penalty += np.linalg.norm(w[start:end])\n        return penalty\n\n    def objective_function(w, _lambda):\n        return logistic_loss(w) + _lambda * group_lasso_penalty(w)\n\n    def prox_group_lasso(v, alpha):\n        u = np.zeros_like(v)\n        for start, end in groups:\n            v_g = v[start:end]\n            norm_v_g = np.linalg.norm(v_g)\n            if norm_v_g > 0:\n                scale = max(0, 1 - alpha / norm_v_g)\n                u[start:end] = scale * v_g\n        return u\n\n    # ------------------ Main Algorithm ------------------\n    all_results = []\n    \n    for _lambda in lambdas:\n        w = np.zeros(d)\n        \n        # Outer PPA loop\n        for k_outer in range(max_outer_iter):\n            w_prev_outer = w.copy()\n            \n            # Inner PGM loop for the PPA subproblem\n            z = w.copy()  # Warm start\n            for k_inner in range(max_inner_iter):\n                z_prev_inner = z.copy()\n                \n                # PGM update step\n                grad_smooth = grad_logistic_loss(z) + (z - w_prev_outer) / t\n                v = z - eta * grad_smooth\n                z = prox_group_lasso(v, eta * _lambda)\n                \n                if np.linalg.norm(z - z_prev_inner) = inner_tol:\n                    break\n            \n            w = z\n            if np.linalg.norm(w - w_prev_outer) = outer_tol:\n                break\n        \n        num_outer_iter = k_outer + 1\n        \n        # ------------------ Post-processing and Result Collection ------------------\n        active_groups = 0\n        for start, end in groups:\n            if np.linalg.norm(w[start:end]) > epsilon:\n                active_groups += 1\n\n        final_objective = objective_function(w, _lambda)\n        \n        all_results.append([active_groups, num_outer_iter, round(final_objective, 6)])\n\n    # ------------------ Final Output Formatting ------------------\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "answer": "$$\n\\boxed{\\texttt{[[3,9,0.228919],[3,9,0.47277],[1,9,0.672808],[0,2,0.693147]]}}\n$$", "id": "3168254"}]}