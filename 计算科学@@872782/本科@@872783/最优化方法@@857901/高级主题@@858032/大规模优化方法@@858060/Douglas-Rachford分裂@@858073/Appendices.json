{"hands_on_practices": [{"introduction": "要真正理解一个迭代算法，最好的方法之一就是亲手进行几步计算。这个练习将引导我们处理一个简单的可行性问题，通过手工计算道格拉斯-拉奇福德算法的迭代步骤，直观地理解其作为一系列投影和反射的几何本质。[@problem_id:3122413]", "problem": "考虑在$\\mathbb{R}^{2}$中寻找两个闭凸集$C$和$D$交集中的一个点的问题，其中$C$和$D$是如下定义的半空间\n$$C := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{2} \\le 0\\}, \\quad D := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{1} + x_{2} \\ge 0\\}.$$\n从点$x^{0} = (1,-3)$开始，应用标准道格拉斯-拉奇福德 (DR) 分裂算法，该可行性问题被表述为最小化$C$和$D$的指示函数之和，并使用单位参数。仅使用基本定义（邻近算子、到半空间的投影以及由投影定义的反射），解析地计算前三个道格拉斯-拉奇福德迭代点$x^{1}$、$x^{2}$和$x^{3}$。解释为什么在该几何排列的$C$和$D$以及该初始化的条件下，序列会表现出有限收敛。将$x^{3}$的值作为向量提供为最终答案。无需四舍五入，不涉及单位。", "solution": "问题是使用道格拉斯-拉奇福德 (DR) 分裂算法，在两个闭凸集$C$和$D$的交集中找到一个点。该问题被表述为最小化两个集合的指示函数之和，即$\\iota_C(x) + \\iota_D(x)$。这些集合已预先验证适用于此应用。没有矛盾或信息缺失。因此，该问题是有效的。\n\n集合由以下公式给出：\n$$C := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{2} \\le 0\\}$$\n$$D := \\{(x_{1},x_{2}) \\in \\mathbb{R}^{2} : x_{1} + x_{2} \\ge 0\\}$$\n\n用于寻找两集合$C$和$D$交集中一点的道格拉斯-拉奇福德分裂算法可以表示为以下迭代形式：\n$$x^{k+1} = T(x^k)$$\n其中算子$T$对于步长参数$\\lambda$定义为：\n$$T(x) = x + \\lambda (P_C(2P_D(x) - x) - P_D(x))$$\n问题指定了单位参数，因此我们设$\\lambda=1$。迭代公式为：\n$$x^{k+1} = x^k + P_C(2P_D(x^k) - x^k) - P_D(x^k)$$\n给定起始点$x^{0} = (1, -3)$。为了计算迭代点，我们首先需要投影算子$P_C$和$P_D$的解析表达式。\n\n**1. 投影算子**\n\n投影$P_S(x)$给出在闭凸集$S$中，按欧几里得范数衡量，与$x$最近的唯一一个点。\n\n- **到$C$的投影：** 集合$C$是下半平面，包括$x_1$轴。对于一个点$x = (x_1, x_2)$，如果$x_2 \\le 0$，该点已在$C$中，所以$P_C(x) = x$。如果$x_2 > 0$，则$C$中最近的点可通过将第二个分量设为$0$来找到。\n$$P_C(x_1, x_2) = (x_1, \\min(x_2, 0))$$\n\n- **到$D$的投影：** 集合$D$是由不等式$x_1 + x_2 \\ge 0$定义的半空间。其边界是直线$x_1 + x_2 = 0$。对于一个点$x = (x_1, x_2)$，如果$x_1 + x_2 \\ge 0$，那么$x \\in D$且$P_D(x) = x$。如果$x_1 + x_2  0$，投影可以通过从$x$沿着法向量$n = (1, 1)$方向移动到边界来找到。对于半空间$a^T z \\ge b$，当$a^T x  b$时，其投影公式为$P(x) = x - \\frac{a^T x - b}{\\|a\\|^2}a$。对于$D$，我们有$a=(1,1)^T$和$b=0$。\n$$P_D(x_1, x_2) = \\begin{cases} (x_1, x_2)  \\text{if } x_1 + x_2 \\ge 0 \\\\ (x_1, x_2) - \\frac{x_1+x_2}{1^2+1^2}(1,1) = \\left(\\frac{x_1-x_2}{2}, \\frac{x_2-x_1}{2}\\right)  \\text{if } x_1 + x_2  0 \\end{cases}$$\n\n**2. 迭代计算**\n\n我们从$x^0 = (1, -3)$开始。我们将中间投影$P_D(x^k)$记为$y^k$。迭代公式为$x^{k+1} = x^k + P_C(2y^k - x^k) - y^k$。\n\n**迭代 $k=0$：**\n- **初始点：** $x^0 = (1, -3)$。\n- **计算$y^0 = P_D(x^0)$：** $x^0$的分量之和为$1 + (-3) = -2$，小于$0$。我们使用点在$D$外部的投影公式：\n$$y^0 = P_D(1, -3) = \\left(\\frac{1 - (-3)}{2}, \\frac{-3 - 1}{2}\\right) = \\left(\\frac{4}{2}, \\frac{-4}{2}\\right) = (2, -2)$$\n- **计算$P_C$的参数：** 参数为$2y^0 - x^0$。这等同于将$x^0$关于集合$D$做反射，即$R_D(x^0) = 2P_D(x^0) - x^0$。\n$$2y^0 - x^0 = 2(2, -2) - (1, -3) = (4, -4) - (1, -3) = (3, -1)$$\n- **应用$P_C$：** 令$v^0 = (3, -1)$。$v^0$的第二个分量是$-1$，小于或等于$0$。因此，$v^0 \\in C$，其投影是它自身：\n$$P_C(3, -1) = (3, -1)$$\n- **计算$x^1$：**\n$$x^1 = x^0 + P_C(2y^0 - x^0) - y^0 = (1, -3) + (3, -1) - (2, -2)$$\n$$x^1 = (1+3-2, -3-1-(-2)) = (2, -4+2) = (2, -2)$$\n\n所以，第一个迭代点是$x^1 = (2, -2)$。\n\n**迭代 $k=1$：**\n- **从$x^1 = (2, -2)$开始。**\n- **计算$y^1 = P_D(x^1)$：** 分量之和为$2 + (-2) = 0$。因为$0 \\ge 0$，点$x^1$在$D$中。因此，投影是该点本身：\n$$y^1 = P_D(2, -2) = (2, -2)$$\n- **计算$P_C$的参数：**\n$$2y^1 - x^1 = 2(2, -2) - (2, -2) = (4, -4) - (2, -2) = (2, -2)$$\n- **应用$P_C$：** 令$v^1 = (2, -2)$。第二个分量是$-2 \\le 0$，所以$v^1 \\in C$。\n$$P_C(2, -2) = (2, -2)$$\n- **计算$x^2$：**\n$$x^2 = x^1 + P_C(2y^1 - x^1) - y^1 = (2, -2) + (2, -2) - (2, -2) = (2, -2)$$\n\n第二个迭代点是$x^2 = (2, -2)$。\n\n**迭代 $k=2$：**\n由于$x^2 = x^1$，点$(2, -2)$是DR算子$T$的一个不动点。所有后续的迭代点都将是相同的。\n$$x^3 = T(x^2) = T(x^1) = x^1 = (2, -2)$$\n因此，前三个迭代点是$x^1 = (2, -2)$，$x^2 = (2, -2)$和$x^3 = (2, -2)$。\n\n**3. 有限收敛性解释**\n\n序列$(x^k)$在一步之内收敛到不动点$x^*=(2,-2)$。在这种情况下，有限收敛是预期的，因为集合$C$和$D$是多面体（具体来说是半空间）。对于这类集合，已知道格拉斯-拉奇福德算法会在有限次迭代内收敛。\n\n对于本问题实例和初始化，其具体机制如下：\n1.  算法的第一步计算出$y^0 = P_D(x^0) = P_D(1, -3) = (2, -2)$。\n2.  这个点$y^0$恰好位于交集$C \\cap D$中：\n    - 对于$C$：$y^0$的第二个分量是$-2$，满足$-2 \\le 0$。所以，$y^0 \\in C$。\n    - 对于$D$：$y^0$是到$D$上的投影，所以它必然在$D$中。实际上，$2+(-2)=0 \\ge 0$。\n    因此，$y^0 \\in C \\cap D$。到其中一个集合的投影落到了一个可行解上。\n3.  算法变为$x^1 = x^0 + P_C(2y^0 - x^0) - y^0$。在这一步中能否收敛到$y^0$取决于$P_C(2y^0-x^0)$这一项。我们来分析一下：\n    $$2y^0 - x^0 = (3, -1)$$\n    由于第二个分量是$-1 \\le 0$，这个点已经在$C$中。这意味着$P_C(2y^0-x^0) = 2y^0-x^0$。\n4.  将此代入$x^1$的更新规则中：\n    $$x^1 = x^0 + (2y^0 - x^0) - y^0 = y^0$$\n    所以，$x^1 = y^0 = (2, -2)$。\n5.  由于$x^1 = (2,-2)$是交集$C \\cap D$中的一个点，它是一个有效解。正如在$k=1$的计算中所示，任何点$x \\in C \\cap D$都是$\\lambda = 1$时DR迭代的不动点。从这个点开始，序列变得平稳：对于所有$k \\ge 1$，$x^k = (2, -2)$。\n\n总而言之，对于此初始化，有限收敛是由于几何排列的缘故，其中初始投影$P_D(x^0)$已经是一个解，并且反射$R_D(x^0)=2P_D(x^0)-x^0$位于第二个集合$C$之内。这种组合在代数上迫使下一个迭代点成为那个解，而该解随后成为一个不动点。", "answer": "$$\\boxed{\\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}}$$", "id": "3122413"}, {"introduction": "道格拉斯-拉奇福德分裂算法的强大之处在于它能有效处理那些可以分解为多个“简单”部分的目标函数和约束。本练习将该方法应用于一个常见的约束最小二乘问题，并通过与简单的“裁剪”策略对比，突显其优越性。这个实践展示了如何通过引入乘积空间来处理更复杂的问题结构。[@problem_id:3122375]", "problem": "要求您设计并实现一个道格拉斯-拉奇福德 (Douglas-Rachford) 分裂法来解决一个约束最小残差问题，并将其与一个朴素的裁剪策略进行比较。令 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\tau  0$。考虑以下凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{\\infty} \\le \\tau.\n$$\n使用辅助变量 $y \\in \\mathbb{R}^{m}$ 将此问题在乘积空间中表述为\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, y \\in \\mathbb{R}^{m}} \\ \\|y\\|_{2} \\quad \\text{subject to} \\quad y = A x - b,\\ \\|x\\|_{\\infty} \\le \\tau.\n$$\n令 $C := \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ 且 $M := \\{(x,y) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{m} : y = A x - b\\}$。定义函数\n$$\nf(x,y) := \\iota_{M}(x,y), \\qquad g(x,y) := \\iota_{C}(x) + \\|y\\|_{2},\n$$\n其中 $\\iota_{S}$ 表示集合 $S$ 的指示函数。您的任务是：\n\n1) 使用邻近算子和欧几里得投影的基本定义，从第一性原理推导出道格拉斯-拉奇福德分裂所需的以下邻近映射。\n\n- 函数 $g$ 在步长 $\\gamma  0$ 处的邻近映射，即计算\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = \\arg\\min_{x,y} \\ \\iota_{C}(x) + \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2}.\n$$\n证明此问题可分解为 $x_{0}$ 到 $C$ 上的投影和 $y$ 的欧几里得范数的邻近映射，并明确描述每个分量的特征。\n\n- 函数 $f$ 在步长 $\\gamma  0$ 处的邻近映射，即计算\n$$\n\\operatorname{prox}_{\\gamma f}(x_{0},y_{0}) = \\arg\\min_{x,y} \\ \\iota_{M}(x,y) + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2}.\n$$\n证明这是 $(x_{0},y_{0})$ 到仿射集 $M$ 上的欧几里得投影，并仅使用基本线性代数（梯度、正规方程以及仿射子空间投影的特性）推导出该投影的闭式表达式。\n\n2) 在乘积空间 $\\mathbb{R}^{n} \\times \\mathbb{R}^{m}$ 中，为求和 $f+g$ 实现道格拉斯-拉奇福德迭代。对于给定的迭代点 $(x,y)$ 和步长 $\\gamma  0$，执行\n- 计算 $u = \\operatorname{prox}_{\\gamma g}(z)$，其中 $z = (x,y)$，\n- 计算 $r = 2u - z$，\n- 计算 $v = \\operatorname{prox}_{\\gamma f}(r)$，\n- 更新 $z \\leftarrow z + v - u$。\n当道格拉斯-拉奇福德间隙 $\\|v-u\\|_{2}$ 和可行性间隙 $\\|A u_{x} - b - u_{y}\\|_{2}$ 均低于一个小的容差时停止，其中 $u_{x}$ 和 $u_{y}$ 表示 $u$ 的 $x$ 和 $y$ 分块。返回候选解 $x_{\\mathrm{DR}} := u_{x}$ 并报告目标值 $\\|A x_{\\mathrm{DR}} - b\\|_{2}$。\n\n3) 作为基线，计算无约束最小二乘解 $x_{\\mathrm{LS}} := \\arg\\min_{x}\\|A x - b\\|_{2}$ 和裁剪解 $x_{\\mathrm{clip}} := \\operatorname{proj}_{C}(x_{\\mathrm{LS}})$，其中投影是到 $\\ell_{\\infty}$ 球上的投影。报告目标值 $\\|A x_{\\mathrm{clip}} - b\\|_{2}$。\n\n4) 测试套件。使用以下四个测试用例；在每个用例中，$A$ 以其行向量给出，$b$ 以其条目给出，$\\tau$ 为一个正标量。所有条目均为实数。\n\n- 用例 1：$m = 4$，$n = 3$，$\\tau = 0.5$，\n  $$\n  A = \\begin{bmatrix}\n  1.0  -0.5  0.3 \\\\\n  0.0  1.2  -0.4 \\\\\n  0.8  0.0  1.0 \\\\\n  -0.3  0.6  0.5\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.2 \\\\ 1.5 \\end{bmatrix}, \\quad\n  \\tau = 0.5.\n  $$\n\n- 用例 2：$m = 5$，$n = 3$，$\\tau = 10.0$，\n  $$\n  A = \\begin{bmatrix}\n  2.0  -1.0  0.0 \\\\\n  0.0  3.0  -1.0 \\\\\n  1.0  0.0  1.0 \\\\\n  0.0  -2.0  1.0 \\\\\n  1.5  0.5  -0.5\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}, \\quad\n  \\tau = 10.0.\n  $$\n\n- 用例 3：$m = 3$，$n = 4$，$\\tau = 0.8$，\n  $$\n  A = \\begin{bmatrix}\n  1.0  0.0  0.5  -0.2 \\\\\n  0.0  1.0  -0.3  0.7 \\\\\n  0.2  -0.5  0.0  1.0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}, \\quad\n  \\tau = 0.8.\n  $$\n\n- 用例 4：$m = 4$，$n = 4$，$\\tau = 0.3$，列秩亏（第三列等于前两列之和），\n  $$\n  A = \\begin{bmatrix}\n  1.0  0.0  1.0  0.5 \\\\\n  0.5  1.0  1.5  -0.3 \\\\\n  -0.2  0.3  0.1  0.0 \\\\\n  0.0  -0.7  -0.7  1.0\n  \\end{bmatrix}, \\quad\n  b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ -0.5 \\\\ 0.2 \\end{bmatrix}, \\quad\n  \\tau = 0.3.\n  $$\n\n5) 最终输出格式。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表。按顺序为每个测试用例附加三个值：\n- 道格拉斯-拉奇福德目标值 $\\|A x_{\\mathrm{DR}} - b\\|_{2}$，四舍五入到六位小数，\n- 裁剪基线目标值 $\\|A x_{\\mathrm{clip}} - b\\|_{2}$，四舍五入到六位小数，\n- 一个整数改进标志，如果 $\\|A x_{\\mathrm{DR}} - b\\|_{2} \\le \\|A x_{\\mathrm{clip}} - b\\|_{2} - 10^{-6}$ 则为 $1$，否则为 $0$。\n\n因此，最终打印的行必须如下所示\n$$\n[\\text{dr}_{1},\\text{clip}_{1},\\text{imp}_{1},\\text{dr}_{2},\\text{clip}_{2},\\text{imp}_{2},\\text{dr}_{3},\\text{clip}_{3},\\text{imp}_{3},\\text{dr}_{4},\\text{clip}_{4},\\text{imp}_{4}],\n$$\n其中每个 $\\text{dr}_{i}$ 和 $\\text{clip}_{i}$ 格式化为六位小数，每个 $\\text{imp}_{i}$ 为整数 $0$ 或 $1$。不应打印任何其他文本。不涉及角度；不需要物理单位。对两个间隙均使用 $\\gamma = 1$ 和 $10^{-9}$ 的停止容差，并将最大迭代次数限制在 $20000$ 次。", "solution": "用户提供了一个有效的、适定的凸优化问题。任务是使用道格拉斯-拉奇福德分裂法解决一个约束最小残差问题，并将其性能与一个简单的基线方法进行比较。该问题表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2} \\quad \\text{subject to} \\quad \\|x\\|_{\\infty} \\le \\tau\n$$\n通过引入辅助变量 $y \\in \\mathbb{R}^m$（使得 $y=Ax-b$），该问题在乘积空间 $\\mathbb{R}^n \\times \\mathbb{R}^m$ 中被重新表述为：\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, y \\in \\mathbb{R}^{m}} \\ \\|y\\|_{2} \\quad \\text{subject to} \\quad y = A x - b,\\ \\|x\\|_{\\infty} \\le \\tau\n$$\n通过定义两个我们希望最小化其和的函数 $f$ 和 $g$，这种结构适用于分裂法：\n$$\nf(x,y) := \\iota_{M}(x,y) \\quad \\text{and} \\quad g(x,y) := \\iota_{C}(x) + \\|y\\|_{2}\n$$\n此处，$\\iota_{S}$ 是集合 $S$ 的指示函数。集合 $C := \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ 是对 $x$ 的 $\\ell_{\\infty}$ 球约束，而 $M := \\{(x,y) \\in \\mathbb{R}^{n} \\times \\mathbb{R}^{m} : y = A x - b\\}$ 表示 $x$ 和 $y$ 之间的仿射关系。道格拉斯-拉奇福德算法需要这两个函数的邻近算子。\n\n**1. $g$ 的邻近算子 ($\\operatorname{prox}_{\\gamma g}$) 的推导**\n\n函数 $g$ 在步长 $\\gamma  0$ 处、点 $(x_0, y_0)$ 的邻近算子由以下最小化问题定义：\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = \\arg\\min_{x,y} \\left\\{ g(x,y) + \\frac{1}{2\\gamma}\\left(\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right) \\right\\}\n$$\n代入 $g(x,y) = \\iota_{C}(x) + \\|y\\|_{2}$ 的定义，问题变为：\n$$\n\\arg\\min_{x,y} \\left\\{ \\iota_{C}(x) + \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2} \\right\\}\n$$\n目标函数在 $x$ 和 $y$ 上是可加分离的，这使我们能够独立地求解每个变量。\n\n对于 $x$ 分量，最小化问题是：\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\iota_{C}(x) + \\frac{1}{2\\gamma}\\|x-x_{0}\\|_{2}^{2} \\right\\}\n$$\n指示函数 $\\iota_C(x)$ 在 $x \\in C$ 时为 $0$，否则为 $+\\infty$。因此，该问题等价于在 $C$ 中寻找欧几里得距离意义下最接近 $x_0$ 的点。根据定义，这就是 $x_0$ 到 $C$ 上的欧几里得投影：\n$$\nx^{*} = \\operatorname{proj}_{C}(x_0)\n$$\n集合 $C = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\le \\tau\\}$ 是一个由 $-\\tau \\le x_i \\le \\tau$（对于每个分量 $i$）定义的超立方体。因此，投影是一个逐分量的裁剪操作：\n$$\n(x^{*})_i = \\max(-\\tau, \\min((x_0)_i, \\tau))\n$$\n\n对于 $y$ 分量，最小化问题是：\n$$\ny^{*} = \\arg\\min_{y} \\left\\{ \\|y\\|_{2} + \\frac{1}{2\\gamma}\\|y-y_{0}\\|_{2}^{2} \\right\\}\n$$\n这是欧几里得范数的邻近算子 $\\operatorname{prox}_{\\gamma \\|\\cdot\\|_2}(y_0)$ 的定义。一阶最优性条件是 $0 \\in \\partial(\\|y^*\\|_2) + \\frac{1}{\\gamma}(y^* - y_0)$，其中 $\\partial$ 是次微分。这意味着 $y_0 - y^* \\in \\gamma \\partial(\\|y^*\\|_2)$。求解此式可得到块软阈值算子：\n$$\ny^{*} = \\max\\left(0, 1 - \\frac{\\gamma}{\\|y_0\\|_2}\\right) y_0\n$$\n为了处理 $y_0=0$ 的情况，可以将其更鲁棒地写为：\n$$\ny^{*} = \\left(1 - \\frac{\\gamma}{\\max(\\|y_0\\|_2, \\gamma)}\\right) y_0\n$$\n综合这些结果， $g$ 的邻近算子可分解为两个独立的操作：\n$$\n\\operatorname{prox}_{\\gamma g}(x_{0},y_{0}) = (\\operatorname{proj}_{C}(x_0), \\operatorname{prox}_{\\gamma \\|\\cdot\\|_2}(y_0))\n$$\n\n**2. $f$ 的邻近算子 ($\\operatorname{prox}_{\\gamma f}$) 的推导**\n\n函数 $f$ 的邻近算子定义为：\n$$\n\\operatorname{prox}_{\\gamma f}(x_{0},y_{0}) = \\arg\\min_{x,y} \\left\\{ \\iota_{M}(x,y) + \\frac{1}{2\\gamma}\\left(\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right) \\right\\}\n$$\n指示函数 $\\iota_M(x,y)$ 将最小化问题限制在仿射子空间 $M$ 上。缩放因子 $1/(2\\gamma)$ 不会改变最小化子，因此该问题等价于寻找 $(x_0, y_0)$ 到 $M$ 上的欧几里得投影：\n$$\n(x^*, y^*) = \\operatorname{proj}_{M}(x_0, y_0) = \\arg\\min_{(x,y) \\in M} \\left\\{\\|x-x_{0}\\|_{2}^{2} + \\|y-y_{0}\\|_{2}^{2}\\right\\}\n$$\n我们使用约束 $y = Ax - b$ 从目标函数中消去 $y$：\n$$\nx^* = \\arg\\min_{x} \\left\\{ \\|x-x_{0}\\|_{2}^{2} + \\|(Ax - b) - y_{0}\\|_{2}^{2} \\right\\} = \\arg\\min_{x} \\left\\{ \\|x-x_{0}\\|_{2}^{2} + \\|Ax - (b+y_{0})\\|_{2}^{2} \\right\\}\n$$\n这是一个无约束二次最小化问题。我们通过将目标函数 $L(x)$ 的梯度设为零来找到解：\n$$\n\\nabla_x L(x) = 2(x - x_0) + 2A^T(Ax - (b+y_0)) = 0\n$$\n重新整理各项，我们得到这个最小二乘问题的正规方程：\n$$\nx - x_0 + A^T A x - A^T b - A^T y_0 = 0 \\implies (I + A^T A)x = x_0 + A^T(b+y_0)\n$$\n矩阵 $I + A^T A$ 是对称正定的，因此是可逆的。即使 $A$ 是秩亏的，这也成立。$x^*$ 的解是：\n$$\nx^* = (I + A^T A)^{-1} (x_0 + A^T(b+y_0))\n$$\n相应的 $y^*$ 则由子空间约束确定：\n$$\ny^* = A x^* - b\n$$\n这就为 $\\operatorname{prox}_{\\gamma f}(x_0, y_0)$ 提供了一个闭式表达式。\n\n**3. 算法实现**\n\n道格拉斯-拉奇福德迭代按规定实现。从初始值 $z_0 = (x_0, y_0)$ 开始，通过以下方式生成迭代序列：\n$$\n\\begin{align*}\nu_{k+1} = \\operatorname{prox}_{\\gamma g}(z_k) \\\\\nr_{k+1} = 2u_{k+1} - z_k \\\\\nv_{k+1} = \\operatorname{prox}_{\\gamma f}(r_{k+1}) \\\\\nz_{k+1} = z_k + (v_{k+1} - u_{k+1})\n\\end{align*}\n$$\n当道格拉斯-拉奇福德间隙 $\\|v_{k+1}-u_{k+1}\\|_{2}$ 和可行性间隙 $\\|A (u_{k+1})_x - b - (u_{k+1})_y\\|_{2}$ 均低于指定的容差时，算法终止。解取为 $x_{\\mathrm{DR}} = (u_{k+1})_x$。对于基线方法，使用标准求解器找到无约束最小二乘解 $x_{\\mathrm{LS}} = \\arg\\min_x \\|Ax-b\\|_2$，然后将其裁剪到可行集 $C$ 中，得到 $x_{\\mathrm{clip}} = \\operatorname{proj}_C(x_{\\mathrm{LS}})$。", "answer": "```python\nimport numpy as np\n\ndef douglas_rachford_solver(A, b, tau, gamma, tol, max_iter):\n    \"\"\"\n    Solves the constrained least-residual problem using Douglas-Rachford splitting.\n    min ||Ax - b||_2  s.t. ||x||_inf = tau\n    \"\"\"\n    m, n = A.shape\n    \n    # Initialize iterates for the product space (x, y)\n    x = np.zeros(n)\n    y = np.zeros(m)\n    z = np.concatenate((x, y))\n\n    # Precompute the inverse matrix needed for prox_f\n    # (I + A^T A) is always invertible and well-conditioned\n    I_n = np.eye(n)\n    M_inv = np.linalg.inv(I_n + A.T @ A)\n\n    # Main iteration loop\n    for _ in range(max_iter):\n        xk, yk = z[:n], z[n:]\n\n        # --- Step 1: Compute prox_g(z) ---\n        # Part 1: prox for x is projection onto C (clipping)\n        ux = np.clip(xk, -tau, tau)\n        \n        # Part 2: prox for y is L2-norm block soft-thresholding\n        norm_yk = np.linalg.norm(yk)\n        # This formula is robust for yk = 0\n        uy = (1.0 - gamma / max(norm_yk, gamma)) * yk\n        u = np.concatenate((ux, uy))\n        \n        # --- Step 2: Reflection ---\n        r = 2 * u - z\n        rx, ry = r[:n], r[n:]\n\n        # --- Step 3: Compute prox_f(r) ---\n        # This is the projection onto the affine subspace M: y = Ax - b\n        rhs = rx + A.T @ (b + ry)\n        vx = M_inv @ rhs\n        vy = A @ vx - b\n        v = np.concatenate((vx, vy))\n\n        # --- Check stopping criteria ---\n        dr_gap = np.linalg.norm(v - u)\n        feasibility_gap = np.linalg.norm(A @ ux - b - uy)\n        \n        if dr_gap  tol and feasibility_gap  tol:\n            break\n\n        # --- Step 4: Update z ---\n        z = z + v - u\n\n    # The solution x is the x-part of the u iterate\n    x_dr = u[:n]\n    obj_dr = np.linalg.norm(A @ x_dr - b)\n    \n    return obj_dr\n\ndef baseline_clipping_solver(A, b, tau):\n    \"\"\"\n    Solves the unconstrained least-squares problem and clips the solution.\n    \"\"\"\n    # Find the unconstrained least-squares solution x_LS\n    x_ls = np.linalg.lstsq(A, b, rcond=None)[0]\n    \n    # Clip the solution to the feasible set C (l_infinity ball)\n    x_clip = np.clip(x_ls, -tau, tau)\n    \n    # Calculate the objective value for the clipped solution\n    obj_clip = np.linalg.norm(A @ x_clip - b)\n    \n    return obj_clip\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, -0.5, 0.3],\n                [0.0, 1.2, -0.4],\n                [0.8, 0.0, 1.0],\n                [-0.3, 0.6, 0.5]\n            ]),\n            \"b\": np.array([0.5, -1.0, 0.2, 1.5]),\n            \"tau\": 0.5\n        },\n        {\n            \"A\": np.array([\n                [2.0, -1.0, 0.0],\n                [0.0, 3.0, -1.0],\n                [1.0, 0.0, 1.0],\n                [0.0, -2.0, 1.0],\n                [1.5, 0.5, -0.5]\n            ]),\n            \"b\": np.array([1.0, -2.0, 0.5, -1.0, 0.0]),\n            \"tau\": 10.0\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 0.5, -0.2],\n                [0.0, 1.0, -0.3, 0.7],\n                [0.2, -0.5, 0.0, 1.0]\n            ]),\n            \"b\": np.array([0.1, -0.2, 0.3]),\n            \"tau\": 0.8\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 1.0, 0.5],\n                [0.5, 1.0, 1.5, -0.3],\n                [-0.2, 0.3, 0.1, 0.0],\n                [0.0, -0.7, -0.7, 1.0]\n            ]),\n            \"b\": np.array([0.0, 1.0, -0.5, 0.2]),\n            \"tau\": 0.3\n        }\n    ]\n\n    # Algorithm parameters\n    gamma = 1.0\n    tolerance = 1e-9\n    max_iterations = 20000\n\n    results = []\n    \n    for case in test_cases:\n        A, b, tau = case[\"A\"], case[\"b\"], case[\"tau\"]\n        \n        # Run Douglas-Rachford solver\n        dr_obj = douglas_rachford_solver(A, b, tau, gamma, tolerance, max_iterations)\n        \n        # Run baseline clipping solver\n        clip_obj = baseline_clipping_solver(A, b, tau)\n        \n        # Check for improvement\n        improvement = 1 if dr_obj = clip_obj - 1e-6 else 0\n        \n        # Append results in specified format\n        results.append(f\"{dr_obj:.6f}\")\n        results.append(f\"{clip_obj:.6f}\")\n        results.append(str(improvement))\n\n    # Print the final output string\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3122375"}, {"introduction": "在实际应用中，确定算法何时停止至关重要。这个高级练习将挑战著名的 LASSO 问题，并引导我们从第一性原理出发，推导并实现一个基于“原始-对偶间隙”（primal-dual gap）的停止准则。通过这个练习，我们将算法的不动点与最优性条件及理论保证联系起来，从而学习如何构建一个既实用又可靠的算法。[@problem_id:3122381]", "problem": "您需要研究用于最小化两个正常、闭合、凸函数之和的道格拉斯-拉奇福德 (Douglas-Rachford) 分裂法，并设计一个基于原始-对偶间隙的停止准则，该准则利用道格拉斯-拉奇福德算子的不动点与原始解之间的映射关系。您的任务分为三个部分：从第一性原理推导停止准则，为一类特定的凸函数实现该准则，以及在一系列案例中评估该准则的实际可靠性。\n\n考虑以下凸优化问题\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x),\n$$\n其中函数定义为\n$$\nf(x) = \\tfrac{1}{2}\\|x - c\\|_2^2, \\qquad g(x) = \\lambda \\|x\\|_1,\n$$\n其中向量 $c \\in \\mathbb{R}^n$ 和标量 $\\lambda \\ge 0$ 是给定的。道格拉斯-拉奇福德分裂迭代由邻近算子构建，对于任意正常、闭合、凸函数 $h$ 和参数 $\\gamma  0$，其定义为\n$$\n\\operatorname{prox}_{\\gamma h}(z) \\in \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\{ h(x) + \\tfrac{1}{2\\gamma}\\|x - z\\|_2^2 \\right\\}。\n$$\n\n任务：\n\n- 从邻近算子和凸次微分的定义出发，推导将道格拉斯-拉奇福德算子的不动点 $z^\\star$ 与优化问题的原始解 $x^\\star$ 关联起来的道格拉斯-拉奇福德不动点映射。设计一个基于原始-对偶间隙的停止准则，该准则仅使用可从当前迭代点和给定数据计算的量进行评估。\n- 使用 Fenchel 共轭和 Fenchel–Young 不等式来表示点对 $(x,s)$ 的可计算原始-对偶间隙，其中 $x \\in \\mathbb{R}^n$ 是一个原始候选解，$s \\in \\mathbb{R}^n$ 是受对偶可行性条件约束的对偶候选解。除了定义之外，不要假设任何闭式快捷方式；直接从这些原理推导出必须计算的内容。\n- 在适用时使用强凸性，通过将原始-对偶间隙容差与到真正最小化点的距离上界联系起来，来证明您的停止准则的可靠性。\n\n实现细节：\n\n- 仅使用上述定义，实现松弛参数等于 $1$ 的道格拉斯-拉奇福德分裂法。对于给定的特定函数 $f$ 和 $g$，从第一性原理推导并实现其邻近算子。\n- 在每次迭代中，通过您推导的不动点映射将当前点映射到一个原始候选解 $x_k$，并构建一个满足对偶可行性条件的对偶候选解 $s_k$。在每次迭代中计算原始-对偶间隙，并在其小于或等于预设容差时停止。\n- 对于这个特定问题，精确的原始最小化点由强凸性唯一确定；使用这一点来评估您的停止准则的可靠性，其可靠性通过返回的 $x$ 是否满足由间隙容差所隐含的理论界限来量化。\n\n测试套件：\n\n在以下测试案例上运行您的程序。每个案例是一个元组 $(c, \\lambda, \\gamma, \\text{tol}, \\text{max\\_iters})$，其中 $c \\in \\mathbb{R}^n$，$\\lambda \\ge 0$，$\\gamma  0$，并给出了停止容差和迭代上限。\n\n- 案例 1 (一般情况): $c = (3.0, -1.0, 0.5, -0.2, 0.0)$, $\\lambda = 0.8$, $\\gamma = 1.0$, $\\text{tol} = 10^{-4}$, $\\text{max\\_iters} = 10000$。\n- 案例 2 (边界情况 $\\lambda = 0$): $c = (1.2, -0.7, 0.0, 3.5)$, $\\lambda = 0.0$, $\\gamma = 1.0$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iters} = 10000$。\n- 案例 3 (导致解趋于零的强正则化): $c = (1.0, -1.0, 2.0)$, $\\lambda = 5.0$, $\\gamma = 1.0$, $\\text{tol} = 10^{-6}$, $\\text{max\\_iters} = 10000$。\n- 案例 4 (小步长): $c = (0.1, -0.05, 0.0)$, $\\lambda = 0.01$, $\\gamma = 0.1$, $\\text{tol} = 10^{-8}$, $\\text{max\\_iters} = 20000$。\n- 案例 5 (混合量级和符号): $c = (-2.0, 0.5, 4.0, -3.0)$, $\\lambda = 1.5$, $\\gamma = 0.7$, $\\text{tol} = 10^{-5}$, $\\text{max\\_iters} = 15000$。\n\n输出规范：\n\n- 对于每个测试案例，在以下精确意义上确定基于原始-对偶间隙的停止决策是否实际可靠：如果算法在原始-对偶间隙小于或等于容差 $\\text{tol}$ 时停止，那么返回的 $x$ 必须满足 $\\|x - x^\\star\\|_2 \\le \\sqrt{2\\,\\text{tol}}$，其中 $x^\\star$ 是给定 $(c,\\lambda)$ 下 $F$ 的精确最小化点。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，其中每个条目是相应测试案例可靠性条件的布尔真值，顺序与上面列出的顺序相同。例如，对于三个案例，输出格式必须与 $[\\text{True},\\text{False},\\text{True}]$ 完全一样。", "solution": "用户提供的问题是凸优化领域中一个有效且适定的问题。它要求针对一个 LASSO 型优化特例，推导、实现并评估一个道格拉斯-拉奇福德分裂算法。问题的所有组成部分都具有科学依据、数学上一致且可形式化。\n\n要解决的问题是：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; F(x) \\equiv f(x) + g(x)\n$$\n其中 $f(x) = \\tfrac{1}{2}\\|x - c\\|_2^2$ 且 $g(x) = \\lambda \\|x\\|_1$，对于给定的向量 $c \\in \\mathbb{R}^n$ 和标量 $\\lambda \\ge 0$。\n\n### 1. 道格拉斯-拉奇福德迭代与不动点映射\n\n道格拉斯-拉奇福德 (DR) 分裂法是一种用于最小化两个正常、闭合、凸函数之和的迭代算法。对于问题 $\\min_x (f(x) + g(x))$，松弛参数等于 $1$ 的一种 DR 迭代形式由下式给出：\n$$\nz_{k+1} = z_k + \\operatorname{prox}_{\\gamma f}\\left(2\\operatorname{prox}_{\\gamma g}(z_k) - z_k\\right) - \\operatorname{prox}_{\\gamma g}(z_k)\n$$\n其中 $\\gamma  0$ 是一个步长参数，$\\operatorname{prox}_{\\gamma h}(\\cdot)$ 是函数 $h$ 的邻近算子。\n\n该优化问题的解可以从这次迭代的一个不动点 $z^\\star$ 中找到。不动点 $z^\\star$ 满足 $z_{k+1} = z_k = z^\\star$，这意味着：\n$$\n\\operatorname{prox}_{\\gamma f}\\left(2\\operatorname{prox}_{\\gamma g}(z^\\star) - z^\\star\\right) - \\operatorname{prox}_{\\gamma g}(z^\\star) = 0\n$$\n让我们将原始解候选定义为 $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$。将其代入不动点方程可得：\n$$\nx^\\star = \\operatorname{prox}_{\\gamma f}(2x^\\star - z^\\star)\n$$\n邻近算子的定义表明，$p = \\operatorname{prox}_{\\gamma h}(q)$ 等价于变分包含关系 $q \\in p + \\gamma \\partial h(p)$，其中 $\\partial h$ 是 $h$ 的凸次微分。将此应用于我们的两个条件，我们得到：\n1. 从 $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$: $z^\\star \\in x^\\star + \\gamma \\partial g(x^\\star) \\implies \\frac{z^\\star - x^\\star}{\\gamma} \\in \\partial g(x^\\star)$。\n2. 从 $x^\\star = \\operatorname{prox}_{\\gamma f}(2x^\\star - z^\\star)$: $2x^\\star - z^\\star \\in x^\\star + \\gamma \\partial f(x^\\star) \\implies \\frac{x^\\star - z^\\star}{\\gamma} \\in \\partial f(x^\\star)$。\n\n令 $v = \\frac{x^\\star - z^\\star}{\\gamma}$。那么这两个包含关系变为 $-v \\in \\partial g(x^\\star)$ 和 $v \\in \\partial f(x^\\star)$。将它们相加得到 $0 \\in \\partial f(x^\\star) + \\partial g(x^\\star)$，这是原始问题 $\\min_x F(x)$ 的一阶最优性条件。因此，如果 $z^\\star$ 是 DR 算子的一个不动点，那么 $x^\\star = \\operatorname{prox}_{\\gamma g}(z^\\star)$ 就是一个原始最小化点。这就建立了从不动点到原始解所需的映射关系。\n\n### 2. 原始-对偶间隙与停止准则\n\n为了设计停止准则，我们构建原始-对偶间隙。原始问题是 $P = \\inf_x \\{f(x) + g(x)\\}$。Fenchel 对偶问题是 $D = \\sup_s \\{-f^*(s) - g^*(-s)\\}$，其中 $h^*(s) = \\sup_x \\{\\langle s, x \\rangle - h(x)\\}$ 是 Fenchel 共轭。\n对于原始候选解 $x$ 和对偶候选解 $s$，原始-对偶间隙为：\n$$\n\\text{Gap}(x, s) = (f(x) + g(x)) - (-f^*(s) - g^*(-s)) = f(x) + g(x) + f^*(s) + g^*(-s)\n$$\n根据 Fenchel-Young 不等式，该间隙始终是非负的。\n\n在 DR 算法的每次迭代 $k$ 中，我们生成一个原始候选解 $x_k = \\operatorname{prox}_{\\gamma g}(z_k)$。根据上面的推导，我们有包含关系 $\\frac{z_k - x_k}{\\gamma} \\in \\partial g(x_k)$。我们可以用它来构建一个对偶候选解。我们定义 $u_k = \\frac{z_k - x_k}{\\gamma}$。最优性条件要求找到一个 $s^\\star$ 使得 $-s^\\star \\in \\partial f(x^\\star)$ 并且 $s^\\star \\in \\partial g(x^\\star)$。这表明我们可以设置对偶候选解 $s_k = -u_k = \\frac{x_k-z_k}{\\gamma}$，从而使得 $-s_k \\in \\partial g(x_k)$。\n\n根据 Fenchel-Young 不等式，条件 $-s_k \\in \\partial g(x_k)$ 意味着 $g(x_k) + g^*(-s_k) = \\langle -s_k, x_k \\rangle$。这简化了间隙表达式：\n$$\n\\text{Gap}(x_k, s_k) = f(x_k) + g(x_k) + f^*(s_k) + g^*(-s_k) = f(x_k) + f^*(s_k) + \\langle-s_k, x_k\\rangle\n$$\n我们需要计算我们特定函数的共轭：\n- $f(x) = \\frac{1}{2}\\|x - c\\|_2^2$：其共轭是 $f^*(s) = \\frac{1}{2}\\|s\\|_2^2 + \\langle s, c \\rangle$。\n- $g(x) = \\lambda \\|x\\|_1$：其共轭是当 $\\|s\\|_\\infty \\le \\lambda$ 时 $g^*(s) = 0$，否则为 $\\infty$。\n\n对偶候选解 $s_k$ 的选择还有另一个有利的性质。由于 $-s_k \\in \\partial g(x_k) = \\lambda \\partial \\|x_k\\|_1$，其分量受 $\\lambda$ 限制，即 $\\|-s_k\\|_\\infty \\le \\lambda$。这意味着 $g^*(-s_k) = 0$，这保证了对偶目标是有限的。因此，间隙可以计算为：\n$$\n\\text{Gap}_k = f(x_k) + g(x_k) + f^*(s_k)\n$$\n代入 $f$, $g$ 和 $f^*$ 的表达式：\n$$\n\\text{Gap}_k = \\left(\\frac{1}{2}\\|x_k-c\\|_2^2\\right) + \\left(\\lambda\\|x_k\\|_1\\right) + \\left(\\frac{1}{2}\\|s_k\\|_2^2 + \\langle s_k, c \\rangle\\right)\n$$\n当 $s_k = \\frac{x_k-z_k}{\\gamma}$ 时，上式变为：\n$$\n\\text{Gap}_k = \\frac{1}{2}\\|x_k-c\\|_2^2 + \\lambda\\|x_k\\|_1 + \\frac{1}{2\\gamma^2}\\|x_k-z_k\\|_2^2 + \\frac{1}{\\gamma}\\langle x_k-z_k, c \\rangle\n$$\n这个表达式在每次迭代中仅使用迭代点 $z_k$、导出的 $x_k$ 和问题数据即可计算。当 $\\text{Gap}_k \\le \\text{tol}$ 时算法停止。\n\n### 3. 停止准则的可靠性\n\n目标函数 $F(x) = f(x) + g(x)$ 是一个 $1$-强凸函数（$f$）和一个凸函数（$g$）的和。因此，$F(x)$ 是 $1$-强凸的。对于一个 $\\mu$-强凸函数，以下不等式成立：\n$$\nF(x) - F(x^\\star) \\ge \\frac{\\mu}{2}\\|x - x^\\star\\|_2^2\n$$\n在我们的情况下，$\\mu = 1$。此外，对于任何原始-对偶对 $(x, s)$，间隙为原始目标的次优性提供了一个上界：$\\text{Gap}(x, s) \\ge F(x) - F(x^\\star)$。\n结合这些不等式，我们有：\n$$\n\\text{Gap}_k \\ge F(x_k) - F(x^\\star) \\ge \\frac{1}{2}\\|x_k - x^\\star\\|_2^2\n$$\n这给出了到真正最小化点 $x^\\star$ 的距离的一个界限：\n$$\n\\|x_k - x^\\star\\|_2 \\le \\sqrt{2 \\cdot \\text{Gap}_k}\n$$\n如果算法因为 $\\text{Gap}_k \\le \\text{tol}$ 而终止，则返回的解 $x_k$ 保证满足：\n$$\n\\|x_k - x^\\star\\|_2 \\le \\sqrt{2 \\cdot \\text{tol}}\n$$\n这为可靠的停止准则提供了理论基础。\n\n### 4. 实现细节：邻近算子和精确解\n\n为了实现该算法，我们需要邻近算子的闭式表达式。\n- 对于 $f(x) = \\frac{1}{2}\\|x-c\\|_2^2$：\n$$ \\operatorname{prox}_{\\gamma f}(z) = \\arg\\min_x \\left\\{\\frac{1}{2}\\|x-c\\|_2^2 + \\frac{1}{2\\gamma}\\|x-z\\|_2^2\\right\\} = \\frac{\\gamma c + z}{\\gamma+1} $$\n- 对于 $g(x) = \\lambda\\|x\\|_1$：\n$$ \\operatorname{prox}_{\\gamma g}(z) = \\arg\\min_x \\left\\{\\lambda\\|x\\|_1 + \\frac{1}{2\\gamma}\\|x-z\\|_2^2\\right\\} = S_{\\gamma\\lambda}(z) $$\n其中 $S_\\alpha(z)$ 是分量软阈值算子：$(S_\\alpha(z))_i = \\operatorname{sign}(z_i)\\max(|z_i|-\\alpha, 0)$。\n\n精确解 $x^\\star$ 可以从最优性条件 $0 \\in x^\\star - c + \\lambda\\partial\\|x^\\star\\|_1$ 中找到，这等价于 $x^\\star = \\operatorname{prox}_{\\lambda g}(c) = S_\\lambda(c)$。这使得我们可以直接计算真正的最小化点，以验证我们停止准则的可靠性。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Douglas-Rachford splitting method for a specific convex\n    optimization problem, evaluates a primal-dual gap-based stopping test,\n    and checks its reliability against a theoretical bound.\n    \"\"\"\n\n    def soft_threshold(z, threshold):\n        \"\"\"Component-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - threshold, 0)\n\n    def prox_f(z_in, c_vec, gamma_val):\n        \"\"\"Proximal operator of f(x) = 1/2*||x-c||_2^2.\"\"\"\n        return (gamma_val * c_vec + z_in) / (gamma_val + 1.0)\n\n    def prox_g(z_in, lambda_val, gamma_val):\n        \"\"\"Proximal operator of g(x) = lambda*||x||_1.\"\"\"\n        return soft_threshold(z_in, gamma_val * lambda_val)\n\n    def run_douglas_rachford(c, lam, gam, tol, max_iters):\n        \"\"\"\n        Executes the Douglas-Rachford algorithm for a given test case.\n        \"\"\"\n        n = c.shape[0]\n        z = np.zeros(n)\n        \n        final_x = np.zeros(n)\n        final_gap = np.inf\n\n        for _ in range(max_iters):\n            # Step 1: Compute primal candidate x_k\n            x = prox_g(z, lam, gam)\n            \n            # Step 2: Compute dual candidate s_k and the primal-dual gap\n            s = (x - z) / gam\n            \n            f_x = 0.5 * np.linalg.norm(x - c)**2\n            g_x = lam * np.linalg.norm(x, 1)\n            f_star_s = 0.5 * np.linalg.norm(s)**2 + np.dot(s, c)\n            \n            gap = f_x + g_x + f_star_s\n            \n            # Step 3: Check stopping condition\n            if gap = tol:\n                final_x = x\n                final_gap = gap\n                return final_x, final_gap\n\n            # Step 4: Perform the Douglas-Rachford update for z\n            y_prime = 2 * x - z\n            x_prime = prox_f(y_prime, c, gam)\n            z = z + x_prime - x\n        \n        # If max_iters is reached, return the last computed values\n        final_x = x\n        final_gap = gap\n        return final_x, final_gap\n\n    # Test suite from the problem statement\n    test_cases_data = [\n        (np.array([3.0, -1.0, 0.5, -0.2, 0.0]), 0.8, 1.0, 1e-4, 10000),\n        (np.array([1.2, -0.7, 0.0, 3.5]), 0.0, 1.0, 1e-6, 10000),\n        (np.array([1.0, -1.0, 2.0]), 5.0, 1.0, 1e-6, 10000),\n        (np.array([0.1, -0.05, 0.0]), 0.01, 0.1, 1e-8, 20000),\n        (np.array([-2.0, 0.5, 4.0, -3.0]), 1.5, 0.7, 1e-5, 15000),\n    ]\n\n    results = []\n    for c, lam, gam, tol, max_iters in test_cases_data:\n        # Run the algorithm\n        x_final, gap_final = run_douglas_rachford(c, lam, gam, tol, max_iters)\n\n        # Compute the exact solution\n        x_star = soft_threshold(c, lam)\n\n        # Check the reliability condition\n        # The test is reliable unless the algorithm stops due to tolerance\n        # AND the error bound is violated.\n        stopped_by_tol = (gap_final = tol)\n        \n        # Note: If gap is negative due to floating point error, it should be treated as 0 for the bound.\n        # But the primal-dual gap is theoretically non-negative.\n        bound_violated = (np.linalg.norm(x_final - x_star)  np.sqrt(2 * tol))\n\n        is_reliable = not (stopped_by_tol and bound_violated)\n        results.append(is_reliable)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3122381"}]}