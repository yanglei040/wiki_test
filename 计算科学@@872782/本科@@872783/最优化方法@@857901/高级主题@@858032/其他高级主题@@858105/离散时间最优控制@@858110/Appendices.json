{"hands_on_practices": [{"introduction": "时间延迟在实际控制系统中普遍存在，例如网络延迟或执行器响应滞后，它会严重影响系统性能甚至导致不稳定。本练习将引导你学习一种强大的技术——状态增广法，它能将一个带输入延迟的线性系统转化为一个等效的、无延迟的标准状态空间形式。通过这种转换，我们便可以应用成熟的线性二次调节器（LQR）框架来设计最优控制器，这对于处理现实世界中的控制挑战是一项至关重要的建模与设计技能。[@problem_id:3121144]", "problem": "考虑由 $x_{k+1} = a x_{k} + b u_{k-1}$ 给出的带有一步输入延迟的离散时间线性系统，其中 $x_{k} \\in \\mathbb{R}$ 是状态，$u_{k} \\in \\mathbb{R}$ 是控制输入。延迟为 $d=1$。要求将此延迟系统转换为等效的无延迟增广系统，并通过Riccati递归推导最优控制。\n\n设有限时域性能指标为\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f},$$\n其中 $q$、$r$ 和 $q_{f}$ 为非负标量，$N \\in \\mathbb{N}$ 为时域长度。假设最优控制是使用通过动态规划原理推导出的线性二次调节器 (LQR) 计算的。\n\n任务：\n1. 引入增广状态 $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$，并将无延迟增广动力学写成 $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ 的形式，用 $a$ 和 $b$ 表示 $\\bar{A}$ 和 $\\bar{B}$。\n2. 从动态规划原理出发，证明最优价值函数 $J_{k}^{\\star}(z_{k}) = z_{k}^{\\top} P_{k} z_{k}$ 是增广状态的二次型，并推导 $P_{k}$ 的Riccati递归，以及相应的最优状态反馈控制律 $u_{k}^{\\star} = -K_{k} z_{k}$，用 $\\bar{A}$、$\\bar{B}$、$Q_{z}$ 和 $R$ 表示，其中 $Q_{z}$ 是与给定性能指标一致的增广状态惩罚矩阵。\n3. 对于具体系统参数 $a = 2$、$b = 1$、$q = 1$、$r = 1$、$q_{f} = 1$，时域 $N = 2$，以及初始条件 $x_{0} = 2$、$u_{-1} = 1$，计算最优控制输入 $u_{0}^{\\star}$。您的最终答案必须是一个实数。无需四舍五入。", "solution": "问题陈述是离散时间最优控制理论中一个定义明确的标准问题。它要求推导和应用线性二次调节器（LQR）来解决一个具有单步输入延迟的系统。状态增广法是处理此类延迟的标准技术。该问题具有科学依据，内容自洽，并且为获得唯一解提供了所有必要的参数。因此，该问题是有效的。\n\n### 任务1：增广系统构建\n\n给定的带输入延迟的离散时间线性系统为：\n$$x_{k+1} = a x_{k} + b u_{k-1}$$\n其中 $x_k \\in \\mathbb{R}$ 是状态，$u_k \\in \\mathbb{R}$ 是在时刻 $k$ 的控制输入。增广状态向量定义为 $z_{k} \\triangleq \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix}$。\n\n我们要找到 $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ 形式的增广系统动力学。让我们构建时刻 $k+1$ 的状态向量：\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{(k+1)-1} \\end{pmatrix} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix}$$\n第一个分量 $x_{k+1}$ 由系统动力学给出：$x_{k+1} = a x_{k} + b u_{k-1}$。用 $z_k$ 的分量表示，即为 $x_{k+1} = a x_{k} + b u_{k-1} + 0 \\cdot u_k$。\n第二个分量 $u_k$ 可以写成一个平凡的恒等式：$u_k = 0 \\cdot x_k + 0 \\cdot u_{k-1} + 1 \\cdot u_k$。\n\n将这两个方程合并成矩阵形式，我们得到：\n$$z_{k+1} = \\begin{pmatrix} x_{k+1} \\\\ u_{k} \\end{pmatrix} = \\begin{pmatrix} a  b \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_{k} \\\\ u_{k-1} \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} u_{k}$$\n这与所需形式 $z_{k+1} = \\bar{A} z_{k} + \\bar{B} u_{k}$ 相符，其中增广系统矩阵为：\n$$\\bar{A} = \\begin{pmatrix} a  b \\\\ 0  0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n\n### 任务2：Riccati递归推导\n\n性能指标由下式给出：\n$$J = \\sum_{k=0}^{N-1} \\left( x_{k}^{2} q + u_{k}^{2} r \\right) + x_{N}^{2} q_{f}$$\n我们用增广状态 $z_k$ 重写此成本函数。\n阶段成本为 $L(x_k, u_k) = x_k^2 q + u_k^2 r$。\n由于 $x_k = \\begin{pmatrix} 1  0 \\end{pmatrix} z_k$，我们有 $x_k^2 q = \\left(\\begin{pmatrix} 1  0 \\end{pmatrix} z_k\\right)^2 q = z_k^\\top \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} q \\begin{pmatrix} 1  0 \\end{pmatrix} z_k = z_k^\\top \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix} z_k$。\n因此，阶段成本可以写为 $L(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k$，其中 $Q_z = \\begin{pmatrix} q  0 \\\\ 0  0 \\end{pmatrix}$ 且 $R = r$。\n\n终端成本为 $x_N^2 q_f$。类似地，这变为 $z_N^\\top P_N z_N$，其中 $P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix}$。\n增广LQR问题是在约束 $z_{k+1} = \\bar{A} z_k + \\bar{B} u_k$ 下最小化 $J = \\sum_{k=0}^{N-1} (z_k^\\top Q_z z_k + u_k^\\top R u_k) + z_N^\\top P_N z_N$。\n\n我们使用动态规划。令 $J_k^\\star(z_k)$ 为从时刻 $k$ 的状态 $z_k$ 出发的最优价值函数。Bellman 方程为：\n$$J_k^\\star(z_k) = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + J_{k+1}^\\star(z_{k+1}) \\right]$$\n我们假设价值函数具有二次型形式：$J_k^\\star(z_k) = z_k^\\top P_k z_k$，其中 $P_k$ 是一个对称半正定矩阵。终端条件为 $J_N^\\star(z_N) = z_N^\\top P_N z_N$，因此我们有 $P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix}$。\n\n代入 Bellman 方程：\n$$z_k^\\top P_k z_k = \\min_{u_k} \\left[ z_k^\\top Q_z z_k + u_k^\\top R u_k + (\\bar{A} z_k + \\bar{B} u_k)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k) \\right]$$\n让我们展开最小化内部的项，我们称之为 $\\mathcal{L}_k(z_k, u_k)$：\n$$\\mathcal{L}_k(z_k, u_k) = z_k^\\top Q_z z_k + u_k^\\top R u_k + z_k^\\top \\bar{A}^\\top P_{k+1} \\bar{A} z_k + 2 u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{A} z_k + u_k^\\top \\bar{B}^\\top P_{k+1} \\bar{B} u_k$$\n为了找到最优控制 $u_k^\\star$，我们对 $\\mathcal{L}_k$ 关于 $u_k$ 求梯度并令其为零：\n$$\\frac{\\partial \\mathcal{L}_k}{\\partial u_k} = 2 R u_k + 2 \\bar{B}^\\top P_{k+1} \\bar{A} z_k + 2 \\bar{B}^\\top P_{k+1} \\bar{B} u_k = 0$$\n解出 $u_k$：\n$$(R + \\bar{B}^\\top P_{k+1} \\bar{B}) u_k = - \\bar{B}^\\top P_{k+1} \\bar{A} z_k$$\n$$u_k^\\star = - (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A}) z_k$$\n这是一个状态反馈控制律 $u_k^\\star = -K_k z_k$，其时变增益矩阵为：\n$$K_k = (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} \\bar{B}^\\top P_{k+1} \\bar{A}$$\n将 $u_k^\\star$ 代回 Bellman 方程，得到 $P_k$ 的 Riccati 递归：\n$$z_k^\\top P_k z_k = z_k^\\top Q_z z_k + (u_k^\\star)^\\top R u_k^\\star + (\\bar{A} z_k + \\bar{B} u_k^\\star)^\\top P_{k+1} (\\bar{A} z_k + \\bar{B} u_k^\\star)$$\n经过代数整理，简化为：\n$$P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - (\\bar{A}^\\top P_{k+1} \\bar{B}) (R + \\bar{B}^\\top P_{k+1} \\bar{B})^{-1} (\\bar{B}^\\top P_{k+1} \\bar{A})$$\n该方程从终端条件 $P_N$ 开始逆时求解。一个等价的形式是 $P_k = Q_z + \\bar{A}^\\top P_{k+1} \\bar{A} - K_k^\\top (R + \\bar{B}^\\top P_{k+1} \\bar{B}) K_k$。\n\n### 任务3：数值计算\n\n给定参数为 $a = 2$、$b = 1$、$q = 1$、$r = 1$、$q_f = 1$ 和 $N = 2$。\n增广系统矩阵为：\n$$\\bar{A} = \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix}, \\quad \\bar{B} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\n成本矩阵为：\n$$Q_z = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}, \\quad R = 1$$\n我们需要找到 $u_0^\\star$，这需要计算 $K_0$。而计算 $K_0$ 需要 $P_1$，$P_1$ 又是由 $P_2$ 计算得出的。\n\n**步骤1：计算 $P_2$ (在 $k=N=2$ 时)**\n终端成本矩阵为：\n$$P_2 = P_N = \\begin{pmatrix} q_f  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$$\n\n**步骤2：计算 $P_1$ (在 $k=N-1=1$ 时)**\n我们使用 $k=1$ 时的 Riccati 递归：\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} - (\\bar{A}^\\top P_2 \\bar{B}) (R + \\bar{B}^\\top P_2 \\bar{B})^{-1} (\\bar{B}^\\top P_2 \\bar{A})$$\n让我们计算各项：\n$$\\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  1 \\end{pmatrix}$$\n$$\\bar{B}^\\top P_2 \\bar{B} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 0$$\n$$\\bar{A}^\\top P_2 \\bar{B} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n修正项为零。因此：\n$$P_1 = Q_z + \\bar{A}^\\top P_2 \\bar{A} = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 4  2 \\\\ 2  1 \\end{pmatrix} = \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix}$$\n\n**步骤3：计算 $K_0$ (在 $k=0$ 时)**\n增益 $K_0$ 由下式给出：\n$$K_0 = (R + \\bar{B}^\\top P_1 \\bar{B})^{-1} \\bar{B}^\\top P_1 \\bar{A}$$\n让我们计算各项：\n$$\\bar{B}^\\top P_1 \\bar{B} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1$$\n$$R + \\bar{B}^\\top P_1 \\bar{B} = 1 + 1 = 2$$\n$$\\bar{B}^\\top P_1 \\bar{A} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 5  2 \\\\ 2  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\end{pmatrix}$$\n现在，我们可以计算 $K_0$：\n$$K_0 = (2)^{-1} \\begin{pmatrix} 4  2 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 4  2 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\end{pmatrix}$$\n\n**步骤4：计算最优控制 $u_0^\\star$**\n最优控制为 $u_0^\\star = -K_0 z_0$。\n初始条件为 $x_0 = 2$ 和 $u_{-1} = 1$。初始增广状态为：\n$$z_0 = \\begin{pmatrix} x_0 \\\\ u_{-1} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\n最后，我们计算 $u_0^\\star$：\n$$u_0^\\star = -K_0 z_0 = -\\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = -(2 \\cdot 2 + 1 \\cdot 1) = -(4+1) = -5$$\n在 $k=0$ 时的最优控制输入是 $-5$。", "answer": "$$\\boxed{-5}$$", "id": "3121144"}, {"introduction": "现实世界中的系统往往受到随机性的影响，而噪声的结构对最优控制策略有着深远的影响。本练习将引导你从动态规划的第一性原理出发，分析当系统中存在状态依赖（乘性）噪声时，最优控制律和值函数会发生怎样的变化。通过推导，你将理解为何在这种情况下，著名的确定性等价原理会失效，以及噪声的方差如何直接影响最优反馈增益的设计，从而加深对随机控制理论核心思想的理解。[@problem_id:3121165]", "problem": "考虑一个标量离散时间系统，其过程噪声与状态相关（乘性）\n$$x_{k+1} = a\\,x_k + b\\,u_k + g\\,x_k\\,w_k,$$\n其中 $x_k \\in \\mathbb{R}$ 是状态，$u_k \\in \\mathbb{R}$ 是控制，$a,b,g \\in \\mathbb{R}$ 是已知常数，并且 $\\{w_k\\}$ 是一个独立同分布（i.i.d.）序列，满足 $\\mathbb{E}[w_k] = 0$，$\\mathrm{Var}(w_k) = \\sigma^2 \\in (0,\\infty)$，且具有有限高阶矩。假设阶段成本是二次的，\n$$\\ell(x_k,u_k) = q\\,x_k^2 + r\\,u_k^2,$$\n其中 $q \\ge 0$ 且 $r > 0$，目标是无限时域期望和\n$$J = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} \\ell(x_k,u_k)\\Big],$$\n受上述动力学约束。除非另有说明，否则假设完全状态 $x_k$ 被无误差地测量。您可以假设存在平稳最优策略且矩有限的标准条件成立。\n\n使用离散时间动态规划的基本原理和零均值独立同分布随机变量期望的基本性质，确定下列哪些陈述是正确的。选择所有适用项。\n\nA. 在全状态测量下，最优稳态线性反馈增益不依赖于 $\\sigma^2$；它与忽略噪声的确定性线性二次调节器（LQR）增益一致。\n\nB. 在全状态测量下，最优价值函数仍然是关于 $x$ 的二次函数，其系数满足一个修正的代数方程，该方程包含一个与 $g^2 \\sigma^2$ 成比例的附加项；因此，最优反馈增益依赖于 $\\sigma^2$。\n\nC. 在线性测量和高斯噪声的情况下，当过程噪声是状态相关的 $g\\,x_k\\,w_k$ 时，分离原理和确定性等价通常不成立：最优控制律不是应用于状态估计的确定性LQR。\n\nD. 在线性反馈 $u_k = -K x_k$ 下，均方稳定性要求 $((a - bK)^2 + g^2 \\sigma^2) < 1$；因此，一个能稳定相应确定性闭环的反馈在 $\\sigma^2$ 很大时可能是均方不稳定的。\n\nE. 如果 $g = 0$，最优反馈增益仍然依赖于 $\\sigma^2$，因为噪声通过其方差进入未来成本（cost-to-go）。", "solution": "首先验证问题陈述，以确保其具有科学依据、良定性和客观性。\n\n### 第1步：提取已知条件\n- **系统动力学：** $x_{k+1} = a\\,x_k + b\\,u_k + g\\,x_k\\,w_k$。\n- **状态、控制、常数：** $x_k \\in \\mathbb{R}$，$u_k \\in \\mathbb{R}$，$a, b, g \\in \\mathbb{R}$。\n- **噪声特性：** $\\{w_k\\}$ 是一个独立同分布（i.i.d.）序列，满足 $\\mathbb{E}[w_k] = 0$，$\\mathrm{Var}(w_k) = \\mathbb{E}[w_k^2] = \\sigma^2 \\in (0,\\infty)$，且具有有限高階矩。\n- **阶段成本：** $\\ell(x_k, u_k) = q\\,x_k^2 + r\\,u_k^2$，其中 $q \\ge 0$ 且 $r > 0$。\n- **目标函数：** 最小化 $J = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} \\ell(x_k, u_k)\\Big]$。\n- **假设：** 全状态测量（除非另有说明）、存在平稳最优策略且矩有限。\n\n### 第2步：使用提取的已知条件进行验证\n该问题是离散时间随机最优控制中的一个标准形式，具体来说是一个带乘性（状态相关）噪声的线性二次（LQ）问题。所有已知条件在数学上都是精确且一致的。该模型在科学上是合理的，并常用于控制理论及相关领域。所提供的假设确保了问题的良定性。\n\n### 第3步：结论与行动\n问题陈述是**有效的**。我们可以继续进行求解。\n\n### 从基本原理推导\n\n分析基于最优性原理，该原理指出最优价值函数 $V(x)$ 必须满足Bellman方程。对于一个无限时域的平稳问题，该方程为：\n$$V(x_k) = \\min_{u_k} \\mathbb{E}_{w_k} \\left[ \\ell(x_k, u_k) + V(x_{k+1}) \\right]$$\n鉴于二次阶段成本 $\\ell(x_k, u_k) = q\\,x_k^2 + r\\,u_k^2$，我们假设一个二次形式的价值函数 $V(x) = p\\,x^2$，其中 $p$ 为某个非负常数。\n\n将价值函数和阶段成本代入Bellman方程：\n$$p\\,x_k^2 = \\min_{u_k} \\left( q\\,x_k^2 + r\\,u_k^2 + \\mathbb{E}_{w_k} [p\\,x_{k+1}^2] \\right)$$\n问题的核心在于计算期望项。我们代入系统动力学 $x_{k+1} = a\\,x_k + b\\,u_k + g\\,x_k\\,w_k$：\n$$x_{k+1}^2 = \\left((a\\,x_k + b\\,u_k) + g\\,x_k\\,w_k\\right)^2 = (a\\,x_k + b\\,u_k)^2 + 2\\,g\\,x_k(a\\,x_k + b\\,u_k)w_k + g^2\\,x_k^2\\,w_k^2$$\n对 $w_k$ 取期望，并利用 $\\mathbb{E}[w_k] = 0$ 和 $\\mathbb{E}[w_k^2] = \\sigma^2$ 的事实：\n$$\\mathbb{E}_{w_k}[x_{k+1}^2] = (a\\,x_k + b\\,u_k)^2 + 2\\,g\\,x_k(a\\,x_k + b\\,u_k)\\mathbb{E}[w_k] + g^2\\,x_k^2\\,\\mathbb{E}[w_k^2]$$\n$$\\mathbb{E}_{w_k}[x_{k+1}^2] = (a\\,x_k + b\\,u_k)^2 + g^2\\,\\sigma^2\\,x_k^2 = (a^2 + g^2\\,\\sigma^2)x_k^2 + 2\\,a\\,b\\,x_k\\,u_k + b^2\\,u_k^2$$\n将此结果代回Bellman方程：\n$$p\\,x_k^2 = \\min_{u_k} \\left( q\\,x_k^2 + r\\,u_k^2 + p \\left[ (a^2 + g^2\\,\\sigma^2)x_k^2 + 2\\,a\\,b\\,x_k\\,u_k + b^2\\,u_k^2 \\right] \\right)$$\n$$p\\,x_k^2 = \\min_{u_k} \\left( \\left[ q + p(a^2 + g^2\\,\\sigma^2) \\right]x_k^2 + 2\\,p\\,a\\,b\\,x_k\\,u_k + (r + p\\,b^2)u_k^2 \\right)$$\n为了找到最优控制 $u_k^*$，我们最小化右侧关于 $u_k$ 的二次函数。一阶条件（将关于 $u_k$ 的导数设为零）为：\n$$\\frac{\\partial}{\\partial u_k}(\\dots) = 2\\,p\\,a\\,b\\,x_k + 2(r + p\\,b^2)u_k = 0$$\n解出 $u_k$：\n$$u_k^* = -\\frac{p\\,a\\,b}{r + p\\,b^2}\\,x_k$$\n这是一个线性反馈控制律，$u_k^* = -K\\,x_k$，其平稳反馈增益 $K$ 为：\n$$K = \\frac{p\\,a\\,b}{r + p\\,b^2}$$\n为了求出 $p$ 的值，我们将 $u_k^*$ 代回 $p\\,x_k^2$ 的方程中。经过代数化简，我们可以从两边消去 $x_k^2$，得到修正的离散代数Riccati方程（DARE）：\n$$p = q + p(a^2 + g^2\\,\\sigma^2) - \\frac{(p\\,a\\,b)^2}{r + p\\,b^2}$$\n这可以写成：\n$$p = q + p\\,a^2 - \\frac{p^2\\,a^2\\,b^2}{r + p\\,b^2} + p\\,g^2\\,\\sigma^2$$\n此方程的解 $p$ 决定了成本，并进而决定了最优反馈增益 $K$。\n\n### 逐项分析\n\n**A. 在全状态测量下，最优稳态线性反馈增益不依赖于 $\\sigma^2$；它与忽略噪声的确定性线性二次调节器（LQR）增益一致。**\n\n推导出的 $p$ 的DARE方程是 $p = q + p\\,a^2 + p\\,g^2\\,\\sigma^2 - \\frac{p^2\\,a^2\\,b^2}{r + p\\,b^2}$。该方程包含了 $p\\,g^2\\,\\sigma^2$ 项。对于 $g \\ne 0$ 和 $\\sigma^2 > 0$，此项确保了解 $p$ 依赖于方差 $\\sigma^2$。最优增益为 $K = \\frac{p\\,a\\,b}{r + p\\,b^2}$，是 $p$ 的函数。由于 $p$ 依赖于 $\\sigma^2$，增益 $K$ 也将依赖于 $\\sigma^2$。确定性LQR增益对应于 $g=0$（或 $\\sigma^2=0$）的情况，这将导致不同的 $p$ 值，从而得到不同的增益。因此，该陈述是错误的。\n\n**结论：错误**\n\n**B. 在全状态测量下，最优价值函数仍然是关于 $x$ 的二次函数，其系数满足一个修正的代数方程，该方程包含一个与 $g^2 \\sigma^2$ 成比例的附加项；因此，最优反馈增益依赖于 $\\sigma^2$。**\n\n我们的推导证实了此陈述的所有部分。\n1.  $V(x) = p\\,x^2$ 的拟设导出了一个一致的解，因此价值函数确实是二次的。\n2.  系数 $p$ 的DARE方程是 $p = \\left(q + p\\,a^2 - \\frac{p^2\\,a^2\\,b^2}{r + p\\,b^2}\\right) + p\\,g^2\\,\\sigma^2$。这是确定性系统的标准DARE方程加上一个附加项 $p\\,g^2\\,\\sigma^2$，该项与 $g^2\\,\\sigma^2$ 成比例。\n3.  如选项A的分析所示，因为 $p$ 依赖于 $\\sigma^2$，所以增益 $K$ 也依赖于 $\\sigma^2$。\n\n**结论：正确**\n\n**C. 在线性测量和高斯噪声的情况下，当过程噪声是状态相关的 $g\\,x_k\\,w_k$ 时，分离原理和确定性等价通常不成立：最优控制律不是应用于状态估计的确定性LQR。**\n\n此陈述讨论了部分观测（输出反馈）的情况。标准的分离原理适用于线性的-二次-高斯（LQG）系统，其中过程噪声和测量噪声都是加性的。在给定问题中，过程噪声是乘性的（状态相关）。在随机控制理论中，一个公认的结论是，当存在乘性噪声时，估计与控制的分离（即分离原理）以及确定性等价不成立。最优控制律不仅取决于状态估计，还取决于该估计的不确定性（例如，估计误差的协方差）。控制必须考虑到其行为如何影响未来的不确定性，这是确定性等价控制器所不具备的特性。因此，最优控制器不仅仅是将确定性LQR增益应用于状态估计。\n\n**结论：正确**\n\n**D. 在线性反馈 $u_k = -K x_k$ 下，均方稳定性要求 $((a - bK)^2 + g^2 \\sigma^2) < 1$；因此，一个能稳定相应确定性闭环的反馈在 $\\sigma^2$ 很大时可能是均方不稳定的。**\n\n给定控制律 $u_k = -K\\,x_k$，闭环系统动力学为：\n$$x_{k+1} = a\\,x_k + b(-K\\,x_k) + g\\,x_k\\,w_k = ((a - bK) + g\\,w_k)\\,x_k$$\n均方稳定性要求 $\\lim_{k \\to \\infty} \\mathbb{E}[x_k^2] = 0$。让我们考察二阶矩的演化：\n$$\\mathbb{E}[x_{k+1}^2] = \\mathbb{E}[(((a - bK) + g\\,w_k)\\,x_k)^2] = \\mathbb{E}[((a - bK) + g\\,w_k)^2] \\mathbb{E}[x_k^2]$$\n最后一步是成立的，因为 $w_k$ 与 $x_k$ 独立。平方项的期望为：\n$$\\mathbb{E}[((a - bK) + g\\,w_k)^2] = \\mathbb{E}[(a - bK)^2 + 2g(a-bK)w_k + g^2 w_k^2] = (a - bK)^2 + g^2\\sigma^2$$\n因此，我们得到递推关系 $\\mathbb{E}[x_{k+1}^2] = ((a - bK)^2 + g^2\\sigma^2) \\mathbb{E}[x_k^2]$。为了使二阶矩收敛到零，系数必须小于 $1$。由于系数是非负的，均方稳定性的条件是：\n$$(a - bK)^2 + g^2\\sigma^2  1$$\n这证实了陈述的第一部分。对于第二部分，一个反馈增益 $K$ 如果满足 $|a - bK|  1$ 或 $(a-bK)^2  1$，就能稳定确定性系统 $x_{k+1} = (a - bK)x_k$。很明显，我们可以选择一个 $K$ 使得 $(a-bK)^2  1$ 成立（例如，$(a-bK)^2 = 1-\\epsilon$ 对于某个小的 $\\epsilon  0$），但对于足够大的 $\\sigma^2$，条件 $(a-bK)^2 + g^2\\sigma^2  1$ 将被违反。例如，如果选择的 $K$ 使得 $a-bK = 0.9$，确定性系统是稳定的。然而，均方稳定性要求 $0.81 + g^2\\sigma^2  1$，如果 $g^2\\sigma^2 \\ge 0.19$，该条件将不成立。\n\n**结论：正确**\n\n**E. 如果 $g = 0$，最优反馈增益仍然依赖于 $\\sigma^2$，因为噪声通过其方差进入未来成本（cost-to-go）。**\n\n如果我们在原问题陈述中设 $g=0$，系统动力学变为确定性的：\n$$x_{k+1} = a\\,x_k + b\\,u_k$$\n噪声项 $g\\,x_k\\,w_k$ 恒等于零，因此随机变量 $w_k$ 及其统计量（包括 $\\sigma^2$）不再影响系统的演化。目标函数 $J = \\mathbb{E}\\Big[\\sum_{k=0}^{\\infty} (q\\,x_k^2 + r\\,u_k^2)\\Big]$ 变成了一个确定性的和，因为期望算子现在是多余的。问题简化为标准的确定性LQR问题。该问题的最优反馈增益由标准的DARE方程（$p = q + p\\,a^2 - (p^2\\,a^2\\,b^2)/(r + p\\,b^2)$）求出，它只依赖于 $a, b, q,$ 和 $r$。它不依赖于 $\\sigma^2$。该陈述的前提是错误的。\n\n**结论：错误**", "answer": "$$\\boxed{BCD}$$", "id": "3121165"}, {"introduction": "线性二次调节器（LQR）虽然强大，但其应用局限于线性和二次成本的框架内。当成本函数非凸时，例如在需要从几个离散操作模式中选择时，我们通常无法得到简单的解析解。本练习将指导你实现一个基于动态规划的数值算法，通过对状态空间进行离散化来求解一个带有非凸控制成本的最优控制问题。这个实践将帮助你理解动态规划如何系统性地搜索并找到全局最优解，避免陷入目光短浅的“贪心”策略所导致的局部最优陷阱中，从而真正将理论付诸实践。[@problem_id:3121214]", "problem": "考虑一个确定性的、有限时域的、离散时间最优控制问题，其状态和控制均为标量。系统根据动力学方程 $x_{k+1} = x_k + u_k$ 演化，其中时间索引 $k = 0, 1, \\dots, N-1$ 为整数，初始状态 $x_0 \\in \\mathbb{R}$ 已给定，控制 $u_k$ 被限制在一个固定的、有限的容许值网格 $\\mathcal{U} \\subset \\mathbb{R}$ 中。阶段成本为 $c(x_k,u_k) = q \\, x_k^2 + \\ell(u_k)$，其中非凸的控制成本为\n$$\n\\ell(u) = \\min\\{(u - a)^2,\\,(u + b)^2\\},\n$$\n参数 $a  0$ 和 $b  0$。终端成本为 $\\phi(x_N) = p \\, x_N^2$，其中 $p  0$。需要最小化的总成本是时域内所有阶段成本与终端成本之和。\n\n仅从该控制问题的定义和动态规划的最优性原理出发，推导如何计算最优的未来成本（cost-to-go）和策略。实现一个完整的算法，该算法能够：\n- 在状态空间上构建一个均匀网格 $\\mathcal{X}$，该网格足以包含从 $x_0$ 出发，在时域 $N$ 内，通过控制集 $\\mathcal{U}$ 可达的所有状态。\n- 通过后向动态规划计算状态网格上的最优策略，并从 $x_0$ 开始前向应用该策略，以生成实现的最优控制序列 $(u_0^{\\star},\\dots,u_{N-1}^{\\star})$ 和状态轨迹 $(x_0,\\dots,x_N)$。\n- 沿此轨迹计算实现的最优总成本 $J^{\\star}$。\n- 计算一个贪婪短视策略，该策略在每个时间步选择仅最小化 $\\ell(u_k)$ 的控制 $u_k$（平局必须通过选择绝对值最小的控制来打破，如果仍然平局，则选择数值最小的控制），并从 $x_0$ 开始前向应用该策略以产生贪婪总成本 $J^{\\mathrm{greedy}}$。\n- 通过计算所选 $\\ell(u)$ 的活动分支在实现的最优控制序列中变化的次数来识别策略切换。对于一个控制值 $u$，如果 $(u-a)^2 \\le (u+b)^2$，则将活动分支索引定义为 $0$，否则定义为 $1$。切换事件的数量是分支索引从 $k-1$ 变为 $k$ 的索引 $k$ 的计数。\n- 通过以下规则检测局部最小值陷阱指标：当且仅当贪婪策略在所有时间步上都使用单一分支（无切换）、动态规划策略至少有一次切换，并且 $J^{\\mathrm{greedy}}  J^{\\star}$ 时，返回一个布尔值 `true`。\n\n您的实现必须通过将 $x_{k+1}$ 映射到最近的状态网格点来处理网格上的状态演化。如果 $x_{k+1}$ 超出网格范围，则必须将其裁剪到最近的边界网格点。您必须为动态规划定义一个平局打破规则：当多个控制达到相同的最小值时，选择绝对值最小的控制，如果仍然平局，则选择数值最小的控制。\n\n测试套件。对于以下每个参数集，运行算法并收集该测试用例的结果四元组：$[J^{\\star},\\,S^{\\star},\\,\\text{is\\_greedy\\_suboptimal},\\,\\text{is\\_trap}]$，其中 $J^{\\star}$ 是实现的最优总成本（浮点数），$S^{\\star}$ 是沿实现的最优策略的分支切换次数（整数），$\\text{is\\_greedy\\_suboptimal}$ 是一个布尔值，指示 $J^{\\mathrm{greedy}}  J^{\\star}$ 是否成立，$\\text{is\\_trap}$ 是上面定义的布尔局部最小值陷阱指标。使用状态网格间距 $\\Delta x = 0.25$。\n\n- 情况1（一般情况）：$N = 6$, $q = 1.0$, $p = 5.0$, $a = 1.0$, $b = 1.0$, $x_0 = 2.5$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$。\n- 情况2（边界时域）：$N = 1$, $q = 1.0$, $p = 2.0$, $a = 1.0$, $b = 1.0$, $x_0 = -0.75$, $\\mathcal{U} = \\{-2.0,-1.0,-0.5,0.0,0.5,1.0,2.0\\}$。\n- 情况3（非对称井）：$N = 5$, $q = 0.5$, $p = 10.0$, $a = 2.0$, $b = 0.5$, $x_0 = -3.0$, $\\mathcal{U} = \\{-3.0,-2.5,-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0,2.5,3.0\\}$。\n- 情况4（强终端惩罚）：$N = 8$, $q = 0.1$, $p = 20.0$, $a = 1.5$, $b = 1.5$, $x_0 = 3.0$, $\\mathcal{U} = \\{-2.0,-1.5,-1.0,-0.5,0.0,0.5,1.0,1.5,2.0\\}$。\n\n最终输出格式。您的程序应生成单行输出，其中包含所有测试用例的结果，聚合为一个由方括号括起来的逗号分隔列表，其中每个元素本身是该测试用例的四元组，也用方括号括起来并以逗号分隔。例如：$[ [J_1^{\\star},S_1^{\\star},\\text{flag}_{1},\\text{flag}_{1}], [J_2^{\\star},S_2^{\\star},\\text{flag}_{2},\\text{flag}_{2}], \\dots ]$。不涉及物理单位或角度单位。所有布尔值必须打印为大写的 Python 布尔值（$\\text{True}$ 或 $\\text{False}$）。所有浮点数和整数必须以标准的 Python 表示法打印。", "solution": "用户提供的问题是一个有效的确定性、有限时域、离散时间最优控制问题。该问题是适定的，科学上基于最优控制理论的原理，并且所有参数、约束和目标都已明确定义。核心挑战在于控制成本函数 $\\ell(u)$ 的非凸性质，这使得动态规划 (DP) 成为一种合适的求解方法。解决方案将从最优性原理推导得出。\n\n**1. 问题公式化与最优性原理**\n\n目标是最小化由下式给出的总成本函数 $J$：\n$$J(x_0, u_0, \\dots, u_{N-1}) = \\sum_{k=0}^{N-1} c(x_k, u_k) + \\phi(x_N)$$\n其中阶段成本为 $c(x_k, u_k) = qx_k^2 + \\ell(u_k)$，$\\ell(u) = \\min\\{(u - a)^2, (u + b)^2\\}$，终端成本为 $\\phi(x_N) = px_N^2$。系统动力学为 $x_{k+1} = x_k + u_k$，具有给定的初始状态 $x_0$。控制 $u_k$ 必须从一个有限的容许控制集 $\\mathcal{U}$ 中选择。\n\n根据 Bellman 的最优性原理，一个最优策略具有这样的性质：无论初始状态和初始决策是什么，余下的决策对于由第一个决策所产生的状态而言，必须构成一个最优策略。该原理引出了动态规划的递推关系。\n\n设 $J_k^{\\star}(x)$ 为从时间 $k$ 的状态 $x$ 到时域结束的最优未来成本（cost-to-go）。递推从终端时间 $N$ 开始：\n$$J_N^{\\star}(x) = \\phi(x) = p x^2$$\n对于之前的时间步 $k = N-1, N-2, \\dots, 0$，Bellman 方程将时间 $k$ 的未来成本与时间 $k+1$ 的未来成本关联起来：\n$$J_k^{\\star}(x) = \\min_{u \\in \\mathcal{U}} \\left\\{ c(x, u) + J_{k+1}^{\\star}(x + u) \\right\\}$$\n最小值内的项表示在时间 $k$、状态 $x$ 时选择控制 $u$ 所产生的成本，以及此后按最优方式进行的成本。\n\n**2. 通过离散化的数值解法**\n\n由于状态 $x$ 是连续的，我们无法直接将 $J_k^{\\star}(x)$ 制成表格。该问题指定了一种使用状态空间离散化的数值方法。\n\n首先，我们构建一个均匀的状态网格 $\\mathcal{X}$，保证能包含所有可能的轨迹。最小和最大可达状态分别为 $x_{\\min} = x_0 + N \\cdot \\min(\\mathcal{U})$ 和 $x_{\\max} = x_0 + N \\cdot \\max(\\mathcal{U})$。网格 $\\mathcal{X}$ 在区间 $[x_{\\min}, x_{\\max}]$ 上构建，具有指定的间距 $\\Delta x$。设网格点为 $\\{x_i\\}$。\n\n然后，DP 算法在该网格上随时间后向进行：\n\n**第1步：后向传递（策略和价值函数计算）**\n- **初始化 ($k=N$):** 对于每个状态 $x_i \\in \\mathcal{X}$，计算并存储终端成本：$J_N(x_i) = p x_i^2$。\n- **后向迭代 ($k=N-1, \\dots, 0$):** 对于每个状态 $x_i \\in \\mathcal{X}$：\n  - 对于每个容许控制 $u_j \\in \\mathcal{U}$，计算该特定选择的未来成本：\n    1. 计算下一个状态：$x_{\\text{next}} = x_i + u_j$。\n    2. 将此下一个状态映射到网格。根据问题要求，这包括将 $x_{\\text{next}}$ 裁剪到网格边界，然后找到最近的网格点，我们称之为 $x'_{\\text{next}}$。\n    3. 从这个下一个网格状态检索已经计算出的最优未来成本：$J_{k+1}(x'_{\\text{next}})$。\n    4. 此 $(x_i, u_j)$ 对的总成本为 $V(u_j) = q x_i^2 + \\ell(u_j) + J_{k+1}(x'_{\\text{next}})$。\n  - 找到所有控制中的最小成本：$J_k(x_i) = \\min_{u_j \\in \\mathcal{U}} V(u_j)$。\n  - 识别达到此最小值的控制集合。应用指定的平局打破规则（最小绝对值，然后是最小数值）来选择唯一的最优控制 $\\mu_k^{\\star}(x_i)$。\n  - 存储最优未来成本 $J_k(x_i)$ 和最优控制 $\\mu_k^{\\star}(x_i)$。\n\n在此传递之后，我们得到了对所有网格状态 $x_i$ 和时间步 $k$ 制成表格的最优策略 $\\mu_k^{\\star}(x_i)$ 和最优价值函数 $J_k(x_i)$。\n\n**第2步：前向传递（轨迹实现）**\n在计算出最优策略 $\\mu_k^{\\star}$ 后，我们从初始状态 $x_0$ 开始前向模拟系统，以找到实现的最优轨迹并计算相关成本和指标。\n\n- 用 $x_0^{\\star} = x_0$ 初始化状态轨迹。\n- 对于 $k=0, \\dots, N-1$：\n  1. 取当前真实状态 $x_k^{\\star}$。\n  2. 将 $x_k^{\\star}$ 映射到其最近的网格点，$x'_k = \\text{nearest}(x_k^{\\star}, \\mathcal{X})$。\n  3. 从策略表中查找最优控制：$u_k^{\\star} = \\mu_k^{\\star}(x'_k)$。\n  4. 演化真实状态：$x_{k+1}^{\\star} = x_k^{\\star} + u_k^{\\star}$。\n  5. 存储 $u_k^{\\star}$ 和 $x_{k+1}^{\\star}$。\n\n**3. 所需指标的计算**\n\n- **最优成本 ($J^{\\star}$):** 使用实现的最优状态轨迹 $(x_0^{\\star}, \\dots, x_N^{\\star})$ 和控制序列 $(u_0^{\\star}, \\dots, u_{N-1}^{\\star})$，总成本计算如下：\n  $$J^{\\star} = \\sum_{k=0}^{N-1} (q(x_k^{\\star})^2 + \\ell(u_k^{\\star})) + p(x_N^{\\star})^2$$\n\n- **贪婪策略成本 ($J^{\\mathrm{greedy}}$):** 短视贪婪策略是在每一步都最小化即时控制成本 $\\ell(u)$，而忽略状态成本和未来后果。贪婪控制 $u^{\\mathrm{greedy}}$ 通过 $\\arg\\min_{u \\in \\mathcal{U}} \\ell(u)$ 找到，并按规定打破平局。由于 $\\ell(u)$ 与状态和时间无关，此控制对所有 $k$ 都是恒定的。从 $x_0$ 开始模拟贪婪轨迹，并类似地计算 $J^{\\mathrm{greedy}}$。\n\n- **策略切换 ($S^{\\star}$):** 控制成本 $\\ell(u) = \\min\\{(u-a)^2, (u+b)^2\\}$ 有两个分支。一个控制 $u$ 的活动分支由 $(u-a)^2 \\le (u+b)^2$ 是否成立决定。这可以简化为检查是否 $u \\ge (a-b)/2$。我们定义一个分支索引函数：\n  $$B(u) = \\begin{cases} 0  \\text{if } u \\ge (a-b)/2 \\\\ 1  \\text{if } u  (a-b)/2 \\end{cases}$$\n  我们将此函数应用于最优控制序列 $u_0^{\\star}, \\dots, u_{N-1}^{\\star}$ 以获得一系列分支索引。切换次数 $S^{\\star}$ 是连续时间步之间分支索引发生变化的次数。\n\n- **陷阱指标：** 布尔标志 `is_greedy_suboptimal` ($J^{\\mathrm{greedy}}  J^{\\star}$) 和 `is_trap`（贪婪策略无切换，DP 策略至少有一次切换，且贪婪策略次优）是基于这些结果计算的。“陷阱”表示这样一种情况：简单的贪婪方法陷入了局部有吸引力的控制模式，而最优的 DP 策略则正确地进行前瞻规划并切换模式以实现更低的总成本。\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the algorithm for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {'N': 6, 'q': 1.0, 'p': 5.0, 'a': 1.0, 'b': 1.0, 'x0': 2.5, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n        {'N': 1, 'q': 1.0, 'p': 2.0, 'a': 1.0, 'b': 1.0, 'x0': -0.75, 'U': [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]},\n        {'N': 5, 'q': 0.5, 'p': 10.0, 'a': 2.0, 'b': 0.5, 'x0': -3.0, 'U': [-3.0, -2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]},\n        {'N': 8, 'q': 0.1, 'p': 20.0, 'a': 1.5, 'b': 1.5, 'x0': 3.0, 'U': [-2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0]},\n    ]\n    dx = 0.25\n\n    results = []\n    for params in test_cases:\n        result = _solve_case(_dx=dx, **params)\n        results.append(result)\n    \n    # Format the final output string\n    s_results = [f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]\n    print(f\"[{','.join(s_results)}]\")\n\ndef _solve_case(N, q, p, a, b, x0, U, _dx):\n    \"\"\"\n    Solves a single optimal control problem instance.\n    \"\"\"\n\n    # --- Helper Functions ---\n    def l_cost(u, a_param, b_param):\n        return min((u - a_param)**2, (u + b_param)**2)\n\n    def branch_index(u, a_param, b_param):\n        # branch 0 if (u-a)^2 = (u+b)^2, else 1. This simplifies to u >= (a-b)/2.\n        return 0 if u >= (a_param - b_param) / 2.0 else 1\n\n    def solve_tiebreak(controls):\n        # Sort by |u|, then by u\n        return sorted(controls, key=lambda u_val: (abs(u_val), u_val))[0]\n\n    # --- 1. State Grid Construction ---\n    u_min_val, u_max_val = min(U), max(U)\n    x_min_reach = x0 + N * u_min_val\n    x_max_reach = x0 + N * u_max_val\n    # Use arange; add a small tolerance to include the endpoint\n    X = np.arange(x_min_reach, x_max_reach + _dx / 2.0, _dx)\n    num_states = len(X)\n    x_grid_min_val, x_grid_max_val = X[0], X[-1]\n    \n    def map_to_index(x):\n        clipped_x = np.clip(x, x_grid_min_val, x_grid_max_val)\n        return np.argmin(np.abs(X - clipped_x))\n\n    # --- 2. Dynamic Programming Backward Pass ---\n    J = np.zeros((N + 1, num_states))\n    policy = np.zeros((N, num_states))\n\n    # Terminal cost\n    J[N, :] = p * X**2\n\n    # Iterate backwards from k = N-1 to 0\n    for k in range(N - 1, -1, -1):\n        for i in range(num_states):\n            x = X[i]\n            stage_cost_x = q * x**2\n            \n            costs_for_u = []\n            for u in U:\n                control_cost_l = l_cost(u, a, b)\n                x_next_raw = x + u\n                next_idx = map_to_index(x_next_raw)\n                cost_to_go_next = J[k + 1, next_idx]\n                total_cost_for_u = stage_cost_x + control_cost_l + cost_to_go_next\n                costs_for_u.append(total_cost_for_u)\n                \n            min_cost = min(costs_for_u)\n            J[k, i] = min_cost\n            \n            candidate_us = [u for u, cost in zip(U, costs_for_u) if np.isclose(cost, min_cost)]\n            policy[k, i] = solve_tiebreak(candidate_us)\n\n    # --- 3. Forward Pass (Optimal Trajectory) ---\n    x_traj_opt = np.zeros(N + 1)\n    u_traj_opt = np.zeros(N)\n    x_traj_opt[0] = x0\n\n    for k in range(N):\n        current_x = x_traj_opt[k]\n        current_idx = map_to_index(current_x)\n        u_star_k = policy[k, current_idx]\n        u_traj_opt[k] = u_star_k\n        x_traj_opt[k + 1] = current_x + u_star_k\n    \n    # --- 4. Calculate Optimal Cost (J_star) ---\n    J_star = 0.0\n    for k in range(N):\n        J_star += q * x_traj_opt[k]**2 + l_cost(u_traj_opt[k], a, b)\n    J_star += p * x_traj_opt[N]**2\n\n    # --- 5. Calculate Policy Switches (S_star) ---\n    branch_indices_opt = [branch_index(u, a, b) for u in u_traj_opt]\n    S_star = 0\n    if N > 0:\n        for k in range(1, N):\n            if branch_indices_opt[k] != branch_indices_opt[k-1]:\n                S_star += 1\n\n    # --- 6. Greedy Policy Simulation ---\n    l_vals = [l_cost(u, a, b) for u in U]\n    min_l = min(l_vals)\n    candidate_greedy_us = [u for u, l_val in zip(U, l_vals) if np.isclose(l_val, min_l)]\n    u_greedy = solve_tiebreak(candidate_greedy_us)\n\n    x_traj_greedy = np.zeros(N + 1)\n    x_traj_greedy[0] = x0\n    for k in range(N):\n        x_traj_greedy[k + 1] = x_traj_greedy[k] + u_greedy\n    \n    J_greedy = 0.0\n    for k in range(N):\n        J_greedy += q * x_traj_greedy[k]**2 + l_cost(u_greedy, a, b)\n    J_greedy += p * x_traj_greedy[N]**2\n        \n    # --- 7. Final Indicators ---\n    is_greedy_suboptimal = J_greedy > J_star\n    \n    # Greedy policy uses constant control, so it has 0 switches.\n    greedy_switches = 0\n    \n    # A trap occurs if greedy stays on one branch, DP switches, and DP is better.\n    is_trap = (greedy_switches == 0) and (S_star >= 1) and is_greedy_suboptimal\n\n    return [J_star, S_star, is_greedy_suboptimal, is_trap]\n\nif __name__ == '__main__':\n    solve()\n```", "answer": "$$\\boxed{[[41.5,1,True,True],[2.5625,0,False,False],[30.875,2,True,True],[37.1,1,True,True]]}$$", "id": "3121214"}]}