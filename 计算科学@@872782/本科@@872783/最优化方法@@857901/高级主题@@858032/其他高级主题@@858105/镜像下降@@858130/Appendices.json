{"hands_on_practices": [{"introduction": "为了真正理解镜像下降的威力，我们首先需要一个基准来进行比较。投影次梯度法 (Projected Subgradient Method) 是一个直观且广泛使用的方法，它将无约束的梯度下降思想直接扩展到了有约束的优化问题中。其核心思想非常简单：首先像往常一样沿着梯度的反方向迈出一步，如果这一步使我们超出了可行域，那么就将结果点“拉回”到可行域中最近的一点。在这个实践中，我们将为概率单纯形 (probability simplex) 约束下的优化问题实现这一基准算法，重点是掌握将任意点投影到单纯形上的关键技术。", "problem": "你的任务是实现并比较两种用于在概率单纯形上进行约束凸优化的的一阶方法：欧几里得投影次梯度法和使用负熵镜像映射的镜像下降法。目标函数是线性泛函的逐点最大值。比较必须从第一性原理出发，并在固定的测试集上进行评估。\n\n需要使用的基本定义：\n- 对于所有 $x,y \\in \\mathbb{R}^n$ 和 $\\lambda \\in [0,1]$，如果满足 $f(\\lambda x + (1-\\lambda) y) \\le \\lambda f(x) + (1-\\lambda) f(y)$，则函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是凸函数。对于给定的 $c_i \\in \\mathbb{R}^n$，函数 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$ 的凸性源于仿射函数的逐点最大值是凸函数这一事实。\n- 凸函数 $f$ 在点 $x$ 处的次梯度 $g \\in \\mathbb{R}^n$ 满足对于所有的 $y$ 都有 $f(y) \\ge f(x) + g^\\top (y - x)$。对于 $f(x) = \\max_i c_i^\\top x$，任何使得 $k$ 达到最大值 $c_k^\\top x$ 的 $c_k$ 都是一个有效的次梯度。\n- 一个点 $y \\in \\mathbb{R}^n$ 在一个非空闭凸集 $C$ 上的欧几里得投影是唯一的点 $P_C(y)$，它使得在 $x \\in C$ 上的 $\\|x - y\\|_2$ 最小。\n- $\\mathbb{R}^n$ 中的概率单纯形是 $\\Delta_n = \\{x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\text{ for all } i, \\ \\sum_{i=1}^n x_i = 1\\}$。\n- 给定一个严格凸且可微的函数（镜像映射）$\\psi:\\text{int}(\\Delta_n) \\to \\mathbb{R}$，由 $\\psi$ 生成的Bregman散度是 $D_\\psi(x,y) = \\psi(x) - \\psi(y) - \\nabla \\psi(y)^\\top (x-y)$。对于在单纯形内部定义的负熵镜像映射 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$， $D_\\psi$ 在相差一个加性常数的情况下，简化为Kullback–Leibler散度。\n\n任务：\n- 考虑约束最小化问题 $\\min_{x \\in \\Delta_n} f(x)$，其中 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$。\n- 从均匀点 $x^{(0)} = \\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right)$ 开始，实现两种迭代方法：\n  1. 欧几里得投影次梯度法：在第 $t$ 次迭代时，选择 $f$ 在 $x^{(t)}$ 处的一个次梯度 $g^{(t)}$，进行一次欧几里得步骤 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$，然后计算 $x^{(t+1)}$ 作为 $y^{(t+1)}$ 在 $\\Delta_n$ 上的欧几里得投影。\n  2. 使用负熵的镜像下降法：在第 $t$ 次迭代时，选择 $f$ 在 $x^{(t)}$ 处的一个次梯度 $g^{(t)}$，然后计算 $x^{(t+1)}$ 作为在一阶近端子问题中关于Bregman散度 $D_\\psi$ 的 $\\Delta_n$ 上的最小化子，该散度对应于 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$，即 $x^{(t+1)} = \\arg\\min_{x \\in \\Delta_n} \\left\\{ \\alpha_t \\, g^{(t)}{}^\\top x + D_\\psi(x, x^{(t)}) \\right\\}$。\n- 使用递减步长方案 $\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t+1}}$，其中 $\\alpha_0 = 0.5$，对于 $t = 0,1,\\dots,T-1$，并运行 $T = 300$ 次迭代。所有索引 $t$、$n$、$K$、$\\alpha_0$ 和 $T$ 必须严格按照此处的定义处理。\n\n实验设计与评估：\n- 对于每个测试用例，在 $T$ 次迭代后，计算 $f_{\\text{proj}} = \\max_i c_i^\\top x_{\\text{proj}}^{(T)}$ 和 $f_{\\text{mirror}} = \\max_i c_i^\\top x_{\\text{mirror}}^{(T)}$，其中 $x_{\\text{proj}}^{(T)}$ 和 $x_{\\text{mirror}}^{(T)}$ 分别是欧几里得投影次梯度法和镜像下降法的最终迭代结果。然后报告差值 $d = f_{\\text{proj}} - f_{\\text{mirror}}$，作为一个浮点数。一个正的 $d$ 值表示在相同的步长方案和迭代预算下，镜像下降法获得了比欧几里得投影法更低的目标函数值。\n\n测试集：\n- 所有情况下的维度 $n = 6$。每个测试用例由一个矩阵 $C \\in \\mathbb{R}^{K \\times n}$ 指定，其行为 $c_i^\\top$。请严格使用以下四种情况。\n  1. 情况 A（中度混合系数，内部行为）：\n     $$\n     C = \\begin{bmatrix}\n     0.2  & 0.3  & -0.1  & 0.0  & 0.4  & -0.2 \\\\\n     -0.1  & 0.5  & 0.2  & -0.3  & 0.1  & 0.0 \\\\\n     0.3  & -0.2  & 0.1  & 0.2  & -0.1  & 0.4 \\\\\n     0.0  & 0.1  & 0.3  & 0.5  & -0.4  & 0.2\n     \\end{bmatrix}.\n     $$\n  2. 情况 B（尖峰梯度，各向异性几何）：\n     $$\n     C = \\begin{bmatrix}\n     2.0  & -1.0  & 0.5  & 0.0  & -0.5  & 0.0 \\\\\n     -1.5  & 1.0  & -0.5  & 0.2  & 0.1  & 0.0 \\\\\n     1.0  & -0.5  & 1.5  & -1.0  & 0.0  & 0.0\n     \\end{bmatrix}.\n     $$\n  3. 情况 C（几乎共线的行，平坦的地形）：\n     $$\n     C = \\begin{bmatrix}\n     0.5  & 0.5  & 0.5  & 0.5  & 0.5  & 0.5 \\\\\n     0.45  & 0.45  & 0.45  & 0.45  & 0.45  & 0.45 \\\\\n     0.52  & 0.52  & 0.52  & 0.52  & 0.52  & 0.52\n     \\end{bmatrix}.\n     $$\n  4. 情况 D（由于零成本坐标而导致顶点最优）：\n     $$\n     C = \\begin{bmatrix}\n     1.0  & 0.0  & 0.0  & 0.0  & 0.0  & 0.0 \\\\\n     0.0  & 1.0  & 0.0  & 0.0  & 0.0  & 0.0 \\\\\n     0.0  & 0.0  & 1.0  & 0.0  & 0.0  & 0.0\n     \\end{bmatrix}.\n     $$\n\n要求的最终输出格式：\n- 你的程序应生成单行输出，其中包含四个浮点数差值 $d$ 的结果，以逗号分隔列表的形式，按测试用例的顺序排列，并用方括号括起来，每个浮点数四舍五入到六位小数。例如，输出必须具有 `[d_1,d_2,d_3,d_4]` 的形式，其中每个 $d_j$ 都报告到六位小数。\n\n此问题不涉及任何物理单位或角度单位。所有数值必须作为无单位的实数处理。解决方案应从上述基础定义出发；不要在问题陈述中引入超出这些定义的快捷公式。", "solution": "我们从凸优化问题 $\\min_{x \\in \\Delta_n} f(x)$ 开始，其中 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$，并且每个 $c_i \\in \\mathbb{R}^n$ 都是固定的。函数 $f$ 是凸的，因为它是仿射函数的逐点最大值。在点 $x$ 处的次梯度可以从集合 $\\{c_i : i \\in \\arg\\max_j c_j^\\top x\\}$ 中选择，这是最大值凸函数次梯度特性的直接推论。\n\n欧几里得投影次梯度法：\n- 约束次梯度下降法利用的原理是，对于一个凸函数 $f$ 和一个闭凸集 $C$，形式为 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$（其中 $g^{(t)} \\in \\partial f(x^{(t)})$）的迭代步骤，后跟投影 $x^{(t+1)} = P_C(y^{(t+1)})$，可以确保可行性以及相对于欧几里得范数的非扩张性。欧几里得投影 $P_{\\Delta_n}$ 解决 $\\min_{x \\in \\Delta_n} \\|x - y\\|_2$ 问题，并有唯一解。对于单纯形，此解具有结构 $x_i = \\max\\{y_i - \\tau, 0\\}$，其中阈值 $\\tau$ 的选择要使 $\\sum_i x_i = 1$，这可以通过对 $y$ 进行排序并在 $O(n \\log n)$ 时间内选择满足互补松弛条件的索引来找到。\n\n使用负熵的镜像下降法：\n- 镜像下降法源于使用由严格凸镜像映射 $\\psi$ 导出的几何结构的近端最小化。给定在单纯形内部定义的 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$，相关的Bregman散度为\n$$\nD_\\psi(x,y) = \\sum_{i=1}^n x_i \\log \\frac{x_i}{y_i} - \\sum_{i=1}^n (x_i - y_i),\n$$\n这在相差一个加性常数的情况下是Kullback–Leibler散度。第 $t$ 次迭代的镜像下降更新解决\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\Delta_n} \\left\\{ \\alpha_t \\, g^{(t)}{}^\\top x + D_\\psi(x, x^{(t)}) \\right\\},\n$$\n其中 $g^{(t)} \\in \\partial f(x^{(t)})$。对于这个严格凸问题的一阶最优性条件，对于某个强制 $\\sum_i x_i = 1$ 的拉格朗日乘子 $\\lambda$，可得\n$$\n\\alpha_t g^{(t)} + \\nabla \\psi(x^{(t+1)}) - \\nabla \\psi(x^{(t)}) + \\lambda \\mathbf{1} = 0.\n$$\n使用 $\\nabla \\psi(x)_i = \\log x_i + 1$，（在通过归一化消除 $\\lambda$ 后）这简化为乘性权重更新\n$$\nx^{(t+1)}_i \\propto x^{(t)}_i \\exp\\left(-\\alpha_t g^{(t)}_i\\right),\n$$\n比例常数的选择要使 $\\sum_i x^{(t+1)}_i = 1$。此更新保持了正性，并根据次梯度分量几何地调整权重。\n\n为什么镜像下降几何结构在单纯形上可以优于欧几里得投影：\n- 单纯形 $\\Delta_n$ 是一个概率几何，其中正性和归一化是其内在约束。负熵镜像映射导出的Kullback–Leibler散度天然地与概率的乘性变化相契合。这种几何结构产生的更新严格保持在单纯形内部，避免了突兀的截断为零，并以乘法方式调节坐标。相比之下，欧几里得投影可能导致较大的坐标被投影的阈值效应过分截断，尤其是在步长暂时过大时。这可能导致边界附近的振荡行为，频繁改变激活约束，以及由于无法在没有特定结构提示的情况下恢复的零值而导致信息丢失。\n- 对于像 $f(x) = \\max_i c_i^\\top x$ 这样可能具有尖峰次梯度的函数，镜像下降可以通过指数化和归一化来减弱大分量的影响，从而平滑地适应各向异性。欧几里得投影是加法操作；当梯度高度不均匀时，加法步骤后跟裁剪可能会引入“之”字形轨迹，从而减慢目标值的进展。\n\n程序中使用的算法设计细节：\n- 将 $x^{(0)}$ 初始化为均匀点 $x^{(0)} = \\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right)$。\n- 在两种方法的每次迭代 $t$ 中，选择一个次梯度 $g^{(t)} = c_k$，其中 $k \\in \\arg\\max_i c_i^\\top x^{(t)}$。为了确定性，选择 argmax 集合中最小的索引 $k$。\n- 使用共同的递减步长 $\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t+1}}$，其中 $\\alpha_0 = 0.5$，对于 $t = 0, 1, \\dots, T-1$，并设置 $T = 300$ 次迭代。\n- 投影次梯度法：计算 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$，然后使用单纯形上的欧几里得投影将其投影到 $\\Delta_n$ 上，这可以通过对 $y$ 进行排序、计算阈值 $\\tau$ 并应用 $x^{(t+1)}_i = \\max\\{y^{(t+1)}_i - \\tau, 0\\}$ 以使 $\\sum_i x^{(t+1)}_i = 1$ 来实现。\n- 镜像下降法：计算乘性权重更新 $x^{(t+1)}_i \\propto x^{(t)}_i \\exp\\left(-\\alpha_t g^{(t)}_i\\right)$ 并重新归一化以强制 $\\sum_i x^{(t+1)}_i = 1$。\n- 在 $T$ 次迭代后，计算 $f_{\\text{proj}} = \\max_i c_i^\\top x_{\\text{proj}}^{(T)}$ 和 $f_{\\text{mirror}} = \\max_i c_i^\\top x_{\\text{mirror}}^{(T)}$，然后报告 $d = f_{\\text{proj}} - f_{\\text{mirror}}$。\n\n测试集基本原理：\n- 情况 A 具有混合的正负系数，促进了具有竞争性线性泛函的内部行为；镜像下降的几何结构预计会产生平滑的进展。\n- 情况 B 具有尖峰梯度和各向异性，使得加法步骤加投影容易发生裁剪；镜像下降通常能更好地处理这种各向异性。\n- 情况 C 的行几乎共线，产生一个 $f$ 在 $x \\in \\Delta_n$ 上几乎恒定的地形（事实上，在常数中取最大值时是完全恒定的）；两种方法应该得到相同的目标值，导致 $d$ 接近于零。\n- 情况 D 有三个线性泛函仅影响前三个坐标。其余坐标的成本为零，因此 $f$ 的最小值是通过将所有质量集中在具有零系数的坐标上（这些坐标中的一个顶点）来实现的。欧几里得投影可能通过激进的截断更快地接近这些顶点，而镜像下降由于其内部几何结构，移动可能更谨慎，可能导致 $d  0$。\n\n该程序完全按照推导实现这些算法，并以要求的单行格式打印指定测试集的四个差值 $d$，四舍五入到六位小数。", "answer": "```python\nimport numpy as np\n\ndef project_onto_simplex(v):\n    \"\"\"\n    Euclidean projection onto the probability simplex:\n    Solve min_x ||x - v||_2 subject to x = 0, sum x = 1.\n    \"\"\"\n    n = v.size\n    # Sort v in descending order\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # Find rho: the largest j such that u_j - (cssv_j - 1)/j  0\n    j = np.arange(1, n + 1)\n    cond = u - (cssv - 1) / j  0\n    if not np.any(cond):\n        # In degenerate case, distribute uniformly\n        theta = (cssv[-1] - 1) / n\n    else:\n        rho = np.nonzero(cond)[0][-1]\n        theta = (cssv[rho] - 1) / (rho + 1)\n    w = v - theta\n    w = np.maximum(w, 0.0)\n    # Numerical guard to enforce sum to 1 exactly due to possible rounding\n    s = w.sum()\n    if s == 0:\n        # fallback uniform\n        w = np.ones(n) / n\n    else:\n        w /= s\n    return w\n\ndef subgradient_max_linear(C, x):\n    \"\"\"\n    Subgradient of f(x) = max_i c_i^T x at x: choose c_k with k in argmax.\n    Deterministic tie-breaking: smallest index k.\n    \"\"\"\n    values = C @ x\n    k = int(np.argmax(values))\n    return C[k]\n\ndef projected_subgradient(C, x0, T, alpha0):\n    x = x0.copy()\n    for t in range(T):\n        alpha_t = alpha0 / np.sqrt(t + 1)\n        g = subgradient_max_linear(C, x)\n        y = x - alpha_t * g\n        x = project_onto_simplex(y)\n    return x\n\ndef mirror_descent_entropy(C, x0, T, alpha0):\n    x = x0.copy()\n    for t in range(T):\n        alpha_t = alpha0 / np.sqrt(t + 1)\n        g = subgradient_max_linear(C, x)\n        # Multiplicative weights update\n        # Prevent overflow/underflow via clipping exponent argument\n        # However, typical magnitudes here are small; proceed directly.\n        update = x * np.exp(-alpha_t * g)\n        s = update.sum()\n        if s == 0 or not np.isfinite(s):\n            # Fallback to uniform if numerical issues\n            x = np.ones_like(x) / x.size\n        else:\n            x = update / s\n    return x\n\ndef f_max_linear(C, x):\n    return float(np.max(C @ x))\n\ndef run_case(C, T=300, alpha0=0.5):\n    n = C.shape[1]\n    x0 = np.ones(n) / n\n    x_proj = projected_subgradient(C, x0, T, alpha0)\n    x_mirror = mirror_descent_entropy(C, x0, T, alpha0)\n    f_proj = f_max_linear(C, x_proj)\n    f_mirror = f_max_linear(C, x_mirror)\n    return f_proj - f_mirror\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([\n            [0.2, 0.3, -0.1, 0.0, 0.4, -0.2],\n            [-0.1, 0.5, 0.2, -0.3, 0.1, 0.0],\n            [0.3, -0.2, 0.1, 0.2, -0.1, 0.4],\n            [0.0, 0.1, 0.3, 0.5, -0.4, 0.2]\n        ], dtype=float),\n        np.array([\n            [2.0, -1.0, 0.5, 0.0, -0.5, 0.0],\n            [-1.5, 1.0, -0.5, 0.2, 0.1, 0.0],\n            [1.0, -0.5, 1.5, -1.0, 0.0, 0.0]\n        ], dtype=float),\n        np.array([\n            [0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n            [0.45, 0.45, 0.45, 0.45, 0.45, 0.45],\n            [0.52, 0.52, 0.52, 0.52, 0.52, 0.52]\n        ], dtype=float),\n        np.array([\n            [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n        ], dtype=float),\n    ]\n\n    T = 300\n    alpha0 = 0.5\n\n    results = []\n    for C in test_cases:\n        d = run_case(C, T=T, alpha0=alpha0)\n        results.append(d)\n\n    # Final print statement in the exact required format: 6 decimals.\n    formatted = \"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3165049"}, {"introduction": "现在，让我们进入主角——镜像下降 (Mirror Descent)。与使用标准欧氏距离来衡量“距离”的投影次梯度法不同，镜像下降采用了一种更巧妙的 Bregman 散度 (Bregman divergence)。通过为概率单纯形选择一个量身定制的“镜像映射” (mirror map)——负熵函数，我们得到的散度恰好是著名的 KL 散度 (Kullback-Leibler divergence)。这个看似复杂的理论转变，最终会导出一个极其简洁优美的乘法更新规则，它能自然地保持解在单纯形内部，这也是本次实践的核心。", "problem": "要求您实现并比较两种用于在概率单纯形上进行带约束凸优化的一阶方法：欧几里得投影次梯度法和使用负熵镜像映射的镜像下降法。目标函数是线性泛函的逐点最大值。比较必须从第一性原理出发，并在一个固定的测试集上进行评估。\n\n使用的基本定义：\n- 如果对于所有 $x,y \\in \\mathbb{R}^n$ 和 $\\lambda \\in [0,1]$，都有 $f(\\lambda x + (1-\\lambda) y) \\le \\lambda f(x) + (1-\\lambda) f(y)$，则函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是凸函数。对于给定的 $c_i \\in \\mathbb{R}^n$，函数 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$ 的凸性源于一个事实：仿射函数的逐点最大值是凸函数。\n- 凸函数 $f$ 在点 $x$ 处的一个次梯度 $g \\in \\mathbb{R}^n$ 对所有 $y$ 满足 $f(y) \\ge f(x) + g^\\top (y - x)$。对于 $f(x) = \\max_i c_i^\\top x$，任何使得 $c_k^\\top x$ 达到最大值的 $c_k$ 都是一个有效的次梯度。\n- 一个点 $y \\in \\mathbb{R}^n$ 到一个非空闭凸集 $C$ 上的欧几里得投影，是指在 $x \\in C$ 中使 $\\|x - y\\|_2$ 最小化的唯一一个点 $P_C(y)$。\n- $\\mathbb{R}^n$ 中的概率单纯形是 $\\Delta_n = \\{x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\text{ for all } i, \\ \\sum_{i=1}^n x_i = 1\\}$。\n- 给定一个严格凸且可微的函数（即镜像映射）$\\psi:\\text{int}(\\Delta_n) \\to \\mathbb{R}$，由 $\\psi$ 生成的 Bregman 散度为 $D_\\psi(x,y) = \\psi(x) - \\psi(y) - \\nabla \\psi(y)^\\top (x-y)$。对于在单纯形内部定义的负熵镜像映射 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$， $D_\\psi$ 在一个可加常数的意义上等同于 Kullback–Leibler 散度。\n\n任务：\n- 考虑带约束的最小化问题 $\\min_{x \\in \\Delta_n} f(x)$，其中 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$。\n- 从均匀点 $x^{(0)} = \\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right)$ 开始，实现两种迭代方法：\n  1. 欧几里得投影次梯度法：在第 $t$ 次迭代时，选择 $f$ 在 $x^{(t)}$ 处的一个次梯度 $g^{(t)}$，执行一次欧几里得步长 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$，然后将 $y^{(t+1)}$ 在 $\\Delta_n$ 上进行欧几里得投影，得到 $x^{(t+1)}$。\n  2. 使用负熵的镜像下降法：在第 $t$ 次迭代时，选择 $f$ 在 $x^{(t)}$ 处的一个次梯度 $g^{(t)}$，并计算 $x^{(t+1)}$ 作为使用对应于 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$ 的 Bregman 散度 $D_\\psi$ 的一阶近端子问题在 $\\Delta_n$ 上的最小化子，即 $x^{(t+1)} = \\arg\\min_{x \\in \\Delta_n} \\left\\{ \\alpha_t \\, g^{(t)}{}^\\top x + D_\\psi(x, x^{(t)}) \\right\\}$。\n- 使用递减步长方案 $\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t+1}}$，其中 $\\alpha_0 = 0.5$，$t = 0,1,\\dots,T-1$，并运行 $T = 300$ 次迭代。所有索引 $t$、$n$、$K$、$\\alpha_0$ 和 $T$ 必须严格按照此处的定义处理。\n\n实验设计与评估：\n- 对于每个测试用例，在 $T$ 次迭代后，计算 $f_{\\text{proj}} = \\max_i c_i^\\top x_{\\text{proj}}^{(T)}$ 和 $f_{\\text{mirror}} = \\max_i c_i^\\top x_{\\text{mirror}}^{(T)}$，其中 $x_{\\text{proj}}^{(T)}$ 和 $x_{\\text{mirror}}^{(T)}$ 分别是欧几里得投影次梯度法和镜像下降法的最终迭代点。然后以浮点数形式报告差值 $d = f_{\\text{proj}} - f_{\\text{mirror}}$。一个正的 $d$ 值表示，在相同的步长方案和迭代预算下，镜像下降法相对于欧几里得投影法获得了更低的最终目标值。\n\n测试集：\n- 所有用例的维度均为 $n = 6$。每个测试用例由一个矩阵 $C \\in \\mathbb{R}^{K \\times n}$ 指定，其行向量为 $c_i^\\top$。请严格使用以下四个用例。\n  1. 用例 A（系数中度混合，内部行为）：\n     $$\n     C = \\begin{bmatrix}\n     0.2   0.3   -0.1   0.0   0.4   -0.2 \\\\\n     -0.1   0.5   0.2   -0.3   0.1   0.0 \\\\\n     0.3   -0.2   0.1   0.2   -0.1   0.4 \\\\\n     0.0   0.1   0.3   0.5   -0.4   0.2\n     \\end{bmatrix}.\n     $$\n  2. 用例 B（尖峰梯度，各向异性几何）：\n     $$\n     C = \\begin{bmatrix}\n     2.0   -1.0   0.5   0.0   -0.5   0.0 \\\\\n     -1.5   1.0   -0.5   0.2   0.1   0.0 \\\\\n     1.0   -0.5   1.5   -1.0   0.0   0.0\n     \\end{bmatrix}.\n     $$\n  3. 用例 C（行向量近似共线，平坦地貌）：\n     $$\n     C = \\begin{bmatrix}\n     0.5   0.5   0.5   0.5   0.5   0.5 \\\\\n     0.45   0.45   0.45   0.45   0.45   0.45 \\\\\n     0.52   0.52   0.52   0.52   0.52   0.52\n     \\end{bmatrix}.\n     $$\n  4. 用例 D（因零成本坐标而导致顶点最优）：\n     $$\n     C = \\begin{bmatrix}\n     1.0   0.0   0.0   0.0   0.0   0.0 \\\\\n     0.0   1.0   0.0   0.0   0.0   0.0 \\\\\n     0.0   0.0   1.0   0.0   0.0   0.0\n     \\end{bmatrix}.\n     $$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，按测试用例的顺序给出四个浮点数差值 $d$，并用方括号括起来，每个浮点数四舍五入到六位小数。例如，输出格式必须为 $[d_1,d_2,d_3,d_4]$，其中每个 $d_j$ 都报告到六位小数。\n\n本问题不涉及物理单位或角度单位。所有数值必须作为无单位的实数处理。解答应从上述基础定义出发；不要在问题陈述中引入超出这些定义的快捷公式。", "solution": "我们从凸优化问题 $\\min_{x \\in \\Delta_n} f(x)$ 开始，其中 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$，且每个 $c_i \\in \\mathbb{R}^n$ 都是固定的。函数 $f$ 是凸函数，因为它是仿射函数的逐点最大值。在点 $x$ 处的次梯度可以从集合 $\\{c_i : i \\in \\arg\\max_j c_j^\\top x\\}$ 中选取，这是凸函数最大值的次梯度刻画的直接推论。\n\n欧几里得投影次梯度法：\n- 带约束的次梯度下降法利用以下原理：对于一个凸函数 $f$ 和一个闭凸集 $C$，形如 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$（其中 $g^{(t)} \\in \\partial f(x^{(t)})$）的迭代，后接投影 $x^{(t+1)} = P_C(y^{(t+1)})$，可以保证解的可行性以及相对于欧几里得范数的非扩张性。欧几里得投影 $P_{\\Delta_n}$ 求解 $\\min_{x \\in \\Delta_n} \\|x - y\\|_2$，且存在唯一解。对于单纯形，该解具有 $x_i = \\max\\{y_i - \\tau, 0\\}$ 的结构，其中阈值 $\\tau$ 的选择需满足 $\\sum_i x_i = 1$。通过对 $y$ 进行排序并选择一个满足互补松弛条件的索引，可以在 $O(n \\log n)$ 时间内找到该解。\n\n使用负熵的镜像下降法：\n- 镜像下降法源于在由严格凸镜像映射 $\\psi$ 诱导的几何结构下的近端最小化。给定在单纯形内部定义的 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$，其相关的 Bregman 散度是\n$$\nD_\\psi(x,y) = \\sum_{i=1}^n x_i \\log \\frac{x_i}{y_i} - \\sum_{i=1}^n (x_i - y_i),\n$$\n其在一个可加常数的意义上等同于 Kullback–Leibler 散度。第 $t$ 次迭代的镜像下降更新求解\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\Delta_n} \\left\\{ \\alpha_t \\, g^{(t)}{}^\\top x + D_\\psi(x, x^{(t)}) \\right\\},\n$$\n其中 $g^{(t)} \\in \\partial f(x^{(t)})$。对于这个严格凸问题，一阶最优性条件可以得到，对于某个强制 $\\sum_i x_i = 1$ 的拉格朗日乘子 $\\lambda$，有\n$$\n\\alpha_t g^{(t)} + \\nabla \\psi(x^{(t+1)}) - \\nabla \\psi(x^{(t)}) + \\lambda \\mathbf{1} = 0.\n$$\n利用 $\\nabla \\psi(x)_i = \\log x_i + 1$，并在通过归一化消除 $\\lambda$ 后，这可以简化为乘法权重更新\n$$\nx^{(t+1)}_i \\propto x^{(t)}_i \\exp\\left(-\\alpha_t g^{(t)}_i\\right),\n$$\n其中比例常数的选择需满足 $\\sum_i x^{(t+1)}_i = 1$。此更新保持了正性，并根据次梯度的分量以几何方式调整权重。\n\n为什么镜像下降几何在单纯形上可以优于欧几里得投影：\n- 单纯形 $\\Delta_n$ 是一个概率几何，其中正性和归一化是其内在约束。负熵镜像映射诱导出 Kullback–Leibler 散度，它与概率的乘法变化自然契合。这种几何结构产生的更新会严格保持在单纯形内部，避免突然截断为零，并对坐标进行乘法调制。相比之下，欧几里得投影可能会因其投影的阈值效应而导致大坐标被激进地截断，尤其是在步长瞬间过冲时。这可能导致在边界附近的振荡行为、激活约束的频繁变化，以及由于零值在没有特定结构性线索的情况下无法恢复而导致的信息丢失。\n- 对于像 $f(x) = \\max_i c_i^\\top x$ 这样可能具有尖峰次梯度的函数，镜像下降法可以通过指数化和归一化来减弱大分量的影响，从而平滑地适应各向异性。欧几里得投影以加法方式运作；当梯度高度不均匀时，加法步长后跟裁剪可能会引入Z字形轨迹，从而减缓目标值的下降进度。\n\n程序中使用的算法设计细节：\n- 将 $x^{(0)}$ 初始化为均匀点 $x^{(0)} = \\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right)$。\n- 对于两种方法，在每次迭代 $t$ 时，选择一个次梯度 $g^{(t)} = c_k$，其中 $k \\in \\arg\\max_i c_i^\\top x^{(t)}$。为确保确定性，选择 argmax 集合中最小的索引 $k$。\n- 使用共同的递减步长 $\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t+1}}$，其中 $\\alpha_0 = 0.5$，$t = 0, 1, \\dots, T-1$，并设置 $T = 300$ 次迭代。\n- 投影次梯度法：计算 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$，然后使用到单纯形的欧几里得投影将其投影到 $\\Delta_n$ 上。这可以通过对 $y$ 排序、计算阈值 $\\tau$ 并应用 $x^{(t+1)}_i = \\max\\{y^{(t+1)}_i - \\tau, 0\\}$ 以使 $\\sum_i x^{(t+1)}_i = 1$ 来实现。\n- 镜像下降法：计算乘法权重更新 $x^{(t+1)}_i \\propto x^{(t)}_i \\exp\\left(-\\alpha_t g^{(t)}_i\\right)$ 并重新归一化以强制 $\\sum_i x^{(t+1)}_i = 1$。\n- 在 $T$ 次迭代后，计算 $f_{\\text{proj}} = \\max_i c_i^\\top x_{\\text{proj}}^{(T)}$ 和 $f_{\\text{mirror}} = \\max_i c_i^\\top x_{\\text{mirror}}^{(T)}$，然后报告 $d = f_{\\text{proj}} - f_{\\text{mirror}}$。\n\n测试集设计理据：\n- 用例 A 具有混合的正负系数，促进了具有相互竞争的线性泛函的内部行为；镜像下降的几何结构预计会产生平滑的进展。\n- 用例 B 具有尖峰梯度和各向异性，使得加法步长加投影容易发生裁剪；镜像下降法通常能更好地处理这种各向异性。\n- 用例 C 的行向量近似共线，产生了一个 $f$ 在 $x \\in \\Delta_n$ 上几乎是常数的地貌（实际上，当在常数中取最大值时，它就是常数）；两种方法应该得到相同的目标值，导致 $d$ 接近于零。\n- 用例 D 有三个线性泛函仅影响前三个坐标。其余坐标产生零成本，因此 $f$ 的最小值是通过将所有质量集中在零系数的坐标上（这些坐标中的一个顶点）来实现的。欧几里得投影可能通过激进的截断更快地接近这些顶点，而镜像下降由于其内部几何结构，移动可能更为谨慎，可能导致 $d  0$。\n\n该程序严格按照推导实现这些算法，并按要求的单行格式打印指定测试集的四个差值 $d$，四舍五入到六位小数。", "answer": "```python\nimport numpy as np\n\ndef project_onto_simplex(v):\n    \"\"\"\n    Euclidean projection onto the probability simplex:\n    Solve min_x ||x - v||_2 subject to x = 0, sum x = 1.\n    \"\"\"\n    n = v.size\n    # Sort v in descending order\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # Find rho: the largest j such that u_j - (cssv_j - 1)/j  0\n    j = np.arange(1, n + 1)\n    cond = u - (cssv - 1) / j  0\n    if not np.any(cond):\n        # In degenerate case, distribute uniformly\n        theta = (cssv[-1] - 1) / n\n    else:\n        rho = np.nonzero(cond)[0][-1]\n        theta = (cssv[rho] - 1) / (rho + 1)\n    w = v - theta\n    w = np.maximum(w, 0.0)\n    # Numerical guard to enforce sum to 1 exactly due to possible rounding\n    s = w.sum()\n    if s == 0:\n        # fallback uniform\n        w = np.ones(n) / n\n    else:\n        w /= s\n    return w\n\ndef subgradient_max_linear(C, x):\n    \"\"\"\n    Subgradient of f(x) = max_i c_i^T x at x: choose c_k with k in argmax.\n    Deterministic tie-breaking: smallest index k.\n    \"\"\"\n    values = C @ x\n    k = int(np.argmax(values))\n    return C[k]\n\ndef projected_subgradient(C, x0, T, alpha0):\n    x = x0.copy()\n    for t in range(T):\n        alpha_t = alpha0 / np.sqrt(t + 1)\n        g = subgradient_max_linear(C, x)\n        y = x - alpha_t * g\n        x = project_onto_simplex(y)\n    return x\n\ndef mirror_descent_entropy(C, x0, T, alpha0):\n    x = x0.copy()\n    for t in range(T):\n        alpha_t = alpha0 / np.sqrt(t + 1)\n        g = subgradient_max_linear(C, x)\n        # Multiplicative weights update\n        # Prevent overflow/underflow via clipping exponent argument\n        # However, typical magnitudes here are small; proceed directly.\n        update = x * np.exp(-alpha_t * g)\n        s = update.sum()\n        if s == 0 or not np.isfinite(s):\n            # Fallback to uniform if numerical issues\n            x = np.ones_like(x) / x.size\n        else:\n            x = update / s\n    return x\n\ndef f_max_linear(C, x):\n    return float(np.max(C @ x))\n\ndef run_case(C, T=300, alpha0=0.5):\n    n = C.shape[1]\n    x0 = np.ones(n) / n\n    x_proj = projected_subgradient(C, x0, T, alpha0)\n    x_mirror = mirror_descent_entropy(C, x0, T, alpha0)\n    f_proj = f_max_linear(C, x_proj)\n    f_mirror = f_max_linear(C, x_mirror)\n    return f_proj - f_mirror\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([\n            [0.2, 0.3, -0.1, 0.0, 0.4, -0.2],\n            [-0.1, 0.5, 0.2, -0.3, 0.1, 0.0],\n            [0.3, -0.2, 0.1, 0.2, -0.1, 0.4],\n            [0.0, 0.1, 0.3, 0.5, -0.4, 0.2]\n        ], dtype=float),\n        np.array([\n            [2.0, -1.0, 0.5, 0.0, -0.5, 0.0],\n            [-1.5, 1.0, -0.5, 0.2, 0.1, 0.0],\n            [1.0, -0.5, 1.5, -1.0, 0.0, 0.0]\n        ], dtype=float),\n        np.array([\n            [0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n            [0.45, 0.45, 0.45, 0.45, 0.45, 0.45],\n            [0.52, 0.52, 0.52, 0.52, 0.52, 0.52]\n        ], dtype=float),\n        np.array([\n            [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n        ], dtype=float),\n    ]\n\n    T = 300\n    alpha0 = 0.5\n\n    results = []\n    for C in test_cases:\n        d = run_case(C, T=T, alpha0=alpha0)\n        results.append(d)\n\n    # Final print statement in the exact required format: 6 decimals.\n    formatted = \"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3165049"}, {"introduction": "我们已经分别实现了两种算法，现在是时候让它们进行一场正面对决了。在最后的这个实践环节，我们将在几个精心设计的测试案例上运行这两种算法，并对它们的性能进行分析。我们的目标不仅仅是看哪个算法“赢了”，更重要的是通过分析结果来培养一种直觉：究竟在何种情况下，镜像下降这种“几何感知”的更新方式会比传统的欧氏几何方法表现得更出色，以及其背后的原因是什么。", "problem": "要求您实现并比较两种在概率单纯形上解决约束凸优化问题的一阶方法：欧几里得投影次梯度法和使用负熵镜像映射的镜像下降法。目标函数是线性泛函的逐点最大值。此比较必须从第一性原理出发，并在一个固定的测试集上进行评估。\n\n使用的基本定义：\n- 对于所有 $x,y \\in \\mathbb{R}^n$ 和 $\\lambda \\in [0,1]$，如果 $f(\\lambda x + (1-\\lambda) y) \\le \\lambda f(x) + (1-\\lambda) f(y)$，则函数 $f:\\mathbb{R}^n \\to \\mathbb{R}$ 是凸函数。对于给定的 $c_i \\in \\mathbb{R}^n$，函数 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$ 的凸性源于仿射函数的逐点最大值是凸的这一事实。\n- 凸函数 $f$ 在点 $x$ 的一个次梯度 $g \\in \\mathbb{R}^n$ 对所有 $y$ 满足 $f(y) \\ge f(x) + g^\\top (y - x)$。对于 $f(x) = \\max_i c_i^\\top x$，任何使得 $k$ 达到最大值 $c_k^\\top x$ 的 $c_k$ 都是一个有效的次梯度。\n- 一个点 $y \\in \\mathbb{R}^n$ 到一个非空闭凸集 $C$ 上的欧几里得投影是在 $x \\in C$ 上最小化 $\\|x - y\\|_2$ 的唯一一点 $P_C(y)$。\n- $\\mathbb{R}^n$ 中的概率单纯形是 $\\Delta_n = \\{x \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\text{ for all } i, \\ \\sum_{i=1}^n x_i = 1\\}$。\n- 给定一个严格凸且可微的函数（镜像映射）$\\psi:\\text{int}(\\Delta_n) \\to \\mathbb{R}$，由 $\\psi$ 生成的 Bregman 散度为 $D_\\psi(x,y) = \\psi(x) - \\psi(y) - \\nabla \\psi(y)^\\top (x-y)$。对于在单纯形内部定义的负熵镜像映射 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$， $D_\\psi$ 在相差一个加性常数的情况下可简化为 Kullback–Leibler 散度。\n\n任务：\n- 考虑约束最小化问题 $\\min_{x \\in \\Delta_n} f(x)$，其中 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$。\n- 从均匀点 $x^{(0)} = \\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right)$ 开始，实现两种迭代方法：\n  1. 欧几里得投影次梯度法：在第 $t$ 次迭代时，选择 $f$ 在 $x^{(t)}$ 处的一个次梯度 $g^{(t)}$，进行一次欧几里得步进 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$，然后将 $y^{(t+1)}$ 在 $\\Delta_n$ 上进行欧几里得投影，计算出 $x^{(t+1)}$。\n  2. 使用负熵的镜像下降法：在第 $t$ 次迭代时，选择 $f$ 在 $x^{(t)}$ 处的一个次梯度 $g^{(t)}$，然后通过求解对应于 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$ 的 Bregman 散度 $D_\\psi$ 的一阶近端子问题，计算出在 $\\Delta_n$ 上的最小化子 $x^{(t+1)}$，即 $x^{(t+1)} = \\arg\\min_{x \\in \\Delta_n} \\left\\{ \\alpha_t \\, g^{(t)}{}^\\top x + D_\\psi(x, x^{(t)}) \\right\\}$。\n- 使用递减步长策略 $\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t+1}}$，其中 $\\alpha_0 = 0.5$，对于 $t = 0,1,\\dots,T-1$，并运行 $T = 300$ 次迭代。所有索引 $t$、$n$、$K$、$\\alpha_0$ 和 $T$ 必须严格按照此处的定义处理。\n\n实验设计与评估：\n- 对于每个测试案例，在 $T$ 次迭代后，计算 $f_{\\text{proj}} = \\max_i c_i^\\top x_{\\text{proj}}^{(T)}$ 和 $f_{\\text{mirror}} = \\max_i c_i^\\top x_{\\text{mirror}}^{(T)}$，其中 $x_{\\text{proj}}^{(T)}$ 和 $x_{\\text{mirror}}^{(T)}$ 分别是欧几里得投影次梯度法和镜像下降法的最终迭代结果。然后报告差值 $d = f_{\\text{proj}} - f_{\\text{mirror}}$，结果为浮点数。一个正的 $d$ 值表示在相同的步长策略和迭代预算下，镜像下降法相对于欧几里得投影法获得了更低的最终目标函数值。\n\n测试集：\n- 所有案例的维度均为 $n = 6$。每个测试案例由一个矩阵 $C \\in \\mathbb{R}^{K \\times n}$ 指定，其行为 $c_i^\\top$。请严格使用以下四种情况。\n  1. 案例 A（中等混合系数，内部行为）：\n     $$\n     C = \\begin{bmatrix}\n     0.2   0.3   -0.1   0.0   0.4   -0.2 \\\\\n     -0.1   0.5   0.2   -0.3   0.1   0.0 \\\\\n     0.3   -0.2   0.1   0.2   -0.1   0.4 \\\\\n     0.0   0.1   0.3   0.5   -0.4   0.2\n     \\end{bmatrix}.\n     $$\n  2. 案例 B（尖峰梯度，各向异性几何）：\n     $$\n     C = \\begin{bmatrix}\n     2.0   -1.0   0.5   0.0   -0.5   0.0 \\\\\n     -1.5   1.0   -0.5   0.2   0.1   0.0 \\\\\n     1.0   -0.5   1.5   -1.0   0.0   0.0\n     \\end{bmatrix}.\n     $$\n  3. 案例 C（近乎共线的行，平坦景观）：\n     $$\n     C = \\begin{bmatrix}\n     0.5   0.5   0.5   0.5   0.5   0.5 \\\\\n     0.45   0.45   0.45   0.45   0.45   0.45 \\\\\n     0.52   0.52   0.52   0.52   0.52   0.52\n     \\end{bmatrix}.\n     $$\n  4. 案例 D（因零成本坐标而顶点最优）：\n     $$\n     C = \\begin{bmatrix}\n     1.0   0.0   0.0   0.0   0.0   0.0 \\\\\n     0.0   1.0   0.0   0.0   0.0   0.0 \\\\\n     0.0   0.0   1.0   0.0   0.0   0.0\n     \\end{bmatrix}.\n     $$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个逗号分隔的列表，按测试集案例的顺序包含四个浮点数差值 $d$，并用方括号括起来，每个浮点数四舍五入到六位小数。例如，输出必须具有 $[d_1,d_2,d_3,d_4]$ 的形式，其中每个 $d_j$ 报告到六位小数。\n\n本问题不涉及物理单位或角度单位。所有数值必须作为无单位实数处理。解决方案应从上述基础定义出发；不要在问题陈述中引入超出这些定义的快捷公式。", "solution": "我们从凸优化问题 $\\min_{x \\in \\Delta_n} f(x)$ 开始，其中 $f(x) = \\max_{i \\in \\{1,\\dots,K\\}} c_i^\\top x$，并且每个 $c_i \\in \\mathbb{R}^n$ 是固定的。函数 $f$ 是凸的，因为它是仿射函数的逐点最大值。在点 $x$ 处的次梯度可以从集合 $\\{c_i : i \\in \\arg\\max_j c_j^\\top x\\}$ 中选取，这是凸函数最大值的次梯度特性的直接推论。\n\n欧几里得投影次梯度法：\n- 约束次梯度下降法利用了这样一个原理：对于凸函数 $f$ 和闭凸集 $C$，形式为 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$ 的迭代（其中 $g^{(t)} \\in \\partial f(x^{(t)})$），后跟投影 $x^{(t+1)} = P_C(y^{(t+1)})$，可以确保可行性以及关于欧几里得范数的非扩张性。欧几里得投影 $P_{\\Delta_n}$ 求解 $\\min_{x \\in \\Delta_n} \\|x - y\\|_2$，并存在唯一解。对于单纯形，此解具有结构 $x_i = \\max\\{y_i - \\tau, 0\\}$，其中阈值 $\\tau$ 的选择使得 $\\sum_i x_i = 1$，这可以通过对 $y$ 排序并选择满足互补松弛条件的索引，在 $O(n \\log n)$ 时间内找到。\n\n使用负熵的镜像下降法：\n- 镜像下降法源于使用由严格凸镜像映射 $\\psi$ 诱导的几何进行的近端最小化。给定在单纯形内部定义的 $\\psi(x) = \\sum_{i=1}^n x_i \\log x_i$，相关的 Bregman 散度为\n$$\nD_\\psi(x,y) = \\sum_{i=1}^n x_i \\log \\frac{x_i}{y_i} - \\sum_{i=1}^n (x_i - y_i),\n$$\n这在相差一个加性常数的情况下是 Kullback–Leibler 散度。第 $t$ 次迭代的镜像下降更新求解\n$$\nx^{(t+1)} = \\arg\\min_{x \\in \\Delta_n} \\left\\{ \\alpha_t \\, g^{(t)}{}^\\top x + D_\\psi(x, x^{(t)}) \\right\\},\n$$\n其中 $g^{(t)} \\in \\partial f(x^{(t)})$。这个严格凸问题的一阶最优性条件，对于某个强制 $\\sum_i x_i = 1$ 的拉格朗日乘子 $\\lambda$，得出\n$$\n\\alpha_t g^{(t)} + \\nabla \\psi(x^{(t+1)}) - \\nabla \\psi(x^{(t)}) + \\lambda \\mathbf{1} = 0.\n$$\n使用 $\\nabla \\psi(x)_i = \\log x_i + 1$，这（在通过归一化消除 $\\lambda$ 后）简化为乘法权重更新\n$$\nx^{(t+1)}_i \\propto x^{(t)}_i \\exp\\left(-\\alpha_t g^{(t)}_i\\right),\n$$\n比例常数的选择使得 $\\sum_i x^{(t+1)}_i = 1$。此更新保持了正性，并根据次梯度分量几何地调整权重。\n\n为什么镜像下降几何在单纯形上可以优于欧几里得投影：\n- 单纯形 $\\Delta_n$ 是一种概率几何，其中正性和归一化是内在约束。负熵镜像映射诱导出 Kullback–Leibler 散度，它自然地与概率的乘法变化保持一致。这种几何产生的更新严格保持在单纯形内部，避免了突然截断为零，并以乘法方式调节坐标。相比之下，欧几里得投影可能导致大的坐标被投影的阈值效应积极地截断，尤其是在步长暂时过大时。这可能导致在边界附近出现振荡行为，频繁改变激活约束，以及由于无法在没有特定结构性线索的情况下恢复的零而导致信息丢失。\n- 对于像 $f(x) = \\max_i c_i^\\top x$ 这样可能具有尖峰次梯度的函数，镜像下降可以通过指数化和归一化来抑制大分量的影响，从而平滑地适应各向异性。欧几里得投影是加法操作；当梯度高度不均匀时，加法步进后跟裁剪可能会引入减慢目标值进展的之字形轨迹。\n\n程序中使用的算法设计细节：\n- 将 $x^{(0)}$ 初始化为均匀点 $x^{(0)} = \\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right)$。\n- 在两种方法的每次迭代 $t$ 中，选择一个次梯度 $g^{(t)} = c_k$，其中 $k \\in \\arg\\max_i c_i^\\top x^{(t)}$。为保证确定性，选择 argmax 集合中最小的索引 $k$。\n- 对 $t = 0, 1, \\dots, T-1$ 使用共同的递减步长 $\\alpha_t = \\frac{\\alpha_0}{\\sqrt{t+1}}$，其中 $\\alpha_0 = 0.5$，并设置 $T = 300$ 次迭代。\n- 投影次梯度法：计算 $y^{(t+1)} = x^{(t)} - \\alpha_t g^{(t)}$，然后使用欧几里得投影到单纯形 $\\Delta_n$ 上，这可以通过对 $y$ 排序、计算阈值 $\\tau$ 并应用 $x^{(t+1)}_i = \\max\\{y^{(t+1)}_i - \\tau, 0\\}$ 以使 $\\sum_i x^{(t+1)}_i = 1$ 来实现。\n- 镜像下降法：计算乘法权重更新 $x^{(t+1)}_i \\propto x^{(t)}_i \\exp\\left(-\\alpha_t g^{(t)}_i\\right)$ 并重新归一化以强制 $\\sum_i x^{(t+1)}_i = 1$。\n- 在 $T$ 次迭代后，计算 $f_{\\text{proj}} = \\max_i c_i^\\top x_{\\text{proj}}^{(T)}$ 和 $f_{\\text{mirror}} = \\max_i c_i^\\top x_{\\text{mirror}}^{(T)}$，然后报告 $d = f_{\\text{proj}} - f_{\\text{mirror}}$。\n\n测试集基本原理：\n- 案例 A 具有混合的正负系数，促进了具有竞争性线性泛函的内部行为；镜像下降的几何预计将产生平滑的进展。\n- 案例 B 具有尖峰梯度和各向异性，使得加法步进加投影容易发生裁剪；镜像下降通常能更好地处理这种各向异性。\n- 案例 C 的行几乎共线，产生一个 $f$ 在 $x \\in \\Delta_n$ 上几乎恒定的景观（实际上在取常数中的最大值时是完全恒定的）；两种方法应该产生相同的目标函数值，导致 $d$ 接近于零。\n- 案例 D 有三个仅影响前三个坐标的线性泛函。其余坐标产生零成本，因此 $f$ 的最小值是通过将所有质量集中在具有零系数的坐标上（这些坐标中的一个顶点）来实现的。欧几里得投影可能通过积极截断更快地接近这样的顶点，而镜像下降由于其内部几何特性，移动可能更谨慎，可能导致 $d  0$。\n\n该程序完全按照推导实现这些算法，并以要求的单行格式打印指定测试集的四个差值 $d$，四舍五入到六位小数。", "answer": "```python\nimport numpy as np\n\ndef project_onto_simplex(v):\n    \"\"\"\n    Euclidean projection onto the probability simplex:\n    Solve min_x ||x - v||_2 subject to x = 0, sum x = 1.\n    \"\"\"\n    n = v.size\n    # Sort v in descending order\n    u = np.sort(v)[::-1]\n    cssv = np.cumsum(u)\n    # Find rho: the largest j such that u_j - (cssv_j - 1)/j  0\n    j = np.arange(1, n + 1)\n    cond = u - (cssv - 1) / j  0\n    if not np.any(cond):\n        # In degenerate case, distribute uniformly\n        theta = (cssv[-1] - 1) / n\n    else:\n        rho = np.nonzero(cond)[0][-1]\n        theta = (cssv[rho] - 1) / (rho + 1)\n    w = v - theta\n    w = np.maximum(w, 0.0)\n    # Numerical guard to enforce sum to 1 exactly due to possible rounding\n    s = w.sum()\n    if s == 0:\n        # fallback uniform\n        w = np.ones(n) / n\n    else:\n        w /= s\n    return w\n\ndef subgradient_max_linear(C, x):\n    \"\"\"\n    Subgradient of f(x) = max_i c_i^T x at x: choose c_k with k in argmax.\n    Deterministic tie-breaking: smallest index k.\n    \"\"\"\n    values = C @ x\n    k = int(np.argmax(values))\n    return C[k]\n\ndef projected_subgradient(C, x0, T, alpha0):\n    x = x0.copy()\n    for t in range(T):\n        alpha_t = alpha0 / np.sqrt(t + 1)\n        g = subgradient_max_linear(C, x)\n        y = x - alpha_t * g\n        x = project_onto_simplex(y)\n    return x\n\ndef mirror_descent_entropy(C, x0, T, alpha0):\n    x = x0.copy()\n    for t in range(T):\n        alpha_t = alpha0 / np.sqrt(t + 1)\n        g = subgradient_max_linear(C, x)\n        # Multiplicative weights update\n        # Prevent overflow/underflow via clipping exponent argument\n        # However, typical magnitudes here are small; proceed directly.\n        update = x * np.exp(-alpha_t * g)\n        s = update.sum()\n        if s == 0 or not np.isfinite(s):\n            # Fallback to uniform if numerical issues\n            x = np.ones_like(x) / x.size\n        else:\n            x = update / s\n    return x\n\ndef f_max_linear(C, x):\n    return float(np.max(C @ x))\n\ndef run_case(C, T=300, alpha0=0.5):\n    n = C.shape[1]\n    x0 = np.ones(n) / n\n    x_proj = projected_subgradient(C, x0, T, alpha0)\n    x_mirror = mirror_descent_entropy(C, x0, T, alpha0)\n    f_proj = f_max_linear(C, x_proj)\n    f_mirror = f_max_linear(C, x_mirror)\n    return f_proj - f_mirror\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([\n            [0.2, 0.3, -0.1, 0.0, 0.4, -0.2],\n            [-0.1, 0.5, 0.2, -0.3, 0.1, 0.0],\n            [0.3, -0.2, 0.1, 0.2, -0.1, 0.4],\n            [0.0, 0.1, 0.3, 0.5, -0.4, 0.2]\n        ], dtype=float),\n        np.array([\n            [2.0, -1.0, 0.5, 0.0, -0.5, 0.0],\n            [-1.5, 1.0, -0.5, 0.2, 0.1, 0.0],\n            [1.0, -0.5, 1.5, -1.0, 0.0, 0.0]\n        ], dtype=float),\n        np.array([\n            [0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n            [0.45, 0.45, 0.45, 0.45, 0.45, 0.45],\n            [0.52, 0.52, 0.52, 0.52, 0.52, 0.52]\n        ], dtype=float),\n        np.array([\n            [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n        ], dtype=float),\n    ]\n\n    T = 300\n    alpha0 = 0.5\n\n    results = []\n    for C in test_cases:\n        d = run_case(C, T=T, alpha0=alpha0)\n        results.append(d)\n\n    # Final print statement in the exact required format: 6 decimals.\n    formatted = \"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3165049"}]}