{"hands_on_practices": [{"introduction": "本练习将引导你通过一个基础的连续优化问题，亲手推导出帕累托前沿。我们将使用加权和方法，这是一种将多目标问题转化为单目标问题的核心技术，尤其适用于凸问题。通过从一阶最优性条件出发进行解析求解，你将深刻理解帕累托最优解是如何作为不同目标权重下的“平衡点”而产生的 ([@problem_id:3154179])。", "problem": "考虑一个单决策变量 $x \\in \\mathbb{R}$ 的双目标最小化问题，其目标函数为 $f_{1}(x) = (x - 1)^{2} + 1$ 和 $f_{2}(x) = (x + 1)^{2} + 1$。任务是利用多目标优化的基本原理来刻画这两个目标之间的权衡关系。\n\n从帕累托最优性的定义以及“对于凸问题，最小化目标函数的正加权和可以生成帕累托最优点”这一原理出发，使用权重为 $\\alpha \\in [0, 1]$ 的加权和（WS）标量化 $J_{\\alpha}(x) = \\alpha f_{1}(x) + (1 - \\alpha) f_{2}(x)$，解析地推导出目标空间中帕累托前沿的参数形式。您的推导必须从凸函数的一阶最优性条件和帕累托支配的定义开始。\n\n接下来，考虑最小化问题中目标空间内一个有限集的超体积（HV）指标。该指标定义为由该集合弱支配且由一个参考点作为上界的区域的勒贝格测度，其中该参考点比集合中每个点都严格更差（即分量上更大）。设参考点为 $r = (6, 6)$，对于每个 $\\alpha \\in [0, 1]$，令 $x^{\\star}(\\alpha)$ 为 $J_{\\alpha}(x)$ 的一个最小化点。将单点集 $\\{(f_{1}(x^{\\star}(\\alpha)), f_{2}(x^{\\star}(\\alpha)))\\}$ 视为权重为 $\\alpha$ 时帕累托前沿的近似。\n\n仅使用上述定义和一阶条件，推导该单点集相对于参考点 $r = (6, 6)$ 的超体积 $HV(\\alpha)$ 的闭式表达式，该表达式应为关于 $\\alpha$ 的函数。请以精确、简化的解析形式给出 $HV(\\alpha)$ 的最终表达式。不要对答案进行四舍五入。最终答案必须是单个表达式。", "solution": "用户希望我解决一个多目标优化问题。\n\n### 第一步：提取已知条件\n- **决策变量：** $x \\in \\mathbb{R}$。\n- **目标函数（最小化）：**\n  - $f_{1}(x) = (x - 1)^{2} + 1$\n  - $f_{2}(x) = (x + 1)^{2} + 1$\n- **标量化方法：** 加权和（WS）$J_{\\alpha}(x) = \\alpha f_{1}(x) + (1 - \\alpha) f_{2}(x)$。\n- **权重参数：** $\\alpha \\in [0, 1]$。\n- **优化原理：** 最小化凸标量化函数 $J_{\\alpha}(x)$ 可以生成帕累托最优点。推导过程必须使用一阶最优性条件。\n- **超体积（HV）指标定义：** 对于目标空间中的一个有限集和一个参考点 $r$，HV是由该集合弱支配且以 $r$ 为上界的区域的勒贝格测度。\n- **参考点：** $r = (6, 6)$。\n- **用于HV计算的集合：** 单点集 $\\{(f_{1}(x^{\\star}(\\alpha)), f_{2}(x^{\\star}(\\alpha)))\\}$，其中 $x^{\\star}(\\alpha)$ 是 $J_{\\alpha}(x)$ 的一个最小化点。\n- **最终任务：** 推导超体积 $HV(\\alpha)$ 作为 $\\alpha$ 函数的闭式表达式。\n\n### 第二步：使用提取的已知条件进行验证\n1.  **科学或事实的可靠性：** 该问题基于多目标优化的标准理论。帕累托最优性、针对凸问题的加权和方法以及超体积指标等概念在该领域都是公认的。所提供的函数是简单的凸二次函数，适合进行此类分析。该问题在科学上和数学上都是可靠的。\n2.  **非形式化或不相关：** 该问题是高度形式化的，并且与多目标优化主题直接相关。\n3.  **不完整或矛盾的设置：** 所有必要信息都已提供。目标、约束、标量化方法和参考点都有明确定义。没有矛盾之处。\n4.  **不切实际或不可行：** 该问题是一个理论练习；物理现实性不是一个相关标准，但其数学设置是完全可行的。\n5.  **不适定或结构不良：** 该问题是适定的。目标函数是凸函数，确保了对于任何 $\\alpha \\in (0,1)$，加权和都有唯一的最小化点，从而得到一个定义明确的帕累托前沿和唯一的 $HV(\\alpha)$ 值。\n6.  **伪深刻、琐碎或同义反复：** 该问题需要一个涉及微积分和优化定义应用的多步推导。这是一个实质性的练习，而不是琐碎或同义反复的练习。\n7.  **超出科学可验证性范围：** 推导过程和最终表达式在数学上都是可验证的。\n\n### 第三步：结论与行动\n问题有效。我将继续进行解答。\n\n该双目标最小化问题涉及两个目标函数：$f_{1}(x) = (x - 1)^{2} + 1$ 和 $f_{2}(x) = (x + 1)^{2} + 1$。两个函数在 $\\mathbb{R}$ 上都是凸函数。任务是使用加权和（WS）方法找到帕累托前沿的参数化表示，然后推导超体积指标作为权重 $\\alpha$ 函数的表达式。\n\n首先，我们定义WS标量化，它使用权重 $\\alpha \\in [0, 1]$ 将两个目标合并为单个目标函数 $J_{\\alpha}(x)$：\n$$J_{\\alpha}(x) = \\alpha f_{1}(x) + (1 - \\alpha) f_{2}(x)$$\n代入 $f_{1}(x)$ 和 $f_{2}(x)$ 的表达式：\n$$J_{\\alpha}(x) = \\alpha ((x - 1)^{2} + 1) + (1 - \\alpha) ((x + 1)^{2} + 1)$$\n由于 $f_{1}(x)$ 和 $f_{2}(x)$ 是凸函数，且 $\\alpha \\ge 0$，$1 - \\alpha \\ge 0$，它们的非负加权和 $J_{\\alpha}(x)$ 也是一个凸函数。对于凸函数，可以通过将其关于 $x$ 的一阶导数设为零来找到最小值。这是一阶最优性条件。\n$$\\frac{dJ_{\\alpha}(x)}{dx} = \\frac{d}{dx} \\left[ \\alpha (x - 1)^{2} + \\alpha + (1 - \\alpha) (x + 1)^{2} + (1 - \\alpha) \\right] = 0$$\n$$\\frac{dJ_{\\alpha}(x)}{dx} = \\alpha [2(x - 1)] + (1 - \\alpha) [2(x + 1)] = 0$$\n两边除以 $2$，我们得到：\n$$\\alpha (x - 1) + (1 - \\alpha) (x + 1) = 0$$\n$$\\alpha x - \\alpha + x + 1 - \\alpha x - \\alpha = 0$$\n$$x + 1 - 2\\alpha = 0$$\n求解 $x$，我们得到最优决策变量 $x^{\\star}$ 作为 $\\alpha$ 的函数：\n$$x^{\\star}(\\alpha) = 2\\alpha - 1$$\n这个表达式给出了对于每个 $\\alpha \\in [0, 1]$ 在决策空间中的帕累托最优解。所有这些解的集合，$\\{x | x = 2\\alpha - 1, \\alpha \\in [0, 1]\\}$，对应于区间 $[-1, 1]$。\n\n接下来，我们将这些最优解映射到目标空间以找到帕累托前沿。帕累托前沿是对于 $\\alpha \\in [0, 1]$ 的目标向量集合 $(f_{1}(x^{\\star}(\\alpha)), f_{2}(x^{\\star}(\\alpha)))$。我们将帕累托前沿上的点定义为 $p(\\alpha) = (p_1(\\alpha), p_2(\\alpha))$。\n$$p_1(\\alpha) = f_{1}(x^{\\star}(\\alpha)) = f_{1}(2\\alpha - 1) = ((2\\alpha - 1) - 1)^{2} + 1 = (2\\alpha - 2)^{2} + 1 = 4(\\alpha - 1)^{2} + 1$$\n$$p_2(\\alpha) = f_{2}(x^{\\star}(\\alpha)) = f_{2}(2\\alpha - 1) = ((2\\alpha - 1) + 1)^{2} + 1 = (2\\alpha)^{2} + 1 = 4\\alpha^{2} + 1$$\n帕累托前沿的参数形式为 $(4(\\alpha - 1)^{2} + 1, 4\\alpha^{2} + 1)$，其中 $\\alpha \\in [0, 1]$。\n\n问题的第二部分要求计算单点集 $\\{p(\\alpha)\\}$ 相对于参考点 $r = (6, 6)$ 的超体积（HV）指标。在这个二维案例中，HV对应于由点 $p(\\alpha)$ 和参考点 $r$ 形成的超矩形的面积。\n一个点 $(p_1, p_2)$ 相对于参考点 $(r_1, r_2)$ 的HV计算公式为：\n$$HV = (r_1 - p_1)(r_2 - p_2)$$\n我们必须首先验证参考点 $r = (6, 6)$ 严格支配帕累托前沿上的所有点。\n对于 $\\alpha \\in [0, 1]$，$p_1(\\alpha) = 4(\\alpha - 1)^{2} + 1$ 的最大值在 $\\alpha = 0$ 时取得，为 $p_1(0) = 5$。$p_2(\\alpha) = 4\\alpha^{2} + 1$ 的最大值在 $\\alpha = 1$ 时取得，为 $p_2(1) = 5$。因此，对于前沿上的任何点 $(p_1, p_2)$，都有 $p_1 \\le 5$ 和 $p_2 \\le 5$。由于 $r_1=6$ 和 $r_2=6$，该参考点确实是一个有效的上界。\n\n现在，我们计算 $HV(\\alpha)$ 作为 $\\alpha$ 的函数：\n$$HV(\\alpha) = (6 - p_1(\\alpha))(6 - p_2(\\alpha))$$\n我们需要将 $p_1(\\alpha)$ 和 $p_2(\\alpha)$ 表示为 $\\alpha$ 的多项式：\n$$p_1(\\alpha) = 4(\\alpha^{2} - 2\\alpha + 1) + 1 = 4\\alpha^{2} - 8\\alpha + 5$$\n$$p_2(\\alpha) = 4\\alpha^{2} + 1$$\n将这些代入HV公式：\n$$HV(\\alpha) = (6 - (4\\alpha^{2} - 8\\alpha + 5))(6 - (4\\alpha^{2} + 1))$$\n$$HV(\\alpha) = (1 + 8\\alpha - 4\\alpha^{2})(5 - 4\\alpha^{2})$$\n现在，我们展开乘积以获得 $HV(\\alpha)$ 的简化多项式表达式：\n$$HV(\\alpha) = 1(5) + 1(-4\\alpha^{2}) + 8\\alpha(5) + 8\\alpha(-4\\alpha^{2}) - 4\\alpha^{2}(5) - 4\\alpha^{2}(-4\\alpha^{2})$$\n$$HV(\\alpha) = 5 - 4\\alpha^{2} + 40\\alpha - 32\\alpha^{3} - 20\\alpha^{2} + 16\\alpha^{4}$$\n最后，我们按 $\\alpha$ 的降幂合并同类项：\n$$HV(\\alpha) = 16\\alpha^{4} - 32\\alpha^{3} + (-4 - 20)\\alpha^{2} + 40\\alpha + 5$$\n$$HV(\\alpha) = 16\\alpha^{4} - 32\\alpha^{3} - 24\\alpha^{2} + 40\\alpha + 5$$\n这就是单点帕累托点近似的超体积作为权重 $\\alpha$ 函数的闭式表达式。", "answer": "$$\\boxed{16\\alpha^{4} - 32\\alpha^{3} - 24\\alpha^{2} + 40\\alpha + 5}$$", "id": "3154179"}, {"introduction": "理论上，加权和方法能够找到凸问题的所有帕累托最优解，但现实世界的问题往往更为复杂。本练习以一个直观的机器人路径规划场景为例，探讨当目标之间存在非凸权衡时会发生什么。通过分析不同路径的成本，你将发现并非所有最优的“折衷”方案都能通过简单的线性加权找到，从而揭示支撑解与非支撑解之间的关键区别 ([@problem_id:3154169])。", "problem": "一个移动机器人必须在一个离散化的网格世界中，从一个起始单元格移动到一个目标单元格，同时最小化两个相互冲突的目标：总路径长度和累积碰撞风险。形式上，考虑一个 $6 \\times 6$ 的网格，起点为 $(1,1)$，终点为 $(6,6)$，且运动被限制为沿正坐标方向（向右或向上）的单位步长。障碍物的放置使得存在可行的单调路径，并且环境会产生一个非均匀的风险场，每个单元格的风险值为 $r(i,j) \\ge 0$。对于任何可行路径 $P$，定义两个目标函数\n$$\nL(P) = \\text{沿 } P \\text{ 的步数}, \\qquad R(P) = \\sum_{(i,j) \\in P} r(i,j).\n$$\n该多目标问题是在帕累托优势的意义下最小化 $(L(P), R(P))$。由于障碍物的几何形状，出现了三条具有实测目标向量的典型路线\n$$\nv_{A} = (L(P_{A}), R(P_{A})) = (10, 6), \\quad v_{B} = (L(P_{B}), R(P_{B})) = (12, 4), \\quad v_{C} = (L(P_{C}), R(P_{C})) = (11, 5.3).\n$$\n这些路径说明了障碍物如何导致非凸的权衡：$P_{A}$ 是一条较短但风险较高的通道，$P_{B}$ 是一条较长但更安全的绕行路线，而 $P_{C}$ 是一条平衡两者的中间路线。\n\n考虑标准的线性加权和标量化方法\n$$\nf_{w}(P) = w\\,L(P) + (1-w)\\,R(P), \\qquad w \\in [0,1],\n$$\n这是多目标优化中一种常用的获取受支撑有效解的方法。\n\n从帕累托优势和受支撑有效性的核心定义出发，确定唯一的权重 $w \\in [0,1]$，使得 $f_{w}(P_{A}) = f_{w}(P_{B})$。请将最终答案表示为单个实数，并将其四舍五入到四位有效数字。", "solution": "问题陈述经过严格审查并被认为是有效的。它在科学上基于多目标优化的既定原则，特别是关于路径规划问题。该问题是良定的，提供了确定唯一解所需的所有必要定义、数据和约束。语言客观，设定内部一致，没有事实或逻辑上的缺陷。\n\n问题的核心是找到一个权重因子 $w$，使得两条不同的路径 $P_A$ 和 $P_B$ 在线性加权和标量化方案下具有同等的优选度。需要最小化的两个目标函数是路径长度 $L(P)$ 和累积风险 $R(P)$。标量化目标函数 $f_w(P)$ 将这两个目标合并为一个单一值：\n$$f_{w}(P) = w\\,L(P) + (1-w)\\,R(P)$$\n其中权重 $w$ 被限制在区间 $w \\in [0,1]$ 内。较大的 $w$ 优先考虑最小化路径长度，而较小的 $w$ 优先考虑最小化风险。\n\n所讨论的两条路径 $P_A$ 和 $P_B$ 的目标向量已给出：\n- 路径 $P_A$ 的目标向量为 $(L(P_A), R(P_A)) = (10, 6)$。\n- 路径 $P_B$ 的目标向量为 $(L(P_B), R(P_B)) = (12, 4)$。\n\n我们的任务是找到特定的权重 $w$，使得路径 $P_A$ 的标量化成本等于路径 $P_B$ 的标量化成本。这个条件由以下方程表示：\n$$f_{w}(P_A) = f_{w}(P_B)$$\n\n将标量化函数的定义代入此等式，得到：\n$$w\\,L(P_A) + (1-w)\\,R(P_A) = w\\,L(P_B) + (1-w)\\,R(P_B)$$\n\n现在，我们可以将每条路径的目标分量的具体数值代入此方程：\n$$w(10) + (1-w)(6) = w(12) + (1-w)(4)$$\n\n为了求解 $w$，我们首先展开方程两边的项：\n$$10w + 6 - 6w = 12w + 4 - 4w$$\n\n接下来，我们合并方程两边包含 $w$ 的项和常数项：\n$$4w + 6 = 8w + 4$$\n\n现在，我们重新整理方程以分离出 $w$。我们可以将所有含 $w$ 的项移到一边，常数项移到另一边：\n$$6 - 4 = 8w - 4w$$\n$$2 = 4w$$\n\n最后，我们通过除以 $4$ 来求解 $w$：\n$$w = \\frac{2}{4} = \\frac{1}{2}$$\n\n得到的权重是 $w = 0.5$。该值位于定义的区间 $[0,1]$ 内，证实了它在问题背景下是一个有效的解。\n\n在多目标优化的几何解释中，找到一个 $w$ 使得两个非支配点具有相同的标量化值，意味着在目标空间中找到连接这两点的直线的斜率。这条直线是所有可行目标向量集合的凸包边界的一部分。该线段上的点被称为受支撑有效解。路径 $P_C$ 的存在，其目标向量为 $(11, 5.3)$，在 $w=0.5$ 时产生更高的成本（$f_{0.5}(P_C) = 0.5 \\times 11 + 0.5 \\times 5.3 = 8.15$，而 $f_{0.5}(P_A) = f_{0.5}(P_B) = 8$），说明 $P_C$ 代表一个非受支撑有效解，无法通过这种线性标量化方法找到，这证实了问题中提到的“非凸权衡”。\n\n问题要求最终答案四舍五入到四位有效数字。精确解是 $0.5$。表示为四位有效数字，即为 $0.5000$。", "answer": "$$\n\\boxed{0.5000}\n$$", "id": "3154169"}, {"introduction": "在将多目标优化应用于实际问题时，一个常见的陷阱是不同目标的数值尺度差异巨大，例如一个目标范围是0到1，另一个是0到1000。本练习是一个编程实践，旨在让你亲手验证目标归一化的重要性。你将通过代码探索归一化如何影响加权和方法的结果，并理解为何在进行权衡决策前，将所有目标置于可比较的尺度上是至关重要的一步 ([@problem_id:3162727])。", "problem": "考虑一个双目标最小化问题，其决策变量为 $x$，目标为 $f_1(x)$ 和 $f_2(x)$，并在一个有限候选集 $X$ 上进行评估。帕累托支配是分量式定义的：一个决策 $x_a$ 支配一个决策 $x_b$ 当且仅当对于所有 $i \\in \\{1,2\\}$ 都有 $f_i(x_a) \\le f_i(x_b)$，并且至少存在一个索引 $j \\in \\{1,2\\}$ 使得 $f_j(x_a)  f_j(x_b)$。帕累托集是 $X$ 的一个子集，由 $X$ 中所有不被任何其他决策所支配的决策组成。归一化是使用在候选集 $X$ 上计算的分量式乌托邦点和天底点进行的，即 $z_i^\\text{utopia} = \\min_{x \\in X} f_i(x)$ 和 $z_i^\\text{nadir} = \\max_{x \\in X} f_i(x)$，归一化目标定义为\n$$\nf_i'(x) = \\frac{f_i(x) - z_i^\\text{utopia}}{z_i^\\text{nadir} - z_i^\\text{utopia}}, \\quad i \\in \\{1,2\\}.\n$$\n您的任务是研究归一化如何影响帕累托集以及由等权重线性标量化产生的选择。对于下方的每个测试用例，您必须：\n(1) 按规定构建候选集 $X$，并为所有 $x \\in X$ 计算 $f_1(x)$ 和 $f_2(x)$。\n(2) 根据 $X$ 上的目标值计算 $z_i^\\text{utopia}$ 和 $z_i^\\text{nadir}$，并为所有 $x \\in X$ 构建归一化目标 $f_i'(x)$。\n(3) 使用原始目标 $(f_1,f_2)$ 确定 $X$ 中的帕累托集，并使用归一化目标 $(f_1',f_2')$ 确定帕累托集。报告这两个帕累托集是否相同，使用上面定义的严格帕累托支配。\n(4) 使用等权重 $\\mathbf{w} = (1,1)$，计算未归一化标量化 $g(x) = w_1 f_1(x) + w_2 f_2(x)$ 和归一化标量化 $g'(x) = w_1 f_1'(x) + w_2 f_2'(x)$ 在 $X$ 中的最小化子。如果存在多个最小化子，则选择其中最小的 $x$。\n对于每个测试用例，以列表 $[\\text{pareto\\_equal}, \\text{pareto\\_count}, x^\\star_{\\text{unnorm}}, x^\\star_{\\text{norm}}]$ 的形式生成结果，其中 $\\text{pareto\\_equal}$ 是一个布尔值，指示帕累托集是否相同，$\\text{pareto\\_count}$ 是帕累托集（使用原始目标）中的点数的整数计数，而 $x^\\star_{\\text{unnorm}}$ 和 $x^\\star_{\\text{norm}}$ 分别是给出 $g$ 和 $g'$ 的选定最小化子的浮点数。您的程序应将所有测试用例的结果汇总到单行输出中，其中包含用方括号括起来的逗号分隔列表形式的结果，每个测试用例的结果本身也格式化为用方括号括起来的逗号分隔列表，例如 $[[\\dots],[\\dots],[\\dots]]$。\n使用以下三个科学上合理的测试用例，它们突出了归一化的不同方面：\n测试用例 A（具有严格单调冲突的尺度差异）：\n- 候选集：$X = \\{0, 0.01, 0.02, \\dots, 1.00\\}$。\n- 目标：$f_1(x) = 1000 x$, $f_2(x) = 1 - \\sqrt{x}$。\n测试用例 B（第二目标非单调的尺度差异）：\n- 候选集：$X = \\{0, 0.01, 0.02, \\dots, 1.00\\}$。\n- 目标：$f_1(x) = 1000 x^2$, $f_2(x) = (x - 0.6)^2$。\n测试用例 C（粗网格边界情况）：\n- 候选集：$X = \\{0, 0.5, 1.0\\}$。\n- 目标：$f_1(x) = 1000 x$, $f_2(x) = 1 - x$。\n所有量纲均为无量纲。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果（例如 $[[\\text{result}_1],[\\text{result}_2],[\\text{result}_3]]$）。每个测试用例的结果必须遵循上述格式，并且只包含布尔值、整数和浮点数。", "solution": "用户要求我分析一个双目标最小化问题，重点关注归一化对帕累托集以及通过线性标量化获得的解的影响。该分析必须针对三个特定的测试用例进行。\n\n首先验证问题的正确性和可解性。\n\n### 第 1 步：提取已知条件\n- **决策变量**：$x$\n- **候选集**：一个有限集 $X$。\n- **目标**：两个需要最小化的函数，$f_1(x)$ 和 $f_2(x)$。\n- **帕累托支配**：一个决策 $x_a$ 支配 $x_b$ 当且仅当对于所有 $i \\in \\{1,2\\}$ 都有 $f_i(x_a) \\le f_i(x_b)$，并且至少存在一个索引 $j \\in \\{1,2\\}$ 使得 $f_j(x_a)  f_j(x_b)$。\n- **帕累托集**：$X$ 的子集，包含所有不被 $X$ 中任何其他决策所支配的决策。\n- **归一化**：\n    - 乌托邦点：$z_i^\\text{utopia} = \\min_{x \\in X} f_i(x)$。\n    - 天底点：$z_i^\\text{nadir} = \\max_{x \\in X} f_i(x)$。\n    - 归一化目标：$f_i'(x) = \\frac{f_i(x) - z_i^\\text{utopia}}{z_i^\\text{nadir} - z_i^\\text{utopia}}$，对于 $i \\in \\{1,2\\}$。\n- **线性标量化**：\n    - 权重：$\\mathbf{w} = (w_1, w_2) = (1,1)$。\n    - 未归一化标量化：$g(x) = w_1 f_1(x) + w_2 f_2(x)$。\n    - 归一化标量化：$g'(x) = w_1 f_1'(x) + w_2 f_2'(x)$。\n- **决胜规则**：如果标量化存在多个最小化子，则选择 $x$ 值最小的那个。\n- **测试用例**：\n    - **A**：$X = \\{0, 0.01, \\dots, 1.00\\}$, $f_1(x) = 1000 x$, $f_2(x) = 1 - \\sqrt{x}$。\n    - **B**：$X = \\{0, 0.01, \\dots, 1.00\\}$, $f_1(x) = 1000 x^2$, $f_2(x) = (x - 0.6)^2$。\n    - **C**：$X = \\{0, 0.5, 1.0\\}$, $f_1(x) = 1000 x$, $f_2(x) = 1 - x$。\n- **每个用例的所需输出**：一个列表 $[\\text{pareto\\_equal}, \\text{pareto\\_count}, x^\\star_{\\text{unnorm}}, x^\\star_{\\text{norm}}]$。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题在科学上和数学上是合理的。它涉及多目标优化中的标准且明确定义的概念：帕累托支配、归一化和线性标量化。问题是自包含的，提供了所有必要的定义、函数和数据集。候选集 $X$ 是有限的，目标函数在各自的定义域上是良态的，这确保了乌托邦点和天底点是明确定义的。对于所有指定的测试用例，归一化分母 $z_i^\\text{nadir} - z_i^\\text{utopia}$ 都不为零，因为没有一个目标函数在候选集 $X$ 上是常数。决胜规则确保了标量化最小化有唯一的解。该问题是适定的，不违反任何科学原理，也不包含逻辑矛盾。\n\n### 第 3 步：结论和行动\n问题是有效的。将开发一个完整的解决方案。\n\n### 基于原则的解决方案\n核心任务是分析归一化如何影响帕累托最优性和等权重线性标量化方法的结果。我们将首先建立一个关于帕累托集的理论原则，然后概述计算每个测试用例所需量的算法流程。\n\n**1. 归一化下帕累托集的不变性**\n\n指定的归一化是一种形式为 $f_i'(x) = a_i f_i(x) + b_i$ 的分量式仿射变换，其中：\n$$ a_i = \\frac{1}{z_i^\\text{nadir} - z_i^\\text{utopia}} \\quad \\text{和} \\quad b_i = \\frac{-z_i^\\text{utopia}}{z_i^\\text{nadir} - z_i^\\text{utopia}} $$\n对于在候选集 $X$ 上的非常数目标函数，我们有 $z_i^\\text{nadir} > z_i^\\text{utopia}$，这意味着 $a_i > 0$。具有正缩放因子（$a_i > 0$）的仿射变换是严格递增的。\n\n让我们研究这种变换如何影响帕累托支配关系。假设一个决策 $x_a$ 在原始目标空间中支配一个决策 $x_b$。根据定义：\n1. 对于 $i \\in \\{1,2\\}$，$f_i(x_a) \\le f_i(x_b)$。\n2. 至少存在一个 $j \\in \\{1,2\\}$，使得 $f_j(x_a)  f_j(x_b)$。\n\n由于 $a_i > 0$，我们可以将不等式乘以 $a_i$ 而不改变它们的方向：\n1. 对于 $i \\in \\{1,2\\}$，$a_i f_i(x_a) \\le a_i f_i(x_b)$。\n2. 至少存在一个 $j \\in \\{1,2\\}$，使得 $a_j f_j(x_a)  a_j f_j(x_b)$。\n\n在两边加上常数 $b_i$ 同样保持不等式：\n1. $a_i f_i(x_a) + b_i \\le a_i f_i(x_b) + b_i \\implies f_i'(x_a) \\le f_i'(x_b)$，对于 $i \\in \\{1,2\\}$。\n2. $a_j f_j(x_a) + b_j  a_j f_j(x_b) + b_j \\implies f_j'(x_a)  f_j'(x_b)$，对于至少一个 $j \\in \\{1,2\\}$。\n\n这表明 $x_a$ 在归一化目标空间中支配 $x_b$。通过同样的逻辑，反向推导也成立。因此，原始目标和归一化目标的支配关系是相同的。因此，非支配解的集合——即帕累托集——在这种归一化下是不变的。这意味着对于所有测试用例，`pareto_equal` 的布尔值都将是 `True`。\n\n**2. 算法流程**\n\n对每个测试用例执行以下流程：\n- **步骤 2.1：评估。** 生成候选集 $X$。对每个 $x \\in X$，计算并存储目标向量 $\\mathbf{f}(x) = (f_1(x), f_2(x))$。\n- **步骤 2.2：帕累托集识别。** 通过成对比较来识别帕累托集。对于每个候选解 $x_i$，将其与每个其他候选解 $x_j$ 进行比较。如果有任何 $x_j$ 支配 $x_i$（根据定义），则将 $x_i$ 标记为被支配。所有未被标记为被支配的候选解构成帕累托集。该集的大小即为 `pareto_count`。\n- **步骤 2.3：归一化。** 通过取每个目标 $i \\in \\{1,2\\}$ 的评估目标值的最小值和最大值，找到乌托邦点和天底点 $z_i^\\text{utopia}$ 和 $z_i^\\text{nadir}$。然后为所有 $x \\in X$ 计算归一化目标 $f_i'(x)$。如前所述，这不会改变帕累托集。\n- **步骤 2.4：标量化与最小化。** 为所有 $x \\in X$ 计算未归一化和归一化的标量化目标函数：\n$$ g(x) = f_1(x) + f_2(x) $$\n$$ g'(x) = f_1'(x) + f_2'(x) $$\n找到每个函数的最小值 $\\min_{x \\in X} g(x)$ 和 $\\min_{x \\in X} g'(x)$。识别出达到该最小值的所​​有 $x$ 值的集合。根据决胜规则，从此集合中选择最小的 $x$ 作为最小化子，从而得到 $x^\\star_{\\text{unnorm}}$ 和 $x^\\star_{\\text{norm}}$。\n\n**3. 应用于测试用例**\n\n- **测试用例 A**：$f_1(x) = 1000 x$ 和 $f_2(x) = 1 - \\sqrt{x}$。在 $X = \\{0, 0.01, \\dots, 1.00\\}$ 上，$f_1(x)$ 严格递增，而 $f_2(x)$ 严格递减。这造成了直接冲突，意味着没有解能支配另一个。整个包含 $101$ 个点的集合 $X$ 都是帕累托集。`pareto_count`=101。\n- 未归一化的标量化 $g(x) = 1000x + 1 - \\sqrt{x}$，其导数 $g'(x) = 1000 - \\frac{1}{2\\sqrt{x}}$ 对所有 $x \\in (0,1]$ 均为正。因此函数是递增的。最小值在 $x=0$。$x^\\star_{\\text{unnorm}} = 0$。\n- 归一化后，$f_1'(x) = x$，$f_2'(x) = 1-\\sqrt{x}$。归一化的标量化 $g'(x) = x + 1 - \\sqrt{x}$ 的导数为 $\\frac{dg'}{dx} = 1 - \\frac{1}{2\\sqrt{x}}$。令其为零，得到 $x = 0.25$。这是候选集中的一个点，所以 $x^\\star_{\\text{norm}} = 0.25$。\n- **测试用例 B**：$f_1(x) = 1000 x^2$ 和 $f_2(x) = (x - 0.6)^2$。在 $x \\in [0, 0.6]$ 区间，$f_1$ 增加，$f_2$ 减少，构成帕累托前沿。当 $x > 0.6$ 时，两个目标都增加，所以这些点都被 $x=0.6$ 支配。因此，帕累托集是 $\\{0, 0.01, \\dots, 0.60\\}$，共 $61$ 个点。\n- 未归一化的标量化 $g(x)$ 被 $1000x^2$ 项主导，其最小值在 $x=0$。$x^\\star_{\\text{unnorm}}=0$。\n- 归一化后，$f_1'(x) = x^2$，$f_2'(x) = \\frac{(x-0.6)^2}{0.36}$。归一化的标量化 $g'(x) = x^2 + \\frac{(x-0.6)^2}{0.36}$。令其导数为零得到 $x \\approx 0.44117$。最接近的网格点是 $x=0.44$。$x^\\star_{\\text{norm}}=0.44$。\n- **测试用例 C**：$f_1(x) = 1000 x$ 和 $f_2(x) = 1 - x$ 在粗网格 $X=\\{0, 0.5, 1.0\\}$ 上。目标点为 $(0,1)$, $(500,0.5)$, $(1000,0)$。这三点互不支配，因此帕累托集包含所有 $3$ 个点。\n- 未归一化的标量化 $g(x) = 999x + 1$ 在 $x=0$ 处最小化。$x^\\star_{\\text{unnorm}}=0$。\n- 归一化后，$f_1'(x)=x$，$f_2'(x)=1-x$。归一化的标量化 $g'(x) = x + (1-x) = 1$，对所有 $x \\in X$ 是常数。所有三个点都是最小化子。根据决胜规则选择最小的 $x$，因此 $x^\\star_{\\text{norm}} = 0$。\n\n此分析为接下来的计算实现提供了基础。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multi-objective optimization problem for three test cases.\n    \"\"\"\n\n    def find_pareto_indices(objectives: np.ndarray) -> set:\n        \"\"\"\n        Identifies the indices of Pareto optimal solutions in a set.\n\n        Args:\n            objectives: An (n_candidates, n_objectives) numpy array of objective values.\n\n        Returns:\n            A set of integer indices corresponding to the Pareto optimal solutions.\n        \"\"\"\n        num_candidates = objectives.shape[0]\n        is_dominated = np.zeros(num_candidates, dtype=bool)\n\n        for i in range(num_candidates):\n            # Check if candidate i is dominated by any other candidate j\n            for j in range(num_candidates):\n                if i == j:\n                    continue\n                \n                # Strict Pareto dominance check:\n                # j dominates i if f_k(j) = f_k(i) for all objectives k\n                # and f_k(j)  f_k(i) for at least one objective k.\n                if np.all(objectives[j] = objectives[i]) and np.any(objectives[j]  objectives[i]):\n                    is_dominated[i] = True\n                    break  # i is dominated, no need to check other j's\n\n        pareto_indices = np.where(~is_dominated)[0]\n        return set(pareto_indices)\n\n    def process_case(x_values: np.ndarray, f1_func: callable, f2_func: callable) -> list:\n        \"\"\"\n        Performs the full analysis for a single test case.\n\n        Args:\n            x_values: 1D numpy array of decision variable candidates.\n            f1_func: The first objective function.\n            f2_func: The second objective function.\n\n        Returns:\n            A list containing [pareto_equal, pareto_count, x_star_unnorm, x_star_norm].\n        \"\"\"\n        # (1) Compute objectives for all candidates\n        f1_values = f1_func(x_values)\n        f2_values = f2_func(x_values)\n        objectives = np.vstack([f1_values, f2_values]).T\n\n        # (2) Compute normalized objectives\n        z1_utopia, z1_nadir = np.min(f1_values), np.max(f1_values)\n        z2_utopia, z2_nadir = np.min(f2_values), np.max(f2_values)\n\n        # The problem statement ensures denominators are non-zero for these test cases.\n        denom1 = z1_nadir - z1_utopia if z1_nadir > z1_utopia else 1.0\n        denom2 = z2_nadir - z2_utopia if z2_nadir > z2_utopia else 1.0\n\n        f1_prime = (f1_values - z1_utopia) / denom1\n        f2_prime = (f2_values - z2_utopia) / denom2\n        normalized_objectives = np.vstack([f1_prime, f2_prime]).T\n\n        # (3) Determine Pareto sets and compare\n        original_pareto_indices = find_pareto_indices(objectives)\n        # As proven, normalization does not change the Pareto set.\n        # We can computationally verify this, but it must be true.\n        normalized_pareto_indices = find_pareto_indices(normalized_objectives)\n        \n        pareto_equal = (original_pareto_indices == normalized_pareto_indices)\n        pareto_count = len(original_pareto_indices)\n        \n        # (4) Compute minimizers of scalarizations (w = [1, 1])\n        # Unnormalized scalarization\n        g = f1_values + f2_values\n        min_g = np.min(g)\n        min_g_indices = np.where(np.isclose(g, min_g))[0]\n        # Tie-breaking: select smallest x\n        x_star_unnorm = x_values[min_g_indices[0]]\n\n        # Normalized scalarization\n        g_prime = f1_prime + f2_prime\n        min_g_prime = np.min(g_prime)\n        min_g_prime_indices = np.where(np.isclose(g_prime, min_g_prime))[0]\n        # Tie-breaking: select smallest x\n        x_star_norm = x_values[min_g_prime_indices[0]]\n\n        return [pareto_equal, pareto_count, float(round(x_star_unnorm, 2)), float(round(x_star_norm, 2))]\n\n    # Define the test cases from the problem statement.\n    test_cases_defs = [\n        {\n            \"x_values\": np.linspace(0, 1.0, 101, dtype=float),\n            \"f1\": lambda x: 1000.0 * x,\n            \"f2\": lambda x: 1.0 - np.sqrt(x)\n        },\n        {\n            \"x_values\": np.linspace(0, 1.0, 101, dtype=float),\n            \"f1\": lambda x: 1000.0 * x**2,\n            \"f2\": lambda x: (x - 0.6)**2\n        },\n        {\n            \"x_values\": np.array([0.0, 0.5, 1.0], dtype=float),\n            \"f1\": lambda x: 1000.0 * x,\n            \"f2\": lambda x: 1.0 - x\n        }\n    ]\n\n    results = []\n    for case_def in test_cases_defs:\n        result = process_case(case_def[\"x_values\"], case_def[\"f1\"], case_def[\"f2\"])\n        results.append(result)\n    \n    # Format the results into a single string as specified\n    result_str = str(results).replace(\" \", \"\")\n\n    # Final print statement in the exact required format.\n    print(result_str)\n\nsolve()\n```", "id": "3162727"}]}