{"hands_on_practices": [{"introduction": "标准的马尔可夫决策过程假设成本仅取决于当前状态和行动。但如果成本还依赖于“前一个”行动，例如改变设备设置会产生转换成本，情况会怎样呢？这个练习 [@problem_id:3101515] 将展示一个巧妙的技巧：通过扩展状态定义，将对过去行动的“记忆”包含进来，从而恢复马尔可夫性质。这项技术是为各种具有路径依赖成本的现实世界问题建模的基础。", "problem": "考虑一个有限时域的确定性离散时间控制问题，时间指标为 $t \\in \\{0,1,2\\}$。状态为 $x_t \\in \\mathbb{R}$，控制为 $u_t \\in \\mathcal{U}$，其中 $\\mathcal{U} = \\{-1, 0, 1\\}$。动态方程为 $x_{t+1} = x_t + u_t$。在时间 $t \\in \\{0,1\\}$ 的阶段成本为\n$$\n\\ell(x_t,u_t,u_{t-1}) = x_t^2 + u_t^2 + k \\,\\mathbf{1}\\{u_t \\neq u_{t-1}\\},\n$$\n其中 $k > 0$ 是一个给定的常数，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数。在时间 $t = 2$ 的终端成本为零。\n\n- 解释为什么在存在切换惩罚 $k \\,\\mathbf{1}\\{u_t \\neq u_{t-1}\\}$ 的情况下，仅使用状态 $x_t$ 的过程不满足马尔可夫性，并定义一个增广状态 $s_t = (x_t, u_{t-1})$ 来恢复马尔可夫性。利用最优性原理，为价值函数 $V_t(x,\\bar{u})$ 在时域 $t \\in \\{0,1,2\\}$ 上构建动态规划递推式（贝尔曼方程）。\n\n- 对于具体实例 $k = 1$，初始状态 $x_0 = 1$，以及初始的前一时刻控制 $u_{-1} = 0$，计算最优的总未来成本 $V_0(1,0)$。\n\n- 假设对 $x$ 空间进行离散化，大小为 $|\\mathcal{X}|$，简要讨论将前一时刻的控制增广到状态中对每次动态规划（DP）回溯所需评估的状态数量的计算影响。\n\n你的最终数值答案应该是指定实例的最优总成本 $V_0(1,0)$。无需四舍五入。", "solution": "这个问题是一个确定性的有限时域最优控制问题。求解需要通过动态规划应用最优性原理。分析按要求分为三个部分进行。\n\n首先，我们解决问题的非马尔可夫性，并构建相应的动态规划递推式。\n如果未来状态的条件概率分布在给定当前和过去状态的情况下仅依赖于当前状态，则该随机过程是马尔可夫的。在最优控制的背景下，如果时刻 $t$ 的最优控制仅依赖于当前状态 $x_t$，则决策过程是马尔可夫的。需要最小化的总成本是阶段成本之和，即 $\\sum_{t=0}^{1} \\ell(x_t, u_t, u_{t-1})$。在时刻 $t$ 的阶段成本 $\\ell(x_t, u_t, u_{t-1}) = x_t^2 + u_t^2 + k \\,\\mathbf{1}\\{u_t \\neq u_{t-1}\\}$ 明确地依赖于前一时刻的控制 $u_{t-1}$。为了选择最优控制 $u_t$，决策者在时刻 $t$ 必须同时知道状态 $x_t$ 和前一时刻的控制 $u_{t-1}$，以便正确评估切换惩罚。由于 $u_{t-1}$ 不属于状态 $x_t$ 的一部分，仅凭 $x_t$ 中包含的信息不足以为未来做出最优决策。因此，使用状态 $x_t$ 的决策过程不是马尔可夫的。\n\n为了恢复马尔可夫性，我们必须增广状态以包含决策所需的所有信息。我们为 $t \\in \\{0, 1, 2\\}$ 定义一个增广状态 $s_t = (x_t, u_{t-1})$。根据这个定义，时刻 $t$ 的状态是 $s_t = (x_t, u_{t-1})$，控制是 $u_t$。状态转移由 $s_{t+1} = (x_{t+1}, u_t) = (x_t + u_t, u_t)$ 给出。新状态 $s_{t+1}$ 仅依赖于当前状态 $s_t$ 和当前控制 $u_t$。阶段成本 $\\ell(x_t, u_t, u_{t-1})$ 可以写成一个函数 $\\tilde{\\ell}(s_t, u_t)$，它只依赖于增广状态和控制。这种结构确保了该过程是一个马尔可夫决策过程。\n\n价值函数 $V_t(s_t)$ 表示从时刻 $t$ 的状态 $s_t$ 开始的最优未来成本。我们将其写为 $V_t(x_t, u_{t-1})$。由最优性原理推导出的贝尔曼方程为：\n$$V_t(x_t, u_{t-1}) = \\min_{u_t \\in \\mathcal{U}} \\left\\{ \\ell(x_t, u_t, u_{t-1}) + V_{t+1}(x_{t+1}, u_t) \\right\\}$$\n代入问题的具体细节，对于 $t \\in \\{0, 1\\}$，递推式为：\n$$V_t(x_t, u_{t-1}) = \\min_{u_t \\in \\{-1, 0, 1\\}} \\left\\{ x_t^2 + u_t^2 + k \\, \\mathbf{1}\\{u_t \\neq u_{t-1}\\} + V_{t+1}(x_t + u_t, u_t) \\right\\}$$\n终端条件是时刻 $t=2$ 的成本为零，这意味着对于所有可能的 $x_2$ 和 $u_1$，$V_2(x_2, u_1) = 0$。\n\n第二，我们计算特定实例 $k=1$，$x_0=1$ 和 $u_{-1}=0$ 下的最优成本 $V_0(1, 0)$。我们从 $t=2$ 开始逆向时间进行计算。\n\n**时刻 $t=2$**：终端价值函数给定为零。\n$$V_2(x_2, u_1) = 0$$\n\n**时刻 $t=1$**：我们计算 $V_1(x_1, u_0)$。\n$$V_1(x_1, u_0) = \\min_{u_1 \\in \\{-1, 0, 1\\}} \\left\\{ x_1^2 + u_1^2 + 1 \\cdot \\mathbf{1}\\{u_1 \\neq u_0\\} + V_2(x_1 + u_1, u_1) \\right\\}$$\n由于 $V_2(\\cdot, \\cdot) = 0$，这可以简化为：\n$$V_1(x_1, u_0) = x_1^2 + \\min_{u_1 \\in \\{-1, 0, 1\\}} \\left\\{ u_1^2 + \\mathbf{1}\\{u_1 \\neq u_0\\} \\right\\}$$\n最小化取决于 $u_0 \\in \\{-1, 0, 1\\}$ 的值。\n- 如果 $u_0 = 0$：需要最小化的项是 $\\min \\{(-1)^2 + 1, 0^2 + 0, 1^2 + 1\\} = \\min\\{2, 0, 2\\} = 0$。因此，$V_1(x_1, 0) = x_1^2$。\n- 如果 $u_0 = 1$：需要最小化的项是 $\\min \\{(-1)^2 + 1, 0^2 + 1, 1^2 + 0\\} = \\min\\{2, 1, 1\\} = 1$。因此，$V_1(x_1, 1) = x_1^2 + 1$。\n- 如果 $u_0 = -1$：需要最小化的项是 $\\min \\{(-1)^2 + 0, 0^2 + 1, 1^2 + 1\\} = \\min\\{1, 1, 2\\} = 1$。因此，$V_1(x_1, -1) = x_1^2 + 1$。\n\n**时刻 $t=0$**：我们计算 $V_0(1, 0)$。\n$$V_0(1, 0) = \\min_{u_0 \\in \\{-1, 0, 1\\}} \\left\\{ 1^2 + u_0^2 + 1 \\cdot \\mathbf{1}\\{u_0 \\neq 0\\} + V_1(1 + u_0, u_0) \\right\\}$$\n我们对每个可能的控制 $u_0$ 计算大括号中的表达式：\n- 对于 $u_0 = -1$：成本为 $1^2 + (-1)^2 + \\mathbf{1}\\{-1 \\neq 0\\} + V_1(1 - 1, -1) = 1 + 1 + 1 + V_1(0, -1)$。使用我们导出的函数，$V_1(0, -1) = 0^2 + 1 = 1$。总成本为 $1+1+1+1 = 4$。\n- 对于 $u_0 = 0$：成本为 $1^2 + 0^2 + \\mathbf{1}\\{0 \\neq 0\\} + V_1(1 + 0, 0) = 1 + 0 + 0 + V_1(1, 0)$。使用我们导出的函数，$V_1(1, 0) = 1^2 = 1$。总成本为 $1+0+0+1 = 2$。\n- 对于 $u_0 = 1$：成本为 $1^2 + 1^2 + \\mathbf{1}\\{1 \\neq 0\\} + V_1(1 + 1, 1) = 1 + 1 + 1 + V_1(2, 1)$。使用我们导出的函数，$V_1(2, 1) = 2^2 + 1 = 5$。总成本为 $1+1+1+5 = 8$。\n\n比较这三种结果，最小成本为 $2$，在 $u_0=0$ 时取得。\n因此，$V_0(1, 0) = 2$。\n\n第三，我们讨论状态增广的计算影响。\n在一个具有像 $\\mathbb{R}$ 这样的连续状态空间的典型动态规划设置中，我们必须对空间进行离散化才能数值地应用该算法。设 $x$ 分量的离散化空间为 $\\mathcal{X}$，其大小为 $|\\mathcal{X}|$。控制空间 $\\mathcal{U}$ 已经是离散的，大小为 $|\\mathcal{U}|=3$。\n\n如果没有状态增广，（假设的、不正确的）状态空间将只是 $\\mathcal{X}$，大小为 $|\\mathcal{X}|$。对每个状态的一次 DP 回溯将涉及遍历 $|\\mathcal{U}|$ 个控制。一个时间步更新的总复杂度将与 $|\\mathcal{X}| \\times |\\mathcal{U}|$ 成正比。\n\n使用增广状态 $s_t = (x_t, u_{t-1})$，状态空间变成离散化的 $x$ 空间和控制空间的笛卡尔积，即 $\\mathcal{X} \\times \\mathcal{U}$。这个增广状态空间的大小是 $|\\mathcal{X}| \\times |\\mathcal{U}|$。为了计算时刻 $t$ 每个状态的价值函数，我们必须在所有可能的控制 $u_t \\in \\mathcal{U}$ 上进行最小化。因此，对于 $|\\mathcal{X}| \\times |\\mathcal{U}|$ 个状态中的每一个，我们执行 $|\\mathcal{U}|$ 次评估。一个时间步更新（一次完整的 DP 回溯）的总复杂度与 $(|\\mathcal{X}| \\times |\\mathcal{U}|) \\times |\\mathcal{U}| = |\\mathcal{X}| \\times |\\mathcal{U}|^2$ 成正比。\n\n增广使必须评估的状态空间大小增加了 $|\\mathcal{U}|$ 倍。因此，每次 DP 回溯的计算复杂度也增加了 $|\\mathcal{U}|$ 倍。在这个具体问题中， $|\\mathcal{U}| = 3$，所以与忽略切换成本对过去行为依赖性的幼稚（且不正确）模型相比，状态空间大小和每步计算量都增加了三倍。这是“维度灾难”的一种表现，即扩展状态向量会导致状态空间体积和计算成本的指数级增长。", "answer": "$$\n\\boxed{2}\n$$", "id": "3101515"}, {"introduction": "想象一个机器人需要穿过城市到达目的地。它不仅要找到最短路径，还要管理有限的电池电量，并决定何时何地充电。这个练习 [@problem_id:3101499] 精确地模拟了这一场景。我们将学习如何将资源（能量）纳入状态定义，并使用贝尔曼方程来做出关于移动和充电的最优决策。", "problem": "考虑一个有向网络，其节点为 $s$、$A$、$B$、$C$、$D$、$t$。一个代理从节点 $s$ 出发，初始能量资源为 $E_0 = 3$ 个单位，电池容量为 $E_{\\max} = 5$ 个单位。穿行一条有向弧 $(i,j)$ 会产生非负的旅行时间成本 $d_{ij}$ 并消耗能量 $c_{ij}$；代理只有在当前能量 $e$ 满足 $e \\ge c_{ij}$ 时才能穿行 $(i,j)$。一些节点设有充电站：在节点 $A$，充满电的时间成本为 $r_A = 6$ 个单位，在节点 $C$，充满电的时间成本为 $r_C = 3$ 个单位。在节点 $i \\in \\{A,C\\}$ 充电仅当 $e  E_{\\max}$ 时可用，并立即将能量设置为 $E_{\\max}$。所有成本都是无量纲的标量。\n\n有向弧及其旅行时间和能量消耗如下：\n- 从 $s$ 到 $A$：$d_{sA} = 4$， $c_{sA} = 2$。\n- 从 $s$ 到 $B$：$d_{sB} = 3$， $c_{sB} = 3$。\n- 从 $s$ 到 $C$：$d_{sC} = 5$， $c_{sC} = 2$。\n- 从 $A$ 到 $C$：$d_{AC} = 2$， $c_{AC} = 2$。\n- 从 $A$ 到 $D$：$d_{AD} = 5$， $c_{AD} = 4$。\n- 从 $A$ 到 $t$：$d_{At} = 9$， $c_{At} = 5$。\n- 从 $B$ 到 $C$：$d_{BC} = 1$， $c_{BC} = 2$。\n- 从 $B$ 到 $D$：$d_{BD} = 4$， $c_{BD} = 1$。\n- 从 $B$ 到 $t$：$d_{Bt} = 7$， $c_{Bt} = 4$。\n- 从 $C$ 到 $D$：$d_{CD} = 2$， $c_{CD} = 1$。\n- 从 $C$ 到 $t$：$d_{Ct} = 6$， $c_{Ct} = 4$。\n- 从 $D$ 到 $t$：$d_{Dt} = 3$， $c_{Dt} = 2$。\n\n你的任务：\n1. 使用 Bellman 最优性原理，通过资源水平来扩充状态，并定义一个价值函数 $V(i,e)$，表示从具有当前能量 $e \\in \\{0,1,2,3,4,5\\}$ 的节点 $i$ 到达节点 $t$ 的最小剩余旅行时间。清晰地陈述边界条件，并推导包含穿行和充电决策的 $V(i,e)$ 的 Bellman 递推关系。\n2. 在前向动态规划 (DP) 标签设置解释中，分析用于标签剪枝的支配规则：给定同一节点 $i$ 上的两个标签，分别为对偶 $(\\ell_1, e_1)$ 和 $(\\ell_2, e_2)$，其中 $\\ell_k$ 是到达 $i$ 的累积旅行时间，而 $e_k$ 是到达时剩余的能量，陈述一个标签支配另一个标签的充分条件，并使用 Bellman 最优性原理证明其合理性。使用此实例中的一个明确比较来说明该规则。\n3. 在给定约束下，从能量 $E_0 = 3$ 开始，并假设做出最优充电决策，计算从 $s$ 到 $t$ 的最小总旅行时间。将最终答案报告为一个精确整数（无需四舍五入）。", "solution": "该问题是一个带资源约束（能量）的有向图上的最短路径问题。代理的状态由其当前位置（节点）和当前能量水平定义。目标是找到从源节点 $s$ 到终端节点 $t$ 的一条总旅行时间最短的路径。这可以用动态规划来解决。\n\n首先，我们形式化问题参数。节点集合为 $\\mathcal{V} = \\{s, A, B, C, D, t\\}$。初始能量为 $E_0 = 3$，最大能量容量为 $E_{\\max} = 5$。能量水平 $e$ 是一个整数，所以 $e \\in \\{0, 1, 2, 3, 4, 5\\}$。对于网络中的每条弧 $(i,j)$，都有一个相关的旅行时间 $d_{ij} \\ge 0$ 和能量消耗 $c_{ij} \\ge 0$。在节点 $A$ 和 $C$ 可以充电。在 $A$ 充电的时间成本是 $r_A = 6$，在 $C$ 是 $r_C = 3$。充电将能量补充到 $E_{\\max}$。\n\n### 1. Bellman 原理与递推\n\n根据 Bellman 最优性原理，最优路径的任何子路径本身也必须是最优的。为应用此原理，我们通过代理的当前节点 $i \\in \\mathcal{V}$ 和当前能量水平 $e \\in \\{0, 1, \\dots, E_{\\max}\\}$ 来定义状态。\n\n令 $V(i, e)$ 为价值函数，表示从当前状态 $(i, e)$ 到达目标节点 $t$ 的最小可能累积旅行时间。这是一个“未来成本”(cost-to-go)函数。\n\n边界条件在目标节点 $t$ 处定义。从 $t$ 到达 $t$ 的时间为 $0$，无论到达时的能量水平如何。\n$$V(t, e) = 0, \\quad \\forall e \\in \\{0, 1, 2, 3, 4, 5\\}$$\n\n对于任何其他节点 $i \\neq t$，价值函数 $V(i, e)$ 是通过对从状态 $(i, e)$ 出发的所有可能行动的成本取最小值来确定的。可能的行动是穿行到相邻节点或充电（如果在充电站）。\n\n令 $\\mathcal{N}(i)$ 表示存在有向弧 $(i,j)$ 的节点 $j$ 的集合。穿行弧 $(i, j)$ 的成本是旅行时间 $d_{ij}$ 和从新状态 $(j, e - c_{ij})$ 出发的最小未来成本之和。此行动仅在当前能量充足时可行，即 $e \\ge c_{ij}$。\n\nBellman 递推可以根据节点 $i$ 的类型来构建：\n\n**情况1：节点 $i$ 不是充电站 ($i \\in \\{s, B, D\\}$)**\n唯一可能的行动是穿行到相邻节点 $j \\in \\mathcal{N}(i)$。\n$$V(i, e) = \\min_{j \\in \\mathcal{N}(i) \\text{ s.t. } e \\ge c_{ij}} \\{d_{ij} + V(j, e - c_{ij})\\}$$\n如果没有可行的移动（即可行的下一节点集合为空），则成本为无穷大，$V(i, e) = \\infty$，因为从该状态无法到达 $t$。\n\n**情况2：节点 $i$ 是充电站 ($i \\in \\{A, C\\}$)**\n除了穿行，代理还可以选择充电。此行动仅在 $e  E_{\\max}$ 时可用。充电需要时间 $r_i$ 并将能量水平设置为 $E_{\\max}$。后续成本为 $V(i, E_{\\max})$。\n\n如果 $e  E_{\\max}$:\n$$V(i, e) = \\min \\left( r_i + V(i, E_{\\max}), \\quad \\min_{j \\in \\mathcal{N}(i) \\text{ s.t. } e \\ge c_{ij}} \\{d_{ij} + V(j, e - c_{ij})\\} \\right)$$\n其中 $r_i$ 对于 $i=A$ 是 $r_A=6$，对于 $i=C$ 是 $r_C=3$。\n\n如果 $e = E_{\\max}$:\n代理不能再充电。方程简化为非充电节点的情况：\n$$V(i, E_{\\max}) = \\min_{j \\in \\mathcal{N}(i) \\text{ s.t. } E_{\\max} \\ge c_{ij}} \\{d_{ij} + V(j, E_{\\max} - c_{ij})\\}$$\n\n目标是找到从起始状态 $(s, E_0)$ 出发的最小时间，即 $V(s, 3)$。\n\n### 2. 用于标签剪枝的支配规则\n\n在一个前向动态规划算法（例如适用于此问题的 Dijkstra 算法的变体）中，我们从源节点 $s$ 开始探索路径。我们在每个节点维护一组非支配的“标签”。节点 $i$ 的一个标签是一个对偶 $(\\ell, e)$，表示从 $s$ 到 $i$ 的一条路径，其总旅行时间为 $\\ell$，剩余能量为 $e$。\n\n**支配条件：**\n考虑同一节点 $i$ 上的两个标签：$L_1 = (\\ell_1, e_1)$ 和 $L_2 = (\\ell_2, e_2)$。如果标签 $L_1$ 在一个维度（成本或能量）上至少与标签 $L_2$ 一样好，而在另一个维度上严格更优，或者在两个维度上都更优，则称 $L_1$ 支配 $L_2$。$L_1$ 支配 $L_2$ 的一个充分条件是：\n$$\\ell_1 \\le \\ell_2 \\quad \\text{且} \\quad e_1 \\ge e_2$$\n如果两个不等式都成立且至少有一个是严格的，则 $L_1$ 严格支配 $L_2$，并且可以剪除对应于 $L_2$ 的路径。\n\n**证明：**\n证明根植于 Bellman 原理。通过产生标签 $L_k$ 的路径到达目的地 $t$ 的总时间是 $\\ell_k + V(i, e_k)$，其中 $V(i, e_k)$ 是从状态 $(i, e_k)$ 出发的最优未来成本。\n价值函数 $V(i, e)$ 必须是能量 $e$ 的非增函数。这是因为拥有更多能量永远不会是劣势；它扩展了未来可行行动的集合，而在一个更广泛的选择集上取最小值只能更小或相等，即如果 $e_1 \\ge e_2$，则 $V(i, e_1) \\le V(i, e_2)$。\n给定支配条件 $\\ell_1 \\le \\ell_2$ 和 $e_1 \\ge e_2$，可得 $V(i, e_1) \\le V(i, e_2)$。因此：\n$$\\ell_1 + V(i, e_1) \\le \\ell_2 + V(i, e_2)$$\n这意味着从 $L_1$ 所代表的状态继续前进到 $t$ 的最佳可能路径保证不会比从 $L_2$ 出发的最佳路径差。因此，$L_2$ 是冗余的，可以被丢弃。\n\n**示例说明：**\n我们找两条从 $s$ 到节点 $D$ 的路径来说明支配关系。\n路径 1：$s \\to A \\to \\text{充电} \\to D$\n- $s \\to A$：到达 $A$ 时，时间 $\\ell = d_{sA} = 4$，能量 $e = 3 - c_{sA} = 3-2=1$。\n- 在 $A$ 充电：时间变为 $\\ell = 4 + r_A = 4+6=10$。能量变为 $e = E_{\\max} = 5$。\n- $A \\to D$：到达 $D$ 时，时间 $\\ell = 10 + d_{AD} = 10+5=15$，能量 $e = 5 - c_{AD} = 5-4=1$。这在节点 $D$ 产生标签 $L_1 = (15, 1)$。\n\n路径 2：$s \\to C \\to \\text{充电} \\to D$\n- $s \\to C$：到达 $C$ 时，时间 $\\ell = d_{sC} = 5$，能量 $e = 3 - c_{sC} = 3-2=1$。\n- 在 $C$ 充电：时间变为 $\\ell = 5 + r_C = 5+3=8$。能量变为 $e = E_{\\max} = 5$。\n- $C \\to D$：到达 $D$ 时，时间 $\\ell = 8 + d_{CD} = 8+2=10$，能量 $e = 5 - c_{CD} = 5-1=4$。这在节点 $D$ 产生标签 $L_2 = (10, 4)$。\n\n在节点 $D$ 比较 $L_1 = (15, 1)$ 和 $L_2 = (10, 4)$：\n我们观察到 $\\ell_2  \\ell_1$ ($10  15$) 且 $e_2 > e_1$ ($4 > 1$)。支配的两个条件都严格满足。因此，标签 $L_2$ 严格支配标签 $L_1$。对应于 $L_1$ 的路径是劣等的，可以被剪除。\n\n### 3. 最小总旅行时间的计算\n\n我们应用一个前向搜索算法（Dijkstra 算法的一个变体），从成本为 0 的状态 $(s, 3)$ 开始。我们维护一个按旅行时间 $\\ell$ 优先排序的标签 $(\\ell, e, i)$ 的优先队列。\n\n1.  初始化优先队列 `PQ`，加入 `(0, 3, s)`。初始化到 $t$ 的成本为 $\\infty$。\n2.  弹出 `(0, 3, s)`。\n    - $s \\to A$：可行 ($3 \\ge c_{sA}=2$)。在 $A$ 处的新标签：时间 $4$，能量 $1$。将 `(4, 1, A)` 推入队列。\n    - $s \\to B$：可行 ($3 \\ge c_{sB}=3$)。在 $B$ 处的新标签：时间 $3$，能量 $0$。将 `(3, 0, B)` 推入队列。\n    - $s \\to C$：可行 ($3 \\ge c_{sC}=2$)。在 $C$ 处的新标签：时间 $5$，能量 $1$。将 `(5, 1, C)` 推入队列。\n3.  弹出 `(3, 0, B)`。在能量为 0 的节点 $B$。所有从 $B$ 出发的弧都需要能量 $>0$。此路径是死路。\n4.  弹出 `(4, 1, A)`。在能量为 1 的节点 $A$。\n    - 移动：到 $C, D, t$ 的弧分别需要能量 $2, 4, 5$。都不可行。\n    - 充电：可能，因为 $1  E_{\\max}$。成本 $r_A=6$。在 $A$ 处的新标签：时间 $4+6=10$，能量 $5$。将 `(10, 5, A)` 推入队列。\n5.  弹出 `(5, 1, C)`。在能量为 1 的节点 $C$。\n    - 移动：\n        - $C \\to D$：可行 ($1 \\ge c_{CD}=1$)。在 $D$ 处的新标签：时间 $5+2=7$，能量 $1-1=0$。将 `(7, 0, D)` 推入队列。\n        - $C \\to t$：不可行 ($1  c_{Ct}=4$)。\n    - 充电：可能 ($1  E_{\\max}$) 。成本 $r_C=3$。在 $C$ 处的新标签：时间 $5+3=8$，能量 $5$。将 `(8, 5, C)` 推入队列。\n6.  弹出 `(7, 0, D)`。在能量为 0 的节点 $D$。\n    - 移动 $D \\to t$：不可行 ($0  c_{Dt}=2$)。死路。\n7.  弹出 `(8, 5, C)`。在能量为 5 的节点 $C$。\n    - 移动：\n        - $C \\to D$：可行 ($5 \\ge c_{CD}=1$)。在 $D$ 处的新标签：时间 $8+2=10$，能量 $5-1=4$。检查在 $D$ 的支配关系：当前标签是 $(7,0)$。$(10,4)$ 和 $(7,0)$ 均不支配对方。将 `(10, 4, D)` 推入队列。\n        - $C \\to t$：可行 ($5 \\ge c_{Ct}=4$)。在时间 $8+6=14$ 到达 $t$。这是一条有效路径。更新到 $t$ 的成本为 $14$。\n    - 充电：不可能（能量已满）。\n8.  弹出 `(10, 4, D)`。在能量为 4 的节点 $D$。\n    - 移动 $D \\to t$：可行 ($4 \\ge c_{Dt}=2$)。在时间 $10+3=13$ 到达 $t$。这比之前的路径更好。更新到 $t$ 的成本为 $13$。\n9.  弹出 `(10, 5, A)`。在能量为 5 的节点 $A$。\n    - 移动：\n        - $A \\to C$：可行 ($5 \\ge c_{AC}=2$)。在 $C$ 处的新标签：时间 $10+2=12$，能量 $5-2=3$。检查在 $C$ 的支配关系：现有标签为 $(5,1)$ 和 $(8,5)$。标签 $(8,5)$ 支配 $(12,3)$，因为 $812$ 且 $5>3$。剪除此路径。\n        - $A \\to D$：可行 ($5 \\ge c_{AD}=4$)。在 $D$ 处的新标签：时间 $10+5=15$，能量 $5-4=1$。检查在 $D$ 的支配关系：现有标签为 $(7,0)$ 和 $(10,4)$。标签 $(10,4)$ 支配 $(15,1)$，因为 $1015$ 且 $4>1$。剪除此路径。\n        - $A \\to t$：可行 ($5 \\ge c_{At}=5$)。在时间 $10+9=19$ 到达 $t$。这比当前的最小值 $13$ 要差。忽略。\n10. 优先队列现已为空。算法终止。\n\n最小总旅行时间是 $13$。最优路径是 $s \\to C \\to \\text{在 C 充电} \\to D \\to t$。\n- $s \\to C$：时间 $5$，到达时能量为 $3-2=1$。\n- 在 $C$ 充电：时间成本 $3$，总时间为 $5+3=8$，能量为 $5$。\n- $C \\to D$：时间成本 $2$，总时间为 $8+2=10$，到达时能量为 $5-1=4$。\n- $D \\to t$：时间成本 $3$，总时间为 $10+3=13$，到达时能量为 $4-2=2$。\n总时间为 $5+3+2+3=13$。", "answer": "$$\\boxed{13}$$", "id": "3101499"}, {"introduction": "库存管理对任何企业都是一个关键挑战，需要在持有库存的成本和缺货风险之间取得平衡。这个问题 [@problem_id:3101449] 介绍了一个经典的库存控制模型，它是动态规划的一个重要应用。通过为这个无限期随机问题构建贝尔曼方程，我们不仅能找到最优的订货策略，还将探索它如何导出一个著名且直观的策略结构，即 $(s,S)$ 策略。", "problem": "考虑一个带有缺货损失的离散时间库存控制问题。在每个周期开始时，系统观察到现有库存水平 $x \\in \\{0,1,\\dots,M\\}$。决策者选择一个订货量 $q \\in \\{0,1,\\dots,M-x\\}$，该订货量会立即收到。如果 $q>0$，则会产生固定的订货准备成本 $K$。周期内的需求 $D$ 是一个随机变量，其已知的概率质量函数支撑在 $\\{0,1,2,3\\}$ 上。任何超过现有库存的需求都会损失（没有缺货补货）。期末库存为 $\\max\\{0,x+q-D\\}$，下一个状态即为该期末库存。单周期成本包括：如果 $q>0$ 时的固定订货成本，每单位期末库存的持有成本 $h$，以及每单位未满足需求（缺货损失）的缺货惩罚成本 $p$。目标是最小化在折扣因子 $\\beta \\in (0,1)$ 下的无限期界预期折扣成本总和。\n\n从以下基本基础开始：\n- 马尔可夫决策过程 (MDP) 的定义：状态 $x$、行动 $q$、由当前状态和行动通过需求实现决定的状态转移，以及一个定义在状态、行动和需求上的单周期成本函数。\n- Bellman 最优性原理：一个最优策略具有这样的性质，即无论初始状态和决策如何，余下的决策对于由第一个决策导致的状态而言，也必须构成一个最优策略。\n\n任务：\n1. 推导由该 MDP 的 Bellman 最优性原理所蕴含的递归优化方程，将价值函数 $V(x)$ 表示为关于行动的最小值，其内容为预期单步成本加上折扣后的预期未来价值。不要使用任何快捷公式；直接从 MDP 定义和最优性原理推导该递归关系。\n2. 解释为什么在该问题的结构下，状态 $x$ 的任何最优决策都取决于订货后库存水平 $y=x+q$（而不直接取决于 $x$），除了可行性约束 $y \\ge x$。基于此，论证可以预期何种形式的结构性策略（例如，当发生订货时，订货至一个共同的目标水平）。\n3. 使用值迭代实现数值验证，为有限状态空间 $\\{0,1,\\dots,M\\}$ 计算一个最优平稳策略，行动如上所述。使用确定性的平局打破规则，当多个值达到最小成本时，选择最小的订货后水平 $y$ 作为最小化者。\n4. 将 $(s,S)$ 策略结构的验证定义如下：存在整数 $s \\in \\{0,1,\\dots,M\\}$ 和整数 $S \\in \\{0,1,\\dots,M\\}$，使得最优策略对于所有 $x \\le s$ 是“订货至 $S$”，对于所有 $x>s$ 是“不订货”。如果最优策略从不订货（即对所有 $x$ 均为“不订货”），则报告退化对 $(-1,-1)$。通过检查发生订货的状态集合是否恰好是 $\\{0,1,\\dots,s\\}$（对于某个 $s$），并且在所有订货状态中订货后水平是一个共同的 $S$，来数值检验该结构是否成立。\n\n使用以下需求分布、容量和参数测试套件：\n- 需求支撑集和概率：$D \\in \\{0,1,2,3\\}$，其中 $\\mathbb{P}(D=0)=0.1$, $\\mathbb{P}(D=1)=0.3, \\mathbb{P}(D=2)=0.3, \\mathbb{P}(D=3)=0.3$。\n- 容量：$M=10$。\n- 折扣因子：$\\beta=0.95$。\n- 测试用例（每个用例指定 $(K,h,p)$）：\n    1. 用例 A (理想情况): $K=2$, $h=0.5, p=3$。\n    2. 用例 B (订货准备成本边界情况): $K=0, h=0.5, p=3$。\n    3. 用例 C (高订货准备成本): $K=8, h=0.5, p=3$。\n\n你的程序必须：\n- 实现值迭代，为每个测试用例计算最优平稳策略。\n- 为每个用例验证如上定义的 $(s,S)$ 结构。\n- 生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对每个用例，输出三元组 $[\\text{holds}, s, S]$，其中 $\\text{holds}$ 是一个布尔值，表示 $(s,S)$ 结构是否成立，$s$ 和 $S$ 是整数（如果策略从不订货或结构不成立，则两者均使用 $-1$）。例如，一个有效的最终输出格式为 $[[\\text{True},3,7],[\\text{True},4,6],[\\text{False},-1,-1]]$。\n\n不涉及物理单位或角度单位。所有数值和参数均按给定值精确表示。", "solution": "该问题是一个离散时间、无限期界的库存控制问题，具有随机需求、固定的订货准备成本、线性的持有成本和缺货成本，以及一个折扣因子。目标是找到一个能够最小化预期总折扣成本的策略。该问题是良定的，并且是动态规划的一个经典应用，具体而言是使用值迭代算法求解马尔可夫决策过程 (MDP) 的 Bellman 方程。\n\n### 步骤 1：Bellman 方程的推导\n\n系统在每个周期开始时的状态是现有库存水平 $x \\in \\{0, 1, \\dots, M\\}$。决策者选择一个订货量 $q \\in \\{0, 1, \\dots, M-x\\}$。订货被接收后的库存水平为 $y = x+q$，其中 $y \\in \\{x, x+1, \\dots, M\\}$。\n\n给定初始库存 $x$、订货量 $q$ 和实现的需求 $D$，单周期成本由三部分组成：\n1.  如果下订单 ($q>0$)，则产生固定准备成本 $K$。这可以写成 $K \\cdot \\mathbf{1}\\{q>0\\}$，其中 $\\mathbf{1}(\\cdot)$ 是指示函数。\n2.  每单位期末库存的持有成本为 $h$。期末库存为 $\\max\\{0, x+q-D\\}$。成本为 $h \\cdot \\max\\{0, x+q-D\\}$。\n3.  每单位未满足需求的缺货惩罚成本为 $p$。未满足的需求为 $\\max\\{0, D-(x+q)\\}$。成本为 $p \\cdot \\max\\{0, D-(x+q)\\}$。\n\n总的单周期成本为 $C(x, q, D) = K \\cdot \\mathbf{1}\\{q>0\\} + h \\cdot \\max\\{0, x+q-D\\} + p \\cdot \\max\\{0, D-(x+q)\\}$。\n状态转移到下一周期的初始库存 $x' = \\max\\{0, x+q-D\\}$。\n\n令 $V(x)$ 为最优价值函数，表示从状态 $x$ 开始的最小预期无限期界折扣成本。根据 Bellman 最优性原理，$V(x)$ 必须满足 Bellman 方程，该方程指出一个状态的最优价值是在所有可能行动下，即时成本与折扣后预期未来成本之和的最小值。\n\n$$V(x) = \\min_{q \\in \\{0, 1, \\dots, M-x\\}} \\left\\{ \\mathbb{E}_D[C(x, q, D)] + \\beta \\mathbb{E}_D[V(x')] \\right\\}$$\n\n以订货后库存水平 $y=x+q$ 来构建决策更为方便。选择 $q \\in \\{0, 1, \\dots, M-x\\}$ 等价于选择 $y \\in \\{x, x+1, \\dots, M\\}$。指示函数变为 $\\mathbf{1}\\{y>x\\}$。\n\n我们定义 $L(y)$ 为给定订货后库存水平 $y$ 时的预期单周期持有成本和缺货成本：\n$$L(y) = \\mathbb{E}_D[h \\cdot \\max\\{0, y-D\\} + p \\cdot \\max\\{0, D-y\\}] = \\sum_{d \\in \\{0,1,2,3\\}} \\mathbb{P}(D=d) [h \\cdot \\max\\{0, y-d\\} + p \\cdot \\max\\{0, d-y\\}]$$\n从状态 $x$ 选择订货后水平 $y$ 的预期单周期成本为 $K \\cdot \\mathbf{1}\\{y>x\\} + L(y)$。\n\n折扣后的预期未来价值为：\n$$\\beta \\mathbb{E}_D[V(x')] = \\beta \\mathbb{E}_D[V(\\max\\{0, y-D\\})] = \\beta \\sum_{d \\in \\{0,1,2,3\\}} \\mathbb{P}(D=d) V(\\max\\{0, y-d\\})$$\n\n将这些代入 Bellman 方程，我们得到 $V(x)$ 的递归优化方程：\n$$V(x) = \\min_{y \\in \\{x, x+1, \\dots, M\\}} \\left\\{ K \\cdot \\mathbf{1}\\{y>x\\} + L(y) + \\beta \\sum_{d \\in \\{0,1,2,3\\}} \\mathbb{P}(D=d) V(\\max\\{0, y-d\\}) \\right\\}$$\n该方程构成了值迭代算法的基础。\n\n### 步骤 2：策略结构论证\n\n为了理解最优策略的结构，我们可以分析在给定状态 $x$ 下的决策过程。选择是在不订货 ($y=x$) 和订货 ($y>x$) 之间进行。我们可以通过分开这两种情况来重写 Bellman 方程。\n\n我们定义一个辅助函数 $G(y)$，它整合了以库存水平 $y$ 开始一个周期的所有相关成本：\n$$G(y) = L(y) + \\beta \\sum_{d \\in \\{0,1,2,3\\}} \\mathbb{P}(D=d) V(\\max\\{0, y-d\\})$$\n$G(y)$ 表示在任何订货决策之后，库存水平为 $y$ 时，预期即时持有/缺货成本与预期折扣未来成本之和。\n\n使用 $G(y)$，Bellman 方程简化为：\n$$V(x) = \\min_{y \\in \\{x, \\dots, M\\}} \\{ K \\cdot \\mathbf{1}\\{y>x\\} + G(y) \\}$$\n这可以表示为两个备选方案的比较：\n1.  **不订货 ($y=x$)：** 成本为 $G(x)$。\n2.  **订货 ($y>x$)：** 成本为 $K + G(y)$。为了找到订货的最小成本，我们必须选择最佳的订货后水平 $y^* \\in \\{x+1, \\dots, M\\}$。最小成本为 $K + \\min_{y' \\in \\{x+1, \\dots, M\\}} G(y')$。\n\n因此，决策规则是：\n$$V(x) = \\min \\left\\{ G(x), K + \\min_{y' \\in \\{x+1, \\dots, M\\}} G(y') \\right\\}$$\n如果 $G(x) > K + \\min_{y' \\in \\{x+1, \\dots, M\\}} G(y')$，则下订单。\n\n策略的结构取决于函数 $G(y)$ 的形状。在库存理论中，对于具有线性持有成本和缺货成本的问题，预期单周期成本函数 $L(y)$ 是凸函数。由此动态规划产生的价值函数 $V(x)$ 通常也是凸的。因此，函数 $G(y)$ 也是凸的（或者更正式地说，是 K-凸的，这一性质足以证明 $(s,S)$ 策略的最优性）。\n\n一个凸函数 $G(y)$ 具有一个全局最小值（或一个连续的最小值区间）。令 $S$ 为在所有可能水平上最小化 $G(y)$ 的库存水平，即 $S = \\arg\\min_{y \\in \\{0, \\dots, M\\}} G(y)$。这个水平 $S$ 代表了理想的目标库存，它在长期内最优地平衡了持有成本和缺货成本。值得注意的是，$S$ 与当前状态 $x$ 无关。\n\n如果要下订单，决策者旨在达到最有利的库存位置。假设 $x  S$，要达到的最佳订货后水平是 $S$，产生的最小订货成本为 $K+G(S)$。\n从状态 $x$（其中 $x", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the inventory control problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {'K': 2.0, 'h': 0.5, 'p': 3.0},\n        # Case B (boundary on setup cost)\n        {'K': 0.0, 'h': 0.5, 'p': 3.0},\n        # Case C (large setup cost)\n        {'K': 8.0, 'h': 0.5, 'p': 3.0},\n    ]\n\n    # Global parameters\n    M = 10\n    beta = 0.95\n    demand_values = np.array([0, 1, 2, 3])\n    demand_probs = np.array([0.1, 0.3, 0.3, 0.3])\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(M, case['K'], case['h'], case['p'], beta, demand_probs, demand_values)\n        results.append(result)\n\n    # Format output as specified: list of lists, with Python's default bool representation.\n    # e.g., [[True, 3, 7], [True, 4, 6], [False, -1, -1]]\n    final_output_str = f\"[{','.join(map(str, results))}]\"\n    print(final_output_str)\n\ndef _solve_case(M, K, h, p, beta, demand_probs, demand_values):\n    \"\"\"\n    Solves the MDP for a single set of parameters (K, h, p) using value iteration\n    and verifies the (s,S) policy structure.\n    \"\"\"\n    num_states = M + 1\n    num_demands = len(demand_values)\n\n    # 1. Pre-calculate L(y), the expected one-period holding/stockout cost.\n    L = np.zeros(num_states)\n    for y in range(num_states):\n        holding_costs = h * np.maximum(0, y - demand_values)\n        stockout_costs = p * np.maximum(0, demand_values - y)\n        L[y] = np.sum(demand_probs * (holding_costs + stockout_costs))\n\n    # 2. Value Iteration.\n    V = np.zeros(num_states)\n    tolerance = 1e-9\n    max_iter = 10000\n\n    for _ in range(max_iter):\n        V_new = np.zeros(num_states)\n        for x in range(num_states):\n            # Costs for each possible post-order level y = x\n            costs_per_y = np.full(num_states - x, np.inf)\n            \n            for i, y in enumerate(range(x, num_states)):\n                setup_cost = K if y  x else 0\n                \n                # Calculate expected future value\n                next_states = np.maximum(0, y - demand_values)\n                expected_future_V = np.sum(demand_probs * V[next_states])\n                \n                total_cost = setup_cost + L[y] + beta * expected_future_V\n                costs_per_y[i] = total_cost\n            \n            V_new[x] = np.min(costs_per_y)\n\n        if np.max(np.abs(V_new - V))  tolerance:\n            V = V_new\n            break\n        V = V_new\n\n    # 3. Extract the optimal stationary policy.\n    policy = np.zeros(num_states, dtype=int)\n    for x in range(num_states):\n        costs_per_y = np.full(num_states - x, np.inf)\n        \n        for i, y in enumerate(range(x, num_states)):\n            setup_cost = K if y  x else 0\n            next_states = np.maximum(0, y - demand_values)\n            expected_future_V = np.sum(demand_probs * V[next_states])\n            total_cost = setup_cost + L[y] + beta * expected_future_V\n            costs_per_y[i] = total_cost\n\n        # Tie-breaking rule: select smallest y that minimizes cost.\n        min_cost = np.min(costs_per_y)\n        min_indices = np.where(np.isclose(costs_per_y, min_cost))[0]\n        best_y_index = min_indices[0]\n        policy[x] = x + best_y_index\n\n    # 4. Verify the (s,S) structure.\n    ordering_states = {x for x in range(num_states) if policy[x]  x}\n    \n    if not ordering_states:\n        # Never-order policy is a degenerate (s,S) policy with s=-1.\n        return [True, -1, -1]\n\n    # Check for a single target level S.\n    target_levels = {policy[x] for x in ordering_states}\n    if len(target_levels) != 1:\n        return [False, -1, -1]\n    \n    S = target_levels.pop()\n    \n    # Check if ordering states form a contiguous block {0, 1, ..., s}.\n    s = max(ordering_states)\n    if ordering_states == set(range(s + 1)):\n        # By construction, non-ordering states xs will have policy[x]=x.\n        return [True, s, S]\n    else:\n        return [False, -1, -1]\n\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3101449"}]}