## 引言
在优化领域，非凸问题普遍存在于众多科学与工程挑战的核心，从机器学习模型的训练到复杂的[系统设计](@entry_id:755777)，它们因其众多的局部极小值和[鞍点](@entry_id:142576)而极具挑战性。直接求解这类问题往往非常困难，甚至在计算上是不可行的。凸凹过程（Convex-Concave Procedure, [CCP](@entry_id:196059)）为解决一大类被称为差分凸（Difference of Convex, DC）问题的[非凸优化](@entry_id:634396)提供了一个优雅而强大的迭代框架。其核心思想是将一个困难的非凸[函数分解](@entry_id:197881)为一个凸函数与另一个[凸函数](@entry_id:143075)之差，并通过迭代地解决一系列更容易处理的凸子问题来逼近原问题的解。

本文旨在系统地引导您掌握[CCP](@entry_id:196059)这一强大的优化工具。在第一章“原理与机制”中，我们将深入其算法核心，揭示其作为主化-最小化方法的本质，探讨[DC分解](@entry_id:634688)的艺术，并分析其收敛保证与潜在的局限性。随后，在第二章“应用与[交叉](@entry_id:147634)学科联系”中，我们将跨越理论，展示CCP如何在机器学习、[计算机视觉](@entry_id:138301)、信号处理等尖端领域中解决现实世界中的复杂问题，彰显其广泛的实用价值。最后，在“动手实践”部分，您将通过引导式的编码练习，从零开始实现[CCP](@entry_id:196059)算法，解决从稀疏性促进到处理算法失效的各类问题，从而将理论知识转化为实践能力。

## 原理与机制

本章深入探讨凸凹过程（Convex-Concave Procedure, CCP）的核心原理与内在机制。我们将从其作为一种“序列凸规划”方法的思想基础出发，系统性地构建 CCP 的算法框架。在此过程中，我们将阐明如何构造与选择[目标函数](@entry_id:267263)的差分凸（Difference of Convex, DC）分解，分析算法的收敛性质与局限性，并探讨处理约束、非[光滑性](@entry_id:634843)及[数值稳定性](@entry_id:146550)的实用扩展。本章旨在为读者提供一个关于 [CCP](@entry_id:196059) 的严谨、系统且深入的理解。

### 核心原理：主化-最小化框架

凸凹过程（CCP）是一种强大的算法，用于求解形如 $f(x) = g(x) - h(x)$ 的差分凸（DC）[优化问题](@entry_id:266749)，其中 $g(x)$ 和 $h(x)$ 均为凸函数。由于 $-h(x)$ 项的存在，目标函数 $f(x)$ 通常是非凸的，这使得直接优化变得困难。[CCP](@entry_id:196059) 的核心思想是**主化-最小化（Majorization-Minimization, MM）**。我们不去直接最小化困难的非[凸函数](@entry_id:143075) $f(x)$，而是构造一个更容易优化的**代理函数（surrogate function）**序列，并迭代地最小化这些代理函数。

构造代理函数的关键在于处理棘手的凹部分 $-h(x)$。由于 $h(x)$ 是一个凸函数，我们可以利用其一阶性质。对于一个在点 $x^k$ 可微的[凸函数](@entry_id:143075) $h(x)$，其一阶[泰勒展开](@entry_id:145057)是其全局的下界：
$$
h(x) \ge h(x^k) + \nabla h(x^k)^{\top}(x - x^k)
$$
这个不等式对于定义域内的所有 $x$ 都成立。如果 $h(x)$ 在 $x^k$ 处不可微，我们可以用其次梯度的任意元素 $s^k \in \partial h(x^k)$ 来代替梯度 $\nabla h(x^k)$，不等式依然成立：
$$
h(x) \ge h(x^k) + \langle s^k, x - x^k \rangle
$$
这个不等式意味着 $-h(x)$ 的值永远不会超过其线性近似的值，即 $-h(x) \le - \left( h(x^k) + \langle s^k, x - x^k \rangle \right)$。

基于此，在第 $k$ 次迭代，我们保持 $f(x)$ 中易于处理的凸部分 $g(x)$ 不变，而将复杂的凹部分 $-h(x)$替换为其在当前迭代点 $x^k$ 的线性[上界](@entry_id:274738)。这样，我们便构造了在点 $x^k$ 处的代理函数 $f_k(x)$：
$$
f_k(x) = g(x) - \left( h(x^k) + \langle s^k, x - x^k \rangle \right)
$$
这个代理函数 $f_k(x)$ 具备两个至关重要的性质：

1.  **主化性质**：$f_k(x)$ 是原函数 $f(x)$ 的一个全局上界，即对于所有 $x$，都有 $f_k(x) \ge f(x)$。这是因为 $f_k(x) - f(x) = h(x) - (h(x^k) + \langle s^k, x - x^k \rangle) \ge 0$。
2.  **接触性质**：在点 $x^k$ 处，代理函数与原函数的值相等，即 $f_k(x^k) = g(x^k) - h(x^k) = f(x^k)$。

例如，考虑一维函数 $f(x) = x^4 - 3x^2$ 在区间 $[-1, 1]$ 上的优化。我们可以将其分解为 $g(x)=x^4$ 和 $h(x)=3x^2$。在点 $x^k=0$ 处，$h(0)=0$ 且 $h'(0)=0$。因此，代理函数为 $S_0(x) = g(x) - (h(0) + h'(0)(x-0)) = x^4$。显然，$S_0(x) = x^4 \ge x^4 - 3x^2 = f(x)$，且 $S_0(0)=f(0)=0$ [@problem_id:3114708]。

这两个性质共同确保了 CCP 算法的**下降保证（descent guarantee）**。在每一次迭代中，我们通过最小化代理函数来确定下一个迭代点 $x^{k+1} = \arg\min_x f_k(x)$。由于 $x^{k+1}$ 是 $f_k(x)$ 的[最小值点](@entry_id:634980)，我们有 $f_k(x^{k+1}) \le f_k(x^k)$。结合上述两个性质，我们得到一个不等式链：
$$
f(x^{k+1}) \le f_k(x^{k+1}) \le f_k(x^k) = f(x^k)
$$
这个结论 $f(x^{k+1}) \le f(x^k)$ 意味着 CCP 算法生成的序列会使目标函数值单调不增。这个保证对于任何[次梯度](@entry_id:142710) $s^k \in \partial h(x^k)$ 的选择都成立，是该[算法鲁棒性](@entry_id:635315)的一个重要理论基石 [@problem_id:3114737]。

### [CCP](@entry_id:196059) 迭代方案

基于主化-最小化框架，[CCP](@entry_id:196059) 的迭代方案非常清晰。从一个初始点 $x^0$ 开始，算法重复以下步骤直至收敛：

1.  **构造代理函数**：在当前点 $x^k$ 处，选择一个[次梯度](@entry_id:142710) $s^k \in \partial h(x^k)$，并构造凸代理函数 $f_k(x) = g(x) - \left( h(x^k) + \langle s^k, x - x^k \rangle \right)$。
2.  **求解子问题**：求解一个凸[优化问题](@entry_id:266749)，找到代理函数的[最小值点](@entry_id:634980)，作为下一个迭代点：$x^{k+1} = \arg\min_x f_k(x)$。

由于 $g(x)$ 是凸函数，而对 $h(x)$ 的线性化部分是 $x$ 的[仿射函数](@entry_id:635019)，因此代理函数 $f_k(x)$ 是凸的。这意味着每一步的子问题都是一个凸[优化问题](@entry_id:266749)，可以使用成熟高效的凸优化算法来求解。

让我们通过一个具体的例子来演示这个过程 [@problem_id:3145092]。考虑一个二维DC问题，其中 $f = g - h$，函数 $g$ 和 $h$ 定义为：
$$
g(x) = \frac{1}{2}x^{\top}Qx + p^{\top}x, \quad h(x) = \frac{1}{2}\|Ax-b\|_{2}^{2}
$$
其中 $Q = \operatorname{diag}(2,4)$, $p = \begin{pmatrix}-2\\ 6\end{pmatrix}$, $A = \begin{pmatrix} 1  1\\ 0  2\end{pmatrix}$, $b = \begin{pmatrix} 1\\ -1\end{pmatrix}$。$g(x)$ 和 $h(x)$ 的[海森矩阵](@entry_id:139140)分别为 $Q$ 和 $A^{\top}A$，它们都是正定的，因此 $g$ 和 $h$ 都是[凸函数](@entry_id:143075)。

假设当前迭代点为 $x_0 = \begin{pmatrix}0 \\ 0\end{pmatrix}$。首先，我们计算 $h(x)$ 在 $x_0$ 处的线化所需的信息：
$$
h(x_0) = \frac{1}{2}\|A(0)-b\|_2^2 = \frac{1}{2}\|b\|_2^2 = 1
$$
$$
\nabla h(x) = A^{\top}(Ax-b) \implies \nabla h(x_0) = -A^{\top}b = \begin{pmatrix} -1 \\ 1 \end{pmatrix}
$$
接下来，构造代理函数 $\phi(x; x_0) = g(x) - (h(x_0) + \nabla h(x_0)^{\top}(x-x_0))$。我们只需最小化它关于 $x$ 的部分：
$$
x_1 = \arg\min_x \left( g(x) - \nabla h(x_0)^{\top}x \right)
$$
这个子问题的目标函数是 $ \frac{1}{2}x^{\top}Qx + p^{\top}x - \nabla h(x_0)^{\top}x $。这是一个强凸二次函数，其[最小值点](@entry_id:634980)可以通过令其梯度为零得到：
$$
Qx_1 + p - \nabla h(x_0) = 0 \implies Qx_1 = \nabla h(x_0) - p
$$
代入数值：
$$
\begin{pmatrix} 2  0 \\ 0  4 \end{pmatrix} x_1 = \begin{pmatrix} -1 \\ 1 \end{pmatrix} - \begin{pmatrix} -2 \\ 6 \end{pmatrix} = \begin{pmatrix} 1 \\ -5 \end{pmatrix}
$$
解得 $x_1 = \begin{pmatrix} 1/2 \\ -5/4 \end{pmatrix}$。这就是 CCP 的一次迭代。

与直接对整个非[凸函数](@entry_id:143075) $f(x)$ 进行线性化（例如，在序列凸规划 SCP 中）相比，[CCP](@entry_id:196059) 的策略更为精妙和稳健。一个经典的例子是优化 $f(x) = \frac{1}{2}x^2 - |x|$ [@problem_id:3114698]。在 $x_0=2$ 处，$f'(x_0)=1$，SCP 的子问题是最小化线性函数 $x-2$，这是一个无下界的问题，算法因此失败。然而，对于 CCP，我们分解 $g(x)=\frac{1}{2}x^2$ 和 $h(x)=|x|$。在 $x_0=2$ 处，$h(x_0)=2$，$h'(x_0)=1$。CCP 的子问题是最小化 $\frac{1}{2}x^2 - (2+1(x-2)) = \frac{1}{2}x^2 - x$，这是一个良定的强凸问题，其解为 $x_1=1$。这个对比鲜明地展示了 [CCP](@entry_id:196059) 通过保留 $g(x)$ 的凸结构从而保证子问题良定性的优越性。

### DC 分解的艺术

一个函数的 DC 分解 $f=g-h$ 并不是唯一的，而分解方式的选择对 CCP 算法的性能至关重要。

#### 创建 DC 分解

许多非凸函数虽然没有直接的 $g-h$ 形式，但可以通过简单的代数技巧转化为 DC 形式。一个典型的例子是包含[不定二次型](@entry_id:191588)的函数，例如 $f(x) = x^{\top}Qx + c^{\top}x + r$，其中 $Q$ 是一个不定的[对称矩阵](@entry_id:143130)（既有正[特征值](@entry_id:154894)也有负[特征值](@entry_id:154894)）。

我们可以通过“加减”一个足够强的凸二次项 $\alpha\|x\|^2 = \alpha x^{\top}Ix$ 来构造 DC 分解 [@problem_id:3163348]。具体来说，我们将 $f(x)$改写为：
$$
f(x) = (\alpha x^{\top}Ix + c^{\top}x + r) - (\alpha x^{\top}Ix - x^{\top}Qx)
$$
令 $g(x) = \alpha x^{\top}Ix + c^{\top}x + r$ 和 $h(x) = x^{\top}(\alpha I - Q)x$。为了使这个分解有效，即 $g(x)$ 和 $h(x)$ 都是[凸函数](@entry_id:143075)，它们的（二阶）海森矩阵必须是半正定的。
- $g(x)$ 的海森矩阵是 $2\alpha I$，当 $\alpha > 0$ 时，它是正定的。
- $h(x)$ 的[海森矩阵](@entry_id:139140)是 $2(\alpha I - Q)$。它为半正定的充要条件是其所有[特征值](@entry_id:154894)非负。设 $\lambda_i(Q)$ 是 $Q$ 的[特征值](@entry_id:154894)，则 $\alpha I - Q$ 的[特征值](@entry_id:154894)为 $\alpha - \lambda_i(Q)$。因此，必须满足 $\alpha - \lambda_i(Q) \ge 0$ 对所有 $i$ 成立，这等价于 $\alpha \ge \max_i \lambda_i(Q) = \lambda_{\max}(Q)$。

因此，只要选择的参数 $\alpha$ 大于等于 $Q$ 的最大[特征值](@entry_id:154894)，我们就能成功地将一个不定二次规划问题转化为一个 DC 规划问题。

#### 选择 DC 分解

既然分解不唯一，我们自然会问：是否存在“最优”的分解？虽然没有绝对的答案，但一个普遍的启发式法则是，我们希望**代理函数 $f_k(x)$ 尽可能地贴近原函数 $f(x)$**。回顾 $f_k(x) - f(x) = h(x) - (h(x^k) + \langle s^k, x - x^k \rangle)$，这个“间隙”完全由 $h(x)$ 的[非线性](@entry_id:637147)程度决定。因此，一个更“平坦”（即曲率更小）的 $h(x)$ 会产生更紧的代理函数，通常能带来更快的[收敛速度](@entry_id:636873)。

更正式地，算法的[收敛速度](@entry_id:636873)与 $h(x)$ 的梯度[利普希茨常数](@entry_id:146583) $L_h$ 和 $g(x)$ 的强凸模数 $\mu_g$ 的比值 $L_h / \mu_g$ 有关。一个更小的比值通常意味着更快的收敛。这启发我们寻找一种分解，使得 $h(x)$ 尽可能平坦（$L_h$ 小），同时 $g(x)$ 尽可能陡峭（$\mu_g$ 大）。

考虑一个复杂的目标函数 $f(x) = \|Ax-b\|_2^2 + \lambda\|x\|_1 - \mu\|x\|_2$ [@problem_id:3114692]。我们可以考虑以下几种分解：
1.  **自然分解**: $g_I(x) = \|Ax-b\|_2^2 + \lambda\|x\|_1$, $h_I(x) = \mu\|x\|_2$。
2.  **二次项调整分解**: 选择一个 $\tau > 0$，令 $g_{II}(x) = g_I(x) + \tau\|x\|_2^2$, $h_{II}(x) = h_I(x) + \tau\|x\|_2^2$。

通过分析，可以发现第二种分解的曲率比值为 $R_{II}(\tau) = \frac{L_{h_{I}} + 2\tau}{\mu_{g_{I}} + 2\tau}$。与第一种分解的比值 $R_I = \frac{L_{h_{I}}}{\mu_{g_{I}}}$ 相比，可以证明当且仅当 $\mu_{g_I}  L_{h_I}$ 时，存在一个 $\tau>0$ 使得 $R_{II}(\tau)  R_I$。这意味着，如果原始的 $g$ 部分不够“凸”，通过从 $h$ 向 $g$ “借调”一些曲率（通过加减 $\tau\|x\|_2^2$），我们可以改善收敛界，从而可能加速算法。这种对 DC 分解的精心设计是 [CCP](@entry_id:196059) 应用中的一门艺术。

### [收敛性分析](@entry_id:151547)：保证与局限

[CCP](@entry_id:196059) 的下降属性确保了[目标函数](@entry_id:267263)值 $f(x^k)$ 收敛。但是，迭代序列 $x^k$ 本身会收敛到哪里？

#### 稳定点

可以证明，如果 CCP 算法生成的序列 $x^k$ 收敛于一点 $x^*$，那么 $x^*$ 必定是原函数 $f(x)$ 的一个**[稳定点](@entry_id:136617)（stationary point）**。一个稳定点 $x^*$ 满足[一阶最优性条件](@entry_id:634945)，即 $0 \in \partial f(x^*)$。

在可微的情况下，这意味着 $\nabla f(x^*) = 0$。我们可以通过考察 [CCP](@entry_id:196059) 的[不动点](@entry_id:156394)来理解这一点。如果 $x^*$ 是 CCP 迭代的一个[不动点](@entry_id:156394)，那么 $x^* = \arg\min_x \{ g(x) - \nabla h(x^*)^{\top}x \}$。该子问题的[一阶最优性条件](@entry_id:634945)是 $\nabla g(x^*) - \nabla h(x^*) = 0$，这恰好是 $\nabla f(x^*) = 0$。

#### 局部性质与[鞍点](@entry_id:142576)

一个关键的警示是：**CCP 仅保证收敛到[稳定点](@entry_id:136617)，而不能保证收敛到局部[最小值点](@entry_id:634980)，更不用说[全局最小值](@entry_id:165977)点。** 收敛点可能是局部最小值、局部最大值，甚至是[鞍点](@entry_id:142576)。

考虑 DC 函数 $f(\mathbf{x})=g(\mathbf{x})-h(\mathbf{x})$，其中 $g(\mathbf{x})=\frac{1}{2}(x_1^2+x_2^2)$ 和 $h(\mathbf{x})=\frac{1}{4}x_1^2+x_2^2$ [@problem_id:3114758]。[CCP](@entry_id:196059) 的迭代映射为 $\mathbf{x}^{(k+1)} = \nabla h(\mathbf{x}^{(k)}) = (\frac{1}{2}x_1^{(k)}, 2x_2^{(k)})$。点 $\mathbf{x}^* = \mathbf{0}$ 是该映射的一个[不动点](@entry_id:156394)，也是 $f(x)$ 的一个[稳定点](@entry_id:136617)。然而，$f(x)$ 在原点处的[海森矩阵](@entry_id:139140)为：
$$
\nabla^2 f(\mathbf{0}) = \nabla^2 g(\mathbf{0}) - \nabla^2 h(\mathbf{0}) = \begin{pmatrix} 1  0 \\ 0  1 \end{pmatrix} - \begin{pmatrix} 1/2  0 \\ 0  2 \end{pmatrix} = \begin{pmatrix} 1/2  0 \\ 0  -1 \end{pmatrix}
$$
由于[海森矩阵](@entry_id:139140)是不定的（有一个正[特征值](@entry_id:154894)和一个负[特征值](@entry_id:154894)），$\mathbf{x}^* = \mathbf{0}$ 是 $f(x)$ 的一个[鞍点](@entry_id:142576)。如果从点 $\mathbf{x}^{(0)}=(1,0)$ 开始迭代，序列为 $\mathbf{x}^{(k)} = ( (1/2)^k, 0)$，它会收敛到这个[鞍点](@entry_id:142576)。这清晰地表明 CCP 算法可能会被吸引到[鞍点](@entry_id:142576)。

#### 收敛速率与发散风险

在某些情况下，[CCP](@entry_id:196059) 的迭代映射是线性的，形如 $x_{k+1} = Ax_k + b$。这种线性迭代的收敛性完全由[迭代矩阵](@entry_id:637346) $A$ 的**谱半径** $\rho(A)$（即其[特征值](@entry_id:154894)的最大[绝对值](@entry_id:147688)）决定。如果 $\rho(A)  1$，迭代收敛；如果 $\rho(A)  1$，迭代通常会发散。

对于前述的不定二次规划问题，可以推导出 [CCP](@entry_id:196059) 的[迭代矩阵](@entry_id:637346)为 $A = I - \frac{2}{2\alpha + \mu}Q$（这里加入了下一节将讨论的近端项 $\mu$）。矩阵 $A$ 的[特征值](@entry_id:154894)与 $Q$ 的[特征值](@entry_id:154894)直接相关。通过具体数值的例子可以构造出 $\rho(A)  1$ 的情况 [@problem_id:3163348]。这说明，即便是对于有效的 DC 分解，不经调整的 CCP 算法也可能是不稳定的。这促使我们必须考虑算法的改进与正则化。

### 实际实现与扩展

为了使 CCP 成为一个鲁棒且通用的工具，我们需要处理一些实际问题，如子问题的良定性、约束以及函数的非光滑性。

#### 使用近端正则化稳定算法

标准 [CCP](@entry_id:196059) 的一个潜在问题是，如果 $g(x)$ 不是强凸的，那么子问题 $\min_x f_k(x)$ 可能没有唯一解，甚至可能是无下界的。例如，在优化 $f(x)=x_1^2-\mu\|x\|_2$ 时，我们取 $g(x)=x_1^2$ 和 $h(x)=\mu\|x\|_2$。函数 $g(x)$ 在 $x_2$ 方向上是“平坦”的（曲率为零）。如果在 $x^k=0$ 处选择次梯度 $s^k = (0, \mu)^{\top} \in \partial h(0)$，那么 CCP 子问题是最小化 $x_1^2 - \mu x_2$，这在 $x_2$ 方向上是无界的 [@problem_id:3114696]。

一个标准的解决方案是在子问题中加入一个**近端正则化项（proximal regularization term）**：
$$
x^{k+1} = \arg\min_x \left\{ f_k(x) + \frac{\tau}{2}\|x - x^k\|^2 \right\}
$$
其中 $\tau0$ 是一个[正则化参数](@entry_id:162917)。这个二次项使得子问题的[目标函数](@entry_id:267263)具有了 $\tau$-强凸性，从而保证了解的唯一存在性。参数 $\tau$ 的选择提供了对[算法稳定性](@entry_id:147637)的控制。例如，在上述例子中，只要选取 $\tau \ge 1$，就可以保证子问题是 $1$-强凸的，从而解决了无界问题。

加入近端项后，子问题可以被重新解释。通过[配方法](@entry_id:265480)，最小化问题等价于求解 [@problem_id:3114679]：
$$
\min_x \left\{ g(x) + \frac{\tau}{2} \left\| x - \left(x^k + \frac{1}{\tau} s^k \right) \right\|^2 \right\}
$$
这可以看作一个两步过程：首先沿着 $h(x)$ 的（负）凹方向的梯度 $s^k$ 进行一次“上升”步骤得到一个中间点 $y^k = x^k + \frac{1}{\tau}s^k$，然后对 $g(x)$ 在该点 $y^k$ 处执行一个近端步骤。

#### 处理约束

[CCP](@entry_id:196059) 框架可以自然地扩展到[约束优化](@entry_id:635027)问题。

-   **[凸集](@entry_id:155617)约束**: 如果问题包含一个[凸集](@entry_id:155617)约束 $x \in \mathcal{X}$，我们只需将此约束加入到每一步的子问题中：
    $$
    x^{k+1} = \arg\min_{x \in \mathcal{X}} \left\{ f_k(x) + \frac{\tau}{2}\|x - x^k\|^2 \right\}
    $$
    由于子问题本身是凸的，增加一个凸集约束并不会改变其可解性。此时，[最优性条件](@entry_id:634091)会涉及到 $\mathcal{X}$ 在解 $x^{k+1}$ 处的**[法锥](@entry_id:272387)** $N_{\mathcal{X}}(x^{k+1})$ [@problem_id:3114679]。当 $g(x) \equiv 0$ 时，这个更新步骤就简化为在集合 $\mathcal{X}$ 上的一个投影操作。

-   **DC 约束**: [CCP](@entry_id:196059) 甚至可以处理形如 $c(x) := p(x) - q(x) \le 0$ 的 DC 约束，其中 $p, q$ 是凸函数。处理方法与[目标函数](@entry_id:267263)类似：在子问题中，将约束里的凹部分 $-q(x)$ 替换为其在 $x^k$ 处的线性上界。这样，每一步的子问题都是一个具有凸约束的凸规划。一个重要的理论结果是，通过这种方式得到的 [CCP](@entry_id:196059) 算法的任何[不动点](@entry_id:156394)，都满足原始非凸约束问题的 [Karush-Kuhn-Tucker (KKT) 条件](@entry_id:176491) [@problem_id:3114684]。这为我们使用 CCP 求解非凸约束问题提供了坚实的理论基础。

#### 处理非[光滑性](@entry_id:634843)

当 $h(x)$ 在某些点不可微时，其在该点的次梯度 $\partial h(x^k)$ 是一个集合而非单个向量。如前所述，CCP 的下降属性对于从 $\partial h(x^k)$ 中选取的**任何**[次梯度](@entry_id:142710) $s^k$ 都成立。然而，不同的选择会影响算法的实际行为。

考虑 $h(x)$ 是一个分段线性凸函数，例如 $h(x) = \max\{v^{\top}x, -v^{\top}x\}$。在 $v^{\top}x = 0$ 的“扭结”处，[次梯度](@entry_id:142710)是连接 $-v$ 和 $v$ 的线段。
-   如果选择**极端[次梯度](@entry_id:142710)**（如 $v$ 或 $-v$），CCP 的迭代步 $x^{k+1} = H^{-1}(s^k - b)$ （对于二次的 $g$）可能会在两个不同的点之间“之字形”摆动，导致收敛缓慢 [@problem_id:3114737]。
-   一个流行的启发式策略是选择**最小范数次梯度**。在上述例子中，扭结处的最小范数[次梯度](@entry_id:142710)是 $s^k=0$。这个选择消除了由极端[次梯度](@entry_id:142710)引入的方向偏差，通常能够产生更稳定的迭代路径，减少“之字形”行为，从而改善收敛性能。

总之，[CCP](@entry_id:196059) 是一个原理清晰、灵活且可扩展的[非凸优化](@entry_id:634396)框架。理解其作为 MM 算法的本质、DC 分解的艺术、收敛的局部性质以及处理各种实际复杂性的技术，是有效运用这一强大工具的关键。