{"hands_on_practices": [{"introduction": "本次练习将指导您从头开始实现凸凹过程（Convex-Concave Procedure, CCP）。您将从一个简单的一维非凸函数入手，学习创建差分凸（Difference-of-Convex, DC）分解的基本技巧。通过推导迭代更新规则并实现算法，您将亲身体验CCP的核心机制，并观察它如何在一个复杂的能量环境中寻找稳定点。[@problem_id:3114687]", "problem": "要求您为一个单变量凸函数差 (DC) 最小化问题实现凸-凹过程 (CCP)，其示例目标函数为 $f(x) = \\sin(x) + \\alpha x^2$，其中 $\\alpha \\ge 0$ 是一个参数。该任务包含三个部分：构造一个有效的 DC 分解，推导 CCP 更新规则，以及从不同的初始化点运行算法以研究收敛行为与初始曲率的关系。三角函数中使用的所有角度都必须以弧度为单位。\n\n请从以下基础知识开始：\n- 二阶导数判别法：一个二阶可微函数 $q(x)$ 是凸函数的充要条件是，对于其定义域中的所有 $x$，都有 $q''(x) \\ge 0$。\n- $\\sin(x)$ 的梯度是 $\\cos(x)$，其二阶导数是 $-\\sin(x)$。\n- 如果一个可微函数 $r(x)$ 的梯度是 $L$-利普希茨连续的，即对于所有 $x,y$ 都有 $\\|\\nabla r(x) - \\nabla r(y)\\| \\le L \\|x-y\\|$，那么函数 $x \\mapsto \\frac{L}{2}x^2 - r(x)$ 是凸函数，因为它的二阶导数的下界为 $0$。\n\n您的程序需要完成以下任务：\n1) 构造一个 DC 分解 $f(x) = g(x) - h(x)$，其中 $g(x)$ 和 $h(x)$ 均为凸函数。仅使用上述基础知识来证明您构造的合理性。\n2) 仅使用 CCP 最小化 DC 函数的定义——即在第 $k$ 次迭代时，通过对凹部进行线性化得到凸代理函数并将其最小化——为该单变量情况推导显式的迭代更新规则并实现它。您的实现必须是数值稳定的，具有基于决策变量容差的停止准则，并且必须通过最大迭代次数来限制迭代次数以避免不终止。\n3) 对于下述的每个测试用例，运行 CCP 并输出一个总结，该总结将观察到的收敛性与函数 $f$ 在起始点的初始曲率联系起来。具体来说，对每个用例计算：\n   - CCP 找到的计算出的驻点 $x^\\star$。\n   - 目标函数值 $f(x^\\star)$。\n   - 一个布尔值，指示二阶导数 $f''(x^\\star)  0$ 是否成立（在此解释为局部最小值的凭证）。\n   - 一个布尔值，指示算法是否在停止准则下收敛。\n   - 起始点 $x_0$ 处的初始曲率 $f''(x_0)$。\n   - 一个布尔值，指示初始曲率 $f''(x_0)  0$ 是否成立。\n   - 使用的迭代次数。\n\n您的程序必须使用的量定义如下：\n- 函数为 $f(x) = \\sin(x) + \\alpha x^2$。\n- 初始曲率为 $f''(x_0) = -\\sin(x_0) + 2\\alpha$。\n- 局部最小值凭证是 $f''(x^\\star)  0$ 的布尔值。\n- CCP 迭代值是通过在每次迭代中求解对凹部进行线性化而产生的凸子问题得到的。\n\n角度单位要求：\n- 所有三角函数计算必须使用弧度。\n\n测试套件：\n使用以下测试用例，每个用例由 $(\\alpha, x_0)$ 指定，并按前述方法计算与曲率相关的量和运行 CCP。\n- 用例 1：$\\alpha = 0.1$, $x_0 = 0.1$。\n- 用例 2：$\\alpha = 0.1$, $x_0 = 2.5$。\n- 用例 3：$\\alpha = 0.6$, $x_0 = -10.0$。\n- 用例 4：$\\alpha = 0.3$, $x_0 = 4.0$。\n- 用例 5：$\\alpha = 0.5$, $x_0 = \\frac{\\pi}{2}$。\n\n需要实现的算法细节：\n- 使用最小的有效曲率移位常数 $L$，使得您的 DC 分解对所有 $x \\in \\mathbb{R}$ 都有效。\n- 对两次迭代之间 $x$ 的绝对变化使用 $\\varepsilon = 10^{-10}$ 的停止容差。\n- 使用 10000 次的最大迭代上限。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应上面列出的一个测试用例。每个元素本身必须是包含以下 7 个量的列表，顺序为：$x^\\star$、 $f(x^\\star)$、局部最小值凭证、收敛布尔值、$f''(x_0)$、初始曲率正布尔值、迭代次数。例如，整体输出格式必须为 $[ [x^\\star_1,f(x^\\star_1),\\text{min}_1,\\text{conv}_1,f''(x_0^{(1)}),\\text{icurvpos}_1,\\text{iters}_1], \\ldots, [x^\\star_5,f(x^\\star_5),\\text{min}_5,\\text{conv}_5,f''(x_0^{(5)}),\\text{icurvpos}_5,\\text{iters}_5] ]$。", "solution": "用户提供的问题是有效的，因为它以优化理论的原理为科学基础，目标和约束明确，定义良好，并且没有任何矛盾或歧义。我们将继续提供完整的解决方案。\n\n该问题要求实现并分析凸-凹过程 (CCP)，以最小化给定的单变量目标函数 $f(x) = \\sin(x) + \\alpha x^2$（参数 $\\alpha \\ge 0$）。该过程涉及三个主要任务：构造一个有效的凸函数差 (DC) 分解，推导 CCP 迭代更新规则，以及实现该算法以研究其在指定测试用例上的收敛行为。\n\n### 第 1 部分：构造一个有效的 DC 分解\n\n目标函数为 $f(x) = \\sin(x) + \\alpha x^2$。一个 DC 分解将 $f(x)$ 表示为两个凸函数的差，即 $f(x) = g(x) - h(x)$。\n\n项 $\\alpha x^2$ 对于 $\\alpha \\ge 0$ 是凸的，因为它的二阶导数是 $2\\alpha \\ge 0$。项 $\\sin(x)$ 不是凸的，因为它的二阶导数 $-\\sin(x)$ 可以是负数。我们必须分解该函数来处理这个非凸部分。\n\n创建 DC 分解的一种标准方法是加上再减去一个足够大的二次项 $\\frac{L}{2}x^2$，以使各个分量变为凸函数。我们可以将 $f(x)$ 分解为：\n$$f(x) = \\left( \\alpha x^2 + \\frac{L}{2}x^2 \\right) - \\left( \\frac{L}{2}x^2 - \\sin(x) \\right)$$\n我们定义：\n$g(x) = (\\alpha + \\frac{L}{2})x^2$\n$h(x) = \\frac{L}{2}x^2 - \\sin(x)$\n\n要使其成为有效的 DC 分解， $g(x)$ 和 $h(x)$ 都必须是凸函数。\n\n1.  **$g(x)$ 的凸性：**\n    $g(x)$ 的二阶导数是 $g''(x) = 2(\\alpha + \\frac{L}{2}) = 2\\alpha + L$。为使 $g(x)$ 是凸函数，我们需要 $g''(x) \\ge 0$。由于 $\\alpha \\ge 0$ 且我们将选择一个非负常数 $L$，此条件将得到满足。\n\n2.  **$h(x)$ 的凸性：**\n    $h(x)$ 的二阶导数是 $h''(x) = L + \\sin(x)$。根据二阶导数判别法，$h(x)$ 是凸函数的充要条件是对于所有 $x \\in \\mathbb{R}$ 都有 $h''(x) \\ge 0$。这意味着对于所有 $x$ 都必须有 $L + \\sin(x) \\ge 0$。由于 $\\sin(x)$ 的最小值是 $-1$，该条件变为 $L - 1 \\ge 0$，即 $L \\ge 1$。\n\n问题要求使用最小的有效常数 $L$。因此，我们选择 $L = 1$。\n\n当 $L=1$ 时：\n- $g(x) = (\\alpha + \\frac{1}{2})x^2$ 是凸函数，因为对于 $\\alpha \\ge 0$，其二阶导数 $2\\alpha + 1 \\ge 1  0$。\n- $h(x) = \\frac{1}{2}x^2 - \\sin(x)$ 是凸函数，因为对于所有 $x$，其二阶导数 $1 + \\sin(x) \\ge 1 - 1 = 0$。\n\n因此，一个有效的 DC 分解是 $f(x) = g(x) - h(x)$，其中函数如上定义且 $L=1$。\n\n### 第 2 部分：推导 CCP 更新规则\n\nCCP 算法通过迭代求解一系列凸子问题来解决非凸问题 $\\min_x \\{g(x) - h(x)\\}$。在每次迭代 $k$ 中，给定当前迭代点 $x_k$，目标函数中的凹部 $-h(x)$ 被其在 $x_k$ 附近的一阶泰勒近似所替代。这就产生了一个需要最小化的凸代理函数。\n\n$h(x)$ 在 $x_k$ 处的线性化是 $h(x_k) + \\nabla h(x_k)(x - x_k)$。\n在第 $k+1$ 次迭代时的 CCP 子问题是：\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - \\left[ h(x_k) + \\nabla h(x_k)(x - x_k) \\right] \\right\\}$$\n由于项 $h(x_k)$ 和 $-\\nabla h(x_k)(-x_k)$ 相对于优化变量 $x$ 是常数，因此我们可以将问题简化为：\n$$x_{k+1} = \\arg\\min_x \\left\\{ g(x) - x \\nabla h(x_k) \\right\\}$$\n我们需要 $h(x)$ 的梯度：\n$\\nabla h(x) = \\frac{d}{dx} \\left(\\frac{1}{2}x^2 - \\sin(x)\\right) = x - \\cos(x)$。\n所以，$\\nabla h(x_k) = x_k - \\cos(x_k)$。\n\n该子问题的目标函数为：\n$$J(x) = g(x) - x \\nabla h(x_k) = \\left(\\alpha + \\frac{1}{2}\\right)x^2 - (x_k - \\cos(x_k))x$$\n这是一个关于 $x$ 的凸二次函数。为了找到其最小值点，我们将其关于 $x$ 的导数设为零：\n$$\\nabla J(x) = 2\\left(\\alpha + \\frac{1}{2}\\right)x - (x_k - \\cos(x_k)) = 0$$\n$$(2\\alpha + 1)x = x_k - \\cos(x_k)$$\n解出 $x$ 即可得到 $x_{k+1}$ 的显式迭代更新规则：\n$$x_{k+1} = \\frac{x_k - \\cos(x_k)}{2\\alpha + 1}$$\n由于 $\\alpha \\ge 0$，分母 $2\\alpha+1$ 总是大于或等于 $1$，因此该更新是良定义的。\n\n### 第 3 部分：实现与分析\n\n推导出的更新规则被实现为一个迭代算法。从一个初始点 $x_0$ 开始，生成序列 $\\{x_k\\}$，直到连续迭代点之间的绝对差 $|x_{k+1} - x_k|$ 小于指定的容差 $\\varepsilon = 10^{-10}$，或者达到 10000 次的最大迭代次数。\n\n对于每个由一对 $(\\alpha, x_0)$ 定义的测试用例，运行该算法以找到一个驻点 $x^\\star$。CCP 迭代的一个驻点是一个不动点，它必须满足 $x = \\frac{x - \\cos(x)}{2\\alpha+1}$。这可以简化为 $(2\\alpha+1)x = x-\\cos(x)$，或 $2\\alpha x + \\cos(x) = 0$。这个条件与将原始函数 $f(x)$ 的梯度 $f'(x) = 2\\alpha x + \\cos(x)$ 设为零是等价的。因此，CCP 算法正确地收敛到 $f(x)$ 的驻点。\n\n收敛后，为每个测试用例计算并报告以下量：\n- 计算出的驻点 $x^\\star$。\n- 驻点处的目标函数值 $f(x^\\star) = \\sin(x^\\star) + \\alpha(x^\\star)^2$。\n- 作为局部最小值凭证的布尔值，评估为 $f''(x^\\star)  0$。二阶导数为 $f''(x) = -\\sin(x) + 2\\alpha$。\n- 一个指示算法是否在最大迭代次数内收敛的布尔标志。\n- 起始点处的初始曲率 $f''(x_0) = -\\sin(x_0) + 2\\alpha$。\n- 一个指示 $x_0$ 处的初始曲率是否为正的布尔标志。\n- 执行的迭代次数。\n\n该实现将对所有指定的测试用例应用此过程，并按要求格式化结果。", "answer": "```python\nimport numpy as np\n\ndef run_ccp(alpha, x0, tol=1e-10, max_iter=10000):\n    \"\"\"\n    Implements the Convex-Concave Procedure (CCP) for the given objective.\n    \n    Args:\n        alpha (float): The parameter for the objective function.\n        x0 (float): The initial starting point.\n        tol (float): The tolerance for the stopping criterion.\n        max_iter (int): The maximum number of iterations.\n        \n    Returns:\n        list: A list containing the 7 required output quantities.\n    \"\"\"\n    x_k = x0\n    converged = False\n    iters = 0\n\n    for i in range(max_iter):\n        x_k_plus_1 = (x_k - np.cos(x_k)) / (2 * alpha + 1)\n        iters = i + 1\n\n        if np.abs(x_k_plus_1 - x_k)  tol:\n            converged = True\n            x_k = x_k_plus_1\n            break\n        \n        x_k = x_k_plus_1\n    \n    # In case of non-convergence, x_k is the last computed value.\n    if not converged:\n        iters = max_iter\n\n    x_star = x_k\n\n    # Calculate required metrics\n    f_x_star = np.sin(x_star) + alpha * x_star**2\n    f_double_prime_x_star = -np.sin(x_star) + 2 * alpha\n    is_local_min = bool(f_double_prime_x_star > 0)\n\n    f_double_prime_x0 = -np.sin(x0) + 2 * alpha\n    is_initial_curvature_pos = bool(f_double_prime_x0 > 0)\n\n    return [\n        x_star,\n        f_x_star,\n        is_local_min,\n        converged,\n        f_double_prime_x0,\n        is_initial_curvature_pos,\n        iters\n    ]\n\ndef solve():\n    \"\"\"\n    Runs the CCP solver for all test cases and prints the formatted output.\n    \"\"\"\n    test_cases = [\n        (0.1, 0.1),\n        (0.1, 2.5),\n        (0.6, -10.0),\n        (0.3, 4.0),\n        (0.5, np.pi / 2.0)\n    ]\n\n    results = []\n    for alpha, x0 in test_cases:\n        result = run_ccp(alpha, x0)\n        # Ensure booleans are lowercase 'true'/'false' for perfect format matching if needed,\n        # but Python's default `str(bool)` is 'True'/'False' and is acceptable.\n        # Example: list_str = f\"[{','.join(str(item).lower() if isinstance(item, bool) else str(item) for item in result)}]\"\n        results.append(result)\n\n    # Format the final output string according to the strict specification.\n    # The format requires a string representation of a list of lists with no spaces.\n    formatted_results = []\n    for case_result in results:\n        # Convert each item in the sublist to a string\n        str_items = [str(item) for item in case_result]\n        # Join them with commas\n        list_str = ','.join(str_items)\n        # Enclose in brackets\n        formatted_results.append(f\"[{list_str}]\")\n    \n    # Join the formatted sublists with commas\n    main_content = ','.join(formatted_results)\n    \n    # Enclose the entire string in brackets\n    final_output = f\"[{main_content}]\"\n    \n    print(final_output)\n\n# Run the solver\nsolve()\n\n```", "id": "3114687"}, {"introduction": "掌握了基础知识后，本次练习将CCP应用于机器学习中的一个常见问题：促进稀疏性。我们将处理一个非凸目标函数，该函数使用的惩罚项旨在产生比传统 $\\ell_1$ 范数更稀疏的解。通过构建CCP子问题，您将发现它与著名的LASSO问题之间富有洞察力的联系，并理解迭代过程如何影响解的稀疏性。[@problem_id:3114720]", "problem": "考虑非凸目标函数 $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 且 $\\mu  0$。您将使用差分凸规划（DC规划）和凸凹过程（CCP）的框架，构建一个在每次迭代中求解的凸代理子问题。\n\n从DC规划和$\\ell_{1}$范数次梯度的核心定义出发，首先通过引入一个辅助参数 $\\lambda  0$ 并将 $f$ 表示为两个凸函数之差，来构建 $f$ 的一个有效DC分解。然后，利用凸凹过程（CCP）的定义，在当前迭代点 $x^{k}$ 处通过一个次梯度将凹部线性化，以推导出用于生成 $x^{k+1}$ 的最小化的凸代理的显式形式。证明该代理子问题具有类似最小绝对收缩和选择算子（LASSO）的目标结构（二次损失加上一个$\\ell_{1}$惩罚项），并带有一个取决于所选次梯度的额外线性项。\n\n最后，对于具有以下条件的特定实例：\n- $A = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$，\n- $\\mu  0$，$\\lambda  0$，\n- 初始点 $x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，\n- 以及次梯度选择 $s^{0} \\in \\partial \\|x^{0}\\|_{1}$ 为 $s^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，\n\n以闭式解形式计算第一个CCP代理的精确最小化子 $x^{1}$。将您的最终答案表示为一个用 $\\mu$ 表示的、包含两个元素的单行矩阵。不需要四舍五入，也不涉及单位。\n\n除了计算之外，还需基于推导出的代理，解释相对于标准LASSO目标（不含该线性项），额外的线性项如何影响迭代中的稀疏模式。您的解释必须基于第一性原理推导，且不应依赖任何未引入的快捷公式。", "solution": "用户希望使用凸凹过程（CCP）来分析非凸目标函数 $f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$。这涉及三个主要部分：\n1.  推导CCP代理子问题的一般形式。\n2.  在给定特定初始点 $x^0$ 和次梯度选择的情况下，求解第一个迭代点 $x^1$。\n3.  解释代理中的额外线性项对解的稀疏性的作用。\n\n### 问题验证\n首先分析问题陈述。\n\n**第一步：提取已知条件**\n- 目标函数：$f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1}$\n- 矩阵 $A \\in \\mathbb{R}^{m \\times n}$\n- 常数 $\\mu  0$\n- 辅助参数 $\\lambda  0$\n- 迭代点：$x^k$ (当前)，$x^{k+1}$ (下一个)\n- 特定实例：\n    - $A = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$\n    - $\\mu  0$, $\\lambda  0$\n    - 初始点 $x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n    - 次梯度选择 $s^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\in \\partial \\|x^{0}\\|_{1}$\n\n**第二步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题基于凸分析与优化的既定原则，特别是差分凸（DC）规划和凸凹过程（CCP）。这些都是标准技术。所涉及的函数，即平方$\\ell_2$-范数和$\\ell_1$-范数，在数学及其应用中都是基础性的。\n- **良态的：** 问题陈述清晰。它要求推导一个代理函数，并为一个特定的、完全定义的实例计算下一个迭代点。待推导的子问题是凸的，这确保了最小化子的存在。\n- **客观性：** 问题是用精确的数学语言表述的，没有歧义或主观论断。\n\n必须对次梯度的选择进行验证。$\\|x\\|_1$在点$x$处的次梯度是向量$s$的集合，其分量为：如果$x_i \\neq 0$，则$s_i = \\text{sign}(x_i)$；如果$x_i = 0$，则$s_i \\in [-1, 1]$。对于$x^0 = (1, 0)^T$，我们有$x^0_1 = 1$和$x^0_2 = 0$。一个有效的次梯度$s^0$必须满足$s^0_1 = \\text{sign}(1) = 1$且$s^0_2 \\in [-1, 1]$。所给的选择$s^0 = (1, 0)^T$满足这些条件，因为$s^0_1=1$且$s^0_2=0 \\in [-1, 1]$。\n\n**第三步：结论与行动**\n问题被判定为**有效**。我们可以继续进行求解。\n\n### 第一部分：CCP代理子问题的推导\n\n问题的核心是在DC规划框架内表述$f(x)$的优化问题，这需要将$f(x)$表示为两个凸函数之差，即$f(x) = g(x) - h(x)$。问题指定通过引入辅助参数$\\lambda  0$来构造这种分解，并要求得到的子问题是一个带有额外线性项的类LASSO目标。\n\n标准的分解 $g(x) = \\|Ax\\|_2^2$ 和 $h(x) = \\mu\\|x\\|_1$ 无法产生所需的结构。我们必须找到一种分解，其中$\\ell_1$-范数保留在不被线性化的凸函数部分$g(x)$中。\n\n我们通过在目标函数中加上和减去项$\\lambda\\|x\\|_1$来引入$\\lambda  0$：\n$$f(x) = \\|A x\\|_{2}^{2} - \\mu \\|x\\|_{1} = (\\|A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}) - (\\lambda + \\mu)\\|x\\|_{1}$$\n我们定义两个函数，$g(x)$和$h(x)$：\n- $g(x) = \\|A x\\|_{2}^{2} + \\lambda \\|x\\|_{1}$\n- $h(x) = (\\lambda + \\mu)\\|x\\|_{1}$\n\n要使之成为一个有效的DC分解，$g(x)$和$h(x)$都必须是凸函数。\n- 函数$\\|A x\\|_{2}^{2}$是凸的。由于$\\lambda  0$，函数$\\lambda \\|x\\|_{1}$也是凸的。两个凸函数之和是凸的，因此$g(x)$是凸的。\n- 由于$\\lambda  0$且$\\mu  0$，系数$(\\lambda + \\mu)$是正的。一个凸函数（$\\|x\\|_1$）的正数倍仍然是凸的，因此$h(x)$是凸的。\n\n我们成功地将$f(x) = g(x) - h(x)$表示为两个凸函数之差。\n\n凸凹过程（CCP）通过在每一步最小化$f(x)$的一个凸代理来生成一个迭代序列$\\{x^k\\}$。该代理由围绕当前迭代点$x^k$线性化第二个凸函数$h(x)$而构建。根据凸性的定义，对于任何$x$和一个次梯度$s_h(x^k) \\in \\partial h(x^k)$，我们有以下不等式：\n$$h(x) \\ge h(x^k) + \\langle s_h(x^k), x - x^k \\rangle$$\n这意味着 $-h(x) \\le -h(x^k) - \\langle s_h(x^k), x - x^k \\rangle$。\n将此代入$f(x)$的表达式，得到一个上界（即代理函数）：\n$$f(x) = g(x) - h(x) \\le g(x) - (h(x^k) + \\langle s_h(x^k), x - x^k \\rangle) =: \\hat{f}(x; x^k)$$\n下一个迭代点$x^{k+1}$通过最小化这个代理函数找到：\n$$x^{k+1} = \\arg\\min_x \\hat{f}(x; x^k)$$\n项$h(x^k)$和$\\langle s_h(x^k), x^k \\rangle$相对于$x$是常数，因此可以从最小化问题中去掉。问题变为：\n$$x^{k+1} = \\arg\\min_x \\{ g(x) - \\langle s_h(x^k), x \\rangle \\}$$\n我们需要$h(x) = (\\lambda+\\mu)\\|x\\|_1$的次梯度。使用次梯度的缩放法则：\n$$\\partial h(x) = (\\lambda+\\mu) \\partial\\|x\\|_1$$\n令$s(x^k)$为在$x^k$处从$\\partial\\|x\\|_1$中选出的一个次梯度。那么我们可以设$s_h(x^k) = (\\lambda+\\mu)s(x^k)$。\n将$g(x)$和$s_h(x^k)$的表达式代入最小化问题，我们得到待最小化的代理目标的显式形式：\n$$x^{k+1} = \\arg\\min_x \\left\\{ \\|Ax\\|_2^2 + \\lambda \\|x\\|_1 - \\langle (\\lambda+\\mu)s(x^k), x \\rangle \\right\\}$$\n该目标具有LASSO问题的结构（一个二次损失$\\|Ax\\|_2^2$加上一个$\\ell_1$惩罚项$\\lambda\\|x\\|_1$），并带有一个额外的线性项$-\\left((\\lambda+\\mu)s(x^k)\\right)^T x$。\n\n### 第二部分：$x^1$的计算\n\n我们给定了特定的实例：\n- $A = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$，$x^0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，$s^0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n- 参数$\\mu$和$\\lambda$是正常数。\n\n我们需要计算$x^1 = \\arg\\min_x \\{ \\|Ax\\|_2^2 + \\lambda \\|x\\|_1 - ((\\lambda+\\mu)s^0)^T x \\}$。令$x = (x_1, x_2)^T$。\n\n需要最小化的目标函数$L(x)$是：\n$$L(x_1, x_2) = \\left\\| \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right\\|_2^2 + \\lambda \\left\\| \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right\\|_1 - (\\lambda+\\mu) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}^T \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\n展开各项：\n- $\\|Ax\\|_2^2 = x_1^2 + (2x_2)^2 = x_1^2 + 4x_2^2$。\n- $\\lambda\\|x\\|_1 = \\lambda(|x_1| + |x_2|)$。\n- $((\\lambda+\\mu)s^0)^T x = (\\lambda+\\mu)(1 \\cdot x_1 + 0 \\cdot x_2) = (\\lambda+\\mu)x_1$。\n\n目标函数就$x_1$和$x_2$是可分的：\n$$L(x_1, x_2) = \\left( x_1^2 + \\lambda|x_1| - (\\lambda+\\mu)x_1 \\right) + \\left( 4x_2^2 + \\lambda|x_2| \\right)$$\n我们可以独立地最小化这两个部分。\n\n**对$x_1$的最小化**：\n令$F_1(x_1) = x_1^2 - (\\lambda+\\mu)x_1 + \\lambda|x_1|$。我们使用次梯度最优性条件$0 \\in \\partial F_1(x_1)$来找到最小值。\n次梯度为$\\partial F_1(x_1) = 2x_1 - (\\lambda+\\mu) + \\lambda \\partial|x_1|$。\n- 如果$x_1  0$，$\\partial|x_1| = \\{1\\}$。导数为$2x_1 - (\\lambda+\\mu) + \\lambda = 2x_1 - \\mu$。令其为零得到$x_1 = \\frac{\\mu}{2}$。由于$\\mu  0$，此解在定义域$x_1  0$内，是一个候选解。\n- 如果$x_1  0$，$\\partial|x_1| = \\{-1\\}$。导数为$2x_1 - (\\lambda+\\mu) - \\lambda = 2x_1 - (2\\lambda+\\mu)$。令其为零得到$x_1 = \\frac{2\\lambda+\\mu}{2}$。由于$\\lambda, \\mu  0$，该值为正，与假设$x_1  0$矛盾。该定义域内没有解。\n- 如果$x_1 = 0$，$\\partial|x_1| = [-1, 1]$。次梯度集合为$\\partial F_1(0) = -(\\lambda+\\mu) + \\lambda[-1, 1] = [-(\\lambda+\\mu)-\\lambda, -(\\lambda+\\mu)+\\lambda] = [-(2\\lambda+\\mu), -\\mu]$。由于$\\mu  0$，该区间不包含0。所以$x_1=0$不是最小化子。\n唯一的最小化子是$x_1 = \\frac{\\mu}{2}$。\n\n**对$x_2$的最小化**：\n令$F_2(x_2) = 4x_2^2 + \\lambda|x_2|$。我们在$0 \\in \\partial F_2(x_2)$处寻找最小值。\n次梯度为$\\partial F_2(x_2) = 8x_2 + \\lambda \\partial|x_2|$。\n- 如果$x_2  0$，导数为$8x_2 + \\lambda$。令其为零得到$x_2 = -\\frac{\\lambda}{8}$，这与$x_2  0$矛盾（因为$\\lambda0$）。\n- 如果$x_2  0$，导数为$8x_2 - \\lambda$。令其为零得到$x_2 = \\frac{\\lambda}{8}$，这与$x_2  0$矛盾。\n- 如果$x_2 = 0$，次梯度集合为$\\partial F_2(0) = 8(0) + \\lambda[-1, 1] = [-\\lambda, \\lambda]$。由于$\\lambda  0$，该区间包含0。\n因此，唯一的最小化子是$x_2 = 0$。\n\n综合结果，最小化子是$x^1 = \\begin{pmatrix} \\frac{\\mu}{2} \\\\ 0 \\end{pmatrix}$。\n\n### 第三部分：线性项对稀疏性的影响\n\n代理子问题是最小化$J(x) = \\|Ax\\|_2^2 + \\lambda\\|x\\|_1 - c^T x$，其中$c = (\\lambda+\\mu)s(x^k)$。一个标准的LASSO目标函数缺少线性项$-c^T x$。我们来分析该项如何影响稀疏性。\n\n最小化子$x$的一阶最优性条件是$0 \\in \\partial J(x)$：\n$$0 \\in 2A^TAx + \\lambda \\partial\\|x\\|_1 - c$$\n这可以重写为$c \\in 2A^TAx + \\lambda \\partial\\|x\\|_1$。\n我们来考察解中分量$x_i$为零的条件。如果$x_i=0$，那么$\\partial\\|x\\|_1$的第$i$个分量是区间$[-1, 1]$。最优性条件的第$i$个分量变为：\n$$c_i \\in (2A^TAx)_i + \\lambda[-1, 1] \\quad \\text{其中 } x_i=0$$\n这等价于$|(2A^TAx)_i - c_i| \\le \\lambda$。在此表达式中，项$(2A^TAx)_i$表示解的其他非零分量的影响。作为比较，标准LASSO（其中$c=0$）的稀疏性条件是$|(2A^TAx)_i| \\le \\lambda$。\n\n向量$c$定义为$c = (\\lambda+\\mu)s(x^k)$，其中$s(x^k) \\in \\partial\\|x\\|_1|_{x^k}$。\n- 对于在前一个迭代点非零的分量$i$（$x_i^k \\ne 0$），我们有$s_i(x^k) = \\text{sign}(x_i^k)$。因此，$c_i = (\\lambda+\\mu)\\text{sign}(x_i^k)$。这是一个非零值，其大小为$\\lambda+\\mu$。在下一步中$x_i$变为零的条件是$|(2A^TAx)_i - (\\lambda+\\mu)\\text{sign}(x_i^k)| \\le \\lambda$。由于$|\\lambda+\\mu| > \\lambda$，项$(2A^TAx)_i$必须取一个特定的非零值才能满足该条件，这使得与标准LASSO情况相比，该分量变为零的可能性要小得多。这引入了一种“惯性”或“记忆”效应，即鼓励非零分量保持非零。\n\n- 对于在前一个迭代点为零的分量$j$（$x_j^k = 0$），我们可以选择任何$s_j(x^k) \\in [-1, 1]$。一个常见的选择是$s_j(x^k)=0$。在这种情况下，$c_j=0$，而$x_j$保持为零的条件是$|(2A^TAx)_j| \\le \\lambda$，这与标准LASSO的条件相同。\n\n总而言之，线性项$-c^T x$使得先前活跃（非零）的变量更难变得不活跃（零）。它激励新的迭代点$x^{k+1}$保持前一个次梯度$s(x^k)$的符号模式。这种“惯性”可能导致迭代点比通过求解标准LASSO问题所得到的解更不稀疏，因为它促进了活跃集在迭代间的持续存在。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\mu}{2}  0\n\\end{pmatrix}\n}\n$$", "id": "3114720"}, {"introduction": "每种算法都有其局限性，理解这些局限性是有效应用的关键。这最后一个练习将探讨一个更高级的场景，在该场景中，标准的CCP框架可能会失效，特别是在处理导数无界的非利普希茨（non-Lipschitz）函数时。您将诊断出失效的根源，然后实现一种称为“平滑”的强大技术来解决该问题，从而深入了解算法的理论假设以及在不满足这些假设时的实用补救措施。[@problem_id:3114699]", "problem": "考虑一个常用于诱导稀疏性的一维差分凸优化模型：最小化目标函数 $F(x) = f(x) - g(x)$，其中 $f$ 是凸函数，$g$ 也是凸函数，因此 $-g$ 是凹函数。凸凹过程 (CCP) 在第 $k$ 次迭代时，通过线性化凹部来构造一个凸代理函数：将 $-g(x)$ 替换为其在 $x^{(k)}$ 处的仿射上界，即使用 $-g(x^{(k)}) - s^{(k)}(x - x^{(k)})$，其中 $s^{(k)} \\in \\partial g(x^{(k)})$ (次微分)。最小化这个代理函数以产生下一个迭代点 $x^{(k+1)}$。\n\n您将分析当模型中引入非利普希茨罚项时 CCP 的行为。设 $p \\in (0,1)$，$b  0$ 且 $\\lambda  0$。考虑在非负实数轴上的非凸罚回归目标：\n$$\n\\min_{x \\geq 0} \\; F(x) = \\frac{1}{2}(x - b)^{2} + \\lambda h(x), \\quad \\text{其中 } h(x) = x^{p}.\n$$\n一种常见的差分凸分解是通过引入一个线性项 $c x$（其中 $c  0$）并将 $h(x)$ 写为 $h(x) = c x - \\big(c x - x^{p}\\big)$ 来获得，从而\n$$\nF(x) = \\underbrace{\\frac{1}{2}(x - b)^{2} + \\lambda c x}_{\\text{凸}} \\;-\\; \\underbrace{\\lambda\\big(c x - x^{p}\\big)}_{\\text{凸}}.\n$$\n回答下列问题：\n\n1) 仅使用凸分析的核心定义，证明虽然 $g(x) = \\lambda(c x - x^{p})$ 在 $(0,\\infty)$ 上是凸的，但从 $x^{(0)} = 0$ 开始的 CCP 步骤无法执行，因为凹部 $-g$ 在 $x^{(0)} = 0$ 处的仿射上界没有有限的斜率。精确地刻画当 $x \\downarrow 0$ 时右导数 $g^{\\prime}(x)$ 的特征，并解释为什么这会阻碍在 $x^{(0)} = 0$ 处的 CCP 线性化。\n\n2) 提出一种平滑化方法，该方法能抑制在 $x=0$ 处的非利普希茨行为，同时保持罚项的凹性。定义\n$$\nh_{\\epsilon}(x) = (x + \\epsilon)^{p} - \\epsilon^{p}, \\quad \\epsilon  0,\n$$\n并设 $g_{\\epsilon}(x) = \\lambda\\big(c x - h_{\\epsilon}(x)\\big)$。证明 $g_{\\epsilon}$ 在 $[0,\\infty)$ 上是凸的，并且对于任何固定的 $\\epsilon  0$， $g_{\\epsilon}$ 在 $x=0$ 处的所有单侧导数都是有限的，从而使得在 $x^{(0)} = 0$ 处的 CCP 线性化是良定义的。\n\n3) 为了量化平滑化对优化的影响，将注意力限制在定义域\n$$\n\\mathcal{D} = \\left[ r, \\infty \\right), \\quad \\text{其中 } r = \\big(\\lambda p(1 - p)\\big)^{\\frac{1}{2 - p}},\n$$\n并考虑平滑化后的目标\n$$\nF_{\\epsilon}(x) = \\frac{1}{2}(x - b)^{2} + \\lambda h_{\\epsilon}(x), \\quad x \\in \\mathcal{D}.\n$$\n设 $x^{\\star} \\in \\mathcal{D}$ 表示 $F$ 在 $\\mathcal{D}$ 上的唯一极小化子， $x_{\\epsilon} \\in \\mathcal{D}$ 表示 $F_{\\epsilon}$ 在 $\\mathcal{D}$ 上的唯一极小化子。仅使用基本事实（在 $\\mathcal{D}$ 上的凸性、一阶最优性和连续性论证），推导当 $\\epsilon \\to 0$ 时偏差大小\n$$\nB(\\epsilon) = |x_{\\epsilon} - x^{\\star}|\n$$\n的极限。\n\n你的最终答案必须是当 $\\epsilon \\to 0$ 时 $B(\\epsilon)$ 极限的单个值。本题不要求四舍五入，也没有物理单位。请精确表达最终答案。", "solution": "该问题是有效的。这是优化理论领域中的一组良定义的问题，具体涉及凸凹过程 (CCP) 和非利普希茨正则化子的性质。问题的前提在数学上是合理的，问题是客观的，可以通过严格的推导来回答。\n\n该问题分为三个部分。我们将按顺序逐一解答。\n\n1) 第一部分要求我们证明，对于目标函数 $F(x)$ 的指定差分凸 (DC) 分解，从 $x^{(0)} = 0$ 开始的 CCP 迭代无法执行。给定的 DC 分解是 $F(x) = f(x) - g(x)$，其中 $g(x) = \\lambda(c x - x^{p})$。CCP 算法要求在当前迭代点 $x^{(k)}$ 处通过线性化凹部 $-g(x)$ 来为 $F(x)$ 构造一个凸代理函数。这涉及到将 $-g(x)$ 替换为其一阶泰勒近似，由于 $g(x)$ 的凸性，该近似可作为全局上界。该近似由 $-g(x^{(k)}) - s^{(k)}(x - x^{(k)})$ 给出，其中 $s^{(k)}$ 必须是 $g$ 在 $x^{(k)}$ 处的次梯度，即 $s^{(k)} \\in \\partial g(x^{(k)})$。\n\n为了使这一步在 $x^{(0)} = 0$ 处是良定义的，次微分 $\\partial g(0)$ 必须非空。函数 $g(x) = \\lambda(c x - x^{p})$ 定义在 $[0, \\infty)$ 上，参数为 $\\lambda  0$，$c  0$ 和 $p \\in (0, 1)$。问题指出 $g(x)$ 在 $(0, \\infty)$ 上是凸的。我们可以通过检查其二阶导数来验证这一点：\n$$\ng'(x) = \\lambda(c - p x^{p-1})\n$$\n$$\ng''(x) = \\lambda(-p(p-1)x^{p-2}) = \\lambda p(1-p)x^{p-2}\n$$\n对于 $x  0$，并且给定 $\\lambda  0$ 和 $p \\in (0, 1)$，我们有 $\\lambda p(1-p)  0$ 和 $x^{p-2}  0$。因此，对于所有 $x \\in (0, \\infty)$，$g''(x)  0$，这证实了 $g(x)$ 在 $(0, \\infty)$ 上是严格凸的。\n\n现在，我们必须分析在边界点 $x=0$ 处的行为。对于定义在 $[0, \\infty)$ 上的凸函数，次梯度 $s \\in \\partial g(0)$ 必须是一个有限的实数。我们可以通过其单侧方向导数来刻画边界点处的次微分。$g(x)$ 在 $x=0$ 处的右导数定义为：\n$$\ng'_{+}(0) = \\lim_{h \\downarrow 0} \\frac{g(0+h) - g(0)}{h}\n$$\n由于 $g(0) = \\lambda(c \\cdot 0 - 0^p) = 0$，我们有：\n$$\ng'_{+}(0) = \\lim_{h \\downarrow 0} \\frac{\\lambda(ch - h^p)}{h} = \\lim_{h \\downarrow 0} \\lambda(c - h^{p-1})\n$$\n指数 $p-1$ 是负的，因为 $p \\in (0,1)$。设 $q = 1-p \\in (0,1)$。那么 $h^{p-1} = h^{-q} = \\frac{1}{h^q}$。当 $h \\downarrow 0$ 时，$h^q \\to 0$，因此 $\\frac{1}{h^q} \\to +\\infty$。\n该极限变为：\n$$\ng'_{+}(0) = \\lambda(c - \\lim_{h \\downarrow 0} h^{p-1}) = -\\infty\n$$\n如果方向导数是无穷大，则不可能存在有限的次梯度。次微分 $\\partial g(0)$ 是所有直线 $y=sx$ 的斜率 $s$ 的集合，这些直线是 $g(x)$ 的全局下估计量，即对于所有 $x \\ge 0$ 都有 $\\lambda(cx - x^p) \\ge sx$。对于 $x0$，此不等式等价于 $\\lambda c - \\lambda x^{p-1} \\ge s$。当 $x \\downarrow 0$ 时，左侧趋于 $-\\infty$。没有任何有限的 $s$ 能够对 $0$ 的任何右邻域中的所有 $x$ 满足这个不等式。因此，$\\partial g(0)$ 是空集。\n由于 $\\partial g(0)$ 是空集，因此不可能选择一个次梯度 $s^{(0)} \\in \\partial g(0)$。$-g(x)$ 在 $x=0$ 处的仿射上界的斜率（即 $-s^{(0)}$）不是有限的。因此，从 $x^{(0)} = 0$ 开始的 CCP 步骤无法执行。\n\n2) 第二部分引入了一个平滑化罚项来修正 $x=0$ 处的问题。平滑化罚项为 $h_{\\epsilon}(x) = (x + \\epsilon)^{p} - \\epsilon^{p}$，其中 $\\epsilon  0$。DC 分解中对应的凸部分是 $g_{\\epsilon}(x) = \\lambda\\big(c x - h_{\\epsilon}(x)\\big) = \\lambda\\big(c x - (x + \\epsilon)^{p} + \\epsilon^{p}\\big)$。我们需要证明 $g_{\\epsilon}(x)$ 在 $[0, \\infty)$ 上是凸的，并且它在 $x=0$ 处的导数是有限的。\n\n为了证明凸性，我们检查 $g_{\\epsilon}(x)$ 在 $x \\in [0, \\infty)$ 上的二阶导数：\n$$\ng'_{\\epsilon}(x) = \\frac{d}{dx} \\lambda\\big(c x - (x + \\epsilon)^{p} + \\epsilon^{p}\\big) = \\lambda\\big(c - p(x + \\epsilon)^{p-1}\\big)\n$$\n$$\ng''_{\\epsilon}(x) = \\frac{d}{dx} \\lambda\\big(c - p(x + \\epsilon)^{p-1}\\big) = \\lambda\\big(-p(p-1)(x + \\epsilon)^{p-2}\\big) = \\lambda p(1-p)(x+\\epsilon)^{p-2}\n$$\n对于任何 $x \\ge 0$ 和任何固定的 $\\epsilon  0$，我们有 $x+\\epsilon \\ge \\epsilon  0$。由于 $\\lambda  0$ 且 $p \\in (0, 1)$，因此 $\\lambda p(1-p)  0$。项 $(x+\\epsilon)^{p-2}$ 也是正的且良定义的。因此，对于所有 $x \\in [0, \\infty)$，$g''_{\\epsilon}(x)  0$。在这个区间上的正二阶导数意味着 $g_{\\epsilon}(x)$ 在 $[0, \\infty)$ 上是严格凸的。\n\n接下来，我们必须证明 $g_{\\epsilon}(x)$ 在 $x=0$ 处的单侧导数是有限的。由于 $g_{\\epsilon}(x)$ 在一个包含 $[0, \\infty)$ 的开区间（例如 $(-\\epsilon, \\infty)$）上是可微的，它在 $x=0$ 处的左导数和右导数都存在、相等，并且由导数值 $g'_{\\epsilon}(0)$ 给出。\n$$\ng'_{\\epsilon}(0) = \\lambda\\big(c - p(0 + \\epsilon)^{p-1}\\big) = \\lambda(c - p\\epsilon^{p-1})\n$$\n对于任何固定的 $\\epsilon  0$，$\\epsilon^{p-1}$ 是一个有限的正数。因此，$g'_{\\epsilon}(0)$ 是一个有限的实数。所以，在 $x=0$ 处的所有单侧导数都是有限的。这确保了次微分 $\\partial g_{\\epsilon}(0)$ 是非空单元素集 $\\{g'_{\\epsilon}(0)\\}$。因此，对于平滑化问题，在 $x^{(0)} = 0$ 处的 CCP 线性化是良定义的。\n\n3) 第三部分要求计算当 $\\epsilon \\to 0$ 时偏差大小 $B(\\epsilon) = |x_{\\epsilon} - x^{\\star}|$ 的极限。这里，$x^{\\star}$ 是 $F(x) = \\frac{1}{2}(x - b)^{2} + \\lambda x^{p}$ 在定义域 $\\mathcal{D} = [r, \\infty)$ 上的唯一极小化子，而 $x_{\\epsilon}$ 是平滑化目标 $F_{\\epsilon}(x) = \\frac{1}{2}(x - b)^{2} + \\lambda h_{\\epsilon}(x)$ 在同一定义域 $\\mathcal{D}$ 上的唯一极小化子。常数 $r$ 定义为 $r = \\big(\\lambda p(1 - p)\\big)^{\\frac{1}{2 - p}}$。\n\n首先，我们注意到定义域 $\\mathcal{D}$ 的选择使得 $F(x)$ 和 $F_{\\epsilon}(x)$ 在 $x \\in \\mathcal{D}$ 上都是凸的。对于 $F(x)$，其二阶导数为 $F''(x) = 1 + \\lambda p(p-1)x^{p-2} = 1 - \\lambda p(1-p)x^{p-2}$。对于 $x \\in [r, \\infty)$，我们有 $x \\ge r$，这意味着 $x^{2-p} \\ge r^{2-p} = \\lambda p(1-p)$。因此，$x^{p-2} = \\frac{1}{x^{2-p}} \\le \\frac{1}{\\lambda p(1-p)}$。这得到 $F''(x) = 1 - \\lambda p(1-p)x^{p-2} \\ge 1 - \\lambda p(1-p)\\frac{1}{\\lambda p(1-p)} = 0$。所以 $F(x)$ 在 $\\mathcal{D}$ 上是凸的。类似的论证表明 $F_\\epsilon(x)$ 在 $\\mathcal{D}$ 上是严格凸的。两个函数都是连续且强制的（即当 $x \\to \\infty$ 时，$F(x) \\to \\infty$ 且 $F_\\epsilon(x) \\to \\infty$），这保证了在闭集 $\\mathcal{D}$ 中极小化子 $x^{\\star}$ 和 $x_{\\epsilon}$ 的存在性和唯一性。\n\n为了找到 $|x_{\\epsilon} - x^{\\star}|$ 的极限，我们依赖于极小化子的连续性论证。让我们分析当 $\\epsilon \\to 0$ 时极小化子序列 $\\{x_{\\epsilon}\\}_{\\epsilon  0}$。该序列包含在 $\\mathcal{D}$ 中。可以证明它是有界的。根据 Bolzano-Weierstrass 定理，它必须包含一个收敛子序列。设 $\\{x_{\\epsilon_k}\\}_{k=1}^{\\infty}$ 是这样一个子序列，其中当 $k \\to \\infty$ 时 $\\epsilon_k \\to 0$，并设其极限为 $\\hat{x} = \\lim_{k\\to\\infty} x_{\\epsilon_k}$。由于对于所有 $k$，$x_{\\epsilon_k} \\in \\mathcal{D}$ 且 $\\mathcal{D}$ 是一个闭集，极限点 $\\hat{x}$ 也必须属于 $\\mathcal{D}$。\n\n根据 $x_{\\epsilon_k}$ 作为极小化子的定义，对于任何 $x \\in \\mathcal{D}$，我们有：\n$$\nF_{\\epsilon_k}(x_{\\epsilon_k}) \\le F_{\\epsilon_k}(x)\n$$\n我们取该不等式在 $k \\to \\infty$ 时的极限。函数 $F_{\\epsilon}(y)$ 在 $\\epsilon$ 和 $y$（对于 $y0$）上都是连续的。当 $k \\to \\infty$ 时，我们有 $\\epsilon_k \\to 0$ 和 $x_{\\epsilon_k} \\to \\hat{x}$。因此，左侧收敛：\n$$\n\\lim_{k\\to\\infty} F_{\\epsilon_k}(x_{\\epsilon_k}) = \\lim_{k\\to\\infty} \\left( \\frac{1}{2}(x_{\\epsilon_k} - b)^2 + \\lambda((x_{\\epsilon_k} + \\epsilon_k)^p - \\epsilon_k^p) \\right) = \\frac{1}{2}(\\hat{x} - b)^2 + \\lambda \\hat{x}^p = F(\\hat{x})\n$$\n对于任何固定的 $x \\in \\mathcal{D}$，右侧也收敛：\n$$\n\\lim_{k\\to\\infty} F_{\\epsilon_k}(x) = \\lim_{k\\to\\infty} \\left( \\frac{1}{2}(x - b)^2 + \\lambda((x + \\epsilon_k)^p - \\epsilon_k^p) \\right) = \\frac{1}{2}(x - b)^2 + \\lambda x^p = F(x)\n$$\n对不等式取极限得到：\n$$\nF(\\hat{x}) \\le F(x) \\quad \\text{对于所有 } x \\in \\mathcal{D}\n$$\n这是 $F(x)$ 在 $\\mathcal{D}$ 上的极小化子的定义。由于 $x^{\\star}$ 是 $F(x)$ 在 $\\mathcal{D}$ 上的唯一极小化子，我们必须有 $\\hat{x} = x^{\\star}$。\n\n这表明 $\\{x_{\\epsilon}\\}$ 的任何收敛子序列都必须收敛到 $x^{\\star}$。分析学中的一个标准结果指出，如果一个有界序列的每个子序列都有一个收敛到相同极限的更深子序列，那么原序列也收敛到该极限。因此，整个序列 $\\{x_{\\epsilon}\\}$ 收敛到 $x^{\\star}$：\n$$\n\\lim_{\\epsilon \\to 0} x_{\\epsilon} = x^{\\star}\n$$\n偏差大小的极限可以通过对 $B(\\epsilon)$ 的定义应用极限来找到：\n$$\n\\lim_{\\epsilon \\to 0} B(\\epsilon) = \\lim_{\\epsilon \\to 0} |x_{\\epsilon} - x^{\\star}|\n$$\n根据绝对值函数的连续性：\n$$\n\\lim_{\\epsilon \\to 0} |x_{\\epsilon} - x^{\\star}| = \\left| \\lim_{\\epsilon \\to 0} (x_{\\epsilon} - x^{\\star}) \\right| = \\left| \\left(\\lim_{\\epsilon \\to 0} x_{\\epsilon}\\right) - x^{\\star} \\right| = |x^{\\star} - x^{\\star}| = 0\n$$\n偏差大小的极限是 $0$。", "answer": "$$\n\\boxed{0}\n$$", "id": "3114699"}]}