{"hands_on_practices": [{"introduction": "理论是实践的基石。要真正掌握生成对抗网络，我们必须首先深入理解其核心——minimax博弈的目标函数。本练习将引导你通过分析一个常见的训练技巧——标签平滑（label smoothing），来从第一性原理推导出最优判别器$D^*(x)$的变化。通过这个过程，你将清晰地看到一个简单的目标调整如何从数学上改变博弈的均衡点，从而为理解和设计更稳定的GAN训练策略打下坚实的基础。[@problem_id:3124540]", "problem": "生成对抗网络（GAN）由一个生成器和一个判别器组成。生成器在数据空间上引出一个模型分布 $p_{g}(x)$，而判别器输出一个概率 $D(x) \\in (0,1)$，表示 $x$ 来自真实数据分布 $p_{\\text{data}}(x)$ 的可能性大小。判别器通过最小化二元交叉熵进行训练，这等价于在真实样本和生成样本的混合体上最大化目标的期望对数似然。在标准训练中，真实样本的目标为 $1$，生成样本的目标为 $0$。在标签平滑中，真实样本的目标被移至一个值 $s \\in (0,1)$，而假样本的目标保持在 $t=0$ 不进行平滑处理。\n\n假设判别器关于 $x$ 的目标是，通过从目标为 $s$ 的 $p_{\\text{data}}(x)$ 和目标为 $t=0$ 的 $p_{g}(x)$ 采样而形成的期望二元交叉熵。在连续设置下进行推导，其中 $p_{\\text{data}}$ 和 $p_{g}$ 是共同支撑集上的密度函数，并且期望是通过对 $x$ 积分来计算的。\n\n仅使用二元交叉熵的基本定义和GAN的极小极大公式，从第一性原理推导出：\n\n1) 在真实目标 $s=0.9$ 和假样本目标 $t=0$ 的标签平滑下，逐点最优判别器 $D^{*}(x)$，用 $p_{\\text{data}}(x)$ 和 $p_{g}(x)$ 表示。\n\n2) 在相同的标签平滑下，当生成器与数据匹配（即 $p_{g}(x) = p_{\\text{data}}(x)$）时，在极小极大均衡点上最大化的判别器目标。通过计算为 $s=0.9$ 和 $t=0$ 所获得的闭式表达式，将均衡值表示为一个实数。\n\n请将您的最终答案表示为一个双元素行向量 $\\big[D^{*}(x), V^{*}\\big]$，其中 $V^{*}$ 是均衡值。将数值均衡值四舍五入到四位有效数字。不需要单位。", "solution": "判别器 $D$ 的目标是最大化价值函数 $V(D,G)$，该函数表示正确识别真实样本和生成样本的对数似然。真实样本的训练目标被平滑为 $s$，生成样本的训练目标为 $t$。目标函数是真实数据分布 $p_{\\text{data}}(x)$ 和生成器分布 $p_g(x)$ 上的期望对数似然之和。\n\n对于给定的样本 $x$、判别器输出 $D(x)$ 和目标标签 $y$，其对数似然由二元交叉熵表达式给出：$y \\ln(D(x)) + (1-y) \\ln(1-D(x))$。\n\n总价值函数 $V(D)$ 通过对所有 $x$ 积分来构建：\n$$V(D) = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} [s \\ln(D(x)) + (1-s) \\ln(1-D(x))] + \\mathbb{E}_{x \\sim p_g(x)} [t \\ln(D(x)) + (1-t) \\ln(1-D(x))]$$\n在具有密度的连续设置下，这变为：\n$$V(D) = \\int p_{\\text{data}}(x) [s \\ln(D(x)) + (1-s) \\ln(1-D(x))] dx + \\int p_g(x) [t \\ln(D(x)) + (1-t) \\ln(1-D(x))] dx$$\n为了找到对于固定生成器能最大化此值的最优判别器 $D^*(x)$，我们可以对每个 $x$ 逐点最大化被积函数。令被积函数为 $f(D(x))$：\n$$f(D(x)) = p_{\\text{data}}(x)[s \\ln(D(x)) + (1-s) \\ln(1-D(x))] + p_g(x)[t \\ln(D(x)) + (1-t) \\ln(1-D(x))]$$\n我们收集包含 $\\ln(D(x))$ 和 $\\ln(1-D(x))$ 的项：\n$$f(D(x)) = [s \\cdot p_{\\text{data}}(x) + t \\cdot p_g(x)] \\ln(D(x)) + [(1-s) p_{\\text{data}}(x) + (1-t) p_g(x)] \\ln(1-D(x))$$\n该函数的形式为 $A \\ln(y) + B \\ln(1-y)$，其中 $y=D(x)$，且：\n$A = s \\cdot p_{\\text{data}}(x) + t \\cdot p_g(x)$\n$B = (1-s) p_{\\text{data}}(x) + (1-t) p_g(x)$\n\n为了求最大值，我们计算关于 $y$ 的导数并将其设为 $0$：\n$$\\frac{df}{dy} = \\frac{A}{y} - \\frac{B}{1-y} = 0 \\implies A(1-y) = By \\implies y = \\frac{A}{A+B}$$\n将 $A$ 和 $B$ 代回，最优判别器 $D^*(x)$ 为：\n$$D^*(x) = \\frac{s \\cdot p_{\\text{data}}(x) + t \\cdot p_g(x)}{(s \\cdot p_{\\text{data}}(x) + t \\cdot p_g(x)) + ((1-s) p_{\\text{data}}(x) + (1-t) p_g(x))}$$\n分母简化为：\n$$s \\cdot p_{\\text{data}}(x) + t \\cdot p_g(x) + p_{\\text{data}}(x) - s \\cdot p_{\\text{data}}(x) + p_g(x) - t \\cdot p_g(x) = p_{\\text{data}}(x) + p_g(x)$$\n因此，最优判别器的一般解为：\n$$D^*(x) = \\frac{s \\cdot p_{\\text{data}}(x) + t \\cdot p_g(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n\n**1) 特定标签平滑下的最优判别器**\n\n对于给定的具体情况，真实目标为 $s=0.9$，假样本目标为 $t=0$。将这些值代入 $D^*(x)$ 的通用公式中：\n$$D^*(x) = \\frac{0.9 \\cdot p_{\\text{data}}(x) + 0 \\cdot p_g(x)}{p_{\\text{data}}(x) + p_g(x)} = \\frac{0.9 \\cdot p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)}$$\n这就是逐点最优判别器的表达式。\n\n**2) 均衡状态下的最大化目标**\n\n在极小极大均衡点，生成器完美地模仿了真实数据分布，因此 $p_g(x) = p_{\\text{data}}(x)$。我们首先找到在该均衡点最优判别器的值：\n$$D^*_{\\text{eq}}(x) = \\frac{0.9 \\cdot p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_{\\text{data}}(x)} = \\frac{0.9 \\cdot p_{\\text{data}}(x)}{2 \\cdot p_{\\text{data}}(x)} = \\frac{0.9}{2} = 0.45$$\n在均衡状态下，无论输入 $x$ 是什么，最优判别器都输出一个恒定的概率 $0.45$。\n\n接下来，我们使用 $D(x) = D^*_{\\text{eq}}(x) = 0.45$、均衡条件 $p_g(x) = p_{\\text{data}}(x)$ 以及标签 $s=0.9, t=0$ 来评估价值函数 $V(D)$。令最大化的均衡值为 $V^*$。\n$$V^* = \\int p_{\\text{data}}(x) [0.9 \\ln(0.45) + (1-0.9) \\ln(1-0.45)] dx + \\int p_{\\text{data}}(x) [0 \\ln(0.45) + (1-0) \\ln(1-0.45)] dx$$\n$$V^* = \\int p_{\\text{data}}(x) [0.9 \\ln(0.45) + 0.1 \\ln(0.55)] dx + \\int p_{\\text{data}}(x) [\\ln(0.55)] dx$$\n由于方括号内的项是常数，它们可以从积分中提出。我们利用 $\\int p_{\\text{data}}(x) dx = 1$ 这一事实。\n$$V^* = [0.9 \\ln(0.45) + 0.1 \\ln(0.55)] \\int p_{\\text{data}}(x) dx + [\\ln(0.55)] \\int p_{\\text{data}}(x) dx$$\n$$V^* = (0.9 \\ln(0.45) + 0.1 \\ln(0.55)) \\cdot 1 + \\ln(0.55) \\cdot 1$$\n$$V^* = 0.9 \\ln(0.45) + 0.1 \\ln(0.55) + 1.0 \\ln(0.55)$$\n$$V^* = 0.9 \\ln(0.45) + 1.1 \\ln(0.55)$$\n现在，我们计算数值：\n$\\ln(0.45) \\approx -0.7985077$\n$\\ln(0.55) \\approx -0.5978370$\n$$V^* \\approx 0.9 \\times (-0.7985077) + 1.1 \\times (-0.5978370)$$\n$$V^* \\approx -0.71865693 - 0.65762070$$\n$$V^* \\approx -1.37627763$$\n四舍五入到四位有效数字，我们得到 $V^* = -1.376$。\n最终答案由 $D^*(x)$ 的表达式和 $V^*$ 的数值组成。", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{0.9 \\cdot p_{\\text{data}}(x)}{p_{\\text{data}}(x) + p_g(x)} & -1.376 \\end{pmatrix} } $$", "id": "3124540"}, {"introduction": "理解了目标函数后，下一步便是探索训练过程中的动态平衡。GAN的训练常被比作一场“猫鼠游戏”，生成器和判别器之间的学习速率必须精确协调，否则训练将很快发散。本练习提供了一个绝佳的编程实践机会：你将从零开始构建一个简单的GAN，并亲手实现其交替梯度更新的动态过程。通过系统性地改变判别器与生成器的更新频率比$k$，你将能够凭经验绘制出训练稳定性的“相图”，直观地揭示这一关键超参数对GAN收敛性的深刻影响。[@problem_id:3128933]", "problem": "您将进行一个简化的生成对抗网络（GAN）实验，旨在分析在单步生成器更新中交替进行 $k$ 步判别器更新，如何影响在一维高斯数据集上的训练稳定性。该实验基于第一性原理：一个生成器 $G$ 将标准正态噪声 $z \\sim \\mathcal{N}(0,1)$ 转换为 $x_g = a z + b$，一个判别器 $D$ 通过 $D(x) = \\sigma(w x + c)$ 来估计输入为真实样本的概率，其中 $\\sigma$ 是逻辑 sigmoid 函数。真实数据分布为 $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$。判别器最大化正确分类真实样本和伪造样本的期望对数似然，而生成器则基于判别器的输出最小化非饱和损失。\n\n请从深度学习和概率论中的以下基本依据和核心定义出发：\n- 生成对抗网络（GAN）的目标是生成器和判别器之间的双人极小极大博弈，由数据和噪声分布上的期望值驱动。\n- 基于梯度的学习动态使用蒙特卡洛采样来近似这些期望，并应用梯度上升进行最大化，应用梯度下降进行最小化。\n- 对于线性生成器 $G(z) = a z + b$，其中 $z \\sim \\mathcal{N}(0,1)$，生成器诱导的分布是均值为 $b$、标准差为 $|a|$ 的高斯分布。\n- 逻辑 sigmoid 函数为 $\\sigma(s) = \\frac{1}{1 + e^{-s}}$，计算梯度所需的导数恒等式在统计学和机器学习中已得到充分检验。\n\n您的任务：\n1. 构建交替梯度动态，其中对于生成器参数 $(a,b)$ 在其非饱和目标上进行的每一次梯度下降更新，判别器参数 $(w,c)$ 在其目标上进行 $k$ 次梯度上升更新。对双方使用固定批量大小和固定学习率的蒙特卡洛估计。通过控制输入域来确保 $\\sigma$ 的数值稳定性。\n2. 定义训练稳定性的经验性概念。使用以下复合准则：令 $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$，其中 $\\mu_g(t) = b(t)$ 和 $\\sigma_g(t) = |a(t)|$ 是生成器在训练步骤 $t$ 时的均值和标准差。定义改进分数 $I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)}$（对于一个小的 $\\epsilon$），并定义振荡指数 $O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon}$，其中 $\\theta_g(t) = [a(t), b(t)]^\\top$。如果参数保持有限且有界，$I$ 超过一个最小阈值，并且 $O$ 低于一个最大阈值，则将一次运行分类为稳定。\n3. 通过在一个简单数据集上使用多个 $k$ 值运行训练，凭经验推导出稳定性与 $k$ 值的相图，并报告每次运行是稳定还是不稳定，稳定编码为 $1$，不稳定编码为 $0$。\n\n使用以下数据集、训练和评估设置：\n- 真实数据参数 $(\\mu_r, \\sigma_r)$ 和调度参数 $k$ 因测试用例而异。\n- 噪声分布为 $z \\sim \\mathcal{N}(0,1)$。\n- 生成器为 $G(z) = a z + b$，初始化为 $a = 0.2$ 和 $b = 0.0$。\n- 判别器为 $D(x) = \\sigma(w x + c)$，$(w,c)$ 初始化为 $(0.0, 0.0)$。\n- 判别器目标是真实样本上的 $\\log D(x)$ 和伪造样本上的 $\\log(1 - D(G(z)))$ 的期望和；判别器使用梯度上升。\n- 生成器目标是 $- \\log D(G(z))$ 的期望值（非饱和）；生成器使用梯度下降。\n- 使用学习率 $\\alpha_D = 0.05$ 和 $\\alpha_G = 0.02$，批量大小 $B = 1024$，以及 $T = 200$ 个生成器步骤。\n- 为保证数值稳定性，在应用 $\\sigma(s)$ 之前，将 sigmoid 输入 $s$ 裁剪到区间 $[-50, 50]$。\n\n定义稳定性分类阈值和保障措施：\n- 使用 $\\epsilon = 10^{-8}$ 以稳定分母。\n- 如果任何参数的量级超过 $100$ 或在任何时候变成非数字（not-a-number），则声明为发散。\n- 使用阈值 $I_{\\min} = 0.25$ 和 $O_{\\max} = 2.5$。\n- 一次运行是稳定的，如果它不发散，$I \\ge I_{\\min}$，并且 $O \\le O_{\\max}$。\n\n测试套件：\n- 案例 $1$：$(\\mu_r, \\sigma_r, k, \\text{种子}) = (0.0, 1.0, 0, 42)$。\n- 案例 $2$：$(\\mu_r, \\sigma_r, k, \\text{种子}) = (0.0, 1.0, 1, 42)$。\n- 案例 $3$：$(\\mu_r, \\sigma_r, k, \\text{种子}) = (0.0, 1.0, 5, 42)$。\n- 案例 $4$：$(\\mu_r, \\sigma_r, k, \\text{种子}) = (2.0, 0.5, 2, 123)$。\n- 案例 $5$：$(\\mu_r, \\sigma_r, k, \\text{种子}) = (-1.0, 1.5, 10, 7)$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$ [r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是一个整数，$1$ 表示稳定运行，$0$ 表示不稳定运行。", "solution": "该问题要求对一个简化的生成对抗网络（GAN）的训练稳定性进行经验性研究，将其视为每个生成器更新所对应的判别器更新次数 $k$ 的函数。这包括推导基于梯度的学习动态、实现模拟，并根据一组指定的稳定性准则评估结果。\n\n首先，我们形式化模型的各个组成部分。真实数据分布是一维高斯分布，$x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$。生成器 $G$ 通过一个线性变换，将一个标准正态噪声变量 $z \\sim \\mathcal{N}(0,1)$ 映射到一个样本 $x_g$：\n$$G(z) = a z + b$$\n生成器的参数为 $\\theta_G = (a, b)$。因此，由生成器诱导的分布也是高斯分布，其均值为 $\\mu_g = b$，标准差为 $\\sigma_g = |a|$。判别器 $D$ 是一个逻辑回归器，它对输入 $x$ 来自真实数据分布的概率进行建模：\n$$D(x) = \\sigma(w x + c)$$\n其中 $\\sigma(s) = (1 + e^{-s})^{-1}$ 是逻辑 sigmoid 函数。判别器的参数为 $\\theta_D = (w,c)$。\n\n训练过程是一个极小极大博弈。训练判别器以最大化目标函数 $V_D$，该函数是正确分类真实样本和伪造样本的对数似然之和：\n$$V_D(\\theta_D, \\theta_G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z(z)}[\\log(1 - D(G(z)))]$$\n训练生成器以最小化非饱和目标函数 $V_G$，该函数旨在生成被判别器分类为真实的样本：\n$$V_G(\\theta_G) = -\\mathbb{E}_{z \\sim p_z(z)}[\\log D(G(z))]$$\n\n为了实现学习动态，我们推导这些目标函数关于模型参数的梯度。期望值通过对一个包含 $B$ 个样本的批量进行蒙特卡洛估计来近似。\n\n判别器目标 $V_D$ 关于其参数 $w$ 和 $c$ 的梯度是使用链式法则和 sigmoid 函数的导数性质求得的，具体来说是 $\\frac{d}{ds}\\log \\sigma(s) = 1 - \\sigma(s)$ 和 $\\frac{d}{ds}\\log(1-\\sigma(s)) = -\\sigma(s)$。对于一批真实数据 $\\{x_i\\}_{i=1}^B$ 和生成数据 $\\{x_{g,i}\\}_{i=1}^B$ 的梯度是：\n$$ \\nabla_w \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i))x_i - D(x_{g,i})x_{g,i} \\right) $$\n$$ \\nabla_c \\hat{V}_D = \\frac{1}{B} \\sum_{i=1}^B \\left( (1 - D(x_i)) - D(x_{g,i}) \\right) $$\n判别器参数通过梯度上升进行更新：\n$$ w \\leftarrow w + \\alpha_D \\nabla_w \\hat{V}_D $$\n$$ c \\leftarrow c + \\alpha_D \\nabla_c \\hat{V}_D $$\n\n生成器的非饱和目标 $V_G$ 关于其参数 $a$ 和 $b$ 的梯度是：\n$$ \\nabla_a V_G = \\frac{\\partial V_G}{\\partial a} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial a} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial a} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w z \\right] $$\n$$ \\nabla_b V_G = \\frac{\\partial V_G}{\\partial b} = -\\mathbb{E}_z\\left[ \\frac{\\partial}{\\partial b} \\log D(az+b) \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) \\frac{\\partial(w(az+b)+c)}{\\partial b} \\right] = -\\mathbb{E}_z\\left[ (1 - D(G(z))) w \\right] $$\n使用一批噪声 $\\{z_i\\}_{i=1}^B$ 进行近似，生成器参数通过梯度下降进行更新：\n$$ a \\leftarrow a - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i \\right) = a + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) z_i $$\n$$ b \\leftarrow b - \\alpha_G \\left( -\\frac{w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) \\right) = b + \\frac{\\alpha_G w}{B} \\sum_{i=1}^B (1 - D(G(z_i))) $$\n\n完整的训练算法会进行 $T$ 个生成器步骤。在每一步 $t \\in \\{1, \\dots, T\\}$ 中：\n1.  判别器更新 $k$ 次。对于每次判别器更新，都抽取新的一批真实数据 $x \\sim \\mathcal{N}(\\mu_r, \\sigma_r^2)$ 和噪声 $z \\sim \\mathcal{N}(0,1)$。使用学习率 $\\alpha_D$ 通过梯度上升更新参数 $(w, c)$。\n2.  生成器更新一次。抽取新的一批噪声 $z \\sim \\mathcal{N}(0,1)$。使用学习率 $\\alpha_G$ 通过在非饱和损失上进行梯度下降来更新参数 $(a, b)$。\n在整个模拟过程中，对参数值进行监控。如果任何参数的量级超过 $100$ 或变为非有限值（NaN），则终止运行并归类为发散。sigmoid 函数的输入被裁剪到 $[-50, 50]$ 以防止数值溢出。\n\n完成 $T$ 步后，评估该次训练运行的稳定性。生成器的分布参数 $(\\mu_g(t)=b(t), \\sigma_g(t)=|a(t)|)$ 与目标真实数据参数 $(\\mu_r, \\sigma_r)$ 之间的距离定义为 $d_t = \\sqrt{(\\mu_g(t) - \\mu_r)^2 + (\\sigma_g(t) - \\sigma_r)^2}$。计算两个指标：\n1.  改进分数 $I$，衡量从初始状态到最终状态的距离的相对减少量：\n    $$ I = \\frac{d_0 - d_T}{\\max(d_0, \\epsilon)} $$\n    其中 $d_0$ 和 $d_T$ 分别是步骤 $0$ 和 $T$ 时的距离，$\\epsilon = 10^{-8}$ 是一个用于数值稳定性的小常数。一次运行必须达到 $I \\ge I_{\\min} = 0.25$。\n2.  振荡指数 $O$，衡量生成器参数 $\\theta_g(t)=[a(t), b(t)]^\\top$ 的总路径长度与净位移的比率：\n    $$ O = \\frac{\\sum_{t=1}^T \\|\\theta_g(t) - \\theta_g(t-1)\\|_2}{\\|\\theta_g(T) - \\theta_g(0)\\|_2 + \\epsilon} $$\n    一次运行必须满足 $O \\le O_{\\max} = 2.5$。\n\n一次运行被分类为稳定（输出 $1$）当且仅当它不发散、达到 $I \\ge I_{\\min}$ 并且满足 $O \\le O_{\\max}$。否则，它是不稳定的（输出 $0$）。所提供的实现为每个测试用例执行这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy is not required for this problem.\n\ndef solve():\n    \"\"\"\n    Main function to run the GAN stability analysis for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # (mu_r, sigma_r, k, seed)\n        (0.0, 1.0, 0, 42),\n        (0.0, 1.0, 1, 42),\n        (0.0, 1.0, 5, 42),\n        (2.0, 0.5, 2, 123),\n        (-1.0, 1.5, 10, 7)\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_r, sigma_r, k, seed = case\n        result = run_training_and_evaluate(mu_r, sigma_r, k, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef sigmoid(s):\n    \"\"\"\n    Computes the logistic sigmoid function with input clipping for numerical stability.\n    \"\"\"\n    s_clipped = np.clip(s, -50.0, 50.0)\n    return 1.0 / (1.0 + np.exp(-s_clipped))\n\ndef run_training_and_evaluate(mu_r, sigma_r, k, seed,\n                              a_init=0.2, b_init=0.0,\n                              w_init=0.0, c_init=0.0,\n                              alpha_D=0.05, alpha_G=0.02,\n                              B=1024, T=200,\n                              epsilon=1e-8, divergence_threshold=100.0,\n                              I_min=0.25, O_max=2.5):\n    \"\"\"\n    Simulates the GAN training for one parameter set and evaluates its stability.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Initialize parameters\n    a, b = a_init, b_init\n    w, c = w_init, c_init\n\n    # History for stability metrics\n    theta_g_history = [np.array([a, b])]\n    diverged = False\n\n    # Calculate initial distance to target distribution parameters\n    d_0 = np.sqrt((b - mu_r)**2 + (np.abs(a) - sigma_r)**2)\n\n    # Main training loop (T generator steps)\n    for _ in range(T):\n        # --- Discriminator updates (k steps) ---\n        if k > 0:\n            for _ in range(k):\n                # Sample data\n                x_r = np.random.normal(loc=mu_r, scale=sigma_r, size=B)\n                z = np.random.normal(loc=0.0, scale=1.0, size=B)\n                x_g = a * z + b\n\n                # Discriminator predictions\n                d_real = sigmoid(w * x_r + c)\n                d_fake = sigmoid(w * x_g + c)\n                \n                # Discriminator gradients (for maximizing log-likelihood)\n                grad_w = np.mean((1.0 - d_real) * x_r - d_fake * x_g)\n                grad_c = np.mean((1.0 - d_real) - d_fake)\n\n                # Update D parameters via gradient ascent\n                w += alpha_D * grad_w\n                c += alpha_D * grad_c\n\n                # Check for D divergence\n                if not (np.isfinite(w) and np.isfinite(c) and abs(w) = divergence_threshold and abs(c) = divergence_threshold):\n                    diverged = True\n                    break\n            if diverged:\n                break\n\n        # --- Generator update (1 step) ---\n        z_g = np.random.normal(loc=0.0, scale=1.0, size=B)\n        x_g_g = a * z_g + b\n        d_fake_for_g = sigmoid(w * x_g_g + c)\n\n        # Generator gradients (for minimizing non-saturating loss -log(D(G(z))))\n        grad_V_G_a = -np.mean((1.0 - d_fake_for_g) * w * z_g)\n        grad_V_G_b = -np.mean((1.0 - d_fake_for_g) * w)\n        \n        # Update G parameters via gradient descent\n        a -= alpha_G * grad_V_G_a\n        b -= alpha_G * grad_V_G_b\n\n        # Check for G divergence\n        if not (np.isfinite(a) and np.isfinite(b) and abs(a) = divergence_threshold and abs(b) = divergence_threshold):\n            diverged = True\n            break\n            \n        theta_g_history.append(np.array([a, b]))\n\n    # --- Stability Evaluation ---\n    if diverged:\n        return 0\n\n    a_T, b_T = theta_g_history[-1]\n    \n    # Final distance\n    d_T = np.sqrt((b_T - mu_r)**2 + (np.abs(a_T) - sigma_r)**2)\n    \n    # Improvement Fraction (I)\n    I = (d_0 - d_T) / max(d_0, epsilon)\n\n    # Oscillation Index (O)\n    path_length = np.sum([np.linalg.norm(theta_g_history[t] - theta_g_history[t-1]) for t in range(1, T + 1)])\n    net_displacement = np.linalg.norm(theta_g_history[-1] - theta_g_history[0])\n    O = path_length / (net_displacement + epsilon)\n\n    # Classify as stable (1) or unstable (0)\n    if I >= I_min and O = O_max:\n        return 1\n    else:\n        return 0\n\nsolve()\n```", "id": "3128933"}, {"introduction": "当我们训练好一个GAN模型后，如何客观地评价其性能？这是一个核心且复杂的问题。Fréchet Inception Distance (FID) 是目前评估图像生成质量最主流的指标之一。本练习将带你深入FID的数学核心，首先要求你推导其在高斯特征假设下的闭式解，这能加深你对该度量背后最优传输理论的理解。更重要的是，练习的第二部分将挑战你构建一个特例，用以说明即使是FID这样先进的度量，在特定情况下也可能被“欺骗”，无法检测到诸如“模式坍塌”等严重问题，从而培养你对评估指标的批判性思维。[@problem_id:3128911]", "problem": "在训练生成对抗网络（GAN）时，一个广泛使用的评估指标是 Fréchet Inception 距离（FID），它在学习到的特征空间中比较真实数据和生成数据。令特征提取器为一个映射 $\\phi:\\mathcal{X}\\to\\mathbb{R}^d$，其中 $d\\in\\mathbb{N}$。假设真实图像和生成图像的特征分布分别由参数为 $(\\mu_r,\\Sigma_r)$ 和 $(\\mu_g,\\Sigma_g)$ 的多元正态分布近似，其中 $\\mu_r,\\mu_g\\in\\mathbb{R}^d$，$\\Sigma_r,\\Sigma_g\\in\\mathbb{R}^{d\\times d}$ 是对称半正定矩阵。Fréchet Inception 距离定义为这些高斯近似之间的平方 $2$-Wasserstein 距离，其中平方 $2$-Wasserstein 距离定义为\n$$\nW_2^2(P,Q)\\;=\\;\\inf_{\\gamma\\in\\Pi(P,Q)}\\;\\mathbb{E}_{(X,Y)\\sim\\gamma}\\big[\\|X-Y\\|_2^2\\big],\n$$\n其中 $\\Pi(P,Q)$ 是所有在 $\\mathbb{R}^d$ 上边缘分布为 $P$ 和 $Q$ 的耦合的集合。\n\n任务 (a)：仅使用上述 $W_2^2$ 的定义、多元正态分布的性质以及关于对称半正定矩阵（包括主平方根）的基本线性代数知识，推导 $W_2^2\\big(\\mathcal{N}(\\mu_r,\\Sigma_r),\\mathcal{N}(\\mu_g,\\Sigma_g)\\big)$ 关于 $\\mu_r,\\mu_g,\\Sigma_r,\\Sigma_g$ 的封闭形式表达式。\n\n任务 (b)：构建一个简单的特征空间，在该空间中 Fréchet Inception 距离可能具有误导性。考虑一维情况 $d=1$，图像空间为 $\\mathcal{X}=\\{-1,1\\}$，特征提取器 $\\phi:\\mathcal{X}\\to\\mathbb{R}$ 定义为 $\\phi(x)=|x|$。令真实数据分布在 $-1$ 和 $1$ 上的概率均为 $1/2$，并令生成器以概率 $1$ 确定性地输出 $1$。计算特征空间中得到的高斯参数 $(\\mu_r,\\Sigma_r)$ 和 $(\\mu_g,\\Sigma_g)$，然后对这些参数评估你在 (a) 部分推导出的平方 Fréchet Inception 距离。将最终值报告为一个精确的实数，无需四舍五入。", "solution": "该问题被验证为具有科学依据、定义明确且客观。它是机器学习理论和最优输运中的一个标准问题。所有必要的信息和定义都已提供。\n\n问题包括两部分。部分 (a) 要求推导两个多元正态分布之间平方 $2$-Wasserstein 距离的封闭形式表达式。部分 (b) 要求将此公式应用于一个特定的一维示例，以展示 Fréchet Inception 距离 (FID) 的一个潜在缺陷。\n\n**部分 (a)：两个高斯分布之间平方 $2$-Wasserstein 距离的推导**\n\n令两个多元正态分布为 $P = \\mathcal{N}(\\mu_r, \\Sigma_r)$ 和 $Q = \\mathcal{N}(\\mu_g, \\Sigma_g)$，其中 $\\mu_r, \\mu_g \\in \\mathbb{R}^d$ 是均值，$\\Sigma_r, \\Sigma_g \\in \\mathbb{R}^{d \\times d}$ 是对称半正定协方差矩阵。令 $X \\sim P$ 且 $Y \\sim Q$。\n\n平方 $2$-Wasserstein 距离定义为：\n$$ W_2^2(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\mathbb{E}_{(X,Y) \\sim \\gamma} \\left[ \\|X-Y\\|_2^2 \\right] $$\n其中 $\\Pi(P, Q)$ 是所有在 $\\mathbb{R}^d \\times \\mathbb{R}^d$ 上，边缘分布为 $P$ 和 $Q$ 的联合分布（耦合）$\\gamma$ 的集合。\n\n让我们将随机向量分解为其均值和零均值分量：$X = \\mu_r + X_0$ 和 $Y = \\mu_g + Y_0$，其中 $X_0 \\sim \\mathcal{N}(0, \\Sigma_r)$ 且 $Y_0 \\sim \\mathcal{N}(0, \\Sigma_g)$。期望项可以展开为：\n$$ \\|X-Y\\|_2^2 = \\|(\\mu_r - \\mu_g) + (X_0 - Y_0)\\|_2^2 = \\|\\mu_r - \\mu_g\\|_2^2 + \\|X_0 - Y_0\\|_2^2 + 2(\\mu_r - \\mu_g)^T(X_0 - Y_0) $$\n对任意耦合 $\\gamma$ 取期望：\n$$ \\mathbb{E}_{(X,Y)\\sim\\gamma}\\left[\\|X-Y\\|_2^2\\right] = \\|\\mu_r - \\mu_g\\|_2^2 + \\mathbb{E}_{(X_0,Y_0)\\sim\\gamma'}\\left[\\|X_0 - Y_0\\|_2^2\\right] + 2(\\mu_r - \\mu_g)^T(\\mathbb{E}[X_0] - \\mathbb{E}[Y_0]) $$\n由于 $X_0$ 和 $Y_0$ 是零均值的，第三项为 $2(\\mu_r - \\mu_g)^T(0 - 0) = 0$。$(X,Y)$ 的耦合为零均值分量 $(X_0, Y_0)$ 导出一个边缘分布为 $\\mathcal{N}(0, \\Sigma_r)$ 和 $\\mathcal{N}(0, \\Sigma_g)$ 的耦合 $\\gamma'$。项 $\\|\\mu_r - \\mu_g\\|_2^2$ 对于耦合的选择是常数。因此，下确界仅作用于第二项：\n$$ W_2^2(P,Q) = \\|\\mu_r - \\mu_g\\|_2^2 + \\inf_{\\gamma'} \\mathbb{E}_{(X_0,Y_0)\\sim\\gamma'}\\left[\\|X_0 - Y_0\\|_2^2\\right] $$\n让我们分析下确界内的项：\n$$ \\mathbb{E}\\left[\\|X_0 - Y_0\\|_2^2\\right] = \\mathbb{E}[X_0^T X_0] - 2\\mathbb{E}[X_0^T Y_0] + \\mathbb{E}[Y_0^T Y_0] $$\n对于一个协方差为 $\\Sigma$ 的零均值随机向量 $Z$，我们有 $\\mathbb{E}[Z^T Z] = \\mathbb{E}[\\text{Tr}(ZZ^T)] = \\text{Tr}(\\mathbb{E}[ZZ^T]) = \\text{Tr}(\\Sigma)$。\n因此，$\\mathbb{E}[X_0^T X_0] = \\text{Tr}(\\Sigma_r)$ 且 $\\mathbb{E}[Y_0^T Y_0] = \\text{Tr}(\\Sigma_g)$。这些项与耦合无关。问题简化为：\n$$ \\inf_{\\gamma'} \\mathbb{E}\\left[\\|X_0 - Y_0\\|_2^2\\right] = \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\sup_{\\gamma'} \\mathbb{E}[X_0^T Y_0] $$\n对于高斯测度，最优输运方案（即达到上确界的耦合）是由一个线性变换导出的。我们寻找一个矩阵 $A$，使得如果 $X_0 \\sim \\mathcal{N}(0, \\Sigma_r)$，那么设置 $Y_0 = AX_0$ 会导致 $Y_0$ 具有正确的边缘分布 $\\mathcal{N}(0, \\Sigma_g)$。$Y_0$ 的协方差是 $\\text{Cov}(AX_0) = A \\text{Cov}(X_0) A^T = A\\Sigma_rA^T$。因此我们需要 $A\\Sigma_rA^T = \\Sigma_g$。\n需要最大化的项是 $\\mathbb{E}[X_0^T Y_0] = \\mathbb{E}[X_0^T A X_0]$。对于一个零均值向量 $Z$，使用迹的性质 $\\mathbb{E}[Z^T M Z] = \\text{Tr}(M \\text{Cov}(Z))$，我们得到：\n$$ \\mathbb{E}[X_0^T A X_0] = \\text{Tr}(A \\Sigma_r) $$\n所以，我们必须求解 $\\sup_{A: A\\Sigma_rA^T=\\Sigma_g} \\text{Tr}(A\\Sigma_r)$。\n让我们暂时假设 $\\Sigma_r$ 是可逆的。那么 $\\Sigma_r^{1/2}$ 是其唯一的对称正定平方根。令 $B = A \\Sigma_r^{1/2}$，所以 $A = B \\Sigma_r^{-1/2}$。约束变为 $B \\Sigma_r^{-1/2} \\Sigma_r (\\Sigma_r^{-1/2})^T B^T = BB^T = \\Sigma_g$。我们需要最大化 $\\text{Tr}(B\\Sigma_r^{-1/2}\\Sigma_r) = \\text{Tr}(B\\Sigma_r^{1/2})$。\n满足 $BB^T = \\Sigma_g$ 的任何矩阵 $B$ 都可以写成 $B = \\Sigma_g^{1/2}O$，其中 $O$ 是一个正交矩阵。我们想要最大化 $\\text{Tr}(\\Sigma_g^{1/2} O \\Sigma_r^{1/2}) = \\text{Tr}(O \\Sigma_r^{1/2} \\Sigma_g^{1/2})$。\n令 $M = \\Sigma_r^{1/2} \\Sigma_g^{1/2}$。令其奇异值分解为 $M = USV^T$。我们想要最大化 $\\text{Tr}(O U S V^T) = \\text{Tr}(V^T O U S)$。矩阵 $O' = V^T O U$ 是正交的。当 $O'=I$ 时，该表达式最大化，最大值为 $\\text{Tr}(S)$。$S$ 中的奇异值是 $M^T M = (\\Sigma_g^{1/2}\\Sigma_r^{1/2})(\\Sigma_r^{1/2}\\Sigma_g^{1/2}) = \\Sigma_g^{1/2} \\Sigma_r \\Sigma_g^{1/2}$ 的特征值的平方根。或者，它们是 $MM^T = \\Sigma_r^{1/2}\\Sigma_g\\Sigma_r^{1/2}$ 的特征值的平方根。这些奇异值的对角矩阵的迹是 $\\text{Tr}(S) = \\text{Tr}\\left(\\left( \\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} \\right)^{1/2}\\right)$。通过连续性论证，这个结果即使对于奇异的 $\\Sigma_r$ 也成立。\n\n综合所有部分：\n$$ \\sup_{\\gamma'} \\mathbb{E}[X_0^T Y_0] = \\text{Tr}\\left(\\left( \\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} \\right)^{1/2}\\right) $$\n所以，平方 $2$-Wasserstein 距离的完整表达式是：\n$$ W_2^2(\\mathcal{N}(\\mu_r,\\Sigma_r), \\mathcal{N}(\\mu_g,\\Sigma_g)) = \\|\\mu_r - \\mu_g\\|_2^2 + \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\text{Tr}\\left( \\left(\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2}\\right)^{1/2} \\right) $$\n\n**部分 (b)：针对特定示例的计算**\n\n我们给定一个一维情况 ($d=1$)：\n- 图像空间 $\\mathcal{X} = \\{-1, 1\\}$。\n- 特征提取器 $\\phi: \\mathcal{X} \\to \\mathbb{R}$ 定义为 $\\phi(x) = |x|$。\n- 真实数据分布 $P_r$：$P_r(X=-1) = \\frac{1}{2}$，$P_r(X=1) = \\frac{1}{2}$。\n- 生成数据分布 $P_g$：一个确定性的输出 $1$。\n\n首先，我们计算特征分布及其参数 $(\\mu, \\Sigma)$。由于 $d=1$，$\\Sigma$ 是一个表示方差的 $1 \\times 1$ 矩阵，即 $\\Sigma = [\\sigma^2]$。\n\n**真实数据特征：**\n真实数据点 $x_r$ 可以是 $-1$ 或 $1$。对应的特征值是 $z_r = \\phi(x_r) = |x_r|$。\n- 如果 $x_r = -1$，$z_r = |-1| = 1$。\n- 如果 $x_r = 1$，$z_r = |1| = 1$。\n真实特征的分布是确定性的：它总是 $1$。\n真实特征的均值是 $\\mu_r = \\mathbb{E}[Z_r] = \\frac{1}{2} \\cdot \\phi(-1) + \\frac{1}{2} \\cdot \\phi(1) = \\frac{1}{2} \\cdot 1 + \\frac{1}{2} \\cdot 1 = 1$。\n真实特征的方差是 $\\sigma_r^2 = \\mathbb{E}[(Z_r - \\mu_r)^2] = \\mathbb{E}[(1-1)^2] = 0$。\n所以，$\\mu_r = 1$ 且 $\\Sigma_r = [0]$。\n\n**生成数据特征：**\n生成器确定性地输出 $x_g=1$。对应的特征是 $z_g = \\phi(1) = |1| = 1$。\n生成特征的分布也是确定性的：它总是 $1$。\n生成特征的均值是 $\\mu_g = \\mathbb{E}[Z_g] = 1$。\n生成特征的方差是 $\\sigma_g^2 = \\mathbb{E}[(Z_g - \\mu_g)^2] = \\mathbb{E}[(1-1)^2] = 0$。\n所以，$\\mu_g = 1$ 且 $\\Sigma_g = [0]$。\n\n现在，我们使用 (a) 部分的公式计算平方 FID。\n$$ \\text{FID}^2 = W_2^2 = \\|\\mu_r - \\mu_g\\|_2^2 + \\text{Tr}(\\Sigma_r) + \\text{Tr}(\\Sigma_g) - 2 \\text{Tr}\\left( \\left(\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2}\\right)^{1/2} \\right) $$\n代入参数：\n- $\\mu_r = 1$，$\\mu_g = 1$。\n- $\\Sigma_r = [0]$，$\\Sigma_g = [0]$。\n\n各项为：\n1. 均值差：$\\|\\mu_r - \\mu_g\\|_2^2 = (1-1)^2 = 0$。\n2. $\\Sigma_r$ 的迹：$\\text{Tr}(\\Sigma_r) = \\text{Tr}([0]) = 0$。\n3. $\\Sigma_g$ 的迹：$\\text{Tr}(\\Sigma_g) = \\text{Tr}([0]) = 0$。\n4. 协方差交叉项：\n   - $\\Sigma_r^{1/2} = [0]^{1/2} = [0]$。\n   - 括号内的乘积是 $\\Sigma_r^{1/2} \\Sigma_g \\Sigma_r^{1/2} = [0][0][0] = [0]$。\n   - 它的主平方根是 $([0])^{1/2} = [0]$。\n   - 迹是 $\\text{Tr}([0]) = 0$。\n   - 整个项是 $-2 \\times 0 = 0$。\n\n将各项相加：\n$$ \\text{FID}^2 = 0 + 0 + 0 - 0 = 0 $$\n最终的平方 Fréchet Inception 距离为 $0$。这个结果是“误导性的”，因为真实数据分布覆盖了两种模式（$-1$ 和 $1$），而生成器已经坍缩到单一模式（$1$）。非单射的特征映射 $\\phi(x)=|x|$ 将真实数据的两种模式都映射到相同的特征表示，使得真实和生成的特征分布完全相同。因此，尽管生成器存在严重的模式坍塌，FID 仍然报告了 $0$ 的完美分数。", "answer": "$$\\boxed{0}$$", "id": "3128911"}]}