{"hands_on_practices": [{"introduction": "变分自编码器（VAE）的训练与任何神经网络一样，依赖于通过反向传播算法来优化其参数。这个过程的核心是计算损失函数相对于网络输出的梯度。本练习将引导你推导一个关键的梯度：对于在许多应用中常见的二元数据，其重构损失相对于解码器最终层激活前输出的梯度。通过完成这个推导，你将具体地理解解码器是如何根据重构误差来更新自身，从而为掌握 VAE 的工作机制打下坚实的数学基础。[@problem_id:66106]", "problem": "在逆向材料设计领域，变分自编码器 (VAEs) 是一种强大的生成模型，用于学习材料的连续潜在表示，并生成具有所需性质的新颖材料结构。一个常见的应用是生成结构指纹，这些指纹通常表示为高维二元向量。\n\n考虑一个 VAE，其解码器网络负责重构一个 $D$ 维二元材料指纹，该指纹由向量 $x \\in \\{0, 1\\}^D$ 表示。解码器的最后一层产生一个激活前值的向量 $z \\in \\mathbb{R}^D$。通过应用逐元素的 sigmoid 激活函数，这些激活前的值被转换为重构指纹概率 $\\hat{x} \\in (0, 1)^D$：\n$$ \\hat{x}_i = \\sigma(z_i) = \\frac{1}{1 + \\exp(-z_i)} $$\n对于每个分量 $i=1, \\dots, D$。\n\n重构的质量由二元交叉熵 (BCE) 损失来衡量，该损失是 VAE 总损失函数的一个组成部分。BCE 重构损失由下式给出：\n$$ L_{rec}(x, \\hat{x}) = - \\sum_{i=1}^{D} [x_i \\log(\\hat{x}_i) + (1 - x_i) \\log(1 - \\hat{x}_i)] $$\n其中 $\\log$ 表示自然对数。\n\n为了使用像反向传播这样基于梯度的优化方法来训练 VAE，有必要计算损失函数相对于网络参数的梯度。这个过程中的一个关键步骤是求取损失相对于最后一层激活前输出 $z$ 的梯度。\n\n推导重构损失相对于激活前向量 $z$ 的梯度向量的解析表达式，记作 $\\nabla_z L_{rec}$。", "solution": "重构损失为\n$$\nL_{rec}(x,\\hat x)=-\\sum_{i=1}^D\\bigl[x_i\\ln\\hat x_i+(1-x_i)\\ln(1-\\hat x_i)\\bigr],\n\\quad \\hat x_i=\\sigma(z_i),\\quad \\sigma'(z_i)=\\hat x_i(1-\\hat x_i).\n$$\n逐项求导，\n$$\n\\frac{\\partial L_{rec}}{\\partial z_i}\n=-\\Bigl[x_i\\frac{1}{\\hat x_i}-(1-x_i)\\frac{1}{1-\\hat x_i}\\Bigr]\\sigma'(z_i)\n=-\\Bigl[x_i(1-\\hat x_i)-(1-x_i)\\hat x_i\\Bigr]\n=\\hat x_i-x_i.\n$$\n因此，向量形式为，\n$$\n\\nabla_z L_{rec}=\\hat x-x.\n$$", "answer": "$$\\boxed{\\hat{x}-x}$$", "id": "66106"}, {"introduction": "变分自编码器（VAE）的强大生成能力源于其目标函数——证据下界（ELBO）中两个部分的精妙平衡：一是确保数据能够被精确重构的重构项，二是塑造一个结构良好、平滑的潜在空间的 KL 散度正则化项。本练习探讨了一个在实践中至关重要的高级问题：当模型试图学习观测噪声的方差 $\\sigma^2$ 时，可能会破坏这种平衡，导致一种被称为“后验坍塌”（posterior collapse）的训练失败模式。分析并理解这一动态过程，对于诊断训练问题和构建稳健的生成模型至关重要。[@problem_id:3100707]", "problem": "考虑一个带有各向同性高斯观测模型的变分自编码器 (VAE)。设观测数据为 $\\{x^{(i)}\\}_{i=1}^{n}$，其中每个 $x^{(i)} \\in \\mathbb{R}^{d}$，潜变量 $z \\in \\mathbb{R}^{k}$，解码器均值为 $f_{\\theta}(z)$，以及一个可学习的标量观测方差 $\\sigma^{2} > 0$，因此 $p(x \\mid z) = \\mathcal{N}\\!\\big(f_{\\theta}(z), \\sigma^{2} I_{d}\\big)$。训练目标是证据下界 (ELBO)，\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2})\n=\n\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log p_{\\theta}(x \\mid z)\\right]\n-\n\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right),\n$$\n其中 $\\mathrm{KL}$ 表示 Kullback-Leibler (KL) 散度。假设 $p(z)$ 是一个固定的标准正态先验，而 $q_{\\phi}(z \\mid x)$ 是一个高斯编码器。\n\n请从第一性原理出发，分析学习观测方差 $\\sigma^{2}$ 如何改变重构项 $\\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log p_{\\theta}(x \\mid z)\\right]$ 和潜变量正则化项 $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)$ 之间的有效平衡，并提出一个有原则的校准程序，以避免平凡解，例如后验坍塌（即，$q_{\\phi}(z \\mid x) \\approx p(z)$ 且解码器忽略 $z$）。\n\n选择所有正确的陈述：\n\nA. 学习 $\\sigma^{2}$ 会在参数梯度中有效地将重构项的贡献按与 $1/\\sigma^{2}$ 成比例的因子进行缩放，这可能导致当 $\\sigma^{2}$ 增加时，KL 项相对更占主导地位；一个有原则的校准方法是对 $\\sigma^{2}$ 施加一个共轭逆伽马先验并执行最大后验 (MAP) 估计，以防止 $\\sigma^{2}$ 膨胀并稳定这种权衡关系。\n\nB. 当学习 $\\sigma^{2}$ 时，ELBO 对输入数据 $x$ 的任何缩放都变得不变，因此不需要校准。\n\nC. 联合优化 $\\sigma^{2}$ 存在一个闭式更新解，该解将 $\\sigma^{2}$ 匹配到平均每维度重构残差，如果无约束地使用，这可能让模型在早期增加 $\\sigma^{2}$ 并忽略潜编码；一个实用的校准方法是在初始预热阶段将 $\\sigma^{2}$ 固定到一个目标噪声水平（或等效地，在预热期间使用 $\\beta  1$ 的 $\\beta$-VAE），然后在之后释放 $\\sigma^{2}$ 并设置一个下限。\n\nD. 学习 $\\sigma^{2}$ 降低了后验坍塌的风险，因为减小重构项的权重会迫使潜编码携带更多信息。\n\nE. 一个合理的校准方法是将 $x$ 归一化到单位方差并固定 $\\sigma^{2} = 1$，或者在已知传感器噪声水平的情况下将其设置为该值，这样重构项就能反映物理噪声的尺度，而 KL 项也不会因为任意选择 $\\sigma^{2}$ 而被隐式地加权。", "solution": "用户希望我分析可学习的观测方差 $\\sigma^2$ 在变分自编码器 (VAE) 中的作用，并评估关于其影响和潜在校准方法的几个陈述。\n\n首先，我将形式化训练目标并分析 $\\sigma^2$ 的影响。问题陈述 VAE 具有各向同性高斯观测模型，$p(x \\mid z) = \\mathcal{N}\\!\\big(f_{\\theta}(z), \\sigma^{2} I_{d}\\big)$。给定潜编码 $z \\in \\mathbb{R}^k$，单个数据点 $x \\in \\mathbb{R}^d$ 的对数似然是：\n$$\n\\log p_{\\theta}(x \\mid z) = \\log \\left( \\frac{1}{(2\\pi \\sigma^2)^{d/2}} \\exp\\left(-\\frac{\\|x - f_{\\theta}(z)\\|^2}{2\\sigma^2}\\right) \\right)\n$$\n$$\n\\log p_{\\theta}(x \\mid z) = -\\frac{d}{2} \\log(2\\pi) - \\frac{d}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - f_{\\theta}(z)\\|^2\n$$\n训练目标是最大化每个数据点的证据下界 (ELBO)：\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2}) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\log p_{\\theta}(x \\mid z)\\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n代入 $\\log p_{\\theta}(x \\mid z)$ 的表达式：\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2}) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[-\\frac{d}{2} \\log(2\\pi) - \\frac{d}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\|x - f_{\\theta}(z)\\|^2\\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n由于涉及 $\\sigma^2$ 和常数的项不依赖于 $z$，我们可以将它们从期望中移出：\n$$\n\\mathcal{L}(\\theta, \\phi, \\sigma^{2}) = -\\frac{d}{2} \\log(2\\pi) - \\frac{d}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n训练 VAE 涉及最大化这个 $\\mathcal{L}$，这等同于最小化负 ELBO，通常表示为损失函数 $L_{VAE} = -\\mathcal{L}$。让我们忽略常数项 $-\\frac{d}{2} \\log(2\\pi)$：\n$$\nL_{VAE} = \\frac{1}{2\\sigma^2} \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right] + \\frac{d}{2} \\log(\\sigma^2) + \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x)\\,\\|\\,p(z)\\right)\n$$\n这个形式揭示了核心的权衡关系。损失由三项组成：由 $\\frac{1}{2\\sigma^2}$ 加权的重构误差项，一个惩罚大 $\\sigma^2$ 的项（来自高斯分布的归一化常数），以及 KL 散度正则化项。\n\n参数 $\\sigma^2$ 直接调节重构与正则化之间的平衡。如果我们固定 $\\theta$ 和 $\\phi$ 并对 $\\sigma^2$ 进行优化，我们可以对 $\\mathcal{L}$ 关于 $\\sigma^2$ 求导并令其为 $0$。设 $R = \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right]$ 为期望平方重构误差。\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial (\\sigma^2)} = \\frac{1}{2(\\sigma^2)^2} R - \\frac{d}{2\\sigma^2} = 0\n$$\n$$\n\\implies \\sigma^2 = \\frac{R}{d} = \\frac{1}{d} \\mathbb{E}_{q_{\\phi}(z \\mid x)}\\!\\left[\\|x - f_{\\theta}(z)\\|^2\\right]\n$$\n这表明最优的 $\\sigma^2$ 是平均每维度重构误差。\n\n在联合优化期间，如果解码器尚未训练好，重构误差 $R$ 将会很大。优化过程会推动 $\\sigma^2$ 变大以进行补偿。随着 $\\sigma^2$ 的增加，重构项的权重 $\\frac{1}{2\\sigma^2}$ 会减小。这使得 KL 散度项在总损失中相对更占主导地位。为了最小化现在占主导地位的 KL 项，模型会推动近似后验 $q_{\\phi}(z \\mid x)$ 去匹配先验 $p(z)$。这被称为“后验坍塌”。潜编码 $z$ 不再携带关于输入 $x$ 的重要信息，解码器学会忽略它们，输出一个平均的重构。这是一个平凡的、不希望出现的局部最小值。\n\n现在我将评估每个选项。\n\n**A. 学习 $\\sigma^{2}$ 会在参数梯度中有效地将重构项的贡献按与 $1/\\sigma^{2}$ 成比例的因子进行缩放，这可能导致当 $\\sigma^{2}$ 增加时，KL 项相对更占主导地位；一个有原则的校准方法是对 $\\sigma^{2}$ 施加一个共轭逆伽马先验并执行最大后验 (MAP) 估计，以防止 $\\sigma^{2}$ 膨胀并稳定这种权衡关系。**\n这个陈述是正确的。要最小化的损失是 $L_{VAE} = \\frac{1}{2\\sigma^2}R + \\frac{d}{2}\\log(\\sigma^2) + \\mathrm{KL}$。重构误差 $R$ 对损失的贡献确实按 $\\frac{1}{2\\sigma^2}$ 进行缩放。随着 $\\sigma^2$ 的增加，这个缩放因子减小，降低了重构项相对于 KL 项的重要性。如上所述，这可能导致后验坍塌。在 $\\sigma^2$ 上放置一个先验是正则化参数的标准贝叶斯方法。高斯分布方差的共轭先验是逆伽马分布。使用此先验进行最大后验估计，会在损失中增加一个正则化项，该项惩罚远离先验众数的方差值，从而有效防止 $\\sigma^2$ 不受控制地增长。这稳定了各项之间的权衡关系。**结论：正确。**\n\n**B. 当学习 $\\sigma^{2}$ 时，ELBO 对输入数据 $x$ 的任何缩放都变得不变，因此不需要校准。**\n这个陈述是错误的。让我们检验一下不变性的说法。假设我们对数据进行缩放 $x' = c x$，其中标量 $c  0$。一个用于缩放后数据的最优解码器会学会相应地缩放其输出，$f'_{\\theta}(z) = c f_{\\theta}(z)$。新的重构误差是 $R' = \\mathbb{E}[\\|x' - f'_{\\theta}(z)\\|^2] = c^2 \\mathbb{E}[\\|x-f_{\\theta}(z)\\|^2] = c^2 R$。缩放后问题的最优方差将是 $(\\sigma')^2 = R'/d = c^2 R/d = c^2 \\sigma^2$。让我们将这些代入 ELBO 中依赖于数据尺度的部分：$\\mathcal{L}_{recon} = -\\frac{1}{2\\sigma^2} R - \\frac{d}{2}\\log(\\sigma^2)$。对于缩放后的问题，这变为：\n$$\n\\mathcal{L}'_{recon} = -\\frac{1}{2(\\sigma')^2} R' - \\frac{d}{2}\\log((\\sigma')^2) = -\\frac{1}{2(c^2\\sigma^2)} (c^2 R) - \\frac{d}{2}\\log(c^2\\sigma^2)\n$$\n$$\n= -\\frac{1}{2\\sigma^2} R - \\frac{d}{2}(\\log(c^2) + \\log(\\sigma^2)) = \\left(-\\frac{1}{2\\sigma^2} R - \\frac{d}{2}\\log(\\sigma^2)\\right) - \\frac{d}{2}\\log(c^2)\n$$\n$$\n= \\mathcal{L}_{recon} - d \\log(c)\n$$\nELBO 不是不变的；它改变了一个常数量 $-d \\log(c)$。因此，这个前提是错误的，校准仍然是一个相关的问题。**结论：错误。**\n\n**C. 联合优化 $\\sigma^{2}$ 存在一个闭式更新解，该解将 $\\sigma^{2}$ 匹配到平均每维度重构残差，如果无约束地使用，这可能让模型在早期增加 $\\sigma^{2}$ 并忽略潜编码；一个实用的校准方法是在初始预热阶段将 $\\sigma^{2}$ 固定到一个目标噪声水平（或等效地，在预热期间使用 $\\beta  1$ 的 $\\beta$-VAE），然后在之后释放 $\\sigma^{2}$ 并设置一个下限。**\n这个陈述准确地描述了其底层机制和一个标准的实用解决方案。如上所述，对于固定的 $\\theta, \\phi$，最优的 $\\sigma^2$ 是 $\\sigma^2 = R/d$，即平均每维度重构残差。关于这可能导致 $\\sigma^2$ 早期增加并随后导致后验坍塌的分析也是正确的。一个常见且有效的缓解策略是“预热”VAE。在这个初始阶段，可以要么将 $\\sigma^2$ 固定在一个合理的值（例如 $\\sigma^2=1$），要么等效地通过乘以一个逐渐退火到 1 的因子 $\\beta  1$ 来减小 KL 项的权重。这两种方法都在初期给予重构项更高的相对权重，迫使解码器学习使用潜编码 $z$ 进行重构。预热后，可以联合学习 $\\sigma^2$，通常带有一个小的下限以防止数值上坍塌到零。**结论：正确。**\n\n**D. 学习 $\\sigma^{2}$ 降低了后验坍塌的风险，因为减小重构项的权重会迫使潜编码携带更多信息。**\n这个陈述的说法与实际发生的情况相反。学习 $\\sigma^2$ 允许模型增加它，这会*减小*重构项的权重。对重构误差的惩罚越小，模型将关于输入 $x$ 的有用信息编码到潜编码 $z$ 中的激励就*越少*。模型更不被“强迫”去使用 $z$。因此，模型通过使 $q_{\\phi}(z \\mid x)$ 信息贫乏并接近先验 $p(z)$ 来最小化 KL 项变得更容易。因此，不加校准地学习 $\\sigma^2$ 会*增加*后验坍塌的风险，而不是减少它。**结论：错误。**\n\n**E. 一个合理的校准方法是将 $x$ 归一化到单位方差并固定 $\\sigma^{2} = 1$，或者在已知传感器噪声水平的情况下将其设置为该值，这样重构项就能反映物理噪声的尺度，而 KL 项也不会因为任意选择 $\\sigma^{2}$ 而被隐式地加权。**\n这个陈述提出了另一个有效的策略：根本不学习 $\\sigma^2$，而是将其固定在一个有原则的值上。如果输入数据 $x$ 经过预处理，使得每个维度都具有近似单位方差，那么将观测方差固定为 $\\sigma^2=1$ 就建立了一个固定的、合理的权衡。这隐含地说明模型应该解释数据结构，任何剩余的残差都被视为噪声，其方差与数据本身的方差相当。如果物理噪声过程是已知的（例如，来自传感器规格），将 $\\sigma^2$ 固定到该已知值是一种更有原则的方法。在这两种情况下，通过固定 $\\sigma^2$，我们阻止模型动态地改变重构项的权重，从而避免导致后验坍塌的不稳定性。这是一个合理且常见的做法。**结论：正确。**\n\n总而言之，陈述 A、C 和 E 是对学习 $\\sigma^2$ 的影响和有效的校准策略的正确描述。", "answer": "$$\\boxed{ACE}$$", "id": "3100707"}, {"introduction": "变分自编码器（VAE）的框架并不仅限于处理连续的潜在变量。对于那些天然具有分类结构的数据，使用离散的潜在变量来建模往往更为合适。本练习将引导你进入这一前沿领域，通过一个编程实践来掌握 Gumbel-Softmax 技巧，这是一种使得离散随机变量能够被纳入梯度优化框架的巧妙方法。你将不仅实现一个使用离散潜变量的 VAE，还将通过实验分析该方法关键超参数（温度 $\\tau$）对梯度估计器偏差和方差的影响，从而将深刻的理论与实际的代码实现联系起来。[@problem_id:3198001]", "problem": "考虑一个具有离散潜变量的变分自编码器 (VAE)。设潜变量 $z$ 在集合 $\\{1,2,\\dots,K\\}$ 中取值，其编码器分布 $q(z \\mid x)$ 是一个类别分布，对于 $k \\in \\{1,\\dots,K\\}$，其概率为 $q_k(x)$。设编码器由 logits $\\alpha \\in \\mathbb{R}^K$ 参数化，使得 $q_k(x) = \\mathrm{softmax}(\\alpha)_k$。设先验 $p(z)$ 是一个类别分布，概率为 $p_k$。设解码器指定一个伯努利似然 $p(x=1 \\mid z=k) = \\theta_k$，其中观测值固定为 $x=1$ 且 $\\theta_k \\in (0,1)$ 是已知的。\n\n我们考虑单个观测值 $x=1$ 的证据下界 (ELBO)，\n$$\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{q(z \\mid x)}\\big[ \\log p(x=1 \\mid z) \\big] - \\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big).\n$$\n编码器使用 Gumbel-Softmax 重参数化进行松弛。对于温度 $\\tau  0$，定义松弛样本\n$$\ny = \\mathrm{softmax}\\left(\\frac{\\log \\pi + g}{\\tau}\\right),\n$$\n其中 $\\pi_k = q_k(x)$ 是由 $\\mathrm{softmax}(\\alpha)$ 给出的类别概率，而 $g_k$ 是服从 $\\mathrm{Gumbel}(0,1)$ 分布的独立同分布 Gumbel 随机变量。在实现时请注意，将 $\\log \\pi$ 替换为未归一化的 logits $\\alpha$ 会得到等价的 $y = \\mathrm{softmax}\\left(\\frac{\\alpha + g}{\\tau}\\right)$，因为在 softmax 函数内部加上或减去任意常数都不会改变其输出。\n\n通过将期望项替换为其松弛形式来定义松弛目标，\n$$\n\\widetilde{\\mathcal{L}}(\\alpha,\\tau) = \\mathbb{E}_{g}\\left[ \\sum_{k=1}^K y_k \\log p(x=1 \\mid z=k) \\right] - \\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big),\n$$\n并考虑其关于 $\\alpha$ 的梯度估计器。真实梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$ 使用类别分布的恒等式和 Kullback–Leibler 散度的定义，从第一性原理出发进行解析计算。松弛梯度 $\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)$ 通过 $y$ 并利用重参数化技巧进行估计。\n\n您的任务是：\n- 根据 $\\mathbb{E}_{q(z \\mid x)}\\big[ \\log p(x=1 \\mid z) \\big]$、类别 softmax 映射以及 $\\mathrm{KL}\\big(q(z \\mid x) \\,\\|\\, p(z)\\big)$ 的定义，实现关于 $\\alpha$ 的精确梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$。\n- 通过 Gumbel-Softmax 样本 $y$ 和重参数化技巧实现松弛梯度估计器 $\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)$，其中保持 KL 项精确，仅对期望项进行松弛。\n- 对于下面测试套件中的每个温度 $\\tau$，估计松弛梯度估计器相对于精确梯度的偏差和方差：\n  - 偏差定义为松弛梯度估计器的蒙特卡洛均值与精确梯度之间差值的欧几里得范数，即 $\\|\\mathbb{E}[\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)] - \\nabla_{\\alpha} \\mathcal{L}(\\alpha)\\|_2$。\n  - 方差定义为在独立的 Gumbel 抽样下，松弛梯度估计器各分量样本方差的总和，即 $\\sum_{j=1}^K \\mathrm{Var}\\left[\\left(\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha,\\tau)\\right)_j\\right]$。\n- 为了可复现性，使用固定的随机种子。\n\n使用以下固定配置，以使问题完全指定且可测试：\n- 类别数量 $K = 3$。\n- 编码器 logits $\\alpha = [0.3, -0.5, 0.2]$。\n- 先验概率 $p = [0.6, 0.3, 0.1]$。\n- 解码器概率 $\\theta = [0.9, 0.4, 0.1]$，对于 $p(x=1 \\mid z=k) = \\theta_k$。\n- 单个观测值 $x = 1$。\n- 每个温度 $\\tau$ 的蒙特卡洛样本数：$R = 10000$。\n- 随机种子：$0$。\n- Gumbel 抽样：对于 $u \\sim \\mathrm{Uniform}(0,1)$，独立地为每个分量设置 $g = -\\log(-\\log(u))$。\n\n温度 $\\tau$ 的测试套件：\n1. $\\tau = 5.0$ (高温，强平滑)。\n2. $\\tau = 1.0$ (中等温度)。\n3. $\\tau = 0.5$ (较低温度)。\n4. $\\tau = 0.1$ (极低温度)。\n5. $\\tau = 0.01$ (接近离散的极限)。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个条目对应一个温度，并包含一对浮点数 $[\\text{bias}, \\text{variance}]$（按此顺序）。精确格式为：\n\"[[b_1,v_1],[b_2,v_2],[b_3,v_3],[b_4,v_4],[b_5,v_5]]\"\n输出行中任何地方都没有空格。所有值都应打印为十进制浮点数（无百分号）。", "solution": "用户的请求是分析具有离散潜变量的变分自编码器 (VAE) 的 Gumbel-Softmax 梯度估计器的偏差和方差。这是一个基于机器学习和统计学原理的有效且适定的问题。我们将首先推导证据下界 (ELBO) 精确梯度的解析形式，然后推导松弛梯度估计器的形式。随后，我们将实现这些公式，以计算给定参数集下的偏差和方差。\n\n### 1. 预备知识和定义\n设离散潜变量 $z$ 在 $\\{1, \\dots, K\\}$ 中取值。模型组件如下：\n- **编码器**：$q(z=k \\mid x) = \\pi_k$，其中 $\\pi = \\mathrm{softmax}(\\alpha)$，logits 为 $\\alpha \\in \\mathbb{R}^K$。因此，$\\pi_k = \\frac{e^{\\alpha_k}}{\\sum_{j=1}^K e^{\\alpha_j}}$。\n- **先验**：$p(z=k) = p_k$，一个固定的类别分布。\n- **解码器**：$p(x=1 \\mid z=k) = \\theta_k$，对于固定的观测值 $x=1$ 的伯努利似然。\n\n单个观测值 $x=1$ 的 ELBO 为：\n$$\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{\\pi}\\big[ \\log p(x=1 \\mid z) \\big] - \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n展开各项，我们得到：\n$$\n\\mathcal{L}(\\alpha) = \\sum_{k=1}^K \\pi_k \\log \\theta_k - \\sum_{k=1}^K \\pi_k \\log\\left(\\frac{\\pi_k}{p_k}\\right) = \\sum_{k=1}^K \\pi_k \\left( \\log \\theta_k - \\log \\pi_k + \\log p_k \\right)\n$$\n\n### 2. ELBO 的精确梯度\n为了计算真实梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$，我们将 $\\mathcal{L}(\\alpha)$ 对每个分量 $\\alpha_j$ 求导。我们使用链式法则和 softmax 函数的导数 $\\frac{\\partial \\pi_k}{\\partial \\alpha_j} = \\pi_k(\\delta_{kj} - \\pi_j)$，其中 $\\delta_{kj}$ 是克罗内克 δ (Kronecker delta)。\n\nELBO 关于 $\\pi_k$ 的导数是：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} = (\\log \\theta_k - \\log \\pi_k + \\log p_k) + \\pi_k\\left(-\\frac{1}{\\pi_k}\\right) = \\log\\left(\\frac{\\theta_k p_k}{\\pi_k}\\right) - 1\n$$\n应用链式法则：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_j} = \\sum_{k=1}^K \\frac{\\partial \\mathcal{L}}{\\partial \\pi_k} \\frac{\\partial \\pi_k}{\\partial \\alpha_j} = \\sum_{k=1}^K \\left(\\log\\left(\\frac{\\theta_k p_k}{\\pi_k}\\right) - 1\\right) \\pi_k(\\delta_{kj} - \\pi_j)\n$$\n这可以简化为变分推断中的一个标准结果：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_j} = \\pi_j \\left( \\left(\\log \\theta_j - \\log \\pi_j + \\log p_j\\right) - \\mathcal{L}(\\alpha) \\right)\n$$\n以向量形式表示，令 $C_k = \\log \\theta_k - \\log \\pi_k + \\log p_k$，则梯度为：\n$$\n\\nabla_{\\alpha} \\mathcal{L}(\\alpha) = \\pi \\odot (C - \\mathcal{L}(\\alpha)\\mathbf{1})\n$$\n其中 $\\odot$ 是逐元素乘积，$\\mathbf{1}$ 是全 1 向量。该梯度可以解析地计算。\n\n### 3. 松弛梯度估计器\nGumbel-Softmax 重参数化技巧用于获得一个低方差的梯度估计器。松弛目标为：\n$$\n\\widetilde{\\mathcal{L}}(\\alpha, \\tau) = \\mathbb{E}_{g}\\left[ \\sum_{k=1}^K y_k \\log \\theta_k \\right] - \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n其中 $y = \\mathrm{softmax}\\left(\\frac{\\alpha + g}{\\tau}\\right)$ 且 $g_k \\sim \\mathrm{Gumbel}(0,1)$ 是独立同分布的噪声变量。KL 散度项保持精确，不进行重参数化。\n\n松弛目标的梯度为：\n$$\n\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha, \\tau) = \\nabla_{\\alpha} \\mathbb{E}_{g}\\left[ \\sum_{k=1}^K y_k \\log \\theta_k \\right] - \\nabla_{\\alpha} \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n根据重参数化技巧，我们可以将梯度移到期望内部：\n$$\n\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}(\\alpha, \\tau) = \\mathbb{E}_{g}\\left[ \\nabla_{\\alpha} \\left(\\sum_{k=1}^K y_k \\log \\theta_k \\right) \\right] - \\nabla_{\\alpha} \\mathrm{KL}\\big(\\pi \\,\\|\\, p\\big)\n$$\n该梯度估计器的一个蒙特卡洛单样本，记作 $\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}$，通过对 $g$ 进行一次抽样并计算下式得到：\n$$\n\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}} = \\nabla_{\\alpha} \\left(\\sum_{k=1}^K y_k(\\alpha, g) \\log \\theta_k \\right) - \\nabla_{\\alpha} \\mathrm{KL}\\big(\\pi(\\alpha) \\,\\|\\, p\\big)\n$$\n第一项关于 $\\alpha_j$ 的梯度是：\n$$\n\\frac{\\partial}{\\partial \\alpha_j} \\left(\\sum_{k=1}^K y_k \\log \\theta_k \\right) = \\sum_{k=1}^K \\left(\\frac{\\partial y_k}{\\partial \\alpha_j}\\right) \\log \\theta_k = \\sum_{k=1}^K \\left(\\frac{1}{\\tau} y_k (\\delta_{kj} - y_j)\\right) \\log \\theta_k = \\frac{1}{\\tau} y_j \\left(\\log \\theta_j - \\sum_{k=1}^K y_k \\log \\theta_k \\right)\n$$\nKL 项的梯度是确定性的，由下式给出：\n$$\n\\frac{\\partial}{\\partial \\alpha_j} \\mathrm{KL}(\\pi \\,\\|\\, p) = \\pi_j \\left( (\\log \\pi_j - \\log p_j) - \\mathrm{KL}(\\pi \\,\\|\\, p) \\right)\n$$\n这部分与精确梯度推导中的对应项相同。\n\n### 4. 偏差和方差计算\n对于每个温度 $\\tau$，我们将执行以下步骤：\n1.  计算一次精确梯度 $\\nabla_{\\alpha} \\mathcal{L}(\\alpha)$。\n2.  生成 $R=10000$ 个松弛梯度估计器的样本，$\\{\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}^{(i)}\\}_{i=1}^R$。\n3.  估计松弛梯度的均值：$\\mathbb{E}[\\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}] \\approx \\frac{1}{R} \\sum_{i=1}^R \\widehat{\\nabla_{\\alpha} \\widetilde{\\mathcal{L}}}^{(i)}$。\n4.  将偏差计算为差值的欧几里得范数：$\\|\\text{mean relaxed gradient} - \\text{exact gradient}\\|_2$。\n5.  将总方差计算为松弛梯度估计器样本的逐分量样本方差之和。\n\n我们预期，当 $\\tau \\to 0$ 时，偏差会减小，因为 Gumbel-Softmax 分布趋近于真实的类别分布，使得该估计器是渐近无偏的。相反地，我们预期当 $\\tau \\to 0$ 时，方差会增大，这是由于松弛样本越来越尖锐、接近离散的性质所致。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the bias and variance of the Gumbel-Softmax gradient estimator\n    for a discrete VAE, for various temperatures.\n    \"\"\"\n    \n    def softmax(x):\n        \"\"\"Computes softmax for a 1D vector in a numerically stable way.\"\"\"\n        e_x = np.exp(x - np.max(x))\n        return e_x / e_x.sum()\n\n    # --- Problem Configuration ---\n    K = 3\n    alpha = np.array([0.3, -0.5, 0.2])\n    p = np.array([0.6, 0.3, 0.1])\n    theta = np.array([0.9, 0.4, 0.1])\n    R = 10000\n    random_seed = 0\n    tau_values = [5.0, 1.0, 0.5, 0.1, 0.01]\n\n    log_p = np.log(p)\n    log_theta = np.log(theta)\n\n    # --- 1. Compute the Exact Analytical Gradient of the ELBO ---\n    \n    # Encoder probabilities\n    pi = softmax(alpha)\n    log_pi = np.log(pi)\n\n    # Evidence Lower Bound (ELBO)\n    elbo = np.sum(pi * (log_theta - (log_pi - log_p)))\n\n    # Exact gradient: grad_L_j = pi_j * ( (log_theta_j - log_pi_j + log_p_j) - elbo )\n    C = log_theta - log_pi + log_p\n    grad_L_exact = pi * (C - elbo)\n\n    # --- 2. Compute the deterministic gradient of the KL divergence term ---\n    # This is needed for the relaxed gradient estimator.\n    # grad_KL_j = pi_j * ( (log_pi_j - log_p_j) - KL )\n    kl_div = np.sum(pi * (log_pi - log_p))\n    grad_kl_exact = pi * ((log_pi - log_p) - kl_div)\n\n    # --- 3. Loop over temperatures to compute Bias and Variance ---\n    results = []\n    rng = np.random.default_rng(random_seed)\n\n    for tau in tau_values:\n        # Store all Monte Carlo samples of the relaxed gradient\n        relaxed_grad_samples = np.zeros((R, K))\n\n        for i in range(R):\n            # Sample Gumbel noise G_k ~ Gumbel(0, 1)\n            # Add small epsilon to avoid log(0) in case of u=0 or u=1\n            u = rng.uniform(low=1e-10, high=1.0 - 1e-10, size=K)\n            g = -np.log(-np.log(u))\n\n            # Compute relaxed sample y using Gumbel-Softmax reparameterization\n            y = softmax((alpha + g) / tau)\n\n            # --- Compute a single sample of the relaxed gradient estimator ---\n            # It consists of the gradient of the relaxed expectation term minus the\n            # exact gradient of the KL term.\n            \n            # Gradient of the relaxed expectation term: E_g[sum y_k * log(theta_k)]\n            # dE_rel/d(alpha_j) = (1/tau) * y_j * (log(theta_j) - sum_k y_k * log(theta_k))\n            sum_y_log_theta = np.sum(y * log_theta)\n            grad_E_rel_sample = (1.0 / tau) * y * (log_theta - sum_y_log_theta)\n            \n            # The full relaxed gradient sample\n            relaxed_grad_samples[i, :] = grad_E_rel_sample - grad_kl_exact\n        \n        # --- 4. Compute Bias and Variance from the collected samples ---\n        \n        # Mean of relaxed gradient estimates\n        mean_relaxed_grad = np.mean(relaxed_grad_samples, axis=0)\n        \n        # Bias is the L2 norm of the difference between the mean estimate and the true gradient\n        bias = np.linalg.norm(mean_relaxed_grad - grad_L_exact)\n        \n        # Variance is the sum of component-wise sample variances (ddof=1)\n        var_components = np.var(relaxed_grad_samples, axis=0, ddof=1)\n        total_variance = np.sum(var_components)\n        \n        results.append([bias, total_variance])\n    \n    # --- 5. Format and print the final output ---\n    # Format: \"[[b_1,v_1],[b_2,v_2],...]\" with no spaces\n    output_str = \"[\" + \",\".join([f\"[{b},{v}]\" for b, v in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3198001"}]}