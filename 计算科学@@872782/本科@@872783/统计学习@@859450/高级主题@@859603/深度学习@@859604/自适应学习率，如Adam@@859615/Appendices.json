{"hands_on_practices": [{"introduction": "Adam 优化器的一个核心优势是其对特征缩放的近似不变性。本练习旨在通过一个课程学习任务来实证检验这一关键特性，在该任务中，一个特征的尺度在训练过程中会发生剧烈变化。通过比较 Adam 和梯度下降（SGD）达到目标损失所需步数，你将亲身体验到自适应学习率在处理不同尺度特征时的卓越适应能力。", "problem": "你需要编写一个完整、可运行的程序，以经验性地评估自适应矩估计 (Adam) 相对于普通随机梯度下降 (SGD) 的尺度不变性和适应速度。评估将使用一个课程（curriculum），在训练过程中逐步改变单个特征的尺度。实验的核心是带有平方损失的纯线性回归。该测试要求你从基本原理实现这两种优化器，并测量在每次尺度变化后，每种优化器达到目标损失需要多少梯度步数。\n\n数据集和模型规格：\n- 按照如下方式生成一个包含 $N$ 个独立样本和 $d$ 个特征的合成数据集。固定随机种子为 $42$ 以确保结果的确定性。\n- 令 $N = 512$，$d = 2$。\n- 抽取特征矩阵 $X \\in \\mathbb{R}^{N \\times d}$，其条目独立从标准正态分布中采样。\n- 令真实参数矢量为 $w_{\\text{true}} = [\\,2.0,\\,-3.0\\,]^{\\top}$。\n- 使用带有加性噪声的线性模型生成目标 $y \\in \\mathbb{R}^{N}$：$y = X w_{\\text{true}} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I)$，$\\sigma = 0.1$。\n- 预测函数为 $\\hat{y}(w) = X_{\\text{scaled}} w$，其中 $X_{\\text{scaled}}$ 是通过将 $X$ 的第二个特征列乘以一个正标量 $s$ 得到的。也就是说，如果 $X = [\\,x^{(1)},\\,x^{(2)}\\,]$，则对于一个选定的 $s$，$X_{\\text{scaled}} = [\\,x^{(1)},\\, s \\cdot x^{(2)}\\,]$。\n- 损失函数为均方误差 (MSE) $L(w; X_{\\text{scaled}}, y) = \\frac{1}{N}\\sum_{i=1}^{N}\\left(\\hat{y}_{i}(w) - y_{i}\\right)^{2}$。\n\n课程与评估协议：\n- 课程是一个有限序列的严格正尺度因子 $\\{s_{0}, s_{1}, \\dots, s_{P-1}\\}$，应用于第二个特征，在给定优化器的单次训练运行中按顺序访问。优化器在各阶段之间从其当前状态继续；也就是说，当尺度变为下一个 $s_{p}$ 时，参数矢量 $w$ 和任何优化器内部状态都不会重置。\n- 对于每个阶段 $p \\in \\{0,1,\\dots,P-1\\}$，通过将 $X$ 的第二个特征乘以 $s_{p}$ 来设置当前设计矩阵为 $X_{\\text{scaled}}^{(p)}$，然后使用该固定的 $X_{\\text{scaled}}^{(p)}$ 进行最多 $T$ 次梯度步骤。\n- 定义目标 MSE 阈值为 $\\tau = c \\cdot \\sigma^{2}$，其中 $c = 2.0$。具体来说，当 $\\sigma = 0.1$ 时，$\\tau = 2.0 \\cdot (0.1)^{2}$。\n- 阶段 $p$ 的适应步数定义为在该阶段内，MSE 首次降至最多为 $\\tau$ 时所用的最小梯度步数（如果阶段开始时 MSE 已不大于 $\\tau$，则计为零），或者如果在该阶段内 MSE 从未降至 $\\tau$ 或以下，则等于 $T$。\n- 对于一个课程，将平均适应步数定义为该课程中所有阶段的每阶段适应步数的算术平均值。\n\n需要从基本原理实现的算法：\n- 对于普通随机梯度下降 (SGD)，使用固定的学习率 $\\alpha_{\\text{sgd}}$ 和 MSE 的全批量梯度。$L$ 关于 $w$ 的全批量梯度为\n$$\n\\nabla L(w) \\;=\\; \\frac{2}{N}\\,X_{\\text{scaled}}^{\\top}\\left(X_{\\text{scaled}}w - y\\right).\n$$\n- 对于自适应矩估计 (Adam)，设计更新规则时使用梯度的第一和第二矩的指数移动平均以及偏差校正，使得每个坐标的更新幅度对特征重缩放近似不变。你必须推导并实现对梯度及其逐元素平方使用带衰减参数的指数移动平均，并在形成逐参数归一化步骤之前加入偏差校正。Adam 的任何内部状态（如矩估计或时间索引）在课程各阶段之间不得重置。\n\n超参数：\n- SGD 使用 $\\alpha_{\\text{sgd}} = 0.001$。\n- Adam 使用学习率 $\\alpha_{\\text{adam}} = 0.01$，一阶矩衰减 $\\beta_{1} = 0.9$，二阶矩衰减 $\\beta_{2} = 0.999$，以及数值稳定常数 $\\varepsilon_{\\text{adam}} = 10^{-8}$。\n- 所有步骤均使用全批量梯度。\n- 对于每个课程阶段，最多允许 $T$ 步，其中 $T$ 由每个测试用例指定。\n\n测试套件：\n- 所有测试用例共享由上述相同的 $X$ 和 $y$ 构建的同一个数据集。\n- 每个测试用例指定一个课程和每阶段的步数预算 $T$。\n- 测试用例如下：\n    1. 情况 A (理想情况)：尺度为 $\\left[\\,1.0,\\,10.0,\\,100.0\\,\\right]$，$T = 400$。\n    2. 情况 B (先缩小后放大)：尺度为 $\\left[\\,1.0,\\,0.1,\\,10.0\\,\\right]$，$T = 400$。\n    3. 情况 C (边界情况：无尺度变化)：尺度为 $\\left[\\,1.0\\,\\right]$，$T = 200$。\n\n要求输出：\n- 对于每个测试用例，计算比率\n$$\nr \\;=\\; \\frac{\\text{SGD 在课程上的平均适应步数}}{\\text{Adam 在课程上的平均适应步数}}。\n$$\n- 将三个比率按 $\\left[\\text{情况 A},\\text{情况 B},\\text{情况 C}\\right]$ 的顺序聚合为单行。\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表的结果（例如，$\\left[\\text{resultA},\\text{resultB},\\text{resultC}\\right]$）。每个元素必须是实数。\n\n科学真实性和推导基础：\n- 从线性回归的 MSE 定义、全批量梯度表达式以及带偏差校正的指数移动平均的定义开始。推导通过二阶矩估计的平方根进行逐坐标归一化如何使更新幅度对相应特征的常数重缩放产生近似不变性，并在 Adam 中实现这一点。不要在阶段之间重置优化器状态，以便在突然重缩放后正确测量适应速度。确保你的实现是数值稳定的，并且实验在固定种子下是确定性的。", "solution": "该问题要求在线性回归任务中，对普通全批量梯度下降 (GD) 和自适应矩估计 (Adam) 进行经验性比较。比较的重点是适应速度，通过在单个特征的尺度突然改变后达到目标损失所需的优化步数来衡量。这是一个精心设计的数值实验，旨在突出 Adam 的尺度不变性。问题陈述在科学上是合理的、内部一致的，并为确定性和可复现的结果提供了所有必要的参数。\n\n我们首先对数学设定进行形式化。数据集包含 $N=512$ 个样本和 $d=2$ 个特征。特征矩阵 $X \\in \\mathbb{R}^{N \\times d}$ 是通过从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取元素生成的。目标变量 $y \\in \\mathbb{R}^{N}$ 是通过一个带有加性高斯噪声的线性模型生成的：\n$$\ny = X w_{\\text{true}} + \\varepsilon\n$$\n其中，真实参数矢量为 $w_{\\text{true}} = [2.0, -3.0]^{\\top}$，噪声为 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，且 $\\sigma = 0.1$。为了保证可复现性，数据生成使用了固定的随机种子 $42$。\n\n预测模型是一个线性函数 $\\hat{y}(w) = X_{\\text{scaled}} w$，其中 $X_{\\text{scaled}}$ 是通过将 $X$ 的第二个特征列乘以一个因子 $s > 0$ 得到的。如果 $X = [x^{(1)}, x^{(2)}]$，那么 $X_{\\text{scaled}} = [x^{(1)}, s \\cdot x^{(2)}]$。目标是找到参数矢量 $w \\in \\mathbb{R}^d$ 以最小化均方误差 (MSE) 损失：\n$$\nL(w; X_{\\text{scaled}}, y) = \\frac{1}{N} \\| X_{\\text{scaled}} w - y \\|_2^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\left( (X_{\\text{scaled}} w)_i - y_i \\right)^2\n$$\n最小化此损失的最优参数 $w^*$ 取决于尺度 $s$。它们满足 $X_{\\text{scaled}}^\\top (X_{\\text{scaled}} w^* - y) = 0$。在无噪声的情况下，这意味着 $X_{\\text{scaled}} w^* \\approx X w_{\\text{true}}$，这在 $w_1^* = w_{\\text{true},1}$ 且 $s \\cdot w_2^* = w_{\\text{true},2}$ 时成立。因此，目标参数矢量近似变为 $w^*(s) = [2.0, -3.0/s]^\\top$。\n\n为了最小化损失，我们使用基于梯度的优化。MSE 损失关于 $w$ 的梯度由下式给出：\n$$\n\\nabla_w L(w) = \\frac{2}{N} X_{\\text{scaled}}^{\\top} (X_{\\text{scaled}} w - y)\n$$\n两种优化算法都使用这个梯度，它们都是从基本原理实现的。\n\n首先，我们考虑普通梯度下降 (GD)，问题中称其为 SGD，但实际使用的是全批量梯度。其更新规则为：\n$$\nw_{t+1} = w_t - \\alpha_{\\text{gd}} \\nabla_w L(w_t)\n$$\n其中 $w_t$ 是第 $t$ 步的参数矢量，$\\alpha_{\\text{gd}} = 0.001$ 是固定的学习率。GD 的性能对特征的缩放高度敏感。梯度 $\\nabla_w L(w)$ 的分量取决于尺度 $s$。第二个分量 $\\frac{\\partial L}{\\partial w_2}$ 受到 $s$ 的强烈影响。较大的 $s$ 会使该梯度分量变得非常大，除非 $\\alpha_{\\text{gd}}$ 非常小，否则会导致振荡和不稳定。相反，较小的 $s$ 会导致梯度分量消失，从而减慢对 $w_2$ 的学习速度。\n\n其次，我们实现 Adam 优化器。Adam 维护过去梯度的指数衰减平均（一阶矩）和过去梯度平方的指数衰减平均（二阶矩）。令 $g_t = \\nabla_w L(w_{t-1})$ 为第 $t$ 步的梯度。矩估计更新如下：\n$$\nm_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n$$\n$$\nv_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n$$\n其中 $g_t^2$ 表示逐元素平方，$\\beta_1 = 0.9$ 是一阶矩衰减率，$\\beta_2 = 0.999$ 是二阶矩衰减率。这些是有偏估计，尤其是在初始步骤中。Adam 通过计算以下方式来校正这种偏差：\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t} \\quad \\text{和} \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n最终的参数更新为：\n$$\nw_t = w_{t-1} - \\alpha_{\\text{adam}} \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon_{\\text{adam}}}\n$$\n学习率为 $\\alpha_{\\text{adam}} = 0.01$，还有一个小的稳定常数 $\\varepsilon_{\\text{adam}} = 10^{-8}$。Adam 自适应性的关键在于 $\\sqrt{\\hat{v}_t}$ 项，它对每个参数坐标的更新进行归一化。如果某个参数 $w_j$ 的梯度分量持续较大（例如，由于特征尺度 $s$ 较大），其对应的二阶矩估计 $v_{t,j}$ 也会较大。除以 $\\sqrt{\\hat{v}_{t,j}}$ 会有效地减小该参数的步长。相反，如果梯度较小，步长则被有效地增大。这种机制使得 Adam 的更新步长对特征的尺度近似不变，使其能够在损失函数曲面发生变化时快速适应。\n\n评估协议涉及一个尺度课程 $\\{s_0, s_1, \\dots, s_{P-1}\\}$。对于每个尺度 $s_p$，优化器最多运行 $T$ 步。优化器的状态（GD 的 $w$；Adam 的 $w, m, v, t$）在各个阶段之间保持不变。我们测量每个阶段内将 MSE 降至阈值 $\\tau = c \\cdot \\sigma^2 = 2.0 \\cdot (0.1)^2 = 0.02$ 以下所需的步数。如果在一个阶段开始时损失已经低于 $\\tau$，则该阶段的步数计数为 $0$。如果在 $T$ 步内未达到 $\\tau$，则计数为 $T$。性能指标是一个课程中所有阶段的平均适应步数。最后，我们计算 GD 与 Adam 的平均步数之比。\n\n下面的代码实现了这整个过程，包括数据生成、从基本原理实现的两种优化器，以及针对三个测试用例的指定基于课程的评估。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment, evaluate optimizers, and print the result.\n    \"\"\"\n    \n    # 1. Dataset and Model Specification\n    N, d = 512, 2\n    w_true = np.array([2.0, -3.0])\n    sigma = 0.1\n    random_seed = 42\n\n    def generate_data(N, d, w_true, sigma, seed):\n        \"\"\"Generates synthetic dataset based on problem specifications.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(N, d))\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y = X @ w_true + noise\n        return X, y\n\n    X, y = generate_data(N, d, w_true, sigma, random_seed)\n\n    # 2. Loss and Gradient Functions\n    def mse_loss(w, X_scaled, y):\n        \"\"\"Computes the Mean Squared Error.\"\"\"\n        error = X_scaled @ w - y\n        return np.mean(error**2)\n\n    def gradient(w, X_scaled, y):\n        \"\"\"Computes the full-batch gradient of the MSE loss.\"\"\"\n        N_samples = X_scaled.shape[0]\n        error = X_scaled @ w - y\n        return (2 / N_samples) * X_scaled.T @ error\n\n    # 3. Experiment Runner\n    def run_experiment(optimizer_type, X_base, y_base, curriculum, T, hyperparams):\n        \"\"\"\n        Runs an optimization experiment for a given optimizer and curriculum.\n        Returns the average adaptation steps.\n        \"\"\"\n        w = np.zeros(d)\n        tau = 2.0 * (sigma**2)\n        \n        # Optimizer state\n        if optimizer_type == 'adam':\n            alpha, beta1, beta2, epsilon = hyperparams\n            m = np.zeros(d)\n            v = np.zeros(d)\n            t_global = 0\n        else: # gd\n            alpha, = hyperparams\n\n        phase_adaptation_steps = []\n\n        for s in curriculum:\n            X_scaled = X_base.copy()\n            X_scaled[:, 1] *= s\n\n            # Check loss at the beginning of the phase\n            initial_loss = mse_loss(w, X_scaled, y_base)\n            if initial_loss = tau:\n                phase_adaptation_steps.append(0)\n                continue\n\n            steps_taken_in_phase = T\n            for step in range(1, T + 1):\n                grad = gradient(w, X_scaled, y_base)\n\n                if optimizer_type == 'gd':\n                    w -= alpha * grad\n                else:  # adam\n                    t_global += 1\n                    m = beta1 * m + (1 - beta1) * grad\n                    v = beta2 * v + (1 - beta2) * (grad**2)\n                    \n                    # Bias correction\n                    m_hat = m / (1 - beta1**t_global)\n                    v_hat = v / (1 - beta2**t_global)\n                    \n                    w -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n                \n                current_loss = mse_loss(w, X_scaled, y_base)\n                if current_loss = tau:\n                    steps_taken_in_phase = step\n                    break\n            \n            phase_adaptation_steps.append(steps_taken_in_phase)\n\n        return np.mean(phase_adaptation_steps)\n\n    # 4. Hyperparameters and Test Suite\n    hyperparams_gd = (0.001,)\n    hyperparams_adam = (0.01, 0.9, 0.999, 1e-8)\n\n    test_cases = [\n        {'name': 'Case A', 'scales': [1.0, 10.0, 100.0], 'T': 400},\n        {'name': 'Case B', 'scales': [1.0, 0.1, 10.0], 'T': 400},\n        {'name': 'Case C', 'scales': [1.0], 'T': 200},\n    ]\n\n    ratios = []\n    for case in test_cases:\n        curriculum = case['scales']\n        T = case['T']\n\n        avg_steps_gd = run_experiment('gd', X, y, curriculum, T, hyperparams_gd)\n        avg_steps_adam = run_experiment('adam', X, y, curriculum, T, hyperparams_adam)\n\n        if avg_steps_adam == 0.0:\n            # If Adam takes 0 steps, it's infinitely efficient. Ratio is 1 if GD also takes 0.\n            ratio = 1.0 if avg_steps_gd == 0.0 else np.inf\n        else:\n            ratio = avg_steps_gd / avg_steps_adam\n        \n        ratios.append(ratio)\n    \n    # 5. Final Output\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```", "id": "3096105"}, {"introduction": "尽管 Adam 功能强大，但它并非万能的，在某些非凸问题中可能会收敛到次优解。这个练习将引导你构建一个巧妙的反例，展示 Adam 如何被罕见的大梯度“欺骗”，导致其二阶矩估计 $v_t$ 急剧增大，从而陷入局部最小值，而简单的 SGD 却能成功“逃逸”。通过这个实践，你将对 Adam 优化器的内在机制及其潜在局限性有更深刻的批判性理解。", "problem": "要求您构建并分析一个非凸玩具优化问题，用以说明自适应矩估计 (Adam) 如何可能收敛到次优盆地，而随机梯度下降 (SGD) 却能逃逸，其定性原因与二阶矩累积量 $v_t$ 引起的方向变化有关。您的工作必须以一个完整的、可运行的程序来呈现，该程序在同一个确定性非凸目标上实现 SGD 和 Adam 两种算法，并在一个小型测试套件上比较它们的行为。\n\n从以下基本框架开始：使用源自负梯度方向的迭代一阶更新，对一个关于一维参数 $x$ 的可微标量目标 $L$ 进行最小化。使用指数移动平均的标准定义来实现 Adam 的自适应缩放，无需在此写下其闭式更新公式；您的实现应与 Adam 的广泛实践（包括偏差校正）保持一致。对于 SGD，使用固定的学习率。\n\n将非凸目标定义为\n$$\nL(x) = (x^2 - 1)^2 - c\\,x,\n$$\n其中倾斜参数 $c = 0.3$。其精确梯度为\n$$\n\\nabla L(x) = 4x(x^2 - 1) - c.\n$$\n为了模拟一个能够产生方向变化的、具有结构化可变性的确定性微批次梯度估计器，请在精确梯度上添加一个周期性扰动序列。对于每个迭代索引 $t \\in \\{0,1,2,\\dots\\}$，定义周期 $p = 10$ 和一个扰动\n$$\n\\delta_t = \\begin{cases}\n-a,  \\text{if } t \\bmod p \\neq p-1,\\\\\n+b,  \\text{if } t \\bmod p = p-1,\n\\end{cases}\n$$\n使得梯度估计器为 $g_t(x) = \\nabla L(x) + \\delta_t$。直观上，如果 $b$ 相对于 $a$ 足够大，那么许多小的负向推动加上偶尔出现的大正向尖峰会在一个周期内产生正的平均值，但 Adam 的 $v_t$ 会缩小尖峰的影响并改变净方向。\n\n在此 $g_t(x)$ 上实现这两种优化器，具体细节如下：\n- 使用 $T$ 次迭代，并根据每个测试用例的规定初始化 $x_0$。\n- 对于 SGD，使用恒定的步长 $\\eta$。\n- 对于 Adam，使用步长 $\\alpha$、一阶矩参数 $\\beta_1$、二阶矩参数 $\\beta_2$ 和数值稳定器 $\\varepsilon$。对一阶矩和二阶矩都进行标准的偏差校正。\n\n对于每次运行，通过其符号将最终迭代值 $x_T$ 划分到某个盆地中：如果 $x_T  0$ 则为左盆地（对于给定的 $c$ 是次优的），如果 $x_T > 0$ 则为右盆地（对于给定的 $c$ 是更优的盆地）。对于每个测试用例，输出一个布尔值，指示 Adam 是否最终停在左盆地而 SGD 最终停在右盆地。\n\n测试套件：\n使用以下四组参数集来测试此行为的不同方面。所有情况均使用 $c = 0.3$、周期 $p = 10$、噪声 $a = 0.03$、尖峰 $b = 4.0$ 以及如上定义的扰动。\n\n- 案例 1（正常路径）：$x_0 = 0.1$，$T = 4000$，SGD: $\\eta = 0.01$，Adam: $\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$\\varepsilon = 10^{-8}$。\n- 案例 2（从一个看似平稳的区域开始）：$x_0 = 0.0$，$T = 4000$，SGD: $\\eta = 0.01$，Adam: $\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$\\varepsilon = 10^{-8}$。\n- 案例 3（从略为负值的位置开始）：$x_0 = -0.05$，$T = 4000$，SGD: $\\eta = 0.01$，Adam: $\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$\\varepsilon = 10^{-8}$。\n- 案例 4（改变 $v_t$ 记忆的边界条件）：$x_0 = 0.1$，$T = 4000$，SGD: $\\eta = 0.01$，Adam: $\\alpha = 0.01$，$\\beta_1 = 0.9$，$\\beta_2 = 0.9$，$\\varepsilon = 10^{-8}$。\n\n最终输出规范：\n您的程序应产生单行输出，其中按顺序包含四个测试用例的布尔结果，形式为方括号括起来的逗号分隔列表（例如，`[True,False,True,True]`）。不应打印任何额外文本。在此问题中，所有数字都没有单位。不使用角度。不使用百分比；任何小数部分均如上文指定以小数形式出现。", "solution": "该问题是有效的。它提出了一个在数值优化领域中定义明确的计算任务，该任务基于成熟的原理和算法。其目标是构建并分析一个特定场景，在该场景中 Adam 优化器收敛到次优解，而标准随机梯度下降 (SGD) 找到了一个更优的解。这是一个已知的现象，并且该问题提供了一个精确、确定性的框架来复现它。所有参数和条件都已明确说明，使得该问题自成体系、无歧义且可通过计算验证。\n\n问题的核心在于一个非凸目标函数 $L(x)$ 和一个特殊构造的梯度估计器 $g_t(x)$ 之间的相互作用。\n\n目标函数是一个倾斜的双阱势：\n$$\nL(x) = (x^2 - 1)^2 - c\\,x\n$$\n倾斜参数为 $c = 0.3$。该函数有两个局部最小值。项 $-c\\,x$ (其中 $c0$) 使得正 $x$ 处的最小值为全局最小值（“更优”盆地），而负 $x$ 处的最小值为一个次优的局部最小值（“左”盆地）。任务是观察一个从 $x=0$ 附近开始的优化器能否导航到 $x0$ 的更优盆地。该函数的梯度是：\n$$\n\\nabla L(x) = 4x(x^2 - 1) - c\n$$\n\n优化器不使用真实梯度，而是使用扰动梯度 $g_t(x) = \\nabla L(x) + \\delta_t$。扰动 $\\delta_t$ 是确定性的，周期为 $p=10$。在每 10 次迭代中，有 9 次它提供一个小的负向推动 $\\delta_t = -a = -0.03$。在第 10 次迭代时，它提供一个大的正向尖峰 $\\delta_t = +b = 4.0$。一个周期内的平均扰动为 $\\frac{9(-a) + b}{p} = \\frac{9(-0.03) + 4.0}{10} = 0.373$，其值为正。\n\n我们将实现并比较两种一阶优化算法：\n\n1.  **随机梯度下降 (SGD)**：更新规则是在负梯度方向上以固定学习率 $\\eta$ 进行的简单一步：\n    $$\n    x_{t+1} = x_t - \\eta g_t(x_t)\n    $$\n    对于 SGD，更新方向完全由当前梯度估计 $g_t(x_t)$ 的符号决定。虽然单次更新可能指向不同方向，但多次迭代的累积效应决定了轨迹。对一个周期内的位移求和（对于小 $x$），九次向右的推动的净效应超过了一次向左的推动，从而引导 SGD 朝向右盆地 ($x0$)。\n\n2.  **自适应矩估计 (Adam)**：Adam 根据梯度的的一阶矩和二阶矩的估计值来为每个参数自适应地调整学习率。在第 $t$ 次迭代中，更新涉及以下步骤：\n    -   计算梯度估计：$g_t = g_t(x_{t-1})$\n    -   更新有偏一阶矩（均值）：$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$\n    -   更新有偏二阶矩（非中心方差）：$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$\n    -   计算偏差校正后的矩：$\\hat{m}_t = m_t / (1 - \\beta_1^t)$ 和 $\\hat{v}_t = v_t / (1 - \\beta_2^t)$\n    -   更新参数：$x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}$\n\n在此问题中，Adam 行为的关键在于二阶矩累积量 $v_t$。大的正向梯度尖峰（$\\delta_t = +b$ 时的 $g_t$）会产生一个非常大的 $g_t^2$ 值。这会使 $v_t$ 膨胀。由于 $\\beta_2$ 的值很高（例如 0.999），这种膨胀会持续很多次后续迭代，导致有效学习率 $\\alpha/(\\sqrt{\\hat{v}_t} + \\varepsilon)$ 变得非常小。尖峰本身对应的更新会将 $x$ 推向左侧。随后的九次更新（通常会将 $x$ 推向右侧）的幅度被严重抑制。因此，单次大的向左推动主导了九次微小的向右推动，导致 Adam 收敛到次优的左盆地 ($x0$)。\n\n案例 4 通过设置 $\\beta_2=0.9$ 来检验这一假设。由于二阶矩的记忆更短，$v_t$ 在尖峰后衰减得更快。学习率的抑制是短暂的，这使得随后的向右推动能产生更大的影响。这应该足以让 Adam 克服单次向左的推动，像 SGD 一样逃逸到右盆地。\n\n程序将实现这两种算法，并对四个指定的测试用例运行它们，然后为每个案例确定条件（Adam 收敛到左盆地且 SGD 收敛到右盆地）是否满足。这需要为每个优化器模拟 $T=4000$ 步，然后对最终参数值 $x_T$ 的符号进行分类。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the optimization problem for the given test suite.\n    \"\"\"\n\n    # --- Problem Definitions ---\n\n    def grad_L(x, c):\n        \"\"\"Computes the exact gradient of the objective function L(x).\"\"\"\n        return 4.0 * x * (x**2 - 1.0) - c\n\n    def delta_t(t, p, a, b):\n        \"\"\"Computes the deterministic perturbation at iteration index t.\"\"\"\n        if (t % p) == (p - 1):\n            return b\n        else:\n            return -a\n\n    def run_sgd(x0, T, eta, c, p, a, b):\n        \"\"\"Runs the SGD optimization.\"\"\"\n        x = float(x0)\n        for t in range(T):\n            g_t = grad_L(x, c) + delta_t(t, p, a, b)\n            x -= eta * g_t\n        return x\n\n    def run_adam(x0, T, alpha, beta1, beta2, epsilon, c, p, a, b):\n        \"\"\"Runs the Adam optimization.\"\"\"\n        x = float(x0)\n        m = 0.0\n        v = 0.0\n        \n        # Use iterative updates for powers to maintain numerical stability for large T\n        beta1_power = 1.0\n        beta2_power = 1.0\n\n        for t in range(T):\n            g_t = grad_L(x, c) + delta_t(t, p, a, b)\n            \n            m = beta1 * m + (1.0 - beta1) * g_t\n            v = beta2 * v + (1.0 - beta2) * (g_t**2)\n            \n            # Iteration number is t+1 (1-indexed)\n            t_iter = t + 1\n            beta1_power = beta1**t_iter\n            beta2_power = beta2**t_iter\n\n            m_hat = m / (1.0 - beta1_power)\n            v_hat = v / (1.0 - beta2_power)\n            \n            update = alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n            x -= update\n            \n        return x\n\n    # --- Test Execution ---\n    \n    # Common parameters from the problem statement\n    c_param = 0.3\n    p_param = 10\n    a_param = 0.03\n    b_param = 4.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (happy path)\n        {'x0': 0.1, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 2: (starting at a stationary-looking region)\n        {'x0': 0.0, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 3: (slightly negative start)\n        {'x0': -0.05, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.999, 'adam_eps': 1e-8},\n        # Case 4: (boundary condition altering v_t memory)\n        {'x0': 0.1, 'T': 4000, 'sgd_eta': 0.01, 'adam_alpha': 0.01, 'adam_beta1': 0.9, 'adam_beta2': 0.9, 'adam_eps': 1e-8},\n    ]\n\n    results = []\n    for case in test_cases:\n        # Run SGD simulation\n        x_sgd = run_sgd(case['x0'], case['T'], case['sgd_eta'], c_param, p_param, a_param, b_param)\n        \n        # Run Adam simulation\n        x_adam = run_adam(case['x0'], case['T'], case['adam_alpha'], case['adam_beta1'], case['adam_beta2'], case['adam_eps'], c_param, p_param, a_param, b_param)\n        \n        # Classify basins based on the sign of the final iterate\n        adam_in_left_basin = x_adam  0\n        sgd_in_right_basin = x_sgd > 0\n        \n        # Determine if the specified condition is met\n        condition_met = adam_in_left_basin and sgd_in_right_basin\n        results.append(condition_met)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3096082"}, {"introduction": "在探索了 Adam 的核心特性和局限性之后，让我们将其应用于一个更实际的机器学习问题：稀疏恢复。本练习要求你在一个带有光滑 $\\ell_1$ 正则化的线性回归任务中，比较 Adam 和 SGD 在识别真实稀疏模式方面的效率。这个实践将理论与应用相结合，让你评估不同优化策略在实现特定建模目标（如稀疏性）时的相对优劣。", "problem": "本题要求您通过经验性评估，在给定相同梯度计算预算的情况下，自适应矩估计 (Adam) 是否比随机梯度下降 (SGD) 更能加速稀疏恢复。此项比较必须在一个合成的线性回归任务上进行，该任务使用一个明确指定的对 $\\ell_1$ 正则化器的平滑近似。\n\n请从以下基本概念开始：\n- 经验风险最小化旨在选择参数 $\\mathbf{w} \\in \\mathbb{R}^d$ 以最小化样本平均损失及正则化项。\n- 对于线性模型，平方误差损失是一个经过充分检验的目标函数：对于数据 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ 和响应 $\\mathbf{y} \\in \\mathbb{R}^n$，经验均方误差为 $\\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2$。\n- 稀疏性可以通过 $\\ell_1$ 型惩罚项来促进，但为了能够使用基于梯度的算法，您必须使用 $\\ell_1$ 的平滑近似。\n\n您的程序必须实现以下实验：\n1. 为每个测试用例生成一个合成数据集：\n   - 抽取一个设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其元素为独立的标准正态分布，然后将每列缩放至单位 $\\ell_2$ 范数。\n   - 构建一个 $K$-稀疏的真实参数向量 $\\mathbf{w}^\\star \\in \\mathbb{R}^d$，方法是随机均匀地选择 $K$ 个索引，并从一个量级在 $1$ 左右且符号随机的分布中抽取非零值赋给它们。其余元素为零。\n   - 生成响应 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$，其中 $\\boldsymbol{\\varepsilon}$ 的元素是独立的、均值为 $0$、标准差为 $\\sigma$ 的高斯分布。\n2. 定义目标函数\n   $$L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2},$$\n   其中惩罚项是使用函数 $w \\mapsto \\sqrt{w^2 + \\beta^2}$ 对 $\\|\\mathbf{w}\\|_1$ 的平滑近似，平滑参数为 $\\beta  0$。\n3. 使用大小为 $b$ 的小批量随机梯度从零向量初始化开始更新 $\\mathbf{w}$：\n   - 对于 SGD：在每次迭代 $t$ 时，使用小批量梯度估计进行更新 $\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\widehat{\\nabla} L(\\mathbf{w}_t)$。\n   - 对于 Adam：在每次迭代 $t$ 时，使用标准 Adam 规则和步长 $\\eta_{\\text{Adam}}$ 及固定的超参数进行更新。不要改变相对于 SGD 的梯度预算；为公平起见，两种方法必须使用相同的小批量序列，执行相同的迭代次数 $T$ 和批次大小 $b$。\n4. 定义在迭代 $t$ 时的估计支撑集为 $S_t = \\{ i \\in \\{1,\\dots,d\\} : |w_{t,i}| \\ge \\theta \\}$，真实支撑集为 $S^\\star = \\{ i : w^\\star_i \\ne 0 \\}$。\n5. 将最早恢复时间 $t_{\\text{SGD}}$ (相应地为 $t_{\\text{Adam}}$) 定义为满足 $S_t = S^\\star$ 的最小 $t \\in \\{1,2,\\dots,T\\}$（对于 SGD 和 Adam）。如果在预算内不存在这样的 $t$，则设 $t$ 为 $+\\infty$。\n6. 您的程序必须根据以下规则，判断在相同预算下 Adam 是否比 SGD 更能加速稀疏恢复：\n   - 如果至少有一种方法在预算内精确恢复了支撑集，当 $t_{\\text{Adam}} \\le t_{\\text{SGD}}$ 时，则判定 Adam 加速了恢复。\n   - 如果两种方法在预算内均未精确恢复，则计算最终迭代 $t=T$ 时的支撑集，并比较支撑集的 F1 分数，其中对于集合 $A$ 和 $B$，F1 分数定义为\n     $$\\text{F1}(A,B) = \\frac{2|A \\cap B|}{2|A \\cap B| + |A \\setminus B| + |B \\setminus A|}.$$\n     在这种情况下，如果 $\\text{F1}(S_T^{\\text{Adam}}, S^\\star)  \\text{F1}(S_T^{\\text{SGD}}, S^\\star)$，则判定 Adam 加速了恢复。\n7. 每个测试用例的随机数生成必须是确定性的。每个用例使用固定的种子以确保结果可复现。SGD 和 Adam 的小批量序列必须完全相同。\n\n实现必须遵循基本定义，不得依赖任何没有根据的捷径。特别是，平滑 $\\ell_1$ 惩罚项的梯度必须从第一性原理推导得出。\n\n测试套件：\n- 用例 1：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (50,200,5,0.05,0.05,10^{-3},20,0.05,0.01,400,0.05)$\n- 用例 2：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (60,120,8,0.2,0.1,10^{-2},30,0.03,0.01,600,0.08)$\n- 用例 3：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (40,100,4,0,0.02,10^{-3},25,0.04,0.01,300,0.04)$\n- 用例 4：$(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta) = (80,200,10,0.15,0.15,5\\times10^{-2},40,0.02,0.005,800,0.1)$\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[True,False,True,False]”），其中每个布尔值对应于上述顺序中相应测试用例的判定结果。", "solution": "该问题是一项要求进行经验性研究的计算任务，旨在比较 Adam 优化器与随机梯度下降 (SGD) 在稀疏恢复任务上的性能。该任务被表述为一个带有 $\\ell_1$ 正则化器平滑近似的线性回归问题。\n\n### 步骤1：问题验证\n\n**1.1. 提取给定条件**\n\n- **目标函数：** $L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2}$，参数为 $\\mathbf{w} \\in \\mathbb{R}^d$。\n- **合成数据生成：**\n    - 设计矩阵 $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$，其元素来自独立的 $\\mathcal{N}(0,1)$ 分布，各列被缩放至单位 $\\ell_2$ 范数。\n    - 真实参数向量 $\\mathbf{w}^\\star \\in \\mathbb{R}^d$ 是 $K$-稀疏的。$K$ 个非零位置是随机均匀选择的。非零值的量级在 1 左右，符号随机。\n    - 响应向量 $\\mathbf{y} = \\mathbf{X}\\mathbf{w}^\\star + \\boldsymbol{\\varepsilon}$，噪声 $\\boldsymbol{\\varepsilon}$ 的元素是来自 $\\mathcal{N}(0, \\sigma^2)$ 的独立同分布随机变量。\n- **优化算法：**\n    - 使用批大小为 $b$、总迭代次数为 $T$ 的小批量随机梯度。\n    - 初始化：$\\mathbf{w}_0 = \\mathbf{0}$。\n    - SGD 更新：$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\widehat{\\nabla} L(\\mathbf{w}_t)$。\n    - Adam 更新：使用步长为 $\\eta_{\\text{Adam}}$ 的标准 Adam 规则。\n    - 两种方法使用相同的小批量序列。\n- **评估标准：**\n    - 真实支撑集：$S^\\star = \\{ i : w^\\star_i \\ne 0 \\}$。\n    - 迭代 $t$ 时的估计支撑集：$S_t = \\{ i : |w_{t,i}| \\ge \\theta \\}$。\n    - 最早恢复时间 $t_{\\text{opt}}$：使得 $S_t = S^\\star$ 的最小 $t \\in \\{1,\\dots,T\\}$。若不存在这样的 $t$，则 $t_{\\text{opt}} = +\\infty$。\n- **比较规则：**\n    - 如果至少有一种方法恢复了支撑集（即 $\\min(t_{\\text{SGD}}, t_{\\text{Adam}})  +\\infty$）：若 $t_{\\text{Adam}} \\le t_{\\text{SGD}}$，则 Adam 加速恢复。\n    - 如果两种方法均未恢复：若 $\\text{F1}(S_T^{\\text{Adam}}, S^\\star) > \\text{F1}(S_T^{\\text{SGD}}, S^\\star)$，则 Adam 加速恢复。\n    - F1 分数：$\\text{F1}(A,B) = \\frac{2|A \\cap B|}{2|A \\cap B| + |A \\setminus B| + |B \\setminus A|}$。\n- **可复现性：** 每个测试用例必须使用固定的随机种子。\n- **测试用例：** 四个用例，每个都为参数元组 $(d,n,K,\\sigma,\\lambda,\\beta,b,\\eta_{\\text{SGD}},\\eta_{\\text{Adam}},T,\\theta)$ 指定了具体值。\n\n**1.2. 验证与结论**\n\n- **科学依据：** 该问题牢固地植根于统计学习、优化和稀疏方法的既定原则。所有组成部分（线性回归、$\\ell_1$ 正则化、SGD、Adam）都是标准的。对不可微的 $\\ell_1$ 范数使用平滑近似是一种众所周知的技术。\n- **良构性：** 该问题是良构的。目标明确定义，由于使用了固定种子，实验过程是确定性的，并且比较指标是明确的，涵盖了所有可能的结果。每个测试用例都存在唯一且有意义的解。\n- **客观性：** 问题以精确、定量的术语陈述，没有主观或模糊的语言。\n\n该问题没有表现出任何诸如科学上不健全、信息缺失或逻辑矛盾等缺陷。“量级在1左右”这一术语是合成数据生成中的标准惯例，可解释为从一个围绕1的均匀分布等分布中抽样，并不会使问题无效。\n\n**结论：问题有效。**\n\n### 步骤2：解法推导与算法设计\n\n问题的核心是使用两种不同的基于梯度的优化器来最小化目标函数 $L(\\mathbf{w})$，并比较它们在识别真实参数向量 $\\mathbf{w}^\\star$ 的稀疏支撑集方面的有效性。\n\n**2.1. 目标函数及其梯度**\n\n目标函数为\n$$L(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\|_2^2 + \\lambda \\sum_{i=1}^d \\sqrt{w_i^2 + \\beta^2}$$\n该函数是数据保真项 $L_{\\text{data}}(\\mathbf{w})$ 和正则化项 $L_{\\text{reg}}(\\mathbf{w})$ 的和。为了应用基于梯度的优化器，我们必须计算梯度 $\\nabla L(\\mathbf{w}) = \\nabla L_{\\text{data}}(\\mathbf{w}) + \\nabla L_{\\text{reg}}(\\mathbf{w})$。\n\n数据保真项（均方误差）的梯度为：\n$$ \\nabla L_{\\text{data}}(\\mathbf{w}) = \\frac{1}{n} \\mathbf{X}^T (\\mathbf{X}\\mathbf{w} - \\mathbf{y}) $$\n正则化项的梯度通过对每个分量 $w_j$ ($j \\in \\{1, \\dots, d\\}$) 求导得到：\n$$ \\frac{\\partial}{\\partial w_j} L_{\\text{reg}}(\\mathbf{w}) = \\lambda \\frac{\\partial}{\\partial w_j} \\sqrt{w_j^2 + \\beta^2} = \\lambda \\frac{1}{2\\sqrt{w_j^2 + \\beta^2}} (2w_j) = \\frac{\\lambda w_j}{\\sqrt{w_j^2 + \\beta^2}} $$\n正则化项的完整梯度是一个向量，其每个分量由上述表达式给出。这可以使用逐元素操作紧凑地写出：\n$$ \\nabla L_{\\text{reg}}(\\mathbf{w}) = \\lambda \\frac{\\mathbf{w}}{\\sqrt{\\mathbf{w}^2 + \\beta^2}} $$\n其中平方和平方根是逐元素应用的。由于 $\\beta > 0$，分母始终为严格正数，确保梯度是良定义的。\n\n**2.2. 随机小批量梯度**\n\n在实践中，对于大的 $n$，我们使用在数据小批量上计算的梯度随机近似。设 $I \\subset \\{1, \\dots, n\\}$ 是一个大小为 $b$ 的索引集，定义了一个小批量。随机梯度 $\\widehat{\\nabla} L(\\mathbf{w})$ 为：\n$$ \\widehat{\\nabla} L(\\mathbf{w}) = \\frac{1}{b} \\mathbf{X}_I^T (\\mathbf{X}_I \\mathbf{w} - \\mathbf{y}_I) + \\lambda \\frac{\\mathbf{w}}{\\sqrt{\\mathbf{w}^2 + \\beta^2}} $$\n其中 $\\mathbf{X}_I$ 和 $\\mathbf{y}_I$ 分别是 $\\mathbf{X}$ 的行和 $\\mathbf{y}$ 的元素，对应于索引集 $I$。梯度的正则化部分是使用完整参数向量 $\\mathbf{w}$ 计算的，不是随机的。\n\n**2.3. 优化算法**\n\n两种算法都从 $\\mathbf{w}_0 = \\mathbf{0}$ 开始，运行 $T$ 次迭代。在每次迭代 $t=0, 1, \\dots, T-1$ 中，它们使用相同的小批量计算梯度 $\\mathbf{g}_t = \\widehat{\\nabla} L(\\mathbf{w}_t)$。\n\n- **随机梯度下降 (SGD):** 更新规则是在负梯度方向上简单地迈出一步：\n$$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{SGD}} \\cdot \\mathbf{g}_t $$\n\n- **自适应矩估计 (Adam):** Adam 维护梯度及其平方的移动平均值。设 $\\beta_1=0.9$, $\\beta_2=0.999$ 和 $\\epsilon=10^{-8}$ 为标准超参数。\n    1. 初始化一阶矩向量 $\\mathbf{m}_0 = \\mathbf{0}$ 和二阶矩向量 $\\mathbf{v}_0 = \\mathbf{0}$。\n    2. 更新有偏矩估计：\n    $$ \\mathbf{m}_{t+1} = \\beta_1 \\mathbf{m}_t + (1 - \\beta_1) \\mathbf{g}_t $$\n    $$ \\mathbf{v}_{t+1} = \\beta_2 \\mathbf{v}_t + (1 - \\beta_2) \\mathbf{g}_t^2 \\quad \\text{(逐元素平方)} $$\n    3. 计算偏差校正后的矩估计：\n    $$ \\hat{\\mathbf{m}}_{t+1} = \\frac{\\mathbf{m}_{t+1}}{1 - \\beta_1^{t+1}} $$\n    $$ \\hat{\\mathbf{v}}_{t+1} = \\frac{\\mathbf{v}_{t+1}}{1 - \\beta_2^{t+1}} $$\n    4. 更新参数：\n    $$ \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_{\\text{Adam}} \\frac{\\hat{\\mathbf{m}}_{t+1}}{\\sqrt{\\hat{\\mathbf{v}}_{t+1}} + \\epsilon} $$\n\n**2.4. 实验流程与评估**\n\n对于每个测试用例，执行以下流程：\n1.  **设置：** 使用特定的随机种子以确保可复现性。\n2.  **数据生成：** 按照规定生成合成数据集 $(\\mathbf{X}, \\mathbf{y})$ 和真实稀疏向量 $\\mathbf{w}^\\star$。记录真实支撑集 $S^\\star$。\n3.  **迭代：** SGD 和 Adam 并行运行 $T$ 次迭代。在每次迭代 $t$ 中，使用相同的小批量为两个更新计算梯度。\n4.  **支撑集恢复跟踪：** 每次参数更新（生成 $\\mathbf{w}_{t+1}$）后，通过用 $\\theta$ 对权重的绝对值进行阈值处理来计算估计的支撑集 $S_{t+1}$。将其与真实支撑集 $S^\\star$ 进行比较。如果它们匹配，则记录恢复时间（迭代次数 $t+1$）。这对两个优化器都进行。初始恢复时间设置为 $+\\infty$。\n5.  **比较：** 在 $T$ 次迭代后，根据指定的规则做出最终决定：\n    - 如果至少有一个优化器找到了支撑集，则恢复时间更小（对 Adam 而言或相等）的那个获胜。\n    - 如果都没有找到支撑集，则使用它们最终估计的支撑集相对于真实支撑集的 F1 分数来比较它们的性能。F1 分数较高的优化器获胜。\n对于估计支撑集 $S$ 和真实支撑集 $S^\\star$，F1 分数的计算方式如下：\n$$ \\text{F1}(S, S^\\star) = \\frac{2|S \\cap S^\\star|}{|S| + |S^\\star|} $$\n这与问题陈述中给出的公式是等价的。返回一个布尔值，指示是否认为 Adam 相对于 SGD 加速了恢复。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (50, 200, 5, 0.05, 0.05, 1e-3, 20, 0.05, 0.01, 400, 0.05),\n        (60, 120, 8, 0.2, 0.1, 1e-2, 30, 0.03, 0.01, 600, 0.08),\n        (40, 100, 4, 0, 0.02, 1e-3, 25, 0.04, 0.01, 300, 0.04),\n        (80, 200, 10, 0.15, 0.15, 5e-2, 40, 0.02, 0.005, 800, 0.1),\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        decision = solve_one_case(params, seed=i)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_one_case(params, seed):\n    \"\"\"\n    Executes the experiment for a single set of parameters.\n    \"\"\"\n    d, n, K, sigma, lamb, beta, b, eta_sgd, eta_adam, T, theta = params\n    \n    # Use a fixed seed for each case for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic dataset\n    X = rng.standard_normal(size=(n, d))\n    X /= np.linalg.norm(X, axis=0, keepdims=True)\n\n    w_star = np.zeros(d)\n    support_indices = rng.choice(d, K, replace=False)\n    signs = rng.choice([-1, 1], K)\n    magnitudes = rng.uniform(0.5, 1.5, K)\n    w_star[support_indices] = signs * magnitudes\n    \n    S_star_set = set(support_indices)\n\n    noise = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = X @ w_star + noise\n\n    # Pre-generate mini-batch indices for all T iterations to ensure fairness\n    mini_batch_indices = [rng.choice(n, b, replace=False) for _ in range(T)]\n\n    # Initialize parameters for both optimizers\n    w_sgd = np.zeros(d)\n    w_adam = np.zeros(d)\n    \n    # Adam-specific parameters\n    m = np.zeros(d)\n    v = np.zeros(d)\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    t_sgd_recovery = np.inf\n    t_adam_recovery = np.inf\n\n    for t in range(T):\n        # Get the same mini-batch for both optimizers\n        batch_idx = mini_batch_indices[t]\n        X_b, y_b = X[batch_idx], y[batch_idx]\n\n        # --- SGD Step ---\n        grad_data_sgd = (1/b) * X_b.T @ (X_b @ w_sgd - y_b)\n        grad_reg_sgd = lamb * w_sgd / np.sqrt(w_sgd**2 + beta**2)\n        grad_sgd = grad_data_sgd + grad_reg_sgd\n        w_sgd -= eta_sgd * grad_sgd\n        \n        # --- Adam Step ---\n        grad_data_adam = (1/b) * X_b.T @ (X_b @ w_adam - y_b)\n        grad_reg_adam = lamb * w_adam / np.sqrt(w_adam**2 + beta**2)\n        grad_adam = grad_data_adam + grad_reg_adam\n\n        m = beta1 * m + (1 - beta1) * grad_adam\n        v = beta2 * v + (1 - beta2) * (grad_adam**2)\n        \n        # Bias correction\n        t_iter = t + 1\n        m_hat = m / (1 - beta1**t_iter)\n        v_hat = v / (1 - beta2**t_iter)\n        \n        w_adam -= eta_adam * m_hat / (np.sqrt(v_hat) + epsilon)\n\n        # --- Evaluation at iteration t+1 ---\n        # Check for SGD recovery\n        if np.isinf(t_sgd_recovery):\n            S_sgd_current = set(np.where(np.abs(w_sgd) >= theta)[0])\n            if S_sgd_current == S_star_set:\n                t_sgd_recovery = t + 1\n        \n        # Check for Adam recovery\n        if np.isinf(t_adam_recovery):\n            S_adam_current = set(np.where(np.abs(w_adam) >= theta)[0])\n            if S_adam_current == S_star_set:\n                t_adam_recovery = t + 1\n\n    # Final comparison logic\n    if min(t_sgd_recovery, t_adam_recovery)  np.inf:\n        return t_adam_recovery = t_sgd_recovery\n    else:\n        # Neither recovered, compare F1 scores at the final iteration\n        S_T_sgd = set(np.where(np.abs(w_sgd) >= theta)[0])\n        S_T_adam = set(np.where(np.abs(w_adam) >= theta)[0])\n        \n        def f1_score(A, B):\n            intersection_size = len(A.intersection(B))\n            if len(A) == 0 and len(B) == 0:\n                return 1.0\n            denominator = len(A) + len(B)\n            if denominator == 0:\n                return 0.0\n            \n            return 2.0 * intersection_size / denominator\n\n        f1_sgd = f1_score(S_T_sgd, S_star_set)\n        f1_adam = f1_score(S_T_adam, S_star_set)\n        \n        return f1_adam > f1_sgd\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3096054"}]}