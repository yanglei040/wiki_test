{"hands_on_practices": [{"introduction": "理解学习率对优化过程的影响是深度学习实践的基石。本练习通过一个简化的二次目标函数，让你亲手探索梯度下降法的核心动态。你将通过编写代码来模拟和分析，当学习率与损失函数的曲率（由其Hessian矩阵的特征值体现）相互作用时，优化过程如何表现出稳定收敛、振荡或发散等不同行为，从而为你调整学习率提供坚实的理论直觉。 [@problem_id:3154406]", "problem": "考虑二次目标函数 $f(x) = \\tfrac{1}{2} x^\\top H x$，其中 $H \\in \\mathbb{R}^{d \\times d}$ 是一个实对称矩阵（海森矩阵），且 $x \\in \\mathbb{R}^d$。使用恒定学习率 $\\eta > 0$ 的梯度下降法应用更新 $x_{t+1} = x_t - \\eta \\nabla f(x_t)$，其中迭代由 $t \\in \\{0,1,2,\\dots\\}$ 索引。在二次型情况下，这会产生一个线性迭代 $x_{t+1} = (I - \\eta H) x_t$。迭代量 $x_t$ 的行为（收敛、振荡或发散）取决于学习率 $\\eta$ 和 $H$ 的特征值之间的相互作用。\n\n您的任务是编写一个程序，该程序针对一组给定的矩阵 $H$ 和学习率 $\\eta$ 的小型测试集，将执行以下操作：\n- 从指定的初始点 $x_0$ 开始，使用更新规则 $x_{t+1} = x_t - \\eta H x_t$ 模拟梯度下降，进行 $T$ 次迭代。\n- 计算迭代算子 $I - \\eta H$ 的谱半径，其定义为 $I - \\eta H$ 特征值的最大绝对值。\n- 通过计算比率 $\\|x_T\\|_2 / \\|x_0\\|_2$ 来量化范数的经验变化。\n- 使用一个由谱半径确定的整数代码对行为进行分类：\n  - $0$：如果谱半径严格小于 $1$，\n  - $1$：如果谱半径在小的数值容差内等于 $1$，\n  - $2$：如果谱半径大于 $1$。\n\n使用以下测试集。对于所有情况，使用 $x_0 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$ 和 $T = 60$。\n- 情况 A（稳定，理想路径）：$H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.3$。\n- 情况 B（边界条件）：$H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = \\tfrac{2}{3}$。\n- 情况 C（超出边界）：$H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.9$。\n- 情况 D（带有负特征值的非凸边缘情况）：$H = \\begin{bmatrix}-1  0 \\\\ 0  4\\end{bmatrix}$，$\\eta = 0.3$。\n- 情况 E（病态但仍在边界内）：$H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.19$。\n- 情况 F（病态且超出边界）：$H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.21$。\n\n实现细节和输出：\n- 对于每种情况，计算 $H$ 的特征值 $\\{\\lambda_i\\}$，然后计算 $I - \\eta H$ 的谱半径为 $\\max_i |1 - \\eta \\lambda_i|$。\n- 模拟迭代 $x_{t+1} = x_t - \\eta H x_t$（其中 $t = 0,1,\\dots,T-1$），并计算比率 $r_{\\text{emp}} = \\|x_T\\|_2 / \\|x_0\\|_2$。\n- 使用上述整数代码对行为进行分类，该代码由 $I - \\eta H$ 的谱半径确定。\n- 每种情况的最终输出是一个列表，其中按顺序包含谱半径（一个浮点数）、经验比率 $r_{\\text{emp}}$（一个浮点数）和分类代码（一个整数）。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有六种情况的结果，形式为用方括号括起来的逗号分隔列表，其中每种情况贡献一个形式为 $[\\text{spectral\\_radius}, \\text{empirical\\_ratio}, \\text{classification}]$ 的列表。例如，格式为 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots,[a_6,b_6,c_6]]$。", "solution": "该问题要求分析将梯度下降算法应用于二次目标函数 $f(x) = \\frac{1}{2} x^\\top H x$ 时的行为。该分析涉及基于海森矩阵 $H$ 和学习率 $\\eta$ 的性质的理论计算，以及迭代过程的经验模拟。\n\n问题的核心在于理解将梯度下降应用于此特定目标函数所产生的线性动力系统。该函数的梯度为 $\\nabla f(x) = Hx$。梯度下降的更新规则由下式给出：\n$$\nx_{t+1} = x_t - \\eta \\nabla f(x_t)\n$$\n代入梯度，我们得到一个线性递推关系：\n$$\nx_{t+1} = x_t - \\eta H x_t = (I - \\eta H) x_t\n$$\n其中 $I$ 是单位矩阵，$G = I - \\eta H$ 是迭代算子。这个递推关系的解是 $x_t = G^t x_0$。迭代量 $x_t$ 的长期行为由矩阵 $G$ 的性质决定。\n\n为了使迭代序列 $x_t$ 收敛到零向量（如果 $H$ 是正定的，零向量是唯一最小值），其充分必要条件是迭代矩阵 $G$ 的谱半径严格小于 $1$。谱半径，记作 $\\rho(G)$，定义为 $G$ 的特征值的最大绝对值。\n$$\n\\rho(G) = \\max_{i} |\\lambda_i(G)|\n$$\n其中 $\\lambda_i(G)$ 是 $G$ 的特征值。\n\n如果 $\\lambda_i(H)$ 是海森矩阵 $H$ 的特征值，那么 $G = I - \\eta H$ 的特征值由 $1 - \\eta \\lambda_i(H)$ 给出。因此，谱半径可以直接由 $H$ 的特征值和学习率 $\\eta$ 计算得出：\n$$\n\\rho(I - \\eta H) = \\max_{i} |1 - \\eta \\lambda_i(H)|\n$$\n梯度下降算法的行为可以根据 $\\rho(I - \\eta H)$ 的值进行分类：\n- 如果 $\\rho(I - \\eta H)  1$，迭代量 $\\|x_t\\|_2$ 的范数将衰减到 $0$。这是收敛的条件。\n- 如果 $\\rho(I - \\eta H) = 1$，迭代量的范数可能保持有界（振荡），或者如果大小为 $1$ 的特征值具有大于 $1$ 的若尔当块，则范数可能多项式增长。在这个问题中，由于 $H$ 是对称的，$G$ 也是对称的，并且所有若尔当块的大小都为 $1$，因此范数不会发散。\n- 如果 $\\rho(I - \\eta H) > 1$，迭代量 $\\|x_t\\|_2$ 的范数将指数级增长，该方法发散。\n\n对于一个凸问题，其中 $H$ 是正定的（所有 $\\lambda_i(H) > 0$），收敛条件 $\\rho(I - \\eta H)  1$ 简化为找到一个 $\\eta$ 使得对于所有 $i$ 都有 $|1 - \\eta \\lambda_i(H)|  1$。这个不等式等价于 $-1  1 - \\eta \\lambda_i(H)  1$，这意味着 $0  \\eta \\lambda_i(H)  2$。这必须对所有特征值 $\\lambda_i(H)$ 都成立，这导出了学习率的著名条件：$0  \\eta  \\frac{2}{\\lambda_{\\max}(H)}$，其中 $\\lambda_{\\max}(H)$ 是 $H$ 的最大特征值。\n\n范数的经验比率 $\\|x_T\\|_2 / \\|x_0\\|_2$ 预计与谱半径相关。对于对称的 $G$，其诱导 2-范数等于谱半径，即 $\\|G\\|_2 = \\rho(G)$。因此，我们有界 $\\|x_t\\|_2 \\le \\|G\\|_2^t \\|x_0\\|_2 = \\rho(G)^t \\|x_0\\|_2$。因此，比率 $\\|x_t\\|_2 / \\|x_0\\|_2$ 的上界是 $\\rho(G)^t$。实际值取决于初始向量 $x_0$ 在 $G$ 的特征空间上的投影。\n\n我们现在分析每个测试用例，其初始向量为 $x_0 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$，迭代次数为 $T=60$。\n\n**情况 A：** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.3$。\n- $H$ 的特征值：$\\lambda_1 = 3$，$\\lambda_2 = 1$。\n- $I - \\eta H$ 的特征值：$1 - 0.3 \\cdot 3 = 0.1$ 和 $1 - 0.3 \\cdot 1 = 0.7$。\n- 谱半径：$\\rho = \\max(|0.1|, |0.7|) = 0.7$。\n- 分类：由于 $\\rho = 0.7  1$，分类为 $0$（收敛）。\n- 经验比率预计会很小。\n\n**情况 B：** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = \\frac{2}{3}$。\n- $H$ 的特征值：$\\lambda_1 = 3$，$\\lambda_2 = 1$。\n- $I - \\eta H$ 的特征值：$1 - \\frac{2}{3} \\cdot 3 = -1$ 和 $1 - \\frac{2}{3} \\cdot 1 = \\frac{1}{3}$。\n- 谱半径：$\\rho = \\max(|-1|, |\\frac{1}{3}|) = 1.0$。\n- 分类：由于 $\\rho=1.0$，分类为 $1$（边界）。误差向量的一个分量会振荡，而另一个分量会衰减。\n\n**情况 C：** $H = \\begin{bmatrix}3  0 \\\\ 0  1\\end{bmatrix}$，$\\eta = 0.9$。\n- $H$ 的特征值：$\\lambda_1 = 3$，$\\lambda_2 = 1$。\n- $I - \\eta H$ 的特征值：$1 - 0.9 \\cdot 3 = -1.7$ 和 $1 - 0.9 \\cdot 1 = 0.1$。\n- 谱半径：$\\rho = \\max(|-1.7|, |0.1|) = 1.7$。\n- 分类：由于 $\\rho=1.7 > 1$，分类为 $2$（发散）。迭代量将呈指数增长。\n\n**情况 D：** $H = \\begin{bmatrix}-1  0 \\\\ 0  4\\end{bmatrix}$，$\\eta = 0.3$。\n- 该目标函数是非凸的，因为 $H$ 有一个负特征值。\n- $H$ 的特征值：$\\lambda_1 = -1$，$\\lambda_2 = 4$。\n- $I - \\eta H$ 的特征值：$1 - 0.3 \\cdot (-1) = 1.3$ 和 $1 - 0.3 \\cdot 4 = -0.2$。\n- 谱半径：$\\rho = \\max(|1.3|, |-0.2|) = 1.3$。\n- 分类：由于 $\\rho=1.3 > 1$，分类为 $2$（发散）。梯度下降将会发散，远离原点的鞍点。\n\n**情况 E：** $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.19$。\n- 该矩阵是病态的，其条件数为 $\\kappa = \\frac{10}{0.1} = 100$。理论上的学习率上限是 $\\frac{2}{\\lambda_{\\max}} = \\frac{2}{10} = 0.2$。\n- $H$ 的特征值：$\\lambda_1 = 10$，$\\lambda_2 = 0.1$。\n- $I - \\eta H$ 的特征值：$1 - 0.19 \\cdot 10 = -0.9$ 和 $1 - 0.19 \\cdot 0.1 = 0.981$。\n- 谱半径：$\\rho = \\max(|-0.9|, |0.981|) = 0.981$。\n- 分类：由于 $\\rho = 0.981  1$，分类为 $0$（收敛）。然而，由于谱半径非常接近 $1$，收敛会很慢。\n\n**情况 F：** $H = \\begin{bmatrix}10  0 \\\\ 0  0.1\\end{bmatrix}$，$\\eta = 0.21$。\n- 与情况 E 相同的病态矩阵，但学习率 $\\eta=0.21$ 略高于 $0.2$ 的稳定性阈值。\n- $H$ 的特征值：$\\lambda_1 = 10$，$\\lambda_2 = 0.1$。\n- $I - \\eta H$ 的特征值：$1 - 0.21 \\cdot 10 = -1.1$ 和 $1 - 0.21 \\cdot 0.1 = 0.979$。\n- 谱半径：$\\rho = \\max(|-1.1|, |0.979|) = 1.1$。\n- 分类：由于 $\\rho=1.1 > 1$，分类为 $2$（发散）。这表明梯度下降对学习率的敏感性，尤其是在病态问题中。\n\n以下程序将实现这些计算和模拟。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the behavior of gradient descent on a quadratic objective function\n    for a given suite of test cases.\n\n    For each case, it computes:\n    1. The spectral radius of the iteration operator.\n    2. The empirical ratio of norms after T iterations.\n    3. A classification code based on the spectral radius.\n    \"\"\"\n    \n    # Define the six test cases from the problem statement.\n    test_cases = [\n        # Case A: stable, happy path\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 0.3},\n        # Case B: boundary condition\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 2.0 / 3.0},\n        # Case C: over the bound\n        {'H': np.array([[3.0, 0.0], [0.0, 1.0]]), 'eta': 0.9},\n        # Case D: nonconvex edge case\n        {'H': np.array([[-1.0, 0.0], [0.0, 4.0]]), 'eta': 0.3},\n        # Case E: ill-conditioned but under the bound\n        {'H': np.array([[10.0, 0.0], [0.0, 0.1]]), 'eta': 0.19},\n        # Case F: ill-conditioned and over the bound\n        {'H': np.array([[10.0, 0.0], [0.0, 0.1]]), 'eta': 0.21}\n    ]\n\n    # Shared parameters for all simulations\n    x0 = np.array([1.0, -1.0])\n    T = 60\n    norm_x0 = np.linalg.norm(x0)\n    \n    all_results = []\n    \n    for case in test_cases:\n        H = case['H']\n        eta = case['eta']\n        \n        # 1. Compute the spectral radius of the iteration operator G = I - eta*H\n        # Since H is symmetric, its eigenvalues are real. We use eigvalsh for stability.\n        # The eigenvalues of G are 1 - eta * lambda_i(H).\n        eigenvalues_H = np.linalg.eigvalsh(H)\n        eigenvalues_G = 1.0 - eta * eigenvalues_H\n        spectral_radius = np.max(np.abs(eigenvalues_G))\n        \n        # 2. Simulate gradient descent for T iterations\n        x_t = x0.copy()\n        for _ in range(T):\n            # Update rule: x_{t+1} = x_t - eta * H * x_t\n            x_t = x_t - eta * (H @ x_t)\n            \n        # 3. Compute the empirical change in norm\n        norm_xT = np.linalg.norm(x_t)\n        empirical_ratio = norm_xT / norm_x0\n        \n        # 4. Classify the behavior based on the spectral radius\n        classification = 0  # Default to convergence\n        if np.isclose(spectral_radius, 1.0):\n            classification = 1  # Boundary condition\n        elif spectral_radius > 1.0:\n            classification = 2  # Divergence\n            \n        all_results.append([spectral_radius, empirical_ratio, classification])\n\n    # Format the final output string as a list of lists.\n    # e.g., [[valA1,valA2,valA3],[valB1,valB2,valB3],...]\n    result_strings = []\n    for res in all_results:\n        # Format each sublist to '[val1,val2,val3]' without extra spaces\n        formatted_res = f\"[{res[0]},{res[1]},{res[2]}]\"\n        result_strings.append(formatted_res)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3154406"}, {"introduction": "Adam是目前深度学习领域最主流的优化器之一，但其实际应用中的一些技巧，如梯度累积，会微妙地改变其内部状态。本练习将引导你深入探索Adam优化器的“引擎盖下”，比较标准逐次更新与梯度累积更新两种策略对其内部动量和二阶矩估计的影响。通过量化这两种方案的差异，你将更深刻地理解Adam的偏置校正机制以及它在处理梯度时的动态特性，这对于在资源受限的环境下进行高效训练至关重要。 [@problem_id:3154393]", "problem": "考虑自适应矩估计 (Adam) 优化器，它维护随机梯度的一阶矩和二阶矩的指数移动平均值。设梯度序列表示为 $\\{g_t\\}_{t=1}^T$，其中 $T$ 是一个正整数。对于给定的动量参数 $\\beta_1 \\in (0,1)$ 和 $\\beta_2 \\in (0,1)$，零初始化的一阶矩和二阶矩递推关系定义如下\n$$\nm_t \\leftarrow \\beta_1 m_{t-1} + (1 - \\beta_1) g_t, \\quad v_t \\leftarrow \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2,\n$$\n其中 $m_0 = 0$ 且 $v_0 = 0$。偏差修正后的矩为\n$$\n\\hat{m}_t \\leftarrow \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t \\leftarrow \\frac{v_t}{1 - \\beta_2^t}.\n$$\n现在定义一个梯度累积方案，其累积频率为 $k \\in \\mathbb{N}$，且 $k$ 能整除 $T$。将 $T$ 个梯度划分为 $B = T/k$ 个长度为 $k$ 的连续块。对于块索引 $b \\in \\{1,\\dots,B\\}$，定义块平均梯度为\n$$\n\\bar{g}_b \\;=\\; \\frac{1}{k} \\sum_{i=(b-1)k+1}^{bk} g_i.\n$$\n在梯度累积下，优化器仅在块边界处更新，Adam 矩的递推关系是每个块更新一次，如下所示：\n$$\nm_b^{\\mathrm{A}} \\leftarrow \\beta_1 m_{b-1}^{\\mathrm{A}} + (1 - \\beta_1)\\bar{g}_b,\\quad\nv_b^{\\mathrm{A}} \\leftarrow \\beta_2 v_{b-1}^{\\mathrm{A}} + (1 - \\beta_2)\\bar{g}_b^2,\n$$\n其中 $m_0^{\\mathrm{A}} = 0$ 且 $v_0^{\\mathrm{A}} = 0$。相应的偏差修正后的矩为：\n$$\n\\hat{m}_b^{\\mathrm{A}} \\leftarrow \\frac{m_b^{\\mathrm{A}}}{1 - \\beta_1^b}, \\quad \\hat{v}_b^{\\mathrm{A}} \\leftarrow \\frac{v_b^{\\mathrm{A}}}{1 - \\beta_2^b}.\n$$\n与此相反，每步更新方案（无累积）在每个时间步 $t$ 应用标准的 Adam 递推关系。为了在同一时间轴上比较两种方案的优化器内部状态，我们仅在块边界 $t \\in \\{k, 2k, \\dots, T\\}$ 处评估每步更新方案的偏差修正后的矩，并将其与累积方案在 $b \\in \\{1,\\dots,B\\}$ 处的偏差修正后的矩进行比较，其中 $t = bk$ 与 $b$ 对应。\n\n任务：对于下面每个指定的测试用例，计算两种方案在一阶和二阶偏差修正矩之间、在所有块边界上的最大绝对差：\n$$\nD_m \\;=\\; \\max_{b \\in \\{1,\\dots,B\\}} \\left| \\hat{m}_{bk}^{\\mathrm{U}} - \\hat{m}_b^{\\mathrm{A}} \\right|, \\quad\nD_v \\;=\\; \\max_{b \\in \\{1,\\dots,B\\}} \\left| \\hat{v}_{bk}^{\\mathrm{U}} - \\hat{v}_b^{\\mathrm{A}} \\right|,\n$$\n其中 $\\hat{m}_{t}^{\\mathrm{U}}$ 和 $\\hat{v}_{t}^{\\mathrm{U}}$ 是来自每步更新方案的偏差修正矩，而 $\\hat{m}_b^{\\mathrm{A}}$ 和 $\\hat{v}_b^{\\mathrm{A}}$ 是来自累积方案的偏差修正矩。\n\n输入是隐式的：您的程序必须实现以上定义，并为以下每个测试用例计算 $(D_m, D_v)$。本问题不涉及物理单位。当使用三角函数时，角度以弧度为单位。\n\n测试套件：\n- 用例 1（恒定梯度，理想情况）：$T = 20$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$k = 4$，对于所有 $t \\in \\{1,\\dots,20\\}$，$g_t = 1$。\n- 用例 2（交替符号，压力测试二阶矩）：$T = 20$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$k = 2$，当 $t$ 为奇数时 $g_t = 1$，当 $t$ 为偶数时 $g_t = -1$。\n- 用例 3（线性变化梯度）：$T = 30$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$k = 3$，对于 $t \\in \\{1,\\dots,30\\}$，$g_t = \\frac{t}{T}$。\n- 用例 4（边界条件 $k=1$）：$T = 25$，$\\beta_1 = 0.9$，$\\beta_2 = 0.999$，$k = 1$，对于 $t \\in \\{1,\\dots,25\\}$，$g_t = \\sin(t)$，其中 $t$ 以弧度为单位。\n\n输出规范：\n- 对于每个用例 $i \\in \\{1,2,3,4\\}$，计算 $(D_m^{(i)}, D_v^{(i)})$。\n- 您的程序应生成单行输出，其中包含按以下顺序排列的、以逗号分隔的浮点数列表：\n$$\n\\left[ D_m^{(1)}, D_v^{(1)}, D_m^{(2)}, D_v^{(2)}, D_m^{(3)}, D_v^{(3)}, D_m^{(4)}, D_v^{(4)} \\right].\n$$\n不应打印任何其他文本。", "solution": "该问题要求对 Adam 优化器在两种不同更新策略下的内部状态进行对比分析：一种是标准的每步更新方案，另一种是梯度累积方案。目标是量化这两种方案在同步时间点上所生成的偏差修正后的一阶和二阶矩估计值之间的最大差异。\n\n该分析的关键在于为两种方案精确地实现指定的递推关系，然后计算其最大绝对差。我们来将此过程形式化。\n\n首先，我们为任一给定的测试用例定义问题参数：总步数 $T$、动量参数 $\\beta_1$ 和 $\\beta_2$、累积频率 $k$ 以及梯度序列 $\\{g_t\\}_{t=1}^T$。\n\n第一种方案是 **每步更新方案**，我们用上标 U 表示。这是 Adam 的标准应用。一阶矩 $m_t^{\\mathrm{U}}$ 和二阶矩 $v_t^{\\mathrm{U}}$ 在每个时间步 $t \\in \\{1, \\dots, T\\}$ 计算。以 $m_0^{\\mathrm{U}} = 0$ 和 $v_0^{\\mathrm{U}} = 0$ 初始化，递推关系如下：\n$$\nm_t^{\\mathrm{U}} \\leftarrow \\beta_1 m_{t-1}^{\\mathrm{U}} + (1 - \\beta_1) g_t\n$$\n$$\nv_t^{\\mathrm{U}} \\leftarrow \\beta_2 v_{t-1}^{\\mathrm{U}} + (1 - \\beta_2) g_t^2\n$$\n偏差修正后的矩计算如下：\n$$\n\\hat{m}_t^{\\mathrm{U}} \\leftarrow \\frac{m_t^{\\mathrm{U}}}{1 - \\beta_1^t}\n$$\n$$\n\\hat{v}_t^{\\mathrm{U}} \\leftarrow \\frac{v_t^{\\mathrm{U}}}{1 - \\beta_2^t}\n$$\n\n第二种方案是 **梯度累积方案**，用上标 A 表示。在此方案中，梯度会计算 $k$ 步，但优化器的内部状态每 $k$ 步组成的块只更新一次。总更新次数为 $B = T/k$。对于每个块 $b \\in \\{1, \\dots, B\\}$，我们首先计算块平均梯度：\n$$\n\\bar{g}_b = \\frac{1}{k} \\sum_{i=(b-1)k+1}^{bk} g_i\n$$\n然后使用此平均梯度更新优化器的矩。以 $m_0^{\\mathrm{A}} = 0$ 和 $v_0^{\\mathrm{A}} = 0$ 初始化，对于 $b \\in \\{1, \\dots, B\\}$ 的递推关系如下：\n$$\nm_b^{\\mathrm{A}} \\leftarrow \\beta_1 m_{b-1}^{\\mathrm{A}} + (1 - \\beta_1)\\bar{g}_b\n$$\n$$\nv_b^{\\mathrm{A}} \\leftarrow \\beta_2 v_{b-1}^{\\mathrm{A}} + (1 - \\beta_2)\\bar{g}_b^2\n$$\n一个关键的细节是，此方案的偏差修正取决于优化器的更新次数 $b$，而不是全局时间步 $t$。偏差修正后的矩为：\n$$\n\\hat{m}_b^{\\mathrm{A}} \\leftarrow \\frac{m_b^{\\mathrm{A}}}{1 - \\beta_1^b}\n$$\n$$\n\\hat{v}_b^{\\mathrm{A}} \\leftarrow \\frac{v_b^{\\mathrm{A}}}{1 - \\beta_2^b}\n$$\n\n为了比较这两种方案，我们在共同的时间点上评估它们的状态。这些点是块边界，出现在时间步 $t = bk$ 处，其中 $b \\in \\{1, \\dots, B\\}$。我们将方案 U 在时间 $t=bk$ 的状态（即 $\\hat{m}_{bk}^{\\mathrm{U}}$ 和 $\\hat{v}_{bk}^{\\mathrm{U}}$）与方案 A 在其第 $b$ 次更新后的状态（即 $\\hat{m}_b^{\\mathrm{A}}$ 和 $\\hat{v}_b^{\\mathrm{A}}$）进行比较。\n\n最终任务是计算一阶矩和二阶矩在所有块边界上的最大绝对差。这些量用 $D_m$ 和 $D_v$ 表示：\n$$\nD_m = \\max_{b \\in \\{1,\\dots,B\\}} \\left| \\hat{m}_{bk}^{\\mathrm{U}} - \\hat{m}_b^{\\mathrm{A}} \\right|\n$$\n$$\nD_v = \\max_{b \\in \\{1,\\dots,B\\}} \\left| \\hat{v}_{bk}^{\\mathrm{U}} - \\hat{v}_b^{\\mathrm{A}} \\right|\n$$\n\n解决每个测试用例的算法如下：\n1.  根据测试用例规范生成完整的梯度序列 $\\{g_t\\}_{t=1}^T$。\n2.  **对于每步更新方案 (U)：**\n    a. 初始化 $m_0^{\\mathrm{U}} = 0$ 和 $v_0^{\\mathrm{U}} = 0$。\n    b. 从 $t=1$ 迭代到 $T$，计算并存储整个序列 $\\{m_t^{\\mathrm{U}}\\}$ 和 $\\{v_t^{\\mathrm{U}}\\}$。\n    c. 确定块边界时间步：$t_{b} = bk$，其中 $b=1, \\dots, B$。\n    d. 在这些时间步上，计算偏差修正后的矩 $\\hat{m}_{t_b}^{\\mathrm{U}}$ 和 $\\hat{v}_{t_b}^{\\mathrm{U}}$。\n3.  **对于梯度累积方案 (A)：**\n    a. 将梯度序列 $\\{g_t\\}$ 划分为 $B$ 个大小为 $k$ 的块，并计算块平均梯度序列 $\\{\\bar{g}_b\\}_{b=1}^B$。\n    b. 初始化 $m_0^{\\mathrm{A}} = 0$ 和 $v_0^{\\mathrm{A}} = 0$。\n    c. 从 $b=1$ 迭代到 $B$，计算序列 $\\{m_b^{\\mathrm{A}}\\}$ 和 $\\{v_b^{\\mathrm{A}}\\}$。\n    d. 在同一个循环中，计算偏差修正后的矩 $\\hat{m}_b^{\\mathrm{A}}$ 和 $\\hat{v}_b^{\\mathrm{A}}$。\n4.  **计算差异：**\n    a. 对于每个块索引 $b=1, \\dots, B$，计算绝对差 $|\\hat{m}_{bk}^{\\mathrm{U}} - \\hat{m}_b^{\\mathrm{A}}|$ 和 $|\\hat{v}_{bk}^{\\mathrm{U}} - \\hat{v}_b^{\\mathrm{A}}|$。\n    b. 该测试用例的最终结果 $D_m$ 和 $D_v$ 分别是在各自的绝对差序列中找到的最大值。\n\n此过程将应用于四个指定的测试用例中的每一个。值得注意的是，在用例 4 中，$k=1$，此时累积方案与每步更新方案完全相同。块平均梯度 $\\bar{g}_b$ 就是 $g_b$，块的数量 $B$ 等于 $T$，并且偏差修正的指数（$t=bk=b$ 对比 $b$）也完全一致。因此，对于 $k=1$ 的情况，我们逻辑上必然会得出 $D_m = 0$ 和 $D_v = 0$。这可以作为对实现的一个有价值的合理性检查。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Adam optimizer comparison problem for all specified test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: constant gradients\n        {'T': 20, 'beta1': 0.9, 'beta2': 0.999, 'k': 4, 'g_func_key': 'constant'},\n        # Case 2: alternating sign gradients\n        {'T': 20, 'beta1': 0.9, 'beta2': 0.999, 'k': 2, 'g_func_key': 'alternating'},\n        # Case 3: linearly varying gradients\n        {'T': 30, 'beta1': 0.9, 'beta2': 0.999, 'k': 3, 'g_func_key': 'linear'},\n        # Case 4: boundary condition k=1\n        {'T': 25, 'beta1': 0.9, 'beta2': 0.999, 'k': 1, 'g_func_key': 'sin'},\n    ]\n\n    results = []\n    for case in test_cases:\n        Dm, Dv = compute_differences(**case)\n        results.append(Dm)\n        results.append(Dv)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_differences(T, beta1, beta2, k, g_func_key):\n    \"\"\"\n    Computes the maximum absolute differences D_m and D_v for a single test case.\n\n    Args:\n        T (int): Total number of time steps.\n        beta1 (float): Exponential decay rate for the first moment estimates.\n        beta2 (float): Exponential decay rate for the second moment estimates.\n        k (int): Gradient accumulation frequency.\n        g_func_key (str): Key to select the gradient generation function.\n\n    Returns:\n        tuple[float, float]: A tuple containing (D_m, D_v).\n    \"\"\"\n    # Generate the gradient sequence using 1-based indexing for mathematical consistency.\n    t_math = np.arange(1, T + 1)\n    if g_func_key == 'constant':\n        g = np.ones(T)\n    elif g_func_key == 'alternating':\n        g = (-1)**(t_math - 1)\n    elif g_func_key == 'linear':\n        g = t_math / T\n    elif g_func_key == 'sin':\n        g = np.sin(t_math)\n    else:\n        # This case should not be reached with the given problem statement.\n        raise ValueError(\"Invalid g_func_key specified.\")\n\n    # --- Scheme U: Update-Every-Step ---\n    m_U = np.zeros(T + 1)\n    v_U = np.zeros(T + 1)\n    for t in range(1, T + 1):\n        # Python g array is 0-indexed, so we use t-1\n        m_U[t] = beta1 * m_U[t - 1] + (1 - beta1) * g[t - 1]\n        v_U[t] = beta2 * v_U[t - 1] + (1 - beta2) * (g[t - 1]**2)\n\n    # --- Scheme A: Gradient Accumulation ---\n    B = T // k\n    \n    # Reshape gradient vector to (B, k) and compute mean over the k dimension.\n    g_reshaped = g.reshape((B, k))\n    g_bar = g_reshaped.mean(axis=1)\n\n    m_A = np.zeros(B + 1)\n    v_A = np.zeros(B + 1)\n    for b in range(1, B + 1):\n        # Python g_bar array is 0-indexed, so we use b-1\n        m_A[b] = beta1 * m_A[b - 1] + (1 - beta1) * g_bar[b - 1]\n        v_A[b] = beta2 * v_A[b - 1] + (1 - beta2) * (g_bar[b - 1]**2)\n\n    # --- Comparison at Block Boundaries ---\n    \n    # Indices for U scheme correspond to t = bk\n    block_boundaries_t = np.arange(k, T + 1, k)\n    # Indices for A scheme correspond to b\n    block_indices_b = np.arange(1, B + 1)\n    \n    # Check if number of boundaries is consistent\n    if len(block_boundaries_t) != B:\n        # This is another sanity check that should not fail given T % k == 0\n        raise ValueError(\"Mismatch in number of block boundaries.\")\n        \n    # Get moments at boundaries\n    m_U_at_boundaries = m_U[block_boundaries_t]\n    v_U_at_boundaries = v_U[block_boundaries_t]\n    m_A_at_boundaries = m_A[1:] # m_A[0] is initial state\n    v_A_at_boundaries = v_A[1:] # v_A[0] is initial state\n\n    # Bias-correct the moments.\n    # For U, the exponent in the correction term is t = bk.\n    # For A, the exponent is b.\n    m_hat_U = m_U_at_boundaries / (1 - beta1**block_boundaries_t)\n    v_hat_U = v_U_at_boundaries / (1 - beta2**block_boundaries_t)\n    \n    # np.power might be more robust for large exponents, but ** is sufficient here.\n    m_hat_A = m_A_at_boundaries / (1 - beta1**block_indices_b)\n    v_hat_A = v_A_at_boundaries / (1 - beta2**block_indices_b)\n\n    # --- Compute Maximum Absolute Differences ---\n    D_m = np.max(np.abs(m_hat_U - m_hat_A))\n    D_v = np.max(np.abs(v_hat_U - v_hat_A))\n\n    return D_m, D_v\n\n# Execute the main function to produce the final output.\nsolve()\n```", "id": "3154393"}, {"introduction": "虽然Adam优化器功能强大，但研究人员仍在不断探索其改进方案，以应对复杂和充满噪声的损失函数。本练习让你扮演研究者的角色，在一个特意构建的、具有尖锐梯度的非凸目标函数上，对比标准Adam和其变体AdaBelief的性能。你将学习如何通过定量指标来评估优化器的稳定性，并直观地看到AdaBelief如何通过其独特的“信念”机制来平滑训练过程，从而加深对自适应优化器设计哲学的理解。 [@problem_id:3154379]", "problem": "你需要实现并比较两种自适应优化方法，在一个特意构建的、具有尖锐梯度的一维目标函数上，并定量评估在随机噪声下的训练稳定性。这两种方法是自适应矩估计（Adam）和 AdaBelief，后者是 Adam 的一个变体，其二阶累加器基于当前梯度与其一阶矩估计（优化器的“信念”）之间的偏差。\n\n构建目标函数\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,x^2 \\;+\\; s\\bigl(1 - \\cos(\\omega x)\\bigr) \\;+\\; \\tfrac{1}{2}\\,s\\bigl(1 - \\cos(3\\,\\omega x)\\bigr),\n$$\n其中 $x \\in \\mathbb{R}$，$s > 0$，$ \\omega > 0$。该选择确保了在中等大小的 $s$ 值下，在 $x = 0$ 附近存在唯一的极小值点，而梯度\n$$\n\\nabla f(x) \\;=\\; x \\;+\\; s\\,\\omega\\,\\sin(\\omega x) \\;+\\; \\tfrac{3}{2}\\,s\\,\\omega\\,\\sin(3\\,\\omega x)\n$$\n则表现出由 $\\omega$ 控制的高频尖峰。训练动态是随机的：使用带噪声的梯度 $g_t = \\nabla f(x_t) + \\xi_t$，其中 $\\xi_t$ 是从零均值、方差为 $\\sigma^2$ 的高斯分布中抽取的独立样本，即 $\\xi_t \\sim \\mathcal{N}(0,\\sigma^2)$。\n\n从指定的初始点 $x_0$ 开始，在相同的已实现的噪声序列 $\\{\\xi_t\\}_{t=1}^T$ 上对每个优化器运行 $T$ 次更新，以确保公平比较。使用一阶和二阶项的标准偏差校正指数移动平均来实现这两种优化器；除了方法的标准定义外，不要硬编码任何闭式解或简化表达式。使用固定的超参数 $\\beta_1 = 0.9$，$\\beta_2 = 0.999$ 和 $\\varepsilon = 10^{-8}$。\n\n为每个测试用例的每个优化器定义并计算以下指标：\n- 最终目标函数值 $f(x_T)$。\n- 平均更新幅度的平方\n$$\n\\overline{\\Delta^2} \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} (x_{t} - x_{t-1})^2.\n$$\n- 振荡次数，定义为索引对 $(t-1,t)$ 的数量，其中 $t \\in \\{1,\\dots,T\\}$，使得 $x_{t-1}$ 和 $x_t$ 符号相反（即乘积 $x_{t-1} x_t  0$）。该指标衡量穿越 $x=0$ 直线的次数。\n\n对于每个测试用例，生成一个布尔值结果，指明在带噪声的梯度下 AdaBelief 是否比 Adam 更好地稳定了训练。如果以下所有条件同时成立，则声明 AdaBelief “更稳定”：\n- 其平均更新幅度的平方严格小于 Adam 的，即 $\\overline{\\Delta^2}_{\\text{AdaBelief}}  \\overline{\\Delta^2}_{\\text{Adam}}$（允许 $10^{-12}$ 的数值容差）。\n- 其振荡次数小于或等于 Adam 的。\n- 其最终目标函数值不比 Adam 的差超过 $10^{-3}$ 的容差，即 $f(x_T)_{\\text{AdaBelief}} \\le f(x_T)_{\\text{Adam}} + 10^{-3}$。\n\n使用以下测试套件；每个案例指定 $(\\omega, s, \\sigma, \\alpha, T, \\text{seed}, x_0)$:\n- 案例 1：$\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.1$, $\\alpha = 0.01$, $T = 300$, $\\text{seed} = 42$, $x_0 = 2.0$。\n- 案例 2：$\\omega = 60.0$, $s = 0.6$, $\\sigma = 0.3$, $\\alpha = 0.005$, $T = 500$, $\\text{seed} = 123$, $x_0 = -3.0$。\n- 案例 3（边界情况，无噪声）：$\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.0$, $\\alpha = 0.005$, $T = 300$, $\\text{seed} = 7$, $x_0 = 1.0$。\n- 案例 4（高噪声）：$\\omega = 25.0$, $s = 0.4$, $\\sigma = 0.5$, $\\alpha = 0.01$, $T = 500$, $\\text{seed} = 99$, $x_0 = 2.5$。\n\n你的程序应产生单行输出，其中包含一个用方括号括起来的、逗号分隔的结果列表（例如，\"[true,true,true,true]\"），其中每个条目都是一个布尔值，表示 AdaBelief 对于相应测试用例是否比 Adam 更稳定。不应打印任何额外文本。", "solution": "该问题要求对两种自适应优化算法 Adam 和 AdaBelief，在一个指定的一维非凸目标函数上，在随机噪声环境下进行比较分析。目标是根据一组定量指标，确定哪种优化器表现出更高的稳定性。解决方案的步骤是：首先定义问题的数学组成部分，然后详细说明优化器的实现和评估指标，最后对每个测试用例执行此过程。\n\n### 1. 问题描述\n\n需要最小化的目标函数由下式给出：\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\,x^2 \\;+\\; s\\bigl(1 - \\cos(\\omega x)\\bigr) \\;+\\; \\tfrac{1}{2}\\,s\\bigl(1 - \\cos(3\\,\\omega x)\\bigr)\n$$\n其中 $x \\in \\mathbb{R}$ 是待优化的参数，$s > 0$ 和 $\\omega > 0$ 是控制函数形状的常数。第一项 $\\tfrac{1}{2}x^2$ 提供了一个凸的基础，而余弦项则引入了高频的非凸振荡。\n\n该函数关于 $x$ 的梯度是：\n$$\n\\nabla f(x) \\;=\\; x \\;+\\; s\\,\\omega\\,\\sin(\\omega x) \\;+\\; \\tfrac{3}{2}\\,s\\,\\omega\\,\\sin(3\\,\\omega x)\n$$\n优化在随机设置下进行。在每个时间步 $t$，优化器无法获得真实的梯度 $\\nabla f(x_t)$，而是其带噪声的版本：\n$$\ng_t = \\nabla f(x_t) + \\xi_t\n$$\n其中 $x_t$ 是第 $t$ 步的参数值，$\\xi_t$ 是从零均值高斯分布中独立采样的噪声项，$\\xi_t \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n### 2. 优化器算法\n\nAdam 和 AdaBelief 都是自适应学习率方法，它们使用梯度的指数移动平均（一阶矩）和梯度平方的指数移动平均（二阶矩）来为每个参数调整学习率。优化从 $x_0$ 初始化，运行 $T$ 步。固定的超参数为 $\\beta_1 = 0.9$，$\\beta_2 = 0.999$ 和 $\\varepsilon = 10^{-8}$。学习率用 $\\alpha$ 表示。\n\n在从 $1$ 到 $T$ 的每一步 $t$：\n1.  计算带噪声的梯度 $g_t = \\nabla f(x_{t-1}) + \\xi_t$。\n2.  更新一阶矩估计 $m_t$：\n    $$\n    m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n    $$\n3.  更新二阶矩估计 $v_t$。这一步是两种算法的区别所在。\n    -   **Adam**：二阶矩是梯度平方的指数移动平均值。\n        $$\n        v_t^{\\text{Adam}} = \\beta_2 v_{t-1}^{\\text{Adam}} + (1 - \\beta_2) g_t^2\n        $$\n    -   **AdaBelief**：二阶矩是当前梯度 $g_t$ 与其一阶矩估计 $m_t$ 之间差值平方的指数移动平均值。这就是对当前梯度的“信念”。\n        $$\n        v_t^{\\text{AdaBelief}} = \\beta_2 v_{t-1}^{\\text{AdaBelief}} + (1 - \\beta_2) (g_t - m_t)^2\n        $$\n4.  计算偏差校正后的矩估计。由于 $m_0$ 和 $v_0$ 初始化为 $0$，估计值会偏向于零，尤其是在初始步骤中。该偏差按如下方式校正：\n    $$\n    \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n    $$\n    $$\n    \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n    $$\n5.  更新参数 $x_t$：\n    $$\n    x_t = x_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\varepsilon}\n    $$\n    $\\varepsilon$ 项是一个小常数，用于防止除以零。\n\n### 3. 评估指标与稳定性判据\n\n为了定量比较 Adam 和 AdaBelief 的稳定性，为每个优化器的轨迹 $\\{x_t\\}_{t=0}^T$ 计算三个指标：\n\n-   **最终目标函数值 $f(x_T)$**：在最终参数 $x_T$ 处的目标函数值。\n-   **平均更新幅度的平方 $\\overline{\\Delta^2}$**：该指标衡量优化过程中的平均步长，计算如下：\n    $$\n    \\overline{\\Delta^2} \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} (x_{t} - x_{t-1})^2\n    $$\n    较小的值表明收敛过程更平滑、更稳定。\n-   **振荡次数**：参数轨迹穿越 $x=0$ 直线的次数。这被计为满足 $x_{t-1} \\cdot x_t  0$ 的索引 $t \\in \\{1, \\dots, T\\}$ 的数量。较少的振荡次数表明通向最小值的路径更直接。\n\n当且仅当以下三个条件全部满足时，AdaBelief 被宣告比 Adam “更稳定”：\n1.  $\\overline{\\Delta^2}_{\\text{AdaBelief}}  \\overline{\\Delta^2}_{\\text{Adam}}$（对于严格不等式，有 $10^{-12}$ 的数值容差）。\n2.  $\\text{oscillation\\_count}_{\\text{AdaBelief}} \\le \\text{oscillation\\_count}_{\\text{Adam}}$。\n3.  $f(x_T)_{\\text{AdaBelief}} \\le f(x_T)_{\\text{Adam}} + 10^{-3}$，意味着其最终目标函数值不显著差于 Adam 的。\n\n### 4. 实现策略\n\n对于由参数 $(\\omega, s, \\sigma, \\alpha, T, \\text{seed}, x_0)$ 定义的每个测试用例，执行以下过程：\n\n1.  使用指定的 `seed` 和标准差 $\\sigma$，生成一个单一的高斯噪声样本序列 $\\{\\xi_t\\}_{t=1}^T$。对两个优化器使用相同的噪声序列，对于公平比较它们的动态至关重要。\n2.  执行两次独立的优化运行，一次用于 Adam，一次用于 AdaBelief，两者都从 $x_0$ 开始，并使用相同的学习率 $\\alpha$ 和噪声序列 $\\{\\xi_t\\}_{t=1}^T$。\n3.  在每次运行期间，存储参数的整个轨迹 $\\{x_0, x_1, \\dots, x_T\\}$。\n4.  两次运行都完成后，根据各自的轨迹为每个优化器计算三个评估指标（$f(x_T)$、$\\overline{\\Delta^2}$ 和振荡次数）。\n5.  将这些指标与三个稳定性条件进行比较，为该测试用例生成一个单一的布尔值结果。\n6.  对所有四个测试用例重复此过程，并将最终结果编译成一个列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_optimization_and_compare(w, s, sig, alpha, T, seed, x0):\n    \"\"\"\n    Runs Adam and AdaBelief optimizers for a given test case and compares them\n    based on the specified stability criteria.\n    \"\"\"\n    BETA1 = 0.9\n    BETA2 = 0.999\n    EPSILON = 1e-8\n\n    # Define objective function and its gradient\n    f = lambda x: 0.5 * x**2 + s * (1 - np.cos(w * x)) + 0.5 * s * (1 - np.cos(3 * w * x))\n    grad_f = lambda x: x + s * w * np.sin(w * x) + 1.5 * s * w * np.sin(3 * w * x)\n\n    # Generate a single noise sequence for both optimizers for a fair comparison\n    rng = np.random.default_rng(seed)\n    noise = rng.normal(loc=0.0, scale=sig, size=T)\n\n    # --- Run Adam Optimizer ---\n    x_adam_traj = np.zeros(T + 1)\n    x_adam_traj[0] = x0\n    m_adam, v_adam = 0.0, 0.0\n    for t in range(1, T + 1):\n        # Current parameter value is from the previous step\n        current_x = x_adam_traj[t-1]\n        \n        # Calculate stochastic gradient\n        g = grad_f(current_x) + noise[t-1]\n        \n        # Update moments\n        m_adam = BETA1 * m_adam + (1 - BETA1) * g\n        v_adam = BETA2 * v_adam + (1 - BETA2) * (g**2)\n        \n        # Bias correction\n        m_hat = m_adam / (1 - BETA1**t)\n        v_hat = v_adam / (1 - BETA2**t)\n        \n        # Update parameter\n        x_adam_traj[t] = current_x - alpha * m_hat / (np.sqrt(v_hat) + EPSILON)\n\n    # --- Run AdaBelief Optimizer ---\n    x_adabelief_traj = np.zeros(T + 1)\n    x_adabelief_traj[0] = x0\n    m_adabelief, v_adabelief = 0.0, 0.0\n    for t in range(1, T + 1):\n        current_x = x_adabelief_traj[t-1]\n        \n        g = grad_f(current_x) + noise[t-1]\n        \n        m_adabelief = BETA1 * m_adabelief + (1 - BETA1) * g\n        # The key difference: second moment uses (g - m)^2\n        v_adabelief = BETA2 * v_adabelief + (1 - BETA2) * ((g - m_adabelief)**2)\n        \n        m_hat = m_adabelief / (1 - BETA1**t)\n        v_hat = v_adabelief / (1 - BETA2**t)\n        \n        x_adabelief_traj[t] = current_x - alpha * m_hat / (np.sqrt(v_hat) + EPSILON)\n\n    # --- Calculate Metrics for Adam ---\n    f_T_adam = f(x_adam_traj[T])\n    delta_sq_avg_adam = np.mean((x_adam_traj[1:] - x_adam_traj[:-1])**2)\n    osc_count_adam = np.sum((x_adam_traj[:-1] * x_adam_traj[1:])  0)\n\n    # --- Calculate Metrics for AdaBelief ---\n    f_T_adabelief = f(x_adabelief_traj[T])\n    delta_sq_avg_adabelief = np.mean((x_adabelief_traj[1:] - x_adabelief_traj[:-1])**2)\n    osc_count_adabelief = np.sum((x_adabelief_traj[:-1] * x_adabelief_traj[1:])  0)\n    \n    # --- Compare metrics based on stability criteria ---\n    # 1. Avg squared update magnitude is strictly smaller (with tolerance)\n    cond1 = delta_sq_avg_adabelief  delta_sq_avg_adam - 1e-12\n    # 2. Oscillation count is less than or equal\n    cond2 = osc_count_adabelief = osc_count_adam\n    # 3. Final objective is not worse by more than a tolerance\n    cond3 = f_T_adabelief = f_T_adam + 1e-3\n\n    return cond1 and cond2 and cond3\n\ndef solve():\n    # Define the test cases as tuples of:\n    # (omega, s, sigma, alpha, T, seed, x0)\n    test_cases = [\n        # Case 1\n        (25.0, 0.4, 0.1, 0.01, 300, 42, 2.0),\n        # Case 2\n        (60.0, 0.6, 0.3, 0.005, 500, 123, -3.0),\n        # Case 3\n        (25.0, 0.4, 0.0, 0.005, 300, 7, 1.0),\n        # Case 4\n        (25.0, 0.4, 0.5, 0.01, 500, 99, 2.5),\n    ]\n\n    results = []\n    for case in test_cases:\n        omega, s, sigma, alpha, T, seed, x0 = case\n        is_more_stable = run_optimization_and_compare(omega, s, sigma, alpha, T, seed, x0)\n        results.append(str(is_more_stable).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3154379"}]}