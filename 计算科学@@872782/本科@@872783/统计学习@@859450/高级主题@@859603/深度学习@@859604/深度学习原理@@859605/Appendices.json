{"hands_on_practices": [{"introduction": "理论上，深度网络的敏感度（即输入的小扰动会否引起输出的巨大变化）与其权重矩阵的范数密切相关。这一特性，即利普希茨连续性，是衡量模型鲁棒性的关键。本实践将通过一个数值实验，让你亲手验证如何通过控制网络权重（模拟谱范数正则化），来直接影响其利普希茨常数和对输入的鲁棒性 [@problem_id:3113400]。", "problem": "考虑一个具有整流线性单元（ReLU）激活函数的全连接双层前馈网络，它将输入 $\\mathbf{x} \\in \\mathbb{R}^2$ 映射到一个标量输出。该模型为\n$$\n\\mathbf{h}(\\mathbf{x}) = \\operatorname{ReLU}\\!\\left(W_1 \\mathbf{x} + \\mathbf{b}_1\\right), \\qquad\nf(\\mathbf{x}) = W_2 \\mathbf{h}(\\mathbf{x}) + b_2,\n$$\n其中 $W_1 \\in \\mathbb{R}^{3 \\times 2}$，$\\mathbf{b}_1 \\in \\mathbb{R}^3$，$W_2 \\in \\mathbb{R}^{1 \\times 3}$，$b_2 \\in \\mathbb{R}$。设训练集为固定集合 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $n = 8$，$\\mathbf{x}_i \\in \\mathbb{R}^2$，$y_i \\in \\{-1, +1\\}$。\n\n参数的数值如下：\n- $W_1 = \\begin{bmatrix} 1.0  0.5 \\\\ -0.3  1.2 \\\\ 0.7  -0.8 \\end{bmatrix}$，$\\mathbf{b}_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，\n- $W_2 = \\begin{bmatrix} 0.9  0.7  -0.6 \\end{bmatrix}$，$b_2 = -0.1$。\n\n数据集为：\n- 正类 ($y=+1$): $\\mathbf{x}_1 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$，$\\mathbf{x}_2 = \\begin{bmatrix} 1.2 \\\\ 0.9 \\end{bmatrix}$，$\\mathbf{x}_3 = \\begin{bmatrix} 0.8 \\\\ 1.1 \\end{bmatrix}$，$\\mathbf{x}_4 = \\begin{bmatrix} 1.1 \\\\ 1.2 \\end{bmatrix}$。\n- 负类 ($y=-1$): $\\mathbf{x}_5 = \\begin{bmatrix} -1.0 \\\\ -1.0 \\end{bmatrix}$，$\\mathbf{x}_6 = \\begin{bmatrix} -1.2 \\\\ -0.9 \\end{bmatrix}$，$\\mathbf{x}_7 = \\begin{bmatrix} -0.8 \\\\ -1.1 \\end{bmatrix}$，$\\mathbf{x}_8 = \\begin{bmatrix} -1.1 \\\\ -1.2 \\end{bmatrix}$。\n\n你将通过一个由非负惩罚强度 $a \\ge 0$ 参数化的确定性重缩放规则，对线性层施加谱范数惩罚。对于给定的 $a$，将惩罚后的权重定义为\n$$\n\\widetilde{W}_1(a) = \\frac{1}{1+a}\\, W_1,\\qquad \\widetilde{W}_2(a) = \\frac{1}{1+a}\\, W_2,\n$$\n同时保持偏置不变，即 $\\widetilde{\\mathbf{b}}_1(a) = \\mathbf{b}_1$ 且 $\\widetilde{b}_2(a) = b_2$。随着 $a$ 的增加，此规则单调地减小线性映射的谱范数，反映了惩罚项 $\\sum_\\ell \\|W_\\ell\\|_2$ 的效果。\n\n你的任务是：\n1. 仅从分析学和线性代数的核心定义出发，推导网络 $f$ 的全局利普希茨常数的一个可计算上界。该上界应表示为线性映射的函数，并利用整流线性单元（ReLU）是 $1$-利普希茨这一事实。你必须使用算子范数的定义、激活函数的 $1$-利普希茨性质以及利普希茨函数的复合规则来证明该界的合理性。\n2. 使用利普希茨连续性和分类间隔的定义，推导翻转 $y_i f(\\mathbf{x}_i)$ 符号所需的最小 $\\ell_2$-范数扰动的样本级别下界。该下界仅用任务1中的利普希茨界和带符号间隔 $m_i = y_i f(\\mathbf{x}_i)$ 表示。解释为什么非正间隔的样本贡献的下界为零。\n3. 将经验 $0$-$1$ 风险（训练分类误差）定义为 $\\widehat{R}(f) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{y_i f(\\mathbf{x}_i) \\le 0\\}$。对通过上述重缩放规则得到的每个惩罚模型 $f_a$ 计算该风险。\n\n实现一个程序，对于以下测试集中的每个惩罚参数 $a$，\n- $a \\in \\{\\, 0,\\, 0.5,\\, 3,\\, 50 \\,\\}$，\n执行施加步骤以获得 $\\widetilde{W}_1(a)$ 和 $\\widetilde{W}_2(a)$，然后计算：\n- 一个由你的推导证明其合理性的 $f_a$ 的有效利普希茨上界 $L(a)$，\n- 平均样本级别鲁棒性下界 $\\frac{1}{n}\\sum_{i=1}^n r_i(a)$，其中 $r_i(a)$ 是任务2中推导出的下界，\n- 经验风险 $\\widehat{R}(f_a)$。\n\n你的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素本身也是一个列表 $[L(a), \\overline{r}(a), \\widehat{R}(f_a)]$，按此顺序对应于相应的 $a$，每个值都表示为十进制数。例如，一个包含四个测试用例的输出必须具有单行形式 $[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$。不涉及物理单位或角度单位；所有值都是实数。", "solution": "该问题被评估为有效的，因为它在科学上基于统计学习的原理，是良定的（提供了所有必要的数据），并且使用标准数学定义进行了客观的表述。\n\n此处我们介绍推导过程和计算策略。对于给定的惩罚强度 $a \\ge 0$，网络函数 $f_a: \\mathbb{R}^2 \\to \\mathbb{R}$ 由复合函数定义\n$$\nf_a(\\mathbf{x}) = g_3(g_2(g_1(\\mathbf{x})))\n$$\n其中\n- $g_1(\\mathbf{x}) = \\widetilde{W}_1(a) \\mathbf{x} + \\widetilde{\\mathbf{b}}_1(a) = \\frac{1}{1+a}W_1 \\mathbf{x} + \\mathbf{b}_1$\n- $g_2(\\mathbf{h}) = \\operatorname{ReLU}(\\mathbf{h})$\n- $g_3(\\mathbf{h}) = \\widetilde{W}_2(a) \\mathbf{h} + \\widetilde{b}_2(a) = \\frac{1}{1+a}W_2 \\mathbf{h} + b_2$\n\n所有向量范数 $\\|\\cdot\\|$ 均假定为欧几里得 $\\ell_2$-范数，矩阵范数 $\\|\\cdot\\|_2$ 为相应的诱导算子范数（谱范数）。\n\n### 任务1：全局利普希茨常数上界的推导\n\n如果对于所有 $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$，不等式 $\\|g(\\mathbf{u}) - g(\\mathbf{v})\\| \\le L \\|\\mathbf{u} - \\mathbf{v}\\|$ 成立，则函数 $g: \\mathbb{R}^m \\to \\mathbb{R}^k$ 是 $L$-利普希茨连续的。满足此条件的最小 $L$ 是 $g$ 的利普希茨常数。一个关键性质是，对于利普希茨函数的复合 $g = g_N \\circ \\dots \\circ g_1$，其复合函数也是利普希茨的，其常数 $L_g \\le L_{g_N} \\cdots L_{g_1}$。我们将此性质应用于网络。\n\n1.  **仿射映射的利普希茨常数**：设 $g(\\mathbf{x}) = W\\mathbf{x} + \\mathbf{b}$。对于任意 $\\mathbf{u}, \\mathbf{v}$，我们有\n    $$\n    \\|g(\\mathbf{u}) - g(\\mathbf{v})\\| = \\|(W\\mathbf{u} + \\mathbf{b}) - (W\\mathbf{v} + \\mathbf{b})\\| = \\|W(\\mathbf{u} - \\mathbf{v})\\|\n    $$\n    根据诱导算子范数的定义，对于任意向量 $\\mathbf{z}$，有 $\\|W\\mathbf{z}\\| \\le \\|W\\|_2 \\|\\mathbf{z}\\|$。因此，\n    $$\n    \\|g(\\mathbf{u}) - g(\\mathbf{v})\\| \\le \\|W\\|_2 \\|\\mathbf{u} - \\mathbf{v}\\|\n    $$\n    仿射映射的利普希茨常数是其线性部分的谱范数。因此，$g_1$ 和 $g_3$ 的利普希茨常数分别为 $L_{g_1} = \\|\\widetilde{W}_1(a)\\|_2$ 和 $L_{g_3} = \\|\\widetilde{W}_2(a)\\|_2$。\n\n2.  **激活函数的利普希茨常数**：问题中指出，整流线性单元 $\\operatorname{ReLU}(z) = \\max(0, z)$ 是 $1$-利普希茨的。对于逐元素向量函数 $g_2(\\mathbf{h}) = \\operatorname{ReLU}(\\mathbf{h})$，其相对于 $\\ell_2$-范数的利普希茨常数也是 $1$。为了证明这一点，\n    $$\n    \\|g_2(\\mathbf{u}) - g_2(\\mathbf{v})\\|_2^2 = \\|\\operatorname{ReLU}(\\mathbf{u}) - \\operatorname{ReLU}(\\mathbf{v})\\|_2^2 = \\sum_j (\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j))^2\n    $$\n    由于标量 ReLU 是 $1$-利普希茨的，所以有 $|\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j)| \\le |u_j - v_j|$。两边平方可得 $(\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j))^2 \\le (u_j - v_j)^2$。对所有分量求和可得\n    $$\n    \\sum_j (\\operatorname{ReLU}(u_j) - \\operatorname{ReLU}(v_j))^2 \\le \\sum_j (u_j - v_j)^2 \\implies \\|g_2(\\mathbf{u}) - g_2(\\mathbf{v})\\|_2^2 \\le \\|\\mathbf{u} - \\mathbf{v}\\|_2^2\n    $$\n    两边取平方根，我们得到 $\\|g_2(\\mathbf{u}) - g_2(\\mathbf{v})\\|_2 \\le \\|\\mathbf{u} - \\mathbf{v}\\|_2$。因此，$L_{g_2} = 1$。\n\n3.  **复合**：整个网络 $f_a$ 的利普希茨常数 $L(f_a)$ 的上界是各个函数利普希茨常数的乘积：\n    $$\n    L(f_a) \\le L_{g_3} \\cdot L_{g_2} \\cdot L_{g_1} = \\|\\widetilde{W}_2(a)\\|_2 \\cdot 1 \\cdot \\|\\widetilde{W}_1(a)\\|_2\n    $$\n    使用惩罚规则 $\\widetilde{W}_\\ell(a) = \\frac{1}{1+a}W_\\ell$ 和性质 $\\|\\lambda W\\|_2 = |\\lambda|\\|W\\|_2$，我们得到上界，并将其记为 $L(a)$：\n    $$\n    L(a) = \\left\\| \\frac{1}{1+a}W_2 \\right\\|_2 \\left\\| \\frac{1}{1+a}W_1 \\right\\|_2 = \\frac{1}{(1+a)^2} \\|W_2\\|_2 \\|W_1\\|_2\n    $$\n    这就是 $f_a$ 的利普希茨常数的可计算上界。\n\n### 任务2：样本级别鲁棒性下界的推导\n\n我们的目标是找到一个扰动 $\\mathbf{\\delta}$ 的 $\\ell_2$-范数下界，该扰动能翻转样本 $(\\mathbf{x}_i, y_i)$ 的分类结果。在原始分类正确，即 $y_i f_a(\\mathbf{x}_i)  0$ 的情况下，如果 $y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) \\le 0$，则发生符号翻转。带符号间隔为 $m_i(a) = y_i f_a(\\mathbf{x}_i)$。\n\n1.  **情况1：正确分类的样本** ($m_i(a)  0$) 。\n    根据 $f_a$ 的利普希茨常数 $L(a)$ 的定义：\n    $$\n    |f_a(\\mathbf{x}_i + \\mathbf{\\delta}) - f_a(\\mathbf{x}_i)| \\le L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    两边乘以 $|y_i|=1$ 不会改变该不等式：\n    $$\n    |y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) - y_i f_a(\\mathbf{x}_i)| \\le L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    这等价于以下双边不等式：\n    $$\n    -L(a) \\|\\mathbf{\\delta}\\|_2 \\le y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) - y_i f_a(\\mathbf{x}_i) \\le L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    整理左侧不等式，可得到扰动后间隔的一个下界：\n    $$\n    y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) \\ge y_i f_a(\\mathbf{x}_i) - L(a) \\|\\mathbf{\\delta}\\|_2 = m_i(a) - L(a) \\|\\mathbf{\\delta}\\|_2\n    $$\n    为了使符号翻转成为可能，新的间隔 $y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta})$ 必须能够变为非正数。这要求其下界也为非正数：\n    $$\n    m_i(a) - L(a) \\|\\mathbf{\\delta}\\|_2 \\le 0\n    $$\n    这意味着任何成功的扰动 $\\mathbf{\\delta}$ 都必须满足：\n    $$\n    \\|\\mathbf{\\delta}\\|_2 \\ge \\frac{m_i(a)}{L(a)}\n    $$\n    因此，$\\frac{m_i(a)}{L(a)}$ 是导致错误分类所需最小扰动幅值的一个下界。\n\n2.  **情况2：错误分类的样本** ($m_i(a) \\le 0$) 。\n    如果样本已经被错误分类或位于决策边界上，那么条件 $y_i f_a(\\mathbf{x}_i + \\mathbf{\\delta}) \\le 0$ 对于零扰动 $\\mathbf{\\delta} = \\mathbf{0}$ 已经满足。此扰动的范数为 $\\|\\mathbf{0}\\|_2 = 0$。因此，所需扰动范数的下界为 $0$。\n\n综合两种情况，样本级别鲁棒性下界 $r_i(a)$ 可以表示为：\n$$\nr_i(a) = \\frac{\\max(0, m_i(a))}{L(a)} = \\frac{\\max(0, y_i f_a(\\mathbf{x}_i))}{L(a)}\n$$\n平均下界为 $\\overline{r}(a) = \\frac{1}{n} \\sum_{i=1}^n r_i(a)$。\n\n### 任务3：经验 $0$-$1$ 风险\n\n经验 $0$-$1$ 风险，或称训练分类误差，衡量训练集中被模型错误分类的样本比例。它被定义为在数据集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 上 $0$-$1$ 损失的平均值。$0$-$1$ 损失函数由 $\\mathbf{1}\\{y f(\\mathbf{x}) \\le 0\\}$ 给出，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数（如果条件为真，则为 $1$，否则为 $0$）。\n因此，经验风险为：\n$$\n\\widehat{R}(f_a) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{y_i f_a(\\mathbf{x}_i) \\le 0\\} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{m_i(a) \\le 0\\}\n$$\n计算该值的方法是：在所有 $n=8$ 个训练样本上评估模型，检查每个样本的间隔符号，统计非正间隔的数量，然后除以 $n$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Lipschitz bound, robustness bound, and risk for a two-layer NN.\n    \"\"\"\n    # 1. Define the initial network parameters and dataset.\n    W1 = np.array([\n        [1.0, 0.5],\n        [-0.3, 1.2],\n        [0.7, -0.8]\n    ])\n    b1 = np.array([0.1, -0.2, 0.05])\n    \n    W2 = np.array([[0.9, 0.7, -0.6]])\n    b2 = -0.1\n\n    # Dataset\n    X = np.array([\n        [1.0, 1.0], [1.2, 0.9], [0.8, 1.1], [1.1, 1.2],  # Positive class\n        [-1.0, -1.0], [-1.2, -0.9], [-0.8, -1.1], [-1.1, -1.2]  # Negative class\n    ])\n    y = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n    n = len(y)\n\n    # Test suite for penalty parameter 'a'\n    a_values = [0.0, 0.5, 3.0, 50.0]\n\n    # 2. Pre-compute constant parts of the Lipschitz bound.\n    # The spectral norm (ord=2) of a 1D array is its Euclidean norm.\n    norm_W1 = np.linalg.norm(W1, ord=2)\n    norm_W2 = np.linalg.norm(W2, ord=2)\n    L0 = norm_W1 * norm_W2\n\n    # 3. Define the network's forward pass.\n    def forward_pass(x_in, w1_p, b1_p, w2_p, b2_p):\n        \"\"\"Computes the forward pass of the neural network.\"\"\"\n        # Layer 1: linear transformation + bias\n        z1 = x_in @ w1_p.T + b1_p\n        # Layer 1: ReLU activation\n        h = np.maximum(0, z1)\n        # Layer 2: linear transformation + bias\n        output = h @ w2_p.T + b2_p\n        return output.flatten()\n\n    results = []\n    # 4. Iterate through each penalty parameter 'a' and perform computations.\n    for a in a_values:\n        # Enforce the spectral norm penalty on weights\n        factor = 1.0 / (1.0 + a)\n        W1_a = W1 * factor\n        W2_a = W2 * factor\n        # Biases remain unchanged as per the problem statement\n        b1_a = b1\n        b2_a = b2\n\n        # Task 1: Compute the Lipschitz upper bound L(a)\n        # L(a) = ||W2_a|| * ||W1_a|| = (1/(1+a))^2 * ||W2|| * ||W1||\n        L_a = L0 / ((1.0 + a) ** 2)\n\n        # Compute network outputs for the current penalized model f_a\n        outputs_a = forward_pass(X, W1_a, b1_a, W2_a, b2_a)\n        \n        # Compute signed margins m_i(a) = y_i * f_a(x_i)\n        margins_a = y * outputs_a\n\n        # Task 2: Compute the average sample-wise robustness lower bound r_bar(a)\n        # r_i(a) = max(0, m_i(a)) / L(a)\n        if L_a > 0:\n            robustness_bounds = np.maximum(0, margins_a) / L_a\n        else: # Handle a -> infinity case where L_a -> 0\n            # If margin is positive, bound is infinite. If zero/negative, it's zero.\n            # Not strictly needed for the given 'a' values but good practice.\n            robustness_bounds = np.zeros_like(margins_a)\n            robustness_bounds[margins_a > 0] = np.inf\n            \n        r_bar_a = np.mean(robustness_bounds)\n\n        # Task 3: Compute the empirical 0-1 risk R_hat(f_a)\n        # R_hat = (1/n) * sum(I(m_i(a) = 0))\n        num_errors = np.sum(margins_a = 0)\n        R_hat_a = num_errors / n\n        \n        # Store results for this 'a'\n        results.append([L_a, r_bar_a, R_hat_a])\n\n    # 5. Format and print the final output as specified.\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3113400"}, {"introduction": "一个训练好的分类器可能对其预测过于自信或信心不足，其输出的概率值并不能准确反映真实的可能性。这个问题被称为模型的“校准”问题，对于构建可靠的人工智能系统至关重要。本练习将引导你实现一种简单而有效的后处理技术——温度缩放，并使用布里尔分数来衡量和改善模型的校准程度 [@problem_id:3113339]。", "problem": "考虑一个多类分类器，它为每个输入输出一个实值分数向量，通常称为 logits。在深度学习中，这些 logits 通过多项逻辑斯谛模型与概率预测相关联，该模型将 logits 映射到一个概率单纯形。温度缩放 (TS) 在构建概率之前调整这些 logits，以校正错误的校准。在逐类温度缩放中，每个类别都有其自己的正温度。本问题的目标是根据基本的多项逻辑斯谛原理推导出逐类温度缩放映射，并实现一个评估，以分析这种缩放如何影响被称为布里尔分数的平方误差校准风险。\n\n从以下基本基础开始：\n- 分类器是概率性的，通过应用一个保持概率单纯形并与多项模型的最大似然兼容的链接函数，生成一个在 $C$ 个类别上的分类分布。\n- 类别标签 $y \\in \\{0,\\dots,C-1\\}$ 的 one-hot 编码是一个向量，其在真实类别对应的坐标上为 1，其他位置为 0。\n- 概率单纯形上的平方偏差是一种有意义且经过充分检验的预测误差度量。\n\n你的任务：\n1. 从多项逻辑斯谛原理出发，推导应用于 logits 的逐类温度缩放变换以及由此产生的概率映射。确保逐类温度是严格的正实数，并解释较大温度与较小温度对置信度的影响。\n2. 使用 one-hot 编码，将布里尔分数表示为数据集上预测概率与真实标签之间平方偏差的均值。解释为什么这个量度量了概率单纯形上平方误差意义下的校准质量。\n3. 实现一个程序，对于提供的测试套件，计算逐类温度缩放前后的布里尔分数，然后返回其变化量。\n\n测试套件：\n- 案例 $1$（理想路径，不平衡标签，中等温度）。设 $C = 3$。设 logits 矩阵 $Z^{(1)} \\in \\mathbb{R}^{5 \\times 3}$，标签 $y^{(1)} \\in \\{0,1,2\\}^5$ 和类别温度 $T^{(1)} \\in \\mathbb{R}_{0}^3$ 为：\n$$\nZ^{(1)} =\n\\begin{bmatrix}\n2.5  1.0  0.0 \\\\\n1.8  0.2  0.1 \\\\\n0.5  1.3  2.0 \\\\\n1.2  0.8  1.5 \\\\\n3.0  2.5  0.5\n\\end{bmatrix},\\quad\ny^{(1)} = \\begin{bmatrix}0 \\\\ 0 \\\\ 2 \\\\ 2 \\\\ 0\\end{bmatrix},\\quad\nT^{(1)} = \\begin{bmatrix}1.8 \\\\ 1.0 \\\\ 0.7\\end{bmatrix}.\n$$\n- 案例 $2$（边界条件，中性温度）。设 $C = 3$。设\n$$\nZ^{(2)} =\n\\begin{bmatrix}\n0.0  0.0  0.0 \\\\\n2.0  2.0  2.0 \\\\\n1.5  0.5  1.0 \\\\\n0.2  0.1  0.3\n\\end{bmatrix},\\quad\ny^{(2)} = \\begin{bmatrix}0 \\\\ 2 \\\\ 1 \\\\ 1\\end{bmatrix},\\quad\nT^{(2)} = \\begin{bmatrix}1.0 \\\\ 1.0 \\\\ 1.0\\end{bmatrix}.\n$$\n- 案例 $3$（边缘情况，对多数 logit 进行强烈的过度自信削减，锐化少数）。设 $C = 3$。设\n$$\nZ^{(3)} =\n\\begin{bmatrix}\n4.0  1.0  1.0 \\\\\n5.0  0.0  0.5 \\\\\n3.0  2.5  2.5 \\\\\n4.5  1.5  1.5 \\\\\n6.0  0.5  0.5 \\\\\n5.5  0.1  0.1\n\\end{bmatrix},\\quad\ny^{(3)} = \\begin{bmatrix}1 \\\\ 2 \\\\ 1 \\\\ 2 \\\\ 1 \\\\ 2\\end{bmatrix},\\quad\nT^{(3)} = \\begin{bmatrix}5.0 \\\\ 0.5 \\\\ 0.5\\end{bmatrix}.\n$$\n\n实现细节：\n- 对于每个案例 $i \\in \\{1,2,3\\}$，通过将多项逻辑斯谛链接函数应用于 $Z^{(i)}$ 来计算缩放前的概率向量。然后，通过先将每个 logit 坐标按其类别温度的倒数进行缩放，再应用相同的多项逻辑斯谛链接函数，来计算逐类温度缩放后的概率。请使用数值稳定的实现。\n- 将布里尔分数计算为预测概率与 $y^{(i)}$ 的 one-hot 向量之间平方偏差在所有类别上求和后，再在样本上求均值。\n- 对于每个案例，报告三个实数：缩放前的布里尔分数 ($b_i$)，缩放后的布里尔分数 ($a_i$)，以及定义为 $a_i - b_i$ 的变化量（记为 $\\Delta_i$）。不涉及物理单位、角度或百分比。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含 $9$ 个用逗号分隔的实数，并用方括号括起来，顺序如下\n$$\n[b_1, a_1, \\Delta_1, b_2, a_2, \\Delta_2, b_3, a_3, \\Delta_3].\n$$", "solution": "首先将根据指定标准验证问题陈述。\n\n### 步骤 1：提取已知信息\n- **主题**：统计学习中的深度学习原理，特别是用于分类器校准的逐类温度缩放。\n- **基本概念**：\n    - 分类器是概率性的，生成一个在 $C$ 个类别上的分类分布。\n    - 链接函数保持概率单纯形，并与多项模型的最大似然兼容。\n    - 类别标签 $y \\in \\{0, \\dots, C-1\\}$ 的 one-hot 编码是一个向量，其在真实类别对应的坐标上为 1，其他位置为 0。\n    - 概率单纯形上的平方偏差是一种预测误差的度量。\n- **任务 1**：从多项逻辑斯谛原理出发，推导逐类温度缩放变换及由此产生的概率映射。解释温度的作用。逐类温度是严格的正实数 ($T \\in \\mathbb{R}_{0}^C$)。\n- **任务 2**：将布里尔分数表示为预测概率与真实的 one-hot 编码标签之间平方偏差的均值。解释为什么这能量度校准质量。\n- **任务 3**：实现一个程序，为三个测试案例计算逐类温度缩放前后的布里尔分数及其变化量。\n- **测试案例**：\n    - **案例 1**：$C=3$。\n        $Z^{(1)} = \\begin{bmatrix} 2.5  1.0  0.0 \\\\ 1.8  0.2  0.1 \\\\ 0.5  1.3  2.0 \\\\ 1.2  0.8  1.5 \\\\ 3.0  2.5  0.5 \\end{bmatrix}$，$y^{(1)} = [0, 0, 2, 2, 0]^\\top$，$T^{(1)} = [1.8, 1.0, 0.7]^\\top$。\n    - **案例 2**：$C=3$。\n        $Z^{(2)} = \\begin{bmatrix} 0.0  0.0  0.0 \\\\ 2.0  2.0  2.0 \\\\ 1.5  0.5  1.0 \\\\ 0.2  0.1  0.3 \\end{bmatrix}$，$y^{(2)} = [0, 2, 1, 1]^\\top$，$T^{(2)} = [1.0, 1.0, 1.0]^\\top$。\n    - **案例 3**：$C=3$。\n        $Z^{(3)} = \\begin{bmatrix} 4.0  1.0  1.0 \\\\ 5.0  0.0  0.5 \\\\ 3.0  2.5  2.5 \\\\ 4.5  1.5  1.5 \\\\ 6.0  0.5  0.5 \\\\ 5.5  0.1  0.1 \\end{bmatrix}$，$y^{(3)} = [1, 2, 1, 2, 1, 2]^\\top$，$T^{(3)} = [5.0, 0.5, 0.5]^\\top$。\n- **实现细节**：\n    - 对于一个 logit 矩阵 $Z^{(i)}$，使用多项逻辑斯谛链接函数计算缩放前的概率。\n    - 对于缩放后的概率，首先将每个 logit 坐标 $z_j$ 乘以 $1/T_j$ 进行缩放，然后应用链接函数。\n    - 布里尔分数是预测概率与 one-hot 标签之间平方偏差之和在样本间的均值。\n- **输出格式**：包含 $9$ 个逗号分隔数字的一行：$[b_1, a_1, \\Delta_1, b_2, a_2, \\Delta_2, b_3, a_3, \\Delta_3]$，其中 $b_i$ 是缩放前的布里尔分数，$a_i$ 是缩放后的分数，$\\Delta_i = a_i - b_i$。\n\n### 步骤 2：使用提取的已知信息进行验证\n根据提供的标准对问题进行评估。\n- **科学依据**：该问题基于统计学习和深度学习的既定原则。多项逻辑斯谛模型（softmax 函数）、温度缩放和布里尔分数都是模型校准领域中标准且有据可查的概念。其前提在事实上和科学上都是合理的。\n- **适定性**：问题定义清晰。输入（logits、标签、温度）被明确无误地提供。所需的输出以精确的格式指定。数学运算（softmax、布里尔分数计算）定义良好，确保每个测试案例都存在唯一解。\n- **客观性**：问题以精确、客观、形式化的数学语言陈述。它没有主观性、模糊性和基于观点的主张。\n\n该问题未表现出任何列出的无效性缺陷。它在科学上是合理的、可形式化的、完整的、现实的和适定的。\n\n### 步骤 3：结论与行动\n此问题是**有效的**。将提供一个解决方案。\n\n### 解答\n\n本解答分两部分进行：首先，根据问题陈述对所要求的量进行理论推导；其次，通过实现来计算具体的数值结果。\n\n#### 1. 逐类温度缩放与概率映射的推导\n\n对于一个在 $C$ 个类别上、与多项逻辑斯谛模型兼容的概率分类器，其基本链接函数是 softmax 函数。对于单个输入实例，分类器产生一个实值分数向量，即 logits，$\\mathbf{z} = [z_0, z_1, \\dots, z_{C-1}]^\\top \\in \\mathbb{R}^C$。分配给类别 $j$ 的概率 $p_j$ 由下式给出：\n$$\np_j = \\frac{\\exp(z_j)}{\\sum_{k=0}^{C-1} \\exp(z_k)}\n$$\n概率向量 $\\mathbf{p} = [p_0, p_1, \\dots, p_{C-1}]^\\top$ 位于 $(C-1)$ 维概率单纯形上，即对所有 $j$ 都有 $p_j \\ge 0$ 且 $\\sum_{j=0}^{C-1} p_j = 1$。\n\n逐类温度缩放引入一个正温度向量 $\\mathbf{T} = [T_0, T_1, \\dots, T_{C-1}]^\\top$，其中每个 $T_j \\in \\mathbb{R}_{0}$。该缩放变换在 softmax 函数之前应用于 logits。按照规定，每个 logit $z_j$ 按其对应类别温度 $T_j$ 的倒数进行缩放。类别 $j$ 的缩放后 logit，记作 $z'_j$，为：\n$$\nz'_j = \\frac{z_j}{T_j}\n$$\n将 softmax 函数应用于缩放后的 logits 向量 $\\mathbf{z'} = [z'_0, z'_1, \\dots, z'_{C-1}]^\\top$，即可得到逐类温度缩放后的概率 $p'_j$：\n$$\np'_j = \\frac{\\exp(z'_j)}{\\sum_{k=0}^{C-1} \\exp(z'_k)} = \\frac{\\exp(z_j / T_j)}{\\sum_{k=0}^{C-1} \\exp(z_k / T_k)}\n$$\n这就是推导出的逐类温度缩放的概率映射。$T_j  0$ 的条件至关重要，以避免除以零并保持 logits 的顺序，这对于将其解释为分数是基础性的（即，在其他条件相同的情况下，更高的 logit 应对应更高的概率）。\n\n温度 $T_j$ 的作用是调节对类别 $j$ 预测的置信度。\n- 如果 $T_j  1$，logit $z_j$ 的大小会被减小。这会“软化”输出的概率分布，使其更接近均匀分布。一个大的温度（$T_j \\to \\infty$）会将类别 $j$ 的概率推向 $1/C$（假设所有其他温度也很大），从而降低模型的置信度。\n- 如果 $0  T_j  1$，logit $z_j$ 的大小会被放大。这会“锐化”输出的概率分布，使其更集中在具有最高缩放后 logit 的类别上。当 $T_j \\to 0^+$ 时，概率 $p'_j$ 被推向 $0$ 或 $1$，从而增加模型表达的置信度。\n- 如果 $T_j = 1$，logit $z_j$ 保持不变，映射简化为标准的 softmax 函数。逐类缩放允许跨类别对置信度进行差异化调整，这在类别不平衡或模型对不同类别表现出系统性不同校准误差的情况下非常有用。标准的“温度缩放”是所有 $T_j$ 都等于单个标量 $T$ 的特殊情况。\n\n#### 2. 用于校准评估的布里尔分数\n\n布里尔分数是一种适度评分规则，用于衡量概率预测的准确性。对于单个实例，给定一个预测概率向量 $\\mathbf{p} = [p_0, \\dots, p_{C-1}]^\\top$ 和真实类别标签 $c_{true}$，我们首先将真实标签表示为一个 one-hot 向量 $\\mathbf{y} \\in \\{0, 1\\}^C$，其中如果 $j=c_{true}$ 则 $y_j = 1$，否则 $y_j=0$。\n\n该单个实例的布里尔分数是预测概率向量 $\\mathbf{p}$ 与真实 one-hot 向量 $\\mathbf{y}$ 之间的平方欧几里得距离：\n$$\nB_{\\text{instance}} = \\sum_{j=0}^{C-1} (p_j - y_j)^2\n$$\n对于一个包含 $N$ 个实例的数据集，其预测概率矩阵为 $P \\in \\mathbb{R}^{N \\times C}$，one-hot 标签矩阵为 $Y \\in \\mathbb{R}^{N \\times C}$，则总体布里尔分数是实例分数的平均值：\n$$\nB = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=0}^{C-1} (P_{ij} - Y_{ij})^2\n$$\n这个量度量了校准质量，因为它直接量化了预测概率相对于真实结果的均方误差。对于特定实例，一个完美校准且准确的模型会为真实类别分配概率 1，为所有其他类别分配概率 0，即 $\\mathbf{p} = \\mathbf{y}$。在这种理想情况下，分数贡献为 0。布里尔分数既惩罚过度自信（例如，对一个结果不正确的类别预测 $p_j=0.9$，而 $y_j=0$），也惩罚置信度不足（例如，对一个正确的类别预测 $p_j=0.6$，而 $y_j=1$）。最小化布里尔分数鼓励模型生成在欧几里得意义上更接近于与真实结果对应的概率单纯形顶点的概率向量。较低的布里尔分数表示更好的校准和准确性。\n\n#### 3. 计算实现\n\n将实现以下算法以解决问题的计算任务。对于每个测试案例 $i \\in \\{1, 2, 3\\}$：\n1.  设给定数据为 $Z^{(i)}$、$y^{(i)}$ 和 $T^{(i)}$。设 $N$ 为样本数，$C$ 为类别数。\n2.  从标签向量 $y^{(i)}$ 构建 one-hot 编码的标签矩阵 $Y^{(i)} \\in \\mathbb{R}^{N \\times C}$。\n3.  **缩放前**：\n    a. 对 logit 矩阵 $Z^{(i)}$ 的每一行应用数值稳定的 softmax 函数，计算概率矩阵 $P_{\\text{before}}^{(i)}$。对于一个 logit 向量 $\\mathbf{z}$，稳定的 softmax 函数为 $p_j = \\frac{\\exp(z_j - z_{\\max})}{\\sum_k \\exp(z_k - z_{\\max})}$，其中 $z_{\\max} = \\max_k z_k$。\n    b. 计算布里尔分数 $b_i = \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{j=0}^{C-1} ( (P_{\\text{before}}^{(i)})_{kj} - Y_{kj}^{(i)} )^2$。\n4.  **缩放后**：\n    a. 计算缩放后的 logit 矩阵 $Z_{\\text{scaled}}^{(i)}$，其中 $(Z_{\\text{scaled}}^{(i)})_{kj} = Z_{kj}^{(i)} / T_j^{(i)}$。\n    b. 对 $Z_{\\text{scaled}}^{(i)}$ 的每一行应用稳定的 softmax 函数，计算概率矩阵 $P_{\\text{after}}^{(i)}$。\n    c. 计算布里尔分数 $a_i = \\frac{1}{N} \\sum_{k=1}^{N} \\sum_{j=0}^{C-1} ( (P_{\\text{after}}^{(i)})_{kj} - Y_{kj}^{(i)} )^2$。\n5.  **变化量**：计算差值 $\\Delta_i = a_i - b_i$。\n6.  收集结果 $[b_1, a_1, \\Delta_1, b_2, a_2, \\Delta_2, b_3, a_3, \\Delta_3]$ 用于最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Brier score before and after class-wise temperature scaling\n    for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"Z\": np.array([\n                [2.5, 1.0, 0.0],\n                [1.8, 0.2, 0.1],\n                [0.5, 1.3, 2.0],\n                [1.2, 0.8, 1.5],\n                [3.0, 2.5, 0.5]\n            ]),\n            \"y\": np.array([0, 0, 2, 2, 0]),\n            \"T\": np.array([1.8, 1.0, 0.7])\n        },\n        {\n            \"Z\": np.array([\n                [0.0, 0.0, 0.0],\n                [2.0, 2.0, 2.0],\n                [1.5, 0.5, 1.0],\n                [0.2, 0.1, 0.3]\n            ]),\n            \"y\": np.array([0, 2, 1, 1]),\n            \"T\": np.array([1.0, 1.0, 1.0])\n        },\n        {\n            \"Z\": np.array([\n                [4.0, 1.0, 1.0],\n                [5.0, 0.0, 0.5],\n                [3.0, 2.5, 2.5],\n                [4.5, 1.5, 1.5],\n                [6.0, 0.5, 0.5],\n                [5.5, 0.1, 0.1]\n            ]),\n            \"y\": np.array([1, 2, 1, 2, 1, 2]),\n            \"T\": np.array([5.0, 0.5, 0.5])\n        }\n    ]\n\n    def stable_softmax(z, axis=-1):\n        \"\"\"\n        Numerically stable softmax function for a matrix of logits.\n        Args:\n            z (np.ndarray): A matrix of logits, shape (N, C).\n            axis (int): The axis to apply softmax over.\n        Returns:\n            np.ndarray: A matrix of probabilities, shape (N, C).\n        \"\"\"\n        z_max = np.max(z, axis=axis, keepdims=True)\n        exp_z = np.exp(z - z_max)\n        sum_exp_z = np.sum(exp_z, axis=axis, keepdims=True)\n        return exp_z / sum_exp_z\n\n    def calculate_brier_score(P, y_one_hot):\n        \"\"\"\n        Calculates the Brier score.\n        Args:\n            P (np.ndarray): Probability matrix, shape (N, C).\n            y_one_hot (np.ndarray): One-hot encoded labels, shape (N, C).\n        Returns:\n            float: The mean Brier score.\n        \"\"\"\n        # Sum of squared differences for each sample\n        sum_sq_diff = np.sum((P - y_one_hot)**2, axis=1)\n        # Mean across all samples\n        return np.mean(sum_sq_diff)\n\n    results = []\n    for case in test_cases:\n        Z = case[\"Z\"]\n        y = case[\"y\"]\n        T = case[\"T\"]\n\n        N, C = Z.shape\n\n        # Create one-hot encoded labels\n        y_one_hot = np.eye(C)[y]\n\n        # --- Before scaling ---\n        # Compute probabilities\n        P_before = stable_softmax(Z)\n        # Compute Brier score\n        brier_before = calculate_brier_score(P_before, y_one_hot)\n\n        # --- After scaling ---\n        # Scale the logits\n        Z_scaled = Z / T\n        # Compute probabilities\n        P_after = stable_softmax(Z_scaled)\n        # Compute Brier score\n        brier_after = calculate_brier_score(P_after, y_one_hot)\n\n        # --- Change ---\n        delta = brier_after - brier_before\n        \n        results.extend([brier_before, brier_after, delta])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3113339"}, {"introduction": "深度学习的优化过程常常找到多个不同的参数解，它们都能在训练集上达到很低的损失。这些解是孤立的“好点”，还是位于一个连通的“解决方案山谷”中？本实践将带你探索损失函数的几何景观，通过在参数空间中构造线性路径，来检验不同解之间的“模式连通性”及其表征的一致性 [@problem_id:3113426]。", "problem": "本题要求您通过评估沿线性参数路径的经验风险并量化表示一致性，来研究一个简单的两层神经网络中的模式连通性。目标是实现一个程序，对于一组指定的测试用例，确定其端点是否通过参数空间中的一条低损失线性路径相连，并报告一个表示一致性统计量。\n\n定义、模型与数据集：\n- 考虑一个带有整流线性单元（ReLU）激活函数的两层前馈神经网络。其假设类别为\n$$\nf_{\\theta}(x) \\;=\\; W_2 \\,\\sigma\\!\\left(W_1 x + b_1\\right) + b_2,\n$$\n其中参数为 $\\theta=(W_1,b_1,W_2,b_2)$，$\\sigma(z)=\\max\\{0,z\\}$ 是按元素施加的激活函数，输入 $x\\in\\mathbb{R}^d$，隐藏层宽度为 $h$，输出维度为 $1$。在所有测试用例中，使用 $d=h=2$，$b_1=\\mathbf{0}\\in\\mathbb{R}^2$ 以及 $b_2=0$。\n- 使用数据集 $\\{(x_i,y_i)\\}_{i=1}^n$，其中包含 $n=6$ 个输入向量和标量标签：\n  - 输入 $x_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}$，$x_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}$，$x_3=\\begin{bmatrix}1\\\\1\\end{bmatrix}$，$x_4=\\begin{bmatrix}1\\\\-1\\end{bmatrix}$，$x_5=\\begin{bmatrix}-1\\\\1\\end{bmatrix}$，$x_6=\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$。\n  - 标签由一个固定的目标函数定义 $y_i=\\max\\{0, x_{i,1}\\}+\\max\\{0, x_{i,2}\\}$，其中 $i\\in\\{1,\\dots,6\\}$。\n- 经验风险是均方误差（MSE），它是经验风险最小化（ERM）的一个特例：\n$$\nL(\\theta)\\;=\\;\\frac{1}{n}\\sum_{i=1}^n \\left( f_{\\theta}(x_i) - y_i \\right)^2.\n$$\n\n线性参数路径与目标：\n- 对于两组参数集 $\\theta_a$ 和 $\\theta_b$，定义在参数空间中的线性插值\n$$\n\\theta(t) \\;=\\; (1-t)\\,\\theta_a + t\\,\\theta_b,\\quad t\\in[0,1],\n$$\n该插值逐分量应用于 $W_1$、$b_1$、$W_2$ 和 $b_2$。\n- 对于一个包含 $m=21$ 个在 $[0,1]$ 区间内等间距值的有限网格 $T=\\{t_k\\}_{k=0}^{m-1}$（即 $t_k=\\frac{k}{20}$），评估沿该路径的经验风险：$\\{L(\\theta(t_k))\\}_{k=0}^{m-1}$。\n- 定义损失势垒高度\n$$\nB(\\theta_a,\\theta_b)\\;=\\;\\max_{t\\in T} L(\\theta(t)) \\;-\\; \\max\\{L(\\theta_a),\\,L(\\theta_b)\\}.\n$$\n如果 $B(\\theta_a,\\theta_b)\\le \\delta$，则称参数对 $(\\theta_a,\\theta_b)$ 在容差 $\\delta$ 下是线性模式连通的。在本问题中，使用 $\\delta=10^{-9}$。\n- 将在参数 $\\theta$ 下数据集上的隐藏表示定义为堆叠的隐藏层激活值\n$$\nH(\\theta) \\;\\in\\; \\mathbb{R}^{n\\times h},\\qquad H(\\theta)_{i,:} \\;=\\; \\sigma\\!\\left(W_1 x_i + b_1\\right)^{\\top}.\n$$\n令 $v(\\theta)\\in\\mathbb{R}^{nh}$ 为 $H(\\theta)$ 按行主序扁平化后的向量化形式。定义余弦相似度\n$$\n\\mathrm{sim}(u,v)\\;=\\;\\frac{\\langle u,v\\rangle}{\\|u\\|_2\\,\\|v\\|_2},\n$$\n并遵循以下约定：如果 $\\|u\\|_2=\\|v\\|_2=0$，则 $\\mathrm{sim}(u,v)=1$；如果 $\\|u\\|_2$ 或 $\\|v\\|_2$ 中只有一个为 $0$，则 $\\mathrm{sim}(u,v)=0$。\n- 对于路径 $\\theta(t)$，将在 $t$ 处的表示一致性定义为\n$$\nR(t)\\;=\\;\\max\\!\\left\\{\\,\\mathrm{sim}\\big(v(\\theta(t)),v(\\theta_a)\\big),\\;\\mathrm{sim}\\big(v(\\theta(t)),v(\\theta_b)\\big)\\right\\}.\n$$\n通过其最差情况下的表示一致性来总结一条路径\n$$\nR_{\\min}\\;=\\;\\min_{t\\in T} R(t).\n$$\n\n测试套件：\n评估以下三个测试用例。在所有用例中，均取 $b_1=\\mathbf{0}$ 和 $b_2=0$。\n- 用例 #1 (相同端点)：\n  - $\\theta_a$：$W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$，$W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n  - $\\theta_b$：$W_{1,b}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$，$W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n- 用例 #2 (置换隐藏单元)：\n  - $\\theta_a$：$W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$，$W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n  - $\\theta_b$：$W_{1,b}=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}$，$W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n- 用例 #3 (微小扰动)：\n  - $\\theta_a$：$W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}$，$W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n  - $\\theta_b$：$W_{1,b}=\\begin{bmatrix}1  0.05\\\\ 0.05  1\\end{bmatrix}$，$W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n\n您必须实现的内容：\n- 对每个用例，计算：\n  - 在网格 $T$ 上的势垒 $B(\\theta_a,\\theta_b)$ 以及布尔连通性决策 $\\mathbf{conn}=\\big(B(\\theta_a,\\theta_b)\\le \\delta\\big)$。\n  - 在同一网格 $T$ 上的最差情况表示一致性 $R_{\\min}$。\n- 输出格式：\n  - 您的程序应生成单行输出，其中包含一个列表，每个测试用例对应一个元素。每个元素本身必须是一个形式为 $[\\mathbf{conn}, r]$ 的双元素列表，其中 $\\mathbf{conn}$ 是一个布尔值，$r$ 是等于 $R_{\\min}$ 并四舍五入到六位小数的浮点数。\n  - 具体来说，输出必须像单行 Python 风格的列表字面量，例如 $[[\\text{boolean}, \\text{float}], [\\text{boolean}, \\text{float}], [\\text{boolean}, \\text{float}]]$。\n\n仅限科学起点：\n- 仅使用经验风险最小化（ERM）、均方误差（MSE）、整流线性单元（ReLU）的定义、参数空间中的线性插值和余弦相似度的基本原理来推导您的计算。\n- 不涉及角度或物理单位，因此除上述定义外，不需要其他单位约定。\n\n您的任务：\n- 实现一个完整的程序，按照上述三个用例的规定执行计算，并以所描述的精确输出格式打印结果。", "solution": "用户要求对一个简单的两层神经网络的模式连通性进行分析。这需要验证问题陈述的有效性，如果问题陈述有效，则提供详细的解决方案和相应的 Python 实现。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n*   **模型 ($f_{\\theta}(x)$)**：一个带有 ReLU 激活函数的两层前馈神经网络，$f_{\\theta}(x) = W_2 \\sigma(W_1 x + b_1) + b_2$。\n*   **参数 ($\\theta$)**：$\\theta=(W_1,b_1,W_2,b_2)$。\n*   **激活函数 ($\\sigma$)**：按元素的 ReLU，$\\sigma(z)=\\max\\{0,z\\}$。\n*   **维度**：输入维度 $d=2$，隐藏层宽度 $h=2$，输出维度 $1$。\n*   **固定参数**：对于所有测试用例，偏置均为零：$b_1=\\mathbf{0}\\in\\mathbb{R}^2$ 和 $b_2=0$。\n*   **数据集**：$n=6$ 个输入-输出对 $\\{(x_i,y_i)\\}_{i=1}^6$。\n    *   输入 $X$：$x_1=\\begin{bmatrix}1\\\\0\\end{bmatrix}, x_2=\\begin{bmatrix}0\\\\1\\end{bmatrix}, x_3=\\begin{bmatrix}1\\\\1\\end{bmatrix}, x_4=\\begin{bmatrix}1\\\\-1\\end{bmatrix}, x_5=\\begin{bmatrix}-1\\\\1\\end{bmatrix}, x_6=\\begin{bmatrix}-1\\\\-1\\end{bmatrix}$。\n    *   标签 $Y$：$y_i=\\max\\{0, x_{i,1}\\}+\\max\\{0, x_{i,2}\\}$。\n*   **经验风险 ($L(\\theta)$)**：均方误差（MSE），$L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n ( f_{\\theta}(x_i) - y_i )^2$。\n*   **参数路径**：线性插值 $\\theta(t) = (1-t)\\theta_a + t\\theta_b$，其中 $t\\in[0,1]$。\n*   **评估网格 ($T$)**：在 $[0,1]$ 中 $m=21$ 个等距点，$t_k=k/(m-1) = k/20$。\n*   **损失势垒 ($B(\\theta_a,\\theta_b)$)**：$B(\\theta_a,\\theta_b) = \\max_{t\\in T} L(\\theta(t)) - \\max\\{L(\\theta_a), L(\\theta_b)\\}$。\n*   **连通性条件**：如果 $B(\\theta_a,\\theta_b) \\le \\delta$，则为线性模式连通，容差 $\\delta=10^{-9}$。\n*   **隐藏表示 ($H(\\theta)$)**：$n \\times h$ 矩阵，其中 $H(\\theta)_{i,:} = \\sigma(W_1 x_i + b_1)^{\\top}$。\n*   **向量化表示 ($v(\\theta)$)**：$H(\\theta)$ 按行主序扁平化后的向量，$v(\\theta) \\in \\mathbb{R}^{nh}$。\n*   **余弦相似度 ($\\mathrm{sim}(u,v)$)**：$\\frac{\\langle u,v\\rangle}{\\|u\\|_2\\,\\|v\\|_2}$。约定：若 $\\|u\\|_2=\\|v\\|_2=0$，则 $\\mathrm{sim}=1$；若只有一个范数为零，则 $\\mathrm{sim}=0$。\n*   **表示一致性 ($R(t)$)**：$R(t) = \\max\\{\\mathrm{sim}(v(\\theta(t)),v(\\theta_a)), \\mathrm{sim}(v(\\theta(t)),v(\\theta_b))\\}$。\n*   **最差情况一致性 ($R_{\\min}$)**：$R_{\\min} = \\min_{t\\in T} R(t)$。\n*   **测试用例**：\n    1.  **相同**：$\\theta_a=\\theta_b$，其中 $W_{1,a}=\\begin{bmatrix}1  0\\\\ 0  1\\end{bmatrix}, W_{2,a}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n    2.  **置换**：$\\theta_a$ 如用例 1。$\\theta_b$ 的参数为 $W_{1,b}=\\begin{bmatrix}0  1\\\\ 1  0\\end{bmatrix}, W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n    3.  **扰动**：$\\theta_a$ 如用例 1。$\\theta_b$ 的参数为 $W_{1,b}=\\begin{bmatrix}1  0.05\\\\ 0.05  1\\end{bmatrix}, W_{2,b}=\\begin{bmatrix}1  1\\end{bmatrix}$。\n*   **输出规范**：单行打印一个 Python 列表的列表：`[[conn, r], [conn, r], [conn, r]]`，其中 `conn` 是布尔值，$r$ 是四舍五入到 6 位小数的 $R_{\\min}$。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n该问题在深度学习理论领域具有科学依据，特别是关于损失地貌和模型对称性的研究。问题提法恰当，为进行唯一的确定性计算提供了所有必要的数据、参数和定义。其语言客观、正式。不存在矛盾、信息缺失或违反科学原则的情况。计算任务是可行的，并且所用定义在该领域内是标准的。\n\n**步骤 3：结论与行动**\n\n该问题是**有效的**。将开发并实现一个解决方案。\n\n### 推导与解决方案设计\n\n该问题要求实现一个计算过程，以评估简单神经网络参数空间中线性路径的性质。我们将通过首先定义核心组件，然后将它们应用于指定的测试用例来构建解决方案。\n\n**1. 模型与损失函数**\n由于零偏置约束（$b_1=\\mathbf{0}, b_2=0$），模型简化为：\n$$\nf_{\\theta}(x) = W_2 \\sigma(W_1 x)\n$$\n参数为 $\\theta = (W_1, W_2)$。经验风险 $L(\\theta)$ 是 $f_{\\theta}(x_i)$ 与目标标签 $y_i=\\max\\{0, x_{i,1}\\}+\\max\\{0, x_{i,2}\\}$ 在数据集上差值平方的均值。\n\n分析用例 #1 中参数所表示的函数是有启发性的。在此用例中，$W_{1,a} = I$（单位矩阵），$W_{2,a} = \\begin{bmatrix}1  1\\end{bmatrix}$。网络计算如下：\n$$\nf_{\\theta_a}(x) = \\begin{bmatrix}1  1\\end{bmatrix} \\sigma\\left(\\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\\right) = \\begin{bmatrix}1  1\\end{bmatrix} \\begin{bmatrix}\\max\\{0, x_1\\} \\\\ \\max\\{0, x_2\\}\\end{bmatrix} = \\max\\{0, x_1\\} + \\max\\{0, x_2\\}\n$$\n这正是目标函数 $y$。因此，对于 $\\theta_a$（以及任何其他定义相同函数的 $\\theta$），损失 $L(\\theta)$ 为 $0$。\n\n**2. 路径插值与分析**\n对于每个测试用例，我们考虑两个端点 $\\theta_a$ 和 $\\theta_b$。线性路径定义为 $\\theta(t) = ((1-t)W_{1,a} + tW_{1,b}, (1-t)W_{2,a} + tW_{2,b})$，其中 $t \\in [0, 1]$。我们使用网格 $T=\\{0, 1/20, 2/20, \\dots, 1\\}$ 对此路径进行离散化。\n\n对于每个 $t_k \\in T$，我们必须计算：\n-   **损失 $L(\\theta(t_k))$**：这涉及使用插值参数 $\\theta(t_k)$ 对 $n=6$ 个数据点中的每一个进行前向传播，并计算 MSE。\n-   **表示一致性 $R(t_k)$**：这需要：\n    a.  计算隐藏表示 $H(\\theta_a)$、$H(\\theta_b)$ 和 $H(\\theta(t_k))$。$H(\\theta)$ 的第 $(i,:)$ 行为 $\\sigma(W_1 x_i)^\\top$。\n    b.  将这些矩阵向量化为 $v(\\theta_a)$、$v(\\theta_b)$ 和 $v(\\theta(t_k))$。\n    c.  使用提供的定义和针对零向量的约定，计算余弦相似度 $\\mathrm{sim}(v(\\theta(t_k)), v(\\theta_a))$ 和 $\\mathrm{sim}(v(\\theta(t_k)), v(\\theta_b))$。\n    d.  $R(t_k)$ 是这两个相似度的最大值。\n\n**3. 最终指标**\n在网格 $T$ 上评估路径性质后，我们为每个测试用例计算汇总统计量：\n-   **势垒高度 $B(\\theta_a, \\theta_b)$**：根据收集的损失 $\\{L(\\theta(t_k))\\}$ 计算，即 $\\max_{k} L(\\theta(t_k)) - \\max\\{L(\\theta(t_0)), L(\\theta(t_{m-1}))\\}$。\n-   **连通性 $\\mathbf{conn}$**：一个布尔值，如果 $B(\\theta_a, \\theta_b) \\le 10^{-9}$ 则为 `True`，否则为 `False`。\n-   **最差情况一致性 $R_{\\min}$**：收集到的一致性中的最小值，即 $\\min_{k} R(t_k)$。\n\n该过程将系统地应用于三个测试用例中的每一个。\n\n-   **用例 #1 (相同)**：由于 $\\theta_a=\\theta_b$，路径是恒定的：对所有 $t$，$\\theta(t)=\\theta_a$。所有 $k$ 的损失 $L(\\theta(t_k))$ 都将为 $0$。因此，$B=0$，$\\mathbf{conn}$ 为 `True`。表示 $v(\\theta(t_k))$ 也是恒定的，与 $v(\\theta_a)$ 和 $v(\\theta_b)$ 相同，所以所有 $k$ 的 $\\mathrm{sim}=1$。因此，$R_{\\min}=1$。\n\n-   **用例 #2 (置换)**：在这里，$\\theta_b$ 表示与 $\\theta_a$ 相同的函数，但隐藏神经元被交换了。$W_{1,b}$ 是一个置换矩阵，而 $W_{2,b}$ 是对称的，因此置换的效果被抵消了。因此 $L(\\theta_a)=L(\\theta_b)=0$。然而，已知这些置换解之间的线性插值会通过一个高损失区域，在该区域网络的有效容量会降低。我们预期 $B  \\delta$，$\\mathbf{conn}$ 为 `False`。表示也会沿路径发散，尤其是在 $t=0.5$ 附近，导致 $R_{\\min}  1$。\n\n-   **用例 #3 (扰动)**：$\\theta_b$ 是 $\\theta_a$ 的一个微小扰动。两个点都位于同一个低损失盆地内。它们之间的线性路径预期会停留在这个盆地中。我们预计损失势垒非常小，$B \\approx 0$，所以 $\\mathbf{conn}$ 应该为 `True`。隐藏表示在整个路径上应保持非常相似，从而得到一个非常接近 $1$ 的 $R_{\\min}$。\n\n实现将遵循此逻辑，为前向传播、损失、表示提取和相似度计算创建辅助函数，然后遍历测试用例以计算并格式化最终结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the mode connectivity problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Definitions ---\n\n    # Dataset\n    X = np.array([\n        [1, 0], [0, 1], [1, 1], [1, -1], [-1, 1], [-1, -1]\n    ])\n    # Target function y = max(0, x_1) + max(0, x_2)\n    Y = np.maximum(0, X[:, 0]) + np.maximum(0, X[:, 1])\n    n = X.shape[0]\n\n    # Parameters for analysis\n    m = 21  # Number of grid points\n    delta = 1e-9  # Connectivity tolerance\n    t_grid = np.linspace(0.0, 1.0, m)\n\n    # --- Helper Functions ---\n\n    def relu(z):\n        \"\"\"Element-wise Rectified Linear Unit.\"\"\"\n        return np.maximum(0, z)\n\n    def forward_pass(W1, W2, x):\n        \"\"\"Computes the network output for a single input vector x.\"\"\"\n        # Biases b1 and b2 are zero\n        a1 = relu(W1 @ x)\n        output = W2 @ a1\n        return output[0] # Return scalar\n\n    def mse_loss(W1, W2, X_data, Y_data):\n        \"\"\"Computes the Mean Squared Error loss over the dataset.\"\"\"\n        squared_errors = []\n        for i in range(X_data.shape[0]):\n            y_pred = forward_pass(W1, W2, X_data[i])\n            squared_errors.append((y_pred - Y_data[i])**2)\n        return np.mean(squared_errors)\n\n    def get_vectorized_representation(W1, X_data):\n        \"\"\"Computes the vectorized hidden representation v(theta).\"\"\"\n        # H(theta) is n x h\n        h = W1.shape[0]\n        n_data = X_data.shape[0]\n        H = np.zeros((n_data, h))\n        for i in range(n_data):\n            H[i, :] = relu(W1 @ X_data[i])\n        return H.flatten() # Row-major flattening by default\n\n    def cosine_similarity(u, v):\n        \"\"\"Computes cosine similarity with special conventions for zero vectors.\"\"\"\n        norm_u = np.linalg.norm(u)\n        norm_v = np.linalg.norm(v)\n\n        if norm_u == 0 and norm_v == 0:\n            return 1.0\n        if norm_u == 0 or norm_v == 0:\n            return 0.0\n        \n        return np.dot(u, v) / (norm_u * norm_v)\n\n    # --- Test Case Definitions ---\n\n    # Case #1: Identical endpoints\n    W1_a1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_a1 = np.array([[1.0, 1.0]])\n    W1_b1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_b1 = np.array([[1.0, 1.0]])\n\n    # Case #2: Permuted hidden units\n    W1_a2 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_a2 = np.array([[1.0, 1.0]])\n    W1_b2 = np.array([[0.0, 1.0], [1.0, 0.0]])\n    W2_b2 = np.array([[1.0, 1.0]])\n\n    # Case #3: Small perturbation\n    W1_a3 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    W2_a3 = np.array([[1.0, 1.0]])\n    W1_b3 = np.array([[1.0, 0.05], [0.05, 1.0]])\n    W2_b3 = np.array([[1.0, 1.0]])\n\n    test_cases = [\n        (W1_a1, W2_a1, W1_b1, W2_b1),\n        (W1_a2, W2_a2, W1_b2, W2_b2),\n        (W1_a3, W2_a3, W1_b3, W2_b3)\n    ]\n\n    final_results = []\n    \n    # --- Main Calculation Loop ---\n    \n    for W1_a, W2_a, W1_b, W2_b in test_cases:\n        \n        path_losses = []\n        path_consistencies = []\n\n        # Pre-compute representations at endpoints\n        v_a = get_vectorized_representation(W1_a, X)\n        v_b = get_vectorized_representation(W1_b, X)\n\n        for t in t_grid:\n            # Interpolate parameters\n            W1_t = (1 - t) * W1_a + t * W1_b\n            W2_t = (1 - t) * W2_a + t * W2_b\n\n            # 1. Calculate loss along the path\n            loss_t = mse_loss(W1_t, W2_t, X, Y)\n            path_losses.append(loss_t)\n\n            # 2. Calculate representation consistency\n            v_t = get_vectorized_representation(W1_t, X)\n            \n            sim_a = cosine_similarity(v_t, v_a)\n            sim_b = cosine_similarity(v_t, v_b)\n            \n            R_t = max(sim_a, sim_b)\n            path_consistencies.append(R_t)\n\n        # Calculate final metrics for the case\n        loss_endpoints = max(path_losses[0], path_losses[-1])\n        max_path_loss = max(path_losses)\n        \n        barrier = max_path_loss - loss_endpoints\n        conn = barrier = delta\n        \n        R_min = min(path_consistencies)\n        \n        # Store result in the required format\n        final_results.append([conn, round(R_min, 6)])\n\n    # --- Format and Print Output ---\n    \n    # Format each inner list to string, join with commas, and wrap in brackets\n    str_results = [f\"[{res[0]}, {res[1]}]\" for res in final_results]\n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```", "id": "3113426"}]}