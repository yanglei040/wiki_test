{"hands_on_practices": [{"introduction": "这项实践探讨了“长期依赖”这一根本性挑战，这也是开发LSTM网络的主要动机。我们将分析经典的“加法问题”，这是一个旨在凸显标准循环神经网络 (RNN) 如何在处理长序列时丢失来自遥远过去信息的思想实验。通过推导和比较RNN、GRU和LSTM的“训练信号幅度”，你将定量地看到门控机制对于在长序列中保存信息的关键作用 [@problem_id:3191191]。", "problem": "您的任务是形式化并分析序列学习中著名的“加法问题”，重点关注深度学习中的长期依赖问题。加法问题的定义如下：给定一个长度为 $L$、区间为 $[0,1]$ 的实数序列，其中正好有两个位置被标记，目标是这两个标记位置上数值的总和。模型逐个时间步处理该序列，并在最后的时间步输出一个标量。为了分离长期依赖的影响，考虑最坏情况，即第一个标记位置在 $t=1$，最终输出在 $t=L$ 产生，因此最早相关时间步的学习信号必须穿过 $L-1$ 个循环转换。\n\n使用三种架构进行分析：\n- 循环神经网络 (RNN)，其循环更新使用平滑非线性函数，\n- 门控循环单元 (GRU)，\n- 长短期记忆 (LSTM)。\n\n假设遵循以下科学上标准且被广泛接受的基础：\n- 复合函数的链式求导法则适用于随时间反向传播，因此较早时间步的梯度是跨时间步的雅可比矩阵的乘积。\n- 为保证稳定性，反向传播梯度的幅度由沿时间步传递记忆的路径上的循环雅可比因子的算子范数决定。\n- 对于循环神经网络 (RNN)，其在原点附近的循环雅可比主要由循环权重矩阵决定，其长期行为由该矩阵的谱半径控制。\n- 门控循环单元 (GRU) 和长短期记忆 (LSTM) 架构中的门控机制将前一个状态与门值相乘，从而直接缩放流经主记忆路径的梯度。\n\n为了使比较明确且计算上易于处理，采用以下一致的简化进行分析：\n- 将循环神经网络 (RNN) 的循环雅可比近似视为时间不变的，其谱半径为 $\\rho$。\n- 将长短期记忆 (LSTM) 的遗忘门和门控循环单元 (GRU) 的更新门视为跨时间步的恒定标量：LSTM 的遗忘门为 $f$，GRU 的更新门为 $z$。\n- 使用归一化的输出和损失，以便循环路径之外的恒定乘法因子可以被吸收到单个正常量 $\\alpha$ 中。\n\n在所有计算中使用的参数值：\n- 循环神经网络 (RNN) 的 $\\rho = 0.90$，\n- 长短期记忆 (LSTM) 的 $f = 0.99$，\n- 门控循环单元 (GRU) 的 $z = 0.05$，\n- $\\alpha = 1.0$。\n\n将最早标记输入的“训练信号幅度”定义为：损失函数相对于最早标记输入的梯度，沿主记忆路径从时间 $t=L$ 反向传播到 $t=1$ 时的幅度。利用上述基础（链式法则和雅可比矩阵的乘积），推导每种架构的训练信号幅度表达式，使其成为 $L$ 和给定参数的函数。然后在程序中实现这些表达式。\n\n测试套件：\n- 序列长度 $L \\in \\{50, 75, 100, 500, 1000\\}$。\n- 对于每个 $L$，按以下顺序计算每种架构的训练信号幅度：循环神经网络 (RNN)、门控循环单元 (GRU)、长短期记忆 (LSTM)。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格。该列表按 $L$ 的升序将整个测试套件的结果展开。也就是说，输出必须是：\n$[$RNN$(50)$,GRU$(50)$,LSTM$(50)$,RNN$(75)$,GRU$(75)$,LSTM$(75)$,RNN$(100)$,GRU$(100)$,LSTM$(100)$,RNN$(500)$,GRU$(500)$,LSTM$(500)$,RNN$(1000)$,GRU$(1000)$,LSTM$(1000)]$，\n其中每个条目都是一个表示训练信号幅度（无单位）的浮点数。最终输出为浮点数；不涉及物理单位或角度，也不得打印百分比。", "solution": "用户要求对“加法问题”进行形式化分析，以比较三种循环架构——循环神经网络 (RNN)、门控循环单元 (GRU) 和长短期记忆 (LSTM)——处理长期依赖的能力。该分析侧重于从最后时间步反向传播到第一个相关输入的梯度信号的幅度。\n\n问题验证如下：\n- **步骤 1：提取已知条件**\n  - 问题：针对长度为 $L$ 的序列的“加法问题”。\n  - 目标：计算当输出位于 $t=L$ 时，时间 $t=1$ 处的输入的“训练信号幅度”。\n  - 架构：RNN, GRU, LSTM。\n  - 基础：\n    - 随时间反向传播基于链式法则。\n    - 梯度幅度由循环雅可比矩阵的算子范数决定。\n    - RNN 的雅可比行为由循环权重矩阵的谱半径控制。\n    - GRU 和 LSTM 中的门控机制缩放梯度流。\n  - 分析简化：\n    - RNN 循环雅可比被视为时间不变的，其谱半径为 $\\rho$。\n    - LSTM 的遗忘门是恒定标量 $f$。\n    - GRU 的更新门是恒定标量 $z$。\n    - 单个正常量 $\\alpha$ 吸收了循环路径之外的所有恒定乘法因子。\n  - 参数值：\n    - $\\rho = 0.90$\n    - $f = 0.99$\n    - $z = 0.05$\n    - $\\alpha = 1.0$\n  - 测试套件：\n    - 序列长度 $L \\in \\{50, 75, 100, 500, 1000\\}$。\n    - 计算顺序：对每个 $L$，依次计算 RNN, GRU, LSTM。\n\n- **步骤 2：使用提取的已知条件进行验证**\n  - **科学依据：** 该问题牢固地植根于深度学习的既定原则，特别是循环网络中的梯度流分析（梯度消失/爆炸问题）。这些简化是为了创建一个复杂系统的易于处理的分析模型而采取的标准做法。\n  - **适定性：** 问题定义清晰，包含了所有必要的参数和假设，从而为每种情况得出一个唯一的、可计算的解。\n  - **客观性：** 语言精确且技术性强，没有主观性。\n\n- **步骤 3：结论与行动**\n  - 该问题被判定为**有效**。简化条件被明确说明，用于分离梯度传播的核心机制，这是一种标准且富有信息量的分析技术。着手解决问题。\n\n分析训练信号幅度的核心原理是随时间反向传播 (BPTT)。损失函数 $\\mathcal{L}$ 相对于某个时间步 $t$ 的隐藏状态 $h_t$ 的梯度是通过链式法则，从更晚的时间步 $t+1$ 传播梯度来计算的：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_t} = \\frac{\\partial \\mathcal{L}}{\\partial h_{t+1}} \\frac{\\partial h_{t+1}}{\\partial h_t}\n$$\n为了从在时间 $t=L$ 计算的损失中找到相对于时间 $t=1$ 的状态的梯度，我们必须递归地应用此法则 $L-1$ 步：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_L} \\left( \\prod_{t=2}^{L} \\frac{\\partial h_t}{\\partial h_{t-1}} \\right)\n$$\n项 $\\frac{\\partial h_t}{\\partial h_{t-1}}$ 是时间 $t$ 的循环雅可比矩阵。对于 $t=1$ 处的输入，“训练信号幅度”主要由该雅可比矩阵乘积的幅度决定，它确定了来自 $t=L$ 处输出的误差信号在传播回 $t=1$ 时被放大或减弱的程度。问题定义了一个常数 $\\alpha$ 来吸收所有非循环因子，例如 $\\frac{\\partial \\mathcal{L}}{\\partial h_L}$ 和最后一步的 $\\frac{\\partial h_1}{\\partial x_1}$。因此，信号幅度 $S(L)$ 与雅可比矩阵乘积的范数成正比。让我们针对每种架构进行分析。\n\n**循环神经网络 (RNN)**\nRNN 的隐藏状态更新形式为 $h_t = \\phi(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$，其中 $\\phi$ 是一个非线性激活函数，如 $\\tanh$。循环雅可比为 $\\frac{\\partial h_t}{\\partial h_{t-1}} = \\text{diag}(\\phi'(...))W_{hh}$。这些矩阵乘积的长期行为由循环权重矩阵 $W_{hh}$ 的谱半径 $\\rho$ 决定。该问题通过假设每个雅可比步骤的有效幅度贡献是一个常数因子 $\\rho$ 来简化分析。将信号跨越 $L-1$ 个时间步传播，导致该因子被乘以 $L-1$ 次。\n因此，对于长度为 $L$ 的序列，训练信号幅度 $S_{RNN}$ 为：\n$$\nS_{RNN}(L) = \\alpha \\rho^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $\\rho = 0.90$，我们有：\n$$\nS_{RNN}(L) = (0.90)^{L-1}\n$$\n\n**长短期记忆 (LSTM)**\nLSTM 处理长期依赖能力的关键在于其单元状态 $c_t$，它通过门控机制更新：$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$。这里，$f_t$ 是遗忘门，$\\odot$ 表示逐元素乘法。从 $c_t$到 $c_{t-1}$ 的通过单元状态的梯度路径主要由遗忘门缩放，即 $\\frac{\\partial c_t}{\\partial c_{t-1}} \\approx \\text{diag}(f_t)$。该问题通过假设遗忘门在所有时间步都是一个恒定标量 $f$ 来简化这一点。沿主记忆路径反向流动的梯度信号在每一步都被 $f$ 缩放，共 $L-1$ 步。\n训练信号幅度 $S_{LSTM}$ 为：\n$$\nS_{LSTM}(L) = \\alpha f^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $f = 0.99$，我们有：\n$$\nS_{LSTM}(L) = (0.99)^{L-1}\n$$\n\n**门控循环单元 (GRU)**\nGRU 的状态更新为 $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$，其中 $z_t$ 是更新门。项 $(1-z_t)$ 充当动态遗忘门，控制将多少前一状态 $h_{t-1}$ 传递到当前状态 $h_t$。因此，$h_t$ 相对于 $h_{t-1}$ 的梯度直接受此因子缩放。该问题通过假设更新门是一个恒定标量 $z$ 来简化分析。因此，在 $L-1$ 个反向传播步骤中，每一步的缩放因子都是 $(1-z)$。\n训练信号幅度 $S_{GRU}$ 为：\n$$\nS_{GRU}(L) = \\alpha (1-z)^{L-1}\n$$\n给定 $\\alpha = 1.0$ 和 $z = 0.05$，缩放因子为 $(1-0.05) = 0.95$。我们有：\n$$\nS_{GRU}(L) = (0.95)^{L-1}\n$$\n\n现在将实现这些推导出的表达式，以计算指定测试套件所需的值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the training-signal magnitude for RNN, GRU, and LSTM architectures\n    for the \"adding problem\" over various sequence lengths.\n    \"\"\"\n    # Define the parameters from the problem statement.\n    rho_rnn = 0.90  # Spectral radius for RNN\n    f_lstm = 0.99   # Forget gate value for LSTM\n    z_gru = 0.05    # Update gate value for GRU\n    alpha = 1.0     # Constant multiplicative factor (can be omitted as it's 1.0)\n    \n    # Define the test cases for sequence length L from the problem statement.\n    test_cases_L = [50, 75, 100, 500, 1000]\n\n    results = []\n    for L in test_cases_L:\n        # The number of recurrent transitions is L-1.\n        exponent = L - 1\n\n        # Calculate the training-signal magnitude for each architecture.\n        # S(L) = alpha * (base)^(L-1)\n        \n        # RNN\n        s_rnn = alpha * (rho_rnn ** exponent)\n        \n        # GRU\n        # The scaling factor is (1 - z)\n        s_gru = alpha * ((1 - z_gru) ** exponent)\n        \n        # LSTM\n        s_lstm = alpha * (f_lstm ** exponent)\n        \n        # Append results in the specified order: RNN, GRU, LSTM\n        results.append(s_rnn)\n        results.append(s_gru)\n        results.append(s_lstm)\n\n    # Format the final output as a comma-separated list of floats in brackets.\n    # e.g., [val1,val2,val3,...]\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3191191"}, {"introduction": "在了解了门控机制的重要性之后，我们现在深入LSTM的核心：细胞状态。本练习将剖析细胞状态的更新方程，揭示LSTM如何能够跨时间步完美地保存信息，从而形成一条“记忆高速公路”。你将首先证明完美记忆保存的条件，然后实现一个实用的诊断工具来识别网络在何时积极保存其状态，从而将理论原则与具体的分析方法联系起来 [@problem_id:3142761]。", "problem": "考虑一个长短期记忆（LSTM）单元，其单元状态更新由以下基本的、逐元素的递归关系定义\n$$\n\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t,\n$$\n其中，$t\\in\\{1,2,\\ldots,T\\}$，$\\mathbf{c}_t\\in\\mathbb{R}^d$ 是时间步 $t$ 的单元状态，$\\mathbf{f}_t\\in[0,1]^d$ 是遗忘门，$\\mathbf{i}_t\\in[0,1]^d$ 是输入门，$\\mathbf{g}_t\\in[-1,1]^d$ 是候选单元输入，$\\odot$ 表示哈达玛积。假设给定一个初始状态 $\\mathbf{c}_0\\in\\mathbb{R}^d$。完全基于此定义以及向量范数和不等式的标准性质进行分析。\n\n任务：\n1) 证明：如果对于所有 $t$，都有 $\\mathbf{f}_t=\\mathbf{1}$ 和 $\\mathbf{i}_t=\\mathbf{0}$，那么对于所有 $t$，都有 $\\mathbf{c}_t=\\mathbf{c}_0$。你的推理应仅使用单元更新定义和有效的证明方法。\n2) 提出一种原则性的诊断方法，用于在 $\\mathbf{c}_t$ 可观测时，检测一个已训练网络中接近保守的内存段。定义一个标量化的单步度量 $s_t$，使用欧几里得范数量化相对变化，并基于递归定义和范数不等式解释其合理性。将接近保守的段定义为 $s_t$ 保持在给定容差 $\\varepsilon0$ 以下的连续时间索引序列。\n3) 将该诊断方法实现为一个算法。给定 $\\{\\mathbf{c}_t\\}_{t=0}^T$、一个容差 $\\varepsilon0$ 和一个最小段长度 $L\\in\\mathbb{N}$，该算法返回：\n   - 满足 $s_t\\le \\varepsilon$ 的最长连续时间步 $t\\in\\{1,\\ldots,T\\}$ 的整数长度，以及\n   - 一个布尔值，指示是否存在至少一个长度不小于 $L$ 的序列。\n使用欧几里得范数，并定义\n$$\ns_t=\\frac{\\lVert \\mathbf{c}_t-\\mathbf{c}_{t-1}\\rVert_2}{\\lVert \\mathbf{c}_{t-1}\\rVert_2+\\delta},\n$$\n其中固定数值稳定器 $\\delta=10^{-12}$。\n4) 对于下面的每个测试用例，首先通过使用指定的 $\\mathbf{f}_t$、$\\mathbf{i}_t$、$\\mathbf{g}_t$ 和 $\\mathbf{c}_0$ 模拟LSTM递归关系，生成 $\\{\\mathbf{c}_t\\}_{t=0}^T$。然后应用你的诊断方法来产生两个所需的输出。\n\n测试套件：\n- 用例 A (完美保守)：$T=20$, $d=3$, $\\mathbf{c}_0=[2,-1,0.5]$，对于所有 $t$，$\\mathbf{f}_t=\\mathbf{1}$，$\\mathbf{i}_t=\\mathbf{0}$，$\\mathbf{g}_t=[0.3,-0.4,0.5]$，$\\varepsilon=10^{-12}$, $L=10$。\n- 用例 B (通过遗忘门导致的缓慢指数漂移)：$T=30$, $d=1$, $\\mathbf{c}_0=[1.0]$，对于所有 $t$，$\\mathbf{f}_t=[0.99]$，$\\mathbf{i}_t=[0.0]$，$\\mathbf{g}_t=[0.0]$，$\\varepsilon=0.005$, $L=5$。\n- 用例 C (小的恒定输入注入)：$T=25$, $d=2$, $\\mathbf{c}_0=[1.0,1.0]$，对于所有 $t$，$\\mathbf{f}_t=[1.0,1.0]$，$\\mathbf{i}_t=[0.02,0.02]$，$\\mathbf{g}_t=[0.5,0.5]$，$\\varepsilon=0.02$, $L=10$。\n- 用例 D (零状态，精确保守)：$T=10$, $d=3$, $\\mathbf{c}_0=[0.0,0.0,0.0]$，对于所有 $t$，$\\mathbf{f}_t=\\mathbf{1}$，$\\mathbf{i}_t=\\mathbf{0}$，$\\mathbf{g}_t=[0.7,-0.3,0.1]$，$\\varepsilon=10^{-12}$, $L=5$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个元素对应一个测试用例，顺序为 A、B、C、D，并且其本身是一个包含两个元素的列表，内容为：\n- 最长接近保守序列的整数长度，和\n- 一个布尔值，指示是否存在长度至少为 $L$ 的序列。\n该行不得包含空格。例如，一个包含两个用例的有效输出应如下所示：$[[3,True],[0,False]]$。", "solution": "此问题经评估为有效。它在科学上基于循环神经网络的原理，特别是长短期记忆（LSTM）单元架构。任务设定合理、客观且自成体系，为获得唯一解提供了所有必要的定义、方程和数据。该问题要求结合数学证明、原则性的先验推理和算法实现，这是高级计算科学问题的标准格式。\n\n### 任务1：状态保守性证明\n\n我们被要求证明，如果对于所有时间步 $t \\in \\{1, 2, \\ldots, T\\}$，遗忘门 $\\mathbf{f}_t = \\mathbf{1}$（全一向量）且输入门 $\\mathbf{i}_t = \\mathbf{0}$（全零向量），那么单元状态保持不变，即对于所有 $t$，$\\mathbf{c}_t = \\mathbf{c}_0$。\n\n我们将使用数学归纳法原理。设 $P(t)$ 为命题 $\\mathbf{c}_t = \\mathbf{c}_0$。\n\n**基本情况：** 我们必须证明 $P(1)$ 为真。单元状态更新方程如下：\n$$\n\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t\n$$\n对于 $t=1$，方程变为：\n$$\n\\mathbf{c}_1=\\mathbf{f}_1\\odot \\mathbf{c}_0+\\mathbf{i}_1\\odot \\mathbf{g}_1\n$$\n代入给定条件 $\\mathbf{f}_1 = \\mathbf{1}$ 和 $\\mathbf{i}_1 = \\mathbf{0}$：\n$$\n\\mathbf{c}_1 = \\mathbf{1} \\odot \\mathbf{c}_0 + \\mathbf{0} \\odot \\mathbf{g}_1\n$$\n与向量 $\\mathbf{1}$ 的哈达玛积是恒等操作（$\\mathbf{1} \\odot \\mathbf{v} = \\mathbf{v}$），而与向量 $\\mathbf{0}$ 的积结果为零向量（$\\mathbf{0} \\odot \\mathbf{v} = \\mathbf{0}$）。因此：\n$$\n\\mathbf{c}_1 = \\mathbf{c}_0 + \\mathbf{0} = \\mathbf{c}_0\n$$\n基本情况 $P(1)$ 成立。\n\n**归纳假设：** 假设对于某个整数 $k \\ge 1$，命题 $P(k)$ 为真，即 $\\mathbf{c}_k = \\mathbf{c}_0$。\n\n**归纳步骤：** 我们必须证明 $P(k+1)$ 为真，即 $\\mathbf{c}_{k+1} = \\mathbf{c}_0$。$t=k+1$ 的更新规则是：\n$$\n\\mathbf{c}_{k+1}=\\mathbf{f}_{k+1}\\odot \\mathbf{c}_k+\\mathbf{i}_{k+1}\\odot \\mathbf{g}_{k+1}\n$$\n代入给定条件 $\\mathbf{f}_{k+1} = \\mathbf{1}$ 和 $\\mathbf{i}_{k+1} = \\mathbf{0}$：\n$$\n\\mathbf{c}_{k+1}=\\mathbf{1}\\odot \\mathbf{c}_k+\\mathbf{0}\\odot \\mathbf{g}_{k+1}\n$$\n现在，我们应用归纳假设 $\\mathbf{c}_k = \\mathbf{c}_0$：\n$$\n\\mathbf{c}_{k+1} = \\mathbf{1} \\odot \\mathbf{c}_0 + \\mathbf{0}\n$$\n$$\n\\mathbf{c}_{k+1} = \\mathbf{c}_0\n$$\n因此，$P(k+1)$ 为真。\n\n根据数学归纳法原理，对于所有整数 $t \\ge 1$，$\\mathbf{c}_t = \\mathbf{c}_0$。证明完毕。\n\n### 任务2：接近保守内存的原则性诊断方法\n\n目标是设计一个标量度量 $s_t$，用以量化单元状态 $\\mathbf{c}_t$ 在相邻时间步之间的变化。如果一个状态不发生变化，即 $\\mathbf{c}_t \\approx \\mathbf{c}_{t-1}$，则称其为“保守的”。\n\n让我们分析变化向量 $\\Delta\\mathbf{c}_t = \\mathbf{c}_t - \\mathbf{c}_{t-1}$。使用递归关系：\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t) - \\mathbf{c}_{t-1}\n$$\n通过加上再减去 $\\mathbf{1}\\odot\\mathbf{c}_{t-1} = \\mathbf{c}_{t-1}$，我们可以分离出门控效应：\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t\\odot \\mathbf{c}_{t-1} - \\mathbf{c}_{t-1}) + (\\mathbf{i}_t\\odot \\mathbf{g}_t)\n$$\n从第一项中提出 $\\mathbf{c}_{t-1}$，得到：\n$$\n\\Delta\\mathbf{c}_t = (\\mathbf{f}_t - \\mathbf{1})\\odot \\mathbf{c}_{t-1} + \\mathbf{i}_t\\odot \\mathbf{g}_t\n$$\n此方程揭示了单元状态变化的两个来源：\n1.  **遗忘：** 项 $(\\mathbf{f}_t - \\mathbf{1})\\odot \\mathbf{c}_{t-1}$ 表示前一状态 $\\mathbf{c}_{t-1}$ 中被“遗忘”的部分。状态保守要求此项接近于零，这在 $\\mathbf{f}_t \\approx \\mathbf{1}$ 时发生。\n2.  **输入：** 项 $\\mathbf{i}_t\\odot \\mathbf{g}_t$ 表示写入单元状态的新信息。状态保守要求此项接近于零，这在 $\\mathbf{i}_t \\approx \\mathbf{0}$ 时发生（因为 $\\mathbf{g}_t$ 是有界的）。\n\n为量化此变化的幅度，我们使用欧几里得范数 $\\lVert \\Delta\\mathbf{c}_t \\rVert_2 = \\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$。这给出了变化的绝对度量。然而，对于一个小的状态向量，某个幅度的变化可能很显著，但对于一个大的状态向量则可以忽略不计。因此，一个更稳健的度量是*相对*变化，它通过状态向量本身的幅度来归一化绝对变化。\n\n所提出的诊断方法\n$$\ns_t=\\frac{\\lVert \\mathbf{c}_t-\\mathbf{c}_{t-1}\\rVert_2}{\\lVert \\mathbf{c}_{t-1}\\rVert_2+\\delta},\n$$\n正是这种相对变化。它衡量了变化向量的幅度占前一状态向量幅度的比例。\n\n*   通过使用欧几里得范数，它提供了状态向量所有 $d$ 个维度上总变化的标量摘要。\n*   通过用 $\\lVert \\mathbf{c}_{t-1} \\rVert_2$ 进行归一化，它成为一个尺度不变的度量。\n*   小常数 $\\delta  0$ 是一个标准的数值稳定器，用于防止在 $\\mathbf{c}_{t-1} = \\mathbf{0}$ 的情况下出现除以零的错误。在这种特殊情况下，如测试用例 D 所示，如果 $\\mathbf{f}_t=\\mathbf{1}$ 且 $\\mathbf{i}_t=\\mathbf{0}$，那么 $\\mathbf{c}_t = \\mathbf{c}_{t-1} = \\mathbf{0}$，分子为零，从而 $s_t=0$，正确地指示了完美的保守性。\n\n一个非常小的 $s_t$ 值（即对于某个小的 $\\varepsilon  0$，$s_t \\le \\varepsilon$）表明 $\\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$ 相对于 $\\lVert \\mathbf{c}_{t-1} \\rVert_2$ 很小。当变化的两个来源——遗忘和输入——都最小时，即当 $\\mathbf{f}_t \\approx \\mathbf{1}$ 和 $\\mathbf{i}_t \\approx \\mathbf{0}$ 时，此条件得到满足。因此，$s_t$ 是一个用于检测接近保守内存段的原则性且恰当的诊断方法。\n\n### 任务3：诊断实现的算法\n\n该算法的输入为单元状态序列 $\\{\\mathbf{c}_t\\}_{t=0}^T$、一个容差 $\\varepsilon$ 和一个最小长度 $L$。它分为两个主要阶段进行。\n\n**阶段1：计算每步的相对变化**\n1.  初始化一个空列表 `s_values`。\n2.  对 $t$ 从 1 迭代到 $T$。\n    a. 计算分子：`norm_diff` = $\\lVert \\mathbf{c}_t - \\mathbf{c}_{t-1} \\rVert_2$。\n    b. 计算分母：`norm_prev` = $\\lVert \\mathbf{c}_{t-1} \\rVert_2 + \\delta$。\n    c. 计算 $s_t = \\frac{\\text{norm\\_diff}}{\\text{norm\\_prev}}$ 并将其附加到 `s_values`。\n\n**阶段2：寻找最长的接近保守序列**\n1.  初始化 `max_run_length = 0` 和 `current_run_length = 0`。\n2.  遍历 `s_values`（对应于 $t=1, \\ldots, T$）。\n    a. 如果当前值 $s_t \\le \\varepsilon$：\n       i. 将 `current_run_length` 增加 1。\n    b. 否则（序列中断）：\n       i. 更新 `max_run_length = \\max(\\text{max\\_run\\_length, current\\_run\\_length})`。\n       ii. 重置 `current_run_length = 0`。\n3.  循环结束后，执行最后一次更新以考虑延伸到序列末尾的序列：`max_run_length = \\max(\\text{max\\_run\\_length, current\\_run\\_length})`。\n4.  确定布尔输出：`found_long_run = (\\text{max\\_run\\_length} \\ge L)`。\n5.  返回元组 (`max_run_length`, `found_long_run`)。\n\n该算法首先计算每步的度量，然后应用标准的线性扫描来查找满足给定条件的最长连续子序列，从而正确地实现了所需的诊断功能。\n\n### 任务4：应用于测试用例\n\n上述算法，结合对每个测试用例的 LSTM 递归关系的初步模拟，得出了结果。模拟步骤只是对 $t=1, \\ldots, T$ 迭代应用方程 $\\mathbf{c}_t=\\mathbf{f}_t\\odot \\mathbf{c}_{t-1}+\\mathbf{i}_t\\odot \\mathbf{g}_t$。实现代码在最终答案中提供。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LSTM memory conservation problem by simulating the cell state\n    updates for given test cases and then applying a diagnostic to find\n    near-conserved segments.\n    \"\"\"\n    \n    delta = 1e-12\n\n    test_cases = [\n        {\n            # Case A: perfect conservation\n            \"T\": 20, \"d\": 3, \"c0\": np.array([2.0, -1.0, 0.5]),\n            \"ft_gen\": lambda t: np.ones(3),\n            \"it_gen\": lambda t: np.zeros(3),\n            \"gt_gen\": lambda t: np.array([0.3, -0.4, 0.5]),\n            \"epsilon\": 1e-12, \"L\": 10\n        },\n        {\n            # Case B: slow exponential drift via forget gate\n            \"T\": 30, \"d\": 1, \"c0\": np.array([1.0]),\n            \"ft_gen\": lambda t: np.array([0.99]),\n            \"it_gen\": lambda t: np.array([0.0]),\n            \"gt_gen\": lambda t: np.array([0.0]),\n            \"epsilon\": 0.005, \"L\": 5\n        },\n        {\n            # Case C: small constant input injection\n            \"T\": 25, \"d\": 2, \"c0\": np.array([1.0, 1.0]),\n            \"ft_gen\": lambda t: np.ones(2),\n            \"it_gen\": lambda t: np.array([0.02, 0.02]),\n            \"gt_gen\": lambda t: np.array([0.5, 0.5]),\n            \"epsilon\": 0.02, \"L\": 10\n        },\n        {\n            # Case D: zero state, exact conservation\n            \"T\": 10, \"d\": 3, \"c0\": np.array([0.0, 0.0, 0.0]),\n            \"ft_gen\": lambda t: np.ones(3),\n            \"it_gen\": lambda t: np.zeros(3),\n            \"gt_gen\": lambda t: np.array([0.7, -0.3, 0.1]),\n            \"epsilon\": 1e-12, \"L\": 5\n        }\n    ]\n\n    def simulate_lstm(T, c0, ft_gen, it_gen, gt_gen):\n        \"\"\"Generates the cell state sequence {c_t}.\"\"\"\n        c_series = [c0]\n        c_prev = c0\n        for t in range(1, T + 1):\n            ft = ft_gen(t)\n            it = it_gen(t)\n            gt = gt_gen(t)\n            # LSTM cell state update recurrence\n            c_t = ft * c_prev + it * gt\n            c_series.append(c_t)\n            c_prev = c_t\n        return c_series\n\n    def run_diagnostic(c_series, epsilon, L):\n        \"\"\"\n        Calculates the longest run of near-conservation and checks if it\n        meets the minimum length L.\n        \"\"\"\n        T = len(c_series) - 1\n        \n        # Stage 1: Compute per-step relative change s_t\n        s_values = []\n        for t in range(1, T + 1):\n            c_t = c_series[t]\n            c_prev = c_series[t-1]\n            \n            norm_diff = np.linalg.norm(c_t - c_prev, ord=2)\n            norm_prev = np.linalg.norm(c_prev, ord=2)\n            \n            s_t = norm_diff / (norm_prev + delta)\n            s_values.append(s_t)\n            \n        # Stage 2: Find the longest run where s_t = epsilon\n        max_run_length = 0\n        current_run_length = 0\n        for s_t in s_values:\n            if s_t = epsilon:\n                current_run_length += 1\n            else:\n                if current_run_length  max_run_length:\n                    max_run_length = current_run_length\n                current_run_length = 0\n        \n        # Final check for a run extending to the end\n        if current_run_length  max_run_length:\n            max_run_length = current_run_length\n            \n        found_long_run = max_run_length = L\n        \n        return [max_run_length, found_long_run]\n\n    results = []\n    for case in test_cases:\n        # Generate the time series data for the cell states\n        c_series = simulate_lstm(\n            case[\"T\"], case[\"c0\"], case[\"ft_gen\"],\n            case[\"it_gen\"], case[\"gt_gen\"]\n        )\n        # Apply the diagnostic to the generated series\n        result = run_diagnostic(c_series, case[\"epsilon\"], case[\"L\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified, with no spaces.\n    case_strings = [f\"[{r[0]},{'True' if r[1] else 'False'}]\" for r in results]\n    final_output_string = f\"[{','.join(case_strings)}]\"\n    # Correct python's True/False to the problem's True/False\n    final_output_string = final_output_string.replace(\"'True'\", \"True\").replace(\"'False'\", \"False\")\n    print(final_output_string)\n\nsolve()\n```", "id": "3142761"}, {"introduction": "这项实践将我们对LSTM门控的理解应用于生物信息学中的一个具体任务，以展示输入门的特定功能。受生物学中“基因敲除”实验的启发，我们将模拟一个用于DNA序列分析的LSTM，并观察完全禁用输入门会产生什么效果。通过这个动手模拟，你将清晰地看到输入门在将新的相关信息（如基因基序）写入细胞记忆中的关键作用 [@problem_id:2425706]。", "problem": "考虑一个为脱氧核糖核酸（DNA）序列分析定义的单层长短期记忆（LSTM）循环神经网络，其核苷酸输入采用 one-hot 编码。设时间步 $t$ 的输入为 $x_t \\in \\mathbb{R}^{4}$，将核苷酸编码为 $x_t = [\\mathbb{1}\\{A\\}, \\mathbb{1}\\{C\\}, \\mathbb{1}\\{G\\}, \\mathbb{1}\\{T\\}]$。隐藏状态为 $h_t \\in \\mathbb{R}^{2}$，细胞状态为 $c_t \\in \\mathbb{R}^{2}$。对于 $t \\in \\{1,\\dots,T\\}$，LSTM 的动态由以下方程定义：\n$$\ni_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i), \\quad\nf_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f), \\quad\no_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o),\n$$\n$$\ng_t = \\tanh(W_g x_t + U_g h_{t-1} + b_g),\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t, \\quad\nh_t = o_t \\odot \\tanh(c_t),\n$$\n其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，$\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$，且 $\\odot$ 表示逐元素乘法。初始条件为 $h_0 = [0,0]^\\top$ 和 $c_0 = [0,0]^\\top$。\n\n所有的权重矩阵和偏置向量都是固定的，并给出如下。对于输入门：\n$$\nW_i = \\begin{bmatrix}\n1.0  -1.0  0.5  0.0 \\\\\n0.3  0.8  -0.5  0.2\n\\end{bmatrix}, \\quad\nU_i = \\begin{bmatrix}\n0.2  -0.1 \\\\\n0.1  0.2\n\\end{bmatrix}, \\quad\nb_i = \\begin{bmatrix}\n0.0 \\\\\n-0.2\n\\end{bmatrix}.\n$$\n对于遗忘门：\n$$\nW_f = \\begin{bmatrix}\n-0.5  0.6  0.1  0.3 \\\\\n0.2  -0.3  0.4  -0.2\n\\end{bmatrix}, \\quad\nU_f = \\begin{bmatrix}\n0.1  0.0 \\\\\n0.0  0.1\n\\end{bmatrix}, \\quad\nb_f = \\begin{bmatrix}\n0.5 \\\\\n0.5\n\\end{bmatrix}.\n$$\n对于输出门：\n$$\nW_o = \\begin{bmatrix}\n0.7  -0.4  0.0  0.2 \\\\\n-0.6  0.1  0.3  0.5\n\\end{bmatrix}, \\quad\nU_o = \\begin{bmatrix}\n0.2  0.1 \\\\\n-0.1  0.2\n\\end{bmatrix}, \\quad\nb_o = \\begin{bmatrix}\n0.0 \\\\\n0.1\n\\end{bmatrix}.\n$$\n对于候选状态：\n$$\nW_g = \\begin{bmatrix}\n0.3  0.3  -0.2  0.1 \\\\\n0.5  -0.7  0.2  -0.1\n\\end{bmatrix}, \\quad\nU_g = \\begin{bmatrix}\n0.1  -0.2 \\\\\n0.2  0.1\n\\end{bmatrix}, \\quad\nb_g = \\begin{bmatrix}\n0.0 \\\\\n0.0\n\\end{bmatrix}.\n$$\n\n定义一个基序为一个长度为 $m$ 的固定核苷酸字符串，例如对于基序 $ACG$，$m=3$。对于任意长度为 $T$ 的 DNA 序列，定义基序写入幅度为\n$$\n\\mathcal{M} = \\sum_{t \\in \\mathcal{I}} \\left\\| i_t \\odot g_t \\right\\|_1,\n$$\n其中 $\\left\\| \\cdot \\right\\|_1$ 表示对两个坐标取绝对值然后求和，$\\mathcal{I}$ 是属于基序任何一次出现的时间索引集合（如果基序在起始索引 $s$ 处出现，则 $\\{s, s+1, \\dots, s+m-1\\} \\subset \\mathcal{I}$）。\n\n为了模拟输入门敲除（knockout），对所有 $t$ 设置 $i_t \\equiv [0, 0]^\\top$，并保持所有其他方程不变。为了模拟遗忘偏置边界条件，将一个常数偏移量 $\\Delta \\in \\mathbb{R}$ 加到 $b_f$ 的两个条目上，即使用 $b_f' = b_f + [\\Delta, \\Delta]^\\top$。\n\n你的任务是为以下每个测试用例计算基序写入幅度 $\\mathcal{M}$。one-hot 编码为 $A \\mapsto [1,0,0,0]$, $C \\mapsto [0,1,0,0]$, $G \\mapsto [0,0,1,0]$, $T \\mapsto [0,0,0,1]$。对于所有用例，初始条件 $h_0$ 和 $c_0$ 均如上文规定。\n\n测试套件：\n- 用例 $1$（一般情况）：长度 $T = 63$ 的序列 $S_1$ 由 $T^{30}$、 $ACG$ 和 $T^{30}$ 连接而成；基序为 $ACG$；无敲除；遗忘偏置偏移量 $\\Delta = 0$。\n- 用例 $2$（输入门敲除）：与用例 1 相同，但进行输入门敲除（即对所有 $t$，$i_t \\equiv 0$）。\n- 用例 $3$（高遗忘门边界条件）：与用例 1 相同，但将遗忘偏置偏移量 $\\Delta = 5.0$ 应用于 $b_f$ 的两个条目。\n- 用例 $4$（不存在基序）：长度 $T = 63$ 的序列 $S_4$ 由 $T^{63}$ 构成；基序为 $ACG$；无敲除；遗忘偏置偏移量 $\\Delta = 0$。\n\n你的程序应产生单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果（例如 $[r_1,r_2,r_3,r_4]$），其中每个 $r_k$ 是用例 $k$ 的基序写入幅度 $\\mathcal{M}$，四舍五入到 6 位小数。不涉及物理单位，也不使用角度或百分比。在给定上述定义的情况下，输出必须是确定性的。", "solution": "所提出的问题是一个适定且有科学依据的计算任务。它要求基于一套完整且一致的定义方程和参数，实现一个标准的长短期记忆（LSTM）网络的前向传播。其目标是在四种不同的场景下，计算一个精确定义的度量——基序写入幅度 $\\mathcal{M}$。该问题是有效的，并且每种情况都有一个唯一的、确定性的解。\n\n问题的核心在于 LSTM 状态更新方程的迭代应用。对于从 $1$ 到 $T$ 的每个时间步 $t$，网络的隐藏状态 $h_t$ 和细胞状态 $c_t$ 是根据输入 $x_t$ 和前一状态 $h_{t-1}$、$c_{t-1}$ 计算得出的。初始状态被给定为零向量：$h_0 = [0,0]^\\top$ 和 $c_0 = [0,0]^\\top$。\n\n控制方程如下：\n$$\ni_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i)\n$$\n$$\nf_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f)\n$$\n$$\no_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o)\n$$\n$$\ng_t = \\tanh(W_g x_t + U_g h_{t-1} + b_g)\n$$\n$$\nc_t = f_t \\odot c_{t-1} + i_t \\odot g_t\n$$\n$$\nh_t = o_t \\odot \\tanh(c_t)\n$$\n激活函数是 sigmoid 函数 $\\sigma(z) = (1 + e^{-z})^{-1}$ 和双曲正切函数 $\\tanh(z)$。所有的权重矩阵（$W$）、循环矩阵（$U$）和偏置向量（$b$）都以固定的数值提供。输入 $x_t$ 是一个 4 维的 one-hot 向量，对应于四个核苷酸 {A, C, G, T} 之一。\n\n待计算的量是基序写入幅度，定义为：\n$$\n\\mathcal{M} = \\sum_{t \\in \\mathcal{I}} \\left\\| i_t \\odot g_t \\right\\|_1\n$$\n此处，$\\mathcal{I}$ 是与指定基序在输入序列中位置相对应的时间索引 $t$ 的集合。项 $i_t \\odot g_t$ 表示在时间 $t$ 写入细胞状态的新信息。L1 范数 $\\left\\| \\cdot \\right\\|_1$ 将该向量各分量的绝对值求和，从而量化其幅度。\n\n计算过程如下：\n1.  将 $h_0$ 和 $c_0$ 初始化为维度为 2 的零向量。\n2.  对于每个时间步 $t = 1, \\dots, T$：\n    a. 根据输入序列中相应位置的核苷酸确定输入向量 $x_t$。\n    b. 使用给定的方程和参数计算门激活值 $i_t, f_t, o_t$ 和候选状态 $g_t$。\n    c. 计算项 $i_t \\odot g_t$ 并将其存储。\n    d. 将细胞状态更新为 $c_t$，隐藏状态更新为 $h_t$。这些将成为下一个时间步的 $c_{t-1}$ 和 $h_{t-1}$。\n3.  识别出基序出现的时间索引集合 $\\mathcal{I}$。\n4.  通过对所有 $t \\in \\mathcal{I}$ 的已存储向量 $i_t \\odot g_t$ 的 L1 范数求和来计算 $\\mathcal{M}$。\n\n我们现在分析四个测试用例中的每一个。\n\n用例 1：一般情况。\n序列是 $S_1 = T^{30}ACGT^{30}$，基序是 $ACG$。序列长度为 $T = 63$。基序长度为 $m=3$，出现一次，从时间步 $t=31$ 开始。因此，相关的时间索引集合为 $\\mathcal{I} = \\{31, 32, 33\\}$。计算过程是运行 LSTM 63 个时间步，然后对 $t=31, 32, 33$ 的向量 $i_t \\odot g_t$ 的 L1 范数求和。\n\n用例 2：输入门敲除。\n模拟与用例 1 相同，但有约束条件，即对所有 $t$，$i_t \\equiv [0, 0]^\\top$。这直接意味着被求和的项 $i_t \\odot g_t$ 始终是零向量。因此，总和 $\\mathcal{M}$ 必须恰好为 $0$。不需要进行复杂的计算。\n\n用例 3：高遗忘门边界条件。\n模拟与用例 1 相同，但遗忘门偏置被修改为 $b_f' = b_f + [\\Delta, \\Delta]^\\top$，其中 $\\Delta = 5.0$。遗忘门 $f_t$ 的 sigmoid 函数的自变量将显著增加，导致 $f_t$ 非常接近 $[1, 1]^\\top$。这意味着网络将很少“忘记”其先前的细胞状态。计算过程与用例 1 相同，但使用 $b_f'$ 代替 $b_f$。\n\n用例 4：不存在基序。\n序列是 $S_4 = T^{63}$。基序 $ACG$ 在此序列中不出现。因此，索引集合 $\\mathcal{I}$ 是空集。根据定义，对空集的求和为 $0$。因此，$\\mathcal{M} = 0$。此用例同样不需要计算。\n\n实现将遵循这一逻辑，对用例 1 和 3 执行完整的前向传播，并为用例 2 和 4 提供 0 这个平凡解。所有数值都必须以适当的精度进行处理。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the motif write magnitude for an LSTM under various conditions.\n    \"\"\"\n\n    # Helper function for the sigmoid activation\n    def sigmoid(z):\n        # Clip to avoid overflow in exp, which can happen with large inputs like in Case 3\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n\n    # Helper function for the tanh activation. numpy's is numerically stable.\n    def tanh(z):\n        return np.tanh(z)\n\n    # Define all fixed weight matrices and bias vectors as numpy arrays.\n    # Dimensions are (output_dim, input_dim).\n    params = {\n        'Wi': np.array([[1.0, -1.0, 0.5, 0.0], [0.3, 0.8, -0.5, 0.2]]),\n        'Ui': np.array([[0.2, -0.1], [0.1, 0.2]]),\n        'bi': np.array([[0.0], [-0.2]]),\n        'Wf': np.array([[-0.5, 0.6, 0.1, 0.3], [0.2, -0.3, 0.4, -0.2]]),\n        'Uf': np.array([[0.1, 0.0], [0.0, 0.1]]),\n        'bf': np.array([[0.5], [0.5]]),\n        'Wo': np.array([[0.7, -0.4, 0.0, 0.2], [-0.6, 0.1, 0.3, 0.5]]),\n        'Uo': np.array([[0.2, 0.1], [-0.1, 0.2]]),\n        'bo': np.array([[0.0], [0.1]]),\n        'Wg': np.array([[0.3, 0.3, -0.2, 0.1], [0.5, -0.7, 0.2, -0.1]]),\n        'Ug': np.array([[0.1, -0.2], [0.2, 0.1]]),\n        'bg': np.array([[0.0], [0.0]])\n    }\n\n    # One-hot encoding for DNA nucleotides. Shape is (4, 1).\n    one_hot_map = {\n        'A': np.array([[1], [0], [0], [0]]),\n        'C': np.array([[0], [1], [0], [0]]),\n        'G': np.array([[0], [0], [1], [0]]),\n        'T': np.array([[0], [0], [0], [1]])\n    }\n\n    def run_simulation(sequence, motif, knockout, delta_bf):\n        \"\"\"\n        Performs a full forward pass of the LSTM and calculates the motif write magnitude.\n        \"\"\"\n        # Trivial cases can be returned immediately.\n        if knockout:\n            return 0.0\n        if motif not in sequence:\n            return 0.0\n        \n        T = len(sequence)\n        h_dim = 2\n        \n        # Initial conditions for hidden and cell states\n        h_prev = np.zeros((h_dim, 1))\n        c_prev = np.zeros((h_dim, 1))\n\n        # History to store the term i_t * g_t for each time step\n        it_gt_history = []\n        \n        # Apply forget-bias offset if specified\n        bf_mod = params['bf'] + delta_bf * np.ones((h_dim, 1))\n\n        # Iterate through the sequence (time steps t=1 to T)\n        for t_idx in range(T):\n            xt = one_hot_map[sequence[t_idx]]\n            \n            # LSTM gate and state calculations\n            it = sigmoid(params['Wi'] @ xt + params['Ui'] @ h_prev + params['bi'])\n            ft = sigmoid(params['Wf'] @ xt + params['Uf'] @ h_prev + bf_mod)\n            ot = sigmoid(params['Wo'] @ xt + params['Uo'] @ h_prev + params['bo'])\n            gt = tanh(params['Wg'] @ xt + params['Ug'] @ h_prev + params['bg'])\n\n            # Store the i_t * g_t term (element-wise product)\n            it_gt_history.append(it * gt)\n\n            # Update cell state and hidden state\n            ct = ft * c_prev + it * gt\n            ht = ot * tanh(ct)\n            \n            # Update previous states for the next iteration\n            h_prev = ht\n            c_prev = ct\n\n        # Calculate the motif write magnitude M\n        motif_write_magnitude = 0.0\n        m = len(motif)\n        \n        # Find all occurrences of the motif to determine the set of indices I.\n        # This implementation correctly handles non-overlapping and overlapping motifs.\n        motif_time_indices = set()\n        for i in range(T - m + 1):\n            if sequence[i:i+m] == motif:\n                for j in range(m):\n                    # The problem uses 1-based indexing for time t, which maps to\n                    # 0-based array index i. The motif at sequence indices i to i+m-1\n                    # corresponds to time steps t=i+1 to t=i+m.\n                    motif_time_indices.add(i + j)\n        \n        # Sum the L1 norms over the identified time indices\n        for idx in motif_time_indices:\n            motif_write_magnitude += np.sum(np.abs(it_gt_history[idx]))\n            \n        return motif_write_magnitude\n\n    # Define the test cases from the problem statement.\n    seq1 = 'T' * 30 + 'ACG' + 'T' * 30\n    seq4 = 'T' * 63\n    motif_str = 'ACG'\n    \n    test_cases = [\n        # Case 1: General case\n        {'sequence': seq1, 'motif': motif_str, 'knockout': False, 'delta_bf': 0.0},\n        # Case 2: Input gate knockout\n        {'sequence': seq1, 'motif': motif_str, 'knockout': True,  'delta_bf': 0.0},\n        # Case 3: High forget gate bias\n        {'sequence': seq1, 'motif': motif_str, 'knockout': False, 'delta_bf': 5.0},\n        # Case 4: No motif present\n        {'sequence': seq4, 'motif': motif_str, 'knockout': False, 'delta_bf': 0.0}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation(\n            case['sequence'],\n            case['motif'],\n            case['knockout'],\n            case['delta_bf']\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "2425706"}]}