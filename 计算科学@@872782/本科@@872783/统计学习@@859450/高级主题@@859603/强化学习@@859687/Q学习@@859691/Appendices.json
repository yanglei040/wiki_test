{"hands_on_practices": [{"introduction": "Q学习的核心在于最大化累积奖励，因此奖励函数的设计至关重要。一个自然而然的问题是：我们对奖励函数进行怎样的变换，才不会改变最终的最优策略？此练习探讨了奖励变换对最优策略不变性的影响，通过一个具体的反例说明了某些看似无害的“奖励归一化”操作，为何可能导致策略的根本性逆转，从而加深学习者对奖励塑造（reward shaping）背后原理的理解。[@problem_id:3163629]", "problem": "考虑一个无限时域折扣马尔可夫决策过程 (MDP)，其状态集为 $\\{s_0,s_1,s_2\\}$，折扣因子为 $\\gamma = 0.9$。状态 $s_0$ 有两个可用动作，$a_L$ 和 $a_R$，其确定性转移和即时奖励如下：\n- 在 $s_0$ 中采取动作 $a_L$ 产生即时奖励 $r(s_0,a_L) = 0$ 并转移到 $s_1$。\n- 在 $s_0$ 中采取动作 $a_R$ 产生即时奖励 $r(s_0,a_R) = 1$ 并转移到 $s_2$。\n\n状态 $s_1$ 和 $s_2$ 是终止状态，因为每个状态都只有一个虚拟动作，该动作会导向一个吸收性的终止条件：\n- 在 $s_1$ 中，唯一的动作产生即时奖励 $r(s_1,\\text{dummy}) = 5$，然后转移到一个没有后续奖励的终止吸收条件。\n- 在 $s_2$ 中，唯一的动作产生即时奖励 $r(s_2,\\text{dummy}) = 0$，然后转移到一个没有后续奖励的终止吸收条件。\n\n按通常意义定义最优动作价值函数 $Q^\\star(s,a)$，即从状态-动作对 $(s,a)$ 开始，此后遵循最优策略所能获得的期望折扣回报的上确界。现在定义一种逐状态的奖励归一化，它通过以下方式将原始奖励函数 $r$ 映射到一个新函数 $r_{\\text{norm}}$\n$$\nr_{\\text{norm}}(s,a) \\;=\\; r(s,a) \\;-\\; \\mu(s), \\quad \\text{其中 } \\mu(s) \\equiv \\frac{1}{|\\mathcal{A}(s)|} \\sum_{a' \\in \\mathcal{A}(s)} r(s,a')\n$$\n且 $\\mathcal{A}(s)$ 是状态 $s$ 下的可用动作集。令 $Q^\\star_{\\text{norm}}$ 为在奖励函数 $r_{\\text{norm}}$ 和相同折扣因子 $\\gamma$ 下的最优动作价值函数。\n\n仅使用回报、最优性和 Bellman 最优性原理的定义，推断这种逐状态归一化对 $s_0$ 处动作排序的影响。然后，考虑表格型 Q 学习的更新规则\n$$\nQ_{t+1}(s_t,a_t) \\;=\\; Q_t(s_t,a_t) \\;+\\; \\alpha_t \\Big( r_t \\;+\\; \\gamma \\max_{a'} Q_t(s_{t+1},a') \\;-\\; Q_t(s_t,a_t) \\Big),\n$$\n其中 $\\alpha_t \\in (0,1]$ 是学习率，并讨论此更新规则对奖励 $r$ 的仿射变换的不变性。\n\n选择所有正确的陈述：\n\nA. 在上述 MDP 中，将 $r$ 替换为逐状态中心化奖励 $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$，与原始奖励相比，会颠倒 $Q^\\star(s_0,a_L)$ 和 $Q^\\star(s_0,a_R)$ 的排序。\n\nB. 对于任何 MDP 和任何 $\\gamma \\in (0,1)$，将 $r$ 替换为 $r'(s,a,s') = r(s,a,s') + c$（其中 $c \\in \\mathbb{R}$ 是一个常数）会使每个状态下的最优贪心动作集合保持不变。\n\nC. 对于任何 MDP 和任何 $\\gamma \\in (0,1)$，将 $r$ 替换为 $r'(s,a,s') = a\\, r(s,a,s')$（其中标量 $a > 0$）会保留每个状态下的最优贪心动作集合。\n\nD. 对于任何 MDP 和任何 $\\gamma \\in (0,1)$，将 $r$ 替换为 $r'(s,a,s') = r(s,a,s') + b(s)$（其中 $b(s)$ 是一个仅依赖于当前状态 $s$ 的任意函数）会保留每个状态下的最优贪心动作集合。\n\nE. 在具有任意初始化 $Q_0$ 的表格型 Q 学习中，将奖励替换为 $r'(s,a,s') = r(s,a,s') + c$（其中 $c$ 为常数），将使得随时间 $t$ 变化的整个贪心策略轨迹与在 $r$ 下的轨迹严格相同（在每个时间点对每个访问过的状态都做出相同的动作选择），无论 $Q_0$ 如何。\n\nF. 令 $r'(s,a,s') = a\\, r(s,a,s') + b$，其中 $a > 0$ 且 $b \\in \\mathbb{R}$。如果初始化 $Q'_0(s,a) = a\\, Q_0(s,a) + \\tfrac{b}{1-\\gamma}$ 并在 $r'$ 上运行表格型 Q 学习，同时从 $Q_0$ 开始在 $r$ 上运行表格型 Q 学习，那么对于每个时间 $t$ 和每个状态-动作对 $(s,a)$，都有 $Q'_t(s,a) = a\\, Q_t(s,a) + \\tfrac{b}{1-\\gamma}$，因此两条学习轨迹上的贪心策略在每个时间点都重合。", "solution": "首先验证问题陈述，以确保其科学上合理、良定且客观。\n\n### 问题验证\n\n**步骤 1：提取的已知条件**\n- 考虑一个无限时域折扣马尔可夫决策过程 (MDP)。\n- 状态集：$\\{s_0, s_1, s_2\\}$。\n- 折扣因子：$\\gamma = 0.9$。\n- 状态 $s_0$ 的动作和确定性转移/奖励：\n  - 动作 $a_L$：转移到 $s_1$，即时奖励 $r(s_0, a_L) = 0$。\n  - 动作 $a_R$：转移到 $s_2$，即时奖励 $r(s_0, a_R) = 1$。\n- 状态 $s_1$：单个虚拟动作产生即时奖励 $r(s_1, \\text{dummy}) = 5$ 并转移到终止吸收状态。\n- 状态 $s_2$：单个虚拟动作产生即时奖励 $r(s_2, \\text{dummy}) = 0$ 并转移到终止吸收状态。\n- 最优动作价值函数：$Q^\\star(s,a)$ 定义为期望折扣回报的上确界。\n- 逐状态奖励归一化：$r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$，其中 $\\mu(s) = \\frac{1}{|\\mathcal{A}(s)|} \\sum_{a' \\in \\mathcal{A}(s)} r(s,a')$。\n- 表格型 Q 学习更新规则：$Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \\alpha_t ( r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a') - Q_t(s_t,a_t) )$，学习率为 $\\alpha_t \\in (0,1]$。\n\n**步骤 2：使用提取的已知条件进行验证**\n- **科学依据：** 该问题在 MDP 和强化学习的标准数学框架内表述。所有术语和概念都是标准的且定义明确。\n- **良定性：** 该 MDP 的确定性转移和奖励被完全指定。所提出的问题是关于价值函数和学习算法性质的明确定义的数学探究。待计算的量存在唯一解。\n- **客观性：** 问题以精确、正式的语言陈述，没有歧义或主观内容。\n- **一致性和完整性：** 问题提供了所有必要信息，并且不包含内部矛盾。\n\n**步骤 3：结论和措施**\n问题有效。接下来对每个选项进行分析。\n\n### 解答与选项分析\n\n最优动作价值函数 $Q^\\star(s, a)$ 满足 Bellman 最优方程。对于确定性转移 $s \\xrightarrow{a} s'$，该方程为：\n$$Q^\\star(s,a) = r(s,a) + \\gamma \\max_{a'} Q^\\star(s', a') = r(s,a) + \\gamma V^\\star(s')$$\n其中 $V^\\star(s) = \\max_a Q^\\star(s,a)$ 是最优状态价值函数。\n\n**A. 对特定 MDP 进行奖励归一化分析**\n\n首先，我们计算原始奖励函数 $r$ 的最优 Q 值。\n状态 $s_1$ 和 $s_2$ 实际上是终止状态。在这些状态下采取一个动作会得到最终奖励，然后过程结束（转移到一个 $V=0$ 的吸收状态）。\n对于状态 $s_1$：\n$$Q^\\star(s_1, \\text{dummy}) = r(s_1, \\text{dummy}) + \\gamma \\cdot 0 = 5$$\n$$V^\\star(s_1) = 5$$\n对于状态 $s_2$：\n$$Q^\\star(s_2, \\text{dummy}) = r(s_2, \\text{dummy}) + \\gamma \\cdot 0 = 0$$\n$$V^\\star(s_2) = 0$$\n现在，对于状态 $s_0$：\n$$Q^\\star(s_0, a_L) = r(s_0, a_L) + \\gamma V^\\star(s_1) = 0 + 0.9 \\cdot 5 = 4.5$$\n$$Q^\\star(s_0, a_R) = r(s_0, a_R) + \\gamma V^\\star(s_2) = 1 + 0.9 \\cdot 0 = 1$$\n排序为 $Q^\\star(s_0, a_L)  Q^\\star(s_0, a_R)$，因此最优动作是 $a_L$。\n\n接下来，我们计算归一化奖励 $r_{\\text{norm}}$。\n对于 $s_0$，$\\mathcal{A}(s_0)=\\{a_L, a_R\\}$，所以 $|\\mathcal{A}(s_0)|=2$。\n$$\\mu(s_0) = \\frac{1}{2}(r(s_0, a_L) + r(s_0, a_R)) = \\frac{1}{2}(0+1) = 0.5$$\n$$r_{\\text{norm}}(s_0, a_L) = r(s_0, a_L) - \\mu(s_0) = 0 - 0.5 = -0.5$$\n$$r_{\\text{norm}}(s_0, a_R) = r(s_0, a_R) - \\mu(s_0) = 1 - 0.5 = 0.5$$\n对于 $s_1$，$\\mathcal{A}(s_1)=\\{\\text{dummy}\\}$，所以 $|\\mathcal{A}(s_1)|=1$。\n$$\\mu(s_1) = r(s_1, \\text{dummy}) = 5$$\n$$r_{\\text{norm}}(s_1, \\text{dummy}) = 5 - 5 = 0$$\n对于 $s_2$，$\\mathcal{A}(s_2)=\\{\\text{dummy}\\}$，所以 $|\\mathcal{A}(s_2)|=1$。\n$$\\mu(s_2) = r(s_2, \\text{dummy}) = 0$$\n$$r_{\\text{norm}}(s_2, \\text{dummy}) = 0 - 0 = 0$$\n\n现在我们计算 $r_{\\text{norm}}$ 的最优 Q 值，$Q^\\star_{\\text{norm}}$。\n对于 $s_1$：$Q^\\star_{\\text{norm}}(s_1, \\text{dummy}) = r_{\\text{norm}}(s_1, \\text{dummy}) = 0$，所以 $V^\\star_{\\text{norm}}(s_1) = 0$。\n对于 $s_2$：$Q^\\star_{\\text{norm}}(s_2, \\text{dummy}) = r_{\\text{norm}}(s_2, \\text{dummy}) = 0$，所以 $V^\\star_{\\text{norm}}(s_2) = 0$。\n对于 $s_0$：\n$$Q^\\star_{\\text{norm}}(s_0, a_L) = r_{\\text{norm}}(s_0, a_L) + \\gamma V^\\star_{\\text{norm}}(s_1) = -0.5 + 0.9 \\cdot 0 = -0.5$$\n$$Q^\\star_{\\text{norm}}(s_0, a_R) = r_{\\text{norm}}(s_0, a_R) + \\gamma V^\\star_{\\text{norm}}(s_2) = 0.5 + 0.9 \\cdot 0 = 0.5$$\n新的排序是 $Q^\\star_{\\text{norm}}(s_0, a_R)  Q^\\star_{\\text{norm}}(s_0, a_L)$，因此最优动作是 $a_R$。\n$s_0$ 处的动作排序被颠倒了。\n\nA 的结论：**正确**。\n\n**B. 对常数奖励平移的不变性**\n\n令 $r'(s,a,s') = r(s,a,s') + c$。令 $Q^\\star$ 和 $Q'^\\star$ 分别为对应的最优动作价值函数。\n我们可以通过对价值迭代的步骤进行归纳或直接验证来证明 $Q'^\\star(s,a) = Q^\\star(s,a) + \\frac{c}{1-\\gamma}$。\n假设此关系成立。$Q'^\\star$ 的 Bellman 方程为：\n$$Q'^\\star(s,a) = \\mathbb{E}_{s'}[r(s,a,s') + c + \\gamma \\max_{a'} Q'^\\star(s', a')]$$\n代入我们假设的关系：\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = \\mathbb{E}_{s'}[r(s,a,s') + c + \\gamma \\max_{a'} (Q^\\star(s', a') + \\frac{c}{1-\\gamma})]$$\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = \\mathbb{E}_{s'}[r(s,a,s')] + c + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^\\star(s', a')] + \\gamma \\frac{c}{1-\\gamma}$$\n$$Q^\\star(s,a) + \\frac{c}{1-\\gamma} = (\\mathbb{E}_{s'}[r(s,a,s')] + \\gamma \\mathbb{E}_{s'}[\\max_{a'} Q^\\star(s', a')]) + (c + \\frac{\\gamma c}{1-\\gamma})$$\n第一个括号是 $Q^\\star(s,a)$。第二个是 $\\frac{c(1-\\gamma)+\\gamma c}{1-\\gamma} = \\frac{c}{1-\\gamma}$。该方程成立。\n状态 $s$ 的最优策略由 $\\arg\\max_a Q^\\star(s,a)$ 给出。对于新的奖励，它是 $\\arg\\max_a Q'^\\star(s,a)$。\n$$\\arg\\max_a Q'^\\star(s,a) = \\arg\\max_a \\left( Q^\\star(s,a) + \\frac{c}{1-\\gamma} \\right) = \\arg\\max_a Q^\\star(s,a)$$\n由于 $\\frac{c}{1-\\gamma}$ 对于动作 $a$ 是一个常数，它不影响 argmax。最优动作集合保持不变。\n\nB 的结论：**正确**。\n\n**C. 对奖励正向缩放的不变性**\n\n令 $r'(s,a,s') = a\\,r(s,a,s')$ 且 $a > 0$。令 $Q^\\star$ 和 $Q'^\\star$ 分别为对应的最优动作价值函数。\n我们断言 $Q'^\\star(s,a) = a\\,Q^\\star(s,a)$。我们使用 Bellman 方程来验证这一点。\n$$Q'^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma \\max_{a'} Q'^\\star(s', a')]$$\n$$a\\,Q^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma \\max_{a'} (a\\,Q^\\star(s', a'))]$$\n因为 $a0$，所以 $\\max_{a'} (a\\,X(a')) = a\\,\\max_{a'} X(a')$。\n$$a\\,Q^\\star(s,a) = \\mathbb{E}_{s'}[a\\,r(s,a,s') + \\gamma a \\max_{a'} Q^\\star(s', a')]$$\n$$a\\,Q^\\star(s,a) = a \\left( \\mathbb{E}_{s'}[r(s,a,s') + \\gamma \\max_{a'} Q^\\star(s', a')] \\right)$$\n除以 $a$ 后，恢复为 $Q^\\star$ 的 Bellman 方程。该关系成立。\n新策略由以下确定：\n$$\\arg\\max_a Q'^\\star(s,a) = \\arg\\max_a (a\\,Q^\\star(s,a))$$\n由于 $a > 0$，最大化 $a\\,Q^\\star(s,a)$ 等价于最大化 $Q^\\star(s,a)$。最优动作集合保持不变。\n\nC 的结论：**正确**。\n\n**D. 对状态依赖奖励平移的不变性**\n\n令 $r'(s,a,s') = r(s,a,s') + b(s)$。这是一种通用的奖励塑造形式。为使最优策略保持不变，塑造必须是“基于势”的形式 $F(s,s') = \\gamma\\Phi(s') - \\Phi(s)$，其中 $\\Phi$ 是状态上的某个实值函数。一个仅依赖于源状态的塑造项 $b(s)$ 通常不具有这种形式。\n我们可以用选项 A 的结果作为一个直接的反例。在该情况下，奖励变换为 $r_{\\text{norm}}(s,a) = r(s,a) - \\mu(s)$。这是该选项中变换的一个实例，其中 $b(s) = -\\mu(s)$。正如我们计算的，状态 $s_0$ 的最优策略从偏好 $a_L$ 变为偏好 $a_R$。因此，状态依赖的奖励平移通常不会保留最优贪心动作集合。\n\nD 的结论：**错误**。\n\n**E. Q 学习轨迹在常数奖励平移下的不变性**\n\n该陈述声称，对于 $r' = r+c$ 和任意初始化 $Q_0$（意味着 $Q'_0 = Q_0$），贪心策略的序列是相同的。\n让我们追踪第一次更新。假设在 $t=0$ 时，我们处于状态 $s_0$，贪心动作为 $a_0 = \\arg\\max_a Q_0(s_0,a)$。两个系统选择相同的动作。在观察到转移到 $s_1$ 和奖励 $r_0$（或 $r'_0 = r_0+c$）后，更新如下：\n$$Q_1(s_0, a_0) = (1-\\alpha_0)Q_0(s_0,a_0) + \\alpha_0(r_0 + \\gamma \\max_{a'} Q_0(s_1,a'))$$\n$$Q'_1(s_0, a_0) = (1-\\alpha_0)Q'_0(s_0,a_0) + \\alpha_0(r'_0 + \\gamma \\max_{a'} Q'_0(s_1,a'))$$\n使用 $Q'_0 = Q_0$ 和 $r'_0=r_0+c$：\n$$Q'_1(s_0, a_0) = (1-\\alpha_0)Q_0(s_0,a_0) + \\alpha_0(r_0+c + \\gamma \\max_{a'} Q_0(s_1,a'))$$\n$$Q'_1(s_0, a_0) = Q_1(s_0, a_0) + \\alpha_0 c$$\n对于任何其他状态-动作对 $(s,a) \\ne (s_0, a_0)$，不发生更新，因此 $Q_1(s,a) = Q_0(s,a)$ 且 $Q'_1(s,a) = Q_0(s,a)$。\n现在考虑时间 $t=1$ 时 $s_0$ 的贪心策略。它是 $\\arg\\max_a Q_1(s_0,a)$ 对比 $\\arg\\max_a Q'_1(s_0,a)$。\n除 $a_0$ 之外的动作的 Q 值与 $Q_0$ 相比没有变化，而 $Q(s_0,a_0)$ 已被更新。$Q'$ 的更新包含一个额外的项 $\\alpha_0 c$。\n这可能会改变动作排序。设 $Q_0(s_0, a_0) = 10, Q_0(s_0, a_1)=9.9$。选择动作 $a_0$。设 TD 目标较低，例如 $r_0 + \\gamma \\max Q_0(s_1, \\cdot) = 5$，且 $\\alpha_0=0.1$。\n$Q_1(s_0, a_0) = (1-0.1) \\cdot 10 + 0.1 \\cdot 5 = 9.5$。在 $s_0$ 的新策略偏好 $a_1$，因为 $Q_1(s_0, a_1)=9.9  9.5$。\n对于 $Q'$，设 $c=20$。TD 目标为 $5+20=25$。\n$Q'_1(s_0, a_0) = (1-0.1) \\cdot 10 + 0.1 \\cdot 25 = 9 + 2.5 = 11.5$。在 $s_0$ 的新策略仍然偏好 $a_0$，因为 $Q'_1(s_0,a_0)=11.5  Q'_1(s_0,a_1)=9.9$。\n贪心策略出现分歧。\n\nE 的结论：**错误**。\n\n**F. Q 学习轨迹在特殊初始化下的仿射变换不变性**\n\n奖励变换为 $r' = a\\,r + b$（$a0$），初始化为 $Q'_0(s,a) = a\\,Q_0(s,a) + \\frac{b}{1-\\gamma}$。该陈述声称对于所有 $t$，$Q'_t(s,a) = a\\,Q_t(s,a) + \\frac{b}{1-\\gamma}$，并且策略重合。\n如果这个 Q 值关系对所有 $t$ 都成立，那么贪心策略是相同的：\n$\\arg\\max_{a'} Q'_t(s, a') = \\arg\\max_{a'} (a\\,Q_t(s,a') + \\frac{b}{1-\\gamma}) = \\arg\\max_{a'} Q_t(s,a')$。\n这意味着两个过程遵循相同的 $(s_t, a_t)$ 轨迹。\n我们通过归纳法证明 Q 值关系。\n基本情况 ($t=0$)：根据初始化的定义，该关系成立。\n归纳步骤：假设对于所有 $(s,a)$，$Q'_t(s,a) = a\\,Q_t(s,a) + \\frac{b}{1-\\gamma}$。考虑在步骤 $t$ 对 $(s_t, a_t)$ 的更新。\n$$Q'_{t+1}(s_t,a_t) = (1-\\alpha_t)Q'_t(s_t,a_t) + \\alpha_t \\left( r'_t + \\gamma \\max_{a'} Q'_t(s_{t+1},a') \\right)$$\n代入归纳假设和 $r'_t=ar_t+b$：\n$$Q'_{t+1}(s_t,a_t) = (1-\\alpha_t)\\left(a\\,Q_t(s_t,a_t) + \\frac{b}{1-\\gamma}\\right) + \\alpha_t \\left( ar_t+b + \\gamma \\max_{a'} \\left(a\\,Q_t(s_{t+1},a') + \\frac{b}{1-\\gamma}\\right) \\right)$$\n因为 $a0$，$\\max$ 项简化为：\n$$ = (1-\\alpha_t)\\left(a\\,Q_t + \\frac{b}{1-\\gamma}\\right) + \\alpha_t \\left( ar_t+b + \\gamma \\left(a\\max_{a'}Q_t + \\frac{b}{1-\\gamma}\\right) \\right)$$\n对乘以 $a$ 的项进行分组：\n$$a \\left[ (1-\\alpha_t)Q_t(s_t,a_t) + \\alpha_t \\left( r_t + \\gamma \\max_{a'} Q_t(s_{t+1},a') \\right) \\right] = a\\,Q_{t+1}(s_t,a_t)$$\n对涉及 $b$ 的项进行分组：\n$$ (1-\\alpha_t)\\frac{b}{1-\\gamma} + \\alpha_t b + \\alpha_t \\gamma \\frac{b}{1-\\gamma} = b \\left[ \\frac{1-\\alpha_t}{1-\\gamma} + \\alpha_t + \\frac{\\alpha_t \\gamma}{1-\\gamma} \\right]$$\n$$ = b \\left[ \\frac{1-\\alpha_t + \\alpha_t(1-\\gamma) + \\alpha_t\\gamma}{1-\\gamma} \\right] = b \\left[ \\frac{1-\\alpha_t + \\alpha_t - \\alpha_t\\gamma + \\alpha_t\\gamma}{1-\\gamma} \\right] = b \\frac{1}{1-\\gamma}$$\n结合这些，我们得到 $Q'_{t+1}(s_t,a_t) = a\\,Q_{t+1}(s_t,a_t) + \\frac{b}{1-\\gamma}$。\n对于任何 $(s,a) \\neq (s_t, a_t)$，值保持不变，因此关系也成立。归纳完成。\n\nF 的结论：**正确**。", "answer": "$$\\boxed{ABCF}$$", "id": "3163629"}, {"introduction": "在随机环境中，奖励的内在不确定性会给Q值的估计带来噪声。当多个动作的真实价值相近时，这种噪声可能导致智能体在不同动作间频繁切换，形成所谓的“策略振荡”，从而降低学习效率。本练习提供了一个动手编程的机会，通过模拟一个简单的场景，你将亲眼观察到策略振荡现象，并实现一种经典的平滑技术——Polyak平均，来验证它在稳定Q值估计和抑制策略振荡方面的有效性。[@problem_id:3163593]", "problem": "考虑一个有限马尔可夫决策过程 (MDP)，其具有单一状态 $s$ 和两个动作 $a \\in \\{0,1\\}$。其动态是确定性的：采取任何动作都会使系统以概率 $1$ 停留在状态 $s$。在时间 $t$ 的即时奖励 $R_t$ 是随机的，并取决于所选的动作：以动作 $a$ 为条件，奖励 $R_t$ 是从一个均值为 $\\mu_a$、标准差为 $\\sigma$ 的正态分布中独立抽取的。设折扣因子为 $\\gamma \\in (0,1)$。\n\n定义在时间 $t$ 的动作价值函数 $Q_t(s,a)$，对于两个动作，初始值均为 $Q_0(s,a)=0$。学习过程遵循贝尔曼最优性递归的一种随机近似，形式为带有学习率 $\\alpha \\in (0,1]$ 的单步时序差分更新：\n$$\nQ_{t+1}(s,a_t) = (1-\\alpha) Q_t(s,a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right),\n$$\n且对于 $a \\neq a_t$，有 $Q_{t+1}(s,a) = Q_t(s,a)$。在时间 $t$ 的动作选择是相对于当前估计的贪心选择，即 $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$，平局则通过选择较小的动作索引来确定性地打破。\n\n定义在时间 $t \\ge 2$ 发生“策略翻转”，指在时间 $t$ 选择的贪心动作与在时间 $t-1$ 选择的贪心动作不同。\n\n现在引入一个平滑动作价值估计量 $\\bar{Q}_t(s,a)$，初始值为 $\\bar{Q}_0(s,a)=0$，由原始 $Q_t$ 的 Polyak 风格的指数移动平均定义：\n$$\n\\bar{Q}_t(s,a) = \\rho \\,\\bar{Q}_{t-1}(s,a) + (1-\\rho) \\, Q_t(s,a),\n$$\n其中平滑系数为 $\\rho \\in [0,1]$。在平滑变体中，动作选择是相对于 $\\bar{Q}_t$ 的贪心选择，即 $a_t \\in \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$，并使用相同的平局打破规则。底层的 $Q_t$ 使用与上述相同的时序差分规则进行更新；在每个时间步的 $Q_t$ 更新之后应用平滑更新。\n\n您的任务是为每个测试用例在 $T$ 步的时间范围内实现两个模拟：\n- 一个原始贪心模拟：相对于 $Q_t$ 进行贪心动作选择。\n- 一个平滑贪心模拟：相对于 $\\bar{Q}_t$ 进行贪心动作选择。\n\n对于每个模拟，计算在 $T$ 步内的策略翻转总数。为保证可复现性，请在每个测试用例中使用指定的随机种子来初始化一个伪随机数生成器，并从正确的正态分布中抽取奖励。\n\n为每个测试用例报告一个布尔值，当且仅当平滑贪心选择下的翻转次数严格小于原始贪心选择下的翻转次数时，该值为真，否则为假。\n\n使用以下测试套件，其中每个用例指定为 $(\\mu_0,\\mu_1,\\sigma,\\alpha,\\gamma,T,\\rho,\\text{seed})$:\n- 用例 1: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 123)$。\n- 用例 2: $(0.05, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 456)$。\n- 用例 3: $(0.0, 0.0, 0.0, 0.9, 0.9, 200, 0.95, 789)$。\n- 用例 4: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 1.0, 321)$。\n\n所有量均为无单位量。您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目是对应测试用例的布尔结果，按顺序排列，例如 $[{\\text{True}},{\\text{False}},{\\text{True}},{\\text{True}}]$。", "solution": "已分析并验证用户提供的问题陈述。\n\n### 第 1 步：提取已知条件\n\n-   **MDP**: 一个单一状态 $s$ 和两个动作 $a \\in \\{0,1\\}$。\n-   **转移**: 确定性，$P(s' = s | s, a) = 1$。\n-   **奖励**: 随机，对于选择的动作 $a_t$，有 $R_t \\sim \\mathcal{N}(\\mu_a, \\sigma)$。\n-   **折扣因子**: $\\gamma \\in (0,1)$。\n-   **动作价值函数**: $Q_t(s,a)$，初始值为 $Q_0(s,a)=0$ for $a \\in \\{0,1\\}$。\n-   **学习率**: $\\alpha \\in (0,1]$。\n-   **TD 更新规则**: $Q_{t+1}(s,a_t) = (1-\\alpha) Q_t(s,a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right)$，且对于 $a \\neq a_t$，有 $Q_{t+1}(s,a) = Q_t(s,a)$。\n-   **原始动作选择**: 贪心，$a_t \\in \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$。平局通过选择较小的动作索引（即动作 0）来打破。\n-   **平滑动作价值函数**: $\\bar{Q}_t(s,a) = \\rho \\,\\bar{Q}_{t-1}(s,a) + (1-\\rho) \\, Q_t(s,a)$，初始值为 $\\bar{Q}_0(s,a)=0$。\n-   **平滑系数**: $\\rho \\in [0,1]$。\n-   **平滑动作选择**: 贪心，$a_t \\in \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$，具有相同的平局打破规则。在每个时间步的 $Q_t$ 更新之后应用平滑更新。\n-   **策略翻转**: 在时间 $t \\ge 2$ 发生，当贪心动作 $a_t$ 与贪心动作 $a_{t-1}$ 不同时。\n-   **任务**: 对于每个测试用例，在 $T$ 步的时间范围内模拟两种情景（原始贪心选择和平滑贪心选择）。计算每种情景下的策略翻转总数。\n-   **输出**: 对于每个测试用例，输出一个布尔值：如果平滑模拟的翻转次数严格少于原始模拟，则为真，否则为假。\n-   **测试套件**: $(\\mu_0, \\mu_1, \\sigma, \\alpha, \\gamma, T, \\rho, \\text{seed})$\n    -   用例 1: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 123)$\n    -   用例 2: $(0.05, 0.0, 1.0, 0.9, 0.9, 400, 0.95, 456)$\n    -   用例 3: $(0.0, 0.0, 0.0, 0.9, 0.9, 200, 0.95, 789)$\n    -   用例 4: $(0.0, 0.0, 1.0, 0.9, 0.9, 400, 1.0, 321)$\n\n### 第 2 步：使用提取的已知条件进行验证\n\n该问题在科学上基于强化学习的既定理论，特别是 Q-learning。它描述了一个定义明确的计算任务。所有参数、初始条件、更新规则和动作选择机制都已明确定义。为每个测试用例使用随机种子确保了随机模拟是可复现的，并产生唯一的解。在时间 $t \\ge 2$ 定义“策略翻转”略显非传统（因为它忽略了 $a_0$ 和 $a_1$ 之间的第一次潜在翻转），但这个定义是明确的，并且可以照此实现。该问题是客观、可形式化的，并提出了一个在标准 Q-learning 和一种带有价值函数平滑的变体之间进行的非平凡比较。\n\n### 第 3 步：结论与行动\n\n问题陈述被判定为**有效**。将提供解决方案。\n\n### 基于原则的算法设计\n\n该问题要求在一个简单的单状态马尔可夫决策过程 (MDP) 中实现并比较 Q-learning 智能体的两个变体。解决方案的核心是构建一个模拟，该模拟迭代地更新动作价值估计并记录智能体的行为。\n\n由于 MDP 只有一个状态 $s$，动作价值函数 $Q_t(s,a)$ 可以简化为一个双元素向量，其中元素对应于动作 $a=0$ 和 $a=1$ 的价值。让我们将此向量表示为 $\\mathbf{Q}_t = [Q_t(s,0), Q_t(s,1)]$。类似地，平滑动作价值函数由向量 $\\bar{\\mathbf{Q}}_t$ 表示。\n\n对于每个测试用例，在 $T$ 步的时间范围（从 $t=0$ 到 $t=T-1$）内运行两个独立的模拟。每个模拟都必须使用指定的种子重新初始化随机数生成器，以确保底层随机奖励过程的可比性。\n\n**模拟循环 ($t = 0, \\dots, T-1$)**\n\n模拟以离散时间步进行。在每个时间步 $t$ 开始时，智能体拥有从上一步结转而来的动作价值估计 $\\mathbf{Q}_t$ 和（在平滑情况下）$\\bar{\\mathbf{Q}}_t$。\n\n1.  **动作选择**: 根据当前的价值估计选择一个动作 $a_t$。\n    -   在“原始”模拟中，智能体相对于 $\\mathbf{Q}_t$ 是贪心的：$a_t = \\arg\\max_{a \\in \\{0,1\\}} Q_t(s,a)$。\n    -   在“平滑”模拟中，智能体相对于 $\\bar{\\mathbf{Q}}_t$ 是贪心的：$a_t = \\arg\\max_{a \\in \\{0,1\\}} \\bar{Q}_t(s,a)$。\n    -   平局打破规则规定，如果 $Q_t(s,0) = Q_t(s,1)$（或 $\\bar{Q}_t(s,0) = \\bar{Q}_t(s,1)$），则选择动作 $a=0$。\n\n2.  **奖励观察**: 从正态分布 $\\mathcal{N}(\\mu_{a_t}, \\sigma)$ 中抽取一个奖励 $R_t$。\n\n3.  **动作价值更新 ($Q$-更新)**: 核心学习步骤使用单步时序差分 (TD) 更新。所选动作 $a_t$ 的价值被更新，而另一个保持不变。新的价值向量 $\\mathbf{Q}_{t+1}$ 计算如下：\n    $$\n    Q_{t+1}(s, a_t) = (1-\\alpha) Q_t(s, a_t) + \\alpha \\left( R_t + \\gamma \\max_{b \\in \\{0,1\\}} Q_t(s,b) \\right)\n    $$\n    $$\n    Q_{t+1}(s, a) = Q_t(s, a) \\quad \\text{for } a \\neq a_t\n    $$\n    请注意，TD 目标 $R_t + \\gamma \\max_{b} Q_t(s,b)$ 是使用*当前更新之前*的价值估计 $\\mathbf{Q}_t$ 计算的。\n\n4.  **平滑动作价值更新 ($\\bar{Q}$-更新)**: 此步骤仅在平滑模拟中执行。根据问题描述，此更新在 $Q$-更新*之后*发生。新的平滑价值向量 $\\bar{\\mathbf{Q}}_{t+1}$ 作为其前一个值 $\\bar{\\mathbf{Q}}_t$ 和新计算的原始值 $\\mathbf{Q}_{t+1}$ 的指数移动平均 (EMA) 来计算：\n    $$\n    \\bar{\\mathbf{Q}}_{t+1} = \\rho \\bar{\\mathbf{Q}}_t + (1-\\rho) \\mathbf{Q}_{t+1}\n    $$\n    这种平滑旨在抑制由奖励的随机性引起的原始 Q 值的波动，可能导致更稳定的策略。\n\n**策略翻转计算**\n\n完成模拟的 $T$ 步后，可获得已采取动作的历史记录 $(a_0, a_1, \\dots, a_{T-1})$。每当时间 $t$ 的动作与时间 $t-1$ 的动作不同时，就计为一次策略翻转。根据问题的字面定义，此计数是针对 $t \\ge 2$ 进行的。因此，翻转总数是 $a_t \\ne a_{t-1}$ for $t \\in \\{2, 3, \\dots, T-1\\}$ 的次数。\n\n**最终比较**\n\n对于每个测试用例，将平滑模拟的翻转次数（`smoothed_flips`）与原始模拟的翻转次数（`raw_flips`）进行比较。该用例的最终输出是严格不等式 `smoothed_flips  raw_flips` 的布尔结果。", "answer": "```python\nimport numpy as np\n\ndef run_simulation(params, use_smoothing):\n    \"\"\"\n    Runs a single Q-learning simulation for a given set of parameters.\n\n    Args:\n        params (tuple): A tuple containing the simulation parameters:\n                        (mu0, mu1, sigma, alpha, gamma, T, rho, seed).\n        use_smoothing (bool): If True, uses smoothed Q-values for action selection.\n\n    Returns:\n        int: The total number of policy flips.\n    \"\"\"\n    mu0, mu1, sigma, alpha, gamma, T, rho, seed = params\n    mu = np.array([mu0, mu1])\n\n    rng = np.random.default_rng(seed)\n\n    Q = np.zeros(2)\n    Q_bar = np.zeros(2) if use_smoothing else None\n    \n    actions_history = []\n\n    for t in range(T):\n        # 1. Select action\n        if use_smoothing:\n            # 贪心动作选择（相对于平滑Q值）\n            # 平局打破：选择较小的索引（动作0）\n            action = 0 if Q_bar[0] >= Q_bar[1] else 1\n        else:\n            # 贪心动作选择（相对于原始Q值）\n            action = 0 if Q[0] >= Q[1] else 1\n        actions_history.append(action)\n\n        # 2. Get reward\n        reward = rng.normal(loc=mu[action], scale=sigma)\n\n        # 3. Update raw Q-value\n        # TD目标使用该步骤开始时的Q值\n        td_target = reward + gamma * np.max(Q)\n        Q_next = Q.copy()\n        Q_next[action] = (1 - alpha) * Q[action] + alpha * td_target\n        Q = Q_next\n        \n        # 4. Update smoothed Q-value (if applicable)\n        # 此更新使用新计算的Q值\n        if use_smoothing:\n            Q_bar = rho * Q_bar + (1 - rho) * Q\n            \n    # 计算 t >= 2 时的翻转次数\n    flips = 0\n    if T >= 3:\n        for t in range(2, T):\n            if actions_history[t] != actions_history[t-1]:\n                flips += 1\n            \n    return flips\n\ndef solve():\n    \"\"\"\n    通过为每个测试用例运行模拟并比较原始Q学习和平滑Q学习之间的策略翻转次数来解决问题。\n    \"\"\"\n    test_cases = [\n        # (mu0,   mu1, sigma, alpha, gamma,   T,  rho, seed)\n        (0.0,   0.0,   1.0,   0.9,   0.9, 400, 0.95,  123),\n        (0.05,  0.0,   1.0,   0.9,   0.9, 400, 0.95,  456),\n        (0.0,   0.0,   0.0,   0.9,   0.9, 200, 0.95,  789),\n        (0.0,   0.0,   1.0,   0.9,   0.9, 400,  1.0,  321),\n    ]\n\n    results = []\n    for case in test_cases:\n        # 运行原始Q学习模拟\n        raw_flips = run_simulation(case, use_smoothing=False)\n        \n        # 运行平滑Q学习模拟\n        smoothed_flips = run_simulation(case, use_smoothing=True)\n        \n        # 比较翻转次数并附加布尔结果\n        results.append(smoothed_flips  raw_flips)\n\n    # 格式化并打印最终输出\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3163593"}, {"introduction": "为了提高学习效率，现代强化学习算法常常使用经验回放（experience replay）机制。然而，均匀地从经验池中采样可能效率低下，特别是当关键的、信息量大的经验非常稀有时。本练习将引导你分析一种重要的改进技术——优先经验回放（prioritized experience replay），它通过更频繁地采样“重要”经验来加速学习。你将通过解析推导和代码实现，量化这种非均匀采样引入的估计偏差，并理解如何运用重要性采样（importance sampling）来修正偏差，从而深入探索偏差与方差之间的权衡。[@problem_id:3163626]", "problem": "给定一个小型、有限的马尔可夫决策过程（MDP），要求您分析优先经验回放对 Q 学习中一步时间差分统计量估计的影响。分析基础为以下一系列定义。\n\n从具有有限状态空间和动作空间的马尔可夫决策过程、Q 学习更新规则以及时间差分误差的标准定义开始。在 Q 学习中，对于一个采样得到的转移 $(s,a,r,s')$，时间差分误差定义为 $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$，其中 $\\gamma \\in [0,1)$ 且 $Q$ 表示当前的动作价值函数。优先回放根据分配给每个转移的优先级，按照一个非均匀概率从回放缓冲区 $\\mathcal{D}$ 中采样转移。优先级通常是绝对时间差分误差的函数。一种常见的方案将转移 $i$ 的采样概率设置为 $p(i) = \\dfrac{(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}}{\\sum_{j \\in \\mathcal{D}} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha}}$，其中 $\\epsilon > 0$ 确保所有转移的概率非零，而 $\\alpha \\ge 0$ 控制优先级的程度。在估计缓冲区上的均匀分布的期望时，使用重要性采样权重来校正由非均匀采样引入的偏差。\n\n在本问题中，您将在一个简化但具有科学意义的设定下工作，以量化优先采样下估计量的偏差和方差，并使用重要性采样来校正偏差。考虑一个大小为 $N$ 的回放缓冲区 $\\mathcal{D}$，其中包含从一个固定的数据收集策略中抽取的转移。在一个 MDP 中，这些转移有两种性质上截然不同的类型：“稀有”转移（奖励为 $R > 0$）和“常见”转移（奖励为 $0$）。假设对于所有 $(s,a)$，Q 值固定为 $Q(s,a)=0$，且折扣因子 $\\gamma$ 是 $[0,1)$ 中的任意值；在 $Q(s,a)=0$ 的情况下，每个转移的时间差分误差等于其即时奖励，即 $\\delta_i = r_i$。缓冲区包含 $N_{\\text{rare}}$ 个稀有转移和 $N_{\\text{common}}=N-N_{\\text{rare}}$ 个常见转移。对于带参数 $\\alpha \\ge 0$ 和 $\\epsilon > 0$ 的优先回放，每个转移 $i$ 以概率\n$$\np(i) = \\frac{(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}}{\\sum_{j=1}^{N} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha}}.\n$$\n被采样。\n\n您需要估计时间差分误差在缓冲区上的均匀均值，\n$$\n\\mu \\equiv \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i,\n$$\n使用基于从优先分布 $p(i)$ 中有放回地抽取的 $m$ 个独立样本的三个估计量：\n- 无加权优先采样估计量（无校正），定义为所抽取 $\\delta$ 值的样本均值（等效于重要性指数 $\\beta = 0$）。\n- 使用指数为 $\\beta \\in (0,1)$ 的重要性采样权重的部分校正估计量，定义为 $w(i)^{\\beta} \\delta_i$ 的样本均值，其中 $w(i) = \\frac{1/N}{p(i)}$。\n- 使用 $\\beta = 1$ 的完全校正无偏估计量，定义为 $w(i) \\delta_i$ 的样本均值。\n\n对于这三个估计量中的每一个，请解析地（非蒙特卡洛模拟）计算在从 $p(i)$ 中进行 $m$ 次独立同分布抽样时，样本均值估计量的偏差和方差。您可以使用以下基本事实：\n- $m$ 个独立同分布随机变量的样本均值的期望值等于总体期望。\n- $m$ 个独立同分布随机变量的样本均值的方差等于总体方差除以 $m$。\n- 对于任何在有限多个值上的离散分布，其期望和方差可通过对支持点的有限求和来计算。\n\n您必须实现一个完整的、可运行的程序，该程序对下面的测试套件中的每个参数集计算：\n- 无校正的优先采样下（等效于 $\\beta=0$）样本均值估计量的解析偏差和方差。\n- 使用指定的部分校正指数 $\\beta \\in (0,1)$ 时的解析偏差和方差。\n- 完全校正（$\\beta=1$）时的解析偏差和方差。\n\n使用以下测试套件参数集。每个集是一个元组 $(N, N_{\\text{rare}}, R, \\alpha, \\epsilon, m, \\beta_{\\text{half}})$，所有量均采用标准数学单位：\n- 案例 A (正常路径): $(N=\\;1000, N_{\\text{rare}}=\\;10, R=\\;1.0, \\alpha=\\;0.6, \\epsilon=\\;10^{-3}, m=\\;256, \\beta_{\\text{half}}=\\;0.5)$。\n- 案例 B (强优先级和极端稀有): $(N=\\;1000, N_{\\text{rare}}=\\;1, R=\\;2.0, \\alpha=\\;1.0, \\epsilon=\\;10^{-6}, m=\\;64, \\beta_{\\text{half}}=\\;0.5)$。\n- 案例 C (无优先级基线): $(N=\\;1000, N_{\\text{rare}}=\\;10, R=\\;1.0, \\alpha=\\;0.0, \\epsilon=\\;10^{-3}, m=\\;256, \\beta_{\\text{half}}=\\;0.5)$。\n- 案例 D (无稀有事件边界): $(N=\\;100, N_{\\text{rare}}=\\;0, R=\\;1.0, \\alpha=\\;0.7, \\epsilon=\\;10^{-4}, m=\\;32, \\beta_{\\text{half}}=\\;0.5)$。\n- 案例 E (零奖励边界): $(N=\\;200, N_{\\text{rare}}=\\;20, R=\\;0.0, \\alpha=\\;0.9, \\epsilon=\\;10^{-3}, m=\\;128, \\beta_{\\text{half}}=\\;0.5)$。\n\n对于每个案例，您的程序应输出一个包含六个浮点数的列表：\n$[\\text{bias}_{\\beta=0}, \\text{var}_{\\beta=0}, \\text{bias}_{\\beta=\\beta_{\\text{half}}}, \\text{var}_{\\beta=\\beta_{\\text{half}}}, \\text{bias}_{\\beta=1}, \\text{var}_{\\beta=1}]$,\n其中 $\\text{var}$ 表示样本均值估计量的方差，$\\text{bias}$ 表示估计量的期望值与 $\\mu$ 之间的差。所有结果必须四舍五入到六位小数。\n\n最终输出格式要求：您的程序应生成单行输出，其中包含一个逗号分隔的各案例结果列表，每个案例结果是如上所述的列表，所有结果都包含在一对单独的方括号内，例如：\n\"[[x_{1},x_{2},x_{3},x_{4},x_{5},x_{6}],[y_{1},y_{2},y_{3},y_{4},y_{5},y_{6}],...]\".", "solution": "此问题的目标是解析地计算在优先经验回放条件下，三种不同时间差分（TD）误差估计量的偏差和方差。该分析在一个简化但具有代表性的马尔可夫决策过程（MDP）设定中进行。这三种估计量分别对应于不使用重要性采样（IS）校正（$\\beta=0$）、部分校正（$\\beta \\in (0,1)$）和完全校正（$\\beta=1$）。\n\n分析过程如下：\n1.  定义简化的 MDP 和回放缓冲区的组成部分。\n2.  确定 TD 误差的真实均值，这是待估计的目标量。\n3.  建立优先回放方案下的采样概率公式。\n4.  为从回放缓冲区中抽取的单个校正后样本定义一个随机变量。\n5.  对于任意校正指数 $\\beta$，推导该随机变量的期望和方差的一般表达式。\n6.  根据这些表达式，为三个指定的 $\\beta$ 值计算 $m$ 次抽样的样本均值估计量的偏差和方差。\n\n**1. 模型和回放缓冲区构成**\n回放缓冲区 $\\mathcal{D}$ 包含 $N$ 个转移。这些转移分为两类：\n- $N_{\\text{rare}}$ 个“稀有”转移，每个奖励为 $R$，因此 TD 误差为 $\\delta_i = R$。\n- $N_{\\text{common}} = N - N_{\\text{rare}}$ 个“常见”转移，每个奖励为 $0$，因此 TD 误差为 $\\delta_j = 0$。\n\n这一简化源于问题陈述，其中动作价值函数对所有状态-动作对 $(s,a)$ 固定为 $Q(s,a)=0$，使得 TD 误差 $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ 等于即时奖励 $r$。\n\n**2. 目标量：真实均值**\n我们希望估计的量是在缓冲区 $\\mathcal{D}$ 上假设均匀分布的 TD 误差的均值。这个真实均值，记为 $\\mu$，是：\n$$ \\mu = \\frac{1}{N} \\sum_{i=1}^{N} \\delta_i = \\frac{N_{\\text{rare}} \\cdot R + N_{\\text{common}} \\cdot 0}{N} = \\frac{N_{\\text{rare}} R}{N} $$\n\n**3. 优先采样分布**\n采样转移 $i$ 的概率 $p(i)$ 与 $(\\lvert \\delta_i \\rvert + \\epsilon)^{\\alpha}$ 成正比。\n对于稀有转移，$\\lvert \\delta_i \\rvert = R$（假设 $R \\ge 0$），因此优先级项为 $(R + \\epsilon)^{\\alpha}$。\n对于常见转移，$\\lvert \\delta_j \\rvert = 0$，因此优先级项为 $(\\epsilon)^{\\alpha}$。\n\n归一化常数 $Z$ 是缓冲区中所有转移的这些优先级项的总和：\n$$ Z = \\sum_{j=1}^{N} (\\lvert \\delta_j \\rvert + \\epsilon)^{\\alpha} = N_{\\text{rare}}(R + \\epsilon)^{\\alpha} + N_{\\text{common}}(\\epsilon)^{\\alpha} $$\n采样单个特定稀有转移的概率是：\n$$ p_{\\text{rare}} = \\frac{(R + \\epsilon)^{\\alpha}}{Z} $$\n采样单个特定常见转移的概率是：\n$$ p_{\\text{common}} = \\frac{(\\epsilon)^{\\alpha}}{Z} $$\n\n**4. 校正后的样本估计量**\n通过对从优先分布 $p(i)$ 中抽取的 $m$ 个独立同分布（i.i.d.）样本取样本均值，可以构成 $\\mu$ 的一个估计量。为了校正采样偏差，每个采样的 TD 误差 $\\delta_i$ 乘以一个重要性采样权重 $w(i) = \\frac{1/N}{p(i)}$ 的 $\\beta$ 次方，其中 $\\beta \\in [0,1]$。\n\n设 $X_{\\beta}$ 是表示单个校正后样本的随机变量，$X_{\\beta} = w(i)^{\\beta} \\delta_i$。估计量是 $\\hat{\\mu}_{\\beta} = \\frac{1}{m} \\sum_{k=1}^{m} X_{\\beta}^{(k)}$，其中每个 $X_{\\beta}^{(k)}$ 都是一次独立抽样。\n\n$X_{\\beta}$ 的值取决于抽取的是稀有转移还是常见转移：\n- 如果抽取到稀有转移，则 $\\delta_i=R$，其值为 $x_{\\text{rare}} = \\left(\\frac{1/N}{p_{\\text{rare}}}\\right)^{\\beta} R$。这种情况发生的总概率为 $P(\\text{draw rare}) = N_{\\text{rare}} p_{\\text{rare}}$。\n- 如果抽取到常见转移，则 $\\delta_j=0$，其值为 $x_{\\text{common}} = 0$。这种情况发生的总概率为 $P(\\text{draw common}) = N_{\\text{common}} p_{\\text{common}}$。\n\n**5. 偏差和方差的一般推导**\n估计量 $\\hat{\\mu}_{\\beta}$ 的偏差和方差可以从单次抽样的期望 $E[X_\\beta]$ 和方差 $V[X_\\beta]$ 中得出。\n- 偏差：$B[\\hat{\\mu}_{\\beta}] = E[\\hat{\\mu}_{\\beta}] - \\mu = E[X_{\\beta}] - \\mu$。\n- 估计量方差：$V[\\hat{\\mu}_{\\beta}] = \\frac{V[X_{\\beta}]}{m} = \\frac{E[X_{\\beta}^2] - (E[X_{\\beta}])^2}{m}$。\n\n$X_{\\beta}$ 的期望是：\n$$ E[X_{\\beta}] = x_{\\text{rare}} \\cdot P(\\text{draw rare}) + x_{\\text{common}} \\cdot P(\\text{draw common}) $$\n$$ E[X_{\\beta}] = \\left(\\frac{1}{N p_{\\text{rare}}}\\right)^{\\beta} R \\cdot (N_{\\text{rare}} p_{\\text{rare}}) + 0 = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-\\beta} N^{-\\beta} $$\n$X_{\\beta}^2$ 的期望是：\n$$ E[X_{\\beta}^2] = (x_{\\text{rare}})^2 \\cdot P(\\text{draw rare}) + (x_{\\text{common}})^2 \\cdot P(\\text{draw common}) $$\n$$ E[X_{\\beta}^2] = \\left(\\left(\\frac{1}{N p_{\\text{rare}}}\\right)^{\\beta} R\\right)^2 \\cdot (N_{\\text{rare}} p_{\\text{rare}}) + 0 = R^2 \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-2\\beta} N^{-2\\beta} $$\n\n**6. 对特定 $\\beta$ 值的分析**\n现在可以将这些通用公式应用于我们感兴趣的三种情况。\n\n**情况 1：无校正 ($\\beta=0$)**\n估计量是未校正的 TD 误差的简单样本均值。\n- 期望：$E[X_0] = R \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}}$。\n- 偏差：$B[\\hat{\\mu}_0] = R \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}} - \\mu$。\n- 二阶矩：$E[X_0^2] = R^2 \\cdot N_{\\text{rare}} \\cdot p_{\\text{rare}}$。\n- 估计量方差：$V[\\hat{\\mu}_0] = \\frac{1}{m}(R^2 N_{\\text{rare}} p_{\\text{rare}} - (R N_{\\text{rare}} p_{\\text{rare}})^2) = \\frac{1}{m} R^2 N_{\\text{rare}} p_{\\text{rare}}(1 - N_{\\text{rare}} p_{\\text{rare}})$。\n这个估计量通常是有偏的，与均匀采样基线相比方差较低，因为优先采样侧重于特定类型的转移。\n\n**情况 2：完全校正 ($\\beta=1$)**\n该估计量使用标准的重要性采样校正。\n- 期望：$E[X_1] = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-1} N^{-1} = \\frac{R N_{\\text{rare}}}{N} = \\mu$。\n- 偏差：$B[\\hat{\\mu}_1] = \\mu - \\mu = 0$。正如 IS 理论所预期的，该估计量是无偏的。\n- 二阶矩：$E[X_1^2] = R^2 \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-2} N^{-2} = \\frac{R^2 N_{\\text{rare}}}{N^2 p_{\\text{rare}}}$。\n- 估计量方差：$V[\\hat{\\mu}_1] = \\frac{1}{m}(E[X_1^2] - \\mu^2) = \\frac{1}{m}\\left(\\frac{R^2 N_{\\text{rare}}}{N^2 p_{\\text{rare}}} - \\left(\\frac{R N_{\\text{rare}}}{N}\\right)^2\\right) = \\frac{R^2}{m N^2}\\left(\\frac{N_{\\text{rare}}}{p_{\\text{rare}}} - N_{\\text{rare}}^2\\right)$。\n该估计量的方差可能高于或低于均匀采样的方差，这取决于均匀分布和优先分布之间的不匹配程度。\n\n**情况 3：部分校正 ($\\beta = \\beta_{\\text{half}} \\in (0,1)$)**\n这代表了一种权衡，旨在减少未校正估计量的偏差，同时控制完全校正估计量可能过高的方差。偏差和方差通过将 $\\beta = \\beta_{\\text{half}}$ 代入第 5 节推导的通用公式直接计算。\n- 偏差：$B[\\hat{\\mu}_{\\beta_{\\text{half}}}] = R \\cdot N_{\\text{rare}} \\cdot (p_{\\text{rare}})^{1-\\beta_{\\text{half}}} N^{-\\beta_{\\text{half}}} - \\mu$。\n- 估计量方差：$V[\\hat{\\mu}_{\\beta_{\\text{half}}}] = \\frac{1}{m}\\left( R^2 N_{\\text{rare}} (p_{\\text{rare}})^{1-2\\beta_{\\text{half}}} N^{-2\\beta_{\\text{half}}} - (E[X_{\\beta_{\\text{half}}}])^2 \\right)$。\n\n这些推导出的表达式允许对每个测试案例所需量进行直接的解析计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the analytic bias and variance of TD error estimators\n    under prioritized experience replay for a suite of test cases.\n    \"\"\"\n    # Parameter sets as (N, N_rare, R, alpha, epsilon, m, beta_half)\n    test_cases = [\n        (1000, 10, 1.0, 0.6, 1e-3, 256, 0.5), # Case A\n        (1000, 1, 2.0, 1.0, 1e-6, 64, 0.5),  # Case B\n        (1000, 10, 1.0, 0.0, 1e-3, 256, 0.5), # Case C\n        (100, 0, 1.0, 0.7, 1e-4, 32, 0.5),   # Case D\n        (200, 20, 0.0, 0.9, 1e-3, 128, 0.5), # Case E\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, N_rare, R, alpha, epsilon, m, beta_half = case\n        \n        # Use floating point numbers for calculations to ensure precision and\n        # compatibility with functions like np.power.\n        N, N_rare, R, m = float(N), float(N_rare), float(R), float(m)\n\n        case_results = []\n\n        # The true mean of the TD errors over the uniform buffer distribution.\n        # This is the target value for our estimators.\n        mu = (N_rare * R) / N if N > 0 else 0.0\n        \n        # The derived analytical formulas work for boundary cases (N_rare=0 or R=0)\n        # where mu becomes 0. In these cases, all TD errors are 0, so any sample is 0,\n        # leading to an estimator that is always 0, with zero bias and variance.\n        # The formulas correctly reflect this.\n        \n        if N_rare == 0 or R == 0.0 or N == 0:\n            p_rare = 1.0/N if N > 0 else 1.0 # Uniform sampling, not used in calculations but for completeness\n        else:\n            N_common = N - N_rare\n            # Calculate priorities for rare and common transitions.\n            # priority is proportional to (|delta| + epsilon)^alpha\n            priority_term_rare = np.power(R + epsilon, alpha)\n            priority_term_common = np.power(epsilon, alpha)\n            \n            # Normalization constant Z is the sum of all priorities.\n            Z = N_rare * priority_term_rare + N_common * priority_term_common\n            \n            # Probability of sampling a single specific rare transition.\n            # Z cannot be zero since N > 0 and epsilon > 0.\n            p_rare = priority_term_rare / Z\n        \n        betas_to_test = [0.0, beta_half, 1.0]\n        \n        for beta in betas_to_test:\n            if N == 0:\n                bias = 0.0\n                var_estimator = 0.0\n            else:\n                # Let X_beta be the random variable for a single corrected sample: w(i)^beta * delta_i\n                # We calculate its population expectation E[X_beta] and variance Var(X_beta).\n                \n                # E[X_beta] = R * N_rare * (p_rare)^(1-beta) * N^(-beta)\n                E_X_beta = R * N_rare * np.power(p_rare, 1 - beta) * np.power(N, -beta)\n                \n                # Bias of the estimator is E[estimator] - true_mean\n                bias = E_X_beta - mu\n                \n                # E[X_beta^2] = R^2 * N_rare * (p_rare)^(1-2*beta) * N^(-2*beta)\n                E_X_beta_sq = np.power(R, 2) * N_rare * np.power(p_rare, 1 - 2 * beta) * np.power(N, -2 * beta)\n                \n                # Population variance Var(X_beta) = E[X_beta^2] - (E[X_beta])^2\n                V_X_beta = E_X_beta_sq - np.power(E_X_beta, 2)\n                \n                # The estimator's variance is Var(X_beta) / m for a sample size of m.\n                var_estimator = V_X_beta / m\n            \n            # Due to floating point inaccuracies, a variance that should be zero\n            # might calculate as a tiny negative number. Clamp at 0.\n            var_estimator = max(0.0, var_estimator)\n            \n            case_results.extend([bias, var_estimator])\n            \n        all_results.append([round(x, 6) for x in case_results])\n\n    # Format the final output string as a list of lists, per requirements.\n    # e.g., \"[[r1_1, r1_2, ...], [r2_1, r2_2, ...]]\"\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3163626"}]}