{"hands_on_practices": [{"introduction": "掌握马尔可夫决策过程的第一步是学习如何求解它。这个练习将指导你实现价值迭代和策略评估这两种求解 MDP 的基石算法。通过这个实践 [@problem_id:3245192]，你不仅能学会如何计算最优价值函数，还能将这些动态规划方法与求解线性方程组的经典数值方法（如 Jacobi 和 Gauss-Seidel 迭代）联系起来，从而深化对背后数学原理的理解。", "problem": "给定一个有折扣的、有限状态、有限动作的马尔可夫决策过程 (MDP)，其目标是通过迭代方法计算贝尔曼最优方程的唯一不动点。设状态集为 $\\mathcal{S} = \\{0, 1, \\dots, n-1\\}$，动作集为 $\\mathcal{A} = \\{0, 1, \\dots, m-1\\}$。对于每个状态-动作对 $(s,a)$，存在一个期望即时奖励 $r(s,a)$ 和一个关于下一状态的转移概率核 $P(s' \\mid s,a)$。折扣因子为 $\\gamma \\in [0,1)$。用 $v \\in \\mathbb{R}^n$ 表示状态的价值向量。\n\n基本原理：\n- 贝尔曼最优算子 $\\mathcal{T}$ 对 $v \\in \\mathbb{R}^n$ 的定义是分量式的：\n$$\n(\\mathcal{T} v)(s) \\triangleq \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v(s') \\right).\n$$\n- 最优价值 $v^\\star$ 是 $\\mathcal{T}$ 的唯一不动点，满足 $v^\\star = \\mathcal{T} v^\\star$。\n- 对于任何固定策略 $\\pi : \\mathcal{S} \\to \\mathcal{A}$，策略评估方程是线性的：\n$$\nv_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi, \\quad \\text{等价于} \\quad (I - \\gamma P_\\pi) v_\\pi = r_\\pi,\n$$\n其中 $r_\\pi(s) \\triangleq r(s,\\pi(s))$ 且 $(P_\\pi)_{s,s'} \\triangleq P(s' \\mid s,\\pi(s))$。\n\n任务：\n- 为最优方程实现两种迭代方案：\n  1. 同步价值迭代（雅可比式），该方法使用前一次迭代的结果同时更新 $v$ 的所有分量，\n  2. 高斯-赛德尔式价值迭代，该方法在每次扫描中按固定顺序原地更新分量，并立即重用新更新的分量。\n- 为评估给定的固定策略 $\\pi$ 实现两种迭代方案：\n  1. 针对线性系统 $(I - \\gamma P_\\pi) v = r_\\pi$ 的同步策略评估（雅可比方法），\n  2. 针对同一线性系统的高斯-赛德尔策略评估。\n\n停止规则：\n- 对于每种方法，从 $v^{(0)} = 0$（零向量）开始。在每次完整迭代（一次完整的同步更新或一次对所有状态的完整原地扫描）后，计算更新幅度\n$$\n\\Delta^{(k)} \\triangleq \\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty,\n$$\n并在第一个满足 $\\Delta^{(k)} \\le \\varepsilon$ 的 $k$ 处停止，其中 $\\varepsilon$ 是给定的容差。对所有方法和测试使用相同的 $\\varepsilon$。计算并报告收敛所需的迭代次数。\n\n与线性系统迭代方法的关系：\n- 对于固定策略的情况，设 $A \\triangleq I - \\gamma P_\\pi$ 和 $b \\triangleq r_\\pi$，同步策略评估与应用于 $A v = b$ 的雅可比方法一致，而原地策略评估与应用于 $A v = b$ 的高斯-赛德尔方法一致。\n\n测试套件：\n实现您的程序以运行以下三个测试。每个测试指定 $(P, R, \\gamma, \\pi)$，其中 $P$ 是一个分量为 $P[s,a,s']$ 的张量，$R$ 是一个分量为 $R[s,a] = r(s,a)$ 的矩阵，$\\gamma$ 是折扣因子，$\\pi$ 是用于策略评估子测试的固定策略。\n\n- 测试 1（正常路径，较小的 $\\gamma$）：\n  - 状态：$\\{0,1,2\\}$，$n = 3$；动作：$\\{0,1\\}$，$m = 2$。\n  - 转移 $P$：\n    - 动作 0：$P(0 \\mid 0,0) = 0.5$, $P(1 \\mid 0,0) = 0.5$；$P(1 \\mid 1,0) = 0.5$, $P(2 \\mid 1,0) = 0.5$；$P(2 \\mid 2,0) = 1.0$。\n    - 动作 1：$P(0 \\mid 0,1) = 0.7$, $P(1 \\mid 0,1) = 0.3$；$P(0 \\mid 1,1) = 0.4$, $P(2 \\mid 1,1) = 0.6$；$P(1 \\mid 2,1) = 1.0$。\n    - 所有未指定的 $P(s' \\mid s,a)$ 均为 0。\n  - 奖励 $R$：$R(0,0) = 5$, $R(1,0) = 0$, $R(2,0) = 0$；$R(0,1) = 4$, $R(1,1) = 1$, $R(2,1) = 2$。\n  - 折扣：$\\gamma = 0.9$。\n  - 用于评估的固定策略：$\\pi(0) = 0$, $\\pi(1) = 1$, $\\pi(2) = 0$。\n- 测试 2（边界情况，$\\gamma$ 接近 1）：\n  - 与测试 1 相同的 $P$ 和 $R$。\n  - 折扣：$\\gamma = 0.99$。\n  - 用于评估的固定策略：与测试 1 相同的 $\\pi$。\n- 测试 3（边缘情况，吸收结构）：\n  - 状态：$\\{0,1\\}$，$n = 2$；动作：$\\{0,1\\}$，$m = 2$。\n  - 转移 $P$：\n    - 动作 0：$P(1 \\mid 0,0) = 1.0$；$P(1 \\mid 1,0) = 1.0$。\n    - 动作 1：$P(0 \\mid 0,1) = 1.0$；$P(1 \\mid 1,1) = 1.0$。\n    - 所有未指定的 $P(s' \\mid s,a)$ 均为 0。\n  - 奖励 $R$：$R(0,0) = 1$, $R(1,0) = 0$；$R(0,1) = 0$, $R(1,1) = 0$。\n  - 折扣：$\\gamma = 0.95$。\n  - 用于评估的固定策略：$\\pi(0) = 1$, $\\pi(1) = 0$。\n  \n容差：\n- 对所有方法和所有测试使用 $\\varepsilon = 10^{-8}$。\n\n要求的输出：\n- 对于每个测试，计算并返回以下四个整数：\n  1. $N_{\\text{opt, sync}}$：同步价值迭代收敛的迭代次数，\n  2. $N_{\\text{opt, GS}}$：高斯-赛德尔式价值迭代收敛的迭代次数，\n  3. $N_{\\text{eval, Jacobi}}$：同步策略评估（雅可比）收敛的迭代次数，\n  4. $N_{\\text{eval, GS}}$：高斯-赛德尔策略评估收敛的迭代次数。\n- 将三个测试的结果按以下顺序汇总到一个扁平列表中\n$$\n[\\;N_{\\text{opt, sync}}^{(1)},\\; N_{\\text{opt, GS}}^{(1)},\\; N_{\\text{eval, Jacobi}}^{(1)},\\; N_{\\text{eval, GS}}^{(1)},\\; N_{\\text{opt, sync}}^{(2)},\\; N_{\\text{opt, GS}}^{(2)},\\; N_{\\text{eval, Jacobi}}^{(2)},\\; N_{\\text{eval, GS}}^{(2)},\\; N_{\\text{opt, sync}}^{(3)},\\; N_{\\text{opt, GS}}^{(3)},\\; N_{\\text{eval, Jacobi}}^{(3)},\\; N_{\\text{eval, GS}}^{(3)}\\;].\n$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，不含空格。例如，如果有两个测试，每个测试有两个整数，则格式为“[1,2,3,4]”。在此问题中，该行必须按上述顺序包含 $12$ 个整数。", "solution": "对问题陈述进行验证。\n\n### 步骤 1：提取已知信息\n\n- **集合**：状态空间 $\\mathcal{S} = \\{0, 1, \\dots, n-1\\}$，动作空间 $\\mathcal{A} = \\{0, 1, \\dots, m-1\\}$。\n- **MDP 组件**：\n  - 状态-动作对 $(s,a)$ 的期望即时奖励：$r(s,a)$。\n  - 转移概率核：$P(s' \\mid s,a)$。\n  - 折扣因子：$\\gamma \\in [0,1)$。\n- **价值向量**：$v \\in \\mathbb{R}^n$。\n- **贝尔曼最优算子**：$(\\mathcal{T} v)(s) \\triangleq \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v(s') \\right)$。\n- **最优价值函数**：$v^\\star$ 是满足 $v^\\star = \\mathcal{T} v^\\star$ 的唯一不动点。\n- **固定策略评估**：对于策略 $\\pi : \\mathcal{S} \\to \\mathcal{A}$，其价值函数 $v_\\pi$ 满足线性系统 $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$，其中 $r_\\pi(s) \\triangleq r(s,\\pi(s))$ 且 $(P_\\pi)_{s,s'} \\triangleq P(s' \\mid s,\\pi(s))$。\n- **需要实现的迭代方案**：\n  1. 同步价值迭代（雅可比式）。\n  2. 高斯-赛德尔式价值迭代。\n  3. 同步策略评估（针对线性系统的雅可比方法）。\n  4. 针对线性系统的高斯-赛德尔策略评估。\n- **停止规则**：\n  - 初始条件：$v^{(0)} = 0$。\n  - 迭代更新：$\\Delta^{(k)} \\triangleq \\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty$。\n  - 终止条件：在第一个满足 $\\Delta^{(k)} \\le \\varepsilon$ 的迭代 $k$ 处停止。\n  - 容差：$\\varepsilon = 10^{-8}$。\n- **测试套件**：提供了三个具体的测试用例，每个用例定义了转移张量 $P$、奖励矩阵 $R$、折扣因子 $\\gamma$ 和一个固定策略 $\\pi$。\n  - 测试 1：$n=3, m=2, \\gamma=0.9$。\n  - 测试 2：$n=3, m=2, \\gamma=0.99$。\n  - 测试 3：$n=2, m=2, \\gamma=0.95$。\n- **要求的输出**：三个测试中每种方法的迭代次数（$N_{\\text{opt, sync}}, N_{\\text{opt, GS}}, N_{\\text{eval, Jacobi}}, N_{\\text{eval, GS}}$），汇总成一个包含 $12$ 个整数的列表。\n\n### 步骤 2：使用提取的已知信息进行验证\n\n- **科学依据**：该问题基于马尔可夫决策过程和动态规划的标准理论。贝尔曼最优方程、策略评估方程以及迭代方法（价值迭代、雅可比、高斯-赛德尔）是该领域和数值线性代数中的基本概念。条件 $\\gamma \\in [0,1)$至关重要，因为它确保贝尔曼算子 $\\mathcal{T}$ 是关于无穷范数的收缩映射，这保证了唯一不动点 $v^\\star$ 的存在以及价值迭代从任何起点开始的收敛性。这是动态规划的一个基石性成果。该问题在科学上和数学上都是合理的。\n- **适定性**：该问题是适定的。对于每种算法和测试用例，任务是找出达到指定精度所需的迭代次数。由于 $\\gamma  1$，所有描述的迭代方法都保证收敛到唯一解。初始条件 $v^{(0)}=0$ 已给出，停止准则 $\\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty \\le \\varepsilon$ 是明确的。因此，对于每个要求的计算，都存在一个唯一的、稳定的、有意义的整数解（迭代次数）。\n- **客观性**：该问题使用精确的数学语言和定义进行陈述。所有数据和要求都是客观指定的，没有主观解释的余地。\n- **完整性和一致性**：问题陈述是自洽的。它为每个测试用例提供了所有必要的数据（$P$, $R$, $\\gamma$, $\\pi$）、所需的算法、初始条件和精确的停止规则。所提供的信息中没有矛盾之处。\n- **其他缺陷**：该问题没有表现出任何其他缺陷，例如不切实际、结构不良、微不足道或无法验证。测试用例探讨了算法的不同方面，例如对 $\\gamma$ 的敏感性以及在有吸收状态时的行为。\n\n### 步骤 3：结论与行动\n\n该问题是有效的。将提供一个合理的解决方案。\n\n目标是计算给定马尔可夫决策过程 (MDP) 的最优价值函数 $v^\\star$ 并评估给定的策略 $\\pi$。我们将实现四种经典的迭代算法来实现这一目标，并使用指定的收敛准则。对于每种算法，我们都从一个所有条目均为 0 的初始价值向量 $v^{(0)}$ 开始。\n\n前两种方法解决非线性的贝尔曼最优方程 $v^\\star = \\mathcal{T}v^\\star$。\n\n第一种算法是**同步价值迭代**。该方法类似于线性系统的雅可比方法。在每次迭代 $k$ 中，每个状态 $s$ 的价值都使用前一次迭代 $v^{(k-1)}$ 的价值同时更新。整个向量 $v$ 的更新规则是通过应用贝尔曼算子给出：$v^{(k)} = \\mathcal{T}v^{(k-1)}$。按分量写，即：\n$$\nv^{(k)}(s) = \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s,a) v^{(k-1)}(s') \\right) \\quad \\text{for all } s \\in \\mathcal{S}.\n$$\n计算 $v^{(k)}$ 的所有分量需要 $v^{(k-1)}$ 的一个完整副本。\n\n第二种算法是**高斯-赛德尔式价值迭代**。该方法执行原地更新，类似于高斯-赛德尔方法。状态按固定顺序进行迭代，例如 $s = 0, 1, \\dots, n-1$。在计算状态 $s$ 的新价值时，算法会使用所有其他状态的最新计算值。具体来说，对于状态 $s'  s$，使用当前迭代 $k$ 的新值，而对于状态 $s' \\ge s$，则使用迭代 $k-1$ 的旧值。迭代 $k$ 的扫描中的更新规则是：\n$$\nv(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\left( r(s,a) + \\gamma \\sum_{s'=0}^{s-1} P(s' \\mid s,a) v^{(k)}(s') + \\gamma \\sum_{s'=s}^{n-1} P(s' \\mid s,a) v^{(k-1)}(s') \\right).\n$$\n这很自然地通过原地更新价值向量来实现。高斯-赛德尔式更新通常比其同步对应方法收敛得更快。\n\n接下来的两种方法求解用于策略评估的线性方程组 $(I - \\gamma P_\\pi) v_\\pi = r_\\pi$。这可以重写为不动点方程 $v_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi$。\n\n第三种算法是**同步策略评估**，这正是应用于该系统的**雅可比方法**。从 $v^{(0)}=0$ 开始，迭代格式为：\n$$\nv^{(k)} = r_\\pi + \\gamma P_\\pi v^{(k-1)}.\n$$\n按分量写，即：\n$$\nv^{(k)}(s) = r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) v^{(k-1)}(s').\n$$\n与同步价值迭代一样，需要保留前一次迭代 $v^{(k-1)}$ 的一个完整副本。\n\n第四种算法是**高斯-赛德尔策略评估**。这将**高斯-赛德尔方法**应用于策略评估系统。更新是原地执行的。对于固定顺序 $s=0, 1, \\dots, n-1$ 中的每个状态 $s$，更新规则是：\n$$\nv(s) \\leftarrow r(s, \\pi(s)) + \\gamma \\sum_{s' \\in \\mathcal{S}} P(s' \\mid s, \\pi(s)) v(s').\n$$\n右侧的求和隐式地使用了当前扫描中已更新状态的新值和尚未更新状态的旧值。\n\n对于所有四种方法，迭代将持续进行，直到价值向量的变化足够小，即通过无穷范数衡量：$\\lVert v^{(k)} - v^{(k-1)} \\rVert_\\infty \\le \\varepsilon$，其中给定容差为 $\\varepsilon = 10^{-8}$。满足此条件所需的迭代次数 $k$ 将被记录下来，用于每种方法和每个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef synchronous_value_iteration(P, R, gamma, tol):\n    \"\"\"\n    Computes the optimal value function using synchronous value iteration (Jacobi-style).\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states, _ = R.shape\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        # Vectorized computation of Q-values for all state-action pairs\n        q_values = R + gamma * (P @ v_prev)  # Shape (n_states, n_actions)\n        v = np.max(q_values, axis=1)        # Shape (n_states,)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef gauss_seidel_value_iteration(P, R, gamma, tol):\n    \"\"\"\n    Computes the optimal value function using Gauss-Seidel-style value iteration.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states, _ = R.shape\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        for s in range(n_states):\n            # The mat-vec product uses the current state of v, which includes\n            # in-place updates from the current sweep for s'  s.\n            q_values_s = R[s, :] + gamma * (P[s, :, :] @ v)\n            v[s] = np.max(q_values_s)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef synchronous_policy_evaluation(P, R, pi, gamma, tol):\n    \"\"\"\n    Evaluates a fixed policy using the synchronous Jacobi method.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        pi (np.ndarray): Fixed policy vector.\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states = len(pi)\n    \n    # Construct r_pi and P_pi from the policy\n    s_indices = np.arange(n_states)\n    r_pi = R[s_indices, pi]\n    P_pi = P[s_indices, pi, :]\n    \n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        v = r_pi + gamma * (P_pi @ v_prev)\n        \n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef gauss_seidel_policy_evaluation(P, R, pi, gamma, tol):\n    \"\"\"\n    Evaluates a fixed policy using the Gauss-Seidel method.\n\n    Args:\n        P (np.ndarray): Transition probability tensor P[s, a, s'].\n        R (np.ndarray): Reward matrix R[s, a].\n        pi (np.ndarray): Fixed policy vector.\n        gamma (float): Discount factor.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        int: Number of iterations to converge.\n    \"\"\"\n    n_states = len(pi)\n    \n    # Construct r_pi and P_pi from the policy\n    s_indices = np.arange(n_states)\n    r_pi = R[s_indices, pi]\n    P_pi = P[s_indices, pi, :]\n\n    v = np.zeros(n_states)\n    k = 0\n    while True:\n        k += 1\n        v_prev = v.copy()\n        \n        for s in range(n_states):\n            # In-place update using the current state of v\n            v[s] = r_pi[s] + gamma * (P_pi[s, :] @ v)\n\n        delta = np.max(np.abs(v - v_prev))\n        if delta = tol:\n            return k\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test cases, then prints the results.\n    \"\"\"\n    tol = 1e-8\n\n    # --- Test Case 1 ---\n    # n=3, m=2, gamma=0.9\n    P1 = np.zeros((3, 2, 3))\n    P1[0, 0, 0] = 0.5; P1[0, 0, 1] = 0.5\n    P1[1, 0, 1] = 0.5; P1[1, 0, 2] = 0.5\n    P1[2, 0, 2] = 1.0\n    P1[0, 1, 0] = 0.7; P1[0, 1, 1] = 0.3\n    P1[1, 1, 0] = 0.4; P1[1, 1, 2] = 0.6\n    P1[2, 1, 1] = 1.0\n    R1 = np.array([[5., 4.], [0., 1.], [0., 2.]])\n    gamma1 = 0.9\n    pi1 = np.array([0, 1, 0])\n\n    # --- Test Case 2 ---\n    # n=3, m=2, gamma=0.99 (same P and R as Test 1)\n    P2 = P1\n    R2 = R1\n    gamma2 = 0.99\n    pi2 = pi1\n    \n    # --- Test Case 3 ---\n    # n=2, m=2, gamma=0.95\n    P3 = np.zeros((2, 2, 2))\n    P3[0, 0, 1] = 1.0\n    P3[1, 0, 1] = 1.0\n    P3[0, 1, 0] = 1.0\n    P3[1, 1, 1] = 1.0\n    R3 = np.array([[1., 0.], [0., 0.]])\n    gamma3 = 0.95\n    pi3 = np.array([1, 0])\n\n    test_cases = [\n        (P1, R1, gamma1, pi1),\n        (P2, R2, gamma2, pi2),\n        (P3, R3, gamma3, pi3),\n    ]\n\n    results = []\n    for P, R, gamma, pi in test_cases:\n        N_opt_sync = synchronous_value_iteration(P, R, gamma, tol)\n        N_opt_GS = gauss_seidel_value_iteration(P, R, gamma, tol)\n        N_eval_Jacobi = synchronous_policy_evaluation(P, R, pi, gamma, tol)\n        N_eval_GS = gauss_seidel_policy_evaluation(P, R, pi, gamma, tol)\n        results.extend([N_opt_sync, N_opt_GS, N_eval_Jacobi, N_eval_GS])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3245192"}, {"introduction": "理论模型到实际应用的桥梁，在于处理从数据中学习时遇到的不确定性。当价值函数是从有限且带有噪声的经验中估计得到时，一个被称为“最大化偏差”（maximization bias）的微妙问题就会出现，严重影响学习效果。这个练习 [@problem_id:3145285] 提供了一个绝佳的机会，让你通过第一性原理推导出这种偏差，并理解双 Q 学习（Double Q-learning）如何巧妙地解决这个问题，从而为构建更稳健的强化学习智能体打下基础。", "problem": "考虑一个马尔可夫决策过程 (MDP)，其中有一个非终止状态 $s$ 和两个可用动作 $a_{1}$ 和 $a_{2}$。真实的动作值为 $q^{\\ast}(s,a_{1})=\\mu_{1}$ 和 $q^{\\ast}(s,a_{2})=\\mu_{2}$，其中 $\\mu_{1}\\geq \\mu_{2}$。假设一个表格型智能体执行一步时序差分自举，其中目标中的唯一误差来源是在带噪声的、无偏的值估计上使用贪心算子。具体来说，智能体当前的估计值为 $\\hat{q}(s,a_{i})=\\mu_{i}+\\varepsilon_{i}$，其中 $\\varepsilon_{1}$ 和 $\\varepsilon_{2}$ 是独立同分布的，服从 $\\mathcal{N}(0,\\sigma^{2})$ 分布。在此步骤的目标中不涉及额外的奖励或折扣；重点完全在于由带噪声估计上的贪心最大化所引起的偏差。\n\n1. 使用 Q-学习目标 $T_{Q}(s)=\\max\\{\\hat{q}(s,a_{1}),\\hat{q}(s,a_{2})\\}$ 的定义，从第一性原理出发，推导期望偏差 $b_{Q}=\\mathbb{E}[T_{Q}(s)]-\\mu_{1}$ 关于 $\\Delta=\\mu_{1}-\\mu_{2}$ 和 $\\sigma$ 的解析表达式。\n\n2. 现在考虑双重 Q-学习 (Double Q-learning, DQL)，它维护两个独立的无偏估计器 $Q^{A}$ 和 $Q^{B}$，两者具有相同的噪声模型：$\\hat{q}^{A}(s,a_{i})=\\mu_{i}+\\varepsilon^{A}_{i}$ 和 $\\hat{q}^{B}(s,a_{i})=\\mu_{i}+\\varepsilon^{B}_{i}$，其中所有噪声项都是独立同分布的，服从 $\\mathcal{N}(0,\\sigma^{2})$ 分布。双重 Q-学习的目标是 $T_{DQ}(s)=\\hat{q}^{B}(s,\\arg\\max_{a}\\hat{q}^{A}(s,a))$。推导期望偏差 $b_{DQ}=\\mathbb{E}[T_{DQ}(s)]-\\mu_{1}$ 关于 $\\Delta$ 和 $\\sigma$ 的解析表达式。\n\n3. 通过计算 $\\Delta b=b_{Q}-b_{DQ}$ 来量化双重 Q-学习带来的偏差减少量，并将其表示为关于 $\\Delta$、$\\sigma$ 和标准正态累积分布函数 $\\Phi$ 的单个封闭形式解析表达式。您的最终答案必须是这一个表达式。将 $\\Phi(x)$ 定义为标准正态分布的累积分布函数。不要提供数值近似。", "solution": "这个问题提得很好且有科学依据，代表了对强化学习中最大化偏差的标准理论分析。我们将按要求分三部分进行推导。\n\n设定包含一个状态 $s$ 和两个动作 $a_1, a_2$，其真实值为 $q^{\\ast}(s,a_{1})=\\mu_{1}$ 和 $q^{\\ast}(s,a_{2})=\\mu_{2}$，其中 $\\mu_{1}\\geq \\mu_{2}$。智能体的值估计是带噪声且无偏的。对于一个通用动作 $a_i$，其估计值为 $\\hat{q}(s,a_{i})=\\mu_{i}+\\varepsilon_{i}$，其中 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ 是独立同分布 (i.i.d.) 的随机变量。我们定义 $\\Delta = \\mu_{1}-\\mu_{2} \\geq 0$。我们用 $\\phi(x)$ 和 $\\Phi(x)$ 分别表示标准正态分布 $\\mathcal{N}(0,1)$ 的概率密度函数 (PDF) 和累积分布函数 (CDF)。\n\n第一部分：Q-学习的期望偏差, $b_{Q}$\n\nQ-学习的目标由 $T_{Q}(s)=\\max\\{\\hat{q}(s,a_{1}),\\hat{q}(s,a_{2})\\}$ 给出。\n期望偏差定义为 $b_{Q}=\\mathbb{E}[T_{Q}(s)]-\\mu_{1}$，其中 $\\mu_1$ 是真实最优动作的值。\n$b_{Q} = \\mathbb{E}[\\max\\{\\mu_{1}+\\varepsilon_{1}, \\mu_{2}+\\varepsilon_{2}\\}] - \\mu_{1}$。\n我们使用恒等式 $\\max(x,y) = x + \\max(0, y-x)$。\n$\\mathbb{E}[\\max\\{\\mu_{1}+\\varepsilon_{1}, \\mu_{2}+\\varepsilon_{2}\\}] = \\mathbb{E}[\\mu_{1}+\\varepsilon_{1} + \\max\\{0, (\\mu_{2}+\\varepsilon_{2}) - (\\mu_{1}+\\varepsilon_{1})\\}]$。\n根据期望的线性性质，并且由于 $\\mathbb{E}[\\varepsilon_{1}]=0$：\n$\\mathbb{E}[T_{Q}(s)] = \\mu_{1} + \\mathbb{E}[\\max\\{0, (\\mu_{2}-\\mu_{1}) + (\\varepsilon_{2}-\\varepsilon_{1})\\}]$。\n因此，偏差为：\n$b_{Q} = \\mathbb{E}[\\max\\{0, -\\Delta + (\\varepsilon_{2}-\\varepsilon_{1})\\}]$。\n令新的随机变量为 $W = \\varepsilon_{2}-\\varepsilon_{1}$。因为 $\\varepsilon_1$ 和 $\\varepsilon_2$ 是独立同分布的 $\\mathcal{N}(0, \\sigma^2)$，所以 $W$ 也服从正态分布。\n$\\mathbb{E}[W] = \\mathbb{E}[\\varepsilon_{2}] - \\mathbb{E}[\\varepsilon_{1}] = 0 - 0 = 0$。\n$\\text{Var}(W) = \\text{Var}(\\varepsilon_{2}) + \\text{Var}(\\varepsilon_{1}) = \\sigma^2 + \\sigma^2 = 2\\sigma^2$，由于独立性。\n所以，$W \\sim \\mathcal{N}(0, 2\\sigma^2)$。\n令 $Z = W - \\Delta$。则 $Z \\sim \\mathcal{N}(-\\Delta, 2\\sigma^2)$。偏差为 $b_Q = \\mathbb{E}[\\max\\{0, Z\\}]$。\n这是一个右截尾正态变量的期望。对于随机变量 $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y^2)$，其公式为 $\\mathbb{E}[\\max\\{0,Y\\}] = \\mu_Y \\Phi(\\frac{\\mu_Y}{\\sigma_Y}) + \\sigma_Y \\phi(\\frac{\\mu_Y}{\\sigma_Y})$。\n在我们的例子中，$\\mu_Y = -\\Delta$ 且 $\\sigma_Y = \\sqrt{2\\sigma^2} = \\sigma\\sqrt{2}$。\n将这些代入公式中：\n$b_{Q} = (-\\Delta) \\Phi\\left(\\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right) + \\sigma\\sqrt{2} \\phi\\left(\\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right)$。\n使用恒等式 $\\Phi(-x) = 1-\\Phi(x)$ 和 $\\phi(-x) = \\phi(x)$：\n$b_{Q} = -\\Delta(1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})) + \\sigma\\sqrt{2}\\phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$。\n\n第二部分：双重 Q-学习的期望偏差, $b_{DQ}$\n\n双重 Q-学习使用两个独立的估计器 $Q^A$ 和 $Q^B$，其估计值为 $\\hat{q}^{A}(s,a_{i})=\\mu_{i}+\\varepsilon^{A}_{i}$ 和 $\\hat{q}^{B}(s,a_{i})=\\mu_{i}+\\varepsilon^{B}_{i}$，其中所有噪声项 $\\varepsilon^{A}_i, \\varepsilon^{B}_i$ 都是独立同分布的 $\\mathcal{N}(0,\\sigma^{2})$。\n目标是 $T_{DQ}(s)=\\hat{q}^{B}(s,a^{\\ast})$，其中 $a^{\\ast} = \\arg\\max_{a}\\hat{q}^{A}(s,a)$。\n期望偏差为 $b_{DQ} = \\mathbb{E}[T_{DQ}(s)] - \\mu_1$。\n我们通过对动作选择进行条件化来计算期望，该选择仅依赖于 A 的估计值。\n$\\mathbb{E}[T_{DQ}(s)] = \\mathbb{E}_{A,B}[\\hat{q}^{B}(s, a^{\\ast})] = \\mathbb{E}_{A}[\\mathbb{E}_{B}[\\hat{q}^{B}(s, a^{\\ast}) | a^{\\ast}]]$。\n由于 $a^{\\ast}$ 由噪声 $\\varepsilon^A$ 决定，它独立于噪声 $\\varepsilon^B$。对 B 的内层期望得出了所选动作的 B 估计的真实均值：\n$\\mathbb{E}_{B}[\\hat{q}^{B}(s, a^{\\ast}) | a^{\\ast}] = \\mu_{a^{\\ast}}$。\n所以，$\\mathbb{E}[T_{DQ}(s)] = \\mathbb{E}_{A}[\\mu_{a^{\\ast}}]$。\n期望是关于 $a^{\\ast}$ 的选择的：\n$\\mathbb{E}_{A}[\\mu_{a^{\\ast}}] = P(a^{\\ast}=a_1)\\mu_1 + P(a^{\\ast}=a_2)\\mu_2$。\n我们来计算概率 $P(a^{\\ast}=a_1)$。\n$P(a^{\\ast}=a_1) = P(\\hat{q}^{A}(s,a_1) \\geq \\hat{q}^{A}(s,a_2)) = P(\\mu_1+\\varepsilon^{A}_1 \\geq \\mu_2+\\varepsilon^{A}_2)$。\n$P(a^{\\ast}=a_1) = P(\\varepsilon^{A}_1 - \\varepsilon^{A}_2 \\geq \\mu_2 - \\mu_1) = P(\\varepsilon^{A}_1 - \\varepsilon^{A}_2 \\geq -\\Delta)$。\n随机变量 $W' = \\varepsilon^{A}_1 - \\varepsilon^{A}_2$ 服从 $\\mathcal{N}(0, 2\\sigma^2)$ 分布。\n$P(W' \\geq -\\Delta) = P\\left(\\frac{W'}{\\sigma\\sqrt{2}} \\geq \\frac{-\\Delta}{\\sigma\\sqrt{2}}\\right)$。令 $U = W'/(\\sigma\\sqrt{2}) \\sim \\mathcal{N}(0,1)$。\n$P(a^{\\ast}=a_1) = P(U \\geq \\frac{-\\Delta}{\\sigma\\sqrt{2}}) = 1 - \\Phi(\\frac{-\\Delta}{\\sigma\\sqrt{2}}) = \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$。\n并且 $P(a^{\\ast}=a_2) = 1 - P(a^{\\ast}=a_1) = 1 - \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}})$。\n现在，将这些概率代回到 $\\mathbb{E}[T_{DQ}(s)]$ 的表达式中：\n$\\mathbb{E}[T_{DQ}(s)] = \\mu_1 \\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}) + \\mu_2 (1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}))$。\n偏差是：\n$b_{DQ} = \\mathbb{E}[T_{DQ}(s)] - \\mu_1 = \\mu_1 \\Phi(\\dots) + \\mu_2 (1-\\Phi(\\dots)) - \\mu_1$。\n$b_{DQ} = (\\Phi(\\dots) - 1)\\mu_1 + (1-\\Phi(\\dots))\\mu_2 = (1-\\Phi(\\dots))(\\mu_2-\\mu_1) = -(1-\\Phi(\\frac{\\Delta}{\\sigma\\sqrt{2}}))\\Delta$。\n\n第三部分：偏差减少量, $\\Delta b$\n\n偏差减少量是差值 $\\Delta b = b_{Q} - b_{DQ}$。\n$b_{Q} = -\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right) + \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)$。\n$b_{DQ} = -\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right)$。\n从 $b_{Q}$ 中减去 $b_{DQ}$：\n$\\Delta b = \\left[-\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right) + \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right] - \\left[-\\Delta\\left(1-\\Phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)\\right)\\right]$。\n涉及 $\\Delta$ 和 $\\Phi$ 的项完全抵消了。\n$\\Delta b = \\sigma\\sqrt{2}\\phi\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)$。\n问题要求最终答案用 $\\Delta$、$\\sigma$ 和 $\\Phi$ 表示。然而，严格的推导表明 CDF 项 $\\Phi$ 被消去了，留下了一个依赖于 PDF $\\phi$ 的表达式。为了给出一个自包含的封闭形式表达式，我们代入标准正态 PDF 的定义，$\\phi(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$。\n$\\Delta b = \\sigma\\sqrt{2} \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{\\Delta}{\\sigma\\sqrt{2}}\\right)^2\\right)$。\n$\\Delta b = \\frac{\\sigma\\sqrt{2}}{\\sqrt{2}\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{2 \\cdot 2\\sigma^2}\\right)$。\n$\\Delta b = \\frac{\\sigma}{\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{4\\sigma^2}\\right)$。\n这就是偏差减少量的最终解析表达式。由于精确抵消，它是 $\\Delta$ 和 $\\sigma$ 的函数，但不是 $\\Phi$ 的函数。这是唯一正确的最终表达式。", "answer": "$$\\boxed{\\frac{\\sigma}{\\sqrt{\\pi}} \\exp\\left(-\\frac{\\Delta^2}{4\\sigma^2}\\right)}$$", "id": "3145285"}, {"introduction": "在许多现实场景中，我们希望在不实际部署新策略的情况下，仅利用历史数据来评估其性能，这就是离策略评估（Off-Policy Evaluation）的核心任务。这项技能对于安全、高效地改进决策系统至关重要。本练习 [@problem_id:3145244] 将引导你实现并比较三种关键的离策略评估方法：基于模型的直接估计、基于重要性采样的方法，以及结合两者优点的强大双重稳健（Doubly Robust）估计器。", "problem": "您将获得三个在马尔可夫决策过程（MDP）设定下的离策略评估任务。目标是实现一个双重鲁棒估计器，该估计器结合了基于模型的价值估计和重要性采样修正。这项工作必须从概率论的基本原理和马尔可夫性质推导得出。您的程序必须为每个测试用例计算三个量：每决策重要性采样估计值、目标策略下基于模型的价值估计值以及双重鲁棒估计值。您的程序必须输出单行内容，该行包含一个列表的列表，每个内部列表为对应测试用例的三个浮点数，并按指定顺序排列。\n\n使用的基本原理：\n- 马尔可夫决策过程（MDP）由一个有限状态空间 $S$、一个有限动作空间 $A$、一个转移核 $P(s' \\mid s,a)$、一个即时奖励函数 $r(s,a)$、一个折扣因子 $\\gamma \\in (0,1]$、一个有限期限 $H \\in \\mathbb{N}$ 和一个描述决策规则的策略 $\\pi(a \\mid s)$ 组成。\n- 马尔可夫性质指出，对于每个时间步 $t$，有 $P(s_{t+1} \\mid s_{0:t}, a_{0:t}) = P(s_{t+1} \\mid s_t, a_t)$，并且 $r_t = r(s_t,a_t)$。\n- 在策略 $\\pi$ 下，从一个初始状态分布 $d_0(s)$ 开始的期望折扣回报为 $V^\\pi = \\mathbb{E}_{s_0 \\sim d_0}\\left[\\sum_{t=0}^{H-1} \\gamma^t r(s_t,a_t)\\right]$，其中 $(s_t,a_t)$ 遵循由 $\\pi$ 控制的 MDP 动态过程。\n- 重要性采样（IS）基于恒等式 $\\mathbb{E}_p[f(X)] = \\mathbb{E}_q\\left[f(X)\\frac{p(X)}{q(X)}\\right]$，当 $p$ 相对于 $q$ 绝对连续时成立。该方法逐步应用于轨迹，以修正行为策略和目标策略之间的不匹配。\n- 对观测到的转移进行最大似然估计（MLE）可用于构建基于模型的估计 $\\hat{P}(s' \\mid s,a)$ 和 $\\hat{r}(s,a)$，这些估计可以导出用于计算基于模型的价值函数的动态规划递归式。\n\n您的实现要求：\n- 对于每个测试用例，使用提供的行为轨迹，通过最大似然估计来估计模型 $\\hat{P}(s' \\mid s,a)$。为确保概率有良好定义，对每个状态-动作对的下一个状态计数使用加一（Laplace）平滑。对于即时奖励估计器 $\\hat{r}(s,a)$，使用观测到的每个对 $(s,a)$ 的奖励的经验平均值；如果某个对 $(s,a)$ 从未被观测到，则设置 $\\hat{r}(s,a) = 0$。\n- 通过有限期限 $H$ 上的动态规划，计算目标策略 $\\pi$ 下基于模型的价值，记为 $\\hat{V}^\\pi$。使用由每个测试用例的初始状态数据集导出的经验初始状态分布 $d_0$ 来评估期望回报：$\\hat{V}^\\pi = \\sum_{s \\in S} d_0(s)\\hat{V}_0(s)$，其中 $\\hat{V}_t(s)$ 通过递归式 $\\hat{Q}_t(s,a) = \\hat{r}(s,a) + \\gamma \\sum_{s' \\in S} \\hat{P}(s' \\mid s,a)\\hat{V}_{t+1}(s')$ 和 $\\hat{V}_t(s) = \\sum_{a \\in A} \\pi(a \\mid s)\\hat{Q}_t(s,a)$ 计算，终端边界条件为对所有 $s \\in S$ 都有 $\\hat{V}_H(s)=0$。\n- 使用行为轨迹和逐步似然比 $\\rho_{0:t} = \\prod_{k=0}^{t} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}$ 计算每决策重要性采样（IS）估计器，其中 $b(a \\mid s)$ 是给定测试用例的行为策略。每决策 IS 估计值是逐步修正奖励的折扣总和在所有回合（episodes）上的平均值。\n- 实现一个双重鲁棒（DR）估计器，它使用上面计算的基于模型的 $\\hat{Q}_t$ 和 $\\hat{V}_t$ 作为控制变量，并将它们与每决策重要性权重相结合。其构造必须遵循以下原则：如果重要性权重是正确的，或者模型是正确的，则该估计器是无偏的。使用基于相同的 $\\rho_{0:t}$ 权重以及上面定义的动态规划 $\\hat{Q}_t$ 和 $\\hat{V}_t$ 的标准逐步组合；不要引入裁剪。\n- 所有计算必须仅基于提供的轨迹和策略；不允许使用外部数据或用户输入。\n\n测试套件：\n对于每个测试用例，状态空间为 $S = \\{0,1,2\\}$，动作空间为 $A = \\{0,1\\}$。回合是固定长度的，并以元组序列 $(s_t,a_t,r_t,s_{t+1})$ 的形式给出，其中 $t=0,\\dots,H-1$。策略 $b(a \\mid s)$ 和 $\\pi(a \\mid s)$ 按状态指定为动作上的概率分布。\n\n- 测试用例 1 (正常路径)：\n    - 折扣因子：$\\gamma = 0.95$。\n    - 期限：$H=3$。\n    - 行为策略 $b$：\n        - $b(0 \\mid 0) = 0.7$, $b(1 \\mid 0) = 0.3$。\n        - $b(0 \\mid 1) = 0.4$, $b(1 \\mid 1) = 0.6$。\n        - $b(0 \\mid 2) = 0.5$, $b(1 \\mid 2) = 0.5$。\n    - 目标策略 $\\pi$：\n        - $\\pi(0 \\mid 0) = 0.2$, $\\pi(1 \\mid 0) = 0.8$。\n        - $\\pi(0 \\mid 1) = 0.6$, $\\pi(1 \\mid 1) = 0.4$。\n        - $\\pi(0 \\mid 2) = 0.3$, $\\pi(1 \\mid 2) = 0.7$。\n    - 回合（每个长度为 $H=3$）：\n        1. $(0,0,1.0,1),(1,1,0.5,2),(2,1,1.2,2)$\n        2. $(0,1,1.5,2),(2,1,1.0,2),(2,0,0.7,1)$\n        3. $(1,0,0.8,0),(0,1,1.1,2),(2,1,1.0,2)$\n        4. $(2,0,0.9,1),(1,0,0.6,0),(0,1,1.4,2)$\n        5. $(1,1,0.4,2),(2,1,1.3,2),(2,0,0.5,1)$\n        6. $(0,0,1.1,1),(1,1,0.6,2),(2,1,1.0,2)$\n- 测试用例 2 (边界条件：行为策略等于目标策略)：\n    - 折扣因子：$\\gamma = 0.95$。\n    - 期限：$H=3$。\n    - 行为策略 $b$ 和目标策略 $\\pi$ 相同：\n        - $(0 \\mid 0) = 0.5$, $(1 \\mid 0) = 0.5$。\n        - $(0 \\mid 1) = 0.2$, $(1 \\mid 1) = 0.8$。\n        - $(0 \\mid 2) = 0.6$, $(1 \\mid 2) = 0.4$。\n    - 回合（每个长度为 $H=3$）：\n        1. $(0,0,1.0,1),(1,1,1.2,2),(2,0,0.4,1)$\n        2. $(1,1,0.7,2),(2,0,0.6,0),(0,1,1.1,2)$\n        3. $(2,0,0.9,1),(1,1,0.8,2),(2,1,1.0,2)$\n        4. $(0,1,1.4,2),(2,0,0.5,0),(0,1,1.2,2)$\n        5. $(2,0,0.6,0),(0,1,1.0,2),(2,0,0.7,1)$\n- 测试用例 3 (边缘案例：无裁剪的大重要性权重)：\n    - 折扣因子：$\\gamma = 0.95$。\n    - 期限：$H=4$。\n    - 行为策略 $b$：\n        - $b(0 \\mid 0) = 0.8$, $b(1 \\mid 0) = 0.2$。\n        - $b(0 \\mid 1) = 0.3$, $b(1 \\mid 1) = 0.7$。\n        - $b(0 \\mid 2) = 0.7$, $b(1 \\mid 2) = 0.3$。\n    - 目标策略 $\\pi$：\n        - $\\pi(0 \\mid 0) = 0.1$, $\\pi(1 \\mid 0) = 0.9$。\n        - $\\pi(0 \\mid 1) = 0.85$, $\\pi(1 \\mid 1) = 0.15$。\n        - $\\pi(0 \\mid 2) = 0.1$, $\\pi(1 \\mid 2) = 0.9$。\n    - 回合（每个长度为 $H=4$）：\n        1. $(0,0,0.9,1),(1,1,0.3,2),(2,0,0.5,0),(0,1,1.0,2)$\n        2. $(1,1,0.6,2),(2,1,0.9,2),(2,0,0.4,1),(1,0,0.7,0)$\n        3. $(2,0,0.8,0),(0,0,0.9,1),(1,1,0.6,2),(2,1,1.2,2)$\n        4. $(0,1,1.3,2),(2,0,0.5,1),(1,0,0.6,0),(0,1,0.9,2)$\n\n输出规范：\n- 您的程序应生成单行输出，其中包含一个以逗号分隔的列表，该列表被方括号包围。每个测试用例贡献一个内部列表，其中包含三个浮点数，顺序为 $[\\text{IS}, \\hat{V}^\\pi, \\text{DR}]$。因此，总输出必须看起来像 $[[x_{11},x_{12},x_{13}],[x_{21},x_{22},x_{23}],[x_{31},x_{32},x_{33}]]$，其中每个 $x_{ij}$ 是一个浮点数。\n\n额外约束：\n- 离策略评估（OPE）、重要性采样（IS）、双重鲁棒（DR）和最大似然估计（MLE）这些缩写词在您的解决方案中首次使用时必须明确定义。\n- 角度单位或物理单位在此处无关；不要包含它们。\n- 不允许用户输入。所有数据已在上方提供，并且必须嵌入到解决方案中。", "solution": "该问题要求在有限期限马尔可夫决策过程（MDP）中，实现并比较三种用于离策略评估（OPE）的估计器。离策略评估（OPE）是利用在一种策略（行为策略 $b$）下收集的数据，来估计一个新策略（目标策略 $\\pi$）的期望回报的任务。我们被要求使用以下方法计算 $\\pi$ 的价值：\n1.  一个每决策重要性采样（IS）估计器。\n2.  一个基于模型的估计器，其模型通过最大似然估计（MLE）学习得到。\n3.  一个双重鲁棒（DR）估计器，它结合了前两种方法。\n\n我们将首先根据提供的规范，正式定义每个估计器的组成部分和推导过程。状态空间为 $S = \\{0, 1, 2\\}$，动作空间为 $A = \\{0, 1\\}$，期限 $H$ 和折扣因子 $\\gamma$ 因测试用例而异。数据以 $N$ 条轨迹集合的形式提供，其中每条轨迹 $i$ 是一个转移序列 $\\tau^{(i)} = \\{(s_t^{(i)}, a_t^{(i)}, r_t^{(i)}, s_{t+1}^{(i)})\\}_{t=0}^{H-1}$。\n\n**1. 基于模型的估计**\n\n基于模型的方法包括两个阶段：首先，从数据中学习 MDP 的模型；其次，使用该模型计算目标策略 $\\pi$ 的价值。\n\n**1.1. 模型学习**\n我们从所有轨迹的聚合数据集中估计转移概率 $\\hat{P}(s' \\mid s, a)$ 和奖励函数 $\\hat{r}(s, a)$。\n-   **转移模型 $\\hat{P}$**：我们使用带有加一（Laplace）平滑的最大似然估计（MLE）。令 $N(s,a,s')$ 为在状态 $s$ 执行动作 $a$ 后观测到转移到状态 $s'$ 的次数。令 $N(s,a) = \\sum_{s' \\in S} N(s,a,s')$ 为状态-动作对 $(s,a)$ 的总计数。平滑后的概率为：\n    $$\n    \\hat{P}(s' \\mid s, a) = \\frac{N(s, a, s') + 1}{\\sum_{s'' \\in S} (N(s, a, s'') + 1)} = \\frac{N(s, a, s') + 1}{N(s, a) + |S|}\n    $$\n    这种平滑确保没有转移被赋予零概率，这对于动态规划步骤至关重要。\n-   **奖励模型 $\\hat{r}$**：状态-动作对 $(s,a)$ 的奖励被估计为该对观测到的所有奖励的经验平均值。令 $R(s,a)$ 为在状态 $s$ 执行动作 $a$ 后观测到的奖励总和。\n    $$\n    \\hat{r}(s, a) = \\begin{cases} \\frac{R(s, a)}{N(s, a)}  \\text{if } N(s, a) > 0 \\\\ 0  \\text{if } N(s, a) = 0 \\end{cases}\n    $$\n\n**1.2. 通过动态规划计算价值**\n利用估计的模型 $(\\hat{P}, \\hat{r})$，我们可以使用动态规划计算目标策略 $\\pi$ 的状态价值函数 $\\hat{V}_t(s)$ 和状态-动作价值函数 $\\hat{Q}_t(s,a)$。递归从期限 $H$ 开始向后进行：\n-   **边界条件**：在终端时间步 $t=H$，价值为零：对所有 $s \\in S$，$\\hat{V}_H(s) = 0$。\n-   **向后递归 (对于 $t = H-1, \\dots, 0$)**：\n    1.  计算状态-动作价值函数 $\\hat{Q}_t(s,a)$：\n        $$\n        \\hat{Q}_t(s, a) = \\hat{r}(s, a) + \\gamma \\sum_{s' \\in S} \\hat{P}(s' \\mid s, a) \\hat{V}_{t+1}(s')\n        $$\n    2.  计算目标策略 $\\pi$ 下的状态价值函数 $\\hat{V}_t(s)$：\n        $$\n        \\hat{V}_t(s) = \\sum_{a \\in A} \\pi(a \\mid s) \\hat{Q}_t(s, a)\n        $$\n-   **最终的基于模型的估计**：策略 $\\pi$ 的总价值是在时间 $t=0$ 时对初始状态分布 $d_0$ 的期望价值。我们使用数据集中初始状态 $s_0^{(i)}$ 的经验分布：\n    $$\n    d_0(s) = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(s_0^{(i)} = s)\n    $$\n    其中 $\\mathbb{I}(\\cdot)$ 是指示函数。那么，基于模型的估计值为：\n    $$\n    \\hat{V}^\\pi_{\\text{Model}} = \\sum_{s \\in S} d_0(s) \\hat{V}_0(s) = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}_0(s_0^{(i)})\n    $$\n\n**2. 每决策重要性采样（IS）估计器**\n\n重要性采样（IS）是一种在拥有从某个分布生成的样本时，估计另一个分布属性的技术。在 OPE 中，它用于修正行为策略 $b$ 和目标策略 $\\pi$ 之间的不匹配。每决策变体在轨迹的每一步都应用此修正。\n\n单个时间步 $t$ 的重要性权重是比率 $\\rho_t = \\frac{\\pi(a_t \\mid s_t)}{b(a_t \\mid s_t)}$。一条轨迹截至时间 $t$ 的累积重要性权重是逐步权重的乘积：\n$$\n\\rho_{0:t} = \\prod_{k=0}^{t} \\rho_k = \\prod_{k=0}^{t} \\frac{\\pi(a_k \\mid s_k)}{b(a_k \\mid s_k)}\n$$\n每决策 IS 估计器 $\\hat{V}^\\pi_{\\text{PDIS}}$ 是所有轨迹上折扣奖励总和的平均值，其中每一项都由相应的累积重要性权重重新加权：\n$$\n\\hat{V}^\\pi_{\\text{PDIS}} = \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{H-1} \\gamma^t \\rho_{0:t}^{(i)} r_t^{(i)}\n$$\n该估计器是无偏的，但可能具有高方差，特别是当策略 $\\pi$ 和 $b$ 非常不同时，这会导致大的重要性权重。\n\n**3. 双重鲁棒（DR）估计器**\n\n双重鲁棒（DR）估计器结合了基于模型和 IS 的估计器，以利用两者的优点。它之所以是“双重鲁棒”的，是因为如果学习到的模型 $(\\hat{P}, \\hat{r})$ 是正确的，*或者*重要性权重是正确的（即行为策略 $b$ 是已知的），它就能提供对真实值 $V^\\pi$ 的无偏估计。\n\nDR 估计器使用基于模型的估计作为控制变量，以减少 IS 估计器的方差。单条轨迹 $i$ 的逐步 DR 估计器构造如下：\n$$\n\\hat{V}_{\\text{DR}}^{(i)} = \\hat{V}_0(s_0^{(i)}) + \\sum_{t=0}^{H-1} \\gamma^t \\rho_{0:t}^{(i)} \\delta_t^{(i)}\n$$\n其中 $\\hat{V}_0(s_0^{(i)})$ 是对初始状态的基于模型的价值预测，而 $\\delta_t^{(i)}$ 是模型在观测到的转移上的单步时序差分误差，定义为：\n$$\n\\delta_t^{(i)} = r_t^{(i)} + \\gamma \\hat{V}_{t+1}(s_{t+1}^{(i)}) - \\hat{Q}_t(s_t^{(i)}, a_t^{(i)})\n$$\n在这里，$r_t^{(i)}$ 和 $s_{t+1}^{(i)}$ 是从轨迹中实际观测到的奖励和下一个状态，而 $\\hat{V}_{t+1}$ 和 $\\hat{Q}_t$ 是在学习到的模型上通过动态规划计算出的价值函数。如果模型是完美的，那么 $\\mathbb{E}[\\delta_t^{(i)}] = 0$，估计器就简化为基于模型的估计器。如果模型是错误的，第二项通过重要性采样比率加权来修正模型的误差。\n\n最终的 DR 估计是所有轨迹的平均值：\n$$\n\\hat{V}^\\pi_{\\text{DR}} = \\frac{1}{N} \\sum_{i=1}^N \\hat{V}_{\\text{DR}}^{(i)}\n$$\n该估计器通常比 IS 估计器具有低得多的方差，同时保留了在比基于模型的估计器更弱的条件下无偏的特性。\n\n**算法流程总结**\n对于每个测试用例，计算过程如下：\n1.  初始化用于策略、轨迹和 MDP 参数（$|S|$, $|A|$, $\\gamma$, $H$）的数据结构。\n2.  **模型估计**：\n    -   解析轨迹数据以计算计数 $N(s,a,s')$、$N(s,a)$ 和奖励总和 $R(s,a)$。\n    -   使用拉普拉斯平滑计算转移模型 $\\hat{P}$，并使用经验平均值计算奖励模型 $\\hat{r}$。\n3.  **基于模型的评估**：\n    -   初始化 $\\hat{V}_H(s) = 0$。\n    -   从 $H-1$ 向下迭代到 $0$，计算所有的 $\\hat{Q}_t$ 和 $\\hat{V}_t$ 数组。\n    -   通过对数据集中初始状态的 $\\hat{V}_0(s_0)$ 求平均值，计算 $\\hat{V}^\\pi_{\\text{Model}}$。\n4.  **IS 和 DR 估计**：\n    -   将 IS 和 DR 估计的总和初始化为零。\n    -   对于数据集中的每条轨迹 $i$：\n        -   初始化累积重要性权重 $\\rho_{prod} = 1.0$，以及用于 IS 和 DR 的每条轨迹的总和。\n        -   轨迹 $i$ 的 DR 总和以基于模型的价值 $\\hat{V}_0(s_0^{(i)})$ 开始。\n        -   从 $0$ 迭代到 $H-1$：\n            -   更新 $\\rho_{prod} \\leftarrow \\rho_{prod} \\times \\frac{\\pi(a_t^{(i)} \\mid s_t^{(i)})}{b(a_t^{(i)} \\mid s_t^{(i)})}$。\n            -   将 $\\gamma^t \\rho_{prod} r_t^{(i)}$ 添加到轨迹 $i$ 的 IS 总和中。\n            -   计算 $\\delta_t^{(i)} = r_t^{(i)} + \\gamma \\hat{V}_{t+1}(s_{t+1}^{(i)}) - \\hat{Q}_t(s_t^{(i)}, a_t^{(i)})$。\n            -   将 $\\gamma^t \\rho_{prod} \\delta_t^{(i)}$ 添加到轨迹 $i$ 的 DR 总和中。\n        -   将完成的每条轨迹的总和加到 IS 和 DR 的总和中。\n5.  **最终估计**：\n    -   将总和除以轨迹数 $N$，得到最终的 $\\hat{V}^\\pi_{\\text{PDIS}}$ 和 $\\hat{V}^\\pi_{\\text{DR}}$ 估计值。\n6.  收集当前测试用例的三个估计值 $[\\hat{V}^\\pi_{\\text{PDIS}}, \\hat{V}^\\pi_{\\text{Model}}, \\hat{V}^\\pi_{\\text{DR}}]$。\n7.  对所有测试用例重复此过程，并格式化最终输出。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the off-policy evaluation problem for the three given test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Test Case 1\n        {\n            \"gamma\": 0.95,\n            \"H\": 3,\n            \"b\": np.array([[0.7, 0.3], [0.4, 0.6], [0.5, 0.5]]),\n            \"pi\": np.array([[0.2, 0.8], [0.6, 0.4], [0.3, 0.7]]),\n            \"episodes\": [\n                [(0, 0, 1.0, 1), (1, 1, 0.5, 2), (2, 1, 1.2, 2)],\n                [(0, 1, 1.5, 2), (2, 1, 1.0, 2), (2, 0, 0.7, 1)],\n                [(1, 0, 0.8, 0), (0, 1, 1.1, 2), (2, 1, 1.0, 2)],\n                [(2, 0, 0.9, 1), (1, 0, 0.6, 0), (0, 1, 1.4, 2)],\n                [(1, 1, 0.4, 2), (2, 1, 1.3, 2), (2, 0, 0.5, 1)],\n                [(0, 0, 1.1, 1), (1, 1, 0.6, 2), (2, 1, 1.0, 2)],\n            ],\n        },\n        # Test Case 2\n        {\n            \"gamma\": 0.95,\n            \"H\": 3,\n            \"b\": np.array([[0.5, 0.5], [0.2, 0.8], [0.6, 0.4]]),\n            \"pi\": np.array([[0.5, 0.5], [0.2, 0.8], [0.6, 0.4]]),\n            \"episodes\": [\n                [(0, 0, 1.0, 1), (1, 1, 1.2, 2), (2, 0, 0.4, 1)],\n                [(1, 1, 0.7, 2), (2, 0, 0.6, 0), (0, 1, 1.1, 2)],\n                [(2, 0, 0.9, 1), (1, 1, 0.8, 2), (2, 1, 1.0, 2)],\n                [(0, 1, 1.4, 2), (2, 0, 0.5, 0), (0, 1, 1.2, 2)],\n                [(2, 0, 0.6, 0), (0, 1, 1.0, 2), (2, 0, 0.7, 1)],\n            ],\n        },\n        # Test Case 3\n        {\n            \"gamma\": 0.95,\n            \"H\": 4,\n            \"b\": np.array([[0.8, 0.2], [0.3, 0.7], [0.7, 0.3]]),\n            \"pi\": np.array([[0.1, 0.9], [0.85, 0.15], [0.1, 0.9]]),\n            \"episodes\": [\n                [(0, 0, 0.9, 1), (1, 1, 0.3, 2), (2, 0, 0.5, 0), (0, 1, 1.0, 2)],\n                [(1, 1, 0.6, 2), (2, 1, 0.9, 2), (2, 0, 0.4, 1), (1, 0, 0.7, 0)],\n                [(2, 0, 0.8, 0), (0, 0, 0.9, 1), (1, 1, 0.6, 2), (2, 1, 1.2, 2)],\n                [(0, 1, 1.3, 2), (2, 0, 0.5, 1), (1, 0, 0.6, 0), (0, 1, 0.9, 2)],\n            ]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        gamma = case[\"gamma\"]\n        H = case[\"H\"]\n        b = case[\"b\"]\n        pi = case[\"pi\"]\n        episodes = case[\"episodes\"]\n        num_episodes = len(episodes)\n        \n        S_size = b.shape[0]\n        A_size = b.shape[1]\n\n        # 1. Model Estimation (MLE with Laplace smoothing)\n        transition_counts = np.zeros((S_size, A_size, S_size))\n        reward_sums = np.zeros((S_size, A_size))\n        sa_counts = np.zeros((S_size, A_size))\n\n        for ep in episodes:\n            for t, (s, a, r, s_next) in enumerate(ep):\n                transition_counts[s, a, s_next] += 1\n                reward_sums[s, a] += r\n                sa_counts[s, a] += 1\n        \n        P_hat = np.zeros((S_size, A_size, S_size))\n        r_hat = np.zeros((S_size, A_size))\n\n        for s in range(S_size):\n            for a in range(A_size):\n                if sa_counts[s, a] > 0:\n                    r_hat[s, a] = reward_sums[s, a] / sa_counts[s, a]\n                    # Laplace smoothing for P_hat\n                    P_hat[s, a, :] = (transition_counts[s, a, :] + 1) / (sa_counts[s, a] + S_size)\n                else:\n                    # If (s,a) is not observed, r_hat is 0 and P_hat is uniform (from smoothing)\n                    r_hat[s, a] = 0.0\n                    P_hat[s, a, :] = 1.0 / S_size\n\n        # 2. Model-based value estimation (Dynamic Programming)\n        V_hat = np.zeros((H + 1, S_size))\n        Q_hat = np.zeros((H, S_size, A_size))\n\n        for t in range(H - 1, -1, -1):\n            V_next = V_hat[t + 1, :]\n            expected_V_next = np.sum(P_hat * V_next, axis=2) # Shape: (S_size, A_size)\n            Q_hat[t, :, :] = r_hat + gamma * expected_V_next\n            V_hat[t, :] = np.sum(pi * Q_hat[t, :, :], axis=1)\n\n        initial_states = [ep[0][0] for ep in episodes]\n        V_model_based = np.mean([V_hat[0, s0] for s0 in initial_states])\n        \n        # 3. IS and DR estimation\n        total_is_return = 0.0\n        total_dr_return = 0.0\n        \n        for i in range(num_episodes):\n            ep = episodes[i]\n            s0 = ep[0][0]\n            \n            # Per-decision IS\n            episode_is_return = 0.0\n            rho_product = 1.0\n            for t in range(H):\n                s, a, r, s_next = ep[t]\n                rho_t = pi[s, a] / b[s, a]\n                rho_product *= rho_t\n                episode_is_return += (gamma**t) * rho_product * r\n            total_is_return += episode_is_return\n\n            # Doubly Robust\n            episode_dr_return = V_hat[0, s0]\n            rho_product = 1.0\n            for t in range(H):\n                s, a, r, s_next = ep[t]\n                rho_t = pi[s, a] / b[s, a]\n                rho_product *= rho_t\n                \n                delta_t = r + gamma * V_hat[t + 1, s_next] - Q_hat[t, s, a]\n                episode_dr_return += (gamma**t) * rho_product * delta_t\n            total_dr_return += episode_dr_return\n            \n        V_is = total_is_return / num_episodes\n        V_dr = total_dr_return / num_episodes\n        \n        all_results.append([V_is, V_model_based, V_dr])\n\n    # Format output as a list of lists of floats\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3145244"}]}