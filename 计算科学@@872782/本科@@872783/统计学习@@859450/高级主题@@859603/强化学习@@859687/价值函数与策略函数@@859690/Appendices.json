{"hands_on_practices": [{"introduction": "理论知识的最好检验是实践。本练习将引导你运用动态规划中的反向归纳法，为一个已知模型的有限时域马尔可夫决策过程（MDP）求解。通过从最后一步倒推计算，你将亲身体会未来奖励如何影响当前决策，并理解为何在有限时域问题中，最优策略常常是非平稳的——即最佳行动会随时间的推移而改变。", "problem": "考虑以下有限期马尔可夫决策过程 (MDP)，其期域为 $H = 3$。状态空间为 $\\mathcal{S} = \\{s, j, z\\}$。动作空间取决于状态：在状态 $s$ 时，智能体可以选择动作 $a$ 和 $b$；在状态 $j$ 和 $z$ 时，只有一个可用动作。单步奖励函数 $r(\\cdot,\\cdot)$ 和转移动态如下：\n- 如果当前状态为 $s$ 且选择动作 $a$，则即时奖励为 $r(s,a) = 2$，下一状态以概率 $1$ 为 $z$。\n- 如果当前状态为 $s$ 且选择动作 $b$，则即时奖励为 $r(s,b) = 0$，下一状态以概率 $\\tfrac{1}{2}$ 为 $j$，以概率 $\\tfrac{1}{2}$ 为 $s$。\n- 如果当前状态为 $j$，则即时奖励为 $r(j,\\cdot) = 5$，下一状态以概率 $1$ 为 $z$。\n- 如果当前状态为 $z$，则即时奖励为 $r(z,\\cdot) = 0$，下一状态以概率 $1$ 为 $z$。\n\n假设每个片段（episode）恰好有 $H = 3$ 个决策阶段，并且在每个阶段 $t \\in \\{1,2,3\\}$，从该阶段的当前状态-动作对累积奖励。设一个随时间变化的策略表示为 $\\pi = \\{\\pi_{1}, \\pi_{2}, \\pi_{3}\\}$，其中 $\\pi_{t}$ 在时间 $t$ 将当前状态映射到一个动作。设策略的有限期价值函数定义为\n$$\nV_{t}^{\\pi}(x) \\equiv \\mathbb{E}^{\\pi}\\!\\left[\\sum_{k=t}^{H} r\\!\\left(S_{k}, A_{k}\\right)\\,\\middle|\\, S_{t} = x \\right],\n$$\n对于任何状态 $x \\in \\mathcal{S}$ 和时间 $t \\in \\{1,2,3\\}$。这里 $\\mathbb{E}^{\\pi}[\\cdot]$ 表示在由策略 $\\pi$、马尔可夫性质和给定的转移概率所引出的轨迹分布下的期望。\n\n仅使用这些定义、马尔可夫性质和全期望定律，完成以下任务：\n1) 对于每个时间 $t \\in \\{1,2,3\\}$，确定在最优策略下，哪个动作在状态 $s$ 能最大化期望回报，并由此证明最优策略在期域内是稳态的还是非稳态的。\n2) 计算 $t \\in \\{1,2,3\\}$ 的最优有限期价值 $V_{t}^{\\ast}(s)$，并比较它们在不同 $t$ 时的值，解释它们为何不同。\n\n请给出 $V_{1}^{\\ast}(s)$ 的精确值作为你的最终答案，形式为最简分数。不要四舍五入；请提供精确值。", "solution": "该问题提供了对有限期马尔可夫决策过程 (MDP) 的完整且一致的描述。它在科学上基于随机最优控制理论，是适定的和客观的。所有必要的参数——状态空间 $\\mathcal{S}$、动作空间 $\\mathcal{A}$、奖励函数 $r$、转移概率 $P$ 和期域 $H$——都得到了明确定义。该问题是有效的，可以使用动态规划的标准方法（反向归纳法）来解决。\n\n对于一个策略 $\\pi = \\{\\pi_1, \\pi_2, \\dots, \\pi_H\\}$，在时间 $t$ 从状态 $x$ 开始的价值函数定义为：\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[\\sum_{k=t}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n我们可以推导出该价值函数的递归关系。通过拆分求和项并应用全期望定律：\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}\\left[r(S_t, A_t) + \\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n$$V_{t}^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_t, A_t) | S_t = x] + \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right]$$\n给定策略 $A_t = \\pi_t(S_t)$，第一项是 $r(x, \\pi_t(x))$。对于第二项，我们以下一状态 $S_{t+1}$ 为条件：\n$$\\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_t = x \\right] = \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) \\, \\mathbb{E}^{\\pi}\\left[\\sum_{k=t+1}^{H} r(S_k, A_k) \\, \\middle| \\, S_{t+1} = x' \\right]$$\n根据定义，内部的期望是 $V_{t+1}^{\\pi}(x')$。因此，我们得到了给定策略 $\\pi$ 的贝尔曼方程：\n$$V_{t}^{\\pi}(x) = r(x, \\pi_t(x)) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, \\pi_t(x)) V_{t+1}^{\\pi}(x')$$\n最优价值函数 $V_t^*(x) = \\max_{\\pi} V_t^\\pi(x)$ 是通过在每一步选择使右侧最大化的动作来找到的。这给出了有限期域的贝尔曼最优性方程：\n$$V_t^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_{t+1}^*(x') \\right\\}$$\n这个过程从最后的时间步 $H$ 开始，然后向后推导。对于 $t=H=3$，价值函数定义中的求和只有一项：\n$$V_3^{\\pi}(x) = \\mathbb{E}^{\\pi}[r(S_3, A_3) | S_3=x] = r(x, \\pi_3(x))$$\n因此，在时间 $t=3$ 时的最优价值函数就是最大即时奖励：\n$$V_3^*(x) = \\max_{u \\in \\mathcal{A}(x)} r(x, u)$$\n这可以看作是应用贝尔曼方程，并对所有状态 $x \\in \\mathcal{S}$ 设置终端条件 $V_{H+1}^*(x) = V_4^*(x) = 0$。\n\n我们现在用反向归纳法来解决这个问题。期域是 $H=3$。\n\n**时间步 $t=3$：**\n我们为每个状态 $x \\in \\{s, j, z\\}$ 计算 $V_3^*(x)$。\n-   对于状态 $s$：$V_3^*(s) = \\max_{u \\in \\{a,b\\}} r(s,u) = \\max\\{r(s,a), r(s,b)\\} = \\max\\{2, 0\\} = 2$。最优动作是 $\\pi_3^*(s) = a$。\n-   对于状态 $j$：只有一个动作。$V_3^*(j) = r(j, \\cdot) = 5$。\n-   对于状态 $z$：只有一个动作。$V_3^*(z) = r(z, \\cdot) = 0$。\n\n**时间步 $t=2$：**\n我们使用 $V_3^*(x')$ 的值来计算 $V_2^*(x)$。\n$$V_2^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_3^*(x') \\right\\}$$\n-   对于状态 $s$：我们比较采取动作 $a$ 和动作 $b$ 的价值。\n    -   动作 $a$：期望回报是 $r(s, a) + P(z|s,a)V_3^*(z) = 2 + 1 \\cdot 0 = 2$。\n    -   动作 $b$：期望回报是 $r(s, b) + P(j|s,b)V_3^*(j) + P(s|s,b)V_3^*(s) = 0 + \\frac{1}{2} \\cdot V_3^*(j) + \\frac{1}{2} \\cdot V_3^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(2) = \\frac{5}{2} + 1 = \\frac{7}{2}$。\n    -   $V_2^*(s) = \\max\\{2, \\frac{7}{2}\\} = \\frac{7}{2}$。最优动作是 $\\pi_2^*(s) = b$。\n-   对于状态 $j$：$V_2^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_3^*(z) = 5 + 1 \\cdot 0 = 5$。\n-   对于状态 $z$：$V_2^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_3^*(z) = 0 + 1 \\cdot 0 = 0$。\n\n**时间步 $t=1$：**\n我们使用 $V_2^*(x')$ 的值来计算 $V_1^*(x)$。\n$$V_1^*(x) = \\max_{u \\in \\mathcal{A}(x)} \\left\\{ r(x, u) + \\sum_{x' \\in \\mathcal{S}} P(x'|x, u) V_2^*(x') \\right\\}$$\n-   对于状态 $s$：我们比较采取动作 $a$ 和动作 $b$ 的价值。\n    -   动作 $a$：期望回报是 $r(s, a) + P(z|s,a)V_2^*(z) = 2 + 1 \\cdot 0 = 2$。\n    -   动作 $b$：期望回报是 $r(s, b) + P(j|s,b)V_2^*(j) + P(s|s,b)V_2^*(s) = 0 + \\frac{1}{2} \\cdot V_2^*(j) + \\frac{1}{2} \\cdot V_2^*(s) = \\frac{1}{2}(5) + \\frac{1}{2}(\\frac{7}{2}) = \\frac{5}{2} + \\frac{7}{4} = \\frac{10}{4} + \\frac{7}{4} = \\frac{17}{4}$。\n    -   $V_1^*(s) = \\max\\{2, \\frac{17}{4}\\} = \\frac{17}{4}$。最优动作是 $\\pi_1^*(s) = b$。\n-   对于状态 $j$：$V_1^*(j) = r(j, \\cdot) + P(z|j, \\cdot)V_2^*(z) = 5 + 1 \\cdot 0 = 5$。\n-   对于状态 $z$：$V_1^*(z) = r(z, \\cdot) + P(z|z, \\cdot)V_2^*(z) = 0 + 1 \\cdot 0 = 0$。\n\n我们现在可以回答具体问题了。\n\n1) 对于每个时间 $t \\in \\{1,2,3\\}$，在状态 $s$ 的最优动作是：\n-   在 $t=3$ 时，$\\pi_3^*(s) = a$，因为 $r(s,a)=2 > r(s,b)=0$。\n-   在 $t=2$ 时，$\\pi_2^*(s) = b$，因为动作 $b$ 的期望回报（即 $\\frac{7}{2}$）大于动作 $a$ 的回报（即 $2$）。\n-   在 $t=1$ 时，$\\pi_1^*(s) = b$，因为动作 $b$ 的期望回报（即 $\\frac{17}{4}$）大于动作 $a$ 的回报（即 $2$）。\n如果对于任何给定状态的最优动作不随时间变化，则策略是稳态的。这里，状态 $s$ 的最优动作不是恒定的：$\\pi_1^*(s) = b$, $\\pi_2^*(s) = b$，但是 $\\pi_3^*(s) = a$。因此，最优策略 $\\pi^*$ 是**非稳态的**。这是有限期 MDP 的一个典型特征；动作的选择取决于剩余的时间。在 $t=3$ 时，没有未来，智能体贪婪地获取即时奖励。在 $t=1$ 和 $t=2$ 时，智能体通过选择动作 $b$ 放弃了 $2$ 的即时奖励，以换取转移到具有更高未来价值的状态（$j$ 和 $s$ 本身）的机会，从而获得更大的总期望回报。\n\n2) $t \\in \\{1,2,3\\}$ 的最优有限期价值 $V_t^*(s)$ 为：\n-   $V_1^*(s) = \\frac{17}{4} = 4.25$\n-   $V_2^*(s) = \\frac{7}{2} = 3.5$\n-   $V_3^*(s) = 2$\n比较它们，我们看到 $V_1^*(s) > V_2^*(s) > V_3^*(s)$。\n价值 $V_t^*(s)$ 表示从时间 $t$ 到期域结束（$t=3$）所能累积的最大可能期望总奖励。\n$V_3^*(s)$ 是一个阶段（$k=3$）内的期望奖励。\n$V_2^*(s)$ 是两个阶段（$k=2,3$）内的期望奖励。\n$V_1^*(s)$ 是三个阶段（$k=1,2,3$）内的期望奖励。\n由于此 MDP 中的所有单步奖励都是非负的，所以随着阶段数的增加，累积奖励是非递减的。在最后一个步骤之前的每一步，都有机会累积更多奖励。例如，$V_2^*(s)$ 大于 $V_3^*(s)$，因为与从 $t=3$ 开始相比，从 $t=2$ 开始会多一个时间步来收集奖励。严格不等式成立，因为在 $t=2$ 时，$s$ 处的最优动作会导致在 $t=3$ 时可以获得正奖励的状态。同样地，$V_1^*(s)$ 大于 $V_2^*(s)$，因为还有一个阶段（$t=1$）可以收集奖励。\n\n要求的最终答案是 $V_1^*(s)$ 的精确值。\n$$V_1^*(s) = \\frac{17}{4}$$", "answer": "$$\\boxed{\\frac{17}{4}}$$", "id": "3190850"}, {"introduction": "当我们不知道环境的完整模型时，智能体该如何学习？这个练习将我们带入 Q学习的世界，并聚焦于其核心挑战之一：探索与利用的权衡。你将通过概念论证和具体计算，来理解一种被称为“乐观初始化”的巧妙技巧如何激励智能体进行系统性探索，从而在未知环境中有效地学习最优策略 ([@problem_id:3190816])。", "problem": "考虑一个具有有限状态集和有界奖励的确定性马尔可夫决策过程（MDP）。设智能体使用表格型动作价值学习，其单步更新规则为\n$$\nQ_{t+1}(s_{t},a_{t}) \\leftarrow (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right),\n$$\n其中，对于所有状态-动作对，有 $Q_{0}(s,a)=Q_{0}>0$，学习率 $\\alpha \\in (0,1]$，折扣因子 $\\gamma \\in (0,1)$。在每个决策时刻 $t$，智能体根据 $Q_{t}$ 贪婪地选择动作，平局由一个固定的、依赖于状态但任意的排序规则打破。奖励是有界的，$0 \\le r_{t} \\le R_{\\max}$。\n\nA部分（概念性）。仅使用上述定义，并且除了确定性和有界奖励外，不假设任何特定的MDP结构，请论证为何在纯贪婪策略下，乐观初始化 $Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ 会鼓励探索。具体而言，请论证在任何状态下，一旦一个动作被尝试至少一次，其 $Q$ 值就会下降到低于该状态下任何尚未尝试的动作的 $Q$ 值，从而确保智能体最终会尝试该状态下所有可用的动作。\n\nB部分（计算）。现在将问题具体化为一个确定性的网格世界，其中有一个非终止的起始单元格 $s$ 和一个终止的目标单元格 $g$。在 $s$ 中正好有两个可用动作：\n- 动作 $a_{R}$ 确定性地移动到 $g$ 并产生奖励 $r=1$，之后回合结束，智能体被重置到 $s$，不再有进一步的奖励。\n- 动作 $a_{L}$ 确定性地使智能体停留在 $s$ 并产生奖励 $r=0$。\n\n设智能体使用相同的单步更新规则，学习率 $\\alpha=1$，折扣因子 $\\gamma \\in (0,1)$，贪婪动作选择，平局时偏向于 $a_{L}$，以及初始化 $Q_{0}(s,a_{L})=Q_{0}(s,a_{R})=Q_{0}>0$，而 $Q_{0}(g,\\cdot)=0$。定义收敛时间 $T$ 为最小的整数，使得在恰好 $T$ 次动作选择及其相关更新后，下一次访问 $s$ 时的贪婪动作是 $a_{R}$，并且在所有后续对 $s$ 的访问中都保持为 $a_{R}$。\n\n推导 $T$ 作为 $Q_{0}$ 和 $\\gamma$ 的函数的封闭形式表达式。你的最终答案必须是单一的解析表达式，而不是不等式或方程。不需要进行数值取整。", "solution": "用户提供了一个关于确定性马尔可夫决策过程中Q学习的两部分问题。该问题定义明确，科学上基于强化学习的原理，并包含了完整解答所需的所有信息。\n\n### A部分：通过乐观初始化论证探索的合理性\n\n目标是论证为何在纯贪婪策略下，乐观初始化 $Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ 会鼓励探索。在此背景下，探索意味着智能体最终会尝试任何给定状态下的所有可用动作。\n\n首先，我们必须理解数量 $\\frac{R_{\\max}}{1-\\gamma}$ 的意义。对于任何状态 $s$ 和任何策略 $\\pi$，状态价值函数 $V^{\\pi}(s)$ 是从状态 $s$ 开始并遵循策略 $\\pi$ 的未来奖励的期望折扣总和。由于奖励是有界的，满足 $r_t \\le R_{\\max}$，所以任何状态的最大可能价值受折扣最大奖励之和的限制：\n$$V^*(s) = \\max_{\\pi} V^{\\pi}(s) \\le \\sum_{t=0}^{\\infty} \\gamma^t R_{\\max} = \\frac{R_{\\max}}{1-\\gamma}$$\n最优动作价值函数 $Q^*(s,a)$ 也同样是有界的。因此，对于所有 $(s,a)$ 对，条件 $Q_{0}(s,a) = Q_{0} > \\frac{R_{\\max}}{1-\\gamma}$ 意味着所有初始Q值都严格大于任何可能的最优Q值。这就是“乐观初始化”的定义。\n\n现在，我们论证任何动作一旦被执行，其Q值将被更新为一个严格小于 $Q_0$ 的值。这将使得该状态下任何未曾尝试的动作对贪婪智能体显得更具吸引力，从而鼓励智能体选择它们。\n\n我们用归纳法证明，对于所有 $t \\ge 1$，任何至少更新过一次的Q值 $Q_t(s,a)$ 都严格小于 $Q_0$，而任何未被更新的Q值都保持为 $Q_0$。\n设 $(s_t, a_t)$ 是在决策时刻 $t$选择的状态-动作对。更新规则是：\n$$Q_{t+1}(s_{t},a_{t}) = (1-\\alpha)Q_{t}(s_{t},a_{t}) + \\alpha\\left(r_{t} + \\gamma \\max_{a'} Q_{t}(s_{t+1},a')\\right)$$\n所有其他Q值保持不变：对于 $(s,a) \\neq (s_t,a_t)$，$Q_{t+1}(s,a) = Q_t(s,a)$。\n\n**基本情况：** 在 $t=0$ 时，没有值被更新过。所有值都是 $Q_0$。假设在 $t=0$ 时，智能体在状态 $s_0$ 中采取动作 $a_0$。Q值为 $Q_0(s_0, a_0) = Q_0$。\n更新规则给出：\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha\\left(r_{0} + \\gamma \\max_{a'} Q_{0}(s_{1},a')\\right)$$\n由于对于所有 $a'$ 都有 $Q_0(s_1, a') = Q_0$，表达式简化为：\n$$Q_{1}(s_{0},a_{0}) = (1-\\alpha)Q_{0} + \\alpha(r_{0} + \\gamma Q_{0})$$\n为了证明 $Q_1(s_0, a_0) < Q_0$，我们需要证明目标值 $r_0 + \\gamma Q_0$ 小于 $Q_0$。根据给定条件 $Q_0 > \\frac{R_{\\max}}{1-\\gamma}$，可得 $Q_0(1-\\gamma) > R_{\\max}$。由于 $r_0 \\le R_{\\max}$，我们有 $r_0 < Q_0(1-\\gamma)$，整理后得到 $r_0 + \\gamma Q_0 < Q_0$。\n新值 $Q_1(s_0, a_0)$ 是 $Q_0$ 和一个严格小于 $Q_0$ 的值的凸组合。由于 $\\alpha \\in (0,1]$，新值必须严格小于 $Q_0$。所有其他Q值保持为 $Q_0$。\n\n**归纳步骤：** 假设在时刻 $t$，所有Q值都满足 $Q_t(s,a) \\le Q_0$。我们来计算 $(s_t, a_t)$ 的更新。目标值为 $T_t = r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$。\n根据归纳假设，$\\max_{a'} Q_t(s_{t+1}, a') \\le Q_0$。因此，目标值是有界的：\n$$T_t \\le r_t + \\gamma Q_0$$\n如前所述，$r_t + \\gamma Q_0 < Q_0$。所以，$T_t < Q_0$。\n新值为 $Q_{t+1}(s_t, a_t) = (1-\\alpha)Q_t(s_t, a_t) + \\alpha T_t$。这是一个 $Q_t(s_t, a_t) \\le Q_0$ 和 $T_t < Q_0$ 的凸组合。因此，$Q_{t+1}(s_t, a_t) < Q_0$。\n对于任何其他对 $(s,a) \\neq (s_t, a_t)$，$Q_{t+1}(s,a) = Q_t(s,a) \\le Q_0$。\n因此，通过归纳法，任何Q值一旦更新，就会变得严格小于 $Q_0$。\n\n**结论：** 考虑任何状态 $s$。设 $A_{tried}$ 为在状态 $s$ 中已尝试过的动作集合，而 $A_{untried}$ 为未尝试过的动作集合。对于任何 $a_{tried} \\in A_{tried}$，其值 $Q_t(s, a_{tried})$ 至少被更新过一次，所以 $Q_t(s, a_{tried}) < Q_0$。对于任何 $a_{untried} \\in A_{untried}$，其值从未被更新过，所以 $Q_t(s, a_{untried}) = Q_0$。\n一个处于状态 $s$ 的贪婪智能体选择动作 $a = \\arg\\max_{a'} Q_t(s, a')$。最大值是 $Q_0$，对应于 $A_{untried}$ 中的任何一个动作。因此，智能体保证会从 $A_{untried}$ 中选择一个动作（具体选择哪个由平局打破规则决定）。这个过程会一直持续到 $A_{untried}$ 为空，从而确保状态 $s$ 中的每个动作最终都会被尝试。这表明，使用贪婪策略的乐观初始化会鼓励系统性探索。\n\n### B部分：收敛时间的计算\n\n问题指定了一个确定性的MDP，有两个状态 $s$ 和 $g$，以及从 $s$ 出发的两个动作 $a_L$ 和 $a_R$。我们已知 $\\alpha=1$。单步更新简化为：\n$$Q_{t+1}(s_t, a_t) \\leftarrow r_t + \\gamma \\max_{a'} Q_t(s_{t+1}, a')$$\n\n让我们追踪Q值 $Q(s, a_L)$ 和 $Q(s, a_R)$ 的演变。\n初始值：$Q_0(s, a_L) = Q_0$ 和 $Q_0(s, a_R) = Q_0$。终止状态的值对于所有 $t$ 都是 $Q_t(g, \\cdot) = 0$。\n\n**步骤 0 (t=0):**\n- 状态: $s_0=s$。Q值相等: $Q_0(s, a_L) = Q_0(s, a_R)$。\n- 动作: 平局打破规则偏向 $a_L$，所以 $a_0 = a_L$。\n- 结果: 奖励 $r_0=0$，下一状态 $s_1=s$。\n- 更新: $Q_1(s, a_L) \\leftarrow r_0 + \\gamma \\max_{a'} Q_0(s, a') = 0 + \\gamma \\max(Q_0, Q_0) = \\gamma Q_0$。\n- 其他Q值不变: $Q_1(s, a_R) = Q_0(s, a_R) = Q_0$。\n- 经过1次动作后，值为 $Q_1(s, a_L) = \\gamma Q_0$ 和 $Q_1(s, a_R) = Q_0$。\n\n**步骤 1 (t=1):**\n- 状态: $s_1=s$。由于 $\\gamma \\in (0,1)$ 且 $Q_0>0$，我们有 $Q_1(s, a_R) > Q_1(s, a_L)$。\n- 动作: 贪婪选择是 $a_1 = a_R$。\n- 结果: 奖励 $r_1=1$，下一状态 $s_2=g$ (终止)。回合结束。\n- 更新: $Q_2(s, a_R) \\leftarrow r_1 + \\gamma \\max_{a'} Q_1(g, a') = 1 + \\gamma \\cdot 0 = 1$。\n- 其他Q值不变: $Q_2(s, a_L) = Q_1(s, a_L) = \\gamma Q_0$。\n- 经过2次动作后，智能体被重置到状态 $s$，值为 $Q_2(s, a_L) = \\gamma Q_0$ 和 $Q_2(s, a_R) = 1$。\n\n**后续步骤 (t $\\ge$ 2):**\n智能体现在处于状态 $s$，并比较 $Q(s, a_L) = \\gamma Q_0$ 和 $Q(s, a_R) = 1$。贪婪选择取决于哪个更大。\n设 $m$ 为连续额外选择 $a_L$ 的次数。这种情况会持续发生，只要 $Q(s,a_L) \\ge Q(s,a_R)$。\n\n- 如果 $\\gamma Q_0 < 1$，在 $t=2$ 时智能体选择 $a_R$。更新为 $Q_3(s, a_R) \\leftarrow 1$，所以值不改变。$a_R$ 的选择变得稳定。在 $T=2$ 步后达到收敛。在这种情况下，$m=0$。\n- 如果 $\\gamma Q_0 \\ge 1$，在 $t=2$ 时智能体选择 $a_L$。\n  - 更新: $Q_3(s, a_L) \\leftarrow 0 + \\gamma \\max\\{Q_2(s, a_L), Q_2(s, a_R)\\} = \\gamma \\max\\{\\gamma Q_0, 1\\} = \\gamma (\\gamma Q_0) = \\gamma^2 Q_0$。\n  - $Q_3(s, a_R)=1$。\n- 在 $t=3$ 时，智能体比较 $\\gamma^2 Q_0$ 和 $1$。如果 $\\gamma^2 Q_0 \\ge 1$，它会再次选择 $a_L$。\n\n这种模式会继续下去。设 $m$ 为在初始 $(a_L, a_R)$ 序列之后选择 $a_L$ 的次数。如果条件 $Q(s,a_L) \\ge 1$ 成立，则 $a_L$ 会被第 $(k+1)$ 次选择（其中 $k=0, 1, ..., m-1$）。在初始回合（2步）和 $k$ 次额外的 $a_L$ 选择之后，值为 $Q(s, a_L) = \\gamma^{k+1} Q_0$。因此，再次选择 $a_L$ 的条件是 $\\gamma^{k+1} Q_0 \\ge 1$。\n\n$m$ 是满足 $\\gamma^{k+1} Q_0 \\ge 1$ 的非负整数 $k$ 的数量。\n$$\\gamma^{k+1} \\ge \\frac{1}{Q_0}$$\n以 $1/\\gamma > 1$ 为底取对数：\n$$k+1 \\le \\log_{1/\\gamma}\\left(\\frac{1}{Q_0^{-1}}\\right) = \\log_{1/\\gamma}(Q_0)$$\n$$k \\le \\log_{1/\\gamma}(Q_0) - 1$$\n我们需要计算满足此条件的非负整数 $k$ 的数量。\n如果 $\\log_{1/\\gamma}(Q_0) < 1$ (即 $Q_0 < 1/\\gamma$ 或 $\\gamma Q_0 < 1$)，那么 $\\log_{1/\\gamma}(Q_0) - 1 < 0$，所以没有满足该不等式的非负整数 $k$。在这种情况下，$m=0$。\n如果 $\\log_{1/\\gamma}(Q_0) \\ge 1$，非负整数为 $k=0, 1, \\dots, \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor$。数量为 $m = \\lfloor \\log_{1/\\gamma}(Q_0) - 1 \\rfloor + 1 = \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor$。\n综合这些情况，$m = \\max(0, \\lfloor \\log_{1/\\gamma}(Q_0) \\rfloor)$。\n\n直到收敛的总动作数 $T$ 是所采取动作的总和。\n- 初始 $a_L$ 的 $1$ 次动作。\n- 第一次 $a_R$ 的 $1$ 次动作。\n- 后续 $a_L$ 选择的 $m$ 次动作。\n总动作数: $T = 1 + 1 + m = 2 + m$。\n在这 $T$ 次动作之后，在决策时刻 $t=T$，值 $Q_T(s, a_L)$ 变为 $\\gamma^{m+1} Q_0$。根据 $m$ 的定义，我们有 $\\gamma^{m+1}Q_0 < 1$。所以，在 $t=T$ 时，智能体比较 $Q_T(s, a_L) < 1$ 和 $Q_T(s, a_R) = 1$ 并选择 $a_R$。这个选择在未来的所有访问中都保持稳定。这与问题对 $T$ 的定义相符。\n\n因此，$T$ 的最终表达式为：\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\log_{1/\\gamma}(Q_0) \\right\\rfloor\\right)$$\n这也可以用自然对数写成：\n$$T = 2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)$$", "answer": "$$ \\boxed{2 + \\max\\left(0, \\left\\lfloor \\frac{\\ln(Q_0)}{\\ln(1/\\gamma)} \\right\\rfloor\\right)} $$", "id": "3190816"}, {"introduction": "现实世界的许多应用依赖于从固定的历史数据集中学习，即离线强化学习。本练习是一个重要的警示案例，它揭示了有限数据中的统计偏差如何导致模型过拟合，并做出灾难性的决策。你将分析一个场景，其中从“看似很好”的数据中学到的策略，其真实表现反而更差。这个被称为“策略退化”(policy degradation)的现象，旨在培养你对数据驱动决策的批判性思维，并让你认识到在离线设定中进行可靠评估的极端重要性。", "problem": "考虑一个有限时域的单步马尔可夫决策过程 (MDP)，它有一个非终止状态 $s$ 和两个动作 $a$ 和 $b$。当在状态 $s$ 执行任一动作时，该回合立即终止并产生一个随机奖励。奖励分布如下：\n- 如果执行动作 $a$，奖励 $R_a$ 以 $0.5$ 的概率为 $+1$，以 $0.5$ 的概率为 $-1$。\n- 如果执行动作 $b$，奖励 $R_b$ 确定性地为 $+0.4$。\n\n假设一个用于收集静态数据集的行为策略 $\\mu$ 在状态 $s$ 均匀随机地选择动作，因此数据集中有 $n_a=5$ 个来自动作 $a$ 的样本和 $n_b=5$ 个来自动作 $b$ 的样本。在该数据集的一次特定实现中，所有五个观测到的动作 $a$ 的奖励恰好都是 $+1$，而所有五个观测到的动作 $b$ 的奖励都是 $+0.4$。一个灵活的动作价值函数模型 $\\hat q(s,\\cdot)$ 通过在这些数据上进行经验风险最小化来拟合，并精确地插值了经验均值，在状态 $s$ 上得到 $\\hat q(s,a)=1$ 和 $\\hat q(s,b)=0.4$。\n\n定义一个策略 $\\pi$ 为 $\\pi(s)=b$（确定性的，即在每个回合中，在状态 $s$ 都选择 $b$），并定义一个相对于 $\\hat q$ 的贪心策略 $\\pi'$ 为 $\\pi'(s)=\\arg\\max_{u\\in\\{a,b\\}} \\hat q(s,u)$。设折扣因子 $\\gamma=1$，时域为单个时间步。价值函数 $v^\\pi(s)$ 由标准的固定策略评估表达式 $v^\\pi(s)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t R_t \\mid S_0=s\\right]$ 定义，在这里它简化为策略所引出的动作分布下的期望即时奖励。\n\n从第一性原理出发——即价值函数 $v^\\pi(s)$、动作价值函数 $q^\\pi(s,a)$（其定义为在状态 $s$ 执行动作 $a$ 然后遵循策略 $\\pi$ 的期望回报）以及贪心改进算子的定义——判断在当前设定下，以下哪些陈述是正确的。你的推理应考虑到统计学习效应（对有限数据的过拟合），并应从给定的定义出发，而不是从任何专门的策略改进保证出发。\n\nA. 在此 MDP 中，当前策略 $\\pi$ 下的 $v^\\pi(s)$ 超过了由 $\\hat q$ 引出的贪心策略 $\\pi'$ 下的 $v^{\\pi'}(s)$，因此使用过拟合的 $\\hat q$ 进行贪心改进会严格地降低状态 $s$ 的真实价值。\n\nB. 一个实际的缓解方法是将数据划分为训练集和验证集，在 $\\hat q$ 模型按训练轮次 $t$ 进行训练时计算一系列贪心策略 $\\{\\pi_t\\}$，在验证集上离线策略地估计 $v^{\\pi_t}(s)$，并在验证集上 $v^{\\pi_t}(s)$ 的估计值首次未能改善的最小 $t$ 处停止（交叉验证的提前停止），从而减少因过拟合导致策略退化的可能性。\n\nC. 因为 $\\hat q$ 的经验训练误差随着训练迭代次数单调递减，所以引出的贪心策略序列的真实价值 $v^{\\pi_t}(s)$ 必然单调递增，因此策略退化不会发生。\n\nD. 如果行为策略 $\\mu$ 探索了两个动作，那么在验证集上进行重要性采样总是能得到 $v^{\\pi_t}(s)$ 的低方差、无偏估计，从而使得提前停止变得不必要。\n\n选择所有适用项。", "solution": "首先验证问题陈述，以确保其科学上合理、良定且客观。\n\n### 第 1 步：提取已知条件\n- **MDP 结构**：有限时域、单步马尔可夫决策过程 (MDP)。\n- **状态空间**：单个非终止状态，记为 $s$。\n- **动作空间**：两个动作，$a$ 和 $b$。\n- **转移/终止**：在状态 $s$ 执行任何动作都会导致回合立即终止。\n- **奖励分布（真实）**：\n    - 动作 $a$：奖励 $R_a$ 以 $0.5$ 的概率为 $+1$，以 $0.5$ 的概率为 $-1$。\n    - 动作 $b$：奖励 $R_b$ 确定性地为 $+0.4$。\n- **行为策略**：$\\mu$ 均匀随机地选择动作：$\\mu(a|s) = \\mu(b|s) = 0.5$。\n- **数据集**：在 $\\mu$ 策略下收集的静态数据集，其中动作 $a$ 有 $n_a=5$ 个样本，动作 $b$ 有 $n_b=5$ 个样本。\n- **观测数据实现**：\n    - 对于动作 $a$ 的所有 $5$ 个样本，观测到的奖励均为 $+1$。\n    - 对于动作 $b$ 的所有 $5$ 个样本，观测到的奖励均为 $+0.4$。\n- **动作价值模型**：一个灵活的模型 $\\hat q(s, \\cdot)$ 通过经验风险最小化进行拟合。\n- **拟合模型值**：该模型精确地插值了经验均值，得到 $\\hat q(s,a)=1$ 和 $\\hat q(s,b)=0.4$。\n- **策略定义**：\n    - $\\pi$ 是一个确定性策略：$\\pi(s)=b$。\n    - $\\pi'$ 是一个相对于 $\\hat q$ 的贪心策略：$\\pi'(s)=\\arg\\max_{u\\in\\{a,b\\}} \\hat q(s,u)$。\n- **参数**：折扣因子 $\\gamma=1$，时域 $T=1$。\n- **价值函数定义**：$v^\\pi(s)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{T-1} \\gamma^t R_t \\mid S_0=s\\right]$。由于 $T=1$ 且 $\\gamma=1$，这简化为 $v^\\pi(s) = \\mathbb{E}[R_1 | S_0=s, A_0 \\sim \\pi(\\cdot|s)]$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学上合理**：该问题是强化学习和统计学习领域内一个精心构建的例子。它使用了诸如 MDP、价值函数、策略、经验风险最小化和过拟合等标准概念。有限样本偏离其真实期望的情景是统计学中的一个基本概念；从一枚公平的硬币中连续掷出5次正面的实现具有非零概率（$(0.5)^5 = 0.03125$），而这里提出的情景与此类似。该问题是科学合理的。\n- **良定**：所有必要信息均已提供。MDP、策略和学习结果都得到了精确定义，允许进行无歧义的计算和评估。\n- **客观**：问题以该领域标准的精确、客观和正式的语言陈述。它不包含任何主观声明。\n- **结论**：该问题是有效的。它提出了一个清晰且有启发性的场景，说明了在离线设定中基于过拟合价值估计进行策略改进的危险。\n\n### 第 3 步：推导与选项分析\n我们基于已验证的陈述来解决问题。\n\n**从第一性原理推导**\n\n1.  **真实动作价值函数**：真实动作价值函数 $q^*(s,u)$ 是在状态 $s$ 执行动作 $u$ 的真实期望奖励。\n    -   对于动作 $a$：$q^*(s,a) = \\mathbb{E}[R_a] = (0.5 \\times 1) + (0.5 \\times -1) = 0.5 - 0.5 = 0$。\n    -   对于动作 $b$：$q^*(s,b) = \\mathbb{E}[R_b] = 1 \\times 0.4 = 0.4$。\n\n2.  **真实最优策略**：最优策略 $\\pi^*$ 选择具有最高真实期望回报的动作。由于 $q^*(s,b) = 0.4 > q^*(s,a) = 0$，最优策略是总是选择动作 $b$。\n\n3.  **策略 $\\pi$ 的价值**：策略 $\\pi$ 定义为 $\\pi(s)=b$。其价值 $v^\\pi(s)$ 是遵循该策略时的期望回报。\n    -   $v^\\pi(s) = \\mathbb{E}_{\\pi}[R_1 | S_0=s] = q^*(s, \\pi(s)) = q^*(s,b) = 0.4$。\n\n4.  **拟合的动作价值函数 $\\hat q$**：问题陈述 $\\hat q$ 是通过经验风险最小化拟合的，并插值了经验均值。\n    -   动作 $a$ 的经验均值：基于 $5$ 个观测到的 $+1$ 奖励，均值为 $\\frac{1}{5}\\sum_{i=1}^{5} 1 = 1$。因此，$\\hat q(s,a) = 1$。\n    -   动作 $b$ 的经验均值：基于 $5$ 个观测到的 $+0.4$ 奖励，均值为 $\\frac{1}{5}\\sum_{i=1}^{5} 0.4 = 0.4$。因此，$\\hat q(s,b) = 0.4$。\n    -   注意，$\\hat q(s,a)=1$ 是对真实价值 $q^*(s,a)=0$ 的严重高估，这是由一个不具代表性但可能出现的随机样本引起的。这就是过拟合。\n\n5.  **贪心策略 $\\pi'$**：策略 $\\pi'$ 相对于 $\\hat q$ 是贪心的。\n    -   $\\pi'(s) = \\arg\\max_{u \\in \\{a,b\\}} \\hat q(s,u) = \\arg\\max(1, 0.4) = a$。\n\n6.  **策略 $\\pi'$ 的价值**：价值 $v^{\\pi'}(s)$ 是遵循 $\\pi'$ 时的真实期望回报。\n    -   $v^{\\pi'}(s) = \\mathbb{E}_{\\pi'}[R_1 | S_0=s] = q^*(s, \\pi'(s)) = q^*(s,a) = 0$。\n\n**逐项分析**\n\n**A. 在此 MDP 中，当前策略 $\\pi$ 下的 $v^\\pi(s)$ 超过了由 $\\hat q$ 引出的贪心策略 $\\pi'$ 下的 $v^{\\pi'}(s)$，因此使用过拟合的 $\\hat q$ 进行贪心改进会严格地降低状态 $s$ 的真实价值。**\n\n-   根据我们的推导，我们有 $v^\\pi(s) = 0.4$ 和 $v^{\\pi'}(s) = 0$。\n-   陈述的第一部分，“$v^\\pi(s)$ 超过 $v^{\\pi'}(s)$”，是正确的，因为 $0.4 > 0$。\n-   策略“改进”步骤将策略从 $\\pi$（选择 $b$）变为 $\\pi'$（选择 $a$）。这一改变是基于过拟合的估计 $\\hat q(s,a) = 1 > \\hat q(s,b) = 0.4$。\n-   这一改变的结果是，策略在状态 $s$ 的真实价值从 $v^\\pi(s)=0.4$ 下降到 $v^{\\pi'}(s)=0$。\n-   这种现象，即由近似价值函数建议的策略改变导致真实策略价值下降，被称为策略退化。这里发生这种情况是因为价值函数 $\\hat q$ 对一个有限的、具有误导性的数据集过拟合了。\n-   该陈述准确地描述了这种情况。\n-   **结论：正确。**\n\n**B. 一个实际的缓解方法是将数据划分为训练集和验证集，在 $\\hat q$ 模型按训练轮次 $t$ 进行训练时计算一系列贪心策略 $\\{\\pi_t\\}$，在验证集上离线策略地估计 $v^{\\pi_t}(s)$，并在验证集上 $v^{\\pi_t}(s)$ 的估计值首次未能改善的最小 $t$ 处停止（交叉验证的提前停止），从而减少因过拟合导致策略退化的可能性。**\n\n-   此选项描述了一种应用于离线强化学习的标准机器学习技术。\n-   当模型过分学习训练数据的细节，从而丧失对新的、未见过数据的泛化能力时，就会发生过拟合。在强化学习中，这可能表现为一个不准确的价值函数，从而导致一个糟糕的策略。\n-   所提出的缓解方法是使用一个不用于训练模型 $\\hat q_t$ 的验证集。在每个训练轮次 $t$，引出的贪心策略 $\\pi_t$ 在这个留出数据上进行评估。这种评估必须是离线策略的（例如，使用重要性采样），因为验证数据是用行为策略 $\\mu$ 收集的，而不是用 $\\pi_t$。\n-   通过跟踪验证集上 $\\pi_t$ 的估计价值，可以观察到模型何时开始过拟合。过拟合会导致验证性能停滞或变差，即使训练性能持续改善。\n-   在最佳验证性能点停止（提前停止）是一种成熟的启发式方法，用于找到一个泛化能力好的模型，并在当前情境下，找到一个在真实环境中表现良好的策略。这直接缓解了 A 中所示的策略退化。\n-   **结论：正确。**\n\n**C. 因为 $\\hat q$ 的经验训练误差随着训练迭代次数单调递减，所以引出的贪心策略序列的真实价值 $v^{\\pi_t}(s)$ 必然单调递增，因此策略退化不会发生。**\n\n-   该陈述假定了模型的训练误差与引出策略的真实价值之间存在必然的正相关关系。\n-   问题设置本身就是一个决定性的反例。模型 $\\hat q$ 被拟合为“精确地插值经验均值”，这意味着它在训练数据上已达到可能的最小经验误差（例如，零均方误差）。让这成为训练的最终状态 $\\hat q_T$。\n-   考虑一个假设的训练起点 $\\hat q_0$，其中 $\\hat q_0(s,a) = \\hat q_0(s,b) = 0$。贪心策略 $\\pi_0$ 将会是选择 $b$（假设平局时倾向于 $b$），其真实价值为 $v^{\\pi_0}(s) = 0.4$。\n-   随着训练的进行，$\\hat q_t(s,a)$ 将从 $0$ 增加到 $1$，而 $\\hat q_t(s,b)$ 将从 $0$ 增加到 $0.4$。$\\hat q_t$ 的经验训练误差将单调递减。\n-   一旦 $\\hat q_t(s,a)$ 大于 $\\hat q_t(s,b)$（这将在 $\\hat q_t(s,a) > 0.4$ 时发生），贪心策略 $\\pi_t$ 将从选择 $b$ 翻转为选择 $a$。在这一确切时刻，策略的真实价值 $v^{\\pi_t}(s)$ 从 $0.4$ 降至 $0$。\n-   因此，$\\hat q$ 的单调递减的训练误差可能对应策略真实价值的急剧下降。该陈述是错误的。\n-   **结论：不正确。**\n\n**D. 如果行为策略 $\\mu$ 探索了两个动作，那么在验证集上进行重要性采样总是能得到 $v^{\\pi_t}(s)$ 的低方差、无偏估计，从而使得提前停止变得不必要。**\n\n-   该选项提出了两个强有力但错误的论断。\n-   首先，它声称只要行为策略是探索性的（即，对于目标策略采取的所有动作 $u$，都有 $\\mu(u|s)>0$），重要性采样（IS）“总是能得到低方差”的估计。虽然 IS 在这些条件下提供无偏估计，但其方差是一个重大的实际问题。IS 估计器的方差取决于重要性权重 $\\rho = \\frac{\\pi(A|S)}{\\mu(A|S)}$ 的方差。如果目标策略 $\\pi$ 与行为策略 $\\mu$ 显著不同，这些比率可能会很大，导致估计的方差极高。“总是低方差”的说法是错误的。\n-   其次，它声称一个好的估计器使得“提前停止变得不必要”。这误解了验证和提前停止的作用。提前停止是一种模型选择算法。它*需要*一个泛化性能的估计器才能工作。拥有一个好的估计器（无偏、低方差）是使提前停止有效的原因；它并不能消除对它的需求。如果没有像提前停止这样的选择标准，人们只会简单地在训练数据上将模型训练至完成，这将直接导致问题中描述的过拟合模型 $\\hat q$ 和次优策略 $\\pi'$。估计器是模型选择的工具，而不是它的替代品。\n-   **结论：不正确。**", "answer": "$$\\boxed{AB}$$", "id": "3190845"}]}