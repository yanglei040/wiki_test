{"hands_on_practices": [{"introduction": "本练习将带您深入了解贝叶斯分析的基本工作流程。通过分析一个泊松-伽马共轭模型，您将推导出率参数的后验分布，并利用该后验分布来计算新观测值的后验预测分布。这个练习旨在巩固贝叶斯模型如何从数据中学习并对未来做出可量化预测的核心概念 [@problem_id:3104618]。", "problem": "一项可靠性研究记录了在不同监测时长下元件故障的次数。对于观测 $i \\in \\{1,\\dots,n\\}$，设 $y_{i}$ 是在暴露时长 $E_{i}$（单位为时间）内观测到的计数。假设一个带暴露偏移量的泊松回归：以未知的基准故障率 $\\theta$（每单位暴露时长）为条件，计数独立地满足 $y_{i} \\mid \\theta \\sim \\text{Poisson}(E_{i}\\,\\theta)$。$\\theta$ 的先验分布是形状-率参数化的伽马分布 $\\text{Gamma}(a,b)$，其密度函数为 $p(\\theta) = \\dfrac{b^{a}}{\\Gamma(a)}\\,\\theta^{a-1}\\exp(-b\\,\\theta)$，其中 $\\theta0$。从泊松似然和伽马先验的定义出发，推导在暴露时长 $E_{\\text{new}}$ 下观测到的未来计数 $y_{\\text{new}}$ 的后验预测分布，将预测概率质量函数 $p(y_{\\text{new}}=k \\mid \\text{data})$ 以 $a$、$b$、$\\sum_{i=1}^{n} y_{i}$、$\\sum_{i=1}^{n} E_{i}$ 和 $E_{\\text{new}}$ 的闭式形式表示，其中 $k$ 为任意非负整数。\n\n现在考虑一个数据集，其中有 $n=5$ 次观测，暴露时长为 $(E_{1},E_{2},E_{3},E_{4},E_{5}) = (0.5,\\,1.0,\\,0.75,\\,0.25,\\,1.5)$，计数为 $(y_{1},y_{2},y_{3},y_{4},y_{5}) = (1,\\,2,\\,2,\\,0,\\,1)$。取先验超参数 $a=2$ 和 $b=2$。使用你推导出的后验预测分布，计算当未来暴露时长为 $E_{\\text{new}}=2$ 时，下一次观测到的计数等于 $4$ 的后验预测概率。将你的最终答案表示为小数，并四舍五入到四位有效数字。", "solution": "该问题要求推导泊松-伽马共轭模型的后验预测分布，并进行后续的数值计算。该过程包括三个主要步骤：首先，确定率参数 $\\theta$ 的后验分布；其次，使用此后验分布推导新观测值的后验预测分布；第三，将此结果应用于所提供的具体数据。\n\n设数据表示为 $\\mathcal{D} = \\{(y_i, E_i)\\}_{i=1}^n$。模型规定，在给定率参数 $\\theta$ 的条件下，计数 $y_i$ 是条件独立的，遵循均值为 $E_i\\theta$ 的泊松分布。数据的似然是各个泊松概率质量函数的乘积：\n$$p(\\mathcal{D} \\mid \\theta) = \\prod_{i=1}^{n} p(y_i \\mid \\theta) = \\prod_{i=1}^{n} \\frac{(E_i \\theta)^{y_i} \\exp(-E_i \\theta)}{y_i!}$$\n这个表达式可以通过分离依赖于 $\\theta$ 的项来重写：\n$$p(\\mathcal{D} \\mid \\theta) = \\left( \\prod_{i=1}^{n} \\frac{E_i^{y_i}}{y_i!} \\right) \\theta^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\theta \\sum_{i=1}^{n} E_i\\right)$$\n作为 $\\theta$ 的函数，似然与 $\\theta^{\\sum y_i} \\exp(-\\theta \\sum E_i)$ 成正比。\n\n$\\theta$ 的先验分布是伽马分布，$\\theta \\sim \\text{Gamma}(a,b)$，其概率密度函数为：\n$$p(\\theta) = \\frac{b^a}{\\Gamma(a)} \\theta^{a-1} \\exp(-b\\theta)$$\n先验的核为 $p(\\theta) \\propto \\theta^{a-1} \\exp(-b\\theta)$。\n\n根据贝叶斯定理，给定数据 $\\mathcal{D}$ 时 $\\theta$ 的后验分布与似然和先验的乘积成正比：\n$$p(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) p(\\theta)$$\n$$p(\\theta \\mid \\mathcal{D}) \\propto \\left( \\theta^{\\sum_{i=1}^{n} y_i} \\exp\\left(-\\theta \\sum_{i=1}^{n} E_i\\right) \\right) \\left( \\theta^{a-1} \\exp(-b\\theta) \\right)$$\n合并包含 $\\theta$ 的项：\n$$p(\\theta \\mid \\mathcal{D}) \\propto \\theta^{a + \\sum_{i=1}^{n} y_i - 1} \\exp\\left(-\\left(b + \\sum_{i=1}^{n} E_i\\right)\\theta\\right)$$\n这是伽马分布的核。因此，由于共轭性，后验分布也是一个伽马分布。具体来说，$\\theta \\mid \\mathcal{D} \\sim \\text{Gamma}(a', b')$，其中后验超参数为：\n$$a' = a + \\sum_{i=1}^{n} y_i$$\n$$b' = b + \\sum_{i=1}^{n} E_i$$\n\n接下来，我们推导在给定暴露时长 $E_{\\text{new}}$ 下新观测值 $y_{\\text{new}}$ 的后验预测分布。这个新观测值的似然是 $y_{\\text{new}} \\mid \\theta \\sim \\text{Poisson}(E_{\\text{new}}\\theta)$。后验预测概率质量函数（PMF）是通过将新观测值的似然与 $\\theta$ 的后验分布的乘积对 $\\theta$ 的所有可能值进行边缘化得到的：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\int_0^\\infty p(y_{\\text{new}}=k \\mid \\theta) \\, p(\\theta \\mid \\mathcal{D}) \\, d\\theta$$\n代入 $y_{\\text{new}}$ 的泊松PMF和 $\\theta$ 的伽马后验密度：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\int_0^\\infty \\left( \\frac{(E_{\\text{new}}\\theta)^k \\exp(-E_{\\text{new}}\\theta)}{k!} \\right) \\left( \\frac{(b')^{a'}}{\\Gamma(a')} \\theta^{a'-1} \\exp(-b'\\theta) \\right) d\\theta$$\n我们可以从积分中提出不依赖于 $\\theta$ 的项：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{E_{\\text{new}}^k (b')^{a'}}{k! \\Gamma(a')} \\int_0^\\infty \\theta^{k+a'-1} \\exp(-(E_{\\text{new}}+b')\\theta) d\\theta$$\n该积分是伽马密度 $\\text{Gamma}(k+a', E_{\\text{new}}+b')$ 的核，其值为 $\\frac{\\Gamma(k+a')}{(E_{\\text{new}}+b')^{k+a'}}$。\n将此结果代回表达式中：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{E_{\\text{new}}^k (b')^{a'}}{k! \\Gamma(a')} \\frac{\\Gamma(k+a')}{(E_{\\text{new}}+b')^{k+a'}}$$\n这可以重排为负二项分布的PMF形式：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\frac{\\Gamma(k+a')}{k!\\Gamma(a')} \\frac{(b')^{a'} E_{\\text{new}}^k}{(E_{\\text{new}}+b')^{k+a'}} = \\binom{k+a'-1}{k} \\left(\\frac{b'}{E_{\\text{new}}+b'}\\right)^{a'} \\left(\\frac{E_{\\text{new}}}{E_{\\text{new}}+b'}\\right)^k$$\n代入 $a'$ 和 $b'$ 的表达式，得到最终的闭式形式：\n$$p(y_{\\text{new}}=k \\mid \\mathcal{D}) = \\binom{k + a + \\sum y_i - 1}{k} \\left(\\frac{b + \\sum E_i}{b + \\sum E_i + E_{\\text{new}}}\\right)^{a + \\sum y_i} \\left(\\frac{E_{\\text{new}}}{b + \\sum E_i + E_{\\text{new}}}\\right)^k$$\n\n对于数值计算部分，我们有给定的数据：\n先验超参数：$a=2$, $b=2$。\n暴露时长：$(E_1, \\dots, E_5) = (0.5, 1.0, 0.75, 0.25, 1.5)$。\n计数：$(y_1, \\dots, y_5) = (1, 2, 2, 0, 1)$。\n未来观测参数：$k=4$, $E_{\\text{new}}=2$。\n\n首先，我们从数据中计算总和：\n$$\\sum_{i=1}^5 y_i = 1+2+2+0+1 = 6$$\n$$\\sum_{i=1}^5 E_i = 0.5+1.0+0.75+0.25+1.5 = 4.0$$\n接下来，我们计算后验超参数 $a'$ 和 $b'$：\n$$a' = a + \\sum y_i = 2 + 6 = 8$$\n$$b' = b + \\sum E_i = 2 + 4.0 = 6.0$$\n现在我们使用推导出的PMF来计算 $p(y_{\\text{new}}=4 \\mid \\mathcal{D})$：\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = \\binom{4+8-1}{4} \\left(\\frac{6}{2+6}\\right)^8 \\left(\\frac{2}{2+6}\\right)^4$$\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = \\binom{11}{4} \\left(\\frac{6}{8}\\right)^8 \\left(\\frac{2}{8}\\right)^4 = \\binom{11}{4} \\left(\\frac{3}{4}\\right)^8 \\left(\\frac{1}{4}\\right)^4$$\n我们计算二项式系数：\n$$\\binom{11}{4} = \\frac{11 \\times 10 \\times 9 \\times 8}{4 \\times 3 \\times 2 \\times 1} = 11 \\times 10 \\times 3 = 330$$\n概率为：\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = 330 \\times \\left(\\frac{3}{4}\\right)^8 \\left(\\frac{1}{4}\\right)^4 = 330 \\times \\frac{3^8}{4^8} \\times \\frac{1^4}{4^4} = 330 \\times \\frac{3^8}{4^{12}}$$\n$$3^8 = 6561$$\n$$4^{12} = 16777216$$\n$$p(y_{\\text{new}}=4 \\mid \\mathcal{D}) = 330 \\times \\frac{6561}{16777216} = \\frac{2165130}{16777216} \\approx 0.1290518$$\n四舍五入到四位有效数字，结果是 $0.1291$。", "answer": "$$\\boxed{0.1291}$$", "id": "3104618"}, {"introduction": "先验不仅仅是起点；它们是嵌入假设和控制模型行为的强大工具。本练习 [@problem_id:3104643] 将指导您在一个存在特征共线性的线性回归情境下，比较简单的高斯先验（类似于岭回归）和复杂的“尖峰-厚板”先验。您将发现不同的先验如何解决参数不可识别性问题，并对参数不确定性得出截然不同的结论。", "problem": "考虑贝叶斯统计学习中的线性回归模型，其中设计矩阵固定，噪声方差已知。设 $X \\in \\mathbb{R}^{n \\times d}$ 为设计矩阵，$w \\in \\mathbb{R}^{d}$ 为参数向量，$y \\in \\mathbb{R}^{n}$ 为观测响应。似然函数为 $y \\mid w \\sim \\mathcal{N}\\!\\left(X w, \\sigma^2 I_n\\right)$，其中 $\\sigma^2 > 0$ 是已知的。您将比较在两种先验下 $w$ 的后验方差，并分析在 $X$ 存在共线性时模型的可辨识性。\n\n这两种先验是：\n- 岭式高斯先验：$w \\sim \\mathcal{N}\\!\\left(0, \\tau^2 I_d\\right)$，其中 $\\tau^2 > 0$。\n- 在通过奇异值分解获得的去相关基中定义的尖峰-厚板先验。设奇异值分解为 $X = U S V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{d \\times d}$ 是正交矩阵，$S \\in \\mathbb{R}^{n \\times d}$ 的对角线上为非负奇异值。定义 $z = V^\\top w$。对于与非零奇异值 $s_i$ 相关联的每个坐标 $z_i$，使用先验 $z_i \\sim \\pi \\,\\mathcal{N}\\!\\left(0, v_1\\right) + \\left(1 - \\pi\\right)\\,\\mathcal{N}\\!\\left(0, v_0\\right)$，其中 $0  \\pi  1$，$v_0 \\ll v_1$。对于与 $X$ 的列空间正交的坐标（对应于零奇异值），也使用相同的混合先验。这种混合先验是在变换后的系数空间中对尖峰-厚板先验的一种众所周知的形式化。\n\n您的任务是为下面给出的每个测试用例计算：\n1. 在岭式高斯先验下 $w$ 的后验协方差的迹。\n2. 在尖峰-厚板先验下，使用奇异值分解基和每个奇异分量内的精确一维共轭更新，计算 $w$ 的后验协方差的迹。\n3. 尖峰-厚板后验协方差迹与岭式后验协方差迹的比率。\n4. 矩阵 $X$ 的秩。\n\n您的程序必须实现以下基于原理的计算：\n- 使用贝叶斯法则和线性高斯共轭性来推导岭式后验及其协方差。在岭式先验下的后验协方差是 $\\left(X^\\top X / \\sigma^2 + I_d / \\tau^2\\right)^{-1}$，其迹是其对角线元素之和。\n- 对于尖峰-厚板先验，旋转到奇异值分解基中。设 $X = U S V^\\top$，令 $r$ 为 $X$ 的数值秩（严格正奇异值的数量），令 $U_r \\in \\mathbb{R}^{n \\times r}$ 为 $U$ 的前 $r$ 列，并令 $s_1, \\dots, s_r$ 为正奇异值。定义充分统计量 $y_{\\text{tilde}} = U_r^\\top y \\in \\mathbb{R}^r$。对于每个 $i \\in \\{1, \\dots, r\\}$，似然函数简化为一维观测 $y_{\\text{tilde}, i} \\sim \\mathcal{N}\\!\\left(s_i z_i, \\sigma^2\\right)$。在混合先验 $z_i \\sim \\pi \\,\\mathcal{N}\\!\\left(0, v_1\\right) + \\left(1 - \\pi\\right)\\,\\mathcal{N}\\!\\left(0, v_0\\right)$ 下，后验是高斯混合，其分量特定的后验方差为 $v_{i, j}^{\\text{post}} = \\left(s_i^2 / \\sigma^2 + 1 / v_j\\right)^{-1}$，后验均值为 $m_{i, j}^{\\text{post}} = v_{i, j}^{\\text{post}} \\,(s_i / \\sigma^2)\\, y_{\\text{tilde}, i}$，其中 $j \\in \\{0, 1\\}$ 表示尖峰 ($v_0$) 或厚板 ($v_1$) 分量。后验分量权重为\n$$\n\\gamma_{i, j} = \\frac{\\pi_j \\, \\mathcal{N}\\!\\left(y_{\\text{tilde}, i}; 0, \\sigma^2 + s_i^2 v_j\\right)}{\\sum_{k \\in \\{0, 1\\}} \\pi_k \\, \\mathcal{N}\\!\\left(y_{\\text{tilde}, i}; 0, \\sigma^2 + s_i^2 v_k\\right)},\n$$\n其中 $\\pi_1 = \\pi$，$\\pi_0 = 1 - \\pi$。$z_i$ 的后验方差为\n$$\n\\operatorname{Var}(z_i \\mid y) = \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} \\, v_{i, j}^{\\text{post}} + \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j}\\, \\left(m_{i, j}^{\\text{post}}\\right)^2 - \\left(\\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j}\\, m_{i, j}^{\\text{post}}\\right)^2.\n$$\n对于与列空间正交的 $d - r$ 个坐标（零奇异值），后验等于先验，给出每个坐标的后验方差为 $\\pi v_1 + (1 - \\pi) v_0$。$w$ 的后验协方差的迹等于在奇异值分解基中所有坐标的后验方差之和，因为迹在正交基变换下是不变的。\n\n在您的解决方案中解释可辨识性的含义：在共线性情况下，参数空间中存在数据无法提供信息的方向（零空间方向），每种先验以不同的方式解决非可辨识性问题。\n\n使用以下固定的测试套件。下面给出的每个数字都必须严格按照规定处理。参数 $\\sigma^2$、$\\tau^2$、$\\pi$、$v_0$ 和 $v_1$ 是标量，程序不得引入任何随机性。\n\n测试用例1（精确共线性，中等噪声）：\n$$\nX_1 =\n\\begin{bmatrix}\n1  1  0 \\\\\n2  2  1 \\\\\n3  3  1 \\\\\n4  4  2 \\\\\n5  5  3 \\\\\n6  6  3 \\\\\n7  7  4 \\\\\n8  8  4\n\\end{bmatrix},\\quad\ny_1 =\n\\begin{bmatrix}\n1.0 \\\\\n2.1 \\\\\n3.1 \\\\\n4.2 \\\\\n5.3 \\\\\n6.3 \\\\\n7.4 \\\\\n8.4\n\\end{bmatrix},\\quad\n\\sigma^2_1 = 0.25,\\quad \\tau^2_1 = 1.0,\\quad \\pi_1 = 0.5,\\quad v_{0,1} = 10^{-6},\\quad v_{1,1} = 1.0.\n$$\n\n测试用例2（近似共线性，中等噪声）：\n$$\nX_2 =\n\\begin{bmatrix}\n1  0.999  0 \\\\\n2  2.001  1 \\\\\n3  2.998  1 \\\\\n4  4.002  2 \\\\\n5  4.997  3 \\\\\n6  6.003  3 \\\\\n7  6.996  4 \\\\\n8  8.004  4\n\\end{bmatrix},\\quad\ny_2 =\n\\begin{bmatrix}\n0.9995 \\\\\n2.1005 \\\\\n3.099 \\\\\n4.201 \\\\\n5.2985 \\\\\n6.3015 \\\\\n7.398 \\\\\n8.402\n\\end{bmatrix},\\quad\n\\sigma^2_2 = 0.25,\\quad \\tau^2_2 = 1.0,\\quad \\pi_2 = 0.5,\\quad v_{0,2} = 10^{-6},\\quad v_{1,2} = 1.0.\n$$\n\n测试用例3（满秩，低噪声）：\n$$\nX_3 =\n\\begin{bmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1 \\\\\n2  0 \\\\\n0  2 \\\\\n2  2\n\\end{bmatrix},\\quad\ny_3 =\n\\begin{bmatrix}\n0.6 \\\\\n0.4 \\\\\n1.0 \\\\\n1.2 \\\\\n0.8 \\\\\n2.0\n\\end{bmatrix},\\quad\n\\sigma^2_3 = 0.01,\\quad \\tau^2_3 = 1.0,\\quad \\pi_3 = 0.5,\\quad v_{0,3} = 10^{-6},\\quad v_{1,3} = 1.0.\n$$\n\n测试用例4（精确共线性，高噪声，强尖峰）：\n$$\nX_4 =\n\\begin{bmatrix}\n1  1  0 \\\\\n2  2  1 \\\\\n3  3  1 \\\\\n4  4  2 \\\\\n5  5  3 \\\\\n6  6  3 \\\\\n7  7  4 \\\\\n8  8  4\n\\end{bmatrix},\\quad\ny_4 =\n\\begin{bmatrix}\n1.0 \\\\\n2.1 \\\\\n3.1 \\\\\n4.2 \\\\\n5.3 \\\\\n6.3 \\\\\n7.4 \\\\\n8.4\n\\end{bmatrix},\\quad\n\\sigma^2_4 = 5.0,\\quad \\tau^2_4 = 1.0,\\quad \\pi_4 = 0.1,\\quad v_{0,4} = 10^{-6},\\quad v_{1,4} = 1.0.\n$$\n\n您的程序必须为每个测试用例 $t \\in \\{1, 2, 3, 4\\}$ 计算一个列表 $[\\operatorname{trace}_{\\text{ridge}, t}, \\operatorname{trace}_{\\text{spike-slab}, t}, \\operatorname{ratio}_t, \\operatorname{rank}(X_t)]$，其中 $\\operatorname{ratio}_t = \\operatorname{trace}_{\\text{spike-slab}, t} / \\operatorname{trace}_{\\text{ridge}, t}$，然后将所有四个列表聚合到单行输出中。\n\n最终输出格式：您的程序应生成一行，包含一个由逗号分隔的四个测试用例列表的列表，用方括号括起来，例如 $[\\,[\\cdot, \\cdot, \\cdot, \\cdot], [\\cdot, \\cdot, \\cdot, \\cdot], [\\cdot, \\cdot, \\cdot, \\cdot], [\\cdot, \\cdot, \\cdot, \\cdot]\\,]$。所有数值答案必须是普通的浮点表示或适当的整数。此问题不涉及任何物理单位或角度。[@problem_id:483]", "solution": "该问题要求分析和比较贝叶斯线性回归模型在两种不同参数向量 $w$ 先验分布下的后验方差：岭式高斯先验和尖峰-厚板混合先验。该分析针对几个测试用例进行，包括设计矩阵 $X$ 中存在共线性的情况。\n\n### 模型设定\n线性回归模型由给定设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和参数向量 $w \\in \\mathbb{R}^d$ 的观测响应 $y \\in \\mathbb{R}^n$ 的似然函数定义：\n$$\np(y \\mid X, w, \\sigma^2) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n)\n$$\n其中 $\\sigma^2 > 0$ 是已知的噪声方差。\n\n### 1. 岭式高斯先验\n第一种先验是参数 $w$ 上的零均值各向同性高斯分布：\n$$\np(w \\mid \\tau^2) = \\mathcal{N}(w \\mid 0, \\tau^2 I_d)\n$$\n其中 $\\tau^2 > 0$ 是先验方差。此先验对应于岭回归中使用的正则化项。\n\n由于高斯先验与高斯似然的共轭性，后验分布 $p(w \\mid y, X, \\sigma^2, \\tau^2)$ 也是高斯分布。后验协方差矩阵由以下公式给出：\n$$\n\\Sigma_{\\text{ridge}} = \\left( \\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d \\right)^{-1}\n$$\n需要计算的量是此后验协方差矩阵的迹 $\\operatorname{trace}(\\Sigma_{\\text{ridge}})$，它表示单个参数分量 $w_i$ 的后验方差之和。\n\n### 2. 尖峰-厚板先验\n第二种先验是尖峰-厚板先验，它是一种由两个高斯分布组成的混合分布：一个方差非常小（“尖峰”），另一个方差较大（“厚板”）。此先验是在通过设计矩阵 $X = U S V^\\top$ 的奇异值分解（SVD）获得的去相关基中定义的。\n\n参数向量 $w$ 被变换到一个新基 $z = V^\\top w$。由于 $V$ 是一个正交矩阵，这是一个旋转变换。先验是在 $z$ 的分量上指定的：\n$$\np(z_i) = \\pi \\mathcal{N}(z_i \\mid 0, v_1) + (1-\\pi) \\mathcal{N}(z_i \\mid 0, v_0)\n$$\n对于每个 $i \\in \\{1, \\dots, d\\}$，其中 $v_0$ 是小的尖峰方差，$v_1$ 是大的厚板方差，$\\pi$ 是属于厚板分量的混合概率。\n\n在变换后的基中，似然模型解耦。令 $\\tilde{y} = U^\\top y$。每个分量 $\\tilde{y}_i$ 的似然变为：\n$$\np(\\tilde{y}_i \\mid z_i, s_i, \\sigma^2) = \\mathcal{N}(\\tilde{y}_i \\mid s_i z_i, \\sigma^2)\n$$\n其中 $s_i$ 是 $X$ 的第 $i$ 个奇异值。\n\n对于每个 $z_i$，我们有一个一维贝叶斯推断问题。给定 $\\tilde{y}_i$ 的 $z_i$ 的后验分布是两个高斯分布的混合。\n$z_i$ 来自于分量 $j \\in \\{0, 1\\}$（其中 $j=0$ 表示尖峰，$j=1$ 表示厚板）的后验概率（责任）由贝叶斯法则给出：\n$$\n\\gamma_{i, j} = \\frac{\\pi_j p(\\tilde{y}_i \\mid \\text{component } j)}{\\sum_{k \\in \\{0, 1\\}} \\pi_k p(\\tilde{y}_i \\mid \\text{component } k)}\n$$\n其中 $\\pi_1 = \\pi$，$\\pi_0 = 1 - \\pi$，而 $p(\\tilde{y}_i \\mid \\text{component } j)$ 是边缘似然 $\\int p(\\tilde{y}_i \\mid z_i) p(z_i \\mid \\text{component } j) dz_i = \\mathcal{N}(\\tilde{y}_i \\mid 0, \\sigma^2 + s_i^2 v_j)$。\n\n$z_i$ 的后验为 $p(z_i | y) = \\sum_{j \\in \\{0, 1\\}} \\gamma_{i,j} \\mathcal{N}(z_i | m_{i,j}^{\\text{post}}, v_{i,j}^{\\text{post}})$，其中分量的后验均值和方差为：\n$$\nv_{i, j}^{\\text{post}} = \\left(\\frac{s_i^2}{\\sigma^2} + \\frac{1}{v_j}\\right)^{-1}\n$$\n$$\nm_{i, j}^{\\text{post}} = v_{i, j}^{\\text{post}} \\left(\\frac{s_i}{\\sigma^2}\\right) \\tilde{y}_i\n$$\n$z_i$ 的总后验方差使用全方差公式求得：\n$$\n\\operatorname{Var}(z_i \\mid y) = \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} v_{i, j}^{\\text{post}} + \\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} \\left(m_{i, j}^{\\text{post}}\\right)^2 - \\left(\\sum_{j \\in \\{0, 1\\}} \\gamma_{i, j} m_{i, j}^{\\text{post}}\\right)^2\n$$\n$w$ 的后验协方差的总迹是 $z_i$ 各分量后验方差之和，这是因为迹在正交变换（$w=Vz$）下具有不变性：\n$$\n\\operatorname{trace}(\\operatorname{Cov}(w \\mid y)) = \\operatorname{trace}(\\operatorname{Cov}(Vz \\mid y)) = \\operatorname{trace}(V \\operatorname{Cov}(z \\mid y) V^\\top) = \\operatorname{trace}(\\operatorname{Cov}(z \\mid y)) = \\sum_{i=1}^d \\operatorname{Var}(z_i \\mid y)\n$$\n因为 $z_i$ 的后验是独立的。\n\n### 3. 可辨识性与共线性\n$X$ 中的共线性意味着某些列是线性相关的，因此 $X$ 不是满秩的。这意味着 $X^\\top X$ 是奇异的，并且有一个非平凡的零空间。对于 $X$ 零空间中的任何向量 $w_0$，$Xw = X(w+w_0)$，这意味着似然函数在零空间定义的方向上是平坦的。仅从数据来看，参数是不可辨识的。\n\n- **岭先验**：此先验通过向半定精度矩阵 $X^\\top X/\\sigma^2$ 添加一个正定矩阵 $I_d/\\tau^2$ 来解决非可辨识性问题。得到的后验精度矩阵总是可逆的。这等同于在频率派统计学中添加一个惩罚项，它将所有参数朝先验均值（零）收缩。对于 $X$ 零空间中的方向，$w$ 的后验就是其先验，方差为 $\\tau^2$。\n\n- **尖峰-厚板先验**：此先验在 SVD 基中处理非可辨识性问题。如果一个奇异值 $s_i$ 为零，则参数空间中相应的方向不受数据影响（$\\tilde{y}_i$ 的似然不依赖于 $z_i$）。因此，$z_i$ 的后验与其先验相同。该分量的后验方差是混合分布的先验方差，即 $\\operatorname{Var}(z_i) = \\pi v_1 + (1-\\pi)v_0$。因此，模型隔离了不可辨识的分量，并对它们依赖于先验，同时使用数据来更新可辨识的分量。对于近似共线性的情况（$s_i$ 非常小但不为零），数据提供的信息很弱。尖峰-厚板先验可以通过为“尖峰”分量分配高的后验概率来适应这种情况，从而有效地将参数 $z_i$ 收缩到零，并承认它没有被数据很好地确定。这提供了比岭先验的均匀收缩更细致的收缩方法。\n\n以下 Python 代码为提供的测试用例实现了这些计算。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian linear regression problem for four test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        (\n            np.array([\n                [1, 1, 0], [2, 2, 1], [3, 3, 1], [4, 4, 2],\n                [5, 5, 3], [6, 6, 3], [7, 7, 4], [8, 8, 4]\n            ]),\n            np.array([1.0, 2.1, 3.1, 4.2, 5.3, 6.3, 7.4, 8.4]),\n            0.25, 1.0, 0.5, 1e-6, 1.0\n        ),\n        # Test case 2\n        (\n            np.array([\n                [1, 0.999, 0], [2, 2.001, 1], [3, 2.998, 1], [4, 4.002, 2],\n                [5, 4.997, 3], [6, 6.003, 3], [7, 6.996, 4], [8, 8.004, 4]\n            ]),\n            np.array([0.9995, 2.1005, 3.099, 4.201, 5.2985, 6.3015, 7.398, 8.402]),\n            0.25, 1.0, 0.5, 1e-6, 1.0\n        ),\n        # Test case 3\n        (\n            np.array([\n                [1, 0], [0, 1], [1, 1], [2, 0], [0, 2], [2, 2]\n            ]),\n            np.array([0.6, 0.4, 1.0, 1.2, 0.8, 2.0]),\n            0.01, 1.0, 0.5, 1e-6, 1.0\n        ),\n        # Test case 4\n        (\n            np.array([\n                [1, 1, 0], [2, 2, 1], [3, 3, 1], [4, 4, 2],\n                [5, 5, 3], [6, 6, 3], [7, 7, 4], [8, 8, 4]\n            ]),\n            np.array([1.0, 2.1, 3.1, 4.2, 5.3, 6.3, 7.4, 8.4]),\n            5.0, 1.0, 0.1, 1e-6, 1.0\n        )\n    ]\n\n    all_results = []\n    for case in test_cases:\n        X, y, sigma2, tau2, pi, v0, v1 = case\n        n, d = X.shape\n\n        # 1. Ridge posterior covariance trace\n        Id = np.identity(d)\n        precision_ridge = (X.T @ X) / sigma2 + Id / tau2\n        cov_ridge = np.linalg.inv(precision_ridge)\n        trace_ridge = np.trace(cov_ridge)\n\n        # 4. Matrix rank\n        rank_X = np.linalg.matrix_rank(X)\n\n        # 2. Spike-and-slab posterior covariance trace\n        _U, s_vals, Vt = np.linalg.svd(X, full_matrices=True)\n        U = _U[:, :d] # Ensure U is n x d\n        \n        y_tilde = U.T @ y\n\n        trace_ss = 0.0\n\n        # Components corresponding to non-zero singular values\n        for i in range(rank_X):\n            s_i = s_vals[i]\n            y_tilde_i = y_tilde[i]\n\n            # Use log-likelihoods for numerical stability\n            log_marginal_lik_0 = np.log(1 - pi) + norm.logpdf(y_tilde_i, loc=0, scale=np.sqrt(sigma2 + s_i**2 * v0))\n            log_marginal_lik_1 = np.log(pi) + norm.logpdf(y_tilde_i, loc=0, scale=np.sqrt(sigma2 + s_i**2 * v1))\n\n            # Log-sum-exp trick to get posterior weights (gammas)\n            if log_marginal_lik_0 > log_marginal_lik_1:\n                gamma0 = 1.0 / (1.0 + np.exp(log_marginal_lik_1 - log_marginal_lik_0))\n                gamma1 = 1.0 - gamma0\n            else:\n                gamma1 = 1.0 / (1.0 + np.exp(log_marginal_lik_0 - log_marginal_lik_1))\n                gamma0 = 1.0 - gamma1\n\n            # Component-specific posterior parameters\n            v_post0 = 1.0 / (s_i**2 / sigma2 + 1.0 / v0)\n            v_post1 = 1.0 / (s_i**2 / sigma2 + 1.0 / v1)\n\n            m_post0 = v_post0 * (s_i / sigma2) * y_tilde_i\n            m_post1 = v_post1 * (s_i / sigma2) * y_tilde_i\n\n            # Posterior variance for z_i using law of total variance\n            var_z_i = (gamma0 * v_post0 + gamma1 * v_post1) + \\\n                      (gamma0 * m_post0**2 + gamma1 * m_post1**2) - \\\n                      (gamma0 * m_post0 + gamma1 * m_post1)**2\n            \n            trace_ss += var_z_i\n\n        # Components corresponding to zero singular values\n        num_zero_sv = d - rank_X\n        if num_zero_sv > 0:\n            var_z_prior = pi * v1 + (1 - pi) * v0\n            trace_ss += num_zero_sv * var_z_prior\n        \n        # 3. Ratio\n        ratio = trace_ss / trace_ridge\n\n        all_results.append([trace_ridge, trace_ss, ratio, float(rank_X)])\n\n    # Format output as specified\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3104643"}, {"introduction": "贝叶斯框架的一个关键优势是它能够清晰地整合各种信息来源。这个动手实践问题 [@problem_id:3104582] 将挑战您构建一个半监督分类器，该分类器能同时从有标签和无标签数据中学习。通过实现一种混合模型方法，您将亲眼见证无标签数据如何能够优化模型对底层数据分布的理解，从而提升其预测性能。", "problem": "您的任务是为一个一维特征构建并比较两种二元贝叶斯分类器，需要采用一种半监督方法，通过混合模型利用未标记数据。该学习任务必须从适用于统计学习和贝叶斯学习理论的基本原理出发进行推导。您必须使用的基本工具包括贝叶斯定理、Beta-伯努利共轭先验和高斯似然。不应假设任何用于目标量的快捷公式；所有量都必须通过基于这些基本原理的推导获得。\n\n请考虑以下模型和数据描述。特征是一个实值随机变量 $x \\in \\mathbb{R}$，类别标签为 $y \\in \\{0,1\\}$。类条件似然是具有已知均值和共有已知标准差的高斯分布：\n$$\np(x \\mid y=0) = \\mathcal{N}(x \\mid \\mu_0, \\sigma^2), \\quad\np(x \\mid y=1) = \\mathcal{N}(x \\mid \\mu_1, \\sigma^2),\n$$\n其中 $\\mu_0 \\in \\mathbb{R}$，$\\mu_1 \\in \\mathbb{R}$ 和 $\\sigma \\in \\mathbb{R}_{0}$ 是给定的。类别概率 $\\pi = p(y=1)$ 的先验是 $\\operatorname{Beta}(\\alpha_0, \\beta_0)$ 分布，其超参数为 $\\alpha_0 \\in \\mathbb{R}_{0}$ 和 $\\beta_0 \\in \\mathbb{R}_{0}$。为您提供了类别 $y=1$ 和 $y=0$ 的已标记计数 $n_1$ 和 $n_0$，以及一个包含未知标签的特征值的未标记集 $U = \\{u_1, \\dots, u_m\\}$。此外，您还获得了一个测试特征值集 $T = \\{t_1, \\dots, t_k\\}$，您必须报告在这两种分类器下，这些测试点的后验类别概率。\n\n任务：\n1. 构建纯监督贝叶斯分类器：将Beta先验与已标记计数相结合，形成 $\\pi$ 的后验分布；然后使用贝叶斯定理为每个 $x \\in T$ 计算 $p(y=1 \\mid x)$。在此分类器中，使用 $\\pi$ 的后验均值作为点估计。\n2. 构建半监督贝叶斯分类器：在混合模型解释下，使用未标记集 $U$ 对先验执行一次软分配更新。具体步骤为：从纯监督分类器对 $\\pi$ 的当前估计开始，为每个 $u \\in U$ 计算贝叶斯责任（Bayes-responsibility）；将这些责任解释为期望类别计数；将这些期望计数作为额外的伪观测值整合到Beta后验中；然后重新计算 $\\pi$ 的后验均值。使用这个更新后的 $\\pi$ 为每个 $x \\in T$ 计算 $p(y=1 \\mid x)$。\n3. 对于每个 $t \\in T$，计算后验偏移，其定义为 $\\Delta(t) = p_{\\text{semi}}(y=1 \\mid t) - p_{\\text{sup}}(y=1 \\mid t)$，其中 $p_{\\text{semi}}(y=1 \\mid t)$ 是半监督后验概率，$p_{\\text{sup}}(y=1 \\mid t)$ 是纯监督后验概率。\n\n用于覆盖不同情况的测试套件参数：\n- 情况1（理想情况，未标记数据提供中等信息量）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (-2.0, 2.0, 1.0, 1.0, 1.0, 5, 3)$，$U = \\{-2.5, -1.5, 1.8, 2.2, 0.0\\}$，$T = \\{-1.0, 0.0, 2.5\\}$。\n- 情况2（边界情况，无未标记数据）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (-1.0, 1.0, 1.0, 2.0, 2.0, 4, 4)$，$U = \\{\\}$，$T = \\{-2.0, 2.0\\}$。\n- 情况3（边缘情况，未标记数据集中在一个类别附近）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (0.0, 3.0, 0.5, 2.0, 2.0, 2, 2)$，$U = \\{2.8, 3.1, 3.3, 2.9\\}$，$T = \\{2.0, 3.0\\}$。\n- 情况4（边界情况，类条件密度无法区分）：$(\\mu_0, \\mu_1, \\sigma, \\alpha_0, \\beta_0, n_1, n_0) = (0.0, 0.0, 1.0, 1.0, 9.0, 1, 9)$，$U = \\{0.1, -0.1, 0.0\\}$，$T = \\{-1.0, 1.0\\}$。\n\n您的程序必须为每种情况实现上述过程，并计算每种情况下后验偏移的列表 $\\{\\Delta(t) : t \\in T\\}$。所有答案都应表示为小数（浮点数）。您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表。列表中的每个元素对应一种情况，其本身是该情况下后验偏移的列表，顺序与 $T$ 中的元素顺序相同。例如，包含四种情况的输出格式如下所示：$[[\\Delta_{1,1}, \\Delta_{1,2}, \\dots],[\\Delta_{2,1}, \\Delta_{2,2}, \\dots],[\\Delta_{3,1}, \\Delta_{3,2}, \\dots],[\\Delta_{4,1}, \\Delta_{4,2}, \\dots]]$。", "solution": "该问题要求为一个一维特征 $x \\in \\mathbb{R}$ 构建并比较一个纯监督贝叶斯分类器和一个半监督贝叶斯分类器。我们将按照要求，从基本原理出发推导所有需要的量。\n\n模型定义如下：\n- 类别标签 $y \\in \\{0, 1\\}$。\n- 类别先验概率 $\\pi = p(y=1)$。此参数的先验是一个Beta分布：$p(\\pi) = \\operatorname{Beta}(\\pi \\mid \\alpha_0, \\beta_0)$。\n- 类条件似然，它们是共享方差的高斯分布：\n$$ p(x \\mid y=0) = \\mathcal{N}(x \\mid \\mu_0, \\sigma^2) $$\n$$ p(x \\mid y=1) = \\mathcal{N}(x \\mid \\mu_1, \\sigma^2) $$\n高斯概率密度函数（PDF）由 $\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$ 给出。为简洁起见，我们将类别1的似然表示为 $L_1(x) = p(x \\mid y=1)$，类别0的似然表示为 $L_0(x) = p(x \\mid y=0)$。\n\n贝叶斯分类的核心是计算给定特征值下一个类别的后验概率 $p(y=1 \\mid x)$。使用贝叶斯定理：\n$$ p(y=1 \\mid x) = \\frac{p(x \\mid y=1)p(y=1)}{p(x)} = \\frac{p(x \\mid y=1)p(y=1)}{p(x \\mid y=1)p(y=1) + p(x \\mid y=0)p(y=0)} $$\n代入 $\\pi = p(y=1)$ 和 $1-\\pi = p(y=0)$，我们得到：\n$$ p(y=1 \\mid x, \\pi) = \\frac{L_1(x) \\pi}{L_1(x) \\pi + L_0(x)(1 - \\pi)} $$\n两种分类器的不同之处在于它们估计 $\\pi$ 的方式。\n\n### 1. 纯监督分类器\n\n对于纯监督分类器，我们使用已标记数据计数（类别 $y=1$ 的计数为 $n_1$，类别 $y=0$ 的计数为 $n_0$）来更新 $\\pi$ 的先验。\n\n$\\pi$ 的先验是 $p(\\pi) = \\operatorname{Beta}(\\pi \\mid \\alpha_0, \\beta_0)$，其形式为：\n$$ p(\\pi) \\propto \\pi^{\\alpha_0 - 1} (1-\\pi)^{\\beta_0 - 1} $$\n在 $n_1+n_0$ 次试验中观测到 $n_1$ 次成功（类别1）和 $n_0$ 次失败（类别0）的似然由伯努利似然函数给出：\n$$ p(D_L \\mid \\pi) \\propto \\pi^{n_1} (1-\\pi)^{n_0} $$\n其中 $D_L$ 表示已标记数据。\n\n$\\pi$ 的后验分布通过应用贝叶斯法则得到：\n$$ p(\\pi \\mid D_L) \\propto p(D_L \\mid \\pi) p(\\pi) \\propto \\left( \\pi^{n_1} (1-\\pi)^{n_0} \\right) \\left( \\pi^{\\alpha_0 - 1} (1-\\pi)^{\\beta_0 - 1} \\right) = \\pi^{\\alpha_0 + n_1 - 1} (1-\\pi)^{\\beta_0 + n_0 - 1} $$\n这是一个Beta分布的核，是Beta先验和伯努利似然之间共轭性的结果。后验分布为：\n$$ p(\\pi \\mid D_L) = \\operatorname{Beta}(\\pi \\mid \\alpha_0 + n_1, \\beta_0 + n_0) $$\n令后验超参数为 $\\alpha_{\\text{sup}} = \\alpha_0 + n_1$ 和 $\\beta_{\\text{sup}} = \\beta_0 + n_0$。\n\n问题指定使用 $\\pi$ 的后验均值作为点估计。一个 $\\operatorname{Beta}(\\alpha, \\beta)$ 分布的均值是 $\\frac{\\alpha}{\\alpha + \\beta}$。因此，$\\pi$ 的监督估计为：\n$$ \\hat{\\pi}_{\\text{sup}} = E[\\pi \\mid D_L] = \\frac{\\alpha_{\\text{sup}}}{\\alpha_{\\text{sup}} + \\beta_{\\text{sup}}} = \\frac{\\alpha_0 + n_1}{\\alpha_0 + n_1 + \\beta_0 + n_0} $$\n那么，对于测试点 $t \\in T$ 的监督后验类别概率是：\n$$ p_{\\text{sup}}(y=1 \\mid t) = \\frac{L_1(t) \\hat{\\pi}_{\\text{sup}}}{L_1(t) \\hat{\\pi}_{\\text{sup}} + L_0(t)(1 - \\hat{\\pi}_{\\text{sup}})} $$\n\n### 2. 半监督分类器\n\n半监督方法通过一次软分配更新来整合未标记数据集 $U = \\{u_1, \\dots, u_m\\}$，这在贝叶斯框架下等同于期望最大化（EM）式算法的一次迭代。\n\n**E-步（软分配）：** 我们首先为每个未标记数据点 $u_i \\in U$ 计算类别1的“责任”（responsibility）。责任 $r_i$ 是在给定类别先验的当前估计 $\\hat{\\pi}_{\\text{sup}}$ 的条件下，点 $u_i$ 属于类别1的后验概率。\n$$ r_i = p(y=1 \\mid u_i, \\hat{\\pi}_{\\text{sup}}) = \\frac{L_1(u_i) \\hat{\\pi}_{\\text{sup}}}{L_1(u_i) \\hat{\\pi}_{\\text{sup}} + L_0(u_i)(1 - \\hat{\\pi}_{\\text{sup}})} $$\n这些责任被解释为期望计数。来自未标记数据的类别1的总期望计数（或伪计数）是 $N_{1,U} = \\sum_{i=1}^m r_i$。类似地，类别0的总期望计数是 $N_{0,U} = \\sum_{i=1}^m (1 - r_i) = m - N_{1,U}$。\n\n**M-步（参数更新）：** 我们通过将这些伪计数与原始的已标记计数一起整合，来更新 $\\pi$ 的后验分布。这意味着将伪计数加到监督后验的超参数上：\n$$ \\alpha_{\\text{semi}} = \\alpha_{\\text{sup}} + N_{1,U} = \\alpha_0 + n_1 + N_{1,U} $$\n$$ \\beta_{\\text{semi}} = \\beta_{\\text{sup}} + N_{0,U} = \\beta_0 + n_0 + m - N_{1,U} $$\n$\\pi$ 的新后验分布是 $p(\\pi \\mid D_L, D_U) = \\operatorname{Beta}(\\pi \\mid \\alpha_{\\text{semi}}, \\beta_{\\text{semi}})$。\n\n更新后的 $\\pi$ 的点估计是这个新后验的均值：\n$$ \\hat{\\pi}_{\\text{semi}} = E[\\pi \\mid D_L, D_U] = \\frac{\\alpha_{\\text{semi}}}{\\alpha_{\\text{semi}} + \\beta_{\\text{semi}}} = \\frac{\\alpha_0 + n_1 + N_{1,U}}{\\alpha_0 + n_1 + \\beta_0 + n_0 + m} $$\n对于测试点 $t \\in T$ 的半监督后验类别概率是：\n$$ p_{\\text{semi}}(y=1 \\mid t) = \\frac{L_1(t) \\hat{\\pi}_{\\text{semi}}}{L_1(t) \\hat{\\pi}_{\\text{semi}} + L_0(t)(1 - \\hat{\\pi}_{\\text{semi}})} $$\n\n### 3. 后验偏移\n\n最后，对于每个测试点 $t \\in T$，后验偏移 $\\Delta(t)$ 定义为半监督和纯监督的类别1后验概率之差：\n$$ \\Delta(t) = p_{\\text{semi}}(y=1 \\mid t) - p_{\\text{sup}}(y=1 \\mid t) $$\n该量衡量了整合未标记数据 $U$ 对点 $t$ 处的分类决策的影响。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It constructs and compares supervised and semi-supervised Bayesian classifiers.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'params': {'mu0': -2.0, 'mu1': 2.0, 'sigma': 1.0, 'alpha0': 1.0, 'beta0': 1.0, 'n1': 5, 'n0': 3},\n            'U': np.array([-2.5, -1.5, 1.8, 2.2, 0.0]),\n            'T': np.array([-1.0, 0.0, 2.5])\n        },\n        {\n            'params': {'mu0': -1.0, 'mu1': 1.0, 'sigma': 1.0, 'alpha0': 2.0, 'beta0': 2.0, 'n1': 4, 'n0': 4},\n            'U': np.array([]),\n            'T': np.array([-2.0, 2.0])\n        },\n        {\n            'params': {'mu0': 0.0, 'mu1': 3.0, 'sigma': 0.5, 'alpha0': 2.0, 'beta0': 2.0, 'n1': 2, 'n0': 2},\n            'U': np.array([2.8, 3.1, 3.3, 2.9]),\n            'T': np.array([2.0, 3.0])\n        },\n        {\n            'params': {'mu0': 0.0, 'mu1': 0.0, 'sigma': 1.0, 'alpha0': 1.0, 'beta0': 9.0, 'n1': 1, 'n0': 9},\n            'U': np.array([0.1, -0.1, 0.0]),\n            'T': np.array([-1.0, 1.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        shifts = solve_case(**case)\n        results.append(shifts)\n    \n    # Print the final result in the exact required format.\n    # The default str(list) in Python provides the correct spacing.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_case(params, U, T):\n    \"\"\"\n    Solves a single case, calculating the posterior shifts.\n    \n    Args:\n        params (dict): Dictionary of model parameters.\n        U (np.ndarray): Array of unlabeled feature values.\n        T (np.ndarray): Array of test feature values.\n        \n    Returns:\n        list: A list of posterior shifts delta(t) for each t in T.\n    \"\"\"\n    mu0, mu1, sigma = params['mu0'], params['mu1'], params['sigma']\n    alpha0, beta0 = params['alpha0'], params['beta0']\n    n1, n0 = params['n1'], params['n0']\n\n    def posterior_y_given_x(x, pi_hat):\n        \"\"\"Calculates p(y=1 | x) for a given pi_hat.\"\"\"\n        # Check for the edge case where likelihoods are identical\n        if mu0 == mu1:\n            return np.full_like(x, pi_hat, dtype=float)\n\n        l1 = norm.pdf(x, loc=mu1, scale=sigma)\n        l0 = norm.pdf(x, loc=mu0, scale=sigma)\n        \n        numerator = l1 * pi_hat\n        denominator = numerator + l0 * (1.0 - pi_hat)\n        \n        # Handle potential division by zero if both likelihoods and pi are zero/one,\n        # although unlikely with the given constraints.\n        # If denominator is 0, it implies l1 and l0 are both 0.\n        # The posterior is undefined, but we can return 0.5 as a neutral guess.\n        # With Gaussian likelihoods this is practically impossible.\n        # Let's set the posterior to pi_hat if likelihoods are equal, for numerical stability.\n        posterior = np.full_like(denominator, pi_hat, dtype=float)\n        # Avoid division by zero when both likelihoods are zero\n        # which can happen far from means, though probabilities would be ~0.\n        valid_den = denominator > 0\n        posterior[valid_den] = numerator[valid_den] / denominator[valid_den]\n        \n        return posterior\n\n    # 1. Supervised-only classifier\n    alpha_sup = alpha0 + n1\n    beta_sup = beta0 + n0\n    pi_sup_hat = alpha_sup / (alpha_sup + beta_sup)\n    p_sup_at_T = posterior_y_given_x(T, pi_sup_hat)\n\n    # 2. Semi-supervised classifier\n    # E-step: Compute responsibilities for unlabeled data\n    m = len(U)\n    N_1_U = 0.0\n    if m > 0:\n        responsibilities = posterior_y_given_x(U, pi_sup_hat)\n        N_1_U = np.sum(responsibilities)\n    \n    # M-step: Update pi estimate\n    alpha_semi = alpha_sup + N_1_U\n    #\n    # The total number of pseudo-observations is m.\n    # N_0_U = m - N_1_U\n    # beta_semi = beta_sup + N_0_U\n    # Combining these steps into one to avoid floating point issues with N_0_U:\n    beta_semi = beta_sup + (m - N_1_U)\n\n    pi_semi_hat = alpha_semi / (alpha_semi + beta_semi)\n    p_semi_at_T = posterior_y_given_x(T, pi_semi_hat)\n    \n    # 3. Compute posterior shifts\n    shifts = p_semi_at_T - p_sup_at_T\n    \n    return shifts.tolist()\n\nsolve()\n\n```", "id": "3104582"}]}