{"hands_on_practices": [{"introduction": "高斯过程回归模型假定数据由一个单一的潜在函数和观测噪声共同生成。这个练习通过一个精心设计的、包含冲突数据点的场景，揭示了观测噪声方差 $\\sigma^2$ 的关键作用。通过分析后验预测如何随 $\\sigma^2$ 变化，你将深刻且直观地理解模型如何调和相互矛盾的证据，以及噪声参数的真正含义 [@problem_id:3122972]。", "problem": "考虑一个一维高斯过程（GP）回归问题，其先验为零均值，协方差函数为平方指数（也称为径向基函数）函数。该先验由以下要素指定：\n- 一个潜函数 $f$ 被建模为高斯过程（GP）：$f \\sim \\mathcal{GP}(0, k)$。\n- 一个协方差函数 $k(x, x') = \\sigma_f^2 \\exp\\!\\left(-\\tfrac{1}{2} \\left(\\tfrac{x - x'}{\\ell}\\right)^2\\right)$，其中超参数 $\\sigma_f^2$ 和 $\\ell$ 是固定的。\n- 带噪声的观测值 $y_i = f(x_i) + \\varepsilon_i$，其中噪声变量 $\\varepsilon_i$ 独立同分布于 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n\n给定一个包含重复输入和相互矛盾输出的数据集：\n- 输入 $X = [x_1, x_2, x_3]^\\top = [0.0, 0.0, 1.0]^\\top$。\n- 输出 $y = [y_1, y_2, y_3]^\\top = [1.0, -1.0, 0.0]^\\top$。\n\n使用平方指数协方差函数，其固定超参数为 $\\sigma_f^2 = 1.0$ 和 $\\ell = 0.6$。假设观测噪声为独立的、方差为 $\\sigma^2$ 的高斯噪声。对于一个测试输入 $x_\\star = 0.0$，您的任务是：\n1. 从训练输出和在 $x_\\star$ 处的潜函数值的联合高斯分布出发，仅使用多元正态分布的核心性质和给定的模型假设。\n2. 推导在观测数据条件下，潜函数值 $f_\\star = f(x_\\star)$ 的后验预测分布，用 $X$, $y$, $x_\\star$, $\\sigma_f^2$, $\\ell$ 和 $\\sigma^2$ 表示。\n3. 通过对相关协方差矩阵进行 Cholesky 分解，为 $f_\\star$ 的后验预测均值 $m_\\star$ 和方差 $v_\\star$ 实现一个数值稳定的计算。\n\n重点关注高斯过程后验如何处理具有矛盾观测值的重复输入，以及观测噪声方差 $\\sigma^2$ 如何解决相互矛盾的 $y$ 值之间的张力。\n\n测试套件：\n- 使用三个噪声方差值 $\\sigma^2 \\in \\{10^{-6}, 0.1, 10.0\\}$。\n- 对于上述集合中的每个 $\\sigma^2$，计算在 $x_\\star = 0.0$ 处潜函数的后验预测均值 $m_\\star$ 和方差 $v_\\star$。\n\n您的程序必须输出一行，其中包含一个逗号分隔的扁平列表，结果按以下顺序排列：\n- $[m_\\star(\\sigma^2{=}10^{-6}), v_\\star(\\sigma^2{=}10^{-6}), m_\\star(\\sigma^2{=}0.1), v_\\star(\\sigma^2{=}0.1), m_\\star(\\sigma^2{=}10.0), v_\\star(\\sigma^2{=}10.0)]$。\n\n注意事项：\n- 所有量均为纯数学量，不涉及物理单位。\n- 不涉及角度，因此不需要角度单位。\n- 输出必须是实数（浮点值）。\n- 实现应数值稳定，并直接使用所述模型计算所需量，不依赖任何外部数据或用户输入。", "solution": "该问题要求，在给定一个包含重复输入和矛盾输出的特定数据集的情况下，计算一维高斯过程（GP）回归模型中潜函数的后验预测均值和方差。我们将首先推导后验预测分布的一般形式，然后针对给定的测试用例，使用 Cholesky 分解实现一个数值稳定的计算来解决此问题。\n\n### 步骤 1：模型设定与联合分布\n\n该模型由潜函数 $f(x)$ 上的零均值 GP 先验和观测值的高斯似然定义。\n先验为 $f \\sim \\mathcal{GP}(0, k(x, x'))$，其中协方差函数是平方指数（或 RBF）核函数：\n$$k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - x'}{\\ell}\\right)^2\\right)$$\n观测值 $y_i$ 通过一个加性的、独立同分布的高斯噪声模型与潜函数值 $f(x_i)$ 相关联：\n$$y_i = f(x_i) + \\varepsilon_i, \\quad \\text{其中 } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n\n给定一组 $N$ 个训练输入 $X = [x_1, \\dots, x_N]^\\top$ 和相应的输出 $\\mathbf{y} = [y_1, \\dots, y_N]^\\top$，我们感兴趣的是在一个新的测试点 $x_\\star$ 处预测潜函数值 $f_\\star = f(x_\\star)$。\n\n根据高斯过程的性质，训练输出 $\\mathbf{y}$ 和测试潜函数值 $f_\\star$ 的联合分布是一个多元高斯分布。设 $\\mathbf{f} = [f(x_1), \\dots, f(x_N)]^\\top$ 为在训练输入处的潜函数值向量。在先验下，$\\mathbf{f}$ 和 $f_\\star$ 的联合分布为：\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^\\top & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\n其中：\n- $K = k(X, X)$ 是训练输入的 $N \\times N$ 协方差矩阵，其元素为 $K_{ij} = k(x_i, x_j)$。\n- $\\mathbf{k}_\\star = k(X, x_\\star)$ 是训练输入和测试输入之间的 $N \\times 1$ 协方差向量，其元素为 $(\\mathbf{k}_\\star)_i = k(x_i, x_\\star)$。\n- $k_{\\star\\star} = k(x_\\star, x_\\star)$ 是在测试输入处的先验方差。\n\n带噪声的观测值由 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$。带噪声的观测值 $\\mathbf{y}$ 和潜函数值 $f_\\star$ 之间的协方差为 $\\text{Cov}(\\mathbf{y}, f_\\star) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}, f_\\star) = \\text{Cov}(\\mathbf{f}, f_\\star) + \\text{Cov}(\\boldsymbol{\\varepsilon}, f_\\star) = \\mathbf{k}_\\star$，因为噪声 $\\boldsymbol{\\varepsilon}$ 与过程 $f$ 独立。观测值的方差为 $\\text{Var}(\\mathbf{y}) = \\text{Var}(\\mathbf{f} + \\boldsymbol{\\varepsilon}) = \\text{Var}(\\mathbf{f}) + \\text{Var}(\\boldsymbol{\\varepsilon}) = K + \\sigma^2 I$。\n\n因此，$\\mathbf{y}$ 和 $f_\\star$ 的联合分布为：\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma^2 I & \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^\\top & k_{\\star\\star} \\end{pmatrix} \\right)\n$$\n\n### 步骤 2：后验预测分布\n\n在给定数据 $(X, \\mathbf{y})$ 和测试点 $x_\\star$ 的条件下，$f_\\star$ 的后验预测分布是条件分布 $p(f_\\star | \\mathbf{y}, X, x_\\star)$。使用多元高斯分布中条件分布的标准公式，我们发现该分布也是高斯分布，$f_\\star | \\mathbf{y}, X, x_\\star \\sim \\mathcal{N}(m_\\star, v_\\star)$，其均值 $m_\\star$ 和方差 $v_\\star$ 由下式给出：\n$$m_\\star = \\mathbf{k}_\\star^\\top (K + \\sigma^2 I)^{-1} \\mathbf{y}$$\n$$v_\\star = k_{\\star\\star} - \\mathbf{k}_\\star^\\top (K + \\sigma^2 I)^{-1} \\mathbf{k}_\\star$$\n\n### 步骤 3：数值稳定的计算\n\n直接计算逆矩阵 $(K + \\sigma^2 I)^{-1}$ 的计算成本很高（时间复杂度为 $N$ 的三次方），并且可能数值不稳定，特别是当矩阵 $K_y = K + \\sigma^2 I$ 是病态矩阵时。一种更稳定的方法是使用 Cholesky 分解。由于 $K_y$ 是对称正定的（对于 $\\sigma^2 > 0$），它可以被唯一地分解为 $K_y = L L^\\top$，其中 $L$ 是一个下三角矩阵。\n\n后验均值 $m_\\star$ 可以通过先求解线性方程组 $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ 得到向量 $\\boldsymbol{\\alpha} = (K + \\sigma^2 I)^{-1} \\mathbf{y}$，然后计算 $m_\\star = \\mathbf{k}_\\star^\\top \\boldsymbol{\\alpha}$ 来获得。使用 Cholesky 因子 $L$，方程组 $L L^\\top \\boldsymbol{\\alpha} = \\mathbf{y}$ 可通过两个步骤求解：\n1.  使用前向代入法求解 $L \\mathbf{z} = \\mathbf{y}$ 以得到 $\\mathbf{z}$。\n2.  使用后向代入法求解 $L^\\top \\boldsymbol{\\alpha} = \\mathbf{z}$ 以得到 $\\boldsymbol{\\alpha}$。\n\n后验方差 $v_\\star$ 可以用类似的方法计算。我们需要计算二次型 $\\mathbf{k}_\\star^\\top K_y^{-1} \\mathbf{k}_\\star$。令 $\\mathbf{v} = L^{-1} \\mathbf{k}_\\star$，这可以通过使用前向代入法求解三角方程组 $L \\mathbf{v} = \\mathbf{k}_\\star$ 来得到。然后，该二次型就是 $\\mathbf{v}^\\top \\mathbf{v}$：\n$$\\mathbf{k}_\\star^\\top K_y^{-1} \\mathbf{k}_\\star = \\mathbf{k}_\\star^\\top (L L^\\top)^{-1} \\mathbf{k}_\\star = \\mathbf{k}_\\star^\\top (L^\\top)^{-1} L^{-1} \\mathbf{k}_\\star = (L^{-1} \\mathbf{k}_\\star)^\\top (L^{-1} \\mathbf{k}_\\star) = \\mathbf{v}^\\top \\mathbf{v}$$\n于是，后验方差为：\n$$v_\\star = k_{\\star\\star} - \\mathbf{v}^\\top \\mathbf{v}$$\n\n### 步骤 4：应用于给定问题\n\n具体的参数和数据如下：\n- 超参数：$\\sigma_f^2 = 1.0$，$\\ell = 0.6$。\n- 训练输入：$X = [0.0, 0.0, 1.0]^\\top$。\n- 训练输出：$\\mathbf{y} = [1.0, -1.0, 0.0]^\\top$。\n- 测试输入：$x_\\star = 0.0$。\n- 噪声方差：$\\sigma^2 \\in \\{10^{-6}, 0.1, 10.0\\}$。\n\n首先，我们计算必要的协方差项。令 $c = k(0.0, 1.0) = 1.0 \\cdot \\exp\\left(-\\frac{(0-1)^2}{2 \\cdot 0.6^2}\\right) = \\exp\\left(-\\frac{1}{0.72}\\right) \\approx 0.24935$。\n- $3 \\times 3$ 的训练协方差矩阵 $K$ 是：\n  $$K = \\begin{pmatrix} k(0,0) & k(0,0) & k(0,1) \\\\ k(0,0) & k(0,0) & k(0,1) \\\\ k(1,0) & k(1,0) & k(1,1) \\end{pmatrix} = \\begin{pmatrix} 1.0 & 1.0 & c \\\\ 1.0 & 1.0 & c \\\\ c & c & 1.0 \\end{pmatrix}$$\n- 训练输入和测试输入之间的协方差向量是：\n  $$\\mathbf{k}_\\star = \\begin{pmatrix} k(0,0) \\\\ k(0,0) \\\\ k(1,0) \\end{pmatrix} = \\begin{pmatrix} 1.0 \\\\ 1.0 \\\\ c \\end{pmatrix}$$\n- 在测试点处的先验方差是：\n  $$k_{\\star\\star} = k(0,0) = 1.0$$\n\n计算中使用矩阵 $K_y = K + \\sigma^2 I_3$。由于问题的对称性——重复的输入 $x=0.0$ 对应对称的输出 $y \\in \\{1.0, -1.0\\}$，测试点也为 $x_\\star=0.0$，以及零均值先验——在 $x_\\star=0.0$ 处的后验预测均值 $m_\\star$ 预期对所有 $\\sigma^2$ 值都为 $0.0$。\n\n后验预测方差 $v_\\star$ 将取决于 $\\sigma^2$：\n- 对于非常小的 $\\sigma^2$（例如 $10^{-6}$），模型被迫用极小的噪声来解释在 $x=0.0$ 处的矛盾数据 $y_1=1.0$ 和 $y_2=-1.0$。它会以高置信度得出结论，即潜函数值 $f(0.0)$ 必须是它们的均值 $0.0$。因此，后验方差 $v_\\star$ 将会非常小。\n- 对于非常大的 $\\sigma^2$（例如 $10.0$），观测值被认为是高噪声的，提供的信息很少。后验分布会回归到先验分布。后验均值 $m_\\star$ 趋近于先验均值（$0$），后验方差 $v_\\star$ 趋近于先验方差 $k_{\\star\\star} = 1.0$。\n- 对于中等的 $\\sigma^2$（例如 $0.1$），方差将介于这两个极端之间。\n\n以下的 Python 实现将为每个给定的 $\\sigma^2$ 计算 $m_\\star$ 和 $v_\\star$ 的精确值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef solve():\n    \"\"\"\n    Computes the posterior predictive mean and variance for a Gaussian Process\n    regression problem with repeated inputs and conflicting outputs.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [1e-6, 0.1, 10.0]\n\n    # --- Problem Definition ---\n    # GP hyperparameters\n    sigma_f_sq = 1.0\n    ell = 0.6\n\n    # Training data\n    X_train = np.array([[0.0], [0.0], [1.0]])\n    y_train = np.array([1.0, -1.0, 0.0])\n\n    # Test point\n    x_star = np.array([[0.0]])\n    \n    # --- Kernel Function ---\n    def squared_exponential_kernel(a, b, sigma_f_sq_val, ell_val):\n        \"\"\"\n        Computes the squared-exponential covariance matrix between two sets of points.\n        a: (N, D) array of N points\n        b: (M, D) array of M points\n        Returns: (N, M) covariance matrix\n        \"\"\"\n        # For 1D inputs shaped (N, 1) and (M, 1), broadcasting (a - b.T)\n        # creates an (N, M) matrix of differences.\n        sq_dist = (a - b.T)**2\n        return sigma_f_sq_val * np.exp(-0.5 * sq_dist / ell_val**2)\n\n    # --- Pre-compute Covariances ---\n    # Covariance matrix of training inputs\n    K = squared_exponential_kernel(X_train, X_train, sigma_f_sq, ell)\n    \n    # Covariance between training and test points\n    k_star = squared_exponential_kernel(X_train, x_star, sigma_f_sq, ell).flatten()\n    \n    # Covariance of the test point with itself (prior variance)\n    k_star_star = squared_exponential_kernel(x_star, x_star, sigma_f_sq, ell)[0, 0]\n\n    results = []\n    \n    for sigma_sq in test_cases:\n        N = X_train.shape[0]\n        \n        # Build the covariance matrix for noisy observations\n        Ky = K + sigma_sq * np.eye(N)\n        \n        # --- Numerically Stable Computation using Cholesky Factorization ---\n        \n        # 1. Perform Cholesky decomposition: Ky = L * L.T\n        try:\n            L = cholesky(Ky, lower=True)\n        except np.linalg.LinAlgError:\n            # This should not happen for sigma_sq > 0 as Ky will be positive definite.\n            # Handle as an error case if it occurs.\n            results.extend([np.nan, np.nan])\n            continue\n            \n        # --- Posterior Mean Calculation ---\n        # m_star = k_star.T @ inv(Ky) @ y_train\n        # Solve L @ z = y_train for z\n        z = solve_triangular(L, y_train, lower=True, check_finite=False)\n        # Solve L.T @ alpha = z for alpha\n        alpha = solve_triangular(L.T, z, lower=False, check_finite=False)\n        \n        m_star = k_star.T @ alpha\n        \n        # --- Posterior Variance Calculation ---\n        # v_star = k_star_star - k_star.T @ inv(Ky) @ k_star\n        # Solve L @ v = k_star for v\n        v = solve_triangular(L, k_star, lower=True, check_finite=False)\n        \n        v_star = k_star_star - v.T @ v\n        \n        results.append(m_star)\n        results.append(v_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3122972"}, {"introduction": "高斯过程的强大之处在于其能够通过选择协方差核函数，将关于函数结构的先验知识融入模型中。这项练习将抽象原理应用于实际问题，展示了如何为周期性数据建模——这是科学与工程中的一项常见任务。你将使用周期核函数在较大的缺失数据段上插值信号，并在此过程中探索高斯过程的另一个关键特性：量化不确定性的能力，这种不确定性会在远离观测数据的区域自然增加 [@problem_id:3122875]。", "problem": "您需要实现一维高斯过程回归，以研究使用周期性协方差对缺失片段进行插值的问题。请仅从以下原理和定义出发，在从这些基础推导出后验量的闭合形式表达式之前，不要假设任何已有的表达式：\n- 高斯过程（GP）是随机变量的集合，其中任何有限子集都服从联合高斯分布。未知函数 $f$ 上的零均值高斯过程先验写作 $f \\sim \\mathcal{GP}(0, k)$，其中 $k$ 是一个半正定协方差函数。\n- 观测是有噪声的：给定输入 $x_1,\\dots,x_n$，观测模型为 $y_i = f(x_i) + \\epsilon_i$，其中噪声 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ 是独立的。\n- 联合多元正态分布的条件化规则给出了任何子集在另一子集条件下的均值和协方差。\n\n您将使用周期性协方差来编码先验知识，即函数 $f$ 以单位间隔重复。使用标准的周期性协方差（也称为指数正弦平方核函数）：\n$$\nk(x,x') \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\frac{2\\,\\sin^2\\!\\big(\\pi\\,|x-x'|/p\\big)}{\\ell^2}\\right),\n$$\n其中振幅为 $\\sigma_f$，长度尺度为 $\\ell$，周期为 $p$。\n\n数据生成过程：\n- 真实函数：$f(x) = \\sin(2\\pi x) + 0.5\\sin(4\\pi x)$，其中正弦函数的输入以弧度为单位。\n- 定义域：$x \\in [0,1]$，匹配一个完整周期 $p = 1$。\n- 训练输入：从包含端点的区间 $[0,1]$ 上的一个 $N_{\\text{train,all}} = 200$ 个点的均匀网格开始。移除所有落入指定缺失片段（间隙）内的点，得到最终的训练集。\n- 观测值：$y_i = f(x_i) + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$，使用种子为 $0$ 的伪随机数生成器生成。\n- 测试输入：包含端点的区间 $[0,1]$ 上的一个 $N_{\\text{test}} = 600$ 个点的均匀网格。\n\n高斯过程先验和似然：\n- 使用零均值、上述周期性核函数以及方差为 $\\sigma_n^2$ 的加性独立高斯噪声。\n\n任务：\n1. 根据上述定义，通过在噪声方差为 $\\sigma_n^2$ 的观测模型下，对训练和测试函数值的联合高斯先验进行条件化，推导并实现在测试输入点的高斯过程后验预测均值和标准差。\n2. 对每个测试输入 $x_\\ast$，计算后验预测均值 $m_\\ast$ 和后验预测标准差 $s_\\ast$。\n3. 根据提供的区间定义间隙的并集 $\\mathcal{G} \\subset [0,1]$。令 $\\mathcal{G}^c$ 表示其在 $[0,1]$ 中的补集。在测试网格上：\n   - 计算间隙内的平均绝对误差，\n     $$\n     \\mathrm{MAE}_{\\text{gap}} = \\frac{1}{|\\{x_\\ast \\in \\mathcal{G}\\}|}\\sum_{x_\\ast \\in \\mathcal{G}} \\left| m_\\ast - f(x_\\ast) \\right|.\n     $$\n   - 计算不确定性膨胀因子，\n     $$\n     R = \\frac{\\text{average of } s_\\ast \\text{ over } x_\\ast \\in \\mathcal{G}}{\\text{average of } s_\\ast \\text{ over } x_\\ast \\in \\mathcal{G}^c}.\n     $$\n\n核函数超参数：\n- 使用 $\\sigma_f = 1.0$，$\\ell = 0.2$ 和 $p=1.0$。\n\n数值和实现细节：\n- 为了数值稳定性，在训练协方差矩阵的对角线上添加一个小的正对角抖动（例如 $\\epsilon_{\\text{jitter}} = 10^{-10}$）。\n- 在污染训练输出和回归模型中都使用精确的训练噪声方差 $\\sigma_n^2$。\n- 所有计算必须通过将伪随机数生成器的种子设置为 $0$ 来确保确定性。\n\n测试套件：\n为以下每个参数集计算 $\\mathrm{MAE}_{\\text{gap}}$ 和 $R$，其中每个案例指定了作为闭区间并集的间隙集 $\\mathcal{G}$ 和噪声标准差 $\\sigma_n$：\n- 案例 1（理想情况）：间隙 $\\mathcal{G} = [0.35, 0.55]$，噪声 $\\sigma_n = 0.05$。\n- 案例 2（窄间隙）：间隙 $\\mathcal{G} = [0.49, 0.51]$，噪声 $\\sigma_n = 0.05$。\n- 案例 3（大间隙，较高噪声）：间隙 $\\mathcal{G} = [0.25, 0.70]$，噪声 $\\sigma_n = 0.10$。\n- 案例 4（边界间隙）：间隙 $\\mathcal{G} = [0.00, 0.10] \\cup [0.90, 1.00]$，噪声 $\\sigma_n = 0.05$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。\n- 对于上述顺序中的每个案例，输出两个浮点数，保留 $6$ 位小数：首先是 $R$，然后是 $\\mathrm{MAE}_{\\text{gap}}$。\n- 因此，最终的单行必须按此顺序包含 $8$ 个数字：\n$$\n[R_1, \\mathrm{MAE}_{\\text{gap},1}, R_2, \\mathrm{MAE}_{\\text{gap},2}, R_3, \\mathrm{MAE}_{\\text{gap},3}, R_4, \\mathrm{MAE}_{\\text{gap},4}].\n$$", "solution": "该问题要求实现使用周期性核函数的一维高斯过程（GP）回归，以预测函数在指定数据间隙中的值。解决方案包括两个主要阶段：首先，从基本原理推导后验预测方程；其次，实现这些方程以计算给定测试用例集所需的指标。\n\n### 高斯过程后验预测分布的推导\n\n令 $n$ 个训练输入的集合表示为矩阵 $X$，相应的带噪观测值表示为向量 $\\mathbf{y}$。令 $m$ 个测试输入的集合表示为 $X_\\ast$。我们寻求后验预测分布 $p(\\mathbf{f}_\\ast | X, \\mathbf{y}, X_\\ast)$，其中 $\\mathbf{f}_\\ast$ 是在测试输入 $X_\\ast$ 处的潜在函数值的向量。\n\n1.  **先验分布**：在函数 $f$ 上设置一个零均值高斯过程先验，记为 $f \\sim \\mathcal{GP}(0, k)$。这意味着在任意点集上的潜在函数值都服从一个多元高斯分布。对于训练点和测试点的组合集，其函数值 $\\mathbf{f}$（在 $X$ 处）和 $\\mathbf{f}_\\ast$（在 $X_\\ast$ 处）的联合先验分布为：\n    $$\n    \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{f}_\\ast \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} K(X, X) & K(X, X_\\ast) \\\\ K(X_\\ast, X) & K(X_\\ast, X_\\ast) \\end{pmatrix} \\right)\n    $$\n    此处，$K(A, B)$ 表示通过将核函数 $k(x, x')$ 应用于集合 $A$ 和 $B$ 中的每对点计算出的协方差矩阵。具体来说，$K(X, X) \\in \\mathbb{R}^{n \\times n}$，$K(X, X_\\ast) \\in \\mathbb{R}^{n \\times m}$，$K(X_\\ast, X) \\in \\mathbb{R}^{m \\times n}$，以及 $K(X_\\ast, X_\\ast) \\in \\mathbb{R}^{m \\times m}$。\n\n2.  **似然**：观测模型为 $y_i = f(x_i) + \\epsilon_i$，其中噪声项 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$ 是独立同分布的。写成向量形式为 $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$，其中 $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$，$I$ 是 $n \\times n$ 的单位矩阵。似然函数为 $p(\\mathbf{y} | \\mathbf{f}) = \\mathcal{N}(\\mathbf{y} | \\mathbf{f}, \\sigma_n^2 I)$。\n\n3.  **可观测量与测试值的联合分布**：为了找到后验预测分布，我们首先需要观测数据 $\\mathbf{y}$ 和测试函数值 $\\mathbf{f}_\\ast$ 的联合分布。由于 $\\mathbf{f}$ 和 $\\boldsymbol{\\epsilon}$ 是独立的零均值高斯随机向量，它们的和 $\\mathbf{y}$ 也是一个零均值高斯向量。因此，$(\\mathbf{y}, \\mathbf{f}_\\ast)$ 的完整联合分布是一个多元高斯分布，其参数确定如下：\n    -   **均值**：$E[\\mathbf{y}] = E[\\mathbf{f} + \\boldsymbol{\\epsilon}] = E[\\mathbf{f}] + E[\\boldsymbol{\\epsilon}] = \\mathbf{0}$。$\\mathbf{f}_\\ast$ 的均值也为 $\\mathbf{0}$。\n    -   **协方差**：分块协方差矩阵计算如下：\n        -   $\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}) = \\text{Cov}(\\mathbf{f}) + \\text{Cov}(\\boldsymbol{\\epsilon}) = K(X, X) + \\sigma_n^2 I$。由于 $\\mathbf{f}$ 和 $\\boldsymbol{\\epsilon}$ 的独立性，交叉协方差项为零。\n        -   $\\text{Cov}(\\mathbf{f}_\\ast, \\mathbf{f}_\\ast) = K(X_\\ast, X_\\ast)$。\n        -   $\\text{Cov}(\\mathbf{y}, \\mathbf{f}_\\ast) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\epsilon}, \\mathbf{f}_\\ast) = \\text{Cov}(\\mathbf{f}, \\mathbf{f}_\\ast) = K(X, X_\\ast)$。\n    因此，联合分布为：\n    $$\n    \\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_\\ast \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix}, \\begin{pmatrix} K(X, X) + \\sigma_n^2 I & K(X, X_\\ast) \\\\ K(X_\\ast, X) & K(X_\\ast, X_\\ast) \\end{pmatrix} \\right)\n    $$\n\n4.  **条件化以求后验**：我们使用条件高斯分布的标准公式。如果 $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$，那么 $p(\\mathbf{b}|\\mathbf{a}) = \\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$，其中：\n    $$\n    \\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a) \\quad \\text{和} \\quad \\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}\n    $$\n    通过进行替换 $\\mathbf{a} \\leftrightarrow \\mathbf{y}$ 和 $\\mathbf{b} \\leftrightarrow \\mathbf{f}_\\ast$，我们得到后验预测分布 $p(\\mathbf{f}_\\ast | X, \\mathbf{y}, X_\\ast) = \\mathcal{N}(\\mathbf{m}_\\ast, \\Sigma_\\ast)$，其中：\n    -   **后验预测均值** $\\mathbf{m}_\\ast$：\n        $$\n        \\mathbf{m}_\\ast = K(X_\\ast, X) (K(X, X) + \\sigma_n^2 I)^{-1} \\mathbf{y}\n        $$\n    -   **后验预测协方差** $\\Sigma_\\ast$：\n        $$\n        \\Sigma_\\ast = K(X_\\ast, X_\\ast) - K(X_\\ast, X) (K(X, X) + \\sigma_n^2 I)^{-1} K(X, X_\\ast)\n        $$\n\n5.  **实现所需量**：每个测试点 $x_{\\ast, i}$ 的后验预测均值是 $\\mathbf{m}_\\ast$ 的第 $i$ 个元素。每个测试点 $s_{\\ast,i}$ 的后验预测标准差是 $\\Sigma_\\ast$ 第 $i$ 个对角元素的平方根。为了数值稳定性，我们避免了矩阵求逆。令 $K_y = K(X, X) + \\sigma_n^2 I$。我们求解线性系统 $K_y \\boldsymbol{\\alpha} = \\mathbf{y}$ 以得到 $\\boldsymbol{\\alpha}$，从而得出 $\\mathbf{m}_\\ast = K(X_\\ast, X) \\boldsymbol{\\alpha}$。类似地，为了计算后验方差，我们求解 $K_y V = K(X, X_\\ast)$ 以得到矩阵 $V$。后验协方差变为 $\\Sigma_\\ast = K(X_\\ast, X_\\ast) - K(X_\\ast, X) V$。\n\n### 计算流程\n\n对于每个测试案例：\n1.  **数据准备**：对 $[0,1]$ 上的初始 $N_{\\text{train,all}} = 200$ 个训练输入集进行筛选，排除位于指定间隙 $\\mathcal{G}$ 内的点。通过计算 $f(x) = \\sin(2\\pi x) + 0.5\\sin(4\\pi x)$ 并添加标准差为 $\\sigma_n$ 的高斯噪声来生成观测输出 $\\mathbf{y}$，每个案例使用不同的随机种子以确保独立试验。测试输入是 $[0,1]$ 上的一个固定的 $N_{\\text{test}} = 600$ 个点的网格。\n2.  **协方差矩阵构建**：使用周期性核函数 $k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{2\\,\\sin^2(\\pi\\,|x-x'|/p)}{\\ell^2}\\right)$（其中 $\\sigma_f=1.0$, $\\ell=0.2$, $p=1.0$）来构建协方差矩阵。\n3.  **后验计算**：带噪训练协方差矩阵 $K_y = K(X, X) + \\sigma_n^2 I$ 的对角线加上了一个抖动项 $\\epsilon_{\\text{jitter}} = 10^{-10}$ 以保证数值稳定性。对 $K_y$ 进行 Cholesky 分解，为求解后验均值 $\\mathbf{m}_\\ast$ 和后验协方差 $\\Sigma_\\ast$ 对角线的线性系统提供了一种高效且稳定的方法。\n4.  **指标评估**：根据测试点 $X_\\ast$ 是否落在间隙 ($\\mathcal{G}$) 内或间隙外 ($\\mathcal{G}^c$) 进行划分。然后根据定义计算指标 $\\mathrm{MAE}_{\\text{gap}}$ 和 $R$。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef solve():\n    \"\"\"\n    Implements one-dimensional Gaussian process regression and evaluates its performance\n    on interpolating data gaps for several test cases.\n    \"\"\"\n    # Define problem constants\n    N_TRAIN_ALL = 200\n    N_TEST = 600\n    SIGMA_F = 1.0\n    L_SCALE = 0.2\n    PERIOD = 1.0\n    JITTER = 1e-10\n\n    # Test suite parameters\n    test_cases = [\n        {\"gaps\": [(0.35, 0.55)], \"sigma_n\": 0.05, \"seed\": 0},\n        {\"gaps\": [(0.49, 0.51)], \"sigma_n\": 0.05, \"seed\": 1},\n        {\"gaps\": [(0.25, 0.70)], \"sigma_n\": 0.10, \"seed\": 2},\n        {\"gaps\": [(0.00, 0.10), (0.90, 1.00)], \"sigma_n\": 0.05, \"seed\": 3},\n    ]\n\n    # Global data grids\n    x_train_all = np.linspace(0.0, 1.0, N_TRAIN_ALL)\n    x_test = np.linspace(0.0, 1.0, N_TEST)\n\n    # True function\n    def true_function(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.sin(4 * np.pi * x)\n\n    # Periodic kernel function\n    def periodic_kernel(a, b, sigma_f, l_scale, p):\n        dist = np.abs(a[:, None] - b[None, :])\n        sin_sq_term = np.sin(np.pi * dist / p)**2\n        return sigma_f**2 * np.exp(-2 * sin_sq_term / l_scale**2)\n\n    results = []\n    \n    # Use a fixed top-level seed for the problem but separate seeds per case\n    # This ensures that if the problem were changed to have one global seed, that\n    # could be easily accommodated. Here, we re-seed for each case to ensure\n    # case-wise determinism as per a common test harness interpretation.\n    # The problem statement seed is 0. We'll use this as the starting point to\n    # generate independent seeds for each case.\n    master_seed = 0\n    rng_master = np.random.default_rng(master_seed)\n    case_seeds = rng_master.integers(0, 2**32, size=len(test_cases))\n\n    for i, case in enumerate(test_cases):\n        sigma_n = case[\"sigma_n\"]\n        gaps = case[\"gaps\"]\n        \n        # Using a fixed seed of 0 for each case for deterministic results as requested\n        # 'generated using a pseudorandom number generator seeded with 0'.\n        # This is interpreted as resetting the generator to the same state for each case's noise generation.\n        rng = np.random.default_rng(0) \n\n        # 1. Data Preparation\n        # Filter training points to exclude those in gaps\n        train_mask = np.ones_like(x_train_all, dtype=bool)\n        for g_min, g_max in gaps:\n            train_mask = (x_train_all  g_min) | (x_train_all > g_max)\n        x_train = x_train_all[train_mask]\n        \n        # Generate noisy observations\n        y_true_train = true_function(x_train)\n        noise = rng.normal(0, sigma_n, size=x_train.shape)\n        y_train = y_true_train + noise\n\n        # 2. Covariance Matrix Construction\n        K_train_train = periodic_kernel(x_train, x_train, SIGMA_F, L_SCALE, PERIOD)\n        K_test_train = periodic_kernel(x_test, x_train, SIGMA_F, L_SCALE, PERIOD)\n        K_test_test = periodic_kernel(x_test, x_test, SIGMA_F, L_SCALE, PERIOD)\n        \n        # 3. Posterior Calculation\n        K_y = K_train_train + (sigma_n**2) * np.eye(len(x_train))\n        K_y[np.diag_indices_from(K_y)] += JITTER\n\n        try:\n            # Use Cholesky decomposition for stable and efficient solution\n            L, lower = cho_factor(K_y, lower=False) # L is U from U^T U = K_y\n            \n            # Solve for alpha = K_y^-1 * y_train\n            alpha = cho_solve((L, lower), y_train)\n            \n            # Calculate posterior mean\n            y_pred_mean = K_test_train @ alpha\n            \n            # Calculate posterior variance\n            # v = K_y^-1 * K_train_test\n            v = cho_solve((L, lower), K_test_train.T)\n            post_var = np.diag(K_test_test) - np.sum(K_test_train * v.T, axis=1)\n            # Ensure variance is non-negative due to potential numerical errors\n            post_var = np.maximum(post_var, 0)\n            y_pred_std = np.sqrt(post_var)\n\n        except np.linalg.LinAlgError:\n            print(f\"Cholesky decomposition failed for case {i+1}. Matrix may not be positive definite.\")\n            results.extend([float('nan'), float('nan')])\n            continue\n\n        # 4. Metric Evaluation\n        test_gap_mask = np.zeros_like(x_test, dtype=bool)\n        for g_min, g_max in gaps:\n            test_gap_mask |= (x_test >= g_min)  (x_test = g_max)\n            \n        y_true_test = true_function(x_test)\n\n        # MAE in gap\n        mae_gap = np.mean(np.abs(y_pred_mean[test_gap_mask] - y_true_test[test_gap_mask]))\n\n        # Uncertainty inflation factor R\n        s_star_gap = y_pred_std[test_gap_mask]\n        s_star_complement = y_pred_std[~test_gap_mask]\n\n        if len(s_star_gap) == 0 or len(s_star_complement) == 0:\n            R = float('nan')\n        else:\n            avg_s_gap = np.mean(s_star_gap)\n            avg_s_complement = np.mean(s_star_complement)\n            R = avg_s_gap / avg_s_complement if avg_s_complement > 0 else float('inf')\n        \n        results.extend([round(R, 6), round(mae_gap, 6)])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3122875"}, {"introduction": "在任何机器学习工作流中，高效的模型选择和验证都是至关重要的步骤。对于高斯过程，可以利用其精妙的数学特性，以比朴素方法高得多的效率执行交叉验证。这个高级练习将挑战你超越简单的预测，深入高斯过程的计算核心。你将推导并实现一种用于留一法交叉验证（LOOCV）的解析捷径，将一个计算成本高昂的过程转变为一个高效的过程，从而体会矩阵代数和统计推断之间强大的相互作用，正是这种相互作用使高斯过程成为一个独特的优雅建模工具 [@problem_id:3122941]。", "problem": "一位数据分析师正在对一个小数据集应用高斯过程回归，并希望使用留一法交叉验证（LOOCV）来高效地评估模型。该模型假设一个零均值高斯过程（GP）先验，其协方差为平方指数核（也称径向基函数核），且观测值上存在独立的高斯噪声。\n\n从以下基本事实出发：\n- 如果一个随机向量服从多元高斯分布，那么其任意分量子集以及它们的条件分布都是高斯分布，其均值和协方差由线性代数条件化规则确定。\n- 对于一个高斯过程回归模型，其潜函数值为 $f(x)$，观测值为 $y = f(x) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2})$ 独立同分布，训练输出的联合分布是多元高斯分布，其协方差矩阵为 $C = K + \\sigma_{n}^{2} I$，其中 $K$ 是核函数的格拉姆矩阵。\n\n考虑在 $\\mathbb{R}^{2}$ 中构成边长为 $1$ 的等边三角形的 $n = 3$ 个输入点：\n- $x_{1} = (0, 0)$，\n- $x_{2} = (1, 0)$，\n- $x_{3} = \\left(\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right)$。\n观测到的输出为 $y = (1.2, 0.7, -0.3)^{\\top}$。\n\n使用平方指数核 $k(x, x') = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{\\|x - x'\\|^{2}}{2 \\ell^{2}}\\right)$，超参数为 $\\sigma_{f}^{2} = 2$，长度尺度 $\\ell$ 的选择使得对于距离为 $1$ 的点，我们有 $\\exp\\!\\left(-\\frac{1}{2 \\ell^{2}}\\right) = \\frac{1}{2}$。观测噪声方差为 $\\sigma_{n}^{2} = 1$。\n\n任务：\n1. 使用上述基础知识和适当的矩阵恒等式（如分块矩阵求逆和舒尔补），推导给定 $y_{-i}$ 时 $y_{i}$ 的 LOOCV 预测分布，并用 $S = C^{-1}$ 的元素和 $\\alpha = S y$ 表示。您的推导必须为每个 $i \\in \\{1,2,3\\}$ 生成关于 $S$ 和 $\\alpha$ 的 LOOCV 预测均值 $\\mu_{-i}$ 和方差 $\\sigma_{-i}^{2}$ 的显式公式。\n2. 对于指定的数据集和超参数，构建训练协方差矩阵 $C$ 并使用适当的矩阵恒等式（不使用数值近似的捷径）计算其逆矩阵 $S$。\n3. 使用您推导的公式，计算这三个点的 LOOCV 平均负对数预测密度（NLPD），其定义为\n$$\\frac{1}{3} \\sum_{i=1}^{3} \\left[ \\frac{1}{2} \\ln\\!\\left(2 \\pi \\sigma_{-i}^{2}\\right) + \\frac{1}{2} \\frac{\\left(y_{i} - \\mu_{-i}\\right)^{2}}{\\sigma_{-i}^{2}} \\right].$$\n\n将您的最终数值答案四舍五入到四位有效数字。不需要单位。", "solution": "首先根据指定标准对问题陈述进行验证。\n\n### 问题验证\n\n**步骤1：提取给定信息**\n- **模型：** 具有零均值先验的高斯过程回归。观测值为 $y = f(x) + \\epsilon$，噪声 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{n}^{2})$ 独立同分布。\n- **联合分布：** 训练输出 $\\mathbf{y}$ 服从多元高斯分布 $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, C)$，其中 $C = K + \\sigma_{n}^{2} I$。\n- **核函数：** 平方指数核 $k(x, x') = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{\\|x - x'\\|^{2}}{2 \\ell^{2}}\\right)$。\n- **数据集大小：** $n = 3$。\n- **输入点：**\n  - $x_{1} = (0, 0)$\n  - $x_{2} = (1, 0)$\n  - $x_{3} = \\left(\\frac{1}{2}, \\frac{\\sqrt{3}}{2}\\right)$\n- **观测输出：** $\\mathbf{y} = (1.2, 0.7, -0.3)^{\\top}$。\n- **超参数：**\n  - 信号方差：$\\sigma_{f}^{2} = 2$。\n  - 长度尺度：$\\ell$ 定义为 $\\exp\\!\\left(-\\frac{1}{2 \\ell^{2}}\\right) = \\frac{1}{2}$。\n  - 噪声方差：$\\sigma_{n}^{2} = 1$。\n- **任务：**\n  1. 推导 LOOCV 预测均值 $\\mu_{-i}$ 和方差 $\\sigma_{-i}^{2}$ 关于 $S = C^{-1}$ 和 $\\alpha = S \\mathbf{y}$ 的公式。\n  2. 计算矩阵 $C$ 和 $S$。\n  3. 计算 LOOCV 平均负对数预测密度（NLPD）。\n\n**步骤2：使用提取的给定信息进行验证**\n- **科学上合理：** 该问题是高斯过程回归的标准应用，这是统计学习中一个完善的课题。所有概念，包括核函数、协方差矩阵和LOOCV，都是标准且数学上合理的。\n- **提法明确：** 问题提供了执行所需推导和计算的所有必要信息。任务定义清晰，能够得出一个唯一的、有意义的数值结果。\n- **客观性：** 问题使用精确、正式的语言陈述，不含任何主观或模糊的术语。\n- **完整性和一致性：** 给定信息是自洽且一致的。输入点如描述般构成一个等边三角形，所有必需的超参数都已直接或通过可解方程间接指定。\n- **无其他缺陷：** 该问题并非微不足道、隐喻性或自相矛盾。这是一个有效且提法明确的机器学习练习。\n\n**步骤3：结论与行动**\n问题有效。将提供解答。\n\n### 解答\n\n解答分为与任务相对应的三个部分。\n\n**任务1：LOOCV 预测公式的推导**\n\n对于留一法交叉验证（LOOCV），我们的目标是使用所有其他观测值 $\\mathbf{y}_{-i} = (y_1, \\dots, y_{i-1}, y_{i+1}, \\dots, y_n)^\\top$ 来预测每个观测值 $y_i$。所有观测值 $\\mathbf{y}$ 的联合分布给定为 $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, C)$。我们根据 $y_i$ 和 $\\mathbf{y}_{-i}$ 之间的划分来对向量 $\\mathbf{y}$ 和协方差矩阵 $C$ 进行分块：\n$$ \\mathbf{y} = \\begin{pmatrix} y_i \\\\ \\mathbf{y}_{-i} \\end{pmatrix}, \\quad C = \\begin{pmatrix} C_{ii}  C_{i, -i} \\\\ C_{-i, i}  C_{-i, -i} \\end{pmatrix} $$\n其中 $C_{i, -i}$ 是一个行向量，$C_{-i, i}$ 是一个列向量，$C_{-i, -i}$ 是从 $C$ 中移除第 $i$ 行和第 $i$ 列后得到的子矩阵。\n\n根据条件高斯分布的规则，预测分布 $p(y_i | \\mathbf{y}_{-i})$ 是一个高斯分布 $\\mathcal{N}(\\mu_{-i}, \\sigma_{-i}^2)$，其均值和方差由以下公式给出：\n$$ \\mu_{-i} = E[y_i | \\mathbf{y}_{-i}] = C_{i, -i} (C_{-i, -i})^{-1} \\mathbf{y}_{-i} $$\n$$ \\sigma_{-i}^2 = \\text{Var}(y_i | \\mathbf{y}_{-i}) = C_{ii} - C_{i, -i} (C_{-i, -i})^{-1} C_{-i, i} $$\n这些公式计算量很大，因为每次迭代都需要对一个 $(n-1) \\times (n-1)$ 的矩阵求逆。我们可以使用完整协方差矩阵的逆 $S = C^{-1}$ 来推导更高效的公式。我们以与 $C$ 相同的方式对 $S$ 进行分块：\n$$ S = C^{-1} = \\begin{pmatrix} S_{ii}  S_{i, -i} \\\\ S_{-i, i}  S_{-i, -i} \\end{pmatrix} $$\n根据分块矩阵求逆公式，逆矩阵 $S$ 的左上角块与 $C_{ii}$ 的舒尔补有关：\n$$ S_{ii} = (C_{ii} - C_{i, -i} (C_{-i, -i})^{-1} C_{-i, i})^{-1} $$\n通过观察，括号内的表达式正是预测方差 $\\sigma_{-i}^2$。因此，我们有：\n$$ \\sigma_{-i}^2 = \\frac{1}{S_{ii}} $$\n为了用 $S$ 和 $\\alpha = S\\mathbf{y}$ 表示预测均值 $\\mu_{-i}$，我们使用分块矩阵求逆公式的另一个结果：\n$$ S_{i, -i} = -S_{ii} C_{i, -i} (C_{-i, -i})^{-1} $$\n由此，我们可以将均值公式中的项表示为 $C_{i, -i} (C_{-i, -i})^{-1} = -S_{i,-i}/S_{ii}$。将此代入 $\\mu_{-i}$ 的表达式：\n$$ \\mu_{-i} = \\left( -\\frac{S_{i, -i}}{S_{ii}} \\right) \\mathbf{y}_{-i} $$\n现在考虑 $\\alpha = S\\mathbf{y}$ 的第 $i$ 个元素：\n$$ \\alpha_i = (S\\mathbf{y})_i = S_{ii} y_i + S_{i, -i} \\mathbf{y}_{-i} $$\n整理此方程得到 $S_{i, -i} \\mathbf{y}_{-i} = \\alpha_i - S_{ii} y_i$。将此代入我们关于 $\\mu_{-i}$ 的表达式：\n$$ \\mu_{-i} = -\\frac{\\alpha_i - S_{ii} y_i}{S_{ii}} = y_i - \\frac{\\alpha_i}{S_{ii}} $$\n因此，用 $S$ 和 $\\alpha$ 表示的 LOOCV 预测均值和方差为：\n$$ \\mu_{-i} = y_i - \\frac{\\alpha_i}{S_{ii}} $$\n$$ \\sigma_{-i}^2 = \\frac{1}{S_{ii}} $$\n\n**任务2：协方差矩阵 $C$ 及其逆矩阵 $S$ 的计算**\n\n首先，我们确定长度尺度超参数 $\\ell$ 的值。\n$$ \\exp\\left(-\\frac{1}{2\\ell^2}\\right) = \\frac{1}{2} \\implies -\\frac{1}{2\\ell^2} = \\ln\\left(\\frac{1}{2}\\right) = -\\ln(2) \\implies \\frac{1}{2\\ell^2} = \\ln(2) $$\n核函数可以写为 $k(x, x') = \\sigma_f^2 \\exp(-\\|x-x'\\|^2 \\ln(2))$。\n给定 $\\sigma_f^2 = 2$，这变为 $k(x, x') = 2 \\cdot 2^{-\\|x-x'\\|^2}$。\n\n接下来，我们计算数据点之间的欧氏距离平方：\n$x_1 = (0, 0)$，$x_2 = (1, 0)$，$x_3 = (\\frac{1}{2}, \\frac{\\sqrt{3}}{2})$。\n$$ \\|x_1-x_2\\|^2 = (0-1)^2 + (0-0)^2 = 1 $$\n$$ \\|x_1-x_3\\|^2 = \\left(0-\\frac{1}{2}\\right)^2 + \\left(0-\\frac{\\sqrt{3}}{2}\\right)^2 = \\frac{1}{4} + \\frac{3}{4} = 1 $$\n$$ \\|x_2-x_3\\|^2 = \\left(1-\\frac{1}{2}\\right)^2 + \\left(0-\\frac{\\sqrt{3}}{2}\\right)^2 = \\frac{1}{4} + \\frac{3}{4} = 1 $$\n所有不同点之间的成对距离均为 $1$。\n\n现在，我们构建格拉姆矩阵 $K$。\n对角线元素为 $K_{ii} = k(x_i, x_i) = 2 \\cdot 2^{-0} = 2$。\n对于 $i \\ne j$ 的非对角线元素为 $K_{ij} = k(x_i, x_j) = 2 \\cdot 2^{-1} = 1$。\n$$ K = \\begin{pmatrix} 2  1  1 \\\\ 1  2  1 \\\\ 1  1  2 \\end{pmatrix} $$\n该矩阵可以写成 $K = I + J$，其中 $I$ 是单位矩阵，$J$ 是全一矩阵。\n\n完整的协方差矩阵 $C$ 是 $C = K + \\sigma_n^2 I$。给定 $\\sigma_n^2 = 1$：\n$$ C = (I+J) + 1 \\cdot I = 2I + J = \\begin{pmatrix} 2  0  0 \\\\ 0  2  0 \\\\ 0  0  2 \\end{pmatrix} + \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} = \\begin{pmatrix} 3  1  1 \\\\ 1  3  1 \\\\ 1  1  3 \\end{pmatrix} $$\n为了求逆矩阵 $S = C^{-1} = (2I+J)^{-1}$，我们使用 Sherman-Morrison-Woodbury 公式，对于形如 $(aI+bJ)^{-1}$ 的矩阵，其逆为 $\\frac{1}{a}I - \\frac{b}{a(a+nb)}J$。这里，$a=2$，$b=1$，$n=3$。\n$$ S = \\frac{1}{2}I - \\frac{1}{2(2+3 \\cdot 1)}J = \\frac{1}{2}I - \\frac{1}{10}J $$\n$$ S = \\frac{1}{2}\\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} - \\frac{1}{10}\\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}-\\frac{1}{10}  -\\frac{1}{10}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{1}{2}-\\frac{1}{10}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  -\\frac{1}{10}  \\frac{1}{2}-\\frac{1}{10} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{10}  -\\frac{1}{10}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{4}{10}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  -\\frac{1}{10}  \\frac{4}{10} \\end{pmatrix} $$\n$$ S = \\frac{1}{10} \\begin{pmatrix} 4  -1  -1 \\\\ -1  4  -1 \\\\ -1  -1  4 \\end{pmatrix} $$\n\n**任务3：平均负对数预测密度（NLPD）的计算**\n\n首先，我们用 $\\mathbf{y} = (1.2, 0.7, -0.3)^{\\top}$ 计算 $\\alpha = S\\mathbf{y}$：\n$$ \\alpha = \\frac{1}{10} \\begin{pmatrix} 4  -1  -1 \\\\ -1  4  -1 \\\\ -1  -1  4 \\end{pmatrix} \\begin{pmatrix} 1.2 \\\\ 0.7 \\\\ -0.3 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4(1.2) - 1(0.7) - 1(-0.3) \\\\ -1(1.2) + 4(0.7) - 1(-0.3) \\\\ -1(1.2) - 1(0.7) + 4(-0.3) \\end{pmatrix} $$\n$$ \\alpha = \\frac{1}{10} \\begin{pmatrix} 4.8 - 0.7 + 0.3 \\\\ -1.2 + 2.8 + 0.3 \\\\ -1.2 - 0.7 - 1.2 \\end{pmatrix} = \\frac{1}{10} \\begin{pmatrix} 4.4 \\\\ 1.9 \\\\ -3.1 \\end{pmatrix} = \\begin{pmatrix} 0.44 \\\\ 0.19 \\\\ -0.31 \\end{pmatrix} $$\n$S$ 的对角线元素均为 $S_{ii} = \\frac{4}{10} = 0.4$。\n对于所有 $i$，LOOCV 预测方差都是相同的：\n$$ \\sigma_{-i}^2 = \\frac{1}{S_{ii}} = \\frac{1}{0.4} = 2.5 $$\n预测残差为 $y_i - \\mu_{-i} = \\frac{\\alpha_i}{S_{ii}}$：\n$$ y_1 - \\mu_{-1} = \\frac{0.44}{0.4} = 1.1 $$\n$$ y_2 - \\mu_{-2} = \\frac{0.19}{0.4} = 0.475 $$\n$$ y_3 - \\mu_{-3} = \\frac{-0.31}{0.4} = -0.775 $$\n每个点的 NLPD 为 $L_i = \\frac{1}{2} \\ln(2\\pi\\sigma_{-i}^2) + \\frac{(y_i - \\mu_{-i})^2}{2\\sigma_{-i}^2}$。\n平均 NLPD 为 $\\frac{1}{3}\\sum_{i=1}^3 L_i$。\n$$ \\text{Avg NLPD} = \\frac{1}{3} \\sum_{i=1}^{3} \\left[ \\frac{1}{2} \\ln(2 \\pi (2.5)) + \\frac{(y_i - \\mu_{-i})^2}{2(2.5)} \\right] $$\n$$ \\text{Avg NLPD} = \\frac{1}{2} \\ln(5\\pi) + \\frac{1}{15} \\sum_{i=1}^{3} (y_i - \\mu_{-i})^2 $$\n现在我们对残差平方求和：\n$$ \\sum_{i=1}^{3} (y_i - \\mu_{-i})^2 = (1.1)^2 + (0.475)^2 + (-0.775)^2 = 1.21 + 0.225625 + 0.600625 = 2.03625 $$\n将此代入平均 NLPD 公式：\n$$ \\text{Avg NLPD} = \\frac{1}{2} \\ln(5\\pi) + \\frac{2.03625}{15} $$\n数值计算：\n$$ \\text{Avg NLPD} \\approx \\frac{1}{2} \\ln(15.70796) + 0.13575 $$\n$$ \\text{Avg NLPD} \\approx \\frac{1}{2} (2.75425) + 0.13575 \\approx 1.377125 + 0.13575 = 1.512875 $$\n四舍五入到四位有效数字，结果是 $1.513$。", "answer": "1.513", "id": "3122941"}]}