{"hands_on_practices": [{"introduction": "核技巧的强大之处在于它不仅仅局限于支持向量机。许多依赖于内积的线性模型都可以被“核化”，从而在复杂的非线性数据上获得强大的表现。本实践将指导您实现核逻辑回归（Kernel Logistic Regression），这需要您推导并应用对偶空间中的牛顿法，从而深入理解核化方法的核心机制，并掌握解决相应优化问题的实用技巧 [@problem_id:3136166]。", "problem": "使用核技巧，完全在对偶空间中通过 Newton 方法实现核逻辑斯谛回归。然后，比较其在零初始化和使用核岭回归解进行热启动时的收敛行为。您的程序必须是一个单一、完整、可运行的脚本，能够以指定格式生成最终结果，无需用户输入。\n\n基本原理和设置：\n- 给定一个二元分类数据集 $\\{(x_i,t_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^d$，目标 $t_i \\in \\{-1, +1\\}$。考虑一个正定核 $k(\\cdot,\\cdot)$，其 Gram 矩阵为 $K \\in \\mathbb{R}^{n \\times n}$，其中 $K_{ij} = k(x_i,x_j)$。令 $f(\\cdot)$ 位于由 $k$ 导出的再生核希尔伯特空间中，并根据表示定理，将其限定为 $f(\\cdot)=\\sum_{i=1}^n \\alpha_i k(x_i,\\cdot)$ 的形式，因此在训练集上的函数值向量为 $f = K \\alpha$，其中 $\\alpha \\in \\mathbb{R}^n$。\n- 使用带有正则化参数 $\\lambda > 0$ 的逻辑斯谛负对数似然：\n  $$J(\\alpha) = \\sum_{i=1}^n \\log\\!\\big(1 + \\exp(-t_i f_i)\\big) + \\frac{\\lambda}{2}\\,\\alpha^\\top K \\alpha,$$\n  其中 $f = K \\alpha$。您必须仅使用链式法则、逻辑斯谛函数的导数以及核矩阵的性质，推导出 $J(\\alpha)$ 相对于 $\\alpha$ 的梯度和 Hessian 矩阵。除了经过充分检验的逻辑斯谛损失公式和线性代数恒等式外，不要假设任何其他结果。\n- 使用长度尺度为 $\\ell > 0$ 的径向基函数（高斯）核：\n  $$k(x,x') = \\exp\\!\\left(-\\frac{\\lVert x-x' \\rVert_2^2}{2 \\ell^2}\\right)。$$\n\n算法要求：\n- 在对偶空间中实现 Newton 方法以最小化 $J(\\alpha)$：\n  - 在每次迭代中，构建关于 $\\alpha$ 的梯度和 Hessian 矩阵，并通过求解一个线性系统来计算 Newton 步长。当完整步长未能使目标函数 $J(\\alpha)$ 下降时，您必须实现回溯线搜索以确保其下降。使用 0.5 的回溯因子。\n  - 当梯度的欧几里得范数最多为 $10^{-6}$ 或迭代次数达到 50 时终止，以先到者为准。当您计算一个搜索方向时，计为一次 Newton 迭代，无论执行了多少次回溯缩减。\n  - 为了数值稳定性，您可以在求解的 Hessian 系统中添加一个微小的岭项 $\\epsilon I$，其中 $\\epsilon = 10^{-10}$。\n- 为标签 $t \\in \\{-1,+1\\}$ 实现使用相同核函数和相同正则化强度 $\\lambda$ 的核岭回归（KRR）。使用标准的 KRR 系统\n  $$(K + \\lambda I)\\,\\alpha_{\\mathrm{krr}} = t,$$\n  通过线性代数精确求解。使用 $\\alpha_{\\mathrm{krr}}$ 作为 Newton 方法的热启动。\n\n测试套件：\n对于以下所有情况，在 $\\mathbb{R}^2$ 中生成两个类别大小相等的高斯斑点数据。使用具有各向同性协方差 $\\sigma^2 I_2$ 的独立同分布高斯噪声。为保证可复现性，请使用指定的伪随机种子。使用给定的长度尺度 $\\ell$ 构建核函数。\n\n- 情况 A (理想情况):\n  - 种子: $0$。\n  - 总样本数: $n = 60$ (两类均衡)。\n  - 均值: 正类均值 $\\mu_+ = (1, 1)$，负类均值 $\\mu_- = (-1, -1)$。\n  - 标准差: $\\sigma = 0.3$。\n  - 核长度尺度: $\\ell = 1.0$。\n  - 正则化: $\\lambda = 10^{-2}$。\n- 情况 B (近常数核边缘情况): \n  - 种子: $1$。\n  - 总样本数: $n = 60$ (均衡)。\n  - 均值: 正类均值 $\\mu_+ = (0.7, -0.7)$，负类均值 $\\mu_- = (-0.7, 0.7)$。\n  - 标准差: $\\sigma = 0.4$。\n  - 核长度尺度: $\\ell = 5.0$。\n  - 正则化: $\\lambda = 10^{-2}$。\n- 情况 C (重叠与更强非线性):\n  - 种子: $2$。\n  - 总样本数: $n = 80$ (均衡)。\n  - 均值: 正类均值 $\\mu_+ = (0.5, 0.5)$，负类均值 $\\mu_- = (-0.5, -0.5)$。\n  - 标准差: $\\sigma = 0.8$。\n  - 核长度尺度: $\\ell = 0.5$。\n  - 正则化: $\\lambda = 10^{-2}$。\n\n计算与比较内容：\n- 对每种情况，运行 Newton 方法两次：\n  1. 使用 $\\alpha^{(0)} = 0$ 初始化，并获得满足终止条件所需的迭代次数 $I_0$。\n  2. 使用 $\\alpha^{(0)} = \\alpha_{\\mathrm{krr}}$ 初始化，并获得所需的迭代次数 $I_{\\mathrm{krr}}$。\n- 对每种情况，定义指示符 $B$，如果 $I_{\\mathrm{krr}} < I_0$ 则 $B$ 等于 $1$，否则等于 $0$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三种情况的结果，格式为逗号分隔的列表的列表，严格遵循以下格式：\n  $$[\\,[I_0^{(A)}, I_{\\mathrm{krr}}^{(A)}, B^{(A)}],\\,[I_0^{(B)}, I_{\\mathrm{krr}}^{(B)}, B^{(B)}],\\,[I_0^{(C)}, I_{\\mathrm{krr}}^{(C)}, B^{(C)}]\\,].$$\n- 除此单行外，不应打印任何额外的文本或空白字符。不涉及任何物理单位，所有计算均为实值浮点运算。", "solution": "用户提供了一个有效的问题。解决方案如下所示。\n\n### I. 问题构建\n\n目标是最小化核逻辑斯谛回归（KLR）的正则化负对数似然。数据集由数据对 $\\{(x_i, t_i)\\}_{i=1}^n$ 组成，其中 $x_i \\in \\mathbb{R}^d$ 是输入向量，$t_i \\in \\{-1, +1\\}$ 是二元类别标签。使用一个正定核 $k(\\cdot, \\cdot)$，决策函数以对偶形式表示为 $f(\\cdot) = \\sum_{j=1}^n \\alpha_j k(x_j, \\cdot)$。在训练数据上的函数评估向量为 $f = K\\alpha$，其中 $K$ 是 $n \\times n$ 的 Gram 矩阵，其元素为 $K_{ij} = k(x_i, x_j)$，$\\alpha \\in \\mathbb{R}^n$ 是对偶系数向量。\n\n相对于 $\\alpha$ 要最小化的目标函数是：\n$$\nJ(\\alpha) = \\sum_{i=1}^n \\log\\left(1 + \\exp(-t_i f_i)\\right) + \\frac{\\lambda}{2} \\alpha^\\top K \\alpha\n$$\n其中 $f_i = (K\\alpha)_i = \\sum_{j=1}^n K_{ij} \\alpha_j$ 且 $\\lambda > 0$ 是正则化参数。此函数是凸函数，保证了唯一最小值的存在。我们将使用 Newton 方法进行优化。\n\n### II. 梯度与 Hessian 矩阵的推导\n\n为应用 Newton 方法，我们必须计算目标函数的梯度 $\\nabla J(\\alpha)$ 和 Hessian 矩阵 $\\nabla^2 J(\\alpha)$。\n\n#### 梯度推导\n\n梯度是 $\\mathbb{R}^n$ 中的一个向量，其分量为 $\\frac{\\partial J}{\\partial \\alpha_k}$。我们应用链式法则。\n$$\n\\frac{\\partial J}{\\partial \\alpha_k} = \\frac{\\partial}{\\partial \\alpha_k} \\left( \\sum_{i=1}^n \\log(1 + e^{-t_i f_i}) \\right) + \\frac{\\partial}{\\partial \\alpha_k} \\left( \\frac{\\lambda}{2} \\alpha^\\top K \\alpha \\right)\n$$\n损失项关于 $f_i$ 的导数是：\n$$\n\\frac{\\partial}{\\partial f_i} \\log(1 + e^{-t_i f_i}) = \\frac{1}{1 + e^{-t_i f_i}} \\cdot (-t_i e^{-t_i f_i}) = -t_i \\frac{e^{-t_i f_i}}{1 + e^{-t_i f_i}} = -t_i \\frac{1}{1 + e^{t_i f_i}} = -t_i \\sigma(-t_i f_i)\n$$\n其中 $\\sigma(z) = (1+e^{-z})^{-1}$ 是 sigmoid 函数。\n$f_i$ 关于 $\\alpha_k$ 的导数是：\n$$\n\\frac{\\partial f_i}{\\partial \\alpha_k} = \\frac{\\partial}{\\partial \\alpha_k} \\sum_{j=1}^n K_{ij} \\alpha_j = K_{ik}\n$$\n对损失部分使用链式法则，将它们结合起来：\n$$\n\\frac{\\partial}{\\partial \\alpha_k} \\left( \\sum_{i=1}^n \\log(1 + e^{-t_i f_i}) \\right) = \\sum_{i=1}^n \\frac{\\partial \\log(1+e^{-t_i f_i})}{\\partial f_i} \\frac{\\partial f_i}{\\partial \\alpha_k} = \\sum_{i=1}^n \\left( -t_i \\sigma(-t_i f_i) \\right) K_{ik}\n$$\n正则化项的梯度对于二次型是标准的（注意 $K$ 是对称的）：\n$$\n\\frac{\\partial}{\\partial \\alpha_k} \\left( \\frac{\\lambda}{2} \\alpha^\\top K \\alpha \\right) = \\lambda (K\\alpha)_k = \\lambda \\sum_{i=1}^n K_{ki} \\alpha_i\n$$\n结合两项，梯度的第 $k$ 个分量是：\n$$\n\\nabla_k J(\\alpha) = \\sum_{i=1}^n K_{ki} (-t_i \\sigma(-t_i f_i)) + \\lambda (K\\alpha)_k\n$$\n在矩阵-向量表示法中，令 $g_f$ 是一个向量，其元素为 $(g_f)_i = -t_i \\sigma(-t_i f_i)$。梯度是：\n$$\n\\nabla J(\\alpha) = K g_f + \\lambda K \\alpha = K(g_f + \\lambda \\alpha)\n$$\n\n#### Hessian 矩阵推导\n\nHessian 矩阵是一个 $n \\times n$ 矩阵，其元素为 $H_{k\\ell} = \\frac{\\partial^2 J}{\\partial \\alpha_k \\partial \\alpha_\\ell}$。我们将梯度分量对 $\\alpha_\\ell$ 求导：\n$$\n\\frac{\\partial^2 J}{\\partial \\alpha_k \\partial \\alpha_\\ell} = \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\sum_{i=1}^n K_{ki} (-t_i \\sigma(-t_i f_i)) \\right) + \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\lambda (K\\alpha)_k \\right)\n$$\n对于损失项，我们再次使用链式法则，对 $f_j$ 求导：\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} (-t_i \\sigma(-t_i f_i)) = \\sum_{j=1}^n \\frac{\\partial (-t_i \\sigma(-t_i f_i))}{\\partial f_j} \\frac{\\partial f_j}{\\partial \\alpha_\\ell}\n$$\n该项仅依赖于 $f_i$，因此对 $j$ 的求和简化为 $j=i$。导数是：\n$$\n\\frac{d}{df_i} (-t_i \\sigma(-t_i f_i)) = -t_i \\cdot \\frac{d}{df_i} \\sigma(-t_i f_i) = -t_i \\cdot \\left[ \\sigma'(-t_i f_i) \\cdot (-t_i) \\right] = t_i^2 \\sigma'(-t_i f_i) = \\sigma'(-t_i f_i)\n$$\n使用性质 $\\sigma'(z) = \\sigma(z)(1-\\sigma(z)) = \\sigma(z)\\sigma(-z)$：\n$$\n\\frac{d}{df_i} (-t_i \\sigma(-t_i f_i)) = \\sigma(-t_i f_i)(1 - \\sigma(-t_i f_i)) = \\sigma(-t_i f_i)\\sigma(t_i f_i)\n$$\n令 $W$ 是一个对角矩阵，其对角元素为 $W_{ii} = \\sigma(t_i f_i)\\sigma(-t_i f_i)$。\n梯度分量 $\\nabla_k J(\\alpha)$ 中损失部分的导数是：\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} \\sum_{i=1}^n K_{ki}(-t_i\\sigma(-t_i f_i)) = \\sum_{i=1}^n K_{ki} W_{ii} K_{i\\ell} = (K W K)_{k\\ell}\n$$\n梯度中正则化部分的导数是：\n$$\n\\frac{\\partial}{\\partial \\alpha_\\ell} (\\lambda (K\\alpha)_k) = \\frac{\\partial}{\\partial \\alpha_\\ell} \\left( \\lambda \\sum_{j=1}^n K_{kj} \\alpha_j \\right) = \\lambda K_{k\\ell}\n$$\n结合各项，Hessian 矩阵是：\n$$\n\\nabla^2 J(\\alpha) = K W K + \\lambda K\n$$\n\n### III. 算法设计\n\n#### Newton 方法\n\n在每次迭代 $m$ 中，我们通过求解线性系统来找到搜索方向 $\\Delta\\alpha^{(m)}$：\n$$\n\\left(\\nabla^2 J(\\alpha^{(m)}) + \\epsilon I\\right) \\Delta\\alpha^{(m)} = -\\nabla J(\\alpha^{(m)})\n$$\n其中 $\\epsilon = 10^{-10}$ 是一个小的正则化项，以确保数值稳定性。然后更新系数向量：\n$$\n\\alpha^{(m+1)} = \\alpha^{(m)} + \\eta \\Delta\\alpha^{(m)}\n$$\n步长 $\\eta$ 由回溯线搜索确定。\n\n#### 回溯线搜索\n\n为保证目标函数 $J(\\alpha)$ 的下降，我们从完整步长 $\\eta=1$ 开始。如果 $J(\\alpha + \\eta\\Delta\\alpha) \\ge J(\\alpha)$，我们重复将步长减半，即 $\\eta \\leftarrow 0.5\\eta$，直到满足条件 $J(\\alpha + \\eta\\Delta\\alpha) < J(\\alpha)$。\n\n#### 核岭回归热启动\n\n核岭回归（KRR）可以看作是逻辑斯谛回归的一个近似，它最小化的是平方损失而非对数似然损失。其对偶系数 $\\alpha_{\\mathrm{krr}}$ 来自于最小化目标函数 $\\sum_i (f(x_i) - t_i)^2 + \\lambda \\|f\\|^2_{\\mathcal{H}}$，这导出了一个闭式解：\n$$\n(K + \\lambda I) \\alpha_{\\mathrm{krr}} = t\n$$\n其中 $t$ 是标签向量 $\\{ -1, +1 \\}$。解 $\\alpha_{\\mathrm{krr}}$ 为 KLR 优化提供了一个良好的初始猜测，因为 KRR 可以看作是 KLR 的一个近似。我们比较从 $\\alpha^{(0)} = 0$ 开始与从 $\\alpha^{(0)} = \\alpha_{\\mathrm{krr}}$ 开始的 Newton 方法的收敛性。每种情况下收敛所需的迭代次数都会被记录下来。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Implements and compares Kernel Logistic Regression solvers.\n    \"\"\"\n\n    # --- Helper Function for Data Generation ---\n    def generate_data(seed, n, mu_pos, mu_neg, sigma):\n        \"\"\"Generates a two-class Gaussian blob dataset.\"\"\"\n        rng = np.random.default_rng(seed)\n        n_pos = n // 2\n        n_neg = n - n_pos\n        \n        # Draw samples from two multivariate normal distributions\n        X_pos = rng.multivariate_normal(mu_pos, sigma**2 * np.eye(2), n_pos)\n        X_neg = rng.multivariate_normal(mu_neg, sigma**2 * np.eye(2), n_neg)\n        \n        # Combine data and create labels\n        X = np.vstack((X_pos, X_neg))\n        t = np.hstack((np.ones(n_pos), -np.ones(n_neg)))\n        \n        return X, t\n\n    # --- Helper Function for RBF Kernel ---\n    def rbf_kernel(X1, X2, length_scale):\n        \"\"\"Computes the RBF (Gaussian) kernel matrix.\"\"\"\n        # Calculate squared Euclidean distances between all pairs of points\n        sq_dists = cdist(X1, X2, 'sqeuclidean')\n        # Apply the kernel function\n        return np.exp(-sq_dists / (2 * length_scale**2))\n\n    # --- Helper Function for Kernel Ridge Regression ---\n    def solve_krr(K, t, lambda_reg):\n        \"\"\"Solves for the dual coefficients of Kernel Ridge Regression.\"\"\"\n        n = K.shape[0]\n        # Solve the linear system (K + lambda*I) * alpha = t\n        A = K + lambda_reg * np.eye(n)\n        alpha_krr = np.linalg.solve(A, t)\n        return alpha_krr\n\n    # --- Kernel Logistic Regression Solver Class ---\n    class KernelLogisticRegression:\n        \"\"\"\n        Implements Kernel Logistic Regression using Newton's method with backtracking.\n        \"\"\"\n        def __init__(self, K, t, lambda_reg, tol=1e-6, max_iter=50, hessian_eps=1e-10):\n            self.K = K\n            self.t = t\n            self.lambda_reg = lambda_reg\n            self.tol = tol\n            self.max_iter = max_iter\n            self.hessian_eps = hessian_eps\n            self.n = K.shape[0]\n\n        def _objective(self, alpha):\n            \"\"\"Computes the KLR objective function J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # Use np.logaddexp for numerical stability: log(1+exp(-x))\n            log_likelihood = np.sum(np.logaddexp(0, -tf))\n            regularization = (self.lambda_reg / 2.0) * (alpha @ self.K @ alpha)\n            return log_likelihood + regularization\n\n        def _gradient(self, alpha):\n            \"\"\"Computes the gradient of J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # Gradient of loss w.r.t. f is -t * sigma(-t*f)\n            g_f = -self.t * expit(-tf)\n            grad = self.K @ (g_f + self.lambda_reg * alpha)\n            return grad\n\n        def _hessian(self, alpha):\n            \"\"\"Computes the Hessian of J(alpha).\"\"\"\n            f = self.K @ alpha\n            tf = self.t * f\n            # W_ii = sigma(t_i*f_i) * (1 - sigma(t_i*f_i)) = sigma(t_i*f_i) * sigma(-t_i*f_i)\n            # expit(x) is the sigmoid function sigma(x)\n            w_diag = expit(tf) * expit(-tf)\n            W = np.diag(w_diag)\n            hess = self.K @ W @ self.K + self.lambda_reg * self.K\n            return hess\n\n        def solve(self, alpha0):\n            \"\"\"Performs optimization starting from alpha0.\"\"\"\n            alpha = np.copy(alpha0)\n            \n            for i in range(self.max_iter):\n                grad = self._gradient(alpha)\n                \n                # Termination condition: gradient norm\n                if np.linalg.norm(grad) = self.tol:\n                    return i\n                \n                # Compute search direction\n                hess = self._hessian(alpha)\n                H_reg = hess + self.hessian_eps * np.eye(self.n)\n                \n                try:\n                    delta_alpha = np.linalg.solve(H_reg, -grad)\n                except np.linalg.LinAlgError:\n                    # Fallback if regularized Hessian is still numerically singular\n                    return i + 1\n\n                # Backtracking line search with factor 0.5\n                eta = 1.0\n                J_current = self._objective(alpha)\n                while True:\n                    alpha_new = alpha + eta * delta_alpha\n                    J_new = self._objective(alpha_new)\n                    \n                    if J_new  J_current:\n                        break # Found a step size that improves objective\n                    \n                    eta *= 0.5\n                    \n                    # Failsafe to prevent infinitely small steps\n                    if eta  1e-12:\n                        break\n                \n                if eta  1e-12:\n                    # Could not find a descent direction, terminate\n                    return i + 1\n\n                alpha = alpha + eta * delta_alpha\n\n            return self.max_iter\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        # Case A: Happy path, well-separated data\n        {'seed': 0, 'n': 60, 'mu_pos': (1, 1), 'mu_neg': (-1, -1), 'sigma': 0.3, 'l': 1.0, 'lambda_reg': 1e-2},\n        # Case B: Near-constant kernel edge case\n        {'seed': 1, 'n': 60, 'mu_pos': (0.7, -0.7), 'mu_neg': (-0.7, 0.7), 'sigma': 0.4, 'l': 5.0, 'lambda_reg': 1e-2},\n        # Case C: High overlap, non-linear case\n        {'seed': 2, 'n': 80, 'mu_pos': (0.5, 0.5), 'mu_neg': (-0.5, -0.5), 'sigma': 0.8, 'l': 0.5, 'lambda_reg': 1e-2},\n    ]\n\n    all_results = []\n    \n    # --- Main Loop to Process Test Cases ---\n    for case in test_cases:\n        # Generate data and kernel matrix\n        X, t = generate_data(case['seed'], case['n'], case['mu_pos'], case['mu_neg'], case['sigma'])\n        K = rbf_kernel(X, X, case['l'])\n    \n        # Initialize the KLR solver\n        klr_solver = KernelLogisticRegression(K, t, case['lambda_reg'])\n        \n        # Run 1: Initialize with alpha = 0\n        alpha0_zero = np.zeros(case['n'])\n        I0 = klr_solver.solve(alpha0_zero)\n        \n        # Run 2: Initialize with KRR warm start\n        alpha_krr = solve_krr(K, t, case['lambda_reg'])\n        Ikrr = klr_solver.solve(alpha_krr)\n        \n        # Determine if warm-start was better\n        B = 1 if Ikrr  I0 else 0\n        \n        all_results.append(f\"[{I0},{Ikrr},{B}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3136166"}, {"introduction": "超越使用标准核函数的范畴，我们可以主动设计核函数来集成关于数据的先验知识，例如数据中存在的对称性。本实践将引导您构建一个对输入信号的符号翻转保持“不变性”的核，这种技术在处理具有相位或方向模糊性的数据时尤其有用。通过将对称性直接编码到核中，您可以构建出更稳健、数据效率更高的模型 [@problem_id:3136231]。", "problem": "考虑设计一种支持向量机 (SVM) 之外的核方法，该方法能编码输入表示在全局符号翻转下的不变性。设输入域为 $\\mathbb{R}^d$ 中的向量，并考虑二元群 $\\{+1,-1\\}$ 通过 $x \\mapsto s x$（其中 $s \\in \\{+1,-1\\}$）在 $\\mathbb{R}^d$ 上的作用。目标是在 $\\mathbb{R}^d$ 上构建一个正定核，当任意输入被其符号翻转版本替换时，该核产生相同的值，然后在一个非支持向量机的方法中使用该核来解决一个涉及具有相位不确定性的波形的监督学习问题。\n\n从以下基本依据出发：\n1. 一个函数 $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ 是一个正定核，当且仅当对于任意有限集合 $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ 和任意实系数 $\\{c_i\\}_{i=1}^n$，都有 $\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i,x_j) \\geq 0$ 成立。\n2. 一个正定核 $k$ 定义了一个再生核希尔伯特空间 (RKHS) $\\mathcal{H}$，使得对于任意 $f \\in \\mathcal{H}$ 和 $x \\in \\mathcal{X}$，再生性质成立：$f(x) = \\langle f, k(\\cdot, x) \\rangle_{\\mathcal{H}}$。\n3. 核岭回归 (KRR) 通过最小化 $\\sum_{i=1}^n (f(x_i) - y_i)^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2$ 来在再生核希尔伯特空间中求解正则化经验风险最小化问题，其中正则化参数 $\\lambda  0$。\n\n您的任务如下：\nA. 基于上述依据，推导在一个已知正定核 $\\kappa$（定义在某个空间 $\\mathcal{Z}$ 上）与一个变换 $\\phi: \\mathcal{X} \\to \\mathcal{Z}$ 复合后，所得到的核 $k(x,y) = \\kappa(\\phi(x), \\phi(y))$ 在 $\\mathcal{X}$ 上成为正定核的条件。利用此条件来论证一种编码全局符号翻转不变性的设计。\nB. 在 $\\mathbb{R}^d$ 上实现两个核：一个基准高斯径向基函数 (RBF) 核，以及一个在 $x \\mapsto -x$ 变换下保持不变的变换核。基准核必须是 $k_{\\text{rbf}}(x,y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$，其中带宽 $\\sigma  0$ 为选定值。不变核必须通过一种有原则的变换来构建，以确保在上述群作用下的不变性。\nC. 使用每个核为标签在 $\\{-1,+1\\}$ 内的二元分类问题实现核岭回归 (KRR)。给定训练数据 $\\{(x_i, y_i)\\}_{i=1}^n$，KRR 预测器必须具有形式 $f(\\cdot) = \\sum_{i=1}^n \\alpha_i k(x_i, \\cdot)$，其中系数满足 $(K + \\lambda I)\\alpha = y$，并且 $K_{ij} = k(x_i, x_j)$，$\\lambda  0$。\nD. 使用 $x(t) = \\sin(2\\pi f t + \\varphi)$ 的样本为两个类别生成合成波形数据，其中未知相位 $\\varphi$ 在 $[0, 2\\pi)$ 上均匀采样，并在 $[0,1)$ 内的 $L$ 个等间隔时间点上进行测量。对于每个波形，以概率 $p \\in [0,1]$ 应用随机全局符号翻转以模拟传感器方向模糊性，并添加标准差为 $\\sigma_{\\text{noise}}  0$ 的独立高斯噪声。将每个波形归一化为单位 $\\ell_2$ 范数。使用两个不同的整数频率来定义类别，并分别分配标签 $+1$ 和 $-1$。\nE. 在不同的相位不确定性、符号翻转和噪声情况下，使用基准核和不变核评估分类准确率。\n\n实现一个单一程序，使用以下测试套件的参数值。对于每个案例，请精确使用指定的值，并固定随机数生成器种子以确保可复现性。此问题不涉及物理单位，所有角度均以弧度为单位。\n\n测试案例 1（具有中等不确定性的一般情况）：\n- 每个类别的训练样本数：60。\n- 每个类别的测试样本数：200。\n- 每个波形的时间采样点数：128。\n- 频率：标签为 $+1$ 时 $f_{+} = 3$，标签为 $-1$ 时 $f_{-} = 7$。\n- 相位分布：在 $[0, 2\\pi)$ 上均匀分布。\n- 符号翻转概率：$p = 0.5$。\n- 噪声标准差：$\\sigma_{\\text{noise}} = 0.05$。\n- 核带宽：$\\sigma = 1.0$。\n- 正则化：$\\lambda = 10^{-3}$。\n- 随机种子：$12345$。\n\n测试案例 2（具有最大符号不确定性且无噪声的边界情况）：\n- 每个类别的训练样本数：20。\n- 每个类别的测试样本数：200。\n- 每个波形的时间采样点数：64。\n- 频率：标签为 $+1$ 时 $f_{+} = 2$，标签为 $-1$ 时 $f_{-} = 5$。\n- 相位分布：在 $[0, 2\\pi)$ 上均匀分布。\n- 符号翻转概率：$p = 1.0$。\n- 噪声标准差：$\\sigma_{\\text{noise}} = 0.0$。\n- 核带宽：$\\sigma = 1.0$。\n- 正则化：$\\lambda = 10^{-3}$。\n- 随机种子：$54321$。\n\n测试案例 3（用于不变性的边缘情况属性检查）：\n- 构建一个单一波形，其中 $L = 256$，频率 $f = 4$，相位 $\\varphi = 0$，无噪声，无符号翻转。\n- 计算该波形与其符号翻转版本之间的不变核值，并将其与该波形与自身之间的不变核值进行比较。\n- 核带宽：$\\sigma = 1.0$。\n\n您的程序必须生成单行输出，其中包含一个逗号分隔的列表形式的结果，并用方括号括起来，顺序如下：\n- 对于测试案例 1：基准分类准确率（浮点数），不变核分类准确率（浮点数）。\n- 对于测试案例 2：基准分类准确率（浮点数），不变核分类准确率（浮点数）。\n- 对于测试案例 3：一个布尔值，指示不变核对于一个波形及其符号翻转版本是否产生与该波形与自身相同的值（使用 $10^{-9}$ 的数值容差）。\n\n例如，最终输出格式必须为 $[result_1,result_2,result_3,result_4,result_5]$，其中 $result_1$ 到 $result_4$ 是浮点数，$result_5$ 是布尔值。", "solution": "该问题陈述是有效的。它在科学上基于统计学习理论，定义明确，包含了所有必要的参数和定义，并且其目标以数学精度进行了陈述。各项任务逻辑结构清晰，从理论推导开始，接着是实现，最后以实证评估结束，构成了一个完整且可验证的核方法练习。\n\n### A. 不变核的推导与设计\n\n第一个任务是建立一种构造新的正定 (PD) 核的方法，然后应用它来设计一个对全局符号翻转不变的核。\n\n一个函数 $k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ 是一个正定核，如果对于任何有限点集 $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ 和任何实系数 $\\{c_i\\}_{i=1}^n$，以下条件成立：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\geq 0\n$$\n设 $\\kappa: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{R}$ 是空间 $\\mathcal{Z}$ 上的一个已知正定核，并设 $\\phi: \\mathcal{X} \\to \\mathcal{Z}$ 是一个任意函数或特征映射。我们可以通过复合定义 $\\mathcal{X}$ 上的一个新核 $k$：$k(x, y) = \\kappa(\\phi(x), \\phi(y))$。为了证明 $k$ 也是 $\\mathcal{X}$ 上的一个正定核，我们取任意有限集 $\\{x_i\\}_{i=1}^n \\subset \\mathcal{X}$ 和系数 $\\{c_i\\}_{i=1}^n \\subset \\mathbb{R}$，并检验其二次型：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) = \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\kappa(\\phi(x_i), \\phi(x_j))\n$$\n让我们定义一组点 $\\{z_i\\}_{i=1}^n \\subset \\mathcal{Z}$，其中 $z_i = \\phi(x_i)$。该表达式变为：\n$$\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\kappa(z_i, z_j)\n$$\n由于 $\\kappa$ 是 $\\mathcal{Z}$ 上的正定核，根据定义，这个和保证是非负的。因此，对于任何特征映射 $\\phi$，$k(x, y) = \\kappa(\\phi(x), \\phi(y))$ 都是 $\\mathcal{X}$ 上的一个有效正定核。\n\n为了编码在群作用 $x \\mapsto s x$（其中 $s \\in G = \\{+1, -1\\}$）下的不变性，我们寻求一个核 $k(x, y)$，使得对于所有 $s \\in G$，都有 $k(sx, y) = k(x, y)$ 和 $k(x, sy) = k(x, y)$。使用复合形式 $k(x, y) = \\kappa(\\phi(x), \\phi(y))$，实现这一目标的一个直接方法是设计一个本身就是不变的特征映射 $\\phi$，即 $\\phi(x) = \\phi(-x)$。例如，如果我们选择 $\\phi(x) = xx^T$，它将 $\\mathbb{R}^d$ 中的一个向量映射到 $\\mathbb{R}^{d \\times d}$ 中的一个矩阵，我们有 $\\phi(-x) = (-x)(-x)^T = xx^T = \\phi(x)$。如果我们随后在矩阵空间上使用线性核 $\\kappa(A, B) = \\langle A, B \\rangle_F = \\text{tr}(A^TB)$，我们得到不变核 $k(x, y) = \\text{tr}((xx^T)^T(yy^T)) = (x^Ty)^2$，这是 2 次齐次多项式核。\n\n一种更通用、更强大的构造不变核的技术是在群作用上对给定的基核进行平均。设 $k_{\\text{base}}$ 是任意一个正定核。一个不变核 $k_{\\text{inv}}$ 可以通过对称化来构造：\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{|G|^2} \\sum_{s_1 \\in G} \\sum_{s_2 \\in G} k_{\\text{base}}(s_1 x, s_2 y)\n$$\n对于我们的群 $G = \\{+1, -1\\}$，这展开为：\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{4} [k_{\\text{base}}(x, y) + k_{\\text{base}}(x, -y) + k_{\\text{base}}(-x, y) + k_{\\text{base}}(-x, -y)]\n$$\n这个新核是多个正定核乘以一个正常数 ($1/4$) 的和，因此它也是一个正定核。问题指定使用高斯 RBF 核作为基核：$k_{\\text{rbf}}(x, y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$。这个核具有性质 $k_{\\text{rbf}}(-u, -v) = \\exp\\left(-\\frac{\\lVert -u-(-v) \\rVert_2^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\lVert -(u-v) \\rVert_2^2}{2\\sigma^2}\\right) = k_{\\text{rbf}}(u, v)$。同样地，$k_{\\text{rbf}}(-x, y) = \\exp\\left(-\\frac{\\lVert -x-y \\rVert_2^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\lVert x+y \\rVert_2^2}{2\\sigma^2}\\right) = k_{\\text{rbf}}(x, -y)$。将这些对称性代入总和中得到：\n$$\nk_{\\text{inv}}(x, y) = \\frac{1}{4} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y) + k_{\\text{rbf}}(x, -y) + k_{\\text{rbf}}(x, y)] = \\frac{1}{2} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y)]\n$$\n这将是我们实现的不变核。它正确地对第二个参数的可能符号进行了平均，并且其对称性也确保了对第一个参数的不变性。\n\n### B. 核的实现\n\n我们在 $\\mathbb{R}^d$ 上实现两个核：\n1.  **基准 RBF 核**：$k_{\\text{rbf}}(x, y) = \\exp\\left(-\\frac{\\lVert x-y \\rVert_2^2}{2\\sigma^2}\\right)$。这是一个标准选择，对 $x$ 和 $y$ 的相对位置敏感。\n2.  **不变 RBF 核**：$k_{\\text{inv}}(x, y) = \\frac{1}{2} [k_{\\text{rbf}}(x, y) + k_{\\text{rbf}}(x, -y)]$。这个核有效地计算了点对 $\\{x, -x\\}$ 和点对 $\\{y, -y\\}$ 之间的相似度，使其对输入的全局符号翻转不敏感。\n\n### C. 核岭回归 (KRR) 的实现\n\nKRR 在与核 $k$ 关联的 RKHS $\\mathcal{H}$ 中寻找一个函数 $f$，该函数最小化正则化最小二乘误差：$\\min_{f \\in \\mathcal{H}} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2$。表示定理 (Representer Theorem) 保证了解的形式为 $f(\\cdot) = \\sum_{i=1}^n \\alpha_i k(x_i, \\cdot)$。将此形式代入目标函数并求解系数 $\\alpha = (\\alpha_1, \\dots, \\alpha_n)^T$，可得到线性系统：\n$$\n(K + \\lambda I)\\alpha = y\n$$\n其中 $K$ 是 $n \\times n$ 的格拉姆矩阵 (Gram matrix)，其元素为 $K_{ij} = k(x_i, x_j)$，$y = (y_1, \\dots, y_n)^T$ 是标签向量，$I$ 是单位矩阵。训练过程包括计算 $K$ 并求解此系统以获得 $\\alpha$。对于新数据点 $x_{\\text{test}}$ 的预测，我们计算函数值 $f(x_{\\text{test}}) = \\sum_{i=1}^n \\alpha_i k(x_i, x_{\\text{test}})$。对于标签为 $\\{-1, +1\\}$ 的二元分类，预测类别为 $\\text{sign}(f(x_{\\text{test}}))$。\n\n### D. 合成数据的生成\n\n我们生成合成波形数据来模拟具有相位和符号模糊性的情景。波形是 $\\mathbb{R}^L$ 中的一个向量，通过在 $L$ 个等间隔时间点 $t \\in [0, 1)$ 上对 $x(t) = \\sin(2\\pi f t + \\varphi)$ 进行采样获得。两个类别由不同的频率 $f_+$ 和 $f_-$ 定义。每个样本的生成过程如下：\n1.  根据类别标签选择一个频率（$f_+$ 或 $f_-$）。\n2.  从均匀分布 $U[0, 2\\pi)$ 中采样一个相位 $\\varphi$。\n3.  生成无噪声波形 $x_{\\text{clean}}(t) = \\sin(2\\pi f t + \\varphi)$。\n4.  添加独立同分布的高斯噪声：$x_{\\text{noisy}} = x_{\\text{clean}} + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I)$。\n5.  应用随机符号翻转：以概率 $p$ 将 $x_{\\text{noisy}}$ 替换为 $-x_{\\text{noisy}}$。\n6.  将最终向量归一化，使其具有单位 $\\ell_2$ 范数：$x = x_{\\text{final}}/\\lVert x_{\\text{final}} \\rVert_2$。这一步至关重要，因为它确保所有输入都位于单位超球面上，使得像 RBF 这样的基于距离的核仅依赖于向量之间的夹角。\n\n### E. 评估程序\n\n在两种具有不同不确定性水平的测试案例中，评估了使用基准核和不变核的 KRR 的性能。对于每个案例，我们生成训练和测试数据集。我们在训练集上为每个核训练两个 KRR 模型，通过计算它们各自的 $\\alpha$ 向量。然后我们预测测试集的标签并计算分类准确率，定义为正确预测标签的比例。对于第三个测试案例，我们通过直接比较特定波形 $x$ 的 $k_{\\text{inv}}(x, x)$ 和 $k_{\\text{inv}}(x, -x)$ 来验证 $k_{\\text{inv}}$ 的不变性属性，确认它们在一个小的数值容差内相等。这验证了理论构造。预期不变核的性能将优于基准核，特别是在符号翻转概率 $p$ 很高时，因为它被设计为对这种特定的数据扰动免疫。", "answer": "```python\nimport numpy as np\n\ndef k_rbf(x, y, sigma):\n    \"\"\"Computes the Gaussian RBF kernel between two vectors.\"\"\"\n    norm_sq = np.sum((x - y)**2)\n    return np.exp(-norm_sq / (2 * sigma**2))\n\ndef k_inv(x, y, sigma):\n    \"\"\"Computes the sign-invariant kernel based on the RBF kernel.\"\"\"\n    val1 = k_rbf(x, y, sigma)\n    val2 = k_rbf(x, -y, sigma)\n    return 0.5 * (val1 + val2)\n\ndef compute_gram_matrix(X, kernel_func, sigma):\n    \"\"\"Computes the Gram matrix for a dataset X and a given kernel.\"\"\"\n    n_samples = X.shape[0]\n    K = np.zeros((n_samples, n_samples))\n    for i in range(n_samples):\n        for j in range(i, n_samples):\n            val = kernel_func(X[i], X[j], sigma)\n            K[i, j] = val\n            K[j, i] = val\n    return K\n\ndef train_krr(K, y, lambda_reg):\n    \"\"\"Trains Kernel Ridge Regression by solving for alpha.\"\"\"\n    n_samples = K.shape[0]\n    I = np.eye(n_samples)\n    alpha = np.linalg.solve(K + lambda_reg * I, y)\n    return alpha\n\ndef predict_krr(X_test, X_train, alpha, kernel_func, sigma):\n    \"\"\"Makes predictions using a trained KRR model.\"\"\"\n    n_test = X_test.shape[0]\n    n_train = X_train.shape[0]\n    \n    K_cross = np.zeros((n_test, n_train))\n    for i in range(n_test):\n        for j in range(n_train):\n            K_cross[i, j] = kernel_func(X_test[i], X_train[j], sigma)\n            \n    f_values = K_cross @ alpha\n    y_pred = np.sign(f_values)\n    # Handle the case where f_value is 0; classify as +1 by convention.\n    y_pred[y_pred == 0] = 1\n    return y_pred\n\ndef generate_data(n_samples_class, L, f_pos, f_neg, p_flip, sigma_noise, rng):\n    \"\"\"Generates synthetic waveform data for two classes.\"\"\"\n    t = np.linspace(0, 1, L, endpoint=False)\n    X = []\n    y = []\n\n    for label, freq in [(1, f_pos), (-1, f_neg)]:\n        for _ in range(n_samples_class):\n            phase = rng.uniform(0, 2 * np.pi)\n            waveform = np.sin(2 * np.pi * freq * t + phase)\n            \n            # Add noise\n            noise = rng.normal(0, sigma_noise, L)\n            waveform += noise\n            \n            # Apply sign flip\n            if rng.random()  p_flip:\n                waveform *= -1\n            \n            # Normalize to unit l2-norm\n            norm = np.linalg.norm(waveform)\n            if norm > 1e-9:\n                waveform /= norm\n            \n            X.append(waveform)\n            y.append(label)\n            \n    return np.array(X), np.array(y)\n\ndef run_classification_test(params):\n    \"\"\"Runs a full classification test for a given set of parameters.\"\"\"\n    rng = np.random.default_rng(params['seed'])\n    \n    # Generate training data\n    X_train, y_train = generate_data(\n        params['n_train_class'], params['L'], params['f_pos'], params['f_neg'],\n        params['p_flip'], params['sigma_noise'], rng)\n    \n    # Generate testing data with a separate RNG sequence\n    X_test, y_test = generate_data(\n        params['n_test_class'], params['L'], params['f_pos'], params['f_neg'],\n        params['p_flip'], params['sigma_noise'], rng)\n\n    accuracies = []\n    for kernel_func in [k_rbf, k_inv]:\n        # Train\n        K_train = compute_gram_matrix(X_train, kernel_func, params['sigma'])\n        alpha = train_krr(K_train, y_train, params['lambda_reg'])\n        \n        # Predict and evaluate\n        y_pred = predict_krr(X_test, X_train, alpha, kernel_func, params['sigma'])\n        accuracy = np.mean(y_pred == y_test)\n        accuracies.append(accuracy)\n        \n    return accuracies\n\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    \n    results = []\n\n    # Test Case 1\n    params1 = {\n        'n_train_class': 60, 'n_test_class': 200, 'L': 128,\n        'f_pos': 3, 'f_neg': 7, 'p_flip': 0.5, 'sigma_noise': 0.05,\n        'sigma': 1.0, 'lambda_reg': 1e-3, 'seed': 12345\n    }\n    acc_base_1, acc_inv_1 = run_classification_test(params1)\n    results.extend([acc_base_1, acc_inv_1])\n\n    # Test Case 2\n    params2 = {\n        'n_train_class': 20, 'n_test_class': 200, 'L': 64,\n        'f_pos': 2, 'f_neg': 5, 'p_flip': 1.0, 'sigma_noise': 0.0,\n        'sigma': 1.0, 'lambda_reg': 1e-3, 'seed': 54321\n    }\n    acc_base_2, acc_inv_2 = run_classification_test(params2)\n    results.extend([acc_base_2, acc_inv_2])\n\n    # Test Case 3\n    L3, f3, sigma3 = 256, 4, 1.0\n    t3 = np.linspace(0, 1, L3, endpoint=False)\n    waveform = np.sin(2 * np.pi * f3 * t3)\n    norm = np.linalg.norm(waveform)\n    if norm > 1e-9:\n        waveform /= norm\n\n    v1 = k_inv(waveform, -waveform, sigma3)\n    v2 = k_inv(waveform, waveform, sigma3)\n    \n    is_invariant = np.isclose(v1, v2, atol=1e-9)\n    results.append(is_invariant)\n\n    # Format and print the final output\n    # Convert boolean to lowercase 'true'/'false' as often expected, although\n    # default str() would be 'True'/'False'. Using default for compliance.\n    output_str = [f\"{r:.10f}\" if isinstance(r, float) else str(r) for r in results]\n    print(f\"[{','.join(output_str)}]\")\n\nsolve()\n\n```", "id": "3136231"}, {"introduction": "核方法的应用远不止于监督学习分类和回归任务。本实践将向您介绍一个前沿应用：使用最大均值差异（Maximum Mean Discrepancy, MMD）作为损失函数来训练生成模型。您将推导并实现MMD关于生成器参数的梯度，并用它来优化一个简单的模型，使其输出的分布与目标数据分布相匹配，从而体验核方法在现代生成建模中的威力 [@problem_id:3136216]。", "problem": "要求您使用再生核希尔伯特空间 (RKHS) 中的最大均值差异 (MMD) 目标，为参数化生成器推导并实现一种基于原理的梯度调优程序，重点关注支持向量机之外的核方法。您的生成器是一个一维的简单位置-尺度模拟器，目标是来自单变量正态分布的样本。您必须仅使用核心定义和基本微积分法则，计算经验平方MMD关于生成器参数的梯度，然后使用此梯度通过梯度下降来调优参数。\n\n从以下基本原理出发：\n- 对于核$k$，分布$\\mathbb{P}$和$\\mathbb{Q}$之间的平方最大均值差异 (MMD) 是其核均值嵌入之差的平方RKHS范数：\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) \\equiv \\lVert \\mu_{\\mathbb{P}} - \\mu_{\\mathbb{Q}} \\rVert_{\\mathcal{H}}^2,$$\n其中$\\mu_{\\mathbb{P}} \\equiv \\mathbb{E}_{X \\sim \\mathbb{P}}[k(\\cdot, X)]$ 和 $\\mu_{\\mathbb{Q}} \\equiv \\mathbb{E}_{Y \\sim \\mathbb{Q}}[k(\\cdot, Y)]$ 位于再生核希尔伯特空间 (RKHS) 中。\n- 使用极化恒等式和RKHS内积的双线性，一个广泛使用且经过充分检验的表达式是\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) = \\mathbb{E}_{X,X' \\sim \\mathbb{P}}[k(X,X')] + \\mathbb{E}_{Y,Y' \\sim \\mathbb{Q}}[k(Y,Y')] - 2 \\mathbb{E}_{X \\sim \\mathbb{P}, Y \\sim \\mathbb{Q}}[k(X,Y)]。$$\n- 给定样本 $\\{x_i\\}_{i=1}^n$ 和 $\\{y_j\\}_{j=1}^m$，$\\operatorname{MMD}^2$ 的经验（有偏）估计量是\n$$\\widehat{\\operatorname{MMD}}^2 = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'}) + \\frac{1}{m^2} \\sum_{j=1}^{m}\\sum_{j'=1}^{m} k(y_j,y_{j'}) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j)。$$\n- 带宽为$\\sigma$的高斯（径向基函数）核是 $k(u,v) = \\exp\\!\\left(-\\frac{\\lVert u - v\\rVert^2}{2\\sigma^2}\\right)$。\n- 使用标准多元微积分（链式法则和乘法法则）来微分复合函数。\n\n生成器模型和任务：\n- 考虑一个一维生成器 $g_{\\theta}(z) = \\theta_1 + \\theta_2 z$，其参数向量为 $\\theta = (\\theta_1, \\theta_2)$，基础噪声 $z \\sim \\mathcal{N}(0,1)$ 在优化步骤中保持固定以确保可复现性。\n- 给定固定的目标数据集 $\\{x_i\\}_{i=1}^n$ 和由固定噪声 $\\{z_j\\}_{j=1}^m$ 生成的生成器样本 $\\{y_j\\}_{j=1}^m$（其中 $y_j = g_{\\theta}(z_j)$），使用高斯核推导 $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$，从上述定义出发，不使用快捷公式。\n- 实现一个程序，该程序：\n  - 计算高斯核的经验平方MMD $\\widehat{\\operatorname{MMD}}^2$。\n  - 使用您的推导计算梯度 $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$。\n  - 使用固定的学习率运行梯度下降，更新$\\theta$指定的步数。\n  - 为每个测试用例返回最终调优的参数和最终的经验平方MMD。\n\n测试套件：\n- 使用以下测试用例。对于每个用例，使用提供的随机种子独立生成目标数据 $\\{x_i\\}$ 和固定的生成器噪声 $\\{z_j\\}$。目标数据从单变量正态分布 $\\mathcal{N}(\\mu_{\\mathrm{tgt}}, s_{\\mathrm{tgt}}^2)$ 中抽取，其中 $\\mu_{\\mathrm{tgt}}$ 是目标均值，$s_{\\mathrm{tgt}}$ 是目标标准差。\n  1. 情况A（理想情况）：$\\mu_{\\mathrm{tgt}} = 1.0$, $s_{\\mathrm{tgt}} = 0.5$, $n = 200$, $m = 200$, 初始 $\\theta^{(0)} = (0.3, 0.8)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 200$, 种子 $= 123$。\n  2. 情况B（恒等目标）：$\\mu_{\\mathrm{tgt}} = 0.0$, $s_{\\mathrm{tgt}} = 1.0$, $n = 200$, $m = 200$, 初始 $\\theta^{(0)} = (0.2, 1.5)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 200$, 种子 $= 456$。\n  3. 情况C（平移和重缩放）：$\\mu_{\\mathrm{tgt}} = -1.0$, $s_{\\mathrm{tgt}} = 0.7$, $n = 200$, $m = 200$, 初始 $\\theta^{(0)} = (0.0, 0.5)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 250$, 种子 $= 789$。\n  4. 情况D（小样本鲁棒性）：$\\mu_{\\mathrm{tgt}} = 0.5$, $s_{\\mathrm{tgt}} = 0.9$, $n = 20$, $m = 20$, 初始 $\\theta^{(0)} = (1.0, 0.2)$, 核带宽 $\\sigma = 1.0$, 学习率 $\\alpha = 0.1$, 步数 $T = 150$, 种子 $= 31415$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，没有空格。对于按A、B、C、D顺序的每个测试用例，输出最终参数 $\\theta_1$，然后是 $\\theta_2$，然后是最终的经验平方MMD值，所有值都四舍五入到恰好$6$位小数。因此，最终输出应为一个长度为$12$的扁平列表，形式为\n$[\\theta_{1}^{A},\\theta_{2}^{A},\\widehat{\\operatorname{MMD}}^{2}_{A},\\theta_{1}^{B},\\theta_{2}^{B},\\widehat{\\operatorname{MMD}}^{2}_{B},\\theta_{1}^{C},\\theta_{2}^{C},\\widehat{\\operatorname{MMD}}^{2}_{C},\\theta_{1}^{D},\\theta_{2}^{D},\\widehat{\\operatorname{MMD}}^{2}_{D}]$,\n其中的数值条目四舍五入到$6$位小数，且不含任何附加文本。", "solution": "我们从再生核希尔伯特空间 (RKHS) 中最大均值差异 (MMD) 的定义开始。对于一个正定核 $k$，其RKHS为 $\\mathcal{H}$，核均值嵌入为 $\\mu_{\\mathbb{P}} \\equiv \\mathbb{E}[k(\\cdot, X)]$ 和 $\\mu_{\\mathbb{Q}} \\equiv \\mathbb{E}[k(\\cdot, Y)]$，平方MMD为\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) \\equiv \\lVert \\mu_{\\mathbb{P}} - \\mu_{\\mathbb{Q}} \\rVert_{\\mathcal{H}}^2.$$\n利用RKHS内积的双线性和再生性质，得到广泛使用且经过充分检验的公式\n$$\\operatorname{MMD}^2(\\mathbb{P},\\mathbb{Q}) = \\mathbb{E}_{X,X'}[k(X,X')] + \\mathbb{E}_{Y,Y'}[k(Y,Y')] - 2 \\mathbb{E}_{X,Y}[k(X,Y)],$$\n其中期望是关于独立副本 $X, X' \\sim \\mathbb{P}$ 和 $Y, Y' \\sim \\mathbb{Q}$ 计算的。对于经验样本 $\\{x_i\\}_{i=1}^{n}$ 和 $\\{y_j\\}_{j=1}^{m}$，有偏估计量为\n$$\\widehat{\\operatorname{MMD}}^2 = \\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'}) + \\frac{1}{m^2} \\sum_{j=1}^{m}\\sum_{j'=1}^{m} k(y_j,y_{j'}) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j)。$$\n\n我们考虑一个一维生成器 $g_{\\theta}(z) = \\theta_1 + \\theta_2 z$，其基础噪声为 $z \\sim \\mathcal{N}(0,1)$。对于固定的噪声样本 $\\{z_j\\}_{j=1}^{m}$，令 $y_j(\\theta) = \\theta_1 + \\theta_2 z_j$。我们使用高斯核\n$$k(u,v) = \\exp\\!\\left(-\\frac{(u-v)^2}{2\\sigma^2}\\right),$$ \n其带宽为 $\\sigma  0$。经验平方MMD是 $\\theta$ 的可微函数，这是通过对 $\\{y_j(\\theta)\\}_{j=1}^{m}$ 的依赖性实现的。\n\n我们现在从第一性原理推导 $\\nabla_{\\theta} \\widehat{\\operatorname{MMD}}^2$。令 $L(\\theta) \\equiv \\widehat{\\operatorname{MMD}}^2(\\{x_i\\}, \\{y_j(\\theta)\\})$。根据链式法则，\n$$\\nabla_{\\theta} L(\\theta) = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\frac{\\partial y_j}{\\partial \\theta},$$ \n其中 $\\frac{\\partial y_j}{\\partial \\theta} = \\begin{bmatrix} \\frac{\\partial y_j}{\\partial \\theta_1} \\\\ \\frac{\\partial y_j}{\\partial \\theta_2} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ z_j \\end{bmatrix}$。\n\n因此，只需计算 $\\frac{\\partial L}{\\partial y_j}$。使用经验表达式，\n$$L(\\theta) = \\underbrace{\\frac{1}{n^2} \\sum_{i=1}^{n}\\sum_{i'=1}^{n} k(x_i,x_{i'})}_{\\text{常数，与 } \\theta \\text{ 无关}} + \\frac{1}{m^2} \\sum_{a=1}^{m}\\sum_{b=1}^{m} k(y_a,y_b) - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j)。$$\n第一项不依赖于 $\\theta$。对于第二项，关于 $y_j$ 的导数汇集了当 $y_j$ 作为第一或第二参数时产生的贡献：\n$$\\frac{\\partial}{\\partial y_j} \\left( \\frac{1}{m^2} \\sum_{a=1}^{m}\\sum_{b=1}^{m} k(y_a,y_b) \\right) = \\frac{1}{m^2} \\left( \\sum_{b=1}^{m} \\frac{\\partial}{\\partial y_j} k(y_j, y_b) + \\sum_{a=1}^{m} \\frac{\\partial}{\\partial y_j} k(y_a, y_j) \\right)。$$\n对于高斯核，关于第二个参数的梯度等于\n$$\\frac{\\partial}{\\partial v} k(u,v) = k(u,v)\\cdot \\frac{u - v}{\\sigma^2}。$$\n因此，\n$$\\frac{\\partial}{\\partial y_j} k(y_j, y_b) = k(y_j, y_b) \\cdot \\frac{y_b - y_j}{\\sigma^2}, \\quad \\frac{\\partial}{\\partial y_j} k(y_a, y_j) = k(y_a, y_j) \\cdot \\frac{y_a - y_j}{\\sigma^2}。$$\n根据对称性 $k(y_j,y_b) = k(y_b,y_j)$，这两个和具有相同的形式，合计为：\n$$\\frac{\\partial}{\\partial y_j} \\left( \\frac{1}{m^2} \\sum_{a,b} k(y_a,y_b) \\right) = \\frac{2}{m^2 \\sigma^2} \\sum_{b=1}^{m} k(y_j, y_b)\\, (y_b - y_j)。$$\n对于交叉项，\n$$\\frac{\\partial}{\\partial y_j} \\left( - \\frac{2}{nm} \\sum_{i=1}^{n}\\sum_{j=1}^{m} k(x_i,y_j) \\right) = - \\frac{2}{nm} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_j} k(x_i, y_j) = - \\frac{2}{nm \\sigma^2} \\sum_{i=1}^{n} k(x_i, y_j)\\, (x_i - y_j)。$$\n将这些组合起来，得到关于每个生成器样本的标量导数：\n$$\\frac{\\partial L}{\\partial y_j} = \\frac{2}{m^2 \\sigma^2} \\sum_{b=1}^{m} k(y_j, y_b)\\, (y_b - y_j) \\;-\\; \\frac{2}{nm \\sigma^2} \\sum_{i=1}^{n} k(x_i, y_j)\\, (x_i - y_j)。$$\n\n使用链式法则，对于 $y_j(\\theta) = \\theta_1 + \\theta_2 z_j$ 可得\n$$\\frac{\\partial L}{\\partial \\theta_1} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot 1, \\quad \\frac{\\partial L}{\\partial \\theta_2} = \\sum_{j=1}^{m} \\frac{\\partial L}{\\partial y_j} \\cdot z_j。$$\n\n算法实现：\n- 使用两两差异和高斯核计算核矩阵 $K_{YY}$（其元素为 $k(y_j, y_b)$）和 $K_{XY}$（其元素为 $k(x_i, y_j)$）。\n- 向量化求和。设 $y \\in \\mathbb{R}^{m}$ 和 $x \\in \\mathbb{R}^{n}$。定义两两差异矩阵 $\\Delta_{YY}$（元素为 $(y_b - y_j)$）和 $\\Delta_{XY}$（元素为 $(x_i - y_j)$）。那么\n$$\\left[ \\frac{\\partial L}{\\partial y_j} \\right]_{j=1}^{m} = \\frac{2}{m^2 \\sigma^2} \\left( K_{YY} \\odot \\Delta_{YY} \\right) \\mathbf{1}_m \\;-\\; \\frac{2}{nm \\sigma^2} \\left( K_{XY} \\odot \\Delta_{XY} \\right)^{\\top} \\mathbf{1}_n,$$\n其中 $\\odot$ 表示逐元素乘法，$\\mathbf{1}_k$ 是长度为 $k$ 的全1向量；通过矩阵-向量运算对适当的轴进行求和。\n- 分别使用权重 $1$ 和 $z_j$ 求和，累加 $\\frac{\\partial L}{\\partial \\theta_1}$ 和 $\\frac{\\partial L}{\\partial \\theta_2}$。\n- 使用学习率 $\\alpha$ 执行梯度下降更新 $\\theta \\leftarrow \\theta - \\alpha \\nabla_{\\theta} L$，迭代指定的步数。\n\n正确性说明：\n- 出发点是MMD的RKHS定义及其经过充分检验的期望展开式；经验估计量是一致的插件估计。\n- 对经验目标的微分仅使用链式法则和高斯核的导数，高斯核处处光滑。\n- 生成器是从参数到样本的可微映射，因此链式法则直接适用；雅可比矩阵 $\\frac{\\partial y_j}{\\partial \\theta}$ 在固定噪声 $z_j$ 上是线性的。\n- 在迭代中使用固定的 $\\{x_i\\}$ 和 $\\{z_j\\}$ 确保了目标函数是确定且可微的，适合梯度下降。\n\n数值规格和输出：\n- 完全按照四种情况的规定实现梯度下降：\n  1. 情况A：$\\mu_{\\mathrm{tgt}} = 1.0$, $s_{\\mathrm{tgt}} = 0.5$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.3, 0.8)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 200$, 种子 $= 123$。\n  2. 情况B：$\\mu_{\\mathrm{tgt}} = 0.0$, $s_{\\mathrm{tgt}} = 1.0$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.2, 1.5)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 200$, 种子 $= 456$。\n  3. 情况C：$\\mu_{\\mathrm{tgt}} = -1.0$, $s_{\\mathrm{tgt}} = 0.7$, $n = 200$, $m = 200$, $\\theta^{(0)} = (0.0, 0.5)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 250$, 种子 $= 789$。\n  4. 情况D：$\\mu_{\\mathrm{tgt}} = 0.5$, $s_{\\mathrm{tgt}} = 0.9$, $n = 20$, $m = 20$, $\\theta^{(0)} = (1.0, 0.2)$, $\\sigma = 1.0$, $\\alpha = 0.1$, $T = 150$, 种子 $= 31415$。\n- 优化后，为每个情况报告最终的 $\\theta_1$、$\\theta_2$ 和最终的经验平方MMD值，四舍五入到$6$位小数，连接成一个扁平列表，不含空格，并用方括号括起来，顺序为A、B、C、D。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rbf_kernel_matrix(a: np.ndarray, b: np.ndarray, sigma: float) -> np.ndarray:\n    # Compute pairwise squared distances and Gaussian kernel.\n    # a shape: (n,), b shape: (m,)\n    diff = a[:, None] - b[None, :]\n    K = np.exp(-0.5 * (diff ** 2) / (sigma ** 2))\n    return K\n\ndef mmd2_biased(x: np.ndarray, y: np.ndarray, sigma: float) -> float:\n    K_xx = rbf_kernel_matrix(x, x, sigma)\n    K_yy = rbf_kernel_matrix(y, y, sigma)\n    K_xy = rbf_kernel_matrix(x, y, sigma)\n    term_xx = K_xx.mean()\n    term_yy = K_yy.mean()\n    term_xy = K_xy.mean()\n    return float(term_xx + term_yy - 2.0 * term_xy)\n\ndef grad_y_mmd2_biased(x: np.ndarray, y: np.ndarray, sigma: float) -> np.ndarray:\n    # Computes gradient of biased empirical MMD^2 w.r.t. each y_j (1D).\n    n = x.shape[0]\n    m = y.shape[0]\n    K_yy = rbf_kernel_matrix(y, y, sigma)           # shape (m, m)\n    K_xy = rbf_kernel_matrix(x, y, sigma)           # shape (n, m)\n    # Differences\n    diff_yy = y[None, :] - y[:, None]               # shape (m, m): (y_b - y_j)\n    diff_xy = x[:, None] - y[None, :]               # shape (n, m): (x_i - y_j)\n    # Weighted sums\n    part1 = (K_yy * diff_yy).sum(axis=1)            # shape (m,)\n    part2 = (K_xy * diff_xy).sum(axis=0)            # shape (m,)\n    factor1 = 2.0 / (m * m * sigma * sigma)\n    factor2 = 2.0 / (n * m * sigma * sigma)\n    grad_y = factor1 * part1 - factor2 * part2      # shape (m,)\n    return grad_y\n\ndef optimize_theta(x: np.ndarray, z: np.ndarray, theta0: tuple, sigma_k: float, lr: float, steps: int) -> tuple:\n    a, b = float(theta0[0]), float(theta0[1])\n    for _ in range(steps):\n        y = a + b * z\n        gy = grad_y_mmd2_biased(x, y, sigma_k)\n        ga = gy.sum()\n        gb = (gy * z).sum()\n        a = a - lr * ga\n        b = b - lr * gb\n    # Final stats\n    y = a + b * z\n    mmd2 = mmd2_biased(x, y, sigma_k)\n    return a, b, mmd2\n\ndef run_case(mu_tgt: float, s_tgt: float, n: int, m: int, theta0: tuple, sigma_k: float, lr: float, steps: int, seed: int):\n    rng = np.random.default_rng(seed)\n    x = rng.normal(loc=mu_tgt, scale=s_tgt, size=n).astype(float)\n    z = rng.normal(loc=0.0, scale=1.0, size=m).astype(float)\n    a, b, mmd2 = optimize_theta(x, z, theta0, sigma_k, lr, steps)\n    return a, b, mmd2\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A\n        {\"mu\": 1.0, \"s\": 0.5, \"n\": 200, \"m\": 200, \"theta0\": (0.3, 0.8), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 200, \"seed\": 123},\n        # Case B\n        {\"mu\": 0.0, \"s\": 1.0, \"n\": 200, \"m\": 200, \"theta0\": (0.2, 1.5), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 200, \"seed\": 456},\n        # Case C\n        {\"mu\": -1.0, \"s\": 0.7, \"n\": 200, \"m\": 200, \"theta0\": (0.0, 0.5), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 250, \"seed\": 789},\n        # Case D\n        {\"mu\": 0.5, \"s\": 0.9, \"n\": 20, \"m\": 20, \"theta0\": (1.0, 0.2), \"sigma_k\": 1.0, \"lr\": 0.1, \"steps\": 150, \"seed\": 31415},\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, mmd2 = run_case(\n            mu_tgt=case[\"mu\"],\n            s_tgt=case[\"s\"],\n            n=case[\"n\"],\n            m=case[\"m\"],\n            theta0=case[\"theta0\"],\n            sigma_k=case[\"sigma_k\"],\n            lr=case[\"lr\"],\n            steps=case[\"steps\"],\n            seed=case[\"seed\"]\n        )\n        results.extend([a, b, mmd2])\n\n    # Format results to 6 decimal places, no spaces inside the list.\n    formatted = \"[\" + \",\".join(f\"{v:.6f}\" for v in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3136216"}]}