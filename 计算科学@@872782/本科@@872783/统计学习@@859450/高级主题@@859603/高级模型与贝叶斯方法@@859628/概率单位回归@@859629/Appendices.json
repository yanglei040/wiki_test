{"hands_on_practices": [{"introduction": "Probit模型通常通过最大化对数似然函数来拟合，这是一个没有闭式解的优化问题。牛顿-拉弗森法是解决此类问题的强大数值算法，它通过迭代逼近最优参数。本练习将带你亲手实现牛顿-拉弗森法中的一步迭代，让你深入了解模型参数是如何被更新的，并理解在极端数据点下保持数值稳定性的重要性。[@problem_id:3162253]", "problem": "考虑一个具有独立同分布 (IID) 观测值 $\\{(x_i,y_i)\\}_{i=1}^n$ 的二元响应模型，其中 $y_i \\in \\{0,1\\}$，$x_i \\in \\mathbb{R}^p$ 是固定预测变量。在 Probit 回归模型中，条件成功概率被建模为 $p_i = \\Pr(y_i=1 \\mid x_i) = \\Phi(\\eta_i)$，其中线性预测器为 $\\eta_i = x_i^\\top \\beta$，$\\Phi(\\cdot)$ 是标准正态累积分布函数。参数向量 $\\beta \\in \\mathbb{R}^p$ 的对数似然是 $\\ell(\\beta) = \\sum_{i=1}^n \\{ y_i \\log \\Phi(\\eta_i) + (1-y_i)\\log(1-\\Phi(\\eta_i)) \\}$。从伯努利似然和微分链式法则的基本定义出发，执行以下任务：\n\n- 推导得分向量 $s(\\beta)$（$\\ell(\\beta)$ 的梯度）和观测 Hessian 矩阵 $H(\\beta)$（$s(\\beta)$ 的雅可比矩阵），并用 $x_i$、$\\eta_i$、$\\Phi(\\eta_i)$ 以及标准正态概率密度函数 $\\varphi(\\eta_i)$ 表示。\n- 基于您的表达式，精确实现一次牛顿-拉弗森（Newton–Raphson）迭代，从当前迭代值 $\\beta^{\\text{old}}$ 生成更新后的参数向量 $\\beta^{\\text{new}}$。您的实现必须对 $\\eta_i$ 的极端值具有数值稳健性，特别是对于大的正或负 $\\eta_i$，在这些情况下直接计算诸如 $\\varphi(\\eta_i)/\\Phi(\\eta_i)$ 或 $\\varphi(\\eta_i)/(1-\\Phi(\\eta_i))$ 之类的比率可能不稳定。使用能够避免灾难性抵消的对数域评估和稳定变换。如果观测到的 Hessian 矩阵是奇异的或病态的，应用一个小的对角岭正则化，该正则化在普通尺度上可以忽略不计，但能确保线性求解的良定义性。\n- 在您的实现注释和解决方案推导中，分析为什么在极端 $\\eta_i$ 附近会出现数值不稳定性，以及您选择的稳定化方法如何缓解它。\n\n您的程序必须实现一个函数，该函数在给定设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$、二元响应向量 $y \\in \\{0,1\\}^n$ 和当前迭代值 $\\beta^{\\text{old}} \\in \\mathbb{R}^p$ 的情况下，返回一次牛顿-拉弗森更新 $\\beta^{\\text{new}}$。使用此函数处理以下测试套件。对于每个测试用例，计算并返回更新后的系数向量 $\\beta^{\\text{new}}$，四舍五入到六位小数。\n\n测试套件（每个用例指定 $(X, y, \\beta^{\\text{old}})$）：\n\n- 用例 A (基线，良态)：$X = \\begin{bmatrix} 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}$, $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$。\n- 用例 B (混合极端尾部)：$X = \\begin{bmatrix} 1  -10 \\\\ 1  0 \\\\ 1  10 \\end{bmatrix}$, $y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n- 用例 C (近乎可分)：$X = \\begin{bmatrix} 1  -6 \\\\ 1  -3 \\\\ 1  0 \\\\ 1  3 \\\\ 1  6 \\end{bmatrix}$, $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n- 用例 D (边界极端)：$X = \\begin{bmatrix} 1  50 \\\\ 1  -50 \\end{bmatrix}$, $y = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $\\beta^{\\text{old}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的列表。每个内部列表对应一个用例，并且必须按顺序包含更新后的系数，四舍五入到六位小数。例如，一个包含两个用例、每个用例有两个系数的输出必须看起来像 $[[a_{11},a_{12}],[a_{21},a_{22}]]$，其中每个 $a_{ij}$ 都是一个十进制数。此问题不涉及物理单位、角度单位或百分比。输出必须与指定格式完全匹配，并按 A、B、C、D 的顺序列出所有四个用例的结果。", "solution": "用户希望解决一个关于 Probit 回归的统计学习问题。\n\n**1. 问题验证**\n\n用户指定了一个计算统计学中的标准问题：实现用于拟合 Probit 回归模型的牛顿-拉弗森算法。所有必要的组件都已提供：模型规范、对数似然函数、任务（推导和实现），以及一组具有明确定义的输入（$X$、$y$、$\\beta^{\\text{old}}$）和所需输出格式的测试用例。该问题要求谨慎处理数值稳定性，这是科学计算中的一个标准且重要的话题。该问题具有科学依据、问题明确且客观。它不包含任何矛盾、歧义或伪科学主张。\n\n**结论：问题有效。**\n\n**2. 解决方案推导与实现策略**\n\n问题的核心是找到 Probit 模型参数 $\\beta$ 的最大似然估计 (MLE)。牛顿-拉弗森算法是一种迭代方法，用于找到函数一阶导数（在本例中为得分向量）的根，这对应于函数本身（对数似然）的极值。单次迭代将当前参数估计值 $\\beta^{\\text{old}}$ 更新为新的估计值 $\\beta^{\\text{new}}$。\n\n对于二元结果 $y_i \\in \\{0, 1\\}$ 和给定的预测变量 $x_i \\in \\mathbb{R}^p$ 的 Probit 模型由成功概率定义：\n$$p_i = \\Pr(y_i=1 \\mid x_i) = \\Phi(x_i^\\top \\beta) = \\Phi(\\eta_i)$$\n其中 $\\eta_i = x_i^\\top \\beta$ 是线性预测器，$\\Phi(\\cdot)$ 是标准正态分布的累积分布函数 (CDF)。$n$ 个独立观测值的对数似然函数为：\n$$\\ell(\\beta) = \\sum_{i=1}^n \\{ y_i \\log \\Phi(\\eta_i) + (1-y_i)\\log(1-\\Phi(\\eta_i)) \\}$$\n\n**2.1. 得分向量推导**\n\n得分向量 $s(\\beta)$ 是对数似然函数关于 $\\beta$ 的梯度。我们通过应用链式法则来找到它。设 $\\varphi(\\cdot)$ 为标准正态分布的概率密度函数 (PDF)，其中 $\\frac{d}{dz}\\Phi(z) = \\varphi(z)$。\n\n单个观测值 $\\ell_i$ 的对数似然关于参数分量 $\\beta_j$ 的导数是：\n$$\\frac{\\partial \\ell_i}{\\partial \\beta_j} = \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_j}$$\n第一项是：\n$$\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i \\frac{1}{\\Phi(\\eta_i)} \\frac{\\partial \\Phi(\\eta_i)}{\\partial \\eta_i} + (1-y_i) \\frac{1}{1-\\Phi(\\eta_i)} \\frac{\\partial (1-\\Phi(\\eta_i))}{\\partial \\eta_i} = y_i \\frac{\\varphi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\varphi(\\eta_i)}{1-\\Phi(\\eta_i)}$$\n第二项很简单 $\\frac{\\partial \\eta_i}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} (x_i^\\top \\beta) = x_{ij}$。\n结合这些，得分向量的第 $j$ 个分量是：\n$$s_j(\\beta) = \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n x_{ij} \\left( y_i \\frac{\\varphi(\\eta_i)}{\\Phi(\\eta_i)} - (1-y_i) \\frac{\\varphi(\\eta_i)}{1-\\Phi(\\eta_i)} \\right)$$\n在向量形式中，为了符号清晰和数值稳定性管理，我们定义两个术语：$\\lambda(\\eta) = \\frac{\\varphi(\\eta)}{\\Phi(\\eta)}$ 和 $\\lambda'(\\eta) = -\\frac{\\varphi(\\eta)}{1-\\Phi(\\eta)}$。得分向量可以表示为：\n$$s(\\beta) = \\sum_{i=1}^n x_i \\cdot \\left( y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i) \\right) = X^\\top c$$\n其中 $c$ 是一个分量为 $c_i = y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i)$ 的向量。\n\n**2.2. 观测 Hessian 矩阵推导**\n\n观测 Hessian 矩阵 $H(\\beta)$ 是对数似然的二阶偏导数矩阵。其元素 $H_{jk}$ 是 $\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}$。我们通过对得分向量分量 $s_j(\\beta)$ 关于 $\\beta_k$ 求导来找到它：\n$$H_{jk}(\\beta) = \\frac{\\partial s_j(\\beta)}{\\partial \\beta_k} = \\sum_{i=1}^n x_{ij} \\frac{\\partial}{\\partial \\beta_k} \\left( \\frac{\\partial \\ell_i}{\\partial \\eta_i} \\right) = \\sum_{i=1}^n x_{ij} \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} \\frac{\\partial \\eta_i}{\\partial \\beta_k} = \\sum_{i=1}^n x_{ij} x_{ik} \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}$$\n我们需要 $\\ell_i$ 关于 $\\eta_i$ 的二阶导数。让我们对 $\\frac{\\partial \\ell_i}{\\partial \\eta_i} = y_i \\lambda(\\eta_i) + (1-y_i) \\lambda'(\\eta_i)$ 求导。使用商法则以及 $\\frac{d}{d\\eta}\\varphi(\\eta) = -\\eta\\varphi(\\eta)$ 这一事实，我们得到：\n$$\\frac{d}{d\\eta}\\lambda(\\eta) = \\frac{d}{d\\eta}\\left(\\frac{\\varphi(\\eta)}{\\Phi(\\eta)}\\right) = \\frac{-\\eta\\varphi(\\eta)\\Phi(\\eta) - \\varphi(\\eta)^2}{\\Phi(\\eta)^2} = -\\eta\\lambda(\\eta) - \\lambda(\\eta)^2$$\n$$\\frac{d}{d\\eta}\\lambda'(\\eta) = \\frac{d}{d\\eta}\\left(-\\frac{\\varphi(\\eta)}{1-\\Phi(\\eta)}\\right) = -\\eta\\lambda'(\\eta) - \\lambda'(\\eta)^2$$\n因此，二阶导数是：\n$$\\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2} = y_i \\left(-\\eta_i \\lambda(\\eta_i) - \\lambda(\\eta_i)^2\\right) + (1-y_i) \\left(-\\eta_i \\lambda'(\\eta_i) - \\lambda'(\\eta_i)^2\\right)$$\nHessian 矩阵可以紧凑地写为 $H(\\beta) = X^\\top D X$，其中 $D$ 是一个对角矩阵，其对角线元素为 $d_{ii} = \\frac{\\partial^2 \\ell_i}{\\partial \\eta_i^2}$。对于严格凹的对数似然函数，$H(\\beta)$ 是负定的。\n\n**2.3. 牛顿-拉弗森更新步骤**\n\n牛顿-拉弗森方法用二次函数逼近对数似然曲面，并步进到其最大值。更新规则是：\n$$\\beta^{\\text{new}} = \\beta^{\\text{old}} - [H(\\beta^{\\text{old}})]^{-1} s(\\beta^{\\text{old}})$$\n在计算上，这是通过求解线性系统 $H(\\beta^{\\text{old}}) \\Delta\\beta = s(\\beta^{\\text{old}})$ 来获得步长 $\\Delta\\beta$，然后更新 $\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\Delta\\beta$ 来实现的。\n\n**2.4. 数值稳定性与正则化**\n\n直接计算 $\\lambda(\\eta_i)$ 和 $\\lambda'(\\eta_i)$ 对于 $\\eta_i$ 的极端值是数值不稳定的。\n- 对于大的正 $\\eta_i$，$1-\\Phi(\\eta_i)$ 和 $\\varphi(\\eta_i)$ 都会下溢到 $0$，使得 $\\lambda'(\\eta_i)$ 成为不稳定的 $0/0$ 形式。\n- 对于大的负 $\\eta_i$，$\\Phi(\\eta_i)$ 和 $\\varphi(\\eta_i)$ 都会下溢到 $0$，使得 $\\lambda(\\eta_i)$ 成为不稳定的 $0/0$ 形式。\n\n为解决此问题，我们在对数域中计算这些量，这能保持数值精度：\n$$\\lambda(\\eta) = \\exp(\\log(\\varphi(\\eta)) - \\log(\\Phi(\\eta)))$$\n$$\\lambda'(\\eta) = -\\exp(\\log(\\varphi(\\eta)) - \\log(1-\\Phi(\\eta)))$$\n专门的函数，如 `scipy.stats.norm.logpdf`、`logcdf` 和 `logsf`（对数生存函数），被设计用来精确计算这些对数量，即使对于分布极端尾部的参数也是如此。\n\n此外，如果数据是完全或近乎可分的，$\\beta$ 的 MLE 位于无穷远处，Hessian 矩阵会变得奇异或病态。这使得求解 $\\Delta\\beta$ 的线性系统无法进行。为防止这种情况，我们对 Hessian 矩阵应用少量的岭正则化。我们求解 $(H - \\epsilon I) \\Delta\\beta = s$，其中 $I$ 是单位矩阵，$\\epsilon$ 是一个小的正常数（例如，$10^{-8}$）。这确保了矩阵是可逆的，同时在良态情况下对解的影响可以忽略不计。这个过程是 Levenberg-Marquardt 类型调整的一种形式，用于稳定数值求逆。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes a single Newton-Raphson update for probit regression coefficients\n    for a suite of test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (baseline, well-behaved)\n        (\n            [[1, -1], [1, 0], [1, 1], [1, 2]],\n            [0, 0, 1, 1],\n            [0, 0.5]\n        ),\n        # Case B (extreme tails mixed)\n        (\n            [[1, -10], [1, 0], [1, 10]],\n            [1, 0, 0],\n            [0, 1]\n        ),\n        # Case C (nearly separated)\n        (\n            [[1, -6], [1, -3], [1, 0], [1, 3], [1, 6]],\n            [0, 0, 0, 1, 1],\n            [0, 1]\n        ),\n        # Case D (boundary extreme)\n        (\n            [[1, 50], [1, -50]],\n            [0, 1],\n            [0, 1]\n        )\n    ]\n\n    def probit_newton_step(X, y, beta_old):\n        \"\"\"\n        Performs one numerically robust Newton-Raphson iteration for probit regression.\n\n        - Analyzes instability and mitigation:\n          Direct computation of ratios like phi(eta)/Phi(eta) is unstable for extreme\n          eta where both numerator and denominator underflow to zero. This function\n          mitigates this by operating in the log domain using scipy's specialized\n          functions (logpdf, logcdf, logsf), which retain precision in the tails.\n          The expression exp(log(phi) - log(Phi)) is numerically stable.\n          \n          For (quasi-)separated data, the Hessian can be singular. This is handled\n          by adding a small diagonal ridge term (epsilon * I), which ensures the\n          matrix is invertible without significantly affecting the step in\n          well-conditioned cases.\n        \"\"\"\n        p = X.shape[1]\n        epsilon = 1e-8  # Regularization parameter\n\n        # 1. Compute linear predictor\n        eta = X @ beta_old\n\n        # 2. Compute components for score and Hessian stably in the log domain\n        log_phi = norm.logpdf(eta)\n        log_Phi = norm.logcdf(eta)\n        log_sf = norm.logsf(eta)  # log(1 - Phi(eta))\n\n        # lambda_i = phi(eta_i) / Phi(eta_i)\n        lambda_i = np.exp(log_phi - log_Phi)\n        # lambda_prime_i = -phi(eta_i) / (1 - Phi(eta_i))\n        lambda_prime_i = -np.exp(log_phi - log_sf)\n\n        # 3. Compute score vector s(beta)\n        s_components = np.where(y == 1, lambda_i, lambda_prime_i)\n        score = X.T @ s_components\n\n        # 4. Compute observed Hessian matrix H(beta)\n        d_i_if_y1 = -eta * lambda_i - lambda_i**2\n        d_i_if_y0 = -eta * lambda_prime_i - lambda_prime_i**2\n        d = np.where(y == 1, d_i_if_y1, d_i_if_y0)\n        \n        # H = X^T * diag(d) * X\n        hessian = (X.T * d) @ X\n\n        # 5. Regularize Hessian and solve for the update step delta_beta\n        hessian_reg = hessian - epsilon * np.eye(p)\n        \n        # The Newton-Raphson update is beta_new = beta_old - inv(H) * s.\n        # We solve H * delta_beta = s, then update beta_new = beta_old - delta_beta.\n        delta_beta = np.linalg.solve(hessian_reg, score)\n\n        # 6. Update beta\n        beta_new = beta_old - delta_beta\n        return beta_new\n\n    results = []\n    for X_list, y_list, beta_old_list in test_cases:\n        X = np.array(X_list, dtype=float)\n        y = np.array(y_list, dtype=float)\n        beta_old = np.array(beta_old_list, dtype=float)\n        beta_new = probit_newton_step(X, y, beta_old)\n        results.append(beta_new)\n    \n    # Format the output exactly as specified.\n    rounded_results = [np.round(res, 6) for res in results]\n    str_results = []\n    for res in rounded_results:\n        # Format each sublist as \"[v1,v2,...]\" without spaces after commas\n        str_results.append(f\"[{','.join(map(str, res))}]\")\n    \n    final_output = f\"[{','.join(str_results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3162253"}, {"introduction": "在拟合模型之后，我们需要评估其性能。ROC曲线和其曲线下面积（AUC）是衡量分类器判别能力的金标准。本练习将通过计算揭示AUC一个深刻而实用的特性：任何严格递增的函数（如probit、logit或cloglog连接函数）应用于模型的原始得分，都不会改变AUC值。这有助于你理解AUC衡量的是模型对观测值排序的能力，而非预测概率的绝对值。[@problem_id:3162345]", "problem": "给定一个二元响应设定，其线性得分函数为 $ \\eta = x^\\top \\beta + b_0 $，其中 $ x \\in \\mathbb{R}^d $ 是输入向量，$ \\beta \\in \\mathbb{R}^d $ 是固定系数向量，$ b_0 \\in \\mathbb{R} $ 是截距。对于二元类别标签 $ Y \\in \\{0,1\\} $，通过将严格递增的链接函数 $ g^{-1} $ 逐点应用于 $ \\eta $ 来计算类别得分。考虑以下三种严格递增的链接函数：probit逆链接函数 $ g^{-1}_{\\text{probit}}(\\eta) = \\Phi(\\eta) $，其中 $ \\Phi $ 是标准正态分布的累积分布函数；logit逆链接函数 $ g^{-1}_{\\text{logit}}(\\eta) = \\Lambda(\\eta) $，其中 $ \\Lambda(\\eta) = \\frac{1}{1 + e^{-\\eta}} $；以及互补log-log逆链接函数 $ g^{-1}_{\\text{cloglog}}(\\eta) = 1 - \\exp(-\\exp(\\eta)) $。受试者工作特征（Receiver Operating Characteristic, ROC）曲线是通过对一个实值得分进行阈值处理，并绘制在所有可能阈值下真阳性率与假阳性率的关系图而定义的。ROC曲线下面积（Area Under the ROC Curve, AUC）即为该ROC曲线下方的面积。根据基本定义，对于任意得分 $ s \\in \\mathbb{R}^n $ 和标签 $ y \\in \\{0,1\\}^n $，ROC曲线仅取决于得分的排序，而AUC等于一个均匀抽取的正例得分严格大于一个均匀抽取的负例得分的概率，加上得分相等概率的一半。从这些定义出发，您必须通过计算证明，将任意严格递增的链接函数 $ \\Phi $、$ \\Lambda $ 和 $ 1 - \\exp(-\\exp(\\cdot)) $ 应用于相同的线性得分 $ \\eta $，会产生相同的ROC曲线，因此也会得到相同的AUC值。\n\n您的程序必须为每个测试用例实现以下步骤：\n1. 根据指定规则生成设计矩阵 $ X \\in \\mathbb{R}^{n \\times d} $ 和标签向量 $ y \\in \\{0,1\\}^n $。\n2. 计算线性得分 $ \\eta = X \\beta + b_0 \\mathbf{1} $，其中 $ \\mathbf{1} $ 是长度为 $ n $ 的全1向量。\n3. 通过将probit逆链接函数 $ \\Phi(\\eta) $、logit逆链接函数 $ \\Lambda(\\eta) $ 和互补log-log逆链接函数 $ 1 - \\exp(-\\exp(\\eta)) $ 逐点应用于 $ \\eta $，计算出三个得分向量。\n4. 使用AUC与Wilcoxon–Mann–Whitney统计量之间的等价关系，或任何其他源于ROC和AUC基本定义并能正确处理得分平局的实现方法，计算每个得分向量相对于 $ y $ 的AUC。\n5. 对每个测试用例，输出一个布尔值，表示三个AUC值是否在 $ 10^{-15} $ 的绝对容忍度内全部相等。\n\n使用以下五个测试用例。所有随机数生成必须使用指定的种子以保证可复现性。对于整数值矩阵，每个元素都从闭区间 $ [-2,2] $ 内的整数中独立均匀抽取。\n\n- 测试用例 $ 1 $（一般情况）：\n  - 种子 $ 0 $，$ n = 200 $，$ d = 3 $，$ \\beta = (1.0, -0.5, 0.8) $，$ b_0 = 0.2 $。\n  - 生成 $ X $，其元素为独立的标准正态分布。\n  - 使用伯努利机制生成标签，成功概率逐点应用 $ \\Phi(\\eta) $。\n\n- 测试用例 $ 2 $（通过截距实现类别不平衡）：\n  - 种子 $ 1 $，$ n = 300 $，$ d = 4 $，$ \\beta = (1.0, 0.5, -1.2, 0.3) $，$ b_0 = -2.0 $。\n  - 生成 $ X $，其元素为独立的标准正态分布。\n  - 使用伯努利机制生成标签，成功概率逐点应用 $ \\Phi(\\eta) $。\n\n- 测试用例 $ 3 $（近乎可分的确定性标签）：\n  - 种子 $ 2 $，$ n = 250 $，$ d = 2 $，$ \\beta = (3.0, -3.0) $，$ b_0 = 0.0 $。\n  - 生成 $ X $，其元素为独立的标准正态分布。\n  - 确定性地生成标签为 $ y_i = \\mathbf{1}\\{\\eta_i  0\\} $，其中 $ i = 1,\\dots,n $。\n\n- 测试用例 $ 4 $（所有得分相等，平局数量最多）：\n  - 无随机性，$ n = 100 $，$ d = 1 $，$ \\beta = (0.0) $，$ b_0 = 0.0 $。\n  - 将 $ X $ 设置为全零矩阵，因此 $ \\eta $ 恒为 $ 0 $。\n  - 确定性地交替设置标签：若 $ i $ 为奇数，则 $ y_i = 1 $；若 $ i $ 为偶数，则 $ y_i = 0 $，其中 $ i = 1,\\dots,n $。\n\n- 测试用例 $ 5 $（导致平局的离散特征）：\n  - 种子 $ 3 $，$ n = 120 $，$ d = 2 $，$ \\beta = (1.0, 2.0) $，$ b_0 = -1.0 $。\n  - 生成 $ X $，其元素为在 $ [-2,2] $ 内独立均匀抽取的整数。\n  - 使用伯努利机制生成标签，成功概率逐点应用 $ \\Phi(\\eta) $。\n\n您的程序必须生成单行输出，其中包含一个列表，内含与五个测试用例按顺序对应的五个布尔值。对每个测试用例，当且仅当由probit、logit和互补log-log变换后的得分计算出的三个AUC值在绝对容忍度 $ 10^{-15} $ 内全部相等时，该布尔值为 True，否则为 False。确切的输出格式必须是单行，结果为用方括号括起来的逗号分隔列表，例如 $[True,True,True,True,True]$。", "solution": "该问题要求通过计算来展示受试者工作特征（ROC）曲线及其对应的曲线下面积（AUC）的一个基本性质。具体来说，该性质断言，对一组得分应用任何严格递增的变换，都不会改变最终的ROC曲线或AUC。本解答将针对统计建模中使用的三种特定的严格递增逆链接函数来验证这一原理：probit函数、logit函数和互补log-log函数。\n\n其核心原理根植于ROC曲线的定义。ROC曲线是在分类阈值变化时，真阳性率（TPR）对假阳性率（FPR）的二维图。对于给定的得分向量 $s$ 和阈值 $\\tau$，这两个率定义如下：\n$$ \\text{TPR}(\\tau) = P(s  \\tau | Y=1) $$\n$$ \\text{FPR}(\\tau) = P(s  \\tau | Y=0) $$\n其中 $Y$ 是真实的二元标签。ROC曲线是所有可能的 $\\tau$ 值对应的 $(\\text{FPR}(\\tau), \\text{TPR}(\\tau))$ 点对的集合。\n\n设 $f: \\mathbb{R} \\to \\mathbb{R}$ 为任意严格递增函数。如果我们将得分 $s$ 变换为一组新的得分 $s' = f(s)$，那么得分的排序将保持不变。也就是说，对于任意两个得分 $s_i$ 和 $s_j$，$s_i  s_j$ 当且仅当 $f(s_i)  f(s_j)$。\n我们来考虑变换后得分的分类规则 $s'  \\tau'$。这等价于 $f(s)  \\tau'$。由于 $f$ 是严格递增的，它存在一个严格递增的逆函数 $f^{-1}$，因此该不等式等价于 $s  f^{-1}(\\tau')$。如果我们设 $\\tau = f^{-1}(\\tau')$，那么对于得分-阈值对 $(s, \\tau)$ 和 $(s', \\tau')$，被分类为正例的样本集合是完全相同的。当 $\\tau'$ 扫过 $s'$ 的所有可能取值时，对应的 $\\tau$ 也扫过 $s$ 的所有可能取值。因此，对于 $s$ 和 $s'$ 生成的 $(\\text{FPR}, \\text{TPR})$ 点对集合是相同的。这意味着它们的ROC曲线完全相同，因此它们的面积（AUC值）也必然相同。\n\n问题中提供的三个链接函数，$g^{-1}_{\\text{probit}}(\\eta) = \\Phi(\\eta)$、$g^{-1}_{\\text{logit}}(\\eta) = \\Lambda(\\eta)$ 和 $g^{-1}_{\\text{cloglog}}(\\eta) = 1 - \\exp(-\\exp(\\eta))$，都是关于线性得分 $\\eta$ 的严格递增函数。因此，由这些函数各自生成的得分所计算出的AUC应该完全相同。\n\n为计算AUC，我们利用其与Wilcoxon-Mann-Whitney U统计量的等价性。AUC是从正例类别中随机均匀抽取一个得分，其严格大于从负例类别中随机均匀抽取一个得分的概率，并对得分平局情况进行校正。对于一个有 $n_1$ 个正例（得分为 $s_{1,i}$）和 $n_0$ 个负例（得分为 $s_{0,j}$）的样本，AUC的估计公式为：\n$$ \\text{AUC} = \\frac{1}{n_1 n_0} \\sum_{i=1}^{n_1} \\sum_{j=1}^{n_0} \\left( \\mathbb{I}(s_{1,i}  s_{0,j}) + \\frac{1}{2} \\mathbb{I}(s_{1,i} = s_{0,j}) \\right) $$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数。这个公式能正确处理得分平局的情况，这对于具有离散特征或恒定得分的测试用例至关重要。如果正例或负例类别为空（$n_1=0$ 或 $n_0=0$），AUC按惯例定义为 $0.5$。\n\n对 $5$ 个测试用例中的每一个，计算流程如下：\n$1$. 设置随机种子（如果指定）以确保可复现性。使用给定的参数（$n, d, \\beta, b_0$），根据指定规则生成设计矩阵 $X$ 和标签向量 $y$。\n$2$. 计算线性得分向量 $\\eta = X \\beta + b_0 \\mathbf{1}$。\n$3$. 将三个逆链接函数逐点应用于 $\\eta$，得到三个得分向量：$s_{\\text{probit}} = \\Phi(\\eta)$、$s_{\\text{logit}} = \\Lambda(\\eta)$ 和 $s_{\\text{cloglog}} = 1 - \\exp(-\\exp(\\eta))$。\n$4$. 对于这三个得分向量中的每一个，使用上述的Wilcoxon-Mann-Whitney统计量公式计算其相对于标签向量 $y$ 的AUC。这一步通过使用向量化的矩阵运算来高效实现。\n$5$. 比较计算出的三个AUC值。如果所有三个值在 $10^{-15}$ 的绝对容忍度内相等，即 $\\max(\\text{AUC}_{\\text{probit}}, \\text{AUC}_{\\text{logit}}, \\text{AUC}_{\\text{cloglog}}) - \\min(\\text{AUC}_{\\text{probit}}, \\text{AUC}_{\\text{logit}}, \\text{AUC}_{\\text{cloglog}})  10^{-15}$，则该测试用例通过。结果以布尔值的形式存储。\n\n此过程将对所有 $5$ 个测试用例重复执行，最终输出将是这些布尔结果的列表。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.special import expit\n\ndef calculate_auc(scores, labels):\n    \"\"\"\n    Calculates the Area Under the ROC Curve (AUC) using the \n    Wilcoxon-Mann-Whitney U statistic formulation. This handles ties correctly.\n    \"\"\"\n    pos_indices = np.where(labels == 1)[0]\n    neg_indices = np.where(labels == 0)[0]\n    \n    n_pos = len(pos_indices)\n    n_neg = len(neg_indices)\n    \n    # If there are no positive or no negative samples, AUC is undefined.\n    # A common convention is to return 0.5.\n    if n_pos == 0 or n_neg == 0:\n        return 0.5\n        \n    pos_scores = scores[pos_indices]\n    neg_scores = scores[neg_indices]\n    \n    # Use broadcasting to efficiently compare all pos vs. neg scores.\n    # Reshape pos_scores to a column vector for broadcasting.\n    pos_scores_reshaped = pos_scores.reshape(-1, 1)\n    \n    # Comparisons result in boolean matrices of shape (n_pos, n_neg).\n    gt_matrix = pos_scores_reshaped  neg_scores\n    eq_matrix = pos_scores_reshaped == neg_scores\n    \n    # The statistic is the sum of pairs where pos  neg + 0.5 * sum of ties.\n    mann_whitney_stat = np.sum(gt_matrix) + 0.5 * np.sum(eq_matrix)\n    \n    return mann_whitney_stat / (n_pos * n_neg)\n\ndef solve():\n    \"\"\"\n    Runs all test cases to verify the AUC invariance property.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general case)\n        {'seed': 0, 'n': 200, 'd': 3, 'beta': np.array([1.0, -0.5, 0.8]), 'b0': 0.2, 'X_type': 'normal', 'y_type': 'bernoulli'},\n        # Test case 2 (class imbalance via intercept)\n        {'seed': 1, 'n': 300, 'd': 4, 'beta': np.array([1.0, 0.5, -1.2, 0.3]), 'b0': -2.0, 'X_type': 'normal', 'y_type': 'bernoulli'},\n        # Test case 3 (near-separable deterministic labels)\n        {'seed': 2, 'n': 250, 'd': 2, 'beta': np.array([3.0, -3.0]), 'b0': 0.0, 'X_type': 'normal', 'y_type': 'deterministic'},\n        # Test case 4 (all scores equal, maximal ties)\n        {'seed': None, 'n': 100, 'd': 1, 'beta': np.array([0.0]), 'b0': 0.0, 'X_type': 'zeros', 'y_type': 'alternating'},\n        # Test case 5 (discrete features inducing ties)\n        {'seed': 3, 'n': 120, 'd': 2, 'beta': np.array([1.0, 2.0]), 'b0': -1.0, 'X_type': 'integer', 'y_type': 'bernoulli'}\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # 1. Generate data\n        if case['seed'] is not None:\n            np.random.seed(case['seed'])\n        \n        n, d = case['n'], case['d']\n        beta, b0 = case['beta'], case['b0']\n        \n        if case['X_type'] == 'normal':\n            X = np.random.randn(n, d)\n        elif case['X_type'] == 'integer':\n            X = np.random.randint(-2, 3, size=(n, d))\n        elif case['X_type'] == 'zeros':\n            X = np.zeros((n, d))\n\n        # 2. Compute linear scores\n        eta = X @ beta + b0\n\n        # Generate labels\n        if case['y_type'] == 'bernoulli':\n            p = norm.cdf(eta) # Data generating process uses probit\n            y = np.random.binomial(1, p)\n        elif case['y_type'] == 'deterministic':\n            y = (eta  0).astype(int)\n        elif case['y_type'] == 'alternating':\n            # y_i = 1 if i is odd, 0 if even, for i=1..n\n            y = np.arange(1, n + 1) % 2\n\n        # 3. Compute scores using different link functions\n        scores_probit = norm.cdf(eta)\n        scores_logit = expit(eta) # scipy.special.expit is the logistic sigmoid\n        scores_cloglog = 1.0 - np.exp(-np.exp(eta))\n\n        # 4. Compute AUC for each score vector\n        auc_probit = calculate_auc(scores_probit, y)\n        auc_logit = calculate_auc(scores_logit, y)\n        auc_cloglog = calculate_auc(scores_cloglog, y)\n        \n        # 5. Check for equality within tolerance\n        aucs = np.array([auc_probit, auc_logit, auc_cloglog])\n        tolerance = 1e-15\n        is_equal = (np.max(aucs) - np.min(aucs))  tolerance\n        results.append(is_equal)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3162345"}, {"introduction": "拟合和评估模型后，最后一步是解释结果。Probit回归的原始系数大小不易直接解释，因为它们的影响是非线性的。本练习将探讨特征标准化如何改变系数，并揭示其与边际效应的内在联系，帮助你将抽象的系数量化为对成功概率的具体影响。[@problem_id:3162301]", "problem": "考虑一个probit链接下的二元响应模型。设 $Y_i \\in \\{0,1\\}$ 为观测结果，协变量 $X_i \\in \\mathbb{R}^{p}$，潜变量为 $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 且在观测间独立。$Y_i = 1$ 的概率为 $P(Y_i=1 \\mid X_i) = \\Phi(X_i^\\top \\beta)$，其中 $\\Phi(\\cdot)$ 表示标准正态累积分布函数（CDF）。参数 $\\beta$ 通过最大化对数似然函数 $\\sum_{i=1}^{n} \\left[ Y_i \\log \\Phi(X_i^\\top \\beta) + (1-Y_i) \\log \\left(1 - \\Phi(X_i^\\top \\beta)\\right) \\right]$ 进行最大似然估计（MLE）。给定一个带有截距项的数据集，对每个非截距特征 $j$ 定义特征标准化为 $Z_{ij} = (X_{ij} - \\mu_j)/\\sigma_j$，其中 $\\mu_j$ 是特征 $j$ 的样本均值，$\\sigma_j$ 是其样本标准差。截距项不进行标准化。\n\n你的任务是实现一个程序，该程序：\n- 从潜变量模型 $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$（其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$）生成合成数据集，并设置 $Y_i = \\mathbf{1}\\{Y_i^*  0\\}$。为保证可复现性，程序必须使用固定的随机种子 $42$。\n- 对每个数据集，在原始特征（包括截距项）上通过最大似然估计（MLE）拟合probit回归以获得 $\\hat{\\beta}$，然后将非截距特征标准化为零均值和单位方差，并重新拟合probit回归以在标准化特征上获得 $\\hat{\\beta}^{\\text{std}}$。\n- 推导并定量验证由标准化引起的线性重参数化。具体来说，对于非截距斜率，从 $\\hat{\\beta}$ 到标准化斜率的确定性映射为每个特征 $j$ 的 $\\hat{\\beta}^{\\text{map}}_j = \\hat{\\beta}_j \\sigma_j$，标准化截距为 $\\hat{\\beta}^{\\text{map}}_0 = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$。计算 $\\hat{\\beta}^{\\text{std}}$ 和 $\\hat{\\beta}^{\\text{map}}$ 在所有系数上的最大绝对差，结果为一个浮点数。\n- 计算均值处的边际效应（MEM）。特征 $k$ 在点 $x$ 处的边际效应为 $\\frac{\\partial}{\\partial x_k} \\Phi(X^\\top \\beta) = \\phi(X^\\top \\beta)\\,\\beta_k$，其中 $\\phi(\\cdot)$ 表示标准正态概率密度函数（PDF）。在原始特征的样本均值处，令 $\\eta_{\\text{mean}} = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$ 并计算 $\\phi(\\eta_{\\text{mean}})$。对于原始（未标准化的）特征，计算单位变化的MEM：$\\text{MEM}^{\\text{orig}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}_j$，以及单位标准差变化的MEM：$\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}_j \\sigma_j$。对于标准化特征，单位变化的MEM等于 $\\text{MEM}^{\\text{std}}_j = \\phi(\\eta_{\\text{mean}})\\,\\hat{\\beta}^{\\text{std}}_j$。计算向量 $\\text{MEM}^{\\text{std}}$ 和 $\\text{MEM}^{\\text{orig,1sd}}$ 之间的最大绝对差，结果为一个浮点数。\n\n使用平滑的目标函数稳健地实现数值优化。仅使用标准正态CDF $\\Phi(\\cdot)$ 和 PDF $\\phi(\\cdot)$ 以及一个基于梯度的优化器。\n\n测试套件：\n- 案例A（单特征）：$n=1000$，一个预测变量 $x_1 \\sim \\mathcal{N}(2,3^2)$，系数 $(\\beta_0,\\beta_1)=(-0.5,1.5)$。\n- 案例B（双特征，异构尺度）：$n=2000$，$x_1 \\sim \\text{Uniform}[0,10]$，$x_2 \\sim \\mathcal{N}(-1,0.5^2)$，系数 $(\\beta_0,\\beta_1,\\beta_2)=(0.5,0.2,-2.0)$。\n- 案例C（双特征，近饱和状态）：$n=1500$，$x_1 \\sim \\mathcal{N}(0,1^2)$，$x_2 \\sim \\mathcal{N}(0,1^2)$，系数 $(\\beta_0,\\beta_1,\\beta_2)=(3.0,-0.5,0.5)$。\n\n对于每个案例，输出两个浮点数：\n- 最大绝对系数映射误差：$\\max_{j \\in \\{0,1,\\dots,p\\}} \\left| \\hat{\\beta}^{\\text{std}}_j - \\hat{\\beta}^{\\text{map}}_j \\right|$。\n- 最大绝对MEM等价性误差：$\\max_{j \\in \\{1,\\dots,p\\}} \\left| \\text{MEM}^{\\text{std}}_j - \\text{MEM}^{\\text{orig,1sd}}_j \\right|$。\n\n最终输出格式：\n你的程序应生成单行输出，其中包含一个列表的列表，对应于三个案例。每个内部列表 $[\\text{mapping\\_error},\\text{mem\\_error}]$ 的值四舍五入到小数点后六位，格式应严格为\n$[[v_{A1},v_{A2}],[v_{B1},v_{B2}],[v_{C1},v_{C2}]]$。", "solution": "该问题要求实现并验证probit回归模型的关键属性，特别是关于特征标准化对估计系数和边际效应的影响。解决方案涉及数据生成、通过最大似然进行模型估计，以及对理论上等价的量进行定量比较。\n\n首先，我们定义probit模型。对于具有协变量向量 $X_i \\in \\mathbb{R}^{p+1}$（包括截距项 $X_{i0}=1$）的观测 $i$，其正向结果 $Y_i=1$ 的概率由标准正态累积分布函数（CDF）$\\Phi(\\cdot)$ 作用于协变量的线性组合给出：\n$$\nP(Y_i=1 \\mid X_i) = \\Phi(X_i^\\top \\beta)\n$$\n这里，$\\beta \\in \\mathbb{R}^{p+1}$ 是待估计的系数向量。该模型源于一个潜变量公式 $Y_i^* = X_i^\\top \\beta + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,1)$ 是独立同分布的标准正态误差，观测结果为 $Y_i = \\mathbf{1}\\{Y_i^*  0\\}$。\n\n系数 $\\beta$ 通过最大化对数似然函数来估计。对于一个包含 $n$ 个观测的数据集 $\\{(X_i, Y_i)\\}_{i=1}^n$，其对数似然函数为：\n$$\n\\mathcal{L}(\\beta) = \\sum_{i=1}^{n} \\left[ Y_i \\log \\Phi(X_i^\\top \\beta) + (1-Y_i) \\log \\left(1 - \\Phi(X_i^\\top \\beta)\\right) \\right]\n$$\n为了使用基于梯度的数值优化器执行此最大化过程，我们需要 $\\mathcal{L}(\\beta)$ 相对于 $\\beta$ 的梯度。令 $\\eta_i = X_i^\\top \\beta$。单个观测 $i$ 的梯度为：\n$$\n\\nabla_{\\beta} \\ell_i(\\beta) = \\frac{\\partial \\ell_i}{\\partial \\beta} = \\left( \\frac{Y_i}{\\Phi(\\eta_i)} - \\frac{1-Y_i}{1 - \\Phi(\\eta_i)} \\right) \\phi(\\eta_i) X_i\n$$\n其中 $\\phi(\\cdot)$ 是标准正态概率密度函数（PDF），即 $\\Phi(\\cdot)$ 的导数。总梯度是所有观测梯度之和：$\\nabla_{\\beta} \\mathcal{L} = \\sum_{i=1}^{n} \\nabla_{\\beta} \\ell_i(\\beta)$。我们将使用该梯度的负数来最小化负对数似然函数 $-\\mathcal{L}(\\beta)$。\n\n接下来，我们分析特征标准化的影响。对于每个非截距特征 $j \\in \\{1, \\dots, p\\}$，我们定义其标准化版本为 $Z_{ij} = (X_{ij} - \\mu_j) / \\sigma_j$，其中 $\\mu_j$ 和 $\\sigma_j$ 分别是特征 $j$ 的样本均值和样本标准差。截距项保持为 $Z_{i0} = X_{i0} = 1$。原始的线性预测变量 $X_i^\\top \\beta$ 可以用标准化特征重新表示。通过代入 $X_{ij} = Z_{ij}\\sigma_j + \\mu_j$，我们得到：\n$$\nX_i^\\top \\beta = \\beta_0 + \\sum_{j=1}^{p} X_{ij}\\beta_j = \\beta_0 + \\sum_{j=1}^{p} (Z_{ij}\\sigma_j + \\mu_j)\\beta_j\n$$\n$$\nX_i^\\top \\beta = \\left(\\beta_0 + \\sum_{j=1}^{p} \\beta_j\\mu_j\\right) + \\sum_{j=1}^{p} Z_{ij}(\\beta_j\\sigma_j)\n$$\n此表达式的形式为标准化特征的线性预测变量 $Z_i^\\top \\beta^{\\text{std}}$，其中新系数 $\\beta^{\\text{std}}$ 与原始系数 $\\beta$ 存在确定性关系：\n$$\n\\beta^{\\text{std}}_0 = \\beta_0 + \\sum_{j=1}^{p} \\beta_j\\mu_j\n$$\n$$\n\\beta^{\\text{std}}_j = \\beta_j\\sigma_j \\quad \\text{for } j \\in \\{1, \\dots, p\\}\n$$\n问题要求我们验证这种重参数化。我们首先通过在原始特征 $X$ 上拟合模型来估计 $\\hat{\\beta}$。然后，我们使用上述公式，结合 $\\hat{\\beta}$、$\\mu_j$ 和 $\\sigma_j$ 计算理论上映射的标准化系数 $\\hat{\\beta}^{\\text{map}}$。我们还通过直接在标准化特征 $Z$ 上拟合模型来估计 $\\hat{\\beta}^{\\text{std}}$。$\\hat{\\beta}^{\\text{map}}$ 和 $\\hat{\\beta}^{\\text{std}}$ 之间的微小差异将从数值上验证重参数化理论，任何差异都可归因于优化器的容差。映射误差计算为 $\\max_j |\\hat{\\beta}^{\\text{std}}_j - \\hat{\\beta}^{\\text{map}}_j|$。\n\n最后，我们考察边际效应，它衡量的是当一个特征发生变化时，成功概率的变化。特征 $k$ 的边际效应为：\n$$\n\\text{ME}_k(X) = \\frac{\\partial P(Y=1 \\mid X)}{\\partial X_k} = \\frac{\\partial}{\\partial X_k} \\Phi(X^\\top \\beta) = \\phi(X^\\top \\beta) \\beta_k\n$$\n这种效应取决于所有协变量 $X$ 的值。一种常见的做法是在特征的样本均值处对其进行评估，这被称为均值处的边际效应（MEM）。令 $\\eta_{\\text{mean}} = \\hat{\\beta}_0 + \\sum_{j=1}^{p} \\hat{\\beta}_j \\mu_j$。请注意，这正是映射后截距 $\\hat{\\beta}^{\\text{map}}_0$ 的公式。\n对于原始模型，$X_j$ 单位变化的MEM为 $\\text{MEM}^{\\text{orig}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j$。$X_j$ 单位标准差变化的MEM为：\n$$\n\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j\\sigma_j\n$$\n对于标准化模型，标准化特征 $Z_j$ 的单位变化等同于原始特征 $X_j$ 的单位标准差变化。标准化模型的MEM在均值处（其中对于 $j \\ge 1$，$\\bar{Z}_j=0$）评估为：\n$$\n\\text{MEM}^{\\text{std}}_j = \\phi(\\hat{\\beta}^{\\text{std}}_0)\\hat{\\beta}^{\\text{std}}_j\n$$\n由于 $\\eta_{\\text{mean}} = \\hat{\\beta}^{\\text{map}}_0 \\approx \\hat{\\beta}^{\\text{std}}_0$，我们可以按照指示在两次计算中一致地使用 $\\phi(\\eta_{\\text{mean}})$。任务是比较 $\\text{MEM}^{\\text{orig,1sd}}_j$ 与 $\\text{MEM}^{\\text{std}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{std}}_j$。这种等价性直接源于系数映射：\n$$\n\\text{MEM}^{\\text{orig,1sd}}_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}_j\\sigma_j = \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{map}}_j \\approx \\phi(\\eta_{\\text{mean}})\\hat{\\beta}^{\\text{std}}_j = \\text{MEM}^{\\text{std}}_j\n$$\n因此，MEM等价性误差 $\\max_{j \\ge 1} | \\text{MEM}^{\\text{std}}_j - \\text{MEM}^{\\text{orig,1sd}}_j |$ 应该非常小，这反映了系数映射的准确性。\n\n计算过程包括：\n1. 使用固定的随机种子从潜变量模型为每个测试案例生成合成数据，以确保可复现性。\n2. 在原始数据上拟合probit模型以获得 $\\hat{\\beta}$。\n3. 计算特征均值 $\\mu_j$ 和标准差 $\\sigma_j$，并创建标准化特征矩阵 $Z$。\n4. 在标准化数据上拟合probit模型以获得 $\\hat{\\beta}^{\\text{std}}$。\n5. 从 $\\hat{\\beta}$ 计算 $\\hat{\\beta}^{\\text{map}}$ 并计算系数映射误差。\n6. 计算边际效应 $\\text{MEM}^{\\text{orig,1sd}}$ 和 $\\text{MEM}^{\\text{std}}$ 并计算MEM等价性误差。\n这为理论关系提供了完整的验证。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n\n    def fit_probit(X, y):\n        \"\"\"Fits a probit model using MLE.\"\"\"\n        \n        def neg_log_likelihood(beta, X, y):\n            \"\"\"Computes the negative log-likelihood for probit regression.\"\"\"\n            eta = X @ beta\n            # Use log-CDF and log-SF for numerical stability\n            log_phi = norm.logcdf(eta)\n            log_one_minus_phi = norm.logsf(eta)\n            \n            # log_phi can be -inf if eta is very small; log_one_minus_phi can be -inf if eta is very large.\n            # Replace -inf with a large negative number to prevent nan in sum.\n            log_phi[np.isneginf(log_phi)] = -1e9\n            log_one_minus_phi[np.isneginf(log_one_minus_phi)] = -1e9\n\n            log_L = np.sum(y * log_phi + (1 - y) * log_one_minus_phi)\n            return -log_L\n\n        def grad_neg_log_likelihood(beta, X, y):\n            \"\"\"Computes the gradient of the negative log-likelihood.\"\"\"\n            eta = X @ beta\n            pdf_val = norm.pdf(eta)\n            cdf_val = norm.cdf(eta)\n            \n            # Handle potential division by zero\n            # cdf_val can be 0 or 1 at the extremes of eta\n            # Hazard rate: lambda(eta) = pdf(eta) / cdf(eta)\n            # Reverse hazard rate: pdf(eta) / (1-cdf(eta))\n            # The limits are well-behaved, but numerically can be tricky\n            # Use a small epsilon to avoid 0 in denominator\n            epsilon = 1e-12\n            \n            term1 = y * (pdf_val / (cdf_val + epsilon))\n            term2 = (1 - y) * (pdf_val / (1 - cdf_val + epsilon))\n            \n            # The term inside the sum is (term1 - term2)\n            # The gradient for coeff beta_j is sum_i(X_ij * (term1_i - term2_i))\n            # which can be computed with a matrix-vector product\n            grad = X.T @ (term1 - term2)\n            return -grad\n\n        initial_guess = np.zeros(X.shape[1])\n        result = minimize(\n            fun=neg_log_likelihood,\n            x0=initial_guess,\n            args=(X, y),\n            method='BFGS',\n            jac=grad_neg_log_likelihood,\n            options={'gtol': 1e-7}\n        )\n        return result.x\n\n    def analyze_case(n, feature_specs, true_beta, rng):\n        \"\"\"Generates data, fits models, and computes errors for one case.\"\"\"\n        # 1. Generate data\n        p_features = len(feature_specs)\n        X_no_intercept = np.zeros((n, p_features))\n\n        for j, spec in enumerate(feature_specs):\n            dist_type, p1, p2 = spec\n            if dist_type == 'norm':\n                X_no_intercept[:, j] = rng.normal(loc=p1, scale=p2, size=n)\n            elif dist_type == 'uniform':\n                X_no_intercept[:, j] = rng.uniform(low=p1, high=p2, size=n)\n        \n        X_orig = np.c_[np.ones(n), X_no_intercept]\n        \n        epsilon = rng.normal(0, 1, n)\n        y_star = X_orig @ np.array(true_beta) + epsilon\n        y = (y_star  0).astype(int)\n\n        # 2. Fit model on original features\n        beta_hat_orig = fit_probit(X_orig, y)\n\n        # 3. Standardize features\n        mu = np.mean(X_no_intercept, axis=0)\n        sigma = np.std(X_no_intercept, axis=0)\n        X_std_no_intercept = (X_no_intercept - mu) / sigma\n        X_std = np.c_[np.ones(n), X_std_no_intercept]\n\n        # 4. Fit model on standardized features\n        beta_hat_std = fit_probit(X_std, y)\n\n        # 5. Compute reparameterization error\n        beta_hat_map = np.zeros_like(beta_hat_orig)\n        beta_hat_map[0] = beta_hat_orig[0] + np.dot(beta_hat_orig[1:], mu)\n        beta_hat_map[1:] = beta_hat_orig[1:] * sigma\n        mapping_error = np.max(np.abs(beta_hat_std - beta_hat_map))\n\n        # 6. Compute marginal effects error\n        eta_mean = beta_hat_orig[0] + np.dot(beta_hat_orig[1:], mu)\n        phi_eta_mean = norm.pdf(eta_mean)\n        \n        mem_orig_1sd = phi_eta_mean * beta_hat_orig[1:] * sigma\n        mem_std = phi_eta_mean * beta_hat_std[1:]\n        mem_error = np.max(np.abs(mem_std - mem_orig_1sd))\n        \n        return [round(mapping_error, 6), round(mem_error, 6)]\n\n    # Fixed random seed for reproducibility of the entire process\n    rng = np.random.default_rng(42)\n\n    test_cases = [\n        # Case A: n=1000, 1 feature N(2, 3^2), beta=(-0.5, 1.5)\n        (1000, [('norm', 2, 3)], [-0.5, 1.5]),\n        # Case B: n=2000, U[0,10], N(-1, 0.5^2), beta=(0.5, 0.2, -2.0)\n        (2000, [('uniform', 0, 10), ('norm', -1, 0.5)], [0.5, 0.2, -2.0]),\n        # Case C: n=1500, N(0,1), N(0,1), beta=(3.0, -0.5, 0.5)\n        (1500, [('norm', 0, 1), ('norm', 0, 1)], [3.0, -0.5, 0.5]),\n    ]\n\n    all_results = []\n    for n, specs, beta in test_cases:\n        case_results = analyze_case(n, specs, beta, rng)\n        all_results.append(case_results)\n\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3162301"}]}