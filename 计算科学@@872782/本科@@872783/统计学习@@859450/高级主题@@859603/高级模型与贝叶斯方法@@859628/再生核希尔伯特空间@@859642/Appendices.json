{"hands_on_practices": [{"introduction": "在我们深入研究再生核希尔伯特空间（RKHS）的强大应用之前，首先需要掌握其基本运算规则。本练习将引导你运用再生核希尔伯特空间的核心性质——再生性，来计算空间中一个简单函数的范数。通过这个练习 [@problem_id:1033834]，你将亲手体验内积和范数是如何通过核函数定义的，这是理解正则化和模型复杂度的关键第一步。", "problem": "令 $\\mathcal{H}$ 是一个定义在 $X = \\mathbb{R}$ 上的实值函数的再生核希尔伯特空间 (RKHS)。该空间 $\\mathcal{H}$ 由一个正定核 $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ 定义。$\\mathcal{H}$ 中的内积记为 $\\langle \\cdot, \\cdot \\rangle_{\\mathcal{H}}$，其特点是具有再生性质：对于任意函数 $f \\in \\mathcal{H}$ 和任意点 $x \\in \\mathbb{R}$，我们有 $f(x) = \\langle f, K_x \\rangle_{\\mathcal{H}}$，其中函数 $K_x \\in \\mathcal{H}$ 定义为 $K_x(y) = K(y, x)$。该性质的一个直接推论是，对于任意两点 $x, y \\in \\mathbb{R}$，相应核函数的内积为 $\\langle K_x, K_y \\rangle_{\\mathcal{H}} = K(x, y)$。函数 $f \\in \\mathcal{H}$ 的范数则由 $\\|f\\|_{\\mathcal{H}} = \\sqrt{\\langle f, f \\rangle_{\\mathcal{H}}}$ 给出。\n\n考虑 $\\mathcal{H}$ 配备高斯核的特定情况：\n$$\nK(x, y) = \\exp(-\\gamma(x-y)^2)\n$$\n其中 $\\gamma$ 是一个正实数参数。\n\n我们定义函数 $f \\in \\mathcal{H}$ 为以点 $a$ 和 $b$ 为中心的两个核函数的线性组合：\n$$\nf = c_a K_a - c_b K_b\n$$\n其中 $c_a, c_b$ 为实常数且 $a, b \\in \\mathbb{R}$。\n\n你的任务是，在参数值 $\\gamma=1$，$c_a=3$，$c_b=1$，$a=0$ 和 $b=1$ 的情况下，计算该函数的范数平方 $\\|f\\|_{\\mathcal{H}}^2$。", "solution": "1. 首先，我们应用内积的双线性以及再生性质 $\\langle K_x, K_y \\rangle_{\\mathcal{H}} = K(x, y)$ 来展开范数平方。对于形式为 $f = c_a K_a - c_b K_b$ 的函数，我们有：\n$$\n\\|f\\|_{\\mathcal{H}}^2 = \\langle c_a K_a - c_b K_b, c_a K_a - c_b K_b \\rangle_{\\mathcal{H}} = c_a^2 K(a, a) + c_b^2 K(b, b) - 2c_a c_b K(a, b)\n$$\n\n2. 代入具体的参数值 $c_a=3, c_b=1, a=0, b=1$ 和核函数 $K(x, y) = \\exp(-\\gamma(x-y)^2)$（其中 $\\gamma=1$），我们得到：\n- $K(0, 0) = \\exp(-(0-0)^2) = 1$\n- $K(1, 1) = \\exp(-(1-1)^2) = 1$\n- $K(0, 1) = \\exp(-(0-1)^2) = e^{-1}$\n\n3. 将这些值代入范数表达式：\n$$\n\\|f\\|_{\\mathcal{H}}^2 = (3)^2 \\cdot K(0,0) + (1)^2 \\cdot K(1,1) - 2 \\cdot 3 \\cdot 1 \\cdot K(0,1)\n$$\n$$\n= 9 \\cdot 1 + 1 \\cdot 1 - 6 e^{-1} = 10 - 6e^{-1}\n$$", "answer": "$$\\boxed{10-6e^{-1}}$$", "id": "1033834"}, {"introduction": "理解了单个函数后，我们进而探索如何处理整个数据集在特征空间中的表示。这个练习 [@problem_id:3170311] 聚焦于一个核心概念：特征空间中心化，它在核主成分分析（Kernel PCA）等方法中至关重要。你将推导出中心化操作对应的矩阵形式，并通过一个具体的例子发现该操作对于数据平移的不变性，从而深刻理解核方法如何捕捉数据的内在结构而非其绝对位置。", "problem": "考虑一个在输入集 $\\mathcal{X}$ 上的正半定核 $k$，其相关的特征映射为 $\\phi:\\mathcal{X}\\to\\mathcal{H}$，映入一个再生核希尔伯特空间 (RKHS) $\\mathcal{H}$，并满足对于所有 $x,x'\\in\\mathcal{X}$，有 $k(x,x')=\\langle \\phi(x),\\phi(x')\\rangle_{\\mathcal{H}}$。给定一个数据集 $\\{x_{i}\\}_{i=1}^{n}\\subset\\mathcal{X}$，并将其格拉姆矩阵 (Gram matrix) 记为 $K\\in\\mathbb{R}^{n\\times n}$，其元素为 $K_{ij}=k(x_{i},x_{j})$。定义样本均值嵌入 $\\mu:=\\frac{1}{n}\\sum_{i=1}^{n}\\phi(x_{i})\\in\\mathcal{H}$ 和中心化特征向量 $\\tilde{\\phi}(x_{i}):=\\phi(x_{i})-\\mu$。中心化格拉姆矩阵 $K^{\\mathrm{c}}\\in\\mathbb{R}^{n\\times n}$ 由内积 $K^{\\mathrm{c}}_{ij}:=\\langle \\tilde{\\phi}(x_{i}),\\tilde{\\phi}(x_{j})\\rangle_{\\mathcal{H}}$ 构成。\n\n1. 仅使用上述定义和基础线性代数，推导出一个关于 $K^{\\mathrm{c}}$ 的矩阵形式的闭式表达式，该表达式用 $K$ 以及一个由单位矩阵和全1向量构造的 $n\\times n$ 矩阵来表示。您的表达式必须完全化简，并且只依赖于 $K$ 和 $n$。\n\n2. 现在特化为 $\\mathbb{R}^{2}$ 上的线性核 $k(u,v)=u^{\\top}v$。构造两个各包含三个点的数据集：\n   - 数据集 $\\mathcal{A}$：$x_{1}=(0,0)$，$x_{2}=(1,0)$，$x_{3}=(0,1)$。\n   - 数据集 $\\mathcal{B}$：对于所有 $i$，$y_{i}=x_{i}+c$，其中 $c=(2,2)$。\n   计算未中心化的格拉姆矩阵 $K^{\\mathcal{A}}$ 和 $K^{\\mathcal{B}}$，然后计算它们的中心化版本 $K^{\\mathcal{A},\\mathrm{c}}$ 和 $K^{\\mathcal{B},\\mathrm{c}}$。\n\n3. 将中心化格拉姆矩阵之差的弗罗贝尼乌斯范数 $\\|K^{\\mathcal{A},\\mathrm{c}}-K^{\\mathcal{B},\\mathrm{c}}\\|_{F}$ 报告为一个实数。无需四舍五入。\n\n最后，在特征空间中均值嵌入的背景下解释该结果，说明为什么这两个数据集可以有相同的中心化格拉姆矩阵，但未中心化的格拉姆矩阵却不同。", "solution": "该解答分为三部分，遵循问题陈述的结构。\n\n**第一部分：中心化格拉姆矩阵表达式的推导**\n\n我们的任务是推导中心化格拉姆矩阵 $K^{\\mathrm{c}}$ 的一个矩阵形式闭式表达式，该表达式用未中心化的格拉姆矩阵 $K$ 和样本大小 $n$ 来表示。\n\n中心化格拉姆矩阵 $K^{\\mathrm{c}}$ 的元素定义为 $K^{\\mathrm{c}}_{ij} = \\langle \\tilde{\\phi}(x_{i}), \\tilde{\\phi}(x_{j}) \\rangle_{\\mathcal{H}}$，其中 $\\tilde{\\phi}(x_{i}) = \\phi(x_{i}) - \\mu$ 是中心化特征向量，而 $\\mu = \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k})$ 是样本均值嵌入。\n\n利用内积的双线性性质，我们展开 $K^{\\mathrm{c}}_{ij}$ 的表达式：\n$$\nK^{\\mathrm{c}}_{ij} = \\langle \\phi(x_{i}) - \\mu, \\phi(x_{j}) - \\mu \\rangle_{\\mathcal{H}} = \\langle \\phi(x_{i}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} - \\langle \\phi(x_{i}), \\mu \\rangle_{\\mathcal{H}} - \\langle \\mu, \\phi(x_{j}) \\rangle_{\\mathcal{H}} + \\langle \\mu, \\mu \\rangle_{\\mathcal{H}}\n$$\n\n我们现在使用核函数 $k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}}$ 和格拉姆矩阵的元素 $K_{ij} = k(x_i, x_j)$ 来表示每一项。\n\n第一项就是未中心化格拉姆矩阵元素的定义：\n$$\n\\langle \\phi(x_{i}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} = k(x_{i}, x_{j}) = K_{ij}\n$$\n\n对于第二项，我们代入 $\\mu$ 的定义：\n$$\n\\langle \\phi(x_{i}), \\mu \\rangle_{\\mathcal{H}} = \\left\\langle \\phi(x_{i}), \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}\\langle \\phi(x_{i}), \\phi(x_{k}) \\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}k(x_{i}, x_{k}) = \\frac{1}{n}\\sum_{k=1}^{n}K_{ik}\n$$\n\n由于内积的对称性，第三项是类似的：\n$$\n\\langle \\mu, \\phi(x_{j}) \\rangle_{\\mathcal{H}} = \\left\\langle \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}), \\phi(x_{j}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}\\langle \\phi(x_{k}), \\phi(x_{j}) \\rangle_{\\mathcal{H}} = \\frac{1}{n}\\sum_{k=1}^{n}k(x_{k}, x_{j}) = \\frac{1}{n}\\sum_{k=1}^{n}K_{kj}\n$$\n\n对于第四项，我们有：\n$$\n\\langle \\mu, \\mu \\rangle_{\\mathcal{H}} = \\left\\langle \\frac{1}{n}\\sum_{k=1}^{n}\\phi(x_{k}), \\frac{1}{n}\\sum_{l=1}^{n}\\phi(x_{l}) \\right\\rangle_{\\mathcal{H}} = \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}\\langle \\phi(x_{k}), \\phi(x_{l}) \\rangle_{\\mathcal{H}} = \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}K_{kl}\n$$\n\n将这些项组合起来，单个元素 $K^{\\mathrm{c}}_{ij}$ 的表达式为：\n$$\nK^{\\mathrm{c}}_{ij} = K_{ij} - \\frac{1}{n}\\sum_{k=1}^{n}K_{ik} - \\frac{1}{n}\\sum_{k=1}^{n}K_{kj} + \\frac{1}{n^2}\\sum_{k=1}^{n}\\sum_{l=1}^{n}K_{kl}\n$$\n\n为了以矩阵形式表示，设 $I_n$ 为 $n \\times n$ 单位矩阵，$J_n$ 为 $n \\times n$ 全1矩阵。上述表达式可以被识别为矩阵乘积的 $(i,j)$ 项。\n项 $\\frac{1}{n}\\sum_{k=1}^{n}K_{ik}$ 是矩阵 $\\frac{1}{n}KJ_n$ 的 $(i,j)$ 项。\n项 $\\frac{1}{n}\\sum_{k=1}^{n}K_{kj}$ 是矩阵 $\\frac{1}{n}J_nK$ 的 $(i,j)$ 项。\n项 $\\frac{1}{n^2}\\sum_{k,l}K_{kl}$ 是矩阵 $\\frac{1}{n^2}J_nKJ_n$ 的 $(i,j)$ 项。\n\n因此，我们可以写出矩阵方程：\n$$\nK^{\\mathrm{c}} = K - \\frac{1}{n}KJ_n - \\frac{1}{n}J_nK + \\frac{1}{n^2}J_nKJ_n\n$$\n\n这个表达式可以被因式分解。我们定义中心化矩阵 $H_n = I_n - \\frac{1}{n}J_n$。这个矩阵是按照要求由单位矩阵和全1矩阵构造的。我们现在计算乘积 $H_n K H_n$：\n$$\nH_n K H_n = \\left(I_n - \\frac{1}{n}J_n\\right) K \\left(I_n - \\frac{1}{n}J_n\\right) = \\left(K - \\frac{1}{n}J_nK\\right) \\left(I_n - \\frac{1}{n}J_n\\right)\n$$\n$$\n= K - \\frac{1}{n}KJ_n - \\frac{1}{n}J_nK + \\frac{1}{n^2}J_nKJ_n\n$$\n这正是我们为 $K^{\\mathrm{c}}$ 推导出的表达式。因此，闭式表达式为：\n$$\nK^{\\mathrm{c}} = H_n K H_n = \\left(I_n - \\frac{1}{n}J_n\\right) K \\left(I_n - \\frac{1}{n}J_n\\right)\n$$\n\n**第二部分：针对特定数据集的计算**\n\n我们特化到 $\\mathbb{R}^2$ 上的线性核 $k(u,v)=u^{\\top}v$ 且 $n=3$ 的情况。\n数据集为：\n- 数据集 $\\mathcal{A}$：$x_{1}=(0,0)$，$x_{2}=(1,0)$，$x_{3}=(0,1)$。\n- 数据集 $\\mathcal{B}$：$y_{i}=x_{i}+c$ 且 $c=(2,2)$，得到 $y_{1}=(2,2)$，$y_{2}=(3,2)$，$y_{3}=(2,3)$。\n\n首先，我们计算未中心化的格拉姆矩阵 $K^{\\mathcal{A}}$，其元素为 $K^{\\mathcal{A}}_{ij} = x_i^\\top x_j$：\n$$\nK^{\\mathcal{A}} = \\begin{pmatrix} x_1^\\top x_1  x_1^\\top x_2  x_1^\\top x_3 \\\\ x_2^\\top x_1  x_2^\\top x_2  x_2^\\top x_3 \\\\ x_3^\\top x_1  x_3^\\top x_2  x_3^\\top x_3 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\n\n接下来，我们计算未中心化的格拉姆矩阵 $K^{\\mathcal{B}}$，其元素为 $K^{\\mathcal{B}}_{ij} = y_i^\\top y_j$：\n$$\nK^{\\mathcal{B}} = \\begin{pmatrix} y_1^\\top y_1  y_1^\\top y_2  y_1^\\top y_3 \\\\ y_2^\\top y_1  y_2^\\top y_2  y_2^\\top y_3 \\\\ y_3^\\top y_1  y_3^\\top y_2  y_3^\\top y_3 \\end{pmatrix} = \\begin{pmatrix} 8  10  10 \\\\ 10  13  12 \\\\ 10  12  13 \\end{pmatrix}\n$$\n\n现在我们计算中心化的格拉姆矩阵。对于 $n=3$，中心化矩阵是：\n$$\nH_3 = I_3 - \\frac{1}{3}J_3 = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix}\n$$\n\n我们计算 $K^{\\mathcal{A},\\mathrm{c}} = H_3 K^{\\mathcal{A}} H_3$：\n$$\nK^{\\mathcal{A},\\mathrm{c}} = \\frac{1}{3}\\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix} \\begin{pmatrix} 0  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix}\n$$\n$$\n= \\frac{1}{9}\\begin{pmatrix} 0  -1  -1 \\\\ 0  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix} 2  -1  -1 \\\\ -1  2  -1 \\\\ -1  -1  2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 2  -1  -1 \\\\ -1  5  -4 \\\\ -1  -4  5 \\end{pmatrix}\n$$\n\n或者，对于线性核，特征映射为 $\\phi(x)=x$。中心化特征为 $\\tilde{x}_i = x_i - \\mu_x$。\n$\\mu_x = \\frac{1}{3}(x_1+x_2+x_3) = \\frac{1}{3}((0,0)+(1,0)+(0,1)) = (\\frac{1}{3},\\frac{1}{3})$。\n数据集 $\\mathcal{B}$ 的中心化特征向量为 $\\tilde{y}_i = y_i - \\mu_y$。其均值为 $\\mu_y = \\frac{1}{3}\\sum(x_i+c) = (\\frac{1}{3}\\sum x_i) + c = \\mu_x+c$。\n所以，$\\tilde{y}_i = (x_i+c) - (\\mu_x+c) = x_i - \\mu_x = \\tilde{x}_i$。\n由于两个数据集的中心化特征向量相同 ($\\tilde{y}_i = \\tilde{x}_i$)，它们的成对内积也必定相同。\n$$\nK^{\\mathcal{B},\\mathrm{c}}_{ij} = \\langle \\tilde{y}_i, \\tilde{y}_j \\rangle = \\tilde{y}_i^\\top \\tilde{y}_j = \\tilde{x}_i^\\top \\tilde{x}_j = K^{\\mathcal{A},\\mathrm{c}}_{ij}\n$$\n因此，$K^{\\mathcal{B},\\mathrm{c}} = K^{\\mathcal{A},\\mathrm{c}}$。我们得到：\n$$\nK^{\\mathcal{A},\\mathrm{c}} = K^{\\mathcal{B},\\mathrm{c}} = \\frac{1}{9}\\begin{pmatrix} 2  -1  -1 \\\\ -1  5  -4 \\\\ -1  -4  5 \\end{pmatrix}\n$$\n\n**第三部分：弗罗贝尼乌斯范数与解释**\n\n我们需要报告中心化格拉姆矩阵之差的弗罗贝尼乌斯范数。由于 $K^{\\mathcal{A},\\mathrm{c}} = K^{\\mathcal{B},\\mathrm{c}}$，它们的差是 $3 \\times 3$ 的零矩阵 $\\mathbf{0}_{3\\times 3}$。弗罗贝尼乌斯范数定义为 $\\|A\\|_F = \\sqrt{\\sum_{i,j} |A_{ij}|^2}$。\n对于零矩阵，此范数为：\n$$\n\\|K^{\\mathcal{A},\\mathrm{c}} - K^{\\mathcal{B},\\mathrm{c}}\\|_{F} = \\|\\mathbf{0}_{3\\times 3}\\|_{F} = \\sqrt{0} = 0\n$$\n\n**解释：**\n结果 $\\|K^{\\mathcal{A},\\mathrm{c}}-K^{\\mathcal{B},\\mathrm{c}}\\|_{F} = 0$ 表明，数据集 $\\mathcal{A}$ 和 $\\mathcal{B}$ 的中心化格拉姆矩阵是相同的，尽管它们未中心化的格拉姆矩阵 $K^{\\mathcal{A}}$ 和 $K^{\\mathcal{B}}$ 是不同的。在均值嵌入和特征映射的背景下，这种不变性有明确的解释。\n\n在特征空间中进行中心化，即通过操作 $\\tilde{\\phi}(x) = \\phi(x) - \\mu$，旨在去除关于数据云在 RKHS $\\mathcal{H}$ 中「位置」（均值 $\\mu$）的信息，而只保留其「形状」（数据点相对于其均值的几何结构）的信息。\n\n对于线性核 $k(u,v)=u^\\top v$ 的特定情况，特征映射可以看作是恒等映射，即 $\\phi(x)=x$。数据集 $\\mathcal{B}$ 是数据集 $\\mathcal{A}$ 经过一个常数向量 $c$ 的刚性平移得到的，即 $y_i = x_i + c$。由于特征映射是线性的，输入空间中的这种平移对应于特征空间中完全相同的平移：$\\phi(y_i) = \\phi(x_i+c) = x_i+c = \\phi(x_i) + \\phi(c)$（这里我们可以将 $c$ 视为 $\\phi(c)$）。\n\n因此，数据集 $\\mathcal{B}$ 的均值嵌入也相对于数据集 $\\mathcal{A}$ 的均值嵌入平移了 $c$：\n$\\mu_{\\mathcal{B}} = \\frac{1}{n}\\sum_i \\phi(y_i) = \\frac{1}{n}\\sum_i (\\phi(x_i)+c) = (\\frac{1}{n}\\sum_i \\phi(x_i)) + c = \\mu_{\\mathcal{A}} + c$。\n\n当我们计算数据集 $\\mathcal{B}$ 的中心化特征向量时，平移项 $c$ 完美地抵消了：\n$\\tilde{\\phi}(y_i) = \\phi(y_i) - \\mu_{\\mathcal{B}} = (\\phi(x_i) + c) - (\\mu_{\\mathcal{A}} + c) = \\phi(x_i) - \\mu_{\\mathcal{A}} = \\tilde{\\phi}(x_i)$。\n\n两个数据集的中心化特征向量结果是相同的。由于中心化格拉姆矩阵 $K^\\mathrm{c}$ 是由这些中心化向量的内积构成的，因此它对两个数据集必须是相同的。这表明对于线性核，数据相对于其均值的几何结构对于输入空间中数据集的全局平移是不变的。因此，依赖于 $K^\\mathrm{c}$ 的算法，例如核主成分分析 (Kernel PCA)，将从两个数据集中提取出完全相同的方差结构。", "answer": "$$\n\\boxed{0}\n$$", "id": "3170311"}]}