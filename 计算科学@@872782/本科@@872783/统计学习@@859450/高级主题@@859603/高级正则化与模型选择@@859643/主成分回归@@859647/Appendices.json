{"hands_on_practices": [{"introduction": "在主成分回归（PCR）的实际应用中，最关键的步骤之一是选择要保留的主成分数量 $k$。这个练习将指导您通过动手编程，比较两种选择 $k$ 的常用策略：一种是基于解释方差的启发式方法，另一种是基于交叉验证（CV）的预测误差评估。通过模拟不同场景，您将亲身体会到为何直接以预测性能为导向的交叉验证方法通常更为可靠，并理解仅关注预测变量 $X$ 自身变异性的局限性。", "problem": "您的任务是实现主成分回归（PCR），并比较两种选择成分数量 $k$ 的标准：通过目标累积解释方差阈值（例如 $\\tau = 0.95$）选择 $k$，以及通过最小化 K-折交叉验证（CV）的均方误差（MSE）来选择 $k$。您的实现必须基于核心定义和经过充分检验的公式：主成分分析（PCA）、奇异值分解（SVD）、最小二乘线性回归和 K-折交叉验证。\n\n基本原理：\n- 主成分分析（PCA）将一个中心化的数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 转换为按方差排序的正交主成分。如果 $X$ 的瘦奇异值分解（SVD）为 $X = U S V^\\top$，那么主轴是 $V$ 的列，奇异值是 $S$ 的对角线元素，样本协方差特征值与 $S^2$ 成正比。前 $k$ 个成分的解释方差比是 $S^2$ 的前 $k$ 个元素的累积和除以 $S^2$ 的所有元素的总和。\n- 主成分回归（PCR）使用前 $k$ 个主成分的得分来拟合响应变量 $y \\in \\mathbb{R}^n$ 的线性模型。为了包含截距，需要将 $X$ 和 $y$ 中心化，通过最小二乘法将 $y$ 对前 $k$ 个主成分得分列进行回归，然后将 $y$ 的训练均值加回到预测值中。\n- K-折交叉验证（CV）将索引 $\\{1,\\dots,n\\}$ 划分为 $K$ 个折，在 $K-1$ 个折上进行训练，在留出的一个折上进行验证，并计算各折验证均方误差（MSE）的平均值。在大小为 $m$ 的验证集上，预测值 $\\hat{y}$ 相对于真实值 $y$ 的 MSE 为 $\\frac{1}{m} \\sum_{i=1}^m (y_i - \\hat{y}_i)^2$。\n\n定义与任务：\n1. 按如下方式实现 PCR：\n   - 在训练集上，通过减去列均值来中心化 $X$，通过减去其均值来中心化 $y$。\n   - 对中心化后的训练预测变量计算瘦 SVD $X_{\\text{train}} = U S V^\\top$。\n   - 使用 $V$ 的前 $k$ 列构成主成分得分 $Z_{\\text{train}} = X_{\\text{train}} V_{[:,1:k]}$。\n   - 通过最小二乘法拟合 $Z_{\\text{train}}$ 上的系数。在测试集上进行预测时，使用训练集的均值对测试集的 $X$ 进行中心化，投影到相同的 $V_{[:,1:k]}$ 上，然后将 $y$ 的训练均值加回到中心化的预测值上。\n2. 通过解释方差选择 $k$：计算中心化的完整 $X$ 的奇异值 $S$，形成 $S^2$，并选择最小的 $k$，使得 $S^2$ 的前 $k$ 个元素的累积比率与 $S^2$ 所有元素总和之比至少为 $\\tau$。\n3. 通过 CV MSE 选择 $k$：对于每个 $k \\in \\{1,2,\\dots,p\\}$，计算具有 $k$ 个成分的 PCR 的平均 K-折 CV MSE，并选择使其最小化的 $k$。如果出现平局，则选择最小的 $k$。\n4. 对于每种情景，报告元组 $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$，其中 $k_{\\text{var}}$ 是通过解释方差选择的 $k$，$k_{\\text{cv}}$ 是通过 CV MSE 选择的 $k$。为保持一致性，将 MSE 值四舍五入到 4 位小数。\n\n数据生成模型：\n- 设 $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$ 是一个目标特征值向量。生成样本 $x_i \\in \\mathbb{R}^p$ 的方式为 $x_i = g_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$，其中 $g_i \\sim \\mathcal{N}(\\mathbf{0}, I_p)$ 独立同分布。这将产生协方差为 $\\operatorname{diag}(\\boldsymbol{\\lambda})$ 且主轴等于坐标轴（单位旋转）的数据。\n- 通过一个向量 $\\beta_{\\text{rot}} \\in \\mathbb{R}^p$ 在主成分基中定义一个系数向量 $\\beta \\in \\mathbb{R}^p$，该向量指定了每个主成分对响应的贡献。由于旋转是单位矩阵，直接使用 $\\beta = \\beta_{\\text{rot}}$。\n- 按 $y_i = x_i^\\top \\beta + \\epsilon_i$ 生成响应，其中 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ 独立同分布。\n\n测试套件：\n使用 $K = 5$ 的 K-折交叉验证来实现上述过程。主轴使用单位旋转，并考虑以下四种情景。对于每种情景，将用于数据生成和折分配的随机种子设置为指定的值。\n\n- 情景 1（信号与最高方差成分对齐）：\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (1, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 42$\n- 情景 2（信号与最低方差成分对齐）：\n  - $n = 200$, $p = 8$\n  - $\\boldsymbol{\\lambda} = (9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05)$\n  - $\\beta_{\\text{rot}} = (0, 0, 0, 0, 0, 0, 0, 1)$\n  - $\\sigma_\\epsilon = 0.2$\n  - $\\tau = 0.95$\n  - seed $= 7$\n- 情景 3（平坦方差谱，高噪声，多成分信号）：\n  - $n = 300$, $p = 12$\n  - $\\boldsymbol{\\lambda} = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)$\n  - $\\beta_{\\text{rot}} = (1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 1.5$\n  - $\\tau = 0.95$\n  - seed $= 123$\n- 情景 4（阈值为 1，低噪声，信号位于前两个成分上）：\n  - $n = 120$, $p = 10$\n  - $\\boldsymbol{\\lambda} = (5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2)$\n  - $\\beta_{\\text{rot}} = (1, 1, 0, 0, 0, 0, 0, 0, 0, 0)$\n  - $\\sigma_\\epsilon = 0.1$\n  - $\\tau = 1.0$\n  - seed $= 99$\n\n不涉及角度单位。不涉及物理单位。最终输出为数值，并且必须按如下格式报告为整数和浮点数的列表。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个情景贡献一个列表 $[k_{\\text{var}}, k_{\\text{cv}}, \\text{MSE}(k_{\\text{var}}), \\text{MSE}(k_{\\text{cv}})]$，其中 MSE 条目四舍五入到 4 位小数。例如，输出应类似于 $[[1,4,0.1234,0.0567],[\\dots],\\dots]$。", "solution": "该问题要求实现主成分回归（PCR），并比较两种选择最优主成分数量 $k$ 的不同方法。第一种方法是基于累积解释方差阈值 $\\tau$ 来选择 $k$。第二种方法是通过最小化 K-折交叉验证（CV）估计的均方误差（MSE）来选择 $k$。该比较在四种具有指定数据生成参数的模拟情景下进行。\n\n### 1. 数据生成模型\n\n数据由线性模型生成。对于指定的样本数 $n$ 和预测变量数 $p$，预测变量矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的构建方式使其总体协方差矩阵为对角矩阵，其对角线元素由向量 $\\boldsymbol{\\lambda} \\in \\mathbb{R}^p$ 给出。具体来说，$X$ 的每一行 $x_i^\\top$ 的生成方式如下：\n$$ x_i = G_i \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}}) $$\n其中 $G_i$ 是一个行向量，包含 $p$ 个来自标准正态分布 $\\mathcal{N}(0, 1)$ 的独立样本。这等同于生成一个元素为独立同分布 $\\mathcal{N}(0, 1)$ 的矩阵 $G \\in \\mathbb{R}^{n \\times p}$，并设置 $X = G \\operatorname{diag}(\\sqrt{\\boldsymbol{\\lambda}})$。该数据分布的总体主轴是标准基向量（一个单位旋转）。\n\n响应向量 $y \\in \\mathbb{R}^n$ 根据以下线性模型生成：\n$$ y_i = x_i^\\top \\beta + \\epsilon_i $$\n其中 $\\beta \\in \\mathbb{R}^p$ 是真实回归系数向量，$\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ 是独立同分布的误差项。系数向量 $\\beta$ 直接由 $\\beta = \\beta_{\\text{rot}}$ 给出，其中 $\\beta_{\\text{rot}}$ 指定了沿每个主成分轴的信号强度。\n\n### 2. 主成分回归（PCR）算法\n\nPCR 是一个两阶段过程，首先使用主成分分析（PCA）降低预测变量的维度，然后对得到的成分进行线性回归。对于训练集 $(X_{\\text{train}}, y_{\\text{train}})$ 和测试集 $X_{\\text{test}}$，该过程规定如下：\n\n1.  **数据中心化**：将训练数据中心化以使其均值为零。使用训练数据的均值对测试数据进行中心化，以防止信息泄露。\n    $$ \\bar{x}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} x_{\\text{train},i}, \\quad \\bar{y}_{\\text{train}} = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} y_{\\text{train},i} $$\n    $$ X_{\\text{c,train}} = X_{\\text{train}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top, \\quad y_{\\text{c,train}} = y_{\\text{train}} - \\bar{y}_{\\text{train}} $$\n    $$ X_{\\text{c,test}} = X_{\\text{test}} - \\mathbf{1}\\bar{x}_{\\text{train}}^\\top $$\n\n2.  **主成分提取**：通过计算中心化后的训练预测变量矩阵的奇异值分解（SVD）来找到主轴：\n    $$ X_{\\text{c,train}} = U S V^\\top $$\n    $V \\in \\mathbb{R}^{p \\times p}$ 的列是主轴（或载荷向量）。\n\n3.  **得分投影**：将训练数据投影到前 $k$ 个主轴上，以获得得分矩阵 $Z_{\\text{train},k} \\in \\mathbb{R}^{n_{\\text{train}} \\times k}$：\n    $$ Z_{\\text{train},k} = X_{\\text{c,train}} V_k $$\n    其中 $V_k \\in \\mathbb{R}^{p \\times k}$ 包含 $V$ 的前 $k$ 列。\n\n4.  **对得分进行回归**：使用普通最小二乘法拟合一个线性模型，将中心化的响应 $y_{\\text{c,train}}$ 对得分 $Z_{\\text{train},k}$ 进行回归，得到系数 $\\hat{\\theta}_k \\in \\mathbb{R}^k$：\n    $$ \\hat{\\theta}_k = (Z_{\\text{train},k}^\\top Z_{\\text{train},k})^{-1} Z_{\\text{train},k}^\\top y_{\\text{c,train}} $$\n\n5.  **预测**：对测试集进行预测时，首先将中心化的测试数据 $X_{\\text{c,test}}$ 投影到相同的 $k$ 个主轴上以获得测试得分 $Z_{\\text{test},k}$，然后应用拟合的系数 $\\hat{\\theta}_k$，最后将训练响应的均值加回：\n    $$ Z_{\\text{test},k} = X_{\\text{c,test}} V_k $$\n    $$ \\hat{y}_{\\text{test}} = Z_{\\text{test},k} \\hat{\\theta}_k + \\bar{y}_{\\text{train}} $$\n\n### 3. 成分选择 I：累积解释方差\n\n此方法根据捕获的预测变量方差的比例来选择成分数量 $k$。它应用于完整数据集 $X$。\n\n1.  中心化完整数据矩阵：$X_c = X - \\mathbf{1}\\bar{x}^\\top$。\n2.  计算其 SVD：$X_c = U S V^\\top$。$S$ 的对角线元素是奇异值，$s_1 \\ge s_2 \\ge \\dots \\ge s_p \\ge 0$。\n3.  第 $j$ 个主成分解释的方差与 $s_j^2$ 成正比。前 $k$ 个成分的累积解释方差比（CEVR）为：\n    $$ \\text{CEVR}(k) = \\frac{\\sum_{j=1}^k s_j^2}{\\sum_{j=1}^p s_j^2} $$\n4.  给定一个阈值 $\\tau$，成分数量 $k_{\\text{var}}$ 被选为满足该阈值的最小 $k$：\n    $$ k_{\\text{var}} = \\min \\{ k \\in \\{1, 2, \\dots, p\\} \\mid \\text{CEVR}(k) \\ge \\tau \\} $$\n\n### 4. 成分选择 II：K-折交叉验证\n\n此方法通过直接估计每个可能的 $k$ 值的 PCR 模型的预测误差来选择 $k$。\n\n1.  将包含 $n$ 个样本的数据集划分为 $K$ 个大小近似相等的不相交的折。对于本问题，$K=5$。\n2.  对于每个候选成分数 $k \\in \\{1, 2, \\dots, p\\}$：\n    a. 一个外层循环遍历每个折 $j=1, \\dots, K$。\n    b. 在每次迭代中，第 $j$ 个折被指定为验证集 $(X_{\\text{val}}, y_{\\text{val}})$，其余的 $K-1$ 个折构成训练集 $(X_{\\text{train}}, y_{\\text{train}})$。\n    c. 在 $(X_{\\text{train}}, y_{\\text{train}})$ 上训练一个具有 $k$ 个成分的 PCR 模型。\n    d. 使用训练好的模型对验证集 $X_{\\text{val}}$ 进行预测，得到 $\\hat{y}_{\\text{val}}$。\n    e. 计算该折的均方误差（MSE）：$\\text{MSE}_j(k) = \\frac{1}{n_{\\text{val}}} \\sum_{i \\in \\text{fold }j} (y_{\\text{val},i} - \\hat{y}_{\\text{val},i})^2$。\n3.  在遍历所有折之后，$k$ 个成分的交叉验证 MSE 是各折特定 MSE 的平均值：\n    $$ \\text{CV-MSE}(k) = \\frac{1}{K} \\sum_{j=1}^K \\text{MSE}_j(k) $$\n4.  最优成分数 $k_{\\text{cv}}$ 是使该平均误差最小化的那个。平局通过选择最小的 $k$ 来打破。\n    $$ k_{\\text{cv}} = \\operatorname{argmin}_{k \\in \\{1, \\dots, p\\}} \\text{CV-MSE}(k) $$\n\n### 5. 实现与评估\n\n对于每个情景，我们首先生成数据 $(X, y)$。然后我们应用两种选择标准。基于方差的方法得出 $k_{\\text{var}}$。对所有 $k \\in \\{1, \\dots, p\\}$ 运行交叉验证过程，生成一个 CV-MSE 值的列表。从这个列表中，我们确定 $k_{\\text{cv}}$ 为最小化者。最终报告的值是 $k_{\\text{var}}$、$k_{\\text{cv}}$、为 $k_{\\text{var}}$ 预先计算的 CV-MSE（即 $\\text{CV-MSE}(k_{\\text{var}})$）以及最小 CV-MSE（即 $\\text{CV-MSE}(k_{\\text{cv}})$）。这为两种所选模型的预测性能提供了直接比较。使用随机种子来确保数据生成和交叉验证中折分配的可复现性。", "answer": "```python\nimport numpy as np\n\ndef generate_data(n, p, lambdas, beta_rot, sigma_eps, seed):\n    \"\"\"Generates data for a given scenario.\"\"\"\n    rng = np.random.default_rng(seed)\n    # Generate standard normal data\n    G = rng.normal(size=(n, p))\n    # Scale columns to achieve target covariance\n    sqrt_lambdas = np.sqrt(lambdas)\n    X = G * sqrt_lambdas\n    \n    # Generate response\n    # Since principal axes are standard basis, beta = beta_rot\n    beta = beta_rot\n    epsilon = rng.normal(scale=sigma_eps, size=n)\n    y = X @ beta + epsilon\n    \n    return X, y\n\ndef pcr_predict(X_train, y_train, X_test, k):\n    \"\"\"\n    Fits a PCR model on training data and predicts on test data.\n    \"\"\"\n    # 1. Centering\n    x_mean = np.mean(X_train, axis=0)\n    y_mean = np.mean(y_train)\n    \n    X_train_c = X_train - x_mean\n    y_train_c = y_train - y_mean\n    X_test_c = X_test - x_mean\n    \n    # 2. PCA on training data\n    # The problem specifies thin SVD X = U S V^T.\n    # np.linalg.svd returns U, s, Vt where s is a 1D array of singular values.\n    try:\n        _, s, Vt = np.linalg.svd(X_train_c, full_matrices=False)\n    except np.linalg.LinAlgError:\n         # In rare cases with highly collinear data, SVD can fail.\n         # This shouldn't happen with the specified data generation, but as a fallback:\n         return np.full(X_test.shape[0], y_mean) # Predict the mean\n         \n    V = Vt.T\n    \n    # 3. Project onto first k components\n    V_k = V[:, :k]\n    Z_train_k = X_train_c @ V_k\n    \n    # 4. Fit linear model on scores\n    # Using lstsq for numerical stability\n    theta_k, _, _, _ = np.linalg.lstsq(Z_train_k, y_train_c, rcond=None)\n    \n    # 5. Predict on test data\n    Z_test_k = X_test_c @ V_k\n    y_pred_c = Z_test_k @ theta_k\n    y_pred = y_pred_c + y_mean\n    \n    return y_pred\n\ndef select_k_variance(X, tau):\n    \"\"\"\n    Selects k based on cumulative explained variance.\n    \"\"\"\n    # Center the data\n    X_c = X - np.mean(X, axis=0)\n    \n    # SVD\n    _, s, _ = np.linalg.svd(X_c, full_matrices=False)\n    \n    # Explained variance ratios\n    s_squared = s**2\n    total_variance = np.sum(s_squared)\n    \n    if total_variance == 0:\n        return 1\n\n    cumulative_variance_ratio = np.cumsum(s_squared) / total_variance\n    \n    # Find smallest k such that ratio >= tau\n    k_var_candidates = np.where(cumulative_variance_ratio >= tau)[0]\n    if len(k_var_candidates) == 0:\n        # This occurs if tau=1.0 and there's numerical imprecision\n        k_var = X.shape[1]\n    else:\n        k_var = k_var_candidates[0] + 1 # +1 for 1-based indexing\n        \n    return k_var\n\ndef select_k_cv(X, y, p, K, seed):\n    \"\"\"\n    Selects k by K-fold Cross-Validation and returns CV MSE for all k.\n    \"\"\"\n    n = X.shape[0]\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n)\n    rng.shuffle(indices)\n    \n    folds = np.array_split(indices, K)\n    \n    cv_mses = []\n    \n    for k in range(1, p + 1):\n        fold_mses = []\n        for j in range(K):\n            val_indices = folds[j]\n            train_indices = np.concatenate([folds[i] for i in range(K) if i != j])\n            \n            X_train, y_train = X[train_indices], y[train_indices]\n            X_val, y_val = X[val_indices], y[val_indices]\n            \n            y_pred = pcr_predict(X_train, y_train, X_val, k)\n            \n            mse = np.mean((y_val - y_pred)**2)\n            fold_mses.append(mse)\n            \n        avg_mse = np.mean(fold_mses)\n        cv_mses.append(avg_mse)\n        \n    cv_mses = np.array(cv_mses)\n    # Get k that minimizes CV MSE, breaking ties by choosing smallest k.\n    # np.argmin() naturally does this.\n    k_cv = np.argmin(cv_mses) + 1 # +1 for 1-based indexing\n    \n    return k_cv, cv_mses\n\ndef solve():\n    \"\"\"\n    Main function to run the scenarios and print results.\n    \"\"\"\n    scenarios = [\n        {'n': 200, 'p': 8, 'lambdas': np.array([9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05]), \n         'beta_rot': np.array([1, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 0.2, 'tau': 0.95, 'seed': 42},\n        {'n': 200, 'p': 8, 'lambdas': np.array([9, 3, 1, 0.5, 0.3, 0.2, 0.1, 0.05]), \n         'beta_rot': np.array([0, 0, 0, 0, 0, 0, 0, 1]), 'sigma_eps': 0.2, 'tau': 0.95, 'seed': 7},\n        {'n': 300, 'p': 12, 'lambdas': np.ones(12), \n         'beta_rot': np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 1.5, 'tau': 0.95, 'seed': 123},\n        {'n': 120, 'p': 10, 'lambdas': np.array([5, 4, 3, 2, 1.5, 1, 0.8, 0.6, 0.4, 0.2]), \n         'beta_rot': np.array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0]), 'sigma_eps': 0.1, 'tau': 1.0, 'seed': 99}\n    ]\n    \n    K = 5\n    results = []\n\n    for params in scenarios:\n        X, y = generate_data(params['n'], params['p'], params['lambdas'], \n                             params['beta_rot'], params['sigma_eps'], params['seed'])\n        \n        # Method 1: Variance Explained Threshold\n        k_var = select_k_variance(X, params['tau'])\n        \n        # Method 2: Cross-Validation\n        k_cv, cv_mses_all_k = select_k_cv(X, y, params['p'], K, params['seed'])\n        \n        # Get MSEs for the chosen k values\n        mse_for_k_var = cv_mses_all_k[k_var - 1]\n        mse_for_k_cv = cv_mses_all_k[k_cv - 1]\n        \n        results.append([k_var, k_cv, mse_for_k_var, mse_for_k_cv])\n        \n    # Format output as specified\n    formatted_results = []\n    for res in results:\n        # Rounding is handled by the format specifier\n        s = f\"[{res[0]},{res[1]},{res[2]:.4f},{res[3]:.4f}]\"\n        formatted_results.append(s)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3160814"}, {"introduction": "上一个练习通过实践展示了基于交叉验证的模型选择通常优于基于解释方差的方法。这个练习则通过一个思想实验，从根本上揭示其背后的原因。通过分析一个预测变量已经正交的特殊情况，我们可以清晰地看到，主成分分析（PCA）本质上是一种“无监督”的技术，它在选择成分时只考虑了预测变量 $X$ 的内部结构，而完全忽略了响应变量 $Y$。这个练习将帮助您深刻理解PCR的这一内在特性，以及为什么它可能会丢弃对预测至关重要的低方差成分。", "problem": "考虑一个中心化的预测矩阵 $X \\in \\mathbb{R}^{n \\times p}$，其列两两正交，因此 $X^\\top X$ 是对角矩阵，以及一个中心化的响应向量 $Y \\in \\mathbb{R}^{n}$。对 $X$ 进行主成分分析 (PCA)，主成分回归 (PCR) 定义为将 $Y$ 对前 $k$ 个主成分得分进行回归，这些主成分得分对应于 $X$ 的经验协方差的 $k$ 个最大特征值。假设在回归阶段使用普通最小二乘法。根据 PCA 和 PCR 的基本定义，选择所有正确的陈述。\n\nA. 当 $X$ 的列正交时，具有 $k$ 个成分的 PCR 等价于对 $X$ 中具有最大样本方差的 $k$ 个列进行普通最小二乘法。\n\nB. 在列正交的情况下，通过选择 $k$ 个最大方差的主成分来选择 $k$ 对于预测是可取的，因为高方差方向通常包含更多关于 $Y$ 的信号。\n\nC. 存在这样的数据生成过程：最具预测性的方向位于 $X$ 的低方差列中，因此基于方差的选择会损害样本外预测性能。\n\nD. 如果 $Y$ 中的噪声方差很高且 $X$ 的列是正交的，那么使用所有 $p$ 个列总是比使用任何 $k  p$ 个成分产生更低的样本外预测误差。", "solution": "该问题的设置旨在通过一个特例来探究主成分回归（PCR）的根本性质。我们已知预测变量矩阵 $X$ 的列是中心化且两两正交的。让我们分析这对此处的 PCA 意味着什么。\n\n**PCA 在正交预测变量上的应用**\n\n1.  **协方差矩阵**：$X$ 的样本协方差矩阵为 $S = \\frac{1}{n-1} X^\\top X$。由于 $X$ 的列 $x_1, \\dots, x_p$ 是正交的，即对于 $i \\neq j$，有 $x_i^\\top x_j = 0$，因此 $X^\\top X$ 是一个对角矩阵：\n    $$ X^\\top X = \\text{diag}(\\|x_1\\|^2, \\|x_2\\|^2, \\dots, \\|x_p\\|^2) $$\n    因此，协方差矩阵 $S$ 也是对角的，其对角线元素为 $S_{jj} = \\frac{1}{n-1} \\|x_j\\|^2$。由于列是中心化的，这正是第 $j$ 列的样本方差。\n\n2.  **特征分解**：PCA 寻找 $S$ 的特征向量（主成分载荷）和特征值。对于一个对角矩阵，特征向量就是标准基向量 $e_j = (0, \\dots, 1, \\dots, 0)^\\top$，特征值就是对角线上的元素，即各列的方差。\n\n3.  **主成分得分**：第 $j$ 个主成分得分向量是原始数据在第 $j$ 个主成分方向上的投影：$z_j = X v_j$。在这里，载荷向量 $v_j$ 是标准基向量 $e_j$（经过适当重新排序以使方差递减）。因此，$z_j = X e_j = x_j$。这意味着**在列正交的情况下，主成分得分就是原始的预测变量列**。\n\n4.  **PCR 步骤**：PCR 选择与 $k$ 个最大特征值相关联的主成分。由于特征值就是原始列的方差，这等同于选择具有最大样本方差的 $k$ 个原始预测变量列。然后，PCR 将响应 $Y$ 对这 $k$ 个选定的列进行普通最小二ろしく乘（OLS）回归。\n\n现在我们来逐一评估每个选项。\n\n**A. 当 $X$ 的列正交时，具有 $k$ 个成分的 PCR 等价于对 $X$ 中具有最大样本方差的 $k$ 个列进行普通最小二乘法。**\n正如我们上面推导的，这正是 PCR 在这种特殊情况下的操作方式。它执行了一个两步过程：首先，根据样本方差对原始列进行排序并选择前 $k$ 个；其次，对这些选定的列进行 OLS 回归。这个陈述是正确的。\n\n**B. 在列正交的情况下，通过选择 $k$ 个最大方差的主成分来选择 $k$ 对于预测是可取的，因为高方差方向通常包含更多关于 $Y$ 的信号。**\n这个陈述提供了一个支持基于方差进行选择的理由。然而，这个理由是一个启发式的假设，并不总是成立。一个预测变量的方差（$X$ 的内部属性）与其对响应变量的预测能力（$X$ 和 $Y$ 之间的关系）之间没有必然的联系。PCR 是一种“无监督”的降维技术，因为它在选择成分时完全忽略了 $Y$。因此，声称高方差意味着高信号是具有误导性的。这个陈述不正确。\n\n**C. 存在这样的数据生成过程：最具预测性的方向位于 $X$ 的低方差列中，因此基于方差的选择会损害样本外预测性能。**\n这是陈述 B 中错误逻辑的反例。我们可以很容易地构建这样一个场景。假设我们有两个正交预测变量 $x_1$ 和 $x_2$，其中 $\\text{Var}(x_1) = 100$（高方差），$\\text{Var}(x_2) = 0.1$（低方差）。假设真实的数据生成模型为 $Y = 50 x_2 + \\epsilon$，其中 $x_1$ 与 $Y$ 无关。在这种情况下，所有预测信号都包含在低方差变量 $x_2$ 中。如果我们使用 $k=1$ 的 PCR，它会选择方差最高的变量 $x_1$ 进行回归，并丢弃真正具有预测性的变量 $x_2$。这将导致一个预测性能非常差的模型。因此，基于方差的选择确实可能是有害的。这个陈述是正确的。\n\n**D. 如果 $Y$ 中的噪声方差很高且 $X$ 的列是正交的，那么使用所有 $p$ 个列总是比使用任何 $k  p$ 个成分产生更低的样本外预测误差。**\n这个陈述与偏倚-方差权衡的原理相悖。使用所有 $p$ 个列相当于进行一个完整的 OLS 回归。这个模型是无偏的，但其预测方差是所有 $p$ 个系数估计方差的总和。当噪声方差很高时，这些系数估计会非常不稳定，导致预测方差很大。通过使用 $k  p$ 个成分（即丢弃一些预测变量），PCR 引入了偏倚（如果被丢弃的变量的真实系数非零），但它也减小了模型的方差（因为需要估计的系数更少）。在高噪声的情况下，方差的减少量很可能超过由偏倚引入的误差增量，从而使得 PCR 模型（$k  p$）的总体预测误差低于完整的 OLS 模型（$k = p$）。因此，这个陈述是错误的。\n\n综上所述，正确的陈述是 A 和 C。", "answer": "$$\\boxed{AC}$$", "id": "3160754"}, {"introduction": "虽然交叉验证是选择最佳模型复杂度的强大工具，但其重复拟合模型的计算成本可能非常高，尤其是在处理大型数据集时。这个练习将引导您推导一个优雅且极为实用的理论结果：预测残差平方和（PRESS）统计量。您将证明，对于主成分回归（以及其他线性模型），我们可以通过一次完整的模型拟合，解析地计算出留一法交叉验证（LOOCV）的误差，从而避免了 $n$ 次重复计算。这项练习不仅能加深您对模型评估理论的理解，也为您提供了在实践中高效执行模型选择的利器。", "problem": "一个材料工程团队正在研究一种新型高熵合金的性能。他们的目标是基于一组 $p$ 个定量预测变量，为该合金的断裂韧性（用 $y$ 表示）建立一个预测模型。这些预测变量由向量 $\\mathbf{x} \\in \\mathbb{R}^p$ 表示，包括组成元素的浓度和各种热机械加工参数。\n\n该团队收集了一个包含 $n$ 个合金样本的数据集。设 $n \\times p$ 矩阵 $\\mathbf{X}$ 包含所有样本的预测变量值，而 $n \\times 1$ 向量 $\\mathbf{y}$ 包含相应的断裂韧性测量值。为简便起见，假设 $\\mathbf{X}$ 的列和向量 $\\mathbf{y}$ 已经过中心化处理，均值为零。\n\n由于合金设计的特性，许多预测变量预计会高度相关。为了解决这种多重共线性问题，该团队决定使用主成分回归（Principal Component Regression, PCR），这是一个两阶段的过程。首先，对预测变量矩阵 $\\mathbf{X}$ 进行主成分分析（Principal Component Analysis, PCA），以获得一组新的正交变量，称为主成分。其次，将响应 $\\mathbf{y}$ 对这些主成分的一个子集进行回归。\n\n使用前 $k$ 个主成分的 PCR 模型定义如下：\n1. 主成分得分计算为 $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$，其中 $\\mathbf{V}_k$ 是一个 $p \\times k$ 矩阵，其列是前 $k$ 个主成分载荷向量（即 $\\mathbf{X}$ 的样本协方差矩阵的特征向量）。\n2. 对 $\\mathbf{Z}_k$ 上的 $\\mathbf{y}$ 进行线性回归，以获得拟合值 $\\hat{\\mathbf{y}}^{(k)}$。\n\nPCR 的一个关键步骤是选择最佳的主成分数量 $k$。一种常见的方法是选择能使通过交叉验证估计的预测误差最小化的 $k$。该团队计划使用留一法交叉验证（Leave-One-Out Cross-Validation, LOOCV）。性能指标是预测残差平方和（Predicted Residual Sum of Squares, PRESS），定义为 $\\text{PRESS}(k) = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)}^{(k)})^2$，其中 $\\hat{y}_{(-i)}^{(k)}$ 是使用除观测值 $i$ 之外的所有数据训练的、包含 $k$ 个主成分的 PCR 模型对第 $i$ 个观测值的预测。\n\n尽管这个过程定义明确，但重复拟合 PCR 模型 $n$ 次（每次留出一个观测值）在计算上是低效的。你的任务是推导出一个 $\\text{PRESS}(k)$ 的高效解析表达式，以避免这种重复拟合。\n\n将 $\\text{PRESS}(k)$ 统计量表示为一种仅需在包含 $n$ 个观测值的完整数据集上拟合一次 PCR 模型即可计算的形式。你的最终表达式应包含普通残差 $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$ 和与 PCR 模型相关的适当投影矩阵（或“帽子矩阵”）的对角元素 $h_{ii}^{(k)}$。", "solution": "目标是为包含 $k$ 个主成分的主成分回归（PCR）模型找到一个计算上高效的预测残差平方和（PRESS）统计量公式。\n\n首先，我们回顾一个适用于任何形式为 $\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$ 的线性模型的一般性结果，其中 $\\mathbf{H}$ 是投影矩阵或“帽子矩阵”。从一个不包含第 $i$ 个观测值拟合的模型中对该观测值的预测 $\\hat{y}_{(-i)}$，可以与在所有数据上拟合的模型的结果相关联。第 $i$ 个观测值的留一法交叉验证（LOOCV）残差由以下著名公式给出：\n$$ y_i - \\hat{y}_{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} $$\n其中 $\\hat{y}_i$ 是来自完整模型的第 $i$ 个观测值的拟合值，$e_i = y_i - \\hat{y}_i$ 是普通残差，而 $h_{ii}$ 是帽子矩阵 $\\mathbf{H}$ 的第 $i$ 个对角元素。\n\nPRESS 统计量是这些 LOOCV 残差的平方和：\n$$ \\text{PRESS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_{(-i)})^2 = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i}{1-h_{ii}} \\right)^2 $$\n为了将此应用于我们的 PCR 问题，我们需要确定具有 $k$ 个主成分的 PCR 模型的特定帽子矩阵 $\\mathbf{H}^{(k)}$，并找出其对角元素 $h_{ii}^{(k)}$。\n\n在 PCR 中，模型使用主成分得分 $\\mathbf{Z}_k = \\mathbf{X}\\mathbf{V}_k$ 作为预测变量来构建。中心化响应 $\\mathbf{y}$ 对 $\\mathbf{Z}_k$ 的线性回归产生估计系数：\n$$ \\hat{\\boldsymbol{\\theta}}_k = (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\n拟合值 $\\hat{\\mathbf{y}}^{(k)}$ 则由下式给出：\n$$ \\hat{\\mathbf{y}}^{(k)} = \\mathbf{Z}_k \\hat{\\boldsymbol{\\theta}}_k = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T \\mathbf{y} $$\n通过将其与一般形式 $\\hat{\\mathbf{y}}^{(k)} = \\mathbf{H}^{(k)} \\mathbf{y}$ 进行比较，我们可以确定具有 $k$ 个主成分的 PCR 的帽子矩阵为：\n$$ \\mathbf{H}^{(k)} = \\mathbf{Z}_k (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} \\mathbf{Z}_k^T $$\n得分矩阵 $\\mathbf{Z}_k$ 的列是得分向量 $\\mathbf{z}_1, \\mathbf{z}_2, \\dots, \\mathbf{z}_k$。主成分的一个关键性质是这些得分向量彼此正交：当 $j \\neq m$ 时，$\\mathbf{z}_j^T \\mathbf{z}_m = 0$。这意味着矩阵 $\\mathbf{Z}_k^T \\mathbf{Z}_k$ 是一个 $k \\times k$ 的对角矩阵：\n$$ \\mathbf{Z}_k^T \\mathbf{Z}_k = \\text{diag}(\\mathbf{z}_1^T\\mathbf{z}_1, \\mathbf{z}_2^T\\mathbf{z}_2, \\dots, \\mathbf{z}_k^T\\mathbf{z}_k) = \\text{diag}(\\|\\mathbf{z}_1\\|^2, \\|\\mathbf{z}_2\\|^2, \\dots, \\|\\mathbf{z}_k\\|^2) $$\n其中 $\\|\\mathbf{z}_j\\|^2 = \\sum_{i=1}^{n} z_{ij}^2$ 是第 $j$ 个得分向量的欧几里得范数的平方。\n\n其逆矩阵也是对角的：\n$$ (\\mathbf{Z}_k^T \\mathbf{Z}_k)^{-1} = \\text{diag}(\\|\\mathbf{z}_1\\|^{-2}, \\|\\mathbf{z}_2\\|^{-2}, \\dots, \\|\\mathbf{z}_k\\|^{-2}) $$\n将此代入 $\\mathbf{H}^{(k)}$ 的表达式，我们看到帽子矩阵是一系列秩一投影矩阵的和：\n$$ \\mathbf{H}^{(k)} = \\begin{pmatrix} \\mathbf{z}_1  \\dots  \\mathbf{z}_k \\end{pmatrix} \\begin{pmatrix} \\|\\mathbf{z}_1\\|^{-2}   \\\\  \\ddots  \\\\   \\|\\mathbf{z}_k\\|^{-2} \\end{pmatrix} \\begin{pmatrix} \\mathbf{z}_1^T \\\\ \\vdots \\\\ \\mathbf{z}_k^T \\end{pmatrix} = \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} $$\n我们感兴趣的是该矩阵的对角元素 $h_{ii}^{(k)}$。外积矩阵 $\\mathbf{z}_j \\mathbf{z}_j^T$ 的第 $(i,i)$ 个元素就是 $z_{ij}^2$，其中 $z_{ij}$ 是向量 $\\mathbf{z}_j$ 的第 $i$ 个元素。因此，$\\mathbf{H}^{(k)}$ 的对角元素为：\n$$ h_{ii}^{(k)} = \\left( \\sum_{j=1}^{k} \\frac{\\mathbf{z}_j \\mathbf{z}_j^T}{\\|\\mathbf{z}_j\\|^2} \\right)_{ii} = \\sum_{j=1}^{k} \\frac{(\\mathbf{z}_j \\mathbf{z}_j^T)_{ii}}{\\|\\mathbf{z}_j\\|^2} = \\sum_{j=1}^{k} \\frac{z_{ij}^2}{\\sum_{l=1}^{n} z_{lj}^2} $$\n现在我们拥有了所有的组成部分。对于一个包含 $k$ 个主成分的 PCR 模型，其 PRESS 统计量可以通过首先将模型拟合到完整数据集以获得普通残差 $e_i^{(k)} = y_i - \\hat{y}_i^{(k)}$ 和主成分得分 $z_{ij}$ 来计算。然后，使用推导出的公式将这些值组合起来。\n\n$\\text{PRESS}(k)$ 的最终表达式为：\n$$ \\text{PRESS}(k) = \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i^{(k)}}{1 - h_{ii}^{(k)}} \\right)^2 = \\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^2 $$\n这个公式允许在对完整数据集执行一次 PCA 和回归后，高效地计算任何 $k$ 值的 PRESS 统计量。", "answer": "$$\\boxed{\\sum_{i=1}^{n} \\left( \\frac{e_i^{(k)}}{1-h_{ii}^{(k)}} \\right)^{2}}$$", "id": "1951651"}]}