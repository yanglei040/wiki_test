{"hands_on_practices": [{"introduction": "经验风险最小化（ERM）的一个核心挑战是过拟合，即模型为了完美匹配训练数据而变得过于复杂。这个练习通过一个简单的多项式拟合问题，直观地展示了ERM如何导致一个振荡剧烈的函数。我们将通过引入一个惩罚模型“曲折度”的结构风险最小化（SRM）方法，学习如何选择一个更平滑、更可能具有良好泛化能力的模型。[@problem_id:3118270]", "problem": "考虑一个由$4$个有序输入-输出对组成的数据集 $D = \\{(x_i,y_i)\\}_{i=1}^{4}$，其中 $x_1=0$, $x_2=1$, $x_3=2$, $x_4=3$ 且 $y_1=0$, $y_2=3$, $y_3=0$, $y_4=3$。设假设类 $\\mathcal{H}_d$ 是所有次数最多为 $d$ 的多项式 $f:\\mathbb{R}\\to\\mathbb{R}$。对于一个样本 $(x,y)$ 的损失是平方损失 $\\ell(f;(x,y)) = (y - f(x))^2$。经验风险是样本均方损失\n$$\nR_{\\text{emp}}(f;D) \\;=\\; \\frac{1}{4}\\sum_{i=1}^{4} \\bigl(y_i - f(x_i)\\bigr)^2.\n$$\n定义两个学习原则：\n- 经验风险最小化 (ERM)：选择任意 $\\hat{f}_{\\text{ERM},d} \\in \\arg\\min_{f\\in \\mathcal{H}_d} R_{\\text{emp}}(f;D)$。\n- 结构风险最小化 (SRM)，带一个对（离散化的）一阶导数施加的全变分惩罚项：对于一个函数 $f$，在内部索引 $i$ 处的离散二阶差分定义为 $\\Delta^2 f(x_i) := f(x_{i+1}) - 2 f(x_i) + f(x_{i-1})$ （使用自然排序 $x_1  x_2  x_3  x_4$）。全变分惩罚项为\n$$\n\\mathrm{TV}(f;D) \\;=\\; \\sum_{i=2}^{3} \\left|\\, \\Delta^2 f(x_i) \\,\\right|.\n$$\n给定一个正则化权重 $\\lambda > 0$，SRM 目标函数为\n$$\nJ_{\\lambda}(f;D) \\;=\\; R_{\\text{emp}}(f;D) \\;+\\; \\lambda\\, \\mathrm{TV}(f;D).\n$$\n\n任务：\n1. 仅使用上述定义，论证为什么在 $\\mathcal{H}_3$ 上的 ERM 可以在数据集 $D$ 上达到0的经验风险，但却对应于一个在样本点上振荡的拟合。\n2. 计算唯一的最小二乘线性预测器 $\\hat{f}_{\\text{ERM},1}(x) = a x + b$，它在 $\\mathcal{H}_1$ 上最小化 $R_{\\text{emp}}(f;D)$，并评估其经验风险 $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D)$ 及其惩罚项 $\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D)$。\n3. 考虑 $\\mathcal{H}_3$ 中的三次插值多项式 $\\hat{f}_{\\text{ERM},3}$，它通过满足对所有 $i\\in\\{1,2,3,4\\}$ 都有 $\\hat{f}_{\\text{ERM},3}(x_i)=y_i$ 来达到零经验风险。评估其经验风险 $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D)$ 及其惩罚项 $\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D)$。\n4. 设 $\\lambda^{\\star}$ 是这样一个 $\\lambda$ 的临界值，在该值下 SRM 目标函数对于线性最小二乘预测器 $\\hat{f}_{\\text{ERM},1}$ 和三次插值多项式 $\\hat{f}_{\\text{ERM},3}$ 无差异，即 $J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},1};D) = J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},3};D)$。精确计算 $\\lambda^{\\star}$，并将你的最终答案表示为单个最简分数。\n\n你的最终答案必须是 $\\lambda^{\\star}$ 的唯一精确值，以分数形式表示。不要四舍五入。", "solution": "问题有效。我们按顺序解决每个任务来进行解答。\n\n数据集为 $D = \\{(x_i, y_i)\\}_{i=1}^{4}$，其中的点是 $(0,0)$、$(1,3)$、$(2,0)$ 和 $(3,3)$。假设类 $\\mathcal{H}_d$ 是次数最多为 $d$ 的多项式。经验风险是 $R_{\\text{emp}}(f;D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - f(x_i))^2$。惩罚项是 $\\mathrm{TV}(f;D) = \\sum_{i=2}^{3} |\\Delta^2 f(x_i)|$，其中 $\\Delta^2 f(x_i) = f(x_{i+1}) - 2 f(x_i) + f(x_{i-1})$。\n\n任务 1：关于在 $\\mathcal{H}_3$ 上进行 ERM 的论证\n一个在 $\\mathcal{H}_3$ 中的多项式具有形式 $f(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$，它有4个自由参数（系数 $c_0, c_1, c_2, c_3$）。为了找到一个最小化经验风险的函数 $f \\in \\mathcal{H}_3$，我们试图使平方误差 $(y_i - f(x_i))^2$ 尽可能小。如果我们能找到一个插值数据的函数，即对所有 $i \\in \\{1, 2, 3, 4\\}$ 都有 $f(x_i) = y_i$，那么每个平方误差项都将为零。这将导致经验风险 $R_{\\text{emp}}(f;D) = 0$。由于风险是非负项的和，0是可能的最小值。\n\n我们4个数据点的插值条件 $f(x_i) = y_i$ 构成了一个包含4个未知系数的4元线性方程组。该系统的矩阵是基于不同输入点 $x_1=0, x_2=1, x_3=2, x_4=3$ 的范德蒙矩阵 (Vandermonde matrix)。由于这些点是不同的，该矩阵是可逆的，这保证了存在一个次数最多为3的唯一多项式穿过所有四个数据点。设这个多项式为 $\\hat{f}_{\\text{ERM},3}$。对于这个多项式，$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - y_i)^2 = 0$。因此，在 $\\mathcal{H}_3$ 上的 ERM 可以达到零经验风险。\n\ny值的序列是 $0, 3, 0, 3$。一个插值这些点的函数必须从 $x=0$ 处的 $y=0$ 上升到 $x=1$ 处的 $y=3$，然后回落到 $x=2$ 处的 $y=0$，再上升到 $x=3$ 处的 $y=3$。这种上升-下降-上升的行为是振荡函数的特征。这样的函数在区间 $(0,3)$ 内必须至少有两个转折点（极值点）。这与简单的单调函数形成对比，是过拟合的典型标志，即模型过于复杂，不仅捕捉了潜在趋势，还捕捉了样本中的噪声或随机波动。\n\n任务 2：线性预测器 $\\hat{f}_{\\text{ERM},1}$\n我们要找到最小化 $R_{\\text{emp}}(f;D)$ 的线性函数 $\\hat{f}_{\\text{ERM},1}(x) = ax+b$。这等同于最小化平方误差和 $S(a,b) = \\sum_{i=1}^{4} (y_i - (ax_i+b))^2$。我们通过将关于 $a$ 和 $b$ 的偏导数设为零来找到最小值。\n$$S(a,b) = (0 - (a \\cdot 0 + b))^2 + (3 - (a \\cdot 1 + b))^2 + (0 - (a \\cdot 2 + b))^2 + (3 - (a \\cdot 3 + b))^2$$\n正规方程为 $\\frac{\\partial S}{\\partial a} = 0$ 和 $\\frac{\\partial S}{\\partial b} = 0$。\n等等，这里有一个计算错误。我们重新计算。\n让我们重新评估偏导数的和：\n$\\frac{\\partial S}{\\partial b} = \\sum_{i=1}^4 2(y_i - ax_i - b)(-1) = 0$，可以简化为 $\\sum(y_i - ax_i - b) = 0$，或者 $(\\sum y_i) - a(\\sum x_i) - 4b = 0$。\n由于 $\\sum x_i = 0+1+2+3 = 6$ 和 $\\sum y_i = 0+3+0+3=6$，我们有 $6 - 6a - 4b = 0$，可以简化为 $3a+2b=3$。\n\n$\\frac{\\partial S}{\\partial a} = \\sum_{i=1}^4 2(y_i - ax_i - b)(-x_i) = 0$，可以简化为 $\\sum(y_i x_i - ax_i^2 - bx_i) = 0$，或者 $(\\sum x_i y_i) - a(\\sum x_i^2) - b(\\sum x_i) = 0$。\n$\\sum x_i = 6$。\n$\\sum x_i^2 = 0^2+1^2+2^2+3^2 = 0+1+4+9=14$。\n$\\sum x_i y_i = (0)(0) + (1)(3) + (2)(0) + (3)(3) = 0+3+0+9=12$。\n所以，$12 - 14a - 6b = 0$，可以简化为 $7a+3b=6$。\n\n我们解这个线性方程组：\n1) $3a+2b=3$\n2) $7a+3b=6$\n将(1)式乘以 $3$，(2)式乘以 $2$：\n$9a+6b=9$\n$14a+6b=12$\n用第二个方程减去第一个方程得到 $5a=3 \\implies a = \\frac{3}{5}$。\n代入(1)式：$3(\\frac{3}{5}) + 2b = 3 \\implies \\frac{9}{5} + 2b = 3 \\implies 2b = 3 - \\frac{9}{5} = \\frac{6}{5} \\implies b = \\frac{3}{5}$。\n所以，线性预测器是 $\\hat{f}_{\\text{ERM},1}(x) = \\frac{3}{5}x + \\frac{3}{5}$。\n\n现在我们评估其经验风险 $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D)$：\n$f(x_1=0) = \\frac{3}{5}$，误差 $e_1 = 0 - \\frac{3}{5} = -\\frac{3}{5}$。\n$f(x_2=1) = \\frac{6}{5}$，误差 $e_2 = 3 - \\frac{6}{5} = \\frac{9}{5}$。\n$f(x_3=2) = \\frac{9}{5}$，误差 $e_3 = 0 - \\frac{9}{5} = -\\frac{9}{5}$。\n$f(x_4=3) = \\frac{12}{5}$，误差 $e_4 = 3 - \\frac{12}{5} = \\frac{3}{5}$。\n平方误差的和是 $\\sum e_i^2 = (-\\frac{3}{5})^2 + (\\frac{9}{5})^2 + (-\\frac{9}{5})^2 + (\\frac{3}{5})^2 = \\frac{9}{25} + \\frac{81}{25} + \\frac{81}{25} + \\frac{9}{25} = \\frac{180}{25} = \\frac{36}{5}$。\n$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D) = \\frac{1}{4} \\sum e_i^2 = \\frac{1}{4} \\cdot \\frac{36}{5} = \\frac{9}{5}$。\n\n接下来，我们评估其惩罚项 $\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D)$：\n输入 $x_i$ 是等间距的。对于任何线性函数 $f(x)=ax+b$ 和等间距的点 $x_{i-1}, x_i, x_{i+1}$，离散二阶差分为零：\n$\\Delta^2 f(x_i) = f(x_{i+1}) - 2f(x_i) + f(x_{i-1}) = (a x_{i+1}+b) - 2(a x_i+b) + (a x_{i-1}+b) = a(x_{i+1}-2x_i+x_{i-1}) = a((x_i+h)-2x_i+(x_i-h)) = 0$，其中 $h$ 是间距。\n因此，$\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D) = |\\Delta^2 f(x_2)| + |\\Delta^2 f(x_3)| = |0| + |0| = 0$。\n\n任务 3：三次插值多项式 $\\hat{f}_{\\text{ERM},3}$\n根据定义，这个函数对数据进行插值，所以对所有 $i$ 都有 $\\hat{f}_{\\text{ERM},3}(x_i) = y_i$。\n它的经验风险是 $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - \\hat{f}_{\\text{ERM},3}(x_i))^2 = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - y_i)^2 = 0$。\n\n为了评估其惩罚项 $\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D)$，我们用 $y_i$ 的值代替 $f(x_i)$：\n$f(x_1)=y_1=0$, $f(x_2)=y_2=3$, $f(x_3)=y_3=0$, $f(x_4)=y_4=3$。\n$\\Delta^2 f(x_2) = f(x_3) - 2f(x_2) + f(x_1) = y_3 - 2y_2 + y_1 = 0 - 2(3) + 0 = -6$。\n$\\Delta^2 f(x_3) = f(x_4) - 2f(x_3) + f(x_2) = y_4 - 2y_3 + y_2 = 3 - 2(0) + 3 = 6$。\n$\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D) = |\\Delta^2 f(x_2)| + |\\Delta^2 f(x_3)| = |-6| + |6| = 6+6=12$。\n\n任务 4：临界值 $\\lambda^{\\star}$\nSRM 目标函数是 $J_{\\lambda}(f;D) = R_{\\text{emp}}(f;D) + \\lambda \\mathrm{TV}(f;D)$。\n我们正在寻找使得两个模型的目标函数相等的 $\\lambda^{\\star}$ 值：\n$J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},1};D) = J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},3};D)$。\n\n对于线性模型 $\\hat{f}_{\\text{ERM},1}$：\n$J_{\\lambda}(\\hat{f}_{\\text{ERM},1};D) = R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D) + \\lambda\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D) = \\frac{9}{5} + \\lambda \\cdot 0 = \\frac{9}{5}$。\n\n对于三次模型 $\\hat{f}_{\\text{ERM},3}$：\n$J_{\\lambda}(\\hat{f}_{\\text{ERM},3};D) = R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) + \\lambda\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D) = 0 + \\lambda \\cdot 12 = 12\\lambda$。\n\n在 $\\lambda=\\lambda^{\\star}$ 处令它们相等：\n$\\frac{9}{5} = 12\\lambda^{\\star}$\n解出 $\\lambda^{\\star}$：\n$\\lambda^{\\star} = \\frac{1}{12} \\cdot \\frac{9}{5} = \\frac{9}{60}$。\n化简分数：\n$\\lambda^{\\star} = \\frac{3 \\cdot 3}{3 \\cdot 20} = \\frac{3}{20}$。\n\n这个 $\\lambda^{\\star}$ 值代表了无差异点。对于 $\\lambda  \\lambda^{\\star}$，SRM 目标函数偏好更复杂的三次模型（因为 $12\\lambda  9/5$）。对于 $\\lambda > \\lambda^{\\star}$，它偏好更简单的线性模型（因为 $12\\lambda > 9/5$），对三次拟合的振荡性质的惩罚重于对其完美数据拟合的奖励。", "answer": "$$\\boxed{\\frac{3}{20}}$$", "id": "3118270"}, {"introduction": "模型的性能不仅取决于其复杂性，还取决于其对数据中异常值的鲁棒性。本练习探讨了在使用标准铰链损失（hinge loss）时，单个异常值如何“绑架”ERM决策，导致分类器性能下降。我们将实践一种SRM策略，通过最小化一个泛化上界来选择一个“有上限的”鲁棒损失函数，从而学会如何构建一个不受异常值过度影响的分类器。[@problem_id:3118281]", "problem": "考虑标签为 $y \\in \\{-1,+1\\}$ 的二元分类问题，以及一维特征 $x \\in \\mathbb{R}$ 上的线性评分函数 $f(x) = w x + b$。在标记点 $(x,y)$ 处的铰链损失定义为 $\\ell_{\\text{hinge}}(y,f(x)) = \\max\\{0, 1 - y f(x)\\}$。经验风险最小化 (ERM) 旨在最小化训练集上损失的经验均值。结构风险最小化 (SRM) 通过最小化一个高概率上界来选择一个假设（包括其相关的超参数），该上界考虑了其期望损失的经验拟合度和复杂度。\n\n给定以下六个标记点的数据集，其设计旨在让一个离群值在 使用铰链损失时使 ERM 陷入窘境：\n- $(x_{1}, y_{1}) = (1.0, +1)$,\n- $(x_{2}, y_{2}) = (1.2, +1)$,\n- $(x_{3}, y_{3}) = (0.8, +1)$,\n- $(x_{4}, y_{4}) = (-1.0, -1)$,\n- $(x_{5}, y_{5}) = (-1.2, -1)$,\n- $(x_{6}, y_{6}) = (20.0, -1)$。\n\n考虑两个候选线性评分函数：\n- $f_{A}(x) = x$,\n- $f_{B}(x) = 0.2 x + 0.1$。\n\n任务：\n1. 使用所提供的数据集，计算 $f_{A}$ 和 $f_{B}$ 的经验铰链风险，并确定 ERM（使用铰链损失）偏好哪个评分函数。简要说明为什么在这种情况下离群值会使 ERM 陷入窘境。\n2. 定义在水平 $\\tau > 0$ 处的截断（Huber化）铰链损失为 $\\ell_{\\tau}(y,f(x)) = \\min\\{\\ell_{\\text{hinge}}(y,f(x)), \\tau\\}$。对于 SRM，考虑由 $\\tau \\in \\{0.7, 0.9, 5.1, 21\\}$ 索引的结构，并为每个 $\\tau$ 选择 $\\{f_{A}, f_{B}\\}$ 中最小化 $\\ell_{\\tau}$ 经验均值的评分函数。\n3. 从经过充分检验的关于有界损失的集中不等式出发，推导期望截断铰链风险的一个高概率上界，该上界用 $\\ell_{\\tau}$ 的经验均值、以分母 $n$（样本大小）计算的 $\\ell_{\\tau}$ 的经验方差、$\\tau$ 和置信参数 $\\delta \\in (0,1)$ 来表示。然后，对于此数据集，设置 $\\delta = 3 \\exp(-3)$ 和 $n = 6$，并将该界简化为一个关于经验均值、经验标准差和 $\\tau$ 的显式闭式函数。\n4. 对于每个 $\\tau \\in \\{0.7, 0.9, 5.1, 21\\}$，为步骤2中经验最小化的评分函数计算得到的界，并选择使该界最小的 $\\tau$。\n\n将最优 $\\tau$ 报告为单个实数。将您的答案四舍五入到四位有效数字。", "solution": "首先根据指定标准对问题进行验证。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n- **领域**：二元分类，标签 $y \\in \\{-1,+1\\}$，一维特征 $x \\in \\mathbb{R}$。\n- **评分函数形式**：$f(x) = wx + b$。\n- **铰链损失**：$\\ell_{\\text{hinge}}(y,f(x)) = \\max\\{0, 1 - y f(x)\\}$。\n- **数据集 ($n=6$)**：\n    - $(x_{1}, y_{1}) = (1.0, +1)$\n    - $(x_{2}, y_{2}) = (1.2, +1)$\n    - $(x_{3}, y_{3}) = (0.8, +1)$\n    - $(x_{4}, y_{4}) = (-1.0, -1)$\n    - $(x_{5}, y_{5}) = (-1.2, -1)$\n    - $(x_{6}, y_{6}) = (20.0, -1)$\n- **候选评分函数**：\n    - $f_{A}(x) = x$\n    - $f_{B}(x) = 0.2 x + 0.1$\n- **截断铰链损失**：对于 $\\tau > 0$，$\\ell_{\\tau}(y,f(x)) = \\min\\{\\ell_{\\text{hinge}}(y,f(x)), \\tau\\}$。\n- **SRM结构**：由 $\\tau \\in \\{0.7, 0.9, 5.1, 21\\}$ 索引。\n- **任务1**：计算 $f_A$ 和 $f_B$ 的经验铰链风险，确定 ERM 的偏好，并说明离群值的作用。\n- **任务2**：对于给定集合中的每个 $\\tau$，使用 $\\ell_{\\tau}$ 在 $\\{f_A, f_B\\}$ 中找到经验最小化的评分函数。\n- **任务3**：推导期望截断铰链风险的一个高概率上界，然后对于 $n=6$ 和 $\\delta = 3 \\exp(-3)$ 将其简化。\n- **任务4**：使用该界从给定集合中选择最优的 $\\tau$。\n- **最终答案**：报告最优的 $\\tau$，四舍五入到四位有效数字。\n\n**步骤2：使用提取的已知条件进行验证**\n该问题在统计学习理论方面有坚实的科学基础，使用了 ERM、SRM、铰链损失和集中不等式等标准概念。问题是适定的，提供了执行所要求的计算并得出唯一答案所需的所有数据、函数和参数。语言客观而精确。该问题是一个自包含的、多步骤的练习，在计算上和概念上都是合理的。没有矛盾、歧义或违反科学原理的地方。\n\n**步骤3：结论与行动**\n该问题被判定为**有效**。将提供完整解答。\n\n### 解答\n\n解答过程将依次处理四个任务。\n\n**任务1：使用铰链损失的经验风险最小化 (ERM)**\n\n经验铰链风险是在大小为 $n=6$ 的数据集上的平均铰链损失：$R_{\\text{emp}}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_{\\text{hinge}}(y_i, f(x_i))$。我们为 $f_A$ 和 $f_B$ 计算这个值。\n\n对于 $f_A(x) = x$：\n- $(1.0, +1)$: $y f_A(x) = 1 \\cdot 1.0 = 1.0$。损失 $\\ell_1 = \\max\\{0, 1-1.0\\} = 0$。\n- $(1.2, +1)$: $y f_A(x) = 1 \\cdot 1.2 = 1.2$。损失 $\\ell_2 = \\max\\{0, 1-1.2\\} = 0$。\n- $(0.8, +1)$: $y f_A(x) = 1 \\cdot 0.8 = 0.8$。损失 $\\ell_3 = \\max\\{0, 1-0.8\\} = 0.2$。\n- $(-1.0, -1)$: $y f_A(x) = -1 \\cdot (-1.0) = 1.0$。损失 $\\ell_4 = \\max\\{0, 1-1.0\\} = 0$。\n- $(-1.2, -1)$: $y f_A(x) = -1 \\cdot (-1.2) = 1.2$。损失 $\\ell_5 = \\max\\{0, 1-1.2\\} = 0$。\n- $(20.0, -1)$: $y f_A(x) = -1 \\cdot 20.0 = -20.0$。损失 $\\ell_6 = \\max\\{0, 1-(-20.0)\\} = 21.0$。\n$f_A$ 的总铰链损失为 $0+0+0.2+0+0+21.0 = 21.2$。\n经验风险为 $R_{\\text{emp}}(f_A) = \\frac{21.2}{6} = \\frac{106}{30} = \\frac{53}{15} \\approx 3.5333$。\n\n对于 $f_B(x) = 0.2x + 0.1$：\n- $(1.0, +1)$: $y f_B(x) = 1 \\cdot (0.2 \\cdot 1.0 + 0.1) = 0.3$。损失 $\\ell_1 = \\max\\{0, 1-0.3\\} = 0.7$。\n- $(1.2, +1)$: $y f_B(x) = 1 \\cdot (0.2 \\cdot 1.2 + 0.1) = 0.34$。损失 $\\ell_2 = \\max\\{0, 1-0.34\\} = 0.66$。\n- $(0.8, +1)$: $y f_B(x) = 1 \\cdot (0.2 \\cdot 0.8 + 0.1) = 0.26$。损失 $\\ell_3 = \\max\\{0, 1-0.26\\} = 0.74$。\n- $(-1.0, -1)$: $y f_B(x) = -1 \\cdot (0.2 \\cdot (-1.0) + 0.1) = 0.1$。损失 $\\ell_4 = \\max\\{0, 1-0.1\\} = 0.9$。\n- $(-1.2, -1)$: $y f_B(x) = -1 \\cdot (0.2 \\cdot (-1.2) + 0.1) = 0.14$。损失 $\\ell_5 = \\max\\{0, 1-0.14\\} = 0.86$。\n- $(20.0, -1)$: $y f_B(x) = -1 \\cdot (0.2 \\cdot 20.0 + 0.1) = -4.1$。损失 $\\ell_6 = \\max\\{0, 1-(-4.1)\\} = 5.1$。\n$f_B$ 的总铰链损失为 $0.7+0.66+0.74+0.9+0.86+5.1 = 8.96$。\n经验风险为 $R_{\\text{emp}}(f_B) = \\frac{8.96}{6} = \\frac{224}{150} = \\frac{112}{75} \\approx 1.4933$。\n\n由于 $R_{\\text{emp}}(f_B) \\approx 1.4933  R_{\\text{emp}}(f_A) \\approx 3.5333$，ERM 偏好评分函数 $f_B(x)$。\n\n离群值 $(20.0, -1)$ 之所以使 ERM “陷入窘境”，是因为铰链损失是无界的。对于 $f_A(x) = x$，该点产生的间隔为 $y_6 f_A(x_6) = -20$，导致一个非常大的损失 $21$。这一个点贡献了 $f_A$ 总经验风险的 $21.0/21.2 \\approx 99\\%$。为了最小化总风险，ERM 被迫选择 $f_B$，尽管 $f_B$ 在五个“干净”的数据点上表现更差（它没有误分类任何点，但在所有点上的间隔都更小，导致所有点都有非零损失），但它在离群值上产生的损失要小得多，为 $5.1$。铰链损失的无界性使得 ERM 准则对具有大分值的离群值高度敏感。\n\n**任务2：使用截断铰链损失的经验风险**\n\n使用截断铰链损失 $\\ell_{\\tau}$ 的经验风险是 $R_{\\text{emp},\\tau}(f) = \\frac{1}{n} \\sum_{i=1}^{n} \\min\\{\\ell_{\\text{hinge}}(y_i, f(x_i)), \\tau\\}$。我们使用任务1中计算的铰链损失。\n$f_A$ 的各点铰链损失：$\\{0, 0, 0.2, 0, 0, 21.0\\}$。\n$f_B$ 的各点铰链损失：$\\{0.7, 0.66, 0.74, 0.9, 0.86, 5.1\\}$。\n\n对于 $\\tau=0.7$：\n- $R_{\\text{emp},0.7}(f_A) = \\frac{1}{6}(\\min(0,0.7) + \\min(0,0.7) + \\min(0.2,0.7) + \\min(0,0.7) + \\min(0,0.7) + \\min(21.0,0.7)) = \\frac{0+0+0.2+0+0+0.7}{6} = \\frac{0.9}{6} = 0.15$。\n- $R_{\\text{emp},0.7}(f_B) = \\frac{1}{6}(\\min(0.7,0.7) + \\min(0.66,0.7) + \\min(0.74,0.7) + \\min(0.9,0.7) + \\min(0.86,0.7) + \\min(5.1,0.7)) = \\frac{0.7+0.66+0.7+0.7+0.7+0.7}{6} = \\frac{4.16}{6} \\approx 0.6933$。\n- 偏好：$f_A$。\n\n对于 $\\tau=0.9$：\n- $R_{\\text{emp},0.9}(f_A) = \\frac{1}{6}(0+0+0.2+0+0+\\min(21.0,0.9)) = \\frac{1.1}{6} \\approx 0.1833$。\n- $R_{\\text{emp},0.9}(f_B) = \\frac{1}{6}(0.7+0.66+0.74+\\min(0.9,0.9)+\\min(0.86,0.9)+\\min(5.1,0.9)) = \\frac{0.7+0.66+0.74+0.9+0.86+0.9}{6} = \\frac{4.76}{6} \\approx 0.7933$。\n- 偏好：$f_A$。\n\n对于 $\\tau=5.1$：\n- $R_{\\text{emp},5.1}(f_A) = \\frac{1}{6}(0+0+0.2+0+0+\\min(21.0,5.1)) = \\frac{5.3}{6} \\approx 0.8833$。\n- $R_{\\text{emp},5.1}(f_B) = \\frac{1}{6}(0.7+0.66+0.74+0.9+0.86+\\min(5.1,5.1)) = \\frac{8.96}{6} \\approx 1.4933$。\n- 偏好：$f_A$。\n\n对于 $\\tau=21$：\n- $R_{\\text{emp},21}(f_A) = \\frac{1}{6}(0+0+0.2+0+0+\\min(21.0,21)) = \\frac{21.2}{6} \\approx 3.5333$。\n- $R_{\\text{emp},21}(f_B) = \\frac{1}{6}(0.7+0.66+0.74+0.9+0.86+5.1) = \\frac{8.96}{6} \\approx 1.4933$。\n- 偏好：$f_B$。\n\n**任务2总结：**\n- 对于 $\\tau \\in \\{0.7, 0.9, 5.1\\}$，经验最小化评分函数是 $f_A$。\n- 对于 $\\tau = 21$，经验最小化评分函数是 $f_B$。\n\n**任务3：风险界的推导**\n\n我们需要推导期望风险 $R_{\\tau}(f) = \\mathbb{E}[\\ell_{\\tau}]$ 的一个上界，该上界用其经验均值 $\\hat{R}_{\\tau}$、经验方差 $\\hat{\\sigma}_{n,\\tau}^2$、损失界 $\\tau$、样本大小 $n$ 和置信参数 $\\delta$ 表示。截断损失 $\\ell_{\\tau}$ 在 $[0, \\tau]$ 范围内有界。问题要求一个基于经验方差的界，这指向一种经验Bernstein型不等式。我们使用 Maurer 和 Pontil 的一个标准结果。对于一组在 $[0, \\tau]$ 内有界的独立同分布随机变量 $L_i$，其经验均值为 $\\hat{R} = \\frac{1}{n}\\sum L_i$，经验方差（分母为 $n$）为 $\\hat{\\sigma}_n^2 = \\frac{1}{n}\\sum (L_i - \\hat{R})^2$，以下不等式至少以 $1-\\delta$ 的概率成立：\n$$R \\le \\hat{R} + \\sqrt{\\frac{2\\hat{\\sigma}_n^2 \\ln(3/\\delta)}{n}} + \\frac{3\\tau\\ln(3/\\delta)}{n}$$\n给定 $n=6$ 和 $\\delta = 3 \\exp(-3)$。首先，我们计算 $\\ln(3/\\delta)$ 项：\n$$\\ln\\left(\\frac{3}{\\delta}\\right) = \\ln\\left(\\frac{3}{3\\exp(-3)}\\right) = \\ln(\\exp(3)) = 3$$\n将 $n=6$ 和 $\\ln(3/\\delta)=3$ 代入不等式：\n$$R \\le \\hat{R} + \\sqrt{\\frac{2\\hat{\\sigma}_n^2 (3)}{6}} + \\frac{3\\tau(3)}{6}$$\n$$R \\le \\hat{R} + \\sqrt{\\hat{\\sigma}_n^2} + \\frac{9\\tau}{6}$$\n简化后得到作为经验均值 $\\hat{R}_{\\tau}$、经验标准差 $\\hat{\\sigma}_{n,\\tau}$ 和 $\\tau$ 的函数的最终闭式界：\n$$\\text{Bound} = \\hat{R}_{\\tau} + \\hat{\\sigma}_{n,\\tau} + 1.5\\tau$$\n\n**任务4：计算界并选择最优 $\\tau$**\n\n我们现在为每个 $\\tau$ 值计算这个界，使用任务2中相应的经验最小化函数。我们使用 $\\hat{R}_{\\tau}$ 和 $\\hat{\\sigma}_{n,\\tau}$ 表示截断损失的经验统计量。经验标准差计算为 $\\hat{\\sigma}_{n,\\tau} = \\sqrt{\\frac{1}{n}\\sum L_i^2 - \\hat{R}_{\\tau}^2}$。\n\n对于 $\\tau=0.7$ (使用 $f_A$)：\n- 损失 $L_i$: $\\{0, 0, 0.2, 0, 0, 0.7\\}$。\n- $\\hat{R}_{0.7} = \\frac{0.9}{6} = 0.15$。\n- $\\sum L_i^2 = 0^2+0^2+0.2^2+0^2+0^2+0.7^2 = 0.04+0.49=0.53$。\n- $\\hat{\\sigma}_{n,0.7} = \\sqrt{\\frac{0.53}{6} - (0.15)^2} = \\sqrt{0.088333... - 0.0225} = \\sqrt{0.065833...} \\approx 0.25658$。\n- Bound$_{0.7} = 0.15 + 0.25658 + 1.5 \\times 0.7 = 0.15 + 0.25658 + 1.05 = 1.45658$。\n\n对于 $\\tau=0.9$ (使用 $f_A$)：\n- 损失 $L_i$: $\\{0, 0, 0.2, 0, 0, 0.9\\}$。\n- $\\hat{R}_{0.9} = \\frac{1.1}{6} \\approx 0.18333$。\n- $\\sum L_i^2 = 0.2^2+0.9^2 = 0.04+0.81=0.85$。\n- $\\hat{\\sigma}_{n,0.9} = \\sqrt{\\frac{0.85}{6} - (\\frac{1.1}{6})^2} = \\sqrt{0.141666... - 0.033611...} = \\sqrt{0.108055...} \\approx 0.32872$。\n- Bound$_{0.9} = \\frac{1.1}{6} + 0.32872 + 1.5 \\times 0.9 \\approx 0.18333 + 0.32872 + 1.35 = 1.86205$。\n\n对于 $\\tau=5.1$ (使用 $f_A$)：\n- 损失 $L_i$: $\\{0, 0, 0.2, 0, 0, 5.1\\}$。\n- $\\hat{R}_{5.1} = \\frac{5.3}{6} \\approx 0.88333$。\n- $\\sum L_i^2 = 0.2^2+5.1^2 = 0.04+26.01=26.05$。\n- $\\hat{\\sigma}_{n,5.1} = \\sqrt{\\frac{26.05}{6} - (\\frac{5.3}{6})^2} = \\sqrt{4.341666... - 0.779722...} = \\sqrt{3.561944...} \\approx 1.88731$。\n- Bound$_{5.1} = \\frac{5.3}{6} + 1.88731 + 1.5 \\times 5.1 \\approx 0.88333 + 1.88731 + 7.65 = 10.42064$。\n\n对于 $\\tau=21$ (使用 $f_B$)：\n- 损失 $L_i$: $\\{0.7, 0.66, 0.74, 0.9, 0.86, 5.1\\}$。\n- $\\hat{R}_{21} = \\frac{8.96}{6} \\approx 1.49333$。\n- $\\sum L_i^2 = 0.7^2+0.66^2+0.74^2+0.9^2+0.86^2+5.1^2 = 0.49+0.4356+0.5476+0.81+0.7396+26.01 = 29.0328$。\n- $\\hat{\\sigma}_{n,21} = \\sqrt{\\frac{29.0328}{6} - (\\frac{8.96}{6})^2} = \\sqrt{4.8388 - 2.230044...} = \\sqrt{2.608755...} \\approx 1.61516$。\n- Bound$_{21} = \\frac{8.96}{6} + 1.61516 + 1.5 \\times 21 \\approx 1.49333 + 1.61516 + 31.5 = 34.60849$。\n\n比较计算出的界限：\n- $\\tau=0.7$ 的界: $\\approx 1.4566$\n- $\\tau=0.9$ 的界: $\\approx 1.8621$\n- $\\tau=5.1$ 的界: $\\approx 10.4206$\n- $\\tau=21$ 的界: $\\approx 34.6085$\n\n在 $\\tau=0.7$ 处达到最小界限。这个 $\\tau$ 值在经验风险项（$\\hat{R}_{\\tau}$）、复杂度/方差项（$\\hat{\\sigma}_{n,\\tau}$）和来自界结构的正则化项（$1.5\\tau$）之间提供了最佳的权衡。\n\n给定集合中的最优 $\\tau$ 值为 $0.7$。四舍五入到四位有效数字，即为 $0.7000$。", "answer": "$$\\boxed{0.7000}$$", "id": "3118281"}, {"introduction": "“模型复杂度”这个概念如何被精确量化？这个练习将我们从直观的惩罚项带入统计学习理论的核心，引入经验Rademacher复杂度的概念——一个衡量函数类别拟合随机噪声能力的强大工具。你将通过编程实践，学习如何使用这个度量来实施SRM，从而在不同阶数的多项式中进行模型选择，并探索特征缩放等实际因素如何影响模型复杂度的评估。[@problem_id:3118242]", "problem": "考虑一维多项式回归，其假设类别按多项式次数索引。对于每个整数次数 $k \\in \\{0,1,2,3,4\\}$，定义假设类别 $\\mathcal{H}_k$，其由函数 $f_{\\boldsymbol{w}}(x) = \\sum_{j=0}^{k} w_j x^j$ 组成，其中系数向量 $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ 满足欧几里得范数约束 $\\|\\boldsymbol{w}\\|_2 \\leq B$。使用以下基础：经验风险最小化 (ERM) 的定义、结构风险最小化 (SRM) 的定义以及经验 Rademacher 复杂度的定义。对于每个 $k$，ERM 选择一个系数向量 $\\hat{\\boldsymbol{w}}_k$ 以最小化平方损失下的经验风险。SRM 通过最小化经验风险与一个依赖于数据的复杂度项之间的权衡来选择次数 $k$。\n\n按如下方式确定性地构建数据集。设样本大小为 $n = 20$。设输入 $x_i$ 在区间 $[-1,1]$ 内均匀分布，即 $x_i = -1 + \\frac{2(i-1)}{n-1}$，对于 $i = 1,2,\\dots,n$。设输出由一个带噪声的二次模型生成：$y_i = 1 - 2 x_i + 0.5 x_i^2 + \\epsilon_i$，其中 $\\epsilon_i$ 是独立的高斯噪声变量，满足 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 且 $\\sigma = 0.1$。使用固定的随机种子 $123$ 生成 $(\\epsilon_i)_{i=1}^n$ 以确保可复现性。\n\n对于任何缩放因子 $s > 0$，定义缩放后的输入 $x_i^{(s)} = s \\cdot x_i$。对于每个次数 $k$，定义特征映射 $\\boldsymbol{\\phi}_k(x) = (x^0, x^1, \\dots, x^k)^\\top \\in \\mathbb{R}^{k+1}$ 和相应的设计矩阵 $\\Phi_k^{(s)} \\in \\mathbb{R}^{n \\times (k+1)}$，其第 $i$ 行为 $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top$。\n\n对于每个 $k$ 和缩放因子 $s$，计算：\n- 在 ERM 解处平方损失下的经验风险，\n  $$\\hat{R}_k^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat{\\boldsymbol{w}}_k^{(s)\\top} \\boldsymbol{\\phi}_k(x_i^{(s)})\\right)^2,$$\n  其中 $\\hat{\\boldsymbol{w}}_k^{(s)}$ 是经验风险在 $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ 上的任意一个最小化器，并且在多个最小化器之间可以任意选择。\n- 在缩放数据集 $(x_i^{(s)})_{i=1}^n$ 上评估的类别 $\\mathcal{H}_k$ 的经验 Rademacher 复杂度，\n  $$\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right],$$\n  其中 $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots,\\sigma_n)$ 具有独立的 Rademacher 项，满足 $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = \\frac{1}{2}$，且 $B = 1$。通过对 $\\boldsymbol{\\sigma}$ 的 $M = 2000$ 次独立抽样进行蒙特卡洛平均来近似此期望，使用固定的随机种子 $999$ 以确保可复现性。\n\n定义在缩放 $s$ 和权衡参数 $c > 0$ 下对次数 $k$ 的 SRM 选择准则为\n$$\\mathrm{SRM}(k; s,c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s).$$\n选择次数\n$$k^*(s,c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s,c),$$\n若出现平局，则选择较小的 $k$。\n\n使用计算出的经验 Rademacher 复杂度值，讨论特征缩放 $x \\mapsto s x$ 如何通过特征向量 $\\boldsymbol{\\phi}_k(x_i^{(s)})$ 的大小以及出现在上确界中的范数来影响 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$，以及这反过来又如何影响 SRM 选择的次数 $k^*(s,c)$。\n\n实现一个完整、可运行的程序，该程序：\n- 按规定构建数据集 $(x_i,y_i)_{i=1}^n$，其中 $n = 20$，$x_i \\in [-1,1]$，$\\sigma = 0.1$，噪声的随机种子为 $123$。\n- 对于每个次数 $k \\in \\{0,1,2,3,4\\}$、测试套件中的每个缩放 $s$ 和每个 $c$，计算 $\\hat{R}_k^{(s)}$，使用随机种子 $999$ 通过 $M = 2000$ 次蒙特卡洛样本近似 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$，然后计算 $\\mathrm{SRM}(k; s,c)$ 和 $k^*(s,c)$。\n- 生成单行输出，其中包含所有测试用例的选择次数 $k^*(s,c)$，形式为方括号括起来的逗号分隔列表。\n\n使用以下测试套件，该套件探讨了不同方面：\n- 情况 1：$s = 1.0$, $c = 0.05$ (基准缩放和中等复杂度惩罚；理想路径)。\n- 情况 2：$s = 0.5$, $c = 0.05$ (减小的特征幅度；复杂度降低)。\n- 情况 3：$s = 2.0$, $c = 0.05$ (增大的特征幅度；复杂度增加)。\n- 情况 4：$s = 1.0$, $c = 0.5$ (强复杂度惩罚；将边界推向较低次数)。\n- 情况 5：$s = 0.1$, $c = 0.05$ (非常小的缩放；复杂度显著降低)。\n- 情况 6：$s = 1.0$, $c = 0.0$ (无复杂度惩罚的纯 ERM；边缘情况)。\n\n您的程序应生成单行输出，其中包含结果，形式为方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5,r_6]$，其中每个 $r_i$ 是一个整数次数）。", "solution": "### 问题验证\n\n第一步是严格验证问题陈述。\n\n**步骤1：提取给定条件**\n- **假设类别**：对于 $k \\in \\{0,1,2,3,4\\}$，$\\mathcal{H}_k = \\{f_{\\boldsymbol{w}}(x) = \\sum_{j=0}^{k} w_j x^j : \\boldsymbol{w} \\in \\mathbb{R}^{k+1}, \\|\\boldsymbol{w}\\|_2 \\leq B\\}$。\n- **数据集**：样本大小 $n = 20$。输入 $x_i = -1 + \\frac{2(i-1)}{n-1}$ 对于 $i=1,\\dots,n$。输出 $y_i = 1 - 2 x_i + 0.5 x_i^2 + \\epsilon_i$，其中 $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$，$\\sigma = 0.1$。噪声生成使用随机种子 $123$。\n- **特征缩放**：对于 $s > 0$，缩放后的输入为 $x_i^{(s)} = s \\cdot x_i$。\n- **设计矩阵**：$\\Phi_k^{(s)} \\in \\mathbb{R}^{n \\times (k+1)}$，其第 $i$ 行为 $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top = ((x_i^{(s)})^0, \\dots, (x_i^{(s)})^k)$。\n- **经验风险 (ERM)**：$\\hat{R}_k^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\boldsymbol{w}}_k^{(s)\\top} \\boldsymbol{\\phi}_k(x_i^{(s)}))^2$，其中 $\\hat{\\boldsymbol{w}}_k^{(s)}$ 在 $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ 上最小化此风险。\n- **经验 Rademacher 复杂度**：$\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} [ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) ]$，其中 $B=1$ 且 $\\sigma_i$ 为 Rademacher 变量。该期望通过使用随机种子 $999$ 的 $M=2000$ 个样本的蒙特卡洛平均来近似。\n- **结构风险最小化 (SRM)**：选择准则为 $\\mathrm{SRM}(k; s,c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$。\n- **模型选择**：$k^*(s,c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s,c)$，平局时选择较小的 $k$。\n- **任务**：为给定的 $(s, c)$ 对测试套件计算 $k^*(s,c)$。\n- **测试套件**：\n    1. $s=1.0, c=0.05$\n    2. $s=0.5, c=0.05$\n    3. $s=2.0, c=0.05$\n    4. $s=1.0, c=0.5$\n    5. $s=0.1, c=0.05$\n    6. $s=1.0, c=0.0$\n\n**步骤2：使用提取的给定条件进行验证**\n- **科学依据**：该问题牢固地植根于统计学习理论。经验风险最小化、结构风险最小化和 Rademacher 复杂度是基本概念。提供的公式是标准定义。多项式回归模拟的设置是用于说明这些概念的一个典型例子。该问题在科学上是合理的。\n- **适定性**：该问题是适定的。数据集是确定性地指定的（除了一个带种子的随机过程）。ERM 和 SRM 的目标函数被明确定义。平方损失的 ERM 是一个标准的无约束最小二乘问题，只要设计矩阵具有满列秩（对于不同点上的多项式特征，这是成立的），该问题就有唯一解。Rademacher 复杂度定义中的上确界有一个众所周知的闭式解。平局决胜规则确保了唯一答案。\n- **客观性**：问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n- **完整性和一致性**：所有必要的参数（$n, k, \\sigma, B, M$，随机种子，测试用例）都已提供。没有矛盾之处。ERM 被明确定义为在 $\\mathbb{R}^{k+1}$ 上的无约束优化，而用于复杂度计算的假设类别 $\\mathcal{H}_k$ 则受 $\\|\\boldsymbol{w}\\|_2 \\leq B$ 约束。这是学习理论中一个标准且一致的范式，其中类别的复杂度与用于最小化的具体算法分开分析。\n\n**步骤3：结论与行动**\n问题有效。我们可以继续进行求解。\n\n### 解法推导\n\n该解法要求实现结构风险最小化 (SRM) 原则，以为给定数据集选择最佳多项式次数 $k$。这涉及平衡经验风险（模型拟合训练数据的好坏程度）和从假设类别的经验 Rademacher 复杂度派生出的复杂度惩罚项。我们必须对由特征缩放因子 $s$ 和复杂度权衡参数 $c$ 定义的几种场景执行此操作。\n\n**1. 数据集生成**\n首先，我们按规定构建数据集 $(x_i, y_i)_{i=1}^n$。样本大小为 $n=20$。输入 $x_i$ 是区间 $[-1, 1]$ 内的 $n$ 个均匀间隔点。真实模型是一个二次函数 $f^*(x) = 1 - 2x + 0.5x^2$ 并被高斯噪声所干扰。\n输出为 $y_i = f^*(x_i) + \\epsilon_i$，其中 $\\epsilon_i$ 从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取，且 $\\sigma=0.1$。使用固定的随机种子 ($123$) 确保噪声向量 $(\\epsilon_1, \\dots, \\epsilon_n)$ 是可复现的。\n\n**2. 经验风险最小化 (ERM)**\n对于每个次数 $k \\in \\{0, 1, 2, 3, 4\\}$ 和缩放因子 $s$，我们需要找到最小化经验风险的模型。经验风险是训练数据上的均方误差：\n$$ \\hat{R}(\\boldsymbol{w}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)})\\right)^2 = \\frac{1}{n} \\|\\boldsymbol{y} - \\Phi_k^{(s)} \\boldsymbol{w}\\|_2^2 $$\n问题要求找到在所有 $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ 上的最小化器 $\\hat{\\boldsymbol{w}}_k^{(s)}$。这是一个标准的无约束线性最小二乘问题。解 $\\hat{\\boldsymbol{w}}_k^{(s)}$ 由正规方程组给出：\n$$ \\hat{\\boldsymbol{w}}_k^{(s)} = (\\Phi_k^{(s)\\top} \\Phi_k^{(s)})^{-1} \\Phi_k^{(s)\\top} \\boldsymbol{y} $$\n其中 $\\Phi_k^{(s)}$ 是设计矩阵，其第 $i$ 行为 $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top = (1, sx_i, (sx_i)^2, \\dots, (sx_i)^k)$。在数值上，最好使用 QR 分解或 SVD 等方法来求解，正如在 `numpy.linalg.lstsq` 中实现的那样。\n在找到 $\\hat{\\boldsymbol{w}}_k^{(s)}$ 之后，我们计算最小化的经验风险 $\\hat{R}_k^{(s)} = \\frac{1}{n} \\|\\boldsymbol{y} - \\Phi_k^{(s)} \\hat{\\boldsymbol{w}}_k^{(s)}\\|_2^2$。\n\n**3. 经验 Rademacher 复杂度**\n经验 Rademacher 复杂度衡量一个函数类别拟合随机噪声的能力。对于类别 $\\mathcal{H}_k$，它是在特定的（缩放后的）数据点 $x_i^{(s)}$ 上定义的：\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right] $$\n我们可以简化期望内的项。该和式可以用线性代数重写：\n$$ \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) = \\boldsymbol{w}^\\top \\left( \\sum_{i=1}^{n} \\sigma_i \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right) = \\boldsymbol{w}^\\top (\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma})$$\n其中 $\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_n)^\\top$。根据柯西-施瓦茨不等式，$\\boldsymbol{w}^\\top \\boldsymbol{v}$ 在球 $\\|\\boldsymbol{w}\\|_2 \\leq B$ 上的上确界是 $B\\|\\boldsymbol{v}\\|_2$。因此，上确界是：\n$$ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\boldsymbol{w}^\\top (\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}) = B \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2 $$\n将其代回，我们得到：\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\frac{B}{n} \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2 \\right] $$\n这个期望难以精确计算。我们通过对 Rademacher 向量 $\\boldsymbol{\\sigma}^{(j)}$ 的 $M=2000$ 次独立样本进行蒙特卡洛平均来近似它：\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) \\approx \\frac{1}{M} \\sum_{j=1}^{M} \\frac{B}{n} \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}^{(j)}\\|_2 $$\n其中 $B=1$，$n=20$，并使用固定的随机种子 ($999$) 以确保 $\\boldsymbol{\\sigma}^{(j)}$ 样本的可复现性。\n\n**4. 结构风险最小化与模型选择**\n在为每个次数 k 计算出经验风险 $\\hat{R}_k^{(s)}$ 和复杂度 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$ 之后，我们评估 SRM 准则：\n$$ \\mathrm{SRM}(k; s, c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) $$\n该准则在模型的拟合优度（低 $\\hat{R}_k^{(s)}$）与其复杂度（低 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$）之间取得平衡。参数 $c$ 控制对复杂度的惩罚强度。最后一步是为给定的对 $(s, c)$ 选择使该准则最小化的次数 $k^*$：\n$$ k^*(s, c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s, c) $$\n平局通过选择较小的次数 $k$ 来解决。\n\n**5. 特征缩放分析**\n缩放因子 $s$ 直接影响特征向量的大小，并因此影响 Rademacher 复杂度。缩放后的特征向量是 $\\boldsymbol{\\phi}_k(sx) = (1, sx, (sx)^2, \\dots, (sx)^k)^\\top$。与未缩放的向量 $\\boldsymbol{\\phi}_k(x)$ 相比，第 $j$ 个分量被乘以 $s^j$。这意味着涉及 $\\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2$ 的 Rademacher 复杂度项会受到影响。\n- 如果 $s > 1$，对应于较高次幂的特征分量会被放大。这会增加范数 $\\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2$，导致一个更大的 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$，特别是对于较大的 $k$。一个更大的复杂度项会起到更强的惩罚作用，从而偏好更简单的模型（较小的 $k^*$）。\n- 如果 $s  1$，特征分量会被衰减。这会减小范数，导致一个更小的 $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$。降低的复杂度惩罚使得使用更复杂的模型变得“更便宜”，如果经验风险 $\\hat{R}_k^{(s)}$ 的相应减少足够大，这可能导致选择一个更高的 $k^*$。\n- 对于 $c=0$，SRM 退化为纯粹的 ERM，它会选择训练误差最低的模型。对于嵌套的多项式类别，这将总是可用的最高次数，即 $k=4$，因为它具有最大的自由度来拟合数据中的噪声。\n- 对于大的 $c$，复杂度惩罚在 SRM 准则中占主导地位，从而强烈偏好最简单的模型（低 $k$）。\n\n这些影响将在问题陈述中提供的测试用例中观察到。例如，真实模型是二次的（$k=2$）。我们期望具有良好选择参数（例如，$s=1.0, c=0.05$）的 SRM 能够正确识别出 $k=2$。增加 $s$ 或 $c$ 应将选择推向 $k2$，而减小 $s$ 应使更高次数更具竞争力，可能导致过拟合（$k>2$）。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the polynomial regression model selection problem using SRM.\n    \"\"\"\n    # Define problem parameters\n    n = 20\n    degrees = [0, 1, 2, 3, 4]\n    B = 1.0\n    sigma = 0.1\n    noise_seed = 123\n    monte_carlo_samples = 2000\n    monte_carlo_seed = 999\n\n    # Define the test suite\n    test_cases = [\n        # (s, c)\n        (1.0, 0.05),  # Case 1: Baseline\n        (0.5, 0.05),  # Case 2: Reduced feature magnitudes\n        (2.0, 0.05),  # Case 3: Increased feature magnitudes\n        (1.0, 0.5),   # Case 4: Strong complexity penalty\n        (0.1, 0.05),  # Case 5: Very small scaling\n        (1.0, 0.0),   # Case 6: Pure ERM (no penalty)\n    ]\n\n    # Generate the dataset\n    x = np.linspace(-1.0, 1.0, n)\n    noise_rng = np.random.RandomState(noise_seed)\n    epsilon = noise_rng.normal(0, sigma, n)\n    y_true = 1 - 2 * x + 0.5 * x**2\n    y = y_true + epsilon\n\n    # Prepare for Monte Carlo approximation of Rademacher complexity\n    rademacher_rng = np.random.RandomState(monte_carlo_seed)\n    sigma_matrix = rademacher_rng.choice([-1, 1], size=(monte_carlo_samples, n))\n\n    results = []\n    # Loop over all test cases\n    for s, c in test_cases:\n        srm_values = []\n        # Loop over all degrees k\n        for k in degrees:\n            # 1. Construct scaled design matrix\n            x_scaled = s * x\n            # np.vander creates columns in decreasing power order by default.\n            # a Vandermonde matrix of order N-1 for a k-th degree polynomial. so N=k+1.\n            # increasing=True puts powers as (x^0, x^1, ..., x^k)\n            phi = np.vander(x_scaled, N=k + 1, increasing=True)\n            \n            # 2. Compute Empirical Risk (ERM)\n            # Find w_hat using unconstrained least squares\n            w_hat, residuals, _, _ = np.linalg.lstsq(phi, y, rcond=None)\n            \n            # Calculate predictions and empirical risk\n            if residuals.size == 0:\n                # If the fit is perfect or underdetermined, residuals is empty\n                emp_risk = 0.0\n            else:\n                emp_risk = residuals[0] / n\n            \n            # 3. Approximate Empirical Rademacher Complexity\n            # phi.T has shape (k+1, n)\n            # sigma_matrix.T has shape (n, M)\n            # The product has shape (k+1, M)\n            # Each column is phi.T @ sigma_j\n            term_inside_norm = phi.T @ sigma_matrix.T\n            # Take L2 norm over axis 0 (columns) -> shape (M,)\n            norms = np.linalg.norm(term_inside_norm, axis=0)\n            \n            rad_complexity = np.mean((B / n) * norms)\n            \n            # 4. Compute SRM criterion\n            srm = emp_risk + c * rad_complexity\n            srm_values.append(srm)\n        \n        # 5. Select best k\n        # np.argmin breaks ties by taking the first occurrence (smallest k)\n        k_star = degrees[np.argmin(srm_values)]\n        results.append(k_star)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3118242"}]}