{"hands_on_practices": [{"introduction": "统计学习的核心问题之一是从数据中估计一个期望值，例如经验风险。这个练习将探讨如何使用集中不等式来量化我们对估计值有多大的信心。通过直接比较两种基本的不等式——霍夫丁不等式和伯恩斯坦不等式，你将亲身体会到一个关键原则：利用关于数据分布的更多信息（例如方差）可以得到更紧的界，从而在达到相同置信度时需要更少的样本 [@problem_id:3189962]。", "problem": "一位数据科学家正在估计一个二元分类任务中的有界损失的均值。设 $\\{X_{i}\\}_{i=1}^{n}$ 是独立同分布的随机变量，满足 $X_{i} \\in [0,1]$ 且 $\\mathbb{E}[X_{i}] = \\mu$。目标是选择一个样本量 $n$，使得经验均值 $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ 以至少 $1 - \\delta$ 的概率满足 $|\\hat{\\mu}_{n} - \\mu| \\leq \\varepsilon$，其中 $\\varepsilon \\in (0,1)$ 和 $\\delta \\in (0,1)$ 是给定的设计参数。\n\n考虑两种方法：\n\n1) 一种仅使用范围的方法，只利用 $X_{i} \\in [0,1]$ 这一信息。\n\n2) 一种感知方差的方法，额外利用已知 $\\operatorname{Var}(X_{i}) = \\sigma^{2}$（其中 $\\sigma^{2} \\in [0, \\tfrac{1}{4}]$）这一知识。\n\n定义 $n_{\\text{H}}(\\varepsilon,\\delta)$ 为在仅使用范围的方法下获得的充足样本量，$n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$ 为在感知方差的方法下获得的充足样本量。令样本量节省因子为\n$$\nS(\\varepsilon,\\delta,\\sigma^{2}) \\equiv \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}.\n$$\n\n使用关于有界独立随机变量的基础集中不等式结果，以及在可用时利用方差的知识，推导 $S(\\varepsilon,\\delta,\\sigma^{2})$ 仅作为 $\\varepsilon$ 和 $\\sigma^{2}$ 函数的精确简化表达式。你的最终答案必须是单一的闭式表达式；不允许出现不等式或隐式定义。不要提供数值近似。", "solution": "问题要求推导样本量节省因子 $S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})}$，其中 $n_{\\text{H}}$ 和 $n_{\\text{B}}$ 是从集中不等式推导出的充足样本量。目标是找到一个仅依赖于 $\\varepsilon$ 和 $\\sigma^{2}$ 的 $S$ 的表达式。这将通过对每种情况应用适当的基础集中不等式，然后计算推导出的样本量之比来实现。\n\n首先，我们确定仅使用范围方法的充足样本量 $n_{\\text{H}}(\\varepsilon,\\delta)$。给定 $\\{X_{i}\\}_{i=1}^{n}$ 是独立同分布 (i.i.d.) 的随机变量，且 $X_{i} \\in [0,1]$。设 $\\mathbb{E}[X_i] = \\mu$ 且 $\\hat{\\mu}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$。适用于这种情况的工具是 Hoeffding 不等式，它界定了一组有界独立随机变量之和与其期望值之间的偏差。对于样本均值，Hoeffding 不等式表述为：\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp(-2n\\varepsilon^2)$$\n这个不等式成立，因为每个 $X_i$ 的范围是 $1-0=1$。我们要求偏差大于 $\\varepsilon$ 的概率最多为 $\\delta$：\n$$2 \\exp(-2n\\varepsilon^2) \\le \\delta$$\n为了找到一个充足的样本量 $n$，我们对 $n$ 求解这个不等式：\n$$\\exp(-2n\\varepsilon^2) \\le \\frac{\\delta}{2}$$\n$$-2n\\varepsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$2n\\varepsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n因此，仅使用范围方法的充足样本量为：\n$$n_{\\text{H}}(\\varepsilon,\\delta) = \\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\n接下来，我们确定感知方差方法的充足样本量 $n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})$。除了之前的条件，我们还知道 $\\operatorname{Var}(X_{i}) = \\sigma^{2}$。对于有界变量，包含方差信息的基础结果是 Bernstein 不等式。我们将其应用于零均值变量 $Y_i = X_i - \\mu$。我们有 $\\mathbb{E}[Y_i] = 0$ 和 $\\operatorname{Var}(Y_i) = \\operatorname{Var}(X_i) = \\sigma^2$。由于 $X_i \\in [0,1]$ 且 $\\mu = \\mathbb{E}[X_i] \\in [0,1]$，变量 $Y_i$ 是有界的，即 $|Y_i| = |X_i - \\mu| \\le \\max(\\mu, 1-\\mu) \\le 1$。我们取界限 $M=1$。一种常见的双边 Bernstein 不等式形式是：\n$$P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right| \\ge \\varepsilon\\right) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + M\\varepsilon/3)}\\right)$$\n将 $|\\hat{\\mu}_{n} - \\mu|$ 替换 $\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right|$ 并代入 $M=1$：\n$$P(|\\hat{\\mu}_{n} - \\mu| \\ge \\varepsilon) \\le 2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right)$$\n我们要求这个概率最多为 $\\delta$：\n$$2 \\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\delta$$\n求解充足的样本量 $n$：\n$$\\exp\\left(-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)}\\right) \\le \\frac{\\delta}{2}$$\n$$-\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\le \\ln\\left(\\frac{\\delta}{2}\\right)$$\n$$\\frac{n\\varepsilon^2}{2(\\sigma^2 + \\varepsilon/3)} \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)$$\n$$n \\ge \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n因此，感知方差方法的充足样本量为：\n$$n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)$$\n\n最后，我们通过计算 $n_{\\text{H}}$ 和 $n_{\\text{B}}$ 的比率来计算样本量节省因子 $S(\\varepsilon,\\delta,\\sigma^{2})$：\n$$S(\\varepsilon,\\delta,\\sigma^{2}) = \\frac{n_{\\text{H}}(\\varepsilon,\\delta)}{n_{\\text{B}}(\\varepsilon,\\delta,\\sigma^{2})} = \\frac{\\frac{1}{2\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}{\\frac{2(\\sigma^2 + \\varepsilon/3)}{\\varepsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)}$$\n项 $\\ln\\left(\\frac{2}{\\delta}\\right)$ 和 $\\varepsilon^2$ 被消去，得到了一个按要求仅依赖于 $\\sigma^2$ 和 $\\varepsilon$ 的表达式：\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1/2}{2(\\sigma^2 + \\varepsilon/3)} = \\frac{1}{4(\\sigma^2 + \\varepsilon/3)}$$\n为了提供一个分母中不含分数的简化闭式表达式，我们可以将其重写为：\n$$S(\\sigma^{2},\\varepsilon) = \\frac{1}{4\\sigma^2 + \\frac{4\\varepsilon}{3}} = \\frac{1}{\\frac{12\\sigma^2 + 4\\varepsilon}{3}} = \\frac{3}{12\\sigma^2 + 4\\varepsilon} = \\frac{3}{4(3\\sigma^2 + \\varepsilon)}$$\n这就是样本量节省因子的最终简化表达式。", "answer": "$$\n\\boxed{\\frac{3}{4(3\\sigma^2 + \\varepsilon)}}\n$$", "id": "3189962"}, {"introduction": "在机器学习中，我们通常不是评估单个固定的假设，而是在一个庞大的假设类别中寻找最佳假设。这就引出了一个更深刻的问题：我们如何保证在整个假设类别中，经验风险都能一致地接近真实风险？这个问题通过引入一致收敛的概念来解决，它为模型的泛化能力提供了理论基石 [@problem_id:3189954]。通过这个练习，你将为一个基础的函数类别计算其VC维（一种复杂度的度量），并应用著名的Dvoretzky–Kiefer–Wolfowitz不等式来推导确保一致收敛所需的样本量。", "problem": "考虑实数线上的单调阈值函数所构成的假设类，定义为 $\\mathcal{H} = \\{ h_{t} : \\mathbb{R} \\to \\{0,1\\} \\mid h_{t}(x) = \\mathbf{1}\\{x \\le t\\},\\ t \\in \\mathbb{R} \\}$。设 $X_{1},\\dots,X_{n}$ 是从 $\\mathbb{R}$ 上的任意分布 $P$ 中抽取的独立同分布的实值样本，其累积分布函数为 $F$。对于 $h \\in \\mathcal{H}$，定义真实风险 $P(h) = \\mathbb{E}[h(X)]$ 和经验风险 $P_{n}(h) = \\frac{1}{n} \\sum_{i=1}^{n} h(X_{i})$。\n\n任务：\n1. 从第一性原理出发，通过直接应用打散（shattering）的定义，计算 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维度。\n2. 设 $\\epsilon \\in (0,1)$ 且 $\\delta \\in (0,1)$。仅使用 Dvoretzky–Kiefer–Wolfowitz 不等式的精确常数形式作为基础事实，推导最小的整数样本量 $n^{\\star}(\\epsilon,\\delta)$，使得对于 $\\mathbb{R}$ 上的每一个分布 $P$，以至少 $1-\\delta$ 的概率（针对样本的抽取），一致偏差界\n$$\n\\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| \\le \\epsilon\n$$\n同时成立。\n3. 通过将 $\\mathcal{H}$ 上的一致偏差问题归约为一个在其累积分布函数于中位数处连续且严格递增的分布下，固定阈值处的二项比例偏差问题，论证在任何形式为\n$$\n\\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big|  \\epsilon \\right) \\le K \\exp\\!\\big( - c\\, n \\epsilon^{2} \\big),\n$$\n的无分布亚高斯尾界中，对于所有足够大的 $n$ 和固定区间 $(0,\\epsilon_{0}]$ 中的所有 $\\epsilon$ 都成立时，可能的最大指数常数 $c$ 是一个固定的数值。确定这个最优的 $c$。\n\n请将你的最终答案以单行矩阵的形式报告，顺序为：$\\mathcal{H}$ 的 VC 维度、$n^{\\star}(\\epsilon,\\delta)$ 的精确闭式表达式以及最优指数常数 $c$。无需四舍五入，答案必须是精确的闭式表达式。", "solution": "用户要求我解决一个统计学习理论中的三部分问题。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n\n*   假设类：$\\mathcal{H} = \\{ h_{t} : \\mathbb{R} \\to \\{0,1\\} \\mid h_{t}(x) = \\mathbf{1}\\{x \\le t\\},\\ t \\in \\mathbb{R} \\}$。\n*   数据：$X_{1},\\dots,X_{n}$ 是从一个分布 $P$ 中抽取的独立同分布（i.i.d.）的实值样本，其累积分布函数（CDF）为 $F$。\n*   真实风险：$P(h) = \\mathbb{E}[h(X)]$。\n*   经验风险：$P_{n}(h) = \\frac{1}{n} \\sum_{i=1}^{n} h(X_{i})$。\n*   任务 1：计算 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维度。\n*   任务 2：找到最小的整数样本量 $n^{\\star}(\\epsilon,\\delta)$，使得对于 $\\epsilon, \\delta \\in (0,1)$，$\\Pr\\left(\\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)| \\le \\epsilon\\right) \\ge 1-\\delta$，使用 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式的精确常数形式。\n*   任务 3：在无分布亚高斯尾界 $\\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big|  \\epsilon \\right) \\le K \\exp\\!\\big( - c\\, n \\epsilon^{2} \\big)$ 中，确定最优常数 $c$。\n\n**第 2 步：使用提取的已知条件进行验证**\n\n*   **科学依据**：该问题在统计学习理论、概率论和大偏差理论的基本原理方面有坚实的基础。所有概念，如 VC 维度、一致收敛和 DKW 不等式，都是标准的且有严格定义。\n*   **良定性**：每个任务都是良定的。任务 1 是一个标准的计算。任务 2 是一个指定不等式的直接应用。任务 3 是一个更高级但定义明确的问题，涉及一个著名不等式中常数的最优性，并对使用的方法给出了明确的提示。\n*   **客观性**：问题以精确、客观的数学语言陈述。\n*   **完整性与一致性**：问题是自洽的，没有矛盾。在统计学习理论的背景下，对“Dvoretzky–Kiefer–Wolfowitz 不等式的精确常数形式”的引用是明确的。\n\n**第 3 步：结论与行动**\n\n问题是有效的。将提供完整的解答。\n\n### 解题过程\n\n解答分为三部分，对应于问题陈述中的三个任务。\n\n**任务 1：$\\mathcal{H}$ 的 VC 维度**\n一个假设类 $\\mathcal{H}$ 的 Vapnik–Chervonenkis (VC) 维度，记作 $\\mathrm{VCdim}(\\mathcal{H})$，是 $\\mathcal{H}$ 能够打散（shatter）的最大点集的大小。如果对于每一种可能的标签组合 $(y_1, \\dots, y_m) \\in \\{0, 1\\}^m$，都存在一个假设 $h \\in \\mathcal{H}$ 使得对所有的 $i \\in \\{1, \\dots, m\\}$ 都有 $h(x_i) = y_i$，那么点集 $\\{x_1, \\dots, x_m\\}$ 就被称作被 $\\mathcal{H}$ 打散了。我们这个类中的假设是 $h_t(x) = \\mathbf{1}\\{x \\le t\\}$。\n\n首先，我们证明 $\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$。考虑任意单个点 $\\{x_1\\}$。有 $2^1 = 2$ 种可能的标签：$\\{0\\}$ 和 $\\{1\\}$。\n*   要获得标签 $\\{1\\}$，我们需要 $h_t(x_1) = 1$，即 $x_1 \\le t$。我们可以选择 $t = x_1$。\n*   要获得标签 $\\{0\\}$，我们需要 $h_t(x_1) = 0$，即 $x_1 > t$。我们可以选择 $t = x_1 - 1$。\n因为单个点可以被两种方式标记，所以单个点可以被打散。因此，$\\mathrm{VCdim}(\\mathcal{H}) \\ge 1$。\n\n接下来，我们证明 $\\mathrm{VCdim}(\\mathcal{H})  2$。考虑任意两个不同点的集合 $\\{x_1, x_2\\}$。不失一般性，设 $x_1  x_2$。有 $2^2 = 4$ 种可能的标签组合：$(0,0), (0,1), (1,0), (1,1)$。让我们看看是否能实现所有这些组合。\n*   对于 $(0,0)$：我们需要 $h_t(x_1)=0$ 和 $h_t(x_2)=0$。这意味着 $x_1 > t$ 和 $x_2 > t$。我们可以选择任意 $t  x_1$，例如 $t=x_1 - 1$。\n*   对于 $(1,1)$：我们需要 $h_t(x_1)=1$ 和 $h_t(x_2)=1$。这意味着 $x_1 \\le t$ 和 $x_2 \\le t$。我们可以选择任意 $t \\ge x_2$，例如 $t=x_2$。\n*   对于 $(1,0)$：我们需要 $h_t(x_1)=1$ 和 $h_t(x_2)=0$。这意味着 $x_1 \\le t$ 和 $x_2 > t$。我们可以选择任意满足 $x_1 \\le t  x_2$ 的 $t$，例如 $t=x_1$。\n*   对于 $(0,1)$：我们需要 $h_t(x_1)=0$ 和 $h_t(x_2)=1$。这意味着 $x_1 > t$ 和 $x_2 \\le t$。这将意味着 $t  x_1$ 且 $t \\ge x_2$。因为我们假设了 $x_1  x_2$，所以不可能找到这样的 $t$。\n\n因为对于任意一对点 $\\{x_1, x_2\\}$ 且 $x_1  x_2$，标签组合 $(0,1)$ 都无法被生成，所以 $\\mathcal{H}$ 无法打散任何大小为 2 的集合。因此，VC 维度是 1。\n\n**任务 2：样本量 $n^{\\star}(\\epsilon,\\delta)$**\n我们要界定的一致偏差是 $\\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)|$。对于给定的假设类 $\\mathcal{H}$，真实风险和经验风险分别是：\n$P(h_t) = \\mathbb{E}[\\mathbf{1}\\{X \\le t\\}] = \\Pr(X \\le t) = F(t)$，其中 $F$ 是数据生成分布 $P$ 的累积分布函数（CDF）。\n$P_n(h_t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\} = \\hat{F}_n(t)$，其中 $\\hat{F}_n$ 是经验累积分布函数。\n因此，一致偏差等价于真实 CDF 和经验 CDF 之间的柯尔莫哥洛夫-斯米尔诺夫距离：\n$$ \\sup_{h \\in \\mathcal{H}} \\big| P(h) - P_{n}(h) \\big| = \\sup_{t \\in \\mathbb{R}} \\big| F(t) - \\hat{F}_n(t) \\big| $$\n问题要求使用由 Massart (1990) 建立的 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式的精确常数形式。该不等式表明，对于任意 $n \\ge 1$ 和任意 $\\epsilon > 0$，\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le 2 \\exp(-2n\\epsilon^2). $$\n我们想找到最小的整数 $n^{\\star}(\\epsilon, \\delta)$，使得互补事件的概率至少为 $1-\\delta$：\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| \\le \\epsilon \\right) \\ge 1 - \\delta. $$\n这等价于确保尾部事件的概率最多为 $\\delta$：\n$$ \\Pr\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\le \\delta. $$\n通过应用 DKW 不等式，我们可以通过将其上界设置为小于或等于 $\\delta$ 来满足这个条件：\n$ 2 \\exp(-2n\\epsilon^2) \\le \\delta $。\n我们现在对这个不等式求解 $n$：\n$$ \\exp(-2n\\epsilon^2) \\le \\frac{\\delta}{2} $$\n两边取自然对数：\n$$ -2n\\epsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\n两边乘以 $-1$ 并反转不等号：\n$$ 2n\\epsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right) $$\n最后，分离出 $n$：\n$$ n \\ge \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) $$\n由于 $n$ 必须是整数，满足此条件的最小整数 $n$ 是右侧表达式向上取整的值。\n$$ n^{\\star}(\\epsilon,\\delta) = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil. $$\n\n**任务 3：最优指数常数 $c$**\n我们被要求寻找可能的最大常数 $c$，使得对于某个常数 $K$，以下界限对所有分布 $P$、所有足够大的 $n$ 和所有 $\\epsilon \\in (0, \\epsilon_0]$ 成立：\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{h \\in \\mathcal{H}} |P(h) - P_n(h)| > \\epsilon \\right) \\le K \\exp(-cn\\epsilon^2). $$\n左侧是 $\\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right)$。常数为 $c=2$ 的 DKW 不等式表明这样的界存在。为了证明 $c=2$ 是最优（可能的最大）值，我们必须建立一个匹配的关于衰减率的下界。\n\n我们按照提示，将问题简化为二项比例的偏差问题。对所有分布的概率上确界必须大于或等于任何单个分布的概率。此外，对所有阈值 $t$ 的上确界必须大于或等于在任何单个阈值 $t_0$ 处的偏差。\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t \\in \\mathbb{R}} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{P_0}\\!\\left( |F_{0}(t_0) - \\hat{F}_{n,0}(t_0)| > \\epsilon \\right) $$\n对于任何特定的分布 $P_0$ 和阈值 $t_0$。\n\n让我们选择 $P_0$ 为 $[0,1]$ 上的均匀分布，即 $X \\sim U(0,1)$，其 CDF 为 $F_0(t) = t$（对于 $t \\in [0,1]$）。我们选择阈值 $t_0 = 1/2$，这是该分布的中位数。在这一点上，$F_0(1/2) = 1/2$。\n经验值为 $\\hat{F}_{n,0}(1/2) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le 1/2\\}$。令 $Y_i = \\mathbf{1}\\{X_i \\le 1/2\\}$。由于 $X_i \\sim U(0,1)$，所以 $Y_i$ 是独立同分布的伯努利随机变量，成功概率为 $p=\\Pr(X_i \\le 1/2) = F_0(1/2) = 1/2$。\n和 $S_n = \\sum_{i=1}^n Y_i = n \\hat{F}_{n,0}(1/2)$ 服从二项分布，$S_n \\sim \\mathrm{Binomial}(n, 1/2)$。\n\n在 $t_0=1/2$ 处的偏差是 $|\\hat{F}_{n,0}(1/2) - F_0(1/2)| = |\\frac{S_n}{n} - \\frac{1}{2}|$。\n这个量的大偏差行为是众所周知的。根据 Sanov 定理，概率 $\\Pr(|\\frac{S_n}{n} - \\frac{1}{2}| > \\epsilon)$ 以由 Kullback-Leibler (KL) 散度给出的速率指数衰减。对于大的 $n$ 和小的 $\\epsilon$，\n$$ \\Pr\\left(\\left|\\frac{S_n}{n} - \\frac{1}{2}\\right| > \\epsilon\\right) \\approx \\exp\\left(-n \\inf_{|q-1/2| \\ge \\epsilon} D_{KL}(q || 1/2)\\right), $$\n其中 $D_{KL}(q || p) = q \\ln(q/p) + (1-q)\\ln((1-q)/(1-p))$。下确界在边界处达到，例如在 $q = 1/2 + \\epsilon$ 处。让我们分析速率函数 $D_{KL}(1/2+\\epsilon || 1/2)$：\n$$ D_{KL}(1/2+\\epsilon || 1/2) = \\left(\\frac{1}{2}+\\epsilon\\right) \\ln\\left(\\frac{1/2+\\epsilon}{1/2}\\right) + \\left(\\frac{1}{2}-\\epsilon\\right) \\ln\\left(\\frac{1/2-\\epsilon}{1/2}\\right) $$\n$$ = \\left(\\frac{1}{2}+\\epsilon\\right) \\ln(1+2\\epsilon) + \\left(\\frac{1}{2}-\\epsilon\\right) \\ln(1-2\\epsilon) $$\n使用泰勒级数展开 $\\ln(1+x) = x - x^2/2 + x^3/3 - \\dots$ 对于小的 $x$：\n$$ \\ln(1+2\\epsilon) = 2\\epsilon - \\frac{(2\\epsilon)^2}{2} + O(\\epsilon^3) = 2\\epsilon - 2\\epsilon^2 + O(\\epsilon^3) $$\n$$ \\ln(1-2\\epsilon) = -2\\epsilon - \\frac{(-2\\epsilon)^2}{2} + O(\\epsilon^3) = -2\\epsilon - 2\\epsilon^2 + O(\\epsilon^3) $$\n将这些代入 KL 散度的表达式中：\n$$ D_{KL}(1/2+\\epsilon || 1/2) \\approx \\left(\\frac{1}{2}+\\epsilon\\right)(2\\epsilon - 2\\epsilon^2) + \\left(\\frac{1}{2}-\\epsilon\\right)(-2\\epsilon - 2\\epsilon^2) $$\n$$ = (\\epsilon - \\epsilon^2 + 2\\epsilon^2 - 2\\epsilon^3) + (-\\epsilon - \\epsilon^2 + 2\\epsilon^2 + 2\\epsilon^3) + O(\\epsilon^4) $$\n$$ = 2\\epsilon^2 + O(\\epsilon^4) $$\n对于小的 $\\epsilon$，大偏差率中的主导项是 $2\\epsilon^2$。这意味着对于大的 $n$，存在一个常数 $C'0$ 使得\n$$ \\sup_{P} \\Pr\\!\\left( \\sup_{t} |F(t) - \\hat{F}_n(t)| > \\epsilon \\right) \\ge \\Pr_{U(0,1)}\\!\\left( \\left|\\hat{F}_n(1/2) - 1/2\\right| > \\epsilon \\right) \\ge C' \\exp(-2n\\epsilon^2). $$\n如果我们有一个形式为 $K \\exp(-cn\\epsilon^2)$ 的上界，它必须容纳这个下界。为了使不等式 $C' \\exp(-2n\\epsilon^2) \\le K \\exp(-cn\\epsilon^2)$ 对所有大的 $n$ 都成立，上界的指数衰减率不能快于下界的指数衰减率。这要求 $-c \\ge -2$，即 $c \\le 2$。\n\n由于 DKW 不等式提供了一个 $c=2$ 的上界，而我们对特定情况的分析表明不可能有 $c>2$，因此常数 $c$ 的最大可能值为 2。", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil  2 \\end{pmatrix}}\n$$", "id": "3189954"}, {"introduction": "在掌握了一致收敛的基本思想之后，我们可以运用更强大的工具来分析和比较现代机器学习中使用的复杂模型。Rademacher复杂度是一种依赖于数据的度量，它能精妙地刻画一个假设类别的“丰富度”或其拟合随机噪声的能力，这直接关系到模型的过拟合风险。这项实践将引导你使用Rademacher复杂度来剖析$L_1$和$L_2$正则化对线性分类器泛化性能的不同影响 [@problem_id:3189970]，从而为$L_1$正则化在高维场景中的优势提供一个深刻的理论解释。", "problem": "考虑在 $\\mathbb{R}^d$ 上具有 $f_{\\mathbf{w}}(\\mathbf{x}) = \\langle \\mathbf{w}, \\mathbf{x} \\rangle$ 形式的实值分数的线性分类。给定一个固定样本 $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\{-1, +1\\}$。假设对于所有 $i \\in \\{1,\\dots,n\\}$，特征向量满足 $\\|\\mathbf{x}_i\\|_2 \\le R_2$ 和 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$，其中 $R_2, R_\\infty > 0$ 是已知的半径。考虑两个假设类\n$\\mathcal{H}_2 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_2 \\le B_2\\}$ 和 $\\mathcal{H}_1 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_1 \\le B_1\\}$，\n其半径为 $B_2, B_1 > 0$。设 $\\ell:\\mathbb{R}\\times\\{-1,+1\\}\\to[0,1]$ 是一个在其第一个参数上 1-利普希茨的损失函数。$\\ell \\circ \\mathcal{H}_p$ 的一致收敛可以通过经验Rademacher复杂度和集中不等式来控制。\n\n哪个选项正确地描述了 $\\mathcal{H}_2$ 和 $\\mathcal{H}_1$ 的经验Rademacher复杂度如何随 $n$, $d$, $R_2$ 和 $R_\\infty$ 缩放，并正确地解释了任何维度依赖性的几何原因？\n\nA. 对于任何这样的样本，$\\mathcal{H}_2$ 的经验Rademacher复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\dfrac{B_2 R_2}{\\sqrt{n}}$，而 $\\mathcal{H}_1$ 的经验Rademacher复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$。因此，$\\ell_1$ 类产生一个 $\\sqrt{\\log d}$ 因子，因为它的对偶范数是 $\\ell_\\infty$（坐标上的最大值），而 $\\ell_2$ 类的界中没有显式的 $d$。使用 $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\,\\|\\mathbf{x}\\|_\\infty$，如果 $B_1 \\approx B_2$ 且 $d$ 很大，$\\ell_1$ 类的复杂度可以小约 $\\sqrt{d/\\log d}$ 的因子。\n\nB. 如果 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，那么 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_2}{\\sqrt{n}}$（无维度关系），因为 $\\ell_1$ 的对偶范数是 $\\ell_2$；因此，$\\ell_1$ 正则化消除了对 $d$ 的任何依赖。\n\nC. 当 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 时，由于取坐标上的最大值，$\\ell_2$ 类在其Rademacher复杂度中必然表现出 $\\sqrt{\\log d}$ 因子，而在相同条件下，$\\ell_1$ 类是无维度关系的。\n\nD. 在对 $\\|\\mathbf{x}_i\\|_\\infty$ 没有任何界的情况下，只要 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，$\\ell_1$ 和 $\\ell_2$ 类的经验Rademacher复杂度在通用常数因子内是相同的。", "solution": "该问题陈述是统计学习理论中的一个有效练习。我们将首先推导经验Rademacher复杂度的相关界，然后评估每个选项。\n\n在样本 $S = \\{z_1, \\dots, z_n\\}$ 上，函数类 $\\mathcal{F}$ 的经验Rademacher复杂度定义为：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{f \\in \\mathcal{F}} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i f(z_i) \\right] $$\n其中 $\\sigma_1, \\dots, \\sigma_n$ 是独立的Rademacher随机变量，即 $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = 1/2$。\n\n在这个问题中，函数是损失函数 $\\ell$ 和线性函数类 $\\mathcal{H}_p$ 的复合。设完整的函数类为 $\\mathcal{L}_p = \\{(\\mathbf{x}, y) \\mapsto \\ell(\\langle \\mathbf{w}, \\mathbf{x} \\rangle, y) \\mid \\mathbf{w} \\in \\mathcal{H}_p \\}$。样本为 $S = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$。\n损失函数 $\\ell$ 在其第一个参数上是1-利普希茨的。根据Ledoux-Talagrand收缩不等式，复合类 $\\mathcal{L}_p$ 的Rademacher复杂度受限于其基础线性函数类的Rademacher复杂度。设 $\\mathcal{F}_p = \\{\\mathbf{x} \\mapsto \\langle \\mathbf{w}, \\mathbf{x} \\rangle \\mid \\mathbf{w} \\in \\mathcal{H}_p \\}$。收缩原理给出：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{L}_p) \\le 1 \\cdot \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) $$\n我们基于数据点 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_n\\}$ 计算 $\\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p)$：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\mathbf{w} \\in \\mathcal{H}_p} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle \\right] $$\n根据内积的线性性，这可以写成：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\frac{1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\mathbf{w} \\in \\mathcal{H}_p} \\left\\langle \\mathbf{w}, \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\rangle \\right] $$\n上确界 $\\sup_{\\|\\mathbf{w}\\|_p \\le B_p} \\langle \\mathbf{w}, \\mathbf{v} \\rangle$ 等于 $B_p \\|\\mathbf{v}\\|_{p^*}$，其中 $\\|\\cdot\\|_{p^*}$ 是 $\\|\\cdot\\|_p$ 的对偶范数。这给出了通用公式：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_p) = \\frac{B_p}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_{p^*} \\right] $$\n\n现在我们分析两种具体情况。\n\n**情况1：$\\ell_2$ 类, $\\mathcal{H}_2 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_2 \\le B_2\\}$**\n这里，$p=2$，并且对偶范数也是 $\\ell_2$ 范数，因为 $(p^*) = 2$。\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_2) = \\frac{B_2}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2 \\right] $$\n使用Jensen不等式 ($\\mathbb{E}[X] \\le \\sqrt{\\mathbb{E}[X^2]}$):\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2 \\right] \\le \\sqrt{ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_2^2 \\right] } $$\n平方根内的项是：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\langle \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i, \\sum_{j=1}^n \\sigma_j \\mathbf{x}_j \\right\\rangle \\right] = \\sum_{i=1}^n \\sum_{j=1}^n \\mathbb{E}[\\sigma_i \\sigma_j] \\langle \\mathbf{x}_i, \\mathbf{x}_j \\rangle = \\sum_{i=1}^n \\|\\mathbf{x}_i\\|_2^2 $$\n这里使用了 $\\mathbb{E}[\\sigma_i \\sigma_j] = \\delta_{ij}$ 这个事实，因为 $\\sigma_i$ 是独立的并且均值为零。\n给定 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，我们有 $\\sum_{i=1}^n \\|\\mathbf{x}_i\\|_2^2 \\le n R_2^2$。\n因此，\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_2) \\le \\frac{B_2}{n} \\sqrt{n R_2^2} = \\frac{B_2 R_2}{\\sqrt{n}} $$\n这为 $\\mathcal{H}_2$ 的复杂度提供了一个无维度关系的界。\n\n**情况2：$\\ell_1$ 类, $\\mathcal{H}_1 = \\{\\mathbf{w} \\in \\mathbb{R}^d : \\|\\mathbf{w}\\|_1 \\le B_1\\}$**\n这里，$p=1$，对偶范数是 $\\ell_\\infty$ 范数，$p^* = \\infty$。\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) = \\frac{B_1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_\\infty \\right] $$\n$\\ell_\\infty$ 范数是分量的最大绝对值。设 $\\mathbf{z} = \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i$。那么 $\\|\\mathbf{z}\\|_\\infty = \\max_{j \\in \\{1,\\dots,d\\}} |z_j| = \\max_{j \\in \\{1,\\dots,d\\}} \\left| \\sum_{i=1}^n \\sigma_i x_{i,j} \\right|$。\n数量 $\\mathbb{E}[\\max_j |\\sum_i \\sigma_i x_{i,j}|]$ 可以使用次高斯变量最大值的标准结果来界定。一个著名的界（与Massart引理相关）是：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\max_{j=1,\\dots,d} \\left| \\sum_{i=1}^n \\sigma_i x_{i,j} \\right| \\right] \\le \\sqrt{2 \\log(2d)} \\cdot \\max_{j=1,\\dots,d} \\sqrt{\\sum_{i=1}^n x_{i,j}^2} $$\n我们已知 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$，这意味着对所有 $i, j$ 都有 $|x_{i,j}| \\le R_\\infty$。因此，对任意 $j$，$\\sum_{i=1}^n x_{i,j}^2 \\le \\sum_{i=1}^n R_\\infty^2 = n R_\\infty^2$。\n将此代入界中：\n$$ \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\left\\| \\sum_{i=1}^n \\sigma_i \\mathbf{x}_i \\right\\|_\\infty \\right] \\le \\sqrt{2 \\log(2d)} \\cdot \\sqrt{n R_\\infty^2} = R_\\infty \\sqrt{n} \\sqrt{2 \\log(2d)} $$\n这得出了 $\\mathcal{H}_1$ 的复杂度界：\n$$ \\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) \\le \\frac{B_1}{n} \\left( R_\\infty \\sqrt{n} \\sqrt{2 \\log(2d)} \\right) = \\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}} $$\n这个界表现出对维度 $d$ 的温和对数依赖。\n\n现在，我们评估这些选项。\n\n**A.** “对于任何这样的样本，$\\mathcal{H}_2$ 的经验Rademacher复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\dfrac{B_2 R_2}{\\sqrt{n}}$，而 $\\mathcal{H}_1$ 的经验Rademacher复杂度满足 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$。因此，$\\ell_1$ 类产生一个 $\\sqrt{\\log d}$ 因子，因为它的对偶范数是 $\\ell_\\infty$（坐标上的最大值），而 $\\ell_2$ 类的界中没有显式的 $d$。使用 $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\,\\|\\mathbf{x}\\|_\\infty$，如果 $B_1 \\approx B_2$ 且 $d$ 很大，$\\ell_1$ 类的复杂度可以小约 $\\sqrt{d/\\log d}$ 的因子。”\n- 提出的两个界正是上面使用范数约束的自然配对（$B_2, R_2$ 和 $B_1, R_\\infty$）推导出来的。这部分是**正确的**。\n- 对 $\\sqrt{\\log d}$ 因子来源的解释也是**正确的**：它源于对 $\\ell_\\infty$ 对偶范数固有的 $d$ 个坐标上的最大值进行界定，而 $\\ell_2$ 对偶范数的计算避免了这一点，从而得到了一个无维度关系的界（当数据在 $\\ell_2$ 中有界时）。\n- 最后的比较是高维统计中的一个标准论证。如果我们假设数据受 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 约束，那么 $\\|\\mathbf{x}_i\\|_2$ 的最紧可用界是 $R_2 = \\sqrt{d} R_\\infty$。相应的 $\\mathcal{H}_2$ 的Rademacher界将按 $\\frac{B_2 (\\sqrt{d} R_\\infty)}{\\sqrt{n}}$ 比例缩放，即与 $\\sqrt{d}$ 成正比。$\\mathcal{H}_1$ 的界按 $\\frac{B_1 R_\\infty \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$ 比例缩放，即与 $\\sqrt{\\log d}$ 成正比。对于大的 $d$，$\\mathcal{H}_2$ 的界与 $\\mathcal{H}_1$ 的界之比约为 $\\sqrt{d}/\\sqrt{\\log d}$。这意味着 $\\ell_1$ 类的复杂度界要小得多。该陈述措辞谨慎，表述为“可以有更小的复杂度”，这得到了对这些界的分析的支持。这部分是**正确的**。\n因此，整个选项是正确的。\n\n**B.** “如果 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，那么 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\dfrac{B_1 R_2}{\\sqrt{n}}$（无维度关系），因为 $\\ell_1$ 的对偶范数是 $\\ell_2$；因此，$\\ell_1$ 正则化消除了对 $d$ 的任何依赖。”\n- 不等式 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$ 可以被推导出来。$\\widehat{\\mathfrak{R}}_S(\\mathcal{F}_1) = \\frac{B_1}{n} \\mathbb{E}_{\\boldsymbol{\\sigma}} [ \\| \\sum_i \\sigma_i \\mathbf{x}_i \\|_\\infty ]$。因为对于任何向量 $\\mathbf{v}$，都有 $\\|\\mathbf{v}\\|_\\infty \\le \\|\\mathbf{v}\\|_2$，所以我们有 $\\mathbb{E}[\\|\\cdot\\|_\\infty] \\le \\mathbb{E}[\\|\\cdot\\|_2]$。使用 $\\mathcal{H}_2$ 分析的结果，$\\mathbb{E}[\\| \\sum_i \\sigma_i \\mathbf{x}_i \\|_2] \\le R_2 \\sqrt{n}$。因此，该不等式成立。\n- 然而，其理由“因为 $\\ell_1$ 的对偶范数是 $\\ell_2$”在事实上是**不正确的**。$\\ell_1$ 的对偶范数是 $\\ell_\\infty$。$\\ell_2$ 的对偶范数是 $\\ell_2$。这个基本错误使得整个推理无效。\n\n**C.** “当 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 时，由于取坐标上的最大值，$\\ell_2$ 类在其Rademacher复杂度中必然表现出 $\\sqrt{\\log d}$ 因子，而在相同条件下，$\\ell_1$ 类是无维度关系的。”\n- 这个陈述颠倒了这两个类的角色。$\\ell_2$ 类的复杂度界与 $\\ell_2$ 对偶范数有关，它不涉及坐标上的最大值，也不会引入 $\\sqrt{\\log d}$ 因子。使用 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 会给出一个与 $\\sqrt{d}$ 成正比的 $\\mathcal{H}_2$ 的界（因为 $\\|\\mathbf{x}\\|_2 \\le \\sqrt{d}\\|\\mathbf{x}\\|_\\infty$），而不是 $\\sqrt{\\log d}$。\n- 相反地，正是 $\\ell_1$ 类由于其 $\\ell_\\infty$ 对偶范数，其复杂度界依赖于 $\\sqrt{\\log d}$，而且恰好是在 $\\|\\mathbf{x}_i\\|_\\infty \\le R_\\infty$ 的条件下。\n- 整个陈述是**不正确的**。\n\n**D.** “在对 $\\|\\mathbf{x}_i\\|_\\infty$ 没有任何界的情况下，只要 $\\|\\mathbf{x}_i\\|_2 \\le R_2$，$\\ell_1$ 和 $\\ell_2$ 类的经验Rademacher复杂度在通用常数因子内是相同的。”\n- 在 $\\|\\mathbf{x}_i\\|_2 \\le R_2$ 的条件下，我们有界 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_2) \\le \\frac{B_2 R_2}{\\sqrt{n}}$，并且如B的分析所示，$\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2}{\\sqrt{n}}$。虽然上界看起来相似，但实际的复杂度是 $\\frac{B_2}{n}\\mathbb{E}[\\|\\sum\\sigma_i\\mathbf{x}_i\\|_2]$ 和 $\\frac{B_1}{n}\\mathbb{E}[\\|\\sum\\sigma_i\\mathbf{x}_i\\|_\\infty]$。范数 $\\|\\cdot\\|_2$ 和 $\\|\\cdot\\|_\\infty$ 并非在通用常数范围内等价；它们的关系依赖于维度 $d$ ($1 \\le \\|\\mathbf{v}\\|_2/\\|\\mathbf{v}\\|_\\infty \\le \\sqrt{d}$)。正如反例（例如，数据向量是正交的与共线的）所示，这两个复杂度的比率可能取决于 $n$ 和数据结构，而不仅仅是一个通用常数。这个主张过于强烈，通常是错误的。例如，如果我们使用包含维度的 $\\mathcal{H}_1$ 的界，该界由 $\\|\\mathbf{x}_i\\|_\\infty \\le \\|\\mathbf{x}_i\\|_2 \\le R_2$ 推导得出，我们得到 $\\widehat{\\mathfrak{R}}_S(\\mathcal{H}_1) \\le \\frac{B_1 R_2 \\sqrt{2 \\log(2d)}}{\\sqrt{n}}$，这与 $\\mathcal{H}_2$ 的界明确相差一个 $\\sqrt{\\log d}$ 因子。因此，复杂度是不同的。这个选项是**不正确的**。\n\n根据分析，只有选项A是完全正确的。", "answer": "$$\\boxed{A}$$", "id": "3189970"}]}