{"hands_on_practices": [{"introduction": "选择损失函数是决定一个算法稳定性的基础。这项练习将直接比较均值（源于平方损失）和中位数（源于绝对损失）的稳定性，以揭示其中一个如何对异常值具有更强的鲁棒性，这是算法稳定性的一个关键方面。通过这个具体的例子，你将亲手验证和理解，在面对含有极端值的数据时，不同的风险最小化策略会产生稳定程度截然不同的模型。[@problem_id:3098721]", "problem": "您将分析两种常数函数回归算法在重尾误差下的算法稳定性。考虑一个数据集 $S = \\{(x_j, y_j)\\}_{j=1}^n$，其中输入 $x_j \\in \\mathbb{R}$，输出 $y_j \\in \\mathbb{R}$。定义两种学习算法，它们通过最小化经验风险来拟合常数预测器 $f_S(x) \\equiv c$：(a) 平方损失回归，返回样本均值；(b) 绝对损失回归（中位数回归或分位数为 $q = \\tfrac{1}{2}$ 的分位数回归），返回样本中位数。对于单点替换，将 $S^{(i,y^\\star)}$ 定义为仅将 $S$ 中的第 $i$ 个响应 $y_i$ 替换为 $y^\\star$ 而保持所有其他数据对不变所得到的数据集。对于任意固定的输入 $x_0 \\in \\mathbb{R}$，将算法 $A$ 在 $(S, i, y^\\star)$ 上的经验点替换稳定性定义为\n$$\n\\Delta_A(S,i,y^\\star) \\equiv \\bigl| f_S(x_0) - f_{S^{(i,y^\\star)}}(x_0) \\bigr|.\n$$\n由于预测器是常数函数，$\\Delta_A(S,i,y^\\star)$ 不依赖于 $x_0$。\n\n您的任务是编写一个程序，对于一个确定性的重尾数据集族 $S$，计算 $\\Delta_{\\text{mean}}$ 和 $\\Delta_{\\text{median}}$，并为一组指定的测试用例验证是否 $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$。您必须使用重尾分布的分位数网格来确定性地构造 $S$，具体方法如下。\n\n1. 位置为 $0$、尺度为 $\\gamma  0$ 的 Cauchy 分布：对于 $j \\in \\{0,1,\\dots,n-1\\}$，令\n$$\np_j = \\frac{j + \\tfrac{1}{2}}{n}, \\quad y_j = \\gamma \\cdot \\tan\\!\\bigl(\\pi(p_j - \\tfrac{1}{2})\\bigr).\n$$\n2. 尺度（最小值）为 $x_m  0$、形状为 $\\alpha  0$ 的 Pareto 分布：对于 $j \\in \\{0,1,\\dots,n-1\\}$，令\n$$\np_j = \\frac{j + \\tfrac{1}{2}}{n}, \\quad y_j = \\frac{x_m}{(1 - p_j)^{1/\\alpha}}.\n$$\n\n在这两种情况下，由于输入对于常数预测器是无关紧要的，因此对所有 $j$ 设 $x_j \\equiv 0$。当 $n$ 为偶数时，样本中位数必须定义为两个中心顺序统计量的平均值。对于每个测试用例，计算\n$$\n\\Delta_{\\text{mean}}(S,i,y^\\star) = \\bigl|\\bar{y}(S) - \\bar{y}(S^{(i,y^\\star)})\\bigr|, \\quad \n\\Delta_{\\text{median}}(S,i,y^\\star) = \\bigl|\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})\\bigr|.\n$$\n返回一个布尔值，指示是否 $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$。\n\n使用以下参数值测试套件，它们共同涵盖了典型、边界和边缘场景。每个测试用例指定了分布族、数据集大小 $n$、重尾参数、替换索引 $i$（从零开始）以及替换值 $y^\\star$。\n\n- 测试用例 1（对称重尾，n为奇数）：\n  - 族：Cauchy\n  - 参数：$n = 101$, $\\gamma = 1$\n  - 替换：$i = 0$, $y^\\star = 10^6$。\n- 测试用例 2（对称重尾，n为偶数）：\n  - 族：Cauchy\n  - 参数：$n = 100$, $\\gamma = 1$\n  - 替换：$i = 99$, $y^\\star = -10^6$。\n- 测试用例 3（非对称重尾，n为奇数）：\n  - 族：Pareto\n  - 参数：$n = 99$, $x_m = 1$, $\\alpha = 1.5$\n  - 替换：$i = 0$, $y^\\star = 10^9$。\n- 测试用例 4（非对称重尾，n为偶数）：\n  - 族：Pareto\n  - 参数：$n = 50$, $x_m = 1$, $\\alpha = 2$\n  - 替换：$i = 0$, $y^\\star = 10^6$。\n\n您的程序必须为每个测试用例计算一个布尔值，该值指示在指定的点替换下，中位数回归是否至少与均值回归一样稳定，即是否\n$$\n\\Delta_{\\text{median}}(S,i,y^\\star) \\le \\Delta_{\\text{mean}}(S,i,y^\\star).\n$$\n\n最终输出格式：您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[r_1,r_2,r_3,r_4]$），其中每个 $r_k$ 是测试用例 $k$ 的布尔值。\n\n不涉及物理单位。所有数值结果都是无单位的实数和布尔值。不使用角度。不使用百分比。禁止使用任何随机性；您必须完全按照规定使用分位数网格来确定性地构造 $S$。", "solution": "该问题被评估为**有效**。它在科学上基于统计学习理论，特别是算法稳定性，并且问题提出得很好，提供了所有必要的数据和定义。任务是比较基于均值（平方损失）和中位数（绝对损失）的回归估计器在确定性生成的重尾数据集上的稳定性。该问题是客观的、可形式化的并且计算上是可行的。我们将继续提供完整解决方案。\n\n我们的目标是比较两个常数回归预测器（样本均值和样本中位数）的点替换稳定性。算法 $A$ 的稳定性是通过当训练集 $S$ 中的一个点被改变时其输出预测器的变化来衡量的。稳定性度量定义为：\n$$\n\\Delta_A(S,i,y^\\star) \\equiv \\bigl| f_S(x_0) - f_{S^{(i,y^\\star)}}(x_0) \\bigr|\n$$\n其中 $S^{(i,y^\\star)}$ 是数据集 $S$ 中第 $i$ 个响应 $y_i$ 被替换为新值 $y^\\star$ 后的数据集。由于预测器是常数函数 $f_S(x) \\equiv c$，这简化为在原始数据集和修改后数据集上拟合的常数值之间的绝对差。\n\n设数据集为 $S = \\{y_j\\}_{j=0}^{n-1}$，包含 $n$ 个标量响应。输入 $x_j$ 是无关紧要的。\n\n**1. 样本均值的稳定性**\n\n第一种算法，对应于最小化平方误差损失 $\\sum_{j=0}^{n-1} (y_j - c)^2$，产生样本均值作为其估计：\n$$\nf_S(x) \\equiv \\bar{y}(S) = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j\n$$\n修改后的数据集 $S^{(i,y^\\star)}$ 包含元素 $\\{y_0, \\dots, y_{i-1}, y^\\star, y_{i+1}, \\dots, y_{n-1}\\}$。这个新数据集的样本均值为：\n$$\n\\bar{y}(S^{(i,y^\\star)}) = \\frac{1}{n} \\left( \\left(\\sum_{j=0}^{n-1} y_j\\right) - y_i + y^\\star \\right) = \\frac{1}{n} \\sum_{j=0}^{n-1} y_j - \\frac{y_i}{n} + \\frac{y^\\star}{n} = \\bar{y}(S) + \\frac{y^\\star - y_i}{n}\n$$\n因此，均值的稳定性度量 $\\Delta_{\\text{mean}}$ 为：\n$$\n\\Delta_{\\text{mean}}(S,i,y^\\star) = \\bigl| \\bar{y}(S) - \\bar{y}(S^{(i,y^\\star)}) \\bigr| = \\left| \\bar{y}(S) - \\left( \\bar{y}(S) + \\frac{y^\\star - y_i}{n} \\right) \\right| = \\left| - \\frac{y^\\star - y_i}{n} \\right| = \\frac{|y_i - y^\\star|}{n}\n$$\n此公式表明，均值的变化与扰动的大小 $|y_i - y^\\star|$ 成正比，与数据集大小 $n$ 成反比。对于大的扰动（离群值），变化可以是任意大的。\n\n**2. 样本中位数的稳定性**\n\n第二种算法，对应于最小化绝对误差损失 $\\sum_{j=0}^{n-1} |y_j - c|$，产生样本中位数作为其估计：\n$$\nf_S(x) \\equiv \\operatorname{med}(S)\n$$\n样本中位数是根据排序后的数据计算的。设 $y_{(0)} \\le y_{(1)} \\le \\dots \\le y_{(n-1)}$ 为数据集 $S$ 的顺序统计量。\n- 如果 $n$ 是奇数，中位数是中心元素，$\\operatorname{med}(S) = y_{((n-1)/2)}$。\n- 如果 $n$ 是偶数，中位数是两个中心元素的平均值，$\\operatorname{med}(S) = \\frac{1}{2} (y_{(n/2 - 1)} + y_{(n/2)})$。\n\n与均值不同，中位数的稳定性 $\\Delta_{\\text{median}}$ 没有一个简单的封闭形式表达式。它必须通过算法计算：\n1.  生成初始数据集 $S = \\{y_j\\}_{j=0}^{n-1}$。\n2.  计算其中位数 $\\operatorname{med}(S)$。\n3.  通过将 $y_i$ 替换为 $y^\\star$ 来构造修改后的数据集 $S^{(i,y^\\star)}$。\n4.  计算修改后数据集的中位数 $\\operatorname{med}(S^{(i,y^\\star)})$。这需要重新排序或找到新的中心元素。\n5.  计算稳定性：$\\Delta_{\\text{median}}(S,i,y^\\star) = |\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})|$。\n\n中位数是众所周知的稳健统计量。它的值取决于数据点的秩，而不是它们的大小。因此，与均值相比，用一个极端离群值 $y^\\star$ 替换一个点 $y_i$ 通常导致中位数的更小变化，因为它可能只将中位数的位置移动到相邻的数据点。\n\n**3. 数据集生成**\n\n数据集是使用重尾分布的分位数函数（逆累积分布函数）确定性地构建的。这确保了数据具有所需的分布特性，而无需引入随机性。\n分位数网格点为 $p_j = \\frac{j + 0.5}{n}$，其中 $j \\in \\{0, 1, \\dots, n-1\\}$。\n\n- **Cauchy 分布：** 位置为 $0$，尺度为 $\\gamma  0$。分位数函数为 $F^{-1}(p) = \\gamma \\tan(\\pi(p - 0.5))$。\n  $$\n  y_j = \\gamma \\cdot \\tan\\!\\bigl(\\pi(p_j - \\tfrac{1}{2})\\bigr)\n  $$\n- **Pareto 分布：** 尺度（最小值）为 $x_m  0$，形状为 $\\alpha  0$。分位数函数为 $F^{-1}(p) = x_m / (1-p)^{1/\\alpha}$。\n  $$\n  y_j = \\frac{x_m}{(1 - p_j)^{1/\\alpha}}\n  $$\n由于分位数函数是单调的，并且 $p_j$ 值是有序的，因此生成的数据集 $\\{y_j\\}$ 已经是排序好的。\n\n**4. 每个测试用例的计算**\n\n对于每个指定的测试用例，我们执行以下步骤：\n1.  定义参数：分布族、$n$、重尾参数（$\\gamma$ 或 $(x_m, \\alpha)$）、替换索引 $i$ 和替换值 $y^\\star$。\n2.  使用相应的公式生成初始的已排序数据集 $S = \\{y_j\\}_{j=0}^{n-1}$。\n3.  使用解析公式计算均值的稳定性：$\\Delta_{\\text{mean}} = \\frac{|y_i - y^\\star|}{n}$。\n4.  通过算法计算中位数的稳定性：\n    a.  计算原始中位数 $\\operatorname{med}(S)$。\n    b.  通过将 $y_i$ 替换为 $y^\\star$ 创建修改后的数据集 $S^{(i,y^\\star)}$。\n    c.  计算新的中位数 $\\operatorname{med}(S^{(i,y^\\star)})$。\n    d.  计算 $\\Delta_{\\text{median}} = |\\operatorname{med}(S) - \\operatorname{med}(S^{(i,y^\\star)})|$。\n5.  比较稳定性值并确定是否 $\\Delta_{\\text{median}} \\le \\Delta_{\\text{mean}}$。结果是一个布尔值。\n\n将此过程系统地应用于问题描述中提供的所有测试用例，以生成最终的布尔结果列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes and compares the algorithmic stability of mean and median regression\n    under point replacement for deterministically generated heavy-tailed datasets.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'family': 'Cauchy', 'n': 101, 'gamma': 1, 'xm': None, 'alpha': None, 'i': 0, 'y_star': 1e6},\n        {'family': 'Cauchy', 'n': 100, 'gamma': 1, 'xm': None, 'alpha': None, 'i': 99, 'y_star': -1e6},\n        {'family': 'Pareto', 'n': 99, 'gamma': None, 'xm': 1, 'alpha': 1.5, 'i': 0, 'y_star': 1e9},\n        {'family': 'Pareto', 'n': 50, 'gamma': None, 'xm': 1, 'alpha': 2, 'i': 0, 'y_star': 1e6},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Unpack parameters for the current test case\n        n = case['n']\n        i = case['i']\n        y_star = case['y_star']\n        \n        # Step 1: Generate the initial dataset S = {y_j}\n        # The quantile points p_j\n        p = (np.arange(n) + 0.5) / n\n        \n        if case['family'] == 'Cauchy':\n            gamma = case['gamma']\n            # Generate sorted data using the Cauchy quantile function\n            y = gamma * np.tan(np.pi * (p - 0.5))\n        elif case['family'] == 'Pareto':\n            xm = case['xm']\n            alpha = case['alpha']\n            # Generate sorted data using the Pareto quantile function\n            y = xm / (1 - p)**(1 / alpha)\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown distribution family: {case['family']}\")\n\n        # The data y is generated sorted, as it's based on an ordered sequence of quantiles p_j\n        # and monotonic quantile functions.\n        \n        # Step 2: Compute stability of the mean\n        # We use the analytical formula: Delta_mean = |y_i - y_star| / n\n        y_i = y[i]\n        delta_mean = np.abs(y_i - y_star) / n\n        \n        # Step 3: Compute stability of the median\n        # a. Calculate the median of the original dataset\n        med_s = np.median(y)\n        \n        # b. Create the modified dataset S^(i, y_star)\n        y_mod = y.copy()\n        y_mod[i] = y_star\n        \n        # c. Calculate the median of the modified dataset\n        # np.median internally sorts the array, which is necessary here.\n        med_s_mod = np.median(y_mod)\n        \n        # d. Compute the stability measure for the median\n        delta_median = np.abs(med_s - med_s_mod)\n        \n        # Step 4: Compare stabilities and record the boolean result\n        is_median_more_stable = (delta_median = delta_mean)\n        results.append(is_median_more_stable)\n\n    # Final print statement in the exact required format.\n    # The str() of a boolean in Python is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3098721"}, {"introduction": "在第一个练习的基础上，我们将探讨更复杂的线性回归模型。模型的稳定性不仅受标签（$y$ 值）中异常值的影响，特征空间（$X$ 值）中的异常值——即高杠杆点——同样能破坏模型的稳定性。这项练习将引导你通过公式推导，精确揭示一个数据点的杠杆值和正则化强度 $\\lambda$ 是如何共同控制算法稳定性的。[@problem_id:3098822]", "problem": "您将通过量化在移除单个高杠杆率训练点时，学习到的预测器如何变化，来研究岭回归中的算法稳定性。请从第一性原理出发使用线性代数。从岭回归的经验风险最小化（ERM）公式、正规方程以及用于矩阵逆的秩一更新的 Sherman–Morrison 恒等式开始。您也可以使用 Cauchy–Schwarz 不等式以及帽子矩阵和杠杆分数的定义。不要使用或假设任何直接给出最终表达式的结果；相反，应从基本定义推导它们。\n\n给定一个由设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 和响应向量 $y \\in \\mathbb{R}^{n}$ 组成的训练集，考虑正则化参数为 $\\lambda \\gt 0$ 的岭回归，其中学习到的权重向量 $w_S \\in \\mathbb{R}^{d}$ 最小化目标函数\n$$\nJ(w) = \\tfrac{1}{2}\\|X w - y\\|_2^2 + \\tfrac{\\lambda}{2}\\|w\\|_2^2.\n$$\n对于任意 $x \\in \\mathbb{R}^{d}$，将学习到的预测器定义为 $f_S(x) = x^\\top w_S$。设 $S \\setminus \\{i\\}$ 表示移除了第 $i$ 个样本 $(x_i, y_i)$ 的数据集，并设 $f_{S \\setminus \\{i\\}}(x)$ 为相应的预测器。设 $A = X^\\top X + \\lambda I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。\n\n任务：\n1. 从岭回归的正规方程和 Sherman–Morrison 恒等式出发，推导在任意查询点 $x \\in \\mathbb{R}^{d}$ 处预测变化 $f_S(x) - f_{S \\setminus \\{i\\}}(x)$ 的精确表达式。该表达式应以 $A^{-1}$、 $x$、 $x_i$、残差 $y_i - f_S(x_i)$ 以及第 $i$ 个训练点在岭回归下的杠杆分数 $h_{ii}$ 来表示。杠杆分数定义为岭回归帽子矩阵 $H = X A^{-1} X^\\top$ 的第 $i$ 个对角元素，即 $h_{ii} = x_i^\\top A^{-1} x_i$。\n2. 使用由 $A^{-1}$ 导出的二次型上的 Cauchy–Schwarz 不等式，推导 $\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$ 的一个可计算上界，该上界仅依赖于 $\\lvert y_i - f_S(x_i) \\rvert$、杠杆分数 $h_{ii}$ 和查询杠杆率 $h_x = x^\\top A^{-1} x$。\n3. 对于下面的每个测试用例，在一个有限查询集 $Q$ 上计算两个量：\n   - 实际的最大预测变化 $\\max_{x \\in Q} \\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$。\n   - 仅依赖杠杆率的 Cauchy–Schwarz 上界 $\\max_{x \\in Q} \\Big(\\sqrt{h_x \\, h_{ii}} \\cdot \\lvert y_i - f_S(x_i) \\rvert \\big/ (1 - h_{ii})\\Big)$。\n   同时报告被移除点的杠杆分数 $h_{ii}$。解释高杠杆分数如何指示稳定性风险。\n\n实现要求：\n- 您的程序必须完全按照指定的方式实现推导并计算所要求的量。\n- 所有向量和矩阵均为实值，所有计算在维度上都是一致的。\n- 本问题中不涉及物理单位或角度。\n\n测试套件：\n对于每个用例，给定 $(X, y, \\lambda, i, Q)$:\n- 用例 1（高杠杆率、大残差、弱正则化）：\n  - $X = \\begin{bmatrix} 0.0 \\\\ 0.2 \\\\ 0.4 \\\\ 0.6 \\\\ 0.8 \\\\ 10.0 \\end{bmatrix}$，作为一个 $n \\times d$ 矩阵，其中 $n = 6$，$d = 1$。\n  - $y = \\begin{bmatrix} 0.0,\\, 0.4,\\, 0.8,\\, 1.2,\\, 1.6,\\, 35.0 \\end{bmatrix}^\\top$。\n  - $\\lambda = 10^{-6}$。\n  - $i = 5$（零基索引；这是第六行，即杠杆点）。\n  - $Q = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 2.0 \\\\ 5.0 \\\\ 10.0 \\end{bmatrix}$，作为一个 $m \\times d$ 矩阵，其中 $d = 1$。\n- 用例 2（相同数据，更强正则化）：\n  - $X, y$ 如用例 1 所示。\n  - $\\lambda = 5.0$。\n  - $i = 5$。\n  - $Q$ 如用例 1 所示。\n- 用例 3（无杠杆率异常值，弱正则化）：\n  - $X = \\begin{bmatrix} 0.0 \\\\ 0.25 \\\\ 0.5 \\\\ 0.75 \\\\ 1.0 \\\\ 1.25 \\end{bmatrix}$，作为一个 $n \\times d$ 矩阵，其中 $n = 6$，$d = 1$。\n  - $y$ 由 $y_j = 1.5 \\cdot x_j + 0.5 + \\varepsilon_j$ 给出，其中 $\\varepsilon = \\begin{bmatrix} 0.0,\\, 0.02,\\, -0.01,\\, 0.0,\\, 0.03,\\, -0.02 \\end{bmatrix}^\\top$，即 $y = \\begin{bmatrix} 0.5,\\, 0.875,\\, 1.24,\\, 1.625,\\, 2.03,\\, 2.355 \\end{bmatrix}^\\top$。\n  - $\\lambda = 10^{-6}$。\n  - $i = 2$。\n  - $Q = \\begin{bmatrix} 0.0 \\\\ 0.5 \\\\ 1.0 \\\\ 1.5 \\end{bmatrix}$。\n- 用例 4（高杠杆率点与趋势对齐，弱正则化）：\n  - $X$ 如用例 1 所示。\n  - $y$ 由精确的线性趋势 $y_j = 2.0 \\cdot x_j$ 给出，即 $y = \\begin{bmatrix} 0.0,\\, 0.4,\\, 0.8,\\, 1.2,\\, 1.6,\\, 20.0 \\end{bmatrix}^\\top$。\n  - $\\lambda = 10^{-6}$。\n  - $i = 5$。\n  - $Q$ 如用例 1 所示。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。\n- 对于每个用例，按此顺序输出三个浮点数：\n  $[\\text{actual\\_max\\_change}, \\, h_{ii}, \\, \\text{cs\\_bound}]$。\n- 将四个用例的结果连接成一个单一的扁平列表。\n- 将每个浮点数四舍五入到 $6$ 位小数。\n- 使用占位符的所需结构示例：$[a\\_1, h\\_1, b\\_1, a\\_2, h\\_2, b\\_2, a\\_3, h\\_3, b\\_3, a\\_4, h\\_4, b\\_4]$。", "solution": "该问题要求通过推导和计算移除单个数据点时预测器的变化，来分析岭回归的算法稳定性。推导将按要求从第一性原理出发。\n\n### 步骤 1：预测变化的推导 (任务 1)\n\n岭回归的目标函数由下式给出：\n$$\nJ(w) = \\frac{1}{2}\\|Xw - y\\|_2^2 + \\frac{\\lambda}{2}\\|w\\|_2^2\n$$\n其中 $X \\in \\mathbb{R}^{n \\times d}$ 是设计矩阵，$y \\in \\mathbb{R}^{n}$ 是响应向量，$w \\in \\mathbb{R}^{d}$ 是权重向量，$\\lambda  0$ 是正则化参数。\n\n为了找到最小化 $J(w)$ 的最优权重向量 $w_S$，我们计算关于 $w$ 的梯度并将其设为零：\n$$\n\\nabla_w J(w) = X^\\top(Xw - y) + \\lambda w = 0\n$$\n这导出了岭回归的正规方程：\n$$\n(X^\\top X + \\lambda I_d)w = X^\\top y\n$$\n定义 $A = X^\\top X + \\lambda I_d$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵，则完整数据集 $S$ 的解为：\n$$\nw_S = (X^\\top X + \\lambda I_d)^{-1} X^\\top y = A^{-1} X^\\top y\n$$\n由于 $\\lambda  0$ 且 $X^\\top X$ 是半正定的，因此 $A$ 保证是正定的，从而可逆。\n\n现在，考虑数据集 $S \\setminus \\{i\\}$，其中第 $i$ 个样本 $(x_i, y_i)$ 被移除。设 $X_{\\setminus i} \\in \\mathbb{R}^{(n-1) \\times d}$ 和 $y_{\\setminus i} \\in \\mathbb{R}^{n-1}$ 分别是移除了第 $i$ 行和第 $i$ 个元素的数据矩阵和向量。此缩减数据集的权重向量 $w_{S \\setminus \\{i\\}}$ 是相应岭回归问题的解：\n$$\nw_{S \\setminus \\{i\\}} = (X_{\\setminus i}^\\top X_{\\setminus i} + \\lambda I_d)^{-1} X_{\\setminus i}^\\top y_{\\setminus i}\n$$\n我们可以用完整数据集的量来表示 $X_{\\setminus i}^\\top X_{\\setminus i}$ 和 $X_{\\setminus i}^\\top y_{\\setminus i}$。矩阵 $X^\\top X$ 是外积之和 $\\sum_{j=1}^n x_j x_j^\\top$。移除第 $i$ 个点得到：\n$$\nX_{\\setminus i}^\\top X_{\\setminus i} = \\sum_{j \\neq i} x_j x_j^\\top = \\left(\\sum_{j=1}^n x_j x_j^\\top\\right) - x_i x_i^\\top = X^\\top X - x_i x_i^\\top\n$$\n类似地，对于叉积项：\n$$\nX_{\\setminus i}^\\top y_{\\setminus i} = \\sum_{j \\neq i} x_j y_j = \\left(\\sum_{j=1}^n x_j y_j\\right) - x_i y_i = X^\\top y - x_i y_i\n$$\n设 $A_{\\setminus i} = X_{\\setminus i}^\\top X_{\\setminus i} + \\lambda I_d$。代入 $X_{\\setminus i}^\\top X_{\\setminus i}$ 的表达式：\n$$\nA_{\\setminus i} = (X^\\top X - x_i x_i^\\top) + \\lambda I_d = (X^\\top X + \\lambda I_d) - x_i x_i^\\top = A - x_i x_i^\\top\n$$\n这表明 $A_{\\setminus i}$ 是 $A$ 的一个秩一更新。我们可以使用 Sherman–Morrison 公式求其逆：对于一个可逆矩阵 $M$ 和向量 $u, v$，有 $(M - uv^\\top)^{-1} = M^{-1} + \\frac{M^{-1}uv^\\top M^{-1}}{1 - v^\\top M^{-1}u}$。\n令 $M=A$，$u=v=x_i$，我们得到：\n$$\nA_{\\setminus i}^{-1} = (A - x_i x_i^\\top)^{-1} = A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - x_i^\\top A^{-1}x_i}\n$$\n项 $x_i^\\top A^{-1} x_i$ 是第 $i$ 个点的杠杆分数，记作 $h_{ii}$。所以，\n$$\nA_{\\setminus i}^{-1} = A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - h_{ii}}\n$$\n现在我们可以写出 $w_{S \\setminus \\{i\\}}$ 的表达式：\n$$\nw_{S \\setminus \\{i\\}} = A_{\\setminus i}^{-1} (X^\\top y - x_i y_i) = \\left(A^{-1} + \\frac{A^{-1}x_i x_i^\\top A^{-1}}{1 - h_{ii}}\\right) (X^\\top y - x_i y_i)\n$$\n展开此式，并代入 $w_S = A^{-1} X^\\top y$：\n$$\nw_{S \\setminus \\{i\\}} = A^{-1}X^\\top y - A^{-1}x_i y_i + \\frac{A^{-1}x_i x_i^\\top A^{-1}X^\\top y - A^{-1}x_i (x_i^\\top A^{-1}x_i) y_i}{1-h_{ii}} \\\\\n= w_S - A^{-1}x_i y_i + \\frac{A^{-1}x_i x_i^\\top w_S - A^{-1}x_i h_{ii} y_i}{1-h_{ii}}\n$$\n让我们求权重向量的变化量 $w_S - w_{S \\setminus \\{i\\}}$：\n$$\nw_S - w_{S \\setminus \\{i\\}} = A^{-1}x_i y_i - \\frac{A^{-1}x_i (x_i^\\top w_S - h_{ii} y_i)}{1 - h_{ii}} \\\\\n= A^{-1}x_i \\left( y_i - \\frac{x_i^\\top w_S - h_{ii} y_i}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\left( \\frac{y_i(1 - h_{ii}) - (x_i^\\top w_S - h_{ii} y_i)}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\left( \\frac{y_i - y_i h_{ii} - x_i^\\top w_S + h_{ii} y_i}{1 - h_{ii}} \\right) \\\\\n= A^{-1}x_i \\frac{y_i - x_i^\\top w_S}{1 - h_{ii}}\n$$\n项 $y_i - x_i^\\top w_S$ 是第 $i$ 个点的残差，即 $y_i - f_S(x_i)$。在查询点 $x$ 处的预测器变化为 $f_S(x) - f_{S \\setminus \\{i\\}}(x) = x^\\top(w_S - w_{S \\setminus \\{i\\}})$。代入权重变化的表达式：\n$$\nf_S(x) - f_{S \\setminus \\{i\\}}(x) = x^\\top \\left( A^{-1}x_i \\frac{y_i - x_i^\\top w_S}{1 - h_{ii}} \\right)\n$$\n重新排列标量，我们得到预测变化的精确表达式：\n$$\nf_S(x) - f_{S \\setminus \\{i\\}}(x) = \\frac{y_i - f_S(x_i)}{1 - h_{ii}} x^\\top A^{-1} x_i\n$$\n\n### 步骤 2：Cauchy-Schwarz 上界的推导 (任务 2)\n\n为了界定预测变化的幅度，我们取上述推导表达式的绝对值：\n$$\n\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert = \\left\\lvert \\frac{y_i - f_S(x_i)}{1 - h_{ii}} x^\\top A^{-1} x_i \\right\\rvert = \\frac{\\lvert y_i - f_S(x_i) \\rvert}{\\lvert 1 - h_{ii} \\rvert} \\lvert x^\\top A^{-1} x_i \\rvert\n$$\n由于 $A = X^\\top X + \\lambda I_d$ 且 $\\lambda  0$，$A$ 是对称正定（SPD）矩阵。其逆 $A^{-1}$ 也是对称正定矩阵。对于任何 SPD 矩阵 $M$，我们可以定义一个内积 $\\langle u, v \\rangle_M = u^\\top M v$。此内积的 Cauchy-Schwarz 不等式为 $\\lvert \\langle u, v \\rangle_M \\rvert^2 \\le \\langle u, u \\rangle_M \\langle v, v \\rangle_M$。\n将此应用于 $M = A^{-1}$，$u = x$ 和 $v = x_i$：\n$$\n\\lvert x^\\top A^{-1} x_i \\rvert \\le \\sqrt{(x^\\top A^{-1} x)(x_i^\\top A^{-1} x_i)}\n$$\n使用给定的查询杠杆率 $h_x = x^\\top A^{-1} x$ 和点的杠杆分数 $h_{ii} = x_i^\\top A^{-1} x_i$ 的定义，不等式变为：\n$$\n\\lvert x^\\top A^{-1} x_i \\rvert \\le \\sqrt{h_x h_{ii}}\n$$\n已知岭回归的杠杆分数 $h_{ii}$ 在范围 $(0, 1)$ 内。因此，$1 - h_{ii}  0$，且 $\\lvert 1 - h_{ii} \\rvert = 1 - h_{ii}$。将 $\\lvert x^\\top A^{-1} x_i \\rvert$ 的界限代入预测变化幅度的表达式，我们得到上界：\n$$\n\\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert \\le \\frac{\\lvert y_i - f_S(x_i) \\rvert}{1 - h_{ii}} \\sqrt{h_x h_{ii}}\n$$\n这就是所求的可计算上界。\n\n### 步骤 3：数值计算与解释 (任务 3)\n\n现在将实施推导出的公式，为给定的测试用例计算所需的量。这些量是：\n1.  $\\max_{x \\in Q} \\lvert f_S(x) - f_{S \\setminus \\{i\\}}(x) \\rvert$：在查询集 $Q$ 上的最大实际预测变化。\n2.  $h_{ii}$：被移除点的杠杆分数。\n3.  $\\max_{x \\in Q} \\Big(\\frac{\\lvert y_i - f_S(x_i) \\rvert}{1 - h_{ii}} \\sqrt{h_x h_{ii}}\\Big)$：在查询集 $Q$ 上 Cauchy–Schwarz 上界的最大值。\n\n对这些量在不同测试用例中的分析，揭示了关于算法稳定性的关键见解。推导出的界限表明，一个点 $(x_i, y_i)$ 具有影响力（即，移除它会导致预测器发生较大变化），需要同时满足两个条件：\n-   该点具有接近 $1$ 的高杠杆分数 $h_{ii}$。这使得分母 $1 - h_{ii}$ 很小，从而放大了效应。\n-   该点具有较大的残差 $\\lvert y_i - f_S(x_i) \\rvert$，意味着它不能被在完整数据集上训练的模型很好地解释。\n\n用例 1（高杠杆率，大残差）和用例 4（高杠杆率，小残差）将凸显这种相互作用。用例 2 展示了增加正则化 $\\lambda$ 如何影响稳定性，而用例 3 作为没有高杠杆率点的基准。请注意，对于 $d=1$ 的情况，Cauchy-Schwarz 不等式成为等式，因此实际变化和界限将是相同的。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the algorithmic stability problem for ridge regression.\n    The function iterates through predefined test cases, calculates the required\n    quantities based on the derived formulas, and prints the results in the\n    specified format.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: High-leverage with large residual, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [35.0]]),\n            \"lambda\": 1e-6,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        },\n        # Case 2: Same data, stronger regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [35.0]]),\n            \"lambda\": 5.0,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        },\n        # Case 3: No leverage outlier, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.25], [0.5], [0.75], [1.0], [1.25]]),\n            \"y\": np.array([[0.5], [0.875], [1.24], [1.625], [2.03], [2.355]]),\n            \"lambda\": 1e-6,\n            \"i\": 2,\n            \"Q\": np.array([[0.0], [0.5], [1.0], [1.5]])\n        },\n        # Case 4: High-leverage aligned with the trend, weak regularization\n        {\n            \"X\": np.array([[0.0], [0.2], [0.4], [0.6], [0.8], [10.0]]),\n            \"y\": np.array([[0.0], [0.4], [0.8], [1.2], [1.6], [20.0]]),\n            \"lambda\": 1e-6,\n            \"i\": 5,\n            \"Q\": np.array([[0.0], [1.0], [2.0], [5.0], [10.0]])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X, y, lambda_reg, i, Q = case[\"X\"], case[\"y\"], case[\"lambda\"], case[\"i\"], case[\"Q\"]\n        n, d = X.shape\n\n        # Step 1: Compute full dataset solution w_S\n        A = X.T @ X + lambda_reg * np.eye(d)\n        A_inv = np.linalg.inv(A)\n        w_S = A_inv @ X.T @ y\n\n        # Step 2: Extract point i and compute its leverage and residual\n        x_i_row = X[i:i+1, :]  # Shape (1, d)\n        x_i_col = x_i_row.T    # Shape (d, 1)\n        y_i_val = y[i, 0]\n\n        # Leverage score h_ii = x_i^T A^{-1} x_i\n        h_ii = (x_i_row @ A_inv @ x_i_col)[0, 0]\n\n        # Residual r_i = y_i - f_S(x_i)\n        f_S_xi = (x_i_row @ w_S)[0, 0]\n        r_i = y_i_val - f_S_xi\n\n        # Step 3: Compute actual maximum prediction change over Q\n        # Change formula: (r_i / (1 - h_ii)) * x^T A^{-1} x_i\n        change_factor = r_i / (1 - h_ii)\n        \n        # Q @ A_inv @ x_i_col broadcasts the calculation over all x in Q\n        cross_terms = Q @ A_inv @ x_i_col # Shape (m, 1)\n        \n        actual_changes = np.abs(change_factor * cross_terms)\n        actual_max_change = np.max(actual_changes)\n\n        # Step 4: Compute the Cauchy-Schwarz upper bound over Q\n        # Bound formula: |r_i|/(1-h_ii) * sqrt(h_x * h_ii)\n        \n        # Compute query leverages h_x = x^T A^{-1} x for all x in Q\n        # np.sum((Q @ A_inv) * Q, axis=1) is an efficient way to get diagonals of Q A_inv Q.T\n        h_Q = np.sum((Q @ A_inv) * Q, axis=1) # Shape (m,)\n        \n        bounds_vec = (np.abs(r_i) / (1 - h_ii)) * np.sqrt(h_Q * h_ii)\n        cs_bound = np.max(bounds_vec)\n\n        # Append results for this case\n        results.extend([actual_max_change, h_ii, cs_bound])\n        \n    # Format the final output string\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3098822"}, {"introduction": "本章的最后一个练习将回答一个关键问题：我们为什么要在意算法稳定性？这个练习精心设计了一个场景，其中两种算法在训练集上达到了完全相同且完美的表现（零训练误差），但它们的稳定性却截然不同。通过比较它们的泛化性能，你将揭示一个核心原理：更稳定的算法倾向于拥有更好的泛化能力。[@problem_id:3098731]", "problem": "您将实现并分析两种确定性学习算法。这两种算法在相同的假设类别中选择决策阈值，但表现出不同的算法稳定性。该设定是纯粹数学性的，并且是自包含的。\n\n考虑一个在实线上的二元分类问题，其实例空间为 $\\mathcal{X} = [-1,1]$，标签为 $\\mathcal{Y} = \\{-1,+1\\}$。真实标签函数为 $y(x) = \\mathrm{sign}(x)$，并约定 $\\mathrm{sign}(0)=+1$。假设类别是阈值集合 $\\mathcal{H} = \\{ h_t : h_t(x) = \\mathrm{sign}(x - t),\\ t \\in \\mathbb{R} \\}$，损失函数为 $0$-$1$ 损失 $\\ell(h_t,(x,y)) = \\mathbb{1}[h_t(x) \\neq y]$。\n\n一个大小为 $n = n_{-} + n_{+}$ 的训练样本 $S$ 的构建方式如下：首先从 $[-1,-a]$ 中独立均匀地抽取 $n_{-}$ 个负特征点（每个点标记为 $-1$），然后从 $[a,1]$ 中独立均匀地抽取 $n_{+}$ 个正特征点（每个点标记为 $+1$），其中 $a \\in (0,1)$ 是一个给定的间隔参数。此抽样规则使得样本对于区间 $(\\max\\{x_i : y_i=-1\\}, \\min\\{x_i : y_i=+1\\}]$ 中的任何阈值 $t$ 都是线性可分的。\n\n定义两种学习算法，它们将训练集 $S$ 映射到 $\\mathcal{H}$ 中的一个假设：\n- 算法 A（边缘阈值）：设 $p_{\\min}(S)$ 表示 $S$ 中最小的正特征值。算法 A 输出 $h_{t_A}$，其中 $t_A = p_{\\min}(S)$。\n- 算法 B（收缩边缘）：使用相同的 $p_{\\min}(S)$，算法 B 输出 $h_{t_B}$，其中 $t_B = \\tfrac{1}{2} \\cdot p_{\\min}(S)$。\n\n请注意，$t_A$ 和 $t_B$ 都位于区间 $(\\max\\{x_i : y_i=-1\\}, \\min\\{x_i : y_i=+1\\}]$ 内，因此两种算法在 $S$ 上都达到相同的训练误差（即零）。\n\n对于给定的训练集 $S$，定义留一法变体 $S^{(i)}$，即从 $S$ 中移除第 $i$ 个样本。对于一个算法 $\\mathcal{A}\\in\\{\\text{A},\\text{B}\\}$，将由 $S$ 产生的假设记为 $h_S^{\\mathcal{A}}$，由 $S^{(i)}$ 产生的假设记为 $h_{S^{(i)}}^{\\mathcal{A}}$。\n\n定义算法 $\\mathcal{A}$ 的经验留一法稳定性估计量，通过在固定评估网格 $\\mathcal{G} \\subset [-1,1]$ 上对损失的绝对差值取平均得到：\n$$\n\\widehat{\\beta}^{\\mathcal{A}}(S,\\mathcal{G}) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{1}{|\\mathcal{G}|} \\sum_{x \\in \\mathcal{G}} \\left| \\ell\\!\\left(h_S^{\\mathcal{A}}, (x, y(x))\\right) \\;-\\; \\ell\\!\\left(h_{S^{(i)}}^{\\mathcal{A}}, (x, y(x))\\right) \\right| \\right).\n$$\n使用相同的网格 $\\mathcal{G}$ 来估计 $h_S^{\\mathcal{A}}$ 的泛化误差（总体风险）为\n$$\n\\widehat{L}^{\\mathcal{A}}(S,\\mathcal{G}) \\;=\\; \\frac{1}{|\\mathcal{G}|} \\sum_{x \\in \\mathcal{G}} \\ell\\!\\left(h_S^{\\mathcal{A}}, (x, y(x))\\right).\n$$\n\n实现要求：\n- 使用一个均匀评估网格 $\\mathcal{G}$，该网格包含恰好 $m=100001$ 个点，范围覆盖 $[-1,1]$（包含端点）。\n- 对于每个测试用例，按规定生成训练数据 $S$，计算 $t_A$ 和 $t_B$，确认两种算法在 $S$ 上的训练误差相同，并计算 $\\widehat{\\beta}^{\\mathrm{A}}(S,\\mathcal{G})$、$\\widehat{\\beta}^{\\mathrm{B}}(S,\\mathcal{G})$、$\\widehat{L}^{\\mathrm{A}}(S,\\mathcal{G})$ 和 $\\widehat{L}^{\\mathrm{B}}(S,\\mathcal{G})$。\n- 对于每个测试用例，当且仅当两个条件同时成立时，输出一个等于 true 的布尔值：$\\widehat{\\beta}^{\\mathrm{B}}(S,\\mathcal{G})  \\widehat{\\beta}^{\\mathrm{A}}(S,\\mathcal{G})$ 和 $\\widehat{L}^{\\mathrm{B}}(S,\\mathcal{G}) \\le \\widehat{L}^{\\mathrm{A}}(S,\\mathcal{G})$。如果算法 A 和 B 在 $S$ 上的训练误差不相同，则该测试用例视为 false。\n\n测试套件：\n提供以下参数集 $(n_{-}, n_{+}, a, \\text{seed})$ 的结果：\n- 情况 1（通用、平衡、中等间隔）：$(50, 50, 0.2, 1)$。\n- 情况 2（通用、平衡、小间隔）：$(50, 50, 0.01, 7)$。\n- 情况 3（小样本、平衡、中等间隔）：$(5, 5, 0.2, 3)$。\n- 情况 4（样本数量不平衡、中等间隔）：$(80, 20, 0.2, 5)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、逗号分隔的布尔值列表（例如，“[True,True,False,True]”）。不应打印任何额外文本。\n\n不涉及物理单位。不出现角度。如果在内部计算任何分数，应将其作为标准浮点数处理；不要打印百分比。\n\n您的任务：\n编写一个完整的、可运行的程序，对每个测试用例执行上述操作，并以确切要求的格式打印单行聚合结果。程序不得接受用户输入，也不得访问外部文件或网络。", "solution": "该问题定义明确且科学上合理，设置在统计学习理论的标准框架内。它要求对两种学习算法——“边缘阈值”（算法 A）和“收缩边缘”（算法 B）——进行计算分析，分析其泛化误差和算法稳定性。\n\n### 1. 问题阐述\n\n设定是在域 $\\mathcal{X} = [-1,1]$ 上的一个二元分类问题。\n真实标签函数是 $y(x) = \\mathrm{sign}(x)$，约定 $\\mathrm{sign}(0) = +1$。这意味着对于 $x \\in [-1, 0)$，$y(x) = -1$；对于 $x \\in [0, 1]$，$y(x) = +1$。\n假设类别由阈值分类器组成，$\\mathcal{H} = \\{ h_t : t \\in \\mathbb{R} \\}$，其中 $h_t(x) = \\mathrm{sign}(x - t)$。为与真实情况保持一致，我们定义当 $x \\ge t$ 时 $h_t(x) = +1$，当 $x  t$ 时 $h_t(x) = -1$。\n\n大小为 $n = n_{-} + n_{+}$ 的训练数据 $S$ 由从 $[-1, -a]$ 均匀抽取的 $n_{-}$ 个点（标记为 $-1$）和从 $[a, 1]$ 均匀抽取的 $n_{+}$ 个点（标记为 $+1$）组成，其中 $a \\in (0,1)$。\n\n两种算法定义如下：\n-   **算法 A**：$h_{t_A}$，其中 $t_A = p_{\\min}(S)$，$p_{\\min}(S) = \\min\\{x_i : (x_i, +1) \\in S\\}$。\n-   **算法 B**：$h_{t_B}$，其中 $t_B = \\frac{1}{2} p_{\\min}(S)$。\n\n损失函数是 $0$-$1$ 损失，$\\ell(h_t,(x,y)) = \\mathbb{1}[h_t(x) \\neq y]$。\n\n### 2. 训练误差分析\n\n如果一个阈值 $t$ 能将所有负样本和正样本分开，那么它就能完美分类训练集 $S$。这要求 $\\max\\{x_i : y_i=-1\\}  t \\le \\min\\{x_i : y_i=+1\\}$。令 $n_{\\max}(S) = \\max\\{x_i : y_i=-1\\}$ 和 $p_{\\min}(S) = \\min\\{x_i : y_i=+1\\}$。\n根据数据生成过程，我们有 $n_{\\max}(S) \\le -a$ 和 $p_{\\min}(S) \\ge a$。可分离阈值的区间是 $(n_{\\max}(S), p_{\\min}(S)]$，该区间保证包含 $(-a, a]$。\n\n对于算法 A，阈值是 $t_A = p_{\\min}(S)$。因为 $n_{\\max}(S)  0$ 且 $p_{\\min}(S) > 0$，我们有 $n_{\\max}(S)  p_{\\min}(S)$，因此 $t_A$ 是一个可分离阈值。\n对于算法 B，阈值是 $t_B = \\frac{1}{2} p_{\\min}(S)$。因为 $p_{\\min}(S) \\ge a$，我们有 $t_B \\ge a/2$。由于 $n_{\\max}(S) \\le -a$，且对于 $a \\in (0,1)$ 有 $a/2 > -a$，所以我们有 $n_{\\max}(S)  t_B$。此外，$t_B = \\frac{1}{2}p_{\\min}(S)  p_{\\min}(S)$。因此，$n_{\\max}(S)  t_B  p_{\\min}(S)$，这意味着 $t_B$ 也是一个可分离阈值。\n\n由于 $t_A$ 和 $t_B$ 都是可分离阈值，两种算法都实现了 $0$ 的训练误差。它们的训练误差相同的条件得到了满足。\n\n### 3. 泛化误差分析\n\n泛化误差在包含 $m=100001$ 个点的 $[-1,1]$ 网格 $\\mathcal{G}$ 上进行经验估计：\n$$ \\widehat{L}^{\\mathcal{A}}(S,\\mathcal{G}) = \\frac{1}{|\\mathcal{G}|} \\sum_{x \\in \\mathcal{G}} \\ell(h_{t_{\\mathcal{A}}}, (x, y(x))) $$\n如果 $h_t(x) \\neq y(x)$，则点 $x$ 被 $h_t$ 错误分类。由于 $t_A = p_{\\min}(S)$ 和 $t_B = \\frac{1}{2}p_{\\min}(S)$ 都是正数（因为 $p_{\\min}(S) \\ge a > 0$），错误分类只可能发生在 $x \\ge 0$ 且我们预测为 $-1$ 的情况下。这发生在 $x \\ge 0$ 且 $x  t$ 的情况下，即对于区间 $[0, t)$ 中的点。\n因此，\n$$ \\widehat{L}^{\\mathcal{A}}(S,\\mathcal{G}) \\approx \\frac{t_A}{2} = \\frac{p_{\\min}(S)}{2} \\quad \\text{和} \\quad \\widehat{L}^{\\mathcal{B}}(S,\\mathcal{G}) \\approx \\frac{t_B}{2} = \\frac{p_{\\min}(S)}{4} $$\n由于 $p_{\\min}(S)>0$，我们总是有 $\\widehat{L}^{\\mathrm{B}}(S,\\mathcal{G})  \\widehat{L}^{\\mathrm{A}}(S,\\mathcal{G})$，因此 $\\widehat{L}^{\\mathrm{B}}(S,\\mathcal{G}) \\le \\widehat{L}^{\\mathrm{A}}(S,\\mathcal{G})$ 的条件得到满足。\n\n### 4. 稳定性分析\n\n留一法稳定性估计量为：\n$$ \\widehat{\\beta}^{\\mathcal{A}}(S,\\mathcal{G}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{1}{|\\mathcal{G}|} \\sum_{x \\in \\mathcal{G}} | \\ell(h_S^{\\mathcal{A}}, (x, y(x))) - \\ell(h_{S^{(i)}}^{\\mathcal{A}}, (x, y(x))) | \\right) $$\n内部和度量了当第 $i$ 个点被移除时，在网格上损失变化的平均值。这个损失的变化只发生在 $h_S$ 和 $h_{S^{(i)}}$ 做出不同预测的区域。这仅当阈值 $t$ 改变时才会发生。\n\n阈值 $t_A$ 和 $t_B$ 都仅依赖于 $p_{\\min}(S)$。因此，阈值仅在移除的样本恰好是定义了 $p_{\\min}(S)$ 的那个正样本时才会改变。\n设 $p_1 = p_{\\min}(S)$，并设 $p_2$ 是 $S$ 中第二小的正特征值。\n-   如果移除的样本不是 $p_1$，则 $p_{\\min}(S^{(i)}) = p_1$，阈值不变，内部和为 $0$。\n-   如果移除的样本是 $p_1$，并且 $p_1$ 在正样本中是唯一的，则 $p_{\\min}(S^{(i)}) = p_2$。\n    -   对于算法 A，阈值从 $t_A = p_1$ 变为 $t'_A = p_2$。损失变化的区域是区间 $[p_1, p_2)$。\n    -   对于算法 B，阈值从 $t_B = p_1/2$ 变为 $t'_B = p_2/2$。损失变化的区域是区间 $[p_1/2, p_2/2)$。\n\n由于 $p_2 > p_1 > 0$，区间 $[p_1, p_2)$ 的长度为 $p_2-p_1$，而区间 $[p_1/2, p_2/2)$ 的长度为 $(p_2-p_1)/2$。因此，算法 B 引起的损失变化区域更小。\n如果 $p_1$ 不是唯一的，则移除一个 $p_1$ 样本不会改变 $p_{\\min}$，因此稳定性为 $0$。\n在 $p_1$ 是唯一的情况下，算法 B 的稳定性估计量 $\\widehat{\\beta}^{\\mathrm{B}}$ 将严格小于算法 A 的 $\\widehat{\\beta}^{\\mathrm{A}}$。因此，$\\widehat{\\beta}^{\\mathrm{B}}(S,\\mathcal{G})  \\widehat{\\beta}^{\\mathrm{A}}(S,\\mathcal{G})$ 条件在阈值发生变化时成立。\n\n### 5. 结论\n\n算法 B（收缩边缘）比算法 A（边缘阈值）更稳定，并且具有更低的泛化误差。因此，对于任何随机抽样，这两个条件都应成立。最终结果取决于具体的随机数种子是否会产生一个唯一的最小正样本的情况。如果最小正样本不是唯一的，那么两种算法的稳定性估计都将为零，导致 $\\widehat{\\beta}^{\\mathrm{B}}  \\widehat{\\beta}^{\\mathrm{A}}$ 条件为假。不过，对于连续均匀分布，样本值重复的概率为零，因此我们预期在所有测试用例中条件都会成立。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes two deterministic learning algorithms for a binary classification problem,\n    evaluating their algorithmic stability and generalization error.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n_minus, n_plus, a, seed)\n        (50, 50, 0.2, 1),\n        (50, 50, 0.01, 7),\n        (5, 5, 0.2, 3),\n        (80, 20, 0.2, 5),\n    ]\n\n    results = []\n\n    # Evaluation grid setup\n    m = 100001\n    grid = np.linspace(-1.0, 1.0, m, dtype=np.float64)\n    \n    # Ground truth labels on the grid: y(x)=sign(x) with sign(0)=+1\n    y_grid = np.where(grid >= 0, 1, -1)\n\n    def sign_classifier(x, t):\n        # h_t(x) = sign(x-t) with sign(0)=+1 convention, i.e., h_t(x)=+1 for x>=t\n        return np.where(x >= t, 1, -1)\n\n    for n_minus, n_plus, a, seed in test_cases:\n        # Set seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Generate training data S\n        X_neg = np.random.uniform(low=-1.0, high=-a, size=n_minus)\n        X_pos = np.random.uniform(low=a, high=1.0, size=n_plus)\n        \n        S_X = np.concatenate((X_neg, X_pos))\n        S_y = np.concatenate((np.full(n_minus, -1), np.full(n_plus, 1)))\n        \n        n = n_minus + n_plus\n\n        # 2. Compute thresholds t_A and t_B from the full sample S\n        p_min_S = np.min(X_pos)\n        t_A_S = p_min_S\n        t_B_S = 0.5 * p_min_S\n\n        # 3. Confirm training errors are identical (and zero) as per problem spec\n        pred_A_S = sign_classifier(S_X, t_A_S)\n        train_err_A = np.sum(pred_A_S != S_y)\n\n        pred_B_S = sign_classifier(S_X, t_B_S)\n        train_err_B = np.sum(pred_B_S != S_y)\n        \n        if train_err_A != train_err_B or train_err_A != 0:\n            results.append(False)\n            continue\n        \n        # 4. Compute generalization error estimates L_hat(S)\n        loss_A_S_grid = np.where(sign_classifier(grid, t_A_S) != y_grid, 1, 0)\n        L_hat_A = np.mean(loss_A_S_grid)\n\n        loss_B_S_grid = np.where(sign_classifier(grid, t_B_S) != y_grid, 1, 0)\n        L_hat_B = np.mean(loss_B_S_grid)\n\n        # 5. Compute stability estimates beta_hat(S) using the efficient analytical method\n        unique_pos_samples = np.unique(X_pos)\n        counts = {val: count for val, count in zip(*np.unique(X_pos, return_counts=True))}\n        \n        p1 = unique_pos_samples[0]\n        count_p1 = counts[p1]\n\n        beta_hat_A = 0.0\n        beta_hat_B = 0.0\n\n        # Change in threshold only occurs if the minimum positive point is unique\n        if count_p1 == 1 and len(unique_pos_samples) > 1:\n            p2 = unique_pos_samples[1]\n            \n            # The LOO sum has only one non-zero term.\n            # Its contribution is the mean difference in loss over the grid.\n            \n            # For algorithm A, threshold changes from p1 to p2.\n            # The symmetric difference of misclassification sets is [p1, p2).\n            diff_A_points = np.sum((grid >= p1)  (grid  p2))\n            mean_diff_A = diff_A_points / m\n            beta_hat_A = mean_diff_A / n\n            \n            # For algorithm B, threshold changes from p1/2 to p2/2.\n            # The symmetric difference is [p1/2, p2/2).\n            diff_B_points = np.sum((grid >= p1/2)  (grid  p2/2))\n            mean_diff_B = diff_B_points / m\n            beta_hat_B = mean_diff_B / n\n\n        # If count_p1 > 1 or n_plus=1 (so len(unique_pos_samples)=1), \n        # the minimum value is not unique or no p2 exists. Removing any single point\n        # does not change the minimum. Thus, beta_hats remain 0.\n\n        # 6. Check conditions and store result\n        condition1 = beta_hat_B  beta_hat_A\n        condition2 = L_hat_B = L_hat_A\n        \n        results.append(condition1 and condition2)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3098731"}]}