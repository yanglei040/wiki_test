{"hands_on_practices": [{"introduction": "模型的可解释性不仅要处理线性关系，还必须能够量化非线性效应的影响。这个练习 [@problem_id:3173352] 提供了一个理想化的场景，通过解析推导来理解SHAP如何处理一个常见的非线性关系——饱和效应，例如药物剂量反应曲线。通过这个练习，你将亲手计算一个特征在特定取值下的贡献，并将其与其在所有可能取值下的平均贡献进行比较，从而直观地理解SHAP值“局部准确性”和“基线”的概念。", "problem": "考虑一个用于解释具有饱和效应的剂量-反应效应的双特征回归模型。设预测函数为\n$$\nf(x_{1},x_{2}) \\;=\\; g(x_{1}) \\;+\\; b\\,x_{2} \\;+\\; c,\n$$\n其中剂量-反应部分是饱和的，\n$$\ng(x_{1}) \\;=\\; L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr),\n$$\n其中常数 $L>0$ 且 $k>0$。假设在训练数据分布下，特征是统计独立的，其中 $X_{1} \\sim \\mathrm{Exponential}(\\lambda)$（率参数为 $\\lambda>0$），而 $X_{2}$ 具有有限均值 $\\mu_{2}$ 和有限方差。\n\n使用Shapley加性解释 (SHAP) 的定义，其中特征 $i$ 在观测值 $x=(x_{1},x_{2})$ 处的Shapley值 $\\phi_{i}$ 定义为在所有联盟上的平均边际贡献，\n$$\n\\phi_{i}(x) \\;=\\; \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{i\\}} \\frac{|S|!\\,\\bigl(2-|S|-1\\bigr)!}{2!}\\,\\Bigl(\\,\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S},\\,X_{i}=x_{i}\\bigr]\\;-\\;\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S}\\bigr]\\,\\Bigr),\n$$\n对于此模型和分布设置，推导SHAP值 $\\phi_{1}(x)$ 作为 $x_{1}$、$L$、$k$ 和 $\\lambda$ 的函数的闭式表达式。您的推导必须从上述定义开始，并且只使用给定的模型结构和分布假设。根据您推导的表达式，讨论当 $x_{1}$ 从近似线性区域（小的 $x_{1}$）移动到饱和平台（大的 $x_{1}$）时，$\\phi_{1}(x)$ 如何演变，但最终答案只需报告所要求的闭式表达式。不需要数值近似，也不需要四舍五入。请将您的最终答案表示为关于 $x_{1}$、$L$、$k$ 和 $\\lambda$ 的单个闭式解析表达式。", "solution": "用户希望我验证问题，如果问题有效，则解决它。\n\n### 步骤1：提取已知条件\n已知条件如下：\n- 预测函数：$f(x_{1},x_{2}) = g(x_{1}) + b\\,x_{2} + c$\n- 剂量-反应部分：$g(x_{1}) = L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr)$\n- 常数：$L>0$, $k>0$, $\\lambda>0$, $b$, $c$\n- 特征分布：特征是统计独立的。$X_{1} \\sim \\mathrm{Exponential}(\\lambda)$。$X_{2}$ 具有有限均值 $\\mu_{2}$ 和有限方差。\n- 特征 $i$ 的SHAP定义：\n$$\n\\phi_{i}(x) \\;=\\; \\sum_{S \\subseteq \\{1,2\\}\\setminus\\{i\\}} \\frac{|S|!\\,\\bigl(2-|S|-1\\bigr)!}{2!}\\,\\Bigl(\\,\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S},\\,X_{i}=x_{i}\\bigr]\\;-\\;\\mathbb{E}\\bigl[f(X)\\,\\big|\\,X_{S}=x_{S}\\bigr]\\,\\Bigr)\n$$\n- 目标：推导 $\\phi_{1}(x)$ 的闭式表达式。\n\n### 步骤2：使用提取的已知条件进行验证\n根据指定标准验证问题：\n- **科学依据**：该问题在统计学习和可解释性人工智能领域有充分的依据。该模型是一个带有标准饱和分量的加性模型。分布是常见的，并且Shapley值的定义是特征归因的标准定义。\n- **适定性**：问题是适定的。它提供了推导唯一解析解所需的所有信息——模型形式、分布假设、特征独立性以及SHAP值的具体定义。\n- **客观性**：问题使用精确的数学语言陈述，没有任何主观性或歧义。\n\n该问题没有表现出任何列出的缺陷：\n1.  **科学或事实不健全**：无。数学和统计设置是合理的。\n2.  **无法形式化或不相关**：无。该问题是一个与SHAP主题直接相关的形式化数学推导。\n3.  **不完整或矛盾的设置**：无。特征独立性的假设和指定的分布足以进行推导。$X_2$ 的属性（有限均值）是所需的全部信息。\n4.  **不切实际或不可行**：无。模型结构和分布选择在实践中很常见。\n5.  **不适定或结构不良**：无。该问题导向一个单一、稳定且有意义的解。\n6.  **伪深刻、琐碎或同义反复**：无。推导需要严格应用期望和条件化原理，并非微不足道。\n7.  **超出科学可验证性**：无。推导是一个完全可验证的数学过程。\n\n### 步骤3：结论和行动\n问题有效。我将继续进行推导。\n\n为了推导特征1在观测值 $x=(x_{1}, x_{2})$ 处的Shapley值 $\\phi_{1}(x)$，我们使用给定的定义。所有特征的集合是 $\\{1, 2\\}$，所以我们计算的是 $\\phi_{1}(x)$。排除特征1的特征子集 $S$ 是 $\\{2\\}$ 的子集。因此，可能的联盟 $S$ 是空集 $\\emptyset$ 和集合 $\\{2\\}$。\n\n$\\phi_{1}(x)$ 的SHAP公式是这两个联盟贡献的总和：\n$$\n\\phi_{1}(x) = \\frac{0!(2-0-1)!}{2!}\\left(\\mathbb{E}[f(X)|X_1=x_1] - \\mathbb{E}[f(X)]\\right) + \\frac{1!(2-1-1)!}{2!}\\left(\\mathbb{E}[f(X)|X_1=x_1, X_2=x_2] - \\mathbb{E}[f(X)|X_2=x_2]\\right)\n$$\n组合权重为：\n- 对于 $S=\\emptyset$，$|S|=0$：$\\frac{0!1!}{2!} = \\frac{1 \\cdot 1}{2} = \\frac{1}{2}$。\n- 对于 $S=\\{2\\}$，$|S|=1$：$\\frac{1!0!}{2!} = \\frac{1 \\cdot 1}{2} = \\frac{1}{2}$。\n\n因此，$\\phi_{1}(x)$ 的表达式变为：\n$$\n\\phi_{1}(x) = \\frac{1}{2}\\left(\\mathbb{E}[f(X)|X_1=x_1] - \\mathbb{E}[f(X)]\\right) + \\frac{1}{2}\\left(f(x_1, x_2) - \\mathbb{E}[f(X)|X_2=x_2]\\right)\n$$\n注意，由于所有变量都已指定，$\\mathbb{E}[f(X)|X_1=x_1, X_2=x_2] = f(x_1, x_2)$。\n\n接下来，我们必须计算四个期望项。模型是 $f(X_{1},X_{2}) = g(X_{1}) + b\\,X_{2} + c$，其中 $g(X_{1}) = L\\bigl(1 - \\exp(-k\\,X_{1})\\bigr)$。特征 $X_{1}$ 和 $X_{2}$ 是独立的。$X_{1} \\sim \\mathrm{Exponential}(\\lambda)$ 且 $\\mathbb{E}[X_{2}] = \\mu_{2}$。\n\n首先，我们计算非线性分量 $g(X_{1})$ 的期望：\n$$\n\\mathbb{E}[g(X_{1})] = \\int_{0}^{\\infty} g(x_{1}) p(x_{1}) dx_{1} = \\int_{0}^{\\infty} L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr) \\lambda\\exp(-\\lambda\\,x_{1}) dx_{1}\n$$\n$$\n= L\\lambda \\int_{0}^{\\infty} \\left(\\exp(-\\lambda\\,x_{1}) - \\exp(-(k+\\lambda)x_{1})\\right) dx_{1}\n$$\n$$\n= L\\lambda \\left[ -\\frac{1}{\\lambda}\\exp(-\\lambda\\,x_{1}) - \\left(-\\frac{1}{k+\\lambda}\\exp(-(k+\\lambda)x_{1})\\right) \\right]_{0}^{\\infty}\n$$\n$$\n= L\\lambda \\left[ \\left(0 - 0\\right) - \\left(-\\frac{1}{\\lambda} + \\frac{1}{k+\\lambda}\\right) \\right] = L\\lambda \\left(\\frac{1}{\\lambda} - \\frac{1}{k+\\lambda}\\right)\n$$\n$$\n= L\\lambda \\left(\\frac{k+\\lambda-\\lambda}{\\lambda(k+\\lambda)}\\right) = L\\lambda \\left(\\frac{k}{\\lambda(k+\\lambda)}\\right) = \\frac{L k}{k+\\lambda}\n$$\n\n现在我们为SHAP公式计算这四个期望项：\n1.  $\\mathbb{E}[f(X)]$: 使用期望的线性和独立性：\n    $$\n    \\mathbb{E}[f(X)] = \\mathbb{E}[g(X_{1}) + b\\,X_{2} + c] = \\mathbb{E}[g(X_{1})] + b\\,\\mathbb{E}[X_{2}] + c = \\frac{L k}{k+\\lambda} + b\\,\\mu_{2} + c\n    $$\n2.  $\\mathbb{E}[f(X)|X_1=x_1]$: 以 $X_1=x_1$ 为条件，并使用独立性：\n    $$\n    \\mathbb{E}[f(X)|X_1=x_1] = \\mathbb{E}[g(X_{1}) + b\\,X_{2} + c|X_1=x_1] = g(x_1) + b\\,\\mathbb{E}[X_{2}] + c = g(x_1) + b\\,\\mu_{2} + c\n    $$\n3.  $\\mathbb{E}[f(X)|X_2=x_2]$: 以 $X_2=x_2$ 为条件，并使用独立性：\n    $$\n    \\mathbb{E}[f(X)|X_2=x_2] = \\mathbb{E}[g(X_{1}) + b\\,X_{2} + c|X_2=x_2] = \\mathbb{E}[g(X_1)] + b\\,x_2 + c = \\frac{L k}{k+\\lambda} + b\\,x_2 + c\n    $$\n4.  $f(x_1, x_2) = g(x_1) + b\\,x_2 + c$\n\n现在我们将这些代入 $\\phi_{1}(x)$ 表达式的两个项中。\n\n第1项（联盟 $S=\\emptyset$）：\n$$\n\\frac{1}{2}\\left(\\mathbb{E}[f(X)|X_1=x_1] - \\mathbb{E}[f(X)]\\right) = \\frac{1}{2}\\left( (g(x_1) + b\\,\\mu_{2} + c) - (\\frac{L k}{k+\\lambda} + b\\,\\mu_{2} + c) \\right)\n$$\n$$\n= \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right)\n$$\n\n第2项（联盟 $S=\\{2\\}$）：\n$$\n\\frac{1}{2}\\left(f(x_1, x_2) - \\mathbb{E}[f(X)|X_2=x_2]\\right) = \\frac{1}{2}\\left( (g(x_1) + b\\,x_2 + c) - (\\frac{L k}{k+\\lambda} + b\\,x_2 + c) \\right)\n$$\n$$\n= \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right)\n$$\n对于具有独立特征的加性模型，正如预期的那样，来自特征2的贡献（通过 $b$、$x_2$、$\\mu_2$）被抵消了。\n\n最后，我们将这两项相加得到 $\\phi_{1}(x)$：\n$$\n\\phi_{1}(x) = \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right) + \\frac{1}{2}\\left( g(x_1) - \\frac{L k}{k+\\lambda} \\right) = g(x_1) - \\frac{L k}{k+\\lambda}\n$$\n代入 $g(x_1)$ 的表达式：\n$$\n\\phi_{1}(x) = L\\bigl(1 - \\exp(-k\\,x_{1})\\bigr) - \\frac{L k}{k+\\lambda}\n$$\n这个表达式表示特征1在值 $x_1$ 处的贡献相对于其在数据分布上的平均贡献。\n\n问题还要求讨论 $\\phi_{1}(x)$ 的行为。推导出的表达式是 $\\phi_{1}(x_{1}) = g(x_{1}) - \\mathbb{E}[g(X_{1})]$。$\\phi_{1}(x_{1})$ 作为 $x_{1}$ 的函数的形状与 $g(x_{1})$ 相同，但向下垂直平移了一个常数量 $\\mathbb{E}[g(X_{1})] = \\frac{L k}{k+\\lambda}$。\n- 对于小的 $x_{1}$ ($k\\,x_{1} \\ll 1$)，我们有 $\\exp(-k\\,x_{1}) \\approx 1 - k\\,x_{1}$。所以，$g(x_{1}) \\approx L(1 - (1 - k\\,x_{1})) = L k\\,x_{1}$。在这个线性区域，$\\phi_{1}(x_{1}) \\approx L k\\,x_{1} - \\frac{L k}{k+\\lambda}$。SHAP值从 $\\phi_{1}(0) = -\\frac{L k}{k+\\lambda}$ 开始，并随 $x_{1}$ 线性增加。零剂量的SHAP值为负，表明其效果小于剂量的平均效果。\n- 对于大的 $x_{1}$ ($k\\,x_{1} \\gg 1$)，我们有 $\\exp(-k\\,x_{1}) \\to 0$。所以，$g(x_{1}) \\to L$。在这个饱和平台，$\\phi_{1}(x_{1}) \\to L - \\frac{L k}{k+\\lambda} = L\\left(1 - \\frac{k}{k+\\lambda}\\right) = \\frac{L\\lambda}{k+\\lambda}$。SHAP值渐近地趋向于一个恒定的正值，代表了剂量相对于其平均影响的最大影响。\n\n最终答案是推导出的闭式表达式。", "answer": "$$\n\\boxed{L\\left(1 - \\exp(-k\\,x_{1})\\right) - \\frac{L k}{k + \\lambda}}\n$$", "id": "3173352"}, {"introduction": "一个好的解释根植于一个好的模型，而一个好的模型依赖于高质量的特征工程。这个动手实践 [@problem_id:3173394] 将向你展示，当特征工程存在缺陷时（例如，將月份这样的周期性特征错误地编码为序数），SHAP值会如何产生误导性解释。通过对比错误的序数编码和正确的正弦/余弦编码，你将学会如何利用SHAP作为诊断工具，来验证你的特征表示是否捕捉了数据的内在结构，并确保解释的合理性。", "problem": "要求您实现一个可复现的实验，以演示当 Shapley 加性解释 (SHAP) 用于周期性特征的序数编码时会如何产生误导，以及周期性正弦-余弦编码如何恢复平衡的归因。核心对象是一个确定性模型，它将日历月份映射到一个标量输出。请使用以下规范。\n\n- 基本原理：使用合作博弈论中 Shapley 值的定义以及 SHAP (Shapley Additive Explanations) 中针对缺失特征的条件期望语义。具体来说，值函数使用给定观测特征子集的模型输出的条件期望，而一个特征的 Shapley 值是其在所有可能的子集和排列上的平均边际贡献。不要假设任何特设的快捷公式；从定义和基本对称性出发构建推理。\n\n- 数据集和目标：构建月份集合 $\\{1,2,\\dots,12\\}$。将每个月份 $m$ 与一个以弧度为单位的角度 $\\theta_m = 2\\pi m / 12$ 相关联。定义一个确定性基准真相模型\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m),\n$$\n其中 $A = 2$，$B = 1$。\n\n- 两种编码和模型：\n  1. 序数编码：将月份视为单个整型特征 $m$，并对 $m \\in \\{1,\\dots,12\\}$ 上的基准真相输出 $f(m)$ 拟合一个最小二乘线性模型 $g(m) = \\alpha m + \\beta$。在具有单个特征的 SHAP 框架中，实例 $m$ 的 Shapley 值是模型在 $m$ 处的输出与在月份经验分布下的基线期望之间的差值，即，\n  $$\n  \\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)],\n  $$\n  其中 $M$ 在 $\\{1,\\dots,12\\}$ 上均匀分布。\n  2. 周期性正弦-余弦编码：用两个特征 $s_m = \\sin(\\theta_m)$ 和 $c_m = \\cos(\\theta_m)$ 来表示每个月份 $m$，并使用线性模型 $h(s,c) = A s + B c$。对于 SHAP，使用基于月份经验均匀分布的条件期望语义。Shapley 值 $\\phi_s(m)$ 和 $\\phi_c(m)$ 应通过联盟 $\\{\\}$、$\\{s\\}$、$\\{c\\}$ 和 $\\{s,c\\}$ 的条件期望和边际贡献，利用 $\\sin$ 和 $\\cos$ 在 12 个等间隔角度上的对称性，从第一性原理计算得出。\n\n- 角度单位：所有角度必须以弧度为单位。\n\n- 测试套件：计算以下量来评估误解和归因平衡。对所有期望值的计算，均使用 $\\{1,\\dots,12\\}$ 上的均匀月份经验分布。\n  1. 跨循环边界的邻接误解检查：计算序数模型下边界处相邻月份的绝对 SHAP 差距，\n  $$\n  \\Delta_{\\text{ord}} = \\left|\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)\\right|,\n  $$\n  并与周期性模型的 SHAP 总和差距进行比较，\n  $$\n  \\Delta_{\\text{cyc}} = \\left|(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))\\right|.\n  $$\n  输出一个布尔值，指示是否 $\\Delta_{\\text{ord}} > \\Delta_{\\text{cyc}}$。\n  2. 对于具有相同正弦值但不同余弦值的月份（$m=1$ 和 $m=5$），进行等正弦值归因平衡检查：输出两个布尔值，\n     - $\\phi_s(1)$ 是否在数值容差 $10^{-12}$ 内等于 $\\phi_s(5)$，\n     - $\\phi_c(1)$ 和 $\\phi_c(5)$ 是否在数值容差 $10^{-12}$ 内大小相等、符号相反；即 $\\phi_c(1)+\\phi_c(5)$ 是否在数值上为零。\n  3. 在正弦主导的月份（$m=3$，角度 $\\theta_3 = \\pi/2$）的明确 SHAP 分量：将 $\\phi_s(3)$ 和 $\\phi_c(3)$ 作为浮点数输出。\n  4. 在余弦主导的月份（$m=6$，角度 $\\theta_6 = \\pi$）的明确 SHAP 分量：将 $\\phi_s(6)$ 和 $\\phi_c(6)$ 作为浮点数输出。\n  5. SHAP 总和一致性：对于月份 $m \\in \\{1,3,5,6,12\\}$，验证 $\\phi_s(m)+\\phi_c(m)$ 是否在容差 $10^{-12}$ 内等于 $f(m)$（回顾一下，根据对称性，基线期望 $\\mathbb{E}[f(M)]$ 为零）。输出一个布尔值，该值当且仅当此一致性对所有五个月份都成立时为真。\n  6. 邻接边界的误解比率：输出浮点数\n  $$\n  R = \\frac{\\Delta_{\\text{ord}}}{\\max(\\Delta_{\\text{cyc}}, 10^{-12})}.\n  $$\n\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序完全如下：\n```\n[adjacency_boolean, equal_sine_boolean, cos_opposite_boolean, phi_s(3), phi_c(3), phi_s(6), phi_c(6), sum_consistency_boolean, R]\n```\n所有数值输出必须是标准浮点格式，不带单位；所有布尔值必须是标准逻辑值。", "solution": "问题陈述经验证是科学上合理的、良定的、客观且完整的。它提出了一个严谨的练习，旨在应用 Shapley 加性解释 (SHAP) 的基本原理，以演示周期性变量特征工程中的一个已知问题。我们将继续提供完整的解决方案。\n\n问题的核心是计算和比较周期性特征（日历月份）的两种不同表示的 SHAP 值。基准真相模型是月份 $m \\in \\{1, 2, \\dots, 12\\}$ 的一个确定性函数：\n$$\nf(m) = A \\sin(\\theta_m) + B \\cos(\\theta_m)\n$$\n其中 $A = 2$，$B = 1$，且 $\\theta_m = 2\\pi m / 12$。月份的经验分布在 $\\{1, \\dots, 12\\}$ 上是均匀的。\n\n### 序数编码模型分析\n\n首先，我们考虑序数编码，其中月份被视为单个整型特征 $m$。使用普通最小二乘法将一个简单的线性模型 $g(m) = \\alpha m + \\beta$ 拟合到基准真相数据 $(m, f(m))$（$m=1, \\dots, 12$）。\n\n系数 $\\alpha$ 和 $\\beta$ 由以下公式给出：\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - \\bar{m})(f(m) - \\bar{f})}{\\sum_{m=1}^{12} (m - \\bar{m})^2}, \\quad \\beta = \\bar{f} - \\alpha \\bar{m}\n$$\n在这里，$\\bar{m} = \\mathbb{E}[M] = \\frac{1}{12}\\sum_{m=1}^{12} m = 6.5$。基准真相函数的平均值 $\\bar{f} = \\mathbb{E}[f(M)]$ 为：\n$$\n\\bar{f} = \\frac{1}{12}\\sum_{m=1}^{12} \\left(A \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\cos\\left(\\frac{2\\pi m}{12}\\right)\\right) = 0\n$$\n这是因为在一个等距点的完整周期内，正弦和余弦的总和为零。\n当 $\\bar{f}=0$ 时，系数简化为：\n$$\n\\alpha = \\frac{\\sum_{m=1}^{12} (m - 6.5)f(m)}{\\sum_{m=1}^{12} (m - 6.5)^2}, \\quad \\beta = -\\alpha \\bar{m} = -6.5\\alpha\n$$\n分母是一个标准和，$\\sum_{m=1}^{12} (m - 6.5)^2 = 143$。分子可以进行符号计算：\n$$\n\\sum_{m=1}^{12} m f(m) = A \\sum_{m=1}^{12} m \\sin\\left(\\frac{2\\pi m}{12}\\right) + B \\sum_{m=1}^{12} m \\cos\\left(\\frac{2\\pi m}{12}\\right)\n$$\n使用已知的有限三角函数求和恒等式，$\\sum_{k=1}^{N} k \\sin(2\\pi k/N) = -N/2 \\cot(\\pi/N)$ 和 $\\sum_{k=1}^{N} k \\cos(2\\pi k/N) = N/2$。对于 $N=12$，$A=2$ 和 $B=1$：\n$$\n\\sum_{m=1}^{12} m f(m) = 2 \\left(-6 \\cot\\left(\\frac{\\pi}{12}\\right)\\right) + 1 \\left(\\frac{12}{2}\\right) = -12(2+\\sqrt{3}) + 6 = -24 - 12\\sqrt{3} + 6 = -18 - 12\\sqrt{3}\n$$\n由于 $\\sum f(m)=0$，分子是 $\\sum (m-6.5)f(m) = \\sum m f(m) = -18 - 12\\sqrt{3}$。\n$$\n\\alpha = \\frac{-18 - 12\\sqrt{3}}{143} \\approx -0.27122\n$$\n单特征序数模型的 SHAP 值为 $\\phi_{\\text{ord}}(m) = g(m) - \\mathbb{E}[g(M)]$。\n$$\n\\mathbb{E}[g(M)] = \\mathbb{E}[\\alpha M + \\beta] = \\alpha\\mathbb{E}[M] + \\beta = \\alpha\\bar{m} + \\beta = \\alpha\\bar{m} + (\\bar{f} - \\alpha\\bar{m}) = \\bar{f} = 0\n$$\n因此，$\\phi_{\\text{ord}}(m) = g(m) = \\alpha m + \\beta$。循环边界处的邻接差距为：\n$$\n\\Delta_{\\text{ord}} = |\\phi_{\\text{ord}}(12) - \\phi_{\\text{ord}}(1)| = |(12\\alpha+\\beta) - (\\alpha+\\beta)| = |11\\alpha| = 11 \\frac{18 + 12\\sqrt{3}}{143} = \\frac{18 + 12\\sqrt{3}}{13} \\approx 2.9834\n$$\n如此大的差距反映了模型未能理解月份 12 与月份 1 是相邻的。\n\n### 周期性正弦-余弦编码模型分析\n\n接下来，我们使用双特征编码：$s_m = \\sin(\\theta_m)$ 和 $c_m = \\cos(\\theta_m)$。模型是 $h(s, c) = As + Bc$，这正是基准真相模型本身。我们从第一性原理计算实例 $(s_m, c_m)$ 的 SHAP 值 $\\phi_s(m)$ 和 $\\phi_c(m)$。\n\n特征（参与者）为 $F=\\{s, c\\}$。Shapley 值为：\n$$\n\\phi_s(m) = \\frac{1}{2}\\left(v(\\{s\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{c\\})\\right)\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}\\left(v(\\{c\\}) - v(\\emptyset)\\right) + \\frac{1}{2}\\left(v(\\{s,c\\}) - v(\\{s\\})\\right)\n$$\n值函数 $v(S)$ 是模型输出的条件期望，其中联盟 $S$ 中的特征固定为实例的值，而其他特征则在其条件分布上取平均。该分布在 12 个月上是均匀的。\n\n1.  $v(\\emptyset) = \\mathbb{E}[h(S,C)] = \\mathbb{E}[f(M)] = 0$，如前所示。\n2.  $v(\\{s,c\\}) = \\mathbb{E}[h(S,C) | s=s_m, c=c_m] = h(s_m, c_m) = A s_m + B c_m = f(m)$。\n3.  $v(\\{s\\}) = \\mathbb{E}[h(S,C) | s=s_m] = A s_m + B \\mathbb{E}[C | s=s_m]$。由于余弦函数对于具有相同正弦值的角度（即 $\\theta$ 和 $\\pi-\\theta$）具有对称性，我们对所有 $m$ 都有 $\\mathbb{E}[C | s=s_m] = 0$。因此，$v(\\{s\\}) = A s_m$。\n4.  $v(\\{c\\}) = \\mathbb{E}[h(S,C) | c=c_m] = A \\mathbb{E}[S | c=c_m] + B c_m$。由于正弦函数对于具有相同余弦值的角度（即 $\\theta$ 和 $2\\pi-\\theta$）具有对称性，我们对所有 $m$ 都有 $\\mathbb{E}[S | c=c_m] = 0$。因此，$v(\\{c\\}) = B c_m$。\n\n将这些值代入 Shapley 公式：\n$$\n\\phi_s(m) = \\frac{1}{2}(A s_m - 0) + \\frac{1}{2}((A s_m + B c_m) - B c_m) = A s_m\n$$\n$$\n\\phi_c(m) = \\frac{1}{2}(B c_m - 0) + \\frac{1}{2}((A s_m + B c_m) - A s_m) = B c_m\n$$\nSHAP 值就是线性模型的各个项：$\\phi_s(m) = 2 \\sin(\\theta_m)$ 和 $\\phi_c(m) = \\cos(\\theta_m)$。SHAP 归因的总和是 $\\phi_s(m) + \\phi_c(m) = A s_m + B c_m = f(m)$。这与 SHAP 属性 $\\sum \\phi_i = f(x) - \\mathbb{E}[f]$ 一致，因为 $\\mathbb{E}[f]=0$。\n\n周期性模型的邻接差距为：\n$$\n\\Delta_{\\text{cyc}} = |(\\phi_s(12)+\\phi_c(12)) - (\\phi_s(1)+\\phi_c(1))| = |f(12) - f(1)|\n$$\n$$\nf(12) = 2\\sin(2\\pi) + \\cos(2\\pi) = 1\n$$\n$$\nf(1) = 2\\sin(\\pi/6) + \\cos(\\pi/6) = 2(1/2) + \\sqrt{3}/2 = 1+\\sqrt{3}/2\n$$\n$$\n\\Delta_{\\text{cyc}} = |1 - (1+\\sqrt{3}/2)| = \\sqrt{3}/2 \\approx 0.8660\n$$\n\n### 测试套件评估\n\n我们现在计算所需的量。\n\n1.  **邻接误解**：$\\Delta_{\\text{ord}} \\approx 2.9834$ 且 $\\Delta_{\\text{cyc}} \\approx 0.8660$。由于 $2.9834 > 0.8660$，输出为 `True`。序数模型在年边界处产生了一个巨大的人为不连续性。\n2.  **等正弦值归因**：对于 $m=1$ ($\\theta=\\pi/6$) 和 $m=5$ ($\\theta=5\\pi/6$)，$\\sin(\\theta_1)=\\sin(\\theta_5)=1/2$。$\\phi_s(1) = A s_1 = 2(1/2) = 1$。$\\phi_s(5) = A s_5 = 2(1/2) = 1$。它们相等，所以输出为 `True`。\n3.  **相反余弦值归因**：对于 $m=1, 5$，$\\cos(\\theta_1)=\\sqrt{3}/2$ 且 $\\cos(\\theta_5)=-\\sqrt{3}/2$。$\\phi_c(1) = B c_1 = \\sqrt{3}/2$。$\\phi_c(5) = B c_5 = -\\sqrt{3}/2$。它们的和为 0，所以输出为 `True`。归因正确地反映了特征间的关系。\n4.  **$m=3$ 时的 SHAP 分量**：$\\theta_3=\\pi/2$，所以 $s_3=1, c_3=0$。$\\phi_s(3) = A s_3 = 2(1) = 2.0$。$\\phi_c(3) = B c_3 = 1(0) = 0.0$。\n5.  **$m=6$ 时的 SHAP 分量**：$\\theta_6=\\pi$，所以 $s_6=0, c_6=-1$。$\\phi_s(6) = A s_6 = 2(0) = 0.0$。$\\phi_c(6) = B c_6 = 1(-1) = -1.0$。\n6.  **SHAP 总和一致性**：如上所示，对于所有 $m$，$\\phi_s(m) + \\phi_c(m) = f(m)$。此检查将对所有指定的月份通过。输出为 `True`。\n7.  **误解比率**：\n    $$\n    R = \\frac{\\Delta_{\\text{ord}}}{\\Delta_{\\text{cyc}}} = \\frac{(18 + 12\\sqrt{3})/13}{\\sqrt{3}/2} = \\frac{2(18 + 12\\sqrt{3})}{13\\sqrt{3}} = \\frac{36\\sqrt{3} + 72}{39} = \\frac{12\\sqrt{3} + 24}{13} \\approx 3.4450\n    $$\n    序数模型的边界差距是周期性模型的 3.4 倍以上，这量化了其误解程度。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a reproducible experiment on SHAP values for cyclic features.\n    \"\"\"\n    # Define constants from the problem statement\n    A = 2.0\n    B = 1.0\n    TOL = 1e-12\n\n    # Construct the base dataset\n    months = np.arange(1, 13)\n    thetas = 2 * np.pi * months / 12\n    f_vals = A * np.sin(thetas) + B * np.cos(thetas)\n\n    # --- Ordinal Model Analysis ---\n    # Fit a least-squares linear model g(m) = alpha*m + beta\n    alpha, beta = np.polyfit(months, f_vals, 1)\n\n    # The SHAP value is phi_ord(m) = g(m) - E[g(M)].\n    # E[g(M)] = E[alpha*M + beta] = alpha*E[M] + beta.\n    # Since beta = mean(f) - alpha*mean(m) and mean(f) is numerically ~0,\n    # E[g(M)] is also numerically ~0.\n    # Therefore, phi_ord(m) is approximately g(m).\n    phi_ord_12 = alpha * 12 + beta\n    phi_ord_1 = alpha * 1 + beta\n    delta_ord = np.abs(phi_ord_12 - phi_ord_1)\n\n    # --- Cyclic Sine-Cosine Model Analysis ---\n    # Features are s_m = sin(theta_m) and c_m = cos(theta_m)\n    # The model is h(s,c) = A*s + B*c.\n    # As derived in the solution, due to feature distribution symmetries,\n    # the SHAP values are phi_s(m) = A*s_m and phi_c(m) = B*c_m.\n    s_vals = np.sin(thetas)\n    c_vals = np.cos(thetas)\n\n    phi_s = A * s_vals\n    phi_c = B * c_vals\n    \n    # Month indices for numpy arrays (0-indexed)\n    m1_idx, m3_idx, m5_idx, m6_idx, m12_idx = 0, 2, 4, 5, 11\n\n    # SHAP sum for the cyclic model\n    phi_sum_cyclic_12 = phi_s[m12_idx] + phi_c[m12_idx]\n    phi_sum_cyclic_1 = phi_s[m1_idx] + phi_c[m1_idx]\n    delta_cyc = np.abs(phi_sum_cyclic_12 - phi_sum_cyclic_1)\n\n    # --- Test Suite Evaluation ---\n\n    # 1. Adjacency misinterpretation check\n    adjacency_boolean = delta_ord > delta_cyc\n\n    # 2. Equal-sine attribution balance for m=1 and m=5\n    equal_sine_boolean = np.isclose(phi_s[m1_idx], phi_s[m5_idx], atol=TOL)\n\n    # 3. Opposite-cosine attribution balance for m=1 and m=5\n    cos_opposite_boolean = np.isclose(phi_c[m1_idx] + phi_c[m5_idx], 0, atol=TOL)\n\n    # 4. Explicit SHAP components at m=3\n    phi_s_3 = phi_s[m3_idx]\n    phi_c_3 = phi_c[m3_idx]\n\n    # 5. Explicit SHAP components at m=6\n    phi_s_6 = phi_s[m6_idx]\n    phi_c_6 = phi_c[m6_idx]\n\n    # 6. SHAP-sum consistency\n    test_months_indices = [m1_idx, m3_idx, m5_idx, m6_idx, m12_idx]\n    shap_sums = phi_s[test_months_indices] + phi_c[test_months_indices]\n    f_vals_test = f_vals[test_months_indices]\n    sum_consistency_boolean = np.all(np.isclose(shap_sums, f_vals_test, atol=TOL))\n    \n    # 7. Misinterpretation ratio\n    R = delta_ord / max(delta_cyc, 1e-12)\n\n    # Assemble final results list\n    results = [\n        adjacency_boolean,\n        equal_sine_boolean,\n        cos_opposite_boolean,\n        phi_s_3,\n        phi_c_3,\n        phi_s_6,\n        phi_c_6,\n        sum_consistency_boolean,\n        R\n    ]\n\n    # Format and print the final output\n    # Booleans are lowercased for standard Python str() conversion\n    formatted_results = [str(r).lower() if isinstance(r, bool) else f\"{r:.10f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3173394"}, {"introduction": "在现实世界的数据中，特征之间常常存在相关性（或共線性），这对模型解释提出了巨大挑战：如果两个特征几乎传递相同的信息，功劳应该如何分配？这个练习 [@problem_id:3173371] 构建了一个包含强共線性特征的合成数据集，并要求你深入TreeSHAP的内部机制，通过计算路径概率来精确分析其如何在这两个冗余特征之间分配贡献。这将帮助你理解SHAP在处理共線性问题时的行为特点及其背后的原理。", "problem": "给你一个双特征决策树模型和一个表现出强共线性的合成数据生成过程。你的任务是实现一个程序，使用 TreeSHAP（树 Shapley 加性解释）的路径概率语义来计算该树的 Shapley 加性解释 (SHAP)，并定量检验两个相关特征是否获得相等的贡献。程序必须以指定格式产生单行输出。\n\n考虑以下设置。\n\n- 数据生成过程：\n  - 有两个特征 $X_1$ 和 $X_2$。根据 $X_1 \\sim \\mathcal{N}(0,1)$，$\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 和 $X_2 = X_1 + \\epsilon$ 生成数据。这里 $\\mathcal{N}$ 表示正态分布。假设 $X_1$ 和 $\\epsilon$ 之间是独立的。\n- 模型：一个固定的深度为 2 的决策树，其结构如下：\n  - 根节点在阈值为 0 处对 $X_1$ 进行分裂。如果 $x_1 \\le 0$，则进入左子节点；否则，进入右叶节点。\n  - 左子节点在阈值为 0 处对 $X_2$ 进行分裂。如果 $x_2 \\le 0$，则进入值为 $v_{LL}$ 的左叶节点；否则，进入值为 $v_{LR}$ 的右叶节点。\n  - 根节点的右子节点是值为 $v_R$ 的叶节点。\n  - 使用 $v_{LL}=-1.0$，$v_{LR}=+1.0$ 和 $v_R=+2.0$。\n- 待解释的实例：使用固定点 $x=(x_1,x_2)=(0.2,0.25)$。\n\n使用的基本原理和定义：\n- 让来自合作博弈论的 Shapley 值定义每个特征的归因。对于 $d=2$ 个特征和联盟 $S \\subseteq \\{1,2\\}$ 的值函数 $v(S)$，特征 $i$ 的 Shapley 值是所有特征排列的平均边际贡献。不提供快捷公式；请使用基本定义。\n- 在 TreeSHAP 语义下，当分裂中使用的某个特征在联盟 $S$ 中缺失时，模型预测的评估方式是沿着两个子分支进行，并按到达该节点的随机训练样本走该分支的经验或分布概率（路径概率）进行加权。对于给定的树，这简化为使用在每次分裂时向左或向右的概率，条件是已经到达该分裂点。\n\n用作基础起点的数学事实：\n- 对于均值为零、单位方差、相关系数为 $\\rho$ 的二元正态分布，其象限概率为 $P(X \\le 0, Y \\le 0) = \\tfrac{1}{4} + \\tfrac{1}{2\\pi}\\arcsin(\\rho)$。这是一个经过充分检验的公式。\n- 对于给定的数据生成过程，$(X_1,X_2)$ 是联合正态分布，其中 $\\operatorname{Var}(X_1)=1$，$\\operatorname{Var}(X_2)=1+\\sigma^2$，且 $\\operatorname{Cov}(X_1,X_2)=1$。因此，相关系数为 $\\rho = \\dfrac{1}{\\sqrt{1+\\sigma^2}}$。\n\n你的任务：\n1) 根据第一性原理和上述定义，在 TreeSHAP 语义下，为固定点 $x=(0.2,0.25)$ 推导出联盟 $S\\in\\{\\emptyset,\\{1\\},\\{2\\},\\{1,2\\}\\}$ 的值 $v(S)$。特别地，你必须计算当 $X_1$ 或 $X_2$ 缺失时，在根节点对 $X_1$ 进行分裂和在左子节点对 $X_2$ 进行分裂时的路径概率。使用从数据生成分布中获得的确切概率（而非经验抽样）。\n2) 使用 $d=2$ 个特征的 Shapley 定义，计算固定点 $x$ 的每个特征的归因 $\\phi_1$ 和 $\\phi_2$。\n3) 通过改变 $\\sigma$ 来研究共线性对 TreeSHAP 路径概率的影响，并确定 SHAP 是否为 $X_1$ 和 $X_2$ 分配了相等的贡献。对每个 $\\sigma$ 值，量化差异 $\\phi_1 - \\phi_2$。\n\n测试集：\n- 使用以下 $\\sigma$ 值集合：$\\{0.0,\\;0.1,\\;0.5,\\;1.0,\\;3.0\\}$。\n\n要求输出：\n- 对于测试集中的每个 $\\sigma$，计算 Shapley 值并返回一个列表 $[\\phi_1,\\phi_2,\\phi_1-\\phi_2]$，其中每个值都四舍五入到 6 位小数。\n- 你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是对应一个 $\\sigma$ 的列表，顺序与给定顺序相同。例如，一个有效的输出格式是 $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$，其中的数值条目四舍五入到 6 位小数。\n\n不涉及物理单位。角度、百分比或其他单位约定在此不适用。所有数值量必须是纯实数。最终结果必须完全由给定的定义和指定的测试集确定。解决方案不得使用任何外部数据或随机性，也不得需要用户输入。", "solution": "## 问题验证\n\n### 步骤 1：提取已知条件\n- **数据生成过程**：\n    - 两个特征，$X_1$ 和 $X_2$。\n    - $X_1 \\sim \\mathcal{N}(0,1)$。\n    - $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$。\n    - $X_2 = X_1 + \\epsilon$。\n    - $X_1$ 和 $\\epsilon$ 相互独立。\n- **模型**：一个固定的深度为 2 的决策树。\n    - 根节点：在阈值为 0 处对 $X_1$ 进行分裂。如果 $x_1 \\le 0$ 则向左，如果 $x_1 > 0$ 则向右。\n    - 根节点的右子节点：值为 $v_R = +2.0$ 的叶节点。\n    - 根节点的左子节点：在阈值为 0 处对 $X_2$ 进行分裂。如果 $x_2 \\le 0$ 则向左，如果 $x_2 > 0$ 则向右。\n    - 左子节点的左叶节点：值为 $v_{LL} = -1.0$。\n    - 左子节点的右叶节点：值为 $v_{LR} = +1.0$。\n- **待解释的实例**：$x=(x_1,x_2)=(0.2,0.25)$。\n- **基本定义**：\n    - 对于 $d=2$ 个特征，特征 $i$ 的 Shapley 值：定义为所有排列上的平均边际贡献。\n    - TreeSHAP 语义：当分裂中使用的某个特征在联盟 $S$ 中缺失时，沿着两个子分支进行，并按到达该节点的随机样本走该分支的路径概率进行加权。使用来自数据生成分布的确切概率。\n- **数学事实**：\n    - 对于均值为零、单位方差、相关系数为 $\\rho$ 的二元正态分布 $(X,Y)$，其象限概率为 $P(X \\le 0, Y \\le 0) = \\tfrac{1}{4} + \\tfrac{1}{2\\pi}\\arcsin(\\rho)$。\n    - 对于给定的过程，$(X_1, X_2)$ 是联合正态分布，其中 $\\operatorname{Var}(X_1)=1$，$\\operatorname{Var}(X_2)=1+\\sigma^2$，$\\operatorname{Cov}(X_1,X_2)=1$，相关系数为 $\\rho = \\dfrac{1}{\\sqrt{1+\\sigma^2}}$。\n- **测试集**：$\\sigma \\in \\{0.0, 0.1, 0.5, 1.0, 3.0\\}$。\n- **要求输出**：对于每个 $\\sigma$，一个列表 $[\\phi_1, \\phi_2, \\phi_1-\\phi_2]$，四舍五入到 6 位小数。最终输出是这些列表的列表。\n\n### 步骤 2：使用提取的已知条件进行验证\n根据验证标准对问题进行评估：\n- **科学依据**：该问题植根于合作博弈论（Shapley 值）的既定理论及其在机器学习可解释性（SHAP）中的应用。数据生成过程和模型是合成的，但在数学上是明确定义的。所有组成部分都与标准统计和概率论一致。\n- **适定性**：该问题是适定的。它提供了所有必要的信息：一个特定的模型、一个特定的数据分布、一个待解释的特定实例，以及用于计算所需量的明确定义。这些任务导向一个唯一的、可计算的解。\n- **客观性**：该问题以精确、客观的数学语言陈述。它没有歧义、主观性或基于意见的主张。\n\n该问题没有任何无效性缺陷。它是在统计学习这一特定主题内的一个严谨、自洽的理论练习。\n\n### 步骤 3：结论与行动\n该问题是 **有效的**。将提供一个完整的解决方案。\n\n## 解题推导\n\n两个特征 $\\phi_1$ 和 $\\phi_2$ 的 Shapley 值定义如下：\n$$ \\phi_1 = \\frac{1}{2}\\left[ (v(\\{1\\}) - v(\\emptyset)) + (v(\\{1,2\\}) - v(\\{2\\})) \\right] $$\n$$ \\phi_2 = \\frac{1}{2}\\left[ (v(\\{2\\}) - v(\\emptyset)) + (v(\\{1,2\\}) - v(\\{1\\})) \\right] $$\n这里，$v(S)$ 是联盟 $S \\subseteq \\{1, 2\\}$ 的值函数，表示在给定 $S$ 中特征值的情况下模型的期望输出。我们必须计算所有四个联盟：$\\emptyset$、$\\{1\\}$、$\\{2\\}$ 和 $\\{1,2\\}$ 的 $v(S)$。待解释的实例是 $x = (x_1, x_2) = (0.2, 0.25)$。设模型表示为 $f(x_1, x_2)$。\n\n### 1. 联盟值 $v(S)$ 的计算\n\n- **$v(\\{1,2\\})$**：两个特征都已知。我们将实例 $x=(0.2, 0.25)$ 输入决策树。\n  - 根节点对 $X_1$ 进行分裂。因为 $x_1 = 0.2 > 0$，我们进入右子节点。\n  - 右子节点是一个值为 $v_R = 2.0$ 的叶节点。\n  - 因此，$v(\\{1,2\\}) = f(0.2, 0.25) = 2.0$。\n\n- **$v(\\{1\\})$**：特征 $X_1$ 已知 ($x_1=0.2$)，而 $X_2$ 未知。根据 TreeSHAP 语义，我们通过对 $X_2$ 的分布求平均来评估模型。\n  - 根节点对 $X_1$ 进行分裂。因为 $x_1 = 0.2 > 0$，路径固定走向右子节点。\n  - 模型输出为 $v_R = 2.0$，与 $X_2$ 的值无关。对 $X_2$ 的分裂不在这条路径上。\n  - 因此，期望的计算很简单：$v(\\{1\\}) = E[f(0.2, X_2)] = 2.0$。\n\n- **$v(\\{2\\})$**：特征 $X_2$ 已知 ($x_2=0.25$)，而 $X_1$ 未知。我们对 $X_1$ 的分布求平均。\n  - 根节点对缺失的 $X_1$ 进行分裂。根据指定的“路径概率语义”，我们沿着两个分支前进，并按边际概率加权。\n  - 路径 1（左）：以概率 $P(X_1 \\le 0) = 0.5$ 进入左子树。\n  - 路径 2（右）：以概率 $P(X_1 > 0) = 0.5$ 进入右叶节点。输出为 $v_R = 2.0$。\n  - 在左子树中，模型对 $X_2$ 进行分裂。我们知道 $x_2=0.25$。因为 $0.25 > 0$，我们进入该子树的右叶节点，其值为 $v_{LR}=1.0$。\n  - 结合这些路径，期望输出为：\n    $$ v(\\{2\\}) = P(X_1 \\le 0) \\cdot v_{LR} + P(X_1 > 0) \\cdot v_R = 0.5 \\cdot (1.0) + 0.5 \\cdot (2.0) = 0.5 + 1.0 = 1.5 $$\n  - 注意，这个值与相关性参数 $\\sigma$ 无关。\n\n- **$v(\\emptyset)$**：两个特征都未知。这是模型在整个数据分布上的基线期望输出。\n  - $v(\\emptyset) = E[f(X_1, X_2)] = P(X_1 > 0) \\cdot v_R + P(X_1 \\le 0, X_2 \\le 0) \\cdot v_{LL} + P(X_1 \\le 0, X_2 > 0) \\cdot v_{LR}$。\n  - 我们有 $P(X_1 > 0) = 0.5$。叶节点的值为 $v_R=2.0$，$v_{LL}=-1.0$，$v_{LR}=1.0$。\n  - 我们使用提供的二元正态象限概率公式：$P(X_1 \\le 0, X_2 \\le 0) = \\frac{1}{4} + \\frac{1}{2\\pi}\\arcsin(\\rho)$，其中 $\\rho = \\frac{1}{\\sqrt{1+\\sigma^2}}$。\n  - 另一个叶节点的概率是 $P(X_1 \\le 0, X_2 > 0) = P(X_1 \\le 0) - P(X_1 \\le 0, X_2 \\le 0) = 0.5 - (\\frac{1}{4} + \\frac{\\arcsin(\\rho)}{2\\pi}) = \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi}$。\n  - 将这些代入期望公式：\n    $$ v(\\emptyset) = 0.5 \\cdot (2.0) + \\left(\\frac{1}{4} + \\frac{\\arcsin(\\rho)}{2\\pi}\\right)(-1.0) + \\left(\\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi}\\right)(1.0) $$\n    $$ v(\\emptyset) = 1.0 - \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi} + \\frac{1}{4} - \\frac{\\arcsin(\\rho)}{2\\pi} $$\n    $$ v(\\emptyset) = 1.0 - \\frac{2\\arcsin(\\rho)}{2\\pi} = 1.0 - \\frac{\\arcsin(\\rho)}{\\pi} $$\n\n### 2. Shapley 值 $\\phi_1$ 和 $\\phi_2$ 的计算\n\n现在我们将联盟值代入 Shapley 公式。\n- 对于 $\\phi_1$：\n  - 在给定 $\\emptyset$ 的条件下 $\\{1\\}$ 的边际贡献：$v(\\{1\\}) - v(\\emptyset) = 2.0 - \\left(1.0 - \\frac{\\arcsin(\\rho)}{\\pi}\\right) = 1.0 + \\frac{\\arcsin(\\rho)}{\\pi}$。\n  - 在给定 $\\{2\\}$ 的条件下 $\\{1\\}$ 的边际贡献：$v(\\{1,2\\}) - v(\\{2\\}) = 2.0 - 1.5 = 0.5$。\n  - 平均边际贡献：\n    $$ \\phi_1 = \\frac{1}{2} \\left[ \\left(1.0 + \\frac{\\arcsin(\\rho)}{\\pi}\\right) + 0.5 \\right] = \\frac{1}{2} \\left[ 1.5 + \\frac{\\arcsin(\\rho)}{\\pi} \\right] = 0.75 + \\frac{\\arcsin(\\rho)}{2\\pi} $$\n\n- 对于 $\\phi_2$：\n  - 在给定 $\\emptyset$ 的条件下 $\\{2\\}$ 的边际贡献：$v(\\{2\\}) - v(\\emptyset) = 1.5 - \\left(1.0 - \\frac{\\arcsin(\\rho)}{\\pi}\\right) = 0.5 + \\frac{\\arcsin(\\rho)}{\\pi}$。\n  - 在给定 $\\{1\\}$ 的条件下 $\\{2\\}$ 的边际贡献：$v(\\{1,2\\}) - v(\\{1\\}) = 2.0 - 2.0 = 0.0$。\n  - 平均边际贡献：\n    $$ \\phi_2 = \\frac{1}{2} \\left[ \\left(0.5 + \\frac{\\arcsin(\\rho)}{\\pi}\\right) + 0.0 \\right] = \\frac{1}{2} \\left[ 0.5 + \\frac{\\arcsin(\\rho)}{\\pi} \\right] = 0.25 + \\frac{\\arcsin(\\rho)}{2\\pi} $$\n\n### 3. 共线性研究\n问题要求量化差异 $\\phi_1 - \\phi_2$。\n$$ \\phi_1 - \\phi_2 = \\left(0.75 + \\frac{\\arcsin(\\rho)}{2\\pi}\\right) - \\left(0.25 + \\frac{\\arcsin(\\rho)}{2\\pi}\\right) = 0.5 $$\n此结果表明，对于给定的模型结构和待解释的实例，特征归因的差异是恒定的，不依赖于共线性的程度 $\\sigma$。由 $\\rho=1/\\sqrt{1+\\sigma^2}$ 表示的共线性确实会影响基线值 $v(\\emptyset)$，从而影响单个的 SHAP 值 $\\phi_1$ 和 $\\phi_2$，但它对两者的影响是相同的，导致在它们的差值中被抵消。归因的不相等性（$\\phi_1 > \\phi_2$）源于树的结构和实例的特定特征值；对于 $x_1=0.2$，模型的输出仅由 $X_1$ 决定，这使得它对这个特定预测更具影响力。\n\n现在将实现程序，为指定的 $\\sigma$ 值测试集计算这些值。\n- 对于每个 $\\sigma$，我们计算 $\\rho = 1/\\sqrt{1+\\sigma^2}$。对于 $\\sigma=0$，$\\rho=1$。\n- 然后我们计算 $\\phi_1 = 0.75 + \\arcsin(\\rho)/(2\\pi)$ 和 $\\phi_2 = 0.25 + \\arcsin(\\rho)/(2\\pi)$。\n- 差值固定为 $\\phi_1 - \\phi_2 = 0.5$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes SHAP values for a two-feature decision tree model with collinear data.\n    \"\"\"\n\n    # Define the test cases for sigma from the problem statement.\n    test_cases_sigma = [0.0, 0.1, 0.5, 1.0, 3.0]\n\n    all_results = []\n    \n    for sigma in test_cases_sigma:\n        # Step 1: Calculate the correlation rho based on sigma.\n        # The relationship is rho = 1 / sqrt(1 + sigma^2).\n        if sigma == 0.0:\n            rho = 1.0\n        else:\n            rho = 1.0 / np.sqrt(1.0 + sigma**2)\n\n        # Step 2: Calculate arcsin(rho).\n        arcsin_rho = np.arcsin(rho)\n        \n        # Step 3: Compute phi_1 and phi_2 using the derived formulas.\n        # phi_1 = 0.75 + arcsin(rho) / (2 * pi)\n        # phi_2 = 0.25 + arcsin(rho) / (2 * pi)\n        phi_1 = 0.75 + arcsin_rho / (2.0 * np.pi)\n        phi_2 = 0.25 + arcsin_rho / (2.0 * np.pi)\n        \n        # Step 4: Compute the difference phi_1 - phi_2.\n        # As derived, this difference is a constant 0.5.\n        diff = phi_1 - phi_2\n        \n        # Store the results for this sigma value.\n        all_results.append([phi_1, phi_2, diff])\n\n    # Step 5: Format the output string as specified.\n    # The output should be a single line: [[a1,b1,c1],[a2,b2,c2],...]\n    # Each number must be rounded to 6 decimal places.\n    result_strings = []\n    for res_list in all_results:\n        # Format the numbers in the inner list as strings with 6 decimal places.\n        s_list = [f\"{val:.6f}\" for val in res_list]\n        # Join them into a string representation of a list: \"[v1,v2,v3]\"\n        result_strings.append(f\"[{','.join(s_list)}]\")\n    \n    # Join the string representations of inner lists with commas.\n    final_output_str = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3173371"}]}