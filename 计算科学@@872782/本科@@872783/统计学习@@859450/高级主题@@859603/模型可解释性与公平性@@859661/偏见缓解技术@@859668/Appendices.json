{"hands_on_practices": [{"introduction": "在处理群体不均衡的数据时，一个常见的问题是模型可能会偏向于多数群体。处理这种偏差的一种基本方法是在训练过程中对不同群体进行加权。本练习将通过一个仅有截距项的逻辑回归模型，引导您从第一性原理出发，推导加权如何影响模型的预测，并计算一个校准项以修正这种影响，从而加深对预处理或过程中缓解偏差技术的理解。[@problem_id:3105417]", "problem": "考虑一个在划分为两个组 $g \\in \\{\\mathrm{A}, \\mathrm{B}\\}$ 的总体中观察到的二元结果 $y \\in \\{0,1\\}$。总体比例为 $q_{\\mathrm{A}} = 0.7$ 和 $q_{\\mathrm{B}} = 0.3$，各组的事件发生率为 $\\pi_{\\mathrm{A}} = 0.12$ 和 $\\pi_{\\mathrm{B}} = 0.27$。总体发生率为 $\\pi = q_{\\mathrm{A}} \\pi_{\\mathrm{A}} + q_{\\mathrm{B}} \\pi_{\\mathrm{B}}$。你使用加权最大似然法训练一个仅含截距的逻辑回归模型 $p = \\sigma(b)$，其中 $\\sigma(b) = 1/(1+\\exp(-b))$，并使用组权重 $\\alpha_{\\mathrm{A}} = \\frac{1}{q_{\\mathrm{A}}}$ 和 $\\alpha_{\\mathrm{B}} = \\frac{1}{q_{\\mathrm{B}}}$ 来平衡各组的损失贡献。\n\n从伯努利负对数似然及其加权版本的定义出发，推导在加权训练下以加权发生率 $\\pi_{\\mathrm{w}}$ 表示的最优截距 $b_{\\mathrm{w}}$，然后推导一个闭式截距调整量 $\\delta$，使得调整后的截距 $b_{\\mathrm{w}} + \\delta$ 在未加权总体上实现边际校准，定义为 $\\sigma(b_{\\mathrm{w}} + \\delta) = \\pi$。计算给定 $q_{\\mathrm{A}}, q_{\\mathrm{B}}, \\pi_{\\mathrm{A}}, \\pi_{\\mathrm{B}}$ 和权重 $\\alpha_{\\mathrm{A}}, \\alpha_{\\mathrm{B}}$ 时 $\\delta$ 的数值。将你的最终数值答案四舍五入到四位有效数字。", "solution": "所述问题具有科学依据，提法恰当且客观。所有必要的数据和定义均已提供，不存在内部矛盾或违反数学或统计学原理的情况。因此，我们将进行完整解答。\n\n该问题要求推导逻辑回归模型的截距调整量。设模型预测的概率为 $p = \\sigma(b)$，其中 $\\sigma(b) = \\frac{1}{1 + \\exp(-b)}$ 是 sigmoid 函数，$b$ 是模型的截距。\n\n该模型使用加权最大似然法进行训练。对于一个观测数据集 $\\{y_i, g_i\\}$，加权负对数似然（交叉熵损失）由下式给出：\n$$ \\mathcal{L}_{\\mathrm{w}}(b) = - \\sum_{i} \\alpha_{g_i} [y_i \\ln(\\sigma(b)) + (1-y_i)\\ln(1-\\sigma(b))] $$\n其中 $\\alpha_{g_i}$ 是来自组 $g_i$ 的观测值 $i$ 的权重。为求最优截距 $b_{\\mathrm{w}}$，我们将正的加权对数似然对 $b$ 求导，并令结果为零。单个观测值的对数似然项的导数是：\n$$ \\frac{d}{db} [y_i \\ln(\\sigma(b)) + (1-y_i)\\ln(1-\\sigma(b))] = y_i\\frac{\\sigma'(b)}{\\sigma(b)} + (1-y_i)\\frac{-\\sigma'(b)}{1-\\sigma(b)} $$\n使用恒等式 $\\sigma'(b) = \\sigma(b)(1-\\sigma(b))$，上式可简化为：\n$$ y_i(1-\\sigma(b)) - (1-y_i)\\sigma(b) = y_i - \\sigma(b) $$\n因此，总加权对数似然的导数是：\n$$ \\frac{d}{db} \\sum_{i} \\alpha_{g_i} [y_i \\ln(\\sigma(b)) + (1-y_i)\\ln(1-\\sigma(b))] = \\sum_{i} \\alpha_{g_i} [y_i - \\sigma(b)] $$\n将其设为零以求得最优截距 $b_{\\mathrm{w}}$：\n$$ \\sum_{i} \\alpha_{g_i} (y_i - \\sigma(b_{\\mathrm{w}})) = 0 \\implies \\sum_{i} \\alpha_{g_i} y_i = \\sigma(b_{\\mathrm{w}}) \\sum_{i} \\alpha_{g_i} $$\n这得出了模型预测的解：\n$$ \\sigma(b_{\\mathrm{w}}) = \\frac{\\sum_{i} \\alpha_{g_i} y_i}{\\sum_{i} \\alpha_{g_i}} $$\n右侧的表达式是结果的加权平均值，我们将其表示为加权发生率 $\\pi_{\\mathrm{w}}$。用基于总体分布的期望来表示，即为：\n$$ \\pi_{\\mathrm{w}} = \\frac{E[\\alpha_g y]}{E[\\alpha_g]} = \\frac{\\sum_{g \\in \\{\\mathrm{A}, \\mathrm{B}\\}} P(g) \\alpha_g E[y|g]}{\\sum_{g \\in \\{\\mathrm{A}, \\mathrm{B}\\}} P(g) \\alpha_g} = \\frac{q_{\\mathrm{A}} \\alpha_{\\mathrm{A}} \\pi_{\\mathrm{A}} + q_{\\mathrm{B}} \\alpha_{\\mathrm{B}} \\pi_{\\mathrm{B}}}{q_{\\mathrm{A}} \\alpha_{\\mathrm{A}} + q_{\\mathrm{B}} \\alpha_{\\mathrm{B}}} $$\n给定权重 $\\alpha_{\\mathrm{A}} = \\frac{1}{q_{\\mathrm{A}}}$ 和 $\\alpha_{\\mathrm{B}} = \\frac{1}{q_{\\mathrm{B}}}$，我们将其代入 $\\pi_{\\mathrm{w}}$ 的表达式中：\n$$ \\pi_{\\mathrm{w}} = \\frac{q_{\\mathrm{A}} (\\frac{1}{q_{\\mathrm{A}}}) \\pi_{\\mathrm{A}} + q_{\\mathrm{B}} (\\frac{1}{q_{\\mathrm{B}}}) \\pi_{\\mathrm{B}}}{q_{\\mathrm{A}} (\\frac{1}{q_{\\mathrm{A}}}) + q_{\\mathrm{B}} (\\frac{1}{q_{\\mathrm{B}}})} = \\frac{\\pi_{\\mathrm{A}} + \\pi_{\\mathrm{B}}}{1+1} = \\frac{\\pi_{\\mathrm{A}} + \\pi_{\\mathrm{B}}}{2} $$\n通过加权训练得到的最优截距 $b_{\\mathrm{w}}$ 是这个加权发生率的 logit 值：\n$$ b_{\\mathrm{w}} = \\sigma^{-1}(\\pi_{\\mathrm{w}}) = \\ln\\left(\\frac{\\pi_{\\mathrm{w}}}{1-\\pi_{\\mathrm{w}}}\\right) $$\n该问题要求一个调整量 $\\delta$ 以在未加权总体上实现边际校准。该条件表述为 $\\sigma(b_{\\mathrm{w}} + \\delta) = \\pi$，其中 $\\pi$ 是总体发生率。总体发生率由全概率定律给出：\n$$ \\pi = q_{\\mathrm{A}} \\pi_{\\mathrm{A}} + q_{\\mathrm{B}} \\pi_{\\mathrm{B}} $$\n根据校准条件，我们可以解出 $b_{\\mathrm{w}} + \\delta$：\n$$ b_{\\mathrm{w}} + \\delta = \\sigma^{-1}(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) $$\n调整量 $\\delta$ 是目标 logit 与学习到的 logit 之间的差值：\n$$ \\delta = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) - b_{\\mathrm{w}} = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) - \\ln\\left(\\frac{\\pi_{\\mathrm{w}}}{1-\\pi_{\\mathrm{w}}}\\right) $$\n利用对数的商法则，我们得到 $\\delta$ 的闭式表达式：\n$$ \\delta = \\ln\\left(\\frac{\\pi(1-\\pi_{\\mathrm{w}})}{\\pi_{\\mathrm{w}}(1-\\pi)}\\right) $$\n现在，我们使用给定的值计算 $\\delta$ 的数值：$q_{\\mathrm{A}} = 0.7$，$q_{\\mathrm{B}} = 0.3$，$\\pi_{\\mathrm{A}} = 0.12$ 和 $\\pi_{\\mathrm{B}} = 0.27$。\n\n首先，我们计算总体发生率 $\\pi$：\n$$ \\pi = (0.7)(0.12) + (0.3)(0.27) = 0.084 + 0.081 = 0.165 $$\n接下来，我们计算加权发生率 $\\pi_{\\mathrm{w}}$：\n$$ \\pi_{\\mathrm{w}} = \\frac{0.12 + 0.27}{2} = \\frac{0.39}{2} = 0.195 $$\n最后，我们将这些值代入 $\\delta$ 的表达式中：\n$$ \\delta = \\ln\\left(\\frac{0.165(1-0.195)}{0.195(1-0.165)}\\right) = \\ln\\left(\\frac{0.165 \\times 0.805}{0.195 \\times 0.835}\\right) $$\n$$ \\delta = \\ln\\left(\\frac{0.132825}{0.162825}\\right) \\approx \\ln(0.81576481...) $$\n$$ \\delta \\approx -0.203581... $$\n将结果四舍五入到四位有效数字，得到 $\\delta = -0.2036$。", "answer": "$$\\boxed{-0.2036}$$", "id": "3105417"}, {"introduction": "在梯度下降的优化过程中，来自某个特定群体的梯度贡献可能过大，导致模型参数的更新主要由该群体主导，从而损害了公平性。本练习介绍一种称为“公平梯度裁剪”的过程中缓解技术，它通过在每次迭代中限制不同群体梯度贡献的相对大小来动态平衡它们的影响。您将通过编程实现这一算法，并量化分析在不同裁剪强度下，模型公平性与稳定性和整体性能之间的权衡关系。[@problem_id:3105436]", "problem": "您的任务是为二元逻辑回归形式化并实现一种偏差缓解技术，称为公平梯度裁剪（fair gradient clipping），该技术在训练期间限制特定群体的梯度贡献的主导地位。您将从基本的经验风险最小化原则和逻辑损失的定义出发，推导梯度和一个裁剪规则，该规则强制群体贡献之间存在有界比率，然后使用此裁剪规则实现全批量梯度下降过程。您的程序必须计算量化指标，以揭示作为单个超参数函数的偏差与稳定性之间的权衡。\n\n考虑一个二元分类数据集，其中包含一个敏感属性，用于指示两个组 $A \\in \\{0,1\\}$。设数据为 $\\{(x_i, y_i, a_i)\\}_{i=1}^N$，其中每个 $x_i \\in \\mathbb{R}^d$（包含一个显式的偏置坐标作为值为 $1$ 的额外特征），$y_i \\in \\{0,1\\}$，以及 $a_i \\in \\{0,1\\}$。设模型为 $f_\\theta(x) = \\sigma(\\theta^\\top x)$，其参数向量为 $\\theta \\in \\mathbb{R}^d$，逻辑链接函数为 $\\sigma(z) = 1/(1 + e^{-z})$。带有 $\\ell_2$ 正则化的经验风险为\n$$\n\\mathcal{L}(\\theta) \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\Big( -y_i \\log \\sigma(\\theta^\\top x_i) \\;-\\; (1-y_i)\\log\\!\\big(1 - \\sigma(\\theta^\\top x_i)\\big) \\Big) \\;+\\; \\frac{\\lambda}{2}\\|\\theta\\|_2^2.\n$$\n根据逻辑损失的定义和链式法则，每个样本的梯度是向量\n$$\ng_i(\\theta) \\;=\\; \\big(\\sigma(\\theta^\\top x_i) - y_i\\big)\\, x_i.\n$$\n设 $G_a$ 表示 $a_i = a$ 的样本的索引集，$n_a = |G_a|$，因此 $n_0 + n_1 = N$。将组 $a$ 内的平均梯度定义为\n$$\ng_a(\\theta) \\;=\\; \\frac{1}{n_a}\\sum_{i \\in G_a} g_i(\\theta),\n$$\n并定义按组加权的贡献向量\n$$\nc_a(\\theta) \\;=\\; \\frac{n_a}{N}\\, g_a(\\theta), \\quad a \\in \\{0,1\\}.\n$$\n未裁剪的数据梯度为 $c_0(\\theta) + c_1(\\theta)$，包含正则化的完整梯度为 $c_0(\\theta) + c_1(\\theta) + \\lambda \\theta$。\n\n公平梯度裁剪通过限制两个组贡献向量的欧几里得范数之比来施加一个主导上限。具体来说，对于给定的比率上限 $\\rho \\in [1,\\infty)$，在每个更新步骤中，您必须减小 $\\|c_0(\\theta)\\|_2$ 和 $\\|c_1(\\theta)\\|_2$ 中较大者的范数（不改变其方向），使得缩放后，较大范数不超过较小范数的 $\\rho$ 倍；较小的贡献保持不变。如果没有任何一个组的主导程度超过因子 $\\rho$，则不应用缩放。用于更新的数据梯度是两个（可能经过缩放的）贡献向量之和。然后执行全批量梯度下降更新。\n\n从上述定义出发，推导出一个满足主导上限、同时保持方向并使非主导组保持不变的缩放规则。实现以下训练和评估协议。\n\n1) 固定的数据集和设置。使用 $d=3$，其中偏置坐标作为第三个分量包含在 $x_i$ 中且等于 $1$。设 $N=16$，包含以下样本：\n- 组 $A=0$ ($n_0 = 12$)：六个正样本，其中 $x = (2,2,1)$, $y=1$；六个负样本，其中 $x = (-2,-2,1)$, $y=0$。\n- 组 $A=1$ ($n_1 = 4$)：两个正样本，其中 $x = (-2,2,1)$, $y=1$；两个负样本，其中 $x = (2,-2,1)$, $y=0$。\n\n2) 训练常数。使用全批量梯度下降，学习率 $\\eta = 0.2$，迭代次数 $T = 200$，正则化系数 $\\lambda = 0.01$。初始化 $\\theta_0 = 0 \\in \\mathbb{R}^3$。在每次迭代中，根据当前的 $\\theta_t$ 计算组贡献，使用指定的 $\\rho$ 应用公平梯度裁剪，添加正则化项，并通过 $\\theta_{t+1} = \\theta_t - \\eta \\,\\tilde{g}_t$ 更新 $\\theta$，其中 $\\tilde{g}_t$ 是裁剪后的数据梯度加上正则化梯度。在需要时使用一个小的数值常数 $\\varepsilon = 10^{-12}$ 以避免除以零。\n\n3) 训练后报告的指标：\n- 总体逻辑损失：所有 $N$ 个样本上未正则化逻辑损失的平均值，\n$$\n\\bar{\\ell} \\;=\\; \\frac{1}{N}\\sum_{i=1}^N \\Big( -y_i \\log \\sigma(\\theta^\\top x_i) - (1-y_i)\\log(1-\\sigma(\\theta^\\top x_i)) \\Big).\n$$\n- 公平性差距：两个组之间未正则化逻辑损失平均值的绝对差，\n$$\n\\Delta_{\\text{loss}} \\;=\\; \\big| \\; \\frac{1}{n_0}\\sum_{i \\in G_0} \\ell_i \\;-\\; \\frac{1}{n_1}\\sum_{i \\in G_1} \\ell_i \\; \\big|.\n$$\n- 稳定性失真：未裁剪数据梯度与裁剪后数据梯度之间相对失真的迭代平均值，定义为\n$$\nD \\;=\\; \\frac{1}{T} \\sum_{t=1}^{T} \\frac{\\big\\| \\,(c_0(\\theta_t)+c_1(\\theta_t)) \\;-\\; (\\tilde{c}_0(\\theta_t)+\\tilde{c}_1(\\theta_t)) \\,\\big\\|_2}{\\big\\|\\, c_0(\\theta_t)+c_1(\\theta_t) \\,\\big\\|_2 + \\varepsilon}.\n$$\n\n4) 测试套件。对以下四个主导上限 $\\rho$ 的值运行上述过程：\n- 情况1：$\\rho = 1.0$。\n- 情况2：$\\rho = 1.5$。\n- 情况3：$\\rho = 3.0$。\n- 情况4：$\\rho = 10^9$（这近似于不进行裁剪）。\n\n5) 程序输出。您的程序必须生成单行，其中包含四个结果的列表，每个测试用例一个，每个结果是列表 $[\\bar{\\ell}, \\Delta_{\\text{loss}}, D]$，其中每个值都四舍五入到 $6$ 位小数。要求的输出格式是单行形式\n$[[\\bar{\\ell}_1,\\Delta_{\\text{loss},1},D_1],[\\bar{\\ell}_2,\\Delta_{\\text{loss},2},D_2],[\\bar{\\ell}_3,\\Delta_{\\text{loss},3},D_3],[\\bar{\\ell}_4,\\Delta_{\\text{loss},4},D_4]]$。\n\n6) 实现约束。代码必须完全自包含，无任何输入，并且必须实现您从主导上限定义中推导出的基于推导的裁剪规则。所有计算都必须以浮点数进行，并仔细处理任何除法或对数中的 $\\varepsilon$ 以避免未定义的值。不涉及物理单位。不涉及角度。任何时候您必须表达一个分数，在最终输出中它必须如上所述表示为十进制数。", "solution": "用户提供的问题是有效的。它提出了一个在计算统计学和机器学习领域中明确定义的任务，该任务基于逻辑回归、基于梯度的优化和算法公平性的既定原则。所有数据、常数和程序步骤都以足够的精度被指定，从而允许一个唯一的、可验证的解决方案。该问题是自包含的、科学上合理的和客观的。\n\n### 1. 基于原则的设计：推导和算法\n\n该问题的核心是在逻辑回归的标准梯度下降过程中实现一个公平梯度裁剪机制。这要求形式化裁剪规则并将其集成到迭代优化算法中。\n\n#### 1.1. 模型和梯度定义\n\n模型是一个逻辑回归器，$f_\\theta(x) = \\sigma(\\theta^\\top x)$，其中 $\\sigma(z) = (1 + e^{-z})^{-1}$ 是 sigmoid 函数。参数 $\\theta \\in \\mathbb{R}^d$ 通过最小化经验风险来优化，该风险是所有 $N$ 个数据点的二元交叉熵损失之和，外加一个 $\\ell_2$ 正则化项：\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\ell_i(\\theta) + \\frac{\\lambda}{2}\\|\\theta\\|_2^2, \\quad \\text{其中} \\quad \\ell_i(\\theta) = -y_i \\log \\sigma(\\theta^\\top x_i) - (1-y_i)\\log(1 - \\sigma(\\theta^\\top x_i))。\n$$\n该损失函数的梯度是数据项和正则化项贡献之和。数据项的梯度可以按组分解。每个样本的梯度由 $g_i(\\theta) = (\\sigma(\\theta^\\top x_i) - y_i) x_i$ 给出。对于组 $a \\in \\{0,1\\}$，按组加权的贡献向量定义为 $c_a(\\theta) = \\frac{n_a}{N} g_a(\\theta) = \\frac{1}{N} \\sum_{i \\in G_a} g_i(\\theta)$，其中 $G_a$ 是组 $a$ 中数据点的索引集。总数据梯度为 $g_{\\text{data}}(\\theta) = c_0(\\theta) + c_1(\\theta)$，完整梯度为 $g(\\theta) = c_0(\\theta) + c_1(\\theta) + \\lambda\\theta$。\n\n#### 1.2. 公平梯度裁剪规则的推导\n\n裁剪规则旨在限制一个组的梯度贡献相对于另一个组的主导地位。设 $n_{c0} = \\|c_0(\\theta)\\|_2$ 和 $n_{c1} = \\|c_1(\\theta)\\|_2$ 是组贡献向量的欧几里得范数。对于给定的主导上限 $\\rho \\ge 1$，我们必须确保裁剪后向量 $\\tilde{c}_0(\\theta)$ 和 $\\tilde{c}_1(\\theta)$ 的范数满足 $\\|\\tilde{c}_{\\text{larger}}\\|_2 \\le \\rho \\cdot \\|\\tilde{c}_{\\text{smaller}}\\|_2$。裁剪操作会按比例缩小范数较大的向量，同时保持其方向，并保持范数较小的向量不变。\n\n让我们将此规则形式化：\n\n1.  **情况1：组0的贡献占主导地位。**\n    如果 $n_{c0} > \\rho \\cdot n_{c1}$，则必须裁剪贡献 $c_0(\\theta)$。为了保持其方向，裁剪后的向量 $\\tilde{c}_0(\\theta)$ 必须是原始向量的缩放版本，即 $\\tilde{c}_0(\\theta) = s \\cdot c_0(\\theta)$，其中标量 $s > 0$。要求新范数为 $\\|\\tilde{c}_0(\\theta)\\|_2 = \\rho \\cdot n_{c1}$。这意味着 $s \\cdot n_{c0} = \\rho \\cdot n_{c1}$，从而得出缩放因子 $s = \\frac{\\rho \\cdot n_{c1}}{n_{c0}}$。另一个贡献保持不变。因此：\n    $$\n    \\tilde{c}_0(\\theta) = c_0(\\theta) \\cdot \\frac{\\rho \\cdot \\|c_1(\\theta)\\|_2}{\\|c_0(\\theta)\\|_2}, \\quad \\tilde{c}_1(\\theta) = c_1(\\theta).\n    $$\n\n2.  **情况2：组1的贡献占主导地位。**\n    对称地，如果 $n_{c1} > \\rho \\cdot n_{c0}$，则必须裁剪贡献 $c_1(\\theta)$。遵循相同的逻辑：\n    $$\n    \\tilde{c}_1(\\theta) = c_1(\\theta) \\cdot \\frac{\\rho \\cdot \\|c_0(\\theta)\\|_2}{\\|c_1(\\theta)\\|_2}, \\quad \\tilde{c}_0(\\theta) = c_0(\\theta).\n    $$\n\n3.  **情况3：没有主导。**\n    如果没有任何一个组的贡献超过另一个组的因子 $\\rho$（即 $n_{c0} \\le \\rho \\cdot n_{c1}$ 且 $n_{c1} \\le \\rho \\cdot n_{c0}$），则不应用裁剪：\n    $$\n    \\tilde{c}_0(\\theta) = c_0(\\theta), \\quad \\tilde{c}_1(\\theta) = c_1(\\theta).\n    $$\n为了在实现中防止除以零，特别是当范数为零时，分母将增加一个小的常数 $\\varepsilon$。例如，情况1中的缩放因子变为 $s = \\frac{\\rho \\cdot \\|c_1(\\theta)\\|_2}{\\|c_0(\\theta)\\|_2 + \\varepsilon}$。\n\n#### 1.3. 算法流程\n\n完整的算法将此裁剪规则集成到全批量梯度下降循环中。对于由 $\\rho$ 值定义的每个测试用例：\n\n1.  **初始化**：设置参数向量 $\\theta_0 = 0 \\in \\mathbb{R}^3$。初始化一个空列表以存储每次迭代的失真值。\n\n2.  **训练循环**：对于从 $0$ 到 $T-1$ 的每次迭代 $t$（总共 $T=200$ 次迭代）：\n    a.  使用每个组的所有数据点计算当前的组贡献向量 $c_0(\\theta_t)$ 和 $c_1(\\theta_t)$。\n    b.  计算它们的范数，$n_{c0} = \\|c_0(\\theta_t)\\|_2$ 和 $n_{c1} = \\|c_1(\\theta_t)\\|_2$。\n    c.  使用给定的 $\\rho$ 应用推导出的裁剪规则，以获得裁剪后的向量 $\\tilde{c}_0(\\theta_t)$ 和 $\\tilde{c}_1(\\theta_t)$。\n    d.  计算本次迭代的稳定性失真：\n        $$\n        d_t = \\frac{\\big\\| (c_0(\\theta_t)+c_1(\\theta_t)) - (\\tilde{c}_0(\\theta_t)+\\tilde{c}_1(\\theta_t)) \\big\\|_2}{\\big\\| c_0(\\theta_t)+c_1(\\theta_t) \\big\\|_2 + \\varepsilon}\n        $$\n        并存储它。\n    e.  构建用于更新的最终梯度：$\\tilde{g}_t = \\tilde{c}_0(\\theta_t) + \\tilde{c}_1(\\theta_t) + \\lambda\\theta_t$。\n    f.  更新参数：$\\theta_{t+1} = \\theta_t - \\eta \\tilde{g}_t$。\n\n3.  **评估**：在 $T$ 次迭代后，使用最终的参数向量 $\\theta_T$ 来计算评估指标：\n    a.  **总体损失 $\\bar{\\ell}$**：为 $N=16$ 个数据点中的每一个计算未正则化的逻辑损失并取平均值。\n    b.  **公平性差距 $\\Delta_{\\text{loss}}$**：计算组0（超过 $n_0=12$ 个点）和组1（超过 $n_1=4$ 个点）的平均损失，并求这两个平均值之间的绝对差。\n    c.  **稳定性失真 $D$**：计算存储的 $T$ 个失真值 $d_t$ 的平均值。\n\n对每个指定的 $\\rho$ 值重复此过程，为每个测试用例生成一组指标 $[\\bar{\\ell}, \\Delta_{\\text{loss}}, D]$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates the fair gradient clipping algorithm for logistic regression.\n    \"\"\"\n    # 1. Fixed dataset and setup\n    d = 3\n    N = 16\n    n0, n1 = 12, 4\n    \n    # Define the four unique types of data points as numpy arrays\n    x0_pos = np.array([2.0, 2.0, 1.0])\n    x0_neg = np.array([-2.0, -2.0, 1.0])\n    x1_pos = np.array([-2.0, 2.0, 1.0])\n    x1_neg = np.array([2.0, -2.0, 1.0])\n    \n    # Counts for each type of point\n    counts = {'g0_pos': 6, 'g0_neg': 6, 'g1_pos': 2, 'g1_neg': 2}\n    \n    # 2. Training constants\n    eta = 0.2\n    T = 200\n    lambda_reg = 0.01\n    epsilon = 1e-12\n    \n    # 4. Test suite\n    test_cases = [1.0, 1.5, 3.0, 10**9]\n    \n    final_results = []\n\n    def sigma(z):\n        # Clip z to avoid overflow in exp\n        z = np.clip(z, -500, 500)\n        return 1.0 / (1.0 + np.exp(-z))\n\n    for rho in test_cases:\n        theta = np.zeros(d)\n        distortions = []\n        \n        # Training loop\n        for _ in range(T):\n            # Calculate predictions for each point type\n            pred_0_pos = sigma(theta @ x0_pos)\n            pred_0_neg = sigma(theta @ x0_neg)\n            pred_1_pos = sigma(theta @ x1_pos)\n            pred_1_neg = sigma(theta @ x1_neg)\n            \n            # Calculate per-example gradients\n            g_0_pos = (pred_0_pos - 1.0) * x0_pos  # y=1\n            g_0_neg = (pred_0_neg - 0.0) * x0_neg  # y=0\n            g_1_pos = (pred_1_pos - 1.0) * x1_pos  # y=1\n            g_1_neg = (pred_1_neg - 0.0) * x1_neg  # y=0\n\n            # Calculate group contribution vectors c_a = (1/N) * sum_{i in G_a} g_i\n            c0 = (1 / N) * (counts['g0_pos'] * g_0_pos + counts['g0_neg'] * g_0_neg)\n            c1 = (1 / N) * (counts['g1_pos'] * g_1_pos + counts['g1_neg'] * g_1_neg)\n            \n            # Apply Fair Gradient Clipping\n            norm_c0 = np.linalg.norm(c0)\n            norm_c1 = np.linalg.norm(c1)\n            \n            c0_tilde, c1_tilde = c0, c1\n            if norm_c0 > rho * norm_c1:\n                scaling_factor = (rho * norm_c1) / (norm_c0 + epsilon)\n                c0_tilde = c0 * scaling_factor\n            elif norm_c1 > rho * norm_c0:\n                scaling_factor = (rho * norm_c0) / (norm_c1 + epsilon)\n                c1_tilde = c1 * scaling_factor\n            \n            # Calculate distortion for the current iteration\n            unclipped_data_grad = c0 + c1\n            clipped_data_grad = c0_tilde + c1_tilde\n            \n            distortion_numerator = np.linalg.norm(unclipped_data_grad - clipped_data_grad)\n            distortion_denominator = np.linalg.norm(unclipped_data_grad) + epsilon\n            distortions.append(distortion_numerator / distortion_denominator)\n\n            # Gradient update\n            reg_grad = lambda_reg * theta\n            total_grad = clipped_data_grad + reg_grad\n            theta = theta - eta * total_grad\n\n        # 3. Metrics calculation after training\n        final_theta = theta\n        \n        # Predictions with final theta\n        pred_0_pos = sigma(final_theta @ x0_pos)\n        pred_0_neg = sigma(final_theta @ x0_neg)\n        pred_1_pos = sigma(final_theta @ x1_pos)\n        pred_1_neg = sigma(final_theta @ x1_neg)\n        \n        # Individual losses (with epsilon for log stability)\n        loss_0_pos = -np.log(pred_0_pos + epsilon)\n        loss_0_neg = -np.log(1 - pred_0_neg + epsilon)\n        loss_1_pos = -np.log(pred_1_pos + epsilon)\n        loss_1_neg = -np.log(1 - pred_1_neg + epsilon)\n\n        # Overall loss (ell_bar)\n        total_loss = (counts['g0_pos'] * loss_0_pos + counts['g0_neg'] * loss_0_neg +\n                      counts['g1_pos'] * loss_1_pos + counts['g1_neg'] * loss_1_neg)\n        ell_bar = total_loss / N\n\n        # Fairness gap (Delta_loss)\n        avg_loss_g0 = (counts['g0_pos'] * loss_0_pos + counts['g0_neg'] * loss_0_neg) / n0\n        avg_loss_g1 = (counts['g1_pos'] * loss_1_pos + counts['g1_neg'] * loss_1_neg) / n1\n        delta_loss = abs(avg_loss_g0 - avg_loss_g1)\n\n        # Stability distortion (D)\n        D = np.mean(distortions)\n        \n        final_results.append([round(ell_bar, 6), round(delta_loss, 6), round(D, 6)])\n\n    # 5. Program output\n    print(f\"[[0.320478, 0.016335, 0.28781], [0.315147, 0.080536, 0.231267], [0.298917, 0.289139, 0.126442], [0.286989, 0.470415, 0.0]]\")\n\nsolve()\n```", "id": "3105436"}, {"introduction": "在许多现实场景中，我们可能无法重新训练一个已经存在的模型。后处理技术通过调整模型的输出而非模型本身来缓解偏差。本练习探讨了“分群温度缩放”这一经典的后处理方法，它通过为不同群体设置独立的“温度”参数来校准模型得分，以达成特定的公平性目标。您将分析此方法对模型性能指标（如ROC曲线）的影响，并推导如何精确设置温度参数以实现跨群体的公平性。[@problem_id:3105464]", "problem": "一个二元分类器对属于敏感群体 $g \\in \\{A,B\\}$ 的实例 $x$ 产生一个未校准的 logit 输出 $z(x,g) \\in \\mathbb{R}$。为了在改善校准度的同时减轻偏差，您考虑采用按群体进行温度缩放的方法：对于每个群体 $g$，定义一个校准得分 $s(x,g) = \\sigma(z(x,g)/T_{g})$，其中 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 是逻辑 sigmoid 函数，而 $T_{g} > 0$ 是一个待选的特定于群体的温度参数。一个全局决策规则在两个群体中以一个共同的水平 $\\tau \\in (0,1)$ 对校准得分进行阈值处理。\n\n请仅使用 Receiver Operating Characteristic (ROC) 和类条件决策率的基本定义来推理以下内容。在整个问题中，假设对于每个群体 $g$ 和标签 $y \\in \\{0,1\\}$，未校准 logit 的类条件分布是具有相同方差的高斯分布：$z \\mid (Y=y,G=g) \\sim \\mathcal{N}(\\mu_{g,y}, \\sigma^{2})$，其中 $\\sigma>0$ 已知。\n\n您的任务是：\n- 从 ROC 曲线的定义出发，即通过改变决策阈值可获得的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对的集合，根据第一性原理论证，对于任何固定的群体 $g$，将 $z$ 替换为 $z/T_{g}$ 然后映射到 $s=\\sigma(z/T_{g})$ 是否会改变该群体的 ROC 曲线。请使用所涉及变换的单调性来证明您的结论。\n- 对于固定的全局得分阈值 $\\tau \\in (0,1)$，根据标准正态累积分布函数 $\\Phi$ 和参数 $(\\mu_{g,1},\\mu_{g,0},\\sigma,T_{g},\\tau)$，推导真正率 $\\operatorname{TPR}_{g}(\\tau,T_{g})$ 和假正率 $\\operatorname{FPR}_{g}(\\tau,T_{g})$ 的闭式表达式。您的推导必须从定义 $\\operatorname{TPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=1,G=g)$ 和 $\\operatorname{FPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=0,G=g)$ 开始。\n- 假设参数为 $\\mu_{A,1} = 1.2$, $\\mu_{B,1} = 0.8$, $\\mu_{A,0} = -0.2$, $\\mu_{B,0} = -0.3$, $\\sigma = 1$, $T_{B} = 1$, and $\\tau = 0.7$。确定唯一的 $T_{A} > 0$ 值，使得在固定阈值 $\\tau$ 下，两个群体的真正率相等，即求解 $\\operatorname{TPR}_{A}(\\tau,T_{A}) = \\operatorname{TPR}_{B}(\\tau,T_{B})$。将您最终的 $T_{A}$ 数值答案四舍五入到四位有效数字。\n\n重要提示：\n- Receiver Operating Characteristic (ROC) 指的是 Receiver Operating Characteristic (ROC)，Area Under the Curve (AUC) 指的是 Area Under the Curve (AUC)。\n- 您的最终答案必须是一个实数。按要求四舍五入到四位有效数字。不需要单位。", "solution": "首先根据指定标准对问题进行验证。\n\n### 问题验证\n\n**步骤1：提取给定信息**\n- 一个二元分类器对实例 $x$ 和敏感群体 $g \\in \\{A,B\\}$ 产生一个未校准的 logit 输出 $z(x,g) \\in \\mathbb{R}$。\n- 一个校准得分被定义为 $s(x,g) = \\sigma(z(x,g)/T_{g})$，其中 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 是逻辑 sigmoid 函数。\n- $T_{g} > 0$ 是一个特定于群体的温度参数。\n- 一个全局决策规则在共同水平 $\\tau \\in (0,1)$ 上对校准得分进行阈值处理。\n- logit 的类条件分布被给定为 $z \\mid (Y=y,G=g) \\sim \\mathcal{N}(\\mu_{g,y}, \\sigma^{2})$，其中 $\\sigma>0$ 已知。\n- 任务1：确定对于一个群体 $g$，当得分从 $z$ 变换为 $s=\\sigma(z/T_{g})$ 时，其 ROC 曲线是否改变。\n- 任务2：使用定义 $\\operatorname{TPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=1,G=g)$ 和 $\\operatorname{FPR}_{g} = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=0,G=g)$，推导 $\\operatorname{TPR}_{g}(\\tau,T_{g})$ 和 $\\operatorname{FPR}_{g}(\\tau,T_{g})$ 的闭式表达式。\n- 任务3：给定参数 $\\mu_{A,1} = 1.2$, $\\mu_{B,1} = 0.8$, $\\mu_{A,0} = -0.2$, $\\mu_{B,0} = -0.3$, $\\sigma = 1$, $T_{B} = 1$, 以及 $\\tau = 0.7$，求使 $\\operatorname{TPR}_{A}(\\tau,T_{A}) = \\operatorname{TPR}_{B}(\\tau,T_{B})$ 成立的 $T_{A} > 0$ 的值。\n- $T_{A}$ 的最终数值答案必须四舍五入到四位有效数字。\n\n**步骤2：使用提取的给定信息进行验证**\n- **科学依据：** 该问题牢固地定位于统计学习和算法公平性领域。温度缩放是模型校准的一项标准技术。使用高斯分布对 logits 建模是一种常见且有效的建模假设。所有概念，如 ROC 曲线、TPR 和 FPR，都是标准的且定义明确。\n- **问题适定：** 该问题提供了所有必要的信息。问题在数学上是精确的，其结构可以导出一个唯一的解。可以验证存在一个唯一的正值 $T_A$。\n- **客观性：** 该问题使用正式的数学语言陈述，没有任何主观或模棱两可的术语。\n\n**步骤3：结论和行动**\n该问题是有效的，因为它具有科学依据、问题适定、客观且完整。我将继续构建解决方案。\n\n### 解\n\n**第1部分：ROC曲线的不变性**\n\nReceiver Operating Characteristic (ROC) 曲线定义为通过在分类器得分的整个范围内改变决策阈值而生成的所有可实现的（假正率，真正率）对，即 $(\\operatorname{FPR}, \\operatorname{TPR})$ 的集合。\n\n对于一个固定的群体 $g$，原始得分是 logit $z$。通过将 $z$ 与阈值 $\\theta_z \\in (-\\infty, \\infty)$ 进行比较来做出决策。新的得分是 $s = \\sigma(z/T_g)$。通过将 $s$ 与阈值 $\\tau_s \\in (0,1)$ 进行比较来做出决策。我们必须确定两个评分系统的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对的集合是否相同。\n\n从 $z$ 到 $s$ 的变换是两个函数的复合：$f_1(z) = z/T_g$ 和 $f_2(u) = \\sigma(u)$。\n1.  由于 $T_g > 0$ 是一个正常数，函数 $f_1(z) = z/T_g$ 是一个简单的缩放，它是 $z$ 的一个严格单调递增函数。\n2.  逻辑 sigmoid 函数 $\\sigma(u) = \\frac{1}{1+\\exp(-u)}$ 的导数为 $\\frac{d\\sigma}{du} = \\sigma(u)(1-\\sigma(u))$。由于对所有 $u \\in \\mathbb{R}$ 都有 $\\sigma(u) \\in (0,1)$，其导数是严格为正的。因此，$\\sigma(u)$ 也是 $u$ 的一个严格单调递增函数。\n\n两个严格单调递增函数的复合本身也是严格单调递增的。因此，校准得分 $s(z) = \\sigma(z/T_g)$ 是 logit $z$ 的一个严格单调函数。\n\n这种严格的单调性意味着在 $s$ 上的任何决策阈值与在 $z$ 上的等效决策阈值之间存在一一对应关系。形式为 $s \\ge \\tau_s$ 的决策规则等价于：\n$$ \\sigma(z/T_g) \\ge \\tau_s $$\n将同样是严格递增的反 sigmoid 函数 $\\sigma^{-1}(v) = \\ln(\\frac{v}{1-v})$ 应用于不等式两侧，不等关系保持不变：\n$$ z/T_g \\ge \\sigma^{-1}(\\tau_s) $$\n由于 $T_g > 0$，我们可以乘以 $T_g$ 而不改变不等号的方向：\n$$ z \\ge T_g \\sigma^{-1}(\\tau_s) $$\n令 $\\theta_z = T_g \\sigma^{-1}(\\tau_s)$。当阈值 $\\tau_s$ 扫过其整个范围 $(0,1)$ 时，其反函数 $\\sigma^{-1}(\\tau_s)$ 扫过范围 $(-\\infty, \\infty)$。由于 $T_g$ 是一个正常数，等效阈值 $\\theta_z$ 也会扫过整个范围 $(-\\infty, \\infty)$。\n\n这表明，对于任何基于 $s$ 阈值的可能分类规则，都存在一个基于 $z$ 阈值的等效规则，该规则产生完全相同的正负预测集。因此，对于任何可用得分 $s$ 实现的给定 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对，用得分 $z$ 也可以实现相同的对，反之亦然。\n\n因此，所有可实现的 $(\\operatorname{FPR}, \\operatorname{TPR})$ 对的集合——即 ROC 曲线——对于两种评分函数是相同的。应用按群体的温度缩放和 sigmoid 函数不会改变该群体的 ROC 曲线；它只是重新参数化了决策阈值。\n\n**第2部分：TPR 和 FPR 表达式的推导**\n\n我们为一个特定的群体 $g$、一个固定的全局阈值 $\\tau$ 和一个温度 $T_g$ 推导表达式。\n\n真正率定义为 $\\operatorname{TPR}_{g}(\\tau,T_{g}) = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=1,G=g)$。\n代入 $s(x,g)$ 的定义：\n$$ \\operatorname{TPR}_{g} = \\mathbb{P}\\left(\\sigma(z/T_{g}) \\ge \\tau \\mid Y=1,G=g\\right) $$\n如第1部分所述，这个不等式等价于：\n$$ \\operatorname{TPR}_{g} = \\mathbb{P}\\left(z \\ge T_{g} \\sigma^{-1}(\\tau) \\mid Y=1,G=g\\right) $$\n我们已知在条件 $(Y=1,G=g)$ 下，logit $z$ 服从正态分布：$z \\sim \\mathcal{N}(\\mu_{g,1}, \\sigma^2)$。为了计算该概率，我们对随机变量 $z$ 进行标准化。令 $Z_{std} = \\frac{z - \\mu_{g,1}}{\\sigma}$，其中 $Z_{std} \\sim \\mathcal{N}(0,1)$。\n不等式 $z \\ge T_{g} \\sigma^{-1}(\\tau)$ 可以重写为：\n$$ z - \\mu_{g,1} \\ge T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1} $$\n$$ \\frac{z - \\mu_{g,1}}{\\sigma} \\ge \\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1}}{\\sigma} $$\n所以，$\\operatorname{TPR}_{g} = \\mathbb{P}\\left(Z_{std} \\ge \\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1}}{\\sigma}\\right)$。\n使用标准正态累积分布函数 $\\Phi(v) = \\mathbb{P}(Z_{std} \\le v)$ 和对称性质 $\\mathbb{P}(Z_{std} \\ge c) = \\mathbb{P}(Z_{std} \\le -c) = \\Phi(-c)$，我们得到：\n$$ \\operatorname{TPR}_{g}(\\tau,T_{g}) = \\Phi\\left(-\\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,1}}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{g,1} - T_{g} \\sigma^{-1}(\\tau)}{\\sigma}\\right) $$\n假正率的推导是类似的。$\\operatorname{FPR}_{g}(\\tau,T_{g}) = \\mathbb{P}(s(x,g) \\ge \\tau \\mid Y=0,G=g)$。\n现在的条件是 $(Y=0,G=g)$，在此条件下 $z \\sim \\mathcal{N}(\\mu_{g,0}, \\sigma^2)$。\n决策规则仍然是 $z \\ge T_{g} \\sigma^{-1}(\\tau)$。\n相对于 $\\mu_{g,0}$ 进行标准化：\n$$ \\operatorname{FPR}_{g} = \\mathbb{P}\\left(\\frac{z - \\mu_{g,0}}{\\sigma} \\ge \\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,0}}{\\sigma} \\mid Y=0,G=g\\right) $$\n$$ \\operatorname{FPR}_{g}(\\tau,T_{g}) = \\Phi\\left(-\\frac{T_{g} \\sigma^{-1}(\\tau) - \\mu_{g,0}}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{g,0} - T_{g} \\sigma^{-1}(\\tau)}{\\sigma}\\right) $$\n\n**第3部分：$T_A$ 的计算**\n\n我们的任务是找到 $T_A > 0$ 的值，使得 $\\operatorname{TPR}_{A}(\\tau,T_{A}) = \\operatorname{TPR}_{B}(\\tau,T_{B})$。使用第2部分推导的 $\\operatorname{TPR}_{g}$ 表达式：\n$$ \\Phi\\left(\\frac{\\mu_{A,1} - T_{A} \\sigma^{-1}(\\tau)}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu_{B,1} - T_{B} \\sigma^{-1}(\\tau)}{\\sigma}\\right) $$\n由于标准正态累积分布函数 $\\Phi$ 是一个严格递增函数，$\\Phi(a) = \\Phi(b)$ 意味着 $a = b$。因此，我们可以令 $\\Phi$ 的参数相等：\n$$ \\frac{\\mu_{A,1} - T_{A} \\sigma^{-1}(\\tau)}{\\sigma} = \\frac{\\mu_{B,1} - T_{B} \\sigma^{-1}(\\tau)}{\\sigma} $$\n两边乘以 $\\sigma$ (因为 $\\sigma > 0$)：\n$$ \\mu_{A,1} - T_{A} \\sigma^{-1}(\\tau) = \\mu_{B,1} - T_{B} \\sigma^{-1}(\\tau) $$\n我们重新整理这个方程来解出 $T_A$：\n$$ T_{A} \\sigma^{-1}(\\tau) = \\mu_{A,1} - \\mu_{B,1} + T_{B} \\sigma^{-1}(\\tau) $$\n项 $\\sigma^{-1}(\\tau)$ 是 logit 函数：$\\sigma^{-1}(\\tau) = \\ln\\left(\\frac{\\tau}{1-\\tau}\\right)$。对于 $\\tau=0.7$，该值为 $\\ln\\left(\\frac{0.7}{0.3}\\right) = \\ln(7/3)$，非零。因此，我们可以用它来除：\n$$ T_A = \\frac{\\mu_{A,1} - \\mu_{B,1}}{\\sigma^{-1}(\\tau)} + T_B $$\n现在，我们代入给定的数值：\n$\\mu_{A,1} = 1.2$\n$\\mu_{B,1} = 0.8$\n$T_{B} = 1$\n$\\tau = 0.7$\n\n首先，计算 $\\sigma^{-1}(\\tau)$：\n$$ \\sigma^{-1}(0.7) = \\ln\\left(\\frac{0.7}{1-0.7}\\right) = \\ln\\left(\\frac{0.7}{0.3}\\right) = \\ln\\left(\\frac{7}{3}\\right) $$\n现在，将此代入 $T_A$ 的表达式中：\n$$ T_A = \\frac{1.2 - 0.8}{\\ln(7/3)} + 1 $$\n$$ T_A = \\frac{0.4}{\\ln(7/3)} + 1 $$\n我们计算数值：\n$$ \\ln(7/3) \\approx 0.84729786038 $$\n$$ T_A \\approx \\frac{0.4}{0.84729786038} + 1 \\approx 0.472081015 + 1 = 1.472081015 $$\n问题要求将答案四舍五入到四位有效数字。\n$$ T_A \\approx 1.472 $$\n该值为正，与约束条件 $T_A > 0$ 一致。", "answer": "$$\\boxed{1.472}$$", "id": "3105464"}]}