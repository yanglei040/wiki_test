{"hands_on_practices": [{"introduction": "本练习要求您从头开始实现AdaBoost算法，以探索其核心的动态行为。通过在一个精心设计的小型数据集上运行您的实现，您将直接观察到样本权重和加权错误率 $\\varepsilon_t$ 在迭代过程中的振荡现象。此练习的重点是理解收缩率参数 $\\nu$ 如何作为一种正则化工具来抑制这些振荡，从而稳定学习过程并最终影响模型的分类间隔。[@problem_id:3095513]", "problem": "要求你实现并分析一种自适应提升（AdaBoost）算法的变体，该算法应用于一个特意构建的小型数据集，此数据集会引发加权误差序列的振荡。你的程序必须是一个完整、可运行的实现，遵循以下规范，并以要求的格式输出单行结果。\n\n背景与基础：\n- 工作在标签为 $\\{-1,+1\\}$ 的二元分类问题框架下。\n- 在第 $t$ 轮提升中，存在一个覆盖 $N$ 个训练样本的权重分布 $D_t$。\n- 一个弱学习器 $h_t$ 将输入映射到 $\\{-1,+1\\}$，其加权分类误差为 $\\varepsilon_t = \\sum_{i=1}^N D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(x_i)\\}$。\n- 每一轮从一个固定的字典中选择一个能最小化加权误差的弱学习器 $h_t$。每轮的权重更新必须通过最小化关于系数 $\\alpha_t$ 的逐轮指数损失 $\\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))$ 来推导；该最小化过程定义了归一化因子 $Z_t$ 和归一化后到 $D_{t+1}$ 的更新。你还必须考虑一个收缩参数 $\\nu \\in (0,1]$，它将选定的 $\\alpha_t$ 乘以一个因子 $\\nu$ 以抑制振荡。不允许使用其他启发式方法或修改。\n\n数据集与弱学习器字典：\n- 精确使用以下包含 $N=12$ 个点的数据集，这些点位于 $\\mathbb{R}^2$ 中，标签为 $y \\in \\{-1,+1\\}$：\n  - 四个 $(x_1,x_2)=(+1,-1)$ 的副本，其 $y=-1$。\n  - 四个 $(x_1,x_2)=(-1,+1)$ 的副本，其 $y=-1$。\n  - 两个 $(x_1,x_2)=(+1,+1)$ 的副本，其 $y=+1$。\n  - 两个 $(x_1,x_2)=(-1,-1)$ 的副本，其 $y=-1$。\n- 弱学习器字典精确地由两个固定的决策树桩组成：\n  - $h^{(1)}(x) = \\mathrm{sign}(x_1)$。\n  - $h^{(2)}(x) = \\mathrm{sign}(x_2)$。\n- 在每个提升轮次 $t$，选择加权误差 $\\varepsilon_t$ 较小的弱学习器。如果出现平局，选择索引较小的那个（即选择 $h^{(1)}$）。\n\n程序要求：\n- 实现 AdaBoost 算法，运行 $T$ 轮，使用收缩参数 $\\nu \\in (0,1]$，并从均匀分布 $D_1(i)=1/N$ 开始。\n- 在每一轮 $t$：\n  - 计算两个弱学习器的加权误差并选择 $h_t$。\n  - 通过最小化给定选定 $h_t$ 的逐轮指数损失来确定系数 $\\alpha_t$。然后应用收缩得到 $\\nu\\alpha_t$ 作为实际的更新幅度。\n  - 通过使用收缩后系数的指数更新来更新分布 $D_{t+1}$，并用该更新所隐含的因子 $Z_t$ 对其进行归一化。\n  - 累加得分函数 $F_t(x) = \\sum_{s=1}^t \\nu\\alpha_s h_s(x)$ 以跟踪间隔。\n- 在 $T$ 轮之后，计算以下量值：\n  - 乘积 $\\prod_{t=1}^T Z_t$。\n  - 最小间隔 $\\min_{i \\in \\{1,\\dots,N\\}} y_i F_T(x_i)$。\n  - $(\\varepsilon_t)_{t=1}^T$ 的一阶差分序列（即 $\\Delta_t = \\varepsilon_{t+1} - \\varepsilon_t$，$t=1,\\dots,T-1$）中的符号变化次数，在计数符号变化时忽略任何 $\\Delta_t=0$ 的情况。\n  - 最大振荡幅度 $\\max_{t=1,\\dots,T-1} |\\varepsilon_{t+1} - \\varepsilon_t|$。\n\n测试套件：\n- 你的程序必须为以下五个参数设置运行算法，每个设置都被视为一个独立的测试用例：\n  1. $T=12$, $\\nu=1.0$。\n  2. $T=12$, $\\nu=0.5$。\n  3. $T=12$, $\\nu=0.2$。\n  4. $T=1$, $\\nu=1.0$。\n  5. $T=30$, $\\nu=0.05$。\n- 对于每个测试用例，按以下顺序计算一个包含四个条目的结果列表：$[\\prod_{t=1}^T Z_t,\\ \\min_i y_i F_T(x_i),\\ \\text{符号变化次数},\\ \\text{最大振荡幅度}]$。\n\n数值与格式要求：\n- 所有浮点输出必须使用标准四舍五入到该精度的最近可表示值，精确到 $6$ 位小数。\n- 所有整数必须是精确整数。\n- 你的程序应该生成单行输出，其中包含五个测试用例的结果列表，以逗号分隔，并用方括号括起来，例如：\n  - 输出格式：$[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],\\dots,[r_{51},r_{52},r_{53},r_{54}]]$\n其中 $r_{jk}$ 表示测试用例 $j$ 的第 $k$ 个结果。\n\n科学真实性与推导期望：\n- 你必须从加权误差、指数损失和归一化 $Z_t$ 的基本定义出发，并遵循选择最小化逐轮指数损失的系数的规则。不要在问题陈述中引入无关的公式或捷径。\n- 数据集和弱学习器字典是经过精心选择的，以便随着数据的不同部分被赋予更高的权重，加权误差 $\\varepsilon_t$ 会在不同轮次间振荡。你的实现和输出将量化这些振荡如何影响 $Z_t$ 和间隔的增长，以及收缩（由 $\\nu$ 控制）如何抑制振荡并改变结果。\n\n无用户输入：\n- 代码必须是完全自包含的，并且不得从输入或文件中读取数据。", "solution": "所提出的问题是有效的。它在科学上基于统计学习的原理，特别是 AdaBoost 算法，并且问题定义良好，具有完整且一致的定义、数据和目标。任务是实现一个带有收缩参数的 AdaBoost 变体，将其应用于指定的数据集，并分析其行为，特别是在提升轮次中加权误差的振荡情况。\n\n### AdaBoost 更新的理论基础\n\nAdaBoost 算法的核心是训练样本的迭代重新加权。在每一轮 $t$，我们寻找一个弱学习器 $h_t$ 和一个相应的系数 $\\alpha_t$，以最小化当前数据加权分布 $D_t$ 上的指数损失函数。\n\n逐轮指数损失由下式给出：\n$$L(\\alpha_t) = \\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))$$\n其中 $y_i \\in \\{-1, +1\\}$ 是真实标签，$h_t(x_i) \\in \\{-1, +1\\}$ 是所选弱学习器的预测。对于正确分类，$y_i h_t(x_i)$ 项为 $+1$，对于错误分类，则为 $-1$。\n\n为了找到最优的 $\\alpha_t$，我们将 $L(\\alpha_t)$ 对 $\\alpha_t$ 求导，并令结果为零：\n$$\\frac{\\partial L}{\\partial \\alpha_t} = \\sum_{i=1}^N D_t(i)\\,\\exp(-\\alpha_t y_i h_t(x_i))(-y_i h_t(x_i)) = 0$$\n我们可以将求和分解为正确分类和错误分类的样本。设 $\\varepsilon_t = \\sum_{i=1}^N D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(x_i)\\}$ 为 $h_t$ 的加权误差。正确分类样本的权重之和为 $1-\\varepsilon_t$。方程变为：\n$$-\\sum_{i: y_i=h_t(x_i)} D_t(i)\\,e^{-\\alpha_t} + \\sum_{i: y_i\\neq h_t(x_i)} D_t(i)\\,e^{\\alpha_t} = 0$$\n$$-(1-\\varepsilon_t)e^{-\\alpha_t} + \\varepsilon_t e^{\\alpha_t} = 0$$\n$$\\varepsilon_t e^{\\alpha_t} = (1-\\varepsilon_t) e^{-\\alpha_t}$$\n$$e^{2\\alpha_t} = \\frac{1-\\varepsilon_t}{\\varepsilon_t}$$\n这得到了系数 $\\alpha_t$ 的著名公式，前提是 $\\varepsilon_t \\in (0, 1)$：\n$$\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)$$\n问题引入了一个收缩参数 $\\nu \\in (0, 1]$，它用于抑制更新。因此，更新的有效系数是 $\\nu\\alpha_t$。\n\n下一轮的权重 $D_{t+1}$ 通过减小正确分类样本的权重和增加错误分类样本的权重来更新。未归一化的权重为：\n$$D'_{t+1}(i) = D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))$$\n为确保 $D_{t+1}$ 是一个有效的概率分布，我们用一个因子 $Z_t$ 进行归一化，该因子是未归一化权重的总和：\n$$Z_t = \\sum_{i=1}^N D'_{t+1}(i) = \\sum_{i=1}^N D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))$$\n因此，最终的权重更新规则是：\n$$D_{t+1}(i) = \\frac{D_t(i) \\exp(-\\nu\\alpha_t y_i h_t(x_i))}{Z_t}$$\n\n### 算法流程\n\n实现流程如下：\n\n1.  **数据表示**：数据集包含 $N=12$ 个点，但只有 $4$ 个唯一的 $(x, y)$ 对。为了提高效率，我们对这些唯一点进行操作，并为每个点关联一个计数：\n    -   类型 A：$(+1, -1), y=-1$，计数：$4$\n    -   类型 B：$(-1, +1), y=-1$，计数：$4$\n    -   类型 C：$(+1, +1), y=+1$，计数：$2$\n    -   类型 D：$(-1, -1), y=-1$，计数：$2$\n    每个唯一点类型的总初始权重是其计数除以 $N=12$。\n2.  **弱学习器**：两个弱学习器是 $h^{(1)}(x) = \\mathrm{sign}(x_1)$ 和 $h^{(2)}(x) = \\mathrm{sign}(x_2)$。对于每个唯一点，它们的预测和正确性都已预先计算好。\n    -   $h^{(1)}$ 错误分类类型 A 的点。\n    -   $h^{(2)}$ 错误分类类型 B 的点。\n3.  **初始化**：从 $N=12$ 个点上的均匀权重分布开始，这分别对应于唯一点类型 A、B、C、D 的权重 $4/12, 4/12, 2/12, 2/12$。\n4.  **提升迭代**：对于从 $1$ 到 $T$ 的每一轮 $t$：\n    a.  计算每个弱学习器的加权误差：$\\varepsilon_t^{(j)} = \\sum_{k \\in \\{A,B,C,D\\}} w_t(k) \\cdot \\mathbb{1}\\{y_k \\neq h^{(j)}(x_k)\\}$。这里，$w_t(k)$ 是点类型 $k$ 的总权重。\n    b.  选择具有最小加权误差 $\\varepsilon_t$ 的学习器 $h_t$。平局决胜规则规定，如果 $\\varepsilon_t^{(1)} = \\varepsilon_t^{(2)}$，则选择 $h^{(1)}$。\n    c.  计算 $\\alpha_t = \\frac{1}{2} \\ln((1-\\varepsilon_t)/\\varepsilon_t)$。为保证数值稳定性，将 $\\varepsilon_t$ 截断在严格介于 $0$ 和 $1$ 之间。\n    d.  使用收缩后的系数 $\\nu\\alpha_t$ 计算归一化因子 $Z_t$。\n    e.  根据推导出的更新规则更新每个唯一点类型的权重 $w_{t+1}$。\n    f.  存储 $\\varepsilon_t$、$Z_t$ 以及选定的收缩系数 $\\nu\\alpha_t$ 和学习器 $h_t$ 以供后处理。\n5.  **最终计算**：在 $T$ 轮之后：\n    a.  **$Z_t$ 的乘积**：计算 $\\Pi_{t=1}^T Z_t$。\n    b.  **最小间隔**：最终分类器为 $H(x) = \\mathrm{sign}(F_T(x))$，其中 $F_T(x) = \\sum_{t=1}^T \\nu\\alpha_t h_t(x)$。点 $x_i$ 的间隔为 $y_i F_T(x_i)$。我们找到所有唯一点中的最小间隔。\n    c.  **误差振荡分析**：计算差分序列 $\\Delta_t = \\varepsilon_{t+1} - \\varepsilon_t$，$t=1, \\dots, T-1$。\n        i.  符号变化的次数通过计算 $\\Delta_t$ 序列中正负值之间的切换次数来找到，忽略任何 $\\Delta_t=0$ 的情况。\n        ii. 最大振荡幅度为 $\\max_{t=1, \\dots, T-1} |\\Delta_t|$。\n    对于 $T \\le 1$ 的情况，振荡指标定义为 $0$。\n\n此结构化流程将应用于问题陈述中定义的五个测试用例中的每一个。所有浮点结果均四舍五入到六位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the AdaBoost simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (T, nu)\n        (12, 1.0),\n        (12, 0.5),\n        (12, 0.2),\n        (1, 1.0),\n        (30, 0.05),\n    ]\n\n    results = []\n    for T, nu in test_cases:\n        result = run_adaboost(T, nu)\n        results.append(result)\n\n    # Format the final output string as a list of lists.\n    # The str() of a list is its string representation, e.g., '[1, 2, 3]'.\n    # We join these string representations with commas and enclose in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef run_adaboost(T, nu):\n    \"\"\"\n    Implements the specified AdaBoost algorithm for T rounds with shrinkage nu.\n    \n    Args:\n        T (int): The number of boosting rounds.\n        nu (float): The shrinkage parameter.\n\n    Returns:\n        list: A list containing the four required metrics:\n              [product of Z_t, min margin, sign changes, max oscillation amplitude].\n    \"\"\"\n    # Dataset definition (compact form for unique points)\n    # Types: A, B, C, D\n    # x_unique = np.array([[+1, -1], [-1, +1], [+1, +1], [-1, -1]])\n    y_unique = np.array([-1, -1, 1, -1])\n    counts = np.array([4, 4, 2, 2])\n    N = counts.sum()\n\n    # Weak learners' predictions on unique points\n    # h1(x) = sign(x1), h2(x) = sign(x2)\n    h1_preds = np.array([1, -1, 1, -1])\n    h2_preds = np.array([-1, 1, 1, -1])\n\n    # Pre-calculate y_i * h(x_i) for each learner\n    y_h1 = y_unique * h1_preds  # Result: [-1, 1, 1, 1] -> h1 misclassifies type A\n    y_h2 = y_unique * h2_preds  # Result: [1, -1, 1, 1] -> h2 misclassifies type B\n\n    # Initialize weights based on counts\n    weights = counts / N\n\n    # Storage for per-round quantities\n    all_Z_t = []\n    all_err_t = []\n    \n    # Store coefficients and chosen learners to compute final score function F_T\n    alphas_nu = []\n    hs_preds = []\n\n    # Epsilon for numerical stability to avoid log(0) or division by zero\n    epsilon = 1e-15\n\n    for _ in range(T):\n        # Calculate weighted errors for each learner\n        err1 = np.sum(weights[y_h1 == -1])\n        err2 = np.sum(weights[y_h2 == -1])\n\n        # Select learner based on minimum error (with tie-breaking)\n        if err1 <= err2:\n            err_t = err1\n            y_h_t = y_h1\n            hs_preds.append(h1_preds)\n        else:\n            err_t = err2\n            y_h_t = y_h2\n            hs_preds.append(h2_preds)\n\n        # Clip error to avoid numerical issues\n        err_t = np.clip(err_t, epsilon, 1.0 - epsilon)\n        \n        # Calculate alpha\n        alpha_t = 0.5 * np.log((1 - err_t) / err_t)\n        \n        # Store shrunk alpha\n        alphas_nu.append(nu * alpha_t)\n\n        # Update weights\n        w_update_exp = np.exp(-nu * alpha_t * y_h_t)\n        Z_t = np.sum(weights * w_update_exp)\n        weights = weights * w_update_exp / Z_t\n\n        # Store round results\n        all_Z_t.append(Z_t)\n        all_err_t.append(err_t)\n\n    # 1. Product of Z_t\n    prod_Z = np.prod(all_Z_t)\n\n    # 2. Minimum margin\n    F_T = np.zeros(len(y_unique))\n    for i in range(T):\n        F_T += alphas_nu[i] * hs_preds[i]\n    margins = y_unique * F_T\n    min_margin = np.min(margins)\n\n    # For T = 1, oscillation metrics are 0\n    if T <= 1:\n        sign_changes = 0\n        max_osc_amp = 0.0\n    else:\n        # 3. Number of sign changes in error differences\n        err_diffs = np.diff(all_err_t)\n        # Filter out differences that are effectively zero\n        nonzero_diffs = err_diffs[np.abs(err_diffs) > epsilon]\n        \n        if len(nonzero_diffs) < 2:\n            sign_changes = 0\n        else:\n            signs = np.sign(nonzero_diffs)\n            sign_changes = np.sum(np.diff(signs) != 0)\n        \n        # 4. Maximum oscillation amplitude\n        max_osc_amp = np.max(np.abs(err_diffs))\n\n    # Round final results to 6 decimal places, keeping integers as ints\n    return [\n        round(prod_Z, 6),\n        round(min_margin, 6),\n        int(sign_changes),\n        round(max_osc_amp, 6)\n    ]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3095513"}, {"introduction": "标准的AdaBoost算法对噪声和异常值非常敏感，一个反复被错分的样本权重会呈指数级增长，这种现象被称为“权重爆炸”。本练习将通过一个模拟来量化这种权重放大效应，并评估一种简单而直观的防御策略：权重裁剪。您将比较标准AdaBoost和带有权重上限 $\\Lambda$ 的变体，以直观感受该方法在抑制极端权重增长方面的有效性。[@problem_id:3095556]", "problem": "自适应提升（Adaptive Boosting, AdaBoost）算法在二元分类任务中，通过多轮迭代维护一个样本权重分布，以使后续的弱学习器重点关注难分类的样本。考虑样本标签为 $y_i \\in \\{-1, +1\\}$，在第 $t$ 轮的弱假设为 $h_t(x_i) \\in \\{-1, +1\\}$。该算法开始时，为 $N$ 个样本设置均匀权重 $w_1(i) = 1/N$。在每一轮，加权错误率定义为被错分样本的权重之和，即 $\\varepsilon_t = \\sum_{i=1}^N w_t(i) \\,\\mathbf{1}\\{y_i \\neq h_t(x_i)\\}$。弱学习器的贡献由一个系数 $\\alpha_t$ 进行缩放，而权重分布则通过指数规则进行更新：\n$$\nw_{t+1}(i) \\propto w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big),\n$$\n随后进行归一化，以使 $\\sum_{i=1}^N w_{t+1}(i) = 1$。当一个样本受到对抗性扰动，导致 $h_t(x)$ 在多轮中符号翻转时，其权重会因乘法因子而反复放大。本问题要求您量化这种放大效应，并评估一种通过在归一化前裁剪极端损失贡献（即裁剪权重）的防御方法。\n\n从上述核心定义出发，实现一个模拟器。给定一个固定的数据集和每轮预先确定的弱假设输出，该模拟器计算一个目标样本的权重在以下两种机制下的增长情况：\n- 标准的 AdaBoost 更新机制；以及\n- 一种修剪损失的防御机制，其中归一化前的候选权重 $\\tilde{w}_{t+1}(i) = w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big)$ 会被一个固定的上限值 $\\Lambda \\in (0,1)$ 裁剪，即 $\\tilde{w}_{t+1}^{\\mathrm{clip}}(i) = \\min\\{\\tilde{w}_{t+1}(i), \\Lambda\\}$，然后重新归一化以使总和为 $1$。\n\n您的程序必须：\n- 使用包含 $N=4$ 个样本的数据集，其标签为 $y = [+1, +1, -1, -1]$。\n- 对于下方的每个测试用例，模拟 $T$ 轮，根据当前权重计算 $\\varepsilon_t$，设置 $\\alpha_t = \\tfrac{1}{2}\\ln\\!\\big(\\tfrac{1-\\varepsilon_t}{\\varepsilon_t}\\big)$，并按所述方式更新权重。\n- 对于一个目标索引 $s$，计算在标准更新机制下的增长因子 $g_{\\mathrm{std}} = \\dfrac{w_{T+1}(s)}{w_1(s)}$ 和在裁剪更新机制下的增长因子 $g_{\\mathrm{clip}} = \\dfrac{w^{\\mathrm{clip}}_{T+1}(s)}{w_1(s)}$（其中 $w_1(s) = 1/N$）。如果 $\\varepsilon_t$ 在数值上等于 $0$ 或 $1$，为保持数值稳定性，应分别将其视为 $\\varepsilon_t = 10^{-12}$ 或 $\\varepsilon_t = 1 - 10^{-12}$。\n\n测试用例规范（每个用例提供 $T$、所有 $i$ 在每轮的假设输出 $h_t(x_i)$、裁剪上限 $\\Lambda$ 以及目标索引 $s$）：\n\n- 用例 $1$（正常路径，偶尔的对抗性翻转）：$T=5$, $s=0$, $\\Lambda=0.2$，各轮输出\n  - $t=1$: $[+1,+1,-1,+1]$\n  - $t=2$: $[+1,-1,-1,-1]$\n  - $t=3$: $[-1,+1,-1,-1]$\n  - $t=4$: $[+1,+1,+1,-1]$\n  - $t=5$: $[+1,+1,-1,+1]$\n- 用例 $2$（早期轮次中对目标样本的重度对抗性翻转）：$T=6$, $s=0$, $\\Lambda=0.2$，各轮输出\n  - $t=1$: $[-1,+1,-1,-1]$\n  - $t=2$: $[-1,+1,-1,-1]$\n  - $t=3$: $[-1,+1,-1,-1]$\n  - $t=4$: $[-1,+1,-1,-1]$\n  - $t=5$: $[+1,+1,-1,+1]$\n  - $t=6$: $[+1,-1,-1,-1]$\n- 用例 $3$（目标样本保持正确；其他样本偶尔错误）：$T=4$, $s=0$, $\\Lambda=0.2$，各轮输出\n  - $t=1$: $[+1,+1,-1,+1]$\n  - $t=2$: $[+1,-1,-1,-1]$\n  - $t=3$: $[+1,+1,+1,-1]$\n  - $t=4$: $[+1,+1,-1,+1]$\n- 用例 $4$（每轮有两个错误，加权错误率接近边界值）：$T=6$, $s=0$, $\\Lambda=0.2$，各轮输出\n  - $t=1$: $[-1,+1,+1,-1]$\n  - $t=2$: $[+1,-1,+1,-1]$\n  - $t=3$: $[-1,+1,-1,+1]$\n  - $t=4$: $[+1,-1,-1,+1]$\n  - $t=5$: $[-1,+1,+1,-1]$\n  - $t=6$: $[+1,-1,+1,-1]$\n\n您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，其顺序为 $[g_{\\mathrm{std},1},g_{\\mathrm{clip},1},g_{\\mathrm{std},2},g_{\\mathrm{clip},2},g_{\\mathrm{std},3},g_{\\mathrm{clip},3},g_{\\mathrm{std},4},g_{\\mathrm{clip},4}]$，每个值都四舍五入到六位小数。无需考虑物理单位。角度不适用。不应使用百分比；所有量都应表示为实数。", "solution": "该问题是有效的。它在科学上基于统计学习的原理，特别是 AdaBoost 算法。该问题定义明确，提供了一套完整且一致的定义、数据和参数，以执行确定性模拟。目标清晰，需要一个具有一定挑战性但可行的实现。\n\n解决方案将针对一个固定的数据集和预先确定的弱学习器输出，模拟 AdaBoost 算法的一系列迭代过程。我们将计算在两种不同的更新规则下样本权重的演变：标准的 AdaBoost 更新规则和一种包含权重裁剪作为对抗权重放大防御机制的修改版本。\n\nAdaBoost 的核心原理是通过组合弱学习器来顺序构建一个强分类器。它通过在训练样本上维护一个权重分布来实现这一点。在每一轮中，训练一个新的弱学习器以最小化加权错误率，从而有效地关注先前学习器错分的样本。然后，根据新的弱学习器是否正确分类每个样本来更新其权重。\n\n首先，我们建立初始状态。数据集有 $N=4$ 个样本，标签为 $y = [+1, +1, -1, -1]$。过程从第 $t=1$ 轮开始，采用均匀权重分布，即每个样本 $i$ 的权重为 $w_1(i) = 1/N = 1/4 = 0.25$。我们将运行两个并行的模拟，一个用于标准算法，另一个用于裁剪变体。两者都从这些相同的初始权重开始。\n\n模拟过程从 $t=1$ 到 $t=T$ 迭代进行 $T$ 轮。在每一轮 $t$ 中，我们为标准模拟和裁剪模拟执行以下步骤，需要注意的是，每个模拟的权重 $w_t^{\\mathrm{std}}(i)$ 和 $w_t^{\\mathrm{clip}}(i)$ 将在第一轮后独立演变。\n\n1.  **计算加权错误率（$\\varepsilon_t$）**：当前弱假设 $h_t$ 的加权错误率是所有错分样本的权重之和。如果 $y_i \\neq h_t(x_i)$，或者等价地，如果乘积 $y_i h_t(x_i) = -1$，则样本 $i$ 被错分。\n    $$\n    \\varepsilon_t = \\sum_{i=1}^N w_t(i) \\,\\mathbf{1}\\{y_i \\neq h_t(x_i)\\}\n    $$\n    此计算对标准模拟和裁剪模拟分别使用它们各自的当前权重进行，从而得到 $\\varepsilon_t^{\\mathrm{std}}$ 和 $\\varepsilon_t^{\\mathrm{clip}}$。为防止除以零或对数错误，如果 $\\varepsilon_t$ 在数值上为 $0$ 或 $1$，则分别将其限制在 $10^{-12}$ 或 $1 - 10^{-12}$。\n\n2.  **计算假设权重（$\\alpha_t$）**：弱学习器 $h_t$ 对最终分类器的贡献由其权重 $\\alpha_t$ 决定。这基于加权错误率 $\\varepsilon_t$ 计算。\n    $$\n    \\alpha_t = \\frac{1}{2}\\ln\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right)\n    $$\n    一个优于随机猜测的学习器将有 $\\varepsilon_t < 0.5$，从而得到 $\\alpha_t > 0$。此计算也对每个模拟分别进行，产生 $\\alpha_t^{\\mathrm{std}}$ 和 $\\alpha_t^{\\mathrm{clip}}$。\n\n3.  **更新样本权重**：算法的核心在于更新样本权重。更新规则会乘法性地增加错分样本的权重，并减少正确分类样本的权重。下一轮的未归一化权重 $\\tilde{w}_{t+1}(i)$ 为：\n    $$\n    \\tilde{w}_{t+1}(i) = w_t(i)\\,\\exp\\!\\big(-\\alpha_t\\,y_i\\,h_t(x_i)\\big)\n    $$\n    由于 $y_i h_t(x_i)$ 对于正确分类是 $+1$，对于错误分类是 $-1$，该规则简化为将权重乘以 $e^{\\alpha_t}$（对于错误）或 $e^{-\\alpha_t}$（对于正确）。\n\n4.  **应用裁剪并归一化**：\n    *   **标准模拟**：未归一化的权重 $\\tilde{w}_{t+1}^{\\mathrm{std}}(i)$ 通过除以它们的总和 $Z_t^{\\mathrm{std}} = \\sum_{j=1}^N \\tilde{w}_{t+1}^{\\mathrm{std}}(j)$ 进行归一化，以确保它们为下一轮形成一个有效的概率分布：\n        $$\n        w_{t+1}^{\\mathrm{std}}(i) = \\frac{\\tilde{w}_{t+1}^{\\mathrm{std}}(i)}{Z_t^{\\mathrm{std}}}\n        $$\n    *   **裁剪模拟**：未归一化的权重 $\\tilde{w}_{t+1}^{\\mathrm{clip}}(i)$ 首先被裁剪到最大值 $\\Lambda$。这一步限制了任何单个样本的影响。\n        $$\n        \\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(i) = \\min\\{\\tilde{w}_{t+1}^{\\mathrm{clip}}(i), \\Lambda\\}\n        $$\n        这些裁剪后的权重随后通过它们的总和 $Z_t^{\\mathrm{clip}} = \\sum_{j=1}^N \\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(j)$ 进行归一化：\n        $$\n        w_{t+1}^{\\mathrm{clip}}(i) = \\frac{\\tilde{w}_{t+1}^{\\mathrm{post\\_clip}}(i)}{Z_t^{\\mathrm{clip}}}\n        $$\n\n这个过程重复进行 $T$ 轮。在最后一轮之后，我们获得权重分布 $w_{T+1}^{\\mathrm{std}}$ 和 $w_{T+1}^{\\mathrm{clip}}$。\n\n最后，对于索引为 $s$ 的目标样本，我们计算增长因子。增长因子衡量样本权重从其初始均匀值 $w_1(s) = 1/N$ 的放大程度。\n$$\ng_{\\mathrm{std}} = \\frac{w_{T+1}^{\\mathrm{std}}(s)}{w_1(s)} \\quad \\text{和} \\quad g_{\\mathrm{clip}} = \\frac{w_{T+1}^{\\mathrm{clip}}(s)}{w_1(s)}\n$$\n这些值量化了裁剪防御在减轻目标样本权重爆炸方面的有效性。实现将对每个指定的测试用例执行这些步骤。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates AdaBoost and a clipped-weight variant to compute weight growth factors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (5,  # T\n         [[1, 1, -1, 1], [1, -1, -1, -1], [-1, 1, -1, -1], [1, 1, 1, -1], [1, 1, -1, 1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 2\n        (6,  # T\n         [[-1, 1, -1, -1], [-1, 1, -1, -1], [-1, 1, -1, -1], [-1, 1, -1, -1], [1, 1, -1, 1], [1, -1, -1, -1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 3\n        (4,  # T\n         [[1, 1, -1, 1], [1, -1, -1, -1], [1, 1, 1, -1], [1, 1, -1, 1]],  # h_t\n         0.2,  # Lambda\n         0),  # s\n        # Case 4\n        (6,  # T\n         [[-1, 1, 1, -1], [1, -1, 1, -1], [-1, 1, -1, 1], [1, -1, -1, 1], [-1, 1, 1, -1], [1, -1, 1, -1]],  # h_t\n         0.2,  # Lambda\n         0)  # s\n    ]\n\n    # Dataset specification\n    y = np.array([1, 1, -1, -1])\n    N = 4\n    \n    results = []\n    \n    for case in test_cases:\n        T, h_rounds_list, Lambda, s = case\n        h_rounds = np.array(h_rounds_list)\n\n        # Initialize weights for both standard and clipped simulations\n        w_std = np.full(N, 1.0 / N)\n        w_clip = np.full(N, 1.0 / N)\n\n        for t in range(T):\n            h_t = h_rounds[t]\n\n            # --- Standard AdaBoost update ---\n            # 1. Calculate weighted error\n            is_misclassified_std = (y != h_t)\n            eps_std = np.sum(w_std[is_misclassified_std])\n            eps_std = np.clip(eps_std, 1e-12, 1.0 - 1e-12)\n            \n            # 2. Calculate hypothesis weight\n            alpha_std = 0.5 * np.log((1.0 - eps_std) / eps_std)\n            \n            # 3. Update example weights\n            exponents_std = -alpha_std * y * h_t\n            w_tilde_std = w_std * np.exp(exponents_std)\n            \n            # 4. Normalize\n            w_std = w_tilde_std / np.sum(w_tilde_std)\n\n            # --- Clipped AdaBoost update ---\n            # 1. Calculate weighted error\n            is_misclassified_clip = (y != h_t)\n            eps_clip = np.sum(w_clip[is_misclassified_clip])\n            eps_clip = np.clip(eps_clip, 1e-12, 1.0 - 1e-12)\n\n            # 2. Calculate hypothesis weight\n            alpha_clip = 0.5 * np.log((1.0 - eps_clip) / eps_clip)\n            \n            # 3. Update example weights (unnormalized)\n            exponents_clip = -alpha_clip * y * h_t\n            w_tilde_clip = w_clip * np.exp(exponents_clip)\n\n            # 4. Apply clipping and then normalize\n            w_tilde_clip_clipped = np.minimum(w_tilde_clip, Lambda)\n            sum_clipped = np.sum(w_tilde_clip_clipped)\n            if sum_clipped > 0:\n                w_clip = w_tilde_clip_clipped / sum_clipped\n            else: # Handle case where all weights are clipped to 0\n                w_clip = np.full(N, 1.0 / N)\n\n        # Calculate final growth factors\n        w1_s = 1.0 / N\n        g_std = w_std[s] / w1_s\n        g_clip = w_clip[s] / w1_s\n        \n        results.append(f\"{g_std:.6f}\")\n        results.append(f\"{g_clip:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3095556"}, {"introduction": "在前一个练习的基础上，本实践将从更根本的损失函数角度来解决AdaBoost的噪声敏感性问题。您将实现一个更稳健的AdaBoost变体，它使用一种“Huber化”的指数损失函数，该函数对具有较大错误的样本不那么敏感。通过在包含对抗性标签噪声的数据上进行实验，您将对比标准指数损失和Huber化损失的性能，从而深入理解损失函数的选择如何直接影响算法的稳健性。[@problem_id:3095542]", "problem": "本题要求您研究对抗性标签噪声对自适应增强（AdaBoost）的影响，并实现一个使用Huber化指数损失的鲁棒变体。您必须从第一性原理推导关键的更新步骤，然后在一个程序中实现这两种方法，以比较它们在对抗性条件下的行为。\n\n场景为二元分类，标签集为 $\\{-1,+1\\}$。设 $\\{(x_i,y_i)\\}_{i=1}^n$ 为一个训练集，其中 $x_i \\in \\mathbb{R}$ 且 $y_i \\in \\{-1,+1\\}$。考虑一个分步加性模型 $F_t(x) = F_{t-1}(x) + \\alpha_t h_t(x)$，其中 $F_0(x) \\equiv 0$，$h_t$ 是一个输出为 $\\{-1,+1\\}$ 的决策树桩，$\\alpha_t \\in \\mathbb{R}$ 是一个步长。\n\n在每一轮 $t$，我们根据所选凸代理损失函数关于间隔 $z_i = y_i F_{t-1}(x_i)$ 的负导数，定义一个在训练样本上的经验分布 $D_t$。对于指数损失 $\\phi(z) = \\exp(-z)$，这会得到 $D_t(i) \\propto \\exp(-y_i F_{t-1}(x_i))$。对于下面定义的Huber化指数损失 $\\phi_\\delta(z)$，该分布类似地使用 $-\\phi_\\delta'(z_i)$。\n\n您将研究一个对抗方，在每一轮 $t$，它被允许翻转最多比例为 $\\rho \\in [0,1/2]$ 的标签，以最大化该轮在指数损失下的归一化因子。形式上，在学习器使用干净标签和分布 $D_t$ 选择 $h_t$ 之后，对抗方输出一个标签向量 $y^{(t)} \\in \\{-1,+1\\}^n$，使得 $y^{(t)}$ 与干净标签 $y$ 之间的汉明距离最多为 $\\lfloor \\rho n \\rfloor$，并且 $y^{(t)}$ 在该轮固定的分类器 $h_t$ 和分布 $D_t$ 下，通过选择翻转哪些标签来最大化归一化因子\n$$\nZ_t(\\alpha) \\;=\\; \\sum_{i=1}^n D_t(i)\\,\\exp\\!\\big(-\\alpha\\, y^{(t)}_i\\, h_t(x_i)\\big)\n$$\n学习器随后根据所选的损失函数和对抗性标签 $y^{(t)}$ 选择 $\\alpha_t$ 并更新 $F_t$。\n\n您必须：\n\n- 从经验风险与凸代理以及分步加性模型的定义出发，通过第一性原理推导以下组成部分。\n\n  1) 对于指数损失，展示对于固定的分布 $D_t$ 和分类器 $h_t$，轮次的归一化因子 $Z_t$ 如何依赖于加权分类误差 $\\varepsilon_t = \\sum_{i=1}^n D_t(i)\\,\\mathbf{1}[y^{(t)}_i \\neq h_t(x_i)]$。利用这一点，确定在最多可翻转 $\\lfloor \\rho n \\rfloor$ 个标签的约束下，对抗方的最优翻转策略，并且学习器会通过可能翻转 $h_t$ 的符号来强制执行弱学习条件，以使 $\\varepsilon_t \\leq 1/2$。\n\n  2) 对于指数损失，推导在使用对抗性标签 $y^{(t)}$ 的情况下，使沿方向 $h_t$ 的经验指数风险最小化的步长 $\\alpha_t$。\n\n  3) 提出一个带参数 $\\delta > 0$ 的Huber化指数损失，其定义为\n  $$\n  \\phi_\\delta(z) \\;=\\;\n  \\begin{cases}\n  \\exp(-z), & \\text{if } z \\geq -\\delta \\\\\n  \\exp(\\delta) - \\exp(\\delta)(z+\\delta), & \\text{if } z < -\\delta\n  \\end{cases}\n  $$\n  证明 $-\\phi_\\delta'(z)$ 是一个裁剪后的重要性权重，并推导在最小化 $\\sum_{i=1}^n \\phi_\\delta\\!\\big(y^{(t)}_i (F_{t-1}(x_i) + \\alpha h_t(x_i))\\big)$ 关于 $\\alpha$ 的问题中，$\\alpha_t$ 的一维最优性条件。提供一个数值稳定的程序（例如，在导数上使用区间限定加二分法）来找到 $\\alpha_t$。\n\n- 实现两种共享相同弱学习器的算法：\n\n  a) 一个标准的AdaBoost变体，在每一轮中，使用指数损失从干净标签 $y$ 计算 $D_t(i) \\propto \\exp(-y_i F_{t-1}(x_i))$，在 $(x_i,y_i)$ 上训练一个加权决策树桩 $h_t$，让对抗方通过最优地翻转最多 $\\lfloor \\rho n \\rfloor$ 个标签来生成 $y^{(t)}$，以最大化该轮在 $D_t$ 下 $h_t$ 的归一化因子，使用 $y^{(t)}$ 计算指数损失的 $\\alpha_t$，然后更新 $F_t$。\n\n  b) 一个鲁棒的变体，使用带参数 $\\delta > 0$ 的Huber化指数损失从干净标签 $y$ 计算分布 $D_t(i) \\propto -\\phi_\\delta'(y_i F_{t-1}(x_i))$，在 $(x_i,y_i)$ 上训练相同的加权树桩，让相同的对抗方翻转标签以生成 $y^{(t)}$（对抗方仍然针对指数归一化因子，而不是Huber化的），通过最小化沿 $h_t$ 方向使用 $y^{(t)}$ 的Huber化经验风险来计算 $\\alpha_t$，然后更新 $F_t$。\n\n- 使用一个固定的、确定性的数据集，大小为 $n=60$，由在区间 $[-1,1]$ 内等距分布的 $x_i$ 和 $y_i = \\operatorname{sign}(x_i)$ 定义，约定 $\\operatorname{sign}(0)=+1$。使用一个由实线上的阈值和 $\\{-1,+1\\}$ 中的极性定义的决策树桩类，其预测规则为 $h(x) = s \\cdot \\operatorname{sign}(x - \\theta)$，其中平局情况使用 $+1$。在所有实验中训练 $T=20$ 轮。\n\n- 对抗方在每一轮的操作如下：给定当前分布 $D_t$（根据干净标签 $y$ 和所选损失计算）以及选定的树桩 $h_t$（使用干净标签训练），对抗方通过翻转 $y$ 中最多 $\\lfloor \\rho n \\rfloor$ 个条目来输出 $y^{(t)}$，这些条目被选择以最大化该轮的指数归一化因子，约束条件是学习器将通过必要时翻转 $h_t$ 的符号来强制执行弱学习条件，以确保加权误差最多为 $1/2$。因此，对抗方不得将加权误差推高到 $1/2$ 以上。\n\n- 对于评估，必须在 $T$ 轮后，使用最终分类器 $\\operatorname{sign}(F_T(x))$ 在干净标签 $y$ 上计算经验分类误差，其中平局计为 $+1$。将误差报告为 $[0,1]$ 内的实数。\n\n实现一个单一程序，运行以下参数三元组 $(\\rho, \\text{loss}, \\delta)$ 的测试套件，并输出 $T$ 轮后得到的最终干净标签误差（浮点数）：\n\n- 测试用例 1: $(0.0,\\ \\text{exp},\\ 0.0)$\n- 测试用例 2: $(0.3,\\ \\text{exp},\\ 0.0)$\n- 测试用例 3: $(0.3,\\ \\text{huber},\\ 0.5)$\n- 测试用例 4: $(0.45,\\ \\text{huber},\\ 0.5)$\n- 测试用例 5: $(0.45,\\ \\text{exp},\\ 0.0)$\n\n您的程序应生成单行输出，其中包含五个结果，形式为逗号分隔的列表，并用方括号括起来（例如，$[0.0000,0.1000,0.0500,0.1000,0.2000]$）。不允许有其他输出。所有数字必须打印为小数点后恰好有四位的小数。\n\n所有角度（如有）均以弧度为单位，但不需要进行角度计算。不涉及物理单位。每个测试用例的最终答案是 $[0,1]$ 内的浮点数。", "solution": "该问题要求对标准AdaBoost和鲁棒AdaBoost在对抗性标签噪声下的更新规则进行理论推导，然后进行比较性实现。\n\n### 第1步：推导\n\n设训练集为 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{-1, +1\\}$。分步加性模型为 $F_t(x) = F_{t-1}(x) + \\alpha_t h_t(x)$，其中 $F_0(x) = 0$。第 $t$ 轮的样本权重由一个分布 $D_t(i)$ 给出，该分布源于一个凸损失函数 $\\phi$。设 $z_i = y_i F_{t-1}(x_i)$ 为样本 $i$ 关于干净标签 $y_i$ 的间隔。权重为 $D_t(i) \\propto -\\phi'(z_i)$。在弱学习器 $h_t$ 使用干净数据 $(x, y)$ 和权重 $D_t$ 进行训练后，一个对抗方引入了对抗性标签 $y^{(t)}$。\n\n#### 1. 对抗方的最优策略\n\n对抗方的目标是通过翻转最多 $k = \\lfloor \\rho n \\rfloor$ 个标签来最大化指数损失归一化因子 $Z_t(\\alpha)$。学习器使用步长 $\\alpha_t > 0$（因为强制执行了弱学习条件）。\n$$\nZ_t(\\alpha) = \\sum_{i=1}^n D_t(i)\\,\\exp(-\\alpha\\, y^{(t)}_i\\, h_t(x_i))\n$$\n乘积 $y^{(t)}_i h_t(x_i)$ 对于 $h_t$ 在对抗性标签 $y_i^{(t)}$ 上的正确分类为 $+1$，不正确分类为 $-1$。我们可以将求和分为正确和不正确分类的样本：\n$$\nZ_t(\\alpha) = \\sum_{i: y^{(t)}_i = h_t(x_i)} D_t(i)\\,e^{-\\alpha} + \\sum_{i: y^{(t)}_i \\neq h_t(x_i)} D_t(i)\\,e^{\\alpha}\n$$\n设 $\\varepsilon_t = \\sum_{i=1}^n D_t(i)\\,\\mathbf{1}[y^{(t)}_i \\neq h_t(x_i)]$ 为 $h_t$ 关于对抗性标签 $y^{(t)}$ 和分布 $D_t$ 的加权分类误差。由于 $\\sum_{i=1}^n D_t(i) = 1$，正确分类样本的权重之和为 $1-\\varepsilon_t$。归一化因子变为：\n$$\nZ_t(\\alpha) = (1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha}\n$$\n为了在固定的 $\\alpha > 0$ 下最大化 $Z_t(\\alpha)$，我们分析其关于 $\\varepsilon_t$ 的导数：\n$$\n\\frac{\\partial Z_t(\\alpha)}{\\partial \\varepsilon_t} = e^{\\alpha} - e^{-\\alpha}\n$$\n对于 $\\alpha > 0$，我们有 $e^{\\alpha} > 1$ 和 $e^{-\\alpha} < 1$，因此 $e^{\\alpha} - e^{-\\alpha} > 0$。因此，$Z_t(\\alpha)$ 是 $\\varepsilon_t$ 的单调递增函数。对抗方的最优策略是最大化加权误差 $\\varepsilon_t$。\n\n对抗方从干净标签 $y$ 开始，通过翻转标签来生成 $y^{(t)}$。设 $h_t$ 是学习器选择的弱学习器（在干净标签上强制执行弱学习条件后）。设 $I_{corr} = \\{i \\mid y_i = h_t(x_i)\\}$ 是 $h_t$ 在干净标签上正确分类的点的集合，$I_{incorr} = \\{i \\mid y_i \\neq h_t(x_i)\\}$ 是不正确分类的点的集合。初始时，误差为 $\\varepsilon_{t,clean} = \\sum_{i \\in I_{incorr}} D_t(i)$。\n- 如果对抗方翻转点 $i \\in I_{corr}$ 的标签，该点将变为错误分类，误差增加 $D_t(i)$。\n- 如果对抗方翻转点 $i \\in I_{incorr}$ 的标签，该点将变为正确分类，误差减少 $D_t(i)$。\n\n为了最大化 $\\varepsilon_t$，对抗方应仅翻转 $I_{corr}$ 中点的标签。为了实现误差的最大增量，对抗方应贪心地翻转 $I_{corr}$ 中对应最大权重 $D_t(i)$ 的点的标签。\n\n对抗方受到两个约束：\n1. 翻转次数最多为 $k = \\lfloor \\rho n \\rfloor$。\n2. 最终的对抗性误差必须满足 $\\varepsilon_t \\le 1/2$。如果 $\\varepsilon_t > 1/2$，学习器将翻转 $h_t$ 的符号，导致新的误差为 $1-\\varepsilon_t < 1/2$。这会降低误差，从而挫败对抗方的目标。\n\n因此，对抗方的策略如下：\n1. 识别正确分类点的集合 $I_{corr}$ 及其权重 $D_t(i)$。\n2. 按权重降序对这些点进行排序。\n3. 遍历排序后的列表，逐个翻转标签，直到已进行 $k$ 次翻转或翻转下一个标签会导致总加权误差超过 $1/2$。\n\n#### 2. 指数损失的步长\n\n学习器选择 $\\alpha_t$ 以最小化使用对抗性标签 $y^{(t)}$ 的经验风险。对于指数损失，这等同于最小化 $Z_t(\\alpha)$：\n$$\n\\alpha_t = \\arg\\min_{\\alpha > 0} \\left( (1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha} \\right)\n$$\n我们令关于 $\\alpha$ 的导数为零：\n$$\n\\frac{d}{d\\alpha} \\left( (1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha} \\right) = -(1-\\varepsilon_t)e^{-\\alpha} + \\varepsilon_t e^{\\alpha} = 0\n$$\n$$\n\\varepsilon_t e^{\\alpha} = (1-\\varepsilon_t)e^{-\\alpha} \\implies e^{2\\alpha} = \\frac{1-\\varepsilon_t}{\\varepsilon_t}\n$$\n求解 $\\alpha$ 得到步长：\n$$\n\\alpha_t = \\frac{1}{2} \\ln\\left(\\frac{1 - \\varepsilon_t}{\\varepsilon_t}\\right)\n$$\n这对于 $\\varepsilon_t \\in (0, 1)$ 是有效的。弱学习条件 $\\varepsilon_t \\le 1/2$ 确保了 $\\alpha_t \\ge 0$。\n\n#### 3. Huber化的指数损失\n\nHuber化的指数损失定义为：\n$$\n\\phi_\\delta(z) =\n\\begin{cases}\n\\exp(-z), & \\text{if } z \\geq -\\delta \\\\\n\\exp(\\delta) - \\exp(\\delta)(z+\\delta), & \\text{if } z < -\\delta\n\\end{cases}\n$$\n该函数在 $z=-\\delta$ 处是连续的，因为 $\\exp(-(-\\delta)) = \\exp(\\delta)$ 且 $\\exp(\\delta) - \\exp(\\delta)(-\\delta+\\delta) = \\exp(\\delta)$。其导数为：\n$$\n\\phi'_\\delta(z) =\n\\begin{cases}\n-\\exp(-z), & \\text{if } z > -\\delta \\\\\n-\\exp(\\delta), & \\text{if } z < -\\delta\n\\end{cases}\n$$\n导数在 $z=-\\delta$ 处也是连续的，值为 $-\\exp(\\delta)$。用于权重计算的负导数为：\n$$\n-\\phi'_\\delta(z) =\n\\begin{cases}\n\\exp(-z), & \\text{if } z > -\\delta \\\\\n\\exp(\\delta), & \\text{if } z < -\\delta\n\\end{cases}\n$$\n这可以紧凑地写成 $-\\phi'_\\delta(z) = \\min(\\exp(-z), \\exp(\\delta))$。这表明 $-\\phi'_\\delta(z)$ 是指数权重 $\\exp(-z)$ 的一个“裁剪”版本，权重被限制在最大值 $\\exp(\\delta)$。这防止了少数具有非常大负间隔的点主导权重分布，从而赋予了鲁棒性。\n\n为了找到步长 $\\alpha_t$，我们使用Huber化损失和对抗性标签 $y^{(t)}$ 最小化总经验风险：\n$$\nJ(\\alpha) = \\sum_{i=1}^n \\phi_\\delta\\big(y^{(t)}_i (F_{t-1}(x_i) + \\alpha h_t(x_i))\\big)\n$$\n设 $z'_i = y^{(t)}_i F_{t-1}(x_i)$ 和 $m_i = y^{(t)}_i h_t(x_i)$。我们想最小化 $\\sum_i \\phi_\\delta(z'_i + \\alpha m_i)$。我们令关于 $\\alpha$ 的导数为零：\n$$\n\\frac{dJ}{d\\alpha} = \\sum_{i=1}^n m_i \\phi'_\\delta(z'_i + \\alpha m_i) = 0\n$$\n由于 $\\phi_\\delta$ 是凸的，这个条件足以保证最小值。设 $C = \\{i \\mid m_i = 1\\}$ 和 $M = \\{i \\mid m_i = -1\\}$ 分别为 $h_t$ 在标签 $y^{(t)}$ 上正确和不正确分类的索引集合。方程变为：\n$$\n\\sum_{i \\in C} \\phi'_\\delta(z'_i + \\alpha) + \\sum_{i \\in M} (-1) \\phi'_\\delta(z'_i - \\alpha) = 0\n$$\n$$\n\\sum_{i \\in C} (-\\phi'_\\delta(z'_i + \\alpha)) = \\sum_{i \\in M} (-\\phi'_\\delta(z'_i - \\alpha))\n$$\n代入 $-\\phi'_\\delta(z)$ 的表达式：\n$$\n\\sum_{i \\in C} \\min(\\exp(-(z'_i + \\alpha)), \\exp(\\delta)) = \\sum_{i \\in M} \\min(\\exp(-(z'_i - \\alpha)), \\exp(\\delta))\n$$\n这是一个关于 $\\alpha$ 的非线性方程。设待求根的函数为\n$$\nG(\\alpha) = \\sum_{i \\in M} \\min(\\exp(-z'_i + \\alpha), \\exp(\\delta)) - \\sum_{i \\in C} \\min(\\exp(-z'_i - \\alpha), \\exp(\\delta))\n$$\n函数 $G(\\alpha)$ 对于 $\\alpha \\ge 0$ 是单调非减的。我们可以使用像二分法这样的数值方法来找到根 $\\alpha_t$。\n\n**$\\alpha_t$ 的数值求解过程（Huber化损失）：**\n1.  如上定义函数 $G(\\alpha)$。\n2.  检查在 $\\alpha=0$ 处的值。如果 $G(0) \\geq 0$，则最小值为 $\\alpha_t=0$，因为增加 $\\alpha$ 不会使 $G(\\alpha)$ 减小。\n3.  如果 $G(0) < 0$，则存在一个正根。我们需要找到搜索的上界。我们可以从一个猜测值开始，例如 $\\alpha_{high}=1.0$，并将其加倍直到 $G(\\alpha_{high}) > 0$。设下界为 $\\alpha_{low}=0$。\n4.  在区间 $[\\alpha_{low}, \\alpha_{high}]$ 上应用二分法：\n    a. 计算 $\\alpha_{mid} = (\\alpha_{low} + \\alpha_{high})/2$。\n    b. 如果 $G(\\alpha_{mid}) < 0$，则根在下半区间的上半部分：设置 $\\alpha_{low} = \\alpha_{mid}$。\n    c. 如果 $G(\\alpha_{mid}) \\geq 0$，则根在下半区间：设置 $\\alpha_{high} = \\alpha_{mid}$。\n5.  重复固定次数的迭代（例如100次）或直到区间宽度足够小。得到的 $\\alpha_{mid}$ 就是我们的 $\\alpha_t$。\n\n这完成了所需的推导。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that runs the test suite and prints the final results.\n    \"\"\"\n    \n    # --- Problem Setup ---\n    n = 60\n    T = 20\n    x = np.linspace(-1, 1, n)\n    y = np.sign(x)\n    y[y == 0] = 1 # Per problem spec, sign(0) = +1\n\n    # Define a consistent sign function for tie-breaking\n    def sign_with_tiebreak(arr):\n        s = np.sign(arr)\n        s[s == 0] = 1\n        return s\n\n    # --- Weak Learner: Decision Stump ---\n    def find_best_stump(x, y, D):\n        \"\"\"Finds the best decision stump (threshold, polarity) for weighted data.\"\"\"\n        best_err = np.inf\n        best_theta = 0\n        best_s = 1\n        \n        # Define potential thresholds\n        thresholds = (x[:-1] + x[1:]) / 2\n        \n        for theta in thresholds:\n            # Polarity s = +1\n            s = 1\n            h_pred = s * sign_with_tiebreak(x - theta)\n            err = D @ (h_pred != y)\n            \n            if err < best_err:\n                best_err = err\n                best_theta = theta\n                best_s = s\n\n            # Polarity s = -1 (error is 1 - err for s=+1)\n            if 1 - err < best_err:\n                best_err = 1 - err\n                best_theta = theta\n                best_s = -s\n                \n        # The learner enforces weak learning by choosing min(err, 1-err),\n        # so best_err is guaranteed to be <= 0.5.\n        return best_theta, best_s, best_err\n\n    # --- Adversary ---\n    def adversary_flip(y_clean, h_pred, D, rho, n):\n        \"\"\"Flips labels to maximize weighted error.\"\"\"\n        y_adv = np.copy(y_clean)\n        if rho == 0:\n            return y_adv\n        \n        k = int(np.floor(rho * n))\n        \n        # Initial error on clean labels\n        err_adv = D @ (y_adv != h_pred)\n        \n        # Identify correctly classified points on clean labels\n        # These are the candidates for flipping to increase error\n        corr_indices = np.where(y_clean == h_pred)[0]\n        \n        # Sort these candidates by their weights in descending order\n        candidate_weights = D[corr_indices]\n        sorted_candidates = corr_indices[np.argsort(-candidate_weights)]\n        \n        flips_done = 0\n        for idx in sorted_candidates:\n            if flips_done >= k:\n                break\n            \n            # Check if flipping this label violates the error <= 0.5 constraint\n            if err_adv + D[idx] <= 0.5:\n                # Perform the flip\n                y_adv[idx] *= -1\n                err_adv += D[idx]\n                flips_done += 1\n            else:\n                # Flipping this would push error > 0.5, so we stop.\n                # The adversary will not do this as the learner would counter it.\n                break\n                \n        return y_adv\n\n    # --- Huberized Alpha Calculation ---\n    def find_alpha_huber(y_adv, F_prev, h_pred, delta, n):\n        \"\"\"Finds alpha_t for Huberized loss using bisection.\"\"\"\n        z_prime = y_adv * F_prev\n        m = y_adv * h_pred\n        \n        exp_delta = np.exp(delta)\n\n        C_indices = np.where(m == 1)[0]\n        M_indices = np.where(m == -1)[0]\n\n        def G(alpha):\n            if alpha < 0: return -np.inf # We search for alpha >= 0\n            \n            term_C = np.sum(np.minimum(np.exp(-(z_prime[C_indices] + alpha)), exp_delta))\n            term_M = np.sum(np.minimum(np.exp(-(z_prime[M_indices] - alpha)), exp_delta))\n            \n            return term_M - term_C\n\n        # Check if alpha=0 is already the solution\n        if G(0) >= 0:\n            return 0.0\n\n        # Find an upper bound for the search\n        alpha_low = 0.0\n        alpha_high = 1.0\n        while G(alpha_high) < 0:\n            alpha_high *= 2.0\n            if alpha_high > 1e6: # Safety break\n                return 0.0\n\n        # Bisection\n        for _ in range(100): # 100 iterations is more than enough\n            alpha_mid = (alpha_low + alpha_high) / 2\n            if G(alpha_mid) < 0:\n                alpha_low = alpha_mid\n            else:\n                alpha_high = alpha_mid\n        \n        return (alpha_low + alpha_high) / 2\n        \n    # --- Main Boosting Algorithm ---\n    def run_boosting(rho, loss_type, delta, x, y, n, T):\n        \"\"\"Runs the boosting algorithm for a given configuration.\"\"\"\n        F = np.zeros(n)\n        exp_delta = np.exp(delta) if delta > 0 else 0\n        \n        for t in range(T):\n            # 1. Compute weights D_t\n            if loss_type == 'exp':\n                w = np.exp(-y * F)\n            elif loss_type == 'huber':\n                z = y * F\n                w = np.minimum(np.exp(-z), exp_delta)\n            \n            D = w / np.sum(w)\n            \n            # 2. Train weak learner h_t on clean data\n            theta, s, _ = find_best_stump(x, y, D)\n            h_pred = s * sign_with_tiebreak(x - theta)\n            \n            # 3. Adversary acts\n            y_adv = adversary_flip(y, h_pred, D, rho, n)\n            \n            # 4. Compute step size alpha_t\n            if loss_type == 'exp':\n                eps_t = D @ (y_adv != h_pred)\n                # Add small epsilon for numerical stability\n                eps_t = np.clip(eps_t, 1e-10, 1 - 1e-10)\n                alpha = 0.5 * np.log((1 - eps_t) / eps_t)\n            elif loss_type == 'huber':\n                alpha = find_alpha_huber(y_adv, F, h_pred, delta, n)\n\n            # 5. Update model\n            F += alpha * h_pred\n        \n        # Evaluate final classifier on clean labels\n        y_hat = sign_with_tiebreak(F)\n        final_err = np.mean(y_hat != y)\n        \n        return final_err\n\n    # --- Run Test Suite ---\n    test_cases = [\n        (0.0, 'exp', 0.0),   # Test case 1\n        (0.3, 'exp', 0.0),   # Test case 2\n        (0.3, 'huber', 0.5), # Test case 3\n        (0.45, 'huber', 0.5),# Test case 4\n        (0.45, 'exp', 0.0)   # Test case 5\n    ]\n\n    results = []\n    for rho, loss, delta in test_cases:\n        error = run_boosting(rho, loss, delta, x, y, n, T)\n        results.append(f\"{error:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3095542"}]}