{"hands_on_practices": [{"introduction": "提升算法通过将新模型拟合到当前集成模型的“残差”上来顺序地纠正错误。这些残差的性质由所选损失函数的梯度决定。这个练习 [@problem_id:3105987] 提供了一个基础的理论推导，要求你为逻辑损失（LogitBoost）导出更新规则，并将其目标与指数损失（AdaBoost）的目标进行比较，从而揭示不同的损失函数如何以不同的尺度逼近相同的底层类别概率。", "problem": "考虑标签为 $y \\in \\{-1,+1\\}$ 的二元分类问题，以及一个通过提升法分阶段构建的加性得分函数 $F(x)$，$F_{m}(x) = F_{m-1}(x) + \\nu h_{m}(x)$，其中 $\\nu \\in (0,1]$ 是一个固定的学习率，$h_{m}$ 是在第 $m$ 阶段选择的基学习器。训练目标是使用逻辑斯蒂损失 $\\ell_{\\log}(y,F) = \\log(1+\\exp(-yF))$ 的经验风险最小化 (ERM)。在每个阶段，应用函数梯度下降 (FGD)：选择 $h_{m}$ 来近似经验风险关于 $F$ 在 $F_{m-1}$ 处评估的负梯度。\n\n任务：\n1. 从 $\\ell_{\\log}(y,F)$ 的定义和 ERM 目标出发，推导决定 FGD 分阶段更新方向的每个样本的负梯度 $\\{-\\partial \\ell_{\\log}(y_{i},F)/\\partial F\\}_{F=F_{m-1}(x_{i})}$。将您的结果明确表示为 $y_{i}$ 和 $F_{m-1}(x_{i})$ 的函数。\n2. 使用总体风险 $\\mathbb{E}[\\ell_{\\log}(Y,F(X)) \\mid X=x]$，其中 $Y \\in \\{-1,+1\\}$ 且 $\\pi(x) \\equiv \\mathbb{P}(Y=+1 \\mid X=x)$，推导在每个 $x$ 处最小化 $\\mathbb{E}[\\ell_{\\log}(Y,F(X)) \\mid X=x]$ 的总体最优得分 $F^{\\star}(x)$ 的闭式表达式。然后，将此校准与指数损失 $\\ell_{\\exp}(y,F)=\\exp(-yF)$ 下的类似最小化器进行对比，解释在对数几率缩放上的差异。\n3. 提供您在逻辑斯蒂损失下得到的 $F^{\\star}(x)$ 的单个闭式解析表达式作为最终答案。无需四舍五入，也不涉及物理单位。\n\n您的推导必须从第一性原理（ERM 和 FGD 的定义，以及给定的损失函数）出发，并进行逻辑推理。不要使用或引用任何预先推导的提升法更新公式。", "solution": "该问题陈述被评估为有效。其科学基础在于统计学习理论的原理，特别是提升法和函数梯度下降。该问题是适定的、客观的且内部一致的，为进行所需的推导提供了所有必要的定义和数据。\n\n### 任务1：每个样本负梯度的推导\n\n经验风险最小化 (ERM) 的目标是找到一个函数 $F$，以最小化在训练数据上的总损失。对于包含 $N$ 个样本 $\\{(x_i, y_i)\\}_{i=1}^N$ 的数据集，经验风险 $R_{\\text{emp}}(F)$ 是平均损失：\n$$\nR_{\\text{emp}}(F) = \\frac{1}{N} \\sum_{i=1}^N \\ell_{\\log}(y_i, F(x_i))\n$$\n在函数梯度下降 (FGD) 中，在每个阶段 $m$，我们寻找一个指向风险泛函负梯度方向的函数 $h_m$。对于有限数据集，这简化为找到一个基学习器 $h_m(x)$，它使用当前的得分函数 $F_{m-1}(x)$，最好地近似在每个数据点 $x_i$ 处评估的负梯度分量。对于第 $i$ 个数据点 $(x_i, y_i)$，每个样本的负梯度是 $h_m(x_i)$ 应该近似的目标。\n\n给定逻辑斯蒂损失函数 $\\ell_{\\log}(y,F) = \\log(1+\\exp(-yF))$。我们必须计算它关于得分 $F$ 的偏导数。令 $u = -yF$。根据链式法则，$\\frac{\\partial \\ell_{\\log}}{\\partial F} = \\frac{d \\ell_{\\log}}{du} \\frac{\\partial u}{\\partial F}$。\n\n导数是：\n$$\n\\frac{d}{du} \\log(1+\\exp(u)) = \\frac{1}{1+\\exp(u)} \\cdot \\exp(u)\n$$\n$$\n\\frac{\\partial u}{\\partial F} = \\frac{\\partial}{\\partial F}(-yF) = -y\n$$\n将这些结合起来，我们得到损失关于 $F$ 的偏导数：\n$$\n\\frac{\\partial \\ell_{\\log}(y,F)}{\\partial F} = \\frac{\\exp(-yF)}{1+\\exp(-yF)} \\cdot (-y) = \\frac{-y \\exp(-yF)}{1+\\exp(-yF)}\n$$\n为了简化这个表达式，我们可以将分子和分母同乘以 $\\exp(yF)$：\n$$\n\\frac{\\partial \\ell_{\\log}(y,F)}{\\partial F} = \\frac{-y \\exp(-yF) \\cdot \\exp(yF)}{(1+\\exp(-yF)) \\cdot \\exp(yF)} = \\frac{-y}{\\exp(yF) + 1}\n$$\n因此，作为函数更新方向的负梯度是：\n$$\n-\\frac{\\partial \\ell_{\\log}(y,F)}{\\partial F} = \\frac{y}{1 + \\exp(yF)}\n$$\n在提升算法的第 $m$ 阶段，对于每个样本 $(x_i, y_i)$，这个梯度在当前的得分函数 $F = F_{m-1}(x_i)$ 处进行评估。因此，定义分阶段更新方向的每个样本的负梯度是：\n$$\nr_{im} = \\left[-\\frac{\\partial \\ell_{\\log}(y_i, F)}{\\partial F}\\right]_{F=F_{m-1}(x_i)} = \\frac{y_i}{1 + \\exp(y_i F_{m-1}(x_i))}\n$$\n这些值 $r_{im}$ 通常被称为“残差”或“伪残差”，是新的基学习器 $h_m$ 被训练来拟合的目标。\n\n### 任务2：总体最优得分的推导与比较\n\n总体最优得分 $F^\\star(x)$ 是在特征空间中每个点 $x$ 处最小化期望损失的函数。这是条件总体风险 $J(F) = \\mathbb{E}[\\ell(Y, F(X)) \\mid X=x]$ 的最小化器。该期望是针对随机变量 $Y$ 在给定 $X=x$ 的条件下的期望。\n\n给定 $Y \\in \\{-1, +1\\}$ 且 $\\pi(x) \\equiv \\mathbb{P}(Y=+1 \\mid X=x)$，我们有 $\\mathbb{P}(Y=-1 \\mid X=x) = 1-\\pi(x)$。逻辑斯蒂损失的条件风险是：\n$$\nJ_{\\log}(F(x)) = \\mathbb{P}(Y=+1 \\mid X=x) \\cdot \\ell_{\\log}(+1, F(x)) + \\mathbb{P}(Y=-1 \\mid X=x) \\cdot \\ell_{\\log}(-1, F(x))\n$$\n$$\nJ_{\\log}(F(x)) = \\pi(x) \\log(1+\\exp(-F(x))) + (1-\\pi(x)) \\log(1+\\exp(F(x)))\n$$\n为了找到最小化器 $F^\\star(x)$，我们将 $J_{\\log}(F(x))$ 对 $F(x)$ 求导，并令结果为零。为简单起见，我们可以将 $F(x)$ 表示为 $F$。\n$$\n\\frac{dJ_{\\log}(F)}{dF} = \\pi(x) \\left(\\frac{-\\exp(-F)}{1+\\exp(-F)}\\right) + (1-\\pi(x)) \\left(\\frac{\\exp(F)}{1+\\exp(F)}\\right)\n$$\n将导数设为零：\n$$\n\\pi(x) \\left(\\frac{-\\exp(-F)}{1+\\exp(-F)}\\right) + (1-\\pi(x)) \\left(\\frac{\\exp(F)}{1+\\exp(F)}\\right) = 0\n$$\n$$\n(1-\\pi(x)) \\frac{\\exp(F)}{1+\\exp(F)} = \\pi(x) \\frac{\\exp(-F)}{1+\\exp(-F)}\n$$\n为了简化右侧，我们将其分子和分母同乘以 $\\exp(F)$：\n$$\n\\frac{\\exp(-F)}{1+\\exp(-F)} = \\frac{\\exp(-F)\\exp(F)}{(1+\\exp(-F))\\exp(F)} = \\frac{1}{\\exp(F)+1}\n$$\n将此代回方程可得：\n$$\n(1-\\pi(x)) \\frac{\\exp(F)}{1+\\exp(F)} = \\pi(x) \\frac{1}{1+\\exp(F)}\n$$\n由于 $1+\\exp(F)$ 恒为正，我们可以两边同乘以它：\n$$\n(1-\\pi(x)) \\exp(F) = \\pi(x)\n$$\n解出 $\\exp(F)$：\n$$\n\\exp(F) = \\frac{\\pi(x)}{1-\\pi(x)}\n$$\n最后，两边取自然对数，得到逻辑斯蒂损失的总体最优得分：\n$$\nF^\\star(x) = \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n$$\n这个表达式是条件概率 $\\pi(x)$ 的对数几率（或 logit）。\n\n现在，我们对指数损失 $\\ell_{\\exp}(y,F) = \\exp(-yF)$ 进行相同的分析。条件风险是：\n$$\nJ_{\\exp}(F(x)) = \\pi(x) \\exp(-F(x)) + (1-\\pi(x)) \\exp(F(x))\n$$\n对 $F(x)$（再次表示为 $F$）求导并设为零：\n$$\n\\frac{dJ_{\\exp}(F)}{dF} = -\\pi(x) \\exp(-F) + (1-\\pi(x)) \\exp(F) = 0\n$$\n$$\n(1-\\pi(x)) \\exp(F) = \\pi(x) \\exp(-F)\n$$\n两边同乘以 $\\exp(F)$ 得：\n$$\n(1-\\pi(x)) \\exp(2F) = \\pi(x)\n$$\n$$\n\\exp(2F) = \\frac{\\pi(x)}{1-\\pi(x)}\n$$\n取自然对数并解出 $F$，得到指数损失的总体最优得分：\n$$\nF^\\star_{\\exp}(x) = \\frac{1}{2} \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n$$\n比较这两个最优得分，我们发现：\n$$\nF^\\star_{\\log}(x) = \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right) \\quad \\text{and} \\quad F^\\star_{\\exp}(x) = \\frac{1}{2} \\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)\n$$\n比较表明，使用逻辑斯蒂损失的提升法 (LogitBoost) 和使用指数损失的提升法 (AdaBoost) 都产生一个与类概率 $\\pi(x)$ 的对数几率成正比的得分函数 $F(x)$。然而，其缩放比例相差一个因子 $2$。指数损失下的得分函数收敛到对数几率的一半，而逻辑斯蒂损失下的得分函数直接收敛到对数几率。这意味着，要从学到的得分 $F(x)$ 中恢复概率估计 $\\pi(x)$，对于逻辑斯蒂损失，必须使用变换 $\\pi(x) = 1/(1+\\exp(-F(x)))$，而对于指数损失，则必须使用变换 $\\pi(x) = 1/(1+\\exp(-2F(x)))$。\n\n### 任务3：最终答案表达式\n\n最终答案是在任务2中推导出的，在逻辑斯蒂损失下总体最优得分 $F^{\\star}(x)$ 的闭式解析表达式。", "answer": "$$\n\\boxed{\\ln\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right)}\n$$", "id": "3105987"}, {"introduction": "在理解了理论之后，下一步是亲手构建算法。这个动手编程练习 [@problem_id:3125539] 要求你使用回归树桩作为弱学习器，从头开始实现一个完整的梯度提升回归器。通过实现核心的逐步拟合过程，你将对算法的运作机制获得实践性的理解，并探究一个关键超参数——学习率 $\\nu$——如何作为一种正则化形式来控制模型复杂度。", "problem": "考虑一个监督回归问题，其训练数据为 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 且 $y_i \\in \\mathbb{R}$。目标是以分阶段的方式在加性函数 $f$ 上最小化经验风险 $R(f)$。其基本出发点是，使用均方误差（MSE）损失的经验风险最小化由以下目标函数决定：\n$$\nR(f) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - f(\\mathbf{x}_i)\\right)^2.\n$$\n在函数空间中使用梯度下降的分阶段加性建模中，在每个阶段 $m$，我们选择一个弱学习器 $h_m$ 来近似在当前模型 $f_{m-1}$ 处评估的损失函数的负梯度，并按如下方式进行更新：\n$$\nf_m = f_{m-1} + \\nu \\, h_m,\n$$\n其中 $\\nu \\in (0, 1]$ 是一个分阶段收缩参数，起到隐式正则化的作用。\n\n您必须实现使用平方误差损失和轴对齐回归桩作为弱学习器的梯度提升算法。一个回归桩 $h(\\mathbf{x})$ 的定义是：选择一个特征索引 $j \\in \\{1, \\dots, d\\}$ 和一个阈值 $t \\in \\mathbb{R}$，当 $x_j  t$ 时预测常数 $a$，否则预测常数 $b$。对于目标向量 $\\mathbf{r} \\in \\mathbb{R}^n$ 的平方误差拟合，对于任何固定的 $(j,t)$，最优常数是阈值两侧目标的均值。\n\n给定一个固定的训练集，包含 $n = 14$ 个在 $d = 2$ 维空间中的点：\n$$\n\\mathbf{X} = \\big[(-2.0,-1.0),\\, (-2.0,1.0),\\, (-1.0,-1.0),\\, (-1.0,1.0),\\, (0.0,-1.0),\\, (0.0,1.0),\\, (1.0,-1.0),\\, (1.0,1.0),\\, (2.0,-1.0),\\, (2.0,1.0),\\, (-3.0,0.0),\\, (3.0,0.0),\\, (0.5,-0.5),\\, (-0.5,0.5)\\big],\n$$\n以及用于计算目标值的确定性规则\n$$\ny = 2 \\cdot \\mathbb{1}\\{x_1  0\\} \\;-\\; \\mathbb{1}\\{x_2  0\\} \\;+\\; 0.3\\,x_1 \\;+\\; 0.1\\,x_2,\n$$\n您必须使用此规则从 $\\mathbf{X}$ 精确计算 $\\mathbf{y} \\in \\mathbb{R}^{14}$（无任何随机性）。初始化 $f_0(\\mathbf{x}) \\equiv 0$，并在固定的 $M$ 下，对每个阶段 $m = 1, 2, \\dots, M$ 计算伪残差\n$$\nr_i^{(m)} = y_i - f_{m-1}(\\mathbf{x}_i),\n$$\n通过最小化训练点上的平方误差，将单个回归桩 $h_m$ 拟合到 $\\{(\\mathbf{x}_i, r_i^{(m)})\\}_{i=1}^n$，并更新 $f_m = f_{m-1} + \\nu h_m$。\n\n研究作为隐式正则化的分阶段收缩参数 $\\nu$：将提升阶段数固定为 $M = 12$，并对每个指定的 $\\nu$ 值，评估欧几里得预测范数\n$$\n\\|f_M\\|_2 \\;=\\; \\left(\\sum_{i=1}^n f_M(\\mathbf{x}_i)^2\\right)^{1/2}\n$$\n和训练损失\n$$\nR(f_M) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\big(y_i - f_M(\\mathbf{x}_i)\\big)^2.\n$$\n\n您的实现必须通过考虑所有特征 $j \\in \\{1,\\dots,d\\}$ 和所有由训练样本中选定特征的连续不同值的中点构成的阈值 $t$ 来构建回归桩。对于任何候选 $(j,t)$，最优左常数 $a$ 是满足 $x_{ij}  t$ 的索引对应目标的均值，最优右常数 $b$ 是满足 $x_{ij} \\ge t$ 的索引对应目标的均值。在所有特征的所有候选项中，选择在训练数据上最小化残差平方和的那个桩。如果一个特征没有有效的分裂点（所有值都相等），则跳过该特征；如果所有特征都没有有效的分裂点，则使用等于目标全局均值的常数预测器作为桩。\n\n测试套件：\n- 精确使用五个收缩值 $\\nu \\in \\{0.01,\\, 0.1,\\, 0.3,\\, 0.7,\\, 1.0\\}$。\n- 对所有测试，固定 $M = 12$ 个阶段。\n- 对于每个 $\\nu$，计算数对 $\\left(\\|f_M\\|_2,\\, R(f_M)\\right)$，并四舍五入到6位小数。\n\n答案规格：\n- 对于给定顺序中的每个 $\\nu$，将数对 $\\left[\\|f_M\\|_2, R(f_M)\\right]$ 作为包含两个浮点数的列表输出。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个数对也用自己的方括号括起来，且没有空格。例如，输出应如下所示\n$$\n\\big[[\\|f_M\\|_2^{(1)}, R(f_M)^{(1)}],[\\|f_M\\|_2^{(2)}, R(f_M)^{(2)}],\\dots\\big],\n$$\n具体呈现为单行字符串，如 \"[[n1,l1],[n2,l2],[n3,l3],[n4,l4],[n5,l5]]\"，其中每个 $n_k$ 和 $l_k$ 都是四舍五入到6位小数的浮点数。", "solution": "用户要求实现一个用于回归问题的梯度提升算法，该算法使用回归桩作为弱学习器。目标是评估收缩参数 $\\nu$ 对最终模型的预测范数和训练损失的影响。该过程首先对问题陈述进行形式化验证，我们发现该问题陈述在科学上是合理的、适定的且完整的。\n\n### 使用平方误差损失的梯度提升原理\n\n梯度提升是一种集成学习方法，它以分阶段的方式从一组弱学习器中构建出一个强大的预测模型。其核心思想是将优化问题视为函数空间中的一种梯度下降。\n\n目标是找到一个函数 $f(\\mathbf{x})$，以最小化由损失函数 $L(y, f(\\mathbf{x}))$ 定义的经验风险。对于本问题，指定的损失是均方误差（MSE），经验风险由下式给出：\n$$\nR(f) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(\\mathbf{x}_i)) = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_i - f(\\mathbf{x}_i)\\right)^2\n$$\n其中 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ 是训练集，包含 $n=14$ 个样本，且 $\\mathbf{x}_i \\in \\mathbb{R}^2$。\n\n该算法按以下步骤进行：\n\n1.  **初始化**：模型用一个常数值进行初始化。问题指定初始模型为 $f_0(\\mathbf{x}) \\equiv 0$。因此，在训练数据上的预测向量为 $\\mathbf{f}_0 = \\mathbf{0} \\in \\mathbb{R}^n$。\n\n2.  **分阶段加性建模**：对于每个阶段 $m = 1, 2, \\dots, M$，模型进行迭代更新。在函数梯度下降的背景下，我们寻求添加一个函数 $h_m$，它能将当前模型 $f_{m-1}$ 沿损失函数的负梯度方向移动。关于函数值 $f(\\mathbf{x}_i)$ 的负梯度分量为：\n    $$\n    r_i^{(m)} = -\\left[\\frac{\\partial L(y_i, F)}{\\partial F}\\right]_{F=f_{m-1}(\\mathbf{x}_i)} = -\\left[\\frac{\\partial}{\\partial F} \\frac{1}{2}(y_i - F)^2\\right]_{F=f_{m-1}(\\mathbf{x}_i)} = y_i - f_{m-1}(\\mathbf{x}_i)\n    $$\n    这些量 $r_i^{(m)}$ 被称为第 $m$ 阶段的伪残差。\n\n3.  **弱学习器拟合**：一个弱学习器，在本例中是一个回归桩 $h_m(\\mathbf{x})$，被拟合来预测伪残差。这是通过解决以下最小二乘问题来完成的：\n    $$\n    h_m = \\arg\\min_{h} \\sum_{i=1}^n \\left(r_i^{(m)} - h(\\mathbf{x}_i)\\right)^2\n    $$\n\n4.  **模型更新**：通过添加由收缩参数 $\\nu$ 缩放的新弱学习器来更新完整模型：\n    $$\n    f_m(\\mathbf{x}) = f_{m-1}(\\mathbf{x}) + \\nu h_m(\\mathbf{x})\n    $$\n    收缩参数 $\\nu \\in (0, 1]$ 减小了每个弱学习器的影响，这起到了一种正则化作用，以防止过拟合。\n\n此过程重复固定的阶段数，即 $M=12$。\n\n### 拟合回归桩\n\n回归桩是只有一个分裂的决策树。它由一个特征索引 $j \\in \\{1, \\dots, d\\}$、一个阈值 $t \\in \\mathbb{R}$ 以及两个常数输出值 $a$ 和 $b$ 定义。其函数形式为：\n$$\nh(\\mathbf{x}; j, t, a, b) = a \\cdot \\mathbb{1}\\{x_j  t\\} + b \\cdot \\mathbb{1}\\{x_j \\ge t\\}\n$$\n其中 $\\mathbb{1}\\{\\cdot\\}$ 是指示函数。\n\n为了找到给定一组伪残差 $\\{r_i^{(m)}\\}_{i=1}^n$ 的最优桩 $h_m$，执行以下步骤：\n1.  对于每个特征 $j \\in \\{1, 2\\}$：\n    a.  识别训练数据 $\\mathbf{X}$ 中特征 $j$ 的一组唯一值 $\\{v_1, v_2, \\dots, v_k\\}$。\n    b.  通过取每对连续唯一值的中点来构建一组候选阈值 $\\{t_s\\}_{s=1}^{k-1}$：$t_s = (v_s + v_{s+1})/2$。\n    c.  对于每个候选 $(j, t_s)$：\n        i.   将样本索引划分为两个集合：$I_{left} = \\{i \\mid x_{ij}  t_s\\}$ 和 $I_{right} = \\{i \\mid x_{ij} \\ge t_s\\}$。\n        ii.  对于此分裂，最小化平方误差和的最优常数 $a$ 和 $b$ 是每个分区内残差的均值：\n             $$\n             a = \\frac{1}{|I_{left}|} \\sum_{i \\in I_{left}} r_i^{(m)}, \\quad b = \\frac{1}{|I_{right}|} \\sum_{i \\in I_{right}} r_i^{(m)}\n             $$\n        iii. 计算此桩的平方误差和 (SSE)：\n             $$\n             SSE(j, t_s) = \\sum_{i \\in I_{left}} (r_i^{(m)} - a)^2 + \\sum_{i \\in I_{right}} (r_i^{(m)} - b)^2\n             $$\n2.  最佳桩 $(j^*, t^*, a^*, b^*)$ 是在所有可能的特征 $j$ 和阈值 $t_s$ 中产生最小 SSE 的那个。如果任何特征都不可能进行有效的分裂（例如，如果所有样本具有相同的特征向量），则该桩默认使用等于残差全局均值的常数预测器。\n\n### 执行计划\n\n对于每个指定的收缩值 $\\nu \\in \\{0.01, 0.1, 0.3, 0.7, 1.0\\}$，实现过程遵循以下步骤：\n1.  **数据准备**：使用给定的确定性规则从特征矩阵 $\\mathbf{X}$ 计算目标值 $y_i$：\n    $$\n    y_i = 2 \\cdot \\mathbb{1}\\{x_{i1} > 0\\} - \\mathbb{1}\\{x_{i2}  0\\} + 0.3x_{i1} + 0.1x_{i2}\n    $$\n2.  **提升循环**：\n    - 将预测向量 $\\mathbf{f}_0$ 初始化为全零。\n    - 对于 $m = 1, \\dots, 12$：\n        a. 计算伪残差 $\\mathbf{r}^{(m)} = \\mathbf{y} - \\mathbf{f}_{m-1}$。\n        b. 将最优回归桩 $h_m$ 拟合到数据对 $(\\mathbf{X}, \\mathbf{r}^{(m)})$。\n        c. 从拟合好的桩在训练数据 $\\mathbf{X}$ 上获取预测值 $\\mathbf{h}_m$。\n        d. 更新模型预测：$\\mathbf{f}_m = \\mathbf{f}_{m-1} + \\nu \\mathbf{h}_m$。\n3.  **评估**：经过 $M=12$ 个阶段后，最终的预测向量为 $\\mathbf{f}_M$。计算以下两个量：\n    - **欧几里得预测范数**：$\\|f_M\\|_2 = \\sqrt{\\sum_{i=1}^n f_M(\\mathbf{x}_i)^2} = \\|\\mathbf{f}_M\\|_2$。\n    - **训练损失（MSE）**：$R(f_M) = \\frac{1}{n} \\sum_{i=1}^n (y_i - f_M(\\mathbf{x}_i))^2$。\n4.  **输出格式化**：将得到的数对 $(\\|f_M\\|_2, R(f_M))$ 四舍五入到6位小数并存储。最终输出是一个列表，其中包含每个 $\\nu$ 值对应的这些数对。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Gradient Boosting with Regression Stumps for a specified regression task.\n    \"\"\"\n    \n    # --- Problem Givens ---\n    X_data = np.array([\n        [-2.0, -1.0], [-2.0, 1.0], [-1.0, -1.0], [-1.0, 1.0],\n        [0.0, -1.0], [0.0, 1.0], [1.0, -1.0], [1.0, 1.0],\n        [2.0, -1.0], [2.0, 1.0], [-3.0, 0.0], [3.0, 0.0],\n        [0.5, -0.5], [-0.5, 0.5]\n    ])\n    n_samples, n_features = X_data.shape\n    M = 12\n    nu_values = [0.01, 0.1, 0.3, 0.7, 1.0]\n\n    def compute_y(X):\n        \"\"\"Computes the target values based on the given deterministic rule.\"\"\"\n        x1 = X[:, 0]\n        x2 = X[:, 1]\n        y = (2.0 * (x1 > 0) - 1.0 * (x2  0) + 0.3 * x1 + 0.1 * x2)\n        return y\n\n    class RegressionStump:\n        \"\"\"A regression stump weak learner.\"\"\"\n        def __init__(self):\n            self.feature_index = None\n            self.threshold = None\n            self.left_value = None\n            self.right_value = None\n\n        def fit(self, X, y):\n            \"\"\"\n            Finds the best split (feature and threshold) for the stump.\n            The best split is the one that minimizes the sum of squared errors.\n            \"\"\"\n            n_samples, n_features = X.shape\n            best_sse = float('inf')\n            \n            # Default prediction is the global mean if no split is found\n            global_mean = np.mean(y)\n\n            for j in range(n_features):\n                feature_values = X[:, j]\n                unique_values = np.unique(feature_values)\n\n                if len(unique_values) = 1:\n                    continue  # Cannot split on this feature\n\n                thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n\n                for t in thresholds:\n                    left_indices = np.where(feature_values  t)[0]\n                    right_indices = np.where(feature_values >= t)[0]\n                    \n                    if len(left_indices) == 0 or len(right_indices) == 0:\n                        continue\n\n                    y_left, y_right = y[left_indices], y[right_indices]\n                    \n                    mean_left = np.mean(y_left)\n                    mean_right = np.mean(y_right)\n                    \n                    sse = np.sum((y_left - mean_left)**2) + np.sum((y_right - mean_right)**2)\n\n                    if sse  best_sse:\n                        best_sse = sse\n                        self.feature_index = j\n                        self.threshold = t\n                        self.left_value = mean_left\n                        self.right_value = mean_right\n            \n            # If no split improved upon the initial 'inf' sse, it means no valid split was found.\n            # In this case, use the global mean.\n            if self.feature_index is None:\n                self.left_value = global_mean\n                self.right_value = global_mean\n\n        def predict(self, X):\n            \"\"\"Makes predictions using the fitted stump.\"\"\"\n            if self.feature_index is None:\n                # No split was found, predict the global mean for all samples.\n                return np.full(X.shape[0], self.left_value)\n            \n            feature_values = X[:, self.feature_index]\n            return np.where(feature_values  self.threshold, self.left_value, self.right_value)\n\n    # --- Main Execution Logic ---\n    y_true = compute_y(X_data)\n    results = []\n\n    for nu in nu_values:\n        f_m_preds = np.zeros(n_samples)  # f_0(x) = 0\n\n        for _ in range(M):\n            residuals = y_true - f_m_preds\n            \n            stump = RegressionStump()\n            stump.fit(X_data, residuals)\n            \n            h_m_preds = stump.predict(X_data)\n            \n            f_m_preds += nu * h_m_preds\n\n        # After M stages, f_m_preds holds the final predictions f_M\n        norm_fM = np.linalg.norm(f_m_preds)\n        loss_fM = np.mean((y_true - f_m_preds)**2)\n\n        results.append([round(norm_fM, 6), round(loss_fM, 6)])\n\n    # Format the final output string as specified\n    formatted_pairs = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(formatted_pairs)}]\")\n\nsolve()\n```", "id": "3125539"}, {"introduction": "标准的平方误差损失对异常值高度敏感。我们可以通过选择一个受大误差影响较小的损失函数（如Huber损失）来使我们的模型更加稳健。在这个实践中 [@problem_id:3125607]，你将扩展你的梯度提升实现，比较标准 $L_2$ 损失和稳健的Huber损失在存在重尾噪声时的性能，这展示了一种构建可靠的真实世界模型的关键技术。", "problem": "要求您实现一个小型的、自包含的实验，比较在观测噪声具有重尾分布时，梯度提升回归在两种不同损失函数下的行为。您将使用经验风险最小化和分步加性模型的框架。实验设置如下。\n\n假设输入为标量 $x \\in \\mathbb{R}$，真实回归函数为 $f^*(x) = 2x$。观测值根据 $y = f^*(x) + \\epsilon$ 生成，其中 $\\epsilon$ 服从自由度为 $\\nu$ 的学生t分布，并在 $\\nu  2$ 时进行缩放以使方差为单位1。具体来说，如果 $t \\sim t_\\nu$ 是一个标准的学生t变量，则定义 $\\epsilon = t \\cdot \\sqrt{\\frac{\\nu - 2}{\\nu}}$，这样当 $\\nu  2$ 时 $\\mathrm{Var}(\\epsilon) = 1$。\n\n您将构建两个梯度提升回归器，它们仅在损失函数的选择上有所不同：\n- 最小二乘损失 $L_2$：$\\ell(y, F) = \\frac{1}{2}(y - F)^2$。\n- 带阈值 $\\delta  0$ 的Huber损失：\n  $$\\ell(y,F) = \\begin{cases}\n  \\frac{1}{2}(y - F)^2,  \\text{if } |y - F| \\le \\delta,\\\\\n  \\delta |y - F| - \\frac{1}{2}\\delta^2,  \\text{otherwise.}\n  \\end{cases}$$\n\n两个模型都必须实现为分步加性模型 $F_m(x) = F_{m-1}(x) + \\eta \\, h_m(x)$，其中 $m = 1,2,\\dots,M$，$\\eta \\in (0,1]$ 是给定的学习率，$h_m$ 是一个决策树桩弱学习器，它将 $x$ 映射到两个区域 $(-\\infty, s]$ 和 $(s, \\infty)$，并根据当前伪残差上的最小二乘法确定区域特定的常数预测值。初始预测 $F_0$ 必须在常数范围内最小化所选损失的经验风险：\n- 对于 $L_2$ 损失，这是 $y$ 的经验均值。\n- 对于带阈值 $\\delta$ 的Huber损失，最优常数 $c$ 满足 $\\sum_{i=1}^n \\psi(y_i - c) = 0$，其中当 $|r| \\le \\delta$ 时 $\\psi(r) = r$，否则 $\\psi(r) = \\delta \\,\\mathrm{sign}(r)$。您必须通过求解一维凸函数求根问题 $\\sum_{i=1}^n \\psi(y_i - c) = 0$ 来计算这个 $c$，使用一种能保证找到解的区间法。\n\n在每次提升迭代 $m$ 中，您必须：\n- 计算关于预测的每个样本的梯度 $g_i^{(m)} = \\frac{\\partial \\ell(y_i, F_{m-1}(x_i))}{\\partial F}$，在当前模型 $F_{m-1}$ 处求值。对于 $L_2$ 损失，这等于 $F_{m-1}(x_i) - y_i$。对于Huber损失，它等于 $-\\psi(y_i - F_{m-1}(x_i))$，使用与上面相同的 $\\psi$。\n- 将伪残差定义为负梯度 $r_i^{(m)} = - g_i^{(m)}$。\n- 通过最小化所有单阈值分裂下的经验平方误差，将决策树桩 $h_m$ 拟合到目标 $\\{r_i^{(m)}\\}_{i=1}^n$。对于固定的分裂阈值 $s$，最优的区域常数是分裂两侧目标的经验均值。您必须选择能在两个区域上产生最小平方误差总和的分裂。\n\n您必须为每种损失评估以下两个统计量：\n- 经过 $M$ 次迭代后的最终中位数绝对误差：$\\mathrm{MedAE} = \\mathrm{median}_i\\left(|y_i - F_M(x_i)|\\right)$。\n- 所有提升迭代中梯度幅值中位数的平均值，定义为\n  $$\\overline{G} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{median}_i\\left(|g_i^{(m)}|\\right),$$\n  其中 $g_i^{(m)}$ 如上定义，对于每个 $m$，中位数是根据样本索引 $i \\in \\{1,\\dots,n\\}$ 计算的。\n\n实现上述过程，并在以下测试集上运行它。对于每个测试用例，从区间 $[-1, 1]$ 上的均匀分布中独立同分布地生成 $n$ 个输入 $x_i$，并使用指定的自由度 $\\nu$ 生成如上所述的输出。使用指定的随机种子以保证可复现性。使用相同的数据集训练两种损失的模型，并在该数据集上报告指标。两种损失使用相同的弱学习器类别和学习率。\n\n测试集参数组，每个参数组为 $(\\nu, \\delta, M, \\eta, n, \\text{seed})$：\n- 案例 1：$(\\nu=\\;3.0,\\; \\delta=\\;1.0,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12345)$。\n- 案例 2：$(\\nu=\\;10.0,\\; \\delta=\\;1.0,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12346)$。\n- 案例 3：$(\\nu=\\;2.5,\\; \\delta=\\;0.5,\\; M=\\;30,\\; \\eta=\\;0.1,\\; n=\\;256,\\; \\text{seed}=\\;12347)$。\n\n您的程序必须生成单行输出，其中包含一个列表的列表，每个子列表对应一个测试用例，每个子列表等于 \n$[\\mathrm{MedAE}_{L_2}, \\mathrm{MedAE}_{\\text{Huber}}, \\overline{G}_{L_2}, \\overline{G}_{\\text{Huber}}]$，并四舍五入到六位小数。确切的输出格式必须是单行字符串，形式为 \n$[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}]]$ \n，其中不含任何空格。\n\n您必须使用的基本原理：\n- 函数 $F$ 的经验风险最小化：最小化 $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, F(x_i))$。\n- 分步加性建模：$F_m = F_{m-1} + \\eta h_m$，其中选择 $h_m$ 以降低经验风险。\n- 负梯度拟合：在迭代 $m$ 时，将 $h_m$ 拟合到目标 $r_i^{(m)} = -\\left.\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F}\\right|_{F=F_{m-1}}$。\n- 区域上的最小二乘最优常数：该区域上目标的经验均值最小化了平方误差之和。\n\n不应假设任何其他公式。您必须在实现中从这些原则推导出所有额外的步骤。本问题中没有物理单位。不使用角度。不使用百分比。\n\n您的程序应生成单行输出，包含一个用方括号括起来的、逗号分隔的列表的列表，例如 $[[r_{11},r_{12},r_{13},r_{14}],[r_{21},r_{22},r_{23},r_{24}],[r_{31},r_{32},r_{33},r_{34}]]$.", "solution": "该问题要求实现并比较两种梯度提升回归模型，一种使用标准的最小二乘 ($L_2$) 损失，另一种使用更稳健的Huber损失。该比较在一个观测噪声呈现重尾特性的设置下进行，这种特性通过使用学生t分布来模拟。实现基于经验风险最小化和分步加性建模的原则。\n\n模型的一般结构是一个分步加性函数，形式如下：\n$$F_m(x) = F_{m-1}(x) + \\eta \\, h_m(x)$$\n其中 $m=1, 2, \\dots, M$ 是提升迭代次数，$\\eta \\in (0, 1]$ 是学习率，$h_m(x)$ 是一个称为弱学习器的简单函数。整体模型 $F_M(x)$ 是对真实函数 $f^*(x)$ 的近似。该过程从一个初始模型 $F_0(x)$ 开始，它是一个常数值，被选择用来最小化所选损失函数的经验风险。\n\n在每次迭代 $m$ 中，梯度提升算法通过添加一个指向损失曲面上最速下降方向的弱学习器来寻求改进模型。这个方向由损失函数相对于当前模型预测值的负梯度给出。每个样本的负梯度被称为伪残差：\n$$r_i^{(m)} = -\\left[\\frac{\\partial \\ell(y_i, F(x_i))}{\\partial F}\\right]_{F=F_{m-1}}$$\n然后将弱学习器 $h_m(x)$ 拟合到这些伪残差上，通常通过最小化平方误差来实现。\n\n指定的弱学习器是一个决策树桩，它在分裂点 $s$ 处将输入空间 $\\mathbb{R}$ 分割为两个区域：$(-\\infty, s]$ 和 $(s, \\infty)$。树桩在每个区域产生一个恒定的预测值。为了将树桩 $h_m$ 拟合到伪残差 $\\{r_i^{(m)}\\}_{i=1}^n$，我们寻找分裂点 $s$ 和两个恒定的预测值 $c_1$ 和 $c_2$，以最小化平方误差之和：\n$$\\sum_{i: x_i \\le s} (r_i^{(m)} - c_1)^2 + \\sum_{i: x_i  s} (r_i^{(m)} - c_2)^2$$\n对于任何给定的分裂 $s$，最优常数是各自区域中伪残差的均值。通过遍历所有可能的分裂点并选择导致总平方误差和最小的分裂点来找到最佳分裂点 $s$。一个高效的算法可以在 $O(n \\log n)$ 时间内找到最优分裂，方法是首先按特征 $x$ 对数据进行排序，然后使用累加和来计算每个潜在分裂的平方误差和。\n\n每种损失函数的具体细节如下：\n\n**1. 最小二乘 ($L_2$) 损失**\n损失函数为 $\\ell(y, F) = \\frac{1}{2}(y - F)^2$。\n- **初始预测 $F_0$**：为了最小化经验风险 $\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}(y_i - c)^2$，我们将关于 $c$ 的导数设为零：$\\sum_i -(y_i - c) = 0$。这得到最优常数 $c = \\frac{1}{n}\\sum_{i=1}^n y_i$，即目标的样本均值。所以，$F_0(x) = \\bar{y}$。\n- **伪残差**：在 $F_{m-1}(x_i)$ 处，关于预测 $F$ 的梯度是 $\\frac{\\partial \\ell}{\\partial F} = F - y$。因此，伪残差为：\n$$r_i^{(m)} = -(F_{m-1}(x_i) - y_i) = y_i - F_{m-1}(x_i)$$\n这些就是模型在第 $m-1$ 次迭代时的普通残差。\n\n**2. Huber损失**\n带阈值 $\\delta  0$ 的Huber损失函数定义为：\n$$\\ell(y,F) = \\begin{cases}\n  \\frac{1}{2}(y - F)^2,  \\text{if } |y - F| \\le \\delta \\\\\n  \\delta |y - F| - \\frac{1}{2}\\delta^2,  \\text{otherwise.}\n  \\end{cases}$$\n这个损失函数对于小误差表现为二次方，对于大误差表现为线性，这使得它比 $L_2$ 损失对异常值不那么敏感。\n- **初始预测 $F_0$**：最优常数 $c$ 最小化经验风险 $\\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, c)$。最优性条件是总损失关于 $c$ 的导数为零。这导出了方程 $\\sum_{i=1}^n \\psi(y_i - c) = 0$，其中 $\\psi$ 是Huber损失对其参数 $r=y-F$ 的导数：\n$$\\psi(r) = \\frac{\\partial \\ell}{\\partial r} = \\begin{cases}\n  r,  \\text{if } |r| \\le \\delta \\\\\n  \\delta \\cdot \\mathrm{sign}(r),  \\text{otherwise.}\n  \\end{cases}$$\n函数 $g(c) = \\sum_{i=1}^n \\psi(y_i - c)$ 是单调且连续的，因此可以在一个合适的区间（如 $[\\min(y), \\max(y)]$）上使用二分法可靠地找到其根。\n- **伪残差**：在 $F_{m-1}(x_i)$ 处，关于预测 $F$ 的梯度是 $\\frac{\\partial \\ell}{\\partial F} = -\\psi(y - F)$。因此，伪残差为：\n$$r_i^{(m)} = -(-\\psi(y_i - F_{m-1}(x_i))) = \\psi(y_i - F_{m-1}(x_i))$$\n这意味着残差 $y_i - F_{m-1}(x_i)$ 会通过裁剪函数 $\\psi$ 来生成弱学习器的目标。这种对大残差的抑制是Huber方法稳健性的核心。\n\n对于指定的每个测试用例，执行以下步骤：\n1.  生成一个包含 $n$ 个点 $(x_i, y_i)$ 的数据集。输入 $x_i$ 从 Uniform$(-1, 1)$ 分布中抽取。输出生成为 $y_i = 2x_i + \\epsilon_i$，其中噪声 $\\epsilon_i$ 从自由度为 $\\nu$ 的学生t分布中采样，并乘以 $\\sqrt{(\\nu-2)/\\nu}$ 以确保当 $\\nu  2$ 时方差为1。\n2.  在此数据集上训练两个梯度提升模型，迭代 $M$ 次，学习率为 $\\eta$：一个使用 $L_2$ 损失，另一个使用Huber损失。\n3.  对每个模型，计算两个指标：\n    - 最终中位数绝对误差：$\\mathrm{MedAE} = \\mathrm{median}_i(|y_i - F_M(x_i)|)$。\n    - 平均中位数梯度幅值：$\\overline{G} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{median}_i(|g_i^{(m)}|)$，其中 $g_i^{(m)}$ 是第 $m$ 次迭代时每个样本的梯度。\n\n该实现将这些原则封装到一个主函数中，该主函数遍历所有测试用例，还有用于运行给定损失的提升算法的函数，以及用于拟合决策树桩和为Huber损失寻找初始常数的辅助工具。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as t_dist\nfrom scipy.optimize import bisect\n\nclass DecisionStump:\n    \"\"\"A decision stump for 1D regression.\"\"\"\n    def __init__(self, split_val, left_pred, right_pred):\n        self.split_val = split_val\n        self.left_pred = left_pred\n        self.right_pred = right_pred\n\n    def predict(self, x):\n        \"\"\"Predicts the output for a given input array x.\"\"\"\n        if self.split_val is None:\n            # If no split, predict the same value for all inputs.\n            return np.full_like(x, self.left_pred, dtype=float)\n        \n        predictions = np.full_like(x, self.right_pred, dtype=float)\n        predictions[x = self.split_val] = self.left_pred\n        return predictions\n\ndef fit_stump(x, r):\n    \"\"\"\n    Fits a decision stump to predict residuals r from feature x.\n    The stump is chosen to minimize the sum of squared errors.\n    \"\"\"\n    n = len(x)\n    if n == 0:\n        return DecisionStump(None, 0.0, 0.0)\n\n    sort_idx = np.argsort(x)\n    x_sorted, r_sorted = x[sort_idx], r[sort_idx]\n\n    best_sse = np.inf\n    best_split_val = None\n    best_left_pred = None\n    best_right_pred = None\n\n    r_sum_total = np.sum(r_sorted)\n    r2_sum_total = np.sum(r_sorted**2)\n\n    left_sum_r = 0.0\n    left_sum_r2 = 0.0\n\n    for i in range(n - 1):\n        left_sum_r += r_sorted[i]\n        left_sum_r2 += r_sorted[i]**2\n        n_left = i + 1\n        \n        # Avoid splitting between identical x values\n        if x_sorted[i] == x_sorted[i+1]:\n            continue\n\n        n_right = n - n_left\n        right_sum_r = r_sum_total - left_sum_r\n        right_sum_r2 = r2_sum_total - left_sum_r2\n\n        sse_left = left_sum_r2 - (left_sum_r**2) / n_left\n        sse_right = right_sum_r2 - (right_sum_r**2) / n_right\n        total_sse = sse_left + sse_right\n\n        if total_sse  best_sse:\n            best_sse = total_sse\n            best_split_val = (x_sorted[i] + x_sorted[i+1]) / 2.0\n            best_left_pred = left_sum_r / n_left\n            best_right_pred = right_sum_r / n_right\n    \n    if best_split_val is None:\n        # All x values are the same, no split is possible.\n        pred = r.mean()\n        return DecisionStump(None, pred, pred)\n    \n    return DecisionStump(best_split_val, best_left_pred, best_right_pred)\n\ndef find_huber_initial_constant(y, delta):\n    \"\"\"Finds the optimal initial constant for Huber loss via root-finding.\"\"\"\n    def huber_obj_grad(c):\n        residuals = y - c\n        return np.sum(np.clip(residuals, -delta, delta))\n\n    # The optimal constant is guaranteed to be within the range of y\n    y_min, y_max = np.min(y), np.max(y)\n    if y_min == y_max:\n        return y_min\n        \n    return bisect(huber_obj_grad, y_min, y_max)\n\ndef run_gbm(x, y, M, eta, loss_type, delta):\n    \"\"\"Runs a gradient boosting machine for regression.\"\"\"\n    n = len(y)\n    \n    # Initialize model F_0\n    if loss_type == 'l2':\n        F = np.full(n, np.mean(y))\n    elif loss_type == 'huber':\n        f0 = find_huber_initial_constant(y, delta)\n        F = np.full(n, f0)\n    else:\n        raise ValueError(\"Unknown loss type\")\n        \n    all_median_gradients = []\n\n    for _ in range(M):\n        # Compute pseudo-residuals (negative gradients)\n        if loss_type == 'l2':\n            gradients = F - y\n            residuals = -gradients\n        elif loss_type == 'huber':\n            residuals_unclipped = y - F\n            residuals = np.clip(residuals_unclipped, -delta, delta)\n            gradients = -residuals\n        \n        all_median_gradients.append(np.median(np.abs(gradients)))\n\n        # Fit a weak learner (decision stump) to the pseudo-residuals\n        stump = fit_stump(x, residuals)\n        \n        # Update the model\n        F += eta * stump.predict(x)\n        \n    # Calculate final metrics\n    med_ae = np.median(np.abs(y - F))\n    avg_med_grad = np.mean(all_median_gradients)\n    \n    return med_ae, avg_med_grad\n\n\ndef solve():\n    \"\"\"Main function to run the specified test suite and print results.\"\"\"\n    test_cases = [\n        # (nu,      delta, M,  eta,   n,   seed)\n        (3.0,     1.0,   30, 0.1, 256, 12345),\n        (10.0,    1.0,   30, 0.1, 256, 12346),\n        (2.5,     0.5,   30, 0.1, 256, 12347)\n    ]\n    \n    all_case_results = []\n    \n    for nu, delta, M, eta, n, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        # Generate data\n        x = rng.uniform(low=-1.0, high=1.0, size=n)\n        \n        # Generate noise from scaled Student's t-distribution\n        t_samples = t_dist.rvs(df=nu, size=n, random_state=rng)\n        # Scale to unit variance (valid since nu > 2 for all cases)\n        noise = t_samples * np.sqrt((nu - 2.0) / nu)\n        \n        # Generate observations\n        y = 2.0 * x + noise\n\n        # Run L2 model\n        l2_medae, l2_avg_g = run_gbm(x, y, M, eta, 'l2', delta=None)\n        \n        # Run Huber model\n        huber_medae, huber_avg_g = run_gbm(x, y, M, eta, 'huber', delta=delta)\n        \n        all_case_results.append([l2_medae, huber_medae, l2_avg_g, huber_avg_g])\n\n    # Format the output string\n    output_parts = []\n    for case_res in all_case_results:\n        formatted_res = ','.join([f\"{val:.6f}\" for val in case_res])\n        output_parts.append(f\"[{formatted_res}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3125607"}]}