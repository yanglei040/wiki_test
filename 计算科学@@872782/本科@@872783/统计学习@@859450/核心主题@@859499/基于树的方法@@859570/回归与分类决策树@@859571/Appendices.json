{"hands_on_practices": [{"introduction": "线性模型在处理非线性关系时会遇到困难，一个经典的例子就是“异或”（XOR）问题。本练习将通过编码实践，直观地展示决策树如何利用其轴对齐的切分（axis-aligned splits）轻松地捕捉这种非线性结构，并与线性模型的表现进行对比。通过这个练习，你将深入理解决策树在处理复杂决策边界时的独特优势。[@problem_id:3113048]", "problem": "构建一个程序，该程序模拟具有异或（XOR）结构的二元分类数据，并比较深度为$2$的决策树分类器与线性最小二乘分类器的性能。数据生成过程定义如下。从单位区间上的连续均匀分布中独立抽取特征 $x_1$ 和 $x_2$，即 $x_1 \\sim \\text{Uniform}(0,1)$ 和 $x_2 \\sim \\text{Uniform}(0,1)$。对于给定的阈值 $a \\in (0,1)$ 和 $b \\in (0,1)$，通过异或规则定义标签\n$$\nY = \\mathbf{1}\\big((x_1 > a) \\oplus (x_2 > b)\\big),\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数，$\\oplus$ 表示逻辑异或。以 $\\eta \\in [0,1/2)$ 的比率，通过以概率 $\\eta$ 翻转 $Y$ 来独立地用对称标签噪声污染标签。\n\n从基本原理开始实现两个分类器：\n\n- 线性最小二乘分类器：选择系数 $\\mathbf{w} \\in \\mathbb{R}^3$ 以最小化训练集上线性分数 $f(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2$ 与 $\\{0,1\\}$ 中二元标签之间的经验均方误差。通过 $\\hat{Y} = \\mathbf{1}(f(\\mathbf{x}) \\ge 0.5)$ 进行分类。报告在一个独立抽取的测试集上的错分率。\n\n- 最大深度为 $2$ 的决策树分类器，具有轴对齐分裂，使用Gini不纯度贪心构建。在任何包含标签多重集的节点上，将Gini不纯度定义为\n$$\nG = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2,\n$$\n其中 $p_k$ 是该节点处类别 $k$ 的经验比例。对于在阈值 $t$ 处对特征 $j \\in \\{1,2\\}$ 进行的候选分裂，将节点划分为左子节点 $\\{x_j \\le t\\}$ 和右子节点 $\\{x_j > t\\}$，并选择使子节点不纯度的加权平均值最小化的分裂。将候选阈值限制在节点处观测到的连续排序唯一特征值之间的中点。当达到最大深度 $2$ 或节点为纯节点时，停止分裂。在叶节点预测多数类，平局时选择类别 $0$。如果多个分裂产生相同的不纯度，则通过选择较小的特征索引来打破平局，如果仍然平局，则选择较小的阈值。\n\n对于下面的每个测试用例，您必须：\n\n- 使用指定的参数 $(a,b,\\eta)$ 和独立的随机种子为抽样生成大小为 $n_{\\text{train}}$ 的训练集和大小为 $n_{\\text{test}}$ 的独立测试集。使用提供的确切种子以确保可复现性。\n\n- 仅使用训练集训练两个模型。\n\n- 计算每个模型在测试集上的测试错分率，即 $\\hat{Y} \\ne Y$ 的经验比例。\n\n- 对于每个测试用例，返回一对实数 $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$。\n\n测试套件（每行表示 $(n_{\\text{train}}, n_{\\text{test}}, a, b, \\eta, \\text{seed}_{\\text{train}}, \\text{seed}_{\\text{test}})$）：\n\n- 用例 $1$：(400, 5000, 0.5, 0.5, 0.0, 42, 4242)。\n- 用例 $2$：(400, 5000, 0.2, 0.8, 0.0, 1, 11)。\n- 用例 $3$：(2000, 5000, 0.5, 0.5, 0.1, 7, 77)。\n- 用例 $4$：(30, 5000, 0.5, 0.5, 0.0, 2024, 2025)。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素本身是对应于各个用例的双元素列表 $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$。例如，输出应如下所示\n$$\n[[e_1^{\\text{lin}}, e_1^{\\text{tree}}],[e_2^{\\text{lin}}, e_2^{\\text{tree}}],[e_3^{\\text{lin}}, e_3^{\\text{tree}}],[e_4^{\\text{lin}}, e_4^{\\text{tree}}]]\n$$\n每个 $e_i^{\\cdot}$ 都是一个实数。不应打印任何附加文本。", "solution": "该问题要求在一个表现出异或（XOR）结构的合成数据集上，实现并比较两种不同的分类算法。问题陈述的有效性得到了确认，因为它在科学上基于统计学习理论，通过特定的数据生成协议和确定性的算法定义得到了良好设定，并且其表述是客观的。我们将进行完整的求解。\n\n该问题的核心在于数据生成过程。特征 $x_1$ 和 $x_2$ 从区间 $[0,1]$ 上的独立均匀分布中抽取。二元标签 $Y$ 由这些特征是否分别超过给定阈值 $a$ 和 $b$ 的异或运算定义：$Y = \\mathbf{1}\\big((x_1 > a) \\oplus (x_2 > b)\\big)$。此规则将单位正方形特征空间划分为四个象限，标签在 $0$ 和 $1$ 之间交替。这种结构不是线性可分的，对线性模型提出了一个经典的挑战。通过以指定的概率 $\\eta$ 翻转真实标签 $Y$ 来引入标签噪声。\n\n我们将构建和评估两种分类器：线性最小二乘分类器和深度为 $2$ 的决策树。\n\n**1. 线性最小二乘分类器**\n\n该方法将线性回归模型重新用于分类任务。该模型假定特征与分数函数之间存在线性关系，$f(\\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2$。对于一个训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$，其中 $\\mathbf{x}_i = (x_{i1}, x_{i2})$ 且 $y_i \\in \\{0, 1\\}$，选择系数 $\\mathbf{w} = (w_0, w_1, w_2)^T$ 以最小化经验均方误差（MSE）：\n$$\n\\text{MSE}(\\mathbf{w}) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} (y_i - f(\\mathbf{x}_i))^2\n$$\n这是一个标准的普通最小二乘法（OLS）问题。通过定义一个大小为 $n_{\\text{train}} \\times 3$ 的设计矩阵 $\\mathbf{X}_b$，其中第 $i$ 行为 $(1, x_{i1}, x_{i2})$，目标可以写成向量形式，即最小化 $\\|\\mathbf{y} - \\mathbf{X}_b \\mathbf{w}\\|_2^2$。最优系数向量 $\\hat{\\mathbf{w}}$ 通过求解正规方程找到，得出闭式解：\n$$\n\\hat{\\mathbf{w}} = (\\mathbf{X}_b^T \\mathbf{X}_b)^{-1} \\mathbf{X}_b^T \\mathbf{y}\n$$\n其中假设 $\\mathbf{X}_b^T \\mathbf{X}_b$ 是可逆的。一旦从训练数据中确定了 $\\hat{\\mathbf{w}}$，就通过对线性分数进行阈值处理来对新数据点进行预测。问题指定阈值为 $0.5$：\n$$\n\\hat{Y} = \\mathbf{1}(f(\\mathbf{x}) \\ge 0.5) = \\mathbf{1}(\\hat{w}_0 + \\hat{w}_1 x_1 + \\hat{w}_2 x_2 \\ge 0.5)\n$$\n\n**2. 决策树分类器**\n\n决策树分类器是一种非线性模型，它将特征空间划分为多个超矩形，并为每个超矩形分配一个类别标签。决策树是从根节点开始自上而下贪心构建的。在每个节点，算法寻找最佳的轴对齐分裂，将数据分成两个子节点。\n\n“最佳”分裂被定义为最大程度减少不纯度的分裂。这里使用的不纯度度量是Gini不纯度，对于包含经验类别比例为 $p_k$（$k \\in \\{0,1\\}$）的标签多重集的节点，其定义为：\n$$\nG = 1 - \\sum_{k \\in \\{0,1\\}} p_k^2 = 1 - (p_0^2 + p_1^2)\n$$\nGini不纯度为 $0$ 表示一个纯节点（所有样本都属于一个类别）。对于一个候选分裂，在父节点 $S$ 上的数据被划分为一个左子节点 $S_L = \\{\\mathbf{x_i} \\in S \\mid x_{ij} \\le t\\}$ 和一个右子节点 $S_R = \\{\\mathbf{x_i} \\in S \\mid x_{ij} > t\\}$。分裂的质量通过子节点的Gini不纯度的加权平均值来衡量：\n$$\nI_{\\text{split}} = \\frac{|S_L|}{|S|} G(S_L) + \\frac{|S_R|}{|S|} G(S_R)\n$$\n算法穷举搜索所有特征 $j \\in \\{1,2\\}$ 和所有有效的候选阈值 $t$，以找到最小化 $I_{\\text{split}}$ 的分裂。候选阈值被限制为当前节点观测到的特征的连续唯一排序值之间的中点。\n\n建树过程受特定规则的约束：\n- **最大深度**：一旦树达到深度 $2$，分裂就停止。深度为 $0$ 的树是单个根节点；深度为 $2$ 的树有一个根、其子节点及其孙子节点（必须是叶节点）。\n- **纯度**：如果节点是纯的，该节点的分裂也会停止。\n- **预测**：在叶节点，预测的类别是该节点中样本的多数类。平局时选择类别 $0$。\n- **分裂平局处理**：如果多个分裂产生相同的最小Gini不纯度，则首先选择特征索引较小的分裂（$x_1$ 优先于 $x_2$）来打破平局，如果仍然平局，则选择阈值较小的分裂。\n\n**评估**\n\n对于每个指定的测试用例，我们将使用提供的参数 $(a,b,\\eta)$ 和随机种子，生成一个大小为 $n_{\\text{train}}$ 的训练集和一个大小为 $n_{\\text{test}}$ 的独立测试集。两个模型都将在训练数据上进行训练。它们的性能将通过计算在测试集上的错分率——即错误预测的比例——来评估。每个用例的最终输出将是一对错分率 $[\\text{error}_{\\text{linear}}, \\text{error}_{\\text{tree}}]$。", "answer": "```python\nimport numpy as np\n\ndef generate_data(n_samples, a, b, eta, seed):\n    \"\"\"\n    Generates synthetic XOR data with label noise.\n    \n    Args:\n        n_samples (int): Number of data points to generate.\n        a (float): Threshold for feature x1.\n        b (float): Threshold for feature x2.\n        eta (float): Symmetric label noise rate.\n        seed (int): Random seed for reproducibility.\n        \n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing features (X) and labels (Y).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.uniform(0, 1, size=(n_samples, 2))\n    x1, x2 = X[:, 0], X[:, 1]\n    \n    # True labels based on the XOR rule\n    y_true = np.logical_xor(x1 > a, x2 > b).astype(int)\n    \n    # Introduce symmetric label noise\n    noise_mask = rng.random(n_samples)  eta\n    y_noisy = y_true.copy()\n    y_noisy[noise_mask] = 1 - y_noisy[noise_mask]\n    \n    return X, y_noisy\n\nclass LinearLeastSquaresClassifier:\n    \"\"\"\n    A linear classifier trained by minimizing mean squared error.\n    \"\"\"\n    def __init__(self):\n        self.w = None\n\n    def fit(self, X_train, y_train):\n        \"\"\"\n        Fits the linear model using the normal equations.\n        \n        Args:\n            X_train (np.ndarray): Training features.\n            y_train (np.ndarray): Training labels.\n        \"\"\"\n        X_b = np.c_[np.ones(X_train.shape[0]), X_train]\n        try:\n            # Solve (X_b^T X_b) w = X_b^T y\n            XtX = X_b.T @ X_b\n            XtX_inv = np.linalg.inv(XtX)\n            self.w = XtX_inv @ X_b.T @ y_train\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse if matrix is singular\n            self.w = np.linalg.pinv(X_b) @ y_train\n\n    def predict(self, X):\n        \"\"\"\n        Predicts labels for new data.\n        \n        Args:\n            X (np.ndarray): Features of data to predict.\n            \n        Returns:\n            np.ndarray: Predicted binary labels {0, 1}.\n        \"\"\"\n        if self.w is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n        X_b = np.c_[np.ones(X.shape[0]), X]\n        scores = X_b @ self.w\n        return (scores >= 0.5).astype(int)\n\nclass DecisionTreeClassifier:\n    \"\"\"\n    A decision tree classifier with Gini impurity and max depth.\n    \"\"\"\n    class Node:\n        def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):\n            self.feature_index = feature_index\n            self.threshold = threshold\n            self.left = left\n            self.right = right\n            self.value = value\n        \n        def is_leaf(self):\n            return self.value is not None\n\n    def __init__(self, max_depth=2):\n        self.max_depth = max_depth\n        self.root = None\n        self.n_classes_ = 2 # Fixed for this problem\n\n    def _gini(self, y):\n        \"\"\"Calculates Gini impurity.\"\"\"\n        if y.size == 0:\n            return 0.0\n        p = np.bincount(y, minlength=self.n_classes_) / y.size\n        return 1 - np.sum(p**2)\n\n    def _majority_vote(self, y):\n        \"\"\"Predicts class, breaking ties in favor of 0.\"\"\"\n        counts = np.bincount(y, minlength=self.n_classes_)\n        return 0 if counts[0] >= counts[1] else 1\n\n    def _find_best_split(self, X, y):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples, n_features = X.shape\n        if n_samples = 1:\n            return None\n\n        parent_gini = self._gini(y)\n        best_gini = parent_gini\n        best_split = None\n\n        for feature_idx in range(n_features):\n            unique_vals = np.unique(X[:, feature_idx])\n            if unique_vals.size = 1:\n                continue\n            \n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n            for t in thresholds:\n                left_indices = X[:, feature_idx] = t\n                right_indices = ~left_indices\n                \n                y_left, y_right = y[left_indices], y[right_indices]\n                \n                if y_left.size == 0 or y_right.size == 0:\n                    continue\n\n                p_left = y_left.size / n_samples\n                p_right = y_right.size / n_samples\n                \n                weighted_gini = p_left * self._gini(y_left) + p_right * self._gini(y_right)\n\n                # Tie-breaking logic as per problem description\n                if weighted_gini  best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_idx, t)\n                elif weighted_gini == best_gini:\n                    if best_split is not None:\n                        if feature_idx  best_split[0]:\n                            best_split = (feature_idx, t)\n                        elif feature_idx == best_split[0] and t  best_split[1]:\n                            best_split = (feature_idx, t)\n\n        return best_split\n\n    def _build_tree(self, X, y, depth):\n        \"\"\"Recursively builds the decision tree.\"\"\"\n        is_pure = len(np.unique(y)) == 1\n        is_max_depth = depth >= self.max_depth\n\n        if is_pure or is_max_depth:\n            leaf_value = self._majority_vote(y)\n            return self.Node(value=leaf_value)\n\n        best_split = self._find_best_split(X, y)\n        if best_split is None:\n            leaf_value = self._majority_vote(y)\n            return self.Node(value=leaf_value)\n            \n        feature_idx, threshold = best_split\n        left_mask = X[:, feature_idx] = threshold\n        right_mask = ~left_mask\n        \n        left_child = self._build_tree(X[left_mask, :], y[left_mask], depth + 1)\n        right_child = self._build_tree(X[right_mask, :], y[right_mask], depth + 1)\n        \n        return self.Node(feature_idx, threshold, left_child, right_child)\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree from training data.\"\"\"\n        y_int = y.astype(int)\n        self.root = self._build_tree(X, y_int, 0)\n    \n    def _predict_single(self, x, node):\n        \"\"\"Traverses the tree for a single prediction.\"\"\"\n        if node.is_leaf():\n            return node.value\n        \n        if x[node.feature_index] = node.threshold:\n            return self._predict_single(x, node.left)\n        else:\n            return self._predict_single(x, node.right)\n            \n    def predict(self, X):\n        \"\"\"Predicts labels for new data.\"\"\"\n        if self.root is None:\n            raise RuntimeError(\"Model has not been trained yet.\")\n        return np.array([self._predict_single(x, self.root) for x in X])\n\ndef solve():\n    test_cases = [\n        (400, 5000, 0.5, 0.5, 0.0, 42, 4242),\n        (400, 5000, 0.2, 0.8, 0.0, 1, 11),\n        (2000, 5000, 0.5, 0.5, 0.1, 7, 77),\n        (30, 5000, 0.5, 0.5, 0.0, 2024, 2025),\n    ]\n\n    results = []\n    for params in test_cases:\n        n_train, n_test, a, b, eta, seed_train, seed_test = params\n\n        # Generate data\n        X_train, y_train = generate_data(n_train, a, b, eta, seed_train)\n        X_test, y_test = generate_data(n_test, a, b, eta, seed_test)\n\n        # Linear Least-Squares Classifier\n        linear_model = LinearLeastSquaresClassifier()\n        linear_model.fit(X_train, y_train)\n        y_pred_linear = linear_model.predict(X_test)\n        error_linear = np.mean(y_pred_linear != y_test)\n        \n        # Decision Tree Classifier\n        tree_model = DecisionTreeClassifier(max_depth=2)\n        tree_model.fit(X_train, y_train)\n        y_pred_tree = tree_model.predict(X_test)\n        error_tree = np.mean(y_pred_tree != y_test)\n        \n        results.append([error_linear, error_tree])\n    \n    # Format the final output string exactly as requested\n    output_str = \"[\" + \",\".join([f\"[{e_lin},{e_tree}]\" for e_lin, e_tree in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3113048"}, {"introduction": "决策树的贪心构建算法虽然高效，但也可能导致模型对训练数据中的虚假关联（spurious correlations）产生过拟合。本练习将构建一个包含这种陷阱的数据集，并探索如何通过设置 `w_{\\min}`（最小叶节点样本数）这一正则化参数来有效避免此类过拟合问题。这个实践将帮助你理解正则化在决策树中的重要性，以及它如何提高模型的泛化能力。[@problem_id:3112969]", "problem": "考虑使用贪心分裂的决策树进行二元分类，该分裂由经验风险最小化驱动。对于预测单个类别的叶节点，其经验风险由错分率定义。决策桩 (decision stump) 是一种深度为 $1$ 的决策树，它在单个特征阈值上进行一次分裂。设训练集为 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\{0,1\\}$，并设所有训练样本的权重均为单位权重 (1)。在特征 $j \\in \\{0,1\\}$ 上以阈值 $\\tau$ 进行分裂，会将数据划分为左子节点 $\\{i : x_{i,j} \\le \\tau\\}$ 和右子节点 $\\{i : x_{i,j}  \\tau\\}$。决策桩的经验风险是子节点错分率的加权和。最小子节点权重约束要求每个子节点必须包含至少 $w_{\\min}$ 的总样本权重，在单位权重下，这等价于每个子节点至少有 $w_{\\min}$ 个样本。贪心决策桩选择在满足最小子节点权重约束的条件下，使经验风险最小化的特征和阈值。如果没有有效的分裂能够降低父节点的经验风险，决策桩将默认不进行分裂，并预测多数类。\n\n你将构建一个科学上合理的情景，其中小亚组中出现的伪相关性可能会误导贪心分裂。数据生成过程如下，所有随机变量均独立：\n\n- 训练集大小 $N = 200$，测试集大小 $M = 5000$。\n- 对每个样本，特征 $x_0$ 从 $x_{0} \\sim \\mathcal{N}(0,1)$ 中抽取。标签由 $y = \\mathbb{1}\\{x_{0} + \\epsilon  0\\}$ 生成，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 3.0$；由于噪声较大，这导致特征 $x_0$ 的信息量很弱。\n- 训练数据的特征 $x_1$ 从 $x_{1} \\sim \\mathrm{Uniform}(-2,2)$ 中抽取，但对于正标签训练样本中一个大小为 $s = 15$ 的小亚组，其 $x_{1}$ 被设置为极端值 $10$。这在一个小亚组中制造了伪相关性，贪心算法可以通过在 $x_1$ 上设置一个高阈值来分离这个亚组，从而利用这种相关性。\n- 测试数据的特征 $x_1$ 对所有样本均从 $x_{1} \\sim \\mathrm{Uniform}(-2,2)$ 中抽取，没有极端值，因此这种伪模式不具有泛化性。\n\n将预测叶节点内多数类的叶节点的经验风险定义为该叶节点中不属于多数类的样本所占的比例。对于一次分裂，经验风险是所有子节点错分数量之和除以 $N$。贪心决策桩检查每个特征的连续排序后不同特征值之间的中点作为阈值。它选择能产生最小经验风险并且对两个子节点都满足最小子节点权重约束 $w_{\\min}$ 的分裂；平局情况通过选择较小的特征索引，然后是较小的阈值来打破。如果没有有效的分裂能够降低父节点的经验风险，决策桩将不进行分裂，并预测整体的多数类。\n\n任务：\n- 实现所述的数据生成和带有最小子节点权重约束的决策桩学习。\n- 在训练集上训练决策桩，并通过检查所选特征是否为 $x_1$ 来测试学习到的分裂是否是伪分裂。如果决策桩不分裂或选择了 $x_0$，则视为避免了伪分裂。\n\n测试套件：\n- 使用上述固定的训练和测试分布，其中 $N = 200$，$M = 5000$，以及 $s = 15$。\n- 评估以下最小子节点权重 $w_{\\min}$ 的值：\n  1. $w_{\\min} = 1$（无有效约束）。\n  2. $w_{\\min} = 15$（边界值等于伪亚组的大小）。\n  3. $w_{\\min} = 16$（略高于伪亚组的大小）。\n  4. $w_{\\min} = 500$（值过大以至于不可能进行分裂）。\n\n对于每种情况，你的程序必须输出一个布尔值，指示学习到的决策桩是否如上文定义的那样避免了伪分裂。最终输出格式必须是一行，包含一个按测试套件顺序排列的四个布尔值的列表，以逗号分隔并用方括号括起来，例如 $[b_1,b_2,b_3,b_4]$，其中每个 $b_i$ 为 $\\mathrm{True}$ 或 $\\mathrm{False}$。", "solution": "## 问题验证\n\n### 步骤 1：提取已知信息\n- **模型**：用于二元分类的决策桩（深度为1的决策树）。\n- **数据**：训练集 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\{0,1\\}$。所有样本权重均为 1。\n- **目标**：基于最小化经验风险的贪心分裂选择。\n- **分裂规则**：在特征 $j$ 上以阈值 $\\tau$ 进行分裂，创建左子节点 $\\{i : x_{i,j} \\le \\tau\\}$ 和右子节点 $\\{i : x_{i,j}  \\tau\\}$。\n- **叶节点风险**：叶节点的经验风险是其错分率，即该叶节点中不属于多数类的样本比例。\n- **决策桩风险**：决策桩的经验风险是两个子节点错分数量之和除以总样本数 $N$。\n- **约束**：最小子节点权重约束 $w_{\\min}$ 要求每个子节点至少包含 $w_{\\min}$ 个样本。\n- **候选阈值**：连续排序后不同特征值之间的中点。\n- **平局处理**：如果多个分裂产生相同的最小风险，则选择特征索引较小的那个。如果特征索引也相同，则选择阈值较小的那个。\n- **不分裂条件**：如果没有有效分裂能降低父节点的经验风险，决策桩则不进行分裂，并预测整体的多数类。\n- **训练数据生成 ($N=200$)**：\n    - $x_0 \\sim \\mathcal{N}(0,1)$。\n    - $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma=3.0$。\n    - $y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$，其中 $\\mathbb{1}$ 是指示函数。\n    - 对于大多数样本，$x_1 \\sim \\mathrm{Uniform}(-2,2)$。\n    - 一个大小为 $s=15$ 的正标签（$y=1$）训练样本亚组的 $x_1$ 值被设为 $10$。\n- **测试数据生成 ($M=5000$)**：\n    - $x_0 \\sim \\mathcal{N}(0,1)$。\n    - $\\epsilon \\sim \\mathcal{N}(0, 3.0^2)$。\n    - $y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$。\n    - 对所有样本，$x_1 \\sim \\mathrm{Uniform}(-2,2)$（无伪模式）。\n- **任务**：对于一组 $w_{\\min}$ 值，确定训练后的决策桩是否避免了在特征 $x_1$ 上的伪分裂。避免伪分裂定义为选择特征 $x_0$ 或完全不进行分裂。\n- **测试套件**：$w_{\\min} \\in \\{1, 15, 16, 500\\}$。\n\n### 步骤 2：使用提取的已知信息进行验证\n- **科学依据**：该问题在统计学习原理方面有充分的依据。它描述了一种标准算法（带贪心训练的决策桩），并使用合成数据生成过程来研究一种常见的病态问题：对伪相关性的过拟合。使用最小子节点大小（`w_min`）是一种标准的正则化技术，用以防止此类过拟合。\n- **适定性**：问题定义得非常精确。所有参数（$N, M, s, \\sigma$）、算法程序（贪心搜索、风险计算、阈值选择）、约束（$w_{\\min}$）和打破平局的规则都已明确定义。这确保了对于给定的随机种子，该过程会产生一个唯一的、确定性的结果。\n- **客观性**：问题以客观的数学语言陈述。任务是实现指定的算法并报告其行为，这是一个纯粹的计算练习，没有主观性。\n\n该问题未违反任何无效性标准。这是一个在计算统计学中定义明确、科学上合理的问题。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个解决方案。\n\n## 解决方案\n\n该问题要求我们实现一个决策桩学习算法，并在一个为产生伪相关性而专门构建的数据集上测试其行为。目标是观察“最小子节点权重”约束 $w_{\\min}$ 如何影响算法对这种伪模式的敏感性。\n\n### 1. 原理与实验设计\n问题的核心在于一个真实（但信息量弱）的特征 $x_0$ 和一个伪“完美”特征 $x_1$ 之间的冲突。\n- **特征 $x_0$**：该特征通过方程 $y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$ 与标签 $y$ 有因果关系。然而，高噪声方差（$\\sigma^2=9.0$）使得这种关系很弱，这意味着在 $x_0$ 上的分裂可能只会带来经验风险的适度降低。\n- **特征 $x_1$**：该特征通常不具信息量。然而，一小部分大小为 $s=15$ 的 $y=1$ 训练样本被人为地赋予了极端值 $x_1=10$。贪心算法可以发现一个在 $x_1$ 上的分裂（例如，在 2 和 10 之间的阈值 $\\tau$），它能完美地将这 15 个样本分离到一个“纯”的子节点中，错分率为零。这可以在训练集上导致整体经验风险的大幅降低，使其成为一个极具吸引力但却是伪造的分裂。\n- **通过 $w_{\\min}$ 进行正则化**：最小子节点权重约束 $w_{\\min}$ 是一种正则化形式。通过要求每个子节点包含最少数量的样本，我们可以禁止那些隔离非常小亚组的分裂。如果 $w_{\\min}$ 设置得比伪亚组的大小（$s=15$）更大，贪心算法将被阻止做出这种局部最优但全局糟糕的选择，可能迫使其选择更稳健的特征 $x_0$ 或根本不分裂。\n\n### 2. 数据生成\n首先，我们为大小为 $N=200$ 的训练集实现数据生成过程。为保证可复现性，使用固定的随机种子。\n1.  从标准正态分布生成特征 $x_0$，$x_0 \\sim \\mathcal{N}(0,1)$。\n2.  从均值为 0、标准差为 $\\sigma=3.0$ 的正态分布生成噪声 $\\epsilon$，$\\epsilon \\sim \\mathcal{N}(0, 3.0^2)$。\n3.  计算二元标签 $y = \\mathbb{1}\\{x_0 + \\epsilon  0\\}$。\n4.  从均匀分布生成特征 $x_1$，$x_1 \\sim \\mathrm{Uniform}(-2,2)$。\n5.  识别所有 $y=1$ 的样本索引。从这个集合中，随机选择 $s=15$ 个索引，并将其对应的 $x_1$ 值设置为 $10.0$。这就注入了伪相关性。\n6.  将 $x_0$ 和 $x_1$ 合并成一个特征矩阵 $X$。\n\n### 3. 决策桩算法\n决策桩的训练过程是找到单个分裂（一个特征 $j$ 和一个阈值 $\\tau$），在满足 $w_{\\min}$ 约束的条件下，最小化总错分计数。\n1.  **计算父节点风险**：首先，计算根节点（即不进行分裂时）的错分计数。这是整个数据集中少数类的计数。该值作为要超越的初始 `best_misclass` 分数。\n2.  **遍历所有分裂**：\n    - 对每个特征 $j \\in \\{0, 1\\}$：\n        - 确定候选阈值的集合。这些是特征 $x_j$ 的连续唯一值之间的中点。\n        - 按升序对每个阈值 $\\tau$ 进行操作：\n            a.  **划分数据**：将样本划分为左集合（$x_j \\le \\tau$）和右集合（$x_j  \\tau$）。\n            b.  **检查约束**：计算左（$n_{left}$）右（$n_{right}$）集合中的样本数量。如果 $n_{left}  w_{\\min}$ 或 $n_{right}  w_{\\min}$，则这是一个无效分裂；继续下一个阈值。\n            c.  **计算风险**：对于有效分裂，计算总错分计数。这是左子节点和右子节点中错分数量的总和。子节点的错分计数是该子节点内少数类的样本数。\n            d.  **更新最佳分裂**：将当前分裂的错分计数与迄今为止找到的 `best_misclass` 进行比较。如果当前计数严格更小（``），则用这个新计数更新 `best_misclass`，并将当前特征 $j$ 和阈值 $\\tau$ 记录为最佳分裂。严格不等式和循环顺序（先特征 0 后特征 1；先小阈值）正确实现了指定的平局处理规则。\n3.  **返回结果**：在检查完所有有效分裂后，返回最佳分裂的特征索引。如果没有分裂优于父节点风险，则返回初始特征索引 $-1$，表示没有进行分裂。\n\n### 4. 测试用例分析\n对测试套件 $\\{1, 15, 16, 500\\}$ 中的每个 $w_{\\min}$ 值执行该过程。\n-   **情况 1：$w_{\\min} = 1$**：此约束微不足道。在 $x_1$ 上的伪分裂创建了一个大小为 15 的子节点，满足 $\\ge 1$。这个分裂非常有吸引力，因为它创建了一个纯节点，导致风险显著降低。预计算法会选择特征 $x_1$。结果应为 `False`（未避免伪分裂）。\n-   **情况 2：$w_{\\min} = 15$**：伪分裂创建了一个大小恰好为 15 的子节点。由于约束是 $n_{child} \\ge w_{\\min}$，这个分裂仍然有效（$15 \\ge 15$）。预计算法仍会选择特征 $x_1$。结果应为 `False`。\n-   **情况 3：$w_{\\min} = 16$**：伪分裂现在是无效的，因为它大小为 15 的子节点不满足 16 的最小权重（$15  16$）。算法被迫忽略这个“陷阱”，必须寻找替代分裂。它要么会在信息量较弱的特征 $x_0$ 上选择一个分裂（如果能降低风险），要么根本不分裂。无论哪种情况，特征 $x_1$ 都不会被选中。结果应为 `True`（避免了伪分裂）。\n-   **情况 4：$w_{\\min} = 500$**：总样本数为 $N=200$。不可能创建两个都满足至少有 500 个样本的约束的子节点。因此，不存在有效的分裂。算法将不会分裂。由于没有选择特征 $x_1$，结果为 `True`。\n\n实现将生成一个与这四种结果相对应的布尔值列表。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the data generation and decision stump learning to test the effect\n    of the minimum child weight constraint on avoiding spurious splits.\n    \"\"\"\n    # Define problem parameters\n    N = 200\n    s = 15\n    sigma = 3.0\n    w_min_cases = [1, 15, 16, 500]\n\n    # Set a fixed seed for reproducibility of the random dataset\n    np.random.seed(42)\n\n    # --- Data Generation --\n    # This block creates the single training set used for all test cases.\n    \n    # Feature x_0 is drawn from a standard normal distribution\n    x0 = np.random.randn(N)\n    # The label y is determined by x_0 plus high-variance noise\n    epsilon = np.random.normal(0, sigma, N)\n    y_train = (x0 + epsilon > 0).astype(int)\n\n    # Feature x_1 is mostly uniform noise\n    x1 = np.random.uniform(-2, 2, N)\n    \n    # Identify indices of positive-labeled samples to inject the spurious pattern\n    positive_indices = np.where(y_train == 1)[0]\n    \n    # Set x_1 to an extreme value for a small subgroup of 's' positive samples.\n    # This creates a spurious correlation that a greedy algorithm might exploit.\n    if len(positive_indices) >= s:\n        spurious_indices = np.random.choice(positive_indices, size=s, replace=False)\n        x1[spurious_indices] = 10.0\n    else:\n        # This case is unlikely with N=200 but makes the code more robust.\n        x1[positive_indices] = 10.0\n\n    X_train = np.column_stack((x0, x1))\n    \n    results = []\n    for w_min in w_min_cases:\n        # --- Decision Stump Training ---\n        n_samples = X_train.shape[0]\n\n        # Calculate the misclassification count of the parent node (no split scenario).\n        # This is the number of samples in the minority class.\n        n_pos_parent = np.sum(y_train)\n        parent_misclass = min(n_pos_parent, n_samples - n_pos_parent)\n\n        best_misclass = parent_misclass\n        best_split = {'feature': -1, 'threshold': np.inf}\n\n        # Iterate through features (j=0 for x_0, j=1 for x_1)\n        for j in range(X_train.shape[1]):\n            feature_values = X_train[:, j]\n            \n            # Candidate thresholds are midpoints of unique sorted feature values.\n            unique_vals = np.unique(feature_values)\n            if len(unique_vals)  2:\n                continue\n            thresholds = (unique_vals[:-1] + unique_vals[1:]) / 2.0\n\n            # Evaluate each potential split\n            for tau in thresholds:\n                # Partition data into left and right children\n                left_mask = feature_values = tau\n                \n                n_left = np.sum(left_mask)\n                n_right = n_samples - n_left\n\n                # Verify the minimum child weight constraint\n                if n_left  w_min or n_right  w_min:\n                    continue\n\n                # Calculate misclassifications in the left child\n                y_left = y_train[left_mask]\n                misclass_left = min(np.sum(y_left), n_left - np.sum(y_left))\n                \n                # Calculate misclassifications in the right child\n                right_mask = ~left_mask\n                y_right = y_train[right_mask]\n                misclass_right = min(np.sum(y_right), n_right - np.sum(y_right))\n\n                current_misclass = misclass_left + misclass_right\n\n                # A split is chosen only if it strictly reduces the misclassification count.\n                # The loop order (j=0 then j=1; tau ascending) ensures that ties are\n                # broken by smaller feature index, then smaller threshold.\n                if current_misclass  best_misclass:\n                    best_misclass = current_misclass\n                    best_split = {'feature': j, 'threshold': tau}\n        \n        selected_feature = best_split['feature']\n        \n        # The split is considered non-spurious if the selected feature is not x_1 (index 1).\n        # This includes cases where x_0 is chosen or no split is made (feature = -1).\n        avoids_spurious = (selected_feature != 1)\n        results.append(avoids_spurious)\n\n    # Print the final list of booleans in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3112969"}, {"introduction": "贪心算法的“短视”特性是其另一个潜在缺陷，即它可能选择一个在当前步骤看来最优、但对全局结构而言次优的切分。本练习将设计一个包含“干扰”特征的数据集，以揭示这种短视行为如何导致决策树错失数据中真实的层次结构。通过这个例子，我们将更深刻地理解贪心策略的局限性以及特征工程在模型构建中的关键作用。[@problem_id:3113028]", "problem": "您的任务是设计并分析一个具有加性层次结构的合成回归数据集，并评估一个具有小分裂预算的贪心分类与回归树 (CART) 程序是能恢复预期的结构，还是会倾向于误导性的局部不纯度增益。工作背景为回归决策树，其中经验风险由均方误差度量。\n\n根据以下生成模型，构建一个包含特征 $x_1$、$x_2$、一个干扰特征 $z$ 以及响应 $Y$ 的数据集。对于给定的样本大小 $n$，独立地从 $x_1 \\sim \\mathrm{Uniform}[0,1]$ 和 $x_2 \\sim \\mathrm{Uniform}[0,1]$ 中抽样。固定阈值 $a \\in (0,1)$ 和 $b \\in (0,1)$，并定义指示函数 $\\mathbb{I}\\{x_1  a\\}$ 和 $\\mathbb{I}\\{x_2  b\\}$。令 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ 和 $\\xi \\sim \\mathcal{N}(0,\\sigma_z^2)$ 为独立噪声。将响应和干扰项定义为\n$$\nY = \\alpha \\,\\mathbb{I}\\{x_1  a\\} + \\beta \\,\\mathbb{I}\\{x_2  b\\} + \\varepsilon,\n\\quad\nz = \\lambda \\left(\\mathbb{I}\\{x_1  a\\} + \\mathbb{I}\\{x_2  b\\}\\right) + \\xi,\n$$\n其中 $\\alpha$、$\\beta$、$\\lambda$、$\\sigma$ 和 $\\sigma_z$ 是固定的实数参数。数据集由 $n$ 个对 $(x_1, x_2, z, Y)$ 的独立观测值组成。\n\n实现一个贪心最佳优先 CART 回归树，该树最多使用 $S$ 次二元、轴对齐分裂，并遵守最小叶节点大小为 $m$ 的规则。对于一个包含响应 $\\{y_i\\}_{i \\in \\mathcal{I}}$ 的节点，其不纯度为与节点均值之间的经验平方偏差和，即\n$$\n\\mathrm{SSE}(\\mathcal{I}) = \\sum_{i \\in \\mathcal{I}} \\left(y_i - \\bar{y}_{\\mathcal{I}}\\right)^2,\n\\quad \\text{where } \\bar{y}_{\\mathcal{I}} = \\frac{1}{|\\mathcal{I}|}\\sum_{i \\in \\mathcal{I}} y_i.\n$$\n在每一步中，从所有当前叶节点和所有遵守最小叶节点大小 $m$ 的有效分裂中，选择能够产生最大 $\\mathrm{SSE}$ 减少量的分裂，即父节点的 $\\mathrm{SSE}$ 与其子节点的 $\\mathrm{SSE}$ 之和之间的最大差值。将该分裂应用于该叶节点。重复此过程，直到已进行 $S$ 次分裂或没有有效分裂能产生正的减少量为止。\n\n“恢复层次结构”的定义如下：在分裂预算为 $S = 2$ 的情况下，当且仅当算法选择的两次分裂（顺序不限）同时使用了 $x_1$ 和 $x_2$，并且没有仅使用干扰特征 $z$ 时，贪心树才算“恢复了层次结构”。如果一次或两次分裂仅使用了 $z$（且分裂特征集合中不包含 $x_1$ 和 $x_2$），则认为贪心过程“被误导性的局部不纯度增益所困”。\n\n为进行额外评估，还需计算在严格两次分裂的约束下可达到的最佳 $\\mathrm{SSE}$，其中每次分裂分别使用 $x_1$ 和 $x_2$ 各一次（顺序不限），且其中一个根节点的子节点由另一个特征进行分裂，同时始终遵守最小叶节点大小 $m$ 的规则。这个受约束的最佳值代表了与预期加性层次结构对齐的最佳“结构化双分裂”树。\n\n您的程序必须：\n- 对每个测试用例，使用指定的参数和固定的随机种子生成数据集，以确保可复现性。\n- 构建一个最多进行 $S=2$ 次分裂的贪心最佳优先 CART 树。\n- 根据定义判断贪心树是否“恢复了层次结构”。\n- （可选）计算贪心 $\\mathrm{SSE}$ 并与最佳“结构化双分裂”$\\mathrm{SSE}$ 进行比较，但要求的输出是关于恢复情况的决策。\n\n测试套件：\n为以下四个测试用例提供结果。在每个用例中，以布尔值的形式报告贪心过程是否“恢复了层次结构”。使用所列出的参数和随机种子。\n\n- 用例 $1$ (理想路径): $n = 2000$, $\\alpha = 1.0$, $\\beta = 1.0$, $a = 0.5$, $b = 0.5$, $\\sigma = 0.1$, $\\lambda = 0.2$, $\\sigma_z = 0.5$, $S = 2$, $m = 25$, 种子 $= 0$。\n- 用例 $2$ (强干扰项陷阱): $n = 2000$, $\\alpha = 1.0$, $\\beta = 1.0$, $a = 0.5$, $b = 0.5$, $\\sigma = 0.1$, $\\lambda = 2.0$, $\\sigma_z = 0.05$, $S = 2$, $m = 25$, 种子 $= 1$。\n- 用例 $3$ (偏斜阈值): $n = 2000$, $\\alpha = 1.5$, $\\beta = 0.5$, $a = 0.85$, $b = 0.2$, $\\sigma = 0.1$, $\\lambda = 0.8$, $\\sigma_z = 0.2$, $S = 2$, $m = 20$, 种子 $= 2$。\n- 用例 $4$ (噪声主导): $n = 2000$, $\\alpha = 1.0$, $\\beta = 1.0$, $a = 0.5$, $b = 0.5$, $\\sigma = 1.0$, $\\lambda = 0.5$, $\\sigma_z = 0.5$, $S = 2$, $m = 25$, 种子 $= 3$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个用例的结果，格式为方括号内以逗号分隔的布尔值列表（例如，“[True,False,True,False]”）。不允许有任何额外输出。所有角度、物理单位或百分比均不适用于此任务，因此不需要单位说明。唯一的输出是如上定义的布尔值。", "solution": "我们通过抽样 $x_1 \\sim \\mathrm{Uniform}[0,1]$ 和 $x_2 \\sim \\mathrm{Uniform}[0,1]$，设置阈值 $a$ 和 $b$，并定义响应\n$$\nY = \\alpha \\,\\mathbb{I}\\{x_1  a\\} + \\beta \\,\\mathbb{I}\\{x_2  b\\} + \\varepsilon,\n\\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2).\n$$\n来构建一个基于加性层次结构的数据集。这是一个关于指示特征 $\\mathbb{I}\\{x_1  a\\}$ 和 $\\mathbb{I}\\{x_2  b\\}$ 的加性模型，它产生了四个具有分段常数均值的区域，这些区域的均值相差 $\\alpha$ 和 $\\beta$。为了创建一个能够误导贪心分裂的特征，我们添加一个干扰项\n$$\nz = \\lambda \\left(\\mathbb{I}\\{x_1  a\\} + \\mathbb{I}\\{x_2  b\\}\\right) + \\xi,\n\\quad \\xi \\sim \\mathcal{N}(0,\\sigma_z^2),\n$$\n它与指示函数的潜在和相关。如果 $\\lambda$ 很大且 $\\sigma_z$ 很小，在根节点上对 $z$ 进行分裂可能会导致经验平方误差和 (SSE) 的即时大幅减少，其减少量可能超过在真实结构特征 $x_1$ 或 $x_2$ 上分裂所获得的减少量。然而，在分裂预算限制为 $S = 2$ 的情况下，将第一次分裂用在 $z$ 上会限制树表示加性层次结构的能力，因为只剩下一次额外的分裂机会。而要理想地在所有区域完全分离 $x_1$ 和 $x_2$ 的贡献，需要在树的顶部或接近顶部对两个信号都进行分裂，并且通常需要对两个子节点都进行分裂，这超出了两次分裂的预算。\n\n贪心最佳优先 CART 程序由平方误差的经验风险最小化原则定义：在任何索引集为 $\\mathcal{I}$ 的节点，其不纯度为\n$$\n\\mathrm{SSE}(\\mathcal{I}) = \\sum_{i \\in \\mathcal{I}} \\left(y_i - \\bar{y}_{\\mathcal{I}}\\right)^2,\n\\quad \\bar{y}_{\\mathcal{I}} = \\frac{1}{|\\mathcal{I}|}\\sum_{i \\in \\mathcal{I}} y_i.\n$$\n给定一个将 $\\mathcal{I}$ 分裂为左右子节点 $\\mathcal{I}_L$ 和 $\\mathcal{I}_R$ 的候选分裂，不纯度的减少量为\n$$\n\\Delta = \\mathrm{SSE}(\\mathcal{I}) - \\left( \\mathrm{SSE}(\\mathcal{I}_L) + \\mathrm{SSE}(\\mathcal{I}_R) \\right).\n$$\n最佳优先的贪心树生长过程会重复地在所有当前叶节点和所有遵守最小叶节点大小 $m$ 的有效轴对齐阈值中，选择使 $\\Delta$ 最大化的分裂。这与以下原则一致：对于平方损失，在每个叶节点中最小化经验风险的分段常数拟合是该叶节点的均值，而最优分裂是那个能产生最大总叶内平方误差减少量的分裂。\n\n为了高效且确定性地实现这一点，我们：\n- 每个测试用例使用固定的随机种子来生成 $(x_1,x_2)$、噪声，并因此生成 $(z,Y)$。\n- 对于任何节点和特征，按特征值对节点样本进行排序，并在连续的唯一值之间评估候选阈值，同时约束生成的子叶节点至少包含 $m$ 个样本。对于每个分裂点，我们通过累积和与平方累积和来计算左、右子节点的 $\\mathrm{SSE}$。节点上的最佳分裂是 $\\Delta$ 最大的那个。\n- 在最佳优先方案中，每次迭代我们都扫描所有当前叶节点，计算每个叶节点的最佳分裂，选择能实现最大 $\\Delta$ 的叶节点和分裂，并应用它。我们记录每次所选分裂使用的特征。在最多进行 $S=2$ 次分裂后，或者当没有有效分裂能产生正的 $\\Delta$ 时，我们停止。\n\n为了评估树是否“恢复了层次结构”，我们检查贪心算法在最多两次分裂中使用的特征集合。如果 $x_1$ 和 $x_2$ 都被使用（顺序不限），我们宣布成功。如果分裂特征集合中不包含 $x_1$ 和 $x_2$，我们则宣布贪心过程“被误导性的局部不纯度增益所困”。\n\n为了提供额外的视角，我们还计算了被约束为对 $x_1$ 和 $x_2$ 各使用一次的最佳结构化双分裂树。这是通过枚举在 $\\{x_1,x_2\\}$ 之一上的所有可能的根分裂，然后对于每个根分裂的选择和阈值，枚举在另一个特征上应用于左或右子节点的最佳有效第二次分裂（遵守 $m$），并选择使总 $\\mathrm{SSE}$ 最小化的组合来实现的。这个受约束的最优解代表了遵循预期加性结构的最佳双分裂近似。\n\n测试用例旨在探究：\n- 一个“理想路径”（happy path），其中干扰项很弱，因此贪心算法应该选择 $x_1$ 和 $x_2$。\n- 一个“陷阱”，其中干扰项很强且噪声很低，这会鼓励贪心算法在 $z$ 上分裂，并且在只有 $S=2$ 次分裂预算的情况下可能再次在 $z$ 上分裂。\n- 偏斜的阈值，其中区域大小差异显著，用于测试最小叶节点大小和分裂增益之间的相互作用。\n- 一个噪声主导的场景，其中增益被削弱，分裂选择可能不稳定。\n\n程序计算并打印一个单行列表，其中包含四个布尔值，按指定顺序对应于每个用例的恢复决策。此输出直接满足了交付可量化、可测试结果的要求，这些结果反映了贪心 CART 是恢复了预期的层次结构，还是被干扰特征导致的局部不纯度减少所误导。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef sse_of_indices(y, idx):\n    if idx.size == 0:\n        return 0.0\n    y_sub = y[idx]\n    mean = y_sub.mean()\n    return float(((y_sub - mean) ** 2).sum())\n\ndef best_split_on_feature(X_col, y, idx, min_leaf):\n    # Returns (best_gain, threshold, left_idx, right_idx, thr_value_at_split)\n    # Using efficient cumulative sums on the subset defined by idx.\n    if idx.size  2 * min_leaf:\n        return (0.0, None, None, None, None)\n    # Sort by feature values\n    order = np.argsort(X_col[idx], kind='mergesort')  # stable\n    idx_sorted = idx[order]\n    x_sorted = X_col[idx_sorted]\n    y_sorted = y[idx_sorted]\n    n = idx_sorted.size\n\n    # Precompute cumulative sums for SSE computation\n    csum = np.cumsum(y_sorted)\n    csum2 = np.cumsum(y_sorted ** 2)\n\n    total_sum = csum[-1]\n    total_sum2 = csum2[-1]\n\n    best_gain = 0.0\n    best_pos = None\n    # Evaluate splits between positions pos-1 and pos, where each side has >= min_leaf\n    # Positions are 1..n-1, we require min_leaf = pos = n - min_leaf\n    start = min_leaf\n    end = n - min_leaf\n    if end = start - 1:\n        return (0.0, None, None, None, None)\n\n    # To avoid splitting at identical feature values that lead to empty child, we skip positions where x_sorted[pos-1] == x_sorted[pos]\n    parent_sse = float(total_sum2 - (total_sum ** 2) / n)\n\n    for pos in range(start, end + 1):\n        if x_sorted[pos - 1] == x_sorted[pos]:\n            continue\n        left_n = pos\n        right_n = n - pos\n\n        left_sum = csum[pos - 1]\n        right_sum = total_sum - left_sum\n\n        left_sum2 = csum2[pos - 1]\n        right_sum2 = total_sum2 - left_sum2\n\n        left_sse = float(left_sum2 - (left_sum ** 2) / left_n)\n        right_sse = float(right_sum2 - (right_sum ** 2) / right_n)\n\n        gain = parent_sse - (left_sse + right_sse)\n        if gain > best_gain:\n            best_gain = gain\n            best_pos = pos\n\n    if best_pos is None:\n        return (0.0, None, None, None, None)\n\n    # Threshold as midpoint between consecutive feature values\n    thr = 0.5 * (x_sorted[best_pos - 1] + x_sorted[best_pos])\n    left_mask = X_col[idx] = thr\n    left_idx = idx[left_mask]\n    right_idx = idx[~left_mask]\n    return (best_gain, thr, left_idx, right_idx, thr)\n\ndef best_split_for_leaf(X, y, idx, min_leaf):\n    # Iterate over all features to find best split on this leaf\n    n_features = X.shape[1]\n    parent_sse = sse_of_indices(y, idx)\n    best = {\n        \"gain\": 0.0,\n        \"feature\": None,\n        \"threshold\": None,\n        \"left_idx\": None,\n        \"right_idx\": None,\n    }\n    for j in range(n_features):\n        gain, thr, left_idx, right_idx, _ = best_split_on_feature(X[:, j], y, idx, min_leaf)\n        if gain > best[\"gain\"]:\n            best[\"gain\"] = gain\n            best[\"feature\"] = j\n            best[\"threshold\"] = thr\n            best[\"left_idx\"] = left_idx\n            best[\"right_idx\"] = right_idx\n    return best\n\ndef greedy_best_first_cart_two_splits(X, y, split_budget, min_leaf):\n    # Best-first: at each iteration, evaluate best split on each current leaf, pick the best overall.\n    # Track which features were used in performed splits.\n    n = X.shape[0]\n    initial_idx = np.arange(n, dtype=int)\n    leaves = [initial_idx]\n    features_used = []\n    splits_done = 0\n\n    while splits_done  split_budget:\n        best_overall = None\n        best_leaf_pos = None\n        # Evaluate best split for each leaf\n        for pos, idx in enumerate(leaves):\n            candidate = best_split_for_leaf(X, y, idx, min_leaf)\n            if candidate[\"gain\"] = 1e-12:\n                continue\n            if best_overall is None or candidate[\"gain\"] > best_overall[\"gain\"]:\n                best_overall = candidate\n                best_leaf_pos = pos\n        if best_overall is None:\n            break  # no positive gain split available\n        # Apply the best split\n        features_used.append(best_overall[\"feature\"])\n        left_idx = best_overall[\"left_idx\"]\n        right_idx = best_overall[\"right_idx\"]\n        # Replace the split leaf with its two children\n        leaves.pop(best_leaf_pos)\n        leaves.append(left_idx)\n        leaves.append(right_idx)\n        splits_done += 1\n\n    # Compute final SSE of the greedy tree\n    total_sse = 0.0\n    for idx in leaves:\n        total_sse += sse_of_indices(y, idx)\n    return features_used, total_sse\n\ndef best_structured_two_split_sse(X, y, min_leaf, feature_indices):\n    # Constrained: exactly two splits, use each of the two specified features exactly once.\n    # Enumerate: choose root feature and threshold; then split one child on the other feature with best possible threshold.\n    # Return minimal SSE achievable under these constraints.\n    n = X.shape[0]\n    all_idx = np.arange(n, dtype=int)\n    best_total_sse = sse_of_indices(y, all_idx)\n\n    f1, f2 = feature_indices\n    for root_feat in [f1, f2]:\n        other_feat = f2 if root_feat == f1 else f1\n        # Enumerate root splits on root_feat\n        # Use same best_split_on_feature logic but we need to scan all thresholds manually to evaluate second split too.\n        # We'll generate all valid root thresholds (midpoints) and then for each compute best child split on other_feat.\n        # Prepare sorted indices by root feature\n        Xr = X[:, root_feat]\n        order = np.argsort(Xr[all_idx], kind='mergesort')\n        idx_sorted = all_idx[order]\n        xr_sorted = Xr[idx_sorted]\n        y_sorted = y[idx_sorted]\n        n_all = idx_sorted.size\n\n        # Candidate split positions\n        start = min_leaf\n        end = n_all - min_leaf\n        if end = start - 1:\n            continue\n\n        # Generate possible positions where feature value changes\n        valid_positions = []\n        for pos in range(start, end + 1):\n            if xr_sorted[pos - 1] != xr_sorted[pos]:\n                valid_positions.append(pos)\n        if not valid_positions:\n            continue\n\n        for pos in valid_positions:\n            thr_root = 0.5 * (xr_sorted[pos - 1] + xr_sorted[pos])\n            left_mask = Xr[all_idx] = thr_root\n            left_idx = all_idx[left_mask]\n            right_idx = all_idx[~left_mask]\n\n            # For each child, compute best split on other_feat; choose the better of splitting left or right.\n            # Option A: split left child by other_feat\n            gain_left, thr_left, left_left_idx, left_right_idx, _ = best_split_on_feature(X[:, other_feat], y, left_idx, min_leaf)\n            if gain_left = 0.0:\n                sse_left_after = sse_of_indices(y, left_idx)\n            else:\n                sse_left_after = sse_of_indices(y, left_left_idx) + sse_of_indices(y, left_right_idx)\n            sse_right_after = sse_of_indices(y, right_idx)\n            total_sse_a = sse_left_after + sse_right_after\n\n            # Option B: split right child by other_feat\n            gain_right, thr_right, right_left_idx, right_right_idx, _ = best_split_on_feature(X[:, other_feat], y, right_idx, min_leaf)\n            sse_right_after_b = sse_of_indices(y, right_idx) if gain_right = 0.0 else (sse_of_indices(y, right_left_idx) + sse_of_indices(y, right_right_idx))\n            sse_left_after_b = sse_of_indices(y, left_idx)\n            total_sse_b = sse_left_after_b + sse_right_after_b\n\n            best_total_sse = min(best_total_sse, total_sse_a, total_sse_b)\n\n    return best_total_sse\n\ndef generate_data(n, alpha, beta, a, b, sigma, lam, sigma_z, seed):\n    rng = np.random.default_rng(seed)\n    x1 = rng.uniform(0.0, 1.0, size=n)\n    x2 = rng.uniform(0.0, 1.0, size=n)\n    i1 = (x1 > a).astype(float)\n    i2 = (x2 > b).astype(float)\n    eps = rng.normal(0.0, sigma, size=n)\n    xi = rng.normal(0.0, sigma_z, size=n)\n    y = alpha * i1 + beta * i2 + eps\n    z = lam * (i1 + i2) + xi\n    # Features in order: x1, x2, z\n    X = np.column_stack([x1, x2, z])\n    return X, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a dict of parameters.\n    test_cases = [\n        # Case 1: happy path\n        dict(n=2000, alpha=1.0, beta=1.0, a=0.5, b=0.5, sigma=0.1, lam=0.2, sigma_z=0.5, S=2, m=25, seed=0),\n        # Case 2: strong distractor trap\n        dict(n=2000, alpha=1.0, beta=1.0, a=0.5, b=0.5, sigma=0.1, lam=2.0, sigma_z=0.05, S=2, m=25, seed=1),\n        # Case 3: skewed thresholds\n        dict(n=2000, alpha=1.5, beta=0.5, a=0.85, b=0.2, sigma=0.1, lam=0.8, sigma_z=0.2, S=2, m=20, seed=2),\n        # Case 4: noise dominated\n        dict(n=2000, alpha=1.0, beta=1.0, a=0.5, b=0.5, sigma=1.0, lam=0.5, sigma_z=0.5, S=2, m=25, seed=3),\n    ]\n\n    results = []\n    for params in test_cases:\n        X, y = generate_data(\n            n=params[\"n\"],\n            alpha=params[\"alpha\"],\n            beta=params[\"beta\"],\n            a=params[\"a\"],\n            b=params[\"b\"],\n            sigma=params[\"sigma\"],\n            lam=params[\"lam\"],\n            sigma_z=params[\"sigma_z\"],\n            seed=params[\"seed\"],\n        )\n        # Greedy best-first CART with at most S splits\n        features_used, greedy_sse = greedy_best_first_cart_two_splits(\n            X, y, split_budget=params[\"S\"], min_leaf=params[\"m\"]\n        )\n        # Determine recovery: features_used must include both x1 (index 0) and x2 (index 1)\n        used_set = set(features_used)\n        recovered = (0 in used_set) and (1 in used_set)\n        results.append(recovered)\n\n        # Optionally, compute structured best SSE (not used in printed results but can be kept for inspection if needed)\n        # structured_sse = best_structured_two_split_sse(X, y, min_leaf=params[\"m\"], feature_indices=(0, 1))\n        # diff = greedy_sse - structured_sse\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(['True' if r else 'False' for r in results])}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3113028"}]}