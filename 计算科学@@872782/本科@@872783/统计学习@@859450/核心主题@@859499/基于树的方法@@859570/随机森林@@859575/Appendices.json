{"hands_on_practices": [{"introduction": "决策树的核心在于其分裂准则，它决定了在每个节点如何划分数据。本练习将深入探讨两种最常见的分裂准则——基尼不纯度（Gini impurity）和信息熵（entropy）——的数学基础。通过在一个类不平衡的假设场景中比较它们的表现，你将亲身体会到它们在处理稀有类别时的敏感性差异，这是在现实世界应用中一个至关重要的考量。[@problem_id:3166111]", "problem": "给定一个类别为 $\\{0,1\\}$ 的二元分类问题。设类别先验为 $P(Y=1)=\\pi$ 和 $P(Y=0)=1-\\pi$，其中 $\\pi \\in [0,1]$。考虑一个基于二元特征 $X \\in \\{0,1\\}$ 的单一二元候选分裂，其类条件概率为 $P(X=1 \\mid Y=1)=a$ 和 $P(X=1 \\mid Y=0)=b$，其中 $a \\in [0,1]$ 且 $b \\in [0,1]$。该分裂将父节点划分为两个子节点：$X=1$ 的样本进入左子节点，$X=0$ 的样本进入右子节点。\n\n使用以下定义作为基本依据：\n- 对于一个正类概率为 $p$ 的节点，其 Gini 不纯度为 $G(p)=2p(1-p)$。\n- 对于一个正类概率为 $p$ 的节点，其 Shannon 熵（单位为比特）为 $H(p)=-p\\log_2 p -(1-p)\\log_2(1-p)$，并约定 $0\\log_2 0$ 的值为 $0$。\n- 对于任意不纯度函数 $I(\\cdot)$，将一个正类概率为 $p$ 的父节点分裂为两个子节点时，其不纯度减少量（对于熵而言也称为信息增益）为 $I(p)-w_L I(p_L)-w_R I(p_R)$，其中 $w_L$ 和 $w_R$ 分别是进入左、右子节点的样本比例，而 $p_L$ 和 $p_R$ 分别是左、右子节点内部的正类概率。\n\n基于这些定义，推导出 $w_L$、$w_R$、$p_L$ 和 $p_R$ 关于 $\\pi$、$a$ 和 $b$ 的表达式，然后计算 Gini 不纯度和 Shannon 熵的不纯度减少量。对于固定的 $(a,b)$ 对和给定的 $\\pi$，将 Gini 和熵的不纯度减少量分别表示为 $R_G(\\pi;a,b)$ 和 $R_H(\\pi;a,b)$。\n\n你的任务是编写一个完整、可运行的程序，该程序：\n- 使用端点值 $\\pi=0.01$ 和 $\\pi=0.50$ 来评估分裂质量对稀有类别的敏感度，方法是计算敏感度比率\n  $$S_G(a,b)=\\frac{R_G(0.01;a,b)}{R_G(0.50;a,b)} \\quad \\text{和} \\quad S_H(a,b)=\\frac{R_H(0.01;a,b)}{R_H(0.50;a,b)}。$$\n- 如果分母 $R_G(0.50;a,b)$ 为 $0$，则定义 $S_G(a,b)=0.0$。如果分母 $R_H(0.50;a,b)$ 为 $0$，则定义 $S_H(a,b)=0.0$。此约定确保在退化情况下输出有明确定义。\n\n仅使用所提供的定义来实现上述计算，不得引用任何其他预先推导出的结果。\n\n测试套件：\n- 使用以下五个 $(a,b)$ 对作为不同的测试用例，以探究不同的信号机制和边缘行为：\n  - 用例一（强分裂）：$(a,b)=(0.90,0.10)$。\n  - 用例二（弱分裂）：$(a,b)=(0.60,0.40)$。\n  - 用例三（无信号边界）：$(a,b)=(0.50,0.50)$。\n  - 用例四（高真阳性率和中等假阳性率）：$(a,b)=(0.99,0.49)$。\n  - 用例五（负样本中的稀有触发）：$(a,b)=(0.30,0.01)$。\n\n最终输出格式要求：\n- 你的程序必须输出一行，包含一个扁平化的 10 个浮点数列表，顺序为\n  $$[S_G(a_1,b_1), S_H(a_1,b_1), S_G(a_2,b_2), S_H(a_2,b_2), \\dots, S_G(a_5,b_5), S_H(a_5,b_5)],$$\n  其中 $(a_1,b_1),\\dots,(a_5,b_5)$ 遵循上述测试套件的顺序。\n- 每个数字都必须以十进制形式打印。不涉及物理单位或角度。\n- 列表必须以逗号分隔，并用方括号括起来，不含额外的空格或文本。\n\n你的程序必须是自包含的，不需要任何输入，并且确定性地执行。", "solution": "该问题是在一个标准的贝叶斯概率框架内定义的。设 $\\pi = P(Y=1)$ 是正类的先验概率。特征 $X$ 是二元的，其类条件概率给定为 $P(X=1 \\mid Y=1) = a$ 和 $P(X=1 \\mid Y=0) = b$。分裂根据 $X$ 的值对数据进行划分，$X=1$ 进入左子节点，$X=0$ 进入右子节点。\n\n**中间量的推导：**\n\n1.  **父节点正类概率 $p$**：\n    父节点包含分裂前的所有样本。因此，父节点中正类的概率就是正类的整体先验概率。\n    $$p = P(Y=1) = \\pi$$\n\n2.  **子节点样本比例 $w_L$ 和 $w_R$**：\n    这些是观测到 $X=1$（左子节点）和 $X=0$（右子节点）的边际概率。我们使用全概率定律来找到它们。\n    - $w_L$ 是左子节点（$X=1$）中的样本比例：\n        $$w_L = P(X=1) = P(X=1 \\mid Y=1)P(Y=1) + P(X=1 \\mid Y=0)P(Y=0)$$\n        代入给定值：\n        $$w_L = a\\pi + b(1-\\pi)$$\n    - $w_R$ 是右子节点（$X=0$）中的样本比例：\n        $$w_R = P(X=0) = 1 - P(X=1) = 1 - w_L$$\n        或者，使用 $X=0$ 的类条件概率，即 $P(X=0 \\mid Y=1) = 1-a$ 和 $P(X=0 \\mid Y=0) = 1-b$：\n        $$w_R = P(X=0 \\mid Y=1)P(Y=1) + P(X=0 \\mid Y=0)P(Y=0) = (1-a)\\pi + (1-b)(1-\\pi)$$\n        很容易验证 $w_L + w_R = (a\\pi + b(1-\\pi)) + ((1-a)\\pi + (1-b)(1-\\pi)) = \\pi(a+1-a) + (1-\\pi)(b+1-b) = \\pi + (1-\\pi) = 1$。\n\n3.  **子节点正类概率 $p_L$ 和 $p_R$**：\n    这些是后验概率，使用贝叶斯定理计算。\n    -   $p_L$ 是在 $X=1$ 的条件下，左子节点中正类的概率。\n        $$p_L = P(Y=1 \\mid X=1) = \\frac{P(X=1 \\mid Y=1)P(Y=1)}{P(X=1)}$$\n        代入先前推导的表达式：\n        $$p_L = \\frac{a\\pi}{w_L} = \\frac{a\\pi}{a\\pi + b(1-\\pi)}$$\n        这仅在 $w_L > 0$ 时有定义。如果 $w_L=0$，则左子节点为空，其对不纯度减少量的贡献为零。\n    -   $p_R$ 是在 $X=0$ 的条件下，右子节点中正类的概率。\n        $$p_R = P(Y=1 \\mid X=0) = \\frac{P(X=0 \\mid Y=1)P(Y=1)}{P(X=0)}$$\n        代入先前推导的表达式：\n        $$p_R = \\frac{(1-a)\\pi}{w_R} = \\frac{(1-a)\\pi}{(1-a)\\pi + (1-b)(1-\\pi)}$$\n        这仅在 $w_R > 0$ 时有定义。如果 $w_R=0$，则右子节点为空。\n\n**不纯度减少量的计算：**\n\n有了这些量，我们就可以使用提供的公式计算任何不纯度函数 $I(\\cdot)$ 的不纯度减少量：\n$$R_I(\\pi;a,b) = I(p) - [w_L I(p_L) + w_R I(p_R)]$$\n其中 $p=\\pi$。\n\n-   **对于 Gini 不纯度**：\n    Gini 不纯度为 $G(p) = 2p(1-p)$。减少量为：\n    $$R_G(\\pi;a,b) = G(\\pi) - [w_L G(p_L) + w_R G(p_R)]$$\n    $$R_G(\\pi;a,b) = 2\\pi(1-\\pi) - \\left[ w_L \\cdot 2p_L(1-p_L) + w_R \\cdot 2p_R(1-p_R) \\right]$$\n\n-   **对于 Shannon 熵**：\n    Shannon 熵为 $H(p) = -p\\log_2 p - (1-p)\\log_2(1-p)$。该减少量，也称为信息增益，为：\n    $$R_H(\\pi;a,b) = H(\\pi) - [w_L H(p_L) + w_R H(p_R)]$$\n    $$R_H(\\pi;a,b) = \\left( -\\pi\\log_2 \\pi - (1-\\pi)\\log_2(1-\\pi) \\right) - \\left[ w_L H(p_L) + w_R H(p_R) \\right]$$\n\n这些表达式在提供的程序中被直接实现。程序为测试套件中的每个 $(a, b)$ 对计算 $\\pi=0.01$ 和 $\\pi=0.50$ 时的这些减少量值，然后计算所需的敏感度比率 $S_G$ 和 $S_H$。分母为零的特殊情况（当 $a=b$ 时发生，此时分裂不提供信息）通过按规定将比率设置为 $0.0$ 来处理。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates sensitivity ratios for Gini and Entropy impurity reductions based on a binary split.\n    \"\"\"\n\n    def shannon_entropy(p):\n        \"\"\"\n        Calculates Shannon entropy H(p) = -p*log2(p) - (1-p)*log2(1-p).\n        Handles p=0 and p=1 cases where H(p)=0.\n        \"\"\"\n        if p == 0 or p >= 1:\n            return 0.0\n        return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n    def gini_impurity(p):\n        \"\"\"\n        Calculates Gini impurity G(p) = 2*p*(1-p).\n        \"\"\"\n        return 2 * p * (1 - p)\n\n    def calculate_reductions(pi, a, b):\n        \"\"\"\n        Calculates the impurity reduction for Gini and Shannon entropy.\n\n        Args:\n            pi (float): The prior probability of class 1, P(Y=1).\n            a (float): The class-conditional probability P(X=1 | Y=1).\n            b (float): The class-conditional probability P(X=1 | Y=0).\n\n        Returns:\n            tuple: A tuple containing (Gini reduction, Shannon entropy reduction).\n        \"\"\"\n        # If a=b, feature X is independent of class Y, so impurity reduction is 0.\n        if np.isclose(a, b):\n            return 0.0, 0.0\n        \n        # Parent node positive class probability is pi.\n        p_parent = pi\n\n        # Calculate proportions of samples in left (X=1) and right (X=0) children.\n        w_L = a * pi + b * (1 - pi)\n        w_R = 1.0 - w_L\n\n        # Calculate parent node impurity.\n        parent_gini = gini_impurity(p_parent)\n        parent_entropy = shannon_entropy(p_parent)\n\n        # Calculate weighted average of child node impurities.\n        gini_children = 0.0\n        entropy_children = 0.0\n\n        # Contribution from left child (X=1).\n        if w_L > 0:\n            p_L = (a * pi) / w_L\n            gini_children += w_L * gini_impurity(p_L)\n            entropy_children += w_L * shannon_entropy(p_L)\n\n        # Contribution from right child (X=0).\n        if w_R > 0:\n            p_R = ((1 - a) * pi) / w_R\n            gini_children += w_R * gini_impurity(p_R)\n            entropy_children += w_R * shannon_entropy(p_R)\n        \n        # Impurity reduction is parent impurity minus weighted child impurity.\n        reduction_gini = parent_gini - gini_children\n        reduction_entropy = parent_entropy - entropy_children\n        \n        return reduction_gini, reduction_entropy\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.90, 0.10), # Case one (strong split)\n        (0.60, 0.40), # Case two (weak split)\n        (0.50, 0.50), # Case three (no signal boundary)\n        (0.99, 0.49), # Case four (high true positive and moderate false positive)\n        (0.30, 0.01), # Case five (rare trigger among negatives)\n    ]\n\n    pi_rare = 0.01\n    pi_balanced = 0.50\n\n    results = []\n    for a, b in test_cases:\n        # Calculate reductions for the rare class (imbalanced) scenario.\n        R_G_rare, R_H_rare = calculate_reductions(pi_rare, a, b)\n        \n        # Calculate reductions for the balanced class scenario.\n        R_G_balanced, R_H_balanced = calculate_reductions(pi_balanced, a, b)\n\n        # Calculate sensitivity ratios S_G and S_H.\n        # Handle the case where the denominator is zero.\n        S_G = 0.0 if np.isclose(R_G_balanced, 0) else R_G_rare / R_G_balanced\n        S_H = 0.0 if np.isclose(R_H_balanced, 0) else R_H_rare / R_H_balanced\n        \n        results.extend([S_G, S_H])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3166111"}, {"introduction": "掌握了随机森林的构建和解释后，是时候面对一个来自应用建模领域的现实警示了。本练习将模拟一个常见的陷阱——“目标泄漏”（target leakage），即关于结果的信息无意中污染了预测特征。通过这个实践，你将看到一个朴素使用的随机森林模型会如何错误地将泄漏特征识别为最重要，从而学会作为一名数据科学家所必需的批判性思维。[@problem_id:2386893]", "problem": "您正在对一个信贷组合中的二元贷款违约结果进行建模。对于每个观测值 $i \\in \\{1,\\dots,n\\}$，设二元目标变量为 $y_i \\in \\{0,1\\}$，其中 $y_i = 1$ 表示违约。您将生成具有经济可解释性的合成协变量，然后添加一个会泄露关于 $y_i$ 信息的微弱事后协变量。然后，您必须量化一个决策桩集成（一种单次分裂决策森林）如何按重要性对协变量进行排序，并为每个测试用例报告最重要协变量的从零开始的索引。索引必须以整数形式报告。\n\n数据生成过程：\n- 设基础协变量的数量为 $p_b = 5$。对每个观测值 $i$，从一个均值为零的多元正态分布中抽取一个基础特征向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$，其协方差矩阵为 $\\Sigma(\\rho) \\in \\mathbb{R}^{p_b \\times p_b}$，定义如下\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\n其中 $I_{p_b}$ 是大小为 $p_b$ 的单位矩阵，$\\mathbf{1}$ 是 $p_b$ 维的全一向量。标量 $\\rho \\in (-\\frac{1}{p_b-1},1)$ 控制基础特征间的公共相关性。\n- 独立于 $\\mathbf{x}_i$ 抽取一个特异性宏观因子 $m_i \\sim \\mathcal{N}(0,1)$。\n- 通过逻辑指数定义一个潜得分数 $s_i$：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i,\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\beta_1 = 0.8$，$\\beta_2 = -1.0$，$\\beta_3 = 0.6$，$\\beta_4 = 0.0$，$\\beta_5 = 0.5$，以及 $\\gamma = 0.7$。\n- 通过逻辑函数定义违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}.\n$$\n- 对每个 $i$ 独立地从伯努利分布中抽取二元结果 $y_i \\sim \\text{Bernoulli}(p_i)$。\n- 定义一个泄露目标信息的事后协变量 $z_i$ 如下\n$$\nz_i = \\lambda \\, y_i + \\delta_i,\n$$\n其中 $\\delta_i \\sim \\mathcal{N}(0,\\sigma^2)$ 与其他所有变量独立。参数 $\\lambda \\in \\mathbb{R}$ 控制泄露的强度，$\\sigma \\ge 0$ 控制掩盖泄露的噪声量。\n- 为进行建模，您将按如下方式构成特征向量 $\\tilde{\\mathbf{x}}_i$。如果测试用例标志 $\\text{include\\_leak} = 1$，则设\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}, z_i\\big] \\in \\mathbb{R}^{p},\n$$\n其中 $p = p_b + 1$，泄露协变量位于从零开始的索引 $p_b$ 处。如果 $\\text{include\\_leak} = 0$，则设\n$$\n\\tilde{\\mathbf{x}}_i = \\big[x_{i,1}, x_{i,2}, x_{i,3}, x_{i,4}, x_{i,5}\\big] \\in \\mathbb{R}^{p},\n$$\n其中 $p = p_b$。\n\n模型与重要性：\n- 考虑一个由 $T=200$ 个决策桩（单次分裂决策树）组成的集成。对于每棵树 $t \\in \\{1,\\dots,T\\}$：\n  - 通过从 $\\{1,\\dots,n\\}$ 中有放回地抽样索引，生成一个大小为 $n$ 的自助采样样本。\n  - 令 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$，并从 $\\{0,\\dots,p-1\\}$ 中均匀随机地选择 $m_{\\text{try}}$ 个不同的特征用于考虑分裂。\n  - 对于每个选定的特征 $j$，考虑形式为 $x_{j} \\le \\tau$ 的分裂，其中阈值 $\\tau$ 取自该特征在自助采样样本上连续排序观测值的中点，并排除会导致空子节点的阈值。设 $G(S)$ 表示一组二元标签 $S$ 的基尼不纯度：\n  $$\n  G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2,\n  $$\n  其中 $n_c$ 是集合 $S$ 中类别 $c$ 的计数。对于一个标签多重集为 $S$ 的父节点，分裂为左子节点 $S_L$ 和右子节点 $S_R$，不纯度减少量定义为\n  $$\n  \\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right).\n  $$\n  - 在所考虑的特征和阈值中，选择能使 $\\Delta G$ 最大化的特征 $j^\\star$ 和阈值 $\\tau^\\star$。通过在 $(j^\\star,\\tau^\\star)$ 处分裂来生成一个决策桩。\n  - 将所选分裂实现的不纯度减少量 $\\Delta G^\\star$ 归因于特征 $j^\\star$。\n- 定义特征 $j$ 的重要性为在 $T$ 棵树上归因于它的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}.\n$$\n- 对于每个测试用例，计算使 $I_j$ 最大化的索引 $j_{\\max} \\in \\{0,\\dots,p-1\\}$。如果出现平局（即最大值不唯一），则取达到最大值的最小索引。\n\n测试套件：\n为保证可复现性，每个测试用例在数据生成和集成构建中都使用一个独立的随机种子 $s$。使用以下四个测试用例，每个用例由元组 $(n,\\sigma,\\lambda,\\rho,\\text{include\\_leak}, s)$ 指定：\n- 用例 A: $(3000, 0.1, 0.9, 0.2, 1, 11)$。\n- 用例 B: $(3000, 0.3, 0.6, 0.2, 1, 12)$。\n- 用例 C: $(1200, 0.1, 0.9, 0.2, 0, 13)$。\n- 用例 D: $(3000, 0.6, 0.4, 0.2, 1, 14)$。\n\n程序所需行为及输出：\n- 对于每个测试用例，根据上述过程生成数据，训练前述的集成模型，计算特征重要性 $\\{I_j\\}_{j=0}^{p-1}$，并返回最重要特征的从零开始的索引 $j_{\\max}$。\n- 您的程序必须生成单行输出，其中包含四个索引，格式为方括号内以逗号分隔的列表，顺序与测试用例相同，例如 [$i_1$,$i_2$,$i_3$,$i_4$]。不应打印任何额外文本。", "solution": "所呈现的问题是计算统计学和机器学习领域一个有效且适定的练习，特别关注基于树的集成模型中特征重要性的评估。数据生成过程被严格定义，并在金融计量经济学的标准模型中有其科学依据。任务是使用决策桩集成中的基尼不纯度减少量来量化特征重要性，并识别出最具影响力的特征，尤其是在存在“泄露”协变量的情况下。该问题是客观、自洽且算法明确的，从而可以得到一个唯一的、可复现的解。\n\n对于每个测试用例，求解方法分两个主要阶段进行：数据生成和带有重要性计算的模型训练。所有数学实体，包括变量、参数和数值，都按要求使用 LaTeX 表示。\n\n**1. 数据生成过程**\n\n对于每个指定的测试用例，根据以下随机过程生成一个大小为 $n$ 的合成数据集。使用随机种子 $s$ 来确保可复现性。\n\n- **基础协变量**：一组 $p_b = 5$ 个基础协变量，对每个观测值 $i \\in \\{1, \\dots, n\\}$ 表示为向量 $\\mathbf{x}_i \\in \\mathbb{R}^{p_b}$，是从一个多元正态分布 $\\mathcal{N}(\\mathbf{0}, \\Sigma(\\rho))$ 中抽取的。协方差矩阵 $\\Sigma(\\rho)$ 定义为\n$$\n\\Sigma(\\rho) = (1-\\rho) I_{p_b} + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\n其中 $I_{p_b}$ 是 $p_b \\times p_b$ 的单位矩阵，$\\mathbf{1}$ 是一个 $p_b$ 维的全一向量。参数 $\\rho$ 控制这些基础特征间的等相关性。\n\n- **潜得分数与违约概率**：独立地抽取一个特异性因子 $m_i \\sim \\mathcal{N}(0,1)$。潜得分数 $s_i$ 构建为基础协变量和宏观因子的线性组合：\n$$\ns_i = \\beta_0 + \\sum_{j=1}^{p_b} \\beta_j x_{i,j} + \\gamma m_i\n$$\n系数固定为 $\\beta_0 = -0.5$，$\\boldsymbol{\\beta}_{1..p_b} = [0.8, -1.0, 0.6, 0.0, 0.5]$，以及 $\\gamma = 0.7$。请注意，$x_{i,4}$ 的系数为 $\\beta_4 = 0.0$，这使得该特征在构造上相对于潜得分数是无信息的。然后使用标准逻辑函数将潜得分数转换为违约概率 $p_i$：\n$$\np_i = \\frac{1}{1 + e^{-s_i}}\n$$\n\n- **二元结果**：二元目标变量 $y_i \\in \\{0, 1\\}$，表示未违约 ($0$) 或违约 ($1$)，是从一个参数为所生成概率的伯努利分布中抽取的，$y_i \\sim \\text{Bernoulli}(p_i)$。\n\n- **泄露协变量**：生成一个事后协变量 $z_i$ 以模拟来自目标变量的信息泄露。其定义为：\n$$\nz_i = \\lambda \\, y_i + \\delta_i\n$$\n其中 $\\delta_i$ 是从正态分布 $\\mathcal{N}(0, \\sigma^2)$ 中抽取的噪声项。参数 $\\lambda$ 控制泄露的强度，$\\sigma$ 控制噪声水平。$|\\lambda|$ 与 $\\sigma$ 的高比率意味着 $z_i$ 和 $y_i$ 之间存在强烈的、易于检测的联系。\n\n- **最终特征矩阵**：组装完整的特征矩阵 $\\tilde{\\mathbf{X}}$。如果 `include_leak` 标志为 $1$，则特征集为 $\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}, z_i] \\in \\mathbb{R}^{6}$。泄露特征 $z_i$ 位于最后的位置（从零开始的索引为 $5$）。如果标志为 $0$，则只使用基础协变量，$\\tilde{\\mathbf{x}}_i = [x_{i,1}, \\dots, x_{i,5}] \\in \\mathbb{R}^{5}$。特征的数量用 $p$ 表示。\n\n**2. 特征重要性量化**\n\n每个特征的重要性通过训练一个由 $T = 200$ 个决策桩组成的集成来确定。决策桩是只有一个分裂点的决策树。\n\n- **集成构建**：对于集成中的 $T$ 个决策桩中的每一个：\n    1. 通过从完整数据集 $(\\tilde{\\mathbf{X}}, \\mathbf{y})$ 中有放回地抽样，创建一个大小为 $n$ 的自助采样样本。\n    2. 随机选择一个由 $m_{\\text{try}} = \\lceil \\sqrt{p} \\rceil$ 个不同特征组成的子集。\n    3. 对每个选定的特征，找到最优分裂。一个分裂由一个特征 $j$ 和一个阈值 $\\tau$ 定义。分裂的质量由基尼不纯度减少量 $\\Delta G$ 来衡量。一组标签 $S$ 的基尼不纯度由以下公式给出：\n    $$\n    G(S) = 1 - \\sum_{c \\in \\{0,1\\}} \\left(\\frac{n_c}{|S|}\\right)^2\n    $$\n    其中 $n_c$ 是集合 $S$ 中类别 $c$ 的计数。不纯度减少量是父节点的不纯度与两个子节点不纯度的加权平均值之差。\n    4. 选择能产生最大不纯度减少量 $\\Delta G^\\star$ 的特征 $j^\\star$ 和阈值 $\\tau^\\star$ 作为决策桩的分裂点。潜在的阈值是自助采样样本中该特征的连续唯一排序值的中点。\n\n- **重要性聚合**：一个特征 $j$ 的重要性，记为 $I_j$，计算为它在集成中所有树上所产生的不纯度减少量之和：\n$$\nI_j = \\sum_{t=1}^{T} \\Delta G_t \\cdot \\mathbf{1}\\{j_t^\\star = j\\}\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n- **结果**：最后，对于每个测试用例，找出具有最高重要性得分的特征的从零开始的索引 $j_{\\max} = \\arg\\max_j I_j$。平局通过选择最小的索引来解决。该索引即为该用例的输出。该过程使用 Python 实现，遵循指定的库和随机种子，以确保可验证的结果。", "answer": "```python\nimport numpy as np\nfrom math import ceil, sqrt\n\ndef _gini_impurity(y):\n    \"\"\"Calculates the Gini impurity of a set of binary labels.\"\"\"\n    n_samples = len(y)\n    if n_samples == 0:\n        return 0.0\n    n1_count = np.sum(y)\n    p1 = n1_count / n_samples\n    p0 = 1.0 - p1\n    return 1.0 - (p0**2 + p1**2)\n\ndef _find_best_split(X_boot, y_boot, feature_indices):\n    \"\"\"Finds the best split for a single decision stump.\"\"\"\n    n_samples = len(y_boot)\n    if n_samples = 1:\n        return -1, -1.0\n\n    best_gain = -1.0\n    best_feature = -1\n\n    gini_parent = _gini_impurity(y_boot)\n    n1_total = np.sum(y_boot)\n    \n    for j in feature_indices:\n        feature_values = X_boot[:, j]\n        \n        unique_vals = np.unique(feature_values)\n        if len(unique_vals)  2:\n            continue\n            \n        # Efficiently find the best split for feature j\n        sorted_indices = np.argsort(feature_values)\n        y_sorted = y_boot[sorted_indices]\n        x_sorted = feature_values[sorted_indices]\n        \n        y_cumsum = np.cumsum(y_sorted)\n\n        split_points = np.where(x_sorted[:-1] != x_sorted[1:])[0]\n\n        for i in split_points:\n            n_left = i + 1\n            n_right = n_samples - n_left\n\n            n1_left = y_cumsum[i]\n            n1_right = n1_total - n1_left\n\n            gini_left = 1.0 - ((n1_left / n_left)**2 + ((n_left - n1_left) / n_left)**2)\n            gini_right = 1.0 - ((n1_right / n_right)**2 + ((n_right - n1_right) / n_right)**2)\n            \n            gain = gini_parent - (n_left / n_samples * gini_left + n_right / n_samples * gini_right)\n\n            if gain > best_gain:\n                best_gain = gain\n                best_feature = j\n                \n    return best_feature, best_gain\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, lambda, rho, include_leak, seed)\n        (3000, 0.1, 0.9, 0.2, 1, 11),\n        (3000, 0.3, 0.6, 0.2, 1, 12),\n        (1200, 0.1, 0.9, 0.2, 0, 13),\n        (3000, 0.6, 0.4, 0.2, 1, 14),\n    ]\n\n    results = []\n\n    for n, sigma, lam, rho, include_leak, seed in test_cases:\n        # Set seed for reproducibility for the entire test case\n        np.random.seed(seed)\n\n        # 1. Data Generation\n        p_b = 5\n        betas = np.array([0.8, -1.0, 0.6, 0.0, 0.5])\n        beta_0 = -0.5\n        gamma = 0.7\n        \n        # Covariance matrix\n        cov_matrix = (1 - rho) * np.identity(p_b) + rho * np.ones((p_b, p_b))\n        \n        # Base features\n        X_base = np.random.multivariate_normal(np.zeros(p_b), cov_matrix, n)\n        \n        # Macro factor\n        m = np.random.randn(n)\n        \n        # Latent score\n        s = beta_0 + X_base @ betas + gamma * m\n        \n        # Default probability\n        p_default = 1.0 / (1.0 + np.exp(-s))\n        \n        # Binary outcome\n        y = np.random.binomial(1, p_default, n)\n        \n        # Assemble final feature matrix\n        if include_leak == 1:\n            delta = np.random.normal(0, sigma, n)\n            z = lam * y + delta\n            X = np.c_[X_base, z]\n        else:\n            X = X_base\n        \n        n_samples, p = X.shape\n\n        # 2. Ensemble Training and Importance Calculation\n        T = 200\n        m_try = ceil(sqrt(p))\n        importances = np.zeros(p)\n        \n        all_indices = np.arange(n_samples)\n\n        for _ in range(T):\n            # Bootstrap sample\n            boot_indices = np.random.choice(all_indices, size=n_samples, replace=True)\n            X_boot, y_boot = X[boot_indices], y[boot_indices]\n            \n            # Feature subsampling\n            feature_indices = np.random.choice(p, size=m_try, replace=False)\n            \n            # Find best split for this stump\n            best_feature, best_gain = _find_best_split(X_boot, y_boot, feature_indices)\n            \n            if best_feature != -1:\n                importances[best_feature] += best_gain\n\n        # 3. Find the most important feature index\n        j_max = np.argmax(importances)\n        results.append(j_max)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386893"}]}