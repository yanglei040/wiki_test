{"hands_on_practices": [{"introduction": "要掌握贝叶斯信息准则（BIC），第一步是熟练运用其计算公式。这个练习提供了一个负二项回归模型的假设场景，旨在让您直接应用BIC公式。在解决这个问题时[@problem_id:806248]，请特别注意正确识别模型中待估计参数的总数 $k$，因为这是实践中一个常见的易错点。", "problem": "**背景**\n\n一个随机变量 $Y$ 服从负二项分布，其均值为 $\\mu > 0$，散布参数为 $\\theta > 0$，记为 $Y \\sim \\text{NB}(\\mu, \\theta)$，其概率质量函数由以下公式给出：\n$$ P(Y=y) = \\frac{\\Gamma(y+\\theta)}{\\Gamma(y+1)\\Gamma(\\theta)} \\left(\\frac{\\theta}{\\theta+\\mu}\\right)^\\theta \\left(\\frac{\\mu}{\\theta+\\mu}\\right)^y, \\quad \\text{for } y \\in \\{0, 1, 2, \\dots\\}. $$\n其均值和方差分别为 $E[Y] = \\mu$ 和 $\\text{Var}(Y) = \\mu + \\mu^2/\\theta$。\n\n在负二项回归模型中，响应变量 $Y_i$ 的条件均值 $\\mu_i$ 被建模为预测变量向量 $\\mathbf{x}_i$ 的函数。一种常见的设定使用对数链接函数：\n$$ \\log(\\mu_i) = \\mathbf{x}_i^T \\boldsymbol{\\beta}, $$\n其中 $\\boldsymbol{\\beta}$ 是回归系数向量。此模型中需要从数据中估计的参数是回归系数 $\\boldsymbol{\\beta}$ 和公共散布参数 $\\theta$。\n\n贝叶斯信息准则 (BIC) 是一种广泛使用的模型选择准则，其定义为：\n$$ \\text{BIC} = k \\ln(n) - 2 \\hat{L}, $$\n其中 $n$ 是样本量，$k$ 是模型中估计参数的总数，$\\hat{L}$ 是模型对数似然函数的最大化值。\n\n**问题描述**\n\n一位生物统计学家正在使用负二项回归模型对植物叶片上的病斑数量进行建模。该分析基于 $n$ 片叶子的样本。回归模型包含 $p$ 个不同的预测变量（例如植物年龄、土壤pH值和日照暴露度）以及一个截距项。\n\n在进行最大似然估计后，确定该模型的对数似然函数的最大化值为：\n$$ \\hat{L} = Cnp - D n \\log(n) $$\n其中 $C$ 和 $D$ 是给定的正常数。\n\n您的任务是推导此负二项回归模型的贝叶斯信息准则 (BIC) 的表达式，用 $n$、$p$、$C$ 和 $D$ 表示。", "solution": "1. 估计参数的数量：\n$$\nk = p + 1\\ (\\text{系数，包括截距}) + 1\\ (\\theta)\n= p + 2.\n$$\n2. BIC 定义：\n$$\n\\mathrm{BIC} = k\\ln(n) \\;-\\; 2\\,\\hat L.\n$$\n3. 代入 $k$ 和 $\\hat L$：\n$$\n\\mathrm{BIC} \n= (p+2)\\ln(n) \\;-\\; 2\\bigl(Cnp - Dn\\ln(n)\\bigr).\n$$\n4. 展开并简化：\n$$\n\\mathrm{BIC}\n= (p+2)\\ln(n) - 2Cnp + 2D\\,n\\ln(n).\n$$", "answer": "$$\\boxed{(p + 2)\\ln(n) + 2D\\,n\\ln(n) - 2C\\,n\\,p}$$", "id": "806248"}, {"introduction": "BIC的核心特点在于其惩罚项 $k \\ln(n)$，该项随样本量 $n$ 的增大而增强。本练习[@problem_id:2734851]通过将BIC与另一个常用的模型选择准则——赤池信息准则（AIC）进行对比，来深入探讨这一关键属性。通过推导出一个临界样本量，您将深刻理解为何BIC在处理大型数据集时，倾向于选择更简洁（参数更少）的模型，这正是其“简约性”偏好的体现。", "problem": "考虑一个固定系统发育树上的两个用于核苷酸演化的嵌套连续时间马尔可夫链替换模型，一个是有 $k_{s}$ 个自由参数的更简单的模型 $\\mathcal{M}_{s}$，另一个是有 $k_{c}$ 个自由参数的更复杂的模型 $\\mathcal{M}_{c}$，其中 $\\Delta k = k_{c} - k_{s} > 0$。设比对长度为 $n$ 个独立同分布的位点。假设对于在模型 $\\mathcal{M}_{s}$ 下生成的数据，模型 $\\mathcal{M}_{c}$ 相对于 $\\mathcal{M}_{s}$ 的最大对数似然增益是一个固定的常数 $D = \\ln \\hat{L}_{c} - \\ln \\hat{L}_{s} > 0$，该常数不随 $n$ 的增加而增加（这反映了一种不随额外独立位点的增加而变化的过拟合增益）。使用赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 的标准定义，并将 $n$ 视为样本量，完成以下任务：\n\n- 从基本原理出发，解释 BIC 中的惩罚项如何依赖于 $\\ln n$，而 AIC 中的惩罚项不依赖于 $n$。\n- 推导一个精确的封闭形式表达式，表示 BIC 将选择 $\\mathcal{M}_{s}$ 而 AIC 仍然选择 $\\mathcal{M}_{c}$ 时的最小比对长度 $n_{\\star}$，该表达式应以 $D$ 和 $\\Delta k$ 表示。\n\n假设 $D > \\Delta k$，这样对于较小的 $n$，赤池信息准则初始时会偏好 $\\mathcal{M}_{c}$。你的最终答案必须是关于 $n_{\\star}$ 的单个解析表达式。不需要进行数值计算，也不需要单位。", "solution": "我们从模型选择中使用的两个信息准则的标准定义开始。对于一个最大化似然为 $\\hat{L}$、参数数量为 $k$、样本量为 $n$ 的模型，赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 分别为\n$$\n\\mathrm{AIC} = -2 \\ln \\hat{L} + 2 k, \\quad \\mathrm{BIC} = -2 \\ln \\hat{L} + k \\ln n.\n$$\n这些是经过充分检验的公式，分别源自于 Kullback–Leibler 风险最小化的渐近估计，以及在特定先验正则性条件下对边际似然的大样本近似。\n\n首先，我们展示惩罚项如何随 $n$ 变化。AIC 的惩罚项是 $2k$，它相对于 $n$ 是一个常数。BIC 的惩罚项是 $k \\ln n$，它随 $n$ 对数增长，因为当 $n > 1$ 时，$\\ln n$ 是 $n$ 的单调递增函数。因此，随着 $n$ 的增长，BIC 的惩罚项与 $\\ln n$ 成正比无界增长，而 AIC 的惩罚项保持不变。\n\n接下来，我们比较这两个嵌套模型 $\\mathcal{M}_{c}$（复杂）和 $\\mathcal{M}_{s}$（简单）。定义 $\\Delta k = k_{c} - k_{s} > 0$ 和 $D = \\ln \\hat{L}_{c} - \\ln \\hat{L}_{s} > 0$。考虑这两个准则的差值：\n$$\n\\Delta \\mathrm{AIC} \\equiv \\mathrm{AIC}_{c} - \\mathrm{AIC}_{s} = \\left[-2 \\ln \\hat{L}_{c} + 2 k_{c}\\right] - \\left[-2 \\ln \\hat{L}_{s} + 2 k_{s}\\right] = -2(\\ln \\hat{L}_{c} - \\ln \\hat{L}_{s}) + 2 (k_{c} - k_{s}) = -2D + 2 \\Delta k.\n$$\n类似地，\n$$\n\\Delta \\mathrm{BIC} \\equiv \\mathrm{BIC}_{c} - \\mathrm{BIC}_{s} = \\left[-2 \\ln \\hat{L}_{c} + k_{c} \\ln n\\right] - \\left[-2 \\ln \\hat{L}_{s} + k_{s} \\ln n\\right] = -2(\\ln \\hat{L}_{c} - \\ln \\hat{L}_{s}) + (k_{c} - k_{s}) \\ln n = -2D + \\Delta k \\, \\ln n.\n$$\n当一个模型产生更小的值时，它被相应的准则所偏好。因此，当 $\\Delta \\mathrm{AIC} < 0$ 时 AIC 偏好 $\\mathcal{M}_{c}$，当 $\\Delta \\mathrm{AIC} > 0$ 时偏好 $\\mathcal{M}_{s}$；对于 BIC 和 $\\Delta \\mathrm{BIC}$ 也是同理。根据假设，$D > \\Delta k$，这意味着\n$$\n\\Delta \\mathrm{AIC} = -2D + 2 \\Delta k < 0,\n$$\n因此，赤池信息准则偏好 $\\mathcal{M}_{c}$，这与 $n$ 的大小无关，因为 $\\Delta \\mathrm{AIC}$ 不依赖于 $n$。\n\n为了让贝叶斯信息准则选择更简单的模型，我们需要 $\\Delta \\mathrm{BIC} > 0$，即\n$$\n-2D + \\Delta k \\, \\ln n > 0 \\quad \\Longleftrightarrow \\quad \\ln n > \\frac{2D}{\\Delta k} \\quad \\Longleftrightarrow \\quad n > \\exp\\!\\left(\\frac{2D}{\\Delta k}\\right).\n$$\n因此，当贝叶斯信息准则在偏好简单模型方面超过赤池信息准则（而 AIC 仍然偏好复杂模型）时的最小比对长度，出现在以下阈值处：\n$$\nn_{\\star} = \\exp\\!\\left(\\frac{2D}{\\Delta k}\\right).\n$$\n在所述的 $D > \\Delta k$ 假设下，这个阈值是良定义的。该假设确保了 AIC 持续偏好 $\\mathcal{M}_{c}$ 而不受 $n$ 影响，而一旦 $\\ln n$ 惩罚项超过了固定的过拟合增益 $D$，BIC 最终会偏好 $\\mathcal{M}_{s}$。", "answer": "$$\\boxed{\\exp\\!\\left(\\frac{2D}{\\Delta k}\\right)}$$", "id": "2734851"}, {"introduction": "从理论到实践，让我们来看一个更接近真实数据分析挑战的场景。在回归分析中，多重共线性（即预测变量之间高度相关）是一个常见难题。这个动手编程练习[@problem_id:3102696]将指导您通过模拟生成共线性数据，并观察BIC如何有效地惩罚模型复杂度，从而倾向于舍弃冗余的变量。这项实践将抽象的BIC分数与具体的模型选择决策以及如方差膨胀因子（VIF）等诊断工具联系起来。", "problem": "考虑一个高斯线性回归模型，其中响应向量 $y \\in \\mathbb{R}^n$ 是由两个带截距项的预测变量 $x_1 \\in \\mathbb{R}^n$ 和 $x_2 \\in \\mathbb{R}^n$ 生成的。您将研究 $x_1$ 和 $x_2$ 之间的高度共线性通过贝叶斯信息准则 (BIC) 对模型选择的影响，并将其与方差膨胀因子 (VIF) 以及最大化对数似然的变化联系起来。任务是从高斯线性模型的似然函数出发，从第一性原理实现所有内容。\n\n基本基础：\n- 假设数据由高斯线性模型 $y \\mid X, \\beta, \\sigma^2 \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n)$ 生成，其中 $X$ 是包含截距项和预测变量的设计矩阵，$\\beta$ 是回归系数向量，$\\sigma^2$ 是噪声方差。对数似然必须从此模型导出。\n- 通过使用独立标准正态噪声构建与 $x_1$ 具有目标相关性的 $x_2$ 来生成共线性预测变量。\n\n实现所需的定义：\n- 贝叶斯信息准则 (BIC) 必须使用其定义来实现，该定义基于高斯线性模型的最大化对数似然以及一个与自由参数数量和样本大小成比例的复杂度项。将所有回归系数（包括截距项）加上方差参数计为自由参数。\n- 预测变量 $x_j$ 的方差膨胀因子 (VIF) 是通过将 $x_j$ 对其余预测变量进行回归时的决定系数来定义的。\n\n每次模拟需要比较的候选模型：\n- 完整模型：截距项、$x_1$ 和 $x_2$。\n- 简化模型：仅含截距项和 $x_1$。\n\n对于下面测试套件中的每组参数，执行以下步骤：\n1. 首先从标准正态分布生成长度为 $n$ 的预测变量 $x_1$，然后设置 $x_2 = \\rho \\, x_1 + \\sqrt{1 - \\rho^2} \\, \\eta$ 来构建相关的预测变量 $x_1$ 和 $x_2$，其中 $\\eta$ 是一个独立的标准正态向量。使用提供的随机种子以确保可复现性。\n2. 根据 $y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$（其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$）使用指定参数生成 $y$。设置截距项 $\\alpha = 0$。\n3. 对于两个候选模型中的每一個，计算回归系数和噪声方差的最大似然估计，然后计算最大化对数似然。使用这些值来计算贝叶斯信息准则 (BIC)，其中自由参数的数量计为所有回归系数（包括截距项）加上方差参数。\n4. 通过将 $x_2$ 对截距项和 $x_1$ 进行回归，并使用该辅助回归中的决定系数，来计算完整模型中 $x_2$ 相对于 $x_1$ 的方差膨胀因子 (VIF)。\n5. 通过检查简化模型的 BIC 是否严格低于完整模型的 BIC 来判断 BIC 是否舍弃了冗余变量。将此报告为一个整数：如果简化模型更优（舍弃 $x_2$），则为 $1$，否则为 $0$。\n6. 计算完整模型和简化模型之间最大化对数似然的绝对差。\n7. 对于每个测试用例，输出一个包含三项的列表：整数选择指示符、四舍五入到四位小数的 $x_2$ 的 VIF，以及四舍五入到六位小数的最大化对数似然的绝对差。\n\n测试套件参数：\n- 用例 $1$：$n = 200$, $\\rho = 0.95$, $\\beta_1 = 1.0$, $\\beta_2 = 0.0$, $\\sigma = 1.0$, seed $= 42$。\n- 用例 $2$：$n = 200$, $\\rho = 0.95$, $\\beta_1 = 1.0$, $\\beta_2 = 0.1$, $\\sigma = 1.0$, seed $= 123$。\n- 用例 $3$：$n = 200$, $\\rho = 0.0$, $\\beta_1 = 1.0$, $\\beta_2 = 0.5$, $\\sigma = 1.0$, seed $= 7$。\n- 用例 $4$：$n = 40$, $\\rho = 0.9$, $\\beta_1 = 1.0$, $\\beta_2 = 0.0$, $\\sigma = 1.0$, seed $= 777$。\n- 用例 $5$：$n = 200$, $\\rho = 0.99$, $\\beta_1 = 1.0$, $\\beta_2 = 0.5$, $\\sigma = 1.0$, seed $= 2024$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格。每个元素对应于给定顺序的一个测试用例，并且其本身是按步骤 $7$ 中所述格式化的列表。例如，输出应类似于 $[[s_1,v_1,d_1],[s_2,v_2,d_2],\\dots]$，其中 $s_i$ 是选择指示符，$v_i$ 是四舍五入后的 VIF，$d_i$ 是用例 $i$ 的四舍五入后的绝对对数似然差。", "solution": "该问题要求在高斯线性回归模型与共线性预测变量的背景下，对贝叶斯信息准则 (BIC) 进行深入分析。该分析涉及从第一性原理实现必要的统计工具——最大似然估计 (MLE)、BIC 和方差膨胀因子 (VIF)。\n\n### 理论框架\n\n#### 1. 高斯线性模型与最大似然估计\n\n此分析的基础是高斯线性模型。我们假设响应向量 $y \\in \\mathbb{R}^n$ 的生成方式如下：\n$$\ny = X\\beta + \\varepsilon\n$$\n其中 $X$ 是一个 $n \\times (p+1)$ 的设计矩阵（包括一个截距列），$\\beta \\in \\mathbb{R}^{p+1}$ 是回归系数向量，$\\varepsilon$ 是独立同分布 (i.i.d.) 的高斯噪声向量，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n\n单个观测值 $y_i$ 的概率密度函数为：\n$$\nf(y_i \\mid x_i, \\beta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right)\n$$\n鉴于观测值是独立的，整个数据集的似然函数是单个密度的乘积：\n$$\nL(\\beta, \\sigma^2 \\mid y, X) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - x_i^T\\beta)^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - X\\beta\\|^2\\right)\n$$\n对数似然函数 $\\ell(\\beta, \\sigma^2) = \\log L(\\beta, \\sigma^2)$ 为：\n$$\n\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\|y - X\\beta\\|^2\n$$\n为了找到最大似然估计 (MLE)，我们相对于 $\\beta$ 和 $\\sigma^2$ 最大化 $\\ell$。\n相对于 $\\beta$ 最大化 $\\ell$ 等价于最小化残差平方和 (RSS)，即 $\\|y - X\\beta\\|^2$。这产生了标准的普通最小二乗 (OLS) 估计量：\n$$\n\\hat{\\beta}_{MLE} = (X^TX)^{-1}X^Ty\n$$\n将 $\\hat{\\beta}_{MLE}$ 代入对数似然函数并对 $\\sigma^2$求导，得到方差的 MLE：\n$$\n\\hat{\\sigma}^2_{MLE} = \\frac{1}{n}\\|y - X\\hat{\\beta}_{MLE}\\|^2 = \\frac{\\text{RSS}}{n}\n$$\n最大化对数似然，记为 $\\ell(\\hat{\\theta})$，其中 $\\hat{\\theta} = (\\hat{\\beta}_{MLE}, \\hat{\\sigma}^2_{MLE})$，是通过将这些 MLE 代回对数似然函数得到的：\n$$\n\\ell(\\hat{\\theta}) = -\\frac{n}{2}\\left( \\log(2\\pi) + \\log(\\hat{\\sigma}^2_{MLE}) + 1 \\right)\n$$\n这个公式是计算 BIC 的核心。\n\n#### 2. 贝叶斯信息准则 (BIC)\n\nBIC 是一种模型选择准则，它在模型拟合度（通过最大化对数似然来衡量）和模型复杂度之间进行权衡。其公式为：\n$$\n\\text{BIC} = k \\log(n) - 2\\ell(\\hat{\\theta})\n$$\n其中 $n$ 是样本数量，$k$ 是模型中的自由参数数量。较低的 BIC 值表示更优的模型。\n\n根据规定，$k$ 是所有估计参数的数量。对于一个有 $p$ 个预测变量的线性模型，我们有 $p+1$ 个回归系数（包括截距项）和一个方差参数 $\\sigma^2$。因此，$k = p+2$。\n对于所考虑的两个模型：\n- **完整模型 ($M_f$)**：$y$ 对截距项、$x_1$ 和 $x_2$ 进行回归。这里，$p=2$，所以自由参数的数量是 $k_f = 2 + 2 = 4$。\n- **简化模型 ($M_r$)**：$y$ 对截距项和 $x_1$ 进行回归。这里，$p=1$，所以自由参数的数量是 $k_r = 1 + 2 = 3$。\n\n每个模型的 BIC 为：\n$$\n\\text{BIC}_f = 4 \\log(n) - 2\\ell(\\hat{\\theta}_f)\n$$\n$$\n\\text{BIC}_r = 3 \\log(n) - 2\\ell(\\hat{\\theta}_r)\n$$\n如果 $\\text{BIC}_r  \\text{BIC}_f$，则简化模型更优。\n\n#### 3. 方差膨胀因子 (VIF)\n\nVIF 量化了 OLS 回归中多重共线性的严重程度。预测变量 $x_j$ 的 VIF 由以下公式给出：\n$$\n\\text{VIF}_j = \\frac{1}{1 - R_j^2}\n$$\n其中 $R_j^2$ 是将 $x_j$ 对模型中其他预测变量（包括截距项）进行辅助回归得到的决定系数。较高的 VIF（通常 $>5$ 或 $>10$）表明该预测变量与其他预测变量高度相关，这会使其系数估计的方差膨胀。\n\n对于这个问题，我们计算完整模型中 $x_2$ 的 VIF。这涉及到将 $x_2$ 对截距项和 $x_1$ 进行回归。该回归的 $R^2$，即 $R^2_{x_2|x_1}$，计算如下：\n$$\nR^2_{x_2|x_1} = 1 - \\frac{\\text{RSS}_{aux}}{\\text{TSS}_{aux}}\n$$\n其中 $\\text{RSS}_{aux}$ 是辅助回归的残差平方和，$\\text{TSS}_{aux}$ 是 $x_2$ 的总平方和。\n\n### 计算步骤\n\n对每个测试用例，执行以下计算序列：\n\n1.  **数据生成**：\n    - 使用指定的种子初始化随机数生成器以确保可复现性。\n    - 从标准正态分布 $\\mathcal{N}(0, 1)$ 中抽取一个 $n$ 维向量 $x_1$。\n    - 同样从 $\\mathcal{N}(0, 1)$ 中抽取一个独立的 $n$ 维噪声向量 $\\eta$。\n    - 构建相关的预测变量 $x_2$ 为 $x_2 = \\rho x_1 + \\sqrt{1 - \\rho^2} \\eta$，其中 $\\rho$ 是目标相关系数。\n    - 从 $\\mathcal{N}(0, \\sigma^2 I_n)$ 中抽取一个噪声向量 $\\varepsilon$。\n    - 通过真实模型生成响应向量 $y$：$y = \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon$（截距项 $\\alpha = 0$）。\n\n2.  **模型拟合与 BIC 计算**：\n    - 设计一个统一的函数，用于计算任何给定设计矩阵 $X$ 和响应 $y$ 的最大化对数似然和参数数量。\n    - **完整模型 ($M_f$)**：通过连接一列1、$x_1$ 和 $x_2$ 来形成设计矩阵 $X_f$。使用 $X_f$ 和 $y$ 计算最大化对数似然 $\\ell(\\hat{\\theta}_f)$ 和参数数量 $k_f=4$，然后用它们来计算 $\\text{BIC}_f$。\n    - **简化模型 ($M_r$)**：用一列1和 $x_1$ 组成设计矩阵 $X_r$。类似地，计算 $\\ell(\\hat{\\theta}_r)$、$k_r=3$ 和 $\\text{BIC}_r$。\n\n3.  **VIF 计算**：\n    - 计算 $x_2$ 的 VIF。辅助设计矩阵 $X_{aux}$ 由一列1和 $x_1$ 组成。\n    - 对 $x_2$ 和 $X_{aux}$ 执行 OLS 回归，以找到残差平方和 $\\text{RSS}_{aux}$。\n    - 计算 $x_2$ 的总平方和 $\\text{TSS}_{aux}$。\n    - 使用它们的定义来计算决定系数 $R^2$以及随后的 VIF。\n\n4.  **结果聚合**：\n    - 确定选择指示符：如果 $\\text{BIC}_r  \\text{BIC}_f$，则为 $1$，否则为 $0$。\n    - 计算最大化对数似然的绝对差 $|\\ell(\\hat{\\theta}_f) - \\ell(\\hat{\\theta}_r)|$。\n    - 该用例的最终输出是一个列表，包含选择指示符、$x_2$ 的 VIF（四舍五入到4位小数）和对数似然差（四舍五入到6位小数）。对所有测试用例重复此过程。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs the full analysis for each test case as specified in the problem statement.\n    This involves data generation, model fitting (full and reduced), BIC comparison,\n    VIF calculation, and log-likelihood difference computation.\n    \"\"\"\n    test_cases = [\n        # (n, rho, beta1, beta2, sigma, seed)\n        (200, 0.95, 1.0, 0.0, 1.0, 42),\n        (200, 0.95, 1.0, 0.1, 1.0, 123),\n        (200, 0.0, 1.0, 0.5, 1.0, 7),\n        (40, 0.9, 1.0, 0.0, 1.0, 777),\n        (200, 0.99, 1.0, 0.5, 1.0, 2024),\n    ]\n\n    results = []\n\n    def compute_mle_metrics(X, y):\n        \"\"\"\n        Computes the maximized log-likelihood and parameter count for a linear model.\n\n        Args:\n            X (np.ndarray): The design matrix (n_samples, n_features_with_intercept).\n            y (np.ndarray): The response vector (n_samples,).\n\n        Returns:\n            tuple: A tuple containing:\n                - float: The maximized log-likelihood.\n                - int: The number of free parameters (coefficients + variance).\n        \"\"\"\n        n_samples = X.shape[0]\n        p_coeffs = X.shape[1]\n\n        # Solve for coefficients and RSS using numerically stable least squares\n        try:\n            _, rss_array, _, _ = np.linalg.lstsq(X, y, rcond=None)\n            # lstsq returns rss as an array, even if it's a single value\n            rss = rss_array[0]\n        except np.linalg.LinAlgError:\n            return -np.inf, p_coeffs + 1\n        \n        # MLE for variance sigma^2\n        mle_var = rss / n_samples\n\n        if mle_var  1e-9:  # Avoid log(0) for perfect fits\n            log_likelihood = np.inf if np.allclose(y, X @ np.linalg.lstsq(X, y, rcond=None)[0]) else -np.inf\n        else:\n            # Maximized log-likelihood for Gaussian model\n            log_likelihood = -n_samples / 2.0 * (np.log(2.0 * np.pi) + np.log(mle_var) + 1.0)\n            \n        # Number of free parameters: p coefficients + 1 variance parameter\n        k = p_coeffs + 1\n        return log_likelihood, k\n\n    for n, rho, beta1, beta2, sigma, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Construct correlated predictors\n        x1 = rng.normal(size=n)\n        eta = rng.normal(size=n)\n        x2 = rho * x1 + np.sqrt(1 - rho**2) * eta\n\n        # Step 2: Generate response variable y\n        epsilon = rng.normal(loc=0, scale=sigma, size=n)\n        y = beta1 * x1 + beta2 * x2 + epsilon  # Intercept alpha is 0\n\n        # Step 3: Analyze full and reduced models\n        # Full model: intercept, x1, x2\n        X_f = np.c_[np.ones(n), x1, x2]\n        logL_f, k_f = compute_mle_metrics(X_f, y)\n        bic_f = k_f * np.log(n) - 2 * logL_f\n\n        # Reduced model: intercept, x1\n        X_r = np.c_[np.ones(n), x1]\n        logL_r, k_r = compute_mle_metrics(X_r, y)\n        bic_r = k_r * np.log(n) - 2 * logL_r\n\n        # Step 4: Compute VIF for x2\n        # Auxiliary regression: x2 on intercept and x1\n        X_aux = np.c_[np.ones(n), x1]\n        try:\n            _, rss_aux_array, _, _ = np.linalg.lstsq(X_aux, x2, rcond=None)\n            rss_aux = rss_aux_array.item()\n            # Total sum of squares for x2\n            tss_aux = np.sum((x2 - np.mean(x2))**2)\n            if tss_aux  1e-9: # x2 has no variance, R^2 is undefined\n                vif_x2 = 1.0\n            else:\n                r_squared_aux = 1 - rss_aux / tss_aux\n                if r_squared_aux >= 1.0: # Perfect collinearity\n                    vif_x2 = np.inf\n                else:    \n                    vif_x2 = 1 / (1 - r_squared_aux)\n        except np.linalg.LinAlgError:\n            vif_x2 = np.inf\n\n        # Step 5: Determine selection based on BIC\n        selection_indicator = 1 if bic_r  bic_f else 0\n\n        # Step 6: Compute absolute difference in maximized log-likelihood\n        log_likelihood_diff = np.abs(logL_f - logL_r)\n\n        # Step 7: Store results for this case\n        results.append([\n            selection_indicator,\n            round(vif_x2, 4),\n            round(log_likelihood_diff, 6)\n        ])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102696"}]}