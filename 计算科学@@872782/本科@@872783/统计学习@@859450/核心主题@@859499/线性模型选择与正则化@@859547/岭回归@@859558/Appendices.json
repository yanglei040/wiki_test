{"hands_on_practices": [{"introduction": "理解岭回归的最佳方式是从其最基本的形式入手。这个练习 [@problem_id:1951876] 将引导您从岭回归的核心——惩罚损失函数出发，针对一个简单的单变量模型进行推导。通过亲手推导系数的解析解并将其应用于一个微型数据集，您将直观地理解模型是如何在拟合数据（最小化误差）与约束模型复杂度（惩罚系数大小）之间取得平衡的。", "problem": "在机器学习的背景下，我们的任务是对于一组包含 $n$ 个数据点 $(x_i, y_i)$ 的集合，拟合一个不含截距项的简单线性模型 $y = \\beta x$。为防止在小数据集上发生过拟合，我们采用岭回归。系数 $\\beta$ 的岭估计值是通过最小化惩罚平方和误差（也称为目标函数 $L(\\beta)$）得到的值：\n$$L(\\beta) = \\sum_{i=1}^{n} (y_i - \\beta x_i)^2 + \\lambda \\beta^2$$\n其中 $\\lambda > 0$ 是控制收缩量的正则化参数。\n\n您的任务有两部分。首先，通过最小化目标函数 $L(\\beta)$，推导出岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 关于数据点 $(x_i, y_i)$ 和参数 $\\lambda$ 的通用闭式表达式。\n\n其次，将此推导出的表达式应用于一个包含两个点的特定数据集：$(x_1, y_1) = (1, 3)$ 和 $(x_2, y_2) = (2, 5)$。使用正则化参数 $\\lambda = 1$ 计算岭估计 $\\hat{\\beta}_{\\text{ridge}}$ 的数值。\n\n以精确分数形式提供最终的数值。", "solution": "我们最小化无截距线性模型 $y=\\beta x$ 的惩罚平方和误差，其目标函数为\n$$\nL(\\beta)=\\sum_{i=1}^{n}(y_{i}-\\beta x_{i})^{2}+\\lambda \\beta^{2}, \\quad \\lambda>0.\n$$\n展开平方项并合并 $\\beta$ 的同类项：\n$$\nL(\\beta)=\\sum_{i=1}^{n}\\left(y_{i}^{2}-2\\beta x_{i}y_{i}+\\beta^{2}x_{i}^{2}\\right)+\\lambda \\beta^{2}\n= \\sum_{i=1}^{n}y_{i}^{2}-2\\beta \\sum_{i=1}^{n}x_{i}y_{i}+\\beta^{2}\\sum_{i=1}^{n}x_{i}^{2}+\\lambda \\beta^{2}.\n$$\n对 $\\beta$ 求导并将导数设为零（一阶最优性条件）：\n$$\n\\frac{dL}{d\\beta}=-2\\sum_{i=1}^{n}x_{i}y_{i}+2\\beta \\sum_{i=1}^{n}x_{i}^{2}+2\\lambda \\beta=0.\n$$\n解出 $\\beta$：\n$$\n2\\beta\\left(\\sum_{i=1}^{n}x_{i}^{2}+\\lambda\\right)=2\\sum_{i=1}^{n}x_{i}y_{i}\n\\quad \\Longrightarrow \\quad\n\\hat{\\beta}_{\\text{ridge}}=\\frac{\\sum_{i=1}^{n}x_{i}y_{i}}{\\sum_{i=1}^{n}x_{i}^{2}+\\lambda}.\n$$\n二阶导数为\n$$\n\\frac{d^{2}L}{d\\beta^{2}}=2\\sum_{i=1}^{n}x_{i}^{2}+2\\lambda>0,\n$$\n因此该解是唯一的最小值点。\n\n将此应用于 $(x_{1},y_{1})=(1,3)$，$(x_{2},y_{2})=(2,5)$ 且 $\\lambda=1$ 的情况：\n$$\n\\sum_{i=1}^{2}x_{i}y_{i}=1\\cdot 3+2\\cdot 5=13,\\qquad\n\\sum_{i=1}^{2}x_{i}^{2}=1^{2}+2^{2}=5.\n$$\n因此，\n$$\n\\hat{\\beta}_{\\text{ridge}}=\\frac{13}{5+1}=\\frac{13}{6}.\n$$", "answer": "$$\\boxed{\\frac{13}{6}}$$", "id": "1951876"}, {"introduction": "在掌握了单变量模型之后，下一步是将其推广到更普遍、更强大的多变量情景中。现实世界的数据分析任务通常涉及多个预测变量，因此使用矩阵表示法是至关重要的。这项练习 [@problem_id:1951893] 让您能够熟练运用岭回归的标准矩阵形式进行计算，这是将岭回归应用于复杂数据集的关键一步。", "problem": "在机器学习领域，岭回归是用于正则化线性回归模型的一种常用技术。这对于防止过拟合并处理预测变量之间的多重共线性特别有用。岭回归的系数向量估计量 $\\hat{\\beta}_{\\lambda}$ 由以下公式给出：\n$$ \\hat{\\beta}_{\\lambda} = (X^T X + \\lambda I)^{-1} X^T y $$\n此处，$X$ 是设计矩阵，$y$ 是观测结果向量，$I$ 是相应维度的单位矩阵，$\\lambda$ 是一个非负的正则化参数。\n\n假设对于一个包含两个预测变量的特定数据集，以下量已被预先计算：\n$$ X^T X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} \\quad \\text{and} \\quad X^T y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} $$\n使用正则化参数 $\\lambda = 5$，确定岭回归系数向量 $\\hat{\\beta}_5$。", "solution": "岭回归估计量定义为\n$$\n\\hat{\\beta}_{\\lambda} = (X^{T}X + \\lambda I)^{-1} X^{T} y.\n$$\n根据给定的数据，\n$$\nX^{T}X = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix}, \\quad X^{T} y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\lambda = 5.\n$$\n计算正则化矩阵：\n$$\nX^{T}X + \\lambda I = \\begin{pmatrix} 10  5 \\\\ 5  10 \\end{pmatrix} + 5 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 15  5 \\\\ 5  15 \\end{pmatrix}.\n$$\n对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆矩阵由以下公式给出：\n$$\n\\left(\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\\right)^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}.\n$$\n应用此公式，\n$$\n\\det(X^{T}X + \\lambda I) = 15 \\cdot 15 - 5 \\cdot 5 = 225 - 25 = 200,\n$$\n所以\n$$\n(X^{T}X + \\lambda I)^{-1} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix}.\n$$\n那么\n$$\n\\hat{\\beta}_{5} = \\frac{1}{200} \\begin{pmatrix} 15  -5 \\\\ -5  15 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 15 \\cdot 3 - 5 \\cdot 1 \\\\ -5 \\cdot 3 + 15 \\cdot 1 \\end{pmatrix} = \\frac{1}{200} \\begin{pmatrix} 40 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{5} \\\\ 0 \\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{5}\\\\0\\end{pmatrix}}$$", "id": "1951893"}, {"introduction": "我们已经学会了在给定正则化参数 $\\lambda$ 的情况下如何计算岭回归系数，但一个关键问题随之而来：如何选择一个最优的 $\\lambda$ 值？这个参数直接控制着模型的偏倚-方差权衡，其选择对模型性能至关重要。这项练习 [@problem_id:1951879] 概述了K折交叉验证的逻辑步骤，这是机器学习中用于选择像 $\\lambda$ 这样的超参数的黄金标准方法。", "problem": "一位数据科学家的任务是使用岭回归构建一个预测模型。岭回归是一种线性回归，它包含一个惩罚项来收缩系数估计，这对于缓解多重共线性和防止过拟合特别有用。这个惩罚的强度由一个非负的调整参数 $\\lambda$ 控制。建模过程中的一个关键步骤是从一组候选值中选择 $\\lambda$ 的最优值。K折交叉验证是实现这一目标的常用方法。\n\n该数据科学家确定了执行K折交叉验证以找到最优 $\\lambda$ 并构建最终模型的以下关键操作。预测误差使用均方误差（MSE）来衡量。\n\n(i) 从候选集中选择能在各折中产生最小平均MSE的 $\\lambda$ 值。\n(ii) 对于每个候选的 $\\lambda$ 值，通过遍历K个折来计算平均MSE，每次在K-1个折上训练模型，并在预留的那个折上进行验证。\n(iii) 将整个数据集随机划分为K个大小近似相等的子集，或称为“折”。\n(iv) 使用前一步中选择的最优 $\\lambda$ 值，在*整个*数据集上训练一个最终的岭回归模型。\n\n这些操作的正确逻辑顺序是什么？\n\nA. (iii) -> (i) -> (ii) -> (iv)\n\nB. (ii) -> (iii) -> (i) -> (iv)\n\nC. (iii) -> (ii) -> (i) -> (iv)\n\nD. (iii) -> (ii) -> (iv) -> (i)\n\nE. (ii) -> (i) -> (iv) -> (iii)", "solution": "岭回归通过最小化带惩罚的最小二乘目标来拟合系数 $\\beta$\n$$\n\\frac{1}{n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{2}^{2},\n$$\n其中 $\\lambda \\geq 0$ 是一个通过交叉验证选择的调整参数。为了选择 $\\lambda$，定义一个候选集 $\\Lambda$ 并按如下方式执行K折交叉验证。\n\n首先，将数据集索引 $\\{1,\\dots,n\\}$ 随机划分为 $K$ 个大小近似相等的不相交的折 $I_{1},\\dots,I_{K}$，这对应于操作(iii)。对于每个 $\\lambda \\in \\Lambda$，遍历折 $k=1,\\dots,K$：在由 $I_{-k} = \\{1,\\dots,n\\} \\setminus I_{k}$ 索引的训练集上使用 $\\lambda$ 拟合岭回归模型以获得 $\\hat{\\beta}^{(-k,\\lambda)}$，为 $i \\in I_{k}$ 计算验证预测值 $\\hat{y}_{i}^{(-k,\\lambda)}$，并计算该折的MSE\n$$\n\\mathrm{MSE}_{k}(\\lambda) = \\frac{1}{|I_{k}|} \\sum_{i \\in I_{k}} \\left(y_{i} - \\hat{y}_{i}^{(-k,\\lambda)}\\right)^{2}.\n$$\n然后计算 $\\lambda$ 的平均交叉验证误差，\n$$\n\\overline{\\mathrm{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^{K} \\mathrm{MSE}_{k}(\\lambda),\n$$\n这对应于操作(ii)。通过下式选择最优调整参数\n$$\n\\lambda^{*} = \\arg\\min_{\\lambda \\in \\Lambda} \\overline{\\mathrm{MSE}}(\\lambda),\n$$\n这对应于操作(i)。最后，使用 $\\lambda^{*}$ 在整个数据集上重新拟合岭回归模型，这对应于操作(iv)。\n\n因此，正确的逻辑顺序是 (iii) → (ii) → (i) → (iv)，即选项C。", "answer": "$$\\boxed{C}$$", "id": "1951879"}]}