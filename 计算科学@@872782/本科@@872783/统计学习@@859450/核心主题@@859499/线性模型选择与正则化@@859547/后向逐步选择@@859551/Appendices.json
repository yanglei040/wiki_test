{"hands_on_practices": [{"introduction": "理论知识为你打下了基础，现在是时候通过编码来亲身体验逐步选择的魅力与微妙之处了。第一个练习将引导你同时实现向后逐步选择和它的“孪生兄弟”——向前逐步选择。通过在精心设计的不同数据场景下运行这两种算法，你将直观地看到它们的贪婪搜索路径，并理解为何不同的起点（全模型 vs. 空模型）和路径有时会导致选出完全不同的预测变量子集 [@problem_id:3105032]。", "problem": "给定一个具有固定设计矩阵和响应向量的线性回归设定。基本基础包括高斯误差线性模型、普通最小二乘法 (OLS) 和贝叶斯信息准则 (BIC) 的定义。设数据包含排列成矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的预测变量和响应 $y \\in \\mathbb{R}^{n}$。线性模型假设 $y = \\beta_{0}\\mathbf{1} + X\\beta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2}I)$，$\\beta_{0} \\in \\mathbb{R}$ 是截距，$\\beta \\in \\mathbb{R}^{p}$ 是系数。普通最小二乘法 (OLS) 通过最小化残差平方和 (RSS) 来估计 $(\\beta_{0},\\beta)$。在高斯误差下，最大化对数似然产生一个定义为贝叶斯信息准则 (BIC) 的信息准则，对于预测变量的子集 $S \\subseteq \\{1,2,\\dots,p\\}$，并将截距计入参数中，其形式为\n$$\\mathrm{BIC}(S) = n \\cdot \\log\\left(\\frac{\\mathrm{RSS}(S)}{n}\\right) + k \\cdot \\log(n),$$\n其中 $k = |S| + 1$ 计算了截距和 $|S|$ 个选定的预测变量。残差平方和 $\\mathrm{RSS}(S)$ 是通过使用增广了一个全一列和由 $S$ 索引的列的设计矩阵进行 OLS 拟合来计算的。\n\n您的任务是实现两种由贝叶斯信息准则 (BIC) 驱动的逐步选择程序：\n- 从空模型开始的前向选择：从 $S=\\varnothing$ 开始，在每一步考虑添加一个变量 $j \\notin S$，该变量能最小化 $\\mathrm{BIC}(S \\cup \\{j\\})$，并且当且仅当这样做会严格减小当前的 BIC 时才添加它；当没有添加任何变量能严格减小当前的 BIC 时停止。\n- 从全模型开始的后向消除：从 $S=\\{1,2,\\dots,p\\}$ 开始，在每一步考虑移除一个变量 $j \\in S$，该变量能最小化 $\\mathrm{BIC}(S \\setminus \\{j\\})$，并且当且仅当这样做会严格减小当前的 BIC 时才移除它；当没有移除任何变量能严格减小当前的 BIC 时停止。\n\n在计算 $\\mathrm{BIC}(S)$ 时，始终使用包含截距的 OLS 拟合。决断平局必须是确定性的：如果多个候选项产生的 BIC 值在一个小容差范围内相等，则选择具有最小预测变量索引的候选项。所有索引都应作为基于 $1$ 的索引来处理和报告。\n\n在相同的数据集上实现这两种程序，以比较 BIC 选择的路径和最终子集。\n\n测试套件：\n在以下三个确定性测试用例上运行您的程序。对于每个用例，使用独立的标准正态变量和给定的种子精确生成 $X$ 和 $y$。\n\n- 用例 A（代理混淆导致不同的最终子集）：\n  - 参数：$n=80$, $p=3$, seed $=1$。\n  - 生成：\n    - 使用给定的种子，抽取长度为 $n$ 的向量 $x_{1}, x_{2}, z, e \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,1)$。\n    - 定义 $x_{3} = x_{1} + x_{2} + 0.1 z$。\n    - 定义 $y = x_{1} + x_{2} + 0.05 e$。\n    - 设置 $X = [x_{1}, x_{2}, x_{3}]$。\n  - 预期的定性行为：前向选择倾向于首先选择代理变量 $x_{3}$ 并停止，而后向消除倾向于丢弃代理变量并保留 $\\{x_{1},x_{2}\\}$。\n\n- 用例 B（单一强信号，预期结果一致）：\n  - 参数：$n=60$, $p=5$, seed $=2$。\n  - 生成：\n    - 使用给定的种子，为 $j \\in \\{1,2,3,4,5\\}$ 抽取独立的 $x_{j} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,1)$ 和 $e \\sim \\mathcal{N}(0,1)$。\n    - 定义 $y = 3.0 \\cdot x_{4} + 0.1 e$。\n    - 设置 $X = [x_{1}, x_{2}, x_{3}, x_{4}, x_{5}]$。\n  - 预期的定性行为：两种方法都应该选择 $\\{4\\}$。\n\n- 用例 C（中度相关的真实变量对，预期结果一致但路径不同）：\n  - 参数：$n=100$, $p=6$, seed $=3$。\n  - 生成：\n    - 使用给定的种子，抽取长度为 $n$ 的向量 $x_{1}, z_{1}, x_{3}, x_{4}, x_{5}, x_{6}, e \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,1)$。\n    - 定义 $x_{2} = x_{1} + 0.5 z_{1}$。\n    - 定义 $y = 1.5 x_{1} + 1.5 x_{2} + 0.2 e$。\n    - 设置 $X = [x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}]$。\n  - 预期的定性行为：两种方法都应该选择 $\\{1,2\\}$，但由于起点和贪婪决策的不同，路径（访问的子集序列）可能会有所不同。\n\n输出规范：\n对于每个测试用例，您的程序必须按顺序输出一个包含以下确切元素的列表：\n- 前向路径：一个子集列表，其中每个子集是一个升序排列的基于 $1$ 的索引列表；包括初始的空子集和每次接受添加后直到终止的每个子集。\n- 后向路径：一个子集列表，其中每个子集是一个升序排列的基于 $1$ 的索引列表；包括初始的全子集和每次接受移除后直到终止的每个子集。\n- 最终的前向子集：一个升序排列的基于 $1$ 的索引列表。\n- 最终的后向子集：一个升序排列的基于 $1$ 的索引列表。\n- 一个布尔值，指示最终的前向子集是否等于最终的后向子集。\n\n您的程序应生成一行输出，其中包含三个测试用例的结果，格式为用方括号括起来的逗号分隔列表（例如，$[r_{A}, r_{B}, r_{C}]$），其中每个 $r_{\\cdot}$ 是该用例的上述列表。此问题不涉及物理单位或角度单位，所有答案均为指定的纯数值或布尔值。", "solution": "该问题要求实现和比较两种逐步模型选择算法：前向选择和后向消除，应用于线性回归设定。模型选择的指导原则是贝叶斯信息准则 (BIC)，它在模型拟合度和复杂性之间进行权衡。\n\n### 1. 基本原理\n\n线性模型定义为 $y = \\beta_0\\mathbf{1} + X\\beta + \\varepsilon$，其中误差 $\\varepsilon$ 假设为独立且服从正态分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2I)$。目标是从 $X$ 的列中选择一个最优的预测变量子集。\n\n**贝叶斯信息准则 (BIC)**：\n对于由预测变量子集 $S$ 定义的模型，BIC 由下式给出：\n$$ \\mathrm{BIC}(S) = n \\cdot \\log\\left(\\frac{\\mathrm{RSS}(S)}{n}\\right) + k \\cdot \\log(n) $$\n其中：\n- $n$ 是观测值的数量。\n- $\\mathrm{RSS}(S)$ 是包含预测变量 $S$ 的模型的残差平方和。\n- $k = |S| + 1$ 是模型中的参数数量，包括截距。\n\n较低的 BIC 值表示更好的模型。第一项 $n \\cdot \\log(\\mathrm{RSS}(S)/n)$ 衡量拟合优度（lack of fit），而第二项 $k \\cdot \\log(n)$ 是对模型复杂度的惩罚，随参数数量的增加而增加。\n\n**普通最小二乘法 (OLS)**：\n$\\mathrm{RSS}(S)$ 是通过使用 OLS 拟合指定模型来计算的。对于一个设计矩阵 $X_S$（包括一个截距和对应于 $S$ 的 $X$ 的列），OLS 系数估计 $\\hat{\\beta}_S$ 最小化了观测响应与预测响应之间的平方差之和。$\\hat{\\beta}_S$ 由正规方程 $(X_S^T X_S)\\hat{\\beta}_S = X_S^T y$ 的解给出。\n\n### 2. 算法设计\n\n解决方案的一个核心组成部分是一个函数，用于计算任意预测变量子集的 BIC。\n\n**BIC 计算函数**：\n此函数接受一组预测变量索引 $S$、完整的设计矩阵 $X$ 和响应向量 $y$。\n1.  它通过将一列全一向量（用于截距）与由 $S$ 索引的 $X$ 的列拼接，来构建模型的设计矩阵 $X_S$。\n2.  它使用 `numpy.linalg.lstsq(X_S, y)` 来解决 OLS 问题。该函数是稳健的，并提供 OLS 系数以及至关重要的 RSS。\n3.  如果矩阵 $X_S$ 是秩亏的（由于多重共线性），会出现一种特殊情况。在这种情况下，`numpy.linalg.lstsq` 会为残差返回一个空数组。此时必须使用计算出的系数手动计算 RSS：$\\mathrm{RSS}(S) = \\sum_{i=1}^{n} (y_i - (X_S\\hat{\\beta}_S)_i)^2$。\n4.  知道了 RSS、$n$ 和 $k = |S| + 1$ 后，使用其定义计算 BIC。\n\n**前向选择算法**：\n这是一种贪婪的、自下而上的搜索算法。\n1.  **初始化**：从空模型（仅含截距）开始，$S_0 = \\varnothing$。计算其 BIC，即 $\\mathrm{BIC}(S_0)$。在解路径中记录初始的空集。\n2.  **迭代**：在每一步，考虑添加当前模型中没有的每个预测变量。对于当前模型 $S_{curr}$，找出能使模型 $S_{cand} = S_{curr} \\cup \\{j\\}$ 具有最小可能 BIC 的预测变量 $j \\notin S_{curr}$。\n3.  **决策**：如果 $\\mathrm{BIC}(S_{cand})$ 严格小于 $\\mathrm{BIC}(S_{curr})$，则接受该变更。更新当前模型 $S_{curr} \\leftarrow S_{cand}$，更新当前 BIC，并将新子集附加到路径中。\n4.  **终止**：如果没有单个预测变量的添加能够严格减小 BIC，则算法终止。\n决断平局的规则（选择最小的预测变量索引）通过按索引升序检查候选预测变量来确定性地处理。\n\n**后向消除算法**：\n这是一种贪婪的、自上而下的搜索算法。\n1.  **初始化**：从包含所有 $p$ 个预测变量的全模型开始，$S_0 = \\{1, 2, \\dots, p\\}$。计算其 BIC，即 $\\mathrm{BIC}(S_0)$。在解路径中记录这个全集。\n2.  **迭代**：在每一步，考虑移除当前模型中的每个预测变量。对于当前模型 $S_{curr}$，找出其移除能使模型 $S_{cand} = S_{curr} \\setminus \\{j\\}$ 具有最小可能 BIC 的预测变量 $j \\in S_{curr}$。\n3.  **决策**：如果 $\\mathrm{BIC}(S_{cand})$ 严格小于 $\\mathrm{BIC}(S_{curr})$，则接受该变更。更新 $S_{curr} \\leftarrow S_{cand}$，更新 BIC，并将新的、更小的子集附加到路径中。\n4.  **终止**：如果没有单个预测变量的移除能够严格减小 BIC，则算法终止。\n决断平局的处理方式与前向选择相同。\n\n### 3. 执行与输出\n\n解决方案首先实现用于 BIC 计算和两种搜索算法的辅助函数。对于每个指定的测试用例：\n1.  使用规定的参数和随机种子通过 `numpy.random.default_rng` 生成数据（$X$ 和 $y$），以确保可复现性。\n2.  在该数据集上执行前向和后向选择过程，得出它们各自的路径和最终选择的子集。\n3.  将结果——前向路径、后向路径、最终前向子集、最终后向子集，以及一个指示最终子集是否相同的布尔值——编译成该测试用例的列表。\n4.  最后，将所有测试用例的结果按照输出规范格式化为单个紧凑的字符串，并打印出来。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares forward and backward stepwise selection using BIC.\n    \"\"\"\n\n    def calculate_bic(S_indices, X, y):\n        \"\"\"\n        Calculates BIC for a model with predictors indexed by S_indices.\n        S_indices: a set of 0-based predictor indices.\n        \"\"\"\n        n = X.shape[0]\n        k = len(S_indices) + 1\n        \n        intercept = np.ones((n, 1))\n        if not S_indices:\n            X_S = intercept\n        else:\n            sorted_indices = sorted(list(S_indices))\n            X_S = np.hstack([intercept, X[:, sorted_indices]])\n\n        coeffs, residuals, rank, s = np.linalg.lstsq(X_S, y, rcond=None)\n        \n        # In case of rank deficiency, `residuals` is empty. We must calculate RSS manually.\n        if residuals.size == 0:\n            y_pred = X_S @ coeffs\n            rss = np.sum((y - y_pred)**2)\n        else:\n            rss = residuals[0]\n\n        # Handle perfect fit case where rss might be effectively zero\n        if rss  1e-12:\n            return -np.inf\n            \n        bic = n * np.log(rss / n) + k * np.log(n)\n        return bic\n\n    def forward_selection(X, y):\n        \"\"\"\n        Performs forward stepwise selection using BIC.\n        \"\"\"\n        n, p = X.shape\n        current_S = set()\n        path = [[]]\n        current_bic = calculate_bic(current_S, X, y)\n        \n        while True:\n            best_bic_in_step = current_bic\n            best_candidate_to_add = -1\n            \n            # Candidates are predictors not currently in the model, sorted for tie-breaking\n            candidates = sorted(list(set(range(p)) - current_S))\n            \n            for j in candidates:\n                test_S = current_S | {j}\n                test_bic = calculate_bic(test_S, X, y)\n                \n                if test_bic  best_bic_in_step:\n                    best_bic_in_step = test_bic\n                    best_candidate_to_add = j\n            \n            if best_candidate_to_add != -1:\n                current_S.add(best_candidate_to_add)\n                current_bic = best_bic_in_step\n                path.append(sorted([idx + 1 for idx in current_S]))\n            else:\n                break\n                \n        final_subset = sorted([idx + 1 for idx in current_S])\n        return path, final_subset\n\n    def backward_elimination(X, y):\n        \"\"\"\n        Performs backward stepwise elimination using BIC.\n        \"\"\"\n        n, p = X.shape\n        current_S = set(range(p))\n        path = [list(range(1, p + 1))] # Handles p=0 case correctly\n\n        if p == 0:\n            return path, []\n\n        current_bic = calculate_bic(current_S, X, y)\n        \n        while True:\n            if not current_S:\n                break\n                \n            best_bic_in_step = current_bic\n            best_candidate_to_remove = -1\n            \n            # Candidates for removal, sorted for tie-breaking\n            candidates = sorted(list(current_S))\n            \n            for j in candidates:\n                test_S = current_S - {j}\n                test_bic = calculate_bic(test_S, X, y)\n                \n                if test_bic  best_bic_in_step:\n                    best_bic_in_step = test_bic\n                    best_candidate_to_remove = j\n                    \n            if best_candidate_to_remove != -1:\n                current_S.remove(best_candidate_to_remove)\n                current_bic = best_bic_in_step\n                path.append(sorted([idx + 1 for idx in current_S]))\n            else:\n                break\n                \n        final_subset = sorted([idx + 1 for idx in current_S])\n        return path, final_subset\n\n    def generate_data(case_id):\n        \"\"\"\n        Generates data for a given test case ID.\n        \"\"\"\n        if case_id == 'A':\n            n, p, seed = 80, 3, 1\n            rng = np.random.default_rng(seed)\n            x1, x2, z, e = [rng.standard_normal(n) for _ in range(4)]\n            x3 = x1 + x2 + 0.1 * z\n            y = x1 + x2 + 0.05 * e\n            X = np.vstack([x1, x2, x3]).T\n        elif case_id == 'B':\n            n, p, seed = 60, 5, 2\n            rng = np.random.default_rng(seed)\n            X = rng.standard_normal((n, p))\n            e = rng.standard_normal(n)\n            y = 3.0 * X[:, 3] + 0.1 * e # x4 is at 0-based index 3\n        else: # Case 'C'\n            n, p, seed = 100, 6, 3\n            rng = np.random.default_rng(seed)\n            x1, z1, x3, x4, x5, x6, e = [rng.standard_normal(n) for _ in range(7)]\n            x2 = x1 + 0.5 * z1\n            y = 1.5 * x1 + 1.5 * x2 + 0.2 * e\n            X = np.vstack([x1, x2, x3, x4, x5, x6]).T\n        return X, y\n\n    def to_compact_str(obj):\n        \"\"\"\n        Recursively converts a Python object to a compact string representation without spaces.\n        \"\"\"\n        if isinstance(obj, list):\n            return f\"[{','.join(to_compact_str(item) for item in obj)}]\"\n        elif isinstance(obj, bool):\n            return 'True' if obj else 'False'\n        else:\n            return repr(obj)\n\n    test_cases_ids = ['A', 'B', 'C']\n    all_results = []\n    \n    for case_id in test_cases_ids:\n        X, y = generate_data(case_id)\n        \n        fwd_path, fwd_final = forward_selection(X, y)\n        bwd_path, bwd_final = backward_elimination(X, y)\n        \n        are_equal = (fwd_final == bwd_final)\n        \n        case_result = [fwd_path, bwd_path, fwd_final, bwd_final, are_equal]\n        all_results.append(case_result)\n        \n    final_output_str = f\"[{','.join(to_compact_str(res) for res in all_results)}]\"\n    print(final_output_str)\n\nsolve()\n```", "id": "3105032"}, {"introduction": "自动化模型选择工具功能强大，但如果盲目使用，可能会掉入“数据泄漏”的陷阱，构建出看似完美却毫无泛化能力的模型。这个练习将模拟一个典型的数据泄漏场景，让你亲眼见证朴素的向后选择是如何被一个“作弊”的预测变量所欺骗的。更重要的是，你将亲手实现一个泄漏检测程序，学会在模型选择之前主动识别并清除这些问题变量，从而建立起更加稳健的建模流程 [@problem_id:3101321]。", "problem": "您必须编写一个完整的程序，该程序能够构建合成回归数据集，并演示朴素的后向逐步选择如何可能因数据泄露而保留一个仅因此才具有预测性的变量，而一个能够感知泄露的预处理步骤可以在选择前检测并移除此类变量。该程序必须实现泄露检测和后向逐步选择，并且必须为指定的测试套件输出结果。所有计算必须以纯数学和逻辑术语表示，无需外部输入。\n\n请从以下基础开始：\n- 普通最小二乘（OLS）线性回归假设响应向量 $y \\in \\mathbb{R}^n$ 是由预测变量 $X \\in \\mathbb{R}^{n \\times p}$ 和一个截距通过 $y = \\beta_0 \\mathbf{1} + X \\beta + \\varepsilon$ 生成的，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- OLS 估计量最小化残差平方和（RSS），其中 $\\mathrm{RSS} = \\lVert y - \\hat{y} \\rVert_2^2$。\n- 在高斯噪声模型下，最大化对数似然可以用 $\\mathrm{RSS}$ 表示，对于一个有 $k$ 个参数（包括截距）和 $n$ 个观测值的模型，其贝叶斯信息准则（BIC）为 $ \\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + k \\log(n)$。较低的 $\\mathrm{BIC}$ 值表示模型更优。\n- 后向逐步选择从一组完整的候选特征开始，每次迭代移除单个特征，选择能最大程度降低所选信息准则的移除方案，并且当没有单个特征的移除能改善该准则时停止。\n- 两个向量 $a, b \\in \\mathbb{R}^n$ 之间的样本相关性是皮尔逊相关性，即协方差除以标准差的乘积。\n\n您的程序必须：\n1) 实现由 BIC 驱动的后向逐步选择：\n   - 给定 $X \\in \\mathbb{R}^{n \\times p}$ 和 $y \\in \\mathbb{R}^n$，始终包含截距（不计入可移除的 $p$ 个候选特征），从所有 $p$ 个特征开始，在每一步评估移除每个剩余的单个特征。当且仅当移除某个特征后得到的 BIC 严格小于当前 BIC 时，才移除该特征，并选择使 BIC 最小化的移除方案。重复此过程，直到没有移除操作可以降低 BIC 为止。如果所有特征都被移除，模型将简化为仅含截距的模型。\n\n2) 实现一个泄露检测程序，该程序仅使用 $X$、$y$ 和观测索引向量 $t = [0, 1, \\dots, n-1]^\\top$ 来标记可疑变量：\n   - 规则 A（直接标签复制检测）：如果单个特征 $x_j$ 与 $y$ 的绝对相关性大于一个高阈值 $\\tau_{\\text{direct}} = 0.995$，则将 $x_j$ 标记为泄露。\n   - 规则 B（时间代理检测）：如果单个特征 $x_j$ 与时间索引向量 $t$ 的绝对相关性大于 $\\tau_{\\text{time}} = 0.98$ 并且与 $y$ 的绝对相关性大于 $\\tau_{\\text{time},y} = 0.7$，则将 $x_j$ 标记为可能基于时间的泄露代理。\n   - 所有阈值 $\\tau_{\\text{direct}}$、$\\tau_{\\text{time}}$ 和 $\\tau_{\\text{time},y}$ 都是固定的，必须严格按照规定使用。\n\n3) 对每个数据集，运行两次后向选择：\n   - 朴素选择：对所有特征进行，不移除泄露。\n   - 感知泄露的选择：在移除泄露检测程序标记的所有特征后，对剩余特征应用后向选择。\n\n4) 对每个数据集，返回一个包含以下内容的三元组：\n   - 朴素选择后选定的特征索引列表。\n   - 感知泄露的选择后选定的特征索引列表。\n   - 一个布尔值，指示指定的候选泄露特征（始终是 $X$ 的最后一列）是否被泄露检测程序标记。\n\n两个索引列表都必须引用 $X$ 的原始列索引（从 $0$ 到 $p-1$），并且必须按升序排序。\n\n数据集生成细节（测试套件）：\n- 对于所有数据集，通过水平拼接三个块来构建 $X$：$X = [X_{\\text{true}} \\;|\\; X_{\\text{noise}} \\;|\\; x_{\\text{leak}}]$，其中 $X_{\\text{true}} \\in \\mathbb{R}^{n \\times q}$ 是具有非零系数的真实预测变量，$X_{\\text{noise}} \\in \\mathbb{R}^{n \\times r}$ 是纯噪声预测变量，$x_{\\text{leak}} \\in \\mathbb{R}^n$ 是指定的候选泄露特征。真实响应生成方式为 $y = X_{\\text{true}} \\beta + \\varepsilon$，或在指定的时间趋势情况下为 $y = X_{\\text{true}} \\beta + \\gamma \\tilde{t} + \\varepsilon$，其中 $\\tilde{t}$ 是均值为零、单位方差的标准化时间索引向量。噪声 $\\varepsilon$ 是标准差为 $\\sigma$ 的独立高斯噪声。在所有回归拟合中使用截距。\n\n提供以下四个具有固定参数和随机种子以保证可复现性的数据集：\n- 情况 1（直接泄露的理想路径）：\n  - $n = 120$，$q = 2$，$r = 3$，$\\beta = [3.0, -2.0]$，$\\sigma = 1.5$。\n  - 随机种子 $42$。\n  - 泄露类型：直接标签复制；通过 $x_{\\text{leak}} = y + \\eta$ 构建，其中 $\\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{leak}}^2)$ 且 $\\sigma_{\\text{leak}} = 0.02$。\n- 情况 2（无泄露，独立候选特征）：\n  - $n = 100$，$q = 3$，$r = 2$，$\\beta = [1.2, -0.8, 1.5]$，$\\sigma = 2.0$。\n  - 随机种子 $7$。\n  - 泄露类型：随机；构建与其他所有变量独立的 $x_{\\text{leak}} \\sim \\mathcal{N}(0, 1)$。\n- 情况 3（小样本，多噪声特征）：\n  - $n = 25$，$q = 2$，$r = 6$，$\\beta = [1.0, 0.5]$，$\\sigma = 1.0$。\n  - 随机种子 $123$。\n  - 泄露类型：随机；构建与其他所有变量独立的 $x_{\\text{leak}} \\sim \\mathcal{N}(0, 1)$。\n- 情况 4（通过趋势实现的时间代理泄露）：\n  - $n = 150$，$q = 1$，$r = 2$，$\\beta = [1.0]$，$\\sigma = 0.5$。\n  - 随机种子 $99$。\n  - 令 $\\tilde{t}$ 为标准化时间索引向量。响应 $y = X_{\\text{true}} \\beta + \\gamma \\tilde{t} + \\varepsilon$，其中 $\\gamma = 2.0$。\n  - 泄露类型：时间代理；设置 $x_{\\text{leak}} = \\tilde{t}$。\n\n对于所有情况，$X_{\\text{true}}$ 和 $X_{\\text{noise}}$ 的条目是独立同分布的标准正态随机变量，并且指定的泄露变量被放置为 $X$ 的最后一列，使其索引为 $p-1$，其中 $p = q + r + 1$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为一个无空格、由方括号括起的逗号分隔列表。每个数据集贡献一个形式为 $[\\text{list\\_before},\\text{list\\_after},\\text{flagged}]$ 的三元组，其中列表包含整数，flagged 值为布尔值。因此，总体输出是这四个数据集对应的四个三元组的列表，例如，$[[[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot]]$。\n\n此问题不涉及任何物理单位、角度或百分比。所有数值阈值和参数必须严格按照规定使用。程序必须是自包含的，使用指定的随机种子，并且不需要任何输入。", "solution": "所提出的问题是有效的。这是一个定义明确、自成体系且具有科学依据的统计学习练习，要求在不同数据条件下实现和比较特征选择程序。所有必要的参数、算法和条件都已明确提供，从而能够得出唯一且可验证的解决方案。\n\n任务是构建一个程序，该程序演示朴素的后向逐步特征选择算法在存在数据泄露时的不可靠性，以及一个旨在检测此类泄露的预处理步骤如何纠正此问题。这将通过实现必要的统计组件并将其应用于四个不同的合成数据集来实现。\n\n解决方案的结构如下：首先，我们定义用于模型选择的普通最小二乘（OLS）回归模型和贝叶斯信息准则（BIC）。其次，我们形式化后向逐步选择算法。第三，我们指定用于检测数据泄露的启发式方法。最后，我们概述生成数据集和运行比较分析的程序。\n\n**1. 线性回归与模型选择**\n\n我们假设标准线性模型，其中响应向量 $y \\in \\mathbb{R}^n$ 由预测变量的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和随机误差项 $\\varepsilon \\in \\mathbb{R}^n$ 生成。包含截距项的模型由下式给出：\n$$ y = \\beta_0 \\mathbf{1} + X \\beta + \\varepsilon $$\n其中 $\\beta_0 \\in \\mathbb{R}$ 是截距，$\\mathbf{1}$ 是一个 n 维全一向量，$\\beta \\in \\mathbb{R}^p$ 是特征系数向量，并且误差被假定为独立同分布，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n\n普通最小二乘（OLS）方法找到最小化残差平方和（RSS）的估计值 $\\hat{\\beta}_0$ 和 $\\hat{\\beta}$：\n$$ \\mathrm{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\lVert y - \\hat{y} \\rVert_2^2 $$\n其中 $\\hat{y} = \\hat{\\beta}_0 \\mathbf{1} + X \\hat{\\beta}$ 是拟合值。\n\n为了在一组具有不同预测变量子集的候选模型中进行模型选择，我们采用贝叶斯信息准则（BIC）。对于一个基于 $n$ 个观测值、具有 $k$ 个估计参数（包括截距）的模型，BIC 定义为：\n$$ \\mathrm{BIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + k \\log(n) $$\n项 $n \\log(\\mathrm{RSS}/n)$ 与模型的拟合优度（最大化对数似然）相关，而 $k \\log(n)$ 是模型复杂度的惩罚项。较低的 BIC 值表示一个在拟合度和复杂度之间取得更好平衡的更简约的模型。\n\n**2. 后向逐步选择算法**\n\n后向逐步选择是一种用于特征选择的贪心启发式算法。它从包含所有候选预测变量的模型开始，迭代地移除最不有用的预测变量，直到没有进一步的移除能够提高模型质量。由 BIC 驱动的算法如下：\n\n设 $\\mathcal{F}$ 为所有 $p$ 个候选特征索引的集合，$\\mathcal{F} = \\{0, 1, \\dots, p-1\\}$。\n1.  **初始化**：从完整模型开始，其中活动特征集为 $\\mathcal{S}_{\\text{current}} = \\mathcal{F}$。计算此模型的 BIC，即 $\\mathrm{BIC}_{\\text{current}}$。\n2.  **迭代**：\n    a. 对于 $\\mathcal{S}_{\\text{current}}$ 中的每个特征 $j$，考虑一个特征集为 $\\mathcal{S}_{\\text{trial}, j} = \\mathcal{S}_{\\text{current}} \\setminus \\{j\\}$ 的试验模型。计算其对应的 BIC，即 $\\mathrm{BIC}_{\\text{trial}, j}$。\n    b. 找到移除后导致 BIC 下降最大（即 BIC 值最小）的特征 $j^*$：$j^* = \\arg\\min_{j \\in \\mathcal{S}_{\\text{current}}} \\mathrm{BIC}_{\\text{trial}, j}$。令最佳结果的 BIC 为 $\\mathrm{BIC}_{\\text{best}} = \\mathrm{BIC}_{\\text{trial}, j^*}$。\n    c. **决策**：如果 $\\mathrm{BIC}_{\\text{best}}  \\mathrm{BIC}_{\\text{current}}$，则移除是有益的。更新活动集 $\\mathcal{S}_{\\text{current}} \\leftarrow \\mathcal{S}_{\\text{current}} \\setminus \\{j^*\\}$ 并更新 $\\mathrm{BIC}_{\\text{current}} \\leftarrow \\mathrm{BIC}_{\\text{best}}$。重复迭代。\n    d. 如果 $\\mathrm{BIC}_{\\text{best}} \\ge \\mathrm{BIC}_{\\text{current}}$，则没有单个特征的移除能改善模型。算法终止。\n3.  **输出**：最终选定的特征集为 $\\mathcal{S}_{\\text{current}}$。如果该集合为空，则最终模型为仅含截距的模型。\n\n**3. 数据泄露检测**\n\n当来自建模过程外部的信息被不当地包含在特征集中时，就会发生数据泄露，这通常会导致不切实际的高预测性能。我们实现一个预处理程序，基于两种简单而强大的启发式方法来标记和移除可疑特征。设 $x_j$ 为第 $j$ 个特征的向量，$y$ 为响应向量，$t = [0, 1, \\dots, n-1]^\\top$ 为观测时间索引向量。相关性是指皮尔逊相关系数。\n\n-   **规则 A（直接标签复制检测）**：一个几乎是目标变量完美复制品的特征是一种典型的数据泄露形式。如果它与响应的绝对相关性高于一个高阈值 $\\tau_{\\text{direct}}$，则被标记。\n    $$ |\\text{corr}(x_j, y)| > \\tau_{\\text{direct}} = 0.995 $$\n-   **规则 B（时间代理检测）**：在时间序列或有序数据中，一个特征可能不直接复制标签，但可能成为时间的代理，而时间本身可能具有驱动响应的强劲趋势。如果这样一个特征与时间索引和响应都高度相关，则被标记。\n    $$ |\\text{corr}(x_j, t)| > \\tau_{\\text{time}} = 0.98 \\quad \\text{and} \\quad |\\text{corr}(x_j, y)| > \\tau_{\\text{time},y} = 0.7 $$\n\n**4. 实验步骤**\n\n对于四个指定的数据集中的每一个，我们执行以下步骤：\n1.  **数据生成**：根据其规格合成数据集 $(X, y)$，包括真实预测变量的数量（$q$）、噪声预测变量的数量（$r$）、回归系数（$\\beta$）以及指定泄露特征 $x_{p-1}$ 的泄露类型。为保证可复现性，随机数生成器已设定种子。\n2.  **朴素选择**：将后向逐步选择算法应用于完整的特征矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y$。记录下所选特征索引的结果集。\n3.  **感知泄露的选择**：\n    a. 将泄露检测程序应用于 $(X, y)$ 以识别一组被标记的特征索引 $\\mathcal{F}_{\\text{flagged}}$。我们记录指定的泄露特征（索引 $p-1$）是否在该集合中。\n    b. 通过从 $X$ 中移除与 $\\mathcal{F}_{\\text{flagged}}$ 对应的列来创建清理后的特征矩阵 $X_{\\text{clean}}$。\n    c. 将后向逐步选择应用于 $(X_{\\text{clean}}, y)$。\n    d. 将从 $X_{\\text{clean}}$ 中选择的索引映射回它们在 $X$ 中的原始索引。记录这个最终集合。\n4.  **输出**：对于每个数据集，组装一个三元组，包含来自朴素选择的已排序索引列表、来自感知泄露的选择的已排序索引列表，以及一个指示指定泄露特征是否被标记的布尔值。\n\n此比较分析旨在突出以下场景：朴素方法因泄露变量具有强大（但虚假）的预测能力而错误地保留了它，而感知泄露的预处理步骤则正确地识别并移除了它，从而产生一个更稳健、更具泛化性的模型。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the entire simulation as specified.\n    \"\"\"\n\n    def calculate_bic(X_subset, y):\n        \"\"\"\n        Calculates the Bayesian Information Criterion (BIC) for a linear model.\n        \n        Args:\n            X_subset: Design matrix for the model. Can be None for intercept-only.\n            y: Response vector.\n            \n        Returns:\n            BIC value.\n        \"\"\"\n        n = len(y)\n        if X_subset is None or X_subset.shape[1] == 0:\n            k = 1  # Intercept only\n            rss = np.sum((y - np.mean(y))**2)\n        else:\n            num_features = X_subset.shape[1]\n            k = num_features + 1  # a coefficient for each feature + intercept\n            X_aug = np.c_[np.ones(n), X_subset]\n            try:\n                coeffs, _, _, _ = np.linalg.lstsq(X_aug, y, rcond=None)\n                rss = np.sum((y - (X_aug @ coeffs))**2)\n            except np.linalg.LinAlgError:\n                return np.inf  # Should not happen with this problem's data generation\n\n        # To prevent log(0) for perfect fits\n        if rss  1e-9:\n            rss = 1e-9\n            \n        bic = n * np.log(rss / n) + k * np.log(n)\n        return bic\n\n    def backward_selection(X, y):\n        \"\"\"\n        Performs backward stepwise selection using BIC.\n        \n        Args:\n            X: Full design matrix of candidate predictors.\n            y: Response vector.\n            \n        Returns:\n            A sorted list of indices of the selected features.\n        \"\"\"\n        p = X.shape[1]\n        current_indices = list(range(p))\n        if not current_indices:\n            return []\n        \n        current_bic = calculate_bic(X, y)\n\n        while len(current_indices) > 0:\n            bics_on_removal = []\n            for idx_to_remove in current_indices:\n                trial_indices = [i for i in current_indices if i != idx_to_remove]\n                if not trial_indices:\n                    X_trial = None\n                else:\n                    X_trial = X[:, trial_indices]\n                \n                bic = calculate_bic(X_trial, y)\n                bics_on_removal.append((bic, idx_to_remove))\n            \n            if not bics_on_removal:\n                break\n\n            best_bic, removed_idx = min(bics_on_removal)\n\n            if best_bic  current_bic:\n                current_bic = best_bic\n                current_indices.remove(removed_idx)\n            else:\n                break # No further improvement\n        \n        return sorted(current_indices)\n\n    def detect_leakage(X, y):\n        \"\"\"\n        Detects suspicious features based on correlation rules.\n        \n        Args:\n            X: Design matrix.\n            y: Response vector.\n            \n        Returns:\n            A set of indices of flagged features.\n        \"\"\"\n        n, p = X.shape\n        flagged_indices = set()\n        \n        t = np.arange(n)\n        \n        tau_direct = 0.995\n        tau_time = 0.98\n        tau_time_y = 0.7\n\n        for j in range(p):\n            x_j = X[:, j]\n            if np.std(x_j) == 0: continue\n            \n            # Rule A: Direct label-copy detection\n            corr_xy = np.corrcoef(x_j, y)[0, 1]\n            if abs(corr_xy) > tau_direct:\n                flagged_indices.add(j)\n\n            # Rule B: Time-proxy detection\n            if np.std(t) == 0: continue\n            corr_xt = np.corrcoef(x_j, t)[0, 1]\n            if abs(corr_xt) > tau_time and abs(corr_xy) > tau_time_y:\n                 flagged_indices.add(j)\n        \n        return flagged_indices\n\n    def generate_dataset(n, q, r, beta, sigma, seed, leak_type, gamma=None, sigma_leak=None):\n        \"\"\"\n        Generates a synthetic dataset based on specified parameters.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        X_true = rng.standard_normal((n, q))\n        X_noise = rng.standard_normal((n, r))\n        \n        epsilon = rng.normal(0, sigma, n)\n        \n        if leak_type == 'time_proxy':\n            t = np.arange(n)\n            t_tilde = (t - np.mean(t)) / np.std(t)\n            y = X_true @ beta + gamma * t_tilde + epsilon\n            x_leak = t_tilde.reshape(-1, 1)\n        else:\n            y = X_true @ beta + epsilon\n            if leak_type == 'direct_leakage':\n                eta = rng.normal(0, sigma_leak, n)\n                x_leak = (y + eta).reshape(-1, 1)\n            else: # 'random'\n                x_leak = rng.standard_normal((n, 1))\n        \n        X_parts = []\n        if q > 0: X_parts.append(X_true)\n        if r > 0: X_parts.append(X_noise)\n        X_parts.append(x_leak)\n\n        X = np.hstack(X_parts)\n        return X, y\n\n    test_cases = [\n        {'n': 120, 'q': 2, 'r': 3, 'beta': [3.0, -2.0], 'sigma': 1.5, 'seed': 42, 'leak_type': 'direct_leakage', 'sigma_leak': 0.02},\n        {'n': 100, 'q': 3, 'r': 2, 'beta': [1.2, -0.8, 1.5], 'sigma': 2.0, 'seed': 7, 'leak_type': 'random'},\n        {'n': 25, 'q': 2, 'r': 6, 'beta': [1.0, 0.5], 'sigma': 1.0, 'seed': 123, 'leak_type': 'random'},\n        {'n': 150, 'q': 1, 'r': 2, 'beta': [1.0], 'sigma': 0.5, 'seed': 99, 'leak_type': 'time_proxy', 'gamma': 2.0},\n    ]\n\n    results = []\n    for params in test_cases:\n        X, y = generate_dataset(**params)\n        p = X.shape[1]\n        leakage_feature_idx = p - 1\n\n        # 1. Naive selection\n        naive_selected = backward_selection(X, y)\n        \n        # 2. Leakage-aware selection\n        flagged_indices = detect_leakage(X, y)\n        leak_feature_is_flagged = leakage_feature_idx in flagged_indices\n\n        clean_indices_map = [i for i in range(p) if i not in flagged_indices]\n        if not clean_indices_map:\n            aware_selected = []\n        else:\n            X_clean = X[:, clean_indices_map]\n            selected_clean_indices = backward_selection(X_clean, y)\n            aware_selected = [clean_indices_map[i] for i in selected_clean_indices]\n\n        results.append((naive_selected, aware_selected, leak_feature_is_flagged))\n\n    # Format the final output string without spaces\n    result_strings = []\n    for naive_list, aware_list, flag in results:\n        naive_str = f\"[{','.join(map(str, naive_list))}]\"\n        aware_str = f\"[{','.join(map(str, aware_list))}]\"\n        flag_str = str(flag)\n        result_strings.append(f\"[{naive_str},{aware_str},{flag_str}]\")\n    \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3101321"}, {"introduction": "模型构建的艺术不仅在于选择变量，还在于如何表达它们。最后一个动手实践将带你深入一个更细微但同样关键的问题：特征工程如何影响向后逐步选择的结果。你将探索对于分类变量的不同编码方式（如基线编码和效应编码）如何改变变量间的共线性结构，并观察到这可能导致向后选择算法的路径和最终模型的微小甚至显著的变化，这对于理解模型选择的稳定性至关重要 [@problem_id:3101334]。", "problem": "您接获一项线性回归建模任务，其中包含连续和分类两种类型的预测变量。您的任务是，在对单个分类变量进行不同编码的情况下，实现向后逐步选择法。您的目标是通过可复现的计算来证明，向后逐步选择法对分类变量的编码方式敏感，尤其是在编码与其他预测变量之间存在共线性时。您必须编写一个完整、可运行的程序，该程序能够构建合成数据，使用赤池信息准则（Akaike Information Criterion, AIC）实现向后剔除法，并报告不同编码下最终 AIC 值的差异。\n\n请从以下基本模型开始：高斯线性模型假设响应向量 $\\mathbf{y} \\in \\mathbb{R}^{n}$ 的生成方式为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是一个固定的设计矩阵，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是一个未知参数向量，且 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{n})$。在高斯噪声下，$\\boldsymbol{\\beta}$ 的最大似然估计与普通最小二乘法一致。对于一个已拟合的模型，其残差平方和为 $\\mathrm{RSS}$，样本量为 $n$，有效参数数量为 $k$，则赤池信息准则（AIC）在不考虑与模型无关的加性常数的情况下，由下式给出：\n$$\n\\mathrm{AIC} = n \\log\\!\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k.\n$$\n当 $\\mathbf{X}$ 的列之间存在精确的线性相关（共线性）时，使用 $\\mathbf{X}$ 的矩阵秩作为有效参数数量 $k$。\n\n考虑一个具有 $K$ 个水平的单个分类预测变量，将通过对比（contrasts）的方式将其纳入设计矩阵。您将比较三种编码方式：\n- 基线编码（使用一个参照水平的独热编码）：选择一个参照水平，并为非参照水平包含 $K-1$ 个指示符列。\n- 使用不同参照水平的基线编码。\n- 效应编码（和为零的对比）：包含 $K-1$ 列；对于列 $j \\in \\{1,\\dots,K-1\\}$，将水平 $j$ 编码为 $+1$，最后一个水平编码为 $-1$，所有其他水平编码为 $0$。\n\n请按如下方式实现向后逐步选择法。从一个包含截距、两个连续预测变量以及分类预测变量的所有对比列的全模型开始。在每一步中，考虑移除任何一个非截距列。对于每个候选的移除操作，重新拟合模型并计算 AIC。如果候选模型中的最小 AIC 严格小于当前模型的 AIC，则移除相应的列并重复此过程；否则，停止。若出现平局，选择当前设计矩阵中列索引最小的候选者。始终保留截距。\n\n合成数据的构建必须如下所示。对于每个测试用例，固定 $K=4$ 个水平，各组大小相等，总样本量为 $n = 160$。设组标签为 $G \\in \\{0,1,2,3\\}$，每个水平恰好有 $n/4$ 个观测值。生成一个连续预测变量 $x \\sim \\mathcal{N}(0,1)$，在观测值之间独立。定义第二个连续预测变量 $z$，使其通过以下方式与分类结构和 $x$ 共线：\n$$\nz = \\alpha \\, h(G) + 0.5\\,x + \\eta_{z}, \\quad \\eta_{z} \\sim \\mathcal{N}(0, 0.3^{2}),\n$$\n其中 $h(0)=-1.0$，$h(1)=-0.2$，$h(2)=0.3$，$h(3)=0.9$。定义响应变量：\n$$\ny = \\beta_{0} + \\beta_{x}\\,x + \\beta_{z}\\,z + \\gamma(G) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, 1.0^{2}),\n$$\n其固定系数为 $\\beta_{0}=1.0$，$\\beta_{x}=2.0$，$\\beta_{z}=1.0$，组效应为 $\\gamma(0)=0.4$，$\\gamma(1)=-0.1$，$\\gamma(2)=0.5$，$\\gamma(3)=-0.8$。所有高斯噪声均使用独立抽样。为保证每个测试用例的可复现性，请使用固定的随机种子。\n\n测试套件。在以下三个参数对 $(\\text{seed}, \\alpha)$ 下运行上述实验：\n- 测试用例 1：$(11, 0.4)$，一个较低共线性设置。\n- 测试用例 2：$(29, 0.9)$，一个中等共线性设置。\n- 测试用例 3：$(47, 1.2)$，一个较高共线性设置。\n\n对于每个测试用例，拟合三个向后逐步选择过程，每种编码方式各一个：\n- 使用参照水平 $0$ 的基线编码。\n- 使用参照水平 $1$ 的基线编码。\n- 如上所述的效应编码。\n\n对于每个测试用例，计算在三种编码方式下由向后逐步选择所选出的模型的最终 AIC，然后报告该测试用例中这三个 AIC 值之间的最大绝对成对差异：\n$$\nd = \\max\\left\\{\\,\\left|A_{(0)} - A_{(1)}\\right|, \\left|A_{(0)} - A_{(\\mathrm{eff})}\\right|, \\left|A_{(1)} - A_{(\\mathrm{eff})}\\right|\\,\\right\\},\n$$\n其中 $A_{(0)}$ 是使用参照水平 $0$ 的基线编码下的最终 AIC，$A_{(1)}$ 是使用参照水平 $1$ 的基线编码下的最终 AIC，$A_{(\\mathrm{eff})}$ 是使用效应编码下的最终 AIC。\n\n最终输出格式。您的程序必须产生单行输出，其中包含一个由三个浮点数组成的列表，按上述顺序对应三个测试用例，每个值等于该测试用例的 $d$ 值，四舍五入到 $6$ 位小数，并以用方括号括起来的逗号分隔列表形式打印（例如，$[0.123456,0.000001,0.314159]$）。此问题不涉及物理单位。不使用角度。不要打印任何附加文本。", "solution": "用户提供的问题是一项有效的统计学计算练习。所有给定条件、约束和目标都清晰明确，构成了一个自洽且适定的问题。任务是为线性模型实现向后逐步选择，并证明其对分类预测变量编码方式的敏感性，尤其是在存在共线性的情况下。解决方案的步骤是：首先根据指定的模型生成合成数据，然后使用赤池信息准则（AIC）实现向后剔除算法，最后针对三种不同的编码方案运行此过程，以量化结果的差异。\n\n### 1. 合成数据生成\n\n对于每个测试用例，我们生成一个大小为 $n=160$ 的数据集。过程如下：\n\n1.  **随机种子**：使用指定的种子初始化随机数生成器以确保可复现性。\n2.  **分类预测变量 $G$**：创建一个具有 $K=4$ 个水平（索引为 $\\{0, 1, 2, 3\\}$）的分类变量 $G$。每个水平的观测数量相等，均为 $n/K = 40$。\n3.  **连续预测变量 $x$**：从标准正态分布中抽取一个独立的连续预测变量 $x$，$x_i \\sim \\mathcal{N}(0, 1)$，其中 $i=1, \\dots, n$。\n4.  **共线预测变量 $z$**：构建第二个连续预测变量 $z$，使其与 $x$ 和分类变量 $G$ 均共线。其值由以下方程确定：\n    $$\n    z = \\alpha \\, h(G) + 0.5\\,x + \\eta_{z}\n    $$\n    其中 $\\eta_{z} \\sim \\mathcal{N}(0, 0.3^2)$ 是一个噪声项，$\\alpha$ 是控制共线性程度的参数，$h(G)$ 是从分类水平到数值的映射：$h(0)=-1.0$，$h(1)=-0.2$，$h(2)=0.3$，$h(3)=0.9$。\n5.  **响应变量 $y$**：响应变量 $y$ 由一个线性模型生成，该模型包含截距、两个连续预测变量以及来自该分类变量的直接效应：\n    $$\n    y = \\beta_{0} + \\beta_{x}\\,x + \\beta_{z}\\,z + \\gamma(G) + \\varepsilon\n    $$\n    系数固定为 $\\beta_{0}=1.0$，$\\beta_{x}=2.0$，$\\beta_{z}=1.0$。分类效应为 $\\gamma(0)=0.4$，$\\gamma(1)=-0.1$，$\\gamma(2)=0.5$，$\\gamma(3)=-0.8$。误差项 $\\varepsilon$ 从标准正态分布中抽取，$\\varepsilon \\sim \\mathcal{N}(0, 1.0^2)$。\n\n### 2. 分类变量编码\n\n具有 $K=4$ 个水平的单个分类预测变量 $G$ 必须转换为设计矩阵 $\\mathbf{X}$ 的数值列。我们测试三种不同的编码方案，每种方案都产生 $K-1=3$ 列。\n\n1.  **基线编码（参照水平为 0）**：这是一种独热编码形式。水平 $0$ 是参照水平。创建三个二元列，分别对应水平 $1$、$2$ 和 $3$。对于水平 $j \\in \\{1, 2, 3\\}$ 中的观测值，第 $j$ 列为 $1$，其他列为 $0$。对于参照水平 $0$ 中的观测值，所有三列都为 $0$。\n\n2.  **基线编码（参照水平为 1）**：这与上述方法类似，但选择水平 $1$ 作为参照。为水平 $0$、$2$ 和 $3$ 创建三个二元列。\n\n3.  **效应编码（和为零的对比）**：这种编码也称为偏差编码。我们选择一个水平作为参照（此处为水平 $3$），并为非参照水平 $\\{0, 1, 2\\}$ 创建 $K-1=3$ 列。对于非参照水平 $j \\in \\{0, 1, 2\\}$ 中的观测值，相应列设置为 $1$，其他两列为 $0$。对于参照水平 $3$ 中的观测值，所有三列都设置为 $-1$。这确保了对比矩阵的各列之和为零。\n\n对于每种编码，通过组合一个全为 1 的列（用于截距）、预测变量 $x$ 和 $z$ 的列，以及代表分类变量的三列来构建完整的设计矩阵 $\\mathbf{X}$。这会产生一个初始的 $160 \\times 6$ 矩阵。\n\n### 3. 模型拟合与 AIC 计算\n\n线性模型 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 使用普通最小二乘法 (OLS) 进行拟合。$\\boldsymbol{\\beta}$ 的 OLS 估计为 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{+} \\mathbf{X}^T\\mathbf{y}$，其中 $(\\cdot)^{+}$ 表示摩尔-彭若斯伪逆，这是处理 $\\mathbf{X}$ 中潜在共线性（秩亏）所必需的。残差平方和为 $\\mathrm{RSS} = ||\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}||_2^2$。\n\n模型的质量使用赤池信息准则（AIC）进行评估，其公式为：\n$$\n\\mathrm{AIC} = n \\log\\left(\\frac{\\mathrm{RSS}}{n}\\right) + 2k\n$$\n这里，$n$ 是样本量，$k$ 是模型中的有效参数数量。根据规定，我们使用设计矩阵 $\\mathbf{X}$ 的秩作为 $k$。这可以通过数值线性代数方法（例如 `numpy.linalg.lstsq` 提供的方法）在计算中找到。\n\n### 4. 向后逐步选择算法\n\n分析的核心是一种向后剔除算法，该算法迭代地从模型中移除预测变量，以找到使 AIC 最小化的子集。\n\n1.  **初始化**：从全模型开始，该模型包含截距、$x$、$z$ 和 3 个分类预测变量列。计算其 AIC，作为 `current_aic`。\n2.  **迭代**：进入一个循环，只要模型仍有改进空间就继续。\n    a. **候选生成**：对于当前模型中的每个预测变量列（不包括截距），通过临时移除该列来创建一个候选模型。\n    b. **候选评估**：对于每个候选模型，使用 OLS 拟合它并计算其 AIC。\n    c. **选择**：识别具有最小 AIC 的候选模型。平局决胜规则指定选择对应于移除原始索引最小的列的模型。\n    d. **决策**：如果这个最小候选 AIC 严格小于 `current_aic`，则“接受”该移除操作。该预测变量将从模型中永久移除，`current_aic` 更新为这个新的最小值，循环进入下一次迭代。\n    e. **终止**：如果最小候选 AIC 不严格小于 `current_aic`，则说明没有任何移除操作能改善模型。算法终止，最终模型是上一步的模型。\n\n所选模型的最终 AIC 即为该过程的结果。\n\n### 5. 计算实验与最终度量\n\n对每个测试用例 $(\\text{seed}, \\alpha)$ 执行整个过程。对于一个给定的测试用例，向后选择过程会运行三次，每种分类编码方案各一次。这将产生三个最终 AIC 值：$A_{(0)}$（基线编码，参照=0）、$A_{(1)}$（基线编码，参照=1）和 $A_{(\\mathrm{eff})}$（效应编码）。\n\n每个测试用例要报告的最终度量是这三个 AIC 值之间的最大绝对成对差异：\n$$\nd = \\max\\left\\{\\,\\left|A_{(0)} - A_{(1)}\\right|, \\left|A_{(0)} - A_{(\\mathrm{eff})}\\right|, \\left|A_{(1)} - A_{(\\mathrm{eff})}\\right|\\,\\right\\}\n$$\n这个值 $d$ 量化了在给定的共线性水平 $\\alpha$ 下，最终模型的 AIC 对编码选择的敏感性。然后将三个测试用例的结果汇总并打印出来。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (11, 0.4),  # Test case 1: (seed, alpha)\n        (29, 0.9),  # Test case 2: (seed, alpha)\n        (47, 1.2),  # Test case 3: (seed, alpha)\n    ]\n\n    results = []\n    for seed, alpha in test_cases:\n        # 1. Generate synthetic data\n        y, G, x, z = _generate_data(seed, alpha)\n        \n        final_aics = {}\n        \n        # 2. Run backward selection for each encoding\n        encodings = [\"baseline_ref0\", \"baseline_ref1\", \"effect_coding\"]\n        for encoding_name in encodings:\n            X_full = _get_design_matrix(encoding_name, G, x, z)\n            final_aic = _backward_selection(X_full, y)\n            final_aics[encoding_name] = final_aic\n            \n        # 3. Compute the maximum absolute difference in final AICs\n        d = max(\n            abs(final_aics[\"baseline_ref0\"] - final_aics[\"baseline_ref1\"]),\n            abs(final_aics[\"baseline_ref0\"] - final_aics[\"effect_coding\"]),\n            abs(final_aics[\"baseline_ref1\"] - final_aics[\"effect_coding\"])\n        )\n        results.append(d)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _generate_data(seed, alpha):\n    \"\"\"Generates synthetic data based on the problem specification.\"\"\"\n    n = 160\n    K = 4\n    rng = np.random.default_rng(seed)\n\n    # Group labels G, 40 per level {0, 1, 2, 3}\n    G = np.repeat(np.arange(K), n // K)\n\n    # Continuous predictor x\n    x = rng.normal(loc=0.0, scale=1.0, size=n)\n\n    # Collinear predictor z\n    h_map = {0: -1.0, 1: -0.2, 2: 0.3, 3: 0.9}\n    h_G = np.array([h_map[g] for g in G])\n    eta_z = rng.normal(loc=0.0, scale=0.3, size=n)\n    z = alpha * h_G + 0.5 * x + eta_z\n\n    # Response variable y\n    beta_0, beta_x, beta_z = 1.0, 2.0, 1.0\n    gamma_map = {0: 0.4, 1: -0.1, 2: 0.5, 3: -0.8}\n    gamma_G = np.array([gamma_map[g] for g in G])\n    epsilon = rng.normal(loc=0.0, scale=1.0, size=n)\n    y = beta_0 + beta_x * x + beta_z * z + gamma_G + epsilon\n    \n    return y, G, x, z\n\ndef _get_design_matrix(encoding_name, G, x, z):\n    \"\"\"Constructs the full design matrix X for a given encoding.\"\"\"\n    n = len(G)\n    K = 4\n    n_cat_cols = K - 1\n    \n    cat_cols = np.zeros((n, n_cat_cols))\n\n    if encoding_name == \"baseline_ref0\":\n        # Reference level is 0. Create indicators for levels 1, 2, 3.\n        for i in range(1, K):\n            cat_cols[G == i, i - 1] = 1.0\n    elif encoding_name == \"baseline_ref1\":\n        # Reference level is 1. Create indicators for levels 0, 2, 3.\n        cat_cols[G == 0, 0] = 1.0\n        cat_cols[G == 2, 1] = 1.0\n        cat_cols[G == 3, 2] = 1.0\n    elif encoding_name == \"effect_coding\":\n        # Sum-to-zero contrasts, reference level is 3.\n        for i in range(K - 1): # Levels 0, 1, 2\n            cat_cols[G == i, i] = 1.0\n        cat_cols[G == (K - 1), :] = -1.0\n    \n    # Combine all columns: intercept, x, z, categorical columns\n    X_full = np.c_[np.ones(n), x, z, cat_cols]\n    return X_full\n\ndef _calculate_aic(X, y):\n    \"\"\"Calculates AIC for a given model.\"\"\"\n    n = len(y)\n    \n    # Fit model using OLS and get RSS and rank.\n    # rcond=None ensures use of machine precision for rank determination.\n    res = np.linalg.lstsq(X, y, rcond=None)\n    rss = res[1][0] if res[1].size > 0 else np.sum((y - X @ res[0])**2)\n    k = res[2] # Effective number of parameters is the rank\n\n    # Protect against log of zero or very small numbers\n    if rss  1e-12:\n        return -np.inf\n    \n    aic = n * np.log(rss / n) + 2 * k\n    return aic\n\ndef _backward_selection(X_full, y):\n    \"\"\"Performs backward stepwise selection using AIC.\"\"\"\n    num_total_cols = X_full.shape[1]\n    active_indices = list(range(num_total_cols))\n    \n    current_aic = _calculate_aic(X_full, y)\n\n    while True:\n        candidate_models = []\n        # Intercept at index 0 is always kept.\n        removable_indices = [i for i in active_indices if i > 0]\n        \n        if not removable_indices:\n            break\n\n        for idx_to_remove in removable_indices:\n            temp_indices = [i for i in active_indices if i != idx_to_remove]\n            X_temp = X_full[:, temp_indices]\n            aic_candidate = _calculate_aic(X_temp, y)\n            candidate_models.append((aic_candidate, idx_to_remove))\n        \n        # Sort by AIC (primary) and original column index (secondary, for tie-breaking)\n        candidate_models.sort()\n        \n        best_candidate_aic, best_idx_to_remove = candidate_models[0]\n        \n        if best_candidate_aic  current_aic:\n            current_aic = best_candidate_aic\n            active_indices.remove(best_idx_to_remove)\n        else:\n            # No improvement, stop selection\n            break\n            \n    return current_aic\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3101334"}]}