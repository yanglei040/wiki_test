{"hands_on_practices": [{"introduction": "为了对 LASSO 建立稳固的直觉，我们从一个理想化的场景开始：一个正交设计矩阵。这种简化剥离了诸如多重共线性之类的混淆因素，使我们能够以最纯粹的形式观察变量选择机制。这个练习将引导您在这种设定下从第一性原理推导出 LASSO 的解，揭示其等价于一个“软阈值”操作，理解这种联系是掌握正则化参数 $\\lambda$ 如何直接控制模型中包含哪些变量的基础。[@problem_id:3191263]", "problem": "考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。假设 $X$ 的列是标准正交的，即 $X^\\top X = I_p$。最小绝对值收敛和选择算子 (LASSO) 估计量 $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{p}$ 定义为以下优化问题的解\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\},\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\| \\cdot \\|_{1}$ 和 $\\| \\cdot \\|_{2}$ 分别表示 $\\ell_{1}$范数和 $\\ell_{2}$范数。\n\n仅从上述优化定义、标准正交条件 $X^\\top X = I_p$ 以及标准的一阶最优性条件（包括针对 $\\ell_1$范数的次梯度微积分）出发，推导 $\\hat{\\beta}(\\lambda)$ 关于 $z := X^\\top y$ 和 $\\lambda$ 的坐标级刻画。利用此刻画，陈述坐标 $\\hat{\\beta}_{j}(\\lambda)$ 为非零的充分必要条件，并用 $z_{j}$ 和 $\\lambda$ 直接表示。\n\n然后，考虑一个 $p = 6$ 的特定标准正交设计，其中 $z = X^\\top y$ 由下式给出\n$$\nz = \\begin{pmatrix} 2.7 & -1.3 & 0.9 & 3.4 & -0.2 & -2.1 \\end{pmatrix}.\n$$\n对于四个正则化值 $\\lambda \\in \\{0.5, 1.0, 2.0, 3.0\\}$，确定相应的选择集 $\\{ j : \\hat{\\beta}_{j}(\\lambda) \\neq 0 \\}$。\n\n最后，作为你的计算报告，请提供一个行向量，其元素是这些选择集按 $\\lambda = 0.5$, $\\lambda = 1.0$, $\\lambda = 2.0$, $\\lambda = 3.0$ 顺序排列的基数。无需四舍五入。", "solution": "该问题是有效的，因为它具有科学依据、定义明确且客观。这是统计学习理论中的一个标准问题，涉及在标准正交设计矩阵这一简化假设下 LASSO 估计量的性质。我现在开始解答。\n\nLASSO 估计量 $\\hat{\\beta}(\\lambda)$ 是以下优化问题的解：\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2} \\| y - X \\beta \\|_{2}^{2} + \\lambda \\| \\beta \\|_{1} \\right\\}\n$$\n目标函数，我们称之为 $L(\\beta)$，由一个可微的二次项和一个不可微但凸的 $\\ell_1$范数惩罚项组成。我们可以利用给定的标准正交条件 $X^\\top X = I_p$ 来简化二次项部分，其中 $I_p$ 是 $p \\times p$ 的单位矩阵。\n我们展开平方 $\\ell_2$范数：\n$$\n\\| y - X \\beta \\|_{2}^{2} = (y - X \\beta)^\\top (y - X \\beta) = y^\\top y - 2 y^\\top X \\beta + \\beta^\\top X^\\top X \\beta\n$$\n代入 $X^\\top X = I_p$，表达式变为：\n$$\n\\| y - X \\beta \\|_{2}^{2} = y^\\top y - 2 (X^\\top y)^\\top \\beta + \\beta^\\top I_p \\beta = \\|y\\|_2^2 - 2 z^\\top \\beta + \\|\\beta\\|_2^2\n$$\n其中我们使用了定义 $z := X^\\top y$。\n\n现在，目标函数 $L(\\beta)$ 可以写成：\n$$\nL(\\beta) = \\frac{1}{2} \\left( \\|y\\|_2^2 - 2 z^\\top \\beta + \\|\\beta\\|_2^2 \\right) + \\lambda \\| \\beta \\|_{1}\n$$\n为了找到最小值，我们可以忽略常数项 $\\frac{1}{2}\\|y\\|_2^2$，这使得优化问题等价于最小化：\n$$\n\\tilde{L}(\\beta) = \\frac{1}{2} \\|\\beta\\|_2^2 - z^\\top \\beta + \\lambda \\| \\beta \\|_{1}\n$$\n这个目标函数是坐标可分的。我们可以将其写成关于分量 $j=1, \\dots, p$ 的和：\n$$\n\\tilde{L}(\\beta) = \\sum_{j=1}^{p} \\left( \\frac{1}{2} \\beta_j^2 - z_j \\beta_j + \\lambda |\\beta_j| \\right)\n$$\n为了最小化 $\\tilde{L}(\\beta)$，我们可以对求和中的每一项分别关于其对应的 $\\beta_j$ 进行最小化。令 $f_j(\\beta_j) = \\frac{1}{2} \\beta_j^2 - z_j \\beta_j + \\lambda |\\beta_j|$。\n由于 $|\\beta_j|$ 在 $\\beta_j=0$ 处不可微，我们使用次梯度微积分。一阶最优性条件表明，$0$ 必须位于最小值 $\\hat{\\beta}_j$ 处 $f_j(\\beta_j)$ 的次梯度中。\n$f_j(\\beta_j)$ 的次梯度由下式给出：\n$$\n\\partial f_j(\\beta_j) = \\beta_j - z_j + \\lambda \\cdot \\partial(|\\beta_j|)\n$$\n绝对值函数 $|\\beta_j|$ 的次梯度是：\n$$\n\\partial(|\\beta_j|) = \\begin{cases} \\{1\\} & \\text{if } \\beta_j > 0 \\\\ \\{-1\\} & \\text{if } \\beta_j < 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\n最优性条件 $0 \\in \\partial f_j(\\hat{\\beta}_j)$ 转化为 $z_j - \\hat{\\beta}_j \\in \\lambda \\cdot \\partial(|\\hat{\\beta}_j|)$。我们针对最优值 $\\hat{\\beta}_j$ 分三种情况分析这个条件：\n\n情况1：$\\hat{\\beta}_j > 0$。\n次梯度 $\\partial(|\\hat{\\beta}_j|)$ 为 $\\{1\\}$。条件变为 $z_j - \\hat{\\beta}_j = \\lambda$，解得 $\\hat{\\beta}_j = z_j - \\lambda$。为使该解与假设 $\\hat{\\beta}_j > 0$ 一致，我们必须有 $z_j - \\lambda > 0$，即 $z_j > \\lambda$。\n\n情况2：$\\hat{\\beta}_j < 0$。\n次梯度 $\\partial(|\\hat{\\beta}_j|)$ 为 $\\{-1\\}$。条件变为 $z_j - \\hat{\\beta}_j = -\\lambda$，解得 $\\hat{\\beta}_j = z_j + \\lambda$。为使该解与假设 $\\hat{\\beta}_j < 0$ 一致，我们必须有 $z_j + \\lambda < 0$，即 $z_j < -\\lambda$。\n\n情况3：$\\hat{\\beta}_j = 0$。\n次梯度 $\\partial(|\\hat{\\beta}_j|)$ 为区间 $[-1, 1]$。条件变为 $z_j - 0 \\in \\lambda \\cdot [-1, 1]$，即 $z_j \\in [-\\lambda, \\lambda]$，或 $|z_j| \\le \\lambda$。\n\n综合这三种情况，得到 LASSO 解 $\\hat{\\beta}(\\lambda)$ 的坐标级刻画：\n$$\n\\hat{\\beta}_j(\\lambda) = \\begin{cases} z_j - \\lambda & \\text{if } z_j > \\lambda \\\\ 0 & \\text{if } |z_j| \\le \\lambda \\\\ z_j + \\lambda & \\text{if } z_j < -\\lambda \\end{cases}\n$$\n这就是软阈值算子，可以紧凑地写为 $\\hat{\\beta}_j(\\lambda) = \\text{sign}(z_j) (|z_j| - \\lambda)_+$，其中 $(x)_+ = \\max(x, 0)$。\n\n根据此刻画，我们可以陈述坐标 $\\hat{\\beta}_j(\\lambda)$ 为非零的充分必要条件。系数 $\\hat{\\beta}_j(\\lambda)$ 非零当且仅当我们处于情况1 ($z_j > \\lambda$) 或情况2 ($z_j < -\\lambda$)。这两个条件可以合并为一个单一条件：\n$$\n|z_j| > \\lambda\n$$\n这就是 $\\hat{\\beta}_j(\\lambda) \\neq 0$ 的充分必要条件。\n\n现在，我们将此应用于具体问题。向量 $z = X^\\top y$ 的分量为：\n$z_1 = 2.7$, $z_2 = -1.3$, $z_3 = 0.9$, $z_4 = 3.4$, $z_5 = -0.2$, $z_6 = -2.1$。\n我们计算这些分量的绝对值：\n$|z_1| = 2.7$, $|z_2| = 1.3$, $|z_3| = 0.9$, $|z_4| = 3.4$, $|z_5| = 0.2$, $|z_6| = 2.1$。\n\n我们需要为 $\\lambda \\in \\{0.5, 1.0, 2.0, 3.0\\}$ 确定选择集 $S_\\lambda = \\{ j : \\hat{\\beta}_{j}(\\lambda) \\neq 0 \\} = \\{ j : |z_j| > \\lambda \\}$。\n\n对于 $\\lambda = 0.5$：\n我们检查哪些 $|z_j| > 0.5$。\n$|z_1| = 2.7 > 0.5$\n$|z_2| = 1.3 > 0.5$\n$|z_3| = 0.9 > 0.5$\n$|z_4| = 3.4 > 0.5$\n$|z_5| = 0.2 \\ngtr 0.5$\n$|z_6| = 2.1 > 0.5$\n选择集为 $S_{0.5} = \\{1, 2, 3, 4, 6\\}$。其基数为 $|S_{0.5}| = 5$。\n\n对于 $\\lambda = 1.0$：\n我们检查哪些 $|z_j| > 1.0$。\n$|z_1| = 2.7 > 1.0$\n$|z_2| = 1.3 > 1.0$\n$|z_3| = 0.9 \\ngtr 1.0$\n$|z_4| = 3.4 > 1.0$\n$|z_5| = 0.2 \\ngtr 1.0$\n$|z_6| = 2.1 > 1.0$\n选择集为 $S_{1.0} = \\{1, 2, 4, 6\\}$。其基数为 $|S_{1.0}| = 4$。\n\n对于 $\\lambda = 2.0$：\n我们检查哪些 $|z_j| > 2.0$。\n$|z_1| = 2.7 > 2.0$\n$|z_2| = 1.3 \\ngtr 2.0$\n$|z_3| = 0.9 \\ngtr 2.0$\n$|z_4| = 3.4 > 2.0$\n$|z_5| = 0.2 \\ngtr 2.0$\n$|z_6| = 2.1 > 2.0$\n选择集为 $S_{2.0} = \\{1, 4, 6\\}$。其基数为 $|S_{2.0}| = 3$。\n\n对于 $\\lambda = 3.0$：\n我们检查哪些 $|z_j| > 3.0$。\n$|z_1| = 2.7 \\ngtr 3.0$\n$|z_2| = 1.3 \\ngtr 3.0$\n$|z_3| = 0.9 \\ngtr 3.0$\n$|z_4| = 3.4 > 3.0$\n$|z_5| = 0.2 \\ngtr 3.0$\n$|z_6| = 2.1 \\ngtr 3.0$\n选择集为 $S_{3.0} = \\{4\\}$。其基数为 $|S_{3.0}| = 1$。\n\n问题要求按给定的 $\\lambda$ 值顺序，提供这些基数的行向量。基数序列为 $5$, $4$, $3$, $1$。", "answer": "$$\n\\boxed{\\begin{pmatrix} 5 & 4 & 3 & 1 \\end{pmatrix}}\n$$", "id": "3191263"}, {"introduction": "从理论走向实践，我们必须解决 LASSO 在真实世界数据中的行为，其中预测变量通常具有不同的尺度。本练习使用计算方法来探究 LASSO 对特征缩放的敏感性。你会发现，由于 $\\ell_1$ 惩罚项统一地应用于所有系数，具有较大尺度（例如，较高方差）的预测变量可能会被不公平地偏爱。这个动手实践突显了为什么标准化特征是确保 LASSO 变量选择既公平又有效的一个关键预处理步骤。[@problem_id:3191255]", "problem": "考虑线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$。最小绝对收缩和选择算子 (lasso) 估计量 $\\hat{\\beta}$ 定义为以下目标函数的最小化器\n$$\n\\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n其中 $\\lambda \\ge 0$ 是一个正则化参数，$\\lVert \\cdot \\rVert_1$ 表示绝对值之和。Lasso 的“变量选择”属性指的是其解 $\\hat{\\beta}$ 中的某些元素可以恰好为零，从而选择了一个预测变量子集的现象。\n\n任务：研究预测变量的缩放对 lasso 变量选择的影响。具体来说，比较预测变量未标准化时与预测变量标准化（中心化至均值为零，并缩放至单位经验标准差）时，lasso 所选择的变量。\n\n从上述定义出发，实现一个通用算法，通过优化 lasso 目标函数来为给定的设计矩阵 $X$、响应向量 $y$ 和正则化参数 $\\lambda$ 计算 $\\hat{\\beta}$。使用坐标级迭代方法，不依赖任何闭式“捷径”解。\n\n对于以下所有测试用例，使用 $p=3$ 个预测变量和 $n=90$ 个样本。按如下方式构造 $X$：将 $n$ 个样本划分为三个大小相等（均为 30）的不相交的块。对于预测变量 $j \\in \\{1,2,3\\}$，通过从均值为 $0$、标准差为 $s_j$ 的正态分布中独立抽样来填充其在块 $j$ 中的值，并将其在块 $j$ 之外的值设为 $0$。这会产生三个相互正交的预测变量，它们具有由 $\\{s_j\\}$ 决定的不同经验方差。为了保证可复现性，使用一个固定种子为 $0$ 的伪随机数生成器。对于每个测试用例，精确设置 $\\varepsilon = 0$ 和 $y = X \\beta$。\n\n对于每个测试用例：\n- 使用原始的 $X$（未标准化）计算 $\\hat{\\beta}_{\\text{unstd}}$。\n- 使用按列标准化（均值为 $0$，单位经验标准差）的 $X$ 计算 $\\hat{\\beta}_{\\text{std}}$。\n- 将所选变量集定义为索引 $j \\in \\{1,2,3\\}$ 的列表，其中 $|\\hat{\\beta}_j| > \\tau$，容差 $\\tau = 10^{-8}$。\n- 报告两个所选变量集，使用基于 1 的索引（即，预测变量标记为 $1,2,3$）。\n- 同时报告一个布尔值，表示这两个集合是否不同。\n\n使用以下四个测试用例，每个用例均 $n=90$：\n1. 均衡方差（理想情况）：$s = [1.0, 1.0, 1.0]$，$\\beta = [0.4, 0.2, 0.06]$，$\\lambda = 0.05$。\n2. 大尺度噪声效应（未标准化情况可能选择一个小系数但大方差的预测变量）：$s = [0.5, 3.0, 1.0]$，$\\beta = [0.2, 0.05, 0.15]$，$\\lambda = 0.10$。\n3. 重要的小尺度预测变量（未标准化情况可能会丢弃一个尺度小但重要的预测变量，而标准化后会保留它）：$s = [0.2, 1.0, 1.0]$，$\\beta = [0.6, 0.0, 0.0]$，$\\lambda = 0.05$。\n4. 边界条件（非常大的正则化参数使所有系数都为零）：$s = [2.0, 0.5, 1.5]$，$\\beta = [1.0, 0.3, 0.7]$，$\\lambda = 10.0$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果必须是一个列表，包含：未标准化设计的所选变量列表、标准化设计的所选变量列表，以及表示这两个集合是否不同的布尔值。例如，一个有效的输出格式是\n$$\n\\text{[ [ [1,2], [1,2], False ], [ [2], [], True ], \\dots ]}\n$$\n其中内部列表包含整数，布尔值为 $\\text{True}$ 或 $\\text{False}$。", "solution": "我们从 lasso 目标函数开始\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\}.\n$$\n这是一个凸优化问题，一种计算 $\\hat{\\beta}$ 的原则性方法是使用由次梯度微积分引导的坐标级优化。核心观察是，平方误差项是可微的，而绝对值惩罚项在 $0$ 点不可微，但具有明确定义的次微分。卡鲁什-库恩-塔克 (Karush–Kuhn–Tucker, KKT) 条件提供了必要的最优性准则。对于坐标级更新，我们考虑在更新坐标 $j$ 时的部分残差：\n$$\nr_{(j)} = y - \\sum_{k \\ne j} X_k \\beta_k,\n$$\n其中 $X_k$ 是 $X$ 的第 $k$ 列。在保持其他坐标固定的情况下，对目标函数关于 $\\beta_j$ 进行优化，问题简化为最小化一个关于 $\\beta_j$ 的一维凸函数。第 $j$ 个坐标的次梯度条件是\n$$\n\\frac{1}{n} X_j^\\top \\left( y - \\sum_{k=1}^p X_k \\beta_k \\right) + \\lambda \\cdot g_j = 0,\n$$\n其中 $g_j \\in \\partial |\\beta_j|$，当 $\\beta_j \\ne 0$ 时 $g_j = \\operatorname{sign}(\\beta_j)$，当 $\\beta_j = 0$ 时 $g_j \\in [-1,1]$。使用部分残差 $r_{(j)}$ 重新整理得到\n$$\n\\frac{1}{n} X_j^\\top r_{(j)} - \\frac{1}{n} X_j^\\top X_j \\beta_j + \\lambda \\cdot g_j = 0.\n$$\n定义标量\n$$\np_j := \\frac{1}{n} X_j^\\top r_{(j)}, \\quad z_j := \\frac{1}{n} X_j^\\top X_j.\n$$\n坐标级最优性问题简化为求解\n$$\n\\min_{\\beta_j \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} z_j \\beta_j^2 - p_j \\beta_j + \\lambda |\\beta_j| \\right\\}.\n$$\n通过配方法和应用次梯度微积分，解由软阈值算子给出：\n$$\n\\beta_j \\leftarrow \\frac{1}{z_j} \\cdot \\operatorname{Soft}(p_j, \\lambda),\n$$\n其中 $\\operatorname{Soft}(a,t) = \\operatorname{sign}(a)\\cdot \\max\\{|a| - t, 0\\}$。此更新在 $j=1,\\dots,p$ 上反复迭代直至收敛（例如，直到最大坐标变化小于一个很小的容差）。该算法是循环坐标下降的一种形式，并保证对这个凸目标函数收敛。\n\n现在我们分析缩放的影响。假设我们将预测变量 $j$ 缩放一个因子 $\\alpha_j > 0$，即 $\\tilde{X}_j = \\alpha_j X_j$。那么\n$$\n\\tilde{p}_j = \\frac{1}{n} \\tilde{X}_j^\\top r_{(j)} = \\alpha_j \\cdot \\frac{1}{n} X_j^\\top r_{(j)} = \\alpha_j p_j,\n$$\n并且\n$$\n\\tilde{z}_j = \\frac{1}{n} \\tilde{X}_j^\\top \\tilde{X}_j = \\alpha_j^2 \\cdot \\frac{1}{n} X_j^\\top X_j = \\alpha_j^2 z_j.\n$$\n坐标更新变为\n$$\n\\tilde{\\beta}_j \\leftarrow \\frac{1}{\\alpha_j^2 z_j} \\cdot \\operatorname{Soft}(\\alpha_j p_j, \\lambda).\n$$\n变量 $j$ 是否被选择（即 $\\tilde{\\beta}_j = 0$ 与否）取决于 $|\\alpha_j p_j|$ 是否超过 $\\lambda$；显然，缩放改变了有效的阈值穿越，因为 $p_j$ 随 $\\alpha_j$ 线性缩放。当预测变量被标准化为单位经验标准差时，缩放因子被吸收到标准化的设计矩阵中，使得跨变量的 $p_j$ 比较更加均衡。\n\n在我们的构造中，预测变量在三个块上具有不相交的支持集，使它们相互正交，即对于 $j \\ne k$ 有 $X_j^\\top X_k = 0$。考虑无噪声情况，其中 $y = X \\beta$。那么\n$$\np_j = \\frac{1}{n} X_j^\\top r_{(j)} = \\frac{1}{n} X_j^\\top y = \\frac{1}{n} X_j^\\top X_j \\beta_j = z_j \\beta_j.\n$$\n因此，选择准则 $|\\operatorname{Soft}(p_j,\\lambda)| > 0$ 简化为 $|p_j| > \\lambda$，即\n$$\n|z_j \\beta_j| > \\lambda.\n$$\n对于未标准化的预测变量，$z_j$ 反映了它们的经验尺度；$z_j$ 较大的变量在相同的 $\\lambda$ 下更可能被选中，即使 $|\\beta_j|$ 很小。在标准化为单位经验方差后（使得 $z_j \\approx 1$），选择条件近似变为 $|\\beta_j| > \\lambda$，从而使所有预测变量的阈值均等化，并可能改变所选集合。\n\n算法设计：\n- 使用指定的块-正态构造和固定种子生成 $X$，以确保可复现性。\n- 构造 $y = X \\beta$ 并设 $\\varepsilon = 0$，以隔离缩放效应。\n- 使用从次梯度/卡鲁什-库恩-塔克分析推导出的软阈值更新，实现循环坐标下降。\n- 通过减去列均值并除以列经验标准差（并对零标准差进行保护性处理）来标准化 $X$。\n- 对 $X$ 和标准化后的 $X$ 运行 lasso 求解器，然后将 $|\\hat{\\beta}_j| > \\tau$（其中 $\\tau = 10^{-8}$）的变量确定为所选变量。\n- 报告两种选择的基于 1 的索引，以及一个表示集合是否不同的布尔值。\n\n测试套件中的覆盖范围：\n- 均衡方差：两种选择一致，因为所有 $j$ 的 $z_j$ 都相等。\n- 大尺度噪声效应：在未标准化的情况下，一个系数小但 $z_j$ 大的预测变量可能被选中，但在标准化后则不会。\n- 重要的小尺度预测变量：当 $z_j$ 相对于 $\\lambda$ 较小时，未标准化的情况可能会丢弃一个真正重要的预测变量，而标准化会保留它。\n- 边界条件：一个非常大的 $\\lambda$ 会在两种情况下都产生空模型。\n\n最终的程序遵循这些原则，并打印一行输出，其中包含每个测试用例的未标准化选择索引、标准化选择索引以及表示二者是否差异的布尔值。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(a, t):\n    if a > t:\n        return a - t\n    elif a < -t:\n        return a + t\n    else:\n        return 0.0\n\ndef lasso_coordinate_descent(X, y, lam, max_iter=10000, tol=1e-12):\n    \"\"\"\n    Solve the lasso problem:\n      (1/(2n)) ||y - X b||_2^2 + lam ||b||_1\n    using cyclic coordinate descent with soft-thresholding.\n    \"\"\"\n    n, p = X.shape\n    b = np.zeros(p, dtype=float)\n    # Residual r = y - X b; with b=0 initially, r=y\n    r = y.copy()\n    for _ in range(max_iter):\n        b_old = b.copy()\n        for j in range(p):\n            xj = X[:, j]\n            # Add back the old contribution of coordinate j to residual\n            r += xj * b[j]\n            # Compute p_j and z_j\n            p_j = np.dot(xj, r) / n\n            z_j = np.dot(xj, xj) / n\n            # Update via soft-threshold\n            b[j] = soft_threshold(p_j, lam) / (z_j if z_j > 0 else 1.0)\n            # Subtract the new contribution\n            r -= xj * b[j]\n        # Convergence check\n        if np.max(np.abs(b - b_old)) < tol:\n            break\n    return b\n\ndef standardize_columns(X):\n    \"\"\"\n    Center columns to mean zero and scale to unit empirical std (ddof=0).\n    Returns standardized X, along with means and stds.\n    \"\"\"\n    means = X.mean(axis=0)\n    Xc = X - means\n    stds = Xc.std(axis=0, ddof=0)\n    stds_safe = np.where(stds == 0, 1.0, stds)\n    Xstd = Xc / stds_safe\n    return Xstd, means, stds_safe\n\ndef make_block_X(n, s):\n    \"\"\"\n    Construct X with 3 predictors, each active on a disjoint block of length n//3,\n    with values ~ N(0, s_j) in its block and zeros elsewhere.\n    \"\"\"\n    assert n % 3 == 0\n    m = n // 3\n    rng = np.random.default_rng(0)\n    X = np.zeros((n, 3), dtype=float)\n    for j in range(3):\n        start = j * m\n        end = (j + 1) * m\n        X[start:end, j] = rng.normal(0.0, s[j], size=m)\n    return X\n\ndef selected_indices(b, tol=1e-8):\n    # 1-based indexing for output\n    return [i + 1 for i, bi in enumerate(b) if abs(bi) > tol]\n\ndef run_case(n, s, beta_true, lam):\n    X = make_block_X(n, s)\n    y = X @ np.array(beta_true, dtype=float)\n    # Unstandardized lasso\n    b_unstd = lasso_coordinate_descent(X, y, lam)\n    sel_unstd = selected_indices(b_unstd)\n    # Standardized lasso\n    X_std, _, _ = standardize_columns(X)\n    b_std = lasso_coordinate_descent(X_std, y, lam)\n    sel_std = selected_indices(b_std)\n    differ = (sel_unstd != sel_std)\n    return [sel_unstd, sel_std, differ]\n\ndef solve():\n    test_cases = [\n        # 1. Balanced variances\n        {\"n\": 90, \"s\": [1.0, 1.0, 1.0], \"beta\": [0.4, 0.2, 0.06], \"lam\": 0.05},\n        # 2. Large scale noise effect: unstandardized selects a large-scale small-coefficient predictor\n        {\"n\": 90, \"s\": [0.5, 3.0, 1.0], \"beta\": [0.2, 0.05, 0.15], \"lam\": 0.10},\n        # 3. Important small-scale predictor: unstandardized drops it, standardized retains it\n        {\"n\": 90, \"s\": [0.2, 1.0, 1.0], \"beta\": [0.6, 0.0, 0.0], \"lam\": 0.05},\n        # 4. Boundary: very large lambda\n        {\"n\": 90, \"s\": [2.0, 0.5, 1.5], \"beta\": [1.0, 0.3, 0.7], \"lam\": 10.0},\n    ]\n    results = []\n    for case in test_cases:\n        res = run_case(case[\"n\"], case[\"s\"], case[\"beta\"], case[\"lam\"])\n        results.append(res)\n    print(f\"{results}\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3191255"}, {"introduction": "在实际应用中，另一个重要挑战是高度相关预测变量的存在。本练习研究 LASSO 在这种情况下的典型行为，并将其与一个更稳健的替代方法进行对比。你会观察到，当面对一组相关变量时，LASSO 往往会任意选择一个代表并丢弃其余的。通过实现并比较 LASSO 与弹性网络（Elastic Net），你将理解“分组效应”，并学会何时应采用更高级的技术以实现稳定且有意义的变量选择。[@problem_id:3191214]", "problem": "考虑一个具有分组、高度相关预测变量的线性回归模型。设 $n$ 表示观测数量，$p$ 表示预测变量数量。假设数据由模型 $y = X \\beta + \\varepsilon$ 生成，其中 $X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^{p}$，且噪声 $\\varepsilon \\in \\mathbb{R}^{n}$ 是独立同分布 (IID) 的，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。预测变量被组织成两个大小相等（均为3）的不相交组，使得每组内的预测变量具有共同的成对相关参数 $\\rho \\in [0,1)$。\n\n您的任务是编写一个完整的程序，该程序：\n- 通过求解优化问题，实现由 $\\ell_1$ 范数惩罚的最小二乘回归，即最小绝对收缩和选择算子 (LASSO)：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1,\n$$\n并通过求解以下问题来实现弹性网络 (Elastic Net, EN)，它增加了一个二次惩罚项：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{\\lambda_2}{2} \\|\\beta\\|_2^2.\n$$\n- 使用一种对凸目标函数有保证的、有原则的迭代算法，例如带有软阈值化的坐标下降法，并对标准化后的特征进行操作。该算法不应依赖任何外部机器学习库。\n- 使用科学上合理的相​​关构造方法，构建包含两组预测变量（每组3个）的合成预测变量矩阵 $X$。具体来说，对于每组，设 $z \\in \\mathbb{R}^n$ 是一个潜在因子，满足 $z \\sim \\mathcal{N}(0, I_n)$，对于组中的每个预测变量 $j$，设 $\\epsilon_j \\sim \\mathcal{N}(0, I_n)$ 且独立于 $z$。每个预测变量定义为\n$$\nx_j = \\sqrt{\\rho} \\, z + \\sqrt{1-\\rho} \\, \\epsilon_j,\n$$\n然后通过减去样本均值并进行缩放来标准化每一列，以精确满足 $(1/n) \\sum_{i=1}^n x_{ij}^2 = 1$。连接这两个组以形成 $X \\in \\mathbb{R}^{n \\times 6}$。\n- 通过 $y = X \\beta + \\varepsilon$ 生成响应，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。\n- 对于每种拟合方法，将选定的变量定义为满足 $|\\hat{\\beta}_j| > \\tau$ 的索引 $j$，其中固定阈值 $\\tau = 10^{-3}$。\n- 按组报告选定变量的数量（组内满足 $|\\hat{\\beta}_j| > \\tau$ 的 $j$ 的计数），并使用这些计数来评估以下特性：\n    1. LASSO 在每个高度相关的组中只选择一个代表变量的情况。\n    2. LASSO 完全错过整个弱信号组的情况。\n    3. 与弹性网络对比，由于二次惩罚项，弹性网络倾向于在组内选择多个相关的预测变量。\n\n使用以下带有固定种子的测试套件以确保可复现性。对于每个测试用例，使用指定的参数构建数据并拟合 LASSO 和弹性网络。在所有情况下，用给定的数值表示 $n$、$\\rho$、$\\sigma$ 和惩罚水平。\n\n测试套件：\n- 测试用例 1（理想情况：一个强信号组和一个弱信号组，两者都高度相关）：\n    - $n = 800$\n    - 组 1 相关性 $\\rho_1 = 0.9$，组 2 相关性 $\\rho_2 = 0.9$\n    - 真实系数：组 1 的 $\\beta = (1, 1, 1)$，组 2 的 $\\beta = (0.2, 0.2, 0.2)$\n    - 噪声标准差 $\\sigma = 0.1$\n    - LASSO 惩罚 $\\lambda = 0.25$\n    - 弹性网络惩罚 $\\lambda_1 = 0.15$，$\\lambda_2 = 0.1$\n    - 随机种子 $123$\n    - 此用例需输出的布尔值评估：\n        - $b_{1,1}$：LASSO 在组 1 中恰好选择一个变量。\n        - $b_{1,2}$：LASSO 在组 2 中选择零个变量。\n        - $b_{1,3}$：弹性网络在组 1 中至少选择两个变量。\n        - $b_{1,4}$：弹性网络在组 2 中至少选择一个变量。\n- 测试用例 2（边界情况：一个弱信号相关组和一个空信号组）：\n    - $n = 800$\n    - 组 1 相关性 $\\rho_1 = 0.95$，组 2 相关性 $\\rho_2 = 0.95$\n    - 真实系数：组 1 的 $\\beta = (0, 0, 0)$，组 2 的 $\\beta = (0.3, 0.3, 0.3)$\n    - 噪声标准差 $\\sigma = 0.1$\n    - LASSO 惩罚 $\\lambda = 0.30$\n    - 弹性网络惩罚 $\\lambda_1 = 0.18$，$\\lambda_2 = 0.12$\n    - 随机种子 $456$\n    - 此用例需输出的布尔值评估：\n        - $b_{2,1}$：LASSO 在组 2 中选择零个变量。\n        - $b_{2,2}$：弹性网络在组 2 中至少选择一个变量。\n- 测试用例 3（边界情况：不相关的预测变量）：\n    - $n = 800$\n    - 组 1 相关性 $\\rho_1 = 0.0$，组 2 相关性 $\\rho_2 = 0.0$\n    - 真实系数：组 1 的 $\\beta = (1, 1, 1)$，组 2 的 $\\beta = (0.2, 0.2, 0.2)$\n    - 噪声标准差 $\\sigma = 0.1$\n    - LASSO 惩罚 $\\lambda = 0.25$\n    - 弹性网络惩罚 $\\lambda_1 = 0.15$，$\\lambda_2 = 0.1$\n    - 随机种子 $789$\n    - 此用例需输出的布尔值评估：\n        - $b_{3,1}$：LASSO 和弹性网络对两组的选择计数相同（即两种方法对每组的选择数量相等）。\n\n最终输出格式：\n- 您的程序应生成单行输出，按顺序包含所有布尔结果\n$$\n[b_{1,1}, b_{1,2}, b_{1,3}, b_{1,4}, b_{2,1}, b_{2,2}, b_{3,1}],\n$$\n用方括号括起来，并用逗号分隔（例如，“[True,False,True,...]”）。", "solution": "该问题要求实现和比较两种正则化线性回归模型，即 LASSO 和弹性网络，以展示它们在存在高度相关预测变量时的变量选择特性。验证过程确认了该问题在科学上是合理的、提法得当的，并为可复现的数值实验提供了所有必要的规范。\n\n### 1. 问题表述和数据生成\n\n我们考虑一个线性模型 $y = X \\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是预测变量矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是独立同分布 (IID) 的高斯噪声，满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。预测变量的数量为 $p=6$，分为大小为 3 的两组。\n\n预测变量矩阵 $X$ 的构造旨在使其每组内部具有特定的相关性结构。对于具有指定成对相关性 $\\rho$ 的组，每个预测变量列 $x_j$ 通过一个潜在因子模型生成：\n$$\nx_j = \\sqrt{\\rho} \\, z + \\sqrt{1-\\rho} \\, \\epsilon_j\n$$\n其中 $z \\in \\mathbb{R}^n$ 是一个共享的潜在因子，满足 $z \\sim \\mathcal{N}(0, I_n)$，而 $\\epsilon_j \\in \\mathbb{R}^n$ 是独立的特异性噪声项，也服从 $\\mathcal{N}(0, I_n)$ 分布。这种构造确保了同一组内任意两个预测变量 $x_j$ 和 $x_k$ 之间的理论相关性为 $\\text{Cov}(x_j, x_k) = \\rho$。这两个组是独立生成的。构造完成后，连接起来的矩阵 $X \\in \\mathbb{R}^{n \\times 6}$ 被标准化，使得每一列的样本均值为 0，样本二阶矩为 1，即 $(1/n) \\sum_{i=1}^n x_{ij}^2 = 1$。\n\n### 2. 正则化回归模型与算法\n\n任务是实现两种相关的模型：\n\n**LASSO（最小绝对收缩和选择算子）**求解以下优化问题：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\n$\\ell_1$ 范数惩罚项 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 能诱导稀疏性，迫使一些系数恰好为零。\n\n**弹性网络 (EN)** 在 LASSO 目标函数的基础上增加了一个 $\\ell_2$ 范数惩罚项：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\frac{1}{2n} \\| y - X \\beta \\|_2^2 + \\lambda_1 \\|\\beta\\|_1 + \\frac{\\lambda_2}{2} \\|\\beta\\|_2^2\n$$\n二次惩罚项 $\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2$ 会鼓励“分组效应”，即高度相关的预测变量倾向于被一起选择或一起剔除。\n\n我们使用**坐标下降法**来求解这些凸优化问题。该算法在保持所有其他系数固定的情况下，迭代地更新每个系数 $\\beta_j$。对于弹性网络的目标函数，第 $j$ 个系数的更新规则是通过仅对 $\\beta_j$ 最小化目标函数来导出的。这会得到一个涉及软阈值算子 $S_\\alpha(z) = \\text{sgn}(z) \\max(|z|-\\alpha, 0)$ 的闭式解。$\\beta_j$ 的更新公式为：\n$$\n\\beta_j^{\\text{new}} \\leftarrow \\frac{S_{\\lambda_1}(z_j)}{1 + \\lambda_2}\n$$\n其中 $z_j = \\frac{1}{n} X_j^T (y - \\sum_{k \\neq j} X_k \\beta_k)$ 是第 $j$ 个预测变量与偏残差之间的简单协方差。LASSO 的更新是 $\\lambda_2=0$ 的特例。算法会遍历所有系数进行迭代，直到 $\\beta$ 向量的变化小于指定的容差。对于这个问题，我们使用标准化的预测变量和中心化的响应向量，这是拟合无截距正则化模型的标准做法。\n\n### 3. 选择特性的评估\n\n将实现的模型应用于三个具有固定参数和随机种子的测试用例，以确保可复现性。对于每个模型拟合，如果估计的系数 $\\hat{\\beta}_j$ 的绝对值大于一个很小的阈值，即 $|\\hat{\\beta}_j| > \\tau = 10^{-3}$，则认为变量 $j$ 被“选中”。然后我们计算两个预测变量组中各自被选中变量的数量。\n\n- **测试用例 1** 使用高度相关的预测变量（$\\rho=0.9$），其中一个为强信号组（$\\beta_j=1$），另一个为弱信号组（$\\beta_j=0.2$）。此设置旨在展示 LASSO 从强信号组中选择单个代表，并完全忽略弱信号组，而弹性网络由于其分组效应，会从强信号组中选择多个预测变量，并从弱信号组中至少选择一个。\n- **测试用例 2** 对比了一个空信号组（$\\beta_j=0$）和一个信号较弱但相关性极高（$\\rho=0.95$）的组（$\\beta_j=0.3$）。在高惩罚下，预计 LASSO 会错过这个弱信号组，而弹性网络的分组效应可能有助于其识别该组的信号。\n- **测试用例 3** 检验了预测变量不相关（$\\rho=0.0$）的情况。在这种情况下，弹性网络的分组优势被抵消。该测试评估 LASSO 和弹性网络的选择行为是否会变得相同，这取决于具体的惩罚参数。\n\n程序为每个用例计算指定的布尔评估，以测试这些预期行为并提供量化结果。最终输出将这些布尔结果汇总到一个列表中。", "answer": "```python\nimport numpy as np\n\ndef generate_correlated_group(n, p_group, rho, rng):\n    \"\"\"Generates a group of correlated predictors using a given RNG.\"\"\"\n    # Latent common factor\n    z = rng.standard_normal(size=(n, 1))\n    # Independent idiosyncratic noise for each predictor\n    epsilon = rng.standard_normal(size=(n, p_group))\n    \n    # Construct predictors\n    X_group = np.sqrt(rho) * z + np.sqrt(1 - rho) * epsilon\n    return X_group\n\ndef generate_data(n, p_group, rho1, rho2, beta_true, sigma, seed):\n    \"\"\"Generates the full design matrix X and response y.\"\"\"\n    # Seed sequence for reproducible and independent random number generation\n    ss = np.random.SeedSequence(seed)\n    child_seeds = ss.spawn(3)\n    rng1, rng2, rng_noise = [np.random.default_rng(s) for s in child_seeds]\n\n    X1 = generate_correlated_group(n, p_group, rho1, rng1)\n    X2 = generate_correlated_group(n, p_group, rho2, rng2)\n    \n    # Concatenate to form the full design matrix\n    X = np.hstack((X1, X2))\n    \n    # Standardize X: mean 0 and sample variance 1.\n    # This satisfies (1/n) * sum(x_ij^2) = 1 because mean is 0.\n    X_mean = X.mean(axis=0)\n    X_std = X.std(axis=0, ddof=0)\n    # Avoid division by zero if a column is constant\n    X_std[X_std == 0] = 1.0\n    X_standardized = (X - X_mean) / X_std\n    \n    # Generate noise and response y\n    epsilon = rng_noise.normal(0, sigma, size=n)\n    y = X_standardized @ beta_true + epsilon\n    \n    return X_standardized, y\n\ndef fit_elastic_net(X, y, lambda1, lambda2, max_iter=2000, tol=1e-6):\n    \"\"\"\n    Fits an Elastic Net model using coordinate descent.\n    LASSO is a special case where lambda2 = 0.\n    \"\"\"\n    n, p = X.shape\n    \n    # Center y for intercept-less model\n    y_centered = y - y.mean()\n    \n    beta = np.zeros(p)\n    \n    for _ in range(max_iter):\n        beta_old = beta.copy()\n        \n        for j in range(p):\n            # Calculate partial residual r_j = y - sum_{k!=j} X_k beta_k\n            r_j = y_centered - (X @ beta - X[:, j] * beta[j])\n            \n            # Argument to the soft-thresholding function\n            z_j = (X[:, j] @ r_j) / n\n            \n            # Apply the soft-thresholding update for Elastic Net\n            if z_j > lambda1:\n                beta[j] = (z_j - lambda1) / (1 + lambda2)\n            elif z_j < -lambda1:\n                beta[j] = (z_j + lambda1) / (1 + lambda2)\n            else:\n                beta[j] = 0.0\n        \n        # Check for convergence\n        if np.max(np.abs(beta - beta_old)) < tol:\n            break\n            \n    return beta\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.9, \"rho2\": 0.9,\n            \"beta_true\": np.array([1, 1, 1, 0.2, 0.2, 0.2]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.25,\n            \"en_lambda1\": 0.15, \"en_lambda2\": 0.1,\n            \"seed\": 123\n        },\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.95, \"rho2\": 0.95,\n            \"beta_true\": np.array([0, 0, 0, 0.3, 0.3, 0.3]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.30,\n            \"en_lambda1\": 0.18, \"en_lambda2\": 0.12,\n            \"seed\": 456\n        },\n        {\n            \"n\": 800,\n            \"p_group\": 3,\n            \"rho1\": 0.0, \"rho2\": 0.0,\n            \"beta_true\": np.array([1, 1, 1, 0.2, 0.2, 0.2]),\n            \"sigma\": 0.1,\n            \"lasso_lambda\": 0.25,\n            \"en_lambda1\": 0.15, \"en_lambda2\": 0.1,\n            \"seed\": 789\n        }\n    ]\n\n    all_results = []\n    tau = 1e-3\n    p_group = 3\n\n    # Test Case 1\n    case = test_cases[0]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g1 = np.sum(selected_lasso[:p_group])\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g1 = np.sum(selected_en[:p_group])\n    count_en_g2 = np.sum(selected_en[p_group:])\n\n    b_1_1 = (count_lasso_g1 == 1)\n    b_1_2 = (count_lasso_g2 == 0)\n    b_1_3 = (count_en_g1 >= 2)\n    b_1_4 = (count_en_g2 >= 1)\n    all_results.extend([b_1_1, b_1_2, b_1_3, b_1_4])\n\n    # Test Case 2\n    case = test_cases[1]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g2 = np.sum(selected_en[p_group:])\n    \n    b_2_1 = (count_lasso_g2 == 0)\n    b_2_2 = (count_en_g2 >= 1)\n    all_results.extend([b_2_1, b_2_2])\n\n    # Test Case 3\n    case = test_cases[2]\n    X, y = generate_data(case[\"n\"], case[\"p_group\"], case[\"rho1\"], case[\"rho2\"],\n                         case[\"beta_true\"], case[\"sigma\"], case[\"seed\"])\n    \n    beta_lasso = fit_elastic_net(X, y, case[\"lasso_lambda\"], 0)\n    beta_en = fit_elastic_net(X, y, case[\"en_lambda1\"], case[\"en_lambda2\"])\n\n    selected_lasso = np.abs(beta_lasso) > tau\n    selected_en = np.abs(beta_en) > tau\n\n    count_lasso_g1 = np.sum(selected_lasso[:p_group])\n    count_lasso_g2 = np.sum(selected_lasso[p_group:])\n    count_en_g1 = np.sum(selected_en[:p_group])\n    count_en_g2 = np.sum(selected_en[p_group:])\n\n    b_3_1 = (count_lasso_g1 == count_en_g1) and (count_lasso_g2 == count_en_g2)\n    all_results.append(b_3_1)\n\n    print(f\"[{','.join(str(r) for r in all_results)}]\")\n\nsolve()\n```", "id": "3191214"}]}