{"hands_on_practices": [{"introduction": "理论知识是基础，但真正的理解来自于实践。在第一个动手练习中，我们将从第一性原理出发，为LASSO（最小绝对收缩和选择算子）目标函数推导坐标下降的单坐标更新法则。这个练习旨在巩固你对软阈值算子如何从优化问题中自然产生的理解，并让你亲手构建一个可以工作的坐标下降求解器，为后续更深入的探索打下坚实的基础。[@problem_id:2861565]", "problem": "您的任务是利用凸优化和信号处理的原理，为最小绝对值收缩和选择算子 (LASSO) 问题推导并实现一个循环坐标下降算法。\n\n考虑设计矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和观测向量 $b \\in \\mathbb{R}^{m}$ 的 LASSO 目标函数：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 是给定的正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数。\n\n您的任务是：\n1. 从第一性原理出发，为 $f(x)$ 的循环坐标下降法推导单坐标最小化更新规则。从 $f(x)$ 的定义和 $\\ell_1$ 范数的次梯度最优性条件开始，论证在保持所有其他坐标固定的情况下，关于单个坐标 $x_i$ 对 $f(x)$ 的最小化。仅使用基本事实，包括凸函数的性质、绝对值的次梯度和基本线性代数。不要先验地假设任何特定的闭式更新。\n2. 证明坐标级最小解是通过将软阈值算子应用于当前迭代值和残差的仿射函数得到的。清晰地定义您推导中引入的所有量。\n3. 实现一个使用所推导更新规则的循环坐标下降算法。您的实现必须：\n   - 维护残差 $r \\triangleq b - A x$，并在每次坐标更新后增量式地更新它，以达到每个坐标更新 $\\mathcal{O}(m)$ 的成本。\n   - 使用由 $S_{\\tau}(z) \\triangleq \\mathrm{sign}(z)\\max(|z| - \\tau, 0)$ 定义的软阈值算子。\n   - 当一个完整周期内任何坐标的最大绝对变化小于容差 $\\varepsilon$ 或达到最大轮数时终止。\n   - 返回最终迭代值 $x$，并在要求时返回每轮结束时的目标函数值序列以评估单调性。\n\n您可以使用的基本知识：\n- $\\|\\cdot\\|_2^2$ 和 $\\|\\cdot\\|_1$ 的凸性及其次梯度的性质。\n- 次梯度最优性条件：对于凸函数 $f$ 的一个最优解 $x^\\star$，有 $0 \\in \\partial f(x^\\star)$。\n- 绝对值的次微分：对于 $t \\in \\mathbb{R}$，如果 $t \\ne 0$，则 $\\partial |t| = \\{\\mathrm{sign}(t)\\}$；如果 $t = 0$，则 $\\partial |t| = [-1,1]$。\n- 用于残差更新的线性代数恒等式。\n\n将目标函数值定义为：\n$$\nf(x) = \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\n\n测试套件：\n实现您的程序以运行以下五个测试用例，并将结果汇总到单行输出中。\n\n- 测试 1 (标准正交列，解析解检验)：设置 $A = I_4$，$b = [3,-1,0.2,-0.5]^\\top$，$\\lambda = 0.7$。运行您的坐标下降算法得到 $x_{\\mathrm{cd}}$。对于标准正交列，已知的解析解为 $x^\\star = S_{\\lambda}(A^\\top b) = S_{\\lambda}(b)$。输出标量\n  $$\n  e_1 \\triangleq \\|x_{\\mathrm{cd}} - x^\\star\\|_{\\infty}.\n  $$\n\n- 测试 2 (通用高矩阵系统，Karush–Kuhn–Tucker (KKT) 条件检验)：生成一个 $A \\in \\mathbb{R}^{60 \\times 30}$ 矩阵，其元素为独立的标准正态分布，然后将每列归一化为单位 $\\ell_2$ 范数。使用固定的伪随机种子 $0$ 以使实例具有确定性。定义 $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$，其在索引 $0,5,10,15,20$ 处的非零项分别为 $[2.5,-1.7,1.2,-0.9,1.8]$，其他位置为零。设 $b = A x_{\\mathrm{true}} + \\eta$，其中 $\\eta \\in \\mathbb{R}^{60}$ 的元素是使用相同种子 $0$ 生成的、标准差为 $0.01$ 的独立正态分布。设 $\\lambda = 0.05$。运行坐标下降得到 $x_{\\mathrm{cd}}$。验证 LASSO 的 KKT 条件：令 $g \\triangleq A^\\top(A x_{\\mathrm{cd}} - b)$，\n  - 若 $x_{\\mathrm{cd},i} \\ne 0$，则 $g_i + \\lambda \\,\\mathrm{sign}(x_{\\mathrm{cd},i}) = 0$。\n  - 若 $x_{\\mathrm{cd},i} = 0$，则 $|g_i| \\le \\lambda$。\n  由于数值误差，在这些检查中实现一个 $10^{-4}$ 的容差。输出布尔值 $b_2$，指示所有坐标是否在容差范围内满足 KKT 条件。\n\n- 测试 3 (大正则化导致解为零)：使用 $A = I_4$，$b = [3,-1,0.2,-0.5]^\\top$ 和 $\\lambda = 10^6$。输出布尔值 $b_3$，指示返回的解是否在 $10^{-12}$ 的绝对容差内为零向量。\n\n- 测试 4 (零正则化简化为最小二乘)：使用伪随机种子 $1$ 生成一个 $A \\in \\mathbb{R}^{40 \\times 10}$ 矩阵，其元素为独立的标准正态分布。使用种子 $2$ 生成一个 $b \\in \\mathbb{R}^{40}$ 向量，其元素为独立的标准正态分布。设 $\\lambda = 0$。令 $x_{\\mathrm{ls}}$ 表示通过标准线性最小二乘法计算出的最小化 $\\frac{1}{2}\\|A x - b\\|_2^2$ 的最小二乘解。运行坐标下降得到 $x_{\\mathrm{cd}}$。输出标量\n  $$\n  e_4 \\triangleq \\frac{\\|x_{\\mathrm{cd}} - x_{\\mathrm{ls}}\\|_2}{\\max(\\|x_{\\mathrm{ls}}\\|_2, 10^{-12})}.\n  $$\n\n- 测试 5 (目标函数值随轮数单调下降)：使用伪随机种子 $3$ 生成 $A \\in \\mathbb{R}^{30 \\times 15}$ 和 $b \\in \\mathbb{R}^{30}$，其元素为独立的标准正态分布。设 $\\lambda = 0.1$。记录每次完整遍历所有坐标后的目标函数值，并验证该序列在数值容差 $10^{-10}$ 内是非增的。输出布尔值 $b_5$，指示单调性是否成立。\n\n您的程序应生成单行输出，其中包含按顺序 $[e_1, b_2, b_3, e_4, b_5]$ 排列、用方括号括起来的逗号分隔列表。此问题不涉及物理单位，也与角度单位无关。所有数值输出应为指定的实数或布尔值，不带百分号。您的实现必须对给定的实例具有鲁棒性，并且不应需要任何用户输入。", "solution": "我们从凸优化问题开始\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; f(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，且 $\\lambda \\ge 0$。函数 $f$ 是凸函数，因为它是凸函数 $\\frac{1}{2}\\|A x - b\\|_2^2$ 和凸函数 $\\lambda \\|x\\|_1$ 的和。\n\n循环坐标下降在保持其他坐标固定的同时，一次针对一个坐标对 $f$ 进行最小化。固定一个索引 $i \\in \\{1,\\dots,n\\}$，并用 $a_i \\in \\mathbb{R}^m$ 表示 $A$ 的第 $i$ 列。设 $x \\in \\mathbb{R}^n$ 为当前迭代值，并定义残差\n$$\nr \\triangleq b - A x.\n$$\n因为 $A x = \\sum_{j=1}^n a_j x_j$，只将 $x_i$ 更改为新值 $t \\in \\mathbb{R}$ 会得到新向量 $x^{(i \\leftarrow t)}$ 和新残差\n$$\nr^{(i \\leftarrow t)} = b - A x^{(i \\leftarrow t)} = b - \\left(A x + a_i (t - x_i)\\right) = r - a_i (t - x_i).\n$$\n目标函数作为 $t$ 的函数（其他坐标固定）变为\n\\begin{align*}\n\\phi_i(t) \\triangleq \\frac{1}{2}\\|A x^{(i \\leftarrow t)} - b\\|_2^2 + \\lambda \\left(\\sum_{j \\ne i} |x_j| + |t|\\right) \\\\\n= \\frac{1}{2}\\|r^{(i \\leftarrow t)}\\|_2^2 + \\lambda |t| + \\text{与 t 无关的常数} \\\\\n= \\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2 + \\lambda |t| + \\text{常数}.\n\\end{align*}\n使用 $\\|u - v\\|_2^2 = \\|u\\|_2^2 - 2 u^\\top v + \\|v\\|_2^2$ 展开平方范数，我们得到\n\\begin{align*}\n\\frac{1}{2}\\|r - a_i (t - x_i)\\|_2^2\n= \\frac{1}{2}\\|r\\|_2^2 - (t - x_i) a_i^\\top r + \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2.\n\\end{align*}\n舍去与 $t$ 无关的项，坐标级目标简化为单变量凸函数\n$$\n\\tilde{\\phi}_i(t) \\triangleq \\frac{1}{2}\\|a_i\\|_2^2 (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t|.\n$$\n通过配方法，定义 $d_i \\triangleq \\|a_i\\|_2^2$ 和\n$$\nc_i \\triangleq x_i + \\frac{a_i^\\top r}{d_i} \\quad \\text{当 } d_i  0 \\text{ 时}.\n$$\n那么\n\\begin{align*}\n\\tilde{\\phi}_i(t)\n= \\frac{1}{2} d_i (t - x_i)^2 - (t - x_i) a_i^\\top r + \\lambda |t| \\\\\n= \\frac{1}{2} d_i \\left(t - x_i - \\frac{a_i^\\top r}{d_i}\\right)^2 - \\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i} + \\lambda |t|.\n\\end{align*}\n忽略常数 $-\\frac{1}{2}\\frac{(a_i^\\top r)^2}{d_i}$，对 $t$ 最小化 $\\tilde{\\phi}_i(t)$ 等价于最小化\n$$\n\\psi_i(t) \\triangleq \\frac{1}{2} d_i (t - c_i)^2 + \\lambda |t|.\n$$\n这个一维凸问题的次梯度最优性条件是\n$$\n0 \\in \\partial \\psi_i(t^\\star) = d_i (t^\\star - c_i) + \\lambda \\,\\partial |t^\\star|,\n$$\n其中绝对值的次微分是：如果 $t^\\star \\ne 0$，则 $\\partial |t^\\star| = \\{\\mathrm{sign}(t^\\star)\\}$；如果 $t^\\star = 0$，则 $\\partial |t^\\star| = [-1, 1]$。\n\n考虑两种情况。\n\n情况 1：$t^\\star \\ne 0$。那么次梯度条件是\n$$\nd_i (t^\\star - c_i) + \\lambda \\,\\mathrm{sign}(t^\\star) = 0 \\;\\;\\Longleftrightarrow\\;\\; t^\\star = c_i - \\frac{\\lambda}{d_i} \\,\\mathrm{sign}(t^\\star).\n$$\n这意味着 $|c_i| > \\lambda/d_i$，解通过将 $c_i$ 向零收缩 $\\lambda/d_i$ 同时保持其符号来获得：\n$$\nt^\\star = \\mathrm{sign}(c_i)\\left(|c_i| - \\frac{\\lambda}{d_i}\\right).\n$$\n\n情况 2：$t^\\star = 0$。那么次梯度条件变为\n$$\n0 \\in - d_i c_i + \\lambda [-1,1] \\;\\;\\Longleftrightarrow\\;\\; |d_i c_i| \\le \\lambda \\;\\;\\Longleftrightarrow\\;\\; |c_i| \\le \\frac{\\lambda}{d_i}.\n$$\n结合两种情况，得到软阈值形式\n$$\nt^\\star = S_{\\lambda/d_i}(c_i) \\triangleq \\mathrm{sign}(c_i)\\max\\left(|c_i| - \\frac{\\lambda}{d_i}, \\, 0 \\right).\n$$\n等价地，使用残差定义 $r = b - A x$，我们有\n$$\nc_i = x_i + \\frac{a_i^\\top r}{d_i} = \\frac{a_i^\\top r + d_i x_i}{d_i},\n$$\n所以坐标级最小解是\n$$\nx_i \\leftarrow S_{\\lambda/\\|a_i\\|_2^2}\\!\\left(\\frac{a_i^\\top r + \\|a_i\\|_2^2 x_i}{\\|a_i\\|_2^2}\\right).\n$$\n如果 $d_i = \\|a_i\\|_2^2 = 0$（一个零列），$x_i$ 的任何变化都不会影响二次项；对于 $\\lambda > 0$，$\\lambda |t|$ 的最小解是 $t^\\star = 0$。在我们的实现中，如果 $d_i = 0$ 且 $\\lambda > 0$，我们设置 $x_i \\leftarrow 0$；如果 $\\lambda = 0$ 且 $d_i = 0$，该坐标无关紧要，可以保持不变。\n\n高效的残差更新：如果 $\\Delta_i \\triangleq x_i^{\\text{new}} - x_i^{\\text{old}}$，则\n$$\nr^{\\text{new}} = b - A x^{\\text{new}} = b - \\left(A x^{\\text{old}} + a_i \\Delta_i\\right) = r^{\\text{old}} - a_i \\Delta_i,\n$$\n其成本为 $\\mathcal{O}(m)$ 次操作。\n\n收敛性和单调性：每次坐标更新都会在该坐标上精确地最小化 $f$，因此每次坐标更新后 $f$ 是非增的，从而在每轮（对所有坐标的一次完整遍历）后也是非增的。当一轮中的最大绝对坐标变化低于容差或达到最大轮数时，算法终止。\n\n通过 Karush–Kuhn–Tucker (KKT) 条件进行最优性验证：令 $g(x) \\triangleq A^\\top (A x - b)$ 为光滑部分的梯度。LASSO 中 $x^\\star$ 的最优性 KKT 条件是\n$$\n0 \\in g(x^\\star) + \\lambda \\,\\partial \\|x^\\star\\|_1,\n$$\n这等价于分量级条件\n$$\n\\begin{cases}\ng_i(x^\\star) + \\lambda \\,\\mathrm{sign}(x_i^\\star) = 0,  \\text{如果 } x_i^\\star \\ne 0, \\\\\n|g_i(x^\\star)| \\le \\lambda,  \\text{如果 } x_i^\\star = 0.\n\\end{cases}\n$$\n在实践中，我们在一个小的数值容差内检查这些等式和不等式。\n\n测试用例和输出：我们实现指定的五个测试用例并计算\n- $e_1 = \\|x_{\\mathrm{cd}} - S_{\\lambda}(b)\\|_\\infty$ 对于标准正交列，\n- $b_2$ 指示 KKT 条件在高矩阵系统中是否在容差内满足，\n- $b_3$ 指示对于非常大的 $\\lambda$，解是否为零，\n- $e_4$ 当 $\\lambda=0$ 时与最小二乘解的相对误差，\n- $b_5$ 指示目标函数值是否在各轮中单调非增。\n\n最终程序将结果作为单个列表 $[e_1, b_2, b_3, e_4, b_5]$ 在一行上输出。", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z: float, tau: float) -> float:\n    \"\"\"Soft-thresholding operator S_tau(z) = sign(z) * max(|z| - tau, 0).\"\"\"\n    if tau == 0:\n        return z\n    abs_z = abs(z)\n    if abs_z = tau:\n        return 0.0\n    return np.copysign(abs_z - tau, z)\n\ndef objective_value(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float) -> float:\n    r = A @ x - b\n    return 0.5 * float(r.T @ r) + lambd * float(np.linalg.norm(x, 1))\n\ndef coordinate_descent_lasso(\n    A: np.ndarray,\n    b: np.ndarray,\n    lambd: float,\n    max_epochs: int = 2000,\n    tol: float = 1e-8,\n    record_objective: bool = False\n):\n    \"\"\"\n    Cyclic coordinate descent for LASSO:\n        minimize 0.5 * ||A x - b||_2^2 + lambd * ||x||_1.\n    Uses residual updates for O(m) per coordinate.\n    \"\"\"\n    m, n = A.shape\n    x = np.zeros(n, dtype=float)\n    r = b.copy()  # r = b - A x, initially x=0\n    obj_hist = []\n\n    # Precompute squared column norms d_i = ||a_i||_2^2\n    col_sq_norms = np.sum(A * A, axis=0)\n\n    for epoch in range(max_epochs):\n        max_delta = 0.0\n        for i in range(n):\n            ai = A[:, i]\n            di = col_sq_norms[i]\n            xi_old = x[i]\n\n            if di == 0.0:\n                # If the column is zero, the quadratic does not depend on x_i.\n                # Minimizer of lambd * |t| is t=0 for lambd>0; do nothing if lambd==0.\n                xi_new = 0.0 if lambd > 0 else xi_old\n            else:\n                # c_i = x_i + (a_i^T r) / d_i\n                # Note: r is b - A x_current, so a_i^T r needs to be adjusted\n                # to represent correlation with partial residual.\n                # r_partial_i = b - sum_{k!=i} A_k x_k = r + A_i x_i\n                # c_i argument to soft-threshold is (a_i^T r_partial_i) / d_i\n                # This is (a_i^T (r + a_i x_i)) / d_i = (a_i^T r)/d_i + x_i\n                ci = xi_old + float(ai.T @ r) / di\n                # Update via soft-thresholding with threshold lambd / d_i\n                xi_new = soft_threshold(ci, lambd / di)\n\n            delta = xi_new - xi_old\n            if delta != 0.0:\n                # Update residual: r_new = r_old - a_i * delta\n                r -= ai * delta\n                x[i] = xi_new\n            \n            max_delta = max(max_delta, abs(delta))\n\n        if record_objective:\n            obj_hist.append(objective_value(A, b, x, lambd))\n\n        if max_delta  tol:\n            break\n\n    if record_objective:\n        return x, obj_hist\n    return x\n\ndef kkt_satisfied(A: np.ndarray, b: np.ndarray, x: np.ndarray, lambd: float, tol: float = 1e-4) -> bool:\n    \"\"\"\n    Check KKT conditions for LASSO:\n      g = A^T (A x - b)\n      If x_i != 0: g_i + lambd * sign(x_i) = 0\n      If x_i == 0: |g_i| = lambd\n    with numerical tolerance tol.\n    \"\"\"\n    g = A.T @ (A @ x - b)\n    for i in range(x.size):\n        xi = x[i]\n        gi = g[i]\n        if abs(xi) > 1e-12:\n            if abs(gi + lambd * np.sign(xi)) > tol:\n                return False\n        else:\n            if abs(gi) - lambd > tol:\n                return False\n    return True\n\ndef test_suite():\n    results = []\n\n    # Test 1: Orthonormal columns (A = I), analytical solution S_lambda(b)\n    A1 = np.eye(4)\n    b1 = np.array([3.0, -1.0, 0.2, -0.5])\n    lambd1 = 0.7\n    x1 = coordinate_descent_lasso(A1, b1, lambd1, max_epochs=100, tol=1e-12)\n    # Analytical solution: S_lambda(b)\n    x1_star = np.array([soft_threshold(b1[i], lambd1) for i in range(4)])\n    e1 = float(np.max(np.abs(x1 - x1_star)))\n    results.append(e1)\n\n    # Test 2: General tall system, KKT check\n    rng = np.random.default_rng(0)\n    A2 = rng.standard_normal((60, 30))\n    # Normalize columns to unit norm\n    col_norms = np.linalg.norm(A2, axis=0)\n    # Avoid division by zero in extremely unlikely all-zero columns\n    col_norms[col_norms == 0.0] = 1.0\n    A2 = A2 / col_norms\n    x_true = np.zeros(30)\n    nz_idx = [0, 5, 10, 15, 20]\n    nz_vals = [2.5, -1.7, 1.2, -0.9, 1.8]\n    for idx, val in zip(nz_idx, nz_vals):\n        x_true[idx] = val\n    noise = rng.normal(0.0, 0.01, size=60)\n    b2 = A2 @ x_true + noise\n    lambd2 = 0.05\n    x2 = coordinate_descent_lasso(A2, b2, lambd2, max_epochs=2000, tol=1e-10)\n    b2_ok = kkt_satisfied(A2, b2, x2, lambd2, tol=1e-4)\n    results.append(bool(b2_ok))\n\n    # Test 3: Large lambda -> zero solution\n    lambd3 = 1e6\n    x3 = coordinate_descent_lasso(A1, b1, lambd3, max_epochs=50, tol=1e-14)\n    b3_ok = bool(np.allclose(x3, 0.0, atol=1e-12))\n    results.append(bool(b3_ok))\n\n    # Test 4: Zero lambda -> least squares\n    rng1 = np.random.default_rng(1)\n    A4 = rng1.standard_normal((40, 10))\n    rng2 = np.random.default_rng(2)\n    b4 = rng2.standard_normal(40)\n    lambd4 = 0.0\n    x4_cd = coordinate_descent_lasso(A4, b4, lambd4, max_epochs=5000, tol=1e-12)\n    # Least squares solution via numpy\n    x4_ls, *_ = np.linalg.lstsq(A4, b4, rcond=None)\n    denom = max(np.linalg.norm(x4_ls), 1e-12)\n    e4 = float(np.linalg.norm(x4_cd - x4_ls) / denom)\n    results.append(e4)\n\n    # Test 5: Monotone objective decrease across epochs\n    rng3 = np.random.default_rng(3)\n    A5 = rng3.standard_normal((30, 15))\n    b5 = rng3.standard_normal(30)\n    lambd5 = 0.1\n    x5, obj_hist = coordinate_descent_lasso(A5, b5, lambd5, max_epochs=200, tol=1e-10, record_objective=True)\n    # Check nonincreasing sequence within small tolerance\n    diffs = np.diff(obj_hist)\n    b5_ok = bool(np.all(diffs = 1e-10))\n    results.append(bool(b5_ok))\n\n    return results\n\ndef solve():\n    results = test_suite()\n    # Format booleans and floats in a single list\n    # Convert to string with Python default formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2861565"}, {"introduction": "在拥有了一个基本的求解器之后，我们来探讨一个在实践中应用正则化模型时至关重要的方面。这个练习通过一个精心设计的虚拟数据集，揭示了特征尺度的差异如何无意中影响LASSO的解，因为坐标下降的更新步骤对每个特征列的范数很敏感。通过比较标准化和非标准化特征下的结果，你将直观地理解为何数据预处理是实现公平有效特征选择的关键步骤。[@problem_id:3111928]", "problem": "考虑最小绝对收缩和选择算子 (LASSO) 回归问题，该问题旨在寻找系数 $\\beta \\in \\mathbb{R}^p$ 以最小化以下目标函数：\n$$\n\\frac{1}{2}\\,\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta \\rVert_1,\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$ 是一个设计矩阵，其列为 $X_{\\cdot j}$，$y \\in \\mathbb{R}^n$ 是一个响应向量，$\\lambda \\ge 0$ 是一个正则化参数。在循环坐标下降法中，每次更新单个系数 $\\beta_j$，同时保持所有其他坐标固定，并重复循环遍历所有坐标直至收敛。在实践中，列 $X_{\\cdot j}$ 的尺度会显著影响坐标级更新，当特征未经标准化时，这可能会扭曲有效的收缩效果并使特征选择产生偏差。对于本任务，特征标准化指的是缩放每一列 $X_{\\cdot j}$，使其欧几里得范数的平方等于样本大小 $n$，即 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$。\n\n您的任务是编写一个完整、可运行的程序，该程序：\n- 实现循环坐标下降法以最小化 LASSO 目标函数，从 $\\beta = 0$ 开始，迭代直至系数在 $\\ell_\\infty$ 范数下的变化量小于一个固定的容差 $\\tau$，或达到最大迭代次数。\n- 对每个测试用例，以两种模式运行求解器：不进行特征标准化和进行特征标准化（将每列缩放至其欧几里得范数的平方等于 $n$）。\n- 使用以下确定性的玩具数据集，这些数据集旨在揭示特征标准化的缺失如何扭曲坐标更新并使特征选择产生偏差。在所有情况下，随机抽样均来自标准正态分布，并且为保证可复现性，必须在每种情况下按指定的方式为伪随机数生成器设置种子。\n\n测试套件定义：\n- 情况 A（尺度倾斜和共线性特征）：\n  - 参数：$n = 60$，$p = 2$，$c = 20$，$\\lambda = 200$，种子 $= 13$。\n  - 数据生成：抽取 $u \\in \\mathbb{R}^n$，$e_1 \\in \\mathbb{R}^n$，$e_2 \\in \\mathbb{R}^n$ 和 $e_y \\in \\mathbb{R}^n$，其中每个向量的元素均为独立同分布的标准正态随机变量。令 $x_1 = u + 0.01\\, e_1$，$x_2 = c\\, u + 0.5\\, e_2$ 及 $y = u + 0.01\\, e_y$。构成 $X = [x_1, x_2]$。\n- 情况 B（基准情况，具有一个相关特征和一个无关特征的等尺度特征）：\n  - 参数：$n = 80$，$p = 2$，$\\lambda = 30$，种子 $= 7$。\n  - 数据生成：抽取 $u \\in \\mathbb{R}^n$，$v \\in \\mathbb{R}^n$，$e_1 \\in \\mathbb{R}^n$，$e_2 \\in \\mathbb{R}^n$ 和 $e_y \\in \\mathbb{R}^n$。令 $x_1 = u + 0.01\\, e_1$，$x_2 = v + 0.01\\, e_2$（其中 $u$ 与 $v$ 相互独立），以及 $y = u + 0.01\\, e_y$。构成 $X = [x_1, x_2]$。\n- 情况 C（具有非常强正则化的边缘情况）：\n  - 参数：$n = 80$，$p = 2$，$\\lambda = 2000$，种子 $= 11$。\n  - 数据生成：与情况 B 相同。\n\n算法要求：\n- 使用在每次迭代中跨坐标维护的残差 $r = y - X \\beta$ 来实现循环坐标下降更新，当单个 $\\beta_j$ 发生变化时，增量地更新 $r$。\n- 使用收敛容差 $\\tau = 10^{-8}$ 和最多 1000 次遍历所有坐标的完整周期。\n- 收敛后，将满足 $\\lvert \\beta_j \\rvert > 10^{-8}$ 的索引 $j$ 识别为选定特征。\n\n最终输出规格：\n- 对每个测试用例，生成一个包含以下内容的三元组：\n  - 未进行标准化时选定特征的索引列表。\n  - 进行标准化时选定特征的索引列表。\n  - 一个布尔值，指示这两个列表是否不同。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素都必须是上面描述的三元组，字面上表示为 Python 风格的列表。例如，格式应类似于 $[\\,[\\dots],[\\dots],\\text{True}\\,,\\, \\dots]$，但打印为标准的 Python 列表字面量，例如 `[[[1],[0],True],[[0],[0],False],[[],[],False]]`。不涉及任何物理单位、角度单位或百分比；所有输出均为纯粹的数学列表和布尔值。", "solution": "该问题经评估是有效的。这是一个在统计学习和数值优化领域内的良置、有科学依据且客观的问题。所有必要的参数、数据生成过程和算法约束都得到了明确定义，从而可以得到一个唯一且可验证的解。\n\n该任务是实现一种循环坐标下降算法来解决 LASSO（最小绝对收缩和选择算子）回归问题。需要最小化的目标函数是：\n$$\nL(\\beta) = \\frac{1}{2}\\,\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta \\rVert_1\n$$\n其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\lambda \\ge 0$ 是正则化参数。$\\ell_1$范数惩罚项 $\\lVert \\beta \\rVert_1 = \\sum_{j=1}^p |\\beta_j|$ 会在解中引入稀疏性。\n\n坐标下降法一次只针对单个系数 $\\beta_j$ 优化目标函数，同时保持所有其他系数 $\\beta_k$ (对于 $k \\ne j$) 固定。这个过程会循环遍历所有系数，直至收敛。\n\n为推导单个系数 $\\beta_j$ 的更新规则，我们将目标函数视为仅关于 $\\beta_j$ 的函数：\n$$\nL(\\beta_j) = \\frac{1}{2} \\sum_{i=1}^n \\left(y_i - \\sum_{k \\ne j} X_{ik}\\beta_k - X_{ij}\\beta_j\\right)^2 + \\lambda \\sum_{k \\ne j} |\\beta_k| + \\lambda |\\beta_j|\n$$\n令 $r_{(-j)} = y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k$ 为部分残差向量，其中 $X_{\\cdot k}$ 是 $X$ 的第 $k$ 列。$L(\\beta_j)$ 中依赖于 $\\beta_j$ 的项是：\n$$\nL_j(\\beta_j) = \\frac{1}{2} \\lVert r_{(-j)} - X_{\\cdot j}\\beta_j \\rVert_2^2 + \\lambda |\\beta_j|\n$$\n这是一个一维 LASSO 问题。为了找到最小值，我们对 $\\beta_j$ 求次梯度并将其设为 0。次梯度 $\\partial L_j(\\beta_j)$ 为：\n$$\n\\partial L_j(\\beta_j) = -X_{\\cdot j}^T (r_{(-j)} - X_{\\cdot j}\\beta_j) + \\lambda \\partial |\\beta_j|\n$$\n其中 $\\partial |\\beta_j|$ 是绝对值函数的次梯度。将次梯度设为 0 得到最优性条件：\n$$\nX_{\\cdot j}^T X_{\\cdot j} \\beta_j - X_{\\cdot j}^T r_{(-j)} \\in \\lambda \\partial |\\beta_j|\n$$\n令 $\\rho_j = X_{\\cdot j}^T r_{(-j)}$ 且 $z_j = X_{\\cdot j}^T X_{\\cdot j} = \\lVert X_{\\cdot j} \\rVert_2^2$。该条件变为 $z_j \\beta_j - \\rho_j \\in \\lambda \\partial |\\beta_j|$。其解由软阈值算子 $S(\\cdot, \\cdot)$ 给出：\n$$\n\\hat{\\beta}_j = S\\left(\\frac{\\rho_j}{z_j}, \\frac{\\lambda}{z_j}\\right)\n$$\n其中 $S(a, \\nu) = \\text{sign}(a) \\max(|a| - \\nu, 0)$。这可以分段写为：\n$$\n\\hat{\\beta}_j =\n\\begin{cases}\n(\\rho_j + \\lambda) / z_j  \\text{if } \\rho_j  -\\lambda \\\\\n(\\rho_j - \\lambda) / z_j  \\text{if } \\rho_j > \\lambda \\\\\n0  \\text{if } |\\rho_j| \\le \\lambda\n\\end{cases}\n$$\n为了高效实现，我们维护完整残差 $r = y - X\\beta$ 并对其进行增量更新。部分残差项 $\\rho_j$ 可以用完整残差表示。设 $\\beta^{\\text{old}}$ 为更新 $\\beta_j$ 之前的系数向量。当前的完整残差是 $r = y - \\sum_k X_{\\cdot k}\\beta_k^{\\text{old}}$。部分残差是 $r_{(-j)} = y - \\sum_{k \\ne j} X_{\\cdot k}\\beta_k^{\\text{old}} = r + X_{\\cdot j}\\beta_j^{\\text{old}}$。\n因此，$\\rho_j = X_{\\cdot j}^T (r + X_{\\cdot j}\\beta_j^{\\text{old}}) = X_{\\cdot j}^T r + z_j \\beta_j^{\\text{old}}$。\n\n循环坐标下降算法的流程如下：\n1. 初始化 $\\beta = 0$，$r = y$。对所有 $j=1, \\dots, p$ 预计算 $z_j = \\lVert X_{\\cdot j} \\rVert_2^2$。\n2. 对于每次迭代（周期）：\n3.   对于每个坐标 $j=1, \\dots, p$：\n    a. 设 $\\beta_j^{\\text{old}}$ 为第 $j$ 个系数的当前值。\n    b. 计算 $\\rho_j = X_{\\cdot j}^T (r + X_{\\cdot j}\\beta_j^{\\text{old}})$。\n    c. 使用 $\\rho_j$ 和 $z_j$ 通过软阈值规则计算新系数 $\\beta_j^{\\text{new}} = S(\\rho_j/z_j, \\lambda/z_j)$。\n    d. 计算系数变化 $\\Delta_j = \\beta_j^{\\text{new}} - \\beta_j^{\\text{old}}$。\n    e. 更新系数：$\\beta_j \\leftarrow \\beta_j^{\\text{new}}$。\n    f. 用系数变化更新残差：$r \\leftarrow r - X_{\\cdot j}\\Delta_j$。\n    g. 在周期内跟踪最大系数变化量 $\\max_j |\\Delta_j|$。\n4.   在一个完整周期后，如果最大系数变化量小于容差 $\\tau=10^{-8}$，或达到最大迭代次数，则停止。\n\n该问题要求以两种模式运行此算法：一种使用原始数据 $X$，另一种使用标准化后的特征。标准化确保了有效的正则化不会因特征的不同尺度而产生偏差。指定的标准化规则是将每列 $X_{\\cdot j}$ 缩放以形成新列 $X'_{\\cdot j}$，使其欧几里得范数的平方等于样本大小 $n$，即 $\\lVert X'_{\\cdot j} \\rVert_2^2 = n$。第 $j$ 列的缩放因子是 $s_j = \\sqrt{n / z_j}$。标准化后的矩阵是 $X'_{\\cdot j} = s_j X_{\\cdot j}$。当使用 $X'$ 时，更新规则中所有的 $z'_j$ 都等于 $n$，使得收缩项 $\\lambda/n$ 对所有系数都是一致的。\n\n解决方案程序为指定的测试用例实现了该算法，生成数据，在有和没有标准化的两种情况下运行求解器，并报告所选特征的索引（其中 $|\\beta_j| > 10^{-8}$）。", "answer": "```python\nimport numpy as np\n\ndef coordinate_descent(X, y, lambda_val, tol=1e-8, max_iter=1000):\n    \"\"\"\n    Solves the LASSO problem using cyclic coordinate descent.\n    Objective: 0.5 * ||y - X beta||^2 + lambda * ||beta||_1\n    \"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    r = y.copy()\n\n    z = np.sum(X**2, axis=0)\n\n    for _ in range(max_iter):\n        max_change = 0.0\n\n        for j in range(p):\n            beta_old_j = beta[j]\n            \n            if z[j] == 0:\n                continue\n            \n            rho_j = X[:, j].T @ (r + X[:, j] * beta_old_j)\n            \n            a = rho_j / z[j]\n            nu = lambda_val / z[j]\n            beta_new_j = np.sign(a) * np.maximum(np.abs(a) - nu, 0)\n            \n            delta_j = beta_new_j - beta_old_j\n            if delta_j != 0:\n                beta[j] = beta_new_j\n                r -= X[:, j] * delta_j\n            \n            max_change = max(max_change, abs(delta_j))\n\n        if max_change  tol:\n            break\n            \n    return beta\n\ndef solve_case(n, p, lambda_val, seed, case_params=None):\n    rng = np.random.default_rng(seed)\n    \n    # Data generation\n    if case_params: # Case A\n        c = case_params['c']\n        u = rng.standard_normal(n)\n        e1 = rng.standard_normal(n)\n        e2 = rng.standard_normal(n)\n        ey = rng.standard_normal(n)\n        x1 = u + 0.01 * e1\n        x2 = c * u + 0.5 * e2\n        y = u + 0.01 * ey\n        X = np.stack([x1, x2], axis=1)\n    else: # Cases B and C\n        u = rng.standard_normal(n)\n        v = rng.standard_normal(n)\n        e1 = rng.standard_normal(n)\n        e2 = rng.standard_normal(n)\n        ey = rng.standard_normal(n)\n        x1 = u + 0.01 * e1\n        x2 = v + 0.01 * e2\n        y = u + 0.01 * ey\n        X = np.stack([x1, x2], axis=1)\n    \n    # --- Run without standardization ---\n    beta_unstd = coordinate_descent(X, y, lambda_val)\n    selected_unstd = np.where(np.abs(beta_unstd) > 1e-8)[0].tolist()\n\n    # --- Run with standardization ---\n    X_std = X.copy()\n    col_norms_sq = np.sum(X**2, axis=0)\n    col_norms_sq[col_norms_sq == 0] = 1.0 # Avoid division by zero\n    scalers = np.sqrt(n / col_norms_sq)\n    X_std = X_std * scalers\n\n    beta_std_scaled = coordinate_descent(X_std, y, lambda_val)\n    # Rescale coefficients back to original feature space\n    beta_std = beta_std_scaled * scalers\n    selected_std = np.where(np.abs(beta_std) > 1e-8)[0].tolist()\n    \n    are_different = selected_unstd != selected_std\n    \n    return [selected_unstd, selected_std, are_different]\n\ndef main():\n    results = []\n    # Case A\n    results.append(solve_case(n=60, p=2, lambda_val=200, seed=13, case_params={'c': 20}))\n    # Case B\n    results.append(solve_case(n=80, p=2, lambda_val=30, seed=7))\n    # Case C\n    results.append(solve_case(n=80, p=2, lambda_val=2000, seed=11))\n    \n    # The output format has to be exactly as requested, without spaces.\n    # Python's str() adds spaces, so we build it manually.\n    result_str = \"[\"\n    for i, res in enumerate(results):\n        sel_unstd, sel_std, diff = res\n        result_str += f\"[{sel_unstd},{sel_std},{'True' if diff else 'False'}]\"\n        if i  len(results) - 1:\n            result_str += \",\"\n    result_str += \"]\"\n    \n    print(result_str)\n\nif __name__ == \"__main__\":\n    main()\n```", "id": "3111928"}, {"introduction": "最后，我们深入研究一个当特征完全相关时，LASSO算法所展现出的一个更微妙且有趣的特性。这个练习将揭示，坐标下降法找到的具体解可能依赖于坐标更新的顺序，这一现象被称为“路径依赖”。你将通过实验验证，尽管不同的更新路径可能得到不同的系数向量，但它们都会达到相同的最优目标函数值和模型预测结果，从而突显了在这种情况下LASSO解的非唯一性。[@problem_id:3111866]", "problem": "您的任务是为最小绝对收缩和选择算子 (LASSO) 实现一个循环坐标下降算法，该算法最小化以下凸目标函数\n$$\n\\min_{\\boldsymbol{\\beta} \\in \\mathbb{R}^p} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1,\n$$\n其中，$n$ 是样本数量，$p$ 是特征数量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{y} \\in \\mathbb{R}^{n}$ 是响应向量，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是系数向量，$\\lambda \\in \\mathbb{R}_{\\ge 0}$ 是正则化参数，$\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数，$\\left\\| \\cdot \\right\\|_1$ 表示 $L_1$ 范数。从基本的凸优化原理和次梯度最优性条件出发，推导并实现一个坐标更新法则，该法则在保持其他坐标固定的同时，沿单个坐标最小化目标函数。您的实现必须：\n- 在遍历所有坐标的过程中，按照指定的坐标顺序执行循环更新。\n- 在每次坐标更新后，高效地维护和更新残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。\n- 当单次完整遍历中坐标的最大绝对变化小于一个容差（使用 $10^{-12}$）或达到最大遍历次数（使用 $100$）时终止。\n\n要研究的核心现象是当 $\\boldsymbol{X}$ 的两列完全相同时解的非唯一性。在这种情况下，目标函数仅取决于重复系数的和，并且在重复坐标间的不同分配可以达到相同的目标值。您必须通过实验证明坐标下降的路径依赖性和对称性破缺：改变坐标更新的顺序会导致重复特征的系数分配不同，尽管预测结果相同。\n\n实现一个程序，对每个测试用例运行两次坐标下降算法，使用两种不同的坐标顺序：首先是顺序 $[0,1]$，然后是顺序 $[1,0]$。计算并报告捕捉两次运行之间差异的量化指标。\n\n对所有测试用例使用以下固定数据：\n- 令 $n = 10$ 并定义向量 $\\boldsymbol{x} = [1,2,3,4,5,6,7,8,9,10]^\\top$。\n- 构造一个 $\\boldsymbol{X} \\in \\mathbb{R}^{10 \\times 2}$，使其两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。\n- 定义三个测试用例，它们的响应向量 $\\boldsymbol{y}$ 和正则化参数 $\\lambda$ 不同：\n  1. 测试用例 A (正常路径)：$\\boldsymbol{y} = 3 \\boldsymbol{x}$ 且 $\\lambda = 5$。\n  2. 测试用例 B (符号边界情况)：$\\boldsymbol{y} = -3 \\boldsymbol{x}$ 且 $\\lambda = 5$。\n  3. 测试用例 C (边界情况)：$\\boldsymbol{y} = 3 \\boldsymbol{x}$ 且 $\\lambda = 200$。\n\n初始化和停止条件：\n- 对于所有运行，将 $\\boldsymbol{\\beta}$ 初始化为零向量。\n- 按规定使用容差 $10^{-12}$ 和最大遍历次数 $100$。\n\n对于每个测试用例，运行算法两次（一次使用坐标更新顺序 $[0,1]$，一次使用 $[1,0]$），并计算以下四个指标：\n1. 两个系数向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{\\beta}^{(01)} - \\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，以浮点数形式表示。\n2. 重复系数之和的绝对差值，即 $ \\left| \\left( \\beta^{(01)}_0 + \\beta^{(01)}_1 \\right) - \\left( \\beta^{(10)}_0 + \\beta^{(10)}_1 \\right) \\right| $，以浮点数形式表示。\n3. 两个预测向量之差的欧几里得范数，即 $ \\left\\| \\boldsymbol{X}\\boldsymbol{\\beta}^{(01)} - \\boldsymbol{X}\\boldsymbol{\\beta}^{(10)} \\right\\|_2 $，以浮点数形式表示。\n4. 一个指示对称性破缺的布尔值，定义为在至少一次运行中重复系数是否不同，即是否满足 $ \\left| \\beta^{(01)}_0 - \\beta^{(01)}_1 \\right| > 10^{-9} $ 或 $ \\left| \\beta^{(10)}_0 - \\beta^{(10)}_1 \\right| > 10^{-9} $。\n\n您的程序应生成单行输出，其中包含三个测试用例的结果，形式为用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是按上述顺序排列的四个指标的列表。例如，输出必须具有以下形式\n$$\n[\\,[m_{A,1},m_{A,2},m_{A,3},b_A],\\,[m_{B,1},m_{B,2},m_{B,3},b_B],\\,[m_{C,1},m_{C,2},m_{C,3},b_C]\\,],\n$$\n其中 $m_{\\cdot,\\cdot}$ 是浮点数，$b_{\\cdot}$ 是布尔值。不涉及物理单位或角度；所有量都是无量纲的实数。通过遵循目标函数的凸性和基于次梯度的坐标最小化定义，确保科学真实性，不要依赖临时的启发式方法。", "solution": "用户提供的问题是有效的。这是一个定义明确且具有科学依据的计算统计学任务，特别关注用于 LASSO 回归的坐标下降算法。所有必要的参数和条件都已提供，目标是探索在存在共线性特征时解的路径依赖这一已知现象，这是优化中的一个标准课题。\n\n### 1. LASSO 目标函数\n\n问题要求最小化 LASSO 目标函数，该函数由一个最小二乘数据保真项和一个 $L_1$ 范数正则化项组成。对于系数向量 $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$，目标函数 $L(\\boldsymbol{\\beta})$ 由下式给出：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta} \\right\\|_2^2 + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_1\n$$\n这里，$\\boldsymbol{y} \\in \\mathbb{R}^n$ 是响应向量，$\\boldsymbol{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$n$ 是样本数，$p$ 是特征数，$\\lambda \\ge 0$ 是正则化参数。$L_1$ 范数定义为 $\\left\\| \\boldsymbol{\\beta} \\right\\|_1 = \\sum_{j=1}^p |\\beta_j|$。此目标函数是凸函数，这保证了全局最小值的存在。\n\n### 2. 坐标更新法则的推导\n\n坐标下降法一次只对一个坐标进行优化，同时保持所有其他坐标固定。为了推导第 $k$ 个系数 $\\beta_k$ 的更新法则，我们视所有其他系数 $\\beta_j$（对于 $j \\neq k$）为常数。\n\n目标函数可以写成如下形式，以分离出包含 $\\beta_k$ 的项：\n$$\nL(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\left\\| \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda \\sum_{j \\neq k} |\\beta_j| + \\lambda |\\beta_k|\n$$\n我们定义偏残差 $\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j$。这是如果从模型中移除特征 $k$ 时的残差。不涉及 $\\beta_k$ 的项对于 $\\beta_k$ 的最小化是常数。因此，需要为 $\\beta_k$ 最小化的目标是：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left\\| \\boldsymbol{r}_k - \\boldsymbol{X}_{:,k}\\beta_k \\right\\|_2^2 + \\lambda |\\beta_k|\n$$\n展开二次项得到：\n$$\nL_k(\\beta_k) = \\frac{1}{2n} \\left( \\boldsymbol{r}_k^\\top \\boldsymbol{r}_k - 2\\beta_k \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k^2 \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k} \\right) + \\lambda |\\beta_k|\n$$\n为了找到这个凸函数的最小值，我们使用次梯度最优性条件，该条件表明在最小值点，$0$ 必须在 $L_k(\\beta_k)$ 的次梯度中。绝对值函数 $|\\cdot|$ 的次梯度是符号函数 `sgn`，它在 $0$ 点是一个集值函数。\n$$\n\\frac{\\partial L_k(\\beta_k)}{\\partial \\beta_k} = \\frac{1}{n} \\left( -\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k + \\beta_k \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) \\right) + \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n令次梯度包含 $0$：\n$$\n0 \\in \\frac{1}{n} \\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k - \\beta_k \\frac{1}{n} \\left(\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}\\right) - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n我们定义 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{r}_k$ 和 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top \\boldsymbol{X}_{:,k}$。条件变为：\n$$\n\\beta_k z_k \\in \\rho_k - \\lambda \\cdot \\text{sgn}(\\beta_k)\n$$\n这导出了软阈值函数 $S(a, \\delta) = \\text{sgn}(a)\\max(|a|-\\delta, 0)$。$\\beta_k$ 的解是：\n$$\n\\beta_k = \\frac{S(\\rho_k, \\lambda)}{z_k}\n$$\n这个更新法则沿着第 $k$ 个坐标最小化目标函数。\n\n### 3. 坐标下降算法与高效更新\n\n循环坐标下降算法重复遍历所有坐标 $k=0, 1, \\dots, p-1$ 直到收敛。为了计算效率，我们避免在每一步重新计算偏残差 $\\boldsymbol{r}_k$。取而代之的是，我们维护全残差 $\\boldsymbol{r} = \\boldsymbol{y} - \\boldsymbol{X}\\boldsymbol{\\beta}$。偏残差可以用全残差和更新前 $\\beta_k$ 的当前值来表示：\n$$\n\\boldsymbol{r}_k = \\boldsymbol{y} - \\sum_{j \\neq k} \\boldsymbol{X}_{:,j}\\beta_j = \\left(\\boldsymbol{y} - \\sum_{j=1}^p \\boldsymbol{X}_{:,j}\\beta_j \\right) + \\boldsymbol{X}_{:,k}\\beta_k = \\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k\n$$\n因此，项 $\\rho_k$ 可以计算为 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k)$，其中 $\\boldsymbol{r}$ 和 $\\beta_k$ 是坐标 $k$ 更新前的值。\n\n将 $\\beta_k$ 从 $\\beta_k^{\\text{old}}$ 更新到 $\\beta_k^{\\text{new}}$ 后，全残差 $\\boldsymbol{r}$ 也必须更新。预测值的变化是 $\\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。新的残差是：\n$$\n\\boldsymbol{r}^{\\text{new}} = \\boldsymbol{r}^{\\text{old}} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})\n$$\n这是一个高效的 $O(n)$ 更新。算法如下：\n\n1.  初始化 $\\boldsymbol{\\beta} = \\boldsymbol{0}$ 和 $\\boldsymbol{r} = \\boldsymbol{y}$。对所有 $k$ 预先计算 $z_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top\\boldsymbol{X}_{:,k}$。\n2.  对于每次遍历，直到达到最大遍历次数：\n    a. 初始化 `max_abs_change` 为 $0$。\n    b. 对于指定顺序中的每个坐标 $k$：\n        i.   存储 $\\beta_k^{\\text{old}} = \\beta_k$。\n        ii.  计算 $\\rho_k = \\frac{1}{n}\\boldsymbol{X}_{:,k}^\\top(\\boldsymbol{r} + \\boldsymbol{X}_{:,k}\\beta_k^{\\text{old}})$。\n        iii. 计算 $\\beta_k^{\\text{new}} = S(\\rho_k, \\lambda) / z_k$。\n        iv.  更新残差：$\\boldsymbol{r} \\leftarrow \\boldsymbol{r} - \\boldsymbol{X}_{:,k}(\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}})$。\n        v.   更新系数：$\\beta_k \\leftarrow \\beta_k^{\\text{new}}$。\n        vi.  用 $|\\beta_k^{\\text{new}} - \\beta_k^{\\text{old}}|$ 更新 `max_abs_change`。\n    c. 如果 `max_abs_change` 小于容差（例如，$10^{-12}$），则终止。\n3.  返回最终的系数向量 $\\boldsymbol{\\beta}$。\n\n### 4. 相同列导致的非唯一性\n\n问题设定了一个场景，其中 $\\boldsymbol{X}$ 的两列相同，即 $\\boldsymbol{X}_{:,0} = \\boldsymbol{X}_{:,1} = \\boldsymbol{x}$。在这种情况下，模型预测为：\n$$\n\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X}_{:,0}\\beta_0 + \\boldsymbol{X}_{:,1}\\beta_1 = \\boldsymbol{x}\\beta_0 + \\boldsymbol{x}\\beta_1 = (\\beta_0 + \\beta_1)\\boldsymbol{x}\n$$\n目标函数的数据保真项仅取决于系数之和 $\\beta_{\\text{sum}} = \\beta_0 + \\beta_1$。$L_1$ 惩罚项是 $\\lambda(|\\beta_0| + |\\beta_1|)$。优化问题变为：\n$$\n\\min_{\\beta_0, \\beta_1} \\; \\frac{1}{2n} \\left\\| \\boldsymbol{y} - (\\beta_0 + \\beta_1)\\boldsymbol{x} \\right\\|_2^2 + \\lambda(|\\beta_0| + |\\beta_1|)\n$$\n虽然和 $\\beta_{\\text{sum}}$ 的最优值是唯一的，但可以有无穷多对 $(\\beta_0, \\beta_1)$ 产生这个和。对于任何给定的最优 $\\beta_{\\text{sum}}$，假设 $\\beta_{\\text{sum}} \\neq 0$，当 $\\beta_{\\text{sum}}$ 的全部大小被分配给单个系数时（例如，$\\beta_0 = \\beta_{\\text{sum}}, \\beta_1 = 0$ 或反之亦然），$L_1$ 惩罚项 $\\lambda(|\\beta_0| + |\\beta_1|)$ 被最小化。\n\n循环坐标下降算法根据更新顺序打破了这种对称性。当首先更新坐标 $0$ 时，它将尽可能多地吸收信号，为坐标 $1$ 留下一个很小的残差。如果残差信号低于由 $\\lambda$ 设定的阈值，$\\beta_1$ 的更新将为零。将顺序反转为 $[1, 0]$ 将导致 $\\beta_1$ 首先吸收信号，从而得到一个不同的最终系数向量 $\\boldsymbol{\\beta}$，尽管和 $\\beta_0+\\beta_1$ 以及预测值 $\\boldsymbol{X}\\boldsymbol{\\beta}$ 将是相同的（在数值精度范围内）。这种路径依赖性是坐标下降法在非严格凸目标函数上的一个关键特征。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the coordinate descent for LASSO to demonstrate path dependence.\n    \"\"\"\n\n    def coordinate_descent(X, y, lambda_val, order, tol, max_passes):\n        \"\"\"\n        Performs cyclic coordinate descent for LASSO.\n        \"\"\"\n        n, p = X.shape\n        beta = np.zeros(p)\n        residual = y.copy()\n\n        # Precompute column normalization factors\n        z = np.array([(1/n) * X[:, k].T @ X[:, k] for k in range(p)])\n\n        def soft_threshold(a, delta):\n            return np.sign(a) * np.maximum(np.abs(a) - delta, 0)\n\n        for _ in range(max_passes):\n            max_abs_change = 0.0\n            \n            for k in order:\n                beta_old_k = beta[k]\n                \n                # Calculate rho_k using the efficient residual update formula\n                # rho_k = (1/n) * X_k^T * (y - sum_{j!=k} X_j * beta_j)\n                # which is equivalent to (1/n) * X_k^T * (residual + X_k * beta_old_k)\n                rho_k = (1/n) * X[:, k].T @ (residual + X[:, k] * beta_old_k)\n\n                # Update beta_k using soft-thresholding\n                # Check for z[k] being zero to avoid division by zero\n                if z[k] > 0:\n                    beta[k] = soft_threshold(rho_k, lambda_val) / z[k]\n                else:\n                    beta[k] = 0.0\n                \n                delta_beta_k = beta[k] - beta_old_k\n                \n                if delta_beta_k != 0:\n                    # Update residual efficiently\n                    residual -= X[:, k] * delta_beta_k\n\n                max_abs_change = max(max_abs_change, np.abs(delta_beta_k))\n\n            if max_abs_change  tol:\n                break\n                \n        return beta\n\n    # --- Fixed Data ---\n    n = 10\n    x_vec = np.arange(1, 11, dtype=float)\n    X = np.stack([x_vec, x_vec], axis=1)\n    \n    # --- Algorithm Parameters ---\n    tol = 1e-12\n    max_passes = 100\n    sym_tol = 1e-9\n\n    # --- Test Cases ---\n    test_cases_params = [\n        # Case A: happy path\n        {'y': 3 * x_vec, 'lambda': 5.0},\n        # Case B: sign edge case\n        {'y': -3 * x_vec, 'lambda': 5.0},\n        # Case C: boundary case (high regularization)\n        {'y': 3 * x_vec, 'lambda': 200.0},\n    ]\n\n    results = []\n    \n    for params in test_cases_params:\n        y = params['y']\n        lambda_val = params['lambda']\n\n        # Run with order [0, 1]\n        beta_01 = coordinate_descent(X, y, lambda_val, order=[0, 1], tol=tol, max_passes=max_passes)\n        \n        # Run with order [1, 0]\n        beta_10 = coordinate_descent(X, y, lambda_val, order=[1, 0], tol=tol, max_passes=max_passes)\n\n        # --- Compute Metrics ---\n        # 1. Euclidean norm of the difference between the two coefficient vectors\n        metric1 = np.linalg.norm(beta_01 - beta_10)\n\n        # 2. Absolute difference in the sum of the duplicate coefficients\n        metric2 = np.abs(np.sum(beta_01) - np.sum(beta_10))\n\n        # 3. Euclidean norm of the difference between the two prediction vectors\n        pred_01 = X @ beta_01\n        pred_10 = X @ beta_10\n        metric3 = np.linalg.norm(pred_01 - pred_10)\n\n        # 4. Boolean indicating symmetry-breaking\n        m4_cond1 = np.abs(beta_01[0] - beta_01[1]) > sym_tol\n        m4_cond2 = np.abs(beta_10[0] - beta_10[1]) > sym_tol\n        metric4 = m4_cond1 or m4_cond2\n\n        results.append([metric1, metric2, metric3, bool(metric4)])\n\n    # Format output as specified. Using str() is fine, but needs to match format exactly.\n    # A manual construction is more robust.\n    final_output = str(results).replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n```", "id": "3111866"}]}