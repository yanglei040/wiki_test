{"hands_on_practices": [{"introduction": "在高维数据中，首先想到的一个简单方法是逐个筛选特征。独立确信筛选（Sure Independence Screening, SIS）正是基于这一思想，它通过计算每个特征与响应变量的边际相关性来进行快速降维。这个练习将引导你通过构建特定的数据场景，亲手验证SIS在理想情况下的有效性，并揭示其在特征相关时可能出现的严重筛选错误，从而深刻理解边际方法的局限性。[@problem_id:3186687]", "problem": "您需要编写一个完整的程序，用于在小样本大特征（即特征数量远超样本数量，$p \\gg n$）的情景下，评估确定独立性筛选（Sure Independence Screening, SIS）的筛选性能。工作环境为经典线性模型，响应为 $y \\in \\mathbb{R}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $y = X \\beta + \\varepsilon$。SIS 规则通过特征与响应之间样本皮尔逊相关系数的绝对值对特征进行排序，并保留前 $k$ 个特征。给定一组构造的测试用例，您的程序必须为每个用例确定 SIS 是否成功地在前 $k$ 个特征中保留了所有真正活跃的特征。\n\n使用的基本原理：\n- 线性回归设置 $y = X \\beta + \\varepsilon$。\n- 列 $X_{j}$ 与 $y$ 之间的样本皮尔逊相关系数定义为\n$$\n\\mathrm{corr}(X_{j}, y) \\;=\\; \\frac{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_{j})(y_{i} - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_{j})^{2}} \\; \\sqrt{\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}} }.\n$$\n特征 $j$ 的 SIS 分数是 $|\\mathrm{corr}(X_{j}, y)|$。\n\n您的程序必须为指定的测试套件实现以下评估协议。对每个测试用例：\n1. 完全按照下述规定构造 $X$ 和 $y$，确保在计算样本相关性时，所有列和响应都经过均值中心化处理。\n2. 计算所有 $j \\in \\{0,1,\\dots,p-1\\}$ 的绝对样本相关性 $|\\mathrm{corr}(X_{j}, y)|$。\n3. 按分数降序选择前 $k$ 个特征的索引，按列索引递增的顺序打破平局（即，一个稳定的排序）。\n4. 报告一个布尔值，指示是否每个真正活跃的索引都出现在这前 $k$ 个索引中。\n\n设计构造和测试套件：\n以下所有构造都是确定性的线性代数过程。在每种情况下，当一个步骤需要在 $\\mathbb{R}^{n}$ 中抽取一个随机向量时，生成一个具有独立标准正态分布条目的向量，然后对其进行均值中心化处理；当需要进行标准正交化时，对先前构造的向量使用 Gram–Schmidt 方法，以获得单位范数、相互正交的向量。对于“投影到 $y$ 的正交补空间上”，将向量 $v$ 替换为 $v - \\frac{\\langle v, y \\rangle}{\\langle y, y \\rangle} y$。\n\n- 测试用例 1（独立，理想情况）：\n  - 参数：$n = 60$， $p = 200$，活跃特征数 $s = 3$，顶部选择大小 $k = 3$。\n  - 构造 $s$ 个标准正交列向量 $u_{1}, u_{2}, u_{3} \\in \\mathbb{R}^{n}$，并设置活跃集 $S = \\{10, 50, 150\\}$。定义系数 $\\beta_{\\text{active}} = [1.0, -1.5, 0.5]$。设置 $y = \\sum_{\\ell=1}^{3} \\beta_{\\text{active},\\ell} u_{\\ell}$，无噪声。将 $u_{1}, u_{2}, u_{3}$ 分别作为 $X$ 在索引 $S$ 处的列。对于每个其他列索引 $j \\notin S$，构造一个随机的均值中心化向量 $v_{j}$，并将其投影到 $y$ 的正交补空间上，使得 $\\mathrm{corr}(v_{j}, y) = 0$，然后将其作为列 $X_{j}$。\n  - 预期行为：只有活跃列与 $y$ 具有非零相关性，因此当 $k = 3$ 时，SIS 应包含所有活跃索引。\n\n- 测试用例 2（相关抵消与影子变量；失败模式）：\n  - 参数：$n = 40$， $p = 300$， $k = 2$，相关性调整参数 $\\epsilon = 0.1$。\n  - 构造两个标准正交向量 $a, d \\in \\mathbb{R}^{n}$。定义活跃列 $X_{5} = a$ 和 $X_{25} = a + \\epsilon d$。设置 $y = X_{5} - X_{25} = -\\epsilon d$（无噪声）。引入一个影子变量 $X_{0} = y$。对于每个其他列索引 $j \\notin \\{0,5,25\\}$，构造一个随机的均值中心化向量，并将其投影到 $y$ 的正交补空间上，然后将其作为 $X_{j}$。\n  - 预期行为：$|\\mathrm{corr}(X_{0}, y)| = 1$， $|\\mathrm{corr}(X_{25}, y)| = \\frac{\\epsilon}{\\sqrt{1 + \\epsilon^{2}}}$，且 $|\\mathrm{corr}(X_{5}, y)| = 0$，而所有其他列的相关性为零。当 $k = 2$ 时，SIS 选择 $\\{0, 25\\}$ 并错过了索引 5，因此 SIS 未能保留所有活跃特征。\n\n- 测试用例 3（相关同号信号与影子变量；成功模式）：\n  - 参数：$n = 50$， $p = 300$， $k = 3$，相关性调整参数 $\\epsilon = 0.5$。\n  - 构造两个标准正交向量 $a, d \\in \\mathbb{R}^{n}$。定义活跃列 $X_{10} = a$ 和 $X_{20} = a + \\epsilon d$。设置 $y = X_{10} + X_{20}$（无噪声）。引入一个影子变量 $X_{0} = y$。对于每个其他列索引 $j \\notin \\{0,10,20\\}$，构造一个随机的均值中心化向量，并将其投影到 $y$ 的正交补空间上，然后将其作为 $X_{j}$。\n  - 预期行为：$|\\mathrm{corr}(X_{0}, y)| = 1$，且两个活跃列都与 $y$ 有大的正相关性。当 $k = 3$ 时，SIS 应该在所选集合中包含两个活跃索引。\n\n- 测试用例 4（重复边缘情况；在最小 $k$ 值时失败）：\n  - 参数：$n = 30$， $p = 1000$， $k = 1$。\n  - 构造一个均值中心化、单位范数的向量 $a \\in \\mathbb{R}^{n}$。在索引 50 处定义真正的活跃列 $X_{50} = a$，并设置 $y = X_{50}$（无噪声）。定义一个重复列 $X_{0} = X_{50}$。对于每个其他列索引 $j \\notin \\{0,50\\}$，构造一个随机的均值中心化向量，并将其投影到 $y$ 的正交补空间上，然后将其作为 $X_{j}$。\n  - 预期行为：$X_{0}$ 和 $X_{50}$ 与 $y$ 的相关性大小都为 $1$。当 $k=1$ 且通过递增索引打破平局时，SIS 选择索引 0 并排除了真正的活跃索引 50，因此 SIS 失败。\n\n对于每个用例，将“筛选成功”定义为一个布尔值，当且仅当活跃索引集 $S$ 是所选的前 $k$ 个索引的子集时，该值为真。您的程序必须输出一行，其中包含上述用例的四个布尔结果，格式为用方括号括起来的逗号分隔列表，例如 `[True,False,True,False]`。不需要用户输入，也不得使用外部数据。所有量都是无单位的，必须完全按照规定计算。", "solution": "该问题要求在四种指定测试用例下，评估确定独立性筛选（SIS）方法在一个高维线性回归设置（$p \\gg n$）中的表现。任务的核心是根据确定性的线性代数过程为每个案例构造数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，然后确定 SIS 是否成功识别出所有真正活跃的特征。\n\n首先，我们将问题形式化。线性模型为 $y = X\\beta + \\varepsilon$。SIS 通过特征与响应 $y$ 之间样本皮尔逊相关系数的大小来对特征进行排序。特征 $j$ 的样本相关系数为：\n$$\n\\mathrm{corr}(X_j, y) = \\frac{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_j)(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (X_{ij} - \\bar{X}_j)^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n$$\n其中 $X_j$ 是 $X$ 的第 $j$ 列。由于构造中的所有向量都指定为均值中心化的（即 $\\bar{X}_j = 0$ 和 $\\bar{y} = 0$），该公式简化为余弦相似度：\n$$\n\\mathrm{corr}(X_j, y) = \\frac{\\langle X_j, y \\rangle}{\\|X_j\\|_2 \\|y\\|_2}\n$$\n特征 $j$ 的 SIS 分数是 $|\\mathrm{corr}(X_j, y)|$。在计算所有 $p$ 个特征的分数后，选择前 $k$ 个特征。分数的平局通过选择列索引较小的特征来打破。筛选成功被定义为真正活跃特征索引的集合 $S$ 是所选前 $k$ 个索引的子集的事件。\n\n评估通过分析每个测试用例来进行。\n\n**测试用例 1：独立，理想情况**\n- 参数：$n = 60$，$p = 200$，活跃特征数 $s = 3$，顶部选择大小 $k = 3$。\n- 活跃集 $S = \\{10, 50, 150\\}$，系数 $\\beta_{\\text{active}} = [1.0, -1.5, 0.5]$。\n- 构造：我们构造三个标准正交、均值中心化的向量 $u_1, u_2, u_3 \\in \\mathbb{R}^{n}$。响应为 $y = 1.0 u_1 - 1.5 u_2 + 0.5 u_3$。$X$ 的活跃列设置为 $X_{10} = u_1$，$X_{50} = u_2$ 和 $X_{150} = u_3$。所有其他列 $X_j$（$j \\notin S$）被构造成与 $y$ 正交，这意味着 $\\langle X_j, y \\rangle = 0$，因此 $\\mathrm{corr}(X_j, y) = 0$。\n- 相关性分析：\n    - $y$ 的范数是 $\\|y\\|_2 = \\sqrt{1.0^2 + (-1.5)^2 + 0.5^2} = \\sqrt{1 + 2.25 + 0.25} = \\sqrt{3.5}$。\n    - 对于活跃特征 10：$\\mathrm{corr}(X_{10}, y) = \\frac{\\langle u_1, y \\rangle}{\\|u_1\\|_2 \\|y\\|_2} = \\frac{1.0}{\\sqrt{3.5}}$。\n    - 对于活跃特征 50：$\\mathrm{corr}(X_{50}, y) = \\frac{\\langle u_2, y \\rangle}{\\|u_2\\|_2 \\|y\\|_2} = \\frac{-1.5}{\\sqrt{3.5}}$。\n    - 对于活跃特征 150：$\\mathrm{corr}(X_{150}, y) = \\frac{\\langle u_3, y \\rangle}{\\|u_3\\|_2 \\|y\\|_2} = \\frac{0.5}{\\sqrt{3.5}}$。\n- 结果：活跃特征的绝对相关性为 $\\frac{1.5}{\\sqrt{3.5}}$、$\\frac{1.0}{\\sqrt{3.5}}$ 和 $\\frac{0.5}{\\sqrt{3.5}}$。它们都非零。所有其他特征的相关性为 $0$。因此，这三个活跃特征将被排在前 3 位。当 $k=3$ 时，SIS 恰好选择活跃特征集 $S=\\{10, 50, 150\\}$。条件 $S \\subseteq \\{10, 50, 150\\}$ 被满足。\n- 结论：**True**\n\n**测试用例 2：相关抵消与影子变量**\n- 参数：$n = 40$，$p = 300$，$k = 2$，$\\epsilon = 0.1$。\n- 活跃集 $S = \\{5, 25\\}$。\n- 构造：我们构造两个标准正交、均值中心化的向量 $a, d \\in \\mathbb{R}^{n}$。活跃列为 $X_5 = a$ 和 $X_{25} = a + \\epsilon d$。响应为 $y = X_5 - X_{25} = -\\epsilon d$。在索引 0 处引入一个“影子”变量 $X_0 = y$。所有其他列与 $y$ 的相关性为零。\n- 相关性分析：\n    - 对于影子特征 0：$X_0=y$，所以 $\\mathrm{corr}(X_0,y)=1$。\n    - 对于活跃特征 5：$\\mathrm{corr}(X_5, y) = \\frac{\\langle a, -\\epsilon d \\rangle}{\\|a\\|_2 \\|-\\epsilon d\\|_2} = \\frac{-\\epsilon \\langle a, d \\rangle}{\\epsilon \\|d\\|_2} = 0$，因为 $\\langle a, d \\rangle = 0$。\n    - 对于活跃特征 25：$\\mathrm{corr}(X_{25}, y) = \\frac{\\langle a+\\epsilon d, -\\epsilon d \\rangle}{\\|a+\\epsilon d\\|_2 \\|-\\epsilon d\\|_2} = \\frac{-\\epsilon^2 \\langle d,d \\rangle}{\\sqrt{\\|a\\|^2+\\epsilon^2\\|d\\|^2} (\\epsilon\\|d\\|_2)} = \\frac{-\\epsilon^2}{\\sqrt{1+\\epsilon^2} \\cdot \\epsilon} = \\frac{-\\epsilon}{\\sqrt{1+\\epsilon^2}}$。\n- 结果：分数为 $|\\mathrm{corr}(X_0,y)| = 1$、$|\\mathrm{corr}(X_5,y)| = 0$ 和 $|\\mathrm{corr}(X_{25},y)| = \\frac{\\epsilon}{\\sqrt{1+\\epsilon^2}}$。对于 $\\epsilon = 0.1$，索引 25 的分数为 $\\frac{0.1}{\\sqrt{1.01}} \\approx 0.0995$。所有其他特征的相关性为零。最高分属于索引 0 和 25。当 $k=2$ 时，SIS 选择集合 $\\{0, 25\\}$。真正的活跃集是 $S=\\{5, 25\\}$。条件 $S \\subseteq \\{0, 25\\}$ 不被满足，因为 $5 \\notin \\{0, 25\\}$。\n- 结论：**False**\n\n**测试用例 3：相关同号信号与影子变量**\n- 参数：$n = 50$，$p = 300$，$k = 3$，$\\epsilon = 0.5$。\n- 活跃集 $S = \\{10, 20\\}$。\n- 构造：我们使用两个标准正交、均值中心化的向量 $a, d \\in \\mathbb{R}^{n}$。活跃列为 $X_{10} = a$ 和 $X_{20} = a + \\epsilon d$。响应为 $y = X_{10} + X_{20} = 2a + \\epsilon d$。一个影子变量为 $X_0 = y$。其他列的相关性为零。\n- 相关性分析：\n    - 对于影子特征 0：$X_0=y$，所以 $\\mathrm{corr}(X_0,y)=1$。\n    - 对于活跃特征 10：$\\mathrm{corr}(X_{10}, y) = \\frac{\\langle a, 2a + \\epsilon d \\rangle}{\\|a\\|_2 \\|2a + \\epsilon d\\|_2} = \\frac{2}{\\sqrt{4+\\epsilon^2}}$。\n    - 对于活跃特征 20：$\\mathrm{corr}(X_{20}, y) = \\frac{\\langle a+\\epsilon d, 2a + \\epsilon d \\rangle}{\\|a+\\epsilon d\\|_2 \\|2a + \\epsilon d\\|_2} = \\frac{2+\\epsilon^2}{\\sqrt{1+\\epsilon^2}\\sqrt{4+\\epsilon^2}}$。\n- 结果：当 $\\epsilon=0.5$ 时：\n    - $|\\mathrm{corr}(X_0,y)| = 1$。\n    - $|\\mathrm{corr}(X_{10},y)| = \\frac{2}{\\sqrt{4+0.25}} = \\frac{2}{\\sqrt{4.25}} \\approx 0.970$。\n    - $|\\mathrm{corr}(X_{20},y)| = \\frac{2+0.25}{\\sqrt{1+0.25}\\sqrt{4+0.25}} = \\frac{2.25}{\\sqrt{1.25}\\sqrt{4.25}} \\approx 0.976$。\n所有其他特征的相关性为零。得分最高的三个特征是 0、10 和 20。当 $k=3$ 时，SIS 选择 $\\{0, 10, 20\\}$。真正的活跃集是 $S=\\{10, 20\\}$。条件 $S \\subseteq \\{0, 10, 20\\}$ 被满足。\n- 结论：**True**\n\n**测试用例 4：重复边缘情况**\n- 参数：$n = 30$，$p = 1000$，$k = 1$。\n- 活跃集 $S = \\{50\\}$。\n- 构造：我们使用一个均值中心化、单位范数的向量 $a \\in \\mathbb{R}^{n}$。活跃列是 $X_{50} = a$，响应为 $y = X_{50} = a$。在索引 0 处引入一个重复列：$X_0 = X_{50} = a$。其他列的相关性为零。\n- 相关性分析：\n    - 对于活跃特征 50：$\\mathrm{corr}(X_{50}, y) = \\frac{\\langle a, a \\rangle}{\\|a\\|_2 \\|a\\|_2} = 1$。\n    - 对于重复特征 0：$\\mathrm{corr}(X_0, y) = \\frac{\\langle a, a \\rangle}{\\|a\\|_2 \\|a\\|_2} = 1$。\n- 结果：特征 0 和 50 的最高分都为 1。所有其他特征的分数为 0。这会产生一个平局。问题规定，平局通过递增的列索引来打破。因此，特征 0 的排名高于特征 50。当 $k=1$ 时，SIS 仅选择集合 $\\{0\\}$。真正的活跃集是 $S=\\{50\\}$。条件 $S \\subseteq \\{0\\}$ 不被满足。\n- 结论：**False**", "answer": "```python\nimport numpy as np\n\ndef _mean_center(v):\n    \"\"\"Mean-centers a vector.\"\"\"\n    return v - np.mean(v)\n\ndef _gram_schmidt_process(vectors):\n    \"\"\"\n    Performs modified Gram-Schmidt orthonormalization on a list of vectors.\n    Assumes vectors are columns of a matrix. Returns a list of orthonormal vectors.\n    \"\"\"\n    basis = []\n    for v in vectors:\n        v_orth = v.copy()\n        # Subtract projection on previous basis vectors\n        for u in basis:\n            v_orth -= np.dot(v_orth, u) * u\n        \n        norm = np.linalg.norm(v_orth)\n        if norm  1e-12:\n            basis.append(v_orth / norm)\n    return basis\n\ndef _run_sis_test(X, y, k, active_indices):\n    \"\"\"\n    Performs Sure Independence Screening and checks for success.\n    \n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n        k (int): Number of features to select.\n        active_indices (set): Set of true active feature indices.\n        \n    Returns:\n        bool: True if all active indices are selected, False otherwise.\n    \"\"\"\n    n, p = X.shape\n    \n    # Per the problem, X and y are constructed from mean-centered vectors.\n    # To be robust, a full Pearson correlation calculation is implemented.\n    y_centered = y - np.mean(y)\n    y_norm = np.linalg.norm(y_centered)\n    \n    scores = np.zeros(p)\n    for j in range(p):\n        col = X[:, j]\n        col_centered = col - np.mean(col)\n        col_norm = np.linalg.norm(col_centered)\n        \n        if col_norm  1e-12 and y_norm  1e-12:\n            corr = np.dot(col_centered, y_centered) / (col_norm * y_norm)\n            scores[j] = abs(corr)\n        else:\n            scores[j] = 0.0\n\n    # Tie-breaking: sort by decreasing score, then increasing index.\n    indices = np.arange(p)\n    # np.lexsort sorts by the last key first.\n    # So, (indices, -scores) sorts by -scores, then by indices for ties.\n    sorted_indices = np.lexsort((indices, -scores))\n    top_k_indices = sorted_indices[:k]\n    \n    # Check if all active indices are in the top k\n    success = set(active_indices).issubset(set(top_k_indices))\n    return success\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # A fixed seed ensures reproducibility for the \"random\" vector constructions.\n    rng = np.random.default_rng(0)\n    results = []\n\n    # Test Case 1: independent, happy path\n    n1, p1, s1, k1 = 60, 200, 3, 3\n    active_indices1 = {10, 50, 150}\n    beta_active1 = np.array([1.0, -1.5, 0.5])\n    \n    initial_vectors1 = [ _mean_center(rng.standard_normal(n1)) for _ in range(s1)]\n    basis1 = _gram_schmidt_process(initial_vectors1)\n    u1, u2, u3 = basis1[0], basis1[1], basis1[2]\n    \n    y1 = beta_active1[0] * u1 + beta_active1[1] * u2 + beta_active1[2] * u3\n    \n    X1 = np.zeros((n1, p1))\n    X1[:, 10] = u1\n    X1[:, 50] = u2\n    X1[:, 150] = u3\n    \n    y1_norm_sq = np.dot(y1, y1)\n    for j in range(p1):\n        if j not in active_indices1:\n            vj = _mean_center(rng.standard_normal(n1))\n            X1[:, j] = vj - (np.dot(vj, y1) / y1_norm_sq) * y1\n    results.append(_run_sis_test(X1, y1, k1, active_indices1))\n\n    # Test Case 2: correlated cancellation\n    n2, p2, k2, epsilon2 = 40, 300, 2, 0.1\n    active_indices2 = {5, 25}\n    \n    initial_vectors2 = [_mean_center(rng.standard_normal(n2)) for _ in range(2)]\n    basis2 = _gram_schmidt_process(initial_vectors2)\n    a2, d2 = basis2[0], basis2[1]\n    \n    X2 = np.zeros((n2, p2))\n    X5 = a2\n    X25 = a2 + epsilon2 * d2\n    y2 = X5 - X25\n    \n    X2[:, 5] = X5\n    X2[:, 25] = X25\n    X2[:, 0] = y2\n    \n    y2_norm_sq = np.dot(y2, y2)\n    for j in range(p2):\n        if j not in {0, 5, 25}:\n            vj = _mean_center(rng.standard_normal(n2))\n            X2[:, j] = vj - (np.dot(vj, y2) / y2_norm_sq) * y2\n    results.append(_run_sis_test(X2, y2, k2, active_indices2))\n\n    # Test Case 3: correlated, same-sign signals\n    n3, p3, k3, epsilon3 = 50, 300, 3, 0.5\n    active_indices3 = {10, 20}\n    \n    initial_vectors3 = [_mean_center(rng.standard_normal(n3)) for _ in range(2)]\n    basis3 = _gram_schmidt_process(initial_vectors3)\n    a3, d3 = basis3[0], basis3[1]\n\n    X3 = np.zeros((n3, p3))\n    X10 = a3\n    X20 = a3 + epsilon3 * d3\n    y3 = X10 + X20\n\n    X3[:, 10] = X10\n    X3[:, 20] = X20\n    X3[:, 0] = y3\n\n    y3_norm_sq = np.dot(y3, y3)\n    for j in range(p3):\n        if j not in {0, 10, 20}:\n            vj = _mean_center(rng.standard_normal(n3))\n            X3[:, j] = vj - (np.dot(vj, y3) / y3_norm_sq) * y3\n    results.append(_run_sis_test(X3, y3, k3, active_indices3))\n\n    # Test Case 4: duplication edge case\n    n4, p4, k4 = 30, 1000, 1\n    active_indices4 = {50}\n    \n    initial_vector4 = _mean_center(rng.standard_normal(n4))\n    a4 = initial_vector4 / np.linalg.norm(initial_vector4)\n    \n    X4 = np.zeros((n4, p4))\n    X50 = a4\n    y4 = X50\n    \n    X4[:, 50] = X50\n    X4[:, 0] = X50\n    \n    y4_norm_sq = np.dot(y4, y4)\n    if y4_norm_sq  1e-12:\n        for j in range(p4):\n            if j not in {0, 50}:\n                vj = _mean_center(rng.standard_normal(n4))\n                X4[:, j] = vj - (np.dot(vj, y4) / y4_norm_sq) * y4\n    results.append(_run_sis_test(X4, y4, k4, active_indices4))\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3186687"}, {"introduction": "在了解了边际筛选方法的不足之后，我们转向更强大的正则化方法，如LASSO。LASSO因其能够同时进行系数收缩和变量选择而备受青睐，但它也并非万能。这个练习将带你从LASSO的优化理论（KKT条件）出发，推导并实现一个关键概念——“不可分条件”（Irrepresentable Condition），它精确地刻画了LASSO能否成功恢复真实特征集。通过亲手实现，你将明白为何特征间的高度相关性是LASSO在高维领域面临的核心挑战。[@problem_id:3186678]", "problem": "考虑线性模型 $y = X \\beta + \\varepsilon$，其处于 $p \\gg n$ 的小样本大维度情景下，其中 $X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^{p}$，$\\varepsilon \\in \\mathbb{R}^{n}$。最小绝对收缩和选择算子 (LASSO) 估计量被定义为凸目标函数的最小化子：\n$$\n\\widehat{\\beta}_{\\lambda} \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\left\\{\\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\\right\\},\n$$\n其中 $\\lambda  0$ 是一个正则化参数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$-范数。精确支撑集恢复问题旨在探究是否 $\\operatorname{supp}(\\widehat{\\beta}_{\\lambda}) = \\operatorname{supp}(\\beta)$。\n\n仅从凸优化的 Karush-Kuhn-Tucker (KKT) 最优性条件、$\\ell_1$-范数的次梯度以及样本格拉姆矩阵 $G = X^{\\top} X / n$ 的定义出发，推导 LASSO 实现精确支撑集恢复所必须满足的一个必要不等式，该不等式被称为不可表示条件。你的推导必须从线性模型和 KKT 条件开始，并且必须指明由 $G$ 的分块量化的特征之间的相关性如何控制恢复真实支撑集的能力。请精确解释在 $p \\gg n$ 情景下，特征相关性如何可能违反此条件，从而阻碍精确支撑集恢复。\n\n然后，实现一个程序，为提供的一个包含三个合成场景的测试套件执行以下步骤，每个场景都旨在探究不可表示条件的不同方面：\n\n- 对于每个场景，生成一个具有 $n$ 行和 $p$ 列的设计矩阵 $X$、一个具有已知支撑集 $S \\subset \\{1,\\dots,p\\}$ 的真实系数向量 $\\beta$，以及一个响应向量 $y = X\\beta + \\varepsilon$，其中 $\\varepsilon$ 从零均值高斯分布中抽取。$X$ 的所有列都必须标准化为均值为 $0$、欧几里得范数为 $\\sqrt{n}$，$y$ 必须中心化为均值为 $0$（无截距项）。支撑集 $S$ 固定为前 $k$ 个索引 $\\{0,1,\\dots,k-1\\}$，其系数为严格为正的相等大小的值。每个场景使用固定的随机种子以确保可复现性。\n\n- 仅使用样本格拉姆矩阵 $G = X^{\\top} X / n$，计算不可表示指数\n$$\n\\mu = \\left\\|G_{S^c,S} \\, G_{S,S}^{-1} \\, \\operatorname{sgn}(\\beta_S)\\right\\|_{\\infty},\n$$\n其中 $S^c$ 是 $S$ 的补集，$G_{S,S}$ 是由 $S$ 索引的 $G$ 的主子矩阵，$G_{S^c,S}$ 是行在 $S^c$ 中、列在 $S$ 中的 $G$ 的子矩阵，$\\operatorname{sgn}(\\cdot)$ 是逐元素符号函数。如果 $G_{S,S}$ 不可逆，则使用数值稳定的替代方法（例如，Moore-Penrose 伪逆）来评估该表达式。当且仅当 $\\mu  1$ 时，判定不可表示条件成立。\n\n- 通过坐标下降法拟合 LASSO 以获得 $\\widehat{\\beta}_{\\lambda}$，对每个场景使用上述目标函数和给定的 $\\lambda$。通过检查 $\\widehat{\\beta}_{\\lambda}$ 的非零项索引集合是否等于 $S$ 来判断是否实现了精确支撑集恢复，其中如果估计系数的绝对值严格大于 $10^{-6}$，则该索引被视为非零。\n\n测试套件和参数：\n1. 理想路径（弱相关性）：\n   - $n = 30$, $p = 80$, $k = 5$, 系数大小 $b = 1.0$, 噪声标准差 $\\sigma = 0.01$, 正则化参数 $\\lambda = 0.2$, 随机种子 $0$。\n   - 构建方法：$X$ 具有独立的标准正态分布元素；除了随机抽样外，不强制施加特殊的相关结构。\n2. 近边界相关（单个强相关变量）：\n   - $n = 30$, $p = 80$, $k = 5$, $b = 1.0$, $\\sigma = 0.05$, $\\lambda = 0.05$, 随机种子 $1$。\n   - 构建方法：从独立的标准正态分布 $X$ 开始，然后对于索引为 $50, 51, 52$ 的三个非支撑集列，设置 $X_{\\cdot, j} \\leftarrow 0.95 \\cdot X_{\\cdot, 0} + \\sqrt{1 - 0.95^2} \\cdot u_j$，其中 $u_j$ 是一个独立的标准正态向量。这为真实支撑集特征创建了一个高度相关的代理变量。\n3. 反例（多个强相关变量）：\n   - $n = 30$, $p = 80$, $k = 5$, $b = 1.0$, $\\sigma = 0.01$, $\\lambda = 0.2$, 随机种子 $2$。\n   - 构建方法：从独立的标准正态分布 $X$ 开始，然后对于索引为 $60, 61, 62, 63, 64$ 的五个非支撑集列，设置：\n     - $X_{\\cdot, 60} \\leftarrow 0.8 \\cdot X_{\\cdot, 0} + 0.8 \\cdot X_{\\cdot, 1} + 0.1 \\cdot u_{60}$,\n     - $X_{\\cdot, 61} \\leftarrow 0.8 \\cdot X_{\\cdot, 1} + 0.8 \\cdot X_{\\cdot, 2} + 0.1 \\cdot u_{61}$,\n     - $X_{\\cdot, 62} \\leftarrow 0.8 \\cdot X_{\\cdot, 2} + 0.8 \\cdot X_{\\cdot, 3} + 0.1 \\cdot u_{62}$,\n     - $X_{\\cdot, 63} \\leftarrow 0.8 \\cdot X_{\\cdot, 3} + 0.8 \\cdot X_{\\cdot, 4} + 0.1 \\cdot u_{63}$,\n     - $X_{\\cdot, 64} \\leftarrow 0.8 \\cdot X_{\\cdot, 0} + 0.8 \\cdot X_{\\cdot, 4} + 0.1 \\cdot u_{64}$,\n     其中每个 $u_j$ 是一个独立的标准正态向量。这产生了多个强相关变量，其组合效应挑战了不可表示条件。\n\n对于每个场景，在按上述方式构建 $X$ 后，将每列标准化为均值为 $0$、欧几里得范数为 $\\sqrt{n}$，将 $y$ 中心化为均值为 $0$，并设置真实支撑集 $S = \\{0,1,2,3,4\\}$，其中对于 $j \\in S$，$\\beta_j = b$，否则 $\\beta_j = 0$。使用给定的 $\\sigma$ 从 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 中抽取 $\\varepsilon$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个场景贡献一个形如 $[\\mu, c, r]$ 的列表，包含以下部分：\n- $\\mu$：不可表示指数，为一个四舍五入到六位小数的浮点数，\n- $c$：一个布尔值，指示不可表示条件是否成立（如果 $\\mu  1$ 则为真，否则为假），\n- $r$：一个布尔值，指示 LASSO 是否恢复了精确支撑集（如果 $\\operatorname{supp}(\\widehat{\\beta}_{\\lambda}) = S$ 则为真，否则为假）。\n\n例如，最终输出应类似于 `[[\\mu_1,c_1,r_1],[\\mu_2,c_2,r_2],[\\mu_3,c_3,r_3]]`，其中符号被数值替换。本问题不适用任何物理单位或角度单位。所有随机抽取必须在指定的随机种子下执行，以确保可复现性。", "solution": "该问题要求推导 LASSO 估计量实现精确支撑集恢复的不可表示条件，并实现一个程序来测试此条件。\n\n### 不可表示条件的推导\n\n我们从线性模型 $y = X \\beta + \\varepsilon$ 开始，其中 $X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^{p}$，$y \\in \\mathbb{R}^{n}$，$\\varepsilon \\in \\mathbb{R}^{n}$ 是一个噪声向量。LASSO 估计量 $\\widehat{\\beta}_{\\lambda}$ 最小化目标函数：\n$$L(\\beta) = \\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1$$\n这是一个凸优化问题。一个向量 $\\widehat{\\beta}$ 是解，当且仅当零向量是 $L(\\beta)$ 在 $\\widehat{\\beta}$ 处的次微分的一个元素。$L(\\beta)$ 的次微分由 $\\partial L(\\beta) = \\nabla \\left(\\frac{1}{2n}\\|y - X\\beta\\|_2^2\\right) + \\lambda \\partial \\|\\beta\\|_1$ 给出。\n\n最小二乘项的梯度是 $\\frac{1}{n}X^{\\top}(X\\beta - y)$。$\\ell_1$-范数 $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$ 的次微分是向量 $z \\in \\mathbb{R}^p$ 的集合，使得对于每个分量 $j$：\n$$\nz_j = \\begin{cases}\n\\operatorname{sgn}(\\beta_j)  \\text{若 } \\beta_j \\neq 0 \\\\\nv_j \\in [-1, 1]  \\text{若 } \\beta_j = 0\n\\end{cases}\n$$\n其中 $\\operatorname{sgn}(\\cdot)$ 是符号函数。\n\n因此，$\\widehat{\\beta}_{\\lambda}$ 的 Karush-Kuhn-Tucker (KKT) 最优性条件是，存在一个次梯度向量 $z$，其中如果 $\\widehat{\\beta}_{\\lambda,j} \\neq 0$ 则 $z_j = \\operatorname{sgn}(\\widehat{\\beta}_{\\lambda,j})$，如果 $\\widehat{\\beta}_{\\lambda,j} = 0$ 则 $|z_j| \\le 1$，使得：\n$$ \\frac{1}{n}X^{\\top}(X\\widehat{\\beta}_{\\lambda} - y) + \\lambda z = 0 \\implies \\frac{1}{n}X^{\\top}(y - X\\widehat{\\beta}_{\\lambda}) = \\lambda z $$\n\n现在，我们假设 LASSO 实现了精确支撑集恢复。令 $S = \\operatorname{supp}(\\beta)$ 为真实支撑集，即 $\\beta_j \\neq 0$ 的索引集合。精确支撑集恢复的假设意味着 $\\operatorname{supp}(\\widehat{\\beta}_{\\lambda}) = S$。这表明对于 $j \\in S$，$\\widehat{\\beta}_{\\lambda,j} \\neq 0$，而对于 $j \\in S^c$，$\\widehat{\\beta}_{\\lambda,j} = 0$，其中 $S^c$ 是 $S$ 的补集。\n\n我们将设计矩阵 $X$ 划分为 $X_S$ 和 $X_{S^c}$，分别对应于 $S$ 和 $S^c$ 中的列。类似地，我们将像 $\\beta$ 这样的向量划分为 $\\beta_S$ 和 $\\beta_{S^c}$。在支撑集恢复的假设下，$X\\widehat{\\beta}_{\\lambda} = X_S \\widehat{\\beta}_{\\lambda,S}$，因为 $\\widehat{\\beta}_{\\lambda,S^c} = 0$。\n\nKKT 条件现在可以根据划分 $(S, S^c)$ 分为两部分：\n1.  对于索引 $j \\in S$（激活集）：\n    $$ \\frac{1}{n}X_S^{\\top}(y - X_S\\widehat{\\beta}_{\\lambda,S}) = \\lambda \\operatorname{sgn}(\\widehat{\\beta}_{\\lambda,S}) $$\n2.  对于索引 $j \\in S^c$（非激活集）：\n    $$ \\left|\\frac{1}{n}X_{S^c}^{\\top}(y - X_S\\widehat{\\beta}_{\\lambda,S})\\right| \\le \\lambda \\quad (\\text{逐元素不等式}) $$\n\n为了使精确恢复有意义，特别是在 $\\lambda \\to 0$ 的渐近意义上，我们需要符号一致性：$\\operatorname{sgn}(\\widehat{\\beta}_{\\lambda,S}) = \\operatorname{sgn}(\\beta_S)$。我们假设这一点成立。第一个 KKT 条件变为：\n$$ \\frac{1}{n}X_S^{\\top}(y - X_S\\widehat{\\beta}_{\\lambda,S}) = \\lambda \\operatorname{sgn}(\\beta_S) $$\n代入线性模型 $y = X_S\\beta_S + \\varepsilon$（因为 $\\beta_{S^c} = 0$）：\n$$ \\frac{1}{n}X_S^{\\top}(X_S\\beta_S + \\varepsilon - X_S\\widehat{\\beta}_{\\lambda,S}) = \\lambda \\operatorname{sgn}(\\beta_S) $$\n使用样本格拉姆矩阵的定义 $G = X^{\\top}X/n$，其子矩阵为 $G_{S,S} = X_S^{\\top}X_S/n$。方程简化为：\n$$ G_{S,S}(\\beta_S - \\widehat{\\beta}_{\\lambda,S}) + \\frac{1}{n}X_S^{\\top}\\varepsilon = \\lambda \\operatorname{sgn}(\\beta_S) $$\n假设 $G_{S,S}$ 是可逆的，我们可以表示出差值 $(\\beta_S - \\widehat{\\beta}_{\\lambda,S})$：\n$$ \\beta_S - \\widehat{\\beta}_{\\lambda,S} = G_{S,S}^{-1} \\left( \\lambda \\operatorname{sgn}(\\beta_S) - \\frac{1}{n}X_S^{\\top}\\varepsilon \\right) $$\n\n现在我们分析非激活集 $S^c$ 的第二个 KKT 条件。我们代入残差 $y - X_S\\widehat{\\beta}_{\\lambda,S}$ 的表达式：\n$$ y - X_S\\widehat{\\beta}_{\\lambda,S} = (X_S\\beta_S + \\varepsilon) - X_S\\widehat{\\beta}_{\\lambda,S} = X_S(\\beta_S - \\widehat{\\beta}_{\\lambda,S}) + \\varepsilon $$\n代入 $(\\beta_S - \\widehat{\\beta}_{\\lambda,S})$ 的表达式：\n$$ y - X_S\\widehat{\\beta}_{\\lambda,S} = X_S G_{S,S}^{-1} \\left( \\lambda \\operatorname{sgn}(\\beta_S) - \\frac{1}{n}X_S^{\\top}\\varepsilon \\right) + \\varepsilon $$\n现在，我们将其代入非激活集的 KKT 条件：\n$$ \\left| \\frac{1}{n}X_{S^c}^{\\top} \\left( X_S G_{S,S}^{-1} \\lambda \\operatorname{sgn}(\\beta_S) - X_S G_{S,S}^{-1} \\frac{1}{n}X_S^{\\top}\\varepsilon + \\varepsilon \\right) \\right| \\le \\lambda $$\n使用 $G_{S^c,S} = X_{S^c}^{\\top}X_S/n$ 并整理各项：\n$$ \\left| \\lambda G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) + \\frac{1}{n}(X_{S^c}^{\\top} - X_{S^c}^{\\top}X_S G_{S,S}^{-1} \\frac{1}{n}X_S^{\\top})\\varepsilon \\right| \\le \\lambda $$\n两边除以 $\\lambda  0$：\n$$ \\left| G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) + \\frac{1}{\\lambda} \\cdot (\\text{噪声项}) \\right| \\le 1 $$\n为使此不等式成立，特别是在一个小的 $\\lambda$ 范围内，其中噪声项可能不可忽略，表达式的确定性部分必须严格小于 1。如果向量 $G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S)$ 的任何元素的大小等于或大于 1，那么少量的噪声 $\\varepsilon$ 就能轻易导致该不等式被违反。因此，稳健的支撑集恢复的一个必要条件是**不可表示条件**：\n$$ \\left\\| G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) \\right\\|_{\\infty}  1 $$\n量 $\\mu = \\left\\| G_{S^c,S} G_{S,S}^{-1} \\operatorname{sgn}(\\beta_S) \\right\\|_{\\infty}$ 是不可表示指数。\n\n### 特征相关性与 $p \\gg n$ 情景的作用\n\n不可表示条件突显了特征相关性的关键作用。\n- $G_{S,S} = X_S^{\\top}X_S/n$ 代表了真实预测变量*之间*的相关性。如果这些预测变量高度相关（多重共线性），$G_{S,S}$ 就会是病态的，其逆矩阵 $G_{S,S}^{-1}$ 将会有很大的元素。\n- $G_{S^c,S} = X_{S^c}^{\\top}X_S/n$ 代表了真实预测变量（在 $S$ 中）与不相关的“噪声”预测变量（在 $S^c$ 中）*之间*的相关性。\n\n乘积 $G_{S^c,S} G_{S,S}^{-1}$ 可以解释为当每个噪声预测变量 $X_j$（$j \\in S^c$）对真实预测变量集合 $X_S$ 进行回归时的回归系数矩阵。如果一个噪声预测变量 $X_j$ 可以被真实预测变量的线性组合很好地近似，那么 $G_{S^c,S} G_{S,S}^{-1}$ 中对应的行将会有很大的元素。当这种线性组合与真实系数 $\\beta_S$ 的符号一致时，不可表示指数 $\\mu$ 就可能超过 1。\n\n在这种情况下，LASSO 无法区分真实的稀疏模型和一个包含了高度相关的噪声预测变量的模型。“不可表示”一词意味着真实预测变量绝不能被噪声预测变量“表示”。\n\n在小样本、大维度（$p \\gg n$）情景下，有几个因素会加剧这个问题：\n1.  **高维度性**：当有大量噪声预测变量（$p-k \\gg n$）时，在 $S^c$ 中找到一个或多个与 $S$ 中变量伪相关性很高的预测变量的概率急剧增加。\n2.  **病态性**：当真实预测变量的数量 $k$ 接近样本量 $n$ 时，矩阵 $G_{S,S}$ 变得病态或奇异，导致 $\\|G_{S,S}^{-1}\\|$ 激增。这会放大激活集和非激活集之间任何已有的相关性。\n3.  **伪相关性**：当 $n$ 较小时，样本格拉姆矩阵 $G$ 是总体协方差的一个带噪声的估计。随机性可能产生在底层数据生成过程中并不存在的强样本相关性，从而导致该条件被违反。\n\n这些因素的结合使得在 $p \\gg n$ 情景下用 LASSO 实现精确支撑集恢复非常具有挑战性。不可表示条件为这一挑战提供了精确的数学表述，表明成功与否关键取决于设计矩阵的相关结构。", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef _generate_data(n, p, k, b, sigma, seed, construction):\n    \"\"\"Generates synthetic data for a single LASSO scenario.\"\"\"\n    rng = np.random.default_rng(seed)\n    # Start with an independent standard normal design matrix\n    X = rng.standard_normal((n, p))\n    \n    # Apply specific correlation structures based on the scenario\n    if construction == 'single_correlate':\n        # Create non-support features highly correlated with a single support feature\n        for j in [50, 51, 52]:\n            u_j = rng.standard_normal(n)\n            X[:, j] = 0.95 * X[:, 0] + np.sqrt(1.0 - 0.95**2) * u_j\n    elif construction == 'multiple_correlates':\n        # Create non-support features that are linear combinations of multiple support features\n        correlates_def = {\n            60: [0, 1], 61: [1, 2], 62: [2, 3], 63: [3, 4], 64: [0, 4]\n        }\n        for j, support_indices in correlates_def.items():\n            u_j = rng.standard_normal(n)\n            # Coefficients are chosen to likely violate the irrepresentable condition\n            linear_combo = 0.8 * X[:, support_indices[0]] + 0.8 * X[:, support_indices[1]]\n            X[:, j] = linear_combo + 0.1 * u_j\n            \n    # Standardize columns of X: mean 0, Euclidean norm sqrt(n)\n    X -= X.mean(axis=0)\n    col_norms = np.linalg.norm(X, axis=0)\n    # Avoid division by zero for columns that might be all zero by adding a small epsilon.\n    X = X / (col_norms + 1e-9) * np.sqrt(n)\n    \n    # Define true coefficient vector beta and support S\n    beta_true = np.zeros(p)\n    beta_true[:k] = b\n    S = set(range(k))\n    \n    # Generate response vector y\n    epsilon = rng.standard_normal(n) * sigma\n    y = X @ beta_true + epsilon\n    \n    # Center y\n    y -= y.mean()\n    \n    return X, y, beta_true, S\n\ndef _calculate_mu(X, S, beta_true, n, p):\n    \"\"\"Calculates the irrepresentable index mu.\"\"\"\n    # Compute the sample Gram matrix\n    G = (X.T @ X) / n\n    \n    # Partition the Gram matrix according to the support S and its complement Sc\n    S_list = sorted(list(S))\n    Sc_list = sorted(list(set(range(p)) - S))\n    \n    G_Sc_S = G[np.ix_(Sc_list, S_list)]\n    G_S_S = G[np.ix_(S_list, S_list)]\n    \n    # Compute the inverse of G_S_S using the Moore-Penrose pseudoinverse for stability\n    # The check_finite=False argument is needed for some edge cases with older scipy versions.\n    G_S_S_inv = linalg.pinv(G_S_S, check_finite=False)\n\n    # Get the sign vector of the true coefficients on the support\n    sgn_beta_S = np.sign(beta_true[S_list])\n    \n    # Compute the vector whose max absolute value is the irrepresentable index\n    v = G_Sc_S @ G_S_S_inv @ sgn_beta_S\n    \n    # The irrepresentable index mu is the infinity norm of this vector\n    mu = np.max(np.abs(v))\n    \n    return mu\n\ndef _lasso_cd(X, y, lambda_, max_iter=1000, tol=1e-7):\n    \"\"\"Fits the LASSO estimator using coordinate descent.\"\"\"\n    n, p = X.shape\n    beta = np.zeros(p)\n    \n    # Coordinate descent with efficient residual updates.\n    # Because X columns are normalized to have squared L2-norm of n,\n    # the updates simplify nicely.\n    residual = y.copy()\n\n    for _ in range(max_iter):\n        max_change = 0.0\n        for j in range(p):\n            beta_j_old = beta[j]\n            \n            # The argument for the soft-thresholding operator is a_j.\n            # a_j = (Xj.T @ (y - X@beta_except_j)) / n\n            # This can be efficiently computed as:\n            # a_j = (Xj.T @ current_residual)/n + (Xj.T @ Xj / n) * beta_j_old\n            # Since Xj.T @ Xj / n = 1 in our setup\n            a_j = (X[:, j] @ residual) / n + beta_j_old\n            \n            # Apply the soft-thresholding operator S_lambda(a_j)\n            beta[j] = np.sign(a_j) * max(abs(a_j) - lambda_, 0.0)\n            \n            delta_beta_j = beta[j] - beta_j_old\n            if delta_beta_j != 0.0:\n                residual -= X[:, j] * delta_beta_j\n            \n            if abs(delta_beta_j)  max_change:\n                max_change = abs(delta_beta_j)\n        \n        if max_change  tol:\n            break\n            \n    return beta\n\ndef _check_recovery(beta_hat, S, tol=1e-6):\n    \"\"\"Checks if the estimated support equals the true support.\"\"\"\n    S_hat = {i for i, b in enumerate(beta_hat) if abs(b)  tol}\n    return S_hat == S\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Scenario 1: Weak correlations, recovery expected.\n        {'n': 30, 'p': 80, 'k': 5, 'b': 1.0, 'sigma': 0.01, 'lambda_': 0.2, 'seed': 0, 'construction': 'independent'},\n        # Scenario 2: Strong correlation, recovery challenging.\n        {'n': 30, 'p': 80, 'k': 5, 'b': 1.0, 'sigma': 0.05, 'lambda_': 0.05, 'seed': 1, 'construction': 'single_correlate'},\n        # Scenario 3: Multiple strong correlates, recovery expected to fail.\n        {'n': 30, 'p': 80, 'k': 5, 'b': 1.0, 'sigma': 0.01, 'lambda_': 0.2, 'seed': 2, 'construction': 'multiple_correlates'},\n    ]\n\n    results = []\n    for params in test_cases:\n        # Unpack parameters\n        n, p, k, b = params['n'], params['p'], params['k'], params['b']\n        sigma, lambda_ = params['sigma'], params['lambda_']\n        seed, construction = params['seed'], params['construction']\n        \n        # 1. Generate data\n        X, y, beta_true, S = _generate_data(n, p, k, b, sigma, seed, construction)\n        \n        # 2. Compute irrepresentable index and condition\n        mu = _calculate_mu(X, S, beta_true, n, p)\n        irrep_cond_holds = mu  1.0\n        \n        # 3. Fit LASSO model using coordinate descent\n        beta_hat = _lasso_cd(X, y, lambda_)\n\n        # 4. Check for exact support recovery\n        recovery_succeeded = _check_recovery(beta_hat, S)\n        \n        results.append([mu, irrep_cond_holds, recovery_succeeded])\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{res[0]:.6f},{str(res[1])},{str(res[2])}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3186678"}, {"introduction": "前面的练习主要集中在线性模型框架下。然而，在许多现实问题中，特征与响应之间的关系可能是非线性的。这个高级练习将引导你探索一种强大的非参数贝叶斯方法——带有自动相关性确定（ARD）核的高斯过程（GP），它能够在进行非线性回归的同时自动评估特征的重要性。你将从零开始实现一个GP模型，通过最大化边际似然来学习核函数的长度尺度参数，并亲眼见证无关特征的长度尺度如何被自动“拉伸”从而失效，这为你提供了一种在复杂模型中进行特征选择的全新视角。[@problem_id:3186634]", "problem": "在统计学习中，考虑小样本、高维度的情景，即数据点数量 $n$ 远小于特征数量 $p$。在进行特征相关性分析的同时对非线性关系进行建模，一种常见的方法是使用带有自动相关性确定（ARD）核的高斯过程（GP）回归模型，通常实例化为对每个特征维度使用独立长度尺度的径向基函数（RBF）核。\n\n您需要实现一个完整、可运行的程序，该程序能够：\n- 构建 $p \\gg n$ 的合成数据集，其中只有一个已知的特征维度子集与响应变量相关，其余维度均不相关。\n- 通过最大化关于超参数的边缘似然，来拟合带有 ARD RBF 核的高斯过程（GP）。\n- 使用学习到的 ARD 长度尺度对特征相关性进行排序，并评估正确识别出的真实相关特征的数量。\n\n理论基础：\n- 高斯过程（GP）回归假设先验 $f \\sim \\mathcal{GP}(0, k(\\cdot, \\cdot))$，其中 $k(\\cdot, \\cdot)$ 是一个正定核函数。\n- 对于训练输入 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 和目标 $\\mathbf{y} \\in \\mathbb{R}^{n}$，在协方差矩阵为 $\\mathbf{K}$ 的高斯过程下，其边缘对数似然为\n$$\n\\log p(\\mathbf{y} \\mid \\theta) = -\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}^{-1} \\mathbf{y} - \\frac{1}{2} \\log\\det(\\mathbf{K}) - \\frac{n}{2}\\log(2\\pi),\n$$\n其中 $\\theta$ 表示超参数。这个公式是高斯过程回归中一个经过充分检验的事实。\n- 带有每个维度长度尺度 $l_1, \\ldots, l_p$ 和信号标准差 $\\sigma_f$ 的 ARD RBF 核定义为\n$$\nk_{\\text{ARD}}(\\mathbf{x}, \\mathbf{x}') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\sum_{j=1}^p \\frac{(x_j - x_j')^2}{l_j^2}\\right).\n$$\n- 观测噪声被建模为独立同分布（i.i.d.）的高斯噪声，其标准差为 $\\sigma_n$，这会在协方差矩阵上加上 $\\sigma_n^2 \\mathbf{I}$。\n\n程序要求：\n1. 基于上述理论基础，推导出一个有原则的算法，通过最大化 GP 边缘似然来学习对数超参数 $\\{\\log l_j\\}_{j=1}^p$、$\\log \\sigma_f$ 和 $\\log \\sigma_n$。您在解决方案中的推导必须从边缘对数似然出发，并使用矩阵微积分来获得可用于优化的梯度。请勿使用未从基础推导出的简化公式。\n2. 为小 $n$ 大 $p$ 的情景高效地实现该算法。您的实现必须：\n   - 构建每个维度的 ARD 成对平方距离贡献，并组装核矩阵。\n   - 使用数值稳定的线性代数方法，包括用于求解线性系统和计算 $\\log\\det(\\mathbf{K})$ 的 Cholesky 分解。\n   - 在 $\\mathbf{K}$ 的对角线上加入一个小的扰动项（jitter）以确保正定性，这是一种符合科学实际的做法。\n   - 使用基于梯度的方法优化对数超参数，并设置适当的边界以确保其在指数化后为正数。\n3. 拟合后，计算学习到的长度尺度 $\\{l_j\\}$，按长度尺度的倒数 $s_j = 1/l_j$ 对特征进行排序（越大的 $s_j$ 表示相关性越高），并选择前 $k$ 个特征，其中 $k$ 等于测试用例中真实相关特征的数量。计算真正例（true positives）的数量（即在前 $k$ 个特征中正确识别出的相关维度）。\n\n测试套件：\n实现四个测试用例以评估解决方案的不同方面。对于每个测试用例，输入和数据生成必须如下：\n\n- 所有用例的通用数据生成方式：\n  - 独立地生成 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，其元素 $x_{ij} \\sim \\mathcal{N}(0, 1)$。\n  - 令 $\\mathcal{R} \\subset \\{0,1,\\ldots,p-1\\}$ 为相关特征的索引集，每个测试用例分别指定。\n  - 使用非线性函数生成目标 $\\mathbf{y}$\n  $$\n  y_i = \\sum_{j \\in \\mathcal{R}} \\left[\\sin\\left(x_{ij}\\right) + 0.5\\, x_{ij}\\right] + \\epsilon_i,\n  $$\n  其中 $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$ 独立同分布。通过减去其经验均值来中心化目标变量。\n  - 定义 $k = |\\mathcal{R}|$。\n\n- 每个测试的具体参数值：\n  1. 理想情景用例：$n = 25$, $p = 40$, $\\mathcal{R} = \\{2,5,11\\}$, $\\sigma_{\\text{noise}} = 0.05$, 随机种子为 $0$。\n  2. 单一相关维度：$n = 12$, $p = 50$, $\\mathcal{R} = \\{0\\}$, $\\sigma_{\\text{noise}} = 0.10$, 随机种子为 $1$。\n  3. 极端的小 $n$ 大 $p$ 情景：$n = 8$, $p = 60$, $\\mathcal{R} = \\{3,7\\}$, $\\sigma_{\\text{noise}} = 0.10$, 随机种子为 $2$。\n  4. 所有维度均不相关（边界情况）：$n = 20$, $p = 30$, $\\mathcal{R} = \\varnothing$（空集），$\\sigma_{\\text{noise}} = 0.15$, 随机种子为 $3$。\n\n最终输出规范：\n- 对于每个测试用例，计算按长度尺度倒数 $s_j = 1/l_j$ 排序的前 $k$ 个特征中被正确识别的相关特征的整数数量。\n- 您的程序应生成单行输出，其中包含四个测试用例的结果，格式为方括号括起来的逗号分隔列表，例如 `[r_1,r_2,r_3,r_4]`，其中每个 $r_i$ 是对应第 $i$ 个测试用例的整数。", "solution": "该问题要求实现一个带有自动相关性确定（ARD）径向基函数（RBF）核的高斯过程（GP）回归模型，以便在小 $n$ 大 $p$ 的情景下执行特征相关性排序。此任务的核心是推导并实现一个通过最大化边缘对数似然来学习核超参数的算法。\n\n### 问题验证\n问题陈述已被解析和验证。\n- **给定条件**：问题提供了 GP 的数学定义、边缘对数似然函数、ARD RBF 核、观测噪声模型、精确的数据生成过程，以及一组包含所有必要参数的四个具体测试用例。\n- **验证**：该问题在科学上基于统计学习中成熟的高斯过程理论。问题提法得当，目标明确，指令完整且无矛盾。术语精确客观。该任务并非无足轻重，需要使用矩阵微积分进行理论推导和进行数值稳定的实现。\n- **结论**：问题有效。\n\n### 算法推导\n\n目标是找到最大化观测数据边缘对数似然的超参数。我们处理负边缘对数似然（NLL），并旨在将其最小化。NLL 由下式给出：\n$$\n\\text{NLL}(\\theta) = \\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}_y^{-1} \\mathbf{y} + \\frac{1}{2} \\log\\det(\\mathbf{K}_y) + \\frac{n}{2}\\log(2\\pi)\n$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^n$ 是中心化后的训练目标， $n$ 是数据点数量，$\\mathbf{K}_y$ 是带噪观测值的 $n \\times n$ 协方差矩阵。其定义为：\n$$\n\\mathbf{K}_y = \\mathbf{K}_{\\text{ARD}} + \\sigma_n^2 \\mathbf{I}\n$$\n此处，$\\mathbf{K}_{\\text{ARD}}$ 是由 ARD RBF 核生成的核矩阵，$\\sigma_n$ 是观测噪声的标准差，$\\mathbf{I}$ 是单位矩阵。$\\mathbf{K}_{\\text{ARD}}$ 的元素由下式给出：\n$$\n[\\mathbf{K}_{\\text{ARD}}]_{ik} = k_{\\text{ARD}}(\\mathbf{x}_i, \\mathbf{x}_k) = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\sum_{j=1}^p \\frac{(x_{ij} - x_{kj})^2}{l_j^2}\\right)\n$$\n需要优化的超参数集是 $\\theta' = (\\sigma_f, l_1, \\ldots, l_p, \\sigma_n)$。为确保这些参数保持正值，我们优化它们的对数：$\\theta = (\\zeta_f, \\lambda_1, \\ldots, \\lambda_p, \\zeta_n)$，其中 $\\zeta_f = \\log \\sigma_f$，$\\lambda_j = \\log l_j$，$ \\zeta_n = \\log \\sigma_n$。\n\n我们使用基于梯度的优化方法，这需要计算 NLL 相对于每个对数超参数的偏导数。令 $\\phi$ 为 $\\theta$ 中任意一个对数超参数。NLL 的导数为：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = \\frac{1}{2} \\mathbf{y}^\\top \\frac{\\partial \\mathbf{K}_y^{-1}}{\\partial \\phi} \\mathbf{y} + \\frac{1}{2} \\frac{\\partial \\log\\det(\\mathbf{K}_y)}{\\partial \\phi}\n$$\n使用矩阵微积分恒等式 $\\frac{\\partial \\mathbf{A}^{-1}}{\\partial \\phi} = -\\mathbf{A}^{-1}\\frac{\\partial \\mathbf{A}}{\\partial \\phi}\\mathbf{A}^{-1}$ 和 $\\frac{\\partial \\log\\det \\mathbf{A}}{\\partial \\phi} = \\text{tr}\\left(\\mathbf{A}^{-1}\\frac{\\partial \\mathbf{A}}{\\partial \\phi}\\right)$，我们得到：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = -\\frac{1}{2} \\mathbf{y}^\\top \\mathbf{K}_y^{-1}\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\mathbf{K}_y^{-1} \\mathbf{y} + \\frac{1}{2} \\text{tr}\\left(\\mathbf{K}_y^{-1}\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\right)\n$$\n我们定义 $\\boldsymbol{\\alpha} = \\mathbf{K}_y^{-1}\\mathbf{y}$。表达式简化为：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = -\\frac{1}{2} \\boldsymbol{\\alpha}^\\top \\frac{\\partial \\mathbf{K}_y}{\\partial \\phi} \\boldsymbol{\\alpha} + \\frac{1}{2} \\text{tr}\\left(\\mathbf{K}_y^{-1}\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\right)\n$$\n使用迹恒等式 $\\mathbf{u}^\\top \\mathbf{M} \\mathbf{u} = \\text{tr}(\\mathbf{M}\\mathbf{u}\\mathbf{u}^\\top)$，这可以紧凑地写为：\n$$\n\\frac{\\partial \\text{NLL}}{\\partial \\phi} = -\\frac{1}{2} \\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)\\frac{\\partial \\mathbf{K}_y}{\\partial \\phi}\\right)\n$$\n接下来，我们推导 $\\mathbf{K}_y$ 相对于每个对数超参数的偏导数。\n\n1.  **关于对数信号标准差 $\\zeta_f = \\log \\sigma_f$ 的导数**：\n    我们有 $\\sigma_f^2 = e^{2\\zeta_f}$。核矩阵为 $\\mathbf{K}_{\\text{ARD}} = e^{2\\zeta_f} \\mathbf{K}_{\\text{base}}$，其中 $[\\mathbf{K}_{\\text{base}}]_{ik} = \\exp(-\\frac{1}{2}\\sum_j (x_{ij}-x_{kj})^2/l_j^2)$。\n    $$\n    \\frac{\\partial \\mathbf{K}_y}{\\partial \\zeta_f} = \\frac{\\partial \\mathbf{K}_{\\text{ARD}}}{\\partial \\zeta_f} = \\frac{\\partial (e^{2\\zeta_f})}{\\partial \\zeta_f} \\mathbf{K}_{\\text{base}} = 2e^{2\\zeta_f} \\mathbf{K}_{\\text{base}} = 2\\mathbf{K}_{\\text{ARD}}\n    $$\n    梯度为：$\\frac{\\partial \\text{NLL}}{\\partial \\zeta_f} = -\\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)\\mathbf{K}_{\\text{ARD}}\\right)$。\n\n2.  **关于对数噪声标准差 $\\zeta_n = \\log \\sigma_n$ 的导数**：\n    噪声项为 $\\sigma_n^2 \\mathbf{I} = e^{2\\zeta_n} \\mathbf{I}$。\n    $$\n    \\frac{\\partial \\mathbf{K}_y}{\\partial \\zeta_n} = \\frac{\\partial (e^{2\\zeta_n}\\mathbf{I})}{\\partial \\zeta_n} = 2 e^{2\\zeta_n}\\mathbf{I} = 2\\sigma_n^2\\mathbf{I}\n    $$\n    梯度为：$\\frac{\\partial \\text{NLL}}{\\partial \\zeta_n} = -\\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)\\sigma_n^2\\mathbf{I}\\right) = -\\sigma_n^2 \\text{tr}\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right)$。\n\n3.  **关于对数长度尺度 $\\lambda_j = \\log l_j$ 的导数**：\n    核函数依赖于 $l_j^{-2} = e^{-2\\lambda_j}$。令 $\\mathbf{D}_j$ 为第 $j$ 个特征的成对平方距离矩阵，即 $[\\mathbf{D}_j]_{ik} = (x_{ij} - x_{kj})^2$。\n    $$\n    \\frac{\\partial \\mathbf{K}_{\\text{ARD}}}{\\partial \\lambda_j} = \\mathbf{K}_{\\text{ARD}} \\circ \\frac{\\partial}{\\partial \\lambda_j} \\left(-\\frac{1}{2} \\sum_{m=1}^p \\frac{\\mathbf{D}_m}{l_m^2}\\right) = \\mathbf{K}_{\\text{ARD}} \\circ \\left(-\\frac{1}{2} \\mathbf{D}_j(-2e^{-2\\lambda_j})\\right) = \\mathbf{K}_{\\text{ARD}} \\circ \\frac{\\mathbf{D}_j}{l_j^2}\n    $$\n    其中 $\\circ$ 表示逐元素的哈达玛积（Hadamard product）。梯度为：\n    $$\n    \\frac{\\partial \\text{NLL}}{\\partial \\lambda_j} = -\\frac{1}{2} \\text{tr}\\left(\\left(\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}\\right) \\left(\\mathbf{K}_{\\text{ARD}} \\circ \\frac{\\mathbf{D}_j}{l_j^2}\\right)\\right)\n    $$\n\n### 实现策略\n\n-   **数值稳定性**：为计算 NLL 及其梯度，我们必须计算涉及 $\\mathbf{K}_y^{-1}$ 和 $\\log\\det(\\mathbf{K}_y)$ 的项。我们通过使用 $\\mathbf{K}_y = \\mathbf{L}\\mathbf{L}^\\top$ 的 Cholesky 分解来避免显式矩阵求逆。在 $\\mathbf{K}_y$ 的对角线上添加一个小的扰动项（jitter，$10^{-8}$）以确保其正定性。\n    -   $\\log\\det(\\mathbf{K}_y) = 2 \\sum_{i} \\log([\\mathbf{L}]_{ii})$。\n    -   $\\boldsymbol{\\alpha} = \\mathbf{K}_y^{-1}\\mathbf{y}$ 通过 `cho_solve` 高效求解。\n    -   $\\mathbf{K}_y^{-1}$ 同样通过求解 $\\mathbf{K}_y \\mathbf{X} = \\mathbf{I}$ 使用 `cho_solve` 来计算。\n-   **效率**：梯度向量的计算涉及 $p+2$ 个偏导数。矩阵 $\\boldsymbol{\\alpha}\\boldsymbol{\\alpha}^\\top - \\mathbf{K}_y^{-1}$ 只计算一次。主要成本在于遍历 $p$ 个特征维度来计算每个 $\\lambda_j$ 的梯度。对于对称矩阵，迹 $\\text{tr}(\\mathbf{A}\\mathbf{B})$ 可以计算为 `np.sum(A * B.T)`，即 `np.sum(A * B)`。因此，每个梯度分量的计算复杂度为 $O(n^2)$，导致总梯度计算成本为 $O(pn^2)$，这在 $n \\ll p$ 的情景下是高效的。\n-   **优化**：使用 `scipy.optimize.minimize` 函数和 `L-BFGS-B` 方法来最小化 NLL。该方法可以利用计算出的梯度，并允许对超参数设置边界以防止数值问题。\n-   **特征排序**：优化后，提取学习到的长度尺度 $l_j$。每个特征 $j$ 的相关性得分定义为 $s_j = 1/l_j$。一个小的 $l_j$ 意味着核函数对特征 $j$ 在短距离上的变化很敏感，表明其相关性高。因此，一个大的得分 $s_j$ 对应于高的相关性。特征按其得分降序排列。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cholesky, cho_solve\nfrom scipy.spatial.distance import pdist, squareform\n\ndef solve():\n    \"\"\"\n    Executes the full pipeline for all test cases as specified in the problem.\n    \"\"\"\n\n    test_cases = [\n        # (n, p, relevant_features, noise_std, seed)\n        (25, 40, {2, 5, 11}, 0.05, 0),\n        (12, 50, {0}, 0.10, 1),\n        (8, 60, {3, 7}, 0.10, 2),\n        (20, 30, set(), 0.15, 3),\n    ]\n\n    results = []\n    for n, p, R_indices, sigma_noise, seed in test_cases:\n        # 1. Generate synthetic data\n        X, y = generate_data(n, p, R_indices, sigma_noise, seed)\n        k = len(R_indices)\n\n        # 2. Fit GP model\n        # The state vector for optimization: [log(sf), log(sn), log(l_1), ..., log(l_p)]\n        # Initial guess for hyperparameters\n        initial_log_hyperparams = np.concatenate([\n            [np.log(1.0)],          # log(sigma_f)\n            [np.log(0.1)],          # log(sigma_n)\n            np.zeros(p)             # log(l_j) for j=1..p\n        ])\n\n        # Bounds for hyperparameters (on log scale) to prevent numerical issues\n        # log(1e-5) approx -11.5, log(1e5) approx 11.5\n        bounds = [(-10, 10)] * 2 + [(-10, 10)] * p\n\n        # Optimization\n        res = minimize(\n            objective_and_grad,\n            initial_log_hyperparams,\n            args=(X, y),\n            method='L-BFGS-B',\n            jac=True,\n            bounds=bounds\n        )\n\n        # 3. Evaluate feature relevance\n        learned_log_hyperparams = res.x\n        log_l_vec = learned_log_hyperparams[2:]\n        l_vec = np.exp(log_l_vec)\n\n        # Relevance scores are inverse lengthscales\n        relevance_scores = 1.0 / l_vec\n        \n        # Get indices of top k features. If k=0, this is an empty set.\n        top_k_indices = set()\n        if k > 0:\n            ranked_indices = np.argsort(relevance_scores)[::-1]\n            top_k_indices = set(ranked_indices[:k])\n        \n        # Count true positives\n        true_positives = len(top_k_indices.intersection(R_indices))\n        results.append(true_positives)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(n, p, R_indices, sigma_noise, seed):\n    \"\"\"\n    Generates synthetic dataset for a given configuration.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    X = rng.normal(0, 1, size=(n, p))\n    \n    y = np.zeros(n)\n    if R_indices:\n        for j in R_indices:\n            y += np.sin(X[:, j]) + 0.5 * X[:, j]\n\n    noise = rng.normal(0, sigma_noise, size=n)\n    y += noise\n    \n    # Center the target variable\n    y -= np.mean(y)\n    \n    return X, y\n\ndef objective_and_grad(log_hyperparams, X, y):\n    \"\"\"\n    Computes the negative log-likelihood and its gradient for GP regression.\n    \"\"\"\n    n, p = X.shape\n    jitter = 1e-8\n\n    # Unpack hyperparameters\n    log_sf = log_hyperparams[0]\n    log_sn = log_hyperparams[1]\n    log_l_vec = log_hyperparams[2:]\n\n    sf2 = np.exp(2 * log_sf)\n    sn2 = np.exp(2 * log_sn)\n    l_vec = np.exp(log_l_vec)\n\n    # --- Construct Kernel Matrix K_y ---\n    # Using scipy.spatial.distance for efficiency\n    # scaled_sq_dists is the matrix with elements D_ik = sum_j (x_ij - x_kj)^2 / l_j^2\n    X_scaled = X / l_vec\n    scaled_sq_dists = squareform(pdist(X_scaled, 'sqeuclidean'))\n    \n    K = sf2 * np.exp(-0.5 * scaled_sq_dists)\n    Ky = K + (sn2 + jitter) * np.eye(n)\n\n    # --- Compute Negative Log-Likelihood (NLL) ---\n    try:\n        L = cholesky(Ky, lower=True)\n    except np.linalg.LinAlgError:\n        # If matrix is not positive definite, return a large value\n        return 1e9, np.zeros_like(log_hyperparams)\n\n    # Solve K_y * alpha = y using cholesky factor\n    alpha = cho_solve((L, True), y)\n    \n    log_det_Ky = 2 * np.sum(np.log(np.diag(L)))\n    \n    nll = 0.5 * y.dot(alpha) + 0.5 * log_det_Ky + 0.5 * n * np.log(2 * np.pi)\n\n    # --- Compute Gradients of NLL ---\n    # Common term in gradients: (alpha * alpha^T - K_y^-1)\n    Ky_inv = cho_solve((L, True), np.eye(n))\n    A = np.outer(alpha, alpha) - Ky_inv\n\n    # Gradient w.r.t. log(sigma_f)\n    dK_d_log_sf = 2 * K\n    grad_log_sf = -0.5 * np.sum(A * dK_d_log_sf)\n\n    # Gradient w.r.t. log(sigma_n)\n    dK_d_log_sn = 2 * sn2 * np.eye(n)\n    grad_log_sn = -0.5 * np.sum(A * dK_d_log_sn)\n    \n    # Gradients w.r.t. log(l_j) for j=1...p\n    grad_log_l = np.zeros(p)\n    # The term A*K is common inside the loop. Pre-compute it.\n    # No, it's not. The derivative is (K o D_j/l_j^2).\n    \n    for j in range(p):\n        # Pairwise squared distances for dimension j\n        sq_dist_j = squareform(pdist(X[:, j:j+1], 'sqeuclidean'))\n        \n        # Derivative of K w.r.t log(l_j) is K o (D_j / l_j^2)\n        dK_d_log_lj = K * (sq_dist_j / (l_vec[j]**2))\n        grad_log_l[j] = -0.5 * np.sum(A * dK_d_log_lj)\n\n    gradients = np.concatenate([[grad_log_sf], [grad_log_sn], grad_log_l])\n    \n    return nll, gradients\n\nsolve()\n```", "id": "3186634"}]}