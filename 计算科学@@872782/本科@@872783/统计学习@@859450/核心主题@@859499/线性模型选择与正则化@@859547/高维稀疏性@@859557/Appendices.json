{"hands_on_practices": [{"introduction": "在高维设定中，一个简单直接的初步步骤是根据特征与响应变量的个体相关性来进行特征筛选。这项基于“独立性筛选”（Sure Independence Screening, SIS）的练习，将让你亲手实现这一想法。更重要的是，你将发现当特征之间存在相关性时，这种看似直观的方法为何会失效，从而理解为何需要更复杂的模型 [@problem_id:3174653]。", "problem": "您将通过“独立性筛选”（Sure Independence Screening, SIS）来分析特征筛选，并评估在存在相关预测变量的情况下，按边际相关性对特征进行排序何时会失效。考虑以下高维稀疏线性模型：响应变量 $y \\in \\mathbb{R}$ 和 $p$ 维特征向量 $x \\in \\mathbb{R}^{p}$ 服从模型 $y = x^{\\top}\\beta + \\varepsilon$，其中 $\\beta \\in \\mathbb{R}^{p}$ 是稀疏的（只有一小部分条目非零），$x$ 的均值为零，协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{p \\times p}$，而 $\\varepsilon$ 是独立于 $x$ 的零均值噪声。独立性筛选（SIS）通过特征与 $y$ 的边际相关性大小对特征进行排序，并保留前 $d$ 个特征。\n\n从基本原理出发：使用协方差的定义以及 $\\varepsilon$ 和 $x$ 的独立性，用 $\\Sigma$ 和 $\\beta$ 来表示第 $j$ 个特征与响应之间的总体边际协方差。假设特征已经标准化，使得对所有 $j$ 都有 $\\operatorname{Var}(x_j) = 1$，并注意按相关性绝对值排序等价于按协方差绝对值排序（相差一个正常数）。对于相关参数为 $\\rho \\in [0,1)$ 的一阶自回归（AR(1)）高斯设计，其协方差矩阵是托普利茨（Toeplitz）矩阵，其元素为 $\\Sigma_{jk} = \\rho^{|j-k|}$。\n\n您的任务是实现一个程序，对于每个指定的测试用例，仅使用 $\\Sigma$ 和 $\\beta$ 计算所有 $p$ 个特征的总体边际相关性分数（不要模拟有限样本），按绝对分数选择前 $d$ 个特征（通过较小的特征索引来打破平局），并返回一个布尔值，指示所选集合是否包含真实支撑集（即，SIS 在该案例中是否具有确定筛选性质）。\n\n使用以下测试套件。在每种情况下，都有 $p = 1000$ 个特征，索引从 $0$ 到 $999$。对于参数为 $\\rho$ 的 AR(1) 设计，特征 $j$ 的总体分数为\n$$\nr_j \\propto \\sum_{k \\in S} \\beta_k \\, \\rho^{|j-k|},\n$$\n其中 $S$ 是非零系数的索引集。比例常数对于所有 $j$ 都是相同的，因此与排序无关。\n\n测试用例：\n\n- 案例 A（独立，成功）：$p = 1000$，$\\rho = 0$，支撑集 $S = \\{0, 200, 400, 600, 800\\}$，非零系数对于 $k \\in S$ 为 $\\beta_k = 3$，保留水平 $d = 5$。\n- 案例 B（中度相关，成功）：$p = 1000$，$\\rho = 0.3$，支撑集 $S = \\{0, 200, 400, 600, 800\\}$，非零系数对于 $k \\in S$ 为 $\\beta_k = 3$，保留水平 $d = 5$。\n- 案例 C（最近邻抑制，失败）：$p = 1000$，$\\rho = 0.9$，支撑集 $S = \\{10, 11\\}$，非零系数由 $\\beta_{10} = 1$ 和 $\\beta_{11} = -1/\\rho$ 给出，保留水平 $d = 5$。\n- 案例 D（次近邻抑制，失败）：$p = 1000$，$\\rho = 0.95$，支撑集 $S = \\{100, 102\\}$，非零系数由 $\\beta_{100} = 1$ 和 $\\beta_{102} = -1/\\rho^{2}$ 给出，保留水平 $d = 5$。\n\n程序要求：\n\n- 对于每个案例，计算向量 $r \\in \\mathbb{R}^{p}$，其条目为 $r_j = \\sum_{k \\in S} \\beta_k \\, \\rho^{|j-k|}$，并按 $|r_j|$ 对特征进行排序。\n- 按 $|r_j|$ 降序选择前 $d$ 个索引，平局时优先选择较小的索引。\n- 每个案例输出一个布尔值，指示是否满足 $S \\subseteq \\widehat{S}_d$，其中 $\\widehat{S}_d$ 是所选的 $d$ 个索引的集合。\n\n最终输出格式：\n\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表；例如，`[True,False,True,False]`。\n\n不涉及物理单位或角度。所有结果均为布尔值。程序必须完全自包含，无需用户输入或外部数据。", "solution": "该问题要求通过计算几种测试用例在 AR(1) 协方差结构下的总体水平边际相关性分数来分析独立性筛选（SIS）。我们必须确定在每种情况下真实的稀疏支撑集是否被恢复。\n\n首先，我们从基本原理推导第 $j$ 个特征 $x_j$ 和响应 $y$ 之间的总体边际协方差。模型由 $y = x^{\\top}\\beta + \\varepsilon$ 给出。特征向量 $x$ 的均值为零，即 $E[x] = 0$，其协方差矩阵为 $\\Sigma = E[xx^{\\top}]$。噪声项 $\\varepsilon$ 的均值为零，即 $E[\\varepsilon]=0$，且独立于 $x$。\n\n$x_j$ 和 $y$ 之间的协方差定义为：\n$$\n\\operatorname{Cov}(x_j, y) = E[x_j y] - E[x_j]E[y]\n$$\n由于 $E[x_j] = 0$（因为它是零均值向量 $x$ 的一个分量），第二项为零。我们通过代入 $y$ 的模型来计算第一项：\n$$\n\\operatorname{Cov}(x_j, y) = E[x_j y] = E[x_j (x^{\\top}\\beta + \\varepsilon)] = E[x_j x^{\\top}\\beta] + E[x_j \\varepsilon]\n$$\n由于 $x$ 和 $\\varepsilon$ 的独立性，我们有 $E[x_j \\varepsilon] = E[x_j]E[\\varepsilon] = 0 \\cdot 0 = 0$。剩余项可以展开为：\n$$\nE[x_j x^{\\top}\\beta] = E\\left[x_j \\sum_{k=1}^{p} x_k \\beta_k\\right]\n$$\n根据期望的线性性，我们可以将期望移到求和内部：\n$$\n\\sum_{k=1}^{p} E[x_j x_k] \\beta_k\n$$\n项 $E[x_j x_k]$ 是协方差矩阵 $\\Sigma$ 的第 $(j,k)$ 个元素的定义，记为 $\\Sigma_{jk}$。因此，边际协方差为：\n$$\n\\operatorname{Cov}(x_j, y) = \\sum_{k=1}^{p} \\Sigma_{jk} \\beta_k\n$$\n该表达式对应于向量乘积 $\\Sigma\\beta$ 的第 $j$ 个元素。\n\n接下来，我们建立按协方差和相关性排序的等价性。相关性定义为：\n$$\n\\operatorname{Corr}(x_j, y) = \\frac{\\operatorname{Cov}(x_j, y)}{\\sqrt{\\operatorname{Var}(x_j)\\operatorname{Var}(y)}}\n$$\n问题陈述中指出，特征是标准化的，使得对所有 $j$ 都有 $\\operatorname{Var}(x_j) = 1$。响应的方差 $\\operatorname{Var}(y)$ 对所有特征 $j$ 都是常数。具体来说，$\\operatorname{Var}(y) = \\operatorname{Var}(x^{\\top}\\beta + \\varepsilon) = \\operatorname{Var}(x^{\\top}\\beta) + \\operatorname{Var}(\\varepsilon) = \\beta^{\\top}\\Sigma\\beta + \\sigma^2_{\\varepsilon}$，这是一个正常数。因此，\n$$\n\\operatorname{Corr}(x_j, y) = \\frac{\\operatorname{Cov}(x_j, y)}{\\sqrt{\\operatorname{Var}(y)}}\n$$\n由于 $\\sqrt{\\operatorname{Var}(y)}$ 是一个不依赖于 $j$ 的正常数，按相关性绝对值 $|\\operatorname{Corr}(x_j, y)|$ 对特征进行排序等价于按协方差绝对值 $|\\operatorname{Cov}(x_j, y)|$ 进行排序。\n\n我们现在将此特化到 AR(1) 协方差结构，其中 $\\Sigma_{jk} = \\rho^{|j-k|}$，$\\rho \\in [0, 1)$。边际协方差变为：\n$$\n\\operatorname{Cov}(x_j, y) = \\sum_{k=1}^{p} \\rho^{|j-k|} \\beta_k\n$$\n由于系数向量 $\\beta$ 是稀疏的，其支撑集为 $S = \\{k \\mid \\beta_k \\neq 0\\}$，求和可以限制在 $k \\in S$ 上：\n$$\n\\operatorname{Cov}(x_j, y) = \\sum_{k \\in S} \\rho^{|j-k|} \\beta_k\n$$\n这正是问题陈述中给出的分数 $r_j$。我们将使用此公式来计算所有 $p$ 个特征的筛选分数。\n\n如果在给定情况下，真实的支撑集 $S$ 是筛选过程所选索引集 $\\widehat{S}_d$ 的子集，则认为该情况满足确定筛选性质。也就是说，我们必须检查是否 $S \\subseteq \\widehat{S}_d$。\n\n每个测试用例的算法如下：\n1.  定义参数 $p$、$\\rho$、$d$、支撑集 $S$ 和非零系数 $\\beta_k$。\n2.  构造分数向量 $r \\in \\mathbb{R}^p$，其元素为 $r_j = \\sum_{k \\in S} \\beta_k \\rho^{|j-k|}$。\n3.  计算绝对分数 $|r_j|$。\n4.  按绝对分数降序对特征索引 $j \\in \\{0, \\dots, p-1\\}$ 进行排序。平局时，优先选择较小的索引。\n5.  从此排序列表中选择前 $d$ 个索引，形成集合 $\\widehat{S}_d$。\n6.  验证是否 $S \\subseteq \\widehat{S}_d$。结果是一个布尔值。\n\n让我们简要分析一下测试用例：\n-   **案例 A 和 B**：真实支撑集 $S$ 中的特征分布得很开。对于 $\\rho=0$（案例 A），分数仅在 $S$ 中的特征上非零，保证了恢复。对于 $\\rho=0.3$（案例 B），相关性是中等的，预计 $S$ 中特征的分数将占主导地位，从而成功恢复。\n-   **案例 C**：在高相关性（$\\rho=0.9$）和 $S$ 中相邻特征具有相反符号系数（$\\beta_{10}=1$, $\\beta_{11}=-1/\\rho$）的情况下，预计会发生信号抵消效应。特征 $j=10$ 的分数为 $r_{10} = \\rho^{|10-10|}\\beta_{10} + \\rho^{|10-11|}\\beta_{11} = 1 \\cdot \\beta_{10} + \\rho \\cdot \\beta_{11} = 1 + \\rho(-1/\\rho) = 1-1=0$。由于其他特征将具有非零分数，特征 $10$ 将不会进入前 $d=5$ 名，导致 SIS 失败。\n-   **案例 D**：此案例创建了类似的抵消，但涉及次近邻。对于特征 $j=100$，分数为 $r_{100} = \\rho^{|100-100|}\\beta_{100} + \\rho^{|100-102|}\\beta_{102} = 1 \\cdot \\beta_{100} + \\rho^2 \\cdot \\beta_{102} = 1 + \\rho^2(-1/\\rho^2) = 1-1=0$。同样，SIS 将无法选择特征 $100$。\n\n实现将通过数值计算来执行这些计算，以确认结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Sure Independence Screening (SIS) problem for the given test cases.\n    \"\"\"\n    \n    p = 1000\n\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"p\": p,\n            \"rho\": 0.0,\n            \"S\": {0, 200, 400, 600, 800},\n            \"betas\": {0: 3, 200: 3, 400: 3, 600: 3, 800: 3},\n            \"d\": 5\n        },\n        {\n            \"name\": \"Case B\",\n            \"p\": p,\n            \"rho\": 0.3,\n            \"S\": {0, 200, 400, 600, 800},\n            \"betas\": {0: 3, 200: 3, 400: 3, 600: 3, 800: 3},\n            \"d\": 5\n        },\n        {\n            \"name\": \"Case C\",\n            \"p\": p,\n            \"rho\": 0.9,\n            \"S\": {10, 11},\n            \"betas\": {10: 1, 11: -1/0.9},\n            \"d\": 5\n        },\n        {\n            \"name\": \"Case D\",\n            \"p\": p,\n            \"rho\": 0.95,\n            \"S\": {100, 102},\n            \"betas\": {100: 1, 102: -1/(0.95**2)},\n            \"d\": 5\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p_val = case[\"p\"]\n        rho = case[\"rho\"]\n        S_set = case[\"S\"]\n        betas_dict = case[\"betas\"]\n        d = case[\"d\"]\n\n        # Ensure S_indices are sorted for consistent beta ordering\n        S_indices = np.array(sorted(list(S_set)))\n        betas_S = np.array([betas_dict[k] for k in S_indices])\n        \n        j_indices = np.arange(p_val)\n        scores = np.zeros(p_val, dtype=np.float64)\n\n        if rho == 0.0:\n            # For rho=0, Sigma is identity, so scores are just beta_j\n            scores[S_indices] = betas_S\n        else:\n            # Use broadcasting for efficiency:\n            # j_indices_col is (p, 1)\n            # S_indices is (len(S),)\n            # abs_diffs will be (p, len(S))\n            j_indices_col = j_indices[:, np.newaxis]\n            abs_diffs = np.abs(j_indices_col - S_indices)\n            \n            # rho_powers will be (p, len(S))\n            rho_powers = rho ** abs_diffs\n            \n            # Matrix-vector product to get scores (p, len(S)) x (len(S),) - (p,)\n            scores = rho_powers @ betas_S\n\n        abs_scores = np.abs(scores)\n        \n        # Sort indices: primary key -abs_scores (descending), secondary key j_indices (ascending for ties)\n        sorted_indices = np.lexsort((j_indices, -abs_scores))\n        \n        # Select top d features\n        selected_indices_set = set(sorted_indices[:d])\n        \n        # Check if true support is a subset of the selected set\n        is_recovered = S_set.issubset(selected_indices_set)\n        results.append(is_recovered)\n        \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3174653"}, {"introduction": "作为一种能够同时考虑所有特征的更稳健方法，LASSO 是高维稀疏建模的主力。这项练习将聚焦于 LASSO 解随着罚项参数 $\\lambda$ 变化的动态过程 [@problem_id:3174656]。你将通过编写坐标下降求解器，观察到一个令人意外的现象：一个特征可能先进入模型，之后又被剔除，这揭示了 LASSO 解路径并非总是单调的，尤其是在特征相关的情况下。", "problem": "您必须编写一个完整、可运行的程序，构建三个确定性线性回归实例，并分析最小绝对值收缩和选择算子 (lasso) 随惩罚参数变化的路径。核心目标是检验 lasso 路径上的支撑集单调性，构建一个变量进入活跃集后又离开的案例，并通过 Karush–Kuhn–Tucker (KKT) 条件来量化变化。所有计算都必须使用带有标准经验风险缩放的 lasso 目标函数进行。\n\nlasso 估计量定义为以下凸函数的最小化器\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\left\\{ \\frac{1}{2n} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 \\right\\},\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^n$，$\\beta \\in \\mathbb{R}^p$，且 $\\lambda \\ge 0$ 是正则化强度。本问题中所有矩阵和向量都是无量纲的；因此，不涉及物理单位。\n\n您必须实现一个坐标下降求解器，对于任意固定的 $\\lambda$，找到一个近似最小化器 $\\hat{\\beta}(\\lambda)$，然后必须验证 KKT 最优性条件。对于此目标函数，并且当 $X$ 的列被缩放以满足对所有 $j$ 都有 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ 时，KKT 条件陈述如下，其中残差为 $r = y - X \\hat{\\beta}$：\n- 如果 $\\hat{\\beta}_j \\ne 0$，则 $X_{\\cdot j}^{\\top} r / n = \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_j)$，\n- 如果 $\\hat{\\beta}_j = 0$，则 $\\lvert X_{\\cdot j}^{\\top} r / n \\rvert \\le \\lambda$。\n\n您的程序必须：\n1. 为每个测试用例生成一个 $\\lambda$ 值的网格，该网格是从 $\\lambda_{\\max}$ 到 $\\lambda_{\\min}$ 的递减序列，其中\n$$\n\\lambda_{\\max} \\equiv \\max_{1 \\le j \\le p} \\frac{\\lvert X_{\\cdot j}^{\\top} y \\rvert}{n},\n\\quad\n\\lambda_{\\min} \\equiv 0.01 \\,\\lambda_{\\max},\n$$\n总共有 40 个对数间隔的值，包含端点。\n2. 对于网格上的每个 $\\lambda$，使用热启动的循环坐标下降计算近似的 lasso 解 $\\hat{\\beta}(\\lambda)$，其中每个特征列都经过预先缩放以满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$（需要此缩放，以便闭式坐标更新是在水平 $\\lambda$ 上的软阈值操作）。\n3. 沿着路径 $\\{\\lambda_\\ell\\}_{\\ell=1}^{40}$（其中 $\\lambda_1 = \\lambda_{\\max} \\ge \\cdots \\ge \\lambda_{40} = \\lambda_{\\min}$），为每个 $\\ell$ 追踪活跃集 $A(\\lambda_\\ell) \\equiv \\{ j : \\hat{\\beta}_j(\\lambda_\\ell) \\ne 0 \\}$，使用数值阈值 $\\lvert \\hat{\\beta}_j \\rvert  10^{-8}$ 来判断非零。\n4. 对每个测试用例，计算：\n   - 一个布尔标志，指示活跃集是否是支撑集单调的，即对于每个特征索引 $j$，一旦 $j$ 在某个 $\\lambda_\\ell$ 处变为活跃，它在所有后续更小的惩罚参数下都保持活跃（也就是说，没有在 $\\lambda_\\ell$ 处非零的 $j$ 在之后的某个 $\\lambda_m$ 处（其中 $m  \\ell$）变为零）。\n   - 一个布尔标志，指示是否存在至少一个特征沿着路径进入模型后又离开，意味着存在索引 $\\ell  m$ 使得 $\\hat{\\beta}_j(\\lambda_\\ell) \\ne 0$ 且 $\\hat{\\beta}_j(\\lambda_m) = 0$。\n   - 整个路径上的最大 KKT 违反度，定义为\n     $$\n     \\max_{\\ell \\in \\{1,\\dots,40\\}} \\max_{j \\in \\{1,\\dots,p\\}} v_{j}(\\lambda_\\ell),\n     $$\n     其中\n     $$\n     v_j(\\lambda) \\equiv\n     \\begin{cases}\n     \\left| \\dfrac{X_{\\cdot j}^{\\top} r(\\lambda)}{n} - \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_j(\\lambda)) \\right|,  \\text{if } \\hat{\\beta}_j(\\lambda) \\ne 0, \\\\\n     \\max\\!\\left( 0, \\left| \\dfrac{X_{\\cdot j}^{\\top} r(\\lambda)}{n} \\right| - \\lambda \\right),  \\text{if } \\hat{\\beta}_j(\\lambda) = 0,\n     \\end{cases}\n     $$\n     且 $r(\\lambda) = y - X \\hat{\\beta}(\\lambda)$。\n\n测试套件规范。用给定的构造方法实现以下三个确定性案例。所有随机性必须使用种子 0 来确保确定性。\n\n- 案例 1（正交设计；预期支撑集单调）：\n  1. 设 $n = 12$ 和 $p = 4$。\n  2. 生成一个随机矩阵 $G \\in \\mathbb{R}^{n \\times p}$，其条目为独立标准正态分布，使用种子 0，并通过对 $G$ 进行薄 QR 分解获得其列的标准正交基 $Q$。\n  3. 设 $X = \\sqrt{n} \\, Q$，以使 $X^{\\top} X / n = I_p$。\n  4. 设 $y = X \\beta^{\\star}$，其中 $\\beta^{\\star} = (0.8, 0.4, 0, 0)^\\top$。\n\n- 案例 2（高共线性；创建一个进入后又离开的变量）：\n  1. 设 $n = 60$ 和 $p = 3$。使用种子 0 生成三个独立的高斯向量 $a, b, c \\in \\mathbb{R}^n$。\n  2. 通过 Gram–Schmidt 正交化 $a, b, c$ 以获得标准正交向量 $u, v, w$。\n  3. 定义原始特征 $x_2^{\\mathrm{raw}} = u$，$x_3^{\\mathrm{raw}} = v$ 和 $x_1^{\\mathrm{raw}} = u + v + 0.05\\, w$。\n  4. 缩放每一列以使 $\\lVert x_j \\rVert_2^2 = n$，即对 $j \\in \\{1,2,3\\}$ 有 $x_j = \\sqrt{n} \\, x_j^{\\mathrm{raw}} / \\lVert x_j^{\\mathrm{raw}} \\rVert_2$，并设 $X = [x_1, x_2, x_3]$。\n  5. 设 $y = u + v$。通过这种构造，$x_1$ 最初与 $y$ 的边际相关性最大，因此它倾向于首先进入，然而，对于足够小的 $\\lambda$，使用两个坐标的精确拟合 $y = x_2 + x_3$ 可能占主导地位，导致 $x_1$ 离开。\n\n- 案例 3（欠定情况 $p  n$；稀疏信号）：\n  1. 设 $n = 30$ 和 $p = 50$。\n  2. 使用种子 0 生成具有独立标准正态分布条目的 $X$，然后缩放每一列以满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$。\n  3. 定义一个稀疏向量 $\\beta^{\\star} \\in \\mathbb{R}^p$，在索引 1, 5, 10, 20, 30 处恰有 5 个非零值，分别为 $(1.5, -1.0, 0.8, 1.2, -0.7)$，其余均为零。\n  4. 设 $y = X \\beta^{\\star}$。\n\n算法要求：\n- 沿 $\\lambda$ 路径使用带热启动的循环坐标下降。对于满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ 的列归一化矩阵 $X$，索引 $j$ 的坐标更新由软阈值算子给出：\n$$\n\\beta_j \\leftarrow \\mathcal{S}\\!\\left( \\frac{X_{\\cdot j}^{\\top} r}{n} + \\beta_j, \\; \\lambda \\right),\n\\quad\n\\mathcal{S}(z,\\lambda) \\equiv \\mathrm{sign}(z)\\,\\max\\{ \\lvert z \\rvert - \\lambda, \\, 0 \\},\n$$\n其中 $r = y - X \\beta$ 是当前残差，更新之后接着执行 $r \\leftarrow r - X_{\\cdot j} \\, (\\beta_j^{\\mathrm{new}} - \\beta_j^{\\mathrm{old}})$。\n- 迭代进行多遍，直到两次连续完整遍之间的 $\\beta$ 无穷范数差低于 $10^{-9}$，或直到达到 $10{,}000$ 次迭代，以先发生者为准。使用最终的 $\\beta$ 进行 KKT 检查。\n\n输出内容：\n- 对于每个案例 $i \\in \\{1,2,3\\}$，返回一个列表 $[b_i, \\ell_i, \\kappa_i]$，其中 $b_i$ 是布尔型的支撑集单调性标志，$\\ell_i$ 是布尔型的进入后离开标志，$\\kappa_i$ 是整个路径上的最大 KKT 违反度，四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含三个测试用例的结果，格式为由逗号分隔的、每个用例一个列表的列表，并用方括号括起来。例如，所需格式的输出是\n\"[ [True,False,0.0],[False,True,0.000001],[True,False,0.0] ]\"\n但使用您为三个案例计算的值。\n\n不提供用户输入；您的程序必须能直接运行，并严格按照指定格式打印一行。", "solution": "我们从 lasso 的定义和 Karush–Kuhn–Tucker (KKT) 条件推导出算法及其原理，然后设计测试用例，以探究活跃集在递减惩罚路径上的单调性。\n\n基础理论。lasso 估计量最小化\n$$\nF(\\beta;\\lambda) \\equiv \\frac{1}{2n} \\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n该函数在 $\\beta$ 上是凸的。对于任何具有可分离不可微项的凸函数，每个坐标采用精确一维最小化的坐标下降法会收敛到全局最优解。对于带有满足 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$ 的标准化列的最小二乘项，坐标 $j$ 上的一维子问题简化为软阈值操作。具体来说，令 $r = y - X \\beta$ 表示残差，并认识到\n$$\n\\frac{1}{2n} \\lVert y - X \\beta \\rVert_2^2\n=\n\\frac{1}{2n} \\lVert r + X_{\\cdot j} \\beta_j \\rVert_2^2\n=\n\\frac{1}{2n} \\lVert r \\rVert_2^2 + \\frac{1}{n} \\beta_j X_{\\cdot j}^{\\top} r + \\frac{1}{2n} \\lVert X_{\\cdot j} \\rVert_2^2 \\beta_j^2,\n$$\n因此，当 $\\beta_{-j}$ 固定时，逐坐标的目标函数是\n$$\n\\phi(\\beta_j) = \\frac{1}{2} \\beta_j^2 - \\left(\\frac{X_{\\cdot j}^{\\top} r}{n} \\right) \\beta_j + \\lambda \\lvert \\beta_j \\rvert + \\text{constant},\n$$\n使用了 $\\lVert X_{\\cdot j} \\rVert_2^2 / n = 1$。该函数由软阈值更新最小化\n$$\n\\beta_j \\leftarrow \\mathcal{S}\\!\\left( \\frac{X_{\\cdot j}^{\\top} r}{n} + \\beta_j, \\; \\lambda \\right),\n\\quad\n\\mathcal{S}(z,\\lambda) = \\mathrm{sign}(z)\\,\\max\\{ \\lvert z \\rvert - \\lambda, \\, 0 \\}.\n$$\n更新 $\\beta_j$ 后，我们通过以下方式精确更新残差\n$$\nr \\leftarrow r - X_{\\cdot j} \\, (\\beta_j^{\\mathrm{new}} - \\beta_j^{\\mathrm{old}}).\n$$\n\nKKT 最优性条件。对于带有绝对值惩罚项的复合目标函数，一阶最优性条件表明 0 必须属于其次微分：\n$$\n0 \\in - \\frac{1}{n} X^{\\top} (y - X \\hat{\\beta}) + \\lambda \\, \\partial \\lVert \\hat{\\beta} \\rVert_1.\n$$\n记 $r = y - X \\hat{\\beta}$ 并表示绝对值的次梯度\n$$\n\\partial \\lvert t \\rvert =\n\\begin{cases}\n\\{\\mathrm{sign}(t)\\},  t \\ne 0,\\\\\n[-1,1],  t = 0,\n\\end{cases}\n$$\n我们得到逐坐标的条件：\n- 如果 $\\hat{\\beta}_j \\ne 0$，则 $X_{\\cdot j}^{\\top} r / n = \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_j)$，\n- 如果 $\\hat{\\beta}_j = 0$，则 $\\lvert X_{\\cdot j}^{\\top} r / n \\rvert \\le \\lambda$。\n\n这些条件既可以作为我们数值求解器的正确性检查，也可以作为解读路径上支撑集变化的视角。\n\n路径与热启动。在 $\\lambda_{\\max} \\equiv \\max_j \\lvert X_{\\cdot j}^{\\top} y \\rvert / n$ 时，零向量 $\\beta = 0$ 满足 KKT 条件：$r = y$ 且根据构造对每个 $j$ 都有 $\\lvert X_{\\cdot j}^{\\top} r / n \\rvert \\le \\lambda_{\\max}$。随着 $\\lambda$ 减小，解路径是分段线性的，当变量的 KKT 不等式变为紧约束时，新变量可能进入活跃集。在每个连续的 $\\lambda$ 处，使用前一个 $\\hat{\\beta}$ 对坐标下降进行热启动，有助于快速收敛并追踪路径。\n\n支撑集单调性与离开事件。在 $X^{\\top} X / n = I$ 的正交设计中，问题在坐标上解耦，解为\n$$\n\\hat{\\beta}_j(\\lambda) = \\mathcal{S}\\!\\left( \\frac{X_{\\cdot j}^{\\top} y}{n}, \\lambda \\right),\n$$\n因此一旦 $\\hat{\\beta}_j$ 随着 $\\lambda$ 减小变为非零，它对于所有更小的 $\\lambda$ 都保持非零。在一般的相关设计中，lasso 路径不一定是单调的：一个变量可能因为高的边际相关性而早期进入，但随后在较小的 $\\lambda$ 值下被其他相关的预测变量所取代，因为后者们能更有效地集体降低目标函数。在这种情况下，存在索引 $\\ell  m$ 使得 $\\hat{\\beta}_j(\\lambda_\\ell) \\ne 0$ 且 $\\hat{\\beta}_j(\\lambda_m) = 0$；这是一个离开事件，构成了对单调性的违反。\n\n测试用例设计。\n\n- 案例 1 构造一个具有标准正交列的矩阵 $X$，以使 $X^{\\top} X / n = I_p$，并将 $y$ 设置为一个稀疏线性组合。由于解耦，支撑集单调性应该成立，并且不会发生离开事件。\n\n- 案例 2 构造了三个近似共线的特征：$x_2$ 和 $x_3$ 是标准正交的，而 $x_1$ 近似为它们的和加上一个微小的扰动。响应 $y = u + v$ 等于与 $x_2$ 和 $x_3$ 对齐的和。对于大的 $\\lambda$，所有系数都为零。对于中等大的 $\\lambda$，$x_1$ 与 $y$ 的边际相关性最大，因此它倾向于首先进入。然而，对于足够小的 $\\lambda$，使用 $x_2$ 和 $x_3$ 可以精确拟合 $y$ 且残差为零，这使得为了减少惩罚项而放弃 $x_1$ 同时保持良好拟合变得有利；这就实现了 $x_1$ 的进入后离开事件。离开点处的 KKT 转换反映了，在 $\\hat{\\beta}_1$ 达到零的时刻，其平稳性条件从等式 $X_{\\cdot 1}^{\\top} r / n = \\lambda \\,\\mathrm{sign}(\\hat{\\beta}_1)$ 变为不等式 $\\lvert X_{\\cdot 1}^{\\top} r / n \\rvert \\le \\lambda$。\n\n- 案例 3 是一个 $p  n$ 的欠定问题，带有一个稀疏的真实信号。这用于检查求解器在高维设置下的鲁棒性和 KKT 条件的遵守情况，在这种设置下，许多非活跃坐标必须满足 KKT 不等式条件。支撑集单调性可能成立也可能不成立，取决于列之间的相关性。\n\n算法细节与正确性检查。\n\n- 列缩放。我们缩放每一列 $X_{\\cdot j}$ 以使 $\\lVert X_{\\cdot j} \\rVert_2^2 = n$，这使得逐坐标的二次项具有单位曲率，并验证了在水平 $\\lambda$ 上的软阈值坐标更新的有效性。\n\n- 收敛。我们执行循环遍，直到两次完整遍之间 $\\beta$ 的最大绝对变化降至 $10^{-9}$ 以下或达到 $10{,}000$ 遍。热启动和残差维护确保了效率。\n\n- KKT 验证。对于每个 $\\lambda$ 及其收敛的 $\\hat{\\beta}$，我们计算\n$$\ng_j(\\lambda) \\equiv \\frac{X_{\\cdot j}^{\\top} r(\\lambda)}{n}, \\quad r(\\lambda) = y - X \\hat{\\beta}(\\lambda),\n$$\n并定义逐坐标的违反度\n$$\nv_j(\\lambda) =\n\\begin{cases}\n\\left| g_j(\\lambda) - \\lambda \\, \\mathrm{sign}(\\hat{\\beta}_j(\\lambda)) \\right|,  \\hat{\\beta}_j(\\lambda) \\ne 0,\\\\\n\\max\\left( 0, \\lvert g_j(\\lambda) \\rvert - \\lambda \\right),  \\hat{\\beta}_j(\\lambda) = 0.\n\\end{cases}\n$$\n在所有 $\\lambda$ 和 $j$ 上的最大违反度在数值上应该很小（与收敛容差在同一数量级），从而证明其接近最优。\n\n- 单调性与离开检测。对于一个 $\\lambda$ 序列 $\\lambda_1  \\cdots  \\lambda_{40}$ 及其对应的解，我们记录一个布尔活跃度矩阵，指示 $\\lvert \\hat{\\beta}_j(\\lambda_\\ell) \\rvert  10^{-8}$ 是否成立。如果对于每个 $j$，一旦在某个 $\\ell$ 处变为活跃，它在所有后续的索引 $m  \\ell$ 处都保持活跃，则支撑集单调性成立。如果存在 $j$ 和索引 $\\ell  m$ 使得 $\\hat{\\beta}_j(\\lambda_\\ell) \\ne 0$ 且 $\\hat{\\beta}_j(\\lambda_m) = 0$，则检测到离开事件。\n\n输出。对于三个案例中的每一个，我们输出 $[b_i, \\ell_i, \\kappa_i]$，其中 $b_i$ 是支撑集单调性标志，$\\ell_i$ 是离开事件标志，$\\kappa_i$ 是最大 KKT 违反度，四舍五入到六位小数。程序打印一行，其中包含一个由这三个列表组成的列表。\n\n这种设计直接检验了正交设计产生单调支撑集，而相关设计可能产生非单调路径（变量进入后又离开）的理论预期。基于 KKT 的违反度度量标准定量地验证了整个路径上计算出的解。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef soft_threshold(z, lam):\n    if z  lam:\n        return z - lam\n    elif z  -lam:\n        return z + lam\n    else:\n        return 0.0\n\ndef coordinate_descent_lasso(X, y, lam, beta_init=None, tol=1e-9, max_sweeps=10000):\n    \"\"\"\n    Solve min (1/(2n))||y - X b||^2 + lam * ||b||_1\n    Requires columns of X scaled so that ||X_j||^2 = n for all j.\n    Uses cyclic coordinate descent with residual updates and warm start.\n\n    Returns beta, residual, and a dict with convergence info.\n    \"\"\"\n    n, p = X.shape\n    if beta_init is None:\n        beta = np.zeros(p, dtype=float)\n    else:\n        beta = beta_init.copy()\n\n    r = y - X @ beta  # residual\n    prev_beta = beta.copy()\n    for sweep in range(max_sweeps):\n        max_change = 0.0\n        for j in range(p):\n            # Compute z_j = X_j^T r / n + beta_j\n            zj = (X[:, j].dot(r)) / n + beta[j]\n            new_bj = soft_threshold(zj, lam)\n            delta = new_bj - beta[j]\n            if delta != 0.0:\n                r -= X[:, j] * delta\n                beta[j] = new_bj\n                chg = abs(delta)\n                if chg  max_change:\n                    max_change = chg\n        if max_change  tol:\n            break\n        prev_beta[:] = beta\n    info = {\"sweeps\": sweep + 1, \"max_change\": max_change}\n    return beta, r, info\n\ndef kkt_max_violation(X, y, beta, lam):\n    \"\"\"\n    Compute maximum KKT violation for lasso with scaling 1/(2n)||y - Xb||^2 + lam ||b||_1\n    Columns of X must satisfy ||X_j||^2 = n.\n    \"\"\"\n    n, p = X.shape\n    r = y - X @ beta\n    g = (X.T @ r) / n  # X^T r / n\n    violations = np.empty(p, dtype=float)\n    active = np.abs(beta)  1e-12\n    # Active: |g_j - lam sign(bj)|; Inactive: max(0, |g_j| - lam)\n    if np.any(active):\n        violations[active] = np.abs(g[active] - lam * np.sign(beta[active]))\n    if np.any(~active):\n        violations[~active] = np.maximum(0.0, np.abs(g[~active]) - lam)\n    return float(np.max(violations))\n\ndef lambda_path(X, y, L=40, min_ratio=0.01):\n    n, p = X.shape\n    # lambda_max = max_j |X_j^T y|/n (so beta=0 satisfies KKT)\n    lam_max = float(np.max(np.abs(X.T @ y)) / n)\n    lam_min = lam_max * min_ratio\n    # Log-spaced decreasing path including endpoints\n    path = np.exp(np.linspace(np.log(lam_max), np.log(lam_min), L))\n    return path\n\ndef standardize_columns_to_n(X):\n    # Scale columns so that ||X_j||^2 = n\n    n, p = X.shape\n    Xs = X.copy().astype(float)\n    for j in range(p):\n        cj = np.linalg.norm(Xs[:, j])\n        if cj == 0:\n            continue\n        # want cj_new^2 = n - scale by sqrt(n)/cj\n        Xs[:, j] *= (np.sqrt(n) / cj)\n    return Xs\n\ndef orthonormal_columns_from_gaussian(n, p, rng):\n    G = rng.standard_normal((n, p))\n    # Thin QR\n    Q, _ = np.linalg.qr(G)\n    # Ensure exactly p columns\n    Q = Q[:, :p]\n    return Q\n\ndef gram_schmidt_columns(A):\n    # Orthonormalize columns of A using classical Gram-Schmidt with re-orthogonalization\n    n, p = A.shape\n    Q = np.zeros_like(A, dtype=float)\n    for j in range(p):\n        v = A[:, j].astype(float).copy()\n        for _ in range(2):  # re-orthogonalize twice for stability\n            for k in range(j):\n                proj = np.dot(Q[:, k], v) * Q[:, k]\n                v = v - proj\n        normv = np.linalg.norm(v)\n        if normv  1e-14:\n            # If degenerate, fill with a random orthogonal direction deterministically\n            # Here we fallback to a standard basis vector not in the span if possible\n            e = np.zeros(n)\n            idx = j % n\n            e[idx] = 1.0\n            v = e.copy()\n            # re-orthogonalize\n            for k in range(j):\n                v -= np.dot(Q[:, k], v) * Q[:, k]\n            normv = np.linalg.norm(v)\n        Q[:, j] = v / normv\n    return Q\n\ndef analyze_case(X, y, L=40):\n    n, p = X.shape\n    # Standardize columns so ||X_j||^2 = n\n    Xs = standardize_columns_to_n(X)\n    # Build lambda path\n    lam_seq = lambda_path(Xs, y, L=L, min_ratio=0.01)\n    # Warm-start coordinate descent\n    beta = np.zeros(p, dtype=float)\n    active_matrix = np.zeros((L, p), dtype=bool)\n    max_kkt = 0.0\n    for t, lam in enumerate(lam_seq):\n        beta, r, info = coordinate_descent_lasso(Xs, y, lam, beta_init=beta, tol=1e-9, max_sweeps=10000)\n        # Record activity\n        active = np.abs(beta)  1e-8\n        active_matrix[t, :] = active\n        # KKT violation\n        kkt_v = kkt_max_violation(Xs, y, beta, lam)\n        if kkt_v  max_kkt:\n            max_kkt = kkt_v\n    # Monotonicity: once active, stays active\n    monotone = True\n    leaving = False\n    for j in range(p):\n        act = active_matrix[:, j]\n        # If there's a pattern True then later False - leaving and non-monotone\n        first_active_idx_arr = np.where(act)[0]\n        if len(first_active_idx_arr) > 0:\n            first_active = first_active_idx_arr[0]\n            # If any later inactive exists\n            if np.any(~act[first_active:]):\n                monotone = False\n                leaving = True\n                break\n    return monotone, leaving, round(max_kkt, 6)\n\ndef build_case_1():\n    # Case 1: Orthogonal design via QR\n    rng = np.random.default_rng(0)\n    n, p = 12, 4\n    Q = orthonormal_columns_from_gaussian(n, p, rng)\n    X = np.sqrt(n) * Q  # ensures X^T X / n = I\n    beta_star = np.array([0.8, 0.4, 0.0, 0.0])\n    y = X @ beta_star\n    return X, y\n\ndef build_case_2():\n    # Case 2: Highly collinear construction with enter-then-leave behavior\n    rng = np.random.default_rng(0)\n    n, p = 60, 3\n    a = rng.standard_normal(n)\n    b = rng.standard_normal(n)\n    c = rng.standard_normal(n)\n    U = gram_schmidt_columns(np.column_stack([a, b, c]))\n    u = U[:, 0]; v = U[:, 1]; w = U[:, 2]\n    x2_raw = u.copy()\n    x3_raw = v.copy()\n    x1_raw = u + v + 0.05 * w\n    # Stack and scale to ||x_j||^2 = n\n    Xraw = np.column_stack([x1_raw, x2_raw, x3_raw])\n    # Scale each raw column to norm sqrt(n)\n    X = standardize_columns_to_n(Xraw)\n    # Response\n    y = u + v\n    return X, y\n\ndef build_case_3():\n    # Case 3: Underdetermined, sparse signal\n    rng = np.random.default_rng(0)\n    n, p = 30, 50\n    X = rng.standard_normal((n, p))\n    X = standardize_columns_to_n(X)\n    beta_star = np.zeros(p)\n    nonzero_idx = [0, 4, 9, 19, 29] # Problem says 1,5,10,20,30 (1-based)\n    beta_vals = [1.5, -1.0, 0.8, 1.2, -0.7]\n    for idx, val in zip(nonzero_idx, beta_vals):\n        beta_star[idx] = val\n    y = X @ beta_star\n    return X, y\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [\n        build_case_1(),\n        build_case_2(),\n        build_case_3(),\n    ]\n    results = []\n    for (X, y) in cases:\n        monotone, leaving, max_kkt = analyze_case(X, y, L=40)\n        results.append([monotone, leaving, max_kkt])\n    # Final print statement in the exact required format.\n    # Print as a single list of per-case lists; booleans and floats.\n    def format_case(case):\n        # case is [bool, bool, float]\n        b1, b2, f = case\n        return f\"[{str(b1)},{str(b2)},{f:.6f}]\"\n    out = \"[\" + \",\".join(format_case(c) for c in results) + \"]\"\n    print(out)\n\nsolve()\n```", "id": "3174656"}, {"introduction": "前面的练习引发了一个更根本的问题：在什么条件下，我们能保证 LASSO 正确识别出真实的稀疏模型？本练习将引入“不可分条件”（irrepresentable condition），这是一个关键的理论概念 [@problem_id:3174691]。通过一个精心设计的模拟实验，你将亲眼见证违反此条件如何导致 LASSO 在样本量很大时仍然会选择错误的特征，从而将抽象的理论与具体的实践联系起来。", "problem": "您需要构建一个高维线性回归实例族，并以算法方式测试最小绝对收缩和选择算子 (LASSO) 何时能实现符号一致性，以及何时即使在样本量很大时也会失败。该任务关注不可表示条件在稀疏恢复中的作用。请完全在纯数学环境中进行操作（无物理单位）。您的程序必须实现所有计算，并按如下规定生成最终结果。\n\n考虑一个线性模型，其响应向量为 $y \\in \\mathbb{R}^n$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中列 $x_j \\in \\mathbb{R}^n$ 已标准化为经验方差为1，以及一个未知的稀疏向量 $\\beta^\\star \\in \\mathbb{R}^p$。对于惩罚项 $\\lambda \\ge 0$，LASSO 估计量 $\\hat{\\beta}(\\lambda)$ 定义为以下公式的最小化器\n$$\n\\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1.\n$$\n定义活动集（支撑集）$S = \\{ j : \\beta^\\star_j \\ne 0 \\}$ 和非活动集 $S^c$。如果对于给定的数据集，存在某个 $\\lambda$ 使得 $\\mathrm{sign}(\\hat{\\beta}_S(\\lambda)) = \\mathrm{sign}(\\beta^\\star_S)$ 且 $\\hat{\\beta}_{S^c}(\\lambda) = 0$ 精确成立，则称 LASSO 是符号一致的。\n\n使用的基本依据和定义：\n- LASSO 的 Karush–Kuhn–Tucker (KKT) 条件指出，在最优解 $\\hat{\\beta}$ 处，次梯度平稳性为\n$$\n\\frac{1}{n} X^\\top (y - X \\hat{\\beta}) = \\lambda z,\n$$\n其中 $z \\in \\partial \\lVert \\hat{\\beta} \\rVert_1$ 满足：如果 $\\hat{\\beta}_j \\ne 0$，则 $z_j = \\mathrm{sign}(\\hat{\\beta}_j)$；如果 $\\hat{\\beta}_j = 0$，则 $z_j \\in [-1,1]$。\n- 总体 Gram (协方差) 矩阵为 $\\Sigma = \\mathbb{E}[X^\\top X / n]$，对于索引集 $A,B$，块 $\\Sigma_{A,B}$ 是行在 $A$ 中、列在 $B$ 中的子矩阵。\n- 不可表示条件是\n$$\n\\left\\lVert \\Sigma_{S^c,S} \\, \\Sigma_{S,S}^{-1} \\, \\mathrm{sign}(\\beta^\\star_S) \\right\\rVert_\\infty  1.\n$$\n当左侧项至少为1时，该条件被违背。\n\n数据生成模型。对于每个测试用例，您必须按如下方式生成 $X$ 和 $y$。\n- 固定 $p = 6$ 和 $S = \\{1,2\\}$，其中 $\\beta^\\star_1 = \\beta^\\star_2 = b$，对于 $j \\in \\{3,4,5,6\\}$，$\\beta^\\star_j = 0$。使用 $b = 1$。\n- 给定 $\\rho \\in (0,1)$ 和 $\\alpha \\in (0,1)$。通过对具有独立同分布条目的潜在标准正态向量 $u_1, u_2, u_3 \\in \\mathbb{R}^n$进行采样来构建 $X$ 的前三列，并设置：\n  - $x_1 = u_1$,\n  - $x_2 = \\rho \\, u_1 + \\sqrt{1-\\rho^2} \\, u_2$,\n  - $c = \\alpha / (1+\\rho)$ 且 $v_3 = 1 - 2 \\alpha^2/(1+\\rho)$；则 $x_3 = c (x_1 + x_2) + \\sqrt{\\max(v_3, 10^{-12})}\\, u_3$。\n  这确保了总体相关性满足 $\\mathbb{E}[x_1^\\top x_1 / n] = \\mathbb{E}[x_2^\\top x_2 / n] = \\mathbb{E}[x_3^\\top x_3 / n] = 1$，$\\mathbb{E}[x_1^\\top x_2 / n] = \\rho$，以及 $\\mathbb{E}[x_3^\\top x_1 / n] = \\mathbb{E}[x_3^\\top x_2 / n] = \\alpha$。\n- 对于第4到第6列，采样独立的标准正态向量 $x_j$，其条目独立同分布，且独立于 $u_1, u_2, u_3$。\n- 将 $X$ 的每一列中心化，并标准化为经验方差为1；即，在中心化后，将每列除以 $\\sqrt{(1/n)\\sum_{i=1}^n x_{ij}^2}$。\n- 生成无噪声响应 $y = X \\beta^\\star$。使用零噪声以隔离结构性效应。\n\n该模型中的不可表示条件。当 $S = \\{1,2\\}$ 且符号为 $\\mathrm{sign}(\\beta^\\star_S) = (1,1)$ 时，总体 Gram 子矩阵满足：\n$$\n\\Sigma_{S,S} = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}, \\qquad\n\\Sigma_{\\{3\\},S} = \\begin{pmatrix} \\alpha  \\alpha \\end{pmatrix}.\n$$\n直接计算得出标量\n$$\n\\left\\lVert \\Sigma_{\\{3\\},S} \\, \\Sigma_{S,S}^{-1} \\, \\mathrm{sign}(\\beta^\\star_S) \\right\\rVert_\\infty\n= \\frac{2 \\alpha}{1+\\rho}.\n$$\n因此，当且仅当 $\\frac{2\\alpha}{1+\\rho} \\ge 1$ 时，不可表示条件被违背。\n\n算法要求。为 LASSO 目标函数实现一个坐标下降求解器\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\; \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1,\n$$\n使用 $G = X^\\top X/n$ 和 $c = X^\\top y/n$ 的 Gram 形式，以及软阈值更新。对于在 $\\lambda_{\\max}$ 和 $\\lambda_{\\min}$ 之间呈对数间隔的一系列 $\\lambda$ 值，其中\n$$\n\\lambda_{\\max} = \\lVert c \\rVert_\\infty, \\qquad \\lambda_{\\min} = 10^{-3} \\lambda_{\\max},\n$$\n对每个 $\\lambda$ 使用热启动计算 $\\hat{\\beta}(\\lambda)$，并确定是否存在任何 $\\lambda$ 使其解与 $\\beta^\\star$ 符号一致。\n\n测试套件。使用以下四个测试用例，每个用例由 $(n, p, \\rho, \\alpha, b, \\text{seed})$ 指定，其中 $p=6$ 且 $b=1$：\n- 用例 A (违背，大边界)：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.8, b = 1, \\text{seed} = 123)$。此处 $\\frac{2\\alpha}{1+\\rho} = \\frac{1.6}{1.5} \\approx 1.066\\ge 1$。\n- 用例 B (不违背)：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.3, b = 1, \\text{seed} = 456)$。此处 $\\frac{2\\alpha}{1+\\rho} = \\frac{0.6}{1.5} = 0.4  1$。\n- 用例 C (近边界，略低于边界)：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.74, b = 1, \\text{seed} = 789)$。此处 $\\frac{2\\alpha}{1+\\rho} \\approx 0.9867  1$。\n- 用例 D (近边界，略高于边界)：$(n = 3000, p = 6, \\rho = 0.5, \\alpha = 0.755, b = 1, \\text{seed} = 321)$。此处 $\\frac{2\\alpha}{1+\\rho} \\approx 1.0067 \\ge 1$。\n\n要求输出。对于每个用例，返回一个布尔值，指示在网格上是否存在至少一个 $\\lambda$，使得 LASSO 解与 $\\beta^\\star$ 符号一致（如果存在这样的 $\\lambda$，则为 True，否则为 False）。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按 A、B、C、D 的顺序显示结果，例如：\"[False,True,True,False]\"。\n\n所有随机性必须由提供的种子控制，以确保可复现性。角度单位和物理单位不适用。百分比不适用。答案是布尔值。", "solution": "该问题定义明确、科学合理，并为一项计算实验提供了完整的规范，以测试 LASSO 估计量的稀疏恢复理论的预测。理论背景，包括不可表示条件及其在给定数据模型下的具体形式，都得到了正确陈述。数据生成过程明确且可通过算法实现。任务是数值验证 LASSO 在不可表示条件成立与被违背的情况下是否能实现符号一致性。\n\n解决方案通过为四个测试用例中的每一个实施指定的程序来进行。针对一系列正则化参数 $\\lambda$ 网格计算完整的 LASSO 路径，并检查每个得到的估计量 $\\hat{\\beta}(\\lambda)$ 的符号一致性。\n\n**1. 数据生成与预处理**\n\n对于每个测试用例，我们都给定参数 $(n, p, \\rho, \\alpha, b, \\text{seed})$。真实系数向量 $\\beta^\\star \\in \\mathbb{R}^p$ 是稀疏的，其中 $p=6$，非零项仅存在于活动集 $S=\\{1, 2\\}$ 中，且 $\\beta^\\star_1 = \\beta^\\star_2 = b = 1$。其余分量为零。\n\n$n \\times p$ 的设计矩阵 $X$ 按以下方式构建：\n- 我们首先生成三个独立同分布的潜在标准正态向量 $u_1, u_2, u_3 \\in \\mathbb{R}^n$。\n- $X$ 的前三列，记为 $x_1, x_2, x_3$，被构造成具有特定的总体相关性结构。\n  - $x_1 = u_1$\n  - $x_2 = \\rho \\, u_1 + \\sqrt{1-\\rho^2} \\, u_2$\n  - $x_3 = c (x_1 + x_2) + \\sqrt{\\max(v3, 10^{-12})}\\, u_3$，其中 $c = \\frac{\\alpha}{1+\\rho}$ 且 $v_3 = 1 - \\frac{2\\alpha^2}{1+\\rho}$。此构造专门用于设定总体相关性 $\\mathbb{E}[x_1^\\top x_2 / n] = \\rho$ 和 $\\mathbb{E}[x_1^\\top x_3 / n] = \\mathbb{E}[x_2^\\top x_3 / n] = \\alpha$，同时保持所有列的单位方差。\n- 其余列 $x_4, \\dots, x_6$ 作为独立的标准正态向量生成，使得它们在期望上与所有其他列不相关。\n- 然后，通过减去其均值来对所得矩阵 $X$ 的每一列进行中心化，并将其标准化以使经验方差为1。对于列 $x_j$，这意味着将其替换为 $(x_j - \\bar{x}_j) / \\sigma_j$，其中 $\\sigma_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\bar{x}_j)^2}$。\n- 最后，生成无噪声响应向量 $y = X \\beta^\\star$。\n\n**2. 通过坐标下降求解 LASSO**\n\nLASSO 估计量 $\\hat{\\beta}(\\lambda)$ 最小化目标函数：\n$$ L(\\beta; \\lambda) = \\frac{1}{2n}\\lVert y - X \\beta \\rVert_2^2 + \\lambda \\lVert \\beta \\rVert_1 $$\n我们采用坐标下降法，这是一种迭代优化算法，它一次只对一个系数最小化目标函数，而保持所有其他系数固定。此过程循环重复，直至收敛。\n\n目标函数可以用样本 Gram 矩阵 $G = X^\\top X/n$ 和向量 $c = X^\\top y/n$ 表示为：\n$$ L(\\beta; \\lambda) = \\frac{1}{2} \\beta^\\top G \\beta - c^\\top \\beta + \\lambda \\lVert \\beta \\rVert_1 + \\text{constant} $$\n$L$ 的光滑部分关于 $\\beta_j$ 的偏导数是 $(G\\beta)_j - c_j$。将次梯度设为零，得到坐标 $j$ 的平稳性条件：\n$$ 0 \\in (G\\beta)_j - c_j + \\lambda \\partial |\\beta_j|$$\n其中 $\\partial|\\beta_j|$ 是绝对值函数的次梯度。由于列标准化确保了 $G$ 的对角元素为 $G_{jj}=1$，我们可以写出 $(G\\beta)_j = \\beta_j + \\sum_{k \\ne j} G_{jk}\\beta_k$。$\\beta_j$ 的更新规则变为：\n$$ \\beta_j \\leftarrow \\mathcal{S}_\\lambda \\left( c_j - \\sum_{k \\ne j} G_{jk}\\beta_k \\right) $$\n其中 $\\mathcal{S}_\\lambda(\\cdot)$ 是软阈值算子，定义为 $\\mathcal{S}_\\lambda(z) = \\mathrm{sign}(z) \\max(|z| - \\lambda, 0)$。\n\n为了测试符号一致性，我们必须计算在一系列 $\\lambda$ 值上的解路径 $\\{\\hat{\\beta}(\\lambda)\\}$。我们定义了一个从 $\\lambda_{\\max} = \\lVert c \\rVert_\\infty$ 到 $\\lambda_{\\min} = 10^{-3} \\lambda_{\\max}$ 的100个对数间隔值的网格。我们从 $\\hat{\\beta}(\\lambda_{\\max})=\\mathbf{0}$ 开始，并将每个 $\\lambda$ 的解用作下一个较小 $\\lambda$ 的“热启动”，这显著加快了收敛速度。\n\n**3. 符号一致性验证**\n\n对于计算路径上的每个解 $\\hat{\\beta}(\\lambda)$，我们检查其与真实向量 $\\beta^\\star$ 的符号一致性。如果同时满足两个条件，则一个解是符号一致的：\n1.  **正确的支撑集恢复**：对应于非活动集 $S^c = \\{3, 4, 5, 6\\}$ 的系数必须全部精确为零。即，对于所有 $j \\in S^c$，$\\hat{\\beta}_j(\\lambda) = 0$。\n2.  **正确的符号恢复**：活动集 $S=\\{1, 2\\}$ 中系数的符号必须与真实系数的符号相匹配。由于 $\\beta^\\star_1 = \\beta^\\star_2 = 1$，这要求 $\\hat{\\beta}_1(\\lambda)  0$ 且 $\\hat{\\beta}_2(\\lambda)  0$。\n\n对于每个测试用例，我们遍历 $\\lambda$ 路径上的解。如果任何一个 $\\hat{\\beta}(\\lambda)$ 满足这两个条件，则认为该算法在该用例中实现了符号一致性，结果为 `True`。如果在遍历整个路径后没有找到这样的解，则结果为 `False`。此过程直接测试了符号一致性定义所要求的合适 $\\lambda$ 的存在性。结果预计将与基于不可表示条件的理论预测一致，该条件在用例 B 和 C 中成立，但在用例 A 和 D 中被违背。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the LASSO sign consistency experiment.\n    \"\"\"\n\n    def soft_threshold(rho, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(rho) * np.maximum(np.abs(rho) - lam, 0.)\n\n    def coordinate_descent_lasso(beta_init, G, c, lam, max_iter, tol):\n        \"\"\"\n        Coordinate descent for LASSO using the Gram matrix formulation.\n        \n        Args:\n            beta_init (np.ndarray): Initial guess for beta.\n            G (np.ndarray): Gram matrix (X'X)/n.\n            c (np.ndarray): Correlation vector (X'y)/n.\n            lam (float): Regularization parameter.\n            max_iter (int): Maximum number of cycles.\n            tol (float): Convergence tolerance.\n            \n        Returns:\n            np.ndarray: The estimated LASSO coefficients.\n        \"\"\"\n        p = len(beta_init)\n        beta = beta_init.copy()\n        \n        for _ in range(max_iter):\n            beta_old = beta.copy()\n            for j in range(p):\n                # Calculate rho_j = c_j - sum_{k != j} G_jk * beta_k\n                # G[j,j] is 1 due to standardization.\n                rho_j = c[j] - (np.dot(G[j, :], beta) - beta[j])\n                beta[j] = soft_threshold(rho_j, lam)\n            \n            if np.max(np.abs(beta - beta_old))  tol:\n                break\n                \n        return beta\n\n    def check_sign_consistency_for_case(n, p, rho, alpha, b, seed):\n        \"\"\"\n        Generates data for one case, runs the LASSO path, and checks for sign consistency.\n        \n        Args:\n            n, p, rho, alpha, b: Model parameters.\n            seed (int): Random seed for reproducibility.\n            \n        Returns:\n            bool: True if a sign-consistent solution is found, False otherwise.\n        \"\"\"\n        # 1. Set seed for reproducibility\n        np.random.seed(seed)\n        \n        # 2. Define true beta vector\n        beta_star = np.zeros(p)\n        beta_star[0] = b  # Corresponds to index 0\n        beta_star[1] = b  # Corresponds to index 1\n        \n        # 3. Generate data according to the specified model\n        u1 = np.random.randn(n)\n        u2 = np.random.randn(n)\n        u3 = np.random.randn(n)\n        \n        X = np.zeros((n, p))\n        X[:, 0] = u1\n        X[:, 1] = rho * u1 + np.sqrt(1 - rho**2) * u2\n        \n        const_c = alpha / (1 + rho)\n        v3 = 1 - 2 * alpha**2 / (1 + rho)\n        X[:, 2] = const_c * (X[:, 0] + X[:, 1]) + np.sqrt(max(v3, 1e-12)) * u3\n        \n        for j in range(3, p):\n            X[:, j] = np.random.randn(n)\n            \n        # 4. Center and standardize X to have empirical variance 1\n        X_mean = np.mean(X, axis=0)\n        X = X - X_mean\n        X_std = np.sqrt(np.mean(X**2, axis=0))\n        X_std[X_std  1e-9] = 1.0 # Avoid division by zero\n        X = X / X_std\n        \n        # 5. Generate noiseless response vector\n        y = X @ beta_star\n        \n        # 6. Set up for LASSO path computation\n        G = (X.T @ X) / n\n        c = (X.T @ y) / n\n        \n        lambda_max = np.max(np.abs(c))\n        if lambda_max  1e-9: # Handle edge case of zero correlation\n            return False # No coefficients will be selected.\n\n        lambda_min = 1e-3 * lambda_max\n        n_lambdas = 100\n        lambda_grid = np.logspace(np.log10(lambda_max), np.log10(lambda_min), num=n_lambdas)\n        \n        # 7. Run LASSO path and check for sign consistency\n        beta_hat = np.zeros(p)\n        sign_beta_star_S = np.sign(beta_star[:2])\n        found_consistent = False\n        \n        for lam in lambda_grid:\n            beta_hat = coordinate_descent_lasso(beta_hat, G, c, lam, max_iter=2000, tol=1e-9)\n            \n            # Check for exact zeros in the inactive set\n            inactive_is_zero = np.all(np.abs(beta_hat[2:])  1e-9)\n            \n            if inactive_is_zero:\n                # Check for correct signs in the active set\n                signs_hat_S = np.sign(beta_hat[:2])\n                active_signs_correct = np.all(signs_hat_S == sign_beta_star_S)\n                \n                if active_signs_correct:\n                    found_consistent = True\n                    break \n                    \n        return found_consistent\n\n    test_cases = [\n        # Case A: (n, p, rho, alpha, b, seed)\n        (3000, 6, 0.5, 0.8, 1, 123),\n        # Case B:\n        (3000, 6, 0.5, 0.3, 1, 456),\n        # Case C:\n        (3000, 6, 0.5, 0.74, 1, 789),\n        # Case D:\n        (3000, 6, 0.5, 0.755, 1, 321),\n    ]\n    \n    results = []\n    for case in test_cases:\n        result = check_sign_consistency_for_case(*case)\n        results.append(result)\n\n    # Format output as specified: [res_A,res_B,res_C,res_D]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3174691"}]}