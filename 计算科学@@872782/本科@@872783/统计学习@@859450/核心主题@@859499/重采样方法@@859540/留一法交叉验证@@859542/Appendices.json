{"hands_on_practices": [{"introduction": "本练习为理解留一法交叉验证（LOOCV）建立了一个重要的基准。通过为一个仅预测样本均值的简单模型推导LOOCV误差，你将揭示交叉验证误差与数据集内在变异性之间的直接数学联系 [@problem_id:1912461]。这提供了一个坚实的理论基础，阐明了在最简单的回归情境下LOOCV所度量的内容。", "problem": "在统计学习领域，交叉验证是一种基本技术，用于评估统计分析的结果在独立数据集上的泛化能力。一种常见的变体是留一法交叉验证（Leave-One-Out Cross-Validation, LOOCV）。\n\n考虑一个包含 $n$ 个观测值的数据集，$y_1, y_2, \\ldots, y_n$。我们希望评估一个非常简单的预测模型：对于任何给定的训练集，模型对新数据点的预测就是该训练集中观测值的算术平均值。\n\n该数据集的LOOCV过程包含 $n$ 次迭代。在第 $i$ 次迭代中（$i=1, \\ldots, n$），第 $i$ 个观测值 $y_i$ 被作为验证集，其余的 $n-1$ 个观测值被用作训练集。模型在这 $n-1$ 个观测值上进行训练，并对被留出的观测值 $y_i$ 做出预测。然后计算预测值与实际值 $y_i$ 之间的平方误差。\n\n你的任务是推导LOOCV均方误差（Mean Squared Error, MSE）的通用闭式表达式，该值是所有 $n$ 次迭代中这些平方误差的平均值。用观测值数量 $n$ 和整个数据集的样本方差 $s^2$ 来表示你的最终答案。样本方差定义为 $s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$，其中 $\\bar{y}$ 是所有 $n$ 个观测值的样本均值。", "solution": "我们考虑的预测模型输出的是训练集的算术平均值。设完整样本的均值为 $\\bar{y} = \\frac{1}{n}\\sum_{j=1}^{n} y_{j}$。在第 $i$ 次LOOCV迭代中，训练集排除了 $y_i$，因此留一法均值为\n$$\n\\bar{y}_{-i} = \\frac{1}{n-1}\\sum_{\\substack{j=1 \\\\ j\\neq i}}^{n} y_{j} = \\frac{n\\bar{y} - y_{i}}{n-1}.\n$$\n对留出的 $y_i$ 的预测误差为\n$$\ny_{i} - \\bar{y}_{-i} = y_{i} - \\frac{n\\bar{y} - y_{i}}{n-1} = \\frac{n(y_{i} - \\bar{y})}{n-1}.\n$$\n因此，第 $i$ 次迭代的平方误差为\n$$\n\\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{n^{2}}{(n-1)^{2}}(y_{i} - \\bar{y})^{2}.\n$$\nLOOCV均方误差（MSE）是这些平方误差在 $i=1,\\ldots,n$ 上的平均值：\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\sum_{i=1}^{n} \\left(y_{i} - \\bar{y}_{-i}\\right)^{2} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}.\n$$\n使用样本方差的定义 $s^{2} = \\frac{1}{n-1}\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2}$，我们代入 $\\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} = (n-1)s^{2}$ 得到\n$$\n\\text{LOOCV MSE} = \\frac{1}{n}\\cdot \\frac{n^{2}}{(n-1)^{2}} \\cdot (n-1) s^{2} = \\frac{n}{n-1} s^{2}.\n$$\n因此，对于以均值作为预测器的模型，其LOOCV均方误差为 $\\frac{n}{n-1}s^{2}$。", "answer": "$$\\boxed{\\frac{n}{n-1}s^{2}}$$", "id": "1912461"}, {"introduction": "从理论推导转向具体计算，本练习将揭开LOOCV过程的神秘面纱。你将使用k最近邻分类器，在一个小型的、假设的材料科学数据集上手动应用LOOCV [@problem_id:90086]。通过逐步完成每一折的计算，你将巩固对每个数据点如何轮流充当验证集的理解，从而在分类任务中对LOOCV的机制有更直观的感受。", "problem": "在计算材料科学领域，机器学习模型被越来越多地用于预测材料性质并加速新材料的发现。一个常见的任务是根据一组计算或实验特征对材料进行分类。\n\n考虑一个简化的分类问题，将二维材料分为“平庸绝缘体”（0类）或“拓扑绝缘体”（1类）。该分类基于两个特征：$f_1$，即每个原子的剥离能（单位：eV/原子），以及 $f_2$，即电子带隙（单位：eV）。我们将使用1-最近邻（1-NN）分类器来完成此任务。\n\n1-NN 算法通过将新数据点分配给其在训练数据中唯一的最近邻的类别来进行分类。“邻近度”由距离度量确定，我们将使用标准的欧几里得距离。对于特征空间中的两个点 $\\mathbf{p}=(p_1, p_2)$ 和 $\\mathbf{q}=(q_1, q_2)$，欧几里得距离为 $d(\\mathbf{p}, \\mathbf{q}) = \\sqrt{(p_1-q_1)^2 + (p_2-q_2)^2}$。\n\n为了在一个小型数据集上评估该分类器的性能，而不将其划分为单独的训练集和测试集，我们采用留一法交叉验证（LOOCV）。在LOOCV中，对于一个包含 $N$ 个点的数据集，我们执行 $N$ 次迭代。在每次迭代 $i$ 中，第 $i$ 个数据点被留出作为测试样本，模型在剩余的 $N-1$ 个数据点上进行训练。然后将对留出点的预测与其真实类别进行比较。总体准确率是正确分类的点的比例。\n\n您将获得以下包含五种二维材料的数据集：\n\n| 材料ID | 特征 $f_1$ | 特征 $f_2$ | 类别标签 |\n| :--- | :---: | :---: | :---: |\n| A | 1.0 | 1.0 | 0 |\n| B | 2.0 | 2.0 | 0 |\n| C | 5.0 | 5.0 | 1 |\n| D | 6.0 | 4.0 | 1 |\n| E | 2.5 | 2.5 | 1 |\n\n如果到最近邻的距离出现平局，则选择在数据集表格中出现较早的邻居（即，A在B之前，B在C之前，依此类推）。\n\n推导此数据集上1-NN分类器的留一法交叉验证（LOOCV）准确率。", "solution": "目标是计算给定包含 $N=5$ 种材料的数据集上，1-最近邻（1-NN）分类器的留一法交叉验证（LOOCV）准确率。准确率定义为：\n$$\n\\text{准确率} = \\frac{\\text{正确分类的样本数}}{\\text{样本总数}}\n$$\n\n我们将对数据集中的每种材料进行一次迭代，共执行 $N=5$ 次迭代。在每次迭代中，我们留出一种材料作为测试点，并将其余四种用作训练集。我们从训练集中找到距离测试点最近的邻居，并将其类别分配给测试点。\n\n为了找到最近的邻居，我们需要计算欧几里得距离。请注意，比较欧几里得距离的平方 $d^2 = (p_1-q_1)^2 + (p_2-q_2)^2$ 等同于比较距离本身，并且可以避免处理平方根。\n\n**第1次迭代：留出材料A**\n-   测试点：A = (1.0, 1.0)，真实类别 = 0\n-   训练集：{B, C, D, E}\n-   从A点出发的距离平方：\n    -   $d^2(A, B) = (1.0 - 2.0)^2 + (1.0 - 2.0)^2 = (-1.0)^2 + (-1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(A, C) = (1.0 - 5.0)^2 + (1.0 - 5.0)^2 = (-4.0)^2 + (-4.0)^2 = 16.0 + 16.0 = 32.0$\n    -   $d^2(A, D) = (1.0 - 6.0)^2 + (1.0 - 4.0)^2 = (-5.0)^2 + (-3.0)^2 = 25.0 + 9.0 = 34.0$\n    -   $d^2(A, E) = (1.0 - 2.5)^2 + (1.0 - 2.5)^2 = (-1.5)^2 + (-1.5)^2 = 2.25 + 2.25 = 4.5$\n-   最小距离平方为2.0，对应于材料B。\n-   最近邻是B。B的类别是0。\n-   对A的预测类别：0。\n-   结果：正确（真实值：0，预测值：0）。\n\n**第2次迭代：留出材料B**\n-   测试点：B = (2.0, 2.0)，真实类别 = 0\n-   训练集：{A, C, D, E}\n-   从B点出发的距离平方：\n    -   $d^2(B, A) = (2.0 - 1.0)^2 + (2.0 - 1.0)^2 = (1.0)^2 + (1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(B, C) = (2.0 - 5.0)^2 + (2.0 - 5.0)^2 = (-3.0)^2 + (-3.0)^2 = 9.0 + 9.0 = 18.0$\n    -   $d^2(B, D) = (2.0 - 6.0)^2 + (2.0 - 4.0)^2 = (-4.0)^2 + (-2.0)^2 = 16.0 + 4.0 = 20.0$\n    -   $d^2(B, E) = (2.0 - 2.5)^2 + (2.0 - 2.5)^2 = (-0.5)^2 + (-0.5)^2 = 0.25 + 0.25 = 0.5$\n-   最小距离平方为0.5，对应于材料E。\n-   最近邻是E。E的类别是1。\n-   对B的预测类别：1。\n-   结果：错误（真实值：0，预测值：1）。\n\n**第3次迭代：留出材料C**\n-   测试点：C = (5.0, 5.0)，真实类别 = 1\n-   训练集：{A, B, D, E}\n-   从C点出发的距离平方：\n    -   $d^2(C, A) = (5.0 - 1.0)^2 + (5.0 - 1.0)^2 = (4.0)^2 + (4.0)^2 = 16.0 + 16.0 = 32.0$\n    -   $d^2(C, B) = (5.0 - 2.0)^2 + (5.0 - 2.0)^2 = (3.0)^2 + (3.0)^2 = 9.0 + 9.0 = 18.0$\n    -   $d^2(C, D) = (5.0 - 6.0)^2 + (5.0 - 4.0)^2 = (-1.0)^2 + (1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(C, E) = (5.0 - 2.5)^2 + (5.0 - 2.5)^2 = (2.5)^2 + (2.5)^2 = 6.25 + 6.25 = 12.5$\n-   最小距离平方为2.0，对应于材料D。\n-   最近邻是D。D的类别是1。\n-   对C的预测类别：1。\n-   结果：正确（真实值：1，预测值：1）。\n\n**第4次迭代：留出材料D**\n-   测试点：D = (6.0, 4.0)，真实类别 = 1\n-   训练集：{A, B, C, E}\n-   从D点出发的距离平方：\n    -   $d^2(D, A) = (6.0 - 1.0)^2 + (4.0 - 1.0)^2 = (5.0)^2 + (3.0)^2 = 25.0 + 9.0 = 34.0$\n    -   $d^2(D, B) = (6.0 - 2.0)^2 + (4.0 - 2.0)^2 = (4.0)^2 + (2.0)^2 = 16.0 + 4.0 = 20.0$\n    -   $d^2(D, C) = (6.0 - 5.0)^2 + (4.0 - 5.0)^2 = (1.0)^2 + (-1.0)^2 = 1.0 + 1.0 = 2.0$\n    -   $d^2(D, E) = (6.0 - 2.5)^2 + (4.0 - 2.5)^2 = (3.5)^2 + (1.5)^2 = 12.25 + 2.25 = 14.5$\n-   最小距离平方为2.0，对应于材料C。\n-   最近邻是C。C的类别是1。\n-   对D的预测类别：1。\n-   结果：正确（真实值：1，预测值：1）。\n\n**第5次迭代：留出材料E**\n-   测试点：E = (2.5, 2.5)，真实类别 = 1\n-   训练集：{A, B, C, D}\n-   从E点出发的距离平方：\n    -   $d^2(E, A) = (2.5 - 1.0)^2 + (2.5 - 1.0)^2 = (1.5)^2 + (1.5)^2 = 2.25 + 2.25 = 4.5$\n    -   $d^2(E, B) = (2.5 - 2.0)^2 + (2.5 - 2.0)^2 = (0.5)^2 + (0.5)^2 = 0.25 + 0.25 = 0.5$\n    -   $d^2(E, C) = (2.5 - 5.0)^2 + (2.5 - 5.0)^2 = (-2.5)^2 + (-2.5)^2 = 6.25 + 6.25 = 12.5$\n    -   $d^2(E, D) = (2.5 - 6.0)^2 + (2.5 - 4.0)^2 = (-3.5)^2 + (-1.5)^2 = 12.25 + 2.25 = 14.5$\n-   最小距离平方为0.5，对应于材料B。\n-   最近邻是B。B的类别是0。\n-   对E的预测类别：0。\n-   结果：错误（真实值：1，预测值：0）。\n\n**最终准确率计算**\n我们总结结果如下：\n-   A：分类正确\n-   B：分类错误\n-   C：分类正确\n-   D：分类正确\n-   E：分类错误\n\n正确分类的样本总数为3。\n样本总数为5。\n\nLOOCV准确率为：\n$$\n\\text{准确率} = \\frac{3}{5}\n$$", "answer": "$$ \\boxed{\\frac{3}{5}} $$", "id": "90086"}, {"introduction": "本练习进入了一个更贴近现实且需要编码的场景，展示了交叉验证的一个关键优势。通过构建一个包含离群点的数据集，你将亲眼见证为何朴素的训练误差在模型选择上可能产生误导，以及LOOCV如何提供更稳健的性能评估 [@problem_id:3139300]。这个练习突出了LOOCV在选择一个能够良好泛化的模型（尤其是在存在噪声或异常数据点时）方面的实际效用。", "problem": "您将为$k$-近邻（k-NN）分类器实现并分析留一法交叉验证（LOOCV），并将其应用于为揭示单个极端离群点对模型选择的影响而构建的数据集。其目的是从第一性原理出发，证明为什么LOOCV可以选择一个较大的$k$来抵消离群点的影响，而训练重代入误差则倾向于选择$k=1$。从以下基本定义开始。\n\n假设有一个包含$N$个带标签点的数据集 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\{0,1\\}$。$k$-近邻分类器通过以下步骤为一个点预测标签：\n- 计算与所有候选邻居的欧几里得距离（在留一法交叉验证下，不包括查询点本身），\n- 选择距离最小的$k$个点（如果出现距离相等的情况，则确定性地通过较小的原始索引来打破平局），\n- 返回这$k$个邻居中获得多数票的类别（对于奇数$k$，这可以避免票数平局；如果仍然出现罕见的平局，则选择其邻居距离总和较小的类别来打破平局，如果仍然平局，则选择较小的类别标签）。\n\n将预测器$\\hat{f}$在数据集上的经验$0\\text{-}1$损失定义为$$\\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}\\{\\hat{f}(x_i) \\neq y_i\\},$$其中$\\mathbb{I}\\{\\cdot\\}$是指示函数。将训练重代入误差定义为通过使用包含$x_i$的同一数据集对每个$x_i$进行分类，并允许$x_i$被选为自身邻居之一时所测得的经验$0\\text{-}1$损失。将留一法交叉验证（LOOCV）误差定义为通过使用从训练集中排除了$x_i$的模型对每个$x_i$进行分类时所测得的经验$0\\text{-}1$损失。\n\n您的程序必须：\n1. 严格遵循上述规则，实现$k$-近邻分类、$\\mathbb{R}^2$中的欧几里得距离、训练重代入误差和留一法交叉验证误差。\n2. 对于下面测试套件中的每个数据集和候选值 $k \\in \\{1,3\\}$，计算：\n   - 最小化LOOCV误差的$k$值，如果多个$k$值产生相同的最小误差，则使用“选择最小的$k$”这一平局打破规则，\n   - 最小化训练重代入误差的$k$值，使用相同的平局打破规则。\n3. 生成单行输出，其中包含按顺序排列的每个数据集的结果，格式为方括号括起来的逗号分隔列表，其中每个数据集的结果本身是一个双元素列表 $[k_{\\text{LOOCV}}, k_{\\text{train}}]$。例如，输出应如下所示：$$[[k_1^{\\text{LOOCV}},k_1^{\\text{train}}],[k_2^{\\text{LOOCV}},k_2^{\\text{train}}],[k_3^{\\text{LOOCV}},k_3^{\\text{train}}]].$$\n\n测试套件（所有坐标都在$\\mathbb{R}^2$中，无物理单位）：\n- 情况A（一个极端离群点位于簇的中心）：\n  - 类别0的点：$(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$。\n  - 类别1的离群点：$(0,0)$。\n  - 候选k值：$\\{1,3\\}$。\n- 情况B（无离群点；分离良好的簇）：\n  - 类别0的点：$(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$。\n  - 类别1的点：$(10,0)$, $(-10,0)$, $(0,10)$, $(0,-10)$。\n  - 候选k值：$\\{1,3\\}$。\n- 情况C（一个极端离群点，远离所有其他点，但与其中一个簇共享标签）：\n  - 类别0的点：$(1,0)$, $(-1,0)$, $(0,1)$, $(0,-1)$, $(100,100)$。\n  - 类别1的点：$(3,0)$, $(3,1)$, $(4,0)$, $(4,1)$。\n  - 候选k值：$\\{1,3\\}$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，无空格，格式与$$[[k_A^{\\text{LOOCV}},k_A^{\\text{train}}],[k_B^{\\text{LOOCV}},k_B^{\\text{train}}],[k_C^{\\text{LOOCV}},k_C^{\\text{train}}]]$$完全一致。输出中的数字必须是整数。", "solution": "该问题要求对$k$-近邻（$k$-NN）分类器的训练重代入误差和留一法交叉验证（LOOCV）误差进行比较分析。我们将根据指定规则实现分类器和误差度量，并将其应用于三个不同的数据集，候选$k$值为$\\{1, 3\\}$。目标是确定每种误差度量所选择的最优$k$值，分别表示为$k_{\\text{train}}$和$k_{\\text{LOOCV}}$。\n\n基本定义如下。数据集是$N$个点的集合 $\\{(x_i, y_i)\\}_{i=1}^N$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\{0,1\\}$。对查询点$x$的预测$\\hat{f}(x)$是通过对$k$个最近的训练点进行多数投票得出的，距离为欧几里得距离。距离相等时通过点的原始索引较小者来打破平局。训练重代入误差$E_{\\text{train}}$是在完整训练集上计算的$0\\text{-}1$损失，允许一个点成为其自身的邻居。LOOCV误差$E_{\\text{LOOCV}}$是通过对每个点$x_i$使用在排除了$(x_i, y_i)$的数据集上训练的模型进行预测，然后取误差的平均值来计算的$0\\text{-}1$损失。每种度量的最优$k$是使相应误差最小化的那个；平局时选择较小的$k$。\n\n我们将系统地分析每种情况。\n\n**情况A：一个极端离群点位于簇的中心。**\n数据集包含$N=5$个点。\n类别0：$P_0=(1,0)$, $P_1=(-1,0)$, $P_2=(0,1)$, $P_3=(0,-1)$。\n类别1：$P_4=(0,0)$。\n\n**训练重代入误差分析（$E_{\\text{train}}$）：**\n对于$k=1$：当对训练集中的一个点$x_i$进行分类时，它最近的邻居永远是它自己，距离为$0$。因此，预测的标签$\\hat{f}(x_i)$就是它自己的标签$y_i$。这导致零个错误分类。\n$E_{\\text{train}}(k=1) = \\frac{0}{5} = 0$。\n对于$k=3$：我们为每个点找到$3$个最近的邻居，包括该点本身。\n- 对于任何类别0的点（例如，$P_0=(1,0)$），它的邻居是它自己（$P_0$，类别0，距离0）、离群点$P_4$（$P_4$，类别1，距离1）和另一个类别0的点（$P_2$或$P_3$，距离$\\sqrt{2}$）。标签为$\\{0, 1, 0\\}$，所以多数票为类别0。预测正确。这对$P_0, P_1, P_2, P_3$都成立。\n- 对于离群点$P_4=(0,0)$（类别1），它的邻居是它自己（$P_4$，类别1，距离0）以及任意两个类别0的点，这些点距离都为1。根据索引平局打破规则，我们选择$P_0$和$P_1$。邻居的标签是$\\{1, 0, 0\\}$。多数票为类别0。预测为0，但真实标签是1。这是一个错误分类。\n$k=3$时的总错误分类数为$1$。\n$E_{\\text{train}}(k=3) = \\frac{1}{5} = 0.2$。\n比较误差，$E_{\\text{train}}(k=1) = 0  E_{\\text{train}}(k=3) = 0.2$。因此，$k_{\\text{train}} = 1$。\n\n**留一法交叉验证误差分析（$E_{\\text{LOOCV}}$）：**\n对于$k=1$：我们使用其他$N-1$个点对每个点$x_i$进行分类。\n- 对于任何类别0的点（例如，$P_0=(1,0)$），它在剩余集合$\\{P_1, P_2, P_3, P_4\\}$中最近的邻居是离群点$P_4=(0,0)$（类别1），距离为1。预测为类别1，而真实标签为0。这是一个错误分类。所有4个类别0的点都以这种方式被错误分类。\n- 对于离群点$P_4=(0,0)$（类别1），它在集合$\\{P_0, P_1, P_2, P_3\\}$中最近的邻居是它们中的任何一个（距离都为1）。根据索引平局打破规则，我们选择$P_0=(1,0)$（类别0）。预测为类别0，而真实标签为1。这是一个错误分类。\n$k=1$时的总错误分类数为$5$。\n$E_{\\text{LOOCV}}(k=1) = \\frac{5}{5} = 1.0$。\n对于$k=3$：\n- 对于任何类别0的点（例如，$P_0=(1,0)$），它在剩余集合$\\{P_1, P_2, P_3, P_4\\}$中最近的3个邻居是$P_4$（类别1，距离1）、$P_2$（类别0，距离$\\sqrt{2}$）和$P_3$（类别0，距离$\\sqrt{2}$）。标签为$\\{1, 0, 0\\}$。多数票为类别0。预测正确。这对所有4个类别0的点都成立。\n- 对于离群点$P_4=(0,0)$（类别1），它在集合$\\{P_0, P_1, P_2, P_3\\}$中最近的3个邻居是这些点中的任意三个（距离都为1）。根据索引平局打破规则，我们选择$P_0, P_1, P_2$，它们都属于类别0。标签为$\\{0, 0, 0\\}$。预测为类别0，而真实标签为1。这是一个错误分类。\n$k=3$时的总错误分类数为$1$。\n$E_{\\text{LOOCV}}(k=3) = \\frac{1}{5} = 0.2$。\n比较误差，$E_{\\text{LOOCV}}(k=3) = 0.2  E_{\\text{LOOCV}}(k=1) = 1.0$。因此，$k_{\\text{LOOCV}} = 3$。\n情况A的结果是 $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [3, 1]$。\n\n**情况B：无离群点；分离良好的簇。**\n数据集包含$N=8$个点。\n类别0：$\\{(1,0), (-1,0), (0,1), (0,-1)\\}$。\n类别1：$\\{(10,0), (-10,0), (0,10), (0,-10)\\}$。\n遵循相同的方法：\n$E_{\\text{train}}(k=1) = 0$。对于$k=3$，类别1簇中的每个点都会被错误分类，因为它（除了自身之外）的两个最近邻居来自类别0簇，导致4个错误。$E_{\\text{train}}(k=3) = \\frac{4}{8} = 0.5$。所以$k_{\\text{train}} = 1$。\n$E_{\\text{LOOCV}}(k=1)$：每个类别1的点的最近邻居是类别0的点，导致4个错误。每个类别0的点的最近邻居是另一个类别0的点，导致0个错误。$E_{\\text{LOOCV}}(k=1) = \\frac{4}{8} = 0.5$。\n$E_{\\text{LOOCV}}(k=3)$：每个类别1的点的三个最近邻居都是类别0的点，导致4个错误。每个类别0的点的三个最近邻居是其他类别0的点，导致0个错误。$E_{\\text{LOOCV}}(k=3) = \\frac{4}{8} = 0.5$。\n误差持平：$E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = 0.5$。根据平局打破规则，我们选择最小的$k$。因此，$k_{\\text{LOOCV}} = 1$。\n情况B的结果是 $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$。\n\n**情况C：一个远离所有其他点的极端离群点。**\n数据集包含$N=9$个点。\n类别0：$\\{(1,0), (-1,0), (0,1), (0,-1), (100,100)\\}$。\n类别1：$\\{(3,0), (3,1), (4,0), (4,1)\\}$。\n遵循相同的方法：\n$E_{\\text{train}}(k=1) = 0$。对于$k=3$，只有离群点$(100,100)$被错误分类，因为它（除了自身之外）的两个最近邻居来自类别1的簇。$E_{\\text{train}}(k=3) = \\frac{1}{9}$。所以$k_{\\text{train}} = 1$。\n$E_{\\text{LOOCV}}(k=1)$：当离群点$(100,100)$被留出时，它最近的邻居是一个类别1的点，导致一个错误。所有其他点都被正确分类。$E_{\\text{LOOCV}}(k=1) = \\frac{1}{9}$。\n$E_{\\text{LOOCV}}(k=3)$：当离群点$(100,100)$被留出时，它的三个最近邻居都是类别1的点，导致一个错误。所有其他点都被正确分类。$E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$。\n误差持平：$E_{\\text{LOOCV}}(k=1) = E_{\\text{LOOCV}}(k=3) = \\frac{1}{9}$。根据平局打破规则，我们选择最小的$k$。因此，$k_{\\text{LOOCV}} = 1$。\n情况C的结果是 $[k_{\\text{LOOCV}}, k_{\\text{train}}] = [1, 1]$。\n\n从每种情况中收集最终结果。\n- 情况A：$[3, 1]$\n- 情况B：$[1, 1]$\n- 情况C：$[1, 1]$\n这些结果展示了关键的洞见：训练重代入误差总是会轻易地选择$k=1$（因为其误差始终为0），这是该误差度量的一个病态特征。LOOCV提供了对泛化误差更真实的估计。在情况A中，它正确地识别出需要一个更大的$k=3$来对中心离群点保持鲁棒性。在情况B和C中，数据的几何结构使得$k=1$更优或与$k=3$持平，根据平局打破规则选择了$k=1$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes LOOCV vs. training error for k-NN on three datasets.\n    \"\"\"\n\n    test_cases = [\n        # Case A: one extreme outlier centered in the cluster\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[0, 0]]),\n        },\n        # Case B: no outlier; well-separated clusters\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]),\n            \"class_1_points\": np.array([[10, 0], [-10, 0], [0, 10], [0, -10]]),\n        },\n        # Case C: an extreme outlier far from all other points\n        {\n            \"class_0_points\": np.array([[1, 0], [-1, 0], [0, 1], [0, -1], [100, 100]]),\n            \"class_1_points\": np.array([[3, 0], [3, 1], [4, 0], [4, 1]]),\n        }\n    ]\n    \n    candidate_k_values = [1, 3]\n    final_results = []\n\n    for case_data in test_cases:\n        # Prepare dataset with original indices for tie-breaking\n        points = []\n        labels = []\n        \n        index = 0\n        for p in case_data[\"class_0_points\"]:\n            points.append(p)\n            labels.append(0)\n            index += 1\n        for p in case_data[\"class_1_points\"]:\n            points.append(p)\n            labels.append(1)\n            index += 1\n\n        points = np.array(points)\n        labels = np.array(labels)\n        full_dataset = list(zip(range(len(points)), points, labels))\n\n        # --- k-NN Implementation ---\n        def euclidean_distance(p1, p2):\n            return np.sqrt(np.sum((p1 - p2)**2))\n\n        def k_nn_predict(query_point, training_data, k):\n            if not training_data:\n                # Undefined, but for this problem, training set is never empty\n                return 0 \n            \n            distances = []\n            for idx, train_point, label in training_data:\n                dist = euclidean_distance(query_point, train_point)\n                distances.append((dist, idx, label))\n            \n            # Sort by distance, then by original index for tie-breaking\n            distances.sort(key=lambda x: (x[0], x[1]))\n            \n            neighbors = distances[:k]\n            \n            neighbor_labels = [label for _, _, label in neighbors]\n            \n            # Majority vote\n            count_0 = neighbor_labels.count(0)\n            count_1 = neighbor_labels.count(1)\n\n            if count_0 > count_1:\n                return 0\n            elif count_1 > count_0:\n                return 1\n            else: # Vote tie-break (unlikely for odd k here, but for completeness)\n                sum_dist_0 = sum(dist for dist, _, label in neighbors if label == 0)\n                sum_dist_1 = sum(dist for dist, _, label in neighbors if label == 1)\n                if sum_dist_0  sum_dist_1:\n                    return 0\n                elif sum_dist_1  sum_dist_0:\n                    return 1\n                else: # Tie in sum of distances, choose smaller class label\n                    return 0\n        \n        # --- Error Calculation ---\n        \n        # Training Resubstitution Error\n        train_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                # In training error, the point itself is part of the training set\n                # and will be the nearest neighbor.\n                if k == 1:\n                    predicted_label = true_label\n                else:\n                    predicted_label = k_nn_predict(query_point, full_dataset, k)\n                    \n                if predicted_label != true_label:\n                    misclassifications += 1\n            train_errors[k] = misclassifications / len(points)\n        \n        # LOOCV Error\n        loocv_errors = {}\n        for k in candidate_k_values:\n            misclassifications = 0\n            for i in range(len(points)):\n                query_point = points[i]\n                true_label = labels[i]\n                \n                # Create LOOCV training set by excluding point i\n                loocv_train_set = full_dataset[:i] + full_dataset[i+1:]\n                \n                predicted_label = k_nn_predict(query_point, loocv_train_set, k)\n                \n                if predicted_label != true_label:\n                    misclassifications += 1\n            loocv_errors[k] = misclassifications / len(points)\n\n        # --- Model Selection (Find best k) ---\n        \n        # Tie-breaking rule: choose smallest k\n        min_loocv_error = float('inf')\n        best_k_loocv = -1\n        for k in sorted(loocv_errors.keys()):\n            if loocv_errors[k]  min_loocv_error:\n                min_loocv_error = loocv_errors[k]\n                best_k_loocv = k\n        \n        min_train_error = float('inf')\n        best_k_train = -1\n        # For k=1 training error is always 0, so it will always be selected\n        # if 1 is a candidate k.\n        for k in sorted(train_errors.keys()):\n            if train_errors[k]  min_train_error:\n                min_train_error = train_errors[k]\n                best_k_train = k\n                \n        final_results.append([best_k_loocv, best_k_train])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{k_l},{k_t}]\" for k_l, k_t in final_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3139300"}]}