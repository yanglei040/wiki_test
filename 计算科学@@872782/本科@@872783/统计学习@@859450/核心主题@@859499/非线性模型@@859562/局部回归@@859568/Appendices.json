{"hands_on_practices": [{"introduction": "在实际数据分析中，我们经常遇到包含分类变量的数据集，其中不同组别的数据可能展现出截然不同的趋势。这个练习将指导您在一个包含两个不同组别的假设场景中应用局部回归（LOESS）。您将通过实践，探索如何为每个组别单独拟合模型，并比较使用统一的“平滑窗口”参数与为每个组别选择特定参数的效果，从而深刻理解平滑参数$s$在平衡偏差和方差中的关键作用，以及如何根据数据的局部特性进行调整。[@problem_id:3141278]", "problem": "您将实现并分析局部回归（局部估计散点平滑，LOESS），通过在组内拟合独立的平滑器，并比较在不同样本量下使用合并跨度参数与特定于组的跨度参数的效果，来研究分类交互作用。您的程序必须是一个完整的、可运行的实现，能够生成合成数据、拟合 LOESS 模型、根据已知的真实函数评估样本外误差，并以指定格式报告比较结果。\n\n使用的基本原理：\n- 加权最小二乘法和局部线性回归：对于一个目标位置 $x_0$，局部线性拟合通过选择系数 $\\beta_0$ 和 $\\beta_1$ 来定义，以最小化加权平方和 $\\sum_{i=1}^{n} w_i(x_0)\\,\\big(y_i - \\beta_0 - \\beta_1 (x_i - x_0)\\big)^2$，其中 $w_i(x_0) \\ge 0$ 是基于邻近度的权重。\n- 通过跨度选择邻域：对于跨度比例 $s \\in (0,1]$ 和样本量 $n$，使用预测变量 $x$ 中离 $x_0$ 最近的 $k = \\lceil s\\,n \\rceil$ 个数据点构成邻域。\n- 核加权：使用一个紧支撑核，该核函数对邻域内较远的点赋予较低权重，权重在边界处平滑递减至 $0$。\n\n任务概述：\n1. 各组数据生成。有两个组 $g \\in \\{A,B\\}$。对于每个组 $g$，生成训练数据 $\\{(x_i^{(g)}, y_i^{(g)})\\}_{i=1}^{n_g}$，其中 $x_i^{(g)} \\sim \\mathrm{Uniform}[0,1]$ 并且\n   - 组 A：$f_A(x) = \\sin(2\\pi x) + 0.5\\,x$，噪声标准差 $\\sigma_A = 0.20$。\n   - 组 B：$f_B(x) = \\cos(\\pi x) - x$，噪声标准差 $\\sigma_B = 0.05$。\n   - 观测值：$y_i^{(g)} = f_g(x_i^{(g)}) + \\varepsilon_i^{(g)}$，其中 $\\varepsilon_i^{(g)} \\sim \\mathcal{N}(0,\\sigma_g^2)$，在 $i$ 和 $g$ 之间相互独立。\n2. 建模。按照基本原理中的描述，在每个组内分别使用局部线性拟合来拟合 LOESS。实现：\n   - 对于跨度 $s$ 和组样本量 $n_g$，邻域大小为 $k = \\lceil s\\,n_g \\rceil$，并强制要求 $k \\ge 2$。\n   - 邻域内的 Tricube 核权重：对于距离 $d_i = |x_i^{(g)} - x_0|$ 和 $x_0$ 的 $k$ 个最近邻中的最大距离 $d_{\\max} = \\max$，定义在 $d_{\\max}$ 处平滑衰减至 $0$ 的权重。使用局部线性加权最小二乘法在 $x_0$ 处获得拟合值。\n3. 评估。在 $[0,1]$ 上构建一个由 $M = 200$ 个等距点组成的评估网格 $x_{\\mathrm{eval}}$。对于每个组 $g$，计算相对于无噪声真实值的均方误差：\n   $$\\mathrm{MSE}_g = \\frac{1}{M}\\sum_{j=1}^{M} \\big(\\widehat{f}_g(x_{\\mathrm{eval},j}) - f_g(x_{\\mathrm{eval},j})\\big)^2.$$\n   将组合误差报告为简单平均值\n   $$\\mathrm{MSE}_{\\mathrm{combined}} = \\frac{\\mathrm{MSE}_A + \\mathrm{MSE}_B}{2}.$$\n4. 比较。对于每个测试用例，计算两种组合误差：\n   - 合并跨度设置：对两个组使用相同的跨度 $s_{\\mathrm{global}}$（拟合仍然在每个组的数据内部分别进行）。\n   - 特定于组的跨度：对组 A 使用跨度 $s_A$，对组 B 使用跨度 $s_B$。\n   定义性能差异\n   $$\\Delta = \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{group-specific}} - \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{pooled-span}}.$$\n\n实现和数值细节：\n- 仅使用局部线性拟合（多项式次数为 $1$）。\n- 在由跨度决定的 $k$-最近邻窗口内使用 tricube 核。如果在某个 $x_0$ 处的加权最小二乘系统是病态的，您可以回退到使用局部常数加权平均。\n- 为确保可复现性，每个测试用例使用下面提供的独立伪随机数生成器种子。\n- 不涉及单位或角度。所有数值答案必须以实数形式报告。\n\n测试套件：\n每个测试用例是一个元组 $(n_A, n_B, s_{\\mathrm{global}}, s_A, s_B, \\text{seed})$：\n- 用例 1：$(80, 80, 0.4, 0.3, 0.5, 20201)$\n- 用例 2：$(20, 120, 0.4, 0.25, 0.5, 20202)$\n- 用例 3：$(8, 8, 0.6, 0.8, 0.4, 20203)$\n- 用例 4：$(200, 15, 0.3, 0.2, 0.6, 20204)$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含四个测试用例的 $\\Delta$ 值，四舍五入到 $6$ 位小数，以逗号分隔列表的形式并用方括号括起来；例如，$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$。", "solution": "我们从通过加权最小二乘法进行局部建模的原理开始。对于给定的目标位置 $x_0$ 和训练样本 $\\{(x_i,y_i)\\}_{i=1}^{n}$，通过最小化加权平方和，可以获得 $x_0$ 附近的局部线性近似\n$$\nS(\\beta_0,\\beta_1;x_0) = \\sum_{i=1}^{n} w_i(x_0)\\,\\big(y_i - \\beta_0 - \\beta_1 (x_i - x_0)\\big)^2,\n$$\n其中 $w_i(x_0)\\ge 0$ 是基于邻近度的权重。最小化器得到\n$$\n\\widehat{f}(x_0) = \\widehat{\\beta}_0(x_0),\n$$\n即在 $x_0$ 处的局部线性估计。\n\n邻域选择由跨度比例 $s \\in (0,1]$ 控制。对于组大小 $n_g$，我们设置 $k=\\lceil s\\,n_g\\rceil$，取预测变量空间中 $x_0$ 的 $k$ 个最近邻，并仅使用这些邻居计算局部拟合。这实现了局部回归（LOESS）至关重要的局部性。\n\n我们在 $k$-最近邻窗口内使用 tricube 核。对于 $k$ 个选定的邻居，令 $d_i=|x_i-x_0|$ 且 $d_{\\max}=\\max_i d_i$。邻居 $i$ 的 tricube 权重是一个平滑函数，满足在 $d_i=0$ 时 $w_i(x_0)=1$，在 $d_i=d_{\\max}$ 时 $w_i(x_0)=0$，其连续导数减小了更远点的影响。然后将这些权重应用于上述加权最小二乘准则中。如果 $d_{\\max}=0$（所有 $k$ 个邻居都在 $x_0$ 处）或加权设计矩阵是病态的，一个稳健的备选方案是使用局部常数加权平均，这对应于设置 $\\widehat{\\beta}_1(x_0)=0$ 并从 $y_i$ 的加权平均值计算 $\\widehat{\\beta}_0(x_0)$。\n\n为了数值化地实现局部线性估计，构建以 $x_0$ 为中心的设计矩阵，\n$$\nX(x_0) = \\begin{bmatrix} 1  x_{(1)}-x_0 \\\\ \\vdots  \\vdots \\\\ 1  x_{(k)}-x_0 \\end{bmatrix},\\quad\n\\mathbf{y} = \\begin{bmatrix} y_{(1)} \\\\ \\vdots \\\\ y_{(k)} \\end{bmatrix},\\quad\nW(x_0) = \\mathrm{diag}\\big(w_{(1)}(x_0),\\ldots,w_{(k)}(x_0)\\big),\n$$\n其中 $(\\cdot)$ 表示仅限于 $k$-最近邻的排序。加权最小二乘解为\n$$\n\\widehat{\\boldsymbol{\\beta}}(x_0) = \\arg\\min_{\\boldsymbol{\\beta}} \\| W(x_0)^{1/2} (\\mathbf{y} - X(x_0)\\boldsymbol{\\beta})\\|_2^2,\n$$\n我们通过求解 $W(x_0)^{1/2}X(x_0)$ 和 $W(x_0)^{1/2}\\mathbf{y}$ 的最小二乘问题来计算该解。预测值为 $\\widehat{f}(x_0)=\\widehat{\\beta}_0(x_0)$。\n\n我们将此过程分别应用于每个组 $g\\in\\{A,B\\}$：\n\n- 数据生成。对于每个组，抽取 $x_i^{(g)} \\sim \\mathrm{Uniform}[0,1]$，然后使用指定的真实函数设置 $y_i^{(g)} = f_g(x_i^{(g)}) + \\varepsilon_i^{(g)}$，其中 $\\varepsilon_i^{(g)} \\sim \\mathcal{N}(0,\\sigma_g^2)$，\n  $$\n  f_A(x) = \\sin(2\\pi x) + 0.5\\,x,\\quad \\sigma_A = 0.20,\\qquad\n  f_B(x) = \\cos(\\pi x) - x,\\quad \\sigma_B = 0.05.\n  $$\n  独立的伪随机种子确保了测试用例之间的可复现性。\n\n- 评估。在 $[0,1]$ 中构建一个由 $M=200$ 个等距点组成的评估网格 $x_{\\mathrm{eval}}$。对于每个组 $g$，计算\n  $$\n  \\mathrm{MSE}_g = \\frac{1}{M}\\sum_{j=1}^{M} \\big(\\widehat{f}_g(x_{\\mathrm{eval},j}) - f_g(x_{\\mathrm{eval},j})\\big)^2.\n  $$\n  通过简单平均合并各组的误差\n  $$\n  \\mathrm{MSE}_{\\mathrm{combined}} = \\frac{\\mathrm{MSE}_A + \\mathrm{MSE}_B}{2}.\n  $$\n\n- 跨度策略比较。对于每个测试用例，我们拟合两种配置：\n  1. 合并跨度：对组 A 和组 B 使用相同的跨度 $s_{\\mathrm{global}}$。\n  2. 特定于组：对组 A 使用跨度 $s_A$，对组 B 使用跨度 $s_B$。\n  定义差异\n  $$\n  \\Delta = \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{group-specific}} - \\mathrm{MSE}_{\\mathrm{combined}}^{\\text{pooled-span}}.\n  $$\n  负的 $\\Delta$ 表示特定于组的跨度具有优势，而正的 $\\Delta$ 则表示合并跨度更有利。\n\n每个测试用例 $(n_A, n_B, s_{\\mathrm{global}}, s_A, s_B, \\text{seed})$ 的算法步骤：\n1. 使用提供的种子设置伪随机数生成器。\n2. 分别为组 A 和组 B 生成 $n_A$ 和 $n_B$ 个样本。\n3. 在 $[0,1]$ 中构建包含 $M=200$ 个点的 $x_{\\mathrm{eval}}$。\n4. 对每个组进行两次 LOESS 拟合：\n   - 使用跨度 $s_{\\mathrm{global}}$ 获得 $\\widehat{f}_A^{\\mathrm{pool}}$ 和 $\\widehat{f}_B^{\\mathrm{pool}}$。\n   - 使用跨度 $s_A$ 和 $s_B$ 获得 $\\widehat{f}_A^{\\mathrm{grp}}$ 和 $\\widehat{f}_B^{\\mathrm{grp}}$。\n5. 计算 $\\mathrm{MSE}_A^{\\mathrm{pool}}$、$\\mathrm{MSE}_B^{\\mathrm{pool}}$、$\\mathrm{MSE}_A^{\\mathrm{grp}}$、$\\mathrm{MSE}_B^{\\mathrm{grp}}$，然后计算组合误差和 $\\Delta$。\n6. 将 $\\Delta$ 四舍五入到 $6$ 位小数以便报告。\n\n边缘情况和数值稳定性：\n- 确保 $k=\\lceil s\\,n_g\\rceil \\ge 2$ 以便至少用两个点拟合一条直线；当因极端舍入导致 $k2$ 时，强制 $k=2$。\n- 如果在任何 $x_0$ 处 $d_{\\max}=0$，则对邻域使用相等权重并计算局部常数拟合。\n- 通过按 $\\sqrt{w_i}$ 进行缩放来使用加权最小二乘求解器，这种方法数值稳定，并避免了显式求逆。\n\n程序以单行形式输出指定测试套件的四个 $\\Delta$ 值：$[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4]$，每个值都四舍五入到 $6$ 位小数。", "answer": "```python\nimport numpy as np\n\n# No external inputs; deterministic seeds are provided in the test cases.\n\ndef tricube_weights(distances):\n    \"\"\"\n    Compute tricube weights for a vector of distances scaled by the maximum.\n    distances: 1D array of nonnegative distances for the selected neighbors.\n    Returns: weights array of same shape.\n    \"\"\"\n    dmax = distances.max()\n    if dmax == 0.0:\n        # All neighbors coincide with x0; use equal weights\n        return np.ones_like(distances)\n    u = distances / dmax\n    # Tricube kernel: (1 - u^3)^3 on [0,1], 0 outside.\n    w = (1 - u**3)**3\n    # Numerical safety: clip negatives (can occur at the boundary due to FP)\n    w[w  0] = 0.0\n    return w\n\ndef loess_local_linear_predict(x_train, y_train, x_eval, span):\n    \"\"\"\n    Local linear LOESS predictions at x_eval given training data and span.\n    Uses k = ceil(span * n) nearest neighbors with tricube weights.\n    Falls back to locally constant weighted average if needed.\n    \"\"\"\n    x = np.asarray(x_train, dtype=float)\n    y = np.asarray(y_train, dtype=float)\n    xe = np.asarray(x_eval, dtype=float)\n    n = x.shape[0]\n    # Ensure at least 2 points in neighborhood for local linear fit\n    k = int(np.ceil(span * n))\n    if k  2:\n        k = 2\n    if k > n:\n        k = n\n\n    y_pred = np.empty_like(xe, dtype=float)\n\n    for idx, x0 in enumerate(xe):\n        # Compute absolute distances to x0\n        dists = np.abs(x - x0)\n        # Select k nearest neighbors (indices)\n        if k  n:\n            # Use argpartition for efficiency\n            nn_idx = np.argpartition(dists, k - 1)[:k]\n        else:\n            nn_idx = np.arange(n)\n        # Extract local data\n        x_local = x[nn_idx]\n        y_local = y[nn_idx]\n        d_local = dists[nn_idx]\n        # Compute tricube weights on the local neighborhood\n        w = tricube_weights(d_local)\n\n        # Build weighted design for local linear fit centered at x0\n        # Design matrix columns: [1, x - x0]\n        X = np.column_stack((np.ones_like(x_local), x_local - x0))\n        # Apply sqrt weights for weighted least squares\n        sqrt_w = np.sqrt(w)\n        Xw = X * sqrt_w[:, None]\n        yw = y_local * sqrt_w\n\n        # Solve weighted least squares: minimize ||Xw * beta - yw||_2\n        # Use lstsq for numerical stability\n        try:\n            beta, *_ = np.linalg.lstsq(Xw, yw, rcond=None)\n            y_pred[idx] = beta[0]  # fitted value at x0 is intercept in centered design\n        except np.linalg.LinAlgError:\n            # Fallback: locally constant weighted average\n            if w.sum() > 0:\n                y_pred[idx] = np.sum(w * y_local) / np.sum(w)\n            else:\n                # Degenerate case: no weights (shouldn't happen); use simple mean\n                y_pred[idx] = y_local.mean()\n\n    return y_pred\n\ndef f_A(x):\n    return np.sin(2 * np.pi * x) + 0.5 * x\n\ndef f_B(x):\n    return np.cos(np.pi * x) - x\n\ndef generate_group_data(n, group, rng):\n    if group == 'A':\n        x = rng.uniform(0.0, 1.0, size=n)\n        y = f_A(x) + rng.normal(0.0, 0.20, size=n)\n    elif group == 'B':\n        x = rng.uniform(0.0, 1.0, size=n)\n        y = f_B(x) + rng.normal(0.0, 0.05, size=n)\n    else:\n        raise ValueError(\"Unknown group\")\n    return x, y\n\ndef mse_against_truth(y_hat, y_true):\n    diff = y_hat - y_true\n    return float(np.mean(diff * diff))\n\ndef run_case(nA, nB, s_global, sA, sB, seed):\n    rng = np.random.default_rng(seed)\n    # Generate training data\n    xA, yA = generate_group_data(nA, 'A', rng)\n    xB, yB = generate_group_data(nB, 'B', rng)\n    # Evaluation grid\n    M = 200\n    x_eval = np.linspace(0.0, 1.0, M)\n    # Truth on grid\n    truth_A = f_A(x_eval)\n    truth_B = f_B(x_eval)\n\n    # Pooled-span predictions (fit separately within groups, same span)\n    yhatA_pool = loess_local_linear_predict(xA, yA, x_eval, s_global)\n    yhatB_pool = loess_local_linear_predict(xB, yB, x_eval, s_global)\n    mseA_pool = mse_against_truth(yhatA_pool, truth_A)\n    mseB_pool = mse_against_truth(yhatB_pool, truth_B)\n    mse_pool_combined = 0.5 * (mseA_pool + mseB_pool)\n\n    # Group-specific span predictions\n    yhatA_grp = loess_local_linear_predict(xA, yA, x_eval, sA)\n    yhatB_grp = loess_local_linear_predict(xB, yB, x_eval, sB)\n    mseA_grp = mse_against_truth(yhatA_grp, truth_A)\n    mseB_grp = mse_against_truth(yhatB_grp, truth_B)\n    mse_grp_combined = 0.5 * (mseA_grp + mseB_grp)\n\n    # Difference\n    delta = mse_grp_combined - mse_pool_combined\n    return delta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (nA, nB, s_global, sA, sB, seed)\n    test_cases = [\n        (80, 80, 0.4, 0.3, 0.5, 20201),\n        (20, 120, 0.4, 0.25, 0.5, 20202),\n        (8, 8, 0.6, 0.8, 0.4, 20203),\n        (200, 15, 0.3, 0.2, 0.6, 20204),\n    ]\n\n    results = []\n    for case in test_cases:\n        nA, nB, s_global, sA, sB, seed = case\n        delta = run_case(nA, nB, s_global, sA, sB, seed)\n        # Round to 6 decimals as required\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3141278"}, {"introduction": "标准的回归方法，包括基础的LOESS，对异常值（outliers）非常敏感，这些异常值可能会严重扭曲拟合结果。本练习旨在解决这一关键限制，引导您构建一个“稳健”的LOESS估计器。您将亲手实现一种迭代重加权局部回归算法，该算法能自动识别并降低异常值的影响，这涉及到稳健统计学中的核心概念，如M-估计、双平方权重函数和中位数绝对偏差（MAD），是迈向实用数据分析的重要一步。[@problem_id:3141335]", "problem": "您需要实现一个完整的程序，该程序构建一个迭代重加权局部回归估计器，称为局部估计散点图平滑（LOcally Estimated Scatterplot Smoothing, LOESS），并分析其在预测变量中存在高杠杆离群值污染下的稳健性。该程序必须是自包含的，并且不应需要任何外部输入。算法应基于成熟的原理：加权最小二乘法和稳健尺度估计，问题陈述中不提供快捷公式。\n\n核心任务是使用由一个跨度参数 $\\alpha \\in (0,1]$ 决定的邻域，在评估点上执行阶数为 $d$ 的局部多项式回归。邻域权重必须平滑地依赖于预测变量中的距离，稳健迭代必须乘性地对残差进行重新加权，以减少响应变量中极端偏差的影响。稳健性重加权必须使用一个与从残差分布派生的尺度相关联的再降权函数。该算法必须执行指定的迭代次数，从均匀的残差权重开始。\n\n您将根据以下规则构建一个数据集：\n- 设 $n$ 为总观测数，其中未污染的预测变量值 $x$ 从 $[0,1]$ 上的均匀分布中独立抽样，并对应于由真实函数 $f(x)$ 加上加性噪声生成的响应 $y$。\n- 真实函数为 $f(x) = \\sin(2\\pi x)$，对于 $x$ 在 $[0,1]$ 中。\n- 噪声是独立同分布的，每次观测都被一个均值为零、标准差为 $\\sigma$ 的高斯噪声扰动。\n- 污染比例 $p \\in [0,1]$ 表示被高杠杆离群值替换的观测值比例：这些离群值的预测变量值位于定义域的极端位置（两端平衡），其响应值则朝相反方向移动一个大的固定振幅 $A$ 以形成对抗。\n- 污染会替换掉干净的点，以保持总样本量 $n$ 不变。\n\n稳健性分析通过在一个密集的 $[0,1]$ 评估网格上，比较 LOESS 估计器相对于真实函数 $f(x)$ 的预测质量来进行。对于每个污染比例 $p$，计算 LOESS 预测在网格上相对于 $f(x)$ 的均方根误差（RMSE）。如果此 RMSE 与无污染情况下的 RMSE 之比超过阈值 $\\tau$，则定义在 $p$ 处发生崩溃。\n\n您的程序必须实现该算法，并为以下固定的参数值（这些是测试套件）生成结果：\n- 总样本量 $n = 200$。\n- 多项式阶数 $d = 1$（局部线性）。\n- 跨度参数 $\\alpha = 0.8$。\n- 噪声标准差 $\\sigma = 0.1$。\n- 离群值振幅 $A = 8.0$。\n- 初始拟合后的稳健迭代次数为 $2$ 次。\n- 阈值 $\\tau = 3.0$。\n- 评估网格：$[0,1]$ 上的 $200$ 个等距点。\n- 污染比例 $p \\in \\{0.0, 0.1, 0.3, 0.5\\}$。\n\n程序必须使用固定的随机种子以确保确定性行为。对于每个 $p$ 值，您必须计算一个布尔值，指示在该 $p$ 值下是否发生崩溃。此外，计算给定集合中导致崩溃的最小污染比例，如果没有任何比例导致崩溃，则为 $1.0$。\n\n最终输出格式规范：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果。\n- 该列表应包含一系列布尔值（每个污染比例一个，顺序相同），后跟一个浮点数，给出发生崩溃的最小污染比例，四舍五入到 $3$ 位小数。\n- 例如，如果最后两个污染比例导致崩溃，并且最小的此类比例是 $0.3$，则输出将是形如 `\"[False,True,True,True,0.300]\"` 的一行。", "solution": "该问题要求实现并分析一种稳健的局部回归算法，特别是 LOESS（局部估计散点图平滑），也称为局部加权回归。该分析涉及评估当数据被高杠杆离群值污染时其崩溃特性。\n\n该问题在科学上是合理的，算法上是具体的，并且在数量上是明确定义的。它基于非参数统计和稳健估计的既定原则。所有参数和过程都已指定，使得该问题是适定的，并且对于给定的随机性来源，允许一个唯一的、可验证的解。因此，该问题被认为是有效的，下面提供了完整的解决方案。\n\n解决方案的结构如下：\n1.  数据生成过程的详细描述，包括离群值的引入。\n2.  迭代重加权 LOESS 算法的逐步阐述。\n3.  性能评估和崩溃分析的方法。\n\n**1. 数据生成**\n\n数据集包含 $n = 200$ 个观测值。其中一部分比例为 $p$ 的是离群值，而其余的 $n(1-p)$ 是“干净”的数据点。\n\n干净的数据由以下模型生成：\n$$ y_i = f(x_i) + \\epsilon_i $$\n其中，真实的底层函数是 $f(x) = \\sin(2\\pi x)$，对于 $x \\in [0, 1]$。预测变量 $x_i$ 从均匀分布 $x_i \\sim U[0, 1]$ 中抽取。噪声项 $\\epsilon_i$ 是从均值为零、标准差为 $\\sigma = 0.1$ 的正态分布中独立同分布抽取的，即 $\\epsilon_i \\sim N(0, \\sigma^2)$。\n\n数量为 $n_{out} = \\text{round}(n \\times p)$ 的点被指定为离群值。这些离群值被构造成高杠杆和对抗性的。离群值的预测变量值被放置在定义域的极端位置，$x=0$ 和 $x=1$，以最大化其对拟合的影响（杠杆作用）。这些位置是平衡的，$\\lfloor n_{out}/2 \\rfloor$ 个离群值在 $x=0$ 处，$\\lceil n_{out}/2 \\rceil$ 个离群值在 $x=1$ 处。它们对应的响应值从真实函数值上偏移了一个大的振幅 $A=8.0$。由于 $f(0)=f(1)=0$，位于 $x=0$ 的离群值响应被设置为 $y=+A$，而位于 $x=1$ 的离群值响应被设置为 $y=-A$。这些离群值替换了等量随机选择的干净点，以保持总样本量为 $n$。\n\n**2. 迭代重加权 LOESS 算法**\n\nLOESS 是一种非参数方法，它通过执行一系列局部加权回归来拟合数据的平滑曲线。其核心思想是，对于任何评估点 $x_0$，拟合是由 $x_0$ 周围一小组数据点决定的。离 $x_0$ 更近的点获得更大的权重。通过迭代地降低前一次拟合中具有大残差的点的权重来实现稳健性。\n\n算法按以下步骤进行，总共进行 $1+k_{robust}$ 次迭代，其中 $k_{robust}=2$ 是稳健重加权步骤的次数。\n\n**步骤 2.1：初始化**\n最初，所有点都被认为是同等可靠的。所有数据点 $(x_i, y_i)$ 的稳健性权重 $\\delta_i$ 初始化为 1：\n$$ \\delta_i^{(0)} = 1 \\quad \\text{for } i = 1, \\dots, n $$\n\n**步骤 2.2：局部回归拟合（每次迭代）**\n对于评估点集中的每个点 $x_0$，将一个局部多项式拟合到数据上。在这个问题中，多项式阶数为 $d=1$（局部线性拟合）。\n\n**2.2.1. 邻域选择：**\n定义一个邻域 $\\mathcal{N}(x_0)$，它由 $q$ 个训练点 $(x_i, y_i)$ 组成，这些点的 $x_i$ 值最接近 $x_0$。邻域的大小 $q$ 由跨度参数 $\\alpha = 0.8$ 决定：\n$$ q = \\lceil \\alpha n \\rceil = \\lceil 0.8 \\times 200 \\rceil = 160 $$\n令 $\\Delta(x_0)$ 为 $x_0$ 到其在 $x_i$ 中的第 $q$ 个最近邻的距离。这个距离定义了邻域的半宽。\n\n**2.2.2. 邻域加权：**\n$\\mathcal{N}(x_0)$ 中的每个点 $x_i$ 根据其到 $x_0$ 的距离被分配一个邻域权重 $w_i(x_0)$。使用标准的三立方权重函数：\n$$ w_i(x_0) = W\\left(\\frac{|x_i - x_0|}{\\Delta(x_0)}\\right) $$\n其中，当 $|u|  1$ 时，$W(u) = (1 - |u|^3)^3$，否则 $W(u) = 0$。此函数给予位于 $x_0$ 的点最高权重，并平滑地将权重减小到邻域边缘的零。\n\n**2.2.3. 加权最小二乘法 (WLS)：**\n在每个评估点 $x_0$ 处，我们将一个局部线性模型 $g(x) = \\beta_0 + \\beta_1 (x - x_0)$ 拟合到 $\\mathcal{N}(x_0)$ 中的数据。系数 $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)^T$ 通过最小化加权残差平方和来找到：\n$$ \\min_{\\beta_0, \\beta_1} \\sum_{i \\in \\mathcal{N}(x_0)} v_i(x_0) [y_i - (\\beta_0 + \\beta_1(x_i - x_0))]^2 $$\n在当前迭代中，点 $i$ 的总权重 $v_i(x_0)$ 是其邻域权重 $w_i(x_0)$ 和其当前稳健性权重 $\\delta_i$ 的乘积：\n$$ v_i(x_0) = w_i(x_0) \\delta_i $$\n这个 WLS 问题可以使用正规方程求解：$(\\mathbf{X}^T \\mathbf{V} \\mathbf{X})\\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{V} \\mathbf{y}$，其中 $\\mathbf{X}$ 是设计矩阵，其行为 $(1, x_i - x_0)$，$\\mathbf{y}$ 是响应向量，$\\mathbf{V}$ 是总权重 $v_i(x_0)$ 的对角矩阵。\n在 $x_0$ 处的 LOESS 拟合值为 $\\hat{y}(x_0) = \\hat{\\beta}_0$。\n\n**步骤 2.3：稳健性重加权**\n在初始拟合（迭代 0）之后，以及对于每个后续的稳健迭代，稳健性权重 $\\delta_i$ 都会被更新。这一步使得算法对响应变量 $y$ 中的离群值具有稳健性。\n\n**2.3.1. 残差计算：**\n首先，我们计算所有训练点的残差 $r_i = y_i - \\hat{y}(x_i)$，其中 $\\hat{y}(x_i)$ 是使用当前迭代的拟合在训练点 $x_i$ 本身的 LOESS 预测值。\n\n**2.3.2. 稳健尺度估计：**\n残差的散布使用中位数绝对偏差（Median Absolute Deviation, MAD）进行稳健估计：\n$$ s = \\frac{1}{C} \\cdot \\text{median}_{i} |r_i| $$\n常数 $C = \\Phi^{-1}(0.75) \\approx 0.6745$，其中 $\\Phi^{-1}$ 是标准正态分布的分位数函数，如果残差是正态分布的，这使得 $s$ 成为标准差 $\\sigma$ 的近似无偏估计量。\n\n**2.3.3. 稳健性权重更新：**\n用于下一次迭代的新稳健性权重 $\\delta_i$ 使用双平方（或双权）函数计算，这是一种再降M估计量：\n$$ \\delta_i \\leftarrow B\\left(\\frac{r_i}{6s}\\right) $$\n其中，当 $|u| \\le 1$ 时，$B(u) = (1 - u^2)^2$，否则 $B(u)=0$。因子 $6$ 是一个在效率和稳健性之间取得平衡的调节常数。残差大于 $6s$ 的点被赋予零权重，从而有效地将它们从下一次拟合迭代中移除。\n\n这个过程（步骤 2.2 和 2.3）重复指定的稳健迭代次数（$k_{robust}=2$）。最终的 LOESS 拟合在最后一次迭代中获得。\n\n**3. 性能评估和崩溃分析**\n\n通过在一个由 $N_{grid}=200$ 个在 $[0, 1]$ 中等距分布的点组成的精细评估网格上，计算其相对于真实函数 $f(x)$ 的均方根误差（RMSE），来评估 LOESS 估计器的稳健性。对于给定的污染比例 $p$，RMSE为：\n$$ \\text{RMSE}_p = \\sqrt{\\frac{1}{N_{grid}} \\sum_{j=1}^{N_{grid}} (\\hat{y}_{p}(x_{eval, j}) - f(x_{eval, j}))^2} $$\n其中 $\\hat{y}_{p}$ 是在污染比例为 $p$ 下的最终 LOESS 估计值。\n\n如果在某个污染水平 $p$ 下的 RMSE 显著高于无污染（$p=0.0$）时的基线 RMSE，则定义为发生崩溃。条件是：\n$$ \\frac{\\text{RMSE}_p}{\\text{RMSE}_{0.0}} > \\tau $$\n阈值为 $\\tau=3.0$。\n\n程序为每个指定的污染比例 $p \\in \\{0.0, 0.1, 0.3, 0.5\\}$ 计算这个值。最终输出包括一个对应每个 $p$ 的布尔值，指示是否发生崩溃，以及在该集合中观察到崩溃的最小 $p$ 值。如果在所有测试的比例下都没有发生崩溃，则该值报告为 $1.0$。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the robustness of an iteratively reweighted LOESS estimator.\n    \"\"\"\n    \n    # --- Fixed Parameters from Problem Statement ---\n    N = 200\n    DEGREE = 1\n    ALPHA = 0.8\n    SIGMA = 0.1\n    OUTLIER_AMPLITUDE = 8.0\n    ROBUST_ITERS = 2\n    BREAKDOWN_THRESHOLD = 3.0\n    N_GRID = 200\n    P_VALUES = [0.0, 0.1, 0.3, 0.5]\n    SEED = 42\n\n    # --- Helper Functions for LOESS ---\n\n    def tricube_weight(u):\n        \"\"\"Tricube weight function for neighborhood weights.\"\"\"\n        u = np.abs(u)\n        return np.where(u  1, (1 - u**3)**3, 0)\n\n    def bisquare_weight(u):\n        \"\"\"Bisquare weight function for robustness weights.\"\"\"\n        u = np.abs(u)\n        return np.where(u = 1, (1 - u**2)**2, 0)\n\n    def loess_fit(eval_points, x_data, y_data, robust_weights, alpha, degree):\n        \"\"\"\n        Performs the LOESS fit for a given set of evaluation points.\n        \"\"\"\n        n_data = len(x_data)\n        q = int(np.ceil(alpha * n_data))\n        \n        y_pred = np.zeros(len(eval_points))\n\n        for i, x0 in enumerate(eval_points):\n            dists = np.abs(x_data - x0)\n            \n            # Find neighborhood\n            sorted_indices = np.argsort(dists)\n            neighborhood_indices = sorted_indices[:q]\n            \n            # Neighborhood radius\n            h = dists[sorted_indices[q-1]]\n            \n            if h == 0.0:\n                # All points in neighborhood are at x0. Take weighted average.\n                mask = dists == 0\n                r_w = robust_weights[mask]\n                y_at_x0 = y_data[mask]\n                if np.sum(r_w) > 0:\n                    y_pred[i] = np.sum(r_w * y_at_x0) / np.sum(r_w)\n                else:\n                    y_pred[i] = np.mean(y_at_x0) # Fallback if all weights are zero\n                continue\n\n            # Data for local regression\n            x_hood = x_data[neighborhood_indices]\n            y_hood = y_data[neighborhood_indices]\n            \n            # Neighborhood weights (tricube)\n            scaled_dists = dists[neighborhood_indices] / h\n            neighborhood_w = tricube_weight(scaled_dists)\n            \n            # Total weights (neighborhood * robustness)\n            robust_w_hood = robust_weights[neighborhood_indices]\n            total_w = neighborhood_w * robust_w_hood\n            \n            # Weighted Least Squares\n            X = np.ones((q, degree + 1))\n            for d in range(1, degree + 1):\n                X[:, d] = (x_hood - x0)**d\n            \n            W = np.diag(total_w)\n            \n            A_matrix = X.T @ W @ X\n            b_vector = X.T @ W @ y_hood\n            \n            try:\n                beta = np.linalg.solve(A_matrix, b_vector)\n                y_pred[i] = beta[0]\n            except np.linalg.LinAlgError:\n                # Fallback to weighted average if matrix is singular\n                if np.sum(total_w) > 0:\n                    y_pred[i] = np.sum(total_w * y_hood) / np.sum(total_w)\n                else: \n                    # If all weights are 0, use unweighted neighborhood mean\n                    y_pred[i] = np.mean(y_hood)\n                    \n        return y_pred\n\n    def robust_loess_pipeline(x_data, y_data, eval_points, alpha, degree, n_robust_iters):\n        \"\"\"\n        Runs the full iteratively reweighted LOESS pipeline.\n        \"\"\"\n        n_data = len(x_data)\n        robust_weights = np.ones(n_data)\n        \n        # Mad constant for normal distribution\n        mad_c = norm.ppf(0.75) # Approximately 0.6745\n\n        # Total iterations = 1 (initial) + n_robust_iters\n        for k in range(n_robust_iters + 1):\n            if k > 0: # Update robust weights for iterations 1, 2, ...\n                # Predict at training points to get residuals\n                y_hat_train = loess_fit(x_data, x_data, y_data, robust_weights, alpha, degree)\n                residuals = y_data - y_hat_train\n                \n                # Robust scale estimation (MAD)\n                s = np.median(np.abs(residuals)) / mad_c\n                if s == 0:\n                    # If MAD is 0, no re-weighting unless there are non-zero residuals\n                    if np.any(residuals != 0):\n                        # Use a small non-zero scale to avoid division by zero\n                        s = np.mean(np.abs(residuals)) / mad_c\n                    else: # All residuals are zero, no need to re-weight\n                        break\n\n                # Bisquare robustness weights\n                scaled_residuals = residuals / (6 * s)\n                robust_weights = bisquare_weight(scaled_residuals)\n        \n        # Final fit on the evaluation grid using final weights\n        y_hat_eval = loess_fit(eval_points, x_data, y_data, robust_weights, alpha, degree)\n        return y_hat_eval\n\n    # --- Main Analysis Logic ---\n    rng = np.random.default_rng(SEED)\n\n    def f_true(x):\n        return np.sin(2 * np.pi * x)\n\n    def generate_data(p):\n        n_clean = int(N * (1-p))\n        n_outliers = N - n_clean\n\n        # Generate base clean data\n        x_clean_full = rng.uniform(0, 1, N)\n        y_clean_full = f_true(x_clean_full) + rng.normal(0, SIGMA, N)\n        \n        # Select a subset of clean points\n        clean_indices = rng.choice(N, size=n_clean, replace=False)\n        x_final = x_clean_full[clean_indices]\n        y_final = y_clean_full[clean_indices]\n\n        if n_outliers > 0:\n            # Generate outliers\n            n_out_x0 = n_outliers // 2\n            n_out_x1 = n_outliers - n_out_x0\n            \n            x_outliers = np.concatenate([np.zeros(n_out_x0), np.ones(n_out_x1)])\n            y_outliers = np.concatenate([\n                np.full(n_out_x0, OUTLIER_AMPLITUDE),\n                np.full(n_out_x1, -OUTLIER_AMPLITUDE)\n            ])\n            \n            # Combine clean and outlier data\n            x_final = np.concatenate([x_final, x_outliers])\n            y_final = np.concatenate([y_final, y_outliers])\n            \n        return x_final, y_final\n\n    eval_grid = np.linspace(0, 1, N_GRID)\n    y_true_grid = f_true(eval_grid)\n    \n    rmses = []\n    for p in P_VALUES:\n        x, y = generate_data(p)\n        y_hat = robust_loess_pipeline(x, y, eval_grid, ALPHA, DEGREE, ROBUST_ITERS)\n        \n        rmse = np.sqrt(np.mean((y_hat - y_true_grid)**2))\n        rmses.append(rmse)\n\n    # --- Breakdown Analysis ---\n    rmse0 = rmses[0]\n    breakdowns = [r / rmse0 > BREAKDOWN_THRESHOLD for r in rmses]\n    \n    breakdown_p = 1.0\n    for i, p in enumerate(P_VALUES):\n        if breakdowns[i]:\n            breakdown_p = p\n            break\n            \n    # Format and print the final result\n    result_list = [str(b) for b in breakdowns]\n    result_list.append(f\"{breakdown_p:.3f}\")\n    \n    print(f\"[{','.join(result_list)}]\")\n\n\nsolve()\n```", "id": "3141335"}, {"introduction": "除了作为一种强大的平滑工具，局部回归还可以被创造性地用于更高级的探索性数据分析。这个练习将展示LOESS如何像一个可调谐的“滤波器”，将复杂信号分解为不同尺度或频率的组成部分。通过在一系列递增的平滑窗口（spans）下迭代应用LOESS，您将学习如何执行多尺度分解，从而获得类似于信号处理中傅里叶分析或小波分析的洞察力，并对平滑参数的真正含义有更深刻的直观理解。[@problem_id:3141280]", "problem": "基于加权最小二乘和局部多项式逼近的基本原理，实现一个多尺度局部估计散点图平滑（LOESS）分解。其目标是在逐渐增大的跨度上构建一系列局部加权线性拟合，并解释每个尺度下的残差。从定义开始，位置 $x_0$ 处的局部回归估计是通过最小化加权平方误差和得到的，其中权重随着与 $x_0$ 距离的增加而减小。使用一个紧支撑、单調递减的核函数来定义局部性。满足这些性质的一个标准选择是三次立方核函数。对于每个跨度参数 $ \\alpha \\in (0,1] $，邻域大小为 $k = \\lceil \\alpha n \\rceil$，其中 $n$ 是观测点的数量，权重在超出第 $k$ 个最近邻的距离后为零。\n\n设数据为 $ \\{(x_i, y_i)\\}_{i=1}^n $，其中 $x_i$ 严格递增，$y_i$ 为实数值。对于给定的跨度 $ \\alpha $，定义LOESS算子 $L_{\\alpha}$ 如下：对于每个位置 $x_0$，根据绝对距离 $|x_i - x_0|$ 选择 $k = \\lceil \\alpha n \\rceil$ 个最近的点。令 $d_{\\max}$ 为这 $k$ 个邻居中的最大距离。对每个 $i$，定义权重\n$$\nw_i(x_0) = \\begin{cases}\n\\left(1 - \\left(\\frac{|x_i - x_0|}{d_{\\max}}\\right)^3\\right)^3,  \\text{if } |x_i - x_0| \\le d_{\\max}, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n通过最小化以下表达式来获得 $x_0$ 处的局部线性拟合\n$$\n\\sum_{i=1}^n w_i(x_0) \\left( y_i - \\beta_0(x_0) - \\beta_1(x_0)\\,(x_i - x_0) \\right)^2\n$$\n关于 $ \\beta_0(x_0) $ 和 $ \\beta_1(x_0) $。$x_0$ 处的 LOESS 估计值为 $ \\hat{y}(x_0) = \\beta_0(x_0) $。\n\n使用一个严格递增的跨度序列 $ \\alpha_1  \\alpha_2  \\cdots  \\alpha_m $ 来构建多尺度分解。通过以下方式迭代定义残差和分量\n$$\nr_0 = y, \\quad c_s = L_{\\alpha_s}(r_{s-1}), \\quad r_s = r_{s-1} - c_s, \\quad s = 1,2,\\dots,m.\n$$\n这产生以下分解\n$$\ny = \\sum_{s=1}^m c_s + r_m,\n$$\n其中 $c_s$ 表示在尺度 $s$ 捕获的分量，$r_m$ 是最终的余项。\n\n所有三角函数的参数必须以弧度解释。在所有涉及随机性的计算中，使用固定的伪随机种子 $42$ 以确保可复现性。\n\n测试套件和任务：\n- 对于每个测试用例，根据指定的参数生成合成数据，并应用上述多尺度 LOESS 分解。然后计算所需的度量指标，并根据下面定义的阈值返回布尔结果。\n\n- 测试用例 1 (一般情况):\n  - 参数：$n = 201$，$x_i$ 在 $[0,1]$ 上均匀分布，跨度 $(\\alpha_1,\\alpha_2,\\alpha_3) = (0.1,0.3,0.6)$。\n  - 信号分量：\n    $$\n    g_3(x) = 0.8 \\sin(2\\pi x), \\quad g_2(x) = 0.4 \\sin(10\\pi x), \\quad g_1(x) = 0.2 \\sin(30\\pi x),\n    $$\n    $$\n    y(x) = g_3(x) + g_2(x) + g_1(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2), \\quad \\sigma = 0.05.\n    $$\n  - 分解：计算 $c_1, c_2, c_3$ 和 $r_3$。\n  - 度量指标：计算 $c_1$ 和 $g_1$ 之间、$c_2$ 和 $g_2$ 之间、以及 $c_3$ 和 $g_3$ 之间的皮尔逊相关系数绝对值。\n  - 阈值和布尔值：\n    $$\n    b_{11} = \\left(|\\mathrm{corr}(c_1,g_1)| \\ge 0.8\\right), \\quad\n    b_{12} = \\left(|\\mathrm{corr}(c_2,g_2)| \\ge 0.9\\right), \\quad\n    b_{13} = \\left(|\\mathrm{corr}(c_3,g_3)| \\ge 0.95\\right).\n    $$\n\n- 测试用例 2 (边界条件：较小的 $n$ 和较高的噪声):\n  - 参数：$n = 61$，$x_i$ 在 $[0,1]$ 上均匀分布，跨度 $(\\alpha_1,\\alpha_2,\\alpha_3) = (0.15,0.4,0.7)$。\n  - 信号分量：\n    $$\n    g_3(x) = 0.7 \\sin(2\\pi x), \\quad g_2(x) = 0.5 \\sin(8\\pi x), \\quad g_1(x) = 0.3 \\sin(24\\pi x),\n    $$\n    $$\n    y(x) = g_3(x) + g_2(x) + g_1(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2), \\quad \\sigma = 0.10.\n    $$\n  - 分解：计算 $c_1, c_2, c_3$ 和 $r_3$。\n  - 度量指标：$c_1$ 和 $g_1$ 之间、$c_2$ 和 $g_2$ 之间、以及 $c_3$ 和 $g_3$ 之间的皮尔逊相关系数绝对值。\n  - 阈值和布尔值：\n    $$\n    b_{21} = \\left(|\\mathrm{corr}(c_1,g_1)| \\ge 0.6\\right), \\quad\n    b_{22} = \\left(|\\mathrm{corr}(c_2,g_2)| \\ge 0.8\\right), \\quad\n    b_{23} = \\left(|\\mathrm{corr}(c_3,g_3)| \\ge 0.9\\right).\n    $$\n\n- 测试用例 3 (边缘情况：主导趋势，无中频或高频分量):\n  - 参数：$n = 121$，$x_i$ 在 $[0,1]$ 上均匀分布，跨度 $(\\alpha_1,\\alpha_2,\\alpha_3) = (0.1,0.3,0.6)$。\n  - 信号分量：\n    $$\n    g_3(x) = 0.9 \\cos(1.5\\pi x), \\quad g_2(x) = 0, \\quad g_1(x) = 0,\n    $$\n    $$\n    y(x) = g_3(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2), \\quad \\sigma = 0.05.\n    $$\n  - 分解：计算 $c_1, c_2, c_3$ 和 $r_3$。\n  - 度量指标：令 $\\mathrm{Var}(z)$ 表示样本的经验方差。计算 $v_y = \\mathrm{Var}(y)$、$v_{c_3} = \\mathrm{Var}(c_3)$ 和 $v_{12} = \\mathrm{Var}(c_1) + \\mathrm{Var}(c_2)$。同时计算 $|\\mathrm{corr}(c_3,g_3)|$。\n  - 阈值和布尔值：\n    $$\n    b_{31} = \\left(\\frac{v_{c_3}}{v_y} \\ge 0.7\\right), \\quad\n    b_{32} = \\left(\\frac{v_{12}}{v_y} \\le 0.2\\right), \\quad\n    b_{33} = \\left(|\\mathrm{corr}(c_3,g_3)| \\ge 0.9\\right).\n    $$\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的布尔结果，聚合为一个用方括号括起来的逗号分隔列表。顺序必须是\n$$\n[b_{11}, b_{12}, b_{13}, b_{21}, b_{22}, b_{23}, b_{31}, b_{32}, b_{33}].\n$$\n所有布尔值必须是编程语言的原生布尔文字格式。", "solution": "用户要求实现一个多尺度局部估计散点图平滑（LOESS）分解。将对该过程进行验证，然后遵循指定的约束条件开发一个完整的解决方案。\n\n### 步骤 1：提取已知条件\n\n-   **数据**：一组 $n$ 个观测值 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $x_i$ 严格递增，$y_i$ 为实数值。\n-   **LOESS 算子定义**：\n    -   **跨度参数**：$\\alpha \\in (0,1]$。\n    -   **邻域大小**：$k = \\lceil \\alpha n \\rceil$。\n    -   **邻域**：对于每个点 $x_0$，具有最小距离 $|x_i - x_0|$ 的 $k$ 个点。\n    -   **最大距离**：$d_{\\max}$ 是邻域内的最大距离 $|x_i - x_0|$。\n    -   **权重函数（三次立方核函数）**：对于每个点 $x_i$ 相对于目标点 $x_0$：\n        $$\n        w_i(x_0) = \\begin{cases}\n        \\left(1 - \\left(\\frac{|x_i - x_0|}{d_{\\max}}\\right)^3\\right)^3,  \\text{if } |x_i - x_0| \\le d_{\\max}, \\\\\n        0,  \\text{otherwise}.\n        \\end{cases}\n        $$\n    -   **局部多项式拟合**：在每个 $x_0$ 处，找到最小化加权平方误差和的系数 $\\beta_0(x_0)$ 和 $\\beta_1(x_0)$：\n        $$\n        \\sum_{i=1}^n w_i(x_0) \\left( y_i - \\beta_0(x_0) - \\beta_1(x_0)\\,(x_i - x_0) \\right)^2\n        $$\n    -   **LOESS 估计**：在 $x_0$ 处的估计值为 $\\hat{y}(x_0) = \\beta_0(x_0)$。该算子表示为 $L_{\\alpha}$。\n-   **多尺度分解**：\n    -   **跨度**：一个严格递增的序列 $\\alpha_1  \\alpha_2  \\cdots  \\alpha_m$。\n    -   **迭代过程**：对于 $s = 1, 2, \\dots, m$：\n        -   初始残差：$r_0 = y$。\n        -   分量：$c_s = L_{\\alpha_s}(r_{s-1})$。\n        -   下一个残差：$r_s = r_{s-1} - c_s$。\n    -   **分解公式**：$y = \\sum_{s=1}^m c_s + r_m$。\n-   **可复现性**：使用固定的伪随机种子 $42$。\n-   **测试用例**：提供了三个具体的测试用例，包含 $n$、 $x_i$ 分布、 $\\alpha$ 值、信号分量 $g_s(x)$ 和噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 的参数。每个用例都定义了需要评估的度量指标（皮尔逊相关系数、方差比）和布尔阈值（$b_{ij}$）。\n-   **最终输出**：一个单行字符串，表示布尔结果的列表：$[b_{11}, b_{12}, b_{13}, b_{21}, b_{22}, b_{23}, b_{31}, b_{32}, b_{33}]$。\n\n### 步骤 2：使用提取的已知条件进行验证\n\n1.  **科学依据**：该问题描述了 LOESS，一种由 Cleveland (1979) 开发的、成熟的非参数回归方法。使用局部多项式回归、加权最小二乘法和三次立方核函数都是该方法的标准组成部分。多尺度分解是一种有效的统计技术，用于分析不同分辨率下的时间序列或其他序列数据，概念上类似于小波分解或季节性趋势分解（STL）。\n2.  **适定性**：问题的核心是为局部线性拟合求解一个加权最小二乘（WLS）问题。对于一个给定的点 $x_0$，只要设计矩阵是满秩的，WLS 问题就有唯一解。在 $x_0$ 处的局部线性拟合的设计矩阵包含常数项（1）和线性项（$x_i - x_0$）的列。由于问题规定 $x_i$ 是严格递增的，任何大小为 $k \\ge 2$ 的邻域都将包含具有不同 $x$ 值的点。这确保了设计矩阵不是秩亏的，并且 $(\\beta_0, \\beta_1)$ 的唯一解存在。所有测试用例使用的参数都导致 $k \\ge 10$，因此问题是适定的。\n3.  **客观性**：所有的定义、参数和任务都以数学和算法的精度进行了规定。信号生成、噪声模型和评估指标（相关性、方差）都是客观且可量化的。\n4.  **完整性**：问题提供了所有必要的信息：LOESS 平滑器的数学公式、迭代分解的结构，以及包含所有必需参数和评估标准的完整指定的测试用例。没有缺少任何必要信息。\n5.  **一致性**：已知条件中没有矛盾之处。三次立方核函数的定义与其在 LOESS 中的使用是一致的。分解过程在逻辑上是合理的。测试用例旨在以有意義的方式探究分解的行为（例如，较小的跨度捕获较高频率的分量）。\n\n### 步骤 3：结论与行动\n\n该问题具有科学依据、适定、客观、完整且内部一致。这是计算统计学中的一个标准数值实现任务。该问题是 **有效的**。将提供一个解决方案。\n\n### 求解推导\n\n解决方案需要实现 LOESS 算子 $L_{\\alpha}$，然后迭代地应用它。\n\n#### LOESS 平滑器：$L_{\\alpha}$\n对于给定的数据集 $(x, y)$ 和跨度 $\\alpha$，平滑向量 $\\hat{y} = L_{\\alpha}(y)$ 是通过在数据集中的每个点 $x_j$ (其中 $j=1, \\dots, n$) 应用局部回归过程来计算的。\n\n1.  **邻域选择**：在每个目标点 $x_j$ 处，我们必须从集合 $\\{x_i\\}_{i=1}^n$ 中识别出其 $k = \\lceil \\alpha n \\rceil$ 个最近邻。这通过计算所有 $i$ 的距离 $|x_i - x_j|$ 并选择与最小距离对应的 $k$ 个点来完成。\n\n2.  **权重计算**：为邻域中的点计算三次立方权重。设邻居索引集为 $\\mathcal{N}_j$。此邻域中的最大距离为 $d_{\\max,j} = \\max_{i \\in \\mathcal{N}_j} |x_i - x_j|$。然后为 $i \\in \\mathcal{N}_j$ 计算权重为 $w_i(x_j) = (1 - (|x_i-x_j|/d_{\\max,j})^3)^3$。所有其他权重为零。\n\n3.  **加权最小二乘（WLS）**：我们求解最小化目标函数的系数 $\\beta_0(x_j)$ 和 $\\beta_1(x_j)$。这是一个 WLS 问题。让上划线表示限制在邻域 $\\mathcal{N}_j$ 内的向量/矩阵。模型是 $\\bar{y} \\approx \\beta_0 + \\beta_1(\\bar{x} - x_j)$。设计矩阵是 $\\mathbf{X}_j = \\begin{pmatrix} 1  \\bar{x}_1 - x_j \\\\ \\vdots  \\vdots \\\\ 1  \\bar{x}_k - x_j \\end{pmatrix}$。令 $\\mathbf{W}_j$ 是一个对角矩阵，其对角线上是权重 $w_i(x_j)$。$\\boldsymbol{\\beta}_j = [\\beta_0(x_j), \\beta_1(x_j)]^T$ 的 WLS 解由以下公式给出：\n    $$\n    \\boldsymbol{\\beta}_j = (\\mathbf{X}_j^T \\mathbf{W}_j \\mathbf{X}_j)^{-1} \\mathbf{X}_j^T \\mathbf{W}_j \\bar{y}\n    $$\n    $x_j$ 处的 LOESS 估计是此局部拟合的截距项，$\\hat{y}_j = \\beta_0(x_j)$。\n\n#### 多尺度分解\n这是一个 LOESS 平滑器的直接迭代应用。从初始数据作为第一个残差开始，$r_0 = y$：\n-   第一个分量，捕获最精细的尺度，是 $c_1 = L_{\\alpha_1}(r_0)$。\n-   更新残差：$r_1 = r_0 - c_1$。\n-   第二个分量通过平滑新的残差找到：$c_2 = L_{\\alpha_2}(r_1)$。\n-   再次更新残差：$r_2 = r_1 - c_2$。\n-   这个过程对所有 $m$ 个跨度继续进行。\n\n最终分解为 $y = c_1 + c_2 + \\dots + c_m + r_m$，其中 $r_m$ 是最终残差。\n\n#### 测试用例和度量指标\n实现将遵循此逻辑为每个测试用例生成数据，应用分解，并计算所需的度量指标。对于相关性，将使用皮尔逊相关系数。对于方差，根据“经验方差”的含义，将使用样本方差（$ddof=1$）。计算出的度量指标将与给定的阈值进行比较，以生成最终的布尔值。", "answer": "```python\nimport numpy as np\n\ndef loess_smoother(x, y, alpha):\n    \"\"\"\n    Computes the LOESS smoothed values for a given 1D data set.\n\n    Args:\n        x (np.ndarray): The independent variable values, must be sorted.\n        y (np.ndarray): The dependent variable values.\n        alpha (float): The span parameter, determining the neighborhood size.\n\n    Returns:\n        np.ndarray: The smoothed y-values.\n    \"\"\"\n    n = len(x)\n    k = int(np.ceil(alpha * n))\n    \n    # In all test cases, k >= 10, so this check is not strictly necessary but good practice.\n    if k  2:\n        raise ValueError(\"Span alpha is too small, resulting in k  2 neighbors.\")\n\n    y_hat = np.empty(n)\n\n    for j in range(n):\n        x0 = x[j]\n\n        # 1. Find k-nearest neighbors and max distance\n        distances = np.abs(x - x0)\n        neighbor_indices = np.argsort(distances)[:k]\n        d_max = distances[neighbor_indices[-1]]\n\n        # 2. Calculate tri-cube weights. Handle d_max=0 as a safeguard.\n        if d_max == 0:\n            y_hat[j] = np.mean(y[neighbor_indices])\n            continue\n\n        u = distances[neighbor_indices] / d_max\n        weights = (1 - u**3)**3\n\n        # 3. Perform weighted least squares\n        x_neigh = x[neighbor_indices]\n        y_neigh = y[neighbor_indices]\n        \n        # Design matrix is centered at x0\n        X_neigh = np.vstack([np.ones(k), x_neigh - x0]).T\n\n        # Efficiently compute (X.T * W * X) and (X.T * W * y)\n        X_w = X_neigh * weights[:, np.newaxis]\n        A = X_w.T @ X_neigh\n        b = X_w.T @ y_neigh\n        \n        try:\n            # Solve for beta = [beta0, beta1]\n            beta = np.linalg.solve(A, b)\n            # The fitted value at x0 is the intercept term\n            y_hat[j] = beta[0]\n        except np.linalg.LinAlgError:\n            # Fallback to local weighted average if matrix is singular (numerically)\n            y_hat[j] = np.sum(weights * y_neigh) / np.sum(weights)\n\n    return y_hat\n\ndef multi_scale_loess(x, y, alphas):\n    \"\"\"\n    Performs a multi-scale LOESS decomposition.\n\n    Args:\n        x (np.ndarray): The independent variable values.\n        y (np.ndarray): The dependent variable values.\n        alphas (tuple or list): A sequence of increasing span parameters.\n\n    Returns:\n        tuple: A tuple containing the list of components and the final residual.\n    \"\"\"\n    residuals = y.copy()\n    components = []\n    for alpha in alphas:\n        c = loess_smoother(x, residuals, alpha)\n        components.append(c)\n        residuals -= c\n    return components, residuals\n\ndef pearson_corr(u, v):\n    \"\"\"\n    Computes the Pearson correlation coefficient between two vectors.\n    Handles constant vectors by returning 0.0.\n    \"\"\"\n    if np.std(u) == 0 or np.std(v) == 0:\n        return 0.0\n    return np.corrcoef(u, v)[0, 1]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    np.random.seed(42)\n    final_results = []\n\n    # --- Test Case 1 ---\n    n1 = 201\n    x1 = np.linspace(0, 1, n1)\n    alphas1 = (0.1, 0.3, 0.6)\n    sigma1 = 0.05\n    \n    g1_high_freq = 0.2 * np.sin(30 * np.pi * x1)\n    g1_med_freq = 0.4 * np.sin(10 * np.pi * x1)\n    g1_low_freq = 0.8 * np.sin(2 * np.pi * x1)\n\n    eps1 = np.random.normal(0, sigma1, n1)\n    y1 = g1_high_freq + g1_med_freq + g1_low_freq + eps1\n    \n    components1, _ = multi_scale_loess(x1, y1, alphas1)\n    c1, c2, c3 = components1\n    \n    corr1 = np.abs(pearson_corr(c1, g1_high_freq))\n    corr2 = np.abs(pearson_corr(c2, g1_med_freq))\n    corr3 = np.abs(pearson_corr(c3, g1_low_freq))\n\n    b11 = corr1 >= 0.8\n    b12 = corr2 >= 0.9\n    b13 = corr3 >= 0.95\n    final_results.extend([b11, b12, b13])\n\n    # --- Test Case 2 ---\n    n2 = 61\n    x2 = np.linspace(0, 1, n2)\n    alphas2 = (0.15, 0.4, 0.7)\n    sigma2 = 0.10\n    \n    g2_high_freq = 0.3 * np.sin(24 * np.pi * x2)\n    g2_med_freq = 0.5 * np.sin(8 * np.pi * x2)\n    g2_low_freq = 0.7 * np.sin(2 * np.pi * x2)\n\n    eps2 = np.random.normal(0, sigma2, n2)\n    y2 = g2_high_freq + g2_med_freq + g2_low_freq + eps2\n    \n    components2, _ = multi_scale_loess(x2, y2, alphas2)\n    c1, c2, c3 = components2\n    \n    corr1 = np.abs(pearson_corr(c1, g2_high_freq))\n    corr2 = np.abs(pearson_corr(c2, g2_med_freq))\n    corr3 = np.abs(pearson_corr(c3, g2_low_freq))\n    \n    b21 = corr1 >= 0.6\n    b22 = corr2 >= 0.8\n    b23 = corr3 >= 0.9\n    final_results.extend([b21, b22, b23])\n\n    # --- Test Case 3 ---\n    n3 = 121\n    x3 = np.linspace(0, 1, n3)\n    alphas3 = (0.1, 0.3, 0.6)\n    sigma3 = 0.05\n    \n    g3_low_freq = 0.9 * np.cos(1.5 * np.pi * x3)\n    \n    eps3 = np.random.normal(0, sigma3, n3)\n    y3 = g3_low_freq + eps3\n\n    components3, _ = multi_scale_loess(x3, y3, alphas3)\n    c1, c2, c3 = components3\n    \n    # Use ddof=1 for sample variance as per \"empirical variance\"\n    v_y = np.var(y3, ddof=1)\n    v_c3 = np.var(c3, ddof=1)\n    v_12 = np.var(c1, ddof=1) + np.var(c2, ddof=1)\n    corr3 = np.abs(pearson_corr(c3, g3_low_freq))\n    \n    # Avoid division by zero if variance is zero\n    if v_y > 0:\n        b31 = (v_c3 / v_y) >= 0.7\n        b32 = (v_12 / v_y) = 0.2\n    else: # Should not happen with the given data generation\n        b31, b32 = False, False\n\n    b33 = corr3 >= 0.9\n    final_results.extend([b31, b32, b33])\n    \n    # Format the final output string\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3141280"}]}