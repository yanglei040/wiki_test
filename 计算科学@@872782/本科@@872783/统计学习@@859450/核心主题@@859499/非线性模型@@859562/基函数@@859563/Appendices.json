{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本节将通过一系列动手练习，帮助你深入理解基函数的核心概念及其在实际问题中的应用。第一个练习将回归本源，探讨如何构建一组正交基。在函数空间中，正交基能够极大简化计算，类似于在几何学中使用正交坐标系。通过运用格拉姆-施密特（Gram-Schmidt）过程，你将亲手将一组简单的非正交函数（如 $v_1(x) = 1$ 和 $v_2(x) = x$）转化为一组正交基，为后续更复杂的应用打下坚实的数学基础。[@problem_id:2161554]", "problem": "在数值分析和逼近理论中，从一个更简单的非正交集合构造一组正交基函数通常很有用。考虑在区间 $[0, 1]$ 上连续的实值函数空间。在此空间中，两个函数 $f(x)$ 和 $g(x)$ 的内积定义为：\n$$ \\langle f, g \\rangle = \\int_{0}^{1} f(x)g(x) \\, dx $$\n如果两个函数的内积为零，则认为它们是正交的。\n\n从初始的非正交基函数集 $\\{v_1(x), v_2(x)\\}$（其中 $v_1(x) = 1$ 和 $v_2(x) = x$）出发，通过应用以下步骤构造一个新的正交基函数集 $\\{u_1(x), u_2(x)\\}$：\n1.  将第一个正交函数设置为与第一个初始函数相同：$u_1(x) = v_1(x)$。\n2.  通过减去 $v_2(x)$ 在 $u_1(x)$ 上的投影来构造第二个正交函数：\n    $$ u_2(x) = v_2(x) - \\frac{\\langle v_2, u_1 \\rangle}{\\langle u_1, u_1 \\rangle} u_1(x) $$\n\n求所得的正交函数 $u_1(x)$ 和 $u_2(x)$。请将答案以函数对的形式呈现。", "solution": "我们在 $[0,1]$ 上的连续实值函数空间中进行计算，其内积为 $\\langle f,g\\rangle=\\int_{0}^{1}f(x)g(x)\\,dx$。初始集合为 $v_{1}(x)=1$ 和 $v_{2}(x)=x$。\n\n步骤 1：根据给定步骤，设 $u_{1}(x)=v_{1}(x)=1$。\n\n步骤 2：计算 $v_{2}$ 在 $u_{1}$ 上的投影系数：\n$$\n\\langle v_{2},u_{1}\\rangle=\\int_{0}^{1}x\\cdot 1\\,dx=\\int_{0}^{1}x\\,dx=\\frac{1}{2},\n\\qquad\n\\langle u_{1},u_{1}\\rangle=\\int_{0}^{1}1\\cdot 1\\,dx=\\int_{0}^{1}1\\,dx=1.\n$$\n因此，\n$$\nu_{2}(x)=v_{2}(x)-\\frac{\\langle v_{2},u_{1}\\rangle}{\\langle u_{1},u_{1}\\rangle}u_{1}(x)\n=x-\\frac{\\frac{1}{2}}{1}\\cdot 1=x-\\frac{1}{2}.\n$$\n\n正交性验证：\n$$\n\\langle u_{1},u_{2}\\rangle=\\int_{0}^{1}1\\cdot\\left(x-\\frac{1}{2}\\right)\\,dx=\\int_{0}^{1}x\\,dx-\\frac{1}{2}\\int_{0}^{1}1\\,dx=\\frac{1}{2}-\\frac{1}{2}=0,\n$$\n所以 $u_{1}$ 和 $u_{2}$ 是正交的。因此，该正交函数对是 $u_{1}(x)=1$ 和 $u_{2}(x)=x-\\frac{1}{2}$。", "answer": "$$\\boxed{\\begin{pmatrix}1 & x-\\frac{1}{2}\\end{pmatrix}}$$", "id": "2161554"}, {"introduction": "正交性不仅仅是一个优美的数学概念，它在统计建模中具有至关重要的实用价值，尤其是在保证数值稳定性方面。这个练习将引导你通过编程实验，直观地对比两种常见的多项式基：单项式基（$x^j$）和正交多项式基（勒让德多项式）。你将研究当输入数据经过简单的缩放和平移（$x \\mapsto ax+b$）后，两种基在回归分析中的表现。通过这个练习，你将深刻体会到为什么正交基在实际应用中是更受青睐的选择，因为它们能有效避免由病态设计矩阵（ill-conditioned design matrix）引发的种种数值问题。[@problem_id:3102236]", "problem": "在统计学习的背景下，考虑使用线性基展开的监督回归。假设一个数据集由输入 $x_i$ 和目标 $t_i$（$i=1,\\dots,n$）组成，并考虑一个线性模型 $m(x) = \\sum_{j=0}^{d} w_j \\,\\phi_j(x)$，其中基函数 $\\phi_j$ 是固定的。最小二乘估计量通过最小化残差平方和来确定，并且可以使用经过充分测试的数值线性代数方法，从一个设计矩阵 $X$（其元素为 $X_{ij} = \\phi_j(x_i)$）计算得出。矩阵的2-范数条件数，记作 $\\kappa_2(X)$，量化了求解以 $X$ 为系数矩阵的线性系统的数值稳定性。\n\n本问题研究输入在仿射变换 $x \\mapsto y = a x + b$ 下的重缩放对两类基函数的影响：单项式（Vandermonde）基和正交多项式基。单项式基定义为 $\\phi_j(x) = x^j$（$j=0,\\dots,d$），其设计矩阵 $V$ 是范德蒙矩阵。这里考虑的正交多项式基是在标准化输入 $z$ 上求值的勒让德（Legendre）多项式族 $\\{P_j(z)\\}_{j=0}^{d}$。标准化是通过以下映射将观测到的输入范围映射到区间 $[-1,1]$ 来执行的：\n$$\nz = \\frac{2(x - m_x)}{r_x}, \\quad m_x = \\frac{x_{\\min} + x_{\\max}}{2}, \\quad r_x = x_{\\max} - x_{\\min},\n$$\n对于重缩放后的输入 $y$，也进行类似的标准化：\n$$\nz' = \\frac{2(y - m_y)}{r_y}, \\quad m_y = \\frac{y_{\\min} + y_{\\max}}{2}, \\quad r_y = y_{\\max} - y_{\\min}.\n$$\n勒让德多项式满足奇偶性质 $P_j(-z) = (-1)^j P_j(z)$。\n\n从上述核心定义出发，您将推导以下声明的算法测试：\n- 正交多项式基的不变性声明：当设计矩阵由标准化输入构建时，通过最小二乘法计算出的系数在变换 $x \\mapsto y = a x + b$ 下是稳定的：当 $a > 0$ 时系数不变，当 $a  0$ 时则发生一次可预测的奇偶性调整。\n- 范德蒙基的敏感性声明：单项式基的系数和设计矩阵的条件数对缩放因子 $a$ 和位移 $b$ 很敏感，通常会导致巨大的变化。\n\n您必须实现一个程序，对于下面指定的每个测试用例，按纯数学术语执行以下步骤：\n1. 在指定区间 $[x_{\\min}, x_{\\max}]$ 上生成等间距的输入 $x_i$。\n2. 计算目标 $t_i = f(x_i)$，其中 $f(x) = \\sin(x)$，角度单位为弧度。\n3. 使用基于 $x$ 构建的标准化输入 $z$ 上的正交多项式基 $\\{P_j(z)\\}_{j=0}^{d}$ 拟合一个 $d$ 次模型，得到系数 $w^{\\mathrm{leg}}$。然后应用重缩放 $y_i = a x_i + b$，从 $y$ 重新构建标准化输入 $z'$，并重新拟合以获得系数 $w'^{\\mathrm{leg}}$。\n4. 通过检查 $w'^{\\mathrm{leg}}$ 是否在对第 $j$ 个系数进行奇偶性调整 $(\\operatorname{sign}(a))^j$ 后与 $w^{\\mathrm{leg}}$ 相等来测试不变性声明，即检查 $\\max_j \\left| w'^{\\mathrm{leg}}_j - \\left(\\operatorname{sign}(a)\\right)^j w^{\\mathrm{leg}}_j \\right|$ 是否低于容差 $\\epsilon$。\n5. 使用单项式基对原始输入 $x$ 拟合一个 $d$ 次模型，得到系数 $w^{\\mathrm{van}}$，并对重缩放后的输入 $y$ 拟合模型，得到 $w'^{\\mathrm{van}}$。计算相对系数变化\n$$\n\\Delta = \\frac{\\left\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\right\\|_2}{\\left\\| w^{\\mathrm{van}} \\right\\|_2 + 10^{-12}},\n$$\n以及条件数比率\n$$\n\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)},\n$$\n其中 $V_x$ 和 $V_y$ 分别是基于 $x$ 和 $y$ 的范德蒙设计矩阵。\n6. 为每个测试用例返回一个列表 $[B, \\Delta, \\rho]$，其中 $B$ 是步骤4中不变性测试的布尔结果。\n\n使用具有标准2-范数目标的最小二乘法，并使用2-范数计算条件数。在不变性测试中使用容差 $\\epsilon = 10^{-9}$。正弦函数的角度单位必须是弧度。本问题中除弧度外不出现其他物理单位。\n\n测试套件：\n- 用例 1：$n = 50$，次数 $d = 10$，$x_{\\min} = -3$，$x_{\\max} = 3$，$a = 2$，$b = 1$。\n- 用例 2：$n = 6$，次数 $d = 5$，$x_{\\min} = -1$，$x_{\\max} = 1$，$a = -1$，$b = 0$。\n- 用例 3：$n = 50$，次数 $d = 10$，$x_{\\min} = -1$，$x_{\\max} = 2$，$a = 100$，$b = -5$。\n- 用例 4：$n = 50$，次数 $d = 10$，$x_{\\min} = -3$，$x_{\\max} = 3$，$a = 0.01$，$b = 100$。\n- 用例 5：$n = 50$，次数 $d = 10$，$x_{\\min} = -2$，$x_{\\max} = 4$，$a = 1$，$b = 10$。\n\n您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试用例对应一个元素。每个元素本身必须是 $[B,\\Delta,\\rho]$ 形式的列表。例如，一个有效的输出格式是 $[[\\text{True},0.1,2.0],[\\text{False},3.5,100.0]]$。", "solution": "该问题是有效的，因为它在科学上基于数值线性代数和统计回归的原理，问题陈述清晰且提供了所有必要信息，表述客观。它对两种常见的基函数选择进行了标准的、可验证的比较。\n\n问题的核心在于分析线性模型在输入变量经过仿射变换 $x \\mapsto y = a x + b$ 后的数值稳定性和系数不变性。我们研究两种类型的基展开：一种在标准化域上使用正交多项式（勒让德多项式），另一种使用朴素的单项式基。\n\n**1. 正交多项式基：勒让德多项式**\n\n勒让德多项式基稳定性的关键在于输入变量的标准化。对于在区间 $[v_{\\min}, v_{\\max}]$ 上定义的任何变量 $v$，其标准化映射由下式给出：\n$$\nz(v) = \\frac{2(v - m_v)}{r_v}, \\quad \\text{其中 } m_v = \\frac{v_{\\min} + v_{\\max}}{2} \\text{ and } r_v = v_{\\max} - v_{\\min}.\n$$\n此变换将区间 $[v_{\\min}, v_{\\max}]$ 映射到 $[-1, 1]$，这是勒让德多项式的规范正交域。然后使用基函数 $\\phi_j(x) = P_j(z(x))$ 构建模型。\n\n让我们分析仿射变换 $y = a x + b$ 对标准化变量的影响。原始输入 $x_i$ 位于 $[x_{\\min}, x_{\\max}]$ 内。重缩放后的输入 $y_i$ 将位于一个新的区间 $[y_{\\min}, y_{\\max}]$ 内。\n\n情况 1：$a > 0$。该变换是保序的。\n$y_{\\min} = a x_{\\min} + b$ 且 $y_{\\max} = a x_{\\max} + b$。\n新的中点是 $m_y = \\frac{(a x_{\\min} + b) + (a x_{\\max} + b)}{2} = a \\frac{x_{\\min} + x_{\\max}}{2} + b = a m_x + b$。\n新的范围是 $r_y = (a x_{\\max} + b) - (a x_{\\min} + b) = a (x_{\\max} - x_{\\min}) = a r_x$。\n新的标准化变量 $z'$ 为：\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{a r_x} = \\frac{2a(x - m_x)}{a r_x} = z(x).\n$$\n由于 $z'(y_i) = z(x_i)$，在新点上求值的基函数与旧的基函数相同：$\\phi_j(y_i) = P_j(z'(y_i)) = P_j(z(x_i)) = \\phi_j(x_i)$。因此，两次拟合的设计矩阵是相同的。由于目标向量 $\\mathbf{t}$ 保持不变，系数的最小二乘解也必定相同：$w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$。\n\n情况 2：$a  0$。该变换是逆序的。\n$y_{\\min} = a x_{\\max} + b$ 且 $y_{\\max} = a x_{\\min} + b$。\n中点 $m_y = a m_x + b$ 的推导类似。\n新的范围是 $r_y = (a x_{\\min} + b) - (a x_{\\max} + b) = a (x_{\\min} - x_{\\max}) = -a (x_{\\max} - x_{\\min}) = |a| r_x$。\n新的标准化变量 $z'$ 为：\n$$\nz'(y) = \\frac{2(y - m_y)}{r_y} = \\frac{2((ax+b) - (am_x+b))}{|a| r_x} = \\frac{a}{|a|} \\frac{2(x - m_x)}{r_x} = -z(x), \\text{ 因为 } a  0 \\implies a/|a| = -1.\n$$\n新拟合的基函数为 $\\phi_j(y_i) = P_j(z'(y_i)) = P_j(-z(x_i))$。利用勒让德多项式的奇偶性质 $P_j(-z) = (-1)^j P_j(z)$，我们得到 $\\phi_j(y_i) = (-1)^j P_j(z(x_i))$。\n令 $X^{\\mathrm{leg}}$ 为对 $x$ 拟合的设计矩阵，其元素为 $(X^{\\mathrm{leg}})_{ij} = P_j(z(x_i))$。令 $X'^{\\mathrm{leg}}$ 为对 $y$ 拟合的设计矩阵，其元素为 $(X'^{\\mathrm{leg}})_{ij} = P_j(z'(y_i))$。它们之间的关系是 $(X'^{\\mathrm{leg}})_{ij} = (-1)^j (X^{\\mathrm{leg}})_{ij}$。这可以写成矩阵形式 $X'^{\\mathrm{leg}} = X^{\\mathrm{leg}} S$，其中 $S$ 是一个对角矩阵，其对角元素为 $S_{jj} = (-1)^j$。\n\n两个最小二乘问题分别是找到 $w^{\\mathrm{leg}}$ 和 $w'^{\\mathrm{leg}}$ 以最小化 $\\| X^{\\mathrm{leg}} w^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$ 和 $\\| X'^{\\mathrm{leg}} w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$。将矩阵间的关系代入第二个问题，得到 $\\| (X^{\\mathrm{leg}} S) w'^{\\mathrm{leg}} - \\mathbf{t} \\|_2^2$。令 $w^* = S w'^{\\mathrm{leg}}$。问题变为找到 $w^*$ 以最小化 $\\| X^{\\mathrm{leg}} w^* - \\mathbf{t} \\|_2^2$。根据最小二乘解的唯一性，有 $w^* = w^{\\mathrm{leg}}$。因此，$S w'^{\\mathrm{leg}} = w^{\\mathrm{leg}}$。由于 $S$ 是其自身的逆（$S^2=I$），我们可以解出 $w'^{\\mathrm{leg}}$：$w'^{\\mathrm{leg}} = S^{-1} w^{\\mathrm{leg}} = S w^{\\mathrm{leg}}$，这意味着 $w'^{\\mathrm{leg}}_j = (-1)^j w^{\\mathrm{leg}}_j$。\n\n使用 $\\operatorname{sign}(a)$ 结合两种情况，我们发现 $w'^{\\mathrm{leg}}_j = (\\operatorname{sign}(a))^j w^{\\mathrm{leg}}_j$。不变性测试在数值容差 $\\epsilon=10^{-9}$ 内检查此关系。布尔值 $B$ 是此测试的结果。\n\n**2. 单项式基：范德蒙矩阵**\n\n单项式基定义为 $\\phi_j(x) = x^j$。设计矩阵 $V_x$ 的元素为 $(V_x)_{ij} = x_i^j$，它是一个范德蒙矩阵。与正交基不同，该基没有自我修正的标准化机制。\n\n当输入变换为 $y = ax+b$ 时，新的基函数为 $\\phi_j(y) = (ax+b)^j$。使用二项式展开：\n$$\n\\phi_j(y) = (ax+b)^j = \\sum_{k=0}^{j} \\binom{j}{k} (ax)^k b^{j-k} = \\sum_{k=0}^{j} \\left[ \\binom{j}{k} a^k b^{j-k} \\right] x^k = \\sum_{k=0}^{j} C_{jk} \\phi_k(x)\n$$\n每个新的基函数都是旧的、最高次数相同的基函数的线性组合。这在系数向量 $w^{\\mathrm{van}}$ 和 $w'^{\\mathrm{van}}$ 之间建立了复杂的关系，排除了任何简单的不变性。相对变化 $\\Delta = \\frac{\\| w'^{\\mathrm{van}} - w^{\\mathrm{van}} \\|_2}{\\| w^{\\mathrm{van}} \\|_2 + 10^{-12}}$ 预计会很大。\n\n范德蒙矩阵是出了名的病态矩阵，特别是当点所在区间远离原点或具有非常大/小的尺度时。矩阵的列是 $x_i$ 的幂，它们可能变得几乎共线。例如，如果所有的 $|x_i| \\gg 1$，向量 $[x_i^d]$ 和 $[x_i^{d-1}]$ 的方向将非常相似，导致很高的条件数 $\\kappa_2(V_x)$。变换 $y=ax+b$ 会极大地改变输入域的尺度和位置，这通常会加剧此问题。条件数之比 $\\rho = \\frac{\\kappa_2(V_y)}{\\kappa_2(V_x)}$ 量化了这种不稳定性。大的 $|a|$ 或 $|b|$ 值预计会产生大的 $\\rho$ 值。\n\n**3. 算法实现**\n\n上述分析的实现如下：\n- 对于每个测试用例，我们生成 $n$ 个输入 $x_i$ 和目标 $t_i = \\sin(x_i)$。\n- 对于勒让德基，我们首先分别对输入 $x_i$ 和重缩放后的输入 $y_i$ 进行标准化，然后计算勒让德多项式 $P_j$（$j=0, \\dots, d$）的值，从而构建设计矩阵 $X^{\\mathrm{leg}}$ 和 $X'^{\\mathrm{leg}}$。系数 $w^{\\mathrm{leg}}$ 和 $w'^{\\mathrm{leg}}$ 使用 `numpy.linalg.lstsq` 求得。然后测试不变性声明，得到布尔值 $B$。\n- 对于单项式基，我们使用 `numpy.vander` 构建范德蒙矩阵 $V_x$ 和 $V_y$。系数 $w^{\\mathrm{van}}$ 和 $w'^{\\mathrm{van}}$ 同样通过最小二乘法求得。然后计算相对系数变化 $\\Delta$ 和条件数比率 $\\rho$（使用 `numpy.linalg.cond`）。\n- 收集所有测试用例的结果 $[B, \\Delta, \\rho]$ 并按指定格式进行格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre\n\ndef run_case(n, d, x_min, x_max, a, b):\n    \"\"\"\n    Performs the calculations for a single test case.\n    \"\"\"\n    # Step 1: Generate evenly spaced inputs and compute targets\n    x = np.linspace(x_min, x_max, n, dtype=np.float64)\n    t = np.sin(x)\n\n    # --- Part 1: Orthogonal Polynomial Basis (Legendre) ---\n\n    # Step 3 (part 1): Fit the model using the original inputs x\n    m_x = (x_min + x_max) / 2.0\n    r_x = x_max - x_min\n    # Handle the case where the interval has zero width\n    z = 2.0 * (x - m_x) / r_x if r_x != 0 else np.zeros_like(x)\n    \n    X_leg_x = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_x[:, j] = p_j(z)\n        \n    w_leg, _, _, _ = np.linalg.lstsq(X_leg_x, t, rcond=None)\n\n    # Step 3 (part 2): Apply rescaling y = ax + b and refit the model\n    y = a * x + b\n    y_min, y_max = np.min(y), np.max(y)\n    \n    m_y = (y_min + y_max) / 2.0\n    r_y = y_max - y_min\n    z_prime = 2.0 * (y - m_y) / r_y if r_y != 0 else np.zeros_like(y)\n\n    X_leg_y = np.zeros((n, d + 1), dtype=np.float64)\n    for j in range(d + 1):\n        p_j = legendre(j)\n        X_leg_y[:, j] = p_j(z_prime)\n\n    w_prime_leg, _, _, _ = np.linalg.lstsq(X_leg_y, t, rcond=None)\n\n    # Step 4: Test the invariance claim for the Legendre basis coefficients\n    epsilon = 1e-9\n    s_a = np.sign(a) if a != 0 else 1\n    # The parity factor is (sign(a))^j for the j-th coefficient\n    parity_factor = np.array([s_a**j for j in range(d + 1)])\n    \n    expected_w_prime_leg = parity_factor * w_leg\n    max_abs_diff = np.max(np.abs(w_prime_leg - expected_w_prime_leg))\n    B = bool(max_abs_diff  epsilon)\n\n    # --- Part 2: Monomial Basis (Vandermonde) ---\n\n    # Step 5 (part 1): Fit models using the monomial basis\n    V_x = np.vander(x, d + 1, increasing=True)\n    w_van, _, _, _ = np.linalg.lstsq(V_x, t, rcond=None)\n\n    V_y = np.vander(y, d + 1, increasing=True)\n    w_prime_van, _, _, _ = np.linalg.lstsq(V_y, t, rcond=None)\n    \n    # Step 5 (part 2): Compute the relative coefficient change Delta\n    delta_num = np.linalg.norm(w_prime_van - w_van)\n    delta_den = np.linalg.norm(w_van) + 1e-12\n    Delta = delta_num / delta_den\n\n    # Step 5 (part 3): Compute the condition number ratio rho\n    kappa_x = np.linalg.cond(V_x, 2)\n    kappa_y = np.linalg.cond(V_y, 2)\n    \n    rho = kappa_y / kappa_x if kappa_x != 0 else np.inf\n\n    # Step 6: Return the list of results for this case\n    return [B, Delta, rho]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # n, d, xmin, xmax, a, b\n        (50, 10, -3.0, 3.0, 2.0, 1.0),\n        (6, 5, -1.0, 1.0, -1.0, 0.0),\n        (50, 10, -1.0, 2.0, 100.0, -5.0),\n        (50, 10, -3.0, 3.0, 0.01, 100.0),\n        (50, 10, -2.0, 4.0, 1.0, 10.0),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_case(*case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format [B, Delta, rho] is achieved by Python's default str(list).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3102236"}, {"introduction": "在统计学习中，“最佳拟合”的含义取决于我们如何定义函数间的“距离”或“相似度”。这个练习为你搭建了一座连接理论与实践的桥梁，让你探索两种不同的“最佳”函数近似方法。一种是基于连续函数空间的 $L^2$ 投影，它代表了理论上最接近目标函数的近似；另一种是基于有限训练数据集的经验投影，这等价于我们熟知的最小二乘法。通过对比这两种方法在不同数据分布下的表现，你将理解理论模型与数据驱动模型之间的差异和联系，并认识到训练数据的分布如何影响最终的拟合结果。[@problem_id:3102308]", "problem": "给定一个在有限维子空间中使用基函数在两种不同内积下进行函数逼近的任务。设定如下。令输入域为闭区间 $[0,1]$。考虑候选基函数 $\\{v_k(x)\\}_{k=0}^{m-1}$，其中 $v_k(x) = x^k$ 且 $m = 4$，因此模型子空间是 $\\{1, x, x^2, x^3\\}$ 的张成空间。在 $[0,1]$ 上的平方可积函数空间上定义两种内积：\n- 连续 $L^2$ 内积：对于函数 $g$ 和 $h$，$\\langle g, h \\rangle_{L^2} = \\int_{0}^{1} g(x) h(x) \\, dx$。\n- 与有限训练集 $\\{x_i\\}_{i=1}^n$ 相关联的经验内积：$\\langle g, h \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} g(x_i) h(x_i)$。\n\n目标函数为 $f(x) = \\sin(6 \\pi x) + x$，其中 $\\sin$ 的角度以弧度为单位。\n\n您的任务是：\n1. 仅使用内积空间的核心定义和 Gram–Schmidt 正交化过程，为模型子空间构建关于每种内积的一组标准正交基：\n   - 一个关于 $\\langle \\cdot, \\cdot \\rangle_{L^2}$ 的 $L^2$-标准正交基 $\\{\\psi_k\\}_{k=0}^{m-1}$。\n   - 一个在给定特定训练集 $\\{x_i\\}_{i=1}^n$ 下关于 $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ 的经验-标准正交基 $\\{\\phi_k\\}_{k=0}^{m-1}$。\n   每组标准正交基都必须使用相应的内积，通过 Gram–Schmidt 过程从候选集 $\\{v_k\\}_{k=0}^{m-1}$ 获得。\n\n2. 构建 $f$ 在模型子空间上的两个投影：\n   - $L^2$ 投影 $P_{L^2} f = \\sum_{k=0}^{m-1} b_k \\, \\psi_k$，其系数为 $b_k = \\langle f, \\psi_k \\rangle_{L^2}$。\n   - 经验投影 $P_{\\text{emp}} f = \\sum_{k=0}^{m-1} a_k \\, \\phi_k$，其系数为 $a_k = \\langle f, \\phi_k \\rangle_{\\text{emp}}$。\n   请注意，$P_{L^2} f$ 仅依赖于连续内积，而 $P_{\\text{emp}} f$ 通过 $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ 依赖于训练集。\n\n3. 对于下面测试套件中指定的每个训练集，在一个包含 $[0,1]$ 中 201 个均匀间隔点的测试网格上评估这两种逼近，即 $T = \\{t_j\\}_{j=0}^{200}$，其中 $t_j = \\frac{j}{200}$。计算这两种预测在 $T$ 上的最大绝对逐点差：\n   $$\\Delta = \\max_{t \\in T} \\left| P_{\\text{emp}} f(t) - P_{L^2} f(t) \\right|.$$\n   正弦函数使用的角度单位必须是弧度。不涉及任何物理单位。\n\n测试套件（每一项定义一个训练集 $\\{x_i\\}_{i=1}^n$）：\n- 情况 1（均匀中点）：$n = 101$ 且 $x_i = \\frac{i + 0.5}{101}$，对于整数 $i = 0, 1, \\dots, 100$。\n- 情况 2（在 $0$ 附近二次聚集）：$n = 101$ 且 $x_i = \\left(\\frac{i + 0.5}{101}\\right)^2$，对于整数 $i = 0, 1, \\dots, 100$。\n- 情况 3（小型固定集）：$n = 4$ 且 $x = [0.05, 0.2, 0.5, 0.95]$。\n\n您的程序必须：\n- 仅使用上述定义，实现关于每种内积的 Gram–Schmidt 正交化。\n- 按所述方法构建 $P_{L^2} f$ 和 $P_{\\text{emp}} f$。\n- 对于每种情况，在指定的测试网格上计算 $\\Delta$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个情况的结果，格式为逗号分隔的列表，并用方括号括起来，例如，“[r1,r2,r3]”。\n- 每个 $r_k$ 都必须是精确到 $6$ 位小数的浮点数。", "solution": "该问题要求在一个有限维多项式子空间内，对目标函数 $f(x)$ 的两种不同逼近进行比较分析。问题的核心在于理解内积的选择如何影响“最佳”逼近，而内积定义了函数空间的几何结构。我们给定一个模型子空间 $V = \\text{span}\\{1, x, x^2, x^3\\}$，这是次数至多为 3 的多项式空间。这些逼近是 $f(x)$ 在 $V$ 上关于两种不同内积的正交投影：连续 $L^2$ 内积和从有限数据集导出的离散经验内积。\n\n构建这些投影的基本工具是 Gram-Schmidt 正交化过程。给定一个带有内积 $\\langle \\cdot, \\cdot \\rangle$ 的向量空间和一组线性无关的向量 $\\{v_k\\}_{k=0}^{m-1}$，此过程会为 $\\{v_k\\}$ 的张成空间生成一个标准正交基 $\\{u_k\\}_{k=0}^{m-1}$。该过程是迭代的：\n令 $w_0 = v_0$ 且 $u_0 = \\frac{w_0}{\\|w_0\\|}$。\n对于 $k = 1, 2, \\dots, m-1$，我们计算\n$$w_k = v_k - \\sum_{j=0}^{k-1} \\langle v_k, u_j \\rangle u_j$$\n$$u_k = \\frac{w_k}{\\|w_k\\|}$$\n其中 $\\|g\\| = \\sqrt{\\langle g, g \\rangle}$ 是由内积导出的范数。向量 $\\sum_{j=0}^{k-1} \\langle v_k, u_j \\rangle u_j$ 是 $v_k$ 在由 $\\{u_0, \\dots, u_{k-1}\\}$ 张成的子空间上的正交投影，因此 $w_k$ 是 $v_k$ 正交于此子空间的分量。\n\n一旦找到了子空间 $V$ 的一组标准正交基 $\\{u_k\\}_{k=0}^{m-1}$，任何函数 $f$ 在 $V$ 上的正交投影由下式给出：\n$$P_V f = \\sum_{k=0}^{m-1} \\langle f, u_k \\rangle u_k$$\n这个投影是 $V$ 中唯一的元素，它最小化了对于 $p \\in V$ 的距离 $\\|f - p\\|$。\n\n现在我们将此框架应用于指定的两种内积。候选基是 $\\{v_k(x) = x^k\\}_{k=0}^{3}$。\n\n**1. $L^2$ 投影 ($P_{L^2} f$)**\n\n在区间 $[0,1]$ 上的连续 $L^2$ 内积定义为：\n$$\\langle g, h \\rangle_{L^2} = \\int_{0}^{1} g(x) h(x) \\, dx$$\n我们从 $\\{v_k(x) = x^k\\}_{k=0}^{3}$ 构建一个 $L^2$-标准正交基 $\\{\\psi_k(x)\\}_{k=0}^{3}$。\n\n对于 $k=0$：\n$w_0(x) = v_0(x) = 1$。\n$\\|w_0\\|^2_{L^2} = \\int_0^1 1^2 \\,dx = 1$。\n$\\psi_0(x) = \\frac{w_0(x)}{\\|w_0\\|_{L^2}} = 1$。\n\n对于 $k=1$：\n$v_1$ 在 $\\{\\psi_0\\}$ 张成空间上的投影是 $\\langle v_1, \\psi_0 \\rangle_{L^2} \\psi_0(x)$。\n$\\langle v_1, \\psi_0 \\rangle_{L^2} = \\int_0^1 x \\cdot 1 \\,dx = \\frac{1}{2}$。\n$w_1(x) = v_1(x) - \\frac{1}{2} \\psi_0(x) = x - \\frac{1}{2}$。\n$\\|w_1\\|^2_{L^2} = \\int_0^1 (x - \\frac{1}{2})^2 \\,dx = \\frac{1}{12}$。\n$\\psi_1(x) = \\frac{w_1(x)}{\\|w_1\\|_{L^2}} = \\sqrt{12}(x - \\frac{1}{2})$。\n\n对 $k=2$ 和 $k=3$ 继续此过程，以获得 $\\psi_2(x)$ 和 $\\psi_3(x)$。所得的多项式是在 $[0,1]$ 上经过缩放的移位 Legendre 多项式。\n\n目标函数是 $f(x) = \\sin(6 \\pi x) + x$。$f$ 在模型子空间上的 $L^2$ 投影是：\n$$P_{L^2} f(x) = \\sum_{k=0}^{3} b_k \\psi_k(x), \\quad \\text{其中 } b_k = \\langle f, \\psi_k \\rangle_{L^2} = \\int_0^1 f(x) \\psi_k(x) \\,dx$$\n这些系数 $b_k$ 是常数，因为它们仅依赖于固定的函数 $f$ 和 $L^2$ 内积，而不依赖于任何训练数据。这些积分将通过数值方法计算。\n\n**2. 经验投影 ($P_{\\text{emp}} f$)**\n\n经验内积是根据训练集 $\\{x_i\\}_{i=1}^n$ 定义的：\n$$\\langle g, h \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} g(x_i) h(x_i)$$\n与 $L^2$ 的情况不同，这种内积以及由此产生的标准正交基和投影，都依赖于点集 $\\{x_i\\}$ 的具体选择。对于每个测试用例，我们都给定了一组不同的点。\n\n对于一个特定的训练集，我们使用相同的 Gram-Schmidt 过程从 $\\{v_k(x) = x^k\\}_{k=0}^{3}$ 构建一个经验-标准正交基 $\\{\\phi_k(x)\\}_{k=0}^{3}$，但使用 $\\langle \\cdot, \\cdot \\rangle_{\\text{emp}}$ 代替 $\\langle \\cdot, \\cdot \\rangle_{L^2}$。\n\n$f$ 的投影则为：\n$$P_{\\text{emp}} f(x) = \\sum_{k=0}^{3} a_k \\phi_k(x), \\quad \\text{其中 } a_k = \\langle f, \\phi_k \\rangle_{\\text{emp}} = \\frac{1}{n} \\sum_{i=1}^{n} f(x_i) \\phi_k(x_i)$$\n多项式 $P_{\\text{emp}} f(x)$ 是将一个 3 次多项式拟合到数据点 $(x_i, f(x_i))$ 的普通最小二乘解。在情况 3 中，点数 $n=4$ 等于基函数的数量 $m=4$，此时该投影成为在给定的四个点上对 $f(x)$ 进行插值的唯一次数至多为 3 的多项式。\n\n**3. 投影的比较**\n\n问题要求在精细的测试网格 $T = \\{t_j = \\frac{j}{200}\\}_{j=0}^{200}$ 上计算两种投影之间的最大绝对逐点差：\n$$\\Delta = \\max_{t \\in T} \\left| P_{\\text{emp}} f(t) - P_{L^2} f(t) \\right|$$\n该度量标准量化了数据驱动的逼近 ($P_{\\text{emp}} f$) 与“真实”函数空间逼近 ($P_{L^2} f$) 之间的偏差程度。当点 $\\{x_i\\}$ 的经验分布与 $[0,1]$ 上的均匀分布非常相似时（如情况 1），偏差预计会很小；而当分布倾斜（情况 2）或稀疏（情况 3）时，偏差会较大。以下代码为每个指定的训练集实现了这整个过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the maximum difference between L2 and empirical projections\n    for three different training sets.\n    \"\"\"\n\n    # Define the target function and basis functions\n    m = 4\n    target_func = lambda x: np.sin(6 * np.pi * x) + x\n    basis_v = [np.poly1d([1] + [0] * k) for k in range(m - 1, -1, -1)]\n\n    # Define the test grid\n    test_grid = np.linspace(0, 1, 201)\n\n    # =========================================================================\n    # Part 1: L2 Projection (independent of test cases)\n    # =========================================================================\n\n    def l2_inner_product(p1, p2):\n        integrand = p1 * p2\n        return quad(integrand, 0, 1)[0]\n\n    def gram_schmidt(basis_functions, inner_prod_func):\n        \"\"\"\n        Applies classical Gram-Schmidt to a list of polynomial functions.\n        \"\"\"\n        orthonormal_basis = []\n        for v in basis_functions:\n            w = v\n            for u in orthonormal_basis:\n                proj_coeff = inner_prod_func(v, u)\n                w = w - proj_coeff * u\n            \n            norm_w_sq = inner_prod_func(w, w)\n            # Add a small epsilon for numerical stability, though problem setup\n            # guarantees linear independence.\n            if norm_w_sq  1e-20:\n                # This should not be reached with the given problem sets.\n                # Handle gracefully by returning a zero polynomial if needed.\n                u_new = np.poly1d([0])\n            else:\n                u_new = w / np.sqrt(norm_w_sq)\n            \n            orthonormal_basis.append(u_new)\n        return orthonormal_basis\n\n    # Construct the L2-orthonormal basis {psi_k}\n    psi_basis = gram_schmidt(basis_v, l2_inner_product)\n\n    # Compute the coefficients b_k for the L2 projection\n    b_coeffs = []\n    for psi_k in psi_basis:\n        integrand = lambda x: target_func(x) * psi_k(x)\n        b_k = quad(integrand, 0, 1)[0]\n        b_coeffs.append(b_k)\n\n    # Construct the L2 projection polynomial P_L2 f\n    p_l2_f = np.poly1d([0])\n    for b_k, psi_k in zip(b_coeffs, psi_basis):\n        p_l2_f += b_k * psi_k\n\n    # Evaluate the L2 projection on the test grid\n    p_l2_f_values = p_l2_f(test_grid)\n\n    # =========================================================================\n    # Part 2: Empirical Projections (for each test case)\n    # =========================================================================\n\n    # Define test cases\n    test_cases = [\n        # Case 1: n = 101, uniform midpoints\n        (lambda: (101, (np.arange(101) + 0.5) / 101)),\n        # Case 2: n = 101, quadratic cluster near 0\n        (lambda: (101, ((np.arange(101) + 0.5) / 101)**2)),\n        # Case 3: n = 4, small, fixed set\n        (lambda: (4, np.array([0.05, 0.2, 0.5, 0.95])))\n    ]\n\n    results = []\n    for case_generator in test_cases:\n        n, x_nodes = case_generator()\n\n        # Define the empirical inner product for the current training set\n        def empirical_inner_product(p1, p2):\n            return np.mean(p1(x_nodes) * p2(x_nodes))\n\n        # Construct the empirical-orthonormal basis {phi_k}\n        phi_basis = gram_schmidt(basis_v, empirical_inner_product)\n\n        # Compute the coefficients a_k for the empirical projection\n        f_at_nodes = target_func(x_nodes)\n        a_coeffs = []\n        for phi_k in phi_basis:\n            a_k = np.mean(f_at_nodes * phi_k(x_nodes))\n            a_coeffs.append(a_k)\n\n        # Construct the empirical projection polynomial P_emp f\n        p_emp_f = np.poly1d([0])\n        for a_k, phi_k in zip(a_coeffs, phi_basis):\n            p_emp_f += a_k * phi_k\n\n        # Evaluate the empirical projection on the test grid\n        p_emp_f_values = p_emp_f(test_grid)\n\n        # Compute the maximum absolute pointwise difference Delta\n        delta = np.max(np.abs(p_emp_f_values - p_l2_f_values))\n        results.append(delta)\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3102308"}]}