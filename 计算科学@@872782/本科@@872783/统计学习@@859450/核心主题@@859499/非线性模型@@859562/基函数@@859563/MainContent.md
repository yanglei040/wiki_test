## 引言
在[统计学习](@entry_id:269475)领域，线性回归因其简单、易于解释而成为基础。然而，现实世界的数据关系往往远超线性范畴，这暴露了线性模型的局限性。如何构建既能捕捉复杂[非线性](@entry_id:637147)模式，又保留[线性模型](@entry_id:178302)部分优点的模型？[基函数](@entry_id:170178)（Basis Functions）为这一挑战提供了强大而优雅的解决方案。

本文旨在全面解析[基函数](@entry_id:170178)的理论与实践。我们将从基本原理出发，逐步深入其在多个学科领域的应用，最终通过动手实践巩固所学知识。读者将学习到如何利用[基函数](@entry_id:170178)将复杂的[非线性](@entry_id:637147)问题转化为熟悉的线性框架，并掌握选择和使用不同基系统的关键考量。

在接下来的内容中，我们将其分为三个核心章节。第一章“原理与机制”将奠定理论基础，详细阐述[基函数](@entry_id:170178)展开的框架、不同基（如多项式、[样条](@entry_id:143749)）的选择对[数值稳定性](@entry_id:146550)和[模型可识别性](@entry_id:186414)的影响，以及正则化在控制[模型复杂度](@entry_id:145563)中的关键作用。第二章“应用与跨学科联系”将视野扩展到实际应用，展示[基函数](@entry_id:170178)如何在工程物理、信号处理和现代机器学习中解决具体问题，揭示其作为连接理论与实践的桥梁价值。最后，在“动手实践”部分，你将通过具体的编程练习，亲手实现和对比不同的[基函数](@entry_id:170178)方法，将理论知识转化为实践技能。

## 原理与机制

在之前的章节中，我们探讨了[线性回归](@entry_id:142318)模型，其核心假设是响应变量与预测变量之间存在[线性关系](@entry_id:267880)。然而，在许多实际应用中，这种假设过于严格，无法捕捉现实世界中普遍存在的复杂[非线性](@entry_id:637147)模式。为了突破线性模型的局限性，同时保留其易于解释和计算的优点，我们引入一种强大的技术：**[基函数](@entry_id:170178)（basis functions）** 展开。

本章的核心思想是通过一组预先选择的[非线性](@entry_id:637147)函数来变换输入变量，然后对这些变换后的变量应用线性模型。这种方法将[非线性回归](@entry_id:178880)问题巧妙地转化为一个更高维度的线性回归问题，使我们能够利用成熟的线性模型理论和算法来拟合复杂的非线性关系。我们将深入探讨[基函数](@entry_id:170178)的选择、模型的[数值稳定性](@entry_id:146550)和可识别性，以及正则化在控制[模型复杂度](@entry_id:145563)中的作用。

### [基函数](@entry_id:170178)展开框架：在新空间中实现线性

[基函数](@entry_id:170178)展开方法的核心是将一个函数 $f(x)$ 表示为一组已知函数（即[基函数](@entry_id:170178)）$\{\phi_j(x)\}_{j=1}^m$ 的线性组合。对于一个单变量输入 $x$，模型可以写成：

$$
f(x) = \sum_{j=1}^{m} \beta_j \phi_j(x)
$$

这里的 $\phi_j(\cdot)$ 是第 $j$ 个[基函数](@entry_id:170178)，而 $\beta_j$ 是对应的系数。通过选择不同的[基函数](@entry_id:170178)，我们可以生成各种灵活的[非线性](@entry_id:637147)函数。例如，一个简单的选择是多项式[基函数](@entry_id:170178)，$\phi_j(x) = x^j$，这将模型转化为一个[多项式回归](@entry_id:176102)：

$$
f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_m x^m
$$

给定一组训练数据 $\{(x_i, y_i)\}_{i=1}^n$，我们的目标是估计系数向量 $\boldsymbol{\beta} = (\beta_1, \dots, \beta_m)^\top$。为此，我们首先构建一个**[设计矩阵](@entry_id:165826)（design matrix）** $X$，其第 $i$ 行第 $j$ 列的元素为 $X_{ij} = \phi_j(x_i)$。这样，模型在所有训练点上的预测值可以紧凑地写为 $X\boldsymbol{\beta}$。估计系数的最常用方法是最小化[残差平方和](@entry_id:174395)（Sum of Squared Residuals, SSR）：

$$
\hat{\boldsymbol{\beta}} = \arg\min_{\boldsymbol{\beta}} \| \mathbf{y} - X\boldsymbol{\beta} \|_2^2
$$

这是一个标准的普通最小二乘（Ordinary Least Squares, OLS）问题。只要[设计矩阵](@entry_id:165826) $X$ 的列是[线性无关](@entry_id:148207)的，解就是唯一的，由正规方程给出：$\hat{\boldsymbol{\beta}} = (X^\top X)^{-1} X^\top \mathbf{y}$。

通过这种方式，我们巧妙地将一个关于[原始变量](@entry_id:753733) $x$ 的[非线性拟合](@entry_id:136388)问题，转化为了一个关于系数 $\boldsymbol{\beta}$ 的线性拟合问题。模型的[非线性](@entry_id:637147)能力完全蕴含在[基函数](@entry_id:170178)的选择中。

### 基的选择：表示、稳定性与约束

[基函数](@entry_id:170178)的选择对模型的性能至关重要，它不仅决定了模型能表示的函数类型，还深刻影响到模型的[数值稳定性](@entry_id:146550)和[可解释性](@entry_id:637759)。

#### 函数空间与基表示

对于一个给定的函数类别，例如[分段线性函数](@entry_id:273766)，可能有多种不同的[基函数](@entry_id:170178)集都能表示该类别中的任何函数。换句话说，不同的基可以张成相同的**函数空间（function space）**。

一个重要的原则是，只要两组基张成相同的函数空间，那么对于任意给定的数据集，使用[普通最小二乘法](@entry_id:137121)得到的**拟合值（fitted values）** $\hat{\mathbf{y}}$ 是完全相同的。这是因为拟合值是响应向量 $\mathbf{y}$ 在[设计矩阵](@entry_id:165826)[列空间](@entry_id:156444)上的[正交投影](@entry_id:144168)，而列空间仅依赖于所张成的[函数空间](@entry_id:143478)，与具体的基表示无关。然而，不同基下的系数 $\hat{\boldsymbol{\beta}}$ 会有显著差异，其解释也截然不同。

例如，考虑一个在 $x=2$ 处有一个结点（knot）的连续[分段线性函数](@entry_id:273766)空间。我们可以用**截断幂基（truncated power basis）** $\{1, x, (x-2)_+\}$ 来表示它，其中 $(t)_+ = \max\{t, 0\}$。我们也可以用一个等价的**双折页基（two-hinge basis）** $\{1, (2-x)_+, (x-2)_+\}$ 来表示。尽管这两个基的形式不同，但它们张成了完全相同的函数空间。因此，对同一组数据进行OLS拟合，得到的预测函数 $\hat{f}(x)$ 将是相同的，但两组基对应的系数向量将通过一个[可逆线性变换](@entry_id:149915)相关联。[@problem_id:3102272]

#### 数值稳定性与多重共線性

尽管不同的基可能在理论上等价，但它们的数值属性可能天差地别。一个理想的基应该使其[设计矩阵](@entry_id:165826) $X$ 的列向量尽可能地“不相关”。如果某些列向量之间存在近似[线性关系](@entry_id:267880)，即**多重共線性（multicollinearity）**，那么[格拉姆矩阵](@entry_id:203297)（Gram matrix）$X^\top X$ 将会是病态的（ill-conditioned），其[逆矩阵](@entry_id:140380)的元素会非常大。这直接导致了OLS[系数估计](@entry_id:175952)量 $\hat{\boldsymbol{\beta}}$ 的[方差](@entry_id:200758)急剧膨胀，使得系数的估计变得极不稳定，其[置信区间](@entry_id:142297)也会变得非常宽。

截断幂基是一个典型的例子。对于样条函数，当结点增多或阶数提高时，截断幂[基函数](@entry_id:170178)（如 $x, (x-\xi_1)_+, (x-\xi_2)_+, \dots$）之间会变得高度相关。例如，函数 $x$ 和 $(x-0.01)_+$ 在大部分定义域上几乎是平行的。

相比之下，**B-[样条](@entry_id:143749)基（B-spline basis）** 是一个在数值上表现优越得多的选择。B-样条[基函数](@entry_id:170178)具有**局部支撑（local support）**的特性，即每个[基函数](@entry_id:170178)只在一小段区间上非零。这使得[设计矩阵](@entry_id:165826) $X$ 的大部分列是稀疏且近似正交的。因此，$X^\top X$ 变成一个[带状矩阵](@entry_id:746657)，具有良好的条件数。使用B-[样条](@entry_id:143749)基可以显著降低多重共線性，得到更稳定的[系数估计](@entry_id:175952)和更可靠的置信区间。[@problem_id:3102239] 我们可以通过[计算设计](@entry_id:167955)矩阵的**条件数（condition number）** $\kappa(X) = \sigma_{\max}(X) / \sigma_{\min}(X)$ 来量化多重共線性，其中 $\sigma_{\max}$ 和 $\sigma_{\min}$ 分别是最大和最小奇异值。条件数越接近1，[数值稳定性](@entry_id:146550)越好。对于相同的[函数空间](@entry_id:143478)，[B样条](@entry_id:172303)基的条件数通常远小于截断幂基。

#### 正交性的作用

最理想的情况是[设计矩阵](@entry_id:165826)的列是**正交的（orthogonal）**，即对于任意 $j \neq k$，列向量的[点积](@entry_id:149019)为零。如果我们将它们归一化，就得到一个**标准正交（orthonormal）**设计，此时 $X^\top X = I$。在这种情况下：
1.  [正规方程](@entry_id:142238)的解变得非常简单：$\hat{\beta}_j = X_j^\top \mathbf{y}$，即每个系数都可以独立计算。
2.  系数的估计是解耦的，一个系数的估计值不受其他[基函数](@entry_id:170178)的影响。
3.  [系数估计](@entry_id:175952)的[方差](@entry_id:200758)也变得简单：$\mathrm{Var}(\hat{\beta}_j) = \sigma^2$。

我们可以通过**格拉姆-施密特（Gram-Schmidt）**过程将任意一组[线性无关](@entry_id:148207)的[基向量](@entry_id:199546)转化为一组标准正交基。[@problem_id:3102316] 此外，一个重要的结论是，在岭回归（Ridge Regression）这类对系数范数进行惩罚的模型中，对基进行[正交变换](@entry_id:155650)会改变系数向量，但不会改变最终的预测函数 $\hat{f}(x)$。[@problem_id:3102316]

### 常用基系统及其属性

#### 多项式基

多项式[基函数](@entry_id:170178) $\{1, x, x^2, \dots, x^m\}$ 是最简单和最直观的选择。然而，它们具有**全局支撑**，这意味着改变任何一个系数 $\beta_j$ 都会影响整个函数 $f(x)$ 的形状。这使得多项式函数在区间边界附近表现不佳，并且外插（extrapolation）能力很差，高阶多项式尤其容易在数据范围之外产生剧烈[振荡](@entry_id:267781)（[龙格现象](@entry_id:142935)）。

#### [样条](@entry_id:143749)基

**样条（splines）**是[分段多项式](@entry_id:634113)，它们在称为“结点”的连接点处满足一定的光滑性条件。例如，[三次样条](@entry_id:140033)是分段三次多项式，并在结点处连续、[一阶导数](@entry_id:749425)连续、[二阶导数](@entry_id:144508)连续。这种局部化的特性使得[样条](@entry_id:143749)非常灵活，能够适应数据的局部变化，并且通常比全局多项式具有更好的拟合效果和数值稳定性。如前所述，B-样条基是实现样条回归的首选。

#### [傅里叶基](@entry_id:201167)

[傅里叶基](@entry_id:201167)由一组正弦和余弦函数构成，如 $\{\cos(2\pi kx), \sin(2\pi kx)\}_{k=1}^m$。这组基天然适用于拟合**周期性（periodic）**的信号。当数据采样点在定义域上[均匀分布](@entry_id:194597)时，离散[傅里叶基](@entry_id:201167)函数构成的[设计矩阵](@entry_id:165826)列是正交的，这带来了巨大的计算和统计优势。

然而，当底层的真实函数 $f(x)$ 是[非周期性](@entry_id:275873)的（例如 $f(x)=x$ 在 $[0,1]$ 上，$f(0) \neq f(1)$），强行使用周期性的[傅里叶基](@entry_id:201167)会导致模型失配。模型会试图用周期函数来逼近一个在边界处有“跳跃”的函数，这会导致在[边界点](@entry_id:176493) $x=0$ 和 $x=1$ 附近产生明显的[振荡](@entry_id:267781)伪影，即**[吉布斯现象](@entry_id:138701)（Gibbs phenomenon）**。在这种情况下，**自然样条（natural splines）**是更好的选择，因为它通过在边界处施加[线性约束](@entry_id:636966)（例如，[二阶导数](@entry_id:144508)为零）来避免强制周期性，从而得到更平滑、更准确的边界拟合。[@problem_id:3102240]

#### 数据采样方式的影响

[基函数](@entry_id:170178)的性质，尤其是正交性，可能与数据的采样方式密切相关。[傅里叶基](@entry_id:201167)的离散正交性严格依赖于**等距采样（equispaced sampling）**。如果采样点 $x_i$ 是**不规则（irregular）**[分布](@entry_id:182848)的，[傅里叶基](@entry_id:201167)列向量之间的正交性就会丧失，导致[设计矩阵](@entry_id:165826)出现多重共線性，进而使[系数估计](@entry_id:175952)的[方差](@entry_id:200758)增大。[@problem_id:3102265]

相比之下，B-[样条](@entry_id:143749)等具有局部支撑的[基函数](@entry_id:170178)对采样方式的鲁棒性要强得多。只要在每个[基函数](@entry_id:170178)的支撑区间内有足够的数据点，[设计矩阵](@entry_id:165826)通常仍能保持良好的[数值条件](@entry_id:136760)。因此，对于不规则采样的数据，B-[样条](@entry_id:143749)是比[傅里叶基](@entry_id:201167)更自然、更稳健的选择。[@problem_id:3102265]

### 模型的可识别性问题

**可识别性（Identifiability）**是指模型参数是否能被数据唯一确定。在[线性模型](@entry_id:178302) $y = X\boldsymbol{\beta} + \varepsilon$ 的框架下，参数 $\boldsymbol{\beta}$ 可被唯一识别的充分必要条件是[设计矩阵](@entry_id:165826) $X$ 是列满秩的，即其所有列向量线性无关。若 $X$ 的列线性相关，则存在无穷多组 $\boldsymbol{\beta}$ 能产生相同的拟合值 $X\boldsymbol{\beta}$。

#### 截距项与[基函数](@entry_id:170178)冗余

在使用[基函数](@entry_id:170178)时，一个常见的可识别性问题与**截距项（intercept）**有关。如果我们的模型包含一个显式的截距项（即[设计矩阵](@entry_id:165826)中有一列全为1的向量，记为 $\mathbf{1}$），并且我们使用的[基函数](@entry_id:170178) $\{ \phi_j(x) \}_{j=1}^m$ 自身或其[线性组合](@entry_id:154743)可以生成一个[常数函数](@entry_id:152060)（即 $\sum_j c_j \phi_j(x) = C$ 对所有 $x$ 成立），那么[设计矩阵](@entry_id:165826)的列就会线性相关。最简单的情况是，[基函数](@entry_id:170178)中本身就包含[常数函数](@entry_id:152060) $\phi_1(x) = 1$。此时，[设计矩阵](@entry_id:165826)中将出现两列完全相同的向量 $\mathbf{1}$，导致完美的多重共線性，参数无法唯一识别。[@problem_id:3102294]

解决此问题的方法很简单：要么从[基函数](@entry_id:170178)集中移除常数项，要么在模型中不设置显式截距项。

一个相关的操作是**中心化（centering）**。如果我们对所有非恒定的[基函数](@entry_id:170178)列向量和响应向量 $y$ 都进行中心化（即减去各自的均值），那么在新模型中，OLS拟合出的截距项恰好为零。如果只对[基函数](@entry_id:170178)列向量进行中心化，那么拟合出的截距项将等于响应向量的样本均值 $\bar{y}$。重要的是，对 predictor 列进行中心化处理，虽然改变了系数的估计值和解释，但并不会改变模型的[列空间](@entry_id:156444)，因此最终的拟合值 $\hat{y}$ 保持不变。[@problem_id:3102294]

#### 可加模型中的共線性

可识别性问题在**可加模型（additive models）**中表现得更为微妙。一个可加模型假设函数可以分解为各个输入变量的函数之和，例如：

$$
f(x_1, x_2) = \alpha + f_1(x_1) + f_2(x_2)
$$

其中 $f_1$ 和 $f_2$ 都用各自的[基函数](@entry_id:170178)展开。这里立刻出现一个模糊性：我们可以从 $f_1$ 中减去一个常数 $c$，同时给 $f_2$ 加上同一个常数 $c$，而它们的和 $f_1+f_2$ 保持不变。为了解决这个问题，通常会施加约束，比如要求每个函数分量的样本均值为零，即 $\sum_i f_j(x_{ij}) = 0$。这个约束可以唯一地确定截距项 $\alpha = \bar{y}$。[@problem_id:3102267]

然而，即使施加了中心化约束，如果输入变量 $x_1$ 和 $x_2$ 之间存在近似函数关系（例如，$x_2 \approx g(x_1)$），并且 $f_1$ 和 $f_2$ 的基[函数空间](@entry_id:143478)存在重叠，那么可识别性问题依然存在。此时，可能存在一个非零的中心化函数 $h(x_1)$，它近似等于另一个中心化函数 $-k(x_2)$。这意味着我们可以将拟合的函数分量分解为 $(\hat{f}_1+h, \hat{f}_2-k)$，而它们的和几乎不变。在这种情况下，数据本身无法唯一地区分 $f_1$ 和 $f_2$ 的贡献，只能稳定地估计它们的和 $f_1+f_2$。[@problem_id:3102267] 要得到唯一的分解，必须引入额外的结构，例如通过正则化（penalization）或正交化（orthogonalization）来强制选择一个“首选”解。

### 正则化与模型选择

当使用大量[基函数](@entry_id:170178)时，模型会变得非常灵活，容易过拟合训练数据中的噪声。**正则化（Regularization）**是控制[模型复杂度](@entry_id:145563)、[防止过拟合](@entry_id:635166)的关键技术。它通过在最小二乘[目标函数](@entry_id:267263)中加入一个惩罚项来实现。

$$
\min_{\boldsymbol{\beta}} \| \mathbf{y} - X\boldsymbol{\beta} \|_2^2 + \lambda P(\boldsymbol{\beta})
$$

惩罚项 $P(\boldsymbol{\beta})$ 的形式决定了正则化的行为。

#### [稀疏性](@entry_id:136793)与光滑性

两种最常见的正则化策略是基于**稀疏性（sparsity）**和**光滑性（smoothness）**的。

*   **$\ell_1$ 正则化 (Lasso)**：当惩罚项是系数的$\ell_1$范数 $P(\boldsymbol{\beta}) = \|\boldsymbol{\beta}\|_1 = \sum_j |\beta_j|$ 时，即为[Lasso回归](@entry_id:141759)。$\ell_1$ 惩罚的一个显著特性是它能够将许多系数精确地压缩到零，从而实现**变量选择（variable selection）**。当应用于一个大规[模的基](@entry_id:156416)函数集（如高阶多项式）时，Lasso可以自动筛选出“重要”的[基函数](@entry_id:170178)，生成一个稀疏、易于解释的模型。[@problem_id:3102280]

*   **$\ell_2$ 型正则化 (Ridge / [平滑样条](@entry_id:637498))**：当惩罚项是系数的$\ell_2$范数的平方 $P(\boldsymbol{\beta}) = \|\boldsymbol{\beta}\|_2^2 = \sum_j \beta_j^2$ 时，即为岭回归。它会收缩所有系数，但通常不会将它们变为零。在[基函数](@entry_id:170178)设置中，更常见的是惩罚函数的[光滑性](@entry_id:634843)。例如，在**[平滑样条](@entry_id:637498)（smoothing splines）**中，惩罚项是函数[二阶导数](@entry_id:144508)的积分平方，$\int (f''(x))^2 dx$。这个惩罚项惩罚函数的“弯曲度”或“粗糙度”，倾向于产生**光滑（smooth）**的拟合曲线。这种惩罚可以转化为对B-[样条](@entry_id:143749)基系数的一种二次惩罚，因此属于$\ell_2$类型。它不会产生稀疏的系数向量，而是通过集体收缩来控制光滑度。[@problem_id:3102280]

这两种策略代表了模型构建哲学的不同权衡：Lasso追求简约和[可解释性](@entry_id:637759)，而[平滑样条](@entry_id:637498)追求拟合函数的光滑性和稳定性。[@problem_id:3102280] [@problem_id:3102265]

#### 模型的[有效自由度](@entry_id:161063)

在经典线性模型中，模型的复杂度由参数个数（即自由度）衡量。但在正则化模型中，由于系数被收缩，参数的“有效”数量减少了。一个通用的**[有效自由度](@entry_id:161063)（effective degrees of freedom）**的定义是：

$$
\mathrm{df}(\lambda) = \sum_{i=1}^n \frac{\partial \hat{y}_i}{\partial y_i}
$$

它衡量的是拟合值 $\hat{y}_i$ 对其自身观测值 $y_i$ 的平均敏感度。对于Lasso，在一个重要的特例——标准正交[设计矩阵](@entry_id:165826)下——可以证明[有效自由度](@entry_id:161063)恰好等于模型中**非零系数的个数**。[@problem_id:3102230] 这个简洁的结果为理解Lasso模型的复杂度提供了一个直观的途径。然而，当[基函数](@entry_id:170178)相关时（即 $X$ 非正交），这个简单的关系就不再成立，自由度的计算变得非常复杂，因为它涉及到[系数估计](@entry_id:175952)的复杂依赖结构。[@problem_id:3102230]

### 对偶视角：与[核方法](@entry_id:276706)的联系

我们到目前为止都是在“原始”空间中考虑问题，即直接优化 $m$ 个[基函数](@entry_id:170178)系数 $\beta_j$。这在[基函数](@entry_id:170178)数量 $m$ 远小于样本量 $n$ ($m \ll n$) 时是高效的。然而，在某些情况下，我们可能希望使用非常大甚至无穷多的[基函数](@entry_id:170178)，此时 $m \ge n$。

在这种情况下，一个**对偶（dual）**视角变得更有利。许多正则化[线性模型](@entry_id:178302)的解都可以表示为训练数据点的[线性组合](@entry_id:154743)。例如，对于[岭回归](@entry_id:140984)，拟合函数可以写成：

$$
\hat{f}(x) = \sum_{i=1}^n \alpha_i k(x, x_i)
$$

其中 $k(x, z) = \sum_{j=1}^m \phi_j(x) \phi_j(z)$ 是由[基函数](@entry_id:170178)定义的**[核函数](@entry_id:145324)（kernel function）**。我们不再求解 $m$ 维的 $\boldsymbol{\beta}$，而是求解 $n$ 维的 $\boldsymbol{\alpha}$。当 $n \ll m$ 时，求解对偶问题在计算上要高效得多。

这种[对偶表示](@entry_id:146263)是**[核方法](@entry_id:276706)（kernel methods）**和**[再生核希尔伯特空间](@entry_id:633928)（Reproducing Kernel Hilbert Space, RKHS）**理论的基石。它允许我们隐式地使用无限维的特征空间，只要我们能定义一个有效的核函数。这种思想极大地扩展了线性模型的能力，为支持向量机等高级算法铺平了道路。[@problem_id:3102305]

总而言之，[基函数](@entry_id:170178)方法是一座桥梁，它连接了简单[线性模型](@entry_id:178302)和复杂的[非线性](@entry_id:637147)世界。通过精心选择[基函数](@entry_id:170178)、理解其数值属性和可识别性，并结合[正则化技术](@entry_id:261393)，我们可以构建出既灵活又稳健的[统计模型](@entry_id:165873)来应对各种数据分析挑战。