{"hands_on_practices": [{"introduction": "评估模型对新数据的泛化能力是统计学习的基石。虽然留一法交叉验证（LOOCV）是一种严谨的方法，但简单地将模型重新拟合 $n$ 次在计算上是令人望而却步的。本练习 [@problem_id:3123636] 揭示了一种针对具有恒等连结函数的广义可加模型（GAM）的优雅而高效的计算捷径，它利用了线性平滑（或“帽子”）矩阵的性质。通过理解这种联系，你可以在没有巨大计算负担的情况下评估模型的性能和乐观度。", "problem": "在恒等链接的广义可加模型 (GAM) 框架下，您将执行一个回归任务，其中响应向量表示为 $y \\in \\mathbb{R}^n$，协变量向量表示为 $x \\in \\mathbb{R}^n$，拟合函数由固定的平滑基函数的线性组合表示。具体来说，考虑一个单变量可加模型 $f(x) = \\sum_{k=1}^{p} \\beta_k \\,\\phi_k(x)$，其设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 固定，定义为 $X_{ik} = \\phi_k(x_i)$，以及一个通过惩罚最小二乘法施加平滑性的半正定惩罚矩阵 $K \\in \\mathbb{R}^{p \\times p}$。系数 $\\beta \\in \\mathbb{R}^p$ 的惩罚最小二乘估计量是以下表达式的唯一最小化子：\n$$\n\\| y - X \\beta \\|_2^2 + \\lambda \\, \\beta^{\\top} K \\beta,\n$$\n其中 $\\lambda \\ge 0$ 是平滑参数。该估计量产生的拟合值 $\\hat{y} \\in \\mathbb{R}^n$ 可以写成数据 $y$ 经由矩阵 $S \\in \\mathbb{R}^{n \\times n}$ 的线性平滑，即 $\\hat{y} = S y$。\n\n您的任务是，仅利用平滑矩阵 $S$ 的性质，为恒等链接的 GAM 实现一种高效的留一交叉验证 (LOOCV) 计算，而无需显式地重新拟合模型 $n$ 次。此外，您还需要利用线性平滑器的结构和噪声方差的一致估计量，来估计平方误差损失下 $\\hat{y}$ 的乐观度。\n\n使用的基础知识：\n- 在惩罚最小二乘法下，恒等链接的广义可加模型 (GAM) 的定义。\n- 惩罚最小二乘估计量的线性代数性质，包括对于适当选择的 $\\lambda$ 和 $K$，其对称正定性。\n- 平方误差损失的留一交叉验证 (LOOCV) 的定义，以及对于线性平滑器，乐观度作为样本内误差和样本外预测误差的期望差异的概念。\n- 在零均值可加独立噪声下，线性平滑器的协方差恒等式。\n\n数据生成和基函数规定：\n- 令 $x_i$ 为区间 $[0,1]$ 上的 $n$ 个等距点，即 $x_i = \\frac{i-1}{n-1}$，其中 $i \\in \\{1,\\dots,n\\}$。\n- 根据 $y_i = \\sin(2\\pi x_i) + \\varepsilon_i$ 生成 $y_i$，其中角度以弧度为单位，$\\varepsilon_i$ 是从均值为 $0$、标准差为 $\\sigma$ 的正态分布中抽取的独立同分布样本。\n- 使用固定的随机种子以保证可复现性。\n- 使用以下基函数构建具有 $p = 2 + m$ 列的设计矩阵 $X$：\n  - $\\phi_1(x) = 1$ (截距项)，\n  - $\\phi_2(x) = x$ (线性项)，\n  - 对于 $j \\in \\{1,\\dots,m\\}$，在 $(0,1)$ 中有等距的内部节点 $c_j$，定义截断三次基函数 $\\phi_{2+j}(x) = \\max(x - c_j, 0)^3$。\n- 令惩罚矩阵 $K$ 为对角矩阵，其前两项为零（对应未惩罚的截距项和线性项），其余所有对角项为一（对应惩罚的截断三次基系数）。\n- 对于每个测试用例，通过求解惩罚正规方程组来获得拟合值 $\\hat{y}$，隐式定义与此惩罚估计量对应的平滑矩阵 $S$，然后：\n  1. 仅利用 $S$ 的性质，计算所有 $i \\in \\{1,\\dots,n\\}$ 的 LOOCV 预测值 $\\hat{y}^{(-i)}$，而无需重新拟合 $n$ 次。\n  2. 计算样本内均方误差 $M_{\\text{in}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$。\n  3. 计算 LOOCV 均方误差 $M_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i^{(-i)})^2$。\n  4. 用两种方式估计平方误差损失下 $\\hat{y}$ 的乐观度：\n     - 作为差值 $M_{\\text{LOO}} - M_{\\text{in}}$。\n     - 通过一个基于线性平滑器结构和噪声方差的一致估计量的解析估计，该估计仅使用 $S$ 的性质和残差。\n- 您的实现应尽可能避免显式构建完整的 $S$ 矩阵，并且必须利用高效的线性代数方法，从 $X$、$K$ 和 $\\lambda$ 计算所需的 $S$ 的对角线元素或迹。\n\n测试套件：\n对于下面的每个元组 $(n, \\sigma, \\text{seed}, m, \\lambda)$，生成数据并计算上述四个量。$\\sin(2\\pi x)$ 中的角度以弧度为单位。测试套件包括四个案例：\n- 案例 1：$(n, \\sigma, \\text{seed}, m, \\lambda) = (50, 0.2, 42, 8, 0.1)$。\n- 案例 2：$(n, \\sigma, \\text{seed}, m, \\lambda) = (50, 0.2, 42, 8, 0.0001)$。\n- 案例 3：$(n, \\sigma, \\text{seed}, m, \\lambda) = (50, 0.2, 42, 8, 10.0)$。\n- 案例 4：$(n, \\sigma, \\text{seed}, m, \\lambda) = (12, 0.15, 0, 4, 0.2)$。\n\n最终输出：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个条目对应一个测试用例，并且其本身是按 $[M_{\\text{in}}, M_{\\text{LOO}}, M_{\\text{LOO}} - M_{\\text{in}}, \\text{Optimism}_{\\text{analytic}}]$ 顺序排列的四个浮点数列表。例如，\n$[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$，\n其中 $a_k$、$b_k$、$c_k$ 和 $d_k$ 是案例 $k$ 的四个量。输出中的所有数值都必须是没有任何单位的实数（浮点数）。", "solution": "用户的要求是实现一个评估带有恒等链接的广义可加模型 (GAM) 的程序，特别关注在不重复拟合模型的情况下，高效计算留一交叉验证 (LOOCV) 误差和乐观度估计。\n\n### **问题验证**\n\n问题陈述已经过评估并被确定为**有效**。它在科学上基于线性平滑器和惩罚最小二乘回归的既定统计理论。问题设定良好，提供了计算唯一且有意义解所需的所有必要定义、数据生成过程和参数。语言客观，要求可形式化。\n\n### **理论框架**\n\n问题的核心在于由惩罚最小二乘估计量定义的线性平滑器的性质。\n\n**1. 惩罚最小二乘法与平滑矩阵 ($S$)**\n\n给定关于系数向量 $\\beta \\in \\mathbb{R}^p$ 的最小化目标函数：\n$$\nL(\\beta) = \\| y - X \\beta \\|_2^2 + \\lambda \\, \\beta^{\\top} K \\beta\n$$\n通过将梯度 $\\nabla_{\\beta} L(\\beta)$ 设置为零，可以得到正规方程组：\n$$\n(X^{\\top} X + \\lambda K) \\beta = X^{\\top} y\n$$\n系数的估计量为：\n$$\n\\hat{\\beta} = (X^{\\top} X + \\lambda K)^{-1} X^{\\top} y\n$$\n拟合值 $\\hat{y} \\in \\mathbb{R}^n$ 是响应向量 $y$ 的一个线性变换：\n$$\n\\hat{y} = X \\hat{\\beta} = X (X^{\\top} X + \\lambda K)^{-1} X^{\\top} y\n$$\n这定义了平滑矩阵（或帽子矩阵）$S \\in \\mathbb{R}^{n \\times n}$：\n$$\nS = X (X^{\\top} X + \\lambda K)^{-1} X^{\\top}\n$$\n由于惩罚矩阵 $K$ 是对称的，所以 $S$ 是一个对称矩阵。\n\n**2. 高效的留一交叉验证 (LOOCV)**\n\nLOOCV 需要从一个对除第 $i$ 个点之外的所有数据进行拟合的模型中，计算对每个观测值 $y_i$ 的预测。对于任何线性平滑器 $\\hat{y} = Sy$，一个著名的结果允许我们从对完整数据的一次拟合中直接计算 LOOCV 残差：\n$$\ny_i - \\hat{y}_i^{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - S_{ii}}\n$$\n其中 $\\hat{y}_i^{(-i)}$ 是在没有观测值 $i$ 的情况下拟合模型对 $y_i$ 的预测，而 $S_{ii}$ 是平滑矩阵 $S$ 的第 $i$ 个对角线元素。\n\n然后，LOOCV 均方误差 $M_{\\text{LOO}}$ 可以高效地计算如下：\n$$\nM_{\\text{LOO}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i^{(-i)})^2 = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\frac{y_i - \\hat{y}_i}{1 - S_{ii}} \\right)^2\n$$\n这个公式避免了 $n$ 次重新拟合模型。主要的计算任务是找到对角线元素 $S_{ii}$。显式地构建 $n \\times n$ 矩阵 $S$ 是低效的。相反，我们这样计算 $S_{ii}$：\n$$\nS_{ii} = (X (X^{\\top} X + \\lambda K)^{-1} X^{\\top})_{ii} = X_{i, \\cdot} (X^{\\top} X + \\lambda K)^{-1} X_{i, \\cdot}^{\\top}\n$$\n其中 $X_{i, \\cdot}$ 是 $X$ 的第 $i$ 行。可以通过首先计算 $p \\times p$ 的逆矩阵 $M_{inv} = (X^{\\top} X + \\lambda K)^{-1}$，然后执行矩阵向量乘法来为所有 $i$ 计算它。一个高效的实现是 `np.sum((X @ M_inv) * X, axis=1)`。\n\n**3. 乐观度估计**\n\n乐观度量化了样本内误差在多大程度上低估了真实的预测误差。我们被要求用两种方式来估计它。\n\n*   **方法 1：使用 LOOCV ($M_{\\text{LOO}} - M_{\\text{in}}$)**\n    LOOCV 提供了对真实预测误差的近似无偏估计。因此，LOOCV 误差与样本内误差之间的差值可作为给定数据集上乐观度的直接估计。\n    $$\n    \\widehat{\\text{Optimism}}_{\\text{CV}} = M_{\\text{LOO}} - M_{\\text{in}}\n    $$\n    其中 $M_{\\text{in}} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$。\n\n*   **方法 2：解析估计**\n    对于一个可加误差模型（$y_i = f(x_i) + \\varepsilon_i$ 且 $\\text{Var}(\\varepsilon_i) = \\sigma^2$）下的线性平滑器，期望的乐观度由下式给出：\n    $$\n    \\mathbb{E}[\\text{Optimism}] = \\frac{2}{n} \\text{tr}(S) \\sigma^2\n    $$\n    我们可以通过代入 $\\sigma^2$ 的估计量来形成一个估计值：\n    $$\n    \\widehat{\\text{Optimism}}_{\\text{analytic}} = \\frac{2}{n} \\text{tr}(S) \\hat{\\sigma}^2\n    $$\n    项 $\\text{tr}(S)$ 是模型的*有效自由度*。利用迹的循环性质，可以在不构建 $S$ 的情况下高效地计算它：\n    $$\n    \\text{tr}(S) = \\text{tr}(X (X^{\\top} X + \\lambda K)^{-1} X^{\\top}) = \\text{tr}(X^{\\top} X (X^{\\top} X + \\lambda K)^{-1})\n    $$\n    进一步的简化得出了一个更高效的公式：\n    $$\n    \\text{tr}(S) = \\text{tr}((X^{\\top}X + \\lambda K - \\lambda K)(X^{\\top}X + \\lambda K)^{-1}) = \\text{tr}(I - \\lambda K (X^{\\top}X + \\lambda K)^{-1}) = p - \\lambda\\,\\text{tr}(K(X^{\\top}X + \\lambda K)^{-1})\n    $$\n    噪声方差 $\\sigma^2$ 的一个一致估计量是基于残差平方和，并根据拟合所用的有效自由度进行调整：\n    $$\n    \\hat{\\sigma}^2 = \\frac{\\|y - \\hat{y}\\|^2}{n - \\text{tr}(S)}\n    $$\n\n### **实现计划**\n\n对于每个测试用例 `(n, sigma, seed, m, lambda)`：\n1.  按规定生成数据 $(x_i, y_i)$。$x_i$ 在 $[0, 1]$ 上均匀分布，且 $y_i = \\sin(2\\pi x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n2.  使用截距项、线性项和带有 $m$ 个内部节点的截断三次样条基函数，构建 $n \\times (2+m)$ 的设计矩阵 $X$。\n3.  构建 $(2+m) \\times (2+m)$ 的对角惩罚矩阵 $K$，其前两个对角线元素为零，其余对角线元素为一。\n4.  通过计算 $M_{inv} = (X^{\\top} X + \\lambda K)^{-1}$，然后计算 $\\hat{\\beta} = M_{inv} X^{\\top} y$ 来求解系数 $\\hat{\\beta}$。\n5.  计算拟合值 $\\hat{y} = X\\hat{\\beta}$ 和残差 $r = y - \\hat{y}$。\n6.  计算样本内均方误差 $M_{\\text{in}} = \\frac{1}{n} \\sum r_i^2$。\n7.  使用 $X$ 和 $M_{inv}$ 计算平滑矩阵的对角线元素 $S_{ii}$。\n8.  使用包含 $r_i$ 和 $S_{ii}$ 的公式计算 LOOCV 均方误差 $M_{\\text{LOO}}$。\n9.  计算第一个乐观度估计，即 $M_{\\text{LOO}} - M_{\\text{in}}$。\n10. 使用高效的迹公式计算有效自由度 $\\text{tr}(S)$。\n11. 使用残差和 $\\text{tr}(S)$ 估计噪声方差 $\\hat{\\sigma}^2$。\n12. 使用 $\\text{tr}(S)$ 和 $\\hat{\\sigma}^2$ 计算第二个解析的乐观度估计。\n13. 整理计算出的四个量：$[M_{\\text{in}}, M_{\\text{LOO}}, M_{\\text{LOO}} - M_{\\text{in}}, \\text{Optimism}_{\\text{analytic}}]$。\n最终输出将是所有测试用例的这些列表的列表，格式化为单个字符串。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the GAM LOOCV and optimism estimation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (n, sigma, seed, m, lambda)\n        (50, 0.2, 42, 8, 0.1),\n        (50, 0.2, 42, 8, 0.0001),\n        (50, 0.2, 42, 8, 10.0),\n        (12, 0.15, 0, 4, 0.2),\n    ]\n\n    def process_case(n, sigma, seed, m, lambda_val):\n        \"\"\"\n        Processes a single GAM regression case, computing in-sample error, LOOCV error,\n        and two estimates of optimism.\n\n        Args:\n            n (int): Number of data points.\n            sigma (float): Standard deviation of the noise.\n            seed (int): Random seed for reproducibility.\n            m (int): Number of interior knots for the spline basis.\n            lambda_val (float): Smoothing parameter.\n\n        Returns:\n            list: A list containing four quantities: [M_in, M_LOO, Optimism_CV, Optimism_analytic].\n        \"\"\"\n        # Step 1: Data Generation\n        rng = np.random.default_rng(seed)\n        x = np.linspace(0.0, 1.0, n, dtype=np.float64)\n        y_true = np.sin(2 * np.pi * x)\n        epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = y_true + epsilon\n\n        # Step 2: Basis and Penalty Construction\n        p = 2 + m\n        \n        # Knots are m equally spaced points in the interior of (0, 1)\n        knots = np.linspace(0.0, 1.0, m + 2, dtype=np.float64)[1:-1]\n\n        # Design matrix X\n        X = np.zeros((n, p), dtype=np.float64)\n        X[:, 0] = 1.0\n        X[:, 1] = x\n        for j in range(m):\n            X[:, 2 + j] = np.maximum(x - knots[j], 0)**3\n\n        # Penalty matrix K\n        K = np.zeros((p, p), dtype=np.float64)\n        np.fill_diagonal(K[2:, 2:], 1.0)\n        \n        # Step 3: Model Fitting\n        X_T_X = X.T @ X\n        M = X_T_X + lambda_val * K\n        M_inv = np.linalg.inv(M)\n\n        beta_hat = M_inv @ X.T @ y\n        y_hat = X @ beta_hat\n\n        # Step 4: Calculate Required Quantities\n        residuals = y - y_hat\n        \n        # In-sample Mean Squared Error (M_in)\n        M_in = np.mean(residuals**2)\n        \n        # LOOCV Mean Squared Error (M_LOO)\n        # Compute diagonal of smoother matrix S = X * (X^T*X + lambda*K)^-1 * X^T\n        diag_S = np.sum((X @ M_inv) * X, axis=1)\n\n        # LOOCV residuals and MSE\n        loo_residuals = residuals / (1.0 - diag_S)\n        M_LOO = np.mean(loo_residuals**2)\n\n        # Optimism Estimate 1: From LOOCV\n        optimism_loo = M_LOO - M_in\n\n        # Optimism Estimate 2: Analytic formula\n        # Efficiently compute trace(S) using the identity: tr(S) = p - lambda*tr(K*M_inv)\n        trace_S = p - lambda_val * np.sum(np.diag(M_inv)[2:])\n\n        rss = np.sum(residuals**2)\n        df_residuals = n - trace_S\n        \n        if df_residuals  1e-9:\n             optimism_analytic = np.nan\n        else:\n            sigma_hat_sq = rss / df_residuals\n            optimism_analytic = (2.0 / n) * trace_S * sigma_hat_sq\n\n        return [M_in, M_LOO, optimism_loo, optimism_analytic]\n        \n    results = []\n    for case in test_cases:\n        result = process_case(*case)\n        results.append(result)\n\n    # The format '[[a,b,c,d],[e,f,g,h]]' is produced by str(list_of_lists) with spaces removed.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3123636"}, {"introduction": "在对计数数据（例如疾病案例数或区域内的物种数量）进行建模时，考虑不同水平的“暴露度”（如人口规模、观察时间）至关重要。在泊松 GAMs 中，这通过一个偏置项（offset）来处理。本练习 [@problem_id:3123654] 介绍了一种强大的诊断技术，用于验证偏置项是否被正确指定。通过比较包含和不包含偏置项时拟合的平滑函数的形状，你可以检测到那些否则可能被忽略的系统性误差。", "problem": "您需要在一个自包含的程序中实现泊松广义可加模型 (GAM) 的暴露验证。其目的是通过比较在移除偏移量并重新拟合模型后，估计的平滑分量如何变化，来检测错误指定的暴露（偏移量）。\n\n基本原理和设置：\n- 考虑由 $i = 1, \\dots, n$ 索引的独立观测值，其中响应变量 $Y_i$ 被建模为一个均值为 $\\mu_i$ 的泊松随机变量，即 $Y_i \\sim \\mathrm{Poisson}(\\mu_i)$ 且 $\\mathrm{E}[Y_i] = \\mu_i$。\n- 广义可加模型 (GAM) 假设通过对数连接函数 $\\eta_i = \\log(\\mu_i)$ 对正则参数施加一个可加结构，其中\n$$\n\\eta_i = \\mathrm{offset}_i + \\beta_0 + f_1(x_{i1}) + f_2(x_{i2}).\n$$\n- 偏移量 $\\mathrm{offset}_i$ 是已知的对数暴露项，$f_j(\\cdot)$ 是我们从数据中估计的未知平滑函数。\n- 将每个平滑函数 $f_j$ 写成基展开形式 $f_j(x_{ij}) = \\sum_{k=1}^{m_j} \\theta_{jk} B_{jk}(x_{ij})$，其中 $B_{jk}$ 是固定的样条基函数，$\\theta_{jk}$ 是待估计的系数。对每个系数向量的二阶离散差分引入二次粗糙度惩罚，以控制平滑度。\n- 使用迭代重加权最小二乘法 (IRLS) 拟合泊松 GAM。这是一种基于广义线性模型最大似然理论的标准方法：在第 $t$ 次迭代中，构建工作响应 $z^{(t)}$ 和权重 $W^{(t)}$，并求解一个惩罚加权最小二乘系统以获得系数。偏移量以加法形式进入线性预测器。\n- 为验证暴露设定，需拟合模型两次：第一次使用提供的偏移向量，第二次移除偏移量（即设为零）。对于每个平滑分量 $j$，计算在观测协变量处的拟合平滑函数，记为 $\\hat{f}_j^{\\mathrm{with}}$ 和 $\\hat{f}_j^{\\mathrm{drop}}$。为比较形状而非水平，通过减去均值并除以标准差来对每个向量进行标准化。计算均方差\n$$\n\\Delta_j = \\frac{1}{n} \\sum_{i=1}^n \\left(\\tilde{f}_{j,i}^{\\mathrm{with}} - \\tilde{f}_{j,i}^{\\mathrm{drop}}\\right)^2,\n$$\n其中 $\\tilde{f}_{j,i}^{\\mathrm{with}}$ 表示在观测值 $i$ 处 $\\hat{f}_j^{\\mathrm{with}}$ 的标准化值，$\\tilde{f}_{j,i}^{\\mathrm{drop}}$ 同理。如果 $\\max_j \\Delta_j$ 超过给定的阈值 $\\tau$，则宣布暴露设定错误。\n\n您的程序必须实现：\n- 在 $[0,1]$ 上构建具有均匀分布内部节点的立方B样条基 $B_{jk}(x)$。\n- 对每个平滑函数的系数施加二阶差分惩罚，为所有平滑函数生成一个块对角惩罚矩阵，同时保持截距项不受惩罚。\n- 针对带对数连接函数和可选偏移量的泊松模型，实现惩罚IRLS算法，在每次迭代中求解一个对称线性系统以获得系数。\n- 通过计算 $j=1,2$ 的 $\\Delta_j$ 并将 $\\max_j \\Delta_j$ 与阈值 $\\tau$ 进行比较，来进行暴露验证。\n\n用于评估的数据生成（固定的、可复现的）：\n- 对于每个测试案例，在 $[0,1]$ 上独立均匀地生成协变量 $x_{i1}, x_{i2}$，并根据一个真实模型模拟计数 $Y_i$，该模型为\n$$\n\\log(\\mu_i) = \\beta_0^\\star + f_1^\\star(x_{i1}) + f_2^\\star(x_{i2}),\n$$\n其中 $\\beta_0^\\star = 1.0$，$f_1^\\star(x) = \\sin(2\\pi x)$，以及 $f_2^\\star(x) = 0.5\\,(x - 0.5)^2$。真实的暴露是常数1，因此真实的偏移量为零。在不同测试案例中，向“带偏移量”的拟合提供不同的偏移向量，以模拟正确和错误的暴露设定。\n\n测试套件：\n提供以下测试案例；对于每个案例，使用指定的参数模拟数据并执行暴露验证，根据阈值 $\\tau$ 返回一个布尔值，指示是否存在错误指定：\n\n1. 案例A（理想路径，正确的暴露）：\n   - $n = 400$，随机种子 $= 2021$，\n   - 在“带偏移量”拟合中使用的偏移量：对所有 $i$，$\\mathrm{offset}_i = 0$，\n   - 阈值 $\\tau = 0.2$，\n   - 预期行为：形状应保持稳定，因此布尔值应为false。\n\n2. 案例B（与$x_{1}$相关的错误指定暴露）：\n   - $n = 400$，随机种子 $= 2022$，\n   - 在“带偏移量”拟合中使用的偏移量：$\\mathrm{offset}_i = 1.5\\,(x_{i1} - 0.5)$，\n   - 阈值 $\\tau = 0.2$，\n   - 预期行为：移除偏移量会因补偿效应而揭示形状差异，因此布尔值应为true。\n\n3. 案例C（边缘案例，具有非线性$x_{2}$模式和较小样本的错误指定暴露）：\n   - $n = 120$，随机种子 $= 2023$，\n   - 在“带偏移量”拟合中使用的偏移量：$\\mathrm{offset}_i = 1.5 \\cos(2\\pi x_{i2})$，\n   - 阈值 $\\tau = 0.2$，\n   - 预期行为：非线性的错误指定会产生显著的形状变化，因此布尔值应为true。\n\n建模和算法细节必须严格按照上述规定实现。所有随机性必须由测试套件中给定的种子控制。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含案例A、B和C的三个布尔值，格式为方括号内以逗号分隔的列表，例如，“[False,True,True]”。", "solution": "用户要求实现一个用于泊松广义可加模型（GAM）的暴露验证程序。该问题是自包含的且科学上合理的，基于统计学习的既定原则，包括GAM、B样条、惩罚似然以及迭代重加权最小二乘（IRLS）算法。验证逻辑定义清晰，测试案例也用所有必要的参数进行了规定。我将着手解决。\n\n任务的核心是拟合泊松GAM两次：一次使用指定的偏移量（对数暴露），一次不使用（偏移量设为零）。这两个拟合得到的估计平滑函数的形状差异被用来诊断暴露项潜在的错误指定。\n\n详细步骤如下：\n\n1.  **数据模拟**：对于每个测试案例，我们首先根据提供的真实模型模拟一个数据集。协变量 $x_{i1}, x_{i2}$ 从 $[0, 1]$ 上的均匀分布中抽取。然后，响应变量 $Y_i$ 从一个泊松分布中抽样，其均值 $\\mu_i$ 的对数由一个真实的可加模型给出：\n    $$\n    \\log(\\mu_i) = \\beta_0^\\star + f_1^\\star(x_{i1}) + f_2^\\star(x_{i2})\n    $$\n    其中 $\\beta_0^\\star = 1.0$，$f_1^\\star(x) = \\sin(2\\pi x)$，以及 $f_2^\\star(x) = 0.5(x - 0.5)^2$。\n\n2.  **模型设定**：待拟合的GAM由线性预测器定义：\n    $$\n    \\eta_i = \\mathrm{offset}_i + \\beta_0 + f_1(x_{i1}) + f_2(x_{i2})\n    $$\n    每个平滑函数 $f_j$ 由使用立方B样条的基展开表示。我们将为每个平滑函数使用 $m_j=10$ 个基函数，这提供了足够的灵活性来捕捉潜在的真实函数。模型矩阵 $X$ 是通过连接一个代表截距项 $\\beta_0$ 的全1列向量以及 $f_1$ 和 $f_2$ 的基矩阵而构建的。\n\n3.  **惩罚似然**：为防止过拟合并确保数值稳定性，通过惩罚其对应系数的粗糙度来鼓励函数 $f_j$ 的平滑性。对每个B样条展开的系数应用二阶差分惩罚。完整的惩罚矩阵 $S$ 是块对角矩阵，其中有一个对应于未惩罚截距项 $\\beta_0$ 的零块，以及对应于每个平滑分量的惩罚矩阵 $\\lambda_j S_j$。对两个平滑函数都使用固定的平滑参数 $\\lambda_j=1.0$。\n\n4.  **通过惩罚IRLS进行模型拟合**：模型系数通过最大化惩罚泊松对数似然来估计。这是通过迭代重加权最小二乘（IRLS）算法实现的。在每次迭代中，我们更新当前均值 $\\mu^{(t)}$ 的估计，并构建一个工作响应 $z^{(t)}$ 和一个对角权重矩阵 $W^{(t)}$。然后，通过求解以下惩罚加权最小二乘系统来找到更新后的系数向量 $\\beta^{(t+1)}$：\n    $$\n    (X^T W^{(t)} X + S) \\beta^{(t+1)} = X^T W^{(t)} z'^{(t)}\n    $$\n    其中 $z'^{(t)}$ 是根据固定偏移量调整后的工作响应。此迭代过程持续进行，直到系数收敛。\n\n5.  **暴露验证**：对每个测试案例执行两次拟合过程：\n    a.  **“带偏移量”拟合**：使用测试案例中指定的偏移向量。\n    b.  **“移除偏移量”拟合**：使用一个全零的偏移向量。\n\n    这将产生两组估计系数，$\\hat{\\beta}^{\\mathrm{with}}$ 和 $\\hat{\\beta}^{\\mathrm{drop}}$。根据这些系数，我们计算每个分量的拟合平滑函数，例如 $\\hat{f}_{j}^{\\mathrm{with}} = X_j \\hat{\\theta}_j^{\\mathrm{with}}$。为了在不考虑水平位移的情况下比较它们的形状，将四个得到的拟合平滑函数向量都进行标准化，使其均值为0，标准差为1。\n\n6.  **差异度量和决策**：为每个分量 $j \\in \\{1, 2\\}$ 计算标准化的“带偏移量”和“移除偏移量”拟合平滑函数之间的均方差 $\\Delta_j$。\n    $$\n    \\Delta_j = \\frac{1}{n} \\sum_{i=1}^n \\left(\\tilde{f}_{j,i}^{\\mathrm{with}} - \\tilde{f}_{j,i}^{\\mathrm{drop}}\\right)^2\n    $$\n    如果这些差异的最大值 $\\max(\\Delta_1, \\Delta_2)$ 超过给定的阈值 $\\tau$，则将该暴露标记为错误指定。\n\n实现过程将这些步骤封装到一系列函数中：用于生成B样条基和惩罚矩阵的辅助函数，一个核心P-IRLS求解器，以及一个主驱动函数，用于按问题陈述中的规定为每个测试案例执行验证。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.interpolate import BSpline\n\ndef get_bspline_basis(x, n_basis, degree):\n    \"\"\"\n    Constructs a B-spline basis matrix with uniformly spaced knots in [0, 1].\n\n    Args:\n        x (np.ndarray): 1D array of data points, assumed to be in [0, 1].\n        n_basis (int): The number of basis functions.\n        degree (int): The degree of the spline (e.g., 3 for cubic).\n\n    Returns:\n        np.ndarray: The basis matrix of shape (len(x), n_basis).\n    \"\"\"\n    n_interior_knots = n_basis - degree - 1\n    if n_interior_knots  0:\n        n_interior_knots = 0\n    \n    # Create uniformly spaced interior knots in (0, 1)\n    interior_knots = np.linspace(0.0, 1.0, n_interior_knots + 2)[1:-1]\n    \n    # Full knot vector with clamped boundaries\n    knots = np.concatenate([\n        np.zeros(degree + 1),\n        interior_knots,\n        np.ones(degree + 1)\n    ])\n    \n    basis_matrix = np.zeros((len(x), n_basis))\n    for i in range(n_basis):\n        # Create a spline basis function by setting the i-th coefficient to 1\n        c = np.zeros(n_basis)\n        c[i] = 1.0\n        spl = BSpline(knots, c, degree, extrapolate=False)\n        basis_matrix[:, i] = spl(x)\n\n    basis_matrix[np.isnan(basis_matrix)] = 0.0\n    \n    return basis_matrix\n\n\ndef get_penalty_matrix(n_basis, penalty_lambda):\n    \"\"\"\n    Constructs the penalty matrix for penalizing second differences of coefficients.\n\n    Args:\n        n_basis (int): Number of coefficients to penalize.\n        penalty_lambda (float): The smoothing parameter.\n\n    Returns:\n        np.ndarray: The (n_basis x n_basis) penalty matrix S.\n    \"\"\"\n    # Create the second-order difference matrix D\n    D = np.diff(np.eye(n_basis), n=2, axis=0)\n    # The penalty matrix is S = D^T D\n    S = D.T @ D\n    return penalty_lambda * S\n\n\ndef fit_gam_poisson(y, X, S_full, offset, n_iter=25, tol=1e-6):\n    \"\"\"\n    Fits a Poisson GAM using Penalized Iteratively Reweighted Least Squares (P-IRLS).\n\n    Args:\n        y (np.ndarray): The response vector (counts).\n        X (np.ndarray): The full model matrix (intercept and basis matrices).\n        S_full (np.ndarray): The block-diagonal penalty matrix.\n        offset (np.ndarray): The offset vector (log-exposure).\n        n_iter (int): Maximum number of IRLS iterations.\n        tol (float): Convergence tolerance for the coefficient vector.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta.\n    \"\"\"\n    n_coeffs = X.shape[1]\n    \n    # Initialize coefficients\n    beta = np.zeros(n_coeffs)\n    \n    for _ in range(n_iter):\n        beta_old = beta.copy()\n        \n        eta = X @ beta + offset\n        eta = np.clip(eta, -20, 20)\n        mu = np.exp(eta)\n        mu[mu  1e-8] = 1e-8\n        \n        weights = mu\n        working_response = eta - offset + (y - mu) / mu\n        \n        LHS = X.T @ (weights[:, np.newaxis] * X) + S_full\n        RHS = X.T @ (weights * working_response)\n        \n        try:\n            beta = np.linalg.solve(LHS, RHS)\n        except np.linalg.LinAlgError:\n            beta = np.linalg.pinv(LHS) @ RHS\n            \n        if np.linalg.norm(beta - beta_old) / (np.linalg.norm(beta_old) + 1e-8)  tol:\n            break\n            \n    return beta\n\n\ndef standardize_vec(v):\n    \"\"\"\n    Standardizes a vector to have mean 0 and standard deviation 1.\n    Handles the case where the standard deviation is zero.\n    \"\"\"\n    s = np.std(v)\n    if s  1e-8:\n        return np.zeros_like(v, dtype=float)\n    return (v - np.mean(v)) / s\n\ndef execute_validation_case(n, seed, offset_func, tau):\n    \"\"\"\n    Sets up and runs a single exposure validation case.\n    \"\"\"\n    # Model hyperparameters\n    n_basis = 10\n    degree = 3\n    lambda_val = 1.0\n\n    # 1. Generate data based on the true model\n    rng = np.random.default_rng(seed)\n    x1 = rng.uniform(0, 1, n)\n    x2 = rng.uniform(0, 1, n)\n    \n    beta0_star = 1.0\n    f1_star = np.sin(2 * np.pi * x1)\n    f2_star = 0.5 * (x2 - 0.5)**2\n    \n    log_mu_true = beta0_star + f1_star + f2_star\n    mu_true = np.exp(log_mu_true)\n    y = rng.poisson(mu_true)\n    \n    # 2. Build model design matrix X\n    B1 = get_bspline_basis(x1, n_basis=n_basis, degree=degree)\n    B2 = get_bspline_basis(x2, n_basis=n_basis, degree=degree)\n    X = np.c_[np.ones(n), B1, B2]\n    \n    # 3. Build the full penalty matrix S_full\n    S1 = get_penalty_matrix(n_basis, lambda_val)\n    S2 = get_penalty_matrix(n_basis, lambda_val)\n    \n    n_coeffs = 1 + n_basis + n_basis\n    S_full = np.zeros((n_coeffs, n_coeffs))\n    S_full[1:1+n_basis, 1:1+n_basis] = S1\n    S_full[1+n_basis:, 1+n_basis:] = S2\n    \n    # 4. Fit the GAM with and without the specified offset\n    offset_with = offset_func(x1, x2)\n    offset_drop = np.zeros(n)\n    \n    beta_with = fit_gam_poisson(y, X, S_full, offset_with)\n    beta_drop = fit_gam_poisson(y, X, S_full, offset_drop)\n    \n    # 5. Extract, standardize, and compare the fitted smooth components\n    theta1_with = beta_with[1:1+n_basis]\n    theta2_with = beta_with[1+n_basis:]\n    theta1_drop = beta_drop[1:1+n_basis]\n    theta2_drop = beta_drop[1+n_basis:]\n    \n    f1_hat_with = B1 @ theta1_with\n    f2_hat_with = B2 @ theta2_with\n    f1_hat_drop = B1 @ theta1_drop\n    f2_hat_drop = B2 @ theta2_drop\n    \n    f1_tilde_with = standardize_vec(f1_hat_with)\n    f2_tilde_with = standardize_vec(f2_hat_with)\n    f1_tilde_drop = standardize_vec(f1_hat_drop)\n    f2_tilde_drop = standardize_vec(f2_hat_drop)\n\n    delta1 = np.mean((f1_tilde_with - f1_tilde_drop)**2)\n    delta2 = np.mean((f2_tilde_with - f2_tilde_drop)**2)\n    \n    # 6. Declare mis-specification if the max difference exceeds the threshold\n    return max(delta1, delta2) > tau\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: Correct exposure (offset=0, matches true model)\n        {'n': 400, 'seed': 2021, 'offset_func': lambda x1, x2: np.zeros(len(x1)), 'tau': 0.2},\n        # Case B: Mis-specified exposure correlated with x1\n        {'n': 400, 'seed': 2022, 'offset_func': lambda x1, x2: 1.5 * (x1 - 0.5), 'tau': 0.2},\n        # Case C: Mis-specified exposure non-linearly related to x2\n        {'n': 120, 'seed': 2023, 'offset_func': lambda x1, x2: 1.5 * np.cos(2 * np.pi * x2), 'tau': 0.2}\n    ]\n    \n    results = []\n    for case in test_cases:\n        is_misspecified = execute_validation_case(\n            n=case['n'],\n            seed=case['seed'],\n            offset_func=case['offset_func'],\n            tau=case['tau']\n        )\n        results.append(is_misspecified)\n    \n    # Print results in the specified format [boolean1,boolean2,...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3123654"}, {"introduction": "GAM 的一个关键优势是其灵活性，但这也会带来挑战，尤其是在外推方面。基于样条的模型对于远超训练数据范围的输入值，可能会产生非常不可靠和极端的预测。本练习 [@problem_id:3123638] 在用于分类的逻辑 GAM 的背景下探讨了这个关键问题。你将研究不同的正则化惩罚（相当于对模型系数施加先验）如何能有效地“驯服”模型在尾部的行为，从而获得更稳健和合理的预测。", "problem": "要求您研究单个预测变量的逻辑广义可加模型 (GAM) 中的外推行为。该模型由伯努利似然和标准 logit 链接函数定义：对于标记数据 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{0,1\\}$，条件均值 $\\mu_i = \\mathbb{E}[y_i \\mid x_i]$ 被建模为 $g(\\mu_i) = \\eta_i$，其中 $g(\\cdot)$ 是 logit 函数 $g(p) = \\log\\left(\\frac{p}{1-p}\\right)$，而 $\\eta_i = \\beta_0 + f(x_i)$ 是包含一个未知平滑函数 $f(\\cdot)$ 的可加预测器。函数 $f(\\cdot)$ 将使用三次回归样条基来表示。目标是量化对于远在训练域之外的预测变量值 $x$，拟合的分类概率 $\\hat{p}(x)$ 的行为，并使用先验或惩罚项来控制这种行为，以避免出现接近 $0$ 或 $1$ 的极端概率。\n\n从以下基本基础开始：\n- 单个观测值的伯努利对数似然为 $\\ell(\\mu; y) = y \\log(\\mu) + (1-y) \\log(1-\\mu)$，其中 $\\mu = \\mathbb{E}[y]$。\n- 逻辑回归的标准链接函数是 logit，$g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)$，因此线性预测器 $\\eta$ 满足 $\\mu = \\frac{1}{1 + e^{-\\eta}}$。\n- 用于广义线性模型的迭代重加权最小二乘法 (IRLS) 使用从线性预测器中对数似然的二阶泰勒近似导出的加权最小二乘更新。\n\n使用带有内部节点的三次回归样条截断幂基来表示平滑函数 $f(x)$。具体来说，令基函数为\n- $b_0(x) = 1$，\n- $b_1(x) = x$，\n- $b_2(x) = x^2$,\n- $b_3(x) = x^3$,\n- 以及，对于内部节点 $c_j$，$b_{3+j}(x) = \\max(x - c_j, 0)^3$。\n那么 $f(x) = \\sum_{j} \\beta_j b_j(x)$，其中 $\\beta_j$ 是通过最大化惩罚对数似然来估计的系数。\n\n为避免在远离训练数据的 $x$ 处出现极端的 $\\hat{p}(x)$，对系数施加高斯先验，这等效于 IRLS 正规方程中的岭惩罚。该惩罚是一个对角矩阵，其非截距项系数对应的条目为非负值。对高阶多项式项 $x^2$、$x^3$ 以及截断三次项施加更强的惩罚，会强制产生更保守的尾部，并减少对数几率在训练区域外幅值无界增长的趋势。\n\n您的程序必须：\n- 生成大小为 $n = 200$ 的合成训练数据，其中预测变量值 $x_i$ 从区间 $[-2,2]$ 中均匀抽取，使用固定的随机种子 $s = 42$ 以确保可复现性。真实的数据生成模型为 $y_i \\sim \\text{Bernoulli}(p_i)$，其中\n$$\np_i = \\frac{1}{1 + \\exp\\left(-\\left(\\alpha + f^*(x_i)\\right)\\right)}, \\quad \\alpha = 0.2,\\quad f^*(x) = 2 \\sin(x).\n$$\n- 构建三次回归样条基，其内部节点位于 $c_1 = -1.5$、$c_2 = -0.5$、$c_3 = 0.5$ 和 $c_4 = 1.5$。\n- 使用 IRLS 拟合一个惩罚逻辑 GAM。在每个 IRLS 步骤中，求解带有对角岭惩罚的加权正规方程，该惩罚的条目编码了一个均值为零、方差为选定值的关于 $\\beta$ 的高斯先验。截距项 $\\beta_0$ 不得被惩罚。\n- 对于给定的惩罚配置，计算四个外推点 $x = -10$、$x = -6$、$x = 6$ 和 $x = 10$ 处的拟合概率 $\\hat{p}(x)$，并以浮点数形式返回它们。\n\n测试套件：\n运行以下四个测试用例，每个用例对应一个不同的对角惩罚配置（列表条目按顺序对应于系数 $[b_0,b_1,b_2,b_3,b_{4},b_{5},b_{6},b_{7}]$）：\n- 用例 A（除数值稳定化外无惩罚）：$[0, 0, 0, 0, 0, 0, 0, 0]$。\n- 用例 B（对所有非截距项施加统一的温和惩罚）：$[0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]$。\n- 用例 C（对高阶项施加强惩罚，对线性项施加较轻惩罚）：$[0, 0.1, 100, 100, 10, 10, 10, 10]$。\n- 用例 D（对所有非截距项施加非常强的惩罚）：$[0, 1000, 1000, 1000, 1000, 1000, 1000, 1000]$。\n\n对于每个用例，拟合模型并计算包含四个浮点数的列表 $[\\hat{p}(-10), \\hat{p}(-6), \\hat{p}(6), \\hat{p}(10)]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有四个用例的结果，格式为逗号分隔的浮点数列表的列表，不含空格，并用方括号括起来。例如：$[[p_{A,-10},p_{A,-6},p_{A,6},p_{A,10}],[p_{B,-10},p_{B,-6},p_{B,6},p_{B,10}],[p_{C,-10},p_{C,-6},p_{C,6},p_{C,10}],[p_{D,-10},p_{D,-6},p_{D,6},p_{D,10}]]$。单位是 $[0,1]$ 范围内的纯数，解释为概率，并以小数形式表示。", "solution": "该问题要求实现并分析用于二元分类的惩罚逻辑广义可加模型 (GAM)。主要目标是通过将模型拟合到合成数据，并在不同正则化方案下，评估在远超出训练数据范围的点上的预测概率，从而研究模型的外推行为。\n\n### 模型设定\n对于给定的单个预测变量 $x_i$，二元响应 $y_i \\in \\{0, 1\\}$ 的模型基于伯努利分布。条件均值 $\\mu_i = \\mathbb{E}[y_i | x_i]$，即概率 $p(y_i=1|x_i)$，通过 logit 链接函数与线性预测器 $\\eta_i$ 相关联：\n$$\ng(\\mu_i) = \\log\\left(\\frac{\\mu_i}{1-\\mu_i}\\right) = \\eta_i\n$$\n对链接函数求逆，可得到作为线性预测器函数的概率：\n$$\n\\mu_i = \\frac{1}{1 + e^{-\\eta_i}}\n$$\n线性预测器 $\\eta_i$ 被建模为截距项 $\\beta_0$ 和平滑函数 $f(x_i)$ 的可加组合：\n$$\n\\eta_i = \\beta_0 + f(x_i)\n$$\n\n### 样条基表示\n平滑函数 $f(x)$ 使用带有截断幂基的三次回归样条来表示。这使我们能够将非线性函数 $f(x)$ 表示为基函数的线性组合。完整的线性预测器 $\\eta(x)$ 可以写成一个线性模型 $\\eta(x) = \\mathbf{B}(x)^T \\boldsymbol{\\beta}$，其中 $\\mathbf{B}(x)$ 是在 $x$ 处求值的基函数向量，$\\boldsymbol{\\beta}$ 是待估计的系数向量。\n\n对于此问题，预测变量值 $x$ 的基向量定义为：\n$$\n\\mathbf{B}(x) = [1, x, x^2, x^3, (x - c_1)_+^3, (x - c_2)_+^3, (x - c_3)_+^3, (x - c_4)_+^3]^T\n$$\n其中 $(u)_+ = \\max(u, 0)$ 是正部函数。该基包含用于截距 $\\beta_0$ 的一个常数项、最高 3 次的多项式项（$x$、$x^2$、$x^3$），以及对应 4 个内部节点 $c_j$ 中每一个的一个截断三次项。指定的节点为 $c_1=-1.5$，$c_2=-0.5$，$c_3=0.5$ 和 $c_4=1.5$。这产生一个 8 维系数向量 $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_7]^T$。对于一组 $n$ 个观测值 $\\{x_i\\}_{i=1}^n$，我们构建 $n \\times 8$ 的设计矩阵 $X$，其中第 $i$ 行为 $\\mathbf{B}(x_i)^T$。所有观测值的线性预测器向量则为 $\\boldsymbol{\\eta} = X\\boldsymbol{\\beta}$。\n\n### 通过迭代重加权最小二乘法 (IRLS) 进行惩罚估计\n系数 $\\boldsymbol{\\beta}$ 通过最大化惩罚对数似然来估计。$n$ 个观测值的伯努利对数似然为：\n$$\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left( y_i \\eta_i - \\log(1 + e^{\\eta_i}) \\right)\n$$\n为了控制模型复杂度并防止过拟合，特别是高次多项式在训练范围外的不稳定行为，我们增加了一个惩罚项。该惩罚项等效于对系数施加一个零均值高斯先验。截距项 $\\beta_0$ 不被惩罚。惩罚对数似然为：\n$$\n\\ell_p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) - \\frac{1}{2}\\sum_{j=1}^{7} \\lambda_j \\beta_j^2 = \\ell(\\boldsymbol{\\beta}) - \\frac{1}{2}\\boldsymbol{\\beta}^T S \\boldsymbol{\\beta}\n$$\n其中 $S$ 是一个对角惩罚矩阵，其对角线元素为 $[\\lambda_0, \\lambda_1, \\dots, \\lambda_7]$，并且我们设定 $\\lambda_0 = 0$。\n\n$\\ell_p(\\boldsymbol{\\beta})$ 的最大化使用迭代重加权最小二乘 (IRLS) 算法进行。IRLS 是 Newton 方法的一个应用。在每次迭代 $(t)$ 中，它通过求解一个惩罚加权最小二乘问题来更新系数向量。更新规则为：\n$$\n\\boldsymbol{\\beta}^{(t+1)} = (X^T W^{(t)} X + S)^{-1} X^T W^{(t)} \\mathbf{z}^{(t)}\n$$\n此处：\n- $\\boldsymbol{\\beta}^{(t)}$ 是上一次迭代的系数向量。\n- $W^{(t)}$ 是一个权重对角矩阵，其元素为 $W_{ii}^{(t)} = \\mu_i^{(t)}(1-\\mu_i^{(t)})$，其中 $\\mu_i^{(t)}$ 是使用 $\\boldsymbol{\\beta}^{(t)}$ 计算出的拟合概率。\n- $\\mathbf{z}^{(t)}$ 是工作响应向量，其元素为 $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}(1-\\mu_i^{(t)})}$。\n\n算法以 $\\boldsymbol{\\beta}^{(0)} = \\mathbf{0}$ 初始化，并运行固定次数的迭代。\n\n### 实现与分析\n该过程首先生成大小为 $n=200$ 的合成训练数据。预测变量值 $x_i$ 从 $[-2, 2]$ 上的均匀分布中抽取。二元结果 $y_i$ 从伯努利分布中采样，其真实概率为 $p_i = (1 + \\exp(-(\\alpha + f^*(x_i))))^{-1}$，其中 $\\alpha=0.2$ 且 $f^*(x) = 2\\sin(x)$。固定的随机种子 $s=42$ 确保了可复现性。\n\n对于四个指定的惩罚配置（用例 A-D）中的每一个，执行 IRLS 算法以找到估计的系数向量 $\\hat{\\boldsymbol{\\beta}}$。这些配置代表：\n- 用例 A ($[0, 0, 0, 0, 0, 0, 0, 0]$)：无正则化。\n- 用例 B ($[0, 0.5, \\dots, 0.5]$)：对所有非截距项施加一个温和、统一的惩罚。\n- 用例 C ($[0, 0.1, 100, 100, 10, \\dots, 10]$)：对二次、三次和样条项施加强惩罚，对线性项施加弱惩罚。\n- 用例 D ($[0, 1000, \\dots, 1000]$)：对所有非截距项施加非常强的惩罚，迫使模型趋向于一个常数对数几率（仅截距项）。\n\n拟合后，使用模型在外推点 $x \\in \\{-10, -6, 6, 10\\}$ 预测概率 $\\hat{p}(x)$。这通过首先为这些点构建设计矩阵 $X_{extrap}$，然后计算对数几率 $\\boldsymbol{\\eta}_{extrap} = X_{extrap} \\hat{\\boldsymbol{\\beta}}$，最后应用逆 logit 函数 $\\hat{\\mathbf{p}}_{extrap} = (1 + \\exp(-\\boldsymbol{\\eta}_{extrap}))^{-1}$ 来完成。比较四个用例的结果表明，增加对高阶项的惩罚如何抑制外推行为，使预测概率远离 $0$ 和 $1$ 的极端值。", "answer": "```python\nimport numpy as np\n\ndef build_design_matrix(x, knots):\n    \"\"\"\n    Constructs the design matrix for a cubic regression spline with a truncated power basis.\n\n    Args:\n        x (np.ndarray): 1D array of predictor values.\n        knots (np.ndarray): 1D array of interior knot locations.\n\n    Returns:\n        np.ndarray: The design matrix X.\n    \"\"\"\n    n_obs = len(x)\n    n_knots = len(knots)\n    # Total columns: 1 (intercept) + 3 (polynomial terms) + n_knots\n    X = np.zeros((n_obs, 4 + n_knots))\n    \n    X[:, 0] = 1.0\n    X[:, 1] = x\n    X[:, 2] = x**2\n    X[:, 3] = x**3\n    \n    for i, knot in enumerate(knots):\n        X[:, 4 + i] = np.maximum(x - knot, 0)**3\n        \n    return X\n\n\ndef fit_penalized_gam(X, y, penalty_diag, n_iter=25):\n    \"\"\"\n    Fits a penalized logistic GAM using Iteratively Reweighted Least Squares (IRLS).\n\n    Args:\n        X (np.ndarray): The design matrix.\n        y (np.ndarray): The binary response vector.\n        penalty_diag (np.ndarray): The diagonal entries of the penalty matrix.\n        n_iter (int): The number of IRLS iterations.\n\n    Returns:\n        np.ndarray: The estimated coefficient vector beta.\n    \"\"\"\n    n_features = X.shape[1]\n    beta = np.zeros(n_features)\n    S = np.diag(penalty_diag)\n    \n    # Epsilon to prevent mu=0 or mu=1, which cause infinite weights/responses\n    epsilon = 1e-8\n\n    for _ in range(n_iter):\n        eta = X @ beta\n        \n        # Clip eta to avoid numerical overflow in exp()\n        eta = np.clip(eta, -25, 25)\n\n        mu = 1.0 / (1.0 + np.exp(-eta))\n        \n        # Clip mu to prevent weights from becoming exactly zero\n        mu = np.clip(mu, epsilon, 1.0 - epsilon)\n        \n        weights = mu * (1.0 - mu)\n        z = eta + (y - mu) / weights\n        \n        # Efficiently compute the left-hand side and right-hand side of the normal equations\n        # This avoids forming the full n x n weight matrix W\n        X_w = X * weights[:, np.newaxis]\n        lhs = X.T @ X_w + S\n        rhs = X.T @ (weights * z)\n        \n        try:\n            beta = np.linalg.solve(lhs, rhs)\n        except np.linalg.LinAlgError:\n            # Add a small ridge for numerical stability if lhs is singular.\n            # This is relevant for the unpenalized case (Case A).\n            beta = np.linalg.solve(lhs + np.eye(n_features) * 1e-8, rhs)\n\n    return beta\n\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation, fit models for each test case,\n    and generate the final output in the required format.\n    \"\"\"\n    # Define the four test cases for the penalty vector\n    test_cases = [\n        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5],\n        [0.0, 0.1, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0],\n        [0.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0, 1000.0]\n    ]\n\n    # --- Problem constants and data generation ---\n    n_samples = 200\n    seed = 42\n    alpha = 0.2\n    knots = np.array([-1.5, -0.5, 0.5, 1.5])\n    extrapolation_points = np.array([-10.0, -6.0, 6.0, 10.0])\n\n    # Generate synthetic training data for reproducibility\n    rng = np.random.default_rng(seed)\n    x_train = rng.uniform(-2.0, 2.0, n_samples)\n    true_log_odds = alpha + 2.0 * np.sin(x_train)\n    p_true = 1.0 / (1.0 + np.exp(-true_log_odds))\n    y_train = rng.binomial(1, p_true, n_samples)\n    \n    # --- Construct design matrices ---\n    X_train = build_design_matrix(x_train, knots)\n    X_extrap = build_design_matrix(extrapolation_points, knots)\n\n    all_results = []\n    \n    # --- Loop through test cases, fit model, and predict ---\n    for penalty_values in test_cases:\n        penalty_diag = np.array(penalty_values)\n        \n        # Fit the penalized GAM model using IRLS\n        beta_hat = fit_penalized_gam(X_train, y_train, penalty_diag)\n        \n        # Predict probabilities at extrapolation points\n        eta_extrap = X_extrap @ beta_hat\n        p_extrap = 1.0 / (1.0 + np.exp(-eta_extrap))\n        \n        all_results.append(list(p_extrap))\n        \n    # --- Format final output string ---\n    # The required format is [[...],[...],...] with no spaces and decimal float representation.\n    formatted_lists = []\n    for result_list in all_results:\n        # Format each inner list to ensure decimal float representation without scientific notation\n        inner_str = f\"[{','.join([f'{val:.15f}' for val in result_list])}]\"\n        formatted_lists.append(inner_str)\n    \n    final_output = f\"[[{','.join(formatted_lists)}]]\"\n    print(final_output)\n\nsolve()\n```", "id": "3123638"}]}