{"hands_on_practices": [{"introduction": "支持向量机 (SVM) 的几何目标，即最小化 $w$ 的范数，对输入特征的尺度非常敏感。这个练习 [@problem_id:3147185] 将通过一个具体的例子来清晰地展示这一点。我们将看到，对一个特征轴进行显著拉伸会如何扭曲“最大间隔”解，以及一个简单的预处理步骤（称为白化）如何能够恢复一个更平衡、更直观的决策边界。", "problem": "一个二维二元数据集按如下方式构建。首先，在一个内在的、标准化的坐标系 $z \\in \\mathbb{R}^2$ 中定义四个点：\n- 类别 $+1$：$z^{(+)}_1 = (1,1)$，$z^{(+)}_2 = (1.5,1.5)$。\n- 类别 $-1$：$z^{(-)}_1 = (-1,-1)$，$z^{(-)}_2 = (-1.5,-1.5)$。\n\n这些点位于直线 $z_2 = z_1$ 上，并且关于原点对称。然后，通过各向异性的线性缩放 $x = S z$ 获得观测特征，其中\n$$\nS = \\mathrm{diag}(M,\\,1),\n$$\n其中 $M \\gg 1$ 量化了第一个和第二个特征之间的极端方差差异（第一个特征的尺度要大得多）。因此，在观测坐标 $x \\in \\mathbb{R}^2$ 中，这四个点是\n$$\nx^{(+)}_1 = (M,\\,1),\\quad x^{(+)}_2 = (1.5M,\\,1.5),\\quad x^{(-)}_1 = (-M,\\,-1),\\quad x^{(-)}_2 = (-1.5M,\\,-1.5).\n$$\n假设数据以原点为中心（根据对称性成立），并考虑硬间隔最大间隔分类器，即支持向量机（SVM），它在满足完美分离约束的条件下，最小化法向量的平方欧几里得范数。\n\n这里的白化（Whitening）定义为将观测特征重新缩放至单位方差和零均值的线性变换。在此构造中，因为第二个特征已经具有单位尺度，而第一个特征按 $M$ 进行了缩放，所以白化简化为将线性映射 $W = S^{-1} = \\mathrm{diag}(1/M,\\,1)$ 应用于 $x$。\n\n仅使用最大间隔分类器的定义，即作为最小化法向量平方范数并受线性分离约束的凸规划的解，推断在观测空间中与白化后分离超平面的方向。哪个陈述最准确？\n\nA. 在 $M \\gg 1$ 的观测坐标中，最大间隔法向量与 $(M,\\,1)$ 成比例，产生一条近乎垂直的分离线（超平面几乎平行于 $x_2$ 轴）。通过 $W = S^{-1}$ 进行白化后，最大间隔法向量与 $(1,\\,1)$ 成比例，得到与 $x_1 + x_2 = 0$ 正交的平衡对角分离器。\n\nB. 白化不会改变最大间隔法向量，因为支持向量机（SVM）对特征缩放是不变的。\n\nC. 即使不进行白化，切换到具有任何有限惩罚参数 $C$ 的软间隔 SVM，也会使观测坐标中的分离器旋转至与 $(1,\\,1)$ 成比例。\n\nD. 在观测坐标中，对于所有 $M$，最大间隔法向量都与 $(1,\\,1)$ 成比例，但当 $M \\gg 1$ 时，白化会将其旋转至朝向 $(M,\\,1)$。", "solution": "用户希望我分析硬间隔支持向量机（SVM）在各向异性特征缩放下的行为。\n\n### 第 1 步：问题验证\n\n**1. 提取已知条件：**\n-   **数据集：** 一个二维二元数据集。\n-   **内在坐标 ($z \\in \\mathbb{R}^2$)：**\n    -   类别 $+1$：$z^{(+)}_1 = (1,1)$，$z^{(+)}_2 = (1.5,1.5)$。\n    -   类别 $-1$：$z^{(-)}_1 = (-1,-1)$，$z^{(-)}_2 = (-1.5,-1.5)$。\n-   **各向异性缩放：** 观测坐标 $x \\in \\mathbb{R}^2$ 是通过线性变换 $x = Sz$ 从内在坐标 $z$ 获得的，其中缩放矩阵为 $S = \\mathrm{diag}(M, 1)$ 且 $M \\gg 1$。\n-   **观测坐标 ($x \\in \\mathbb{R}^2$)：**\n    -   类别 $+1$：$x^{(+)}_1 = (M,1)$，$x^{(+)}_2 = (1.5M,1.5)$。\n    -   类别 $-1$：$x^{(-)}_1 = (-M,-1)$，$x^{(-)}_2 = (-1.5M,-1.5)$。\n-   **分类器：** 一个硬间隔最大间隔分类器（SVM），它在满足完美分离的条件下，最小化法向量的平方欧几里得范数 $\\|w\\|^2$。\n-   **白化：** 一个线性变换 $W = S^{-1} = \\mathrm{diag}(1/M, 1)$ 应用于观测特征 $x$。该变换将 $x$ 映射回 $z$，即 $z = Wx$。\n-   **假设：** 数据以原点为中心。根据构造，此假设成立。\n\n**2. 使用提取的已知条件进行验证：**\n-   **科学依据：** 该问题基于支持向量机和线性代数的标准且完善的数学框架。所有概念都是统计学习的基础。该设置在科学上是合理的。\n-   **适定性：** 数据被明确定义并且是线性可分的。对于线性可分数据，硬间隔 SVM 的优化问题是一个凸问题，其分离超平面有唯一解。问题清晰，可以通过数学推导来回答。\n-   **客观性：** 该问题使用精确的数学语言陈述，没有主观或含糊的术语。条件 $M \\gg 1$ 是分析渐近行为的标准约定。\n\n**3. 结论与行动：**\n问题陈述有效。这是统计学习领域中一个测试 SVM 核心属性的适定问题。我将继续进行求解。\n\n### 第 2 步：求解推导\n\n最大间隔分类器寻找一个由法向量 $w$ 和偏移量 $b$ 定义的分离超平面，该超平面是以下优化问题的解：\n$$\n\\min_{w, b} \\frac{1}{2} \\|w\\|^2\n$$\n约束条件为：\n$$\ny_i (w^T x_i + b) \\geq 1 \\quad \\text{对于所有数据点 } (x_i, y_i).\n$$\n\n根据给定数据点的对称性（对于每个带有标签 $y_i$ 的点 $x_i$，都存在一个带有标签 $-y_i$ 的点 $-x_i$），最优分离超平面必须穿过原点。这意味着偏移量 $b=0$。问题简化为：\n$$\n\\min_{w} \\frac{1}{2} \\|w\\|^2 \\quad \\text{约束条件为} \\quad y_i (w^T x_i) \\geq 1.\n$$\n\n#### 在观测空间 ($x$) 中的分析\n\n观测空间中的数据点是 $x^{(+)}_1 = (M,1)$，$x^{(+)}_2 = (1.5M,1.5)$，$x^{(-)}_1 = (-M,-1)$，和 $x^{(-)}_2 = (-1.5M,-1.5)$。\n所有这些点都是共线的，位于穿过原点且方向向量为 $v = (M,1)$ 的直线上。每个点 $x_i$ 都可以写成 $x_i = c_i v$ 的形式，其中 $c_i$ 是某个标量。对于类别 $+1$，标量是 $c_1=1$，$c_2=1.5$；对于类别 $-1$，标量是 $c_3=-1$，$c_4=-1.5$。对应的标签是 $y_1=1, y_2=1, y_3=-1, y_4=-1$。\n\n设此空间中的法向量为 $w_x$。约束条件为：\n$y_i (w_x^T (c_i v)) \\geq 1 \\implies (y_i c_i) (w_x^T v) \\geq 1$。\n\n我们来计算乘积 $y_i c_i$：\n-   $y_1 c_1 = (+1)(1) = 1$\n-   $y_2 c_2 = (+1)(1.5) = 1.5$\n-   $y_3 c_3 = (-1)(-1) = 1$\n-   $y_4 c_4 = (-1)(-1.5) = 1.5$\n\n所有 $y_i c_i$ 的值都是正的。设 $K_x = w_x^T v$。对所有四个点的约束变为 $1 \\cdot K_x \\geq 1$ 和 $1.5 \\cdot K_x \\geq 1$。为了使两者都成立，我们只需要满足从具有最小 $y_i c_i$ 的点导出的更严格的约束，即 $K_x \\geq 1$。\n\n优化问题是最小化 $\\frac{1}{2} \\|w_x\\|^2$，约束条件为 $w_x^T v \\geq 1$，即 $M w_{x1} + w_{x2} \\geq 1$。$\\|w_x\\|^2$ 的最小值将在约束激活时出现，即等式成立时：$M w_{x1} + w_{x2} = 1$。\n\n我们使用拉格朗日乘子法。设 $L(w_x, \\lambda) = \\frac{1}{2}(w_{x1}^2 + w_{x2}^2) - \\lambda(M w_{x1} + w_{x2} - 1)$。\n对 $w_{x1}$ 和 $w_{x2}$ 求偏导：\n$$\n\\frac{\\partial L}{\\partial w_{x1}} = w_{x1} - \\lambda M = 0 \\implies w_{x1} = \\lambda M\n$$\n$$\n\\frac{\\partial L}{\\partial w_{x2}} = w_{x2} - \\lambda = 0 \\implies w_{x2} = \\lambda\n$$\n将这些代入约束方程：\n$$\nM(\\lambda M) + \\lambda = 1 \\implies \\lambda(M^2+1) = 1 \\implies \\lambda = \\frac{1}{M^2+1}\n$$\n因此，最优法向量为：\n$$\nw_x = \\left(\\frac{M}{M^2+1}, \\frac{1}{M^2+1}\\right)\n$$\n该向量与 $(M,1)$ 成比例。对于 $M \\gg 1$，$w_x$ 的方向接近 $(1,0)$，这是一个水平向量。分离超平面由 $w_x^T x = 0$ 定义，即 $M x_1 + x_2 = 0$。这是直线 $x_2 = -M x_1$。对于大的 $M$，这是一条非常陡峭的线，即一条近乎垂直的分离线，几乎平行于 $x_2$ 轴。\n\n#### 在白化空间 ($z$) 中的分析\n\n白化变换 $z=Wx$ 将观测点 $x_i$ 映射回内在点 $z_i$。分析是在这些点上进行的：$z^{(+)}_1 = (1,1)$，$z^{(+)}_2 = (1.5,1.5)$，$z^{(-)}_1 = (-1,-1)$，和 $z^{(-)}_2 = (-1.5,-1.5)$。\n这些点位于穿过原点且方向向量为 $u = (1,1)$ 的直线上。\n同样的逻辑适用。设此空间中的法向量为 $w_z$。优化问题是最小化 $\\frac{1}{2} \\|w_z\\|^2$，约束条件为 $y_i(w_z^T z_i) \\geq 1$。这简化为最小化 $\\frac{1}{2} \\|w_z\\|^2$，约束条件为 $w_z^T u \\geq 1$。\n\n当 $w_z^T u = 1$ 时达到最小值，即 $w_{z1} + w_{z2} = 1$。\n使用拉格朗日乘子法处理 $L(w_z, \\lambda) = \\frac{1}{2}(w_{z1}^2 + w_{z2}^2) - \\lambda(w_{z1} + w_{z2} - 1)$：\n$$\n\\frac{\\partial L}{\\partial w_{z1}} = w_{z1} - \\lambda = 0 \\implies w_{z1} = \\lambda\n$$\n$$\n\\frac{\\partial L}{\\partial w_{z2}} = w_{z2} - \\lambda = 0 \\implies w_{z2} = \\lambda\n$$\n代入约束条件：\n$$\n\\lambda + \\lambda = 1 \\implies 2\\lambda=1 \\implies \\lambda = \\frac{1}{2}\n$$\n最优法向量为：\n$$\nw_z = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)\n$$\n该向量与 $(1,1)$ 成比例。分离超平面是 $w_z^T z = 0$，即 $z_1 + z_2 = 0$。这是直线 $z_2 = -z_1$，一条斜率为 $-1$ 的对角线。它与数据所在的直线 $z_2 = z_1$ 正交。这是一个平衡的分离器。\n\n### 第 3 步：逐项分析选项\n\n**A. 在 $M \\gg 1$ 的观测坐标中，最大间隔法向量与 $(M,\\,1)$ 成比例，产生一条近乎垂直的分离线（超平面几乎平行于 $x_2$ 轴）。通过 $W = S^{-1}$ 进行白化后，最大间隔法向量与 $(1,\\,1)$ 成比例，得到与 $x_1 + x_2 = 0$ 正交的平衡对角分离器。**\n-   对观测坐标的分析是正确的。法向量与 $(M,1)$ 成比例，对于大的 $M$，分离线 $x_2 = -M x_1$ 近乎垂直。\n-   对白化坐标的分析也是正确的。法向量与 $(1,1)$ 成比例，分离器是平衡的对角线 $z_1+z_2=0$。\n-   最后的短语“与 $x_1 + x_2 = 0$ 正交”措辞有些尴尬。但是，这个选项是对结果最准确的描述。\n**结论：正确**\n\n**B. 白化不会改变最大间隔法向量，因为支持向量机（SVM）对特征缩放是不变的。**\n这个陈述在两点上是错误的。首先，我们的推导表明法向量的方向从与 $(M,1)$ 成比例变为与 $(1,1)$ 成比例。其次，给出的理由是错误的。众所周知，标准 SVM 对特征缩放*不是*不变的，这正是该问题所演示的现象。\n**结论：错误**\n\n**C. 即使不进行白化，切换到具有任何有限惩罚参数 $C$ 的软间隔 SVM，也会使观测坐标中的分离器旋转至与 $(1,\\,1)$ 成比例。**\n在软间隔 SVM 中，法向量由 $w_x = \\sum_i \\alpha_i y_i x_i$ 给出。由于每个数据点 $x_i$ 都是向量 $v=(M,1)$ 的标量倍数，即 $x_i = c_i v$，因此得到的法向量是 $w_x = \\sum_i \\alpha_i y_i c_i v = (\\sum_i \\alpha_i y_i c_i) v$。这表明对于任何一组非零的拉格朗日乘子 $\\alpha_i$，$w_x$ 都必须与 $v=(M,1)$ 成比例。因此，分离器的方向不会改变，也不会变得与 $(1,1)$ 成比例。\n**结论：错误**\n\n**D. 在观测坐标中，对于所有 $M$，最大间隔法向量都与 $(1,\\,1)$ 成比例，但当 $M \\gg 1$ 时，白化会将其旋转至朝向 $(M,\\,1)$。**\n这个陈述与我们的发现完全相反。在观测坐标中，法向量与 $(M,1)$ 成比例，而不是 $(1,1)$。白化后，它与 $(1,1)$ 成比例，而不是 $(M,1)$。\n**结论：错误**", "answer": "$$\\boxed{A}$$", "id": "3147185"}, {"introduction": "现实世界的数据很少是完美线性可分的，软间隔分类器通过引入松弛变量 $\\xi_i$ 来解决这个问题。然而，我们应当如何量化这些违例的“代价”呢？这个练习 [@problem_id:3147193] 深入探讨了在松弛变量上使用标准线性惩罚 ($\\sum \\xi_i$) 与二次惩罚 ($\\sum \\xi_i^2$) 之间的深刻差异。通过这个对比，我们将揭示处理分类错误的两种不同策略，以及惩罚函数的选择如何反映我们对数据中离群点的不同假设。", "problem": "一个二元分类数据集由带标签的点 $\\{(x_i, y_i)\\}_{i=1}^n$ 组成，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\{-1, +1\\}$。线性分类器由权重向量 $w \\in \\mathbb{R}^2$ 和偏置 $b \\in \\mathbb{R}$ 指定，导出决策函数 $f(x) = \\operatorname{sign}(w^\\top x + b)$。在最大间隔分类中，几何间隔定义为 $\\gamma = 1 / \\|w\\|$，而软间隔分类引入松弛变量 $\\xi_i \\ge 0$ 以允许违反单位间隔约束 $y_i \\,(w^\\top x_i + b) \\ge 1 - \\xi_i$。考虑一种双目标方法，该方法试图在最大化间隔 $\\gamma$ 的同时，最小化由松弛变量平方和 $\\sum_{i=1}^n \\xi_i^2$衡量的总违规量。为了得到单一目标，引入一个正的正则化参数 $C  0$，用于权衡间隔和违规量的大小。\n\n现在，为揭示二次松弛惩罚与线性惩罚在集中误差方面的不同之处，请分析以下具体数据集和两个候选超平面。该数据集有 $7$ 个点：\n- 正类点：$x_1 = (2, 0)$ 对应 $y_1 = +1$，$x_2 = (3, 0)$ 对应 $y_2 = +1$，以及一个离群点 $x_3 = (-4, 0)$ 对应 $y_3 = +1$。\n- 负类点：$x_4 = (-3, 0)$ 对应 $y_4 = -1$，$x_5 = (-2.5, 0)$ 对应 $y_5 = -1$，$x_6 = (-2, 0)$ 对应 $y_6 = -1$，$x_7 = (-1.5, 0)$ 对应 $y_7 = -1$。\n\n考虑两个固定的候选超平面，每个超平面都具有相同的权重范数 $\\|w\\| = 1$（因此它们有相同的间隔 $\\gamma = 1$）：\n- 超平面 $H_1$：$w = (1, 0)$ 和 $b = 0$。\n- 超平面 $H_2$：$w = (1, 0)$ 和 $b = 2$。\n\n对于每个超平面，松弛变量 $\\xi_i$ 由约束 $y_i \\,(w^\\top x_i + b) \\ge 1 - \\xi_i$ 和 $\\xi_i \\ge 0$ 确定。因为 $H_1$ 和 $H_2$ 的间隔相同，所以在带有二次惩罚的双目标下，选择问题简化为比较各自的和 $\\sum_{i=1}^7 \\xi_i^2$。关于在二次松弛惩罚下哪个超平面更优，以及这个选择与在线性松弛惩罚 $\\sum_{i=1}^7 \\xi_i$下哪个更优有何不同，以下哪个陈述是正确的？\n\nA. 在二次惩罚下，$H_1$ 更优，因为平方后，一个大的违规比几个小的违规代价更小；在线性惩罚下，$H_2$ 会更优。\n\nB. 在二次惩罚下，$H_2$ 更优，因为将违规量分散到几个较小的松弛变量上可以减少 $\\sum \\xi_i^2$，即使线性和 $\\sum \\xi_i$ 增加；在线性惩罚下，$H_1$ 会更优。\n\nC. 在二次惩罚下，$H_1$ 和 $H_2$ 是等价的，因为它们有相同的间隔和相同的松弛变量平方和；线性惩罚也得出等价的结论。\n\nD. 在没有核函数或对偶变量的情况下，无法确定 $H_1$ 和 $H_2$ 之间的优劣；间隔相等导致无法得出任何结论。", "solution": "### 步骤1：提取已知条件\n问题提供了以下信息：\n- 一个二元分类数据集 $\\{(x_i, y_i)\\}_{i=1}^7$，其中 $x_i \\in \\mathbb{R}^2$ 且 $y_i \\in \\{-1, +1\\}$。\n- 线性分类器定义为 $f(x) = \\operatorname{sign}(w^\\top x + b)$。\n- 几何间隔为 $\\gamma = 1 / \\|w\\|$。\n- 软间隔约束为 $y_i \\,(w^\\top x_i + b) \\ge 1 - \\xi_i$，松弛变量 $\\xi_i \\ge 0$。\n- 目标是一个双目标优化：最大化间隔 $\\gamma$ 并最小化松弛变量平方和 $\\sum_{i=1}^n \\xi_i^2$。这与最小化线性松弛变量和 $\\sum_{i=1}^n \\xi_i$ 形成对比。\n- 数据集包含 $7$ 个点：\n    - 正类 ($y_i = +1$): $x_1 = (2, 0)$，$x_2 = (3, 0)$，$x_3 = (-4, 0)$。\n    - 负类 ($y_i = -1$): $x_4 = (-3, 0)$，$x_5 = (-2.5, 0)$，$x_6 = (-2, 0)$，$x_7 = (-1.5, 0)$。\n- 给出两个候选超平面进行评估：\n    - 超平面 $H_1$: $w_1 = (1, 0)$，$b_1 = 0$。\n    - 超平面 $H_2$: $w_2 = (1, 0)$，$b_2 = 2$。\n- 两个超平面具有相同的权重范数 $\\|w_1\\| = \\|w_2\\| = \\sqrt{1^2 + 0^2} = 1$，因此具有相同的间隔 $\\gamma = 1$。\n- 在二次惩罚目标下，$H_1$ 和 $H_2$ 之间的比较简化为比较它们各自的松弛变量平方和 $\\sum_{i=1}^7 \\xi_i^2$。问题还要求在线性惩罚 $\\sum_{i=1}^7 \\xi_i$ 下进行比较。\n\n### 步骤2：使用提取的已知条件进行验证\n问题陈述验证如下：\n- **科学依据充分：** 该问题是统计学习中的一个标准练习，特别是关于支持向量机（SVM）在不同松弛惩罚函数（$L_2$ vs. $L_1$）下的性质。所有定义和概念都是该领域的标准内容。\n- **适定性：** 问题是适定的。它提供了一个具体的数据集和两个特定的超平面进行评估。任务是计算松弛变量和相应的惩罚总和，这是一个确定性的、定义明确的过程。\n- **客观性：** 语言正式、精确，没有歧义或主观性。\n\n问题陈述是完整的、一致的，并且科学上是合理的。它提出了一个有效的、可解的问题。\n\n### 步骤3：推导与分析\n由于两个超平面的间隔相同（$\\gamma=1$），它们之间的优劣完全由总违规项决定。我们需要为每个超平面计算线性松弛变量之和 $\\sum \\xi_i$ 和松弛变量平方和 $\\sum \\xi_i^2$。\n\n每个点 $(x_i, y_i)$ 的松弛变量 $\\xi_i$ 由约束 $y_i(w^\\top x_i + b) \\ge 1 - \\xi_i$ 和 $\\xi_i \\ge 0$ 确定。为了最小化惩罚，我们必须选择满足第一个约束的最小可能非负 $\\xi_i$。这给出：\n$$ \\xi_i = \\max(0, 1 - y_i(w^\\top x_i + b)) $$\n\n#### 超平面 $H_1: w_1 = (1, 0), b_1 = 0$ 的分析\n对于 $H_1$，max函数内的项是 $1 - y_i(w_1^\\top x_i + b_1) = 1 - y_i((1, 0)^\\top(x_{i,1}, x_{i,2}) + 0) = 1 - y_i x_{i,1}$。\n我们为每个点计算松弛变量 $\\xi_{i,1}$：\n- $x_1=(2,0), y_1=+1$: $\\xi_{1,1} = \\max(0, 1 - (+1)(2)) = \\max(0, -1) = 0$。\n- $x_2=(3,0), y_2=+1$: $\\xi_{2,1} = \\max(0, 1 - (+1)(3)) = \\max(0, -2) = 0$。\n- $x_3=(-4,0), y_3=+1$: $\\xi_{3,1} = \\max(0, 1 - (+1)(-4)) = \\max(0, 1+4) = 5$。\n- $x_4=(-3,0), y_4=-1$: $\\xi_{4,1} = \\max(0, 1 - (-1)(-3)) = \\max(0, 1-3) = 0$。\n- $x_5=(-2.5,0), y_5=-1$: $\\xi_{5,1} = \\max(0, 1 - (-1)(-2.5)) = \\max(0, 1-2.5) = 0$。\n- $x_6=(-2,0), y_6=-1$: $\\xi_{6,1} = \\max(0, 1 - (-1)(-2)) = \\max(0, 1-2) = 0$。\n- $x_7=(-1.5,0), y_7=-1$: $\\xi_{7,1} = \\max(0, 1 - (-1)(-1.5)) = \\max(0, 1-1.5) = 0$。\n\n$H_1$ 的松弛变量是 $\\{0, 0, 5, 0, 0, 0, 0\\}$。\n- **线性惩罚:** $\\sum_{i=1}^7 \\xi_{i,1} = 5$。\n- **二次惩罚:** $\\sum_{i=1}^7 \\xi_{i,1}^2 = 5^2 = 25$。\n\n#### 超平面 $H_2: w_2 = (1, 0), b_2 = 2$ 的分析\n对于 $H_2$，该项是 $1 - y_i(w_2^\\top x_i + b_2) = 1 - y_i((1, 0)^\\top(x_{i,1}, x_{i,2}) + 2) = 1 - y_i (x_{i,1} + 2)$。\n我们为每个点计算松弛变量 $\\xi_{i,2}$：\n- $x_1=(2,0), y_1=+1$: $\\xi_{1,2} = \\max(0, 1 - (+1)(2+2)) = \\max(0, -3) = 0$。\n- $x_2=(3,0), y_2=+1$: $\\xi_{2,2} = \\max(0, 1 - (+1)(3+2)) = \\max(0, -4) = 0$。\n- $x_3=(-4,0), y_3=+1$: $\\xi_{3,2} = \\max(0, 1 - (+1)(-4+2)) = \\max(0, 1 - (-2)) = 3$。\n- $x_4=(-3,0), y_4=-1$: $\\xi_{4,2} = \\max(0, 1 - (-1)(-3+2)) = \\max(0, 1 - (-1)(-1)) = \\max(0, 0) = 0$。\n- $x_5=(-2.5,0), y_5=-1$: $\\xi_{5,2} = \\max(0, 1 - (-1)(-2.5+2)) = \\max(0, 1 - (-1)(-0.5)) = \\max(0, 1-0.5) = 0.5$。\n- $x_6=(-2,0), y_6=-1$: $\\xi_{6,2} = \\max(0, 1 - (-1)(-2+2)) = \\max(0, 1 - 0) = 1$。\n- $x_7=(-1.5,0), y_7=-1$: $\\xi_{7,2} = \\max(0, 1 - (-1)(-1.5+2)) = \\max(0, 1 - (-1)(0.5)) = \\max(0, 1+0.5) = 1.5$。\n\n$H_2$ 的松弛变量是 $\\{0, 0, 3, 0, 0.5, 1, 1.5\\}$。\n- **线性惩罚:** $\\sum_{i=1}^7 \\xi_{i,2} = 3 + 0.5 + 1 + 1.5 = 6$。\n- **二次惩罚:** $\\sum_{i=1}^7 \\xi_{i,2}^2 = 3^2 + 0.5^2 + 1^2 + 1.5^2 = 9 + 0.25 + 1 + 2.25 = 12.5$。\n\n#### 比较\n- **二次惩罚 ($\\sum \\xi_i^2$):**\n  - $H_1$ 的成本: $25$。\n  - $H_2$ 的成本: $12.5$。\n  由于 $12.5  25$，在二次惩罚下，$H_2$ 更优。\n\n- **线性惩罚 ($\\sum \\xi_i$):**\n  - $H_1$ 的成本: $5$。\n  - $H_2$ 的成本: $6$。\n  由于 $5  6$，在线性惩罚下，$H_1$ 更优。\n\n核心洞见是，二次（$L_2$）惩罚会重罚大误差。超平面 $H_1$ 对离群点 $x_3$ 的处理很差，产生了一个单一的大松弛变量 $\\xi_3=5$，导致一个非常大的平方惩罚 $25$。超平面 $H_2$ 进行了调整以更好地容纳这个离群点（将其松弛变量减小到 $\\xi_3=3$），但代价是为其他点引入了几个较小的违规。$H_2$ 的松弛变量平方和（$12.5$）远低于 $H_1$ 的，因为平方运算对单一的大值 $5$ 的惩罚，比对一组较小的值 $\\{3, 0.5, 1, 1.5\\}$ 的惩罚更重。\n\n相反，线性（$L_1$）惩罚只是简单地将违规量相加。对于 $H_1$，总违规量是 $5$。对于 $H_2$，总违规量是 $6$。因此，线性惩罚更偏好 $H_1$，因为它正确分类了更多的点，即使它犯了一个非常大的错误。这表明 $L_1$ 惩罚对离群点更具鲁棒性，因为它容忍少数点上的大错误，以换取在大多数点上更好的整体性能，从而导致一个更“稀疏”的违规集。\n\n### 逐项分析选项\n\n**A. 在二次惩罚下，$H_1$ 更优，因为平方后，一个大的违规比几个小的违规代价更小；在线性惩罚下，$H_2$ 会更优。**\n- “在二次惩罚下，$H_1$ 更优”的说法是**不正确**的。我们的计算表明 $H_2$ 更优（$12.5  25$）。\n- “平方后，一个大的违规比几个小的违规代价更小”的推理从根本上是**不正确**的。平方使得大值的代价不成比例地更高。\n- “在线性惩罚下，$H_2$ 会更优”的说法是**不正确**的。我们的计算表明 $H_1$ 更优（$5  6$）。\n\n**B. 在二次惩罚下，$H_2$ 更优，因为将违规量分散到几个较小的松弛变量上可以减少 $\\sum \\xi_i^2$，即使线性和 $\\sum \\xi_i$ 增加；在线性惩罚下，$H_1$ 会更优。**\n- “在二次惩罚下，$H_2$ 更优”的说法是**正确**的（$12.5  25$）。\n- “将违规量分散到几个较小的松弛变量上可以减少 $\\sum \\xi_i^2$”的推理是**正确**的。这是 $L_2$ 惩罚的关键特性。我们的计算证实了这一点：$\\sum \\xi_{i,2}^2 = 12.5$ 小于 $\\xi_{3,1}^2=25$。“即使线性和 $\\sum \\xi_i$ 增加”的条件也满足（$6 > 5$）。\n- “在线性惩罚下，$H_1$ 会更优”的说法是**正确**的（$5  6$）。\n- 这个选项正确地指出了在两种惩罚方案下的首选超平面，并提供了正确的概念性推理。\n\n**C. 在二次惩罚下，$H_1$ 和 $H_2$ 是等价的，因为它们有相同的间隔和相同的松弛变量平方和；线性惩罚也得出等价的结论。**\n- “它们有……相同的松弛变量平方和”的说法是**不正确**的（$25 \\neq 12.5$）。\n- “线性惩罚也得出等价的结论”的说法是**不正确**的（$5 \\neq 6$）。\n\n**D. 在没有核函数或对偶变量的情况下，无法确定 $H_1$ 和 $H_2$ 之间的优劣；间隔相等导致无法得出任何结论。**\n- 这个说法是**不正确**的。问题是完全确定的，可以通过在原始公式中直接计算来解决。间隔相等简化了问题，允许直接比较松弛惩罚项。核函数和对偶变量是用于解决SVM优化问题的工具，而不是用于评估预定义的候选解。\n\n基于详细分析，选项B是唯一完全正确的陈述。", "answer": "$$\\boxed{B}$$", "id": "3147193"}, {"introduction": "为了真正掌握支持向量机，我们必须理解其对偶形式的力量，它将视角从直接寻找分离超平面转向为每个数据点寻找最优权重。这个练习 [@problem_id:3147179] 通过一个简单而深刻的场景，为我们提供了一个观察对偶世界的窗口：当我们复制一个支持向量时会发生什么？这个思想实验揭示了对偶变量 ($\\alpha_i$)、Karush-Kuhn-Tucker (KKT) 条件以及支持向量定义之间的内在联系。", "problem": "考虑在硬间隔支持向量机 (SVM) 设置中使用线性核的二元分类。您将分析复制一个支持向量对对偶变量和间隔向量的影响。\n\n一个二维数据集包含一个正例和一个负例：\n- 正例：$x_{+} = (1, 0)$，标签为 $y_{+} = +1$。\n- 负例：$x_{-} = (-1, 0)$，标签为 $y_{-} = -1$。\n\n假设数据是可分的。硬间隔 SVM 的原问题是最小化 $\\frac{1}{2}\\|w\\|^{2}$，约束条件为对所有 $i$ 都有 $y_{i}(w^{\\top} x_{i} + b) \\geq 1$。相应的对偶问题是最大化\n$$\n\\sum_{i=1}^{n} \\alpha_{i} - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_{i} \\alpha_{j} y_{i} y_{j} K(x_{i}, x_{j})\n$$\n约束条件为对所有 $i$ 都有 $\\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0$ 和 $\\alpha_{i} \\geq 0$，其中 $K(x, x') = x^{\\top} x'$ 是线性核。Karush–Kuhn–Tucker (KKT) 条件适用。\n\n任务：\n1. 仅使用上述基本定义，为包含两个点 $x_{+}$ 和 $x_{-}$ 的原始数据集确定最优分离超平面参数 $(w, b)$。论证在最优解处哪些约束是激活的。\n2. 表达原始数据集的最优对偶变量 $\\alpha_{+}$ 和 $\\alpha_{-}$，并验证它们满足对偶可行性条件和关于 $w$ 的平稳性条件。\n3. 现在通过复制正支持向量来构建一个新数据集：将单个正例替换为位于同一位置 $x_{+} = (1, 0)$ 的 $k$ 个相同副本，所有副本的标签均为 $y = +1$，同时保持单个负例不变。使用对偶公式以及对称性/凸性论证，确定在最优解处，这 $k$ 个相同正点的对偶变量如何分布，以及间隔向量 $w$ 是否改变。\n4. 最后，设 $k = 5$，并计算在最优解处分配给 $k$ 个相同正支持向量中每一个的对偶变量的值。以一个精确数值的形式给出你的最终答案。不需要四舍五入。", "solution": "我们从带有线性核的硬间隔支持向量机 (SVM) 的原问题和对偶问题定义开始，并应用 Karush–Kuhn–Tucker (KKT) 条件。\n\n步骤 1：求解原始数据集的原问题。\n\n数据集包含两个点：\n- $x_{+} = (1, 0)$，标签为 $y_{+} = +1$，\n- $x_{-} = (-1, 0)$，标签为 $y_{-} = -1$。\n\n原问题是\n$$\n\\min_{w, b} \\ \\frac{1}{2} \\|w\\|^{2} \\quad \\text{subject to} \\quad y_{i} (w^{\\top} x_{i} + b) \\geq 1 \\ \\text{for all } i.\n$$\n明确写出约束条件：\n- 对于正例点：$y_{+} (w^{\\top} x_{+} + b) = (+1)(w_{1} \\cdot 1 + w_{2} \\cdot 0 + b) = w_{1} + b \\geq 1$。\n- 对于负例点：$y_{-} (w^{\\top} x_{-} + b) = (-1)(-w_{1} + 0 + b) = w_{1} - b \\geq 1$。\n\n在最大间隔解中，支持向量满足等式（根据互补松弛性），因此我们设\n$$\nw_{1} + b = 1, \\quad w_{1} - b = 1.\n$$\n将两个等式相加得到 $2 w_{1} = 2$，因此 $w_{1} = 1$。相减得到 $2 b = 0$，因此 $b = 0$。根据对称性和约束条件，$w_{2} = 0$ 在满足可行性的前提下最小化了范数。因此，\n$$\nw = (1, 0), \\quad b = 0.\n$$\n\n步骤 2：求解原始数据集的对偶变量。\n\n对偶变量 $\\alpha_{+}$ 和 $\\alpha_{-}$ 必须满足：\n- 对偶可行性：$\\alpha_{+} \\geq 0$, $\\alpha_{-} \\geq 0$，\n- 等式约束：$\\alpha_{+} y_{+} + \\alpha_{-} y_{-} = 0 \\implies \\alpha_{+} - \\alpha_{-} = 0$，\n- 关于 $w$ 的平稳性：$w = \\sum_{i} \\alpha_{i} y_{i} x_{i}$。\n\n计算平稳性条件：\n$$\nw = \\alpha_{+} \\cdot (+1) \\cdot (1, 0) + \\alpha_{-} \\cdot (-1) \\cdot (-1, 0) = (\\alpha_{+} + \\alpha_{-}, 0).\n$$\n我们已经求得 $w = (1, 0)$，所以\n$$\n\\alpha_{+} + \\alpha_{-} = 1.\n$$\n结合 $\\alpha_{+} - \\alpha_{-} = 0$，可得\n$$\n\\alpha_{+} = \\alpha_{-} = \\frac{1}{2}.\n$$\n这些值满足对偶可行性和 KKT 条件。互补松弛性也得到满足，因为两个约束都是激活的，并且两个 $\\alpha$ 都严格为正。\n\n步骤 3：复制正支持向量并分析对偶问题。\n\n创建一个新数据集，其中有 $k$ 个位于 $x_{+} = (1, 0)$ 的相同正例，每个标签为 $y = +1$，以及一个位于 $x_{-} = (-1, 0)$ 的负例，标签为 $y = -1$。设这 $k$ 个重复正例的对偶变量为 $\\alpha_{+,1}, \\dots, \\alpha_{+,k}$，负例的对偶变量为 $\\alpha_{-}$。\n\n对偶约束条件是：\n- 对于 $j \\in \\{1, \\dots, k\\}$ 有 $\\alpha_{+,j} \\geq 0$，以及 $\\alpha_{-} \\geq 0$，\n- 等式约束：$\\sum_{j=1}^{k} \\alpha_{+,j} \\cdot (+1) + \\alpha_{-} \\cdot (-1) = 0$，即\n$$\n\\sum_{j=1}^{k} \\alpha_{+,j} = \\alpha_{-}.\n$$\n关于 $w$ 的平稳性：\n$$\nw = \\sum_{j=1}^{k} \\alpha_{+,j} \\cdot (+1) \\cdot (1, 0) + \\alpha_{-} \\cdot (-1) \\cdot (-1, 0) = \\left( \\sum_{j=1}^{k} \\alpha_{+,j} + \\alpha_{-}, \\ 0 \\right).\n$$\n因此第一个分量是 $w_{1} = \\sum_{j=1}^{k} \\alpha_{+,j} + \\alpha_{-}$。使用等式约束 $\\sum_{j=1}^{k} \\alpha_{+,j} = \\alpha_{-}$，我们得到\n$$\nw_{1} = 2 \\sum_{j=1}^{k} \\alpha_{+,j}.\n$$\n\n对于这些点的几何结构，最优原超平面保持不变：两个位置 $(\\pm 1, 0)$ 的垂直平分线仍然是最大间隔分离器，因此最优的 $w$ 和 $b$ 仍然是 $w = (1, 0)$ 和 $b = 0$。因此，\n$$\n1 = w_{1} = 2 \\sum_{j=1}^{k} \\alpha_{+,j} \\quad \\Rightarrow \\quad \\sum_{j=1}^{k} \\alpha_{+,j} = \\frac{1}{2}.\n$$\n因此，根据等式约束，$\\alpha_{-} = \\frac{1}{2}$，和之前一样。\n\n为了确定 $\\alpha_{+,j}$ 在重复点中的分布，考虑对偶目标函数\n$$\n\\mathcal{L}(\\alpha) = \\sum_{j=1}^{k} \\alpha_{+,j} + \\alpha_{-} - \\frac{1}{2} \\left[ \\sum_{j=1}^{k} \\sum_{\\ell=1}^{k} \\alpha_{+,j} \\alpha_{+, \\ell} y_{+} y_{+} K(x_{+}, x_{+}) + 2 \\sum_{j=1}^{k} \\alpha_{+,j} \\alpha_{-} y_{+} y_{-} K(x_{+}, x_{-}) + \\alpha_{-}^{2} y_{-}^{2} K(x_{-}, x_{-}) \\right].\n$$\n由于所有重复的正例在标签和特征向量上都是相同的，因此正例对应的 Gram 矩阵块具有恒定的项：\n- $K(x_{+}, x_{+}) = x_{+}^{\\top} x_{+} = 1$，\n- $K(x_{+}, x_{-}) = x_{+}^{\\top} x_{-} = -1$，\n- $K(x_{-}, x_{-}) = x_{-}^{\\top} x_{-} = 1$，\n并且 $y_{+} y_{+} = 1$, $y_{+} y_{-} = -1$, $y_{-}^{2} = 1$。\n\n线性项 $\\sum_{j=1}^{k} \\alpha_{+,j} + \\alpha_{-}$ 仅依赖于 $\\sum_{j=1}^{k} \\alpha_{+,j}$，而不依赖于单个分量。关于重复正例的二次项包含 $\\sum_{j=1}^{k} \\sum_{\\ell=1}^{k} \\alpha_{+,j} \\alpha_{+,\\ell}$，对于固定的和 $S = \\sum_{j=1}^{k} \\alpha_{+,j}$，根据凸性和对称性，当所有 $\\alpha_{+,j}$ 相等时，该项被最小化：\n$$\n\\sum_{j=1}^{k} \\sum_{\\ell=1}^{k} \\alpha_{+,j} \\alpha_{+,\\ell} = \\left( \\sum_{j=1}^{k} \\alpha_{+,j} \\right)^{2} = S^{2},\n$$\n并且，在和为固定值 $S$ 的非负向量中，当所有 $\\alpha_{+,j}$ 相等时，欧几里得范数 $\\sum_{j=1}^{k} \\alpha_{+,j}^{2}$ 最小。因此，在等式约束下，最大化的选择是平均分配权重：\n$$\n\\alpha_{+,1} = \\alpha_{+,2} = \\cdots = \\alpha_{+,k} = \\frac{S}{k}.\n$$\n我们已经确定 $S = \\frac{1}{2}$，因此\n$$\n\\alpha_{+,j} = \\frac{1}{2k} \\quad \\text{for all } j \\in \\{1, \\dots, k\\}, \\quad \\alpha_{-} = \\frac{1}{2}.\n$$\n间隔向量 $w$ 保持不变，因为\n$$\nw = \\sum_{j=1}^{k} \\alpha_{+,j} y_{+} x_{+} + \\alpha_{-} y_{-} x_{-} = \\left( \\sum_{j=1}^{k} \\alpha_{+,j} + \\alpha_{-}, \\ 0 \\right) = \\left( \\frac{1}{2} + \\frac{1}{2}, \\ 0 \\right) = (1, 0).\n$$\n\n步骤 4：计算 $k=5$ 时的请求值。\n\n设 $k = 5$。那么\n$$\n\\alpha_{+,j} = \\frac{1}{2k} = \\frac{1}{10} \\quad \\text{对于 5 个相同的正支持向量中的每一个}.\n$$\n这是一个精确的有理数值，不需要四舍五入。", "answer": "$$\\boxed{\\frac{1}{10}}$$", "id": "3147179"}]}