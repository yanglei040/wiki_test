{"hands_on_practices": [{"introduction": "在层次聚类中，选择相异性度量是基础性的一步。不同的度量标准捕捉了不同概念上的相似性；例如，欧几里得距离对向量的大小和位置敏感，而余弦距离则纯粹关注方向。通过这项练习 [@problem_id:3129024]，您将直接比较这两种基本度量，并观察它们如何导致截然不同的聚类结果，从而培养为您自己的数据选择合适度量的直觉。", "problem": "给定两种定义实向量空间中观测值（表示为向量）之间相异性的方法：欧几里得距离和余弦距离。在层次聚类中，树状图是一个有根树，它是根据链接规则（在本问题中为平均链接）通过连续合并簇而得到的。树状图会导出叶节点之间的共表型距离，定义为两个叶节点在树中首次合并的高度。树状图对原始相异性的拟合优度通常通过原始成对相异性与共表型距离之间的皮尔逊积矩相关系数（PPMCC）来评估。此外，对于同一叶节点集上的两个树状图，它们之间的 Robinson–Foulds (RF) 距离根据其簇分裂来定义，并量化了它们的结构差异。\n\n从以下基本概念出发：\n- 度量空间是一个偶对 $(\\mathcal{X}, d)$，其中 $d$ 是一个满足非负性、不可辨识者同一性、对称性和三角不等式的距离函数。\n- 层次凝聚聚类过程通过从单例簇开始，根据链接规则迭代地合并具有最小相异性的两个簇，从而构建一个有根二叉树。在平均链接中，两个簇之间的相异性是两个簇中元素之间成对距离的平均值。\n- 两个叶节点之间的共表型距离是它们在树状图中首次合并的高度。共表型相关系数是原始成对距离向量与树状图导出的共表型距离向量之间的 PPMCC，两者都按相同的压缩顺序排列。\n- 同一叶节点集上两个有根树之间的 Robinson–Foulds 距离是它们的非平凡簇分裂（所有内部节点的叶节点集，不包括单例和全集）集合之间的对称差的大小。\n\n任务：\n1. 对于下述每个测试用例，使用具有平均链接的层次凝聚聚类构建两个树状图：一个基于欧几里得距离，另一个基于余弦距离。\n2. 计算每个树状图相对于其原始相异性向量（欧几里得树状图对应欧几里得距离，余弦树状图对应余弦距离）的共表型相关系数。\n3. 计算在相同叶节点集上构建的欧几里得树状图和余弦树状图之间的 Robinson–Foulds 距离。\n4. 对于每个测试用例，返回一个包含三个值的列表：欧几里得共表型相关系数（浮点数）、余弦共表型相关系数（浮点数）和 Robinson–Foulds 距离（整数）。\n\n角度单位说明：凡是出现角度，单位均为度。在生成向量时（如果需要），您的程序必须在内部将给定的角度视为度，尽管实际计算中可能会转换为弧度。\n\n测试套件（每个测试用例是 $\\mathbb{R}^2$ 中的一组点）：\n- 测试用例 A（方向上分离的簇）：\n  - 射线 $R_1$ 上的点：$[1, 0]$, $[2, 0]$, $[3, 0]$。\n  - 射线 $R_2$ 上的点：$[\\tfrac{1}{2}, \\tfrac{\\sqrt{3}}{2}]$, $[1, \\sqrt{3}]$, $[\\tfrac{3}{2}, \\tfrac{3\\sqrt{3}}{2}]$。\n- 测试用例 B（特征缩放失真）：\n  - 取测试用例 A 中的所有点，并将每个点的第一个坐标乘以 $10$。\n- 测试用例 C（具有大尺度差异的近共线射线）：\n  - 设 $\\theta_A = 44^\\circ$ 且 $\\theta_B = 46^\\circ$。\n  - 定义单位方向向量 $u_A = [\\cos(\\theta_A), \\sin(\\theta_A)]$ 和 $u_B = [\\cos(\\theta_B), \\sin(\\theta_B)]$。\n  - 点：$1 \\cdot u_A$, $2 \\cdot u_A$, $4 \\cdot u_A$, $10 \\cdot u_B$, $11 \\cdot u_B$, $12 \\cdot u_B$。\n\n输出规范：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对应于每个测试用例的元素本身必须是一个形式为 $[c_{\\text{eucl}}, c_{\\text{cos}}, r]$ 的列表，其中 $c_{\\text{eucl}}$ 是浮点数形式的欧几里得共表型相关系数，$c_{\\text{cos}}$ 是浮点数形式的余弦共表型相关系数，而 $r$ 是整数形式的 Robinson–Foulds 距离。例如，一个包含三个测试用例的输出应如下所示：$[[c_1^{\\text{eucl}}, c_1^{\\text{cos}}, r_1],[c_2^{\\text{eucl}}, c_2^{\\text{cos}}, r_2],[c_3^{\\text{eucl}}, c_3^{\\text{cos}}, r_3]]$。", "solution": "用户提供的问题已经过验证，并被确定为一个有效且适定的科学问题。其定义、数据和任务清晰、自洽，并基于统计学习和数值计算的既定原则。所有必要信息均已提供。\n\n### 基于原则的解决方案\n\n目标是比较层次凝聚聚类在三个不同数据集上应用两种不同相异性度量（欧几里得距离和余弦距离）所产生的结果。该比较使用两种定量指标进行：共表型相关系数（CPCC），用于衡量树状图对原始相异性的保真度；以及 Robinson-Foulds (RF) 距离，用于衡量两个树状图之间的拓扑差异。\n\n对于每个测试用例，解决方案系统地按以下步骤进行：\n\n1.  **计算相异性矩阵**：\n    对于每个测试用例，我们从 $\\mathbb{R}^2$中的一组 $N$ 个点开始，表示为一个 $N \\times 2$ 的矩阵 $X$。计算两个独立的成对相异性矩阵。\n    -   **欧几里得距离**：这是两个向量之差的标准 $L_2$ 范数。对于点 $p_i = (p_{i1}, p_{i2})$ 和 $p_j = (p_{j1}, p_{j2})$，欧几里得距离为 $d_E(p_i, p_j) = \\sqrt{(p_{i1} - p_{j1})^2 + (p_{i2} - p_{j2})^2}$。该度量对向量在空间中的尺度和绝对位置都很敏感。\n    -   **余弦距离**：该度量衡量两个向量之间的夹角余弦，使其对其尺度不敏感（即，用一个正常数缩放向量不会改变它们的余弦距离）。它定义为 $d_C(p_i, p_j) = 1 - \\frac{p_i \\cdot p_j}{\\|p_i\\| \\|p_j\\|}$，其中 $\\|p\\|$ 表示 $p$ 的欧几里得范数。余弦距离为 $0$ 意味着向量指向同一方向，而距离为 $1$ 意味着它们是正交的。\n\n    这些相异性矩阵以压缩的一维数组格式计算，正如 SciPy 等科学计算库所要求的那样。此格式存储了对称相异性矩阵的上三角部分。\n\n2.  **层次凝聚聚类**：\n    使用计算出的相异性矩阵，我们执行层次聚类。选择的算法是凝聚型的，采用“平均”链接标准。该过程首先将 $N$ 个点中的每一个都视为其自身的簇。在随后的每一步中，具有最小相异性的两个簇被合并。对于平均链接，两个簇 $C_a$ 和 $C_b$ 之间的相异性定义为两个簇中点之间所有成对相异性的平均值：\n    $$d(C_a, C_b) = \\frac{1}{|C_a||C_b|} \\sum_{p_i \\in C_a, p_j \\in C_b} d(p_i, p_j)$$\n    这个过程重复 $N-1$ 次，直到所有点都包含在一个单一的簇中。结果是一个树状图，即一个表示簇的嵌套合并的树形结构。聚类算法的输出是一个链接矩阵，记为 $Z$，它编码了合并的序列以及每次合并发生时的相异性（高度）。我们生成两个这样的矩阵：$Z_{\\text{Eucl}}$（来自欧几里得距离）和 $Z_{\\text{cos}}$（来自余弦距离）。\n\n3.  **共表型相关系数（CPCC）计算**：\n    CPCC 评估树状图在多大程度上保留了数据点的原始成对相异性。首先，我们从树状图计算共表型距离矩阵。两点 $p_i$ 和 $p_j$ 之间的共表型距离 $d_{\\text{coph}}(p_i, p_j)$ 是它们在树状图中首次合并到同一个簇时的高度。这个高度对应于被合并的两个子簇之间的链接距离。\n    然后，CPCC 作为原始相异性矩阵的元素与对应的共表型距离矩阵的元素之间的皮尔逊积矩相关系数来计算。设 $d$ 为原始相异性向量，$d_{\\text{coph}}$ 为共表型距离向量，两者均为压缩形式。CPCC，即 $c$，为：\n    $$c = \\frac{\\sum_{i", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import linkage, cophenet\n\ndef get_splits(Z: np.ndarray) - set:\n    \"\"\"\n    Computes the set of nontrivial splits from a linkage matrix.\n\n    A split is the set of leaf indices in a cluster formed at an internal node.\n    Nontrivial splits exclude singletons and the full set of leaves.\n\n    Args:\n        Z: The linkage matrix from scipy.cluster.hierarchy.linkage.\n\n    Returns:\n        A set of frozensets, where each frozenset contains the integer indices\n        of the leaves in a nontrivial split.\n    \"\"\"\n    N = Z.shape[0] + 1\n    # The clusters dictionary maps a node index to the set of leaf indices under it.\n    # Leaf nodes (0 to N-1) are initialized with their own index.\n    # Internal nodes (N to 2N-2) are added during the iteration.\n    clusters = {i: frozenset({i}) for i in range(N)}\n    splits = set()\n    \n    # Each row in Z represents an internal node and a merge operation.\n    for i in range(N - 1):\n        # Indices of the two clusters being merged.\n        c1_idx = int(Z[i, 0])\n        c2_idx = int(Z[i, 1])\n        \n        # Retrieve the leaf sets for the clusters being merged.\n        leaves1 = clusters[c1_idx]\n        leaves2 = clusters[c2_idx]\n        \n        # Form the new cluster by taking the union of the leaf sets.\n        new_cluster_leaves = leaves1.union(leaves2)\n        \n        # The new cluster corresponds to a new internal node.\n        # Its index is N + i.\n        new_cluster_idx = N + i\n        clusters[new_cluster_idx] = new_cluster_leaves\n        \n        # A split is nontrivial if it is not a singleton (guaranteed by merge)\n        # and not the full set of all leaves.\n        if len(new_cluster_leaves)  N:\n            splits.add(new_cluster_leaves)\n            \n    return splits\n\ndef solve_case(points: np.ndarray) - list:\n    \"\"\"\n    Performs clustering and evaluation for a single test case.\n\n    Args:\n        points: A NumPy array of shape (N, D) representing the data points.\n\n    Returns:\n        A list containing [c_eucl, c_cos, rf_dist].\n    \"\"\"\n    # Euclidean-based analysis\n    dist_eucl = pdist(points, 'euclidean')\n    Z_eucl = linkage(dist_eucl, method='average')\n    c_eucl, _ = cophenet(Z_eucl, dist_eucl)\n    splits_eucl = get_splits(Z_eucl)\n    \n    # Cosine-based analysis\n    dist_cos = pdist(points, 'cosine')\n    # Handle potential NaN from pdist if a zero-vector is present. This is not\n    # expected for the given test cases but is good practice.\n    if np.any(np.isnan(dist_cos)):\n        dist_cos = np.nan_to_num(dist_cos)\n    Z_cos = linkage(dist_cos, method='average')\n    c_cos, _ = cophenet(Z_cos, dist_cos)\n    splits_cos = get_splits(Z_cos)\n    \n    # Robinson-Foulds distance calculation\n    rf_dist = len(splits_eucl.symmetric_difference(splits_cos))\n    \n    return [c_eucl, c_cos, rf_dist]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the analysis, and print the results.\n    \"\"\"\n    # Test Case A: Directionally separated clusters\n    case_a = np.array([\n        [1.0, 0.0], [2.0, 0.0], [3.0, 0.0],\n        [0.5, np.sqrt(3)/2], [1.0, np.sqrt(3)], [1.5, 3*np.sqrt(3)/2]\n    ])\n\n    # Test Case B: Feature scaling distortion\n    case_b = case_a.copy()\n    case_b[:, 0] *= 10\n\n    # Test Case C: Nearly collinear rays with large magnitude differences\n    theta_A_deg = 44.0\n    theta_B_deg = 46.0\n    theta_A_rad = np.deg2rad(theta_A_deg)\n    theta_B_rad = np.deg2rad(theta_B_deg)\n    u_A = np.array([np.cos(theta_A_rad), np.sin(theta_A_rad)])\n    u_B = np.array([np.cos(theta_B_rad), np.sin(theta_B_rad)])\n    case_c = np.array([\n        1.0 * u_A, 2.0 * u_A, 4.0 * u_A,\n        10.0 * u_B, 11.0 * u_B, 12.0 * u_B\n    ])\n\n    test_cases = [case_a, case_b, case_c]\n    \n    results = []\n    for case in test_cases:\n        result = solve_case(case)\n        results.append(result)\n\n    # Format the output string to match the specified format without extra spaces.\n    case_strings = []\n    for res in results:\n        # Format each list [float, float, int] into a string \"[f,f,i]\"\n        case_strings.append(f\"[{res[0]:.10f},{res[1]:.10f},{res[2]}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3129024"}, {"introduction": "选择了距离度量后，特征的尺度会极大地影响结果，特别是对于像 Ward 联结法这样基于方差的方法。数值范围大的特征可能会主导距离计算，掩盖其他特征中的结构，而标准化则将所有特征置于同等地位。这项练习 [@problem_id:3129004] 提供了一种量化方法来分析特征缩放的影响，帮助您不仅理解其重要性，还能精确地了解其影响方式以及是哪些特征在驱动聚类结构的变化。", "problem": "给定一组数据矩阵，要求您研究特征缩放对使用 Ward's 最小方差准则的凝聚式层次聚类产生的影响。本研究的基础是簇内离散度的定义以及因合并两个簇而引起的簇内平方和增量。设 $X \\in \\mathbb{R}^{n \\times d}$ 表示一个包含 $n$ 个样本和 $d$ 个特征的数据矩阵。将簇 $C \\subset \\{1,\\dots,n\\}$ 在特征 $j$ 上的均值定义为 $\\mu_{C,j} = \\frac{1}{|C|}\\sum_{i \\in C} x_{ij}$。对于两个不相交的簇 $A$ 和 $B$，Ward 合并高度（即因合并 $A$ 和 $B$ 而导致的簇内平方和的增量）为\n$$\n\\Delta(A,B;X) = \\frac{|A|\\,|B|}{|A|+|B|}\\sum_{j=1}^d \\left( \\mu_{A,j} - \\mu_{B,j} \\right)^2.\n$$\n特征标准化按列定义，即减去特征均值并除以总体标准差。对于特征 $j$，设 $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^n x_{ij}$ 且 $\\sigma_j = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_{ij}-\\bar{x}_j)^2}$。标准化数据矩阵 $Z \\in \\mathbb{R}^{n \\times d}$ 定义为：当 $\\sigma_j  0$ 时，$z_{ij} = \\frac{x_{ij}-\\bar{x}_j}{\\sigma_j}$；当 $\\sigma_j = 0$ 时，对所有 $i$ 都有 $z_{ij} = 0$（常数特征列无法被重新缩放至单位方差，在中心化后被设为零）。\n\n任务：\n- 对于每个给定的测试矩阵 $X$，对原始矩阵 $X$ 使用 Ward's 方法进行凝聚式层次聚类，得到合并序列。使用此合并序列，为每个合并步骤 $t$ 计算原始合并高度 $\\Delta_t^{\\text{raw}} = \\Delta(A_t,B_t;X)$ 和在同一对簇上计算的标准化合并高度 $\\Delta_t^{\\text{std}} = \\Delta(A_t,B_t;Z)$，其中 $Z$ 是 $X$ 的标准化版本（使用上述规则），而 $(A_t,B_t)$ 是根据原始数据 Ward's 过程在步骤 $t$ 合并的簇。\n- 通过计算标量来量化因标准化引起的合并高度变化\n$$\nS_{\\text{total}} = \\sum_{t=1}^{n-1} \\left| \\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}} \\right|.\n$$\n- 识别驱动这些变化的变量。对于每个合并步骤 $t$，将高度差异分解为各特征的贡献：\n$$\nc_{j}(t) = \\frac{|A_t|\\,|B_t|}{|A_t|+|B_t|}\\left[\\left(\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}}\\right)^2 - \\left(\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}}\\right)^2\\right],\n$$\n其中 $\\mu_{C,j}^{\\text{raw}}$ 和 $\\mu_{C,j}^{\\text{std}}$ 分别是在 $X$ 和 $Z$ 上计算的簇均值。对每个特征 $j$，聚合其在所有合并步骤中的绝对贡献：\n$$\nS_j = \\sum_{t=1}^{n-1} \\left| c_j(t) \\right|.\n$$\n将驱动变量集合定义为使 $S_j$ 达到最大值的索引 $j$（包括所有并列情况），以零为基准的索引并按升序报告。\n\n您的程序必须为以下数据矩阵测试套件实现上述计算。每个矩阵都明确指定。请确保数值的合理性和内部一致性，并使用以下确切值。\n\n测试套件：\n- 测试 1 ($n=6$, $d=3$)：\n$$\nX^{(1)} = \\begin{bmatrix}\n100  0  0.10 \\\\\n102  0.50  0.20 \\\\\n98  -0.20  0.00 \\\\\n0  5  -0.10 \\\\\n3  4.50  -0.30 \\\\\n-2  5.20  0.00\n\\end{bmatrix}.\n$$\n- 测试 2 ($n=5$, $d=3$)，第一列为常数：\n$$\nX^{(2)} = \\begin{bmatrix}\n10  -1  50 \\\\\n10  0  51 \\\\\n10  1  49 \\\\\n10  2  48 \\\\\n10  -2  52\n\\end{bmatrix}.\n$$\n- 测试 3 ($n=8$, $d=4$)，具有不同尺度：\n$$\nX^{(3)} = \\begin{bmatrix}\n100.0  0.0  0.10  10000.0 \\\\\n101.5  -0.5  0.05  10002.0 \\\\\n98.5  0.3  0.15  9998.0 \\\\\n100.8  -0.2  0.12  10001.0 \\\\\n0.0  5.0  -0.10  10050.0 \\\\\n-1.5  4.5  -0.05  10052.0 \\\\\n2.0  5.2  -0.12  10049.0 \\\\\n-0.8  4.8  -0.08  10051.5\n\\end{bmatrix}.\n$$\n- 测试 4 ($n=3$, $d=2$)，最小案例：\n$$\nX^{(4)} = \\begin{bmatrix}\n0  0 \\\\\n10  0 \\\\\n0  10\n\\end{bmatrix}.\n$$\n\n输出规格：\n- 对于每个测试矩阵 $X^{(k)}$，计算 $S_{\\text{total}}^{(k)}$ 和驱动变量索引集 $J^{(k)} = \\{ j : S_j \\text{ is maximal} \\}$。\n- 将每个 $S_{\\text{total}}^{(k)}$ 四舍五入到 $6$ 位小数。\n- 您的程序应生成单行输出，其中包含结果，格式为一个由方括号括起来的逗号分隔列表，每个测试用例的结果本身是一个形式为 $[S_{\\text{total}}^{(k)},[j_1,j_2,\\dots]]$ 的列表，其中特征索引以零为基准并按升序排列。例如：$[[s_1,[j_{1,1},j_{1,2}]], [s_2,[j_{2,1}]], [s_3,[j_{3,1},j_{3,2},j_{3,3}]], [s_4,[j_{4,1}]]]$。输出字符串中不允许出现任何空格。\n\n约束与说明：\n- 在 $\\mathbb{R}^d$ 中使用欧几里得几何以及完全基于簇内平方和的 Ward's 最小方差准则。合并序列必须通过对原始数据矩阵 $X$ 应用 Ward's 方法获得，并且标准化的合并高度必须在相同的合并序列上进行评估。\n- 所有计算均为纯数值计算，不涉及物理单位。角度不适用。\n- 确保对 $\\sigma_j = 0$ 的常数特征列进行稳健处理；根据上述定义，这些列在中心化后变为零，并且对标准化的差异没有贡献。", "solution": "用户提供的问题经评估为**有效**。这是一个定义明确且自成体系的统计学习计算任务，基于层次聚类和数据标准化的既定原则。问题陈述没有科学上的不健全、模糊或矛盾之处。\n\n### 基于原则的解决方案设计\n\n该问题要求分析在使用 Ward's 方法的凝聚式层次聚类中，特征标准化如何影响合并高度。解决方案围绕以下核心原则和计算步骤构建。\n\n1.  **特征标准化**：第一步是将原始数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 转换为标准化矩阵 $Z \\in \\mathbb{R}^{n \\times d}$。这是许多机器学习算法中一个常见的预处理步骤，以确保具有较大尺度的特征不会主导分析。对每个特征 $j$ 的指定转换为 $z_{ij} = (x_{ij} - \\bar{x}_j) / \\sigma_j$，其中 $\\bar{x}_j$ 是特征均值，$\\sigma_j$ 是总体标准差。此过程将每个特征中心化使其均值为 $0$，并缩放使其标准差为 $1$。一个关键细节是处理 $\\sigma_j = 0$ 的常数特征。对于此类特征，问题规定标准化的值 $z_{ij}$ 对所有样本 $i$ 都设为 $0$。这是合乎逻辑的，因为常数特征不提供区分样本的信息，在中心化（减去其自身均值）后，它会变成一个全零列。\n\n2.  **Ward's 层次聚类**：聚类过程在**原始数据矩阵** $X$ 上执行。Ward's 最小方差法是凝聚式聚类的一种形式。它从每个数据点作为其自身的簇开始，在每一步中，合并那对导致总簇内平方和（ESS）增量最小的簇。总 ESS 是每个点与其所属簇的质心之间平方距离的总和。合并两个簇 $A$ 和 $B$ 时 ESS 的增量由 Ward 合并高度的公式给出：\n    $$\n    \\Delta(A,B;X) = \\frac{|A|\\,|B|}{|A|+|B|}\\sum_{j=1}^d \\left( \\mu_{A,j} - \\mu_{B,j} \\right)^2\n    $$\n    其中 $|C|$ 是簇 $C$ 的大小，$\\mu_{C,j}$ 是簇 $C$ 在特征 $j$ 上的均值。这个公式表示合并簇的质心之间的平方欧几里得距离，并由一个取决于它们大小的因子加权。应用此方法会产生一个包含 $n-1$ 次合并的序列 $(A_t, B_t)_{t=1}^{n-1}$，这定义了层次结构。\n\n3.  **合并高度的比较分析**：分析的核心是比较在原始数据 $X$ 与标准化数据 $Z$ 上计算的合并高度。关键在于，合并序列是根据原始数据 $X$ 一次性确定的。对于这个固定序列中的每个合并步骤 $t$，计算两个量：\n    -   原始合并高度，$\\Delta_t^{\\text{raw}} = \\Delta(A_t, B_t; X)$。\n    -   标准化合并高度，$\\Delta_t^{\\text{std}} = \\Delta(A_t, B_t; Z)$。这使用相同的公式计算，但簇均值是根据标准化数据 $Z$ 得出的。\n\n4.  **变化的量化**：标准化的效果通过两个聚合指标来量化：\n    -   $S_{\\text{total}} = \\sum_{t=1}^{n-1} | \\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}} |$：这个标量衡量了整个聚类过程中合并高度的总绝对变化。\n    -   $S_j = \\sum_{t=1}^{n-1} | c_j(t) |$：这个指标按特征分解了变化。项 $c_j(t)$ 分离出特征 $j$ 对合并步骤 $t$ 高度差异的贡献。对所有合并步骤的绝对贡献求和，给出了特征 $j$ 对高度差异总影响的度量。$S_j$ 值大的特征，其缩放会显著改变簇间距离，因此被识别为变化的“驱动变量”。驱动变量集定义为对应于 $S_j$ 最大值的特征索引。\n\n### 算法实现\n\n通过对每个提供的测试矩阵执行以下步骤来实现解决方案：\n\n1.  读取大小为 $n \\times d$ 的输入矩阵 $X$。\n2.  计算标准化矩阵 $Z$。对每一列 $j$，计算均值 $\\bar{x}_j$ 和总体标准差 $\\sigma_j$。如果 $\\sigma_j  0$，按规定计算 $z_{ij}$。如果 $\\sigma_j = 0$，将 $Z$ 的第 $j$ 列设为零。\n3.  使用 `scipy.cluster.hierarchy.linkage` 并在 `method='ward'` 的设置下对 $X$ 执行 Ward's 层次聚类。这将返回一个 $(n-1) \\times 4$ 的链接矩阵 $L$，它编码了合并序列。\n4.  初始化 $S_{\\text{total}} = 0$ 和一个长度为 $d$ 的零向量 $S_j$。\n5.  遍历链接矩阵 $L$ 中描述的每个合并 $t=1, \\dots, n-1$：\n    a. 识别在此步骤中被合并的两个簇，$A_t$ 和 $B_t$。这是通过从链接矩阵中递归地将簇索引追溯到原始数据点来完成的。\n    b. 计算大小 $|A_t|$ 和 $|B_t|$ 以及系数 $w_t = \\frac{|A_t|\\,|B_t|}{|A_t|+|B_t|}$。\n    c. 计算原始数据（$\\mu_{A_t}^{\\text{raw}}, \\mu_{B_t}^{\\text{raw}}$）和标准化数据（$\\mu_{A_t}^{\\text{std}}, \\mu_{B_t}^{\\text{std}}$）的两个簇的质心向量。\n    d. 计算原始和标准化的合并高度：\n       $\\Delta_t^{\\text{raw}} = w_t \\sum_{j=1}^d (\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}})^2$\n       $\\Delta_t^{\\text{std}} = w_t \\sum_{j=1}^d (\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}})^2$\n    e. 更新总变化：$S_{\\text{total}} \\leftarrow S_{\\text{total}} + |\\Delta_t^{\\text{std}} - \\Delta_t^{\\text{raw}}|$。\n    f. 计算各特征对高度差异的贡献：$c_j(t) = w_t [(\\mu_{A_t,j}^{\\text{std}} - \\mu_{B_t,j}^{\\text{std}})^2 - (\\mu_{A_t,j}^{\\text{raw}} - \\mu_{B_t,j}^{\\text{raw}})^2]$。\n    g. 更新各特征的聚合变化：对所有 $j=1, \\dots, d$，有 $S_j \\leftarrow S_j + |c_j(t)|$。\n6.  遍历所有合并后，找到 $S_j$ 向量中的最大值。\n7.  识别所有满足 $S_j$ 等于此最大值的特征索引 $j$（使用一个小的容差进行浮点数比较）。这些是驱动变量。\n8.  按规定格式化最终结果，包括四舍五入后的 $S_{\\text{total}}$ 和排序后的驱动变量索引列表。\n\n这个系统化的过程确保所有量都根据其定义进行计算，从而得出正确且可验证的最终答案。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        np.array([\n            [100, 0, 0.10],\n            [102, 0.50, 0.20],\n            [98, -0.20, 0.00],\n            [0, 5, -0.10],\n            [3, 4.50, -0.30],\n            [-2, 5.20, 0.00]\n        ], dtype=float),\n        np.array([\n            [10, -1, 50],\n            [10, 0, 51],\n            [10, 1, 49],\n            [10, 2, 48],\n            [10, -2, 52]\n        ], dtype=float),\n        np.array([\n            [100.0, 0.0, 0.10, 10000.0],\n            [101.5, -0.5, 0.05, 10002.0],\n            [98.5, 0.3, 0.15, 9998.0],\n            [100.8, -0.2, 0.12, 10001.0],\n            [0.0, 5.0, -0.10, 10050.0],\n            [-1.5, 4.5, -0.05, 10052.0],\n            [2.0, 5.2, -0.12, 10049.0],\n            [-0.8, 4.8, -0.08, 10051.5]\n        ], dtype=float),\n        np.array([\n            [0, 0],\n            [10, 0],\n            [0, 10]\n        ], dtype=float)\n    ]\n\n    results = [_solve_one_case(X) for X in test_cases]\n    \n    # Format the output string to remove spaces\n    print(str(results).replace(\" \", \"\"))\n\ndef _solve_one_case(X):\n    \"\"\"\n    Solves the problem for a single data matrix X.\n    \"\"\"\n    n, d = X.shape\n\n    # 1. Standardize data matrix X to get Z\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)  # Population std dev (ddof=0 is default)\n    \n    # Use np.divide for safe division, setting output to 0 where std is 0\n    Z = np.divide(X - means, stds, out=np.zeros_like(X, dtype=float), where=stds!=0)\n\n    # 2. Perform Ward's clustering on raw data X\n    L = linkage(X, method='ward', metric='euclidean')\n\n    # 3. Initialize collectors for S_total and S_j\n    S_total = 0.0\n    S_j = np.zeros(d, dtype=float)\n    \n    # Cache for cluster point indices\n    # clusters[i] stores the list of original point indices for cluster i\n    clusters = {i: [i] for i in range(n)}\n    \n    # 4. Iterate through the n-1 merges from the linkage matrix\n    for t in range(n - 1):\n        # Identify merged clusters by their indices\n        c1_idx, c2_idx = int(L[t, 0]), int(L[t, 1])\n        \n        # Retrieve the original data point indices for each cluster\n        A_points = clusters[c1_idx]\n        B_points = clusters[c2_idx]\n        \n        # Create the new merged cluster and add it to the cache\n        new_cluster_idx = n + t\n        clusters[new_cluster_idx] = A_points + B_points\n        \n        nA, nB = len(A_points), len(B_points)\n        coeff = (nA * nB) / (nA + nB)\n        \n        # Calculate cluster means (centroids) for raw and standardized data\n        mu_A_raw = np.mean(X[A_points, :], axis=0)\n        mu_B_raw = np.mean(X[B_points, :], axis=0)\n        \n        mu_A_std = np.mean(Z[A_points, :], axis=0)\n        mu_B_std = np.mean(Z[B_points, :], axis=0)\n        \n        # Calculate merge heights (increase in ESS)\n        delta_raw = coeff * np.sum((mu_A_raw - mu_B_raw)**2)\n        delta_std = coeff * np.sum((mu_A_std - mu_B_std)**2)\n        \n        # Accumulate total change\n        S_total += np.abs(delta_std - delta_raw)\n        \n        # Calculate feature-wise contributions to the height difference\n        diff_sq_std = (mu_A_std - mu_B_std)**2\n        diff_sq_raw = (mu_A_raw - mu_B_raw)**2\n        c_t = coeff * (diff_sq_std - diff_sq_raw)\n        \n        # Accumulate absolute feature-wise contributions\n        S_j += np.abs(c_t)\n        \n    # 5. Identify driving variables (indices of max S_j)\n    max_Sj = np.max(S_j)\n    # Use a tolerance for floating-point comparison\n    driving_vars = np.where(np.isclose(S_j, max_Sj))[0].tolist()\n    \n    return [round(S_total, 6), driving_vars]\n\nsolve()\n```", "id": "3129004"}, {"introduction": "聚类分析中的一个关键挑战是确定数据中“正确”的簇数量。轮廓系数提供了一种度量，用于评估每个数据点与其所属簇的契合度以及与相邻簇的分离度。通过这项练习 [@problem_id:3129027]，您将学会使用轮廓系数作为评估不同聚类划分的有力工具，并就从树状图中提取的最佳簇数 $k$ 做出数据驱动的决策。", "problem": "给定欧几里得平面中的有限点集，您需要执行使用平均链接的凝聚式层次聚类，然后通过轮廓系数在多个切割水平上评估聚类质量。您的任务是实现一个程序，对于每个指定的数据集和给定的最大簇数，计算在不同切割水平上的平均轮廓系数，并选择使该平均值最大化的簇数。\n\n需要使用的基本基础和定义：\n- 欧几里得距离：对于点 $x_{i} \\in \\mathbb{R}^{2}$ 和 $x_{j} \\in \\mathbb{R}^{2}$，定义距离 $d(i,j) = \\lVert x_{i} - x_{j} \\rVert_{2}$。\n- 使用平均链接的凝聚式层次聚类：从单例簇开始，重复合并其成员之间平均成对距离最小的一对簇，直到只剩下一个簇。这个过程会生成一个二叉树状图。\n- 切割树状图：对于整数 $k \\geq 2$，通过在树状图中选择一个能产生 $k$ 个连通分量的水平切割层，可以获得一个恰好产生 $k$ 个簇的“切割”。使用直接强制生成恰好 $k$ 个簇的准则。\n- 点 $i$ 的轮廓系数：对于点集 $\\{x_{1},\\dots,x_{n}\\}$ 上的一个划分为不相交非空簇 $\\{C_{1},\\dots,C_{k}\\}$ 的聚类，令 $C(i)$ 表示包含点 $i$ 的簇。定义\n  - 簇内不相似性 $a(i)$ 定义为 $d(i,j)$ 在所有 $j \\in C(i)$ 且 $j \\neq i$ 上的平均值。如果 $\\lvert C(i) \\rvert = 1$，则设 $a(i) = 0$。\n  - 对于任何其他簇 $C' \\neq C(i)$，定义从 $i$到 $C'$ 的跨簇平均不相似性为 $d(i,j)$ 在所有 $j \\in C'$ 上的平均值。令 $b(i)$ 为这些平均值在所有 $C' \\neq C(i)$ 上的最小值。\n  - 点 $i$ 的轮廓系数为\n    $$ s(i) = \\begin{cases}\n    0,  \\text{if } \\lvert C(i) \\rvert = 1, \\\\\n    \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{otherwise, with the convention } s(i)=0 \\text{ if } a(i)=b(i)=0.\n    \\end{cases} $$\n- 切割水平 $k$ 上的平均轮廓系数：$\\bar{s}(k) = \\dfrac{1}{n} \\sum_{i=1}^{n} s(i)$，其中 $n$ 是点的数量。\n- 选择规则：对于每个数据集，对 $\\{2,3,\\dots, \\min\\{k_{\\max}, n\\}\\}$ 中的所有整数 $k$ 计算 $\\bar{s}(k)$。令 $k^{\\star}$ 为任何达到最大平均轮廓系数的 $k$。如果出现绝对容差 $\\varepsilon = 10^{-12}$ 内的平局，则选择其中最小的 $k$。\n\n需嵌入程序中的输入（不允许外部输入）：\n- 您必须使用欧几里得距离和平均链接进行层次聚类，并在每次切割时产生恰好 $k$ 个簇。\n- 包含三个数据集的测试套件，每个数据集由一个平面坐标列表和一个参数 $k_{\\max}$ 指定：\n  - 数据集 A（具有良好分离的两簇结构），包含 $n = 12$ 个点和 $k_{\\max} = 5$：\n    - 点：\n      $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$, $(0.5,0)$, $(0,0.5)$,\n      $(5,5)$, $(5,6)$, $(6,5)$, $(6,6)$, $(5.5,5)$, $(5,5.5)$.\n  - 数据集 B（具有良好分离的三簇结构），包含 $n = 15$ 个点和 $k_{\\max} = 6$：\n    - 点：\n      $(-6,0)$, $(-6,0.2)$, $(-6,-0.2)$, $(-5.8,0)$, $(-6.2,0)$,\n      $(0,0)$, $(0,0.2)$, $(0,-0.2)$, $(0.2,0)$, $(-0.2,0)$,\n      $(6,0)$, $(6,0.2)$, $(6,-0.2)$, $(5.8,0)$, $(6.2,0)$.\n  - 数据集 C（退化的相同点），包含 $n = 5$ 个点和 $k_{\\max} = 4$：\n    - 点：\n      $(10,-3)$, $(10,-3)$, $(10,-3)$, $(10,-3)$, $(10,-3)$.\n\n程序要求：\n- 对于每个数据集，使用上述定义为 $\\{2,\\dots,\\min\\{k_{\\max}, n\\}\\}$ 中的所有整数 $k$ 计算 $\\bar{s}(k)$，然后根据容差为 $\\varepsilon = 10^{-12}$ 的平局打破规则选择 $k^{\\star}$。\n- 您的程序应生成单行输出，其中包含每个数据集选择的 $k^{\\star}$，按 A、B、C 的顺序，以逗号分隔的列表形式并用方括号括起来。例如，一个有效的输出格式如 $[2,3,2]$。\n- 答案值为整数。\n\n注意：不涉及物理单位或角度。所有用于打破平局的数值比较都必须使用指定的绝对容差 $\\varepsilon = 10^{-12}$。请确保您的实现与上述定义一致。", "solution": "用户提供的问题是有效的。它在科学上基于统计学习的既定原则，特别是凝聚式层次聚类和使用轮廓系数进行聚类验证。所有术语，如欧几里得距离 ($d(i,j)$)、平均链接和轮廓分数 ($s(i)$)，都经过了数学上的明确定义。该问题是自包含的，提供了所有必要的数据和约束，包括数据集、聚类参数、要评估的簇数范围 ($k$)，以及精确的平局打破规则。该计算任务是可行的，并为每个数据集导出一个唯一的、可验证的解决方案。\n\n该解决方案的实现遵循了针对每个所提供数据集的结构化、多步骤过程。\n\n**步骤 1：凝聚式层次聚类**\n\n对于每个数据集，我们首先计算所有点之间的成对距离。问题指定了欧几里得距离，$d(i,j) = \\lVert x_{i} - x_{j} \\rVert_{2}$。这些距离被组织成一个压缩距离矩阵。\n\n接着，我们使用平均链接准则执行凝聚式层次聚类。该方法从将每个数据点视为一个单例簇开始。然后，它迭代地合并两个簇 $C_A$ 和 $C_B$，这两个簇具有最小的平均成对距离，定义如下：\n$$\nD(C_A, C_B) = \\frac{1}{|C_A| |C_B|} \\sum_{i \\in C_A} \\sum_{j \\in C_B} d(i,j)\n$$\n这个过程一直持续到所有点都包含在一个单独的簇中，从而产生一个称为树状图的二叉树结构。整个聚类过程对每个数据集执行一次。使用 `scipy.spatial.distance.pdist` 函数进行距离计算，并使用带有 `method='average'` 的 `scipy.cluster.hierarchy.linkage` 函数创建树状图。\n\n**步骤 2：树状图切割与簇分配**\n\n对于每个具有 $n$ 个点和给定最大簇数 $k_{\\max}$ 的数据集，我们评估 $k \\in \\{2, 3, \\dots, \\min\\{k_{\\max}, n\\}\\}$ 的聚类配置。对于此范围内的每个整数 $k$，我们“切割”树状图，将数据划分为恰好 $k$ 个簇。这是通过识别层次结构中的前 $n-k$ 次合并，并将点分配到产生的 $k$ 个分支来实现的。为此，我们使用带有 `criterion='maxclust'` 选项的 `scipy.cluster.hierarchy.fcluster` 函数。这为每个 $k$ 值生成了所有 $n$ 个点的簇标签集。\n\n**步骤 3：轮廓分数计算**\n\n对于每个划分为 $k$ 个簇的分区，我们计算平均轮廓分数 $\\bar{s}(k)$ 以评估其质量。单个点 $i$ 的轮廓分数，记作 $s(i)$，衡量了它与其分配的簇相比于相邻簇的拟合程度。其计算方式如下：\n\n1.  **簇内不相似性, $a(i)$**：这是点 $i$ 到其所在簇 $C(i)$ 中所有其他点 $j$ 的平均距离。\n    $$ a(i) = \\frac{1}{|C(i)| - 1} \\sum_{j \\in C(i), j \\neq i} d(i,j) $$\n    根据问题定义，如果点 $i$ 位于一个单例簇中 ($|C(i)| = 1$)，则 $a(i) = 0$。\n\n2.  **簇间不相似性, $b(i)$**：对于每个其他簇 $C'$ ($C' \\neq C(i)$)，我们计算点 $i$ 到 $C'$ 中所有点的平均距离。$b(i)$ 是这些值在所有其他簇上的最小值。\n    $$ b(i) = \\min_{C' \\neq C(i)} \\left\\{ \\frac{1}{|C'|} \\sum_{j \\in C'} d(i,j) \\right\\} $$\n\n3.  **轮廓系数 $s(i)$**：点 $i$ 的轮廓系数由以下公式给出：\n    $$ s(i) = \\begin{cases} 0,  \\text{if } |C(i)| = 1 \\\\ \\dfrac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{otherwise} \\end{cases} $$\n    如果 $a(i) = b(i) = 0$，则使用一个特殊约定，即 $s(i)$ 定义为 $0$。这种情况与包含相同点的数据集（如数据集 C）相关。\n\n对于 $k$-簇划分的平均轮廓分数 $\\bar{s}(k)$，是所有 $n$ 个数据点上 $s(i)$ 的平均值：\n$$ \\bar{s}(k) = \\frac{1}{n} \\sum_{i=1}^{n} s(i) $$\n\n**步骤 4：最优簇数选择**\n\n在计算完所有有效 $k$ 的 $\\bar{s}(k)$ 后，我们选择最优簇数 $k^{\\star}$。选择规则要求找到使平均轮廓分数最大化的 $k$。为处理可能的平局，问题指定了一个两步规则：\n1.  首先，确定达到的最大分数 $S_{\\max} = \\max_{k} \\{\\bar{s}(k)\\}$。\n2.  然后，识别所有分数在绝对容差 $\\varepsilon = 10^{-12}$ 内与 $S_{\\max}$ 相等的 $k$ 值集合。如果 $|\\bar{s}(k') - S_{\\max}| \\leq \\varepsilon$，则分数 $\\bar{s}(k')$ 被视为平局。\n3.  从这个“并列最大”的 $k$ 值集合中，选择最小的一个。\n\n这个过程确保了每个数据集都有一个唯一的、确定性的 $k^{\\star}$。该逻辑的实现方式是：首先计算所有分数，然后找到最大值，最后按 $k$ 的递增顺序遍历分数，以找到第一个落在最大值容差范围内的分数。\n\n对于数据集 A（两个良好分离的组），预计在 $k=2$ 时轮廓分数最高。对于数据集 B（三个良好分离的组），预计 $k=3$ 是最优的。对于数据集 C（所有点都相同），所有距离都为 $0$，导致对于所有点和所有聚类，$a(i) = 0$ 且 $b(i) = 0$。因此，对于所有 $k$，$s(i)=0$ 且 $\\bar{s}(k)=0$。平局打破规则将选择测试的最小 $k$，即 $2$。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.hierarchy import linkage, fcluster\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_average_silhouette(dist_matrix, labels, n):\n    \"\"\"\n    Calculates the average silhouette score for a given clustering.\n    Follows the definitions specified in the problem statement.\n    \"\"\"\n    if n == 0:\n        return 0.0\n\n    unique_labels, label_indices, label_counts = np.unique(labels, return_inverse=True, return_counts=True)\n    num_clusters = len(unique_labels)\n\n    if num_clusters = 1:\n        return 0.0\n\n    silhouettes = np.zeros(n)\n    \n    for i in range(n):\n        my_label_idx = label_indices[i]\n        my_label = unique_labels[my_label_idx]\n        my_cluster_size = label_counts[my_label_idx]\n\n        # Per problem definition, s(i) = 0 for singleton clusters.\n        if my_cluster_size == 1:\n            silhouettes[i] = 0.0\n            continue\n        \n        # Calculate a(i): mean distance to other points in the same cluster.\n        in_cluster_mask = (labels == my_label)\n        in_cluster_mask[i] = False\n        a_i = np.sum(dist_matrix[i, in_cluster_mask]) / (my_cluster_size - 1)\n\n        # Calculate b(i): min mean distance to points in any other cluster.\n        b_i = np.inf\n        for j, other_label in enumerate(unique_labels):\n            if j == my_label_idx:\n                continue\n            \n            other_cluster_mask = (labels == other_label)\n            mean_dist = np.mean(dist_matrix[i, other_cluster_mask])\n            b_i = min(b_i, mean_dist)\n\n        # Calculate s(i).\n        denominator = max(a_i, b_i)\n        if denominator == 0:\n            # Handles the convention where s(i)=0 if a(i)=b(i)=0.\n            silhouettes[i] = 0.0\n        else:\n            silhouettes[i] = (b_i - a_i) / denominator\n            \n    return np.mean(silhouettes)\n\ndef find_optimal_k(points, k_max):\n    \"\"\"\n    Performs hierarchical clustering and finds the optimal k based on silhouette score.\n    \"\"\"\n    n = len(points)\n    epsilon = 1e-12\n\n    if n = 1:\n        # According to problem constraints (k=2), n must be at least 2.\n        # This case should not be reached.\n        return None\n\n    # Step 1: Perform hierarchical clustering with average linkage.\n    condensed_dist_matrix = pdist(points, 'euclidean')\n    linkage_matrix = linkage(condensed_dist_matrix, method='average')\n    \n    # We need the full n x n distance matrix for silhouette calculations.\n    dist_matrix = squareform(condensed_dist_matrix)\n\n    # Step 2: Evaluate average silhouette for each k.\n    silhouette_scores = {}\n    k_range = range(2, min(k_max, n) + 1)\n    \n    if not k_range:\n        return None\n\n    for k in k_range:\n        labels = fcluster(linkage_matrix, t=k, criterion='maxclust')\n        avg_silhouette = calculate_average_silhouette(dist_matrix, labels, n)\n        silhouette_scores[k] = avg_silhouette\n\n    # Step 3: Select the optimal k using the specified tie-breaking rule.\n    if not silhouette_scores:\n        return None\n        \n    # Find the true maximum score\n    max_score = max(silhouette_scores.values())\n\n    # Find the smallest k that is tied for the maximum score\n    best_k = -1\n    for k in sorted(silhouette_scores.keys()):\n        score = silhouette_scores[k]\n        if abs(score - max_score) = epsilon:\n            best_k = k\n            break # Found the smallest k in the tie-set, so we can stop.\n    \n    return best_k\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"points\": np.array([\n                (0,0), (0,1), (1,0), (1,1), (0.5,0), (0,0.5),\n                (5,5), (5,6), (6,5), (6,6), (5.5,5), (5,5.5)\n            ]),\n            \"k_max\": 5\n        },\n        # Dataset B\n        {\n            \"points\": np.array([\n                (-6,0), (-6,0.2), (-6,-0.2), (-5.8,0), (-6.2,0),\n                (0,0), (0,0.2), (0,-0.2), (0.2,0), (-0.2,0),\n                (6,0), (6,0.2), (6,-0.2), (5.8,0), (6.2,0)\n            ]),\n            \"k_max\": 6\n        },\n        # Dataset C\n        {\n            \"points\": np.array([\n                (10,-3), (10,-3), (10,-3), (10,-3), (10,-3)\n            ]),\n            \"k_max\": 4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k_star = find_optimal_k(case[\"points\"], case[\"k_max\"])\n        results.append(k_star)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3129027"}]}