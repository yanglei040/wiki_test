{"hands_on_practices": [{"introduction": "视觉检查“肘部点”主观性强，因此将这一启发式方法转化为自动化算法至关重要。本练习将指导您实现一个基于连续 WCSS 下降率之比 $R(k)$ 的定量规则。通过在不同分离度的簇上测试该算法，您将学习如何将启发式规则转化为具体代码，并理解簇的可分离性如何影响 WCSS 曲线的形状。[@problem_id:3107505]", "problem": "您的任务是设计并实现一个算法，通过分析簇内平方和（WCSS）来选择数据集中的簇数。对于给定的簇数 $k$，数据矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 的 WCSS 定义为每个点到其所属簇的质心的平方欧几里得距离之和。该度量源于 $k$-means 目标函数，该函数将 $d$ 维空间中的 $n$ 个点划分到 $k$ 个簇中，以最小化到簇质心的平方距离之和。众所周知，随着簇数 $k$ 的增加，目标值 $W(k)$ 是非递增的，因为每个额外的簇都允许对数据进行更精细的划分。\n\n您的算法必须从 $k$-means 聚类的定义出发，使用多次随机初始化来计算整数 $k \\in \\{1,2,\\dots,K_{\\max}\\}$ 的 WCSS 函数 $W(k)$，以降低对初始化的敏感性，然后评估比率\n$$\nR(k) \\equiv \\frac{W(k-1) - W(k)}{W(k) - W(k+1)}\n$$\n对于整数 $k \\in \\{2,3,\\dots,K_{\\max}-1\\}$。选择规则如下：\n- 选择满足 $R(k) \\ge \\gamma$ 的最小整数 $k$，其中 $\\gamma > 0$ 是一个给定的阈值。\n- 如果没有 $k$ 满足该不等式，则选择 $\\{2,\\dots,K_{\\max}-1\\}$ 中使 $R(k)$ 最大化的整数 $k$；如果存在平局，则选择平局中最小的 $k$。\n\n您必须在分离度递增的合成高斯混合数据上验证该算法。数据生成方式如下：\n- 设混合成分的真实数量为 $m_{\\text{true}} = 3$，维度为 $d = 2$，每个簇的样本量为 $n_c$（因此总样本量 $n = 3 n_c$）。\n- 对于给定的分离参数 $s \\ge 0$，定义簇均值为\n$$\n\\mu_1 = (-s, 0), \\quad \\mu_2 = (s, 0), \\quad \\mu_3 = (0, s),\n$$\n并为每个簇从均值为 $\\mu_j$、协方差矩阵为 $\\sigma^2 I_2$ 的多元正态分布中独立抽取 $n_c$ 个点，其中 $\\sigma = 1$，$I_2$ 是 $2 \\times 2$ 的单位矩阵。\n- 合并所有簇的样本以形成数据集 $X$。\n\n实现要求：\n- 使用标准的 Lloyd 算法进行 $k$-means 计算 $W(k)$，并通过 $r$ 次随机重启来减轻局部最小值的影响。对于每个 $k$，返回 $r$ 次运行中最小的 WCSS。\n- 如果在更新过程中某个簇变为空，则将其质心重新初始化为一个随机数据点。\n- 通过使用固定的伪随机数生成器种子来确保可复现性。\n- 在计算 $R(k)$ 时，如果分母 $W(k) - W(k+1)$ 在数值上为非正数或低于一个小的容差（例如，小于 $10^{-12}$），则将 $R(k)$ 视为 $+\\infty$，以反映 WCSS 已经达到一个平台期，增加簇数不会再使其减小。\n\n测试套件：\n实现您的算法，为以下参数集生成输出，每个参数集代表一种不同的分离度和阈值情况：\n- 情况1：$n_c = 200$, $s = 4.0$, $\\gamma = 1.3$, $K_{\\max} = 8$, $r = 10$。\n- 情况2：$n_c = 200$, $s = 2.5$, $\\gamma = 1.2$, $K_{\\max} = 8$, $r = 10$。\n- 情况3：$n_c = 200$, $s = 1.0$, $\\gamma = 1.2$, $K_{\\max} = 8$, $r = 10$。\n- 情况4：$n_c = 200$, $s = 0.0$, $\\gamma = 1.2$, $K_{\\max} = 8$, $r = 10$。\n\n您的程序必须为每种情况生成合成数据，运行上述选择算法，并以整数形式输出每种情况选择的 $k$ 值。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四种情况所选整数的逗号分隔列表，并用方括号括起来，例如 `[k_1,k_2,k_3,k_4]`。不涉及物理单位。不使用角度。所有数值输出均表示为纯整数，不带附加文本。", "solution": "该问题要求设计并实现一个算法，来确定给定数据集的最佳簇数 $k$。这是无监督学习中一个被称为模型选择的基本问题。指定的方法是一种基于簇内平方和（WCSS）行为的启发式方法，WCSS 是评估 $k$-means 算法产生的聚类质量的常用指标。\n\n首先，我们将问题的各个组成部分形式化。\n\n**$k$-means 聚类与簇内平方和 (WCSS)**\n\n给定一个数据集 $X = \\{x_1, x_2, \\dots, x_n\\}$，其中每个数据点 $x_i \\in \\mathbb{R}^d$，$k$-means 算法旨在将这 $n$ 个点划分到 $k$ 个不相交的集合或簇 $C_1, C_2, \\dots, C_k$ 中，以最小化 WCSS。WCSS，记为 $W(k)$，是每个点与其所属簇的质心之间的平方欧几里得距离之和。其数学定义为：\n$$\nW(k) = \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\|x_i - \\mu_j\\|^2\n$$\n其中 $\\mu_j = \\frac{1}{|C_j|} \\sum_{x_i \\in C_j} x_i$ 是簇 $C_j$ 中点的质心（均值）。\n\n最小化此目标的标准迭代过程是 Lloyd 算法：\n1.  **初始化**：选择 $k$ 个初始质心，通常是随机选择 $k$ 个数据点。\n2.  **分配步骤**：将每个数据点 $x_i$ 分配到与其最近的质心 $\\mu_j$ 相对应的簇 $C_j$ 中。即，$j = \\arg\\min_{l \\in \\{1,\\dots,k\\}} \\|x_i - \\mu_l\\|^2$。\n3.  **更新步骤**：将每个质心 $\\mu_j$ 重新计算为分配给簇 $C_j$ 的所有数据点的均值。\n重复这两个步骤，直到簇分配不再改变。在每次迭代中，$W(k)$ 的值保证是非递增的。然而，该算法可能会收敛到目标函数的局部最小值。为缓解此问题，通常会使用不同的随机初始化多次运行整个过程（$r$ 次重启），并选择产生最低 $W(k)$ 的解。\n\n随着簇数 $k$ 的增加，WCSS $W(k)$ 是一个非递增函数。在 $k=n$ 的极端情况下，每个点自成一簇，WCSS 变为 $0$。$W(k)$ 与 $k$ 的关系图通常显示出初始急剧下降，然后趋于平缓，形成一个类似“肘部”的形状。最佳簇数通常凭经验确定为这个“肘点”，在该点之后，增加更多的簇在 WCSS 减小方面的回报递减。\n\n**选择启发式方法**\n\n问题定义了一个具体的量化规则来定位这个肘点。它涉及计算 $k \\in \\{2, 3, \\dots, K_{\\max}-1\\}$ 的比率 $R(k)$：\n$$\nR(k) \\equiv \\frac{W(k-1) - W(k)}{W(k) - W(k+1)}\n$$\n分子 $W(k-1) - W(k)$ 表示将簇数从 $k-1$ 增加到 $k$ 时 WCSS 的减少量。分母 $W(k) - W(k+1)$ 表示随后从 $k$ 增加到 $k+1$ 时的减少量。$R(k)$ 的值很大，表明增加第 $k$ 个簇带来的好处远大于增加第 $(k+1)$ 个簇带来的好处。这正是“肘部”条件：WCSS 相对于 $k$ 的曲线在点 $k$ 之前比在点 $k$ 之后陡峭得多。\n\n选择规则定义如下：\n1.  选择满足 $R(k) \\ge \\gamma$ 的最小整数 $k \\in \\{2, \\dots, K_{\\max}-1\\}$，其中 $\\gamma > 0$ 是一个指定的阈值。\n2.  如果不存在这样的 $k$，则选择在 $\\{2, \\dots, K_{\\max}-1\\}$ 范围内使比率 $R(k)$ 最大化的 $k$。如果最大值出现平局，则选择最小的 $k$。\n\n对分母有一种特殊情况处理：如果 $W(k) - W(k+1)$ 在数值上接近于零（小于或等于 $10^{-12}$ 的容差），我们将 $R(k)$ 解释为 $+\\infty$。这正确地捕捉了 WCSS 已经达到平台期的情况，使得 $k$ 成为肘点的一个有力候选。\n\n**算法流程**\n\n对于每个测试用例，实现将遵循以下步骤：\n\n1.  **数据生成**：对于给定的分离度 $s$、每簇样本量 $n_c$ 和维度 $d=2$，生成一个包含 $n=3n_c$ 个点的数据集。数据从三个高斯分布的混合模型中抽取，其均值分别为 $\\mu_1 = (-s, 0)$、$\\mu_2 = (s, 0)$ 和 $\\mu_3 = (0, s)$，并共享一个各向同性的协方差矩阵 $\\sigma^2 I_2$（其中 $\\sigma=1$）。使用固定的随机数生成器种子以确保可复现性。\n\n2.  **WCSS 曲线计算**：对于从 $1$ 到 $K_{\\max}$ 的每个整数 $k$：\n    a. 将变量 `min_wcss` 初始化为无穷大。\n    b. 独立执行 $r$ 次 Lloyd 的 $k$-means 算法。对于每次运行：\n        i.   通过从 $X$ 中随机选择 $k$ 个不同的数据点来初始化 $k$ 个质心。\n        ii.  迭代执行分配和更新步骤，直到收敛（即簇分配稳定）。\n        iii. 一个关键细节是处理空簇。如果在更新步骤中，某个簇没有任何分配的点，则通过从 $X$ 中选择一个新的随机数据点来重新初始化其质心。这确保了活动簇的数量保持为 $k$。\n        iv.  收敛后，计算最终划分的 WCSS。\n        v.   更新 `min_wcss = min(min_wcss, current_wcss)`。\n    c. 将得到的 `min_wcss` 存储为 $W(k)$ 的值。\n\n3.  **最佳 $k$ 值选择**：\n    a. 使用计算出的序列 $W(1), W(2), \\dots, W(K_{\\max})$，根据指定公式计算 $k \\in \\{2, \\dots, K_{\\max}-1\\}$ 的比率 $R(k)$，包括对接近零的分母的特殊处理。\n    b. 从 $k=2$ 迭代到 $K_{\\max}-1$，找到第一个满足 $R(k) \\ge \\gamma$ 的 $k$。如果找到，这个 $k$ 就是结果。\n    c. 如果循环完成而没有找到这样的 $k$，则找到使 $R(k)$ 最大化的 $k$。平局规则（选择最小的 $k$）可以通过查找最大值索引的标准方法自然处理。这个 $k$ 就是结果。\n\n将此完整流程应用于问题陈述中定义的四个参数集中的每一个。", "answer": "```python\nimport numpy as np\n\ndef run_kmeans(X, k, num_restarts, max_iter, rng):\n    \"\"\"\n    Runs the k-means algorithm with multiple restarts and returns the best result.\n    \n    Args:\n        X (np.ndarray): The data matrix, shape (n_samples, n_features).\n        k (int): The number of clusters.\n        num_restarts (int): The number of times to run k-means with different initializations.\n        max_iter (int): The maximum number of iterations for a single run.\n        rng (np.random.Generator): The random number generator for reproducibility.\n        \n    Returns:\n        float: The minimum WCSS found across all restarts.\n    \"\"\"\n    n_samples, n_features = X.shape\n    min_wcss = np.inf\n\n    for _ in range(num_restarts):\n        # Initialize centroids by choosing k random points from X without replacement\n        initial_indices = rng.choice(n_samples, k, replace=False)\n        centroids = X[initial_indices]\n        \n        prev_assignments = np.zeros(n_samples, dtype=int) - 1\n\n        for iteration in range(max_iter):\n            # Assignment step\n            # Calculate squared Euclidean distances from each point to each centroid\n            # Shape: (n_samples, k)\n            dist_sq = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2, axis=2)\n            assignments = np.argmin(dist_sq, axis=1)\n\n            # Check for convergence\n            if np.array_equal(assignments, prev_assignments):\n                break\n            prev_assignments = assignments\n\n            # Update step\n            new_centroids = np.zeros((k, n_features))\n            counts = np.zeros(k, dtype=int)\n            \n            # Efficiently sum points for each cluster\n            np.add.at(new_centroids, assignments, X)\n            counts = np.bincount(assignments, minlength=k)\n            \n            non_empty_mask = counts > 0\n            empty_mask = ~non_empty_mask\n            \n            # Calculate means for non-empty clusters\n            new_centroids[non_empty_mask] /= counts[non_empty_mask, np.newaxis]\n            \n            # Handle empty clusters by re-initializing centroids to random data points\n            num_empty = np.sum(empty_mask)\n            if num_empty > 0:\n                random_points_indices = rng.choice(n_samples, num_empty, replace=True)\n                new_centroids[empty_mask] = X[random_points_indices]\n            \n            centroids = new_centroids\n        \n        # Calculate WCSS for the converged clustering\n        current_wcss = 0.0\n        for j in range(k):\n            cluster_points = X[assignments == j]\n            if len(cluster_points) > 0:\n                current_wcss += np.sum((cluster_points - centroids[j]) ** 2)\n        \n        if current_wcss  min_wcss:\n            min_wcss = current_wcss\n            \n    return min_wcss\n\ndef solve():\n    \"\"\"\n    Main function to run the cluster selection algorithm for all test cases.\n    \"\"\"\n    # Fixed seed for reproducibility\n    rng = np.random.default_rng(seed=12345)\n    max_iter_kmeans = 100 # Set a reasonable max iteration count for Lloyd's\n    den_tolerance = 1e-12\n\n    # (n_c, s, gamma, K_max, r)\n    test_cases = [\n        (200, 4.0, 1.3, 8, 10),\n        (200, 2.5, 1.2, 8, 10),\n        (200, 1.0, 1.2, 8, 10),\n        (200, 0.0, 1.2, 8, 10),\n    ]\n\n    results = []\n    \n    for case_params in test_cases:\n        n_c, s, gamma, K_max, r = case_params\n        \n        # 1. Generate synthetic data\n        n_features = 2\n        means = np.array([[-s, 0], [s, 0], [0, s]])\n        cov = np.identity(n_features) * 1.0  # sigma=1\n        \n        data_parts = []\n        for i in range(3):\n            data_parts.append(rng.multivariate_normal(means[i], cov, size=n_c))\n        X = np.vstack(data_parts)\n        \n        # 2. Compute WCSS curve\n        wcss_values = []\n        for k_val in range(1, K_max + 1):\n            if k_val == 1:\n                # WCSS for k=1 is total sum of squares\n                wcss = np.sum((X - X.mean(axis=0))**2)\n            else:\n                wcss = run_kmeans(X, k_val, r, max_iter_kmeans, rng)\n            wcss_values.append(wcss)\n            \n        # 3. Apply the selection rule\n        k_search_range = range(2, K_max)\n        r_values = {}\n        \n        for k_val in k_search_range:\n            # W(k-1) is at index k-2, W(k) at k-1, W(k+1) at k\n            w_km1 = wcss_values[k_val - 2]\n            w_k = wcss_values[k_val - 1]\n            w_kp1 = wcss_values[k_val]\n            \n            numerator = w_km1 - w_k\n            denominator = w_k - w_kp1\n            \n            if denominator = den_tolerance:\n                r_values[k_val] = np.inf\n            else:\n                r_values[k_val] = numerator / denominator\n\n        # Find smallest k such that R(k) >= gamma\n        selected_k = -1\n        for k_val in sorted(r_values.keys()):\n            if r_values[k_val] >= gamma:\n                selected_k = k_val\n                break\n        \n        # If no such k exists, find k that maximizes R(k)\n        if selected_k == -1:\n            if not r_values: # This can happen if K_max  3\n                 # Problem constraints ensure K_max is at least 8, so r_values is non-empty.\n                 # This branch is for robustness, but not expected to be hit.\n                 # As no k in the search range can be chosen, an error value might be appropriate.\n                 # However, based on the problem, we must choose from the range.\n                 # With K_max=8, the range is [2, 7], so there is always something to choose.\n                pass\n            \n            # Find the k corresponding to the maximum R value.\n            # max() on dict.items() with a key lambda is one way.\n            # np.argmax on a list of values is more direct.\n            k_options = list(r_values.keys())\n            r_option_values = [r_values[k] for k in k_options]\n            \n            # np.argmax returns the first index of the maximum, which handles tie-breaking\n            # (select smallest k) correctly.\n            best_k_idx = np.argmax(r_option_values)\n            selected_k = k_options[best_k_idx]\n\n        results.append(selected_k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3107505"}, {"introduction": "任何启发式方法都有其局限性，“肘部法则”也不例外。本练习旨在探索肘部法则失效的一个经典场景：数据点对称地分布在一个球体（或圆形）上。您将构建这样一个“对抗性”数据集，以理解为何在此情况下肘部不明显，并学习如何通过数据变换打破对称性，从而使肘部重新出现，这有助于加深对 WCSS 几何意义的理解。[@problem_id:3107514]", "problem": "您的任务是评估簇内平方和如何随簇数量的变化而变化，并构建一个对于基于视觉“肘部”的簇数量选择方法具有对抗性的数据集。考虑最小化总簇内平方欧几里得距离的经典划分方法。设数据集为有限点集 $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$。对于给定的整数 $k \\ge 1$，将簇内平方和 $W(k)$ 定义为，在所有将点分配到 $k$ 个簇的方案以及所有 $k$ 个质心的选择中，各点到其所属簇质心的平方距离之和的最小值。即，\n$$\nW(k) \\equiv \\min_{\\text{partitions into }k\\text{ clusters}} \\sum_{c=1}^k \\sum_{x_i \\in C_c} \\|x_i - \\mu_c\\|_2^2,\n$$\n其中 $C_c$ 表示第 $c$ 个簇，$\\mu_c$ 是 $C_c$ 的质心。\n\n您的程序必须实现一种算法，使用标准的 Lloyd 迭代优化（通常称为 k-均值）并配合谨慎的初始化来近似 $W(k)$。然后，您必须通过在球面上放置点来构建一个对抗性数据集，该数据集能在指定的 $k$ 值范围内使 $W(k)$ 曲线平坦化，并且提出并实现对该数据集的确定性变形，以重新引入一个可见的肘部，并解释肘部出现的原因。\n\n在二维设置 $\\mathbb{R}^2$ 中，使用单位圆构建球面。具体来说：\n- 通过在角度 $\\theta_j = \\frac{2\\pi j}{n}$（其中 $j = 0, 1, \\dots, n - 1$，以弧度计）处放置点，并经由 $x_j = (r \\cos \\theta_j, r \\sin \\theta_j)$ 映射到笛卡尔坐标，生成在半径 $r=1$ 的单位圆上均匀分布的 $n$ 个点。\n- 为了近似给定数据集的 $W(k)$，请使用 k-means++ 风格的种子点选择和多次重启来实现 Lloyd 算法。使用平方欧几里得距离 $\\|\\cdot\\|_2^2$。当需要时，通过将质心重新初始化到最远的点来稳健地处理空簇。在给定固定随机种子的情况下，算法必须是确定性的。\n\n定义以下量化标准：\n1. 平坦线检测。对于一个连续整数范围 $\\{k_{\\min}, k_{\\min} + 1, \\dots, k_{\\max}\\}$，计算序列 $W(k)$ 和归一化逐次下降量\n$$\n\\Delta(k) = \\frac{W(k) - W(k+1)}{W(1)} \\quad \\text{for } k \\in \\{k_{\\min}, \\dots, k_{\\max}-1\\}.\n$$\n如果对于给定的阈值 $\\tau \\in (0,1)$，$\\max_{k} \\Delta(k) \\le \\tau$ 成立，则声明曲线“在该范围内平坦化”。\n\n2. 通过离散曲率进行肘部检测。对于整数 $k \\in \\{2, \\dots, K-1\\}$，定义离散二阶差分\n$$\nD(k) = W(k-1) - 2 W(k) + W(k+1).\n$$\n返回使 $D(k)$ 在 $k \\in \\{2, \\dots, K-1\\}$ 上最大化的肘部位置 $k^\\star$。若出现平局，必须通过选择最小的 $k$ 来打破。\n\n构建对抗性数据集（圆上的点）和一个通过在垂直方向上以因子 $s \\in (0,1)$ 进行各向异性缩放获得的变形数据集，即将每个点 $(x, y)$ 映射到 $(x, s y)$，其中 $s = 0.5$。\n\n您的程序必须为下面指定的测试套件生成输出。凡使用到角度之处，均以弧度解释。\n\n测试套件：\n- 测试用例 1（对抗性平坦线检查）：\n    - 数据集：半径 $r=1$ 的单位圆上的 $n=360$ 个点。\n    - 评估所有整数 $k \\in \\{1, 2, \\dots, 12\\}$ 的 $W(k)$。\n    - 平坦线检测范围：$k_{\\min} = 6$, $k_{\\max} = 12$。\n    - 阈值：$\\tau = 0.025$。\n    - 输出：一个布尔值，指示曲线是否在 $\\{6, 7, \\dots, 12\\}$ 范围内根据上述标准平坦化。\n\n- 测试用例 2（变形后的肘部）：\n    - 数据集：取半径 $r=1$ 的相同 $n=360$ 个圆上点，然后以因子 $s=0.5$ 进行垂直缩放变形。\n    - 评估所有整数 $k \\in \\{1, 2, \\dots, 12\\}$ 的 $W(k)$。\n    - 使用离散二阶差分 $D(k)$ 在 $\\{2, 3, \\dots, 11\\}$ 范围内选择肘部 $k^\\star$，如上所述。\n    - 输出：整数 $k^\\star$。\n\n- 测试用例 3（边界范围非平坦性检查）：\n    - 数据集：半径 $r=1$ 的单位圆上的 $n=360$ 个点。\n    - 评估所有整数 $k \\in \\{1, 2, \\dots, 12\\}$ 的 $W(k)$。\n    - 平坦线检测范围：$k_{\\min} = 2$, $k_{\\max} = 5$。\n    - 阈值：$\\tau = 0.03$。\n    - 输出：一个布尔值，指示曲线是否在 $\\{2, 3, 4, 5\\}$ 范围内根据上述标准平坦化。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果（例如，“[result1,result2,result3]”）。这三个结果必须分别按顺序对应于测试用例 1、2 和 3 的输出，并且类型必须为布尔型、整型和布尔型。", "solution": "当前任务要求对簇内平方和（表示为 $W(k)$）作为簇数量 $k$ 的函数进行严谨的检验。此分析旨在通过构建一个使“肘部法则”失效的数据集以及一个使其成功的该数据集的修改版本，来展示这种选择最优 $k$ 值的常用启发式方法的局限性。\n\n### 原理一：簇内平方和与 k-均值聚类\n\n给定一个在 $d$ 维欧几里得空间 $\\mathbb{R}^d$ 中的 $n$ 个点的集合 $\\{x_i\\}_{i=1}^n$。基于划分的聚类的目标是将这些点分组成 $k$ 个不同的、非空的集合或簇 $\\{C_1, C_2, \\dots, C_k\\}$，这些簇共同构成一个完备且互斥的划分。\n\n此任务的标准目标函数是总簇内平方和（$WSS$），它衡量了簇的紧凑程度。对于一个给定的划分，它是每个点到其所属簇质心的平方欧几里得距离的总和。一个簇 $C_c$ 的质心 $\\mu_c$ 是其算术平均值：$\\mu_c = \\frac{1}{|C_c|} \\sum_{x_i \\in C_c} x_i$。\n\n量 $W(k)$ 定义为在所有可能将数据划分为 $k$ 个簇的方案中，可能达到的最小 $WSS$：\n$$\nW(k) \\equiv \\min_{\\{C_c\\}_{c=1}^k} \\sum_{c=1}^k \\sum_{x_i \\in C_c} \\|x_i - \\mu_c\\|_2^2\n$$\n找到该函数的真正全局最小值是一个 NP-难问题。因此，通常采用迭代启发式算法来寻找一个好的局部最小值。其中最常见的是 Lloyd 算法，通常被称为 k-均值算法。\n\n该算法流程如下：\n1.  **初始化**：选择 $k$ 个初始质心。明智的选择对获得好的结果至关重要。k-means++ 种子点选择策略是一种行之有效的方法，它倾向于选择分布良好的初始质心，从而提高了算法的质量和收敛速度。\n2.  **迭代**：重复以下两个步骤直至收敛。\n    a.  **分配步骤**：将每个数据点 $x_i$ 分配给与其最近的质心 $\\mu_c$ 相对应的簇 $C_c$，即最小化 $\\|x_i - \\mu_c\\|_2^2$。\n    b.  **更新步骤**：将每个簇的质心 $\\mu_c$ 重新计算为分配给该簇的所有点的均值。\n3.  **终止**：当簇分配在两次迭代之间不再变化时，算法收敛。\n\n由于该算法对初始条件敏感，标准做法是使用不同的随机初始化（重启）运行多次，并选择产生最小 $W(k)$ 的聚类结果。\n\n### 原理二：“肘部法则”及其对抗性案例\n\n$W(k)$ 是一个关于 $k$ 的单调递减函数。随着 $k$ 的增加，点被划分到更小、更多的簇中，这必然会减少到各自质心的平方距离之和。$W(1)$ 是数据相对于全局均值的总平方和，如果所有点都不同，则 $W(n) = 0$。\n\n肘部法是一种用于估计“优良”$k$ 值的视觉启发式方法。人们绘制 $W(k)$ 相对于 $k$ 的曲线，并寻找一个“肘部”点，在该点 $W(k)$ 的下降速率急剧减缓。其直觉在于，该点代表了模型复杂度（更多的簇）和解释能力（更低的 $WSS$）之间的一种权衡。\n\n该方法的一个**对抗性数据集**是指不会产生清晰肘部的数据集。一个典型的例子是一组具有高度对称性分布的点，例如在球面上均匀分布。在指定的 $\\mathbb{R}^2$ 情况下，我们使用半径 $r=1$ 的单位圆上的 $n=360$ 个点，其坐标为 $(r \\cos \\theta_j, r \\sin \\theta_j)$，角度为 $\\theta_j = \\frac{2\\pi j}{n}$。\n\n该数据集的旋转对称性意味着除了 $k=1$ 或 $k=n$ 之外，没有本质上“正确”的簇数量。$k$ 个质心的最优放置位置将趋向于一个内接于该圆的正 k 边形。随着 $k$ 的增加，$W(k)$ 的减少非常平滑和渐进。不存在某一点，增加一个簇会带来不成比例的巨大好处，因此 $W(k)$ 曲线缺乏明显的肘部。这一点可以通过归一化逐次下降量 $\\Delta(k) = \\frac{W(k) - W(k+1)}{W(1)}$ 来量化。对于圆形数据，该值预计会很小，并且在一定的 $k$ 值范围内变化缓慢，导致曲线根据问题的标准呈现“平坦化”状态。\n\n### 原理三：通过各向异性缩放重新引入结构\n\n要恢复一个显著的肘部，必须打破数据集的对称性。问题提出了一种各向异性缩放变换，将每个点 $(x, y)$ 映射到 $(x, s y)$，缩放因子 $s = 0.5$。这将单位圆变换为一个沿 x 轴的半长轴为 1、沿 y 轴的半短轴为 0.5 的椭圆。\n\n这种变形对点的分布有关键影响。虽然这些点仍然保持其在圆上的顺序，但连续点之间的欧几里得距离发生了变化。在椭圆位于 $(\\pm 1, 0)$ 的高曲率端点附近，点变得密集；而在位于 $(0, \\pm 0.5)$ 的低曲率顶部和底部附近，点变得稀疏。\n\n这创造了一个清晰的结构特征：两组密集的点群。一个 $k=2$ 的聚类可以通过在每个高密度区域放置一个质心来有效地捕捉这种结构。当从 $k=1$（单个质心在原点）移动到 $k=2$ 时，这会导致 $WSS$ 的非常显著的减少。对于 $k2$，后续 $WSS$ 的减少则不那么剧烈。这种急剧的初始下降之后是更平缓的减少，从而在 $k=2$ 处形成一个突出的肘部。\n\n这个视觉上的肘部可以通过找到离散二阶差分 $D(k) = W(k-1) - 2W(k) + W(k+1)$ 的最大值来定量检测。该值是二阶导数的近似，并且在诸如肘部之类的高凸性点处值较大。因此，我们预期对于变形后的数据集，$k^\\star = \\arg\\max_k D(k)$ 的结果将是 2。\n\n### 算法实现策略\n\n该解决方案将作为一个 Python 程序实现，其结构如下：\n\n1.  **数据生成**：一个函数将生成所需的两个数据集：单位圆上的 $n=360$ 个点，以及这些点的各向异性缩放版本。\n2.  **k-均值算法**：一个核心函数将实现 k-均值算法。\n    -   它将使用多次重启以确保高质量的解决方案。\n    -   每次运行将使用 k-means++ 播种方法进行初始化，以实现确定性和鲁棒的行为，由一个主随机种子控制。\n    -   迭代过程将处理空簇，方法是将空簇的质心重新播种到距离任何现有非空质心最远的数据点。\n3.  **$W(k)$ 计算**：一个包装函数将通过重复调用 k-均值函数，为给定数据集在指定的 $k$ 值范围内计算整个 $W(k)$ 曲线。\n4.  **测试用例评估**：\n    -   **平坦线检查**：一个函数将实现标准 $\\max_{k} \\Delta(k) \\le \\tau$，通过为圆形数据集计算指定范围内的归一化下降量。\n    -   **肘部检测**：一个函数将为变形数据集计算离散二阶差分 $D(k)$，并找到使该值最大化的 $k^\\star$，平局则选择较小的 $k$。\n5.  **主执行部分**：脚本的主体部分将协调这些组件来执行三个测试用例，收集结果（两个布尔值和一个整数），并以指定格式打印它们。使用 `numpy` 和 `scipy.spatial.distance.cdist` 将确保高效的向量化计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\n# Global seed for full determinism of the stochastic k-means algorithm.\nRANDOM_SEED = 42\n\ndef _kmeans_plusplus_init(data, k, rng):\n    \"\"\"\n    Initializes k-means centroids using the k-means++ algorithm for a single run.\n    This ensures deterministic initialization given a random number generator.\n    \"\"\"\n    n_samples, n_features = data.shape\n    centroids = np.zeros((k, n_features))\n\n    # 1. Choose the first centroid uniformly at random from the data points.\n    first_idx = rng.choice(n_samples)\n    centroids[0] = data[first_idx]\n\n    # 2. For each subsequent centroid, choose it with probability proportional to D(x)^2.\n    for i in range(1, k):\n        # Calculate the squared distance from each point to the nearest centroid.\n        dist_sq = cdist(data, centroids[:i, :], 'sqeuclidean')\n        min_dist_sq = np.min(dist_sq, axis=1)\n\n        # Create a probability distribution and choose the next centroid.\n        sum_dist_sq = np.sum(min_dist_sq)\n        if sum_dist_sq == 0:  # Edge case: all points are centroids\n            probs = None # Uniform probability\n        else:\n            probs = min_dist_sq / sum_dist_sq\n        \n        next_idx = rng.choice(n_samples, p=probs)\n        centroids[i] = data[next_idx]\n\n    return centroids\n\ndef _single_kmeans_run(data, k, max_iter, rng):\n    \"\"\"\n    Performs a single run of Lloyd's k-means algorithm.\n    \"\"\"\n    centroids = _kmeans_plusplus_init(data, k, rng)\n\n    for _ in range(max_iter):\n        # Assignment step: assign each point to the nearest centroid.\n        dist_sq = cdist(data, centroids, 'sqeuclidean')\n        labels = np.argmin(dist_sq, axis=1)\n\n        # Update step: recalculate centroids as the mean of assigned points.\n        new_centroids = np.zeros_like(centroids)\n        counts = np.bincount(labels, minlength=k)\n        \n        non_empty_indices = np.where(counts > 0)[0]\n        empty_indices = np.where(counts == 0)[0]\n\n        for j in non_empty_indices:\n            new_centroids[j] = data[labels == j].mean(axis=0)\n\n        # Robustly handle empty clusters as per the problem description.\n        if len(empty_indices) > 0:\n            active_centroids = new_centroids[non_empty_indices]\n            \n            if len(active_centroids) > 0:\n                dists_to_active = cdist(data, active_centroids, 'sqeuclidean')\n                min_dists_sq = np.min(dists_to_active, axis=1)\n                \n                # To handle multiple empty clusters, find a set of distinct farthest points.\n                farthest_indices = np.argsort(min_dists_sq)[-len(empty_indices):][::-1]\n\n                for i, empty_idx in enumerate(empty_indices):\n                    new_centroids[empty_idx] = data[farthest_indices[i]]\n            else:\n                 # This case (all clusters empty) is unlikely with this problem's parameters.\n                 # If it occurs, re-initialize all centroids.\n                 new_centroids = _kmeans_plusplus_init(data, k, rng)\n\n        # Check for convergence.\n        if np.allclose(centroids, new_centroids):\n            break\n        \n        centroids = new_centroids\n\n    # Calculate final WSS.\n    final_dist_sq = cdist(data, centroids, 'sqeuclidean')\n    final_labels = np.argmin(final_dist_sq, axis=1)\n    wss = np.sum(final_dist_sq[np.arange(len(data)), final_labels])\n    \n    return wss\n\ndef _kmeans(data, k, num_restarts, seed_rng):\n    \"\"\"\n    Runs k-means with multiple restarts and returns the best WSS.\n    \"\"\"\n    best_wss = np.inf\n    # Spawn new independent random number generators for each restart for reproducibility.\n    child_seeds = seed_rng.spawn(num_restarts)\n    \n    for i in range(num_restarts):\n        run_rng = np.random.default_rng(child_seeds[i])\n        wss = _single_kmeans_run(data, k, max_iter=100, rng=run_rng)\n        if wss  best_wss:\n            best_wss = wss\n            \n    return best_wss\n\ndef _generate_points(n, r, s=None):\n    \"\"\"\n    Generates n points on a circle, optionally applying anisotropic scaling.\n    \"\"\"\n    angles = np.linspace(0, 2 * np.pi, n, endpoint=False)\n    points = np.zeros((n, 2))\n    points[:, 0] = r * np.cos(angles)\n    points[:, 1] = r * np.sin(angles)\n    \n    if s is not None:\n        points[:, 1] *= s\n        \n    return points\n\ndef _compute_w_curve(data, k_range, num_restarts=10, rng=None):\n    \"\"\"\n    Computes the W(k) curve for a range of k values.\n    \"\"\"\n    if rng is None:\n        # Create a top-level generator for seeding the k-means runs.\n        rng = np.random.default_rng(RANDOM_SEED)\n\n    w_values = []\n    for k in k_range:\n        if k == 1:\n            centroid = np.mean(data, axis=0)\n            wss = np.sum(cdist(data, centroid.reshape(1, -1), 'sqeuclidean'))\n            w_values.append(wss)\n        else:\n            w_values.append(_kmeans(data, k, num_restarts, rng))\n    return w_values\n\ndef _check_flatline(w_values, k_min, k_max, tau):\n    \"\"\"\n    Checks if the W(k) curve is \"flatlined\" over a given range.\n    \"\"\"\n    w1 = w_values[0] # Corresponds to W(1)\n    max_delta = 0.0\n    for k in range(k_min, k_max):\n        # w_values is 0-indexed, so W(k) is at index k-1.\n        wk = w_values[k - 1]\n        wk_plus_1 = w_values[k]\n        delta_k = (wk - wk_plus_1) / w1\n        if delta_k > max_delta:\n            max_delta = delta_k\n    return max_delta = tau\n\ndef _find_elbow(w_values, K):\n    \"\"\"\n    Finds the elbow location k* using the discrete second difference.\n    \"\"\"\n    # D(k) is defined for k in {2, ..., K-1}\n    # w_values[i] corresponds to W(i+1)\n    D_values = []\n    for k in range(2, K):\n        w_km1 = w_values[k - 2]  # W(k-1)\n        w_k = w_values[k - 1]    # W(k)\n        w_kp1 = w_values[k]      # W(k+1)\n        Dk = w_km1 - 2 * w_k + w_kp1\n        D_values.append(Dk)\n        \n    # argmax breaks ties by choosing the first occurrence (smallest k).\n    # The calculated D_values correspond to k = 2, 3, ..., so we add 2 to the index.\n    best_k_idx = np.argmax(D_values)\n    return best_k_idx + 2\n\ndef solve():\n    # Define parameters from the problem statement.\n    n = 360\n    r = 1.0\n    s_deform = 0.5\n    k_max_eval = 12\n    k_range = list(range(1, k_max_eval + 1))\n    \n    # Create a master random number generator to ensure all parts of the\n    # calculation are deterministic from a single seed.\n    main_rng = np.random.default_rng(RANDOM_SEED)\n\n    # Generate datasets.\n    points_circle = _generate_points(n, r)\n    points_deformed = _generate_points(n, r, s=s_deform)\n\n    # Pre-calculate W(k) curves to avoid re-computation for tests 1 and 3.\n    w_circle = _compute_w_curve(points_circle, k_range, rng=main_rng)\n    w_deformed = _compute_w_curve(points_deformed, k_range, rng=main_rng)\n    \n    # --- Test Case 1: Adversarial flatline check ---\n    k_min1, k_max1, tau1 = 6, 12, 0.025\n    result1 = _check_flatline(w_circle, k_min1, k_max1, tau1)\n\n    # --- Test Case 2: Elbow after deformation ---\n    # elbow detection range is k in {2, ..., K-1}, and K=12 for the curve.\n    result2 = _find_elbow(w_deformed, k_max_eval)\n\n    # --- Test Case 3: Boundary-range non-flatness check ---\n    k_min3, k_max3, tau3 = 2, 5, 0.03\n    result3 = _check_flatline(w_circle, k_min3, k_max3, tau3)\n\n    # Combine results and print in the specified format.\n    results = [result1, result2, result3]\n    # The print must exactly match the format '[result1,result2,result3]'\n    # Python's str() for bool is 'True'/'False', which is correct.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3107514"}, {"introduction": "在理解了肘部法则的几何原理后，我们转向一个关键的实际问题：数据预处理。由于 $k$-均值聚类依赖于欧氏距离，各特征的尺度至关重要。本练习将揭示特征缩放的“双刃剑”效应：您将创建并分析一些数据集，在这些数据集中，特征归一化有时能揭示隐藏的簇结构，有时却会因放大了噪声而掩盖真实结构。[@problem_id:3107563]", "problem": "要求您实现一个实验，量化在欧几里得 $k$-均值方法中选择聚类数量时，将特征逐一归一化至单位方差对簇内平方和（WCSS）曲线中肘部检测的影响。该实验必须实现为一个完整、可运行的程序。关注的重点是簇内平方和（WCSS）以及特征缩放对欧几里得距离的影响。\n\n使用以下基础理论：\n- 欧几里得 $k$-均值旨在最小化簇内平方和（WCSS），其定义为 $$W(k) \\;=\\; \\sum_{i=1}^{n} \\left\\| x_i - \\mu_{\\mathrm{cluster}(i)} \\right\\|_2^2,$$ 其中 $x_i \\in \\mathbb{R}^d$ 是第 $i$ 个数据向量，$\\mu_{\\mathrm{cluster}(i)}$ 是 $x_i$ 所属簇的质心，$n$ 是样本数量，$d$ 是特征数量，$k$ 是聚类数量。\n- 将特征逐一归一化至单位方差意味着对每个特征 $j \\in \\{1,\\dots,d\\}$ 使用以下公式进行变换：$$x_{ij}' \\;=\\; \\frac{x_{ij} - \\bar{x}_j}{s_j},$$ 其中 $\\bar{x}_j$ 是特征 $j$ 的样本均值，$s_j$ 是其样本标准差。如果 $s_j = 0$，则对所有样本保持该特征不变，即使用 $x_{ij}' = x_{ij}$。\n\n您必须实现以下过程：\n1. 使用带有多次随机重启的劳埃德算法（Lloyd's algorithm）来实现欧几里得 $k$-均值，以增强稳健性。对于每个 $k$，返回最小化的 $W(k)$。\n2. 按如下方式实现肘部检测。考虑点集 $p_k = (k, W(k))$，其中 $k \\in \\{1,2,\\dots,K_{\\max}\\}$。将肘部索引 $\\hat{k}$ 定义为使点 $p_k$ 到经过 $p_1$ 和 $p_{K_{\\max}}$ 的直线的垂直距离最大的整数 $k \\in \\{2,\\dots,K_{\\max}-1\\}$。您的程序必须使用基础欧几里得几何精确计算此距离。\n\n构建三个合成数据集，每个数据集都具有指定的参数和随机种子，以形成一个测试套件。对于每个数据集，计算两次肘部索引：一次使用原始（未缩放）特征，另一次使用归一化至单位方差的特征。数据集如下：\n\n- 测试用例 1（两个特征，归一化揭示肘部）：\n  - 维度 $d = 2$。\n  - 真实聚类数 $c = 3$。\n  - 每个簇的样本数 $n_c = 60$（总计 $n = 180$）。\n  - 随机种子 $= 123$。\n  - 簇的构造：特征 1 是标准差为 $\\sigma_x = 12$ 的独立高斯噪声。特征 2 的簇均值分别为 $-6$、$0$ 和 $6$，簇内高斯噪声的标准差为 $\\sigma_y = 0.6$。\n\n- 测试用例 2（十个特征，归一化通过放大多个噪声特征来掩盖肘部）：\n  - 维度 $d = 10$。\n  - 真实聚类数 $c = 3$。\n  - 每个簇的样本数 $n_c = 50$（总计 $n = 150$）。\n  - 随机种子 $= 321$。\n  - 簇的构造：特征 1 带有聚类信号，其均值分别为 $-12$、$0$ 和 $12$，簇内高斯噪声的标准差为 $\\sigma_{\\mathrm{sig}} = 0.5$。其余 9 个特征是标准差为 $\\sigma_{\\mathrm{noise}} = 5$、均值为零的独立高斯噪声。\n\n- 测试用例 3（包含零方差特征的边缘案例）：\n  - 维度 $d = 3$。\n  - 真实聚类数 $c = 3$。\n  - 每个簇的样本数 $n_c = 40$（总计 $n = 120$）。\n  - 随机种子 $= 777$。\n  - 簇的构造：特征 1 对所有样本均为常数 $0$（零方差）。特征 2 的簇均值分别为 $-4$、$0$ 和 $4$，簇内高斯噪声的标准差为 $\\sigma_y = 0.7$。特征 3 是标准差为 $\\sigma_z = 1$、均值为零的独立高斯噪声。\n\n对于每个测试用例，使用如下的最大聚类数 $K_{\\max}$：\n- 测试用例 1：$K_{\\max} = 8$。\n- 测试用例 2：$K_{\\max} = 8$。\n- 测试用例 3：$K_{\\max} = 7$。\n\n实现要求：\n- 对于每个测试用例，完全按照规定生成数据。\n- 对于每个测试用例，计算两次肘部索引 $\\hat{k}$（原始数据和归一化数据）。\n- 对 $k$-均值使用 $5$ 次随机重启，每次重启最多迭代 $300$ 次。\n- 仅使用欧几里得距离和标准算术运算。\n- 完全按照规定处理零方差特征。\n\n最终输出规范：\n- 您的程序应生成单行输出，其中包含按以下顺序排列的六个肘部索引： $[\\hat{k}_{1,\\mathrm{raw}}, \\hat{k}_{1,\\mathrm{norm}}, \\hat{k}_{2,\\mathrm{raw}}, \\hat{k}_{2,\\mathrm{norm}}, \\hat{k}_{3,\\mathrm{raw}}, \\hat{k}_{3,\\mathrm{norm}}]$。\n- 输出必须是单行 Python 列表字面量，整数之间用逗号分隔，并用方括号括起来（例如 $[2,3,3,2,2,3]$）。\n\n不涉及物理单位或角度。所有数值答案均为整数。确保程序是自包含的，在给定指定种子的情况下是确定性的，并且仅使用指定的库。", "solution": "该问题被评估为有效。它是一个在统计学习领域内定义明确、有科学依据且客观的计算问题。所有参数、定义和过程都得到了明确的规定，从而可以得到唯一且可复现的解。\n\n解决方案通过实现指定的实验来进行，该实验涉及几个相互关联的组成部分：合成数据生成、特征归一化、$k$-均值算法的稳健实现，以及用于肘部检测的几何方法。\n\n**1. 基本原理**\n\n问题的核心在于聚类算法和数据预处理的交叉点。欧几里得 $k$-均值算法旨在最小化簇内平方和（WCSS），这是一个对特征尺度敏感的目标函数。\n\n- **簇内平方和 (WCSS):** 对于给定的聚类数 $k$，WCSS 定义为\n$$W(k) \\;=\\; \\sum_{j=1}^{k} \\sum_{x_i \\in C_j} \\left\\| x_i - \\mu_j \\right\\|_2^2$$\n其中 $C_j$ 是第 $j$ 个簇中数据点的集合，$\\mu_j$ 是簇 $C_j$ 的质心，$\\| \\cdot \\|_2$ 是欧几里得范数。由于欧几里得范数对每个维度的平方差求和，因此具有较大数据范围或方差的特征会对总距离产生不成比例的影响，可能掩盖其他特征中的潜在结构。\n\n- **特征归一化：** 为缓解此问题，通常会对特征进行缩放。本问题指定了归一化至单位方差（标准化）。对于每个特征 $j$，变换公式为\n$$x_{ij}' \\;=\\; \\frac{x_{ij} - \\bar{x}_j}{s_j}$$\n其中 $\\bar{x}_j$ 是特征 $j$ 的样本均值，$s_j$ 是其样本标准差。样本标准差计算公式为 $s_j = \\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n(x_{ij}-\\bar{x}_j)^2}$。此变换使每个特征的均值为 $0$，标准差为 $1$。问题规定，如果某个特征的方差为零（$s_j = 0$），则该特征应保持不变，即 $x_{ij}' = x_{ij}$。\n\n**2. 算法实现**\n\n整个实验被封装在一个程序中，该程序为三个测试用例中的每一个系统地执行所需的步骤。\n\n- **数据生成：** 对于每个测试用例，使用带种子的随机数生成器（`numpy.random.default_rng`）根据精确的规范生成一个合成数据集，以确保可复现性。参数（维度 $d$、真实聚类数 $c$、每个簇的样本数 $n_c$、均值和标准差）用于构建聚类结构先验已知的数据矩阵。\n\n- **$k$-均值算法：** 基于劳埃德算法（Lloyd's algorithm）开发了一个稳健的欧几里得 $k$-均值实现。\n    - **初始化：** 为了开始劳埃德算法的一次迭代，通过从数据集中随机选择 $k$ 个不同的数据点作为初始质心（Forgy 方法）。\n    - **迭代：** 该算法分两步进行：\n        1. **分配步骤：** 将每个数据点 $x_i$ 分配给最近质心对应的簇，以最小化 $\\| x_i - \\mu_j \\|_2^2$。这可以使用平方欧几里得距离高效计算。\n        2. **更新步骤：** 将每个簇的质心 $\\mu_j$ 重新计算为分配给它的所有数据点的均值。如果某个簇变为空，则保留其前一次迭代的质心位置，以保持稳定性和恒定的聚类数量。\n    - **收敛：** 当质心位置在两次迭代之间不再变化，或达到最大迭代次数（$300$）时，迭代过程停止。\n    - **稳健性：** 为避免 $k$-均值中常见的局部最优解，整个劳埃德算法使用不同的随机初始化运行 $5$ 次（重启）。在这些重启中产生最小 WCSS $W(k)$ 的聚类结果被选为该 $k$ 值的结果。\n\n- **肘部检测：** WCSS 曲线中的“肘部”是确定最优聚类数的一种启发式方法。问题定义了一种确定性的方法来定位它。\n    - 通过对每个 $k \\in \\{1, 2, \\dots, K_{\\max}\\}$ 运行 $k$-均值算法，生成一系列点 $p_k = (k, W(k))$。\n    - 定义一条直线 $L$ 穿过第一个点 $p_1 = (1, W(1))$ 和最后一个点 $p_{K_{\\max}} = (K_{\\max}, W(K_{\\max}))$。\n    - 对于每个中间点 $p_k$（其中 $k \\in \\{2, \\dots, K_{\\max}-1\\}$），计算其到直线 $L$ 的垂直距离。从点 $p_0$ 到由点 $p_a$ 和 $p_b$ 定义的直线的距离可以使用平行四边形面积公式求得：\n    $$ \\text{distance} = \\frac{|\\det(\\vec{v}, \\vec{w})|}{\\|\\vec{v}\\|} = \\frac{|v_x w_y - v_y w_x|}{\\sqrt{v_x^2 + v_y^2}} $$\n    其中 $\\vec{v} = p_b - p_a$ 且 $\\vec{w} = p_0 - p_a$。\n    - 肘部索引 $\\hat{k}$ 是使该垂直距离最大化的 $k$ 值。\n\n- **实验流程：** 对于三个测试用例中的每一个，整个过程执行两次：一次在原始未缩放数据上，一次在应用特征逐一归一化至单位方差后的数据上。将得到的六个肘部索引（$\\hat{k}_{1,\\mathrm{raw}}, \\hat{k}_{1,\\mathrm{norm}}, \\hat{k}_{2,\\mathrm{raw}}, \\hat{k}_{2,\\mathrm{norm}}, \\hat{k}_{3,\\mathrm{raw}}, \\hat{k}_{3,\\mathrm{norm}}$）收集起来并作为最终输出。该实验旨在展示归一化如何能够揭示隐藏的聚类结构（测试用例 1），或通过放大噪声来掩盖它们（测试用例 2），同时也能正确处理零方差特征等边缘情况（测试用例 3）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Implements and runs the experiment to quantify the effect of feature normalization\n    on elbow detection in k-means clustering.\n    \"\"\"\n\n    def generate_data(case_id, d, c, n_c, rng):\n        \"\"\"Generates synthetic dataset for a given test case.\"\"\"\n        n_total = c * n_c\n        \n        if case_id == 1:\n            # Case 1: Normalization reveals the elbow\n            # Feature 1: High-variance noise\n            # Feature 2: Low-variance signal\n            x1 = rng.normal(loc=0, scale=12, size=n_total)\n            x2 = np.zeros(n_total)\n            means = [-6, 0, 6]\n            sigma_y = 0.6\n            for i, mean in enumerate(means):\n                start, end = i * n_c, (i + 1) * n_c\n                x2[start:end] = rng.normal(loc=mean, scale=sigma_y, size=n_c)\n            X = np.stack((x1, x2), axis=1)\n            \n        elif case_id == 2:\n            # Case 2: Normalization obscures the elbow\n            # Feature 1: Signal\n            # Features 2-10: Moderate-variance noise\n            x_sig = np.zeros(n_total)\n            means = [-12, 0, 12]\n            sigma_sig = 0.5\n            for i, mean in enumerate(means):\n                start, end = i * n_c, (i + 1) * n_c\n                x_sig[start:end] = rng.normal(loc=mean, scale=sigma_sig, size=n_c)\n            \n            x_noise = rng.normal(loc=0, scale=5, size=(n_total, d - 1))\n            X = np.concatenate((x_sig.reshape(-1, 1), x_noise), axis=1)\n\n        elif case_id == 3:\n            # Case 3: Edge case with a zero-variance feature\n            x1 = np.zeros(n_total) # Zero variance\n            x2 = np.zeros(n_total)\n            means = [-4, 0, 4]\n            sigma_y = 0.7\n            for i, mean in enumerate(means):\n                start, end = i * n_c, (i + 1) * n_c\n                x2[start:end] = rng.normal(loc=mean, scale=sigma_y, size=n_c)\n            \n            x3 = rng.normal(loc=0, scale=1, size=n_total)\n            X = np.stack((x1, x2, x3), axis=1)\n        \n        return X\n\n    def normalize_features(X):\n        \"\"\"Normalizes features to unit variance as specified.\"\"\"\n        X_norm = X.copy()\n        means = np.mean(X, axis=0)\n        # Use ddof=1 for sample standard deviation\n        stds = np.std(X, axis=0, ddof=1)\n        \n        for j in range(X.shape[1]):\n            if stds[j] > 1e-12: # Check for non-zero standard deviation\n                X_norm[:, j] = (X[:, j] - means[j]) / stds[j]\n            # If stds[j] is zero, the feature is left unchanged as per instructions\n        return X_norm\n\n    def _kmeans_single_run(X, k, max_iter, rng):\n        \"\"\"Performs a single run of Lloyd's algorithm for k-means.\"\"\"\n        n_samples = X.shape[0]\n        \n        # Forgy initialization: Choose k random distinct points as initial centroids\n        initial_indices = rng.choice(n_samples, size=k, replace=False)\n        centroids = X[initial_indices]\n        \n        for _ in range(max_iter):\n            # Assignment step: an O(N*k*d) operation, vectorized\n            distances_sq = cdist(X, centroids, 'sqeuclidean')\n            labels = np.argmin(distances_sq, axis=1)\n            \n            # Update step: Compute new centroids\n            new_centroids = np.copy(centroids)\n            for j in range(k):\n                points_in_cluster = X[labels == j]\n                if len(points_in_cluster) > 0:\n                    new_centroids[j] = np.mean(points_in_cluster, axis=0)\n                # Else: cluster is empty, retain previous centroid\n            \n            # Convergence check\n            if np.allclose(centroids, new_centroids):\n                break\n            \n            centroids = new_centroids\n        \n        # Final WCSS calculation\n        wcss = 0.0\n        for j in range(k):\n            points_in_cluster = X[labels == j]\n            if len(points_in_cluster) > 0:\n                dist_sq = np.sum((points_in_cluster - centroids[j])**2)\n                wcss += dist_sq\n                \n        return wcss\n\n    def kmeans_multirestart(X, k, n_restarts, max_iter, parent_rng):\n        \"\"\"Runs k-means with multiple random restarts and returns the best WCSS.\"\"\"\n        min_wcss = np.inf\n        \n        # Special case k=1\n        if k == 1:\n            centroid = np.mean(X, axis=0)\n            return np.sum((X - centroid)**2)\n            \n        # Generate seeds for each restart from the parent RNG for reproducibility\n        restart_seeds = parent_rng.integers(low=0, high=2**32 - 1, size=n_restarts)\n        \n        for i in range(n_restarts):\n            rng_restart = np.random.default_rng(restart_seeds[i])\n            wcss = _kmeans_single_run(X, k, max_iter, rng_restart)\n            if wcss  min_wcss:\n                min_wcss = wcss\n        return min_wcss\n        \n    def find_elbow(wcss_values, K_max):\n        \"\"\"Finds the elbow point using the perpendicular distance method.\"\"\"\n        points = np.array([(k, wcss) for k, wcss in enumerate(wcss_values, 1)])\n        \n        p1 = points[0]\n        p_Kmax = points[K_max - 1]\n        \n        line_vec = p_Kmax - p1\n        line_vec_norm_sq = np.sum(line_vec**2)\n        \n        if line_vec_norm_sq == 0:\n            return -1 # Should not happen in this problem\n            \n        distances = []\n        # k ranges from 2 to K_max - 1\n        for i in range(1, K_max - 1):\n            p_k = points[i]\n            point_vec = p_k - p1\n            \n            # Perpendicular distance using cross-product magnitude equivalent in 2D\n            numerator = np.abs(line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0])\n            distance = numerator / np.sqrt(line_vec_norm_sq)\n            distances.append(distance)\n        \n        if not distances:\n            return -1 # K_max is too small\n\n        # +2 because distances index is 0..N-1, corresponds to k=2..K_max-1\n        elbow_k = np.argmax(distances) + 2\n        return int(elbow_k)\n\n    # --- Main Execution ---\n    test_cases = [\n        {'case_id': 1, 'd': 2, 'c': 3, 'n_c': 60, 'seed': 123, 'K_max': 8},\n        {'case_id': 2, 'd': 10, 'c': 3, 'n_c': 50, 'seed': 321, 'K_max': 8},\n        {'case_id': 3, 'd': 3, 'c': 3, 'n_c': 40, 'seed': 777, 'K_max': 7}\n    ]\n    \n    k_means_restarts = 5\n    k_means_max_iter = 300\n    \n    results = []\n    for case in test_cases:\n        # Create a single RNG for all randomness within a test case (data + kmeans)\n        # This ensures that both raw and norm runs use the same sequence of random inits\n        case_rng = np.random.default_rng(case['seed'])\n        \n        X = generate_data(\n            case_id=case['case_id'], d=case['d'], c=case['c'], \n            n_c=case['n_c'], rng=case_rng\n        )\n        K_max = case['K_max']\n        \n        # 1. Run on raw data\n        wcss_raw = []\n        for k in range(1, K_max + 1):\n            wcss = kmeans_multirestart(X, k, k_means_restarts, k_means_max_iter, case_rng)\n            wcss_raw.append(wcss)\n        elbow_raw = find_elbow(wcss_raw, K_max)\n        results.append(elbow_raw)\n        \n        # 2. Run on normalized data\n        X_norm = normalize_features(X)\n        wcss_norm = []\n        for k in range(1, K_max + 1):\n            wcss = kmeans_multirestart(X_norm, k, k_means_restarts, k_means_max_iter, case_rng)\n            wcss_norm.append(wcss)\n        elbow_norm = find_elbow(wcss_norm, K_max)\n        results.append(elbow_norm)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3107563"}]}