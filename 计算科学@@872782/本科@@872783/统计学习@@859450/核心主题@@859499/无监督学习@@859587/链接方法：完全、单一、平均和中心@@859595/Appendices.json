{"hands_on_practices": [{"introduction": "层次聚类的一个理想特性是单调性，即合并的高度（或距离）应随着聚类过程的进行而严格增加。然而，并非所有连锁方法都满足此属性。本练习将通过一个具体的数值示例，引导您手动计算质心连锁法的合并过程，从而揭示其可能违反单调性的反直觉行为。", "problem": "考虑在 $\\mathbb{R}^{2}$ 中使用欧几里得度量的质心联动层次聚合聚类 (HAC)。质心联动将两个簇之间的距离定义为其算术平均值（质心）之间的欧几里得距离。给定 $3$ 个数据点\n$$\n\\mathbf{x}_{1}=\\left(-\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{2}=\\left(\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{3}=\\left(0,\\,\\frac{\\sqrt{13}}{4}\\right).\n$$\n从单元素簇 $\\{\\mathbf{x}_{1}\\}$、$\\{\\mathbf{x}_{2}\\}$ 和 $\\{\\mathbf{x}_{3}\\}$ 开始，执行质心联动 HAC。在每一步中，合并质心距离最小的一对簇，并将该步选择的质心距离记录为相应的合并高度。精确计算：\n- 第一个合并高度 $h_{1}$，\n- 第一步后合并簇的质心，\n- 第二个合并高度 $h_{2}$，\n并数值验证 $h_{2}  h_{1}$。", "solution": "该问题是有效的，因为它是一个适定且有科学依据的练习，旨在应用质心联动层次聚合聚类 (HAC) 算法，这是统计学习中的一个标准课题。它提供了所有必要的数据和定义，以得出一个唯一且可验证的解。\n\n过程从 $3$ 个单元素簇开始，$C_{1}=\\{\\mathbf{x}_{1}\\}$、$C_{2}=\\{\\mathbf{x}_{2}\\}$ 和 $C_{3}=\\{\\mathbf{x}_{3}\\}$，其中数据点为：\n$$\n\\mathbf{x}_{1}=\\left(-\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{2}=\\left(\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{3}=\\left(0,\\,\\frac{\\sqrt{13}}{4}\\right)\n$$\n在质心联动中，两个簇之间的距离是它们质心之间的欧几里得距离。对于单元素簇，质心就是点本身。设 $\\mu_i$ 是簇 $C_i$ 的质心。初始时，$\\mu_{1}=\\mathbf{x}_{1}$，$\\mu_{2}=\\mathbf{x}_{2}$，以及 $\\mu_{3}=\\mathbf{x}_{3}$。\n\n首先，我们计算初始质心之间的成对欧几里得距离的平方：\n$C_{1}$ 和 $C_{2}$ 之间的平方距离是：\n$$\nd(C_{1}, C_{2})^{2} = ||\\mu_{1} - \\mu_{2}||^{2} = ||\\mathbf{x}_{1} - \\mathbf{x}_{2}||^{2} = \\left\\|\\left(-\\frac{1}{2}-\\frac{1}{2},\\,0-0\\right)\\right\\|^{2} = \\|(-1, 0)\\|^{2} = (-1)^{2} + 0^{2} = 1\n$$\n$C_{1}$ 和 $C_{3}$ 之间的平方距离是：\n$$\nd(C_{1}, C_{3})^{2} = ||\\mu_{1} - \\mu_{3}||^{2} = ||\\mathbf{x}_{1} - \\mathbf{x}_{3}||^{2} = \\left\\|\\left(-\\frac{1}{2}-0,\\,0-\\frac{\\sqrt{13}}{4}\\right)\\right\\|^{2} = \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2} = \\frac{1}{4} + \\frac{13}{16} = \\frac{4}{16} + \\frac{13}{16} = \\frac{17}{16}\n$$\n$C_{2}$ 和 $C_{3}$ 之间的平方距离是：\n$$\nd(C_{2}, C_{3})^{2} = ||\\mu_{2} - \\mu_{3}||^{2} = ||\\mathbf{x}_{2} - \\mathbf{x}_{3}||^{2} = \\left\\|\\left(\\frac{1}{2}-0,\\,0-\\frac{\\sqrt{13}}{4}\\right)\\right\\|^{2} = \\left(\\frac{1}{2}\\right)^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2} = \\frac{1}{4} + \\frac{13}{16} = \\frac{4}{16} + \\frac{13}{16} = \\frac{17}{16}\n$$\n相应的距离是 $d(C_{1}, C_{2}) = \\sqrt{1} = 1$，以及 $d(C_{1}, C_{3}) = d(C_{2}, C_{3}) = \\sqrt{\\frac{17}{16}} = \\frac{\\sqrt{17}}{4}$。\n为了找到最小的距离，我们比较 $1$ 和 $\\frac{\\sqrt{17}}{4}$。将两个值平方得到 $1^{2}=1$ 和 $\\left(\\frac{\\sqrt{17}}{4}\\right)^{2}=\\frac{17}{16}$。因为 $1  \\frac{17}{16}$，所以我们有 $1  \\frac{\\sqrt{17}}{4}$。\n因此，最小的距离是 $d(C_{1}, C_{2}) = 1$。\n\n第一步是合并最近的簇 $C_{1}$ 和 $C_{2}$。合并高度 $h_{1}$ 就是这个最小距离。\n$$\nh_{1} = 1\n$$\n设新簇为 $C_{12} = C_{1} \\cup C_{2} = \\{\\mathbf{x}_{1}, \\mathbf{x}_{2}\\}$。这个新簇的质心 $\\mu_{12}$ 是其组成点的算术平均值：\n$$\n\\mu_{12} = \\frac{\\mathbf{x}_{1} + \\mathbf{x}_{2}}{2} = \\frac{1}{2} \\left[ \\left(-\\frac{1}{2}, 0\\right) + \\left(\\frac{1}{2}, 0\\right) \\right] = \\frac{1}{2} (0, 0) = (0, 0)\n$$\n合并后簇的质心是原点 $(0,0)$。\n\n算法进入第二步。现在我们有两个簇：$C_{12}$ 和 $C_{3}$。下一次合并必须在这两个簇之间进行。第二个合并高度 $h_{2}$ 是它们质心 $\\mu_{12}$ 和 $\\mu_{3}$ 之间的距离。\n$$\nh_{2} = d(C_{12}, C_{3}) = ||\\mu_{12} - \\mu_{3}|| = ||(0, 0) - \\left(0, \\frac{\\sqrt{13}}{4}\\right)|| = \\left\\|\\left(0, -\\frac{\\sqrt{13}}{4}\\right)\\right\\|\n$$\n计算其大小：\n$$\nh_{2} = \\sqrt{0^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2}} = \\sqrt{\\frac{13}{16}} = \\frac{\\sqrt{13}}{4}\n$$\n现在我们验证发生了倒置，即 $h_{2}  h_{1}$。\n我们需要比较 $h_2 = \\frac{\\sqrt{13}}{4}$ 和 $h_1 = 1$。不等式是 $\\frac{\\sqrt{13}}{4}  1$。两边乘以 $4$，我们得到 $\\sqrt{13}  4$。由于两边都是正数，我们可以将它们平方得到 $13  16$。这个不等式是成立的，这证实了 $h_{2}  h_{1}$。质心联动方法产生了一个非单调的树状图，这是该联动类型的一个已知特性。\n\n最后，我们计算倒置幅度 $\\Delta$，定义为 $\\Delta=h_{2}-h_{1}$。\n$$\n\\Delta = \\frac{\\sqrt{13}}{4} - 1\n$$\n为了将其写成单个表达式，我们使用一个共同的分母：\n$$\n\\Delta = \\frac{\\sqrt{13} - 4}{4}\n$$\n这就是倒置幅度的精确封闭形式表达式。", "answer": "$$\n\\boxed{\\frac{\\sqrt{13}-4}{4}}\n$$", "id": "3140573"}, {"introduction": "在实际数据分析中，离群点是影响聚类结果的常见因素。不同的连锁方法对离群点的敏感度差异很大，理解这一点对于选择合适的算法至关重要。通过本编码实践，您将探索单一连锁、完整连锁、平均连锁和质心连锁在面对离群点时的不同表现，并加深对单一连锁与最小生成树（MST）之间内在联系的理解 [@problem_id:3140585]。", "problem": "给定一个基础数据集，它在欧几里得度量下的平面上包含两个紧密的簇。第一个簇 $A$ 由五个点组成：$(-0.2,-0.2)$、$(-0.2,0.2)$、$(0.2,-0.2)$、$(0.2,0.2)$ 和 $(0,0)$。第二个簇 $B$ 是通过将 $A$ 中的每个点沿 $x$ 轴平移 $4$ 个单位得到的，因此 $B$ 由 $(3.8,-0.2)$、$(3.8,0.2)$、$(4.2,-0.2)$、$(4.2,0.2)$ 和 $(4,0)$ 组成。所有距离均为欧几里得距离，并以无单位坐标表示。\n\n从这个基础数据集开始，你将按如下方式添加离群点。对于给定的标量距离 $R > 0$ 和整数数量 $s \\ge 1$，你将添加 $s$ 个离群点，它们位于坐标 $(-R,y_i)$ 处，其中 $y_i$ 是以 $0$ 为中心、步长为 $0.2$ 的均匀间隔偏移量，具体为 $y_i = 0.2\\,(i - (s-1)/2)$，其中 $i \\in \\{0,1,\\dots,s-1\\}$。当 $R$ 很大时，这种构造确保了每个添加的离群点与簇 $A$ 的质心距离约为 $R$，并且远离簇 $B$。\n\n你必须编写一个完整、可运行的程序，该程序：\n- 构建基础数据集并计算两两之间的欧几里得距离。\n- 计算由基础数据集导出的完全图的最小生成树（MST; Minimum Spanning Tree），使用等于欧几里得距离的边权重。令 $W_0$ 表示此基础MST中的最大边权重。\n- 对于每个测试用例，将指定的离群点添加到基础数据集中，重新计算MST，并令 $W_1$ 表示新MST中的最大边权重。计算增量 $\\Delta W = W_1 - W_0$。同时计算 $m_{\\text{new}}$，即新MST中至少涉及一个新增离群点节点的边的数量。\n- 对每个数据集（基础数据集和带离群点的数据集），使用四种链接方法进行层次凝聚聚类：单链接、全链接、平均链接和质心链接。对于每种方法，提取最终的合并高度 $H$（在链接结果中最后一次合并时记录的高度）。对于每个测试用例，计算相对于基础数据集的最终合并高度的增量，即 $\\Delta H_{\\text{single}}$、$\\Delta H_{\\text{complete}}$、$\\Delta H_{\\text{average}}$ 和 $\\Delta H_{\\text{centroid}}$。\n\n推理的基本依据：使用链接方法的定义，以及在由两两之间距离作为边权重的度量空间下，单链接聚类与最小生成树上的Kruskal算法之间的著名等价性。\n\n你的程序必须实现计算，并为以下 $(R,s)$ 值的测试套件生成结果：\n- $(1.0,1)$：一个离群点，位置相对靠近簇 $A$。\n- $(4.0,2)$：两个离群点，其位置与簇质心之间的间距相当。\n- $(8.0,1)$：一个离群点，位置远离两个簇。\n- $(3.0,3)$：三个离群点，位于中等距离处。\n\n对于每个测试用例，你的程序应按以下顺序输出一个包含六个值的列表：\n- $\\Delta H_{\\text{single}}$\n- $\\Delta H_{\\text{complete}}$\n- $\\Delta H_{\\text{average}}$\n- $\\Delta H_{\\text{centroid}}$\n- $\\Delta W$\n- $m_{\\text{new}}$\n\n所有实数值输出必须表示为十进制浮点数，计数 $m_{\\text{new}}$ 必须表示为整数。\n\n最终输出格式：你的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个逗号分隔的列表，该列表由每个测试用例的子列表组成，并用方括号括起来，不含空格。例如，它看起来应该像这样：$[[v_{1,1},v_{1,2},\\dots,v_{1,6}],[v_{2,1},\\dots,v_{2,6}],\\dots]$，其中 $v_{i,j}$ 表示第 $i$ 个测试用例的第 $j$ 个值。", "solution": "该问题要求对离群点对层次聚类和最小生成树（MST）结构的影响进行计算分析。解决方案涉及生成一个基础数据集，添加指定的离群点，然后为多个测试用例计算关键指标的变化。\n\n方法论步骤如下：\n\n1.  **数据集构建**：\n    基础数据集由二维平面上的两个不同簇 $A$ 和 $B$ 构成。簇 $A$ 由五个点 $\\{(-0.2,-0.2), (-0.2,0.2), (0.2,-0.2), (0.2,0.2), (0,0)\\}$ 定义。簇 $B$ 是簇 $A$ 沿正 $x$ 轴平移 $4$ 个单位的结果，得到的点为 $\\{(3.8,-0.2), (3.8,0.2), (4.2,-0.2), (4.2,0.2), (4,0)\\}$。基础数据集中的总点数为 $10$。对于由一对 $(R, s)$ 定义的每个测试用例，其中 $R > 0$ 是一个距离， $s \\ge 1$ 是一个整数计数，$s$ 个离群点将被生成。这些离群点被放置在坐标 $(-R, y_i)$ 处，其垂直偏移量 $y_i$ 由公式 $y_i = 0.2 \\times (i - (s-1)/2)$ 定义，其中 $i \\in \\{0, 1, \\dots, s-1\\}$。这种构造将离群点放置在簇 $A$ 的左侧。\n\n2.  **核心分析程序**：\n    对于任何给定的点集，都会计算一套标准指标。此程序首先应用于基础数据集，然后应用于每个增强数据集（基础数据集+离群点）。\n    -   **两两间距离**：分析始于计算数据集中所有点之间的两两欧几里得距离矩阵。该距离矩阵作为完全图的边权重，其中点是顶点。\n    -   **最小生成树（MST）**：使用标准算法（如Prim算法或Kruskal算法）计算此加权完全图的MST。我们确定 $W$，即MST中存在的最大边权重。\n    -   **层次凝聚聚类**：使用四种不同的链接方法（单链接、全链接、平均链接和质心链接）执行层次聚类。对于每种方法，结果是一个描述合并过程的链接矩阵。提取的关键指标是最终合并高度 $H$，这是最后两个簇合并形成包含所有数据点的单个簇时的距离或相异度值。\n\n3.  **理论联系：单链接与MST**：\n    图论和聚类中的一个基本原理指出，单链接层次聚类等同于构建一个MST。具体来说，单链接聚类中的合并序列对应于构建MST的Kruskal算法中添加边的序列。一个直接的推论是，单链接聚类中的最终合并高度 $H_{\\text{single}}$ 必须等于MST中最长边的权重 $W$。这为计算提供了一个关键的自洽性检验，因为我们预期在所有测试用例中都会发现 $\\Delta H_{\\text{single}} = \\Delta W$。\n\n4.  **差异分析**：\n    问题的核心是量化因引入离群点而导致的指标变化。\n    -   **基线计算**：首先，对基础数据集运行分析程序，以获得最大MST边权重（$W_0$）以及四种链接方法（$H_{\\text{single,0}}$、$H_{\\text{complete,0}}$、$H_{\\text{average,0}}$ 和 $H_{\\text{centroid,0}}$）的最终合并高度的基线值。\n    -   **测试用例计算**：对于每个测试用例 $(R,s)$，将指定的离群点添加到基础数据集中。然后，对这个新的、更大的数据集运行相同的分析程序，以计算新值：$W_1$、$H_{\\text{single,1}}$、$H_{\\text{complete,1}}$、$H_{\\text{average,1}}$ 和 $H_{\\text{centroid,1}}$。\n    -   **变化计算**：增量（用 $\\Delta$ 表示）计算为新值与基线值之间的差：\n        $$ \\Delta H_{\\text{method}} = H_{\\text{method,1}} - H_{\\text{method,0}} $$\n        $$ \\Delta W = W_1 - W_0 $$\n    -   **MST边计数**：计算一个附加指标 $m_{\\text{new}}$。这是新MST中至少有一个端点对应于新增离群点的边的数量。这量化了离群点是如何融入数据的连接结构中的。\n\n5.  **实现**：\n    所述过程通过一个Python程序实现。`numpy` 库用于数值运算和数组管理。`scipy` 库为科学计算提供了必要的高级函数：`scipy.spatial.distance.pdist` 和 `scipy.spatial.distance.squareform` 用于距离计算，`scipy.sparse.csgraph.minimum_spanning_tree` 用于MST，以及 `scipy.cluster.hierarchy.linkage` 用于层次聚类。值得注意的是，对于'centroid'链接方法，`linkage` 函数需要原始数据点作为输入，而对于'single'、'complete'和'average'方法，它作用于压缩的距离矩阵。程序遍历指定的测试套件，为每个用例计算六个所需的输出值，并将它们格式化为指定的字符串格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse.csgraph import minimum_spanning_tree\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the impact of outliers on hierarchical \n    clustering and Minimum Spanning Trees (MST).\n    \"\"\"\n\n    def analyze_dataset(points):\n        \"\"\"\n        Computes clustering and MST metrics for a given set of points.\n\n        Args:\n            points (np.ndarray): A NumPy array of shape (n, 2) representing n points.\n\n        Returns:\n            A tuple containing:\n            - H_single (float): Final merge height for single linkage.\n            - H_complete (float): Final merge height for complete linkage.\n            - H_average (float): Final merge height for average linkage.\n            - H_centroid (float): Final merge height for centroid linkage.\n            - W (float): Maximum edge weight in the MST.\n            - mst (csr_matrix): The computed Minimum Spanning Tree.\n        \"\"\"\n        n_points = points.shape[0]\n        if n_points  2:\n            return 0.0, 0.0, 0.0, 0.0, 0.0, None\n\n        # Compute pairwise Euclidean distances\n        condensed_dist = pdist(points, 'euclidean')\n        dist_matrix = squareform(condensed_dist)\n\n        # Compute Minimum Spanning Tree (MST)\n        mst = minimum_spanning_tree(dist_matrix)\n        W = mst.max()\n\n        # Perform hierarchical clustering\n        # Single linkage\n        Z_single = linkage(condensed_dist, method='single')\n        H_single = Z_single[-1, 2]\n\n        # Complete linkage\n        Z_complete = linkage(condensed_dist, method='complete')\n        H_complete = Z_complete[-1, 2]\n\n        # Average linkage\n        Z_average = linkage(condensed_dist, method='average')\n        H_average = Z_average[-1, 2]\n\n        # Centroid linkage (requires original data points, not distance matrix)\n        Z_centroid = linkage(points, method='centroid')\n        H_centroid = Z_centroid[-1, 2]\n\n        return H_single, H_complete, H_average, H_centroid, W, mst\n\n    # --- Step 1: Define and analyze the base dataset ---\n    # Cluster A\n    cluster_A = np.array([\n        [-0.2, -0.2],\n        [-0.2,  0.2],\n        [ 0.2, -0.2],\n        [ 0.2,  0.2],\n        [ 0.0,  0.0]\n    ])\n\n    # Cluster B (translation of A by 4 units on x-axis)\n    cluster_B = cluster_A + np.array([4.0, 0.0])\n\n    base_dataset = np.vstack([cluster_A, cluster_B])\n    n_base_points = base_dataset.shape[0]\n\n    # Calculate baseline metrics\n    H_single_0, H_complete_0, H_average_0, H_centroid_0, W_0, _ = analyze_dataset(base_dataset)\n\n    # --- Step 2: Process each test case ---\n    test_cases = [\n        (1.0, 1),\n        (4.0, 2),\n        (8.0, 1),\n        (3.0, 3)\n    ]\n\n    all_results = []\n    for R, s in test_cases:\n        # Generate outlier points\n        y_offsets = 0.2 * (np.arange(s) - (s - 1) / 2.0)\n        outliers = np.zeros((s, 2))\n        outliers[:, 0] = -R\n        outliers[:, 1] = y_offsets\n\n        # Create the new dataset with outliers\n        new_dataset = np.vstack([base_dataset, outliers])\n\n        # Analyze the new dataset\n        H_single_1, H_complete_1, H_average_1, H_centroid_1, W_1, new_mst = analyze_dataset(new_dataset)\n        \n        # Compute the change in metrics (delta values)\n        delta_H_single = H_single_1 - H_single_0\n        delta_H_complete = H_complete_1 - H_complete_0\n        delta_H_average = H_average_1 - H_average_0\n        delta_H_centroid = H_centroid_1 - H_centroid_0\n        delta_W = W_1 - W_0\n\n        # Compute m_new: number of MST edges involving an outlier\n        # Outlier indices start at n_base_points\n        rows, cols = new_mst.nonzero()\n        m_new = np.sum((rows >= n_base_points) | (cols >= n_base_points))\n        \n        # Collect results for the current test case\n        case_results = [\n            delta_H_single,\n            delta_H_complete,\n            delta_H_average,\n            delta_H_centroid,\n            delta_W,\n            int(m_new)\n        ]\n        all_results.append(case_results)\n\n    # --- Step 3: Format the final output string ---\n    result_strings = []\n    for res_list in all_results:\n        # Convert all items to their string representation\n        # The last value m_new is already an int.\n        stringified_list = [str(item) for item in res_list]\n        result_strings.append(f\"[{','.join(stringified_list)}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3140585"}, {"introduction": "真实世界的数据往往不符合理想的高斯分布，而是呈现出“重尾”现象，即存在远离中心的极端值。本练习旨在通过生成和分析重尾合成数据，对比完整连锁和平均连锁方法的稳健性。这个高级实践将帮助您理解为何完整连锁在某些情况下能更好地抵抗由数据尾部引起的过早合并 [@problem_id:3140593]。", "problem": "您需要实现一个层次凝聚聚类实验，以比较不同链接方法在重尾合成数据上的表现。目标是以可复现的方式量化完全链接法（complete linkage）相比于平均链接法（average linkage）如何更好地抵抗跨簇尾部的过早合并，而平均链接法会平滑距离并可能更早地合并。您的程序必须是最终答案部分指定的完整、可运行的程序。\n\n从核心定义出发，使用以下基本基础：\n- 一个使用欧几里得距离的度量空间，其中对于任何点 $x,y \\in \\mathbb{R}^2$，距离为 $d(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$。\n- 层次凝聚聚类（Hierarchical agglomerative clustering, HAC），它在指定的簇间距离下迭代地合并两个最近的簇。簇间距离取决于链接方法：\n  - 完全链接法（Complete linkage）：对于簇 $A$ 和 $B$，距离为 $D_{\\text{complete}}(A,B)=\\max_{x\\in A,y\\in B} d(x,y)$。\n  - 平均链接法（Average linkage）：对于簇 $A$ 和 $B$，距离为 $D_{\\text{average}}(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)$。\n- 从自由度为 $\\nu$ 的学生t分布（Student’s $t$-distribution），记为 $\\mathrm{Student}\\text{-}t(\\nu)$，生成的重尾合成数据，其两个坐标独立采样并按因子 $\\sigma$ 缩放。\n\n对于每个测试用例，按如下方式在 $\\mathbb{R}^2$ 中生成两个簇：\n1. 为左侧簇抽取 $n$ 个点，为右侧簇抽取 $n$ 个点。每个点的坐标都独立地从 $\\mathrm{Student}\\text{-}t(\\nu)$ 分布中采样，然后乘以 $\\sigma$。\n2. 将左侧簇按向量 $(-\\Delta/2, 0)$ 平移，右侧簇按 $(\\Delta/2, 0)$ 平移，其中 $\\Delta$ 是中心间距。\n3. 将左侧簇的点标记为标签 $0$，右侧簇的点标记为标签 $1$。\n4. 使用固定的随机种子以确保可复现性。\n\n对于每种链接方法（完全和平均），在欧几里得度量下执行 HAC。跟踪某个簇首次同时包含标签 $0$ 和 $1$ 时的谱系图合并高度。将这些高度分别表示为 $h_{\\text{complete}}$ 和 $h_{\\text{average}}$。每个测试用例中我们关注的量是浮点数 $h_{\\text{complete}} - h_{\\text{average}}$。正值表示完全链接法比平均链接法在更大的谱系图高度上更能抵抗跨标签的尾部合并。\n\n将上述过程实现为一个程序，并在以下测试套件上运行它，该套件为每个用例指定了 $n$、$\\nu$、$\\sigma$、$\\Delta$ 和随机种子：\n- 测试用例 1（一般重尾分离）：$n=80$, $\\nu=3$, $\\sigma=0.8$, $\\Delta=5.0$, 种子 $=2023$。\n- 测试用例 2（中心更近，尾部更重）：$n=80$, $\\nu=2$, $\\sigma=1.0$, $\\Delta=2.0$, 种子 $=7$。\n- 测试用例 3（大分离，中度重尾）：$n=60$, $\\nu=5$, $\\sigma=0.8$, $\\Delta=8.0$, 种子 $=99$。\n- 测试用例 4（近高斯尾）：$n=100$, $\\nu=25$, $\\sigma=1.0$, $\\Delta=4.0$, 种子 $=123$。\n\n不涉及物理单位或角度单位。所有输出均为不带单位的实数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，严格按照测试用例的顺序，例如 $[r_1,r_2,r_3,r_4]$，其中每个 $r_i$ 是为测试用例 $i$ 计算的 $h_{\\text{complete}} - h_{\\text{average}}$。输出必须是浮点数。", "solution": "所述问题是有效的。这是一个定义明确的计算实验，其基础是统计学习中已确立的原则，特别是层次凝聚聚类（HAC）。该问题是自包含的，所有必要的参数、定义和过程都已明确指定。由于使用了固定的随机种子，该问题是科学合理的、客观的，并且对每个测试用例都能得出唯一、可验证的结果。\n\n目标是量化完全链接法和平均链接法在应用于具有两个不同重尾簇的数据集时的行为差异。核心假设是，完全链接法对簇之间的最大距离敏感，因此与平均链接法相比，在抵抗尾部相互交错的簇的过早合并方面更具鲁棒性，而平均链接法会平均掉这些较大的距离。\n\n解决方案是通过对每个测试用例执行一系列严格定义的步骤来实现的。\n\n**步骤 1：合成数据生成**\n\n对于每个测试用例，我们在 $\\mathbb{R}^2$ 中生成一个由两个不同簇组成的数据集。总点数为 $2n$。\n每个点 $(x_1, x_2)$ 的生成过程如下：\n1.  从自由度为 $\\nu$ 的学生t分布（Student's $t$-distribution），记为 $\\mathrm{Student}\\text{-}t(\\nu)$，中抽取两个独立的随机变量 $z_1$ 和 $z_2$。选择此分布是因为其具有重尾特性，特别是当 $\\nu$ 值较小时。当 $\\nu \\to \\infty$ 时，$\\mathrm{Student}\\text{-}t(\\nu)$ 分布收敛于标准正态分布 $\\mathcal{N}(0,1)$。\n2.  这些变量按因子 $\\sigma$ 进行缩放，得到坐标对 $(\\sigma z_1, \\sigma z_2)$。\n3.  我们为“左”簇生成 $n$ 个这样的点，为“右”簇生成 $n$ 个点。\n4.  为了分离这些簇，左簇的点按向量 $(-\\Delta/2, 0)$ 平移，右簇的点按向量 $(\\Delta/2, 0)$ 平移。这将两个簇的名义中心沿第一坐标轴分置，相距 $\\Delta$。\n5.  源自左簇的点被赋予标签 $0$，源自右簇的点被赋予标签 $1$。这个真实标签用于评估聚类结果。\n通过为每个测试用例初始化具有特定种子的随机数生成器，整个过程变得确定性和可复现。\n\n**步骤 2：层次凝聚聚类（HAC）**\n\nHAC 是一种构建簇层次结构的迭代算法。它从将 $2n$ 个数据点中的每一个都视为一个单点簇开始。在随后的每一步中，两个最近的簇被合并成一个新的、更大的簇。这个过程一直持续到只剩下一个包含所有数据点的簇为止。合并的序列形成一个称为谱系图（dendrogram）的二叉树结构。在谱系图中两个簇合并的“高度”对应于合并发生时的簇间距离。\n\n如何度量两个非单点簇之间距离的选择由链接方法决定。问题指定欧几里得距离，$d(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$，作为单个点 $x, y \\in \\mathbb{R}^2$ 之间的基本度量。\n\n**步骤 3：链接方法**\n\n簇间距离使用两种不同的链接准则计算：\n\n1.  **完全链接法（Complete Linkage）**：两个簇 $A$ 和 $B$ 之间的距离定义为 $A$ 中任意点与 $B$ 中任意点之间的最大距离。\n    $$D_{\\text{complete}}(A,B)=\\max_{x\\in A,y\\in B} d(x,y)$$\n    这种“最远邻”方法是保守的。要使两个簇在此度量下被认为是“近”的，它们的所有点都必须与另一个簇的所有点相对较近。此属性使完全链接法对异常值敏感，并通常产生更紧凑、球形的簇。如果两个主簇的尾部包含相距很远的点，预计它会抵抗合并这两个簇。\n\n2.  **平均链接法（Average Linkage）**：簇 $A$ 和 $B$ 之间的距离是来自每个簇的点之间所有成对距离的平均值。\n    $$D_{\\text{average}}(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)$$\n    此方法提供了一种更“中心”的簇距离度量，有效地平均了少数远距离点的影响。它对异常值的敏感性低于完全链接法，但如果簇在平均意义上很近，即使它们的边界没有很好地分离，也可能导致它们合并。\n\n**步骤 4：量化合并点**\n\n对于每个测试用例和每种链接方法（完全和平均），我们执行 HAC。我们感兴趣的是确定算法首次合并来自两个原始真实标签簇（标签 $0$ 和标签 $1$）的点的精确时刻——由谱系图合并高度表示。\n\n过程如下：\n1.  初始化 $2n$ 个簇，每个数据点一个，并存储它们的真实标签（$0$ 或 $1$）。\n2.  从最低高度到最高高度，遍历 HAC 算法的合并过程。\n3.  每次合并都会形成一个新簇。我们通过取合并的两个较小簇的标签集的并集来确定此新簇中存在的真实标签集。\n4.  当一个新形成的簇首次同时包含标签 $0$ 和标签 $1$ 时，我们记录相应的合并高度。对于完全链接法，设此高度为 $h_{\\text{complete}}$；对于平均链接法，设为 $h_{\\text{average}}$。\n5.  该测试用例最终关注的量是差值 $h_{\\text{complete}} - h_{\\text{average}}$。此差值为正值证实了以下假设：在这种类型的数据上，与平均链接法相比，完全链接法将两个初始簇的分离状态维持到了一个更大的距离阈值。\n\n此完整协议被系统地应用于指定的全部 4 个测试用例，产生 4 个数值结果。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\nfrom scipy.cluster import hierarchy\n\ndef solve():\n    \"\"\"\n    Main function to run the hierarchical clustering experiment on the test suite.\n    \"\"\"\n    # Test cases specify n (points per cluster), nu (degrees of freedom for t-dist),\n    # sigma (scaling factor), Delta (inter-center separation), and a random seed.\n    test_cases = [\n        # (n, nu, sigma, Delta, seed)\n        (80, 3, 0.8, 5.0, 2023), # Test case 1\n        (80, 2, 1.0, 2.0, 7),    # Test case 2\n        (60, 5, 0.8, 8.0, 99),   # Test case 3\n        (100, 25, 1.0, 4.0, 123),  # Test case 4\n    ]\n\n    results = []\n    for n, nu, sigma, delta, seed in test_cases:\n        # Generate the synthetic data for the current test case.\n        X, labels = generate_data(n, nu, sigma, delta, seed)\n\n        # Calculate the first cross-label merge height for complete linkage.\n        h_complete = find_first_cross_label_merge_height(X, labels, 'complete')\n\n        # Calculate the first cross-label merge height for average linkage.\n        h_average = find_first_cross_label_merge_height(X, labels, 'average')\n\n        # The quantity of interest is the difference in merge heights.\n        result = h_complete - h_average\n        results.append(result)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef generate_data(n, nu, sigma, delta, seed):\n    \"\"\"\n    Generates two heavy-tailed clusters in 2D space.\n\n    Args:\n        n (int): Number of points per cluster.\n        nu (float): Degrees of freedom for the Student's t-distribution.\n        sigma (float): Scaling factor for the distribution.\n        delta (float): Separation distance between cluster centers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]:\n            - A NumPy array of shape (2*n, 2) containing the coordinates of all points.\n            - A NumPy array of shape (2*n,) containing the ground-truth labels (0 or 1).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate points for the left cluster from the Student's t-distribution.\n    # The rvs function is used with a generator for reproducible randomness.\n    left_cluster = t.rvs(df=nu, size=(n, 2), random_state=rng) * sigma\n    left_cluster[:, 0] -= delta / 2\n\n    # Generate points for the right cluster.\n    right_cluster = t.rvs(df=nu, size=(n, 2), random_state=rng) * sigma\n    right_cluster[:, 0] += delta / 2\n\n    # Combine points and create corresponding labels.\n    X = np.vstack([left_cluster, right_cluster])\n    labels = np.array([0] * n + [1] * n)\n\n    return X, labels\n\ndef find_first_cross_label_merge_height(X, labels, method):\n    \"\"\"\n    Performs HAC and finds the first merge height that combines points from different labels.\n\n    Args:\n        X (np.ndarray): The data points, shape (N, 2).\n        labels (np.ndarray): The ground-truth labels, shape (N,).\n        method (str): The linkage method ('complete' or 'average').\n\n    Returns:\n        float: The dendrogram height of the first cross-label merge.\n    \"\"\"\n    N = X.shape[0]\n\n    # Perform hierarchical agglomerative clustering using the specified method.\n    # The linkage matrix Z encodes the dendrogram.\n    Z = hierarchy.linkage(X, method=method, metric='euclidean')\n\n    # `cluster_labels` will store the set of original labels for each cluster.\n    # Initially, clusters 0 to N-1 are the original points.\n    cluster_labels = {i: {labels[i]} for i in range(N)}\n\n    # Iterate through each merge recorded in the linkage matrix Z.\n    # Each row `i` represents a merge creating a new cluster with index `N + i`.\n    for i in range(Z.shape[0]):\n        # Get the indices of the two clusters being merged.\n        # Indices  N are original points, indices >= N are newly formed clusters.\n        id1, id2 = int(Z[i, 0]), int(Z[i, 1])\n\n        # Retrieve the sets of original labels for the merging clusters.\n        labels1 = cluster_labels[id1]\n        labels2 = cluster_labels[id2]\n\n        # The new cluster's labels are the union of the merged clusters' labels.\n        new_labels = labels1.union(labels2)\n\n        # The new cluster's index is N + i. We store its label set.\n        cluster_id = N + i\n        cluster_labels[cluster_id] = new_labels\n\n        # Check if the new cluster is \"impure\", i.e., contains both original labels.\n        if 0 in new_labels and 1 in new_labels:\n            # If so, this is the first cross-label merge.\n            # The height of this merge is given in the 3rd column of Z.\n            merge_height = Z[i, 2]\n            return merge_height\n\n    # This part should not be reached in a valid scenario with two labels.\n    # It would imply all points had the same label, which is not the case.\n    return np.inf\n\n# Execute the main function.\nsolve()\n```", "id": "3140593"}]}