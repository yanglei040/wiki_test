{"hands_on_practices": [{"introduction": "理论上，加权最小二乘法 (WLS) 通过为方差较小的观测值赋予较大权重来提高估计效率。然而，当某些权重因理论上的“最优”选择而变得极大时，这在实践中会带来数值不稳定的风险。本练习将引导您通过一个具体案例[@problem_id:3128028]，探讨当误差方差与预测变量的绝对值成正比时，直接使用逆方差作为权重所引发的问题，并演示如何通过引入一个小的稳定化参数来有效解决这一问题。", "problem": "考虑具有异方差误差的线性回归模型，其中响应变量被建模为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，对于 $i = 1, \\dots, n$。假设误差满足 $E[\\varepsilon_i] = 0$ 和 $\\operatorname{Var}(\\varepsilon_i) = \\sigma^2 |x_i|$，其中 $\\sigma^2 > 0$ 是一个未知的比例常数。加权最小二乘法 (WLS) 使用正权重 $w_i$ 来最小化 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$。当 $\\operatorname{Var}(\\varepsilon_i)$ 与 $|x_i|$ 成正比时，一个自然的选择是取 $w_i$ 与 $1/|x_i|$ 成比例，但这可能会过度放大 $x_i$ 接近 $0$ 的观测值并扭曲截距估计。为了缓解这个问题，考虑使用稳定化权重 $w_i = 1 / (|x_i| + \\delta)$，其中 $\\delta \\ge 0$ 是一个稳定化参数。\n\n从线性模型的定义出发，并基于异方差下最佳线性无偏估计量源于使用误差方差的倒数进行适当加权的原理，推导在由 $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$ 隐含的一般对角权重矩阵和对角误差方差结构下截距估计量的方差。从第一性原理出发，不使用问题陈述中提供的快捷公式，解释为什么当某些 $|x_i|$ 非常小时，$w_i = 1/|x_i|$ 的选择会导致截距估计的不稳定性，以及稳定化的选择 $w_i = 1/(|x_i| + \\delta)$ 如何限制接近零的 $x_i$ 的影响。\n\n然后，实现一个程序，对下面指定的每个测试用例，计算以下两个量：\n- WLS 截距估计量方差与普通最小二乘法 (OLS) 截距估计量方差的比率 $r$，其中 OLS 为所有 $i$ 设置 $w_i = 1$，但在评估估计量方差时仍然遵循 $\\operatorname{Var}(\\varepsilon_i)=\\sigma^2|x_i|$。\n- 加权正规矩阵 $X^\\top W X$ 的谱条件数 $\\kappa$，其中 $X$ 是设计矩阵，其第一列为 1，第二列为 $x_i$，$W = \\operatorname{diag}(w_1, \\dots, w_n)$。\n\n你的程序必须：\n- 使用 $X = \\begin{bmatrix} 1  x_1 \\\\ \\vdots  \\vdots \\\\ 1  x_n \\end{bmatrix}$。\n- 使用 $\\Sigma = \\operatorname{diag}(\\sigma^2 |x_1|, \\dots, \\sigma^2 |x_n|)$ 来表示误差方差结构。\n- 仅使用上述模型假设计算 WLS 下的截距方差。\n- 仅使用上述模型假设计算 OLS 下的截距方差。\n- 对所有测试用例设置 $\\sigma^2 = 1$。\n- 将每个报告的浮点数四舍五入到六位小数。\n- 生成一行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，并且本身是一个逗号分隔的对 $[r,\\kappa]$，不含空格。\n\n测试套件（每个测试用例指定 $x_i$ 值的向量和稳定化参数 $\\delta$）：\n1. 正常路径（无零值）：$x = (-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0)$，$\\delta = 0.0$。\n2. 边界敏感性（近零值，无精确零值）：$x = (-2.0, -1.0, -0.5, -10^{-8}, 10^{-8}, 0.1, 0.3, 0.6, 1.0, 2.0)$，$\\delta = 0.0$。\n3. 使用精确零值进行稳定化：$x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$，$\\delta = 0.1$。\n4. 强稳定化：$x = (-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0)$，$\\delta = 0.5$。\n\n答案规格：\n- 对每个测试用例，按上述方式输出对 $[r,\\kappa]$。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的所有测试用例结果。输出字符串中不得有任何空格。", "solution": "该问题陈述是统计学习中的一个有效练习，涉及异方差下加权最小二乘法 (WLS) 估计量的性质。它要求从第一性原理进行理论推导和解释，并通过数值实现来阐释这些概念。问题的所有组成部分都具有科学依据、定义明确且内部一致。\n\n线性模型以矩阵形式给出：$y = X\\beta + \\varepsilon$，其中 $y$ 是一个 $n \\times 1$ 的观测向量，$X$ 是 $n \\times 2$ 的设计矩阵，$\\beta = [\\beta_0, \\beta_1]^\\top$ 是系数向量，$\\varepsilon$ 是 $n \\times 1$ 的误差向量。对误差的假设是 $E[\\varepsilon] = 0$ 和 $\\operatorname{Var}(\\varepsilon) = E[\\varepsilon \\varepsilon^\\top] = \\Sigma$，其中 $\\Sigma$ 是一个对角矩阵，其元素为 $[\\Sigma]_{ii} = \\sigma^2|x_i|$。\n\n加权最小二乘法 (WLS) 估计量 $\\hat{\\beta}_{WLS}$ 是通过最小化加权残差平方和 $S(\\beta) = (y - X\\beta)^\\top W (y - X\\beta)$ 得到的，其中 $W$ 是一个正权重 $w_i$ 构成的对角矩阵。该最小化问题的解由加权正规方程给出：\n$$ (X^\\top W X) \\hat{\\beta}_{WLS} = X^\\top W y $$\n这得到估计量：\n$$ \\hat{\\beta}_{WLS} = (X^\\top W X)^{-1} X^\\top W y $$\n\n该估计量的方差推导如下。由于 $\\hat{\\beta}_{WLS}$ 是 $y$ 的线性函数，其方差为：\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = \\operatorname{Var}((X^\\top W X)^{-1} X^\\top W y) $$\n利用 $\\beta$ 是一个常数向量以及 $\\operatorname{Var}(y) = \\operatorname{Var}(X\\beta + \\varepsilon) = \\operatorname{Var}(\\varepsilon) = \\Sigma$ 这一事实，我们有：\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W) \\operatorname{Var}(y) (X^\\top W)^\\top ((X^\\top W X)^{-1})^\\top $$\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W^\\top X) (X^\\top W X)^{-1} $$\n由于 $W$ 和 $X^\\top W X$ 是对称的，这简化为“三明治”公式：\n$$ \\operatorname{Var}(\\hat{\\beta}_{WLS}) = (X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1} $$\n截距估计量的方差 $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$ 是这个 $2 \\times 2$ 矩阵的第一个对角元素，即 $(1,1)$ 项。这个通用公式对任何正权重 $W$ 的选择都有效，并且不限于 $W \\propto \\Sigma^{-1}$ 的“最优”情况。\n\n现在我们分析权重的选择。\n产生最佳线性无偏估计量 (BLUE) 的 WLS 最优权重与误差方差的倒数成正比，即 $w_i \\propto 1/\\operatorname{Var}(\\varepsilon_i) = 1/(\\sigma^2|x_i|)$。我们选择 $w_i = 1/|x_i|$（这在权重定义中隐含地设置了 $\\sigma^2=1$，这是允许的，因为估计量对于权重的常数缩放是不变的）。在这种特殊情况下，权重矩阵是 $W = (1/\\sigma^2)\\Sigma^{-1}$（如果我们包含 $\\sigma^2$ 项）。让我们看看这对三明治公式的中间项有何影响：\n$$ X^\\top W \\Sigma W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) \\Sigma \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\n以及外侧的项：\n$$ X^\\top W X = X^\\top \\left(\\frac{1}{\\sigma^2}\\Sigma^{-1}\\right) X = \\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X $$\n将这些代入三明治公式得到：\n$$ \\operatorname{Var}(\\hat{\\beta}_{BLUE}) = \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right) \\left(\\frac{1}{\\sigma^2} X^\\top \\Sigma^{-1} X\\right)^{-1} = \\sigma^2 (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top W X)^{-1} \\sigma^2 $$\n（使用 $W = \\Sigma^{-1}/\\sigma^2$ 会得到 $\\sigma^4(X^\\top \\Sigma^{-1} X)^{-1}$ 等。如果我们设置 $W = \\Sigma^{-1}$，则 $\\operatorname{Var} = (X^\\top\\Sigma^{-1}X)^{-1}$）。当选择 $w_i=1/|x_i|$ 且 $\\sigma^2=1$ 时，我们得到 $W = \\Sigma^{-1}$ 且 $\\operatorname{Var}(\\hat{\\beta})=(X^\\top W X)^{-1}$。\n\n当某个观测值 $x_k$ 非常接近 0，而我们使用理论上的最优权重 $w_i = 1/|x_i|$（即 $\\delta=0$）时，就会出现不稳定性。\n1.  **数值不稳定性**：权重 $w_k = 1/|x_k|$ 变得极大。正规矩阵 $X^\\top W X = \\begin{pmatrix} \\sum w_i  \\sum w_i x_i \\\\ \\sum w_i x_i  \\sum w_i x_i^2 \\end{pmatrix}$ 变得病态。$\\sum w_i$ 项由 $w_k$ 主导，使得左上角的元素变得巨大，而其他元素的缩放程度不同。例如，$\\sum w_i x_i^2 = \\sum |x_i|$ 并不由第 $k$ 项主导。这种量级上的差异使得矩阵近似奇异，这可以通过一个非常大的谱条件数 $\\kappa$ 来量化。对此类矩阵求逆在数值上是不稳定的，并且对数据的微小扰动高度敏感。\n2.  **估计扭曲**：WLS 目标函数 $\\sum w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$ 由 $i=k$ 的项主导。为了最小化这个和，回归线被强制要求极度接近点 $(x_k, y_k)$，即 $y_k \\approx \\hat{\\beta}_0 + \\hat{\\beta}_1 x_k$。当 $x_k \\to 0$ 时，这意味着 $\\hat{\\beta}_0 \\approx y_k$。截距的估计几乎完全由单个观测值 $y_k$ 决定，而 $y_k$ 本身的值又受到随机误差的影响。这种对单个数据点影响的极端放大会扭曲截距估计，使其变得不可靠。\n\n带有 $\\delta > 0$ 的稳定化权重 $w_i = 1 / (|x_i| + \\delta)$ 缓解了这个问题。\n当 $x_i \\to 0$ 时，稳定化权重 $w_i \\to 1/\\delta$。因此，权重被 $1/\\delta$ 从上方限定。这防止了任何单个权重变得任意大。因此，没有任何单个观测值可以主导最小二乘拟合。矩阵 $X^\\top W X$ 中的所有元素都保持有界且数量级相当，从而导致更小（更好）的条件数和更稳定、鲁棒的数值解。最终得到的截距估计 $\\hat{\\beta}_0$ 将是所有数据点之间的一个平衡折衷，正如最小二乘法原理所期望的那样。其代价是，这些非最优权重导致的估计量不再是 BLUE，与理论上的（但不稳定的）最优估计量相比，其方差可能会增加。\n\n对于实现部分，我们计算以下量：\n- WLS 截距估计量的方差 $\\operatorname{Var}(\\hat{\\beta}_{0, WLS})$ 是 $(X^\\top W X)^{-1} (X^\\top W \\Sigma W X) (X^\\top W X)^{-1}$ 的 $(1,1)$ 项，其中 $W=\\operatorname{diag}(1/(|x_i|+\\delta))$ 且 $\\Sigma=\\operatorname{diag}(|x_i|)$（$\\sigma^2=1$）。\n- OLS 截距估计量的方差 $\\operatorname{Var}(\\hat{\\beta}_{0, OLS})$ 使用相同的一般三明治公式计算，但所有 $i$ 的权重 $w_i=1$。因此，我们设置 $W_{OLS}=I$（单位矩阵）。其方差是 $(X^\\top X)^{-1} (X^\\top \\Sigma X) (X^\\top X)^{-1}$ 的 $(1,1)$ 项。即使 OLS 估计量本身不使用此信息，这种方法也能正确地考虑真实的异方差误差结构。\n- 比率是 $r = \\operatorname{Var}(\\hat{\\beta}_{0, WLS}) / \\operatorname{Var}(\\hat{\\beta}_{0, OLS})$。\n- 条件数是 $\\kappa = \\kappa(X^\\top W X)$。\n\n程序将对每个指定的测试用例执行这些计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the ratio of WLS to OLS intercept variance and the condition number\n    of the weighted normal matrix for several test cases of heteroscedastic linear regression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -1e-8, 1e-8, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.0},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.1},\n        {\"x\": np.array([-2.0, -1.0, -0.5, -0.2, 0.0, 0.05, 0.1, 0.3, 0.6, 1.0, 2.0]), \"delta\": 0.5},\n    ]\n\n    results = []\n    \n    # Set the unknown constant of proportionality to 1 as specified.\n    sigma_squared = 1.0\n\n    for case in test_cases:\n        x_vec = case[\"x\"]\n        delta = case[\"delta\"]\n        n = len(x_vec)\n\n        # Construct the design matrix X\n        X = np.vstack((np.ones(n), x_vec)).T\n\n        # Construct the true error covariance matrix Sigma\n        # Var(epsilon_i) = sigma^2 * |x_i|\n        Sigma = np.diag(sigma_squared * np.abs(x_vec))\n\n        # --- WLS Calculation ---\n        # Construct the WLS weight matrix W\n        # w_i = 1 / (|x_i| + delta)\n        # Handle the case where |x_i| + delta is zero to avoid division by zero.\n        w_vals_denom = np.abs(x_vec) + delta\n        # If a denominator is zero (only if x_i=0 and delta=0), the weight should be infinite.\n        # We cap it at a very large number to maintain numerical stability, although\n        # the problem is designed to show instability.\n        w_vals = np.divide(1.0, w_vals_denom, where=w_vals_denom!=0, out=np.full_like(w_vals_denom, 1e18))\n        W = np.diag(w_vals)\n\n        # Calculate the weighted normal matrix for WLS\n        X_T_W_X = X.T @ W @ X\n        \n        # Calculate the \"sandwich\" middle part for WLS\n        X_T_W_Sigma_W_X = X.T @ W @ Sigma @ W @ X\n\n        # Calculate the variance-covariance matrix for the WLS estimator\n        # Var(beta_hat_WLS) = (X'WX)^-1 (X'W Sigma WX) (X'WX)^-1\n        X_T_W_X_inv = np.linalg.inv(X_T_W_X)\n        var_beta_wls = X_T_W_X_inv @ X_T_W_Sigma_W_X @ X_T_W_X_inv\n\n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_wls = var_beta_wls[0, 0]\n\n        # Calculate the spectral condition number of the weighted normal matrix\n        kappa = np.linalg.cond(X_T_W_X)\n\n        # --- OLS Calculation ---\n        # For OLS, the weights w_i are all 1.\n        # The estimator is (X'X)^-1 X'y, and its variance must be calculated\n        # using the true heteroscedastic covariance matrix Sigma.\n        X_T_X = X.T @ X\n        \n        # OLS \"sandwich\" middle part\n        X_T_Sigma_X = X.T @ Sigma @ X\n\n        # Calculate the variance-covariance matrix for the OLS estimator\n        # Var(beta_hat_OLS) = (X'X)^-1 (X' Sigma X) (X'X)^-1\n        X_T_X_inv = np.linalg.inv(X_T_X)\n        var_beta_ols = X_T_X_inv @ X_T_Sigma_X @ X_T_X_inv\n        \n        # Extract variance of the intercept estimator (beta_0)\n        var_b0_ols = var_beta_ols[0, 0]\n\n        # --- Ratio Calculation ---\n        # Ratio of WLS intercept variance to OLS intercept variance\n        if var_b0_ols == 0:\n            # Handle potential division by zero, though unlikely in these cases\n            r = np.inf if var_b0_wls > 0 else 0.0\n        else:\n            r = var_b0_wls / var_b0_ols\n        \n        results.append((r, kappa))\n\n    # Format the final output string as specified, with 6 decimal places and no spaces\n    output_str = \",\".join([f\"[{r:.6f},{k:.6f}]\" for r, k in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3128028"}, {"introduction": "面对异方差性，加权最小二乘法并非唯一的解决方案。在某些情况下，例如当误差是乘性而非加性时，对响应变量进行数据转换（如对数变换）同样可以有效地稳定方差。本练习[@problem_id:3128007]将让您扮演数据科学家的角色，对这两种策略进行正面比较，通过评估残差诊断指标和在测试集上的预测准确性，来判断在特定场景下哪种方法更为优越。", "problem": "在存在由乘性噪声引起的异方差性的情况下，您需要比较两种回归策略。基本设定是一个单变量线性预测器，其严格为正的响应由一个乘性噪声模型生成。需要使用的基本知识包括：线性回归及其平方和目标的定义、异方差性作为观测间误差方差非恒定的概念，以及当方差取决于响应的大小时，加权最小二乘法 (WLS) 通过最小化加权残差平方和来稳定方差的原理。您必须实现这两种策略，评估残差诊断，并评估预测准确性。\n\n每个测试用例的数据生成过程：\n- 对于每个观测值 $i \\in \\{1, \\dots, n\\}$，预测变量 $x_i$ 从区间 $[0, L]$ 上的均匀分布中独立同分布地抽取。\n- 噪声 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 在 $i$ 之间独立抽取。\n- 生成响应 $y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\varepsilon_i)$，这会在原始尺度上引入乘性噪声，在对数尺度上引入加性噪声。\n\n需要实现的建模策略：\n1. 对数转换模型（在对数尺度上使用普通最小二乘法 (OLS)）：\n   - 通过最小化对数尺度上的非加权平方和，在训练分割上拟合线性模型 $\\log y_i = \\gamma_0 + \\gamma_1 x_i + e_i$。\n   - 对于原始响应尺度上的预测，使用 Duan 的涂抹估计：对于训练残差 $r_j = \\log y_j - (\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_j)$，定义涂抹因子 $S = \\frac{1}{n_{\\text{train}}}\\sum_{j=1}^{n_{\\text{train}}} \\exp(r_j)$。通过 $\\hat{y}_i^{\\text{log}} = \\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_i) \\cdot S$ 在原始尺度上进行预测。\n\n2. 原始尺度上的加权最小二乘法 (WLS)：\n   - 通过最小化权重为 $w_i = 1 / y_i^2$ 的加权平方和，在训练分割上拟合线性模型 $y_i = \\beta_0 + \\beta_1 x_i + \\eta_i$。\n\n残差诊断：\n- 对于每个拟合模型，计算其工作尺度上的训练残差和拟合值。\n- 将异方差性指数 $H$ 定义为绝对残差与相同尺度上拟合值之间的绝对皮尔逊相关系数，即 $H = \\left|\\operatorname{corr}\\left(|r_i|, \\hat{m}_i\\right)\\right|$，其中 $r_i$ 是残差，$\\hat{m}_i$ 是拟合值。较小的 $H$ 值表示更好的残差诊断结果。\n\n预测准确性：\n- 使用在测试分割上计算的原始响应尺度上的均方根误差 (RMSE)，即 $\\operatorname{RMSE} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_i - \\hat{y}_i)^2}$。\n- 对于对数转换模型，使用上面定义的涂抹反变换；对于 WLS 模型，直接使用其在原始尺度上的拟合预测值。\n\n训练/测试分割：\n- 对于每个测试用例，按训练比例 $p_{\\text{train}}$ 将数据随机分割为训练集和测试集。使用提供的随机种子以确保可复现性。\n\n比较规则和最终输出：\n- 对于残差诊断，每个测试用例返回一个整数 $a$，如果对数转换模型的 $H$ 严格小于 WLS 模型，则 $a=0$；如果 WLS 模型的 $H$ 严格小于对数转换模型，则 $a=1$；如果在容差 $\\tau$ 内相等，则 $a=2$。\n- 对于预测准确性，每个测试用例返回一个整数 $b$，如果对数转换模型的 $\\operatorname{RMSE}$ 严格小于 WLS 模型，则 $b=0$；如果 WLS 模型的 $\\operatorname{RMSE}$ 严格小于对数转换模型，则 $b=1$；如果在相同的容差 $\\tau$ 内相等，则 $b=2$。\n- 使用容差 $\\tau = 10^{-8}$。\n\n测试套件参数：\n- 案例 1：$n=200$，$\\beta_0=1.0$，$\\beta_1=0.8$，$\\sigma=0.5$， $L=3.0$，$p_{\\text{train}}=0.7$，seed $=42$。\n- 案例 2：$n=200$，$\\beta_0=1.0$，$\\beta_1=0.8$，$\\sigma=0.05$，$L=3.0$，$p_{\\text{train}}=0.7$，seed $=202$。\n- 案例 3：$n=60$，$\\beta_0=0.5$，$\\beta_1=1.2$，$\\sigma=0.6$， $L=2.0$，$p_{\\text{train}}=0.7$，seed $=7$。\n- 案例 4：$n=400$，$\\beta_0=0.2$，$\\beta_1=0.9$，$\\sigma=0.9$，$L=4.0$，$p_{\\text{train}}=0.7$，seed $=99$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试用例贡献一个双整数列表 $[a,b]$，顺序与测试套件中的顺序相同。例如，一个有效的输出格式为 $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$。此问题不涉及任何物理单位或角度，也没有百分比输出；所有计算量均为无量纲量。", "solution": "该问题是有效的。它在科学上基于统计回归建模的既定原则，特别是解决了异方差性问题。问题陈述清晰，提供了一套完整且一致的指令、数据生成过程和评估指标，确保每个测试用例都有唯一且可验证的解。语言客观、正式。\n\n解决方法论涉及为一个因乘性误差结构而表现出异方差性的数据集比较两种回归策略。对于每个测试用例，我们首先生成数据，然后将其分割为训练集和测试集。接着，我们应用两种不同的建模方法：对对数转换后的数据使用普通最小二乘法 (OLS)，以及对原始数据使用加权最小二乘法 (WLS)。最后，我们基于残差诊断（异方差性指数）和预测准确性（均方根误差）来评估和比较这两个模型。\n\n**1. 数据生成与分割**\n\n对于 $n$ 个观测中的每一个，预测变量 $x_i$ 从一个均匀分布中抽取：\n$$x_i \\sim \\mathcal{U}(0, L)$$\n响应变量 $y_i$ 由一个对数正态模型生成，其中噪声在对数尺度上是加性的：\n$$y_i = \\exp(\\beta_0 + \\beta_1 x_i + \\varepsilon_i), \\quad \\text{其中 } \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$$\n这个过程意味着，在原始尺度上，误差是乘性的，且 $y_i$ 的方差在不同的 $x_i$ 值上不是恒定的，这种情况被称为异方差性。具体来说，$\\operatorname{Var}(y_i|x_i)$ 与 $(\\mathbb{E}[y_i|x_i])^2$ 成正比。\n\n然后，根据比例 $p_{\\text{train}}$，将生成的规模为 $n$ 的数据集划分为训练集和测试集。固定的随机种子确保了此分割的可复现性。设 $n_{\\text{train}} = \\lfloor n \\cdot p_{\\text{train}} \\rfloor$ 和 $n_{\\text{test}} = n - n_{\\text{train}}$。\n\n**2. 策略 1：对数转换普通最小二乘法 (OLS)**\n\n该方法首先通过对响应变量应用对数变换来线性化关系并稳定方差。拟合到训练数据的模型是：\n$$\\log y_i = \\gamma_0 + \\gamma_1 x_i + e_i$$\n这是一个标准的线性模型，其参数 $\\boldsymbol{\\gamma} = [\\gamma_0, \\gamma_1]^T$ 通过最小化误差平方和 $\\sum e_i^2$ 使用 OLS 进行估计。解由正规方程给出：\n$$\\hat{\\boldsymbol{\\gamma}} = (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^T \\mathbf{z}_{\\text{train}}$$\n其中 $\\mathbf{z}_{\\text{train}}$ 是对数转换后的训练响应向量，即 $(\\log y_1, \\dots, \\log y_{n_{\\text{train}}})^T$，而 $\\mathbf{X}_{\\text{train}}$ 是一个 $n_{\\text{train}} \\times 2$ 的设计矩阵，包含一列全为 1 的列和一列 $x_i$ 值。\n\n对于残差诊断，我们使用训练数据。拟合值为 $\\hat{\\mathbf{z}}_{\\text{train}} = \\mathbf{X}_{\\text{train}} \\hat{\\boldsymbol{\\gamma}}$，残差为 $\\mathbf{r}_{\\text{log}} = \\mathbf{z}_{\\text{train}} - \\hat{\\mathbf{z}}_{\\text{train}}$。异方差性指数计算如下：\n$$H_{\\text{log}} = \\left|\\operatorname{corr}\\left(|\\mathbf{r}_{\\text{log}}|, \\hat{\\mathbf{z}}_{\\text{train}}\\right)\\right|$$\n\n对于预测，我们必须将预测值从对数尺度反变换回原始尺度。一个朴素的反变换 $\\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_i)$ 会产生偏差。我们使用 Duan 的涂抹估计来纠正这一点。涂抹因子 $S$ 从训练残差计算得出：\n$$S = \\frac{1}{n_{\\text{train}}}\\sum_{j=1}^{n_{\\text{train}}} \\exp(r_{\\text{log}, j})$$\n测试集在原始尺度上的预测值为：\n$$\\hat{y}_i^{\\text{log}} = \\exp(\\hat{\\gamma}_0 + \\hat{\\gamma}_1 x_{\\text{test}, i}) \\cdot S$$\n预测准确性通过测试集上的均方根误差 (RMSE) 来衡量：\n$$\\operatorname{RMSE}_{\\text{log}} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_i^{\\text{log}})^2}$$\n\n**3. 策略 2：加权最小二乘法 (WLS)**\n\n该策略直接在原始尺度上对线性关系 $y_i = \\beta_0 + \\beta_1 x_i + \\eta_i$ 进行建模，但通过为每个观测值分配与其方差成反比的权重来解决异方差性问题。鉴于 $\\operatorname{Var}(y_i|x_i)$ 近似与 $y_i^2$ 成正比，我们使用权重 $w_i = 1/y_i^2$。WLS 最小化加权残差平方和 $\\sum w_i \\eta_i^2$。参数估计 $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1]^T$ 通过求解以下方程得到：\n$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}_{\\text{train}}^T \\mathbf{W} \\mathbf{X}_{\\text{train}})^{-1} \\mathbf{X}_{\\text{train}}^T \\mathbf{W} \\mathbf{y}_{\\text{train}}$$\n其中 $\\mathbf{W}$ 是一个对角矩阵，其对角线上的元素为权重 $w_i = 1/y_{\\text{train}, i}^2$。\n\n残差诊断在原始尺度上进行。拟合值为 $\\hat{\\mathbf{y}}_{\\text{wls, train}} = \\mathbf{X}_{\\text{train}} \\hat{\\boldsymbol{\\beta}}$，残差为 $\\mathbf{r}_{\\text{wls}} = \\mathbf{y}_{\\text{train}} - \\hat{\\mathbf{y}}_{\\text{wls, train}}$。异方差性指数为：\n$$H_{\\text{wls}} = \\left|\\operatorname{corr}\\left(|\\mathbf{r}_{\\text{wls}}|, \\hat{\\mathbf{y}}_{\\text{wls, train}}\\right)\\right|$$\n\n测试集的预测值直接使用拟合模型得出：\n$$\\hat{y}_i^{\\text{wls}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{\\text{test}, i}$$\n测试集上相应的 RMSE 为：\n$$\\operatorname{RMSE}_{\\text{wls}} = \\sqrt{\\frac{1}{n_{\\text{test}}}\\sum_{i=1}^{n_{\\text{test}}} (y_{\\text{test}, i} - \\hat{y}_i^{\\text{wls}})^2}$$\n\n**4. 比较**\n\n对于每个测试用例，使用容差 $\\tau = 10^{-8}$ 在两个标准上对两个模型进行比较。\n- **残差诊断 (输出 $a$)**：\n  - 如果 $H_{\\text{log}}  H_{\\text{wls}} - \\tau$，则 $a=0$。\n  - 如果 $H_{\\text{wls}}  H_{\\text{log}} - \\tau$，则 $a=1$。\n  - 否则，$a=2$。\n- **预测准确性 (输出 $b$)**：\n  - 如果 $\\operatorname{RMSE}_{\\text{log}}  \\operatorname{RMSE}_{\\text{wls}} - \\tau$，则 $b=0$。\n  - 如果 $\\operatorname{RMSE}_{\\text{wls}}  \\operatorname{RMSE}_{\\text{log}} - \\tau$，则 $b=1$。\n  - 否则，$b=2$。\n\n最终输出是所有指定测试用例的 $[a, b]$ 对的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the comparison for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case 1: n=200, beta0=1.0, beta1=0.8, sigma=0.5, L=3.0, p_train=0.7, seed=42\n        (200, 1.0, 0.8, 0.5, 3.0, 0.7, 42),\n        # Case 2: n=200, beta0=1.0, beta1=0.8, sigma=0.05, L=3.0, p_train=0.7, seed=202\n        (200, 1.0, 0.8, 0.05, 3.0, 0.7, 202),\n        # Case 3: n=60, beta0=0.5, beta1=1.2, sigma=0.6, L=2.0, p_train=0.7, seed=7\n        (60, 0.5, 1.2, 0.6, 2.0, 0.7, 7),\n        # Case 4: n=400, beta0=0.2, beta1=0.9, sigma=0.9, L=4.0, p_train=0.7, seed=99\n        (400, 0.2, 0.9, 0.9, 4.0, 0.7, 99),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(*case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _solve_case(n, beta_0, beta_1, sigma, L, p_train, seed, tau=1e-8):\n    \"\"\"\n    Solves a single test case for the heteroscedasticity problem.\n    \"\"\"\n    # 1. Generate Data\n    np.random.seed(seed)\n    x = np.random.uniform(0, L, n)\n    # Note: np.random.normal's scale parameter is standard deviation (sigma), not variance (sigma^2)\n    eps = np.random.normal(0, sigma, n)\n    y = np.exp(beta_0 + beta_1 * x + eps)\n\n    # 2. Split Data\n    n_train = int(np.floor(n * p_train))\n    indices = np.arange(n)\n    np.random.shuffle(indices)\n    train_indices = indices[:n_train]\n    test_indices = indices[n_train:]\n\n    x_train, y_train = x[train_indices], y[train_indices]\n    x_test, y_test = x[test_indices], y[test_indices]\n    \n    # --- Strategy 1: Log-transform model (OLS on log scale) ---\n    log_y_train = np.log(y_train)\n    X_train = np.vstack([np.ones(n_train), x_train]).T\n    \n    # Fit using OLS normal equations\n    gamma_hat = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ log_y_train\n    \n    # Residual diagnostics\n    log_y_fit_train = X_train @ gamma_hat\n    res_log = log_y_train - log_y_fit_train\n    \n    # Correlation for H_log. Handle constant array case for robustness.\n    if np.std(np.abs(res_log))  1e-12 or np.std(log_y_fit_train)  1e-12:\n        corr_log = 0.0\n    else:\n        corr_log = np.corrcoef(np.abs(res_log), log_y_fit_train)[0, 1]\n    H_log = np.abs(corr_log)\n    \n    # Prediction and RMSE with Duan's smearing\n    smearing_factor = np.mean(np.exp(res_log))\n    n_test = len(x_test)\n    X_test = np.vstack([np.ones(n_test), x_test]).T\n    log_y_pred_test = X_test @ gamma_hat\n    y_pred_log = np.exp(log_y_pred_test) * smearing_factor\n    RMSE_log = np.sqrt(np.mean((y_test - y_pred_log)**2))\n\n    # --- Strategy 2: Weighted Least Squares (WLS) on original scale ---\n    weights = 1 / y_train**2\n    W_diag = np.diag(weights)\n    \n    # Fit using WLS normal equations\n    X_train_T_W = X_train.T @ W_diag\n    beta_hat = np.linalg.inv(X_train_T_W @ X_train) @ (X_train_T_W @ y_train)\n\n    # Residual diagnostics\n    y_fit_wls_train = X_train @ beta_hat\n    res_wls = y_train - y_fit_wls_train\n\n    # Correlation for H_wls. Handle constant array case for robustness.\n    if np.std(np.abs(res_wls))  1e-12 or np.std(y_fit_wls_train)  1e-12:\n        corr_wls = 0.0\n    else:\n        corr_wls = np.corrcoef(np.abs(res_wls), y_fit_wls_train)[0, 1]\n    H_wls = np.abs(corr_wls)\n\n    # Prediction and RMSE\n    y_pred_wls = X_test @ beta_hat\n    RMSE_wls = np.sqrt(np.mean((y_test - y_pred_wls)**2))\n\n    # --- Comparison ---\n    # a: Residual diagnostics comparison\n    if H_log  H_wls - tau:\n        a = 0\n    elif H_wls  H_log - tau:\n        a = 1\n    else:\n        a = 2\n        \n    # b: Predictive accuracy comparison\n    if RMSE_log  RMSE_wls - tau:\n        b = 0\n    elif RMSE_wls  RMSE_log - tau:\n        b = 1\n    else:\n        b = 2\n\n    return [a, b]\n\nsolve()\n```", "id": "3128007"}, {"introduction": "在前面的练习中，我们都假设了异方差的具体形式是已知的。然而在现实世界的问题中，方差的结构本身通常是未知的，需要从数据中进行估计。本练习[@problem_id:3128024]将介绍一种强大的通用技术——可行加权最小二乘法 (Feasible WLS)，它通过一个两步过程来解决这个问题：首先利用初步的 OLS 残差来建立方差模型，然后使用该模型预测的方差来进行加权回归。您还将通过此练习检验该方法对高杠杆率离群值的敏感性，从而更深刻地理解稳健性的重要。", "problem": "给定一个包含单个预测变量和异方差误差结构的简单线性回归模型。对于索引为 $i = 1, \\dots, n$ 的观测值，假设数据根据以下方式生成 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i,$ 且 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\lvert x_i \\rvert^\\alpha,$ 其中 $\\sigma^2 > 0$ 且 $\\alpha \\in \\mathbb{R}$。您的任务是，通过使用初始普通最小二乘 (OLS) 拟合的残差从数据中估计未知指数 $\\alpha$，来推导并实现一个可行的加权最小二乘 (WLS) 估计量，然后检验该程序对预测变量 $x$ 中离群点的敏感性。\n\n基本原理：\n- 普通最小二乘法 (OLS) 最小化 $\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_i)^2$，并求解由一阶最优性条件产生的正规方程。\n- 加权最小二乘法 (WLS) 最小化 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$，其中权重 $\\{w_i\\}$ 是预先指定的，并求解加权正规方程。对于已知方差的异方差模型，最优权重与条件方差成反比。\n- 如果方差模型未知，但已参数化指定到 $\\alpha$，一个可行的 WLS 程序会从数据中估计 $\\alpha$，并将该估计值代入权重中。\n\n实现要求：\n1. 对每个测试用例，使用以下协议确定性地生成数据。\n   - 使用指定的伪随机数生成器种子，从标准正态分布 $N(0,1)$ 中独立抽取 $x_i$。\n   - 使用与抽取 $x_i$ 无关的不同种子，从标准正态分布 $N(0,1)$ 中独立抽取 $z_i$。\n   - 定义一个稳定化的幅值函数 $m(x) = \\max(\\lvert x \\rvert, \\delta)$，其中 $\\delta = 10^{-8}$ 以确保数值稳定性。在所有与方差相关的计算（包括数据生成、权重构建和辅助回归）中，凡是出现 $\\lvert x \\rvert$ 的地方都使用 $m(x)$，以避免如 $\\log 0$ 等未定义的操作，并在处理负 $\\alpha$ 时避免奇异点。\n   - 生成误差 $\\varepsilon_i = \\sigma \\, m(x_i)^{\\alpha/2} z_i$，使得 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$ 稳定地逼近目标形式。\n   - 生成响应 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$。\n2. 对于每个数据集（无离群点）：\n   - 拟合 OLS 以获得残差 $r_i = y_i - \\hat{\\beta}_0^{\\text{OLS}} - \\hat{\\beta}_1^{\\text{OLS}} x_i$。\n   - 通过对 $\\log(r_i^2)$ 进行关于截距和 $\\log(m(x_i))$ 的辅助 OLS 回归来估计 $\\alpha$，即，将 $t_i = \\log(\\max(r_i^2,\\delta))$ 回归到 $1$ 和 $z_i = \\log(m(x_i))$ 上。将斜率估计值记为 $\\hat{\\alpha}$。\n   - 构建 WLS 权重 $w_i = m(x_i)^{-\\hat{\\alpha}}$ 并计算 $\\beta_1$ 的 WLS 估计值，记为 $\\hat{\\beta}_1^{\\text{WLS}}$。\n3. $x$ 中的离群点敏感性分析：\n   - 创建原始预测变量向量的副本，并将索引 $j = \\lfloor n/2 \\rfloor$ 处的一个条目替换为大小为 $s \\cdot s_x$ 的离群点，其中 $s$ 是给定的离群点尺度，$s_x$ 是原始 $x$ 的样本标准差。\n   - 使用与上述相同的噪声样本 $\\{z_i\\}$，通过 $\\varepsilon_i^{\\text{out}} = \\sigma \\, m(x_i^{\\text{out}})^{\\alpha/2} z_i$ 从更新后的 $x$ 重新生成 $\\varepsilon_i$，并设置 $y_i^{\\text{out}} = \\beta_0 + \\beta_1 x_i^{\\text{out}} + \\varepsilon_i^{\\text{out}}$。\n   - 重复可行的 WLS 程序以获得 $\\hat{\\alpha}^{\\text{out}}$ 和 $\\hat{\\beta}_1^{\\text{WLS,out}}$。\n   - 报告定义为 $\\Delta_\\alpha = \\lvert \\hat{\\alpha}^{\\text{out}} - \\hat{\\alpha} \\rvert$ 和 $\\Delta_{\\beta_1} = \\lvert \\hat{\\beta}_1^{\\text{WLS,out}} - \\hat{\\beta}_1^{\\text{WLS}} \\rvert$ 的敏感性度量指标。\n4. 数值报告要求：\n   - 对于每个测试用例，报告一个包含六个浮点数的列表，保留四位小数：\n     - $\\hat{\\alpha}$，\n     - $\\hat{\\beta}_1^{\\text{WLS}}$，\n     - $\\hat{\\alpha}^{\\text{out}}$，\n     - $\\hat{\\beta}_1^{\\text{WLS,out}}$，\n     - $\\Delta_\\alpha$，\n     - $\\Delta_{\\beta_1}$。\n5. 随机性与可复现性：\n   - 在所有测试用例中使用相同的基础种子，但通过添加下面测试套件中指定的固定偏移量来派生不同的种子，以保证确定性和可复现的输出。\n   - 对于测试用例索引 $k$（从 0 开始），使用种子 $s_x = s_0 + k$ 生成 $\\{x_i\\}$，使用种子 $s_z = s_0 + 100k$ 生成 $\\{z_i\\}$，其中 $s_0 = 2757$。\n\n测试套件：\n为以下四个参数集提供结果。对每个参数集，都按照上述描述的注入 $x$ 离群点前后的完整程序进行操作。\n- 测试 1 (理想情况，方差随 $\\lvert x \\rvert$ 递增)：$n = 200$, $\\beta_0 = 1.0$, $\\beta_1 = 2.0$, $\\sigma = 1.0$, $\\alpha = 1.0$，离群点尺度 $s = 25.0$。\n- 测试 2 (同方差边界情况)：$n = 200$, $\\beta_0 = 0.5$, $\\beta_1 = -1.5$, $\\sigma = 1.0$, $\\alpha = 0.0$，离群点尺度 $s = 25.0$。\n- 测试 3 (方差随 $\\lvert x \\rvert$ 递减)：$n = 200$, $\\beta_0 = 0.0$, $\\beta_1 = 1.0$, $\\sigma = 1.0$, $\\alpha = -1.0$，离群点尺度 $s = 25.0$。\n- 测试 4 (小样本，中度异方差性)：$n = 30$, $\\beta_0 = 1.0$, $\\beta_1 = 0.5$, $\\sigma = 0.5$, $\\alpha = 0.5$，离群点尺度 $s = 50.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的、以逗号分隔的结果列表。每个元素对应一个测试用例，本身也是一个列表，顺序为\n$[\\hat{\\alpha}, \\hat{\\beta}_1^{\\text{WLS}}, \\hat{\\alpha}^{\\text{out}}, \\hat{\\beta}_1^{\\text{WLS,out}}, \\Delta_\\alpha, \\Delta_{\\beta_1}]$，\n所有条目均保留四位小数。例如：\n[$[\\cdots]$, $[\\cdots]$, $[\\cdots]$, $[\\cdots]$]。", "solution": "所提出的问题要求推导并实现一个可行的加权最小二乘 (WLS) 程序，以处理简单线性回归模型中的异方差性。此任务的核心是估计控制方差结构的参数，然后分析此估计程序对预测变量中高杠杆率离群点的敏感性。\n\n数据从以下模型生成：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i, \\quad i = 1, \\dots, n\n$$\n其中误差 $\\varepsilon_i$ 是异方差的，其条件方差结构为：\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 \\lvert x_i \\rvert^\\alpha\n$$\n这里，$\\beta_0$ 和 $\\beta_1$ 是回归系数，$\\sigma^2 > 0$ 是一个方差尺度参数，而 $\\alpha \\in \\mathbb{R}$ 是一个指数，它决定了误差方差如何随预测变量 $x_i$ 的大小而变化。为了数值稳定性，特别是在 $x_i=0$ 或 $\\alpha$ 为负值时，我们采用一个稳定化的幅值函数 $m(x) = \\max(\\lvert x \\rvert, \\delta)$，其中 $\\delta$ 是一个小的正常数，因此操作上的方差模型为 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$。\n\n在矩阵表示法中，模型为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $\\mathbf{y}$ 是 $n \\times 1$ 的响应向量，$\\mathbf{X}$ 是 $n \\times 2$ 的设计矩阵，其列分别为截距项和预测变量 $x_i$，$\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$ 是系数向量，$\\boldsymbol{\\varepsilon}$ 是 $n \\times 1$ 的误差向量。给定 $\\mathbf{X}$，误差的协方差矩阵为 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon} \\mid \\mathbf{X}) = \\boldsymbol{\\Omega}$，这是一个对角矩阵，其对角元素为 $(\\boldsymbol{\\Omega})_{ii} = \\sigma^2 m(x_i)^\\alpha$。\n\n如果误差方差已知，$\\boldsymbol{\\beta}$ 的最优估计量将是加权最小二乘 (WLS) 估计量，它最小化加权残差平方和 $\\sum_{i=1}^n w_i (y_i - \\beta_0 - \\beta_1 x_i)^2$。最优权重 $w_i$ 与方差成反比，即 $w_i \\propto (\\operatorname{Var}(\\varepsilon_i \\mid x_i))^{-1}$。我们可以设置权重 $w_i = m(x_i)^{-\\alpha}$。WLS 估计量由以下公式给出：\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\n其中 $\\mathbf{W} = \\operatorname{diag}(w_1, \\dots, w_n)$ 是权重的对角矩阵。\n\n然而，参数 $\\alpha$ 是未知的，必须从数据中估计。这导致了一个称为可行 WLS (或可行广义最小二乘法, FGLS) 的多阶段程序。\n\n**第一步：初始普通最小二乘 (OLS) 估计**\n首先，我们忽略异方差性并计算 $\\boldsymbol{\\beta}$ 的 OLS 估计量：\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n在异方差性下，OLS 估计量仍然是无偏的，但不再是最佳线性无偏估计量 (BLUE)；其标准误也是有偏的。然后我们计算 OLS 残差 $r_i = y_i - (\\hat{\\beta}_0^{\\text{OLS}} + \\hat{\\beta}_1^{\\text{OLS}} x_i)$，这些残差可作为不可观测的真实误差 $\\varepsilon_i$ 的代理变量。\n\n**第二步：用于估计 $\\alpha$ 的辅助回归**\n可行程序的核心是使用残差来估计未知的方差参数 $\\alpha$。方差模型为 $\\operatorname{Var}(\\varepsilon_i \\mid x_i) = \\sigma^2 m(x_i)^\\alpha$。取自然对数可得到一个线性关系：\n$$\n\\log(\\operatorname{Var}(\\varepsilon_i \\mid x_i)) = \\log(\\sigma^2) + \\alpha \\log(m(x_i))\n$$\n由于 $E[r_i^2 \\mid x_i]$ 近似于 $\\operatorname{Var}(\\varepsilon_i \\mid x_i)$，我们可以建立一个辅助回归模型。根据问题规范，我们将 $t_i = \\log(\\max(r_i^2, \\delta))$ 回归到一个截距和 $z_i = \\log(m(x_i))$ 上：\n$$\nt_i = \\gamma_0 + \\gamma_1 z_i + u_i\n$$\n其中 $\\gamma_0$ 是 $\\log(\\sigma^2)$ 的估计值加上一个与 $\\log(\\chi^2_1)$ 噪声期望相关的项，而 $\\gamma_1$ 是 $\\alpha$ 的估计值。我们使用 OLS 拟合这个辅助模型，并将斜率估计值作为我们对 $\\alpha$ 的估计，记为 $\\hat{\\alpha}$。\n\n**第三步：使用估计权重进行 WLS**\n利用估计值 $\\hat{\\alpha}$，我们构建可行权重：\n$$\n\\hat{w}_i = m(x_i)^{-\\hat{\\alpha}}\n$$\n我们构成估计的权重矩阵 $\\hat{\\mathbf{W}} = \\operatorname{diag}(\\hat{w}_1, \\dots, \\hat{w}_n)$ 并计算可行的 WLS 估计量：\n$$\n\\hat{\\boldsymbol{\\beta}}^{\\text{FWLS}} = (\\mathbf{X}^T \\hat{\\mathbf{W}} \\mathbf{X})^{-1} \\mathbf{X}^T \\hat{\\mathbf{W}} \\mathbf{y}\n$$\n我们提取此向量的第二个分量作为我们的最终估计值 $\\hat{\\beta}_1^{\\text{WLS}}$。\n\n**离群点敏感性分析**\n问题要求分析该程序对预测变量中高杠杆点的敏感性。通过将单个值 $x_j$（在索引 $j = \\lfloor n/2 \\rfloor$ 处）替换为一个大值 $x_j^{\\text{out}} = s \\cdot s_x$ 来引入一个离群点，其中 $s_x$ 是原始预测变量的样本标准差，$s$ 是一个大的缩放因子。这将创建一个新的数据集 $(x_i^{\\text{out}}, y_i^{\\text{out}})$。\n\n然后，在含有离群点的数据集上重复整个可行的 WLS 程序。离群点 $x_j^{\\text{out}}$ 是一个高杠杆点，意味着它具有影响回归线的强大潜力。这种影响会贯穿整个估计链：\n1.  初始的 OLS 拟合 $(\\hat{\\boldsymbol{\\beta}}^{\\text{OLS,out}})$ 将被离群点严重扭曲。\n2.  这种扭曲的拟合会导致一组扭曲的残差 $r_i^{\\text{out}}$，尤其是在离群点索引 $j$ 处可能有一个非常大的残差。\n3.  巨大的离群点残差可能会主导辅助回归，导致估计值 $\\hat{\\alpha}^{\\text{out}}$ 表现不佳。\n4.  损坏的估计值 $\\hat{\\alpha}^{\\text{out}}$ 会导致不合适的权重 $\\hat{w}_i^{\\text{out}}$，这可能无法正确地增加或减少观测值的权重，从而导致有偏的最终估计值 $\\hat{\\beta}_1^{\\text{WLS,out}}$。\n\n敏感性通过绝对差 $\\Delta_\\alpha = \\lvert \\hat{\\alpha}^{\\text{out}} - \\hat{\\alpha} \\rvert$ 和 $\\Delta_{\\beta_1} = \\lvert \\hat{\\beta}_1^{\\text{WLS,out}} - \\hat{\\beta}_1^{\\text{WLS}} \\rvert$ 来量化。此分析展示了自动化多阶段统计程序的一个关键弱点：一个不稳健的初始步骤可能会破坏整个分析过程，即使后续步骤在理论上是为处理潜在问题（如 WLS 用于处理异方差性）而设计的。\n\n该实现将使用指定的参数和随机种子为每个测试用例确定性地生成数据。然后，它将对原始数据集和注入了离群点的数据集执行所述的可行 WLS 程序，计算所需的估计值，并计算敏感性度量指标。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n\n    test_cases = [\n        # (n, beta_0, beta_1, sigma, alpha, s)\n        (200, 1.0, 2.0, 1.0, 1.0, 25.0),\n        (200, 0.5, -1.5, 1.0, 0.0, 25.0),\n        (200, 0.0, 1.0, 1.0, -1.0, 25.0),\n        (30, 1.0, 0.5, 0.5, 0.5, 50.0),\n    ]\n\n    all_results = []\n    for i, params in enumerate(test_cases):\n        case_result = solve_case(*params, case_idx=i)\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef solve_case(n, beta0, beta1, sigma, alpha, s, case_idx):\n    \"\"\"\n    Solves a single test case for the Feasible WLS procedure and its\n    sensitivity to an outlier.\n    \"\"\"\n    s0 = 2757\n    delta = 1e-8\n\n    def m_func(x_vec):\n        \"\"\"Stabilized magnitude function.\"\"\"\n        return np.maximum(np.abs(x_vec), delta)\n\n    def run_feasible_wls(x, y):\n        \"\"\"\n        Performs the complete two-stage feasible WLS estimation.\n        \n        Args:\n            x (np.array): Predictor variable vector.\n            y (np.array): Response variable vector.\n\n        Returns:\n            tuple: A tuple containing (alpha_hat, beta1_wls_hat).\n        \"\"\"\n        n_local = len(x)\n        # Stage 1: OLS to get residuals\n        X_mat = np.vstack([np.ones(n_local), x]).T\n        try:\n            beta_ols = np.linalg.solve(X_mat.T @ X_mat, X_mat.T @ y)\n        except np.linalg.LinAlgError:\n            # Use pseudo-inverse for singular matrices, though unlikely here\n            beta_ols = np.linalg.pinv(X_mat.T @ X_mat) @ X_mat.T @ y\n        \n        residuals = y - X_mat @ beta_ols\n\n        # Stage 2: Auxiliary regression to estimate alpha\n        log_sq_residuals = np.log(np.maximum(residuals**2, delta))\n        log_m_x = np.log(m_func(x))\n        Z_mat = np.vstack([np.ones(n_local), log_m_x]).T\n        \n        try:\n            gamma = np.linalg.solve(Z_mat.T @ Z_mat, Z_mat.T @ log_sq_residuals)\n        except np.linalg.LinAlgError:\n            gamma = np.linalg.pinv(Z_mat.T @ Z_mat) @ Z_mat.T @ log_sq_residuals\n            \n        alpha_hat = gamma[1]\n\n        # Stage 3: WLS with estimated weights\n        weights = m_func(x)**(-alpha_hat)\n        W_mat = np.diag(weights)\n        \n        try:\n            beta_wls = np.linalg.solve(X_mat.T @ W_mat @ X_mat, X_mat.T @ W_mat @ y)\n        except np.linalg.LinAlgError:\n            beta_wls = np.linalg.pinv(X_mat.T @ W_mat @ X_mat) @ (X_mat.T @ W_mat @ y)\n            \n        beta1_wls_hat = beta_wls[1]\n\n        return alpha_hat, beta1_wls_hat\n\n    # --- Data Generation (No Outlier) ---\n    rng_x = np.random.default_rng(seed=s0 + case_idx)\n    rng_z = np.random.default_rng(seed=s0 + 100 * case_idx)\n    \n    x_original = rng_x.normal(loc=0, scale=1, size=n)\n    z_noise = rng_z.normal(loc=0, scale=1, size=n)\n    \n    eps_original = sigma * m_func(x_original)**(alpha / 2) * z_noise\n    y_original = beta0 + beta1 * x_original + eps_original\n\n    alpha_hat, beta1_wls_hat = run_feasible_wls(x_original, y_original)\n\n    # --- Outlier Injection and Analysis ---\n    x_outlier = x_original.copy()\n    outlier_idx = n // 2\n    # Use ddof=1 for sample standard deviation\n    s_x = np.std(x_original, ddof=1)\n    x_outlier[outlier_idx] = s * s_x\n    \n    # Regenerate responses with the new x vector but same noise draws\n    eps_outlier = sigma * m_func(x_outlier)**(alpha / 2) * z_noise\n    y_outlier = beta0 + beta1 * x_outlier + eps_outlier\n\n    alpha_hat_out, beta1_wls_hat_out = run_feasible_wls(x_outlier, y_outlier)\n\n    # --- Calculate Sensitivity Metrics ---\n    delta_alpha = np.abs(alpha_hat_out - alpha_hat)\n    delta_beta1 = np.abs(beta1_wls_hat_out - beta1_wls_hat)\n\n    # --- Package results for printing ---\n    result_list = [\n        round(alpha_hat, 4),\n        round(beta1_wls_hat, 4),\n        round(alpha_hat_out, 4),\n        round(beta1_wls_hat_out, 4),\n        round(delta_alpha, 4),\n        round(delta_beta1, 4)\n    ]\n\n    return result_list\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3128024"}]}