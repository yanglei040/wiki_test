{"hands_on_practices": [{"introduction": "我们的第一项练习旨在通过计算来验证一个核心理论问题：普通最小二乘法（OLS）和广义最小二乘法（GLS）的估计量在何种条件下是完全相同的。通过在一系列精心设计的误差协方差结构下比较OLS和GLS的估计结果，这项实践将帮助你建立关于何时可以安全使用OLS的直观认识，并观察到随着误差相关性的增强，两种估计量是如何产生差异的。[@problem_id:3112106]", "problem": "给定一个可能带有相关误差项的线性模型：$y = X \\beta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$ 是满列秩矩阵，$y \\in \\mathbb{R}^{n}$，误差向量 $\\varepsilon$ 的均值为 $0$，协方差矩阵 $\\Sigma \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。普通最小二乘 (OLS) 估计量定义为未加权平方损失的最小化器，而广义最小二乘 (GLS) 估计量定义为由逆协方差加权的平方损失的最小化器。您的任务是实现一个程序，该程序针对一个固定的数据集和一套协方差设定，计算 OLS 和 GLS 估计量之间差值的欧几里得范数，以评估它们在何种情况下完全一致，以及 GLS 对偏离完全相等条件的微小变化的敏感程度。\n\n推导和实现的基本依据：\n- OLS 估计量是函数 $\\beta \\mapsto \\|y - X \\beta\\|_{2}^{2}$ 的唯一最小化器。\n- GLS 估计量是函数 $\\beta \\mapsto (y - X \\beta)^{\\top} \\Sigma^{-1} (y - X \\beta)$ 的唯一最小化器。\n\n数据集（在所有测试用例中固定）：\n- 样本量和特征数：$n = 6$，$p = 2$。\n- 设计矩阵：\n$$\nX = \\begin{bmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n1 & 2 \\\\\n1 & 3 \\\\\n1 & 4 \\\\\n1 & 5\n\\end{bmatrix}.\n$$\n- 响应向量：\n$$\ny = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n2 \\\\\n4 \\\\\n3 \\\\\n5\n\\end{bmatrix}.\n$$\n\n协方差矩阵 $\\Sigma$ 的测试套件：\n1. 完全相等，球形误差：$\\Sigma_{1} = \\sigma^{2} I$，其中 $\\sigma^{2} = 2.0$，$I$ 是 $n \\times n$ 的单位矩阵。\n2. 完全相等，一般条件：$\\Sigma_{2} = \\sigma^{2} I + X \\Lambda X^{\\top}$，其中 $\\sigma^{2} = 1.5$ 且\n$$\n\\Lambda = \\begin{bmatrix}\n0.7 & 0.2 \\\\\n0.2 & 0.5\n\\end{bmatrix}.\n$$\n3. 近似相等，与 $\\operatorname{col}(X)$ 正交的微小扰动：$\\Sigma_{3} = \\Sigma_{2} + \\epsilon \\, v v^{\\top}$，其中 $\\epsilon = 10^{-6}$ 且\n$$\nv = \\begin{bmatrix}\n1 \\\\\n-2 \\\\\n1 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix},\n$$\n其中 $v$ 与 $X$ 的每一列都正交。\n4. 弱自相关：$\\Sigma_{4}$ 是参数为 $\\rho = 0.1$、方差为 $\\sigma^{2} = 1.0$ 的一阶自回归 (AR(1)) 协方差矩阵，即对于所有索引 $i,j$，其元素为 $(\\Sigma_{4})_{ij} = \\sigma^{2} \\rho^{|i - j|}$。\n5. 强自相关：$\\Sigma_{5}$ 是参数为 $\\rho = 0.8$、方差为 $\\sigma^{2} = 1.0$ 的 AR(1) 协方差矩阵，即对于所有索引 $i,j$，其元素为 $(\\Sigma_{5})_{ij} = \\sigma^{2} \\rho^{|i - j|}$。\n\n您的程序必须：\n- 计算作为未加权平方损失的唯一最小化器的 OLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$。\n- 对于测试套件中的每个 $\\Sigma$，计算作为 $\\Sigma^{-1}$-加权平方损失的唯一最小化器的 GLS 估计量 $\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma)$。\n- 对于每个测试用例，计算差值的欧几里得范数 $\\|\\hat{\\beta}_{\\mathrm{OLS}} - \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma)\\|_{2}$。\n- 生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的值为按 $\\Sigma_{1}, \\Sigma_{2}, \\Sigma_{3}, \\Sigma_{4}, \\Sigma_{5}$ 顺序排列的结果，每个值四舍五入到 $8$ 位小数。\n\n测试用例设计说明：\n- 用例 $\\Sigma_{1}$ 测试 OLS 和 GLS 完全一致的基准条件。\n- 用例 $\\Sigma_{2}$ 测试一个非平凡的完全相等条件。\n- 用例 $\\Sigma_{3}$ 测试对一个微小扰动的敏感性，该扰动在一个与 $X$ 列正交的方向上违反了完全相等条件。\n- 用例 $\\Sigma_{4}$ 和 $\\Sigma_{5}$ 测试在结构化相关（弱和强 AR(1)）下与相等条件的偏离程度不断增加的情况。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含五个结果，形式为一个用方括号括起来的逗号分隔列表（例如 $[r_{1},r_{2},r_{3},r_{4},r_{5}]$），其中每个 $r_{k}$ 是一个四舍五入到 $8$ 位小数的浮点数。不得打印任何其他文本。", "solution": "用户希望通过计算普通最小二乘 (OLS) 和广义最小二乘 (GLS) 估计量在不同误差协方差矩阵设定下的差值的欧几里得范数，来比较这两种估计量。\n\n### 步骤 1：理论公式\n\n线性模型由 $y = X \\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^{n}$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是满列秩的设计矩阵，$\\beta \\in \\mathbb{R}^{p}$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^{n}$ 是误差向量，其均值 $\\mathbb{E}[\\varepsilon] = 0$，协方差矩阵 $\\operatorname{Cov}(\\varepsilon) = \\Sigma$ 是一个对称正定矩阵。\n\n**普通最小二乘 (OLS) 估计量**\n\nOLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$ 通过最小化残差平方和 (RSS) 得到：\n$$\n\\text{RSS}(\\beta) = (y - X\\beta)^{\\top}(y - X\\beta) = \\| y - X\\beta \\|_{2}^{2}\n$$\n为求最小值，我们对 $\\beta$ 求导并令梯度为零：\n$$\n\\frac{\\partial \\text{RSS}}{\\partial \\beta} = -2X^{\\top}(y - X\\beta) = 0\n$$\n$$\nX^{\\top}y - X^{\\top}X\\beta = 0 \\implies X^{\\top}X\\beta = X^{\\top}y\n$$\n由于假定 $X$ 具有满列秩，矩阵 $X^{\\top}X$ 是可逆的。因此，唯一的 OLS 估计量为：\n$$\n\\hat{\\beta}_{\\mathrm{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y\n$$\n\n**广义最小二乘 (GLS) 估计量**\n\nGLS 估计量 $\\hat{\\beta}_{\\mathrm{GLS}}$ 通过最小化一个加权残差平方和来考虑误差协方差结构：\n$$\n\\text{RSS}_{\\Sigma}(\\beta) = (y - X\\beta)^{\\top}\\Sigma^{-1}(y - X\\beta)\n$$\n这可以看作是对一个变换后的模型执行 OLS。由于 $\\Sigma$ 是对称正定矩阵，其逆矩阵 $\\Sigma^{-1}$ 也是对称正定的。我们可以找到一个矩阵 $L$ 使得 $\\Sigma^{-1} = L^{\\top}L$（例如，通过 Cholesky 分解）。目标函数变为：\n$$\n\\text{RSS}_{\\Sigma}(\\beta) = (y - X\\beta)^{\\top}L^{\\top}L(y - X\\beta) = \\|L(y - X\\beta)\\|_{2}^{2} = \\|y' - X'\\beta\\|_{2}^{2}\n$$\n其中 $y' = Ly$ 且 $X' = LX$。这个变换后问题的 OLS 解为：\n$$\n\\hat{\\beta}_{\\mathrm{GLS}} = ((X')^{\\top}X')^{-1}(X')^{\\top}y' = (X^{\\top}L^{\\top}LX)^{-1}X^{\\top}L^{\\top}Ly\n$$\n将 $\\Sigma^{-1} = L^{\\top}L$ 代回，我们得到 GLS 估计量：\n$$\n\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma) = (X^{\\top}\\Sigma^{-1}X)^{-1}X^{\\top}\\Sigma^{-1}y\n$$\n\n### 步骤 2：OLS 和 GLS 的等价条件\n\nOLS 和 GLS 估计量相等，即 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}$，当且仅当设计矩阵的列空间 $\\operatorname{col}(X)$ 是协方差矩阵 $\\Sigma$ 的一个不变子空间。这意味着对于任何向量 $z \\in \\operatorname{col}(X)$，我们必须有 $\\Sigma z \\in \\operatorname{col}(X)$。陈述此条件的一个实用方法是，必须存在一个 $p \\times p$ 矩阵 $M$ 使得 $\\Sigma X = X M$。\n\n我们来根据这个条件检查前三个测试用例。\n1.  $\\Sigma_1 = \\sigma^2 I$：此时，$\\Sigma_1 X = (\\sigma^2 I) X = X(\\sigma^2 I_p)$。条件成立，其中 $M = \\sigma^2 I_p$，所以 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_1)$。\n2.  $\\Sigma_2 = \\sigma^2 I + X \\Lambda X^\\top$：此时，$\\Sigma_2 X = (\\sigma^2 I + X \\Lambda X^\\top)X = \\sigma^2 X + X\\Lambda(X^\\top X) = X(\\sigma^2 I_p + \\Lambda(X^\\top X))$。条件成立，其中 $M = \\sigma^2 I_p + \\Lambda(X^\\top X)$，所以 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_2)$。\n3.  $\\Sigma_3 = \\Sigma_2 + \\epsilon v v^\\top$：给定向量 $v$ 与 $X$ 的每一列正交，这意味着 $X^\\top v = 0$，因此 $v^\\top X = (X^\\top v)^\\top = 0$。\n    于是，$\\Sigma_3 X = (\\Sigma_2 + \\epsilon v v^\\top)X = \\Sigma_2 X + \\epsilon v(v^\\top X) = \\Sigma_2 X + 0 = \\Sigma_2 X$。\n    由于不变性条件对 $\\Sigma_2$ 成立，它对 $\\Sigma_3$ 也必定成立。因此，我们预期 $\\hat{\\beta}_{\\mathrm{OLS}} = \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_3)$。\n\n对于用例 4 和 5，自回归协方差结构通常不满足此不变性属性，因此我们预期估计量之间存在非零差异。\n\n### 步骤 3：计算流程\n\n解决方案按以下步骤实现：\n1.  将给定的数据集、设计矩阵 $X$ 和响应向量 $y$ 定义为数值数组。样本量为 $n=6$，特征数为 $p=2$。\n2.  使用其闭式解公式计算一次 OLS 估计量 $\\hat{\\beta}_{\\mathrm{OLS}}$。为保证数值稳定性，直接求解线性方程组 $X^{\\top}X \\beta = X^{\\top}y$，而不是显式计算 $(X^{\\top}X)^{-1}$。\n3.  遍历协方差矩阵 $\\Sigma$ 的五个指定测试用例：\n    a. 对每个用例，构造相应的 $6 \\times 6$ 矩阵 $\\Sigma_k$。\n    b. 对于具有 AR($1$) 结构的用例 $\\Sigma_4$ 和 $\\Sigma_5$，协方差矩阵是一个对称的 Toeplitz 矩阵，其中 $(\\Sigma)_{ij} = \\sigma^2 \\rho^{|i-j|}$。\n    c. 使用其公式计算 GLS 估计量 $\\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_k)$。同样，为保证稳定性，直接求解线性方程组 $(X^{\\top}\\Sigma_k^{-1}X)\\beta = X^{\\top}\\Sigma_k^{-1}y$。\n4.  对每个用例，计算差向量的欧几里得范数 $\\|\\hat{\\beta}_{\\mathrm{OLS}} - \\hat{\\beta}_{\\mathrm{GLS}}(\\Sigma_k)\\|_{2}$。\n5.  收集五个范数，并按规定格式化为单个字符串。\n\n理论分析预测，前三个范数将为零（或由于浮点精度限制而接近于零），而后两个范数将为非零，其中 $\\Sigma_5$ ($\\rho=0.8$) 的范数预期将大于 $\\Sigma_4$ ($\\rho=0.1$) 的范数。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve():\n    \"\"\"\n    Computes the Euclidean norm of the difference between OLS and GLS estimators\n    for a given dataset and five different error covariance structures.\n    \"\"\"\n    # Fixed dataset\n    n, p = 6, 2\n    X = np.array([\n        [1.0, 0.0],\n        [1.0, 1.0],\n        [1.0, 2.0],\n        [1.0, 3.0],\n        [1.0, 4.0],\n        [1.0, 5.0]\n    ])\n    y = np.array([1.0, 2.0, 2.0, 4.0, 3.0, 5.0])\n\n    # Compute OLS estimator once\n    # Solve (X^T X) beta = X^T y\n    beta_ols = np.linalg.solve(X.T @ X, X.T @ y)\n\n    # --- Define the five covariance matrices ---\n\n    # Case 1: Spherical errors\n    sigma2_1 = 2.0\n    Sigma1 = sigma2_1 * np.identity(n)\n\n    # Case 2: General exact-equality condition\n    sigma2_2 = 1.5\n    Lambda = np.array([[0.7, 0.2], [0.2, 0.5]])\n    Sigma2 = sigma2_2 * np.identity(n) + X @ Lambda @ X.T\n\n    # Case 3: Near-equality, perturbation orthogonal to col(X)\n    epsilon = 1e-6\n    v = np.array([1.0, -2.0, 1.0, 0.0, 0.0, 0.0])\n    Sigma3 = Sigma2 + epsilon * np.outer(v, v)\n\n    # Case 4: Weak autoregressive correlation (AR(1))\n    rho4 = 0.1\n    sigma2_4 = 1.0\n    c4 = sigma2_4 * (rho4 ** np.arange(n))\n    Sigma4 = toeplitz(c4)\n\n    # Case 5: Strong autoregressive correlation (AR(1))\n    rho5 = 0.8\n    sigma2_5 = 1.0\n    c5 = sigma2_5 * (rho5 ** np.arange(n))\n    Sigma5 = toeplitz(c5)\n\n    # List of covariance matrices for iteration\n    sigmas = [Sigma1, Sigma2, Sigma3, Sigma4, Sigma5]\n    \n    results = []\n\n    # Iterate through each covariance matrix to compute GLS and the difference norm\n    for Sigma in sigmas:\n        # Compute GLS estimator\n        # Solve (X^T Sigma^-1 X) beta = X^T Sigma^-1 y\n        Sigma_inv = np.linalg.inv(Sigma)\n        XT_S_inv = X.T @ Sigma_inv\n        \n        A_gls = XT_S_inv @ X\n        b_gls = XT_S_inv @ y\n        \n        beta_gls = np.linalg.solve(A_gls, b_gls)\n        \n        # Compute the Euclidean norm of the difference\n        diff_norm = np.linalg.norm(beta_ols - beta_gls)\n        results.append(diff_norm)\n\n    # Format the final output string as per requirements\n    output_str = f\"[{','.join([f'{r:.8f}' for r in results])}]\"\n    print(output_str)\n\nsolve()\n\n```", "id": "3112106"}, {"introduction": "在了解了误差协方差矩阵 $\\Sigma$ 会影响OLS和GLS的一致性后，我们进一步探究另一个关键因素：设计矩阵 $X$ 的结构。这项练习将向你展示，GLS相对于OLS的效率增益并非一成不变，而是显著地依赖于回归量（即 $X$ 的列）之间的关系以及它们与误差结构之间的相互作用。通过为固定的误差相关性构建具有不同共线性的设计矩阵，你将亲手发现是什么驱动了GLS的优势。[@problem_id:3112084]", "problem": "考虑具有相关误差的线性回归模型，定义为 $y = X\\beta + \\varepsilon$。其中，$y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是固定回归量的设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是均值为零的误差向量，其协方差矩阵为 $\\Sigma \\in \\mathbb{R}^{n \\times n}$。仅从这些定义出发，并利用随机向量的线性变换遵循 $\\operatorname{Cov}(A z) = A\\,\\operatorname{Cov}(z)\\,A^\\top$（对于任何相容矩阵 $A$），以及矩阵的逆和转置遵循标准线性代数性质这一事实，推导普通最小二乘法 (OLS) 估计量和广义最小二乘法 (GLS) 估计量的协方差矩阵表达式。然后，在固定的相关误差协方差 $\\Sigma$ 下，针对几个固定的设计 $X$，使用这些表达式来量化 GLS 相对于 OLS 的相对效率。广义最小二乘法 (GLS) 是指在已知误差协方差 $\\Sigma$ 的情况下进行估计，而普通最小二乘法 (OLS) 是指忽略 $\\Sigma$ 进行估计。\n\n对于给定的设计 $X$ 和误差协方差 $\\Sigma$，将 GLS 相对于 OLS 的标量效率定义为两个估计量协方差矩阵的迹之比，即 $\\operatorname{Eff}(X,\\Sigma) = \\dfrac{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}})\\big)}{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}})\\big)}$，其中 $\\operatorname{tr}(\\cdot)$ 表示方阵的迹。该量捕捉了每个参数的平均方差：接近 $1$ 的值表示增益最小，而较大的值表示 GLS 相对于 OLS 的效率增益更大。\n\n您的任务是实现一个程序，对于固定的相关误差协方差 $\\Sigma$，构建几个能体现 GLS 最小和最大增益的设计矩阵 $X$，为每个矩阵计算 $\\operatorname{Eff}(X,\\Sigma)$，并报告结果。您必须在相同的 $\\Sigma$ 下，通过使用正交列来设计 $X$ 以使 GLS 增益最小化，并通过使用近似共线性来使增益最大化。此外，还要包含一个误差不相关的边界情况，以验证您的实现。\n\n使用以下测试套件：\n\n- 设 $n = 8$ 且 $p = 2$。对于除边界情况外的所有情况，将 $\\Sigma$ 固定为一阶自回归协方差，其参数 $\\rho = 0.8$ 且创新方差为 $1$，即 $\\Sigma \\in \\mathbb{R}^{8 \\times 8}$，其元素为 $\\Sigma_{ij} = \\rho^{|i-j|}$，适用于所有 $i,j \\in \\{1,\\dots,8\\}$ 和 $\\rho = 0.8$。\n- 对于每种情况，设 $X$ 的第一列为截距向量 $\\mathbf{1}_8 = [1,1,1,1,1,1,1,1]^\\top$（八个一），第二列为 $x_1$，具体如下：\n  1. 情况 A（正交列，目标为最小增益）：$x_1 = [-3,-2,-1,0,1,2,3,0]^\\top$，该向量满足 $\\mathbf{1}_8^\\top x_1 = 0$，因此在欧几里得内积下与截距正交。\n  2. 情况 B（近似共线性，目标为最大增益）：$x_1 = \\mathbf{1}_8 + 10^{-3} \\cdot [-3,-2,-1,0,1,2,3,0]^\\top$，这使得第二列与截距近似成比例。\n  3. 情况 C（中度共线性）：$x_1 = \\mathbf{1}_8 + 10^{-1} \\cdot [-3,-2,-1,0,1,2,3,0]^\\top$，这提供了与截距的中度相关性。\n  4. 情况 D（不相关误差的边界检查）：使用与情况 B 中相同的 $X$，但设置 $\\Sigma = I_8$（$8 \\times 8$ 单位矩阵），对应于 $\\rho = 0$。\n\n对于每种情况，计算如上定义的标量效率 $\\operatorname{Eff}(X,\\Sigma)$。将每个结果四舍五入到 $6$ 位小数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按上述情况的顺序排列，即 $[\\operatorname{Eff}_A,\\operatorname{Eff}_B,\\operatorname{Eff}_C,\\operatorname{Eff}_D]$。每个条目都是一个四舍五入到 $6$ 位小数的浮点数。不应打印其他任何文本或空格。", "solution": "该问题要求推导普通最小二乘法 (OLS) 和广义最小二乘法 (GLS) 估计量的协方差矩阵，并利用它们为几个指定情景计算相对效率指标。\n\n首先，我们通过从给定的定义中推导出必要的协方差矩阵来建立理论基础。线性回归模型为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$，$\\varepsilon \\in \\mathbb{R}^n$ 是误差项，满足 $E[\\varepsilon] = 0$ 和 $\\operatorname{Cov}(\\varepsilon) = \\Sigma$。\n\n**OLS 估计量协方差矩阵的推导**\n\nOLS 估计量定义为 $\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y$。它是在 $\\operatorname{Cov}(\\varepsilon) = \\sigma^2 I$ 的假设下推导出来的，但在这里，我们在真实、更一般的协方差结构 $\\Sigma$ 下评估其性能。将 $y$ 的模型方程代入：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top (X\\beta + \\varepsilon) = (X^\\top X)^{-1} X^\\top X\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon = \\beta + (X^\\top X)^{-1} X^\\top \\varepsilon\n$$\n该估计量是无偏的，因为 $E[\\hat{\\beta}_{\\text{OLS}}] = E[\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon] = \\beta + (X^\\top X)^{-1} X^\\top E[\\varepsilon] = \\beta$。$\\hat{\\beta}_{\\text{OLS}}$ 的协方差矩阵为：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\operatorname{Cov}(\\beta + (X^\\top X)^{-1} X^\\top \\varepsilon) = \\operatorname{Cov}((X^\\top X)^{-1} X^\\top \\varepsilon)\n$$\n使用给定的随机向量线性变换规则 $\\operatorname{Cov}(Az) = A\\operatorname{Cov}(z)A^\\top$，我们令 $A = (X^\\top X)^{-1} X^\\top$ 和 $z = \\varepsilon$。这得到：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = \\left( (X^\\top X)^{-1} X^\\top \\right) \\operatorname{Cov}(\\varepsilon) \\left( (X^\\top X)^{-1} X^\\top \\right)^\\top\n$$\n代入 $\\operatorname{Cov}(\\varepsilon) = \\Sigma$ 并简化转置项：\n$$\n\\left( (X^\\top X)^{-1} X^\\top \\right)^\\top = (X^\\top)^\\top \\left((X^\\top X)^{-1}\\right)^\\top = X \\left((X^\\top X)^\\top\\right)^{-1} = X(X^\\top X)^{-1}\n$$\n最后一步利用了 $X^\\top X$ 是对称矩阵的事实。因此，OLS 估计量的协方差矩阵为：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}}) = (X^\\top X)^{-1} X^\\top \\Sigma X (X^\\top X)^{-1}\n$$\n\n**GLS 估计量协方差矩阵的推导**\n\nGLS 估计量考虑了已知的协方差 $\\Sigma$，其定义为 $\\hat{\\beta}_{\\text{GLS}} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} y$。遵循类似的步骤，我们将 $y$ 的模型代入：\n$$\n\\hat{\\beta}_{\\text{GLS}} = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} (X\\beta + \\varepsilon) = \\beta + (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\varepsilon\n$$\nGLS 估计量也是无偏的，因为 $E[\\hat{\\beta}_{\\text{GLS}}] = \\beta$。其协方差为：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = \\operatorname{Cov}\\left( (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\varepsilon \\right)\n$$\n令 $B = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}$ 并使用 $\\operatorname{Cov}(Bz) = B\\operatorname{Cov}(z)B^\\top$：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = B \\Sigma B^\\top = \\left((X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}\\right) \\Sigma \\left((X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1}\\right)^\\top\n$$\n转置项为 $\\left(\\Sigma^{-1}\\right)^\\top (X^\\top)^\\top \\left((X^\\top \\Sigma^{-1} X)^{-1}\\right)^\\top = \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1}$，因为 $\\Sigma$ 和 $X^\\top\\Sigma^{-1}X$ 是对称的。代回：\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = (X^\\top \\Sigma^{-1} X)^{-1} X^\\top \\Sigma^{-1} \\Sigma \\Sigma^{-1} X (X^\\top \\Sigma^{-1} X)^{-1}\n$$\n$$\n\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}}) = (X^\\top \\Sigma^{-1} X)^{-1} (X^\\top \\Sigma^{-1} X) (X^\\top \\Sigma^{-1} X)^{-1} = (X^\\top \\Sigma^{-1} X)^{-1}\n$$\n这是 GLS 估计量协方差的标准结果。\n\n**相对效率与实施计划**\n\n标量相对效率由这两个协方差矩阵的迹之比给出：\n$$\n\\operatorname{Eff}(X,\\Sigma) = \\dfrac{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{OLS}})\\big)}{\\operatorname{tr}\\big(\\operatorname{Cov}(\\hat{\\beta}_{\\text{GLS}})\\big)} = \\dfrac{\\operatorname{tr}\\left( (X^\\top X)^{-1} X^\\top \\Sigma X (X^\\top X)^{-1} \\right)}{\\operatorname{tr}\\left( (X^\\top \\Sigma^{-1} X)^{-1} \\right)}\n$$\n实施将首先定义一个 Python 函数，用于为给定的设计矩阵 $X$ 和协方差矩阵 $\\Sigma$ 计算此量。该函数将使用 `numpy` 库进行所有线性代数运算，包括矩阵乘法、转置、求逆和迹的计算。\n\n对于四个指定的情况中的每一种，我们将为相应的 $X$ 和 $\\Sigma$ 构建 `numpy` 数组。将为 $\\rho=0.8$ 和 $n=8$ 构建自回归协方差矩阵 $\\Sigma_{ij} = \\rho^{|i-j|}$。设计矩阵 $X$ 将根据每种情况的指定（正交、近似共线、中度共线）构建，包含一个截距列和第二列。边界情况对 $\\Sigma$ 使用单位矩阵。\n\n将为四种情况中的每一种计算效率，预计边界情况（情况 D）将产生 $1.0$ 的效率，因为当误差不相关（$\\Sigma=\\sigma^2I$）时，OLS 是最佳线性无偏估计量 (BLUE)，且与 GLS 相同。这可作为对推导出的公式和实施的关键验证。然后将最终结果按照指定要求进行四舍五入和格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_efficiency(X, Sigma):\n    \"\"\"\n    Computes the relative efficiency of GLS vs. OLS.\n\n    Args:\n        X (np.ndarray): The design matrix of shape (n, p).\n        Sigma (np.ndarray): The error covariance matrix of shape (n, n).\n\n    Returns:\n        float: The scalar efficiency Eff(X, Sigma).\n    \"\"\"\n    # Numerator: Trace of the OLS estimator's covariance matrix\n    # Cov(b_OLS) = (X'X)^-1 X' Sigma X (X'X)^-1\n    try:\n        XT = X.T\n        XTX_inv = np.linalg.inv(XT @ X)\n        cov_ols = XTX_inv @ XT @ Sigma @ X @ XTX_inv\n        tr_cov_ols = np.trace(cov_ols)\n    except np.linalg.LinAlgError:\n        # This can happen if X is not full rank, though not expected\n        # for the given test cases.\n        return np.nan\n\n    # Denominator: Trace of the GLS estimator's covariance matrix\n    # Cov(b_GLS) = (X' Sigma^-1 X)^-1\n    try:\n        Sigma_inv = np.linalg.inv(Sigma)\n        cov_gls = np.linalg.inv(XT @ Sigma_inv @ X)\n        tr_cov_gls = np.trace(cov_gls)\n    except np.linalg.LinAlgError:\n        # This can happen if Sigma is singular or X' Sigma^-1 X is singular.\n        # Not expected for the given test cases.\n        return np.nan\n\n    # The efficiency ratio\n    if tr_cov_gls == 0:\n        return np.inf if tr_cov_ols > 0 else np.nan\n    \n    return tr_cov_ols / tr_cov_gls\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating the GLS vs. OLS efficiency for four cases.\n    \"\"\"\n    # Define problem parameters\n    n = 8\n    rho = 0.8\n    \n    # Construct AR(1) covariance matrix for Cases A, B, C\n    # Sigma_ij = rho^|i-j|\n    indices = np.arange(n)\n    Sigma_AR1 = rho**np.abs(indices[:, np.newaxis] - indices)\n\n    # Construct identity covariance matrix for Case D\n    Sigma_I = np.identity(n)\n\n    # Common vectors for constructing design matrices\n    intercept = np.ones(n)\n    v = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 0.0])\n\n    # --- Case A (orthogonal columns) ---\n    x1_A = v\n    X_A = np.column_stack((intercept, x1_A))\n    eff_A = compute_efficiency(X_A, Sigma_AR1)\n\n    # --- Case B (near collinearity) ---\n    x1_B = intercept + 1e-3 * v\n    X_B = np.column_stack((intercept, x1_B))\n    eff_B = compute_efficiency(X_B, Sigma_AR1)\n    \n    # --- Case C (moderate collinearity) ---\n    x1_C = intercept + 1e-1 * v\n    X_C = np.column_stack((intercept, x1_C))\n    eff_C = compute_efficiency(X_C, Sigma_AR1)\n\n    # --- Case D (boundary check with uncorrelated errors) ---\n    # Use the same X as in Case B but with Sigma = I\n    X_D = X_B\n    eff_D = compute_efficiency(X_D, Sigma_I)\n\n    results = [eff_A, eff_B, eff_C, eff_D]\n    \n    # Format the output as specified\n    # The requirement is rounding to 6 decimal places.\n    # The f-string format specifier '{:.6f}' correctly handles this rounding.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "3112084"}, {"introduction": "最后的练习将我们带入一个更高级和实际的应用场景，其中误差相关性源于网络结构，这在地理空间数据或网络流量分析中很常见。此问题的核心挑战在于计算效率：当处理大型数据集时，直接计算和求逆一个巨大的协方差矩阵 $\\boldsymbol{\\Sigma}$ 是不可行的。这项实践将指导你利用误差精度矩阵 $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$ 的稀疏性，通过实现一种高效的稀疏乔列斯基分解（sparse Cholesky factorization）来解决GLS问题，这是现代统计计算中的一项关键技能。[@problem_id:3112081]", "problem": "给定一个定义在路径图网络节点上的、具有相关误差的线性模型。结果向量表示为 $\\mathbf{y} \\in \\mathbb{R}^n$，设计矩阵表示为 $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$，误差向量表示为 $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$。模型为 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，其中 $E[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ 且 $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}) = \\boldsymbol{\\Sigma}$。由于路径图上的共享链接，误差是相关的；这种相关性通过一个稀疏精度（逆协方差）矩阵 $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$ 来编码。\n\n从以下基本原理出发：\n- 线性模型定义 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$，误差为零均值。\n- 假设 $\\boldsymbol{\\varepsilon}$ 服从协方差为 $\\boldsymbol{\\Sigma}$ 的多元正态分布，因此对数似然（在相差一个加性常数的情况下）与二次型 $(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})$ 的负值成正比。\n- 适用于带状矩阵的稀疏线性代数原理，以及适用于对称正定矩阵的 Cholesky 分解。\n\n网络诱导的误差相关模型。考虑一个包含 $n$ 个节点的路径图，其中精度矩阵 $\\mathbf{Q}$ 定义为\n$$\n\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}},\n$$\n其中 $\\alpha > 0$, $w \\ge 0$, $\\mathbf{I}_n$ 是 $n \\times n$ 的单位矩阵，$\\mathbf{L}_{\\text{path}}$ 是路径图的图拉普拉斯矩阵：$\\mathbf{L}_{\\text{path}}$ 在两个端点处的对角线元素为 $1$，在内部节点处的对角线元素为 $2$，对于相邻节点的次对角线和超对角线元素为 $-1$，所有其他元素均为 $0$。这使得 $\\mathbf{Q}$ 成为对称正定的三对角矩阵。你的任务是利用这种结构，通过一个专为三对角矩阵定制的稀疏 Cholesky 程序来有效利用稀疏性，从而高效地实现广义最小二乘法 (GLS)。\n\n要求：\n- 仅从所述基本原理出发（不要使用任何提供给你的快捷估计量表达式），推导出获得 $\\boldsymbol{\\beta}$ 估计量的计算步骤，该估计量能最小化由精度矩阵 $\\mathbf{Q}$ 诱导的相应二次型。\n- 为 $\\mathbf{Q}$ 实现一个带状（三对角）稀疏 Cholesky 分解。即，计算一个下双对角矩阵 $\\mathbf{L}$ 使得 $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$，仅使用对对称正定三对角矩阵有效的 $O(n)$ 递推关系。\n- 使用该分解将问题转换为一个关于适当白化变量的普通最小二乘问题，并求解 $\\boldsymbol{\\beta}$。\n- 你不能依赖于密集 Cholesky 程序。你的实现必须明确利用 $\\mathbf{Q}$ 的三对角稀疏性。\n\n测试套件。对于下面的每个测试用例，通过给定的 $\\alpha$ 和 $w$ 构建 $\\mathbf{Q}$，执行基于稀疏 Cholesky 的 GLS，并返回估计的系数向量 $\\widehat{\\boldsymbol{\\beta}}$，四舍五入到 $6$ 位小数。\n\n- 测试用例 A (理想情况):\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.4$。\n  - $\\mathbf{X} \\in \\mathbb{R}^{5 \\times 2}$，各行为 $\\big([1,0],[1,1],[1,2],[1,3],[1,4]\\big)$。\n  - $\\mathbf{y} = [1.0, 2.2, 2.0, 3.6, 5.1]^\\top$。\n- 测试用例 B (边界情况，不相关误差):\n  - $n = 5$, $\\alpha = 1.0$, $w = 0.0$。\n  - $\\mathbf{X}$ 和 $\\mathbf{y}$ 与测试用例 A 中相同。\n- 测试用例 C (更强相关性和更高维度):\n  - $n = 6$, $\\alpha = 0.5$, $w = 1.0$。\n  - $\\mathbf{X} \\in \\mathbb{R}^{6 \\times 3}$，各行为 $\\big([1,0,0],[1,1,1],[1,2,0],[1,3,1],[1,4,0],[1,5,1]\\big)$。\n  - $\\mathbf{y} = [0.5, 1.6, 2.1, 3.2, 3.7, 4.8]^\\top$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个元素是按 A、B、C 顺序排列的一个测试用例的估计系数列表。每个系数必须四舍五入到 $6$ 位小数。例如，一个有效的输出形状类似于 $[[b_{A,1},b_{A,2}], [b_{B,1},b_{B,2}], [b_{C,1},b_{C,2},b_{C,3}]]$，其中包含实际的数值。\n- 本问题中没有物理单位或角度。\n- 所有答案必须是数字（浮点数），且该单行必须严格是所描述的聚合列表，不得包含任何额外文本。", "solution": "该问题要求计算一个线性模型的广义最小二乘法 (GLS) 估计量，该模型中的误差表现出由一个特定的三对角精度矩阵 $\\mathbf{Q}$ 定义的相关性。解决方案必须从基本原理推导得出，并且必须通过自定义的稀疏 Cholesky 分解来利用 $\\mathbf{Q}$ 的稀疏结构。\n\n线性模型由 $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$ 给出，其中 $\\mathbf{y} \\in \\mathbb{R}^n$ 是结果向量，$\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ 是待估计的系数向量，$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^n$ 是误差向量。假设误差服从多元正态分布 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$，其均值为 $\\mathbf{0}$，协方差矩阵为 $\\boldsymbol{\\Sigma}$。精度矩阵定义为 $\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}$。\n\n$\\boldsymbol{\\varepsilon}$ 的概率密度函数与 $\\exp\\left(-\\frac{1}{2}\\boldsymbol{\\varepsilon}^\\top \\mathbf{Q} \\boldsymbol{\\varepsilon}\\right)$ 成正比。代入 $\\boldsymbol{\\varepsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}$，数据的对数似然（在相差一个加性常数的情况下）与 $-\\frac{1}{2}(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})$ 成正比。最大似然估计量 $\\widehat{\\boldsymbol{\\beta}}$ 是使该二次型最小化的向量，该二次型即为 GLS 目标函数：\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top \\mathbf{Q} (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\n$$\n问题指明精度矩阵 $\\mathbf{Q}$ 是对称正定的，这允许进行唯一的 Cholesky 分解 $\\mathbf{Q} = \\mathbf{L}\\mathbf{L}^\\top$，其中 $\\mathbf{L}$ 是一个实的下三角矩阵。将此代入目标函数：\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^\\top (\\mathbf{L}\\mathbf{L}^\\top) (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) = \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)^\\top \\left(\\mathbf{L}^\\top(\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})\\right)\n$$\n这可以重写为标准的平方和形式。让我们通过左乘 $\\mathbf{L}^\\top$ 来定义“白化”变量：\n$$\n\\mathbf{y}' = \\mathbf{L}^\\top\\mathbf{y}\n$$\n$$\n\\mathbf{X}' = \\mathbf{L}^\\top\\mathbf{X}\n$$\n目标函数随后变为针对变换后变量的普通最小二乘 (OLS) 目标函数：\n$$\nS(\\boldsymbol{\\beta}) = (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta})^\\top (\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}) = ||\\mathbf{y}' - \\mathbf{X}'\\boldsymbol{\\beta}||_2^2\n$$\n这个 OLS 问题的解 $\\widehat{\\boldsymbol{\\beta}}$ 可通过求解正规方程组得到：\n$$\n(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'\n$$\n因此，计算策略是：\n$1$. 构建精度矩阵 $\\mathbf{Q}$。\n$2$. 计算其 Cholesky 因子 $\\mathbf{L}$。\n$3$. 使用 $\\mathbf{L}$ 对数据 $\\mathbf{y}$ 和 $\\mathbf{X}$ 进行白化。\n$4$. 解决由此产生的 OLS 问题。\n\n高效实现的关键在于利用 $\\mathbf{Q}$ 的特定结构。它被给定为 $\\mathbf{Q} = \\alpha \\mathbf{I}_n + w \\mathbf{L}_{\\text{path}}$，其中 $\\alpha > 0$, $w \\ge 0$，而 $\\mathbf{L}_{\\text{path}}$ 是路径图的拉普拉斯矩阵。这使得 $\\mathbf{Q}$ 成为一个对称三对角矩阵。\n设 $\\mathbf{Q}$ 的对角线为 $d_0, d_1, \\dots, d_{n-1}$，其次对角线为 $e_0, e_1, \\dots, e_{n-2}$。根据 $\\mathbf{L}_{\\text{path}}$ 的定义：\n- 对角线：$d_i = \\alpha + 2w$ 对于 $i \\in \\{1, \\dots, n-2\\}$，以及 $d_0 = d_{n-1} = \\alpha + w$。\n- 次对角线：$e_i = -w$ 对于 $i \\in \\{0, \\dots, n-2\\}$。\n\n对称三对角矩阵的 Cholesky 因子 $\\mathbf{L}$ 是一个下双对角矩阵。设其对角线为 $l_0, \\dots, l_{n-1}$，其次对角线为 $m_0, \\dots, m_{n-2}$。将 $\\mathbf{Q}$ 的元素与 $\\mathbf{L}\\mathbf{L}^\\top$ 的元素相等：\n$$\n\\mathbf{Q}_{i,i} = d_i = l_i^2 + m_{i-1}^2 \\quad (\\text{其中 } m_{-1}=0)\n$$\n$$\n\\mathbf{Q}_{i,i-1} = e_{i-1} = l_{i-1}m_{i-1}\n$$\n这导出了以下用于计算 $\\mathbf{L}$ 元素的 $O(n)$ 递推关系：\n$1$. $l_0 = \\sqrt{d_0}$\n$2$. 对于 $i = 1, \\dots, n-1$:\n   a. $m_{i-1} = e_{i-1} / l_{i-1}$\n   b. $l_i = \\sqrt{d_i - m_{i-1}^2}$\n这就构成了一个稀疏 Cholesky 分解。\n\n接下来，必须高效地执行白化步骤 $\\mathbf{v}' = \\mathbf{L}^\\top \\mathbf{v}$。矩阵 $\\mathbf{L}^\\top$ 是一个上双对角矩阵，其对角线为 $l_i$，超对角线为 $(\\mathbf{L}^\\top)_{i, i+1} = m_i$。该乘法可以在 $O(n)$ 时间内完成，而无需构建密集矩阵：\n$$\nv'_i = l_i v_i + m_i v_{i+1} \\quad \\text{对于 } i = 0, \\dots, n-2\n$$\n$$\nv'_{n-1} = l_{n-1} v_{n-1}\n$$\n应用这种稀疏乘法来计算 $\\mathbf{y}' = \\mathbf{L}^\\top \\mathbf{y}$ 以及 $\\mathbf{X}' = \\mathbf{L}^\\top \\mathbf{X}$ 的每一列。白化的总成本为 $O(np)$。\n\n最后，构建并求解 $p \\times p$ 的正规方程组 $(\\mathbf{X}')^\\top \\mathbf{X}' \\widehat{\\boldsymbol{\\beta}} = (\\mathbf{X}')^\\top \\mathbf{y}'$ 以获得 $\\widehat{\\boldsymbol{\\beta}}$，通常使用标准的线性系统求解器。整个过程避免了构建密集的 $n \\times n$ 矩阵，并且计算效率高。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    def perform_gls(n, alpha, w, X, y):\n        \"\"\"\n        Performs Generalized Least Squares using sparse Cholesky factorization.\n\n        Args:\n            n (int): Number of observations.\n            alpha (float): Parameter for the precision matrix.\n            w (float): Parameter for the precision matrix.\n            X (np.ndarray): Design matrix of shape (n, p).\n            y (np.ndarray): Outcome vector of shape (n,).\n\n        Returns:\n            np.ndarray: The estimated coefficient vector beta_hat.\n        \"\"\"\n        # Step 1: Construct the diagonal and sub-diagonal of the tridiagonal precision matrix Q.\n        # Q = alpha * I + w * L_path\n        q_diag = np.full(n, alpha + 2 * w)\n        if n > 0:\n            q_diag[0] = alpha + w\n            q_diag[-1] = alpha + w\n        q_subdiag = np.full(n - 1, -w)\n\n        # Step 2: Perform sparse Cholesky factorization Q = LL^T.\n        # L is a lower bidiagonal matrix with diagonal l_diag and sub-diagonal m_subdiag.\n        l_diag = np.zeros(n)\n        m_subdiag = np.zeros(n - 1)\n\n        if n > 0:\n            l_diag[0] = np.sqrt(q_diag[0])\n            for i in range(1, n):\n                m_subdiag[i - 1] = q_subdiag[i - 1] / l_diag[i - 1]\n                # The argument to sqrt is guaranteed to be positive because Q is positive definite.\n                l_diag[i] = np.sqrt(q_diag[i] - m_subdiag[i - 1]**2)\n\n        # Step 3: Whiten the variables using L^T.\n        # L^T is an upper bidiagonal matrix.\n        # y_prime = L^T * y and X_prime = L^T * X.\n        y_prime = np.zeros(n)\n        X_prime = np.zeros_like(X, dtype=float)\n        p = X.shape[1]\n\n        # Whiten y\n        if n > 0:\n            if n > 1:\n                for i in range(n - 1):\n                    y_prime[i] = l_diag[i] * y[i] + m_subdiag[i] * y[i + 1]\n            y_prime[n - 1] = l_diag[n - 1] * y[n - 1]\n\n        # Whiten X column by column\n        if n > 0:\n            for j in range(p):\n                x_col = X[:, j]\n                x_prime_col = np.zeros(n)\n                if n > 1:\n                    for i in range(n - 1):\n                        x_prime_col[i] = l_diag[i] * x_col[i] + m_subdiag[i] * x_col[i + 1]\n                x_prime_col[n - 1] = l_diag[n - 1] * x_col[n - 1]\n                X_prime[:, j] = x_prime_col\n        \n        # Step 4: Solve the OLS problem for the whitened variables.\n        # (X_prime^T * X_prime) * beta_hat = X_prime^T * y_prime\n        A_ols = X_prime.T @ X_prime\n        b_ols = X_prime.T @ y_prime\n        beta_hat = np.linalg.solve(A_ols, b_ols)\n\n        return beta_hat\n\n    test_cases = [\n        # Test Case A\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.4,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]], dtype=float),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case B\n        {\n            \"n\": 5, \"alpha\": 1.0, \"w\": 0.0,\n            \"X\": np.array([[1, 0], [1, 1], [1, 2], [1, 3], [1, 4]], dtype=float),\n            \"y\": np.array([1.0, 2.2, 2.0, 3.6, 5.1]),\n        },\n        # Test Case C\n        {\n            \"n\": 6, \"alpha\": 0.5, \"w\": 1.0,\n            \"X\": np.array([[1, 0, 0], [1, 1, 1], [1, 2, 0], [1, 3, 1], [1, 4, 0], [1, 5, 1]], dtype=float),\n            \"y\": np.array([0.5, 1.6, 2.1, 3.2, 3.7, 4.8]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        beta_hat = perform_gls(case[\"n\"], case[\"alpha\"], case[\"w\"], case[\"X\"], case[\"y\"])\n        # Round to 6 decimal places and convert to a list for formatting.\n        results.append(np.round(beta_hat, 6).tolist())\n\n    # Format the output as a list of lists.\n    # The f-string calls str() on each element, which correctly formats the inner lists.\n    print(f\"{results}\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3112081"}]}