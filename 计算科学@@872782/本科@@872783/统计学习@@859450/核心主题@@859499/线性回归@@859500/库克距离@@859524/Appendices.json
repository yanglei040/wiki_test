{"hands_on_practices": [{"introduction": "“影响点”的定义并非只有一种。本练习旨在通过比较不同的诊断工具，帮助您深入理解影响度的多面性。您将动手计算并比较库克距离（$D_i$），它衡量的是一个观测点对所有模型系数的整体影响，以及预测残差平方和（PRESS）残差，它衡量的是对该观测点自身的预测误差。通过这个练习[@problem_id:3111497]，您将发现“最具影响力的点”会根据您所选择的评估标准而改变，从而学会更批判性地解读回归诊断结果。", "problem": "考虑一个带截距的单变量普通最小二乘（OLS）线性回归，其中设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，其中 $p = 2$ 列对应于截距和单个预测变量。设预测变量值为 $x_i \\in \\mathbb{R}$（$i = 0, 1, \\dots, n-1$），响应为 $y_i \\in \\mathbb{R}$。模型为 $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$，OLS 估计量 $\\hat{\\beta}$ 由正规方程定义。定义帽子矩阵 $H$ 为到 $X$ 列空间的正交投影算子，残差 $r = y - X\\hat{\\beta}$，均方误差 $\\mathrm{MSE}$ 为残差平方和除以自由度 $n - p$。库克距离 $D_i$ 衡量删除观测值 $i$ 对回归估计的整体影响，而观测值 $i$ 的预测残差平方和（PRESS）残差（记为 $\\mathrm{PRESS}_i$）是观测值 $i$ 的留一法残差。\n\n您的任务是：\n- 从 OLS 正规方程和帽子矩阵 $H$ 的定义出发，推导表达式，使您能够为每个观测值 $i$ 计算库克距离 $D_i$ 和留一法残差的大小 $\\lvert \\mathrm{PRESS}_i \\rvert$，而无需重新拟合回归 $n$ 次。\n- 将删除观测值 $i$ 在特定目标点 $x^\\ast$ 处的预测损害定义为全数据拟合与删除观测值 $i$ 后的拟合在 $x^\\ast$ 处的预测值之间的绝对变化，即 $h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert$。\n- 使用这些定义，构建一个数据示例，并证明当在特定 $x^\\ast$ 处评估预测损害时，按库克距离 $D_i$ 对观测值进行的排序与按 $\\lvert \\mathrm{PRESS}_i \\rvert$ 的大小建议的排序不同。换句话说，证明被 $D_i$ 标记为最具损害性的观测值与删除后对 $x^\\ast$ 处预测改变最大的观测值不同，并且这两种情况也可能与具有最大 $\\lvert \\mathrm{PRESS}_i \\rvert$ 的观测值不同。\n\n使用以下科学上一致、自洽的数据集（其构造使得 OLS 拟合等于一条简单直线，且残差与 $X$ 的列空间正交）：\n- 观测值数量 $n = 5$。\n- 预测变量值 $x = [-2, -1, 0, 1, 2]$。\n- 选择目标直线 $y = 1 + x$ 和满足 $X^\\top r = 0$ 的残差 $r = [1, -2.5, 3, -2.5, 1]$；定义 $y_i = 1 + x_i + r_i$（$i = 0, 1, 2, 3, 4$）。\n\n测试套件（三个目标点）：\n- 情况 1：$x^\\ast = 2.5$。\n- 情况 2：$x^\\ast = 0.0$。\n- 情况 3：$x^\\ast = 10.0$。\n\n实现要求：\n- 从第一性原理出发：使用 OLS 正规方程、帽子矩阵的定义和线性代数恒等式来获得计算公式；不要重新拟合模型 $n$ 次。\n- 对于每个测试用例，计算：\n    1. 具有最大库克距离 $D_i$ 的观测值的索引（使用从零开始的索引）。\n    2. 具有最大 $\\lvert \\mathrm{PRESS}_i \\rvert$ 的观测值的索引（从零开始）。\n    3. 具有最大预测损害 $h_i(x^\\ast)$ 的观测值的索引（从零开始）。\n    4. 一个布尔值，指示最大库克距离是否对应于 $x^\\ast$ 处的最大预测损害。\n    5. 一个布尔值，指示最大 $\\lvert \\mathrm{PRESS}_i \\rvert$ 是否对应于 $x^\\ast$ 处的最大预测损害。\n- 如果出现平局（即值相等），则选择最小的索引。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含三个测试用例的结果，格式为方括号括起来的逗号分隔列表。每个测试用例的结果必须是 $[\\text{argmaxD}, \\text{argmaxPRESS}, \\text{argmaxHarm}, \\text{CookEqualsHarm}, \\text{PRESSEqualsHarm}]$ 的形式，并聚合成 $[[\\dots],[\\dots],[\\dots]]$ 在单行上。所有索引都是从零开始的整数，布尔值必须是 $\\text{True}$ 或 $\\text{False}$。", "solution": "该问题要求推导普通最小二乘（OLS）回归中几种影响诊断指标的计算公式，并将其应用于特定数据集，以证明不同的诊断指标可能对观测值的影响力给出不同的排序。\n\n### **1. 理论推导**\n\n设 OLS 模型为 $y = X\\beta + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$，$X \\in \\mathbb{R}^{n \\times p}$，$\\beta \\in \\mathbb{R}^p$，$\\varepsilon$ 是误差向量。$\\beta$ 的 OLS 估计由正规方程 $X^\\top X \\hat{\\beta} = X^\\top y$ 给出，从而得到 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$。预测值为 $\\hat{y} = X\\hat{\\beta} = Hy$，其中 $H = X(X^\\top X)^{-1}X^\\top$ 是帽子矩阵。$H$ 的对角元素，记为 $h_{ii}$，是每个观测值的杠杆值。残差为 $r = y - \\hat{y} = (I-H)y$。\n\n令 $\\hat{\\beta}^{(-i)}$ 表示当从数据集中移除观测值 $i$（对应的数据行为 $x_i^\\top$，响应为 $y_i$）时 $\\beta$ 的 OLS 估计。我们的目标是计算与此留一法（LOO）模型相关的诊断指标，而无需显式地重新拟合回归 $n$ 次。\n\n核心结果是系数向量变化量 $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$ 的表达式。这可以使用 Sherman-Morrison 公式（用于计算秩-1 更新矩阵的逆）高效地推导出来。矩阵 $(X^{(-i)})^\\top X^{(-i)}$ 可以写成 $X^\\top X$ 的秩-1 更新：\n$$ (X^{(-i)})^\\top X^{(-i)} = \\sum_{j \\neq i} x_j x_j^\\top = X^\\top X - x_i x_i^\\top $$\n应用 Sherman-Morrison 公式 $(A - uv^\\top)^{-1} = A^{-1} + \\frac{A^{-1}uv^\\top A^{-1}}{1 - v^\\top A^{-1}u}$，其中 $A=X^\\top X$ 且 $u=v=x_i$，得到：\n$$ ((X^{(-i)})^\\top X^{(-i)})^{-1} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - x_i^\\top (X^\\top X)^{-1} x_i} = (X^\\top X)^{-1} + \\frac{(X^\\top X)^{-1} x_i x_i^\\top (X^\\top X)^{-1}}{1 - h_{ii}} $$\n留一法估计为 $\\hat{\\beta}^{(-i)} = ((X^{(-i)})^\\top X^{(-i)})^{-1} (X^{(-i)})^\\top y^{(-i)}$。使用上述恒等式以及 $(X^{(-i)})^\\top y^{(-i)} = X^\\top y - x_i y_i$，经过代数简化，我们得到著名的结果：\n$$ \\hat{\\beta} - \\hat{\\beta}^{(-i)} = \\frac{(X^\\top X)^{-1} x_i (y_i - x_i^\\top \\hat{\\beta})}{1 - h_{ii}} = \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n其中 $r_i = y_i - \\hat{y}_i$ 是第 $i$ 个普通残差。该表达式是高效计算所有所需留一法诊断指标的核心。\n\n#### **PRESS 残差（$\\mathrm{PRESS}_i$）**\n预测残差平方和（PRESS）残差是使用未包含观测值 $i$ 的模型预测 $y_i$ 时的误差。\n$$ \\mathrm{PRESS}_i = y_i - \\hat{y}_i^{(-i)} = y_i - x_i^\\top \\hat{\\beta}^{(-i)} $$\n代入 $\\hat{\\beta}^{(-i)} = \\hat{\\beta} - (\\hat{\\beta} - \\hat{\\beta}^{(-i)})$：\n$$ \\mathrm{PRESS}_i = y_i - x_i^\\top \\left( \\hat{\\beta} - \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = (y_i - x_i^\\top \\hat{\\beta}) + \\frac{x_i^\\top (X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n$$ \\mathrm{PRESS}_i = r_i + \\frac{h_{ii} r_i}{1 - h_{ii}} = r_i \\left( 1 + \\frac{h_{ii}}{1 - h_{ii}} \\right) = \\frac{r_i}{1 - h_{ii}} $$\n这为 PRESS 残差提供了一个简单的公式，仅使用普通残差 $r_i$ 和杠杆值 $h_{ii}$。我们关心的是它的大小，即 $\\lvert \\mathrm{PRESS}_i \\rvert$。\n\n#### **库克距离（$D_i$）**\n库克距离衡量删除观测值 $i$ 对拟合值向量的影响。其定义为：\n$$ D_i = \\frac{(\\hat{y} - \\hat{y}^{(-i)})^\\top (\\hat{y} - \\hat{y}^{(-i)})}{p \\cdot \\mathrm{MSE}} $$\n其中 $\\hat{y}^{(-i)} = X \\hat{\\beta}^{(-i)}$ 且 $\\mathrm{MSE} = \\frac{1}{n-p} \\sum_{j=1}^n r_j^2$。分子是拟合值向量之间的欧氏距离的平方。\n$$ \\hat{y} - \\hat{y}^{(-i)} = X(\\hat{\\beta} - \\hat{\\beta}^{(-i)}) = X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} $$\n分子变为：\n$$ \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right)^\\top \\left( X \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right) = \\frac{r_i^2}{(1 - h_{ii})^2} x_i^\\top (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} x_i = \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\n将此代入 $D_i$ 的定义：\n$$ D_i = \\frac{1}{p \\cdot \\mathrm{MSE}} \\frac{r_i^2 h_{ii}}{(1 - h_{ii})^2} $$\n该公式允许从残差 $r_i$、杠杆值 $h_{ii}$、预测变量数量 $p$ 和均方误差 $\\mathrm{MSE}$ 计算 $D_i$。\n\n#### **预测损害（$h_i(x^\\ast)$）**\n在目标点 $x^\\ast$ 处的预测损害是指删除观测值 $i$ 时该点预测值的绝对变化。令 $x_{new}^\\top = [1, x^\\ast]$。\n$$ h_i(x^\\ast) = \\lvert \\hat{y}(x^\\ast) - \\hat{y}^{(-i)}(x^\\ast) \\rvert = \\lvert x_{new}^\\top \\hat{\\beta} - x_{new}^\\top \\hat{\\beta}^{(-i)} \\rvert = \\lvert x_{new}^\\top (\\hat{\\beta} - \\hat{\\beta}^{(-i)}) \\rvert $$\n使用我们关于 $\\hat{\\beta} - \\hat{\\beta}^{(-i)}$ 的核心结果：\n$$ h_i(x^\\ast) = \\left\\lvert x_{new}^\\top \\frac{(X^\\top X)^{-1} x_i r_i}{1 - h_{ii}} \\right\\rvert = \\frac{\\lvert r_i \\rvert}{1 - h_{ii}} \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert \\mathrm{PRESS}_i \\rvert \\cdot \\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert $$\n这表明预测损害是 PRESS 残差的大小，被一个依赖于目标点 $x^\\ast$ 和被删除点 $x_i$ 的因子重新加权。\n\n### **2. 应用于数据集**\n\n给定 $n=5$，$p=2$，预测变量 $x = [-2, -1, 0, 1, 2]^\\top$，以及残差 $r = [1, -2.5, 3, -2.5, 1]^\\top$。设计矩阵 $X$ 为：\n$$ X = \\begin{pmatrix} 1  -2 \\\\ 1  -1 \\\\ 1  0 \\\\ 1  1 \\\\ 1  2 \\end{pmatrix} $$\n该问题的构造使得 $y_i = (1+x_i) + r_i$ 的 OLS 拟合恰好是 $\\hat{y}_i = 1+x_i$。预测变量值是中心化的，$\\sum_{i=0}^{4} x_i = 0$。这简化了 $X^\\top X$：\n$$ X^\\top X = \\begin{pmatrix} n  \\sum x_i \\\\ \\sum x_i  \\sum x_i^2 \\end{pmatrix} = \\begin{pmatrix} 5  0 \\\\ 0  10 \\end{pmatrix} \\implies (X^\\top X)^{-1} = \\begin{pmatrix} 1/5  0 \\\\ 0  1/10 \\end{pmatrix} $$\n观测值 $i$ 的杠杆值为 $h_{ii} = x_i^\\top (X^\\top X)^{-1} x_i = \\begin{pmatrix} 1  x_i \\end{pmatrix} \\begin{pmatrix} 1/5  0 \\\\ 0  1/10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ x_i \\end{pmatrix} = \\frac{1}{5} + \\frac{x_i^2}{10}$。\n对于 $i=0, \\dots, 4$ 的杠杆值为 $[0.6, 0.3, 0.2, 0.3, 0.6]$。\n\n残差平方和为 $\\mathrm{RSS} = \\sum r_i^2 = 1^2 + (-2.5)^2 + 3^2 + (-2.5)^2 + 1^2 = 23.5$。\n均方误差为 $\\mathrm{MSE} = \\frac{\\mathrm{RSS}}{n-p} = \\frac{23.5}{5-2} = \\frac{23.5}{3}$。\n\n现在我们可以计算这些诊断指标。\n- 库克距离的因子是 $D_i \\propto r_i^2 \\frac{h_{ii}}{(1-h_{ii})^2}$。与 $D_i$ 成比例的值为 $[3.75, 3.8265, 2.8125, 3.8265, 3.75]$。最大值出现在索引 $1$ 和 $3$ 处。选择最小的索引：$\\text{argmax}(D_i) = 1$。\n- PRESS 残差的大小为 $|\\mathrm{PRESS}_i| = \\frac{|r_i|}{1-h_{ii}}$。值为 $[2.5, 3.5714, 3.75, 3.5714, 2.5]$。最大值出现在索引 $2$ 处：$\\text{argmax}(|\\mathrm{PRESS}_i|) = 2$。\n\n预测损害因子为 $\\lvert x_{new}^\\top (X^\\top X)^{-1} x_i \\rvert = \\lvert [1, x^\\ast] [1/5, x_i/10]^\\top \\rvert = \\lvert \\frac{1}{5} + \\frac{x^\\ast x_i}{10} \\rvert$。\n\n- **情况 1：$x^\\ast = 2.5$**\n$h_i(2.5) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0.25 x_i \\rvert$。值为 $[0.75, 0.1786, 0.75, 1.6071, 1.75]$。\n$\\text{argmax}(h_i(2.5)) = 4$。\n结果：$[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$。\n\n- **情况 2：$x^\\ast = 0.0$**\n$h_i(0.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + 0 \\rvert = 0.2 |\\mathrm{PRESS}_i|$。排序与 $|\\mathrm{PRESS}_i|$ 相同。\n$\\text{argmax}(h_i(0.0)) = 2$。\n结果：$[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=2, \\text{False}, \\text{True}]$。\n\n- **情况 3：$x^\\ast = 10.0$**\n$h_i(10.0) = |\\mathrm{PRESS}_i| \\cdot \\lvert 0.2 + x_i \\rvert$。值为 $[4.5, 2.8571, 0.75, 4.2857, 5.5]$。\n$\\text{argmax}(h_i(10.0)) = 4$。\n结果：$[\\text{argmaxD}=1, \\text{argmaxPRESS}=2, \\text{argmaxHarm}=4, \\text{False}, \\text{False}]$。\n\n此分析表明，“最具影响力的点”这一概念是依赖于上下文的。库克距离对整个数据空间的影响进行平均，并将观测值 $1$（和 $3$）识别为最具影响力的点，这是因为其中等大小的残差和中等杠杆值的组合。PRESS 分数仅关注自我预测，由于观测值 $2$ 具有最大的残差大小，因此将其识别为最具影响力的点。然而，预测损害取决于目标点 $x^\\ast$。对于远离数据中心（即 $|x^\\ast|$ 较大）的预测，高杠杆值点（如 $0$ 和 $4$）变得主导，因为删除它们对回归斜率的影响最大。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regression influence diagnostics problem.\n\n    The solution proceeds in three main steps:\n    1.  Setup: Define the dataset (x, r), the number of observations (n), and\n        the number of parameters (p). The response y and coefficients are\n        implicitly defined by the problem statement.\n    2.  Pre-computation: Calculate quantities that are constant across all test\n        cases. This includes the (X^T X)^-1 matrix, leverages (h_ii),\n        mean squared error (MSE), Cook's distances (D_i), and PRESS residuals.\n        The indices of the observations with the largest D_i and |PRESS_i|\n        are determined here.\n    3.  Per-case computation: Loop through each target point x_star provided\n        in the test suite. For each case, calculate the predictive harm h_i(x_star)\n        for all observations, find the index of the observation with the\n        largest harm, and compare this index with the pre-computed indices for\n        D_i and |PRESS_i|.\n    \"\"\"\n    # 1. Setup based on the problem statement\n    n = 5  # Number of observations\n    p = 2  # Number of parameters (intercept + 1 predictor)\n    x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    r = np.array([1.0, -2.5, 3.0, -2.5, 1.0]) # Ordinary residuals\n\n    # 2. Pre-computation of constant quantities\n    \n    # Design matrix X would be np.c_[np.ones(n), x]\n    # Since sum(x) = 0, X^T*X is a diagonal matrix.\n    # X_T_X = [[n, sum(x)], [sum(x), sum(x^2)]] = [[5, 0], [0, 10]]\n    X_T_X_inv = np.array([[1/n, 0], [0, 1/np.sum(x**2)]])\n\n    # Calculate leverages (h_ii). h_ii = x_i^T * (X^T*X)^-1 * x_i\n    # where x_i is the i-th row of X, i.e., [1, x_i].\n    h_ii = np.zeros(n)\n    for i in range(n):\n        x_i_row = np.array([1, x[i]])\n        h_ii[i] = x_i_row @ X_T_X_inv @ x_i_row\n\n    # Calculate Mean Squared Error (MSE)\n    rss = np.sum(r**2)\n    mse = rss / (n - p)\n\n    # Calculate Cook's Distances (D_i)\n    # D_i = r_i^2 * h_ii / (p * MSE * (1 - h_ii)^2)\n    D_i = (r**2 / (p * mse)) * (h_ii / (1 - h_ii)**2)\n    argmax_D = np.argmax(D_i)\n\n    # Calculate magnitudes of PRESS residuals\n    # |PRESS_i| = |r_i / (1 - h_ii)|\n    press_i_mag = np.abs(r / (1 - h_ii))\n    argmax_press = np.argmax(press_i_mag)\n\n    # 3. Per-case computation for each target point x_star\n    test_cases = [2.5, 0.0, 10.0]\n    results = []\n\n    for x_star in test_cases:\n        # Calculate predictive harm h_i(x_star)\n        # h_i(x_star) = |PRESS_i| * |x_new^T * (X^T*X)^-1 * x_i|\n        # where x_new = [1, x_star] and x_i = [1, x[i]]\n        \n        harm_factor = np.zeros(n)\n        x_new_row = np.array([1, x_star])\n        for i in range(n):\n            x_i_row = np.array([1, x[i]])\n            harm_factor[i] = np.abs(x_new_row @ X_T_X_inv @ x_i_row)\n        \n        h_i = press_i_mag * harm_factor\n        argmax_harm = np.argmax(h_i)\n        \n        # Check if the argmax indices match\n        cook_equals_harm = (argmax_D == argmax_harm)\n        press_equals_harm = (argmax_press == argmax_harm)\n        \n        # Append the results for the current case\n        case_result = [\n            int(argmax_D),\n            int(argmax_press),\n            int(argmax_harm),\n            bool(cook_equals_harm),\n            bool(press_equals_harm)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists is the desired format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3111497"}, {"introduction": "多重共线性是回归分析中的一个常见陷阱，它会使得模型解释和诊断变得不稳定。这个实践将引导您进行一个数值实验，亲眼见证在模型中引入一个几乎冗余的预测变量，会如何显著地改变各个数据点的库克距离分布，尽管模型的整体预测结果几乎保持不变。通过这个练习[@problem_id:3111583]，您将学到一个至关重要的教训：一个高的库克距离值，有时可能只是模型设定问题的“幻象”，而非数据点本身真的存在问题。", "problem": "你的任务是实现一个数值实验，以说明在线性回归中，由于多重共线性而引入一个近乎冗余的预测变量，会如何显著地重新分配每个观测点的 Cook 距离，同时保持拟合预测值几乎不变。你的实现必须是完全确定性的，并且必须为每个提供的测试用例计算一个布尔值。当且仅当以下两个条件同时成立时，该布尔值为真：拟合值的最大绝对变化很小，且归一化的 Cook 距离向量差异达到不可忽略的程度。\n\n从线性回归的以下基本原理开始。考虑一个响应向量 $y \\in \\mathbb{R}^{n}$ 和一个满列秩的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$。普通最小二乘（OLS）估计量 $\\hat{\\beta} \\in \\mathbb{R}^{p}$ 最小化残差平方和，并满足正规方程组。拟合值为 $\\hat{y} = X \\hat{\\beta}$，残差为 $e = y - \\hat{y}$。帽子矩阵为 $H = X (X^{\\top} X)^{-1} X^{\\top}$，其对角线元素为 $h_{ii}$。均方误差为 $\\operatorname{MSE} = \\|e\\|_{2}^{2} / (n - p)$。观测点 $i$ 的 Cook 距离是衡量 $i$ 对所有拟合系数影响的度量，根据移除第 $i$ 个观测点时系数的变化 $\\hat{\\beta} - \\hat{\\beta}_{(i)}$ 来定义，也可以等价地通过 $e$ 和 $h_{ii}$ 来表示。\n\n你的程序必须基于这些原理，实现对两个模型的拟合值和 Cook 距离的计算：一个基础模型，其预测变量为 $\\{1, x_{1}, x_{2}\\}$；一个扩展模型，其预测变量为 $\\{1, x_{1}, x_{2}, x_{3}\\}$。在多重共线性情景中，$x_{3}$ 被构造成与 $x_{1}$ 近乎冗余。在非冗余情景中，$x_{3}$ 是独立的，并在 $y$ 中携带额外信号。\n\n对于每个测试用例，执行以下操作：\n- 使用固定的随机种子生成大小为 $n$ 的数据以保证可复现性。从标准正态分布中独立抽取 $x_{1}, x_{2}$。根据下述用例描述构造 $x_{3}$。使用给定的系数和高斯噪声构造响应 $y$。在每个设计矩阵中始终包含一个全为 1 的截距列。\n- 使用普通最小二乘法拟合基础模型和扩展模型，以获得 $\\hat{y}_{\\mathrm{base}}$ 和 $\\hat{y}_{\\mathrm{ext}}$、残差 $e_{\\mathrm{base}}$ 和 $e_{\\mathrm{ext}}$，以及帽子矩阵对角线元素 $h_{ii}^{\\mathrm{base}}$ 和 $h_{ii}^{\\mathrm{ext}}$。\n- 计算每个模型中每个观测点的 Cook 距离，然后将每个模型的 Cook 距离除以其自身的总和进行归一化，以获得两个关于观测点的概率向量。如果总和为零，则该模型的归一化 Cook 距离向量使用全零向量。\n- 定义拟合值的最大绝对变化为 $\\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}|$。\n- 定义重新分配幅度为归一化 Cook 距离向量之间的 $\\ell_{1}$ 距离，即 $L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}|$。\n- 设小变化容差为 $\\varepsilon_{\\mathrm{pred}} = 0.05$（无单位，因为响应是抽象的）。设重新分配阈值为 $\\delta_{D} = 0.10$（无单位）。当且仅当 $\\Delta_{\\max} \\le \\varepsilon_{\\mathrm{pred}}$ 且 $L_{1} \\ge \\delta_{D}$ 时，测试用例的布尔结果为真。\n\n实现三个测试用例，涵盖一个理想路径、一个更强的多重共线性边界情况和一个对比性的非冗余情景：\n- 测试用例 1（理想路径，近乎冗余的预测变量）：\n  - 种子 $= 42$，$n = 120$，系数 $(\\beta_{0}, \\beta_{1}, \\beta_{2}) = (0.5, 2.0, -1.5)$，噪声标准差 $\\sigma = 0.5$。\n  - 构造 $x_{3} = x_{1} + \\tau \\,\\eta$，其中 $\\eta \\sim \\mathcal{N}(0, 1)$ 独立于 $x_{1}$，且 $\\tau = 0.02$。\n  - 构造 $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ 且不直接依赖于 $x_{3}$。\n- 测试用例 2（边界情况，更强的近冗余性和更小的样本）：\n  - 种子 $= 7$，$n = 40$，系数 $(\\beta_{0}, \\beta_{1}, \\beta_{2}) = (1.0, 1.8, 0.7)$，噪声标准差 $\\sigma = 0.3$。\n  - 构造 $x_{3} = x_{1} + \\tau \\,\\eta$，其中 $\\tau = 0.005$。\n  - 构造 $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\varepsilon$。\n- 测试用例 3（对比情况，带有额外信号的非冗余预测变量）：\n  - 种子 $= 99$，$n = 120$，系数 $(\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}) = (0.5, 2.0, -1.5, 1.5)$，噪声标准差 $\\sigma = 0.5$。\n  - 构造 $x_{3}$ 为一个独立的标准正态变量，独立于 $x_{1}$ 和 $x_{2}$。\n  - 构造 $y = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\varepsilon$。\n\n你的程序必须按上述定义计算每个测试用例的布尔结果，并生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格，例如 `[r₁,r₂,r₃]`，其中每个 $r_k$ 为 `true` 或 `false`。\n\n你的程序的最终输出必须是所描述格式的唯一一行。不允许用户输入，此问题不涉及物理单位；所有报告的量均为无单位量。角度单位和百分比在此不适用。你的代码必须是自包含的，并能使用提供的种子进行复现。预期的输出是布尔值，并且实现必须严格遵守上述步骤。", "solution": "对用户提供的问题陈述进行验证。\n\n### 步骤 1：提取已知条件\n- **任务**：实现一个数值实验，展示在线性回归中，一个近乎冗余的预测变量如何在最小程度改变拟合值的同时，影响 Cook 距离。\n- **模型**：\n    - 基础模型，预测变量为 $\\{1, x_{1}, x_{2}\\}$。\n    - 扩展模型，预测变量为 $\\{1, x_{1}, x_{2}, x_{3}\\}$。\n- **核心公式**：\n    - 普通最小二乘（OLS）估计量：$\\hat{\\beta}$，由正规方程组得出。\n    - 拟合值：$\\hat{y} = X \\hat{\\beta}$。\n    - 残差：$e = y - \\hat{y}$。\n    - 帽子矩阵：$H = X (X^{\\top} X)^{-1} X^{\\top}$，对角线元素为 $h_{ii}$。\n    - 均方误差（MSE）：$\\operatorname{MSE} = \\|e\\|_{2}^{2} / (n - p)$。\n    - Cook 距离 $D_i$：给定为可通过 $e$ 和 $h_{ii}$ 表示。标准公式为 $D_i = \\frac{e_i^2}{p \\cdot \\operatorname{MSE}} \\left[ \\frac{h_{ii}}{(1 - h_{ii})^2} \\right]$。\n- **流程**：\n    1. 使用固定种子为样本量 $n$ 生成数据（$x_1, x_2, x_3, y$）。始终包含一个截距列。\n    2. 拟合基础模型和扩展模型，以获取 $\\hat{y}_{\\mathrm{base}}$、$\\hat{y}_{\\mathrm{ext}}$ 以及用于计算 Cook 距离的统计量。\n    3. 计算两个模型中每个观测点的 Cook 距离。\n    4. 通过将每组 Cook 距离除以其各自的总和，将其归一化为概率向量 $d^{\\mathrm{base}}$ 和 $d^{\\mathrm{ext}}$。如果总和为零，则归一化向量为全零。\n    5. 计算 $\\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}|$。\n    6. 计算 $L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}|$。\n- **决策规则**：\n    - 当且仅当 $\\Delta_{\\max} \\le \\varepsilon_{\\mathrm{pred}}$ 且 $L_{1} \\ge \\delta_{D}$ 时，结果为 `true`。\n    - 阈值：$\\varepsilon_{\\mathrm{pred}} = 0.05$ 和 $\\delta_{D} = 0.10$。\n- **测试用例**：\n    - **用例 1**：种子=$42$, $n=120$, $(\\beta_0, \\beta_1, \\beta_2) = (0.5, 2.0, -1.5)$, $\\sigma=0.5$。$x_3 = x_1 + \\tau\\eta$，其中 $\\tau=0.02$。$y$ 由基础模型的预测变量生成。\n    - **用例 2**：种子=$7$, $n=40$, $(\\beta_0, \\beta_1, \\beta_2) = (1.0, 1.8, 0.7)$, $\\sigma=0.3$。$x_3 = x_1 + \\tau\\eta$，其中 $\\tau=0.005$。$y$ 由基础模型的预测变量生成。\n    - **用例 3**：种子=$99$, $n=120$, $(\\beta_0, \\beta_1, \\beta_2, \\beta_3) = (0.5, 2.0, -1.5, 1.5)$, $\\sigma=0.5$。$x_3 \\sim \\mathcal{N}(0,1)$ 是独立的。$y$ 由扩展模型的预测变量生成。\n- **输出格式**：单行 `[r1,r2,r3]`，其中 $r_k$ 是 `true` 或 `false`。\n\n### 步骤 2：使用提取的已知条件进行验证\n1.  **科学/事实合理性**：该问题很好地基于线性回归、多重共线性和影响诊断（Cook 距离）的理论。所有定义和概念都是统计学中的标准内容。\n2.  **形式化/相关性**：该问题是一个定义明确的计算任务，与理解统计学习中的影响诊断直接相关。\n3.  **完整性/一致性**：该问题已完全指定。它提供了所有必要的参数、数据生成过程、度量公式和决策阈值。没有矛盾之处。\n4.  **现实性/可行性**：该任务是一个数值模拟，并且在计算上是可行的。参数是合理的。在共线预测变量 $x_3$ 中引入噪声可确保设计矩阵保持满秩，从而避免奇异性问题。\n5.  **适定性**：该问题是适定的。步骤是确定性的（给定种子），从而导向唯一的解。\n6.  **琐碎性**：该问题并非琐碎；它需要正确实现 OLS 回归和相关的诊断，这涉及仔细的数值计算。它阐释了回归分析中一个概念上重要的点。\n7.  **可验证性**：通过使用给定的种子重新运行指定的算法，结果是可在计算上验证的。\n\n### 步骤 3：结论与行动\n该问题是**有效的**。将提供一个解决方案。\n\n### 解决方案推导\n\n解决方案涉及按描述实现一个数值实验。实现的核心将是一个执行普通最小二乘（OLS）回归并计算必要统计量的函数。为了数值稳定性，尤其是在存在多重共线性的情况下，将使用 QR 分解来获得 OLS 解。\n\n**1. OLS 拟合和诊断计算**\n给定设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，OLS 估计量 $\\hat{\\beta}$ 最小化 $\\|y - X\\beta\\|_2^2$。\n设计矩阵 $X$ 分解为 $X = QR$，其中 $Q \\in \\mathbb{R}^{n \\times p}$ 具有标准正交列（$Q^{\\top}Q = I_p$），$R \\in \\mathbb{R}^{p \\times p}$ 是一个上三角矩阵。\n通过求解稳定的三角系统 $R\\hat{\\beta} = Q^{\\top}y$ 来找到 OLS 估计量 $\\hat{\\beta}$。\n然后按如下方式计算关键量：\n- **拟合值**：$\\hat{y} = X\\hat{\\beta}$。\n- **残差**：$e = y - \\hat{y}$。\n- **杠杆值**：帽子矩阵 $H = QQ^{\\top}$ 的对角线元素 $h_{ii}$。对于每个观测点 $i$，杠杆值为 $h_{ii} = \\sum_{j=1}^{p} Q_{ij}^2$。\n- **均方误差（MSE）**：$\\operatorname{MSE} = \\frac{\\|e\\|_2^2}{n-p} = \\frac{\\sum_{i=1}^n e_i^2}{n-p}$。\n- **Cook 距离**：对于每个观测点 $i=1, \\dots, n$，使用以下公式计算 Cook 距离：\n$$ D_i = \\frac{e_i^2}{p \\cdot \\operatorname{MSE}} \\cdot \\frac{h_{ii}}{(1 - h_{ii})^2} $$\n这里，$p$ 是预测变量的数量，包括截距，因此对于基础模型 $p=3$，对于扩展模型 $p=4$。\n\n**2. 数据生成和模型比较**\n对于每个测试用例，我们首先根据指定的参数生成数据。\n- 预测变量 $x_1, x_2$ 和误差项 $\\varepsilon$ 从高斯分布中抽取。\n- 预测变量 $x_3$ 根据测试用例的情景构造：或者与 $x_1$ 近乎共线（$x_3 = x_1 + \\tau\\eta$），或者作为独立的预测变量。\n- 响应向量 $y$ 构造为预测变量的线性组合加上噪声。\n\n然后拟合两个模型：\n- **基础模型**：将 $y$ 对截距、$x_1$ 和 $x_2$ 进行回归。这会产生拟合值 $\\hat{y}_{\\mathrm{base}}$ 和 Cook 距离（$D^{\\mathrm{base}}$）。\n- **扩展模型**：将 $y$ 对截距、$x_1$、$x_2$ 和 $x_3$ 进行回归。这会产生 $\\hat{y}_{\\mathrm{ext}}$ 和 Cook 距离（$D^{\\mathrm{ext}}$）。\n\n**3. 度量计算和评估**\n计算以下两个度量来比较模型：\n- **拟合值的最大绝对变化**：\n$$ \\Delta_{\\max} = \\max_{i} |\\hat{y}_{\\mathrm{ext}, i} - \\hat{y}_{\\mathrm{base}, i}| $$\n这衡量了添加 $x_3$ 对模型预测的影响。\n- **影响的重新分配**：首先，将每个模型的 Cook 距离归一化，使其总和为 1：\n$$d_i = \\frac{D_i}{\\sum_{j=1}^n D_j}$$\n重新分配由两个归一化向量之间的 $\\ell_1$ 距离量化：\n$$ L_{1} = \\sum_{i=1}^{n} |d^{\\mathrm{ext}}_{i} - d^{\\mathrm{base}}_{i}| $$\n$L_1$ 的可能范围是 $[0, 2]$。接近 $0$ 的值意味着影响分布在两个模型中相似，而较大的值表示显著的重新分配。\n\n最后，对于每个测试用例，评估布尔条件 $(\\Delta_{\\max} \\le 0.05) \\land (L_{1} \\ge 0.10)$。当引入多重共线性时（用例 1 和 2），预计此条件为真，因为近乎冗余的预测变量 $x_3$ 不会显著改变拟合值，但会使影响度量不稳定。当添加一个有意义的独立预测变量时（用例 3），预计此条件为假，因为这应该会显著改变（并改善）拟合值，使 $\\Delta_{\\max}$ 变大。\n\n实现将为每个测试用例精确遵循这些步骤，以生成最终的布尔结果列表。", "answer": "```python\nimport numpy as np\n\ndef fit_ols_and_get_stats(X, y):\n    \"\"\"\n    Fits an OLS model using QR decomposition for numerical stability and computes\n    fitted values and Cook's distances.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n, p).\n        y (np.ndarray): Response vector of shape (n,).\n\n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: Fitted values (y_hat).\n            - np.ndarray: Cook's distances for each observation.\n    \"\"\"\n    n, p = X.shape\n\n    # For robustness against collinearity, use QR decomposition to solve the least squares problem.\n    try:\n        Q, R = np.linalg.qr(X)\n        # Solve R @ beta_hat = Q.T @ y\n        qTy = Q.T @ y\n        beta_hat = np.linalg.solve(R, qTy)\n    except np.linalg.LinAlgError:\n        # Fallback for singular matrix, although problem setup should avoid this.\n        beta_hat = np.linalg.pinv(X) @ y\n        # Recalculate Q for leverage if pinv was used.\n        Q, R = np.linalg.qr(X)\n\n    y_hat = X @ beta_hat\n    residuals = y - y_hat\n    \n    # Leverages (diagonal of hat matrix H = Q @ Q.T) are sums of squares of Q's rows.\n    H_diag = np.sum(Q**2, axis=1)\n\n    # Mean Squared Error\n    if n > p:\n        rss = np.sum(residuals**2)\n        mse = rss / (n - p)\n    else:\n        mse = 0.0\n\n    # Cook's Distances\n    cooks_d = np.zeros(n)\n    if mse > 1e-15:  # Avoid division by zero if it's a perfect fit\n        # Formula: D_i = (e_i^2 / (p * MSE)) * (h_ii / (1 - h_ii)^2)\n        term1 = (residuals**2) / (p * mse)\n        denom = (1 - H_diag)**2\n        # Use np.divide to handle cases where denom is zero (i.e., h_ii=1)\n        # to prevent inf/nan values, which would disrupt normalization.\n        term2 = np.divide(H_diag, denom, out=np.zeros_like(H_diag), where=denom > 1e-15)\n        cooks_d = term1 * term2\n            \n    return y_hat, cooks_d\n\ndef process_case(seed, n, y_coeffs, sigma, tau, case_type):\n    \"\"\"\n    Runs one full test case: generates data, fits models, computes metrics, and returns the boolean result.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Generate data\n    x1 = rng.standard_normal(n)\n    x2 = rng.standard_normal(n)\n    \n    if case_type == 'collinear':\n        eta = rng.standard_normal(n)\n        x3 = x1 + tau * eta\n        y = y_coeffs[0] + y_coeffs[1] * x1 + y_coeffs[2] * x2 + rng.normal(0, sigma, n)\n    elif case_type == 'non-redundant':\n        x3 = rng.standard_normal(n)\n        y = y_coeffs[0] + y_coeffs[1] * x1 + y_coeffs[2] * x2 + y_coeffs[3] * x3 + rng.normal(0, sigma, n)\n    \n    intercept = np.ones(n)\n    X_base = np.column_stack([intercept, x1, x2])\n    X_ext = np.column_stack([intercept, x1, x2, x3])\n    \n    # 2. Fit base and extended models\n    y_hat_base, cooks_d_base = fit_ols_and_get_stats(X_base, y)\n    y_hat_ext, cooks_d_ext = fit_ols_and_get_stats(X_ext, y)\n    \n    # 3. Calculate metrics\n    # Max absolute change in fitted values\n    delta_max = np.max(np.abs(y_hat_ext - y_hat_base))\n    \n    # L1 distance between normalized Cook's distances\n    sum_cooks_base = np.sum(cooks_d_base)\n    sum_cooks_ext = np.sum(cooks_d_ext)\n    \n    norm_cooks_base = cooks_d_base / sum_cooks_base if sum_cooks_base > 0 else np.zeros_like(cooks_d_base)\n    norm_cooks_ext = cooks_d_ext / sum_cooks_ext if sum_cooks_ext > 0 else np.zeros_like(cooks_d_ext)\n    \n    l1_dist = np.sum(np.abs(norm_cooks_ext - norm_cooks_base))\n    \n    # 4. Apply decision rule\n    epsilon_pred = 0.05\n    delta_D = 0.10\n    \n    condition_fit = delta_max <= epsilon_pred\n    condition_dist = l1_dist >= delta_D\n    \n    return condition_fit and condition_dist\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run them, and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Happy path, nearly redundant predictor\n        {'seed': 42, 'n': 120, 'y_coeffs': (0.5, 2.0, -1.5), 'sigma': 0.5, 'tau': 0.02, 'case_type': 'collinear'},\n        # Case 2: Boundary, stronger near-redundancy and smaller sample\n        {'seed': 7, 'n': 40, 'y_coeffs': (1.0, 1.8, 0.7), 'sigma': 0.3, 'tau': 0.005, 'case_type': 'collinear'},\n        # Case 3: Contrast, non-redundant predictor with additional signal\n        {'seed': 99, 'n': 120, 'y_coeffs': (0.5, 2.0, -1.5, 1.5), 'sigma': 0.5, 'tau': None, 'case_type': 'non-redundant'}\n    ]\n    \n    results = []\n    for case_params in test_cases:\n        result = process_case(**case_params)\n        results.append(str(result).lower())\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3111583"}, {"introduction": "在掌握了衡量整体影响的方法后，我们可以将诊断分析提升到一个更精细的层次。本练习将指导您从第一性原理出发，推导并实现一个“局部”影响度量，专门用来评估单个观测点对特定模型系数（例如，交互项系数）的影响。这个高级练习[@problem_id:3111539]让您能够超越泛泛的“影响大”的结论，精确地回答“这个点对哪个模型参数的影响最大”的问题，这对于验证复杂的交互效应模型尤为重要。", "problem": "给定一个带有交互项的线性回归设定。考虑一个响应向量 $y \\in \\mathbb{R}^{n}$ 和一个设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$。该矩阵包含一个截距列、两个预测变量 $x_1$ 和 $x_2$ 以及它们的交互项 $x_1 x_2$，因此 $p = 4$。假设普通最小二乘（OLS）模型为 $y = X \\beta + \\varepsilon$，其中 $\\mathbb{E}[\\varepsilon] = 0$ 且 $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I$。OLS 估计量是误差平方和的最小化器，可通过正规方程组求得。对于每个观测索引 $i \\in \\{0,1,\\dots,n-1\\}$，通过在移除 $(X,y)$ 的第 $i$ 行后重新拟合 OLS 来定义留一法估计量。设全样本估计与留一法估计之间的差异由向量 $\\Delta_i \\in \\mathbb{R}^{p}$ 表示。\n\n您的任务是：\n- 仅使用普通最小二乘（OLS）估计、留一法重新拟合以及由 $X^\\top X$ 导出的基于样本的二次型的基本定义，推导出一个有原则的、衡量每个观测总体影响的标量度量。该度量通过 $X^\\top X$ 中的信息和噪声方差的估计来缩放参数偏移 $\\Delta_i$，并按模型维度 $p$ 进行归一化。\n- 进一步，定义一个局部对应度量，该度量仅关注系数的指定子集 $S \\subset \\{0,1,\\dots,p-1\\}$。这是通过将参数偏移限制在 $S$ 中的坐标上，并使用 $X^\\top X$ 的相应主子矩阵，再按 $|S|$ 和相同的噪声方差估计进行归一化来实现的。对于本问题，$S$ 将始终是仅包含交互项系数索引的单元素集。\n- 通过直接重新拟合（一次全拟合和 $n$ 次留一法拟合）和适当的二次型，以算法方式实现这些定义。不要假设任何关于留一法或影响力的快捷恒等式；从第一性原理出发计算定义所要求的内容。\n- 将索引解释为从0开始。当要求计算局部度量与总体度量之间的比率且分母为零时，将该观测的比率定义为 $0$。\n\n使用以下测试套件。在所有情况下，按 $[1, x_1, x_2, x_1 x_2]$ 的顺序构建 $X$ 的列，并令 $S = \\{3\\}$（仅交互项系数）。使用相同的潜在数据生成系数 $\\beta^\\star = (\\beta_0,\\beta_1,\\beta_2,\\beta_3) = (1.0, 2.0, -1.5, 0.8)$ 与明确提供的噪声向量一同构成响应值。下面所有数值都应完全按照给定的值使用。\n\n测试用例 1（一个具有极端交互项值和较大响应偏差的点）：\n- $n = 10$.\n- $x_1 = [-1.0, -0.5, 0.0, 0.5, 1.0, -0.8, 0.7, -0.2, 0.3, 5.0]$.\n- $x_2 = [0.9, -1.1, 0.2, -0.4, 0.6, -0.7, 0.8, 0.3, -0.5, 5.0]$.\n- 噪声向量 $\\eta = [0.05, -0.04, 0.01, -0.02, 0.03, 0.02, -0.01, 0.00, 0.01, 0.00]$。\n- 通过 $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$（对于 $i = 0,\\dots,9$）构成 $y$，然后为最后一个响应值 $y_9$ 增加一个 $8.0$ 的额外偏差：$y_9 \\leftarrow y_9 + 8.0$。\n- 此案例的必需输出：\n  1. 使 $S = \\{3\\}$ 上的局部度量最大化的索引 $i^\\star$（一个整数）。\n  2. 在 $i^\\star$ 处的总体度量（一个浮点数），四舍五入到 $6$ 位小数。\n  3. 在 $i^\\star$ 处的局部度量（一个浮点数），四舍五入到 $6$ 位小数。\n\n测试用例 2（无极端值；中等程度的交互作用）：\n- $n = 12$.\n- $x_1 = [-1.2, -0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9, 1.2, -0.4, 0.4, 0.1]$.\n- $x_2 = [0.5, -0.4, 0.7, -0.8, 0.3, -0.2, 0.1, -0.1, 0.2, 0.9, -0.9, 0.0]$.\n- 噪声向量 $\\eta = [0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, -0.02, 0.00]$。\n- 通过 $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$（对于 $i = 0,\\dots,11$）构成 $y$。\n- 设阈值为 $\\tau = 0.05$。\n- 此案例的必需输出：\n  1. 所有 $i$ 上的最大总体度量（一个浮点数），四舍五入到 $6$ 位小数。\n  2. 局部度量严格大于 $\\tau$ 的点的数量（一个整数）。\n\n测试用例 3（一个高杠杆率的交互作用点，但残差较小）：\n- $n = 6$.\n- $x_1 = [-2.0, -1.0, 0.0, 1.0, 2.0, 4.0]$.\n- $x_2 = [0.5, -0.5, 0.2, -0.2, 0.5, 4.0]$.\n- 噪声向量 $\\eta = [0.00, 0.01, -0.01, 0.00, 0.02, 0.00]$。\n- 通过 $y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 (x_{1,i} x_{2,i}) + \\eta_i$（对于 $i = 0,\\dots,5$）构成 $y$。\n- 此案例的必需输出：\n  1. 所有 $i$ 的比率 $\\rho_i$ 的均值，其中当总体度量非零时，$\\rho_i$ 是在 $i$ 处的局部度量与总体度量的比率，如果总体度量等于 $0$，则 $\\rho_i = 0$。将此均值报告为一个浮点数，四舍五入到 $6$ 位小数。\n\n实现说明和约束：\n- 通过无偏残差方差 $\\hat{\\sigma}^2 = \\text{RSS}/(n-p)$ 从全拟合中估计噪声方差，其中 $\\text{RSS}$ 是来自全拟合的残差平方和，且 $p = 4$。\n- 对于每个观测 $i$，进行一次留一法重新拟合以获得相应的参数偏移。\n- 子集 $S$ 是 $\\{3\\}$，即仅交互项系数。\n- 所有索引都是从0开始的。\n- 您的程序应生成单行输出，其中按顺序包含所有三个测试用例的结果，展平为一个列表，并用方括号括起来，值之间用逗号分隔且无空格。具体来说，输出格式为\n  $[i^\\star, \\text{overall at } i^\\star, \\text{partial at } i^\\star, \\max \\text{ overall}, \\text{count partial}>\\tau, \\text{mean ratio}]$\n  其中浮点数按规定四舍五入到 $6$ 位小数。", "solution": "该问题要求推导并实现线性回归模型中观测值的两种影响度量，然后将这些度量应用于三个不同的测试用例。验证证实该问题具有科学依据、是适定的且客观的。我们可以着手提供解决方案。\n\n### 1. 理论框架\n\n该问题设定在普通最小二乘（OLS）线性回归的背景下。\n\n#### 1.1. 模型与估计\n模型由 $y = X \\beta + \\varepsilon$ 给出，其中 $y \\in \\mathbb{R}^n$ 是响应向量，$X \\in \\mathbb{R}^{n \\times p}$ 是设计矩阵，$\\beta \\in \\mathbb{R}^p$ 是系数向量，$\\varepsilon \\in \\mathbb{R}^n$ 是未观测到的误差向量，满足 $\\mathbb{E}[\\varepsilon] = 0$ 和 $\\operatorname{Var}(\\varepsilon) = \\sigma^2 I$。参数向量的维度为 $p=4$，且 $X$ 的列分别对应截距、两个预测变量 $x_1$ 和 $x_2$ 以及它们的交互项 $x_1 x_2$。\n\nOLS 估计量 $\\hat{\\beta}$ 最小化残差平方和 $\\|y - X\\beta\\|^2$。其解通过正规方程组得出：\n$$ (X^\\top X) \\hat{\\beta} = X^\\top y $$\n如果 $X^\\top X$ 可逆，则唯一解为 $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$。\n\n此全数据拟合的残差为 $e = y - X\\hat{\\beta}$。残差平方和为 $\\text{RSS} = e^\\top e$。误差方差 $\\sigma^2$ 的一个无偏估计量是：\n$$ \\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n-p} $$\n该估计量将用于缩放影响度量。\n\n#### 1.2. 留一法（LOO）估计\n对于每个观测 $i \\in \\{0, 1, \\dots, n-1\\}$，我们通过从 $X$ 和 $y$ 中移除第 $i$ 行来定义留一法（LOO）数据集。将它们记为 $X_{(i)} \\in \\mathbb{R}^{(n-1) \\times p}$ 和 $y_{(i)} \\in \\mathbb{R}^{n-1}$。\n\n通过将模型拟合到这个缩减的数据集，可以获得 LOO OLS 估计 $\\hat{\\beta}_{(i)}$：\n$$ \\hat{\\beta}_{(i)} = (X_{(i)}^\\top X_{(i)})^{-1} X_{(i)}^\\top y_{(i)} $$\n移除观测 $i$ 后系数估计的变化即为参数偏移向量：\n$$ \\Delta_i = \\hat{\\beta} - \\hat{\\beta}_{(i)} $$\n\n### 2. 影响度量的推导\n\n问题要求两种有原则的影响度量，我们根据所提供的规范进行推导。\n\n#### 2.1. 总体影响度量\n该度量必须使用 $X^\\top X$ 中的信息和估计的噪声方差 $\\hat{\\sigma}^2$ 来缩放参数偏移 $\\Delta_i$，并按模型维度 $p$进行归一化。\n\n在设计矩阵 $X$ 的几何背景下，衡量偏移向量 $\\Delta_i$ “大小”的一个自然方法是使用由矩阵 $X^\\top X$ 导出的二次型。这个量 $\\Delta_i^\\top (X^\\top X) \\Delta_i$ 表示由全模型和 LOO 模型做出的预测之间的平方差之和：\n$$ \\|X\\hat{\\beta} - X\\hat{\\beta}_{(i)}\\|^2 = \\|X(\\hat{\\beta} - \\hat{\\beta}_{(i)})\\|^2 = \\|X\\Delta_i\\|^2 = (X\\Delta_i)^\\top(X\\Delta_i) = \\Delta_i^\\top X^\\top X \\Delta_i $$\n该项量化了移除观测 $i$ 对拟合值 $\\hat{y}$ 的总体影响。\n\n为了创建一个标准化的无量纲度量，我们用估计的误差方差 $\\hat{\\sigma}^2$ 对其进行缩放。再通过参数数量 $p$ 进行归一化，得到每个参数的平均影响。这将文字描述形式化为以下总体影响度量 $D_i^{\\text{overall}}$：\n$$ D_i^{\\text{overall}} = \\frac{\\Delta_i^\\top (X^\\top X) \\Delta_i}{p \\hat{\\sigma}^2} $$\n这个公式等价于库克距离（Cook's distance）的标准定义。\n\n#### 2.2. 局部影响度量\n需要一个局部对应度量，它关注系数的一个子集 $S \\subset \\{0, 1, \\dots, p-1\\}$。该度量通过将参数偏移向量限制在其在 $S$ 中的分量（记为 $\\Delta_{i,S}$），并使用 $X^\\top X$ 的相应主子矩阵（记为 $(X^\\top X)_S$）来构建。结果通过子集的大小 $|S|$ 和方差估计 $\\hat{\\sigma}^2$ 进行归一化。\n\n遵循相同的逻辑，局部偏移的二次型为 $\\Delta_{i,S}^\\top (X^\\top X)_S \\Delta_{i,S}$。应用指定的归一化定义了局部影响度量 $D_{i,S}^{\\text{partial}}$：\n$$ D_{i,S}^{\\text{partial}} = \\frac{\\Delta_{i,S}^\\top (X^\\top X)_S \\Delta_{i,S}}{|S| \\hat{\\sigma}^2} $$\n对于此问题，子集是 $S=\\{3\\}$，对应于交互项 $x_1 x_2$ 的系数。因此 $|S|=1$。向量 $\\Delta_{i,S}$ 变为标量 $\\Delta_{i,3} = \\hat{\\beta}_3 - \\hat{\\beta}_{(i),3}$，矩阵 $(X^\\top X)_S$ 是标量元素 $(X^\\top X)_{3,3}$（使用从0开始的索引）。局部度量简化为：\n$$ D_{i,\\{3\\}}^{\\text{partial}} = \\frac{(\\Delta_{i,3})^2 (X^\\top X)_{3,3}}{\\hat{\\sigma}^2} $$\n\n### 3. 算法实现\n\n将直接遵循这些推导出的公式来实现解决方案，而不借助任何用于 LOO 估计的代数捷径（例如涉及帽子矩阵的那些）。\n\n对于每个测试用例：\n1.  根据给定的数据生成过程和噪声，构建设计矩阵 $X \\in \\mathbb{R}^{n \\times 4}$（列为 $[1, x_1, x_2, x_1 x_2]$）和响应向量 $y \\in \\mathbb{R}^n$。\n2.  通过求解正规方程组 $(X^\\top X) \\hat{\\beta} = X^\\top y$ 来计算全数据 OLS 估计 $\\hat{\\beta}$。\n3.  计算无偏方差估计 $\\hat{\\sigma}^2 = \\frac{1}{n-p} \\|y - X\\hat{\\beta}\\|^2$。\n4.  遍历每个观测 $i=0, \\dots, n-1$：\n    a. 通过删除第 $i$ 行来构建 LOO 数据 $(X_{(i)}, y_{(i)})$。\n    b. 通过求解 $(X_{(i)}^\\top X_{(i)}) \\hat{\\beta}_{(i)} = X_{(i)}^\\top y_{(i)}$ 来计算 LOO 估计 $\\hat{\\beta}_{(i)}$。\n    c. 计算参数偏移 $\\Delta_i = \\hat{\\beta} - \\hat{\\beta}_{(i)}$。\n    d. 使用上述推导的公式计算 $D_i^{\\text{overall}}$ 和 $D_{i,\\{3\\}}^{\\text{partial}}$。\n5.  收集计算出的度量并计算每个测试用例所需的特定输出（例如，最大值、计数、比率的均值）。\n\n所有数值结果将按要求四舍五入到 $6$ 位小数。如果 $D_i^{\\text{overall}}=0$，则比率 $\\rho_i = D_{i,\\{3\\}}^{\\text{partial}} / D_i^{\\text{overall}}$ 定义为 $0$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases and print the final result.\n    \"\"\"\n    beta_star = np.array([1.0, 2.0, -1.5, 0.8])\n    p = 4\n    S_idx = 3\n\n    # Test Case 1\n    # Givens\n    n1 = 10\n    x1_1 = np.array([-1.0, -0.5, 0.0, 0.5, 1.0, -0.8, 0.7, -0.2, 0.3, 5.0])\n    x2_1 = np.array([0.9, -1.1, 0.2, -0.4, 0.6, -0.7, 0.8, 0.3, -0.5, 5.0])\n    eta1 = np.array([0.05, -0.04, 0.01, -0.02, 0.03, 0.02, -0.01, 0.00, 0.01, 0.00])\n    y1_mod = (9, 8.0)\n    \n    # Process Case 1\n    overall1, partial1 = process_case(n1, x1_1, x2_1, beta_star, eta1, y1_mod, p, S_idx)\n    i_star = np.argmax(partial1)\n    result1 = int(i_star)\n    result2 = round(overall1[i_star], 6)\n    result3 = round(partial1[i_star], 6)\n\n    # Test Case 2\n    # Givens\n    n2 = 12\n    x1_2 = np.array([-1.2, -0.9, -0.6, -0.3, 0.0, 0.3, 0.6, 0.9, 1.2, -0.4, 0.4, 0.1])\n    x2_2 = np.array([0.5, -0.4, 0.7, -0.8, 0.3, -0.2, 0.1, -0.1, 0.2, 0.9, -0.9, 0.0])\n    eta2 = np.array([0.01, -0.02, 0.03, -0.01, 0.00, 0.02, -0.01, 0.00, 0.01, 0.02, -0.02, 0.00])\n    tau = 0.05\n    \n    # Process Case 2\n    overall2, partial2 = process_case(n2, x1_2, x2_2, beta_star, eta2, None, p, S_idx)\n    result4 = round(np.max(overall2), 6)\n    result5 = int(np.sum(partial2 > tau))\n\n    # Test Case 3\n    # Givens\n    n3 = 6\n    x1_3 = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 4.0])\n    x2_3 = np.array([0.5, -0.5, 0.2, -0.2, 0.5, 4.0])\n    eta3 = np.array([0.00, 0.01, -0.01, 0.00, 0.02, 0.00])\n\n    # Process Case 3\n    overall3, partial3 = process_case(n3, x1_3, x2_3, beta_star, eta3, None, p, S_idx)\n    ratios = np.zeros_like(overall3)\n    non_zero_mask = overall3 != 0\n    ratios[non_zero_mask] = partial3[non_zero_mask] / overall3[non_zero_mask]\n    result6 = round(np.mean(ratios), 6)\n\n    # Combine results and print\n    final_results = [result1, result2, result3, result4, result5, result6]\n    print(f\"[{','.join(map(str, final_results))}]\")\n\n\ndef process_case(n, x1, x2, beta_star, eta, y_mod, p, S_idx):\n    \"\"\"\n    Performs the full analysis for a single test case.\n    \"\"\"\n    # 1. Construct design matrix X and response vector y\n    X = np.column_stack([np.ones(n), x1, x2, x1*x2])\n    y = X @ beta_star + eta\n    if y_mod:\n        y[y_mod[0]] += y_mod[1]\n\n    # 2. Full data fit\n    XTX = X.T @ X\n    XTy = X.T @ y\n    beta_hat = np.linalg.solve(XTX, XTy)\n    \n    # 3. Estimate noise variance\n    residuals = y - X @ beta_hat\n    rss = residuals.T @ residuals\n    sigma2_hat = rss / (n - p)\n\n    overall_measures = np.zeros(n)\n    partial_measures = np.zeros(n)\n\n    # 4. Leave-one-out refitting loop\n    for i in range(n):\n        # Create LOO data\n        X_i = np.delete(X, i, axis=0)\n        y_i = np.delete(y, i, axis=0)\n        \n        # LOO fit\n        XTX_i = X_i.T @ X_i\n        XTy_i = X_i.T @ y_i\n        try:\n            beta_hat_i = np.linalg.solve(XTX_i, XTy_i)\n        except np.linalg.LinAlgError:\n            # Handle cases where LOO matrix is singular, not expected for these tests\n            beta_hat_i = np.full(p, np.nan)\n\n        # Parameter shift\n        delta_i = beta_hat - beta_hat_i\n        \n        # Avoid issues with division by zero or NaN propagation\n        if np.isnan(delta_i).any() or sigma2_hat == 0:\n            overall_measures[i] = 0.0\n            partial_measures[i] = 0.0\n            continue\n        \n        # Overall influence measure\n        numerator_overall = delta_i.T @ XTX @ delta_i\n        d_overall = numerator_overall / (p * sigma2_hat)\n        overall_measures[i] = d_overall\n\n        # Partial influence measure\n        delta_i_S = delta_i[S_idx] \n        XTX_S_val = XTX[S_idx, S_idx]\n        \n        numerator_partial = (delta_i_S**2) * XTX_S_val\n        # |S| = 1, so no division by |S|\n        d_partial = numerator_partial / sigma2_hat\n        partial_measures[i] = d_partial\n\n    return overall_measures, partial_measures\n\nsolve()\n\n```", "id": "3111539"}]}