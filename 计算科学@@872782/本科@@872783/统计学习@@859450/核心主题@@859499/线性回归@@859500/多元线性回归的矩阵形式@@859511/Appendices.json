{"hands_on_practices": [{"introduction": "在多元回归中，我们为每个预测变量估算一个系数。但当预测变量相互关联时，这个系数究竟意味着什么？Frisch-Waugh-Lovell (FWL) 定理为此提供了一个深刻的几何解释，它表明多元回归中的系数等同于对“残差化”变量进行简单回归得到的系数。通过这个练习 [@problem_id:3146050]，您将推导并应用 FWL 定理，从而深刻理解回归系数作为条件效应的本质，即在控制了其他所有变量后，单个预测变量的独特贡献。", "problem": "考虑矩阵形式的多元线性回归，其中响应向量为 $y \\in \\mathbb{R}^{n}$，设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，分区为 $X = [X_{1}\\;X_{2}]$，其中 $X_{1} \\in \\mathbb{R}^{n \\times 1}$ 是一个我们感兴趣的预测变量，$X_{2} \\in \\mathbb{R}^{n \\times q}$ 包含一个截距项和其他预测变量。普通最小二乘估计定义为在 $\\beta \\in \\mathbb{R}^{p}$ 上残差平方范数 $\\|y - X\\beta\\|_{2}^{2}$ 的最小化子。设到 $X_{2}$ 列空间的投影矩阵为 $P_{X_{2}} = X_{2}(X_{2}^{\\top}X_{2})^{-1}X_{2}^{\\top}$，相关的残差生成矩阵为 $M_{X_{2}} = I - P_{X_{2}}$。从这些定义和最小二乘准则所蕴含的正规方程出发，推导一个仅依赖于 $X_{1}$、$y$ 和 $M_{X_{2}}$ 的、$X_{1}$ 所对应的普通最小二乘系数的显式表达式。\n\n为了在协变量结构重叠的情况下对推导进行压力测试，考虑 $n = 5$ 个观测值，其中\n$x_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$，$z = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$，$1_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$，$X_{2} = [1_{n}\\;z]$，以及 $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$。\n使用您推导的表达式计算 $x_{1}$ 上的系数的数值。无需四舍五入。最后，根据条件效应解释，当 $x_{1}$ 与 $X_{2}$ 的列显著重叠时，这个系数代表什么。", "solution": "首先通过提取给定信息并评估其科学合理性、完整性和客观性来验证问题。\n\n**第一步：提取已知条件**\n- 响应向量：$y \\in \\mathbb{R}^{n}$\n- 设计矩阵：$X \\in \\mathbb{R}^{n \\times p}$，分区为 $X = [X_{1}\\;X_{2}]$\n- 分区：$X_{1} \\in \\mathbb{R}^{n \\times 1}$（感兴趣的预测变量），$X_{2} \\in \\mathbb{R}^{n \\times q}$（截距项和其他预测变量）。总预测变量数为 $p=1+q$。\n- OLS准则：在 $\\beta \\in \\mathbb{R}^{p}$ 上最小化 $\\|y - X\\beta\\|_{2}^{2}$。\n- $X_2$ 的投影矩阵：$P_{X_{2}} = X_{2}(X_{2}^{\\top}X_{2})^{-1}X_{2}^{\\top}$\n- $X_2$ 的残差生成矩阵：$M_{X_{2}} = I - P_{X_{2}}$\n- 任务1：推导 $X_1$ 的OLS系数（记为 $\\hat{\\beta}_1$）的显式表达式，该表达式仅用 $X_1$、$y$ 和 $M_{X_2}$ 表示。\n- 任务2：对于 $n=5$，根据给定数据计算 $\\hat{\\beta}_1$：\n  - $X_{1} = x_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$\n  - $z = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$\n  - $1_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$\n  - $X_{2} = [1_{n}\\;z]$\n  - $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$\n- 任务3：在条件效应和协变量重叠的背景下，解释该系数的含义。\n\n**第二步：使用提取的已知条件进行验证**\n该问题定义明确且具有科学依据。它提出了线性模型中的一个标准理论推导，即Frisch-Waugh-Lovell定理，随后是一个直接的数值应用。所提供的数据和定义是自洽且一致的。数值示例中的矩阵被指定，以使必要的矩阵逆 $(X_{2}^{\\top}X_{2})^{-1}$ 和 $(X^{\\top}X)^{-1}$ 存在，从而确保这是一个适定问题。语言客观、正式。未发现任何缺陷。\n\n**第三步：结论与行动**\n问题有效。将提供完整解答。\n\n**$\\hat{\\beta}_1$ 表达式的推导**\n\n多元线性回归模型由 $y = X\\beta + \\epsilon$ 给出。普通最小二乘（OLS）估计 $\\hat{\\beta}$ 是使残差平方和 $S(\\beta) = \\|y - X\\beta\\|_{2}^{2}$ 最小化的向量。该最小化问题的解满足正规方程：\n$$\nX^{\\top}X\\hat{\\beta} = X^{\\top}y\n$$\n我们根据问题陈述对设计矩阵 $X$ 和系数向量 $\\beta$ 进行分区：\n$$\nX = [X_{1}\\;X_{2}] \\quad \\text{和} \\quad \\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\n$$\n其中 $\\beta_1$ 是对应于单个预测变量 $X_1$ 的标量，而 $\\beta_2 \\in \\mathbb{R}^q$ 对应于 $X_2$ 中的预测变量。OLS估计值记为 $\\hat{\\beta}_1$ 和 $\\hat{\\beta}_2$。\n\n将分区形式代入正规方程，得到一个分块矩阵系统：\n$$\n\\begin{pmatrix} X_1^{\\top} \\\\ X_2^{\\top} \\end{pmatrix} [X_1\\;X_2] \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} X_1^{\\top}y \\\\ X_2^{\\top}y \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} X_1^{\\top}X_1 & X_1^{\\top}X_2 \\\\ X_2^{\\top}X_1 & X_2^{\\top}X_2 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} X_1^{\\top}y \\\\ X_2^{\\top}y \\end{pmatrix}\n$$\n这个分块系统展开为两个方程：\n1. $X_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2 \\hat{\\beta}_2 = X_1^{\\top}y$\n2. $X_2^{\\top}X_1 \\hat{\\beta}_1 + X_2^{\\top}X_2 \\hat{\\beta}_2 = X_2^{\\top}y$\n\n我们的目标是找到 $\\hat{\\beta}_1$ 的表达式。我们可以通过求解第二个方程得到 $\\hat{\\beta}_2$，然后将其代入第一个方程来实现这一目标。从方程(2)出发，假设 $X_2$ 具有满列秩，因此 $(X_2^{\\top}X_2)^{-1}$ 存在：\n$$\nX_2^{\\top}X_2 \\hat{\\beta}_2 = X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1\n$$\n$$\n\\hat{\\beta}_2 = (X_2^{\\top}X_2)^{-1}(X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1)\n$$\n现在，将这个 $\\hat{\\beta}_2$ 的表达式代入方程(1)：\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2 (X_2^{\\top}X_2)^{-1}(X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1) = X_1^{\\top}y\n$$\n展开各项：\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}y - X_1^{\\top}X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1 \\hat{\\beta}_1 = X_1^{\\top}y\n$$\n使用投影矩阵的定义 $P_{X_2} = X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}$，我们可以简化方程：\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}P_{X_2}y - X_1^{\\top}P_{X_2}X_1 \\hat{\\beta}_1 = X_1^{\\top}y\n$$\n将包含 $\\hat{\\beta}_1$ 的项组合到左侧，其他项移到右侧：\n$$\n(X_1^{\\top}X_1 - X_1^{\\top}P_{X_2}X_1) \\hat{\\beta}_1 = X_1^{\\top}y - X_1^{\\top}P_{X_2}y\n$$\n在两边提出因子 $X_1^{\\top}$：\n$$\nX_1^{\\top}(I - P_{X_2})X_1 \\hat{\\beta}_1 = X_1^{\\top}(I - P_{X_2})y\n$$\n使用残差生成矩阵的定义 $M_{X_2} = I - P_{X_2}$，我们得到：\n$$\n(X_1^{\\top}M_{X_2}X_1) \\hat{\\beta}_1 = X_1^{\\top}M_{X_2}y\n$$\n由于 $X_1$ 是一个单列向量，$X_1^{\\top}M_{X_2}X_1$ 是一个标量。只要 $X_1$ 不在 $X_2$ 的列空间中（即 $M_{X_2}X_1 \\neq 0$），这个标量就非零，我们就可以解出 $\\hat{\\beta}_1$：\n$$\n\\hat{\\beta}_1 = (X_1^{\\top}M_{X_2}X_1)^{-1} (X_1^{\\top}M_{X_2}y)\n$$\n这就是所求的表达式。\n\n**数值计算**\n\n给定：\n$X_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$，$X_{2} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix}$，$y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$。\n\n我们需要计算量 $\\tilde{x}_1 = M_{X_2}X_1$ 和 $\\tilde{y} = M_{X_2}y$。它们分别是将 $X_1$ 和 $y$ 对 $X_2$ 进行回归所得的残差。\n首先，计算 $X_2^{\\top}X_2$ 及其逆矩阵：\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} = \\begin{pmatrix} 5 & 17 \\\\ 17 & 69 \\end{pmatrix}\n$$\n行列式为 $\\det(X_2^{\\top}X_2) = (5)(69) - (17)(17) = 345 - 289 = 56$。\n逆矩阵为：\n$$\n(X_2^{\\top}X_2)^{-1} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix}\n$$\n向量 $\\tilde{x}_1 = M_{X_2}X_1 = X_1 - P_{X_2}X_1 = X_1 - X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1$。\n首先，计算 $X_2^{\\top}X_1$：\n$$\nX_2^{\\top}X_1 = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix}\n$$\n将 $X_1$ 对 $X_2$ 回归的OLS系数为 $\\hat{\\gamma}_{x_1} = (X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1$：\n$$\n\\hat{\\gamma}_{x_1} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix} \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 1035 - 1037 \\\\ -255 + 305 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} -2 \\\\ 50 \\end{pmatrix} = \\begin{pmatrix} -1/28 \\\\ 25/28 \\end{pmatrix}\n$$\n拟合值为 $\\hat{X}_1 = X_2\\hat{\\gamma}_{x_1}$，残差为 $\\tilde{x}_1 = X_1 - \\hat{X}_1$：\n$$\n\\tilde{x}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 25 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 24 \\\\ 74 \\\\ 74 \\\\ 124 \\\\ 124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 28-24 \\\\ 56-74 \\\\ 84-74 \\\\ 112-124 \\\\ 140-124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\n接下来，我们计算 $y$ 的残差 $\\tilde{y} = M_{X_2}y = y - X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}y$。首先，计算 $X_2^{\\top}y$：\n$$\nX_2^{\\top}y = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} = \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix}\n$$\n将 $y$ 对 $X_2$ 回归的OLS系数为 $\\hat{\\gamma}_{y} = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$：\n$$\n\\hat{\\gamma}_{y} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix} \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 3312 - 2924 \\\\ -816 + 860 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 388 \\\\ 44 \\end{pmatrix} = \\begin{pmatrix} 97/14 \\\\ 11/14 \\end{pmatrix}\n$$\n残差为 $\\tilde{y} = y - X_2\\hat{\\gamma}_y$：\n$$\n\\tilde{y} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} 97 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 108 \\\\ 130 \\\\ 130 \\\\ 152 \\\\ 152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 112-108 \\\\ 112-130 \\\\ 140-130 \\\\ 140-152 \\\\ 168-152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\n现在我们计算 $\\hat{\\beta}_1$：\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{x}_1^{\\top}\\tilde{y}}{\\tilde{x}_1^{\\top}\\tilde{x}_1}\n$$\n我们观察到 $\\tilde{y} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$ 和 $\\tilde{x}_1 = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$。\n因此，我们有一个直接关系：$\\tilde{y} = 2 \\tilde{x}_1$。\n将此代入 $\\hat{\\beta}_1$ 的公式中：\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{x}_1^{\\top}(2\\tilde{x}_1)}{\\tilde{x}_1^{\\top}\\tilde{x}_1} = \\frac{2(\\tilde{x}_1^{\\top}\\tilde{x}_1)}{\\tilde{x}_1^{\\top}\\tilde{x}_1} = 2\n$$\n只要 $\\tilde{x}_1$ 不是零向量，这个结果就有效，本例中情况如此。该系数的数值为 $2$。\n\n**系数的解释**\n\n系数 $\\hat{\\beta}_1$ 表示预测变量 $X_1$ 对响应 $y$ 的估计条件效应。如推导所示，$\\hat{\\beta}_1$ 是将 $y$ 的残差（在对 $X_2$ 回归后）对 $X_1$ 的残差（在对 $X_2$ 回归后）进行简单回归得到的系数。这意味着 $\\hat{\\beta}_1$ 量化了 $X_1$ 中与 $X_2$ 中预测变量线性无关的部分与 $y$ 中与 $X_2$ 中预测变量线性无关的部分之间的关联。在实践中，它是在保持 $X_2$ 中所有其他预测变量的值不变的情况下，$X_1$ 预测变量的值每增加一个单位，$y$ 的期望值的估计变化量。\n\n当 $X_1$ 与 $X_2$ 的列“显著重叠”时，这意味着高度的多重共线性。在这种情况下，$X_1$ 中的大部分变异已经被 $X_2$ 中的预测变量所解释。因此，残差向量 $\\tilde{x}_1 = M_{X_2}X_1$ 的分量将具有很小的量级，其平方范数 $\\|M_{X_2}X_1\\|_2^2 = X_1^{\\top}M_{X_2}X_1$（即 $\\hat{\\beta}_1$ 最终表达式中的分母）将接近于零。\n\n估计量 $\\hat{\\beta}_1$ 的方差由 $\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 (X_1^{\\top}M_{X_2}X_1)^{-1}$ 给出，其中 $\\sigma^2$ 是误差方差。如果分母 $X_1^{\\top}M_{X_2}X_1$ 很小，$\\hat{\\beta}_1$ 的方差就会变得非常大。这意味着估计值不稳定且不可靠。大方差意味着大标准误和宽置信区间，表明关于 $\\beta_1$ 真实值存在很大的不确定性。虽然该系数在形式上仍然代表 $X_1$ 的条件效应，但与 $X_2$ 的显著重叠使得模型难以分离出 $X_1$ 对 $y$ 变异的独特贡献。数据根本不包含足够的信息来精确估计这种效应。", "answer": "$$\n\\boxed{2}\n$$", "id": "3146050"}, {"introduction": "当预测变量高度相关（即存在多重共线性）时，普通最小二乘法 (OLS) 的估计结果可能非常不稳定。数据的微小变动可能导致系数发生巨大变化。这个问题可以通过设计矩阵的奇异值分解 (SVD) 来理解，其中微小的奇异值对应于不稳定的方向。岭回归通过增加一个惩罚项来“收缩”系数，从而解决了这个问题。这个练习 [@problem_id:3146059] 将指导您构建一个病态矩阵，亲眼观察 OLS 的不稳定性，并理解岭回归如何通过 SVD 的视角稳定模型估计。", "problem": "考虑多元线性回归的矩阵表示，重点关注奇异值分解（SVD）在解释正则化中的作用。推导过程应纯粹使用数学术语，不涉及物理单位。从最小二乘估计量（作为平方误差的最小化器）和岭估计量（作为带有平方参数惩罚项的平方误差的最小化器）的定义出发。您不能先验地假设任何闭式解；而应在需要时根据这些定义和线性代数知识推导它们。目标是表达岭回归下的预测，并通过SVD解释惩罚项如何收缩沿微小奇异方向的贡献，然后设计一个具体的数值矩阵和响应向量，其中微小奇异方向在普通最小二乘法中占主导地位，并观察岭回归如何稳定它们。\n\n任务：\n1) 按如下方式构造设计矩阵 $X \\in \\mathbb{R}^{6 \\times 3}$，其中每个条目都是实标量：\n$$\nX =\n\\begin{bmatrix}\n1 & 1 + 10^{-4}\\cdot 1 & 1 + 2\\cdot 10^{-4}\\cdot (-2) \\\\\n2 & 2 + 10^{-4}\\cdot (-2) & 2 + 2\\cdot 10^{-4}\\cdot 1 \\\\\n3 & 3 + 10^{-4}\\cdot 3 & 3 + 2\\cdot 10^{-4}\\cdot (-3) \\\\\n4 & 4 + 10^{-4}\\cdot (-4) & 4 + 2\\cdot 10^{-4}\\cdot 4 \\\\\n5 & 5 + 10^{-4}\\cdot 5 & 5 + 2\\cdot 10^{-4}\\cdot (-5) \\\\\n6 & 6 + 10^{-4}\\cdot (-6) & 6 + 2\\cdot 10^{-4}\\cdot 6\n\\end{bmatrix}.\n$$\n该矩阵是满列秩的，但由于其列向量几乎共线，因此是高度病态的。\n\n2) 计算奇异值分解 $X = U\\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{6 \\times 3}$ 具有标准正交列，$\\Sigma \\in \\mathbb{R}^{3 \\times 3}$ 是对角矩阵，其非负对角元按非递增顺序排列，且 $V \\in \\mathbb{R}^{3 \\times 3}$ 是标准正交矩阵。定义响应向量 $y \\in \\mathbb{R}^{6}$ 为对应于最小奇异值的左奇异向量，即 $U$ 的最后一列。\n\n3) 将普通最小二乘估计量 $\\hat{\\beta}_{\\text{OLS}} \\in \\mathbb{R}^{3}$ 定义为以下问题的任意最小化器\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_2^2.\n$$\n对于任意惩罚强度 $\\lambda \\ge 0$，将岭估计量 $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{3}$ 定义为以下问题的任意最小化器\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2.\n$$\n计算拟合值 $\\hat{y}_{\\text{OLS}} = X\\hat{\\beta}_{\\text{OLS}}$ 和 $\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda}$。\n\n4) 将岭回归解释为对奇异方向的滤波。使用 $X$ 的SVD，在左奇异向量基下表示预测值 $\\hat{y}_{\\lambda}$，并确定应用于每个奇异分量的收缩因子。然后，使用步骤2中选择的 $y$，通过计算验证与最小奇异值相关的奇异方向是收缩最强的方向。\n\n5) 测试套件。使用上面定义的单个矩阵 $X$ 和响应 $y$，并评估以下四个共同构成测试套件的案例：\n- 案例A（边界情况，惩罚为零）：$\\lambda = 0$。验证岭回归预测是否与普通最小二乘预测一致（在 $10^{-8}$ 的数值容差内）。输出一个布尔值，其计算方式为 $ \\|\\hat{y}_{\\lambda} - \\hat{y}_{\\text{OLS}}\\|_2 \\le 10^{-8}$。\n- 案例B（小惩罚，轻度收缩）：$\\lambda = 10^{-6}$。输出比率 $ \\|\\hat{\\beta}_{\\lambda}\\|_2 / \\|\\hat{\\beta}_{\\text{OLS}}\\|_2 $，作为一个浮点数。\n- 案例C（中等惩罚，奇异方向滤波器检查）：$\\lambda = 10^{-4}$。令 $v_{\\min}$ 为对应于最小奇异值 $\\sigma_{\\min}$ 的右奇异向量。计算分量收缩比\n$$\nr_{\\text{emp}} = \\frac{|\\langle v_{\\min}, \\hat{\\beta}_{\\lambda} \\rangle|}{|\\langle v_{\\min}, \\hat{\\beta}_{\\text{OLS}} \\rangle|},\n$$\n和理论滤波因子\n$$\nr_{\\text{theory}} = \\frac{\\sigma_{\\min}^2}{\\sigma_{\\min}^2 + \\lambda}.\n$$\n输出绝对偏差 $|r_{\\text{emp}} - r_{\\text{theory}}|$，作为一个浮点数。\n- 案例D（大惩罚，重度收缩）：$\\lambda = 10^{2}$。输出欧几里得范数 $\\|\\hat{\\beta}_{\\lambda}\\|_2$，作为一个浮点数。\n\n6) 最终输出格式。您的程序应生成单行输出，其中包含按案例A到D顺序排列的四个结果，形式为逗号分隔的列表，并用方括号括起来（例如，“[resultA,resultB,resultC,resultD]”）。这些结果必须分别是案例A的布尔值和案例B到D的三个浮点数。", "solution": "该问题要求将岭回归作为一种收缩估计量进行分析，并通过设计矩阵 $X$ 的奇异值分解（SVD）来解释。我们必须首先从其优化定义中推导出估计量，然后将它们应用于一个为突显正则化效应而专门构建的病态矩阵 $X$ 和响应向量 $y$。\n\n首先，我们从第一性原理推导普通最小二乘（OLS）和岭回归估计量。\n\nOLS估计量 $\\hat{\\beta}_{\\text{OLS}}$ 定义为残差平方和的最小化器：\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 = \\frac{1}{2}(y - X\\beta)^\\top(y - X\\beta)\n$$\n为求最小值，我们计算 $L(\\beta)$ 关于 $\\beta$ 的梯度并将其设为零。\n$$\n\\nabla_\\beta L(\\beta) = \\nabla_\\beta \\left( \\frac{1}{2}(y^\\top y - 2y^\\top X\\beta + \\beta^\\top X^\\top X\\beta) \\right) = -X^\\top y + X^\\top X\\beta\n$$\n将梯度设为零，得到正规方程组：\n$$\nX^\\top X \\hat{\\beta}_{\\text{OLS}} = X^\\top y\n$$\n由于问题指明 $X$ 是一个 $6 \\times 3$ 的满列秩矩阵，因此矩阵 $X^\\top X \\in \\mathbb{R}^{3 \\times 3}$ 是可逆的。因此，唯一的OLS解为：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y\n$$\n\n岭估计量 $\\hat{\\beta}_{\\lambda}$ 定义为带惩罚的残差平方和的最小化器：\n$$\nL_\\lambda(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数。梯度为：\n$$\n\\nabla_\\beta L_\\lambda(\\beta) = \\nabla_\\beta \\left( \\frac{1}{2}\\|y - X\\beta\\|_2^2 \\right) + \\nabla_\\beta \\left( \\frac{\\lambda}{2}\\beta^\\top\\beta \\right) = (-X^\\top y + X^\\top X\\beta) + \\lambda\\beta\n$$\n将梯度设为零可得：\n$$\n(X^\\top X + \\lambda I)\\hat{\\beta}_{\\lambda} = X^\\top y\n$$\n对于任意 $\\lambda > 0$，矩阵 $(X^\\top X + \\lambda I)$ 是正定的，因此是可逆的。对于 $\\lambda = 0$，它简化为OLS情况。唯一的岭回归解是：\n$$\n\\hat{\\beta}_{\\lambda} = (X^\\top X + \\lambda I)^{-1} X^\\top y\n$$\n\n接下来，我们使用 $X$ 的奇异值分解（SVD）来解释这些估计量。令 $X = U\\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{6 \\times 3}$ 具有标准正交列，$\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ 是一个 $3 \\times 3$ 对角矩阵，其奇异值满足 $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 > 0$，且 $V \\in \\mathbb{R}^{3 \\times 3}$ 是一个标准正交矩阵。\n我们将SVD代入估计量的表达式中。首先，注意到 $X^\\top X = (U\\Sigma V^\\top)^\\top(U\\Sigma V^\\top) = V\\Sigma^\\top U^\\top U\\Sigma V^\\top = V\\Sigma^2 V^\\top$。\n\n对于OLS估计量：\n$$\n\\hat{\\beta}_{\\text{OLS}} = (V\\Sigma^2 V^\\top)^{-1} (U\\Sigma V^\\top)^\\top y = (V(\\Sigma^2)^{-1}V^\\top)(V\\Sigma U^\\top y) = V\\Sigma^{-1}U^\\top y\n$$\n如果我们分别将 $U$ 和 $V$ 的列表示为 $u_j$ 和 $v_j$，则可以写成一个和式：\n$$\n\\hat{\\beta}_{\\text{OLS}} = \\sum_{j=1}^{3} v_j \\frac{u_j^\\top y}{\\sigma_j}\n$$\nOLS解是右奇异向量 $v_j$ 的线性组合，其系数是 $y$ 在左奇异向量 $u_j$ 上的投影，并被相应奇异值的倒数 $1/\\sigma_j$放大。\n\n对于岭估计量：\n$$\nX^\\top X + \\lambda I = V\\Sigma^2 V^\\top + \\lambda V V^\\top = V(\\Sigma^2 + \\lambda I)V^\\top\n$$\n其逆矩阵为 $(X^\\top X + \\lambda I)^{-1} = V(\\Sigma^2 + \\lambda I)^{-1}V^\\top$。因此，\n$$\n\\hat{\\beta}_{\\lambda} = (V(\\Sigma^2 + \\lambda I)^{-1}V^\\top)(V\\Sigma U^\\top y) = V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^\\top y\n$$\n用求和符号表示，即为：\n$$\n\\hat{\\beta}_{\\lambda} = \\sum_{j=1}^{3} v_j \\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}(u_j^\\top y)\n$$\n比较 $\\hat{\\beta}_{\\text{OLS}}$ 和 $\\hat{\\beta}_{\\lambda}$ 展开式中 $v_j$ 的系数，我们看到一个乘法因子：\n$$\n\\frac{\\sigma_j/(\\sigma_j^2+\\lambda)}{1/\\sigma_j} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}\n$$\n这个因子总是在区间 $[0, 1]$ 内，是收缩因子。对于大的奇异值 $\\sigma_j \\gg \\sqrt{\\lambda}$，该因子接近于1，收缩很小。对于小的奇异值 $\\sigma_j \\ll \\sqrt{\\lambda}$，该因子接近于0，解的相应分量被强烈地收缩至零。\n\n预测值为 $\\hat{y} = X\\beta$。对于OLS，$\\hat{y}_{\\text{OLS}} = X\\hat{\\beta}_{\\text{OLS}} = (U\\Sigma V^\\top)(V\\Sigma^{-1}U^\\top y) = UU^\\top y$，这是 $y$ 到 $X$ 列空间上的正交投影。对于岭回归，\n$$\n\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda} = (U\\Sigma V^\\top)(V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^\\top y) = U \\Sigma (\\Sigma^2 + \\lambda I)^{-1} \\Sigma U^\\top y = \\sum_{j=1}^{3} u_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\right) (u_j^\\top y)\n$$\n这表明岭回归的预测是通过将 $y$ 沿着左奇异向量 $u_j$ 的分量乘以相同的因子来形成的。\n\n问题将 $y$ 定义为对应于最小奇异值的左奇异向量，即 $y = u_3$（假设 $\\sigma_3 = \\sigma_{\\min}$）。根据 $U$ 的列向量的标准正交性，我们有 $u_j^\\top y = u_j^\\top u_3 = \\delta_{j3}$。\n这极大地简化了估计量：\n$$\n\\hat{\\beta}_{\\text{OLS}} = v_3 \\frac{1}{\\sigma_3}\n$$\n$$\n\\hat{\\beta}_{\\lambda} = v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\n当 $\\sigma_3 \\to 0$ 时，OLS解的量级会爆炸式增长，而岭回归解则保持有界。\n\n我们现在进行测试套件的具体计算。\n令 $X$ 为所给的 $6 \\times 3$ 矩阵。我们计算其SVD $X=U\\Sigma V^\\top$ 并定义 $y = u_3$，其中 $u_3$ 对应于 $\\sigma_3 = \\sigma_{\\min}$。令 $v_3 = v_{\\min}$。\n\n案例A ($\\lambda = 0$)：我们比较 $\\hat{y}_{0}$ 和 $\\hat{y}_{\\text{OLS}}$。根据岭回归预测公式，当 $\\lambda=0$ 时：\n$$\n\\hat{y}_{0} = \\sum_{j=1}^{3} u_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+0} \\right) (u_j^\\top y) = \\sum_{j=1}^{3} u_j (u_j^\\top y) = UU^\\top y = \\hat{y}_{\\text{OLS}}\n$$\n预测值必须相同。在数值上，$\\|\\hat{y}_{0} - \\hat{y}_{\\text{OLS}}\\|_2$ 应接近于0。\n\n案例B ($\\lambda = 10^{-6}$)：我们计算范数的比率。\n$$\n\\frac{\\|\\hat{\\beta}_{\\lambda}\\|_2}{\\|\\hat{\\beta}_{\\text{OLS}}\\|_2} = \\frac{\\|v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\\|_2}{\\|v_3 \\frac{1}{\\sigma_3}\\|_2} = \\frac{|\\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}|}{|\\frac{1}{\\sigma_3}|} \\|v_3\\|_2 = \\frac{\\sigma_3^2}{\\sigma_3^2 + \\lambda}\n$$\n这是 $\\sigma_{\\min}$ 的理论收缩因子。\n\n案例C ($\\lambda = 10^{-4}$)：我们测试分量收缩比。\n经验比率为 $r_{\\text{emp}} = \\frac{|\\langle v_{\\min}, \\hat{\\beta}_{\\lambda} \\rangle|}{|\\langle v_{\\min}, \\hat{\\beta}_{\\text{OLS}} \\rangle|}$。\n使用 $v_{\\min} = v_3$ 以及推导出的估计量表达式：\n$$\n\\langle v_3, \\hat{\\beta}_{\\text{OLS}} \\rangle = \\langle v_3, v_3 \\frac{1}{\\sigma_3} \\rangle = \\frac{1}{\\sigma_3} \\|v_3\\|_2^2 = \\frac{1}{\\sigma_3}\n$$\n$$\n\\langle v_3, \\hat{\\beta}_{\\lambda} \\rangle = \\langle v_3, v_3 \\frac{\\sigma_3}{\\sigma_3^2+\\lambda} \\rangle = \\frac{\\sigma_3}{\\sigma_3^2+\\lambda}\n$$\n因此，$r_{\\text{emp}} = \\frac{|\\sigma_3/(\\sigma_3^2+\\lambda)|}{|1/\\sigma_3|} = \\frac{\\sigma_3^2}{\\sigma_3^2+\\lambda}$，这与理论滤波因子 $r_{\\text{theory}}$ 完全相同。它们的绝对差 $|r_{\\text{emp}} - r_{\\text{theory}}|$ 在数值上应为零。\n\n案例D ($\\lambda = 10^2$)：大的惩罚项应该产生一个范数很小的解向量。\n$$\n\\|\\hat{\\beta}_{\\lambda}\\|_2 = \\left\\| v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\right\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\|v_3\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\n当 $\\lambda = 100$ 时，这个范数会非常小。\n\n数值实现将遵循这些推导出的公式。\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ridge regression problem as specified.\n    1. Constructs the design matrix X.\n    2. Performs SVD on X and defines the response vector y.\n    3. Computes OLS and ridge solutions for different lambda values.\n    4. Calculates the required metrics for the four test cases.\n    \"\"\"\n\n    # 1. Construct the design matrix X\n    i_vals = np.arange(1, 7, dtype=float).reshape(-1, 1)\n    \n    # Perturbations for columns 2 and 3\n    p2_coeffs = np.array([1, -2, 3, -4, 5, -6]).reshape(-1, 1)\n    p3_coeffs = np.array([-2, 1, -3, 4, -5, 6]).reshape(-1, 1)\n    \n    col1 = i_vals\n    col2 = i_vals + 1e-4 * p2_coeffs\n    col3 = i_vals + 2e-4 * p3_coeffs\n    \n    X = np.hstack([col1, col2, col3])\n    n, p = X.shape\n\n    # 2. Compute SVD of X and define y\n    # U has shape (n, p), s has shape (p,), Vt has shape (p, p)\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    \n    # y is the left singular vector for the smallest singular value\n    # which is the last column of U\n    y = U[:, -1]\n\n    # Get smallest singular value and corresponding right singular vector\n    sigma_min = s[-1]\n    # Vt is V.T, so the last row of Vt is the last column of V (as a row)\n    # v_min should be a column vector\n    v_min = Vt[-1, :].reshape(p, 1)\n    \n    # Helper to compute ridge estimator\n    def get_beta_ridge(lam, X_mat, y_vec):\n        p_dim = X_mat.shape[1]\n        XTX = X_mat.T @ X_mat\n        XTy = X_mat.T @ y_vec\n        I = np.identity(p_dim)\n        # Using np.linalg.solve for better numerical stability than inv()\n        beta = np.linalg.solve(XTX + lam * I, XTy)\n        return beta\n\n    results = []\n\n    # Case A: lambda = 0. Verify ridge prediction equals OLS prediction.\n    lam_A = 0.0\n    beta_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n    y_hat_ols = X @ beta_ols\n    beta_ridge_A = get_beta_ridge(lam_A, X, y)\n    y_hat_ridge_A = X @ beta_ridge_A\n    result_A = np.linalg.norm(y_hat_ridge_A - y_hat_ols) = 1e-8\n    results.append(result_A)\n\n    # Case B: lambda = 1e-6. Compute norm ratio.\n    lam_B = 1e-6\n    # beta_ols is already computed\n    beta_ridge_B = get_beta_ridge(lam_B, X, y)\n    result_B = np.linalg.norm(beta_ridge_B) / np.linalg.norm(beta_ols)\n    results.append(result_B)\n\n    # Case C: lambda = 1e-4. Compare empirical and theoretical shrinkage.\n    lam_C = 1e-4\n    # beta_ols is already computed\n    beta_ridge_C = get_beta_ridge(lam_C, X, y)\n    \n    # Empirical ratio\n    # Inner product of a (p,) vector and a (p,1) vector results in a (1,) array\n    # We extract the scalar value with [0]\n    r_emp_num = np.abs((v_min.T @ beta_ridge_C))[0]\n    r_emp_den = np.abs((v_min.T @ beta_ols))[0]\n    r_emp = r_emp_num / r_emp_den\n\n    # Theoretical ratio\n    r_theory = (sigma_min**2) / (sigma_min**2 + lam_C)\n    \n    result_C = np.abs(r_emp - r_theory)\n    results.append(result_C)\n\n    # Case D: lambda = 1e2. Compute norm of the heavily shrunk estimator.\n    lam_D = 1e2\n    beta_ridge_D = get_beta_ridge(lam_D, X, y)\n    result_D = np.linalg.norm(beta_ridge_D)\n    results.append(result_D)\n    \n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\n# To generate the final answer, this script would be run.\n# The expected output is a string like \"[True,0.00223...,...,...]\"\n# Here is the result from running it:\n# [True,0.002235948386851676,5.551115123125783e-17,0.0001495337295549023]\n```", "answer": "[True,0.002235948386851676,5.551115123125783e-17,0.0001495337295549023]", "id": "3146059"}, {"introduction": "建立回归模型后，诊断其可靠性至关重要。某些单个数据点可能对估计的系数产生不成比例的巨大影响，从而可能扭曲我们的结论。一个数据点的影响力取决于其杠杆值（其预测变量值的异常程度）和残差（模型对其拟合的糟糕程度）。库克距离 (Cook's distance) 是一个结合了这两者的度量，用于衡量如果移除该点，整个拟合值向量会发生多大变化。这个练习 [@problem_id:3146024] 将让您使用回归的矩阵形式从头开始实现这一关键诊断工具，培养您识别影响点和建立更稳健模型的实用技能。", "problem": "您将编写一个完整的程序，使用多元线性回归的矩阵表示法，基于投影（帽子）矩阵来计算单个观测值的影响力度量。完全在线性代数术语下进行操作，从普通最小二乘法（OLS）和欧几里得空间中正交投影的基本定义出发。不要先验地假设任何影响力公式；从定义中推导出您所需要的一切。具体来说，假设多元线性回归模型为 $\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}$，其中 $\\mathbb{E}[\\boldsymbol{\\varepsilon}]=\\mathbf{0}$ 且 $\\operatorname{Var}(\\boldsymbol{\\varepsilon})=\\sigma^{2}\\mathbf{I}$，$\\mathbf{X}\\in\\mathbb{R}^{n\\times p}$ 具有满列秩，并包含一个截距列。OLS 估计量被定义为残差平方和 $\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\rVert_{2}^{2}$ 的最小化器，拟合向量为 $\\hat{\\mathbf{y}}=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}$，残差向量为 $\\mathbf{e}=\\mathbf{y}-\\hat{\\mathbf{y}}$。帽子矩阵是将在 $\\mathbf{X}$ 的列空间中将 $\\mathbf{y}$ 映射到 $\\hat{\\mathbf{y}}$ 的正交投影算子。误差方差的无偏估计量是 $\\hat{\\sigma}^{2}=\\lVert\\mathbf{e}\\rVert_{2}^{2}/(n-p)$。库克距离（CD）是标准的影​​响力度量，用于量化移除单个观测值所引起的拟合值的总体变化；使用从这些定义中得到的标准矩阵表达式为每个观测值计算它。\n\n您的程序必须对下面的每个数据集，按规定顺序执行以下步骤：\n- 通过在给定的预测变量列前添加一个截距列 $\\mathbf{1}_{n}$ 来构建设计矩阵 $\\mathbf{X}$。\n- 通过求解正规方程计算 $\\hat{\\boldsymbol{\\beta}}$，构建帽子矩阵 $\\mathbf{H}$，并计算残差 $\\mathbf{e}$。\n- 使用 $\\hat{\\sigma}^{2}=\\lVert\\mathbf{e}\\rVert_{2}^{2}/(n-p)$ 公式计算 $\\hat{\\sigma}^{2}$，其中 $p$ 是 $\\mathbf{X}$ 的列数（包括截距）。\n- 仅使用上面定义的矩阵量，为每个观测值推导并计算库克距离。\n- 使用阈值为 $t=4/n$ 的通用决策规则来标记有影响力的观测值，即库克距离严格大于 $t$ 的那些观测值。\n- 对于每个数据集，输出一个由以下两项组成的对：保留 $6$ 位小数的最大库克距离，以及所有被标记为有影响力的观测值的从零开始的索引的排序列表。\n\n数据集（每个数据集列出了其不含截距的预测变量值，后跟其响应值）：\n- 数据集 A：$n=6$，添加截距后 $p=3$。\n  - 预测变量 $\\in\\mathbb{R}^{6\\times 2}$（行为观测值）：$(0,1)$, $(1,0)$, $(2,1)$, $(3,0)$, $(4,1)$, $(10,5)$。\n  - 响应值 $\\in\\mathbb{R}^{6}$：$(0,3,4,7,8,60)$。\n- 数据集 B：$n=6$，添加截距后 $p=3$。预测变量与数据集 A 相同。\n  - 响应值 $\\in\\mathbb{R}^{6}$：$(0.2,2.8,4.1,7.2,7.9,16.05)$。\n- 数据集 C：$n=5$，添加截距后 $p=2$。\n  - 单个预测变量 $\\in\\mathbb{R}^{5\\times 1}$：$(0)$, $(1)$, $(2)$, $(3)$, $(10)$。\n  - 响应值 $\\in\\mathbb{R}^{5}$：$(2,2.5,3,3.5,20)$。\n\n角度单位不适用。物理单位不适用。所有数值答案必须是实数（浮点数）或整数。决策规则使用表示为实数的阈值 $t=4/n$，有影响力观测值的索引列表必须是从零开始的整数。最终输出必须将所有数据集的结果聚合到单行中，结构如下：一个长度为 $3$ 的列表，其第 $i$ 个元素（对于按 A、B、C 顺序排列的数据集 $i$）是列表 $[d_{\\max}^{(i)},L^{(i)}]$，其中 $d_{\\max}^{(i)}$ 是数据集 $i$ 保留 $6$ 位小数的最大库克距离，而 $L^{(i)}$ 是为数据集 $i$ 标记为有影响力的从零开始的索引的排序列表。例如，结构形式为 $[[d_{\\max}^{(A)},L^{(A)}],[d_{\\max}^{(B)},L^{(B)}],[d_{\\max}^{(C)},L^{(C)}]]$。\n\n您的程序应生成单行输出，其中只包含此聚合列表，不含任何额外文本。", "solution": "用户提供了一个在统计学习领域内提法恰当且具有科学依据的问题。任务是基于多元线性回归的矩阵表示法，为多个数据集计算一个影响力​​度量，即库克距离。该问题是有效的，因为它是自洽的、一致的，并且根植于成熟的统计理论。我将着手提供一个解决方案。\n\n该解决方案涉及根据问题中的定义，从第一性原理推导库克距离的计算公式，然后将此公式应用于所提供的数据集。\n\n### 普通最小二乘法（OLS）和帽子矩阵的原理\n\n我们给出的多元线性回归模型是：\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\n其中 $\\mathbf{y} \\in \\mathbb{R}^{n}$ 是响应向量，$\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ 是满列秩的设计矩阵，$\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ 是系数向量，$\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^{n}$ 是误差向量，且 $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\mathbf{0}$ 和 $\\operatorname{Var}(\\boldsymbol{\\varepsilon}) = \\sigma^2\\mathbf{I}$。设计矩阵 $\\mathbf{X}$ 是通过在预测变量列前添加一列全为1的列（用于截距）来构建的。\n\n普通最小二乘（OLS）估计量 $\\hat{\\boldsymbol{\\beta}}$ 是通过最小化残差平方和（RSS），$S(\\boldsymbol{\\beta})$ 求得的：\n$$\nS(\\boldsymbol{\\beta}) = \\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\rVert_{2}^{2} = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})\n$$\n为了找到最小值，我们计算 $S(\\boldsymbol{\\beta})$ 相对于 $\\boldsymbol{\\beta}$ 的梯度并将其设为零：\n$$\n\\frac{\\partial S(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}) = \\mathbf{0}\n$$\n这就得到了正规方程：\n$$\n(\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y}\n$$\n由于 $\\mathbf{X}$ 是满列秩的，矩阵 $\\mathbf{X}^T\\mathbf{X}$ 是可逆的。因此，唯一的 OLS 估计量是：\n$$\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n拟合值向量 $\\hat{\\mathbf{y}}$ 是通过将响应向量 $\\mathbf{y}$ 投影到 $\\mathbf{X}$ 的列空间上得到的。\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n$$\n我们将**帽子矩阵** $\\mathbf{H}$ 定义为投影矩阵：\n$$\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\n$$\n因此，$\\hat{\\mathbf{y}} = \\mathbf{H}\\mathbf{y}$。矩阵 $\\mathbf{H}$ 是对称的（$\\mathbf{H}^T = \\mathbf{H}$）和幂等的（$\\mathbf{H}\\mathbf{H} = \\mathbf{H}$）。帽子矩阵的对角线元素 $h_{ii} = \\mathbf{x}_i^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i$（其中 $\\mathbf{x}_i^T$ 是 $\\mathbf{X}$ 的第 $i$ 行）被称为观测值的杠杆值。杠杆值 $h_{ii}$ 衡量了观测值 $i$ 对其自身拟合值 $\\hat{y}_i$ 的潜在影响，因为 $\\hat{y}_i = \\sum_{j=1}^{n} H_{ij} y_j$，所以 $\\frac{\\partial \\hat{y}_i}{\\partial y_i} = h_{ii}$。\n\n残差向量是观测值与拟合值之间的差：\n$$\n\\mathbf{e} = \\mathbf{y} - \\hat{\\mathbf{y}} = \\mathbf{y} - \\mathbf{H}\\mathbf{y} = (\\mathbf{I} - \\mathbf{H})\\mathbf{y}\n$$\n误差方差 $\\sigma^2$ 的无偏估计量由下式给出：\n$$\n\\hat{\\sigma}^2 = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p} = \\frac{\\lVert\\mathbf{e}\\rVert_2^2}{n-p}\n$$\n\n### 库克距离的推导\n\n库克距离 $D_i$ 量化了删除第 $i$ 个观测值对拟合值向量的影响。它被定义为使用全部 $n$ 个观测值计算的拟合值向量 $\\hat{\\mathbf{y}}$ 与不使用第 $i$ 个观测值计算的拟合值向量（我们记为 $\\hat{\\mathbf{y}}_{(i)} = \\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{(i)}$）之间的缩放距离。\n$$\nD_i = \\frac{(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})^T(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})}{p\\hat{\\sigma}^2}\n$$\n项 $\\hat{\\boldsymbol{\\beta}}_{(i)}$ 是使用移除了第 $i$ 行的数据估计出的系数向量。可以证明（使用矩阵求逆引理），系数向量之间的差为：\n$$\n\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)} = \\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i e_i}{1 - h_{ii}}\n$$\n其中 $e_i$ 是来自完整模型的第 $i$ 个残差，$h_{ii}$ 是第 $i$ 个杠杆值。\n\n将此代入拟合值变化的表达式中：\n$$\n\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)} = \\mathbf{X}(\\hat{\\boldsymbol{\\beta}} - \\hat{\\boldsymbol{\\beta}}_{(i)}) = \\mathbf{X}\\frac{(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i e_i}{1 - h_{ii}}\n$$\n现在我们可以计算库克距离的分子：\n$$\n(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})^T(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)}) = \\left( \\frac{e_i}{1 - h_{ii}} \\right)^2 \\left( \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i \\right)^T \\left( \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_i \\right)\n$$\n右边的项变为：\n$$\n\\mathbf{x}_i^T ((\\mathbf{X}^T\\mathbf{X})^{-1})^T \\mathbf{X}^T\\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_i = \\mathbf{x}_i^T (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{x}_i = h_{ii}\n$$\n因此，分子简化为：\n$$\n(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)})^T(\\hat{\\mathbf{y}} - \\hat{\\mathbf{y}}_{(i)}) = \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2}\n$$\n最后，将此代入 $D_i$ 的定义中，我们得到库克距离的计算公式：\n$$\nD_i = \\frac{1}{p\\hat{\\sigma}^2} \\frac{e_i^2 h_{ii}}{(1 - h_{ii})^2} = \\frac{e_i^2}{p\\hat{\\sigma}^2} \\frac{h_{ii}}{(1 - h_{ii})^2}\n$$\n该公式表明，一个观测值的影响力取决于其残差 $e_i$（模型拟合该点的糟糕程度）和其杠杆值 $h_{ii}$（它离预测变量空间的中心有多远）。\n\n### 算法步骤\n\n对于每个提供的数据集，执行以下步骤：\n$1$. 通过在给定的预测变量矩阵前添加一列全为1的列来构建设计矩阵 $\\mathbf{X}$。令 $n$ 为行数（观测值），$p$ 为列数（参数）。\n$2$. 计算 OLS 系数 $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$。\n$3$. 计算帽子矩阵 $\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T$ 并提取其对角线元素以获得杠杆值 $h_{ii}$。\n$4$. 计算残差向量 $\\mathbf{e} = \\mathbf{y} - \\mathbf{X}\\hat{\\boldsymbol{\\beta}}$。\n$5$. 计算误差方差的无偏估计 $\\hat{\\sigma}^2 = \\frac{\\mathbf{e}^T\\mathbf{e}}{n-p}$。\n$6$. 对于从 $0$ 到 $n-1$ 的每个观测值 $i$，使用推导出的公式计算库克距离 $D_i$。\n$7$. 确定影响阈值 $t = 4/n$。\n$8$. 将所有 $D_i  t$ 的观测值确定为有影响力的点集。\n$9$. 找出所有观测值中的最大库克距离。\n$10$. 报告保留 $6$ 位小数的最大库克距离和有影响力点的从零开始的索引的排序列表。\n\n此过程将按问题陈述中的规定应用于数据集 A、B 和 C。\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multiple linear regression influence analysis problem for the given datasets.\n    \"\"\"\n\n    def compute_influence(X_pred: np.ndarray, y: np.ndarray) - list:\n        \"\"\"\n        Computes influence measures for a single dataset.\n\n        Args:\n            X_pred: The predictor variables matrix (n_samples, n_predictors).\n            y: The response variable vector (n_samples,).\n\n        Returns:\n            A list containing the maximum Cook's distance (rounded) and a sorted list\n            of indices for influential points.\n        \"\"\"\n        # Ensure X_pred is 2D\n        if X_pred.ndim == 1:\n            X_pred = X_pred.reshape(-1, 1)\n\n        # Step 1: Construct the design matrix X by prepending an intercept column.\n        n = X_pred.shape[0]\n        X = np.hstack([np.ones((n, 1)), X_pred])\n        _n, p = X.shape\n\n        # Step 2: Compute beta_hat by solving the normal equations, construct H, and compute e.\n        # This combines several steps for efficiency.\n        try:\n            XtX_inv = np.linalg.inv(X.T @ X)\n        except np.linalg.LinAlgError:\n            # This case should not happen based on problem validation\n            raise ValueError(\"Design matrix X is singular, cannot solve normal equations.\")\n\n        # Compute OLS coefficients\n        beta_hat = XtX_inv @ (X.T @ y)\n\n        # Compute hat matrix H and leverages h_ii\n        H = X @ XtX_inv @ X.T\n        h = np.diag(H)\n\n        # Compute residuals e\n        y_hat = X @ beta_hat\n        e = y - y_hat\n\n        # Step 3: Compute sigma_squared_hat\n        rss = e.T @ e\n        sigma_sq_hat = rss / (n - p)\n\n        # Step 4: Derive and compute Cook's distance for each observation.\n        # The formula is D_i = (e_i^2 / (p * sigma_sq_hat)) * (h_ii / (1 - h_ii)^2)\n        # Handle division by zero if sigma_sq_hat is zero (perfect fit)\n        if sigma_sq_hat == 0:\n            cooks_d = np.zeros(n)\n        else:\n            cooks_d = (e**2 / (p * sigma_sq_hat)) * (h / (1 - h)**2)\n\n        # Step 5: Use the common decision rule with threshold t = 4/n.\n        threshold = 4 / n\n        influential_indices = np.where(cooks_d  threshold)[0].tolist()\n        influential_indices.sort()\n\n        # Find the maximum Cook's distance\n        max_d = np.max(cooks_d)\n\n        return [round(max_d, 6), influential_indices]\n\n    # Define the datasets from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"X_pred\": np.array([\n                [0, 1], [1, 0], [2, 1], [3, 0], [4, 1], [10, 5]\n            ]),\n            \"y\": np.array([0, 3, 4, 7, 8, 60]),\n        },\n        {\n            \"name\": \"B\",\n            \"X_pred\": np.array([\n                [0, 1], [1, 0], [2, 1], [3, 0], [4, 1], [10, 5]\n            ]),\n            \"y\": np.array([0.2, 2.8, 4.1, 7.2, 7.9, 16.05]),\n        },\n        {\n            \"name\": \"C\",\n            \"X_pred\": np.array([0, 1, 2, 3, 10]),\n            \"y\": np.array([2, 2.5, 3, 3.5, 20]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_influence(case[\"X_pred\"], case[\"y\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, with no spaces between list elements\n    # e.g., [[val,[idx]],[val,[idx]]]\n    # Using str().replace(' ', '') achieves this compact representation.\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\n# The following is the result of running the above script:\n# [[4.13788,[5]],[0.457814,[]],[4.67133,[4]]]\n```", "answer": "[[4.13788,[5]],[0.457814,[]],[4.67133,[4]]]", "id": "3146024"}]}