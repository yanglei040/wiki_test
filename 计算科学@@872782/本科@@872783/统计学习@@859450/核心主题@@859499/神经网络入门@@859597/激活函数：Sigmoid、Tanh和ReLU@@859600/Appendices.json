{"hands_on_practices": [{"introduction": "为了理解激活函数如何影响学习过程，我们可以从一个简化的模型入手。这个练习通过分析一个单参数回归问题的梯度流动态，来探索不同激活函数（$S$型、$\\tanh$ 和 ReLU）下的损失函数曲面。通过寻找并分析其临界点的稳定性，我们可以直观地理解为何梯度下降能够引导模型参数收敛到理想值。[@problem_id:3094674]", "problem": "考虑一个统计学习中的标量输入回归场景，其中输入 $x \\in \\{-1, +1\\}$ 以等概率抽取，并采用教师-学生范式。教师模型以一个固定的参数 $\\theta > 0$ 输出 $y = \\phi(\\theta x)$，而学生模型为 $f_{w}(x) = \\phi(w x)$，其中 $w \\in \\mathbb{R}$ 是可训练参数。激活函数 $\\phi$ 是以下之一：\n- Logistic Sigmoid函数 $\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$，\n- 双曲正切函数 $\\tanh(z) = \\frac{\\exp(z) - \\exp(-z)}{\\exp(z) + \\exp(-z)}$，\n- 整流线性单元 $\\mathrm{ReLU}(z) = \\max(0, z)$。\n\n学习目标是期望平方误差损失，损失函数采用标准形式 $\\ell(y, f) = \\frac{1}{2}(y - f)^{2}$，因此期望风险为\n$$\nL(w) = \\mathbb{E}\\!\\left[\\ell\\big(y, f_{w}(x)\\big)\\right] = \\mathbb{E}\\!\\left[\\frac{1}{2}\\big(\\phi(\\theta x) - \\phi(w x)\\big)^{2}\\right],\n$$\n其中期望是关于 $x$ 的分布计算的。\n\n训练遵循由常微分方程（ODE）给出的梯度流动力学\n$$\n\\dot{w}(t) = - \\frac{\\mathrm{d}}{\\mathrm{d}w} L(w(t)).\n$$\n\n从上述基本定义出发，不使用任何快捷公式，推导每种激活函数 $\\phi \\in \\{\\sigma, \\tanh, \\mathrm{ReLU}\\}$ 的梯度流ODE的临界点（平衡点），并利用对平衡点周围局部行为的原则性分析（例如，在适用时通过 $L$ 的二阶导数，或在不可微情况下通过适当的次梯度论证）来分类其稳定性。最后，以 $(\\sigma, \\tanh, \\mathrm{ReLU})$ 的顺序，用关于 $\\theta$ 的闭式解析表达式给出稳定平衡值 $w^{\\star}$ 的有序三元组。请使用 $\\mathrm{pmatrix}$ 环境将你的最终答案表示为单行矩阵。无需四舍五入。", "solution": "用户提供的问题经验证是提法得当、科学上合理且内部一致的。它提出了一个统计学习中的标准理论练习，要求将微积分和稳定性分析应用于一个明确定义的动力系统。所有必要的定义和条件均已提供。\n\n分析过程如下：推导期望风险函数，为三种指定的激活函数分别找出其临界点，然后对这些点的稳定性进行分类，以确定稳定平衡点。\n\n期望风险由下式给出\n$$\nL(w) = \\mathbb{E}\\!\\left[\\frac{1}{2}\\big(\\phi(\\theta x) - \\phi(w x)\\big)^{2}\\right]\n$$\n期望是关于输入分布 $P(x=+1) = P(x=-1) = \\frac{1}{2}$ 计算的。展开期望可得：\n$$\nL(w) = \\frac{1}{2} P(x=+1) \\left(\\phi(\\theta \\cdot 1) - \\phi(w \\cdot 1)\\right)^2 + \\frac{1}{2} P(x=-1) \\left(\\phi(\\theta \\cdot (-1)) - \\phi(w \\cdot (-1))\\right)^2\n$$\n代入概率，我们得到\n$$\nL(w) = \\frac{1}{4} \\left[ \\left(\\phi(\\theta) - \\phi(w)\\right)^2 + \\left(\\phi(-\\theta) - \\phi(-w)\\right)^2 \\right]\n$$\n梯度流 $\\dot{w}(t) = - \\frac{\\mathrm{d}}{\\mathrm{d}w} L(w(t))$ 的平衡点是势函数 $L(w)$ 的临界点，即满足 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 的 $w$ 值。一个稳定平衡点对应于 $L(w)$ 的一个局部最小值，这可以通过分析二阶导数 $\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2}$ 或通过考察 $L(w)$ 的局部行为来确定。\n\n我们分别分析每一种激活函数 $\\phi$。\n\n**1. Logistic Sigmoid函数: $\\phi(z) = \\sigma(z) = \\frac{1}{1 + \\exp(-z)}$**\n\nSigmoid函数具有性质 $\\sigma(-z) = 1 - \\sigma(z)$。将其代入 $L(w)$ 的表达式中：\n$$\nL(w) = \\frac{1}{4} \\left[ (\\sigma(\\theta) - \\sigma(w))^2 + (1 - \\sigma(\\theta) - (1 - \\sigma(w)))^2 \\right]\n$$\n$$\nL(w) = \\frac{1}{4} \\left[ (\\sigma(\\theta) - \\sigma(w))^2 + (\\sigma(w) - \\sigma(\\theta))^2 \\right] = \\frac{1}{2} (\\sigma(\\theta) - \\sigma(w))^2\n$$\n为了找到临界点，我们计算 $L(w)$ 关于 $w$ 的导数。Sigmoid函数的导数是 $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$。\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{2} \\cdot 2 (\\sigma(\\theta) - \\sigma(w)) \\cdot (-\\sigma'(w)) = - (\\sigma(\\theta) - \\sigma(w)) \\sigma'(w)\n$$\n临界点出现在 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 的地方。这意味着 $\\sigma(\\theta) - \\sigma(w) = 0$ 或 $\\sigma'(w) = 0$。\n对于任何有限的 $w \\in \\mathbb{R}$，$\\sigma'(w) = \\sigma(w)(1-\\sigma(w))$ 项是严格为正的。因此，我们必须有 $\\sigma(\\theta) - \\sigma(w) = 0$。\n由于 $\\sigma(z)$ 是一个严格单调递增的函数，$\\sigma(w) = \\sigma(\\theta)$ 意味着 $w = \\theta$。唯一的临界点是 $w = \\theta$。\n\n为了分类其稳定性，我们考察二阶导数：\n$$\n\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2} = \\frac{\\mathrm{d}}{\\mathrm{d}w} \\left[ - (\\sigma(\\theta) - \\sigma(w)) \\sigma'(w) \\right] = \\sigma'(w)\\sigma'(w) - (\\sigma(\\theta) - \\sigma(w))\\sigma''(w) = (\\sigma'(w))^2 - (\\sigma(\\theta) - \\sigma(w))\\sigma''(w)\n$$\n在临界点 $w = \\theta$ 处：\n$$\n\\left.\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2}\\right|_{w=\\theta} = (\\sigma'(\\theta))^2 - (\\sigma(\\theta) - \\sigma(\\theta))\\sigma''(\\theta) = (\\sigma'(\\theta))^2\n$$\n由于 $\\theta > 0$，所以 $\\sigma'(\\theta) > 0$，因此 $(\\sigma'(\\theta))^2 > 0$。正的二阶导数表明 $L(w)$ 在 $w=\\theta$ 处有一个局部最小值。因此，$w^\\star = \\theta$ 是一个稳定平衡点。\n\n**2. 双曲正切函数: $\\phi(z) = \\tanh(z)$**\n\n双曲正切函数是一个奇函数，即 $\\tanh(-z) = -\\tanh(z)$。将其代入 $L(w)$ 的表达式中：\n$$\nL(w) = \\frac{1}{4} \\left[ (\\tanh(\\theta) - \\tanh(w))^2 + (-\\tanh(\\theta) - (-\\tanh(w)))^2 \\right]\n$$\n$$\nL(w) = \\frac{1}{4} \\left[ (\\tanh(\\theta) - \\tanh(w))^2 + (\\tanh(w) - \\tanh(\\theta))^2 \\right] = \\frac{1}{2} (\\tanh(\\theta) - \\tanh(w))^2\n$$\n$L(w)$ 的函数形式与Sigmoid情况相同。我们计算导数，注意 $\\tanh'(z) = 1 - \\tanh^2(z) = \\mathrm{sech}^2(z)$。\n$$\n\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{2} \\cdot 2 (\\tanh(\\theta) - \\tanh(w)) \\cdot (-\\tanh'(w)) = - (\\tanh(\\theta) - \\tanh(w)) \\tanh'(w)\n$$\n临界点出现在 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 的地方。导数 $\\tanh'(w) = \\mathrm{sech}^2(w)$ 对于任何有限的 $w \\in \\mathbb{R}$ 都是严格为正的。因此，我们必须有 $\\tanh(\\theta) - \\tanh(w) = 0$。\n由于 $\\tanh(z)$ 是严格单调递增的，$\\tanh(w) = \\tanh(\\theta)$ 意味着 $w = \\theta$。唯一的临界点是 $w = \\theta$。\n\n为了判断稳定性，我们在 $w=\\theta$ 处计算二阶导数：\n$$\n\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2} = (\\tanh'(w))^2 - (\\tanh(\\theta) - \\tanh(w))\\tanh''(w)\n$$\n$$\n\\left.\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2}\\right|_{w=\\theta} = (\\tanh'(\\theta))^2 - (\\tanh(\\theta) - \\tanh(\\theta))\\tanh''(\\theta) = (\\tanh'(\\theta))^2\n$$\n由于 $\\theta > 0$，所以 $\\tanh'(\\theta) > 0$，因此 $(\\tanh'(\\theta))^2 > 0$。这表明在 $w=\\theta$ 处有一个局部最小值。因此，$w^\\star = \\theta$ 是一个稳定平衡点。\n\n**3. 整流线性单元: $\\phi(z) = \\mathrm{ReLU}(z) = \\max(0, z)$**\n\n给定 $\\theta > 0$，我们有 $\\mathrm{ReLU}(\\theta) = \\theta$ 和 $\\mathrm{ReLU}(-\\theta) = 0$。风险函数变为：\n$$\nL(w) = \\frac{1}{4} \\left[ (\\theta - \\mathrm{ReLU}(w))^2 + (0 - \\mathrm{ReLU}(-w))^2 \\right] = \\frac{1}{4} \\left[ (\\theta - \\mathrm{ReLU}(w))^2 + (\\mathrm{ReLU}(-w))^2 \\right]\n$$\nReLU函数是分段线性的，所以我们分区域分析 $L(w)$。\n\n情况 (i)：$w > 0$。此时，$\\mathrm{ReLU}(w) = w$ 且 $\\mathrm{ReLU}(-w) = 0$。\n$$\nL(w) = \\frac{1}{4} \\left[ (\\theta - w)^2 + 0^2 \\right] = \\frac{1}{4}(\\theta - w)^2\n$$\n导数为 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{4} \\cdot 2(\\theta - w) \\cdot (-1) = -\\frac{1}{2}(\\theta - w)$。\n令 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = 0$ 得到 $w = \\theta$。由于 $\\theta > 0$，这个临界点位于区域 $w>0$ 内。二阶导数为 $\\frac{\\mathrm{d}^2L}{\\mathrm{d}w^2} = \\frac{1}{2} > 0$，所以 $w = \\theta$ 是一个局部最小值，因此是一个稳定平衡点。\n\n情况 (ii)：$w  0$。此时，$\\mathrm{ReLU}(w) = 0$ 且 $\\mathrm{ReLU}(-w) = -w$。\n$$\nL(w) = \\frac{1}{4} \\left[ (\\theta - 0)^2 + (-w)^2 \\right] = \\frac{1}{4}(\\theta^2 + w^2)\n$$\n导数为 $\\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\frac{1}{4}(2w) = \\frac{w}{2}$。对于 $w  0$，$\\frac{\\mathrm{d}L}{\\mathrm{d}w}  0$，所以在这个区域内没有临界点。梯度流为 $\\dot{w} = -L'(w) = -w/2  0$，意味着状态 $w$ 会向 $0$ 移动。\n\n情况 (iii)：$w=0$。函数 $L(w)$ 在 $w=0$ 处不可微。我们分析其次梯度。如果 $0 \\in \\partial L(w^\\star)$，则点 $w^\\star$ 是一个临界点，其中 $\\partial L$ 是次微分。\n在不可微点的次微分是左导数和右导数之间的区间。\n右导数：$\\lim_{w\\to 0^+} \\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\lim_{w\\to 0^+} \\left[-\\frac{1}{2}(\\theta - w)\\right] = -\\frac{\\theta}{2}$。\n左导数：$\\lim_{w\\to 0^-} \\frac{\\mathrm{d}L}{\\mathrm{d}w} = \\lim_{w\\to 0^-} \\frac{w}{2} = 0$。\n在 $w=0$ 处的次微分是 $\\partial L(0) = [-\\frac{\\theta}{2}, 0]$。由于 $0 \\in [-\\frac{\\theta}{2}, 0]$ (因为 $\\theta0$)，$w=0$ 是一个临界点。\n\n为了评估 $w=0$ 的稳定性，我们考察其邻域内的势函数 $L(w)$。\n$L(0) = \\frac{1}{4}(\\theta^2)$。\n对于一个小的 $w  0$，$L(w) = \\frac{1}{4}(\\theta-w)^2 = \\frac{1}{4}(\\theta^2 - 2\\theta w + w^2)$。\n差值为 $L(w) - L(0) = \\frac{1}{4}(-2\\theta w + w^2) = \\frac{w}{4}(w-2\\theta)$。\n对于 $w \\in (0, 2\\theta)$，这个差值为负，意味着 $L(w)  L(0)$。如果一个点附近存在势能更低的点，系统可以演化到那些点，那么这个点就不可能是稳定平衡点。由于对于任何小的扰动 $\\epsilon0$，$L(\\epsilon)  L(0)$，系统将离开 $w=0$（具体来说，是朝向 $w=\\theta$）。因此，$w=0$ 是一个不稳定平衡点。\n\n对于ReLU激活函数，唯一的稳定平衡点是 $w^\\star = \\theta$。\n\n**结果总结**\n- 对于 $\\phi = \\sigma(z)$，稳定平衡点是 $w^\\star = \\theta$。\n- 对于 $\\phi = \\tanh(z)$，稳定平衡点是 $w^\\star = \\theta$。\n- 对于 $\\phi = \\mathrm{ReLU}(z)$，稳定平衡点是 $w^\\star = \\theta$。\n\n按 $(\\sigma, \\tanh, \\mathrm{ReLU})$ 顺序排列的稳定平衡值 $w^{\\star}$ 的有序三元组是 $(\\theta, \\theta, \\theta)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\theta  \\theta  \\theta\n\\end{pmatrix}\n}\n$$", "id": "3094674"}, {"introduction": "当我们将神经元堆叠成深度网络时，一个核心挑战是如何确保信号在层间有效传播，避免梯度消失或爆炸。这个问题引入了“混沌边缘”(edge-of-chaos)理论，要求你推导平均场灵敏度 $\\chi$ 并计算出维持信号稳定的临界权重初始化方差 $\\sigma_w^2$。这个练习揭示了为何像 Glorot/Xavier 和 He 这样的初始化方案是成功训练深度网络的关键。[@problem_id:3094645]", "problem": "考虑一个深度为 $L$ 的全连接网络，每层宽度为 $n$，偏置为零，且对于层索引 $\\ell \\in \\{1, \\dots, L\\}$，其权重 $W_{ij}^{\\ell} \\sim \\mathcal{N}(0, \\sigma_w^2/n)$ 是独立同分布的。设输入 $h^0$ 的坐标是独立同分布的，其均值为 $0$，方差有限。定义第 $\\ell$ 层的预激活为 $z^{\\ell} = W^{\\ell} h^{\\ell-1}$，激活为 $h^{\\ell} = \\phi(z^{\\ell})$，其中 $\\phi$ 是逻辑S型函数 $\\sigma(x) = \\frac{1}{1 + \\exp(-x)}$、双曲正切函数 $\\tanh(x)$ 或整流线性单元 (ReLU) $\\mathrm{ReLU}(x) = \\max\\{0, x\\}$ 中的一种。根据中心极限定理 (CLT)，在无限宽度极限 $n \\to \\infty$ 下，$z^{\\ell}$ 的每个坐标近似服从均值为 $0$、方差为 $q_{\\ell}$ 的高斯分布。\n\n关注输入的一个无穷小扰动 $\\delta h^0$ 的传播。定义平均场灵敏度 $\\chi$ 为在无限宽度极限下，扰动的期望平方范数从一层传播到下一层时变化的乘法因子。从以上定义出发，并使用第一性原理，用权重方差 $\\sigma_w^2$ 和在典型预激活处求值的激活导数的性质来表示 $\\chi$。然后，在小方差情况 $q_{\\ell} \\ll 1$（因此典型的预激活值接近 $0$）下，计算 $\\phi = \\sigma$、$\\phi = \\tanh$ 和 $\\phi = \\mathrm{ReLU}$ 时的 $\\chi$，并确定使信号传播处于临界稳定状态（即 $\\chi = 1$）的 $\\sigma_w^2$ 的临界初始化值。按 $\\phi = \\sigma, \\tanh, \\mathrm{ReLU}$ 的顺序提供这三个临界值。将你的最终答案表示为单行矩阵。无需四舍五入。", "solution": "用户希望找到三种不同激活函数的临界权重方差 $\\sigma_w^2$，使得深度神经网络中的信号传播是临界稳定的。这种状态以平均场灵敏度 $\\chi = 1$ 为特征，可以防止信号扰动在网络层中传播时发生指数级增长（爆炸）或衰减（消失）。\n\n### 步骤1：平均场灵敏度 $\\chi$ 的推导\n\n我们首先分析一个无穷小扰动从第 $\\ell-1$ 层到第 $\\ell$ 层的传播。第 $\\ell$ 层的预激活和激活由下式给出：\n$$z^{\\ell} = W^{\\ell} h^{\\ell-1}$$\n$$h^{\\ell} = \\phi(z^{\\ell})$$\n其中 $h^{\\ell-1}$ 是前一层的 $n$ 维激活向量，$W^{\\ell}$ 是第 $\\ell$ 层的 $n \\times n$ 权重矩阵。权重 $W_{ij}^{\\ell}$ 是从高斯分布 $\\mathcal{N}(0, \\sigma_w^2/n)$ 中抽取的独立同分布 (i.i.d.) 随机变量。\n\n第 $\\ell-1$ 层激活中的一个无穷小扰动 $\\delta h^{\\ell-1}$ 会在第 $\\ell$ 层的预激活中引起一个扰动 $\\delta z^{\\ell}$：\n$$\\delta z^{\\ell} = W^{\\ell} \\delta h^{\\ell-1}$$\n这接着会在第 $\\ell$ 层的激活中引起一个扰动 $\\delta h^{\\ell}$。对于无穷小扰动，我们可以使用激活函数 $\\phi$ 的一阶泰勒展开：\n$$\\delta h^{\\ell} = \\phi(z^{\\ell} + \\delta z^{\\ell}) - \\phi(z^{\\ell}) \\approx \\phi'(z^{\\ell}) \\odot \\delta z^{\\ell}$$\n其中 $\\odot$ 表示逐元素（哈达玛）积。用坐标形式表示为 $(\\delta h^{\\ell})_i = \\phi'((z^{\\ell})_i) (\\delta z^{\\ell})_i$。\n\n平均场灵敏度 $\\chi$ 定义为在无限宽度极限（$n \\to \\infty$）下，连续层之间扰动的期望平方范数之比：\n$$\\chi = \\frac{\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2]}{\\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2]}$$\n期望 $\\mathbb{E}[\\cdot]$ 是对初始输入 $h^0$ 的分布和所有网络权重的分布求得的。\n\n让我们计算分子 $\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2]$：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = \\mathbb{E}\\left[\\sum_{i=1}^{n} ((\\delta h^{\\ell})_i)^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} (\\phi'((z^{\\ell})_i))^2 ((\\delta z^{\\ell})_i)^2\\right]$$\n根据期望的线性性，并注意到所有神经元索引 $i$ 在统计上是相同的：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = n \\, \\mathbb{E}\\left[(\\phi'((z^{\\ell})_i))^2 ((\\delta z^{\\ell})_i)^2\\right]$$\n在平均场（无限宽度）极限下，一个被称为“混沌”的关键现象出现了。预激活 $(z^{\\ell})_i = \\sum_j W_{ij}^{\\ell} h_{j}^{\\ell-1}$ 作为许多弱相关项的和，根据中心极限定理成为一个高斯随机变量。至关重要的是，它的分布与构成它的权重的具体实现无关。这使我们能够将对预激活项 $\\phi'((z^{\\ell})_i)$ 的期望与扰动项 $(\\delta z^{\\ell})_i$ 分开。设 $Z$ 是一个代表典型预激活的随机变量，且 $Z \\sim \\mathcal{N}(0, q_{\\ell})$。期望可以解耦为：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = n \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\, \\mathbb{E}[((\\delta z^{\\ell})_i)^2]$$\n现在我们计算预激活扰动平方的期望，即 $\\mathbb{E}[((\\delta z^{\\ell})_i)^2]$：\n$$\\mathbb{E}[((\\delta z^{\\ell})_i)^2] = \\mathbb{E}\\left[\\left(\\sum_{j=1}^{n} W_{ij}^{\\ell} (\\delta h^{\\ell-1})_j\\right)^2\\right] = \\mathbb{E}\\left[\\sum_{j,k=1}^{n} W_{ij}^{\\ell} W_{ik}^{\\ell} (\\delta h^{\\ell-1})_j (\\delta h^{\\ell-1})_k\\right]$$\n权重 $W^{\\ell}$ 与扰动 $\\delta h^{\\ell-1}$ 无关，因此我们可以分离期望。由于权重是独立同分布的，且 $\\mathbb{E}[W_{ij}^{\\ell} W_{ik}^{\\ell}] = \\delta_{jk} \\mathrm{Var}(W_{ij}^{\\ell}) = \\delta_{jk} (\\sigma_w^2/n)$：\n$$\\mathbb{E}[((\\delta z^{\\ell})_i)^2] = \\sum_{j,k=1}^{n} \\mathbb{E}[W_{ij}^{\\ell} W_{ik}^{\\ell}] \\, \\mathbb{E}[(\\delta h^{\\ell-1})_j (\\delta h^{\\ell-1})_k] = \\sum_{j=1}^{n} \\frac{\\sigma_w^2}{n} \\mathbb{E}[((\\delta h^{\\ell-1})_j)^2]$$\n假设神经元之间的扰动不相关（这是独立同分布随机结构的一个结果），并且由于所有神经元 $j$ 的统计相似性：\n$$\\mathbb{E}[((\\delta z^{\\ell})_i)^2] = \\frac{\\sigma_w^2}{n} \\sum_{j=1}^{n} \\mathbb{E}[((\\delta h^{\\ell-1})_j)^2] = \\frac{\\sigma_w^2}{n} \\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2]$$\n将此代入 $\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2]$ 的表达式中：\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = n \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\left( \\frac{\\sigma_w^2}{n} \\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2] \\right)$$\n$$\\mathbb{E}[\\|\\delta h^{\\ell}\\|^2] = \\sigma_w^2 \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\, \\mathbb{E}[\\|\\delta h^{\\ell-1}\\|^2]$$\n由此，我们确定灵敏度 $\\chi$ 为：\n$$\\chi = \\sigma_w^2 \\, \\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2]$$\n\n### 步骤2：计算每种激活函数的临界 $\\sigma_w^2$\n\n问题指定了小方差情况，$q_{\\ell} \\ll 1$。这意味着预激活 $z^{\\ell}$ 集中在0附近。我们必须找到导致 $\\chi=1$ 的 $\\sigma_w^2$ 的临界值。\n\n**1. 逻辑S型函数：** $\\phi(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}$\n对于像sigmoid这样的平滑函数，在极限 $q_{\\ell} \\to 0$ 时，高斯分布 $\\mathcal{N}(0, q_{\\ell})$ 趋近于0处的狄拉克δ函数。因此，期望值就变成了在 $z=0$ 处求值：\n$$\\mathbb{E}_{Z \\sim \\mathcal{N}(0, q_{\\ell})}[(\\phi'(Z))^2] \\approx (\\phi'(0))^2$$\n首先，我们求导数 $\\phi'(x)$：\n$$\\phi'(x) = \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) = \\frac{\\exp(-x)}{(1+\\exp(-x))^2}$$\n在 $x=0$ 处：\n$$\\phi'(0) = \\sigma(0)(1-\\sigma(0)) = \\frac{1}{1+1}\\left(1-\\frac{1}{1+1}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$$\n灵敏度为 $\\chi \\approx \\sigma_w^2 (\\phi'(0))^2 = \\sigma_w^2 (1/4)^2 = \\sigma_w^2/16$。\n为了达到临界稳定，$\\chi=1$：\n$$\\frac{\\sigma_w^2}{16} = 1 \\implies \\sigma_w^2 = 16$$\n\n**2. 双曲正切函数：** $\\phi(x) = \\tanh(x)$\n这也是一个平滑函数，所以我们使用与sigmoid函数相同的近似方法。\n其导数为 $\\phi'(x) = 1 - \\tanh^2(x)$。\n在 $x=0$ 处：\n$$\\phi'(0) = 1 - \\tanh^2(0) = 1 - 0^2 = 1$$\n灵敏度为 $\\chi \\approx \\sigma_w^2 (\\phi'(0))^2 = \\sigma_w^2 (1)^2 = \\sigma_w^2$。\n为了达到临界稳定，$\\chi=1$：\n$$\\sigma_w^2 = 1$$\n这是众所周知的 Glorot/Xavier 初始化条件。\n\n**3. 整流线性单元 (ReLU):** $\\phi(x) = \\mathrm{ReLU}(x) = \\max\\{0, x\\}$\nReLU函数在 $x=0$ 处不可微。其导数是亥维赛阶跃函数 $\\phi'(x) = H(x)$，当 $x0$ 时为1，当 $x0$ 时为0。我们不能在 $x=0$ 附近使用泰勒展开。相反，我们必须直接计算期望 $\\mathbb{E}[(\\phi'(Z))^2]$。\n设 $Z \\sim \\mathcal{N}(0, q_{\\ell})$。导数的平方为 $(\\phi'(Z))^2 = (H(Z))^2$。由于 $H(Z)$ 的取值在 $\\{0, 1\\}$ 中，我们有 $(H(Z))^2 = H(Z)$。\n$$\\mathbb{E}[(\\phi'(Z))^2] = \\mathbb{E}[H(Z)] = P(Z  0)$$\n由于高斯分布 $\\mathcal{N}(0, q_{\\ell})$ 关于其均值0对称，所以抽取值为正的概率恰好是 $1/2$。\n$$P(Z  0) = \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2\\pi q_{\\ell}}} \\exp\\left(-\\frac{z^2}{2q_{\\ell}}\\right) dz = \\frac{1}{2}$$\n这个结果与方差 $q_{\\ell}$ 无关（只要 $q_{\\ell}0$）。\n灵敏度为 $\\chi = \\sigma_w^2 \\cdot \\frac{1}{2}$。\n为了达到临界稳定，$\\chi=1$：\n$$\\frac{\\sigma_w^2}{2} = 1 \\implies \\sigma_w^2 = 2$$\n这是众所周知的 He 初始化条件。\n\n对于 $\\phi = \\sigma$、$\\tanh$ 和 $\\mathrm{ReLU}$，$\\sigma_w^2$ 的三个临界值分别为 $16$、$1$ 和 $2$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n16  1  2\n\\end{pmatrix}\n}\n$$", "id": "3094645"}, {"introduction": "理论分析为我们提供了深刻的见解，但通过实践来观察理论的实际效果同样重要。此练习提供了一个编码任务，你将通过一个特殊的“课程学习”方案来训练一个单神经元模型，该方案会逐步增强输入信号的幅度。通过这个实验，你将能亲眼观察到 $S$型函数和 $\\tanh$ 函数的饱和如何导致梯度消失，并将其与 ReLU 函数的稳健行为进行对比。[@problem_id:3094585]", "problem": "考虑一个通过梯度下降在合成数据集上训练的单神经元模型。该神经元根据输入向量 $x \\in \\mathbb{R}^d$、权重向量 $w \\in \\mathbb{R}^d$ 和偏置 $b \\in \\mathbb{R}$ 计算标量预激活值 $z = w^\\top x + b$，然后输出 $y_{\\text{hat}} = g(z)$，其中 $g$ 是标准激活函数之一。训练目标是预测值与固定目标值 $y_i$ 之间的均方误差（MSE）$\\frac{1}{N}\\sum_{i=1}^N \\left(y_{\\text{hat},i} - y_i\\right)^2$。数据集生成一次，并在所有测试用例中保持不变。\n\n基本基础：\n- 使用微积分中的链式法则来推导损失函数关于参数 $w$ 和 $b$ 的梯度，并使用学习率为 $\\eta$ 的标准梯度下降更新规则 $w \\leftarrow w - \\eta \\nabla_w L$, $b \\leftarrow b - \\eta \\nabla_b L$。\n- 使用激活函数的定义：逻辑（sigmoid）函数 $\\sigma(z)$、双曲正切函数 $\\tanh(z)$ 和修正线性单元（ReLU）。\n\n课程（Curriculum）设置：\n- 课程（curriculum）在不同轮次（epoch）中缩放输入的大小。对于轮次索引 $e \\in \\{0,1,\\dots,E-1\\}$，定义一个标量 $s_e  0$，并在保持数据集和目标不变的情况下，对缩放后的输入 $x^{(e)} = s_e x$进行训练。这使得在第 $e$ 轮中，有效的预激活值从 $z = w^\\top x + b$ 变为 $z^{(e)} = w^\\top (s_e x) + b$。\n- 对于主要测试，课程安排 $s_e$ 随 $e$ 递增；为了覆盖边界情况，也包含了一个 $s_e$ 为常数的测试用例。\n\n数据集规范：\n- 将随机种子固定为 $123$。\n- 使用输入维度 $d = 10$ 和样本数量 $N = 256$。\n- 从均值为零、协方差为单位矩阵的标准多元正态分布中独立抽取输入 $x_i \\in \\mathbb{R}^d$。\n- 独立地，从同一分布中抽取一个教师权重向量 $v \\in \\mathbb{R}^d$（使用相同的种子以确保可复现性）。\n- 定义目标 $y_i = \\sigma(v^\\top x_i)$，因此 $y_i \\in (0,1)$。\n\n模型初始化：\n- 初始化 $w = 0_d$（$\\mathbb{R}^d$ 中的零向量）。\n- 初始化 $b = 5.0$。\n- 使用学习率 $\\eta = 10^{-6}$。\n- 训练 $E$ 轮。每一轮使用当前的课程缩放因子 $s_e$ 并执行一次全批量梯度下降更新。\n\n科学目标与预测：\n- 从链式法则出发，推断当 $|z|$ 变得很大时，每种激活函数的梯度大小的行为。预测 $\\tanh$ 和 $\\sigma$ 的饱和（当 $|z|$ 很大时导数趋近于 $0$）如何在输入大小递增的课程下影响学习，并与 ReLU 无界的正半轴（当 $z  0$ 时导数约等于 $1$，当 $z  0$ 时导数等于 $0$）进行对比。解释在以下不同安排下，参数更新的预期相对难度。\n- 通过在每个测试用例下进行训练并报告一个量化性能指标来凭经验进行验证。\n\n评估指标：\n- 对于每个测试用例，计算在任何训练开始前、缩放因子 $s = 1$ 时的初始损失 $L_{\\text{init}}$，以及在所有轮次结束后、缩放因子 $s = 1$ 时的最终损失 $L_{\\text{final}}$。报告每个测试用例的损失减少率 $r = \\frac{L_{\\text{init}} - L_{\\text{final}}}{L_{\\text{init}}}$，结果为单个浮点数。\n\n激活函数：\n- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$。\n- $\\tanh(z)$ 是双曲正切函数。\n- $\\text{ReLU}(z) = \\max(0, z)$。\n\n测试套件：\n所有用例均使用 $E = 10$ 轮。\n1. 激活函数 $g = \\sigma$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 3.0$。\n2. 激活函数 $g = \\tanh$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 3.0$。\n3. 激活函数 $g = \\text{ReLU}$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 3.0$。\n4. 边界覆盖：激活函数 $g = \\sigma$，恒定课程 $s_e = 1$ 对所有 $e$ 成立（无增长）。\n5. 激活函数 $g = \\text{ReLU}$，指数课程 $s_e = \\alpha^e$，其中 $\\alpha = 1.5$（中度增长）。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，“[r1,r2,r3,r4,r5]”），其中每个 $r_k$ 是测试用例 $k$ 的损失减少率。不涉及任何物理单位或角度。输出中的所有数值均为浮点数。", "solution": "用户提出的问题是一个有效且定义明确的科学探究，旨在研究不同神经网络激活函数在特定训练课程下的行为。我现在将提供完整的解决方案。\n\n### 1. 理论框架\n\n该问题研究了一个单神经元模型，其由预激活函数 $z = w^\\top x + b$ 和激活函数 $y_{\\text{hat}} = g(z)$ 定义，其中 $x \\in \\mathbb{R}^d$ 是输入，$w \\in \\mathbb{R}^d$ 是权重，$b \\in \\mathbb{R}$ 是偏置。该模型使用全批量梯度下降进行训练，以最小化均方误差（MSE）损失 $L = \\frac{1}{N}\\sum_{i=1}^N (y_{\\text{hat},i} - y_i)^2$。\n\n引入了一个课程（curriculum），其中在每个轮次 $e$ 中，输入 $x_i$ 被一个因子 $s_e$ 缩放。因此，在第 $e$ 轮中样本 $i$ 的预激活值为 $z_i^{(e)} = w^\\top(s_e x_i) + b$。参数根据以下规则更新：\n$$ w \\leftarrow w - \\eta \\nabla_w L $$\n$$ b \\leftarrow b - \\eta \\nabla_b L $$\n其中 $\\eta$ 是学习率。\n\n### 2. 梯度推导\n\n为了理解课程的效果，我们推导损失函数 $L$ 关于参数 $w$ 和 $b$ 的梯度。在第 $e$ 轮次中，对单个样本 $i$ 使用链式法则：\n$$ \\frac{\\partial L_i}{\\partial w} = \\frac{\\partial L_i}{\\partial y_{\\text{hat},i}} \\frac{\\partial y_{\\text{hat},i}}{\\partial z_i^{(e)}} \\frac{\\partial z_i^{(e)}}{\\partial w} $$\n各分量为：\n- $\\frac{\\partial L_i}{\\partial y_{\\text{hat},i}} = 2(y_{\\text{hat},i} - y_i) = 2(g(z_i^{(e)}) - y_i)$\n- $\\frac{\\partial y_{\\text{hat},i}}{\\partial z_i^{(e)}} = g'(z_i^{(e)})$\n- $\\frac{\\partial z_i^{(e)}}{\\partial w} = \\frac{\\partial}{\\partial w} (w^\\top (s_e x_i) + b) = s_e x_i$\n\n对 $N$ 个样本的批次进行平均，权重向量 $w$ 的梯度为：\n$$ \\nabla_w L = \\frac{2s_e}{N} \\sum_{i=1}^N (g(z_i^{(e)}) - y_i) g'(z_i^{(e)}) x_i $$\n\n类似地，对于偏置 $b$，预激活值的导数为 $\\frac{\\partial z_i^{(e)}}{\\partial b} = 1$。偏置 $b$ 的梯度为：\n$$ \\nabla_b L = \\frac{2}{N} \\sum_{i=1}^N (g(z_i^{(e)}) - y_i) g'(z_i^{(e)}) $$\n\n关键的观察是，课程缩放因子 $s_e$ 直接作为权重梯度 $\\nabla_w L$ 的乘数，但不是偏置梯度 $\\nabla_b L$ 的乘数。此外，$s_e$ 会影响预激活值 $z_i^{(e)}$ 的大小，这又会影响关键的导数项 $g'(z_i^{(e)})$。\n\n### 3. 激活函数导数分析\n\n$g'(z)$ 这一项决定了误差信号有多少被反向传播给参数。它的行为，尤其是在 $|z|$ 很大时，至关重要。\n\n- **Sigmoid（逻辑）函数, $\\sigma(z) = \\frac{1}{1 + e^{-z}}$**：\n  其导数为 $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$。当 $z$ 为大的正数或负数时，$\\sigma(z)$ 分别趋近于 $1$ 或 $0$。在这两种情况下，乘积 $\\sigma(z)(1 - \\sigma(z))$都趋近于 $0$。这种在函数“饱和”区域梯度变得极小的现象被称为梯度消失问题。\n\n- **双曲正切函数, $\\tanh(z)$**：\n  其导数为 $\\tanh'(z) = 1 - \\tanh^2(z)$。与 sigmoid 类似，当 $|z| \\to \\infty$ 时，$\\tanh(z)$ 趋近于 $\\pm 1$，其导数 $\\tanh'(z)$ 趋近于 $0$。它也因饱和而遭受梯度消失问题。\n\n- **修正线性单元 (ReLU), $\\text{ReLU}(z) = \\max(0, z)$**：\n  其导数为：当 $z  0$ 时 $\\text{ReLU}'(z) = 1$，当 $z  0$ 时为 $0$。对于任何正输入，梯度是恒定的，不会消失。这个特性防止了在正半轴的饱和，使得梯度可以无阻碍地流动。\n\n### 4. 训练动态预测\n\n问题指定了初始状态为 $w=0_d$ 和 $b=5.0$。在第一个轮次（$e=0$）中，任何样本的预激活值为 $z_i^{(0)} = w_0^\\top(s_0 x_i) + b_0 = 0 + 5.0 = 5.0$。这个大的正偏置立即将神经元置于一个激活函数特性表现出显著差异的区域。\n\n- **测试用例 1 ($\\sigma$, $\\alpha=3.0$) 和 2 ($\\tanh$, $\\alpha=3.0$)**：\n  初始预激活值 $z=5.0$ 对 sigmoid 和 tanh 来说都处于饱和区域。$\\sigma'(5.0) \\approx 0.0066$ 且 $\\tanh'(5.0) \\approx 0.00018$。这些梯度已经非常小了。课程安排 $s_e = 3.0^e$ 会导致 $z_i^{(e)} = s_e(w^\\top x_i) + b$ 变得更大（假设 $w$ 不会变为强负值），从而将神经元推向更深的饱和区。这将使 $g'(z)$ 更接近于零。虽然 $\\nabla_w L$ 中的 $s_e$ 项呈指数增长，但随着 $z$ 的增加，$g'(z)$ 项呈指数衰减。后者的效应占主导地位，实际上会停止学习。我们预测 sigmoid 的损失减少会非常小，而 tanh 的损失减少会更小，因为它饱和得更快。\n\n- **测试用例 3 (ReLU, $\\alpha=3.0$)**：\n  初始预激活值 $z=5.0$ 处于 ReLU 的激活区域，其中 $\\text{ReLU}'(z) = 1$。梯度不会消失。权重梯度 $\\nabla_w L$ 与巨大且不断增长的缩放因子 $s_e$成正比。这对权重来说就像一个巨大且不断增大的学习率，虽然可能不稳定，但应该会促进快速学习，特别是考虑到基础学习率 $\\eta=10^{-6}$ 非常小。我们预测会有显著的损失减少。\n\n- **测试用例 4 ($\\sigma$, $s_e=1$)**：\n  这是一个对照用例。缩放因子是恒定的。神经元仍然在饱和状态下初始化（$z=5.0$），因此由于梯度小，学习会很慢。但是，与用例 1 不同，预激活值不会被一个增长的 $s_e$ 主动推向更深的饱和区。我们预测会有一个低但不可忽略的损失减少，其值应大于用例 1。\n\n- **测试用例 5 (ReLU, $\\alpha=1.5$)**：\n  与用例 3 类似，ReLU 避免了饱和问题。课程缩放因子 $s_e = 1.5^e$ 更为温和。这仍應提供强大的学习信号，而不会像 $\\alpha=3.0$ 的情况那样有同樣的不穩定风险。我们预测会有显著的损失减少，如果用例 3 中更激进的缩放导致了优化问题，其效果可能與用例 3 相当甚至更好。\n\n总而言之，预期的性能顺序（损失减少率 $r$）为 $r_2  r_1  r_4  r_3 \\approx r_5$。这个实验旨在通过经验证明激活函数饱和的后果。\n\n### 5. 算法实现\n\n解决方案是使用 `numpy` 库在 Python 中实现的。\n1. **数据生成**：一个函数根据指定的分布和随机种子生成固定的数据集 $(X, y)$ 和教师向量 $v$。\n2. **模型组件**：为 sigmoid、tanh 和 ReLU 激活函数及其各自的导数定义了辅助函数。也定义了一个用于 MSE 损失的函数。\n3. **训练过程**：一个 `train` 函数封装了核心逻辑。它将数据集、激活函数、课程安排和超参数作为输入。在函数内部，它初始化权重和偏置。然后，它迭代指定的轮次数。在每一轮中，它会：\n    a. 计算当前的缩放因子 $s_e$。\n    b. 使用缩放后的输入计算预激活值 $z^{(e)}$。\n    c. 计算预测值 $y_{\\text{hat}}$ 和激活函数导数值 $g'(z^{(e)})$。\n    d. 使用推导出的矩阵形式表达式计算梯度 $\\nabla_w L$ 和 $\\nabla_b L$。\n    e. 更新参数 $w$ 和 $b$。\n4. **评估**：主 `solve` 函数协调五个测试用例。对于每个用例，它计算在缩放因子 $s=1$ 时的初始损失 $L_{\\text{init}}$ 和训练后同样在缩放因子 $s=1$ 时的最终损失 $L_{\\text{final}}$。然后计算并存储损失减少率 $r = (L_{\\text{init}} - L_{\\text{final}}) / L_{\\text{init}}$。\n5. **输出**：将收集到的五个测试用例的比率格式化为所需的字符串格式。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Validates and solves the problem of training a single neuron with different \n    activation functions under a scaling curriculum.\n    \"\"\"\n\n    # --- Dataset Specification ---\n    D = 10  # Input dimension\n    N = 256  # Number of samples\n    SEED = 123\n\n    # --- Model and Training Parameters ---\n    W_INIT = np.zeros((D, 1))\n    B_INIT = 5.0\n    ETA = 1e-6  # Learning rate\n    E = 10  # Number of epochs\n\n    def generate_data(d, n, seed):\n        \"\"\"Generates the synthetic dataset.\"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.standard_normal(size=(n, d))\n        v = rng.standard_normal(size=(d, 1))\n        # Targets are generated by a teacher neuron with sigmoid activation\n        y = 1 / (1 + np.exp(-(X @ v)))\n        return X, y\n\n    # Generate the dataset once, fixed for all test cases.\n    X_data, y_data = generate_data(D, N, SEED)\n    \n    # --- Activation Functions and Derivatives ---\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    def sigmoid_prime(z):\n        s_z = sigmoid(z)\n        return s_z * (1 - s_z)\n\n    def tanh(z):\n        return np.tanh(z)\n\n    def tanh_prime(z):\n        return 1 - np.tanh(z)**2\n\n    def relu(z):\n        return np.maximum(0, z)\n\n    def relu_prime(z):\n        return (z > 0).astype(float)\n\n    # --- Loss Function ---\n    def mse_loss(y_hat, y):\n        return np.mean((y_hat - y)**2)\n\n    def train(X, y, g, g_prime, schedule, epochs, eta):\n        \"\"\"\n        Trains a single neuron model using full-batch gradient descent.\n        \"\"\"\n        d = X.shape[1]\n        n = X.shape[0]\n        w = np.copy(W_INIT)\n        b = B_INIT\n\n        for e in range(epochs):\n            s_e = schedule[e]\n            \n            # Forward pass with current epoch's scale\n            X_scaled = s_e * X\n            z = X_scaled @ w + b\n            y_hat = g(z)\n            \n            # Gradient computation\n            error = y_hat - y\n            g_prime_z = g_prime(z)\n            \n            # Note: For grad_w, the s_e factor is implicitly included in X_scaled.\n            # grad_w = (2/n) * X_scaled.T @ (error * g_prime_z)\n            # The problem formulation shows s_e explicitly. Both are equivalent:\n            # (s_e * X).T = s_e * X.T\n            grad_w = (2 * s_e / n) * X.T @ (error * g_prime_z)\n            grad_b = (2 / n) * np.sum(error * g_prime_z)\n            \n            # Parameter update\n            w -= eta * grad_w\n            b -= eta * grad_b\n            \n        return w, b\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        # Case 1: Sigmoid, aggressive exponential curriculum\n        {'name': 'Sigmoid_alpha3.0', 'g': sigmoid, 'g_prime': sigmoid_prime, \n         'schedule': [3.0**e for e in range(E)]},\n        # Case 2: Tanh, aggressive exponential curriculum\n        {'name': 'Tanh_alpha3.0', 'g': tanh, 'g_prime': tanh_prime, \n         'schedule': [3.0**e for e in range(E)]},\n        # Case 3: ReLU, aggressive exponential curriculum\n        {'name': 'ReLU_alpha3.0', 'g': relu, 'g_prime': relu_prime, \n         'schedule': [3.0**e for e in range(E)]},\n        # Case 4: Sigmoid, constant curriculum (control)\n        {'name': 'Sigmoid_alpha1.0', 'g': sigmoid, 'g_prime': sigmoid_prime, \n         'schedule': [1.0 for _ in range(E)]},\n        # Case 5: ReLU, moderate exponential curriculum\n        {'name': 'ReLU_alpha1.5', 'g': relu, 'g_prime': relu_prime, \n         'schedule': [1.5**e for e in range(E)]},\n    ]\n\n    results = []\n    for case in test_cases:\n        g, schedule = case['g'], case['schedule']\n        \n        # Calculate initial loss at scale s=1\n        z_init = X_data @ W_INIT + B_INIT\n        y_hat_init = g(z_init)\n        L_init = mse_loss(y_hat_init, y_data)\n        \n        # Train the model\n        w_final, b_final = train(X_data, y_data, case['g'], case['g_prime'], schedule, E, ETA)\n        \n        # Calculate final loss at scale s=1\n        z_final = X_data @ w_final + b_final\n        y_hat_final = g(z_final)\n        L_final = mse_loss(y_hat_final, y_data)\n        \n        # Compute the loss reduction ratio\n        # Handle division by zero for L_init, although unlikely here.\n        if L_init == 0:\n            # If initial loss is 0, any reduction is also 0.\n            # If L_final is also 0, ratio is ill-defined, 0 is a safe choice.\n            # If L_final > 0, something is very wrong, but ratio is negative.\n            # Let's define the ratio as 0 if L_init is 0.\n            r = 0.0 if L_final == 0 else -np.inf\n        else:\n            r = (L_init - L_final) / L_init\n        \n        results.append(r)\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3094585"}]}