{"hands_on_practices": [{"introduction": "逻辑回归模型的核心在于理解其系数的含义，特别是如何量化一个预测变量对结果对数优势比（log-odds）的影响。本练习 [@problem_id:1931449] 提供了一个具体的计算场景，帮助你掌握这一关键的解释技能，并深化对系数 $\\beta_1$ 实际意义的理解。", "problem": "一个研究小组正在研究影响学生在一项以难度著称的专业认证考试中成功与否的因素。他们从一个考生样本中收集数据，记录了他们花费的学习小时数 ($x$) 以及他们是通过 ($Y=1$) 还是未通过 ($Y=0$) 考试。\n\n他们拟合了一个简单的逻辑回归模型来预测通过的概率 $p = P(Y=1|x)$。该模型由通过考试的对数优势比（log-odds）的方程给出：\n$$ \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x $$\n将模型与数据拟合后，估计的系数为 $\\hat{\\beta}_0 = -5.2$ 和 $\\hat{\\beta}_1 = 0.115$。\n\n一名已经学习了一段时间的学生决定再多学习7个小时。根据拟合的模型，该学生通过考试的对数优势比会发生什么变化？将你的答案表示为一个四舍五入到三位有效数字的数值。", "solution": "我们给出了一个关于对数优势比的简单逻辑回归模型：\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_{0} + \\beta_{1} x.$$\n如果学习时间从 $x$ 增加到 $x + \\Delta x$，对数优势比的变化是\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\left[\\beta_{0} + \\beta_{1}(x+\\Delta x)\\right] - \\left[\\beta_{0} + \\beta_{1}x\\right] = \\beta_{1}\\Delta x.$$\n使用估计的斜率 $\\hat{\\beta}_{1} = 0.115$ 和 $\\Delta x = 7$，变化量为\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\hat{\\beta}_{1} \\cdot 7 = 0.115 \\cdot 7 = 0.805.$$\n四舍五入到三位有效数字，结果是 $0.805$。", "answer": "$$\\boxed{0.805}$$", "id": "1931449"}, {"introduction": "在现实世界的数据分析中，我们经常会遇到分类预测变量，例如客户的订阅等级。本练习 [@problem_id:1931482] 将引导你学习如何使用“虚拟变量”将这些非数值信息整合到逻辑回归模型中。掌握这一方法对于正确构建和解释包含分类数据的模型至关重要，它能帮助你理解如何为不同类别设置基准并解读其对应的系数。", "problem": "一位数据科学家正在构建一个模型，用于预测一项基于订阅的软件服务的客户流失情况。我们感兴趣的结果是一个二元变量 $Y$，其中，如果客户流失（取消订阅），则 $Y=1$；如果不流失，则 $Y=0$。唯一可用的预测变量是客户的 `Subscription Tier`（订阅等级），这是一个具有三个不同级别：“Basic”、“Standard”和“Premium”的分类变量。\n\n该数据科学家决定使用逻辑回归模型来估计流失概率 $p = P(Y=1)$。为了引入该分类预测变量，他们使用虚拟变量编码，并选择“Basic”等级作为参考类别。\n\n设 $p$ 为客户流失的概率。根据客户的订阅等级，以下哪个方程正确表示了客户流失的对数几率 $\\ln\\left(\\frac{p}{1-p}\\right)$ 的逻辑回归模型？\n\nA. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Standard}} + \\beta_2 X_{\\text{Premium}}$，其中，对于 'Standard' 等级，$X_{\\text{Standard}} = 1$，否则为 0；对于 'Premium' 等级，$X_{\\text{Premium}} = 1$，否则为 0。\n\nB. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 Z$，其中，对于 'Basic' 等级，$Z=1$；对于 'Standard' 等级，$Z=2$；对于 'Premium' 等级，$Z=3$。\n\nC. $p = \\beta_0 + \\beta_1 X_{\\text{Standard}} + \\beta_2 X_{\\text{Premium}}$，其中，对于 'Standard' 等级，$X_{\\text{Standard}} = 1$，否则为 0；对于 'Premium' 等级，$X_{\\text{Premium}} = 1$，否则为 0。\n\nD. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Basic}} + \\beta_2 X_{\\text{Standard}} + \\beta_3 X_{\\text{Premium}}$，其中，对于相应的等级，$X_{\\text{level}} = 1$，否则为 0。\n\nE. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Premium}}$，其中，如果等级是 'Premium'，$X_{\\text{Premium}} = 1$，否则为 0。", "solution": "我们使用逻辑回归对二元结果 $Y \\in \\{0,1\\}$ 进行建模，该模型将 $p=P(Y=1)$ 的对数几率（logit）指定为预测变量的线性函数：\n$$\n\\ln\\left(\\frac{p}{1-p}\\right)=\\eta=\\beta_{0}+\\text{(linear combination of predictors)}.\n$$\n对于一个具有三个级别（$L=3$）的分类预测变量，并采用参考编码，我们引入 $L-1=2$ 个虚拟变量。以 'Basic' 为参考类别，定义\n- 如果等级是 'Standard'，则 $X_{\\text{Standard}}=1$，否则为 $0$。\n- 如果等级是 'Premium'，则 $X_{\\text{Premium}}=1$，否则为 $0$。\n这样，对于 'Basic' 等级，两个指示变量都为 $0$。\n\n那么，正确的逻辑回归模型是\n$$\n\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}X_{\\text{Standard}}+\\beta_{2}X_{\\text{Premium}}.\n$$\n通过代入可得：\n- 对于 'Basic' 等级：$X_{\\text{Standard}}=0$，$X_{\\text{Premium}}=0$，因此 $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}$。\n- 对于 'Standard' 等级：$X_{\\text{Standard}}=1$，$X_{\\text{Premium}}=0$，因此 $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}$。\n- 对于 'Premium' 等级：$X_{\\text{Standard}}=0$，$X_{\\text{Premium}}=1$，因此 $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{2}$。\n这与以 'Basic' 为参考类别的标准虚拟变量编码方法相符。\n\n评估各个选项：\n- A 完全匹配上述的虚拟变量逻辑回归设定，因此是正确的。\n- B 使用单个数值代码 $Z \\in \\{1,2,3\\}$，并假设有序代码之间存在线性效应，这不等同于以 'Basic' 为参考类别的虚拟变量编码，并且在类别之间强加了不合理的线性关系。\n- C 将 $p$ 建模为预测变量的线性函数，而不是对数几率；这不是逻辑回归。\n- D 包含三个虚拟变量和一个截距项；由于 $X_{\\text{Basic}}+X_{\\text{Standard}}+X_{\\text{Premium}}=1$，这会导致完全多重共线性（虚拟变量陷阱），因此这不是一个有效的设定。\n- E 只为 'Premium' 使用了一个虚拟变量，将 'Standard' 和 'Basic' 合并在一起；这并没有通过完整的虚拟编码来表示一个以 'Basic' 为参考的三级因子。\n\n因此，正确选项是 A。", "answer": "$$\\boxed{A}$$", "id": "1931482"}, {"introduction": "掌握了如何解释模型后，下一个挑战是亲手构建一个模型。本练习 [@problem_id:3142117] 将带你深入逻辑回归的“引擎室”，从第一性原理出发，推导并实现用于求解模型参数的牛顿-拉夫森迭代算法。通过亲手编写代码并计算Tjur判别系数和McFadden伪决定系数等拟合优度指标，你将对模型的训练过程和性能评估建立起深刻而扎实的理解。", "problem": "本题要求您从第一性原理出发，实现一个二元逻辑回归模型，并比较两种解释方差的度量：Tjur 判别系数和 McFadden 伪决定系数。请从以下统计学习的基本基础开始：\n- 二元结果被建模为条件独立的伯努利随机变量，其成功概率为 $p_i \\in (0,1)$。\n- 逻辑回归模型通过 logistic 函数指定协变量与成功概率之间的关联，即 $p_i = \\sigma(\\eta_i)$，其中 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$，线性预测器为 $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$，而 $\\boldsymbol{\\beta}$ 是一个包含截距的未知参数向量。\n- 最大似然估计 (MLE) 原理选择能使伯努利模型和 logistic 关联所蕴含的对数似然最大化的 $\\boldsymbol{\\beta}$。\n\n任务：\n1. 从伯努利似然和 logistic 关联的定义出发（不使用任何其他预先推导的公式），推导以 $\\boldsymbol{\\beta}$ 表示的对数似然，并说明如何获得牛顿-拉夫逊迭代法以计算最大似然估计 $\\widehat{\\boldsymbol{\\beta}}$。然后，实现一个牛顿-拉夫逊求解器，该求解器：\n   - 默认情况下，向设计矩阵添加一列全为 1 的截距项。\n   - 迭代直至参数增量的欧几里得范数小于容差 $10^{-12}$ 或达到 $100$ 次迭代。\n2. 找到 $\\widehat{\\boldsymbol{\\beta}}$ 后，计算所有观测值的拟合概率 $\\widehat{p}_i = \\sigma(\\mathbf{x}_i^{\\top}\\widehat{\\boldsymbol{\\beta}})$。\n3. 从第一性原理出发，将 Tjur 判别系数定义为所有 $y_i = 1$ 的案例中 $\\widehat{p}_i$ 的均值与所有 $y_i = 0$ 的案例中 $\\widehat{p}_i$ 的均值之差。同样，从第一性原理出发，使用完整模型和空模型（仅含截距）的最大化对数似然来定义 McFadden 伪决定系数，并使用自然对数进行计算。\n4. 使用以下测试套件，每个案例提供一个由单个预测变量（程序必须自动添加截距）和二元响应向量组成的设计矩阵。对于每个案例，计算 Tjur 和 McFadden 两种度量。\n   - 案例 A（无明显信号）：预测变量值 $x$ 和标签 $y$\n     - $x = \\big[0,0,0,0,0,0,1,1,1,1,1,1\\big]$\n     - $y = \\big[0,1,0,1,0,1,0,1,0,1,0,1\\big]$\n   - 案例 B（中等信号）：预测变量值 $x$ 和标签 $y$\n     - $x = \\big[\\underbrace{0,\\dots,0}_{10\\ \\text{times}},\\underbrace{1,\\dots,1}_{10\\ \\text{times}}\\big]$\n     - $y = \\big[1,0,0,1,0,0,0,0,0,0,\\ 1,1,1,1,1,1,1,1,0,0\\big]$\n   - 案例 C（信号较强但非完美）：预测变量值 $x$ 和标签 $y$\n     - $x = \\big[\\underbrace{0,\\dots,0}_{12\\ \\text{times}},\\underbrace{1,\\dots,1}_{8\\ \\text{times}}\\big]$\n     - $y = \\big[1,0,0,0,0,0,0,0,0,1,0,1,\\ 1,1,1,1,1,1,1,0\\big]$\n5. 最终输出格式：您的程序应生成单行输出，包含一个由方括号括起来的逗号分隔列表。该列表必须按顺序包含案例 A、案例 B、案例 C 的 Tjur 和 McFadden 度量，均为四舍五入到六位小数的浮点值，即：\n   - 输出顺序：$\\big[\\text{Tjur}_A,\\text{McFadden}_A,\\text{Tjur}_B,\\text{McFadden}_B,\\text{Tjur}_C,\\text{McFadden}_C\\big]$。\n   - 格式示例：$\\big[0.123456,0.234567,0.345678,0.456789,0.567890,0.678901\\big]$。\n\n本问题不涉及物理量或角度，因此不需要单位。所有数值答案应按指定顺序打印为小数点后六位的十进制浮点数。", "solution": "问题陈述经评估有效。它在科学上基于统计学习的原理，问题提出得当、客观且内部一致。它提供了推导和实现二元逻辑回归模型、计算指定的解释方差度量以及将其应用于所提供测试案例所需的所有信息。该任务是计算统计学中一个标准的、有一定难度的练习。我们可以开始求解。\n\n解决方案的结构如下：首先，我们推导逻辑回归模型的数学基础，包括对数似然函数和牛顿-拉夫逊优化算法。其次，我们定义两种解释方差的度量：Tjur 判别系数和 McFadden 伪决定系数。最后，我们描述将这些原理应用于测试案例的实现过程。\n\n**1. 逻辑回归模型及其最大似然估计的推导**\n\n二元逻辑回归模型用于对 $i=1, \\dots, N$ 个观测值的二元结果变量 $y_i \\in \\{0, 1\\}$ 进行建模，其基础是预测向量 $\\mathbf{x}_i \\in \\mathbb{R}^k$。\n\n**对数似然函数**\n每个结果 $y_i$ 都被建模为一个独立的伯努利随机变量 $Y_i \\sim \\text{Bernoulli}(p_i)$ 的实现，其中 $p_i = P(Y_i=1 | \\mathbf{x}_i)$。其概率质量函数为 $P(Y_i=y_i) = p_i^{y_i}(1-p_i)^{1-y_i}$。逻辑回归模型通过 logit 关联函数将概率 $p_i$ 与预测变量 $\\mathbf{x}_i$ 联系起来。概率的对数优势（或 logit）被建模为预测变量的线性函数：\n$$ \\text{logit}(p_i) = \\log\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta} $$\n其中 $\\boldsymbol{\\beta} \\in \\mathbb{R}^k$ 是模型参数的向量。线性部分 $\\eta_i = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ 称为线性预测器。\n通过对 logit 函数求逆，我们得到概率 $p_i$ 为 sigmoid 函数 $\\sigma(\\cdot)$ 应用于线性预测器的结果：\n$$ p_i = \\sigma(\\eta_i) = \\sigma(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}} $$\n鉴于观测值的独立性，整个数据集 $(\\mathbf{y}, \\mathbf{X})$ 的似然函数是各个概率的乘积，其中 $\\mathbf{y} = (y_1, \\dots, y_N)^{\\top}$，$\\mathbf{X}$ 是以 $\\mathbf{x}_i^{\\top}$ 为行的设计矩阵：\n$$ L(\\boldsymbol{\\beta}) = \\prod_{i=1}^N p_i^{y_i} (1-p_i)^{1-y_i} $$\n在计算上，使用对数似然 $\\ell(\\boldsymbol{\\beta}) = \\log L(\\boldsymbol{\\beta})$ 更为方便：\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i \\log p_i + (1-y_i) \\log(1-p_i) \\right] $$\n我们可以利用关系式 $\\log\\left(\\frac{p_i}{1-p_i}\\right) = \\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}$ 和 $\\log(1-p_i) = -\\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}})$，将其直接用 $\\boldsymbol{\\beta}$ 表示。\n$$ \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^N \\left[ y_i(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] $$\n这就是需要最大化以求得最大似然估计 (MLE) $\\widehat{\\boldsymbol{\\beta}}$ 的对数似然函数。\n\n**用于最大化的牛顿-拉夫逊方法**\n最大似然估计 $\\widehat{\\boldsymbol{\\beta}}$ 通过求解 $\\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{0}$ 得到。由于该方程是非线性的，我们使用迭代数值方法，即牛顿-拉夫逊法。最大化 $\\ell(\\boldsymbol{\\beta})$ 的更新规则是：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - \\left[ \\mathbf{H}(\\boldsymbol{\\beta}^{(t)}) \\right]^{-1} \\nabla \\ell(\\boldsymbol{\\beta}^{(t)}) $$\n其中 $\\nabla \\ell(\\boldsymbol{\\beta})$ 是对数似然的梯度向量，$\\mathbf{H}(\\boldsymbol{\\beta})$ 是其 Hessian 矩阵。\n\n**对数似然的梯度**\n梯度 $\\nabla \\ell(\\boldsymbol{\\beta})$ 是一个由偏导数 $\\frac{\\partial \\ell}{\\partial \\beta_j}$（对于 $j=1, \\dots, k$）组成的向量。\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\frac{\\partial}{\\partial \\beta_j} \\sum_{i=1}^N \\left[ y_i(\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}) - \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] = \\sum_{i=1}^N \\left[ y_i x_{ij} - \\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) \\right] $$\n使用链式法则，$\\frac{\\partial}{\\partial \\beta_j} \\log(1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}) = \\frac{e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}}{1+e^{\\mathbf{x}_i^{\\top}\\boldsymbol{\\beta}}} \\cdot x_{ij} = p_i x_{ij}$。\n$$ \\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^N (y_i - p_i) x_{ij} $$\n以矩阵形式表示，其中 $\\mathbf{p}$ 是概率 $p_i$ 的向量，梯度为：\n$$ \\nabla \\ell(\\boldsymbol{\\beta}) = \\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}) $$\n\n**对数似然的 Hessian 矩阵**\nHessian 矩阵 $\\mathbf{H}$ 的元素为 $H_{jl} = \\frac{\\partial^2 \\ell}{\\partial \\beta_l \\partial \\beta_j}$。\n$$ H_{jl} = \\frac{\\partial}{\\partial \\beta_l} \\sum_{i=1}^N (y_i - p_i) x_{ij} = \\sum_{i=1}^N -x_{ij} \\frac{\\partial p_i}{\\partial \\beta_l} $$\n我们需要 $\\frac{\\partial p_i}{\\partial \\beta_l} = \\frac{d\\sigma(\\eta_i)}{d\\eta_i} \\frac{\\partial \\eta_i}{\\partial \\beta_l}$。sigmoid 函数的导数是 $\\sigma'(\\eta) = \\sigma(\\eta)(1-\\sigma(\\eta)) = p_i(1-p_i)$。\n因此，$\\frac{\\partial p_i}{\\partial \\beta_l} = p_i(1-p_i)x_{il}$。将其代回：\n$$ H_{jl} = - \\sum_{i=1}^N x_{ij} x_{il} p_i(1-p_i) $$\n以矩阵形式表示，令 $\\mathbf{W}$ 为对角矩阵，其对角线元素为 $W_{ii} = p_i(1-p_i)$。Hessian 矩阵为：\n$$ \\mathbf{H}(\\boldsymbol{\\beta}) = -\\mathbf{X}^{\\top}\\mathbf{W}\\mathbf{X} $$\n牛顿-拉夫逊更新变为：\n$$ \\boldsymbol{\\beta}^{(t+1)} = \\boldsymbol{\\beta}^{(t)} - (-\\mathbf{X}^{\\top}\\mathbf{W}^{(t)}\\mathbf{X})^{-1} (\\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}^{(t)})) = \\boldsymbol{\\beta}^{(t)} + (\\mathbf{X}^{\\top}\\mathbf{W}^{(t)}\\mathbf{X})^{-1} \\mathbf{X}^{\\top}(\\mathbf{y} - \\mathbf{p}^{(t)}) $$\n这就是迭代重加权最小二乘 (IRLS) 算法的更新规则。\n\n**2. 解释方差的度量**\n\n**Tjur 判别系数 ($D$)**\n该系数度量了两种结果类别的拟合概率分布的分离程度。从第一性原理出发，它是 $y_i=1$ 的观测值的平均拟合概率与 $y_i=0$ 的观测值的平均拟合概率之差。\n$$ D = \\mathbb{E}[\\widehat{p} | Y=1] - \\mathbb{E}[\\widehat{p} | Y=0] $$\n样本估计值 $\\widehat{D}$ 是根据拟合概率 $\\widehat{p}_i = \\sigma(\\mathbf{x}_i^{\\top}\\widehat{\\boldsymbol{\\beta}})$ 计算得出的：\n$$ \\widehat{D} = \\frac{\\sum_{i=1}^N y_i \\widehat{p}_i}{\\sum_{i=1}^N y_i} - \\frac{\\sum_{i=1}^N (1-y_i) \\widehat{p}_i}{\\sum_{i=1}^N (1-y_i)} $$\nD 值接近 1 表示完美分离，而接近 0 表示没有分离。\n\n**McFadden 伪 R 方 ($R^2_{\\text{McF}}$)**\n该度量类似于线性回归中的决定系数 ($R^2$)。它将拟合模型的对数似然 $\\ell_M = \\ell(\\widehat{\\boldsymbol{\\beta}})$ 与仅含截距的空模型的对数似然 $\\ell_0 = \\ell(\\widehat{\\boldsymbol{\\beta}}_0)$ 进行比较。空模型代表了一个基准，其中预测变量没有影响，成功的概率是一个常数，由样本均值 $\\bar{y}$ 估计。\n对数似然作为模型拟合度的度量，始终是非正数。完美拟合将产生为 0 的对数似然。McFadden R 方定义为：\n$$ R^2_{\\text{McF}} = 1 - \\frac{\\ell_M}{\\ell_0} $$\n该值被解释为预测变量相比于空模型在模型拟合度上的比例改进。其范围从 0（无改进）到一个理论上小于 1 的最大值。\n\n**3. 实现策略**\n\n实现将包含一个 Python 脚本。\n1.  将创建一个函数 `newton_raphson_solver`，用于为给定的设计矩阵 $\\mathbf{X}$ 和响应向量 $\\mathbf{y}$ 寻找最大似然估计 $\\widehat{\\boldsymbol{\\beta}}$。它将从 $\\boldsymbol{\\beta}=\\mathbf{0}$ 开始，迭代应用牛顿-拉夫逊更新规则，直到参数增量的欧几里得范数低于 $10^{-12}$ 或达到 $100$ 次迭代。\n2.  对于每个测试案例，我们将通过取给定的预测向量 $x$ 并在其前面添加一列全为 1 的截距项来构建一个“完整”设计矩阵。\n3.  我们还将构建一个“空”设计矩阵，它只是一个全为 1 的列，用于拟合仅含截距的模型。\n4.  完整模型和空模型都将使用 `newton_raphson_solver` 进行拟合，以获得 $\\widehat{\\boldsymbol{\\beta}}_{\\text{full}}$ 和 $\\widehat{\\boldsymbol{\\beta}}_{\\text{null}}$。\n5.  将计算完整模型的拟合概率 $\\widehat{\\mathbf{p}}$。Tjur D 将根据这些概率计算得出。\n6.  将计算最大化的完整模型 ($\\ell_M$) 和最大化的空模型 ($\\ell_0$) 的对数似然。\n7.  McFadden R 方将使用公式 $1 - \\ell_M / \\ell_0$ 计算。\n8.  此过程将对所有三个测试案例重复进行，并将结果收集并以指定格式打印。将使用 `scipy.special.expit` 函数来实现数值稳定的 sigmoid 函数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Main function to run the logistic regression analysis for all test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]),\n            np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n        ),\n        (\n            np.array([0]*10 + [1]*10),\n            np.array([1,0,0,1,0,0,0,0,0,0, 1,1,1,1,1,1,1,1,0,0])\n        ),\n        (\n            np.array([0]*12 + [1]*8),\n            np.array([1,0,0,0,0,0,0,0,0,1,0,1, 1,1,1,1,1,1,1,0])\n        )\n    ]\n\n    results = []\n    for x, y in test_cases:\n        tjur_d, mcfadden_r2 = compute_metrics(x, y)\n        results.append(round(tjur_d, 6))\n        results.append(round(mcfadden_r2, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef newton_raphson_solver(X, y, tol=1e-12, max_iter=100):\n    \"\"\"\n    Finds the MLE for logistic regression parameters using Newton-Raphson.\n\n    Args:\n        X (np.ndarray): Design matrix of shape (n_samples, n_features).\n        y (np.ndarray): Response vector of shape (n_samples,).\n        tol (float): Convergence tolerance for the parameter increment norm.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        np.ndarray: The estimated parameter vector beta.\n    \"\"\"\n    # Initialize beta to zeros\n    beta = np.zeros(X.shape[1])\n    \n    for _ in range(max_iter):\n        # Linear predictor\n        eta = X @ beta\n        \n        # Probabilities using a numerically stable sigmoid (expit)\n        p = expit(eta)\n        \n        # Diagonal weight matrix W with weights w_i = p_i * (1 - p_i)\n        # Add a small epsilon for numerical stability in case p is 0 or 1\n        w = p * (1 - p) + 1e-15\n        W = np.diag(w)\n        \n        # Gradient of the log-likelihood\n        gradient = X.T @ (y - p)\n        \n        # Hessian of the log-likelihood\n        hessian = -X.T @ W @ X\n        \n        # Newton-Raphson step\n        try:\n            step = -np.linalg.inv(hessian) @ gradient\n        except np.linalg.LinAlgError:\n            # In case of singularity, break and return current best estimate.\n            # This can happen with perfect separation, but not in test cases.\n            break\n\n        # Update beta\n        beta = beta + step\n        \n        # Check for convergence\n        if np.linalg.norm(step)  tol:\n            break\n            \n    return beta\n\ndef calculate_log_likelihood(p, y):\n    \"\"\"\n    Calculates the log-likelihood of a Bernoulli model.\n\n    Args:\n        p (np.ndarray): Vector of success probabilities.\n        y (np.ndarray): Vector of binary outcomes.\n\n    Returns:\n        float: The total log-likelihood.\n    \"\"\"\n    # Add a small epsilon to prevent log(0)\n    eps = 1e-15\n    p_clipped = np.clip(p, eps, 1 - eps)\n    return np.sum(y * np.log(p_clipped) + (1 - y) * np.log(1 - p_clipped))\n\ndef compute_metrics(x, y):\n    \"\"\"\n    Computes Tjur's D and McFadden's R^2 for a given dataset.\n\n    Args:\n        x (np.ndarray): Single-predictor vector.\n        y (np.ndarray): Response vector.\n\n    Returns:\n        tuple: A tuple containing (tjur_d, mcfadden_r2).\n    \"\"\"\n    # 1. Prepare design matrices for full and null models\n    X_full = np.c_[np.ones(x.shape[0]), x]\n    X_null = np.ones((y.shape[0], 1))\n    \n    # 2. Fit both models to get parameter estimates\n    beta_full = newton_raphson_solver(X_full, y)\n    beta_null = newton_raphson_solver(X_null, y)\n    \n    # 3. Compute Tjur's coefficient of discrimination\n    p_hat_full = expit(X_full @ beta_full)\n    \n    p_if_y1 = p_hat_full[y == 1]\n    p_if_y0 = p_hat_full[y == 0]\n    \n    mean_p1 = np.mean(p_if_y1) if len(p_if_y1) > 0 else 0\n    mean_p0 = np.mean(p_if_y0) if len(p_if_y0) > 0 else 0\n    tjur_d = mean_p1 - mean_p0\n\n    # 4. Compute McFadden's pseudo-R-squared\n    # Log-likelihood of the full model\n    ll_full = calculate_log_likelihood(p_hat_full, y)\n    \n    # Log-likelihood of the null model\n    p_hat_null = expit(X_null @ beta_null)\n    ll_null = calculate_log_likelihood(p_hat_null, y)\n    \n    # Handle the case where ll_null is zero to avoid division by zero\n    if ll_null == 0:\n        mcfadden_r2 = 1.0 if ll_full == 0 else 0.0\n    else:\n        mcfadden_r2 = 1 - (ll_full / ll_null)\n        \n    return tjur_d, mcfadden_r2\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3142117"}]}