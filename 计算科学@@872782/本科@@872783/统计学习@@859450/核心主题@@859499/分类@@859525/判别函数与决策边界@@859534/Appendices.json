{"hands_on_practices": [{"introduction": "线性分类器的核心是其决策边界——一个由判别函数定义的超平面。这个练习提供了一个绝佳的动手实践机会，让你不仅仅从数据点出发，更是从明确的几何约束出发，从头推导决策边界的参数。通过解决这个问题[@problem_id:3116612]，你将亲身体验到数学约束如何直接塑造特征空间中的分隔线，从而加深对判别函数基本原理的理解。", "problem": "在统计学习中，线性判别函数定义为 $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{x} \\in \\mathbb{R}^{2}$ 是特征向量，$\\mathbf{w} \\in \\mathbb{R}^{2}$ 是权重向量，$b \\in \\mathbb{R}$ 是偏置。决策边界是满足 $g(\\mathbf{x}) = 0$ 的点集。考虑一个二元线性分类器的约束优化问题，该分类器必须满足两类约束：\n- 锚点约束，要求决策边界穿过两个指定点 $\\mathbf{p}_{1} = (0,0)$ 和 $\\mathbf{p}_{2} = (2,2)$，这构成了约束 $\\mathbf{w}^{\\top}\\mathbf{p}_{i} + b = 0$ (对于 $i \\in \\{1,2\\}$)。\n- 单位间隔分类约束，作用于两个带标签样本 $(\\mathbf{x}_{+}, y_{+})$ 和 $(\\mathbf{x}_{-}, y_{-})$，其中 $y_{+} = +1$，$y_{-} = -1$，$\\mathbf{x}_{+} = (4,3)$ 且 $\\mathbf{x}_{-} = (2,5)$。这构成了约束 $y_{j}(\\mathbf{w}^{\\top}\\mathbf{x}_{j} + b) \\geq 1$ (对于 $j \\in \\{+, -\\}$)。\n\n从以上定义出发，仅使用判别函数的基本原理以及线性和等式约束的可行性分析，判断该约束集是否可行（即是否存在满足所有约束的 $\\mathbf{w}$ 和 $b$）。如果可行，考虑在给定约束条件下最小化权重向量的欧几里得范数的平方 $\\|\\mathbf{w}\\|^{2}$ 的优化问题。推导在这些约束下 $\\|\\mathbf{w}\\|^{2}$ 的最小值。将最终答案表示为一个实数。无需四舍五入。", "solution": "用户提供了一个线性分类器背景下的约束优化问题。任务是首先确定约束集的可行性，如果可行，则求出权重向量的欧几里得范数的平方 $\\|\\mathbf{w}\\|^{2}$ 的最小值。\n\n该问题由以下要素定义：\n- 线性判别函数 $g(\\mathbf{x}) = \\mathbf{w}^{\\top}\\mathbf{x} + b$，其中 $\\mathbf{x} \\in \\mathbb{R}^{2}$，$\\mathbf{w} \\in \\mathbb{R}^{2}$，$b \\in \\mathbb{R}$。\n- 由 $g(\\mathbf{x}) = 0$ 的点集定义的决策边界。\n- 关于参数 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$ 和 $b$ 的一组约束。\n\n首先，我们必须验证问题陈述。该问题是统计学习理论领域一个定义明确的数学任务。它具有科学依据、客观，并提供了继续进行所需的所有信息。它要求确定可行性，这是分析约束系统的标准部分。因此，该问题是有效的，我们可以继续进行求解。\n\n求解过程包括两个主要部分：分析约束的可行性，然后解决优化问题。\n\n**第一部分：约束分析与可行性**\n\n约束分为两类：锚点约束（等式）和分类约束（不等式）。\n\n1.  **锚点约束：** 这些约束要求决策边界穿过两个指定点 $\\mathbf{p}_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $\\mathbf{p}_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$。这意味着这些点必须满足决策边界方程 $\\mathbf{w}^{\\top}\\mathbf{x} + b = 0$。\n\n    对于 $\\mathbf{p}_{1} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$：\n    $$ \\mathbf{w}^{\\top}\\mathbf{p}_{1} + b = w_1(0) + w_2(0) + b = 0 $$\n    这立即得出条件 $b = 0$。\n\n    对于 $\\mathbf{p}_{2} = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$，并使用 $b=0$ 的结果：\n    $$ \\mathbf{w}^{\\top}\\mathbf{p}_{2} + 0 = w_1(2) + w_2(2) = 0 $$\n    $$ 2w_1 + 2w_2 = 0 \\implies w_1 + w_2 = 0 \\implies w_2 = -w_1 $$\n    因此，锚点约束将三个自由参数（$w_1$、$w_2$、$b$）减少到单个自由度。偏置 $b$ 必须为 $0$，权重向量必须具有形式 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ -w_1 \\end{pmatrix}$，其中 $w_1 \\in \\mathbb{R}$ 为某个标量。决策边界由 $w_1 x_1 - w_1 x_2 = 0$ 定义。如果 $w_1 \\neq 0$，则可简化为 $x_1-x_2=0$，这正是穿过两个锚点的直线，与预期相符。\n\n2.  **单位间隔分类约束：** 这些约束由不等式 $y_{j}(\\mathbf{w}^{\\top}\\mathbf{x}_{j} + b) \\geq 1$ 给出，适用于两个带标签样本。我们将锚点约束的结果（$b=0$ 和 $\\mathbf{w} = \\begin{pmatrix} w_1 \\\\ -w_1 \\end{pmatrix}$）代入这些不等式。\n\n    对于正样本 $(\\mathbf{x}_{+}, y_{+})$，其中 $\\mathbf{x}_{+} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix}$ 和 $y_{+} = +1$：\n    $$ (+1) \\left( \\mathbf{w}^{\\top}\\mathbf{x}_{+} + 0 \\right) \\geq 1 $$\n    $$ \\begin{pmatrix} w_1 & -w_1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} \\geq 1 $$\n    $$ 4w_1 - 3w_1 \\geq 1 $$\n    $$ w_1 \\geq 1 $$\n\n    对于负样本 $(\\mathbf{x}_{-}, y_{-})$，其中 $\\mathbf{x}_{-} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}$ 和 $y_{-} = -1$：\n    $$ (-1) \\left( \\mathbf{w}^{\\top}\\mathbf{x}_{-} + 0 \\right) \\geq 1 $$\n    $$ (-1) \\left( \\begin{pmatrix} w_1 & -w_1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix} \\right) \\geq 1 $$\n    $$ (-1) (2w_1 - 5w_1) \\geq 1 $$\n    $$ (-1) (-3w_1) \\geq 1 $$\n    $$ 3w_1 \\geq 1 \\implies w_1 \\geq \\frac{1}{3} $$\n\n3.  **可行性结论：**\n    对唯一剩下的参数 $w_1$ 的完整约束集是：\n    - $w_1 \\geq 1$\n    - $w_1 \\geq \\frac{1}{3}$\n\n    这两个条件的交集是区间 $[1, \\infty)$。由于该区间非空（例如，$w_1 = 2$ 是一个有效选择），因此约束集是可行的。存在满足所有给定条件的参数 $\\mathbf{w}$ 和 $b$。\n\n**第二部分：$\\|\\mathbf{w}\\|^{2}$ 的最小化**\n\n在确定了可行性之后，我们现在继续解决优化问题：\n在推导出的约束条件下，最小化 $\\|\\mathbf{w}\\|^{2}$。\n\n目标函数是 $\\mathbf{w}$ 的欧几里得范数的平方：\n$$ \\|\\mathbf{w}\\|^{2} = w_1^2 + w_2^2 $$\n使用来自锚点约束的关系 $w_2 = -w_1$，我们可以将目标函数仅用 $w_1$ 表示：\n$$ \\|\\mathbf{w}\\|^{2} = w_1^2 + (-w_1)^2 = 2w_1^2 $$\n\n因此，优化问题被简化为一个一维问题：\n最小化 $f(w_1) = 2w_1^2$\n约束条件为 $w_1 \\geq 1$。\n\n函数 $f(w_1) = 2w_1^2$ 是一个抛物线，其顶点（全局最小值）在 $w_1=0$ 处。在定义域 $w_1 > 0$ 上，该函数是严格递增的。$w_1$ 的可行域是区间 $[1, \\infty)$。由于函数在整个区间上是递增的，其在此定义域上的最小值将出现在最左侧的边界点，即 $w_1 = 1$。\n\n因此，目标函数的最小值通过在 $w_1=1$ 处计算 $f(w_1)$ 得到：\n$$ \\min(\\|\\mathbf{w}\\|^{2}) = f(1) = 2(1)^2 = 2 $$\n\n该最小值在参数 $w_1=1$ 时取得，这意味着 $w_2 = -1$ 且 $b=0$。最优权重向量是 $\\mathbf{w}^{*} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。其范数平方的最小值是 $\\|\\mathbf{w}^{*}\\|^{2} = 1^2 + (-1)^2 = 2$。\n这个解满足所有约束：\n- $w_1 = 1 \\geq 1$ (激活约束)。\n- $w_1 = 1 \\geq \\frac{1}{3}$。\n因此，最小值为 $2$。", "answer": "$$\\boxed{2}$$", "id": "3116612"}, {"introduction": "在掌握了线性边界的基础后，我们将探索更复杂且更敏感的决策边界。这个问题[@problem_id:3116628]构建了一个假设的对抗场景，旨在研究数据中的微小变化——特别是翻转边界附近数据点的标签——会如何戏剧性地改变一个更灵活的非线性分类器的边界形态（或拓扑结构）。这个练习将让你深入理解模型选择、数据完整性以及最终决策边界稳定性之间的相互作用。", "problem": "考虑在实数轴上的一个二元分类问题，有三个训练样本，其输入为 $x_{1}=-1$、$x_{2}=0$ 和 $x_{3}=1$，对应的标签为 $y_{1}=-1$、$y_{2}=+1$ 和 $y_{3}=+1$。分类器的判别函数是一个实值函数 $f(x)$，其决策边界是满足 $f(x)=0$ 的点集，正决策区域为 $\\{x:\\,f(x) \\ge 0\\}$。\n\n在此数据上训练了两个分类器，规则如下：\n\n- 线性最小二乘分类器 $f_{\\mathrm{lin}}(x)=w x + b$ 是通过关于 $w$ 和 $b$ 最小化 $\\sum_{i=1}^{3}\\bigl(y_{i}-\\bigl(w x_{i}+b\\bigr)\\bigr)^{2}$ 得到的。将每个训练点的间隔定义为 $m_{i}=|f_{\\mathrm{lin}}(x_{i})|$。允许对手仅在间隔不超过阈值 $\\tau=\\frac{7}{10}$ 的训练点上翻转标签。\n\n- 在多项式特征空间中的一个2次核分类器（等效于一个二次判别式）$f_{\\mathrm{ker}}(x)=a x^{2}+b x + c$ 是通过对标签进行精确插值得到的，即在没有正则化的情况下强制执行 $f_{\\mathrm{ker}}(x_{i})=y_{i}$（对于 $i=1,2,3$）。$f_{\\mathrm{ker}}$ 在一维空间中决策边界的拓扑结构由 $f_{\\mathrm{ker}}(x)=0$ 的不同实根的数量决定：两个不同的实根对应两个边界点（正决策区域可能不连通），一个重复的实根对应单个边界点，没有实根则对应没有边界。\n\n从给定的标签开始，确定需要多少次最小的对抗性标签翻转（仅限于间隔 $m_{i}\\le \\tau$ 的允许训练点），才能将核分类器决策边界的拓扑结构从具有两个不同的实根改变为没有实根。请以单个整数形式提供您的答案。无需四舍五入。", "solution": "该问题要求找到改变核分类器决策边界拓扑结构所需的最小对抗性标签翻转次数。该过程包括三个主要步骤：首先，通过分析线性最小二乘分类器来确定哪些训练点容易受到标签翻转的影响；其次，描述初始核分类器的特征；第三，检查允许的翻转对核分类器决策边界的影响。\n\n首先，我们确定线性最小二乘分类器 $f_{\\mathrm{lin}}(x)=w x + b$ 的参数。该分类器在数据点 $(x_1, y_1) = (-1, -1)$、$(x_2, y_2) = (0, 1)$ 和 $(x_3, y_3) = (1, 1)$ 上进行训练。参数 $w$ 和 $b$ 是通过最小化平方误差和 $J(w, b) = \\sum_{i=1}^{3} (y_i - (w x_i + b))^2$ 来找到的。\n成本函数为：\n$$J(w,b) = (y_1 - (wx_1+b))^2 + (y_2 - (wx_2+b))^2 + (y_3 - (wx_3+b))^2$$\n$$J(w,b) = (-1 - (-w+b))^2 + (1 - (0+b))^2 + (1 - (w+b))^2$$\n$$J(w,b) = (-1+w-b)^2 + (1-b)^2 + (1-w-b)^2$$\n为了最小化 $J(w, b)$，我们将其关于 $w$ 和 $b$ 的偏导数设为 $0$。\n$$\\frac{\\partial J}{\\partial b} = 2(-1+w-b)(-1) + 2(1-b)(-1) + 2(1-w-b)(-1) = 0$$\n$$ (1-w+b) + (b-1) + (w+b-1) = 0 $$\n$$ 3b - 1 = 0 \\implies b = \\frac{1}{3} $$\n$$\\frac{\\partial J}{\\partial w} = 2(-1+w-b)(1) + 2(1-w-b)(-1) = 0$$\n$$ (-1+w-b) - (1-w-b) = 0 $$\n$$ 2w - 2 = 0 \\implies w = 1 $$\n所以，线性最小二乘分类器为 $f_{\\mathrm{lin}}(x) = x + \\frac{1}{3}$。\n\n接下来，我们计算每个训练点的间隔 $m_i = |f_{\\mathrm{lin}}(x_i)|$。\n对于 $x_1 = -1$：$f_{\\mathrm{lin}}(-1) = -1 + \\frac{1}{3} = -\\frac{2}{3}$。间隔为 $m_1 = |-\\frac{2}{3}| = \\frac{2}{3}$。\n对于 $x_2 = 0$：$f_{\\mathrm{lin}}(0) = 0 + \\frac{1}{3} = \\frac{1}{3}$。间隔为 $m_2 = \\frac{1}{3}$。\n对于 $x_3 = 1$：$f_{\\mathrm{lin}}(1) = 1 + \\frac{1}{3} = \\frac{4}{3}$。间隔为 $m_3 = \\frac{4}{3}$。\n\n对手只有在间隔 $m_i$ 不超过阈值 $\\tau = \\frac{7}{10}$ 时才能翻转标签 $y_i$。\n$m_1 = \\frac{2}{3} \\approx 0.667$。由于 $\\frac{2}{3}  \\frac{7}{10}$，标签 $y_1$ 可以被翻转。\n$m_2 = \\frac{1}{3} \\approx 0.333$。由于 $\\frac{1}{3}  \\frac{7}{10}$，标签 $y_2$ 可以被翻转。\n$m_3 = \\frac{4}{3} \\approx 1.333$。由于 $\\frac{4}{3} > \\frac{7}{10}$，标签 $y_3$ 不能被翻转。\n因此，在 $x_1=-1$ 和 $x_2=0$ 处的标签是可翻转的。\n\n现在我们分析初始的2次核分类器 $f_{\\mathrm{ker}}(x) = ax^2 + bx + c$。它通过对初始标签 $y_1 = -1$、$y_2 = 1$、$y_3 = 1$ 进行精确插值来确定。\n插值条件为：\n$f_{\\mathrm{ker}}(-1) = a(-1)^2 + b(-1) + c = a - b + c = -1$\n$f_{\\mathrm{ker}}(0) = a(0)^2 + b(0) + c = c = 1$\n$f_{\\mathrm{ker}}(1) = a(1)^2 + b(1) + c = a + b + c = 1$\n将 $c=1$ 代入第一个和第三个方程，得到：\n$a - b + 1 = -1 \\implies a - b = -2$\n$a + b + 1 = 1 \\implies a + b = 0$\n将这两个方程相加得到 $2a = -2$，所以 $a = -1$。将 $a=-1$ 代入 $a+b=0$ 得到 $b=1$。\n初始核分类器为 $f_{\\mathrm{ker}}(x) = -x^2 + x + 1$。\n其决策边界的拓扑结构由 $f_{\\mathrm{ker}}(x) = 0$ 的根确定。不同实根的数量取决于判别式 $\\Delta = b^2 - 4ac$。\n对于初始分类器，$\\Delta = (1)^2 - 4(-1)(1) = 1 + 4 = 5$。由于 $\\Delta > 0$，存在两个不同的实根，如问题所述。\n\n目标是使用最少次数的允许翻转，将拓扑结构从“两个不同的实根”（$\\Delta > 0$）变为“没有实根”（$\\Delta  0$）。让我们测试单次翻转的情形。\n\n情况1：翻转 $x_1$ 处的标签。原始标签为 $y_1=-1$；新标签为 $y'_1 = 1$。标签集变为 $(y'_1, y_2, y_3) = (1, 1, 1)$。我们通过对这些新标签进行插值来找到新的核分类器 $f'_{\\mathrm{ker}}(x) = a'x^2+b'x+c'$。\n$a' - b' + c' = 1$\n$c' = 1$\n$a' + b' + c' = 1$\n将 $c'=1$ 代入可得：\n$a' - b' + 1 = 1 \\implies a' - b' = 0$\n$a' + b' + 1 = 1 \\implies a' + b' = 0$\n解此方程组可得 $a'=0$ 和 $b'=0$。\n新的分类器是 $f'_{\\mathrm{ker}}(x) = 0 \\cdot x^2 + 0 \\cdot x + 1 = 1$。决策边界由 $f'_{\\mathrm{ker}}(x) = 0$ 定义，即方程 $1=0$。该方程无解，因此没有实根。这对应于所期望的“没有实根”的拓扑结构。\n这一变化仅通过一次翻转就实现了。\n\n为完整起见，我们检查另一种单次翻转的可能性。\n情况2：翻转 $x_2$ 处的标签。原始标签为 $y_2=1$；新标签为 $y''_2 = -1$。标签集变为 $(y_1, y''_2, y_3) = (-1, -1, 1)$。我们找到新的分类器 $f''_{\\mathrm{ker}}(x) = a''x^2+b''x+c''$。\n$a'' - b'' + c'' = -1$\n$c'' = -1$\n$a'' + b'' + c'' = 1$\n将 $c''=-1$ 代入可得：\n$a'' - b'' - 1 = -1 \\implies a'' - b'' = 0 \\implies a''=b''$\n$a'' + b'' - 1 = 1 \\implies a'' + b'' = 2$\n将 $a''=b''$ 代入第二个方程得到 $2a'' = 2$，所以 $a''=1$ 且 $b''=1$。\n新的分类器是 $f''_{\\mathrm{ker}}(x) = x^2 + x - 1$。\n判别式为 $\\Delta'' = (1)^2 - 4(1)(-1) = 1+4=5$。由于 $\\Delta'' > 0$，拓扑结构仍然是“两个不同的实根”。这次翻转没有达到目标。\n\n由于单次翻转（在 $x_1$ 处）足以将拓扑结构改变为“没有实根”，并且 $1$ 是可能的最小非零翻转次数，因此所需的最小翻转次数为 $1$。", "answer": "$$\\boxed{1}$$", "id": "3116628"}, {"introduction": "真实世界的数据很少是线性可分的。我们最后的实践将从分析性的谜题转向计算性实验，挑战一个经典问题：分离相互缠绕的螺旋线。通过这个练习[@problem_id:3116684]，你将亲身发现简单模型的局限性，并理解为何必须增加判别函数的复杂度——在这里是通过使用更高次的项——才能捕捉到错综复杂的非线性决策边界。", "problem": "考虑平面中的二元分类问题，其决策规则由一个判别函数定义。一个判别函数 $g(\\mathbf{x})$ 将 $\\mathbf{x}\\in\\mathbb{R}^2$ 映射到 $\\mathbb{R}$，并导出由 $g(\\mathbf{x})=0$ 给出的决策边界。$g(\\mathbf{x})\\ge 0$ 的点被分到类别 $+1$，$g(\\mathbf{x})  0$ 的点被分到类别 $-1$。您将研究有界阶数的多项式判别式，并确定恢复分隔两条相互缠绕的螺旋线的光滑边界所需的最小阶数。\n\n推导的基本基础：\n- 判别函数 $g(\\mathbf{x})$ 通过其零水平集 $g(\\mathbf{x})=0$ 定义决策边界。\n- 使用平方损失的经验风险最小化通过最小化 $\\sum_{i=1}^{n} \\left(g(\\mathbf{x}_i)-y_i\\right)^2$ 来拟合判别式，其中 $y_i\\in\\{-1,+1\\}$ 是标签。\n- 多项式基函数产生一个有限维的线性模型。设 $\\alpha=(\\alpha_1,\\alpha_2)$ 是一个非负整数的多重索引，且 $|\\alpha|=\\alpha_1+\\alpha_2$。定义单项式 $\\mathbf{x}^\\alpha=x_1^{\\alpha_1}x_2^{\\alpha_2}$。一个阶数至多为 $m$ 的多项式判别式是 $g_m(\\mathbf{x})=\\sum_{|\\alpha|\\le m} c_\\alpha \\mathbf{x}^\\alpha$，其中 $c_\\alpha\\in\\mathbb{R}$ 是系数。\n- 使用一个小的岭参数的正则化最小二乘可以稳定拟合：对于一个小的 $\\lambda0$，最小化 $\\sum_{i=1}^{n} \\left(g_m(\\mathbf{x}_i)-y_i\\right)^2 + \\lambda \\sum_{|\\alpha|\\le m} c_\\alpha^2$。\n\n数据生成：\n- 两条相互缠绕的阿基米德螺旋线在极坐标中通过 $r=a\\,\\theta$ 进行参数化，其中角度 $\\theta$ 以弧度为单位。对于类别 $+1$，从 $[0, 2\\pi T]$ 上的均匀分布中采样 $\\theta$，并设置 $(x,y)=(r\\cos\\theta,r\\sin\\theta)$，其中 $r=a\\,\\theta$。对于类别 $-1$，使用平移了 $\\pi$ 的角度，即 $\\theta'=\\theta+\\pi$，并设置 $(x',y')=(r'\\cos\\theta',r'\\sin\\theta')$，其中 $r'=a\\,\\theta$。向两个坐标添加标准差为 $\\sigma$ 的独立高斯噪声。通过将 $x$ 和 $y$ 都除以最大半径 $r_{\\max}=a\\cdot 2\\pi T$ 来归一化特征，以使输入位于可比较的范围内。角度必须以弧度为单位。\n\n研究设计：\n- 对于每个数据集，使用固定的比例 $f_{\\text{train}}=0.7$ 用于训练，$f_{\\text{val}}=0.3$ 用于验证，将样本分割为训练集和验证集。每个数据集使用固定的随机种子以确保可复现性。\n- 对于每个候选多项式阶数 $m\\in\\{1,2,\\dots,M_{\\max}\\}$，使用训练集和岭参数 $\\lambda=10^{-6}$，通过正则化最小二乘拟合 $g_m(\\mathbf{x})$，然后计算验证准确率，即预测标签 $\\operatorname{sign}(g_m(\\mathbf{x}))$ 与真实标签相匹配的验证点的比例。当某个验证点的 $g_m(\\mathbf{x})=0$ 时，将其预测视为 $+1$。\n- 将数据集的最小阶数定义为候选集中的最小 $m$，使得验证准确率至少达到指定的阈值 $\\tau$。如果没有阶数达到该阈值，则返回 $-1$。\n\n测试套件：\n- 使用以下参数集，其中每个元组为 $(N,T,a,\\sigma,\\tau,M_{\\max},\\text{seed})$：\n    - 情况 $1$ (理想情况): $(600,3,0.5,0.05,0.85,10,0)$。\n    - 情况 $2$ (低噪声，较少圈数): $(600,2,0.7,0.0,0.90,10,1)$。\n    - 情况 $3$ (更多圈数，中等噪声): $(600,5,0.3,0.12,0.80,12,2)$。\n    - 情况 $4$ (边界情况，高噪声，较少样本): $(300,4,0.6,0.25,0.75,12,3)$。\n\n要求输出：\n- 您的程序应生成单行输出，其中按顺序包含四个测试用例的最小阶数，格式为方括号括起来的逗号分隔列表（例如，$[m_1,m_2,m_3,m_4]$）。每个 $m_i$ 必须是一个整数，其中 $-1$ 表示在该情况下，候选集中的多项式阶数均未达到阈值。", "solution": "用户希望确定判别函数为精确分类属于两条相互缠绕的阿基米德螺旋线的数据点所需的最小多项式阶数。这是统计学习背景下的一个模型选择问题。该解决方案涉及针对一组给定参数的系统性计算实验。\n\n### 1. 问题形式化与数据生成\n\n该任务是 $\\mathbb{R}^2$ 中的一个二元分类问题。两个类别被标记为 $y=+1$ 和 $y=-1$。数据点基于两条相互缠绕的阿基米德螺旋线生成。\n\n阿基米德螺旋线在极坐标 $(r, \\phi)$ 中由方程 $r = a\\phi$ 描述。对于一组给定的参数 $(N, T, a, \\sigma)$，我们为每个类别生成 $N/2$ 个点。\n\n- **类别 $+1$**：我们从区间 $[0, 2\\pi T]$ 上的均匀分布中采样 $N/2$ 个角度 $\\theta_i$。每个点在笛卡尔坐标中由下式给出：\n$$\n\\mathbf{x}_{i,+} = \\begin{pmatrix} r_i\\cos\\theta_i \\\\ r_i\\sin\\theta_i \\end{pmatrix} + \\boldsymbol{\\epsilon}_i, \\quad \\text{其中 } r_i = a\\theta_i\n$$\n项 $\\boldsymbol{\\epsilon}_i$ 表示添加到每个坐标的独立高斯噪声，它从均值为 $0$、标准差为 $\\sigma$ 的正态分布中抽取，即 $\\boldsymbol{\\epsilon}_i \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$，其中 $I$ 是 $2 \\times 2$ 单位矩阵。\n\n- **类别 $-1$**：对于用于类别 $+1$ 的每个角度 $\\theta_i$，类别 $-1$ 的对应点使用一个平移后的角度 $\\theta'_i = \\theta_i + \\pi$ 和半径 $r'_i = a\\theta_i$ 生成。请注意，半径是原始角度 $\\theta_i$ 的函数。其笛卡尔坐标为：\n$$\n\\mathbf{x}_{i,-} = \\begin{pmatrix} r'_i\\cos\\theta'_i \\\\ r'_i\\sin\\theta'_i \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i = \\begin{pmatrix} a\\theta_i\\cos(\\theta_i + \\pi) \\\\ a\\theta_i\\sin(\\theta_i + \\pi) \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i = -\\begin{pmatrix} a\\theta_i\\cos\\theta_i \\\\ a\\theta_i\\sin\\theta_i \\end{pmatrix} + \\boldsymbol{\\epsilon}'_i\n$$\n这种构造意味着，在添加噪声之前，类别 $-1$ 的点与类别 $+1$ 的点相对于原点是完全中心对称的。\n\n- **归一化**：生成所有 $N$ 个点后，通过除以最大可能半径 $r_{\\max} = a \\cdot 2\\pi T$ 来对其坐标进行归一化，以确保特征值落在一致的范围内。\n\n### 2. 多项式判别函数\n\n分类是使用一个阶数至多为 $m$ 的多项式判别函数 $g_m(\\mathbf{x})$ 来执行的。对于一个点 $\\mathbf{x} = (x_1, x_2)^T \\in \\mathbb{R}^2$，该判别函数是单项式的线性组合：\n$$\ng_m(\\mathbf{x}) = \\sum_{|\\alpha| \\le m} c_\\alpha \\mathbf{x}^\\alpha = \\sum_{\\alpha_1+\\alpha_2 \\le m} c_{(\\alpha_1,\\alpha_2)} x_1^{\\alpha_1} x_2^{\\alpha_2}\n$$\n其中 $c_\\alpha$ 是实值系数。这可以表示为线性模型的向量形式，$g_m(\\mathbf{x}) = \\boldsymbol{\\phi}_m(\\mathbf{x})^T \\mathbf{c}$，其中 $\\boldsymbol{\\phi}_m(\\mathbf{x})$ 是单项式基函数的向量（例如，当 $m=2$ 时，$\\boldsymbol{\\phi}_2(\\mathbf{x}) = (1, x_1, x_2, x_1^2, x_1x_2, x_2^2)^T$），$\\mathbf{c}$ 是相应系数 $c_\\alpha$ 的向量。\n\n决策边界是 $g_m(\\mathbf{x})=0$ 的点的集合。决策规则根据判别函数的符号分配类别标签：\n$$\n\\text{class}(\\mathbf{x}) = \\begin{cases} +1  \\text{若 } g_m(\\mathbf{x}) \\ge 0 \\\\ -1  \\text{若 } g_m(\\mathbf{x})  0 \\end{cases}\n$$\n\n### 3. 通过正则化最小二乘进行模型训练\n\n系数向量 $\\mathbf{c}$ 是通过使用正则化最小二乘（岭回归）将模型拟合到训练集 $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n_{\\text{train}}}$ 来确定的。该方法最小化判别函数输出与真实标签之间的平方误差和，再加上对系数幅度的惩罚项：\n$$\n\\min_{\\mathbf{c}} \\sum_{i=1}^{n_{\\text{train}}} (g_m(\\mathbf{x}_i) - y_i)^2 + \\lambda \\sum_{|\\alpha| \\le m} c_\\alpha^2\n$$\n其中 $\\lambda = 10^{-6}$ 是正则化参数。用矩阵形式表示为：\n$$\n\\min_{\\mathbf{c}} \\|\\mathbf{\\Phi}\\mathbf{c} - \\mathbf{y}\\|^2_2 + \\lambda \\|\\mathbf{c}\\|^2_2\n$$\n其中 $\\mathbf{\\Phi}$ 是设计矩阵，其行是 $\\boldsymbol{\\phi}_m(\\mathbf{x}_i)^T$，而 $\\mathbf{y}$ 是标签向量。最优系数 $\\hat{\\mathbf{c}}$ 的解析解由正规方程组给出：\n$$\n\\hat{\\mathbf{c}} = (\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda I)^{-1}\\mathbf{\\Phi}^T\\mathbf{y}\n$$\n其中 $I$ 是适当维度的单位矩阵。\n\n### 4. 模型选择与评估\n\n核心任务是找到达到期望分类性能的最小多项式阶数 $m^*$。其过程如下：\n1.  对于每个测试用例，生成数据并将其分割成训练集（$f_{\\text{train}}=0.7$ 的样本）和验证集（$f_{\\text{val}}=0.3$）。在用固定的随机种子打乱数据集后进行分割，以确保可复现性。\n2.  对于从 $1$ 到 $M_{\\max}$ 的每个候选阶数 $m$：\n    a. 为训练数据构建多项式特征矩阵 $\\mathbf{\\Phi}_{\\text{train}}$。\n    b. 使用岭回归公式计算系数向量 $\\hat{\\mathbf{c}}$ 来训练模型。\n    c. 在验证集上评估训练好的模型。对于每个验证点 $\\mathbf{x}_j$，计算预测标签 $\\hat{y}_j = \\operatorname{sign}(g_m(\\mathbf{x}_j))$。\n    d. 计算验证准确率，即正确分类点的比例：$\\text{Accuracy} = \\frac{1}{n_{\\text{val}}} \\sum_{j=1}^{n_{\\text{val}}} \\mathbb{I}(\\hat{y}_j = y_j)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n3.  要求的最小阶数 $m^*$ 是搜索范围中使得验证准确率大于或等于指定阈值 $\\tau$ 的最小 $m$。\n4.  如果在 $\\{1, \\dots, M_{\\max}\\}$ 中没有阶数 $m$ 满足准确率阈值 $\\tau$，则结果报告为 $-1$。\n\n对测试套件中提供的每个参数集执行此过程，以确定相应的最小阶数。\n\n```python\nimport numpy as np\n\ndef generate_polynomial_features(X, degree):\n    \"\"\"\n    Generates a design matrix for polynomial features up to a given degree.\n\n    Args:\n        X (np.ndarray): Input data of shape (n_samples, 2).\n        degree (int): The maximum degree of the polynomial.\n\n    Returns:\n        np.ndarray: The design matrix of shape (n_samples, n_features).\n    \"\"\"\n    n_samples = X.shape[0]\n    x1 = X[:, 0]\n    x2 = X[:, 1]\n\n    num_features = (degree + 1) * (degree + 2) // 2\n    phi = np.empty((n_samples, num_features))\n\n    feature_idx = 0\n    for m in range(degree + 1):\n        for p1 in range(m + 1):\n            p2 = m - p1\n            phi[:, feature_idx] = (x1**p1) * (x2**p2)\n            feature_idx += 1\n    return phi\n\ndef run_case(N, T, a, sigma, tau, M_max, seed):\n    \"\"\"\n    Runs a single test case to find the minimal polynomial degree.\n\n    Args:\n        N (int): Total number of samples.\n        T (float): Number of turns for the spiral.\n        a (float): Spiral constant (r = a*theta).\n        sigma (float): Standard deviation of Gaussian noise.\n        tau (float): Accuracy threshold.\n        M_max (int): Maximum polynomial degree to test.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        int: The minimal polynomial degree, or -1 if threshold is not met.\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    n_per_class = N // 2\n    \n    # Generate angles for class +1\n    theta = rng.uniform(0, 2 * np.pi * T, size=n_per_class)\n    r = a * theta\n    \n    # Class +1 points\n    x_plus = r * np.cos(theta)\n    y_plus = r * np.sin(theta)\n    \n    # Class -1 points (point-symmetric to class +1)\n    x_minus = -x_plus\n    y_minus = -y_plus\n    \n    X_plus = np.stack((x_plus, y_plus), axis=1)\n    X_minus = np.stack((x_minus, y_minus), axis=1)\n    \n    # Add noise\n    X_plus += rng.normal(0, sigma, size=X_plus.shape)\n    X_minus += rng.normal(0, sigma, size=X_minus.shape)\n    \n    X = np.vstack((X_plus, X_minus))\n    y = np.hstack((np.ones(n_per_class), -np.ones(n_per_class)))\n    \n    # Normalization\n    r_max = a * 2 * np.pi * T\n    if r_max > 0:\n        X /= r_max\n        \n    # 2. Data Splitting (70% train, 30% validation)\n    indices = np.arange(N)\n    rng.shuffle(indices)\n    \n    train_size = int(0.7 * N)\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size:]\n    \n    X_train, y_train = X[train_idx], y[train_idx]\n    X_val, y_val = X[val_idx], y[val_idx]\n\n    lambda_reg = 1e-6\n    \n    # 3. Model Selection Loop\n    for m in range(1, M_max + 1):\n        # Feature Engineering\n        phi_train = generate_polynomial_features(X_train, m)\n        phi_val = generate_polynomial_features(X_val, m)\n\n        # Training (Ridge Regression)\n        d = phi_train.shape[1]\n        I = np.eye(d)\n        \n        try:\n            # Solve normal equations: (Phi^T * Phi + lambda * I) * c = Phi^T * y\n            A = phi_train.T @ phi_train + lambda_reg * I\n            b = phi_train.T @ y_train\n            coeffs = np.linalg.solve(A, b)\n        except np.linalg.LinAlgError:\n            # Failsafe for singular matrix, though unlikely with ridge\n            continue \n\n        # Validation\n        g_val = phi_val @ coeffs\n        \n        # Predictions: sign(g), with sign(0) -> +1\n        # (g_val >= 0) -> bool, * 2 - 1 maps True to 1 and False to -1\n        y_pred = (g_val >= 0) * 2 - 1\n        \n        accuracy = np.mean(y_pred == y_val)\n        \n        if accuracy >= tau:\n            return m\n            \n    return -1\n```", "answer": "[3,2,5,4]", "id": "3116684"}]}