{"hands_on_practices": [{"introduction": "在机器学习中，一个常见的陷阱是过拟合，即模型对训练数据学习得“太好”，以至于失去了对新数据的泛化能力。$k=1$的最近邻分类器是过拟合的一个典型例子，因为它完美地记住了训练集，导致其训练误差（或称“重代入误差”）总是为零。本练习将指导你亲手计算这种具有欺骗性的完美训练误差，并将其与更真实的留一法交叉验证误差进行比较，从而深刻理解为何需要可靠的验证方法来评估模型性能。[@problem_id:3135589]", "problem": "给定一个分类问题，其中有一个有限的训练数据集，每个观测值都是一个对 $(\\mathbf{x}_i, y_i)$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\{0,1\\}$ 是类别标签，适用于 $i = 1, \\dots, n$。基本构成包括：(i) 欧几里得距离的定义 $d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$，(ii) $k$-近邻规则，即选择与查询点距离最小的 $k$ 个训练观测值，(iii) 在 $0$-$1$ 损失下定义的经验风险（重代入误差），即当使用指定分类器和完整训练集预测每个训练点时，错分的平均数量，以及 (iv) 留一法交叉验证，通过对每个训练点，使用移除了该点的训练集对其进行预测，并对所得的 $0$-$1$ 损失求平均来评估分类器。在所有计算中，距离的平局必须确定性地打破：当按距离对邻居排序时，按距离升序排序，如果查询点本身存在，则优先考虑该点，最后按索引升序打破任何剩余的平局。对于 $k \\ge 2$ 的分类中的多数投票，如果 $k$ 个邻居的类别计数出现平局，则选择最小的类别标签。\n\n任务A：从上述定义出发，确认当每个训练点的邻居集包含该点本身时，$k=1$ 近邻分类器的经验风险（重代入误差）为 $0$。然后，计算 $k=1$ 的留一法误差，并将其与下面指定的数据集上 $k=2$ 的重代入误差进行比较。比较必须基于误差率的数值。\n\n任务B：基于观察到的数值比较，分析其对使用重代入误差与留一法误差在 $k=1$ 和 $k=2$ 之间进行模型选择的影响，将您的讨论建立在所提供的定义之上（您的分析应在解决方案中呈现，而不是由代码打印）。\n\n使用以下测试数据集套件，每个数据集由一个特征矩阵和一个标签向量给出。所有特征都是二维的，$d = 2$，所有类别标签都在 $\\{0,1\\}$ 中。距离为欧几里得距离。不涉及物理单位。\n\n- 数据集 $1$ (类别分离良好):\n  - 特征 $X_1$: $\\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  1.0 \\\\ 5.0  5.0 \\\\ 5.0  6.0 \\end{array}\\right]$\n  - 标签 $y_1$: $[0, 0, 1, 1]$\n- 数据集 $2$ (边界附近有重叠):\n  - 特征 $X_2$: $\\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  0.1 \\\\ 0.05  0.0 \\end{array}\\right]$\n  - 标签 $y_2$: $[0, 0, 1]$\n- 数据集 $3$ (混合一个噪声点):\n  - 特征 $X_3$: $\\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  2.0 \\\\ 2.0  0.0 \\\\ 2.0  2.0 \\\\ 1.1  1.0 \\end{array}\\right]$\n  - 标签 $y_3$: $[0, 0, 1, 1, 0]$\n\n对于每个数据集 $j \\in \\{1,2,3\\}$，计算：\n- $E_{j}^{\\text{train}, k=1}$：$k=1$ 时的经验风险（重代入误差），邻居中包含该点本身，\n- $E_{j}^{\\text{LOO}, k=1}$：$k=1$ 时的留一法误差，不包含该点本身，\n- $E_{j}^{\\text{train}, k=2}$：$k=2$ 时的经验风险（重代入误差），邻居中包含该点本身，采用多数投票和指定的平局打破规则。\n\n您的程序应生成单行输出，其中包含一个由三个列表组成的列表，每个数据集一个列表，每个内部列表为 $[E_{j}^{\\text{train}, k=1}, E_{j}^{\\text{LOO}, k=1}, E_{j}^{\\text{train}, k=2}]$。格式必须是方括号括起来的逗号分隔列表，例如：“[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]”。所有值都必须是 $[0,1]$ 范围内的浮点数，表示在指定评估设置下训练点的平均 $0$-$1$ 损失。", "solution": "评估问题陈述的有效性。\n\n### 第1步：提取已知条件\n- **数据集**：一个有限的训练数据集，包含 $n$ 个观测值，每个观测值都是一个对 $(\\mathbf{x}_i, y_i)$，其中 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 是特征向量，$y_i \\in \\{0,1\\}$ 是类别标签，适用于 $i = 1, \\dots, n$。\n- **距离度量**：欧几里得距离，$d(\\mathbf{x}, \\mathbf{z}) = \\|\\mathbf{x} - \\mathbf{z}\\|_2$。\n- **分类器规则**：$k$-近邻 ($k$-NN) 规则选择与查询点距离最小的 $k$ 个训练观测值。\n- **误差度量**：\n    - **经验风险（重代入误差）**在 $0$-$1$ 损失下是当使用在完整训练集上训练的分类器预测每个训练点时的平均错分类数。\n    - **留一法交叉验证 (LOOCV) 误差**是平均 $0$-$1$ 损失，其中每个训练点都使用在所有其他点集合上训练的分类器进行预测。\n- **平局打破规则**：\n    - **距离**：按距离升序排列邻居。如果距离相等，则优先考虑查询点本身（如果存在）。任何剩余的平局按索引 $i$ 升序打破。\n    - **多数投票**：对于 $k \\ge 2$ 的分类，如果 $k$ 个邻居的类别计数出现平局，则选择最小的类别标签（即类别 $0$）。\n- **任务**：\n    - **任务 A**：确认 $k=1$ 的经验风险为 $0$。为三个给定的数据集计算 $k=1$ 的 LOOCV 误差和 $k=2$ 的经验风险。\n    - **任务 B**：分析数值结果对模型选择的影响。\n- **数据集**：提供了三个数据集，每个数据集的 $d=2$ 且 $y_i \\in \\{0,1\\}$。\n    - 数据集 $1$：$X_1 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  1.0 \\\\ 5.0  5.0 \\\\ 5.0  6.0 \\end{array}\\right]$, $y_1 = [0, 0, 1, 1]$。\n    - 数据集 $2$：$X_2 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  0.1 \\\\ 0.05  0.0 \\end{array}\\right]$, $y_2 = [0, 0, 1]$。\n    - 数据集 $3$：$X_3 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  2.0 \\\\ 2.0  0.0 \\\\ 2.0  2.0 \\\\ 1.1  1.0 \\end{array}\\right]$, $y_3 = [0, 0, 1, 1, 0]$。\n- **所需计算**：对于每个数据集 $j \\in \\{1,2,3\\}$，计算 $E_{j}^{\\text{train}, k=1}$、$E_{j}^{\\text{LOO}, k=1}$ 和 $E_{j}^{\\text{train}, k=2}$。\n- **输出格式**：一个包含三个列表的列表：`[[e11,e12,e13],[e21,e22,e23],[e31,e32,e33]]`。\n\n### 第2步：使用提取的已知条件进行验证\n- **科学依据**：问题基于统计学习和模式识别中的基本和标准概念，即 $k$-NN 算法、欧几里得距离以及标准的误差估计方法（重代入和交叉验证）。所有定义都是正确和标准的。\n- **适定性**：问题是适定的。它提供了所有必要的数据、明确的算法和确定性的平局打破规则，确保存在唯一可计算的解。任务是具体和可量化的。\n- **客观性**：问题以精确、客观和形式化的数学语言陈述，没有主观性或歧义。\n\n该问题不存在科学上不健全、不完整、矛盾或模棱两可等任何缺陷。这是统计学习领域中一个定义明确的计算和分析练习。\n\n### 第3步：结论与行动\n问题有效。将提供一个合理的解决方案。\n\n---\n\n### 任务A：确认与计算\n\n**第1部分：确认k=1时的经验风险为零**\n\n分类器的经验风险，或称重代入误差，是通过将分类器应用于训练集中的每个点 $\\mathbf{x}_i$，并根据已知标签 $y_i$ 衡量预测误差来计算的。对于 $k=1$ 近邻分类器，我们必须在完整的训练集 $\\{(\\mathbf{x}_j, y_j)\\}_{j=1}^n$ 中找到距离 $\\mathbf{x}_i$ 最近的单个点。\n\n根据定义，一个点到其自身的欧几里得距离为 $d(\\mathbf{x}_i, \\mathbf{x}_i) = 0$。对于任何其他不同的点 $\\mathbf{x}_j$（其中 $j \\ne i$），距离 $d(\\mathbf{x}_i, \\mathbf{x}_j)$ 必须大于 $0$。因此，任何训练点 $\\mathbf{x}_i$ 的最近邻总是点 $\\mathbf{x}_i$ 本身。问题的平局打破规则明确规定，如果查询点本身存在，则优先考虑该点，这加强了这一结论。\n\n因此，对 $\\mathbf{x}_i$ 标签的 $1$-NN 预测，表示为 $\\hat{y}_i$，是其最近邻的标签，即 $y_i$。此预测的 $0$-$1$ 损失为 $L_{0-1}(\\hat{y}_i, y_i) = L_{0-1}(y_i, y_i) = 0$。由于对训练集中的每个点的预测都是正确的，所以总误差数为 $0$。经验风险作为平均损失，因此为 $E^{\\text{train}, k=1} = \\frac{1}{n} \\sum_{i=1}^n 0 = 0$。这对于所有训练点都唯一的数据集普遍适用。因此，对于所有数据集 $j \\in \\{1,2,3\\}$，$E_{j}^{\\text{train}, k=1} = 0$。\n\n**第2部分：数值计算**\n\n我们现在为每个数据集计算 $E_{j}^{\\text{LOO}, k=1}$ 和 $E_{j}^{\\text{train}, k=2}$。\n\n**数据集 1**：$X_1 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  1.0 \\\\ 5.0  5.0 \\\\ 5.0  6.0 \\end{array}\\right]$，$y_1 = [0, 0, 1, 1]$，$n=4$。\n- **$E_{1}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：在 $\\{\\mathbf{x}_2, \\mathbf{x}_3, \\mathbf{x}_4\\}$ 中最近的点是 $\\mathbf{x}_2$（距离 $1.0$）。预测 $\\hat{y}_1=y_2=0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 1.0)$，$y_2=0$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_3, \\mathbf{x}_4\\}$ 中最近的点是 $\\mathbf{x}_1$（距离 $1.0$）。预测 $\\hat{y}_2=y_1=0$。正确。\n    - 对于 $\\mathbf{x}_3=(5.0, 5.0)$，$y_3=1$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_4\\}$ 中最近的点是 $\\mathbf{x}_4$（距离 $1.0$）。预测 $\\hat{y}_3=y_4=1$。正确。\n    - 对于 $\\mathbf{x}_4=(5.0, 6.0)$，$y_4=1$：在 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\mathbf{x}_3\\}$ 中最近的点是 $\\mathbf{x}_3$（距离 $1.0$）。预测 $\\hat{y}_4=y_3=1$。正确。\n    总误差：$0$。$E_{1}^{\\text{LOO}, k=1} = 0/4 = 0.0$。\n- **$E_{1}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：邻居是其自身和 $\\mathbf{x}_2$。标签为 $\\{0, 0\\}$。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 1.0)$，$y_2=0$：邻居是其自身和 $\\mathbf{x}_1$。标签为 $\\{0, 0\\}$。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(5.0, 5.0)$，$y_3=1$：邻居是其自身和 $\\mathbf{x}_4$。标签为 $\\{1, 1\\}$。多数投票结果为 $1$。正确。\n    - 对于 $\\mathbf{x}_4=(5.0, 6.0)$，$y_4=1$：邻居是其自身和 $\\mathbf{x}_3$。标签为 $\\{1, 1\\}$。多数投票结果为 $1$。正确。\n    总误差：$0$。$E_{1}^{\\text{train}, k=2} = 0/4 = 0.0$。\n\n数据集1的结果：$[0.0, 0.0, 0.0]$。\n\n**数据集 2**：$X_2 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  0.1 \\\\ 0.05  0.0 \\end{array}\\right]$，$y_2 = [0, 0, 1]$，$n=3$。\n- **$E_{2}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：到 $\\mathbf{x}_2, \\mathbf{x}_3$ 的距离为 $0.1, 0.05$。最近的是 $\\mathbf{x}_3$。预测 $\\hat{y}_1=y_3=1$。错误。\n    - 对于 $\\mathbf{x}_2=(0.0, 0.1)$，$y_2=0$：到 $\\mathbf{x}_1, \\mathbf{x}_3$ 的距离为 $0.1, \\sqrt{0.05^2+0.1^2} \\approx 0.112$。最近的是 $\\mathbf{x}_1$。预测 $\\hat{y}_2=y_1=0$。正确。\n    - 对于 $\\mathbf{x}_3=(0.05, 0.0)$，$y_3=1$：到 $\\mathbf{x}_1, \\mathbf{x}_2$ 的距离为 $0.05, \\sqrt{0.05^2+0.1^2} \\approx 0.112$。最近的是 $\\mathbf{x}_1$。预测 $\\hat{y}_3=y_1=0$。错误。\n    总误差：$2$。$E_{2}^{\\text{LOO}, k=1} = 2/3$。\n- **$E_{2}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0.0, 0.0)$，$y_1=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_3$（标签 $1$）。平局。预测为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0.0, 0.1)$，$y_2=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_1$（标签 $0$）。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(0.05, 0.0)$，$y_3=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_1$（标签 $0$）。平局。预测为 $0$。错误。\n    总误差：$1$。$E_{2}^{\\text{train}, k=2} = 1/3$。\n\n数据集2的结果：$[0.0, 2/3, 1/3]$。\n\n**数据集 3**：$X_3 = \\left[\\begin{array}{cc} 0.0  0.0 \\\\ 0.0  2.0 \\\\ 2.0  0.0 \\\\ 2.0  2.0 \\\\ 1.1  1.0 \\end{array}\\right]$，$y_3=[0, 0, 1, 1, 0]$，$n=5$。\n- **$E_{3}^{\\text{LOO}, k=1}$**：\n    - 对于 $\\mathbf{x}_1=(0,0), y_1=0$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{1.1^2+1^2} \\approx 1.487$）。预测 $\\hat{y}_1=y_5=0$。正确。\n    - 对于 $\\mathbf{x}_2=(0,2), y_2=0$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{1.1^2+1^2} \\approx 1.487$）。预测 $\\hat{y}_2=y_5=0$。正确。\n    - 对于 $\\mathbf{x}_3=(2,0), y_3=1$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{0.9^2+1^2} \\approx 1.345$）。预测 $\\hat{y}_3=y_5=0$。错误。\n    - 对于 $\\mathbf{x}_4=(2,2), y_4=1$：最近的是 $\\mathbf{x}_5$（距离 $\\sqrt{0.9^2+1^2} \\approx 1.345$）。预测 $\\hat{y}_4=y_5=0$。错误。\n    - 对于 $\\mathbf{x}_5=(1.1,1), y_5=0$：最近的是 $\\mathbf{x}_3$（距离 $\\approx 1.345$）。预测 $\\hat{y}_5=y_3=1$。错误。\n    总误差：$3$。$E_{3}^{\\text{LOO}, k=1} = 3/5 = 0.6$。\n- **$E_{3}^{\\text{train}, k=2}$**：\n    - 对于 $\\mathbf{x}_1=(0,0), y_1=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_5$（标签 $0$）。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_2=(0,2), y_2=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_5$（标签 $0$）。多数投票结果为 $0$。正确。\n    - 对于 $\\mathbf{x}_3=(2,0), y_3=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_5$（标签 $0$）。平局。预测为 $0$。错误。\n    - 对于 $\\mathbf{x}_4=(2,2), y_4=1$：邻居是其自身（标签 $1$）和 $\\mathbf{x}_5$（标签 $0$）。平局。预测为 $0$。错误。\n    - 对于 $\\mathbf{x}_5=(1.1,1), y_5=0$：邻居是其自身（标签 $0$）和 $\\mathbf{x}_3$（标签 $1$）。平局。预测为 $0$。正确。\n    总误差：$2$。$E_{3}^{\\text{train}, k=2} = 2/5 = 0.4$。\n\n数据集3的结果：$[0.0, 0.6, 0.4]$。\n\n### 任务B：对模型选择影响的分析\n\n计算出的误差率如下：\n- 数据集 $1$：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.0, 0.0]$\n- 数据集 $2$：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.667, 0.333]$\n- 数据集 $3$：$[E^{\\text{train}, k=1}, E^{\\text{LOO}, k=1}, E^{\\text{train}, k=2}] = [0.0, 0.6, 0.4]$\n\n这些结果揭示了误差估计方法的关键特性及其对模型选择的影响，特别是对选择超参数 $k$ 的影响。\n\n1.  **重代入误差过于乐观，是一个糟糕的度量标准**：正如分析确认和数值显示的那样，$k=1$ 时的重代入误差 $E^{\\text{train}, k=1}$ 始终为 $0$。如果实践者通过最小化重代入误差来选择 $k$，他们将总是选择 $k=1$，因为它能获得满分。这个选择对应于一个训练误差为零的模型，因为它已经记住了训练数据。这种现象是过拟合的一个典型例子。该模型方差高，对包括噪声在内的每一个数据点都很敏感，但其训练误差对其在新未见数据上的性能给出了误导性的乐观评估。\n\n2.  **留一法交叉验证提供了更现实的评估**：留一法误差 $E^{\\text{LOO}, k=1}$ 提供了对泛化误差更诚实（偏差更小）的估计。对于类别完全分离的数据集1，LOOCV 正确地识别出 $1$-NN 分类器表现完美（$E_{1}^{\\text{LOO}, k=1}=0.0$）。然而，对于分别具有类别重叠和噪声点的数据集2和3，$k=1$ 的 LOOCV 误差相当大（分别为 $0.667$ 和 $0.6$）。这个高误差正确地表明 $k=1$ 模型不鲁棒且过拟合了训练数据。在噪声区域，一个点的单个最近邻很可能来自错误的类别，导致 LOOCV 报告高误差。\n\n3.  **比较模型复杂度**：选择 $k$ 涉及一个偏差-方差权衡。小的 $k$（如 $k=1$）会导致一个低偏差、高方差的模型，其决策边界非常复杂。更大的 $k$ 会增加偏差但减小方差，从而产生一个更平滑、更不复杂的决策边界。在数据集2和3中，LOOCV 下 $k=1$ 的严重性能下降表明其高方差导致了差的泛化能力。\n\n4.  **对 $E^{\\text{train}, k=2}$ 的解释**：对于有噪声的数据集，$k=2$ 的重代入误差不为零。这是因为预测是两个点的平均结果。例如，在数据集3中，当分类点 $\\mathbf{x}_3$（标签为1）时，它的邻居是其自身（标签为1）和噪声点 $\\mathbf{x}_5$（标签为0）。由此产生的平局通过选择标签0来打破，即使在训练数据上也导致了错分类。虽然 $E^{\\text{train}, k=2}$ 仍然是一个乐观的有偏估计，但它比空洞的 $E^{\\text{train}, k=1}=0$ 更有信息量。问题所要求的比较，$E^{\\text{LOO}, k=1}$ 与 $E^{\\text{train}, k=2}$ 的比较，在方法论上并不是在 $k=1$ 和 $k=2$ 之间进行选择的可靠方式，因为它比较的是一个模型的近无偏误差估计和另一个模型的有偏估计。一个恰当的比较应该在 $E^{\\text{LOO}, k=1}$ 和 $E^{\\text{LOO}, k=2}$ 之间进行。尽管如此，结果（$0.667$ vs $0.333$ 和 $0.6$ vs $0.4$）说明了一个关键点：一个不稳定的模型（$k=1$）可能具有非常高的泛化误差，甚至可能高于一个更稳定模型（$k=2$）的（乐观的）训练误差。\n\n总之，在 k-NN 的模型选择中，重代入误差是一个有缺陷的度量标准，特别是对于小的 $k$。它系统性地偏爱过拟合的模型。像 LOOCV 这样的交叉验证方法为模型的泛化能力提供了更可靠的估计，并且对于调整像 $k$ 这样的超参数至关重要。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes k-NN error rates for three datasets as specified in the problem.\n    \"\"\"\n    datasets = [\n        (\n            np.array([[0.0, 0.0], [0.0, 1.0], [5.0, 5.0], [5.0, 6.0]]),\n            np.array([0, 0, 1, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 0.1], [0.05, 0.0]]),\n            np.array([0, 0, 1])\n        ),\n        (\n            np.array([[0.0, 0.0], [0.0, 2.0], [2.0, 0.0], [2.0, 2.0], [1.1, 1.0]]),\n            np.array([0, 0, 1, 1, 0])\n        )\n    ]\n\n    all_results = []\n\n    for X, y in datasets:\n        n, d = X.shape\n        \n        # Task 1: Empirical risk for k=1 (E_train, k=1)\n        # This is analytically guaranteed to be 0, as each point is its own nearest neighbor.\n        e_train_k1 = 0.0\n        \n        # Task 2: Leave-one-out error for k=1 (E_LOO, k=1)\n        loo_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            min_dist = np.inf\n            best_idx = -1\n            \n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                \n                if dist  min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j  best_idx:\n                        best_idx = j\n            \n            y_pred = y[best_idx]\n            if y_pred != y_true:\n                loo_errors += 1\n        \n        e_loo_k1 = loo_errors / n\n\n        # Task 3: Empirical risk for k=2 (E_train, k=2)\n        train_k2_errors = 0\n        for i in range(n):\n            x_query = X[i]\n            y_true = y[i]\n            \n            # The first neighbor is the point itself.\n            neighbor1_label = y_true\n            \n            # Find the second neighbor (closest among other points).\n            min_dist = np.inf\n            best_idx = -1\n            for j in range(n):\n                if i == j:\n                    continue\n                \n                dist = np.linalg.norm(x_query - X[j])\n                if dist  min_dist:\n                    min_dist = dist\n                    best_idx = j\n                elif dist == min_dist:\n                    # Tie-breaking by ascending index\n                    if j  best_idx:\n                        best_idx = j\n            \n            neighbor2_label = y[best_idx]\n            \n            # Majority vote with tie-breaking\n            if neighbor1_label == neighbor2_label:\n                y_pred = neighbor1_label\n            else:  # Tie in votes (one for each class)\n                y_pred = 0  # Select smallest class label\n                \n            if y_pred != y_true:\n                train_k2_errors += 1\n        \n        e_train_k2 = train_k2_errors / n\n        \n        all_results.append([e_train_k1, e_loo_k1, e_train_k2])\n\n    # Format the final output string as specified.\n    outer_list_str = []\n    for res in all_results:\n        inner_list_str = f\"[{','.join(map(str, res))}]\"\n        outer_list_str.append(inner_list_str)\n    \n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```", "id": "3135589"}, {"introduction": "在现实世界的分类问题中，并非所有错误都生而平等。例如，在医疗诊断中，将重症患者误诊为健康（假阴性）的代价远高于将健康人误判为患者（假阳性）。本练习挑战你将标准的k-NN算法扩展到这种成本敏感的场景。你将首先从风险最小化的基本原则出发，推导出新的决策规则，然后实现一个能够权衡不同类别错分成本的k-NN分类器。[@problem_id:3135604]", "problem": "给定一个二元和多类分类问题场景，其中错分一个样本会产生一定的成本。考虑一个特征向量 $\\mathbf{x} \\in \\mathbb{R}^d$ 和一个类别标签 $y \\in \\mathcal{Y} = \\{0,1,\\dots, K-1\\}$。设错分损失是与类别相关的：预测 $\\hat{y} \\neq y$ 会产生 $C(y)$ 的成本，而正确分类的成本为零。假设对所有 $y \\in \\mathcal{Y}$ 都有 $C(y) \\ge 0$，并且 $C(y)$ 是已知的。目标是设计一个成本敏感的 k-近邻 (k-NN) 分类器，该分类器与最小化期望条件风险的目标一致，并在特定场景下对其进行测试。\n\n任务：\n1) 从分类风险和损失的基本定义出发，推导出在上述类别相关的错分损失下，使点 $\\mathbf{x}$ 处的期望条件风险最小化的决策条件。您的推导应从期望条件风险的定义 $R(\\hat{y} \\mid \\mathbf{x}) = \\mathbb{E}[L(\\hat{y}, Y) \\mid \\mathbf{x}]$ 和给定的损失结构开始，不使用任何预先推导的快捷公式。\n2) 实现一个 k-近邻分类器，使其在与您在第 ($1$) 部分中推导的决策条件一致的意义上是成本敏感的。在 $\\mathbb{R}^d$ 中使用欧几里得距离。假设类别标签是从 0 开始的整数，并且成本向量 $C$ 是一个实值数组，其中 $C(j)$ 位于位置 $j$。如果在最终决策分数上出现多个类别平局，则通过选择最小的类别标签（即最小的整数）来打破平局。\n3) 将您的实现应用于以下独立的测试案例。对于每个案例，计算单个查询点的预测类别。所有点都在 $\\mathbb{R}^2$ 中（无单位），成本也无单位：\n- 案例 1（基准，成本相等）：\n  - 训练输入：(($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - 训练标签：($0$, $0$, $1$, $1$)\n  - 查询点：($0.1$, $0.1$)\n  - $k = 3$\n  - 成本 $C$：($1.0$, $1.0$)\n- 案例 2（成本偏斜导致决策反转）：\n  - 训练输入：(($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - 训练标签：($0$, $0$, $1$, $1$)\n  - 查询点：($0.1$, $0.1$)\n  - $k = 3$\n  - 成本 $C$：($1.0$, $4.0$)\n- 案例 3（多类，三个类别具有不同成本）：\n  - 训练输入：(($0.0$, $0.0$), ($2.0$, $0.0$), ($0.0$, $2.0$))\n  - 训练标签：($0$, $1$, $2$)\n  - 查询点：($0.9$, $0.9$)\n  - $k = 3$\n  - 成本 $C$：($1.0$, $2.0$, $3.0$)\n- 案例 4（边界情况 $k = 1$）：\n  - 训练输入：(($0.0$, $0.0$), ($0.0$, $1.0$), ($1.0$, $0.0$), ($1.0$, $1.0$))\n  - 训练标签：($0$, $0$, $1$, $1$)\n  - 查询点：($0.05$, $0.05$)\n  - $k = 1$\n  - 成本 $C$：($1.0$, $100.0$)\n- 案例 5（通过加权平局，用 $k = 3$ 再现偶数 $k$ 类似的平局行为）：\n  - 训练输入：(($0.0$, $0.0$), ($1.0$, $0.0$), ($2.0$, $0.0$))\n  - 训练标签：($0$, $1$, $1$)\n  - 查询点：($0.9$, $0.0$)\n  - $k = 3$\n  - 成本 $C$：($2.0$, $1.0$)\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。具体来说，按顺序打印案例 1 到 5 的预测类别列表，例如，“[$a_1$, $a_2$, $a_3$, $a_4$, $a_5$]”，其中每个 $a_i$ 是案例 $i$ 的整数预测值。", "solution": "该问题是有效的，因为它在科学上基于统计决策理论，设定完整且一致，并且在计算上是可行的。我们将继续提供完整的解决方案。\n\n解决方案分为两部分。首先，我们推导出在指定的类别相关错分成本下最小化期望条件风险的最优决策规则。其次，我们构建一个实现此决策规则的k-近邻（k-NN）分类器，并将其应用于给定的测试案例。\n\n### 第1部分：最优决策规则的推导\n\n目标是找到一个决策函数 $\\hat{y}(\\mathbf{x})$，对于任何给定的特征向量 $\\mathbf{x}$，该函数从集合 $\\mathcal{Y} = \\{0, 1, \\dots, K-1\\}$ 中选择一个类别标签 $\\hat{y}$，以最小化期望条件风险。\n\n当真实特征向量为 $\\mathbf{x}$ 时，预测类别为 $\\hat{y}$ 的期望条件风险 $R(\\hat{y} \\mid \\mathbf{x})$ 定义为损失函数 $L(\\hat{y}, Y)$ 在给定 $\\mathbf{X} = \\mathbf{x}$ 的情况下，对真实类别 $Y$ 的条件概率分布的期望。\n令 $p(j \\mid \\mathbf{x}) = P(Y=j \\mid \\mathbf{X}=\\mathbf{x})$ 为类别 $j$ 的真实条件概率。风险为：\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = \\mathbb{E}[L(\\hat{y}, Y) \\mid \\mathbf{X}=\\mathbf{x}] = \\sum_{j=0}^{K-1} L(\\hat{y}, j) p(j \\mid \\mathbf{x})\n$$\n问题指定了与类别相关的错分损失：\n$$\nL(\\hat{y}, j) =\n\\begin{cases}\n0  \\text{if } \\hat{y} = j \\\\\nC(j)  \\text{if } \\hat{y} \\neq j\n\\end{cases}\n$$\n其中 $C(j) \\ge 0$ 是错分一个真正属于类别 $j$ 的实例的成本。\n\n将损失函数代入风险方程，我们可以将预测 $\\hat{y}$ 正确（$j = \\hat{y}$）的情况与不正确（$j \\neq \\hat{y}$）的情况分开：\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = L(\\hat{y}, \\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} L(\\hat{y}, j) p(j \\mid \\mathbf{x})\n$$\n由于 $L(\\hat{y}, \\hat{y}) = 0$ 且对于 $j \\neq \\hat{y}$ 有 $L(\\hat{y}, j) = C(j)$，这可以简化为：\n$$\nR(\\hat{y} \\mid \\mathbf{x}) = 0 \\cdot p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x}) = \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\n贝叶斯最优分类器是选择最小化此风险的类别 $\\hat{y}^*$：\n$$\n\\hat{y}^* = \\arg\\min_{\\hat{y} \\in \\mathcal{Y}} R(\\hat{y} \\mid \\mathbf{x}) = \\arg\\min_{\\hat{y} \\in \\mathcal{Y}} \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\n为了使这个最小化问题更易于处理，我们可以重写这个和。所有可能真实类别的总期望错分成本是 $\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x})$。我们可以将这个和表示为：\n$$\n\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x}) = C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) + \\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x})\n$$\n重新整理以分离出我们想要最小化的项：\n$$\n\\sum_{j \\neq \\hat{y}} C(j) p(j \\mid \\mathbf{x}) = \\left( \\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x}) \\right) - C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x})\n$$\n括号中的项 $\\sum_{j=0}^{K-1} C(j) p(j \\mid \\mathbf{x})$ 是期望总成本，不依赖于我们选择的预测 $\\hat{y}$。因此，最小化左侧等同于最大化被减去的项 $C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x})$。\n\n因此，最优决策规则是选择使其错分成本和其条件概率的乘积最大化的类别：\n$$\n\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) p(\\hat{y} \\mid \\mathbf{x}) \\right\\}\n$$\n请注意，如果成本是统一的，即对于所有 $y$ 都有 $C(y) = c$（其中 $c > 0$ 是一个常数），此规则简化为 $\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} p(\\hat{y} \\mid \\mathbf{x})$，这是标准的贝叶斯分类器规则，它最小化错误概率（等同于 0-1 损失）。\n\n### 第2部分：成本敏感的k-近邻分类器\n\nk-NN 算法是一种非参数方法，它从训练数据中估计条件概率 $p(j \\mid \\mathbf{x})$。对于给定的查询点 $\\mathbf{x}$，我们首先确定其邻域 $N_k(\\mathbf{x})$，该邻域由根据某个距离度量（此处为欧几里得距离）最接近 $\\mathbf{x}$ 的 $k$ 个训练样本组成。\n\n条件概率 $p(j \\mid \\mathbf{x})$ 的 k-NN 估计是 $N_k(\\mathbf{x})$ 中属于类别 $j$ 的邻居的比例。令 $n_j(\\mathbf{x})$ 为 $N_k(\\mathbf{x})$ 中类别标签为 $j$ 的数据点的数量。那么，估计值 $\\hat{p}(j \\mid \\mathbf{x})$ 为：\n$$\n\\hat{p}(j \\mid \\mathbf{x}) = \\frac{n_j(\\mathbf{x})}{k}\n$$\n其中 $\\sum_{j=0}^{K-1} n_j(\\mathbf{x}) = k$。\n\n将此估计代入我们推导出的最优决策规则中：\n$$\n\\hat{y}^* = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) \\hat{p}(\\hat{y} \\mid \\mathbf{x}) \\right\\} = \\arg\\max_{\\hat{y} \\in \\mathcal{Y}} \\left\\{ C(\\hat{y}) \\frac{n_{\\hat{y}}(\\mathbf{x})}{k} \\right\\}\n$$\n由于 $k$ 是一个相对于 $\\hat{y}$ 的最大化而言的正数常数，我们可以将其从表达式中移除。我们的成本敏感 k-NN 分类器的最终决策规则是为每个类别 $j$ 计算一个分数 $S(j)$，并选择分数最高的类别：\n$$\n\\hat{y}^* = \\arg\\max_{j \\in \\mathcal{Y}} S(j) \\quad \\text{其中} \\quad S(j) = C(j) n_j(\\mathbf{x})\n$$\n如果多个类别具有相同的最高分数，则通过选择具有最小整数标签的类别来打破平局。\n\n该算法的步骤如下：\n1. 对于一个查询点 $\\mathbf{x}_{query}$，计算其到所有训练点 $\\mathbf{x}_i$ 的欧几里得距离。\n2. 识别出距离最小的 $k$ 个训练点。这些是最近邻。\n3. 对于每个类别 $j \\in \\{0, 1, \\dots, K-1\\}$，计算属于该类别的邻居数量 $n_j$。\n4. 对于每个类别 $j$，计算分数 $S(j) = C(j) \\cdot n_j$。\n5. 预测的类别是分数最高的类别。通过选择最小的类别索引来打破平局。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef cost_sensitive_knn_predict(X_train, y_train, x_query, k, costs):\n    \"\"\"\n    Predicts the class for a single query point using a cost-sensitive k-NN classifier.\n\n    Args:\n        X_train (np.ndarray): Training data feature vectors, shape (n_samples, n_features).\n        y_train (np.ndarray): Training data labels, shape (n_samples,).\n        x_query (np.ndarray): The query point feature vector, shape (n_features,).\n        k (int): The number of neighbors to consider.\n        costs (np.ndarray): The cost vector C, where costs[j] is C(j).\n\n    Returns:\n        int: The predicted class label.\n    \"\"\"\n    # 1. Compute Euclidean distances from the query point to all training points.\n    distances = np.linalg.norm(X_train - x_query, axis=1)\n\n    # 2. Find the indices of the k-nearest neighbors.\n    # np.argsort provides a stable sort, which is sufficient for breaking\n    # distance ties implicitly.\n    neighbor_indices = np.argsort(distances)[:k]\n\n    # Get the labels of the k-nearest neighbors.\n    neighbor_labels = y_train[neighbor_indices]\n\n    # 3. For each class, count the number of neighbors belonging to it.\n    num_classes = len(costs)\n    # np.bincount is highly efficient for this counting task.\n    # It requires non-negative integer labels. We ensure there's a bin for every class\n    # even if a class is not present in the neighbors, by setting minlength.\n    n_j = np.bincount(neighbor_labels, minlength=num_classes)\n\n    # 4. Calculate the score S(j) = C(j) * n_j for each class.\n    scores = costs * n_j\n    \n    # 5. The prediction is the class with the maximum score.\n    # np.argmax breaks ties by returning the index of the first occurrence\n    # of the maximum value, which corresponds to the smallest class label\n    # as required by the tie-breaking rule.\n    prediction = np.argmax(scores)\n    \n    return int(prediction)\n\ndef solve():\n    \"\"\"\n    Sets up and solves the 5 test cases provided in the problem statement.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (baseline, equal costs)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.1, 0.1]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 1.0]),\n        },\n        # Case 2 (skewed costs flip the decision)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.1, 0.1]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 4.0]),\n        },\n        # Case 3 (multi-class, three classes with distinct costs)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [2.0, 0.0], [0.0, 2.0]]),\n            \"y_train\": np.array([0, 1, 2]),\n            \"query\": np.array([0.9, 0.9]),\n            \"k\": 3,\n            \"costs\": np.array([1.0, 2.0, 3.0]),\n        },\n        # Case 4 (boundary case k = 1)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]),\n            \"y_train\": np.array([0, 0, 1, 1]),\n            \"query\": np.array([0.05, 0.05]),\n            \"k\": 1,\n            \"costs\": np.array([1.0, 100.0]),\n        },\n        # Case 5 (even-k-like tie behavior reproduced with k = 3 via weighted tie)\n        {\n            \"X_train\": np.array([[0.0, 0.0], [1.0, 0.0], [2.0, 0.0]]),\n            \"y_train\": np.array([0, 1, 1]),\n            \"query\": np.array([0.9, 0.0]),\n            \"k\": 3,\n            \"costs\": np.array([2.0, 1.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        prediction = cost_sensitive_knn_predict(\n            X_train=case[\"X_train\"],\n            y_train=case[\"y_train\"],\n            x_query=case[\"query\"],\n            k=case[\"k\"],\n            costs=case[\"costs\"],\n        )\n        results.append(prediction)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3135604"}, {"introduction": "在k-NN算法中，超参数$k$的选择至关重要，它直接影响着模型的偏倚-方差权衡。那么，如何才能找到最优的$k$值呢？本高级练习将带你探索两种强大的模型选择策略：一种是广泛应用的经验性方法——交叉验证，另一种则是基于统计理论的复杂方法，它利用集中不等式来估计泛化误差的上界。通过实现和比较这两种方法，你将深入了解统计理论与机器学习实践之间的深刻联系。[@problem_id:3135635]", "problem": "给定一个二元分类任务，要求您使用一种有理论依据的程序来选择$k$-近邻分类器的邻居数$k$，并将其与交叉验证选出的值进行比较。该理论程序必须依赖于邻居标签的经验方差估计和一个提供经验均值误差高概率界的集中不等式。假设使用以下标准设置。\n\n基本设置：\n- 设$(X,Y)$为一个随机对，其中$X \\in \\mathbb{R}^d$，$Y \\in \\{0,1\\}$。\n- 对于一个固定的输入$x$，条件类别概率为$p(x) = \\mathbb{E}[Y \\mid X = x]$。\n- $k$-近邻规则通过计算$x$的$k$个最近训练点的标签均值来估计$p(x)$，并通过将此估计值与$0.5$比较来进行分类。\n- 使用针对有界随机变量的经验Bernstein不等式，这是一个经过充分检验的集中界。对于独立样本$Z_1, \\dots, Z_k \\in [0,1]$，其经验均值为$\\hat{\\mu}$，无偏样本方差为$s^2$，该不等式表明，以至少$1 - \\delta$的概率，\n$$\n\\left|\\hat{\\mu} - \\mu\\right| \\leq \\sqrt{\\frac{2 s^2 \\ln\\left(\\frac{3}{\\delta}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta}\\right)}{k - 1},\n$$\n成立，其中$\\mu = \\mathbb{E}[Z_i]$。在我们的设置中，对于每个训练点$x_i$，令$Z_j$为邻居标签，并设置$\\delta_i = \\delta/n$，以便对所有$n$个训练点应用联合界。\n\n您的程序必须实现以下精确步骤。\n\n1. 数据生成：\n   - 对于每个测试用例，按如下方式在$d$维空间中生成$n$个训练点。令$n_0 = \\lfloor n/2 \\rfloor$和$n_1 = n - n_0$为类别大小。类别$0$的点独立地从多元正态分布$\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I}_d)$中抽取，类别$1$的点独立地从$\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I}_d)$中抽取。相应地分配标签$0$和$1$。最近邻计算使用欧氏距离。每个用例的随机数生成器必须使用指定的种子进行初始化，然后用于确定性地生成训练数据。\n   - 不需要单独的测试集；所有评估都通过对训练集进行留一法来执行。\n\n2. 候选$k$值集合：\n   - 对于每个测试用例，评估该用例中给出的所有候选$k$值。对于以下所有逻辑，当$k = 1$时，将经验Bernstein界中的第二项$\\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}$视为$+\\infty$，从而使得当$k=1$时该界是空泛的。\n\n3. 基于经验Bernstein界的选择：\n   - 对于每个训练点$x_i$，计算它在其他训练点（不包括$x_i$本身）中的$k$个最近邻，并令$\\hat{p}_i(k)$为这$k$个邻居标签的经验均值。令$s_i^2(k)$为这些标签的无偏样本方差，\n     $$\n     s_i^2(k) = \\frac{1}{k - 1} \\sum_{j=1}^k \\left(y_{i,j} - \\hat{p}_i(k)\\right)^2,\n     $$\n     其中$y_{i,j}$是邻居标签。定义每点的界半宽\n     $$\n     B_i(k) = \\sqrt{\\frac{2 s_i^2(k) \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}, \\quad \\delta_i = \\frac{\\delta}{n}.\n     $$\n   - 定义模糊性指标\n     $$\n     A_i(k) = \\begin{cases}\n     1  \\text{若 } \\left|\\hat{p}_i(k) - \\frac{1}{2}\\right| \\leq B_i(k), \\\\\n     0  \\text{否则}.\n     \\end{cases}\n     $$\n     直观上，$A_i(k) = 1$表示经验估计$\\hat{p}_i(k)$在该界宽度内并不确定地落在$\\frac{1}{2}$的某一侧。定义基于界的错分率上界\n     $$\n     U(k) = \\frac{1}{n} \\sum_{i=1}^n A_i(k).\n     $$\n   - 选择$k_{\\text{bound}}$为最小化$U(k)$的$k$值。若出现平局，则选择最小的$k$。\n\n4. 交叉验证比较：\n   - 计算留一法误差\n     $$\n     E_{\\text{LOO}}(k) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i^{(-i)}(k) \\neq y_i \\right\\},\n     $$\n     其中$\\hat{y}_i^{(-i)}(k)$是根据除$x_i$之外的$k$个最近邻为$x_i$预测的标签，决策规则为\n     $$\n     \\hat{y}_i^{(-i)}(k) = \\begin{cases}\n     1  \\text{若 } \\hat{p}_i(k) \\geq \\frac{1}{2}, \\\\\n     0  \\text{否则}.\n     \\end{cases}\n     $$\n     选择$k_{\\text{CV}}$为最小化$E_{\\text{LOO}}(k)$的$k$值。若出现平局，则选择最小的$k$。\n\n5. 输出：\n   - 对于每个测试用例，输出四元组$[k_{\\text{bound}}, U(k_{\\text{bound}}), k_{\\text{CV}}, E_{\\text{LOO}}(k_{\\text{CV}})]$。\n   - 将$U(\\cdot)$和$E_{\\text{LOO}}(\\cdot)$表示为$[0,1]$范围内四舍五入到四位小数的小数。\n   - 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，每个测试用例的格式都应严格遵循无空格的方括号四元组形式。例如，两个用例的输出必须类似于$[[1,0.0000,3,0.0500],[2,0.1000,2,0.1000]]$。\n\n测试套件：\n- 用例 1：$n = 60$, $d = 2$, $\\boldsymbol{\\mu}_0 = (-1,-1)$, $\\boldsymbol{\\mu}_1 = (1,1)$, $\\sigma = 0.3$, 候选$k$值$[1,3,5,7,9,11]$, 总置信度参数$\\delta = 0.1$, 随机种子$0$。\n- 用例 2：$n = 100$, $d = 2$, $\\boldsymbol{\\mu}_0 = (-0.5,0)$, $\\boldsymbol{\\mu}_1 = (0.5,0)$, $\\sigma = 0.6$, 候选$k$值$[2,4,6,8,10,12,14]$, $\\delta = 0.05$, 随机种子$1$。\n- 用例 3：$n = 80$, $d = 1$, $\\boldsymbol{\\mu}_0 = (-0.3)$, $\\boldsymbol{\\mu}_1 = (0.3)$, $\\sigma = 0.8$, 候选$k$值$[1,5,15,25,35]$, $\\delta = 0.2$, 随机种子$2$。\n\n所有最近邻计算必须使用欧氏距离，并排除点本身（在训练集内部进行留一法）。确保所有计算都严格遵守上述定义。", "solution": "用户提供的问题已经过分析和验证。该问题在科学上是合理的、适定的且客观的。所有必要信息均已提供，并且程序步骤定义清晰明确。该问题是计算统计学中的一个实质性任务，要求在$k$-近邻算法的背景下，实现一个数据生成过程和两种不同的模型选择方法。未检测到任何缺陷。\n\n解决方案首先通过从双分量高斯混合模型生成用于二元分类问题的合成数据集开始。随后，执行两个程序，从给定的候选集合中选择最优的邻居数$k$。所有评估均在生成的训练数据上使用留一法进行。\n\n### 1. 数据生成和留一法设置\n\n对于每个测试用例，我们在一个$d$维空间$\\mathbb{R}^d$中生成一个包含$n$个点的训练集。数据包含两个类别，标记为$0$和$1$。每个类别中的点数分别为$n_0 = \\lfloor n/2 \\rfloor$和$n_1 = n - n_0$。类别$0$的点从多元正态分布$\\mathcal{N}(\\boldsymbol{\\mu}_0, \\sigma^2 \\mathbf{I}_d)$中抽取，类别$1$的点从$\\mathcal{N}(\\boldsymbol{\\mu}_1, \\sigma^2 \\mathbf{I}_d)$中抽取。参数$n, d, \\boldsymbol{\\mu}_0, \\boldsymbol{\\mu}_1, \\sigma$以及一个随机种子为每个用例指定，以确保确定性可复现。\n\n所有后续计算都基于留一法（LOO）方案。对于每个带标签$y_i$的训练点$x_i$，所有计算都使用其在其他训练点集合$\\{x_j\\}_{j \\neq i}$中的$k$个最近邻来执行。使用的距离度量是欧氏距离。\n\n### 2. 方法1：基于界的选择以确定$k_{\\text{bound}}$\n\n此方法通过最小化一个分类错误率的上界来选择$k$，该上界由一个集中不等式导出。其核心思想是识别并计数“模糊”的分类。如果一个点$x_i$的估计条件类别概率$\\hat{p}_i(k)$与决策边界$0.5$在统计上不可区分，则认为对该点的分类是模糊的。\n\n令$y_{i,1}, \\dots, y_{i,k}$为$x_i$的$k$个最近邻的标签。局部类别概率通过它们的经验均值来估计：\n$$\n\\hat{p}_i(k) = \\frac{1}{k} \\sum_{j=1}^k y_{i,j}\n$$\n为了量化该估计中的统计不确定性，我们使用经验Bernstein不等式。这为真实局部概率$p(x_i) = \\mathbb{E}[Y \\mid X=x_i]$提供了一个高概率置信区间。以至少$1-\\delta_i$的概率，真实均值$\\mu$（这里是$p(x_i)$）被界定为：\n$$\n|\\hat{p}_i(k) - p(x_i)| \\leq B_i(k)\n$$\n其中$B_i(k)$是界半宽。为确保此界以总置信度$1-\\delta$对所有$n$个训练点同时成立，我们通过设置$\\delta_i = \\delta/n$来应用联合界。半宽$B_i(k)$定义为：\n$$\nB_i(k) = \\sqrt{\\frac{2 s_i^2(k) \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k}} + \\frac{3 \\ln\\left(\\frac{3}{\\delta_i}\\right)}{k - 1}\n$$\n这里，$s_i^2(k)$是邻居标签的无偏样本方差：\n$$\ns_i^2(k) = \\frac{1}{k - 1} \\sum_{j=1}^k \\left(y_{i,j} - \\hat{p}_i(k)\\right)^2\n$$\n对于给定的$k$，如果点$x_i$关于$p(x_i)$的置信区间包含了决策边界$0.5$，则该点被声明为模糊的。这由模糊性指标$A_i(k)$捕捉：\n$$\nA_i(k) = \\begin{cases}\n1  \\text{若 } |\\hat{p}_i(k) - 0.5| \\leq B_i(k) \\\\\n0  \\text{否则}\n\\end{cases}\n$$\n选择$k$值以最小化模糊点的总比例$U(k)$：\n$$\nU(k) = \\frac{1}{n} \\sum_{i=1}^n A_i(k)\n$$\n在此标准下的最优$k$是$k_{\\text{bound}} = \\arg\\min_k U(k)$，平局通过选择最小的$k$来打破。对于$k=1$，$s_i^2(1)$未定义。问题规定在这种情况下界是空泛的，即$B_i(1) = \\infty$，这意味着对所有$i$都有$A_i(1) = 1$，因此$U(1)=1$。\n\n### 3. 方法2：用于确定$k_{\\text{CV}}$的留一法交叉验证\n\n这是一种标准的模型选择经验方法。它直接估计分类器在不同$k$值下的泛化误差。\n\n对于每个训练点$x_i$，使用其在其余数据中的$k$个最近邻来进行预测$\\hat{y}_i^{(-i)}(k)$。预测基于邻居间的多数投票，这等同于对估计概率$\\hat{p}_i(k)$进行阈值化处理：\n$$\n\\hat{y}_i^{(-i)}(k) = \\begin{cases}\n1  \\text{若 } \\hat{p}_i(k) \\geq 0.5 \\\\\n0  \\text{否则}\n\\end{cases}\n$$\n对于给定的$k$，留一法误差是被错误分类的点的比例：\n$$\nE_{\\text{LOO}}(k) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\left\\{ \\hat{y}_i^{(-i)}(k) \\neq y_i \\right\\}\n$$\n其中$\\mathbf{1}\\{\\cdot\\}$是指示函数。在此标准下的最优$k$是$k_{\\text{CV}} = \\arg\\min_k E_{\\text{LOO}}(k)$，平局同样通过选择最小的$k$来打破。此方法选择在以留一法方式评估时，在训练数据本身上表现最佳的$k$。\n\n### 4. 算法流程\n\n对每个测试用例，实现遵循以下步骤：\n1.  使用指定的种子初始化伪随机数生成器，并生成训练数据$(X, y)$。\n2.  为提高效率，预先计算$X$中所有点之间的$n \\times n$成对欧氏距离矩阵。\n3.  对于每个候选$k$值：\n    a. 为总模糊性分数和总LOO误差初始化累加器。\n    b. 对于每个点$x_i$，$i=1, \\dots, n$：\n        i. 使用预先计算的距离矩阵，识别其$k$个最近邻的索引，排除$x_i$本身。\n        ii. 检索这些邻居的标签，并计算$\\hat{p}_i(k)$和$s_i^2(k)$。\n        iii. 计算LOO预测$\\hat{y}_i^{(-i)}(k)$，如果$\\hat{y}_i^{(-i)}(k) \\neq y_i$，则更新LOO误差计数。\n        iv. 计算Bernstein界半宽$B_i(k)$（单独处理$k=1$的情况），并确定模糊性指标$A_i(k)$，更新模糊性分数。\n    c. 计算当前$k$的最终分数$U(k)$和$E_{\\text{LOO}}(k)$。\n4.  在评估所有候选$k$值后，通过找到最小化$U(k)$的$k$来确定$k_{\\text{bound}}$，通过找到最小化$E_{\\text{LOO}}(k)$的$k$来确定$k_{\\text{CV}}$。按规定应用平局打破规则。\n5.  将最终结果格式化为四元组$[k_{\\text{bound}}, U(k_{\\text{bound}}), k_{\\text{CV}}, E_{\\text{LOO}}(k_{\\text{CV}})]$，其中比率值四舍五入到四位小数。", "answer": "```python\nimport numpy as np\n\ndef run_one_case(n, d, mu_0, mu_1, sigma, k_values, delta, seed):\n    \"\"\"\n    Executes a single test case according to the problem description.\n\n    Args:\n        n (int): Total number of training points.\n        d (int): Dimension of the feature space.\n        mu_0 (list or float): Mean vector for class 0.\n        mu_1 (list or float): Mean vector for class 1.\n        sigma (float): Standard deviation for the isotropic covariance.\n        k_values (list): List of candidate k values to evaluate.\n        delta (float): Total confidence parameter for Bernstein bound.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A tuple containing (k_bound, U(k_bound), k_CV, E_LOO(k_CV)).\n    \"\"\"\n    # 1. Data Generation\n    rng = np.random.default_rng(seed)\n    n_0 = n // 2\n    n_1 = n - n_0\n\n    if d == 1:\n        # np.multivariate_normal expects mean to be 1-D array-like\n        mu_0_arr = [mu_0]\n        mu_1_arr = [mu_1]\n    else:\n        mu_0_arr = mu_0\n        mu_1_arr = mu_1\n\n    cov = (sigma**2) * np.identity(d)\n    X_0 = rng.multivariate_normal(mu_0_arr, cov, size=n_0)\n    X_1 = rng.multivariate_normal(mu_1_arr, cov, size=n_1)\n\n    X = np.vstack((X_0, X_1))\n    if d == 1 and X.ndim > 1 and X.shape[1] != d:\n        X = X.reshape(n, d)\n    y = np.array([0] * n_0 + [1] * n_1)\n\n    # 2. Pre-compute pairwise Euclidean distances\n    sum_sq = np.sum(X**2, axis=1, keepdims=True)\n    dist_sq = sum_sq + sum_sq.T - 2 * np.dot(X, X.T)\n    dist_sq[dist_sq  0] = 0  # Correct for numerical floating point inaccuracies\n    distances = np.sqrt(dist_sq)\n\n    results_per_k = []\n    delta_i = delta / n\n    log_term = np.log(3 / delta_i)\n\n    # 3. Iterate over candidate k values\n    for k in k_values:\n        total_A_i = 0\n        total_loo_errors = 0\n\n        # 4. Leave-one-out evaluation for each point\n        for i in range(n):\n            dists_i = distances[i, :]\n            sorted_indices = np.argsort(dists_i)\n            # Exclude self (which is at index 0)\n            neighbor_indices = sorted_indices[1:k + 1]\n            neighbor_labels = y[neighbor_indices]\n\n            p_hat = np.mean(neighbor_labels)\n            \n            # --- Cross-Validation Error Calculation ---\n            y_hat = 1 if p_hat >= 0.5 else 0\n            if y_hat != y[i]:\n                total_loo_errors += 1\n\n            # --- Bernstein Bound-based Ambiguity Calculation ---\n            if k == 1:\n                A_i = 1 # Per problem spec, bound is vacuous, ambiguity is 1.\n            else:\n                s_squared = np.var(neighbor_labels, ddof=1) if k > 1 else 0\n                \n                # handle s_squared is nan when all labels are same\n                if np.isnan(s_squared):\n                    s_squared = 0\n\n                term1 = np.sqrt(2 * s_squared * log_term / k)\n                term2 = 3 * log_term / (k - 1)\n                B_i = term1 + term2\n                \n                if np.abs(p_hat - 0.5) = B_i:\n                    A_i = 1\n                else:\n                    A_i = 0\n            \n            total_A_i += A_i\n\n        U_k = total_A_i / n\n        E_LOO_k = total_loo_errors / n\n        results_per_k.append({'k': k, 'U': U_k, 'E_LOO': E_LOO_k})\n\n    # 5. Select best k based on U(k) and E_LOO(k)\n    best_bound_res = sorted(results_per_k, key=lambda x: (x['U'], x['k']))[0]\n    k_bound = best_bound_res['k']\n    U_at_k_bound = best_bound_res['U']\n\n    best_cv_res = sorted(results_per_k, key=lambda x: (x['E_LOO'], x['k']))[0]\n    k_cv = best_cv_res['k']\n    E_LOO_at_k_cv = best_cv_res['E_LOO']\n\n    return k_bound, U_at_k_bound, k_cv, E_LOO_at_k_cv\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'n': 60, 'd': 2, 'mu_0': [-1,-1], 'mu_1': [1,1], 'sigma': 0.3,\n         'k_values': [1,3,5,7,9,11], 'delta': 0.1, 'seed': 0},\n        {'n': 100, 'd': 2, 'mu_0': [-0.5,0], 'mu_1': [0.5,0], 'sigma': 0.6,\n         'k_values': [2,4,6,8,10,12,14], 'delta': 0.05, 'seed': 1},\n        {'n': 80, 'd': 1, 'mu_0': -0.3, 'mu_1': 0.3, 'sigma': 0.8,\n         'k_values': [1,5,15,25,35], 'delta': 0.2, 'seed': 2},\n    ]\n\n    results_str = []\n    for case in test_cases:\n        k_bound, u_val, k_cv, e_val = run_one_case(\n            case['n'], case['d'], case['mu_0'], case['mu_1'], case['sigma'],\n            case['k_values'], case['delta'], case['seed']\n        )\n        formatted_result = f\"[{k_bound},{u_val:.4f},{k_cv},{e_val:.4f}]\"\n        results_str.append(formatted_result)\n\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3135635"}]}