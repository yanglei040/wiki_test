{"hands_on_practices": [{"introduction": "朴素贝叶斯分类器的核心在于其“朴素”的条件独立性假设。但当特征之间存在关联时，这个假设就会被打破。这个练习通过一个特征被完全复制的极端场景，让你亲手计算并理解朴素贝叶斯是如何“重复计算”证据，从而过度放大其置信度的 [@problem_id:3152503]。这个计算将清晰地揭示模型基本假设与其在现实世界中可能遇到的问题之间的差距。", "problem": "考虑一个二元分类问题，其类别变量为 $Y \\in \\{1,0\\}$，以及一个单一的二元潜在信号 $Z \\in \\{1,0\\}$，该信号通过对抗性复制生成两个观测特征：$X_{1} = Z$ 和 $X_{2} = Z$。类别先验为 $P(Y=1) = \\pi$，其中 $\\pi = 0.3$，因此 $P(Y=0) = 1 - \\pi = 0.7$。潜在信号的条件分布是伯努利分布，其参数为 $P(Z=1 \\mid Y=1) = \\theta_{1}$ 和 $P(Z=1 \\mid Y=0) = \\theta_{0}$，其中 $\\theta_{1} = 0.8$ 且 $\\theta_{0} = 0.4$。你观测到一个新实例 $x = (x_{1}, x_{2})$，其中 $x_{1} = 1$ 且 $x_{2} = 1$。\n\n仅从贝叶斯法则和朴素贝叶斯分类器（NBC）所使用的朴素条件独立性假设的定义出发，推导在 $(X_{1}, X_{2})$ 的真实生成模型下的精确后验对数几率 $\\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)$，以及在 NBC 模型（该模型错误地将 $X_{1}$ 和 $X_{2}$ 视为在给定 $Y$ 时条件独立）下的后验对数几率。然后计算膨胀量\n$$\n\\Delta \\;=\\; \\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} \\;-\\; \\ln\\!\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}}.\n$$\n提供 $\\Delta$ 的数值，并四舍五入到四位有效数字。", "solution": "该问题要求计算膨胀量 $\\Delta$，即对于一个给定的观测值，由朴素贝叶斯分类器（NBC）计算出的后验对数几率与真实后验对数几率之间的差值。观测值为 $x = (x_1, x_2)$，其中 $x_1 = 1$ 且 $x_2 = 1$。\n\n给定观测值 $x$ 时，类别 $Y=1$ 相对于 $Y=0$ 的后验对数几率可从贝叶斯法则推导得出：\n$$\n\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)} = \\frac{P(x \\mid Y=1) P(Y=1)}{P(x \\mid Y=0) P(Y=0)}\n$$\n对等式两边取自然对数，得到对数几率：\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right) = \\ln\\left(\\frac{P(x \\mid Y=1)}{P(x \\mid Y=0)}\\right) + \\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right)\n$$\n右边的第一项是对数似然比，第二项是对数先验几率。类别先验给定为 $P(Y=1) = \\pi = 0.3$，所以 $P(Y=0) = 1-\\pi = 0.7$。对数先验几率项对于真实模型和 NBC 模型都是一个常数：\n$$\n\\ln\\left(\\frac{P(Y=1)}{P(Y=0)}\\right) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\ln\\left(\\frac{0.3}{0.7}\\right)\n$$\n\n首先，我们计算在真实生成模型下的后验对数几率。\n在真实模型中，特征由一个潜在信号 $Z$ 生成，使得 $X_1 = Z$ 且 $X_2 = Z$。对于给定的观测值 $x_1=1$ 和 $x_2=1$，事件 $(X_1=1, X_2=1)$ 与事件 $(Z=1)$ 是相同的。\n因此，在给定类别 $Y=y$ 的情况下，观测到 $x=(1,1)$ 的真实条件似然为：\n$$\nP(x=(1,1) \\mid Y=y)_{\\text{true}} = P(X_1=1, X_2=1 \\mid Y=y) = P(Z=1 \\mid Y=y)\n$$\n问题提供了 $Z$ 的条件分布：\n对于 $y=1$：$P(Z=1 \\mid Y=1) = \\theta_1 = 0.8$。\n对于 $y=0$：$P(Z=1 \\mid Y=0) = \\theta_0 = 0.4$。\n对于观测值 $x=(1,1)$，真实的对数似然比为：\n$$\n\\ln\\left(\\frac{P(x=(1,1) \\mid Y=1)_{\\text{true}}}{P(x=(1,1) \\mid Y=0)_{\\text{true}}}\\right) = \\ln\\left(\\frac{P(Z=1 \\mid Y=1)}{P(Z=1 \\mid Y=0)}\\right) = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\n真实的后验对数几率为：\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}} = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\n$$\n\n接下来，我们计算在朴素贝叶斯分类器（NBC）模型下的后验对数几率。\nNBC 错误地假设特征 $X_1$ 和 $X_2$ 在给定类别 $Y$ 的条件下是条件独立的。因此，似然函数被分解为：\n$$\nP(x=(1,1) \\mid Y=y)_{\\text{NBC}} = P(X_1=1 \\mid Y=y) P(X_2=1 \\mid Y=y)\n$$\n要使用这个公式，我们必须首先求出边际条件概率 $P(X_j=1 \\mid Y=y)$。这些概率由真实的生成过程决定，因为它们代表了 NBC 会从一个无限大的数据集中学习到的参数。我们对潜在变量 $Z$ 进行边缘化：\n$$\nP(X_j=1 \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_j=1, Z=z \\mid Y=y) = \\sum_{z \\in \\{0,1\\}} P(X_j=1 \\mid Z=z, Y=y) P(Z=z \\mid Y=y)\n$$\n生成结构是 $Y \\to Z \\to X_j$，这意味着在给定 $Z$ 的条件下，$X_j$ 与 $Y$ 是条件独立的。因此，$P(X_j=1 \\mid Z=z, Y=y) = P(X_j=1 \\mid Z=z)$。\n因为 $X_j=Z$，我们有 $P(X_j=1 \\mid Z=1) = 1$ 和 $P(X_j=1 \\mid Z=0) = 0$。\n将这些代入，我们得到：\n$$\nP(X_j=1 \\mid Y=y) = 1 \\cdot P(Z=1 \\mid Y=y) + 0 \\cdot P(Z=0 \\mid Y=y) = P(Z=1 \\mid Y=y)\n$$\n所以，所需的边际概率是：\n$P(X_j=1 \\mid Y=1) = P(Z=1 \\mid Y=1) = \\theta_1 = 0.8$。\n$P(X_j=1 \\mid Y=0) = P(Z=1 \\mid Y=0) = \\theta_0 = 0.4$。\n\n在 NBC 假设下，对于 $x=(1,1)$ 的条件似然是：\n$P(x=(1,1) \\mid Y=1)_{\\text{NBC}} = P(X_1=1 \\mid Y=1) P(X_2=1 \\mid Y=1) = \\theta_1 \\cdot \\theta_1 = \\theta_1^2$。\n$P(x=(1,1) \\mid Y=0)_{\\text{NBC}} = P(X_1=1 \\mid Y=0) P(X_2=1 \\mid Y=0) = \\theta_0 \\cdot \\theta_0 = \\theta_0^2$。\nNBC 的对数似然比是：\n$$\n\\ln\\left(\\frac{P(x=(1,1) \\mid Y=1)_{\\text{NBC}}}{P(x=(1,1) \\mid Y=0)_{\\text{NBC}}}\\right) = \\ln\\left(\\frac{\\theta_1^2}{\\theta_0^2}\\right) = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\nNBC 的后验对数几率是：\n$$\n\\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\n$$\n膨胀量 $\\Delta$ 是 NBC 对数几率与真实对数几率之间的差值：\n$$\n\\Delta = \\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{NBC}} - \\ln\\left(\\frac{P(Y=1 \\mid x)}{P(Y=0 \\mid x)}\\right)_{\\text{true}}\n$$\n$$\n\\Delta = \\left(2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\right) - \\left(\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) + \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\right)\n$$\n对数先验几率项被消去，剩下：\n$$\n\\Delta = 2\\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) - \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right) = \\ln\\left(\\frac{\\theta_1}{\\theta_0}\\right)\n$$\n膨胀量代表了 NBC 对来自单一潜在源 $Z$ 的证据进行重复计算的程度，该潜在源被复制成了两个完全相关的特征。\n代入给定的数值 $\\theta_1=0.8$ 和 $\\theta_0=0.4$：\n$$\n\\Delta = \\ln\\left(\\frac{0.8}{0.4}\\right) = \\ln(2)\n$$\n$\\ln(2)$ 的数值约为 $0.693147...$。四舍五入到四位有效数字，我们得到：\n$$\n\\Delta \\approx 0.6931\n$$", "answer": "$$\\boxed{0.6931}$$", "id": "3152503"}, {"introduction": "朴素贝叶斯模型在处理特征之间存在复杂交互作用的问题时会遇到困难。这个练习探讨了经典的“异或”（XOR）问题，在这个问题中，单个特征本身不携带任何分类信息，分类的秘密完全隐藏在特征的组合之中 [@problem_id:3152539]。通过分析模型为何会在此任务上失效，并探索如何通过构建新的“交互特征”来解决这个问题，你将体会到特征工程在机器学习中的强大威力。", "problem": "考虑一个二元分类任务，其类别标签为 $y \\in \\{0,1\\}$，并有两个二元特征 $x_1 \\in \\{0,1\\}$ 和 $x_2 \\in \\{0,1\\}$。总体分布如下：$P(y=1)=P(y=0)=\\tfrac{1}{2}$，并且在给定 $y$ 的条件下，特征对满足\n- 如果 $y=1$，则 $(x_1,x_2)$ 等于 $(0,1)$ 的概率为 $\\tfrac{1}{2}$，等于 $(1,0)$ 的概率为 $\\tfrac{1}{2}$，\n- 如果 $y=0$，则 $(x_1,x_2)$ 等于 $(0,0)$ 的概率为 $\\tfrac{1}{2}$，等于 $(1,1)$ 的概率为 $\\tfrac{1}{2}$。\n假设分类器在一个任意大且具有代表性的样本上训练，因此任何估计的概率都收敛于真实的总体概率。该分类器是一个朴素贝叶斯分类器，它使用贝叶斯定理并结合给定类别下特征条件独立的假设来形成其后验概率。\n\n要求你根据基本定义进行推理：贝叶斯定理 $P(y \\mid \\mathbf{x}) \\propto P(y) P(\\mathbf{x} \\mid y)$，朴素贝叶斯分解 $P(\\mathbf{x} \\mid y) = \\prod_i P(x_i \\mid y)$，以及所述的数据生成分布。基于这些，选择下面所有关于朴素贝叶斯在该分布上的性能以及工程化的交互特征如何弥补其任何不足的正确陈述。这里，对于任何布尔表达式 $E$，用 $\\mathbb{1}[E] \\in \\{0,1\\}$ 表示其指示函数，用 $x_1 \\oplus x_2$ 表示 $x_1$ 和 $x_2$ 的异或。\n\nA. 仅使用特征 $x_1$ 和 $x_2$，朴素贝叶斯在此分布上实现的期望准确率为 $\\tfrac{1}{2}$，无论训练集大小和标准的平滑方法如何，因为似然项在不同类别间不携带区分信息。\n\nB. 如果用工程化的交互特征 $z_{\\oplus} = \\mathbb{1}[x_1 \\oplus x_2]$ 增强特征集，然后对 $\\{x_1,x_2,z_{\\oplus}\\}$ 应用朴素贝叶斯，那么在数据充足的情况下，该分类器可以达到 $1$ 的贝叶斯最优准确率。\n\nC. 仅添加单个交互特征 $z_{\\wedge} = x_1 x_2$ 得到特征集 $\\{x_1,x_2,z_{\\wedge}\\}$，就足以让朴素贝叶斯在该分布上达到 $1$ 的贝叶斯最优准确率。\n\nD. 添加两个交互指示函数 $z_{00}=\\mathbb{1}[x_1=0, x_2=0]$ 和 $z_{11}=\\mathbb{1}[x_1=1, x_2=1]$ 得到特征集 $\\{x_1,x_2,z_{00},z_{11}\\}$，可以让朴素贝叶斯在数据充足的情况下达到 $1$ 的贝叶斯最优准确率。\n\nE. 在没有任何工程化交互特征的情况下，原始特征空间中的线性分类器，例如仅使用 $\\{x_1,x_2\\}$ 的逻辑回归，可以表示异或决策边界并达到 $1$ 的贝叶斯最优准确率。\n\n选择所有适用项。", "solution": "用户要求对一个关于朴素贝叶斯分类器的问题陈述进行严格验证，并评估几条关于其在特定数据分布上性能的陈述。\n\n### 问题验证\n\n**第一步：提取已知条件**\n-   **类别：** $y \\in \\{0, 1\\}$。\n-   **特征：** 两个二元特征，$x_1 \\in \\{0, 1\\}$ 和 $x_2 \\in \\{0, 1\\}$。\n-   **类别先验概率：** $P(y=1) = P(y=0) = \\frac{1}{2}$。\n-   **条件特征分布：**\n    -   如果 $y=1$，则 $(x_1, x_2)$ 为 $(0,1)$ 的概率是 $\\frac{1}{2}$，为 $(1,0)$ 的概率是 $\\frac{1}{2}$。\n    -   如果 $y=0$，则 $(x_1, x_2)$ 为 $(0,0)$ 的概率是 $\\frac{1}{2}$，为 $(1,1)$ 的概率是 $\\frac{1}{2}$。\n-   **分类器模型：** 朴素贝叶斯，使用分解式 $P(\\mathbf{x} \\mid y) = \\prod_i P(x_i \\mid y)$。\n-   **假设：** 训练集任意大且具有代表性，因此估计的概率等于真实的总体概率。这排除了估计误差，将分析重点放在模型的内在偏差上。\n-   **工程化特征：** $z_{\\oplus} = \\mathbb{1}[x_1 \\oplus x_2]$, $z_{\\wedge} = x_1 x_2$, $z_{00} = \\mathbb{1}[x_1=0, x_2=0]$ 以及 $z_{11} = \\mathbb{1}[x_1=1, x_2=1]$。\n\n**第二步：使用提取的已知条件进行验证**\n-   **科学依据：** 该问题是统计学习理论中一个公认的例子。它描述了异或（XOR）问题，这是一个典型的案例，用来说明那些假设特征独立（如朴素贝叶斯）或线性可分性的分类器的局限性。所有概念（贝叶斯定理、朴素贝叶斯、条件独立）都是标准的且在数学上是严谨的。\n-   **适定性：** 问题陈述是完整的，提供了所有必要的概率和分类器的明确定义。无限数据集的假设使问题变为分析模型的渐进行为，这是一个标准的、适定的理论问题。\n-   **客观性：** 该问题以精确、客观的数学语言陈述。\n-   **结论：** 问题陈述是有效的。它在科学上是严谨的、适定的和客观的。这是一个旨在测试对朴素贝叶斯条件独立假设理解的标准问题。\n\n### 解题推导\n\n数据生成过程意味着特征和类别标签之间存在确定性关系：$y = x_1 \\oplus x_2$，其中 $\\oplus$ 表示异或操作。\n-   如果 $y=1$，则 $(x_1, x_2) \\in \\{(0,1), (1,0)\\}$，此时 $x_1 \\oplus x_2 = 1$。\n-   如果 $y=0$，则 $(x_1, x_2) \\in \\{(0,0), (1,1)\\}$，此时 $x_1 \\oplus x_2 = 0$。\n一个完美的分类器（贝叶斯最优分类器）可以达到 $1$ 的准确率。我们现在将评估朴素贝叶斯分类器在不同特征集下的性能。\n\n首先，我们从给定的联合分布 $P(x_1, x_2 \\mid y)$ 中计算条件边缘概率 $P(x_i \\mid y)$，因为这些是朴素贝叶斯分类器将要学习的参数。\n\n**对于 $y=1$：**\n$P(x_1=1 \\mid y=1) = P((1,0) \\mid y=1) + P((1,1) \\mid y=1) = \\frac{1}{2} + 0 = \\frac{1}{2}$。\n$P(x_1=0 \\mid y=1) = 1 - P(x_1=1 \\mid y=1) = \\frac{1}{2}$。\n$P(x_2=1 \\mid y=1) = P((0,1) \\mid y=1) + P((1,1) \\mid y=1) = \\frac{1}{2} + 0 = \\frac{1}{2}$。\n$P(x_2=0 \\mid y=1) = 1 - P(x_2=1 \\mid y=1) = \\frac{1}{2}$。\n\n**对于 $y=0$：**\n$P(x_1=1 \\mid y=0) = P((1,0) \\mid y=0) + P((1,1) \\mid y=0) = 0 + \\frac{1}{2} = \\frac{1}{2}$。\n$P(x_1=0 \\mid y=0) = 1 - P(x_1=1 \\mid y=0) = \\frac{1}{2}$。\n$P(x_2=1 \\mid y=0) = P((0,1) \\mid y=0) + P((1,1) \\mid y=0) = 0 + \\frac{1}{2} = \\frac{1}{2}$。\n$P(x_2=0 \\mid y=0) = 1 - P(x_2=1 \\mid y=0) = \\frac{1}{2}$。\n\n关键的是，对于任何特征 $i \\in \\{1, 2\\}$ 和任何值 $v \\in \\{0, 1\\}$，我们发现 $P(x_i=v \\mid y=1) = P(x_i=v \\mid y=0) = \\frac{1}{2}$。\n\n**A. 仅使用特征 $x_1$ 和 $x_2$，朴素贝叶斯在此分布上实现的期望准确率为 $\\frac{1}{2}$，无论训练集大小和标准的平滑方法如何，因为似然项在不同类别间不携带区分信息。**\n\n朴素贝叶斯分类器预测 $\\hat{y} = \\arg\\max_{y \\in \\{0,1\\}} P(y \\mid x_1, x_2)$。后验概率与先验概率乘以朴素贝叶斯似然成正比：$P(y \\mid x_1, x_2) \\propto P(y) P(x_1 \\mid y) P(x_2 \\mid y)$。令 $S(y)$ 为类别 $y$ 的得分。\n\n对于 $y=1$： $S(1) = P(y=1) P(x_1 \\mid y=1) P(x_2 \\mid y=1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{8}$。\n对于 $y=0$： $S(0) = P(y=0) P(x_1 \\mid y=0) P(x_2 \\mid y=0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{8}$。\n\n对于任何输入 $(x_1, x_2)$，得分都是相同的，$S(1) = S(0)$。这意味着后验概率总是 $P(y=1 \\mid x_1, x_2) = P(y=0 \\mid x_1, x_2) = \\frac{1}{2}$。分类器没有任何信息来做决策，必须进行猜测（例如，通过将平局判给默认类别 $0$）。由于两个类别同样可能，即 $P(y=0) = P(y=1) = \\frac{1}{2}$，任何固定的猜测策略都将产生 $\\frac{1}{2}$ 的准确率。该陈述的理由，“似然项在不同类别间不携带区分信息”，是正确的，因为 $P(x_i \\mid y=1) = P(x_i \\mid y=0)$。\n\n对 A 的判定：**正确**。\n\n**B. 如果用工程化的交互特征 $z_{\\oplus} = \\mathbb{1}[x_1 \\oplus x_2]$ 增强特征集，然后对 $\\{x_1,x_2,z_{\\oplus}\\}$ 应用朴素贝叶斯，那么在数据充足的情况下，该分类器可以达到 $1$ 的贝叶斯最优准确率。**\n\n新特征是 $z_{\\oplus} = x_1 \\oplus x_2$。如前所述，$y=x_1 \\oplus x_2$，所以 $z_{\\oplus}=y$。这个新特征的条件概率是：\n$P(z_{\\oplus}=1 \\mid y=1) = 1$ 且 $P(z_{\\oplus}=0 \\mid y=1) = 0$。\n$P(z_{\\oplus}=1 \\mid y=0) = 0$ 且 $P(z_{\\oplus}=0 \\mid y=0) = 1$。\n\n现在朴素贝叶斯得分为 $S(y) = P(y) P(x_1 \\mid y) P(x_2 \\mid y) P(z_{\\oplus} \\mid y)$。\n让我们测试一个真实标签为 $y=1$ 的输入点，例如 $(x_1, x_2) = (0,1)$。对于此点，$z_{\\oplus}=1$。\n$S(1) = P(y=1) P(x_1=0|1) P(x_2=1|1) P(z_{\\oplus}=1|1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 1 = \\frac{1}{8}$。\n$S(0) = P(y=0) P(x_1=0|0) P(x_2=1|0) P(z_{\\oplus}=1|0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 0 = 0$。\n由于 $S(1) > S(0)$，预测结果为 $\\hat{y}=1$，这是正确的。\n让我们测试一个真实标签为 $y=0$ 的输入点，例如 $(x_1, x_2) = (0,0)$。对于此点，$z_{\\oplus}=0$。\n$S(1) = P(y=1) P(x_1=0|1) P(x_2=0|1) P(z_{\\oplus}=0|1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 0 = 0$。\n$S(0) = P(y=0) P(x_1=0|0) P(x_2=0|0) P(z_{\\oplus}=0|0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 1 = \\frac{1}{8}$。\n由于 $S(0) > S(1)$，预测结果为 $\\hat{y}=0$，这是正确的。\n特征 $z_{\\oplus}$ 是一个完美的预测器。似然项 $P(z_{\\oplus} \\mid y)$ 的存在——对于错误的类别其值为 $0$，对于正确的类别其值为 $1$——确保了错误类别的总得分变为 $0$，从而每次都能做出完美的预测。准确率为 $1$。\n\n对 B 的判定：**正确**。\n\n**C. 仅添加单个交互特征 $z_{\\wedge} = x_1 x_2$ 得到特征集 $\\{x_1,x_2,z_{\\wedge}\\}$，就足以让朴素贝叶斯在该分布上达到 $1$ 的贝叶斯最优准确率。**\n\n新特征是 $z_{\\wedge} = x_1 x_2$。让我们计算其条件概率：\n对于 $y=1$，数据点是 $(0,1)$ 和 $(1,0)$。在这两种情况下，$z_{\\wedge}=0$。因此，$P(z_{\\wedge}=0 \\mid y=1)=1$ 且 $P(z_{\\wedge}=1 \\mid y=1)=0$。\n对于 $y=0$，数据点是 $(0,0)$（概率为 $\\frac{1}{2}$）和 $(1,1)$（概率为 $\\frac{1}{2}$）。对于 $(0,0)$，$z_{\\wedge}=0$。对于 $(1,1)$，$z_{\\wedge}=1$。因此，$P(z_{\\wedge}=0 \\mid y=0)=\\frac{1}{2}$ 且 $P(z_{\\wedge}=1 \\mid y=0)=\\frac{1}{2}$。\n\n得分为 $S(y) = P(y) P(x_1 \\mid y) P(x_2 \\mid y) P(z_{\\wedge} \\mid y)$。让我们测试点 $(x_1, x_2)=(0,0)$，其真实标签为 $y=0$。对于此点，$z_{\\wedge}=0$。\n$S(1) = P(y=1)P(x_1=0|1)P(x_2=0|1)P(z_{\\wedge}=0|1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 1 = \\frac{1}{8}$。\n$S(0) = P(y=0)P(x_1=0|0)P(x_2=0|0)P(z_{\\wedge}=0|0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{16}$。\n由于 $S(1) > S(0)$，分类器预测 $\\hat{y}=1$，这是不正确的。准确率不是 $1$。\n\n对 C 的判定：**不正确**。\n\n**D. 添加两个交互指示函数 $z_{00}=\\mathbb{1}[x_1=0, x_2=0]$ 和 $z_{11}=\\mathbb{1}[x_1=1, x_2=1]$ 得到特征集 $\\{x_1,x_2,z_{00},z_{11}\\}$，可以让朴素贝叶斯在数据充足的情况下达到 $1$ 的贝叶斯最优准确率。**\n\n新特征是 $z_{00}$ 和 $z_{11}$。让我们计算它们的条件概率。\n对于 $y=1$，数据点是 $(0,1)$ 和 $(1,0)$。\n对于这两个点，$x_1=0, x_2=0$ 均为假，所以 $z_{00}=0$。因此 $P(z_{00}=0 \\mid y=1)=1$ 且 $P(z_{00}=1 \\mid y=1)=0$。\n对于这两个点，$x_1=1, x_2=1$ 均为假，所以 $z_{11}=0$。因此 $P(z_{11}=0 \\mid y=1)=1$ 且 $P(z_{11}=1 \\mid y=1)=0$。\n\n对于 $y=0$，数据点是 $(0,0)$（概率 $\\frac{1}{2}$）和 $(1,1)$（概率 $\\frac{1}{2}$）。\n对于 $(0,0)$，$z_{00}=1, z_{11}=0$。对于 $(1,1)$，$z_{00}=0, z_{11}=1$。\n所以，$P(z_{00}=1 \\mid y=0)=\\frac{1}{2}$ 且 $P(z_{00}=0 \\mid y=0)=\\frac{1}{2}$。\n并且，$P(z_{11}=1 \\mid y=0)=\\frac{1}{2}$ 且 $P(z_{11}=0 \\mid y=0)=\\frac{1}{2}$。\n\n得分为 $S(y) = P(y) P(x_1|y) P(x_2|y) P(z_{00}|y) P(z_{11}|y)$。\n让我们测试点 $(x_1, x_2)=(0,0)$，真实标签 $y=0$。对于此点，$z_{00}=1, z_{11}=0$。\n$S(1) = P(y=1)P(x_1=0|1)P(x_2=0|1)P(z_{00}=1|1)P(z_{11}=0|1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 0 \\cdot 1 = 0$。\n$S(0) = P(y=0)P(x_1=0|0)P(x_2=0|0)P(z_{00}=1|0)P(z_{11}=0|0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{32}$。\n$S(0) > S(1)$，所以预测结果为 $\\hat{y}=0$。正确。\n\n让我们测试点 $(x_1, x_2)=(1,1)$，真实标签 $y=0$。对于此点，$z_{00}=0, z_{11}=1$。\n$S(1) = P(y=1)P(x_1=1|1)P(x_2=1|1)P(z_{00}=0|1)P(z_{11}=1|1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 1 \\cdot 0 = 0$。\n$S(0) = P(y=0)P(x_1=1|0)P(x_2=1|0)P(z_{00}=0|0)P(z_{11}=1|0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{32}$。\n$S(0) > S(1)$，所以预测结果为 $\\hat{y}=0$。正确。\n\n对于任何真实标签为 $y=0$ 的点（即 $(0,0)$ 或 $(1,1)$），$z_{00}$ 或 $z_{11}$ 中有一个将为 $1$。但对于这样的点，在计算 $S(1)$ 时，似然项 $P(z_{00}=1|1)$ 或 $P(z_{11}=1|1)$ 将为 $0$，从而迫使 $S(1)=0$。这保证了正确的预测。类似的论证也适用于真实标签为 $y=1$ 的点。对于这些点（例如 $(0,1)$），$z_{00}=0$ 且 $z_{11}=0$。\n$S(1) = P(y=1)P(x_1=0|1)P(x_2=1|1)P(z_{00}=0|1)P(z_{11}=0|1) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot 1 \\cdot 1 = \\frac{1}{8}$。\n$S(0) = P(y=0)P(x_1=0|0)P(x_2=1|0)P(z_{00}=0|0)P(z_{11}=0|0) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{32}$。\n$S(1) > S(0)$，所以预测结果为 $\\hat{y}=1$。正确。\n该分类器对所有输入都完美工作。准确率为 $1$。\n\n对 D 的判定：**正确**。\n\n**E. 在没有任何工程化交互特征的情况下，原始特征空间中的线性分类器，例如仅使用 $\\{x_1,x_2\\}$ 的逻辑回归，可以表示异或决策边界并达到 $1$ 的贝叶斯最优准确率。**\n\n二维空间中的线性分类器创建了一个形式为 $w_0 + w_1 x_1 + w_2 x_2 = 0$ 的线性决策边界（一条直线）。数据点为：\n类别 $1$：$(0,1)$ 和 $(1,0)$。\n类别 $0$：$(0,0)$ 和 $(1,1)$。\n这是经典的异或问题。为了让线性分类器达到完美的准确率，它必须找到权重 $(w_0, w_1, w_2)$，使得 $w_0 + w_1 x_1 + w_2 x_2$ 的符号对于一个类别为正，对于另一个类别为负。假设我们寻求：\n1.  对于 $(0,0)$：$w_0  0$\n2.  对于 $(1,1)$：$w_0 + w_1 + w_2  0$\n3.  对于 $(0,1)$：$w_0 + w_2 > 0$\n4.  对于 $(1,0)$：$w_0 + w_1 > 0$\n\n从 (1) 和 (3) 可得，$w_2 > -w_0$。因为 $w_00$，所以 $-w_0 > 0$，因此 $w_2 > 0$。\n从 (1) 和 (4) 可得，$w_1 > -w_0$。所以 $w_1 > 0$。\n从 (4) 可得，$w_0 + w_1 > 0$。两边同时加上 $w_2$，且已知 $w_2 > 0$，我们得到 $(w_0 + w_1) + w_2 > 0 + w_2 > 0$。这意味着 $w_0 + w_1 + w_2 > 0$。\n这个结果与条件 (2) 相矛盾，该条件要求 $w_0 + w_1 + w_2  0$。\n这个不等式组是不一致的。因此，不存在这样的线性决策边界。异或问题不是线性可分的。一个使用特征 $\\{x_1, x_2\\}$ 的逻辑回归模型是一个线性分类器，因此无法解决此问题以达到 $1$ 的准确率。\n\n对 E 的判定：**不正确**。", "answer": "$$\\boxed{ABD}$$", "id": "3152539"}, {"introduction": "将理论模型转化为可靠的代码是机器学习实践中的关键一步。当处理概率的连乘时，即使是中等数量的特征也可能导致计算结果下溢为零，从而使分类器失效 [@problem_id:3260875]。这个编程练习将指导你构建一个朴素贝叶斯分类器，亲身遭遇并诊断数值下溢问题，然后通过在对数域中进行计算来解决它，确保模型的数值稳定性。", "problem": "您需要实现一个用于具有独立伯努利分量的特征向量的二元朴素贝叶斯分类器（NBC），并分析在存在浮点数下溢时的数值稳定性。推导的基本出发点必须是用于后验概率的贝叶斯法则和朴素贝叶斯分类器（NBC）的独立性假设，并使用电气和电子工程师协会（IEEE）$754$ 浮点语义来讨论下溢问题。\n\n给定一个特征向量 $x \\in \\{0,1\\}^d$，类别 $c \\in \\{0,1\\}$ 的类先验概率 $\\pi_c$，以及特征 $i$ 上类别 $0$ 的类条件伯努利参数 $p_i$ 和类别 $1$ 的类条件伯努利参数 $q_i$，请考虑以下任务：\n- 构建一个分类器，该分类器使用每个特征似然的直接乘积来形成一个与 $\\pi_c$ 乘以似然乘积成正比的后验概率。通过识别计算出的乘积在 IEEE $754$ 双精度标准下是否为 $0$ 来检测何时发生浮点数下溢。\n- 重建分类器，使其使用累积对数似然而不是原始乘积进行操作。通过比较累积对数后验值来实现分类。\n- 检验在特征复制下的决策边界稳定性。通过将整个特征向量重复 $r$ 次来模拟复制，这将使每个类别的似然乘以原始似然的 $r$ 次方，并将相同的因子 $r$ 加到累积对数似然上。验证在对数域中，定义决策边界的对数后验概率差的符号对于正的复制因子 $r$ 是不变的。\n\n您必须为乘积域分类器严格实现以下平局处理规则：如果两个后验乘积都为 $0$，则返回类别标签 $-1$；否则，返回后验乘积严格较大的那个类别（如果相等，也返回 $-1$）。\n\n本问题不涉及物理单位。\n\n您的程序必须评估以下测试套件。对于每个测试用例，将结果组织成一个包含四个项目的列表：$[\\text{underflow\\_at\\_large\\_r}, \\text{naive\\_vs\\_log\\_agree\\_at\\_small\\_r}, \\text{log\\_class\\_at\\_large\\_r}, \\text{log\\_stable\\_between\\_small\\_and\\_large}]$，其中：\n- $\\text{underflow\\_at\\_large\\_r}$ 是一个布尔值，指示在大的复制因子下，是否有任何类别的后验乘积下溢为 $0$。\n- $\\text{naive\\_vs\\_log\\_agree\\_at\\_small\\_r}$ 是一个布尔值，指示在小的复制因子下，朴素乘积域分类器是否与对数域分类器一致。\n- $\\text{log\\_class\\_at\\_large\\_r}$ 是在大的复制因子下，对数域预测的整数类别（$0$ 或 $1$）。\n- $\\text{log\\_stable\\_between\\_small\\_and\\_large}$ 是一个布尔值，指示对数域分类器的预测在小的和大的复制因子下是否相同。\n\n测试套件参数值：\n- 测试用例 $1$（正常路径，无下溢）：\n  - 维度 $d = 12$。\n  - 特征向量 $x$ 有 $6$ 个 $1$，后跟 $6$ 个 $0$。\n  - 类别 $0$：对于前 $6$ 个特征，$p_i = 0.2$；对于后 $6$ 个特征，$p_i = 0.05$。\n  - 类别 $1$：对于前 $6$ 个特征，$q_i = 0.7$；对于后 $6$ 个特征，$q_i = 0.1$。\n  - 先验概率 $\\pi_0 = 0.5$, $\\pi_1 = 0.5$。\n  - 复制因子：小的 $r_{\\text{small}} = 1$，大的 $r_{\\text{large}} = 50$。\n- 测试用例 $2$（由于许多微小的似然而导致下溢）：\n  - 维度 $d = 40$。\n  - 特征向量 $x$ 有 $30$ 个 $1$，后跟 $10$ 个 $0$。\n  - 类别 $0$：对于前 $30$ 个特征，$p_i = 10^{-4}$；对于后 $10$ 个特征，$p_i = 10^{-2}$。\n  - 类别 $1$：对于前 $30$ 个特征，$q_i = 2\\times 10^{-4}$；对于后 $10$ 个特征，$q_i = 2\\times 10^{-2}$。\n  - 先验概率 $\\pi_0 = 0.5$, $\\pi_1 = 0.5$。\n  - 复制因子：小的 $r_{\\text{small}} = 1$，大的 $r_{\\text{large}} = 3$。\n- 测试用例 $3$（边界情况：似然相同，决策由先验主导，在大的 $r$ 值下发生下溢）：\n  - 维度 $d = 50$。\n  - 特征向量 $x$ 有 $45$ 个 $1$，后跟 $5$ 个 $0$。\n  - 类别 $0$：对于所有 $50$ 个特征，$p_i = 10^{-4}$。\n  - 类别 $1$：与类别 $0$ 相同，$q_i = 10^{-4}$。\n  - 先验概率 $\\pi_0 = 0.4$, $\\pi_1 = 0.6$。\n  - 复制因子：小的 $r_{\\text{small}} = 1$，大的 $r_{\\text{large}} = 3$。\n\n您的程序应生成单行输出，其中包含三个测试用例结果列表，格式为逗号分隔的列表，并用方括号括起来，例如 $[[\\dots],[\\dots],[\\dots]]$。内部列表中的每个项目必须是布尔值或整数，并且格式中不得包含空格。", "solution": "该问题要求实现和分析一个用于具有独立伯努利分布特征的二元朴素贝叶斯分类器（NBC）。主要焦点在于直接计算后验概率时由浮点数下溢引起的数值不稳定性，以及如何通过在对数域中进行计算来缓解此问题。\n\n### 理论阐述\n\n分类器的基础是贝叶斯法则，该法则指出给定一个特征向量 $x$，类别 $c$ 的后验概率为：\n$$ P(c|x) = \\frac{P(x|c)P(c)}{P(x)} $$\n为了进行分类，我们寻找使该后验概率最大化的类别 $c$。由于对于给定的 $x$，证据 $P(x)$ 对所有类别都是常数，我们可以将其简化为寻找使似然 $P(x|c)$ 和先验概率 $P(c)$ 的乘积最大化的类别：\n$$ \\hat{c} = \\underset{c \\in \\{0,1\\}}{\\operatorname{argmax}} \\, P(x|c)P(c) $$\n朴素贝叶斯假设假定，在给定类别 $c$ 的条件下，特征 $x_i$ 是条件独立的。这使得似然 $P(x|c)$ 可以表示为各个特征似然的乘积：\n$$ P(x|c) = \\prod_{i=1}^{d} P(x_i|c) $$\n特征被描述为伯努利随机变量。对于一个特征 $x_i \\in \\{0, 1\\}$，类条件概率由 $P(x_i=1|c) = \\theta_{ic}$ 给出。因此，单个特征 $x_i$ 的似然可以紧凑地写为：\n$$ P(x_i|c) = \\theta_{ic}^{x_i} (1 - \\theta_{ic})^{1-x_i} $$\n令 $\\pi_0$ 和 $\\pi_1$ 分别为类别 $c=0$ 和 $c=1$ 的先验概率。令 $p_i$ 为给定类别 $0$ 时特征 $i$ 的伯努利参数（即 $P(x_i=1|c=0)=p_i$），$q_i$ 为给定类别 $1$ 时的参数（即 $P(x_i=1|c=1)=q_i$）。\n\n#### 1. 基于朴素乘积的分类器\n\n类别 $0$ 和类别 $1$ 的未归一化后验概率为：\n$$ \\text{Post}_0(x) = \\pi_0 \\prod_{i=1}^{d} p_i^{x_i}(1-p_i)^{1-x_i} $$\n$$ \\text{Post}_1(x) = \\pi_1 \\prod_{i=1}^{d} q_i^{x_i}(1-q_i)^{1-x_i} $$\n分类是通过比较 $\\text{Post}_0(x)$ 和 $\\text{Post}_1(x)$ 来进行的。然而，这种方法在数值上是不稳定的。许多介于 $0$ 和 $1$ 之间的概率相乘，其乘积会迅速变得小于最小可表示的正浮点数（一种称为下溢的现象）。在 IEEE $754$ 双精度标准下，这将导致乘积计算为 $0.0$，从而丢失所有信息，并可能使比较无法进行。\n\n#### 2. 对数域分类器\n\n为防止下溢，我们可以处理后验概率的对数。由于对数是严格单调函数，最大化 $\\log(\\text{Post}_c(x))$ 等价于最大化 $\\text{Post}_c(x)$。\n$$ \\log(\\text{Post}_c(x)) = \\log(P(c)) + \\sum_{i=1}^{d} \\log(P(x_i|c)) $$\n我们两个类别的对数后验概率为：\n$$ \\text{logPost}_0(x) = \\log(\\pi_0) + \\sum_{i=1}^{d} \\left( x_i \\log(p_i) + (1-x_i)\\log(1-p_i) \\right) $$\n$$ \\text{logPost}_1(x) = \\log(\\pi_1) + \\sum_{i=1}^{d} \\left( x_i \\log(q_i) + (1-x_i)\\log(1-q_i) \\right) $$\n这种表示方法用小数的对数之和（这些对数是负数）代替了小数的乘积。求和运算在对抗下溢和上溢方面比乘法运算稳健得多，从而保持了数值精度。分类规则变为：如果 $\\text{logPost}_1(x) > \\text{logPost}_0(x)$，则预测为类别 $1$，否则预测为类别 $0$（假设有平局决胜规则，例如，默认判为类别 $0$）。\n\n#### 3. 特征复制下的稳定性\n\n特征复制，即将特征向量 $x$ 重复 $r$ 次以形成新的向量 $x^{(r)}$，提供了一种研究分类器在缩放下的行为的方法。对于朴素乘积分类器，新的似然是原始似然的 $r$ 次方：\n$$ P(x^{(r)}|c) = \\left( \\prod_{i=1}^{d} P(x_i|c) \\right)^r $$\n后验概率变为 $\\pi_c (P(x|c))^r$。如果 $P(x|c)  1$，随着 $r$ 的增加，这一项会非常迅速地趋近于 $0$，从而加剧了下溢问题。\n\n在对数域中，复制的效应是一个简单的乘法：\n$$ \\log(P(x^{(r)}|c)) = r \\log(P(x|c)) = r \\sum_{i=1}^{d} \\log(P(x_i|c)) $$\n新的对数后验概率为 $\\text{logPost}_c(x^{(r)}) = \\log(\\pi_c) + r \\cdot \\text{logLikelihood}(x|c)$。决策基于差值的符号：\n$$ D(r) = \\text{logPost}_1(x^{(r)}) - \\text{logPost}_0(x^{(r)}) = (\\log(\\pi_1) - \\log(\\pi_0)) + r \\left( \\sum_{i=1}^{d}\\log(P(x_i|1)) - \\sum_{i=1}^{d}\\log(P(x_i|0)) \\right) $$\n令 $\\Delta_{\\pi} = \\log(\\pi_1) - \\log(\\pi_0)$ 且 $\\Delta_{LL} = \\sum_{i=1}^{d}\\log(P(x_i|1)) - \\sum_{i=1}^{d}\\log(P(x_i|0))$。决策取决于 $D(r) = \\Delta_{\\pi} + r \\cdot \\Delta_{LL}$ 的符号。对于任何正的复制因子 $r > 0$，$r \\cdot \\Delta_{LL}$ 的符号与 $\\Delta_{LL}$ 的符号相同。如果 $\\Delta_{LL} \\neq 0$，对于大的 $r$，$r \\cdot \\Delta_{LL}$ 项将占主导地位，但和的符号对所有 $r$ 都保持不变，除非 $\\Delta_{\\pi}$ 和 $\\Delta_{LL}$ 的符号相反且存在某个 $r_0$ 使得 $D(r_0)=0$。然而，比较 $r=1$ 和一个更大的 $r$ 时，分类应该是稳定的。如果 $\\Delta_{LL} = 0$，决策仅取决于先验概率，也与 $r$ 无关。因此，在没有数值误差的情况下，对数域分类器的预测相对于复制因子 $r > 0$ 是稳定的。\n\n### 实现策略\n\n实现将包括两个用于分类的函数：一个使用直接乘积，另一个使用对数求和。对于每个测试用例，我们将：\n1.  将特征向量 $x$ 以及伯努利参数 $p_i$ 和 $q_i$ 定义为 NumPy 数组。\n2.  实现 `naive_classifier`，该函数计算后验概率 $\\text{Post}_0$ 和 $\\text{Post}_1$ 的复制因子 $r$ 次方，并应用指定的平局处理逻辑。我们将记录原始乘积值以检查下溢（$0.0$）。\n3.  实现 `log_classifier`，该函数通过对对数似然求和，乘以 $r$，然后加上对数先验来计算对数后验概率。\n4.  对于每个测试用例及其指定的小 ($r_{\\text{small}}$) 和大 ($r_{\\text{large}}$) 复制因子，我们将计算所需的四个结果：\n    -   `underflow_at_large_r`：检查在 $r_{\\text{large}}$ 时 `naive_classifier` 的后验概率是否为零。\n    -   `naive_vs_log_agree_at_small_r`：在 $r_{\\text{small}}$ 时比较 `naive_classifier` 和 `log_classifier` 的预测。\n    -   `log_class_at_large_r`：在 $r_{\\text{large}}$ 时来自 `log_classifier` 的预测。\n    -   `log_stable_between_small_and_large`：比较 `log_classifier` 在 $r_{\\text{small}}$ 和 $r_{\\text{large}}$ 时的预测。\n最终输出是一个列表，其中包含每个测试用例的这些结果，并按指定格式进行格式化。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes a binary Naive Bayes Classifier to demonstrate\n    numerical stability issues with underflow.\n    \"\"\"\n\n    def naive_classifier(x, p_params, q_params, pi0, pi1, r):\n        \"\"\"\n        Classifies using direct product of likelihoods.\n        Returns prediction and raw posterior values.\n        \"\"\"\n        # Calculate per-feature likelihoods for class 0\n        p_likelihoods = np.where(x == 1, p_params, 1 - p_params)\n        # Calculate per-feature likelihoods for class 1\n        q_likelihoods = np.where(x == 1, q_params, 1 - q_params)\n\n        # Calculate total likelihood product\n        prod_p = np.prod(p_likelihoods)\n        prod_q = np.prod(q_likelihoods)\n\n        # Apply duplication factor r\n        post0 = pi0 * (prod_p ** r)\n        post1 = pi1 * (prod_q ** r)\n\n        # Apply classification and tie-handling rule\n        if post0 == 0.0 and post1 == 0.0:\n            pred = -1\n        elif post0 == post1:\n            pred = -1\n        elif post1 > post0:\n            pred = 1\n        else:\n            pred = 0\n            \n        return pred, post0, post1\n\n    def log_classifier(x, p_params, q_params, pi0, pi1, r):\n        \"\"\"\n        Classifies using sum of log-likelihoods to avoid underflow.\n        \"\"\"\n        # Calculate per-feature log-likelihoods for class 0\n        log_p_likelihoods = np.where(x == 1, np.log(p_params), np.log(1 - p_params))\n        # Calculate per-feature log-likelihoods for class 1\n        log_q_likelihoods = np.where(x == 1, np.log(q_params), np.log(1 - q_params))\n\n        # Sum log-likelihoods\n        sum_log_p = np.sum(log_p_likelihoods)\n        sum_log_q = np.sum(log_q_likelihoods)\n        \n        # Calculate log posterior\n        log_post0 = np.log(pi0) + r * sum_log_p\n        log_post1 = np.log(pi1) + r * sum_log_q\n        \n        # Classify\n        if log_post1 > log_post0:\n            return 1\n        else:\n            # Default to class 0 in case of a tie\n            return 0\n\n    test_cases = [\n        # Test case 1 (happy path, no underflow)\n        {\n            \"d\": 12,\n            \"x\": np.array([1]*6 + [0]*6),\n            \"p_params\": np.array([0.2]*6 + [0.05]*6),\n            \"q_params\": np.array([0.7]*6 + [0.1]*6),\n            \"pi0\": 0.5, \"pi1\": 0.5,\n            \"r_small\": 1, \"r_large\": 50\n        },\n        # Test case 2 (underflow due to many tiny likelihoods)\n        {\n            \"d\": 40,\n            \"x\": np.array([1]*30 + [0]*10),\n            \"p_params\": np.array([1e-4]*30 + [1e-2]*10),\n            \"q_params\": np.array([2e-4]*30 + [2e-2]*10),\n            \"pi0\": 0.5, \"pi1\": 0.5,\n            \"r_small\": 1, \"r_large\": 3\n        },\n        # Test case 3 (boundary case: identical likelihoods, prior-dominated)\n        {\n            \"d\": 50,\n            \"x\": np.array([1]*45 + [0]*5),\n            \"p_params\": np.array([1e-4]*50),\n            \"q_params\": np.array([1e-4]*50),\n            \"pi0\": 0.4, \"pi1\": 0.6,\n            \"r_small\": 1, \"r_large\": 3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x, p, q, pi0, pi1, r_small, r_large = (\n            case[\"x\"], case[\"p_params\"], case[\"q_params\"], \n            case[\"pi0\"], case[\"pi1\"], case[\"r_small\"], case[\"r_large\"]\n        )\n\n        # Evaluate at large r to check for underflow\n        _, post0_large, post1_large = naive_classifier(x, p, q, pi0, pi1, r_large)\n        underflow_at_large_r = (post0_large == 0.0) or (post1_large == 0.0)\n\n        # Compare naive and log classifiers at small r\n        naive_pred_small, _, _ = naive_classifier(x, p, q, pi0, pi1, r_small)\n        log_pred_small = log_classifier(x, p, q, pi0, pi1, r_small)\n        naive_vs_log_agree_at_small_r = (naive_pred_small == log_pred_small)\n\n        # Get log classifier prediction at large r\n        log_class_at_large_r = log_classifier(x, p, q, pi0, pi1, r_large)\n        \n        # Check stability of log classifier\n        log_stable_between_small_and_large = (log_pred_small == log_class_at_large_r)\n\n        case_results = [\n            underflow_at_large_r,\n            naive_vs_log_agree_at_small_r,\n            log_class_at_large_r,\n            log_stable_between_small_and_large\n        ]\n        results.append(case_results)\n\n    # Format the final output string without spaces inside inner lists.\n    inner_results_str = [f\"[{str(r[0]).lower()},{str(r[1]).lower()},{r[2]},{str(r[3]).lower()}]\" for r in results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3260875"}]}