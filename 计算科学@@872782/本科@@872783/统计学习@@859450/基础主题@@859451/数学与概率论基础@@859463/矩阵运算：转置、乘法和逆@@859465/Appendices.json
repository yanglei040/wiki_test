{"hands_on_practices": [{"introduction": "在统计学习中，构建线性模型的第一步通常是定义设计矩阵。一个看似简单却至关重要的步骤是在特征矩阵中加入一个常数项（截距），这在代数上表现为给设计矩阵增加一列全为1的向量。这个练习 [@problem_id:3146970] 引导我们通过分块矩阵的视角，深入探究这一操作如何改变模型的核心——格拉姆矩阵 $X^\\top X$ 的结构。通过亲自推导，你将理解数据中心化为何能简化模型参数的求解，并掌握分块矩阵求逆这一关键技巧。", "problem": "在统计学习中使用的线性模型里，通常会通过给特征矩阵增加一个全为1的列来引入截距项。考虑一个有 $n=4$ 个观测值和 $p=2$ 个特征的数据集。特征矩阵为\n$$\nX=\\begin{pmatrix}\n0  1\\\\\n1  1\\\\\n2  3\\\\\n3  4\n\\end{pmatrix},\n$$\n截距列为 $\\mathbf{1}=\\begin{pmatrix}1\\\\1\\\\1\\\\1\\end{pmatrix}$。定义增广设计矩阵 $\\tilde{X}=\\begin{pmatrix}\\mathbf{1}  X\\end{pmatrix}$ 和格拉姆矩阵 $\\tilde{G}=\\tilde{X}^{\\top}\\tilde{X}$。\n\n仅使用矩阵转置、乘法和逆的定义，完成以下任务：\n- 将 $\\tilde{G}$ 表示为分块形式，分块方式与截距和特征相容。从矩阵乘法的定义出发，用 $n$、$X$ 的列和以及 $X^{\\top}X$ 来表示每个分块。\n- 基于你的分块表达式，解释为什么对 $X$ 的列进行中心化（即，使每个特征的样本均值为零）会影响非对角块，并能将截距与斜率解耦。\n- 通过求解一个由 $\\tilde{G}$ 构建的分块线性系统，并且不引用任何预先记下的逆矩阵公式，推导 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素的闭式表达式，用 $n$、$X^{\\top}X$ 和 $X$ 的列和表示。\n- 对于上面给定的特定 $X$，计算 $\\tilde{G}^{-1}$ 的 $(1,1)$ 元素的精确值。\n\n以单个精确数的形式提供最终答案。无需四舍五入，也不涉及任何单位。最终答案必须仅为 $\\tilde{G}^{-1}$ 的 (1,1) 元素的值。", "solution": "该问题是适定的，有科学依据，并为求得唯一解提供了所有必要信息。我们可以进行推导和计算。\n\n增广设计矩阵 $\\tilde{X}$ 是通过在特征矩阵 $X$ 前面附加一个全为1的列 $\\mathbf{1}$ 构成的。已知 $\\mathbf{1}$ 是一个 $n \\times 1$ 矩阵，$X$ 是一个 $n \\times p$ 矩阵，我们可以将 $\\tilde{X}$ 写成分块形式 $\\tilde{X} = \\begin{pmatrix} \\mathbf{1}  X \\end{pmatrix}$。对于本问题，$n=4$，$p=2$。\n\n**第一部分：格拉姆矩阵 $\\tilde{G}$ 的分块形式**\n\n格拉姆矩阵 $\\tilde{G}$ 定义为 $\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X}$。为了将其表示为分块形式，我们首先求 $\\tilde{X}$ 的转置：\n$$\n\\tilde{X}^{\\top} = \\begin{pmatrix} \\mathbf{1}  X \\end{pmatrix}^{\\top} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix}\n$$\n此处，$\\mathbf{1}^{\\top}$ 是一个 $1 \\times n$ 的全1行向量，$X^{\\top}$ 是特征矩阵的 $p \\times n$ 转置。\n\n现在，我们执行分块矩阵乘法来求 $\\tilde{G}$：\n$$\n\\tilde{G} = \\tilde{X}^{\\top}\\tilde{X} = \\begin{pmatrix} \\mathbf{1}^{\\top} \\\\ X^{\\top} \\end{pmatrix} \\begin{pmatrix} \\mathbf{1}  X \\end{pmatrix} = \\begin{pmatrix} \\mathbf{1}^{\\top}\\mathbf{1}  \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1}  X^{\\top}X \\end{pmatrix}\n$$\n我们识别每个分块：\n- 左上角分块是 $\\mathbf{1}^{\\top}\\mathbf{1} = \\sum_{i=1}^{n} 1 \\cdot 1 = n$。这是一个 $1 \\times 1$ 的分块（一个标量）。\n- 右上角分块是 $\\mathbf{1}^{\\top}X$。这是一个 $1 \\times p$ 的行向量，其中第 $j$ 个元素是 $\\mathbf{1}$ 与 $X$ 的第 $j$ 列的点积，即 $\\sum_{i=1}^{n} x_{ij}$。因此，$\\mathbf{1}^{\\top}X$ 是 $X$ 的列和组成的行向量。\n- 左下角分块是 $X^{\\top}\\mathbf{1}$。这是一个 $p \\times 1$ 的列向量。它是 $\\mathbf{1}^{\\top}X$ 的转置：$( \\mathbf{1}^{\\top}X )^{\\top} = X^{\\top}(\\mathbf{1}^{\\top})^{\\top} = X^{\\top}\\mathbf{1}$。所以，它是 $X$ 的列和组成的列向量。\n- 右下角分块是 $X^{\\top}X$，即原始特征的 $p \\times p$ 格拉姆矩阵。\n\n因此，$\\tilde{G}$ 的分块形式为：\n$$\n\\tilde{G} = \\begin{pmatrix} n  \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1}  X^{\\top}X \\end{pmatrix}\n$$\n\n**第二部分：中心化特征的影响**\n\n对 $X$ 的列进行中心化，意味着将每个特征列 $\\mathbf{x}_j$ 替换为一个新列 $\\mathbf{x}'_j = \\mathbf{x}_j - \\bar{x}_j\\mathbf{1}$，其中 $\\bar{x}_j = \\frac{1}{n}\\sum_{i=1}^{n} x_{ij}$ 是第 $j$ 个特征的样本均值。令中心化后的矩阵记为 $X_c$。\n\n此变换的关键影响在于每个新列中元素的和。第 $j$ 个中心化列中元素的和为：\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = \\mathbf{1}^{\\top}(\\mathbf{x}_j - \\bar{x}_j\\mathbf{1}) = \\mathbf{1}^{\\top}\\mathbf{x}_j - \\bar{x}_j(\\mathbf{1}^{\\top}\\mathbf{1}) = \\left(\\sum_{i=1}^{n} x_{ij}\\right) - \\bar{x}_j \\cdot n\n$$\n根据均值 $\\bar{x}_j$ 的定义，我们有 $\\sum_{i=1}^{n} x_{ij} = n\\bar{x}_j$。代入上式，得到：\n$$\n\\mathbf{1}^{\\top}\\mathbf{x}'_j = n\\bar{x}_j - n\\bar{x}_j = 0\n$$\n这意味着对于中心化矩阵 $X_c$，其对应的格拉姆矩阵 $\\tilde{G}_c$ 的非对角块将变为零矩阵。右上角分块 $\\mathbf{1}^{\\top}X_c$ 是一个 $1 \\times p$ 的零向量，左下角分块 $X_c^{\\top}\\mathbf{1}$ 是一个 $p \\times 1$ 的零向量。\n\n因此，中心化数据的格拉姆矩阵变为块对角矩阵：\n$$\n\\tilde{G}_c = \\begin{pmatrix} n  \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1}  X_c^{\\top}X_c \\end{pmatrix}\n$$\n块对角矩阵的逆是各分块的逆组成的块对角矩阵：\n$$\n\\tilde{G}_c^{-1} = \\begin{pmatrix} n^{-1}  \\mathbf{0}_{1 \\times p} \\\\ \\mathbf{0}_{p \\times 1}  (X_c^{\\top}X_c)^{-1} \\end{pmatrix}\n$$\n在线性回归中，估计系数向量（截距和斜率）由 $\\hat{\\boldsymbol{\\beta}} = (\\tilde{X}^{\\top}\\tilde{X})^{-1}\\tilde{X}^{\\top}\\mathbf{y}$ 给出。$\\tilde{G}_c^{-1}$ 的块对角结构意味着截距项的估计与斜率系数的估计变得解耦。截距的估计仅依赖于左上角的分块，而斜率的估计仅依赖于右下角的分块。这既简化了计算，也简化了对模型系数的解释。\n\n**第三部分：$\\tilde{G}^{-1}$ 的 (1,1) 元素的推导**\n\n我们希望求出 $\\tilde{G}^{-1}$ 的第一行第一列的元素。该元素是 $\\tilde{G}^{-1}$ 第一列的第一个分量。设 $\\tilde{G}^{-1}$ 的第一列为向量 $\\mathbf{m}_1$。根据矩阵逆的定义，该列是线性系统 $\\tilde{G}\\mathbf{m}_1 = \\mathbf{e}_1$ 的解，其中 $\\mathbf{e}_1$ 是第一个标准基向量，即 $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$。\n\n我们将向量 $\\mathbf{m}_1$ 与 $\\tilde{G}$ 的分块结构相容地进行分块。由于 $\\tilde{G}$ 是一个 $(p+1) \\times (p+1)$ 矩阵，$\\mathbf{m}_1$ 是一个 $(p+1) \\times 1$ 向量。我们将其分块为 $\\mathbf{m}_1 = \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix}$，其中 $m_{11}$ 是一个标量（我们正在寻找的 (1,1) 元素），$\\mathbf{g}$ 是一个 $p \\times 1$ 向量。向量 $\\mathbf{e}_1$ 分块为 $\\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}$。\n\n该线性系统的分块形式为：\n$$\n\\begin{pmatrix} n  \\mathbf{1}^{\\top}X \\\\ X^{\\top}\\mathbf{1}  X^{\\top}X \\end{pmatrix} \\begin{pmatrix} m_{11} \\\\ \\mathbf{g} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\mathbf{0}_{p \\times 1} \\end{pmatrix}\n$$\n这展开为两个方程：\n1. $n \\cdot m_{11} + (\\mathbf{1}^{\\top}X)\\mathbf{g} = 1$\n2. $(X^{\\top}\\mathbf{1})m_{11} + (X^{\\top}X)\\mathbf{g} = \\mathbf{0}_{p \\times 1}$\n\n从第二个方程，我们求解 $\\mathbf{g}$。假设 $X^{\\top}X$ 是可逆的（如果 $X$ 具有满列秩，则该假设成立）：\n$$\n(X^{\\top}X)\\mathbf{g} = -(X^{\\top}\\mathbf{1})m_{11} \\implies \\mathbf{g} = -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11}\n$$\n现在，我们将 $\\mathbf{g}$ 的这个表达式代入第一个方程：\n$$\nn \\cdot m_{11} + (\\mathbf{1}^{\\top}X) \\left( -(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})m_{11} \\right) = 1\n$$\n由于 $m_{11}$ 是一个标量，我们可以将其因子提出：\n$$\nm_{11} \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right) = 1\n$$\n解出 $m_{11}$，即 $\\tilde{G}^{-1}$ 的 (1,1) 元素，我们得到闭式表达式：\n$$\n(\\tilde{G}^{-1})_{11} = m_{11} = \\left( n - (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) \\right)^{-1}\n$$\n\n**第四部分：对特定矩阵 $X$ 进行计算**\n\n给定 $n=4$ 和 $X=\\begin{pmatrix} 0  1\\\\ 1  1\\\\ 2  3\\\\ 3  4 \\end{pmatrix}$。我们计算上述推导公式所需的各部分。\n\n- $X$ 的列和向量是：\n  $\\mathbf{1}^{\\top}X = \\begin{pmatrix} 0+1+2+3  1+1+3+4 \\end{pmatrix} = \\begin{pmatrix} 6  9 \\end{pmatrix}$。\n- 其转置是 $X^{\\top}\\mathbf{1} = \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix}$。\n\n- 矩阵 $X^{\\top}X$ 是：\n  $X^{\\top}X = \\begin{pmatrix} 0  1  2  3 \\\\ 1  1  3  4 \\end{pmatrix} \\begin{pmatrix} 0  1\\\\ 1  1\\\\ 2  3\\\\ 3  4 \\end{pmatrix} = \\begin{pmatrix} 0+1+4+9  0+1+6+12 \\\\ 0+1+6+12  1+1+9+16 \\end{pmatrix} = \\begin{pmatrix} 14  19 \\\\ 19  27 \\end{pmatrix}$。\n\n- 我们需要 $X^{\\top}X$ 的逆。对于一个 $2 \\times 2$ 矩阵 $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$，其逆为 $\\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$。\n  $X^{\\top}X$ 的行列式是 $\\det(X^{\\top}X) = (14)(27) - (19)(19) = 378 - 361 = 17$。\n  其逆为 $(X^{\\top}X)^{-1} = \\frac{1}{17}\\begin{pmatrix} 27  -19 \\\\ -19  14 \\end{pmatrix}$。\n\n- 现在我们计算二次型 $(\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1})$：\n  $$\n  \\begin{align*}\n  (\\mathbf{1}^{\\top}X)(X^{\\top}X)^{-1}(X^{\\top}\\mathbf{1}) = \\begin{pmatrix} 6  9 \\end{pmatrix} \\left( \\frac{1}{17}\\begin{pmatrix} 27  -19 \\\\ -19  14 \\end{pmatrix} \\right) \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} \\\\\n  = \\frac{1}{17} \\begin{pmatrix} 6  9 \\end{pmatrix} \\begin{pmatrix} 27  -19 \\\\ -19  14 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} \\\\\n  = \\frac{1}{17} \\begin{pmatrix} 6(27)+9(-19)  6(-19)+9(14) \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} \\\\\n  = \\frac{1}{17} \\begin{pmatrix} 162-171  -114+126 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} \\\\\n  = \\frac{1}{17} \\begin{pmatrix} -9  12 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 9 \\end{pmatrix} \\\\\n  = \\frac{1}{17}(-9(6) + 12(9)) = \\frac{1}{17}(-54 + 108) = \\frac{54}{17}\n  \\end{align*}\n  $$\n\n- 最后，我们计算 $\\tilde{G}^{-1}$ 的 (1,1) 元素：\n  $$\n  (\\tilde{G}^{-1})_{11} = \\left( n - \\frac{54}{17} \\right)^{-1} = \\left( 4 - \\frac{54}{17} \\right)^{-1}\n  $$\n  $$\n  4 - \\frac{54}{17} = \\frac{4 \\times 17}{17} - \\frac{54}{17} = \\frac{68 - 54}{17} = \\frac{14}{17}\n  $$\n  因此，\n  $$\n  (\\tildeG^{-1})_{11} = \\left(\\frac{14}{17}\\right)^{-1} = \\frac{17}{14}\n  $$", "answer": "$$\n\\boxed{\\frac{17}{14}}\n$$", "id": "3146970"}, {"introduction": "理想的线性模型要求特征之间线性无关，但现实数据中常常出现冗余特征，即所谓的多重共线性问题。这个练习 [@problem_id:3146927] 构建了一个极限情况——一个特征被完全复制——来让你亲手探究这种数据冗余如何导致格拉姆矩阵 $X^\\top X$ 变得奇异（不可逆）。通过分析，你将深刻理解为何标准最小二乘法此时会失效，并从矩阵特征值的角度认识到奇异性的本质，同时探索岭回归等正则化方法如何通过简单的矩阵加法巧妙地“修复”奇异矩阵，从而保证模型解的稳定性和唯一性。", "problem": "在统计学习的线性回归中，令 $X \\in \\mathbb{R}^{n \\times p}$ 表示一个设计矩阵，其列为 $\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1},\\mathbf{x}_p$，其中 $\\mathbf{x}_p = \\mathbf{x}_1$ 且 $\\mathbf{x}_1 \\neq \\mathbf{0}$。考虑格拉姆矩阵 (Gram matrix) $G = X^\\top X$，并回想普通最小二乘 (OLS) 估计量在逆存在时定义为 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$，而岭估计量对任意 $\\lambda  0$ 定义为 $\\hat{\\boldsymbol{\\beta}}_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top \\mathbf{y}$。仅使用矩阵转置、乘法、可逆性和线性无关的核心定义，判断关于复制一个特征对 $G$ 及其可逆性的影响的陈述中，哪些是正确的。\n\n选择所有适用的选项：\n\nA. $G$ 是奇异的，且 $G$ 至少有一个特征值等于 $0$。\n\nB. 在没有正则化的情况下，对于所有 $\\mathbf{y} \\in \\mathbb{R}^n$，OLS 解是唯一的。\n\nC. 对于任何 $\\lambda  0$，即使当 $\\mathbf{x}_p = \\mathbf{x}_1$ 时，矩阵 $X^\\top X + \\lambda I$ 也是可逆的。\n\nD. 复制一列总是会使 $G$ 的每个特征值增加相同的正常数。\n\nE. 如果 $\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1}\\}$ 线性无关，那么从 $X$ 中移除 $\\mathbf{x}_1$ 或 $\\mathbf{x}_p$ 都会得到一个满列秩的矩阵。", "solution": "首先对问题陈述进行严格的验证过程。\n\n### 步骤 1：提取已知条件\n-   $X \\in \\mathbb{R}^{n \\times p}$ 是一个设计矩阵。\n-   $X$ 的列表示为 $\\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_p$。\n-   给出了一个特定的线性相关条件：$\\mathbf{x}_p = \\mathbf{x}_1$。\n-   给出了一个非平凡条件：$\\mathbf{x}_1 \\neq \\mathbf{0}$，其中 $\\mathbf{0}$ 是 $\\mathbb{R}^n$ 中的零向量。\n-   格拉姆矩阵定义为 $G = X^\\top X$。\n-   普通最小二乘 (OLS) 估计量在逆存在的情况下定义为 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$。\n-   岭估计量对任意标量 $\\lambda  0$ 定义为 $\\hat{\\boldsymbol{\\beta}}_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top \\mathbf{y}$。\n-   任务是仅使用矩阵运算和线性无关的定义来评估关于 $G$ 及其可逆性的几个陈述。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题定义明确，在统计学习和线性代数领域中有坚实的理论基础。为设计矩阵、格拉姆矩阵、OLS 和岭回归提供的定义都是标准的。核心条件 $\\mathbf{x}_p = \\mathbf{x}_1$ 代表了完全多重共线性的情况，这是回归分析中的一个经典主题。该问题是客观、自洽的，并且不违反任何科学或数学原理。这是一个有效且适定 (well-posed) 的问题。\n\n### 步骤 3：结论与行动\n问题陈述是**有效的**。现在将对每个选项进行详细的求解和评估。\n\n### 从第一性原理推导\n问题的核心在于条件 $\\mathbf{x}_p = \\mathbf{x}_1$。这意味着矩阵 $X$ 的列不是线性无关的。如果存在不全为零的标量 $c_1, \\dots, c_k$，使得 $c_1\\mathbf{v}_1 + \\dots + c_k\\mathbf{v}_k = \\mathbf{0}$，则向量集 $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_k\\}$ 是线性相关的。\n\n对于 $X = [\\mathbf{x}_1, \\dots, \\mathbf{x}_p]$ 的列，我们有线性组合：\n$$\n1 \\cdot \\mathbf{x}_1 + 0 \\cdot \\mathbf{x}_2 + \\dots + 0 \\cdot \\mathbf{x}_{p-1} + (-1) \\cdot \\mathbf{x}_p = \\mathbf{x}_1 - \\mathbf{x}_p = \\mathbf{0}\n$$\n这可以用矩阵形式表示为 $X\\mathbf{c} = \\mathbf{0}$，其中 $\\mathbf{c}$ 是 $\\mathbb{R}^p$ 中的一个非零向量，由 $\\mathbf{c} = [1, 0, \\dots, 0, -1]^\\top$ 给出。\n\n存在一个非零向量 $\\mathbf{c}$ 使得 $X\\mathbf{c} = \\mathbf{0}$，意味着 $X$ 的零空间 (null space) 是非平凡的。这直接意味着 $X$ 的列是线性相关的，因此 $X$ 不具有满列秩，即 $\\text{rank}(X)  p$。\n\n现在考虑格拉姆矩阵 $G = X^\\top X$。秩的一个基本性质是 $\\text{rank}(X^\\top X) = \\text{rank}(X)$。因此，$\\text{rank}(G)  p$。由于 $G$ 是一个 $p \\times p$ 矩阵，秩小于 $p$ 意味着 $G$ 是奇异的（即不可逆）。\n\n此外，一个方阵是奇异的当且仅当它有一个等于 $0$ 的特征值。我们可以直接证明这一点。使用上面的向量 $\\mathbf{c}$：\n$$\nG\\mathbf{c} = (X^\\top X)\\mathbf{c} = X^\\top (X\\mathbf{c}) = X^\\top \\mathbf{0} = \\mathbf{0}\n$$\n由于 $\\mathbf{c} \\neq \\mathbf{0}$ 且 $G\\mathbf{c} = \\mathbf{0} = 0 \\cdot \\mathbf{c}$，根据特征值和特征向量的定义，$\\mathbf{c}$ 是 $G$ 的一个特征向量，其对应的特征值为 $0$。\n\n### 逐项分析\n\n**A. $G$ 是奇异的，且 $G$ 至少有一个特征值等于 $0$。**\n如上所述，$X$ 的列的线性相关性意味着格拉姆矩阵 $G = X^\\top X$ 是奇异的。一个方阵的奇异性等价于它至少有一个等于 $0$ 的特征值。我们的推导明确地表明 $G$ 有一个特征值为 $0$。因此，这个陈述是问题设定的直接结果。\n**结论：正确。**\n\n**B. 在没有正则化的情况下，对于所有 $\\mathbf{y} \\in \\mathbb{R}^n$，OLS 解是唯一的。**\nOLS 解由正规方程 $(X^\\top X)\\hat{\\boldsymbol{\\beta}} = X^\\top \\mathbf{y}$ 决定。对于任何 $X^\\top \\mathbf{y}$，$\\hat{\\boldsymbol{\\beta}}$ 的唯一解存在的充要条件是矩阵 $X^\\top X = G$ 可逆。根据对选项 A 的分析，$G$ 是奇异的。因此，它的逆 $(X^\\top X)^{-1}$ 不存在。这个方程组对于 $\\hat{\\boldsymbol{\\beta}}$ 要么没有解，要么有无穷多解，但绝不会有唯一解。问题陈述中给出的 OLS 估计量公式 $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$ 明确地以逆矩阵存在为条件，而这里不满足这个条件。\n**结论：错误。**\n\n**C. 对于任何 $\\lambda  0$，即使当 $\\mathbf{x}_p = \\mathbf{x}_1$ 时，矩阵 $X^\\top X + \\lambda I$ 也是可逆的。**\n令 $M = X^\\top X + \\lambda I$。矩阵 $G = X^\\top X$ 是一个格拉姆矩阵，它总是半正定的。这意味着对于任何非零向量 $\\mathbf{v} \\in \\mathbb{R}^p$，有 $\\mathbf{v}^\\top G \\mathbf{v} = \\mathbf{v}^\\top X^\\top X \\mathbf{v} = (X\\mathbf{v})^\\top (X\\mathbf{v}) = \\|X\\mathbf{v}\\|_2^2 \\ge 0$。这也意味着 $G$ 的所有特征值都是非负的。设 $G$ 的特征值为 $\\mu_1, \\dots, \\mu_p$，其中对于所有 $i \\in \\{1, \\dots, p\\}$ 都有 $\\mu_i \\ge 0$。\n矩阵 $M = G + \\lambda I$ 的特征值为 $\\mu_1+\\lambda, \\dots, \\mu_p+\\lambda$。\n因为给定 $\\lambda  0$ 且我们知道 $\\mu_i \\ge 0$，所以 $M$ 的每个特征值都是严格为正的：$\\mu_i + \\lambda > 0$。\n一个矩阵是可逆的，当且仅当它的所有特征值都不为零。由于 $M$ 的所有特征值都是严格为正的，因此对于 $\\lambda  0$，$M = X^\\top X + \\lambda I$ 总是可逆的。这一点与 $X^\\top X$ 是否奇异无关。\n**结论：正确。**\n\n**D. 复制一列总是会使 $G$ 的每个特征值增加相同的正常数。**\n这个陈述的表述不精确，因为它比较了不同维度矩阵的特征值。我们将其解释为比较复制前后格拉姆矩阵的特征值。设 $X_{p-1}$ 是以 $\\{\\mathbf{x}_1, \\dots, \\mathbf{x}_{p-1}\\}$ 为列的矩阵，其 $(p-1) \\times (p-1)$ 的格拉姆矩阵为 $G_{p-1} = X_{p-1}^\\top X_{p-1}$。矩阵 $X$ 是通过将 $\\mathbf{x}_1$ 作为第 $p$ 列添加而形成的。得到的格拉姆矩阵 $G$ 是 $p \\times p$ 的。\n考虑一个简单的反例。设 $p=2, n=1$，且 $\\mathbf{x}_1 = [2]$。复制前的矩阵是 $X_1 = [2]$。其格拉姆矩阵是 $G_1 = X_1^\\top X_1 = [4]$。唯一的特征值是 $4$。\n复制该列后，矩阵为 $X = [2, 2]$。格拉姆矩阵为 $G_2 = X^\\top X = \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} [2, 2] = \\begin{pmatrix} 4  4 \\\\ 4  4 \\end{pmatrix}$。\n$G_2$ 的特征值是特征方程 $\\det(G_2 - \\mu I) = 0$ 的根：\n$$\n\\det\\begin{pmatrix} 4-\\mu  4 \\\\ 4  4-\\mu \\end{pmatrix} = (4-\\mu)^2 - 16 = 0 \\implies (4-\\mu)^2 = 16 \\implies 4-\\mu = \\pm 4\n$$\n特征值为 $\\mu = 0$ 和 $\\mu = 8$。\n原始特征值集合是 $\\{4\\}$，新的集合是 $\\{0, 8\\}$。一个特征值从 $4$ 减小到 $0$，并且出现了一个新的、更大的特征值。陈述中“每个特征值”都增加的说法是错误的。\n**结论：错误。**\n\n**E. 如果 $\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1}\\}$ 线性无关，那么从 $X$ 中移除 $\\mathbf{x}_1$ 或 $\\mathbf{x}_p$ 都会得到一个满列秩的矩阵。**\n矩阵 $X$ 的列为 $[\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_p]$。我们已知 $\\mathbf{x}_p = \\mathbf{x}_1$。\n前提是向量集 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}\\}$ 是线性无关的。\n情况 1：移除列 $\\mathbf{x}_p$。得到的矩阵是 $X' = [\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}]$。根据前提，$X'$ 的列是线性无关的。一个列向量线性无关的矩阵具有满列秩。所以，$X'$ 具有满列秩。\n情况 2：移除列 $\\mathbf{x}_1$。得到的矩阵是 $X'' = [\\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_p]$。代入 $\\mathbf{x}_p = \\mathbf{x}_1$，我们得到 $X'' = [\\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_1]$。$X''$ 的列集合是 $\\{\\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}, \\mathbf{x}_1\\}$，这仅仅是 $X'$ 列集合 $\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_{p-1}\\}$ 的一个置换。一个向量集的线性无关性不依赖于它们的顺序。由于 $X'$ 的列是线性无关的（根据前提），所以 $X''$ 的列也是线性无关的。因此，$X''$ 也具有满列秩。\n该陈述对两种情况都成立。\n**结论：正确。**", "answer": "$$\\boxed{ACE}$$", "id": "3146927"}, {"introduction": "标准最小二乘法在无约束的情况下寻找最佳拟合，但在许多实际应用中，我们可能需要根据先验知识或特定要求，对模型系数施加线性约束。此练习 [@problem_id:3146942] 将引导你运用拉格朗日乘子法这一强大的优化工具，来解决带等式约束的最小二乘问题。你将学习如何构建和求解KKT（Karush-Kuhn-Tucker）条件所导出的分块矩阵系统，从而推导出约束下的最优解。这个过程不仅能让你熟练运用分块矩阵求逆等高级技巧，更能让你体会到线性代数是如何将复杂的约束优化问题转化为一个清晰、可解的矩阵方程。", "problem": "考虑一个线性回归模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。约束最小二乘估计量 $\\hat{\\beta}$ 在满足线性等式约束的条件下，最小化平方误差。从最小二乘目标和等式约束的定义出发，构造拉格朗日函数，并推导出一阶最优性条件，即 Karush-Kuhn-Tucker (KKT) 条件，其中 Karush-Kuhn-Tucker (KKT) 指的是约束优化中的最优性必要条件。然后，使用分块矩阵求逆技术（具体来说，是 Schur 补论证）求解所得的线性系统，以获得 $\\hat{\\beta}$ 的解析表达式。最后，对以下特定数据计算该表达式的值：\n$$\nX = \\begin{bmatrix}\n1  0 \\\\\n1  1 \\\\\n1  2\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{bmatrix}, \\quad\nC = \\begin{bmatrix}\n1  1\n\\end{bmatrix}, \\quad\nd = \\begin{bmatrix}\n1\n\\end{bmatrix}.\n$$\n计算约束最小二乘估计量的第二个分量，记为 $ \\hat{\\beta}_{2} $。最终答案必须是一个实数。请勿四舍五入。", "solution": "该问题要求推导约束最小二乘估计量，并对一个特定的数据集进行评估。\n\n首先，我将验证问题陈述。\n### 步骤 1：提取已知条件\n- 一个线性回归模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^{n}$。\n- 估计量 $\\hat{\\beta}$ 最小化平方误差 $(y - X\\beta)^T(y - X\\beta)$。\n- 该估计量受线性等式约束 $C\\beta = d$ 的限制。\n- 任务是：\n    1. 构造拉格朗日函数。\n    2. 推导 Karush-Kuhn-Tucker (KKT) 条件。\n    3. 使用分块矩阵求逆技术求解所得的线性系统以得到 $\\hat{\\beta}$。\n    4. 对以下特定数据计算该表达式的值：\n    $$\n    X = \\begin{bmatrix}\n    1  0 \\\\\n    1  1 \\\\\n    1  2\n    \\end{bmatrix}, \\quad\n    y = \\begin{bmatrix}\n    1 \\\\\n    2 \\\\\n    3\n    \\end{bmatrix}, \\quad\n    C = \\begin{bmatrix}\n    1  1\n    \\end{bmatrix}, \\quad\n    d = \\begin{bmatrix}\n    1\n    \\end{bmatrix}.\n    $$\n- 最终输出应为估计量的第二个分量 $\\hat{\\beta}_{2}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据：** 该问题是统计学习（线性回归）背景下约束优化的一个标准练习。拉格朗日函数、KKT 条件和分块矩阵求逆的使用是基础且成熟的数学技术。该问题是科学合理的。\n- **适定性：** 问题提供了所有必需的矩阵和向量（$X, y, C, d$）。维度是一致的：$X$ 是 $3 \\times 2$ ($n=3, p=2$)，$y$ 是 $3 \\times 1$，$\\beta$ 是 $2 \\times 1$，$C$ 是 $1 \\times 2$，$d$ 是 $1 \\times 1$。由于 $X$ 的列是线性无关的，矩阵 $X^T X$ 是可逆的，并且约束矩阵 $C$ 具有满行秩。这些条件确保了唯一解的存在。\n- **客观性：** 问题使用精确、形式化的数学语言陈述，没有任何主观性或模糊性。\n\n### 步骤 3：结论与行动\n问题是有效的，因为它是科学合理的、适定的和客观的。我将继续进行求解。\n\n约束最小二乘问题可以表述为：\n$$\n\\text{minimize} \\quad f(\\beta) = \\|y - X\\beta\\|_2^2 = (y - X\\beta)^T(y - X\\beta) \\\\\n\\text{subject to} \\quad C\\beta = d\n$$\n为了解决这个问题，我们构造拉格朗日函数 $\\mathcal{L}(\\beta, \\lambda)$，其中 $\\lambda$ 是拉格朗日乘子向量。\n$$\n\\mathcal{L}(\\beta, \\lambda) = (y - X\\beta)^T(y - X\\beta) + \\lambda^T(C\\beta - d)\n$$\n展开目标函数项：\n$$\n(y - X\\beta)^T(y - X\\beta) = y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta\n$$\n由于 $\\beta^T X^T y$ 是一个标量，它等于其转置 $(y^T X \\beta)^T$。因此，$y^T X \\beta = \\beta^T X^T y$。拉格朗日函数变为：\n$$\n\\mathcal{L}(\\beta, \\lambda) = y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda^T C\\beta - \\lambda^T d\n$$\n通过将拉格朗日函数对 $\\beta$ 和 $\\lambda$ 的梯度设置为零，可以找到一阶最优性条件（KKT 条件）。\n\n关于 $\\beta$ 的梯度：\n$$\n\\nabla_{\\beta} \\mathcal{L}(\\beta, \\lambda) = \\frac{\\partial}{\\partial \\beta} (y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda^T C\\beta - \\lambda^T d) = 0\n$$\n$$\n-2X^T y + 2X^T X \\beta + C^T \\lambda = 0\n$$\n$$\n2X^T X \\beta + C^T \\lambda = 2X^T y \\quad (1)\n$$\n关于 $\\lambda$ 的梯度：\n$$\n\\nabla_{\\lambda} \\mathcal{L}(\\beta, \\lambda) = \\frac{\\partial}{\\partial \\lambda} (\\dots + \\lambda^T(C\\beta - d)) = 0\n$$\n$$\nC\\beta - d = 0 \\implies C\\beta = d \\quad (2)\n$$\n这两个方程构成一个线性方程组，可以表示为分块矩阵形式：\n$$\n\\begin{bmatrix}\n2X^T X  C^T \\\\\nC  0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta \\\\\n\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2X^T y \\\\\nd\n\\end{bmatrix}\n$$\n为了求解 $\\beta$，我们可以使用代数替换法，这等价于应用从 Schur 补导出的分块矩阵求逆公式。从方程（1）出发，假设 $X^T X$ 是可逆的：\n$$\n2X^T X \\beta = 2X^T y - C^T \\lambda\n$$\n$$\n\\beta = (2X^T X)^{-1}(2X^T y - C^T \\lambda) = (X^T X)^{-1}X^T y - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda\n$$\n令 $\\hat{\\beta}_{OLS} = (X^T X)^{-1}X^T y$ 为普通最小二乘（无约束）解。\n$$\n\\beta = \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda\n$$\n将这个 $\\beta$ 的表达式代入方程（2）：\n$$\nC \\left( \\hat{\\beta}_{OLS} - \\frac{1}{2}(X^T X)^{-1}C^T \\lambda \\right) = d\n$$\n$$\nC\\hat{\\beta}_{OLS} - \\frac{1}{2} C(X^T X)^{-1}C^T \\lambda = d\n$$\n我们求解 $\\lambda$，假设矩阵 $C(X^T X)^{-1}C^T$ 是可逆的：\n$$\n\\frac{1}{2} C(X^T X)^{-1}C^T \\lambda = C\\hat{\\beta}_{OLS} - d\n$$\n$$\n\\lambda = 2 \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n最后，我们将 $\\lambda$ 代回到 $\\beta$ 的表达式中，得到约束估计量 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - (X^T X)^{-1}C^T \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n这就是约束最小二乘估计量 $\\hat{\\beta}$ 的解析表达式。\n\n现在，我们对给定数据计算该表达式的值：\n$$\nX = \\begin{bmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix}, \\quad y = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1  1 \\end{bmatrix}, \\quad d = \\begin{bmatrix} 1 \\end{bmatrix}\n$$\n首先，计算 $X^T X$：\n$$\nX^T X = \\begin{bmatrix} 1  1  1 \\\\ 0  1  2 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 1  1 \\\\ 1  2 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1+1 \\cdot 1+1 \\cdot 1  1 \\cdot 0+1 \\cdot 1+1 \\cdot 2 \\\\ 0 \\cdot 1+1 \\cdot 1+2 \\cdot 1  0 \\cdot 0+1 \\cdot 1+2 \\cdot 2 \\end{bmatrix} = \\begin{bmatrix} 3  3 \\\\ 3  5 \\end{bmatrix}\n$$\n接下来，计算逆矩阵 $(X^T X)^{-1}$：\n$$\n\\det(X^T X) = 3 \\cdot 5 - 3 \\cdot 3 = 15 - 9 = 6\n$$\n$$\n(X^T X)^{-1} = \\frac{1}{6} \\begin{bmatrix} 5  -3 \\\\ -3  3 \\end{bmatrix} = \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix}\n$$\n接下来，计算 $X^T y$：\n$$\nX^T y = \\begin{bmatrix} 1  1  1 \\\\ 0  1  2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 1 \\cdot 1+1 \\cdot 2+1 \\cdot 3 \\\\ 0 \\cdot 1+1 \\cdot 2+2 \\cdot 3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix}\n$$\n现在，计算无约束估计量 $\\hat{\\beta}_{OLS}$：\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1}X^T y = \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix} \\begin{bmatrix} 6 \\\\ 8 \\end{bmatrix} = \\begin{bmatrix} (5/6) \\cdot 6 + (-1/2) \\cdot 8 \\\\ (-1/2) \\cdot 6 + (1/2) \\cdot 8 \\end{bmatrix} = \\begin{bmatrix} 5 - 4 \\\\ -3 + 4 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n$$\n现在我们计算公式中修正部分所需的各项。\n$$\nC(X^T X)^{-1}C^T = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/6-1/2  -1/2+1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 2/6  0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{3}\n$$\n$$\n\\left( C(X^T X)^{-1}C^T \\right)^{-1} = \\left( \\frac{1}{3} \\right)^{-1} = 3\n$$\n此外，我们还需要 $C\\hat{\\beta}_{OLS} - d$：\n$$\nC\\hat{\\beta}_{OLS} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1+1=2\n$$\n$$\nC\\hat{\\beta}_{OLS} - d = 2 - 1 = 1\n$$\n以及项 $(X^T X)^{-1}C^T$：\n$$\n(X^T X)^{-1}C^T = \\begin{bmatrix} 5/6  -1/2 \\\\ -1/2  1/2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/6 - 1/2 \\\\ -1/2 + 1/2 \\end{bmatrix} = \\begin{bmatrix} 2/6 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1/3 \\\\ 0 \\end{bmatrix}\n$$\n将所有部分组合起来，求出 $\\hat{\\beta}$：\n$$\n\\hat{\\beta} = \\hat{\\beta}_{OLS} - (X^T X)^{-1}C^T \\left( C(X^T X)^{-1}C^T \\right)^{-1} (C\\hat{\\beta}_{OLS} - d)\n$$\n$$\n\\hat{\\beta} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1/3 \\\\ 0 \\end{bmatrix} (3) (1) = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n$$\n约束最小二乘估计量为 $\\hat{\\beta} = \\begin{bmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$。\n问题要求的是第二个分量 $\\hat{\\beta}_2$。\n$$\n\\hat{\\beta}_2 = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "3146942"}]}