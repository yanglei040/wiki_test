{"hands_on_practices": [{"introduction": "许多统计学习问题，例如著名的LASSO回归，都涉及到一个复合目标函数，它由一个光滑的数据拟合项和一个非光滑的正则化项（如 $L_1$ 范数）组成。近端梯度法 (Proximal Gradient Method) 是解决此类问题的强大工具。在这个实践中，你将亲手实现该算法，并经验性地验证其收敛速度如何受到问题是否强凸的影响，从而将理论收敛率与实际数据属性联系起来。[@problem_id:3126043]", "problem": "考虑统计学习中的复合优化问题，其目标是一个光滑凸项和一个凸非光滑正则化项之和。设 $f(w) = g(w) + h(w)$，其中 $w \\in \\mathbb{R}^n$，$g$ 是可微的凸函数且其梯度是 Lipschitz 连续的，$h$ 是凸函数，可能非光滑但其邻近算子是可计算的。具体来说，您将研究以下情况：\n$$\ng(w) = \\tfrac{1}{2}\\lVert A w - b \\rVert_2^2,\\qquad h(w) = \\lambda \\lVert w \\rVert_1,\n$$\n以下述基本定义和事实为出发点：\n- 光滑项的梯度为 $\\nabla g(w) = A^\\top (A w - b)$。\n- 如果存在 $L \\ge 0$ 使得对于所有的 $u,v$ 都有 $\\lVert \\nabla g(u) - \\nabla g(v) \\rVert_2 \\le L \\lVert u - v \\rVert_2$，则 $g$ 的梯度是 $L$-Lipschitz 的。对于上述二次函数 $g$，最小的有效 $L$ 等于 $A^\\top A$ 的谱范数，即 $L = \\lambda_{\\max}(A^\\top A)$。\n- 如果对于所有的 $u,v$ 都有 $g(v) \\ge g(u) + \\nabla g(u)^\\top (v-u) + \\tfrac{\\mu}{2} \\lVert v-u \\rVert_2^2$，则函数 $g$ 是 $\\mu$-强凸的。对于上述二次函数 $g$，最大的有效 $\\mu$ 等于 $A^\\top A$ 的最小特征值，即 $\\mu = \\lambda_{\\min}(A^\\top A)$，当且仅当 $A$ 是列满秩时，该值严格为正。\n- 对于步长 $\\alpha > 0$，$h$ 的邻近映射为 $\\mathrm{prox}_{\\alpha h}(v) = \\arg\\min_{u \\in \\mathbb{R}^n} \\left\\{ h(u) + \\tfrac{1}{2\\alpha} \\lVert u - v \\rVert_2^2 \\right\\}$。\n\n您将实现固定步长为 $\\alpha = 1/L$ 的邻近梯度法，每次迭代如下：\n$$\nw^{k+1} = \\mathrm{prox}_{\\alpha h}\\big(w^k - \\alpha \\nabla g(w^k)\\big).\n$$\n对于 $h(w) = \\lambda \\lVert w \\rVert_1$，邻近映射是坐标级软阈值，这是一种闭式运算。\n\n您的任务：\n1. 对于每个测试用例，计算 $L = \\lambda_{\\max}(A^\\top A)$ 和 $\\mu = \\lambda_{\\min}(A^\\top A)$。\n2. 从给定的初始点 $w^0$ 开始，实现步长为 $\\alpha = 1/L$ 的邻近梯度法。\n3. 通过运行大量迭代使算法接近稳态，从而获得一个高精度的参考解 $w^\\star$，并令 $f^\\star = f(w^\\star)$。\n4. 按如下方式分析收敛性：\n   - 如果 $\\mu > 0$，通过计算迭代序列尾部片段的比率中位数来估计经验线性因子：\n     $$\n     r_k = \\frac{\\lVert w^{k+1} - w^\\star \\rVert_2}{\\lVert w^{k} - w^\\star \\rVert_2},\n     $$\n     跳过分母为零的索引。将此中位数表示为 $q_{\\text{emp}}$。同时，计算在 $g$ 的 $L$-Lipschitz 连续性和 $\\mu$-强凸性以及步长 $\\alpha = 1/L$ 的条件下，从邻近梯度的标准分析得出的理论预测最坏情况因子 $q_{\\text{th}}$。您的程序应报告经验因子是否在一个小的容差范围内不超过理论预测，以布尔值的形式编码。\n   - 如果 $\\mu = 0$，将 $q_{\\text{th}} = -1.0$ 和 $q_{\\text{emp}} = -1.0$ 设置为哨兵值，并通过构建尾部序列 $c_k = k \\cdot (f(w^k) - f^\\star)$ 并检查它在大多数步骤中是否在小的数值容差内基本非增来验证次线性行为；将此报告为布尔值。\n\n为加强科学真实性和可测试性的实现细节：\n- 对 $\\ell_1$-范数的邻近映射使用软阈值公式。\n- 全程使用欧几里得范数 $\\lVert \\cdot \\rVert_2$。\n- 为近似 $w^\\star$，从给定的 $w^0$ 开始，迭代该方法 $N_\\star = 20000$ 步。\n- 为进行收敛性诊断，从相同的初始点 $w^0$ 运行 $K = 800$ 次迭代，并仅使用最后 $T = 200$ 步来计算经验因子或进行次线性检查。\n- 对于强凸性检查，接受 $q_{\\text{emp}} \\le q_{\\text{th}} + \\varepsilon_{\\text{lin}}$（其中 $\\varepsilon_{\\text{lin}} = 10^{-2}$）为成功。\n- 对于 $\\mu = 0$ 时的次线性检查，如果至少有比例为 $p = 0.95$ 的连续尾部差分满足 $c_{k+1} \\le c_k + \\varepsilon_{\\text{sub}}$（其中 $\\varepsilon_{\\text{sub}} = 10^{-9}$），则认为序列 $\\{c_k\\}$ 是基本非增的。\n- 如果从特征值计算出的 $\\mu$ 小于数值下限 $\\delta = 10^{-12}$，则在决定是 $\\mu = 0$ 分支还是 $\\mu > 0$ 分支时将其视为零。\n\n测试套件：\n为以下三种情况提供结果，每种情况由 $(A,b,\\lambda,w^0)$ 的明确数值条目指定。所有数字都是无量纲的。\n- 情况 1（良态且强凸）：\n  $$\n  A = \\begin{bmatrix}\n  3 & 0\\\\\n  0 & 2\\\\\n  0 & 0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  1\\\\\n  -2\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.2,\\quad\n  w^0 = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}.\n  $$\n- 情况 2（秩亏，非强凸）：\n  $$\n  A = \\begin{bmatrix}\n  1 & 0\\\\\n  1 & 0\\\\\n  0 & 0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  1\\\\\n  0\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.1,\\quad\n  w^0 = \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix}.\n  $$\n- 情况 3（病态且强凸）：\n  $$\n  A = \\begin{bmatrix}\n  10 & 0\\\\\n  0 & 1\\\\\n  0 & 0\n  \\end{bmatrix},\\quad\n  b = \\begin{bmatrix}\n  3\\\\\n  -1\\\\\n  0\n  \\end{bmatrix},\\quad\n  \\lambda = 0.05,\\quad\n  w^0 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.\n  $$\n\n所需输出和格式：\n- 对于上述每个案例，按顺序计算并输出五个值的序列\n  $$\n  \\big[L,\\ \\mu,\\ q_{\\text{th}},\\ q_{\\text{emp}},\\ \\text{check}\\big],\n  $$\n  其中 $L$ 和 $\\mu$ 如上定义，$q_{\\text{th}}$ 是 $\\mu > 0$ 时理论预测的最坏情况线性因子（当 $\\mu = 0$ 时为 $-1.0$），$q_{\\text{emp}}$ 是如上定义的经验因子（当 $\\mu = 0$ 时为 $-1.0$），$\\text{check}$ 是如上定义的布尔值。\n- 您的程序应生成一行输出，其中包含跨三个案例的所有结果，连接成一个逗号分隔的列表，并用方括号括起来，顺序如下\n  $$\n  \\big[L_1,\\mu_1,q_{\\text{th},1},q_{\\text{emp},1},\\text{check}_1,\\ L_2,\\mu_2,q_{\\text{th},2},q_{\\text{emp},2},\\text{check}_2,\\ L_3,\\mu_3,q_{\\text{th},3},q_{\\text{emp},3},\\text{check}_3\\big].\n  $$", "solution": "用户希望分析用于 LASSO 型问题的邻近梯度法的收敛性。该问题定义明确，科学上合理，且所有必要的参数和过程都已明确说明。我将继续提供完整的解决方案。\n\n要最小化的目标函数是 $f(w) = g(w) + h(w)$，其中 $w \\in \\mathbb{R}^n$。其组成部分为：\n- 一个光滑、凸的损失项：$g(w) = \\frac{1}{2}\\lVert A w - b \\rVert_2^2$\n- 一个凸、非光滑的正则化项：$h(w) = \\lambda \\lVert w \\rVert_1$\n\n邻近梯度法通过迭代更新权重向量 $w$。第 $k$ 次迭代由下式给出：\n$$\nw^{k+1} = \\mathrm{prox}_{\\alpha h}\\left(w^k - \\alpha \\nabla g(w^k)\\right)\n$$\n其中 $\\alpha > 0$ 是步长。\n\n首先，我们需要这次迭代的组成部分：\n1.  **$g(w)$ 的梯度**：二次损失项 $g(w)$ 的梯度是：\n    $$\n    \\nabla g(w) = A^\\top(A w - b)\n    $$\n2.  **$h(w)$ 的邻近映射**：对于 $h(w) = \\lambda \\lVert w \\rVert_1$，邻近算子是软阈值函数，它逐元素地应用于向量 $v$：\n    $$\n    [\\mathrm{prox}_{\\alpha h}(v)]_i = \\mathrm{sign}(v_i) \\max(|v_i| - \\alpha \\lambda, 0)\n    $$\n    此操作有效地将 $v$ 的分量向零收缩，将小的分量精确地设置为零。\n\n邻近梯度法的收敛速度取决于 $g(w)$ 的性质，特别是其梯度的 Lipschitz 常数 $L$ 和其强凸常数 $\\mu$。\n-   $\\nabla g(w)$ 的 Lipschitz 常数 $L$ 是矩阵 $A^\\top A$ 的最大特征值（谱范数）。我们将其表示为 $L = \\lambda_{\\max}(A^\\top A)$。\n-   $g(w)$ 的强凸常数 $\\mu$ 是 $A^\\top A$ 的最小特征值。我们将其表示为 $\\mu = \\lambda_{\\min}(A^\\top A)$。$g(w)$ 是强凸的当且仅当 $\\mu > 0$，这在 $A$ 具有列满秩时发生。\n\n问题指定了固定步长 $\\alpha = 1/L$。\n\n每个测试用例的解决过程包括以下步骤：\n1.  **计算常数**：给定矩阵 $A$，我们构造 $A^\\top A$ 并计算其特征值以找到 $L = \\lambda_{\\max}(A^\\top A)$ 和 $\\mu = \\lambda_{\\min}(A^\\top A)$。我们应用一个数值下限 $\\delta = 10^{-12}$：如果计算出的 $\\mu$ 小于 $\\delta$，我们将其视为 $0$。\n\n2.  **找到参考解**：我们从给定的初始点 $w^0$ 开始，运行邻近梯度算法大量迭代，即 $N_\\star = 20000$ 次。最终的迭代值被视为真实解 $w^\\star$ 的高精度近似。那么最优函数值为 $f^\\star = f(w^\\star)$。\n\n3.  **生成用于分析的迭代序列**：我们再次从 $w^0$ 开始运行算法 $K = 800$ 次迭代，存储整个迭代序列 $\\{w^0, w^1, \\dots, w^K\\}$。\n\n4.  **分析收敛性**：分析取决于 $\\mu$ 是否大于 $0$。\n\n    **情况 1：强凸 ($\\mu > 0$)**\n    - 步长为 $\\alpha=1/L$ 的邻近梯度下降理论预测迭代值会线性收敛到解 $w^\\star$。到最优解的距离预计在每次迭代中都会减少一个常数因子。\n    - 理论上的最坏情况收敛因子 $q_{\\text{th}}$ 由下式给出：\n      $$\n      q_{\\text{th}} = 1 - \\frac{\\mu}{L}\n      $$\n      因为 $\\mu \\le L$。\n    - 我们从生成的迭代序列的尾部（最后 $T=200$ 步）估计经验收敛因子 $q_{\\text{emp}}$。对于这个尾部的每一步 $k$，我们计算比率：\n      $$\n      r_k = \\frac{\\lVert w^{k+1} - w^\\star \\rVert_2}{\\lVert w^{k} - w^\\star \\rVert_2}\n      $$\n    - 经验因子 $q_{\\text{emp}}$ 是这些比率 $\\{r_k\\}$ 的中位数。\n    - 最终的检查验证经验速率是否与理论一致，即对于容差 $\\varepsilon_{\\text{lin}} = 10^{-2}$，是否有 $q_{\\text{emp}} \\le q_{\\text{th}} + \\varepsilon_{\\text{lin}}$。\n\n    **情况 2：凸，但非强凸 ($\\mu = 0$)**\n    - 当 $g(w)$ 不是强凸时，收敛速度是次线性的。理论预测 $f(w^k) - f^\\star = \\mathcal{O}(1/k)$。\n    - 这意味着序列 $c_k = k \\cdot (f(w^k) - f^\\star)$ 应该是有界的。我们检查一个更具体的性质：这个序列在尾部是否基本非增。\n    - 我们设置哨兵值 $q_{\\text{th}} = -1.0$ 和 $q_{\\text{emp}} = -1.0$。\n    - 对于算法的最后 $T=200$ 步（从 $k=K-T$ 到 $k=K-1$），我们检查条件 $c_{k+1} \\le c_k + \\varepsilon_{\\text{sub}}$ 是否成立，其中 $\\varepsilon_{\\text{sub}} = 10^{-9}$ 是一个小的容差。\n    - 如果这个条件在尾部至少有 $p=0.95$ 的比例的步数中成立，则检查成功（评估为 `True`）。\n\n此过程应用于所提供的三个测试用例中的每一个。最终输出是每个案例的五个计算值（$L, \\mu, q_{\\text{th}}, q_{\\text{emp}}, \\text{check}$）的连接列表。", "answer": "```python\nimport numpy as np\n\n# Global constants from the problem statement\nN_STAR = 20000\nK = 800\nT = 200\nDELTA = 1e-12\nEPSILON_LIN = 1e-2\nEPSILON_SUB = 1e-9\nP = 0.95\n\ndef g(w, A, b):\n    \"\"\"Computes the value of the smooth term g(w).\"\"\"\n    return 0.5 * np.linalg.norm(A @ w - b)**2\n\ndef h(w, lambda_reg):\n    \"\"\"Computes the value of the non-smooth term h(w).\"\"\"\n    return lambda_reg * np.linalg.norm(w, 1)\n\ndef f_obj(w, A, b, lambda_reg):\n    \"\"\"Computes the value of the objective function f(w).\"\"\"\n    return g(w, A, b) + h(w, lambda_reg)\n\ndef grad_g(w, A, b):\n    \"\"\"Computes the gradient of the smooth term g(w).\"\"\"\n    return A.T @ (A @ w - b)\n\ndef prox_l1(v, step_size, lambda_reg):\n    \"\"\"Computes the proximal operator for the L1 norm (soft-thresholding).\"\"\"\n    return np.sign(v) * np.maximum(0, np.abs(v) - step_size * lambda_reg)\n\ndef run_proximal_gradient(w0, A, b, lambda_reg, alpha, n_iters):\n    \"\"\"Runs the proximal gradient algorithm for a given number of iterations.\"\"\"\n    w = np.copy(w0)\n    w_history = [np.copy(w)]\n    for _ in range(n_iters):\n        gradient = grad_g(w, A, b)\n        w_update = w - alpha * gradient\n        w = prox_l1(w_update, alpha, lambda_reg)\n        w_history.append(np.copy(w))\n    return w_history\n\ndef analyze_convergence(iterates, w_star, L, mu, A, b, lambda_reg):\n    \"\"\"Analyzes the convergence of the iterates based on mu.\"\"\"\n    if mu > 0:\n        # Strongly convex case\n        q_th = 1.0 - mu / L\n        \n        # Tail iterates from w^{K-T} to w^K (T+1 iterates)\n        tail_iterates = iterates[K - T : K + 1]\n        ratios = []\n        for i in range(T):\n            w_k = tail_iterates[i]\n            w_k_plus_1 = tail_iterates[i+1]\n            \n            denom = np.linalg.norm(w_k - w_star)\n            if denom > 1e-15:  # Avoid division by zero\n                num = np.linalg.norm(w_k_plus_1 - w_star)\n                ratios.append(num / denom)\n        \n        q_emp = np.median(ratios) if ratios else 0.0\n        check = q_emp = q_th + EPSILON_LIN\n        \n    else:\n        # Convex, but not strongly convex case\n        q_th = -1.0\n        q_emp = -1.0\n        \n        f_star = f_obj(w_star, A, b, lambda_reg)\n        \n        success_count = 0\n        num_comparisons = T\n        \n        # Check c_{k+1} = c_k + eps for k from K-T to K-1\n        for k in range(K - T, K):\n            f_k = f_obj(iterates[k], A, b, lambda_reg)\n            f_k_plus_1 = f_obj(iterates[k+1], A, b, lambda_reg)\n\n            c_k = k * (f_k - f_star)\n            c_k_plus_1 = (k + 1) * (f_k_plus_1 - f_star)\n            \n            if c_k_plus_1 = c_k + EPSILON_SUB:\n                success_count += 1\n                \n        check = (success_count / num_comparisons) >= P\n\n    return q_th, q_emp, check\n\ndef solve_case(A_list, b_list, lambda_reg, w0_list):\n    \"\"\"Solves a single test case from the problem statement.\"\"\"\n    A = np.array(A_list, dtype=float)\n    b = np.array(b_list, dtype=float)\n    w0 = np.array(w0_list, dtype=float)\n\n    # 1. Compute L and mu\n    AtA = A.T @ A\n    eigenvalues = np.linalg.eigvalsh(AtA)\n    L = np.max(eigenvalues) if eigenvalues.size > 0 else 0.0\n    mu_raw = np.min(eigenvalues) if eigenvalues.size > 0 else 0.0\n    mu = mu_raw if mu_raw >= DELTA else 0.0\n    \n    alpha = 1.0 / L if L > 0 else 1.0\n\n    # 2. Compute reference solution w_star\n    w_star_history = run_proximal_gradient(w0, A, b, lambda_reg, alpha, N_STAR)\n    w_star = w_star_history[-1]\n\n    # 3. Generate iterates for analysis\n    iterates = run_proximal_gradient(w0, A, b, lambda_reg, alpha, K)\n    \n    # 4. Analyze convergence\n    q_th, q_emp, check = analyze_convergence(iterates, w_star, L, mu, A, b, lambda_reg)\n    \n    return [L, mu, q_th, q_emp, str(check).lower()]\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1 (well-conditioned and strongly convex)\n        (\n            [[3, 0], [0, 2], [0, 0]], \n            [1, -2, 0], \n            0.2, \n            [2, -1]\n        ),\n        # Case 2 (rank-deficient, not strongly convex)\n        (\n            [[1, 0], [1, 0], [0, 0]], \n            [1, 0, 0], \n            0.1, \n            [0.5, 0.5]\n        ),\n        # Case 3 (ill-conditioned and strongly convex)\n        (\n            [[10, 0], [0, 1], [0, 0]], \n            [3, -1, 0], \n            0.05, \n            [2, 1]\n        ),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A, b, lambda_reg, w0 = case\n        result = solve_case(A, b, lambda_reg, w0)\n        all_results.extend(result)\n        \n    # Format the final output string\n    formatted_results = []\n    for item in all_results:\n        if isinstance(item, float):\n            formatted_results.append(f\"{item:.10f}\".rstrip('0').rstrip('.'))\n        else:\n            formatted_results.append(str(item))\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3126043"}, {"introduction": "对于梯度光滑的凸函数，我们能否比标准梯度下降法收敛得更快？Nesterov加速梯度法 (Nesterov's Accelerated Gradient) 给出了肯定的答案，它通过引入“动量”项显著提高了收敛速度。在本练习中，你将实现这一著名算法，并验证其理论上的 $O(1/t^2)$ 收敛率，同时通过一个反例来诊断当函数的光滑性假设不成立时算法的行为，这突显了算法与问题结构相匹配的重要性。[@problem_id:3126019]", "problem": "您的任务是，从统计学习中梯度和凸性的角度，实现并实证评估一种针对凸函数的加速一阶优化方法。目标是在平滑性假设下验证理论预期的收敛速度，并在关键平滑性假设被违反时诊断其失效情况。您的程序必须实现一个单一的流程，以对所有测试用例运行相同的加速方法，并为每个用例返回一个布尔决策。\n\n基础和假设：\n- 如果对于所有 $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^d$ 和 $\\theta\\in[0,1]$，都有 $f(\\theta \\mathbf{x}+(1-\\theta)\\mathbf{y})\\le \\theta f(\\mathbf{x})+(1-\\theta) f(\\mathbf{y})$，则函数 $f:\\mathbb{R}^d\\to\\mathbb{R}$ 是凸函数。\n- 如果存在 $L\\ge 0$ 使得对于所有的 $\\mathbf{x},\\mathbf{y}$，都有 $\\|\\nabla f(\\mathbf{x})-\\nabla f(\\mathbf{y})\\|_2\\le L\\|\\mathbf{x}-\\mathbf{y}\\|_2$，则称一个可微函数具有 $L$-利普希茨梯度。这意味着下降引理成立：$f(\\mathbf{y})\\le f(\\mathbf{x})+\\nabla f(\\mathbf{x})^\\top(\\mathbf{y}-\\mathbf{x})+\\frac{L}{2}\\|\\mathbf{y}-\\mathbf{x}\\|_2^2$。\n- Nesterov 加速梯度 (NAG) 是一种加速一阶方法，在 $L$-利普希茨梯度假设下，就目标函数次优性 $f(\\mathbf{x}_t)-f^\\star$ 而言，其能达到最优的最坏情况收敛速度 $O(1/t^2)$，其中 $t$ 表示迭代索引，$f^\\star$ 是最小值。\n\n您的任务：\n- 实现一个单一版本的 Nesterov 加速梯度方法，其步长为 $1/L$，其中 $L$ 将按每个测试用例提供给您的程序。使用标准的恒定步长 $1/L$ 和经典的 Nesterov 外推方案，该方案维护一个辅助序列和一个动量参数；将相同的实现应用于所有测试用例。您的实现必须纯粹通过评估梯度来运行，并且除了给定的 $L$ 之外，不得使用任何线搜索、回溯或预言机。\n- 对于每个测试用例，从给定的初始点 $\\mathbf{x}_0$ 开始，运行该加速方法，迭代次数为规定的 $T$ 次。记录目标值的序列 $f(\\mathbf{x}_t)$。\n- 为实证评估观察到的收敛速度是否与 $O(1/t^2)$ 一致，请执行以下操作：\n  1. 令 $e_t = f(\\mathbf{x}_t) - f^\\star$，其中每个测试用例的 $f^\\star$ 是已知的，并在下文给出。对于 $t=1,2,\\dots,T$，构造序列 $s_t = t^2 e_t$。\n  2. 考虑最后的迭代窗口 $t\\in\\{\\lfloor 0.6T\\rfloor,\\dots,T\\}$。在此窗口上，计算 $\\log e_t$ 对 $\\log t$ 回归的最小二乘斜率 $\\hat{\\alpha}$。同时计算有界性比率 $R = \\max_{t \\text{ in window}} s_t \\,/\\, \\min_{t \\text{ in window}} s_t$。\n  3. 当且仅当两个条件同时成立时，返回一个布尔决策 true：$\\hat{\\alpha}\\le -1.8$ 且 $R \\le 10$。如果序列在任何时候产生了非有限的目标值，则返回 false。\n- 您的程序必须汇总所有测试用例的这些决策，并将它们作为逗号分隔的 Python 列表打印在单行中，例如 $[\\text{True},\\text{False},\\text{True}]$。\n\n实现该方法，并在以下测试套件上运行：\n\n- 测试用例 $\\mathbf{A}$ (一维平滑二次函数，理想情况):\n  - 维度 $d=1$。\n  - 函数 $f(x) = \\tfrac{1}{2}\\, a x^2$，其中 $a=10$，因此梯度为 $\\nabla f(x) = a x$。这是一个凸函数，其 $L=a=10$。\n  - 最小值点 $x^\\star=0$ 和 $f^\\star=0$。\n  - 初始点 $x_0=5$。\n  - 使用 $L=10$ 和 $T=300$ 次迭代。\n\n- 测试用例 $\\mathbf{B}$ (具有不同曲率的高维平滑二次函数):\n  - 维度 $d=5$。\n  - 函数 $f(\\mathbf{x}) = \\tfrac{1}{2}\\,\\mathbf{x}^\\top A \\mathbf{x}$，其中 $A=\\mathrm{diag}(1,3,5,10,20)$，因此 $\\nabla f(\\mathbf{x})=A\\mathbf{x}$。这是一个凸函数，其 $L=\\lambda_{\\max}(A)=20$。\n  - 最小值点 $\\mathbf{x}^\\star=\\mathbf{0}$ 和 $f^\\star=0$。\n  - 初始点 $\\mathbf{x}_0=[5,-4,3,-2,1]^\\top$。\n  - 使用 $L=20$ 和 $T=400$ 次迭代。\n\n- 测试用例 $\\mathbf{C}$ (凸函数但梯度非全局利普希茨；失效诊断):\n  - 维度 $d=5$。\n  - 函数 $f(\\mathbf{x})=\\sum_{i=1}^d |x_i|^{3/2}$。该函数是凸的且连续可微，其梯度分量为 $\\nabla_i f(\\mathbf{x})=\\tfrac{3}{2}\\,\\mathrm{sign}(x_i)\\,|x_i|^{1/2}$，但其梯度在 $\\mathbb{R}^d$ 上不是利普希茨的，因为其雅可比范数在 $\\mathbf{x}=\\mathbf{0}$ 附近是无界的。\n  - 最小值点 $\\mathbf{x}^\\star=\\mathbf{0}$ 和 $f^\\star=0$。\n  - 初始点 $\\mathbf{x}_0=[5,-4,3,-2,1]^\\top$。\n  - 强制该加速方法使用相同的常数 $L=20$ 并运行 $T=400$ 次迭代。\n  - 预期的结果是，根据上述决策规则，观察到的行为将不符合 $O(1/t^2)$。\n\n数值要求：\n- 不涉及角度。\n- 不涉及物理单位。\n- 您的程序的最终输出必须是单行文本，其中包含分别针对测试用例 $\\mathbf{A}$、$\\mathbf{B}$ 和 $\\mathbf{C}$ 的三个布尔决策，格式为方括号内以逗号分隔的列表，例如 $[\\text{True},\\text{True},\\text{False}]$。\n\n您的程序必须是完全自包含的，并且不得读取任何输入。", "solution": "用户提供的问题已经过严格验证，被认为是良构的、有科学依据且内部一致的。所有必要的参数和定义都已提供，以获得一个完整且唯一的解。该问题要求在三个不同的测试用例上实现并实证分析 Nesterov 加速梯度 (NAG) 方法，这些用例旨在强调梯度利普希茨连续性对于实现加速收敛速度的重要性。\n\n### 原理与算法设计\n\nNesterov 加速梯度是一种一阶优化算法，它针对一类凸函数改进了标准梯度下降的收敛速度。对于一个具有 $L$-利普希茨连续梯度（即 $L$-平滑）的凸函数 $f$，标准梯度下降对目标函数次优性 $f(\\mathbf{x}_t) - f^\\star$ 的收敛速度为 $O(1/t)$。NAG 引入一个“动量”项，该项融合了先前步骤的信息以加速收敛，从而为此类函数达到了最优的 $O(1/t^2)$ 收敛速度。\n\n其核心思想是，不在当前迭代点 $\\mathbf{x}_t$ 处计算梯度，而是在一个外推点 $\\mathbf{y}_t$ 处计算，该点代表了沿着近期进展方向的一个“激进”步骤。更新规则结合了标准梯度步和动量步。我们将使用一种常见且有效的 NAG 实现，它维护两个迭代序列 $\\mathbf{x}_t$ 和 $\\mathbf{y}_t$，并按以下方式进行：\n\n令初始点为 $\\mathbf{x}_0$。设置辅助序列的初始点 $\\mathbf{y}_0 = \\mathbf{x}_0$。对于迭代 $t=0, 1, 2, \\dots, T-1$：\n\n1.  **梯度步：** 从辅助点 $\\mathbf{y}_t$ 出发，使用步长 $\\eta = 1/L$ 进行梯度下降，以更新主序列。\n    $$\n    \\mathbf{x}_{t+1} = \\mathbf{y}_t - \\frac{1}{L} \\nabla f(\\mathbf{y}_t)\n    $$\n\n2.  **动量步：** 从新点 $\\mathbf{x}_{t+1}$ 沿着最近一步的方向 $(\\mathbf{x}_{t+1} - \\mathbf{x}_t)$ 向前投影，以更新辅助序列。\n    $$\n    \\mathbf{y}_{t+1} = \\mathbf{x}_{t+1} + \\beta_t (\\mathbf{x}_{t+1} - \\mathbf{x}_t)\n    $$\n    动量参数 $\\beta_t$ 必须谨慎选择。对于我们从0开始计数的迭代器 $t$，一个能保证 $O(1/t^2)$ 速率的标准方案是 $\\beta_t = \\frac{t}{t+3}$。\n\n我们分析其收敛性的迭代序列是 $\\{\\mathbf{x}_t\\}_{t=1}^T$。\n\n### 实证速率验证\n\n理论收敛速度 $f(\\mathbf{x}_t) - f^\\star = O(1/t^2)$ 意味着对于某个常数 $C$ 和较大的 $t$，误差 $e_t = f(\\mathbf{x}_t) - f^\\star$ 的行为类似于 $C/t^2$。我们根据指定的两个条件对此进行实证验证：\n\n1.  **幂律衰减率：** 取对数后，我们得到 $\\log e_t \\approx \\log C - 2 \\log t$。这表明 $\\log e_t$ 和 $\\log t$ 之间存在斜率为 $-2$ 的线性关系。我们在迭代的后半部分（$t \\in \\{\\lfloor 0.6T\\rfloor, \\dots, T\\}$）计算此关系的最小二乘斜率 $\\hat{\\alpha}$。条件 $\\hat{\\alpha} \\le -1.8$ 用于检查观察到的速率是否与理论预测一致，同时允许微小的有限迭代偏差。\n\n2.  **渐近有界性：** 表达式 $s_t = t^2 e_t$ 在 $t \\to \\infty$ 时应趋近于常数 $C$。我们通过在同一分析窗口内计算比率 $R = \\max(s_t) / \\min(s_t)$ 来检查该序列是否“渐近有界”。一个小的比率表明 $s_t$ 正在趋于一个近似恒定的值。条件 $R \\le 10$ 为这种行为提供了一个宽松的容忍度。\n\n当且仅当两个条件都满足时，才认为一个测试用例展示了加速收敛。\n\n### 测试用例分析\n\n-   **测试用例 A 和 B：** 这两个用例涉及二次函数 $f(x) = \\frac{1}{2}ax^2$ 和 $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top A \\mathbf{x}$。其梯度 $\\nabla f(x) = ax$ 和 $\\nabla f(\\mathbf{x}) = A\\mathbf{x}$ 是线性的，因此是全局利普希茨连续的。提供的利普希茨常数（A 为 $L=10$，B 为 $L=20$）是正确的。由于这些函数完全满足 NAG 的假设，我们预期该方法将达到其理论上的 $O(1/t^2)$ 收敛速度。实证分析应证实这一点，从而对这两个用例都得出 `True` 的决策。\n\n-   **测试用例 C：** 函数为 $f(\\mathbf{x}) = \\sum_i |x_i|^{3/2}$。该函数是凸的，但其梯度分量为 $\\nabla_i f(\\mathbf{x}) = \\frac{3}{2}\\mathrm{sign}(x_i)|x_i|^{1/2}$，不是全局利普希茨连续的。其 Hessian 矩阵的对角元素 $\\frac{\\partial^2 f}{\\partial x_i^2} = \\frac{3}{4}|x_i|^{-1/2}$ 在 $x_i \\to 0$ 时是无界的。这意味着函数的局部平滑性在原点附近急剧恶化。NAG 的 $O(1/t^2)$ 速率保证不再成立。使用基于常数 $L=20$ 的固定步长是不合适的；当迭代点接近原点时，局部利普希茨常数可能变得远大于 20，这使得步长 $1/20$ 过大，从而导致收敛缓慢或不稳定。因此，观察到的收敛速度预计将显著慢于 $O(1/t^2)$。实证测试旨在检测这种失效情况，我们预期会得到 `False` 的决策。\n\n实现将精确地遵循此逻辑，对所有用例执行相同的 NAG 算法，并应用指定的决策规则。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the optimization problem across all test cases and print the results.\n    \"\"\"\n\n    def run_nesterov_accelerated_gradient(func_f, grad_f, x0, L, T):\n        \"\"\"\n        Implements Nesterov's Accelerated Gradient (NAG) method.\n\n        Args:\n            func_f: The objective function.\n            grad_f: The gradient of the objective function.\n            x0: The initial point.\n            L: The Lipschitz constant of the gradient.\n            T: The number of iterations.\n\n        Returns:\n            A list of objective values for iterations 1 to T, or None if a non-finite value is encountered.\n        \"\"\"\n        x_k = np.array(x0, dtype=np.float64)\n        y_k = np.array(x0, dtype=np.float64)\n        x_prev_k = np.array(x0, dtype=np.float64)\n        \n        objectives = []\n\n        for k in range(T): # Corresponds to iterations t=1, ..., T\n            x_prev_k = x_k\n            \n            # Gradient evaluation and update step for x\n            grad_val = grad_f(y_k)\n            if not np.all(np.isfinite(grad_val)):\n                return None\n            \n            x_k = y_k - (1.0 / L) * grad_val\n            \n            # Store objective value\n            obj_val = func_f(x_k)\n            if not np.isfinite(obj_val):\n                return None\n            objectives.append(obj_val)\n            \n            # Momentum update step for y\n            momentum_coeff = k / (k + 3.0)\n            y_k = x_k + momentum_coeff * (x_k - x_prev_k)\n            \n        return objectives\n\n    def analyze_convergence(objectives, f_star, T):\n        \"\"\"\n        Analyzes the convergence rate based on the sequence of objective values.\n\n        Args:\n            objectives: A list of T objective values.\n            f_star: The known minimum value of the function.\n            T: The total number of iterations.\n\n        Returns:\n            A boolean decision based on the specified criteria.\n        \"\"\"\n        if objectives is None:\n            return False\n\n        # Calculate errors e_t = f(x_t) - f_star for t=1,...,T\n        e_t = np.array([obj - f_star for obj in objectives])\n\n        # Define analysis window: iterations floor(0.6*T) to T\n        start_t = int(np.floor(0.6 * T))\n        if start_t > T:\n            return False \n        \n        window_indices = np.arange(start_t - 1, T)\n        if len(window_indices)  2:\n            return False\n\n        t_vals = window_indices + 1\n        e_vals = e_t[window_indices]\n        \n        # Filter out non-positive errors to avoid issues with log\n        positive_mask = e_vals > 0\n        if np.sum(positive_mask)  2: # Need at least two points for regression\n            return False\n\n        t_log = np.log(t_vals[positive_mask])\n        e_log = np.log(e_vals[positive_mask])\n\n        # Condition 1: Check the slope of log-log plot\n        try:\n            slope, _ = np.polyfit(t_log, e_log, 1)\n        except np.linalg.LinAlgError:\n            return False # Should not happen with valid data\n        \n        cond1 = slope = -1.8\n\n        # Condition 2: Check the boundedness ratio of s_t = t^2 * e_t\n        s_t = (t_vals**2) * e_vals\n        s_t_valid = s_t[positive_mask]\n        \n        min_s_t = np.min(s_t_valid)\n        if min_s_t = 0:\n            return False\n            \n        max_s_t = np.max(s_t_valid)\n        R = max_s_t / min_s_t\n        cond2 = R = 10.0\n\n        return cond1 and cond2\n\n    # --- Test Cases Definition ---\n    # Case A\n    a_A = 10.0\n    func_A = lambda x: 0.5 * a_A * x**2\n    grad_A = lambda x: a_A * x\n    \n    # Case B\n    A_B = np.diag([1.0, 3.0, 5.0, 10.0, 20.0])\n    func_B = lambda x: 0.5 * x.T @ A_B @ x\n    grad_B = lambda x: A_B @ x\n\n    # Case C\n    # Note: np.sign(0) is 0, which gives the correct gradient at the origin.\n    func_C = lambda x: np.sum(np.power(np.abs(x), 1.5))\n    grad_C = lambda x: 1.5 * np.sign(x) * np.power(np.abs(x), 0.5)\n\n    test_cases = [\n        {\n            \"id\": \"A\",\n            \"func\": func_A, \"grad\": grad_A,\n            \"x0\": np.array([5.0]), \"L\": 10.0, \"T\": 300, \"f_star\": 0.0\n        },\n        {\n            \"id\": \"B\",\n            \"func\": func_B, \"grad\": grad_B,\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0, 1.0]), \"L\": 20.0, \"T\": 400, \"f_star\": 0.0\n        },\n        {\n            \"id\": \"C\",\n            \"func\": func_C, \"grad\": grad_C,\n            \"x0\": np.array([5.0, -4.0, 3.0, -2.0, 1.0]), \"L\": 20.0, \"T\": 400, \"f_star\": 0.0\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        objectives = run_nesterov_accelerated_gradient(\n            case[\"func\"], case[\"grad\"], case[\"x0\"], case[\"L\"], case[\"T\"]\n        )\n        decision = analyze_convergence(objectives, case[\"f_star\"], case[\"T\"])\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\".replace(\"True\", \"true\").replace(\"False\", \"false\"))\n\nsolve()\n```", "id": "3126019"}, {"introduction": "统计学习中的许多优化问题都带有约束，例如，在概率单纯形上寻找最优的概率分布。传统的欧几里得空间梯度下降法无法直接处理这类几何约束。镜像下降法 (Mirror Descent) 为此提供了一个优雅而强大的框架，它通过“镜像映射”来适应问题的几何结构。在这个高级实践中，你将探索使用熵函数作为镜像映射的镜像下降法，并推导和实现指数梯度更新算法，将凸分析中的深刻思想（如Bregman散度）与一个实用的算法联系起来。[@problem_id:3126055]", "problem": "给定维度为 $d$ 的概率单纯形，定义为 $\\Delta_d = \\{ p \\in \\mathbb{R}^d : \\sum_{j=1}^d p_j = 1,\\ p_j \\ge 0\\ \\forall j \\}$，以及熵镜像映射 $\\psi : \\mathrm{int}(\\Delta_d) \\to \\mathbb{R}$，定义为 $\\psi(p) = \\sum_{j=1}^d p_j \\log p_j$，其中 $\\log$ 表示自然对数。考虑凸对数损失 $f(p) = - \\sum_{j=1}^d r_j \\log p_j$，其中 $r \\in \\Delta_d$ 是一个固定的标签分布向量。\n\n任务：\n1) 仅从核心定义（梯度、凸性以及Bregman散度的定义）出发，推导梯度 $\\nabla f(p)$ 和与 $\\psi$ 相关的Bregman散度 $D_\\psi(p,q)$。您的推导必须从 $f(p) = - \\sum_{j=1}^d r_j \\log p_j$ 和定义 $D_\\psi(p,q) = \\psi(p) - \\psi(q) - \\langle \\nabla \\psi(q), p - q \\rangle$ 开始，并且不得在未经证明的情况下假设任何关于熵的专业化结果。\n2) 使用单纯形约束下的一阶最优性条件，确定 $f(p)$ 在 $\\Delta_d$ 上的唯一最小化子 $p^\\star$ 并证明其唯一性。\n3) 为一般可微凸目标 $f$ 推导在 $\\Delta_d$ 上使用镜像映射 $\\psi$ 的镜像下降法，该方法由隐式更新定义\n$$\n\\nabla \\psi(p_{t+1}) \\,=\\, \\nabla \\psi(p_t) \\,-\\, \\eta \\,\\nabla f(p_t),\n$$\n随后强制 $p_{t+1} \\in \\Delta_d$。然后，将您的更新特例化到上述对数损失，以获得 $p_{t+1}$ 关于 $p_t$、$r$ 和步长 $\\eta  0$ 的显式坐标级更新。您的推导必须从隐式更新和 $\\nabla \\psi$ 的定义开始，不应假设任何已知的乘法形式。\n4) 在一个程序中实现特例化的镜像下降，该程序：\n   - 在一个严格为正的 $p_0 \\in \\mathrm{int}(\\Delta_d)$ 处初始化，\n   - 使用恒定步长 $\\eta$ 重复更新 $T$ 次迭代，\n   - 返回最终迭代值 $p_T$ 以及最终的对数损失 $f(p_T)$ 和对于给定的 $q \\in \\mathrm{int}(\\Delta_d)$ 的Bregman散度 $D_\\psi(p_T, q)$。\n   - 使用自然对数。\n   - 在容差范围内数值验证 $p_T$ 位于单纯形内且 $D_\\psi(p_T,q) \\ge 0$。\n   - 此外，使用从零开始的索引报告 $p_T$ 的最大坐标的索引。\n\n实现要求：\n- 仅使用实数算术；无外部数据。\n- 迭代期间确保严格正性；如果为了稳定性需要，可以使用一个小的数值下限 $\\varepsilon$，前提是通过重新归一化到 $\\Delta_d$ 上来保持可行性。\n- 所有计算均使用自然对数。\n- 对于布尔值，使用 $\\tau = 10^{-10}$ 的容差来检查单纯形求和约束和散度的非负性。\n\n测试套件：\n您的程序必须对以下三个测试用例运行镜像下降过程：\n\n- 测试用例1（正常路径）：\n  - $d = 3$，\n  - $p_0 = [\\,0.4,\\, 0.3,\\, 0.3\\,]$，\n  - $r = [\\,0,\\, 1,\\, 0\\,]$，\n  - $\\eta = 0.7$，\n  - $T = 5$，\n  - $q = [\\,0.2,\\, 0.5,\\, 0.3\\,]$。\n\n- 测试用例2（软标签和收敛行为）：\n  - $d = 3$，\n  - $p_0 = [\\,0.34,\\, 0.33,\\, 0.33\\,]$，\n  - $r = [\\,0.7,\\, 0.2,\\, 0.1\\,]$，\n  - $\\eta = 0.3$，\n  - $T = 20$，\n  - $q = [\\,\\frac{1}{3},\\, \\frac{1}{3},\\, \\frac{1}{3}\\,]$。\n\n- 测试用例3（具有非常小初始坐标的边缘情况）：\n  - $d = 3$，\n  - $p_0 = [\\,0.499,\\, 0.499,\\, 0.002\\,]$，\n  - $r = [\\,0,\\, 0,\\, 1\\,]$，\n  - $\\eta = 0.01$，\n  - $T = 50$，\n  - $q = [\\,0.4,\\, 0.4,\\, 0.2\\,]$。\n\n每个测试用例的必需输出：\n- 最终对数损失 $f(p_T)$，作为浮点数。\n- Bregman散度 $D_\\psi(p_T, q)$，作为浮点数。\n- $p_T$ 最大坐标的从零开始的索引，作为整数。\n- 一个布尔值，指示 $p_T$ 是否在容差 $\\tau$ 内位于单纯形中，即 $\\sum_j p_{T,j}$ 与 $1$ 的差值是否最多为 $\\tau$，并且在数值容差内所有坐标都至少为 $0$。\n- 一个布尔值，指示 $D_\\psi(p_T,q) \\ge -\\tau$ 是否成立。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个Python风格的列表，每个测试用例对应一个条目。\n- 每个条目本身都是一个列表，顺序为：$[\\,f(p_T),\\ D_\\psi(p_T,q),\\ \\mathrm{argmax\\_index},\\ \\mathrm{simplex\\_ok},\\ \\mathrm{div\\_ok}\\,]$。\n- 两个浮点数必须四舍五入到6位小数。\n- 所需整体结构的示例（仅供说明）：$[[\\,0.123456,\\, 0.000000,\\, 2,\\, True,\\, True\\,],[\\,\\dots\\,],[\\,\\dots\\,]]$.", "solution": "该问题被评估为有效，因为它科学地基于凸优化和统计学习的原理，是良定的，并且所有定义和参数都得到完整且一致的提供。\n\n### 任务1：梯度推导\n\n第一个任务是推导对数损失函数 $f(p)$ 的梯度以及与熵镜像映射 $\\psi(p)$ 相关联的Bregman散度 $D_\\psi(p,q)$。\n\n**对数损失 $f(p)$ 的梯度**\n\n对数损失函数由 $f(p) = - \\sum_{j=1}^d r_j \\log p_j$ 给出，其中 $r \\in \\Delta_d$ 是一个固定向量，$p \\in \\mathrm{int}(\\Delta_d)$。梯度 $\\nabla f(p)$ 是一个向量，其第 $k$ 个分量是 $f$ 关于 $p_k$ 的偏导数。\n\n$$\n(\\nabla f(p))_k = \\frac{\\partial}{\\partial p_k} \\left( - \\sum_{j=1}^d r_j \\log p_j \\right)\n$$\n\n由于微分算子的线性性，我们可以将其移到求和号内部。项 $r_j \\log p_j$ 仅在 $j=k$ 时才依赖于 $p_k$。因此，只有当求和项的索引为 $k$ 时，导数才非零。\n\n$$\n\\frac{\\partial}{\\partial p_k} (-r_j \\log p_j) = \\begin{cases} -r_k \\frac{1}{p_k}  \\text{if } j=k \\\\ 0  \\text{if } j \\neq k \\end{cases}\n$$\n\n因此，梯度的第 $k$ 个分量是：\n\n$$\n(\\nabla f(p))_k = -\\frac{r_k}{p_k}\n$$\n\n完整的梯度向量是：\n\n$$\n\\nabla f(p) = \\begin{bmatrix} -r_1/p_1 \\\\ -r_2/p_2 \\\\ \\vdots \\\\ -r_d/p_d \\end{bmatrix}\n$$\n\n**镜像映射 $\\psi(p)$ 的梯度**\n\n熵镜像映射是 $\\psi(p) = \\sum_{j=1}^d p_j \\log p_j$。为求其梯度 $\\nabla \\psi(p)$，我们计算关于每个分量 $p_k$ 的偏导数。\n\n$$\n(\\nabla \\psi(p))_k = \\frac{\\partial}{\\partial p_k} \\left( \\sum_{j=1}^d p_j \\log p_j \\right)\n$$\n\n同样，只有在 $j=k$ 的项对导数有贡献。我们对 $\\frac{\\partial}{\\partial p_k}(p_k \\log p_k)$ 使用乘法法则。\n\n$$\n\\frac{\\partial}{\\partial p_k}(p_k \\log p_k) = \\left(\\frac{\\partial p_k}{\\partial p_k}\\right) \\log p_k + p_k \\left(\\frac{\\partial (\\log p_k)}{\\partial p_k}\\right) = 1 \\cdot \\log p_k + p_k \\cdot \\frac{1}{p_k} = \\log p_k + 1\n$$\n\n所以，$\\psi$ 的梯度的第 $k$ 个分量是 $(\\nabla \\psi(p))_k = \\log p_k + 1$。完整的梯度向量是：\n\n$$\n\\nabla \\psi(p) = \\begin{bmatrix} \\log p_1 + 1 \\\\ \\log p_2 + 1 \\\\ \\vdots \\\\ \\log p_d + 1 \\end{bmatrix}\n$$\n\n**Bregman散度 $D_\\psi(p,q)$**\n\nBregman散度定义为 $D_\\psi(p,q) = \\psi(p) - \\psi(q) - \\langle \\nabla \\psi(q), p - q \\rangle$。我们代入上面推导出的 $\\psi$ 和 $\\nabla \\psi$ 的表达式。\n\n$$\nD_\\psi(p,q) = \\left(\\sum_{j=1}^d p_j \\log p_j\\right) - \\left(\\sum_{j=1}^d q_j \\log q_j\\right) - \\sum_{j=1}^d (\\log q_j + 1)(p_j - q_j)\n$$\n\n我们来展开内积项：\n\n$$\n\\sum_{j=1}^d (\\log q_j + 1)(p_j - q_j) = \\sum_{j=1}^d (p_j \\log q_j - q_j \\log q_j + p_j - q_j)\n$$\n\n由于 $p, q \\in \\Delta_d$，它们的分量之和为1：$\\sum_{j=1}^d p_j = 1$ 和 $\\sum_{j=1}^d q_j = 1$。这意味着 $\\sum_{j=1}^d (p_j - q_j) = \\sum p_j - \\sum q_j = 1 - 1 = 0$。所以展开求和的最后一部分消失了。\n\n$$\n\\sum_{j=1}^d (\\log q_j + 1)(p_j - q_j) = \\sum_{j=1}^d (p_j \\log q_j - q_j \\log q_j)\n$$\n\n将其代回 $D_\\psi(p,q)$ 的定义中：\n\n$$\nD_\\psi(p,q) = \\sum_{j=1}^d p_j \\log p_j - \\sum_{j=1}^d q_j \\log q_j - \\left( \\sum_{j=1}^d p_j \\log q_j - \\sum_{j=1}^d q_j \\log q_j \\right)\n$$\n\n涉及 $q_j \\log q_j$ 的项相互抵消。\n\n$$\nD_\\psi(p,q) = \\sum_{j=1}^d p_j \\log p_j - \\sum_{j=1}^d p_j \\log q_j = \\sum_{j=1}^d p_j (\\log p_j - \\log q_j)\n$$\n\n使用对数的性质 $\\log a - \\log b = \\log(a/b)$，我们得到最终形式：\n\n$$\nD_\\psi(p,q) = \\sum_{j=1}^d p_j \\log \\frac{p_j}{q_j}\n$$\n\n这就是著名的Kullback-Leibler (KL) 散度，$D_{KL}(p \\| q)$。\n\n### 任务2：$f(p)$ 在 $\\Delta_d$ 上的唯一最小化子\n\n我们寻求找到 $p^\\star = \\arg\\min_{p \\in \\Delta_d} f(p)$。目标是 $f(p) = - \\sum_{j=1}^d r_j \\log p_j$。这个问题等价于最小化分布 $r$ 和 $p$ 之间的交叉熵。我们可以将其与KL散度联系起来：\n\n$$\nD_{KL}(r \\| p) = \\sum_{j=1}^d r_j \\log \\frac{r_j}{p_j} = \\sum_{j=1}^d r_j \\log r_j - \\sum_{j=1}^d r_j \\log p_j = -H(r) + f(p)\n$$\n\n这里，$H(r) = -\\sum_j r_j \\log r_j$ 是固定分布 $r$ 的熵，是一个常数。因此，最小化 $f(p)$ 等价于最小化 $D_{KL}(r \\| p)$。\n\n根据吉布斯不等式（KL散度的一个基本性质），对于任意两个概率分布 $r$ 和 $p$，$D_{KL}(r \\| p) \\ge 0$。等号成立的充要条件是对于所有的 $j=1, \\dots, d$，都有 $p_j = r_j$。\n\n因为我们是在集合 $\\Delta_d$ 上最小化 $f(p)$，而该集合包含 $r$，所以当 $D_{KL}(r \\| p) = 0$ 时达到最小值。这恰好在 $p=r$ 时发生。因此，唯一的最小化子是 $p^\\star = r$。\n\n这个论证也确立了唯一性。由于 $f(p)$ 在 $r$ 的支撑集上是严格凸的，并且可行集 $\\Delta_d$ 是凸集，因此只能有一个全局最小化子。\n\n### 任务3：镜像下降更新规则\n\n镜像下降算法由隐式更新 $\\nabla \\psi(p_{t+1}) = \\nabla \\psi(p_t) - \\eta \\nabla f(p_t)$ 定义，随后确保 $p_{t+1} \\in \\Delta_d$。\n\n我们为每个坐标 $j=1, \\dots, d$ 写出这个更新：\n\n$$\n(\\nabla \\psi(p_{t+1}))_j = (\\nabla \\psi(p_t))_j - \\eta (\\nabla f(p_t))_j\n$$\n\n使用我们推导出的 $\\psi$ 的梯度，$(\\nabla \\psi(p))_j = \\log p_j + 1$：\n\n$$\n\\log p_{t+1, j} + 1 = (\\log p_{t, j} + 1) - \\eta (\\nabla f(p_t))_j\n$$\n\n常数 $1$ 从两边消去：\n\n$$\n\\log p_{t+1, j} = \\log p_{t, j} - \\eta (\\nabla f(p_t))_j\n$$\n\n对两边取指数，得到一个临时向量 $\\tilde{p}_{t+1}$ 的显式更新，其分量尚不保证和为1：\n\n$$\n\\tilde{p}_{t+1, j} = \\exp(\\log p_{t, j} - \\eta (\\nabla f(p_t))_j) = p_{t, j} \\exp(-\\eta (\\nabla f(p_t))_j)\n$$\n\n现在，我们将这个通用的镜像下降更新特例化到我们特定的对数损失目标 $f(p)$。我们代入我们的结果 $(\\nabla f(p_t))_j = -r_j / p_{t, j}$：\n\n$$\n\\tilde{p}_{t+1, j} = p_{t, j} \\exp\\left(-\\eta \\left(-\\frac{r_j}{p_{t, j}}\\right)\\right) = p_{t, j} \\exp\\left(\\frac{\\eta r_j}{p_{t, j}}\\right)\n$$\n\n这个更新对每个坐标 $j$ 执行。得到的向量 $\\tilde{p}_{t+1}$ 将具有所有正分量（因为 $p_{t,j}0$ 且指数函数总是正的），但通常其和不为1。为了施加约束 $p_{t+1} \\in \\Delta_d$，我们用其和对向量进行归一化。令 $Z_t = \\sum_{k=1}^d \\tilde{p}_{t+1, k}$。最终的、显式的、坐标级的更新是：\n\n$$\np_{t+1, j} = \\frac{\\tilde{p}_{t+1, j}}{Z_t} = \\frac{p_{t, j} \\exp\\left(\\frac{\\eta r_j}{p_{t, j}}\\right)}{\\sum_{k=1}^d p_{t, k} \\exp\\left(\\frac{\\eta r_k}{p_{t, k}}\\right)}\n$$\n\n这个更新规则，被称为指数化梯度更新，确保了如果 $p_t \\in \\mathrm{int}(\\Delta_d)$，那么 $p_{t+1}$ 也将位于 $\\mathrm{int}(\\Delta_d)$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Executes the mirror descent algorithm for a set of test cases\n    and formats the output as per the problem specification.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"d\": 3,\n            \"p0\": np.array([0.4, 0.3, 0.3]),\n            \"r\": np.array([0.0, 1.0, 0.0]),\n            \"eta\": 0.7,\n            \"T\": 5,\n            \"q\": np.array([0.2, 0.5, 0.3]),\n        },\n        # Test case 2 (soft labels and convergence behavior)\n        {\n            \"d\": 3,\n            \"p0\": np.array([0.34, 0.33, 0.33]),\n            \"r\": np.array([0.7, 0.2, 0.1]),\n            \"eta\": 0.3,\n            \"T\": 20,\n            \"q\": np.array([1/3, 1/3, 1/3]),\n        },\n        # Test case 3 (edge case with a very small initial coordinate)\n        {\n            \"d\": 3,\n            \"p0\": np.array([0.499, 0.499, 0.002]),\n            \"r\": np.array([0.0, 0.0, 1.0]),\n            \"eta\": 0.01,\n            \"T\": 50,\n            \"q\": np.array([0.4, 0.4, 0.2]),\n        },\n    ]\n\n    tau = 1e-10\n    results = []\n\n    for case in test_cases:\n        p = case[\"p0\"].copy()\n        r = case[\"r\"]\n        eta = case[\"eta\"]\n        T = case[\"T\"]\n        q = case[\"q\"]\n\n        for _ in range(T):\n            # General mirror descent update for f(p) = -sum(r_j * log(p_j))\n            #nabla_f = -r / p\n            #p_unnormalized = p * np.exp(-eta * nabla_f)\n            # This is p_j * exp(eta * r_j / p_j).\n            \n            # Using log-sum-exp trick for numerical stability, which is good practice although\n            # not strictly required by problem. The derived update is p_j * exp(eta * r_j / p_j).\n            # The direct implementation is simple and sufficient given the test case parameters.\n            p_unnormalized = p * np.exp(eta * r / p)\n            Z = np.sum(p_unnormalized)\n            p = p_unnormalized / Z\n\n        p_T = p\n\n        # 1. Calculate final log-loss f(p_T)\n        # The sum is only over j where r_j > 0 to handle 0 * log(x) terms correctly.\n        active_indices = r > 0\n        p_T_safe_log = np.maximum(p_T, 1e-30) # Prevent log(0)\n        if np.any(active_indices):\n            f_pT = -np.sum(r[active_indices] * np.log(p_T_safe_log[active_indices]))\n        else:\n            f_pT = 0.0\n\n        # 2. Calculate Bregman divergence D_psi(p_T, q), which is KL(p_T || q)\n        # D = sum(p_T_j * log(p_T_j / q_j))\n        # Ensure positivity of arguments to log.\n        p_T_safe = np.maximum(p_T, 1e-30)\n        q_safe = np.maximum(q, 1e-30)\n        D_psi_pT_q = np.sum(p_T * (np.log(p_T_safe) - np.log(q_safe)))\n\n        # 3. Find index of the largest coordinate\n        argmax_index = np.argmax(p_T)\n\n        # 4. Check if p_T is in the simplex\n        is_in_simplex = (abs(np.sum(p_T) - 1.0) = tau) and np.all(p_T >= -tau)\n        \n        # 5. Check non-negativity of divergence\n        is_div_non_negative = D_psi_pT_q >= -tau\n\n        results.append([\n            round(f_pT, 6),\n            round(D_psi_pT_q, 6),\n            int(argmax_index),\n            bool(is_in_simplex),\n            bool(is_div_non_negative)\n        ])\n\n    # Format the final output string exactly as required\n    formatted_results = []\n    for res in results:\n        f_val_str = f\"{res[0]:.6f}\"\n        d_val_str = f\"{res[1]:.6f}\"\n        idx_str = str(res[2])\n        simp_ok_str = str(res[3]).lower()\n        div_ok_str = str(res[4]).lower()\n        formatted_results.append(f\"[{f_val_str},{d_val_str},{idx_str},{simp_ok_str},{div_ok_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3126055"}]}