{"hands_on_practices": [{"introduction": "在统计建模中，多重共线性是一个常见问题，它指的是特征之间存在高度相关性，这会影响模型的稳定性和可解释性。谱分解为我们提供了一个强大的诊断工具。通过分析数据相关性矩阵的特征值，我们可以识别出特征间的近线性依赖关系，因为这些依赖关系对应着极小的特征值。这项实践练习 [@problem_id:3117789] 将指导你通过一个具体的算法来识别并处理这个问题，从而加深对特征值和特征向量在数据分析中实际应用的理解。", "problem": "给定一系列表示为实数矩阵的多元数据集，您需要使用谱分解，通过经验相关矩阵的小特征值来识别近共线特征，从而诊断多重共线性，然后根据相应的特征向量提出要删除的特征。此任务在统计学习的背景下进行，其中特征多重共线性会降低模型的可解释性和数值稳定性。任务要求从核心定义和经过检验的事实出发进行形式化推导，然后实现算法。\n\n从以下基本概念开始：\n- 数据集是一个矩阵 $X \\in \\mathbb{R}^{n \\times p}$，包含 $n$ 个观测值和 $p$ 个特征，其列为 $x_{1}, \\dots, x_{p}$。\n- 特征 $j$ 的经验均值为 $\\bar{x}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}$。\n- 特征 $j$ 的无偏经验方差为 $s_{j}^{2} = \\frac{1}{n-1} \\sum_{i=1}^{n} \\left(X_{ij} - \\bar{x}_{j}\\right)^{2}$。经验标准差为 $s_{j} = \\sqrt{s_{j}^{2}}$。\n- 标准化设计矩阵为 $Z \\in \\mathbb{R}^{n \\times p}$，其中对于所有 $s_j > 0$ 的特征，$Z_{ij} = \\frac{X_{ij} - \\bar{x}_{j}}{s_{j}}$。\n- 经验相关矩阵定义为\n$$\nR = \\frac{1}{n - 1} Z^{\\top} Z \\in \\mathbb{R}^{p \\times p}.\n$$\n这个矩阵 $R$ 是对称、半正定的，并且可以进行谱分解 $R = V \\Lambda V^{\\top}$，其中 $V \\in \\mathbb{R}^{p \\times p}$ 的列是标准正交的（特征向量），$\\Lambda = \\mathrm{diag}(\\lambda_{1}, \\dots, \\lambda_{p})$ 包含特征值。\n- 在主成分分析（PCA）的背景下，每个特征值 $\\lambda_{k}$ 等于 $Z$ 沿着特征向量 $v_{k}$ 方向的方差。小特征值表示方差小的方向，这对应于特征之间近乎冗余的线性关系。\n\n您的程序必须为每个测试用例实现以下过程：\n1. 使用上述定义将每个特征标准化为零均值和单位方差。如果任何特征的经验标准差 $s_{j} = 0$，则立即标记为待删除，并将其从后续的谱分析中排除。在计算 $s_{j}^{2}$ 和 $R$ 时，均使用无偏分母 $n-1$。\n2. 对剩余特征计算经验相关矩阵 $R = \\frac{1}{n-1} Z^{\\top} Z$。\n3. 计算谱分解 $R = V \\Lambda V^{\\top}$，得到特征值 $\\lambda_{k}$ 和特征向量 $v_{k}$。\n4. 给定一个小的特征值阈值 $\\tau > 0$，识别所有满足 $\\lambda_{k} < \\tau$ 的索引 $k$。对于每个这样的 $k$，找到在 $v_{k}$ 上具有最大绝对载荷 $|v_{k,j}|$ 的特征索引 $j^{\\star}$，并建议删除特征 $j^{\\star}$。如果在数值容差 $\\epsilon$ 内有多个特征并列获得最大载荷，选择这些并列特征中索引最大的一个来确定性地打破平局。\n5. 将步骤 4 中所有建议删除的特征与步骤 1 中检测到的任何零方差特征汇总，去重后按升序对结果列表进行排序。索引必须相对于 $X$ 的原始列，并使用从零开始的索引进行报告。\n\n科学现实性与适用性：此过程的依据是，相关（或协方差）矩阵的小特征值源于特征空间中方差接近零的方向，这与特征间的近线性相关性一致，而后者是统计学习中多重共线性的一个标志。\n\n此问题不涉及角度单位和物理单位。不需要百分比。\n\n测试套件：\n对于每个测试用例，程序必须使用提供的 $X$、阈值 $\\tau$ 和平局容差 $\\epsilon$。矩阵如下：\n\n- 测试用例 1（近重复特征；顺利路径）：\n$$\nX^{(1)} =\n\\begin{bmatrix}\n0  0.00 + 0.00  1.00 \\\\\n1  1.00 + 0.05  -0.50 \\\\\n2  2.00 - 0.02  0.70 \\\\\n3  3.00 + 0.10  -1.20 \\\\\n4  4.00 - 0.05  0.30 \\\\\n5  5.00 + 0.03  0.80 \\\\\n6  6.00 - 0.08  -0.90 \\\\\n7  7.00 + 0.02  1.10 \\\\\n8  8.00 + 0.00  -0.40 \\\\\n9  9.00 - 0.07  0.20\n\\end{bmatrix}\n\\quad\n\\tau^{(1)} = 0.05\n\\quad\n\\epsilon^{(1)} = 10^{-12}.\n$$\n解释：第二个特征几乎是第一个特征加上少量噪声；第三个特征不相关。\n\n- 测试用例 2（近似独立特征；未检测到多重共线性的边界条件）：\n$$\nX^{(2)} =\n\\begin{bmatrix}\n0  1.10  0.50 \\\\\n1  -0.90  0.30 \\\\\n2  0.70  -0.60 \\\\\n3  -0.30  0.70 \\\\\n4  0.20  -0.80 \\\\\n5  -0.40  0.10 \\\\\n6  0.80  0.20 \\\\\n7  -0.50  -0.30 \\\\\n8  0.60  0.40 \\\\\n9  -0.70  -0.20 \\\\\n10  0.90  0.00 \\\\\n11  -1.00  0.50\n\\end{bmatrix}\n\\quad\n\\tau^{(2)} = 0.10\n\\quad\n\\epsilon^{(2)} = 10^{-12}.\n$$\n\n- 测试用例 3（一个特征是另外两个特征的近线性组合；带少量噪声的边缘情况）：\n$$\nX^{(3)} =\n\\begin{bmatrix}\n2.00  -1.00  2.00 + (-1.00) + 0.01 \\\\\n3.00  -1.10  3.00 + (-1.10) - 0.02 \\\\\n4.00  -0.90  4.00 + (-0.90) + 0.00 \\\\\n5.50  -1.20  5.50 + (-1.20) + 0.03 \\\\\n6.10  -0.80  6.10 + (-0.80) - 0.01 \\\\\n7.00  -1.05  7.00 + (-1.05) + 0.02 \\\\\n8.20  -1.00  8.20 + (-1.00) - 0.02 \\\\\n9.00  -0.95  9.00 + (-0.95) + 0.01\n\\end{bmatrix}\n\\quad\n\\tau^{(3)} = 0.10\n\\quad\n\\epsilon^{(3)} = 10^{-12}.\n$$\n\n- 测试用例 4（四个特征，存在两个近相关性和轻度噪声；压力测试）：\n设特征为\n$$\nx_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\\\ 9 \\end{bmatrix},\n\\quad\nx_{2} = \\begin{bmatrix} 9.00 \\\\ 7.95 \\\\ 7.00 \\\\ 6.05 \\\\ 5.00 \\\\ 3.95 \\\\ 3.00 \\\\ 2.05 \\\\ 1.00 \\end{bmatrix},\n\\quad\nx_{3} = \\begin{bmatrix} 1.50 \\\\ 0.00 \\\\ 1.70 \\\\ 0.20 \\\\ 1.40 \\\\ 0.30 \\\\ 1.80 \\\\ 0.10 \\\\ 1.60 \\end{bmatrix},\n$$\n和\n$$\nx_{4} = 0.50 \\, x_{1} + 0.50 \\, x_{3} + \\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.00 \\\\ 0.03 \\\\ -0.01 \\\\ 0.02 \\\\ -0.02 \\\\ 0.01 \\\\ 0.00 \\end{bmatrix}.\n$$\n则\n$$\nX^{(4)} = \\begin{bmatrix} x_{1}  x_{2}  x_{3}  x_{4} \\end{bmatrix}\n\\quad\n\\tau^{(4)} = 0.08\n\\quad\n\\epsilon^{(4)} = 10^{-12}.\n$$\n\n最终输出规范：\n- 对于每个测试用例，根据上述汇总规则，输出一个建议删除的、从零开始的特征索引列表。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，其中每个条目本身是对应测试用例的整数列表。例如，一个包含四个测试用例的输出可能看起来像 $[ [0,2], [], [3], [1] ]$，但实际输出字符串中没有空格。确切要求的格式是形如 $[[\\dots],[\\dots],[\\dots],[\\dots]]$ 的单行字符串，其中没有任何空格。", "solution": "经评估，用户提供的问题是有效的。它在科学上基于多元统计和线性代数的原理，问题陈述清晰，具有明确和确定性的算法流程，并且表述客观。该问题涉及统计学习中诊断多重共线性的一种标准技术。\n\n该解决方案源于相关矩阵谱分解的几何解释。多重共线性意味着数据集中的一个特征向量可以被其他特征向量的线性组合近似表示。问题中指定的算法将检测和解决此类相关性的过程形式化。\n\n设数据集由矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 表示，包含 $n$ 个观测值和 $p$ 个特征，其列表示为 $x_1, \\dots, x_p$。诊断多重共线性的第一步是标准化数据。对于每个特征 $j \\in \\{1, \\dots, p\\}$，经验均值 $\\bar{x}_j$ 和无偏经验标准差 $s_j$ 计算如下：\n$$\n\\bar{x}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} X_{ij}\n$$\n$$\ns_{j} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} \\left(X_{ij} - \\bar{x}_{j}\\right)^{2}}\n$$\n一个 $s_j = 0$ 的特征 $j$ 在所有观测值上都是恒定的，不携带任何方差。这样的特征不提供信息，应立即标记为待移除。对于所有 $s_j > 0$ 的特征，我们创建一个标准化的特征向量 $z_j$，其元素为 $Z_{ij} = \\frac{X_{ij} - \\bar{x}_{j}}{s_{j}}$。这些标准化的向量构成了标准化设计矩阵 $Z \\in \\mathbb{R}^{n \\times p'}$ 的列，其中 $p'$ 是非恒定特征的数量。\n\n经验相关矩阵 $R \\in \\mathbb{R}^{p' \\times p'}$ 由标准化数据计算得出：\n$$\nR = \\frac{1}{n - 1} Z^{\\top} Z\n$$\n根据构造，$R$ 是一个对称半正定矩阵。其对角线元素 $R_{jj}$ 均等于 $1$，而非对角线元素 $R_{jk}$ 表示特征 $j$ 和 $k$ 之间的样本相关性。\n\n该诊断程序的基石是 $R$ 的谱分解：\n$$\nR = V \\Lambda V^{\\top}\n$$\n其中 $V$ 是一个正交矩阵，其列 $v_1, \\dots, v_{p'}$ 是 $R$ 的特征向量，而 $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_{p'})$ 是一个对角矩阵，包含相应的非负特征值，按非递减顺序排序。\n\n每个特征向量 $v_k$ 代表特征空间中的一个主轴，相应的特征值 $\\lambda_k$ 代表标准化数据投影到该轴上的方差。特征间的近线性相关性转化为特征空间中数据方差极小的一个方向。因此，一个非常小的特征值 $\\lambda_k \\approx 0$ 表明存在多重共线性。相应的特征向量 $v_k$ 揭示了这种线性关系的性质。具体来说，如果 $\\lambda_k \\approx 0$，那么标准化特征的线性组合 $Z v_k$ 的方差接近于零。这意味着：\n$$\nZ v_k = \\sum_{j=1}^{p'} Z_{:,j} v_{k,j} \\approx 0\n$$\n其中 $v_{k,j}$ 是特征向量 $v_k$ 的第 $j$ 个分量。这个方程表示 $Z$ 的列之间存在近线性相关性。\n\n所规定的算法将这一原则付诸实践。它识别所有低于给定阈值 $\\tau > 0$ 的特征值 $\\lambda_k$。对于每个这样的小特征值，检查其关联的特征向量 $v_k$。特征向量的分量 $v_{k,j}$ 是原始特征在该线性相关性中的“载荷”。为了解决多重共线性问题，必须移除其中一个相关特征。此处采用的一个常用启发式方法是，识别对该相关性贡献最大的特征——即具有最大绝对载荷 $|v_{k,j}|$ 的特征。设该特征索引为 $j^{\\star}$。如果多个特征表现出相同的最大载荷（在数值容差 $\\epsilon$ 内），则通过选择索引最高的特征来打破平局。这个特征 $j^{\\star}$ 被提议删除。\n\n最终要删除的特征集合是零方差特征（初始识别）集合与基于小特征值分析提议删除的特征集合的并集。这些索引经过除重并按升序排序，以提供一个确定性的最终建议。\n\n算法流程如下：\n1.  对于输入矩阵 $X$，使用分母 $n-1$ 计算每个特征列的标准差。识别标准差为 $0$ 的列的索引集合。这些是首批待移除的候选项。\n2.  从 $X$ 中过滤掉零方差的列，形成一个新矩阵 $X'$。保留从 $X'$ 的列索引到 $X$ 的列索引的映射。如果 $X'$ 只有 $1$ 列或更少，则分析停止，只报告零方差的索引。\n3.  标准化 $X'$ 以获得矩阵 $Z$，其中每列的均值为 $0$，标准差为 $1$。\n4.  计算相关矩阵 $R = \\frac{1}{n-1} Z^{\\top} Z$。\n5.  对 $R$ 进行特征分解，以获得特征值 $\\lambda_k$ 和特征向量 $v_k$。\n6.  使用步骤 1 中的索引初始化一个待删除特征的集合。\n7.  遍历每个特征值 $\\lambda_k$。如果 $\\lambda_k < \\tau$：\n    a. 提取相应的特征向量 $v_k$。\n    b. 找到最大绝对载荷 $m = \\max_j |v_{k,j}|$。\n    c. 在简化的特征集中，识别所有满足 $|v_{k,j'}|$ 与 $m$ 的差距在容差 $\\epsilon$ 内（即 $m - |v_{k,j'}| \\le \\epsilon$）的索引 $j'$。\n    d. 从这些并列的索引中，选择最大的一个 $j'_{\\text{drop}}$。\n    e. 将此简化索引 $j'_{\\text{drop}}$ 映射回其在 $X$ 中的原始索引，并将其添加到待删除特征的集合中。\n8.  将最终的索引集合转换为一个排序列表，并作为给定测试用例的结果返回。对所有测试用例重复此过程。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the multicollinearity diagnosis problem for a suite of test cases.\n    \"\"\"\n\n    # Test case 1: Near-duplicate features\n    X1 = np.array([\n        [0.0, 0.00, 1.00], [1.0, 1.00 + 0.05, -0.50], [2.0, 2.00 - 0.02, 0.70],\n        [3.0, 3.00 + 0.10, -1.20], [4.0, 4.00 - 0.05, 0.30], [5.0, 5.00 + 0.03, 0.80],\n        [6.0, 6.00 - 0.08, -0.90], [7.0, 7.00 + 0.02, 1.10], [8.0, 8.00 + 0.00, -0.40],\n        [9.0, 9.00 - 0.07, 0.20]\n    ])\n    tau1 = 0.05\n    eps1 = 1e-12\n\n    # Test case 2: Approximately independent features\n    X2 = np.array([\n        [0.0, 1.10, 0.50], [1.0, -0.90, 0.30], [2.0, 0.70, -0.60], [3.0, -0.30, 0.70],\n        [4.0, 0.20, -0.80], [5.0, -0.40, 0.10], [6.0, 0.80, 0.20], [7.0, -0.50, -0.30],\n        [8.0, 0.60, 0.40], [9.0, -0.70, -0.20], [10.0, 0.90, 0.00], [11.0, -1.00, 0.50]\n    ])\n    tau2 = 0.10\n    eps2 = 1e-12\n\n    # Test case 3: One feature is a near-linear combination of two others\n    X3 = np.array([\n        [2.00, -1.00, 2.00 + (-1.00) + 0.01], [3.00, -1.10, 3.00 + (-1.10) - 0.02],\n        [4.00, -0.90, 4.00 + (-0.90) + 0.00], [5.50, -1.20, 5.50 + (-1.20) + 0.03],\n        [6.10, -0.80, 6.10 + (-0.80) - 0.01], [7.00, -1.05, 7.00 + (-1.05) + 0.02],\n        [8.20, -1.00, 8.20 + (-1.00) - 0.02], [9.00, -0.95, 9.00 + (-0.95) + 0.01],\n    ])\n    tau3 = 0.10\n    eps3 = 1e-12\n\n    # Test case 4: Four features with two near-dependencies\n    x1_4 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1, 1)\n    x2_4 = np.array([9.00, 7.95, 7.00, 6.05, 5.00, 3.95, 3.00, 2.05, 1.00]).reshape(-1, 1)\n    x3_4 = np.array([1.50, 0.00, 1.70, 0.20, 1.40, 0.30, 1.80, 0.10, 1.60]).reshape(-1, 1)\n    delta4 = np.array([0.01, -0.02, 0.00, 0.03, -0.01, 0.02, -0.02, 0.01, 0.00]).reshape(-1, 1)\n    x4_4 = 0.5 * x1_4 + 0.5 * x3_4 + delta4\n    X4 = np.hstack([x1_4, x2_4, x3_4, x4_4])\n    tau4 = 0.08\n    eps4 = 1e-12\n\n    test_cases = [\n        (X1, tau1, eps1),\n        (X2, tau2, eps2),\n        (X3, tau3, eps3),\n        (X4, tau4, eps4),\n    ]\n\n    all_results = []\n\n    for X, tau, epsilon in test_cases:\n        n, p = X.shape\n        drops_to_propose = set()\n\n        if n  2:\n            # Cannot compute variance, add all features to drop if p > 0\n            if p > 0:\n                drops_to_propose.update(range(p))\n            all_results.append(sorted(list(drops_to_propose)))\n            continue\n\n        # Step 1: Standardize and handle zero-variance features\n        stds = np.std(X, axis=0, ddof=1)\n        zero_var_indices = np.where(stds == 0)[0]\n        drops_to_propose.update(zero_var_indices)\n\n        non_zero_var_mask = stds > 0\n        original_indices_map = np.arange(p)[non_zero_var_mask]\n        \n        # If 1 or 0 features remain, no multicollinearity to analyze\n        if len(original_indices_map) = 1:\n            all_results.append(sorted(list(drops_to_propose)))\n            continue\n        \n        X_filtered = X[:, non_zero_var_mask]\n        means_filtered = np.mean(X_filtered, axis=0)\n        stds_filtered = stds[non_zero_var_mask]\n\n        Z = (X_filtered - means_filtered) / stds_filtered\n\n        # Step 2: Compute the empirical correlation matrix R\n        R = (Z.T @ Z) / (n - 1)\n        \n        # Step 3: Compute the spectral decomposition\n        # np.linalg.eigh is for symmetric matrices, returns sorted eigenvalues\n        eigenvalues, eigenvectors = np.linalg.eigh(R)\n\n        # Step 4: Identify features to drop based on small eigenvalues\n        small_eigenvalue_indices = np.where(eigenvalues  tau)[0]\n        \n        for k in small_eigenvalue_indices:\n            eigenvector = eigenvectors[:, k]\n            abs_loadings = np.abs(eigenvector)\n            max_abs_loading = np.max(abs_loadings)\n            \n            # Identify indices of features tied for the largest loading\n            # A feature's loading is considered 'tied' if it's within epsilon of the max\n            tie_indices_filtered = np.where(max_abs_loading - abs_loadings = epsilon)[0]\n            \n            # Break tie by choosing the highest index\n            drop_candidate_filtered_idx = np.max(tie_indices_filtered)\n            \n            # Map back to original feature index\n            original_idx = original_indices_map[drop_candidate_filtered_idx]\n            drops_to_propose.add(original_idx)\n\n        # Step 5: Aggregate, deduplicate, and sort\n        final_drops = sorted(list(drops_to_propose))\n        all_results.append(final_drops)\n\n    # Final print statement in the exact required format\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3117789"}, {"introduction": "主成分分析（PCA）是一种广泛应用的降维技术，它通过寻找数据中方差最大的方向（即主特征向量）来实现。然而，最大方差方向并不总是包含最多的分类信息。这项实践 [@problem_id:3117809] 旨在揭示一个关键情景：当用于分类的最具区分度的信息恰好存在于低方差方向时，盲目应用PCA会损害模型的预测性能。通过这个练习，你将亲身体会到理解一个算法的目标函数（PCA最大化方差）与其在特定任务（如分类）中的适用性之间的重要联系。", "problem": "考虑两个类别，它们被建模为具有相同先验概率和共同的对称正定协方差矩阵的多元正态分布。设共同的协方差矩阵通过谱分解写为正交特征基，其特征对为 $\\{(\\lambda_i,\\mathbf{u}_i)\\}_{i=1}^p$，并按 $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_p  0$ 排序。设均值差为 $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0$，并定义其在协方差特征基中的坐标为 $c_i = \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu}$，其中 $i \\in \\{1,\\dots,p\\}$。具有 $k$ 个分量的主成分分析（PCA）去噪对应于投影到由 $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$ 张成的子空间上，然后在该子空间中进行分类。\n\n仅从以下经过充分检验的事实和核心定义出发：\n- 对于两个具有共同协方差和相同先验概率的多元正态类别，线性判别分析（LDA）决策规则是贝叶斯最优的。\n- 在给定协方差矩阵下，两个均值之间的马氏距离（Mahalanobis distance）是决定贝叶斯错误率的关键量。\n- 谱分解将协方差矩阵表示为一个具有非负特征值的正交基，并允许向主子空间进行正交投影，\n\n推导最优错分概率如何依赖于马氏距离，首先在全空间中，然后在PCA投影到前 $k$ 个特征向量上之后。利用这一点，根据 $\\{(\\lambda_i,c_i)\\}$ 精确地说明PCA去噪在何时会因移除具有判别性的低方差方向而损害分类性能。\n\n对于下方的每个测试用例，你的程序必须计算由PCA去噪引起的最优错分概率的变化，定义为\n- PCA后最优错分概率减去全空间最优错分概率，\n\n并将此变化作为实数返回。正值表示PCA去噪增加了错误率（损害了分类性能），零表示没有变化，负值则表示有所改善。你必须：\n- 将PCA视为保留与 $k$ 个最大特征值 $\\{\\lambda_1,\\dots,\\lambda_k\\}$ 相关的前 $k$ 个特征向量 $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$，\n- 假设 $k \\in \\{0,1,\\dots,p\\}$，\n- 完全在特征基中进行运算，使用给定的 $(\\lambda_i)$ 和 $(c_i)$，\n- 将所有最终数值输出四舍五入到 $6$ 位小数。\n\n测试套件（每个用例是一个三元组 $(\\boldsymbol{\\lambda},\\mathbf{c},k)$，其中 $\\boldsymbol{\\lambda}$ 是列表 $(\\lambda_1,\\dots,lambda_p)$，$\\mathbf{c}$ 是列表 $(c_1,\\dots,c_p)$）：\n- 案例 $1$ (移除具有判别性的低方差方向): $\\boldsymbol{\\lambda} = (\\,10.0,\\,3.0,\\,1.0,\\,0.2\\,)$, $\\mathbf{c} = (\\,0.0,\\,0.0,\\,0.0,\\,1.0\\,)$, $k = 1$.\n- 案例 $2$ (无损害，因为信号位于最主要的方向上): $\\boldsymbol{\\lambda} = (\\,10.0,\\,3.0,\\,1.0,\\,0.2\\,)$, $\\mathbf{c} = (\\,1.0,\\,0.0,\\,0.0,\\,0.0\\,)$, $k = 1$.\n- 案例 $3$ (各向同性协方差；丢弃一些信号分量导致轻微损害): $\\boldsymbol{\\lambda} = (\\,2.0,\\,2.0,\\,2.0,\\,2.0\\,)$, $\\mathbf{c} = (\\,1.0,\\,0.5,\\,0.3,\\,0.2\\,)$, $k = 2$.\n- 案例 $4$ (边界情况；保留所有分量，因此无变化): $\\boldsymbol{\\lambda} = (\\,5.0,\\,1.0,\\,0.5\\,)$, $\\mathbf{c} = (\\,0.4,\\,0.6,\\,0.8\\,)$, $k = 3$.\n- 案例 $5$ (边界情况；保留 $0$ 个分量，因此分类变为随机猜测): $\\boldsymbol{\\lambda} = (\\,4.0,\\,1.0\\,)$, $\\mathbf{c} = (\\,1.0,\\,0.0\\,)$, $k = 0$.\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,\\dots,r_5]$），其中 $r_j$ 是第 $j$ 个测试用例的最优错分概率变化，四舍五入到 $6$ 位小数。", "solution": "我们考虑两个具有相同先验概率、共同协方差以及均值 $\\boldsymbol{\\mu}_0$ 和 $\\boldsymbol{\\mu}_1$ 的多元正态类别。设 $\\Delta\\boldsymbol{\\mu} = \\boldsymbol{\\mu}_1 - \\boldsymbol{\\mu}_0$。将共同的协方差矩阵记为 $\\boldsymbol{\\Sigma}$，其谱分解为 $\\boldsymbol{\\Sigma} = \\sum_{i=1}^p \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$，其中 $\\{\\mathbf{u}_i\\}_{i=1}^p$ 是一个标准正交基，且对所有 $i$ 都有 $\\lambda_i  0$。定义坐标 $c_i = \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu}$。\n\n基于原理的推导：\n- 对于两个具有相同协方差和相同先验概率的多元正态分布，线性判别分析（LDA）规则是贝叶斯最优的。判别方向与 $\\boldsymbol{\\Sigma}^{-1} \\Delta\\boldsymbol{\\mu}$ 成正比。决定贝叶斯错误率的关键标量是在 $\\boldsymbol{\\Sigma}$ 下两个均值之间的马氏距离，定义为\n$$\nd^2 \\;=\\; \\Delta\\boldsymbol{\\mu}^\\top \\boldsymbol{\\Sigma}^{-1} \\Delta\\boldsymbol{\\mu}.\n$$\n- 使用 $\\boldsymbol{\\Sigma}$ 的谱分解，我们有 $\\boldsymbol{\\Sigma}^{-1} = \\sum_{i=1}^p \\lambda_i^{-1} \\mathbf{u}_i \\mathbf{u}_i^\\top$。因此，\n$$\nd^2 \\;=\\; \\Delta\\boldsymbol{\\mu}^\\top \\left( \\sum_{i=1}^p \\lambda_i^{-1} \\mathbf{u}_i \\mathbf{u}_i^\\top \\right) \\Delta\\boldsymbol{\\mu}\n\\;=\\; \\sum_{i=1}^p \\lambda_i^{-1} \\left( \\mathbf{u}_i^\\top \\Delta\\boldsymbol{\\mu} \\right)^2\n\\;=\\; \\sum_{i=1}^p \\frac{c_i^2}{\\lambda_i}.\n$$\n- 在这种情况下，对于相同先验概率，贝叶斯错分概率是 $d$ 的单调递减函数，由标准正态累积分布函数（CDF）在 $-d/2$ 处的值给出：\n$$\nP_{\\text{err, full}} \\;=\\; \\Phi\\!\\left( -\\,\\frac{d}{2} \\right),\n$$\n其中 $\\Phi$ 是标准正态CDF。\n\n现在考虑保留前 $k$ 个特征向量 $\\{\\mathbf{u}_1,\\dots,\\mathbf{u}_k\\}$ 的主成分分析（PCA）去噪。设到该子空间的正交投影算子为 $\\mathbf{P}_k = \\sum_{i=1}^k \\mathbf{u}_i \\mathbf{u}_i^\\top$。在投影空间中，有效均值差为 $\\mathbf{P}_k \\Delta\\boldsymbol{\\mu}$，有效协方差为限制在 $k$ 维子空间中的 $\\mathbf{P}_k \\boldsymbol{\\Sigma} \\mathbf{P}_k = \\sum_{i=1}^k \\lambda_i \\mathbf{u}_i \\mathbf{u}_i^\\top$。此子空间中对应的马氏距离变为\n$$\nd_k^2 \\;=\\; (\\mathbf{P}_k \\Delta\\boldsymbol{\\mu})^\\top \\left( \\mathbf{P}_k \\boldsymbol{\\Sigma} \\mathbf{P}_k \\right)^{\\!-1} (\\mathbf{P}_k \\Delta\\boldsymbol{\\mu})\n\\;=\\; \\sum_{i=1}^k \\frac{c_i^2}{\\lambda_i}.\n$$\n因此，经过PCA去噪后的最优错分概率为\n$$\nP_{\\text{err, PCA}(k)} \\;=\\; \\Phi\\!\\left( -\\,\\frac{d_k}{2} \\right).\n$$\n由PCA去噪引起的错误率变化为\n$$\n\\Delta P_{\\text{err}}(k) \\;=\\; P_{\\text{err, PCA}(k)} \\;-\\; P_{\\text{err, full}}\n\\;=\\; \\Phi\\!\\left( -\\,\\frac{1}{2}\\sqrt{\\sum_{i=1}^k \\frac{c_i^2}{\\lambda_i}} \\right)\n\\;-\\; \\Phi\\!\\left( -\\,\\frac{1}{2}\\sqrt{\\sum_{i=1}^p \\frac{c_i^2}{\\lambda_i}} \\right).\n$$\n从这个表达式可以立即得出：\n- 如果所有的判别能量都位于被丢弃的低方差方向（即，对于 $i \\le k$ 有 $c_i = 0$，而对于某个 $j  k$ 有 $c_j \\ne 0$），那么 $d_k = 0$ 且 $P_{\\text{err, PCA}(k)} = \\Phi(0) = 0.5$，而 $P_{\\text{err, full}}  0.5$；因此 $\\Delta P_{\\text{err}}(k)  0$，PCA会损害分类性能。\n- 如果所有的判别能量都位于前 $k$ 个方向内（即，对于 $i  k$ 有 $c_i = 0$），那么 $d_k = d$ 且 $\\Delta P_{\\text{err}}(k) = 0$；PCA不改变性能。\n- 如果部分判别能量位于前 $k$ 个方向之外，那么 $0 \\le d_k  d$ 且 $\\Delta P_{\\text{err}}(k)  0$；PCA通过移除有用的方向而损害分类性能。当被移除的方向具有较小的 $\\lambda_i$（低方差）但不可忽略的 $|c_i|$（高判别性内容）时，损害通常更大，因为它们通过 $c_i^2/\\lambda_i$ 对 $d^2$ 贡献很大。\n\n计算的算法设计：\n- 对每个测试用例，计算 $d^2 = \\sum_{i=1}^p c_i^2/\\lambda_i$ 和 $d_k^2 = \\sum_{i=1}^k c_i^2/\\lambda_i$（约定 $d_0^2 = 0$）。\n- 计算 $P_{\\text{err, full}} = \\Phi(-\\sqrt{d^2}/2)$ 和 $P_{\\text{err, PCA}(k)} = \\Phi(-\\sqrt{d_k^2}/2)$，使用 $\\Phi(x) = \\tfrac{1}{2}\\left(1 + \\operatorname{erf}\\!\\left(\\tfrac{x}{\\sqrt{2}}\\right)\\right)$。\n- 输出 $\\Delta P_{\\text{err}}(k) = P_{\\text{err, PCA}(k)} - P_{\\text{err, full}}$，四舍五入到 $6$ 位小数。\n\n对所提供案例的定性预期：\n- 案例 1：损害严重，因为所有信号都位于方差最小的方向上，而当 $k = 1$ 时该方向被丢弃。\n- 案例 2：没有损害，因为信号完全位于PCA保留的最主要方向上。\n- 案例 3：轻微损害，因为各向同性的协方差意味着每个被丢弃的分量都会移除成比例的判别性内容。\n- 案例 4：没有损害，因为 $k = p$，所以没有分量被丢弃。\n- 案例 5：在给定设置下损害最大，因为 $k = 0$ 导致错误率为随机水平的 $0.5$。\n\n程序实现了这些计算，并将四舍五入后的变化打印为单个方括号括起来的列表。", "answer": "```python\nimport numpy as np\nimport math\n\ndef normal_cdf(x: float) - float:\n    # Standard normal CDF using the error function\n    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))\n\ndef bayes_error_change(lambdas, c, k) - float:\n    lambdas = np.array(lambdas, dtype=float)\n    c = np.array(c, dtype=float)\n    # Ensure inputs are consistent\n    assert lambdas.ndim == 1 and c.ndim == 1 and lambdas.shape == c.shape\n    p = lambdas.size\n    assert 0 = k = p\n\n    # Compute squared Mahalanobis distances\n    inv_terms = (c ** 2) / lambdas\n    d2_full = float(np.sum(inv_terms))\n    d2_k = float(np.sum(inv_terms[:k])) if k  0 else 0.0\n\n    # Convert to Bayes errors using Phi(-d/2)\n    err_full = normal_cdf(-math.sqrt(d2_full) / 2.0) if d2_full  0.0 else 0.5\n    err_k = normal_cdf(-math.sqrt(d2_k) / 2.0) if d2_k  0.0 else 0.5\n\n    # Change in error (post-PCA minus full)\n    return err_k - err_full\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (lambdas, c, k)\n    test_cases = [\n        # Case 1\n        ([10.0, 3.0, 1.0, 0.2], [0.0, 0.0, 0.0, 1.0], 1),\n        # Case 2\n        ([10.0, 3.0, 1.0, 0.2], [1.0, 0.0, 0.0, 0.0], 1),\n        # Case 3\n        ([2.0, 2.0, 2.0, 2.0], [1.0, 0.5, 0.3, 0.2], 2),\n        # Case 4\n        ([5.0, 1.0, 0.5], [0.4, 0.6, 0.8], 3),\n        # Case 5\n        ([4.0, 1.0], [1.0, 0.0], 0),\n    ]\n\n    results = []\n    for lambdas, c, k in test_cases:\n        delta_err = bayes_error_change(lambdas, c, k)\n        results.append(f\"{delta_err:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3117809"}, {"introduction": "谱方法的应用远不止于分析协方差矩阵。通过分析根据样本间相似性构建的图，这些方法同样可以揭示数据中隐藏的结构。本练习 [@problem_id:3117759] 将展示如何构建一个图拉普拉斯矩阵，并利用其特征向量创建一个“谱嵌入”，将相似的数据点在新的低维空间中聚集在一起。这个过程清晰地展示了特征向量在发现非线性模式和执行半监督学习等高级任务中的强大能力。", "problem": "给定一些小型的已标记和未标记数据集，旨在阐释统计学习中的谱方法。目标是使用余弦相似度从样本构建加权图，建立图拉普拉斯矩阵，并利用基于拉普拉斯特征向量的谱嵌入，在嵌入空间中通过最近原型法执行半监督分类。\n\n出发点：仅使用相似图的基本定义和对称矩阵的谱性质。具体而言：\n- 两个向量 $\\mathbf{x}_i \\in \\mathbb{R}^d$ 和 $\\mathbf{x}_j \\in \\mathbb{R}^d$ 之间的余弦相似度定义为 $\\mathrm{cos}(\\theta_{ij}) = \\dfrac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\|\\mathbf{x}_i\\|\\|\\mathbf{x}_j\\|}$，并约定如果任一范数为零，则相似度设为 $0$。\n- 给定非负对称权重 $w_{ij}$（其中 $i \\neq j$）和 $w_{ii} = 0$，定义度 $d_i = \\sum_{j=1}^n w_{ij}$ 和未归一化的图拉普拉斯矩阵 $L = D - W$，其中 $D = \\mathrm{diag}(d_1,\\dots,d_n)$ 且 $W = [w_{ij}]$。\n- 对于任意非零向量 $\\mathbf{f} \\in \\mathbb{R}^n$，$L$ 的瑞利商为 $R(\\mathbf{f}) = \\dfrac{\\mathbf{f}^\\top L \\mathbf{f}}{\\mathbf{f}^\\top \\mathbf{f}}$。对称矩阵 $L$ 拥有一组与实数特征值相关的标准正交特征向量基，这些特征值按 $0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$ 排序。在正交性约束下 $R(\\mathbf{f})$ 的最小化子对应于 $L$ 的特征向量。\n\n任务：\n- 对于每个数据集，在 $n$ 个样本上使用余弦相似度构建一个全连接的相似图，但将负值截断为零：对于 $i \\neq j$，定义 $w_{ij} = \\max\\{0, \\mathrm{cos}(\\theta_{ij})\\}$，且 $w_{ii} = 0$。\n- 构建未归一化的拉普拉斯矩阵 $L = D - W$。\n- 按如下方式计算 $k$ 维谱嵌入。设 $k$ 为已知的类别数。计算 $L$ 的所有特征对，按升序对特征值进行排序，舍弃对应于最小特征值的特征向量，然后取接下来的 $k$ 个特征向量作为嵌入矩阵 $Y \\in \\mathbb{R}^{n \\times k}$ 的列。样本 $i$ 的嵌入坐标是 $Y$ 的第 $i$ 行 $\\mathbf{y}_i^\\top$。\n- 半监督分类：对于每个类别 $c \\in \\{0,\\dots,k-1\\}$，在嵌入空间中计算一个类别原型，该原型为所有标记为类别 $c$ 的样本的 $\\mathbf{y}_i$ 的算术平均值。然后将每个未标记的样本分配给其原型在欧几里得距离上最近的类别。通过选择最小的类别索引来解决距离相等的情况。\n- 计算在未标记子集上的准确率，即预测类别与提供的真实类别相等的未标记样本所占的比例。将准确率表示为 $[0,1]$ 范围的小数。\n\n为以下测试套件实现上述过程。在每种情况下，$X$ 是数据矩阵，样本为行；$y$ 是标签向量，其中 $-1$ 表示未标记；$y^{\\mathrm{true}}$ 是真实标签向量。所有数值条目均为实数，应视为无量纲。\n\n测试用例 A（两个潜在类别，在 $\\mathbb{R}^3$ 中具有中等分离度）：\n- 样本数 $n = 8$，特征数 $d = 3$，类别数 $k = 2$。\n- 数据矩阵 $X_A$ 的行向量为\n  $\\big(1.0, 0.0, 0.0\\big)$,\n  $\\big(0.9, 0.1, 0.0\\big)$,\n  $\\big(0.95, 0.0, 0.1\\big)$,\n  $\\big(0.85, 0.2, 0.1\\big)$,\n  $\\big(0.0, 1.0, 0.0\\big)$,\n  $\\big(0.1, 0.9, 0.0\\big)$,\n  $\\big(0.0, 0.95, 0.1\\big)$,\n  $\\big(0.2, 0.85, 0.1\\big)$.\n- 真实标签 $y^{\\mathrm{true}}_A = [0, 0, 0, 0, 1, 1, 1, 1]$。\n- 半监督标签 $y_A = [0, -1, -1, -1, 1, -1, -1, -1]$。\n\n测试用例 B（三个潜在类别，在 $\\mathbb{R}^2$ 中大约相隔 $120^\\circ$）：\n- 样本数 $n = 9$，特征数 $d = 2$，类别数 $k = 3$。\n- 设 $s = \\sqrt{3} / 2 \\approx 0.8660254$。\n- 数据矩阵 $X_B$ 的行向量为\n  $\\big(1.0, 0.0\\big)$,\n  $\\big(0.95, 0.1\\big)$,\n  $\\big(0.9, -0.1\\big)$,\n  $\\big(-0.5, s\\big)$,\n  $\\big(-0.6, 0.8\\big)$,\n  $\\big(-0.4, 0.9\\big)$,\n  $\\big(-0.5, -s\\big)$,\n  $\\big(-0.6, -0.8\\big)$,\n  $\\big(-0.4, -0.9\\big)$.\n- 真实标签 $y^{\\mathrm{true}}_B = [0, 0, 0, 1, 1, 1, 2, 2, 2]$。\n- 半监督标签 $y_B = [0, -1, -1, 1, -1, -1, 2, -1, -1]$。\n\n测试用例 C（两个潜在类别，在 $\\mathbb{R}^3$ 中大约相隔 $30^\\circ$）：\n- 样本数 $n = 8$，特征数 $d = 3$，类别数 $k = 2$。\n- 设 $c = \\cos(\\pi/6) \\approx 0.8660254$ 且 $t = \\sin(\\pi/6) = 0.5$。\n- 数据矩阵 $X_C$ 的行向量为\n  $\\big(1.0, 0.0, 0.0\\big)$,\n  $\\big(0.95, 0.1, 0.0\\big)$,\n  $\\big(1.0, 0.05, 0.0\\big)$,\n  $\\big(0.9, 0.2, 0.0\\big)$,\n  $\\big(c, t, 0.0\\big)$,\n  $\\big(0.8, 0.55, 0.0\\big)$,\n  $\\big(0.9, 0.45, 0.0\\big)$,\n  $\\big(0.85, 0.52, 0.05\\big)$.\n- 真实标签 $y^{\\mathrm{true}}_C = [0, 0, 0, 0, 1, 1, 1, 1]$。\n- 半监督标签 $y_C = [0, -1, -1, -1, 1, -1, -1, -1]$。\n\n程序要求：\n- 对于每个测试用例，完全按照描述构建 $W$、$D$ 和 $L$；通过舍弃对应于最小特征值的特征向量并按升序取接下来的 $k$ 个特征向量来计算谱嵌入 $Y$；在嵌入空间中计算类别原型，并使用欧几里得距离和指定的平局决胜规则对未标记点进行分类；计算未标记样本的准确率，表示为 $[0,1]$ 范围内的小数，并四舍五入到六位小数。\n- 您的程序应生成单行输出，其中包含测试用例 A、测试用例 B 和测试用例 C 的三个准确率，按此顺序排列，格式为用方括号括起来的逗号分隔列表，例如 $[0.500000,1.000000,0.666667]$。\n\n程序没有额外的输入，也不允许使用外部数据源。所有计算都是无量纲的，因此不需要物理单位。三角函数常数中的角度以弧度为单位。确保所有数值常数在适用时均使用上述值实现。", "solution": "该问题要求实现一个基于谱图理论的半监督分类算法。该过程将应用于三个不同的测试用例，并需要报告每个数据集未标记部分的准确率。该方法可以分解为五个主要步骤：图构建、拉普拉斯矩阵构建、谱嵌入、分类和评估。\n\n### 步骤 1：邻接矩阵构建\n首先，我们通过构建一个加权邻接矩阵 $W \\in \\mathbb{R}^{n \\times n}$ 来建模 $n$ 个数据样本（表示为向量 $\\mathbf{x}_1, \\dots, \\mathbf{x}_n \\in \\mathbb{R}^d$）之间的关系。该图是全连接的。两个不同样本 $\\mathbf{x}_i$ 和 $\\mathbf{x}_j$ 之间的权重 $w_{ij}$ 来自它们的余弦相似度，该相似度衡量它们之间夹角的余弦值：\n$$\n\\mathrm{cos}(\\theta_{ij}) = \\frac{\\mathbf{x}_i^\\top \\mathbf{x}_j}{\\|\\mathbf{x}_i\\|_2 \\|\\mathbf{x}_j\\|_2}\n$$\n问题指定了一个约定，即如果任一向量的范数为零，则相似度为 $0$。根据问题的指示，负的相似度值被截断为零。权重矩阵的对角线元素设置为零。因此，权重定义如下：\n$$\nw_{ij} = \\begin{cases} \\max\\{0, \\mathrm{cos}(\\theta_{ij})\\}  \\text{if } i \\neq j \\\\ 0  \\text{if } i = j \\end{cases}\n$$\n得到的矩阵 $W$ 是一个具有非负项的对称矩阵。\n\n### 步骤 2：图拉普拉斯矩阵构建\n根据权重矩阵 $W$，我们构建未归一化的图拉普拉斯矩阵 $L$。这需要度矩阵 $D$，它是一个对角矩阵，其对角项 $D_{ii} = d_i$ 是连接到顶点 $i$ 的所有边的权重之和：\n$$\nd_i = \\sum_{j=1}^n w_{ij}\n$$\n未归一化的图拉普拉斯矩阵则由下式给出：\n$$\nL = D - W\n$$\n$L$ 是一个对称半正定矩阵，这是后续谱分析的关键属性。\n\n### 步骤 3：谱嵌入\n谱方法的核心在于分析图拉普拉斯矩阵 $L$ 的特征系统。由于 $L$ 是实对称矩阵，它有 $n$ 个完整的实特征值 $0 = \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$，以及一组相应的标准正交特征向量基 $\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n$。对应于最小非零特征值的特征向量（称为 Fiedler 向量）捕捉了图的全局结构，有效地将其划分为簇。\n\n问题指定了构建一个 $k$ 维嵌入，其中 $k$ 是类别数。我们计算 $L$ 的所有特征对。与最小特征值 $\\lambda_1 = 0$ 相关联的特征向量 $\\mathbf{v}_1$ 被舍弃，因为它是一个常数向量（对于连通图而言）并且不包含用于聚类的信息。通过将接下来的 $k$ 个特征向量作为列来形成嵌入矩阵 $Y \\in \\mathbb{R}^{n \\times k}$：\n$$\nY = [\\mathbf{v}_2, \\mathbf{v}_3, \\dots, \\mathbf{v}_{k+1}]\n$$\n$Y$ 的第 $i$ 行，记作 $\\mathbf{y}_i^\\top$，作为原始样本 $\\mathbf{x}_i$ 的新的 $k$ 维坐标向量。这种从 $\\mathbb{R}^d$ 到 $\\mathbb{R}^k$ 的映射就是谱嵌入。\n\n### 步骤 4：半监督分类\n在将数据嵌入到低维空间后，我们执行分类。数据集为一小部分样本提供了标签，其余样本未标记。对于每个类别 $c \\in \\{0, \\dots, k-1\\}$，我们通过计算属于该类的所有已标记样本的嵌入向量的算术平均值来计算类别原型 $\\mathbf{p}_c$。设 $I_c$ 是标记为类别 $c$ 的样本索引集。原型为：\n$$\n\\mathbf{p}_c = \\frac{1}{|I_c|} \\sum_{i \\in I_c} \\mathbf{y}_i\n$$\n然后，通过将每个未标记的样本 $j$ 分配给嵌入空间中最近原型的类别来进行分类。距离度量是欧几里得距离。预测的标签 $\\hat{y}_j$ 是：\n$$\n\\hat{y}_j = \\arg\\min_{c \\in \\{0, \\dots, k-1\\}} \\|\\mathbf{y}_j - \\mathbf{p}_c\\|_2\n$$\n距离相等的情况通过选择具有最小索引的类别来解决。\n\n### 步骤 5：准确率评估\n最后一步是评估此半监督分类器的性能。准确率计算为预测标签 $\\hat{y}_j$ 与提供的真实标签 $y^{\\mathrm{true}}_j$ 相匹配的未标记样本的比例。设 $S_U$ 为未标记样本的索引集。准确率为：\n$$\n\\text{Accuracy} = \\frac{1}{|S_U|} \\sum_{j \\in S_U} \\mathbb{I}(\\hat{y}_j = y^{\\mathrm{true}}_j)\n$$\n其中 $\\mathbb{I}(\\cdot)$ 是指示函数，如果其参数为真，则为 $1$，否则为 $0$。该过程系统地应用于提供的三个测试用例。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef compute_accuracy(X, y, y_true, k):\n    \"\"\"\n    Computes semi-supervised classification accuracy using spectral methods.\n    \n    Args:\n        X (np.ndarray): Data matrix (n_samples, n_features).\n        y (np.ndarray): Label vector (-1 for unlabeled).\n        y_true (np.ndarray): Ground-truth label vector.\n        k (int): Number of classes.\n\n    Returns:\n        float: Accuracy on the unlabeled subset.\n    \"\"\"\n    n = X.shape[0]\n\n    # Step 1: Construct the Adjacency Matrix W\n    # Normalize rows of X to compute cosine similarity more efficiently.\n    norms = np.linalg.norm(X, axis=1, keepdims=True)\n    X_normalized = np.divide(X, norms, out=np.zeros_like(X, dtype=float), where=norms!=0)\n    \n    # Cosine similarity matrix is the dot product of normalized vectors.\n    cos_sim_matrix = X_normalized @ X_normalized.T\n    \n    # Build W by clipping negative values and setting diagonal to zero.\n    W = np.maximum(0, cos_sim_matrix)\n    np.fill_diagonal(W, 0)\n    \n    # Step 2: Construct the Graph Laplacian L\n    d = np.sum(W, axis=1)\n    D = np.diag(d)\n    L = D - W\n    \n    # Step 3: Compute the Spectral Embedding Y\n    # eigh returns eigenvalues in ascending order and corresponding eigenvectors.\n    eigenvalues, eigenvectors = np.linalg.eigh(L)\n    \n    # Discard the eigenvector for the smallest eigenvalue and take the next k.\n    Y = eigenvectors[:, 1:k+1]\n    \n    # Step 4: Semi-supervised Classification\n    labeled_mask = (y != -1)\n    unlabeled_mask = ~labeled_mask\n    \n    # Compute class prototypes in the embedding space.\n    prototypes = []\n    for c in range(k):\n        class_mask = (y == c)\n        class_embeddings = Y[class_mask, :]\n        \n        # This check is for robustness; problem data guarantees labeled samples for each class.\n        if class_embeddings.shape[0] > 0:\n            prototype = np.mean(class_embeddings, axis=0)\n            prototypes.append(prototype)\n        else:\n            # Handle case where a class has no labeled samples (not in this problem)\n            # A possible strategy is to place the prototype at the origin.\n            prototypes.append(np.zeros(k))\n\n    prototypes = np.array(prototypes)\n    \n    # Classify unlabeled samples by nearest prototype.\n    unlabeled_embeddings = Y[unlabeled_mask, :]\n    predictions = []\n    for emb in unlabeled_embeddings:\n        # Calculate Euclidean distance to all prototypes\n        distances = np.linalg.norm(prototypes - emb, axis=1)\n        # argmin breaks ties by choosing the smallest index, as required.\n        predicted_class = np.argmin(distances)\n        predictions.append(predicted_class)\n    \n    predictions = np.array(predictions)\n    \n    # Step 5: Calculate Accuracy\n    true_labels_unlabeled = y_true[unlabeled_mask]\n    \n    correct_count = np.sum(predictions == true_labels_unlabeled)\n    total_unlabeled = len(true_labels_unlabeled)\n    \n    if total_unlabeled == 0:\n        accuracy = 1.0 #Convention for no unlabeled points\n    else:\n        accuracy = correct_count / total_unlabeled\n        \n    return accuracy\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the algorithm, and print results.\n    \"\"\"\n    # Define constants for test cases B and C\n    s_b = np.sqrt(3) / 2\n    c_c = np.cos(np.pi / 6)\n    t_c = np.sin(np.pi / 6)\n\n    test_cases = [\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 0.0], [0.9, 0.1, 0.0], [0.95, 0.0, 0.1], [0.85, 0.2, 0.1],\n                [0.0, 1.0, 0.0], [0.1, 0.9, 0.0], [0.0, 0.95, 0.1], [0.2, 0.85, 0.1]\n            ]),\n            \"y\": np.array([0, -1, -1, -1, 1, -1, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"k\": 2\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.0], [0.95, 0.1], [0.9, -0.1],\n                [-0.5, s_b], [-0.6, 0.8], [-0.4, 0.9],\n                [-0.5, -s_b], [-0.6, -0.8], [-0.4, -0.9]\n            ]),\n            \"y\": np.array([0, -1, -1, 1, -1, -1, 2, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]),\n            \"k\": 3\n        },\n        {\n            \"X\": np.array([\n                [1.0, 0.0, 0.0], [0.95, 0.1, 0.0], [1.0, 0.05, 0.0], [0.9, 0.2, 0.0],\n                [c_c, t_c, 0.0], [0.8, 0.55, 0.0], [0.9, 0.45, 0.0], [0.85, 0.52, 0.05]\n            ]),\n            \"y\": np.array([0, -1, -1, -1, 1, -1, -1, -1]),\n            \"y_true\": np.array([0, 0, 0, 0, 1, 1, 1, 1]),\n            \"k\": 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        accuracy = compute_accuracy(case[\"X\"], case[\"y\"], case[\"y_true\"], case[\"k\"])\n        results.append(accuracy)\n\n    # Format output to six decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3117759"}]}