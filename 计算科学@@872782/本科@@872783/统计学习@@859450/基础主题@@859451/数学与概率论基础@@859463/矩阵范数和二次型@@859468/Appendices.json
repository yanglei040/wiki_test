{"hands_on_practices": [{"introduction": "现实世界的数据很少是完美的。本练习探讨一个特征被污染的情景，这是应用机器学习中的一个常见挑战。通过解决这个问题，你将亲眼看到谱范数如何量化数据扰动的程度，以及二次损失函数如何衡量由此导致的性能下降。最重要的是，你将推导出一个有针对性的正则化策略，证明正则化不仅是控制模型复杂度的强大工具，也是主动减轻特定数据错误的精确手段。[@problem_id:3146460]", "problem": "考虑一个监督学习情景，其中特征矩阵为 $X \\in \\mathbb{R}^{2 \\times 2}$，响应向量为 $y \\in \\mathbb{R}^{2}$。您将分析特征的秩一扰动如何影响谱范数和二次损失，然后推导出一个正则化选项，以减轻扰动在其主导方向上的影响。\n\n使用以下基本定义作为您的起点：\n- 矩阵 $A$ 的谱范数 $\\|A\\|_{2}$ 是 $A$ 的最大奇异值，等价于 $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$，其中 $\\lambda_{\\max}(\\cdot)$ 表示最大特征值。\n- 对于权重 $w \\in \\mathbb{R}^{2}$，经验二次损失为 $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$。\n- 普通最小二乘法 (OLS) 求解 $\\min_{w} \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$。\n- 岭回归求解 $\\min_{w} \\frac{1}{2}\\|Xw - y\\|_{2}^{2} + \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$，其中 $\\lambda  0$。\n\n设干净数据为\n$$\nX = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n假设特征被一个秩一扰动 $\\Delta X = u v^{\\top}$ 所污染，其中\n$$\nu = \\begin{pmatrix} a \\\\ 0 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a \\geq 0,\n$$\n因此 $\\Delta X = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}$，被污染的特征矩阵为 $X_{\\text{cor}} = X + \\Delta X$。\n\n任务：\n1. 仅使用谱范数的定义，计算由污染引起的谱范数的精确变化，即求出 $\\|X_{\\text{cor}}\\|_{2} - \\|X\\|_{2}$，结果表示为关于 $a$ 的表达式。\n2. 设 $w_{\\text{OLS}}$ 表示在干净数据 $(X,y)$ 上训练的 OLS 解。计算当 $w_{\\text{OLS}}$ 在被污染的特征上评估时，经验二次损失的精确变化，即求出 $L_{\\text{cor}}(w_{\\text{OLS}}) - L(w_{\\text{OLS}})$，结果表示为关于 $a$ 的表达式，其中 $L_{\\text{cor}}(w) = \\frac{1}{2}\\|X_{\\text{cor}}w - y\\|_{2}^{2}$。\n3. 在干净数据上训练岭回归，以获得作为 $\\lambda$ 函数的 $w_{\\lambda}$。推导正则化参数 $\\lambda$ 的一个值（作为 $a$ 的函数），当 $w_{\\lambda}$ 在被污染的特征上评估时，该值能完全抵消残差向量第一个坐标中的污染，即选择 $\\lambda$ 使得 $(X_{\\text{cor}}w_{\\lambda} - y)$ 的第一个条目恰好为零。\n\n将任务 1-3 的最终结果以关于 $a$ 的精确表达式形式，按顺序（谱范数的变化、二次损失的变化、以及起缓解作用的正则化参数）报告在一个单行矩阵中。无需四舍五入。最终答案不带单位。", "solution": "问题陈述定义明确、数学上一致，并基于线性代数和统计学习的原理。我们将按指定顺序解决这三个任务。\n\n设给定的矩阵和向量为：\n$$\nX = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n$$\n污染由 $\\Delta X = \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix}$ 给出，其中 $a \\geq 0$。被污染的矩阵是：\n$$\nX_{\\text{cor}} = X + \\Delta X = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} a  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix}\n$$\n\n**任务1：谱范数的变化**\n\n矩阵 $A$ 的谱范数定义为 $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^{\\top}A)}$。\n\n首先，我们计算干净矩阵 $X$ 的谱范数。\n$$\nX^{\\top}X = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}\n$$\n这个对角矩阵的特征值是其对角线元素，$\\lambda_1 = 9$ 和 $\\lambda_2 = 1$。最大特征值为 $\\lambda_{\\max}(X^{\\top}X) = 9$。\n因此，$X$ 的谱范数为：\n$$\n\\|X\\|_{2} = \\sqrt{9} = 3\n$$\n\n接下来，我们计算被污染矩阵 $X_{\\text{cor}}$ 的谱范数。\n$$\nX_{\\text{cor}}^{\\top}X_{\\text{cor}} = \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} (3+a)^2  0 \\\\ 0  1 \\end{pmatrix}\n$$\n特征值为 $(3+a)^2$ 和 $1$。由于 $a \\geq 0$，我们有 $3+a \\geq 3$，因此 $(3+a)^2 \\geq 9$。最大特征值为 $\\lambda_{\\max}(X_{\\text{cor}}^{\\top}X_{\\text{cor}}) = (3+a)^2$。\n因此，$X_{\\text{cor}}$ 的谱范数为：\n$$\n\\|X_{\\text{cor}}\\|_{2} = \\sqrt{(3+a)^2} = |3+a| = 3+a\n$$\n谱范数的变化为：\n$$\n\\|X_{\\text{cor}}\\|_{2} - \\|X\\|_{2} = (3+a) - 3 = a\n$$\n\n**任务2：二次损失的变化**\n\n首先，我们为干净数据 $(X, y)$ 找到普通最小二乘法 (OLS) 解 $w_{\\text{OLS}}$。OLS 解最小化 $L(w) = \\frac{1}{2}\\|Xw - y\\|_{2}^{2}$，并由 $w_{\\text{OLS}} = (X^{\\top}X)^{-1}X^{\\top}y$ 给出。由于 $X$ 是可逆的，这可以简化为 $w_{\\text{OLS}} = X^{-1}y$。\n$$\nX^{-1} = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nw_{\\text{OLS}} = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n此解在干净数据上的损失为：\n$$\nL(w_{\\text{OLS}}) = \\frac{1}{2}\\|Xw_{\\text{OLS}} - y\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\|\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\\|_{2}^{2} = 0\n$$\n现在，我们在被污染的数据上评估 $w_{\\text{OLS}}$ 的损失，$L_{\\text{cor}}(w_{\\text{OLS}}) = \\frac{1}{2}\\|X_{\\text{cor}}w_{\\text{OLS}} - y\\|_{2}^{2}$。\n$$\nL_{\\text{cor}}(w_{\\text{OLS}}) = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} 3+a \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}\\left\\|\\begin{pmatrix} a \\\\ 0 \\end{pmatrix}\\right\\|_{2}^{2} = \\frac{1}{2}(a^2) = \\frac{a^2}{2}\n$$\n经验二次损失的变化为：\n$$\nL_{\\text{cor}}(w_{\\text{OLS}}) - L(w_{\\text{OLS}}) = \\frac{a^2}{2} - 0 = \\frac{a^2}{2}\n$$\n\n**任务3：起缓解作用的正则化参数**\n\n我们首先找到在干净数据上训练的岭回归解 $w_{\\lambda}$，它由 $w_{\\lambda} = (X^{\\top}X + \\lambda I)^{-1}X^{\\top}y$ 给出。\n我们已经计算出 $X^{\\top}X = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix}$。我们还需要 $X^{\\top}y$：\n$$\nX^{\\top}y = \\begin{pmatrix} 3  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 9 \\\\ 1 \\end{pmatrix}\n$$\n现在我们构造待求逆的项：\n$$\nX^{\\top}X + \\lambda I = \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} \\lambda  0 \\\\ 0  \\lambda \\end{pmatrix} = \\begin{pmatrix} 9+\\lambda  0 \\\\ 0  1+\\lambda \\end{pmatrix}\n$$\n其逆矩阵为：\n$$\n(X^{\\top}X + \\lambda I)^{-1} = \\begin{pmatrix} \\frac{1}{9+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix}\n$$\n那么岭回归解为：\n$$\nw_{\\lambda} = \\begin{pmatrix} \\frac{1}{9+\\lambda}  0 \\\\ 0  \\frac{1}{1+\\lambda} \\end{pmatrix} \\begin{pmatrix} 9 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix}\n$$\n我们需要找到 $\\lambda$，使得残差向量 $(X_{\\text{cor}}w_{\\lambda} - y)$ 的第一个条目为零。设此残差为 $r_{\\text{cor}}$。\n$$\nr_{\\text{cor}} = X_{\\text{cor}}w_{\\lambda} - y = \\begin{pmatrix} 3+a  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (3+a)\\frac{9}{9+\\lambda} \\\\ \\frac{1}{1+\\lambda} \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9(3+a)}{9+\\lambda} - 3 \\\\ \\frac{1}{1+\\lambda} - 1 \\end{pmatrix}\n$$\n将第一个条目设为零：\n$$\n\\frac{9(3+a)}{9+\\lambda} - 3 = 0\n$$\n$$\n9(3+a) = 3(9+\\lambda)\n$$\n两边除以 $3$：\n$$\n3(3+a) = 9+\\lambda\n$$\n$$\n9 + 3a = 9 + \\lambda\n$$\n解出 $\\lambda$ 得到：\n$$\n\\lambda = 3a\n$$\n这个 $\\lambda$ 值确保了污染在残差第一个坐标上的影响被抵消。对于任何 $a0$，岭回归的条件 $\\lambda  0$ 都得到满足。如果 $a=0$，则 $\\lambda=0$，对应于残差已经为零的 OLS 情况。", "answer": "$$\n\\boxed{\\begin{pmatrix} a  \\frac{a^2}{2}  3a \\end{pmatrix}}\n$$", "id": "3146460"}, {"introduction": "特征白化是一种标准的预处理步骤，可以改善模型训练，但当数据协方差矩阵秩亏时，它会变得不稳定。本练习通过引入一个小的正则化项来解决这个问题，这是一种广泛用于确保数值稳定性的技术。你将使用谱定理来分析这种正则化的效果，推导出它与理想化白化过程相比所引入的“失真”的精确表达式，从而更深入地理解在实际数据处理中涉及的权衡。[@problem_id:3146475]", "problem": "在一个用于统计学习的特征预处理流水线中，一个估计的协方差矩阵 $\\hat{\\Sigma} \\in \\mathbb{R}^{4 \\times 4}$被用于白化。然而，由于数据有限，$\\hat{\\Sigma}$是秩亏的。为了稳定白化过程，实践者会添加一个小的岭正则化，并使用 $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$（其中$\\epsilon  0$）。由此产生的近似白化变换为 $\\hat{\\Sigma}_{\\epsilon}^{-1/2}$，对于一个特征向量 $x \\in \\mathbb{R}^{4}$，其对应的白化平方范数为 $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$，这是根据恒等式 $\\| \\hat{\\Sigma}_{\\epsilon}^{-1/2} x \\|_{2}^{2} = x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$ 得出的。\n\n假设 $\\hat{\\Sigma}$ 是对称半正定矩阵，其标准正交特征向量为 $\\{u_{1}, u_{2}, u_{3}, u_{4}\\}$，对应的特征值为 $\\lambda_{1} = 5$, $\\lambda_{2} = 2$, $\\lambda_{3} = 0$, $\\lambda_{4} = 0$。考虑一个特征向量 $x$，其谱分解为 $x = 3 u_{1} + 2 u_{2} + 1 u_{3} + 2 u_{4}$。\n\n一种理想化的白化概念会忽略 $\\hat{\\Sigma}$ 零空间中的方向，它使用 Moore–Penrose 伪逆 (MPP) $\\hat{\\Sigma}^{\\dagger}$，从而得到理想的白化平方范数 $x^{\\top} \\hat{\\Sigma}^{\\dagger} x$。\n\n仅从实对称矩阵的谱定理和上述定义出发，推导失真\n$$\nD(\\epsilon) \\equiv x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x \\;-\\; x^{\\top} \\hat{\\Sigma}^{\\dagger} x\n$$\n关于 $\\epsilon$ 的精确解析表达式。将你的最终答案表示为关于 $\\epsilon$ 的单个闭式解析表达式。不需要数值近似。", "solution": "该问题经核实具有科学依据，定义明确且客观。这是一个应用于统计学习概念的标准线性代数问题。所有必要信息都已提供，问题没有矛盾或含糊不清之处。我现在开始解答。\n\n解答基于实对称矩阵的谱定理。矩阵 $\\hat{\\Sigma} \\in \\mathbb{R}^{4 \\times 4}$ 是对称的，因此可以进行谱分解。我们已知其标准正交特征向量 $\\{u_{1}, u_{2}, u_{3}, u_{4}\\}$ 和对应的特征值 $\\lambda_{1} = 5$, $\\lambda_{2} = 2$, $\\lambda_{3} = 0$, $\\lambda_{4} = 0$。$\\hat{\\Sigma}$ 的谱分解可以写作：\n$$\n\\hat{\\Sigma} = \\sum_{i=1}^{4} \\lambda_i u_i u_i^{\\top}\n$$\n\n首先，我们分析正则化矩阵 $\\hat{\\Sigma}_{\\epsilon} = \\hat{\\Sigma} + \\epsilon I$，其中 $I$ 是 $4 \\times 4$ 的单位矩阵且 $\\epsilon  0$。$\\hat{\\Sigma}$ 的特征向量 $u_i$ 也是 $\\hat{\\Sigma}_{\\epsilon}$ 的特征向量：\n$$\n\\hat{\\Sigma}_{\\epsilon} u_i = (\\hat{\\Sigma} + \\epsilon I) u_i = \\hat{\\Sigma} u_i + \\epsilon I u_i = \\lambda_i u_i + \\epsilon u_i = (\\lambda_i + \\epsilon) u_i\n$$\n因此，$\\hat{\\Sigma}_{\\epsilon}$ 的特征值为 $\\lambda_i' = \\lambda_i + \\epsilon$。对于给定的 $\\hat{\\Sigma}$ 的特征值，$\\hat{\\Sigma}_{\\epsilon}$ 的特征值为 $5+\\epsilon$, $2+\\epsilon$, $\\epsilon$ 和 $\\epsilon$。由于 $\\epsilon  0$，$\\hat{\\Sigma}_{\\epsilon}$ 的所有特征值都严格为正，这意味着 $\\hat{\\Sigma}_{\\epsilon}$ 是可逆的。\n\n逆矩阵 $\\hat{\\Sigma}_{\\epsilon}^{-1}$ 具有相同的特征向量 $u_i$，其特征值是 $\\hat{\\Sigma}_{\\epsilon}$ 特征值的倒数。$\\hat{\\Sigma}_{\\epsilon}^{-1}$ 的谱分解为：\n$$\n\\hat{\\Sigma}_{\\epsilon}^{-1} = \\sum_{i=1}^{4} \\frac{1}{\\lambda_i + \\epsilon} u_i u_i^{\\top}\n$$\n\n接下来，我们计算二次型 $x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x$。向量 $x$ 以基 $\\{u_i\\}$ 的谱分解形式给出：\n$$\nx = 3 u_{1} + 2 u_{2} + 1 u_{3} + 2 u_{4} = \\sum_{i=1}^{4} c_i u_i\n$$\n其中系数为 $c_1=3$, $c_2=2$, $c_3=1$, $c_4=2$。将 $x$ 和 $\\hat{\\Sigma}_{\\epsilon}^{-1}$ 的展开式代入二次型中：\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\left( \\sum_{j=1}^{4} c_j u_j^{\\top} \\right) \\left( \\sum_{i=1}^{4} \\frac{1}{\\lambda_i + \\epsilon} u_i u_i^{\\top} \\right) \\left( \\sum_{k=1}^{4} c_k u_k \\right)\n$$\n利用特征向量的标准正交性 $u_j^{\\top} u_i = \\delta_{ji}$ (克罗内克 delta)，表达式可以显著简化：\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\sum_{i=1}^{4} \\frac{c_i^2}{\\lambda_i + \\epsilon}\n$$\n代入给定的 $\\lambda_i$ 和 $c_i$ 值：\n$$\nx^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x = \\frac{3^2}{5 + \\epsilon} + \\frac{2^2}{2 + \\epsilon} + \\frac{1^2}{0 + \\epsilon} + \\frac{2^2}{0 + \\epsilon} = \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{1}{\\epsilon} + \\frac{4}{\\epsilon} = \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n\n现在，我们考虑 Moore-Penrose 伪逆 (MPP)，即 $\\hat{\\Sigma}^{\\dagger}$。对于对称矩阵，其 MPP 是通过取非零特征值的倒数并保持零特征值为零来得到的。所以，$\\hat{\\Sigma}^{\\dagger}$ 具有相同的特征向量 $u_i$，其特征值 $\\lambda_i^{\\dagger}$ 定义为 $\\lambda_i^{\\dagger} = 1/\\lambda_i$（如果 $\\lambda_i \\neq 0$）和 $\\lambda_i^{\\dagger}=0$（如果 $\\lambda_i=0$）。\n$\\hat{\\Sigma}^{\\dagger}$ 的特征值为：\n$\\lambda_1^{\\dagger} = 1/5$, $\\lambda_2^{\\dagger} = 1/2$, $\\lambda_3^{\\dagger} = 0$, $\\lambda_4^{\\dagger} = 0$。\n二次型 $x^{\\top} \\hat{\\Sigma}^{\\dagger} x$ 的计算方法类似：\n$$\nx^{\\top} \\hat{\\Sigma}^{\\dagger} x = \\sum_{i=1}^{4} c_i^2 \\lambda_i^{\\dagger} = c_1^2 \\lambda_1^{\\dagger} + c_2^2 \\lambda_2^{\\dagger} + c_3^2 \\lambda_3^{\\dagger} + c_4^2 \\lambda_4^{\\dagger}\n$$\n代入数值：\n$$\nx^{\\top} \\hat{\\Sigma}^{\\dagger} x = (3^2) \\left(\\frac{1}{5}\\right) + (2^2) \\left(\\frac{1}{2}\\right) + (1^2)(0) + (2^2)(0) = \\frac{9}{5} + \\frac{4}{2} = \\frac{9}{5} + 2\n$$\n\n最后，我们计算失真 $D(\\epsilon) = x^{\\top} \\hat{\\Sigma}_{\\epsilon}^{-1} x - x^{\\top} \\hat{\\Sigma}^{\\dagger} x$：\n$$\nD(\\epsilon) = \\left( \\frac{9}{5 + \\epsilon} + \\frac{4}{2 + \\epsilon} + \\frac{5}{\\epsilon} \\right) - \\left( \\frac{9}{5} + 2 \\right)\n$$\n为了得到一个单一的闭式表达式，我们将这些项合并成一个有理函数。我们可以按如下方式对各项进行分组：\n$$\nD(\\epsilon) = \\left( \\frac{9}{5 + \\epsilon} - \\frac{9}{5} \\right) + \\left( \\frac{4}{2 + \\epsilon} - 2 \\right) + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{9 \\cdot 5 - 9(5 + \\epsilon)}{5(5 + \\epsilon)} + \\frac{4 - 2(2 + \\epsilon)}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{45 - 45 - 9\\epsilon}{5(5 + \\epsilon)} + \\frac{4 - 4 - 2\\epsilon}{2 + \\epsilon} + \\frac{5}{\\epsilon}\n$$\n$$\nD(\\epsilon) = \\frac{-9\\epsilon}{5(\\epsilon + 5)} - \\frac{2\\epsilon}{\\epsilon + 2} + \\frac{5}{\\epsilon}\n$$\n现在，我们找到一个公分母，即 $5\\epsilon(\\epsilon+2)(\\epsilon+5)$:\n$$\nD(\\epsilon) = \\frac{-9\\epsilon \\cdot \\epsilon(\\epsilon+2) - 2\\epsilon \\cdot 5\\epsilon(\\epsilon+5) + 5 \\cdot 5(\\epsilon+2)(\\epsilon+5)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{-9\\epsilon^2(\\epsilon+2) - 10\\epsilon^2(\\epsilon+5) + 25(\\epsilon^2+7\\epsilon+10)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{(-9\\epsilon^3 - 18\\epsilon^2) - (10\\epsilon^3 + 50\\epsilon^2) + (25\\epsilon^2 + 175\\epsilon + 250)}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n将分子中的项按 $\\epsilon$ 的幂次合并：\n$$\nD(\\epsilon) = \\frac{\\epsilon^3(-9-10) + \\epsilon^2(-18-50+25) + \\epsilon(175) + 250}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n$$\nD(\\epsilon) = \\frac{-19\\epsilon^3 - 43\\epsilon^2 + 175\\epsilon + 250}{5\\epsilon(\\epsilon+2)(\\epsilon+5)}\n$$\n展开分母得到最终表达式：\n$$\nD(\\epsilon) = \\frac{-19\\epsilon^3 - 43\\epsilon^2 + 175\\epsilon + 250}{5\\epsilon^3 + 35\\epsilon^2 + 50\\epsilon}\n$$\n这就是所要求的失真 $D(\\epsilon)$ 的单一闭式解析表达式。", "answer": "$$\n\\boxed{\\frac{-19\\epsilon^{3} - 43\\epsilon^{2} + 175\\epsilon + 250}{5\\epsilon^{3} + 35\\epsilon^{2} + 50\\epsilon}}\n$$", "id": "3146475"}, {"introduction": "许多构成现代统计学习核心的先进优化算法，都依赖于一个称为“邻近算子”的基本构建块。本练习通过要求你推导标准岭回归和更一般的各向异性二次惩罚的邻近算子，来更深入地探究正则化的机理。然后，你将分析这些算子的“收缩”特性，并将其与惩罚矩阵的特征值直接联系起来，从而揭示二次型的几何形状如何决定学习算法的收敛速度。[@problem_id:3146432]", "problem": "考虑一个参数向量 $w \\in \\mathbb{R}^{d}$ 以及在统计学习中常用的两个正则化函数：各向同性岭罚项 $\\phi(w) = \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$ 和各向异性二次罚项 $\\psi(w) = \\frac{1}{2} w^{\\top} Q w$，其中 $Q \\in \\mathbb{R}^{d \\times d}$ 是对称半正定矩阵且 $\\lambda  0$。从正常闭凸函数 $f$ 的近端算子定义出发，\n$$\n\\operatorname{prox}_{\\tau f}(v) = \\arg\\min_{w \\in \\mathbb{R}^{d}} \\left\\{ \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau f(w) \\right\\},\n$$\n推导 $\\operatorname{prox}_{\\tau \\phi}(v)$ 和 $\\operatorname{prox}_{\\tau \\psi}(v)$ 对于任意 $\\tau  0$ 和 $v \\in \\mathbb{R}^{d}$ 的闭式表达式。然后，使用欧几里得范数 $\\|\\cdot\\|_{2}$ 和矩阵上的诱导算子范数，通过用 $\\tau$、$\\lambda$ 以及 $Q$ 的谱性质来表示这些近端映射的收缩因子（利普希茨常数），从而分析它们。特别地，使用 $Q$ 的特征值表示 $\\operatorname{prox}_{\\tau \\psi}$ 的利普希茨常数，并讨论如何使用谱范数 $\\|Q\\|_{2}$ 来界定该常数。\n\n最后，对于以下特定选择\n$$\nQ = \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix}, \\quad \\lambda = 3, \\quad \\tau = \\frac{1}{2},\n$$\n计算 $\\operatorname{prox}_{\\tau \\psi}$ 的精确收缩因子与 $\\operatorname{prox}_{\\tau \\phi}$ 的精确收缩因子的比率。请将最终答案表示为一个精确的数值，无需四舍五入。", "solution": "该问题是有效的，因为它在科学上基于凸优化的原理，信息完备且表述客观。我们开始求解。\n\n函数 $\\tau f$ 的近端算子定义为：\n$$\n\\operatorname{prox}_{\\tau f}(v) = \\arg\\min_{w \\in \\mathbb{R}^{d}} \\left\\{ \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau f(w) \\right\\}\n$$\n\n首先，我们推导岭罚项 $\\phi(w) = \\frac{\\lambda}{2}\\|w\\|_{2}^{2}$ 的近端算子的闭式表达式。需要最小化的目标函数是：\n$$\ng(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau \\phi(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\frac{\\tau \\lambda}{2}\\|w\\|_{2}^{2}\n$$\n对于 $\\lambda  0$ 和 $\\tau  0$，此函数是严格凸且可微的。最小值可以通过将关于 $w$ 的梯度设为零来找到。\n$$\n\\nabla_w g(w) = \\nabla_w \\left( \\frac{1}{2}(w - v)^{\\top}(w - v) + \\frac{\\tau \\lambda}{2} w^{\\top}w \\right) = (w - v) + \\tau \\lambda w\n$$\n将梯度设为零向量：\n$$\n(w - v) + \\tau \\lambda w = 0\n$$\n$$\n(1 + \\tau \\lambda)w = v\n$$\n由于 $\\tau  0$ 且 $\\lambda  0$，标量 $(1 + \\tau \\lambda)$ 大于 $1$，所以我们可以解出 $w$：\n$$\nw = \\frac{1}{1 + \\tau \\lambda} v\n$$\n因此，岭罚项的近端算子为：\n$$\n\\operatorname{prox}_{\\tau \\phi}(v) = \\frac{1}{1 + \\tau \\lambda} v\n$$\n\n接下来，我们推导各向异性二次罚项 $\\psi(w) = \\frac{1}{2} w^{\\top} Q w$ 的近端算子的闭式表达式。需要最小化的目标函数是：\n$$\nh(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\tau \\psi(w) = \\frac{1}{2}\\|w - v\\|_{2}^{2} + \\frac{\\tau}{2} w^{\\top} Q w\n$$\n由于 $Q$ 是对称半正定矩阵，$h(w)$ 是一个凸函数。使用欧几里得范数使其成为严格凸函数。该函数也是可微的。我们通过将梯度设为零来找到最小值。\n$$\n\\nabla_w h(w) = \\nabla_w \\left( \\frac{1}{2}(w - v)^{\\top}(w - v) + \\frac{\\tau}{2} w^{\\top} Q w \\right) = (w - v) + \\tau Q w\n$$\n将梯度设为零向量：\n$$\n(w - v) + \\tau Q w = 0\n$$\n$$\n(I + \\tau Q)w = v\n$$\n矩阵 $I + \\tau Q$ 是可逆的。要证明这一点，令 $\\mu_i$ 为 $Q$ 的一个特征值。由于 $Q$ 是半正定的，$\\mu_i \\ge 0$。$I + \\tau Q$ 的特征值为 $1 + \\tau \\mu_i$。因为 $\\tau  0$，我们有 $1 + \\tau \\mu_i \\ge 1$，所以所有特征值都是正的。所有特征值均为正的对称矩阵是正定矩阵，因此是可逆的。解出 $w$：\n$$\nw = (I + \\tau Q)^{-1} v\n$$\n因此，各向异性二次罚项的近端算子为：\n$$\n\\operatorname{prox}_{\\tau \\psi}(v) = (I + \\tau Q)^{-1} v\n$$\n\n现在，我们分析这些近端映射的收缩因子（利普希茨常数）。对于一个线性映射 $P(v) = Av$，其关于欧几里得范数 $\\|\\cdot\\|_{2}$ 的利普希茨常数是诱导算子范数 $\\|A\\|_{2}$。对于一个对称矩阵 $A$，$\\|A\\|_{2}$ 是其谱半径，即其特征值的最大绝对值。\n\n对于 $\\operatorname{prox}_{\\tau \\phi}(v) = \\frac{1}{1 + \\tau \\lambda} v$，该映射是线性的，其矩阵为 $A_{\\phi} = \\frac{1}{1 + \\tau \\lambda} I$。利普希茨常数 $L_{\\phi}$ 是：\n$$\nL_{\\phi} = \\left\\| \\frac{1}{1 + \\tau \\lambda} I \\right\\|_{2} = \\left| \\frac{1}{1 + \\tau \\lambda} \\right| = \\frac{1}{1 + \\tau \\lambda}\n$$\n最后一个等式成立是因为 $\\tau  0$ 且 $\\lambda  0$。\n\n对于 $\\operatorname{prox}_{\\tau \\psi}(v) = (I + \\tau Q)^{-1} v$，该映射是线性的，其矩阵为 $A_{\\psi} = (I + \\tau Q)^{-1}$。由于 $Q$ 是对称的，$A_{\\psi}$ 也是对称的。其利普希茨常数 $L_{\\psi}$ 是其谱半径。设 $Q$ 的特征值为 $\\mu_1, \\mu_2, \\ldots, \\mu_d$，其中 $\\mu_i \\ge 0$。$A_{\\psi} = (I + \\tau Q)^{-1}$ 的特征值为 $\\frac{1}{1 + \\tau \\mu_i}$。利普希茨常数是这些特征值绝对值的最大值：\n$$\nL_{\\psi} = \\max_{i} \\left| \\frac{1}{1 + \\tau \\mu_i} \\right| = \\max_{i} \\frac{1}{1 + \\tau \\mu_i}\n$$\n对于 $a  0$，函数 $f(x) = \\frac{1}{1+ax}$ 在 $x \\ge 0$ 上是递减函数。因此，当 $\\mu_i$ 取最小值时，函数值达到最大。令 $\\mu_{\\min}(Q)$ 表示 $Q$ 的最小特征值。\n$$\nL_{\\psi} = \\frac{1}{1 + \\tau \\mu_{\\min}(Q)}\n$$\n$Q$ 的谱范数是 $\\|Q\\|_{2} = \\mu_{\\max}(Q)$，即 $Q$ 的最大特征值。由于 $0 \\le \\mu_{\\min}(Q) \\le \\mu_{\\max}(Q) = \\|Q\\|_{2}$，我们可以界定 $L_{\\psi}$。该常数的下界为 $\\frac{1}{1 + \\tau \\|Q\\|_{2}}$，上界为 $1$。具体来说，$\\frac{1}{1 + \\tau \\|Q\\|_{2}} \\le L_{\\psi} \\le 1$。如果 $\\mu_{\\min}(Q) = 0$，则 $1$ 这个上界是紧的。因此，仅知道 $\\|Q\\|_{2}$ 就能提供收缩因子的一个下界。\n\n最后，我们计算在给定具体值下的比率 $\\frac{L_{\\psi}}{L_{\\phi}}$：\n$$\nQ = \\begin{pmatrix} 3  1 \\\\ 1  3 \\end{pmatrix}, \\quad \\lambda = 3, \\quad \\tau = \\frac{1}{2}\n$$\n首先，我们求出岭罚项的收缩因子：\n$$\nL_{\\phi} = \\frac{1}{1 + \\tau \\lambda} = \\frac{1}{1 + (\\frac{1}{2})(3)} = \\frac{1}{1 + \\frac{3}{2}} = \\frac{1}{\\frac{5}{2}} = \\frac{2}{5}\n$$\n接下来，我们求出各向异性罚项的收缩因子。我们需要 $Q$ 的最小特征值。特征方程为 $\\det(Q - \\mu I) = 0$：\n$$\n\\det \\begin{pmatrix} 3 - \\mu  1 \\\\ 1  3 - \\mu \\end{pmatrix} = (3 - \\mu)^{2} - 1 = 0\n$$\n$$\n(3 - \\mu)^{2} = 1 \\implies 3 - \\mu = \\pm 1\n$$\n特征值为 $\\mu = 3 - 1 = 2$ 和 $\\mu = 3 + 1 = 4$。最小特征值为 $\\mu_{\\min}(Q) = 2$。\n收缩因子 $L_{\\psi}$ 为：\n$$\nL_{\\psi} = \\frac{1}{1 + \\tau \\mu_{\\min}(Q)} = \\frac{1}{1 + (\\frac{1}{2})(2)} = \\frac{1}{1 + 1} = \\frac{1}{2}\n$$\n收缩因子的比率为：\n$$\n\\frac{L_{\\psi}}{L_{\\phi}} = \\frac{\\frac{1}{2}}{\\frac{2}{5}} = \\frac{1}{2} \\cdot \\frac{5}{2} = \\frac{5}{4}\n$$", "answer": "$$\\boxed{\\frac{5}{4}}$$", "id": "3146432"}]}