{"hands_on_practices": [{"introduction": "对于同一个总体参数，我们常常有多种有效的估计方法。在本次练习中 [@problem_id:3155663]，我们将通过比较两种不同的总体方差估计量——最大似然估计量 (MLE) 和无偏样本方差，来探讨统计学中的一个基本问题。通过计算和分析，你将发现为什么“无偏”估计量并非总是最优选择，并具体地理解估计量的偏差与其总均方误差 ($MSE$) 之间的关键权衡。", "problem": "一个实验室正在校准一个传感器，其读数被建模为来自均值$\\mu$和方差参数$\\sigma^2$未知的正态分布的独立同分布样本。该实验室收集了一个大小为$n=3$的小样本，得到的读数为$x_1=1$，$x_2=4$，$x_3=7$。实验室希望使用两种不同的估计量来估计总体参数$\\sigma^2$：最大似然估计量（MLE）和无偏样本方差。仅使用核心定义（参数、统计量、估计量）、最大似然估计量（MLE）作为使似然函数最大化的值的定义，以及关于正态模型的经过充分检验的事实，选择所有关于这两种估计量在小样本情况下有何不同以及这种差异如何在这个具体样本中表现出来的正确陈述。\n \n选择所有适用项：\n\nA. 对于这个$n=3$的样本，通过最大化似然函数得到的估计量等于$6$，而无偏样本方差等于$9$；此外，对于$n=3$，基于似然的方差估计量在期望上低估了$\\sigma^2$。\n\nB. 对于这个样本，两个估计量都等于$9$；在均值未知的情况下，对于所有的$n$，两者都是无偏的。\n\nC. 如果总体均值$\\mu$已知，那么无论$n$为何值，$\\sigma^2$的无偏估计量都使用$n-1$作为分母，因此其平均值系统性地大于通过最大化似然函数得到的估计量。\n\nD. 对于均值未知的正态数据，对于所有整数$n \\ge 2$，基于似然的方差估计量的均方误差（MSE）严格小于无偏样本方差。\n\nE. 在像$n=3$这样的小样本情况下，最大似然估计量（MLE）在平均意义上高估了$\\sigma^2$，而无偏样本方差在平均意义上低估了$\\sigma^2$。", "solution": "问题陈述是统计估计领域中一个定义明确的问题。模型（$N(\\mu, \\sigma^2)$）、数据（$x_1=1, x_2=4, x_3=7$）、样本大小（$n=3$）以及估计量（MLE和无偏样本方差）都已明确指定，并且与标准统计理论一致。该问题具有科学依据、客观且内部一致。因此，该问题是有效的，我们可以继续进行求解。\n\n设随机变量$X_1, \\dots, X_n$是从均值$\\mu$未知、方差$\\sigma^2$未知的正态分布中抽取的独立同分布（i.i.d.）样本。待估计的总体参数是$\\sigma^2$。我们给定一个大小为$n=3$的样本，观测值为$x_1=1$，$x_2=4$和$x_3=7$。\n\n首先，我们定义当均值$\\mu$未知时$\\sigma^2$的两个估计量。\n\n方差的**最大似然估计量（MLE）**，记作$\\hat{\\sigma}^2_{\\text{MLE}}$，是使观测数据的联合概率密度函数（即似然函数）最大化的方差参数值。对于均值和方差均未知的正态分布，$\\sigma^2$的MLE由以下公式给出：\n$$ \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\n其中$\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$是样本均值。这个估计量是一个统计量，因为它是样本数据的函数。\n\n**无偏样本方差**，记作$S^2$，是为$\\sigma^2$设计的一个估计量，其期望值等于真实参数$\\sigma^2$。其公式为：\n$$ S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2 $$\n\n现在，我们为给定样本计算这些估计量的具体值（估计值）。\n样本大小为$n=3$。\n数据为$x_1=1$，$x_2=4$，$x_3=7$。\n样本均值为：\n$$ \\bar{x} = \\frac{1+4+7}{3} = \\frac{12}{3} = 4 $$\n离均差平方和为：\n$$ \\sum_{i=1}^3 (x_i - \\bar{x})^2 = (1-4)^2 + (4-4)^2 + (7-4)^2 = (-3)^2 + 0^2 + 3^2 = 9 + 0 + 9 = 18 $$\n使用这个和，我们可以计算出两个估计值：\nMLE估计值为：\n$$ \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{3} \\sum_{i=1}^3 (x_i - \\bar{x})^2 = \\frac{1}{3}(18) = 6 $$\n无偏样本方差估计值为：\n$$ S^2 = \\frac{1}{3-1} \\sum_{i=1}^3 (x_i - \\bar{x})^2 = \\frac{1}{2}(18) = 9 $$\n\n接下来，我们分析这些估计量的一般性质。\n根据定义，无偏样本方差的期望为$E[S^2] = \\sigma^2$。\nMLE的期望可以与$S^2$关联起来：\n$$ E[\\hat{\\sigma}^2_{\\text{MLE}}] = E\\left[\\frac{1}{n} \\sum (x_i - \\bar{x})^2\\right] = E\\left[\\frac{n-1}{n} \\cdot \\frac{1}{n-1} \\sum (x_i - \\bar{x})^2\\right] = \\frac{n-1}{n} E[S^2] = \\frac{n-1}{n}\\sigma^2 $$\n因为对于任何有限的$n > 1$，因子$\\frac{n-1}{n}$严格小于1，所以方差的MLE是一个有偏估计量。具体来说，它在先验期望上低估了真实方差$\\sigma^2$。对于$n=3$，$E[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{2}{3}\\sigma^2$。\n\n现在我们评估每个选项：\n\n**A. 对于这个$n=3$的样本，通过最大化似然函数得到的估计量等于$6$，而无偏样本方差等于$9$；此外，对于$n=3$，基于似然的方差估计量在期望上低估了$\\sigma^2$。**\n- 我们对这个具体样本的计算得出的MLE估计值为$6$。这是正确的。\n- 我们的计算得出的无偏样本方差估计值为$9$。这也是正确的。\n- 我们的理论分析表明$E[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{n-1}{n}\\sigma^2$。对于任何$n>1$，这个值都小于$\\sigma^2$，意味着该估计量在期望上低估了$\\sigma^2$。这对$n=3$成立。\n该陈述的三个部分都符合事实。\n**结论：正确。**\n\n**B. 对于这个样本，两个估计量都等于$9$；在均值未知的情况下，对于所有的$n$，两者都是无偏的。**\n- 第一个分句“两个估计量都等于$9$”是错误的。我们计算出MLE为$6$，无偏估计量为$9$。\n- 第二个分句“对于所有的$n$，两者都是无偏的”也是错误的。MLE是有偏的，因为其期望是$\\frac{n-1}{n}\\sigma^2$，而不是$\\sigma^2$。\n**结论：错误。**\n\n**C. 如果总体均值$\\mu$已知，那么无论$n$为何值，$\\sigma^2$的无偏估计量都使用$n-1$作为分母，因此其平均值系统性地大于通过最大化似然函数得到的估计量。**\n- 如果$\\mu$已知，$\\sigma^2$的MLE是$\\hat{\\sigma}^2_{\\text{MLE, known }\\mu} = \\frac{1}{n}\\sum_{i=1}^n(x_i - \\mu)^2$。\n- 让我们求这个估计量的期望。由于$x_i \\sim N(\\mu, \\sigma^2)$，我们有$E[(x_i - \\mu)^2] = \\text{Var}(x_i) = \\sigma^2$。\n- $E[\\hat{\\sigma}^2_{\\text{MLE, known }\\mu}] = E[\\frac{1}{n}\\sum(x_i - \\mu)^2] = \\frac{1}{n}\\sum E[(x_i-\\mu)^2] = \\frac{1}{n}\\sum\\sigma^2 = \\frac{n\\sigma^2}{n} = \\sigma^2$。\n- 因此，当$\\mu$已知时，MLE是无偏的。在这种情况下，$\\sigma^2$的无偏估计量使用分母$n$，而不是$n-1$。该陈述的前提是错误的。MLE和无偏估计量是同一个函数。\n**结论：错误。**\n\n**D. 对于均值未知的正态数据，对于所有整数$n \\ge 2$，基于似然的方差估计量的均方误差（MSE）严格小于无偏样本方差。**\n- 估计量$\\hat{\\theta}$的均方误差（MSE）为$\\text{MSE}(\\hat{\\theta}) = \\text{Var}(\\hat{\\theta}) + (\\text{Bias}(\\hat{\\theta}))^2$。\n- 对于无偏样本方差$S^2$，偏差为$0$。一个已知的结果是，对于来自正态分布的样本，$\\text{Var}(S^2) = \\frac{2\\sigma^4}{n-1}$。所以，$\\text{MSE}(S^2) = \\frac{2\\sigma^4}{n-1}$。\n- 对于MLE $\\hat{\\sigma}^2_{\\text{MLE}}$，偏差为$E[\\hat{\\sigma}^2_{\\text{MLE}}] - \\sigma^2 = \\frac{n-1}{n}\\sigma^2 - \\sigma^2 = -\\frac{\\sigma^2}{n}$。\n- MLE的方差为$\\text{Var}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\text{Var}(\\frac{n-1}{n}S^2) = (\\frac{n-1}{n})^2 \\text{Var}(S^2) = \\frac{(n-1)^2}{n^2}\\frac{2\\sigma^4}{n-1} = \\frac{2(n-1)\\sigma^4}{n^2}$。\n- MLE的MSE为$\\text{MSE}(\\hat{\\sigma}^2_{\\text{MLE}}) = \\frac{2(n-1)\\sigma^4}{n^2} + (-\\frac{\\sigma^2}{n})^2 = \\frac{2(n-1)\\sigma^4}{n^2} + \\frac{\\sigma^4}{n^2} = \\frac{(2n-2+1)\\sigma^4}{n^2} = \\frac{(2n-1)\\sigma^4}{n^2}$。\n- 我们必须比较$\\text{MSE}(\\hat{\\sigma}^2_{\\text{MLE}})$和$\\text{MSE}(S^2)$。我们比较因子$\\frac{2n-1}{n^2}$和$\\frac{2}{n-1}$。\n- $\\frac{2n-1}{n^2}  \\frac{2}{n-1} \\iff (2n-1)(n-1)  2n^2 \\iff 2n^2 - 3n + 1  2n^2 \\iff -3n + 1  0 \\iff 1  3n \\iff n > 1/3$。\n- 这个不等式对所有整数$n \\ge 1$都成立。由于估计量要求$n \\ge 2$，该不等式严格成立。因此，MLE确实比无偏样本方差具有更小的MSE。这是估计理论中的一个标准结果。\n**结论：正确。**\n\n**E. 在像$n=3$这样的小样本情况下，最大似然估计量（MLE）在平均意义上高估了$\\sigma^2$，而无偏样本方差在平均意义上低估了$\\sigma^2$。**\n- “MLE在平均意义上高估了$\\sigma^2$”：这是错误的。如前所示，$E[\\hat{\\sigma}^2_{\\text{MLE}}] = \\frac{n-1}{n}\\sigma^2  \\sigma^2$。MLE在平均意义上*低估*了$\\sigma^2$。\n- “无偏样本方差在平均意义上低估了$\\sigma^2$”：根据定义，这是错误的。$E[S^2] = \\sigma^2$。它是无偏的，意味着它在平均意义上既不高估也不低估。\n**结论：错误。**\n\n正确的陈述是A和D。", "answer": "$$\\boxed{AD}$$", "id": "3155663"}, {"introduction": "偏差-方差权衡不仅是一个理论概念，更是建模者在实践中需要积极应对的挑战。本次练习 [@problem_id:3155654] 将在岭回归 (ridge regression) 的背景下，生动地展示这一权衡，而岭回归正是现代机器学习的基石之一。你将看到如何通过正则化参数 $\\lambda$ 引入可控的偏差，从而显著降低方差以获得更好的模型，并亲手推导出最小化总预测误差的最优 $\\lambda$ 值。", "problem": "考虑一个监督学习线性模型，其固定设计矩阵为 $X \\in \\mathbb{R}^{3 \\times 2}$，未知参数向量为 $\\beta \\in \\mathbb{R}^{2}$，响应向量为 $Y \\in \\mathbb{R}^{3}$，满足\n$$\nY = X \\beta + \\varepsilon,\n$$\n其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$ 且 $I_{3}$ 表示 $3 \\times 3$ 单位矩阵。我们感兴趣的参数是由 $\\mu = X \\beta$ 定义的均值响应向量 $\\mu \\in \\mathbb{R}^{3}$。考虑 $\\beta$ 的岭收缩估计量，\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y,\n$$\n其收缩参数为 $\\lambda \\geq 0$，以及由此导出的均值响应估计量\n$$\n\\hat{\\mu}_{\\lambda} = X \\hat{\\beta}_{\\lambda}.\n$$\n\n从基本原理出发，使用上述期望、方差和线性模型的定义。不要假定超出这些定义之外的任何结果。您的任务是：\n1. 推导 $\\hat{\\mu}_{\\lambda}$ 相对于 $\\mu$ 的偏差向量，使其成为 $\\lambda$ 和 $X$ 的函数，并推导 $\\hat{\\mu}_{\\lambda}$ 的协方差矩阵，使其成为 $\\lambda$、$X$ 和 $\\sigma^{2}$ 的函数。\n2. 使用您的推导，对于固定的设计矩阵\n$$\nX = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0\n\\end{pmatrix}.\n$$\n将期望平方误差 $E\\!\\left[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}\\right]$ 表示为 $\\lambda$ 的函数。\n3. 对于 $\\beta = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $\\sigma^{2} = 3$，确定使 $E\\!\\left[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}\\right]$ 最小化的 $\\lambda \\geq 0$ 的值。请提供精确值作为最终答案，无需四舍五入。", "solution": "该问题是有效的，因为它具有科学依据、是适定的且客观的。它包含了正则化线性模型理论中的标准推导。我们将按顺序完成这三项任务。\n\n线性模型由 $Y = X \\beta + \\varepsilon$ 给出，其中 $Y \\in \\mathbb{R}^{3}$，$X \\in \\mathbb{R}^{3 \\times 2}$，$\\beta \\in \\mathbb{R}^{2}$，且误差项 $\\varepsilon \\in \\mathbb{R}^{3}$ 服从多元正态分布 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$。这意味着 $E[\\varepsilon] = 0$ 且 $\\text{Cov}(\\varepsilon) = E[\\varepsilon \\varepsilon^{\\top}] = \\sigma^{2} I_{3}$。均值响应向量为 $\\mu = X \\beta$。$\\beta$ 的岭估计量和由此导出的 $\\mu$ 的估计量由下式给出：\n$$\n\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y\n$$\n$$\n\\hat{\\mu}_{\\lambda} = X \\hat{\\beta}_{\\lambda} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y\n$$\n其中 $\\lambda \\geq 0$。\n\n**1. $\\hat{\\mu}_{\\lambda}$ 的偏差向量和协方差矩阵的推导**\n\n首先，我们推导估计量 $\\hat{\\mu}_{\\lambda}$ 的偏差。偏差定义为 $\\text{Bias}(\\hat{\\mu}_{\\lambda}) = E[\\hat{\\mu}_{\\lambda}] - \\mu$。\n为了求 $\\hat{\\mu}_{\\lambda}$ 的期望，我们使用期望算子的线性性。响应向量 $Y$ 的期望是：\n$$\nE[Y] = E[X \\beta + \\varepsilon] = X \\beta + E[\\varepsilon] = X \\beta + 0 = X \\beta\n$$\n因为 $X$ 和 $\\beta$ 被认为是固定的（非随机的）。\n现在我们计算 $\\hat{\\mu}_{\\lambda}$ 的期望：\n$$\nE[\\hat{\\mu}_{\\lambda}] = E[X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} Y]\n$$\n矩阵项相对于期望是常数，所以我们可以将其提出：\n$$\nE[\\hat{\\mu}_{\\lambda}] = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} E[Y] = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} (X \\beta)\n$$\n将此代入偏差的定义中：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta - \\mu = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X \\beta - X \\beta\n$$\n从左边提出因子 $X$，从右边提出因子 $\\beta$：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X \\left[ (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X - I_{2} \\right] \\beta\n$$\n为了简化方括号中的表达式，令 $A = X^{\\top} X$。该表达式变为 $(A + \\lambda I_{2})^{-1} A - I_{2}$。我们可以写成 $I_{2} = (A + \\lambda I_{2})^{-1} (A + \\lambda I_{2})$。\n$$\n(A + \\lambda I_{2})^{-1} A - (A + \\lambda I_{2})^{-1} (A + \\lambda I_{2}) = (A + \\lambda I_{2})^{-1} (A - (A + \\lambda I_{2})) = (A + \\lambda I_{2})^{-1} (-\\lambda I_{2}) = -\\lambda (A + \\lambda I_{2})^{-1}\n$$\n将 $A = X^{\\top} X$ 代回，偏差向量为：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = X \\left[ -\\lambda (X^{\\top} X + \\lambda I_{2})^{-1} \\right] \\beta = -\\lambda X (X^{\\top} X + \\lambda I_{2})^{-1} \\beta\n$$\n接下来，我们推导协方差矩阵 $\\text{Cov}(\\hat{\\mu}_{\\lambda})$。我们使用性质：对于一个随机向量 $Z$ 和一个常数矩阵 $M$，有 $\\text{Cov}(MZ) = M \\text{Cov}(Z) M^{\\top}$。\n令 $H_{\\lambda} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top}$。那么 $\\hat{\\mu}_{\\lambda} = H_{\\lambda} Y$。\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\text{Cov}(H_{\\lambda} Y) = H_{\\lambda} \\text{Cov}(Y) H_{\\lambda}^{\\top}\n$$\n$Y$ 的协方差是：\n$$\n\\text{Cov}(Y) = \\text{Cov}(X \\beta + \\varepsilon) = \\text{Cov}(\\varepsilon) = \\sigma^{2} I_{3}\n$$\n将此代入 $\\hat{\\mu}_{\\lambda}$ 的协方差表达式中：\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = H_{\\lambda} (\\sigma^{2} I_{3}) H_{\\lambda}^{\\top} = \\sigma^{2} H_{\\lambda} H_{\\lambda}^{\\top}\n$$\n矩阵 $H_{\\lambda}$ 是对称的，因为 $(X^{\\top}X + \\lambda I_2)$ 是对称的，因此其逆矩阵也是对称的。\n$H_{\\lambda}^{\\top} = (X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top})^{\\top} = (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{2})^{-1})^{\\top} X^{\\top} = X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} = H_{\\lambda}$。\n所以，$\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} H_{\\lambda} H_{\\lambda} = \\sigma^{2} H_{\\lambda}^{2}$。\n代入 $H_{\\lambda}$ 的定义：\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} \\left( X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} \\right)^{2} = \\sigma^{2} X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{2})^{-1} X^{\\top}\n$$\n\n**2. 针对特定 $X$ 的期望平方误差**\n\n$\\hat{\\mu}_{\\lambda}$ 的期望平方误差，或称均方误差 (MSE)，是 $E[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}]$。这可以分解为偏差的平方范数和协方差矩阵的迹（总方差）之和。\n$E[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = E[(\\hat{\\mu}_{\\lambda} - \\mu)^{\\top}(\\hat{\\mu}_{\\lambda} - \\mu)] = \\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} + \\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda}))$。\n\n我们给定了特定的设计矩阵：\n$$\nX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}\n$$\n首先，我们计算 $X^{\\top}X$：\n$$\nX^{\\top} X = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_{2}\n$$\n现在，我们使用 $X^{\\top}X = I_{2}$ 来具体化偏差和协方差的表达式。\n\n对于偏差项：\n$$\n\\text{Bias}(\\hat{\\mu}_{\\lambda}) = -\\lambda X (I_{2} + \\lambda I_{2})^{-1} \\beta = -\\lambda X ((1+\\lambda)I_{2})^{-1} \\beta = -\\lambda X \\frac{1}{1+\\lambda} I_{2} \\beta = -\\frac{\\lambda}{1+\\lambda} X \\beta = -\\frac{\\lambda}{1+\\lambda} \\mu\n$$\n偏差的平方范数是：\n$$\n\\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} = \\left\\| -\\frac{\\lambda}{1+\\lambda} \\mu \\right\\|^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\mu\\|^{2}\n$$\n我们注意到，对于这个特定的 $X$，$\\mu = X\\beta = \\begin{pmatrix} \\beta_{1} \\\\ \\beta_{2} \\\\ 0 \\end{pmatrix}$。因此，$\\|\\mu\\|^{2} = \\beta_{1}^{2} + \\beta_{2}^{2} = \\|\\beta\\|^{2}$。\n$$\n\\|\\text{Bias}(\\hat{\\mu}_{\\lambda})\\|^{2} = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|^{2}\n$$\n\n对于方差项，我们首先简化协方差矩阵的表达式：\n$$\n\\text{Cov}(\\hat{\\mu}_{\\lambda}) = \\sigma^{2} X(I_{2} + \\lambda I_{2})^{-1}I_{2}(I_{2} + \\lambda I_{2})^{-1}X^{\\top} = \\sigma^{2} X \\left(\\frac{1}{1+\\lambda}I_{2}\\right) I_{2} \\left(\\frac{1}{1+\\lambda}I_{2}\\right) X^{\\top} = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} X X^{\\top}\n$$\n我们来计算 $XX^{\\top}$：\n$$\nXX^{\\top} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  0 \\end{pmatrix}\n$$\n方差项是协方差矩阵的迹：\n$$\n\\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda})) = \\text{Tr}\\left( \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} X X^{\\top} \\right) = \\frac{\\sigma^{2}}{(1+\\lambda)^{2}} \\text{Tr}(XX^{\\top})\n$$\n$XX^{\\top}$ 的迹是 $1+1+0=2$。\n$$\n\\text{Tr}(\\text{Cov}(\\hat{\\mu}_{\\lambda})) = \\frac{2\\sigma^{2}}{(1+\\lambda)^{2}}\n$$\n结合偏差和方差项，期望平方误差是：\n$$\nE[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = \\frac{\\lambda^{2}}{(1+\\lambda)^{2}} \\|\\beta\\|^{2} + \\frac{2\\sigma^{2}}{(1+\\lambda)^{2}} = \\frac{\\lambda^{2}\\|\\beta\\|^{2} + 2\\sigma^{2}}{(1+\\lambda)^{2}}\n$$\n\n**3. 最小化期望平方误差**\n\n给定 $\\beta = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$ 和 $\\sigma^{2} = 3$。首先，我们计算 $\\|\\beta\\|^{2}$：\n$$\n\\|\\beta\\|^{2} = 2^{2} + (-1)^{2} = 4 + 1 = 5\n$$\n将 $\\|\\beta\\|^{2}=5$ 和 $\\sigma^{2}=3$ 代入期望平方误差表达式：\n$$\nE[\\|\\hat{\\mu}_{\\lambda} - \\mu\\|^{2}] = \\frac{5\\lambda^{2} + 2(3)}{(1+\\lambda)^{2}} = \\frac{5\\lambda^{2} + 6}{(1+\\lambda)^{2}}\n$$\n令 $f(\\lambda) = \\frac{5\\lambda^{2} + 6}{(1+\\lambda)^{2}}$。我们想找到最小化 $f(\\lambda)$ 的 $\\lambda \\geq 0$ 的值。为此，我们求 $f(\\lambda)$ 关于 $\\lambda$ 的导数并将其设为零。使用微分的商法则，$[u/v]' = (u'v - uv')/v^2$：\n$$\nf'(\\lambda) = \\frac{(10\\lambda)(1+\\lambda)^{2} - (5\\lambda^{2} + 6)(2(1+\\lambda))}{((1+\\lambda)^{2})^{2}}\n$$\n对于 $\\lambda \\geq 0$，$1+\\lambda \\neq 0$，所以我们可以通过将分子和分母同除以 $1+\\lambda$ 来简化：\n$$\nf'(\\lambda) = \\frac{10\\lambda(1+\\lambda) - 2(5\\lambda^{2} + 6)}{(1+\\lambda)^{3}} = \\frac{10\\lambda^{2} + 10\\lambda - 10\\lambda^{2} - 12}{(1+\\lambda)^{3}} = \\frac{10\\lambda - 12}{(1+\\lambda)^{3}}\n$$\n将导数设为零以找到临界点：\n$$\nf'(\\lambda) = 0 \\implies 10\\lambda - 12 = 0 \\implies 10\\lambda = 12 \\implies \\lambda = \\frac{12}{10} = \\frac{6}{5}\n$$\n临界点是 $\\lambda = \\frac{6}{5}$。该值满足约束 $\\lambda \\geq 0$。为了确认这是一个最小值，我们可以检查导数的符号。对于 $\\lambda \\geq 0$，分母 $(1+\\lambda)^{3}$ 是正的。$f'(\\lambda)$ 的符号由分子 $10\\lambda - 12$ 决定。\n- 对于 $0 \\leq \\lambda  \\frac{6}{5}$，$10\\lambda - 12  0$，所以 $f'(\\lambda)  0$。函数 $f(\\lambda)$ 是递减的。\n- 对于 $\\lambda > \\frac{6}{5}$，$10\\lambda - 12 > 0$，所以 $f'(\\lambda) > 0$。函数 $f(\\lambda)$ 是递增的。\n由于函数在 $\\lambda=\\frac{6}{5}$ 之前递减，之后递增，所以 $\\lambda = \\frac{6}{5}$ 是 $\\lambda \\geq 0$ 上的全局最小值。\n\n最小化期望平方误差的 $\\lambda$ 的值是 $\\frac{6}{5}$。", "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$", "id": "3155654"}, {"introduction": "在许多现实场景中，估计量可能是数据的复杂非线性函数，这使得其偏差等理论性质难以通过解析方式推导。此时，计算统计学便能发挥其强大作用。本次动手练习 [@problem_id:3155706] 将指导你实现两种强大的重采样技术——刀切法 (jackknife) 和自助法 (bootstrap)，来估计并校正这类估计量的偏差。你将通过编写代码，亲身体验这些方法的工作原理，并比较它们在不同情况下的效果。", "problem": "考虑一个由独立同分布 (i.i.d.) 的正观测值 $X_1, X_2, \\dots, X_n$ 组成的数据集。目标参数是总体均值的对数，记为 $\\theta = \\log\\left(E[X]\\right)$。统计量是数据的一个函数，对于数据集 $D$，在此记为 $T(D)$。而估计量是旨在估计某个参数的统计量；它在某个数据集上的实现值称为估计值。在本问题中，使用非线性估计量 $\\hat{\\theta} = T(D) = \\log\\left(\\bar{X}\\right)$，其中 $\\bar{X}$ 是样本均值。$\\hat{\\theta}$ 对 $\\theta$ 的偏差定义为 $B(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$。您的任务是使用刀切法 (jackknife method) 估计偏差，并计算基于自助法 (bootstrap) 的偏差校正，然后在不同的数据生成场景下比较这两种方法。\n\n从上述核心定义出发，实现以下操作，不依赖任何预先推导或简化的公式：\n- 使用基于留一法 (leave-one-out) 样本的刀切法来估计 $\\hat{\\theta}$ 的偏差。\n- 使用非参数自助法（从观测数据的经验分布中进行有放回重抽样）来估计 $\\hat{\\theta}$ 的偏差，并构建一个自助法偏差校正估计量。\n- 对每个数据集，使用已知的数据生成分布计算真实参数值 $\\theta$，并将刀切法偏差校正估计值和自助法偏差校正估计值与 $\\theta$ 进行比较。\n\n您必须完全按照下方的测试套件中的规定生成数据集，并使用给定的随机种子以保证可复现性。所有角度（如有）都必须视为无量纲实数；不涉及任何物理单位。所有输出必须是实数或整数，以普通十进制表示法表示。\n\n测试套件：\n- 情况 $1$ (理想情况，中等样本量，轻度偏斜):\n  - 分布：速率 $\\lambda = 1.0$ 的指数分布，因此 $E[X] = 1/\\lambda$ 且 $\\theta = \\log(1/\\lambda) = -\\log(\\lambda)$。\n  - 样本量：$n = 30$。\n  - 自助法重复次数：$B = 500$。\n  - 随机种子：$123$。\n- 情况 $2$ (边界条件，极小样本):\n  - 分布：速率 $\\lambda = 2.0$ 的指数分布。\n  - 样本量：$n = 3$。\n  - 自助法重复次数：$B = 1000$。\n  - 随机种子：$456$。\n- 情况 $3$ (理想情况，中等样本量，较重度偏斜):\n  - 分布：基础正态参数为 $\\mu = 0.0, \\sigma = 1.0$ 的对数正态分布，因此 $E[X] = \\exp(\\mu + \\sigma^2/2)$ 且 $\\theta = \\log(E[X]) = \\mu + \\sigma^2/2$。\n  - 样本量：$n = 50$。\n  - 自助法重复次数：$B = 1000$。\n  - 随机种子：$789$。\n- 情况 $4$ (边缘情况，重尾):\n  - 分布：基础正态参数为 $\\mu = -0.5, \\sigma = 2.0$ 的对数正态分布。\n  - 样本量：$n = 20$。\n  - 自助法重复次数：$B = 2000$。\n  - 随机种子：$321$。\n\n对于每种情况，执行以下操作：\n- 使用给定的种子从指定分布中生成数据 $X_1, \\dots, X_n$。\n- 计算全样本估计值 $\\hat{\\theta} = \\log(\\bar{X})$。\n- 根据留一法数据集计算刀切法偏差估计，以及相应的刀切法偏差校正估计量。\n- 使用 $B$ 次重抽样计算自助法偏差估计，以及相应的自助法偏差校正估计量。\n- 根据该情况已知的分布参数计算真实参数 $\\theta$。\n- 通过比较绝对误差，计算哪种校正更接近真实参数。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。每个元素对应一种情况，并且本身必须是遵循以下顺序的括号列表：\n$[\\hat{\\theta}, \\theta, b_{\\text{jack}}, b_{\\text{boot}}, \\hat{\\theta}_{\\text{jack,corr}}, \\hat{\\theta}_{\\text{boot,corr}}, I_{\\text{boot-better}}]$,\n其中 $b_{\\text{jack}}$ 是刀切法偏差估计，$b_{\\text{boot}}$ 是自助法偏差估计，$\\hat{\\theta}_{\\text{jack,corr}}$ 是刀切法偏差校正估计值，$\\hat{\\theta}_{\\text{boot,corr}}$ 是自助法偏差校正估计值，而 $I_{\\text{boot-better}}$ 是一个整数指示符，如果自助法偏差校正估计值相对于 $\\theta$ 的绝对误差小于刀切法偏差校正估计值，则其值为 $1$，否则为 $0$。例如，最终输出应类似于 $[[\\dots],[\\dots],[\\dots],[\\dots]]$，所有值均为十进制或整数形式，不含百分号，也无额外文本。", "solution": "用户提供了一个计算统计学领域的问题，要求实现并比较两种基于重抽样的偏差估计技术：刀切法 (jackknife) 和非参数自助法 (nonparametric bootstrap)。\n\n### 1. 问题验证\n\n问题陈述已经过严格验证，并被确定为一个有效的科学问题。\n\n*   **提取已知条件**：问题为四个不同的测试用例提供了所有必要参数，包括数据生成分布（指数分布、对数正态分布）、其参数（$\\lambda$、$\\mu$、$\\sigma$）、样本量（$n$）、自助法重复次数（$B$）以及用于可复现性的随机种子。它清晰地定义了目标参数 $\\theta = \\log(E[X])$、估计量 $\\hat{\\theta} = \\log(\\bar{X})$ 以及所需的输出。\n\n*   **验证结论**：\n    1.  **科学依据**：该问题基于统计推断的基本且公认的原理。刀切法和自助法是估计估计量性质（如偏差和方差）的经典方法。估计量 $\\hat{\\theta} = \\log(\\bar{X})$ 是一个展示样本矩非线性变换中偏差的经典例子。由于对数函数是严格凹函数，根据琴生不等式 (Jensen's inequality) 会产生偏差。具体来说，$E[\\log(\\bar{X})] \\le \\log(E[\\bar{X}]) = \\log(E[X]) = \\theta$。因此，预计会存在负偏差，而这些方法旨在估计此偏差。该问题在科学上是合理的。\n    2.  **适定性**：问题是完全指定的。对于每种情况，分布、参数、样本量和随机种子的组合唯一确定了要生成的数据集。计算刀切法和自助法估计值的过程是标准且明确的。每个测试用例都存在唯一、稳定且有意义的数值解。\n    3.  **客观性**：问题使用精确、形式化的数学和统计语言进行陈述。任务是客观的计算过程，最终的比较基于一个清晰的量化标准（绝对误差）。\n\n该问题通过了所有标准，是有效的。后续章节将详细介绍求解方法。\n\n### 2. 求解方法\n\n将通过系统地对每个测试用例执行所需的计算来实现解决方案。\n\n#### 2.1. 真实参数计算 ($\\theta$)\n真实参数 $\\theta = \\log(E[X])$ 是根据指定分布的已知性质计算的。\n*   对于速率参数为 $\\lambda$ 的指数分布，期望值为 $E[X] = 1/\\lambda$。因此，$\\theta = \\log(1/\\lambda) = -\\log(\\lambda)$。\n*   对于基础正态分布参数为 $\\mu$ 和 $\\sigma$ 的对数正态分布，期望值为 $E[X] = \\exp(\\mu + \\sigma^2/2)$。因此，$\\theta = \\log(E[X]) = \\mu + \\sigma^2/2$。\n\n#### 2.2. 数据生成与全样本估计 ($\\hat{\\theta}$)\n对于每种情况，使用给定的随机种子从指定分布中生成一个大小为 $n$ 的数据集 $D = \\{X_1, \\dots, X_n\\}$。然后计算全样本估计值 $\\hat{\\theta} = \\log(\\bar{X})$，其中 $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$。\n\n#### 2.3. 刀切法偏差估计\n刀切法通过在留一法子样本上系统地重新计算统计量来估计偏差。\n1.  对于每个观测值 $i \\in \\{1, \\dots, n\\}$，通过从原始数据集 $D$ 中移除 $X_i$ 来创建一个子样本 $D_{(-i)}$。\n2.  计算该子样本的样本均值 $\\bar{X}_{(-i)} = \\frac{1}{n-1}\\sum_{j \\ne i} X_j$。这可以高效地计算为 $\\bar{X}_{(-i)} = \\frac{n\\bar{X} - X_i}{n-1}$。\n3.  计算“留一法”估计值 $\\hat{\\theta}_{(-i)} = \\log(\\bar{X}_{(-i)})$。\n4.  刀切法偏差估计 $b_{\\text{jack}}$ 由以下公式给出：\n    $$b_{\\text{jack}} = (n-1) \\left( \\left( \\frac{1}{n} \\sum_{i=1}^n \\hat{\\theta}_{(-i)} \\right) - \\hat{\\theta} \\right)$$\n5.  刀切法偏差校正估计值则为 $\\hat{\\theta}_{\\text{jack,corr}} = \\hat{\\theta} - b_{\\text{jack}}$。\n\n#### 2.4. 自助法偏差估计\n非参数自助法通过从数据的经验分布中模拟估计量的抽样分布来估计偏差。\n1.  生成 $B$ 个自助样本 $D^{\\ast 1}, \\dots, D^{\\ast B}$。每个样本 $D^{\\ast b}$ 是通过从原始数据集 $D$ 中有放回地抽取 $n$ 个观测值而创建的。\n2.  对于每个自助样本 $D^{\\ast b}$，计算统计量的自助法重复值：$\\hat{\\theta}^{\\ast b} = \\log(\\bar{X}^{\\ast b})$，其中 $\\bar{X}^{\\ast b}$ 是样本 $D^{\\ast b}$ 的均值。\n3.  自助法偏差估计 $b_{\\text{boot}}$ 是自助法重复值的平均值与原始全样本估计值之差：\n    $$b_{\\text{boot}} = \\left( \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{\\ast b} \\right) - \\hat{\\theta}$$\n4.  自助法偏差校正估计值为 $\\hat{\\theta}_{\\text{boot,corr}} = \\hat{\\theta} - b_{\\text{boot}}$。这可以写成：\n    $$\\hat{\\theta}_{\\text{boot,corr}} = 2\\hat{\\theta} - \\frac{1}{B} \\sum_{b=1}^B \\hat{\\theta}^{\\ast b}$$\n\n#### 2.5. 比较与最终输出\n对于每个测试用例，计算每个校正后估计值相对于真实参数 $\\theta$ 的绝对误差：\n*   误差 (刀切法): $\\text{err}_{\\text{jack}} = |\\hat{\\theta}_{\\text{jack,corr}} - \\theta|$\n*   误差 (自助法): $\\text{err}_{\\text{boot}} = |\\hat{\\theta}_{\\text{boot,corr}} - \\theta|$\n\n如果 $\\text{err}_{\\text{boot}}  \\text{err}_{\\text{jack}}$，则指示变量 $I_{\\text{boot-better}}$ 设置为 $1$，否则设置为 $0$。每个情况的最终结果是一个包含七个值的有序列表：$[\\hat{\\theta}, \\theta, b_{\\text{jack}}, b_{\\text{boot}}, \\hat{\\theta}_{\\text{jack,corr}}, \\hat{\\theta}_{\\text{boot,corr}}, I_{\\text{boot-better}}]$。所有四种情况的结果被汇总到一个列表的列表中，作为最终输出。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing jackknife and bootstrap bias estimates\n    for the estimator log(sample mean) across four test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"dist\": \"exp\",\n            \"params\": {\"lambda\": 1.0},\n            \"n\": 30,\n            \"B\": 500,\n            \"seed\": 123\n        },\n        {\n            \"dist\": \"exp\",\n            \"params\": {\"lambda\": 2.0},\n            \"n\": 3,\n            \"B\": 1000,\n            \"seed\": 456\n        },\n        {\n            \"dist\": \"lognormal\",\n            \"params\": {\"mu\": 0.0, \"sigma\": 1.0},\n            \"n\": 50,\n            \"B\": 1000,\n            \"seed\": 789\n        },\n        {\n            \"dist\": \"lognormal\",\n            \"params\": {\"mu\": -0.5, \"sigma\": 2.0},\n            \"n\": 20,\n            \"B\": 2000,\n            \"seed\": 321\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        n = case[\"n\"]\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n        dist = case[\"dist\"]\n        params = case[\"params\"]\n\n        # Use a random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate data and compute true theta.\n        if dist == \"exp\":\n            rate = params[\"lambda\"]\n            X = rng.exponential(scale=1.0/rate, size=n)\n            theta_true = -np.log(rate)\n        elif dist == \"lognormal\":\n            mu, sigma = params[\"mu\"], params[\"sigma\"]\n            X = rng.lognormal(mean=mu, sigma=sigma, size=n)\n            theta_true = mu + (sigma**2) / 2.0\n            \n        # 2. Compute the full-sample estimate.\n        x_bar = np.mean(X)\n        theta_hat = np.log(x_bar)\n\n        # 3. Compute jackknife bias estimate and corrected estimator.\n        # Efficiently calculate leave-one-out means\n        x_bar_j = (n * x_bar - X) / (n - 1)\n        theta_hat_j = np.log(x_bar_j)\n        \n        # Jackknife bias\n        theta_hat_j_mean = np.mean(theta_hat_j)\n        b_jack = (n - 1) * (theta_hat_j_mean - theta_hat)\n        \n        # Jackknife bias-corrected estimate\n        theta_hat_jack_corr = theta_hat - b_jack\n\n        # 4. Compute bootstrap bias estimate and corrected estimator.\n        bootstrap_replicates = rng.choice(X, size=(B, n), replace=True)\n        x_bar_b = np.mean(bootstrap_replicates, axis=1)\n        theta_hat_b = np.log(x_bar_b)\n        \n        # Bootstrap bias\n        theta_hat_b_mean = np.mean(theta_hat_b)\n        b_boot = theta_hat_b_mean - theta_hat\n        \n        # Bootstrap bias-corrected estimate\n        theta_hat_boot_corr = theta_hat - b_boot\n        \n        # 5. Compare the corrected estimators.\n        err_jack = np.abs(theta_hat_jack_corr - theta_true)\n        err_boot = np.abs(theta_hat_boot_corr - theta_true)\n        I_boot_better = 1 if err_boot  err_jack else 0\n\n        # Assemble the results for the current case.\n        case_result = [\n            theta_hat, \n            theta_true, \n            b_jack, \n            b_boot, \n            theta_hat_jack_corr, \n            theta_hat_boot_corr, \n            I_boot_better\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string exactly as required.\n    result_strings = []\n    for res in all_results:\n        # Convert each number in the list to its string representation.\n        # The integer indicator will be correctly formatted as an integer string.\n        s = \"[\" + \",\".join(map(str, res)) + \"]\"\n        result_strings.append(s)\n    \n    final_output = \"[\" + \",\".join(result_strings) + \"]\"\n    print(final_output)\n\nsolve()\n```", "id": "3155706"}]}