{"hands_on_practices": [{"introduction": "模型容量是衡量其灵活性的一个关键指标。这个练习将通过一个多项式核模型的例子，引导你从第一性原理出发，亲手计算模型容量（即特征数量），并将其与泛化误差直接联系起来 [@problem_id:3148660]。通过这个实践，你将深刻理解模型复杂度的增加如何导致过拟合，以及为什么量化模型容量至关重要。", "problem": "一个团队正在为一个回归任务比较多项式核模型，该任务有 $p=20$ 个输入变量和 $n=500$ 个独立同分布的样本。多项式核对应一个线性模型，其特征空间由原始输入中总次数不超过 $d$ 的所有单项式构成。该团队观察到，次数 $d=2$ 和次数 $d=4$ 的模型都达到了基本为零的训练误差，而对于 $d=2$ 和 $d=4$，测试均方误差分别约为 $0.08$ 和 $0.20$。\n\n从计数和统计学习的第一性原理出发，完成以下任务。\n\n1) 仅使用单项式的定义，即形如 $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$ 的乘积，其中指数 $\\alpha_{j}$ 为非负整数，其总次数满足 $\\sum_{j=1}^{p} \\alpha_{j} \\le d$，推导出 $p$ 个变量中总次数最多为 $d$ 的不同单项式的数量的闭式表达式。您的推导不得假定任何已有的计数公式，并且应通过一个有效的组合论证来进行。\n\n2) 将第 (1) 部分得到的表达式在 $p=20$，$d=2$ 和 $d=4$ 时进行求值。\n\n3) 在统计学习理论中，一个经过充分检验的事实是：对于具有范数约束的M维有界特征空间中的线性预测器，基于诸如 Rademacher 复杂度等复杂度度量的不依赖于数据的泛化界与 $\\sqrt{M/n}$ 成正比。使用此比例关系作为近似，计算该问题中泛化差距的比率 $r$（次数 $d=4$ 除以次数 $d=2$）。将 $r$ 表示为一个无量纲数，并将您的答案四舍五入到三位有效数字。\n\n4) 使用您在第 (2) 部分中得到的计数结果，简要解释特征维度的变化如何与观察到的测试误差增加相关，以及这对模型的灵活性、容量和可解释性意味着什么。\n\n最终答案只需提交 $r$ 的数值。", "solution": "该问题被评估为有效，因为它在统计学习理论中有科学依据，提法恰当、客观，并包含了解答所需的所有必要信息。\n\n该问题要求完成四项任务：1) 推导单项式数量的公式，2) 对特定参数计算此公式的值，3) 基于给定的比例定律计算泛化差距的比率，以及 4) 对结果提供概念性解释。最终答案被指定为仅提交第 (3) 部分的数值。\n\n**第1部分：单项式数量的推导**\n\n我们被要求找出在 $p$ 个变量 $x_1, x_2, \\dots, x_p$ 中，总次数最多为 $d$ 的不同单项式的数量。一个单项式的形式为 $x_{1}^{\\alpha_{1}} x_{2}^{\\alpha_{2}} \\cdots x_{p}^{\\alpha_{p}}$，其中指数 $\\alpha_j$ 是非负整数。条件是总次数 $\\sum_{j=1}^{p} \\alpha_{j}$ 小于或等于 $d$。\n$$ \\sum_{j=1}^{p} \\alpha_{j} \\le d, \\quad \\alpha_j \\in \\{0, 1, 2, \\dots\\} $$\n这个问题等价于计算上述不等式的非负整数解的数量。为了将这个不等式转换为一个更易于处理的等式，我们引入一个非负整数“松弛”变量 $\\alpha_{p+1}$。我们定义 $\\alpha_{p+1}$ 为：\n$$ \\alpha_{p+1} = d - \\sum_{j=1}^{p} \\alpha_{j} $$\n由于 $\\sum_{j=1}^{p} \\alpha_{j} \\le d$，可以保证 $\\alpha_{p+1} \\ge 0$。原始不等式现在完全等价于找出以下方程的非负整数解的数量：\n$$ \\alpha_1 + \\alpha_2 + \\cdots + \\alpha_p + \\alpha_{p+1} = d $$\n这是一个经典的组合问题，可以按要求使用“隔板法”（或称“星与条”论证）来解决。想象我们有 $d$ 个相同的物品（星，$\\star$）要分配到 $p+1$ 个不同的箱子（即变量 $\\alpha_1, \\dots, \\alpha_{p+1}$）中。我们可以通过将 $d$ 个星排成一排，并使用 $p$ 个隔板（$|$）将其分成 $p+1$ 组来表示这种排列。第 $j$ 组中星的数量对应于 $\\alpha_j$ 的值。\n\n例如，对于 $d=3$ 个星和 $p=2$ 个变量（意味着 $p+1=3$ 个箱子），排列 $\\star|\\star\\star|$ 对应于 $\\alpha_1=1$，$\\alpha_2=2$，以及松弛变量 $\\alpha_3=0$。这对应于次数为 $3$ 的单项式 $x_1^1 x_2^2$。排列 $| \\star | \\star\\star$ 对应于 $\\alpha_1=0$，$\\alpha_2=1$ 和 $\\alpha_3=2$。这意味着 $\\alpha_1+\\alpha_2 = 1 \\le 3$，代表次数为 $1$ 的单项式 $x_2^1$。\n\n为了计算这种排列的数量，我们考虑一个总共有 $d+p$ 个位置的序列。我们需要从这些位置中选择 $p$ 个来放置隔板（剩下的 $d$ 个位置将由星填充）。完成此操作的方法数由二项式系数“($d+p$) 选 $p$”给出。\n\n令 $M(p, d)$ 为单项式的数量。从 $d+p$ 个总位置中选择 $p$ 个位置放置隔板的方法数是：\n$$ M(p, d) = \\binom{p+d}{p} $$\n或者，我们可以选择 $d$ 个位置来放置星，这给出了等价的表达式：\n$$ M(p, d) = \\binom{p+d}{d} = \\frac{(p+d)!}{d! p!} $$\n这是在 $p$ 个变量中总次数最多为 $d$ 的不同单项式数量的闭式表达式。这个计数包括了常数项（次数为 $0$ 的单项式），它对应于解 $\\alpha_1 = \\alpha_2 = \\dots = \\alpha_p = 0$。\n\n**第2部分：表达式求值**\n\n我们有 $p=20$ 个变量，并被要求计算次数为 $d=2$ 和 $d=4$ 的模型的特征数量。\n\n对于 $d=2$，令特征数量为 $M_2$。\n$$ M_2 = M(20, 2) = \\binom{20+2}{2} = \\binom{22}{2} $$\n$$ M_2 = \\frac{22!}{2!(22-2)!} = \\frac{22!}{2!20!} = \\frac{22 \\times 21}{2 \\times 1} = 11 \\times 21 = 231 $$\n\n对于 $d=4$，令特征数量为 $M_4$。\n$$ M_4 = M(20, 4) = \\binom{20+4}{4} = \\binom{24}{4} $$\n$$ M_4 = \\frac{24!}{4!(24-4)!} = \\frac{24!}{4!20!} = \\frac{24 \\times 23 \\times 22 \\times 21}{4 \\times 3 \\times 2 \\times 1} $$\n由于 $4 \\times 3 \\times 2 \\times 1 = 24$，我们可以化简：\n$$ M_4 = 23 \\times 22 \\times 21 = 506 \\times 21 = 10626 $$\n因此，次数为2的模型的特征数量为 $M_2 = 231$，次数为4的模型的特征数量为 $M_4 = 10626$。\n\n**第3部分：泛化差距比率的计算**\n\n我们被告知，对于一个在 $M$ 维特征空间中的线性预测器，典型的不依赖于数据的泛化界与 $\\sqrt{M/n}$ 成正比。泛化差距 $G$ 是测试误差与训练误差之间的差值。我们被要求使用这种比例关系作为近似。\n令 $G_d$ 为次数为 $d$ 的模型的泛化差距。\n$$ G_d \\propto \\sqrt{\\frac{M_d}{n}} $$\n这意味着 $G_d = C \\sqrt{M_d/n}$，其中 $C$ 是一个比例常数，它依赖于与此比率无关的因素，例如对权重的范数约束和数据分布的属性。\n\n泛化差距的预测比率 $r$ 是：\n$$ r = \\frac{G_4}{G_2} = \\frac{C \\sqrt{M_4/n}}{C \\sqrt{M_2/n}} $$\n常数 $C$ 和样本大小 $n$ 被消去了：\n$$ r = \\sqrt{\\frac{M_4}{M_2}} $$\n使用第2部分计算的值：\n$$ r = \\sqrt{\\frac{10626}{231}} $$\n首先，我们化简这个分数：\n$$ \\frac{10626}{231} = 46 $$\n因此，比率 $r$ 是：\n$$ r = \\sqrt{46} $$\n将其表示为一个四舍五入到三位有效数字的数：\n$$ r \\approx 6.7823... $$\n四舍五入到三位有效数字得到 $r \\approx 6.78$。\n\n**第4部分：对观察到的测试误差增加的解释**\n\n特征数量 $M$ 是模型容量或灵活性的直接度量。一个具有更大 $M$ 值的模型可以表示一个更复杂的函数类别。\n在这个问题中，我们有 $p=20$ 个输入和 $n=500$ 个样本。\n- 次数 $d=2$ 的模型有 $M_2=231$ 个特征。由于 $M_2  n$，这是一个可以实现良好拟合但不能保证完美插值的范畴。\n- 次数 $d=4$ 的模型有 $M_4=10626$ 个特征。这里，$M_4 \\gg n$，使得模型处于一个严重过参数化的范畴。\n\n两个模型都达到“基本为零的训练误差”这一观察结果可以用它们的高容量来解释。对于500个数据点，拥有231个特征的 $d=2$ 模型已经非常灵活。而 $d=4$ 的模型处于过参数化范畴（$M_4 > n$），其参数数量多于数据点，这通常使其能够对训练数据进行插值，从而将训练误差降至零。\n\n然而，测试误差是衡量模型对新的、未见过的数据泛化能力的度量。它受到偏差-方差权衡的影响。\n- **灵活性与容量**：从 $d=2$ 变为 $d=4$，特征空间维度从 $M_2=231$ 增加到 $M_4=10626$，模型容量急剧增加。\n- **过拟合与方差**：虽然增加的容量有助于减少训练误差（以及潜在的模型偏差），但其代价是方差大大增加。$d=4$ 的模型拥有海量特征，它不仅拟合了训练数据中的潜在信号，还拟合了该样本特有的随机噪声。这就是过拟合。模型对训练集的具体特性变得高度敏感。\n- **测试误差**：测试均方误差从 $d=2$ 的 $0.08$ 增加到 $d=4$ 的 $0.20$，是过拟合的典型症状。$d=4$ 模型增加的方差超过了任何潜在的偏差减少，导致泛化性能变差。对于这个问题和数据集大小，$d=2$ 模型代表了偏差和方差之间更好的平衡。\n- **可解释性**：可解释性与模型复杂度成反比。一个基于 $M_2=231$ 个特征的线性模型已经非常难以解释。而一个基于 $M_4=10626$ 个特征（涉及多达四个原始变量的复杂乘积）的模型，在所有实际应用中都是一个“黑箱”。人类不可能通过分析数万个系数的个别效应来理解模型的决策过程。相比之下，更简单的 $d=2$ 模型更具可解释性。\n\n总而言之，从 $d=2$ 到 $d=4$ 的转变极大地增加了模型容量，导致了严重的过拟合，其表现为由于高方差而导致的测试误差增加，以及模型可解释性的急剧下降。", "answer": "$$\n\\boxed{6.78}\n$$", "id": "3148660"}, {"introduction": "正则化是控制模型灵活性和防止过拟合的核心技术。这个动手实践要求你通过编程实现一个正则化逻辑回归模型，并观察在不同数据场景下，模型系数如何随着正则化强度的变化而改变 [@problem_id:3148628]。通过追踪系数符号的稳定性，你将直观地理解正则化如何约束模型容量，特别是在处理共线性和噪声时，它如何影响最终模型的可解释性。", "problem": "您的任务是分析逻辑回归系数的符号如何随着正则化强度的变化而变化，并将此行为与模型的灵活性、容量和可解释性联系起来。考虑在经验风险最小化 (ERM) 框架下，使用平方欧几里得范数（也称为 L2）惩罚的二元分类逻辑回归。模型参数为一个权重向量 $w \\in \\mathbb{R}^d$ 和一个截距 $b \\in \\mathbb{R}$，预测值为 $p_i = \\sigma(z_i)$，其中 $z_i = w^\\top x_i + b$ 且 $\\sigma(t) = \\frac{1}{1 + e^{-t}}$。对于数据集 $\\{(x_i, y_i)\\}_{i=1}^n$，其中 $y_i \\in \\{0,1\\}$，给定的正则化参数 $\\lambda \\ge 0$ 的正则化目标是\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{z_i}\\right) - y_i z_i \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2,\n$$\n其中 $\\|w\\|_2$ 是欧几里得范数，截距 $b$ 不受惩罚。\n\n将使用的定义：\n- 系数的符号（带容差 $\\varepsilon  0$）由函数 $S_\\varepsilon:\\mathbb{R}\\to\\{-1,0,1\\}$ 定义，\n$$\nS_\\varepsilon(u) =\n\\begin{cases}\n-1,  \\text{if } u  -\\varepsilon, \\\\\n0,  \\text{if } |u| \\le \\varepsilon, \\\\\n1,  \\text{if } u  \\varepsilon.\n\\end{cases}\n$$\n- 给定一个递增排序的正则化参数网格 $\\Lambda = \\{\\lambda_1, \\lambda_2, \\dots, \\lambda_m\\}$，对于特征索引 $j \\in \\{1, \\dots, d\\}$，路径上的符号变化计数为\n$$\nC_j = \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) \\ne S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\},\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数，$w_j(\\lambda)$ 是在正则化强度为 $\\lambda$ 时拟合的特征 $j$ 的系数。\n- 路径上每个特征的稳定性是没有发生符号变化的相邻对的分数：\n$$\n\\text{stab}_j = \\frac{1}{m-1} \\sum_{k=1}^{m-1} \\mathbf{1}\\left\\{ S_\\varepsilon\\big(w_j(\\lambda_k)\\big) = S_\\varepsilon\\big(w_j(\\lambda_{k+1})\\big) \\right\\}.\n$$\n- 总体稳定性指数是所有特征的平均稳定性：\n$$\n\\text{Stability} = \\frac{1}{d} \\sum_{j=1}^d \\text{stab}_j.\n$$\n\n从逻辑斯谛模型、其损失函数和正则化惩罚的基本定义出发，推导并实现一个正则化目标 $J_\\lambda(w,b)$ 的求解器，该求解器使用基于梯度的优化方法为每个 $\\lambda \\in \\Lambda$ 拟合 $(w(\\lambda), b(\\lambda))$。然后，为下面描述的每个测试用例计算每个特征 $j$ 的 $C_j$ 和总体稳定性 (Stability)。\n\n正则化网格固定为 $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$，容差为 $\\varepsilon = 10^{-6}$，截距 $b$ 不得被惩罚。\n\n为确保覆盖范围的测试套件（三种情况）：\n- 情况 A（通用，信号清晰）：使用种子 0 生成数据。设 $n=120, d=3$。从标准正态分布中独立抽取 $X \\in \\mathbb{R}^{n \\times d}$ 的条目。定义真实参数 $w^* = (3, -2, 0)$ 和 $b^* = 0$。对于每个 $i$，计算 $z_i^* = x_i^\\top w^* + b^*$ 并设置 $y_i = 1$ 如果 $z_i^* \\ge 0$，否则 $y_i = 0$。\n- 情况 B（高度共线性，归因模糊）：使用种子 1 生成数据。设 $n=100, d=3$。从标准正态分布中抽取 $x^{(1)} \\in \\mathbb{R}^n$。创建 $x^{(2)} = x^{(1)} + 0.01 \\cdot \\xi$，其中 $\\xi$ 是使用相同种子序列的标准正态分布。独立地从标准正态分布中抽取 $x^{(3)}$。通过堆叠 $(x^{(1)}, x^{(2)}, x^{(3)})$ 形成 $X$。定义 $w^* = (1, -1, 0), b^* = 0$。对于每个 $i$，设置 $y_i = 1$ 如果 $x_i^{(1)} - x_i^{(2)} \\ge 0$，否则 $y_i = 0$。\n- 情况 C（低信噪比，正则化主导）：使用种子 2 生成数据。设 $n=200, d=3$。从标准正态分布中抽取 $X$。定义 $w^* = (0.2, -0.2, 0)$ 和 $b^* = 0$。使用相同的种子从标准正态分布中抽取独立噪声 $\\eta \\in \\mathbb{R}^n$。计算 $z_i^* = x_i^\\top w^* + b^* + 0.1 \\eta_i$ 并设置 $y_i = 1$ 如果 $z_i^* \\ge 0$，否则 $y_i = 0$。\n\n程序要求：\n- 使用带有精确梯度的基于梯度的优化器，通过最小化 $J_\\lambda(w,b)$ 来为每个 $\\lambda \\in \\Lambda$ 拟合 $(w(\\lambda), b(\\lambda))$；确保截距 $b$ 不受惩罚。\n- 对于每个测试用例，计算 $d=3$ 个特征在 $\\Lambda$ 上的符号变化计数列表 $[C_1, C_2, C_3]$，以及如上定义的总体稳定性 (Stability)，表示为小数点后保留三位的小数。\n- 最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个元素对应一个测试用例，其本身是一个形如 $[[C_1,C_2,C_3],\\text{Stability}]$ 的双元素列表。例如，最终打印的行必须类似于 $[[[c_{A1},c_{A2},c_{A3}],s_A],[[c_{B1},c_{B2},c_{B3}],s_B],[[c_{C1},c_{C2},c_{C3}],s_C]]$，其中所有 $c$ 值为整数，所有 $s$ 值为保留三位小数的小数。\n\n不涉及物理单位或角度，且不应使用百分比。所有数值输出必须是指定格式的整数或小数。必须完全按照描述生成数据集，以确保可复现性。实现不得需要任何外部输入，并且必须能直接运行。", "solution": "用户希望分析在不同 L2 正则化强度下，逻辑回归系数的稳定性。这涉及实现一个逻辑回归求解器，生成三个不同的数据集，对一系列正则化参数 ($\\lambda$) 拟合模型，然后量化该正则化路径上系数符号的稳定性。\n\n### 1. 数学公式\n\n问题是最小化逻辑回归的正则化目标函数：\n$$\nJ_\\lambda(w,b) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\log\\left(1 + e^{w^\\top x_i + b}\\right) - y_i (w^\\top x_i + b) \\right) + \\frac{\\lambda}{2} \\|w\\|_2^2\n$$\n该目标函数是凸函数，确保对于任何 $\\lambda  0$，基于梯度的优化方法都能找到唯一的全局最小值。要使用此类优化器，我们需要 $J_\\lambda$ 关于参数 $w$ 和 $b$ 的梯度。\n\n令 $z_i = w^\\top x_i + b$。sigmoid 函数为 $\\sigma(z_i) = p_i = \\frac{1}{1 + e^{-z_i}}$。\n单个数据点的损失项关于 $z_i$ 的梯度为：\n$$\n\\frac{\\partial}{\\partial z_i} \\left( \\log(1 + e^{z_i}) - y_i z_i \\right) = \\frac{e^{z_i}}{1 + e^{z_i}} - y_i = \\sigma(z_i) - y_i = p_i - y_i\n$$\n使用链式法则，我们可以找到关于 $w$ 和 $b$ 各分量的梯度：\n$$\n\\frac{\\partial z_i}{\\partial w_j} = x_{ij} \\quad \\text{和} \\quad \\frac{\\partial z_i}{\\partial b} = 1\n$$\n目标函数 $J_\\lambda$ 关于权重 $w_j$ 的梯度是：\n$$\n\\nabla_{w_j} J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_j} + \\frac{\\partial}{\\partial w_j} \\left( \\frac{\\lambda}{2} \\|w\\|_2^2 \\right) = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)x_{ij} + \\lambda w_j\n$$\n以向量形式，关于整个权重向量 $w$ 的梯度是：\n$$\n\\nabla_w J_\\lambda = \\frac{1}{n} X^\\top (p - y) + \\lambda w\n$$\n其中 $p$ 是预测概率向量，$y$ 是真实标签向量。\n\n截距 $b$ 不受惩罚。其梯度为：\n$$\n\\nabla_b J_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial J_\\lambda}{\\partial z_i} \\frac{\\partial z_i}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (p_i - y_i)\n$$\n\n这些精确梯度将提供给一个拟牛顿优化算法 (L-BFGS-B)，以便为指定网格中的每个 $\\lambda$ 找到最优的 $(w(\\lambda), b(\\lambda))$。\n\n### 2. 实现策略\n\n解决方案使用 Python 实现，遵循指定的环境。每个测试用例的总体流程如下：\n\n1.  **数据生成**：对于三个测试用例（A、B、C）中的每一个，根据问题的规则生成一个特定的数据集 $(X, y)$，使用带有指定种子的 `numpy.random.default_rng` 以确保可复现性。\n\n2.  **模型拟合**：一个循环遍历固定的正则化网格 $\\Lambda = \\{0.001, 0.01, 0.1, 1.0, 10.0, 100.0\\}$。\n    *   对于每个 $\\lambda \\in \\Lambda$，将目标函数 $J_\\lambda(w,b)$ 及其梯度传递给使用 'L-BFGS-B' 方法的 `scipy.optimize.minimize`。待优化的参数是连接 $w$ 和 $b$ 的单个向量。\n    *   实现采用“热启动”：为给定 $\\lambda_k$ 找到的解 $(w, b)$ 将用作下一个值 $\\lambda_{k+1}$ 优化的初始猜测。这提高了效率，因为相邻 $\\lambda$ 值的解通常很接近。\n    *   通过对损失项使用 `log-sum-exp` 技巧和对 sigmoid 函数使用 `scipy.special.expit` 来确保数值稳定性。\n\n3.  **指标计算**：在为所有 $\\lambda$ 值拟合模型后，分析得到的系数路径（一个 $w$ 向量序列）。\n    *   使用提供的符号函数 $S_\\varepsilon(u)$（其中 $\\varepsilon = 10^{-6}$）确定每个系数 $w_j(\\lambda)$ 的符号。\n    *   通过比较路径中相邻 $\\lambda$ 值处的系数符号，计算每个特征 $j$ 的符号变化次数 $C_j$。\n    *   `Stability` 指数计算为每个特征的稳定性 $\\text{stab}_j$ 的平均值，其中 $\\text{stab}_j$ 是 $w_j$ 符号不发生变化的相邻 $\\lambda$ 对的分数。这等价于 $\\text{stab}_j = 1 - C_j / (m-1)$，其中 $m$ 是 $\\lambda$ 网格中的点数。\n\n4.  **输出格式化**：将每个测试用例的结果（包括符号变化计数列表 $[C_1, C_2, C_3]$ 和总体 `Stability`（四舍五入到三位小数））格式化为指定的精确字符串表示，不含多余空格。最终输出是包含这些结果列表的单行。\n\n### 3. 测试用例的预期行为\n\n*   **情况 A (信号清晰)**：由于特征 1 和 2 具有强的潜在系数，它们的符号应被正确识别并保持稳定，直到强正则化将其收缩至零。特征 3 没有信号，因此其系数应保持在零附近。我们预期符号变化次数少，稳定性高。\n*   **情况 B (高度共线性)**：特征 1 和 2 几乎相同 ($x^{(2)} \\approx x^{(1)}$)，而结果取决于它们的差异 ($x^{(1)} - x^{(2)}$)。这会产生模糊性，因为许多满足 $w_1 \\approx -w_2$ 的 $w_1$ 和 $w_2$ 组合都能产生良好的拟合。L2 惩罚将选择一个唯一的解，但系数路径可能不如情况 A 中平滑，可能导致 $w_1$ 和 $w_2$ 的不稳定性增加和更多的符号变化。\n*   **情况 C (低信噪比)**：真实系数很小，且线性预测器中加入了显著的噪声。模型可能难以从噪声中区分出弱信号，尤其是在低正则化时。随着 $\\lambda$ 的增加，惩罚项将迅速占据主导地位，将已经很小的系数收缩至零。这种情况预计将表现出最大的不稳定性，稳定性得分最低。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.special import expit\n\ndef solve():\n    \"\"\"\n    Solves the logistic regression regularization path problem for three test cases.\n    \"\"\"\n\n    # --- Constants from the problem statement ---\n    LAMBDA_GRID = np.array([0.001, 0.01, 0.1, 1.0, 10.0, 100.0])\n    EPSILON = 1e-6\n\n    # --- Nested Helper Functions ---\n    def _generate_data(case_spec):\n        \"\"\"Generates dataset (X, y) based on the case specification.\"\"\"\n        case_id = case_spec['id']\n        seed = case_spec['seed']\n        n = case_spec['n']\n        d = case_spec['d']\n        rng = np.random.default_rng(seed)\n\n        if case_id == 'A':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            X = rng.standard_normal(size=(n, d))\n            z_star = X @ w_star + b_star\n            y = (z_star >= 0).astype(int)\n        elif case_id == 'B':\n            # Collinear features\n            x1 = rng.standard_normal(size=n)\n            xi = rng.standard_normal(size=n)\n            x2 = x1 + 0.01 * xi\n            x3 = rng.standard_normal(size=n)\n            X = np.stack([x1, x2, x3], axis=1)\n            y = ((x1 - x2) >= 0).astype(int)\n        elif case_id == 'C':\n            w_star = np.array(case_spec['w_star'])\n            b_star = case_spec['b_star']\n            noise_std = case_spec['noise_std']\n            X = rng.standard_normal(size=(n, d))\n            eta = rng.standard_normal(size=n)\n            z_star = X @ w_star + b_star + noise_std * eta\n            y = (z_star >= 0).astype(int)\n        \n        return X, y\n\n    def _objective_and_grad(params, X, y, lambda_val):\n        \"\"\"Computes the objective function J and its gradient.\"\"\"\n        n, d = X.shape\n        w = params[:d]\n        b = params[d]\n\n        z = X @ w + b\n\n        # Numerically stable calculation of log-likelihood part\n        log_1_plus_exp_z = np.maximum(0, z) + np.log(1 + np.exp(-np.abs(z)))\n        loss = np.mean(log_1_plus_exp_z - y * z)\n\n        # L2 regularization term (only on w)\n        reg_term = (lambda_val / 2.0) * np.dot(w, w)\n        J = loss + reg_term\n\n        # Gradient calculation\n        p = expit(z)\n        p_minus_y = p - y\n        grad_w = (1.0 / n) * X.T @ p_minus_y + lambda_val * w\n        grad_b = np.mean(p_minus_y)\n        grad = np.append(grad_w, grad_b)\n        \n        return J, grad\n\n    def _s_epsilon_vec(u_vec, epsilon):\n        \"\"\"Vectorized sign function with tolerance.\"\"\"\n        return np.select([u_vec  -epsilon, u_vec > epsilon], [-1, 1], default=0)\n\n    def _calculate_metrics(w_path, m, d, epsilon):\n        \"\"\"Calculates sign-change counts and stability.\"\"\"\n        w_path_np = np.array(w_path)\n        sign_path = _s_epsilon_vec(w_path_np, epsilon)\n        \n        # Sign-change count C_j\n        sign_diffs = np.diff(sign_path, axis=0)\n        sign_changes = (sign_diffs != 0).astype(int)\n        c_counts = np.sum(sign_changes, axis=0)\n        \n        # Stability\n        num_transitions = m - 1\n        if num_transitions == 0:\n            return c_counts, 1.0\n        \n        stab_j = (num_transitions - c_counts) / num_transitions\n        overall_stability = np.mean(stab_j)\n        \n        return c_counts, overall_stability\n\n    # --- Main Logic ---\n    test_cases = [\n        {'id': 'A', 'seed': 0, 'n': 120, 'd': 3, 'w_star': [3, -2, 0], 'b_star': 0},\n        {'id': 'B', 'seed': 1, 'n': 100, 'd': 3},\n        {'id': 'C', 'seed': 2, 'n': 200, 'd': 3, 'w_star': [0.2, -0.2, 0], 'b_star': 0, 'noise_std': 0.1},\n    ]\n\n    all_results_str = []\n\n    for case_spec in test_cases:\n        X, y = _generate_data(case_spec)\n        d = X.shape[1]\n        \n        w_path = []\n        # Initial guess for the optimization (cold start for the first lambda)\n        params_0 = np.zeros(d + 1)\n        \n        for lambda_val in LAMBDA_GRID:\n            result = minimize(\n                fun=_objective_and_grad,\n                x0=params_0,\n                args=(X, y, lambda_val),\n                method='L-BFGS-B',\n                jac=True\n            )\n            # Use the current solution as a warm start for the next iteration\n            params_0 = result.x\n            w_path.append(result.x[:d])\n\n        c_counts, stability = _calculate_metrics(w_path, len(LAMBDA_GRID), d, EPSILON)\n        \n        c_counts_list = list(c_counts.astype(int))\n        \n        # Format the result string for this case\n        result_str = f\"[[{','.join(map(str, c_counts_list))}],{stability:.3f}]\"\n        all_results_str.append(result_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3148628"}, {"introduction": "在一系列复杂度不同的模型中，如何选择最优的那个？这个练习将引导你实现统计学习理论中的一个基本原则——结构风险最小化（Structural Risk Minimization, SRM）[@problem_id:3148607]。你将通过编写代码来计算并最小化一个包含经验风险和复杂度惩罚项的泛化风险上界，从而在一个嵌套的模型类中做出有理论依据的选择。这个实践将帮助你深入理解在拟合优度与模型复杂度之间进行权衡的精髓。", "problem": "给定一系列嵌套的二元分类假设类 $\\mathcal{H}_1 \\subset \\cdots \\subset \\mathcal{H}_k$，损失为 0-1 损失，样本量为 $n$，以及每个类 $\\mathcal{H}_i$ 内经验风险最小化器的实测经验风险 $\\hat{R}_i$。目标是通过选择一个索引 $i^\\star \\in \\{1,\\ldots,k\\}$ 来实现结构风险最小化（SRM），该索引能最小化真实风险的一致泛化界。选择规则应基于带有 Vapnik–Chervonenkis (VC) 维正则化和结构化置信度分配的一致收敛界。你的程序必须实现以下选择原则：\n- 对于每个 VC 维为 $d_i$ 的类 $\\mathcal{H}_i$，定义分配的置信水平 $\\delta_i$ 为 $\\delta_i = \\delta / 2^i$，其中 $\\delta \\in (0,1)$ 是用户指定的总置信水平。\n- 对于每个 $i \\in \\{1,\\ldots,k\\}$，计算界\n$$\nB_i \\;=\\; \\hat{R}_i \\;+\\; \\Phi(d_i,n,\\delta_i),\n$$\n其中\n$$\n\\Phi(d,n,\\delta)\\;=\\;\\sqrt{\\frac{2}{n}\\left(d \\ln\\!\\left(\\frac{2 e n}{\\max\\{d,1\\}}\\right) + \\ln\\!\\left(\\frac{4}{\\delta}\\right)\\right)}.\n$$\n这里 $e$ 表示欧拉数，$\\ln(\\cdot)$ 是自然对数。项 $d \\ln\\!\\left(\\frac{2 e n}{\\max\\{d,1\\}}\\right)$ 实现了来自 Sauer–Shelah 的关于 Vapnik–Chervonenkis (VC) 维的标准增长函数界，约定当 $d=0$ 时，由于分母中的 $\\max\\{d,1\\}$，该项为零。选择规则是\n$$\ni^\\star \\in \\arg\\min_{1 \\le i \\le k} \\; B_i.\n$$\n如果出现平局，则选择最小的索引 $i$ 来打破平局。\n\n你的任务：\n- 实现上述 SRM 选择，并为下面的每个测试用例返回所选的索引 $i^\\star$。\n- 使用上面指定的确切公式。\n- 不涉及角度。没有物理单位。\n- 最终输出必须是单行，包含所有测试用例的所选索引，以逗号分隔的列表形式包含在方括号内，没有空格，并按照测试套件的顺序。例如，一个有效的输出格式看起来像 $[2,1,3]$。\n\n测试套件：\n对于每个测试用例，给定元组 $(n,\\delta,\\mathbf{d},\\hat{\\mathbf{R}})$，其中 $\\mathbf{d} = [d_1,\\ldots,d_k]$ 且 $\\hat{\\mathbf{R}} = [\\hat{R}_1,\\ldots,\\hat{R}_k]$。\n\n- 测试用例 1：$(n=\\;200,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,3,5,10],\\;\\hat{\\mathbf{R}}=\\;[0.08,0.06,0.05,0.048])$。\n- 测试用例 2：$(n=\\;20000,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,3,5,10],\\;\\hat{\\mathbf{R}}=\\;[0.08,0.06,0.05,0.03])$。\n- 测试用例 3：$(n=\\;400,\\;\\delta=\\;0.1,\\;\\mathbf{d}=\\;[0,2,4],\\;\\hat{\\mathbf{R}}=\\;[0.15,0.10,0.09])$。\n- 测试用例 4：$(n=\\;30,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,2,3],\\;\\hat{\\mathbf{R}}=\\;[0.20,0.15,0.12])$。\n- 测试用例 5：$(n=\\;5000,\\;\\delta=\\;0.05,\\;\\mathbf{d}=\\;[1,10,50,200],\\;\\hat{\\mathbf{R}}=\\;[0.04,0.02,0.0,0.0])$。\n\n你的程序应生成单行输出，其中包含按顺序排列的所选索引的结果，格式为逗号分隔的列表，包含在方括号内，没有空格，形式完全如 $[i_1,i_2,i_3,i_4,i_5]$。", "solution": "用户希望实现结构风险最小化（SRM）原则，以从一组嵌套的假设类中选择一个模型。\n\n### 第 1 步：提取给定信息\n\n问题提供了以下定义和数据：\n- 一系列 $k$ 个嵌套的假设类：$\\mathcal{H}_1 \\subset \\cdots \\subset \\mathcal{H}_k$。\n- 样本量 $n$。\n- 总置信水平 $\\delta \\in (0,1)$。\n- 每个类 $\\mathcal{H}_i$ 的 VC 维向量 $\\mathbf{d} = [d_1, \\ldots, d_k]$。\n- 每个类 $\\mathcal{H}_i$ 中经验风险最小化器的经验风险向量 $\\hat{\\mathbf{R}} = [\\hat{R}_1, \\ldots, \\hat{R}_k]$。\n\n选择规则是选择最小化真实风险上界 $B_i$ 的索引 $i^\\star$。\n每个类 $\\mathcal{H}_i$ 的界由以下公式给出：\n$$\nB_i = \\hat{R}_i + \\Phi(d_i, n, \\delta_i)\n$$\n其中，类 $\\mathcal{H}_i$ 的分配置信水平 $\\delta_i$ 是：\n$$\n\\delta_i = \\frac{\\delta}{2^i}\n$$\n而惩罚项 $\\Phi(d, n, \\delta)$ 定义为：\n$$\n\\Phi(d,n,\\delta) = \\sqrt{\\frac{2}{n}\\left(d \\ln\\!\\left(\\frac{2 e n}{\\max\\{d,1\\}}\\right) + \\ln\\!\\left(\\frac{4}{\\delta}\\right)\\right)}\n$$\n值 $e$ 是欧拉数，$\\ln(\\cdot)$ 是自然对数。依赖于增长函数的项 $d \\ln(\\dots)$ 在 $d=0$ 时取为 0。\n\n选择规则是：\n$$\ni^\\star \\in \\arg\\min_{1 \\le i \\le k} B_i\n$$\n平局通过选择最小的索引 $i$ 来解决。\n\n提供的测试套件如下：\n- 测试用例 1：$(n=200, \\delta=0.05, \\mathbf{d}=[1,3,5,10], \\hat{\\mathbf{R}}=[0.08,0.06,0.05,0.048])$\n- 测试用例 2：$(n=20000, \\delta=0.05, \\mathbf{d}=[1,3,5,10], \\hat{\\mathbf{R}}=[0.08,0.06,0.05,0.03])$\n- 测试用例 3：$(n=400, \\delta=0.1, \\mathbf{d}=[0,2,4], \\hat{\\mathbf{R}}=[0.15,0.10,0.09])$\n- 测试用例 4：$(n=30, \\delta=0.05, \\mathbf{d}=[1,2,3], \\hat{\\mathbf{R}}=[0.20,0.15,0.12])$\n- 测试用例 5：$(n=5000, \\delta=0.05, \\mathbf{d}=[1,10,50,200], \\hat{\\mathbf{R}}=[0.04,0.02,0.0,0.0])$\n\n### 第 2 步：使用提取的给定信息进行验证\n\n根据指定标准对问题进行验证。\n- **科学依据：** 该问题基于统计学习理论中公认的结构风险最小化（SRM）原则。所提供的泛化界公式是一个标准的 Vapnik–Chervonenkis (VC) 界，其中包含了对模型复杂度的惩罚（通过 VC 维 $d$）和结构化置信度分配（$\\delta_i$）来处理多个假设类上的一致收敛。公式的结构与已知的理论结果一致。对 $d=0$ 情况的处理被明确且正确地定义。该问题具有科学合理性。\n- **适定性：** 每个测试用例都提供了所有必要的输入（$n, \\delta, \\mathbf{d}, \\hat{\\mathbf{R}}$）。目标函数 $B_i$ 定义明确。最小化准则结合平局决胜规则（选择最小的索引），确保每个测试用例都有唯一的解。该问题是适定的。\n- **客观性：** 问题使用精确的数学表达式和客观数据进行表述。没有主观或含糊的陈述。\n- **完整性与一致性：** 问题是自包含的。解决问题所需的所有变量和公式都已明确给出。嵌套假设类的前提意味着这些类中的真实风险最小化器将具有非递增的经验风险（$\\hat{R}_1 \\ge \\hat{R}_2 \\ge \\dots$），这一性质在所有测试用例中都成立。设置是完整且内部一致的。\n\n该问题没有表现出任何使其无效的缺陷。\n\n### 第 3 步：结论与行动\n\n问题是有效的。将提供一个合理的解决方案。\n\n### 解决方案\n\n目标是为每个给定的测试用例找到最小化风险界 $B_i$ 的索引 $i^\\star$。总体逻辑是遍历每个假设类 $\\mathcal{H}_i$（其中 $i=1, \\ldots, k$），计算相应的界 $B_i$，然后确定产生最小 $B_i$ 值的索引 $i$。\n\n对于一个给定的测试用例 $(n, \\delta, \\mathbf{d}, \\hat{\\mathbf{R}})$，设 $k$ 为类的数量，即向量 $\\mathbf{d}$ 的长度。我们对从 $1$ 到 $k$ 的每个类索引 $i$ 执行以下计算：\n\n1.  **确定参数**：对于每个类 $i$，我们使用相应的 VC 维 $d_i$ 和经验风险 $\\hat{R}_i$。\n2.  **计算分配的置信度**：类 $i$ 的置信水平 $\\delta_i$ 计算为 $\\delta_i = \\delta / 2^i$。这种分配确保了所有类的总误差概率受 $\\sum_{i=1}^k \\delta_i  \\sum_{i=1}^\\infty \\delta/2^i = \\delta$ 限制。\n3.  **计算惩罚项 $\\Phi_i$**：复杂度惩罚 $\\Phi_i = \\Phi(d_i, n, \\delta_i)$ 使用给定公式计算：\n    $$\n    \\Phi_i = \\sqrt{\\frac{2}{n}\\left(d_i \\ln\\!\\left(\\frac{2 e n}{\\max\\{d_i,1\\}}\\right) + \\ln\\!\\left(\\frac{4}{\\delta_i}\\right)\\right)}\n    $$\n    如规定，如果 $d_i=0$，则 $d_i \\ln(\\dots)$ 项正确地计算为 $0$。`max` 函数防止了除以零。\n4.  **计算风险界 $B_i$**：总风险界是经验风险和惩罚项之和：\n    $$\n    B_i = \\hat{R}_i + \\Phi_i\n    $$\n5.  **选择最优索引 $i^\\star$**：在计算完所有 $k$ 个界 $[B_1, B_2, \\ldots, B_k]$ 之后，我们找到对应于最小界的索引 $i^\\star$。问题规定，在出现平局的情况下，应选择最小的索引 $i$。这通过在界的序列中找到最小值的第一次出现来自然处理。\n\n此过程应用于套件中的每个测试用例。结果，即确定的索引 $i^\\star$ 的列表，然后被格式化为所需的输出字符串。\n\n例如，我们来分析测试用例 1：$(n=200, \\delta=0.05, \\mathbf{d}=[1,3,5,10], \\hat{\\mathbf{R}}=[0.08,0.06,0.05,0.048])$。\n-   对于 $i=1$：$d_1=1$，$\\hat{R}_1=0.08$，$\\delta_1 = 0.05/2^1 = 0.025$。\n    $B_1 = 0.08 + \\sqrt{\\frac{2}{200}\\left(1 \\cdot \\ln(\\frac{2e \\cdot 200}{1}) + \\ln(\\frac{4}{0.025})\\right)} \\approx 0.08 + 0.34737 = 0.42737$。\n-   对于 $i=2$：$d_2=3$，$\\hat{R}_2=0.06$，$\\delta_2 = 0.05/2^2 = 0.0125$。\n    $B_2 = 0.06 + \\sqrt{\\frac{2}{200}\\left(3 \\cdot \\ln(\\frac{2e \\cdot 200}{3}) + \\ln(\\frac{4}{0.0125})\\right)} \\approx 0.06 + 0.48422 = 0.54422$。\n-   对于 $i=3$：$d_3=5$，$\\hat{R}_3=0.05$，$\\delta_3 = 0.05/2^3 = 0.00625$。\n    $B_3 = 0.05 + \\sqrt{\\frac{2}{200}\\left(5 \\cdot \\ln(\\frac{2e \\cdot 200}{5}) + \\ln(\\frac{4}{0.00625})\\right)} \\approx 0.05 + 0.57768 = 0.62768$。\n-   对于 $i=4$：$d_4=10$，$\\hat{R}_4=0.048$，$\\delta_4 = 0.05/2^4 = 0.003125$。\n    $B_4 = 0.048 + \\sqrt{\\frac{2}{200}\\left(10 \\cdot \\ln(\\frac{2e \\cdot 200}{10}) + \\ln(\\frac{4}{0.003125})\\right)} \\approx 0.048 + 0.73515 = 0.78315$。\n\n比较这些界 $[0.42737, 0.54422, 0.62768, 0.78315]$，最小值是 $B_1$。因此，对于此用例，$i^\\star=1$。相同的逻辑适用于所有其他测试用例。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Structural Risk Minimization (SRM) selection principle for a series of nested\n    hypothesis classes, based on a VC dimension generalization bound.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, delta, d_list, R_hat_list)\n        (200, 0.05, [1, 3, 5, 10], [0.08, 0.06, 0.05, 0.048]),\n        (20000, 0.05, [1, 3, 5, 10], [0.08, 0.06, 0.05, 0.03]),\n        (400, 0.1, [0, 2, 4], [0.15, 0.10, 0.09]),\n        (30, 0.05, [1, 2, 3], [0.20, 0.15, 0.12]),\n        (5000, 0.05, [1, 10, 50, 200], [0.04, 0.02, 0.0, 0.0]),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, delta, d_list, R_hat_list = case\n        k = len(d_list)\n        bounds = []\n\n        for i in range(k):\n            # The class index is 1-based (i+1) for the formulas.\n            class_index = i + 1\n            d_i = d_list[i]\n            R_hat_i = R_hat_list[i]\n            \n            # Calculate the allocated confidence level for the current class.\n            delta_i = delta / (2**class_index)\n\n            # Calculate the penalty term Phi(d, n, delta) using the provided formula.\n            # growth_term: d*ln(2*e*n / max(d,1))\n            # The formula is robust: if d_i is 0, the term becomes 0.\n            growth_term = d_i * np.log((2 * np.e * n) / max(d_i, 1.0))\n            \n            # confidence_term: ln(4/delta)\n            confidence_term = np.log(4 / delta_i)\n            \n            # Combine terms to get the full penalty.\n            penalty = np.sqrt((2 / n) * (growth_term + confidence_term))\n\n            # Calculate the risk bound B_i = R_hat_i + Phi_i.\n            bound_i = R_hat_i + penalty\n            bounds.append(bound_i)\n\n        # Find the 1-based index that minimizes the bound.\n        # np.argmin returns the index of the first minimum, which respects the tie-breaking rule.\n        i_star = np.argmin(bounds) + 1\n        results.append(i_star)\n\n    # Format the final output as a comma-separated list in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3148607"}]}