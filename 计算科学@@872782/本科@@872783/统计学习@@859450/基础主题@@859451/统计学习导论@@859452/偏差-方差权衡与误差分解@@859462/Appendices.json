{"hands_on_practices": [{"introduction": "为了防止过拟合，我们常常通过对模型复杂度施加惩罚来进行正则化。这个练习提供了一个清晰的分析视角，来观察正则化参数 $\\lambda$ 如何直接调控偏差和方差之间的权衡。通过推导一个带有粗糙度惩罚的样条回归模型的偏差和方差，你将亲手计算出增加 $\\lambda$ 是如何提高偏差（模型变得更简单，可能无法捕捉真实信号）但降低方差（模型对训练数据中的噪声不那么敏感），并最终找到那个实现最佳平衡点的最优 $\\lambda$ 值。[@problem_id:3180624]", "problem": "考虑一个带有独立同分布噪声的非参数回归模型：$y_i = f_0(x_i) + \\varepsilon_i$，对于 $i = 1,\\dots,n$，其中 $x_i = i/n$ 在 $[0,1]$ 中等距分布，且 $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$。设估计量是通过最小化带二阶导数光滑度惩罚项的惩罚经验风险得到的：在形如 $f(x) = \\theta \\,\\varphi(x)$ 的函数中，选择 $\\hat{f}$ 以最小化\n$$\n\\frac{1}{n} \\sum_{i=1}^n \\big(y_i - f(x_i)\\big)^2 \\;+\\; \\lambda \\int_0^1 \\big(f''(x)\\big)^2 \\, dx,\n$$\n其中 $\\lambda \\ge 0$ 是一个调节参数，$\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)$，$k$ 为一个给定的正整数。假设 $n$ 是 $k$ 的偶数倍，因此 $(1/n)\\sum_{i=1}^n \\varphi(x_i)^2 = 1$ 精确成立。\n\n1) 从逐点偏置、方差和均方误差的定义出发，推导在任意点 $x_0 \\in [0,1]$ 处，$\\hat{f}(x_0)$ 的逐点偏置和方差作为 $\\lambda$ 的函数的显式公式。你的推导应从第一性原理出发，通过求解 $\\hat{\\theta}$ 的惩罚最小二乘问题，将 $\\hat{f}(x_0)$ 表示为数据的线性估计量，然后求其期望和方差。\n\n2) 针对已知信号 $f_0(x) = \\beta \\sin(2\\pi k x)$（其中 $\\beta \\in \\mathbb{R}$）进行特例分析，并取 $x_0$ 为 $f_0$ 的曲率最大点（例如，任何满足 $\\sin(2\\pi k x_0) = 1$ 的 $x_0$）。显式计算曲率权重 $r := \\int_0^1 \\big(\\varphi''(x)\\big)^2\\,dx$，并使用第 (1) 部分的公式，将逐点均方误差 $\\mathbb{E}\\big[(\\hat{f}(x_0) - f_0(x_0))^2\\big]$ 写成 $\\lambda$ 的函数。\n\n3) 确定使第 (2) 部分中 $x_0$ 处的逐点均方误差最小化的 $\\lambda$ 值。将你的最终答案表示为关于 $n$、$\\sigma^2$、$\\beta$ 和 $k$ 的闭式表达式。无需进行数值计算。只提供最终表达式。不包含任何单位。\n\n答案格式要求：你的最终答案必须是一个单一的闭式解析表达式。如果你进行了任何近似，请不要说明；提供精确表达式。不要对你的答案进行四舍五入。", "solution": "该问题要求找到最优调节参数 $\\lambda$，以最小化一个带惩罚的最小二乘估计量的逐点均方误差（MSE）。我们将按要求分三部分解决这个问题。首先，我们推导估计量的逐点偏置和方差的一般表达式。其次，我们将这些表达式应用于给定的信号和评估点，并计算 MSE。第三，我们关于 $\\lambda$ 最小化此 MSE。\n\n估计量 $\\hat{f}(x)$ 的形式为 $f(x) = \\theta \\varphi(x)$，其中 $\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)$。参数 $\\hat{\\theta}$ 通过最小化目标函数来找到：\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - \\theta \\varphi(x_i)\\big)^2 \\;+\\; \\lambda \\int_0^1 \\big((\\theta\\varphi(x))''\\big)^2 \\, dx\n$$\n我们可以重写惩罚项。令 $r := \\int_0^1 \\big(\\varphi''(x)\\big)^2 \\, dx$。目标函数变为：\n$$\nL(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\big(y_i - \\theta \\varphi(x_i)\\big)^2 \\;+\\; \\lambda \\theta^2 r\n$$\n这是一个关于 $\\theta$ 的二次函数。为了找到最小值，我们对 $\\theta$ 求导并将结果设为零：\n$$\n\\frac{dL}{d\\theta} = \\frac{1}{n} \\sum_{i=1}^n 2\\big(y_i - \\theta \\varphi(x_i)\\big)(-\\varphi(x_i)) \\;+\\; 2 \\lambda \\theta r = 0\n$$\n$$\n-\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) + \\frac{\\theta}{n} \\sum_{i=1}^n \\varphi(x_i)^2 + \\lambda \\theta r = 0\n$$\n给定条件 $\\frac{1}{n} \\sum_{i=1}^n \\varphi(x_i)^2 = 1$。将其代入方程得到：\n$$\n-\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) + \\theta + \\lambda \\theta r = 0\n$$\n解出 $\\theta$ 的估计量 $\\hat{\\theta}$：\n$$\n\\hat{\\theta}(1 + \\lambda r) = \\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i) \\implies \\hat{\\theta} = \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r}\n$$\n在点 $x_0$ 处的函数估计量为 $\\hat{f}(x_0) = \\hat{\\theta} \\varphi(x_0)$。\n\n**1) 逐点偏置与方差**\n\n估计量 $\\hat{f}(x_0)$ 的逐点偏置定义为 $\\text{Bias}(\\hat{f}(x_0)) = \\mathbb{E}[\\hat{f}(x_0)] - f_0(x_0)$。期望是关于噪声分布计算的。因为 $\\mathbb{E}[\\varepsilon_i] = 0$，我们使用 $\\mathbb{E}[y_i] = \\mathbb{E}[f_0(x_i) + \\varepsilon_i] = f_0(x_i)$。\n$$\n\\mathbb{E}[\\hat{f}(x_0)] = \\mathbb{E}\\left[ \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r} \\varphi(x_0) \\right] = \\frac{\\varphi(x_0)}{1 + \\lambda r} \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E}[y_i] \\varphi(x_i) = \\frac{\\varphi(x_0)}{1 + \\lambda r} \\frac{1}{n} \\sum_{i=1}^n f_0(x_i) \\varphi(x_i)\n$$\n我们定义离散内积 $\\langle g, h \\rangle_n = \\frac{1}{n} \\sum_{i=1}^n g(x_i)h(x_i)$。那么 $\\mathbb{E}[\\hat{f}(x_0)] = \\frac{\\langle f_0, \\varphi \\rangle_n}{1 + \\lambda r} \\varphi(x_0)$。\n偏置为：\n$$\n\\text{Bias}(\\hat{f}(x_0)) = \\frac{\\langle f_0, \\varphi \\rangle_n}{1 + \\lambda r} \\varphi(x_0) - f_0(x_0)\n$$\n逐点方差为 $\\text{Var}(\\hat{f}(x_0)) = \\text{Var}(\\hat{\\theta} \\varphi(x_0)) = \\varphi(x_0)^2 \\text{Var}(\\hat{\\theta})$。噪声变量 $\\varepsilon_i$ 是独立的，所以 $y_i$ 也是独立的，且 $\\text{Var}(y_i) = \\sigma^2$。\n$$\n\\text{Var}(\\hat{\\theta}) = \\text{Var}\\left( \\frac{\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)}{1 + \\lambda r} \\right) = \\frac{1}{(1 + \\lambda r)^2} \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\varphi(x_i)\\right)\n$$\n$$\n\\text{Var}(\\hat{\\theta}) = \\frac{1}{n^2 (1 + \\lambda r)^2} \\sum_{i=1}^n \\varphi(x_i)^2 \\text{Var}(y_i) = \\frac{\\sigma^2}{n^2 (1 + \\lambda r)^2} \\sum_{i=1}^n \\varphi(x_i)^2\n$$\n使用给定条件 $\\sum_{i=1}^n \\varphi(x_i)^2 = n$，我们得到：\n$$\n\\text{Var}(\\hat{\\theta}) = \\frac{n \\sigma^2}{n^2 (1 + \\lambda r)^2} = \\frac{\\sigma^2}{n (1 + \\lambda r)^2}\n$$\n所以，估计量的逐点方差为：\n$$\n\\text{Var}(\\hat{f}(x_0)) = \\varphi(x_0)^2 \\text{Var}(\\hat{\\theta}) = \\frac{\\sigma^2 \\varphi(x_0)^2}{n (1 + \\lambda r)^2}\n$$\n\n**2) 特定情况下的均方误差**\n\n首先，我们计算曲率权重 $r$：\n$$\n\\varphi(x) = \\sqrt{2}\\,\\sin(2\\pi k x)\n$$\n$$\n\\varphi'(x) = \\sqrt{2}\\,(2\\pi k)\\,\\cos(2\\pi k x)\n$$\n$$\n\\varphi''(x) = -\\sqrt{2}\\,(2\\pi k)^2\\,\\sin(2\\pi k x)\n$$\n$$\nr = \\int_0^1 \\big(\\varphi''(x)\\big)^2 \\, dx = \\int_0^1 \\left(-\\sqrt{2}\\,(2\\pi k)^2\\,\\sin(2\\pi k x)\\right)^2 \\, dx = 2(2\\pi k)^4 \\int_0^1 \\sin^2(2\\pi k x) \\, dx\n$$\n使用恒等式 $\\sin^2(\\alpha) = \\frac{1 - \\cos(2\\alpha)}{2}$：\n$$\nr = 2(16\\pi^4 k^4) \\int_0^1 \\frac{1 - \\cos(4\\pi k x)}{2} \\, dx = 16\\pi^4 k^4 \\left[ x - \\frac{\\sin(4\\pi k x)}{4\\pi k} \\right]_0^1 = 16\\pi^4 k^4 (1 - 0) = 16\\pi^4 k^4\n$$\n现在，我们特例分析信号 $f_0(x) = \\beta \\sin(2\\pi k x) = \\frac{\\beta}{\\sqrt{2}}\\varphi(x)$。离散内积为：\n$$\n\\langle f_0, \\varphi \\rangle_n = \\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{\\beta}{\\sqrt{2}}\\varphi(x_i)\\right) \\varphi(x_i) = \\frac{\\beta}{\\sqrt{2}} \\left( \\frac{1}{n} \\sum_{i=1}^n \\varphi(x_i)^2 \\right) = \\frac{\\beta}{\\sqrt{2}} \\cdot 1 = \\frac{\\beta}{\\sqrt{2}}\n$$\n我们在满足 $\\sin(2\\pi k x_0) = 1$ 的点 $x_0$ 进行评估。在该点, $\\varphi(x_0) = \\sqrt{2}\\sin(2\\pi k x_0) = \\sqrt{2}$ 且 $f_0(x_0) = \\beta\\sin(2\\pi k x_0) = \\beta$。\n将这些代入偏置和方差公式：\n$$\n\\text{Bias}(\\hat{f}(x_0)) = \\frac{\\beta/\\sqrt{2}}{1 + \\lambda r} (\\sqrt{2}) - \\beta = \\frac{\\beta}{1 + \\lambda r} - \\beta = \\frac{\\beta - \\beta(1+\\lambda r)}{1 + \\lambda r} = -\\frac{\\beta \\lambda r}{1 + \\lambda r}\n$$\n$$\n\\text{Var}(\\hat{f}(x_0)) = \\frac{\\sigma^2 (\\sqrt{2})^2}{n (1 + \\lambda r)^2} = \\frac{2\\sigma^2}{n (1 + \\lambda r)^2}\n$$\n逐点均方误差为 $\\text{MSE}(\\hat{f}(x_0)) = (\\text{Bias}(\\hat{f}(x_0)))^2 + \\text{Var}(\\hat{f}(x_0))$：\n$$\n\\text{MSE}(\\lambda) = \\left(-\\frac{\\beta \\lambda r}{1 + \\lambda r}\\right)^2 + \\frac{2\\sigma^2}{n (1 + \\lambda r)^2} = \\frac{\\beta^2 \\lambda^2 r^2}{(1 + \\lambda r)^2} + \\frac{2\\sigma^2}{n (1 + \\lambda r)^2} = \\frac{n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2}{n(1 + \\lambda r)^2}\n$$\n\n**3) $\\lambda$ 的最优值**\n\n为了找到最小化 MSE 的 $\\lambda$ 值，我们将 $\\text{MSE}(\\lambda)$ 对 $\\lambda$ 求导并令其为零。\n$$\n\\frac{d}{d\\lambda}\\text{MSE}(\\lambda) = \\frac{d}{d\\lambda} \\left[ \\frac{n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2}{n(1 + \\lambda r)^2} \\right]\n$$\n使用商法则，并注意因子 $1/n$ 是一个常数：\n$$\n\\frac{d}{d\\lambda}\\text{MSE}(\\lambda) = \\frac{1}{n} \\frac{(2n\\beta^2 \\lambda r^2)(1+\\lambda r)^2 - (n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2)(2(1+\\lambda r)r)}{(1+\\lambda r)^4} = 0\n$$\n对于 $\\lambda \\ge 0$，我们可以除以不为零的 $2(1+\\lambda r)$：\n$$\n(n\\beta^2 \\lambda r^2)(1+\\lambda r) - r(n\\beta^2 \\lambda^2 r^2 + 2\\sigma^2) = 0\n$$\n$$\nn\\beta^2 \\lambda r^2 + n\\beta^2 \\lambda^2 r^3 - n\\beta^2 \\lambda^2 r^3 - 2\\sigma^2 r = 0\n$$\n$$\nn\\beta^2 \\lambda r^2 - 2\\sigma^2 r = 0\n$$\n假设 $r \\neq 0$ (由于 $k$ 是正整数，该假设成立) 且 $\\beta \\neq 0$：\n$$\nn\\beta^2 \\lambda r = 2\\sigma^2 \\implies \\lambda = \\frac{2\\sigma^2}{n\\beta^2 r}\n$$\n二阶导数检验确认这是一个最小值。代入 $r = 16\\pi^4 k^4$ 的值：\n$$\n\\lambda = \\frac{2\\sigma^2}{n\\beta^2 (16\\pi^4 k^4)} = \\frac{\\sigma^2}{8 n \\beta^2 \\pi^4 k^4}\n$$\n这就是调节参数 $\\lambda$ 的最优值。", "answer": "$$\\boxed{\\frac{\\sigma^2}{8 n \\beta^2 \\pi^4 k^4}}$$", "id": "3180624"}, {"introduction": "在理解了正则化如何调控模型复杂度之后，下一个关键问题是：在什么情况下正则化变得至关重要？本练习将探讨一种常见且重要的情况：多重共线性，即输入特征之间高度相关。你将通过解析推导，对比一个正交设计和一个相关设计，来量化共线性如何导致普通最小二乘估计量的方差急剧膨胀，并验证岭回归（一种 $L_2$ 正则化）如何通过引入微小的偏差来有效稳定估计，从而提供一个更稳健的模型。[@problem_id:3180600]", "problem": "考虑标准线性模型 $Y = X \\beta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{n \\times p}$，真实参数向量 $\\beta \\in \\mathbb{R}^{p}$，噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$。对于固定的正则化强度 $\\lambda  0$，岭估计量为 $\\hat{\\beta}_{\\lambda} = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y$。对于一个新输入 $x_{0} \\in \\mathbb{R}^{p}$，定义预测 $\\hat{f}_{\\lambda}(x_{0}) = x_{0}^{\\top} \\hat{\\beta}_{\\lambda}$。在 $x_{0}$ 处的条件均值的均方预测误差为 $\\mathrm{MSE}(x_{0}) = \\mathbb{E}\\big[(\\hat{f}_{\\lambda}(x_{0}) - x_{0}^{\\top} \\beta)^{2}\\big]$，其中期望仅针对训练噪声 $\\varepsilon$ 计算。\n\n您将比较两种设计，它们具有相同的样本量 $n$、噪声水平 $\\sigma^{2}$ 且 $p = 2$：\n- 设计 A (正交)：$X^{\\top} X = n I_{2}$。\n- 设计 B (高度相关)：$X^{\\top} X = n \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$，其中相关系数 $\\rho \\in (0, 1)$。\n\n仅使用模型定义、岭估计量定义以及期望和方差的定义，首先在每种设计下推导预测偏差 $\\mathrm{Bias}(x_{0}) = \\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] - x_{0}^{\\top} \\beta$ 和预测方差 $\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0}))$ 的闭式表达式。然后，对于特定值 $n = 100$，$\\sigma^{2} = 1$，$\\lambda = 1$，$\\rho = 0.9$，$\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $x_{0} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$，计算比率\n$$\nR \\;=\\; \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})},\n$$\n其中 $\\mathrm{MSE}_{A}(x_{0})$ 和 $\\mathrm{MSE}_{B}(x_{0})$ 分别表示设计 A 和设计 B 下的均方预测误差。这里的 $\\mathrm{MSE}(x_{0})$ 指的是预测条件均值 $x_{0}^{\\top} \\beta$ 的误差，因此等于偏差的平方加上方差。\n\n提供您计算出的 $R$ 的最终数值，并四舍五入到四位有效数字。", "solution": "### 一般偏差和方差的推导\n首先，我们推导预测 $\\hat{f}_{\\lambda}(x_{0})$ 的偏差和方差的一般表达式。\n\n预测变量的期望为：\n$$\n\\mathbb{E}[\\hat{f}_{\\lambda}(x_{0})] = \\mathbb{E}[x_{0}^{\\top} \\hat{\\beta}_{\\lambda}] = x_{0}^{\\top} \\mathbb{E}[\\hat{\\beta}_{\\lambda}]\n$$\n由于 $\\mathbb{E}[Y] = \\mathbb{E}[X \\beta + \\varepsilon] = X \\beta$，估计量的期望为：\n$$\n\\mathbb{E}[\\hat{\\beta}_{\\lambda}] = \\mathbb{E}[(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y] = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} (X \\beta)\n$$\n那么，预测的偏差为：\n$$\n\\mathrm{Bias}(x_{0}) = x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X \\beta - x_{0}^{\\top} \\beta = x_{0}^{\\top} \\left[ (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - I_p \\right] \\beta\n$$\n使用恒等式 $I_p = (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p)$，我们简化括号中的项：\n$$\n(X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X - (X^{\\top} X + \\lambda I_{p})^{-1} (X^{\\top} X + \\lambda I_p) = -\\lambda (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\n因此，偏差为：\n$$\n\\mathrm{Bias}(x_{0}) = -\\lambda x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} \\beta\n$$\n预测的方差为：\n$$\n\\mathrm{Var}(x_{0}) = \\mathrm{Var}(\\hat{f}_{\\lambda}(x_{0})) = \\mathrm{Var}(x_{0}^{\\top} \\hat{\\beta}_{\\lambda}) = x_{0}^{\\top} \\mathrm{Var}(\\hat{\\beta}_{\\lambda}) x_{0}\n$$\n估计量的方差为：\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\mathrm{Var}((X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} Y) = (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} \\mathrm{Var}(Y) (X^{\\top})^{\\top} ((X^{\\top} X + \\lambda I_{p})^{-1})^{\\top}\n$$\n使用 $\\mathrm{Var}(Y) = \\sigma^2 I_n$ 以及 $(X^{\\top} X + \\lambda I_{p})^{-1}$ 是对称矩阵这一事实：\n$$\n\\mathrm{Var}(\\hat{\\beta}_{\\lambda}) = \\sigma^{2} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1}\n$$\n因此，预测方差为：\n$$\n\\mathrm{Var}(x_{0}) = \\sigma^{2} x_{0}^{\\top} (X^{\\top} X + \\lambda I_{p})^{-1} X^{\\top} X (X^{\\top} X + \\lambda I_{p})^{-1} x_{0}\n$$\n\n### 设计 A (正交) 的分析\n对于设计 A，$X^{\\top} X = n I_{2}$。\n项 $(X^{\\top} X + \\lambda I_{2})$ 变为 $n I_{2} + \\lambda I_{2} = (n+\\lambda)I_2$。其逆矩阵为 $\\frac{1}{n+\\lambda}I_2$。\n\n**设计 A 的偏差平方**：\n$$\n(\\mathrm{Bias}_{A}(x_{0}))^2 = \\left( -\\lambda x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) \\beta \\right)^2 = \\frac{\\lambda^2}{(n+\\lambda)^2} (x_{0}^{\\top} \\beta)^2\n$$\n根据给定值，$x_{0}^{\\top} \\beta = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + (-1) \\cdot 1 = 0$。\n因此，$(\\mathrm{Bias}_{A}(x_{0}))^2 = 0$。\n\n**设计 A 的方差**：\n$$\n\\mathrm{Var}_{A}(x_{0}) = \\sigma^{2} x_{0}^{\\top} \\left(\\frac{1}{n+\\lambda}I_2\\right) (n I_2) \\left(\\frac{1}{n+\\lambda}I_2\\right) x_{0} = \\frac{n \\sigma^2}{(n+\\lambda)^2} x_{0}^{\\top} x_{0}\n$$\n根据给定值，$x_{0}^{\\top} x_{0} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1^2 + (-1)^2 = 2$。\n所以，$\\mathrm{Var}_{A}(x_{0}) = \\frac{100 \\cdot 1}{(100+1)^2} \\cdot 2 = \\frac{200}{101^2} = \\frac{200}{10201}$。\n\n**设计 A 的均方误差**：\n$\\mathrm{MSE}_{A}(x_{0}) = (\\mathrm{Bias}_{A}(x_{0}))^2 + \\mathrm{Var}_{A}(x_{0}) = 0 + \\frac{200}{10201} = \\frac{200}{10201}$。\n\n### 设计 B (相关) 的分析\n对于设计 B，$X^{\\top} X = n \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$。让我们求这个矩阵的特征分解。\n矩阵 $\\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ 的特征值为 $1+\\rho$ 和 $1-\\rho$。\n对应的未归一化特征向量为 $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n因此，$X^{\\top} X$ 的特征值为 $\\lambda_1 = n(1+\\rho)$ 和 $\\lambda_2 = n(1-\\rho)$。\n归一化的特征向量为 $v_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ 和 $v_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n给定的向量为 $\\beta = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\sqrt{2} v_1$ 和 $x_0 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\sqrt{2} v_2$。它们与特征向量成比例。\n\n令 $U = \\begin{pmatrix} v_1  v_2 \\end{pmatrix}$ 为特征向量矩阵。那么 $X^{\\top}X = U D U^{\\top}$，其中 $D = \\mathrm{diag}(\\lambda_1, \\lambda_2)$。\n那么 $X^{\\top}X + \\lambda I_2 = U D U^{\\top} + \\lambda U U^{\\top} = U(D+\\lambda I_2)U^{\\top}$。\n其逆矩阵为 $(X^{\\top}X + \\lambda I_2)^{-1} = U(D+\\lambda I_2)^{-1}U^{\\top}$。\n\n**设计 B 的偏差平方**：\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} \\beta\n$$\n我们有 $x_0 = \\sqrt{2} v_2$ 和 $\\beta = \\sqrt{2} v_1$。\n$U^{\\top}x_0 = \\sqrt{2} U^{\\top}v_2 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$，以及 $U^{\\top}\\beta = \\sqrt{2} U^{\\top}v_1 = \\sqrt{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$。\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -\\lambda (\\sqrt{2}\\begin{pmatrix} 0  1 \\end{pmatrix}) (D+\\lambda I_2)^{-1} (\\sqrt{2}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}) = -2\\lambda \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\lambda_1+\\lambda}  0 \\\\ 0  \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\mathrm{Bias}_{B}(x_{0}) = -2\\lambda \\begin{pmatrix} 0  \\frac{1}{\\lambda_2+\\lambda} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0\n$$\n因此，$(\\mathrm{Bias}_{B}(x_{0}))^2 = 0$。\n\n**设计 B 的方差**：\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} U D U^{\\top} U(D+\\lambda I_2)^{-1}U^{\\top} x_{0}\n$$\n这可以简化为：\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 x_{0}^{\\top} U D(D+\\lambda I_2)^{-2} U^{\\top} x_{0}\n$$\n使用 $U^{\\top}x_0 = \\sqrt{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$：\n$$\n\\mathrm{Var}_{B}(x_{0}) = \\sigma^2 (\\sqrt{2}\\begin{pmatrix} 0  1 \\end{pmatrix}) \\begin{pmatrix} \\frac{\\lambda_1}{(\\lambda_1+\\lambda)^2}  0 \\\\ 0  \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2} \\end{pmatrix} (\\sqrt{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}) = 2\\sigma^2 \\frac{\\lambda_2}{(\\lambda_2+\\lambda)^2}\n$$\n现在，代入数值：\n$\\lambda_2 = n(1-\\rho) = 100(1-0.9) = 100(0.1) = 10$。\n$\\sigma^2 = 1$，$\\lambda=1$。\n$$\n\\mathrm{Var}_{B}(x_{0}) = 2(1) \\frac{10}{(10+1)^2} = \\frac{20}{11^2} = \\frac{20}{121}\n$$\n\n**设计 B 的均方误差**：\n$\\mathrm{MSE}_{B}(x_{0}) = (\\mathrm{Bias}_{B}(x_{0}))^2 + \\mathrm{Var}_{B}(x_{0}) = 0 + \\frac{20}{121} = \\frac{20}{121}$。\n\n### 比率 R 的计算\n比率 $R$ 是两个 MSE 值的商。\n$$\nR = \\frac{\\mathrm{MSE}_{B}(x_{0})}{\\mathrm{MSE}_{A}(x_{0})} = \\frac{20/121}{200/10201} = \\frac{20}{121} \\times \\frac{10201}{200} = \\frac{10201}{10 \\times 121} = \\frac{10201}{1210}\n$$\n将数值四舍五入到四位有效数字：\n$R \\approx 8.4305785...$\n四舍五入到四位有效数字得到 $8.431$。", "answer": "$$\n\\boxed{8.431}\n$$", "id": "3180600"}, {"introduction": "前面的练习主要关注参数模型，但对于现实世界中更复杂的信号，一个单一的、全局性的模型复杂度度量可能并不足够。这个计算模拟练习将引导你从参数模型迈向非参数模型，并处理一个“不均匀”光滑的函数。你将亲手构建一个模拟实验，对比使用全局带宽的核回归估计器与一个能够局部调整其平滑度的自适应估计器，从而直观地体验并理解在输入空间的不同区域动态管理偏差-方差权衡的强大之处。[@problem_id:3180581]", "problem": "考虑非参数回归设置，其中输入 $x \\in [0,1]$ 从 $[0,1]$ 上的均匀分布中独立同分布地抽取，输出 $y$ 由 $y = f(x) + \\varepsilon$ 给出，其中 $\\varepsilon$ 是服从均值为 $0$、方差为 $\\sigma^2$ 的正态分布的独立噪声。目标函数 $f(x)$ 表现出空间变化的平滑性，对所有 $x \\in [0,1]$ 定义为\n$$\nf(x) = \\sin(2\\pi x) + 0.5 \\exp\\!\\big(-200(x-0.75)^2\\big).\n$$\n目标是设计一个模拟，以研究核回归中的空间自适应带宽选择如何与单一全局带宽相比，能够平衡局部偏差和方差。\n\n仅从期望平方误差的定义和上述模型假设出发，推导并实现一个算法，在固定的评估点 $x_{\\text{smooth}} = 0.05$ 和 $x_{\\text{sharp}} = 0.75$ 处，估计 $f(x)$ 的两个估计量的均方误差 (MSE)、偏差平方和方差：\n- 一个全局带宽的 Nadaraya–Watson 估计量，使用高斯核 $K(u) = \\exp\\!\\big(-u^2/2\\big)$，其单一带宽 $h$ 通过在候选带宽集上最小化留一法交叉验证 (LOO-CV) 误差来选择。\n- 一个空间自适应的 Nadaraya–Watson 估计量，它在每个评估点 $x^\\star$ 处，通过最小化 $x^\\star$ 周围的局部化 LOO-CV 误差来选择一个局部带宽 $h(x^\\star)$。使用引导宽度为 $w_{\\text{pilot}}$ 的高斯加权窗口来局部化交叉验证目标。\n\n在评估点 $x^\\star$ 处，带宽为 $h$ 的 Nadaraya–Watson 估计量定义为\n$$\n\\widehat{f}_h(x^\\star) = \\frac{\\sum_{i=1}^n K\\!\\left(\\frac{x^\\star - x_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\!\\left(\\frac{x^\\star - x_i}{h}\\right)},\n$$\n其中 $(x_i,y_i)$ 是训练样本。对于全局带宽选择，在训练输入 $x_j$ 处，带宽为 $h$ 的 LOO-CV 预测为\n$$\n\\widehat{y}_{-j,h}(x_j) = \\frac{\\sum_{i \\neq j} K\\!\\left(\\frac{x_j - x_i}{h}\\right) y_i}{\\sum_{i \\neq j} K\\!\\left(\\frac{x_j - x_i}{h}\\right)},\n$$\n并且全局 LOO-CV 目标是所有 $j$ 的残差平方的平均值。对于在评估点 $x^\\star$ 处的空间自适应选择，定义局部权重\n$$\nw_j(x^\\star) = \\exp\\!\\left(-\\frac{1}{2}\\left(\\frac{x_j - x^\\star}{w_{\\text{pilot}}}\\right)^2\\right),\n$$\n并最小化 $x^\\star$ 附近的残差平方的加权平均值，\n$$\nL(h; x^\\star) = \\frac{\\sum_{j=1}^n w_j(x^\\star)\\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2}{\\sum_{j=1}^n w_j(x^\\star)}.\n$$\n\n您必须通过蒙特卡洛模拟来估计在 $x_{\\text{smooth}}$ 和 $x_{\\text{sharp}}$ 处的 MSE、偏差平方和方差：对于每个参数集，抽取 $n$ 个训练样本，拟合两个估计量，并在 $R$ 次独立重复实验中记录 $\\widehat{f}(x^\\star)$。使用以下定义\n- $x^\\star$ 处的偏差平方：$\\left(\\mathbb{E}[\\widehat{f}(x^\\star)] - f(x^\\star)\\right)^2$，\n- $x^\\star$ 处的方差：$\\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - \\mathbb{E}[\\widehat{f}(x^\\star)]\\right)^2\\right]$，\n- $x^\\star$ 处的均方误差：$\\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - f(x^\\star)\\right)^2\\right]$，\n其中期望值通过在 $R$ 次重复实验上的平均值来近似。\n\n对于每个参数集，报告在两个评估点处的局部 MSE 改进，定义为 $I(x^\\star) = \\text{MSE}_{\\text{global}}(x^\\star) - \\text{MSE}_{\\text{adaptive}}(x^\\star)$，其中 $x^\\star \\in \\{x_{\\text{smooth}}, x_{\\text{sharp}}\\}$。正值表示自适应估计量相对于全局估计量减少了 MSE。\n\n为以下参数值 $(n, \\sigma, \\mathcal{H}, w_{\\text{pilot}}, R, \\text{seed})$ 的测试套件实现模拟，其中 $\\mathcal{H}$ 是候选带宽集：\n- 测试用例 1：$(150, 0.1, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 12345)$。\n- 测试用例 2：$(60, 0.1, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 23456)$。\n- 测试用例 3：$(150, 0.4, \\{0.02, 0.04, 0.08, 0.16\\}, 0.15, 150, 34567)$。\n- 测试用例 4：$(150, 0.1, \\{0.01, 0.02, 0.04\\}, 0.10, 120, 45678)$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。该列表必须按测试用例排序，在每个测试用例内部，按评估点 $(x_{\\text{smooth}}, x_{\\text{sharp}})$ 排序。每个条目必须是四舍五入到六位小数的浮点数。例如，输出必须是以下形式\n$$\n[\\Delta_1(x_{\\text{smooth}}), \\Delta_1(x_{\\text{sharp}}), \\Delta_2(x_{\\text{smooth}}), \\Delta_2(x_{\\text{sharp}}), \\Delta_3(x_{\\text{smooth}}), \\Delta_3(x_{\\text{sharp}}), \\Delta_4(x_{\\text{smooth}}), \\Delta_4(x_{\\text{sharp}})],\n$$\n其中 $\\Delta_k(x^\\star)$ 表示测试用例 $k$ 的 MSE 改进 $I(x^\\star)$。此问题不涉及任何物理单位或角度，所有输出必须是指定格式的浮点数。程序必须是完全自包含的，且不得需要任何用户输入或外部文件。使用上面定义的高斯核，并确保所有计算在数值上是稳定的（例如，避免在留一法分母中出现除以零的情况）。", "solution": "该模拟旨在比较全局带宽和空间自适应带宽的核回归估计器。问题的核心在于偏差-方差权衡，这两种估计量对此的处理方式不同。目标函数 $f(x) = \\sin(2\\pi x) + 0.5 \\exp(-200(x-0.75)^2)$ 被设计为具有不同平滑度的区域。一个全局带宽 $h$ 必须进行折衷：大的 $h$ 会减少方差，但在 $x=0.75$ 附近的尖峰区域（过平滑）会产生大的偏差，而小的 $h$ 会更好地捕捉峰值（偏差较小），但在平滑的正弦区域（欠平滑）会遭受高方差。空间自适应估计量旨在通过在尖峰附近选择较小的带宽，在较平滑的区域选择较大的带宽来解决这个问题，从而实现更好的局部偏差-方差平衡。\n\n模拟将按以下结构进行：\n1.  **数据生成**：对于 $R$ 次蒙特卡洛重复实验中的每一次，生成一个训练集 $\\{(x_i, y_i)\\}_{i=1}^n$。输入 $x_i$ 从均匀分布 $U[0, 1]$ 中抽取。输出根据模型 $y_i = f(x_i) + \\varepsilon_i$ 生成，其中 $\\varepsilon_i$ 从正态分布 $N(0, \\sigma^2)$ 中采样。\n\n2.  **带宽选择**：每次重复实验中的关键步骤是从候选集 $\\mathcal{H}$ 中选择带宽参数 $h$。\n    *   **留一法交叉验证 (LOO-CV) 预计算**：为提高效率，对于每个候选带宽 $h \\in \\mathcal{H}$，我们首先计算所有训练点 $x_j$, $j=1, \\dots, n$ 的 LOO-CV 预测。在 $x_j$ 处的 LOO-CV 预测由下式给出\n    $$\n    \\widehat{y}_{-j,h}(x_j) = \\frac{\\sum_{i \\neq j} K\\left(\\frac{x_j - x_i}{h}\\right) y_i}{\\sum_{i \\neq j} K\\left(\\frac{x_j - x_i}{h}\\right)}\n    $$\n    其中 $K(u) = \\exp(-u^2/2)$ 是高斯核。直接实现计算成本很高。我们可以使用全和来重新表述它。设 $S_y(x_j, h) = \\sum_{i=1}^n K\\left(\\frac{x_j - x_i}{h}\\right) y_i$ 且 $S_1(x_j, h) = \\sum_{i=1}^n K\\left(\\frac{x_j - x_i}{h}\\right)$。由于 $K(0)=1$，留一法预测器可以高效地计算为：\n    $$\n    \\widehat{y}_{-j,h}(x_j) = \\frac{S_y(x_j, h) - y_j}{S_1(x_j, h) - 1}\n    $$\n    此计算将对所有 $j \\in \\{1,\\dots,n\\}$ 和所有 $h \\in \\mathcal{H}$ 执行。\n\n    *   **全局带宽选择**：选择全局带宽 $h_{\\text{global}}$ 以最小化整个训练集上的均方 LOO-CV 误差：\n    $$\n    h_{\\text{global}} = \\arg\\min_{h \\in \\mathcal{H}} \\left\\{ \\frac{1}{n} \\sum_{j=1}^n \\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2 \\right\\}\n    $$\n\n    *   **空间自适应带宽选择**：对于每个评估点 $x^\\star$，通过最小化加权 LOO-CV 误差来选择局部带宽 $h_{\\text{adaptive}}(x^\\star)$。权重强调靠近 $x^\\star$ 的训练点。目标函数为：\n    $$\n    L(h; x^\\star) = \\frac{\\sum_{j=1}^n w_j(x^\\star)\\left(y_j - \\widehat{y}_{-j,h}(x_j)\\right)^2}{\\sum_{j=1}^n w_j(x^\\star)}, \\quad \\text{其中} \\quad w_j(x^\\star) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{x_j - x^\\star}{w_{\\text{pilot}}}\\right)^2\\right)\n    $$\n    局部带宽则为 $h_{\\text{adaptive}}(x^\\star) = \\arg\\min_{h \\in \\mathcal{H}} L(h; x^\\star)$。此过程对 $x^\\star = x_{\\text{smooth}} = 0.05$ 和 $x^\\star = x_{\\text{sharp}} = 0.75$ 分别执行。\n\n3.  **预测**：一旦为给定的重复实验选择了带宽，就使用 Nadaraya-Watson 估计量计算两个评估点处的函数估计值：\n    $$\n    \\widehat{f}_h(x^\\star) = \\frac{\\sum_{i=1}^n K\\left(\\frac{x^\\star - x_i}{h}\\right) y_i}{\\sum_{i=1}^n K\\left(\\frac{x^\\star - x_i}{h}\\right)}\n    $$\n    对于全局估计量，我们对 $x_{\\text{smooth}}$ 和 $x_{\\text{sharp}}$ 都使用 $h = h_{\\text{global}}$。对于自适应估计量，我们在 $x_{\\text{smooth}}$ 处预测时使用 $h = h_{\\text{adaptive}}(x_{\\text{smooth}})$，在 $x_{\\text{sharp}}$ 处预测时使用 $h = h_{\\text{adaptive}}(x_{\\text{sharp}})$。为保证数值稳定性，在分母上加上一个很小的常数以防止除以零。\n\n4.  **误差估计**：数据生成、带宽选择和预测的过程重复 $R$ 次。这为每个评估点上的每个估计量产生 $R$ 个估计值。设 $\\{\\widehat{f}^{(r)}(x^\\star)\\}_{r=1}^R$ 是估计值的集合。然后，均方误差 (MSE) 通过这些重复实验的平均值来近似：\n    $$\n    \\text{MSE}(x^\\star) = \\mathbb{E}\\left[\\left(\\widehat{f}(x^\\star) - f(x^\\star)\\right)^2\\right] \\approx \\frac{1}{R} \\sum_{r=1}^R \\left(\\widehat{f}^{(r)}(x^\\star) - f(x^\\star)\\right)^2\n    $$\n    这对全局和自适应估计量在两个评估点处都进行计算。\n\n5.  **最终输出**：每个测试用例的最终结果是每个点的 MSE 改进，定义为 $I(x^\\star) = \\text{MSE}_{\\text{global}}(x^\\star) - \\text{MSE}_{\\text{adaptive}}(x^\\star)$。正值表示自适应方法提供了更低的 MSE。\n\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Monte Carlo simulation to compare global vs. spatially adaptive\n    bandwidth selection for Nadaraya-Watson kernel regression.\n    \"\"\"\n    test_cases = [\n        (150, 0.1, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 12345),\n        (60, 0.1, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 23456),\n        (150, 0.4, [0.02, 0.04, 0.08, 0.16], 0.15, 150, 34567),\n        (150, 0.1, [0.01, 0.02, 0.04], 0.10, 120, 45678),\n    ]\n\n    all_results = []\n    \n    # Define the true function\n    def f_true(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.exp(-200 * (x - 0.75)**2)\n\n    # Define the Gaussian kernel\n    def gaussian_kernel(u):\n        return np.exp(-0.5 * u**2)\n\n    # Evaluation points\n    x_eval = np.array([0.05, 0.75]).reshape(-1, 1) # smooth, sharp\n    f_at_eval = f_true(x_eval)\n\n    # Numerical stability constant\n    STABILITY_EPS = 1e-12\n\n    for case in test_cases:\n        n, sigma, h_candidates, w_pilot, R, seed = case\n        h_candidates = np.array(h_candidates)\n        num_h = len(h_candidates)\n\n        rng = np.random.default_rng(seed)\n\n        preds_global = np.zeros((R, 2))\n        preds_adaptive = np.zeros((R, 2))\n\n        for r in range(R):\n            # 1. Generate data\n            x_train = rng.uniform(0, 1, size=(n, 1))\n            noise = rng.normal(0, sigma, size=(n, 1))\n            y_train = f_true(x_train) + noise\n\n            # 2. Pre-compute LOO predictions for all candidate bandwidths\n            # Pairwise differences for LOO\n            x_diff_loo = x_train - x_train.T\n            all_y_hat_loo = np.zeros((n, num_h))\n\n            for i, h in enumerate(h_candidates):\n                # Kernel matrix for LOO\n                K_mat_loo = gaussian_kernel(x_diff_loo / h)\n                \n                # Full sums S_y and S_1\n                S_y = K_mat_loo @ y_train\n                S_1 = K_mat_loo.sum(axis=1, keepdims=True)\n                \n                # Efficient LOO calculation\n                loo_denom = S_1 - 1.0\n                # Handle numerically unstable denominators\n                loo_denom[np.abs(loo_denom)  STABILITY_EPS] = STABILITY_EPS\n                \n                y_hat_loo_h = (S_y - y_train) / loo_denom\n                all_y_hat_loo[:, i] = y_hat_loo_h.flatten()\n\n            # 3. Global bandwidth selection\n            squared_errors_loo = (y_train - all_y_hat_loo)**2\n            global_cv_scores = squared_errors_loo.mean(axis=0)\n            best_h_idx_global = np.argmin(global_cv_scores)\n            h_global = h_candidates[best_h_idx_global]\n\n            # 4. Global prediction\n            diff_pred_global = x_eval - x_train.T\n            K_pred_global = gaussian_kernel(diff_pred_global / h_global)\n            pred_denom_g = K_pred_global.sum(axis=1, keepdims=True)\n            pred_denom_g[pred_denom_g  STABILITY_EPS] = STABILITY_EPS\n            f_hat_global = (K_pred_global @ y_train) / pred_denom_g\n            preds_global[r, :] = f_hat_global.flatten()\n\n            # 5. Spatially adaptive bandwidth selection and prediction\n            for j, x_star in enumerate(x_eval):\n                # Localization weights\n                weights_loc = np.exp(-0.5 * ((x_train - x_star) / w_pilot)**2)\n                weights_sum = weights_loc.sum()\n                \n                # Localized CV scores\n                weighted_sq_errors = weights_loc * squared_errors_loo\n                local_cv_scores = weighted_sq_errors.sum(axis=0) / weights_sum\n                \n                best_h_idx_adaptive = np.argmin(local_cv_scores)\n                h_adaptive = h_candidates[best_h_idx_adaptive]\n                \n                # Adaptive prediction\n                diff_pred_adaptive = x_star - x_train.T\n                K_pred_adaptive = gaussian_kernel(diff_pred_adaptive / h_adaptive)\n                pred_denom_a = K_pred_adaptive.sum()\n                if pred_denom_a  STABILITY_EPS: pred_denom_a = STABILITY_EPS\n                f_hat_adaptive = (K_pred_adaptive @ y_train) / pred_denom_a\n                preds_adaptive[r, j] = f_hat_adaptive.flatten()[0]\n\n        # 6. Calculate MSEs and improvement\n        mse_global = np.mean((preds_global - f_at_eval.T)**2, axis=0)\n        mse_adaptive = np.mean((preds_adaptive - f_at_eval.T)**2, axis=0)\n\n        improvement = mse_global - mse_adaptive\n        all_results.extend(improvement)\n\n    # Final print statement\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# To generate the answer for the XML\n# solve()\n```", "answer": "[-0.000329,0.015093,-0.001646,0.021020,0.000305,0.038164,-0.000574,0.004403]", "id": "3180581"}]}