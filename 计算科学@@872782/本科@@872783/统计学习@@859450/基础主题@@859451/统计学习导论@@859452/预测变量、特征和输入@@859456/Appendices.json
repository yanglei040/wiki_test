{"hands_on_practices": [{"introduction": "原始预测变量的形式并非总是最优的。通过应用诸如 Box-Cox 变换之类的非线性变换，我们可以改善其统计特性（如偏度）或增强其与响应变量之间的线性关系。本练习 [@problem_id:3160298] 将通过一个编程任务，探讨如何系统地评估一系列变换，并比较旨在减少偏度的最佳变换与旨在最大化模型拟合优度的最佳变换，揭示特征工程中这两个目标之间的权衡。", "problem": "您的任务是编程研究单个正预测变量的非线性单调变换如何影响其偏度和线性模型拟合的质量。您将对预测变量（特征、输入）应用 Box–Cox 变换族，并在一个变换参数网格上比较不同结果。\n\n任务的基本基础：\n- 定义正输入的 Box–Cox 变换如下。对于任何 $x \\in \\mathbb{R}_{+}$ 和参数 $\\lambda \\in \\mathbb{R}$，定义\n$$\ng_{\\lambda}(x) = \n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda}  \\text{如果 } \\lambda \\ne 0, \\\\\n\\log(x)  \\text{如果 } \\lambda = 0.\n\\end{cases}\n$$\n- 实值样本 $z_{1},\\dots,z_{n}$ 的样本偏度 $\\gamma_{1}(z)$ 定义为\n$$\n\\gamma_{1}(z) = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{3}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{2}\\right)^{3/2}},\n\\quad \\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n} z_{i}.\n$$\n- 对于一个带截距的线性模型，\n$$\ny_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\stackrel{\\text{iid}}{\\sim} \\mathcal{N}(0,\\sigma^{2}),\n$$\n普通最小二乘（OLS）拟合产生残差 $r_{i}$ 和残差平方和 $\\text{RSS}=\\sum_{i=1}^{n} r_{i}^{2}$。在高斯误差模型下，其中 $\\hat{\\sigma}^{2}=\\text{RSS}/n$，最大化对数似然为\n$$\n\\ell = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\hat{\\sigma}^{2}\\right)+1\\right).\n$$\n\n您的程序必须为每个指定的测试用例执行以下操作：\n1. 数据生成。通过以下方式生成一个正预测变量 $X_{i}$ 和一个响应 $Y_{i}$\n$$\nX_{i} \\sim \\text{LogNormal}(\\mu,\\sigma_{X}^{2}), \\quad \nY_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2}),\n$$\n对于 $i=1,\\dots,n$，使用指定的随机种子以保证可复现性。所有随机抽取必须是独立同分布的。\n\n2. 变换网格。考虑候选集\n$$\n\\Lambda = \\left\\{-2.0,-1.0,-0.5,-0.25,0.0,0.25,0.5,1.0,2.0\\right\\}.\n$$\n\n3. 偏度均衡化。对于每个 $\\lambda \\in \\Lambda$，计算 $Z_{i}^{(\\lambda)}=g_{\\lambda}(X_{i})$ 并计算绝对偏度 $|\\gamma_{1}(Z^{(\\lambda)})|$。令 $\\hat{\\lambda}_{\\text{skew}}$ 为在 $\\Lambda$ 上的任意一个最小化子。如果出现平局，选择按升序扫描 $\\Lambda$ 时遇到的第一个最小化子。\n\n4. 模型拟合最优化。对于每个 $\\lambda \\in \\Lambda$，拟合带截距的 OLS 模型 $Y_{i}=\\beta_{0}^{(\\lambda)}+\\beta_{1}^{(\\lambda)} Z_{i}^{(\\lambda)}+\\varepsilon_{i}^{(\\lambda)}$，计算残差平方和 $\\text{RSS}(\\lambda)$ 以及相应的最大化对数似然\n$$\n\\ell(\\lambda) = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\cdot \\frac{\\text{RSS}(\\lambda)}{n}\\right) + 1\\right).\n$$\n令 $\\hat{\\lambda}_{\\text{fit}}$ 为 $\\ell(\\lambda)$ 在 $\\Lambda$ 上的任意一个最大化子。如果出现平局，选择按升序扫描 $\\Lambda$ 时遇到的第一个最大化子。\n\n5. 相对于恒等变换的改进。定义恒等变换为 $\\lambda=1.0$。令 $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$。定义一个布尔改进指示符 $I$，当且仅当 $\\Delta_{\\text{skew}} > 0$ 时为真。\n\n6. 每个测试用例的输出。对于每个测试用例，输出列表 $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$，其中两个 $\\lambda$ 值和 $\\Delta_{\\text{skew}}$ 必须四舍五入到小数点后四位。布尔值 $I$ 必须是 true 或 false。\n\n测试套件：\n使用以下四个测试用例。对于每个用例，使用指定的参数 $(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon})$：\n- 用例 1：$(0,\\,400,\\,0.0,\\,1.0,\\,1.0,\\,2.0,\\,0.0,\\,1.0)$。\n- 用例 2：$(1,\\,400,\\,0.0,\\,0.25,\\,-0.5,\\,1.5,\\,1.0,\\,1.0)$。\n- 用例 3：$(2,\\,400,\\,0.0,\\,1.25,\\,0.0,\\,1.0,\\,0.5,\\,1.5)$。\n- 用例 4：$(3,\\,400,\\,0.5,\\,1.0,\\,0.5,\\,1.0,\\,-0.5,\\,1.0)$。\n\n不涉及角度单位。本问题中没有物理单位。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个逗号分隔的列表，列表内容为四个用例各自的结果，每个结果本身是一个 $[\\hat{\\lambda}_{\\text{skew}},\\hat{\\lambda}_{\\text{fit}},I,\\Delta_{\\text{skew}}]$ 形式的列表，并且外层列表用方括号括起来。例如，包含两个用例的输出应如下所示：\n$[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$。\n每个内部列表中的所有三个实值条目在文本输出中必须四舍五入到小数点后四位。", "solution": "用户想要研究 Box-Cox 变换对预测变量的影响。我将首先验证问题陈述。\n\n### 第 1 步：提取给定信息\n\n- **Box–Cox 变换**：对于一个正输入 $x \\in \\mathbb{R}_{+}$ 和一个参数 $\\lambda \\in \\mathbb{R}$，该变换定义为：\n$$\ng_{\\lambda}(x) = \n\\begin{cases}\n\\dfrac{x^{\\lambda}-1}{\\lambda}  \\text{如果 } \\lambda \\ne 0, \\\\\n\\log(x)  \\text{如果 } \\lambda = 0.\n\\end{cases}\n$$\n\n- **样本偏度**：对于一个样本 $z = \\{z_{1}, \\dots, z_{n}\\}$，样本偏度为：\n$$\n\\gamma_{1}(z) = \\frac{\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{3}}{\\left(\\frac{1}{n}\\sum_{i=1}^{n}\\left(z_{i}-\\bar{z}\\right)^{2}\\right)^{3/2}}, \\quad \\text{其中 } \\bar{z}=\\frac{1}{n}\\sum_{i=1}^{n} z_{i}.\n$$\n\n- **线性模型与对数似然**：对于一个简单线性回归模型 $y_{i} = \\beta_{0} + \\beta_{1} z_{i} + \\varepsilon_{i}$，其中误差项 $\\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ 是独立同分布的高斯噪声，OLS下的最大化对数似然为：\n$$\n\\ell = -\\frac{n}{2}\\left(\\log\\left(2\\pi \\hat{\\sigma}^{2}\\right)+1\\right),\n$$\n其中 $\\hat{\\sigma}^{2}=\\text{RSS}/n$ 是误差方差的最大似然估计，$\\text{RSS}$ 是残差平方和。\n\n- **数据生成模型**：\n$$\nX_{i} \\sim \\text{LogNormal}(\\mu,\\sigma_{X}^{2}), \\quad \nY_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}, \\quad \\varepsilon_{i} \\sim \\mathcal{N}(0,\\sigma_{\\varepsilon}^{2})\n$$\n对于 $i=1,\\dots,n$，使用指定的随机种子。\n\n- **候选变换参数**：\n$$\n\\Lambda = \\left\\{-2.0,-1.0,-0.5,-0.25,0.0,0.25,0.5,1.0,2.0\\right\\}\n$$\n\n- **流程**：\n    1.  **偏度均衡化**：找到 $\\hat{\\lambda}_{\\text{skew}} \\in \\Lambda$ 使样本绝对偏度 $|\\gamma_{1}(g_{\\lambda}(X))|$ 最小化。\n    2.  **模型拟合最优化**：找到 $\\hat{\\lambda}_{\\text{fit}} \\in \\Lambda$ 使模型 $Y \\sim g_{\\lambda}(X)$ 的对数似然 $\\ell(\\lambda)$ 最大化。\n    3.  **平局处理**：对于两个优化过程，选择按升序扫描 $\\Lambda$ 时遇到的第一个值。\n    4.  **相对于恒等变换的改进**：计算 $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$ 和一个布尔指示符 $I = (\\Delta_{\\text{skew}} > 0)$。\n\n- **输出**：对于每个测试用例，输出一个列表 $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$，浮点数四舍五入到小数点后四位。\n\n- **测试用例**：\n    - 用例 1：$(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (0, 400, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0)$。\n    - 用例 2：$(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (1, 400, 0.0, 0.25, -0.5, 1.5, 1.0, 1.0)$。\n    - 用例 3：$(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (2, 400, 0.0, 1.25, 0.0, 1.0, 0.5, 1.5)$。\n    - 用例 4：$(\\text{seed},n,\\mu,\\sigma_{X},\\beta_{0},\\beta_{1},\\lambda^{\\star},\\sigma_{\\varepsilon}) = (3, 400, 0.5, 1.0, 0.5, 1.0, -0.5, 1.0)$。\n\n### 第 2 步：使用提取的给定信息进行验证\n\n问题是连贯且定义明确的。\n- **科学基础**：该问题建立在统计建模和数据分析的基本和标准概念之上，包括 Box-Cox 变换、线性回归、偏度和基于似然的模型评估。数据模拟协议是研究统计方法的常用方法。\n- **良态**：操作顺序明确。使用固定的随机种子确保了可复现性。对最优 $\\lambda$ 值的搜索是在一个有限的离散集合 $\\Lambda$ 上进行的，并且明确的平局处理规则保证了每个测试用例都有唯一的解。\n- **客观性**：所有定义和任务都以数学精度陈述，没有主观解释的余地。\n- **缺陷分析**：\n    1.  **科学/事实不准确**：无。所有公式和概念都是统计学中的标准内容。\n    2.  **不可形式化/不相关**：该问题完全可以形式化，并且与其在统计学习中陈述的主题高度相关。\n    3.  **不完整/矛盾**：问题说明是完整的，提供了所有必要的参数、数据生成过程、公式和评估标准。没有矛盾之处。\n    4.  **不切实际/不可行**：指定的参数对于模拟研究是合理的。从对数正态分布生成 $X$ 确保了 $X>0$，使得 Box-Cox 变换定义良好。\n    5.  **病态/结构不良**：无。问题结构逻辑清晰，能够导出一个唯一的、有意义的解。\n    6.  **伪深刻/琐碎**：该问题需要将几个非平凡的统计计算集成到一个模拟框架中，以探索特征工程中一个概念上重要的权衡。这是一项实质性的任务。\n    7.  **超出科学可验证范围**：给定随机种子，结果是计算上确定且可验证的。\n\n### 第 3 步：结论与行动\n问题有效。我将继续构建解决方案。\n\n该解决方案将作为一个单一的 Python 脚本实现。对于问题中定义的每个测试用例，该脚本将执行以下步骤序列：\n1.  使用指定的种子初始化随机数生成器以保证可复现性。\n2.  从对数正态分布 $\\text{LogNormal}(\\mu, \\sigma_X^2)$ 生成预测变量数据 $X$，并从正态分布 $\\mathcal{N}(0, \\sigma_\\varepsilon^2)$ 生成噪声项 $\\varepsilon$。参数 $\\mu$、$\\sigma_X$ 和 $\\sigma_\\varepsilon$ 在每个测试用例中提供。\n3.  使用真实的底层关系构建响应变量 $Y$：$Y_{i} = \\beta_{0} + \\beta_{1}\\, g_{\\lambda^{\\star}}(X_{i}) + \\varepsilon_{i}$。\n4.  遍历变换参数的候选集 $\\Lambda$。对于每个 $\\lambda \\in \\Lambda$：\n    a. 对预测变量应用 Box-Cox 变换：$Z^{(\\lambda)} = g_{\\lambda}(X)$。\n    b. 使用提供的公式计算变换后预测变量的样本偏度 $\\gamma_{1}(Z^{(\\lambda)})$。存储该偏度的绝对值。\n    c. 使用普通最小二乘法（OLS）拟合 $Y$ 对 $Z^{(\\lambda)}$ 的简单线性回归模型以获得系数估计。\n    d. 根据 OLS 拟合结果，计算残差平方和 $\\text{RSS}(\\lambda)$。\n    e. 使用 $\\text{RSS}(\\lambda)$ 根据给定公式计算最大化对数似然 $\\ell(\\lambda)$。\n5.  遍历完所有 $\\lambda \\in \\Lambda$ 后，通过找到使存储的绝对偏度值最小化的 $\\lambda$ 来确定 $\\hat{\\lambda}_{\\text{skew}}$。指定的平局处理规则（升序中的第一个）通过使用 `numpy.argmin` 来处理。\n6.  类似地，通过找到使存储的对数似然值最大化的 $\\lambda$ 来确定 $\\hat{\\lambda}_{\\text{fit}}$。平局处理规则通过使用 `numpy.argmax` 来处理。\n7.  计算改进度量 $\\Delta_{\\text{skew}} = \\ell(\\hat{\\lambda}_{\\text{skew}}) - \\ell(1.0)$。值 $\\ell(1.0)$ 对应于使用类恒等变换的模型的对数似然。\n8.  定义布尔指示符 $I$ 为真（如果 $\\Delta_{\\text{skew}} > 0$），否则为假。\n9.  为测试用例组装最终结果，即列表 $[\\hat{\\lambda}_{\\text{skew}}, \\hat{\\lambda}_{\\text{fit}}, I, \\Delta_{\\text{skew}}]$，确保数值四舍五入到小数点后四位。\n10. 处理完所有测试用例后，结果被格式化为列表的列表形式的单个字符串，并打印到标准输出。\n将为 Box-Cox 变换和其他重复计算定义辅助函数，以确保代码的清晰性和正确性。将使用 `scipy.stats.skew` 函数计算偏度，因为它正确实现了问题陈述中指定的有偏估计量。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import skew\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the final result.\n    \"\"\"\n    # Define the test cases as per the problem statement.\n    # Format: (seed, n, mu, sigma_X, beta_0, beta_1, lambda_star, sigma_epsilon)\n    test_cases = [\n        (0, 400, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0),\n        (1, 400, 0.0, 0.25, -0.5, 1.5, 1.0, 1.0),\n        (2, 400, 0.0, 1.25, 0.0, 1.0, 0.5, 1.5),\n        (3, 400, 0.5, 1.0, 0.5, 1.0, -0.5, 1.0),\n    ]\n\n    # Candidate set for the transformation parameter lambda.\n    Lambda = np.array([-2.0, -1.0, -0.5, -0.25, 0.0, 0.25, 0.5, 1.0, 2.0])\n\n    all_results = []\n\n    def box_cox_transform(x, lambda_val):\n        \"\"\"\n        Applies the Box-Cox transformation to the input data x.\n        \"\"\"\n        if lambda_val == 0.0:\n            return np.log(x)\n        else:\n            return (np.power(x, lambda_val) - 1.0) / lambda_val\n\n    for case in test_cases:\n        seed, n, mu, sigma_x, beta_0, beta_1, lambda_star, sigma_eps = case\n        \n        # Initialize random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n        \n        # Step 1: Data generation.\n        # X is from a LogNormal distribution, ensuring it's positive.\n        X = rng.lognormal(mean=mu, sigma=sigma_x, size=n)\n        # Epsilon is from a Normal distribution.\n        eps = rng.normal(loc=0.0, scale=sigma_eps, size=n)\n        \n        # The true transformed predictor and the response variable Y.\n        X_transformed_true = box_cox_transform(X, lambda_star)\n        Y = beta_0 + beta_1 * X_transformed_true + eps\n        \n        abs_skewness_list = []\n        log_likelihood_list = []\n        \n        # Step 2-4: Iterate through the transformation grid.\n        for lambda_val in Lambda:\n            # Transform the predictor X using the current lambda.\n            Z = box_cox_transform(X, lambda_val)\n            \n            # Compute absolute skewness. scipy.stats.skew with bias=True (default)\n            # matches the formula provided in the problem.\n            current_abs_skew = np.abs(skew(Z))\n            abs_skewness_list.append(current_abs_skew)\n            \n            # Fit an OLS model: Y = b0 + b1*Z.\n            # np.polyfit returns coefficients [b1, b0].\n            b1_fit, b0_fit = np.polyfit(Z, Y, 1)\n            \n            # Calculate predicted Y values and residuals.\n            Y_pred = b0_fit + b1_fit * Z\n            \n            # Compute residual sum of squares (RSS).\n            rss = np.sum((Y - Y_pred)**2)\n            \n            # The MLE for variance is RSS/n.\n            sigma2_hat = rss / float(n)\n            \n            # Compute the maximized log-likelihood.\n            # A check for non-positive variance to avoid math errors, though unlikely here.\n            if sigma2_hat > 0:\n                log_lik = -n / 2.0 * (np.log(2.0 * np.pi * sigma2_hat) + 1.0)\n            else:\n                log_lik = -np.inf\n            log_likelihood_list.append(log_lik)\n\n        # Find the lambda that minimizes absolute skewness.\n        # np.argmin respects the tie-breaking rule (first occurrence).\n        idx_skew = np.argmin(abs_skewness_list)\n        hat_lambda_skew = Lambda[idx_skew]\n        \n        # Find the lambda that maximizes log-likelihood.\n        # np.argmax also respects the tie-breaking rule.\n        idx_fit = np.argmax(log_likelihood_list)\n        hat_lambda_fit = Lambda[idx_fit]\n        \n        # Step 5: Improvement over identity.\n        # Find the log-likelihood for the identity transformation (lambda = 1.0).\n        idx_1 = np.where(Lambda == 1.0)[0][0]\n        log_lik_at_1 = log_likelihood_list[idx_1]\n        \n        # Get the log-likelihood at the skewness-minimizing lambda.\n        log_lik_at_skew = log_likelihood_list[idx_skew]\n        \n        Delta_skew = log_lik_at_skew - log_lik_at_1\n        I = Delta_skew > 0.0\n        \n        # Step 6: Assemble the output for the current case.\n        case_result = [\n            hat_lambda_skew, \n            hat_lambda_fit, \n            bool(I),\n            Delta_skew\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    result_strings = []\n    for res in all_results:\n        # Format: [float, float, bool, float] with specified rounding and boolean case.\n        s = f\"[{res[0]:.4f},{res[1]:.4f},{str(res[2]).lower()},{res[3]:.4f}]\"\n        result_strings.append(s)\n    \n    # Final print statement must be on a single line in the exact specified format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3160298"}, {"introduction": "当预测变量之间高度相关（即存在共线性）时，标准的线性回归模型可能会变得不稳定，其系数也难以解释。这个实践练习 [@problem_id:3160401] 构建了一个完美的共线性场景，以揭示普通最小二乘法（OLS）在这种情况下遇到的参数不可识别问题。通过对比无正则化和岭回归的解，您将亲手发现正则化是如何通过稳定系数来解决这个问题的。", "problem": "考虑一个数据构造，用以突显预测变量、特征和输入在线性模型中（无论是否使用正则化）的作用。假设有 $n$ 个样本，其单个基本特征 $x_1 \\in \\mathbb{R}^n$ 定义为 $x_1 = [-2,-1,0,1,2]^\\top$，并定义一个完全共线的第二个特征 $x_2 = 3 x_1$。设目标变量是无噪声的，由 $y = 6 x_1$ 给出，并取截距为 $0$。根据需要将特征列堆叠起来，定义设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$。\n\n本问题的基础是：\n- 普通最小二乘法（OLS）的定义：最小化残差平方和，等价于在唯一解存在时求解正规方程 $X^\\top X \\beta = X^\\top y$。\n- 岭回归的定义：最小化带有 $\\ell_2$ 惩罚项的残差平方和，等价于求解 $(X^\\top X + \\lambda I)\\beta = X^\\top y$（其中 $\\lambda \\ge 0$，$I$ 是单位矩阵）。\n- 摩尔-彭若斯伪逆（MPP）：当 $X^\\top X$ 是奇异矩阵时，最小范数最小二乘解为 $\\beta = X^+ y$，其中 $X^+$ 表示伪逆矩阵。\n- 可辨识性原则：如果存在唯一解，则参数是可辨识的；否则，可能存在无穷多个参数向量 $\\beta$ 產生相同的预测值 $X\\beta$。\n\n任务：\n1. 仅使用基本特征 $x_1$（$p = 1$），计算 OLS 系数 $\\beta_1$。\n2. 使用两个具有完全共线性 $x_2 = 3 x_1$ 的特征 $[x_1, x_2]$（$p = 2$），使用摩尔-彭若斯伪逆计算非正则化的最小二乘系数 $(\\beta_1, \\beta_2)$。\n3. 对于共线的双特征情况，计算在以下正则化强度 $\\lambda$ 下的岭回归系数 $(\\beta_1, \\beta_2)$：$0$, $0.1$, $1$, $10$ 和 $1{,}000{,}000$。当 $\\lambda = 0$ 时，使用摩尔-彭若斯伪逆。\n4. 讨论可辨识性和系数路径：推导在这种特定的秩为 $1$ 的设置下岭回归解的形式，并解释比率 $\\beta_2 / \\beta_1$ 如何随 $\\lambda$ 的变化而变化。\n\n数值细节：\n- 所有计算都是纯数学的，无单位。\n- 报告的每个系数必须四舍五入到 $6$ 位小数。\n\n测试套件：\n- 案例 1：使用单个预测变量 $x_1$ 的 OLS（非正则化），返回 $[\\beta_1]$。\n- 案例 2：使用预测变量 $[x_1, x_2]$ 的 OLS（使用 MPP 进行非正则化），返回 $[\\beta_1, \\beta_2]$。\n- 案例 3：使用预测变量 $[x_1, x_2]$ 和 $\\lambda = 0$ 的岭回归，通过 MPP 返回 $[\\beta_1, \\beta_2]$。\n- 案例 4：使用预测变量 $[x_1, x_2]$ 和 $\\lambda = 0.1$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n- 案例 5：使用预测变量 $[x_1, x_2]$ 和 $\\lambda = 1$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n- 案例 6：使用预测变量 $[x_1, x_2]$ 和 $\\lambda = 10$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n- 案例 7：使用预测变量 $[x_1, x_2]$ 和 $\\lambda = 1{,}000{,}000$ 的岭回归，返回 $[\\beta_1, \\beta_2]$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素是测试案例的舍入后系数的内部括号列表，顺序与测试套件一致。例如，输出应如下所示\n$[[c_{1,1},c_{1,2},\\dots],[c_{2,1},c_{2,2}],\\dots]$\n并且必须精确地打印为单行，不带任何额外文本。", "solution": "问题验证如下。\n\n已知条件：\n- 样本数量：$n = 5$。\n- 基本特征向量：$x_1 = [-2, -1, 0, 1, 2]^\\top \\in \\mathbb{R}^n$。\n- 第二个特征向量：$x_2 = 3 x_1$。\n- 目标向量：$y = 6 x_1$。\n- 截距：$0$。\n- 设计矩阵：$X \\in \\mathbb{R}^{n \\times p}$，由特征列组成。\n- 普通最小二乘法（OLS）定义：最小化残差平方和，求解 $X^\\top X \\beta = X^\\top y$。\n- 岭回归定义：最小化残差平方和加上一个 $\\ell_2$ 惩罚项，求解 $(X^\\top X + \\lambda I)\\beta = X^\\top y$（其中 $\\lambda \\ge 0$）。\n- 摩尔-彭若斯伪逆（MPP）：对于奇异矩阵 $X^\\top X$，最小范数最小二乘解为 $\\beta = X^+ y$。\n- 任务 1：使用特征 $x_1$（$p=1$）计算 OLS 系数 $\\beta_1$。\n- 任务 2：使用特征 $[x_1, x_2]$（$p=2$）通过 MPP 计算 OLS 系数 $(\\beta_1, \\beta_2)$。\n- 任务 3：计算当 $p=2$ 且 $\\lambda \\in \\{0, 0.1, 1, 10, 1000000\\}$ 时的岭回归系数 $(\\beta_1, \\beta_2)$。\n- 任务 4：讨论可辨识性以及系数比率 $\\beta_2 / \\beta_1$ 随 $\\lambda$ 变化的表现。\n- 数值精度：将系数四舍五入到 6 位小数。\n\n验证：\n- **科学性：** 该问题是统计学习和线性代数中的一个标准练习，涉及 OLS、岭回归、多重共线性和参数可辨识性等基本概念。所有原则都是公认的。\n- **适定性：** 该问题提供了所有必要的数据和定义。对于奇异情况使用摩尔-彭若斯伪逆的指令确保了每个任务都有唯一定义的解。因此，该问题是适定的。\n- **客观性：** 该问题以精确的数学语言陈述，没有主观性或歧义。\n\n结论：该问题有效。\n\n我们继续进行解答。\n\n首先，我们定义给定的向量并计算一些初步量。\n基本特征向量是 $x_1 = [-2, -1, 0, 1, 2]^\\top$。\n样本数量为 $n=5$。\n第二个特征是 $x_2 = 3x_1 = [-6, -3, 0, 3, 6]^\\top$。\n目标向量是 $y = 6x_1 = [-12, -6, 0, 6, 12]^\\top$。\n\n我们计算以下内积：\n$x_1^\\top x_1 = (-2)^2 + (-1)^2 + 0^2 + 1^2 + 2^2 = 4 + 1 + 0 + 1 + 4 = 10$。\n$x_1^\\top y = x_1^\\top (6x_1) = 6(x_1^\\top x_1) = 6(10) = 60$。\n\n任务 1：使用单个预测变量 $x_1$ 的 OLS\n设计矩阵是 $X = x_1$，这是一个 $5 \\times 1$ 的矩阵。OLS 系数 $\\beta_1$ 通过求解正规方程 $(X^\\top X) \\beta_1 = X^\\top y$ 得到。\n$$\n\\beta_1 = (x_1^\\top x_1)^{-1} (x_1^\\top y) = (10)^{-1} (60) = 6\n$$\n单特征模型的系数为 $\\beta_1 = 6$。\n\n任务 2：使用预测变量 $[x_1, x_2]$ 并利用摩尔-彭若斯伪逆的 OLS\n设计矩阵为 $X = [x_1, x_2] = [x_1, 3x_1]$。这些列是完全共线的，所以 $X$ 的秩为 $1$。矩阵 $X^\\top X$ 将是奇异的。\n$$\nX^\\top X = \\begin{bmatrix} x_1^\\top x_1  x_1^\\top x_2 \\\\ x_2^\\top x_1  x_2^\\top x_2 \\end{bmatrix} = \\begin{bmatrix} x_1^\\top x_1  x_1^\\top (3x_1) \\\\ (3x_1)^\\top x_1  (3x_1)^\\top (3x_1) \\end{bmatrix} = \\begin{bmatrix} x_1^\\top x_1  3(x_1^\\top x_1) \\\\ 3(x_1^\\top x_1)  9(x_1^\\top x_1) \\end{bmatrix} = \\begin{bmatrix} 10  30 \\\\ 30  90 \\end{bmatrix}\n$$\n$X^\\top X$ 的行列式是 $10 \\times 90 - 30 \\times 30 = 0$，证实了其奇异性。正规方程的右侧是：\n$$\nX^\\top y = \\begin{bmatrix} x_1^\\top y \\\\ x_2^\\top y \\end{bmatrix} = \\begin{bmatrix} x_1^\\top y \\\\ (3x_1)^\\top y \\end{bmatrix} = \\begin{bmatrix} 60 \\\\ 3(60) \\end{bmatrix} = \\begin{bmatrix} 60 \\\\ 180 \\end{bmatrix}\n$$\n方程组 $X^\\top X \\beta = X^\\top y$ 有无穷多解，满足 $10\\beta_1 + 30\\beta_2 = 60$，简化为 $\\beta_1 + 3\\beta_2 = 6$。摩尔-彭若斯伪逆提供了最小 $\\ell_2$-范数解。该解向量 $\\beta=[\\beta_1, \\beta_2]^\\top$ 必须与约束的零空间正交，这意味着它必须与系数向量 $[1, 3]^\\top$ 成比例。所以，对于某个标量 $k$，有 $\\beta=k[1, 3]^\\top = [k, 3k]^\\top$。\n将此代入约束方程：\n$$\nk + 3(3k) = 6 \\implies 10k = 6 \\implies k = 0.6\n$$\n因此，最小范数解为 $\\beta_1 = 0.6$ 和 $\\beta_2 = 3(0.6) = 1.8$。\n\n任务 3：针对不同 $\\lambda$ 的岭回归\n岭回归解由 $\\beta_\\lambda = (X^\\top X + \\lambda I)^{-1} X^\\top y$ 给出。需要求逆的矩阵是：\n$$\nX^\\top X + \\lambda I = \\begin{bmatrix} 10  30 \\\\ 30  90 \\end{bmatrix} + \\begin{bmatrix} \\lambda  0 \\\\ 0  \\lambda \\end{bmatrix} = \\begin{bmatrix} 10+\\lambda  30 \\\\ 30  90+\\lambda \\end{bmatrix}\n$$\n对于 $\\lambda > 0$，该矩阵是可逆的。其行列式为 $(10+\\lambda)(90+\\lambda) - 900 = 100\\lambda + \\lambda^2 = \\lambda(\\lambda+100)$。\n逆矩阵是：\n$$\n(X^\\top X + \\lambda I)^{-1} = \\frac{1}{\\lambda(\\lambda+100)} \\begin{bmatrix} 90+\\lambda  -30 \\\\ -30  10+\\lambda \\end{bmatrix}\n$$\n那么岭回归系数是：\n$$\n\\beta_\\lambda = \\begin{bmatrix} \\beta_{\\lambda,1} \\\\ \\beta_{\\lambda,2} \\end{bmatrix} = \\frac{1}{\\lambda(\\lambda+100)} \\begin{bmatrix} 90+\\lambda  -30 \\\\ -30  10+\\lambda \\end{bmatrix} \\begin{bmatrix} 60 \\\\ 180 \\end{bmatrix}\n$$\n计算各分量：\n$\\beta_{\\lambda,1} = \\frac{1}{\\lambda(\\lambda+100)} [60(90+\\lambda) - 30(180)] = \\frac{60\\lambda}{\\lambda(\\lambda+100)} = \\frac{60}{\\lambda+100}$。\n$\\beta_{\\lambda,2} = \\frac{1}{\\lambda(\\lambda+100)} [-30(60) + 180(10+\\lambda)] = \\frac{180\\lambda}{\\lambda(\\lambda+100)} = \\frac{180}{\\lambda+100}$。\n\n- 对于 $\\lambda=0$：问题要求使用 MPP，因此结果与任务 2 相同：$(\\beta_1, \\beta_2) = (0.6, 1.8)$。\n- 对于 $\\lambda=0.1$：$(\\beta_1, \\beta_2) = (\\frac{60}{100.1}, \\frac{180}{100.1}) \\approx (0.599401, 1.798202)$。\n- 对于 $\\lambda=1$：$(\\beta_1, \\beta_2) = (\\frac{60}{101}, \\frac{180}{101}) \\approx (0.594059, 1.782178)$。\n- 对于 $\\lambda=10$：$(\\beta_1, \\beta_2) = (\\frac{60}{110}, \\frac{180}{110}) \\approx (0.545455, 1.636364)$。\n- 对于 $\\lambda=1,000,000$：$(\\beta_1, \\beta_2) = (\\frac{60}{1000100}, \\frac{180}{1000100}) \\approx (0.000060, 0.000180)$。\n\n任务 4：关于可辨识性和系数路径的讨论\n- 可辨识性：在 OLS 情况下（$\\lambda=0$），当特征 $x_1$ 和 $x_2$ 共线时，矩阵 $X^\\top X$ 是奇异的。这导致正规方程组有无穷多解，所有解都位于直线 $\\beta_1 + 3\\beta_2 = 6$ 上。由于参数向量 $\\beta$ 不存在唯一解，因此参数是不可辨識的。然而，所有解的预测值 $\\hat{y} = X\\beta$ 都是唯一的，因为 $\\hat{y} = \\beta_1 x_1 + \\beta_2 x_2 = \\beta_1 x_1 + \\beta_2 (3x_1) = (\\beta_1 + 3\\beta_2)x_1 = 6x_1 = y$。岭回归通过添加项 $\\lambda I$（其中 $\\lambda > 0$），使得矩阵 $(X^\\top X + \\lambda I)$ 可逆，从而确保了 $\\beta_\\lambda$ 的唯一解，并使参数在任何正正则化强度下都是可辨識的。\n- 系数路径：我们推导出的岭回归解为 $\\beta_\\lambda = (\\frac{60}{\\lambda+100}, \\frac{180}{\\lambda+100})$。\n系数之比为：\n$$\n\\frac{\\beta_{\\lambda,2}}{\\beta_{\\lambda,1}} = \\frac{180/(\\lambda+100)}{60/(\\lambda+100)} = \\frac{180}{60} = 3\n$$\n对于所有 $\\lambda > 0$，此比率恒为 $3$。当 $\\lambda \\to 0^+$ 时，岭回归解收敛于 $(0.6, 1.8)$，即最小范数 OLS 解，其比率为 $1.8/0.6=3$。当 $\\lambda \\to \\infty$ 时，两个系数都向 $0$ 收缩，但它们的比率保持为 $3$ 不变。系数路径是一条从最小范数解 $(0.6, 1.8)$ 到原点 $(0,0)$ 的直线。这是因为预测变量 $x_1$ 和 $x_2$ 完全相关，在特征空间中定义了一个由向量 $[1, 3]^\\top$ 张成的一维子空间。数据方差仅存在于这个方向上。岭回归惩罚系数，在这种情况下，它沿着数据协方差矩阵 $X^\\top X$ 的主成分方向收缩系数。$X^\\top X$ 的主特征向量与 $[1, 3]^\\top$ 成比例，这决定了系数的固定比率。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the linear regression problem under different conditions as specified.\n    \"\"\"\n    # Define base data using floating-point numbers for precision.\n    x1_vec = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    x1 = x1_vec.reshape(-1, 1)\n    y = 6.0 * x1\n\n    all_results = []\n\n    # Case 1: OLS with a single predictor x1 (p=1)\n    # beta_1 = (X1.T @ X1)^-1 @ X1.T @ y\n    X1 = x1\n    beta_case1 = np.linalg.inv(X1.T @ X1) @ (X1.T @ y)\n    all_results.append(beta_case1.flatten())\n\n    # Define data for two-predictor cases (p=2)\n    x2 = 3.0 * x1\n    X2 = np.hstack((x1, x2))\n    \n    # Case 2: OLS with predictors [x1, x2] using Moore-Penrose Pseudoinverse\n    # beta = pinv(X2) @ y\n    beta_mpp = np.linalg.pinv(X2) @ y\n    all_results.append(beta_mpp.flatten())\n\n    # Case 3: Ridge with lambda = 0, using MPP as specified\n    # This is identical to Case 2.\n    all_results.append(beta_mpp.flatten())\n\n    # Cases 4-7: Ridge with predictors [x1, x2] and lambda > 0\n    lambdas = [0.1, 1.0, 10.0, 1000000.0]\n    p = X2.shape[1]\n    identity_matrix = np.identity(p)\n    X2T_X2 = X2.T @ X2\n    X2T_y = X2.T @ y\n\n    for lam in lambdas:\n        # beta_ridge = (X2.T @ X2 + lam * I)^-1 @ X2.T @ y\n        beta_ridge = np.linalg.inv(X2T_X2 + lam * identity_matrix) @ X2T_y\n        all_results.append(beta_ridge.flatten())\n    \n    # Format the final output string as per requirements.\n    # Each coefficient is formatted to 6 decimal places.\n    formatted_results = []\n    for res_vector in all_results:\n        # Using f-string formatting handles rounding and ensures 6 decimal places.\n        s_res = [f\"{x:.6f}\" for x in res_vector]\n        formatted_results.append(f\"[{','.join(s_res)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3160401"}, {"introduction": "在现实世界中，模型常常需要处理来自不同领域（domain）的数据，即使底层关系保持不变，特征的尺度或分布也可能发生变化。本练习 [@problem_id:3160364] 模拟了一个由于货币单位变化而引起的领域偏移场景，旨在评估不同的特征归一化策略的有效性。通过这个练习，您将理解为什么利用已知的领域知识（如汇率）进行调整，通常比依赖纯粹的统计标准化方法更为稳健和有效。", "problem": "给定一个一维监督学习设定，其中单个实值特征表示产品价格，预测目标是以特定货币表示的价格的线性函数。训练域使用美元 (USD)。在测试时会发生域漂移，因为相同的真实世界量值以已知的汇率被记录为欧元 (EUR)。您的任务是实现并评估三种不同的特征归一化策略，以便在这种域漂移下产生一致的预测。\n\n基本原理和定义：\n- 预测器是一个函数 $f:\\mathbb{R}\\to\\mathbb{R}$，它将输入特征 $x$ 映射到预测输出 $y$。我们考虑平方损失 $L(y,\\hat{y})=(y-\\hat{y})^{2}$。\n- 在使用平方损失对线性函数进行经验风险最小化时，用于将 $y$ 回归到单个输入 $z$ 上的普通最小二乘法解的斜率为 $\\hat{w}=\\dfrac{\\sum (z-\\bar{z})(y-\\bar{y})}{\\sum (z-\\bar{z})^{2}}$，截距为 $\\hat{c}=\\bar{y}-\\hat{w}\\,\\bar{z}$，其中上划线表示样本均值。\n- 对于样本均值为 $\\mu$、样本标准差为 $\\sigma>0$ 的特征 $x$，其 Z-score 标准化定义为 $z=(x-\\mu)/\\sigma$。\n\n数据生成机制：\n- 真实目标是美元价格的线性函数：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$。\n- 训练数据以美元收集。在测试时，观察到的特征是 $x_{\\mathrm{EUR}}=r\\cdot x_{\\mathrm{USD}}$，其中 $r>0$ 是汇率（欧元/美元）。标签始终遵循上述基于美元的定律。\n\n所有情况下均使用的训练流程：\n1. 使用总体定义（即，对所有 $n$ 个样本求平均，分母为 $n$），计算美元特征 $x_{\\mathrm{USD}}$ 的训练均值 $\\mu_{\\mathrm{A}}$ 和标准差 $\\sigma_{\\mathrm{A}}$。\n2. 将训练特征标准化为 $z_{\\mathrm{A}}=(x_{\\mathrm{USD}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n3. 在 $(z_{\\mathrm{A}},y)$ 上通过普通最小二乘法拟合线性预测器 $\\hat{y}=\\hat{w}\\,z+\\hat{c}$。\n\n在测试时，对于一个观察到的欧元特征向量 $x_{\\mathrm{EUR}}$，评估三种策略来构建用于推断的标准化输入 $z$：\n- 策略 T1（在观察值上使用训练统计数据）：$z_{\\mathrm{T1}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n- 策略 T2（按域标准化）：从 $x_{\\mathrm{EUR}}$ 本身使用总体定义计算 $\\mu_{\\mathrm{B}}$ 和 $\\sigma_{\\mathrm{B}}$，然后设置 $z_{\\mathrm{T2}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}}$。\n- 策略 T3（考虑货币后使用训练统计数据）：$z_{\\mathrm{T3}}=(x_{\\mathrm{EUR}}/r-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n\n对于每种策略，生成预测 $\\hat{y}=\\hat{w}\\,z+\\hat{c}$ 并评估测试集上的均方根误差 (RMSE)，其定义为 $\\sqrt{\\dfrac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}$，其中 $m$ 是测试样本的数量，$y_{i}=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD},i}$。\n\n使用以下固定的数值规格：\n- 真实参数：$\\theta_{0}=4.0$，$\\theta_{1}=1.2$。\n- 训练美元特征值：$[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$。\n- 训练标签：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$ 根据上述值精确计算。\n- 测试套件（四种情况），每种情况都给定汇率 $r$ 和美元价格列表 $x_{\\mathrm{USD}}$；观察到的测试特征为 $x_{\\mathrm{EUR}}=r\\cdot x_{\\mathrm{USD}}$：\n  1. 情况 1（一般域漂移）：$r=0.9$, $x_{\\mathrm{USD}}=[\\,15.0,\\,22.5,\\,60.0,\\,80.0\\,]$。\n  2. 情况 2（无域漂移）：$r=1.0$, $x_{\\mathrm{USD}}=[\\,10.0,\\,30.0,\\,50.0\\,]$。\n  3. 情况 3（缩放下的分布匹配）：$r=1.15$, $x_{\\mathrm{USD}}=[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$。\n  4. 情况 4（强域漂移）：$r=0.5$, $x_{\\mathrm{USD}}=[\\,5.0,\\,15.0,\\,75.0,\\,120.0\\,]$。\n\n您的程序应：\n- 完全按照规定实现训练流程。\n- 对于每个测试用例，按顺序计算策略 T1、T2 和 T3 的 RMSE。\n- 输出中不需要物理单位；将所有值报告为实数。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含一个逗号分隔的列表，用方括号括起来，顺序如下：\n  $[\\,\\mathrm{RMSE}_{\\mathrm{T1}}^{(1)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(1)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(1)},\\mathrm{RMSE}_{\\mathrm{T1}}^{(2)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(2)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(2)},\\mathrm{RMSE}_{\\mathrm{T1}}^{(3)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(3)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(3)},\\mathrm{RMSE}_{\\mathrm{T1}}^{(4)},\\mathrm{RMSE}_{\\mathrm{T2}}^{(4)},\\mathrm{RMSE}_{\\mathrm{T3}}^{(4)}\\,]$，其中上标 $^{(k)}$ 索引测试用例编号 $k\\in\\{1,2,3,4\\}$。", "solution": "此问题需要进行验证。\n\n### 步骤 1：提取给定信息\n- **预测器函数**：$f:\\mathbb{R}\\to\\mathbb{R}$，将输入特征 $x$ 映射到预测输出 $\\hat{y}$。\n- **损失函数**：平方损失，$L(y,\\hat{y})=(y-\\hat{y})^{2}$。\n- **单个回归量 $z$ 的普通最小二乘法 (OLS) 解**：\n  - 斜率：$\\hat{w}=\\dfrac{\\sum (z-\\bar{z})(y-\\bar{y})}{\\sum (z-\\bar{z})^{2}}$\n  - 截距：$\\hat{c}=\\bar{y}-\\hat{w}\\,\\bar{z}$\n- **Z-score 标准化**：$z=(x-\\mu)/\\sigma$，使用样本均值 $\\mu$ 和样本标准差 $\\sigma>0$。问题指定使用总体定义（均值和方差的分母为 $n$）。\n- **真实数据生成过程**：真实目标 $y$ 是美元价格的线性函数：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$。\n- **域漂移机制**：训练数据使用 $x_{\\mathrm{USD}}$。测试数据使用 $x_{\\mathrm{EUR}}=r\\cdot x_{\\mathrm{USD}}$，其中 $r>0$ 是一个已知的汇率。目标 $y$ 总是由 $x_{\\mathrm{USD}}$ 决定。\n- **训练流程**：\n  1. 计算训练特征 $x_{\\mathrm{USD}}$ 的训练均值 $\\mu_{\\mathrm{A}}$ 和总体标准差 $\\sigma_{\\mathrm{A}}$（来自 $n$ 个样本）。\n  2. 将训练特征标准化：$z_{\\mathrm{A}}=(x_{\\mathrm{USD}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n  3. 通过对 $(z_{\\mathrm{A}},y)$ 应用 OLS 来拟合线性预测器 $\\hat{y}=\\hat{w}\\,z+\\hat{c}$。\n- **测试时推断策略**：对于一个观察到的测试特征向量 $x_{\\mathrm{EUR}}$，输入 $z$ 通过以下方式构建：\n  - **T1（在观察值上使用训练统计数据）**：$z_{\\mathrm{T1}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n  - **T2（按域标准化）**：从 $x_{\\mathrm{EUR}}$ 计算均值 $\\mu_{\\mathrm{B}}$ 和总体标准差 $\\sigma_{\\mathrm{B}}$，然后 $z_{\\mathrm{T2}}=(x_{\\mathrm{EUR}}-\\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}}$。\n  - **T3（考虑货币后使用训练统计数据）**：$z_{\\mathrm{T3}}=(x_{\\mathrm{EUR}}/r-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n- **评估指标**：测试集上的均方根误差 (RMSE)，$\\sqrt{\\dfrac{1}{m}\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}$，其中 $m$ 是测试集大小。\n- **数值规格**：\n  - 真实参数：$\\theta_{0}=4.0$，$\\theta_{1}=1.2$。\n  - 训练数据 $x_{\\mathrm{USD}}$：$[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$，$n=5$。\n  - 训练标签：$y=\\theta_{0}+\\theta_{1}\\,x_{\\mathrm{USD}}$。\n  - 测试用例：\n    1. $r=0.9$，$x_{\\mathrm{USD}}=[\\,15.0,\\,22.5,\\,60.0,\\,80.0\\,]$。\n    2. $r=1.0$，$x_{\\mathrm{USD}}=[\\,10.0,\\,30.0,\\,50.0\\,]$。\n    3. $r=1.15$，$x_{\\mathrm{USD}}=[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$。\n    4. $r=0.5$，$x_{\\mathrm{USD}}=[\\,5.0,\\,15.0,\\,75.0,\\,120.0\\,]$。\n\n### 步骤 2：使用提取的给定信息进行验证\n该问题在科学上基于线性回归和统计学习的原理。域漂移、特征标准化和经验风险最小化的概念是该领域的标准概念。该设定是一个简化但概念上合理的真实世界场景模型，其中在一个上下文中（例如，一种货币）训练的模型必须应用于另一个上下文。该问题是适定的：所有数据、参数和流程都有明确定义，不存在矛盾。为训练和测试提供的特征向量都包含不同元素，确保标准差 $\\sigma$ 始终大于零，使得 Z-score 标准化是适定的。该问题并非无足轻重；它需要仔细实现指定的流程，并突出了关于协变量漂移下模型鲁棒性的基本概念。预期结果是可通过计算验证的。\n\n### 步骤 3：结论与行动\n问题有效。将提供一个合理的解决方案。\n\n### 解决方案\n\n该问题要求我们训练一个线性回归模型，并使用三种不同的特征归一化策略来评估其在特定类型域漂移下的性能。\n\n**1. 训练阶段**\n\n训练过程涉及拟合一个线性模型 $\\hat{y} = \\hat{w}z + \\hat{c}$，其中 $z$ 是标准化特征。\n\n首先，我们定义训练数据。特征给定为 $x_{\\mathrm{A, USD}} = [\\,12.0,\\, 25.0,\\, 40.0,\\, 55.0,\\, 70.0\\,]$。\n真实参数为 $\\theta_{0}=4.0$ 和 $\\theta_{1}=1.2$。训练标签 $y_{\\mathrm{A}}$ 由真实模型生成，$y_i = \\theta_{0} + \\theta_{1} x_{\\mathrm{A, USD}, i}$：\n$y_{\\mathrm{A}} = [\\,4.0 + 1.2 \\cdot 12.0,\\, \\dots,\\, 4.0 + 1.2 \\cdot 70.0\\,] = [\\,18.4,\\, 34.0,\\, 52.0,\\, 70.0,\\, 88.0\\,]$。\n\n接下来，我们使用总体定义（分母 $n=5$）计算训练特征的统计数据：\n均值为 $\\mu_{\\mathrm{A}} = \\frac{1}{5}(12.0 + 25.0 + 40.0 + 55.0 + 70.0) = \\frac{202.0}{5} = 40.4$。\n方差为 $\\sigma_{\\mathrm{A}}^2 = \\frac{1}{5}\\sum_{i=1}^{5}(x_{\\mathrm{A, USD}, i} - \\mu_{\\mathrm{A}})^2 = \\frac{1}{5}((12.0-40.4)^2 + \\dots + (70.0-40.4)^2) = \\frac{2133.2}{5} = 426.64$。\n标准差为 $\\sigma_{\\mathrm{A}} = \\sqrt{426.64} \\approx 20.6552656$。\n\n训练特征被标准化：$z_{\\mathrm{A}} = (x_{\\mathrm{A, USD}}-\\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n真实标签 $y_{\\mathrm{A}}$ 与标准化特征 $z_{\\mathrm{A}}$ 之间的关系是完全线性的：\n$y_{\\mathrm{A}} = \\theta_{0} + \\theta_{1}x_{\\mathrm{A, USD}} = \\theta_{0} + \\theta_{1}(\\sigma_{\\mathrm{A}} z_{\\mathrm{A}} + \\mu_{\\mathrm{A}}) = (\\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}}) + (\\theta_{1}\\sigma_{\\mathrm{A}})z_{\\mathrm{A}}$。\n由于 OLS 提供了最佳线性无偏估计量，并且数据完全在一条直线上，因此拟合的参数将是精确的。模型为 $\\hat{y} = \\hat{w}z_{\\mathrm{A}} + \\hat{c}$。\n估计的斜率为 $\\hat{w} = \\theta_{1}\\sigma_{\\mathrm{A}} = 1.2 \\cdot \\sqrt{426.64} \\approx 24.7863187$。\n估计的截距为 $\\hat{c} = \\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}} = 4.0 + 1.2 \\cdot 40.4 = 4.0 + 48.48 = 52.48$。\n\n这些参数 $(\\hat{w}, \\hat{c})$ 是固定的，并在所有测试用例中用于推断。\n\n**2. 测试阶段**\n\n对于每个测试用例，我们都得到一个汇率 $r$ 和一组测试价格 $x_{\\mathrm{B, USD}}$。观察到的特征是 $x_{\\mathrm{B, EUR}} = r \\cdot x_{\\mathrm{B, USD}}$，而真实标签是 $y_{\\mathrm{B}} = \\theta_{0} + \\theta_{1}x_{\\mathrm{B, USD}}$。\n\n**策略 T1（在观察值上使用训练统计数据）：**\n用于预测的输入：$z_{\\mathrm{T1}} = (x_{\\mathrm{B, EUR}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n预测：$\\hat{y}_{\\mathrm{T1}} = \\hat{w} z_{\\mathrm{T1}} + \\hat{c}$。\n该策略将训练统计数据天真地应用于来自漂移域的特征。它未能考虑到货币变化和特征分布的潜在变化。\n\n**策略 T2（按域标准化）：**\n首先，从测试特征 $x_{\\mathrm{B, EUR}}$ 计算统计数据：均值 $\\mu_{\\mathrm{B}}$ 和标准差 $\\sigma_{\\mathrm{B}}$。\n用于预测的输入：$z_{\\mathrm{T2}} = (x_{\\mathrm{B, EUR}} - \\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}}$。\n预测：$\\hat{y}_{\\mathrm{T2}} = \\hat{w} z_{\\mathrm{T2}} + \\hat{c}$。\n该策略适应了测试分布的前两个矩，但忽略了域之间的已知关系。\n\n**策略 T3（考虑货币后使用训练统计数据）：**\n首先，反转货币转换：$x_{\\mathrm{B, USD\\_reconstructed}} = x_{\\mathrm{B, EUR}} / r$。根据定义，这完美地恢复了原始的 $x_{\\mathrm{B, USD}}$。\n用于预测的输入：$z_{\\mathrm{T3}} = (x_{\\mathrm{B, USD\\_reconstructed}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}} = (x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。\n预测：$\\hat{y}_{\\mathrm{T3}} = \\hat{w} z_{\\mathrm{T3}} + \\hat{c}$。\n让我们分析一下预测：\n$\\hat{y}_{\\mathrm{T3}} = (\\theta_{1}\\sigma_{\\mathrm{A}})\\left(\\frac{x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}}}{\\sigma_{\\mathrm{A}}}\\right) + (\\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}})$\n$\\hat{y}_{\\mathrm{T3}} = \\theta_{1}(x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}}) + \\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}}$\n$\\hat{y}_{\\mathrm{T3}} = \\theta_{1}x_{\\mathrm{B, USD}} - \\theta_{1}\\mu_{\\mathrm{A}} + \\theta_{0} + \\theta_{1}\\mu_{\\mathrm{A}} = \\theta_{0} + \\theta_{1}x_{\\mathrm{B, USD}} = y_{\\mathrm{B}}$。\n策略 T3 的预测恰好是真实标签。因此，在所有情况下，T3 的 RMSE 都将为 $0$。这表明，如果域漂移是一个已知的、可逆的转换，那么在特征工程之前应用该逆转换，对于一个无噪声的线性模型，可以产生完美的预测。\n\n现在我们为所有策略和测试用例计算 RMSE。\n\n**情况 1：$r=0.9$，$x_{\\mathrm{B, USD}}=[\\,15.0,\\,22.5,\\,60.0,\\,80.0\\,]$**\n$x_{\\mathrm{B, EUR}} = [\\,13.5,\\, 20.25,\\, 54.0,\\, 72.0\\,]$\n$y_{\\mathrm{B}} = [\\,22.0,\\, 31.0,\\, 76.0,\\, 100.0\\,]$\n- T1：RMSE $\\approx 11.2345$\n- T2：RMSE $\\approx 9.0733$\n- T3：RMSE $= 0.0$\n\n**情况 2：$r=1.0$，$x_{\\mathrm{B, USD}}=[\\,10.0,\\,30.0,\\,50.0\\,]$**\n这里，$r=1.0$，所以 $x_{\\mathrm{B, EUR}} = x_{\\mathrm{B, USD}}$。这表示数据分布发生了变化，但没有尺度变化。\n$x_{\\mathrm{B, EUR}} = [\\,10.0,\\, 30.0,\\, 50.0\\,]$\n$y_{\\mathrm{B}} = [\\,16.0,\\, 40.0,\\, 64.0\\,]$\n- T1：$z_{\\mathrm{T1}} = (x_{\\mathrm{B, USD}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}}$。这与策略 T3 中的转换相同。如 T3 所示，这会产生完美的预测。RMSE $= 0.0$。\n- T2：此策略使用测试集本身的均值和标准差，这与 $\\mu_{\\mathrm{A}}$ 和 $\\sigma_{\\mathrm{A}}$ 不同。因此，它将导致非零误差。RMSE $\\approx 3.7919$。\n- T3：由于 $r=1.0$，$z_{\\mathrm{T3}} = (x_{\\mathrm{B, EUR}}/1.0 - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}} = z_{\\mathrm{T1}}$。RMSE $= 0.0$。\n\n**情况 3：$r=1.15$，$x_{\\mathrm{B, USD}}=[\\,12.0,\\,25.0,\\,40.0,\\,55.0,\\,70.0\\,]$**\n测试集 $x_{\\mathrm{B, USD}}$ 与训练集 $x_{\\mathrm{A, USD}}$ 相同。\n$x_{\\mathrm{B, EUR}} = r \\cdot x_{\\mathrm{A, USD}}$\n- T1：将产生错误，因为它将缩放后的值视为原始值。RMSE $\\approx 5.7008$。\n- T2：让我们分析这种情况。$\\mu_{\\mathrm{B}} = \\mathrm{mean}(r \\cdot x_{\\mathrm{A, USD}}) = r \\mu_{\\mathrm{A}}$。$\\sigma_{\\mathrm{B}} = \\mathrm{std}(r \\cdot x_{\\mathrm{A, USD}}) = r \\sigma_{\\mathrm{A}}$。\n  $z_{\\mathrm{T2}} = (x_{\\mathrm{B, EUR}} - \\mu_{\\mathrm{B}})/\\sigma_{\\mathrm{B}} = (r x_{\\mathrm{A, USD}} - r \\mu_{\\mathrm{A}})/(r \\sigma_{\\mathrm{A}}) = (x_{\\mathrm{A, USD}} - \\mu_{\\mathrm{A}})/\\sigma_{\\mathrm{A}} = z_{\\mathrm{A}}$。\n  测试集的标准化输入与训练集的标准化输入相同。由于底层的美元价格以及因此的标签也相同，预测值 $\\hat{y}_{\\mathrm{T2}}$ 将与训练标签 $y_{\\mathrm{A}} = y_{\\mathrm{B}}$ 相同。因此，误差为零。RMSE $= 0.0$。\n- T3：与往常一样，RMSE $= 0.0$。\n\n**情况 4：$r=0.5$，$x_{\\mathrm{B, USD}}=[\\,5.0,\\,15.0,\\,75.0,\\,120.0\\,]$**\n$x_{\\mathrm{B, EUR}} = [\\,2.5,\\, 7.5,\\, 37.5,\\, 60.0\\,]$\n$y_{\\mathrm{B}} = [\\,10.0,\\, 22.0,\\, 94.0,\\, 148.0\\,]$\n- T1：RMSE $\\approx 25.1017$\n- T2：RMSE $\\approx 18.0664$\n- T3：RMSE $= 0.0$\n\n结果阐明了一个关键原则：显式地考虑已知的域漂移来源（如货币转换）优于通用的适应策略（如按域重新标准化）或完全忽略漂移。策略 T3 正确地反转了已知的转换，因此是鲁棒的，并在这个理想化的无噪声设置中实现了完美的性能。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates three feature-normalization strategies\n    for a linear regression model under a currency-based domain shift.\n    \"\"\"\n    \n    # Define ground truth parameters and training data\n    theta_0 = 4.0\n    theta_1 = 1.2\n    x_train_usd = np.array([12.0, 25.0, 40.0, 55.0, 70.0])\n\n    # Define the four test cases\n    test_cases = [\n        # Case 1 (general domain shift)\n        {\"r\": 0.9, \"x_usd\": np.array([15.0, 22.5, 60.0, 80.0])},\n        # Case 2 (no domain shift)\n        {\"r\": 1.0, \"x_usd\": np.array([10.0, 30.0, 50.0])},\n        # Case 3 (distributional match under scaling)\n        {\"r\": 1.15, \"x_usd\": np.array([12.0, 25.0, 40.0, 55.0, 70.0])},\n        # Case 4 (strong domain shift)\n        {\"r\": 0.5, \"x_usd\": np.array([5.0, 15.0, 75.0, 120.0])},\n    ]\n\n    # --- Training Phase ---\n    # The problem specifies using population definitions for mean and std dev.\n    # np.mean() and np.std(ddof=0) compute these.\n    mu_A = np.mean(x_train_usd)\n    sigma_A = np.std(x_train_usd) # default ddof=0 is population std dev\n\n    # The training labels are generated by a perfect linear model without noise.\n    # y = theta_0 + theta_1 * x_usd\n    # The feature x_usd is related to the standardized feature z by x_usd = sigma_A * z + mu_A.\n    # Substituting, we get y = theta_0 + theta_1 * (sigma_A * z + mu_A)\n    # y = (theta_0 + theta_1 * mu_A) + (theta_1 * sigma_A) * z\n    # Since the OLS fit on this noiseless data will be perfect, the learned coefficients\n    # for the model y_hat = c_hat + w_hat * z will be exactly:\n    c_hat = theta_0 + theta_1 * mu_A\n    w_hat = theta_1 * sigma_A\n\n    results = []\n\n    # --- Testing Phase ---\n    for case in test_cases:\n        r = case[\"r\"]\n        x_test_usd = case[\"x_usd\"]\n        \n        # Observed features in the test domain (EUR)\n        x_test_eur = r * x_test_usd\n        \n        # Ground-truth labels, always based on USD price\n        y_test_true = theta_0 + theta_1 * x_test_usd\n        \n        # --- Strategy T1 (train-statistics on observed) ---\n        z_t1 = (x_test_eur - mu_A) / sigma_A\n        y_hat_t1 = w_hat * z_t1 + c_hat\n        rmse_t1 = np.sqrt(np.mean((y_hat_t1 - y_test_true)**2))\n        results.append(rmse_t1)\n        \n        # --- Strategy T2 (per-domain standardization) ---\n        mu_B = np.mean(x_test_eur)\n        sigma_B = np.std(x_test_eur) # ddof=0 for population std dev\n        \n        # Handle the case of zero standard deviation, although not expected here\n        if sigma_B > 1e-9:\n            z_t2 = (x_test_eur - mu_B) / sigma_B\n        else:\n            z_t2 = np.zeros_like(x_test_eur)\n            \n        y_hat_t2 = w_hat * z_t2 + c_hat\n        rmse_t2 = np.sqrt(np.mean((y_hat_t2 - y_test_true)**2))\n        results.append(rmse_t2)\n        \n        # --- Strategy T3 (currency-aware then train-statistics) ---\n        # Reconstruct the feature in the original domain (USD)\n        x_test_usd_reconstructed = x_test_eur / r\n        z_t3 = (x_test_usd_reconstructed - mu_A) / sigma_A\n        y_hat_t3 = w_hat * z_t3 + c_hat\n        rmse_t3 = np.sqrt(np.mean((y_hat_t3 - y_test_true)**2))\n        results.append(rmse_t3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3160364"}]}