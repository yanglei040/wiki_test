## 引言
在机器学习的[分类任务](@entry_id:635433)中，**[类别不平衡](@entry_id:636658)（Class Imbalance）** 是一个普遍存在且极具挑战性的问题，它指的是数据集中某个或某些类别的样本数量远少于其他类别。从金融欺诈检测到罕见疾病诊断，从工业产品缺陷识别到网络入侵监测，这些少数类样本往往是我们最关心、最希望模型能够准确识别的对象。然而，当标准的[机器学习算法](@entry_id:751585)与传统的评估指标（如准确率）直接应用于[不平衡数据](@entry_id:177545)时，往往会得到具有误导性的优异结果，而模型实际上并未学到任何有价值的信息，甚至完全忽略了稀有的少数类。

本文旨在系统性地解决这一知识鸿沟，为读者提供一套完整、深入的理解和应对[类别不平衡](@entry_id:636658)问题的方法论。我们将从根本上剖析该问题带来的挑战，并介绍一系列经过实践检验的有效策略。

- 在 **“原理与机制”** 一章中，我们将首先揭示为何准确率等传统指标在不平衡场景下会失效，并引入如[马修斯相关系数](@entry_id:176799)（MCC）、[ROC曲线](@entry_id:182055)和P[R曲线](@entry_id:183670)等更可靠的评估工具。随后，我们将深入探讨三大核心应对策略：数据层面的[重采样方法](@entry_id:144346)、算法层面的代价敏感学习以及后处理阶段的决策阈值调整。

- 接下来，在 **“应用与跨学科连接”** 一章中，我们会将这些理论知识与实际应用相结合，展示处理[类别不平衡](@entry_id:636658)的技术如何在生物信息学、医疗影像、金融风控乃至[算法公平性](@entry_id:143652)等多个交叉学科领域发挥关键作用。

- 最后，通过 **“动手实践”** 部分，您将有机会通过解决具体问题来巩固所学知识，将理论真正转化为实践能力。

通过学习本文，您将能够自信地识别、评估并解决各类[分类任务](@entry_id:635433)中的[类别不平衡](@entry_id:636658)问题，从而构建出更加稳健、可靠和有实际价值的[机器学习模型](@entry_id:262335)。

## 原理与机制

在[分类任务](@entry_id:635433)中，当一个或多个类别的样本数量远少于其他类别时，就会出现**[类别不平衡](@entry_id:636658) (class imbalance)** 问题。这种情况在现实世界的应用中非常普遍，例如在欺诈检测、罕见病诊断或工业缺陷检测中，负例（正常情况）的数量往往远超正例（感兴趣的事件）。虽然一个数据集仅仅存在[类别不平衡](@entry_id:636658)本身并不是问题，但当它与标准的机器学习模型和评估指标相结合时，就会引发一系列挑战。本章将深入探讨这些挑战背后的原理，并系统地介绍应对这些挑战的核心机制。

### [类别不平衡](@entry_id:636658)的挑战：评估指标的陷阱

在类别平衡的数据集上，**准确率 (accuracy)**，即正确分类的样本占总样本的比例，是一个直观且有效的性能度量。然而，在严重不平衡的场景下，准确率会变得极具误导性。

考虑一个[二元分类](@entry_id:142257)问题，其评估结果可以通过一个**[混淆矩阵](@entry_id:635058) (confusion matrix)** 来总结，矩阵包含四个基本计数：**真正例 (True Positives, TP)**、**假正例 (False Positives, FP)**、**真负例 (True Negatives, TN)** 和 **假负例 (False Negatives, FN)**。准确率的定义如下：

$$ Acc = \frac{TP+TN}{TP+FP+TN+FN} $$

现在，让我们设想一个用于罕见病筛查的分类器。假设在一个包含1000人的群体中，只有20人患有该疾病（正例），而其余980人是健康的（负例）。一个“平凡”的分类器，无论输入是什么，都简单地将每个人预测为健康（负例）。这个分类器的性能如下：它没有正确识别任何患者 ($TP=0$)，所有20名患者都被漏诊 ($FN=20$)。同时，它也没有将任何健康人士错误地诊断为患者 ($FP=0$)，并正确识别了所有980名健康人士 ($TN=980$)。

该分类器的准确率将是：

$$ Acc = \frac{0 + 980}{0 + 0 + 980 + 20} = \frac{980}{1000} = 0.98 $$

一个高达98%的准确率给人一种模型性能优异的印象，但实际上它完全没有识别出我们最关心的正例，因此毫无实用价值。这个例子揭示了准确率的根本缺陷：在类别极度不平衡时，它主要由模型在多数类上的表现所主导。

为了更真实地评估模型在[不平衡数据](@entry_id:177545)上的性能，我们需要一个能够同时兼顾所有四个[混淆矩阵](@entry_id:635058)元素的指标。**[马修斯相关系数](@entry_id:176799) (Matthews Correlation Coefficient, MCC)** 就是这样一个强大的指标。它将观察到的分类结果与随机猜测进行比较，可以被认为是[分类任务](@entry_id:635433)中皮尔逊积矩相关系数的一个特例。其定义为：

$$ MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} $$

MCC的取值范围在-1到+1之间。+1表示完美的预测，0表示不比随机猜测好，-1表示预测与真实情况完全相反。对于我们之前那个“平凡”的分类器，由于$TP=0$，MCC的分子为$0 - 0 \times 20 = 0$，因此$MCC=0$。这个值为0的结果准确地反映了该分类器毫无预测能力。

让我们进一步通过一个假设性对比来加深理解 [@problem_id:3127117]。在同一个包含20个正例和980个负例的数据集上，考虑两个分类器：
- 分类器 $C_1$：$TP=0, FP=1, TN=979, FN=20$。这是一个几乎总是预测负例的分类器。
- 分类器 $C_2$：$TP=15, FP=30, TN=950, FN=5$。这是一个更有意义的分类器。

我们计算它们的准确率和MCC：
- 对于 $C_1$：$Acc_{C_1} = \frac{0+979}{1000} = 0.979$，$MCC_{C_1} \approx -0.0045$。
- 对于 $C_2$：$Acc_{C_2} = \frac{15+950}{1000} = 0.965$，$MCC_{C_2} \approx 0.486$。

这个对比鲜明地揭示了问题所在：根据准确率，$C_1$ 似乎优于 $C_2$ ($0.979 > 0.965$)。然而，$C_1$ 的MC[C值](@entry_id:272975)接近于0，表明其性能与随机猜测无异，因为它完全错过了所有正例。相比之下，$C_2$ 尽管准确率稍低，但其MC[C值](@entry_id:272975)远高于0，表明它在识别正例和负例之间取得了有意义的平衡。因此，MCC更真实地反映了分类器的综合性能，尤其是在[类别不平衡](@entry_id:636658)的情况下。

### 超越单一指标：[ROC曲线](@entry_id:182055)与P[R曲线](@entry_id:183670)

除了选择合适的单一评估指标外，分析分类器在不同决策阈值下的行为也至关重要。**[ROC曲线](@entry_id:182055) (Receiver Operating Characteristic Curve)** 和 **P[R曲线](@entry_id:183670) (Precision-Recall Curve)** 是两种最常用的可视化工具。

#### [ROC曲线](@entry_id:182055)及其对类别[分布](@entry_id:182848)的不变性

[ROC曲线](@entry_id:182055)描绘的是**真正例率 (True Positive Rate, TPR)** 与**假正例率 (False Positive Rate, FPR)** 之间的权衡关系。这两个率的定义如下：
- **真正例率 (TPR)**，也称为**召回率 (Recall)** 或**灵敏度 (Sensitivity)**：$\text{TPR} = \frac{TP}{TP+FN}$。它表示在所有真实的正例中，有多少被模型成功识别。
- **假正例率 (FPR)**：$\text{FPR} = \frac{FP}{FP+TN}$。它表示在所有真实的负例中，有多少被模型错误地标记为正例。

[ROC曲线](@entry_id:182055)是通过改变分类器的决策阈值，计算出一系列 `(FPR, TPR)` 对并绘制而成的。一个理想的分类器会尽可能靠近图的左上角（FPR=0, TPR=1）。曲线下方的面积，即 **AUC-ROC (Area Under the ROC Curve)**，是衡量模型整体排序能力的一个常用指标。

[ROC曲线](@entry_id:182055)的一个关键特性是它对类别的先验概率（即类别[分布](@entry_id:182848)）**不敏感**。这是因为TPR和FPR都是在给定真实类别（$Y=1$ 或 $Y=0$）的条件下计算的比率 [@problem_id:3157158]。例如，TPR是基于真实正例群体计算的，而FPR是基于真实负例群体计算的。无论这两个群体的大小比例如何变化，只要分类器模型本身（即其分数[分布](@entry_id:182848)）不变，TPR和FPR的值就不会改变。因此，对于同一个模型，在平衡数据集和[不平衡数据集](@entry_id:637844)上绘制的[ROC曲线](@entry_id:182055)是完全相同的。

#### P[R曲线](@entry_id:183670)与[类别不平衡](@entry_id:636658)的关联

尽管[ROC曲线](@entry_id:182055)的不变性在某些情况下是有利的，但在处理严重的[类别不平衡](@entry_id:636658)问题时，它也可能掩盖关键的性能问题。这时，**P[R曲线](@entry_id:183670) (Precision-Recall Curve)** 就显得尤为重要。P[R曲线](@entry_id:183670)描绘的是**[精确率](@entry_id:190064) (Precision)** 与**召回率 (Recall)** 之间的关系。召回率即TPR，而[精确率](@entry_id:190064)的定义是：

- **[精确率](@entry_id:190064) (Precision)**，也称为**[阳性预测值](@entry_id:190064) (Positive Predictive Value, PPV)**：$\text{Precision} = \frac{TP}{TP+FP}$。它表示在所有被模型预测为正例的样本中，有多少是真正的正例。

与TPR和FPR不同，[精确率](@entry_id:190064)的计算同时涉及到了正例和负例的预测结果（TP和FP），因此它**对类别[分布](@entry_id:182848)非常敏感**。我们可以通过贝叶斯定理来明确这一点。令类别$Y=1$的[先验概率](@entry_id:275634)为$\pi = P(Y=1)$，那么[精确率](@entry_id:190064)可以表示为TPR、FPR和$\pi$的函数 [@problem_id:3157158]：

$$ \text{Precision} = \frac{P(Y=1 \mid \hat{Y}=1)}{1} = \frac{P(\hat{Y}=1 \mid Y=1)P(Y=1)}{P(\hat{Y}=1)} = \frac{\text{TPR} \cdot \pi}{\text{TPR} \cdot \pi + \text{FPR} \cdot (1-\pi)} $$

这个公式清楚地表明，[精确率](@entry_id:190064)直接依赖于先验概率$\pi$。当负例远多于正例时（即$\pi$很小，$1-\pi$接近1），即使FPR非常小，分母中的$\text{FPR} \cdot (1-\pi)$一项也可能变得很大，从而显著拉低[精确率](@entry_id:190064)。

让我们通过一个实例来理解这一点 [@problem_id:3127147]。在一个罕见[事件检测](@entry_id:162810)任务中，假设有100个正例 ($N_+$) 和100,000个负例 ($N_-$)。一个分类器在某个阈值下达到了看似优秀的性能：$TPR=0.9$ 和 $FPR=0.01$。这个点在ROC空间中非常靠近理想的左上角。然而，让我们计算实际的预测结果：
- 预测为正的真实正例数 (TP): $TPR \times N_+ = 0.9 \times 100 = 90$
- 预测为正的真实负例数 (FP): $FPR \times N_- = 0.01 \times 100000 = 1000$

此时的[精确率](@entry_id:190064)是：
$$ \text{Precision} = \frac{TP}{TP+FP} = \frac{90}{90+1000} = \frac{90}{1090} \approx 0.0826 $$

尽管[ROC曲线](@entry_id:182055)显示性能优越，但实际的[精确率](@entry_id:190064)却低得惊人，只有约8.3%。这意味着模型发出的警报中，超过91%都是“狼来了”的假警报。P[R曲线](@entry_id:183670)能够捕捉到这种性能的急剧下降，而[ROC曲线](@entry_id:182055)则会保持乐观。因此，在[类别不平衡](@entry_id:636658)的场景下，尤其当正例是关注的[焦点](@entry_id:174388)时，P[R曲线](@entry_id:183670)及其[曲线下面积](@entry_id:169174) (AUC-PR) 通常是比[ROC曲线](@entry_id:182055)更具[信息量](@entry_id:272315)、更能反映实际业务目标的评估工具。

### 应对不平衡的核心策略

既然我们已经理解了[类别不平衡](@entry_id:636658)带来的评估挑战，接下来将探讨解决这些问题的三种核心策略：**调整决策阈值**、**数据层面方法 (重采样)** 和 **算法层面方法 (代价敏感学习)**。

#### 策略一：调整决策阈值 (Threshold Moving)

许多分类模型（如逻辑回归、[神经网](@entry_id:276355)络）的原始输出是属于某个类别的概率或[置信度](@entry_id:267904)分数。标准的决策规则是，如果模型预测$P(Y=1|x) > 0.5$，则将样本$x$分类为正例。这个0.5的阈值隐含了一个假设：两个类别的先验概率相同，并且两种错误（假正例和假负例）的代价相等。在[类别不平衡](@entry_id:636658)的情况下，这些假设通常都不成立。因此，最简单直接的应对策略就是移动这个决策阈值。

##### 基于贝叶斯决策理论的最优阈值

选择最优阈值的通用框架是**贝叶斯决策理论 (Bayesian Decision Theory)** [@problem_id:3127122]。该理论旨在最小化**预期风险 (expected risk)**。假设我们有一个[代价矩阵](@entry_id:634848)$C$：

$$ C=\begin{pmatrix} C_{00} & C_{01} \\ C_{10} & C_{11} \end{pmatrix} $$

其中$C_{ij}$是将真实类别为$j$的样本预测为类别$i$的代价。通常，正确分类的代价($C_{00}, C_{11}$)为0或很小，而错误分类的代价($C_{10}$代表假正例的代价，$C_{01}$代表假负例的代价)则较高。

对于一个给定的样本$x$，模型输出其后验概率为 $p(x) = P(Y=1|x)$。
- 如果我们预测为正例($\hat{Y}=1$)，预期代价是：$R(\hat{Y}=1|x) = C_{11}p(x) + C_{10}(1-p(x))$
- 如果我们预测为负例($\hat{Y}=0$)，预期代价是：$R(\hat{Y}=0|x) = C_{01}p(x) + C_{00}(1-p(x))$

为了最小化预期风险，我们应该在$R(\hat{Y}=1|x) < R(\hat{Y}=0|x)$时预测为正例。解这个不等式，我们可以得到最优决策规则：当$p(x) > t^*$时预测为1，其中最优阈值$t^*$为：

$$ t^{*} = \frac{C_{10}-C_{00}}{(C_{10}-C_{00})+(C_{01}-C_{11})} $$

这个公式巧妙地揭示了最优阈值完全由不同决策的相对代价决定。例如，如果漏诊一个病的代价 ($C_{01}$) 远高于误诊一个健康人的代价 ($C_{10}$)，那么$t^*$将会变小，使得模型更倾向于预测正例，从而提高召回率。

##### 阈值调整与代价敏感学习的关系

调整阈值与算法层面的代价敏感学习密切相关。考虑一个使用[0-1损失函数](@entry_id:173640)的**代价加权[经验风险最小化](@entry_id:633880) (class-weighted Empirical Risk Minimization)** [@problem_id:3121459]。如果我们为类别1和类别0分别分配权重$\omega_1$和$\omega_0$，那么最小化加权预期风险的决策规则是当 $p(x) > \frac{\omega_0}{\omega_0+\omega_1}$ 时预测为1。这表明，在训练后通过设置阈值$t^* = \frac{\omega_0}{\omega_0+\omega_1}$，等价于在训练时使用权重$\omega_1$和$\omega_0$。

一个特别有启发性的结果是，当我们设置类别权重与该类别样本频率成反比时，即 $\omega_1 \propto 1/\hat{\pi}$ 和 $\omega_0 \propto 1/(1-\hat{\pi})$（其中$\hat{\pi}$是正例在[训练集](@entry_id:636396)中的比例），最优阈值恰好等于正例的经验流行率 (prevalence):

$$ \tau^{\star} = \hat{\pi} $$

例如，如果在一个数据集中，正例只占7.3% ($\hat{\pi}=0.073$)，那么最优决策阈值就不再是0.5，而应该是0.073。这会使得模型更容易将样本预测为正例，以此来补偿正例的稀有性，从而达到更好的性能平衡。

##### [生成模型](@entry_id:177561)中的隐式阈值调整

在像**[朴素贝叶斯](@entry_id:637265) (Naive Bayes)** 这样的生成模型中，类别[先验概率](@entry_id:275634)$\pi$是模型的一部分。模型的决策是基于比较后验概率$P(Y=k|x)$，这又可以通过贝叶斯定理分解为似然函数$p(x|Y=k)$和先验$P(Y=k)$的乘积。当我们计算**对数后验几率 (log-odds)** 时，先验的影响就变得非常清晰 [@problem_id:3127130]：

$$ g(x) = \ln\left(\frac{P(Y=1 \mid x)}{P(Y=0 \mid x)}\right) = \ln\left(\frac{p(x \mid Y=1)}{p(x \mid Y=0)}\right) + \ln\left(\frac{\pi}{1-\pi}\right) $$

[决策边界](@entry_id:146073)定义为$g(x)=0$。上式表明，对数[先验几率](@entry_id:176132) $\ln(\frac{\pi}{1-\pi})$ 作为一个加性偏置项直接作用于决策函数。当类别平衡时（$\pi=0.5$），该项为$\ln(1)=0$。当正例是少数类时（$\pi  0.5$），该项为负，这会“惩罚”正例的预测，使得需要更强的来自似然函数的证据才能将样本分类为正例。这相当于隐式地提高了几率的决策阈值。反过来，如果我们在一个[判别模型](@entry_id:635697)中手动降低决策阈值，就等同于在[生成模型](@entry_id:177561)中假设一个更高的正例先验概率。

#### 策略二：数据层面方法 (Resampling)

数据层面的方法通过修改[训练集](@entry_id:636396)的类别[分布](@entry_id:182848)来减轻不平衡的影响。主要有两种策略：
- **[过采样](@entry_id:270705) (Oversampling)**：增加少数类样本的数量。最简单的方法是**随机[过采样](@entry_id:270705)**，即从少数类样本中进行有放回的[随机抽样](@entry_id:175193)，直到其数量与多数类相当。更高级的方法，如**SMOTE (Synthetic Minority Over-sampling Technique)**，通过在现有少数类样本之间进行插值来创建新的合成样本。
- **[欠采样](@entry_id:272871) (Undersampling)**：减少多数类样本的数量。最简单的方法是**随机[欠采样](@entry_id:272871)**，即从多数类样本中随机丢弃一部分，直到其数量与少数类相当。

##### [重采样](@entry_id:142583)后的[概率校准](@entry_id:636701)

重采样是一个强大而直接的方法，但它带来一个重要的理论后果：它改变了训练数据的先验概率[分布](@entry_id:182848)。假设原始数据集的先验为$\pi=P(Y=1)$，而经过[重采样](@entry_id:142583)后，[训练集](@entry_id:636396)中的有效先验变为$\pi^*$。一个在该[重采样](@entry_id:142583)数据集上训练并校准良好的模型，其输出$p^*(Y=1|x)$是对新的、人为改变过的[分布](@entry_id:182848)的[后验概率](@entry_id:153467)的估计，而不是我们关心的真实[分布](@entry_id:182848)的[后验概率](@entry_id:153467)。

幸运的是，只要类条件密度 $p(x|Y=y)$ 在重采样过程中保持不变（例如，随机重采样或SMOTE都满足此条件），我们就可以对模型的输出进行校准，以恢复真实的后验概率 [@problem_id:3127098]。真实后验几率$O(x)$和模型输出的后验几率$O^*(x)$之间的关系为：

$$ O(x) = \frac{P(Y=1|x)}{1-P(Y=1|x)} = O^*(x) \cdot \frac{\pi(1-\pi^*)}{\pi^*(1-\pi)} $$

通过这个关系，我们可以解出真实的[后验概率](@entry_id:153467)$P(Y=1|x)$：

$$ P(Y=1 \mid x) = \frac{\pi(1-\pi^{*})p^{*}(Y=1 \mid x)}{\pi^{*}(1-\pi) + (\pi - \pi^{*})p^{*}(Y=1 \mid x)} $$

这个公式对于需要获得准确概率估计（而不仅仅是分类决策）的应用至关重要，例如在风险建模或需要结合下游决策理论的场景中。

##### [重采样](@entry_id:142583)对[深度学习模型](@entry_id:635298)的副作用

在[深度学习](@entry_id:142022)中，[重采样](@entry_id:142583)可能会与某些模型组件产生意想不到的交互。例如，**[批量归一化](@entry_id:634986) (Batch Normalization, BN)** 在每个训练批次中计算特征的均值和[方差](@entry_id:200758)，并用它们来归一化该批次的特征。BN的有效性依赖于批次统计量是对整个数据集统计量的[无偏估计](@entry_id:756289)。

然而，[过采样](@entry_id:270705)会改变每个批次内的类别构成。在一个严重不平衡的数据集上，原始批次中绝大多数样本来自多数类。而在[过采样](@entry_id:270705)后，批次中的少数类样本比例会显著增加。如果不同类别的特征[分布](@entry_id:182848)不同（即它们的均值$\mu_k$或[方差](@entry_id:200758)$\sigma_k^2$不同），这种构成的改变将系统性地改变批次的均值和[方差](@entry_id:200758)的[期望值](@entry_id:153208) [@problem_id:3127139]。例如，批次均值的[期望值](@entry_id:153208)是各类别均值的加权平均：$\mathbb{E}[\mu_B] = w \mu_0 + (1 - w) \mu_1$，其中$w$是批次中类别0的比例。[过采样](@entry_id:270705)改变了$w$，从而改变了$\mathbb{E}[\mu_B]$。这种统计量的漂移可能会影响BN层的行为和模型的训练动态，这是在实践中应用重采样于深度网络时需要考虑的一个微妙之处。

#### 策略三：算法层面方法 (Cost-Sensitive Learning)

算法层面的方法直接修改模型的学习过程，使其更加关注少数类。最常见的方法是**代价敏感学习 (cost-sensitive learning)**，通常通过**[类别加权](@entry_id:635159) (class weighting)** 来实现。

我们在讨论阈值移动时已经接触了其背后的思想。这里的区别在于，我们将权重直接整合到[损失函数](@entry_id:634569)中，在训练的每一步都发挥作用。对于样本$(x_i, y_i)$，其对总损失的贡献将被乘以一个与其类别$y_i$对应的权重$c_{y_i}$。

以**逻辑回归**为例，其加权的单样本[损失函数](@entry_id:634569)为 $L = -c_y [y \ln(\sigma(z)) + (1-y) \ln(1-\sigma(z))]$，其中 $z=\theta^\top x$。通过求导，我们可以得到[损失函数](@entry_id:634569)关于模型参数$\theta$的梯度 [@problem_id:3127146]：

$$ \nabla_{\theta} L = c_y(\sigma(z)-y)x $$

这个公式清晰地展示了类别权重$c_y$的作用机制。它直接缩放了梯度的大小。$(\sigma(z)-y)$项是预测误差。因此，权重$c_y$放大了来自特定类别样本的“[纠错](@entry_id:273762)信号”。通常，我们会给少数类一个比多数类大得多的权重（例如，与类别频率成反比）。这样一来，即使少数类的样本数量很少，它们在[梯度下降](@entry_id:145942)的每一步中都能产生足够大的影响，迫使模型更加努力地学习如何正确分类它们，防止模型简单地将所有样本预测为多数类。

#### 扩展议题：[类别不平衡](@entry_id:636658)与[算法公平性](@entry_id:143652)

处理[类别不平衡](@entry_id:636658)的原则和技术与**[算法公平性](@entry_id:143652) (algorithmic fairness)** 领域有着深刻的联系。在许多社会性应用中（如贷款审批、招聘筛选），不同的人口群体（例如按种族或性别划分）可能在结果（如违约率、成功率）上存在不同的**基准率 (base rates)**，这本质上就是一种群体间的[类别不平衡](@entry_id:636658) [@problem_id:3127143]。

例如，假设一个分类器在两个群体$A$和$B$中都满足**[均等化赔率](@entry_id:637744) (equalized odds)**，这意味着两个群体享有相同的TPR和FPR。这通常被认为是一个强公平性标准。然而，如果这两个群体的基准率不同（例如$\pi_B > \pi_A$），那么即使满足了[均等化赔率](@entry_id:637744)，其他重要的[公平性指标](@entry_id:634499)也可能会出现差异。
- **人均假负例率 (per-capita false negative rate)**: $\mathbb{P}(\hat{Y}=0, Y=1 | G=g) = \text{FNR}_g \cdot \pi_g$。由于$\pi_B > \pi_A$，即使FNR相同，群体$B$中符合条件但被错误拒绝的个体总比例也会更高。
- **[阳性预测值](@entry_id:190064) (PPV)**: 正如我们之前所见，PPV是基准率的增函数。因此，$\pi_B > \pi_A$ 意味着 $\text{PPV}_B > \text{PPV}_A$。这意味着来自群体$B$的被预测为正例的个体，其“含金量”更高，这可能导致下游决策中的不公平对待。

这些观察表明，不存在一个能够同时满足所有公平性标准的“万能”分类器。处理[类别不平衡](@entry_id:636658)的技术，如调整阈值或重加权，实际上是在不同的性能指标（如[精确率](@entry_id:190064)与召回率）和不同的公平性标准之间做出权衡。对这些权衡的理解，不仅是技术上的要求，也是负责任地部署机器学习系统所必需的。