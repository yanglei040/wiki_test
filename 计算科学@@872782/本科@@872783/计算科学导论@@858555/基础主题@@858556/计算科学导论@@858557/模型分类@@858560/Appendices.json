{"hands_on_practices": [{"introduction": "理论与实践相结合是掌握科学概念的最佳途径。在许多科学领域，如显微镜学或天文学中，我们测量的是离散事件（如光子到达），这本质上是随机的。然而，使用连续的、确定性的信号（如光强度）进行分析通常更为便捷。本练习探讨了在何种条件下这种关键的近似是有效的，并使用变异系数作为实用指南，为连接基本的离散随机模型（泊松过程）与其连续确定性模型搭建了桥梁。", "problem": "荧光显微镜中的单个像素在曝光窗口 $\\left[0, T\\right]$ 内记录光子的到达。计数探测到的光子数量的离散随机变量 $N$ 被建模为参数为 $\\lambda(T)$ 的泊松随机变量，其中泊松参数是该窗口内预期的到达数量。该仪器由一个具有确定性、连续光子通量（强度）$I(t)$（每秒到达像素的光子数）的光源照射，探测器的量子效率为 $\\eta$（一个到达的光子被探测到的概率）。因此，瞬时到达率为 $r(t) = \\eta I(t)$，一次曝光的预期计数为 $\\lambda(T) = \\int_{0}^{T} r(t)\\, dt = \\eta \\int_{0}^{T} I(t)\\, dt$。显微镜供应商建议，当变异系数 (CV)（定义为 $\\mathrm{CV} = \\sqrt{\\mathrm{Var}(N)}/\\mathbb{E}[N]$）低于容差 $\\varepsilon$ 时，使用连续确定性模型来描述信号。\n\n根据离散与连续、确定性与随机性这些类别，对 $N$ 的模型和 $I(t)$ 的模型进行分类。然后，仅从泊松过程的基本性质 $\\mathbb{E}[N] = \\lambda(T)$ 和 $\\mathrm{Var}(N) = \\lambda(T)$，以及给定的变异系数定义出发，推导出一个关于曝光时间 $T$ 的判据，在该判据下连续确定性近似是可接受的，并为以下工作点计算满足该判据的最小曝光时间 $T_{\\min}$：\n- 在曝光期间，通量是恒定的，$I(t) = I_{0}$，其中 $I_{0} = 8.0 \\times 10^{4}$ 光子/秒。\n- 探测器效率为 $\\eta = 0.60$。\n- 容差为 $\\varepsilon = 0.020$。\n将 $T_{\\min}$ 的最终数值答案四舍五入到三位有效数字。最终时间以秒为单位表示。", "solution": "按要求，该问题将分两部分解决：首先，对光子计数 $N$ 和入射通量 $I(t)$ 的模型进行分类；其次，推导曝光时间 $T$ 的判据并计算最小曝光时间 $T_{\\min}$。\n\n首先，我们验证问题陈述的有效性。\n**第1步：提取已知条件**\n- 曝光窗口：$\\left[0, T\\right]$\n- 光子计数：$N$，一个离散随机变量\n- $N$ 的模型：参数为 $\\lambda(T)$ 的泊松随机变量\n- 光子通量：$I(t)$，一个确定性的连续函数\n- 探测器量子效率：$\\eta$\n- 瞬时到达率：$r(t) = \\eta I(t)$\n- 预期计数（泊松参数）：$\\lambda(T) = \\int_{0}^{T} r(t)\\, dt = \\eta \\int_{0}^{T} I(t)\\, dt$\n- 连续确定性近似的条件：变异系数 $\\mathrm{CV} \\le \\varepsilon$\n- 变异系数的定义：$\\mathrm{CV} = \\sqrt{\\mathrm{Var}(N)}/\\mathbb{E}[N]$\n- $N$ 的泊松分布基本性质：$\\mathbb{E}[N] = \\lambda(T)$ 和 $\\mathrm{Var}(N) = \\lambda(T)$\n- 工作点参数：\n    - 恒定通量：$I(t) = I_{0}$，其中 $I_{0} = 8.0 \\times 10^{4}$ 光子/秒\n    - 探测器效率：$\\eta = 0.60$\n    - 容差：$\\varepsilon = 0.020$\n- 要求：将 $T_{\\min}$ 的最终数值答案四舍五入到三位有效数字。\n\n**第2步：使用提取的已知条件进行验证**\n该问题具有科学依据，描述了光子探测的标准泊松过程模型，这在光学和成像领域是基础性的。所有术语都定义清晰，它们之间的关系（$\\lambda(T)$, $r(t)$, $I(t)$）在物理上是正确的。该问题是适定的，提供了推导判据和计算最终值所需的所有信息。语言客观且无歧义。所提供的数值在物理上是合理的。因此，该问题被认为是**有效的**。\n\n**第1部分：模型分类**\n问题陈述直接给出了分类。\n- 探测到的光子数 $N$ 的模型被描述为一个“离散随机变量”。这意味着 $N$ 的状态空间是离散的（非负整数 $\\{0, 1, 2, ...\\}$），其演化是随机的（由一个概率分布，具体来说是泊松分布所支配）。因此，$N$ 的模型是**离散且随机的**。\n- 光子通量 $I(t)$ 的模型被描述为一个“确定性的、连续的光子通量”。这意味着其状态空间是连续的（非负实数），其演化是确定性的（在任何时间 $t$ 的值都是确定的）。因此，$I(t)$ 的模型是**连续且确定性的**。\n\n**第2部分：最小曝光时间的推导与计算**\n连续确定性近似可接受的条件是变异系数 $\\mathrm{CV}$ 低于或等于容差 $\\varepsilon$。\n$$\n\\mathrm{CV} \\le \\varepsilon\n$$\n代入 $\\mathrm{CV}$ 的定义：\n$$\n\\frac{\\sqrt{\\mathrm{Var}(N)}}{\\mathbb{E}[N]} \\le \\varepsilon\n$$\n问题指出 $N$ 服从参数为 $\\lambda(T)$ 的泊松分布。对于泊松随机变量，其期望值和方差都等于该参数：\n$$\n\\mathbb{E}[N] = \\lambda(T)\n$$\n$$\n\\mathrm{Var}(N) = \\lambda(T)\n$$\n将这些性质代入不等式，得到：\n$$\n\\frac{\\sqrt{\\lambda(T)}}{\\lambda(T)} \\le \\varepsilon\n$$\n由于对于非零信号，$\\lambda(T)$ 必须为正，我们可以将其简化为：\n$$\n\\frac{1}{\\sqrt{\\lambda(T)}} \\le \\varepsilon\n$$\n因为两边都为正，我们可以在取倒数后对不等式两边平方，并反转不等式方向：\n$$\n\\sqrt{\\lambda(T)} \\ge \\frac{1}{\\varepsilon} \\implies \\lambda(T) \\ge \\frac{1}{\\varepsilon^2}\n$$\n现在我们使用 $\\lambda(T)$ 在恒定通量 $I(t) = I_0$ 特定情况下的定义：\n$$\n\\lambda(T) = \\eta \\int_{0}^{T} I(t)\\, dt = \\eta \\int_{0}^{T} I_0\\, dt = \\eta I_0 T\n$$\n将此代入我们推导出的不等式，即可得到关于曝光时间 $T$ 的判据：\n$$\n\\eta I_0 T \\ge \\frac{1}{\\varepsilon^2}\n$$\n为了求得最小曝光时间 $T_{\\min}$，我们求解此不等式的边界条件：\n$$\n\\eta I_0 T_{\\min} = \\frac{1}{\\varepsilon^2}\n$$\n$$\nT_{\\min} = \\frac{1}{\\eta I_0 \\varepsilon^2}\n$$\n现在我们代入给定的数值：\n- $\\eta = 0.60$\n- $I_{0} = 8.0 \\times 10^{4} \\, \\text{s}^{-1}$\n- $\\varepsilon = 0.020 = 2.0 \\times 10^{-2}$\n\n首先，我们计算各项：\n$$\n\\varepsilon^2 = (2.0 \\times 10^{-2})^2 = 4.0 \\times 10^{-4}\n$$\n$$\n\\eta I_0 = (0.60) \\times (8.0 \\times 10^{4} \\, \\text{s}^{-1}) = 4.8 \\times 10^{4} \\, \\text{s}^{-1}\n$$\n$$\n\\eta I_0 \\varepsilon^2 = (4.8 \\times 10^{4} \\, \\text{s}^{-1}) \\times (4.0 \\times 10^{-4}) = 19.2 \\, \\text{s}^{-1}\n$$\n最后，我们计算 $T_{\\min}$：\n$$\nT_{\\min} = \\frac{1}{19.2 \\, \\text{s}^{-1}} \\approx 0.0520833... \\, \\text{s}\n$$\n问题要求答案四舍五入到三位有效数字。前三位有效数字是 $5$、$2$ 和 $0$。第四位是 $8$，所以我们将第三位数字向上取整。\n$$\nT_{\\min} \\approx 0.0521 \\, \\text{s}\n$$\n用科学记数法表示，即为 $5.21 \\times 10^{-2} \\, \\text{s}$。", "answer": "$$\n\\boxed{5.21 \\times 10^{-2}}\n$$", "id": "3160694"}, {"introduction": "当面对真实世界的数据时，我们如何在根本不同的模型类型之间做出选择，例如，一个连续时间的确定性常微分方程（ODE）模型与一个离散时间的随机过程模型？这项高级实践介绍了一种强大且广泛应用的方法：通过赤池信息量准则（Akaike Information Criterion, AIC）进行模型选择。您将实现一个计算流程，将两种模型拟合到数据上，并使用 AIC 来判断哪个模型为观测数据提供了更好的解释，从而在拟合优度与模型复杂度之间取得平衡。", "problem": "给定一个以采样间隔 $\\Delta t$ 均匀采样的标量时间序列。你的任务是为每个数据集判断，是连续时间确定性模型还是离散时间随机模型能更好地解释数据。两个候选模型是：\n\n1. 一个连续时间确定性常微分方程 (ODE)：$$\\frac{dx}{dt} = -k\\,x^3,$$ 使用前向欧拉更新进行离散化以用于拟合，\n$$x_{n+1} \\approx x_n + \\Delta t\\left(-k\\,x_n^3\\right),$$\n并在离散更新残差上加上一个加性高斯建模误差。\n\n2. 一个离散时间随机一阶自回归过程 (AR(1))：$$x_{n+1} = \\phi\\,x_n + \\eta_n,$$ 其中 $\\eta_n \\sim \\mathcal{N}(0,\\sigma_a^2)$ 是独立高斯噪声。\n\n你必须实现一个有原则的模型选择流程，该流程基于高斯假设下的最大似然估计 (MLE) 和 Akaike 信息准则 (AIC)。更准确地说，对于每个数据集：\n- 使用基于数据对 $(x_n,x_{n+1})$（其中 $n=0,\\dots,N-2$）的条件最大似然估计来估计两个模型的参数。\n- 对于基于 ODE 的模型，将残差 $x_{n+1} - \\left(x_n + \\Delta t(-k\\,x_n^3)\\right)$ 视为待估计的、方差为 $\\sigma_c^2$ 的独立高斯建模误差。\n- 对于 AR(1) 模型，将残差 $x_{n+1} - \\phi x_n$ 视为待估计的、方差为 $\\sigma_a^2$ 的独立高斯过程噪声。\n- 在高斯残差假设下，计算每个模型的最大化对数似然，然后使用涉及自由参数数量和最大化对数似然的标准定义计算每个模型的 $AIC$ 值。\n- 选择 $AIC$ 值较小的模型（越低越好）。如果 $AIC$ 值在小于 $10^{-6}$ 的容差范围内数值相等，则选择离散时间随机 AR(1) 模型来打破平局。\n\n你的程序必须解决以下测试套件，其中包含多样的参数设置，以覆盖典型情况、边界行为和符号变化。在所有情况下，时间都是无量纲的，不需要物理单位。\n\n内部生成的测试套件数据集：\n- 情况 $1$ (连续时间确定性 ODE, “理想路径”)：$N=200$, $\\Delta t=0.05$, $k=0.5$, $\\sigma_c=0.05$, $x_0=1.0$。\n- 情况 $2$ (离散时间随机 AR(1), “理想路径”)：$N=200$, $\\Delta t=0.05$, $\\phi=0.8$, $\\sigma_a=0.2$, $x_0=0.0$。\n- 情况 $3$ (离散时间随机 AR(1), 接近单位根边界)：$N=300$, $\\Delta t=0.05$, $\\phi=0.99$, $\\sigma_a=0.05$, $x_0=0.0$。\n- 情况 $4$ (离散时间随机 AR(1), 负系数)：$N=200$, $\\Delta t=0.05$, $\\phi=-0.5$, $\\sigma_a=0.2$, $x_0=0.0$。\n- 情况 $5$ (连续时间确定性 ODE，更高噪声)：$N=200$, $\\Delta t=0.1$, $k=0.9$, $\\sigma_c=0.2$, $x_0=1.2$。\n\n数据生成细节：\n- 对于 ODE 情况，使用前向欧拉更新 $x_{n+1}^{\\text{true}} = x_n - \\Delta t\\,k\\,x_n^3$，然后对下一个样本添加高斯噪声：$x_{n+1} = x_{n+1}^{\\text{true}} + \\varepsilon_n$，其中 $\\varepsilon_n \\sim \\mathcal{N}(0,\\sigma_c^2)$。\n- 对于 AR(1) 情况，使用 $x_{n+1} = \\phi\\,x_n + \\eta_n$，其中 $\\eta_n \\sim \\mathcal{N}(0,\\sigma_a^2)$。\n\n最终输出规范：\n- 对于每个数据集，输出一个整数分类：如果选择连续时间确定性 ODE 模型，则输出 $0$；如果选择离散时间随机 AR(1) 模型，则输出 $1$。\n- 你的程序应生成单行输出，其中包含五个测试用例的结果，形式为用方括号括起来的逗号分隔列表，例如 $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$。", "solution": "该问题要求针对给定的时间序列数据集，在两个候选模型之间进行有原则的模型选择：一个是基于非线性常微分方程 (ODE) 的连续时间确定性模型，另一个是离散时间随机模型，即一阶自回归 (AR(1)) 过程。选择过程将通过参数的最大似然估计 (MLE) 以及随后通过 Akaike 信息准则 (AIC) 进行比较来完成。\n\n设给定的标量时间序列表示为 $\\{x_n\\}_{n=0}^{N-1}$，以均匀间隔 $\\Delta t$ 采样。模型拟合和似然计算将基于 $n = 0, \\dots, N-2$ 的 $M = N-1$ 个转移对 $(x_n, x_{n+1})$。\n\n### 模型 1：连续时间确定性 ODE\n\n第一个模型假定其潜在动力学遵循 ODE $\\frac{dx}{dt} = -k\\,x^3$。我们使用前向欧拉离散化来关联连续样本：$x_{n+1} \\approx x_n + \\Delta t (-k\\,x_n^3)$。这个确定性预测与观测数据之间的差异归因于一个独立同分布 (i.i.d.) 的高斯建模误差 $\\varepsilon_n \\sim \\mathcal{N}(0, \\sigma_c^2)$。因此，观测数据的完整模型为：\n$$x_{n+1} = x_n - \\Delta t\\,k\\,x_n^3 + \\varepsilon_n$$\n待估计的参数是速率常数 $k$ 和误差方差 $\\sigma_c^2$。\n\n在给定 $x_n$ 的条件下观测到 $x_{n+1}$ 的条件概率由一个高斯概率密度函数描述：\n$$p(x_{n+1}|x_n; k, \\sigma_c^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_c^2}} \\exp\\left(-\\frac{(x_{n+1} - (x_n - \\Delta t\\,k\\,x_n^3))^2}{2\\sigma_c^2}\\right)$$\n假设误差 $\\varepsilon_n$ 是独立的，观测序列的总条件对数似然 $\\mathcal{L}_c$ 是各个对数似然之和：\n$$\\mathcal{L}_c(k, \\sigma_c^2) = \\sum_{n=0}^{N-2} \\ln p(x_{n+1}|x_n; k, \\sigma_c^2) = -\\frac{N-1}{2}\\ln(2\\pi\\sigma_c^2) - \\frac{1}{2\\sigma_c^2}\\sum_{n=0}^{N-2}(x_{n+1} - x_n + \\Delta t\\,k\\,x_n^3)^2$$\n为了找到 $k$ 的最大似然估计（记为 $\\hat{k}$），我们必须最小化残差平方和项。令 $y_n = x_n - x_{n+1}$ 和 $z_n = \\Delta t\\,x_n^3$。我们需要最小化 $\\sum_{n=0}^{N-2} (y_n - k\\,z_n)^2$。这是一个标准的线性回归问题，其解为：\n$$\\hat{k} = \\frac{\\sum_{n=0}^{N-2} y_n z_n}{\\sum_{n=0}^{N-2} z_n^2} = \\frac{\\sum_{n=0}^{N-2} (x_n - x_{n+1})(\\Delta t\\,x_n^3)}{\\sum_{n=0}^{N-2} (\\Delta t\\,x_n^3)^2} = \\frac{\\sum_{n=0}^{N-2} (x_n - x_{n+1})x_n^3}{\\Delta t \\sum_{n=0}^{N-2} x_n^6}$$\n方差 $\\sigma_c^2$ 的最大似然估计是使用估计出的 $\\hat{k}$ 计算的残差平方的均值：\n$$\\hat{\\sigma}_c^2 = \\frac{1}{N-1}\\sum_{n=0}^{N-2}(x_{n+1} - (x_n - \\Delta t\\,\\hat{k}\\,x_n^3))^2$$\n将这些估计值代回对数似然函数，得到最大化对数似然 $\\hat{\\mathcal{L}}_c$：\n$$\\hat{\\mathcal{L}}_c = -\\frac{N-1}{2}\\left(\\ln(2\\pi\\hat{\\sigma}_c^2) + 1\\right)$$\nAIC 使用公式 $AIC = 2p - 2\\hat{\\mathcal{L}}$ 计算，其中 $p$ 是估计参数的数量。对于这个模型，$p=2$（对应 $k$ 和 $\\sigma_c^2$）。\n$$AIC_c = 2(2) - 2\\hat{\\mathcal{L}}_c = 4 - 2\\hat{\\mathcal{L}}_c$$\n\n### 模型 2：离散时间随机 AR(1) 过程\n\n第二个模型是一个离散时间一阶自回归过程：\n$$x_{n+1} = \\phi\\,x_n + \\eta_n$$\n其中 $\\eta_n \\sim \\mathcal{N}(0, \\sigma_a^2)$ 是独立同分布的高斯白噪声。待估计的参数是自回归系数 $\\phi$ 和噪声方差 $\\sigma_a^2$。\n\n其结构与第一个模型类似。条件概率密度为：\n$$p(x_{n+1}|x_n; \\phi, \\sigma_a^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} \\exp\\left(-\\frac{(x_{n+1} - \\phi\\,x_n)^2}{2\\sigma_a^2}\\right)$$\n总条件对数似然 $\\mathcal{L}_a$ 为：\n$$\\mathcal{L}_a(\\phi, \\sigma_a^2) = -\\frac{N-1}{2}\\ln(2\\pi\\sigma_a^2) - \\frac{1}{2\\sigma_a^2}\\sum_{n=0}^{N-2}(x_{n+1} - \\phi\\,x_n)^2$$\n$\\phi$ 的最大似然估计（记为 $\\hat{\\phi}$）是通过最小化残差平方和 $\\sum(x_{n+1} - \\phi\\,x_n)^2$ 来找到的，这会得到标准的普通最小二乘 (OLS) 估计量：\n$$\\hat{\\phi} = \\frac{\\sum_{n=0}^{N-2} x_{n+1} x_n}{\\sum_{n=0}^{N-2} x_n^2}$$\n方差 $\\sigma_a^2$ 的最大似然估计同样是残差平方的均值：\n$$\\hat{\\sigma}_a^2 = \\frac{1}{N-1}\\sum_{n=0}^{N-2}(x_{n+1} - \\hat{\\phi}\\,x_n)^2$$\n最大化对数似然 $\\hat{\\mathcal{L}}_a$ 具有与之前相同的形式：\n$$\\hat{\\mathcal{L}}_a = -\\frac{N-1}{2}\\left(\\ln(2\\pi\\hat{\\sigma}_a^2) + 1\\right)$$\n参数数量同样为 $p=2$（对应 $\\phi$ 和 $\\sigma_a^2$），得到的 AIC 值为：\n$$AIC_a = 2(2) - 2\\hat{\\mathcal{L}}_a = 4 - 2\\hat{\\mathcal{L}}_a$$\n\n### 模型选择\n\n对于每个数据集，我们计算 $AIC_c$ 和 $AIC_a$。AIC 值较低的模型被认为能更好地拟合数据，因为它在拟合优度（最大化似然）和模型复杂度（参数数量）之间提供了更好的权衡。选择规则是：\n- 如果 $AIC_c  AIC_a - 10^{-6}$，选择连续时间 ODE 模型（输出 $0$）。\n- 否则，包括平局的情况（其中 $|AIC_c - AIC_a| \\le 10^{-6}$），选择离散时间 AR(1) 模型（输出 $1$）。\n\n这个过程提供了一个定量且客观的方法，用于根据给定的时间序列数据来区分这兩種提出的模型结构。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform model selection for the given test suite.\n    \"\"\"\n    \n    # Set a random seed for reproducibility of the generated data.\n    np.random.seed(0)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'id': 1, 'type': 'ode', 'N': 200, 'dt': 0.05, 'x0': 1.0, 'k': 0.5, 'sigma': 0.05},\n        {'id': 2, 'type': 'ar1', 'N': 200, 'dt': 0.05, 'x0': 0.0, 'phi': 0.8, 'sigma': 0.2},\n        {'id': 3, 'type': 'ar1', 'N': 300, 'dt': 0.05, 'x0': 0.0, 'phi': 0.99, 'sigma': 0.05},\n        {'id': 4, 'type': 'ar1', 'N': 200, 'dt': 0.05, 'x0': 0.0, 'phi': -0.5, 'sigma': 0.2},\n        {'id': 5, 'type': 'ode', 'N': 200, 'dt': 0.1, 'x0': 1.2, 'k': 0.9, 'sigma': 0.2}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Step 1: Generate time series data\n        x = generate_data(case)\n        N = case['N']\n        dt = case['dt']\n        \n        # Step 2: Fit both models and get their estimated parameters\n        k_hat, sigma_c_sq_hat = fit_ode_model(x, dt)\n        phi_hat, sigma_a_sq_hat = fit_ar1_model(x)\n\n        # Step 3: Compute AIC for both models\n        # The number of data points for likelihood is N-1\n        num_points = N - 1\n        # Number of parameters is 2 for both models (k, sigma_c^2) and (phi, sigma_a^2)\n        num_params = 2\n        \n        aic_c = calculate_aic(sigma_c_sq_hat, num_points, num_params)\n        aic_a = calculate_aic(sigma_a_sq_hat, num_points, num_params)\n\n        # Step 4: Compare AIC values and select the model\n        # Tie-breaking rule: if AICs are close, choose AR(1) model.\n        # This translates to: choose ODE only if its AIC is strictly smaller.\n        # The problem specifies a tolerance of 10^-6.\n        if aic_c  aic_a - 1e-6:\n            results.append(0)  # ODE model is selected\n        else:\n            results.append(1)  # AR(1) model is selected\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef generate_data(params):\n    \"\"\"\n    Generates time series data based on the specified model and parameters.\n    \"\"\"\n    N = params['N']\n    dt = params['dt']\n    x0 = params['x0']\n    sigma = params['sigma']\n    \n    x = np.zeros(N)\n    x[0] = x0\n    \n    if params['type'] == 'ode':\n        k = params['k']\n        for n in range(N - 1):\n            x_true_next = x[n] - dt * k * x[n]**3\n            x[n+1] = x_true_next + np.random.normal(0, sigma)\n    elif params['type'] == 'ar1':\n        phi = params['phi']\n        for n in range(N - 1):\n            x[n+1] = phi * x[n] + np.random.normal(0, sigma)\n            \n    return x\n\ndef fit_ode_model(x, dt):\n    \"\"\"\n    Fits the discretized ODE model to the data using MLE.\n    Returns estimated k and estimated residual variance.\n    \"\"\"\n    N = len(x)\n    x_curr = x[:-1]\n    x_next = x[1:]\n    \n    # MLE for k\n    numerator = np.sum((x_curr - x_next) * (x_curr**3))\n    denominator = dt * np.sum(x_curr**6)\n    \n    k_hat = numerator / denominator if denominator != 0 else 0.0\n\n    # Calculate residuals and MLE for variance\n    residuals = x_next - (x_curr - dt * k_hat * x_curr**3)\n    sigma_c_sq_hat = np.mean(residuals**2)\n    \n    return k_hat, sigma_c_sq_hat\n\ndef fit_ar1_model(x):\n    \"\"\"\n    Fits the AR(1) model to the data using MLE.\n    Returns estimated phi and estimated residual variance.\n    \"\"\"\n    x_curr = x[:-1]\n    x_next = x[1:]\n\n    # MLE for phi\n    numerator = np.sum(x_next * x_curr)\n    denominator = np.sum(x_curr**2)\n    \n    phi_hat = numerator / denominator if denominator != 0 else 0.0\n    \n    # Calculate residuals and MLE for variance\n    residuals = x_next - phi_hat * x_curr\n    sigma_a_sq_hat = np.mean(residuals**2)\n\n    return phi_hat, sigma_a_sq_hat\n\ndef calculate_aic(sigma_sq_hat, num_points, num_params):\n    \"\"\"\n    Calculates the Akaike Information Criterion (AIC).\n    \"\"\"\n    # Defensive check for non-positive variance which would make log invalid\n    if sigma_sq_hat = 0:\n        return np.inf\n\n    # Maximized log-likelihood for Gaussian residuals\n    log_likelihood = -num_points / 2.0 * (np.log(2 * np.pi * sigma_sq_hat) + 1.0)\n    \n    # Standard AIC formula\n    aic = 2 * num_params - 2 * log_likelihood\n    \n    return aic\n\nsolve()\n```", "id": "3160633"}]}