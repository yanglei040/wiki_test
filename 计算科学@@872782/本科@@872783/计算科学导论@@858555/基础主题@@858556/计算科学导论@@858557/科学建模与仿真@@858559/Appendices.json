{"hands_on_practices": [{"introduction": "计算科学中的许多问题，从统计物理到金融建模，都归结为对高维积分的求值。当解析解不存在时，数值积分便成为必不可少的工具。本练习将带您探索并比较两种强大的数值积分技术：传统的蒙特卡洛（MC）方法和准蒙特卡洛（QMC）方法。通过亲手实现这两种方法并凭经验估计它们的收敛速度，您将深入理解为何使用低差异序列（如Sobol序列）的QMC方法通常能以更少的样本点获得更高的精度，这正是科学计算中效率与准确性权衡的一个绝佳范例。[@problem_id:3190544]", "problem": "要求您在一个有原则的建模和模拟框架内，研究使用低差异序列的拟蒙特卡罗积分。考虑 $d$ 维积分\n$$\nI(d) \\;=\\; \\int_{[0,1]^d} \\,\\prod_{i=1}^{d} \\cos(\\pi x_i)\\, dx,\n$$\n其中余弦函数的参数使用弧度制。您的任务是为 $I(d)$ 实现两种数值估计器：一种是基于独立同分布均匀样本的普通蒙特卡罗估计器，另一种是基于 Sobol 低差异序列的拟蒙特卡罗估计器。您将根据样本数量 $M$ 凭经验比较它们的收敛速度。\n\n使用的基本原理：大数定律保证了基于 $M$ 个独立同分布样本的普通蒙特卡罗估计器 $\\hat{I}_M$ 收敛于真实积分值，其典型的均方根误差与样本大小的负幂成比例。对于低差异序列，Koksma–Hlawka 不等式将积分误差与被积函数的变化以及点集的差异度联系起来，为拟蒙特卡罗方法的优势提供了理论基础。不要假设任何特定的误差常数或闭式误差率；您必须从模拟输出中估计经验收敛指数。\n\n将给定维度 $d$ 和样本大小 $M$ 的估计器定义为\n$$\n\\hat{I}_M(d) \\;=\\; \\frac{1}{M}\\sum_{j=1}^{M}\\,\\prod_{i=1}^{d}\\cos\\!\\big(\\pi\\, x_{i}^{(j)}\\big),\n$$\n其中每个 $x^{(j)} \\in [0,1]^d$ 要么是普通蒙特卡罗均匀点，要么是拟蒙特卡罗 Sobol 点。绝对积分误差为 $E_M(d) = \\big|\\,\\hat{I}_M(d) - I(d)\\,\\big|$。您必须通过拟合对数-对数关系中的斜率来估计经验收敛指数 $p(d)$\n$$\n\\log(E_M(d)) \\;\\approx\\; a(d) + p(d)\\,\\log(M),\n$$\n该拟合基于一组指定的 $M$ 值，并报告斜率 $p(d)$。\n\n使用以下科学上真实的事实，无需重新推导：(i) $\\int_0^1 \\cos(\\pi x)\\,dx = 0$，(ii) 可分离乘积函数在超立方体上的积分可分解为一维积分的乘积，因此 $I(d) = \\prod_{i=1}^{d}\\int_0^1 \\cos(\\pi x_i)\\,dx_i$。\n\n角度单位：所有三角函数求值均使用弧度。\n\n数值稳定性：在对误差取对数时，确保不会计算 $\\log(0)$；您必须通过添加一个在所考虑尺度上不影响结果的无穷小正数来保证计算的稳健性。\n\n为确保覆盖范围的测试套件和参数化：\n- 维度 $d \\in \\{\\,1,\\,5,\\,20\\,\\}$，以覆盖低维、中维和高维行为。\n- 样本大小限制为2的幂，以符合 Sobol 序列的平衡特性：$M \\in \\{\\,2^m : m \\in \\{\\,5,\\,6,\\,7,\\,8,\\,9,\\,10\\}\\,\\}$，即 $M \\in \\{\\,32,\\,64,\\,128,\\,256,\\,512,\\,1024\\,\\}$。\n- 对于拟蒙特卡罗估计器，使用一个在 $d$ 维空间中、带有数字置乱且固定种子为 $s_{\\mathrm{QMC}}=2024$ 的 Sobol 序列。对于每个测试对 $(d, M)$，通过与2的幂样本大小对齐的以2为基的机制，精确生成 $M$ 个 Sobol 点。\n- 对于普通蒙特卡罗估计器，使用 $[0,1]^d$ 上的独立同分布均匀采样器。为使结果在不同的 $(d, M)$ 之间具有确定性和独立可复现性，使用一个基础种子 $s_{\\mathrm{MC}}=12345$，并且对于每个 $(d, m)$（其中 $M=2^m$），使用由 $s_{\\mathrm{MC}} + 1000\\,d + m$ 播种的伪随机数生成器进行采样。\n\n计算和报告要求：\n- 对于按升序排列的每个 $d \\in \\{\\,1,\\,5,\\,20\\,\\}$，通过对指定 $M$ 值范围内的 $\\log(E_M(d))$ 与 $\\log(M)$ 进行最小二乘线性回归，分别计算使用 Sobol 拟蒙特卡罗样本的经验收敛指数 $p_{\\mathrm{QMC}}(d)$ 和使用普通蒙特卡罗样本的经验收敛指数 $p_{\\mathrm{MC}}(d)$。\n- 最终输出必须是单行，包含一个按顺序排列的 $6$ 个浮点数的列表\n$$\n\\big[\\,p_{\\mathrm{QMC}}(1),\\,p_{\\mathrm{MC}}(1),\\,p_{\\mathrm{QMC}}(5),\\,p_{\\mathrm{MC}}(5),\\,p_{\\mathrm{QMC}}(20),\\,p_{\\mathrm{MC}}(20)\\,\\big],\n$$\n每个数四舍五入到三位小数。该列表必须以逗号分隔，并用方括号括起来，不含空格。\n- 数值指数无需报告物理单位。所有三角函数的参数必须是弧度。\n\n您的程序必须是一个完整的、可运行的脚本，不接受任何输入，并按上述格式精确地产生一行输出。所要求的六个量中的每一个的答案都是一个浮点数。", "solution": "用户的请求是对普通蒙特卡罗（MC）和拟蒙特卡罗（QMC）积分方法进行比较性数值研究。目标是凭经验确定每种方法在应用于特定多维积分时的收敛指数。\n\n问题陈述在科学上是合理的、适定的和完整的。所有必要的数据、参数和程序都清晰明确地提供，从而可以进行直接且可验证的计算求解。该任务完全属于计算科学领域，特别是涉及数值方法和模拟。因此，我们可以着手提供完整的解决方案。\n\n对于维度 $d$，待估计的积分为：\n$$\nI(d) = \\int_{[0,1]^d} \\prod_{i=1}^{d} \\cos(\\pi x_i)\\, dx\n$$\n问题提供了这样一个事实，即一维积分 $\\int_0^1 \\cos(\\pi x)\\,dx = 0$。由于被积函数 $f(x_1, \\ldots, x_d) = \\prod_{i=1}^{d} \\cos(\\pi x_i)$ 是一个可分离函数，多维积分是一维积分的乘积：\n$$\nI(d) = \\prod_{i=1}^{d} \\int_0^1 \\cos(\\pi x_i)\\,dx_i = \\left(\\int_0^1 \\cos(\\pi x)\\,dx\\right)^d = 0^d\n$$\n这意味着对于本问题中考虑的所有维度 $d \\ge 1$，积分的真实值为 $I(d) = 0$。\n\n积分的数值估计 $\\hat{I}_M(d)$ 是通过对 $d$ 维单位超立方体 $[0,1]^d$ 中的一组 $M$ 个点 $\\{x^{(j)}\\}_{j=1}^M$ 上的被积函数值求平均来计算的：\n$$\n\\hat{I}_M(d) = \\frac{1}{M}\\sum_{j=1}^{M} f(x^{(j)}) = \\frac{1}{M}\\sum_{j=1}^{M}\\,\\prod_{i=1}^{d}\\cos\\!\\big(\\pi\\, x_{i}^{(j)}\\big)\n$$\n绝对积分误差为 $E_M(d) = |\\hat{I}_M(d) - I(d)|$。由于 $I(d)=0$，这可以简化为 $E_M(d) = |\\hat{I}_M(d)|$。\n\n主要目标是找到经验收敛指数 $p(d)$，它描述了误差 $E_M(d)$ 如何随着样本数量 $M$ 的增加而减小。这是通过将线性模型拟合到对数-对数数据来实现的：\n$$\n\\log(E_M(d)) \\approx a(d) + p(d)\\,\\log(M)\n$$\n指数 $p(d)$ 是穿过点 $(\\log(M), \\log(E_M(d)))$ 的最佳拟合线的斜率，这些点对应于指定的样本大小 $M \\in \\{32, 64, 128, 256, 512, 1024\\}$。这种线性回归可以使用标准数值库来执行。\n\n将比较两种生成样本点 $x^{(j)}$ 的方法：\n\n1.  **普通蒙特卡罗（MC）：** 点独立同分布地从 $[0,1]^d$ 上的均匀分布中抽取。为确保可复现性，定义了特定的种子协议。对于每个维度 $d$ 和样本大小指数 $m$（其中 $M=2^m$），伪随机数生成器使用种子 $s = 12345 + 1000d + m$ 进行播种。每次试验都会生成一组独立的 $M$ 个点。\n\n2.  **拟蒙特卡罗（QMC）：** 点取自低差异的 Sobol 序列。我们为每个维度 $d$ 使用一个固定的种子 $s_{\\mathrm{QMC}}=2024$ 生成的、经过置乱的 Sobol 序列。为了分析收敛性，我们为每个维度生成一个包含 $1024$ 个点的主集。对于给定的样本大小 $M$，计算时使用该主集中的前 $M$ 个点。这种方法正确地模拟了 QMC 方法固有的渐进式优化特性。\n\n在取对数之前，向误差添加一个小的正常数 `np.finfo(float).eps`，以防止在估计值恰好为零的罕见情况下出现 $\\log(0)$ 的数值问题。\n\n总体算法如下：\n对于每个维度 $d \\in \\{1, 5, 20\\}$：\n1.  **QMC 指数 $p_{\\mathrm{QMC}}(d)$：**\n    a. 使用指定的置乱方式和种子生成一个包含 $1024$ 个点的 Sobol 序列。\n    b. 对于每个 $M \\in \\{32, 64, \\ldots, 1024\\}$，取前 $M$ 个点，计算估计值 $\\hat{I}_M(d)$，找到误差 $E_M(d)$，并记录点对 $(\\log(M), \\log(E_M(d)))$。\n    c. 对这些记录的点对进行线性回归，找到斜率，即 $p_{\\mathrm{QMC}}(d)$。\n2.  **MC 指数 $p_{\\mathrm{MC}}(d)$：**\n    a. 对于每个 $m \\in \\{5, 6, \\ldots, 10\\}$，设置 $M=2^m$。\n    b. 使用指定的播种方案 $s = 12345 + 1000d + m$ 生成一组新的 $M$ 个均匀随机点。\n    c. 计算估计值 $\\hat{I}_M(d)$，找到误差 $E_M(d)$，并记录点对 $(\\log(M), \\log(E_M(d)))$。\n    d. 对这些记录的点对进行线性回归，找到斜率，即 $p_{\\mathrm{MC}}(d)$。\n\n最终结果是一个包含六个计算出的指数的列表，四舍五入到三位小数。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import qmc\n\ndef solve():\n    \"\"\"\n    Computes and compares the empirical convergence exponents of Monte Carlo\n    and Quasi-Monte Carlo integration for a specific multidimensional integral.\n    \"\"\"\n\n    # Define the parameter sets for the simulation.\n    D_VALUES = [1, 5, 20]\n    M_EXPONENTS = list(range(5, 11))  # Corresponds to m = 5, ..., 10\n    M_VALUES = [2**m for m in M_EXPONENTS]\n\n    # Fixed seeds for reproducibility, as specified in the problem.\n    QMC_SEED = 2024\n    MC_BASE_SEED = 12345\n    \n    # Store the final list of 6 exponent values.\n    all_exponents = []\n    \n    # Pre-compute log(M) values for regression.\n    log_M_values = np.log(M_VALUES)\n    \n    # Machine epsilon for numerical stability when taking logs.\n    epsilon = np.finfo(float).eps\n    \n    # Loop over the specified dimensions in ascending order.\n    for d in D_VALUES:\n        \n        # --- Quasi-Monte Carlo (QMC) Calculation ---\n        log_E_qmc = []\n        \n        # Instantiate the Sobol sampler once per dimension.\n        sampler = qmc.Sobol(d=d, scramble=True, seed=QMC_SEED)\n        \n        # Generate the largest required set of points. Subsets will be used for\n        # smaller M to analyze the convergence of a single sequence.\n        all_points_qmc = sampler.random(n=max(M_VALUES))\n        \n        for M in M_VALUES:\n            # Use the first M points of the sequence.\n            points = all_points_qmc[:M, :]\n            \n            # Evaluate the integrand f(x) = product(cos(pi*x_i)) for each point.\n            integrand_values = np.prod(np.cos(np.pi * points), axis=1)\n            \n            # The QMC estimate is the mean of the function values.\n            i_hat_qmc = np.mean(integrand_values)\n            \n            # The true integral value is 0, so the error is the absolute value of the estimate.\n            error_qmc = np.abs(i_hat_qmc)\n            \n            # Store the logarithm of the error.\n            log_E_qmc.append(np.log(error_qmc + epsilon))\n            \n        # Perform linear regression of log(E) vs log(M) to find the slope (exponent).\n        p_qmc, _ = np.polyfit(log_M_values, log_E_qmc, 1)\n        all_exponents.append(p_qmc)\n        \n        # --- Plain Monte Carlo (MC) Calculation ---\n        log_E_mc = []\n        \n        # For MC, a new set of points is generated for each (d, M) pair.\n        for m, M in zip(M_EXPONENTS, M_VALUES):\n            # The seed is a function of d and m, ensuring independent reproducibility.\n            seed = MC_BASE_SEED + 1000 * d + m\n            rng = np.random.default_rng(seed)\n            \n            # Generate M random points in d dimensions.\n            points = rng.random(size=(M, d))\n            \n            # Evaluate the integrand.\n            integrand_values = np.prod(np.cos(np.pi * points), axis=1)\n            \n            # The MC estimate is the mean of the function values.\n            i_hat_mc = np.mean(integrand_values)\n            \n            # Calculate and store the log of the error.\n            error_mc = np.abs(i_hat_mc)\n            log_E_mc.append(np.log(error_mc + epsilon))\n            \n        # Perform linear regression to find the convergence exponent for MC.\n        p_mc, _ = np.polyfit(log_M_values, log_E_mc, 1)\n        all_exponents.append(p_mc)\n\n    # Format the results as a list of strings, rounded to three decimal places.\n    formatted_results = [f\"{val:.3f}\" for val in all_exponents]\n    \n    # Print the final output in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3190544"}, {"introduction": "从工具到应用，我们将注意力转向求解描述物理系统演化的偏微分方程（PDE）。伯格斯方程是流体动力学中的一个典型模型，它巧妙地融合了非线性对流和粘性扩散效应。在本练习中，您不仅将实现一个有限体积法求解器来模拟该方程，更重要的是，您将执行一个关键的验证步骤：使用理查森外推法凭经验测量数值方法的收敛阶。这个过程是科学建模与仿真生命周期中确保代码正确性的基石，能培养您对计算结果的批判性评估能力。[@problem_id:3190541]", "problem": "给定一维黏性Burgers方程的守恒形式，定义在周期性域上，\n$$\\partial_t u + \\partial_x \\left( \\frac{u^2}{2} \\right) = \\nu \\,\\partial_{xx} u,$$\n其域长度为$L$，黏度为$\\nu$，初始条件为$u(x,0) = \\sin(2\\pi x)$，该条件光滑且与周期性边界兼容。考虑在具有$N$个单元的均匀网格上采用以下显式有限体积法，单元宽度为$dx = L/N$，并使用步长为$dt$的前向欧拉时间步进，以在最终时间$T$逼近$u(x,T)$。\n\n对流项使用针对无粘通量$f(u) = u^2/2$的Lax-Friedrichs（也称Rusanov）数值通量进行离散，\n$$F_{i+1/2} = \\frac{1}{2}\\left(f(u_i) + f(u_{i+1})\\right) - \\frac{1}{2}\\alpha\\,(u_{i+1} - u_i),$$\n其中$\\alpha$是通量的合适Lipschitz常数，可以取为当前时刻整个域上$u$的最大绝对值。扩散项通过中心二阶有限差分进行离散\n$$D_i = \\nu \\frac{u_{i+1} - 2u_i + u_{i-1}}{dx^2}.$$\n每个时间步的显式更新公式为\n$$u_i^{n+1} = u_i^n - \\frac{dt}{dx}\\left(F_{i+1/2}^n - F_{i-1/2}^n\\right) + dt\\,D_i^n,$$\n其中周期性边界条件通过环绕索引实现。\n\n对于使用Lax-Friedrichs通量的非线性对流项，此有限体积法在时间上是一阶精度，在空间上通常也是一阶精度（扩散项在空间上是二阶精度）。你将使用Richardson外推原理，根据数值数据估计观测到的收敛阶$p$（关于$dt$）和$q$（关于$dx$），而无需知道精确的解析解。估计必须从第一性原理出发：假设在分辨率参数$h$（$h=dt$或$h=dx$）下的数值解$U_h$满足渐近误差展开$U_h = U + C h^r + \\text{高阶项}$，其中$U$为精确解，$C$为某个常数，$r$为阶数，然后使用在连续加密下的解的差异来确定$r$。\n\n你的程序必须：\n- 实现所述方法，使用均匀网格和周期性边界条件，将$u(x,t)$从$t=0$演化到$t=T$。\n- 对于时间阶估计$p$，固定一个具有$N$个单元的足够精细的空间网格，并使用时间步长$dt$、$dt/2$和$dt/4$计算三个解；然后对这三个在同一网格上的解的差异应用Richardson外推来估计$p$。\n- 对于空间阶估计$q$，固定最终时间$T$和黏度$\\nu$，并在具有$N$、$2N$和$4N$个单元的网格上计算三个解。选择足够小且与$dx^2$成比例的时间步长，以抑制相对于空间误差的时间误差。通过使用单元平均块均化将较精细网格的解投影到较粗糙的网格上来比较解，然后对差异应用Richardson外推来估计$q$。\n\n误差范数必须使用与测量差异所在网格一致的离散$L^2$范数，\n$$\\|e\\|_2 = \\sqrt{dx \\sum_i e_i^2}.$$\n为了在不同网格间比较解以进行空间阶估计，首先通过对每个粗糙单元对应的$r$个连续精细单元进行平均，将较精细的解投影到粗糙网格上，其中$r$是整数加密比，然后在范数计算中使用粗糙网格的$dx$。\n\n必须尊重科学真实性和稳定性。使用满足对流和扩散的合理显式稳定性约束的时间步长；例如，强制$dt$不超过基于$dx$、$\\nu$和域中最大速度$|u|$的保守界限。\n\n测试套件：\n为以下三个参数集计算所要求的收敛阶估计。所有量均为无量纲。\n\n- 情况A（时间阶估计）：\n  - 域长度 $L = 1.0$。\n  - 黏度 $\\nu = 0.05$。\n  - 最终时间 $T = 0.01$。\n  - 网格单元数 $N = 256$。\n  - 使用三种时间分辨率，分别为 $dt = T/512$、$dt/2$ 和 $dt/4$。\n\n- 情况B（空间阶估计）：\n  - 域长度 $L = 1.0$。\n  - 黏度 $\\nu = 0.05$。\n  - 最终时间 $T = 0.0025$。\n  - 使用三种网格，分别为 $N = 64$、$N = 128$、$N = 256$。\n  - 为保证稳定性和抑制时间误差，在$N=64$时使用时间步长 $dt = T/400$，在$N=128$时使用 $dt = T/1600$，在$N=256$时使用 $dt = T/6400$。\n\n- 情况C（较小黏度下的时间阶估计）：\n  - 域长度 $L = 1.0$。\n  - 黏度 $\\nu = 0.02$。\n  - 最终时间 $T = 0.005$。\n  - 网格单元数 $N = 256$。\n  - 使用三种时间分辨率，分别为 $dt = T/1024$、$dt/2$ 和 $dt/4$。\n\n答案规格：\n- 对于情况A，输出一个浮点数，等于估计的时间阶$p$。\n- 对于情况B，输出一个浮点数，等于估计的空间阶$q$。\n- 对于情况C，输出一个浮点数，等于估计的时间阶$p$。\n\n你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序为[情况A结果, 情况B结果, 情况C结果]，每个浮点数四舍五入到三位小数，例如，“[1.000,0.999,1.003]”。", "solution": "用户提供了一个问题，要求实现一个数值方法来求解一维黏性Burgers方程，然后估计该方法的收敛阶。\n\n### 第1步：提取已知条件\n- **偏微分方程**：一维黏性Burgers方程的守恒形式：$\\partial_t u + \\partial_x \\left( \\frac{u^2}{2} \\right) = \\nu \\,\\partial_{xx} u$。\n- **域**：周期性，长度为$L$。\n- **初始条件**：$u(x,0) = \\sin(2\\pi x)$。\n- **数值方法**：均匀网格上的显式有限体积法。\n  - **网格**：$N$个单元，单元宽度$dx = L/N$。\n  - **时间步进**：前向欧拉法，步长为$dt$。\n  - **更新公式**：$u_i^{n+1} = u_i^n - \\frac{dt}{dx}\\left(F_{i+1/2}^n - F_{i-1/2}^n\\right) + dt\\,D_i^n$。\n  - **对流通量**：$f(u) = u^2/2$，使用Lax-Friedrichs (Rusanov) 数值通量离散：\n    $F_{i+1/2} = \\frac{1}{2}\\left(f(u_i) + f(u_{i+1})\\right) - \\frac{1}{2}\\alpha\\,(u_{i+1} - u_i)$。\n  - **Lipschitz常数**：$\\alpha$是整个域上$u$的最大绝对值，$\\alpha = \\max_i(|u_i|)$。\n  - **扩散项**：中心二阶有限差分：$D_i = \\nu \\frac{u_{i+1} - 2u_i + u_{i-1}}{dx^2}$。\n  - **边界条件**：周期性，通过环绕索引实现。\n- **分析任务**：在没有精确解的情况下，使用Richardson外推法对数值解估计收敛阶$p$（时间）和$q$（空间）。\n  - **误差范数**：离散$L^2$范数，$\\|e\\|_2 = \\sqrt{dx \\sum_i e_i^2}$。\n  - **空间比较**：通过对连续单元进行平均，将较精细网格的解投影到较粗糙的网格上。\n- **测试用例**：\n  - **情况A（时间阶）**：$L = 1.0$，$\\nu = 0.05$，$T = 0.01$，$N = 256$。时间步长：$dt = T/512$、$dt/2$、$dt/4$。\n  - **情况B（空间阶）**：$L = 1.0$，$\\nu = 0.05$，$T = 0.0025$。网格：$N = 64$、128、256。时间步长：$N=64$时$dt = T/400$，$N=128$时$dt = T/1600$，$N=256$时$dt = T/6400$。\n  - **情况C（时间阶）**：$L = 1.0$，$\\nu = 0.02$，$T = 0.005$，$N = 256$。时间步长：$dt = T/1024$、$dt/2$、$dt/4$。\n- **输出规格**：单行输出，包含一个由三个浮点数（分别对应情况A、B、C）组成的逗号分隔列表，四舍五入到三位小数，并用方括号括起来。\n\n### 第2步：使用提取的已知条件进行验证\n1.  **科学依据**：该问题基于黏性Burgers方程，这是流体力学和非线性偏微分方程中的一个标准模型方程。所用的数值方法（有限体积法结合Lax-Friedrichs通量和前向欧拉法）是求解此类方程的成熟技术。分析方法（Richardson外推）是数值分析中用于阶验证的基本工具。该问题在计算科学方面有坚实的基础。\n2.  **适定性**：该问题是适定的。给定光滑初始条件和周期性边界条件的偏微分方程具有唯一的稳定解。数值任务定义清晰，所有必要的参数都已提供，可以产生唯一的结果。\n3.  **客观性**：问题以精确、客观的数学和算法术语陈述。没有主观或模糊的语言。\n4.  **完整性与一致性**：问题陈述是自洽的。每种情况的所有参数（$L, \\nu, T, N, dt$）都已给出。数值格式和分析过程都已详细描述。为测试用例提供的时间步长满足显式格式的稳定性约束，确保了数值模拟的可行性。\n5.  **结论**：该问题是有效的、科学合理的、且定义明确。\n\n### 第3步：结论与行动\n问题被判定为**有效**。将开发一个解决方案。\n\n解决方案的核心在于实现一个`burgers_solver`函数，该函数根据指定的有限体积格式将初始条件演化到最终时间$T$。对于周期性边界条件，使用`numpy.roll`进行数组操作非常高效。单元平均解$u_i$从时间步$n$到$n+1$的更新由下式给出：\n$$u_i^{n+1} = u_i^n - \\frac{dt}{dx}\\left(F_{i+1/2}^n - F_{i-1/2}^n\\right) + dt\\,D_i^n$$\n其中$F_{i \\pm 1/2}^n$是单元界面处的数值通量，$D_i^n$是离散化的扩散项，两者都在时间$n$进行评估。各项为：\n- $F_{i+1/2}^n = \\frac{1}{2}\\left(f(u_i^n) + f(u_{i+1}^n)\\right) - \\frac{1}{2}\\alpha^n\\,(u_{i+1}^n - u_i^n)$，其中$f(u) = u^2/2$。\n- $D_i^n = \\nu \\frac{u_{i+1}^n - 2u_i^n + u_{i-1}^n}{dx^2}$。\n- $\\alpha^n = \\max(|u^n|)$。\n\n为了估计收敛阶$r$，我们计算三个加密级别的解，步长分别为$h$、$h/2$和$h/4$。设对应的解为$U_h$、$U_{h/2}$和$U_{h/4}$。假设渐近误差为$U_h = U_{exact} + C h^r + O(h^{r+1})$，解之间的差异为$E_1 = \\|U_h - U_{h/2}\\|_2$和$E_2 = \\|U_{h/2} - U_{h/4}\\|_2$。阶数$r$通过以下公式估计：\n$$r = \\log_2\\left(\\frac{E_1}{E_2}\\right)$$\n对于时间阶估计（$p$），$h=dt$，空间网格是固定的。对于空间阶估计（$q$），$h=dx$，时间步长$dt$按比例缩放以成为高阶项。在比较不同网格上的解时，在计算差异和范数之前，先将较精细的解通过单元平均投影到最粗糙的网格上。\n\n实现将包括：\n1.  一个实现时间步进循环的通用求解器函数。\n2.  用于计算$L^2$范数和将解从精细网格投影到粗糙网格的辅助函数。\n3.  针对每种情况（A、B、C）的特定函数，用于设置参数，为三种所需的分辨率调用求解器，执行Richardson分析，并返回估计的阶数。\n4.  一个主程序，调用特定情况的函数并按要求格式化最终输出。\n初始条件作为一个连续函数，通过在每个有限体积单元的中心对其求值来进行离散化，即$u_i(0) = \\sin(2\\pi x_i/L)$，其中$x_i = (i+0.5)dx$。对于单元平均量，这是一种标准的二阶精度初始化方法。", "answer": "```python\nimport numpy as np\n\ndef l2_norm(e, dx):\n    \"\"\"\n    Computes the discrete L2 norm of a vector e on a grid with spacing dx.\n    Norm definition: ||e||_2 = sqrt(dx * sum(e_i^2)).\n    \"\"\"\n    return np.sqrt(dx * np.sum(e**2))\n\ndef project_to_coarse(u_fine, r):\n    \"\"\"\n    Projects a solution from a fine grid to a coarse grid by cell averaging.\n    Args:\n        u_fine: 1D array on the fine grid.\n        r: Integer refinement ratio (e.g., 2, 4).\n    Returns:\n        1D array on the coarse grid.\n    \"\"\"\n    N_fine = len(u_fine)\n    N_coarse = N_fine // r\n    # Reshape and take the mean over the refinement block\n    return u_fine.reshape(N_coarse, r).mean(axis=1)\n\ndef burgers_solver(L, nu, N, T, n_steps):\n    \"\"\"\n    Solves the 1D viscous Burgers' equation using the specified finite volume method.\n    \"\"\"\n    dx = L / N\n    dt = T / n_steps\n    \n    # Initialize grid at cell centers and solution u\n    x = (np.arange(N) + 0.5) * dx\n    u = np.sin(2 * np.pi * x / L)\n\n    for _ in range(n_steps):\n        # Determine alpha for Lax-Friedrichs flux\n        alpha = np.max(np.abs(u))\n\n        # Get neighbor values using periodic boundary conditions\n        u_p1 = np.roll(u, -1)\n        u_m1 = np.roll(u, 1)\n\n        # Convective flux function f(u) = u^2/2\n        f_u = 0.5 * u**2\n        f_u_p1 = np.roll(f_u, -1)\n        \n        # Lax-Friedrichs numerical flux at interface i+1/2\n        F_ip12 = 0.5 * (f_u + f_u_p1) - 0.5 * alpha * (u_p1 - u)\n        # Flux at interface i-1/2\n        F_im12 = np.roll(F_ip12, 1)\n\n        # Discretized diffusion term\n        D_i = nu * (u_p1 - 2*u + u_m1) / dx**2\n        \n        # Update solution using Forward Euler\n        u = u - (dt / dx) * (F_ip12 - F_im12) + dt * D_i\n    \n    return u\n\ndef estimate_order(U1, U2, U3, dx, refinement_ratio=2):\n    \"\"\"\n    Estimates convergence order using three solutions with successive refinement.\n    \"\"\"\n    E1 = l2_norm(U1 - U2, dx)\n    E2 = l2_norm(U2 - U3, dx)\n    \n    # Avoid division by zero if errors are numerically zero\n    if E2 == 0:\n        return np.inf if E1 > 0 else 0.0\n\n    ratio = E1 / E2\n    return np.log(ratio) / np.log(refinement_ratio)\n\ndef solve_case_A():\n    \"\"\"Temporal order p for Case A.\"\"\"\n    L, nu, T, N = 1.0, 0.05, 0.01, 256\n    dx = L / N\n    n_steps_base = 512\n    \n    U1 = burgers_solver(L, nu, N, T, n_steps_base)\n    U2 = burgers_solver(L, nu, N, T, n_steps_base * 2)\n    U3 = burgers_solver(L, nu, N, T, n_steps_base * 4)\n    \n    return estimate_order(U1, U2, U3, dx)\n\ndef solve_case_B():\n    \"\"\"Spatial order q for Case B.\"\"\"\n    L, nu, T = 1.0, 0.05, 0.0025\n    \n    N1, n_steps1 = 64, 400\n    N2, n_steps2 = 128, 1600\n    N3, n_steps3 = 256, 6400\n    \n    dx1 = L / N1\n\n    U1 = burgers_solver(L, nu, N1, T, n_steps1)\n    U2 = burgers_solver(L, nu, N2, T, n_steps2)\n    U3 = burgers_solver(L, nu, N3, T, n_steps3)\n\n    # Project finer solutions to the coarsest grid (N=64)\n    U2_proj = project_to_coarse(U2, r=2)\n    U3_proj = project_to_coarse(U3, r=4)\n    \n    return estimate_order(U1, U2_proj, dx1)\n\ndef solve_case_C():\n    \"\"\"Temporal order p for Case C.\"\"\"\n    L, nu, T, N = 1.0, 0.02, 0.005, 256\n    dx = L / N\n    n_steps_base = 1024\n\n    U1 = burgers_solver(L, nu, N, T, n_steps_base)\n    U2 = burgers_solver(L, nu, N, T, n_steps_base * 2)\n    U3 = burgers_solver(L, nu, N, T, n_steps_base * 4)\n\n    return estimate_order(U1, U2, U3, dx)\n\ndef solve():\n    \"\"\"Main function to run all test cases and print the results.\"\"\"\n    # Define the test cases from the problem statement.\n    # We will call a specific function for each case.\n    result_A = solve_case_A()\n    result_B = solve_case_B()\n    result_C = solve_case_C()\n    \n    results = [result_A, result_B, result_C]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.3f}' for r in results])}]\")\n\nsolve()\n```", "id": "3190541"}]}