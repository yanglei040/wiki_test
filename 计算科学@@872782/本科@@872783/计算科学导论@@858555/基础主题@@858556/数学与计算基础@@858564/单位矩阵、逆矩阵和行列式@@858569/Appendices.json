{"hands_on_practices": [{"introduction": "一个所有元素均为整数的矩阵，其逆矩阵何时也完全由整数构成？这个问题看似简单，却在密码学、晶体学和整数规划等领域具有重要意义。这个练习将引导你运用逆矩阵的基本公式进行“动手”推理，而非复杂的数值计算。通过这个过程，你将发现一个关于行列式的惊人而优雅的充要条件，它深刻揭示了行列式值与矩阵逆结构之间的内在联系。[@problem_id:1361629]", "problem": "设 $M$ 是一个所有元素都为整数的 $2 \\times 2$ 矩阵。一个已知的事实是，对于这样的矩阵，如果其逆矩阵 $M^{-1}$ 存在且所有元素也都是整数，那么其行列式 $\\det(M)$ 必然是整数。然而，其逆命题不一定成立；整数行列式并不能保证其逆矩阵也是整数矩阵。\n\n本题的任务是，找出哪些行列式的整数值是*充分*条件，可以保证*任何*具有该行列式的此类矩阵 $M$ 的逆矩阵也完全由整数构成，同时这些值也是该性质对所有整数矩阵都成立的*必要*条件。从下列 $\\det(M)$ 的可能值中，选出所有满足此充要条件的选项。\n\nA. $2$\n\nB. $1$\n\nC. $-1$\n\nD. $-2$\n\nE. $0$", "solution": "设元素为整数的 $2 \\times 2$ 矩阵 $M$ 为\n$$\nM = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}\n$$\n其中 $a, b, c, d$ 均为整数，即 $a, b, c, d \\in \\mathbb{Z}$。\n\n$M$ 的行列式为 $\\det(M) = ad - bc$。由于 $a, b, c, d$ 是整数，它们的乘积和差也都是整数。因此，$\\det(M)$ 是一个整数。\n\n如果 $\\det(M) \\neq 0$，则 $M$ 的逆矩阵 $M^{-1}$ 由以下公式给出：\n$$\nM^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix} = \\begin{pmatrix} \\frac{d}{\\det(M)}  \\frac{-b}{\\det(M)} \\\\ \\frac{-c}{\\det(M)}  \\frac{a}{\\det(M)} \\end{pmatrix}\n$$\n\n题目要求找出关于 $\\det(M)$ 的整数值的充要条件，以保证 $M^{-1}$ 的所有元素都是整数。我们通过考虑该条件的充分性和必要性来分析这个问题。\n\n首先，我们分析选项E，即 $\\det(M) = 0$。如果一个矩阵的行列式为零，则该矩阵是奇异的，其逆矩阵不存在。因此，选项E不正确，因为它不满足存在逆矩阵的前提条件。\n\n**充分性分析：**\n我们来检验行列式为 $1$ 或 $-1$ 是否是充分条件。假设 $\\det(M) = k$，其中 $k \\in \\{1, -1\\}$。\n$M^{-1}$ 的元素形式为 $\\frac{x}{k}$，其中 $x$ 是整数 $d, -b, -c, a$ 之一。\n如果 $k=1$，$M^{-1}$ 的元素为 $d, -b, -c, a$，它们都是整数。\n如果 $k=-1$，$M^{-1}$ 的元素为 $-d, b, c, -a$，它们也都是整数。\n因此，如果 $\\det(M)$ 为 $1$ 或 $-1$，就可以保证 $M^{-1}$ 的所有元素都是整数。这意味着选项 B 和 C 给出的条件是充分的。\n\n**必要性分析：**\n现在，我们必须证明这些是唯一可能的值。换句话说，如果一个整数矩阵 $M$ 有一个整数逆矩阵 $M^{-1}$，那么 $\\det(M)$ 是否必须为 $1$ 或 $-1$？\n\n设 $M$ 和 $M^{-1}$ 都是元素为整数的矩阵。根据逆矩阵的定义，我们有恒等式 $M M^{-1} = I$，其中 $I$ 是 $2 \\times 2$ 的单位矩阵。\n利用矩阵乘积的行列式等于行列式乘积的性质，我们有：\n$$\n\\det(M M^{-1}) = \\det(I)\n$$\n$$\n\\det(M) \\det(M^{-1}) = 1\n$$\n由于 $M$ 的元素是整数，其行列式 $\\det(M)$ 必须是整数。\n同理，由于 $M^{-1}$ 的元素是整数，其行列式 $\\det(M^{-1})$ 也必须是整数。\n设 $k_1 = \\det(M)$ 和 $k_2 = \\det(M^{-1})$，其中 $k_1, k_2 \\in \\mathbb{Z}$。该方程变为：\n$$\nk_1 k_2 = 1\n$$\n两个整数的乘积为 $1$ 的唯一可能是它们是 $(1, 1)$ 或 $(-1, -1)$。\n因此，$\\det(M)$ 必定是 $1$ 或 $-1$。\n这证明了条件 $\\det(M) \\in \\{1, -1\\}$ 是必要的。\n\n行列式的任何其他整数值都不是充分条件。例如，考虑选项A，其中 $\\det(M) = 2$。我们可以构造一个具有该行列式的简单整数矩阵：\n$$\nM = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}\n$$\n其行列式为 $2 \\cdot 1 - 0 \\cdot 0 = 2$。其逆矩阵为：\n$$\nM^{-1} = \\frac{1}{2} \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  1 \\end{pmatrix}\n$$\n这个逆矩阵并非所有元素都是整数。因此，$\\det(M)=2$ 不是一个充分条件。对于任何满足 $|k| > 1$ 的整数行列式 $k$，例如选项 A ($2$) 和 D ($-2$)，都可以构造出类似的反例。\n\n综合充分性和必要性分析，能保证一个整数矩阵的逆矩阵也是整数矩阵的行列式值只有 $1$ 和 $-1$。\n查看所给选项：\nA. $2$：无效。\nB. $1$：有效。\nC. $-1$：有效。\nD. $-2$：无效。\nE. $0$：无效（不存在逆矩阵）。\n\n因此，正确的选项是 B 和 C。", "answer": "$$\\boxed{BC}$$", "id": "1361629"}, {"introduction": "从代数性质转向几何应用，行列式不仅仅是一个数值，它描述了线性变换如何缩放和定向空间。在分析复杂非线性映射时，雅可比矩阵的行列式是一个极其强大的工具。本实践要求你编写代码，通过计算雅可比行列式的符号来检验一个二维映射在每一点上是保持方向还是反转方向，这是计算机图形学、流体力学和连续介质力学中的一项核心任务。[@problem_id:3141219]", "problem": "给定定义了从二维实平面到其自身映射的光滑函数，记为 $F:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$，其中 $F(x,y)=\\left(f_1(x,y),f_2(x,y)\\right)$。在计算科学的背景下，此类映射的保向性和局部可逆性可以通过雅可比矩阵及其行列式进行分析。在点 $\\left(x,y\\right)$ 处的雅可比矩阵 $J$ 定义为 $F$ 在 $\\left(x,y\\right)$ 点求值的一阶偏导数矩阵。$\\det J$ 的符号编码了映射是否保向（$\\det J>0$）、反向（$\\det J<0$）或局部奇异（$\\det J=0$）。您的任务是实现一个鲁棒的计算测试，对于给定的映射和矩形域，该测试会采样一个均匀的网格点，使用有限差分近似 $J$ 的元素，并对网格上各处的映射保向行为进行分类。\n\n起点和基本依据：\n- 使用雅可比矩阵 $J$ 作为 $F$ 的偏导数矩阵的定义。\n- 使用众所周知的事实：行列式 $\\det J$ 给出了 $F$ 的局部线性近似的有符号面积缩放因子；在 $\\det J\\neq 0$ 的地方局部可逆性成立，在 $\\det J>0$ 的地方保向，在 $\\det J<0$ 的地方反向。\n\n实现要求：\n- 使用对称有限差分来近似内部每个网格点的偏导数，并在边界处使用单侧差分以保持数值稳定性。\n- 在分类符号时使用一个鲁棒的非零阈值 $\\tau$（一个小的正实数），以减轻浮点和离散化误差。具体而言：\n  - 如果 $\\det J>\\tau$，则将网格点分类为保向。\n  - 如果 $\\det J < -\\tau$，则将网格点分类为反向。\n  - 如果 $|\\det J|\\le \\tau$，则将网格点分类为不确定/奇异。\n- 通过汇总各点的分类来对整个网格上的映射进行分类：\n  - 如果所有网格点都是保向的，则输出 $+1$。\n  - 如果所有网格点都是反向的，则输出 $-1$。\n  - 否则（网格上任何地方出现混合符号和/或接近零的行列式），输出 $0$。\n\n网格采样：\n- 对一个闭合的矩形域 $\\left[x_{\\min},x_{\\max}\\right]\\times\\left[y_{\\min},y_{\\max}\\right]$ 进行均匀采样，在 $x$ 方向上有 $N_x$ 个点，在 $y$ 方向上有 $N_y$ 个点，均包含端点。\n- 设间距为 $h_x=\\frac{x_{\\max}-x_{\\min}}{N_x-1}$ 和 $h_y=\\frac{y_{\\max}-y_{\\min}}{N_y-1}$。\n\n测试套件：\n实现您的程序，对以下五个映射、域和网格分辨率进行分类。对于旋转映射，请使用弧度作为角度单位。\n- 情况 1（恒等映射，理想情况）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(x,y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 2（旋转 $\\pi/4$，理想情况）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(\\cos\\left(\\frac{\\pi}{4}\\right)x-\\sin\\left(\\frac{\\pi}{4}\\right)y,\\ \\sin\\left(\\frac{\\pi}{4}\\right)x+\\cos\\left(\\frac{\\pi}{4}\\right)y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 3（反射，反向）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(-x,y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 4（非线性，混合且奇异）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(x^2,y\\right)$，其中 $N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n- 情况 5（行列式值很小的均匀缩放，边界和数值鲁棒性）：在 $\\left[-1,1\\right]\\times\\left[-1,1\\right]$ 上的 $F(x,y)=\\left(\\varepsilon x,\\varepsilon y\\right)$，其中 $\\varepsilon=10^{-4}$，$N_x=21$，$N_y=21$，$\\tau=10^{-10}$。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含五个情况的分类结果，格式为逗号分隔的列表并用方括号括起来；例如，$\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$，其中每个条目是如上文定义的 $+1$、$-1$ 或 $0$ 之一。\n- 不涉及物理单位，除了指定的弧度角外，没有其他角度出现。输出均为整数。\n\n科学真实性和推导重点：\n- 程序必须从雅可比矩阵和行列式的基本定义中推导出保向性分类，不得依赖任何用于测试套件的预先计算的解析行列式公式。\n- 基于网格的近似实现必须是自洽的、数值上合理的，并且对边界条件和微小行列式具有鲁棒性。", "solution": "经评估，用户提供的问题是有效的。它在科学上基于多变量微积分和数值分析，问题是适定的，具有明确的目标和约束，并且没有矛盾或含糊之处。该问题要求实现一种计算方法，通过分析雅可比矩阵的行列式来对二维映射的保向性质进行分类，这是计算科学中一项标准且有意义的任务。\n\n解决方案首先建立理论基础，然后详细说明数值算法，最后将其应用于指定的测试案例。\n\n### 1. 理论基础：雅可比矩阵与方向\n\n一个光滑映射 $F: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$，定义为 $F(x,y) = (f_1(x,y), f_2(x,y))$，可以在任何点 $(x_0, y_0)$ 处被一个线性变换局部近似。代表这个线性变换的矩阵就是在该点求值的雅可比矩阵 $J$。雅可比矩阵是向量值函数 $F$ 的所有一阶偏导数构成的矩阵：\n\n$$\nJ(x,y) =\n\\begin{pmatrix}\n\\frac{\\partial f_1}{\\partial x}(x,y)  \\frac{\\partial f_1}{\\partial y}(x,y) \\\\\n\\frac{\\partial f_2}{\\partial x}(x,y)  \\frac{\\partial f_2}{\\partial y}(x,y)\n\\end{pmatrix}\n$$\n\n雅可比矩阵的行列式 $\\det(J)$ 是一个标量值，描述了映射如何局部地缩放面积。行列式的符号表明了映射对方向的影响：\n- 如果 $\\det(J) > 0$，映射是**保向的**。它变换一个小区域而不“翻转”它。\n- 如果 $\\det(J) < 0$，映射是**反向的**。它“翻转”一个小区域的方向，类似于反射。\n- 如果 $\\det(J) = 0$，映射在该点是**奇异的**。它会压缩面积，且反函数定理不保证局部可逆性。\n\n### 2. 雅可比矩阵的数值近似\n\n由于我们必须创建一个不依赖于解析导数的通用工具，我们使用有限差分法来数值近似偏导数。我们在一个离散的网格点 $(x_i, y_j)$ 上操作，该网格点覆盖了域 $[x_{\\min}, x_{\\max}] \\times [y_{\\min}, y_{\\max}]$。网格间距为 $h_x = \\frac{x_{\\max} - x_{\\min}}{N_x - 1}$ 和 $h_y = \\frac{y_{\\max} - y_{\\min}}{N_y - 1}$，其中 $N_x$ 和 $N_y$ 是每个方向上的点数。\n\n为了在整个网格上保持数值准确性和稳定性，我们对内部点和边界点使用不同的有限差分公式。\n\n**对于内部点 $(x_i, y_j)$，其中 $0 < i < N_x-1$ 且 $0 < j < N_y-1$：**\n我们使用二阶精度的**对称（中心）差分**公式。对于一个通用函数 $g(x,y)$：\n$$ \\frac{\\partial g}{\\partial x}(x_i, y_j) \\approx \\frac{g(x_i + h_x, y_j) - g(x_i - h_x, y_j)}{2h_x} $$\n$$ \\frac{\\partial g}{\\partial y}(x_i, y_j) \\approx \\frac{g(x_i, y_j + h_y) - g(x_i, y_j - h_y)}{2h_y} $$\n\n**对于边界点：**\n我们必须使用单侧公式。\n- 在左边界（$i=0$）：使用**前向差分**计算 $\\frac{\\partial}{\\partial x}$。\n  $$ \\frac{\\partial g}{\\partial x}(x_0, y_j) \\approx \\frac{g(x_0 + h_x, y_j) - g(x_0, y_j)}{h_x} $$\n- 在右边界（$i=N_x-1$）：使用**后向差分**计算 $\\frac{\\partial}{\\partial x}$。\n  $$ \\frac{\\partial g}{\\partial x}(x_{N_x-1}, y_j) \\approx \\frac{g(x_{N_x-1}, y_j) - g(x_{N_x-1} - h_x, y_j)}{h_x} $$\n类似地，在下边界（$j=0$）和上边界（$j=N_y-1$）分别对 $\\frac{\\partial}{\\partial y}$ 使用前向和后向差分公式。这些公式是一阶精度的。\n\n### 3. 算法流程\n\n对给定域上的一个给定映射 $F$ 进行分类的总体算法如下：\n\n1.  **网格生成**：通过创建两个坐标数组来定义采样网格，一个用于从 $x_{\\min}$ 到 $x_{\\max}$ 的 $x$ 轴（含 $N_x$ 个点），另一个用于从 $y_{\\min}$ 到 $y_{\\max}$ 的 $y$ 轴（含 $N_y$ 个点）。计算步长 $h_x$ 和 $h_y$。\n\n2.  **迭代与分类**：遍历网格上的每个点 $(x_i, y_j)$。对于每个点：\n    a.  计算构成雅可比矩阵的四个偏导数：$\\frac{\\partial f_1}{\\partial x}$, $\\frac{\\partial f_1}{\\partial y}$, $\\frac{\\partial f_2}{\\partial x}$, 和 $\\frac{\\partial f_2}{\\partial y}$。根据点的位置 $(i,j)$ 相对于网格边界选择合适的有限差分公式（中心、前向或后向）。\n    b.  用这四个近似值构造雅可比矩阵 $J_{ij}$。\n    c.  计算其行列式：$\\det(J_{ij}) = \\frac{\\partial f_1}{\\partial x}\\frac{\\partial f_2}{\\partial y} - \\frac{\\partial f_1}{\\partial y}\\frac{\\partial f_2}{\\partial x}$。\n    d.  使用给定的阈值 $\\tau$ 对该点的局部行为进行分类：\n        - 如果 $\\det(J_{ij}) > \\tau$，该点是`保向的`。\n        - 如果 $\\det(J_{ij}) < -\\tau$，该点是`反向的`。\n        - 如果 $|\\det(J_{ij})| \\le \\tau$，该点是`不确定/奇异的`。\n    e.  记录找到的分类类型。我们只需要知道是否至少遇到过每一种类型。\n\n3.  **汇总结果**：在评估完网格上所有点之后，将单个分类结果合并为整个映射在域上的单一结果：\n    a.  如果所有点都被分类为`保向的`，最终输出为 $+1$。\n    b.  如果所有点都被分类为`反向的`，最终输出为 $-1$。\n    c.  如果存在任何分类的混合（例如，一些保向一些反向），或者任何点被分类为`不确定/奇异的`，最终输出为 $0$。\n\n对问题陈述中提供的五个测试案例中的每一个都实施此程序。然后将结果汇总成要求的最终格式。\n- 对于 $F(x,y)=(x,y)$，$\\det(J) = 1$，它大于任何小的正数 $\\tau$。结果是 $+1$。\n- 对于旋转映射，$\\det(J) = \\cos^2(\\frac{\\pi}{4}) + \\sin^2(\\frac{\\pi}{4}) = 1$。结果是 $+1$。\n- 对于 $F(x,y)=(-x,y)$，$\\det(J) = -1$，它小于任何小的负数 $-\\tau$。结果是 $-1$。\n- 对于 $F(x,y)=(x^2,y)$，$\\det(J) = 2x$。在域 $[-1,1]\\times[-1,1]$ 上，该行列式取正、负和零值。因此，这是一个混合情况，结果为 $0$。\n- 对于 $F(x,y)=(\\varepsilon x, \\varepsilon y)$，其中 $\\varepsilon=10^{-4}$ 且 $\\tau=10^{-10}$ 是一个小值，其行列式为 $\\det(J) = \\varepsilon^2 = 10^{-8}$。由于 $10^{-8} > \\tau = 10^{-10}$，该映射始终是保向的。结果是 $+1$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classify_mapping(F, domain, grid_size, tau):\n    \"\"\"\n    Classifies a mapping's orientation behavior on a grid.\n    \n    Args:\n        F (callable): The mapping R^2 -> R^2, F(x, y) = (f1(x,y), f2(x,y)).\n        domain (tuple): A tuple (xmin, xmax, ymin, ymax) defining the rectangular domain.\n        grid_size (tuple): A tuple (Nx, Ny) with the number of grid points.\n        tau (float): The non-zero threshold for classification.\n        \n    Returns:\n        int: +1 for preserving, -1 for reversing, 0 for mixed/singular.\n    \"\"\"\n    xmin, xmax, ymin, ymax = domain\n    Nx, Ny = grid_size\n    \n    # Handle cases where grid is a single line/point. Derivatives require >1 point.\n    if Nx == 1 or Ny == 1:\n        # If Nx > 1 and Ny == 1, we can't compute y derivatives.\n        # As per problem, N_x, N_y are 21, so this is for robustness.\n        # Fallback to singular classification for ill-defined grids.\n        return 0\n\n    hx = (xmax - xmin) / (Nx - 1)\n    hy = (ymax - ymin) / (Ny - 1)\n    \n    x_coords = np.linspace(xmin, xmax, Nx)\n    y_coords = np.linspace(ymin, ymax, Ny)\n\n    # Use component functions for clarity\n    def F1(x, y):\n        return F(x, y)[0]\n\n    def F2(x, y):\n        return F(x, y)[1]\n\n    found_preserving = False\n    found_reversing = False\n    found_singular = False\n    \n    for i in range(Nx):\n        for j in range(Ny):\n            x = x_coords[i]\n            y = y_coords[j]\n            \n            # Calculate partial derivatives using appropriate finite difference schemes\n            \n            # Partial derivative with respect to x\n            if i == 0:  # Forward difference at left boundary\n                df1_dx = (F1(x + hx, y) - F1(x, y)) / hx\n                df2_dx = (F2(x + hx, y) - F2(x, y)) / hx\n            elif i == Nx - 1:  # Backward difference at right boundary\n                df1_dx = (F1(x, y) - F1(x - hx, y)) / hx\n                df2_dx = (F2(x, y) - F2(x - hx, y)) / hx\n            else:  # Central difference for interior\n                df1_dx = (F1(x + hx, y) - F1(x - hx, y)) / (2 * hx)\n                df2_dx = (F2(x + hx, y) - F2(x - hx, y)) / (2 * hx)\n            \n            # Partial derivative with respect to y\n            if j == 0:  # Forward difference at bottom boundary\n                df1_dy = (F1(x, y + hy) - F1(x, y)) / hy\n                df2_dy = (F2(x, y + hy) - F2(x, y)) / hy\n            elif j == Ny - 1:  # Backward difference at top boundary\n                df1_dy = (F1(x, y) - F1(x, y - hy)) / hy\n                df2_dy = (F2(x, y) - F2(x, y - hy)) / hy\n            else:  # Central difference for interior\n                df1_dy = (F1(x, y + hy) - F1(x, y - hy)) / (2 * hy)\n                df2_dy = (F2(x, y + hy) - F2(x, y - hy)) / (2 * hy)\n\n            # Calculate Jacobian determinant\n            det_J = df1_dx * df2_dy - df1_dy * df2_dx\n            \n            # Classify the point and update flags\n            if det_J > tau:\n                found_preserving = True\n            elif det_J  -tau:\n                found_reversing = True\n            else:\n                found_singular = True\n            \n            # Optimization: if mixed behavior is found, we know the result is 0\n            if found_singular or (found_preserving and found_reversing):\n                return 0\n    \n    # Aggregate results after checking all points\n    # The optimization above means we only reach here if all points are of one type.\n    if found_preserving and not found_reversing and not found_singular:\n        return 1\n    elif found_reversing and not found_preserving and not found_singular:\n        return -1\n    else:\n        # This branch handles any cases missed by the loop optimization,\n        # e.g., if the grid was only singular points.\n        return 0\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define test cases\n    theta = np.pi / 4\n    c, s = np.cos(theta), np.sin(theta)\n    epsilon = 1e-4\n\n    test_cases = [\n        {\n            \"F\": lambda x, y: (x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (c * x - s * y, s * x + c * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (-x, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (x**2, y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        },\n        {\n            \"F\": lambda x, y: (epsilon * x, epsilon * y),\n            \"domain\": (-1.0, 1.0, -1.0, 1.0),\n            \"grid_size\": (21, 21),\n            \"tau\": 1e-10\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = classify_mapping(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3141219"}, {"introduction": "现在，我们进入一个更高级的数值线性代数主题。在科学计算中，直接求解大型线性系统通常比显式计算逆矩阵更可取。本练习将探讨如何通过对矩阵进行单位矩阵的倍数平移（即研究 $A + \\lambda I$）来影响其性质和迭代求解器的性能。这是一个综合性的数值实验，它将逆矩阵、行列式、特征值、条件数以及共轭梯度法（计算科学的基石之一）的效率紧密联系在一起。[@problem_id:3141142]", "problem": "要求您使用两种互补的方法来研究位移矩阵的逆的计算，并量化通过单位矩阵的倍数进行的位移如何影响数值条件和迭代求解器的性能。核心对象是位移矩阵 $(A + \\lambda I)$ 的逆，其中 $A$ 是一个实对称正定矩阵，$I$ 是单位矩阵，$\\lambda$ 是一个实标量。您的任务是从基本定义出发，推导、实现和比较以下方法。\n\n基本原理：\n- 单位矩阵 $I$ 对所有向量 $x$ 满足 $Ix = x$。\n- 当逆矩阵存在时，$(A + \\lambda I)^{-1}$ 被定义为满足 $(A + \\lambda I)M = I$ 的唯一矩阵 $M$。\n- 对于一个实对称矩阵 $A$，存在一个正交矩阵 $Q$，使得 $Q^{\\mathsf T} A Q$ 是一个对角元素为实数的对角矩阵。\n- 行列式 $\\det(M)$ 是线性变换 $M$ 对体积的缩放因子，对于三角矩阵，它等于对角元素的乘积。\n- 2-范数条件数 $\\kappa_2(M)$ 量化了解对扰动的敏感度，等于最大奇异值与最小奇异值之比；对于对称正定矩阵，它等于最大特征值与最小特征值之比。\n\n问题设置（为可复现性而固定）：\n- 按如下方式构造一个对称正定矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其中 $n = 20$。设 $R \\in \\mathbb{R}^{n \\times n}$ 的元素由一个以 $7$ 为种子的伪随机数生成器生成的独立标准正态分布。计算 $R$ 的约化正交-上三角分解（QR分解），以获得一个正交矩阵 $Q \\in \\mathbb{R}^{n \\times n}$。设 $d \\in \\mathbb{R}^n$ 是一个向量，其元素 $d_i$ 构成一个从 $10^{-6}$ 到 $10^{2}$（含两端）的等比数列，共 $n$ 项。定义 $A = Q \\,\\mathrm{diag}(d)\\, Q^{\\mathsf T}$。这确保了 $A$ 是对称正定的。\n- 定义一个右侧向量 $b \\in \\mathbb{R}^n$，其元素由一个以 $13$ 为种子的伪随机数生成器生成的独立标准正态分布。\n- 考虑位移参数集合 $\\Lambda = \\{\\lambda_1,\\lambda_2,\\lambda_3,\\lambda_4\\} = \\{0,\\;10^{-3},\\;1,\\;100\\}$。\n\n对每个 $\\lambda \\in \\Lambda$ 执行的任务：\n1. 仅使用 $A$ 的特征分解和基本定义，计算向量 $y_{\\mathrm{eig}}$，使其等于 $(A + \\lambda I)^{-1} b$，过程中不构造任何矩阵的逆。\n2. 实现一个基于共轭梯度法的迭代求解器，用于求解对称正定系统，以计算近似满足 $(A + \\lambda I) y_{\\mathrm{cg}} = b$ 的 $y_{\\mathrm{cg}}$。使用以下停止准则：当相对残差 $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le 10^{-10}$（其中 $r_k = b - (A + \\lambda I) y_k$）时终止，或当达到最大迭代次数 $200$ 次时终止。记录所用的迭代次数。\n3. 计算相对误差 $e_{\\mathrm{rel}} = \\lVert y_{\\mathrm{cg}} - y_{\\mathrm{eig}} \\rVert_2 / \\lVert y_{\\mathrm{eig}} \\rVert_2$。\n4. 使用 $A$ 的构造所隐含的特征值和 $\\lambda I$ 位移的定义，计算 2-范数条件数 $\\kappa_2(A + \\lambda I)$。\n5. 用两种方法计算行列式的自然对数 $\\log \\det(A + \\lambda I)$：(i) 基于特征分解的推理；(ii) 对 $(A + \\lambda I)$ 进行 Cholesky 分解，并利用三角矩阵行列式的性质。将这两种方法计算值的绝对差报告为 $\\Delta_{\\log\\det}$。\n\n数值精度和输出格式：\n- 对于每个 $\\lambda \\in \\Lambda$，生成一个包含四个条目的结果列表，顺序如下：共轭梯度迭代的整数次数、四舍五入到小数点后 $12$ 位的浮点数 $e_{\\mathrm{rel}}$、四舍五入到小数点后 $6$ 位的浮点数 $\\kappa_2(A + \\lambda I)$，以及四舍五入到小数点后 $12$ 位的浮点数 $\\Delta_{\\log\\det}$。\n- 您的程序应生成单行输出，包含所有 $\\lambda \\in \\Lambda$ 的结果，格式为一个由这些列表组成的逗号分隔列表，并用方括号括起来，例如：$[\\,[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],\\dots\\,]$。\n\n测试套件：\n- 使用指定的固定矩阵 $A$ 和向量 $b$，其中 $n = 20$，种子分别为 $7$ 和 $13$，位移集合为 $\\Lambda = \\{0,\\;10^{-3},\\;1,\\;100\\}$。该测试套件覆盖：\n  - 无位移情况 $\\lambda = 0$（边界情况，条件最差）。\n  - 小位移 $\\lambda = 10^{-3}$（部分特征值聚集）。\n  - 中等位移 $\\lambda = 1$（强聚集）。\n  - 大位移 $\\lambda = 100$（接近均匀谱）。\n- 所有答案都应是如上所述的纯数值。不涉及物理单位。\n\n最终输出格式要求：\n- 您的程序必须输出一行，包含上述聚合列表，其中每个浮点数都按指定要求四舍五入，并以定点表示法格式化到指定的小数位数。", "solution": "用户提供的问题在科学上是合理的、适定的、客观的且自洽的。所有必要的参数和方法都已明确定义，并基于数值线性代数的基本原理。这些任务构成了一个标准的、富有洞察力的数值实验，旨在探究位移矩阵的性质，特别是关于特征值分布、条件数以及迭代求解器性能。该问题是有效的。\n\n此处提供一个基于原理的详细解决方案。\n\n### 1. 准备工作与矩阵构造\n\n问题核心是分析位移矩阵 $(A + \\lambda I)$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是一个对称正定（SPD）矩阵，$I$ 是 $n \\times n$ 的单位矩阵，$\\lambda$ 是一个实标量。\n\n矩阵 $A$ 的构造使其具有已知的特征系统。其定义为 $A = Q D Q^{\\mathsf T}$，其中：\n-   $n = 20$。\n-   $Q \\in \\mathbb{R}^{n \\times n}$ 是一个正交矩阵（$Q^{\\mathsf T} Q = Q Q^{\\mathsf T} = I$），通过对一个具有随机元素的矩阵进行 QR 分解得到。这确保了 $A$ 的特征向量（即 $Q$ 的列）是良好分布的。\n-   $D = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$ 是一个对角矩阵，其对角元是 $A$ 的特征值。特征值向量 $d$ 被指定为一个从 $d_1 = 10^{-6}$ 到 $d_n = 10^2$ 的等比数列，共 $n=20$ 项。该数列的公比 $r$ 由 $d_n = d_1 r^{n-1}$ 决定，即 $10^2 = 10^{-6} r^{19}$，或 $r^{19} = 10^8$。因此，$r = (10^8)^{1/19}$。\n\n由于所有特征值 $d_i$ 均为严格正数，矩阵 $A$ 是对称正定（SPD）的。向量 $b \\in \\mathbb{R}^n$ 是一个固定的随机向量。\n\n### 2. 位移矩阵 $(A + \\lambda I)$ 的性质\n\n$(A + \\lambda I)$ 的性质直接由 $A$ 的性质导出。设 $v_i$ 是 $A$ 对应于特征值 $d_i$ 的特征向量，使得 $A v_i = d_i v_i$。将位移矩阵应用于此特征向量，得到：\n$$\n(A + \\lambda I) v_i = A v_i + \\lambda I v_i = d_i v_i + \\lambda v_i = (d_i + \\lambda) v_i\n$$\n这表明 $(A + \\lambda I)$ 与 $A$ 具有相同的特征向量 $v_i$，其特征值则被 $\\lambda$ 位移为 $d_i' = d_i + \\lambda$。因此，位移矩阵的特征分解为：\n$$\nA + \\lambda I = Q (D + \\lambda I) Q^{\\mathsf T}\n$$\n由于所有 $d_i  0$ 且给定的位移 $\\lambda \\ge 0$，所有特征值 $d_i' = d_i + \\lambda$ 都是严格正数。因此，对于所有 $\\lambda \\in \\Lambda = \\{0, 10^{-3}, 1, 100\\}$，$(A + \\lambda I)$ 也是对称正定的。\n\n### 3. 任务1：通过特征分解求精确解\n\n第一个任务是求解线性系统 $(A + \\lambda I)y = b$ 以得到 $y_{\\mathrm{eig}}$，过程中不显式构造矩阵的逆。使用特征分解：\n$$\ny_{\\mathrm{eig}} = (A + \\lambda I)^{-1} b = \\left( Q (D + \\lambda I) Q^{\\mathsf T} \\right)^{-1} b\n$$\n乘积的逆等于逆的乘积，但顺序相反。对于正交矩阵，$Q^{-1} = Q^{\\mathsf T}$。\n$$\ny_{\\mathrm{eig}} = (Q^{\\mathsf T})^{-1} (D + \\lambda I)^{-1} Q^{-1} b = Q (D + \\lambda I)^{-1} Q^{\\mathsf T} b\n$$\n对角矩阵 $(D + \\lambda I)$ 的逆是一个对角元为 $1/(d_i + \\lambda)$ 的对角矩阵。计算过程分三步，避免了矩阵-矩阵乘法：\n1.  将右侧向量 $b$ 变换到 $A$ 的特征基中：$\\hat{b} = Q^{\\mathsf T} b$。\n2.  在特征基中求解系统，这是一个简单的逐元素除法：$\\hat{y}_i = \\hat{b}_i / (d_i + \\lambda)$。这得到了向量 $\\hat{y} = (D + \\lambda I)^{-1} \\hat{b}$。\n3.  将解向量变换回标准基：$y_{\\mathrm{eig}} = Q \\hat{y}$。\n\n该方法数值稳定且高效，可作为本问题的基准真相（ground truth）。\n\n### 4. 任务2：通过共轭梯度（CG）法求解迭代解\n\n共轭梯度（CG）法是一种迭代算法，用于求解线性方程组 $Mx=c$，其中矩阵 $M$ 是对称正定的。在我们的问题中，$M = A + \\lambda I$ 且 $c=b$。\n\n该算法从一个初始猜测 $y_0$（通常是零向量）开始，并迭代地改进解。在每次迭代 $k$ 中，它会计算一个新的解 $y_{k+1}$，该解在特定方向上最小化误差。CG的关键特性是它找到的这些搜索方向是相互 $M$-正交的，这保证了在精确算术下最多 $n$ 步收敛。在浮点算术中，其性能由 $M$ 的条件数决定。\n\n具体的实现将遵循标准算法，当相对残差范数 $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2$ 低于容差 $10^{-10}$ 时，或在达到最大迭代次数 $200$ 次后终止。收敛所需的迭代次数是一个主要输出。\n\n### 5. 任务3：相对误差计算\n\n相对误差 $e_{\\mathrm{rel}}$ 量化了 CG 解 $y_{\\mathrm{cg}}$ 相对于通过特征分解获得的“精确”解 $y_{\\mathrm{eig}}$ 的准确性。其计算公式为：\n$$\ne_{\\mathrm{rel}} = \\frac{\\lVert y_{\\mathrm{cg}} - y_{\\mathrm{eig}} \\rVert_2}{\\lVert y_{\\mathrm{eig}} \\rVert_2}\n$$\n该度量标准直接衡量了迭代近似的质量。\n\n### 6. 任务4：条件数计算\n\n一个SPD矩阵 $M$ 的2-范数条件数 $\\kappa_2(M)$ 是其最大特征值与最小特征值之比：\n$$\n\\kappa_2(M) = \\frac{\\mu_{\\max}}{\\mu_{\\min}}\n$$\n对于我们的位移矩阵 $M = A + \\lambda I$，其特征值为 $d_i + \\lambda$。特征值 $d_i$ 按升序排列，从 $10^{-6}$ 到 $10^{2}$。因此，条件数为：\n$$\n\\kappa_2(A + \\lambda I) = \\frac{\\max_i(d_i + \\lambda)}{\\min_i(d_i + \\lambda)} = \\frac{(10^2) + \\lambda}{ (10^{-6}) + \\lambda}\n$$\n随着 $\\lambda$ 从 $0$ 开始增加，该比率逐渐减小并趋近于 $1$。较小的条件数表示一个条件更好的矩阵，这通常会导致像CG这样的迭代求解器收敛更快。这是该问题旨在通过数值方式证明的核心理论点。\n\n### 7. 任务5：对数行列式比较\n\n行列式的自然对数 $\\log \\det(A + \\lambda I)$ 将通过两种方法计算，以验证数值一致性。\n\n**方法 (i): 使用特征值**\n矩阵的行列式是其特征值的乘积。\n$$\n\\det(A + \\lambda I) = \\prod_{i=1}^n (d_i + \\lambda)\n$$\n为避免在乘以许多数字时可能发生的数值上溢或下溢，我们直接计算行列式的对数：\n$$\n\\log \\det(A + \\lambda I) = \\log\\left(\\prod_{i=1}^n (d_i + \\lambda)\\right) = \\sum_{i=1}^n \\log(d_i + \\lambda)\n$$\n\n**方法 (ii): 使用 Cholesky 分解**\n对于任何 SPD 矩阵 $M$，Cholesky 分解计算出一个唯一的对角元为正的下三角矩阵 $L$，使得 $M = L L^{\\mathsf T}$。行列式则为：\n$$\n\\det(M) = \\det(L L^{\\mathsf T}) = \\det(L) \\det(L^{\\mathsf T}) = (\\det(L))^2\n$$\n三角矩阵的行列式是对角元素的乘积，因此 $\\det(L) = \\prod_{i=1}^n L_{ii}$。\n$$\n\\det(M) = \\left(\\prod_{i=1}^n L_{ii}\\right)^2\n$$\n同样，为了数值稳定性，最好使用对数进行计算：\n$$\n\\log \\det(M) = \\log\\left(\\left(\\prod_{i=1}^n L_{ii}\\right)^2\\right) = 2 \\log\\left(\\prod_{i=1}^n L_{ii}\\right) = 2 \\sum_{i=1}^n \\log(L_{ii})\n$$\n这两种方法所得值的绝对差 $\\Delta_{\\log\\det}$ 应该在机器精度的数量级上，这证实了两种理论方法及其数值实现的正确性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to perform the analysis of the shifted matrix (A + lambda*I).\n    It generates the matrix A, the vector b, and then iterates through a set of\n    shift parameters lambda to compute several quantities of interest.\n    \"\"\"\n\n    # Problem setup (fixed for reproducibility)\n    n = 20\n    seed_A = 7\n    seed_b = 13\n    lambda_set = [0.0, 1e-3, 1.0, 100.0]\n    \n    # Construct the symmetric positive definite matrix A\n    rng_A = np.random.default_rng(seed_A)\n    R = rng_A.standard_normal((n, n))\n    Q, _ = np.linalg.qr(R)\n    \n    d = np.geomspace(1e-6, 1e2, n)\n    D = np.diag(d)\n    A = Q @ D @ Q.T\n\n    # Define the right-hand side vector b\n    rng_b = np.random.default_rng(seed_b)\n    b = rng_b.standard_normal(n)\n\n    def conjugate_gradient(M, c, tol=1e-10, max_iter=200):\n        \"\"\"\n        Implements the Conjugate Gradient method for solving M*y = c.\n        - M: A symmetric positive definite matrix.\n        - c: The right-hand side vector.\n        - tol: The tolerance for the relative residual stopping criterion.\n        - max_iter: The maximum number of iterations.\n        Returns the solution vector y and the number of iterations performed.\n        \"\"\"\n        y = np.zeros_like(c)\n        r = c - M @ y\n        p = r.copy()\n        rs_old = r.T @ r\n        norm_c = np.linalg.norm(c)\n\n        if norm_c == 0:\n            return y, 0\n\n        num_iter = 0\n        for i in range(max_iter):\n            num_iter = i + 1\n            Mp = M @ p\n            alpha = rs_old / (p.T @ Mp)\n            y = y + alpha * p\n            r = r - alpha * Mp\n            \n            rs_new = r.T @ r\n            \n            if np.sqrt(rs_new) / norm_c  tol:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return y, num_iter\n\n    all_results = []\n    \n    for lambda_val in lambda_set:\n        # Define the shifted matrix for the current lambda\n        M = A + lambda_val * np.eye(n)\n        \n        # Task 1: Compute y_eig using eigendecomposition\n        b_hat = Q.T @ b\n        y_hat = b_hat / (d + lambda_val)\n        y_eig = Q @ y_hat\n\n        # Task 2: Compute y_cg using Conjugate Gradient\n        y_cg, cg_iterations = conjugate_gradient(M, b, tol=1e-10, max_iter=200)\n\n        # Task 3: Compute relative error e_rel\n        norm_y_eig = np.linalg.norm(y_eig)\n        e_rel = np.linalg.norm(y_cg - y_eig) / norm_y_eig if norm_y_eig > 0 else 0.0\n\n        # Task 4: Compute 2-norm condition number kappa_2\n        shifted_eigenvalues = d + lambda_val\n        kappa_2 = np.max(shifted_eigenvalues) / np.min(shifted_eigenvalues)\n\n        # Task 5: Compute log-determinant difference\n        # (i) From eigendecomposition\n        log_det_eig = np.sum(np.log(shifted_eigenvalues))\n\n        # (ii) From Cholesky factorization\n        # Using scipy.linalg.cholesky as it is specified in allowed libraries.\n        # np.linalg.cholesky would also work.\n        try:\n            L = scipy.linalg.cholesky(M, lower=True)\n            log_det_chol = 2 * np.sum(np.log(np.diag(L)))\n            delta_log_det = np.abs(log_det_eig - log_det_chol)\n        except np.linalg.LinAlgError:\n            # This should not happen since M is SPD for lambda >= 0\n            delta_log_det = np.nan\n\n        all_results.append([cg_iterations, e_rel, kappa_2, delta_log_det])\n\n    # Format the final output string as per requirements\n    output_parts = []\n    for res in all_results:\n        iters, err, kappa, d_log_det = res\n        part = f\"[{iters},{err:.12f},{kappa:.6f},{d_log_det:.12f}]\"\n        output_parts.append(part)\n        \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3141142"}]}