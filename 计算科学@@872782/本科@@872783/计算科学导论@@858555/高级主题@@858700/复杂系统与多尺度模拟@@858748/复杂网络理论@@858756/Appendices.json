{"hands_on_practices": [{"introduction": "网络中的节点往往会形成紧密的群组，即“社区”。识别这些社区是理解网络结构和功能的关键。本练习将指导你通过两种不同的方法来揭示网络中的社区结构：一种是基于节点邻域的相似性，另一种是利用标签传播算法。通过亲手实现这些方法并使用规范化互信息（Normalized Mutual Information, NMI）来比较它们的结果，你将深入理解社区发现的基本原理和评估方法 [@problem_id:3108222]。", "problem": "给定节点标签在集合 $\\{0,1,2,\\dots,n-1\\}$ 中的简单、无向、无权图。对于一个节点 $i$，定义其开邻域 $N(i)$ 为与 $i$ 相邻的节点集合（即 $i \\notin N(i)$）。从集合论和信息论的第一性原理出发，对每个测试图执行以下步骤：\n\n1. 仅使用集合交集、集合并集和集合基数的定义，计算每对无序节点 $\\{i,j\\}$ 之间的邻域重叠相似度。该相似度基于它们的邻域相对于两个邻域中所有不同邻居总数的重叠程度。如果邻域的并集基数为 $0$，则按约定将 $i \\neq j$ 的成对相似度定义为 $0$，并将所有节点 $i$ 的自相似度定义为 $1$。\n2. 通过在同一节点集上构建相似度图，将这些成对相似度转换为簇。当且仅当节点 $i$ 和 $j$ 之间的相似度大于或等于给定阈值 $\\tau$ 时，它们之间存在一条无向边。将簇定义为该相似度图的连通分量。\n3. 使用标签传播算法 (LPA) 在原始图上独立计算社区分配。为确保确定性，该算法定义如下。初始化每个节点 $i$ 的标签为其自身的索引 $i$。按索引递增的顺序对节点重复进行完全异步扫描；当访问具有非空邻域 $N(i)$ 的节点 $i$ 时，将其标签更新为 $\\{ \\text{label}(u) : u \\in N(i) \\}$ 中最频繁的标签。通过选择最小的标签值来打破平局。如果 $N(i)$ 为空，则保持其当前标签。当一次完整的扫描没有导致任何标签改变，或者扫描次数达到 $100$ 次时，终止算法。\n4. 使用信息论中的归一化互信息来量化两个划分（阈值化相似度簇和标签传播算法社区）之间的一致性。根据节点集上的计数，构建两个划分的簇标签的经验联合分布。然后从该联合分布计算互信息，并使用两个划分的熵的几何平均值对其进行归一化。如果两个划分的熵都为零，则将归一化互信息定义为 $1$。如果其中一个划分的熵为零而另一个不为零，则将归一化互信息定义为 $0$。将每个测试图的结果表示为闭区间 $[0,1]$ 内的一个实数，并四舍五入到 $6$ 位小数。\n\n使用以下图和阈值的测试套件。每个图都是简单、无向、无权的，没有自环。\n\n- 测试图 $1$，其中 $n=8$，阈值 $\\tau=0.5$。边集\n  $E_1 = \\{(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)\\}$。\n- 测试图 $2$，其中 $n=8$，阈值 $\\tau=0.5$。边集\n  $E_2 = E_1 \\cup \\{(3,4)\\}$。\n- 测试图 $3$，其中 $n=5$，阈值 $\\tau=0.3$。边集\n  $E_3 = \\{(0,1),(1,2),(2,0)\\}$，并且节点 $3$ 和 $4$ 是孤立的。\n\n你的程序必须为每个测试图实现上述所有步骤，并生成一行输出，其中包含三个测试的归一化互信息值。这些值应以逗号分隔的浮点数列表形式呈现，四舍五入到 $6$ 位小数，并用方括号括起来，例如 $[x_1,x_2,x_3]$，其中每个 $x_k$ 是测试 $k$ 的结果，四舍五入到 $6$ 位小数。", "solution": "问题陈述已经过仔细验证，并被确定为有效。它在复杂网络理论中有科学依据，具有确定性的算法定义，问题表述适定且客观。所有必要的数据和约定都已提供，确保存在唯一且可验证的解。\n\n该问题要求对三个测试图中的每一个进行四步分析：1. 计算成对邻域重叠相似度矩阵。2. 基于对该相似度矩阵进行阈值化处理的聚类。3. 使用确定性标签传播算法 (LPA) 进行社区检测。4. 使用归一化互信息 (NMI) 量化得到的两个划分之间的一致性。\n\n解题过程按照规定，从第一性原理开始实现每个步骤。\n\n**步骤 1：邻域重叠相似度**\n\n对于一个给定的、具有 $n$ 个标签为 $\\{0, 1, \\dots, n-1\\}$ 的节点的简单无向图，设节点 $i$ 的开邻域是其相邻节点的集合 $N(i)$。两个不同节点 $i$ 和 $j$ 之间的相似度 $S(i,j)$ 由它们的邻域重叠程度相对于它们共同拥有的不同邻居总数来定义。这在形式上是其邻域集合的 Jaccard 指数：\n$$\nS(i,j) = \\frac{|N(i) \\cap N(j)|}{|N(i) \\cup N(j)|}\n$$\n问题提供了两个约定：\n1. 如果 $|N(i) \\cup N(j)| = 0$（即 $i$ 和 $j$ 都是孤立节点）且 $i \\neq j$，则相似度为 $S(i,j) = 0$。\n2. 任何节点 $i$ 的自相似度为 $S(i,i) = 1$。\n\n计算一个 $n \\times n$ 的对称相似度矩阵 $S$，其中每个条目 $S_{ij}$ 存储值 $S(i,j)$。\n\n**步骤 2：阈值化相似度聚类**\n\n从相似度矩阵 $S$ 中导出一个节点划分，我们称之为划分 A。在相同的 $n$ 个节点上构建一个新的“相似度图”。当且仅当节点 $i$ 和 $j$ 之间的相似度 $S(i,j)$ 达到或超过给定的阈值 $\\tau$ 时，它们之间存在一条无向边：\n$$\n(i,j) \\in E_{\\text{sim}} \\iff S(i,j) \\ge \\tau\n$$\n然后将簇定义为该相似度图的连通分量。这些分量可以使用标准的图遍历算法（如广度优先搜索 (BFS) 或深度优先搜索 (DFS)）来识别。每个节点被分配一个与其所属分量对应的簇标签。\n\n**步骤 3：标签传播算法 (LPA)**\n\n使用标签传播算法 (LPA) 获得节点的第二个独立划分，即划分 B。该算法用确定性规则指定，以确保结果唯一。\n- **初始化**：每个节点 $i$ 被赋予其自身的索引作为初始标签，$L(i) \\leftarrow i$。\n- **迭代**：算法以扫描的方式进行。在每次扫描中，节点按其索引从 $0$ 到 $n-1$ 的递增顺序被访问。当访问节点 $i$ 时，其标签会根据其在原始图中的邻居的标签进行更新。\n- **更新规则**：对于具有非空邻域 $N(i)$ 的节点 $i$，其新标签将成为其邻居 $\\{L(u) : u \\in N(i)\\}$ 中最频繁的标签。此更新是异步的，意味着在当前扫描中更新的节点 $j  i$ 的标签会立即用于节点 $i$ 的更新。\n- **平局打破**：如果邻域中有多个标签共享最高频率，则选择数值最小的那个。\n- **孤立节点**：如果 $N(i)$ 为空，$L(i)$ 保持不变。\n- **终止**：当对所有节点的一次完整扫描没有导致任何标签改变，或在最多 $100$ 次扫描后，过程停止。\n\n所有节点的最终标签 $L(i)$ 定义了划分 B 的社区。\n\n**步骤 4：归一化互信息 (NMI)**\n\n划分 A（来自相似度聚类）和划分 B（来自 LPA）之间的一致性通过归一化互信息 (NMI) 进行量化。这需要用到信息论中的概念。\n\n给定 $n$ 个节点的两个划分 A 和 B，我们首先构建它们的联合概率分布。设 A 中的簇为 $\\{a_k\\}$，B 中的社区为 $\\{b_l\\}$。一个节点属于簇 $a_k$ 的概率是 $P(a_k) = |a_k|/n$，属于社区 $b_l$ 的概率是 $P(b_l) = |b_l|/n$。联合概率是 $P(a_k, b_l) = |a_k \\cap b_l|/n$。\n\n一个划分（例如 A）的 Shannon 熵衡量其信息内容或不确定性：\n$$\nH(A) = - \\sum_{k} P(a_k) \\log P(a_k)\n$$\n互信息 $I(A,B)$ 衡量两个划分之间的共享信息：\n$$\nI(A,B) = \\sum_{k} \\sum_{l} P(a_k, b_l) \\log \\frac{P(a_k, b_l)}{P(a_k)P(b_l)}\n$$\nNMI 是通过两个划分各自熵的几何平均值进行归一化的互信息，将结果缩放到 $[0,1]$ 范围内：\n$$\n\\text{NMI}(A,B) = \\frac{I(A,B)}{\\sqrt{H(A)H(B)}}\n$$\n问题为分母为零的情况指定了约定：\n1. 如果 $H(A)=0$ 且 $H(B)=0$（两个划分都是平凡的，只包含一个组），则 $\\text{NMI} = 1$。\n2. 如果 $H(A)$ 或 $H(B)$ 中只有一个为零（一个划分是平凡的，另一个不是），则 $\\text{NMI} = 0$。\n\n每个测试用例的最终结果是此 NMI 值，四舍五入到 $6$ 位小数。", "answer": "```python\nimport numpy as np\n\ndef _calculate_similarity_matrix(adj, n):\n    \"\"\"Computes the Jaccard similarity matrix for node neighborhoods.\"\"\"\n    S = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        S[i, i] = 1.0\n        for j in range(i + 1, n):\n            N_i = adj[i]\n            N_j = adj[j]\n            \n            intersection_size = len(N_i.intersection(N_j))\n            union_size = len(N_i.union(N_j))\n            \n            if union_size == 0:\n                similarity = 0.0\n            else:\n                similarity = intersection_size / union_size\n            \n            S[i, j] = S[j, i] = similarity\n    return S\n\ndef _get_similarity_clusters(S, n, tau):\n    \"\"\"Finds clusters as connected components of the thresholded similarity graph.\"\"\"\n    sim_adj = [set() for _ in range(n)]\n    for i in range(n):\n        for j in range(i + 1, n):\n            if S[i, j] = tau:\n                sim_adj[i].add(j)\n                sim_adj[j].add(i)\n\n    visited = np.full(n, False, dtype=bool)\n    clusters = np.full(n, -1, dtype=int)\n    cluster_id = 0\n    for i in range(n):\n        if not visited[i]:\n            queue = [i]\n            visited[i] = True\n            while queue:\n                u = queue.pop(0)\n                clusters[u] = cluster_id\n                for v in sim_adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        queue.append(v)\n            cluster_id += 1\n    return clusters\n\ndef _get_most_frequent_label(neighbor_labels):\n    \"\"\"Finds the most frequent label, with tie-breaking by smallest value.\"\"\"\n    if not neighbor_labels:\n        return None\n    \n    counts = {}\n    for label in neighbor_labels:\n        counts[label] = counts.get(label, 0) + 1\n    \n    max_freq = 0\n    # In Python 3.7+ dicts are insertion ordered. No assumptions made here.\n    for count in counts.values():\n        if count  max_freq:\n            max_freq = count\n\n    tied_labels = []\n    for label, count in counts.items():\n        if count == max_freq:\n            tied_labels.append(label)\n    \n    return min(tied_labels)\n\ndef _run_lpa(adj, n):\n    \"\"\"Runs the deterministic Label Propagation Algorithm.\"\"\"\n    labels = np.arange(n)\n    max_sweeps = 100\n    \n    for _ in range(max_sweeps):\n        changed = False\n        for i in range(n):\n            neighbors = adj[i]\n            if not neighbors:\n                continue\n            \n            neighbor_labels = [labels[neighbor] for neighbor in neighbors]\n            new_label = _get_most_frequent_label(neighbor_labels)\n            \n            if labels[i] != new_label:\n                labels[i] = new_label\n                changed = True\n        \n        if not changed:\n            break\n            \n    return labels\n\ndef _calculate_nmi(partition_A, partition_B, n):\n    \"\"\"Computes the Normalized Mutual Information between two partitions.\"\"\"\n    unique_labels_A = np.unique(partition_A)\n    unique_labels_B = np.unique(partition_B)\n    num_clusters_A = len(unique_labels_A)\n    num_clusters_B = len(unique_labels_B)\n    \n    map_A = {label: i for i, label in enumerate(unique_labels_A)}\n    map_B = {label: i for i, label in enumerate(unique_labels_B)}\n\n    contingency = np.zeros((num_clusters_A, num_clusters_B), dtype=float)\n    for i in range(n):\n        a_idx = map_A[partition_A[i]]\n        b_idx = map_B[partition_B[i]]\n        contingency[a_idx, b_idx] += 1\n    \n    p_ab = contingency / n\n    p_a = np.sum(p_ab, axis=1)\n    p_b = np.sum(p_ab, axis=0)\n\n    # Calculate entropies\n    h_a = -np.sum(p_a[p_a  0] * np.log(p_a[p_a  0]))\n    h_b = -np.sum(p_b[p_b  0] * np.log(p_b[p_b  0]))\n\n    # Handle zero entropy cases as per problem convention\n    is_ha_zero = np.isclose(h_a, 0)\n    is_hb_zero = np.isclose(h_b, 0)\n    \n    if is_ha_zero and is_hb_zero:\n        return 1.0\n    if is_ha_zero or is_hb_zero:\n        return 0.0\n\n    # Calculate mutual information\n    i_ab = 0.0\n    for r in range(num_clusters_A):\n        for c in range(num_clusters_B):\n            if p_ab[r, c]  0:\n                i_ab += p_ab[r, c] * np.log(p_ab[r, c] / (p_a[r] * p_b[c]))\n    \n    # Calculate NMI\n    nmi = i_ab / np.sqrt(h_a * h_b)\n    return nmi\n\ndef solve_one_case(n, edges, tau):\n    \"\"\"Processes a single test graph through all steps.\"\"\"\n    # Build adjacency list (list of sets for efficient operations)\n    adj = [set() for _ in range(n)]\n    for u, v in edges:\n        adj[u].add(v)\n        adj[v].add(u)\n    \n    # 1. Compute neighborhood-overlap similarity\n    similarity_matrix = _calculate_similarity_matrix(adj, n)\n    \n    # 2. Get clusters from similarity graph\n    partition_A = _get_similarity_clusters(similarity_matrix, n, tau)\n    \n    # 3. Get communities from LPA\n    partition_B = _run_lpa(adj, n)\n    \n    # 4. Quantify consistency with NMI\n    nmi = _calculate_nmi(partition_A, partition_B, n)\n    \n    return nmi\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    E1 = {(0,1),(0,2),(0,3),(1,2),(1,3),(2,3),(4,5),(4,6),(4,7),(5,6),(5,7),(6,7)}\n    E2 = E1.union({(3,4)})\n    E3 = {(0,1),(1,2),(2,0)}\n\n    test_cases = [\n        (8, E1, 0.5),\n        (8, E2, 0.5),\n        (5, E3, 0.3)\n    ]\n\n    results = []\n    for n, edges, tau in test_cases:\n        result = solve_one_case(n, edges, tau)\n        results.append(f\"{result:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3108222"}, {"introduction": "网络是动态演化的，新的连接会不断形成。我们能否根据网络当前的结构来预测未来可能出现的连接？本练习将带你探索“链接预测”这一核心问题 [@problem_id:3108231]。你将实现并比较三种经典的预测方法，从简单的“共同邻居”启发式算法到更复杂的基于网络嵌入的方法，并学习如何使用精确的评估指标来衡量它们的预测能力。这项实践有助于培养将网络视为动态实体而非静态对象来分析的思维。", "problem": "给定一个关于简单、无向、无权网络的时序链接预测任务。目标是实现三种链接预测评分函数，并在多个时间快照上以未来的真实边为基准对它们进行评估。您的程序必须对第一个快照中所有可能的不存在的边进行排序，并评估每种方法识别在第二个快照中实际出现的边的效果。\n\n推理和实现的基本依据：\n- 使用简单无向图、节点邻居集合和节点度的定义，这些都源于第一个快照的邻接关系。\n- 使用基于拓扑（共同邻居和度感知加权）以及图的谱嵌入得到的节点对之间的相似性分数概念。\n\n要求仅使用时间 $t=0$ 的第一个快照实现的方法：\n- 共同邻居：仅根据候选对在第一个快照中邻居集合交集的大小为其分配分数。\n- Adamic–Adar：根据第一个快照中的共同邻居为每个候选对分配分数，其中度数较大的共同邻居贡献小于度数较小的共同邻居。\n- 基于嵌入的方法：通过取邻接结构对称归一化后的前 $d$ 个特征向量，从第一个快照构建节点的 $d$ 维谱嵌入，然后通过其嵌入向量的余弦相似度对节点对进行评分。\n\n评估协议：\n- 候选对是在时间 $t=0$ 的第一个快照中所有不构成边的无序节点对 $(u,v)$，其中 $u  v$。\n- 评估指标为“前 $L$ 位精度”（Precision-at-top-$L$），其中 $L$ 是在时间 $t=1$ 实际出现的新边数。该指标计算预测得分排在前 $L$ 位的候选对中，真实为新边的比例。排序时，得分相等的候选对按字典序排列。", "solution": "所呈现的问题是计算网络科学中一个明确定义的任务，特别是时序链接预测。它要求在一组网络快照上实现并评估三种标准的链接预测评分函数。该问题具有科学依据、客观，并为获得唯一解提供了所有必要信息。\n\n问题的核心是基于网络当前的拓扑结构来预测未来的连接。我们在初始时间 $t=0$ 获得一个简单、无向、无权的图，由其节点集 $V$ 和边集 $E_0$ 表示。我们还获得了在未来时间 $t=1$ 出现的新边集 $P$。目标是根据在 $t=1$ 时出现的可能性，对在 $t=0$ 时所有不存在的边（称为候选对）进行排序。\n\n设时间 $t=0$ 时的图为 $G_0=(V, E_0)$。一个候选对是任何满足 $u  v$ 且 $(u,v) \\notin E_0$ 的无序节点对 $(u,v)$。真实正例集合，记为 $P$，由所有在 $t=1$ 时形成边的候选对组成。该集合的大小为 $L = |P|$。我们必须实现三种评分方法，按分数降序对候选对进行排序，并使用“top-$L$ 精度”来评估每种方法的性能。该指标定义为预测的前 $L$ 个链接中属于真实正例集 $P$ 的比例。\n\n三种必需的评分方法是：\n\n1.  **共同邻居 (CN)**：这是一种基于三元闭合原理的局部相似性度量，该原理假定如果两个节点共享一个共同邻居，它们更可能连接。一对 $(u,v)$ 的分数是它们在 $G_0$ 中共享的邻居数量：\n    $$S_{CN}(u,v) = |\\Gamma_0(u) \\cap \\Gamma_0(v)|$$\n    其中 $\\Gamma_0(x)$ 是图 $G_0$ 中节点 $x$ 的邻居集合。\n\n2.  **Adamic–Adar (AA)**：此方法通过为连接度较低的共同邻居分配更大权重来改进共同邻居分数。其直觉是，共享一个低度邻居比共享一个高度中心节点更能表明相似性。分数通过对共同邻居度的对数的倒数求和来计算：\n    $$S_{AA}(u,v) = \\sum_{z \\in \\Gamma_0(u) \\cap \\Gamma_0(v)} \\frac{1}{\\log k_z}$$\n    其中 $k_z$ 是共同邻居 $z$ 在 $G_0$ 中的度。由于 $u$ 和 $v$ 的任何共同邻居 $z$ 都必须与两者都有边相连，其度 $k_z$ 至少为 $2$，从而确保 $\\log k_z > 0$。\n\n3.  **基于嵌入 (EB) 的方法**：这是一种全局方法，通过将节点嵌入到低维向量空间中来捕捉网络结构。其步骤如下：\n    a. 从图 $G_0$ 构建邻接矩阵 $A$ 和对角度矩阵 $D$。\n    b. 计算对称归一化邻接矩阵，定义为 $A_{sym} = D^{-1/2} A D^{-1/2}$。对于度 $k_i=0$ 的任何节点 $i$， $D^{-1/2}$ 中对应的对角线元素取为 $0$，这导致 $A_{sym}$ 的第 $i$ 行和第 $i$ 列为零。\n    c. 对对称矩阵 $A_{sym}$ 进行特征分解。节点嵌入由与 $d$ 个最大特征值相对应的 $d$ 个特征向量构成。这些特征向量构成一个 $N \\times d$ 嵌入矩阵的列，其中 $N = |V|$。节点 $u$ 的嵌入是该矩阵的第 $u$ 行，记为 $\\vec{x}_u$。\n    d. 一对 $(u,v)$ 的分数是其嵌入向量的余弦相似度：\n    $$S_{EB}(u,v) = \\frac{\\vec{x}_u \\cdot \\vec{x}_v}{\\|\\vec{x}_u\\| \\|\\vec{x}_v\\|}$$\n    根据问题规范，如果 $\\|\\vec{x}_u\\|$ 或 $\\|\\vec{x}_v\\|$ 为零，则分数定义为 $0$。\n\n对于每种方法，都会生成一个候选对及其分数的列表。然后该列表按分数降序排序。分数上的任何平局都通过按字典序（先按 $u$ 升序，再按 $v$ 升序）对节点对 $(u,v)$ 进行排序来确定性地解决。此排序列表中的前 $L$ 对构成预测。然后精度计算如下：\n$$\\text{Precision@L} = \\frac{|\\{\\text{Top } L \\text{ predicted pairs}\\} \\cap P|}{L}$$\n\n将此完整过程应用于提供的三个测试案例中的每一个。最终输出是所有案例中所有方法精度分数的汇总。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Computes link prediction precision for three methods on three test cases.\n    \"\"\"\n\n    def solve_case(nodes, edges_t0, new_edges_t1, d):\n        \"\"\"\n        Processes a single test case for link prediction evaluation.\n\n        Args:\n            nodes (list): A list of node identifiers.\n            edges_t0 (list of tuples): Edges in the graph at time t=0.\n            new_edges_t1 (list of tuples): Edges added at time t=1.\n            d (int): The dimension for the spectral embedding.\n\n        Returns:\n            list: A list of three float values representing the precision-at-top-L\n                  for Common Neighbors, Adamic-Adar, and Embedding-based methods.\n        \"\"\"\n        num_nodes = len(nodes)\n\n        # 1. Build graph representations at t=0\n        adj_matrix = np.zeros((num_nodes, num_nodes), dtype=int)\n        adj_list = {i: set() for i in range(num_nodes)}\n        for u, v in edges_t0:\n            adj_matrix[u, v] = 1\n            adj_matrix[v, u] = 1\n            adj_list[u].add(v)\n            adj_list[v].add(u)\n        \n        degrees = np.sum(adj_matrix, axis=1)\n\n        # 2. Identify candidate and ground truth pairs\n        candidate_pairs = []\n        for i in range(num_nodes):\n            for j in range(i + 1, num_nodes):\n                if adj_matrix[i, j] == 0:\n                    candidate_pairs.append((i, j))\n        \n        ground_truth_positives = {tuple(sorted(edge)) for edge in new_edges_t1}\n        L = len(ground_truth_positives)\n        \n        # If L is 0, precision is typically considered 1.0 or undefined.\n        # The problem cases ensure L  0.\n        if L == 0:\n            return [1.0, 1.0, 1.0]\n\n        precisions = []\n\n        # -- Method 1: Common Neighbors --\n        cn_scores = []\n        for u, v in candidate_pairs:\n            score = float(len(adj_list[u].intersection(adj_list[v])))\n            cn_scores.append((score, u, v))\n\n        cn_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_cn = { (u,v) for score, u, v in cn_scores[:L] }\n        hits_cn = len(top_L_cn.intersection(ground_truth_positives))\n        precisions.append(hits_cn / L)\n\n        # -- Method 2: Adamic-Adar --\n        aa_scores = []\n        for u, v in candidate_pairs:\n            score = 0.0\n            common_neighbors = adj_list[u].intersection(adj_list[v])\n            for z in common_neighbors:\n                # Degree of a common neighbor is at least 2, so log(deg)  0.\n                score += 1 / np.log(degrees[z])\n            aa_scores.append((score, u, v))\n        \n        aa_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_aa = { (u,v) for score, u, v in aa_scores[:L] }\n        hits_aa = len(top_L_aa.intersection(ground_truth_positives))\n        precisions.append(hits_aa / L)\n\n        # -- Method 3: Embedding-based --\n        eb_scores = []\n        # Calculate embedding only if dimension d is positive\n        if d  0:\n            D_inv_sqrt_diag = [1/np.sqrt(deg) if deg  0 else 0 for deg in degrees]\n            D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n            A_sym = D_inv_sqrt @ adj_matrix @ D_inv_sqrt\n            \n            # eigh returns eigenvalues in ascending order.\n            _, eigenvectors = eigh(A_sym)\n            \n            # Take eigenvectors corresponding to the d largest eigenvalues.\n            embedding = eigenvectors[:, -d:]\n\n            for u, v in candidate_pairs:\n                vec_u = embedding[u]\n                vec_v = embedding[v]\n                norm_u = np.linalg.norm(vec_u)\n                norm_v = np.linalg.norm(vec_v)\n                \n                if norm_u == 0 or norm_v == 0:\n                    score = 0.0\n                else:\n                    score = np.dot(vec_u, vec_v) / (norm_u * norm_v)\n                eb_scores.append((score, u, v))\n        else: # If d=0, embedding is trivial, all scores are 0.\n            for u, v in candidate_pairs:\n                eb_scores.append((0.0, u, v))\n\n        eb_scores.sort(key=lambda x: (-x[0], x[1], x[2]))\n        top_L_eb = { (u,v) for score, u, v in eb_scores[:L] }\n        hits_eb = len(top_L_eb.intersection(ground_truth_positives))\n        precisions.append(hits_eb / L)\n        \n        return precisions\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"nodes\": list(range(7)),\n            \"edges_t0\": [(0, 1), (0, 2), (1, 2), (2, 3), (3, 4), (4, 5)],\n            \"new_edges_t1\": [(0, 3), (1, 3), (5, 6)],\n            \"d\": 2\n        },\n        {\n            \"nodes\": list(range(6)),\n            \"edges_t0\": [(0, 1), (0, 2), (0, 3), (3, 4), (4, 5)],\n            \"new_edges_t1\": [(1, 2), (1, 3), (2, 3)],\n            \"d\": 2\n        },\n        {\n            \"nodes\": list(range(5)),\n            \"edges_t0\": [(0, 1)],\n            \"new_edges_t1\": [(2, 3)],\n            \"d\": 1\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = solve_case(case[\"nodes\"], case[\"edges_t0\"], case[\"new_edges_t1\"], case[\"d\"])\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{x:.3f}\" for x in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3108231"}, {"introduction": "在大型网络中，我们往往无法分析所有节点，只能通过采样来估计其性质。随机游走是一种常见的采样方法，但它并非“公平”的——它会偏向于访问度数更高的节点，这便是著名的“友谊悖论”背后的原理。本练习将让你亲手实现一个随机游走采样器，量化其带来的采样偏差，并学习如何利用重要性采样方法来修正这种偏差，从而获得对网络度分布的无偏估计 [@problem_id:3108301]。这个过程将加深你对网络数据分析中潜在陷阱的认识。", "problem": "您的任务是实现一个在无向图上的简单随机游走采样器，并用它来估计图的度分布。度分布用 $P(k)$ 表示，它是一个均匀选择的节点度为 $k$ 的概率。简单随机游走偏向于访问高度节点，这一现象与友谊悖论有关。您的目标是量化这种偏倚，并使用从第一性原理推导出的重要性采样权重来校正它。最终的可交付成果是一个完整的、可运行的程序，该程序对指定的测试套件执行所需的计算，并输出一行包含汇总数值结果。\n\n定义与基础理论：\n- 设 $G = (V, E)$ 是一个有限、简单、无向、连通图。设 $|V| = n$ 和 $|E| = m$。\n- 对于 $i \\in V$，设 $k_i$ 表示节点 $i$ 的度，即节点 $i$ 的邻居数量。\n- 度分布为 $P(k) = \\frac{1}{n}\\sum_{i \\in V} \\mathbf{1}\\{k_i = k\\}$，其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n- 在 $G$ 上的简单随机游走是 $V$ 上的一个马尔可夫链，使得在时间 $t$ 从节点 $i$ 出发，在时间 $t+1$ 的下一个状态是 $i$ 的一个均匀随机的邻居。\n- 无向图上简单随机游走的平稳分布 $\\pi$ 存在并满足细致平衡。根据细致平衡和 $G$ 的无向性，可以推导出 $\\pi_i$ 与 $k_i$ 成正比，因此 $\\pi_i = \\frac{k_i}{2m}$，其中 $2m = \\sum_{i \\in V} k_i$。\n- 因为随机游走根据 $\\pi$ 采样节点，所以度的采样带有规模偏倚。友谊悖论正是源于这种规模偏倚：邻居的期望度往往高于均匀采样的节点。\n- 为了从随机游走样本中估计 $P(k)$，您必须校正规模偏倚。使用基于采样概率与 $k_i$ 成正比这一事实的重要性采样，您应该设计一个自归一化估计量，该估计量使用的权重与 $k_i$ 成反比，从而抵消偏倚，同时确保该估计量是一个有效的概率分布。\n\n需要实现的任务：\n1. 实现一个函数，用于在由邻接表指定的给定无向图上模拟简单随机游走。该游走必须：\n   - 通过从 $V$ 中均匀随机采样来初始化起始节点。\n   - 演化 $B$ 步作为预烧期，您必须丢弃这些步骤。\n   - 然后收集随后的 $S$ 次节点访问作为样本。\n   - 使用以固定种子初始化的伪随机数生成器以保证可复现性。\n2. 通过枚举所有节点来计算图的精确度分布 $P(k)$。\n3. 通过计算观察到的度的经验频率，从 $S$ 个访问过的节点中计算朴素估计量 $\\widehat{P}_{\\text{naive}}(k)$。\n4. 推导并实现一个自归一化重要性采样估计量 $\\widehat{P}_{\\text{w}}(k)$，该估计量校正了 $\\pi_i \\propto k_i$ 所隐含的偏倚。根据上述基础理论，证明使用与 $k_i$ 成反比的权重的合理性，并在代码中实现此估计量。\n5. 通过与真实度分布的 $L\\text{-}1$ 距离（也称为 $L_1$ 范数距离）来量化两种估计量的估计误差：\n   $$\\|\\widehat{P} - P\\|_1 = \\sum_{k \\in \\mathcal{K}} \\left|\\widehat{P}(k) - P(k)\\right|,$$\n   其中 $\\mathcal{K}$ 是图中存在的所有度的并集。报告朴素估计量和加权估计量的此误差。\n6. 四舍五入规则：对于每个测试用例，在生成最终输出之前，将两个 $L_1$ 误差都四舍五入到 $6$ 位小数。\n\n测试套件：\n在以下图和参数上实现并运行您的程序。所有图都是简单、无向和连通的。用邻接表表示图，其中每个节点都由一个整数标记。所有数值都应按给定的方式精确解释。\n\n- 测试用例 1（异构的中心辐射型结构：一个星形图）\n  - 图：一个有 $n = 8$ 个节点的星形图，中心节点 $0$ 连接到节点 $1,2,3,4,5,6,7$。度为：节点 $0$ 的度为 $7$；节点 $1$ 到 $7$ 的度均为 $1$。\n  - 预烧期 $B = 1000$\n  - 样本数 $S = 200000$\n  - 随机种子 $= 314159$\n\n- 测试用例 2（正则图：一个环）\n  - 图：一个有 $n = 10$ 个节点的环，节点标记为 $0,1,\\dots,9$，边为 $\\{(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,0)\\}$。每个节点的度都是 $2$。\n  - 预烧期 $B = 1000$\n  - 样本数 $S = 100000$\n  - 随机种子 $= 271828$\n\n- 测试用例 3（异构连通图）\n  - 节点 $= \\{0,1,2,3,4,5,6,7,8,9,10,11\\}$\n  - 边由一条链和一些额外的连接组成：\n    - 链边：$(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11)$\n    - 额外边：$(5,0),(5,2),(5,7),(5,9),(3,8),(3,10),(1,9),(11,7)$\n    - 这产生的度为：$k_0=2$, $k_1=3$, $k_2=3$, $k_3=4$, $k_4=2$, $k_5=6$, $k_6=2$, $k_7=4$, $k_8=3$, $k_9=4$, $k_{10}=3$, $k_{11}=2$。\n  - 预烧期 $B = 1000$\n  - 样本数 $S = 200000$\n  - 随机种子 $= 424242$\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含三个测试用例的结果，形式为成对的列表，每个测试用例一对。每对分别是朴素估计量和加权估计量的 $L_1$ 误差，四舍五入到 $6$ 位小数。\n- 例如，输出应类似于 $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$，其中 $a_i$ 是测试用例 $i$ 的朴素误差，$b_i$ 是加权误差。\n- 输出必须只有一行，没有多余的文本，空格是可选的，数字必须是小数点后恰好有 $6$ 位的小数。", "solution": "我们从基本定义和一个关于无向图上简单随机游走的著名性质开始。设 $G=(V,E)$ 是一个有限、简单、无向、连通图。对于一个度为 $k_i$ 的节点 $i \\in V$，简单随机游走从 $i$ 转移到任意邻居 $j$ 的概率为 $1/k_i$。平稳分布 $\\pi$ 是一个满足 $\\pi = \\pi P$ 的向量，其中 $P$ 是转移矩阵。对于无向图，细致平衡提供了一个核心恒等式：\n$$\n\\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all edges } (i,j)\\in E.\n$$\n由于对于边 $(i,j)$，$P_{ij} = 1/k_i$ 且 $P_{ji} = 1/k_j$，细致平衡意味着\n$$\n\\frac{\\pi_i}{k_i} = \\frac{\\pi_j}{k_j} \\quad \\text{whenever } (i,j) \\in E,\n$$\n并且由于连通性，这个比率在 $V$ 上是常数。设该常数为 $c$。则 $\\pi_i = c k_i$。归一化到 $\\sum_{i \\in V} \\pi_i = 1$ 可得\n$$\n\\pi_i = \\frac{k_i}{\\sum_{\\ell \\in V} k_\\ell} = \\frac{k_i}{2m}.\n$$\n因此，在平稳状态下，随机游走在任何一步访问节点 $i$ 的概率是 $\\pi_i$，该概率与 $k_i$ 成正比。这直接显示了规模偏倚：游走样本中引出的度分布不是 $P(k)$，而是\n$$\nQ(k) = \\sum_{i \\in V : k_i = k} \\pi_i = \\sum_{i \\in V : k_i = k} \\frac{k_i}{2m} = \\frac{k}{2m} \\cdot \\#\\{i \\in V : k_i = k\\}.\n$$\n由于 $P(k) = \\frac{1}{n}\\#\\{i \\in V : k_i = k\\}$，我们有 $Q(k) \\propto k \\, P(k)$，也就是说，随机游走从 $P(k)$ 的一个规模偏倚版本中采样度，该版本对较大的 $k$ 进行了加权。这解释了友谊悖论：随机选择的邻居的平均度高于均匀采样的节点的平均度。\n\n为了从随机游走样本中估计 $P(k)$，我们必须校正由 $\\pi$ 引起的偏倚。重要性采样提供了原则性的方法。如果样本是根据一个与 $k_i$ 成正比的分布抽取的，那么一种霍维茨-汤普森式的校正方法会根据每个样本的选择概率的倒数来对其进行重新加权。由于 $\\pi_i \\propto k_i$，适当的重要性权重与 $1/k_i$ 成正比。使用自归一化重要性采样，对于一组采样到的节点 $X_1, X_2, \\dots, X_S$ 及其度 $K_s = k_{X_s}$，定义权重 $w_s = 1/K_s$。自归一化估计量为\n$$\n\\widehat{P}_{\\mathrm{w}}(k) = \\frac{\\sum_{s=1}^{S} w_s \\, \\mathbf{1}\\{K_s = k\\}}{\\sum_{s=1}^{S} w_s}.\n$$\n根据其构造，该估计量是一个有效的概率质量函数（非负且总和为1），并且它消除了规模偏倚，因为 $w_s$ 抵消了由采样分布产生的因子 $K_s$。相比之下，朴素估计量\n$$\n\\widehat{P}_{\\mathrm{naive}}(k) = \\frac{1}{S} \\sum_{s=1}^{S} \\mathbf{1}\\{K_s = k\\}\n$$\n在随机游走采样下偏向于更大的度。\n\n为量化估计误差，我们使用真实度分布 $P(k)$ 和估计值 $\\widehat{P}(k)$ 之间的 $L\\text{-}1$ 距离：\n$$\n\\|\\widehat{P} - P\\|_1 = \\sum_{k \\in \\mathcal{K}} \\left|\\widehat{P}(k) - P(k)\\right|,\n$$\n其中 $\\mathcal{K}$ 是图中存在的所有度的并集。该度量可解释为全变差距离的两倍，其值在 $[0,2]$ 区间内。\n\n算法设计：\n- 从邻接表计算精确度 $\\{k_i\\}_{i\\in V}$，并通过对 $V$ 的枚举获得精确的 $P(k)$。\n- 模拟一个简单随机游走：\n  - 使用具有固定种子的伪随机数生成器，从 $V$ 中均匀随机地初始化当前节点，以实现可复现性。\n  - 对于 $B$ 个预烧期步骤，移动到当前节点的均匀随机邻居；不记录样本。\n  - 然后进行 $S$ 步，重复移动并记录访问的节点（或其度）。\n- 从收集到的度 $K_1,\\dots,K_S$ 中计算：\n  - $\\widehat{P}_{\\mathrm{naive}}(k)$，作为观察到的度的经验频率。\n  - $\\widehat{P}_{\\mathrm{w}}(k)$，使用自归一化重要性权重 $w_s = 1/K_s$。\n- 使用图中度的并集支撑集，计算 $L\\text{-}1$ 误差 $\\|\\widehat{P}_{\\mathrm{naive}} - P\\|_1$ 和 $\\|\\widehat{P}_{\\mathrm{w}} - P\\|_1$。\n- 为了数值稳定性和清晰的输出，将两个误差都四舍五入到 $6$ 位小数。\n\n测试套件的预期定性结果：\n- 星形图（测试用例 1）：朴素估计量将显著高估高度的概率质量，并低估度为 $1$ 的概率质量。加权估计量应能显著减少误差。\n- 环形图（测试用例 2）：该图是正则图，所有 $i$ 的 $k_i = 2$，因此没有规模偏倚。朴素和加权估计量都应与真实值非常接近，产生接近零的误差。\n- 异构图（测试用例 3）：朴素估计量将再次偏向于较大的度；与朴素估计量相比，加权估计量应能减少误差。\n\n程序将为指定的测试套件实现这些步骤，并打印一行包含三对 $L_1$ 误差的结果，每对格式为 $[\\text{naive},\\text{weighted}]$，每个数字格式化为恰好 $6$ 位小数，形式为 $[[a_1,b_1],[a_2,b_2],[a_3,b_3]]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_star_graph(n):\n    # Node 0 is center; nodes 1..n-1 are leaves\n    adj = [[] for _ in range(n)]\n    for v in range(1, n):\n        adj[0].append(v)\n        adj[v].append(0)\n    return adj\n\ndef build_cycle_graph(n):\n    adj = [[] for _ in range(n)]\n    for i in range(n):\n        j = (i + 1) % n\n        adj[i].append(j)\n        adj[j].append(i)\n    return adj\n\ndef build_custom_graph_case3():\n    # Nodes 0..11\n    n = 12\n    edges = []\n    # Chain edges\n    chain_edges = [(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,8),(8,9),(9,10),(10,11)]\n    edges.extend(chain_edges)\n    # Additional edges\n    extra_edges = [(5,0),(5,2),(5,7),(5,9),(3,8),(3,10),(1,9),(11,7)]\n    edges.extend(extra_edges)\n    adj = [[] for _ in range(n)]\n    for u,v in edges:\n        adj[u].append(v)\n        adj[v].append(u)\n    return adj\n\ndef degrees_from_adj(adj):\n    return np.array([len(neigh) for neigh in adj], dtype=int)\n\ndef true_degree_distribution(adj):\n    degs = degrees_from_adj(adj)\n    n = len(degs)\n    unique, counts = np.unique(degs, return_counts=True)\n    pk = {int(k): c / n for k, c in zip(unique, counts)}\n    return pk\n\ndef random_walk_samples(adj, burn_in, samples, seed):\n    rng = np.random.default_rng(seed)\n    n = len(adj)\n    if n == 0:\n        return []\n    current = int(rng.integers(0, n))\n    # Burn-in\n    for _ in range(burn_in):\n        neighbors = adj[current]\n        current = int(neighbors[rng.integers(0, len(neighbors))])\n    # Sampling\n    visited = []\n    for _ in range(samples):\n        neighbors = adj[current]\n        current = int(neighbors[rng.integers(0, len(neighbors))])\n        visited.append(current)\n    return visited\n\ndef naive_degree_estimate(adj, visited_nodes):\n    degs = degrees_from_adj(adj)\n    sample_degrees = degs[visited_nodes]\n    if len(sample_degrees) == 0:\n        return {}\n    unique, counts = np.unique(sample_degrees, return_counts=True)\n    total = sample_degrees.size\n    est = {int(k): c / total for k, c in zip(unique, counts)}\n    return est\n\ndef weighted_degree_estimate(adj, visited_nodes):\n    degs = degrees_from_adj(adj)\n    sample_degrees = degs[visited_nodes].astype(float)\n    if len(sample_degrees) == 0:\n        return {}\n    weights = 1.0 / sample_degrees\n    # Self-normalized importance sampling over observed degrees\n    unique_degrees = np.unique(sample_degrees).astype(int)\n    weight_sum = np.sum(weights)\n    # Avoid division by zero, though degrees are =1 in simple graphs\n    if weight_sum == 0.0:\n        return {int(k): 0.0 for k in unique_degrees}\n    est = {}\n    for k in unique_degrees:\n        mask = (sample_degrees == k)\n        wk = np.sum(weights[mask])\n        est[int(k)] = wk / weight_sum\n    return est\n\ndef l1_distance(true_pk, est_pk, support=None):\n    # support is union of keys if None\n    if support is None:\n        support = set(true_pk.keys()).union(set(est_pk.keys()))\n    dist = 0.0\n    for k in support:\n        dist += abs(est_pk.get(k, 0.0) - true_pk.get(k, 0.0))\n    return dist\n\ndef format_results(results):\n    # results is list of (naive_error, weighted_error) floats\n    # Return a string like [[0.123456,0.000001],[...],...]\n    pairs = []\n    for a, b in results:\n        pairs.append(f\"[{a:.6f},{b:.6f}]\")\n    return \"[\" + \",\".join(pairs) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = []\n\n    # Test case 1: Star graph with n=8\n    adj1 = build_star_graph(8)\n    burn_in1 = 1000\n    samples1 = 200000\n    seed1 = 314159\n    test_cases.append((adj1, burn_in1, samples1, seed1))\n\n    # Test case 2: Cycle graph with n=10\n    adj2 = build_cycle_graph(10)\n    burn_in2 = 1000\n    samples2 = 100000\n    seed2 = 271828\n    test_cases.append((adj2, burn_in2, samples2, seed2))\n\n    # Test case 3: Custom heterogeneous graph\n    adj3 = build_custom_graph_case3()\n    burn_in3 = 1000\n    samples3 = 200000\n    seed3 = 424242\n    test_cases.append((adj3, burn_in3, samples3, seed3))\n\n    results = []\n    for adj, burn_in, samples, seed in test_cases:\n        # True distribution\n        pk_true = true_degree_distribution(adj)\n        # Samples\n        visited = random_walk_samples(adj, burn_in, samples, seed)\n        # Naive estimate\n        pk_naive = naive_degree_estimate(adj, visited)\n        # Weighted estimate\n        pk_weighted = weighted_degree_estimate(adj, visited)\n        # L1 distances over true support\n        support = set(pk_true.keys())\n        e_naive = l1_distance(pk_true, pk_naive, support=support)\n        e_weighted = l1_distance(pk_true, pk_weighted, support=support)\n        results.append((e_naive, e_weighted))\n\n    # Final print statement in the exact required format.\n    print(format_results(results))\n\nsolve()\n```", "id": "3108301"}]}