{"hands_on_practices": [{"introduction": "卡尔曼滤波器的标准推导通常依赖于一个简化假设：模型误差和观测误差是相互独立的。本练习将挑战您超越这一理想情况，探索一个更复杂且通常更现实的场景，即这些误差存在交叉相关性。通过从第一性原理推导更新后的分析方程，您将更深入地理解滤波器在面对这些更复杂的统计关系时，如何以最优方式融合信息。[@problem_id:3123941]", "problem": "考虑在集成卡尔曼滤波器（EnKF; Ensemble Kalman Filter）中使用的一维线性高斯状态空间模型中的单个同化步骤。设当前时刻的真实状态为随机变量 $X_{t}$，其先验（预报）分布为 $X_{t} \\sim \\mathcal{N}(\\mu_{f}, P_{f})$。观测模型为 $Y_{t} = H X_{t} + \\epsilon_{t}$，其中 $\\epsilon_{t}$ 是零均值观测噪声，其方差为 $\\operatorname{Var}(\\epsilon_{t}) = R$。状态由一个带有零均值模型误差 $\\eta_{t}$ 的线性动力学模型产生，且观测误差和模型误差是互相关的，因此 $\\operatorname{Cov}(\\eta_{t}, \\epsilon_{t}) = S \\neq 0$。假设 $X_{t-1}$（所有过去的信息）与 $\\eta_{t}$ 和 $\\epsilon_{t}$ 均独立，并且 $\\eta_{t}$ 和 $\\epsilon_{t}$ 服从联合高斯分布。\n\n任务 A（推导）。仅使用 $(X_{t}, Y_{t})$ 的联合正态性以及均值、方差和协方差的基本法则，推导分析均值 $\\mu_{a} = \\mathbb{E}[X_{t} \\mid Y_{t} = y]$ 的表达式，该表达式应正确考虑非零互协方差 $S$。用 $\\mu_{f}$、$P_{f}$、$H$、$R$、$S$ 和实现的观测值 $y$ 来表示你的结果。\n\n任务 B（数值测试）。在 $H = 1$ 的标量情况下，使用数值 $\\mu_{f} = 2.0$、$P_{f} = 1.5$、$R = 0.5$、$S = 0.3$ 和 $y = 2.4$ 计算您在任务 A 中推导出的分析均值。将您的答案四舍五入到 4 位有效数字。将最终结果表示为一个纯数（无单位）。", "solution": "该问题被认为是有效的。这是一个在随机状态估计领域的适定且有科学依据的问题，随机状态估计是计算科学的核心主题。所有必要信息都已提供，问题没有矛盾或歧义。\n\n解答按要求分为两部分：分析均值的推导（任务 A）和数值计算（任务 B）。\n\n### 任务 A：分析均值 $\\mu_a$ 的推导\n\n目标是求条件期望 $\\mu_a = \\mathbb{E}[X_t \\mid Y_t = y]$。问题陈述我们可以假设状态 $X_t$ 和观测 $Y_t$ 的联合分布是高斯分布。我们可以将其写成一个联合向量：\n$$\n\\begin{pmatrix} X_t \\\\ Y_t \\end{pmatrix} \\sim \\mathcal{N}\\left( \n\\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}, \n\\begin{pmatrix} \\Sigma_{XX}  \\Sigma_{XY} \\\\ \\Sigma_{YX}  \\Sigma_{YY} \\end{pmatrix} \n\\right)\n$$\n为了求出该分布的参数，我们使用期望、方差和协方差的基本法则。\n\n**1. 均值向量的推导**\n\n均值向量的分量是 $X_t$ 和 $Y_t$ 的期望。\n- 状态 $X_t$ 的先验均值给定为 $\\mathbb{E}[X_t] = \\mu_f$。\n- 观测 $Y_t$ 的均值从其模型 $Y_t = H X_t + \\epsilon_t$ 推导得出。利用期望的线性性质以及观测噪声 $\\epsilon_t$ 均值为零（$\\mathbb{E}[\\epsilon_t] = 0$）的事实：\n$$\n\\mathbb{E}[Y_t] = \\mathbb{E}[H X_t + \\epsilon_t] = H \\mathbb{E}[X_t] + \\mathbb{E}[\\epsilon_t] = H \\mu_f + 0 = H \\mu_f\n$$\n因此，均值向量为：\n$$\n\\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix} = \\begin{pmatrix} \\mu_f \\\\ H \\mu_f \\end{pmatrix}\n$$\n\n**2. 协方差矩阵的推导**\n\n协方差矩阵的分量是 $X_t$ 和 $Y_t$ 的方差和协方差。\n- $\\Sigma_{XX} = \\operatorname{Var}(X_t)$：状态 $X_t$ 的先验方差给定为 $P_f$。\n$$\n\\Sigma_{XX} = P_f\n$$\n- $\\Sigma_{XY} = \\operatorname{Cov}(X_t, Y_t)$：我们使用 $Y_t$ 的定义和协方差的双线性性质。\n$$\n\\Sigma_{XY} = \\operatorname{Cov}(X_t, Y_t) = \\operatorname{Cov}(X_t, H X_t + \\epsilon_t) = \\operatorname{Cov}(X_t, H X_t) + \\operatorname{Cov}(X_t, \\epsilon_t) = H \\operatorname{Var}(X_t) + \\operatorname{Cov}(X_t, \\epsilon_t)\n$$\n这变为：\n$$\n\\Sigma_{XY} = H P_f + \\operatorname{Cov}(X_t, \\epsilon_t)\n$$\n为了计算 $\\operatorname{Cov}(X_t, \\epsilon_t)$，我们使用信息：状态 $X_t$ 来自一个带有模型误差 $\\eta_t$ 的线性动力学模型。这意味着 $X_t$ 可以写成 $X_t = \\mathcal{M}(X_{t-1}) + \\eta_t$，其中 $\\mathcal{M}$ 代表线性模型动力学。问题陈述 $X_{t-1}$ 与 $\\eta_t$ 和 $\\epsilon_t$ 均独立，且 $\\operatorname{Cov}(\\eta_t, \\epsilon_t) = S$。\n$$\n\\operatorname{Cov}(X_t, \\epsilon_t) = \\operatorname{Cov}(\\mathcal{M}(X_{t-1}) + \\eta_t, \\epsilon_t) = \\operatorname{Cov}(\\mathcal{M}(X_{t-1}), \\epsilon_t) + \\operatorname{Cov}(\\eta_t, \\epsilon_t)\n$$\n由于 $X_{t-1}$ 和 $\\epsilon_t$ 是独立的，它们的协方差为零。因此：\n$$\n\\operatorname{Cov}(X_t, \\epsilon_t) = 0 + S = S\n$$\n将此代回，我们得到：\n$$\n\\Sigma_{XY} = H P_f + S\n$$\n- $\\Sigma_{YX} = \\operatorname{Cov}(Y_t, X_t)$：根据协方差的对称性，$\\Sigma_{YX} = \\Sigma_{XY}^T$。由于这些都是标量，$\\Sigma_{YX} = \\Sigma_{XY} = H P_f + S$。\n\n- $\\Sigma_{YY} = \\operatorname{Var}(Y_t)$：我们计算观测的方差。\n$$\n\\operatorname{Var}(Y_t) = \\operatorname{Var}(H X_t + \\epsilon_t)\n$$\n使用通用公式 $\\operatorname{Var}(A+B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) + 2\\operatorname{Cov}(A,B)$：\n$$\n\\operatorname{Var}(Y_t) = \\operatorname{Var}(H X_t) + \\operatorname{Var}(\\epsilon_t) + 2\\operatorname{Cov}(H X_t, \\epsilon_t)\n$$\n$$\n\\operatorname{Var}(Y_t) = H^2 \\operatorname{Var}(X_t) + \\operatorname{Var}(\\epsilon_t) + 2 H \\operatorname{Cov}(X_t, \\epsilon_t)\n$$\n代入已知量 $P_f$、$R$ 和 $S$：\n$$\n\\Sigma_{YY} = H^2 P_f + R + 2 H S\n$$\n结合这些项，完整的协方差矩阵是：\n$$\n\\begin{pmatrix} \\Sigma_{XX}  \\Sigma_{XY} \\\\ \\Sigma_{YX}  \\Sigma_{YY} \\end{pmatrix} = \\begin{pmatrix} P_f  H P_f + S \\\\ H P_f + S  H^2 P_f + R + 2 H S \\end{pmatrix}\n$$\n\n**3. 条件均值公式**\n\n对于如上定义的分块多元正态随机向量，$X_t$ 在给定 $Y_t = y$ 条件下的条件分布也是正态分布。其均值由标准公式给出：\n$$\n\\mathbb{E}[X_t \\mid Y_t = y] = \\mathbb{E}[X_t] + \\Sigma_{XY} \\Sigma_{YY}^{-1} (y - \\mathbb{E}[Y_t])\n$$\n代入我们推导出的参数：\n$$\n\\mu_a = \\mu_f + (H P_f + S) (H^2 P_f + R + 2 H S)^{-1} (y - H \\mu_f)\n$$\n这个表达式表示在同化观测值 $y$ 之后的状态的更新（分析）均值。它通常写成 $\\mu_a = \\mu_f + K(y - H\\mu_f)$，其中 $K = \\Sigma_{XY} \\Sigma_{YY}^{-1}$ 是卡尔曼增益，在本例中为 $K = \\frac{H P_f + S}{H^2 P_f + R + 2 H S}$。\n\n### 任务 B：数值计算\n\n我们给定以下标量值：\n- $H = 1$\n- $\\mu_f = 2.0$\n- $P_f = 1.5$\n- $R = 0.5$\n- $S = 0.3$\n- $y = 2.4$\n\n我们将这些值代入在任务 A 中推导出的 $\\mu_a$ 表达式中。\n\n首先，计算更新所需的各项：\n- 新息：$y - H \\mu_f = 2.4 - (1)(2.0) = 0.4$\n- 协方差项 $\\Sigma_{XY}$：$H P_f + S = (1)(1.5) + 0.3 = 1.8$\n- 方差项 $\\Sigma_{YY}$：$H^2 P_f + R + 2 H S = (1)^2(1.5) + 0.5 + 2(1)(0.3) = 1.5 + 0.5 + 0.6 = 2.6$\n\n现在，将这些代入更新公式：\n$$\n\\mu_a = \\mu_f + \\Sigma_{XY} \\Sigma_{YY}^{-1} (y - H \\mu_f)\n$$\n$$\n\\mu_a = 2.0 + (1.8) (2.6)^{-1} (0.4)\n$$\n$$\n\\mu_a = 2.0 + \\frac{1.8 \\times 0.4}{2.6} = 2.0 + \\frac{0.72}{2.6}\n$$\n执行除法：\n$$\n\\frac{0.72}{2.6} = \\frac{72}{260} = \\frac{18}{65} \\approx 0.276923...\n$$\n所以，分析均值为：\n$$\n\\mu_a = 2.0 + 0.276923... = 2.276923...\n$$\n问题要求将答案四舍五入到 4 位有效数字。\n$$\n\\mu_a \\approx 2.277\n$$", "answer": "$$\\boxed{2.277}$$", "id": "3123941"}, {"introduction": "一个稳健的数据同化系统必须能够处理不完美的真实世界数据，包括突发的传感器故障或严重错误。本实践将指导您实现一个至关重要的质量控制机制：卡方（$\\chi^2$）新息检验。您将学习如何利用新息向量的统计特性，来量化评估一个新观测值的“意外程度”，并据此做出接受或将其作为异常值拒绝的正式决策。[@problem_id:2382619]", "problem": "给定数据同化中单个分析步骤的线性高斯观测设置。设预报状态的均值为 $\\mathbf{x}_f \\in \\mathbb{R}^n$，协方差为 $\\mathbf{P}_f \\in \\mathbb{R}^{n \\times n}$。测量值 $\\mathbf{y} \\in \\mathbb{R}^m$ 通过线性观测算子 $\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$ 与状态相关，并带有协方差为 $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ 的加性零均值高斯测量误差。定义新息向量 $\\mathbf{v} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_f$ 和新息协方差 $\\mathbf{S} = \\mathbf{H}\\mathbf{P}_f\\mathbf{H}^\\top + \\mathbf{R}$。在模型、协方差和观测值与线性 Kalman 滤波器或集合 Kalman 滤波器在统计上一致的原假设下，二次型 $z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$ 服从自由度为 $m$ 的卡方（$\\chi^2$）分布。对于给定的显著性水平 $\\alpha \\in (0,1)$，将临界值 $c$ 定义为自由度为 $m$ 的 $\\chi^2$ 分布的上 $(1-\\alpha)$ 分位数。一个测量值当且仅当 $z > c$ 时应被作为异常值剔除。\n\n实现一个程序，对下面套件中的每个测试用例，根据上述规则计算接受或拒绝该测量值的决策。每个测试用例的输出必须是一个布尔值：如果测量值被拒绝，则为 $\\,\\mathrm{True}\\,$，否则为 $\\,\\mathrm{False}\\,$。不涉及物理单位。不出现角度。所有概率和显著性水平都必须视为 $[0,1]$ 内的实数。该测试套件指定了不同的场景，包括一个典型案例、一个明显异常值、一个相关的多变量案例，以及一个 $z$ 恰好等于临界值且不得被拒绝的边界案例。\n\n使用以下测试套件，其中所有矩阵都按要求对称，并在需要时为正定：\n\n- 测试用例 1（单变量，典型接受）：\n  - $\\mathbf{H} = [\\,1.0\\,]$\n  - $\\mathbf{x}_f = [\\,0.0\\,]$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 0.25 \\end{bmatrix}$\n  - $\\mathbf{y} = [\\,0.2\\,]$\n  - $\\alpha = 0.05$\n- 测试用例 2（单变量，明显异常值）：\n  - $\\mathbf{H} = [\\,1.0\\,]$\n  - $\\mathbf{x}_f = [\\,0.0\\,]$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 0.5 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 0.1 \\end{bmatrix}$\n  - $\\mathbf{y} = [\\,3.0\\,]$\n  - $\\alpha = 0.01$\n- 测试用例 3（双变量，相关，接受）：\n  - $\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$\n  - $\\mathbf{x}_f = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 1.0  0.5 \\\\ 0.5  2.0 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 0.2  0.1 \\\\ 0.1  0.3 \\end{bmatrix}$\n  - $\\mathbf{y} = \\begin{bmatrix} 1.3 \\\\ -0.8 \\end{bmatrix}$\n  - $\\alpha = 0.05$\n- 测试用例 4（单变量，边界情况；因 $z = c$ 而不拒绝）：\n  - $\\mathbf{H} = [\\,1.0\\,]$\n  - $\\mathbf{x}_f = [\\,0.0\\,]$\n  - $\\mathbf{P}_f = \\begin{bmatrix} 0.0 \\end{bmatrix}$\n  - $\\mathbf{R} = \\begin{bmatrix} 1.0 \\end{bmatrix}$\n  - $\\mathbf{y} = [\\,1.959963984540054\\,]$\n  - $\\alpha = 0.05$\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$\\texttt{[result1,result2,result3,result4]}$），顺序与上述测试用例相同。每个 $\\,\\texttt{result}\\,$ 必须是 $\\texttt{True}$ 或 $\\texttt{False}$。", "solution": "对问题陈述的有效性进行审查。\n\n步骤 1：提取已知条件\n问题为数据同化情境提供了以下定义和数据：\n- 预报状态均值：$\\mathbf{x}_f \\in \\mathbb{R}^n$\n- 预报状态协方差：$\\mathbf{P}_f \\in \\mathbb{R}^{n \\times n}$\n- 测量向量：$\\mathbf{y} \\in \\mathbb{R}^m$\n- 线性观测算子：$\\mathbf{H} \\in \\mathbb{R}^{m \\times n}$\n- 测量误差协方差：$\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$（零均值高斯误差）\n- 新息向量：$\\mathbf{v} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_f$\n- 新息协方差：$\\mathbf{S} = \\mathbf{H}\\mathbf{P}_f\\mathbf{H}^\\top + \\mathbf{R}$\n- 检验统计量：$z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$\n- $z$ 的分布：自由度为 $m$ 的卡方（$\\chi^2$）分布。\n- 显著性水平：$\\alpha \\in (0,1)$\n- 临界值 $c$：自由度为 $m$ 的 $\\chi^2$ 分布的上 $(1-\\alpha)$ 分位数。\n- 拒绝规则：当且仅当 $z > c$ 时拒绝测量值。\n\n问题提供了四个不同的测试用例，其中包含 $\\mathbf{H}$、$\\mathbf{x}_f$、$\\mathbf{P}_f$、$\\mathbf{R}$、$\\mathbf{y}$ 和 $\\alpha$ 的具体数值。\n\n步骤 2：使用提取的已知条件进行验证\n- **科学依据**：问题描述了新息一致性检验，也称为卡方检验，这是数据同化中的一个标准、基本程序，尤其是在 Kalman 滤波器及其集合变体的背景下。其统计基础——即在给定的高斯假设下，二次型 $z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$ 服从 $\\chi^2$ 分布——是多元统计中一个公认的结果。该问题在科学上和数学上都是合理的。\n- **适定性**：每个测试用例都提供了所有必要的输入（$\\mathbf{x}_f, \\mathbf{P}_f, \\mathbf{y}, \\mathbf{H}, \\mathbf{R}, \\alpha$）。目标是根据一个清晰、明确的规则（$z > c$）计算一个布尔决策。对边界情况（$z = c$）的处理已明确规定，确保对于所有可能的 $z$ 值都有唯一解。所提供的协方差矩阵是按要求对称的，并在需要时被描述为正定的，这保证了新息协方差 $\\mathbf{S}$ 是可逆的。因此，该问题是适定的。\n- **客观性**：该问题使用精确的数学术语陈述。评估标准是一个严格的不等式，没有任何主观解释的余地。\n- **完整性与一致性**：问题是自洽的。每个测试用例中所有矩阵和向量的维度对于所需的矩阵运算都是一致的。例如，在测试用例 3 中，$\\mathbf{H}$ 是 $2 \\times 2$，$\\mathbf{x}_f$ 是 $2 \\times 1$，所以 $\\mathbf{H}\\mathbf{x}_f$ 是 $2 \\times 1$，这与 $\\mathbf{y}$ 的维度（$2 \\times 1$）兼容。用于计算 $\\mathbf{S}$ 的维度也是一致的。\n- **其他标准**：该问题是可形式化的，与计算工程相关，其设置是现实的（尽管经过简化），并且是科学上可验证的。它没有违反任何指定的无效条件。\n\n步骤 3：结论与行动\n问题有效。将构建一个解决方案。\n\n解决方案要求为每个提供的测试用例实现卡方检验。每个用例的程序如下：\n\n1.  确定观测空间的维度 $m$，即观测算子 $\\mathbf{H}$ 的行数（或测量向量 $\\mathbf{y}$ 的维度）。该值代表 $\\chi^2$ 分布的自由度。\n2.  计算新息向量 $\\mathbf{v} = \\mathbf{y} - \\mathbf{H}\\mathbf{x}_f$。该向量表示实际测量值 $\\mathbf{y}$ 与投影到观测空间中的预报状态 $\\mathbf{H}\\mathbf{x}_f$ 之间的差异。\n3.  计算新息协方差矩阵 $\\mathbf{S} = \\mathbf{H}\\mathbf{P}_f\\mathbf{H}^\\top + \\mathbf{R}$。该矩阵量化了新息中的总预期不确定性，结合了来自预报状态的不确定性（通过 $\\mathbf{H}$ 传播）和来自测量本身的不确定性。\n4.  计算检验统计量 $z = \\mathbf{v}^\\top \\mathbf{S}^{-1} \\mathbf{v}$。这是一个标量值，表示新息向量与原点之间的马氏距离的平方，并通过其协方差进行归一化。它衡量在给定预期不确定性的情况下，新息是多么“令人意外”。该计算需要计算 $\\mathbf{S}$ 的逆。\n5.  确定给定显著性水平 $\\alpha$ 的临界值 $c$。值 $c$ 是自由度为 $m$ 的 $\\chi^2$ 分布的上分位数，由 $P(\\chi^2_m \\le c) = 1 - \\alpha$ 定义。该值可以使用 $\\chi^2$ 分布的百分点函数（PPF），也称为逆累积分布函数来获得。\n6.  将检验统计量 $z$ 与临界值 $c$ 进行比较。根据指定的规则，如果 $z > c$，则拒绝测量值。这将产生一个布尔结果。边界情况 $z = c$ 导致不拒绝。\n\n此算法将应用于四个测试用例中的每一个。\n\n- 对于**测试用例 1**（单变量，典型接受）：\n  - $m=1$。$\\mathbf{v} = [0.2] - [1.0][0.0] = [0.2]$。\n  - $\\mathbf{S} = [1.0][1.0][1.0]^\\top + [0.25] = [1.25]$。\n  - $z = [0.2]^\\top [1.25]^{-1} [0.2] = 0.2 \\times (1/1.25) \\times 0.2 = 0.032$。\n  - 对于 $\\alpha=0.05$ 和 $m=1$ 的自由度，临界值为 $c = \\chi^2_1\\text{.ppf}(0.95) \\approx 3.841$。\n  - 决策：$0.032 > 3.841$ 为假。接受该测量值。\n\n- 对于**测试用例 2**（单变量，明显异常值）：\n  - $m=1$。$\\mathbf{v} = [3.0] - [1.0][0.0] = [3.0]$。\n  - $\\mathbf{S} = [1.0][0.5][1.0]^\\top + [0.1] = [0.6]$。\n  - $z = [3.0]^\\top [0.6]^{-1} [3.0] = 3.0 \\times (1/0.6) \\times 3.0 = 15.0$。\n  - 对于 $\\alpha=0.01$ 和 $m=1$ 的自由度，临界值为 $c = \\chi^2_1\\text{.ppf}(0.99) \\approx 6.635$。\n  - 决策：$15.0 > 6.635$ 为真。拒绝该测量值。\n\n- 对于**测试用例 3**（双变量，相关，接受）：\n  - $m=2$。$\\mathbf{x}_f = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$。$\\mathbf{y} = \\begin{bmatrix} 1.3 \\\\ -0.8 \\end{bmatrix}$。$\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}$。\n  - $\\mathbf{H}\\mathbf{x}_f = \\mathbf{x}_f = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$。\n  - $\\mathbf{v} = \\begin{bmatrix} 1.3 \\\\ -0.8 \\end{bmatrix} - \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix} = \\begin{bmatrix} 0.3 \\\\ 0.2 \\end{bmatrix}$。\n  - 由于 $\\mathbf{H}$ 是单位矩阵，$\\mathbf{S} = \\mathbf{P}_f + \\mathbf{R} = \\begin{bmatrix} 1.0  0.5 \\\\ 0.5  2.0 \\end{bmatrix} + \\begin{bmatrix} 0.2  0.1 \\\\ 0.1  0.3 \\end{bmatrix} = \\begin{bmatrix} 1.2  0.6 \\\\ 0.6  2.3 \\end{bmatrix}$。\n  - 逆矩阵为 $\\mathbf{S}^{-1} = \\frac{1}{(1.2)(2.3) - (0.6)(0.6)} \\begin{bmatrix} 2.3  -0.6 \\\\ -0.6  1.2 \\end{bmatrix} = \\frac{1}{2.4} \\begin{bmatrix} 2.3  -0.6 \\\\ -0.6  1.2 \\end{bmatrix}$。\n  - $z = \\begin{bmatrix} 0.3  0.2 \\end{bmatrix} \\frac{1}{2.4} \\begin{bmatrix} 2.3  -0.6 \\\\ -0.6  1.2 \\end{bmatrix} \\begin{bmatrix} 0.3 \\\\ 0.2 \\end{bmatrix} = \\frac{1}{2.4} \\begin{bmatrix} 0.3  0.2 \\end{bmatrix} \\begin{bmatrix} 0.57 \\\\ 0.06 \\end{bmatrix} = \\frac{1}{2.4} (0.171 + 0.012) = \\frac{0.183}{2.4} = 0.07625$。\n  - 对于 $\\alpha=0.05$ 和 $m=2$ 的自由度，临界值为 $c = \\chi^2_2\\text{.ppf}(0.95) \\approx 5.991$。\n  - 决策：$0.07625 > 5.991$ 为假。接受该测量值。\n\n- 对于**测试用例 4**（单变量，边界情况）：\n  - $m=1$。$\\mathbf{v} = [1.959963984540054] - [1.0][0.0] = [1.959963984540054]$。\n  - $\\mathbf{S} = [1.0][0.0][1.0]^\\top + [1.0] = [1.0]$。\n  - $z = [1.959963984540054]^\\top [1.0]^{-1} [1.959963984540054] = (1.959963984540054)^2 \\approx 3.841458820694124$。\n  - 对于 $\\alpha=0.05$ 和 $m=1$ 的自由度，临界值为 $c = \\chi^2_1\\text{.ppf}(0.95) \\approx 3.841458820694124$。\n  - $\\mathbf{y}$ 的值被构造成使得 $z$ 恰好等于 $c$。\n  - 决策：$z > c$ 为假。接受该测量值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the outlier detection problem for a suite of test cases\n    based on the Chi-squared innovation consistency test.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (univariate, typical acceptance)\n        {\n            \"H\": np.array([[1.0]]),\n            \"x_f\": np.array([0.0]),\n            \"P_f\": np.array([[1.0]]),\n            \"R\": np.array([[0.25]]),\n            \"y\": np.array([0.2]),\n            \"alpha\": 0.05\n        },\n        # Test case 2 (univariate, clear outlier)\n        {\n            \"H\": np.array([[1.0]]),\n            \"x_f\": np.array([0.0]),\n            \"P_f\": np.array([[0.5]]),\n            \"R\": np.array([[0.1]]),\n            \"y\": np.array([3.0]),\n            \"alpha\": 0.01\n        },\n        # Test case 3 (bivariate, correlated, acceptance)\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"x_f\": np.array([1.0, -1.0]),\n            \"P_f\": np.array([[1.0, 0.5], [0.5, 2.0]]),\n            \"R\": np.array([[0.2, 0.1], [0.1, 0.3]]),\n            \"y\": np.array([1.3, -0.8]),\n            \"alpha\": 0.05\n        },\n        # Test case 4 (univariate, boundary case; do not reject because z = c)\n        {\n            \"H\": np.array([[1.0]]),\n            \"x_f\": np.array([0.0]),\n            \"P_f\": np.array([[0.0]]),\n            \"R\": np.array([[1.0]]),\n            \"y\": np.array([1.959963984540054]),\n            \"alpha\": 0.05\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Extract matrices and parameters for the current case.\n        H = case[\"H\"]\n        x_f = case[\"x_f\"]\n        P_f = case[\"P_f\"]\n        R = case[\"R\"]\n        y = case[\"y\"]\n        alpha = case[\"alpha\"]\n\n        # Step 1: Determine the degrees of freedom.\n        # m is the dimension of the observation space.\n        m = H.shape[0]\n\n        # Step 2: Compute the innovation vector.\n        # v = y - H * x_f\n        v = y - H @ x_f\n        \n        # Step 3: Compute the innovation covariance matrix.\n        # S = H * P_f * H^T + R\n        S = H @ P_f @ H.T + R\n        \n        # Step 4: Compute the test statistic.\n        # z = v^T * S^-1 * v\n        S_inv = np.linalg.inv(S)\n        # Reshape v to be a column vector for correct matrix multiplication if it's 1D\n        if v.ndim == 1:\n            v_col = v[:, np.newaxis]\n            z = (v_col.T @ S_inv @ v_col)[0, 0]\n        else:\n            z = (v.T @ S_inv @ v)[0, 0]\n\n        # Step 5: Determine the critical value.\n        # c is the upper (1-alpha) quantile of the Chi-squared distribution.\n        c = chi2.ppf(1 - alpha, df=m)\n        \n        # Step 6: Apply the decision rule.\n        # Reject if z > c.\n        is_rejected = z > c\n        \n        # The result must be a standard Python boolean\n        results.append(bool(is_rejected))\n\n    # Format output as a string representation of a list of booleans,\n    # with 'True' and 'False' (capitalized), as per Python's str(bool).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382619"}, {"introduction": "集合卡尔曼滤波器（EnKF）的一个关键挑战是，有限大小的集合容易低估真实的预报误差协方差，这可能导致滤波器发散。乘法膨胀是一种广泛用于抵消此效应的技术，但我们该如何选择合适的膨胀因子呢？本练习提供了一种动手实践的方法，通过系统地匹配滤波器的预测统计量与从实际数据中观测到的统计量，来校准这一关键参数。[@problem_id:3123947]", "problem": "给定一个典型的用于集成卡尔曼滤波器 (EnKF) 的线性高斯观测设置。观测模型为 $y = H x + \\varepsilon$，其中 $x$ 是状态，$H$ 是一个已知的线性观测算子，$\\varepsilon$ 是协方差为 $R$ 的零均值高斯噪声。集成预报提供了一个预报均值 $\\bar{x}^f$ 和预报协方差 $P^f$。新息为 $d = y - H \\bar{x}^f$。该集成使用乘性膨胀，它通过一个标量因子 $\\lambda \\geq 0$ 来缩放预報协方差，因此膨胀后的预报协方差为 $\\lambda P^f$。在线性高斯设置中，核心诊断条件是新息协方差与预报和观测误差协方差之间的关系，即乘性膨胀下的理论新息协方差为 $S(\\lambda) = H (\\lambda P^f) H^\\top + R$。\n\n给定一个从新息样本 $d$ 中计算出的新息协方差的经验估计，记为 $\\widehat{S}$。您的任务是通过求解以下约束最小二乘问题来校准乘性膨胀因子 $\\lambda$：\n- 在区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$ 上最小化弗罗贝尼乌斯范数的平方 $J(\\lambda) = \\lVert \\widehat{S} - (H (\\lambda P^f) H^\\top + R) \\rVert_F^2$。\n- 如果 $H P^f H^\\top$ 是零矩阵（即新息对 $\\lambda$ 没有敏感度），则返回默认值 $\\lambda_{\\text{default}}$。\n\n所有矩阵都是实值矩阵，且尺寸兼容。对于下方的每个测试用例，计算一个求解上述问题的标量 $\\lambda^\\star$，然后将其裁剪到区间 $[\\lambda_{\\min}, \\lambda_{\\max}]$。最后，将每个 $\\lambda^\\star$ 四舍五入到三位小数。\n\n测试套件：\n- 案例 1 (标量，良态)：\n  - $H = [1.0]$\n  - $P^f = [0.5]$\n  - $R = [0.2]$\n  - $\\widehat{S} = [0.9]$\n  - $\\lambda_{\\min} = 0.5$, $\\lambda_{\\max} = 2.0$, $\\lambda_{\\text{default}} = 1.0$\n- 案例 2 (二维，精确一致)：\n  - $H = \\begin{bmatrix}1.0  0.0 \\\\ 0.0  1.0\\end{bmatrix}$\n  - $P^f = \\begin{bmatrix}0.3  0.1 \\\\ 0.1  0.2\\end{bmatrix}$\n  - $R = \\begin{bmatrix}0.05  0.0 \\\\ 0.0  0.04\\end{bmatrix}$\n  - $\\widehat{S} = \\begin{bmatrix}0.41  0.12 \\\\ 0.12  0.28\\end{bmatrix}$\n  - $\\lambda_{\\min} = 0.5$, $\\lambda_{\\max} = 2.0$, $\\lambda_{\\text{default}} = 1.0$\n- 案例 3 (标量，下界激活)：\n  - $H = [1.0]$\n  - $P^f = [0.3]$\n  - $R = [0.5]$\n  - $\\widehat{S} = [0.45]$\n  - $\\lambda_{\\min} = 0.8$, $\\lambda_{\\max} = 2.0$, $\\lambda_{\\text{default}} = 1.0$\n- 案例 4 (标量，零敏感度，回退到默认值)：\n  - $H = [0.0]$\n  - $P^f = [0.4]$\n  - $R = [0.1]$\n  - $\\widehat{S} = [0.3]$\n  - $\\lambda_{\\min} = 0.5$, $\\lambda_{\\max} = 2.0$, $\\lambda_{\\text{default}} = 1.33$\n- 案例 5 (标量，上界激活)：\n  - $H = [1.0]$\n  - $P^f = [0.2]$\n  - $R = [0.1]$\n  - $\\widehat{S} = [1.5]$\n  - $\\lambda_{\\min} = 0.5$, $\\lambda_{\\max} = 1.8$, $\\lambda_{\\text{default}} = 1.0$\n\n输出规格：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应测试用例校准后的 $\\lambda^\\star$，并四舍五入到三位小数。例如：$[1.234,0.999,1.500]$。\n\n注：\n- 所有角度均不适用。\n- 不涉及物理单位。", "solution": "The user wants to solve a constrained optimization problem to calibrate a multiplicative inflation factor $\\lambda$ used in an Ensemble Kalman Filter. The objective is to minimize a cost function $J(\\lambda)$, defined as the squared Frobenius norm of the difference between an empirical and a theoretical innovation covariance, subject to interval constraints on $\\lambda$.\n\nThe problem is stated as:\nMinimize $J(\\lambda) = \\lVert \\widehat{S} - S(\\lambda) \\rVert_F^2$ for $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]$.\nThe theoretical innovation covariance is given by $S(\\lambda) = H (\\lambda P^f) H^\\top + R$.\nSubstituting $S(\\lambda)$ into the cost function, we get:\n$$\nJ(\\lambda) = \\lVert \\widehat{S} - (H (\\lambda P^f) H^\\top + R) \\rVert_F^2\n$$\nThe factor $\\lambda$ is a scalar, so it can be moved outside the matrix multiplications:\n$$\nJ(\\lambda) = \\lVert \\widehat{S} - (\\lambda H P^f H^\\top + R) \\rVert_F^2\n$$\nTo simplify the derivation, let us define two auxiliary matrices:\nLet $A = H P^f H^\\top$.\nLet $B = \\widehat{S} - R$.\n\nWith these definitions, the cost function becomes:\n$$\nJ(\\lambda) = \\lVert B - \\lambda A \\rVert_F^2\n$$\nThe squared Frobenius norm of a real matrix $M$ is defined as the sum of the squares of its elements, which can also be expressed as the trace of $M^\\top M$, i.e., $\\lVert M \\rVert_F^2 = \\text{tr}(M^\\top M)$. Applying this to our cost function:\n$$\nJ(\\lambda) = \\text{tr}\\left((B - \\lambda A)^\\top (B - \\lambda A)\\right)\n$$\nExpanding the term inside the trace:\n$$\nJ(\\lambda) = \\text{tr}(B^\\top B - \\lambda B^\\top A - \\lambda A^\\top B + \\lambda^2 A^\\top A)\n$$\nUsing the linearity of the trace operator:\n$$\nJ(\\lambda) = \\text{tr}(B^\\top B) - \\lambda \\text{tr}(B^\\top A) - \\lambda \\text{tr}(A^\\top B) + \\lambda^2 \\text{tr}(A^\\top A)\n$$\nThe matrices $P^f$, $R$, and $\\widehat{S}$ are covariance matrices and are therefore symmetric. This implies that the auxiliary matrices $A$ and $B$ are also symmetric:\n$A^\\top = (H P^f H^\\top)^\\top = (H^\\top)^\\top (P^f)^\\top H^\\top = H P^f H^\\top = A$.\n$B^\\top = (\\widehat{S} - R)^\\top = \\widehat{S}^\\top - R^\\top = \\widehat{S} - R = B$.\n\nUsing the symmetry of $A$ and $B$, and the cyclic property of the trace ($\\text{tr}(XY) = \\text{tr}(YX)$), we can simplify the expression:\n$\\text{tr}(A^\\top B) = \\text{tr}(AB)$ and $\\text{tr}(B^\\top A) = \\text{tr}(BA)$. Since $\\text{tr}(AB) = \\text{tr}(BA)$, the two linear terms are identical.\nThe cost function is a quadratic function of $\\lambda$:\n$$\nJ(\\lambda) = \\lambda^2 \\text{tr}(A^2) - 2\\lambda \\text{tr}(AB) + \\text{tr}(B^2)\n$$\nTo find the value of $\\lambda$ that minimizes this function, we take the derivative with respect to $\\lambda$ and set it to zero:\n$$\n\\frac{dJ}{d\\lambda} = 2\\lambda \\text{tr}(A^2) - 2 \\text{tr}(AB) = 0\n$$\nSolving for $\\lambda$ gives the unconstrained optimal value, which we denote $\\lambda_{uc}$:\n$$\n2\\lambda_{uc} \\text{tr}(A^2) = 2 \\text{tr}(AB) \\implies \\lambda_{uc} = \\frac{\\text{tr}(AB)}{\\text{tr}(A^2)}\n$$\nThis solution is valid if the denominator, $\\text{tr}(A^2)$, is non-zero. The term $\\text{tr}(A^2)$ is equivalent to $\\text{tr}(A^\\top A) = \\lVert A \\rVert_F^2$. This term is zero if and only if $A$ is the zero matrix. The problem statement provides a specific instruction for this case: if $A = H P^f H^\\top$ is the zero matrix, the value $\\lambda_{\\text{default}}$ should be used. This aligns perfectly with our derivation, as a zero denominator would make $\\lambda_{uc}$ undefined.\n\nThe final step is to incorporate the constraints $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]$. The solution to the constrained problem, $\\lambda^\\star$, is obtained by clipping the candidate value (either $\\lambda_{uc}$ or $\\lambda_{\\text{default}}$) to the specified interval.\n\nThe complete algorithm is as follows:\n$1$. Given the matrices $H$, $P^f$, $R$, $\\widehat{S}$ and scalars $\\lambda_{\\min}$, $\\lambda_{\\max}$, $\\lambda_{\\text{default}}$.\n$2$. Calculate the matrix $A = H P^f H^\\top$.\n$3$. Calculate the denominator term $d = \\text{tr}(A^2)$.\n$4$. Check if $d$ is numerically zero.\n   - If $d \\approx 0$, the candidate solution is $\\lambda_{\\text{cand}} = \\lambda_{\\text{default}}$.\n   - Otherwise, calculate the matrix $B = \\widehat{S} - R$, the numerator term $n = \\text{tr}(AB)$, and the candidate solution $\\lambda_{\\text{cand}} = n/d$.\n$5$. The final optimal value $\\lambda^\\star$ is found by clipping $\\lambda_{\\text{cand}}$ to the allowed interval:\n   $$\n   \\lambda^\\star = \\max(\\lambda_{\\min}, \\min(\\lambda_{\\text{cand}}, \\lambda_{\\max}))\n   $$\n$6$. The value of $\\lambda^\\star$ is then rounded to three decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained least-squares problem for calibrating the\n    multiplicative inflation factor lambda for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"H\": np.array([[1.0]]),\n            \"Pf\": np.array([[0.5]]),\n            \"R\": np.array([[0.2]]),\n            \"S_hat\": np.array([[0.9]]),\n            \"lambda_min\": 0.5,\n            \"lambda_max\": 2.0,\n            \"lambda_default\": 1.0,\n        },\n        # Case 2\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Pf\": np.array([[0.3, 0.1], [0.1, 0.2]]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.04]]),\n            \"S_hat\": np.array([[0.41, 0.12], [0.12, 0.28]]),\n            \"lambda_min\": 0.5,\n            \"lambda_max\": 2.0,\n            \"lambda_default\": 1.0,\n        },\n        # Case 3\n        {\n            \"H\": np.array([[1.0]]),\n            \"Pf\": np.array([[0.3]]),\n            \"R\": np.array([[0.5]]),\n            \"S_hat\": np.array([[0.45]]),\n            \"lambda_min\": 0.8,\n            \"lambda_max\": 2.0,\n            \"lambda_default\": 1.0,\n        },\n        # Case 4\n        {\n            \"H\": np.array([[0.0]]),\n            \"Pf\": np.array([[0.4]]),\n            \"R\": np.array([[0.1]]),\n            \"S_hat\": np.array([[0.3]]),\n            \"lambda_min\": 0.5,\n            \"lambda_max\": 2.0,\n            \"lambda_default\": 1.33,\n        },\n        # Case 5\n        {\n            \"H\": np.array([[1.0]]),\n            \"Pf\": np.array([[0.2]]),\n            \"R\": np.array([[0.1]]),\n            \"S_hat\": np.array([[1.5]]),\n            \"lambda_min\": 0.5,\n            \"lambda_max\": 1.8,\n            \"lambda_default\": 1.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        H = case[\"H\"]\n        Pf = case[\"Pf\"]\n        R = case[\"R\"]\n        S_hat = case[\"S_hat\"]\n        lambda_min = case[\"lambda_min\"]\n        lambda_max = case[\"lambda_max\"]\n        lambda_default = case[\"lambda_default\"]\n\n        # Calculate the matrix A = H * P^f * H^T\n        A = H @ Pf @ H.T\n\n        # Calculate the denominator term: trace(A^2)\n        # This is equivalent to the squared Frobenius norm of A.\n        denom = np.trace(A @ A)\n\n        # Check if the denominator is close to zero, which happens iff A is the zero matrix.\n        if np.isclose(denom, 0.0):\n            # If sensitivity to lambda is zero, use the default value.\n            lambda_cand = lambda_default\n        else:\n            # Calculate the matrix B = S_hat - R\n            B = S_hat - R\n            # Calculate the numerator term: trace(A * B)\n            num = np.trace(A @ B)\n            # Calculate the unconstrained optimal lambda\n            lambda_cand = num / denom\n\n        # Clip the result to the interval [lambda_min, lambda_max]\n        lambda_star = np.clip(lambda_cand, lambda_min, lambda_max)\n        \n        # Round to three decimal places.\n        # The output format requires explicit formatting for trailing zeros.\n        results.append(f\"{lambda_star:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3123947"}]}