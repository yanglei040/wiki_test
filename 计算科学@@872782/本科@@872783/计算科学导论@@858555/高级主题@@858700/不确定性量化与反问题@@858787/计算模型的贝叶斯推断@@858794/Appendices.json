{"hands_on_practices": [{"introduction": "贝叶斯推断的一个核心优势是其能够明确地陈述和包含先验假设。然而，一个关键的问题是：我们的推断结果在多大程度上依赖于这些先验假设？本练习 [@problem_id:3101531] 通过计算后验期望对先验超参数的导数，为您提供了一个量化这种敏感性的实践机会。通过这个结合了公式推导和编程实现的练习，您将掌握评估贝叶斯模型稳健性的基本技能。", "problem": "考虑一个标量贝叶斯参数估计问题，其中有一个单一潜参数 $\\,\\theta\\,$ 和一组独立同分布 (i.i.d.) 的观测值 $\\,y_1,\\dots,y_n\\,$。数据生成模型为 $$y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2) \\text{ for } i=1,\\dots,n,$$ 其中 $\\,\\sigma^2\\,$ 是已知且严格为正的。$\\,\\theta\\,$ 的先验分布为 $$\\theta \\mid \\eta \\sim \\mathcal{N}(\\mu_0, \\tau^2(\\eta)),$$ 其中超参数 $\\,\\eta \\in \\mathbb{R}\\,$ 通过 $$\\tau^2(\\eta) = \\exp(\\eta)$$ 控制先验方差。令 $\\,\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i\\,$ 表示样本均值。要求您通过计算导数 $$\\frac{\\partial}{\\partial \\eta}\\,\\mathbb{E}[\\theta \\mid y]$$ 来分析后验期望 $\\,\\mathbb{E}[\\theta \\mid y]\\,$ 关于超参数 $\\,\\eta\\,$ 的敏感性。从上面给出的似然和先验的基本定义出发。使用贝叶斯法则和正态分布的标准性质，推导出 $\\,\\mathbb{E}[\\theta \\mid y]\\,$ 作为 $\\,\\eta\\,$ 函数的显式表达式，然后使用链式法则对其关于 $\\,\\eta\\,$ 求导。不要不经推导就假设任何关于共轭性的结论；通过对 $\\,\\theta\\,$ 和 $\\,y\\,$ 的联合密度的指数部分进行配方，来确定后验分布及其期望。一旦您获得了用 $\\,n\\,$, $\\,\\sigma^2\\,$, $\\,\\mu_0\\,$, $\\,\\bar{y}\\,$ 和 $\\,\\eta\\,$ 表示的导数的闭式表达式，请实现一个程序，为以下参数设置的测试套件计算该导数。所有计算都是纯数值的，不涉及任何物理单位。您的程序必须输出四舍五入到六位小数的浮点值。\n\n测试套件（每种情况指定 $\\,y\\,$ 为一个列表，以及 $\\,\\sigma^2\\,$, $\\,\\mu_0\\,$ 和 $\\,\\eta\\,$）：\n1. $\\,y = [\\,1.2,\\,0.7,\\,1.0,\\,0.9,\\,1.1\\,],\\ \\sigma^2 = 0.25,\\ \\mu_0 = 0.0,\\ \\eta = 0.0.$\n2. $\\,y = [\\,1.2,\\,0.7,\\,1.0,\\,0.9,\\,1.1\\,],\\ \\sigma^2 = 0.25,\\ \\mu_0 = 1.5,\\ \\eta = 0.0.$\n3. $\\,y = [\\,1.2,\\,0.7,\\,1.0,\\,0.9,\\,1.1\\,],\\ \\sigma^2 = 0.25,\\ \\mu_0 = 0.0,\\ \\eta = 5.0.$\n4. $\\,y = [\\,1.2,\\,0.7,\\,1.0,\\,0.9,\\,1.1\\,],\\ \\sigma^2 = 0.25,\\ \\mu_0 = 0.0,\\ \\eta = -5.0.$\n5. $\\,y = [\\,2.0,\\,2.0,\\,2.0\\,],\\ \\sigma^2 = 1.0,\\ \\mu_0 = 2.0,\\ \\eta = 1.0.$\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表，顺序与测试套件中的顺序相同。例如，输出格式必须与 $\\,[$result1,result2,result3,result4,result5$]\\,$ 完全一样，其中每个结果都是一个四舍五入到六位小数的浮点数（例如，$\\,0.123456\\,$ 或 $\\, -0.000314\\,$）。", "solution": "问题陈述经评估有效。它在科学上基于贝叶斯统计理论，是良定的、客观的，并包含得出唯一解所需的所有信息。因此，我们可以继续进行推导和实现。\n\n目标是计算导数 $\\frac{\\partial}{\\partial \\eta}\\,\\mathbb{E}[\\theta \\mid y]$，该导数量化了参数 $\\theta$ 的后验期望相对于超参数 $\\eta$ 的敏感性。推导过程分四步进行：1. 建立似然函数和先验分布。2. 使用贝叶斯法则求 $\\theta$ 的后验分布。3. 确定后验期望 $\\mathbb{E}[\\theta \\mid y]$。4. 对该期望关于 $\\eta$ 求导。\n\n**1. 似然和先验**\n\n数据生成模型指定观测值 $y_1, \\dots, y_n$ 在以 $\\theta$ 为条件下是独立同分布 (i.i.d.) 的，遵循正态分布 $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)$。对于完整数据集 $y = (y_1, \\dots, y_n)$，似然函数 $p(y \\mid \\theta)$ 是各个概率密度的乘积：\n$$p(y \\mid \\theta) = \\prod_{i=1}^n p(y_i \\mid \\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma^2}\\right)$$\n为了确定 $\\theta$ 的后验分布，我们可以忽略常数因子，只关注依赖于 $\\theta$ 的项。因此，似然函数正比于：\n$$p(y \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\theta)^2\\right)$$\n指数中的求和项可以用样本均值 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ 来重新表示：\n$$\\sum_{i=1}^n (y_i - \\theta)^2 = \\sum_{i=1}^n ((y_i - \\bar{y}) - (\\theta - \\bar{y}))^2 = \\sum_{i=1}^n (y_i - \\bar{y})^2 + n(\\theta - \\bar{y})^2$$\n其中交叉项因 $\\sum_{i=1}^n (y_i - \\bar{y}) = 0$ 而消失。由于 $\\sum_{i=1}^n(y_i - \\bar{y})^2$ 不依赖于 $\\theta$，似然函数简化为：\n$$p(y \\mid \\theta) \\propto \\exp\\left(-\\frac{n(\\theta - \\bar{y})^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{(\\theta - \\bar{y})^2}{2(\\sigma^2/n)}\\right)$$\n\n$\\theta$ 的先验分布给定为 $\\theta \\mid \\eta \\sim \\mathcal{N}(\\mu_0, \\tau^2(\\eta))$，其中 $\\tau^2(\\eta) = \\exp(\\eta)$。先验密度为：\n$$p(\\theta \\mid \\eta) \\propto \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau^2(\\eta)}\\right)$$\n\n**2. 后验分布**\n\n根据贝叶斯法则，后验密度 $p(\\theta \\mid y, \\eta)$ 正比于似然和先验的乘积：\n$$p(\\theta \\mid y, \\eta) \\propto p(y \\mid \\theta) \\cdot p(\\theta \\mid \\eta)$$\n$$p(\\theta \\mid y, \\eta) \\propto \\exp\\left(-\\frac{(\\theta - \\bar{y})^2}{2(\\sigma^2/n)}\\right) \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau^2(\\eta)}\\right)$$\n合并指数，我们得到：\n$$p(\\theta \\mid y, \\eta) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(\\theta - \\bar{y})^2}{\\sigma^2/n} + \\frac{(\\theta - \\mu_0)^2}{\\tau^2(\\eta)} \\right] \\right)$$\n为了确定后验分布的形式，我们对指数中的 $\\theta$ 进行配方。令括号中的表达式为 $Q(\\theta)$：\n$$Q(\\theta) = \\frac{n}{\\sigma^2}(\\theta^2 - 2\\bar{y}\\theta + \\bar{y}^2) + \\frac{1}{\\tau^2(\\eta)}(\\theta^2 - 2\\mu_0\\theta + \\mu_0^2)$$\n合并 $\\theta^2$ 和 $\\theta$ 的项：\n$$Q(\\theta) = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2(\\eta)}\\right)\\theta^2 - 2\\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2(\\eta)}\\right)\\theta + C$$\n其中 $C$ 包含不依赖于 $\\theta$ 的项。$\\theta$ 的这种二次型意味着后验分布也是正态分布，记为 $\\mathcal{N}(\\mu_n, \\sigma_n^2)$。这种分布的密度正比于 $\\exp\\left(-\\frac{(\\theta - \\mu_n)^2}{2\\sigma_n^2}\\right)$，其展开为 $\\exp\\left(-\\frac{1}{2\\sigma_n^2}(\\theta^2 - 2\\mu_n\\theta + \\mu_n^2)\\right)$。\n通过将 $\\theta^2$ 和 $\\theta$ 的系数与我们得到的 $Q(\\theta)$ 表达式进行比较，我们求得后验方差 $\\sigma_n^2$ 和后验均值 $\\mu_n$：\n$$\\frac{1}{\\sigma_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2(\\eta)}$$\n$$\\frac{\\mu_n}{\\sigma_n^2} = \\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2(\\eta)}$$\n\n**3. 后验期望**\n\n后验分布的期望 $\\mathbb{E}[\\theta \\mid y]$ 就是其均值 $\\mu_n$。解出 $\\mu_n$ 可得：\n$$\\mu_n = \\sigma_n^2 \\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2(\\eta)}\\right) = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2(\\eta)}}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2(\\eta)}}$$\n代入 $\\tau^2(\\eta) = \\exp(\\eta)$，我们得到作为 $\\eta$ 函数的后验期望：\n$$\\mathbb{E}[\\theta \\mid y] = \\mu_n(\\eta) = \\frac{\\frac{n\\bar{y}}{\\sigma^2} + \\mu_0 \\exp(-\\eta)}{\\frac{n}{\\sigma^2} + \\exp(-\\eta)}$$\n\n**4. 求导**\n\n最后，我们使用商法则 $(\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$ 对 $\\mu_n(\\eta)$ 关于 $\\eta$ 求导。令 $f(\\eta) = \\frac{n\\bar{y}}{\\sigma^2} + \\mu_0 e^{-\\eta}$ 和 $g(\\eta) = \\frac{n}{\\sigma^2} + e^{-\\eta}$。它们的导数是：\n$$f'(\\eta) = -\\mu_0 e^{-\\eta}$$\n$$g'(\\eta) = -e^{-\\eta}$$\n应用商法则：\n$$\\frac{\\partial \\mu_n}{\\partial \\eta} = \\frac{(-\\mu_0 e^{-\\eta})\\left(\\frac{n}{\\sigma^2} + e^{-\\eta}\\right) - \\left(\\frac{n\\bar{y}}{\\sigma^2} + \\mu_0 e^{-\\eta}\\right)(-e^{-\\eta})}{\\left(\\frac{n}{\\sigma^2} + e^{-\\eta}\\right)^2}$$\n化简分子：\n$$\\text{Numerator} = -\\frac{n\\mu_0}{\\sigma^2}e^{-\\eta} - \\mu_0 e^{-2\\eta} + \\frac{n\\bar{y}}{\\sigma^2}e^{-\\eta} + \\mu_0 e^{-2\\eta}$$\n$$\\text{Numerator} = \\frac{n}{\\sigma^2}e^{-\\eta}(\\bar{y} - \\mu_0)$$\n因此，导数的最终表达式为：\n$$\\frac{\\partial}{\\partial \\eta}\\,\\mathbb{E}[\\theta \\mid y] = \\frac{\\frac{n}{\\sigma^2} \\exp(-\\eta) (\\bar{y} - \\mu_0)}{\\left(\\frac{n}{\\sigma^2} + \\exp(-\\eta)\\right)^2}$$\n此公式将被用于实现，以评估给定测试用例的敏感性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the derivative of the posterior expectation with respect to a hyperparameter\n    for a series of test cases.\n    \"\"\"\n    # Test suite (each case specifies y as a list, sigma^2, mu_0, and eta)\n    test_cases = [\n        # 1. \n        {'y': [1.2, 0.7, 1.0, 0.9, 1.1], 'sigma2': 0.25, 'mu0': 0.0, 'eta': 0.0},\n        # 2. \n        {'y': [1.2, 0.7, 1.0, 0.9, 1.1], 'sigma2': 0.25, 'mu0': 1.5, 'eta': 0.0},\n        # 3. \n        {'y': [1.2, 0.7, 1.0, 0.9, 1.1], 'sigma2': 0.25, 'mu0': 0.0, 'eta': 5.0},\n        # 4. \n        {'y': [1.2, 0.7, 1.0, 0.9, 1.1], 'sigma2': 0.25, 'mu0': 0.0, 'eta': -5.0},\n        # 5. \n        {'y': [2.0, 2.0, 2.0], 'sigma2': 1.0, 'mu0': 2.0, 'eta': 1.0},\n    ]\n\n    results = []\n    \n    # The derived formula for the derivative\n    # D = ( (n/sigma^2) * exp(-eta) * (y_bar - mu0) ) / ( (n/sigma^2) + exp(-eta) )^2\n    def calculate_derivative(y_list, sigma2, mu0, eta):\n        \"\"\"\n        Calculates the derivative based on the derived formula.\n        \n        Args:\n            y_list (list): List of observations.\n            sigma2 (float): Known variance of the observations.\n            mu0 (float): Mean of the prior distribution.\n            eta (float): Hyperparameter for the prior variance.\n            \n        Returns:\n            float: The calculated derivative.\n        \"\"\"\n        n = len(y_list)\n        y_bar = np.mean(y_list)\n        \n        # To avoid re-calculation\n        exp_neg_eta = np.exp(-eta)\n        n_over_sigma2 = n / sigma2\n        \n        numerator = n_over_sigma2 * exp_neg_eta * (y_bar - mu0)\n        denominator = (n_over_sigma2 + exp_neg_eta)**2\n        \n        # Handle case where denominator is zero, although unlikely with sigma2 > 0 and n > 0.\n        if denominator == 0:\n            return 0.0\n            \n        return numerator / denominator\n\n    for case in test_cases:\n        derivative = calculate_derivative(case['y'], case['sigma2'], case['mu0'], case['eta'])\n        # Format the result to six decimal places as a string\n        results.append(f\"{derivative:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3101531"}, {"introduction": "在实际应用中，我们常常使用计算模型（如偏微分方程的数值解）来模拟物理过程，而这些模型本身是真实世界的近似。本练习 [@problem_id:3101556] 将带您进入一个更真实的场景：利用贝叶斯方法校准一个一维热传导方程模型。您将亲手实践如何评估观测噪声和模型离散化误差对参数推断的影响，这对于理解和处理计算模型与真实数据之间的差距至关重要。", "problem": "本题要求您为一维热传导模型实现贝叶斯校准，以使用高斯观测模型推断热扩散系数参数 $k$，并研究后验分布如何随观测噪声和计算模型的不同时间离散化而变化。所有量均为无量纲，因此不使用物理单位。\n\n正向模型的规范：\n- 一维热方程为\n$$\n\\frac{\\partial u}{\\partial t}(x,t) = k \\frac{\\partial^2 u}{\\partial x^2}(x,t), \\quad x \\in (0,1), \\ t \\ge 0,\n$$\n其齐次狄利克雷边界条件为 $u(0,t)=0$ 和 $u(1,t)=0$，初始条件为 $u(x,0)=\\sin(\\pi x)$。\n- 对于此初边值问题，分离变量法意味着解保持在第一本征模中。通过 $u(x,t)=a(t)\\sin(\\pi x)$ 定义振幅 $a(t)$。该振幅满足以下线性常微分方程\n$$\n\\frac{da}{dt}(t) = -\\lambda a(t), \\quad \\text{with} \\ \\lambda = k\\pi^2, \\quad a(0)=1.\n$$\n- 其精确解为 $a_{\\text{exact}}(t) = \\exp(-\\lambda t)$，因此 $u(\\tfrac{1}{2},t)=a_{\\text{exact}}(t)$，因为 $\\sin(\\pi/2)=1$。\n\n观测模型：\n- 我们在单个空间位置 $x=\\tfrac{1}{2}$ 和三个时间点 $t \\in \\{0.02, 0.05, 0.1\\}$ 观测温度。将这些观测值堆叠成一个长度为 $3$ 的向量 $y=[u(\\tfrac{1}{2},0.02), u(\\tfrac{1}{2},0.05), u(\\tfrac{1}{2},0.1)]^\\top$。\n- 观测模型为高斯模型：$y \\sim \\mathcal{N}(u(k), \\sigma^2 I)$，其中 $u(k)$ 是给定 $k$ 的模型预测向量，$\\sigma>0$ 是观测噪声标准差，$I$ 是大小为 $3$ 的单位矩阵。\n- 合成数据由真实参数 $k_{\\text{true}}=0.12$ 下的精确模型生成，不添加噪声，即 $y = [\\exp(-\\pi^2 k_{\\text{true}} \\cdot 0.02), \\exp(-\\pi^2 k_{\\text{true}} \\cdot 0.05), \\exp(-\\pi^2 k_{\\text{true}} \\cdot 0.1)]^\\top$。\n\n计算模型离散化：\n- 我们不使用精确振幅，而是通过将时间步进格式应用于线性常微分方程 $da/dt = -\\lambda a$（其中 $\\lambda = k\\pi^2$）来近似 $a(t)$，使用基本步长 $\\Delta t = T/N_{\\text{base}}$（其中 $T=0.1$，$N_{\\text{base}} \\in \\mathbb{N}$）从 $t=0$ 积分到感兴趣的时间点。对于每个观测时间 $t_j \\in \\{0.02,0.05,0.1\\}$，使用 $N_j = t_j/\\Delta t$ 步（对于此处指定的值，这些都是整数）。\n- 考虑两种时间离散化：\n    - 离散化 $\\mathcal{D}_{\\text{EE},10}$：显式欧拉法，其中 $N_{\\text{base}}=10$，更新规则为 $a_{n+1} = a_n - \\Delta t \\lambda a_n$。\n    - 离散化 $\\mathcal{D}_{\\text{CN},100}$：Crank–Nicolson 法，其中 $N_{\\text{base}}=100$，更新规则为 $a_{n+1} = a_n \\frac{1 - \\tfrac{1}{2}\\Delta t \\lambda}{1 + \\tfrac{1}{2}\\Delta t \\lambda}$。\n- 对于每种离散化和每个 $k$，通过从 $a(0)=1$ 步进到每个 $t_j \\in \\{0.02,0.05,0.1\\}$ 的 $a(t_j)$，并计算 $u(\\tfrac{1}{2},t_j)=a(t_j)$ 来获得预测观测向量 $u(k)$。\n\n贝叶斯校准：\n- 对 $k$ 使用区间 $[k_{\\min}, k_{\\max}] = [0.05, 0.25]$ 上的均匀先验，区间外为零。\n- 给定数据 $y$ 和选定的 $\\sigma>0$，后验密度（不含归一化常数）为：当 $k \\in [0.05, 0.25]$ 时，$p(k\\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - u(k)\\|_2^2\\right)$，否则为 $0$。\n- 通过在 $[0.05,0.25]$ 上包含 $N_k = 2001$ 个等距点的均匀网格上进行数值积分来近似后验期望。具体来说，使用从未归一化后验导出的归一化求积权重计算后验均值 $\\mathbb{E}[k\\mid y]$ 和后验标准差 $\\sqrt{\\mathbb{V}[k\\mid y]}$。\n\n测试套件：\n- 使用以下六个测试用例来研究后验对观测噪声水平 $\\sigma$ 和计算离散化的敏感性：\n    1. 用例 1：离散化 $\\mathcal{D}_{\\text{EE},10}$，$\\sigma=0.005$。\n    2. 用例 2：离散化 $\\mathcal{D}_{\\text{EE},10}$，$\\sigma=0.02$。\n    3. 用例 3：离散化 $\\mathcal{D}_{\\text{EE},10}$，$\\sigma=0.1$。\n    4. 用例 4：离散化 $\\mathcal{D}_{\\text{CN},100}$，$\\sigma=0.005$。\n    5. 用例 5：离散化 $\\mathcal{D}_{\\text{CN},100}$，$\\sigma=0.02$。\n    6. 用例 6：离散化 $\\mathcal{D}_{\\text{CN},100}$，$\\sigma=0.1$。\n\n要求输出：\n- 对于每个测试用例，使用指定的先验、数据、离散化和 $\\sigma$ 计算 $k$ 的后验均值和后验标准差。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表应按 $[\\mu_1, s_1, \\mu_2, s_2, \\mu_3, s_3, \\mu_4, s_4, \\mu_5, s_5, \\mu_6, s_6]$ 的顺序排列，其中 $\\mu_i$ 和 $s_i$ 分别是用例 $i$ 的后验均值和后验标准差，每个值都四舍五入到 $6$ 位小数。", "solution": "我们从单位区间上的一维热方程 $\\frac{\\partial u}{\\partial t} = k \\frac{\\partial^2 u}{\\partial x^2}$ 开始，其齐次狄利克雷边界条件为 $u(x,0)=\\sin(\\pi x)$。通过分离变量法和狄利克雷条件下对拉普拉斯算子的特征函数展开，解可以表示为空间正弦模态的求和。鉴于特定的初始条件，只有第一本征模存在，因此\n$$\nu(x,t) = a(t)\\sin(\\pi x),\n$$\n其振幅 $a(t)$ 满足\n$$\n\\frac{da}{dt}(t) = -\\lambda a(t), \\quad \\lambda = k\\pi^2, \\quad a(0)=1.\n$$\n这是一个线性常微分方程，其精确解为 $a_{\\text{exact}}(t) = \\exp(-\\lambda t)$。在 $x=\\tfrac{1}{2}$ 处，$\\sin(\\pi/2)=1$，因此 $u(\\tfrac{1}{2},t)=a(t)$。\n\n观测模型规定，在时间 $t \\in \\{0.02,0.05,0.1\\}$ 的观测向量 $y \\in \\mathbb{R}^3$ 满足高斯似然\n$$\ny \\sim \\mathcal{N}(u(k), \\sigma^2 I),\n$$\n其中 $u(k) \\in \\mathbb{R}^3$ 堆叠了 $t_j \\in \\{0.02,0.05,0.1\\}$ 时的模型输出 $u(\\tfrac{1}{2}, t_j)$。合成数据由 $k_{\\text{true}}=0.12$ 时的精确模型生成，不含噪声：\n$$\ny = \\left[\\exp(-\\pi^2 \\cdot 0.12 \\cdot 0.02), \\ \\exp(-\\pi^2 \\cdot 0.12 \\cdot 0.05), \\ \\exp(-\\pi^2 \\cdot 0.12 \\cdot 0.1)\\right]^\\top.\n$$\n\n对于不同离散化下的计算建模，我们使用单步格式积分振幅常微分方程，基本步长为 $\\Delta t = T/N_{\\text{base}}$ 且 $T=0.1$。对于每个观测时间 $t_j$，步数为 $N_j = t_j/\\Delta t$（对于给定的时间和基本步长，这些都是整数）。所用格式为：\n- 显式欧拉法（记为 $\\mathcal{D}_{\\text{EE},10}$）：$a_{n+1} = a_n - \\Delta t \\lambda a_n$。经过 $N_j$ 步后，$a(t_j)$ 通过从 $a(0)=1$ 开始重复应用因子 $(1-\\Delta t \\lambda)$ 来近似。\n- Crank–Nicolson 法（记为 $\\mathcal{D}_{\\text{CN},100}$）：$a_{n+1} = a_n \\frac{1 - \\tfrac{1}{2}\\Delta t \\lambda}{1 + \\tfrac{1}{2}\\Delta t \\lambda}$。经过 $N_j$ 步后，$a(t_j)$ 通过从 $a(0)=1$ 开始重复应用因子 $\\frac{1 - \\tfrac{1}{2}\\Delta t \\lambda}{1 + \\tfrac{1}{2}\\Delta t \\lambda}$ 来近似。\n\n贝叶斯推断源于贝叶斯定理。在 $[k_{\\min}, k_{\\max}] = [0.05, 0.25]$ 上使用均匀先验，后验密度为\n$$\np(k\\mid y) = \\frac{p(y\\mid k) p(k)}{\\int_{k_{\\min}}^{k_{\\max}} p(y\\mid \\kappa) p(\\kappa)\\, d\\kappa}\n\\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - u(k)\\|_2^2\\right) \\ \\mathbf{1}_{[0.05,0.25]}(k),\n$$\n其中 $\\mathbf{1}_{[0.05,0.25]}$ 是指示函数。为计算后验期望（如均值和方差），我们通过在 $[0.05,0.25]$ 上的一个包含 $N_k=2001$ 个点的均匀网格上进行数值积分来近似积分。设该网格为 $\\{k_i\\}_{i=1}^{N_k}$，并定义未归一化的权重\n$$\nw_i = \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - u(k_i)\\|_2^2\\right).\n$$\n为了在 $\\sigma$ 很小时避免数值下溢，我们在指数化之前通过减去 $\\max_i \\log w_i$ 来执行 log-sum-exp 稳定化；这不会改变归一化的后验权重，因为它将所有权重乘以相同的常数。将归一化权重记为 $\\tilde{w}_i = w_i / \\sum_{j=1}^{N_k} w_j$。然后，后验均值和方差近似为\n$$\n\\mathbb{E}[k\\mid y] \\approx \\sum_{i=1}^{N_k} k_i \\tilde{w}_i, \\quad\n\\mathbb{V}[k\\mid y] \\approx \\sum_{i=1}^{N_k} (k_i - \\mathbb{E}[k\\mid y])^2 \\tilde{w}_i,\n$$\n后验标准差为 $\\sqrt{\\mathbb{V}[k\\mid y]}$。\n\n每个测试用例的算法步骤：\n1. 固定离散化格式和 $N_{\\text{base}}$ 以确定 $\\Delta t = 0.1/N_{\\text{base}}$ 以及对于 $t_j \\in \\{0.02,0.05,0.1\\}$ 的步数 $N_j$。\n2. 在 $[0.05,0.25]$ 上构造网格 $\\{k_i\\}_{i=1}^{2001}$。\n3. 对于每个 $k_i$，计算 $\\lambda_i = \\pi^2 k_i$，并通过使用所选格式将振幅从 $a(0)=1$ 步进到每个 $t_j$ 的 $a(t_j)$ 来计算模型预测值 $u(k_i) \\in \\mathbb{R}^3$。\n4. 形成残差 $r_i = y - u(k_i)$，计算 $e_i = \\|r_i\\|_2^2$，然后计算稳定化的未归一化对数权重 $\\ell_i = -\\frac{1}{2\\sigma^2} e_i$，通过减去 $\\max_i \\ell_i$ 进行移位，取指数得到 $w_i$，并归一化得到 $\\tilde{w}_i$。\n5. 使用归一化权重计算后验均值和标准差。\n\n敏感性预期：\n- 对于较小的 $\\sigma$（例如 $\\sigma=0.005$），似然函数呈尖锐峰值，使用高精度离散化（Crank–Nicolson 法，其中 $N_{\\text{base}}=100$）时，后验均值应非常接近 $k_{\\text{true}}=0.12$，且后验标准差较小。使用更粗糙、偏差更大的离散化（显式欧拉法，其中 $N_{\\text{base}}=10$）时，模型差异将使后验均值偏离 $0.12$，但由于似然函数很尖锐，后验标准差仍然很小。\n- 对于较大的 $\\sigma$（例如 $\\sigma=0.1$），似然函数是弥散的，因此后验分布接近先验分布；后验均值向先验中心 $(0.05+0.25)/2 = 0.15$ 移动，后验标准差增加，趋向于 $[0.05,0.25]$ 上均匀先验的标准差。\n\n程序完全按照规定实现了上述步骤，按给定顺序评估了六个测试用例，并打印出一行包含展平列表 $[\\mu_1, s_1, \\mu_2, s_2, \\mu_3, s_3, \\mu_4, s_4, \\mu_5, s_5, \\mu_6, s_6]$ 的结果，每个值都四舍五入到 $6$ 位小数。", "answer": "```python\nimport numpy as np\n\ndef generate_synthetic_data(k_true, times):\n    lam_true = (np.pi ** 2) * k_true\n    return np.exp(-lam_true * times)\n\ndef forward_amplitudes(k_vals, times, scheme, n_base):\n    \"\"\"\n    Compute model predictions u(1/2, t_j) = a(t_j) for each k in k_vals and each t in times,\n    using time-stepping schemes for the ODE a' = -lambda a, a(0)=1, lambda = pi^2 * k.\n    \n    Parameters:\n        k_vals: array of shape (nk,)\n        times: array of shape (nt,)\n        scheme: 'EE' for explicit Euler, 'CN' for Crank-Nicolson\n        n_base: number of base steps to reach T = max(times)\n    Returns:\n        preds: array of shape (nk, nt)\n    \"\"\"\n    T = np.max(times)\n    dt = T / n_base\n    # Ensure times are integer multiples of dt per problem setup\n    steps_per_time = np.round(times / dt).astype(int)\n    lam = (np.pi ** 2) * k_vals[:, None]  # shape (nk,1) for broadcasting\n    preds = np.empty((k_vals.shape[0], times.shape[0]), dtype=float)\n    if scheme == 'EE':\n        # factor = (1 - dt * lambda)\n        factor = (1.0 - dt * lam)\n        # For each time, raise to the power N_j\n        for j, Nj in enumerate(steps_per_time):\n            preds[:, j] = np.power(factor[:, 0], Nj)\n    elif scheme == 'CN':\n        # factor = (1 - 0.5 dt lambda) / (1 + 0.5 dt lambda)\n        num = (1.0 - 0.5 * dt * lam)\n        den = (1.0 + 0.5 * dt * lam)\n        factor = num / den\n        for j, Nj in enumerate(steps_per_time):\n            preds[:, j] = np.power(factor[:, 0], Nj)\n    else:\n        raise ValueError(\"Unknown scheme. Use 'EE' or 'CN'.\")\n    return preds\n\ndef posterior_stats(y, times, scheme, n_base, sigma, k_min=0.05, k_max=0.25, nk=2001):\n    \"\"\"\n    Compute posterior mean and std of k given data y, model discretization, and sigma.\n    Uniform prior on [k_min, k_max], quadrature over equally spaced grid nk.\n    \"\"\"\n    k_grid = np.linspace(k_min, k_max, nk)\n    preds = forward_amplitudes(k_grid, times, scheme, n_base)  # (nk, nobs)\n    # residuals: (nk, nobs)\n    residuals = preds - y[None, :]\n    err2 = np.sum(residuals ** 2, axis=1)  # (nk,)\n    # Stabilized log-weights\n    logw = -0.5 * err2 / (sigma ** 2)\n    logw -= np.max(logw)\n    w = np.exp(logw)\n    w_sum = np.sum(w)\n    if w_sum == 0.0 or not np.isfinite(w_sum):\n        # Fallback to near-uniform if under/overflow occurs\n        w = np.ones_like(w) / w.size\n        w_sum = 1.0\n    # Normalize\n    w /= w_sum\n    mean = np.sum(k_grid * w)\n    var = np.sum(((k_grid - mean) ** 2) * w)\n    std = np.sqrt(max(var, 0.0))\n    return mean, std\n\ndef solve():\n    # Problem parameters\n    times = np.array([0.02, 0.05, 0.1], dtype=float)\n    k_true = 0.12\n    y = generate_synthetic_data(k_true, times)\n\n    # Test cases: (scheme, n_base, sigma)\n    test_cases = [\n        ('EE', 10, 0.005),\n        ('EE', 10, 0.02),\n        ('EE', 10, 0.1),\n        ('CN', 100, 0.005),\n        ('CN', 100, 0.02),\n        ('CN', 100, 0.1),\n    ]\n\n    results = []\n    for scheme, n_base, sigma in test_cases:\n        mean, std = posterior_stats(y, times, scheme, n_base, sigma)\n        results.append(f\"{mean:.6f}\")\n        results.append(f\"{std:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3101556"}, {"introduction": "前面的练习揭示了模型误差的存在，但如果我们的计算模型不仅是近似的，而且存在系统性的偏差，我们该怎么办？本练习 [@problem_id:3101558] 介绍了一种前沿技术，即使用高斯过程来构建模型差异的统计模型。通过这个综合性实践，您将学习如何同时推断模型参数并量化模型的结构性偏差，从而将参数不确定性、观测误差和模型不足分离开来，实现更精细的推断。", "problem": "考虑一个具有参数化输出和系统性模型偏差的计算模型。您观测到输入 $x_i \\in [0,1]$ 和输出 $y_i \\in \\mathbb{R}$，其中 $i \\in \\{1,\\dots,n\\}$。观测模型为\n$$\ny_i \\;=\\; u(\\theta, x_i) \\;+\\; \\delta(x_i) \\;+\\; \\varepsilon_i,\n$$\n其中 $u(\\theta, x)$ 是一个已知的参数化计算模型，$\\theta \\in \\mathbb{R}$ 是一个待推断的未知参数，$\\delta(\\cdot)$ 是一个捕捉模型偏差的潜在偏差函数，而 $\\varepsilon_i$ 是独立的观测噪声。假设以下基本原理作为出发点：\n- 用于后验推断的 Bayes 法则，\n- 高斯过程 (GP) 先验的定义：$\\delta(\\cdot)$ 服从一个零均值高斯过程 (GP) 先验，其协方差核为 $k(x,x')$，\n- 噪声的高斯分布定律：$\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ 且相互独立。\n\n使用参数化计算模型 $u(\\theta, x) = \\theta x$。$\\delta(\\cdot)$ 上的高斯过程 (GP) 先验使用平方指数核（也称为径向基函数核）\n$$\nk(x,x') \\;=\\; \\alpha^2 \\exp\\!\\Big(-\\frac{(x-x')^2}{2\\ell^2}\\Big),\n$$\n其振幅为 $\\alpha > 0$，长度尺度为 $\\ell > 0$。设 $\\theta$ 的先验为高斯分布 $\\theta \\sim \\mathcal{N}(0,\\tau^2)$，其中 $\\tau > 0$ 已知。将 $\\alpha$、$\\ell$、$\\sigma$ 和 $\\tau$ 视为固定且已知的超参数。\n\n任务：\n1. 仅从 Bayes 法则和联合高斯随机变量的性质出发，通过对 GP 偏差 $\\delta(\\cdot)$ 进行积分，推导给定 $\\theta$ 条件下数据的边缘似然，然后推导给定数据的 $\\theta$ 的后验分布。\n2. 推导给定数据和 $\\theta$ 条件下偏差函数 $\\delta(\\cdot)$ 的后验分布，然后推导当 $\\theta$ 本身不确定时，如何在指定输入 $x^\\star$ 处获得 $\\delta(x^\\star)$ 的后验均值和后验方差。\n3. 设计并实现一个算法，该算法：\n   - 使用对 $\\theta$ 的一维网格进行数值积分的方法，计算 $\\theta$ 的后验均值和方差。\n   - 通过将条件 GP 后验与 $\\theta$ 的不确定性恰当结合，计算 $\\delta(x^\\star)$ 的后验均值和方差。\n   - 对所需的高斯计算使用数值稳定的线性代数方法。\n\n假设使用以下测试用例集。对于每个用例，使用所提供的 $(x,y)$ 数据、超参数 $(\\alpha,\\ell,\\sigma,\\tau)$ 和评估点 $x^\\star$。所有数值都已明确给出：\n- A 用例（一般情况，中等噪声）：\n  - $x = [\\,0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\,]$\n  - $y = [\\,0.05,\\,0.65,\\,1.0,\\,1.33,\\,1.98\\,]$\n  - $\\alpha = 0.4$, $\\ell = 0.3$, $\\sigma = 0.1$, $\\tau = 1.0$, $x^\\star = 0.5$\n- B 用例（接近无噪声边界）：\n  - $x = [\\,0,\\,0.25,\\,0.5,\\,0.75,\\,1.0\\,]$\n  - $y = [\\,0.05,\\,0.65,\\,1.0,\\,1.33,\\,1.98\\,]$\n  - $\\alpha = 0.4$, $\\ell = 0.3$, $\\sigma = 0.000001$, $\\tau = 1.0$, $x^\\star = 0.5$\n- C 用例（数据点少，高噪声，不同超参数）：\n  - $x = [\\,0.1,\\,0.4,\\,0.9\\,]$\n  - $y = [\\,0.4588,\\,0.2088,\\,0.9412\\,]$\n  - $\\alpha = 0.3$, $\\ell = 0.5$, $\\sigma = 0.5$, $\\tau = 2.0$, $x^\\star = 0.5$\n\n您的程序必须：\n- 实现推导过程，为每个用例计算以下四个量：\n  1. $\\theta$ 的后验均值（一个浮点数），\n  2. $\\theta$ 的后验标准差（一个浮点数），\n  3. $\\delta(x^\\star)$ 的后验均值（一个浮点数），\n  4. $\\delta(x^\\star)$ 的后验标准差（一个浮点数）。\n- 将每个浮点数四舍五入到六位小数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是按上述顺序列出的列表。例如：\n$$\n[\\,[\\theta_{\\text{mean}},\\theta_{\\text{std}},\\delta_{\\text{mean}},\\delta_{\\text{std}}],\\,[\\dots],\\,[\\dots]\\,]\n$$\n对所有用例使用 $x^\\star = 0.5$。不应打印任何额外文本；只允许输出单行的方括号列表。", "solution": "该问题要求为一个包含参数项和非参数高斯过程 (GP) 偏差项的计算模型推导并实现一个贝叶斯推断框架。我们被给定了观测模型、所有未知量的先验以及用于计算的特定数据集。\n\n对于一组 $n$ 个数据点 $D = \\{ (x_i, y_i) \\}_{i=1}^n$，观测模型由\n$$y_i = u(\\theta, x_i) + \\delta(x_i) + \\varepsilon_i, \\quad i \\in \\{1, \\dots, n\\}$$\n给出，其中 $u(\\theta, x) = \\theta x$ 是具有单一参数 $\\theta$ 的计算模型。$\\delta(\\cdot)$ 项代表模型偏差，$\\varepsilon_i$ 是观测噪声。\n\n概率性设定如下：\n- 参数 $\\theta$ 的先验是高斯分布：$\\theta \\sim \\mathcal{N}(0, \\tau^2)$。\n- 偏差函数 $\\delta(\\cdot)$ 的先验是零均值高斯过程：$\\delta(\\cdot) \\sim \\mathcal{GP}(0, k(x, x'))$，其平方指数核为 $k(x,x') = \\alpha^2 \\exp(-\\frac{(x-x')^2}{2\\ell^2})$。\n- 观测噪声是独立同分布的高斯噪声：$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$。\n- 超参数 $\\alpha$、$\\ell$、$\\sigma$ 和 $\\tau$ 被视为已知的常数。\n\n让我们以向量形式定义数据：$\\mathbf{y} = [y_1, \\dots, y_n]^T$，$\\mathbf{x} = [x_1, \\dots, x_n]^T$。模型预测值为 $\\mathbf{u}(\\theta) = u(\\theta, \\mathbf{x}) = \\theta\\mathbf{x}$。观测点处的潜在偏差值为 $\\boldsymbol{\\delta} = [\\delta(x_1), \\dots, \\delta(x_n)]^T$，噪声向量为 $\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\dots, \\varepsilon_n]^T$。于是模型为 $\\mathbf{y} = \\theta\\mathbf{x} + \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}$。\n\n根据 GP 先验，向量 $\\boldsymbol{\\delta}$ 服从一个多元正态分布：$\\boldsymbol{\\delta} \\sim \\mathcal{N}(\\mathbf{0}, K)$，其中 $K$ 是一个 $n \\times n$ 的协方差矩阵，其元素为 $K_{ij} = k(x_i, x_j)$。噪声向量服从 $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$，其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n### 第 1 部分：$\\theta$ 的后验分布\n\n为了求 $\\theta$ 的后验分布 $p(\\theta|\\mathbf{y})$，我们首先应用 Bayes 法则：\n$$p(\\theta|\\mathbf{y}) \\propto p(\\mathbf{y}|\\theta) p(\\theta)$$\n先验 $p(\\theta)$ 已给定为 $\\mathcal{N}(\\theta|0, \\tau^2)$。我们需要推导边缘似然 $p(\\mathbf{y}|\\theta)$，这涉及对潜在偏差向量 $\\boldsymbol{\\delta}$ 进行积分。\n\n在 $\\theta$ 取固定值的条件下，模型可以写成 $\\mathbf{y} - \\theta\\mathbf{x} = \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}$。令 $\\mathbf{z}(\\theta) = \\mathbf{y} - \\theta\\mathbf{x}$。向量 $\\mathbf{z}(\\theta)$ 是两个独立的零均值高斯向量 $\\boldsymbol{\\delta}$ 和 $\\boldsymbol{\\varepsilon}$ 的和。因此，$\\mathbf{z}(\\theta)$ 也是一个高斯随机向量。\n它的均值为 $E[\\mathbf{z}(\\theta)] = E[\\boldsymbol{\\delta}] + E[\\boldsymbol{\\varepsilon}] = \\mathbf{0} + \\mathbf{0} = \\mathbf{0}$。\n它的协方差为 $\\text{Cov}(\\mathbf{z}(\\theta)) = \\text{Cov}(\\boldsymbol{\\delta}) + \\text{Cov}(\\boldsymbol{\\varepsilon}) = K + \\sigma^2 I$。我们用 $\\Sigma_y = K + \\sigma^2 I$ 表示这个协方差矩阵。\n\n因此，$\\mathbf{z}(\\theta)$ 的分布是 $\\mathbf{z}(\\theta) \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_y)$。这意味着给定 $\\theta$ 的条件下 $\\mathbf{y}$ 的分布是 $\\mathbf{y}|\\theta \\sim \\mathcal{N}(\\theta\\mathbf{x}, \\Sigma_y)$。边缘似然是该分布的概率密度函数：\n$$p(\\mathbf{y}|\\theta) = \\frac{1}{(2\\pi)^{n/2}|\\det(\\Sigma_y)|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\theta\\mathbf{x})^T \\Sigma_y^{-1}(\\mathbf{y} - \\theta\\mathbf{x})\\right)$$\n$\\theta$ 的未归一化后验是似然与先验的乘积：\n$$p(\\theta|\\mathbf{y}) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - \\theta\\mathbf{x})^T \\Sigma_y^{-1}(\\mathbf{y} - \\theta\\mathbf{x})\\right) \\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$$\n该后验在解析上是一个高斯分布。然而，问题要求通过数值积分来计算其矩。我们定义一个值的网格 $\\{\\theta_j\\}$。对每个 $\\theta_j$，我们计算未归一化的后验值 $\\tilde{p}_j = p(\\mathbf{y}|\\theta_j)p(\\theta_j)$。每个网格点上的后验概率质量近似为 $p(\\theta_j|\\mathbf{y}) \\approx \\tilde{p}_j / \\sum_k \\tilde{p}_k \\Delta\\theta$，其中 $\\Delta\\theta$ 是网格间距。然后通过数值积分（例如，梯形法则）计算后验均值和方差：\n$$E[\\theta|\\mathbf{y}] = \\int \\theta p(\\theta|\\mathbf{y}) d\\theta \\approx \\frac{\\sum_j \\theta_j \\tilde{p}_j \\Delta\\theta}{\\sum_k \\tilde{p}_k \\Delta\\theta}$$\n$$\\text{Var}(\\theta|\\mathbf{y}) = E[\\theta^2|\\mathbf{y}] - (E[\\theta|\\mathbf{y}])^2 \\approx \\frac{\\sum_j \\theta_j^2 \\tilde{p}_j \\Delta\\theta}{\\sum_k \\tilde{p}_k \\Delta\\theta} - (E[\\theta|\\mathbf{y}])^2$$\n\n### 第 2 部分：$\\delta(x^\\star)$ 的后验分布\n\n首先，我们推导在给定数据 $\\mathbf{y}$ 和固定参数值 $\\theta$ 的条件下，在新点 $x^\\star$ 处的偏差 $\\delta^\\star = \\delta(x^\\star)$ 的后验分布。根据高斯过程的性质，训练点上的潜在值 $\\boldsymbol{\\delta}$ 和测试点上的 $\\delta^\\star$ 的联合分布是一个零均值高斯分布：\n$$\n\\begin{pmatrix} \\boldsymbol{\\delta} \\\\ \\delta^\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K & \\mathbf{k}^\\star \\\\ (\\mathbf{k}^\\star)^T & k^{\\star\\star} \\end{pmatrix} \\right)\n$$\n其中 $\\mathbf{k}^\\star$ 是一个 $n \\times 1$ 的向量，其元素为 $(\\mathbf{k}^\\star)_i = k(x_i, x^\\star)$，并且 $k^{\\star\\star} = k(x^\\star, x^\\star) = \\alpha^2$。\n\n我们有观测值 $\\mathbf{z}(\\theta) = \\mathbf{y} - \\theta\\mathbf{x} = \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}$。我们需要给定 $\\mathbf{z}(\\theta)$ 条件下 $\\delta^\\star$ 的条件分布。$(\\delta^\\star, \\mathbf{z}(\\theta))$ 的联合分布也是高斯的。其协方差分量为：\n- $\\text{Cov}(\\delta^\\star, \\delta^\\star) = k^{\\star\\star}$\n- $\\text{Cov}(\\mathbf{z}(\\theta), \\mathbf{z}(\\theta)) = K + \\sigma^2 I = \\Sigma_y$\n- $\\text{Cov}(\\delta^\\star, \\mathbf{z}(\\theta)) = \\text{Cov}(\\delta^\\star, \\boldsymbol{\\delta} + \\boldsymbol{\\varepsilon}) = \\text{Cov}(\\delta^\\star, \\boldsymbol{\\delta}) = (\\mathbf{k}^\\star)^T$\n\n使用条件高斯分布的标准公式，给定 $\\mathbf{y}$ 和 $\\theta$ 的条件下 $\\delta^\\star$ 的后验是一个高斯分布 $\\delta^\\star|\\mathbf{y}, \\theta \\sim \\mathcal{N}(\\mu_{\\delta^\\star|\\theta}, \\sigma^2_{\\delta^\\star|\\theta})$，其中：\n$$E[\\delta^\\star|\\mathbf{y}, \\theta] = \\mu_{\\delta^\\star|\\theta} = (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} (\\mathbf{y} - \\theta\\mathbf{x})$$\n$$\\text{Var}(\\delta^\\star|\\mathbf{y}, \\theta) = \\sigma^2_{\\delta^\\star|\\theta} = k^{\\star\\star} - (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{k}^\\star$$\n注意，条件方差 $\\sigma^2_{\\delta^\\star|\\theta}$ 与 $\\theta$ 和 $\\mathbf{y}$ 无关。\n\n为了考虑 $\\theta$ 的不确定性，我们对其后验分布 $p(\\theta|\\mathbf{y})$ 进行边缘化。我们使用全期望定律和全方差定律。\n$\\delta^\\star$ 的后验均值是：\n$$E[\\delta^\\star|\\mathbf{y}] = E_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]] = E_{p(\\theta|\\mathbf{y})}[(\\mathbf{k}^\\star)^T \\Sigma_y^{-1} (\\mathbf{y} - \\theta\\mathbf{x})]$$\n$$E[\\delta^\\star|\\mathbf{y}] = (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{y} - ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x}) E_{p(\\theta|\\mathbf{y})}[\\theta]$$\n$$E[\\delta^\\star|\\mathbf{y}] = (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{y} - ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x}) E[\\theta|\\mathbf{y}]$$\n$\\delta^\\star$ 的后验方差是：\n$$\\text{Var}(\\delta^\\star|\\mathbf{y}) = E_{p(\\theta|\\mathbf{y})}[\\text{Var}(\\delta^\\star|\\mathbf{y}, \\theta)] + \\text{Var}_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]]$$\n第一项是：\n$$E_{p(\\theta|\\mathbf{y})}[\\sigma^2_{\\delta^\\star|\\theta}] = \\sigma^2_{\\delta^\\star|\\theta} = k^{\\star\\star} - (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{k}^\\star, \\text{ 因为它关于 } \\theta \\text{ 是一个常数}$$\n第二项是：\n$$\\text{Var}_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]] = \\text{Var}_{p(\\theta|\\mathbf{y})}[(\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{y} - \\theta((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x})]$$\n$$\\text{Var}_{p(\\theta|\\mathbf{y})}[E[\\delta^\\star|\\mathbf{y}, \\theta]] = ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x})^2 \\text{Var}_{p(\\theta|\\mathbf{y})}[\\theta]$$\n将这些结合起来，得到总后验方差：\n$$\\text{Var}(\\delta^\\star|\\mathbf{y}) = (k^{\\star\\star} - (\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{k}^\\star) + ((\\mathbf{k}^\\star)^T \\Sigma_y^{-1} \\mathbf{x})^2 \\text{Var}(\\theta|\\mathbf{y})$$\n\n### 第 3 部分：算法实现\n\n对于每个测试用例，算法按以下步骤进行：\n1.  从输入点 $\\mathbf{x}$ 和超参数 $\\alpha, \\ell$ 构建核矩阵 $K$。构成数据协方差 $\\Sigma_y = K + \\sigma^2 I$。\n2.  为确保数值稳定性，计算 $\\Sigma_y$ 的 Cholesky 分解 $\\Sigma_y = LL^T$。使用此分解和一个专门的求解器（如 `scipy.linalg.cho_solve`）来高效且稳定地计算与 $\\Sigma_y^{-1}$ 的乘积，例如 $\\Sigma_y^{-1}\\mathbf{y}$ 和 $\\Sigma_y^{-1}\\mathbf{x}$。\n3.  为 $\\theta$ 建立一个数值网格。为确保网格覆盖后验的高概率区域，使用 $\\theta$ 的后验均值和方差的解析近似来确定网格的中心和范围（例如，$\\mu_\\theta \\pm 6\\sigma_\\theta$）。\n4.  对于网格上的每个点 $\\theta_j$，评估未归一化的对数后验 $\\log p(\\mathbf{y}|\\theta_j) + \\log p(\\theta_j)$。对这些值取指数（使用 log-sum-exp 技巧以防止数值上溢/下溢），得到未归一化的后验概率 $\\tilde{p}_j$。\n5.  在网格上使用数值积分（梯形法则）来计算归一化常数，然后计算后验均值 $E[\\theta|\\mathbf{y}]$ 和方差 $\\text{Var}(\\theta|\\mathbf{y})$。\n6.  使用计算出的 $\\theta$ 的后验均值和方差，利用第 2 部分中推导的公式计算 $\\delta(x^\\star)$ 的最终后验均值和方差。这需要计算诸如 $\\mathbf{k}^\\star$、$k^{\\star\\star}$ 等附加项，以及它们与 $\\Sigma_y^{-1}$ 的乘积。\n7.  所需的量是 $\\theta$ 和 $\\delta(x^\\star)$ 的后验均值和标准差。标准差是计算出的方差的平方根。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    \"\"\"\n    test_cases = [\n        # Case A (general, moderate noise)\n        (\n            [0.0, 0.25, 0.5, 0.75, 1.0],\n            [0.05, 0.65, 1.0, 1.33, 1.98],\n            0.4, 0.3, 0.1, 1.0, 0.5\n        ),\n        # Case B (near noise-free boundary)\n        (\n            [0.0, 0.25, 0.5, 0.75, 1.0],\n            [0.05, 0.65, 1.0, 1.33, 1.98],\n            0.4, 0.3, 0.000001, 1.0, 0.5\n        ),\n        # Case C (few points, high noise, different hyperparameters)\n        (\n            [0.1, 0.4, 0.9],\n            [0.4588, 0.2088, 0.9412],\n            0.3, 0.5, 0.5, 2.0, 0.5\n        )\n    ]\n\n    all_results_lists = []\n    for case in test_cases:\n        x, y, alpha, l, sigma, tau, x_star = case\n        result_list = solve_one_case(x, y, alpha, l, sigma, tau, x_star)\n        all_results_lists.append(result_list)\n\n    # Format the final output string exactly as required, with no spaces.\n    string_parts = []\n    for res_list in all_results_lists:\n        inner_str = f\"[{res_list[0]},{res_list[1]},{res_list[2]},{res_list[3]}]\"\n        string_parts.append(inner_str)\n    final_output = f\"[{','.join(string_parts)}]\"\n    print(final_output)\n\ndef solve_one_case(x, y, alpha, l, sigma, tau, x_star):\n    \"\"\"\n    Computes posterior statistics for one test case.\n    \"\"\"\n    x = np.asarray(x, dtype=float)\n    y = np.asarray(y, dtype=float)\n    n = len(x)\n\n    def kernel(x1, x2, alpha_k, l_k):\n        \"\"\"Squared exponential kernel for 1D inputs.\"\"\"\n        x1 = np.asarray(x1).reshape(-1, 1)\n        x2 = np.asarray(x2).reshape(-1, 1)\n        sqdist = (x1 - x2.T)**2\n        return alpha_k**2 * np.exp(-0.5 / l_k**2 * sqdist)\n\n    # 1. Pre-computation using numerically stable linear algebra\n    X_train = x[:, np.newaxis]\n    K = kernel(X_train, X_train, alpha, l)\n    Sigma_y = K + sigma**2 * np.eye(n)\n\n    # Cholesky decomposition for stable solves: Sigma_y = L L^T\n    try:\n        L, lower = cho_factor(Sigma_y, lower=True)\n    except np.linalg.LinAlgError:\n        # Failsafe for non-positive definite matrix, though not expected here.\n        return [float('nan')] * 4\n\n    # Solve linear systems for vectors needed later.\n    # `cho_solve` computes x in `A x = b` given Cholesky factor of A.\n    v_y = cho_solve((L, lower), y)  # Represents Sigma_y^-1 * y\n    v_x = cho_solve((L, lower), x)  # Represents Sigma_y^-1 * x\n\n    # 2. Compute posterior of theta via numerical integration\n    # Use analytical posterior moments to define an effective integration grid.\n    x_T_Sigma_inv_x = np.dot(x, v_x)\n    y_T_Sigma_inv_x = np.dot(y, v_x)\n    \n    post_var_theta_analytical = 1.0 / (x_T_Sigma_inv_x + 1.0 / tau**2)\n    post_mean_theta_analytical = post_var_theta_analytical * y_T_Sigma_inv_x\n    post_std_theta_analytical = np.sqrt(post_var_theta_analytical)\n\n    # Set up numerical grid for theta\n    n_grid = 2001\n    grid_half_width = 8 * post_std_theta_analytical # A sufficiently wide grid\n    theta_grid = np.linspace(post_mean_theta_analytical - grid_half_width,\n                              post_mean_theta_analytical + grid_half_width, n_grid)\n\n    # Calculate unnormalized log posterior on the grid\n    # log p(y|theta) + log p(theta)\n    y_T_Sigma_inv_y = np.dot(y, v_y)\n    log_likelihood_term = -0.5 * (y_T_Sigma_inv_y - 2 * theta_grid * y_T_Sigma_inv_x + theta_grid**2 * x_T_Sigma_inv_x)\n    log_prior_theta = -0.5 * (theta_grid**2) / tau**2\n    unnorm_log_post_theta = log_likelihood_term + log_prior_theta\n\n    # Normalize posterior using log-sum-exp trick for numerical stability\n    log_p_max = np.max(unnorm_log_post_theta)\n    unnorm_post_theta = np.exp(unnorm_log_post_theta - log_p_max)\n    \n    # Numerical integration using trapezoidal rule\n    normalizing_constant = np.trapz(unnorm_post_theta, theta_grid)\n    post_theta_pdf = unnorm_post_theta / normalizing_constant\n    \n    # Compute posterior moments for theta\n    post_mean_theta = np.trapz(theta_grid * post_theta_pdf, theta_grid)\n    post_var_theta = np.trapz(theta_grid**2 * post_theta_pdf, theta_grid) - post_mean_theta**2\n    post_var_theta = max(0, post_var_theta) # Ensure non-negativity\n    post_std_theta = np.sqrt(post_var_theta)\n\n    # 3. Compute posterior of delta(x_star)\n    X_star = np.array([[x_star]])\n    k_star_vec = kernel(X_train, X_star, alpha, l).flatten() # k(x_i, x_star)\n    k_star_star = alpha**2 # k(x_star, x_star)\n\n    # Solve for v_k_star = Sigma_y^-1 * k_star_vec\n    v_k_star = cho_solve((L, lower), k_star_vec)\n    \n    k_star_T_Sigma_inv_y = np.dot(k_star_vec, v_y)\n    k_star_T_Sigma_inv_x = np.dot(k_star_vec, v_x)\n    k_star_T_Sigma_inv_k_star = np.dot(k_star_vec, v_k_star)\n    \n    # Posterior mean of delta(x_star)\n    post_mean_delta = k_star_T_Sigma_inv_y - k_star_T_Sigma_inv_x * post_mean_theta\n\n    # Posterior variance of delta(x_star)\n    var_delta_given_theta = k_star_star - k_star_T_Sigma_inv_k_star\n    var_of_mean_delta = (k_star_T_Sigma_inv_x**2) * post_var_theta\n    post_var_delta = var_delta_given_theta + var_of_mean_delta\n    post_var_delta = max(0, post_var_delta) # Ensure non-negativity\n    post_std_delta = np.sqrt(post_var_delta)\n    \n    # 4. Format and return results rounded to six decimal places\n    return [\n        round(post_mean_theta, 6),\n        round(post_std_theta, 6),\n        round(post_mean_delta, 6),\n        round(post_std_delta, 6)\n    ]\n\nsolve()\n```", "id": "3101558"}]}