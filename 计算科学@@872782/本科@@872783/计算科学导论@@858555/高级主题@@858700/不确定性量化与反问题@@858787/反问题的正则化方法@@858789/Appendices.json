{"hands_on_practices": [{"introduction": "正则化方法的核心在于为不适定问题引入先验知识，而不同的正则化项对应着不同的先验假设。本实践将对比两种基本的正则化策略：全变分 (Total Variation, TV) 和二次平滑 ($\\ell_2$ a.k.a. Tikhonov)。通过一个信号去量化的具体任务，你将亲手验证TV正则化在保持信号边缘方面的优势，并理解为何它在图像处理等领域中备受青睐。这个练习将帮助你直观地感受正则化项的选择如何直接影响逆问题的解的结构特性。[@problem_id:3185702]", "problem": "考虑一个长度为 $N$ 的一维离散信号，表示为向量 $x \\in \\mathbb{R}^{N}$。观测信号 $y \\in \\mathbb{R}^{N}$ 是通过步长为 $q > 0$ 的均匀中置量化产生的，该过程由算子 $Q_{q} : \\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ 建模，其分量定义为 $y_{i} = q \\cdot \\mathrm{round}\\!\\left(\\frac{x_{i}}{q}\\right)$，其中 $i = 1, \\dots, N$。逆问题是通过施加由量化产生的具有物理意义的约束，并应用正则化以偏好结构简单的解，从 $y$ 中估计 $x$。\n\n量化过程意味着一致性约束：对于每个索引 $i$，未知数 $x_{i}$ 必须位于量化区间 $[y_{i} - \\frac{q}{2}, \\, y_{i} + \\frac{q}{2}]$ 内。令可行集为 $C = \\{x \\in \\mathbb{R}^{N} \\mid y_{i} - \\frac{q}{2} \\le x_{i} \\le y_{i} + \\frac{q}{2} \\ \\text{for all} \\ i\\}$。定义离散前向差分算子 $D : \\mathbb{R}^{N} \\to \\mathbb{R}^{N-1}$ 为 $(Dx)_{i} = x_{i+1} - x_{i}$，其中 $i = 1, \\dots, N-1$。$x$ 的全变分 (Total Variation, TV) 是其离散梯度的 $\\ell_{1}$ 范数，即 $\\mathrm{TV}(x) = \\|Dx\\|_{1} = \\sum_{i=1}^{N-1} |x_{i+1} - x_{i}|$。\n\n您需要实现两种重建方法：\n\n1.  一种 TV 正则化的逆舍入方法：找到一个估计值 $x^{\\mathrm{TV}}$，该估计值解决约束性 TV 最小化问题\n    $$\n    \\min_{x \\in C} \\ \\mathrm{TV}(x).\n    $$\n    此问题强制对量化区间保真，并惩罚快速变化以消除条带伪影。\n\n2.  一种使用梯度欧几里得二范数 ($\\ell_{2}$) 的二次平滑方法：找到一个估计值 $x^{\\ell_{2}}$，该估计值解决\n    $$\n    \\min_{x \\in \\mathbb{R}^{N}} \\ \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\alpha \\|Dx\\|_{2}^{2},\n    $$\n    其中 $\\alpha > 0$ 是一个控制平滑强度的正则化参数。该方法不强制执行区间一致性，因此可能会模糊边缘并引入平滑伪影。\n\n评估时，使用均方误差 (Mean Squared Error, MSE)，其定义为\n$$\n\\mathrm{MSE}(x^{\\star}, x) = \\frac{1}{N} \\sum_{i=1}^{N} (x^{\\star}_{i} - x_{i})^{2},\n$$\n其中 $x^{\\star}$ 是未量化的真实信号。\n\n实现以下测试套件。在所有情况下，使用 $N = 64$。对于每个测试，合成真实信号 $x^{\\star}$，通过舍入到最接近的 $q$ 的倍数来计算 $y = Q_{q}(x^{\\star})$，执行两种重建，计算它们相对于 $x^{\\star}$ 的 MSE，并生成一个布尔值，指示 TV 正则化重建是否至少与 $\\ell_{2}$ 平滑重建一样准确，即 $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_{2}})$ 是否成立。\n\n- 测试用例 1（分段常数信号，粗量化）：\n  - 真实信号 $x^{\\star}$：长度为 $16$ 的分段，电平为 $[0.0, \\, 0.7, \\, -0.1, \\, 0.5]$。\n  - 量化步长 $q = 0.25$。\n  - $\\ell_{2}$ 正则化参数 $\\alpha = 0.5$。\n\n- 测试用例 2（恰好在量化水平上的常数信号）：\n  - 真实信号 $x^{\\star}_{i} = 0.5$ 对所有 $i$ 成立。\n  - 量化步长 $q = 0.5$。\n  - $\\ell_{2}$ 正则化参数 $\\alpha = 0.5$。\n\n- 测试用例 3（高频正弦信号）：\n  - 真实信号 $x^{\\star}_{i} = 0.7 \\sin\\!\\left(2\\pi \\cdot 8 \\cdot \\frac{i-1}{N}\\right)$，其中 $i = 1, \\dots, N$。\n  - 量化步长 $q = 0.3$。\n  - $\\ell_{2}$ 正则化参数 $\\alpha = 0.3$。\n\n- 测试用例 4（分段常数信号，细量化）：\n  - 真实信号 $x^{\\star}$：与测试用例 1 相同。\n  - 量化步长 $q = 0.05$。\n  - $\\ell_{2}$ 正则化参数 $\\alpha = 0.2$。\n\n您的实现细节必须遵循以下原则：\n- 约束性 TV 最小化应作为一个凸优化问题，使用一个遵循区间约束 $C$ 和 TV 正则化项的、有原则的一阶方法来解决。对 $D$ 使用前向差分，对 $D^{\\top}$ 使用相应的伴随算子。为离散算子施加反射（Neumann型）边界行为。\n- $\\ell_{2}$ 平滑解应通过求解由一阶最优性条件产生的正规方程来计算，这将得到一个对称正定三对角线性系统。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果（例如，$[result1,result2,result3,result4]$），其中每个条目是对应测试用例的布尔值，指示 $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_{2}})$ 是否成立。此问题不涉及任何物理单位、角度或百分比；所有量均为无量纲的实标量和向量。", "solution": "该问题要求实现并比较两种不同的正则化方法，用于从一维信号 $x \\in \\mathbb{R}^{N}$ 的量化测量值 $y \\in \\mathbb{R}^{N}$ 中重建该信号。量化是一个步长为 $q > 0$ 的均匀中置过程，定义为 $y_{i} = q \\cdot \\mathrm{round}(x_i / q)$。这种关系意味着真实信号值 $x_i$ 必须位于区间 $[y_i - q/2, y_i + q/2]$ 内，这定义了一个凸可行集 $C$。我们将比较一种强制执行此约束的基于全变分 (TV) 最小化的方法，以及一种不强制此约束的二次平滑方法。\n\n第一种方法，二次或 $\\ell_2$ 平滑，通过求解无约束优化问题来寻求估计值 $x^{\\ell_2}$：\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\ J(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\alpha \\|Dx\\|_{2}^{2}\n$$\n这里，$y$ 是观测到的量化信号，$\\alpha > 0$ 是一个正则化参数，$D$ 是前向差分算子，$(Dx)_i = x_{i+1} - x_i$。目标函数 $J(x)$ 是二次且严格凸的。通过将梯度 $\\nabla J(x)$ 设置为零可以找到唯一的最小值点。\n梯度由下式给出：\n$$\n\\nabla J(x) = (x - y) + 2\\alpha D^{\\top}Dx = 0\n$$\n这产生了正规方程，即关于 $x$ 的线性方程组：\n$$\n(I + 2\\alpha D^{\\top}D)x = y\n$$\n其中 $I$ 是单位矩阵，$D^{\\top}$ 是算子 $D$ 的伴随（转置）算子。矩阵 $L = D^{\\top}D$ 表示带有一维诺伊曼 (Neumann) 边界条件的离散拉普拉斯算子，如问题所规定。对于定义的前向差分算子 $D \\in \\mathbb{R}^{(N-1) \\times N}$，矩阵 $L \\in \\mathbb{R}^{N \\times N}$ 是对称三对角矩阵：\n$$\nL = D^{\\top}D =\n\\begin{pmatrix}\n1  &-1  &  &   \\\\\n-1 & 2  &-1 &   \\\\\n   & \\ddots  &\\ddots & \\ddots  \\\\\n   &  &-1 & 2  &-1 \\\\\n   &   &   &-1 & 1\n\\end{pmatrix}\n$$\n因此，对于 $\\alpha > 0$，矩阵 $A = I + 2\\alpha L$ 也是对称、三对角且严格正定的。这样的线性系统可以使用带状矩阵求解器高效且稳健地求解。\n\n第二种方法是全变分 (TV) 正则化，它通过求解约束凸优化问题来找到估计值 $x^{\\mathrm{TV}}$：\n$$\n\\min_{x \\in C} \\ \\mathrm{TV}(x) \\quad \\text{其中} \\quad \\mathrm{TV}(x) = \\|Dx\\|_1\n$$\n可行集是超矩形 $C = \\{x \\in \\mathbb{R}^{N} \\mid y_i - q/2 \\le x_i \\le y_i + q/2, \\forall i \\}$。此问题在与量化区间保持一致的约束下，最小化信号梯度的 $\\ell_1$ 范数。由于目标函数 $\\mathrm{TV}(x)$ 是凸的但非光滑，且约束定义了一个凸集，该问题非常适合使用一阶原始-对偶算法求解。我们采用 Chambolle-Pock 算法。问题写成 $\\min_x F(Dx) + G(x)$ 的形式，其中 $F(u) = \\|u\\|_1$，$G(x) = I_C(x)$ 是集合 $C$ 的指示函数，如果 $x \\in C$，则其值为 $0$，否则为 $+\\infty$。\n\nChambolle-Pock 迭代（外插参数 $\\theta=1$）如下：\n1. $u^{k+1} = \\mathrm{prox}_{\\sigma F^*} (u^k + \\sigma D \\bar{x}^k)$\n2. $x^{k+1} = \\mathrm{prox}_{\\tau G} (x^k - \\tau D^{\\top} u^{k+1})$\n3. $\\bar{x}^{k+1} = 2x^{k+1} - x^k$\n\n邻近算子是标准的。$G$ 的邻近算子是到集合 $C$ 上的欧几里得投影：\n$$\n(\\mathrm{prox}_{\\tau G}(z))_i = \\Pi_C(z)_i = \\mathrm{clip}(z_i, y_i - q/2, y_i + q/2)\n$$\n$\\ell_1$ 范数的凸共轭 $F^*(u)$ 是 $\\ell_{\\infty}$ 单位球的指示函数。其邻近算子是到该球上的投影：\n$$\n(\\mathrm{prox}_{\\sigma F^*}(v))_i = \\Pi_{\\|\\cdot\\|_{\\infty}\\le 1}(v)_i = \\frac{v_i}{\\max(1, |v_i|)}\n$$\n为保证收敛，步长 $\\tau$ 和 $\\sigma$ 必须满足 $\\tau \\sigma \\|D\\|^2 < 1$。算子范数的平方 $\\|D\\|^2 = \\lambda_{\\max}(D^{\\top}D)$ 的上界为 $4$。我们选择保守的步长 $\\sigma$ 和 $\\tau$ 以确保稳定性。\n\n对于每个测试用例，生成真实信号 $x^{\\star}$，然后对其进行量化以获得 $y$。将两种重建方法应用于 $y$ 以获得 $x^{\\ell_2}$ 和 $x^{\\mathrm{TV}}$。每次重建的准确性均使用相对于原始信号 $x^{\\star}$ 的均方误差 (MSE) 来衡量：$\\mathrm{MSE}(x^{\\star}, x) = \\frac{1}{N} \\|x^{\\star} - x\\|_2^2$。每个用例的最终输出是一个布尔值，指示基于 TV 的重建是否至少与基于 $\\ell_2$ 的重建一样准确，即 $\\mathrm{MSE}(x^{\\star}, x^{\\mathrm{TV}}) \\le \\mathrm{MSE}(x^{\\star}, x^{\\ell_{2}})$ 是否成立。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the signal dequantization problem using two regularization methods\n    and compares their performance on a suite of test cases.\n    \"\"\"\n\n    def generate_x_star(case_num, N):\n        \"\"\"Generates the ground truth signal x_star for a given test case.\"\"\"\n        if case_num == 1 or case_num == 4:\n            x_star = np.zeros(N)\n            levels = [0.0, 0.7, -0.1, 0.5]\n            segment_len = N // 4\n            for i in range(4):\n                start, end = i * segment_len, (i + 1) * segment_len\n                x_star[start:end] = levels[i]\n            return x_star\n        elif case_num == 2:\n            return np.full(N, 0.5)\n        elif case_num == 3:\n            indices = np.arange(N)\n            return 0.7 * np.sin(2 * np.pi * 8 * indices / N)\n        return None\n\n    def quantize(x, q):\n        \"\"\"Performs uniform mid-tread quantization.\"\"\"\n        return q * np.round(x / q)\n\n    def mse(x_star, x):\n        \"\"\"Computes the Mean Squared Error.\"\"\"\n        return np.mean((x_star - x)**2)\n\n    def solve_l2(y, alpha, N):\n        \"\"\"Solves the l2-smoothing problem via normal equations.\"\"\"\n        # A = I + 2*alpha*D^T*D\n        # This matrix is symmetric and tridiagonal.\n        # Main diagonal: 1+2a, 1+4a, ..., 1+4a, 1+2a\n        # Off-diagonals: -2a\n        ab = np.zeros((3, N))\n        \n        # Upper diagonal (u=1)\n        ab[0, 1:] = -2 * alpha\n        \n        # Main diagonal (l=1, u=1)\n        ab[1, :] = 1 + 4 * alpha\n        ab[1, 0] = 1 + 2 * alpha\n        ab[1, -1] = 1 + 2 * alpha\n        \n        # Lower diagonal (l=1)\n        ab[2, :-1] = -2 * alpha\n        \n        x_l2 = solve_banded((1, 1), ab, y)\n        return x_l2\n\n    def solve_tv(y, q, N, iters=2000):\n        \"\"\"Solves the constrained TV minimization problem using Chambolle-Pock.\"\"\"\n        # Step sizes for primal-dual algorithm\n        # We need tau * sigma * ||D||^2  1. ||D||^2 approx 4 for 1D Neumann.\n        # We choose tau*sigma = 0.24, which is  1/4.\n        sigma = 0.5\n        tau = 0.48\n\n        # Initialize variables\n        x = y.copy()\n        x_bar = x.copy()\n        u = np.zeros(N - 1)\n        \n        # Pre-calculate bin limits\n        x_min = y - q / 2.0\n        x_max = y + q / 2.0\n\n        # Operator D and D_T\n        def D(vec):\n            return vec[1:] - vec[:-1]\n\n        def D_T(vec):\n            res = np.zeros(N)\n            res[0] = -vec[0]\n            res[1:-1] = vec[:-1] - vec[1:]\n            res[-1] = vec[-1]\n            return res\n\n        for _ in range(iters):\n            # Dual update (prox of F*)\n            u_next_arg = u + sigma * D(x_bar)\n            u = u_next_arg / np.maximum(1.0, np.abs(u_next_arg))\n            \n            # Primal update (prox of G)\n            x_old = x\n            x_next_arg = x - tau * D_T(u)\n            x = np.clip(x_next_arg, x_min, x_max)\n            \n            # Extrapolation\n            x_bar = 2 * x - x_old\n            \n        return x\n\n    test_cases = [\n        # (case_num, q, alpha)\n        (1, 0.25, 0.5),\n        (2, 0.5, 0.5),\n        (3, 0.3, 0.3),\n        (4, 0.05, 0.2),\n    ]\n\n    results = []\n    N = 64\n\n    for case_num, q, alpha in test_cases:\n        x_star = generate_x_star(case_num, N)\n        y = quantize(x_star, q)\n\n        # L2-smoothing reconstruction\n        x_l2 = solve_l2(y, alpha, N)\n        mse_l2 = mse(x_star, x_l2)\n\n        # TV reconstruction\n        x_tv = solve_tv(y, q, N)\n        mse_tv = mse(x_star, x_tv)\n        \n        results.append(mse_tv = mse_l2)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3185702"}, {"introduction": "在许多现实世界的逆问题中，我们拥有的先验知识可能比简单的“稀疏性”更为精细。例如，我们可能知道未知参数是成组出现或消失的。本实践将引导你超越标准的$\\ell_1$范数稀疏性，探索更强大的结构化稀疏概念——组稀疏性。通过一个精心设计的对比实验，你将看到组套索(Group LASSO)惩罚项如何在选择整个相关系数块方面优于标准$\\ell_1$范数，从而揭示了为问题的内在结构量身定制正则化项的重要性。[@problem_id:3185666]", "problem": "考虑一个线性逆问题，其前向算子已知，未知系数被划分到预定义的、不相交的组中。目标是比较两种用于估计未知系数的正则化选项：普通 $\\ell_{1}$ 范数（促进逐项稀疏性）和组稀疏性惩罚（促进组选择）。您必须编写一个完整的程序，针对给定的测试套件，解决这两个正则化逆问题，并报告哪种方法能正确选择整个活动组。\n\n基本问题设置：\n- 前向模型：给定矩阵 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 和数据 $\\mathbf{y} \\in \\mathbb{R}^{m}$，通过最小化以下形式的目标函数来恢复 $\\mathbf{x} \\in \\mathbb{R}^{n}$\n$$\n\\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\, \\mathcal{R}(\\mathbf{x}).\n$$\n- 两种正则化器 $\\mathcal{R}$ 的选择：\n  1. 普通 $\\ell_{1}$：$\\mathcal{R}(\\mathbf{x}) = \\|\\mathbf{x}\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$。\n  2. 组稀疏性（组套索）：假设索引被划分为 $G$ 个不相交的组 $\\{g_{1},\\dots,g_{G}\\}$；定义 $\\mathcal{R}(\\mathbf{x}) = \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$，其中 $\\mathbf{x}_{g}$ 是 $\\mathbf{x}$ 限制在组 $g$ 上的子向量。\n\n您必须使用的基本基础和假设：\n- 使用最小二乘数据保真项 $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_{2}^{2}$ 和上面定义的两个凸正则化器。\n- 在下面的所有测试用例中，取 $\\mathbf{A} = \\mathbf{I}_{n}$，即大小为 $n \\times n$ 的单位矩阵，因此数据为 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$ 且无噪声。这是一个标准且经过充分测试的设计，它隔离了正则化器对选择的影响，而不会与前向模型的条件数混淆。\n- 组是不相交的且被明确指定。成功标准涉及整个活动组的恢复。\n\n评估定义：\n- 设真实向量为 $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^{n}$，划分为组 $\\{g_{1},\\dots,g_{G}\\}$。如果 $\\|\\mathbf{x}_{\\text{true},g}\\|_{2}  0$，则组 $g$ 称为活动组，否则称为非活动组。\n- 给定一个估计值 $\\widehat{\\mathbf{x}}$，当且仅当满足以下条件时，我们称其正确选择了整个组：\n  - 对于每个活动组 $g$，$\\widehat{\\mathbf{x}}_{g}$ 的所有元素都为非零。\n  - 对于每个非活动组 $g$，$\\widehat{\\mathbf{x}}_{g}$ 的所有元素都恰好为零。\n- 在数值计算中，将绝对值小于容差 $\\varepsilon = 10^{-8}$ 的任何元素视为零，将绝对值大于或等于 $\\varepsilon$ 的任何元素视为非零。\n\n您的程序必须：\n- 对于每个测试用例，解决以下两个问题\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}$，\n  - $\\min_{\\mathbf{x}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g} \\|\\mathbf{x}_{g}\\|_{2}$，\n  使用您选择的任何正确的数值方法，在保证根据上述规则进行稳定识别的数值容差内求解。\n- 对于每种方法和每个测试用例，判断是否正确选择了整个活动组。\n- 将每个测试用例的结果映射到一个整数代码 $c \\in \\{0,1,2,3\\}$，如下所示：\n  - $c = 0$：两种方法都未能正确选择整个活动组，\n  - $c = 1$：只有普通 $\\ell_{1}$ 方法成功，\n  - $c = 2$：只有组稀疏性方法成功，\n  - $c = 3$：两种方法都成功。\n\n测试套件：\n- 所有情况均使用 $n = 6$，组 $g_{1} = [0,1,2]$，$g_{2} = [3,4,5]$，$\\mathbf{A} = \\mathbf{I}_{6}$，以及 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$。\n- 情况1（$\\ell_{1}$ 失败但组稀疏性成功的理想路径）：$\\mathbf{x}_{\\text{true}} = [0.6, 0.6, 0.6, 0, 0, 0]$，$\\lambda = 0.8$。\n- 情况2（具有非常强正则化的边界情况）：$\\mathbf{x}_{\\text{true}} = [1, 1, 1, 0, 0, 0]$，$\\lambda = 5$。\n- 情况3（组内幅度相关，$\\ell_{1}$ 仅保留一个子集）：$\\mathbf{x}_{\\text{true}} = [1.2, 0.3, 0.2, 0, 0, 0]$，$\\lambda = 0.5$。\n- 情况4（弱正则化，两者均成功）：$\\mathbf{x}_{\\text{true}} = [2, 2, 2, 0, 0, 0]$，$\\lambda = 0.1$。\n\n数值和实现要求：\n- 不涉及角度；不涉及物理单位。\n- 使用数值容差 $\\varepsilon = 10^{-8}$ 进行零/非零判断。\n- 您的程序应生成单行输出，其中包含一个逗号分隔的整数列表，用方括号括起来，按四个测试用例的顺序排列为 $[c_{1}, c_{2}, c_{3}, c_{4}]$。\n\n您的任务：\n- 实现求解器，在四个指定的用例上运行它，应用选择规则，并以指定的确切格式打印单行结果。不需要用户输入，也不允许使用外部数据文件。", "solution": "该问题要求比较两种用于线性逆问题的正则化方法：普通 $\\ell_1$ 范数和组稀疏性范数。目标是确定哪种方法能正确识别预定义的活动系数组。通过将前向算子 $\\mathbf{A}$ 设置为单位矩阵 $\\mathbf{I}$ 并提供无噪声数据 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$，该问题得到了简化。这种设置隔离了正则化器相关近端算子的行为。\n\n每个测试用例需要解决的两个优化问题是：\n1.  **普通 $\\ell_1$ 正则化 (LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\|\\mathbf{x}\\|_{1}\n    $$\n2.  **组稀疏性正则化 (Group LASSO):**\n    $$\n    \\min_{\\mathbf{x} \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|\\mathbf{x}-\\mathbf{y}\\|_{2}^{2} + \\lambda \\sum_{g \\in G} \\|\\mathbf{x}_{g}\\|_{2}\n    $$\n其中 $\\mathbf{y} = \\mathbf{x}_{\\text{true}}$，$\\lambda$ 是正则化参数，$G$ 是预定义的、不相交的索引组集合。对于所有测试用例，向量维度为 $n=6$，组为 $g_1$（索引 $\\{0, 1, 2\\}$）和 $g_2$（索引 $\\{3, 4, 5\\}$）。\n\n这些优化问题的解是通过将相应正则化项的近端算子应用于数据 $\\mathbf{y}$ 来找到的。\n\n**普通 $\\ell_1$ 正则化的解**\n\n目标函数相对于 $\\mathbf{x}$ 的各个分量是可分的。问题可以写成：\n$$\n\\sum_{i=1}^{n} \\min_{x_i \\in \\mathbb{R}} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda |x_i| \\right)\n$$\n每个分量 $\\widehat{x}_i$ 的解由软阈值算子给出：\n$$\n\\widehat{x}_i = \\text{prox}_{\\lambda|\\cdot|}(y_i) = \\text{sign}(y_i) \\max(|y_i| - \\lambda, 0)\n$$\n该算子对每个分量单独进行阈值处理。在解 $\\widehat{x}_i$ 中，幅度小于 $\\lambda$ 的分量 $y_i$ 被设置为零。这促进了逐项稀疏性。\n\n**组稀疏性正则化的解**\n\n目标函数相对于系数组是可分的。问题可以写成：\n$$\n\\sum_{g \\in G} \\min_{\\mathbf{x}_g \\in \\mathbb{R}^{|g|}} \\left( \\frac{1}{2}\\|\\mathbf{x}_g - \\mathbf{y}_g\\|_2^2 + \\lambda \\|\\mathbf{x}_g\\|_2 \\right)\n$$\n每个子向量 $\\widehat{\\mathbf{x}}_g$ 的解由组（或块）软阈值算子给出：\n$$\n\\widehat{\\mathbf{x}}_g = \\text{prox}_{\\lambda\\|\\cdot\\|_2}(\\mathbf{y}_g) = \\left( 1 - \\frac{\\lambda}{\\|\\mathbf{y}_g\\|_2} \\right)_+ \\mathbf{y}_g = \\frac{\\mathbf{y}_g}{\\|\\mathbf{y}_g\\|_2} \\max(\\|\\mathbf{y}_g\\|_2 - \\lambda, 0)\n$$\n其中 $(z)_+ = \\max(z, 0)$。该算子作用于整个组。如果数据子向量的欧几里得范数 $\\|\\mathbf{y}_g\\|_2$ 小于 $\\lambda$，则整个对应的解子向量 $\\widehat{\\mathbf{x}}_g$ 被设置为零向量。否则，该子向量被缩放但其方向保持不变。这促进了组稀疏性，意味着一个组中的所有系数要么都为零，要么都可以为非零。\n\n**评估与实现**\n\n对于每个测试用例，我们计算估计值 $\\widehat{\\mathbf{x}}_{\\ell_1}$ 和 $\\widehat{\\mathbf{x}}_{\\text{group}}$。然后我们根据提供的标准评估它们的成功与否。对于一个估计值 $\\widehat{\\mathbf{x}}$，如果满足以下条件，则该方法是成功的：\n1.  对于每个活动组 $g$（其中 $\\|\\mathbf{x}_{\\text{true},g}\\|_2 > 0$），$\\widehat{\\mathbf{x}}_g$ 的所有元素都为非零。\n2.  对于每个非活动组 $g$（其中 $\\|\\mathbf{x}_{\\text{true},g}\\|_2 = 0$），$\\widehat{\\mathbf{x}}_g$ 的所有元素都为零。\n\n在数值上，“零”被定义为绝对值小于 $\\varepsilon = 10^{-8}$。\n\n对于所有测试用例，$\\mathbf{x}_{\\text{true}, g_1}$ 至少有一个非零分量，这使得 $g_1$ 成为活动组。$\\mathbf{x}_{\\text{true}, g_2}$ 是零向量，这使得 $g_2$ 成为非活动组。\n- 如果 $\\lambda$ 足够大，以至于将活动组的某些（但非全部）分量置零（例如，如果 $\\mathbf{y}_{g_1}$ 的分量具有不同的大小），则 $\\ell_1$ 方法将不满足成功标准。\n- 组稀疏性方法旨在避免这种情况：它将每个组视为一个单元，要么将其完全置零，要么保留它（缩放后）。因此，它要么将 $\\widehat{\\mathbf{x}}_{g_1}$ 的所有元素都设置为零，要么保持其所有分量为非零（假设 $\\mathbf{y}_{g_1}$ 没有零元素，这在相关测试用例中是成立的）。\n\n程序将实现这两个近端算子和评估逻辑，计算四个测试用例的结果，并将它们映射到指定的整数代码。最终输出将是这些代码的列表。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the regularization comparison problem for a given test suite.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (x_true_list, lambda_val)\n    test_cases = [\n        # Case 1\n        (np.array([0.6, 0.6, 0.6, 0.0, 0.0, 0.0]), 0.8),\n        # Case 2\n        (np.array([1.0, 1.0, 1.0, 0.0, 0.0, 0.0]), 5.0),\n        # Case 3\n        (np.array([1.2, 0.3, 0.2, 0.0, 0.0, 0.0]), 0.5),\n        # Case 4\n        (np.array([2.0, 2.0, 2.0, 0.0, 0.0, 0.0]), 0.1),\n    ]\n\n    # Global problem parameters\n    groups = [[0, 1, 2], [3, 4, 5]]\n    epsilon = 1e-8\n\n    def solve_l1(y, lam):\n        \"\"\"\n        Solves the l1-regularized problem using soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * ||x||_1\n        \"\"\"\n        return np.sign(y) * np.maximum(np.abs(y) - lam, 0)\n\n    def solve_group_sparsity(y, lam, groups_list):\n        \"\"\"\n        Solves the group sparsity regularized problem using block soft-thresholding.\n        min_x 0.5 * ||x - y||_2^2 + lam * sum_g ||x_g||_2\n        \"\"\"\n        x_hat = np.zeros_like(y)\n        for g_indices in groups_list:\n            y_g = y[g_indices]\n            norm_y_g = np.linalg.norm(y_g)\n            \n            if norm_y_g > lam:\n                scaler = (1 - lam / norm_y_g)\n                x_hat[g_indices] = scaler * y_g\n            else:\n                x_hat[g_indices] = 0.0\n        return x_hat\n\n    def check_success(x_hat, x_true, groups_list, tol):\n        \"\"\"\n        Checks if an estimate x_hat correctly selects entire active groups.\n        Success criterion:\n        1. For active groups, all entries of x_hat_g must be nonzero.\n        2. For inactive groups, all entries of x_hat_g must be zero.\n        \"\"\"\n        for g_indices in groups_list:\n            x_true_g = x_true[g_indices]\n            x_hat_g = x_hat[g_indices]\n            \n            # Determine if the ground-truth group is active\n            is_active = np.linalg.norm(x_true_g) > 0\n            \n            if is_active:\n                # All entries must be nonzero\n                if not np.all(np.abs(x_hat_g) >= tol):\n                    return False\n            else: # inactive\n                # All entries must be zero\n                if not np.all(np.abs(x_hat_g)  tol):\n                    return False\n        return True\n\n    results = []\n    for x_true, lam in test_cases:\n        y = x_true  # Data is noise-free\n\n        # Solve and evaluate for plain l1\n        x_hat_l1 = solve_l1(y, lam)\n        l1_succeeds = check_success(x_hat_l1, x_true, groups, epsilon)\n\n        # Solve and evaluate for group sparsity\n        x_hat_group = solve_group_sparsity(y, lam, groups)\n        group_succeeds = check_success(x_hat_group, x_true, groups, epsilon)\n        \n        # Determine the integer code based on success\n        code = 0\n        if l1_succeeds and group_succeeds:\n            code = 3\n        elif group_succeeds:\n            code = 2\n        elif l1_succeeds:\n            code = 1\n        \n        results.append(code)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3185666"}, {"introduction": "设计和选择一个好的正则化方法只是解决逆问题的一部分，同样重要的是如何科学地评估其性能。本实践将向你揭示一个在计算科学中普遍存在却又极易被忽视的陷阱——“反问题之罪” (Inverse Crime)。这种“罪行”指的是在测试算法时，使用与生成模拟数据完全相同的模型，这往往会导致对算法性能过于乐观的评估。通过本练习，你将量化这种效应，并学习如何通过引入网格不匹配等更真实的设计来避免这种方法论上的谬误，从而获得对算法性能更可靠的评价。[@problem_id:3185734]", "problem": "考虑一个定义在周期域 $[0,1)$ 上的一维线性反问题，其中未知信号 $x(t)$ 被一个已知的核函数模糊，并在加性噪声下被观测。正向模型是线性的，由 $y = A x + \\varepsilon$ 给出，其中 $A$ 是一个离散化的卷积算子，$y$ 是观测数据，$\\varepsilon$ 是加性噪声。你将实现零阶 Tikhonov 正则化来重构 $x$，并定量评估同时使用相同的离散化网格进行数据生成和反演（一种“反演犯罪”）如何人为地夸大性能，以及网格不匹配如何减弱这种效应。\n\n使用以下基本定义和建模假设：\n\n- 离散化：对于一个整数 $n$，定义一个间距为 $\\Delta t = 1/n$ 的均匀网格，并在点 $t_i = i/n$（其中 $i \\in \\{0,1,\\dots,n-1\\}$）上对 $x$ 进行采样。将所得向量记为 $x_n \\in \\mathbb{R}^n$。\n- 真实信号：定义连续函数\n  $$x_{\\mathrm{true}}(t) = 0.5 \\exp\\!\\left(-\\frac{(t-0.3)^2}{2 \\cdot 0.05^2}\\right) + 1.0 \\exp\\!\\left(-\\frac{(t-0.72)^2}{2 \\cdot 0.03^2}\\right) + 0.2 \\sin(2\\pi \\cdot 7 \\cdot t),$$\n  并令 $x_n$ 为其在大小为 $n$ 的网格上的逐点采样。\n- 周期性高斯模糊核：对于大小为 $n$ 的网格，通过下式定义一个离散、归一化、周期性的高斯核 $h_n \\in \\mathbb{R}^n$\n  $$h_n[k] = \\frac{\\exp\\!\\left(-\\frac{d(k)^2}{2 \\sigma^2}\\right)}{\\sum_{j=0}^{n-1} \\exp\\!\\left(-\\frac{d(j)^2}{2 \\sigma^2}\\right)}, \\quad d(k) = \\min\\!\\left(\\frac{k}{n}, 1 - \\frac{k}{n}\\right), \\quad k \\in \\{0,1,\\dots,n-1\\},$$\n  其中 $\\sigma = 0.03$。这将产生一个循环卷积算子 $A_n \\in \\mathbb{R}^{n \\times n}$，它通过循环卷积来应用 $h_n$。\n- 噪声模型：对于无噪声数据 $y \\in \\mathbb{R}^m$，添加噪声 $\\varepsilon$，使得噪声的相对欧几里得范数等于一个指定水平 $\\eta$，即 $\\|\\varepsilon\\|_2 = \\eta \\|y\\|_2$。使用 $\\eta = 0.02$。\n- 零阶 Tikhonov 正则化：给定模型矩阵 $B \\in \\mathbb{R}^{m \\times n}$ 和数据 $z \\in \\mathbb{R}^{m}$，重构结果 $x^\\ast \\in \\mathbb{R}^{n}$ 是下式的最小化子\n  $$\\min_{x \\in \\mathbb{R}^{n}} \\|B x - z\\|_2^2 + \\lambda^2 \\|x\\|_2^2,$$\n  其中 $\\lambda = 0.01$。\n\n网格不匹配以减弱反演犯罪：\n\n- 如果用于生成合成数据的离散化方法和正向算子与反演步骤中重用的完全相同，则会发生反演犯罪。为减弱此效应，我们在一个更精细的网格 $n_f$ 上生成数据，并通过块平均到粗糙网格 $n_c$（其中 $n_f/n_c \\in \\mathbb{N}$）上来形成粗糙测量值。然后，在粗糙网格 $n_c$ 上使用其自身的粗糙算子 $A_{n_c}$ 进行反演，而不是使用一个精确投影的精细算子。\n- 块平均限制：对于 $n_f = r \\cdot n_c$（其中 $r$ 为整数），通过对连续块进行平均，将 $y_{n_f} \\in \\mathbb{R}^{n_f}$ 映射到 $y_{n_c} \\in \\mathbb{R}^{n_c}$：\n  $$(R y_{n_f})[i] = \\frac{1}{r} \\sum_{j=0}^{r-1} y_{n_f}[r \\cdot i + j], \\quad i \\in \\{0,1,\\dots,n_c-1\\}.$$\n\n任务：\n\n- 将正向算子 $A_n$ 实现为一个通过 $h_n$ 产生循环卷积的循环矩阵。\n- 实现噪声添加，以达到欧几里得范数下的相对噪声水平 $\\eta$。\n- 通过求解相关的正规方程来实现零阶 Tikhonov 解。\n\n评估方案：\n\n- 情况1（反演犯罪）：使用 $n_f = 64$，$n_c = 64$。从 $x_{\\mathrm{true}}$ 生成 $x_{64}$，合成无噪声数据 $y = A_{64} x_{64}$，添加水平为 $\\eta$ 的噪声，并使用 $A_{64}$ 和相同的离散化方法在同一网格上进行重构。\n- 情况2（网格不匹配）：使用 $n_f = 128$，$n_c = 64$。生成 $x_{128}$ 和 $y_{128} = A_{128} x_{128}$，添加水平为 $\\eta$ 的噪声，使用 $r = 2$ 进行块平均得到 $y_{64}$，并使用 $A_{64}$ 在粗糙网格上进行重构。\n- 情况3（更强的网格不匹配）：使用 $n_f = 256$，$n_c = 64$。生成 $x_{256}$ 和 $y_{256} = A_{256} x_{256}$，添加水平为 $\\eta$ 的噪声，使用 $r = 4$ 进行块平均得到 $y_{64}$，并使用 $A_{64}$ 在粗糙网格上进行重构。\n\n对于每种情况，计算相对重构误差\n$$e = \\frac{\\|x^\\ast - x_{n_c}\\|_2}{\\|x_{n_c}\\|_2},$$\n其中 $x_{n_c}$ 是在大小为 $n_c$ 的粗糙网格上采样的真实信号。\n\n测试套件和输出：\n\n- 使用上述指定的三种情况，参数为 $\\sigma = 0.03$，$\\eta = 0.02$，$\\lambda = 0.01$，并为噪声生成使用固定的伪随机种子 $1729$。\n- 你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，顺序为 $[e_{\\text{crime}}, e_{128 \\to 64}, e_{256 \\to 64}]$，每个浮点数四舍五入到恰好6位小数。", "solution": "该问题要求在通过 Tikhonov 正则化求解的一维反卷积问题背景下，研究“反演犯罪”现象。当用于生成合成数据以测试反演算法的数值模型与反演算法本身内部使用的模型完全相同时，就会发生反演犯罪。这可能导致对算法性能的人为乐观评估。我们将通过比较一个犯下反演犯罪的案例与两个引入网格不匹配以减弱该效应的案例，来量化这种效应。\n\n正向问题由线性方程 $y = A x + \\varepsilon$ 建模。这里，$x \\in \\mathbb{R}^n$ 表示离散化的未知信号，$A \\in \\mathbb{R}^{m \\times n}$ 是正向算子，$\\varepsilon \\in \\mathbb{R}^m$ 是加性噪声，$y \\in \\mathbb{R}^m$ 是观测数据。\n\n首先，我们定义离散正向模型的组成部分。连续的真实信号由下式给出：\n$$x_{\\mathrm{true}}(t) = 0.5 \\exp\\!\\left(-\\frac{(t-0.3)^2}{2 \\cdot 0.05^2}\\right) + 1.0 \\exp\\!\\left(-\\frac{(t-0.72)^2}{2 \\cdot 0.03^2}\\right) + 0.2 \\sin(2\\pi \\cdot 7 \\cdot t)$$\n该信号在域 $[0,1)$ 上大小为 $n$ 的均匀网格上，于点 $t_i = i/n$（其中 $i \\in \\{0, 1, \\dots, n-1\\}$）处被采样，从而得到向量 $x_n \\in \\mathbb{R}^n$。\n\n正向算子 $A_n$ 表示与一个离散、周期性的高斯模糊核 $h_n \\in \\mathbb{R}^n$ 进行循环卷积。对于给定的网格大小 $n$ 和标准差 $\\sigma = 0.03$，该核定义为：\n$$h_n[k] = \\frac{\\exp\\!\\left(-\\frac{d(k)^2}{2 \\sigma^2}\\right)}{Z}, \\quad \\text{其中} \\quad Z = \\sum_{j=0}^{n-1} \\exp\\!\\left(-\\frac{d(j)^2}{2 \\sigma^2}\\right)$$\n函数 $d(k) = \\min\\!\\left(\\frac{k}{n}, 1 - \\frac{k}{n}\\right)$ 表示网格上的周期性距离。归一化因子 $Z$ 确保核的总和为一。算子 $A_n \\in \\mathbb{R}^{n \\times n}$ 被构造为一个循环矩阵，其第一列是核向量 $h_n$。作用 $A_n x_n$ 等效于循环卷积 $h_n * x_n$。\n\n为了模拟真实的测量，我们引入了加性噪声 $\\varepsilon$。生成一个随机噪声向量，并对其幅度进行缩放，以匹配特定的相对噪声水平 $\\eta = 0.02$。对于无噪声数据 $y_{\\text{clean}}$，噪声向量 $\\varepsilon$ 的构造使其欧几里得范数满足 $\\|\\varepsilon\\|_2 = \\eta \\|y_{\\text{clean}}\\|_2$。使用固定的伪随机种子 $1729$ 确保可复现性。\n\n反问题是从含噪数据 $y$ 中恢复信号 $x$ 的一个估计。由于卷积是一种平滑操作，其逆运算（反卷积）是不适定的，这意味着数据 $y$ 中的小误差可能导致重构信号中的大误差。为稳定求解，我们采用零阶 Tikhonov 正则化。正则化解 $x^\\ast$ 是最小化目标函数的向量：\n$$J(x) = \\|B x - z\\|_2^2 + \\lambda^2 \\|x\\|_2^2$$\n此处，$z$ 是测量向量，$B$ 是用于反演的正向算子，$\\lambda = 0.01$ 是正则化参数，它平衡了数据保真度（第一项）和解的光滑性（第二项，惩罚 $x$ 的范数）。$J(x)$ 的最小化子通过求解相关的正规方程得到：\n$$(B^T B + \\lambda^2 I) x = B^T z$$\n其中 $I$ 是单位矩阵。解由 $x^\\ast = (B^T B + \\lambda^2 I)^{-1} B^T z$ 给出。我们求解这个线性方程组以得到 $x^\\ast$。\n\n我们研究三种情况来考察反演犯罪。在所有情况下，重构都是在大小为 $n_c=64$ 的粗糙网格上使用算子 $A_{64}$ 进行的。用于比较的“真实”信号是在此网格上采样的连续信号，即 $x_{64}$。\n\n情况1（反演犯罪）：在同一网格上生成数据并进行反演。\n- $n_f = 64$，$n_c = 64$。\n- 生成 $x_{64}$ 并计算 $y_{64} = A_{64} x_{64}$。\n- 添加噪声以获得 $z = y_{64,\\text{noisy}}$。\n- 使用 $B=A_{64}$ 和数据 $z$ 重构 $x^\\ast$。\n\n情况2（网格不匹配）：在更精细的网格（$n_f=128$）上生成数据，然后在反演前降采样到粗糙网格（$n_c=64$）。\n- $n_f = 128$，$n_c = 64$。比例为 $r = n_f/n_c = 2$。\n- 生成 $x_{128}$ 和 $y_{128} = A_{128} x_{128}$。\n- 添加噪声以获得 $y_{128,\\text{noisy}}$。\n- 通过块平均生成粗糙数据 $z$：$z[i] = \\frac{1}{2} \\sum_{j=0}^{1} y_{128, \\text{noisy}}[2i + j]$。\n- 使用 $B=A_{64}$ 和数据 $z$ 重构 $x^\\ast$。这避免了反演犯罪，因为数据生成模型（$A_{128}$ 和块平均）与反演模型（$A_{64}$）不同。\n\n情况3（更强的网格不匹配）：与情况2类似，但使用更精细的网格（$n_f=256$）生成数据。\n- $n_f = 256$，$n_c = 64$。比例为 $r = n_f/n_c = 4$。\n- 生成 $x_{256}$ 和 $y_{256} = A_{256} x_{256}$。\n- 添加噪声以获得 $y_{256,\\text{noisy}}$。\n- 使用 $r=4$ 进行块平均生成粗糙数据 $z$。\n- 使用 $B=A_{64}$ 和数据 $z$ 重构 $x^\\ast$。\n\n对于每种情况，性能通过相对重构误差来评估：\n$$e = \\frac{\\|x^\\ast - x_{n_c}\\|_2}{\\|x_{n_c}\\|_2}$$\n其中 $x_{n_c} = x_{64}$ 是在粗糙网格上采样的真实信号。与情况2和情况3相比，情况1中较低的误差将表明由于反演犯罪而产生了人为乐观的结果。", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import circulant\n\ndef solve():\n    \"\"\"\n    Solves the inverse problem for three cases to demonstrate the 'inverse crime'.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    SIGMA = 0.03\n    ETA = 0.02\n    LAMBDA = 0.01\n    SEED = 1729\n    \n    # --- Helper Functions ---\n    def get_true_signal(n):\n        \"\"\"Generates the true signal x_true(t) sampled on a grid of size n.\"\"\"\n        t = np.linspace(0.0, 1.0, n, endpoint=False)\n        term1 = 0.5 * np.exp(-((t - 0.3)**2) / (2 * 0.05**2))\n        term2 = 1.0 * np.exp(-((t - 0.72)**2) / (2 * 0.03**2))\n        term3 = 0.2 * np.sin(2 * np.pi * 7 * t)\n        return term1 + term2 + term3\n\n    def get_kernel(n, sigma):\n        \"\"\"Computes the discrete periodic Gaussian kernel h_n.\"\"\"\n        k = np.arange(n)\n        d = np.minimum(k / n, 1 - k / n)\n        h_unnormalized = np.exp(-(d**2) / (2 * sigma**2))\n        h = h_unnormalized / np.sum(h_unnormalized)\n        return h\n\n    def block_average(y_fine, r):\n        \"\"\"Performs block-averaging from a fine grid to a coarse grid.\"\"\"\n        n_fine = len(y_fine)\n        n_coarse = n_fine // r\n        # Reshape into (n_coarse, r) and take the mean along axis 1\n        return y_fine.reshape(n_coarse, r).mean(axis=1)\n\n    # --- Main Logic ---\n    rng = np.random.default_rng(seed=SEED)\n    \n    test_cases = [\n        (64, 64),  # Case 1: Inverse crime\n        (128, 64), # Case 2: Mesh mismatch\n        (256, 64), # Case 3: Stronger mesh mismatch\n    ]\n    \n    results = []\n\n    # Get the common coarse grid operator and true signal once\n    n_c_fixed = 64\n    x_true_coarse = get_true_signal(n_c_fixed)\n    h_coarse = get_kernel(n_c_fixed, SIGMA)\n    A_coarse = circulant(h_coarse)\n\n    for n_f, n_c in test_cases:\n        # Step 1: Generate data on the fine grid (n_f)\n        x_true_fine = get_true_signal(n_f)\n        h_fine = get_kernel(n_f, SIGMA)\n        A_fine = circulant(h_fine)\n        y_fine_clean = A_fine @ x_true_fine\n        \n        # Step 2: Add noise\n        noise_vec = rng.standard_normal(size=n_f)\n        noise_norm = np.linalg.norm(noise_vec)\n        y_clean_norm = np.linalg.norm(y_fine_clean)\n        \n        # Scale noise to the desired relative level\n        epsilon = ETA * (y_clean_norm / noise_norm) * noise_vec\n        y_fine_noisy = y_fine_clean + epsilon\n\n        # Step 3: Prepare data for inversion\n        if n_f == n_c:\n            # Case 1: No downsampling needed\n            z = y_fine_noisy\n        else:\n            # Cases 2  3: Apply block-averaging\n            r = n_f // n_c\n            z = block_average(y_fine_noisy, r)\n\n        # Step 4: Perform Tikhonov regularization on the coarse grid (n_c)\n        B = A_coarse\n        # Solve the normal equations: (B^T B + lambda^2 I) x = B^T z\n        lhs = B.T @ B + (LAMBDA**2) * np.identity(n_c)\n        rhs = B.T @ z\n        x_reconstructed = np.linalg.solve(lhs, rhs)\n\n        # Step 5: Calculate relative reconstruction error\n        error_norm = np.linalg.norm(x_reconstructed - x_true_coarse)\n        true_norm = np.linalg.norm(x_true_coarse)\n        relative_error = error_norm / true_norm\n        results.append(relative_error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n\n```", "id": "3185734"}]}