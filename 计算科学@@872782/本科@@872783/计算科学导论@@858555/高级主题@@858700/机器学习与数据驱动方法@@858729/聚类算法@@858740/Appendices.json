{"hands_on_practices": [{"introduction": "$k$-means 算法是一个迭代过程，因此确定何时停止是实现该算法的关键一步。一个常见的策略是当聚类分配不再发生变化时停止，这被称为“分配稳定性”准则。另一个更微妙的方法是当算法的目标函数 $J$（簇内平方和）的相对改进变得非常小时停止，这表明算法已经收敛到接近局部最优解的位置。\n\n这项练习 ([@problem_id:3107749]) 要求你从第一性原理出发，推导并实现这两种停止准则。通过在不同数据集上比较它们的表现，你将深入了解它们的实际行为，并理解在某些情况下，一个准则可能比另一个更早或更晚触发。这项实践对于编写高效且稳健的聚类代码至关重要。", "problem": "您的任务是为 $k$-均值聚类算法推导有理论依据的停止准则，并实现一个程序，在一个小型测试套件上比较它们的迭代次数。请从适合大学中级计算科学导论水平的第一性原理出发。\n\n从以下基本概念出发：\n- $\\mathbb{R}^d$ 上的欧几里得范数定义为 $\\lVert x \\rVert = \\sqrt{\\sum_{j=1}^d x_j^2}$，点 $x$ 和 $y$ 之间的欧几里得距离的平方为 $\\lVert x - y \\rVert^2$。\n- 对于给定的一组点 $\\{x_i\\}_{i=1}^n$ 到 $k$ 个簇的划分（分配），簇内平方和目标函数为\n$$\nJ(\\{\\mu_j\\}_{j=1}^k, \\{a_i\\}_{i=1}^n) = \\sum_{i=1}^n \\left\\lVert x_i - \\mu_{a_i} \\right\\rVert^2,\n$$\n其中 $a_i \\in \\{1,\\dots,k\\}$ 是分配给 $x_i$ 的簇索引，$\\mu_j \\in \\mathbb{R}^d$ 是第 $j$ 个簇的中心。\n- 对于固定的分配，关于 $\\mu_j$ 的目标函数最小化器是每个簇中点的算术平均值，即，如果 $C_j = \\{ i : a_i = j \\}$，那么 $\\mu_j^\\star = \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i$，前提是 $|C_j| > 0$。\n\n利用这些事实，首先解释为什么标准的 $k$-均值迭代过程，即交替进行以下两步：\n(1) 将每个点分配到其最近的当前中心（在欧几里得距离平方下），以及\n(2) 将每个中心更新为其所分配点的均值，\n会产生一个单调不增且以 0 为下界的目标值序列 $\\{J_t\\}_{t \\ge 0}$。由此，推导并证明一个基于每次迭代相对改进的停止准则：\n$$\n\\frac{\\Delta J_t}{J_{t-1}}  \\epsilon,\n$$\n其中 $\\Delta J_t = J_{t-1} - J_t$，且 $\\epsilon  0$ 是一个用户指定的容差。为确保当 $J_{t-1}$ 为 0 时的数值鲁棒性，使用 $\\max(J_{t-1}, \\delta)$ 作为分母，其中 $\\delta$ 是一个小的正常量。另外，推导并证明一个基于分配稳定性的停止准则，其定义为连续两次迭代之间任何 $a_i$ 都没有变化。\n\n实现这两种停止准则，并在以下测试套件上比较它们的迭代次数。在所有情况下，使用带有欧几里得距离平方的 $k$-均值算法和带有固定随机种子的确定性 $k$-means++ 初始化，以确保可复现性。将迭代次数计为完整的分配-更新周期数。对于相对改进的计数，使用直至分配稳定性达成的运行序列来评估该比率；如果在达到稳定性之前没有比率满足不等式，则报告分配稳定性的迭代次数作为基于相对改进的计数。\n\n设 $\\delta = 10^{-12}$，$T_{\\max}$ 为算法为防止无限循环而应停止的最大迭代次数。\n\n测试套件：\n- 测试案例 1（分离良好的簇）：\n  - $\\mathbb{R}^2$ 中的数据点：$x_1 = (0,0)$, $x_2 = (0.4,-0.2)$, $x_3 = (-0.3,0.1)$, $x_4 = (5.0,5.0)$, $x_5 = (5.2,4.7)$, $x_6 = (4.8,5.3)$, $x_7 = (5.1,5.1)$, $x_8 = (-0.2,-0.1)$。\n  - 簇的数量：$k = 2$。\n  - 相对改进容差：$\\epsilon = 10^{-4}$。\n  - 最大迭代次数：$T_{\\max} = 100$。\n- 测试案例 2（由于簇重叠导致改进缓慢）：\n  - $\\mathbb{R}^2$ 中的数据点：$x_1 = (0.0,0.0)$, $x_2 = (0.1,0.0)$, $x_3 = (0.0,0.1)$, $x_4 = (0.2,0.2)$, $x_5 = (0.3,0.3)$, $x_6 = (1.0,1.0)$, $x_7 = (1.1,1.2)$, $x_8 = (0.9,1.05)$, $x_9 = (1.2,0.9)$, $x_{10} = (1.05,1.1)$。\n  - 簇的数量：$k = 2$。\n  - 相对改进容差：$\\epsilon = 10^{-6}$。\n  - 最大迭代次数：$T_{\\max} = 200$。\n- 测试案例 3（具有相同点的退化情况）：\n  - $\\mathbb{R}^2$ 中的数据点：$x_1 = (1.0,1.0)$, $x_2 = (1.0,1.0)$, $x_3 = (1.0,1.0)$, $x_4 = (1.0,1.0)$, $x_5 = (1.0,1.0)$, $x_6 = (1.0,1.0)$。\n  - 簇的数量：$k = 3$。\n  - 相对改进容差：$\\epsilon = 10^{-9}$。\n  - 最大迭代次数：$T_{\\max} = 50$。\n\n为确保科学真实性和可复现性的实现细节：\n- 使用带有固定种子的确定性 $k$-means++ 初始化，均匀随机地选择第一个中心，后续中心的选择概率与到最近的已选中心的距离平方成正比。\n- 全程使用欧几里得距离的平方。\n- 在每个分配步骤之后，如果任何簇为空，则确定性地选择一些点重新分配给空簇，以确保在更新中心之前每个簇至少有一个点。需要一个确定性规则；例如，对于每个空簇，移动当前分配给最大簇中索引最大的点。这能在不引入随机性的情况下保证算法的进展。\n- 定义 $J_t$ 为第 $t$ 次迭代中更新步骤后，使用当时的分配和中心计算出的目标值。\n- 定义迭代次数为执行的分配-更新周期的数量。\n\n您的程序应为每个测试案例计算两个整数：直到相对改进准则首次成立时的迭代次数，以及直到达到分配稳定性时的迭代次数。最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，按测试套件顺序将配对展开为\n$[i_{1,\\mathrm{rel}}, i_{1,\\mathrm{stab}}, i_{2,\\mathrm{rel}}, i_{2,\\mathrm{stab}}, i_{3,\\mathrm{rel}}, i_{3,\\mathrm{stab}}]$。", "solution": "该问题要求对 $k$-均值算法的两种停止准则进行有理论依据的推导，并实现一个程序在给定的测试套件上比较它们的性能。该问题提法恰当，科学上合理，并为获得唯一的、可复现的解提供了足够的细节。\n\n首先，我们从所给的第一性原理出发，推导并证明 $k$-均值算法的性质和停止准则。\n\n$k$-均值算法是一个迭代过程，旨在将 $\\mathbb{R}^d$ 中的 $n$ 个数据点 $\\{x_i\\}_{i=1}^n$ 划分成 $k$ 个不相交的簇。其目标是最小化簇内平方和（WCSS），由以下目标函数给出：\n$$\nJ(\\{\\mu_j\\}_{j=1}^k, \\{a_i\\}_{i=1}^n) = \\sum_{i=1}^n \\left\\lVert x_i - \\mu_{a_i} \\right\\rVert^2\n$$\n其中 $a_i$ 是点 $x_i$ 被分配到的簇的索引，$\\mu_j$ 是第 $j$ 个簇的质心。标准算法，也称为 Lloyd 算法，在两个步骤之间交替进行：一个分配步骤和一个更新步骤。\n\n**目标函数的单调性和有界性**\n\n设算法在 $t-1$ 次完整迭代后的状态由质心集合 $\\{\\mu_j^{(t-1)}\\}_{j=1}^k$ 表征。目标值为 $J_{t-1}$。第 $t$ 次迭代包括两个步骤：\n\n1.  **分配步骤**：每个数据点 $x_i$ 被分配到具有最近质心的簇，从而最小化其对 WCSS 的贡献。点 $x_i$ 的新分配 $a_i^{(t)}$ 通过以下方式找到：\n    $$\n    a_i^{(t)} = \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\lVert x_i - \\mu_j^{(t-1)} \\rVert^2\n    $$\n    根据构造，对于给定的质心，这个新分配不会增加目标函数的值。设 $J'$ 是此步骤后，使用新分配 $\\{a_i^{(t)}\\}$ 但旧质心 $\\{\\mu_j^{(t-1)}\\}$ 的目标值。我们有：\n    $$\n    J' = \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t)}}^{(t-1)} \\rVert^2 \\le \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t-1)}}^{(t-1)} \\rVert^2 = J_{t-1}\n    $$\n    这个不等式成立，因为由于 $\\arg\\min$ 操作的性质，左边和式中的每一项都小于或等于右边和式中对应的项。\n\n2.  **更新步骤**：每个簇的质心被更新为分配给它的所有数据点的算术平均值。对于每个簇 $j$，新的质心 $\\mu_j^{(t)}$ 计算如下：\n    $$\n    \\mu_j^{(t)} = \\frac{1}{|C_j^{(t)}|} \\sum_{i \\in C_j^{(t)}} x_i, \\quad \\text{其中 } C_j^{(t)} = \\{i : a_i^{(t)} = j\\}\n    $$\n    如问题所述，$\\mu_j^{(t)}$ 的这一选择是最小化簇 $C_j^{(t)}$ 中点的平方距离和的选择。因此，与分配步骤之后的值相比，此步骤也必定会减少或保持目标函数值不变：\n    $$\n    J_t = \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t)}}^{(t)} \\rVert^2 \\le \\sum_{i=1}^n \\lVert x_i - \\mu_{a_i^{(t)}}^{(t-1)} \\rVert^2 = J'\n    $$\n\n结合这两个步骤，我们有 $J_t \\le J' \\le J_{t-1}$，这证明了目标函数值序列 $\\{J_t\\}_{t \\ge 0}$ 是单调不增的。此外，由于 $J$ 是平方范数之和，它本质上是非负的，即对所有 $t$ 都有 $J_t \\ge 0$。一个单调不增且有下界的序列保证会收敛到一个极限。这种收敛性是停止准则的基础。\n\n**停止准则 1：相对改进**\n\n由于目标值序列 $\\{J_t\\}$ 收敛，每一步的改进量 $\\Delta J_t = J_{t-1} - J_t$ 将趋近于 0。一个简单的准则可以是当 $\\Delta J_t$ 低于某个绝对阈值时停止。然而，这并非尺度不变的；$J$ 的大小取决于数据坐标的尺度。一个更鲁棒的准则是相对改进，它通过目标值本身来归一化减少量：\n$$\n\\frac{J_{t-1} - J_t}{J_{t-1}}  \\epsilon\n$$\n对于某个用户指定的小容差 $\\epsilon  0$。该准则衡量的是改进的比例，并且是无量纲的。为了避免当 $J_{t-1}$ 接近 0 时出现除以零或数值不稳定的情况，分母使用一个小的正常量 $\\delta$ 进行稳定，如问题所指定。最终的、鲁棒的准则是：\n$$\n\\frac{\\Delta J_t}{\\max(J_{t-1}, \\delta)}  \\epsilon\n$$\n这个准则表明算法已经达到了一个点，在该点上进一步的迭代在优化目标函数方面产生的收益递减。\n\n**停止准则 2：分配稳定性**\n\n当簇的分配在迭代之间不再变化时，$k$-均值算法收敛到一个不动点，这是目标函数的一个局部最小值。假设在第 $t$ 次迭代中，分配步骤产生的分配集合 $\\{a_i^{(t)}\\}$ 与前一次迭代的分配 $\\{a_i^{(t-1)}\\}$ 完全相同。\n$$\na_i^{(t)} = a_i^{(t-1)} \\quad \\forall i \\in \\{1, \\dots, n\\}\n$$\n如果分配不变，每个簇 $j$ 的点集 $C_j$ 也就不变。因此，更新步骤将产生与前一次迭代完全相同的质心：\n$$\n\\mu_j^{(t)} = \\frac{1}{|C_j^{(t)}|} \\sum_{i \\in C_j^{(t)}} x_i = \\frac{1}{|C_j^{(t-1)}|} \\sum_{i \\in C_j^{(t-1)}} x_i = \\mu_j^{(t-1)}\n$$\n由于分配和质心都与前一个状态相同，算法已达到一个不动点。所有后续的迭代都将产生相同的结果。因此，检查任意点的簇分配是否发生变化，是判断算法是否收敛到局部最优的确定性测试。这被称为分配稳定性准则。\n\n**实现计划**\n\n实现将遵循问题中的详细规范。对于每个测试案例，将执行单次 $k$-均值算法运行，直到达到分配稳定性或超过最大迭代次数 $T_{\\max}$。\n1.  **初始化**：使用 k-means++ 的确定性版本初始化中心，其中固定的随机种子（我们使用 0）使概率选择过程可复现。\n2.  **迭代循环**：主循环执行分配-更新周期。\n3.  **分配**：使用 `scipy.spatial.distance.cdist` 函数高效计算所有点和中心之间的欧几里得距离平方。\n4.  **空簇**：分配后，检查是否存在空簇。如果存在，则应用确定性规则：对于每个空簇，从当前最大的簇中移动索引最大的点到该空簇。这确保所有 $k$ 个质心都可以被更新。\n5.  **状态追踪**：存储前一次迭代的分配以检查稳定性。在每次更新步骤后计算目标函数值 $J_t$。\n6.  **准则评估**：在运行期间，我们追踪迭代次数。\n    -   `iter_stab`：在分配稳定之前完成的完整周期数。当在第 $t$ 次迭代开始时，发现分配与第 $t-1$ 次迭代的分配相同时，记录此值。此时计数为 $t-1$。\n    -   `iter_rel`：相对改进首次低于 $\\epsilon$ 的迭代次数 $t$。它在计算出 $J_t$ 后进行计算。\n7.  **最终计数**：运行将持续到达到稳定性或 $T_{\\max}$。`iter_rel` 的最终值取为满足条件的第一次迭代。如果条件从未满足，则根据问题说明，将 `iter_rel` 设置为最终的 `iter_stab` 值。\n\n这种设计确保了两种停止准则是基于完全相同的算法轨迹进行评估的，从而提供了公平的比较。", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef kmeans_plusplus_init(X, k, seed):\n    \"\"\"Deterministic k-means++ initialization.\"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = X.shape\n    centers = np.zeros((k, n_features))\n    \n    # 1. Choose the first center uniformly at random (from indices)\n    first_center_idx = rng.choice(n_samples)\n    centers[0] = X[first_center_idx]\n    \n    # 2. Choose remaining k-1 centers\n    for j in range(1, k):\n        # Calculate squared distances to the nearest existing center\n        dist_sq = cdist(X, centers[:j], 'sqeuclidean')\n        min_dist_sq = np.min(dist_sq, axis=1)\n        \n        # Calculate probabilities\n        prob_sum = np.sum(min_dist_sq)\n        if prob_sum == 0:\n            # Handle degenerate case (e.g., all points identical)\n            # Fallback to uniform probability\n            prob = np.full(n_samples, 1/n_samples)\n        else:\n            prob = min_dist_sq / prob_sum\n        \n        # Choose next center based on probabilities\n        next_center_idx = rng.choice(n_samples, p=prob)\n        centers[j] = X[next_center_idx]\n        \n    return centers\n\ndef handle_empty_clusters(assignments, k):\n    \"\"\"Deterministically handle empty clusters by reassigning points.\"\"\"\n    assignments = assignments.copy()\n    cluster_indices, counts = np.unique(assignments, return_counts=True)\n    \n    if len(cluster_indices)  k:\n        all_clusters = set(range(k))\n        assigned_clusters = set(cluster_indices)\n        empty_clusters = sorted(list(all_clusters - assigned_clusters))\n        \n        for j_empty in empty_clusters:\n            # Recalculate largest cluster each time\n            current_counts = np.bincount(assignments, minlength=k)\n            j_largest = np.argmax(current_counts)\n            \n            # Find point with the largest index in the largest cluster\n            indices_in_largest = np.where(assignments == j_largest)[0]\n            point_to_move_idx = np.max(indices_in_largest)\n            \n            # Reassign the point\n            assignments[point_to_move_idx] = j_empty\n            \n    return assignments\n\ndef run_kmeans_comparison(X, k, epsilon, T_max, delta, seed=0):\n    \"\"\"\n    Runs k-means until stability and reports iteration counts for two stopping criteria.\n    \"\"\"\n    n_samples, _ = X.shape\n    \n    # 1. Initialization\n    centers = kmeans_plusplus_init(X, k, seed)\n    \n    # Calculate an initial objective value to compare against J_1.\n    # This J_0 is based on initial centers and the assignments they induce.\n    initial_assignments = np.argmin(cdist(X, centers, 'sqeuclidean'), axis=1)\n    J_prev = np.sum((X - centers[initial_assignments])**2)\n    \n    assignments_prev = np.full(n_samples, -1, dtype=int)\n    \n    iter_rel = None\n    iter_stab = T_max # Default if not reached\n    \n    for t in range(1, T_max + 1):\n        # 2. Assignment step\n        assignments_curr_prime = np.argmin(cdist(X, centers, 'sqeuclidean'), axis=1)\n        \n        # 3. Handle empty clusters (after assignment, before stability check and update)\n        assignments_curr = handle_empty_clusters(assignments_curr_prime, k)\n\n        # 4. Stability check\n        if np.array_equal(assignments_curr, assignments_prev):\n            iter_stab = t - 1\n            break\n        \n        # 5. Update centers\n        new_centers = np.zeros_like(centers)\n        for j in range(k):\n            # This is guaranteed to be non-empty due to handle_empty_clusters\n            points_in_cluster = X[assignments_curr == j]\n            new_centers[j] = np.mean(points_in_cluster, axis=0)\n        centers = new_centers\n\n        # 6. Calculate objective J_t\n        J_curr = np.sum((X - centers[assignments_curr])**2)\n        \n        # 7. Relative improvement check\n        if iter_rel is None:\n            denominator = max(J_prev, delta)\n            relative_improvement = (J_prev - J_curr) / denominator\n            if relative_improvement  epsilon:\n                iter_rel = t\n        \n        # 8. Update state for next iteration\n        J_prev = J_curr\n        assignments_prev = assignments_curr\n    \n    # If the loop finished due to T_max, iter_stab is already T_max.\n    # If stability was reached, `break` was triggered.\n    else: # This `else` belongs to the `for` loop, runs if no `break`\n        iter_stab = T_max\n\n    # \"if no ratio satisfies the inequality before stability is reached, report \n    # the assignment stability iteration count\"\n    if iter_rel is None:\n        iter_rel = iter_stab\n        \n    return iter_rel, iter_stab\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    delta = 1e-12\n    # Using a fixed seed for reproducibility as requested.\n    fixed_seed = 0\n\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.0, 0.0], [0.4, -0.2], [-0.3, 0.1], [5.0, 5.0], \n                [5.2, 4.7], [4.8, 5.3], [5.1, 5.1], [-0.2, -0.1]\n            ]),\n            \"k\": 2, \"epsilon\": 1e-4, \"T_max\": 100\n        },\n        {\n            \"data\": np.array([\n                [0.0, 0.0], [0.1, 0.0], [0.0, 0.1], [0.2, 0.2], [0.3, 0.3], \n                [1.0, 1.0], [1.1, 1.2], [0.9, 1.05], [1.2, 0.9], [1.05, 1.1]\n            ]),\n            \"k\": 2, \"epsilon\": 1e-6, \"T_max\": 200\n        },\n        {\n            \"data\": np.array([\n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0], \n                [1.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n            ]),\n            \"k\": 3, \"epsilon\": 1e-9, \"T_max\": 50\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        iter_rel, iter_stab = run_kmeans_comparison(\n            case[\"data\"], case[\"k\"], case[\"epsilon\"], case[\"T_max\"], delta, seed=fixed_seed\n        )\n        all_results.extend([iter_rel, iter_stab])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3107749"}, {"introduction": "由于 $k$-means 算法依赖于欧几里得距离 $\\lVert x - c \\rVert_2$ 来衡量相似性，因此它对数据特征的尺度（scale）高度敏感。具有较大取值范围的特征会在距离计算中占据主导地位，可能导致聚类结果严重偏离直觉。因此，在应用 $k$-means 之前进行特征缩放，通常是一个至关重要的预处理步骤。\n\n这个练习 ([@problem_id:3107771]) 为你提供了一个亲手验证这种敏感性的机会。你将从数学上推导，随着特征尺度的变化，簇之间的决策边界会如何移动，然后通过编程实现一个模拟来量化这种效应。这项实践将加深你对为什么特征缩放不仅仅是“推荐”选项，而是保证结果有意义的“必要”步骤的理解。", "problem": "您将研究在二维设置中，仅使用固定质心的分配步骤时，逐特征缩放如何改变 $k$-means 算法中的簇分配。您的推导必须基于平方欧氏距离的标准定义和 $k$-means 分配规则。从这些基本定义出发，推导在对角特征缩放情况下，两个固定质心之间的决策边界如何变化。\n\n定义与设置：\n- 考虑一个位于 $\\mathbb{R}^2$ 中的数据集，包含六个点 $x_i \\in \\mathbb{R}^2$：\n  - $x_1 = (0.0, 0.0)$\n  - $x_2 = (0.0, 3.0)$\n  - $x_3 = (3.0, 0.0)$\n  - $x_4 = (2.9, 3.0)$\n  - $x_5 = (1.0, 2.0)$\n  - $x_6 = (2.0, 1.0)$\n- 使用 $k = 2$ 个簇，固定质心为 $c_1 = (0.0, 0.0)$ 和 $c_2 = (2.9, 3.0)$。\n- 对于任意具有严格正分量的缩放向量 $s \\in \\mathbb{R}^2$，定义对角缩放变换 $S = \\mathrm{diag}(s_1, s_2)$ 和缩放后的平方距离 $d_s^2(x, c) = \\lVert S(x - c) \\rVert_2^2$。\n- 在缩放 $s$ 下的 $k$-means 分配规则将一个点 $x$ 分配给使 $d_s^2(x, c_j)$ 最小化的质心 $c_j$。\n\n任务：\n1. 从平方欧氏距离的定义和 $k$-means 分配规则出发，推导 $c_1$ 和 $c_2$ 之间决策边界的解析方程，并将其表示为 $s$ 的函数。在原始坐标 $(x, y)$ 中表达该边界，并将其写成线性形式 $A(s)\\,x + B(s)\\,y + C(s) = 0$，明确地用 $s$、$c_1$ 和 $c_2$ 表示 $A(s)$、$B(s)$ 和 $C(s)$。\n2. 使用您推导出的边界，实现一个程序，该程序：\n   - 使用未缩放情况 $s = (1, 1)$ 计算所有六个点的基线分配。\n   - 对于每个指定的缩放向量 $s$，仅使用分配步骤，在缩放距离 $d_s^2(\\cdot,\\cdot)$ 下，使用固定质心 $c_1$ 和 $c_2$ 重新分配所有点，并计算有多少个点的分配与基线不同。\n   - 计算边界法向量的方向 $\\theta(s) = \\mathrm{atan2}(B(s), A(s))$（以弧度为单位），并将其归一化到区间 $[0, \\pi)$。\n   - 计算边界到原点的有符号距离，由 $d_0(s) = \\dfrac{C(s)}{\\sqrt{A(s)^2 + B(s)^2}}$ 给出。\n3. 对于一个缩放向量测试集，报告分配变化和边界特征。\n\n测试集：\n- 使用以下五个缩放向量 $s = (s_1, s_2)$：\n  - $s = (1.0, 1.0)$\n  - $s = (3.0, 1.0)$\n  - $s = (1.0, 3.0)$\n  - $s = (0.5, 2.0)$\n  - $s = (2.0, 0.5)$\n\n最终输出格式：\n- 对于上面按序列出的每个缩放向量，输出一个形式为 $[\\text{flips}, \\theta, d_0]$ 的列表，其中 $\\text{flips}$ 是其分配与 $s = (1.0, 1.0)$ 时的基线分配不同的点的整数数量，$\\theta$ 是边界法线角度（以弧度为单位），$d_0$ 是边界到原点的有符号距离。\n- 将 $\\theta$ 以弧度表示，并将 $\\theta$ 和 $d_0$ 都四舍五入到 $6$ 位小数。\n- 将五个测试用例的结果聚合到单行中，作为一个由方括号括起来、无空格的逗号分隔列表，例如：\n  - $[[\\text{flips}_1,\\theta_1,d_{0,1}],[\\text{flips}_2,\\theta_2,d_{0,2}],\\dots]$\n\n注意：\n- 不涉及物理单位。\n- 角度必须以弧度表示。\n- 所有作为实数的数值输出都必须四舍五入到 $6$ 位小数。", "solution": "该问题要求分析逐特征缩放如何影响 $k$-means 算法中的分配步骤。这包括在缩放距离度量下推导两个固定质心之间决策边界的方程，然后将其应用于特定数据集。\n\n解决方案分为三个部分：\n1.  一般决策边界方程的推导。\n2.  边界几何特征的计算。\n3.  应用于具体问题数据及计算流程概述。\n\n**1. 决策边界方程的推导**\n\n将一个点 $x \\in \\mathbb{R}^2$ 分配给具有质心 $c_1$ 和 $c_2$ 的两个簇中的一个，取决于确定哪个质心更近。决策边界是与两个质心等距的点的轨迹。在缩放距离度量下，此条件表示为：\n$$\nd_s^2(x, c_1) = d_s^2(x, c_2)\n$$\n其中 $d_s^2(x, c)$ 是缩放后的平方欧氏距离，定义为 $d_s^2(x, c) = \\lVert S(x - c) \\rVert_2^2$。这里，$x = (x, y)$，$c_j = (c_{jx}, c_{jy})$，且 $S = \\mathrm{diag}(s_1, s_2)$ 是对角缩放矩阵，其中 $s_1  0$ 且 $s_2  0$。\n\n变换 $S(x-c)$ 由下式给出：\n$$\nS(x-c) = \\begin{pmatrix} s_1  0 \\\\ 0  s_2 \\end{pmatrix} \\begin{pmatrix} x - c_x \\\\ y - c_y \\end{pmatrix} = \\begin{pmatrix} s_1(x - c_x) \\\\ s_2(y - c_y) \\end{pmatrix}\n$$\n于是，平方欧氏范数为：\n$$\nd_s^2(x, c) = \\lVert S(x - c) \\rVert_2^2 = (s_1(x - c_x))^2 + (s_2(y - c_y))^2 = s_1^2(x - c_x)^2 + s_2^2(y - c_y)^2\n$$\n将此代入决策边界条件：\n$$\ns_1^2(x - c_{1x})^2 + s_2^2(y - c_{1y})^2 = s_1^2(x - c_{2x})^2 + s_2^2(y - c_{2y})^2\n$$\n为求得直线方程，我们展开平方项：\n$$\ns_1^2(x^2 - 2xc_{1x} + c_{1x}^2) + s_2^2(y^2 - 2yc_{1y} + c_{1y}^2) = s_1^2(x^2 - 2xc_{2x} + c_{2x}^2) + s_2^2(y^2 - 2yc_{2y} + c_{2y}^2)\n$$\n二次项 $s_1^2x^2$ 和 $s_2^2y^2$ 在等式两边都出现，因此可以消去。我们得到一个关于 $x$ 和 $y$ 的线性方程：\n$$\n-2s_1^2xc_{1x} + s_1^2c_{1x}^2 - 2s_2^2yc_{1y} + s_2^2c_{1y}^2 = -2s_1^2xc_{2x} + s_1^2c_{2x}^2 - 2s_2^2yc_{2y} + s_2^2c_{2y}^2\n$$\n重新整理各项，将 $x$、$y$ 的系数和常数项归类，我们得到：\n$$\n(2s_1^2c_{2x} - 2s_1^2c_{1x})x + (2s_2^2c_{2y} - 2s_2^2c_{1y})y + (s_1^2c_{1x}^2 + s_2^2c_{1y}^2 - s_1^2c_{2x}^2 - s_2^2c_{2y}^2) = 0\n$$\n此方程符合所求的线性形式 $A(s)x + B(s)y + C(s) = 0$。这些系数可以明确地表示为缩放向量 $s = (s_1, s_2)$ 和质心坐标的函数：\n$$\nA(s) = 2s_1^2(c_{2x} - c_{1x})\n$$\n$$\nB(s) = 2s_2^2(c_{2y} - c_{1y})\n$$\n$$\nC(s) = s_1^2(c_{1x}^2 - c_{2x}^2) + s_2^2(c_{1y}^2 - c_{2y}^2) = \\lVert Sc_1 \\rVert_2^2 - \\lVert Sc_2 \\rVert_2^2\n$$\n\n**2. 边界特征的计算**\n\n使用系数 $A(s)$、$B(s)$ 和 $C(s)$，我们可以计算决策边界的几何性质。\n\n边界法向量的方向 $\\theta(s)$ 是向量 $(A(s), B(s))$ 相对于 x 轴正方向的角度。它由双参数反正切函数给出：\n$$\n\\theta(s) = \\mathrm{atan2}(B(s), A(s))\n$$\n问题要求将此角度归一化到区间 $[0, \\pi)$。由于 `atan2` 函数返回一个在 $(-\\pi, \\pi]$ 内的值，一个简单的调整 `if \\theta  0: \\theta = \\theta + \\pi` 就可以将任何负值结果正确地映射到所需范围，这对应于选择指向上半平面的法向量。\n\n边界到原点的有符号距离 $d_0(s)$ 由点 $(x_0, y_0)$ 到直线 $Ax + By + C = 0$ 的标准距离公式给出，即 $\\frac{Ax_0 + By_0 + C}{\\sqrt{A^2+B^2}}$。对于原点 $(x_0, y_0) = (0, 0)$，该公式简化为：\n$$\nd_0(s) = \\frac{C(s)}{\\sqrt{A(s)^2 + B(s)^2}}\n$$\n$d_0(s)$ 的符号表示原点位于直线的哪一侧。\n\n**3. 应用与计算流程**\n\n提供的具体数据是：\n- 点 $x_1 = (0.0, 0.0)$, $x_2 = (0.0, 3.0)$, $x_3 = (3.0, 0.0)$, $x_4 = (2.9, 3.0)$, $x_5 = (1.0, 2.0)$, $x_6 = (2.0, 1.0)$。\n- 质心 $c_1 = (0.0, 0.0)$ 和 $c_2 = (2.9, 3.0)$。\n\n计算流程如下：\n首先，我们建立基线分配。这是通过使用未缩放的距离来完成的，对应于缩放向量 $s = (1.0, 1.0)$。对于每个点 $x_i$，我们计算 $d_s^2(x_i, c_1)$ 和 $d_s^2(x_i, c_2)$，并将该点分配给产生最小距离的质心。这些分配作为参考。\n\n其次，对于提供的测试集中的每个缩放向量 $s$，我们重复分配过程：\n- 对每个点 $x_i$，计算缩放后的平方距离 $d_s^2(x_i, c_1)$ 和 $d_s^2(x_i, c_2)$。\n- 将 $x_i$ 分配给更近的质心。\n- 将每个点的新分配与其基线分配进行比较。分配发生变化的点的数量被计为 `flips`。\n\n第三，对于每个缩放向量 $s$，我们计算边界特征：\n- 我们将 $c_1=(0,0)$ 和 $c_2=(2.9, 3.0)$ 的具体坐标以及 $s=(s_1,s_2)$ 的分量代入推导出的 $A(s)$、$B(s)$ 和 $C(s)$ 的公式中：\n  $A(s) = 2s_1^2(2.9 - 0) = 5.8s_1^2$\n  $B(s) = 2s_2^2(3.0 - 0) = 6.0s_2^2$\n  $C(s) = s_1^2(0^2 - 2.9^2) + s_2^2(0^2 - 3.0^2) = -8.41s_1^2 - 9.0s_2^2$\n- 然后我们使用这些系数计算 $\\theta(s)$ 和 $d_0(s)$。\n\n最后，收集每个缩放向量的结果——翻转数、归一化角度 $\\theta(s)$ 和有符号距离 $d_0(s)$——并按指定格式进行格式化。所有浮点值都四舍五入到 $6$ 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing k-means assignment changes under feature scaling.\n    \"\"\"\n    # Define the dataset, centroids, and scaling vectors from the problem statement.\n    points = np.array([\n        [0.0, 0.0],\n        [0.0, 3.0],\n        [3.0, 0.0],\n        [2.9, 3.0],\n        [1.0, 2.0],\n        [2.0, 1.0]\n    ])\n\n    centroids = np.array([\n        [0.0, 0.0],\n        [2.9, 3.0]\n    ])\n\n    test_cases = [\n        (1.0, 1.0),\n        (3.0, 1.0),\n        (1.0, 3.0),\n        (0.5, 2.0),\n        (2.0, 0.5)\n    ]\n\n    def get_assignments(points, centroids, s_vector):\n        \"\"\"\n        Calculates cluster assignments for a given scaling vector.\n        \"\"\"\n        s = np.array(s_vector)\n        s_sq = s**2\n        \n        # Broadcasting to compute distances for all points to all centroids\n        # diff shape: (num_points, num_centroids, num_dims)\n        diff = points[:, np.newaxis, :] - centroids[np.newaxis, :, :]\n        \n        # scaled_sq_diffs shape: (num_points, num_centroids, num_dims)\n        # Broadcasting s_sq over the first two dimensions\n        scaled_sq_diffs = diff**2 * s_sq\n        \n        # dist_sq shape: (num_points, num_centroids)\n        # Sum over the dimensions axis\n        dist_sq = np.sum(scaled_sq_diffs, axis=2)\n        \n        # assignments shape: (num_points,)\n        # Find the index of the minimum distance for each point\n        assignments = np.argmin(dist_sq, axis=1)\n        return assignments\n\n    # Calculate baseline assignments for s = (1.0, 1.0)\n    baseline_assignments = get_assignments(points, centroids, test_cases[0])\n    \n    results = []\n    \n    # Process each test case\n    for s_vector in test_cases:\n        s1, s2 = s_vector\n        \n        # 1. Reassign points and count flips\n        current_assignments = get_assignments(points, centroids, s_vector)\n        flips = np.sum(current_assignments != baseline_assignments)\n        \n        # 2. Compute boundary characteristics\n        c1x, c1y = centroids[0]\n        c2x, c2y = centroids[1]\n\n        # Using derived formulas for A(s), B(s), C(s)\n        A = 2 * s1**2 * (c2x - c1x)\n        B = 2 * s2**2 * (c2y - c1y)\n        C = s1**2 * (c1x**2 - c2x**2) + s2**2 * (c1y**2 - c2y**2)\n        \n        # Compute orientation theta\n        theta = np.arctan2(B, A)\n        \n        # Normalize theta to the interval [0, pi)\n        # atan2 returns in (-pi, pi], so adding pi to negative values is sufficient\n        if theta  0:\n            theta += np.pi\n\n        # Compute signed distance d0\n        norm = np.sqrt(A**2 + B**2)\n        d0 = C / norm if norm != 0 else 0.0\n        \n        # Format the result entry for the current test case\n        formatted_entry = f\"[{flips},{theta:.6f},{d0:.6f}]\"\n        results.append(formatted_entry)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3107771"}, {"introduction": "标准 $k$-means 算法的一个关键局限性在于其目标函数——最小化簇内平方和（WCSS）——天然地倾向于创建大小相近的球形簇。当真实的簇在规模上存在巨大差异时，这会导致算法表现不佳，因为较大的簇会将其质心“拉”向自己，甚至侵占较小簇的区域。\n\n这个练习 ([@problem_id:3107780]) 让你能够定量地探索这种偏见。你将创建簇大小不均衡的合成数据集，观察标准 $k$-means 算法如何失效，然后实现一个加权的 $k$-means 变体作为强大的补救措施。这项实践教会你如何批判性地使用算法，并为你提供一种工具来纠正其主要缺陷之一。", "problem": "您需要从头开始实现 $k$-均值聚类算法及其加权变体，并用它们来分析在簇大小相差数量级的合成数据中，标准的 $k$-均值目标函数如何使解偏向于大簇。\n\n请从以下核心定义开始。给定数据点 $\\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$、预先指定的簇数 $k$、簇分配 $z_i \\in \\{1,\\dots,k\\}$ 以及质心 $\\{c_j \\in \\mathbb{R}^d\\}_{j=1}^k$，标准的 $k$-均值目标函数旨在最小化簇内平方和 (Within-Cluster Sum of Squares, WCSS)，其定义为\n$$\nJ(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n \\left\\| x_i - c_{z_i} \\right\\|_2^2,\n$$\n其中 $\\left\\| \\cdot \\right\\|_2$ 表示欧几里得范数。加权的 $k$-均值目标函数引入了非负的点权重 $\\{w_i\\}_{i=1}^n$ 并最小化\n$$\nJ_w(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n w_i \\left\\| x_i - c_{z_i} \\right\\|_2^2.\n$$\n使用迭代的“分配-更新”范式实现 $k$-均值算法：通过最近质心进行分配，并通过最小化关于 $\\{c_j\\}$ 的目标函数来更新质心。加权变体必须将质心更新为已分配点的加权平均值。使用 $k$-means++ 初始化方法来选择初始质心。\n\n通过从具有指定中心和标准差的各向同性高斯分布 $\\mathcal{N}(\\mu, \\sigma^2 I)$ 中独立采样，在二维空间中构建合成数据集。为了保证可复现性，数据集必须使用固定的随机种子生成。评估时，使用已知的数据生成标签来计算以下指标：\n\n- 将小簇错分率定义为源于小簇但被分配给映射到大簇真实中心的质心的点的比例。类似地定义大簇错分率。为了将每个学习到的质心映射到一个真实中心，选择 $\\ell_2$ 距离上最近的真实中心。\n- 将偏差分数 $B$ 定义为\n$$\nB = \\frac{\\text{被错分的小簇点数}}{n_s} - \\frac{\\text{被错分的大簇点数}}{n_\\ell},\n$$\n其中 $n_s$ 和 $n_\\ell$ 分别是小簇和大簇的大小。\n- 对于 $k  2$ 的情况，将大簇的重复计数 $D$ 定义为，其最近的真实中心是大簇真实中心的学习到的质心数量。\n\n您的程序必须实现这些算法并计算以下测试套件。在所有情况下，数据都是二维的，角度不适用，也没有物理单位。\n\n测试 1 (理想情况):\n- 数据：两个簇，大小为 $n_s = 60, n_\\ell = 600$，均值为 $\\mu_s = (-5, 0), \\mu_\\ell = (5, 0)$，标准差为 $\\sigma_s = 1, \\sigma_\\ell = 1$。\n- 聚类：标准 $k$-均值，其中 $k = 2$ 并采用 $k$-means++ 初始化。\n- 随机种子：数据生成和初始化均使用 $42$。\n- 输出：偏差分数 $B$，四舍五入到三位小数。\n\n测试 2 (重叠引发的偏差):\n- 数据：两个簇，大小为 $n_s = 60, n_\\ell = 600$，均值为 $\\mu_s = (0.1, 0), \\mu_\\ell = (0, 0)$，标准差为 $\\sigma_s = 1, \\sigma_\\ell = 1$。\n- 聚类：标准 $k$-均值，其中 $k = 2$ 并采用 $k$-means++ 初始化。\n- 随机种子：数据生成和初始化均使用 $43$。\n- 输出：偏差分数 $B$，四舍五入到三位小数。\n\n测试 3 (极端尺寸差异，额外质心分配):\n- 数据：两个簇，大小为 $n_s = 50, n_\\ell = 5000$，均值为 $\\mu_s = (3, 0), \\mu_\\ell = (0, 0)$，标准差为 $\\sigma_s = 0.3, \\sigma_\\ell = 1.5$。\n- 聚类：标准 $k$-均值，其中 $k = 3$ 并采用 $k$-means++ 初始化。\n- 随机种子：数据生成和初始化均使用 $44$。\n- 输出：大簇的重复计数 $D$，为整数。\n\n测试 4 (通过反尺寸加权进行修正):\n- 数据：与测试 $3$ 相同。\n- 聚类：加权 $k$-均值，其中 $k = 3$ 并采用 $k$-means++ 初始化，权重设置为：源于小簇的点的权重 $w_i = 1/n_s$，源于大簇的点的权重 $w_i = 1/n_\\ell$。\n- 随机种子：数据生成和初始化均使用 $44$。\n- 输出：大簇的重复计数 $D$，为整数。\n\n最终输出格式：您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔的结果列表，顺序为 $[B_{\\text{Test 1}}, B_{\\text{Test 2}}, D_{\\text{Test 3}}, D_{\\text{Test 4}}]$。偏差分数必须四舍五入到三位小数，重复计数必须是整数，生成形如 $[b_1,b_2,d_3,d_4]$ 的列表。", "solution": "该问题要求实现标准和加权 $k$-均值聚类算法，以分析标准方法对大簇的内在偏差。实现将从头开始，包括 $k$-means++ 初始化方案。\n\n### 算法公式化\n\n给定一个包含 $n$ 个数据点 $\\{x_i \\in \\mathbb{R}^d\\}_{i=1}^n$ 的集合和一个指定的簇数 $k$，$k$-均值的目标是通过最小化一个目标函数来找到簇分配 $z_i \\in \\{1, \\dots, k\\}$ 和质心 $\\{c_j \\in \\mathbb{R}^d\\}_{j=1}^k$ 来划分数据。\n\n标准的 $k$-均值算法最小化簇内平方和 (WCSS)，定义为：\n$$\nJ(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n \\left\\| x_i - c_{z_i} \\right\\|_2^2\n$$\n其中 $c_{z_i}$ 是点 $x_i$ 所属簇的质心。\n\n加权 $k$-均值变体通过为每个数据点引入非负权重 $\\{w_i\\}_{i=1}^n$ 来修改此目标函数：\n$$\nJ_w(\\{c_j\\}, \\{z_i\\}) = \\sum_{i=1}^n w_i \\left\\| x_i - c_{z_i} \\right\\|_2^2\n$$\n\n这两个目标函数通常使用一种称为 Lloyd 算法的迭代过程来最小化，该过程在两个步骤之间交替进行，直至收敛：\n\n1.  **分配步骤**：在质心 $\\{c_j\\}$ 固定的情况下，通过将每个数据点 $x_i$ 分配给最近的质心来最小化目标函数。对于标准和加权 $k$-均值，这一步是相同的，因为权重 $w_i  0$ 不影响为给定点 $x_i$ 选择最近的质心。分配规则是：\n    $$\n    z_i \\leftarrow \\underset{j \\in \\{1, \\dots, k\\}}{\\mathrm{argmin}} \\left\\| x_i - c_j \\right\\|_2^2\n    $$\n\n2.  **更新步骤**：在分配 $\\{z_i\\}$ 固定的情况下，通过将每个质心 $c_j$ 更新为其所分配点的新中心来最小化目标函数。令 $C_j = \\{i \\mid z_i = j\\}$ 为分配给簇 $j$ 的点的索引集合。\n    *   对于**标准 $k$-均值**，我们最小化关于 $c_j$ 的 $J_j = \\sum_{i \\in C_j} \\left\\| x_i - c_j \\right\\|_2^2$。将梯度设为零，$\\nabla_{c_j} J_j = \\sum_{i \\in C_j} -2(x_i - c_j) = 0$，得出更新规则：\n        $$\n        c_j \\leftarrow \\frac{1}{|C_j|} \\sum_{i \\in C_j} x_i\n        $$\n        新的质心是簇中各点的均值。\n    *   对于**加权 $k$-均值**，我们最小化 $J_{w,j} = \\sum_{i \\in C_j} w_i \\left\\| x_i - c_j \\right\\|_2^2$。将梯度设为零，$\\nabla_{c_j} J_{w,j} = \\sum_{i \\in C_j} -2 w_i (x_i - c_j) = 0$，得出更新规则：\n        $$\n        c_j \\leftarrow \\frac{\\sum_{i \\in C_j} w_i x_i}{\\sum_{i \\in C_j} w_i}\n        $$\n        新的质心是簇中各点的加权均值。\n\n为减轻对初始质心位置的敏感性，使用 **$k$-means++** 算法进行初始化。它依次选择初始质心，每个后续质心的选择概率与该点到最近的现有质心的平方距离成正比。这种初始化方法倾向于将质心放置在彼此远离的位置，这通常会带来更好、更一致的结果。迭代过程持续进行，直到质心位置的移动低于某个容差（例如，`if np.sum((new_centroids - centroids)**2)  tol:`），或者达到最大迭代次数。\n\n### 合成数据与评估\n\n分析是在从两个各向同性高斯分布 $\\mathcal{N}(\\mu, \\sigma^2 I)$ 的混合中生成的二维合成数据上进行的。我们在数据生成和 $k$-means++ 初始化中都使用固定的随机种子以保证可复现性。\n\n为了量化算法的行为，我们使用数据生成过程中的真实标签。\n- 首先通过寻找欧几里得距离中最近的真实中心，将学习到的簇质心（带有任意标签 $1, \\dots, k$）映射到真实簇中心（$\\mu_s, \\mu_\\ell$）。\n- **偏差分数** $B$ 定义为小簇错分率与大簇错分率之差：\n  $$\n  B = \\frac{\\text{被错分的小簇点数}}{n_s} - \\frac{\\text{被错分的大簇点数}}{n_\\ell}\n  $$\n  正分表示存在对小簇不利的偏差。\n- 当 $k  2$ 时，大簇的**重复计数** $D$ 是指被映射到大簇真实中心的学习到的质心数量。该指标量化了 $k$-均值将额外质心分配给更大、更分散的簇的趋势。\n\n具体的测试案例将展示簇重叠、极端的尺寸差异以及反尺寸加权如何影响这些指标，从而对 $k$-均值算法的行为提供定量观察。实现将在 Python 中进行，使用 `numpy` 进行数值计算，使用 `scipy` 进行高效的距离计算。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef generate_data(ns, nl, mu_s, mu_l, sigma_s, sigma_l, seed):\n    \"\"\"Generates a 2D dataset from two isotropic Gaussian distributions.\"\"\"\n    rng = np.random.default_rng(seed)\n    dim = len(mu_s)\n    \n    small_cluster = rng.multivariate_normal(mu_s, np.eye(dim) * sigma_s**2, size=ns)\n    large_cluster = rng.multivariate_normal(mu_l, np.eye(dim) * sigma_l**2, size=nl)\n    \n    X = np.vstack([small_cluster, large_cluster])\n    # True labels: 0 for small cluster, 1 for large cluster\n    y_true = np.array([0] * ns + [1] * nl) \n    true_centers = np.array([mu_s, mu_l])\n    \n    return X, y_true, true_centers\n\ndef kmeans_plusplus_init(X, k, seed):\n    \"\"\"Initializes k centroids using the k-means++ algorithm.\"\"\"\n    rng = np.random.default_rng(seed)\n    n_samples, n_features = X.shape\n    centroids = np.empty((k, n_features))\n    \n    # 1. Choose first centroid uniformly at random from data points.\n    centroids[0] = X[rng.choice(n_samples)]\n    \n    # For subsequent centroids\n    for i in range(1, k):\n        # 2. Calculate squared distances to the nearest existing centroid.\n        sq_dists = cdist(X, centroids[:i, :], 'sqeuclidean')\n        min_sq_dists = np.min(sq_dists, axis=1)\n        \n        # 3. Choose the next centroid with probability proportional to D(x)^2.\n        if np.sum(min_sq_dists) > 0:\n            probs = min_sq_dists / np.sum(min_sq_dists)\n            next_idx = rng.choice(n_samples, p=probs)\n        else:\n            # Handle cases where all points are duplicates or already chosen.\n            next_idx = rng.choice(n_samples)\n            \n        centroids[i] = X[next_idx]\n        \n    return centroids\n\ndef perform_kmeans(X, k, seed, weights=None, max_iter=100, tol=1e-6):\n    \"\"\"Performs standard or weighted k-means clustering.\"\"\"\n    centroids = kmeans_plusplus_init(X, k, seed)\n    \n    for i in range(max_iter):\n        # Assignment step: find the closest centroid for each point.\n        sq_dists = cdist(X, centroids, 'sqeuclidean')\n        assignments = np.argmin(sq_dists, axis=1)\n        \n        new_centroids = np.copy(centroids)\n        # Update step: recompute centroids based on new assignments.\n        for j in range(k):\n            assigned_points_mask = (assignments == j)\n            if np.any(assigned_points_mask):\n                if weights is None:\n                    # Standard k-means: centroid is the mean.\n                    new_centroids[j] = np.mean(X[assigned_points_mask], axis=0)\n                else:\n                    # Weighted k-means: centroid is the weighted mean.\n                    assigned_weights = weights[assigned_points_mask]\n                    new_centroids[j] = np.average(X[assigned_points_mask], axis=0, weights=assigned_weights)\n        \n        # Check for convergence: if centroids stop moving.\n        if np.sum((new_centroids - centroids)**2)  tol:\n            break\n            \n        centroids = new_centroids\n        \n    return centroids, assignments\n\ndef map_centroids_to_true_centers(centroids, true_centers):\n    \"\"\"Maps each learned centroid to the index of the nearest true center.\"\"\"\n    dists = cdist(centroids, true_centers)\n    mapping = np.argmin(dists, axis=1)\n    return mapping\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'ns': 60, 'nl': 600, 'mu_s': np.array([-5.0, 0.0]), 'mu_l': np.array([5.0, 0.0]), 'sigma_s': 1.0, 'sigma_l': 1.0, 'k': 2, 'seed': 42, 'type': 'bias'},\n        {'ns': 60, 'nl': 600, 'mu_s': np.array([0.1, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 1.0, 'sigma_l': 1.0, 'k': 2, 'seed': 43, 'type': 'bias'},\n        {'ns': 50, 'nl': 5000, 'mu_s': np.array([3.0, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 0.3, 'sigma_l': 1.5, 'k': 3, 'seed': 44, 'type': 'duplication_std'},\n        {'ns': 50, 'nl': 5000, 'mu_s': np.array([3.0, 0.0]), 'mu_l': np.array([0.0, 0.0]), 'sigma_s': 0.3, 'sigma_l': 1.5, 'k': 3, 'seed': 44, 'type': 'duplication_weighted'}\n    ]\n\n    results = []\n\n    for params in test_cases:\n        X, y_true, true_centers = generate_data(\n            params['ns'], params['nl'], \n            params['mu_s'], params['mu_l'],\n            params['sigma_s'], params['sigma_l'],\n            params['seed']\n        )\n        \n        weights = None\n        if params['type'] == 'duplication_weighted':\n            weights = np.zeros_like(y_true, dtype=float)\n            # small cluster points have label 0, large cluster points have label 1\n            weights[y_true == 0] = 1.0 / params['ns']\n            weights[y_true == 1] = 1.0 / params['nl']\n\n        centroids, assignments = perform_kmeans(X, params['k'], params['seed'], weights=weights)\n\n        centroid_to_true_map = map_centroids_to_true_centers(centroids, true_centers)\n\n        if params['type'] == 'bias':\n            # Map each point's assignment to its effective true cluster label\n            mapped_assignments = centroid_to_true_map[assignments]\n            \n            # Small cluster has label 0, large cluster has label 1\n            # Misassigned small point: true label is 0, assigned to centroid mapped to 1\n            misassigned_small_mask = (y_true == 0)  (mapped_assignments == 1)\n            # Misassigned large point: true label is 1, assigned to centroid mapped to 0\n            misassigned_large_mask = (y_true == 1)  (mapped_assignments == 0)\n            \n            num_misassigned_small = np.sum(misassigned_small_mask)\n            num_misassigned_large = np.sum(misassigned_large_mask)\n            \n            ns, nl = params['ns'], params['nl']\n            \n            bias_score = (num_misassigned_small / ns) - (num_misassigned_large / nl)\n            results.append(f\"{bias_score:.3f}\")\n\n        elif 'duplication' in params['type']:\n            # Large cluster has true index 1\n            large_cluster_true_index = 1\n            duplication_count = np.sum(centroid_to_true_map == large_cluster_true_index)\n            results.append(str(duplication_count))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3107780"}]}