{"hands_on_practices": [{"introduction": "神经网络的强大能力源于其将简单的非线性函数（即激活函数）组合成复杂函数的能力。这个练习将通过动手实践来具体展示这一原理。你将学习如何仅使用最基础的修正线性单元（ReLU）来精确构建绝对值函数，并进一步构造一个分段线性网络来近似更复杂的多项式函数 $|x|^3$。通过这个过程，你将为理解神经网络的通用近似能力建立起关键的直觉。[@problem_id:3094549]", "problem": "在计算科学和机器学习中，一个常见的激活函数是修正线性单元 (ReLU)，对于标量输入 $z$ 定义为 $\\mathrm{ReLU}(z) = \\max\\{0, z\\}$。考虑绝对值函数 $f(x) = |x|$ 及其推广形式 $|x|^{p}$，其中指数 $p \\geq 1$。\n\n您将通过复合激活函数和线性层来构建一个小网络，在一个有界区间上近似 $|x|^{p}$，此过程需从第一性原理和经过充分检验的事实出发。\n\n(a) 仅使用修正线性单元 (ReLU) 的定义，推导一个使用两个应用于标量输入 $x$ 的 ReLU 单元来计算 $|x|$ 的表达式。\n\n(b) 令 $p = 3$ 并定义函数 $f(x) = |x|^{3}$ 于区间 $[-1, 1]$ 上。考虑 $[0, 1]$ 上的均匀网格，由整数 $k = 0, 1, \\dots, N$ 的 $t_{k} = k/N$ 给出，其中 $N \\geq 2$ 是一个整数。构建一个双层网络，该网络首先使用您在 (a) 部分的结果将 $x$ 映射到 $t = |x|$，然后输出一个分段线性函数 $g_{N}(x)$，该函数在采样点 $t_k$ 处与 $t^3$ 一致，并且在每个子区间 $[t_{k}, t_{k+1}]$ 上是线性的。推导 $g_{N}(x)$ 的显式解析表达式，仅使用 $x$ 的代数组合以及 ReLU 与 $x$ 的仿射函数的复合。您的推导必须从分段线性插值的定义（即连续采样点之间的直线插值）和 ReLU 的定义出发；不要假设任何预先给定的分段线性函数表示形式。\n\n(c) 使用带有拉格朗日余项形式的泰勒定理以及凸、二阶连续可微函数的性质，推导近似误差 $\\sup_{x \\in [-1, 1]} |g_{N}(x) - |x|^{3}|$ 的一个一致上界，该上界仅是 $N$ 的函数。将最终上界表示为关于 $N$ 的单个闭式解析表达式。无需进行四舍五入。", "solution": "在尝试给出解答之前，将根据所需标准对问题进行验证。\n\n### 第 1 步：提取已知条件\n- 修正线性单元 (ReLU) 的定义：对于标量输入 $z$，$\\mathrm{ReLU}(z) = \\max\\{0, z\\}$。\n- 目标函数：$f(x) = |x|$ 及其推广形式 $|x|^{p}$，其中 $p \\geq 1$。\n- (a) 部分：使用两个 ReLU 单元推导 $|x|$ 的表达式。\n- (b) 部分：\n    - 设 $p=3$，则函数为 $f(x)=|x|^3$，定义在区间 $[-1, 1]$ 上。\n    - $[0, 1]$ 上的均匀网格定义为 $t_{k} = k/N$，其中整数 $k = 0, 1, \\dots, N$，且 $N \\geq 2$。\n    - 构建一个双层网络，该网络将 $x \\to t = |x|$ 映射，然后用分段线性函数 $g_{N}(x)$ 近似 $t^3$。\n    - 对于 $t=|x|$，$g_{N}(x)$ 必须在采样点 $t_k$ 处与 $t^3$ 一致。\n    - $g_{N}(x)$ 在每个子区间 $[t_k, t_{k+1}]$ 上必须是线性的。\n    - $g_N(x)$ 的解析表达式的推导必须仅使用 $x$ 的代数组合以及 ReLU 与 $x$ 的仿射函数的复合，并从分段线性插值的第一性原理和 ReLU 的定义出发。\n- (c) 部分：\n    - 推导近似误差 $\\sup_{x \\in [-1, 1]} |g_{N}(x) - |x|^{3}|$ 的一个一致上界。\n    - 推导必须使用带有拉格朗日余项形式的泰勒定理以及凸、二阶连续可微函数的性质。\n    - 最终上界必须是关于 $N$ 的单个闭式解析表达式。\n\n### 第 2 步：使用提取的已知条件进行验证\n对问题的有效性进行评估：\n- **科学性**：该问题基于数值分析、逼近论和神经网络理论（特别是使用 ReLU 单元的函数表示）中的基本概念。所有概念都是标准的，并且在数学上是严谨的。\n- **适定性**：问题的每个部分都要求进行具有清晰约束条件的特定推导或构建。目标是明确的，预期每个部分都有唯一的解。\n- **客观性**：该问题以精确、形式化的数学语言陈述，不含任何主观性或意见。\n- **完备性与一致性**：所有必要的定义（ReLU、网格点）、约束条件（区间、p 的值、$N \\ge 2$）和所需方法（泰勒定理）均已提供。问题的各个部分在逻辑上是顺序的且一致的。\n\n### 第 3 步：结论与行动\n该问题是**有效的**。这是一个计算科学领域中适定的、有科学依据的问题。将提供完整的解答。\n\n### 解答推导\n\n**(a) 使用 ReLU 单元表示 $|x|$ 的表达式**\n\n绝对值函数定义为如果 $x \\geq 0$ 则 $|x| = x$，如果 $x  0$ 则 $|x| = -x$。我们被要求使用两个修正线性单元来表示它，其定义为 $\\mathrm{ReLU}(z) = \\max\\{0, z\\}$。\n\n考虑 $\\mathrm{ReLU}(x)$ 和 $\\mathrm{ReLU}(-x)$ 的和：\n$\\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x) = \\max\\{0, x\\} + \\max\\{0, -x\\}$。\n\n我们分两种情况分析这个表达式：\n1.  如果 $x \\geq 0$，则 $-x \\leq 0$。表达式变为 $x + 0 = x$。因为 $x \\geq 0$，这等于 $|x|$。\n2.  如果 $x  0$，则 $-x > 0$。表达式变为 $0 + (-x) = -x$。因为 $x  0$，这等于 $|x|$。\n\n在两种情况下，表达式 $\\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)$ 都等于 $|x|$。因此，所求表达式为：\n$$|x| = \\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)$$\n这个构造使用两个 ReLU 单元，一个应用于 $x$，另一个应用于 $-x$。\n\n**(b) $|x|^3$ 的分段线性近似**\n\n我们想要构造一个函数 $g_N(x)$，作为 $|x|^3$ 在 $[-1, 1]$ 上的分段线性近似。构造分两个阶段进行。首先，输入 $x$ 被映射到 $t = |x| \\in [0, 1]$。其次，一个函数 $L(t)$ 在 $[0,1]$ 上近似 $h(t) = t^3$。因此，$g_N(x) = L(|x|)$。\n\n函数 $L(t)$ 定义为在网格点 $t_k = k/N$（其中 $k=0, 1, \\dots, N$）处对 $h(t)=t^3$ 进行插值的分段线性函数。\n$L(t)$ 在网格点处的值为 $L(t_k) = h(t_k) = (k/N)^3$。\n在每个子区间 $[t_k, t_{k+1}]$（其中 $k=0, \\dots, N-1$）上，$L(t)$ 是一条直线。这条线的斜率，记为 $m_k$，是：\n$$m_k = \\frac{L(t_{k+1}) - L(t_k)}{t_{k+1} - t_k} = \\frac{((k+1)/N)^3 - (k/N)^3}{(k+1)/N - k/N} = \\frac{\\frac{1}{N^3}((k+1)^3 - k^3)}{\\frac{1}{N}}$$\n使用展开式 $(k+1)^3 = k^3 + 3k^2 + 3k + 1$，我们有 $(k+1)^3 - k^3 = 3k^2 + 3k + 1$。\n因此斜率为：\n$$m_k = \\frac{3k^2 + 3k + 1}{N^2}$$\n一个连续的分段线性函数可以表示为缩放和平移的 ReLU 函数之和。该构造基于对每个断点处斜率变化的求和。设 $L(t)$ 从 $t=0$ 开始定义。我们有 $L(0) = t_0^3 = 0$。对于 $t \\in [t_0, t_1]$，初始斜率为 $m_0$。在每个后续的断点 $t_k$ 处，斜率从 $m_{k-1}$ 变为 $m_k$。\n函数 $L(t)$可以写成：\n$$L(t) = L(0) + m_0 t + \\sum_{k=1}^{N-1} (m_k - m_{k-1}) \\mathrm{ReLU}(t - t_k)$$\n因为 $L(0)=0$，且初始斜率为 $m_0 = \\frac{3(0)^2 + 3(0) + 1}{N^2} = \\frac{1}{N^2}$，我们需要计算 $k=1, \\dots, N-1$ 时斜率的变化量 $\\Delta m_k = m_k - m_{k-1}$：\n$$\\Delta m_k = \\frac{3k^2 + 3k + 1}{N^2} - \\frac{3(k-1)^2 + 3(k-1) + 1}{N^2}$$\n$$= \\frac{1}{N^2} \\left[ (3k^2 + 3k + 1) - (3(k^2 - 2k + 1) + 3k - 3 + 1) \\right]$$\n$$= \\frac{1}{N^2} \\left[ (3k^2 + 3k + 1) - (3k^2 - 6k + 3 + 3k - 2) \\right] = \\frac{1}{N^2} \\left[ (3k^2 + 3k + 1) - (3k^2 - 3k + 1) \\right] = \\frac{6k}{N^2}$$\n将斜率和斜率变化量代回到 $L(t)$ 的表达式中：\n$$L(t) = \\frac{1}{N^2} t + \\sum_{k=1}^{N-1} \\frac{6k}{N^2} \\mathrm{ReLU}(t - t_k)$$\n现在我们必须将 $g_N(x) = L(|x|)$ 表示为所要求的形式。我们代入 $t=|x|$：\n$$g_N(x) = \\frac{1}{N^2} |x| + \\sum_{k=1}^{N-1} \\frac{6k}{N^2} \\mathrm{ReLU}(|x| - \\frac{k}{N})$$\n为了满足表达式只使用 ReLU 与 $x$ 的仿射函数的复合这一条件，我们使用从第一性原理推导或验证的恒等式。从 (a) 部分可知，$|x| = \\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)$。\n我们还需要一个对于某个常数 $c \\geq 0$ 的 $\\mathrm{ReLU}(|x| - c)$ 的表达式。我们提出 $\\mathrm{ReLU}(|x|-c) = \\mathrm{ReLU}(x-c) + \\mathrm{ReLU}(-x-c)$。\n我们来验证一下。如果 $x \\geq c$，右边是 $(x-c)+0 = x-c$，左边是 $\\mathrm{ReLU}(x-c) = x-c$。如果 $0 \\leq x  c$，右边是 $0+0=0$，左边是 $\\mathrm{ReLU}(x-c)=0$。根据对 $x0$ 的对称性，该恒等式对所有 $x$ 成立。\n\n将这些恒等式代入 $g_N(x)$ 的表达式中：\n$$g_N(x) = \\frac{1}{N^2}(\\mathrm{ReLU}(x) + \\mathrm{ReLU}(-x)) + \\sum_{k=1}^{N-1} \\frac{6k}{N^2} \\left(\\mathrm{ReLU}\\left(x - \\frac{k}{N}\\right) + \\mathrm{ReLU}\\left(-x - \\frac{k}{N}\\right)\\right)$$\n这就是分段线性近似所要求的解析表达式。\n\n**(c) 近似误差的一致上界**\n\n我们需要找到 $\\sup_{x \\in [-1, 1]} |g_N(x) - |x|^3|$ 的一个上界。\n设 $h(t) = t^3$ 且 $t = |x|$。误差为 $\\sup_{t \\in [0, 1]} |L(t) - h(t)|$，其中 $L(t)$ 是 $h(t)$ 的分段线性插值函数。\n我们在单个子区间 $[t_k, t_{k+1}]$ 上分析误差。根据线性插值的误差公式（该公式可由带拉格朗日余项的泰勒定理推导），对于任意 $t \\in (t_k, t_{k+1})$，存在某个 $\\xi \\in (t_k, t_{k+1})$，使得误差 $E(t)=h(t)-L(t)$ 为：\n$$E(t) = \\frac{h''(\\xi)}{2!} (t - t_k)(t - t_{k+1})$$\n为了找到一致上界，我们必须在所有 $t \\in [0, 1]$ 上最大化此表达式的绝对值。\n首先，我们找到 $t \\in [t_k, t_{k+1}]$ 时 $|(t - t_k)(t - t_{k+1})|$ 的最大值。这个二次项在区间的中点 $t = (t_k + t_{k+1})/2$ 处取得最大值。区间长度为 $t_{k+1} - t_k = 1/N$。在中点处，值为 $-(\\frac{1}{2N})(\\frac{1}{2N}) = -\\frac{1}{4N^2}$。因此，最大绝对值为 $\\frac{1}{4N^2}$。\n接下来，我们对二阶导数项 $h''(\\xi)$ 进行界定。函数是 $h(t) = t^3$。\n$$h'(t) = 3t^2$$\n$$h''(t) = 6t$$\n问题陈述要求使用凸函数的性质。对于 $t \\in [0, 1]$，$h''(t) = 6t \\geq 0$，所以 $h(t)$ 是一个凸函数。此外，$h''(t)$ 是 $t$ 的一个单调递增函数。\n在区间 $[t_k, t_{k+1}]$ 上，$h''(\\xi)$ 的最大值在右端点处取得：\n$$\\sup_{\\xi \\in [t_k, t_{k+1}]} |h''(\\xi)| = h''(t_{k+1}) = 6t_{k+1} = 6 \\frac{k+1}{N}$$\n结合这些结果，子区间 $[t_k, t_{k+1}]$ 上的最大误差有如下界限：\n$$\\sup_{t \\in [t_k, t_{k+1}]} |E(t)| \\leq \\frac{1}{2} \\left( \\sup_{\\xi \\in [t_k, t_{k+1}]} |h''(\\xi)| \\right) \\left( \\sup_{t \\in [t_k, t_{k+1}]} |(t-t_k)(t-t_{k+1})| \\right)$$\n$$\\sup_{t \\in [t_k, t_{k+1}]} |E(t)| \\leq \\frac{1}{2} \\left( 6 \\frac{k+1}{N} \\right) \\left( \\frac{1}{4N^2} \\right) = \\frac{3(k+1)}{4N^3}$$\n为了找到整个区间 $[0, 1]$ 上的一致误差界，我们必须找到此局部界在所有子区间上的最大值，即对 $k=0, 1, \\dots, N-1$。\n$$\\sup_{t \\in [0, 1]} |E(t)| = \\max_{k \\in \\{0, \\dots, N-1\\}} \\left( \\sup_{t \\in [t_k, t_{k+1}]} |E(t)| \\right) \\leq \\max_{k \\in \\{0, \\dots, N-1\\}} \\frac{3(k+1)}{4N^3}$$\n表达式 $\\frac{3(k+1)}{4N^3}$ 是 $k$ 的一个增函数。其在 $k \\in \\{0, \\dots, N-1\\}$ 上的最大值出现在 $k = N-1$ 处。\n代入 $k = N-1$ 给出一致上界：\n$$\\text{误差} \\leq \\frac{3((N-1)+1)}{4N^3} = \\frac{3N}{4N^3} = \\frac{3}{4N^2}$$\n这就是作为 $N$ 的函数的近似误差一致上界的最终闭式解析表达式。", "answer": "$$\n\\boxed{\\frac{3}{4N^2}}\n$$", "id": "3094549"}, {"introduction": "一个函数的数学定义并不能完全说明其在实际应用中的表现；代码的实现方式会对性能产生巨大影响，尤其是在如图形处理器（GPU）这样的并行硬件上。这个练习将探讨 ReLU 函数的两种经典实现方式——一种使用 `if-else` 分支，另一种使用无分支的算术技巧——之间的性能权衡。通过这个过程，你将揭示硬件层面的概念，如线程束分化（warp divergence），并理解它们如何影响计算吞吐量。[@problem_id:3094485]", "problem": "您将研究整流线性单元 (ReLU) 激活函数，并在单指令多线程 (SIMT) 执行模型（一种图形处理单元 (GPU) 的简化模型）下比较两种计算实现方式。您必须从第一性原理出发进行推理，并实现一个完整的程序，为指定的测试套件计算理论上预测的吞吐量值（以每秒浮点运算次数 FLOP/s 为单位）。您的程序不得依赖任何硬件特定功能，且必须在中央处理器 (CPU) 上运行，但必须反映此处描述的 SIMT 规则。\n\n对于标量输入 $x$，ReLU 激活函数为 $f(x)=\\max(0,x)$。一种常见的 ReLU 无分支实现是使用绝对值的 $g(x)=\\tfrac{1}{2}(x+\\lvert x\\rvert)$。请从绝对值的基本定义，即 $\\lvert x\\rvert=\\begin{cases}x,  \\text{if } x \\ge 0\\\\-x,  \\text{if } x  0\\end{cases}$，以及 ReLU 的定义 $f(x)=\\max(0,x)$ 出发，推导出 $f(x)=\\tfrac{1}{2}(x+\\lvert x\\rvert)$ 对所有实数 $x$ 均成立。\n\n使用成熟的锁步执行抽象，按如下方式对 SIMT 执行进行建模。一个 SIMT 线程束 (warp) 包含 $w$ 个通道 (lane)，这些通道以锁步方式对不同数据执行相同指令。考虑处理 $N$ 个独立元素，这些元素被划分为 $\\lceil N/w\\rceil$ 个大小为 $w$ 的连续线程束，最后一个线程束可能被部分填充。假设每个元素为正的概率独立地为 $\\pi$，为非正的概率为 $1-\\pi$。当遇到数据依赖分支时（例如，ReLU 的 if-else 实现），如果一个线程束内至少有一个通道采用真路径，且至少有一个通道采用假路径，则该线程束会发生分化。在这种情况下，该线程束会串行化执行路径：它首先执行一个路径，此时只有相应的通道是激活的，然后执行另一个路径，此时其他通道是激活的。\n\n采用以下成本模型，这是一种用于吞吐量分析的常规指令级核算模型。\n- 为每个基本算术运算（加、减、乘、除和绝对值）计为一次浮点运算 (FLOP)；这建立了一个通用单位。为对整体有效工作进行基准测试，每个求值的 ReLU 元素计为一次名义 FLOP。\n- 一条线程束范围的指令恰好消耗一个周期，无论该指令有多少通道是激活的。\n- 对于无分支实现 $g(x)=\\tfrac{1}{2}(x+\\lvert x\\rvert)$，每个元素恰好需要 3 次算术运算：一次绝对值、一次加法和一次乘以 $\\tfrac{1}{2}$。因此，每个完全填充的、包含 $w$ 个元素的线程束，处理一批 $w$ 个元素的工作恰好需要 3 个周期。\n- 对于 if-else 实现 $h(x)=\\begin{cases}x,  \\text{if } x  0\\\\0,  \\text{otherwise}\\end{cases}$，假设有一条线程束范围的比较指令来设置谓词（成本为 1 个周期），以及每条可执行路径有一条谓词化移动或存储指令（每条被调度的路径成本为 1 个周期）。因此，一个具有一致分支结果（所有 $w$ 个通道都为真或所有 $w$ 个通道都为假）的线程束需要 2 个周期，而一个具有混合结果的线程束需要 3 个周期，因为两条路径都会被调度。在输入独立的假设下，推导出每个线程束的期望周期数，作为 $w$ 和 $\\pi$ 的函数。\n\n将处理 $N$ 个元素在固定时钟频率 $F$（单位：周期/秒）下的理论吞吐量（单位：每秒浮点运算次数 FLOP/s）定义为\n$$\nT \\;=\\; \\frac{\\text{有效 FLOPs}}{\\text{时间}} \\;=\\; \\frac{N}{\\frac{\\text{总周期数}}{F}} \\;=\\; \\frac{N\\,F}{\\lceil N/w\\rceil\\cdot C},\n$$\n其中 $C$ 是每个线程束批次的周期数，对于无分支实现，它是一个常数 3；对于 if-else 实现，它是您推导出的期望值。\n\n任务：\n- 从上述基本定义出发，推导出恒等式 $f(x)=\\tfrac{1}{2}(x+\\lvert x\\rvert)$。\n- 从 SIMT 模型和独立性假设出发，推导出 if-else 实现中每个线程束的期望周期数 $C$，以 $w$ 和 $\\pi$ 表示。\n- 实现一个完整的、可运行的程序，该程序使用上面的 $T$ 公式，为下面测试套件中的每种情况计算无分支实现和 if-else 实现的理论吞吐量。所有输出必须以 FLOP/s 表示。将每个吞吐量值四舍五入到最近的整数。\n\n测试套件：\n- 情况 A：$N=1048576$，$w=32$，$\\pi=0.5$，$F=1.0\\times 10^9$。\n- 情况 B：$N=1024$，$w=32$，$\\pi=1.0$，$F=1.0\\times 10^9$。\n- 情况 C：$N=1024$，$w=32$，$\\pi=0.0$，$F=1.0\\times 10^9$。\n- 情况 D：$N=1000$，$w=32$，$\\pi=0.1$，$F=1.0\\times 10^9$。\n- 情况 E：$N=31$，$w=32$，$\\pi=0.5$，$F=1.0\\times 10^9$。\n- 情况 F：$N=100$，$w=1$，$\\pi=0.5$，$F=1.0\\times 10^9$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。对于每个测试用例，首先输出无分支实现的吞吐量，然后输出 if-else 实现的吞吐量，两者都以 FLOP/s 为单位，并四舍五入到最近的整数。因此，整体输出格式为\n$[T_{\\text{branchless},A},T_{\\text{if-else},A},T_{\\text{branchless},B},T_{\\text{if-else},B},\\dots]$。", "solution": "该问题是有效的，因为它在计算科学和计算机体系结构原理方面有科学依据，问题本身是适定的，目标明确，且没有矛盾或歧义。\n\n### 第 1 部分：无分支 ReLU 恒等式的推导\n\n问题要求推导恒等式 $f(x) = g(x)$，其中 $f(x) = \\max(0, x)$ 是整流线性单元 (ReLU) 函数，而 $g(x) = \\frac{1}{2}(x + \\lvert x \\rvert)$ 是其无分支实现。推导从绝对值函数 $\\lvert x \\rvert$ 对任意实数 $x$ 的基本分段定义开始：\n$$\n\\lvert x \\rvert = \\begin{cases}\nx,    \\text{if } x \\ge 0 \\\\\n-x,   \\text{if } x  0\n\\end{cases}\n$$\n我们分两种不相交且穷尽的情况来分析该恒等式，这两种情况覆盖了所有实数 $x$。\n\n**情况 1：$x \\ge 0$**\n\n在这种情况下，左侧 (LHS) 的 ReLU 函数求值为：\n$$\nf(x) = \\max(0, x) = x\n$$\n右侧 (RHS) 的无分支形式使用 $x \\ge 0$ 时 $\\lvert x \\rvert = x$ 的定义求值为：\n$$\ng(x) = \\frac{1}{2}(x + \\lvert x \\rvert) = \\frac{1}{2}(x + x) = \\frac{1}{2}(2x) = x\n$$\n由于 LHS = RHS，该恒等式对 $x \\ge 0$ 成立。\n\n**情况 2：$x  0$**\n\n在这种情况下，左侧 (LHS) 求值为：\n$$\nf(x) = \\max(0, x) = 0\n$$\n右侧 (RHS) 使用 $x  0$ 时 $\\lvert x \\rvert = -x$ 的定义求值为：\n$$\ng(x) = \\frac{1}{2}(x + \\lvert x \\rvert) = \\frac{1}{2}(x + (-x)) = \\frac{1}{2}(0) = 0\n$$\n由于 LHS = RHS，该恒等式对 $x  0$ 成立。\n\n因为恒等式 $f(x) = g(x)$ 对所有可能的实数值 $x$（即 $x \\ge 0$ 和 $x  0$）都成立，所以推导完成。\n\n### 第 2 部分：If-Else 实现的期望周期数推导\n\n单指令多线程 (SIMT) 模型指定了 $w$ 个通道的线程束大小。对于 if-else 实现 $h(x)$，一个在 $w$ 个元素上执行的线程束，如果所有元素遵循相同的分支路径（一致的线程束），则需要 2 个周期；如果元素遵循不同的路径（分化的线程束），则需要 3 个周期。\n\n设 $\\pi$ 是输入元素 $x$ 为正 ($x > 0$) 的概率，$1-\\pi$ 是其为非正 ($x \\le 0$) 的概率。各输入是独立的。一个线程束是一致的，当且仅当发生以下两个互斥事件之一：\n1.  所有 $w$ 个元素都为正。此事件的概率为 $\\pi^w$。\n2.  所有 $w$ 个元素都为非正。此事件的概率为 $(1-\\pi)^w$。\n\n线程束是一致的总概率 $P(\\text{uniform})$ 是这两个事件概率的总和：\n$$\nP(\\text{uniform}) = \\pi^w + (1-\\pi)^w\n$$\n如果一个线程束不是一致的，它就是分化的。因此，分化的概率 $P(\\text{diverge})$ 是：\n$$\nP(\\text{diverge}) = 1 - P(\\text{uniform}) = 1 - (\\pi^w + (1-\\pi)^w)\n$$\n每个线程束的期望周期数 $C$（我们记为 $C_{\\text{if-else}}$）是一致线程束和分化线程束周期成本的加权平均值：\n$$\nC_{\\text{if-else}} = (2 \\text{ cycles}) \\cdot P(\\text{uniform}) + (3 \\text{ cycles}) \\cdot P(\\text{diverge})\n$$\n代入 $P(\\text{diverge})$ 的表达式：\n$$\nC_{\\text{if-else}} = 2 \\cdot P(\\text{uniform}) + 3 \\cdot (1 - P(\\text{uniform}))\n$$\n$$\nC_{\\text{if-else}} = 2 \\cdot P(\\text{uniform}) + 3 - 3 \\cdot P(\\text{uniform})\n$$\n$$\nC_{\\text{if-else}} = 3 - P(\\text{uniform})\n$$\n代入推导出的 $P(\\text{uniform})$ 表达式：\n$$\nC_{\\text{if-else}} = 3 - (\\pi^w + (1-\\pi)^w)\n$$\n这就是 if-else 实现所需期望周期数的公式。在 $\\pi=0$ 或 $\\pi=1$ 的特殊情况下，所有线程束都保证是一致的。对于 $\\pi=1$，$C_{\\text{if-else}} = 3 - (1^w + 0^w) = 3 - 1 = 2$。对于 $\\pi=0$，$C_{\\text{if-else}} = 3 - (0^w + 1^w) = 3 - 1 = 2$。如果 $w=1$，执行是串行的，因此线程束永远不会分化。该公式得出 $C_{\\text{if-else}} = 3 - (\\pi^1 + (1-\\pi)^1) = 3 - \\pi - 1 + \\pi = 2$，这正确地反映了非分化执行。\n\n### 第 3 部分：吞吐量计算\n\n理论吞吐量 $T$（单位：每秒浮点运算次数 FLOP/s）由以下公式给出：\n$$\nT = \\frac{N \\cdot F}{\\lceil N/w \\rceil \\cdot C}\n$$\n其中 $N$ 是元素数量，$F$ 是时钟频率，$w$ 是线程束大小，$C$ 是每个线程束的周期数。项 $\\lceil N/w \\rceil$ 表示处理所有 $N$ 个元素所需的总线程束数量。\n\n对于**无分支实现**，成本是恒定的：\n$$\nC_{\\text{branchless}} = 3\n$$\n因此吞吐量为：\n$$\nT_{\\text{branchless}} = \\frac{N \\cdot F}{\\lceil N/w \\rceil \\cdot 3}\n$$\n对于**if-else 实现**，成本是上面推导的期望值：\n$$\nC_{\\text{if-else}} = 3 - \\pi^w - (1-\\pi)^w\n$$\n吞吐量为：\n$$\nT_{\\text{if-else}} = \\frac{N \\cdot F}{\\lceil N/w \\rceil \\cdot (3 - \\pi^w - (1-\\pi)^w)}\n$$\n下面的程序为指定的测试套件实现了这些公式，并将最终的吞吐量值四舍五入到最近的整数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes theoretical throughputs for branchless and if-else ReLU realizations\n    under a simplified SIMT execution model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: N=1048576, w=32, pi=0.5, F=1.0e9\n        (1048576, 32, 0.5, 1.0e9),\n        # Case B: N=1024, w=32, pi=1.0, F=1.0e9\n        (1024, 32, 1.0, 1.0e9),\n        # Case C: N=1024, w=32, pi=0.0, F=1.0e9\n        (1024, 32, 0.0, 1.0e9),\n        # Case D: N=1000, w=32, pi=0.1, F=1.0e9\n        (1000, 32, 0.1, 1.0e9),\n        # Case E: N=31, w=32, pi=0.5, F=1.0e9\n        (31, 32, 0.5, 1.0e9),\n        # Case F: N=100, w=1, pi=0.5, F=1.0e9\n        (100, 1, 0.5, 1.0e9),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, w, pi, F = case\n\n        # Calculate the total number of warps required.\n        # np.ceil returns a float, which is fine for the division.\n        num_warps = np.ceil(N / w)\n\n        # -- Branchless Realization --\n        # Cost is constant at 3 cycles per warp.\n        C_branchless = 3.0\n        T_branchless = (N * F) / (num_warps * C_branchless)\n        results.append(round(T_branchless))\n\n        # -- If-Else Realization --\n        # Probability of a uniform warp (all true or all false).\n        p_uniform = pi**w + (1 - pi)**w\n        \n        # Expected cycles per warp is 3 - P(uniform).\n        C_ifelse = 3.0 - p_uniform\n        \n        T_ifelse = (N * F) / (num_warps * C_ifelse)\n        results.append(round(T_ifelse))\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3094485"}, {"introduction": "许多流行的激活函数（如 ReLU）并非处处可微，这给基于梯度的优化算法带来了挑战。本练习将深入探讨次梯度这一概念，让你通过模拟来观察在 ReLU 的“拐点”处选择不同的导数值会如何显著影响模型的训练轨迹。这个实践将帮助你理解为什么优化器有时会停滞不前，以及看似微小的理论选择在实践中会产生怎样的后果。[@problem_id:3171901]", "problem": "给定修正线性单元（ReLU）激活函数，其对于标量输入的定义为 $f(x) = \\max(0, x)$。在深度学习的反向传播中，导数 $f'(x)$ 用于链式法则的计算。函数 $f(x)$ 在除 $x=0$ 之外的所有点上都是可微的，在 $x=0$ 处，有效的次梯度集合是区间 $[0, 1]$。在 $x = 0$ 处选择一个次梯度，意味着选择一个值 $c \\in [0, 1]$，用于链式法则中遇到 $f'(0)$ 的任何地方。\n\n考虑在一个固定的数据集上训练一个带有一个 ReLU 单元的单参数模型。对于一个输入 $x$，模型的输出是 $\\hat{y}(x; a) = f(a x)$，其中 $a \\in \\mathbb{R}$ 是可训练的参数。目标输出为 $y(x) = f(x)$。训练目标是均方误差（MSE）\n$$\nL(a) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}(x_i; a) - y(x_i)\\right)^2,\n$$\n数据集固定为 $x \\in \\{-2, -1, 0, 1, 2\\}$，且对于每个输入，$y(x) = f(x)$。使用梯度下降法，学习率 $\\eta  0$，参数初始化 $a_0 \\in \\mathbb{R}$：\n$$\na_{t+1} = a_t - \\eta \\, \\nabla L(a_t),\n$$\n其中梯度 $\\nabla L(a)$ 必须通过链式法则计算，使用 $f$ 的导数定义如下：\n$$\nf'(z) = \\begin{cases}\n0,  z  0 \\\\\nc,  z = 0 \\\\\n1,  z  0\n\\end{cases}\n$$\n其中次梯度选择 $c \\in [0, 1]$ 仅在 $z = 0$ 时应用。对于任何迭代，如果存在某个 $i$ 使得 $a_t x_i = 0$，则对这些项一致地使用上述的 $f'(0) = c$。\n\n你的任务是实现一个程序，该程序：\n1. 如上定义，模拟对 $L(a)$ 的梯度下降。\n2. 在每一步使用链式法则和在 $z=0$ 处指定的次梯度选择来计算 $\\nabla L(a)$。\n3. 对每个测试用例，运行迭代直到 $L(a_t) \\le \\varepsilon$（收敛）或达到最大步数 $T_{\\max}$（不收敛）。\n4. 对每个测试用例，报告收敛所需的步数（整数，如果在 $T_{\\max}$ 内不收敛则使用 $-1$）、最终参数 $a_T$（浮点数）和最终损失 $L(a_T)$（浮点数）。\n\n在所有运行中使用以下固定常量：\n- 容差 $\\varepsilon = 10^{-6}$。\n- 最大步数 $T_{\\max} = 1000$。\n- 数据集 $\\{x_i\\}_{i=1}^{5} = \\{-2, -1, 0, 1, 2\\}$ 且 $y_i = f(x_i)$。\n\n测试套件参数集 $(c, \\eta, a_0)$：\n- 案例 1: $c = 0.0$, $\\eta = 0.1$, $a_0 = 0.0$（边界次梯度可能导致停滞）。\n- 案例 2: $c = 0.1$, $\\eta = 0.1$, $a_0 = 0.0$（拐点处的小次梯度）。\n- 案例 3: $c = 0.5$, $\\eta = 0.1$, $a_0 = 0.0$（中等次梯度）。\n- 案例 4: $c = 1.0$, $\\eta = 0.6$, $a_0 = 0.0$（学习率较大但理论上稳定）。\n- 案例 5: $c = 1.0$, $\\eta = 1.0$, $a_0 = 0.0$（稳定性边界）。\n\n你的程序应该生成单行输出，包含一个用方括号括起来的逗号分隔列表，其中每个元素是对应上述顺序的一个测试用例的三元组 $[s, a_T, L(a_T)]$。这里 $s$ 是收敛的整数步数（如果不收敛则为 $-1$），$a_T$ 是最终参数值，$L(a_T)$ 是最终损失。例如，一个有效的输出格式是\n$$\n[[s_1, a_{T,1}, L(a_{T,1})],[s_2, a_{T,2}, L(a_{T,2})],\\dots].\n$$\n不涉及物理单位或角度；所有量都是实数。", "solution": "该问题是有效的，因为它在科学上基于机器学习和优化的原理，问题定义良好、客观且自成体系。它描述了一个清晰的计算任务，基于诸如 ReLU 激活函数、均方误差损失和梯度下降等标准概念，包括对不可微点使用次梯度。\n\n问题的核心是模拟单参数模型 $\\hat{y}(x; a) = f(a x)$ 的训练，其中 $f$ 是 ReLU 函数 $f(z) = \\max(0, z)$。目标是找到参数 $a$ 以最小化给定数据集上的均方误差（MSE）损失 $L(a)$。\n\n首先，我们用数学方式定义问题的各个组成部分。\n模型输出为 $\\hat{y}(x_i; a) = f(a x_i)$。\n目标输出为 $y_i = f(x_i)$。\n数据集由 $\\{x_i\\}_{i=1}^{5} = \\{-2, -1, 0, 1, 2\\}$ 给出，对应的目标 $y_i$ 为 $\\{0, 0, 0, 1, 2\\}$。\n损失函数是 MSE：\n$$\nL(a) = \\frac{1}{n}\\sum_{i=1}^{n}\\left(\\hat{y}(x_i; a) - y_i\\right)^2 = \\frac{1}{5}\\sum_{i=1}^{5}\\left(f(a x_i) - f(x_i)\\right)^2\n$$\n\n优化使用梯度下降法进行，更新规则为：\n$$\na_{t+1} = a_t - \\eta \\, \\nabla L(a_t)\n$$\n其中 $a_t$ 是在步骤 $t$ 的参数值，$\\eta$ 是学习率，$\\nabla L(a_t)$ 是损失函数关于 $a$ 在 $a_t$ 处求得的梯度。\n\n为了计算梯度，我们对损失函数应用链式法则：\n$$\n\\nabla L(a) = \\frac{d L}{d a} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{d}{da} \\left(f(a x_i) - y_i\\right)^2\n$$\n对于和中的单项，其导数为：\n$$\n\\frac{d}{da} \\left(f(a x_i) - y_i\\right)^2 = 2 \\left(f(a x_i) - y_i\\right) \\cdot \\frac{d}{da}f(a x_i)\n$$\n再次对 $\\frac{d}{da}f(a x_i)$ 应用链式法则，令 $z_i = a x_i$：\n$$\n\\frac{d}{da}f(a x_i) = f'(z_i) \\cdot \\frac{dz_i}{da} = f'(a x_i) \\cdot x_i\n$$\n问题指定了 ReLU 函数的导数（或基于次梯度的导数）的形式：\n$$\nf'(z) = \\begin{cases}\n0,  z  0 \\\\\nc,  z = 0 \\\\\n1,  z  0\n\\end{cases}\n$$\n其中 $c \\in [0, 1]$ 是在不可微点 $z=0$ 处次梯度选择的给定常数。\n\n结合这些结果，梯度的完整表达式是：\n$$\n\\nabla L(a) = \\frac{2}{n}\\sum_{i=1}^{n} \\left(f(a x_i) - y_i\\right) \\cdot x_i \\cdot f'(a x_i)\n$$\n\n模拟过程是迭代进行的。从一个初始参数 $a_0$ 开始，我们进入一个最多 $T_{\\max} = 1000$ 步的循环。在每一步 $t = 0, 1, 2, \\dots$ 中：\n1. 计算当前损失 $L(a_t)$。\n2. 如果 $L(a_t) \\le \\varepsilon = 10^{-6}$，则过程已收敛。我们记录步数 $t$、最终参数 $a_t$ 和最终损失 $L(a_t)$，并终止当前测试用例的模拟。\n3. 如果在没有收敛的情况下达到了最大步数 $T_{\\max}$，则停止模拟。我们将步数记录为 $-1$，同时记录最终参数 $a_{T_{\\max}}$ 和损失 $L(a_{T_{\\max}})$。\n4. 如果没有收敛且未达到步数限制，则使用上述公式计算梯度 $\\nabla L(a_t)$，根据每个数据点 $a_t x_i$ 的符号来评估 $f'(a_t x_i)$。\n5. 参数更新为 $a_{t+1} = a_t - \\eta \\nabla L(a_t)$。\n\n最优参数值为 $a=1$，此时对于数据集中的所有 $x_i$，$\\hat{y}(x_i; 1) = f(x_i) = y_i$，且损失 $L(1) = 0$。梯度下降算法的行为关键取决于参数 $(c, \\eta, a_0)$。例如，当 $a_0=0$ 且 $c=0$ 时，初始梯度 $\\nabla L(0)$ 为零，导致参数永久停留在 $a=0$ 处，从而无法收敛。对于其他 $c  0$ 的值，参数将离开原点，并可以收敛到最优值 $a=1$。学习率 $\\eta$ 控制着这种收敛的稳定性和速度。对于 $\\eta=1.0$ 和 $c=1.0$，更新会导致参数在 0 和 2 之间振荡，永远不会收敛。\n\n实现将针对所提供的每个测试用例遵循此逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the gradient descent simulation problem for a set of test cases.\n    \"\"\"\n    # Fixed constants for all runs\n    epsilon = 1e-6\n    T_max = 1000\n    X = np.array([-2, -1, 0, 1, 2], dtype=float)\n    Y = np.maximum(0, X)\n\n    # Test suite parameter sets (c, eta, a_0)\n    test_cases = [\n        (0.0, 0.1, 0.0),\n        (0.1, 0.1, 0.0),\n        (0.5, 0.1, 0.0),\n        (1.0, 0.6, 0.0),\n        (1.0, 1.0, 0.0),\n    ]\n\n    results = []\n\n    for c, eta, a0 in test_cases:\n        a = float(a0)\n\n        # The simulation runs for t = 0, 1, ..., T_max steps.\n        # At each step t, we check for convergence at a_t.\n        # If not converged, we compute a_{t+1}.\n        for t in range(T_max + 1):\n            \n            # 1. Calculate loss L(a_t)\n            y_hat = np.maximum(0, a * X)\n            current_loss = np.mean((y_hat - Y)**2)\n\n            # 2. Check for convergence\n            if current_loss = epsilon:\n                results.append([t, a, current_loss])\n                break\n            \n            # 3. Check for non-convergence after T_max steps\n            # This is after T_max updates, at step T_max.\n            if t == T_max:\n                results.append([-1, a, current_loss])\n                break\n\n            # 4. Compute gradient\n            grad_sum = 0.0\n            n = len(X)\n            errors = y_hat - Y\n            \n            for i in range(n):\n                z_i = a * X[i]\n                \n                # Derivative of ReLU, f'(z), based on subgradient choice c\n                if z_i > 0:\n                    der_f = 1.0\n                elif z_i  0:\n                    der_f = 0.0\n                else:  # z_i == 0\n                    der_f = c\n                \n                grad_sum += errors[i] * X[i] * der_f\n            \n            grad = (2.0 / n) * grad_sum\n\n            # 5. Update parameter\n            a = a - eta * grad\n    \n    # Final print statement in the exact required format.\n    # The str() representation of a list includes spaces, e.g., '[1, 2.0, 3.0]'.\n    # Joining these with ',' results in '[1, 2.0, 3.0],[4, 5.0, 6.0]'.\n    # This matches the structure of the example.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3171901"}]}