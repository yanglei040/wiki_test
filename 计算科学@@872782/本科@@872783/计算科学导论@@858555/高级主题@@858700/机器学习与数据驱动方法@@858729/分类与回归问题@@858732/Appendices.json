{"hands_on_practices": [{"introduction": "在构建回归模型时，一个核心挑战是在模型复杂性与泛化能力之间找到最佳平衡点。过于简单的模型可能无法捕捉数据中的关键模式（欠拟合），而过于复杂的模型则可能学习到数据中的噪声，导致在未见过的数据上表现不佳（过拟合）。本练习将指导您通过对泛化差距（即验证误差与训练误差之差）进行建模，从而量化过拟合风险，并最终确定最优的多项式阶数。这个过程巧妙地结合了回归（拟合泛化差距随复杂度的变化趋势）与分类（从一组候选模型中选出最优模型），是处理偏差-方差权衡问题的实用方法 [@problem_id:3107026]。", "problem": "给定从拟合到同一数据集的 $d$ 阶多项式回归模型中得到的观测数据。对于每个测试用例，会提供一组阶数 $d \\in \\mathbb{N}$，以及对应的经验训练均方误差 (MSE) $E_{\\text{train}}(d)$ 和经验交叉验证均方误差 $E_{\\text{val}}(d)$。您的任务是通过对泛化差距 $g(d)$ 与 $d$ 进行回归来预测过拟合风险，并确定能够最小化预期泛化误差估计值的最优阶数。\n\n基本原理和定义：\n- 复杂度为 $d$ 时的泛化误差可以表示为 $E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$，其中 $g(d)$ 是泛化差距，定义为 $g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$。在实践中，$E_{\\text{test}}(d)$ 是未知的，我们使用交叉验证作为一种经过充分检验的替代方法，因此我们将差距估计为 $g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$。\n- 过拟合风险随模型复杂度增加的现象，体现在 $g(d)$ 作为 $d$ 的函数的斜率为正。\n\n算法要求：\n- 对于每个测试用例，计算每个观测阶数 $d_k$ 处的 $g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$。\n- 使用普通最小二乘法 (OLS) 将直线 $g(d) \\approx a + b\\,d$ 拟合到观测对 $\\{(d_k, g(d_k))\\}$。\n- 将每个观测阶数处的估计预期泛化误差构建为 $\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$。\n- 将最优阶数 $\\widehat{d}^{\\star}$ 确定为给定集合中能使 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小化的整数阶数。如果在数值容差范围内有多个阶数达到相同的最小值，则选择其中最小的阶数（偏好较低的复杂度）。\n- 每个测试用例报告两个量：差距与阶数回归的估计斜率 $\\widehat{b}$（四舍五入到4位小数），以及确定的最优阶数 $\\widehat{d}^{\\star}$（作为整数）。\n\n测试套件：\n- 测试用例 1：$d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$。\n- 测试用例 2：$d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$。\n- 测试用例 3：$d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$。\n- 测试用例 4：$d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$。\n- 测试用例 5：$d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$。\n\n输出规格：\n- 对于每个测试用例，输出一个双元素列表 $[\\widehat{b}, \\widehat{d}^{\\star}]$，其中 $\\widehat{b}$ 是四舍五入到4位小数的斜率，$\\widehat{d}^{\\star}$ 是一个整数。\n- 您的程序应生成单行输出，其中包含一个列表，该列表按顺序汇总了每个测试用例的输出，且不含空格。每个元素本身必须是如上所述的双元素列表。最终打印输出必须只有一行。\n\n不涉及物理单位。所有角度（如有）在此处均不相关。不需要百分比。\n\n您的程序必须是一个完整、可运行的程序，它在内部定义了上述测试用例，并生成指定的单行输出。", "solution": "### 第 1 步：提取已知信息\n- **模型**：阶数为 $d \\in \\mathbb{N}$ 的多项式回归模型。\n- **每个测试用例的数据**：一组阶数 $d_k$，对应的训练均方误差 (MSE) $E_{\\text{train}}(d_k)$，以及交叉验证均方误差 $E_{\\text{val}}(d_k)$。\n- **定义**：\n    - 泛化误差：$E_{\\text{gen}}(d) = E_{\\text{train}}(d) + g(d)$。\n    - 泛化差距：$g(d) = E_{\\text{test}}(d) - E_{\\text{train}}(d)$。\n    - 经验泛化差距估计：$g(d) \\approx E_{\\text{val}}(d) - E_{\\text{train}}(d)$。\n- **算法要求**：\n    1.  计算每个阶数的经验差距：$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$。\n    2.  使用普通最小二乘法 (OLS) 将直线 $g(d) \\approx a + b\\,d$ 拟合到数据点 $\\{(d_k, g(d_k))\\}$，以找到估计的截距 $\\widehat{a}$ 和斜率 $\\widehat{b}$。\n    3.  估计每个阶数处的预期泛化误差：$\\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}\\,d_k$。\n    4.  将最优阶数 $\\widehat{d}^{\\star}$ 确定为给定集合中能使 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小化的整数阶数。平局打破规则是如果多个阶数产生相同的最小值，则选择最小的阶数。\n    5.  报告斜率 $\\widehat{b}$（四舍五入到4位小数）和整数形式的最优阶数 $\\widehat{d}^{\\star}$。\n- **测试套件**：\n    - **测试用例 1**：$d = [1,2,3,4,5]$, $E_{\\text{train}} = [9.0,6.0,4.5,4.0,3.8]$, $E_{\\text{val}} = [10.0,7.0,5.0,5.2,6.0]$。\n    - **测试用例 2**：$d = [1,2,3,4]$, $E_{\\text{train}} = [5.0,4.2,3.6,3.3]$, $E_{\\text{val}} = [5.0,4.2,3.6,3.3]$。\n    - **测试用例 3**：$d = [1,2,3,4,5,6]$, $E_{\\text{train}} = [12.0,8.0,6.0,5.2,5.0,4.9]$, $E_{\\text{val}} = [11.5,7.3,5.1,4.1,3.7,3.4]$。\n    - **测试用例 4**：$d = [1,10]$, $E_{\\text{train}} = [5.0,0.1]$, $E_{\\text{val}} = [5.1,6.0]$。\n    - **测试用例 5**：$d = [2,3,4]$, $E_{\\text{train}} = [8.0,6.0,5.0]$, $E_{\\text{val}} = [8.7,6.6,5.4]$。\n- **输出规格**：对于每个测试用例，输出一个双元素列表 $[\\widehat{b}, \\widehat{d}^{\\star}]$ 的单行列表，最终输出字符串中没有空格。\n\n### 第 2 步：使用提取的已知信息进行验证\n根据验证标准评估问题陈述。\n- **有科学依据**：该问题基于统计学习理论的基本概念，包括偏差-方差权衡、过拟合、训练误差、验证误差和泛化误差。使用普通最小二乘法来建模模型复杂度和泛化差距之间的关系是一种标准且可靠的分析技术。\n- **适定性**：该问题是适定的。对于每个测试用例，用于 OLS 回归的不同数据点数量至少为2，确保了线性参数有唯一解。寻找最优阶数是在一个有限集上进行最小化，保证有解。平局打破规则确保了解的唯一性。\n- **客观性**：该问题使用精确的数学定义和清晰、明确的算法流程进行陈述。输入是数值，要求的输出定义明确。\n- **缺陷清单**：该问题没有违反任何指定的缺陷。它具有科学性、可形式化、完整性、现实性和适定性。它需要进行重要的计算和推理。\n\n### 第 3 步：结论与行动\n问题有效。将提供完整的解决方案。\n\n该问题要求分析模型性能作为复杂度（具体指多项式回归模型的阶数 $d$）的函数。分析的核心在于理解和建模泛化差距 $g(d)$，它代表模型在未见数据上的性能（通过验证误差 $E_{\\text{val}}$ 近似）与其在训练数据上的性能（$E_{\\text{train}}$）之间的差异。差距随复杂度增加而增大是过拟合的标志。\n\n指定的算法旨在创建泛化误差的平滑估计，以便做出比简单地选择具有最低验证误差的模型更稳健的模型选择决策。验证误差本身可能存在噪声，对泛化差距的趋势进行建模有助于滤除这种噪声。\n\n每个测试用例的步骤如下：\n\n1.  **计算经验泛化差距**：对于每个给定的多项式阶数 $d_k$，我们使用提供的训练和验证误差计算经验泛化差距 $g(d_k)$：\n    $$g(d_k) = E_{\\text{val}}(d_k) - E_{\\text{train}}(d_k)$$\n\n2.  **通过 OLS 建模泛化差距**：我们假设模型复杂度 $d$ 和泛化差距 $g(d)$ 之间存在线性关系。我们使用普通最小二乘法 (OLS) 将一条直线 $\\widehat{g}(d) = \\widehat{a} + \\widehat{b}d$ 拟合到观测点集 $\\{(d_k, g(d_k))\\}$。选择斜率 $\\widehat{b}$ 和截距 $\\widehat{a}$ 以最小化平方差之和 $\\sum_k (g(d_k) - (\\widehat{a} + \\widehat{b}d_k))^2$。对于一组 $n$ 个点，OLS 估计量由以下公式给出：\n    $$ \\widehat{b} = \\frac{\\sum_{k=1}^{n} (d_k - \\bar{d})(g_k - \\bar{g})}{\\sum_{k=1}^{n} (d_k - \\bar{d})^2} $$\n    $$ \\widehat{a} = \\bar{g} - \\widehat{b}\\bar{d} $$\n    其中 $\\bar{d} = \\frac{1}{n}\\sum_k d_k$ 和 $\\bar{g} = \\frac{1}{n}\\sum_k g(d_k)$ 是样本均值。斜率 $\\widehat{b}$ 可作为过拟合风险的直接度量；正的 $\\widehat{b}$ 表示训练性能和验证性能之间的差距随着模型复杂度的增加而扩大。\n\n3.  **估计泛化误差**：使用差距的线性模型，我们构建真实泛化误差的平滑估计 $\\widehat{E}_{\\text{gen}}(d_k)$：\n    $$ \\widehat{E}_{\\text{gen}}(d_k) = E_{\\text{train}}(d_k) + \\widehat{g}(d_k) = E_{\\text{train}}(d_k) + \\widehat{a} + \\widehat{b}d_k $$\n    该估计结合了直接观测到的训练误差（反映模型对数据的拟合程度）和泛化惩罚的正则化估计（考虑了复杂度）。\n\n4.  **确定最优阶数**：最优阶数 $\\widehat{d}^{\\star}$ 被选为给定集合 $\\{d_k\\}$ 中使我们估计的泛化误差 $\\widehat{E}_{\\text{gen}}(d_k)$ 最小的阶数。\n    $$ \\widehat{d}^{\\star} = \\underset{d_k}{\\arg\\min} \\{ \\widehat{E}_{\\text{gen}}(d_k) \\} $$\n    问题规定，如果出现平局，应选择达到最小误差的那些阶数中最小的一个。这反映了简约原则（奥卡姆剃刀）：在其他条件相同的情况下，偏好更简单的模型。\n\n最后，对于每个测试用例，我们报告计算出的斜率 $\\widehat{b}$（过拟合风险的度量）和确定的最优阶数 $\\widehat{d}^{\\star}$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the model selection problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {'d': [1, 2, 3, 4, 5], \n         'E_train': [9.0, 6.0, 4.5, 4.0, 3.8], \n         'E_val': [10.0, 7.0, 5.0, 5.2, 6.0]},\n        # Test case 2\n        {'d': [1, 2, 3, 4], \n         'E_train': [5.0, 4.2, 3.6, 3.3], \n         'E_val': [5.0, 4.2, 3.6, 3.3]},\n        # Test case 3\n        {'d': [1, 2, 3, 4, 5, 6], \n         'E_train': [12.0, 8.0, 6.0, 5.2, 5.0, 4.9], \n         'E_val': [11.5, 7.3, 5.1, 4.1, 3.7, 3.4]},\n        # Test case 4\n        {'d': [1, 10], \n         'E_train': [5.0, 0.1], \n         'E_val': [5.1, 6.0]},\n        # Test case 5\n        {'d': [2, 3, 4], \n         'E_train': [8.0, 6.0, 5.0], \n         'E_val': [8.7, 6.6, 5.4]},\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        # Extract and convert data to numpy arrays for vectorized operations\n        d = np.array(case['d'], dtype=float)\n        E_train = np.array(case['E_train'], dtype=float)\n        E_val = np.array(case['E_val'], dtype=float)\n        \n        # Step 1: Compute the empirical generalization gap\n        g = E_val - E_train\n        \n        # Step 2: Fit a straight line g(d) = a + b*d using OLS.\n        # np.polyfit with degree 1 returns the coefficients [b, a] for slope and intercept.\n        b_hat, a_hat = np.polyfit(d, g, 1)\n        \n        # Step 3: Form the estimated expected generalization error\n        # E_gen_hat(d) = E_train(d) + (a_hat + b_hat*d)\n        E_gen_hat = E_train + a_hat + b_hat * d\n        \n        # Step 4: Classify the optimal degree d_star\n        # np.argmin finds the index of the first occurrence of the minimum value.\n        # Since the input degrees 'd' are sorted, this satisfies the tie-breaking rule\n        # of choosing the smallest degree.\n        min_error_index = np.argmin(E_gen_hat)\n        d_star = int(d[min_error_index])\n        \n        # Format the result for this case as a string \"[b,d_star]\" with no spaces\n        # and b rounded to 4 decimal places.\n        case_result_str = f\"[{b_hat:.4f},{d_star}]\"\n        results_as_strings.append(case_result_str)\n\n    # Final print statement in the exact required format.\n    # The output is a single list containing the results for all test cases.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3107026"}, {"introduction": "经典线性回归模型的一个基本假设是误差项的方差恒定，即同方差性。当这一假设被违背时（即存在异方差性），普通的最小二乘法（OLS）估计将不再是最高效的。本练习提供了一个完整的诊断与修正流程：首先，您将学习使用形式化的统计检验（Breusch-Pagan检验）来“分类”一个数据集是否存在异方差性；然后，您将通过回归方法来估计一个与自变量相关的方差函数，并最终应用加权最小二乘法（WLS）来构建一个更稳健、更高效的模型 [@problem_id:3107027]。", "problem": "要求您编写一个完整的、可运行的程序，该程序针对几个合成数据集，执行三项基于经典线性模型的任务：使用基于拉格朗日乘子原理形式化的残差诊断方法，对误差项条件方差 $\\operatorname{Var}(\\epsilon \\mid x)$ 的异方差性存在与否进行分类；通过回归估计参数化方差函数 $\\sigma^{2}(x)$；以及通过加权最小二乘法重新拟合回归模型。您的实现必须遵循下文详述的、源于基本定义和成熟结果的、基于原理的方法。\n\n基本模型是简单线性回归\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n,\n$$\n其中误差满足 $\\mathbb{E}[\\epsilon_i \\mid x_i] = 0$。在同方差性条件下，条件方差是常数，即 $\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2$；而在异方差性条件下，它随 $x_i$ 变化，即 $\\operatorname{Var}(\\epsilon_i \\mid x_i) = \\sigma^2(x_i)$。普通最小二乘法 (OLS) 选择使残差平方和最小化的系数，\n$$\n(\\widehat{\\beta}_0,\\widehat{\\beta}_1) = \\arg\\min_{\\beta_0,\\beta_1} \\sum_{i=1}^{n} \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2.\n$$\n定义残差 $r_i = y_i - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 x_i$。\n\n任务 A（通过形式化为检验的残差图进行异方差性分类）：使用 Breusch–Pagan 拉格朗日乘子方法作为残差模式诊断的形式化代表。将残差平方对回归量和一个常数项进行回归，\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i,\n$$\n并计算此辅助回归的决定系数 $R^2$。Breusch–Pagan 检验统计量为\n$$\n\\mathrm{LM} = n R^2,\n$$\n在同方差性的原假设下，该统计量近似服从自由度为 $k$ 的卡方分布，其中 $k$ 是辅助回归中非常数回归量的数量。对于本问题，$k = 1$。如果卡方生存概率满足以下条件，则将数据集分类为异方差：\n$$\np\\text{-值} = \\Pr\\left(\\chi^2_k \\ge \\mathrm{LM}\\right)  0.05.\n$$\n\n任务 B（方差函数回归）：使用 OLS 残差，通过将对数残差平方对 $x$ 的一个固定特征进行回归来拟合一个正方差模型：\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i,\n$$\n其中 $\\varepsilon$ 是一个小的正常数，以避免对零取对数。然后将每个 $x_i$ 处的拟合方差定义为\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right).\n$$\n这确保了对所有 $i$ 都有 $\\widehat{\\sigma}_i^2 > 0$。\n\n任务 C（加权最小二乘法重新拟合）：使用权重 $w_i = 1/\\widehat{\\sigma}_i^2$，计算最小化下式的加权最小二乘 (WLS) 估计量\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2,\n$$\n并报告重新拟合的系数。同时计算 OLS 和 WLS 拟合的均方误差 (MSE)，定义为\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2,\\quad\n\\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2.\n$$\n\n您必须遵循的实现细节：\n- 使用以下测试套件，并设置固定的随机种子以保证可复现性。对于每种情况，按指定要求独立地从均匀分布中抽取 $x_i$，生成均值为 0 且标准差函数由指定函数给出的独立高斯噪声 $\\epsilon_i$，并设置 $y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i$。\n- 随机种子：对所有随机生成过程使用种子值 $1337$。\n- 对于每个数据集，使用以下确切参数：\n\n1. 情况1（同方差，“理想路径”）：$n = 200$，$\\beta_0 = 1.25$，$\\beta_1 = -0.75$，$x_i \\sim \\mathrm{Uniform}[-2,2]$，噪声标准差 $\\sigma(x) \\equiv 0.5$（常数）。\n2. 情况2（明显异方差，随 $|x|$ 增加）：$n = 200$，$\\beta_0 = 0.5$，$\\beta_1 = 1.5$，$x_i \\sim \\mathrm{Uniform}[-3,3]$，噪声标准差 $\\sigma(x) = 0.3 + 0.7|x|$。\n3. 情况3（中等样本量和曲线方差的边缘情况）：$n = 60$，$\\beta_0 = 0.0$，$\\beta_1 = 2.0$，$x_i \\sim \\mathrm{Uniform}[-2,2]$，噪声标准差 $\\sigma(x) = 0.2 + 0.4(x+1)^2$。\n\n- 在方差回归步骤中，使用 $\\varepsilon = 10^{-8}$。\n- 在 Breusch–Pagan 辅助回归中，包含一个截距和回归量 $x_i$。对卡方参考分布使用 $k=1$ 的自由度。\n\n每个数据集的所需输出：\n- 一个形如\n$$\n[\\text{hetero\\_boolean},\\; p\\_value,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{OLS}}_1,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_0,\\; \\widehat{\\beta}^{\\mathrm{WLS}}_1,\\; \\mathrm{MSE}_{\\mathrm{OLS}},\\; \\mathrm{MSE}_{\\mathrm{WLS}}].\n$$\n的列表。\n- 布尔值必须像 Python 中那样大写（$\\mathrm{True}$ 或 $\\mathrm{False}$）。所有浮点数必须四舍五入到恰好 $6$ 位小数。\n\n最终程序要求：\n- 您的程序必须使用上述测试套件在内部构建所有数据集，对每个数据集执行任务 A–C，并生成单行输出，该输出包含一个含三个元素（每个测试用例一个）的列表，其中每个元素是上述描述的列表。格式必须是单行：“[$\\dots$]”，条目之间用逗号分隔，无额外文本。例如：“[[True,0.012345,1.234000, ...],[...],[...]]”。\n- 不允许用户输入、外部文件和网络访问。唯一允许使用的库是 Numerical Python (NumPy) 和 Scientific Python (SciPy)。", "solution": "用户提供的问题经评估为**有效**。这是一个在计算统计学领域内定义明确、有科学依据且客观的问题。它概述了在简单线性回归背景下识别和纠正异方差性的标准流程。所有必要的参数、模型和评估标准都得到了明确的规定。\n\n解决方案将通过为三个合成数据集中的每一个系统地实现三个指定任务来开发。该方法论依赖于线性回归、统计检验和加权最小二乘估计的基本原理。\n\n### 基于原理的方法\n\n问题的核心是处理异方差性，这是一种误差项 $\\epsilon_i$ 的方差在不同观测值之间不恒定的情况。标准的普通最小二乘 (OLS) 估计量在异方差性条件下虽然仍是无偏的，但不再是最佳线性无偏估计量 (BLUE)。所提供的任务概述了诊断和缓解此问题的常用工作流程。\n\n#### 1. 数据生成和初始 OLS 拟合\n\n对于每个测试用例，我们首先根据指定的模型生成一个合成数据集：\n$$\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad i=1,\\dots,n\n$$\n自变量 $x_i$ 从均匀分布中抽取，误差项 $\\epsilon_i$ 从正态分布 $\\mathcal{N}(0, \\sigma^2(x_i))$ 中抽取，其中标准差 $\\sigma(x_i)$ 对每种情况都是特定的。\n\n我们首先使用普通最小二乘法 (OLS) 拟合此模型。OLS 估计量 $(\\widehat{\\beta}^{\\mathrm{OLS}}_0, \\widehat{\\beta}^{\\mathrm{OLS}}_1)$ 是通过最小化残差平方和找到的。在矩阵形式中，使用设计矩阵 $\\mathbf{X}$（其中第一列全为1，第二列是 $x_i$ 值的向量）和响应向量 $\\mathbf{y}$，OLS 系数向量 $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}}$ 由下式给出：\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{OLS}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n$$\n从这个拟合中，我们计算 OLS 残差 $r_i = y_i - (\\widehat{\\beta}^{\\mathrm{OLS}}_0 + \\widehat{\\beta}^{\\mathrm{OLS}}_1 x_i)$。这些残差对于后续的诊断和建模步骤至关重要。\n\n#### 2. 任务 A：异方差性分类 (Breusch-Pagan 检验)\n\n第一个任务是正式检验异方差性的存在。Breusch-Pagan 检验基于这样一个思想：如果存在异方差性，那么残差平方 $r_i^2$（作为真实误差方差 $\\sigma_i^2$ 的代理）应与自变量系统地相关。\n\n我们执行一个辅助 OLS 回归：\n$$\nr_i^2 = \\delta_0 + \\delta_1 x_i + u_i\n$$\n检验统计量是拉格朗日乘子 (LM) 统计量，使用来自此辅助回归的决定系数 $R^2$ 计算得出：\n$$\n\\mathrm{LM} = n R^2\n$$\n在同方差性的原假设下，该统计量服从自由度为 $k$ 的卡方分布，其中 $k$ 是辅助回归中自变量的数量（不包括常数项）。在本问题中，$k=1$。\n\n然后我们计算 $p$ 值：\n$$\np\\text{-值} = \\Pr\\left(\\chi^2_1 \\ge \\mathrm{LM}\\right)\n$$\n一个小的 $p$ 值（根据问题要求，具体为 $p  0.05$）提供了反对原假设的证据，从而使我们将数据集分类为异方差。\n\n#### 3. 任务 B：参数化方差函数回归\n\n如果怀疑存在异方差性（或者即使不怀疑，为了本练习的目的），我们继续对方差函数 $\\sigma^2(x_i)$ 进行建模。问题指定了一个灵活且稳健的对数线性模型，以确保估计的方差始终为正。我们将对数残差平方对 $x_i$ 的一个函数进行回归：\n$$\n\\log\\left(r_i^2 + \\varepsilon\\right) = \\gamma_0 + \\gamma_1 \\,\\log\\!\\left(1 + x_i^2\\right) + \\eta_i\n$$\n此处，$\\varepsilon = 10^{-8}$ 是一个小的常数，用于防止在残差恰好为零的情况下对零取对数。通过 OLS 估计参数 $\\widehat{\\gamma}_0$ 和 $\\widehat{\\gamma}_1$ 后，我们可以为每个观测值构建方差的估计：\n$$\n\\widehat{\\sigma}_i^2 = \\exp\\!\\left(\\widehat{\\gamma}_0 + \\widehat{\\gamma}_1 \\,\\log\\!\\left(1 + x_i^2\\right)\\right)\n$$\n\n#### 4. 任务 C：加权最小二乘 (WLS) 重新拟合\n\n有了估计的方差 $\\widehat{\\sigma}_i^2$ 后，我们可以通过使用加权最小二乘 (WLS) 来改进初始的 OLS 拟合。WLS 通过为每个观测值分配一个与其误差方差成反比的权重来解决异方差性问题。权重定义为 $w_i = 1/\\widehat{\\sigma}_i^2$。方差较小（因此信息量更大）的观测值会获得更高的权重。\n\nWLS 估计量是最小化加权残差平方和的向量 $\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}}$：\n$$\n\\sum_{i=1}^{n} w_i \\left(y_i - \\beta_0 - \\beta_1 x_i\\right)^2\n$$\n在矩阵形式中，令 $\\mathbf{W}$ 为权重 $w_i$ 的对角矩阵，WLS 解为：\n$$\n\\widehat{\\boldsymbol{\\beta}}^{\\mathrm{WLS}} = (\\mathbf{X}^T \\mathbf{W} \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{W} \\mathbf{y}\n$$\n在假定的异方差形式下，该估计量是有效的。\n\n最后，我们计算 OLS 和 WLS 拟合的均方误差 (MSE)，以在原始尺度上比较它们的性能：\n$$\n\\mathrm{MSE}_{\\mathrm{OLS}} = \\frac{1}{n} \\sum_{i=1}^{n} r_i^2 \\quad \\text{和} \\quad \\mathrm{MSE}_{\\mathrm{WLS}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\widehat{\\beta}^{\\mathrm{WLS}}_0 - \\widehat{\\beta}^{\\mathrm{WLS}}_1 x_i\\right)^2\n$$\n\n这就完成了整个流程。实现将为每个测试用例精确地遵循这些步骤，收集所需的八个输出值：布尔分类、p 值、两个 OLS 系数、两个 WLS 系数以及两个 MSE 值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the problem by performing OLS, Breusch-Pagan test, variance modeling,\n    and WLS for three synthetic datasets.\n    \"\"\"\n    \n    # Define test cases as per the problem description.\n    test_cases = [\n        # Case 1: Homoscedastic\n        {'n': 200, 'beta0': 1.25, 'beta1': -0.75, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.5},\n        # Case 2: Clearly heteroscedastic\n        {'n': 200, 'beta0': 0.5, 'beta1': 1.5, 'x_range': [-3, 3], 'sigma_func': lambda x: 0.3 + 0.7 * np.abs(x)},\n        # Case 3: Edge case, curved variance\n        {'n': 60, 'beta0': 0.0, 'beta1': 2.0, 'x_range': [-2, 2], 'sigma_func': lambda x: 0.2 + 0.4 * (x + 1)**2}\n    ]\n\n    # Global constants and random number generator\n    RANDOM_SEED = 1337\n    EPSILON_LOG = 1e-8\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Data Generation ---\n        n = case['n']\n        beta0_true, beta1_true = case['beta0'], case['beta1']\n        x = rng.uniform(case['x_range'][0], case['x_range'][1], size=n)\n        sigma = case['sigma_func'](x)\n        epsilon = rng.normal(0, sigma, size=n)\n        y = beta0_true + beta1_true * x + epsilon\n\n        # --- Task A: OLS and Breusch-Pagan Test ---\n        \n        # 1. Initial OLS fit\n        X_ols = np.c_[np.ones(n), x]\n        try:\n            beta_ols = np.linalg.inv(X_ols.T @ X_ols) @ X_ols.T @ y\n        except np.linalg.LinAlgError:\n            # Fallback to lstsq if matrix is singular, though unlikely for this problem.\n            beta_ols, _, _, _ = np.linalg.lstsq(X_ols, y, rcond=None)\n\n        res_ols = y - X_ols @ beta_ols\n        squared_res = res_ols**2\n        \n        # 2. Auxiliary regression for Breusch-Pagan test: r_i^2 on x_i\n        X_aux = np.c_[np.ones(n), x]\n        try:\n            delta = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ squared_res\n        except np.linalg.LinAlgError:\n            delta, _, _, _ = np.linalg.lstsq(X_aux, squared_res, rcond=None)\n            \n        y_pred_aux = X_aux @ delta\n        ss_res_aux = np.sum((squared_res - y_pred_aux)**2)\n        ss_tot_aux = np.sum((squared_res - np.mean(squared_res))**2)\n        \n        # Guard against ss_tot_aux being zero\n        r_squared_aux = 1 - (ss_res_aux / ss_tot_aux) if ss_tot_aux > 0 else 0.0\n\n        # 3. LM statistic and p-value\n        lm_stat = n * r_squared_aux\n        p_value = chi2.sf(lm_stat, df=1)\n        is_heteroscedastic = p_value  0.05\n        \n        # --- Task B: Variance Function Regression ---\n        log_squared_res = np.log(squared_res + EPSILON_LOG)\n        log_x_feature = np.log(1 + x**2)\n        \n        X_var = np.c_[np.ones(n), log_x_feature]\n        try:\n            gamma = np.linalg.inv(X_var.T @ X_var) @ X_var.T @ log_squared_res\n        except np.linalg.LinAlgError:\n            gamma, _, _, _ = np.linalg.lstsq(X_var, log_squared_res, rcond=None)\n\n        sigma2_hat = np.exp(X_var @ gamma)\n        \n        # --- Task C: Weighted Least Squares Refit ---\n        weights = 1.0 / sigma2_hat\n        W = np.diag(weights)\n        \n        try:\n            # Direct WLS formula\n            X_wls = X_ols\n            beta_wls = np.linalg.inv(X_wls.T @ W @ X_wls) @ X_wls.T @ W @ y\n        except np.linalg.LinAlgError:\n            # Fallback using transformed OLS, which is more stable\n            sqrt_w = np.sqrt(weights)\n            y_prime = y * sqrt_w\n            X_prime = X_wls * sqrt_w[:, np.newaxis]\n            beta_wls, _, _, _ = np.linalg.lstsq(X_prime, y_prime, rcond=None)\n\n        # --- MSE Calculation ---\n        mse_ols = np.mean(res_ols**2)\n        res_wls = y - X_wls @ beta_wls\n        mse_wls = np.mean(res_wls**2)\n        \n        # --- Collect and Format Results ---\n        result_list = [\n            is_heteroscedastic,\n            p_value,\n            beta_ols[0],\n            beta_ols[1],\n            beta_wls[0],\n            beta_wls[1],\n            mse_ols,\n            mse_wls,\n        ]\n        all_results.append(result_list)\n\n    # Final print statement in the exact required format.\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        formatted_res = (\n            f\"[{res[0]},\"\n            f\"{res[1]:.6f},\"\n            f\"{res[2]:.6f},\"\n            f\"{res[3]:.6f},\"\n            f\"{res[4]:.6f},\"\n            f\"{res[5]:.6f},\"\n            f\"{res[6]:.6f},\"\n            f\"{res[7]:.6f}]\"\n        )\n        output_str += formatted_res\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3107027"}, {"introduction": "任何模型都有其适用边界，其预测能力在其训练数据的范围之外通常会显著下降。本练习旨在解决一个至关重要的实践问题：我们何时能够信任模型的外推预测？您将学习一个结构化的方法，通过检验模型在训练域之外是否保持了关键的逻辑属性（如单调性）并遵守物理边界，来“分类”外推的安全性。此外，您还将通过回归分析来量化预测置信区间随外推距离的扩张速率，从而更深刻地理解模型不确定性的增长 [@problem_id:3107029]。", "problem": "给定一个已学习的单变量回归模型，表示为多项式函数 $\\hat{f}(x) = \\sum_{k=0}^{K} c_k x^k$ 和一个训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$。您必须编写一个完整的、可运行的程序，该程序针对几个指定的测试用例执行两项任务：(1) 根据逻辑单调性和物理边界标准，分类将 $\\hat{f}(x)$ 外推到训练域外的指定区间是否安全；(2) 通过线性回归估计对称高斯置信区间宽度随超出训练域的距离而扩大的速率。程序必须为每个测试用例输出一个结果，并将所有结果汇总到单行中，格式完全如下文所述。\n\n用作起点的基础定义：\n- 如果对于区间内的每一对 $x_1 \\le x_2$，我们都有 $\\hat{f}(x_1) \\le \\hat{f}(x_2)$，则称函数在该区间上是单调不减的。如果对于所有这样的对，都有 $\\hat{f}(x_1) \\ge \\hat{f}(x_2)$，则称其为单调不增。常数函数既是单调不减的，也是单调不增的。\n- 响应的物理边界由一个闭区间 $[y_{\\min}, y_{\\max}]$ 给出，模型输出不得超过该区间。\n- 对于标称覆盖率为 $q \\in (0,1)$ 且高斯噪声标准差为 $\\sigma  0$ 的双侧高斯置信区间，对称区间的宽度 $w$ 通过标准正态分布 (SND) 的分位数与一个经过充分检验的事实相关联：水平为 $q$ 的对称双侧区间使用 SND 分位数 $z_q$，定义为 $z_q = \\Phi^{-1}\\!\\left(1 - \\frac{1-q}{2}\\right)$，其中 $\\Phi$ 是 SND 累积分布函数。已知标准差 $\\sigma$ 对应的宽度贡献为 $2 z_q \\sigma$。\n- 对于带有截距的响应 $W$ 对标量预测变量 $D$ 的线性回归，在通常假设下且 $\\mathrm{Var}(D)  0$ 时，普通最小二乘斜率等于 $\\mathrm{Cov}(D,W)/\\mathrm{Var}(D)$。\n\n您的程序必须为每个测试用例实现以下逻辑：\n\n1) 基于单调性和边界的安全分类规则：\n- 通过检查 $\\hat{f}$ 在训练区间 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 上是单调不减、单调不增还是（在数值容差内）常数，来确定其单调性类型。为此检查使用一个包含 $N$ 个点的均匀网格，其中 $N = 1000$，容差为 $\\varepsilon = 10^{-8}$。您可以通过检查整个网格上离散导数的符号或任何符合上述定义的等效数值标准来评估单调性。将训练单调性类型声明为以下之一：increasing、decreasing 或 flat。如果在容差范围内没有适用的单调性类型，则分类必须为不安全。\n- 使用相同的 $N$ 和 $\\varepsilon$ 检查 $\\hat{f}$ 在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 上是否与在 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 上具有相同的单调性类型。如果训练类型是 flat，则外推也必须在容差范围内是 flat。\n- 确认在 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$（含端点）的 $N$ 个点的均匀网格上，$\\hat{f}(x)$ 的每个点都在 $[y_{\\min}, y_{\\max}]$ 内，容差为 $\\varepsilon$。\n- 当且仅当单调性条件和边界条件都满足时，外推被分类为安全。\n\n2) 置信区间扩大速率的回归：\n- 对于给定的覆盖水平 $q$ 和噪声标准差 $\\sigma$，将在训练域外一点 $x$ 的合成对称置信区间宽度定义为\n$$\nw(x) = 2 z_q \\sigma \\left(1 + \\lambda \\, d(x)\\right),\n$$\n其中 $d(x)$ 是 $x$ 到训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 最近点的非负距离，即\n$$\nd(x) = \\begin{cases}\nx_{\\mathrm{L}} - x,   \\text{若 } x  x_{\\mathrm{L}},\\\\\n0,   \\text{若 } x \\in [x_{\\mathrm{L}}, x_{\\mathrm{R}}],\\\\\nx - x_{\\mathrm{R}},   \\text{若 } x > x_{\\mathrm{R}}.\n\\end{cases}\n$$\n参数 $\\lambda \\ge 0$ 控制宽度每单位距离增加的速度。对于每个测试用例，在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 中构造 $M = 5$ 个等距点，计算这些点上的 $d(x)$ 和 $w(x)$，并拟合普通最小二乘线性模型 $w = \\alpha + r \\, d$。报告估计的斜率 $r$ 作为扩大速率。将报告的 $r$ 四舍五入到六位小数。\n\n测试套件：\n为以下四个测试用例提供结果。在每个用例中，多项式系数按从低次到高次的顺序列出，因此 $c_0$ 是常数项。\n\n- 用例 1:\n  - $c = \\{1, 2\\}$，所以 $\\hat{f}(x) = 1 + 2 x$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-1, 1]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [1, 2]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [0, 5]$。\n  - 噪声标准差 $\\sigma = 0.5$。\n  - 覆盖水平 $q = 0.95$。\n  - 扩大参数 $\\lambda = 0.1$。\n\n- 用例 2:\n  - $c = \\{1, 4\\}$，所以 $\\hat{f}(x) = 1 + 4 x$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-1, 1]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [1, 3]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [0, 10]$。\n  - 噪声标准差 $\\sigma = 0.3$。\n  - 覆盖水平 $q = 0.90$。\n  - 扩大参数 $\\lambda = 0.2$。\n\n- 用例 3:\n  - $c = \\{0, -1, 0, 1\\}$，所以 $\\hat{f}(x) = x^3 - x$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-0.5, 0.5]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [0.5, 1.0]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [-10, 10]$。\n  - 噪声标准差 $\\sigma = 0.4$。\n  - 覆盖水平 $q = 0.95$。\n  - 扩大参数 $\\lambda = 0.15$。\n\n- 用例 4:\n  - $c = \\{3\\}$，所以 $\\hat{f}(x) = 3$。\n  - 训练域 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}] = [-2, 2]$。\n  - 外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}] = [2, 3]$。\n  - 物理边界 $[y_{\\min}, y_{\\max}] = [0, 5]$。\n  - 噪声标准差 $\\sigma = 0.2$。\n  - 覆盖水平 $q = 0.99$。\n  - 扩大参数 $\\lambda = 0.0$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含所有测试用例的结果，形式为用方括号括起来的逗号分隔列表。每个测试用例结果必须是一个形式为 $[s, r]$ 的双元素列表，其中 $s$ 是一个布尔值（安全为True，不安全为False），$r$ 是四舍五入到六位小数的斜率。例如，包含两个假设结果的输出可能看起来像 \"[[True,0.123456],[False,0.000000]]\"。您的程序必须且只能打印这样的一行，不得包含其他任何内容。", "solution": "用户提供了一个计算问题，要求对给定的多项式回归模型 $\\hat{f}(x)$ 实现一个分为两部分的分析。第一个任务是根据逻辑标准，对模型在其训练域之外进行外推的安全性进行分类。第二个任务是估计一个合成的置信区间的宽度随与训练域距离的增加而扩大的速率。该问题定义明确，科学上基于标准的数值和统计原理，并为得到唯一、可验证的解提供了所有必要的数据和定义。\n\n解决方案通过为每个测试用例实现指定的逻辑来推进。\n\n**第 1 部分：安全分类**\n\n外推的安全性取决于两个条件：保持单调性和遵守物理边界。\n\n1.  **单调性分析**：一个函数在一个区间上被定义为单调不减，如果对于该区间内的任何 $x_1 \\le x_2$，都有 $\\hat{f}(x_1) \\le \\hat{f}(x_2)$。如果 $\\hat{f}(x_1) \\ge \\hat{f}(x_2)$，则为单调不增。常数函数同时满足这两个条件。为了在数值上评估这一点，我们在给定区间内的一个包含 $N=1000$ 个点的精细均匀网格上评估函数 $\\hat{f}(x)$。设这些评估点为 $y_i = \\hat{f}(x_i)$。然后我们检查一阶差分序列 $\\Delta y_i = y_{i+1} - y_i$。\n    -   如果所有 $\\Delta y_i \\ge -\\varepsilon$（对于一个小的容差 $\\varepsilon=10^{-8}$），则该函数被认为是数值上不减的。\n    -   如果所有 $\\Delta y_i \\le \\varepsilon$，则为数值上不增的。\n    -   如果两个条件都成立，函数被分类为 `flat`。如果只有第一个条件成立，则为 `increasing`。如果只有第二个条件成立，则为 `decreasing`。如果两者都不成立，则为非单调的，或 `none`。\n\n    安全规则要求在训练区间 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 上观察到的单调性类型必须在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 上得以保持。如果函数在训练区间上不是单调的，则外推立即被视为不安全。\n\n2.  **物理边界检查**：对于外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 中的所有 $x$，模型输出 $\\hat{f}(x)$ 必须保持在指定的物理边界 $[y_{\\min}, y_{\\max}]$ 内。在数值上，这通过检查评估网格上 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 的所有点 $y_i$ 是否都满足条件 $y_{\\min} - \\varepsilon \\le y_i \\le y_{\\max} + \\varepsilon$ 来验证。\n\n当且仅当单调性得以保持且物理边界得到遵守时，外推被分类为 `safe` (True)。否则，它是不安全的 `unsafe` (False)。\n\n**第 2 部分：置信区间扩大速率**\n\n这部分涉及对一个合成生成的数据集进行回归分析。\n\n1.  **合成数据生成**：问题定义了一个模型，用于描述在某点 $x$ 的对称置信区间的宽度，作为其与训练域距离的函数：\n    $$\n    w(x) = 2 z_q \\sigma \\left(1 + \\lambda \\, d(x)\\right)\n    $$\n    这里，$\\sigma$ 是噪声标准差，$\\lambda$ 是一个扩大参数，$d(x)$ 是点 $x$ 到区间 $[x_{\\mathrm{L}}, x_{\\mathrm{R}}]$ 的距离。$z_q$ 项是对应于双侧置信水平 $q$ 的标准正态分布 (SND) 的分位数，由 $z_q = \\Phi^{-1}((1+q)/2)$ 给出，其中 $\\Phi$ 是 SND 的累积分布函数。为了进行分析，我们在外推区间 $[x_{\\mathrm{A}}, x_{\\mathrm{B}}]$ 中生成 $M=5$ 个等距点 $\\{x_i\\}$，并计算相应的对 $(d_i, w_i)$，其中 $d_i = d(x_i)$ 且 $w_i = w(x_i)$。\n\n2.  **线性回归**：我们使用普通最小二乘法 (OLS) 将形式为 $w = \\alpha + r d$ 的简单线性回归模型拟合到生成的 $(d_i, w_i)$ 对。此回归的斜率 $r$ 表示置信区间宽度随与训练域距离的单位增加而扩大的估计速率。斜率 $r$ 的 OLS 估计由以下公式给出：\n    $$\n    r = \\frac{\\mathrm{Cov}(d, w)}{\\mathrm{Var}(d)} = \\frac{\\sum_{i=1}^{M} (d_i - \\bar{d})(w_i - \\bar{w})}{\\sum_{i=1}^{M} (d_i - \\bar{d})^2}\n    $$\n    其中 $\\bar{d}$ 和 $\\bar{w}$ 分别是 $d_i$ 和 $w_i$ 值的样本均值。为每个测试用例计算此值。请注意，由于合成数据 $w_i$ 是 $d_i$ 的精确线性函数，即 $w_i = (2z_q \\sigma) + (2z_q \\sigma \\lambda) d_i$，因此只要 $\\mathrm{Var}(d) > 0$，OLS 回归将精确地恢复理论斜率 $r = 2z_q \\sigma \\lambda$。\n\n最终的程序遍历指定的测试用例，应用上述逻辑，并将结果汇总到所需的输出格式中。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It encapsulates all logic and helper functions for self-contained execution.\n    \"\"\"\n\n    def check_monotonicity(c_coeffs, interval, N, eps):\n        \"\"\"\n        Numerically determines the monotonicity of a polynomial on an interval.\n        Returns 'increasing', 'decreasing', 'flat', or 'none'.\n        \"\"\"\n        # np.polyval expects coefficients for highest power first.\n        c_rev = c_coeffs[::-1]\n        \n        x_grid = np.linspace(interval[0], interval[1], N)\n        y_vals = np.polyval(c_rev, x_grid)\n        \n        diffs = np.diff(y_vals)\n        \n        is_non_decreasing = np.all(diffs >= -eps)\n        is_non_increasing = np.all(diffs = eps)\n        \n        if is_non_decreasing and is_non_increasing:\n            return 'flat'\n        elif is_non_decreasing:\n            return 'increasing'\n        elif is_non_increasing:\n            return 'decreasing'\n        else:\n            return 'none'\n\n    def check_bounds(c_coeffs, interval, N, y_min, y_max, eps):\n        \"\"\"\n        Checks if the polynomial's output stays within physical bounds on an interval.\n        \"\"\"\n        c_rev = c_coeffs[::-1]\n        \n        x_grid = np.linspace(interval[0], interval[1], N)\n        y_vals = np.polyval(c_rev, x_grid)\n        \n        # Check if all y_vals are within [y_min, y_max] with tolerance\n        return np.all((y_vals >= y_min - eps)  (y_vals = y_max + eps))\n\n    def calculate_distance(x_points, x_L, x_R):\n        \"\"\"\n        Vectorized calculation of the distance from points 'x' to the interval [x_L, x_R].\n        \"\"\"\n        return np.maximum(0, x_points - x_R) + np.maximum(0, x_L - x_points)\n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: c, [x_L, x_R], [x_A, x_B], [y_min, y_max], sigma, q, lambda\n        (np.array([1, 2]), [-1, 1], [1, 2], [0, 5], 0.5, 0.95, 0.1),\n        # Case 2\n        (np.array([1, 4]), [-1, 1], [1, 3], [0, 10], 0.3, 0.90, 0.2),\n        # Case 3\n        (np.array([0, -1, 0, 1]), [-0.5, 0.5], [0.5, 1.0], [-10, 10], 0.4, 0.95, 0.15),\n        # Case 4\n        (np.array([3]), [-2, 2], [2, 3], [0, 5], 0.2, 0.99, 0.0),\n    ]\n\n    results = []\n    \n    # Constants for numerical checks\n    N = 1000\n    M = 5\n    eps = 1e-8\n\n    for case in test_cases:\n        c, train_domain, extrap_interval, phys_bounds, sigma, q, lam = case\n        x_L, x_R = train_domain\n        x_A, x_B = extrap_interval\n        y_min, y_max = phys_bounds\n\n        # --- Part 1: Safety Classification ---\n        train_mono_type = check_monotonicity(c, train_domain, N, eps)\n        \n        is_safe = False\n        if train_mono_type != 'none':\n            extrap_mono_type = check_monotonicity(c, extrap_interval, N, eps)\n            mono_ok = (train_mono_type == extrap_mono_type)\n            \n            if mono_ok:\n                bounds_ok = check_bounds(c, extrap_interval, N, y_min, y_max, eps)\n                if bounds_ok:\n                    is_safe = True\n\n        # --- Part 2: Widening Rate Regression ---\n        z_q = norm.ppf(1 - (1 - q) / 2.0)\n        \n        x_points_regr = np.linspace(x_A, x_B, M)\n        d_vals = calculate_distance(x_points_regr, x_L, x_R)\n        w_vals = 2 * z_q * sigma * (1 + lam * d_vals)\n        \n        var_d = np.var(d_vals, ddof=1)\n        \n        r = 0.0\n        # OLS slope r = Cov(d, w) / Var(d). Check Var(d) > 0.\n        if var_d > 1e-12:\n            cov_dw = np.cov(d_vals, w_vals, ddof=1)[0, 1]\n            r = cov_dw / var_d\n        \n        results.append([is_safe, r])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{s},{r:.6f}]\" for s, r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3107029"}]}