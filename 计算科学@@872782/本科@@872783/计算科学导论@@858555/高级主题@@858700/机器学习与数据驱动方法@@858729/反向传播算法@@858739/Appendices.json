{"hands_on_practices": [{"introduction": "理论学习之后，最好的检验方式莫过于亲手实践。这个练习将带你从最基本的单元——单个神经元——出发，手动计算其参数的梯度。通过为一个简单的线性模型找到精确拟合解，并验证该解确实是损失函数的临界点，你将直观地理解反向传播如何将优化理论与神经网络训练联系起来。[@problem_id:3099996]", "problem": "给定一个具有线性激活的单神经元模型，由参数函数 $f(x; \\theta) = W x + b$ 定义，其中 $\\theta = (W, b)$，$W \\in \\mathbb{R}$ 且 $b \\in \\mathbb{R}$。训练集由三个输入输出对 $(x_i, y_i)$ 组成，其中 $i = 1, 2, 3$，具体为 $(x_1, y_1) = (0, 1)$，$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$。经验风险是平方误差半和，定义为\n$$\nJ(\\theta) = \\frac{1}{2} \\sum_{i=1}^{3} \\left(f(x_i; \\theta) - y_i\\right)^{2}.\n$$\n从微积分链式法则的基本定义以及梯度和 Hessian 矩阵（二阶偏导数矩阵）的定义出发，完成以下操作：\n1. 选择参数 $W$ 和 $b$ 以精确拟合这三个数据点，即对每个 $i \\in \\{1, 2, 3\\}$ 都有 $f(x_i; \\theta) = y_i$。\n2. 使用反向传播（即应用于模型计算图的链式法则），推导梯度 $\\nabla_{\\theta} J(\\theta)$ 并在第1部分选择的精确拟合参数处求值。\n3. 推导 $J(\\theta)$ 关于 $\\theta$ 的 Hessian 矩阵 $H(\\theta)$ 并在精确拟合参数处求值。计算最小特征值 $\\lambda_{\\min}(H)$。\n4. 根据 $\\lambda_{\\min}(H)$ 的符号，简要说明该精确拟合点是 $J(\\theta)$ 的局部最小值点还是鞍点。\n\n提供在解处 $\\lambda_{\\min}(H)$ 的精确值作为你的最终答案。不需要四舍五入。", "solution": "该问题已经过验证，科学上合理、良定、客观，并包含足够的信息以获得唯一解。\n\n任务是分析单个线性神经元模型 $f(x; \\theta) = W x + b$（参数为 $\\theta = (W, b)$）的经验风险函数 $J(\\theta)$。风险定义为三个数据点 $(x_1, y_1) = (0, 1)$、$(x_2, y_2) = (1, 3)$ 和 $(x_3, y_3) = (2, 5)$ 上的平方误差半和。风险函数是：\n$$\nJ(W, b) = \\frac{1}{2} \\sum_{i=1}^{3} \\left( (Wx_i + b) - y_i \\right)^{2}\n$$\n代入给定的数据点：\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (W(0) + b - 1)^{2} + (W(1) + b - 3)^{2} + (W(2) + b - 5)^{2} \\right]\n$$\n$$\nJ(W, b) = \\frac{1}{2} \\left[ (b - 1)^{2} + (W + b - 3)^{2} + (2W + b - 5)^{2} \\right]\n$$\n\n**1. 求精确拟合参数 $\\theta^* = (W, b)$**\n\n为了实现精确拟合，模型必须对所有 $i \\in \\{1, 2, 3\\}$ 满足 $f(x_i; \\theta) = y_i$。这产生了一个关于 $W$ 和 $b$ 的线性方程组：\n\\begin{enumerate}\n    \\item 对于 $(x_1, y_1) = (0, 1)$：$W(0) + b = 1 \\implies b = 1$。\n    \\item 对于 $(x_2, y_2) = (1, 3)$：$W(1) + b = 3 \\implies W + b = 3$。\n    \\item 对于 $(x_3, y_3) = (2, 5)$：$W(2) + b = 5 \\implies 2W + b = 5$。\n\\end{enumerate}\n将第一个方程的 $b=1$ 代入第二个方程，得到 $W + 1 = 3$，这意味着 $W = 2$。\n我们必须验证这些值是否满足第三个方程：$2W + b = 2(2) + 1 = 4 + 1 = 5$，这与 $y_3=5$ 一致。\n因此，精确拟合的参数是 $W = 2$ 和 $b = 1$。我们将此点表示为 $\\theta^* = (2, 1)$。\n\n**2. 推导并在 $\\theta^*$ 处计算梯度 $\\nabla_{\\theta} J(\\theta)$**\n\n$J(\\theta)$ 关于 $\\theta = (W, b)$ 的梯度是 $\\nabla_{\\theta} J = \\begin{pmatrix} \\frac{\\partial J}{\\partial W} \\\\ \\frac{\\partial J}{\\partial b} \\end{pmatrix}$。\n按照反向传播方法的要求使用链式法则，我们将每个点的误差定义为 $e_i(\\theta) = f(x_i; \\theta) - y_i = Wx_i + b - y_i$。损失为 $J = \\frac{1}{2} \\sum_{i=1}^3 e_i^2$。\n偏导数是：\n$$\n\\frac{\\partial J}{\\partial W} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial W} = \\sum_{i=1}^{3} e_i \\cdot x_i = \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i\n$$\n$$\n\\frac{\\partial J}{\\partial b} = \\sum_{i=1}^{3} \\frac{\\partial J}{\\partial e_i} \\frac{\\partial e_i}{\\partial b} = \\sum_{i=1}^{3} e_i \\cdot 1 = \\sum_{i=1}^{3} (Wx_i + b - y_i)\n$$\n在精确拟合点 $\\theta^* = (2, 1)$ 处，根据定义，误差项为零：$e_i(\\theta^*) = Wx_i + b - y_i = 0$ 对所有 $i$ 成立。\n因此，在 $\\theta^*$ 处计算梯度：\n$$\n\\frac{\\partial J}{\\partial W}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) x_i = 0\n$$\n$$\n\\frac{\\partial J}{\\partial b}\\bigg|_{\\theta^*} = \\sum_{i=1}^{3} (0) = 0\n$$\n在精确拟合点处的梯度是零向量：$\\nabla_{\\theta} J(\\theta^*) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。这证实了 $\\theta^*$ 是损失函数 $J(\\theta)$ 的一个临界点。\n\n**3. 推导 Hessian 矩阵 $H(\\theta)$ 并计算其最小特征值**\n\nHessian 矩阵 $H(\\theta)$ 包含 $J(\\theta)$ 的二阶偏导数：\n$$\nH(\\theta) = \\begin{pmatrix} \\frac{\\partial^2 J}{\\partial W^2}  \\frac{\\partial^2 J}{\\partial W \\partial b} \\\\ \\frac{\\partial^2 J}{\\partial b \\partial W}  \\frac{\\partial^2 J}{\\partial b^2} \\end{pmatrix}\n$$\n我们通过对一阶偏导数求导来计算这些值：\n$$\n\\frac{\\partial^2 J}{\\partial W^2} = \\frac{\\partial}{\\partial W} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i^2\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b \\partial W} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) x_i \\right] = \\sum_{i=1}^{3} x_i\n$$\n$$\n\\frac{\\partial^2 J}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left[ \\sum_{i=1}^{3} (Wx_i + b - y_i) \\right] = \\sum_{i=1}^{3} 1 = 3\n$$\n注意，正如预期的那样，$\\frac{\\partial^2 J}{\\partial W \\partial b} = \\frac{\\partial^2 J}{\\partial b \\partial W}$。Hessian 矩阵是常数，不依赖于 $W$ 或 $b$。我们使用给定的输入 $x_1=0$、$x_2=1$、$x_3=2$ 来计算这些和：\n$$\n\\sum_{i=1}^{3} x_i^2 = 0^2 + 1^2 + 2^2 = 0 + 1 + 4 = 5\n$$\n$$\n\\sum_{i=1}^{3} x_i = 0 + 1 + 2 = 3\n$$\nHessian 矩阵是：\n$$\nH = \\begin{pmatrix} 5  3 \\\\ 3  3 \\end{pmatrix}\n$$\n$H$ 的特征值 $\\lambda$ 是特征方程 $\\det(H - \\lambda I) = 0$ 的根：\n$$\n\\det \\begin{pmatrix} 5-\\lambda  3 \\\\ 3  3-\\lambda \\end{pmatrix} = (5-\\lambda)(3-\\lambda) - (3)(3) = 0\n$$\n$$\n15 - 8\\lambda + \\lambda^2 - 9 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 6 = 0\n$$\n使用二次方程求根公式 $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$：\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(6)}}{2} = \\frac{8 \\pm \\sqrt{64 - 24}}{2} = \\frac{8 \\pm \\sqrt{40}}{2}\n$$\n化简 $\\sqrt{40} = \\sqrt{4 \\cdot 10} = 2\\sqrt{10}$：\n$$\n\\lambda = \\frac{8 \\pm 2\\sqrt{10}}{2} = 4 \\pm \\sqrt{10}\n$$\n两个特征值是 $\\lambda_1 = 4 + \\sqrt{10}$ 和 $\\lambda_2 = 4 - \\sqrt{10}$。最小特征值是 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$。\n\n**4. 对临界点 $\\theta^*$ 进行分类**\n\n为了对临界点 $\\theta^*$ 进行分类，我们检查在该点计算的 Hessian 矩阵的特征值的符号。由于 $H$ 是常数，我们使用刚刚计算出的特征值。\n我们知道 $3 = \\sqrt{9}  \\sqrt{10}  \\sqrt{16} = 4$。\n因此，最小特征值 $\\lambda_{\\min}(H) = 4 - \\sqrt{10}$ 是正数，因为 $4  \\sqrt{10}$。\n最大特征值 $\\lambda_{\\max}(H) = 4 + \\sqrt{10}$ 也显然是正数。\n由于 Hessian 矩阵的两个特征值都是正数，所以该 Hessian 矩阵是正定的。根据二阶偏导数检验，Hessian 矩阵为正定的临界点是一个局部最小值点。对于这个二次损失函数，它也是唯一的全局最小值点。该精确拟合点是一个局部最小值点。\n最终答案是最小特征值的值。", "answer": "$$\n\\boxed{4 - \\sqrt{10}}\n$$", "id": "3099996"}, {"introduction": "手动计算虽然能建立直觉，但现代神经网络的复杂性需要自动化工具。这个练习将引导你从零开始，构建一个微型的反向模式自动微分（AD）引擎——这正是反向传播算法在计算框架中的本质。通过设计一个“计算带”（tape）来记录前向传播的每一步操作，并反向回溯以累积梯度，你将揭开深度学习框架核心功能的神秘面紗。[@problem_id:3100018]", "problem": "您的任务是为一种小型标量表达式语言实现逆向模式自动微分（AD），并演示如何通过逆向回放操作记录带（tape）来计算伴随变量，记为 $\\bar{x} = \\partial L / \\partial x$。逆向模式AD与计算图中的反向传播算法是同义词。您的实现必须从第一性原理出发，特别是复合函数微分的链式法则，以及将计算图定义为以标量损失 $L$ 为根节点的有向无环图。您必须设计一个记录带结构，用于记录基本操作的前向执行过程，然后逆向回放此记录带，以使用链式法则累积伴随变量。\n\n您的小型表达式语言必须支持标量变量和常量，以及以下基本操作：二元加法 $+$、二元减法 $-$、二元乘法 $\\cdot$、二元除法 $\\div$、一元正弦 $\\sin(\\cdot)$、一元指数 $\\exp(\\cdot)$ 和一元自然对数 $\\log(\\cdot)$。三角函数中的所有角度都必须以弧度为单位。必须遵守定义域约束，例如，$\\log(\\cdot)$ 的输入必须为严格正数。您必须设计计算记录带以记录每个非叶节点操作及其操作数和前向值，以确保正确的逆向回放。\n\n您的程序必须：\n- 在评估标量损失 $L$ 时，构建内部计算图和记录带。\n- 通过从 $\\bar{L} = \\partial L / \\partial L = 1$ 开始对记录带进行单次逆向回放，为每个输入变量 $x_i$ 计算伴随变量，即 $\\partial L / \\partial x_i$。\n- 为每个测试用例生成一个列表，其第一个元素是标量损失值 $L$，随后的元素是按引入顺序排列的变量的伴随变量。\n\n仅从基本原理出发：复合函数的链式法则、中间值 $v$ 的伴随变量 $\\bar{v} = \\partial L / \\partial v$ 的定义，以及计算图的语义。不要依赖跳过推导路径的预封装微分公式；相反，应使用基础微积分，为每个基本操作推导并实现逆向回放所需的局部偏导数。\n\n实现并运行以下测试套件。在每种情况下，按指定顺序定义变量，使用基本操作构造表达式，并计算输出。所有角度均以弧度为单位，此问题不涉及物理单位。\n\n- 测试用例 $1$（通用复合）：变量 $x, y$，损失 $L = \\sin(x \\cdot y) + \\exp(y)$，其中 $x = 0.5, y = -1.0$。此用例的输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 $2$（涉及零和常数的边界情况）：变量 $x$，损失 $L = x \\cdot 0 + \\sin(0) + \\log(1)$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 $3$（变量重复使用）：变量 $x$，损失 $L = (x \\cdot x) \\cdot x$，其中 $x = 2.0$。输出格式：$[L, \\partial L / \\partial x]$。\n- 测试用例 $4$（除法和对数）：变量 $x, y$，损失 $L = x \\div y + \\log(y)$，其中 $x = 1.0, y = 1.5$。输出格式：$[L, \\partial L / \\partial x, \\partial L / \\partial y]$。\n- 测试用例 $5$（嵌套一元复合）：变量 $x$，损失 $L = \\exp(\\sin(x))$，其中 $x = 0.0$。输出格式：$[L, \\partial L / \\partial x]$。\n\n您的程序应生成单行输出，包含所有测试用例的结果，格式为一个由方括号括起的逗号分隔列表，其中每个测试用例的结果本身也是一个由方括号括起的逗号分隔列表。例如，两个测试用例的输出应如下所示：$[[L_1,\\partial L_1/\\partial x_1,\\dots],[L_2,\\partial L_2/\\partial x_1,\\dots]]$。您的最终输出必须严格遵循此格式，使用标准浮点数。", "solution": "该问题要求从第一性原理实现逆向模式自动微分（AD），通常称为反向传播。该方法通过首先对表达式 $L$ 进行前向评估以计算中间值并记录计算图，然后逆向遍历该图以基于链式法则传播梯度，从而计算标量损失函数 $L$ 关于一组输入变量 $x_i$ 的梯度。\n\n**基本原理：链式法则和伴随变量**\n\n逆向模式AD的基础是微积分的链式法则。如果标量损失 $L$ 是中间变量 $v_j$ 的函数，而 $v_j$ 本身是其他变量 $v_i$ 的函数，则 $L$ 对 $v_i$ 的梯度由通过所有从 $v_i$ 到 $L$ 的路径的贡献总和给出。对于单一路径 $L \\to v_j \\to v_i$，链式法则表明：\n$$\n\\frac{\\partial L}{\\partial v_i} = \\frac{\\partial L}{\\partial v_j} \\frac{\\partial v_j}{\\partial v_i}\n$$\n在AD的语言中，我们定义变量 $v$ 的“伴随变量”为 $\\bar{v} \\equiv \\frac{\\partial L}{\\partial v}$。使用此表示法，链式法则变为：\n$$\n\\bar{v}_i = \\bar{v}_j \\frac{\\partial v_j}{\\partial v_i}\n$$\n逆向模式AD算法利用了这种关系，首先计算 $L$ 的值，然后将伴随变量从 $L$ 反向传播到输入变量。该过程从设定损失函数自身的伴随变量 $\\bar{L} = \\frac{\\partial L}{\\partial L} = 1$ 开始。\n\n**计算图和记录带**\n\n任何标量表达式都可以分解为一系列基本操作（例如，加法、乘法、正弦）。这种分解自然形成一个有向无环图（DAG），其中节点表示数值（输入变量、常量和中间结果），边表示基本操作。\n\n从输入到最终损失 $L$ 的表达式的前向评估用于构建此图。在我们的实现中，我们使用“记录带”（tape）数据结构，它是图的线性化表示。记录带是在前向传播过程中记录的操作的有序列表。记录带上的每个条目存储操作类型、对其输入节点的引用以及对其输出节点的引用。这种记录确保我们拥有完整的结构和逆向传播所需的所有必要中间值。\n\n**前向传播：评估与记录带录制**\n\n前向传播过程如下：\n1. 输入变量和常量被初始化为我们图中的起始节点。\n2. 表达式被顺序评估。每次应用基本操作时，会发生两件事：\n    a. 计算操作的数值结果，并作为图中的一个新节点存储。\n    b. 在记录带上添加一个条目，记录操作类型、其输入节点以及新创建的输出节点。\n\n例如，对于表达式 $z = x \\cdot y$，我们将使用 $x$ 和 $y$ 的当前值计算 $z$ 的值，为 $z$ 创建一个新节点，并在记录带上记录 `('mul', [x_node, y_node], z_node)`。\n\n**逆向传播：伴随变量累积**\n\n一旦前向传播完成并计算出最终损失值 $L$，逆向传播就开始了。它以与创建时相反的顺序遍历记录带。\n1. 一个与图中每个节点相对应的伴随变量数组被初始化为零。\n2. 最终损失节点的伴随变量被设置为 $1$，即 $\\bar{L} = 1$。\n3. 对于记录带上的每个操作 $z = f(x_1, \\dots, x_n)$（按逆序处理）：\n    a. 我们检索已计算出的输出的伴随变量 $\\bar{z}$。\n    b. 我们使用链式法则计算 $\\bar{z}$ 对输入伴随变量的贡献。每个输入 $x_i$ 的伴随变量通过累积此贡献进行更新：\n    $$\n    \\bar{x}_i \\mathrel{+}= \\bar{z} \\cdot \\frac{\\partial z}{\\partial x_i}\n    $$\n    使用累加（$\\mathrel{+}=$）至关重要，因为单个变量可能在多个操作中使用（即，它可能是图中多个子节点的父节点）。其总伴随变量是所有子节点回传的梯度信号之和。逆向回放记录带保证了节点的伴随变量（$\\bar{z}$）在其传播给其输入（$x_i$）之前已被完全计算。\n\n**基本操作的伴随变量更新规则**\n\n对于每个基本操作，其局部偏导数 $\\frac{\\partial z}{\\partial x_i}$ 都是已知的。这些导数所需的输入值（例如，对于 $z = x \\cdot y$，$\\frac{\\partial z}{\\partial x} = y$）可以从前向传播中获得。\n\n- **加法:** $z = x + y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = 1$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= \\bar{z}$。\n\n- **减法:** $z = x - y$\n  - $\\frac{\\partial z}{\\partial x} = 1$, $\\frac{\\partial z}{\\partial y} = -1$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z}$, $\\bar{y} \\mathrel{+}= -\\bar{z}$。\n\n- **乘法:** $z = x \\cdot y$\n  - $\\frac{\\partial z}{\\partial x} = y$, $\\frac{\\partial z}{\\partial y} = x$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot y$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot x$。\n\n- **除法:** $z = x \\div y$\n  - $\\frac{\\partial z}{\\partial x} = \\frac{1}{y}$, $\\frac{\\partial z}{\\partial y} = -\\frac{x}{y^2}$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{y}$, $\\bar{y} \\mathrel{+}= \\bar{z} \\cdot \\left(-\\frac{x}{y^2}\\right)$。\n\n- **正弦:** $z = \\sin(x)$\n  - $\\frac{dz}{dx} = \\cos(x)$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\cos(x)$。\n\n- **指数:** $z = \\exp(x)$\n  - $\\frac{dz}{dx} = \\exp(x) = z$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot z$。\n\n- **自然对数:** $z = \\log(x)$\n  - $\\frac{dz}{dx} = \\frac{1}{x}$。\n  - 更新规则: $\\bar{x} \\mathrel{+}= \\bar{z} \\cdot \\frac{1}{x}$。\n\n**实现设计**\n\n该实现使用两个主要类：`Graph` 和 `Node`。`Graph` 类管理计算的状态：它存储所有节点的 `values`（值）、操作 `tape`（记录带）以及计算出的 `adjoints`（伴随变量）。`Node` 类作为节点ID的包装器，通过重载Python的算术运算符（`+`、`*` 等）来提供直观的接口。当对 `Node` 对象执行像 `c = a + b` 这样的操作时，它会透明地调用关联 `Graph` 对象上的一个方法，该方法执行前向计算，将操作记录在记录带上，并为结果 `c` 返回一个新的 `Node`。这种面向对象的设计允许以自然的方式构建表达式，同时在后台正确构建计算图。在计算出最终的损失 `Node` 后，调用 `Graph.compute_gradients()` 会执行如上所述的逆向传播。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# A consistent execution environment requires no user input and all dependencies declared.\n\n# Define unary functions that can operate on Node objects or raw numbers\ndef sin(node):\n    \"\"\"Computes sine, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.sin(node)\n    return np.sin(node)\n\ndef exp(node):\n    \"\"\"Computes exponential, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.exp(node)\n    return np.exp(node)\n\ndef log(node):\n    \"\"\"Computes natural logarithm, handling both Node objects and numeric types.\"\"\"\n    if isinstance(node, Node):\n        return node.graph.log(node)\n    return np.log(node)\n\nclass Graph:\n    \"\"\"Manages the computational graph, tape, and differentiation process.\"\"\"\n    def __init__(self):\n        # A linearized list of operations representing the computational graph.\n        # Each entry is a tuple: (op_type, [input_node_ids], output_node_id)\n        self.tape = []\n        # Stores the numerical value of each node computed during the forward pass.\n        self.values = []\n        # Stores the adjoint (dL/dv) for each node, computed during the reverse pass.\n        self.adjoints = None\n\n    def _add_node(self, value):\n        \"\"\"Adds a new node (value) to the graph and returns its ID.\"\"\"\n        node_id = len(self.values)\n        self.values.append(value)\n        return node_id\n\n    def variable(self, value):\n        \"\"\"Creates a variable node, which is a leaf in the graph.\"\"\"\n        node_id = self._add_node(value)\n        return Node(self, node_id)\n\n    def _promote_to_node(self, other):\n        \"\"\"Promotes a numeric constant to a Node to allow operations like `x + 5`.\"\"\"\n        if not isinstance(other, Node):\n            # Treat numeric constants as new variable nodes in the graph.\n            return self.variable(other)\n        return other\n\n    # Methods for primitive operations (Forward Pass)\n    def add(self, n1, n2):\n        res_val = self.values[n1.node_id] + self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('add', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sub(self, n1, n2):\n        res_val = self.values[n1.node_id] - self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('sub', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def mul(self, n1, n2):\n        res_val = self.values[n1.node_id] * self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('mul', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def truediv(self, n1, n2):\n        res_val = self.values[n1.node_id] / self.values[n2.node_id]\n        res_id = self._add_node(res_val)\n        self.tape.append(('div', [n1.node_id, n2.node_id], res_id))\n        return Node(self, res_id)\n\n    def sin(self, n1):\n        res_val = np.sin(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('sin', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def exp(self, n1):\n        res_val = np.exp(self.values[n1.node_id])\n        res_id = self._add_node(res_val)\n        self.tape.append(('exp', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def log(self, n1):\n        value = self.values[n1.node_id]\n        if value = 0:\n            raise ValueError(\"Domain error: input to log must be positive.\")\n        res_val = np.log(value)\n        res_id = self._add_node(res_val)\n        self.tape.append(('log', [n1.node_id], res_id))\n        return Node(self, res_id)\n\n    def compute_gradients(self, loss_node):\n        \"\"\"Performs the reverse pass to compute gradients for all nodes.\"\"\"\n        num_nodes = len(self.values)\n        self.adjoints = np.zeros(num_nodes)\n        self.adjoints[loss_node.node_id] = 1.0  # Seed the reverse pass\n\n        # Replay the tape in reverse to propagate adjoints\n        for op_type, input_ids, output_id in reversed(self.tape):\n            adjoint_out = self.adjoints[output_id]\n            \n            if adjoint_out == 0.0:  # Optimization: no gradient to propagate\n                continue\n\n            # Apply the chain rule based on the operation\n            if op_type == 'add':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] += adjoint_out\n            elif op_type == 'sub':\n                self.adjoints[input_ids[0]] += adjoint_out\n                self.adjoints[input_ids[1]] -= adjoint_out\n            elif op_type == 'mul':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out * val1\n                self.adjoints[input_ids[1]] += adjoint_out * val0\n            elif op_type == 'div':\n                val0 = self.values[input_ids[0]]\n                val1 = self.values[input_ids[1]]\n                self.adjoints[input_ids[0]] += adjoint_out / val1\n                self.adjoints[input_ids[1]] -= adjoint_out * val0 / (val1**2)\n            elif op_type == 'sin':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out * np.cos(val0)\n            elif op_type == 'exp':\n                out_val = self.values[output_id] # d/dx(exp(x)) = exp(x)\n                self.adjoints[input_ids[0]] += adjoint_out * out_val\n            elif op_type == 'log':\n                val0 = self.values[input_ids[0]]\n                self.adjoints[input_ids[0]] += adjoint_out / val0\n\nclass Node:\n    \"\"\"A node in the computational graph, with overloaded operators.\"\"\"\n    def __init__(self, graph, node_id):\n        self.graph = graph\n        self.node_id = node_id\n\n    @property\n    def value(self):\n        \"\"\"Get the node's numerical value from its graph.\"\"\"\n        return self.graph.values[self.node_id]\n        \n    @property\n    def adjoint(self):\n        \"\"\"Get the node's adjoint after the reverse pass.\"\"\"\n        if self.graph.adjoints is None:\n            raise RuntimeError(\"Gradients not computed yet. Call `compute_gradients` on the loss node first.\")\n        return self.graph.adjoints[self.node_id]\n    \n    # Left-side binary operators\n    def __add__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(self, other)\n\n    def __sub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(self, other)\n\n    def __mul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(self, other)\n\n    def __truediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(self, other)\n\n    # Right-side binary operators (for expressions like `5 + x`)\n    def __radd__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.add(other, self)\n\n    def __rsub__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.sub(other, self)\n\n    def __rmul__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.mul(other, self)\n\n    def __rtruediv__(self, other):\n        other = self.graph._promote_to_node(other)\n        return self.graph.truediv(other, self)\n\n    def __repr__(self):\n        return f\"Node(id={self.node_id}, value={self.value:.4f})\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: L = sin(x*y) + exp(y), x=0.5, y=-1.0\n        {'vars': {'x': 0.5, 'y': -1.0}, 'expr': lambda x, y: sin(x * y) + exp(y)},\n        # Case 2: L = x*0 + sin(0) + log(1), x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: x * 0.0 + sin(0.0) + log(1.0)},\n        # Case 3: L = (x*x)*x, x=2.0\n        {'vars': {'x': 2.0}, 'expr': lambda x: (x * x) * x},\n        # Case 4: L = x/y + log(y), x=1.0, y=1.5\n        {'vars': {'x': 1.0, 'y': 1.5}, 'expr': lambda x, y: x / y + log(y)},\n        # Case 5: L = exp(sin(x)), x=0.0\n        {'vars': {'x': 0.0}, 'expr': lambda x: exp(sin(x))},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        g = Graph()\n        # Create variable nodes in the specified order (determined by dict insertion order in Python 3.7+)\n        var_nodes = {name: g.variable(val) for name, val in case['vars'].items()}\n        \n        # Build the graph by executing the expression\n        loss_node = case['expr'](**var_nodes)\n        \n        # Compute gradients via reverse-mode AD\n        g.compute_gradients(loss_node)\n        \n        # Collect results: [L, dL/dx1, dL/dx2, ...]\n        case_result = [loss_node.value]\n        for node in var_nodes.values():\n            case_result.append(node.adjoint)\n        \n        all_results.append(f\"[{','.join(map(str, case_result))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n\n```", "id": "3100018"}, {"introduction": "在真实的神经网络架构中，我们经常会遇到像最大池化（Max-Pooling）这样并非处处可微的层。这个练习探讨了当梯度流经这些特殊操作时会发生什么，尤其是在存在多个最大值“并列”的情况下。通过分析不同的“平局决胜”（tie-breaking）规则如何影响梯度计算，你将深入理解次梯度（subgradient）的概念及其对模型训练可复现性的实际影响。[@problem_id:3100048]", "problem": "考虑一个窗口大小为 $2 \\times 2$、步幅为 $2$ 的二维最大池化层，该层应用于一个输入矩阵 $X \\in \\mathbb{R}^{4 \\times 4}$。设标量损失为 $L = \\psi(Y)$，其中 $Y$ 是池化输出，$\\psi$ 是一个可微的标量函数。标准的反向传播算法使用微积分的链式法则和 $\\max$ 函数的定义，将上游梯度 $\\partial L / \\partial Y$ 传回至 $X$。上游梯度的数值为\n$$\nG = \\frac{\\partial L}{\\partial Y} = \\begin{pmatrix}\n1  -1\\\\\n2  0\n\\end{pmatrix}.\n$$\n输入为\n$$\nX = \\begin{pmatrix}\n1  3  5  5\\\\\n2  0  4  3\\\\\n0  -1  1  0\\\\\n2  2  -2  7\n\\end{pmatrix}.\n$$\n四个池化窗口（在 $X$ 中）分别是：左上窗口，覆盖第 $1$–$2$ 行，第 $1$–$2$ 列；右上窗口，覆盖第 $1$–$2$ 行，第 $3$–$4$ 列；左下窗口，覆盖第 $3$–$4$ 行，第 $1$–$2$ 列；右下窗口，覆盖第 $3$–$4$ 行，第 $3$–$4$ 列。你可以假定以下内容为基本依据：复合函数的链式法则、$\\max$ 算子的定义，以及当窗口中存在唯一的最大值时，相对于该窗口输入的梯度将完全传递给该唯一最大值所在的位置，而在其他位置则为零。当出现多个相等的最大值时，函数在该输入处不可微；任何与 $\\max$ 算子一致的次梯度的选择都是可接受的。\n\n假设实现中使用了“选择最小字典序索引”的平局打破规则，即在一个窗口内的相等最大值中，拥有最小 $(\\text{行}, \\text{列})$ 对的那个值会接收该池化条目的全部上游梯度。\n\n基于链式法则和 $\\max$ 算子的定义进行推理，并使用上述数值矩阵，选择所有正确的陈述。\n\nA. 在字典序平局打破规则下，只有在 $(1,2)$、$(1,3)$、$(4,1)$ 和 $(4,4)$ 位置的 $\\partial L / \\partial X$ 条目分别接收上游值 $1$、$-1$、$2$ 和 $0$，且 $\\partial L / \\partial X$ 的所有其他条目都为 $0$。\n\nB. 如果转而采用“在所有最大值之间平分”的规则，那么在右上窗口中，位于 $(1,3)$ 和 $(1,4)$ 的两个相等最大值中的每一个都将接收 $\\frac{-1}{2}$；在左下窗口中，位于 $(4,1)$ 和 $(4,2)$ 的两个相等最大值中的每一个都将接收 $\\frac{2}{2}$，从而产生一个 $\\max$ 算子的确定性有效次梯度。\n\nC. 在没有任何明确的平局处理规则的情况下，即使窗口中存在平局，链式法则也能保证 $\\partial L / \\partial X$ 是唯一的，因为上游梯度 $G$ 是固定的。\n\nD. 使用具有不确定性平局解决方案的实现（例如，在不同的运行中以不同的顺序遇到相等的元素）可能导致对于此输入 $X$，在存在平局的窗口处，$\\partial L / \\partial X$ 在不同运行中会有所不同，即使 $G$ 是固定的；因此，对于这些输入，可复现性被破坏了。\n\n选择所有适用项。", "solution": "该问题要求评估几个关于梯度通过最大池化层进行反向传播的陈述。问题的核心在于理解如何应用链式法则，特别是在池化窗口内因最大值之间存在平局导致 `max` 函数不可微的情况下。\n\n首先，我们必须执行最大池化操作的前向传播。输入是一个 $4 \\times 4$ 的矩阵 $X$，池化窗口大小为 $2 \\times 2$，步幅为 $2$。这将产生一个 $2 \\times 2$ 的输出矩阵 $Y$。\n\n输入矩阵为：\n$$\nX = \\begin{pmatrix}\n1  3  5  5\\\\\n2  0  4  3\\\\\n0  -1  1  0\\\\\n2  2  -2  7\n\\end{pmatrix}\n$$\n\n四个池化窗口及其最大值如下：\n1.  左上（TL）窗口，对应于 $Y_{11}$：\n    $X_{1:2, 1:2} = \\begin{pmatrix} 1  3 \\\\ 2  0 \\end{pmatrix}$。唯一的最大值是 $3$，位于 $X$ 的索引 $(1,2)$ 处。所以，$Y_{11} = 3$。\n\n2.  右上（TR）窗口，对应于 $Y_{12}$：\n    $X_{1:2, 3:4} = \\begin{pmatrix} 5  5 \\\\ 4  3 \\end{pmatrix}$。最大值是 $5$，出现了两次，位于 $X$ 的索引 $(1,3)$ 和 $(1,4)$ 处。这是一个平局。所以，$Y_{12} = 5$。\n\n3.  左下（BL）窗口，对应于 $Y_{21}$：\n    $X_{3:4, 1:2} = \\begin{pmatrix} 0  -1 \\\\ 2  2 \\end{pmatrix}$。最大值是 $2$，出现了两次，位于 $X$ 的索引 $(4,1)$ 和 $(4,2)$ 处。这是一个平局。所以，$Y_{21} = 2$。\n\n4.  右下（BR）窗口，对应于 $Y_{22}$：\n    $X_{3:4, 3:4} = \\begin{pmatrix} 1  0 \\\\ -2  7 \\end{pmatrix}$。唯一的最大值是 $7$，位于 $X$ 的索引 $(4,4)$ 处。所以，$Y_{22} = 7$。\n\n输出矩阵为 $Y = \\begin{pmatrix} 3  5 \\\\ 2  7 \\end{pmatrix}$。\n\n接下来，我们执行反向传播来计算梯度 $\\frac{\\partial L}{\\partial X}$。上游梯度给出如下：\n$$\nG = \\frac{\\partial L}{\\partial Y} = \\begin{pmatrix} 1  -1 \\\\ 2  0 \\end{pmatrix}\n$$\n梯度 $\\frac{\\partial L}{\\partial X}$ 是一个 $4 \\times 4$ 的矩阵，初始化为全零。链式法则表明 $(\\frac{\\partial L}{\\partial X})_{ij} = \\sum_{k,l} \\frac{\\partial L}{\\partial Y_{kl}} \\frac{\\partial Y_{kl}}{\\partial X_{ij}}$。对于最大池化，这意味着对于每个输出元素 $Y_{kl}$，其上游梯度 $\\frac{\\partial L}{\\partial Y_{kl}}$ 被传递到相应窗口中作为最大值的那个（或那些）输入元素 $X_{ij}$。\n\n让我们逐一评估每个选项：\n\n**A. 在字典序平局打破规则下，只有在 $(1,2)$、$(1,3)$、$(4,1)$ 和 $(4,4)$ 位置的 $\\partial L / \\partial X$ 条目分别接收上游值 $1$、$-1$、$2$ 和 $0$，且 $\\partial L / \\partial X$ 的所有其他条目都为 $0$。**\n\n规则是选择具有最小字典序索引（行，列）的最大值。\n\n- **左上窗口 ($Y_{11}$):** 最大值在 $(1,2)$ 处是唯一的。上游梯度 $G_{11} = 1$ 被传递到 $(\\frac{\\partial L}{\\partial X})_{12}$。因此，$(\\frac{\\partial L}{\\partial X})_{12} = 1$。\n- **右上窗口 ($Y_{12}$):** 在索引 $(1,3)$ 和 $(1,4)$ 处，最大值 $5$ 存在平局。根据字典序，$(1,3)$ 小于 $(1,4)$。因此，全部上游梯度 $G_{12} = -1$ 被传递到 $(\\frac{\\partial L}{\\partial X})_{13}$。因此，$(\\frac{\\partial L}{\\partial X})_{13} = -1$。\n- **左下窗口 ($Y_{21}$):** 在索引 $(4,1)$ 和 $(4,2)$ 处，最大值 $2$ 存在平局。根据字典序，$(4,1)$ 小于 $(4,2)$。因此，全部上游梯度 $G_{21} = 2$ 被传递到 $(\\frac{\\partial L}{\\partial X})_{41}$。因此，$(\\frac{\\partial L}{\\partial X})_{41} = 2$。\n- **右下窗口 ($Y_{22}$):** 最大值在 $(4,4)$ 处是唯一的。上游梯度 $G_{22} = 0$ 被传递到 $(\\frac{\\partial L}{\\partial X})_{44}$。因此，$(\\frac{\\partial L}{\\partial X})_{44} = 0$。\n\n综合这些，矩阵 $\\frac{\\partial L}{\\partial X}$ 在位置 $(1,2), (1,3), (4,1), (4,4)$ 处有非零条目（或接收梯度的条目），其值分别为 $1, -1, 2, 0$。所有其他条目均为 $0$。这与陈述完全匹配。\n**结论：正确。**\n\n**B. 如果转而采用“在所有最大值之间平分”的规则，那么在右上窗口中，位于 $(1,3)$ 和 $(1,4)$ 的两个相等最大值中的每一个都将接收 $\\frac{-1}{2}$；在左下窗口中，位于 $(4,1)$ 和 $(4,2)$ 的两个相等最大值中的每一个都将接收 $\\frac{2}{2}$，从而产生一个 $\\max$ 算子的确定性有效次梯度。**\n\n这个选项提出了另一种确定性的平局打破规则。\n- **右上窗口 ($Y_{12}$):** 在 $(1,3)$ 和 $(1,4)$ 有两个最大值。上游梯度 $G_{12} = -1$ 在它们之间平分。每个接收的梯度为 $\\frac{-1}{2}$。这部分陈述是正确的。\n- **左下窗口 ($Y_{21}$):** 在 $(4,1)$ 和 $(4,2)$ 有两个最大值。上游梯度 $G_{21} = 2$ 被平分。每个接收的梯度为 $\\frac{2}{2}=1$。陈述中说每个接收 $\\frac{2}{2}$，这是正确的。\n“平分”规则是确定性的，因为它对于相同的输入总是给出相同的结果。所得的梯度是一个有效的次梯度，因为它对应于选择构成次微分的基向量的凸组合系数相等，即对于 $k$ 个最大值，$\\alpha_i = 1/k$。这是一个有效的选择。\n**结论：正确。**\n\n**C. 在没有任何明确的平局处理规则的情况下，即使窗口中存在平局，链式法则也能保证 $\\partial L / \\partial X$ 是唯一的，因为上游梯度 $G$ 是固定的。**\n\n这个陈述从根本上是错误的。当最大值存在平局时，$\\max$ 函数是不可微的。在这些点上，梯度没有定义。取而代之的是，存在一个有效梯度的集合，称为次微分。例如，在右上窗口中，最大值在 $X_{13}$ 和 $X_{14}$ 之间打平，对于任何 $\\alpha \\in [0,1]$，将梯度以 $\\alpha G_{12}$ 分配给 $(\\frac{\\partial L}{\\partial X})_{13}$ 和以 $(1-\\alpha) G_{12}$ 分配给 $(\\frac{\\partial L}{\\partial X})_{14}$ 的任何形式都是一个有效的次梯度。$\\alpha$ 的选择不是由链式法则本身决定的；它需要一个额外的规则（平局打破规则）。上游梯度 $G$ 是固定的这一事实并不能解决将梯度传递到*何处*的模糊性。由于梯度 $\\frac{\\partial L}{\\partial X}$ 有多个有效的选择，所以它不是唯一的。\n**结论：不正确。**\n\n**D. 使用具有不确定性平局解决方案的实现（例如，在不同的运行中以不同的顺序遇到相等的元素）可能导致对于此输入 $X$，在存在平局的窗口处，$\\partial L / \\partial X$ 在不同运行中会有所不同，即使 $G$ 是固定的；因此，对于这些输入，可复现性被破坏了。**\n\n这个陈述描述了在 C 中讨论的梯度非唯一性的一个实际后果。由于在平局点梯度不是唯一的，任何实现都必须做出选择。如果实现的选择不是确定性的（例如，依赖于在并行环境中可能变化的内存访问模式），那么不同的运行可能会产生不同但同样有效的次梯度。对于给定的输入 $X$，在右上和左下窗口中都存在平局。一个实现可能会选择它遇到的第一个最大值。如果扫描窗口中元素的顺序不固定，“第一个”最大值在不同的运行中可能不同。对于右上窗口，一次运行可能选择 $(1,3)$ 而另一次运行可能选择 $(1,4)$，导致不同的 $\\frac{\\partial L}{\\partial X}$ 矩阵。这就是不可复现性的定义。\n**结论：正确。**", "answer": "$$\\boxed{ABD}$$", "id": "3100048"}]}