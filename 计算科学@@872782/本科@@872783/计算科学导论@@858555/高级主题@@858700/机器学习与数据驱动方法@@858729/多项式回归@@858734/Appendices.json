{"hands_on_practices": [{"introduction": "在构建多项式回归模型时，一个核心问题是选择合适的多项式阶数。虽然增加阶数总能改善模型对训练数据的拟合度，这可能导致模型过于复杂，即过拟合。我们需要一种有原则的方法来判断模型复杂度的提升是否具有统计显著性，而非仅仅源于随机性。本实践将引导你使用嵌套模型的F检验方法，来客观地比较二次模型和三次模型，这是构建简洁且泛化能力强的模型的关键技能。[@problem_id:3158769]", "problem": "考虑在 $n$ 个样本上观测到的一个标量响应 $y$ 和一个标量预测变量 $x$。假设在经典线性模型设定下，满足高斯-马尔可夫假设且误差呈正态分布：数据由 $y = X \\beta + \\varepsilon$ 生成，其中 $X$ 是一个固定的设计矩阵，$\\beta$ 是一个固定但未知的参数向量，误差向量 $\\varepsilon$ 的各分量相互独立，且服从分布 $\\mathcal{N}(0,\\sigma^2)$，其中 $\\sigma^2$ 为未知方差。您将比较两个关于 $y$作为$x$的函数的嵌套多项式回归模型：一个2次简化模型（包括截距项、$x$ 和 $x^2$）和一个3次完整模型（额外包括 $x^3$）。任务是通过在正态线性模型下进行嵌套模型检验，来判断添加 $x^3$ 项是否在拟合优度上提供了统计显著的改进。\n\n编写一个程序，对下述每个测试用例，纯粹以数学方式执行以下步骤：\n- 构建简化模型（设计矩阵为 $X_{\\text{red}} = [\\mathbf{1}, x, x^2]$）和完整模型（设计矩阵为 $X_{\\text{full}} = [\\mathbf{1}, x, x^2, x^3]$），其中 $\\mathbf{1}$ 表示一个长度为 $n$ 的全1列向量。\n- 使用普通最小二乘法（OLS）通过最小化残差平方和来拟合每个模型，从而得到残差平方和 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$。\n- 使用适用于正态分布误差的线性模型的标准嵌套模型检验框架，计算一个比较 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$ 的检验统计量，评估其原假设分布，并计算一个p值，该p值用于检验在给定简化模型项的条件下，三次项没有效应的假设。\n- 判断在给定测试用例的指定显著性水平 $\\alpha$ 下，该改进是否具有统计显著性，并返回一个布尔值，以指示三次项是否显著。\n\n您的程序必须处理以下测试套件。每个测试用例以 $x$ 和 $y$ 值的数组（按顺序列出）以及一个显著性水平 $\\alpha$ 的形式给出：\n1. 用例A（清晰的三次信号，无噪声）：\n   - $x = [-2, -1, 0, 1, 2, 3]$\n   - $y = 0.5 + 0.2 x - 0.1 x^2 + 1.0 x^3$ 在给定的 $x$ 值上按元素计算。\n   - $\\alpha = 0.05$。\n2. 用例B（带微小噪声的二次信号）：\n   - $x = [-2, -1, 0, 1, 2, 3]$\n   - $y = 1.0 + 0.5 x - 0.3 x^2 + \\epsilon$，其中 $\\epsilon$ 按元素给出为 $\\epsilon = [0.005, -0.010, 0.0125, -0.0075, 0.0000, 0.0100]$。\n   - $\\alpha = 0.05$。\n3. 用例C（小样本，带微小噪声的二次信号）：\n   - $x = [-2, -1, 0, 1, 2]$\n   - $y = 0.4 + 0.1 x - 0.2 x^2 + \\epsilon$，其中 $\\epsilon$ 按元素给出为 $\\epsilon = [0.005, -0.003, 0.002, -0.004, 0.001]$。\n   - $\\alpha = 0.05$。\n\n所有数组都必须被视为精确值，不进行任何随机化处理。不存在物理单位；所有量均视为无量纲数。不涉及角度，也不需要百分比格式的输出。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。每个条目按上述顺序对应一个测试用例，并且是一个布尔值，指示在指定的 $\\alpha$ 水平下，包含 $x^3$ 是否具有统计显著性。例如，如果三次项在用例A中显著，而在用例B和C中不显著，则输出可能看起来像 $[{\\rm True},{\\rm False},{\\rm False}]$。", "solution": "该问题要求使用统计假设检验来比较两个嵌套的多项式回归模型。目的是确定在二次模型中添加一个三次项 $x^3$ 是否能在解释响应变量 $y$ 的变异方面提供统计上显著的改进。这是一个标准的模型选择问题，可以使用针对嵌套线性模型的F检验来解决。解决方案的步骤是：定义模型，计算它们各自的拟合优度，构造检验统计量，并将其得出的p值与给定的显著性水平 $\\alpha$ 进行比较。\n\n首先，我们定义这两个模型。简化模型 $\\mathcal{M}_{\\text{red}}$ 是一个2次多项式。其对应的线性模型方程为：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\varepsilon_i $$\n对于 $n$ 个观测值，这可以写成矩阵形式 $y = X_{\\text{red}}\\beta_{\\text{red}} + \\varepsilon$。设计矩阵 $X_{\\text{red}}$ 是一个 $n \\times 3$ 的矩阵，其列分别对应截距项、$x$ 和 $x^2$：\n$$ X_{\\text{red}} = [\\mathbf{1}, x, x^2] $$\n简化模型中的参数数量为 $p_{\\text{red}} = 3$。\n\n完整模型 $\\mathcal{M}_{\\text{full}}$ 是一个3次多项式。其方程为：\n$$ y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + \\varepsilon_i $$\n在矩阵形式下为 $y = X_{\\text{full}}\\beta_{\\text{full}} + \\varepsilon$。设计矩阵 $X_{\\text{full}}$ 是一个 $n \\times 4$ 的矩阵：\n$$ X_{\\text{full}} = [\\mathbf{1}, x, x^2, x^3] $$\n完整模型中的参数数量为 $p_{\\text{full}} = 4$。\n\n两个模型都使用普通最小二乘法（OLS）进行拟合，该方法找到使残差平方和（RSS）最小化的参数估计值 $\\hat{\\beta}$。对于一个设计矩阵为 $X$ 的通用模型，其RSS由下式给出：\n$$ RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\| y - X\\hat{\\beta} \\|_2^2 $$\nOLS解为 $\\hat{\\beta} = (X^T X)^{-1}X^T y$。设 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$ 分别为简化模型和完整模型的最小化残差平方和。由于完整模型更复杂（它包含简化模型的所有项外加一项），它对数据的拟合总是至少与简化模型一样好，这意味着 $RSS_{\\text{full}} \\le RSS_{\\text{red}}$。\n\n问题的核心是检验拟合优度的改善（由差值 $RSS_{\\text{red}} - RSS_{\\text{full}}$ 度量）是否大到可以被认为是统计显著的，还是小到可以合理解释为随机偶然性。这被表述为一个假设检验：\n- 原假设 ($H_0$)：三次项的系数为零（$\\beta_3 = 0$）。简化模型是充分的。\n- 备择假设 ($H_1$)：三次项的系数非零（$\\beta_3 \\ne 0$）。完整模型提供了显著的改进。\n\n为检验这些假设，我们使用嵌套模型的F统计量。该统计量将每个额外参数带来的RSS减少量与来自完整模型的估计误差方差进行比较。其定义如下：\n$$ F = \\frac{(RSS_{\\text{red}} - RSS_{\\text{full}}) / (p_{\\text{full}} - p_{\\text{red}})}{RSS_{\\text{full}} / (n - p_{\\text{full}})} $$\n分子代表每个附加参数带来的RSS平均减少量。附加参数的数量是分子自由度，$df_1 = p_{\\text{full}} - p_{\\text{red}} = 4 - 3 = 1$。分母 $MSE_{\\text{full}} = RSS_{\\text{full}} / (n - p_{\\text{full}})$ 是完整模型的均方误差，在假设完整模型设定正确的情况下，它是误差方差 $\\sigma^2$ 的无偏估计。分母自由度为 $df_2 = n - p_{\\text{full}}$。\n\n在原假设 $H_0$ 下，此F统计量服从自由度为 $df_1$ 和 $df_2$ 的F分布，记作 $F \\sim F_{df_1, df_2}$。\n\n为了做出决策，我们计算p值，它是在假设 $H_0$ 为真的前提下，观测到比计算出的F统计量（$F_{\\text{obs}}$）更极端或同样极端的F统计量的概率。对于F检验，这是一个单尾概率：\n$$ p\\text{-value} = P(F_{df_1, df_2} \\ge F_{\\text{obs}}) $$\n该概率使用F分布的生存函数（或互补累积分布函数）来计算。\n\n最终决策通过将p值与预先设定的显著性水平 $\\alpha$ 进行比较来做出：\n- 如果 $p\\text{-value}  \\alpha$，我们拒绝 $H_0$。证据表明三次项具有统计显著性。\n- 如果 $p\\text{-value} \\ge \\alpha$，我们未能拒绝 $H_0$。没有足够的证据断定三次项是显著的。\n\n当完整模型完美拟合数据时，会出现一种特殊情况，即 $RSS_{\\text{full}} = 0$。这种情况发生在用例A中，其数据是由一个三次多项式无噪声生成的。在这种情况下，F统计量的分母变为 $0$，导致数学上无穷大的F统计量。无穷大的检验统计量对应于 $0$ 的p值。因此，对于任何正的 $\\alpha$，$H_0$ 都会被拒绝。\n\n实现部分将为每个测试用例计算这些量。设计矩阵 $X_{\\text{red}}$ 和 $X_{\\text{full}}$ 根据给定的 $x$ 值构建。残差平方和 $RSS_{\\text{red}}$ 和 $RSS_{\\text{full}}$ 通过使用一个高效的数值线性代数程序（如 `numpy.linalg.lstsq`）来进行最小二乘计算得到。然后计算F统计量及其对应的p值（使用 `scipy.stats.f.sf`），从而得出每个用例的最终布尔结论。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f\n\ndef solve():\n    \"\"\"\n    Performs nested model F-tests for three polynomial regression cases.\n    \"\"\"\n\n    # Helper function to perform the nested F-test for a single case.\n    def perform_nested_model_test(x_vals, y_vals, alpha):\n        \"\"\"\n        Compares a degree 2 vs. degree 3 polynomial model via an F-test.\n\n        Args:\n            x_vals (np.ndarray): The predictor variable values.\n            y_vals (np.ndarray): The response variable values.\n            alpha (float): The significance level for the test.\n\n        Returns:\n            bool: True if the cubic term is significant, False otherwise.\n        \"\"\"\n        n = len(x_vals)\n        p_red = 3  # degree 2 model: intercept, x, x^2\n        p_full = 4 # degree 3 model: intercept, x, x^2, x^3\n\n        # Construct the design matrices for the reduced and full models.\n        # np.vander with increasing=True produces columns [x^0, x^1, x^2, ...]\n        X_red = np.vander(x_vals, N=p_red, increasing=True)\n        X_full = np.vander(x_vals, N=p_full, increasing=True)\n\n        # Calculate the Residual Sum of Squares (RSS) for both models.\n        # np.linalg.lstsq returns RSS as the second element of its output tuple.\n        # This is numerically stabler than inverting the matrix X.T @ X.\n        # The returned RSS is an array, so we extract the scalar value.\n        rss_red = np.linalg.lstsq(X_red, y_vals, rcond=None)[1][0]\n        \n        # When the full model provides a perfect fit, lstsq might return an empty\n        # array for the residuals. In this case, RSS is exactly 0.\n        lstsq_full_result = np.linalg.lstsq(X_full, y_vals, rcond=None)\n        rss_full = lstsq_full_result[1][0] if len(lstsq_full_result[1]) > 0 else 0.0\n\n        # Define degrees of freedom for the F-test\n        df1 = p_full - p_red\n        df2 = n - p_full\n        \n        # Handle the case where the denominator of the F-statistic is zero\n        if rss_full  np.finfo(float).eps:\n            # If RSS_full is effectively zero, the full model is a perfect fit.\n            # This implies an infinite F-statistic and a p-value of 0.\n            p_value = 0.0\n        elif df2 = 0:\n            # If there are no degrees of freedom for the error term, the test\n            # is not well-defined. By convention, we can consider it not significant.\n            # This case does not occur in the given problem set.\n            return False\n        else:\n            # Calculate the F-statistic\n            f_statistic = ((rss_red - rss_full) / df1) / (rss_full / df2)\n            \n            # Calculate the p-value using the survival function (1 - CDF) of the F-distribution\n            p_value = f.sf(f_statistic, df1, df2)\n\n        # The cubic term is significant if the p-value is less than the significance level alpha\n        return p_value  alpha\n\n    # Define the test cases from the problem statement.\n    case_a_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    case_a_y = 0.5 + 0.2*case_a_x - 0.1*case_a_x**2 + 1.0*case_a_x**3\n    case_a_alpha = 0.05\n    \n    case_b_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0, 3.0])\n    case_b_noise = np.array([0.005, -0.010, 0.0125, -0.0075, 0.0000, 0.0100])\n    case_b_y = 1.0 + 0.5*case_b_x - 0.3*case_b_x**2 + case_b_noise\n    case_b_alpha = 0.05\n    \n    case_c_x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n    case_c_noise = np.array([0.005, -0.003, 0.002, -0.004, 0.001])\n    case_c_y = 0.4 + 0.1*case_c_x - 0.2*case_c_x**2 + case_c_noise\n    case_c_alpha = 0.05\n\n    test_cases = [\n        (case_a_x, case_a_y, case_a_alpha),\n        (case_b_x, case_b_y, case_b_alpha),\n        (case_c_x, case_c_y, case_c_alpha),\n    ]\n\n    results = []\n    for x_vals, y_vals, alpha in test_cases:\n        is_significant = perform_nested_model_test(x_vals, y_vals, alpha)\n        results.append(is_significant)\n\n    # Final print statement in the exact required format.\n    # The expected format is `[True,False,False]`.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3158769"}, {"introduction": "当我们尝试使用高阶多项式进行回归分析时，即使模型选择是合理的，也常常会遇到数值计算上的挑战。使用标准基函数 $\\{1, x, x^2, \\dots, x^d\\}$ 构建的设计矩阵，其列向量之间会变得高度相关，导致矩阵病态，进而影响计算结果的稳定性和准确性。本实践通过比较标准范德蒙矩阵与基于正交勒让德多项式构建的矩阵的条件数，让你亲身体验这一问题，并掌握一种构建更可靠、更精确回归模型的实用技术。[@problem_id:2425191]", "problem": "您的任务是定量比较由标准单项式基与正交多项式基构建的多项式回归设计矩阵的数值条件。在多项式回归中，通过选择一组基函数并求解线性最小二乘问题来对数据进行建模。解的数值稳定性在很大程度上取决于设计矩阵的条件数。矩阵的谱条件数根据通过奇异值分解计算出的奇异值来定义，并被广泛用于评估数值稳定性。与朴素的单项式相比，正交多项式可以减少多重共线性并改善条件数，尤其是在输入被缩放到标准区间时。\n\n从最小二乘回归、正交多项式和谱条件数的基本定义出发，实现一个程序，在每个测试用例中，使用相同的输入构建两个设计矩阵：\n- 一个由标准基 $\\{1, x, x^{2}, \\dots \\}$ 构建的单项式范德蒙型设计矩阵。\n- 一个由在相同点上经过仿射归一化后求值的第一类 Legendre 多项式构建的设计矩阵。\n\n对于每个测试用例，请遵循以下要求：\n1. 给定任何实数区间中的一组输入点 $\\{x_{i}\\}_{i=0}^{n-1}$，首先使用仿射映射将它们归一化到区间 $[-1,1]$ 内的 $\\{t_{i}\\}$：\n$$\nt_{i} = \\frac{2\\,(x_{i} - x_{\\min})}{x_{\\max} - x_{\\min}} - 1,\n$$\n其中 $x_{\\min} = \\min_{i} x_{i}$ 且 $x_{\\max} = \\max_{i} x_{i}$。\n2. 对于一个指定的非负整数次数 $d$，构建两个 $n \\times (d+1)$ 的设计矩阵：\n   - 单项式设计矩阵 $A$，其元素为 $A_{i,k} = t_{i}^{k}$，其中 $k = 0, 1, \\dots, d$。\n   - Legendre 设计矩阵 $B$，其元素为 $B_{i,k} = P_{k}(t_{i})$，其中 $k = 0, 1, \\dots, d$，$P_{k}$ 表示在 $[-1,1]$ 上的第 $k$ 阶 Legendre 多项式。\n3. 使用奇异值分解计算谱条件数 $\\kappa_{2}(A)$ 和 $\\kappa_{2}(B)$。使用定义：\n$$\n\\kappa_{2}(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)},\n$$\n其中 $\\sigma_{\\max}(M)$ 和 $\\sigma_{\\min}(M)$ 分别是 $M$ 的最大和最小奇异值。\n4. 对每个测试用例，计算改进比率：\n$$\nr = \\frac{\\kappa_{2}(A)}{\\kappa_{2}(B)}.\n$$\n报告 $r$ 的值，精确到六位小数。\n\n角度单位：下面定义点时，凡是用到余弦函数，其参数单位均为弧度。\n\n您的程序必须实现以下测试套件，并以指定的最终输出格式生成结果。\n\n测试套件：\n- 用例 $1$（等距分布，中等次数）：$n = 50$，$d = 12$，输入点为 $x_{i} = -1 + \\frac{2\\,i}{n-1}$，其中 $i = 0, 1, \\dots, n-1$。\n- 用例 $2$（向一个端点聚集）：$n = 50$，$d = 12$，输入点为 $x_{i} = 1 - \\exp\\left(-\\frac{5\\,i}{n-1}\\right)$，其中 $i = 0, 1, \\dots, n-1$。\n- 用例 $3$（Chebyshev 型内部集中）：$n = 30$，$d = 14$，输入点为 $x_{i} = \\cos\\left(\\frac{\\pi\\,(i+0.5)}{n}\\right)$，其中 $i = 0, 1, \\dots, n-1$。\n- 用例 $4$（宽原始范围，然后归一化）：$n = 40$，$d = 18$，输入点为 $x_{i} = 0 + \\frac{1000\\,i}{n-1}$，其中 $i = 0, 1, \\dots, n-1$。\n\n所有计算必须以双精度进行。对于每个用例，确保 $d+1 \\le n$，以便对于不同的 $t_{i}$，设计矩阵是满列秩的。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含四个测试用例的改进比率，以逗号分隔的列表形式包含在方括号内，按用例 $1$ 到 $4$ 的顺序排列，每个值都四舍五入到恰好六位小数，例如：“[1.234567,8.901234,5.678901,2.345678]”。不应打印任何其他文本。所有值都是无单位的实数，表示为小数点后有六位数字的十进制浮点数。", "solution": "问题陈述已经过验证，并被认定为有效。它具有科学依据，问题定义明确且客观。它提出了一个清晰的计算工程任务，特别是在回归方法的数值分析领域。所有必要的数据和定义都已提供，没有矛盾或模糊之处。因此，我将着手提供一个完整的解决方案。\n\n任务是定量评估在多项式回归问题中，使用正交多项式基（Legendre 多项式）代替标准单项式基时，数值条件的改善情况。该分析的核心在于设计矩阵的属性，该矩阵将回归系数映射到预测值。\n\n在多项式回归问题中，我们试图用一个 $d$ 次多项式来对一组数据点 $\\{ (x_i, y_i) \\}_{i=0}^{n-1}$ 进行建模。该多项式可以表示为基函数 $\\{ \\phi_k(x) \\}_{k=0}^{d}$ 的线性组合：\n$$\nf(x) = \\sum_{k=0}^{d} c_k \\phi_k(x)\n$$\n目标是找到系数 $c_k$，使得模型预测值 $f(x_i)$ 与观测数据 $y_i$ 之间的平方误差总和最小。这是一个线性最小二乘问题。系数向量 $C = [c_0, c_1, \\dots, c_d]^T$ 与预测值向量 $Y_{pred} = [f(x_0), f(x_1), \\dots, f(x_{n-1})]^T$ 之间的关系由以下矩阵方程给出：\n$$\nY_{pred} = M C\n$$\n其中 $M$ 是一个 $n \\times (d+1)$ 的设计矩阵，其元素为 $M_{ik} = \\phi_k(x_i)$。求解系数向量 $C$ 的数值稳定性在很大程度上取决于这个设计矩阵 $M$ 的条件数。对此的一个常用度量是谱条件数 $\\kappa_2(M)$，定义为 $M$ 的最大奇异值与最小奇异值之比：\n$$\n\\kappa_2(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)}\n$$\n大的条件数表示一个病态问题，其中输入数据的微小误差可能导致计算出的系数出现巨大误差，从而使解变得不可靠。\n\n该问题要求比较两种基函数的选择：\n$1$. 标准单项式基 $\\phi_k(x) = x^k$。对于高次数 $d$ 或对于远离原点的聚集输入点 $x_i$，所得设计矩阵（一个范德蒙型矩阵）的列会变得近似线性相关。这会导致一个非常大的条件数。\n$2$. Legendre 多项式基 $\\phi_k(x) = P_k(x)$。Legendre 多项式 $\\{ P_k(t) \\}_{k=0}^{\\infty}$ 在区间 $[-1, 1]$ 上关于 $L^2$ 内积是正交的，即对于 $j \\neq k$，$\\int_{-1}^{1} P_j(t) P_k(t) dt = 0$。这种正交性有助于确保相应设计矩阵的列远非线性相关，从而得到显著更小的条件数和更稳定的数值问题。\n\n为确保实现正交性的好处，输入数据点必须映射到 Legendre 多项式定义的区间 $[-1, 1]$ 上。问题为一组点 $\\{x_i\\}$ 指定了以下仿射变换：\n$$\nt_i = \\frac{2(x_i - x_{\\min})}{x_{\\max} - x_{\\min}} - 1\n$$\n其中 $x_{\\min}$ 和 $x_{\\max}$ 是集合 $\\{x_i\\}$ 中的最小值和最大值。在构建设计矩阵之前，此变换应用于所有输入点。注意，如果 $x_{\\max} = x_{\\min}$，则所有点都相同，此公式无定义。然而，问题确保使用不同的点，因此 $x_{\\max} > x_{\\min}$。\n\n解决此问题的算法步骤如下：\n对于每个测试用例，由点数 $n$、多项式次数 $d$ 和生成点 $\\{x_i\\}$ 的公式指定：\n$1$. 生成 $n$ 个输入点 $\\{x_i\\}_{i=0}^{n-1}$。\n$2$. 计算 $x_{\\min} = \\min_{i} x_i$ 和 $x_{\\max} = \\max_{i} x_i$。\n$3$. 使用仿射映射将点归一化，以获得在区间 $[-1, 1]$ 内的 $\\{t_i\\}_{i=0}^{n-1}$。\n$4$. 构建 $n \\times (d+1)$ 的单项式设计矩阵 $A$，其中第 $i$ 行第 $k$ 列的元素是 $A_{ik} = t_i^k$，对于 $i \\in \\{0, \\dots, n-1\\}$ 和 $k \\in \\{0, \\dots, d\\}$。\n$5$. 构建 $n \\times (d+1)$ 的 Legendre 设计矩阵 $B$，其中 $B_{ik} = P_k(t_i)$。Legendre 多项式 $P_k(t)$ 使用标准的三项递推关系计算：\n   $$ P_0(t) = 1 $$\n   $$ P_1(t) = t $$\n   $$ (k+1)P_{k+1}(t) = (2k+1)tP_k(t) - kP_{k-1}(t) \\quad \\text{for } k \\geq 1 $$\n   我们将使用一个预先存在的库函数进行此评估，这是计算科学中的标准做法。\n$6$. 对于每个矩阵 $A$ 和 $B$，使用奇异值分解（SVD）计算其奇异值。设奇异值为 $\\{\\sigma_j\\}$。\n$7$. 计算每个矩阵的条件数：$\\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A)$ 和 $\\kappa_2(B) = \\sigma_{\\max}(B) / \\sigma_{\\min}(B)$。\n$8$. 计算改进比率 $r = \\kappa_2(A) / \\kappa_2(B)$。\n$9$. 按要求报告四舍五入到六位小数的 $r$ 值。\n\n该过程针对四个指定的测试用例中的每一个都得以实现，并且结果被汇总到指定的输出格式中。使用双精度浮点算术是标准的，并且对于此分析是足够的。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\n\ndef solve():\n    \"\"\"\n    Computes the improvement in design matrix conditioning by using Legendre\n    polynomials over monomials for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 50, \"d\": 12,\n            \"x_generator\": lambda n: -1.0 + 2.0 * np.arange(n) / (n - 1)\n        },\n        {\n            \"n\": 50, \"d\": 12,\n            \"x_generator\": lambda n: 1.0 - np.exp(-5.0 * np.arange(n) / (n - 1))\n        },\n        {\n            \"n\": 30, \"d\": 14,\n            \"x_generator\": lambda n: np.cos(np.pi * (np.arange(n) + 0.5) / n)\n        },\n        {\n            \"n\": 40, \"d\": 18,\n            \"x_generator\": lambda n: 1000.0 * np.arange(n) / (n - 1)\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n = case[\"n\"]\n        d = case[\"d\"]\n        \n        # Step 1: Generate input points\n        x = case[\"x_generator\"](n).astype(np.float64)\n\n        # Step 2: Normalize points to [-1, 1]\n        x_min, x_max = np.min(x), np.max(x)\n        \n        # Handle the case where all points are the same to avoid division by zero.\n        if np.isclose(x_max, x_min):\n            t = np.zeros_like(x)\n        else:\n            t = 2.0 * (x - x_min) / (x_max - x_min) - 1.0\n\n        # Step 3: Construct the monomial design matrix A (Vandermonde matrix)\n        # A_{ik} = t_i^k for k = 0, ..., d\n        # numpy.vander with increasing=True creates columns t^0, t^1, ..., t^d\n        A = np.vander(t, N=d + 1, increasing=True)\n\n        # Step 4: Construct the Legendre design matrix B\n        # B_{ik} = P_k(t_i) for k = 0, ..., d\n        B = np.zeros((n, d + 1), dtype=np.float64)\n        for k in range(d + 1):\n            B[:, k] = eval_legendre(k, t)\n\n        # Step 5: Compute the singular values for both matrices\n        # SVD returns singular values sorted in descending order.\n        # compute_uv=False is an optimization as we only need the singular values.\n        s_A = np.linalg.svd(A, compute_uv=False)\n        s_B = np.linalg.svd(B, compute_uv=False)\n\n        # Step 6: Compute the spectral condition numbers\n        # kappa(M) = sigma_max / sigma_min\n        kappa_A = s_A[0] / s_A[-1]\n        kappa_B = s_B[0] / s_B[-1]\n\n        # Step 7: Compute the improvement ratio\n        improvement_ratio = kappa_A / kappa_B\n        \n        # Step 8: Format the result and add to the list\n        results.append(f\"{improvement_ratio:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2425191"}, {"introduction": "现实世界的数据集很少是完美无瑕的，它们常常包含一些会严重扭曲模型拟合结果的离群点（outliers）。标准的普通最小二乘法（OLS）对这些离群点极为敏感，一个极端值就可能“带偏”整个模型。本实践将量化展示在存在“恶意”数据点时OLS的脆弱性，并引入一种基于Huber损失的稳健回归方法作为替代。通过实现和比较这两种方法，你将深入理解如何构建能够抵御不完美数据影响的、更具鲁棒性的模型。[@problem_id:3175135]", "problem": "您将通过比较普通最小二乘法 (OLS) 和一种基于 Huber 损失的稳健变体，研究多项式回归对对抗性数据投毒的敏感性。您的任务是从基本原理出发，实现基为 $\\{1, x, x^2, \\dots, x^d\\}$ 的 $d$ 次多项式回归，并量化一小组对抗性数据点如何影响拟合模型。评估将在几个预定义的测试用例上进行，以涵盖典型情况、边界情况和高杠杆场景。最终交付成果是一个完整的程序，该程序能够构建数据集、拟合两种模型、计算指定的敏感性指标，并以下面指定的确切格式打印结果。\n\n基本出发点：\n- 多项式回归的定义：给定数据 $(x_i, y_i)$，通过最小化一个数据保真度准则来拟合一个多项式 $p(x) = \\sum_{j=0}^{d} \\beta_j x^j$。\n- 对于 OLS，数据保真度准则是残差平方和 $\\sum_{i=1}^{n} r_i^2$，其中 $r_i = y_i - p(x_i)$。\n- 对于稳健变体，使用 Huber 损失 $\\rho_{\\delta}(r)$，其定义为\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r| \\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2,  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\n使用迭代重加权最小二乘法 (IRLS) 实现稳健拟合，其中每次迭代的权重都从 Huber 得分函数 $\\psi_{\\delta}(r) = \\frac{d}{dr}\\rho_{\\delta}(r)$ 导出，并应用于加权最小二乘求解。您必须使用中位数绝对偏差 (MAD) 来估计残差的尺度，以设定 Huber 阈值。\n\n数据生成：\n- 真实内点模型：一个三次多项式 $p_{\\text{true}}(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$，系数为 $c_3 = 0.5, c_2 = -0.2, c_1 = -1.0, c_0 = 0.3$。\n- 拟合次数：$d = 3$。\n- 内点：在 $[-1, 1]$ 上均匀生成 $n_{\\text{in}} = 30$ 个输入点 $x$。对每个点，设置 $y = p_{\\text{true}}(x) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 为独立高斯噪声，且 $\\sigma = 0.05$。使用固定的随机种子 $s = 20240217$，以确保所有测试用例的内点集相同。\n- 对抗性离群点：对于每个测试用例，将下面列出的一小组指定的 $(x, y)$ 对附加到内点集中。\n\n模型拟合要求：\n- 普通最小二乘法 (OLS)：在 $\\beta \\in \\mathbb{R}^{d+1}$ 上最小化 $\\sum_{i=1}^{n} r_i^2$。\n- 使用 Huber 损失的稳健回归：使用 IRLS 最小化 $\\sum_{i=1}^{n} \\rho_{\\delta}(r_i)$，其中 $r_i = y_i - \\sum_{j=0}^{d} \\beta_j x_i^j$。在每次迭代中，根据以下公式计算权重 $w_i$：\n$$\n\\psi_{\\delta}(r) = \n\\begin{cases}\nr,  \\text{if } |r| \\le \\delta,\\\\\n\\delta \\,\\mathrm{sign}(r),  \\text{if } |r|  \\delta,\n\\end{cases}\n\\quad\\text{and}\\quad\nw_i = \n\\begin{cases}\n1,  \\text{if } r_i = 0\\ \\text{or}\\ |r_i| \\le \\delta,\\\\\n\\frac{\\delta}{|r_i|},  \\text{if } |r_i|  \\delta.\n\\end{cases}\n$$\n使用加权最小二乘法更新 $\\beta$。使用以下规范：\n- 通过中位数绝对偏差从残差 $r$ 估计尺度 $\\hat{s}$：$\\hat{s} = 1.4826 \\cdot \\mathrm{median}(|r - \\mathrm{median}(r)|)$。如果 $\\hat{s}$ 极小，则回退到 $\\max(\\mathrm{std}(r), \\varepsilon)$。\n- Huber 阈值 $\\delta = \\kappa \\hat{s}$，其中 $\\kappa = 1.345$。\n- 当 $\\beta$ 的相对变化低于 $\\tau = 10^{-10}$ 或达到 $M = 100$ 次迭代时收敛。\n- 在任何需要避免除零错误的地方使用数值稳定常数 $\\varepsilon = 10^{-12}$。\n\n敏感性指标：\n- 设 $\\beta^{\\text{clean}}_{\\text{OLS}}$ 和 $\\beta^{\\text{clean}}_{\\text{Huber}}$ 为仅在内点上拟合的系数，$\\beta^{\\text{poison}}_{\\text{OLS}}$ 和 $\\beta^{\\text{poison}}_{\\text{Huber}}$ 为在内点和离群点组合上拟合的系数。\n- 方法 $m \\in \\{\\text{OLS}, \\text{Huber}\\}$ 的系数漂移为 $\\Delta_m = \\|\\beta^{\\text{poison}}_{m} - \\beta^{\\text{clean}}_{m}\\|_2$。\n- 漂移比率（OLS 相对于稳健方法的漂移程度）：\n$$\nR_{\\text{drift}} = \\frac{\\Delta_{\\text{OLS}} + \\varepsilon}{\\Delta_{\\text{Huber}} + \\varepsilon}.\n$$\n- 使用系数向量 $\\beta$ 在内点上的均方根误差 (RMSE) 为\n$$\n\\mathrm{RMSE}_{\\text{in}}(\\beta) = \\sqrt{\\frac{1}{n_{\\text{in}}} \\sum_{i=1}^{n_{\\text{in}}} \\left(y_i^{\\text{in}} - \\sum_{j=0}^{d} \\beta_j (x_i^{\\text{in}})^j\\right)^2 }.\n$$\n- 方法 $m$ 的 RMSE 膨胀率为\n$$\nI_m = \\frac{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{poison}}_{m})}{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{clean}}_{m})}.\n$$\n- RMSE 膨胀比率：\n$$\nR_{\\text{RMSE}} = \\frac{I_{\\text{OLS}}}{I_{\\text{Huber}}}.\n$$\n\n测试套件（固定且确定性）：\n- 全局常量：$d=3$, $c_3 = 0.5, c_2 = -0.2, c_1 = -1.0, c_0 = 0.3$, $n_{\\text{in}} = 30$, $\\sigma = 0.05$, $s = 20240217$, $\\kappa = 1.345$, $\\tau = 10^{-10}$, $M = 100$, $\\varepsilon = 10^{-12}$。\n- 情况 $1$（无投毒）：$k=0$ 个离群点。\n- 情况 $2$（少量高杠杆极端值）：$k=2$，离群点位于 $x = [-3.0, 3.0]$, $y = [60.0, -60.0]$。\n- 情况 $3$（远处点簇）：$k=5$，离群点位于 $x = [4.0, 5.0, 6.0, 7.0, 8.0]$, $y = [200.0, 200.0, 200.0, 200.0, 200.0]$。\n- 情况 $4$（内点域内的垂直离群点）：$k=5$，离群点位于 $x = [-0.8, -0.4, 0.0, 0.4, 0.8]$, $y = [40.0, -40.0, 50.0, -50.0, 45.0]$。\n\n要求的最终输出格式：\n- 您的程序必须生成单行输出，包含一个含有 $8$ 个浮点数的列表：对于每个测试用例，按 $1,2,3,4$ 的顺序，先打印 $R_{\\text{drift}}$，然后打印 $R_{\\text{RMSE}}$。每个值必须四舍五入到恰好 $6$ 位小数。输出必须是单行，包含一个用方括号括起来的逗号分隔列表，例如 $[v_1,v_2,\\dots,v_8]$，不含额外的空格或文本。\n\n注意：\n- 本问题不涉及物理单位。\n- 不涉及角度；不需要角度单位。\n- 在给定指定种子的情况下，所有计算都必须是确定性的。", "solution": "### 步骤 1：提取已知信息\n- **多项式回归模型**: $p(x) = \\sum_{j=0}^{d} \\beta_j x^j$，基为 $\\{1, x, x^2, \\dots, x^d\\}$。\n- **拟合次数**: $d = 3$。\n- **OLS 准则**: 最小化 $\\sum_{i=1}^{n} r_i^2$，其中 $r_i = y_i - p(x_i)$。\n- **Huber 损失准则**: 最小化 $\\sum_{i=1}^{n} \\rho_{\\delta}(r_i)$，其中 $\\rho_{\\delta}(r) = \\begin{cases} \\frac{1}{2} r^2,  \\text{if } |r| \\le \\delta \\\\ \\delta |r| - \\frac{1}{2}\\delta^2,  \\text{if } |r|  \\delta \\end{cases}$。\n- **用于 Huber 回归的 IRLS**: 使用迭代重加权最小二乘法。\n- **IRLS 权重**: $w_i = \\begin{cases} 1,  \\text{if } r_i = 0\\ \\text{or}\\ |r_i| \\le \\delta \\\\ \\frac{\\delta}{|r_i|},  \\text{if } |r_i|  \\delta \\end{cases}$，从得分函数 $\\psi_{\\delta}(r) = \\frac{d}{dr}\\rho_{\\delta}(r)$ 导出。\n- **尺度估计**: $\\hat{s} = 1.4826 \\cdot \\mathrm{median}(|r - \\mathrm{median}(r)|)$，如果 $\\hat{s}$ 极小，则回退到 $\\max(\\mathrm{std}(r), \\varepsilon)$。\n- **Huber 阈值**: $\\delta = \\kappa \\hat{s}$，其中 $\\kappa = 1.345$。\n- **IRLS 收敛条件**: $\\beta$ 的相对变化低于 $\\tau = 10^{-10}$ 或达到最大迭代次数 $M = 100$。\n- **数值稳定常数**: $\\varepsilon = 10^{-12}$。\n- **真实内点模型**: $p_{\\text{true}}(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$，其中 $c_3 = 0.5, c_2 = -0.2, c_1 = -1.0, c_0 = 0.3$。\n- **内点数据**: $n_{\\text{in}} = 30$ 个点，$x$ 在 $[-1, 1]$ 上均匀分布，$y = p_{\\text{true}}(x) + \\epsilon$，其中 $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$ 且 $\\sigma = 0.05$。\n- **随机种子**: $s = 20240217$。\n- **对抗性离群点**:\n    - 情况 1: $k = 0$ 个离群点。\n    - 情况 2: $k = 2$ 个离群点，位于 $(x, y) = ([-3.0, 3.0], [60.0, -60.0])$。\n    - 情况 3: $k = 5$ 个离群点，位于 $(x, y) = ([4.0, 5.0, 6.0, 7.0, 8.0], [200.0, 200.0, 200.0, 200.0, 200.0])$。\n    - 情况 4: $k = 5$ 个离群点，位于 $(x, y) = ([-0.8, -0.4, 0.0, 0.4, 0.8], [40.0, -40.0, 50.0, -50.0, 45.0])$。\n- **敏感性指标**:\n    - 系数漂移: $\\Delta_m = \\|\\beta^{\\text{poison}}_{m} - \\beta^{\\text{clean}}_{m}\\|_2$，其中 $m \\in \\{\\text{OLS}, \\text{Huber}\\}$。\n    - 漂移比率: $R_{\\text{drift}} = \\frac{\\Delta_{\\text{OLS}} + \\varepsilon}{\\Delta_{\\text{Huber}} + \\varepsilon}$。\n    - 内点上的 RMSE: $\\mathrm{RMSE}_{\\text{in}}(\\beta) = \\sqrt{\\frac{1}{n_{\\text{in}}} \\sum_{i=1}^{n_{\\text{in}}} \\left(y_i^{\\text{in}} - \\sum_{j=0}^{d} \\beta_j (x_i^{\\text{in}})^j\\right)^2 }$.\n    - RMSE 膨胀率: $I_m = \\frac{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{poison}}_{m})}{\\mathrm{RMSE}_{\\text{in}}(\\beta^{\\text{clean}}_{m})}$。\n    - RMSE 膨胀比率: $R_{\\text{RMSE}} = \\frac{I_{\\text{OLS}}}{I_{\\text{Huber}}}$。\n\n### 步骤 2：使用提取的已知信息进行验证\n- **科学依据**：该问题基于数值优化和统计学中的标准基本概念，即普通最小二乘法 (OLS)、稳健回归、Huber 损失和迭代重加权最小二乘 (IRLS) 算法。所有定义和程序都是该领域的标准。该问题在科学上是合理的。\n- **适定性**：该问题是适定的。假定设计矩阵是满列秩的（对于 $n_{\\text{in}}=30$ 个不同 $x$ 值和次数 $d=3$ 的多项式回归，这是可以保证的），OLS 会提供唯一解。Huber 损失函数是凸函数，确保了最小化问题有唯一解，而标准的 IRLS 算法是找到该解的可靠方法。\n- **目标**：问题以精确的量化术语陈述。所有参数、数据生成过程、算法和评估指标都得到了正式且无歧义的定义。\n- **完整性与一致性**：提供了所有必要的数值、常数、算法和定义。问题是自洽的，没有矛盾。固定的随机种子确保了可复现性。\n- **现实性与可行性**：该问题是一个计算模拟。指定的计算在标准数值库的能力范围之内，并且在计算上是可行的。这些值是抽象的，不代表物理量，因此避免了物理上的不可能性。\n- **结构与简易性**：问题结构良好，引导用户从数据生成到模型拟合，最后到敏感性分析。它并非微不足道，因为它需要正确实现一个非线性优化算法 (IRLS)，并仔细计算几个特定指标。\n\n### 步骤 3：结论与行动\n该问题有效。将提供一个完整的、合理的解决方案。\n\n### 基于原理的设计\n目标是比较两种回归方法——普通最小二乘法 (OLS) 和使用 Huber 损失的稳健方法——对对抗性数据投毒的敏感性。我们将实现这两种方法，用以将一个 $d$ 次多项式 $p(x) = \\sum_{j=0}^{d} \\beta_j x^j$ 拟合到被离群点污染的数据集。\n\n**1. 线性代数公式化**\n\n两种回归方法都可以在线性代数的框架内表达。对于一组 $n$ 个数据点 $(x_i, y_i)$，该多项式模型可以写成一个线性方程组 $y \\approx X\\beta$，其中 $y = [y_1, \\dots, y_n]^T$ 是观测值向量，$\\beta = [\\beta_0, \\dots, \\beta_d]^T$ 是待确定的系数向量，$X$ 是 $n \\times (d+1)$ 的设计矩阵（一个范德蒙矩阵）：\n$$\nX = \\begin{pmatrix}\n1  x_1  x_1^2  \\dots  x_1^d \\\\\n1  x_2  x_2^2  \\dots  x_2^d \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  x_n  x_n^2  \\dots  x_n^d\n\\end{pmatrix}\n$$\n残差向量定义为 $r = y - X\\beta$。\n\n**2. 普通最小二乘法 (OLS) 回归**\n\nOLS 拟合旨在寻找能够最小化残差平方和（也称为残差向量的 L2 范数平方）的系数向量 $\\beta_{\\text{OLS}}$。\n$$\n\\text{minimize} \\quad L(\\beta) = \\sum_{i=1}^{n} r_i^2 = \\|y - X\\beta\\|_2^2\n$$\n这是关于 $\\beta$ 的二次函数，其最小值可以通过将其梯度设为零来找到：$\\nabla_{\\beta} L(\\beta) = -2X^T(y - X\\beta) = 0$。这导出了著名的正规方程：\n$$\n(X^T X)\\beta = X^T y\n$$\n假设 $X^T X$ 是可逆的（如果 $X$ 是满列秩的，则该条件成立），唯一解为 $\\beta_{\\text{OLS}} = (X^T X)^{-1} X^T y$。在数值计算上，最好使用 QR 分解或 SVD 等方法来求解该系统，这些方法已封装在标准的线性最小二乘求解器中。\n\n**3. 通过 IRLS 实现使用 Huber 损失的稳健回归**\n\nOLS 损失函数的二次性质使其对离群点高度敏感，因为大的残差对总误差的贡献不成比例。稳健回归旨在通过使用一种对于大残差增长较慢的损失函数来缓解此问题。Huber 损失函数 $\\rho_{\\delta}(r)$ 是一种混合函数，对于小残差表现为二次函数，而对于大残差则表现为线性函数：\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r| \\le \\delta,\\\\\n\\delta |r| - \\frac{1}{2}\\delta^2,  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\n参数 $\\delta$ 是一个区分大小残差的阈值。目标是最小化 $\\sum_{i=1}^{n} \\rho_{\\delta}(r_i)$。由于此目标不是一个简单的二次函数，因此需要采用迭代方法。迭代重加权最小二乘法 (IRLS) 是解决此类问题的标准算法。它通过求解一系列加权最小二乘问题来工作，这些问题会收敛到稳健目标的解。\n\n最小化 Huber 目标的一阶条件是，对于每个 $j=0, \\dots, d$，都有 $\\sum_{i=1}^{n} \\psi_{\\delta}(r_i) x_{ij} = 0$，其中 $\\psi_{\\delta}(r) = \\rho'_{\\delta}(r)$ 是得分函数。通过定义一个权重函数 $w(r) = \\psi_{\\delta}(r)/r$，这个条件可以重写为 $\\sum_{i=1}^{n} w(r_i) r_i x_{ij} = 0$，这些是权重为 $w_i = w(r_i)$ 的加权最小二乘问题的正规方程。权重由下式给出：\n$$\nw_i = \\frac{\\psi_{\\delta}(r_i)}{r_i} = \n\\begin{cases}\n1,  \\text{if } |r_i| \\le \\delta, \\\\\n\\frac{\\delta}{|r_i|},  \\text{if } |r_i|  \\delta.\n\\end{cases}\n$$\n该公式引出了 IRLS 算法：\n\n1.  **初始化**: 获得系数的初始估计 $\\beta^{(0)}$。OLS 解是一个合适的选择。\n2.  **迭代**: 对于 $k = 0, 1, 2, \\dots$ 直到收敛：\n    a.  **计算残差**: $r^{(k)} = y - X\\beta^{(k)}$。\n    b.  **估计尺度**: 阈值 $\\delta$ 取决于残差的尺度。一个稳健的尺度估计是中位数绝对偏差 (MAD)：$\\hat{s} = C \\cdot \\text{median}(|r^{(k)} - \\text{median}(r^{(k)})|)$，其中 $C = 1.4826$ 使其在正态分布下成为标准差的一致估计量。如果 $\\hat{s}$ 病态地小（例如，小于一个小的常数 $\\varepsilon$），我们回退到一个更稳定的估计，即 $\\max(\\text{std}(r^{(k)}), \\varepsilon)$。\n    c.  **设定阈值**: $\\delta = \\kappa \\hat{s}$，其中 $\\kappa = 1.345$ 是一个实现高效率的标准选择。\n    d.  **计算权重**: $W^{(k)}$ 是一个对角矩阵，其对角元素为 $w_i^{(k)} = \\min(1, \\delta / |r_i^{(k)}|)$。通过设置 $w_i^{(k)}=1$ 来处理 $r_i^{(k)} = 0$ 的情况。\n    e.  **求解加权最小二乘**: 通过求解 WLS 问题来更新系数：\n        $$\n        \\beta^{(k+1)} = \\arg\\min_{\\beta} \\|(W^{(k)})^{1/2}(y - X\\beta)\\|_2^2\n        $$\n        这等同于求解加权正规方程 $(X^T W^{(k)} X)\\beta^{(k+1)} = X^T W^{(k)} y$。\n    f. **检查收敛**: 如果系数向量的相对变化低于容差 $\\tau$，即 $\\frac{\\|\\beta^{(k+1)} - \\beta^{(k)}\\|_2}{\\|\\beta^{(k)}\\|_2 + \\varepsilon}  \\tau$，或者达到了最大迭代次数 $M$，则过程终止。\n\n**4. 数据生成与分析**\n\n根据问题规范，从一个带有高斯噪声的真实三次多项式生成一个包含 $n_{\\text{in}} = 30$ 个内点的“干净”数据集。该数据集使用特定的随机种子固定。对于四个测试用例中的每一个，通过附加指定的离群点来创建一个“有毒”数据集。\n\n我们首先通过将两个模型拟合到仅包含内点的数据集来计算“干净”的系数向量 $\\beta^{\\text{clean}}_{\\text{OLS}}$ 和 $\\beta^{\\text{clean}}_{\\text{Huber}}$。然后，对于每个测试用例，我们通过将模型拟合到各自的组合数据集来计算“有毒”的向量 $\\beta^{\\text{poison}}_{\\text{OLS}}$ 和 $\\beta^{\\text{poison}}_{\\text{Huber}}$。\n\n每种方法的敏感性通过两个比率进行量化：\n-   $R_{\\text{drift}}$：该指标衡量在引入离群点时，OLS 系数相比 Huber 系数从其“干净”值漂移的程度。远大于 $1$ 的值表明 OLS 的稳定性显著较差。\n-   $R_{\\text{RMSE}}$：该指标衡量 OLS 模型在原始干净数据上的预测准确性相比 Huber 模型退化的程度。远大于 $1$ 的值表明 OLS 模型被“拉”得更远离真实的潜在趋势，导致对内点的拟合效果更差。\n\n计算过程通过循环遍历测试用例、执行所需的拟合，并根据其精确的数学定义计算这些指标来进行。对于情况 $1$（无离群点），“有毒”数据集与“干净”数据集相同，根据定义，其漂移和膨胀比率均为 $1$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform the polynomial regression sensitivity analysis.\n    \"\"\"\n    # ------------------ Global Constants and Test Suite Setup ------------------\n    d = 3\n    true_coeffs = np.array([0.3, -1.0, -0.2, 0.5])  # c0, c1, c2, c3\n    n_in = 30\n    sigma = 0.05\n    seed = 20240217\n    kappa = 1.345\n    tau = 1e-10\n    M = 100\n    eps = 1e-12\n\n    test_cases = [\n        # Case 1: no poisoning\n        {'name': 'Case 1', 'outliers': None},\n        # Case 2: few high-leverage extremes\n        {'name': 'Case 2', 'outliers': {'x': np.array([-3.0, 3.0]), 'y': np.array([60.0, -60.0])}},\n        # Case 3: cluster of distant points\n        {'name': 'Case 3', 'outliers': {'x': np.array([4.0, 5.0, 6.0, 7.0, 8.0]), 'y': np.array([200.0, 200.0, 200.0, 200.0, 200.0])}},\n        # Case 4: vertical outliers within inlier domain\n        {'name': 'Case 4', 'outliers': {'x': np.array([-0.8, -0.4, 0.0, 0.4, 0.8]), 'y': np.array([40.0, -40.0, 50.0, -50.0, 45.0])}}\n    ]\n\n    # ------------------ Helper Functions ------------------\n    def create_design_matrix(x, degree):\n        \"\"\"Creates the Vandermonde design matrix for polynomial regression.\"\"\"\n        return np.vander(x, degree + 1, increasing=True)\n\n    def fit_ols(X, y):\n        \"\"\"Fits OLS regression using numpy.linalg.lstsq.\"\"\"\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        return beta\n\n    def fit_huber(X, y):\n        \"\"\"Fits robust regression with Huber loss using IRLS.\"\"\"\n        # Initial guess with OLS\n        beta = fit_ols(X, y)\n        \n        for _ in range(M):\n            beta_old = beta\n            \n            # Calculate residuals\n            r = y - X @ beta\n            \n            # Estimate scale (MAD)\n            median_r = np.median(r)\n            mad = np.median(np.abs(r - median_r))\n            s = 1.4826 * mad\n            \n            # Fallback for scale estimate\n            if s  eps:\n                s = max(np.std(r), eps)\n            \n            # Set Huber threshold\n            delta = kappa * s\n            \n            # Calculate weights\n            abs_r = np.abs(r)\n            # Find non-zero residuals to avoid division by zero\n            # Where abs_r > delta, weight is delta/abs_r.\n            # Where abs_r = delta, weight is 1.\n            # Combine: weight = min(1, delta/abs_r)\n            # Use np.where to handle the abs_r == 0 case implicitly (weight remains 1)\n            weights = np.where(abs_r > eps, np.minimum(1.0, delta / abs_r), 1.0)\n            \n            # Solve weighted least squares\n            W_sqrt = np.sqrt(weights)\n            X_w = X * W_sqrt[:, np.newaxis]\n            y_w = y * W_sqrt\n            beta = fit_ols(X_w, y_w)\n            \n            # Check for convergence\n            diff = np.linalg.norm(beta - beta_old)\n            norm_beta = np.linalg.norm(beta_old)\n            if diff / (norm_beta + eps)  tau:\n                break\n        \n        return beta\n\n    def calculate_rmse(y_true, X_eval, beta):\n        \"\"\"Calculates RMSE on a given dataset.\"\"\"\n        y_pred = X_eval @ beta\n        return np.sqrt(np.mean((y_true - y_pred)**2))\n\n    # ------------------ Main Logic ------------------\n    # Generate inlier data (fixed for all test cases)\n    rng = np.random.default_rng(seed)\n    x_inlier = rng.uniform(-1, 1, n_in)\n    p_true_inlier = create_design_matrix(x_inlier, d) @ true_coeffs\n    y_inlier = p_true_inlier + rng.normal(0, sigma, n_in)\n    X_inlier = create_design_matrix(x_inlier, d)\n\n    # --- Clean Fits (done once) ---\n    beta_clean_ols = fit_ols(X_inlier, y_inlier)\n    beta_clean_huber = fit_huber(X_inlier, y_inlier)\n\n    rmse_in_clean_ols = calculate_rmse(y_inlier, X_inlier, beta_clean_ols)\n    rmse_in_clean_huber = calculate_rmse(y_inlier, X_inlier, beta_clean_huber)\n    \n    final_results = []\n\n    # --- Loop through test cases ---\n    for case in test_cases:\n        if case['outliers'] is not None:\n            x_outlier, y_outlier = case['outliers']['x'], case['outliers']['y']\n            x_poison = np.concatenate((x_inlier, x_outlier))\n            y_poison = np.concatenate((y_inlier, y_outlier))\n        else:\n            # Case 1: no poisoning, poisoned set is the clean set\n            x_poison, y_poison = x_inlier, y_inlier\n\n        X_poison = create_design_matrix(x_poison, d)\n        \n        # --- Poisoned Fits ---\n        beta_poison_ols = fit_ols(X_poison, y_poison)\n        beta_poison_huber = fit_huber(X_poison, y_poison)\n        \n        # --- Calculate Metrics ---\n        # Coefficient Drift\n        delta_ols = np.linalg.norm(beta_poison_ols - beta_clean_ols)\n        delta_huber = np.linalg.norm(beta_poison_huber - beta_clean_huber)\n        r_drift = (delta_ols + eps) / (delta_huber + eps)\n\n        # RMSE Inflation\n        rmse_in_poison_ols = calculate_rmse(y_inlier, X_inlier, beta_poison_ols)\n        rmse_in_poison_huber = calculate_rmse(y_inlier, X_inlier, beta_poison_huber)\n       \n        # Handle cases where baseline RMSE is near zero\n        inflation_ols = rmse_in_poison_ols / (rmse_in_clean_ols + eps)\n        inflation_huber = rmse_in_poison_huber / (rmse_in_clean_huber + eps)\n        \n        # In this problem, clean RMSEs are non-zero, but this is good practice\n        r_rmse = inflation_ols / (inflation_huber + eps)\n\n        final_results.append(r_drift)\n        final_results.append(r_rmse)\n\n    # Format and print the final output\n    output_str = f\"[{','.join(f'{v:.6f}' for v in final_results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3175135"}]}