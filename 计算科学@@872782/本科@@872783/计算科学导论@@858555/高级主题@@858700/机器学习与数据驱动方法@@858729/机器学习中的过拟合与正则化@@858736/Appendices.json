{"hands_on_practices": [{"introduction": "我们从岭回归（Ridge Regression）这一正则化的基石开始。这个练习将模型训练看作一个多目标优化问题：一方面要最小化训练误差，另一方面也要最小化模型复杂度（以权重范数表示）。通过构建帕累托前沿（Pareto front），你将能够直观地看到正则化中的基本权衡，这比简单地调整单个超参数提供了更深刻的洞察力，并展示了在偏差和方差之间所有可能的折衷 [@problem_id:3168619]。", "problem": "您将通过二次正则化线性模型的双目标视角来研究偏差-方差权衡。请在以下基于经验风险最小化核心定义的数学框架内进行操作：一个带有参数 $w \\in \\mathbb{R}^p$ 的线性预测器通过最小化经验平方误差与二次正则化惩罚项之和进行训练。对于每种正则化强度，分别考虑两个目标：训练均方误差和正则化惩罚项本身的值。目标是构建一系列正则化强度下的权衡曲线的离散近似，并识别出帕累托最优模型。\n\n您必须使用的定义：\n- 训练数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，响应向量为 $y \\in \\mathbb{R}^n$。\n- 线性模型预测为 $\\hat{y} = X w$，其中 $w \\in \\mathbb{R}^p$。\n- 训练均方误差 (MSE) 为 $E_{\\text{train}}(w) = \\frac{1}{n}\\lVert X w - y \\rVert_2^2$。\n- 对于给定的正则化强度 $\\lambda > 0$，二次（岭）正则化惩罚项的值为 $P_\\lambda(w) = \\lambda \\lVert w \\rVert_2^2$。\n- 对于一个固定的数据集和一组固定的 $\\lambda$ 值，为每个 $\\lambda$ 训练一个独立的模型会产生一组点对 $\\left(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda)\\right)$。请将这些点对视为目标空间中的点。\n- 一个目标对为 $(E_i, P_i)$ 的模型 $i$ 被一个目标对为 $(E_j, P_j)$ 的模型 $j$ 帕累托支配，当且仅当 $E_j \\le E_i$ 且 $P_j \\le P_i$，并且两个不等式中至少有一个是严格的。如果一个模型没有被集合中任何其他模型所支配，则该模型是帕累托最优的。\n\n对于下面定义的每个测试用例，您的程序必须执行以下操作：\n1. 使用指定的生成模型生成一个合成数据集。\n2. 对于测试用例中给定的每个 $\\lambda$，训练正则化线性模型并计算点对 $\\left(E_{\\text{train}}(w_\\lambda), P_\\lambda(w_\\lambda)\\right)$。\n3. 确定帕累托最优模型的索引集合（从零开始，与所列 $\\lambda$ 值的顺序一致）。\n4. 统计有多少个模型被支配。\n\n数据生成协议（确定性且科学合理）：\n- 对于给定的种子 $s$、维度 $p$ 和样本量 $n$，从均值为 $0$、方差为 $1$ 的正态分布（记作 $\\mathcal{N}(0,1)$）中独立抽取设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的元素。抽样必须是独立同分布 (i.i.d.) 的。\n- 使用相同的种子 $s$（但其状态已确定性地推进，以确保 $X$ 和 $w^\\star$ 不是相同的抽样结果）从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取真实参数矢量 $w^\\star \\in \\mathbb{R}^p$。\n- 对于指定的 $\\sigma$，从 $\\mathcal{N}(0,\\sigma^2)$ 中独立同分布地抽取噪声 $\\epsilon \\in \\mathbb{R}^n$。\n- 构建响应为 $y = X w^\\star + \\epsilon$。\n\n重要实现说明：\n- 所有正则化强度 $\\lambda$ 都将严格为正。不要包含 $\\lambda = 0$。\n- 您用于识别帕累托最优点集的算法必须遵守上述定义。在浮点运算中，比较实数时应使用一个合理的小容差，以减轻由数值舍入引起的虚假支配，但要保留至少一个坐标上的严格改进。\n\n覆盖典型和边缘场景的测试套件：\n- 测试用例 A（适定，低噪声）：\n  - 种子 $s = 7$\n  - 样本 $n = 40$\n  - 特征 $p = 6$\n  - 噪声标准差 $\\sigma = 10^{-1}$\n  - 正则化强度（按顺序）：$\\lambda \\in \\{10^{-6}, 10^{-4}, 10^{-2}, 10^{-1}, 1, 10\\}$\n- 测试用例 B（欠定，极低噪声）：\n  - 种子 $s = 101$\n  - 样本 $n = 20$\n  - 特征 $p = 50$\n  - 噪声标准差 $\\sigma = 5 \\times 10^{-2}$\n  - 正则化强度（按顺序）：$\\lambda \\in \\{10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1\\}$\n- 测试用例 C（适定，高噪声）：\n  - 种子 $s = 2023$\n  - 样本 $n = 50$\n  - 特征 $p = 10$\n  - 噪声标准差 $\\sigma = 1$\n  - 正则化强度（按顺序）：$\\lambda \\in \\{10^{-5}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10^2\\}$\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个列表，每个测试用例对应一个元素，并按 A、B、C 的顺序排列。\n- 对于每个测试用例，输出一个包含两个条目的列表：\n  1. 一个由帕累托最优模型的从零开始的索引（整数）组成的列表，按升序排序。\n  2. 一个等于被支配模型数量的整数。\n- 具体而言，输出必须是形如\n  - $[ [\\text{pareto\\_A}, \\text{dominated\\_count\\_A}], [\\text{pareto\\_B}, \\text{dominated\\_count\\_B}], [\\text{pareto\\_C}, \\text{dominated\\_count\\_C}] ]$\n  的单行，其中每个 $\\text{pareto\\_X}$ 是如上指定的整数列表。\n- 输出不得包含任何单位或附加文本，且元素必须用逗号分隔，并完全按照所示用方括号括起来。", "solution": "用户提供的问题已经过分析并被确认为有效。该问题科学合理、适定、客观，并包含唯一解所需的所有信息。问题要求从双目标优化的角度分析岭回归，其中两个目标是训练均方误差和正则化惩罚项的值。目标是从一组使用不同正则化强度训练的模型中识别出帕累托最优模型。\n\n解决方案分为三个主要步骤：\n1.  推导正则化线性模型参数的解析解。\n2.  详细说明数据生成过程和两个目标函数。\n3.  描述在已训练模型中识别帕累托最优集的算法。\n\n**1. 正则化线性回归（岭回归）**\n\n问题涉及一个线性模型，其预测响应 $\\hat{y} \\in \\mathbb{R}^n$ 由 $\\hat{y} = Xw$ 给出，其中数据矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，参数矢量为 $w \\in \\mathbb{R}^p$。参数 $w$ 的确定是通过最小化一个目标函数，该函数由平方误差和与一个二次正则化项（也称为 $\\ell_2$ 范数惩罚）组成。这是岭回归的定义性特征。\n\n需要最小化的目标函数是：\n$$\nL(w) = \\sum_{i=1}^n (x_i^T w - y_i)^2 + \\lambda \\sum_{j=1}^p w_j^2\n$$\n用矩阵表示法，这表示为：\n$$\nL(w) = \\lVert Xw - y \\rVert_2^2 + \\lambda \\lVert w \\rVert_2^2\n$$\n其中 $y \\in \\mathbb{R}^n$ 是真实响应的向量，$\\lambda > 0$ 是正则化强度参数。\n\n为了找到最小化 $L(w)$ 的最优参数矢量 $w_\\lambda$，我们计算 $L(w)$ 关于 $w$ 的梯度并将其设为零。\n$$\n\\nabla_w L(w) = \\nabla_w \\left( (Xw - y)^T(Xw - y) + \\lambda w^T w \\right)\n$$\n$$\n\\nabla_w L(w) = \\nabla_w \\left( w^T X^T X w - 2y^T X w + y^T y + \\lambda w^T w \\right)\n$$\n使用标准的矩阵微积分法则，梯度为：\n$$\n\\nabla_w L(w) = 2 X^T X w - 2 X^T y + 2 \\lambda I w\n$$\n其中 $I$ 是 $p \\times p$ 的单位矩阵。将梯度设为零以求最小值：\n$$\n2 X^T X w - 2 X^T y + 2 \\lambda I w = 0\n$$\n$$\n(X^T X + \\lambda I) w = X^T y\n$$\n由于 $X^T X$ 是一个半正定矩阵且 $\\lambda$ 严格为正（$\\lambda > 0$），矩阵 $(X^T X + \\lambda I)$ 是正定的，因此总是可逆的。这确保了对于任何 $\\lambda > 0$，即使在 $n  p$ 的情况下（即 $X^T X$ 是奇异的欠定系统），$w$ 都有唯一解。\n\n对于给定的 $\\lambda$，参数矢量的解是：\n$$\nw_\\lambda = (X^T X + \\lambda I)^{-1} X^T y\n$$\n\n**2. 双目标框架**\n\n该问题将此单目标最小化问题重构成一个双目标分析。对于每个用特定 $\\lambda$ 训练的模型，我们评估两个不同的目标函数：\n1.  **训练均方误差 ($E_{\\text{train}}$)**：该指标衡量模型对训练数据的拟合程度。\n    $$\n    E_{\\text{train}}(w_\\lambda) = \\frac{1}{n} \\lVert Xw_\\lambda - y \\rVert_2^2\n    $$\n2.  **正则化惩罚值 ($P_\\lambda$)**：这是惩罚项本身的值。\n    $$\n    P_\\lambda(w_\\lambda) = \\lambda \\lVert w_\\lambda \\rVert_2^2\n    $$\n对于给定正则化强度集合中的每个 $\\lambda_k$，我们计算目标值对 $(E_k, P_k) = (E_{\\text{train}}(w_{\\lambda_k}), P_{\\lambda_k}(w_{\\lambda_k}))$。\n\n数据 $(X, y)$ 是基于一个真实模型合成生成的。在给定种子 $s$、样本量 $n$、特征维度 $p$ 和噪声水平 $\\sigma$ 的情况下，该过程是确定性的。\n- 使用种子 $s$ 初始化一个随机数生成器。\n- 数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 的元素从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取。\n- 真实参数矢量 $w^\\star \\in \\mathbb{R}^p$ 的元素从 $\\mathcal{N}(0,1)$ 中独立同分布地抽取。\n- 噪声矢量 $\\epsilon \\in \\mathbb{R}^n$ 的元素从 $\\mathcal{N}(0, \\sigma^2)$ 中独立同分布地抽取。\n- 响应向量构造为 $y = Xw^\\star + \\epsilon$。\n\n**3. 帕累托最优性分析**\n\n利用这组目标对 $\\{(E_k, P_k)\\}$，我们识别出帕累托最优模型。在这个双目标背景下（两个目标都是最小化的），如果一个模型在两个目标上都优于或等于另一个模型，并且在至少一个目标上严格更优，则认为它更优越。\n\n- **支配**：一个目标值为 $(E_j, P_j)$ 的模型 $j$ *支配* 一个目标值为 $(E_i, P_i)$ 的模型 $i$，当且仅当：\n  $E_j \\le E_i$ 且 $P_j \\le P_i$，并且至少有一个不等式是严格的（即 $E_j  E_i$ 或 $P_j  P_i$）。\n\n- **帕累托最优性**：如果集合中不存在任何其他模型 $j$ 支配模型 $i$，则模型 $i$ 是*帕累托最优*的。所有帕累托最优点构成的集合称为帕累托前沿。\n\n识别帕累托最优模型并统计被支配模型数量的算法如下：\n1.  对每个测试用例，按规定生成数据 $(X, y)$。\n2.  对每个给定的正则化强度 $\\lambda_k$，使用解析解计算相应的权重矢量 $w_{\\lambda_k}$。\n3.  为每个模型 $k$ 计算目标对 $(E_k, P_k)$。\n4.  初始化一个布尔数组 `is_dominated`，其大小与模型数量相同，所有条目均设为 `False`。\n5.  遍历每个模型 $i$：\n    a. 遍历所有其他模型 $j$（其中 $j \\neq i$）。\n    b. 根据上述定义检查模型 $j$ 是否支配模型 $i$。为处理浮点运算，比较时使用一个小的容差 $\\epsilon_{tol}$。如果 $(E_j \\le E_i + \\epsilon_{tol} \\text{ and } P_j \\le P_i + \\epsilon_{tol})$ 且 $(E_j  E_i - \\epsilon_{tol} \\text{ or } P_j  P_i - \\epsilon_{tol})$，则模型 $j$ 支配模型 $i$。\n    c. 如果找到一个支配模型 $i$ 的模型 $j$，则将 `is_dominated[i]` 设为 `True` 并为提高效率而中断内层循环（对 $j$ 的循环）。\n6.  循环完成后，`is_dominated[k]` 为 `False` 的索引 $k$ 对应于帕累托最优模型。收集这些索引。\n7.  被支配模型的总数是 `is_dominated` 数组中 `True`值的总和。\n8.  每个测试用例的结果是帕累托最优索引的排序列表和被支配模型的计数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef process_case(s, n, p, sigma, lambdas):\n    \"\"\"\n    Generates data, trains models, and performs Pareto analysis for one test case.\n\n    Args:\n        s (int): Seed for the random number generator.\n        n (int): Number of samples.\n        p (int): Number of features.\n        sigma (float): Standard deviation of the noise.\n        lambdas (list[float]): List of regularization strengths.\n\n    Returns:\n        list: A list containing two elements:\n              1. A list of zero-based indices of Pareto-optimal models.\n              2. The integer count of dominated models.\n    \"\"\"\n    # 1. Data Generation using a deterministic protocol\n    rng = np.random.default_rng(seed=s)\n    X = rng.normal(loc=0.0, scale=1.0, size=(n, p))\n    # The RNG state advances, so w_star is drawn from a different state than X\n    w_star = rng.normal(loc=0.0, scale=1.0, size=p)\n    epsilon = rng.normal(loc=0.0, scale=sigma, size=n)\n    y = X @ w_star + epsilon\n\n    # 2. Model Training and Objective Calculation for each lambda\n    objectives = []\n    XTX = X.T @ X\n    XTy = X.T @ y\n    I = np.identity(p)\n    \n    for lmbda in lambdas:\n        # Solve (X'X + lambda*I)w = X'y for w\n        w_lambda = np.linalg.solve(XTX + lmbda * I, XTy)\n\n        # Calculate the two objectives: training MSE and penalty value\n        pred_error = X @ w_lambda - y\n        E_train = (1.0 / n) * np.sum(pred_error**2)\n        P_lambda_val = lmbda * np.sum(w_lambda**2)\n        \n        objectives.append((E_train, P_lambda_val))\n\n    # 3. Pareto Optimality Analysis\n    num_models = len(lambdas)\n    is_dominated = [False] * num_models\n    tol = 1e-12  # Tolerance for floating-point comparisons\n\n    for i in range(num_models):\n        for j in range(num_models):\n            if i == j:\n                continue\n            \n            # Check if model j dominates model i\n            E_i, P_i = objectives[i]\n            E_j, P_j = objectives[j]\n            \n            # Dominance condition: E_j = E_i and P_j = P_i, with one being strict\n            is_le_in_both = (E_j = E_i + tol) and (P_j = P_i + tol)\n            is_lt_in_one = (E_j  E_i - tol) or (P_j  P_i - tol)\n            \n            if is_le_in_both and is_lt_in_one:\n                is_dominated[i] = True\n                break  # Model i is dominated, no need to check other j's\n\n    pareto_indices = [i for i, dominated in enumerate(is_dominated) if not dominated]\n    dominated_count = sum(is_dominated)\n\n    return [pareto_indices, dominated_count]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case A (well-posed, low noise)\n        {'s': 7, 'n': 40, 'p': 6, 'sigma': 1e-1, 'lambdas': [1e-6, 1e-4, 1e-2, 1e-1, 1.0, 10.0]},\n        \n        # Test case B (underdetermined, very low noise)\n        {'s': 101, 'n': 20, 'p': 50, 'sigma': 5e-2, 'lambdas': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]},\n        \n        # Test case C (well-posed, high noise)\n        {'s': 2023, 'n': 50, 'p': 10, 'sigma': 1.0, 'lambdas': [1e-5, 1e-3, 1e-2, 1e-1, 1.0, 1e2]}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case['s'], case['n'], case['p'], case['sigma'], case['lambdas'])\n        results.append(result)\n\n    # Format the output string exactly as required, without extra spaces.\n    outer_list_parts = []\n    for res_pair in results:\n        pareto_indices, dominated_count = res_pair\n        indices_str = '[' + ','.join(map(str, pareto_indices)) + ']'\n        pair_str = f\"[{indices_str},{dominated_count}]\"\n        outer_list_parts.append(pair_str)\n    \n    final_output = '[' + ','.join(outer_list_parts) + ']'\n    print(final_output)\n\nsolve()\n```", "id": "3168619"}, {"introduction": "我们如何衡量一个非线性模型的复杂度？在非线性模型中，仅仅计算参数数量是不够的。这个练习探讨了核岭回归（Kernel Ridge Regression），并引入了“有效自由度”（$d_{\\text{eff}}$）的概念，这是一个从模型的平滑矩阵中导出的强大度量。通过亲手实现 $d_{\\text{eff}}$ 的计算 [@problem_id:3168571]，你将具体理解正则化强度 $\\lambda$ 和核函数长度尺度 $\\ell$ 这两个超参数如何共同控制模型的灵活性及其过拟合的倾向，从而从直观理解转向复杂度的量化分析。", "problem": "给你一个一维训练设计，其输入 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 由闭区间 $\\left[0,1\\right]$ 上的 $n=20$ 个等距点定义，具体为 $x_i = \\dfrac{i-1}{n-1}$，其中 $i \\in \\{1,2,\\dots,n\\}$。考虑使用径向基函数 (RBF) 核的核岭回归 (KRR)。长度尺度为 $\\ell0$ 的 RBF 核定义为 $k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$，相关的格拉姆矩阵 $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ 的元素为 $K_{ij} = k_{\\ell}(x_i,x_j)$。KRR通过在再生核希尔伯特空间上最小化吉洪诺夫正则化的平方损失来估计响应，正则化参数为 $\\lambda \\ge 0$。\n\n从基本定义出发，推导将训练输出 $\\mathbf{y} \\in \\mathbb{R}^n$ 映射到 KRR 拟合值 $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ 的线性平滑算子，并用它将有效自由度 $d_{\\mathrm{eff}}$ 定义为该平滑算子的迹。你的程序必须为给定的 $\\lambda$ 和 $\\ell$ 以及上述固定的设计 $\\mathbf{x}$ 实现一种数值稳定的 $d_{\\mathrm{eff}}$ 计算方法，且不执行显式矩阵求逆。\n\n通过报告以下 $(\\lambda,\\ell)$ 对测试套件的 $d_{\\mathrm{eff}}$ 值来展示过拟合控制：\n$(10^{-6}, \\, 0.05)$,\n$(10^{-6}, \\, 5.0)$,\n$(10^{-1}, \\, 0.2)$,\n$(10^{-1}, \\, 1.0)$,\n$(10^{1}, \\, 0.05)$,\n$(10^{1}, \\, 5.0)$.\n\n你的程序应使用固定的 $\\mathbf{x}$ 和指定的 RBF 核计算每个测试用例的 $d_{\\mathrm{eff}}$，并生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如 $\\left[\\text{结果}_1,\\text{结果}_2,\\dots\\right]$。每个 $d_{\\mathrm{eff}}$ 必须以保留 $6$ 位小数的十进制数打印。不涉及物理单位，也不使用角度。", "solution": "用户希望我解决以下问题：\n给你一个一维训练设计，其输入 $\\mathbf{x} \\in \\mathbb{R}^{n}$ 由闭区间 $\\left[0,1\\right]$ 上的 $n=20$ 个等距点定义，具体为 $x_i = \\dfrac{i-1}{n-1}$，其中 $i \\in \\{1,2,\\dots,n\\}$。考虑使用径向基函数 (RBF) 核的核岭回归 (KRR)。长度尺度为 $\\ell0$ 的 RBF 核定义为 $k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$，相关的格拉姆矩阵 $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ 的元素为 $K_{ij} = k_{\\ell}(x_i,x_j)$。KRR通过在再生核希尔伯特空间上最小化吉洪诺夫正则化的平方损失来估计响应，正则化参数为 $\\lambda \\ge 0$。\n\n从基本定义出发，推导将训练输出 $\\mathbf{y} \\in \\mathbb{R}^n$ 映射到 KRR 拟合值 $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$ 的线性平滑算子，并用它将有效自由度 $d_{\\mathrm{eff}}$ 定义为该平滑算子的迹。你的程序必须为给定的 $\\lambda$ 和 $\\ell$ 以及上述固定的设计 $\\mathbf{x}$ 实现一种数值稳定的 $d_{\\mathrm{eff}}$ 计算方法，且不执行显式矩阵求逆。\n\n通过报告以下 $(\\lambda,\\ell)$ 对测试套件的 $d_{\\mathrm{eff}}$ 值来展示过拟合控制：\n$(10^{-6}, \\, 0.05)$,\n$(10^{-6}, \\, 5.0)$,\n$(10^{-1}, \\, 0.2)$,\n$(10^{-1}, \\, 1.0)$,\n$(10^{1}, \\, 0.05)$,\n$(10^{1}, \\, 5.0)$.\n\n你的程序应使用固定的 $\\mathbf{x}$ 和指定的 RBF 核计算每个测试用例的 $d_{\\mathrm{eff}}$，并生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表，例如 $\\left[\\text{结果}_1,\\text{结果}_2,\\dots\\right]$。每个 $d_{\\mathrm{eff}}$ 必须以保留 $6$ 位小数的十进制数打印。不涉及物理单位，也不使用角度。\n\n### 问题验证\n\n#### 第 1 步：提取已知条件\n- **领域：** 一维回归，核岭回归 (KRR)。\n- **输入设计：** $\\left[0,1\\right]$ 上的 $n=20$ 个等距点。具体来说，$x_i = \\dfrac{i-1}{n-1}$，其中 $i \\in \\{1,2,\\dots,n\\}$。\n- **核函数：** 径向基函数 (RBF) 核，$k_{\\ell}(x,x') = \\exp\\left(-\\dfrac{\\|x-x'\\|^2}{2\\ell^2}\\right)$，长度尺度 $\\ell0$。\n- **格拉姆矩阵：** $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$，其元素为 $K_{ij} = k_{\\ell}(x_i,x_j)$。\n- **模型：** KRR，使用吉洪诺夫正则化参数 $\\lambda \\ge 0$。\n- **任务 1 (推导)：** 推导线性平滑算子 $\\mathbf{S}$（其中 $\\hat{\\mathbf{y}} = \\mathbf{S}\\mathbf{y}$）和有效自由度 $d_{\\mathrm{eff}} = \\mathrm{Tr}(\\mathbf{S})$。\n- **任务 2 (计算)：** 实现一种数值稳定的方法，在不进行显式矩阵求逆的情况下，为给定的设计 $\\mathbf{x}$ 计算 $d_{\\mathrm{eff}}$。\n- **测试用例：** 为以下 $(\\lambda, \\ell)$ 对计算 $d_{\\mathrm{eff}}$：\n  - $(10^{-6}, 0.05)$\n  - $(10^{-6}, 5.0)$\n  - $(10^{-1}, 0.2)$\n  - $(10^{-1}, 1.0)$\n  - $(10^{1}, 0.05)$\n  - $(10^{1}, 5.0)$\n- **输出格式：** 单行输出，包含逗号分隔的结果列表，例如 `[结果1,结果2,...]`，每个结果保留 $6$ 位小数。\n\n#### 第 2 步：使用提取的已知条件进行验证\n1.  **科学性：** 该问题基于核岭回归，这是机器学习和统计学中的一项标准基础技术。RBF 核、格拉姆矩阵、吉洪诺夫正则化和有效自由度的定义都是标准的、数学上正确的。该问题牢固地植根于既定的统计学习理论。\n2.  **适定性：** 所有必要的参数（$n$、$\\mathbf{x}$、核函数、$\\lambda$、$\\ell$）都已指定。任务是计算一个明确定义的量 $d_{\\mathrm{eff}}$。对于任何给定的有效参数集 $(\\lambda  0, \\ell  0)$，$d_{\\mathrm{eff}}$ 解的存在性和唯一性是有保证的。对于不同的点，RBF 格拉姆矩阵 $\\mathbf{K}$ 是正定的，因此对于 $\\lambda \\ge 0$，$\\mathbf{K} + \\lambda\\mathbf{I}$ 是可逆的。该问题是适定的。\n3.  **客观性：** 该问题使用精确、客观的数学语言陈述。不存在主观或基于观点的论断。\n4.  **完整性：** 该问题是自洽的。它提供了完整解决方案所需的所有数据、定义和约束。\n5.  **可行性：** 对于现代硬件来说，$n=20$ 的数值计算在计算上是微不足道的。要求使用数值稳定的方法，这指向了像特征值分解这样的标准、成熟的算法，这是合适且可行的。\n\n#### 第 3 步：结论与行动\n该问题是有效的，因为它科学合理、适定、客观且完整。我将继续进行推导和求解。\n\n###\n核岭回归 (KRR) 问题旨在从再生核希尔伯特空间 (RKHS) $\\mathcal{H}_k$ 中找到一个函数 $f$，以最小化正则化平方损失。对于训练集 $\\{ (x_i, y_i) \\}_{i=1}^n$，目标函数为：\n$$\n\\min_{f \\in \\mathcal{H}_k} \\sum_{i=1}^n (y_i - f(x_i))^2 + \\lambda \\|f\\|_{\\mathcal{H}_k}^2\n$$\n这里，$\\mathbf{y} = (y_1, \\dots, y_n)^T$ 是观测响应向量，$\\lambda \\ge 0$ 是正则化参数。\n\n根据表示定理 (Representer Theorem)，最小化器 $f$ 可以表示为在训练点处求值的核函数的线性组合：\n$$\nf(x) = \\sum_{j=1}^n \\alpha_j k(x, x_j)\n$$\n其中 $\\boldsymbol{\\alpha} = (\\alpha_1, \\dots, \\alpha_n)^T$ 是系数向量。RKHS 范数的平方由 $\\|f\\|_{\\mathcal{H}_k}^2 = \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}$ 给出，其中 $\\mathbf{K}$ 是格拉姆矩阵，其元素为 $K_{ij} = k(x_i, x_j)$。\n\n在训练点处的函数求值向量 $\\mathbf{f} = (f(x_1), \\dots, f(x_n))^T$ 可以写成 $\\mathbf{f} = \\mathbf{K}\\boldsymbol{\\alpha}$。将这些代入目标函数，我们将问题转化为寻找最优系数 $\\boldsymbol{\\alpha}$ 的问题：\n$$\n\\min_{\\boldsymbol{\\alpha} \\in \\mathbb{R}^n} \\|\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}\\|_2^2 + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}\n$$\n这是一个二次优化问题。为了找到最小值，我们对目标函数关于 $\\boldsymbol{\\alpha}$ 求导，并令梯度为零。目标函数为 $L(\\boldsymbol{\\alpha}) = (\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha})^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + \\lambda \\boldsymbol{\\alpha}^T \\mathbf{K} \\boldsymbol{\\alpha}$。\n$$\n\\nabla_{\\boldsymbol{\\alpha}} L = -2\\mathbf{K}^T(\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}) + 2\\lambda \\mathbf{K}\\boldsymbol{\\alpha} = 0\n$$\n由于核是对称的，$\\mathbf{K}^T = \\mathbf{K}$。\n$$\n-\\mathbf{K}\\mathbf{y} + \\mathbf{K}\\mathbf{K}\\boldsymbol{\\alpha} + \\lambda \\mathbf{K}\\boldsymbol{\\alpha} = 0\n$$\n$$\n\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{K}\\mathbf{y}\n$$\n对于不同的点和有效的 RBF 核 ($\\ell0$)，格拉姆矩阵 $\\mathbf{K}$ 是正定的，因此是可逆的。我们可以乘以 $\\mathbf{K}^{-1}$：\n$$\n(\\mathbf{K} + \\lambda\\mathbf{I})\\boldsymbol{\\alpha} = \\mathbf{y}\n$$\n求解 $\\boldsymbol{\\alpha}$，我们得到：\n$$\n\\boldsymbol{\\alpha} = (\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\n拟合值 $\\hat{\\mathbf{y}}$ 是在训练点处的预测值，由 $\\hat{\\mathbf{y}} = \\mathbf{f} = \\mathbf{K}\\boldsymbol{\\alpha}$ 给出。代入 $\\boldsymbol{\\alpha}$ 的表达式：\n$$\n\\hat{\\mathbf{y}} = \\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n$$\n这个方程显示了观测输出 $\\mathbf{y}$ 和拟合值 $\\hat{\\mathbf{y}}$ 之间的线性关系。执行此映射的矩阵称为平滑矩阵 $\\mathbf{S}$：\n$$\n\\mathbf{S} = \\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\n$$\n有效自由度 $d_{\\mathrm{eff}}$ 用于衡量拟合模型的复杂度，定义为平滑矩阵的迹：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}(\\mathbf{S}) = \\mathrm{Tr}\\left(\\mathbf{K}(\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\right)\n$$\n为了在不进行显式矩阵求逆的情况下计算此量，我们对格拉姆矩阵 $\\mathbf{K}$ 进行谱分解。由于 $\\mathbf{K}$ 是实对称矩阵，它可以被对角化为 $\\mathbf{K} = \\mathbf{U\\Sigma U}^T$，其中 $\\mathbf{U}$ 是特征向量的正交矩阵（$\\mathbf{U}\\mathbf{U}^T = \\mathbf{U}^T\\mathbf{U} = \\mathbf{I}$），$\\mathbf{\\Sigma}$ 是相应非负特征值 $\\sigma_i$ 的对角矩阵。\n将此代入 $d_{\\mathrm{eff}}$ 的表达式中：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( (\\mathbf{U\\Sigma U}^T) (\\mathbf{U\\Sigma U}^T + \\lambda\\mathbf{I})^{-1} \\right) \\\\\n= \\mathrm{Tr}\\left( \\mathbf{U\\Sigma U}^T \\left(\\mathbf{U}(\\mathbf{\\Sigma} + \\lambda\\mathbf{I})\\mathbf{U}^T\\right)^{-1} \\right) \\\\\n= \\mathrm{Tr}\\left( \\mathbf{U\\Sigma U}^T (\\mathbf{U}^T)^{-1} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^{-1} \\right)\n$$\n使用 $\\mathbf{U}^{-1} = \\mathbf{U}^T$ 和 $(\\mathbf{U}^T)^{-1} = \\mathbf{U}$：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{U\\Sigma} (\\mathbf{U}^T \\mathbf{U}) (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^T \\right) = \\mathrm{Tr}\\left( \\mathbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\mathbf{U}^T \\right)\n$$\n使用迹的循环性质 $\\mathrm{Tr}(\\mathbf{ABC}) = \\mathrm{Tr}(\\mathbf{CAB})$：\n$$\nd_{\\mathrm{eff}} = \\mathrm{Tr}\\left( \\mathbf{U}^T \\mathbf{U}\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right) = \\mathrm{Tr}\\left( \\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1} \\right)\n$$\n矩阵 $\\mathbf{\\Sigma} (\\mathbf{\\Sigma} + \\lambda\\mathbf{I})^{-1}$ 是对角矩阵，其对角线元素为 $\\frac{\\sigma_i}{\\sigma_i + \\lambda}$。迹是这些对角元素的总和：\n$$\nd_{\\mathrm{eff}} = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i + \\lambda}\n$$\n该公式提供了一种数值稳定的方法来计算 $d_{\\mathrm{eff}}$，即首先求出 $\\mathbf{K}$ 的特征值。实现将为每个 $(\\lambda, \\ell)$ 对构造格拉姆矩阵 $\\mathbf{K}$，计算其特征值，然后应用此公式。\n\n较高的 $d_{\\mathrm{eff}}$ 值（接近 $n=20$）表示模型过于复杂，可能存在过拟合，因为它使用许多参数来拟合数据。这在 $\\lambda$ 很小时发生。较低的 $d_{\\mathrm{eff}}$ 值（接近 $0$）表示模型非常简单、高度正则化，可能存在欠拟合。这在 $\\lambda$ 很大或 $\\ell$ 非常大时发生（这会强制函数接近常数，其复杂度为 $1$）。\n\n- 对于 $(\\lambda=10^{-6}, \\ell=0.05)$：小的 $\\lambda$ 和与点间距相当的长度尺度 $\\ell$ 表明正则化程度低，模型复杂。我们预计 $d_{\\mathrm{eff}} \\approx 20$。\n- 对于 $(\\lambda=10^{-6}, \\ell=5.0)$：小的 $\\lambda$ 但大的 $\\ell$ 会强制函数非常平滑。格拉姆矩阵接近秩为 1 的矩阵，导致有效参数很少。我们预计 $d_{\\mathrm{eff}} \\approx 1$。\n- 对于 $(\\lambda=10^{1}, \\ell=0.05)$：大的 $\\lambda$ 施加了强正则化，尽管 $\\ell$ 很小，但仍降低了复杂度。我们预计 $d_{\\mathrm{eff}}$ 会很小。\n- 对于 $(\\lambda=10^{1}, \\ell=5.0)$：大的 $\\lambda$ 和大的 $\\ell$ 都促进了模型的简化。我们预计 $d_{\\mathrm{eff}}$ 会非常小。\n\n下面的代码实现了这个逻辑。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the effective degrees of freedom for Kernel Ridge Regression\n    with an RBF kernel for a set of (lambda, ell) parameters.\n    \"\"\"\n    \n    # Define fixed problem parameters\n    n = 20\n    x = np.linspace(0, 1, n)\n\n    # Define the test suite of (lambda, ell) pairs\n    test_cases = [\n        (1e-6, 0.05),\n        (1e-6, 5.0),\n        (1e-1, 0.2),\n        (1e-1, 1.0),\n        (1e+1, 0.05),\n        (1e+1, 5.0)\n    ]\n\n    results = []\n    \n    # Use broadcasting to create a matrix of squared distances\n    # x_col has shape (n, 1), x has shape (n,). Broadcasting creates an (n, n) matrix.\n    x_col = x.reshape(-1, 1)\n    sq_dist_matrix = (x_col - x)**2\n\n    for lmbda, ell in test_cases:\n        # Construct the RBF Gram matrix K\n        # K_ij = exp(-||x_i - x_j||^2 / (2 * ell^2))\n        K = np.exp(-sq_dist_matrix / (2 * ell**2))\n\n        # Compute the eigenvalues of the symmetric matrix K.\n        # np.linalg.eigvalsh is numerically stable and optimized for Hermitian matrices.\n        eigenvalues = np.linalg.eigvalsh(K)\n\n        # Compute the effective degrees of freedom using the derived formula.\n        # d_eff = sum(sigma_i / (sigma_i + lambda))\n        d_eff = np.sum(eigenvalues / (eigenvalues + lmbda))\n        \n        # Round the result to 6 decimal places and store as a string\n        results.append(f\"{d_eff:.6f}\")\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3168571"}, {"introduction": "正则化通常被认为是损失函数中一个明确的惩罚项，比如 LASSO 中的 $\\ell_1$ 范数。然而，优化过程本身也可能产生正则化效应。这个高级练习通过比较显式 $\\ell_1$ 正则化与通过提前停止迭代收缩阈值算法（ISTA）所产生的隐式正则化，深入探讨了优化与泛化之间的联系。通过这个练习 [@problem_id:3168592]，你不仅能获得处理稀疏模型基础算法的实践经验，还能揭示现代机器学习中的一个深刻概念：简单地在算法完全收敛前停止它，就可以成为防止过拟合的有力工具。", "problem": "考虑将最小绝对收缩和选择算子 (Least Absolute Shrinkage and Selection Operator, LASSO) 的目标函数视为一个复合函数，其由一个光滑的数据拟合项和一个非光滑的稀疏性促进惩罚项组成。设训练设计矩阵为 $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$，验证设计矩阵为 $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$，对应的响应向量为 $y_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ 和 $y_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$。带有 $\\ell_1$ 惩罚项的经验风险最小化问题为\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1,\n$$\n其中 $\\lambda \\ge 0$ 用于平衡数据拟合度与稀疏性。迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA) 是通过对复合目标函数应用近端梯度下降得到的，该算法交替执行对光滑项的梯度步和应用 $\\ell_1$ 范数的近端算子。在本问题中，您需要将固定次数 $T$ 的 ISTA 迭代展开为一个逐层计算图（将 $T$ 解释为提前停止），并将其泛化行为与带有显式 $\\lambda$ 正则化、运行足够长时间的 ISTA 进行比较。\n\n您的程序必须：\n- 使用以下基础且经过充分测试的规范，构建具有稀疏真实解的合成线性回归数据集：\n  1. 真实参数 $x^\\star \\in \\mathbb{R}^d$ 是 $k$-稀疏的，其恰好有 $k$ 个非零项，这些非零项从零均值单位方差分布中独立抽取。支撑集（非零项的位置）是均匀随机选择的。\n  2. 生成的训练和验证矩阵具有可控的列相关性。对于给定的相关性参数 $\\rho \\in [0,1)$，设 $Z_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ 和 $Z_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$ 具有独立同分布 (i.i.d.) 的标准正态分布项。对每个数据集，抽取一个潜向量 $u_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}}}$ 和 $u_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}}}$，其元素也为 i.i.d. 的标准正态分布。定义\n  $$\n  A_{\\text{train}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{train}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{train}}, \\quad\n  A_{\\text{val}}[:,j] \\;=\\; \\sqrt{1-\\rho}\\; Z_{\\text{val}}[:,j] \\;+\\; \\sqrt{\\rho}\\; u_{\\text{val}},\n  $$\n  对于每个列索引 $j \\in \\{1,\\dots,d\\}$。这种构造方式产生的特征列其相关性由 $\\rho$ 控制，并且在科学上是合理的，可用于研究共线性的影响。\n  3. 训练和验证响应满足\n  $$\n  y_{\\text{train}} \\;=\\; A_{\\text{train}} x^\\star \\;+\\; \\varepsilon_{\\text{train}}, \\quad\n  y_{\\text{val}} \\;=\\; A_{\\text{val}} x^\\star \\;+\\; \\varepsilon_{\\text{val}},\n  $$\n  其中 $\\varepsilon_{\\text{train}}$ 和 $\\varepsilon_{\\text{val}}$ 具有 i.i.d. 的高斯项，其均值为零，标准差为 $\\sigma$（噪声水平）。\n- 从平方损失和 $\\ell_1$ 惩罚项的核心定义出发，通过以下方式实现 ISTA：\n  1. 使用多元微积分的第一性原理计算光滑项的梯度。\n  2. 选择一个常数步长 $s$，该步长至多为光滑项梯度的 Lipschitz 常数的倒数，其中 Lipschitz 常数等于 $\\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$ 的最大特征值。\n  3. 将 $\\ell_1$ 范数的近端算子应用于经梯度更新后的迭代值。\n- 从零向量 $x^{(0)} = 0$ 开始，展开恰好 $T$ 次迭代以获得 $x^{(T)}$，并将有限的 $T$ 解释为提前停止，这是一种源于在收敛前停止优化过程的隐式正则化形式。\n\n对每个数据集，构建两个估计器：\n- 提前停止的估计器，不带显式 $\\ell_1$ 惩罚项：设置 $\\lambda = 0$ 并运行 ISTA 恰好 $T_{\\text{small}}$ 次迭代以获得 $x^{(T_{\\text{small}})}$。\n- 显式正则化的估计器：设置一个预定的 $\\lambda  0$ 并运行 ISTA $T_{\\text{large}}$ 次迭代以近似收敛，并获得 $x^{(T_{\\text{large}})}$。\n\n通过计算两个估计器在验证集上的均方误差 (Mean Squared Error, MSE) 来评估其泛化能力：\n$$\n\\text{MSE}_{\\text{val}}(x) \\;=\\; \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2.\n$$\n对每个数据集，报告提前停止是否比显式 $\\lambda$ 正则化取得了严格更低的验证误差。将此比较结果表示为一个整数：如果 $\\text{MSE}_{\\text{val}}(x^{(T_{\\text{small}})})  \\text{MSE}_{\\text{val}}(x^{(T_{\\text{large}})})$，则为 $1$，否则为 $0$。\n\n使用以下测试套件以确保覆盖不同的情况：\n- 案例 1 (理想情况，中等噪声，弱相关性)：$d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.2$, $\\rho = 0.2$, $T_{\\text{small}} = 10$, $T_{\\text{large}} = 500$, $\\lambda = 0.05$。\n- 案例 2 (边界条件，$T=0$ 且噪声较高)：$d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 1.0$, $\\rho = 0.0$, $T_{\\text{small}} = 0$, $T_{\\text{large}} = 500$, $\\lambda = 0.1$。\n- 案例 3 (强特征相关性，低噪声)：$d = 50$, $n_{\\text{train}} = 80$, $n_{\\text{val}} = 80$, $k = 5$, $\\sigma = 0.05$, $\\rho = 0.9$, $T_{\\text{small}} = 3$, $T_{\\text{large}} = 500$, $\\lambda = 0.01$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,r_3]$），其中每个 $r_i$ 是案例 $i$ 的整数比较结果。不涉及物理单位或角度单位。通过在程序内部为每个案例固定随机种子，确保所有随机选择都是可复现的。", "solution": "用户提供的问题是有效的。这是一个在计算科学领域中的良构、有科学依据且客观的问题，特别关注机器学习中的正则化技术。它提供了一套完整且一致的定义、参数和评估标准，从而可以得出一个唯一且有意义的解。\n\n该问题要求比较 LASSO (Least Absolute Shrinkage and Selection Operator) 模型的两种正则化形式：显式 $\\ell_1$ 惩罚和通过提前停止优化算法实现的隐式正则化。所选的算法是迭代收缩阈值算法 (Iterative Shrinkage-Thresholding Algorithm, ISTA)，这是针对此类问题的标准近端梯度方法。\n\n问题的核心在于求解以下优化目标：\n$$\n\\min_{x \\in \\mathbb{R}^d} \\; F(x) = \\min_{x \\in \\mathbb{R}^d} \\left( \\frac{1}{2 n_{\\text{train}}} \\left\\| A_{\\text{train}} x - y_{\\text{train}} \\right\\|_2^2 \\; + \\; \\lambda \\left\\| x \\right\\|_1 \\right)\n$$\n其中目标 $F(x)$ 是一个复合函数 $F(x) = f(x) + g(x)$。项 $f(x) = \\frac{1}{2 n_{\\text{train}}} \\| A_{\\text{train}} x - y_{\\text{train}} \\|_2^2$ 是光滑、可微的数据拟合项（来自平方损失的经验风险），而 $g(x) = \\lambda \\|x\\|_1$ 是非光滑、凸的正则化项。\n\n### 数据生成\n根据严格的规范构建合成数据集，以确保可控性和可复现性。\n1.  生成一个 $k$-稀疏的真实参数向量 $x^\\star \\in \\mathbb{R}^d$。其支撑集（其 $k$ 个非零元素的位置集合）是均匀随机选择的。这些非零位置上的值是从标准正态分布 $N(0, 1)$ 中抽取的。\n2.  设计矩阵 $A_{\\text{train}} \\in \\mathbb{R}^{n_{\\text{train}} \\times d}$ 和 $A_{\\text{val}} \\in \\mathbb{R}^{n_{\\text{val}} \\times d}$ 被构造成具有指定的特征相关性 $\\rho$。每一列 $A[:,j]$ 是一个独立随机向量 $Z[:,j]$ 和一个公共潜向量 $u$ 的线性组合：\n    $$\n    A[:,j] = \\sqrt{1-\\rho} Z[:,j] + \\sqrt{\\rho} u\n    $$\n    其中 $Z$ 和 $u$ 的元素是独立同分布的标准正态分布。这种构造会导出理论上的相关性 $\\text{Cov}(A[:,i], A[:,j]) = \\rho$（对于 $i \\neq j$）。\n3.  响应向量 $y_{\\text{train}}$ 和 $y_{\\text{val}}$ 是从线性模型 $y = Ax^\\star + \\varepsilon$ 生成的，其中 $\\varepsilon$ 是均值为 $0$、标准差为 $\\sigma$ 的加性高斯噪声。\n\n### 迭代收缩阈值算法 (ISTA)\nISTA 是近端梯度方法的一个实例，适用于最小化形如 $f(x) + g(x)$ 的复合目标函数。每次迭代包括两个步骤：\n1.  对光滑项 $f(x)$ 进行一步梯度下降。\n2.  应用非光滑项 $g(x)$ 的近端算子。\n\n光滑项 $f(x)$ 的梯度由下式给出：\n$$\n\\nabla f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top (A_{\\text{train}} x - y_{\\text{train}})\n$$\n对于步长 $s$，$g(x) = \\lambda \\|x\\|_1$ 的近端算子是软阈值算子 $S_{s\\lambda}(\\cdot)$：\n$$\n\\text{prox}_{s g}(v)_i = S_{s\\lambda}(v_i) = \\text{sign}(v_i) \\max(|v_i| - s\\lambda, 0)\n$$\n综合这些，从初始猜测 $x^{(0)}$ 开始，ISTA 的更新规则为：\n$$\nx^{(t+1)} = S_{s\\lambda} \\left( x^{(t)} - s \\nabla f(x^{(t)}) \\right)\n$$\n如果步长 $s$ 满足 $0  s \\le 1/L$，则该算法的收敛性得到保证，其中 $L$ 是 $\\nabla f(x)$ 的 Lipschitz 常数。$f(x)$ 的 Hessian 矩阵是 $\\nabla^2 f(x) = \\frac{1}{n_{\\text{train}}} A_{\\text{train}}^\\top A_{\\text{train}}$。Lipschitz 常数 $L$ 是该 Hessian 矩阵的最大特征值（即谱范数）。一个安全且标准的选择是 $s = 1/L$。\n\n### 正则化比较\n构建并比较了两种估计器：\n1.  **提前停止的估计器**：该模型使用隐式正则化。我们设置显式正则化参数 $\\lambda = 0$，并运行 ISTA 算法一个较小的固定迭代次数 $T_{\\text{small}}$。当 $\\lambda=0$ 时，软阈值算子变为恒等算子，$S_0(v) = v$，ISTA 简化为对最小二乘目标的标准梯度下降。从 $x^{(0)} = 0$ 开始，几次迭代会使参数向经验风险的最小值移动，但提前停止可以防止参数变得过大并过拟合训练数据。\n2.  **显式正则化的估计器**：该模型使用标准的 LASSO 公式和一个预定的惩罚项 $\\lambda  0$。ISTA 算法运行一个较大的迭代次数 $T_{\\text{large}}$，以近似收敛到显式正则化目标的唯一最小值点。\n\n### 评估\n两种估计器 $x^{(T_{\\text{small}})}$ 和 $x^{(T_{\\text{large}})}$ 的泛化性能在未见过的验证集上进行评估。评估指标是均方误差 (Mean Squared Error, MSE)：\n$$\n\\text{MSE}_{\\text{val}}(x) = \\frac{1}{n_{\\text{val}}} \\left\\| A_{\\text{val}} x - y_{\\text{val}} \\right\\|_2^2\n$$\n每个测试案例的最终输出是一个整数，如果提前停止的估计器比显式正则化的估计器取得了严格更低的验证 MSE，则为 $1$，否则为 $0$。通过为每个测试案例使用固定的随机种子来确保可复现性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_data(d, n_train, n_val, k, sigma, rho, rng):\n    \"\"\"\n    Generates synthetic data for a sparse linear regression problem.\n    \n    Args:\n        d (int): Number of features.\n        n_train (int): Number of training samples.\n        n_val (int): Number of validation samples.\n        k (int): Sparsity level of the true parameter vector.\n        sigma (float): Standard deviation of the noise.\n        rho (float): Correlation parameter for the design matrix columns.\n        rng (numpy.random.Generator): NumPy random number generator instance.\n        \n    Returns:\n        tuple: A_train, y_train, A_val, y_val\n    \"\"\"\n    # 1. Generate k-sparse ground-truth parameter vector x_star\n    x_star = np.zeros(d)\n    support = rng.choice(d, k, replace=False)\n    x_star[support] = rng.standard_normal(k)\n\n    # 2. Generate training and validation design matrices A\n    # Training data\n    Z_train = rng.standard_normal((n_train, d))\n    u_train = rng.standard_normal(n_train)\n    A_train = np.sqrt(1 - rho) * Z_train + np.sqrt(rho) * u_train[:, np.newaxis]\n    \n    # Validation data\n    Z_val = rng.standard_normal((n_val, d))\n    u_val = rng.standard_normal(n_val)\n    A_val = np.sqrt(1 - rho) * Z_val + np.sqrt(rho) * u_val[:, np.newaxis]\n\n    # 3. Generate response vectors y\n    eps_train = sigma * rng.standard_normal(n_train)\n    y_train = A_train @ x_star + eps_train\n    \n    eps_val = sigma * rng.standard_normal(n_val)\n    y_val = A_val @ x_star + eps_val\n\n    return A_train, y_train, A_val, y_val\n\ndef ista(A, y, lambda_reg, T, step_size):\n    \"\"\"\n    Implements the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n    \n    Args:\n        A (np.ndarray): Design matrix.\n        y (np.ndarray): Response vector.\n        lambda_reg (float): L1 regularization parameter.\n        T (int): Number of iterations.\n        step_size (float): Step size for the gradient descent step.\n        \n    Returns:\n        np.ndarray: The estimated parameter vector x after T iterations.\n    \"\"\"\n    n, d = A.shape\n    x = np.zeros(d)\n    \n    if T == 0:\n        return x\n\n    for _ in range(T):\n        # Gradient of the smooth term\n        grad = (1 / n) * A.T @ (A @ x - y)\n        \n        # Gradient update step\n        z = x - step_size * grad\n        \n        # Proximal operator (soft-thresholding)\n        x = np.sign(z) * np.maximum(np.abs(z) - step_size * lambda_reg, 0)\n        \n    return x\n\ndef calculate_mse(A, y, x):\n    \"\"\"Calculates the Mean Squared Error.\"\"\"\n    n = A.shape[0]\n    error = A @ x - y\n    return (1 / n) * np.sum(error**2)\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    test_cases = [\n        # d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda\n        (50, 80, 80, 5, 0.2, 0.2, 10, 500, 0.05),\n        (50, 80, 80, 5, 1.0, 0.0, 0, 500, 0.1),\n        (50, 80, 80, 5, 0.05, 0.9, 3, 500, 0.01),\n    ]\n\n    results = []\n    \n    for i, params in enumerate(test_cases):\n        d, n_train, n_val, k, sigma, rho, T_small, T_large, lambda_val = params\n        \n        # Ensure reproducibility for each case by fixing the seed\n        rng = np.random.default_rng(seed=i)\n\n        # Generate data\n        A_train, y_train, A_val, y_val = generate_data(d, n_train, n_val, k, sigma, rho, rng)\n        \n        # Calculate step size\n        C = (1 / n_train) * (A_train.T @ A_train)\n        L = np.max(np.linalg.eigvalsh(C))\n        step_size = 1.0 / L\n\n        # Early-stopped estimator (lambda = 0)\n        x_early = ista(A_train, y_train, lambda_reg=0.0, T=T_small, step_size=step_size)\n        \n        # Explicitly regularized estimator (lambda > 0)\n        x_reg = ista(A_train, y_train, lambda_reg=lambda_val, T=T_large, step_size=step_size)\n        \n        # Evaluate validation MSE for both\n        mse_early = calculate_mse(A_val, y_val, x_early)\n        mse_reg = calculate_mse(A_val, y_val, x_reg)\n        \n        # Compare and store result\n        result = 1 if mse_early  mse_reg else 0\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3168592"}]}