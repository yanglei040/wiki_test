{"hands_on_practices": [{"introduction": "本次实践将通过一个真实的生物信息学场景，为您提供监督学习和无监督学习的基础实践经验。您将实现K-均值聚类算法以发现转录因子结合谱中的自然分组，并构建一个最近质心分类器以根据已标记的样本预测转录因子的类别。通过完成这项练习，您将巩固对这两种范式核心机制与评估方法的理解 [@problem_id:2432815]。", "problem": "给定一个对源于染色质免疫沉淀测序 (ChIP-seq) 的全基因组结合图谱进行聚类和分类任务的形式化描述。每个图谱对应一个转录因子 (TF)。数据集表示为一个矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$，其中有 $n$ 个 TF（行）和 $m$ 个基因组区间（列），$X_{ij}$ 是一个非负整数计数。对于所有任务，请从基本原理出发定义以下操作和量。\n\n- 行归一化：对于每个 TF 图谱 $\\mathbf{x}_i \\in \\mathbb{R}_{\\ge 0}^m$，定义其 $\\ell_2$-归一化向量为 $\\mathbf{y}_i = \\mathbf{x}_i / \\lVert \\mathbf{x}_i \\rVert_2$，其中 $\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。如果 $\\lVert \\mathbf{x}_i \\rVert_2 = 0$，则将 $\\mathbf{y}_i$ 视作零向量。\n- 余弦距离：对于任意两个归一化向量 $\\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^m$，定义 $d(\\mathbf{u}, \\mathbf{v}) = 1 - \\mathbf{u}^\\top \\mathbf{v}$。\n- 归一化空间中的簇内平方和 (WCSS)：对于将 $\\{1,\\dots,n\\}$ 划分为 $k$ 个非空簇的一个划分 $\\mathcal{P} = \\{S_1, \\dots, S_k\\}$，以及对应的质心 $\\boldsymbol{\\mu}_c = \\frac{1}{|S_c|} \\sum_{i \\in S_c} \\mathbf{y}_i$，定义\n$$\n\\mathrm{WCSS}(\\mathcal{P}) = \\sum_{c=1}^k \\sum_{i \\in S_c} \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_c \\rVert_2^2.\n$$\n- 在归一化空间中使用余弦距离的轮廓系数：对于每个 $i \\in \\{1,\\dots,n\\}$ 及其分配的簇 $c(i)$，定义\n$$\na(i) = \\frac{1}{|S_{c(i)}|-1} \\sum_{j \\in S_{c(i)},\\, j \\ne i} d(\\mathbf{y}_i, \\mathbf{y}_j)\n$$\n如果 $|S_{c(i)}| \\ge 2$，如果 $|S_{c(i)}| = 1$ 则设 $a(i) = 0$。对于任何其他簇 $c' \\ne c(i)$，定义\n$$\n\\bar{d}(i, c') = \\frac{1}{|S_{c'}|} \\sum_{j \\in S_{c'}} d(\\mathbf{y}_i, \\mathbf{y}_j).\n$$\n令 $b(i) = \\min_{c' \\ne c(i)} \\bar{d}(i,c')$。$i$ 的轮廓系数为\n$$\ns(i) = \n\\begin{cases}\n0,  \\text{if } a(i) = 0 \\text{ and } b(i) = 0, \\\\\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{otherwise}.\n\\end{cases}\n$$\n平均轮廓系数为 $\\bar{s} = \\frac{1}{n} \\sum_{i=1}^n s(i)$。\n- 归一化空间中的最近质心分类：对于带标签的类别 $\\mathcal{C}$ 和一个训练集 $T \\subset \\{1,\\dots,n\\}$（其中对于 $i \\in T$ 有标签 $\\ell(i) \\in \\mathcal{C}$），定义类别均值 $\\boldsymbol{\\nu}_c = \\frac{1}{|\\{i \\in T : \\ell(i) = c\\}|} \\sum_{i \\in T, \\ell(i)=c} \\mathbf{y}_i$。对于每个测试样本 $j \\notin T$，预测其类别为 $\\hat{\\ell}(j) = \\arg\\min_{c \\in \\mathcal{C}} \\lVert \\mathbf{y}_j - \\boldsymbol{\\nu}_c \\rVert_2$。准确率是测试样本中正确预测的比例。如果某个类别均值为零向量，则直接使用该零向量。\n\n聚类任务：对于给定的候选集 $\\mathcal{K} \\subset \\mathbb{N}$（其中所有 $k \\in \\mathcal{K}$ 均满足 $k \\ge 2$）和一个无标签数据集，对每个 $k \\in \\mathcal{K}$，在归一化空间中生成一个使 $\\mathrm{WCSS}(\\mathcal{P}_k)$ 最小化的划分 $\\mathcal{P}_k$。令 $\\bar{s}_k$ 为 $\\mathcal{P}_k$ 的平均轮廓系数。选择 $k^\\star = \\arg\\max_{k \\in \\mathcal{K}} \\bar{s}_k$。如果 $\\bar{s}_k$ 出现平局，则选择最小的 $k$。如果所有成对距离均为零（即所有归一化向量都相同），则对于所有 $k$，将所有轮廓系数定义为 $0$。\n\n监督分类任务：对于给定的带标签数据集，在指定的测试集上计算如上定义的最近质心准确率。\n\n数据生成模型：除非另有说明，否则无标签数据集由每个区间的独立泊松分布的混合模型生成。对于簇 $c$，指定均值向量 $\\boldsymbol{\\lambda}^{(c)} \\in \\mathbb{R}_{>0}^m$；簇 $c$ 中的每个 TF $\\mathbf{x}$ 的独立分量 $x_j$ 均从 $\\mathrm{Poisson}(\\lambda^{(c)}_j)$ 分布中抽取，其中 $j \\in \\{1,\\dots,m\\}$。带标签数据集遵循相同的生成方式，其标签由生成它的簇给出。所有随机抽取均使用指定的随机种子完成，且簇按指定的样本数量实现。在所有图谱都是确定性且完全相同的边界情况下，对所有行使用提供的固定向量。\n\n测试套件。精确实现以下四个测试用例；这些测试用例构成您程序的全部输入。\n\n- 测试用例 $1$ (无标签聚类；正常路径):\n  - $n = 90$, $m = 60$, 随机种子 $= 123$。\n  - 三个簇 $C_1, C_2, C_3$，每个簇有 $30$ 个 TF。\n  - 背景均值 $\\mu_{\\mathrm{low}} = 2$ 和高表达均值 $\\mu_{\\mathrm{high}} = 30$。\n  - 均值：\n    - 对于 $C_1$：区间 $0$ 到 $19$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n    - 对于 $C_2$：区间 $20$ 到 $39$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n    - 对于 $C_3$：区间 $40$ 到 $59$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n  - 候选集 $\\mathcal{K} = \\{2,3,4,5\\}$。\n  - 此用例的输出：一个列表 $[k^\\star, \\bar{s}_{k^\\star}]$。\n\n- 测试用例 $2$ (无标签聚类；边界条件):\n  - $n = 20$, $m = 10$。\n  - 所有 TF 均相同且为确定性：$X$ 的每一行都等于同一个固定的非零向量，其所有分量都等于 $5$。\n  - 候选集 $\\mathcal{K} = \\{2,3\\}$。\n  - 此用例的输出：一个列表 $[k^\\star, \\bar{s}_{k^\\star}]$，必须反映为零距离指定的平局决胜规则和轮廓系数定义。\n\n- 测试用例 $3$ (监督分类；带标签评估):\n  - $n = 100$, $m = 40$, 随机种子 $= 321$。\n  - 两个类别 $A$ 和 $B$，每个类别有 $50$ 个 TF，由泊松模型生成。\n  - 背景均值 $\\mu_{\\mathrm{low}} = 2$ 和高表达均值 $\\mu_{\\mathrm{high}} = 25$。\n  - 均值：\n    - 对于类别 $A$：区间 $0$ 到 $19$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n    - 对于类别 $B$：区间 $20$ 到 $39$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n  - 训练/测试集划分：从每个类别中精确使用 $30$ 个 TF 进行训练，其余 $20$ 个 TF 用于测试。数据按 A 类样本在前、B 类样本在后的顺序排列；选择每个类别的前 $30$ 个样本（按此顺序）进行训练，其余用于测试。\n  - 此用例的输出：一个浮点数，等于测试集上的最近质心准确率，以 $[0,1]$ 范围的小数形式表示。\n\n- 测试用例 $4$ (无标签聚类；高维、部分重叠信号):\n  - $n = 60$, $m = 200$, 随机种子 $= 999$。\n  - 两个簇 $C_1, C_2$，每个簇有 $30$ 个 TF。\n  - 背景均值 $\\mu_{\\mathrm{low}} = 3$ 和高表达均值 $\\mu_{\\mathrm{high}} = 15$。\n  - 均值：\n    - 对于 $C_1$：区间 $0$ 到 $99$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n    - 对于 $C_2$：区间 $20$ 到 $119$（含）的均值为 $\\mu_{\\mathrm{high}}$，所有其他区间的均值为 $\\mu_{\\mathrm{low}}$。\n  - 候选集 $\\mathcal{K} = \\{2,3\\}$。\n  - 此用例的输出：一个列表 $[k^\\star, \\bar{s}_{k^\\star}]$。\n\n最终输出格式。您的程序应生成单行输出，其中包含四个测试用例的结果，结果为一个用方括号括起来的逗号分隔列表，顺序如下：测试用例 1、测试用例 2、测试用例 3、测试用例 4。每个测试用例的结果必须是布尔值、整数、浮点数或由这些类型组成的列表，与上述规定完全一致。例如，您的输出应类似于 $[ [k_1^\\star, \\bar{s}_{k_1^\\star}], [k_2^\\star, \\bar{s}_{k_2^\\star}], \\mathrm{acc}_3, [k_4^\\star, \\bar{s}_{k_4^\\star}] ]$，其中包含根据定义计算出的具体数值。不应打印任何额外文本。", "solution": "所提供的问题陈述是有效的。它在生物信息学和机器学习领域具有科学依据，问题阐述清晰，具有明确的数学定义和目标，并且没有任何矛盾、歧义或不合理的假设。该问题要求实现用于对 ChIP-seq 数据图谱进行聚类和分类的算法，并在指定的测试套件上进行评估。\n\n我的方法涉及直接实现指定的数学公式和程序。整个过程分为两个主要部分：一个无监督聚类任务和一个监督分类任务。\n\n首先，我为常用操作建立了辅助函数。原始数据是一个整数计数矩阵 $X \\in \\mathbb{R}_{\\ge 0}^{n \\times m}$，根据指定的泊松模型 $x_j \\sim \\mathrm{Poisson}(\\lambda_j)$ 生成。然后，每个行向量 $\\mathbf{x}_i$ 被归一化为单位向量 $\\mathbf{y}_i = \\mathbf{x}_i / \\lVert \\mathbf{x}_i \\rVert_2$，并注意处理 $\\lVert \\mathbf{x}_i \\rVert_2 = 0$ 的情况，此时 $\\mathbf{y}_i$ 成为零向量。\n\n对于聚类任务，目标是找到一个划分 $\\mathcal{P}_k$，以最小化簇内平方和 (WCSS)，其定义为 $\\mathrm{WCSS}(\\mathcal{P}) = \\sum_{c=1}^k \\sum_{i \\in S_c} \\lVert \\mathbf{y}_i - \\boldsymbol{\\mu}_c \\rVert_2^2$，其中 $\\boldsymbol{\\mu}_c$ 是簇的质心。这是 K-means 聚类的标准目标。由于找到全局最小值是一个 NP-hard 问题，我采用广泛使用的 K-means 算法（Lloyd's algorithm）作为启发式方法。使用了 `scipy.cluster.vq.kmeans` 的实现，该实现通过随机初始化执行多次运行并返回找到的最佳划分，从而为 WCSS 最小值提供了一个稳健的近似。\n\n一旦为每个候选簇数 $k \\in \\mathcal{K}$ 获得了划分 $\\mathcal{P}_k$，就使用平均轮廓系数 $\\bar{s}_k$ 来评估其质量。单个数据点 $i$ 的轮廓系数 $s(i)$ 是衡量其在分配的簇内与相邻簇相比的拟合程度的指标。它定义为 $s(i) = (b(i) - a(i)) / \\max\\{a(i), b(i)\\}$，其中 $a(i)$ 是到其自身簇中其他点的平均距离，而 $b(i)$ 是到任何其他簇中点的最小平均距离。所使用的距离度量是指定的余弦距离 $d(\\mathbf{u}, \\mathbf{v}) = 1 - \\mathbf{u}^\\top \\mathbf{v}$。我的实现计算所有点的 $s(i)$ 并将它们平均以得到 $\\bar{s}_k$。我处理了所有成对距离均为零的特殊情况，此时轮廓系数定义为零。最优簇数 $k^\\star$ 被选为最大化 $\\bar{s}_k$ 的那个，平局则通过选择最小的 $k$ 来解决。\n\n对于监督分类任务，模型是一个最近质心分类器。给定一个带类别标签 $\\{\\ell(i)\\}$ 的归一化向量训练集 $\\{\\mathbf{y}_i\\}$，我首先为每个类别 $c$ 计算均值向量或质心 $\\boldsymbol{\\nu}_c$。对于任何新的测试向量 $\\mathbf{y}_j$，通过找到欧几里得距离上离它最近的质心来预测其类别：$\\hat{\\ell}(j) = \\arg\\min_c \\lVert \\mathbf{y}_j - \\boldsymbol{\\nu}_c \\rVert_2$。分类器的性能由其准确率来衡量，即测试集中正确预测标签的比例。\n\n然后将这些已实现的程序应用于四个指定的测试用例，每个用例都有其自己的数据生成和评估任务参数。严格遵循随机数生成的种子以确保可复现性。最终结果被收集并按要求格式化为单个列表。", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.vq import kmeans, vq\n\ndef solve():\n    \"\"\"\n    Main function to solve the four test cases and print the final result.\n    \"\"\"\n    \n    def generate_data(cluster_means, samples_per_cluster, seed):\n        \"\"\"Generates data from a mixture of Poisson distributions.\"\"\"\n        rng = np.random.default_rng(seed)\n        X_parts = []\n        for i, lambda_vec in enumerate(cluster_means):\n            n_samples = samples_per_cluster[i]\n            m = lambda_vec.shape[0]\n            cluster_data = rng.poisson(lambda_vec, size=(n_samples, m))\n            X_parts.append(cluster_data)\n        return np.vstack(X_parts)\n\n    def normalize_data(X):\n        \"\"\"L2-normalizes the rows of matrix X.\"\"\"\n        norms = np.linalg.norm(X, axis=1, keepdims=True)\n        # Handle zero-norm vectors as per problem statement\n        Y = np.divide(X, norms, out=np.zeros_like(X, dtype=float), where=(norms != 0))\n        return Y\n\n    def calculate_mean_silhouette(Y, labels):\n        \"\"\"Calculates the mean silhouette coefficient for a given partition.\"\"\"\n        n = Y.shape[0]\n        unique_labels, counts = np.unique(labels, return_counts=True)\n        k_found = len(unique_labels)\n        \n        # Pre-compute pairwise cosine distances\n        dot_products = Y @ Y.T\n        np.clip(dot_products, -1.0, 1.0, out=dot_products) # For floating point precision\n        D = 1.0 - dot_products\n\n        # Per problem statement for case of identical vectors\n        if np.allclose(D, 0):\n            return 0.0\n\n        s_coeffs = np.zeros(n)\n        for i in range(n):\n            c_i_label = labels[i]\n            \n            is_in_cluster_i = (labels == c_i_label)\n            count_in_cluster_i = counts[unique_labels == c_i_label][0]\n            \n            if count_in_cluster_i > 1:\n                a_i = np.sum(D[i, is_in_cluster_i]) / (count_in_cluster_i - 1)\n            else: # Singleton cluster\n                a_i = 0.0\n            \n            b_i = np.inf\n            if k_found > 1:\n                for c_prime_label in unique_labels:\n                    if c_prime_label == c_i_label:\n                        continue\n                    is_in_cluster_prime = (labels == c_prime_label)\n                    count_in_cluster_prime = counts[unique_labels == c_prime_label][0]\n                    d_bar = np.sum(D[i, is_in_cluster_prime]) / count_in_cluster_prime\n                    if d_bar  b_i:\n                        b_i = d_bar\n            \n            if np.isinf(b_i): # Only happens if k_found = 1\n                b_i = 0.0\n\n            if a_i == 0.0 and b_i == 0.0:\n                s_coeffs[i] = 0.0\n            else:\n                denom = max(a_i, b_i)\n                s_coeffs[i] = (b_i - a_i) / denom\n        \n        return np.mean(s_coeffs)\n\n    def test_case_1():\n        n, m, seed = 90, 60, 123\n        n_per_cluster = 30\n        mu_low, mu_high = 2, 30\n        K_set = [2, 3, 4, 5]\n        \n        np.random.seed(seed)\n        \n        lambda1 = np.full(m, mu_low); lambda1[0:20] = mu_high\n        lambda2 = np.full(m, mu_low); lambda2[20:40] = mu_high\n        lambda3 = np.full(m, mu_low); lambda3[40:60] = mu_high\n        \n        X = generate_data([lambda1, lambda2, lambda3], [n_per_cluster] * 3, seed)\n        Y = normalize_data(X)\n        \n        s_scores = []\n        for k in K_set:\n            centroids, _ = kmeans(Y, k, iter=100)\n            labels, _ = vq(Y, centroids)\n            s_k = calculate_mean_silhouette(Y, labels)\n            s_scores.append(s_k)\n\n        best_s_idx = np.argmax(s_scores)\n        k_star = K_set[best_s_idx]\n        best_s = s_scores[best_s_idx]\n\n        return [k_star, best_s]\n\n    def test_case_2():\n        n, m = 20, 10\n        K_set = [2, 3]\n        \n        X = np.full((n, m), 5.0)\n        Y = normalize_data(X)\n        \n        # All pairwise distances are zero. By definition, silhouette is 0 for all k.\n        s_scores = {k: 0.0 for k in K_set}\n\n        # Tie-breaking rule: select smallest k\n        k_star = min(K_set)\n        s_k_star = s_scores[k_star]\n        \n        return [k_star, s_k_star]\n\n    def test_case_3():\n        n, m, seed = 100, 40, 321\n        n_per_class = 50\n        mu_low, mu_high = 2, 25\n        \n        np.random.seed(seed)\n        \n        lambdaA = np.full(m, mu_low); lambdaA[0:20] = mu_high\n        lambdaB = np.full(m, mu_low); lambdaB[20:40] = mu_high\n        \n        X = generate_data([lambdaA, lambdaB], [n_per_class] * 2, seed)\n        labels = np.concatenate([np.zeros(n_per_class), np.ones(n_per_class)])\n        \n        train_indices = np.concatenate([np.arange(0, 30), np.arange(50, 80)])\n        test_indices = np.concatenate([np.arange(30, 50), np.arange(80, 100)])\n        \n        Y = normalize_data(X)\n        Y_train, labels_train = Y[train_indices], labels[train_indices]\n        Y_test, labels_test = Y[test_indices], labels[test_indices]\n        \n        mean_A = Y_train[labels_train == 0].mean(axis=0)\n        mean_B = Y_train[labels_train == 1].mean(axis=0)\n        \n        predictions = []\n        for y_j in Y_test:\n            dist_A = np.linalg.norm(y_j - mean_A)\n            dist_B = np.linalg.norm(y_j - mean_B)\n            predictions.append(0 if dist_A  dist_B else 1)\n        \n        return np.mean(np.array(predictions) == labels_test)\n        \n    def test_case_4():\n        n, m, seed = 60, 200, 999\n        n_per_cluster = 30\n        mu_low, mu_high = 3, 15\n        K_set = [2, 3]\n\n        np.random.seed(seed)\n        \n        lambda1 = np.full(m, mu_low); lambda1[0:100] = mu_high\n        lambda2 = np.full(m, mu_low); lambda2[20:120] = mu_high\n        \n        X = generate_data([lambda1, lambda2], [n_per_cluster] * 2, seed)\n        Y = normalize_data(X)\n        \n        s_scores = []\n        for k in K_set:\n            centroids, _ = kmeans(Y, k, iter=100)\n            labels, _ = vq(Y, centroids)\n            s_k = calculate_mean_silhouette(Y, labels)\n            s_scores.append(s_k)\n\n        best_s_idx = np.argmax(s_scores)\n        k_star = K_set[best_s_idx]\n        best_s = s_scores[best_s_idx]\n\n        return [k_star, best_s]\n\n    results = [\n        test_case_1(),\n        test_case_2(),\n        test_case_3(),\n        test_case_4()\n    ]\n\n    # Format the final output string as specified\n    # The default str() for lists includes spaces, which we remove.\n    print(f\"[{','.join(map(lambda x: str(x).replace(' ', ''), results))}]\")\n\nsolve()\n```", "id": "2432815"}, {"introduction": "在掌握了两种方法的基本应用后，本次实践将引导您探索一个更深层次的概念：监督学习与无监督学习的优化目标常常不一致。通过分析三个精心设计的数据集，您将亲眼见证高分类准确率与优良聚类结构（高轮廓系数）之间并非总能划上等号 [@problem_id:3199424]。这个对比练习旨在揭示两者目标上的根本分歧——前者追求预测准确性，后者则致力于发现数据内在的几何结构。", "problem": "在监督学习中，训练一个分类器将特征向量映射到给定的标签，以最小化经验误差；而在无监督学习中，仅从无标签的数据中推断结构。考虑二维欧几里得空间中的点，其特征表示为 $x = (x_1, x_2) \\in \\mathbb{R}^2$，二元类别标签为 $y \\in \\{0, 1\\}$。监督学习的目标是为每个 $x$ 选择预测值 $\\hat{y}$，以最小化训练集上的经验分类误差，这是经验风险最小化（ERM）的一个直接实例。聚类中的无监督目标是将点划分到 $k$ 个组中，以优化一个内聚性和分离性准则；一个广泛使用的选项是基于两两之间欧几里得距离的轮廓分数。本问题将此比较操作化：您将使用一个在标签上训练的简单阈值分类器计算监督学习的准确率，并使用在原始特征上运行的、采用确定性初始化的 K-Means 聚类（K-Means）计算无监督学习的轮廓分数，然后评估三个旨在突出两种目标何时产生分歧的数据集。\n\n定义与所需计算：\n- 监督分类器。限定于对第一个坐标 $x_1$ 的一维决策。对于一个阈值 $t \\in \\mathbb{R}$ 和一个左侧类别 $c_{\\text{left}} \\in \\{0, 1\\}$，分类器预测\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}},  \\text{if } x_1 \\le t,\\\\\n1 - c_{\\text{left}},  \\text{if } x_1  t.\n\\end{cases}\n$$\n在所有由连续排序的 $x_1$ 值之间的中点（包括略低于最小值和略高于最大值的值）构成的阈值中，选择能使训练准确率最大化的 $(t, c_{\\text{left}})$\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right),\n$$\n其中 $N$ 是样本数量，$\\mathbf{1}(\\cdot)$ 是指示函数。在出现平局时，选择 $c_{\\text{left}} = 0$ 和达到最大值的最小 $t$。将 $\\text{Acc}$ 报告为 $[0,1]$ 范围内的一个小数。\n- 无监督聚类。在特征向量 $x^{(i)}$ 上使用欧几里得距离运行 K-Means，设置 $k=2$ 个簇。将两个质心确定性地初始化为具有最小和最大 $x_1$ 值的点。迭代地将点重新分配给最近的质心，并将质心更新为所分配点的均值，直到分配稳定或达到 100 次迭代。如果某个簇变为空，则将其质心重新初始化为距离另一个质心最远（以欧几里得距离计算）的点。收敛后，计算轮廓分数。对于每个点 $i$，令 $a(i)$ 为从 $x^{(i)}$ 到其所属簇中所有其他点的平均欧几里得距离（如果该簇的大小为 1，则令 $a(i)=0$）。令 $b(i)$ 为从 $x^{(i)}$ 到其他所有簇中点的平均欧几里得距离的最小值。轮廓值为\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\}  0,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\n而轮廓分数是所有点的 $s(i)$ 的均值。\n\n测试套件数据集（每个样本以 $(x_1, x_2)$ 及其关联标签 $y$ 的形式给出；顺序是任意的，所有数字都是实数）：\n1. 数据集 A（沿 $x_1$ 轴中度分离但有轻微重叠）：\n   - 类别 0：$(-0.4, 0.0)$, $(-0.2, 0.1)$, $(0.2, -0.1)$, $(0.3, 0.05)$\n   - 类别 1：$(0.2, 0.0)$, $(0.6, -0.1)$, $(0.8, 0.1)$, $(1.0, -0.05)$\n2. 数据集 B（沿 $x_1$ 轴分离度较大，但沿 $x_2$ 轴有较大噪声）：\n   - 类别 0：$(-0.4, 5.0)$, $(-0.2, -5.0)$, $(0.2, 7.0)$, $(0.3, -7.0)$\n   - 类别 1：$(1.2, 6.0)$, $(1.6, -6.0)$, $(1.8, 8.0)$, $(2.0, -8.0)$\n3. 数据集 C（近乎完全重叠：通过构造得到相同的分布）：\n   - 类别 0：$(-0.1, 0.05)$, $(0.0, -0.02)$, $(0.1, 0.03)$, $(-0.05, -0.04)$\n   - 类别 1：$(-0.08, 0.01)$, $(0.02, -0.03)$, $(0.09, 0.04)$, $(-0.02, -0.05)$\n\n您的程序必须实现上述过程，并为每个数据集输出数对 $[\\text{Acc}, \\text{Sil}]$，其中 $\\text{Acc}$ 是监督训练准确率，$\\text{Sil}$ 是最终 K-Means 聚类的轮廓分数。将每个值表示为四舍五入到六位小数的小数。最终输出格式：一行包含一个顶级列表，其中有三对数对，按数据集 A、B、C 的顺序排列，条目之间用逗号分隔，例如 $[[a_1,s_1],[a_2,s_2],[a_3,s_3]]$，数字保留六位小数。", "solution": "用户提供的问题经评估有效。它科学上基于机器学习的原理，提法明确，其确定性过程保证了唯一解，并以客观、正式的语言表述。求解所需的所有数据和定义都已提供且保持一致。我现在将开始提供一个完整的解决方案。\n\n该问题要求对三个数据集上的监督学习和无监督学习进行比较分析。对于每个数据集，我们必须计算两个指标：一个简单监督分类器的准确率和一个无监督聚类算法的轮廓分数。\n\n一个数据集由二维空间中的 $N$ 个点组成，其中每个点 $x^{(i)} = (x_1^{(i)}, x_2^{(i)}) \\in \\mathbb{R}^2$ 关联一个二元标签 $y^{(i)} \\in \\{0, 1\\}$。\n\n### 监督分类：一维阈值分类器\n\n监督学习任务是找到最优的一维阈值分类器，以最小化训练集上的分类误差。这是经验风险最小化（ERM）的一个实例。\n\n对于一个点 $x = (x_1, x_2)$，分类器的预测值 $\\hat{y}(x)$ 基于第一个特征 $x_1$ 上的阈值 $t$ 和决策边界“左侧”的类别分配 $c_{\\text{left}} \\in \\{0, 1\\}$ 来定义：\n$$\n\\hat{y}(x) = \\begin{cases}\nc_{\\text{left}},  \\text{if } x_1 \\le t, \\\\\n1 - c_{\\text{left}},  \\text{if } x_1  t.\n\\end{cases}\n$$\n目标是选择能使训练准确率最大化的数对 $(t, c_{\\text{left}})$，准确率定义为：\n$$\n\\text{Acc} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\left(\\hat{y}(x^{(i)}) = y^{(i)}\\right)\n$$\n其中 $\\mathbf{1}(\\cdot)$ 是指示函数，当其参数为真时值为 1，否则为 0。\n\n为了找到最优分类器，我们必须在一组候选阈值 $t$ 和 $c_{\\text{left}}$ 的两个可能值上进行搜索。分类边界只需在那些能够改变数据划分的位置进行检查。设第一个特征的唯一排序值为 $u_1  u_2  \\dots  u_m$。有意义的阈值集合由这些连续值之间的中点构成，即 $t_j = (u_j + u_{j+1})/2$，其中 $j=1, \\dots, m-1$。此外，为了考虑所有点都被分入单个组的情况，我们测试低于最小 $x_1$ 值（例如 $u_1 - 1$）和高于最大 $x_1$ 值（例如 $u_m + 1$）的阈值。\n\n对于每个候选阈值 $t$，我们评估 $c_{\\text{left}} = 0$ 和 $c_{\\text{left}} = 1$ 两种情况下的准确率。我们选择产生最高准确率的数对 $(t, c_{\\text{left}})$。问题指定了一个平局决胜规则：在准确率相同的情况下，我们必须选择 $c_{\\text{left}} = 0$ 的分类器。如果仍然存在平局，则选择具有最小阈值 $t$ 的那个。\n\n### 无监督聚类：带轮廓分数的 K-Means\n\n无监督任务是使用 K-Means 算法将数据点 $x^{(i)}$ 划分到 $k=2$ 个簇中，然后使用轮廓分数评估此划分的质量。\n\n**K-Means 算法**\n该算法按以下步骤进行：\n1.  **初始化**：两个质心 $\\mu_1$ 和 $\\mu_2$ 分别被确定性地初始化为具有最小和最大第一特征值 $x_1$ 的数据点。\n2.  **迭代**：算法在两个步骤之间进行迭代，直到簇分配稳定或达到 100 次迭代的最大次数。\n    a.  **分配步骤**：根据欧几里得距离，每个数据点 $x^{(i)}$ 被分配给对应最近质心的簇：$C_j \\leftarrow \\{x^{(i)} \\mid \\|x^{(i)} - \\mu_j\\|_2 \\le \\|x^{(i)} - \\mu_l\\|_2 \\text{ for all } l=1, \\dots, k\\}$。\n    b.  **更新步骤**：每个质心 $\\mu_j$ 被重新计算为其簇内所有分配点的均值：$\\mu_j \\leftarrow \\frac{1}{|C_j|} \\sum_{x \\in C_j} x$。\n3.  **空簇处理**：如果在更新步骤中某个簇 $C_j$ 变为空，则其质心将被重新初始化。新质心被设置为整个数据集中距离另一个（非空）簇的质心最远（以欧几里得距离计算）的数据点。\n\n**轮廓分数**\nK-Means 算法收敛后，使用轮廓分数评估得到的聚类结果。对于每个点 $x^{(i)}$，我们计算：\n-   $a(i)$: 从 $x^{(i)}$ 到其自身簇内所有其他点的平均欧几里得距离。如果该簇只包含一个点，则 $a(i) = 0$。\n-   $b(i)$: 从 $x^{(i)}$ 到最近的邻近簇中所有点的平均欧几里得距离。由于 $k=2$，这即是到另一个簇中所有点的平均距离。\n\n点 $i$ 的轮廓值则为：\n$$\ns(i) = \\begin{cases}\n\\frac{b(i) - a(i)}{\\max\\{a(i), b(i)\\}},  \\text{if } \\max\\{a(i), b(i)\\}  0, \\\\\n0,  \\text{otherwise}.\n\\end{cases}\n$$\n整个聚类的轮廓分数是所有数据点的轮廓值 $s(i)$ 的均值。接近 1 的分数表示分离良好的簇，接近 0 的分数表示重叠的簇，而负分则表明点可能被分配到了错误的簇。\n\n### 应用于数据集\n\n所述过程将应用于所提供的三个数据集中的每一个。对于每个数据集，我们首先构建特征矩阵 $X$ 和标签向量 $y$。然后，我们执行监督学习准确率计算和无监督 K-Means/轮廓分数计算。为每个数据集计算得到的结果数对 $[\\text{Acc}, \\text{Sil}]$，每个值都四舍五入到六位小数。\n\n- **数据集 A** 呈现了一个沿 $x_1$ 轴中度分离但有部分重叠的场景，使得监督任务具有一定难度。无监督聚类的结果将取决于点云的几何结构。\n- **数据集 B** 的设计使得标签沿 $x_1$ 轴是完全可分的，但沿 $x_2$ 轴的大方差起到了干扰作用。这测试了无监督算法是否会被噪声特征 $x_2$ 误导，该特征在欧几里得距离度量中占主导地位。\n- **数据集 C** 的两个类别具有高度重叠的分布。预计监督和无监督方法都将表现不佳，导致低准确率（接近随机猜测的 0.5）和低轮廓分数（接近 0）。\n\n最终输出是一个列表，按顺序包含数据集 A、B 和 C 的 $[\\text{Acc}, \\text{Sil}]$ 数对。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef _calculate_accuracy(X, y):\n    \"\"\"\n    Calculates the maximum accuracy of a 1D threshold classifier on x1.\n    \"\"\"\n    x1 = X[:, 0]\n    \n    unique_x1 = np.unique(x1)\n    \n    thresholds = []\n    if len(unique_x1) > 1:\n        # Midpoints between unique consecutive sorted x1 values\n        thresholds.extend((unique_x1[:-1] + unique_x1[1:]) / 2)\n    # Add thresholds to classify all points into one group\n    thresholds.append(unique_x1[0] - 1.0)\n    thresholds.append(unique_x1[-1] + 1.0)\n    \n    # Use a tuple (acc, c_left, t) for lexicographical comparison\n    # Goal: max(acc), min(c_left), min(t)\n    # This is equivalent to finding max of (acc, 1-c_left, -t)\n    best_params = (-1.0, 2, float('inf')) # (acc, c_left, t)\n\n    for t in sorted(list(set(thresholds))):\n        for c_left in [0, 1]:\n            c_right = 1 - c_left\n            y_pred = np.where(x1 = t, c_left, c_right)\n            acc = np.mean(y_pred == y)\n            \n            # Tie-breaking logic\n            current_params = (acc, c_left, t)\n            if current_params[0] > best_params[0]:\n                best_params = current_params\n            elif current_params[0] == best_params[0]:\n                if current_params[1]  best_params[1]:\n                    best_params = current_params\n                elif current_params[1] == best_params[1] and current_params[2]  best_params[2]:\n                    best_params = current_params\n                    \n    return best_params[0]\n\ndef _calculate_silhouette(X):\n    \"\"\"\n    Performs K-Means clustering (k=2) and calculates the silhouette score.\n    \"\"\"\n    N, D = X.shape\n    k = 2\n\n    if N  k:\n        return 0.0\n\n    # 1. Deterministic Initialization\n    idx_min_x1 = np.argmin(X[:, 0])\n    idx_max_x1 = np.argmax(X[:, 0])\n    \n    if idx_min_x1 == idx_max_x1:\n        # Pathological case not present in test data but good practice to handle.\n        # If all x1 are same, initialization would give one centroid.\n        # For this problem's scope, we assume this does not happen.\n        if N > 1:\n             idx_max_x1 = (idx_min_x1 + 1) % N # Pick another point\n        else:\n             return 0.0 # Single point has 0 silhouette score.\n    \n    centroids = np.array([X[idx_min_x1], X[idx_max_x1]], dtype=np.float64)\n    \n    assignments = np.zeros(N, dtype=int)\n    \n    for _ in range(100):\n        # 2. Assignment step (vectorized)\n        distances = np.linalg.norm(X[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)\n        new_assignments = np.argmin(distances, axis=1)\n            \n        # 3. Check for convergence\n        if np.array_equal(assignments, new_assignments):\n            break\n        assignments = new_assignments\n        \n        # 4. Update step with empty cluster handling\n        new_centroids = np.zeros_like(centroids)\n        cluster_sizes = np.bincount(assignments, minlength=k)\n\n        # First, compute centroids for non-empty clusters\n        non_empty_updated = [False, False]\n        for j in range(k):\n            if cluster_sizes[j] > 0:\n                new_centroids[j] = np.mean(X[assignments == j], axis=0)\n                non_empty_updated[j] = True\n\n        # Then, handle any empty clusters\n        for j in range(k):\n            if cluster_sizes[j] == 0:\n                other_j = 1 - j\n                # Use the newly computed centroid of the other cluster\n                other_centroid = new_centroids[other_j]\n                \n                # Reinitialize as the point farthest from the other centroid\n                dists_from_other_centroid = np.linalg.norm(X - other_centroid, axis=1)\n                farthest_point_idx = np.argmax(dists_from_other_centroid)\n                new_centroids[j] = X[farthest_point_idx]\n        \n        centroids = new_centroids\n    \n    # After convergence, use the final assignments\n    assignments = new_assignments\n    \n    # 5. Silhouette score calculation\n    if len(np.unique(assignments))  2:\n        return 0.0\n\n    silhouette_values = np.zeros(N)\n    for i in range(N):\n        point_i = X[i]\n        cluster_idx = assignments[i]\n        \n        # Intra-cluster distance a(i)\n        mask_a = (assignments == cluster_idx)  (np.arange(N) != i)\n        if not np.any(mask_a):\n            a_i = 0.0\n        else:\n            a_i = np.mean(np.linalg.norm(X[mask_a] - point_i, axis=1))\n\n        # Inter-cluster distance b(i)\n        other_cluster_idx = 1 - cluster_idx\n        mask_b = assignments == other_cluster_idx\n        \n        if not np.any(mask_b): # Should not happen if len(unique(assignments)) > 1\n             b_i = 0.0\n        else:\n             b_i = np.mean(np.linalg.norm(X[mask_b] - point_i, axis=1))\n\n        if max(a_i, b_i) == 0:\n            silhouette_values[i] = 0.0\n        else:\n            silhouette_values[i] = (b_i - a_i) / max(a_i, b_i)\n            \n    return np.mean(silhouette_values)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Dataset A\n        {\n            \"class_0\": [(-0.4, 0.0), (-0.2, 0.1), (0.2, -0.1), (0.3, 0.05)],\n            \"class_1\": [(0.2, 0.0), (0.6, -0.1), (0.8, 0.1), (1.0, -0.05)]\n        },\n        # Dataset B\n        {\n            \"class_0\": [(-0.4, 5.0), (-0.2, -5.0), (0.2, 7.0), (0.3, -7.0)],\n            \"class_1\": [(1.2, 6.0), (1.6, -6.0), (1.8, 8.0), (2.0, -8.0)]\n        },\n        # Dataset C\n        {\n            \"class_0\": [(-0.1, 0.05), (0.0, -0.02), (0.1, 0.03), (-0.05, -0.04)],\n            \"class_1\": [(-0.08, 0.01), (0.02, -0.03), (0.09, 0.04), (-0.02, -0.05)]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        points_0 = np.array(case[\"class_0\"])\n        points_1 = np.array(case[\"class_1\"])\n        \n        X = np.concatenate((points_0, points_1), axis=0)\n        y = np.concatenate((np.zeros(len(points_0), dtype=int), np.ones(len(points_1), dtype=int)), axis=0)\n\n        accuracy = _calculate_accuracy(X, y)\n        silhouette = _calculate_silhouette(X)\n        \n        results.append([accuracy, silhouette])\n\n    # Final print statement in the exact required format.\n    results_as_strings = [f\"[{acc:.6f},{sil:.6f}]\" for acc, sil in results]\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3199424"}, {"introduction": "最后的这项实践将探讨两种学习范式之间更微妙、更高级的相互作用。您将通过一个实验，了解数据中潜在的（无监督）聚类结构如何能够主动误导一个常见的监督式特征选择方法 [@problem_id:3199415]。这个练习旨在警示我们，即使任务目标是纯粹的监督学习，忽视数据本身的内在结构也可能导致对特征重要性的判断出现偏差。", "problem": "您的任务是构建一个合成数据集，以分析监督学习和非监督学习之间的相互作用。\n\n请使用以下基本原理：监督线性回归的定义，其目标变量 $Y$ 由 $Y = \\alpha^\\top X + \\epsilon$ 给出，其中 $\\epsilon$ 是加性噪声；以及非监督混合模型的定义，其中特征向量 $X$ 从多元高斯分量的混合中抽取。目标是基于这些基础进行推理，以检验 $X$ 中的多模态结构（来自聚类）是否会误导一个基于单变量相关的监督式特征选择规则。\n\n数据生成过程定义如下。设 $X \\in \\mathbb{R}^d$ 是一个从包含 $K$ 个簇的混合模型中抽取的随机向量。对于以概率 $\\{p_1,\\dots,p_K\\}$（满足 $\\sum_{k=1}^K p_k = 1$）抽样的簇索引 $C \\in \\{1, \\dots, K\\}$，抽取\n$$\nX \\mid C = k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k),\n$$\n其中 $\\mu_k \\in \\mathbb{R}^d$ 是簇 $k$ 的均值向量，$\\Sigma_k$ 是一个对角协方差矩阵，其对角线元素为各特征的簇内方差。然后，定义监督回归目标\n$$\nY = \\alpha^\\top X + \\epsilon,\n$$\n其中 $\\alpha \\in \\mathbb{R}^d$，且 $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ 独立于 $X$。\n\n考虑一个特征选择规则，该规则通过每个独立特征 $X_j$ 与目标 $Y$ 之间的样本 Pearson 相关系数的绝对值对特征进行排序。特征 $j$ 的 Pearson 相关性为\n$$\n\\rho_j = \\frac{\\widehat{\\operatorname{Cov}}(X_j, Y)}{\\widehat{\\sigma}(X_j)\\,\\widehat{\\sigma}(Y)},\n$$\n其中 $\\widehat{\\operatorname{Cov}}$ 是无偏样本协方差，$\\widehat{\\sigma}$ 是无偏样本标准差。如果一个特征在 $\\alpha$ 中对应的系数非零，则称其为“预测性”特征，否则称为“非预测性”特征。\n\n您的程序必须：\n1. 对于每个测试用例，根据上述混合模型和线性回归目标生成 $n$ 个独立的 $(X,Y)$ 样本，使用一个对角矩阵 $\\Sigma_k$，其对角线元素为指定的各特征的簇内标准差，且所有簇的对角矩阵相同。\n2. 对每个特征 $j \\in \\{1,\\dots,d\\}$ 计算样本 Pearson 相关系数的绝对值 $|\\rho_j|$。\n3. 使用以下可量化的“被误导”标准，判断聚类结构是否会误导基于相关的监督式特征选择：如果存在至少一个非预测性特征与 $Y$ 的绝对相关性严格大于至少一个预测性特征的绝对相关性，则该测试用例被标记为被误导。形式上，设 $P = \\{j:\\alpha_j \\neq 0\\}$ 且 $N = \\{j:\\alpha_j = 0\\}$。如果满足以下条件，则该情况为被误导：\n$$\n\\max_{j \\in N} |\\rho_j|  \\min_{i \\in P} |\\rho_i|.\n$$\n\n使用以下测试套件以确保覆盖不同场景：\n\n- 测试用例 1（理想路径下通过对齐的簇均值导致误导）：\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, 0.1, 0.0]$,\n  - 簇均值：$\\mu_1 = [2.0, 0.0, 2.0]$, $\\mu_2 = [-2.0, 0.0, -2.0]$,\n  - 簇内标准差：各特征 $\\sigma = [0.5, 0.1, 0.5]$,\n  - 簇概率：$p_1 = 0.5$, $p_2 = 0.5$,\n  - 噪声标准差：$\\sigma_\\epsilon = 0.2$,\n  - 随机种子：$42$。\n\n- 测试用例 2（在不平衡簇下导致误导）：\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, 0.1, 0.0]$,\n  - 簇均值：$\\mu_1 = [2.0, 0.0, 2.0]$, $\\mu_2 = [-2.0, 0.0, -2.0]$,\n  - 簇内标准差：各特征 $\\sigma = [0.5, 0.1, 0.5]$,\n  - 簇概率：$p_1 = 0.9$, $p_2 = 0.1$,\n  - 噪声标准差：$\\sigma_\\epsilon = 0.2$,\n  - 随机种子：$123$。\n\n- 测试用例 3（无簇分离的边界情况）：\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, 0.1, 0.0]$,\n  - 簇均值：$\\mu_1 = [0.0, 0.0, 0.0]$, $\\mu_2 = [0.0, 0.0, 0.0]$,\n  - 簇内标准差：各特征 $\\sigma = [0.5, 0.1, 0.5]$,\n  - 簇概率：$p_1 = 0.5$, $p_2 = 0.5$,\n  - 噪声标准差：$\\sigma_\\epsilon = 0.2$,\n  - 随机种子：$7$。\n\n- 测试用例 4（簇偏移与监督信号正交的边缘情况）：\n  - $n = 5000$, $d = 3$, $K = 2$,\n  - $\\alpha = [1.0, -1.0, 0.0]$,\n  - 簇均值：$\\mu_1 = [2.0, 2.0, 2.0]$, $\\mu_2 = [-2.0, -2.0, -2.0]$,\n  - 簇内标准差：各特征 $\\sigma = [0.5, 0.5, 0.5]$,\n  - 簇概率：$p_1 = 0.5$, $p_2 = 0.5$,\n  - 噪声标准差：$\\sigma_\\epsilon = 0.2$,\n  - 随机种子：$2024$。\n\n最终输出规格：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表中的每个条目是对应于测试用例 [1,2,3,4] 顺序的布尔值。例如，输出格式必须与 $[r_1,r_2,r_3,r_4]$ 完全一样，其中每个 $r_i$ 为 $True$ 或 $False$。\n- 此问题不涉及任何物理单位。", "solution": "该问题要求我们研究一个场景，其中基于监督标准（与目标变量的相关性）的特征选择可能会被特征空间中潜在的非监督结构（聚类）所误导。我们需要构建一个合成数据集，应用一个特定的基于相关的特征选择规则，并根据一个精确的数学定义来判断该规则是否“被误导”。\n\n问题的核心在于理解特征 $X$ 的统计特性及其与目标 $Y$ 的关系如何受数据生成过程的影响。特征向量 $X \\in \\mathbb{R}^d$ 从高斯混合模型（GMM）中抽取，目标变量 $Y$ 是 $X$ 的线性函数加上加性噪声。\n\n让我们首先分析特征 $X_j$ 和目标 $Y$ 之间的理论关系。Pearson 相关系数 $\\rho_j$ 是协方差 $\\operatorname{Cov}(X_j, Y)$ 的归一化版本。我们来分析这个协方差。\n给定 $Y = \\alpha^\\top X + \\epsilon = \\sum_{i=1}^d \\alpha_i X_i + \\epsilon$，协方差为：\n$$\n\\operatorname{Cov}(X_j, Y) = \\operatorname{Cov}\\left(X_j, \\sum_{i=1}^d \\alpha_i X_i + \\epsilon\\right)\n$$\n根据协方差的线性和 $\\epsilon$ 独立于 $X$（这意味着 $\\operatorname{Cov}(X_j, \\epsilon) = 0$），我们有：\n$$\n\\operatorname{Cov}(X_j, Y) = \\sum_{i=1}^d \\alpha_i \\operatorname{Cov}(X_j, X_i)\n$$\n项 $\\operatorname{Cov}(X_j, X_i)$ 是特征 $j$ 和 $i$ 之间的总协方差。问题指出 $X$ 是从一个混合模型中抽取的。我们可以使用全协方差定律来分解总协方差：\n$$\n\\operatorname{Cov}(X_j, X_i) = \\mathbb{E}[\\operatorname{Cov}(X_j, X_i \\mid C)] + \\operatorname{Cov}(\\mathbb{E}[X_j \\mid C], \\mathbb{E}[X_i \\mid C])\n$$\n这里，$C$ 是表示簇分配的随机变量。\n第一项 $\\mathbb{E}[\\operatorname{Cov}(X_j, X_i \\mid C)]$ 是平均簇内协方差。问题指定簇内协方差矩阵 $\\Sigma_k$ 是对角的，元素为 $\\sigma_{k,j}^2$。此外，所有簇共享相同的对角协方差，因此对于所有 $k$，$\\sigma_{k,j}^2 = \\sigma_j^2$。因此，$\\operatorname{Cov}(X_j, X_i \\mid C=k) = \\delta_{ij} \\sigma_j^2$，其中 $\\delta_{ij}$ 是克罗内克δ函数。对 $C$ 求期望不会改变这一点，所以 $\\mathbb{E}[\\operatorname{Cov}(X_j, X_i \\mid C)] = \\delta_{ij} \\sigma_j^2$。\n\n第二项 $\\operatorname{Cov}(\\mathbb{E}[X_j \\mid C], \\mathbb{E}[X_i \\mid C])$ 是簇均值的协方差。设 $M_j = \\mathbb{E}[X_j \\mid C]$ 是表示特征 $j$ 均值的随机变量，它以概率 $p_k$ 取值 $\\mu_{kj}$（$\\mu_k$ 的第 $j$ 个分量）。这些均值的协方差会在 $X$ 的总协方差矩阵中引入非对角项。\n\n将此代回我们关于 $\\operatorname{Cov}(X_j, Y)$ 的表达式中：\n$$\n\\operatorname{Cov}(X_j, Y) = \\sum_{i=1}^d \\alpha_i \\left( \\delta_{ij} \\sigma_j^2 + \\operatorname{Cov}(M_j, M_i) \\right) = \\alpha_j \\sigma_j^2 + \\sum_{i=1}^d \\alpha_i \\operatorname{Cov}(M_j, M_i)\n$$\n这个方程是关键。它表明特征 $X_j$ 和目标 $Y$ 之间的协方差有两个分量：\n1.  一个直接分量 $\\alpha_j \\sigma_j^2$，与特征自身的系数 $\\alpha_j$ 成正比。对于非预测性特征，此项为零。\n2.  一个间接分量 $\\sum_{i=1}^d \\alpha_i \\operatorname{Cov}(M_j, M_i)$，它源于聚类结构。如果特征 $j$ 的均值与任何预测性特征的均值在不同簇之间存在协变关系，则该分量存在。\n\n如果一个非预测性特征（比如特征 $j$，其中 $\\alpha_j=0$）与 $Y$ 的相关性高于一个真正的预测性特征（特征 $i$，其中 $\\alpha_i \\neq 0$），那么特征选择规则就“被误导”了。对于一个非预测性特征 $j$，协方差简化为：\n$$\n\\operatorname{Cov}(X_j, Y) = \\sum_{i=1, \\alpha_i \\neq 0}^d \\alpha_i \\operatorname{Cov}(M_j, M_i) \\quad (\\text{since } \\alpha_j=0)\n$$\n如果非预测性特征 $j$ 的均值与某个强预测性特征的均值强相关，这个协方差就可能很大且非零。这会在 $X_j$ 和 $Y$ 之间产生一种伪相关，这种相关性并非由直接的因果联系（如线性模型 $Y = \\alpha^\\top X + \\epsilon$ 所定义的）引起，而是由共享的聚类结构所介导。如果这种伪相关强于某个弱预测性特征的相关性，选择规则就会被误导。\n\n检验这一点的算法流程如下：\n1.  对于每个测试用例，使用指定的种子初始化一个随机数生成器，以保证可复现性。\n2.  生成 $n$ 个样本。这包括两个步骤：\n    a. 首先，对于每个样本，从由概率 $\\{p_1, \\dots, p_K\\}$ 定义的分类分布中抽取一个簇分配 $k \\in \\{1, \\dots, K\\}$。\n    b. 其次，对于分配给簇 $k$ 的每个样本，从多元正态分布 $\\mathcal{N}(\\mu_k, \\Sigma)$ 中抽取其特征向量 $X$，其中 $\\Sigma$ 是一个对角矩阵，其对角线元素为给定的各特征方差 $\\sigma_j^2$。\n3.  生成目标变量 $Y$。对于每个样本的特征向量 $X^{(i)}$，计算 $Y^{(i)} = \\alpha^\\top X^{(i)} + \\epsilon^{(i)}$，其中 $\\epsilon^{(i)}$ 从 $\\mathcal{N}(0, \\sigma_\\epsilon^2)$ 中抽取。\n4.  使用包含 $n$ 个样本的完整数据集 $(X, Y)$，计算每个特征列 $X_j$ 与目标向量 $Y$ 之间的样本 Pearson 相关系数 $\\rho_j$。这通常通过一个函数来完成，该函数基于无偏样本协方差和标准差计算相关矩阵。\n5.  识别预测性特征集 $P = \\{j:\\alpha_j \\neq 0\\}$ 和非预测性特征集 $N = \\{j:\\alpha_j = 0\\}$。\n6.  计算非预测性特征中的最大绝对相关性 $\\max_{j \\in N} |\\rho_j|$，以及预测性特征中的最小绝对相关性 $\\min_{i \\in P} |\\rho_i|$。\n7.  检查是否满足“被误导”的条件 $\\max_{j \\in N} |\\rho_j|  \\min_{i \\in P} |\\rho_i|$。结果是一个布尔值。\n\n对所有测试用例重复此过程，并收集布尔结果。这些测试用例旨在探究此机制：\n-   **用例 1**：对齐的簇均值应导致非预测性特征 $X_2$ 与强预测性特征 $X_0$ 高度相关，从而与 $Y$ 产生较大的伪相关。这预计会误导选择规则。\n-   **用例 2**：不平衡的簇会减小但不会消除簇均值之间的协方差，可能仍会导致“被误导”的结果。\n-   **用例 3**：没有簇分离（$\\mu_1 = \\mu_2$）意味着对所有 $j, i$ 都有 $\\operatorname{Cov}(M_j, M_i) = 0$。间接协方差项消失，相关性将正确反映 $\\alpha_j$ 的直接影响，因此预计不会发生误导。\n-   **用例 4**：簇均值偏移的构建使其与信号向量 $\\alpha$ 正交。具体来说，$\\sum_i \\alpha_i \\operatorname{Cov}(M_j, M_i)$ 的计算结果为零。这展示了一种特定的配置，其中存在聚类但不会引发伪相关，因此预计不会发生误导。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates synthetic datasets and determines if a correlation-based\n    feature selection rule is misled by clustering structure.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, 0.1, 0.0]),\n            \"means\": np.array([[2.0, 0.0, 2.0], [-2.0, 0.0, -2.0]]),\n            \"stds\": np.array([0.5, 0.1, 0.5]),\n            \"cluster_probs\": np.array([0.5, 0.5]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 42\n        },\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, 0.1, 0.0]),\n            \"means\": np.array([[2.0, 0.0, 2.0], [-2.0, 0.0, -2.0]]),\n            \"stds\": np.array([0.5, 0.1, 0.5]),\n            \"cluster_probs\": np.array([0.9, 0.1]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 123\n        },\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, 0.1, 0.0]),\n            \"means\": np.array([[0.0, 0.0, 0.0], [0.0, 0.0, 0.0]]),\n            \"stds\": np.array([0.5, 0.1, 0.5]),\n            \"cluster_probs\": np.array([0.5, 0.5]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 7\n        },\n        {\n            \"n\": 5000, \"d\": 3, \"K\": 2,\n            \"alpha\": np.array([1.0, -1.0, 0.0]),\n            \"means\": np.array([[2.0, 2.0, 2.0], [-2.0, -2.0, -2.0]]),\n            \"stds\": np.array([0.5, 0.5, 0.5]),\n            \"cluster_probs\": np.array([0.5, 0.5]),\n            \"sigma_epsilon\": 0.2,\n            \"seed\": 2024\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Unpack parameters for the current test case\n        n, d, K = case[\"n\"], case[\"d\"], case[\"K\"]\n        alpha = case[\"alpha\"]\n        means = case[\"means\"]\n        stds = case[\"stds\"]\n        cluster_probs = case[\"cluster_probs\"]\n        sigma_epsilon = case[\"sigma_epsilon\"]\n        seed = case[\"seed\"]\n\n        # 1. Generate dataset\n        rng = np.random.default_rng(seed)\n\n        # Generate cluster assignments for n samples\n        cluster_indices = np.arange(K)\n        assignments = rng.choice(cluster_indices, size=n, p=cluster_probs)\n\n        # Generate X based on cluster assignments\n        X = np.zeros((n, d))\n        for i in range(n):\n            cluster_idx = assignments[i]\n            mean_vec = means[cluster_idx]\n            # Draw sample from multivariate normal with diagonal covariance\n            X[i, :] = rng.normal(loc=mean_vec, scale=stds)\n            \n        # Generate Y\n        epsilon = rng.normal(loc=0, scale=sigma_epsilon, size=n)\n        Y = X @ alpha + epsilon\n\n        # 2. Compute absolute sample Pearson correlations\n        correlations = []\n        for j in range(d):\n            # np.corrcoef returns a 2x2 matrix. The off-diagonal is the correlation.\n            corr_matrix = np.corrcoef(X[:, j], Y)\n            correlations.append(corr_matrix[0, 1])\n        \n        abs_correlations = np.abs(np.array(correlations))\n        \n        # 3. Determine if the case is \"misled\"\n        predictive_indices = np.where(alpha != 0)[0]\n        non_predictive_indices = np.where(alpha == 0)[0]\n\n        is_misled = False\n        # The condition requires at least one of each type of feature to exist.\n        if predictive_indices.size > 0 and non_predictive_indices.size > 0:\n            min_corr_predictive = np.min(abs_correlations[predictive_indices])\n            max_corr_non_predictive = np.max(abs_correlations[non_predictive_indices])\n            \n            if max_corr_non_predictive > min_corr_predictive:\n                is_misled = True\n        \n        results.append(is_misled)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3199415"}]}