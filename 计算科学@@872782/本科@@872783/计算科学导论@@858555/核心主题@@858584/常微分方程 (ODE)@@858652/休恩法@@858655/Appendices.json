{"hands_on_practices": [{"introduction": "在深入研究一种数值方法的理论特性或进行复杂的编程实现之前，首先必须能通过手动计算来牢固掌握其核心步骤。本练习旨在通过一个简单的初始值问题，帮助你实践休恩方法（Heun's method）的“预测-校正”流程。通过将计算结果与另一种二阶方法——中点法（midpoint method）——进行比较，你将更清晰地理解不同数值格式在单步计算中的具体差异 [@problem_id:2200959]。", "problem": "考虑由常微分方程 $y'(t) = ty$ 和初始条件 $y(1) = 2$ 给出的初值问题 (IVP)。\n\n我们希望使用大小为 $h = 0.2$ 的单步来近似计算 $y(1.2)$ 的值。将使用两种不同的二阶龙格-库塔方法：中点法和休恩法（也称为改进欧拉法或显式梯形法则）。\n\n令 $y_M$ 为使用中点法得到的 $y(1.2)$ 的数值近似值，令 $y_H$ 为使用休恩法得到的 $y(1.2)$ 的数值近似值。\n\n计算绝对差 $|y_H - y_M|$。给出此差的精确数值。", "solution": "我们有初值问题 $y'(t)=f(t,y)=ty$，其中 $y(1)=2$，步长 $h=0.2=\\frac{1}{5}$，我们寻求在 $t_{1}=1+h=1.2=\\frac{6}{5}$ 处的近似值。\n\n对于中点（显式中点）法，更新公式为\n$$\ny_{n+1}=y_{n}+h\\,f\\!\\left(t_{n}+\\frac{h}{2},\\,y_{n}+\\frac{h}{2}f(t_{n},y_{n})\\right).\n$$\n当 $t_{0}=1$, $y_{0}=2$ 时：\n- $k_{1}=f(1,2)=1\\cdot 2=2$。\n- 中点参数：$t_{0}+\\frac{h}{2}=1+\\frac{0.2}{2}=1.1=\\frac{11}{10}$ 和 $y_{0}+\\frac{h}{2}k_{1}=2+\\frac{0.2}{2}\\cdot 2=2.2=\\frac{11}{5}$。\n- $k_{2}=f\\!\\left(\\frac{11}{10},\\frac{11}{5}\\right)=\\frac{11}{10}\\cdot\\frac{11}{5}=\\frac{121}{50}$。\n因此\n$$\ny_{M}=y_{1}=2+h\\,k_{2}=2+\\frac{1}{5}\\cdot\\frac{121}{50}=2+\\frac{121}{250}=\\frac{621}{250}.\n$$\n\n对于休恩法（显式梯形法则），更新公式为\n$$\ny_{n+1}=y_{n}+\\frac{h}{2}\\left(f(t_{n},y_{n})+f\\!\\left(t_{n}+h,\\,y_{n}+h f(t_{n},y_{n})\\right)\\right).\n$$\n当 $t_{0}=1$, $y_{0}=2$ 时：\n- $k_{1}=f(1,2)=2$。\n- 终点处的预测值：$t_{0}+h=1+\\frac{1}{5}=\\frac{6}{5}$ 和 $y_{0}+h k_{1}=2+\\frac{1}{5}\\cdot 2=\\frac{12}{5}$。\n- $k_{2}=f\\!\\left(\\frac{6}{5},\\frac{12}{5}\\right)=\\frac{6}{5}\\cdot\\frac{12}{5}=\\frac{72}{25}$。\n因此\n$$\ny_{H}=y_{1}=2+\\frac{h}{2}(k_{1}+k_{2})=2+\\frac{1}{10}\\left(2+\\frac{72}{25}\\right)\n=2+\\frac{1}{10}\\cdot\\frac{122}{25}=2+\\frac{122}{250}=\\frac{622}{250}=\\frac{311}{125}.\n$$\n\n计算绝对差：\n$$\n|y_{H}-y_{M}|=\\left|\\frac{311}{125}-\\frac{621}{250}\\right|=\\left|\\frac{622}{250}-\\frac{621}{250}\\right|=\\frac{1}{250}.\n$$", "answer": "$$\\boxed{\\frac{1}{250}}$$", "id": "2200959"}, {"introduction": "理论分析表明，休恩方法的全局误差与步长 $h$ 的平方成正比，即具有二阶收敛性。本练习将引导你从手动计算过渡到编程实现，通过数值实验来验证这一重要理论。你将学习如何通过对一系列步长下的全局误差进行对数-对数图分析，从而经验性地确定方法的收敛阶，这是计算科学中一项基本且重要的验证技能 [@problem_id:3259641]。", "problem": "考虑初值问题 (IVP) $y'(t) = f(t,y(t))$，其中 $y(t_0) = y_0$，且函数 $f$ 在闭区间 $[t_0,T]$ 上对其两个参数都是连续可微的。对于一个采用均匀步长 $h$ 和 $N = (T - t_0)/h$ 个步骤的单步法，其在时间 $T$ 的全局离散误差定义为 $E(h) = \\lvert y_N - y(T) \\rvert$，其中 $y_N$ 是该方法产生的对 $y(T)$ 的数值近似。\n\nHeun 方法（也称为显式梯形方法）是一种单步法，它通过将梯形求积法则应用于积分表示 $y(t_{n+1}) = y(t_n) + \\int_{t_n}^{t_{n+1}} f(s,y(s)) \\, ds$，并使用前向欧拉预测子来近似第二个函数求值得到。在均匀步长为 $h$ 的情况下，从 $(t_n,y_n)$ 到 $(t_{n+1},y_{n+1})$ 的迭代步骤如下：\n$$\ny_{n+1} = y_n + \\frac{h}{2}\\left(f(t_n,y_n) + f\\!\\left(t_{n+1}, y_n + h\\,f(t_n,y_n)\\right)\\right).\n$$\n假设 $f$ 和精确解 $y$ 足够光滑，以至于 Heun 方法的局部截断误差是良定义的。\n\n你的任务是通过一组光滑问题的小型测试集，数值上验证 Heun 方法在最终时间 $T$ 的全局误差 $E(h)$ 与 $\\mathcal{O}(h^2)$ 同阶。为此，对下面的每个测试用例：\n- 对一系列 $N$ 值，在 $[t_0,T]$ 上使用均匀步长 $h = (T - t_0)/N$ 实现 Heun 方法。\n- 使用已知的精确解 $y(T)$ 计算在 $T$ 处的全局误差 $E(h)$。\n- 在给定的步长集合上，将观测阶 $p$ 估计为数据对 $\\big(\\log(h), \\log(E(h))\\big)$ 在最小二乘意义下的最佳拟合直线的斜率。具体来说，如果 $(x_i,y_i) = \\big(\\log(h_i),\\log(E(h_i))\\big)$，找到使 $\\sum_i (y_i - \\alpha - p\\,x_i)^2$ 最小化的直线 $y \\approx \\alpha + p\\,x$，并返回其斜率 $p$。\n\n使用以下测试集。当出现三角函数时，所有角度均解释为弧度。\n\n- 测试用例 $1$ (线性、指数衰减): $f(t,y) = -y$, $t_0 = 0$, $y_0 = 1$, $T = 1$, 精确解 $y(t) = e^{-t}$, 步数 $N \\in \\{10, 20, 40, 80\\}$。\n- 测试用例 $2$ (乘性振荡): $f(t,y) = y\\cos(t)$, $t_0 = 0$, $y_0 = 1$, $T = \\pi$, 精确解 $y(t) = \\exp(\\sin(t))$, 步数 $N \\in \\{20, 40, 80, 160\\}$。\n- 测试用例 $3$ (短区间上的非线性增长): $f(t,y) = y^2$, $t_0 = 0$, $y_0 = 1$, $T = 0.5$, 精确解 $y(t) = \\frac{1}{1 - t}$, 步数 $N \\in \\{20, 40, 80, 160\\}$。\n- 测试用例 $4$ (纯强迫项): $f(t,y) = \\sin(t)$, $t_0 = 0$, $y_0 = 0$, $T = 2$, 精确解 $y(t) = 1 - \\cos(t)$, 步数 $N \\in \\{20, 40, 80, 160\\}$。\n\n你的程序必须：\n- 按描述实现 Heun 方法。\n- 对每个测试用例，使用指定的 $N$ 值计算 $(h, E(h))$ 值列表，按描述估计斜率 $p$，并收集这四个斜率。\n- 生成单行输出，其中包含这四个斜率，格式为用方括号括起来的逗号分隔列表，例如 $[p_1,p_2,p_3,p_4]$。每个 $p_i$ 都必须是浮点数。\n\n不涉及物理单位。所有三角函数求值都必须使用弧度作为角度单位。程序必须是自包含的，且不要求任何输入。最终输出是与四个测试用例相对应的四个浮点数斜率。", "solution": "该问题要求对几个初值问题 (IVP) 数值上验证 Heun 方法的收敛阶。对于一个 $p$ 阶单步法，在固定时间 $T$ 和使用步长 $h$ 的情况下，其理论全局误差在 $h \\to 0$ 时预计表现为 $E(h) \\approx C h^p$，其中 $C$ 为某个常数。已知 Heun 方法是二阶方法，因此我们期望求得 $p \\approx 2$。\n\n为了进行数值验证，我们可以分析误差 $E(h)$ 和步长 $h$ 之间的关系。对误差表达式取自然对数可得：\n$$ \\log(E(h)) \\approx \\log(C) + p \\log(h) $$\n该方程具有直线形式 $y = \\alpha + p x$，其中 $y = \\log(E(h))$，$x = \\log(h)$，截距为 $\\alpha = \\log(C)$。这条直线的斜率即为收敛阶 $p$。问题规定，我们必须通过对一系列步长 $h_i$ 生成的数据点集 $(\\log(h_i), \\log(E(h_i)))$ 进行线性最小二乘回归来估计该斜率 $p$。\n\n每个测试用例的总体算法流程如下：\n1.  定义 IVP 参数：函数 $f(t,y)$、初始时间 $t_0$、初始值 $y_0$、最终时间 $T$ 和精确解 $y(t)$。\n2.  对于给定的步数列表 $\\{N_1, N_2, \\dots, N_m\\}$：\n    a. 对每个 $N_i$，计算步长 $h_i = (T - t_0) / N_i$。\n    b. 使用 Heun 方法，以 $N_i$ 步从 $t_0$ 求解到 $T$ 以获得数值近似解 $y_{N_i}$。Heun 方法的迭代公式为：\n    $$ y_{n+1} = y_n + \\frac{h}{2}\\left(f(t_n,y_n) + f\\!\\left(t_{n+1}, y_n + h f(t_n,y_n)\\right)\\right) $$\n    其中 $t_{n+1} = t_n + h$。从 $(t_0, y_0)$ 开始，应用此公式 $N_i$ 次。\n    c. 计算在最终时间 $T$ 的真实解 $y(T)$。\n    d. 计算全局误差 $E(h_i) = |y_{N_i} - y(T)|$。\n    e. 存储数据对 $(h_i, E(h_i))$。\n3.  生成包含 $m$ 个数据点的集合 $\\{(h_i, E(h_i))\\}$ 后，通过取对数将其转换为 $\\{(\\log(h_i), \\log(E(h_i)))\\}$。令 $x_i = \\log(h_i)$ 和 $y_i = \\log(E(h_i))$。\n4.  估计通过点 $(x_i, y_i)$ 的最佳拟合直线的斜率 $p$。对于简单线性回归模型 $y = \\alpha + px$，使误差平方和 $\\sum_{i=1}^m (y_i - (\\alpha + px_i))^2$ 最小化的斜率 $p$ 由以下公式给出：\n$$ p = \\frac{\\sum_{i=1}^m (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^m (x_i - \\bar{x})^2} $$\n其中 $\\bar{x} = \\frac{1}{m}\\sum_{i=1}^m x_i$ 和 $\\bar{y} = \\frac{1}{m}\\sum_{i=1}^m y_i$ 分别是 $x$ 和 $y$ 数据的样本均值。该公式等价于计算 $x$ 和 $y$ 的协方差除以 $x$ 的方差。\n\n对所提供的四个测试用例中的每一个都实施此过程。该实现使用 Python 和 `numpy` 库。`numpy` 提供了必要的数学函数（例如，`exp`、`sin`、`cos`、`log`、`pi`）的实现，并为最小二乘计算提供了便利的数组操作。创建一个函数来实现 Heun 方法，另一个函数用于计算最小二乘斜率。程序的主体部分遍历测试用例，计算指定步数的误差，为每个用例估计收敛阶 $p$，并收集这些值作为最终输出。预计 $p$ 的估计值将接近 $2$，从而证实 Heun 方法对这些光滑问题的二阶精度。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing Heun's method, calculating global errors,\n    and estimating the order of convergence for four different IVPs.\n    \"\"\"\n\n    def heun_solver(f, t0, y0, T, N):\n        \"\"\"\n        Solves an IVP y'(t) = f(t, y) using Heun's method.\n\n        Args:\n            f: The function f(t, y).\n            t0: Initial time.\n            y0: Initial value.\n            T: Final time.\n            N: Number of steps.\n\n        Returns:\n            The numerical approximation of y(T).\n        \"\"\"\n        h = (T - t0) / N\n        t = float(t0)\n        y = float(y0)\n        for _ in range(N):\n            k1 = f(t, y)\n            k2 = f(t + h, y + h * k1)\n            y = y + (h / 2.0) * (k1 + k2)\n            t = t + h\n        return y\n\n    def estimate_order(h_values, E_values):\n        \"\"\"\n        Estimates the order of convergence p from step sizes h and errors E.\n        This is done by finding the slope of the best-fit line for log(E) vs. log(h).\n\n        Args:\n            h_values: A numpy array of step sizes.\n            E_values: A numpy array of corresponding global errors.\n\n        Returns:\n            The estimated order of convergence p.\n        \"\"\"\n        # Take the natural logarithm of step sizes and errors\n        log_h = np.log(h_values)\n        log_E = np.log(E_values)\n        \n        # We want to find the slope p of the line y = alpha + p*x that best fits\n        # the data (x, y) = (log_h, log_E) in the least-squares sense.\n        # The formula for the slope is p = Cov(x, y) / Var(x).\n        x = log_h\n        y = log_E\n        x_mean = np.mean(x)\n        y_mean = np.mean(y)\n        \n        numerator = np.sum((x - x_mean) * (y - y_mean))\n        denominator = np.sum((x - x_mean)**2)\n        \n        p = numerator / denominator\n        return p\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        {\n            \"f\": lambda t, y: -y,\n            \"y_exact\": lambda t: np.exp(-t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 1.0,\n            \"N_values\": [10, 20, 40, 80]\n        },\n        {\n            \"f\": lambda t, y: y * np.cos(t),\n            \"y_exact\": lambda t: np.exp(np.sin(t)),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": np.pi,\n            \"N_values\": [20, 40, 80, 160]\n        },\n        {\n            \"f\": lambda t, y: y**2,\n            \"y_exact\": lambda t: 1.0 / (1.0 - t),\n            \"t0\": 0.0,\n            \"y0\": 1.0,\n            \"T\": 0.5,\n            \"N_values\": [20, 40, 80, 160]\n        },\n        {\n            \"f\": lambda t, y: np.sin(t),\n            \"y_exact\": lambda t: 1.0 - np.cos(t),\n            \"t0\": 0.0,\n            \"y0\": 0.0,\n            \"T\": 2.0,\n            \"N_values\": [20, 40, 80, 160]\n        }\n    ]\n\n    estimated_orders = []\n    \n    # Process each test case\n    for case in test_cases:\n        h_values = []\n        errors = []\n        \n        # Calculate the exact solution at the final time T once\n        y_exact_at_T = case[\"y_exact\"](case[\"T\"])\n        \n        # Run the simulation for each specified number of steps N\n        for N in case[\"N_values\"]:\n            t0, y0, T = case[\"t0\"], case[\"y0\"], case[\"T\"]\n            \n            # Calculate step size\n            h = (T - t0) / N\n            \n            # Get numerical solution using Heun's method\n            y_numerical_at_T = heun_solver(case[\"f\"], t0, y0, T, N)\n            \n            # Calculate global error\n            error = np.abs(y_numerical_at_T - y_exact_at_T)\n            \n            h_values.append(h)\n            errors.append(error)\n        \n        # Estimate the order of convergence p\n        p = estimate_order(np.array(h_values), np.array(errors))\n        estimated_orders.append(p)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, estimated_orders))}]\")\n\nsolve()\n```", "id": "3259641"}, {"introduction": "一个数值方法的阶数（accuracy order）固然重要，但在实际应用中，其计算效率（efficiency）往往是更关键的考量因素。本练习将休恩方法与更经典的四阶龙格-库塔方法（RK4）进行对比，让你在实践中探索精度与计算成本之间的权衡。通过寻找达到相同全局误差容差所需的最少函数求值次数，你将深刻体会到为何高阶方法在追求高精度解时通常更具优势 [@problem_id:2428159]。", "problem": "单变量初值问题 (IVP) 由一个形式为 $y^{\\prime}(t) = f(t,y(t))$ 的常微分方程 (ODE) 和一个初始条件 $y(t_{0}) = y_{0}$ 指定。其精确解 $y(t)$ 满足积分恒等式 $y(t_{n+1}) = y(t_{n}) + \\int_{t_{n}}^{t_{n+1}} f(\\tau,y(\\tau)) \\, d\\tau$。数值单步法在离散网格 $t_{n} = t_{0} + n h$ 上以一个恒定的步长 $h = (T - t_{0})/N$ 来近似 $y(t)$，并生成近似值 $y_{n} \\approx y(t_{n})$。在最终时刻 $T$ 的全局误差定义为 $e_{\\mathrm{global}} = |y_{N} - y(T)|$。\n\n你的任务是实现并比较两种用于求解 IVP 的时间积分方法的效率：\n- 显式预测-校正格式，通常称为休恩方法 (Heun’s method)。\n- 经典的四阶龙格-库塔法 (Fourth-Order Runge-Kutta (RK4))。\n\n效率必须通过在最终时刻 $T$ 达到目标全局误差容差 $\\varepsilon$ 所需的右端项 $f(t,y)$ 的总求值次数来衡量，其中使用均匀的时间步长。对于每种方法，确定使 $e_{\\mathrm{global}} \\le \\varepsilon$ 成立的最小步数 $N$。然后计算总的函数求值次数，对于休恩方法是 $2N$，对于 RK4 方法是 $4N$。\n\n你必须：\n1. 为标量 IVP 实现这两种使用恒定步长的方法。\n2. 对于每种方法，通过以下方式找到满足 $e_{\\mathrm{global}} \\le \\varepsilon$ 的最小 $N$：\n   - 从 $N = 1$（此时 $h = (T - t_{0})/N$）开始，重复将 $N$ 加倍，直到不等式成立。\n   - 然后在最后一个不满足条件的 $N$ 和第一个满足条件的 $N$ 之间执行整数二分搜索，以找到满足 $e_{\\mathrm{global}} \\le \\varepsilon$ 的最小 $N$。\n   - 如果在安全上限 $N_{\\max}$ 内没有 $N$ 满足容差，你必须报告失败。使用 $N_{\\max} = 2^{20}$。\n3. 对每个测试用例，计算效率比 $R = \\dfrac{\\text{休恩方法的函数求值次数}}{\\text{RK4 方法的函数求值次数}}$，并四舍五入到 $3$ 位小数。\n\n使用以下带有精确解的测试套件。在所有情况下，使用 $t_{0} = 0$，状态变量是无量纲的。对于三角函数，使用弧度。\n\n- 测试 A (指数衰减): $f(t,y) = -2 y$, $y(0) = 1$, 精确解 $y(t) = e^{-2 t}$, $T = 1$, $\\varepsilon = 10^{-6}$。\n- 测试 B (指数增长): $f(t,y) = y$, $y(0) = 1$, 精确解 $y(t) = e^{t}$, $T = 1$, $\\varepsilon = 10^{-6}$。\n- 测试 C (受迫线性): $f(t,y) = \\cos(t) - y$, $y(0) = 1$, 精确解 $y(t) = \\dfrac{1}{2}\\left(\\sin t + \\cos t + e^{-t}\\right)$, $T = 10$, $\\varepsilon = 10^{-5}$。\n- 测试 D (逻辑斯谛): $f(t,y) = y\\,(1 - y)$, $y(0) = 0.1$, 精确解 $y(t) = \\dfrac{1}{1 + 9 e^{-t}}$, $T = 5$, $\\varepsilon = 10^{-6}$。\n\n最终输出规格：\n- 对于每个测试用例，将效率比 $R$ 作为浮点数输出，四舍五入到 $3$ 位小数。\n- 你的程序应生成单行输出，其中包含一个逗号分隔的列表，并用方括号括起来（例如 $[r_{A},r_{B},r_{C},r_{D}]$）。\n\n没有物理单位，因此不需要单位转换。如果出现角度，则以弧度为单位。", "solution": "该问题要求对求解一阶常微分方程 (ODE) 的两种数值格式的计算效率进行量化比较：休恩方法和经典的四阶龙格-库塔 (RK4) 方法。效率定义为在最终时刻 $T$ 达到预设全局误差容差 $\\varepsilon$ 所需的右端项函数 $f(t,y)$ 的总求值次数。这是计算科学中的一个标准问题，旨在说明方法的复杂性（以每步的函数求值次数衡量）与其精度（以其收敛阶表征）之间的权衡。\n\n我们考虑在时间区间 $[t_0, T]$ 上由 $y'(t) = f(t,y(t))$ 和初始条件 $y(t_0)=y_0$ 给出的初值问题 (IVP)。数值解在一个均匀网格 $t_n = t_0 + n h$（其中 $n \\in \\{0, 1, \\dots, N\\}$）上计算，步长为常数 $h = (T - t_0)/N$。在最终时刻 $T$ 的全局误差定义为 $e_{\\mathrm{global}} = |y_N - y(T)|$，其中 $y_N$ 是数值近似解，$y(T)$ 是精确解。\n\n休恩方法是一种二阶预测-校正格式。对于从 $t_n$ 到 $t_{n+1}$ 的每一步，它包含两个阶段：\n1.  **预测：** 一个显式欧拉步为下一个时间点的解提供一个一阶精确的猜测值：$\\tilde{y}_{n+1} = y_n + h f(t_n, y_n)$。\n2.  **校正：** 这个初步结果用于评估区间末端的斜率 $f(t_{n+1}, \\tilde{y}_{n+1})$。最终的更新是通过使用步长起点和预估终点的斜率平均值来计算的，这在几何上等同于应用梯形法则进行积分：$y_{n+1} = y_n + \\frac{h}{2} [f(t_n, y_n) + f(t_{n+1}, \\tilde{y}_{n+1})]$。\n该方法每时间步精确执行 $2$ 次函数求值。其全局误差为 $2$ 阶，即 $e_{\\mathrm{global}} = O(h^2)$。\n\n经典的四阶龙格-库塔 (RK4) 方法是一种更复杂的单步法，它通过在每步内使用四个斜率评估的加权平均来获得更高的精度。从 $y_n$ 到 $y_{n+1}$ 的更新规则由下式给出：\n$$y_{n+1} = y_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$\n其中中间斜率的计算如下：\n$$k_1 = f(t_n, y_n)$$\n$$k_2 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_1\\right)$$\n$$k_3 = f\\left(t_n + \\frac{h}{2}, y_n + \\frac{h}{2}k_2\\right)$$\n$$k_4 = f(t_n + h, y_n + h k_3)$$\n该方法每时间步需要精确执行 $4$ 次函数求值。其全局误差为 $4$ 阶，即 $e_{\\mathrm{global}} = O(h^4)$。\n\n为了确定每种方法的效率，我们必须找到满足全局误差条件 $|y_N - y(T)| \\le \\varepsilon$ 的最小步数 $N$。问题为此指定了一个两阶段搜索算法。\n1.  **阶段 1：指数搜索。** 我们从 $N=1$ 开始，并重复将 $N$ 加倍（即 $N=1, 2, 4, 8, \\dots$），直到找到一个值（称之为 $N_{\\mathrm{pass}}$），使得误差容差得到满足。前一次迭代的 $N$ 值（$N_{\\mathrm{fail}}$）也被记录下来。这个过程有效地确定了一个区间 $[N_{\\mathrm{fail}}, N_{\\mathrm{pass}}]$，该区间保证包含所需的最小 $N$。如果对于任何不大于最大值 $N_{\\max} = 2^{20}$ 的 $N$ 都不能满足容差，则认为搜索失败。\n2.  **阶段 2：二分搜索。** 随后，在最后一个失败和第一个成功的步数所构成的区间内执行标准的整数二分搜索，以精确定位使全局误差不大于 $\\varepsilon$ 的最小整数 $N_{\\min}$。\n\n对于一个 $p$ 阶方法，其全局误差的理论行为是 $e_{\\mathrm{global}} \\approx C h^p = C \\left(\\frac{T-t_0}{N}\\right)^p$，其中 $C$ 是一个取决于常微分方程、其导数以及具体方法的常数。为了满足容差 $\\varepsilon$，我们需要 $C \\left(\\frac{T-t_0}{N}\\right)^p \\le \\varepsilon$，这意味着步数必须按 $N \\ge (T-t_0) \\left(\\frac{C}{\\varepsilon}\\right)^{1/p}$ 的规律缩放。\n对于休恩方法（$p=2$），函数求值的总次数是 $E_{\\mathrm{Heun}} = 2 N_{\\mathrm{Heun}} \\propto \\varepsilon^{-1/2}$。\n对于 RK4 方法（$p=4$），求值的总次数是 $E_{\\mathrm{RK4}} = 4 N_{\\mathrm{RK4}} \\propto \\varepsilon^{-1/4}$。\n因此，效率比为 $R = \\frac{E_{\\mathrm{Heun}}}{E_{\\mathrm{RK4}}} \\propto \\frac{\\varepsilon^{-1/2}}{\\varepsilon^{-1/4}} = \\varepsilon^{-1/4}$。这种关系表明，对于小的 $\\varepsilon$，比率 $R$ 预计将显著大于 $1$。这表明，尽管高阶 RK4 方法每步的计算成本更高，但在达到高精度方面具有更高的效率。$R$ 的确切值取决于包含在 $C$ 中的特定于问题的常数，这些常数通过对每个测试用例执行指定的算法以数值方式确定。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, implementing and comparing Heun's and RK4 methods.\n    \"\"\"\n\n    def heun_solver(f, y0, t0, T, N):\n        \"\"\"\n        Solves a scalar IVP y'=f(t,y) using Heun's method with a constant step size.\n        \"\"\"\n        h = (T - t0) / N\n        t = t0\n        y = y0\n        for _ in range(N):\n            # Predictor step\n            f1 = f(t, y)\n            y_tilde = y + h * f1\n            # Corrector step\n            t_next = t + h\n            f2 = f(t_next, y_tilde)\n            y = y + (h / 2.0) * (f1 + f2)\n            t = t_next\n        return y\n\n    def rk4_solver(f, y0, t0, T, N):\n        \"\"\"\n        Solves a scalar IVP y'=f(t,y) using the classical RK4 method with a constant step size.\n        \"\"\"\n        h = (T - t0) / N\n        t = t0\n        y = y0\n        for _ in range(N):\n            k1 = f(t, y)\n            k2 = f(t + h / 2.0, y + h * k1 / 2.0)\n            k3 = f(t + h / 2.0, y + h * k2 / 2.0)\n            k4 = f(t + h, y + h * k3)\n            y = y + (h / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n            t += h\n        return y\n\n    def find_min_N(solver, f, y0, t0, T, y_exact_T, tol, N_max):\n        \"\"\"\n        Finds the minimum number of steps N for a given solver to achieve the desired tolerance.\n        \"\"\"\n        \n        # Helper function to compute error for a given N\n        def get_error(num_steps):\n            y_approx = solver(f, y0, t0, T, num_steps)\n            return np.abs(y_approx - y_exact_T)\n\n        # First, check if N=1 is sufficient\n        if get_error(1) = tol:\n            return 1\n            \n        # Exponential search to find the bracketing interval for N\n        n_fail = 1\n        n_pass = 2\n        while n_pass = N_max:\n            if get_error(n_pass) = tol:\n                break\n            n_fail = n_pass\n            n_pass *= 2\n        else:\n            # This 'else' belongs to the 'while' loop, executed if the loop finishes without a break.\n            return -1 # Indicates failure to meet tolerance within N_max\n\n        # Binary search to find the minimal N\n        low = n_fail + 1\n        high = n_pass\n        min_N = n_pass\n\n        while low = high:\n            mid = low + (high - low) // 2\n            if mid == 0:  # Safety check, should not be reached with low starting > 0\n                low = 1\n                continue\n                \n            if get_error(mid) = tol:\n                min_N = mid\n                high = mid - 1\n            else:\n                low = mid + 1\n        \n        return min_N\n\n    # Test cases as defined in the problem statement.\n    test_cases = [\n        # Test A (exponential decay)\n        {\n            \"f\": lambda t, y: -2.0 * y, \"y0\": 1.0, \"t0\": 0.0, \"T\": 1.0,\n            \"y_exact_func\": lambda t: np.exp(-2.0 * t), \"tol\": 1e-6, \"name\": \"A\"\n        },\n        # Test B (exponential growth)\n        {\n            \"f\": lambda t, y: y, \"y0\": 1.0, \"t0\": 0.0, \"T\": 1.0,\n            \"y_exact_func\": lambda t: np.exp(t), \"tol\": 1e-6, \"name\": \"B\"\n        },\n        # Test C (forced linear)\n        {\n            \"f\": lambda t, y: np.cos(t) - y, \"y0\": 1.0, \"t0\": 0.0, \"T\": 10.0,\n            \"y_exact_func\": lambda t: 0.5 * (np.sin(t) + np.cos(t) + np.exp(-t)), \"tol\": 1e-5, \"name\": \"C\"\n        },\n        # Test D (logistic)\n        {\n            \"f\": lambda t, y: y * (1.0 - y), \"y0\": 0.1, \"t0\": 0.0, \"T\": 5.0,\n            \"y_exact_func\": lambda t: 1.0 / (1.0 + 9.0 * np.exp(-t)), \"tol\": 1e-6, \"name\": \"D\"\n        }\n    ]\n\n    N_max = 2**20\n    results = []\n\n    for case in test_cases:\n        f = case[\"f\"]\n        y0 = case[\"y0\"]\n        t0 = case[\"t0\"]\n        T = case[\"T\"]\n        y_exact_func = case[\"y_exact_func\"]\n        tol = case[\"tol\"]\n        \n        y_exact_T = y_exact_func(T)\n\n        # Find minimal N for Heun's method\n        N_heun = find_min_N(heun_solver, f, y0, t0, T, y_exact_T, tol, N_max)\n        if N_heun == -1:\n            raise RuntimeError(f\"Heun's method failed to converge for case {case['name']}\")\n        evals_heun = 2 * N_heun\n\n        # Find minimal N for RK4 method\n        N_rk4 = find_min_N(rk4_solver, f, y0, t0, T, y_exact_T, tol, N_max)\n        if N_rk4 == -1:\n            raise RuntimeError(f\"RK4 method failed to converge for case {case['name']}\")\n        evals_rk4 = 4 * N_rk4\n\n        # Calculate and round the efficiency ratio\n        ratio = evals_heun / evals_rk4\n        results.append(round(ratio, 3))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2428159"}]}