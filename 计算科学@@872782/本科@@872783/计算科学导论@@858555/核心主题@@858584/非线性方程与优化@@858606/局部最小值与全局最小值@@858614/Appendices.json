{"hands_on_practices": [{"introduction": "我们将从探索最基本的优化算法——梯度下降法开始。本练习将通过一个具体例子，向您展示为何采用固定步长的“一刀切”方法在复杂函数表面上常常会失效[@problem_id:3156569]。您将通过实践了解到，一个能够智能调整步长的自适应策略，可以如何更有效地穿越这些充满挑战的地形，从而找到更优的最小值。", "problem": "考虑函数 $f:\\mathbb{R}^2\\rightarrow\\mathbb{R}$，其定义为 $f(x,y)=x^2+100\\,y^2+\\sin(10\\,x)\\,\\sin(10\\,y)$，其中所有角度均以弧度为单位。该函数结合了一个强各向异性的二次碗型函数和高频振荡项，这会产生许多局部最小值。在计算科学导论中，区分局部最小值与全局最小值，并设计能够穿越狭窄山谷而不会被困住的步长方案至关重要。\n\n从核心定义开始：\n- 如果存在点 $(x^\\star,y^\\star)$ 的一个邻域 $\\mathcal{N}$，使得对于所有 $(x,y)\\in\\mathcal{N}$ 都有 $f(x^\\star,y^\\star)\\le f(x,y)$，则该点是一个局部最小值。\n- 全局最小值满足对于所有 $(x,y)\\in\\mathbb{R}^2$ 都有 $f(x^\\star,y^\\star)\\le f(x,y)$。\n- 基于梯度的方法通过沿最陡下降方向 $-\\nabla f(x,y)$ 移动来更新 $(x,y)$，并采用选定的步长。\n\n您的任务是在有界域 $[-1,1]\\times[-1,1]$ 上对函数 $f$ 实现两种优化方案，每次更新后投影回该域，以避免在合理的搜索区域之外出现无界增长：\n1. 固定步长梯度下降法：$(x_{k+1},y_{k+1})=(x_k,y_k)-\\alpha\\,\\nabla f(x_k,y_k)$，其中 $\\alpha$ 固定，然后将 $(x_{k+1},y_{k+1})$ 投影到 $[-1,1]\\times[-1,1]$ 上。\n2. 使用带有充分下降 (Armijo) 条件的回溯线搜索的自适应步长梯度下降法：从初始步长 $\\alpha_0$ 开始，重复乘以因子 $\\rho$ 进行缩减，直到满足 $f((x,y)-\\alpha\\,\\nabla f(x,y))\\le f(x,y)-c\\,\\alpha\\,\\|\\nabla f(x,y)\\|_2^2$，然后执行该步并投影到 $[-1,1]\\times[-1,1]$ 上。\n\n您必须解析地计算梯度 $\\nabla f(x,y)$，并严格按照这些定义实现算法。$\\sin(\\cdot)$ 和 $\\cos(\\cdot)$ 函数中的角度必须按弧度处理。\n\n实现这两种方法，并在一组旨在揭示狭窄山谷效应以及局部与全局最小值之间区别的初始条件和步长参数上比较它们的行为。使用以下测试套件，其中指定了方法类型、步长参数、初始点 $(x_0,y_0)$ 和最大迭代次数 $K$：\n- 测试 1：固定步长下降法，$\\alpha=0.02$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 2：固定步长下降法，$\\alpha=0.05$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 3：固定步长下降法，$\\alpha=0.0005$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 4：自适应步长下降法，$\\alpha_0=0.1$，$\\rho=0.5$，$c=10^{-4}$，$(x_0,y_0)=(0.6,0.15)$，$K=1000$。\n- 测试 5：自适应步长下降法，$\\alpha_0=0.1$，$\\rho=0.5$，$c=10^{-4}$，$(x_0,y_0)=(0.0,0.9)$，$K=1000$。\n\n对于每个测试，运行算法直到达到最大迭代次数 $K$ 或梯度范数 $\\|\\nabla f(x,y)\\|_2$ 低于 $10^{-8}$。运行结束后，报告最终的目标函数值 $f(x_K,y_K)$，四舍五入到六位小数。不涉及物理单位；角度以弧度为单位。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[r_1,r_2,r_3,r_4,r_5]$），其中每个 $r_i$ 是按上述顺序列出的第 $i$ 个测试的四舍五入到六位小数的最终目标函数值。", "solution": "用户提供了一个有效的、适定的计算科学问题。任务是在给定函数上实现并比较两种基于梯度下降的优化算法，并受限于域约束。\n\n### 问题描述\n\n目标是找到函数 $f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}$ 的局部最小值，其定义如下：\n$$f(x,y) = x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)$$\n优化在有界域 $\\mathcal{D} = [-1, 1] \\times [-1, 1]$ 上进行。函数 $f(x,y)$ 由两部分组成：一个二次碗型项 $x^2 + 100y^2$ 和一个振荡项 $\\sin(10x)\\sin(10y)$。二次项产生了一个沿 $x$ 轴方向的狭长山谷，该项的 Hessian 矩阵 $\\begin{pmatrix} 2  0 \\\\ 0  200 \\end{pmatrix}$ 证明了这一点，其条件数为 $100$，这是一个很大的值。振荡项在这个基础结构上叠加了许多局部最小值，这使得简单的优化算法难以找到全局最小值。\n\n### 梯度计算\n\n基于梯度的方法需要目标函数 $\\nabla f$ 的解析梯度。关于 $x$ 和 $y$ 的偏导数计算如下：\n$$ \\frac{\\partial f}{\\partial x} = \\frac{\\partial}{\\partial x} (x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)) = 2x + 10\\cos(10\\,x)\\sin(10\\,y) $$\n$$ \\frac{\\partial f}{\\partial y} = \\frac{\\partial}{\\partial y} (x^2 + 100\\,y^2 + \\sin(10\\,x)\\,\\sin(10\\,y)) = 200y + 10\\sin(10\\,x)\\cos(10\\,y) $$\n因此，梯度向量为：\n$$ \\nabla f(x,y) = \\begin{pmatrix} 2x + 10\\cos(10\\,x)\\sin(10\\,y) \\\\ 200y + 10\\sin(10\\,x)\\cos(10\\,y) \\end{pmatrix} $$\n所有三角函数均对以弧度为单位的角度进行运算。\n\n### 优化算法\n\n核心算法是投影梯度下降法。在每次迭代 $k$ 中，通过从当前点 $(x_k, y_k)$ 沿负梯度方向（最陡下降方向）移动来找到一个候选点。然后将这个新点投影回可行域 $\\mathcal{D}$。迭代更新公式为：\n$$ (\\tilde{x}_{k+1}, \\tilde{y}_{k+1}) = (x_k, y_k) - \\alpha_k \\nabla f(x_k, y_k) $$\n$$ (x_{k+1}, y_{k+1}) = \\text{Proj}_{\\mathcal{D}}(\\tilde{x}_{k+1}, \\tilde{y}_{k+1}) $$\n其中 $\\alpha_k$ 是第 $k$ 次迭代的步长，$\\text{Proj}_{\\mathcal{D}}$ 是到盒子 $[-1, 1] \\times [-1, 1]$ 上的投影算子，定义为 $(\\text{clip}(x, -1, 1), \\text{clip}(y, -1, 1))$。\n\n实现了两种选择步长 $\\alpha_k$ 的策略：\n\n1.  **固定步长梯度下降法**：所有迭代都使用一个固定的步长 $\\alpha_k = \\alpha$。$\\alpha$ 的选择至关重要：如果太大，算法可能会变得不稳定并发散；如果太小，收敛速度会非常慢。此方法对函数的局部曲率很敏感，而在这个问题中局部曲率变化很大。\n\n2.  **带回溯线搜索的自适应步长梯度下降法**：步长 $\\alpha_k$ 在每次迭代中动态确定。从一个初始猜测值 $\\alpha_0$ 开始，步长重复乘以一个因子 $\\rho \\in (0,1)$ 进行缩减，直到满足 Armijo (或充分下降) 条件：\n    $$ f((x_k,y_k) - \\alpha \\nabla f(x_k,y_k)) \\le f(x_k,y_k) - c \\alpha \\|\\nabla f(x_k,y_k)\\|_2^2 $$\n    对于一个小常数 $c$ (例如 $10^{-4}$)。这确保了每一步在减小目标函数值方面都取得足够的进展，使得算法比固定步长法更稳健，并且通常更高效。根据规定，Armijo 条件在应用投影之前的无约束步骤上进行检查。\n\n### 实现与执行\n\n算法使用 Python 的 `numpy` 库实现。对于五个指定的测试用例中的每一个，相应的算法都使用给定的起始点 $(x_0, y_0)$ 和参数进行初始化。迭代过程持续进行，直到达到最大迭代次数 $K$，或者梯度的 L2-范数 $\\|\\nabla f(x,y)\\|_2$ 低于 $10^{-8}$ 的容差。然后计算并报告在终止点的最终目标函数值 $f(x,y)$。对每个测试用例重复此过程，以比较不同参数设置和方法的效果。\n\n预期的行为是，步长选择不当的固定步长方法（测试 1 和 2）将表现不佳，要么振荡，要么在域边界上陷入较高的目标函数值。非常小的步长（测试 3）将导致进展缓慢。自适应步长方法（测试 4 和 5）预计将是稳健的，能自动找到合适的步长并收敛到一个值较低的局部最小值，从而展示其在复杂优化地形中的优越性。测试 4 和 5 中不同的起始点可能会导致收敛到不同的局部最小值，这突显了局部优化的路径依赖性。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares constant-step and adaptive-step projected gradient\n    descent algorithms for a given function and test suite.\n    \"\"\"\n\n    # Define the objective function f(p), its gradient grad_f(p), and the\n    # projection operator. p is a numpy array [x, y].\n    def f(p):\n        x, y = p\n        return x**2 + 100*y**2 + np.sin(10*x) * np.sin(10*y)\n\n    def grad_f(p):\n        x, y = p\n        df_dx = 2*x + 10*np.cos(10*x)*np.sin(10*y)\n        df_dy = 200*y + 10*np.sin(10*x)*np.cos(10*y)\n        return np.array([df_dx, df_dy])\n\n    def project(p):\n        return np.clip(p, -1, 1)\n\n    # Algorithm 1: Constant-step projected gradient descent\n    def constant_step_descent(alpha, p0, K, tol):\n        p = np.array(p0, dtype=float)\n        for _ in range(K):\n            grad = grad_f(p)\n            if np.linalg.norm(grad)  tol:\n                break\n            p_next_unprojected = p - alpha * grad\n            p = project(p_next_unprojected)\n        return f(p)\n\n    # Algorithm 2: Adaptive-step projected gradient descent with backtracking\n    def adaptive_step_descent(alpha0, rho, c, p0, K, tol):\n        p = np.array(p0, dtype=float)\n        for _ in range(K):\n            grad = grad_f(p)\n            grad_norm = np.linalg.norm(grad)\n            if grad_norm  tol:\n                break\n            \n            # Backtracking line search to satisfy the Armijo condition\n            alpha = alpha0\n            current_f_val = f(p)\n            grad_norm_sq = grad_norm**2\n            \n            while True:\n                p_candidate = p - alpha * grad\n                \n                # Check Armijo condition on the unprojected candidate point\n                if f(p_candidate) = current_f_val - c * alpha * grad_norm_sq:\n                    break\n                \n                alpha *= rho\n                \n                # Failsafe for pathologically small step sizes\n                if alpha  1e-16:\n                    break\n\n            p_next_unprojected = p - alpha * grad\n            p = project(p_next_unprojected)\n        return f(p)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: Constant-step, alpha=0.02\n        {'method': 'constant', 'params': {'alpha': 0.02, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 2: Constant-step, alpha=0.05\n        {'method': 'constant', 'params': {'alpha': 0.05, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 3: Constant-step, alpha=0.0005\n        {'method': 'constant', 'params': {'alpha': 0.0005, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 4: Adaptive-step\n        {'method': 'adaptive', 'params': {'alpha0': 0.1, 'rho': 0.5, 'c': 1e-4, 'p0': (0.6, 0.15), 'K': 1000}},\n        # Test 5: Adaptive-step, different initial point\n        {'method': 'adaptive', 'params': {'alpha0': 0.1, 'rho': 0.5, 'c': 1e-4, 'p0': (0.0, 0.9), 'K': 1000}},\n    ]\n\n    results = []\n    TOLERANCE = 1e-8\n    \n    for case in test_cases:\n        params = case['params']\n        if case['method'] == 'constant':\n            res = constant_step_descent(\n                alpha=params['alpha'],\n                p0=params['p0'],\n                K=params['K'],\n                tol=TOLERANCE\n            )\n        else: # 'adaptive'\n            res = adaptive_step_descent(\n                alpha0=params['alpha0'],\n                rho=params['rho'],\n                c=params['c'],\n                p0=params['p0'],\n                K=params['K'],\n                tol=TOLERANCE\n            )\n        results.append(res)\n    \n    # Final print statement in the exact required format.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3156569"}, {"introduction": "基于梯度的方法非常擅长找到其所在“山谷”的底部，但如果这是一个错误的“山谷”该怎么办？本练习将介绍盆地跳跃（basin hopping），这是一种强大的元启发式算法，它将局部优化与策略性的随机跳跃相结合[@problem_id:3156501]。通过亲手实现这一技术，您将看到它如何系统地摆脱局部陷阱，从而显著提高定位真正全局最小值的机会。", "problem": "您需要编写一个完整且可运行的程序，该程序实现一种盆地跳跃元启发式算法，用于研究二维景观上的局部最小值与全局最小值。目标函数为 $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$，其中正弦函数的所有角度参数必须以弧度处理。计算任务是研究随机跳跃半径如何影响发现全局最小值的能力，通过从随机点开始并与随机化跳跃交错进行的重复局部最小化来实现。\n\n此问题的基础是以下经过充分检验的定义和事实：\n- 如果存在点 $(x^\\star,y^\\star)$ 的一个邻域 $U$，使得对于所有 $(x,y) \\in U$，都有 $f(x^\\star,y^\\star) \\le f(x,y)$，则该点是可微函数 $f$ 的一个局部最小值。如果对于定义域中所有的 $(x,y)$，都有 $f(x^\\star,y^\\star) \\le f(x,y)$，则该点是全局最小值。\n- 对于可微函数，局部最小化点必须满足一阶平稳性条件 $\\nabla f(x^\\star,y^\\star) = \\mathbf{0}$，并且二阶导数（Hessian 矩阵）提供影响局部最优性的局部曲率信息。\n- 盆地跳跃是一种元启发式算法，它将确定性的局部最小化与搜索空间中的随机跳跃交替进行，以逃离吸引盆，从而提高找到全局最小值的机会。\n\n您的程序必须在纯粹的数学和逻辑术语上实现以下内容：\n1. 定义函数 $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$，其有界定义域为 $[-6,6] \\times [-6,6]$。\n2. 使用基于梯度的例程实现 $f$ 的局部最小化，该例程需遵守给定的边界。正弦函数的导数必须以弧度为单位计算角度。\n3. 实现带贪婪接受准则的盆地跳跃算法：从一个随机初始点 $(x_0,y_0)$ 开始，执行局部最小化得到局部最小值 $(x_\\mathrm{loc},y_\\mathrm{loc})$，然后以 $(x_\\mathrm{loc},y_\\mathrm{loc})$ 为中心，在半径为 $r$ 的圆盘上均匀分布地提出一个随机跳跃，将跳跃后的点裁剪到定义域 $[-6,6] \\times [-6,6]$ 内，然后从该裁剪点开始进行局部最小化。仅当新的局部最小值严格优于当前的局部最小值时才接受它；否则，保留当前的。将此过程重复固定次数的步骤。\n4. 使用一维多起点局部最小化来计算 $g(t) = \\sin(t) + 0.2 t^2$ 在 $[-6,6]$ 上的全局最小值的精确近似，并利用可分性推断 $f$ 的全局最小值。具体来说，计算使 $g$ 最小化的 $t^\\star \\in [-6,6]$，那么 $f$ 的全局最小值出现在 $(t^\\star,t^\\star)$ 处，其值为 $f^\\star = 2 g(t^\\star)$。\n5. 使用具有确定性种子的伪随机数生成器（PRNG）以确保可复现性。每个测试用例必须使用从其参数确定性派生的种子。程序中使用的所有随机角度和均匀变量必须分别为弧度和无量纲。\n6. 对于每个测试用例，计算有多少次独立的盆地跳跃运行最终到达全局最小值（在规定的容差范围内），并将此计数报告为整数。\n\n角度单位规范：\n- 所有三角函数求值和角度采样都必须以弧度为单位。\n\n测试套件包含五个测试用例，它们改变跳跃半径 $r$ 以测试算法的不同方面：\n- 情况 $1$（边界情况）：$r = 0.0$（等同于没有跳跃的纯局部最小化）。\n- 情况 $2$（小跳跃）：$r = 0.5$。\n- 情况 $3$（中等跳跃）：$r = 1.5$。\n- 情况 $4$（大跳跃）：$r = 3.0$。\n- 情况 $5$（非常大的跳跃）：$r = 5.0$。\n\n所有测试用例的通用参数：\n- 定义域边界：$[-6,6] \\times [-6,6]$。\n- 每次运行的盆地跳跃步数：$N = 60$。\n- 每个测试用例的独立运行次数：$R = 30$。\n- 全局最优性容差：如果最终目标值与计算出的全局最小值 $f^\\star$ 的差在 $\\varepsilon = 10^{-6}$ 以内，则声明成功。\n\n最终输出规范：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。每个测试用例的结果是 $R$ 次运行中成功次数的整数计数，按上述测试用例的顺序排列。例如，输出格式为 $[s_1,s_2,s_3,s_4,s_5]$，其中每个 $s_i$ 是一个整数。\n\n科学真实性和约束：\n- 盆地跳跃必须在半径为 $r$ 的圆盘面积上均匀采样。为实现均匀性，将半径采样为 $r \\sqrt{u}$，其中 $u$ 在 $[0,1]$ 中均匀分布，角度在 $[0,2\\pi]$ 弧度内均匀分布。\n- 局部最小化必须使用梯度信息，并通过有界方法强制执行定义域边界。\n- 给定每个测试用例的固定种子，所有计算必须是确定性的。\n\n您的程序必须是自包含的，不需要外部输入，并严格遵守指定的最终输出格式。", "solution": "该问题要求实现一种盆地跳跃元启发式算法，以探索一个二维函数的能量景观，并评估该算法寻找全局最小值的能力如何受到其随机跳跃大小的影响。解决方案的构建是有条不紊的，首先分析目标函数以确定真正的全局最小值，然后实现指定的盆地跳跃算法。\n\n目标函数由 $f(x,y) = \\sin(x) + \\sin(y) + 0.2(x^2 + y^2)$ 给出，定义域为 $(x,y) \\in [-6,6] \\times [-6,6]$。此函数的一个关键特性是其可分性，意味着它可以表示为两个独立的一维函数之和：$f(x,y) = g(x) + g(y)$，其中 $g(t) = \\sin(t) + 0.2t^2$。此特性意味着 $f(x,y)$ 的全局最小值出现在点 $(t^\\star, t^\\star)$ 处，其中 $t^\\star$ 是使 $g(t)$ 在区间 $[-6,6]$ 上取得全局最小值的 $t$ 值。\n\n首先，我们必须精确确定这个全局最小值，它将作为成功的基准。$g(t)$ 的最小值点要么是导数 $g'(t) = \\cos(t) + 0.4t$ 为零的驻点，要么是边界点 $t = -6$ 和 $t = 6$。数值研究表明，$g(t)$ 在区间 $[-6,6]$ 内有两个局部最小值。根据规定，采用多起点局部最小化方法来寻找 $g(t)$ 的全局最小值。我们在包含这些局部最小值的子区间（例如 $[-6,0]$ 和 $[0,6]$）上执行有界一维最小化，并将结果值与定义域边界处的函数值 $g(-6)$ 和 $g(6)$进行比较。此过程稳健地确定了 $g(t)$ 的全局最小值位于 $t^\\star \\approx -1.30644$处，其值为 $g(t^\\star) \\approx -0.62638$。因此，二维函数 $f(x,y)$ 的全局最小值为 $f^\\star = f(t^\\star, t^\\star) = 2g(t^\\star) \\approx -1.252767$。这个值将是我们优化运行的目标。\n\n问题的核心是盆地跳跃算法。这是一种随机全局优化技术，旨在克服局部搜索方法的主要弱点：陷入局部最小值。该算法通过迭代应用两个阶段来运作：确定性局部最小化和到新起点的随机“跳跃”或扰动。\n\n对于每个由特定跳跃半径 $r$ 定义的测试用例，实现过程如下：\n$1$. 进行一组 $R=30$ 次的独立运行，以确保结果具有统计意义。每次运行都使用从测试用例和运行索引派生的确定性种子进行初始化，以保证可复现性。\n$2$. 每次运行开始时，从定义域 $[-6,6] \\times [-6,6]$ 中均匀选择一个随机起始点 $(x_0, y_0)$。从该点开始进行局部最小化，以找到第一个局部最小值，该值成为初始找到的“最佳”解 $(x_{\\text{best}}, y_{\\text{best}})$。\n$3$. 局部最小化使用基于梯度的拟牛顿法执行，特别是带箱式约束的有限内存Broyden–Fletcher–Goldfarb–Shanno算法 (`L-BFGS-B`)。该方法高效且遵守定义域边界，符合要求。它使用 $f(x,y)$ 的解析梯度（雅可比），即 $\\nabla f(x,y) = (\\cos(x)+0.4x, \\cos(y)+0.4y)$，以加快收敛速度。\n$4$. 然后，主盆地跳跃循环执行 $N=60$ 步。在每一步中：\n    a. 通过从当前最佳最小值 $x_{\\text{best}}$ 进行随机跳跃来生成一个新的候选点。跳跃是从半径为 $r$ 的圆盘中均匀采样的。为实现这一点，跳跃幅度 $\\rho$ 采样为 $\\rho = r \\sqrt{u}$，其中 $u \\sim U(0,1)$，跳跃角度 $\\theta$ 从 $\\theta \\sim U(0, 2\\pi)$ 中采样。新点是 $x_{\\text{prop}} = x_{\\text{best}} + (\\rho\\cos\\theta, \\rho\\sin\\theta)$。\n    b. 对提出的点 $x_{\\text{prop}}$进行裁剪，以确保其保持在定义域 $[-6,6] \\times [-6,6]$ 内。\n    c. 从这个裁剪后的点开始进行新的局部最小化，得到一个新的局部最小值 $x_{\\text{new}}$，其函数值为 $f_{\\text{new}}$。\n    d. 应用贪婪接受准则：如果 $f_{\\text{new}}  f_{\\text{best}}$，则接受新的最小值作为当前最佳解。否则，算法停留在先前的最佳最小值处。这确保了在局部最小值景观上的搜索轨迹在能量上是单调递减的。\n$5$. 在 $N=60$ 步之后，将最终函数值 $f_{\\text{best}}$ 与预先计算的全局最小值 $f^\\star$ 进行比较。如果 $|f_{\\text{best}} - f^\\star|  \\varepsilon$，其中容差 $\\varepsilon=10^{-6}$，则该次运行被计为成功。\n$6$. 对于给定的跳跃半径 $r$，记录 $R=30$ 次运行中成功的总次数。\n\n对五个指定的半径（$r=0.0, 0.5, 1.5, 3.0, 5.0$）中的每一个重复整个过程，并将得到的成功计数汇总到一个最终列表中。$r=0.0$ 的情况作为一个对照组，对应于没有任何跳跃的多起点局部搜索。变化的半径旨在探测试图找到全局最小值时，探索（大跳跃）与利用（小跳跃）之间的权衡。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize, minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements a basin hopping metaheuristic to find the global minimum of a 2D function.\n    The program evaluates the success rate for different jump radii.\n    \"\"\"\n    # Common parameters for all test cases\n    N_STEPS = 60\n    N_RUNS = 30\n    DOMAIN_BOUNDS_1D = (-6, 6)\n    DOMAIN_BOUNDS_2D = [(-6, 6), (-6, 6)]\n    GLOBAL_TOLERANCE = 1e-6\n    TEST_CASES_R = [0.0, 0.5, 1.5, 3.0, 5.0]\n\n    # --- Step 1: Define objective function and its gradient ---\n    def f(p):\n        \"\"\"The 2D objective function f(x, y).\"\"\"\n        x, y = p\n        return np.sin(x) + np.sin(y) + 0.2 * (x**2 + y**2)\n\n    def jac_f(p):\n        \"\"\"The Jacobian (gradient) of the objective function.\"\"\"\n        x, y = p\n        return np.array([np.cos(x) + 0.4 * x, np.cos(y) + 0.4 * y])\n\n    # --- Step 2: Compute the true global minimum value ---\n    def g(t):\n        \"\"\"The 1D component of the separable function f(x, y).\"\"\"\n        return np.sin(t) + 0.2 * t**2\n\n    # Use multi-start local minimization for the 1D function g(t)\n    # Search for minima in sub-intervals based on analysis of g'(t).\n    res1 = minimize_scalar(g, bounds=(-3, 0), method='bounded')\n    res2 = minimize_scalar(g, bounds=(2, 5), method='bounded')\n    \n    # Consider the domain endpoints as potential minima\n    g_at_endpoints = [g(DOMAIN_BOUNDS_1D[0]), g(DOMAIN_BOUNDS_1D[1])]\n\n    # The global minimum of g(t) is the minimum of all found local minima and endpoint values.\n    global_min_g = min(res1.fun, res2.fun, *g_at_endpoints)\n    \n    # The global minimum of f(x,y) is twice the global minimum of g(t).\n    f_star = 2 * global_min_g\n\n    # --- Step 3: Run basin hopping for each test case ---\n    results = []\n    for i, r in enumerate(TEST_CASES_R):\n        success_count = 0\n        for j in range(N_RUNS):\n            # Use a deterministic seed for reproducibility, unique for each run.\n            seed = i * N_RUNS + j\n            rng = np.random.default_rng(seed)\n\n            # Start from a random initial point within the domain.\n            x0 = rng.uniform(DOMAIN_BOUNDS_1D[0], DOMAIN_BOUNDS_1D[1], size=2)\n\n            # Perform an initial local minimization.\n            res = minimize(f, x0, jac=jac_f, method='L-BFGS-B', bounds=DOMAIN_BOUNDS_2D)\n            x_best = res.x\n            f_best = res.fun\n\n            # Perform N basin hopping steps.\n            for _ in range(N_STEPS):\n                # Propose a random jump, sampled uniformly from a disk of radius r.\n                jump_magnitude = r * np.sqrt(rng.uniform(0, 1))\n                jump_angle = rng.uniform(0, 2 * np.pi)\n                \n                jump_vector = np.array([jump_magnitude * np.cos(jump_angle), \n                                        jump_magnitude * np.sin(jump_angle)])\n                \n                x_proposed = x_best + jump_vector\n                \n                # Clip the new point to stay within the domain.\n                x_proposed_clipped = np.clip(x_proposed, DOMAIN_BOUNDS_1D[0], DOMAIN_BOUNDS_1D[1])\n\n                # Perform a local minimization from the new point.\n                res_new = minimize(f, x_proposed_clipped, jac=jac_f, method='L-BFGS-B', bounds=DOMAIN_BOUNDS_2D)\n                \n                # Greedy acceptance criterion: accept only if strictly better.\n                if res_new.fun  f_best:\n                    x_best = res_new.x\n                    f_best = res_new.fun\n\n            # After all steps, check if the found minimum is the global minimum.\n            if abs(f_best - f_star)  GLOBAL_TOLERANCE:\n                success_count += 1\n                \n        results.append(success_count)\n\n    # --- Step 4: Final Output ---\n    # Print the success counts for each test case in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3156501"}, {"introduction": "除了用随机跳跃来逃离局部最小值，我们能否从一开始就避免陷入这些陷阱呢？本练习将带您探索一种巧妙的连续介质方法（continuation method）[@problem_id:3156481]。您将通过解决一个问题的简化版本入手，然后逐步增加其复杂性，从而有效应对难题，在函数“地形”的演变过程中，您会发现通过追踪最小值的变化路径，往往能成功地避开那些随着复杂性增加而出现的陷阱，并最终抵达全局最优点。", "problem": "编写一个完整且可运行的程序，演示局部最小值的出现如何依赖于截断级数中的傅里叶模态数，以及模型复杂度的延拓策略如何帮助在增加模态时追踪最小值。程序必须构建一个定义在周期域上的单参数函数族，并使用有原则的定义对其进行分析。\n\n使用以下基于核心定义的设置：\n- 令 $x$ 表示定义域 $[0,2\\pi)$ 上的一个角度，单位为弧度。\n- 对于正整数 $M$，定义截断傅里叶级数\n$$\nf_M(x) \\equiv \\sum_{k=1}^{M} \\frac{\\cos(k x)}{k}.\n$$\n- 如果存在 $\\delta0$ 使得对于所有满足 $|x-x^\\star|\\delta$ 的 $x$ 都有 $f_M(x^\\star) \\le f_M(x)$，则点 $x^\\star$ 是 $f_M$ 的一个局部最小值。全局最小值是点 $x^\\dagger$，使得对于整个定义域中的所有 $x$ 都有 $f_M(x^\\dagger) \\le f_M(x)$。\n\n基于基本原理实现的核心任务：\n1) 使用 $N$ 个点的均匀网格（使用 $N=8192$）对定义域 $[0,2\\pi)$ 进行离散化，并通过与周期性边界条件下的直接邻居进行离散比较，数值化地检测 $f_M$ 的不同局部最小值的数量。具体来说，对网格点 $x_i$ 使用离散准则：如果在索引 $i$ 处满足 $f_M(x_i)  f_M(x_{i-1})$ 和 $f_M(x_i)  f_M(x_{i+1})$，则检测到一个离散局部最小值，其中索引对 $N$ 取模以强制周期性。当 $N$ 足够大时，这为连续局部最小值的数量提供了一个稳健的近似。所有角度必须以弧度处理。\n2) 实现带有回溯线搜索的梯度下降法，从给定的初始条件开始寻找 $f_M$ 的一个局部最小化子。使用解析梯度\n$$\n\\frac{d}{dx} f_M(x) \\;=\\; -\\sum_{k=1}^{M} \\sin(kx)\n$$\n来执行对 $x$ 的有原则的最速下降步骤（步进方向是负梯度）。确保步进能减小目标函数值（Armijo型充分下降），并在每次更新后将 $x$ 对 $2\\pi$ 取模以保持在定义域内。当梯度幅值足够小或达到最大迭代次数时停止。\n3) 在复杂度参数 $M$ 上执行延拓：从最粗糙的模型 $M_1=1$ 开始，通过在离散化网格上进行穷举搜索找到其全局最小化子。然后，对于测试套件中每个后续的 $M_j$（严格递增），使用为 $M_{j-1}$ 获得的最小化子作为 $f_{M_j}$ 的梯度下降法的初始条件。这种延拓策略旨在应对随着 $M$ 增加而出现的新局部最小值。\n4) 对于测试套件中的每个 $M_j$，确定：\n   - 在网格上检测到的 $f_{M_j}$ 的局部最小值数量。\n   - 一个成功指标，定义如下：计算 $M_j$ 在网格上的全局最小值，并将其与从前一阶段初始化的梯度下降法返回的点所达到的值进行比较。如果获得的值在网格全局最小值的一个小的非负容差 $\\varepsilon$ 范围内（使用 $\\varepsilon = 10^{-9}$），则记录成功为 $1$，否则记录为 $0$。对于 $M_1=1$，按照约定将成功指标定义为 $1$，因为该过程在最粗糙模型的全局最小化子处初始化。\n\n角度单位要求：\n- 所有角度必须是弧度。\n\n用于覆盖不同方面的测试套件：\n- 所有计算使用 $N=8192$ 个网格点。\n- 使用模态数序列 $M \\in \\{\\,1,2,5,10,20\\,\\}$。这涵盖了：\n  - 具有单个最小值的基本情况 ($M=1$)。\n  - 出现额外局部最小值的中间情况 ($M=2,5$)。\n  - 具有许多局部最小值的更复杂情况 ($M=10,20$)。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果。\n- 对于指定顺序中的每个测试值 $M$，输出一个双元素列表 $[n_M,s_M]$，其中 $n_M$ 是检测到的局部最小值数量（一个整数），$s_M$ 是成功指标（$0$ 或 $1$）。\n- 因此，最终打印的行必须看起来像\n$[[n_{1},s_{1}],[n_{2},s_{2}],[n_{5},s_{5}],[n_{10},s_{10}],[n_{20},s_{20}]]$\n不含任何额外文本。所有值都是无量纲的，角度以弧度为单位。", "solution": "该问题要求分析由截断傅里叶级数定义的单参数函数族 $f_M(x)$。目标是编程研究局部最小值的数量如何随着复杂度参数 $M$（模态数）的增加而变化，并评估一种延拓策略，以在这种不断变化的景观中追踪全局最小值。\n\n所考虑的函数定义在周期域 $x \\in [0, 2\\pi)$ 上：\n$$\nf_M(x) \\equiv \\sum_{k=1}^{M} \\frac{\\cos(k x)}{k}\n$$\n其中 $M$ 是一个正整数。我们将在该域的离散版本上进行分析，该版本由一个包含 $N=8192$ 个点的均匀网格表示，记为 $\\{x_i\\}_{i=0}^{N-1}$，其中 $x_i = \\frac{2\\pi i}{N}$。\n\n核心任务基于以下原则实现：\n\n1. **局部最小值的数值检测：** 如果 $f_M(x^\\star) \\le f_M(x)$ 在 $x^\\star$ 的一个邻域内成立，则点 $x^\\star$ 是一个局部最小值。在我们的离散网格上，我们通过识别一个点 $x_i$ 的函数值是否严格小于其直接邻居的函数值来近似这一点。为了尊重域的周期性，邻居索引对 $N$ 取模。也就是说，如果点 $x_i$ 满足以下条件，则它被检测为一个局部最小值：\n$$\nf_M(x_i)  f_M(x_{i-1 \\pmod N}) \\quad \\text{和} \\quad f_M(x_i)  f_M(x_{i+1 \\pmod N})\n$$\n此类点的总数 $n_M$ 为给定 $M$ 的局部最小值数量提供了一个数值估计。对于足够大的 $N$，这个离散计数是连续函数最小值数量的可靠近似。\n\n2. **梯度下降优化：** 为了从给定的起始点找到一个最小化子，我们采用梯度下降算法。这种迭代方法通过在负梯度方向（即最速下降方向）上采取步进，来寻求最小化 $f_M(x)$。$f_M(x)$ 的解析梯度为：\n$$\n\\frac{d}{dx} f_M(x) = -\\sum_{k=1}^{M} \\sin(kx)\n$$\n一次梯度下降迭代根据以下规则更新当前点 $x_{j}$ 到 $x_{j+1}$：\n$$\nx_{j+1} = \\left( x_j - t_j \\left( \\sum_{k=1}^{M} \\sin(kx_j) \\right) \\right) \\pmod{2\\pi}\n$$\n其中 $t_j  0$ 是步长。新位置对 $2\\pi$ 取模以保持在定义域内。\n\n步长 $t_j$ 使用回溯线搜索来确定，以确保足够的进展。这包括从一个试验步长开始，并迭代地减小它，直到满足 Armijo 条件：\n$$\nf_M(x_{j+1}) \\le f_M(x_j) + \\alpha t_j \\nabla f_M(x_j)^T p_j\n$$\n其中 $p_j = -\\nabla f_M(x_j)$ 是下降方向，$\\alpha \\in (0, 1)$ 是一个控制参数（例如，$\\alpha=0.3$）。此条件保证了步进相对于步长和方向导数提供了函数值的充分下降。当梯度的幅值低于一个小的容差或达到最大迭代次数时，算法终止。\n\n3. **模型复杂度的延拓：** 随着 $M$ 的增加，函数 $f_M(x)$ 会产生更多的局部最小值，这使得像梯度下降这样的局部搜索方法很难从任意起始点找到全局最小值。延拓策略试图通过利用来自更简单模型的解来缓解这个问题。该过程如下：\n- 对于基本情况 $M_1=1$，函数 $f_1(x) = \\cos(x)$ 很简单，在 $x=\\pi$ 处有唯一的全局最小值。这个最小化子通过在离散网格上进行穷举搜索找到。\n- 对于每个后续的、更复杂的模型 $M_j$（$j1$），将为前一个模型 $M_{j-1}$ 找到的最小化子用作应用于 $f_{M_j}$ 的梯度下降算法的初始猜测。其原理是，对于模型复杂度的微小增加，全局最小值的位置预计只会轻微移动，因此前一个解提供了一个良好的起点。\n\n4. **延拓策略的评估：** 为了评估这种延拓的有效性，我们定义了一个成功指标 $s_M$。对于每个 $M$，我们比较我们基于延拓的梯度下降法找到的点处的函数值 $f_M(x_{\\text{GD}}^\\star)$ 与网格上的真实全局最小值 $f_M^\\dagger = \\min_{i} f_M(x_i)$。如果找到的最小值与全局最小值足够接近，则该策略被认为是成功的：\n$$\ns_M = \\begin{cases} 1  \\text{if } f_M(x_{\\text{GD}}^\\star) \\le f_M^\\dagger + \\varepsilon \\\\ 0  \\text{otherwise} \\end{cases}\n$$\n其中 $\\varepsilon$ 是一个小的非负容差，设置为 $10^{-9}$。对于基本情况 $M=1$，成功是按约定定义的，因此 $s_1=1$。这个指标揭示了延拓方法是成功追踪了全局最小值，还是陷入了局部最小值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes a family of functions f_M(x) to demonstrate the emergence of\n    local minima and the use of a continuation strategy.\n    \"\"\"\n    # Define problem parameters from the statement.\n    N = 8192\n    M_VALUES = [1, 2, 5, 10, 20]\n    EPSILON = 1e-9\n\n    # Pre-compute the grid for x.\n    # The domain is [0, 2*pi), so endpoint is False.\n    x_grid = np.linspace(0, 2 * np.pi, N, endpoint=False)\n    \n    # Store the results for each M.\n    results = []\n    \n    # This variable will store the minimizer from the previous stage (M_{j-1})\n    # to be used as the starting point for the current stage (M_j).\n    x_continuation_min = 0.0\n\n    def f_M(x, M):\n        \"\"\"\n        Computes the function f_M(x) = sum_{k=1 to M} cos(k*x)/k.\n        This function is vectorized to handle an array of x values.\n        \"\"\"\n        k = np.arange(1, M + 1)\n        # Use broadcasting for vectorized computation.\n        # x must be a column vector for this to work with a row vector k.\n        if x.ndim == 1:\n            x_col = x[:, np.newaxis]\n        else: # Handle scalar input for gradient descent\n            x_col = np.array([[x]])\n        \n        cos_terms = np.cos(k * x_col) / k\n        # Sum over the k-axis (axis=1)\n        return np.sum(cos_terms, axis=1).squeeze()\n\n    def grad_f_M(x_scalar, M):\n        \"\"\"\n        Computes the gradient of f_M(x) at a single point x.\n        The gradient is -sum_{k=1 to M} sin(k*x).\n        \"\"\"\n        k = np.arange(1, M + 1)\n        return np.sum(-np.sin(k * x_scalar))\n\n    def gradient_descent(x_start, M):\n        \"\"\"\n        Performs gradient descent with backtracking line search to find a local\n        minimum of f_M(x).\n        \"\"\"\n        # Hyperparameters for the optimization algorithm.\n        max_iter = 1000\n        grad_tol = 1e-8\n        alpha = 0.3  # Armijo condition control parameter\n        beta = 0.8   # Backtracking step size reduction factor\n\n        x_current = x_start\n\n        for _ in range(max_iter):\n            # Calculate gradient at the current point.\n            g = grad_f_M(x_current, M)\n\n            # Stop if the gradient is sufficiently small.\n            if np.abs(g)  grad_tol:\n                break\n\n            # Set descent direction (negative gradient).\n            p_k = -g\n            \n            # --- Backtracking Line Search ---\n            t = 1.0  # Initial step size\n            f_current = f_M(np.array([x_current]), M)\n            \n            while True:\n                x_next = x_current + t * p_k\n                # Function f_M is periodic, so modulo is not strictly necessary\n                # for evaluation but good practice.\n                f_next = f_M(np.array([x_next]), M)\n                # Armijo condition: f(x+tp) = f(x) + alpha*t*grad(f)^T*p\n                # Here p = -grad(f), so grad(f)^T*p = -|grad(f)|^2\n                if f_next = f_current - alpha * t * g * g:\n                    break\n                t = beta * t\n\n            # --- Update Step ---\n            # Update position and wrap around the 2*pi domain.\n            x_current = (x_current + t * p_k) % (2 * np.pi)\n            \n        return x_current\n\n    # Main loop to iterate through the specified values of M.\n    for M in M_VALUES:\n        # 1. Evaluate the function f_M on the entire grid.\n        y_grid = f_M(x_grid, M)\n\n        # 2. Detect and count the number of local minima on the grid.\n        # A point is a local minimum if it's smaller than its left and right neighbors.\n        # np.roll handles the periodic boundary conditions.\n        is_local_min = (y_grid  np.roll(y_grid, 1))  (y_grid  np.roll(y_grid, -1))\n        n_M = int(np.sum(is_local_min))\n\n        s_M = 0  # Default to failure (0).\n\n        if M == 1:\n            # For the base case M=1, find the global minimizer by exhaustive grid search.\n            min_idx = np.argmin(y_grid)\n            x_continuation_min = x_grid[min_idx]\n            # Success is defined as 1 for the base case.\n            s_M = 1\n        else:\n            # Continuation: use the minimizer from the previous M as the starting point.\n            x_start = x_continuation_min\n            x_gd_min = gradient_descent(x_start, M)\n            \n            # The result of this optimization becomes the starting point for the next M.\n            x_continuation_min = x_gd_min\n\n            # 4. Evaluate success by comparing the found value with the grid's global minimum.\n            grid_global_min_val = np.min(y_grid)\n            val_at_gd_min = f_M(np.array([x_gd_min]), M)\n\n            if val_at_gd_min = grid_global_min_val + EPSILON:\n                s_M = 1\n        \n        results.append([n_M, s_M])\n\n    # Print the final result in the specified format: [[n1,s1],[n2,s2],...].\n    # Using str() on a list of lists and removing spaces to match the format precisely.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3156481"}]}