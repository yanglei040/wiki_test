{"hands_on_practices": [{"introduction": "理论知识需要通过实践来巩固。本节的第一个练习将带你动手实现割线法，这是一种强大而实用的开放式求根方法，其优点在于不需要计算函数的解析导数。通过解决一个经典的超越方程求根问题 [@problem_id:2422713]，你将掌握割线法迭代的核心逻辑，并熟悉在编程中设置收敛准则与处理不同初始猜测的技巧。", "problem": "考虑一个单实变量的非线性方程，以弧度表示，由 $f(x) = \\sin(x) - \\dfrac{x}{2}$ 给出。解是满足 $f(x) = 0$ 的任意实数 $x$。其平凡解为 $x = 0$。您的任务是根据指定的初始种子点，在不使用任何外部数据的情况下，计算 $f(x) = 0$ 的解的数值近似值。角度必须以弧度处理。\n\n请使用以下测试套件。每个测试用例提供一个由实数值种子点组成的有序对 $(x_0, x_1)$。对于每个测试用例，仅使用问题陈述和种子点所隐含的信息，计算 $f(x)$ 的一个根的近似值 $\\hat{x}$，并同时施加以下停止准则：当当前近似值处的函数绝对值满足 $\\lvert f(\\hat{x}) \\rvert \\le \\tau$ 或连续近似值之间的变化满足 $\\lvert \\Delta \\hat{x} \\rvert \\le \\tau$ 时，终止迭代，其中 $\\tau = 10^{-12}$。为每个测试用例设置 $N_{\\max} = 100$ 次迭代的硬性上限。如果在 $N_{\\max}$ 次迭代内未达到收敛，则返回最后一次的迭代值作为近似解。\n\n测试套件（所有种子点均以弧度为单位）：\n- 用例 1：$(x_0, x_1) = (1.0, 2.0)$\n- 用例 2：$(x_0, x_1) = (-1.0, -2.0)$\n- 用例 3：$(x_0, x_1) = (0.1, -0.1)$\n- 用例 4：$(x_0, x_1) = (1.0, 1.5)$\n\n数值细节要求：\n- 以弧度处理角度。\n- 完全按照规定使用函数 $f(x) = \\sin(x) - \\dfrac{x}{2}$。\n- 使用如上定义的容差 $\\tau = 10^{-12}$ 和迭代上限 $N_{\\max} = 100$。\n\n您的程序应生成单行输出，其中包含用方括号括起来的、以逗号分隔的结果列表。对于每个测试用例，输出近似根 $\\hat{x}$，并四舍五入到 $10$ 位小数。因此，最后一行必须采用 $[\\hat{x}_1,\\hat{x}_2,\\hat{x}_3,\\hat{x}_4]$ 的形式，其中每个 $\\hat{x}_k$ 是一个四舍五入到 $10$ 位小数的十进制格式实数。不允许有任何其他输出。", "solution": "所给问题要求对非线性超越方程 $f(x) = \\sin(x) - \\dfrac{x}{2} = 0$ 的根进行数值近似。除了平凡解 $x=0$ 外，这些根是函数 $y = \\sin(x)$ 和 $y = \\dfrac{x}{2}$ 图像的交点。对这些函数图像的简单分析表明，存在一个正根、一个负根以及位于原点的平凡根。\n\n问题规定必须使用开放法求根，并且为每个测试用例提供了两个初始种子点 $(x_0, x_1)$。这种设置唯一地确定了方法的选择：割线法 (Secant method)。这是一种两点迭代法，它用一条通过最近两次近似点 $(x_{k-1}, f(x_{k-1}))$ 和 $(x_k, f(x_k))$ 的割线来近似函数 $f(x)$。下一个近似值 $x_{k+1}$ 是这条割线的根。\n\n割线法的递推关系式为：\n$$ x_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})} $$\n该算法从初始种子点 $x_0$ 和 $x_1$ 开始，并迭代生成一个近似序列 $\\{x_k\\}$。\n\n对于每个测试用例 $(x_0, x_1)$，实现将按以下步骤进行：\n$1$. 初始化序列中的前两个点，$x_{k-1} = x_0$ 和 $x_k = x_1$。\n$2$. 进入一个最多迭代 $N_{\\max} = 100$ 次的循环。\n$3$. 在循环内部，计算函数值 $f(x_k)$ 和 $f(x_{k-1})$。\n$4$. 必须对分母 $f(x_k) - f(x_{k-1})$ 进行关键检查。如果其绝对值接近于零，则割线近似水平，方法会变得不稳定或因除以零而失败。在这种情况下，必须终止迭代。\n$5$. 使用割线法公式计算下一个近似值 $\\hat{x} \\equiv x_{k+1}$。\n$6$. 如果满足两个指定停止准则中的任何一个，则迭代终止：\n    a) 残差足够小：$|f(\\hat{x})| \\le \\tau$。\n    b) 连续近似值之间的变化足够小：$|\\Delta \\hat{x}| = |x_{k+1} - x_k| \\le \\tau$。\n指定的容差为 $\\tau = 10^{-12}$。\n$7$. 如果未达到收敛，则更新迭代值：将 $x_{k-1}$ 设置为 $x_k$，并将 $x_k$ 设置为 $x_{k+1}$。然后继续循环。\n$8$. 如果循环完成 $N_{\\max}$ 次迭代后仍未收敛，则返回最后计算的近似值 $x_k$ 作为结果。\n\n测试用例简要分析：\n- 用例 1 和 2 的初始种子点分别为 $(1.0, 2.0)$ 和 $(-1.0, -2.0)$，它们分别包围了非平凡的正根和负根。函数 $f(x)$ 是奇函数，即 $f(-x) = -f(x)$，因此预计负根是正根的相反数。预计割线法会快速收敛。\n- 用例 3 的种子点为 $(0.1, -0.1)$，使用的初始点关于原点对称。由于函数 $f(x)$ 的奇对称性，割线法的第一步迭代将得到 $x_2 = 0$，即精确的平凡解。\n- 用例 4 的种子点为 $(1.0, 1.5)$，这是一个更具挑战性的场景。区间 $[1.0, 1.5]$ 包含 $f(x)$ 的一个局部极大值点，该点满足 $f'(x) = \\cos(x) - \\frac{1}{2} = 0$，位于 $x = \\arccos(0.5) = \\frac{\\pi}{3} \\approx 1.047$ 弧度。初始割线的斜率会很小，这可能将下一个迭代点投射到远离根的位置，但该算法足够稳健，最终仍能收敛到正确的正根。\n\n最终的程序将为每个测试用例实现此算法，并按规定格式化所得的近似值。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes numerical approximations for roots of f(x) = sin(x) - x/2\n    using the Secant method for a given set of test cases.\n    \"\"\"\n\n    def f(x: float) - float:\n        \"\"\"The nonlinear function f(x) = sin(x) - x/2.\"\"\"\n        return np.sin(x) - x / 2.0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 2.0),\n        (-1.0, -2.0),\n        (0.1, -0.1),\n        (1.0, 1.5),\n    ]\n\n    # Parameters from the problem statement.\n    TAU = 1e-12\n    N_MAX = 100\n\n    results = []\n    for case in test_cases:\n        x_prev, x_curr = case\n        \n        for _ in range(N_MAX):\n            f_curr = f(x_curr)\n            f_prev = f(x_prev)\n\n            # Denominator for the Secant method formula\n            denominator = f_curr - f_prev\n\n            # If the denominator is very close to zero, the method may fail or stagnate.\n            # This can happen if f(x_curr) is very close to f(x_prev).\n            # The loop will naturally terminate if convergence is not achieved,\n            # so we only protect against division by zero.\n            if abs(denominator)  1e-15:  # Use a small epsilon to avoid division by zero\n                break\n\n            # Calculate the next approximation using the Secant method formula\n            x_next = x_curr - f_curr * (x_curr - x_prev) / denominator\n\n            # Check the stopping criteria:\n            # 1. The absolute value of the function at the new approximation.\n            # 2. The absolute change in successive approximations.\n            if abs(f(x_next)) = TAU or abs(x_next - x_curr) = TAU:\n                x_curr = x_next  # Update to the final converged value\n                break\n\n            # Update points for the next iteration\n            x_prev = x_curr\n            x_curr = x_next\n        \n        # Append the final approximation for this case to the results list.\n        # The problem requires rounding to 10 decimal places.\n        results.append(x_curr)\n\n    # Final print statement in the exact required format.\n    # The format specifier '.10f' ensures rounding to 10 decimal places.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "2422713"}, {"introduction": "牛顿法以其二次收敛速度而闻名，但这种优越性是有条件的。当根的重数大于1时，牛顿法的收敛速度会退化为线性。这个练习旨在深入探究这一现象 [@problem_id:3260046]，你将从第一性原理出发，推导标准牛顿法在重根附近的收敛行为，并进一步设计和实现两种修正算法来恢复其二次收敛性。这项实践不仅能加深你对牛顿法理论的理解，也揭示了数值算法在实际应用中的细微之处与应对策略。", "problem": "考虑将开放式求根方法——牛顿法，应用于具有重根的多项式。设函数定义为 $f(x) = (x - 2)^2(x + 1)$，该函数在 $x = 2$ 处有一个重数为 $m = 2$ 的根，在 $x = -1$ 处有一个重数为 $m = 1$ 的根。请基于第一性原理进行严格分析，研究根的重数对牛顿法局部收敛行为的影响，然后设计并实现两种重数修正方法：一种使用已知的重数 $m$，另一种使用根据导数估计的 $m$。\n\n您的推导必须从以下基本依据出发：\n- 重数为 $m$ 的根的定义：如果 $r$ 是 $f$ 的一个根，则存在一个函数 $g$ 满足 $g(r) \\neq 0$，使得 $f(x) = (x - r)^m g(x)$。\n- 一个足够光滑的函数 $f$ 在点 $x_k$ 附近的一阶泰勒展开：$f(x_{k+1}) \\approx f(x_k) + f'(x_k)(x_{k+1} - x_k)$。\n- 牛顿法的定义：通过在根附近将一阶泰勒近似设为零来获得解的更新，以及局部误差的概念 $e_k = x_k - r$。\n\n基于这些依据：\n- 推导牛顿法在重数为 $m$ 的根附近的局部误差递推关系，并确定其收敛阶和渐进误差常数。\n- 在重数 $m$ 已知的情况下，推导一种重数修正的牛顿法更新公式。\n- 推导一个仅用 $f$、$f'$ 和 $f''$ 表示的实用重数 $m$ 估计量，并用它构建当 $m$ 未知时的重数修正的牛顿法更新公式。\n\n实现要求：\n- 实现三种方法：标准牛顿法、使用已知 $m$ 的重数修正牛顿法，以及使用根据导数估计的 $m$ 的重数修正牛顿法。\n- 对于数值实验，请使用 $f(x) = (x - 2)^2(x + 1)$ 的显式导数，并根据迭代值计算经验收敛阶。将相对于所选目标根 $r$ 的局部误差定义为 $e_k = x_k - r$，并使用三个连续误差通过以下公式计算经验阶：\n$$\np \\approx \\frac{\\ln\\left(\\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert\\right)}{\\ln\\left(\\lvert e_k \\rvert / \\lvert e_{k-1} \\rvert\\right)}.\n$$\n同时，在终止时，使用最后两个非零误差报告最后一个可用的渐进误差常数 $\\rho \\approx \\lvert e_{k+1} \\rvert / \\lvert e_k \\rvert$。如果可用于阶数估计的非零误差少于三个，则报告 $p = 0.0$；如果可用于比率估计的非零误差少于两个，则报告 $\\rho = 0.0$。\n\n迭代的停止准则：\n- 当 $\\lvert f(x_k) \\rvert \\leq 10^{-16}$ 或 $\\lvert e_k \\rvert \\leq 10^{-16}$ 或达到 50 次迭代时终止，以先到者为准。\n\n测试套件：\n- 使用以下 6 个测试用例，每个用例由方法标签、初始猜测值 $x_0$ 和目标根 $r$ 定义：\n    1. 标准牛顿法, $x_0 = 3.5$, $r = 2$。\n    2. 使用已知 $m = 2$ 的重数修正牛顿法, $x_0 = 3.5$, $r = 2$。\n    3. 使用根据导数估计的 $m$ 的重数修正牛顿法, $x_0 = 3.5$, $r = 2$。\n    4. 标准牛顿法, $x_0 = -0.8$, $r = -1$。\n    5. 使用根据导数估计的 $m$ 的重数修正牛顿法, $x_0 = -0.8$, $r = -1$。\n    6. 标准牛顿法, $x_0 = 2.1$, $r = 2$。\n\n对于每个测试用例，报告两个浮点数，四舍五入到六位小数：\n- 根据最后三个非零误差计算的经验收敛阶 $p$。\n- 根据最后两个非零误差计算的最后渐进误差常数 $\\rho$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果，格式为一个逗号分隔的列表，列表中的元素为方括号括起来的数对，每个数对的格式为 $[p,\\rho]$。整体输出应如下所示：\n$[[p_1,\\rho_1],[p_2,\\rho_2],[p_3,\\rho_3],[p_4,\\rho_4],[p_5,\\rho_5],[p_6,\\rho_6]]$。", "solution": "该问题陈述是一个在数值分析领域中定义明确且具有科学依据的练习，具体涉及牛顿法对不同重数根的收敛性质。所有提供的信息都是自洽的、数学上一致的，并且与开放式求根方法的主题直接相关。因此，该问题被认为是有效的，下面将给出完整解答。\n\n问题的核心是分析并修正当牛顿法应用于重数 $m > 1$ 的根时其收敛速度下降的问题。我们将首先推导标准方法的收敛行为，然后推导两种修正方法，最后通过实现它们来验证理论结果。\n\n所考虑的函数是 $f(x) = (x - 2)^2(x + 1)$。该多项式有一个重数为 $m=2$ 的根 $r=2$ 和一个重数为 $m=1$ 的单根 $r=-1$。为了实现算法，需要它的导数：\n$f(x) = x^3 - 3x^2 + 4$\n$f'(x) = 3x^2 - 6x$\n$f''(x) = 6x - 6$\n\n### 1. 标准牛顿法的局部收敛性\n\n设 $r$ 为一个重数 $m \\ge 1$ 的根。根据定义，$f(x)$ 可以写成 $f(x) = (x-r)^m g(x)$，其中 $g(r) \\neq 0$。\n其一阶导数为 $f'(x) = m(x-r)^{m-1}g(x) + (x-r)^m g'(x)$。\n\n标准牛顿法迭代公式为：\n$$x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$$\n代入 $f(x_k)$ 和 $f'(x_k)$ 的表达式：\n$$x_{k+1} = x_k - \\frac{(x_k-r)^m g(x_k)}{m(x_k-r)^{m-1}g(x_k) + (x_k-r)^m g'(x_k)}$$\n设第 $k$ 步的误差为 $e_k = x_k - r$。那么下一步的误差为 $e_{k+1} = x_{k+1} - r$。\n$$e_{k+1} = e_k - \\frac{e_k^m g(x_k)}{m e_k^{m-1}g(x_k) + e_k^m g'(x_k)}$$\n从分母的分数中提出 $e_k^{m-1}$：\n$$e_{k+1} = e_k - \\frac{e_k g(x_k)}{m g(x_k) + e_k g'(x_k)} = e_k \\left( 1 - \\frac{g(x_k)}{m g(x_k) + e_k g'(x_k)} \\right)$$\n$$e_{k+1} = e_k \\left( \\frac{m g(x_k) + e_k g'(x_k) - g(x_k)}{m g(x_k) + e_k g'(x_k)} \\right) = e_k \\left( \\frac{(m-1) g(x_k) + e_k g'(x_k)}{m g(x_k) + e_k g'(x_k)} \\right)$$\n当 $x_k \\to r$ 时，我们有 $e_k \\to 0$ 且 $g(x_k) \\to g(r)$。误差的表达式变为：\n$$e_{k+1} \\approx e_k \\left( \\frac{(m-1) g(r)}{m g(r)} \\right) = \\left( \\frac{m-1}{m} \\right) e_k$$\n这表明对于 $m > 1$，误差在每一步都按一个常数因子减小。这是线性收敛（阶数 $p=1$）的定义。渐进误差常数为 $\\rho = \\lim_{k\\to\\infty} \\frac{\\lvert e_{k+1} \\rvert}{\\lvert e_k \\rvert} = \\frac{m-1}{m}$。对于本问题中的 $m=2$，我们预期是线性收敛，且 $\\rho = 1/2$。\n\n对于单根 ($m=1$)，该比率为 $(1-1)/1 = 0$，意味着超线性收敛。更详细的泰勒级数分析表明，对于 $m=1$，收敛是二次的 ($p=2$)，且有 $e_{k+1} \\approx \\frac{f''(r)}{2f'(r)}e_k^2$。\n\n### 2. 重数修正的牛顿法（已知 $m$）\n\n为了恢复二次收敛，我们可以修改迭代公式。考虑一个新函数 $u(x) = f(x)^{1/m}$。如果 $f(x) = (x-r)^m g(x)$，那么 $u(x) = (x-r)g(x)^{1/m}$。由于 $g(r) \\neq 0$，函数 $u(x)$ 在 $x=r$ 处有一个单根。对 $u(x)$ 应用标准牛顿法可以得到对其根 $r$ 的二次收敛。\n\n$u(x)$ 的迭代公式为 $x_{k+1} = x_k - \\frac{u(x_k)}{u'(x_k)}$。我们可以用 $f(x_k)$ 和 $f'(x_k)$ 来表示它：\n$$u'(x) = \\frac{d}{dx} \\left(f(x)^{1/m}\\right) = \\frac{1}{m}f(x)^{\\frac{1}{m}-1}f'(x)$$\n步长修正项为：\n$$\\frac{u(x)}{u'(x)} = \\frac{f(x)^{1/m}}{\\frac{1}{m}f(x)^{\\frac{1}{m}-1}f'(x)} = \\frac{f(x)^{1/m}}{\\frac{f(x)^{1/m}}{m f(x)}f'(x)} = m \\frac{f(x)}{f'(x)}$$\n这导出了重数修正的牛顿法：\n$$x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$$\n由于这等价于对一个具有单根的函数应用标准牛顿法，因此其收敛是二次的 ($p=2$)。\n\n### 3. 重数修正的牛顿法（估计 $m$）\n\n当 $m$ 未知时，可以从函数及其导数中估计它。我们可以找到一个 $m$ 的极限表达式。使用在根 $r$ 附近的渐近形式 $f(x) \\propto (x-r)^m$、$f'(x) \\propto m(x-r)^{m-1}$ 和 $f''(x) \\propto m(m-1)(x-r)^{m-2}$，我们可以构造一个比率，使得未知项可以消去。\n$$\\lim_{x\\to r} \\frac{f(x)f''(x)}{[f'(x)]^2} = \\frac{(x-r)^m g(r) \\cdot m(m-1)(x-r)^{m-2} g(r)}{[m(x-r)^{m-1} g(r)]^2} = \\frac{m(m-1)(x-r)^{2m-2}}{m^2(x-r)^{2m-2}} = \\frac{m-1}{m}$$\n我们可以解出 $m$：\n$$m = \\frac{1}{1 - \\frac{f(x)f''(x)}{[f'(x)]^2}} = \\frac{[f'(x)]^2}{[f'(x)]^2 - f(x)f''(x)}$$\n这提出了一个自适应方案，在每次迭代 $k$ 中，我们如下估计 $m$：\n$$m_k = \\frac{[f'(x_k)]^2}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$$\n并将此估计用于修正的牛顿法更新中：\n$$x_{k+1} = x_k - m_k \\frac{f(x_k)}{f'(x_k)} = x_k - \\frac{f(x_k)f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$$\n这被认为是哈雷方法 (Halley's method)。已知该方法对于重根能恢复二次收敛 ($p=2$)，对于单根能达到三次收敛 ($p=3$)。\n\n### 实现的各种方法总结\n该实现将分析以下三种迭代方案：\n1.  **标准牛顿法:** $x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}$\n2.  **修正法（已知 $m$）:** $x_{k+1} = x_k - m \\frac{f(x_k)}{f'(x_k)}$\n3.  **修正法（估计 $m$）:** $x_{k+1} = x_k - \\frac{f(x_k)f'(x_k)}{[f'(x_k)]^2 - f(x_k)f''(x_k)}$\n\n这些方法将被应用于测试用例，以经验性地计算收敛阶 $p$ 和渐进误差常数 $\\rho$，预期结果将与理论推导一致。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical analysis problem concerning Newton's method and root multiplicity.\n    \"\"\"\n\n    def f(x_val):\n        \"\"\"The function f(x) = (x-2)^2(x+1).\"\"\"\n        # x**3 - 3*x**2 + 4\n        return x_val**3 - 3 * x_val**2 + 4\n\n    def df(x_val):\n        \"\"\"The first derivative f'(x).\"\"\"\n        # 3*x**2 - 6*x\n        return 3 * x_val**2 - 6 * x_val\n\n    def d2f(x_val):\n        \"\"\"The second derivative f''(x).\"\"\"\n        # 6*x - 6\n        return 6 * x_val - 6\n\n    def run_iteration(method, x_0, r, m_known=None):\n        \"\"\"\n        Runs a specified root-finding iteration and computes empirical convergence metrics.\n\n        Args:\n            method (str): The method to use ('standard', 'corrected_known_m', 'corrected_estimated_m').\n            x_0 (float): The initial guess.\n            r (float): The target root.\n            m_known (int, optional): The known multiplicity for the corrected method.\n\n        Returns:\n            A tuple (p, rho) containing the empirical order and asymptotic ratio.\n        \"\"\"\n        x = float(x_0)\n        errors = [x - r]\n        max_iter = 50\n        tol = 1e-16\n\n        for _ in range(max_iter):\n            fx = f(x)\n\n            if abs(fx) = tol or abs(x - r) = tol:\n                break\n\n            dfx = df(x)\n            if abs(dfx)  np.finfo(float).eps:\n                break\n\n            if method == 'standard':\n                x = x - fx / dfx\n            elif method == 'corrected_known_m':\n                x = x - m_known * fx / dfx\n            elif method == 'corrected_estimated_m':\n                d2fx = d2f(x)\n                denominator = dfx**2 - fx * d2fx\n                if abs(denominator)  np.finfo(float).eps:\n                    m_est = 1.0  # Fallback to standard Newton step\n                else:\n                    m_est = dfx**2 / denominator\n                x = x - m_est * fx / dfx\n            else:\n                raise ValueError(f\"Unknown method: {method}\")\n\n            errors.append(x - r)\n\n        # Using a small threshold to filter out errors that are effectively zero\n        nonzero_errors = [e for e in errors if abs(e) > 1e-18]\n\n        p = 0.0\n        if len(nonzero_errors) >= 3:\n            try:\n                e_k_plus_1 = abs(nonzero_errors[-1])\n                e_k = abs(nonzero_errors[-2])\n                e_k_minus_1 = abs(nonzero_errors[-3])\n\n                log_num = np.log(e_k_plus_1 / e_k)\n                log_den = np.log(e_k / e_k_minus_1)\n\n                if abs(log_den) > np.finfo(float).eps:\n                    p = log_num / log_den\n            except (ValueError, ZeroDivisionError):\n                p = 0.0 # Calculation failed\n\n        rho = 0.0\n        if len(nonzero_errors) >= 2:\n            try:\n                e_k_plus_1 = abs(nonzero_errors[-1])\n                e_k = abs(nonzero_errors[-2])\n                rho = e_k_plus_1 / e_k\n            except ZeroDivisionError:\n                rho = 0.0\n\n        return p, rho\n\n    test_cases = [\n        # (method, initial_guess_x0, target_root_r, known_multiplicity_m)\n        ('standard', 3.5, 2, None),                # Case 1\n        ('corrected_known_m', 3.5, 2, 2),         # Case 2\n        ('corrected_estimated_m', 3.5, 2, None),  # Case 3\n        ('standard', -0.8, -1, None),               # Case 4\n        ('corrected_estimated_m', -0.8, -1, None), # Case 5\n        ('standard', 2.1, 2, None)                 # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        method, x0, r, m_known = case\n        p, rho = run_iteration(method, x0, r, m_known)\n        results.append([round(p, 6), round(rho, 6)])\n\n    formatted_results = [f\"[{p_val:.6f},{rho_val:.6f}]\" for p_val, rho_val in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3260046"}, {"introduction": "在真实的科学计算中，单一算法往往难以兼顾鲁棒性与效率。最后的这个练习将引导你设计一个混合求根算法，它结合了割线法和牛顿法的优点。该算法初期使用计算成本较低的割线法来逼近解，一旦进入合适的收敛盆地，则切换到收敛更快的牛顿法 [@problem_id:3167328]。通过实现这种智能切换策略，你将体验到算法设计的魅力，并学习如何构建在各种条件下都表现出色的高效数值工具。", "problem": "考虑标量非线性方程 $f(x)=x-e^{-x}$。目标是设计并实现一种用于求根的混合开放方法。该方法以割线法风格的途径开始，进入收敛盆地，一旦局部导数的大小足够大（由用户指定的阈值确定），便切换到牛顿法风格的更新。从一个基本原理出发：可微函数 $f(x)$ 的近似根可以通过使用局部线性化和有限差分斜率近似来迭代改进。该方法必须满足以下原则：\n\n- 使用 $f(x)$ 的一阶局部线性模型推导出一个寻求 $f(x)=0$ 的更新。\n- 当有两个不同的迭代点可用时，使用有限差分近似来计算斜率。\n- 一旦导数的绝对值 $|f'(x)|$ 超过指定阈值，就使用精确导数以加速收敛。\n\n实现规格：\n\n1. 为 $f(x)=x-e^{-x}$ 实现一个函数，该函数尝试使用混合方法求解 $f(x)=0$，其方法如下：\n   - 基于两个初始猜测值 $x_0$ 和 $x_1$，以割线法风格的更新开始。\n   - 在每次迭代中，生成下一个迭代点 $x_{k+1}$ 后，评估导数的绝对值 $|f'(x_{k+1})|$。如果 $|f'(x_{k+1})|$ 超过阈值 $\\tau$，则将所有后续迭代切换到使用 $f(x)$ 解析导数的牛顿法风格的更新。\n   - 当函数绝对值 $|f(x_k)|$ 小于容差 $\\varepsilon$ 或绝对变化量 $|x_{k}-x_{k-1}|$ 小于 $\\varepsilon$ 时，或者达到最大迭代次数 $N_{\\max}$ 时终止。如果在达到 $N_{\\max}$ 时仍未满足容差，则返回最后一个迭代点。\n\n2. 用于切换决策和牛顿法风格更新的导数必须对给定的 $f(x)$ 进行解析计算。\n\n3. 数值参数：\n   - 使用容差 $\\varepsilon=10^{-12}$。\n   - 每个测试用例使用指定的迭代次数上限 $N_{\\max}$。\n\n4. 输出格式：\n   - 对于每个测试用例，将根的最终近似值作为浮点数返回，并四舍五入到 $10$ 位小数。\n   - 你的程序应生成单行输出，其中包含用逗号分隔并用方括号括起来的结果（例如，$[r_1,r_2,r_3,r_4]$）。\n\n测试套件与覆盖范围：\n\n- 情况 1（正常路径，中等阈值，在收敛盆地附近早期切换）：$x_0=0.0$, $x_1=1.0$, $\\tau=1.4$, $\\varepsilon=10^{-12}$, $N_{\\max}=50$。\n- 情况 2（边界：阈值高，因此从不使用牛顿法风格的更新，仅使用割线法收敛）：$x_0=10.0$, $x_1=12.0$, $\\tau=1.6$, $\\varepsilon=10^{-12}$, $N_{\\max}=100$。\n- 情况 3（边缘：由于早期迭代点的导数值很大，立即切换）：$x_0=-3.0$, $x_1=-2.0$, $\\tau=1.1$, $\\varepsilon=10^{-12}$, $N_{\\max}=50$。\n- 情况 4（边缘：$N_{\\max}$ 非常小，导致提前终止并得到一个粗略的近似值）：$x_0=0.0$, $x_1=2.0$, $\\tau=1.5$, $\\varepsilon=10^{-12}$, $N_{\\max}=2$。\n\n所有答案都是纯数字，没有物理单位或角度。函数 $f(x)$ 是无量纲的。通过仅在满足切换标准时才转为牛顿法风格的更新，并在其他情况下使用最后两个迭代点继续进行割线法风格的更新，来确保在存在小的有限差分分母时的数值稳定性。", "solution": "该问题要求设计并实现一种混合数值方法，用于求解标量非线性方程 $f(x) = x - e^{-x}$ 的根。该方法从割线法开始，并根据特定准则过渡到牛顿法。\n\n所考虑的函数是 $f(x) = x - e^{-x}$。为了实现牛顿法和切换准则，需要 $f(x)$ 的解析导数。导数 $f'(x)$ 的计算如下：\n$$\nf'(x) = \\frac{d}{dx}(x - e^{-x}) = 1 - (-e^{-x}) = 1 + e^{-x}\n$$\n函数 $f(x)$ 对所有 $x \\in \\mathbb{R}$ 都是连续且可微的。由于对所有实数 $x$，都有 $e^{-x} > 0$，因此导数 $f'(x) = 1 + e^{-x} > 1$。这意味着 $f(x)$ 是一个严格单调递增的函数。一个严格单调的函数最多只能穿过 x 轴一次，这保证了方程 $f(x)=0$ 有一个唯一的实根。\n\n该混合算法由两种基本的开放式求根方法构成：割线法和牛顿法。两者都基于寻找函数局部线性近似的根。\n\n割线法使用基于前两个迭代点 $x_k$ 和 $x_{k-1}$ 的后向有限差分来近似导数 $f'(x_k)$：\n$$\nf'(x_k) \\approx \\frac{f(x_k) - f(x_{k-1})}{x_k - x_{k-1}}\n$$\n将此近似值代入牛顿法公式，得到割线法的更新规则：\n$$\nx_{k+1} = x_k - f(x_k) \\frac{x_k - x_{k-1}}{f(x_k) - f(x_{k-1})}\n$$\n该方法表现出超线性收敛性（阶数约为 $1.618$），但不需要计算解析导数。\n\n牛顿法（也称为牛顿-拉夫逊方法）使用函数在当前迭代点 $x_k$ 处的切线来近似下一个迭代点。这是从 $f(x)$ 在 $x_k$ 附近的一阶泰勒级数展开推导出来的：\n$$\nf(x) \\approx f(x_k) + f'(x_k)(x - x_k)\n$$\n通过设 $f(x) = 0$ 并将根的新近似值指定为 $x_{k+1}$，我们求解 $x_{k+1}$：\n$$\n0 = f(x_k) + f'(x_k)(x_{k+1} - x_k) \\implies x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)}\n$$\n只要导数非零且表现良好，该方法在根附近通常表现出二次收敛性。\n\n指定的混合算法综合了这两种方法。它从割线法开始，使用两个初始猜测值 $x_0$ 和 $x_1$。在每次迭代 $k$ 中，计算一个新的迭代点 $x_{k+1}$。然后算法评估此新点处解析导数的绝对值 $|f'(x_{k+1})|$。如果该值超过给定的阈值 $\\tau$，算法将永久切换到牛顿法进行所有后续迭代。该策略利用计算成本较低的割线法来接近收敛盆地，一旦函数的局部几何形状（由导数的幅度表示）被认为是合适的，就切换到收敛更快的牛顿法。\n\n该算法的实现维护一个状态变量，例如一个布尔标志 `use_newton`，初始化为 false。迭代过程如下：\n1. 用 $x_{k-1} = x_0$ 和 $x_k = x_1$ 进行初始化。\n2. 在每次迭代中，根据步长检查终止条件：$|x_k - x_{k-1}|  \\varepsilon$。\n3. 如果 `use_newton` 为 false，则使用割线法规则计算下一个迭代点 $x_{k+1}$。否则，使用牛顿法规则计算 $x_{k+1}$。\n4. 更新存储的迭代点：$x_{k-1} \\leftarrow x_k$ 和 $x_k \\leftarrow x_{k+1}$。\n5. 根据函数值检查终止条件：$|f(x_k)|  \\varepsilon$。\n6. 如果 `use_newton` 当前为 false，则评估切换准则：如果 $|f'(x_k)| > \\tau$，则将 `use_newton` 设置为 true 用于所有未来的迭代。\n7. 此过程持续进行，直到满足其中一个终止条件或达到最大迭代次数 $N_{\\max}$。如果达到 $N_{\\max}$，则返回最后计算出的迭代点。\n\n此设计为求根提供了一个鲁棒的程序，其测试用例旨在检验算法的各种逻辑路径：标准切换、仅割线法路径、立即切换和提前终止。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n    \n    def f(x: float) - float:\n        \"\"\"The scalar nonlinear function f(x) = x - e^(-x).\"\"\"\n        return x - np.exp(-x)\n\n    def f_prime(x: float) - float:\n        \"\"\"The analytical derivative f'(x) = 1 + e^(-x).\"\"\"\n        return 1.0 + np.exp(-x)\n\n    def hybrid_solver(x0: float, x1: float, tau: float, epsilon: float, n_max: int) - float:\n        \"\"\"\n        Implements the hybrid secant-Newton root-finding method.\n\n        Args:\n            x0 (float): The first initial guess.\n            x1 (float): The second initial guess.\n            tau (float): The derivative magnitude threshold for switching to Newton's method.\n            epsilon (float): The tolerance for termination.\n            n_max (int): The maximum number of iterations.\n\n        Returns:\n            float: The final approximation of the root.\n        \"\"\"\n        xk_minus_1 = float(x0)\n        xk = float(x1)\n        use_newton = False\n\n        # Pre-check for convergence on the second initial guess\n        if abs(f(xk))  epsilon:\n            return xk\n\n        for _ in range(n_max):\n            # Termination condition 1: absolute change in iterates\n            if abs(xk - xk_minus_1)  epsilon:\n                return xk\n\n            # Select method and compute next iterate\n            if use_newton:\n                # Newton-Raphson update\n                fxk = f(xk)\n                fpxk = f_prime(xk)\n                # The derivative f'(x) for this function is always = 1, so no division by zero\n                xk_plus_1 = xk - fxk / fpxk\n            else:\n                # Secant method update\n                fxk = f(xk)\n                fxk_minus_1 = f(xk_minus_1)\n                denominator = fxk - fxk_minus_1\n                # Check for stagnation, which would cause division by zero.\n                if denominator == 0:\n                    return xk\n                xk_plus_1 = xk - fxk * (xk - xk_minus_1) / denominator\n            \n            # Update iterates for the next step\n            xk_minus_1 = xk\n            xk = xk_plus_1\n            \n            # Termination condition 2: absolute function value\n            if abs(f(xk))  epsilon:\n                return xk\n                \n            # Switching condition: check if derivative magnitude exceeds threshold\n            if not use_newton:\n                if abs(f_prime(xk)) > tau:\n                    use_newton = True\n                    \n        # Termination condition 3: maximum iterations reached\n        return xk\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 1.4, 1e-12, 50),     # Case 1\n        (10.0, 12.0, 1.6, 1e-12, 100),   # Case 2\n        (-3.0, -2.0, 1.1, 1e-12, 50),   # Case 3\n        (0.0, 2.0, 1.5, 1e-12, 2)       # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        x0, x1, tau, epsilon, n_max = case\n        root = hybrid_solver(x0, x1, tau, epsilon, n_max)\n        # Round the result to 10 decimal places as specified\n        results.append(round(root, 10))\n\n    # Final print statement in the exact required format.\n    # Using f-string formatting to avoid trailing zeros for integers.\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3167328"}]}