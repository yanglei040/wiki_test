{"hands_on_practices": [{"introduction": "约束不仅仅是限制，它还能从根本上改变解的性质。我们将通过一个有趣的例子来开启我们的实践之旅：一个在无约束情况下既非最大值也非最小值的鞍点，在被限制在特定路径上时，如何转变为一个真正的局部最小值。这个练习将阐明可行流形和切空间在确定最优点中的关键作用，是理解约束优化核心思想的绝佳起点。[@problem_id:3201338]", "problem": "考虑无约束目标函数 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$，其定义为 $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$。施加等式约束 $g(x_{1},x_{2})=x_{2}=0$，因此可行流形为 $\\mathcal{M}=\\{(x_{1},x_{2})\\in\\mathbb{R}^{2}\\mid x_{2}=0\\}$。使用多元微积分的基本定义和等式约束优化的 Karush–Kuhn–Tucker (KKT) 条件，完成以下任务：\n\n1. 找出 $f$ 的无约束驻点，并使用海森矩阵的局部曲率定义对点 $(0,0)$ 进行分类。解释为什么它是 $f$ 在 $\\mathbb{R}^{2}$ 上的一个鞍点。\n\n2. 建立等式约束问题 $\\min f(x_{1},x_{2})$，约束条件为 $g(x_{1},x_{2})=0$，定义拉格朗日函数 $\\mathcal{L}(x_{1},x_{2},\\lambda)=f(x_{1},x_{2})+\\lambda g(x_{1},x_{2})$，并通过一阶 KKT 条件确定候选的约束最优点 $(x^{\\star},\\lambda^{\\star})$。\n\n3. 计算在 $x^{\\star}$ 处的切空间，其定义为 $T_{x^{\\star}}\\mathcal{M}=\\{v\\in\\mathbb{R}^{2}\\mid \\nabla g(x^{\\star})^{\\top}v=0\\}$，并通过在 $T_{x^{\\star}}\\mathcal{M}$ 上计算二次型 $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 来验证局部最小值的二阶条件。\n\n4. 最后，计算受限二次型 $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 在 $T_{x^{\\star}}\\mathcal{M}$ 中所有单位向量 $v$ 上的最小值，即：\n$$\n\\min\\left\\{v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v \\,\\middle|\\, v\\in T_{x^{\\star}}\\mathcal{M},\\, \\|v\\|_{2}=1\\right\\}.\n$$\n将你的最终答案表示为一个精确的实数。无需四舍五入。", "solution": "用户提供了一个关于无约束和约束优化的多部分问题。我将首先验证问题陈述，如果问题有效，则继续进行分步解答。\n\n### 问题验证\n\n**第1步：提取已知条件**\n- 目标函数：$f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$，其中 $f:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$。\n- 等式约束：$g(x_{1},x_{2})=x_{2}=0$。\n- 可行流形：$\\mathcal{M}=\\{(x_{1},x_{2})\\in\\mathbb{R}^{2}\\mid x_{2}=0\\}$。\n- 拉格朗日函数定义：$\\mathcal{L}(x_{1},x_{2},\\lambda)=f(x_{1},x_{2})+\\lambda g(x_{1},x_{2})$。\n- $x^{\\star}$ 处的切空间定义：$T_{x^{\\star}}\\mathcal{M}=\\{v\\in\\mathbb{R}^{2}\\mid \\nabla g(x^{\\star})^{\\top}v=0\\}$。\n- 任务1：找出 $f$ 的无约束驻点，并使用其海森矩阵对点 $(0,0)$ 进行分类。\n- 任务2：建立约束问题，并使用一阶 KKT 条件找到候选最优点 $(x^{\\star},\\lambda^{\\star})$。\n- 任务3：计算切空间 $T_{x^{\\star}}\\mathcal{M}$，并使用 $v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 验证局部最小值的二阶条件。\n- 任务4：计算 $\\min\\left\\{v^{\\top}\\nabla^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v \\,\\middle|\\, v\\in T_{x^{\\star}}\\mathcal{M},\\, \\|v\\|_{2}=1\\right\\}$。\n\n**第2步：使用提取的已知条件进行验证**\n- **科学上成立：** 该问题是多元微积分和优化理论中的一个标准练习。所有定义（拉格朗日函数、KKT 条件、海森矩阵、切空间）都是标准的且在数学上是正确的。\n- **适定性：** 问题陈述清晰，包含一系列逻辑任务。函数是良态的，确保每个部分都存在唯一解。\n- **客观性：** 语言形式化、精确，无任何主观或模糊的术语。\n- **完整性与一致性：** 所有必要的信息，包括函数和定义，都已提供。问题是自洽且无矛盾的。\n\n**第3步：结论与行动**\n问题有效。这是一个适定的、科学上成立的约束优化练习。我将继续进行解答。\n\n### 解答\n\n**第1部分：无约束分析**\n\n首先，我们通过找到其梯度为零的点来求出无约束目标函数 $f(x_{1},x_{2})=x_{1}^{2}-x_{2}^{2}$ 的驻点。$f$ 的梯度为：\n$$\n\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_{1}} \\\\ \\frac{\\partial f}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} \\\\ -2x_{2} \\end{pmatrix}\n$$\n将梯度设为零向量 $\\nabla f(x_{1},x_{2}) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，得到方程组：\n$$\n2x_{1} = 0 \\implies x_{1} = 0\n$$\n$$\n-2x_{2} = 0 \\implies x_{2} = 0\n$$\n因此，唯一的无约束驻点是 $(0,0)$。\n\n接下来，我们通过检查描述局部曲率的 $f$ 的海森矩阵来对此点进行分类。海森矩阵 $\\nabla^{2}f$ 是二阶偏导数矩阵：\n$$\n\\nabla^{2}f(x_{1},x_{2}) = \\begin{pmatrix} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}}  \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{1}}  \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix}\n$$\n海森矩阵是一个常数矩阵。在驻点 $(0,0)$ 处，我们有 $\\nabla^{2}f(0,0) = \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix}$。为了对该点进行分类，我们通过求其特征值来检查该矩阵的定性。特征值是其对角线元素，$\\lambda_{1}=2$ 和 $\\lambda_{2}=-2$。由于一个特征值为正，一个为负，该海森矩阵是不定的。驻点处不定的海森矩阵表明该点是一个鞍点。该函数沿 $x_1$ 轴具有正曲率（向上凹），沿 $x_2$ 轴具有负曲率（向下凹）。\n\n**第2部分：约束优化与 KKT 条件**\n\n我们考虑在等式约束 $g(x_{1},x_{2})=x_{2}=0$ 下最小化 $f(x_{1},x_{2})$ 的问题。该问题的拉格朗日函数为：\n$$\n\\mathcal{L}(x_{1},x_{2},\\lambda) = f(x_{1},x_{2}) + \\lambda g(x_{1},x_{2}) = x_{1}^{2}-x_{2}^{2} + \\lambda x_{2}\n$$\n一阶 Karush–Kuhn–Tucker (KKT) 条件要求拉格朗日函数对所有变量的梯度为零。\n$$\n\\nabla\\mathcal{L}(x_{1},x_{2},\\lambda) = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial x_{1}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial x_{2}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} 2x_{1} \\\\ -2x_{2}+\\lambda \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这给了我们一个三方程的方程组：\n1. $2x_{1} = 0 \\implies x_{1} = 0$\n2. $-2x_{2}+\\lambda = 0$\n3. $x_{2} = 0$\n\n从方程(1)可知，$x_{1}=0$。从方程(3)可知，$x_{2}=0$。将 $x_{2}=0$ 代入方程(2)得到 $-2(0)+\\lambda=0$，这意味着 $\\lambda=0$。\n候选的约束最优点是 $x^{\\star}=(x_{1}^{\\star},x_{2}^{\\star})=(0,0)$，对应的拉格朗日乘子为 $\\lambda^{\\star}=0$。\n\n**第3部分：二阶条件与切空间**\n\n现在我们验证局部最小值的二阶条件。首先，我们确定在点 $x^{\\star}=(0,0)$ 处的切空间 $T_{x^{\\star}}\\mathcal{M}$。约束函数为 $g(x_{1},x_{2})=x_{2}$。其梯度为：\n$$\n\\nabla g(x_{1},x_{2}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\n这个梯度是恒定的。在 $x^{\\star}=(0,0)$ 处，我们有 $\\nabla g(x^{\\star}) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。切空间是所有与 $\\nabla g(x^{\\star})$ 正交的向量 $v=(v_{1}, v_{2})^{\\top} \\in \\mathbb{R}^{2}$ 的集合：\n$$\nT_{x^{\\star}}\\mathcal{M} = \\{ v \\in \\mathbb{R}^{2} \\mid \\nabla g(x^{\\star})^{\\top}v = 0 \\}\n$$\n$$\n\\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} = 0 \\cdot v_{1} + 1 \\cdot v_{2} = v_{2} = 0\n$$\n因此，切空间是所有形式为 $v=(v_{1}, 0)^{\\top}$ 的向量的集合，即 $x_{1}$ 轴。$T_{x^{\\star}}\\mathcal{M} = \\text{span}\\left\\{\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\right\\}$。\n\n接下来，我们计算拉格朗日函数关于变量 $x=(x_{1},x_{2})$ 的海森矩阵：\n$$\n\\nabla_{x}^{2}\\mathcal{L}(x_{1},x_{2},\\lambda) = \\begin{pmatrix} \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{1}^{2}}  \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{1}\\partial x_{2}} \\\\ \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{2}\\partial x_{1}}  \\frac{\\partial^{2} \\mathcal{L}}{\\partial x_{2}^{2}} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix}\n$$\n我们在候选点 $(x^{\\star}, \\lambda^{\\star})=((0,0),0)$ 处计算该海森矩阵。由于海森矩阵是常数矩阵，$\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star}) = \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix}$。\n\n严格局部最小值的二阶充分条件要求对于所有非零向量 $v \\in T_{x^{\\star}}\\mathcal{M}$，二次型 $v^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 是正定的。令 $v \\in T_{x^{\\star}}\\mathcal{M}$ 为一个非零向量，因此 $v=(v_{1},0)^{\\top}$ 且 $v_{1} \\neq 0$。\n$$\nv^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v = \\begin{pmatrix} v_{1}  0 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  -2 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2v_{1}  0 \\end{pmatrix} \\begin{pmatrix} v_{1} \\\\ 0 \\end{pmatrix} = 2v_{1}^{2}\n$$\n因为 $v_{1} \\neq 0$，所以 $v_{1}^{2}0$，因此 $2v_{1}^{2}0$。该条件得到满足，证实了 $x^{\\star}=(0,0)$ 是约束问题的一个严格局部最小值。\n\n**第4部分：最终计算**\n\n我们需要计算二次型 $v^{\\top}\\nabla_{x}^{2}\\mathcal{L}(x^{\\star},\\lambda^{\\star})v$ 在切空间 $T_{x^{\\star}}\\mathcal{M}$ 中所有单位向量 $v$ 上的最小值。这类向量的集合是：\n$$\n\\{v \\in T_{x^{\\star}}\\mathcal{M} \\mid \\|v\\|_{2}=1 \\}\n$$\n切空间中的向量 $v$ 的形式为 $v=(v_{1},0)^{\\top}$。单位范数条件意味着 $\\|v\\|_{2} = \\sqrt{v_{1}^{2}+0^{2}} = |v_{1}| = 1$。这意味着 $v_{1}=1$ 或 $v_{1}=-1$。所以存在两个这样的向量：$v^{(1)}=(1,0)^{\\top}$ 和 $v^{(2)}=(-1,0)^{\\top}$。\n\n从第3部分可知，二次型的值为 $2v_{1}^{2}$。我们必须在条件 $|v_{1}|=1$ 下找到它的最小值。\n如果 $v_{1}=1$，其值为 $2(1)^{2}=2$。\n如果 $v_{1}=-1$，其值为 $2(-1)^{2}=2$。\n在两种情况下，值都是2。因此，受限二次型的最小值（也是最大值）是2。\n这个值代表了目标函数在约束最小值点处沿可行方向的曲率。", "answer": "$$\n\\boxed{2}\n$$", "id": "3201338"}, {"introduction": "了解了约束如何改变问题后，我们现在来探索如何用数值方法求解。本练习将聚焦于等式约束，比较两种相关但截然不同的方法：直接的二次惩罚方法和更精妙的增广拉格朗日方法。通过为一个最小二乘问题实现这两种算法，你将发现为何仅仅惩罚约束违反通常是不够的，以及动态更新拉格朗日乘子如何能带来更准确、更稳健的解。[@problem_id:3201293]", "problem": "您需要实现并比较两种无约束优化方法，用于解决一个二次等式约束最小二乘问题，且仅使用线性代数和迭代更新。目的是检验在固定等式约束下，增广拉格朗日乘子更新与纯二次惩罚方法的效果对比，从而从第一性原理探讨无约束优化与约束优化之间的关系。\n\n使用的基本依据和定义如下：\n- 可微函数的无约束最小值出现在其梯度为零的点。\n- 受等式约束的可微函数的约束最小值由 Karush-Kuhn-Tucker (KKT) 条件描述，该条件通过构造拉格朗日函数并将其关于决策变量和乘子的梯度均设为零而导出。\n- 欧几里得范数由 $\\|\\cdot\\|_2$ 表示。\n- 对于矩阵 $A$ 和向量 $b$，最小二乘目标可以写作 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$。\n- 对于矩阵 $C$ 和向量 $d$，等式约束可以写作 $h(x) = C x - d = 0$。\n- 等式约束的增广拉格朗日函数为 $\\mathcal{L}_\\rho(x,\\lambda) = f(x) + \\lambda^\\top h(x) + \\tfrac{\\rho}{2}\\|h(x)\\|_2^2$，其中 $\\lambda$ 是拉格朗日乘子向量，$\\rho$ 是一个正的惩罚参数。\n\n您的程序必须：\n- 实现增广拉格朗日方法：对于固定的 $\\lambda$，交替地最小化关于 $x$ 的 $\\mathcal{L}_\\rho(x,\\lambda)$，然后使用基于当前约束残差的标准乘子更新规则更新 $\\lambda$。执行固定次数的迭代，并报告最终的约束违反度 $\\|h(x)\\|_2$。\n- 实现纯二次惩罚方法：通过最小化 $f(x) + \\tfrac{\\rho}{2}\\|h(x)\\|_2^2$ 来求解，不使用乘子。在一个测试中，使用单个 $\\rho$ 值；在另一个测试中，使用一个递增的 $\\rho$ 值序列以模拟约束的收紧。报告最终的约束违反度 $\\|h(x)\\|_2$。\n\n科学真实性条件：\n- 对二次子问题使用精确的线性代数求解。\n- 乘子的迭代更新必须基于当前的约束残差。\n- 除上述基本定义外，不使用任何步长启发式方法或捷径。\n\n角度单位和物理单位：不涉及物理量或角度，因此不需要单位或角度规格。\n\n测试套件：\n- 所有问题都在 $\\mathbb{R}^n$ 中，其中 $n=2$，等式约束的维度 $m \\in \\{1,2\\}$。\n- 对每种情况，报告两个量：最终的增广拉格朗日约束违反度和最终的纯惩罚约束违反度。这些必须是浮点数。\n\n情况 $\\mathbf{1}$ (理想路径，良态数据)：\n- $A = \\begin{bmatrix} 2  -1 \\\\ 0  3 \\\\ 1  1 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 4 \\\\ 2 \\end{bmatrix}$。\n- $C = \\begin{bmatrix} 1  2 \\end{bmatrix}$，$d = \\begin{bmatrix} 3 \\end{bmatrix}$。\n- 增广拉格朗日参数：$\\rho = 1.0$，迭代次数 $K = 10$，初始乘子 $\\lambda^{(0)} = \\begin{bmatrix} 0 \\end{bmatrix}$。\n- 纯惩罚参数：单个 $\\rho = 1.0$。\n\n情况 $\\mathbf{2}$ (边界条件：弱惩罚，轻度病态的 $A^\\top A$)：\n- $A = \\begin{bmatrix} 10^{-3}  0 \\\\ 0  1 \\\\ 1  -1 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0.5 \\end{bmatrix}$。\n- $C = \\begin{bmatrix} 1  0 \\end{bmatrix}$，$d = \\begin{bmatrix} 0.2 \\end{bmatrix}$。\n- 增广拉格朗日参数：$\\rho = 10^{-2}$，迭代次数 $K = 50$，初始乘子 $\\lambda^{(0)} = \\begin{bmatrix} 0 \\end{bmatrix}$。\n- 纯惩罚参数：单个 $\\rho = 10^{-2}$。\n\n情况 $\\mathbf{3}$ (边缘情况：冗余等式约束)：\n- $A = \\begin{bmatrix} 1  2 \\\\ 3  4 \\\\ 5  6 \\end{bmatrix}$，$b = \\begin{bmatrix} 7 \\\\ 8 \\\\ 9 \\end{bmatrix}$。\n- $C = \\begin{bmatrix} 1  1 \\\\ 2  2 \\end{bmatrix}$，$d = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 增广拉格朗日参数：$\\rho = 1.0$，迭代次数 $K = 15$，初始乘子 $\\lambda^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$。\n- 纯惩罚参数：递增序列 $\\rho \\in \\{10^{-1}, 1.0, 10.0\\}$。\n\n算法要求：\n- 对于增广拉格朗日方法，在每次迭代 $k \\in \\{0,1,\\dots,K-1\\}$ 中：计算 $\\mathcal{L}_\\rho(x,\\lambda^{(k)})$ 关于 $x$ 的精确最小化子，然后通过加上 $\\rho$ 乘以当前约束残差来更新乘子。在 $K$ 次迭代后，报告 $\\|h(x^{(K)})\\|_2$。\n- 对于纯惩罚方法，为给定的 $\\rho$ 计算精确最小化子（或者对于序列中的每个 $\\rho$ 顺序计算），并在最后一次求解后报告最终的 $\\|h(x)\\|_2$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个包含两个浮点数的列表，顺序为 $\\left[\\text{增广拉格朗日方法所得的 } \\|h(x)\\|_2, \\text{纯惩罚方法所得的 } \\|h(x)\\|_2\\right]$。因此，最终输出必须是 $\\big[\\,[a_1,b_1],[a_2,b_2],[a_3,b_3]\\,\\big]$ 的形式，其中每个 $a_i$ 和 $b_i$ 分别是对应于情况 $\\mathbf{1}$、$\\mathbf{2}$ 和 $\\mathbf{3}$ 的浮点数。", "solution": "该问题是有效的。它提出了计算科学和优化领域中的一个良定任务，该任务基于已建立的数学原理。问题是内容完整的，提供了所有必要的数据和定义。它是客观的、科学上合理的，并且没有矛盾或歧义。\n\n问题的核心是找到等式约束二次规划 (ECQP) 的最小化子 $x \\in \\mathbb{R}^n$：\n$$\n\\min_{x} f(x) = \\frac{1}{2}\\|A x - b\\|_2^2 \\quad \\text{subject to} \\quad h(x) = C x - d = 0\n$$\n其中 $A \\in \\mathbb{R}^{p \\times n}$, $b \\in \\mathbb{R}^p$, $C \\in \\mathbb{R}^{m \\times n}$，$d \\in \\mathbb{R}^m$。我们将实现并比较两种迭代方法，通过解决一系列无约束二次子问题来求解此问题。\n\n首先，我们展开目标函数及其梯度：\n$f(x) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)$\n关于 $x$ 的梯度是：\n$\\nabla_x f(x) = A^\\top A x - A^\\top b$。\n\n我们还需要约束函数范数平方的梯度：\n$\\frac{1}{2}\\|h(x)\\|_2^2 = \\frac{1}{2}\\|Cx - d\\|_2^2 = \\frac{1}{2}(x^\\top C^\\top C x - 2 d^\\top C x + d^\\top d)$\n该项的梯度是：\n$\\nabla_x \\left(\\frac{1}{2}\\|h(x)\\|_2^2\\right) = C^\\top(Cx-d) = C^\\top C x - C^\\top d$。\n\n**1. 增广拉格朗日方法**\n\n此问题的增广拉格朗日函数由下式给出：\n$$\n\\mathcal{L}_\\rho(x, \\lambda) = f(x) + \\lambda^\\top h(x) + \\frac{\\rho}{2}\\|h(x)\\|_2^2\n$$\n其中 $\\lambda \\in \\mathbb{R}^m$ 是拉格朗日乘子向量，$\\rho  0$ 是惩罚参数。\n\n增广拉格朗日方法是一个迭代过程。在每次迭代 $k$ 中，对于一个固定的乘子 $\\lambda^{(k)}$，我们通过最小化关于 $x$ 的 $\\mathcal{L}_\\rho(x, \\lambda^{(k)})$ 来找到下一个迭代点 $x^{(k+1)}$。由于 $\\mathcal{L}_\\rho(x, \\lambda^{(k)})$ 是 $x$ 的二次函数，其最小值在关于 $x$ 的梯度为零处取得：\n$$\n\\nabla_x \\mathcal{L}_\\rho(x, \\lambda^{(k)}) = \\nabla_x f(x) + \\nabla_x(\\lambda^{(k)\\top}(Cx-d)) + \\nabla_x\\left(\\frac{\\rho}{2}\\|Cx - d\\|_2^2\\right) = 0\n$$\n代入梯度表达式：\n$$\n(A^\\top A x - A^\\top b) + C^\\top \\lambda^{(k)} + \\rho (C^\\top C x - C^\\top d) = 0\n$$\n为了求解 $x$，我们合并包含 $x$ 的项：\n$$\n(A^\\top A + \\rho C^\\top C) x = A^\\top b - C^\\top \\lambda^{(k)} + \\rho C^\\top d\n$$\n这是一个形如 $H x = g$ 的线性系统，其中：\n- $H = A^\\top A + \\rho C^\\top C$\n- $g = A^\\top b + C^\\top(\\rho d - \\lambda^{(k)})$\n\n算法按以下步骤进行，其中 $k = 0, 1, \\dots, K-1$：\n1.  求解 $x^{(k+1)}$: $x^{(k+1)} = (A^\\top A + \\rho C^\\top C)^{-1} (A^\\top b + C^\\top(\\rho d - \\lambda^{(k)}))$。\n2.  更新拉格朗日乘子: $\\lambda^{(k+1)} = \\lambda^{(k)} + \\rho h(x^{(k+1)}) = \\lambda^{(k)} + \\rho(C x^{(k+1)} - d)$。\n\n经过 $K$ 次迭代后，最终的约束违反度计算为 $\\|h(x^{(K)})\\|_2 = \\|C x^{(K)} - d\\|_2$。\n\n**2. 纯二次惩罚方法**\n\n纯二次惩罚方法旨在通过最小化无约束惩罚函数来解决约束问题：\n$$\nP_\\rho(x) = f(x) + \\frac{\\rho}{2}\\|h(x)\\|_2^2\n$$\n该函数同样是关于 $x$ 的二次函数。其最小值通过将其梯度设为零来找到：\n$$\n\\nabla_x P_\\rho(x) = \\nabla_x f(x) + \\rho \\nabla_x\\left(\\frac{1}{2}\\|h(x)\\|_2^2\\right) = 0\n$$\n代入梯度表达式：\n$$\n(A^\\top A x - A^\\top b) + \\rho (C^\\top C x - C^\\top d) = 0\n$$\n再次，我们合并包含 $x$ 的项以形成一个线性系统：\n$$\n(A^\\top A + \\rho C^\\top C) x = A^\\top b + \\rho C^\\top d\n$$\n这是一个形如 $H x = g'$ 的线性系统，其中：\n- $H = A^\\top A + \\rho C^\\top C$\n- $g' = A^\\top b + \\rho C^\\top d$\n\n这对应于增广拉格朗日方法中将 $\\lambda$ 固定为 $0$ 的单步计算。对于测试用例，我们为给定的 $\\rho$ 求解此系统一次。对于具有 $\\rho$ 值序列的情况，我们根据问题规定，使用序列中的最终值求解该系统。最终的约束违反度即为 $\\|h(x)\\|_2 = \\|C x - d\\|_2$。\n\n实现将遵循这些推导出的方程，使用 `numpy.linalg.solve` 对子问题进行精确的线性代数求解。", "answer": "```python\nimport numpy as np\n\ndef run_augmented_lagrangian(A, b, C, d, rho, K, lambda0):\n    \"\"\"\n    Implements the Augmented Lagrangian method for a quadratic equality-constrained LS problem.\n\n    Args:\n        A (np.ndarray): Matrix for the LS objective.\n        b (np.ndarray): Vector for the LS objective.\n        C (np.ndarray): Matrix for the equality constraint.\n        d (np.ndarray): Vector for the equality constraint.\n        rho (float): Penalty parameter.\n        K (int): Number of iterations.\n        lambda0 (np.ndarray): Initial Lagrange multipliers.\n\n    Returns:\n        float: The final constraint violation norm ||Cx - d||_2.\n    \"\"\"\n    AtA = A.T @ A\n    Atb = A.T @ b\n    CtC = C.T @ C\n    Ct = C.T\n\n    # The Hessian of the Lagrangian is constant for all iterations\n    H = AtA + rho * CtC\n\n    lambda_k = lambda0.copy()\n    x_k = np.zeros(A.shape[1]) # Initialize x\n\n    for _ in range(K):\n        # Form the right-hand side vector g\n        g = Atb + Ct @ (rho * d - lambda_k)\n        \n        # Solve the linear system for x_{k+1}\n        x_k = np.linalg.solve(H, g)\n        \n        # Update the Lagrange multiplier\n        constraint_residual = C @ x_k - d\n        lambda_k = lambda_k + rho * constraint_residual\n    \n    final_constraint_violation = np.linalg.norm(C @ x_k - d)\n    return final_constraint_violation\n\ndef run_penalty_method(A, b, C, d, rho_values):\n    \"\"\"\n    Implements the pure quadratic penalty method.\n\n    Args:\n        A (np.ndarray): Matrix for the LS objective.\n        b (np.ndarray): Vector for the LS objective.\n        C (np.ndarray): Matrix for the equality constraint.\n        d (np.ndarray): Vector for the equality constraint.\n        rho_values (list or tuple): A single rho or a schedule of rho values.\n\n    Returns:\n        float: The final constraint violation norm ||Cx - d||_2.\n    \"\"\"\n    # Use the final rho from the schedule as per problem description\n    rho = rho_values[-1]\n\n    AtA = A.T @ A\n    Atb = A.T @ b\n    CtC = C.T @ C\n    Ctd = C.T @ d\n    \n    H = AtA + rho * CtC\n    g_prime = Atb + rho * Ctd\n    \n    x = np.linalg.solve(H, g_prime)\n    \n    constraint_violation = np.linalg.norm(C @ x - d)\n    return constraint_violation\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the optimization algorithms.\n    \"\"\"\n    test_cases = [\n        {\n            # Case 1 (happy path, well-conditioned data)\n            \"A\": np.array([[2., -1.], [0., 3.], [1., 1.]]),\n            \"b\": np.array([1., 4., 2.]),\n            \"C\": np.array([[1., 2.]]),\n            \"d\": np.array([3.]),\n            \"alm_params\": {\"rho\": 1.0, \"K\": 10, \"lambda0\": np.array([0.])},\n            \"penalty_params\": {\"rho_values\": [1.0]}\n        },\n        {\n            # Case 2 (boundary condition: weak penalty, mildly ill-conditioned A^T A)\n            \"A\": np.array([[1e-3, 0.], [0., 1.], [1., -1.]]),\n            \"b\": np.array([0., 1., 0.5]),\n            \"C\": np.array([[1., 0.]]),\n            \"d\": np.array([0.2]),\n            \"alm_params\": {\"rho\": 1e-2, \"K\": 50, \"lambda0\": np.array([0.])},\n            \"penalty_params\": {\"rho_values\": [1e-2]}\n        },\n        {\n            # Case 3 (edge case: redundant equality constraints)\n            \"A\": np.array([[1., 2.], [3., 4.], [5., 6.]]),\n            \"b\": np.array([7., 8., 9.]),\n            \"C\": np.array([[1., 1.], [2., 2.]]),\n            \"d\": np.array([0., 0.]),\n            \"alm_params\": {\"rho\": 1.0, \"K\": 15, \"lambda0\": np.array([0., 0.])},\n            \"penalty_params\": {\"rho_values\": [1e-1, 1.0, 10.0]}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, b, C, d = case[\"A\"], case[\"b\"], case[\"C\"], case[\"d\"]\n        \n        # Run Augmented Lagrangian Method\n        alm_params = case[\"alm_params\"]\n        alm_violation = run_augmented_lagrangian(A, b, C, d, **alm_params)\n        \n        # Run Pure Penalty Method\n        penalty_params = case[\"penalty_params\"]\n        penalty_violation = run_penalty_method(A, b, C, d, **penalty_params)\n        \n        results.append(f\"[{alm_violation},{penalty_violation}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3201293"}, {"introduction": "真实世界的问题常常涉及由不等式定义的复杂、甚至非凸的可行域。最后的这个实践将对比处理此类约束的两种强大策略：创建无约束代理问题的惩罚方法，以及强制迭代保持可行性的投影梯度法。你将在挑战性的非凸集合上实现这些方法，并亲眼观察它们的迥异行为，包括惩罚方法陷入不可行局部最小值的可能性。[@problem_id:3201335]", "problem": "您需要实现并比较两种迭代方法，用于解决一个二维约束优化问题，其中可行集是非凸的但具有光滑边界。目标是识别无约束代理何时可能存在不可行的局部极小值点，并验证每步梯度下降后进行正交投影能够强制满足可行性。基本设置为一个光滑的目标函数、光滑的不等式约束和标准的欧几里得几何。\n\n用作基础的基本定义：\n- 一个约束优化问题定义为在由光滑不等式约束 $c_i(\\mathbf{x}) \\le 0$（其中 $i$ 属于一个有限索引集）定义的可行集 $\\mathcal{F}$ 上最小化一个光滑目标函数 $f(\\mathbf{x})$。\n- 一个点 $\\mathbf{x}$ 是可行的，当且仅当所有不等式都满足 $c_i(\\mathbf{x}) \\le 0$。\n- 一个点 $\\mathbf{y}$ 在一个闭集 $\\mathcal{F}$ 上的正交投影是 $\\mathcal{F}$ 中的一个点 $\\Pi_{\\mathcal{F}}(\\mathbf{y})$，它在所有 $\\mathbf{x}\\in\\mathcal{F}$ 中最小化欧几里得距离 $\\|\\mathbf{x}-\\mathbf{y}\\|$。\n\n您的程序必须实现并比较：\n- 一种方法，在每一步对 $f(\\mathbf{x})$ 进行梯度下降，然后正交投影到 $\\mathcal{F}$ 上。您必须从第一性原理出发，为下面给出的特定可行集证明投影的形式。\n- 一种方法，用一个在违约量上具有光滑二次增长的无约束代理替换约束问题，并对该代理应用梯度下降。您必须从第一性原理出发，证明代理的构造。\n\n使用以下两个可行集，两者均为非凸且具有光滑边界：\n- A类情况（环形区域）：$\\mathcal{F} = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid r_{\\mathrm{in}}^2 \\le \\|\\mathbf{x}\\|^2 \\le R_{\\mathrm{out}}^2\\}$，具有两个光滑不等式约束 $c_1(\\mathbf{x}) = r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2 \\le 0$ 和 $c_2(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$。此集合是非凸的，因为它包含一个孔。\n- B类情况（两个圆盘的并集）：$\\mathcal{F} = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid (\\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2) \\le 0\\}$，是两个以 $\\mathbf{c}_1$ 和 $\\mathbf{c}_2$ 为中心、半径为 $r$ 的不相交闭圆盘的并集。边界是光滑的（不相交的圆），且集合是非凸的（不连通分量）。\n\n目标函数是光滑函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$。\n\n实现以下两种算法：\n- 投影梯度下降（Projected Gradient Descent）：迭代 $\\mathbf{y}_{k} = \\mathbf{x}_{k} - \\alpha \\nabla f(\\mathbf{x}_{k})$，然后 $\\mathbf{x}_{k+1} = \\Pi_{\\mathcal{F}}(\\mathbf{y}_{k})$，其中 $\\alpha  0$ 是一个固定的步长。对于到环形区域的投影，需从正交投影的定义出发证明径向裁剪的合理性。对于到两个圆盘并集的投影，需证明如果 $\\mathbf{y}$ 不可行，其正交投影等于到其中一个圆盘的最近投影；如果 $\\mathbf{y}$ 已经可行，则投影等于 $\\mathbf{y}$ 本身。\n- 二次惩罚梯度下降（Quadratic Penalty Gradient Descent）：构建一个无约束代理 $F_{\\mu}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sum_i \\phi(c_i(\\mathbf{x}))$，其中 $\\phi(t)$ 是一个光滑函数，当 $t  0$ 时具有二次增长，当 $t \\le 0$ 时为零，$\\mu  0$ 是一个罚参数。对于环形区域，使用两个不等式惩罚。对于圆盘的并集，对乘积约束使用一个不等式惩罚。应用梯度下降 $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\beta \\nabla F_{\\mu}(\\mathbf{x}_k)$，其中 $\\beta  0$ 为固定步长。\n\n测试套件：\n- 测试 $1$（环形区域，不可行的无约束极小值点）：$r_{\\mathrm{in}} = 1$，$R_{\\mathrm{out}} = 2$，初始点 $\\mathbf{x}_0 = (0.3, 0.4)$，投影步长 $\\alpha = 0.2$，惩罚步长 $\\beta = 0.05$，罚参数 $\\mu = 0.5$，迭代次数 $N = 300$。\n- 测试 $2$（两个圆盘的并集，不可行的无约束极小值点）：圆盘半径 $r = 1$，中心 $\\mathbf{c}_1 = (1.5, 0)$ 和 $\\mathbf{c}_2 = (-1.5, 0)$，初始点 $\\mathbf{x}_0 = (0, 0)$，$\\alpha = 0.2$，$\\beta = 0.05$，$\\mu = 0.1$，迭代次数 $N = 300$。\n- 测试 $3$（环形区域，边界起始，强惩罚）：$r_{\\mathrm{in}} = 1$，$R_{\\mathrm{out}} = 2$，初始点 $\\mathbf{x}_0 = (1.0, 0.0)$，$\\alpha = 0.2$，$\\beta = 0.05$，$\\mu = 2.0$，迭代次数 $N = 300$。\n\n对于每个测试用例，运行两种方法并按以下顺序报告四个量：\n- 投影梯度下降后 $f(\\mathbf{x})$ 的最终值（一个浮点数）。\n- 投影梯度下降最终点的可行性指标（使用指定不等式的布尔值）。\n- 二次惩罚梯度下降后 $f(\\mathbf{x})$ 的最终值（一个浮点数）。\n- 二次惩罚最终点的可行性指标（一个布尔值）。\n\n您的程序应生成一行输出，其中包含来自测试套件的所有结果，扁平化为单个列表，形式为逗号分隔的列表并用方括号括起来（例如，\"[$r_1$,$b_1$,$r_2$,$b_2$,$r_3$,$b_3$,$r_4$,$b_4$,$r_5$,$b_5$,$r_6$,$b_6$]\"），其中 $r_i$ 是浮点数，$b_i$ 是布尔值。不涉及物理单位。角度（如果有）必须以弧度为单位；然而，此问题不要求任何三角函数角度输入或输出。\n\n设计特意包含了在小 $\\mu$ 值下无约束代理可能存在不可行局部最小值的情况，以及一个使用较大 $\\mu$ 以减少不可行性的情况。您的实现必须是完全确定性的，并且不需要用户输入。仅使用指定的运行时环境。", "solution": "该问题要求实现并比较两种不同的迭代方法来解决约束优化问题：投影梯度下降（PGD）和二次惩罚方法。目标是在两个不同的非凸可行集 $\\mathcal{F}$ 上最小化光滑函数 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2$（其中 $\\mathbf{x} \\in \\mathbb{R}^2$）。\n\n首先，我们确定目标函数的梯度。给定 $f(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 = x_1^2 + x_2^2$，其梯度为：\n$$\n\\nabla f(\\mathbf{x}) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 2x_1 \\\\ 2x_2 \\end{pmatrix} = 2\\mathbf{x}\n$$\n\n### 方法一：投影梯度下降（PGD）\n\nPGD 方法是用于约束优化的迭代算法。每次迭代包括两个步骤：对目标函数进行标准的梯度下降步，然后正交投影回可行集 $\\mathcal{F}$。迭代定义为：\n$$\n\\mathbf{y}_{k} = \\mathbf{x}_{k} - \\alpha \\nabla f(\\mathbf{x}_{k})\n$$\n$$\n\\mathbf{x}_{k+1} = \\Pi_{\\mathcal{F}}(\\mathbf{y}_{k})\n$$\n其中 $\\alpha  0$ 是步长，$\\Pi_{\\mathcal{F}}(\\mathbf{y})$ 是点 $\\mathbf{y}$ 到集合 $\\mathcal{F}$ 上的正交投影。该投影是 $\\mathcal{F}$ 中在欧几里得范数意义下离 $\\mathbf{y}$ 最近的点：\n$$\n\\Pi_{\\mathcal{F}}(\\mathbf{y}) = \\arg\\min_{\\mathbf{z} \\in \\mathcal{F}} \\|\\mathbf{z} - \\mathbf{y}\\|\n$$\n代入 $\\nabla f(\\mathbf{x}_k) = 2\\mathbf{x}_k$，梯度步变为 $\\mathbf{y}_k = \\mathbf{x}_k - \\alpha(2\\mathbf{x}_k) = (1 - 2\\alpha)\\mathbf{x}_k$。\n\n我们现在必须为给定的可行集证明投影算子的合理性。\n\n**情况 A：环形区域**\n可行集为 $\\mathcal{F}_A = \\{\\mathbf{x} \\in \\mathbb{R}^2 \\mid r_{\\mathrm{in}} \\le \\|\\mathbf{x}\\| \\le R_{\\mathrm{out}}\\}$。为了找到投影 $\\Pi_{\\mathcal{F}_A}(\\mathbf{y})$，我们对 $\\mathbf{z} \\in \\mathcal{F}_A$ 最小化 $\\|\\mathbf{z} - \\mathbf{y}\\|^2$。由于目标函数 $\\|\\mathbf{z} - \\mathbf{y}\\|^2$ 和集合 $\\mathcal{F}_A$ 都关于原点具有旋转对称性，最优的 $\\mathbf{z}$ 必须位于从原点穿过 $\\mathbf{y}$ 的射线上。因此，假设 $\\mathbf{y} \\ne \\mathbf{0}$，$\\mathbf{z}$ 必须具有 $\\mathbf{z} = \\rho \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|}$ 的形式，其中 $\\rho$ 是某个标量大小。对 $\\mathbf{z}$ 的约束变为 $r_{\\mathrm{in}} \\le \\rho \\le R_{\\mathrm{out}}$。\n最小化问题简化为在区间 $[r_{\\mathrm{in}}, R_{\\mathrm{out}}]$ 中找到 $\\rho$，使得 $\\|\\rho \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|} - \\mathbf{y}\\|^2 = (\\rho - \\|\\mathbf{y}\\|)^2$ 最小化。这是一个一维问题，即在区间 $[r_{\\mathrm{in}}, R_{\\mathrm{out}}]$ 中找到离值 $\\|\\mathbf{y}\\|$ 最近的点。解决方案是将 $\\|\\mathbf{y}\\|$ 钳位到此区间：\n- 如果 $\\|\\mathbf{y}\\|  r_{\\mathrm{in}}$，则 $\\rho = r_{\\mathrm{in}}$。\n- 如果 $r_{\\mathrm{in}} \\le \\|\\mathbf{y}\\| \\le R_{\\mathrm{out}}$，则 $\\rho = \\|\\mathbf{y}\\|$。\n- 如果 $\\|\\mathbf{y}\\|  R_{\\mathrm{out}}$，则 $\\rho = R_{\\mathrm{out}}$。\n这可以写作 $\\rho = \\text{clip}(\\|\\mathbf{y}\\|, r_{\\mathrm{in}}, R_{\\mathrm{out}})$。因此，投影为：\n$$\n\\Pi_{\\mathcal{F}_A}(\\mathbf{y}) = \\text{clip}(\\|\\mathbf{y}\\|, r_{\\mathrm{in}}, R_{\\mathrm{out}}) \\frac{\\mathbf{y}}{\\|\\mathbf{y}\\|}\n$$\n如果 $\\mathbf{y} = \\mathbf{0}$，其范数为 $0  r_{\\mathrm{in}}$。$\\mathcal{F}_A$ 中最近的点是内圆上半径为 $r_{\\mathrm{in}}$ 的所有点。任何这样的点都是有效的投影。为确保确定性，我们可以选择一个特定的点，例如 $(r_{\\mathrm{in}}, 0)$。\n\n**情况 B：两个圆盘的并集**\n可行集为 $\\mathcal{F}_B = D_1 \\cup D_2$，其中 $D_1 = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}_1\\| \\le r\\}$ 和 $D_2 = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}_2\\| \\le r\\}$ 是两个不相交的闭圆盘。投影 $\\Pi_{\\mathcal{F}_B}(\\mathbf{y})$ 是 $\\mathbf{z} \\in D_1 \\cup D_2$ 中使 $\\|\\mathbf{z} - \\mathbf{y}\\|$ 最小化的点。这个最小值必须在 $D_1$ 或 $D_2$ 中达到。设 $\\mathbf{p}_1 = \\Pi_{D_1}(\\mathbf{y})$ 和 $\\mathbf{p}_2 = \\Pi_{D_2}(\\mathbf{y})$ 分别是 $\\mathbf{y}$ 到单个圆盘上的投影。到并集的投影就是离 $\\mathbf{y}$ 更近的那个：\n$$\n\\Pi_{\\mathcal{F}_B}(\\mathbf{y}) = \n\\begin{cases} \n\\mathbf{p}_1  \\text{若 } \\|\\mathbf{y} - \\mathbf{p}_1\\| \\le \\|\\mathbf{y} - \\mathbf{p}_2\\| \\\\\n\\mathbf{p}_2  \\text{其他情况}\n\\end{cases}\n$$\n到单个圆盘 $D = \\{\\mathbf{x} \\mid \\|\\mathbf{x}-\\mathbf{c}\\| \\le r\\}$ 的投影是：\n- 如果 $\\mathbf{y} \\in D$，则 $\\Pi_D(\\mathbf{y}) = \\mathbf{y}$。\n- 如果 $\\mathbf{y} \\notin D$，投影位于边界上，在连接中心 $\\mathbf{c}$ 和 $\\mathbf{y}$ 的线段上。因此，$\\Pi_D(\\mathbf{y}) = \\mathbf{c} + r \\frac{\\mathbf{y}-\\mathbf{c}}{\\|\\mathbf{y}-\\mathbf{c}\\|}$。\n\n### 方法二：二次惩罚梯度下降\n\n该方法通过向目标函数添加一个惩罚项来将约束问题转化为一系列无约束问题，该惩罚项惩罚违反约束的行为。无约束代理函数是：\n$$\nF_{\\mu}(\\mathbf{x}) = f(\\mathbf{x}) + \\mu \\sum_i \\phi(c_i(\\mathbf{x}))\n$$\n其中 $\\mu  0$ 是罚参数，$\\phi(t)$ 是一个惩罚函数。我们使用 $\\phi(t) = (\\max(0, t))^2$，它是光滑的（$C^1$），其导数为 $\\phi'(t) = 2\\max(0, t)$。然后我们对 $F_{\\mu}(\\mathbf{x})$ 应用梯度下降：\n$$\n\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\beta \\nabla F_{\\mu}(\\mathbf{x}_k)\n$$\n步长为 $\\beta  0$。使用链式法则，代理函数的梯度为：\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = \\nabla f(\\mathbf{x}) + \\mu \\sum_i \\phi'(c_i(\\mathbf{x})) \\nabla c_i(\\mathbf{x}) = 2\\mathbf{x} + \\mu \\sum_i 2\\max(0, c_i(\\mathbf{x})) \\nabla c_i(\\mathbf{x})\n$$\n\n**情况 A：环形区域**\n约束为 $c_1(\\mathbf{x}) = r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2 \\le 0$ 和 $c_2(\\mathbf{x}) = \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$。它们的梯度是 $\\nabla c_1(\\mathbf{x}) = -2\\mathbf{x}$ 和 $\\nabla c_2(\\mathbf{x}) = 2\\mathbf{x}$。代理函数 $F_\\mu(\\mathbf{x})$ 的梯度是：\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = 2\\mathbf{x} + 2\\mu \\left( \\max(0, r_{\\mathrm{in}}^2 - \\|\\mathbf{x}\\|^2)(-2\\mathbf{x}) + \\max(0, \\|\\mathbf{x}\\|^2 - R_{\\mathrm{out}}^2)(2\\mathbf{x}) \\right)\n$$\n对于较小的罚参数 $\\mu$，代理函数 $F_{\\mu}(\\mathbf{x})$ 可能在 $\\mathbf{x}=\\mathbf{0}$ 处有一个不可行的局部最小值，这会困住梯度下降算法。\n\n**情况 B：两个圆盘的并集**\n可行集由单个不等式描述：$c(\\mathbf{x}) = (\\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2) \\le 0$。令 $g_1(\\mathbf{x}) = \\|\\mathbf{x}-\\mathbf{c}_1\\|^2 - r^2$ 和 $g_2(\\mathbf{x}) = \\|\\mathbf{x}-\\mathbf{c}_2\\|^2 - r^2$。则 $c(\\mathbf{x}) = g_1(\\mathbf{x})g_2(\\mathbf{x})$。根据乘积法则，$c(\\mathbf{x})$ 的梯度为：\n$$\n\\nabla c(\\mathbf{x}) = g_2(\\mathbf{x})\\nabla g_1(\\mathbf{x}) + g_1(\\mathbf{x})\\nabla g_2(\\mathbf{x}) = g_2(\\mathbf{x})(2(\\mathbf{x}-\\mathbf{c}_1)) + g_1(\\mathbf{x})(2(\\mathbf{x}-\\mathbf{c}_2))\n$$\n代理函数 $F_\\mu(\\mathbf{x})$ 的梯度为：\n$$\n\\nabla F_{\\mu}(\\mathbf{x}) = 2\\mathbf{x} + 2\\mu \\max(0, c(\\mathbf{x})) \\nabla c(\\mathbf{x})\n$$\n与环形区域情况类似，此代理函数可能在 $\\mathbf{x}=\\mathbf{0}$（该点与 $\\mathbf{c}_1$ 和 $-\\mathbf{c}_2$ 等距）处有一个不可行的局部最小值，可能困住算法。\n\n### 可行性检查\n\n经过 $N$ 次迭代后，对于最终点 $\\mathbf{x}_{\\text{final}}$，我们必须检查它是否位于可行集 $\\mathcal{F}$ 中。\n- **对于环形区域：** 检查是否同时满足 $r_{\\mathrm{in}}^2 - \\|\\mathbf{x}_{\\text{final}}\\|^2 \\le 0$ 和 $\\|\\mathbf{x}_{\\text{final}}\\|^2 - R_{\\mathrm{out}}^2 \\le 0$。\n- **对于两个圆盘的并集：** 检查是否满足 $(\\|\\mathbf{x}_{\\text{final}}-\\mathbf{c}_1\\|^2 - r^2)(\\|\\mathbf{x}_{\\text{final}}-\\mathbf{c}_2\\|^2 - r^2) \\le 0$。\n\n实现将对指定的测试用例执行这两种算法，并报告每种方法的最终目标值和可行性状态。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Projected Gradient Descent and Quadratic Penalty methods\n    for constrained optimization on nonconvex sets.\n    \"\"\"\n\n    # --- Helper Functions for Algorithms ---\n\n    def f_obj(x):\n        \"\"\"Objective function f(x) = ||x||^2.\"\"\"\n        return np.dot(x, x)\n\n    def grad_f_obj(x):\n        \"\"\"Gradient of the objective function.\"\"\"\n        return 2 * x\n\n    # --- Projected Gradient Descent (PGD) Implementation ---\n\n    def project_annulus(y, r_in, R_out):\n        \"\"\"Projects a point y onto the annulus.\"\"\"\n        norm_y = np.linalg.norm(y)\n        if norm_y == 0.0:\n            # Project origin to the closest point in the annulus.\n            # Arbitrarily choose (r_in, 0) for determinism.\n            return np.array([r_in, 0.0])\n        \n        clamped_norm = np.clip(norm_y, r_in, R_out)\n        return y * (clamped_norm / norm_y)\n\n    def project_disk(y, c, r):\n        \"\"\"Projects a point y onto a single disk.\"\"\"\n        vec_to_center = y - c\n        dist_from_center = np.linalg.norm(vec_to_center)\n        if dist_from_center = r:\n            return y\n        else:\n            return c + r * vec_to_center / dist_from_center\n\n    def project_union_disks(y, c1, c2, r):\n        \"\"\"Projects a point y onto the union of two disks.\"\"\"\n        p1 = project_disk(y, c1, r)\n        p2 = project_disk(y, c2, r)\n        \n        dist_sq_1 = np.sum((y - p1)**2)\n        dist_sq_2 = np.sum((y - p2)**2)\n        \n        if dist_sq_1 = dist_sq_2:\n            return p1\n        else:\n            return p2\n\n    def run_pgd(x0, alpha, N, proj_func, proj_params):\n        \"\"\"Runs the Projected Gradient Descent algorithm.\"\"\"\n        x = np.array(x0, dtype=float)\n        for _ in range(N):\n            y = x - alpha * grad_f_obj(x)\n            x = proj_func(y, **proj_params)\n        return x\n\n    # --- Quadratic Penalty Method Implementation ---\n\n    def grad_penalty_annulus(x, mu, r_in, R_out):\n        \"\"\"Computes the gradient of the penalty surrogate for the annulus.\"\"\"\n        norm_x_sq = np.dot(x, x)\n        c1 = r_in**2 - norm_x_sq\n        c2 = norm_x_sq - R_out**2\n        \n        grad_c1 = -2 * x\n        grad_c2 = 2 * x\n        \n        # phi'(t) = 2 * max(0, t)\n        penalty_grad = mu * (2 * np.maximum(0, c1) * grad_c1 + 2 * np.maximum(0, c2) * grad_c2)\n        return grad_f_obj(x) + penalty_grad\n\n    def grad_penalty_disks(x, mu, c1, c2, r):\n        \"\"\"Computes the gradient of the penalty surrogate for the union of disks.\"\"\"\n        g1 = np.sum((x - c1)**2) - r**2\n        g2 = np.sum((x - c2)**2) - r**2\n        c_prod = g1 * g2\n\n        if c_prod > 0:\n            grad_g1 = 2 * (x - c1)\n            grad_g2 = 2 * (x - c2)\n            grad_c_prod = g2 * grad_g1 + g1 * grad_c_prod\n            \n            # phi'(t) = 2 * max(0, t)\n            penalty_grad = mu * (2 * c_prod * grad_c_prod)\n            return grad_f_obj(x) + penalty_grad\n        else:\n            return grad_f_obj(x)\n\n    def run_penalty_method(x0, beta, N, grad_penalty_func, grad_params):\n        \"\"\"Runs the Gradient Descent on the quadratic penalty surrogate.\"\"\"\n        x = np.array(x0, dtype=float)\n        for _ in range(N):\n            grad = grad_penalty_func(x, **grad_params)\n            x = x - beta * grad\n        return x\n\n    # --- Feasibility Checkers ---\n    \n    def is_feasible_annulus(x, r_in, R_out):\n        \"\"\"Checks if a point is in the annulus.\"\"\"\n        norm_x_sq = np.dot(x, x)\n        return r_in**2 = norm_x_sq and norm_x_sq = R_out**2\n\n    def is_feasible_disks(x, c1, c2, r):\n        \"\"\"Checks if a point is in the union of two disks.\"\"\"\n        g1 = np.sum((x - c1)**2) - r**2\n        g2 = np.sum((x - c2)**2) - r**2\n        return (g1 * g2) = 0\n\n    # --- Test Suite ---\n    \n    test_cases = [\n        # Test 1: Annulus, infeasible unconstrained minimizer\n        {\n            \"type\": \"annulus\", \"r_in\": 1.0, \"R_out\": 2.0, \"x0\": (0.3, 0.4),\n            \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 0.5, \"N\": 300\n        },\n        # Test 2: Union of disks, infeasible unconstrained minimizer\n        {\n            \"type\": \"disks\", \"r\": 1.0, \"c1\": (1.5, 0.0), \"c2\": (-1.5, 0.0),\n            \"x0\": (0.0, 0.0), \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 0.1, \"N\": 300\n        },\n        # Test 3: Annulus, boundary start, strong penalty\n        {\n            \"type\": \"annulus\", \"r_in\": 1.0, \"R_out\": 2.0, \"x0\": (1.0, 0.0),\n            \"alpha\": 0.2, \"beta\": 0.05, \"mu\": 2.0, \"N\": 300\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        x0 = case[\"x0\"]\n        alpha = case[\"alpha\"]\n        beta = case[\"beta\"]\n        mu = case[\"mu\"]\n        N = case[\"N\"]\n\n        if case[\"type\"] == \"annulus\":\n            r_in, R_out = case[\"r_in\"], case[\"R_out\"]\n            \n            # PGD\n            proj_params = {\"r_in\": r_in, \"R_out\": R_out}\n            x_pgd = run_pgd(x0, alpha, N, project_annulus, proj_params)\n            f_pgd = f_obj(x_pgd)\n            feasible_pgd = is_feasible_annulus(x_pgd, r_in, R_out)\n            \n            # Penalty\n            grad_params = {\"mu\": mu, \"r_in\": r_in, \"R_out\": R_out}\n            x_penalty = run_penalty_method(x0, beta, N, grad_penalty_annulus, grad_params)\n            f_penalty = f_obj(x_penalty)\n            feasible_penalty = is_feasible_annulus(x_penalty, r_in, R_out)\n\n        elif case[\"type\"] == \"disks\":\n            r, c1, c2 = case[\"r\"], np.array(case[\"c1\"]), np.array(case[\"c2\"])\n\n            # PGD\n            proj_params = {\"c1\": c1, \"c2\": c2, \"r\": r}\n            x_pgd = run_pgd(x0, alpha, N, project_union_disks, proj_params)\n            f_pgd = f_obj(x_pgd)\n            feasible_pgd = is_feasible_disks(x_pgd, c1, c2, r)\n\n            # Penalty\n            grad_params = {\"mu\": mu, \"c1\": c1, \"c2\": c2, \"r\": r}\n            x_penalty = run_penalty_method(x0, beta, N, grad_penalty_disks, grad_params)\n            f_penalty = f_obj(x_penalty)\n            feasible_penalty = is_feasible_disks(x_penalty, c1, c2, r)\n\n        results.extend([f_pgd, feasible_pgd, f_penalty, feasible_penalty])\n\n    # Final print statement in the exact required format.\n    # Convert booleans to lowercase 'true'/'false' as str() does by default.\n    final_output = [f\"{val:.6g}\" if isinstance(val, float) else str(val).lower() for val in results]\n    print(f\"[{','.join(final_output)}]\")\n\nsolve()\n```", "id": "3201335"}]}