## 引言
在数字世界中，数据的高效存储和检索是所有计算活动的基础。[文件系统](@entry_id:749324)作为[操作系统](@entry_id:752937)与物理存储设备之间的桥梁，其核心任务之一便是巧妙地管理磁盘空间，将用户眼中连续的文件流映射到物理上可能零散[分布](@entry_id:182848)的存储块上。传统分[配方法](@entry_id:265480)（如链式或[索引分配](@entry_id:750607)）在灵活性和性能之间难以两全，常常导致随机访问缓慢或空间碎片化等问题。为了应对这些挑战，基于盘区（Extent）的分配应运而生，成为现代高性能[文件系统](@entry_id:749324)的基石。

本文深入剖析了基于盘区的分配机制。我们将从其基本原理出发，逐步揭示它如何通过优化数据布局来彻底改变I/O性能。读者将系统地学习到：

在 **“原理与机制”** 一章中，我们将探讨盘区的核心概念，量化其性能优势，并深入研究[空闲空间管理](@entry_id:749584)中的分配与合并策略。此外，我们还将分析用于盘区管理的高效[数据结构](@entry_id:262134)以及它与延迟分配、[写时复制](@entry_id:636568)、[并发控制](@entry_id:747656)等高级功能的复杂互动。

接着，在 **“应用与跨学科连接”** 一章中，我们将视野扩展到真实世界的应用场景。从优化流媒体和数据库的性能，到与RAID阵列和现代SSD（如ZNS设备）等硬件的协同工作，本章将展示盘区分配思想在不同技术层次中的强大生命力。

最后，**“动手实践”** 部分提供了一系列精心设计的问题，旨在通过模拟分配、权衡碎片，以及设计碎片整理方案，帮助读者将理论知识转化为解决实际问题的能力。

通过这三个层层递进的章节，本文将为您构建一个关于基于盘区的分配的完整知识体系，使您不仅能理解其工作原理，更能洞悉其在构建健壮、高效存储系统中的关键作用。

## 原理与机制

在现代[操作系统](@entry_id:752937)中，文件系统的一个核心任务是管理磁盘空间，并将文件的逻辑视图（一个连续的[字节序](@entry_id:747028)列）映射到物理存储介质（由不连续的块组成）上。基于盘区（Extent）的分配是实现这一映射的高级策略。与简单的逐块分[配方法](@entry_id:265480)不同，盘区将文件数据组织成一组连续的物理块。本章将深入探讨基于盘区的分配的基本原理、性能优势、管理机制以及在现代[文件系统](@entry_id:749324)中与[并发控制](@entry_id:747656)和[崩溃一致性](@entry_id:748042)等高级功能的相互作用。

### 盘区的基本概念与性能优势

一个**盘区**（extent）被定义为一个或多个连续物理磁盘块的集合。文件系统将一个文件表示为一个有序的盘区列表。这种表示方法与传统的分配策略（如[连续分配](@entry_id:747800)、[链式分配](@entry_id:751340)和[索引分配](@entry_id:750607)）形成了鲜明对比，并提供了一种灵活的折中方案。

- **[连续分配](@entry_id:747800)**要求将整个文件存储在一个单一的、连续的物理块区域中。这可以看作是文件只由一个盘区组成的特例。虽然它为顺序访问提供了最佳性能，但它存在严重的[外部碎片](@entry_id:634663)问题，并且难以支持文件的动态增长。

- **[链式分配](@entry_id:751340)**将文件的每个块单独放置，并通过块内嵌的指针将它们链接在一起。这可以看作是文件由许多大小为1个块的盘区组成的特例。它解决了[外部碎片](@entry_id:634663)和文件增长的问题，但对随机访问性能极差，因为访问任意块都需要遍历前面的指针链。

- **[索引分配](@entry_id:750607)**通过一个或多个索引块来存储指向文件[数据块](@entry_id:748187)的指针。这也类似于文件由许多大小为1个块的盘区组成，但它通过索引块提供了高效的随机访问。然而，对于大文件，索引块本身可能需要多级结构，增加了元数据开销。

基于盘区的分配是上述方法的泛化。它允许文件由不同长度的多个盘区组成，从而在顺序性能和分配灵活性之间取得了平衡。其主要性能优势在于**摊销定位成本**。在机械硬盘（HDD）上，每次读写操作都包含两个主要的时间成本：**定位时间**（[寻道时间](@entry_id:754621)和[旋转延迟](@entry_id:754428)）和**传输时间**。定位时间通常是固定的开销，而传输时间与传输的数据量成正比。通过将多个逻辑块组合成一个较大的盘区并进行单次I/O操作，系统可以将一次寻道和旋转的成本摊销到更多的数据上，从而显著提高有效吞吐量。

为了量化这一优势，我们可以建立一个简单的[磁盘性能](@entry_id:748541)模型。假设磁盘的平均定位时间为 $t_p$（包括[寻道时间](@entry_id:754621) $t_s$ 和[旋转延迟](@entry_id:754428) $t_r$），持续传输速率为 $\text{rate}$。当读取一个大小为 $E$ 字节的盘区时，总时间为定位时间加上传输时间：

$T_{\text{I/O}} = t_p + \frac{E}{\text{rate}}$

相应的[吞吐量](@entry_id:271802)为：

$\text{Throughput}(E) = \frac{E}{t_p + \frac{E}{\text{rate}}}$

从这个公式可以看出，当盘区大小 $E$ 很小时，吞吐量主要受 $t_p$ 限制。随着 $E$ 的增加，分母中的 $t_p$ 变得不那么重要，吞吐量逐渐接近物理传输速率 $\text{rate}$。一个关键的性能[拐点](@entry_id:144929)出现在传输时间等于定位时间时。我们可以将这个“[拐点](@entry_id:144929)”盘区大小定义为 $E^{\star}$ [@problem_id:3640695]：

$\frac{E^{\star}}{\text{rate}} = t_p = t_s + t_r$

$E^{\star} = (t_s + t_r) \times \text{rate}$

当盘区大小远小于 $E^{\star}$ 时，I/O性能受定位开销主导。当盘区大小远大于 $E^{\star}$ 时，性能则受传输速率主导。因此，文件系统的目标是尽可能分配大于 $E^{\star}$ 的盘区，以充分利用磁盘的带宽。

此外，在非顺序访问模式下，盘区的局部性优势依然存在。例如，在一个步长为 $s$ 的访问模式（访问逻辑块 $0, s, 2s, \dots$）中，如果文件是连续存储的（即一个大盘区），那么两次连续访问之间的物理距离是固定的。如果这个距离小于一个柱面内的块数 $C$，则发生寻道的概率大约为 $s/C$。相反，如果文件块是随机[分布](@entry_id:182848)的（如链式或[索引分配](@entry_id:750607)），那么每次访问几乎都保证会跨越柱面，导致寻道概率接近 $1$ [@problem_id:3642744]。基于盘区的分配通过将[数据块](@entry_id:748187)分组到中等大小的盘区中，保持了局部性，从而在各种访问模式下都能提供比完全随机分配更好的性能。

### 盘区的[空闲空间管理](@entry_id:749584)

虽然盘区能提高性能，但有效管理它们，特别是管理空闲空间，是至关重要的。[文件系统](@entry_id:749324)必须维护一个空闲盘区列表，并在分配新盘区时做出明智的决策，以减缓**[外部碎片](@entry_id:634663)**的产生。[外部碎片](@entry_id:634663)是指总空闲空间足以满足请求，但没有单个足够大的连续空闲区域。

#### 分配策略

当需要为文件分配一个大小为 $s$ 的新盘区时，[文件系统](@entry_id:749324)必须从空闲盘区列表中选择一个合适的候选者。常见的策略包括：

- **最佳适配（Best-fit）**：选择满足 $e \ge s$ 的最小空闲盘区 $e$。这个策略的意图是留下尽可能小的“残羹冷饭”，以期减少浪费。然而，它有一个显著的缺点：它倾向于产生大量非常小的、几乎无法使用的空闲碎片。

- **最大适配（Largest-fit）** 或称 **最差适配（Worst-fit）**：选择满足 $e \ge s$ 的最大空闲盘区 $e$。这个策略的意图是，从最大的空闲盘区分割出一块后，剩余的部分仍然足够大，可以用于未来的大请求。

我们可以通过一个具体的例子来比较这两种策略[@problem_id:3640658]。假设初始空闲空间由四个盘区组成，大小分别为 $\{64, 25, 25, 25\}$ 块。现在系统收到四个连续的分配请求，大小分别为 $\{24, 24, 24, 25\}$ 块。

- **使用最佳适配**：
  1. 请求 $24$：从一个 $25$ 块的盘区分割，剩余 $1$ 块。空闲列表变为 $\{64, 25, 25, 1\}$。
  2. 请求 $24$：再次从一个 $25$ 块的盘区分割，剩余 $1$ 块。空闲列表变为 $\{64, 25, 1, 1\}$。
  3. 请求 $24$：从最后一个 $25$ 块的盘区分割，剩余 $1$ 块。空闲列表变为 $\{64, 1, 1, 1\}$。
  4. 请求 $25$：只能从 $64$ 块的盘区分割，剩余 $39$ 块。最终空闲列表为 $\{39, 1, 1, 1\}$。
  在这个过程中，总共发生了 $4$ 次分割。最终的空闲空间高度碎片化。

- **使用最大适配**：
  1. 请求 $24$：从最大的 $64$ 块盘区分割，剩余 $40$ 块。空闲列表变为 $\{40, 25, 25, 25\}$。
  2. 请求 $24$：再次从最大的 $40$ 块盘区分割，剩余 $16$ 块。空闲列表变为 $\{16, 25, 25, 25\}$。
  3. 请求 $24$：从一个 $25$ 块的盘区分割，剩余 $1$ 块。空闲列表变为 $\{16, 25, 25, 1\}$。
  4. 请求 $25$：使用一个 $25$ 块的盘区，[完美匹配](@entry_id:273916)，不产生碎片。最终空闲列表为 $\{16, 25, 1\}$。
  在这个过程中，只发生了 $3$ 次分割，并且留下了更大、更有用的空闲盘区。这个例子表明，一个看似“浪费”的策略（最大适配）有时能更好地保留大块空闲空间，从而在长期内减少碎片。

#### 病态分片与放置策略

分配策略中一个更微妙的方面是**放置位置**。即使选定了一个空闲盘区，新数据具体放在这个盘区的哪个位置（开头、中间、末尾）也会对碎片产生深远影响。一个设计拙劣的放置策略可能导致**病态分片**（pathological fragmentation）。

考虑一个天真的“中间放置”策略：每次都在最大的空闲盘区的正中间分配一个小请求[@problem_id:3640702]。假设我们从一个大小为 $N$ 的巨大空闲盘区开始，并重复分配大小为 $a$ ($a \ll N$) 的小块。第一次分配后，大小为 $N$ 的盘区被分裂成两个大小约 $N/2$ 的盘区。第二次分配将作用于其中一个 $N/2$ 的盘区，又将其分裂成两个约 $N/4$ 的盘区。这种行为会级联发生，每次分配都倾向于将当前最大的空闲块一分为二，导致最大连续可用空间呈指数级下降。

相比之下，一个更稳健的**边置偏向策略**（edge-biased placement），例如总是从空闲盘区的起始位置分配，会将剩余部分保留为一个单一的、连续的空闲盘区。在这种策略下，经过 $k$ 次分配后，最大可用空间只是线性地从 $N$ 减少到 $N - ka$。这表明，明智的放置策略对于防止灾难性的碎片累积至关重要。

#### 释放与合并

当文件被删除或截断时，其占用的盘区会被释放回空闲空间池。为了对抗碎片化，[文件系统](@entry_id:749324)应该在可能的情况下**合并**（coalesce）相邻的空闲盘区，形成一个更大的连续空闲区域。然而，何时执行合并是一个重要的设计决策。

- **立即合并（Immediate Coalescing）**：在释放盘区的操作中立即检查其前后邻居是否空闲，如果是，则合并。这样做的好处是能尽快恢复大的空闲盘区，可能对后续的分配有利。

- **延迟合并（Lazy Coalescing）**：仅将释放的盘区添加到空闲列表，而将[合并操作](@entry_id:636132)推迟到稍后的某个时间点（例如，在分配新空间时，或者由一个后台线程执行）。

这个选择在高度并发的环境中尤为关键[@problem_id:3640665]。[文件系统](@entry_id:749324)元数据更新通常需要获取锁以保证一致性。在一个[多线程](@entry_id:752340)删除文件的场景中，如果使用立即合并，每次删除操作的[临界区](@entry_id:172793)（持有锁的时间）会变长，因为它包含了查找和合并邻居的成本。这会增加锁的争用，导致其他等待线程的排队延迟急剧上升，从而恶化高百分位（如99%）的操作延迟。

相比之下，延迟合并将合并工作移出了删除操作的关键路径。删除操作的[临界区](@entry_id:172793)变得非常短，只包含将盘区添加到空闲列表的最小更新。这显著降低了锁争用，从而在高并发负载下能够获得更低的尾部延迟。其代价是，空闲空间在一段时间内可能保持碎片化状态，这可能会暂时影响分配性能，直到[合并操作](@entry_id:636132)最终被执行。对于追求低延迟和高并发的系统，延迟合并通常是更优越的选择。

### 用于盘区管理的[数据结构](@entry_id:262134)

随着文件的增长和修改，一个文件可能由成百上千甚至数百万个盘区组成。为了高效地管理这些盘区，[文件系统](@entry_id:749324)需要一个可扩展的元数据结构。简单地在文件的 inode（索引节点）中存储一个盘区列表很快就会变得不切实际。

现代[文件系统](@entry_id:749324)（如XFS、Btrfs、ext4）普遍采用类似 **[B+树](@entry_id:636070)** 的结构来索引文件的盘区，通常称为**盘区树**（extent tree）。在这种结构中：
- **叶节点**包含盘区描述符。每个描述符是一个元组，例如 `(logical_start_block, physical_start_block, length)`，它将文件内的一个逻辑块范围映射到一个物理块范围。
- **内部节点**（或称[索引节点](@entry_id:750667)）包含一系列键和指向下一层节点的指针。键通常是逻辑块号，用于引导搜索。

这种树形结构使得查找任意逻辑块所在的物理位置非常高效。假设一个内部节点可以容纳 $b$ 个子节点指针（即树的分支因子为 $b$），而一个[叶节点](@entry_id:266134)可以容纳 $\ell$ 个盘区描述符。一个包含 $n$ 个盘区的文件需要 $L = \lceil n/\ell \rceil$ 个[叶节点](@entry_id:266134)来存储所有描述符。覆盖这 $L$ 个叶节点所需的[树高](@entry_id:264337) $h$（从根到叶的边数）大约为：

$h = \lceil \log_{b} L \rceil$

由于对数函数的增长非常缓慢，即使对于拥有大量盘区的极其碎片化的文件，盘区[树的高度](@entry_id:264337)通常也很小（例如，3到5层）。这意味着定位文件中任何部分的数据只需要少数几次间接访问。如果树的根节点被缓存在内存中，那么访问任何[数据块](@entry_id:748187)最多只需要 $h$ 次磁盘读取来遍历树的路径[@problem_id:3640750]。这保证了即使在严重碎片化的情况下，访问性能也能优雅地扩展。

### 高级机制与交互

基于盘区的分配与文件系统的其他高级功能（如延迟分配和[写时复制](@entry_id:636568)）相互作用，产生了复杂的行为和性能权衡。

#### 延迟分配（Delayed Allocation）

**延迟分配**是一种优化策略，即[文件系统](@entry_id:749324)在应用程序写入数据到[页缓存](@entry_id:753070)（page cache）时，并不立即为其分配物理磁盘块。相反，它会等到数据需要被写回（writeback）到磁盘时（例如，因为内存压力、周期性刷新或文件关闭），才做出最终的分配决策。

延迟分配的核心优势在于，它允许分配器在最后一刻根据更全面的信息做出更优的决策[@problem_id:3640700]。例如，当一个进程顺序写入一个大文件时，延迟分配可以在[写回](@entry_id:756770)时看到整个写入序列的大小，从而一次性地尝试分配一个或几个大的连续盘区。

这种策略的有效性与系统[状态和](@entry_id:193625)时机密切相关。考虑一个场景：系统在时间 $t=0$ 时空闲空间高度碎片化，最大连续空闲区只有 $30$ MB。但系统知道在 $t=3$ 秒时，一个 $200$ MB 的大文件将被删除，释放出一个巨大的连续空间。一个进程在 $t=0$ 开始以 $40$ MB/s 的速度写入一个新文件。
- **如果内存充足**，写回操作可能被推迟到 $t=3$ 秒之后。届时，分配器将看到那个新释放的 $200$ MB 空间，并可以轻松地为写入的数据分配一个大的、连续的盘区，从而获得极佳的布局。
- **如果内存压力大**，[操作系统](@entry_id:752937)可能会在 $t=3$ 秒之前就强制触发写回（例如，在 $t=2.5$ 秒时）。此时，分配器只能在碎片化的空间中进行选择，导致文件被写入许多小的盘区。更糟糕的是，尽管总空闲空间足够，但分配器可能暂时找不到足够大小的盘区来满足请求，从而导致临时的“空间不足”错误。

因此，延迟分配是一把双刃剑：它为获得更好的数据布局创造了机会窗口，但这个机会窗口的有效性受到内存压力和空闲空间动态变化的制约。

#### [写时复制](@entry_id:636568)（Copy-on-Write, CoW）与文件克隆

许多现代文件系统支持**文件克隆**（也称为 `reflink`），它允许用户创建一个或多个文件的“副本”而无需实际复制数据。所有克隆体最初共享同一组物理盘区，每个共享盘区的元数据中都有一个引用计数器。

当对一个共享盘区进行写入时，**[写时复制](@entry_id:636568)（Copy-on-Write, CoW）**机制被触发[@problem_id:3640729]。[文件系统](@entry_id:749324)不会在原地修改共享数据（因为这会破坏其他克隆体的数据），而是：
1.  分配一块新的、私有的物理空间。
2.  将要写入的数据与原始数据（如果需要）结合，写入这个新空间。
3.  修改写入文件的盘区映射。通常，这需要将原始的共享盘区分裂成三部分：写入点之前未修改的共享部分、新分配的私有部分、以及写入点之后未修改的共享部分。

这种机制对盘区管理有重要影响。考虑一个由单一 $64$ MB 盘区组成的文件被克隆了三次。如果两个克隆体各自执行了 $50$ 次 $64$ KB 的随机写入，那么在忽略重叠的情况下，将发生 $100$ 次CoW操作。
- **[元数据](@entry_id:275500)碎片化**：每次（非边界）写入都会将一个盘区分裂成三个，导致该文件的盘区数量净增两个。经过 $100$ 次写入，文件的盘区数量将从 $1$ 增加到大约 $201$。文件的盘区列表迅速膨胀，增加了元[数据管理](@entry_id:635035)的开销。
- **[外部碎片](@entry_id:634663)压力**：每次CoW都需要分配一块新的、相对较小的物理盘区（本例中为 $64$ KB）。这种对大量小块空间的持续请求会不断地切割大的空闲盘区，加剧了[外部碎片](@entry_id:634663)问题。

需要注意的是，并非所有操作都会导致分裂。例如，对克隆文件进行**追加**操作只会在文件末尾添加新的、私有的盘区，而不会影响原始共享盘区。克隆文件可以独立增长，它们的逻辑长度不必保持一致。

### [并发控制](@entry_id:747656)

在[多处理器系统](@entry_id:752329)中，多个线程或进程可能同时尝试修改同一个文件或分配空间，这要求文件系统提供强大的[并发控制](@entry_id:747656)机制。盘区管理中的锁粒度是一个关键的设计决策。

考虑两个进程并发地向同一个文件追加数据[@problem_id:3640696]。假设由于空闲空间碎片化，每次追加 $8$ 个块的操作都需要分配两个新盘区（一个 $5$ 块，一个 $3$ 块）。

- **文件级锁（Per-file locking）**：在整个追加操作期间（包括分配和链接两个盘区），锁定整个文件。这种粗粒度的锁可以确保每个追加操作是[原子性](@entry_id:746561)的。进程一的两个盘区会连续地出现在文件末尾，然后是进程二的两个盘区（或反之）。这种方式可以防止**盘区交错**，使得文件的逻辑和物理布局更加整洁。但它的缺点是降低了并发度，因为在任何时候只有一个线程能对该文件进行写操作。

- **盘区级锁（Per-extent locking）**：使用更细粒度的锁，例如只在将单个盘区链接到文件末尾的瞬间锁定文件的尾部元数据。在这种模式下，一个进程的两次链接操作之间可能会穿插另一个进程的链接操作。这可能导致盘区交错，例如，最终的盘区序列可能是 `P1-ext1, P2-ext1, P1-ext2, P2-ext2`。这种交错虽然不影响文件的逻辑正确性，但可能损害顺序访问性能。其优点是允许更高的并发度。

细粒度锁也带来了**[死锁](@entry_id:748237)**的风险。例如，一个追加操作可能先锁住文件尾部，然后请求空闲空间锁；而一个空间回收进程可能先锁住空闲空间，然后尝试合并一个盘区，这需要锁住文件尾部。这种“[持有并等待](@entry_id:750367)”的[循环依赖](@entry_id:273976)满足了[死锁的必要条件](@entry_id:752389)。解决方案是建立一个**全局锁序**，例如规定任何线程都必须先获取文件锁，然后才能获取空闲空间锁。这样就打破了[循环等待](@entry_id:747359)，从而避免[死锁](@entry_id:748237)。

### [崩溃一致性](@entry_id:748042)

最后，对盘区元数据的所有修改都必须是**崩溃一致**的。如果系统在[更新过程](@entry_id:273573)中崩溃，元数据不能处于不一致的状态（例如，一个盘区既不属于任何文件，也不在空闲列表中）。实现这一点的两种主要技术是写前日志（WAL）和[写时复制](@entry_id:636568)（COW）。

假设我们需要更新一个盘区描述符，该更新涉及写入一个元数据块[@problem_id:3640738]。

- **写前日志（Write-Ahead Logging, WAL）**：
  1. 将描述变化的“重做”（redo）信息写入一个日志（journal）文件。这至少需要一次持久化I/O。
  2. 将一个“提交”记录写入日志，表示该事务已完成。这需要第二次持久化I/O。
  3. 只有在提交记录持久化后，操作才能返回成功。
  4. 稍后，在方便的时候，系统会将日志中的变更应用到[元数据](@entry_id:275500)的“主”位置（in-place update）。
  在[崩溃恢复](@entry_id:748043)时，系统只需扫描日志并重放已提交但可能尚未应用到主位置的事务。为保证提交的原子性，WAL在返回成功前至少需要**两次**持久化I/O（日志数据 + 提交记录）。

- **[写时复制](@entry_id:636568)（Copy-on-Write, COW）**：
  1. 从不原地修改[元数据](@entry_id:275500)块。相反，将修改后的新版本写入一个新的空闲位置。这需要一次持久化I/O。
  2. 修改指向该块的父指针，使其指向新位置。这个修改本身也是通过CoW完成的，级联向上，最终到达一个唯一的根节点（如超级块）。更新根指针需要第二次持久化I/O。
  3. 为保证一致性，必须先确保新的数据/[元数据](@entry_id:275500)块已持久化，然后才能持久化指向它的父指针。
  对于一个单级[元数据](@entry_id:275500)更新，CoW在返回成功前也需要**两次**持久化I/O（新的元数据块 + 更新后的父块/根）。

虽然两者在最简单的场景下所需的I/O次数相似，但它们的架构影响截然不同。WAL保留了数据的物理位置，而COW则不断地[迁移数](@entry_id:267968)据和[元数据](@entry_id:275500)，这使得它天然地与快照等功能兼容，但也可能导致数据布局的长期退化。

总之，基于盘区的分配是现代[文件系统设计](@entry_id:749343)的基石。它通过在分配灵活性和顺序性能之间取得平衡，为高效的[数据管理](@entry_id:635035)提供了基础。然而，充分发挥其潜力需要复杂的机制来管理空闲空间、组织元数据、处理并发访问并确保崩溃后的一致性。