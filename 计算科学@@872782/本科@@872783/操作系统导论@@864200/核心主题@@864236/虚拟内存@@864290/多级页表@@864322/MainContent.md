## 引言
在现代计算系统中，[虚拟内存](@entry_id:177532)为每个进程提供了一个私有且连续的地址空间，极大地简化了编程和内存管理。实现这一强大抽象的核心[数据结构](@entry_id:262134)是[页表](@entry_id:753080)，它负责将程序的虚拟地址翻译为物理内存地址。然而，随着64位架构的普及，[虚拟地址空间](@entry_id:756510)变得异常庞大，传统的单级[页表](@entry_id:753080)因其巨大的内存开销和对连续物理内存的苛刻要求而变得不再可行。

本文旨在系统性地解决这一规模性挑战，深入剖析现代[操作系统](@entry_id:752937)普遍采用的解决方案——多级[页表](@entry_id:753080)。通过本文的学习，你将全面理解多级页表的设计思想、工作机制及其在真实世界中的复杂应用。

在“原理与机制”一章中，我们将从单级[页表](@entry_id:753080)的局限性出发，揭示多级页表“为[页表](@entry_id:753080)分页”的精妙思想，并详细分析其内存效率、性能影响以及设计中的[时空权衡](@entry_id:755997)。接着，在“应用与跨学科联系”一章，我们将视野拓展到[操作系统](@entry_id:752937)优化、[计算机体系结构](@entry_id:747647)、虚拟化和系统安[全等](@entry_id:273198)领域，展示多级页表如何作为基石支撑起这些高级功能。最后，“动手实践”部分将通过具体的编程问题，巩固你对地址分解、性能分析和[操作系统](@entry_id:752937)与硬件协同工作的理解。让我们一同深入探索这个构建高效、安全、可扩展计算系统的核心技术。

## 原理与机制

在上一章中，我们介绍了[虚拟内存](@entry_id:177532)的基本概念，即[操作系统](@entry_id:752937)如何为每个进程提供一个私有的、连续的地址空间。实现这一抽象的关键[数据结构](@entry_id:262134)是页表。本章将深入探讨现代[操作系统](@entry_id:752937)中用于实现页表的核心技术——多级[页表](@entry_id:753080)。我们将从其设计动机出发，系统性地剖析其工作原理、效率与性能权衡，以及在真实系统中应用的各种高级机制与优化。

### 规模的挑战：为何单级页表不够用

最直观的[页表](@entry_id:753080)实现方式是采用一个简单的线性数组，即**单级页表**（single-level page table）。在这种设计中，系统为进程[虚拟地址空间](@entry_id:756510)中的每一个虚拟页都分配一个**页表项**（Page Table Entry, [PTE](@entry_id:753081)）。[PTE](@entry_id:753081) 存储了虚拟页到物理页帧的映射关系，或者标记该页尚未被映射。

然而，随着现代[计算机体系结构](@entry_id:747647)向 64 位过渡，[虚拟地址空间](@entry_id:756510)变得极其巨大，单级页表的局限性也随之凸显。让我们通过一个具体的例子来分析这个问题 [@problem_id:3660484]。考虑一个拥有 $v=32$ 位虚拟地址的系统，其页面大小为 $P = 4$ KiB（即 $2^{12}$ 字节）。这意味着页内偏移量需要 $p=12$ 位，而**虚拟页号**（Virtual Page Number, VPN）则占据了 $32 - 12 = 20$ 位。

因此，该进程的[虚拟地址空间](@entry_id:756510)中总共有 $2^{20}$（即 1,048,576）个虚拟页。如果每个 PTE 的大小为 $s = 4$ 字节，那么存储整个页表所需的内存为：

$M_{flat} = N_{VP} \times s = 2^{20} \times 4 \text{ bytes} = 2^{22} \text{ bytes} = 4 \text{ MiB}$

这意味着，对于系统中运行的**每一个**进程，[操作系统](@entry_id:752937)都必须为其分配一块固定大小的 4 MiB 连续物理内存来存放页表。这一要求存在两个致命缺陷：
1.  **巨大的内存开销**：即使一个进程只使用了几千字节的内存（例如一个简单的 “Hello, World!” 程序），它依然需要一个完整的 4 MiB [页表](@entry_id:753080)。这造成了严重的内存浪费。对于一个拥有数百个进程的系统，仅[页表](@entry_id:753080)本身就可能消耗数吉字节的宝贵物理内存。
2.  **[连续分配](@entry_id:747800)的困难**：[操作系统](@entry_id:752937)必须找到一块 4 MiB 的**连续**物理内存来存放[页表](@entry_id:753080)。随着系统运行时间的增长，物理内存会变得碎片化，找到这样一块巨大的连续空间将变得愈发困难。

对于 64 位系统，这个问题变得更加严峻。其[虚拟地址空间](@entry_id:756510)的大小是天文数字，使得单级页表在物理上完全不可行。因此，我们需要一种更智能、更节省空间的[页表结构](@entry_id:753084)。

### 分级[页表](@entry_id:753080)：“为[页表](@entry_id:753080)[分页](@entry_id:753087)”的原理

为了解决单级[页表](@entry_id:753080)的规模问题，现代[操作系统](@entry_id:752937)普遍采用**多级[页表](@entry_id:753080)**（multilevel page table），也称为**层级式[页表](@entry_id:753080)**（hierarchical page table）。其核心思想非常巧妙：将庞大而稀疏的线性页表进行拆分，并利用层级结构来管理这些拆分后的小块。简而言之，就是“为[页表](@entry_id:753080)本身进行分页”。

让我们再次以 32 位系统为例 [@problem_id:3660484]，页面大小为 4 KiB，PTE 大小为 4 字节。一个物理页帧可以容纳 $4096 / 4 = 1024$ 个 PTE。基于此，我们可以将原本包含 $2^{20}$ 个条目的巨大线性页表，拆分成若干个大小为 1024 个条目的小[页表](@entry_id:753080)。每个小页表恰好可以存放在一个物理页帧中。

这样，我们总共需要 $2^{20} / 1024 = 2^{20} / 2^{10} = 1024$ 个这样的小[页表](@entry_id:753080)。为了索引这些小页表，我们引入了一个新的、更高层级的表，称为**页目录**（Page Directory）。这个页目录包含 1024 个条目，每个条目指向一个拆分后的小[页表](@entry_id:753080)（现在我们称之为二级[页表](@entry_id:753080)）。由于页目录本身也只有 1024 个条目，所以它也恰好可以存放在一个物理页帧中。

这种结构被称为**二级[页表](@entry_id:753080)**（two-level page table）。地址翻译的过程也相应地发生了变化。原本 20 位的 VPN 被进一步拆分。由于二级[页表](@entry_id:753080)和页目录都包含 $1024 = 2^{10}$ 个条目，所以我们分别需要 10 位来索引它们。因此，20 位的 VPN 被分为：
-   $10$ 位**页目录索引**（Page Directory Index, PDI）
-   $10$ 位**[页表](@entry_id:753080)索引**（Page Table Index, PTI）

硬件（MMU）进行地址翻译的步骤如下：
1.  从控制暂存器（例如 x86 架构下的 CR3 寄存器）中获取页目录的基地址。
2.  使用 VPN 的高 10 位（PDI）作为索引，在页目录中找到对应的**页目录项**（Page Directory Entry, PDE）。
3.  如果 PDE有效，它将指向一个二级[页表](@entry_id:753080)的物理基地址。
4.  使用 VPN 的低 10 位（PTI）作为索引，在该二级[页表](@entry_id:753080)中找到对应的 [PTE](@entry_id:753081)。
5.  如果 [PTE](@entry_id:753081) 有效，它将包含目标物理页帧的基地址（**Page Frame Number, PFN**）。
6.  将 PFN 与原始虚拟地址中的 12 位页内偏移量组合，形成最终的物理地址。

这种设计的精髓在于**按需分配**。只有当一个虚拟地址区间（对应一个完整的二级[页表](@entry_id:753080)所能映射的范围）被实际使用时，[操作系统](@entry_id:752937)才需要为其分配一个二级页表。对于进程地址空间中大量未使用的“空洞”，页目录中对应的 PDE 可以被标记为无效，从而无需为这些区域分配任何二级页表，极大地节省了内存。

### 内存效率分析

多级[页表](@entry_id:753080)的核心优势在于其内存效率，但这个效率高度依赖于进程内存使用的**稀疏度**（sparsity）。

#### 最佳与最差稀疏度场景

我们可以分析两种极端情况来理解这一点：

-   **最佳场景：密集分配**
    当进程使用的内存是连续的大块区域时，多级[页表](@entry_id:753080)的效率最高。假设一个进程映射了 $k$ 个连续的虚拟页。在我们的二级页表示例中，每个二级[页表](@entry_id:753080)可以映射 1024 个页面。因此，映射这 $k$ 个页面只需要 $\lceil k/1024 \rceil$ 个二级[页表](@entry_id:753080)。总的内存开销为页目录（1 页）加上所有已分配的二级页表。其内存占用 $M_{hier}(k) = P \times (1 + \lceil k/1024 \rceil)$，其中 $P$ 是页面大小 [@problem_id:3660484]。这种情况下，内存开销与被映射的页面数量 $k$ 近似成正比，非常高效。

-   **最差场景：分散分配**
    当进程使用的内存极度分散时，情况则大不相同。考虑一个“最差分散分配”场景：进程映射的 $k$ 个页面中的任意两个都不在同一个二级[页表](@entry_id:753080)所能覆盖的地址范围内 [@problem_id:3660548]。这意味着，为了映射这 $k$ 个页面，[操作系统](@entry_id:752937)必须分配 $k$ 个**不同**的二级[页表](@entry_id:753080)，即使每个二级[页表](@entry_id:753080)中只有一个 PTE 是有效的。假设一个二级[页表](@entry_id:753080)包含 $b_2$ 个 [PTE](@entry_id:753081)，每个 [PTE](@entry_id:753081) 大小为 $S$ 字节，那么分配一个二级页表就需要 $b_2 \times S$ 的内存。在这种最差情况下，仅二级页表占用的总内存就高达 $k \cdot b_2 \cdot S$。这说明对于高度稀疏的内存访问模式，多级[页表](@entry_id:753080)的内存开销会随着稀疏映射点的数量线性增长。

#### 盈亏[平衡点](@entry_id:272705)

多级页表并非在所有情况下都优于单级[页表](@entry_id:753080)。它的优势体现在地址空间的[稀疏性](@entry_id:136793)上。我们可以计算出多级页表内存开销低于单级[页表](@entry_id:753080)的“盈亏[平衡点](@entry_id:272705)”。

继续使用 [@problem_id:3660484] 的例子，单级[页表](@entry_id:753080)固定占用 $M_{flat} = 4$ MiB。二级页表的内存占用为 $M_{hier}(k) = 4096 \times (1 + \lceil k/1024 \rceil)$ 字节。我们求解不等式 $M_{hier}(k)  M_{flat}$：

$4096 \left(1 + \left\lceil \frac{k}{1024} \right\rceil\right)  4194304$

$1 + \left\lceil \frac{k}{1024} \right\rceil  1024$

$\left\lceil \frac{k}{1024} \right\rceil \le 1022$

这意味着只要 $\frac{k}{1024} \le 1022$，即 $k \le 1022 \times 1024 = 1046528$，二级页表在内存消耗上就更具优势。考虑到一个典型的进程很少会映射超过一百万个4KB页面（约4GB内存），多级页表在实践中几乎总是更优的选择。

更抽象地看，多级页表引入的“浪费”主要来自于那些被分配但未被充分利用的页表页。在一个包含 $L$ 级、每级分支因子为 $E$ 的页表树中，如果地址访问的稀疏度（即每个页表中有效条目的比例）为 $\rho$，那么总的浪费空间与 $\rho$ 和 $L$ 都有关 [@problem_id:3660531]。稀疏度越低，多级[页表](@entry_id:753080)的优势越明显。

### 体系[结构设计](@entry_id:196229)与约束

多级页表的具体结构，如层级深度 $L$ 和每级使用的索引位数 $b_{\ell}$，并非随意设定，而是受到硬件体系结构的严格约束。

#### 约束1：页表必须放入页帧

一个核心的设计原则是，**任何级别的页表本身都必须能够被完整地存放在一个物理页帧中**。这简化了内存管理，因为[操作系统](@entry_id:752937)可以用管理普通数据页的方式来管理页表页。

这个约束直接限制了单个[页表](@entry_id:753080)的大小，从而限制了它能包含的 [PTE](@entry_id:753081) 数量。如果页大小为 $P$字节，PTE 大小为 $s$ 字节，那么一个[页表](@entry_id:753080)最多能容纳 $P/s$ 个 [PTE](@entry_id:753081)。因此，用于索引该级[页表](@entry_id:753080)的地址位数 $b_{\ell}$ 必须满足：

$2^{b_{\ell}} \le \frac{P}{s} \implies b_{\ell} \le \log_2\left(\frac{P}{s}\right)$

由于 $b_{\ell}$ 必须是整数，所以 $b_{\ell, \max} = \lfloor \log_2(P/s) \rfloor$ [@problem_id:3660499]。例如，在一个 $P=8$ KiB（$2^{13}$字节）、$s=16$字节（$2^4$字节）的系统中，每级索引最多只能使用 $\lfloor \log_2(2^{13}/2^4) \rfloor = 9$ 位。

#### 约束2：[虚拟地址空间](@entry_id:756510)上限

所有级别的索引位数之和，构成了完整的虚拟页号（VPN）。VPN 的总位数 $B_{tot} = \sum_{\ell=1}^{L} b_{\ell}$，加上页内偏移量的位数 $o = \log_2(P)$，不能超过体系结构所支持的总虚拟地址宽度 $v$。

$B_{tot} + o \le v \implies \sum_{\ell=1}^{L} b_{\ell} \le v - o$

综合这两个约束，一个体系结构所能支持的最大虚拟页数量由以下两者中的较小值决定：(1) 由[页表](@entry_id:753080)层级深度和每级最大宽度所限制的总索引位数；(2) 由虚拟地址宽度上限所限制的总索引位数 [@problem_id:3660499]。

#### [页表](@entry_id:753080)树的几何形态权衡

在满足上述约束的前提下，设计者仍然面临一个关键的权衡：是构建一个“**浅而宽**”的[页表](@entry_id:753080)树，还是一个“**深而窄**”的树？[@problem_id:3660458]

-   **浅而宽的树**：在靠近根的层级使用更多的索引位（例如，一个大的 $b_1$）。这会减少[页表](@entry_id:753080)的总层级数 $L$。
    -   **优点**：[页表遍历](@entry_id:753086)（page walk）的深度减小，这意味着在 TLB 未命中时，访问内存的次数减少，从而加快了地址翻译速度。
    -   **缺点**：根页表（第一级页表）会变得非常大（其大小与 $2^{b_1}$ 成正比），增加了固定的内存开销。更重要的是，它降低了对稀疏地址空间的内存利用率。由于第一级页表的一个条目覆盖了更小的地址范围，一个大的连续内存块会被拆分到更多的第一级条目下，而稀疏[分布](@entry_id:182848)的页面也更难共享同一个下级[页表](@entry_id:753080)，导致需要分配更多的中间层页表。

-   **深而窄的树**：在每级使用较少的索引位，从而增加了总层级数 $L$。
    -   **优点**：每级页表都比较小，更适合稀疏的[内存分配](@entry_id:634722)模式，内存效率更高。
    -   **缺点**：[页表遍历](@entry_id:753086)的深度增加，TLB 未命中时的翻译延迟更大。

这个权衡是[页表](@entry_id:753080)设计中的一个核心问题，现代体系结构（如 x86-64 的 4 级页表和 ARMv8 的 3/4 级页表）正是在这个速度与空间的权衡中取得的平衡。

### 多级[页表](@entry_id:753080)的性能影响

虽然多级[页表](@entry_id:753080)巧妙地解决了内存占用的问题，但它也引入了新的性能开销，主要体现在地址翻译的延迟上。

#### [页表遍历](@entry_id:753086)与内存访问放大

多级页表最大的性能代价在于**[页表遍历](@entry_id:753086)**（page walk）。当发生**转译后备缓冲器**（Translation Lookaside Buffer, TLB）未命中时，硬件必须从内存中逐级读取PTE来完成地址翻译。对于一个 $L$ 级的页表，这个过程在最坏情况下需要 $L$ 次额外的内存访问，才能获取到最终数据所在的物理地址。这被称为**内存访问放大**（memory access amplification）。如果一次内存访问需要 100 纳秒，一个 4 级页表的遍历就可能额外增加 400 纳秒的延迟，极大地拖慢了程序执行速度。

#### 用 EMAT 量化性能

我们可以通过计算**[有效内存访问时间](@entry_id:748817)**（Effective Memory Access Time, EMAT）来量化这种性能影响。EMAT 是 TLB 命中时间和 TLB 未命中时间的加权平均值。

$\text{EMAT} = (1 - r) \cdot t_{hit} + r \cdot t_{miss}$

其中 $r$ 是 TLB 未命中率，$t_{hit}$ 是 TLB 命中时的访问时间（通常等于一次[内存访问时间](@entry_id:164004) $t$），而 $t_{miss}$ 是 TLB 未命中时的访问时间。对于 $L$ 级页表，未命中惩罚包括[页表遍历](@entry_id:753086)的 $L$ 次访存和最终的数据访存，所以 $t_{miss} \approx L \cdot t + t = (L+1)t$。更精确的模型会为[页表](@entry_id:753080)访问和数据访问赋予不同的延迟 [@problem_id:3660547]。

考虑一个以 $s=512$ 字节为步长顺序扫描大数组的程序，系统页大小为 $P=4096$ 字节。由于程序每访问 $P/s = 4096/512 = 8$ 次才会跨越一个页边界，因此 TLB 的未命中率 $r$ 为 $1/8$。假设一次[内存访问时间](@entry_id:164004)为 $t=100$ ns，每次[页表遍历](@entry_id:753086)的附加延迟为 $\alpha=50$ ns，[页表](@entry_id:753080)层级为 $L=4$。那么 EMAT 可以计算为：

$\text{EMAT} = t + r \cdot L \alpha = 100 \text{ ns} + \frac{1}{8} \times (4 \times 50 \text{ ns}) = 100 + 25 = 125 \text{ ns}$

相比于理想情况下的 100 ns，这 25% 的性能下降完全是由 TLB 未命中和随后的[页表遍历](@entry_id:753086)造成的。

#### [上下文切换开销](@entry_id:747798)

多级页表的深度 $L$ 还会影响到**上下文切换**的开销 [@problem_id:3660503]。当[操作系统](@entry_id:752937)从一个进程切换到另一个进程时，它必须更改[页表](@entry_id:753080)基地址寄存器（例如 x86 的 CR3）。这一操作会使处理器中所有与地址翻译相关的缓存失效，包括 TLB 和专门用于加速[页表遍历](@entry_id:753086)的**页结构缓存**（Paging-Structure Caches, PSC）。

每次切换后，新进程的第一次内存访问[几乎必然](@entry_id:262518)导致 TLB 未命中，并触发一次“冷缓存”的[页表遍历](@entry_id:753086)。填充这些缓存的开销与[页表](@entry_id:753080)深度 $L$ 成正比（可建模为 $L\phi$）。如果系统每秒进行 $f$ 次[上下文切换](@entry_id:747797)，总的性能损失将显著增加。系统的有效指令吞吐率 $R(L)$ 可以表示为：

$R(L) = 1 - \frac{f(\delta + L\phi)}{\nu}$

其中 $\delta$ 是切换 CR3 的固定开销，$\nu$ 是处理器频率。这个模型清晰地表明，更深的页表（更大的 $L$）会在频繁进行[上下文切换](@entry_id:747797)的系统中导致更低的整体吞吐量。

### 高级机制与优化

为了使多级[页表](@entry_id:753080)在真实世界中既高效又实用，[操作系统](@entry_id:752937)和硬件共同实现了一系列高级机制和优化。

#### [页表项](@entry_id:753081)（[PTE](@entry_id:753081)）详解

一个现代的 64 位 PTE（通常为 8 字节）远不止包含一个 PFN。它是一个精心设计的、硬件和软件共享的数据结构 [@problem_id:3660530]。

-   **硬件必需字段**：
    -   **存在位 (Present/Valid Bit)**：这是最重要的比特位。如果为 1，表示该 [PTE](@entry_id:753081)有效，且 PFN 字段指向一个有效的物理页帧或下一级[页表](@entry_id:753080)。如果为 0，表示无效。
    -   **页帧号 (Page Frame Number, PFN)**：PTE 的核心数据，指向物理内存。其宽度 $w$ 由物理地址宽度 $P_{addr}$ 和页偏移宽度 $o$ 决定：$w = P_{addr} - o$。例如，在一个支持 52 位物理地址和 4 KiB（12 位偏移）页面的系统中，PFN 至少需要 $52-12=40$ 位。
    -   **权限位 (Permission Bits)**：控制对页面的访问权限，如读（Read）、写（Write）、执行（eXecute）。

-   **“存在位”与层级失效**：存在位在多级页表中具有层级效应。当硬件进行[页表遍历](@entry_id:753086)时，如果在任何一级（非叶子节点）遇到一个存在位为 0 的 [PTE](@entry_id:753081)，翻译过程会立即中止并产生一个页错误（page fault） [@problem_id:3660461]。这意味着，一个祖先节点的 [PTE](@entry_id:753081) 无效，会导致其下的整个子树（可能覆盖 GB 甚至 TB 级别的地址空间）都变得不可访问。这个机制使得[操作系统](@entry_id:752937)可以通过简单地清除一个高层[PTE](@entry_id:753081)的存在位，来高效地取消对巨大地址区域的映射。这也引出一个重要的**不变性**：一个被分配（instantiated）的[页表](@entry_id:753080)，其所有祖先[PTE](@entry_id:753081)的存在位都必须为1。验证这一[不变性](@entry_id:140168)需要从根开始遍历[页表](@entry_id:753080)树，并对存在位为0的分支进行剪枝。

-   **[操作系统](@entry_id:752937)管理的字段**：
    -   **访问位 (Accessed Bit)** 和 **[脏位](@entry_id:748480) (Dirty Bit)**：由硬件在页面被读取或写入时自动设置，供[操作系统](@entry_id:752937)的页面替换算法（如 LRU 的变体）使用。
    -   **[操作系统](@entry_id:752937)专用[元数据](@entry_id:275500)**：在一个 64 位的 [PTE](@entry_id:753081) 中，除去硬件必需的 PFN 和各种标志位后，通常还会有一些剩余的比特。例如，在上述 52 位物理地址的例子中，64 位 PTE 中有 $64 - (40+6) = 18$ 个比特可供使用。[操作系统](@entry_id:752937)可以利用这些比特来存储自己的元数据。然而，直接在**有效**[PTE](@entry_id:753081)中使用这些比特存在风险，因为硬件可能将其保留作未来使用。更聪明和普遍的做法是利用**无效**[PTE](@entry_id:753081)。当存在位为 0 时，硬件会忽略 [PTE](@entry_id:753081) 中的 PFN 等字段，这时[操作系统](@entry_id:752937)就可以安全地 repurpose 这些比特来存储例如页面在[交换空间](@entry_id:755701)（swap space）中的位置等信息。

#### [巨页](@entry_id:750413)与 TLB 覆盖范围

前面我们看到，TLB 未命中的代价非常高。对于需要访问大量内存的应用程序（如数据库、科学计算），一个严峻的问题是 **TLB 覆盖范围**（TLB Reach）不足。TLB Reach 指的是 TLB能够同时缓存的地址翻译所覆盖的总内存大小，等于 TLB 条目数乘以页大小（$R = T \times P$）。

在一个拥有 1024 个条目的 TLB 和 4 KiB 页面的系统中，TLB Reach 仅为 $1024 \times 4 \text{KiB} = 4 \text{MiB}$ [@problem_id:3660516]。如果一个应用程序的[工作集](@entry_id:756753)（working set）有几百 MiB 或数 GiB，那么程序将频繁地访问 TLB 无法覆盖的内存区域，导致 TLB 不断[抖动](@entry_id:200248)（thrashing），性能急剧下降。

解决方案是支持多种页大小，特别是**[巨页](@entry_id:750413)**（Huge Pages），例如 2 MiB 或 1 GiB。在多级[页表结构](@entry_id:753084)中，一个较高层级（例如二级）的 PTE 可以被特殊标记，使其直接映射一个大的物理内存块（例如 2 MiB），从而结束[页表遍历](@entry_id:753086)，而不必再访问更低层级的页表。

[巨页](@entry_id:750413)的威力在于它能极大地增加 TLB Reach。一个 TLB 条目现在可以映射一个 2 MiB 或 1 GiB 的区域，而不是区区 4 KiB。考虑一个拥有三种不同大小页面的 TLB 的系统，其工作集为 16 GiB。
-   如果只使用 4 KiB 页面，TLB Reach 为 4 MiB，只能覆盖工作集的 $4 \text{MiB} / 16 \text{GiB} = 1/4096$，未命中率极高。
-   如果[操作系统](@entry_id:752937)智能地使用 1 GiB 页面映射一部分工作集，用 2 MiB 页面映射另一部分，剩下的用 4 KiB 页面，总的 TLB Reach 将是各个 TLB Reach 之和。根据 [@problem_id:3660516] 中的计算，混合大小页面的 TLB Reach 可以达到 $4 \text{GiB} + 256 \text{MiB} + 4 \text{MiB}$，覆盖了[工作集](@entry_id:756753)的很大一部分。
-   其结果是 TLB 未命中率的显著降低。在该问题的具体参数下，从纯 4 KiB 页面切换到混合页面策略，TLB 未命中率的变化 $\Delta r$ 为 $-0.265625$，即未命中率降低了超过 26.5%。对于内存密集型应用，这种优化是提升性能的关键。

综上所述，多级[页表](@entry_id:753080)是一个精妙的、充满权衡的工程解决方案。它以可接受的性能代价，成功地解决了[虚拟内存管理](@entry_id:756522)中的规模扩展问题，并通过与 TLB、[巨页](@entry_id:750413)等硬件机制的协同工作，为现代计算机系统提供了强大而高效的内存抽象。