## 引言
连续[内存分配](@entry_id:634722)是[操作系统内存管理](@entry_id:752942)中最基本也最直观的模型之一，它将进程完整地加载到物理内存的单一连续区域中。尽管概念简单，但在进程动态创建和销毁的真实环境中，这种方法会不可避免地导致一个棘手的问题：[内存碎片](@entry_id:635227)化。随着时间的推移，内存空间会变得千疮百孔，即使总空闲量充足，也可能无法满足新进程的分配请求，从而严重影响系统效率和[吞吐量](@entry_id:271802)。

本文旨在系统性地剖析连续[内存分配](@entry_id:634722)的全貌，从理论根源到实际应用。在接下来的内容中，我们将分三个部分展开：

*   在 **“原理与机制”** 一章中，我们将深入探讨[外部碎片](@entry_id:634663)的成因，比较首次适应、最佳适应等经典分配策略的优劣，并解析内存压缩与[动态重定位](@entry_id:748749)等核心技术。
*   在 **“应用与跨学科连接”** 一章中，我们将视野扩展到真实世界，考察[连续分配](@entry_id:747800)在现代硬件（如DMA和GPU）、实时系统以及文件系统和算法理论等领域的具体应用和深远影响。
*   最后，在 **“动手实践”** 部分，你将通过解决具体问题来动手实践和巩固所学知识。

让我们首先深入“原理与机制”一章，从连续[内存分配](@entry_id:634722)的核心原理及其所面临的根本性挑战开始我们的探索。

## 原理与机制

在本章中，我们将深入探讨连续[内存分配](@entry_id:634722)的核心原理与关键机制。连续[内存分配](@entry_id:634722)，顾名思义，要求将一个进程的完整地址空间存放在物理内存中一个单一、不间断的区域。虽然这个概念直观，但它在实际的动态系统中所引发的挑战，以及为应对这些挑战而设计的精巧机制，构成了[操作系统内存管理](@entry_id:752942)领域的基石。我们将从问题的根源——碎片化——出发，系统地剖析分配策略、碎片整理技术及其背后的权衡。

### 根源性挑战：[外部碎片](@entry_id:634663)

在多道程序环境中，内存的使用是动态的：进程被创建、加载入内存，然后执行完毕、被移出内存。这个过程周而复始，使得内存空间逐渐变成一幅由已分配区域和空闲区域（称为**空闲块**或**孔洞**）交织而成的“马赛克”。[连续分配](@entry_id:747800)的核心约束——要求为新进程找到一个足够大的、单一的连续空闲块——在这种动态变化中会遇到一个严重的问题，即**[外部碎片](@entry_id:634663) (External Fragmentation)**。

[外部碎片](@entry_id:634663)描述了这样一种情况：系统中存在足够多的总空闲内存来满足一个新进程的请求，但这些空闲内存被分割成多个互不相邻的小空闲块，导致没有任何一个单独的空闲块足够大以容纳该进程。

让我们通过一个具体的场景来理解这个概念[@problem_id:3628253]。假设一个系统的总内存为 $1024$ KiB。在某一时刻，内存中散布着多个空闲块，其大小分别为 $96$ KiB、$64$ KiB、$128$ KiB、$32$ KiB 和 $96$ KiB。此刻，空闲内存的总量 $S$ 为：

$$S = 96 + 64 + 128 + 32 + 96 = 416 \text{ KiB}$$

此时，一个新进程到达，请求一个大小为 $r=200$ KiB 的连续内存块。尽管总空闲内存 $S = 416$ KiB 远大于请求量 $r=200$ KiB，但分配请求却会失败。原因在于，当前可用的最大单个连续空闲块仅为 $128$ KiB，无法满足 $200$ KiB 的需求。那些总量充足但无法被利用的、散布在已分配块之间的空闲内存，就是[外部碎片](@entry_id:634663)的具体体现。

值得注意的是，[外部碎片](@entry_id:634663)不同于**[内部碎片](@entry_id:637905) (Internal Fragmentation)**。[内部碎片](@entry_id:637905)发生在一个分配给进程的内存块大于进程实际所需内存时，该内存块内部的未使用部分。例如，如果内存总是以 $4$ KiB 的块为单位进行分配，一个请求 $1$ KiB 的进程将被分配一个 $4$ KiB 的块，从而产生 $3$ KiB 的[内部碎片](@entry_id:637905)。而我们刚才讨论的场景中，问题出在空闲块*之间*，而非*之内*。

### [动态存储分配](@entry_id:748754)策略

既然[外部碎片](@entry_id:634663)是[连续分配](@entry_id:747800)的主要障碍，[操作系统](@entry_id:752937)必须采用智能的策略来从众多空闲块中选择一个进行分配，以期减缓碎片的产生。当一个大小为 $r$ 的请求到达时，分配器会搜索空闲块列表。如果找到了一个大小为 $s$（其中 $s \ge r$）的空闲块，分配器会将其分配给进程。如果 $s > r$，该空闲块会被分割成两部分：一个大小为 $r$ 的已分配块，和一个大小为 $s-r$ 的新的、较小的空闲块。以下是三种经典的分配策略：

1.  **首次适应 (First-Fit)**：从空闲块列表的开头开始搜索，选择第一个大小足以满足请求的空闲块。这个策略速度快，因为它不必检查所有空闲块。

2.  **最佳适应 (Best-Fit)**：搜索整个空闲块列表，选择那个大小足以满足请求的、且尺寸最小的空闲块。这种策略试图通过选择最“紧凑”的匹配来保留大的空闲块，以备将来大请求之需。

3.  **最差适应 (Worst-Fit)**：搜索整个空闲块列表，选择尺寸最大的空闲块。其逻辑是，从最大的空闲块中切分出一小部分后，剩余的空闲块仍然会相当大，从而可能更有用。

不同的策略在不同场景下的表现差异巨大。让我们通过一个例子来揭示首次适应和最佳适应的内在行为差异[@problem_id:3628281]。假设内存中有四个空闲块，按地址顺序[排列](@entry_id:136432)，大小分别为 $\{500, 200, 200, 200\}$。现在有一系列请求到达：$\{190, 190, 190, 500\}$。

-   **首次适应的策略**：
    1.  请求 $190$：首次适应会选择第一个块 $500$，将其分割为已分配的 $190$ 和剩余的 $310$。空闲列表变为 $\{310, 200, 200, 200\}$。
    2.  请求 $190$：再次选择第一个块 $310$，分割后剩余 $120$。空闲列表变为 $\{120, 200, 200, 200\}$。
    3.  请求 $190$：跳过 $120$，选择第二个块 $200$，分割后剩余 $10$。空闲列表变为 $\{120, 10, 200, 200\}$。
    4.  请求 $500$：此时，最大的空闲块仅为 $200$，请求失败。这是典型的[外部碎片](@entry_id:634663)问题，因为总空闲内存为 $120+10+200+200 = 530$，是大于 $500$ 的。

-   **最佳适应的策略**：
    1.  请求 $190$：最佳适应会搜索整个列表，找到最小的能满足请求的块。在 $\{500, 200, 200, 200\}$ 中，最佳选择是一个 $200$ 的块，分割后剩余 $10$。空闲列表变为 $\{500, 10, 200, 200\}$。
    2.  请求 $190$：同样，它会选择另一个 $200$ 的块。空闲列表变为 $\{500, 10, 10, 200\}$。
    3.  请求 $190$：再次选择最后一个 $200$ 的块。空闲列表变为 $\{500, 10, 10, 10\}$。
    4.  请求 $500$：此时，那个从未被小请求染指的 $500$ 大小的块仍然存在，请求可以被成功满足。

这个例子鲜明地展示了最佳适应策略在某些情况下通过“节约”大空闲块来避免碎片化的优势。然而，它的缺点是每次分配都必须搜索整个列表，开销较大，并且它倾向于产生大量非常小的、几乎无用的碎片。

而最差适应策略则有其自身的困境[@problem_id:3628328]。它的初衷是好的：通过总是从最大的块中分配，来保证剩余的空闲块也尽可能大。但这种做法可能导致系统中快速地失去所有的大块。考虑一个场景，初始时有一个 $300$ MiB 的大块和几个 $40$ MiB 的小块。一系列 $10$ MiB 的小请求和 $90$ MiB 的中等请求交替到来。最差适应会持续地从那个 $300$ MiB 的大块中分配，因为在很长一段时间内它都是最大的。这会导致这个大块被逐渐“蚕食”，直到其大小降到 $90$ MiB 以下，从而无法再满足中等请求，尽管那些 $40$ MiB 的块从未被使用过。

### 空闲列表管理：合并与排序

分配策略的有效性还依赖于空闲列表自身的管理。当一个进程终止并释放其内存时，这块内存就成了一个新的空闲块。如果这个新空闲块恰好与一个或多个已有的空闲块在物理上相邻，那么将它们**合并 (Coalescing)** 成一个更大的空闲块就至关重要。合并是抵抗[外部碎片](@entry_id:634663)化的主要手段。

关于何时进行合并，存在两种策略：

-   **即时合并 (Eager Coalescing)**：在释放内存块时，立即检查其物理地址相邻的块是否也为空闲。如果是，则将它们合并。
-   **延迟合并 (Lazy Coalescing)**：在释放时不进行检查，只是简单地将释放的块加入空闲列表。[合并操作](@entry_id:636132)被推迟到稍后进行，例如，当分配请求失败时，系统可能会触发一次全局合并。

即时合并听起来更优，因为它能更快地创建出大空闲块。然而，其性能影响是微妙的，且与空闲列表的组织方式密切相关。让我们考虑一个复杂的场景[@problem_id:3628307]，其中空闲列表按后进先出（LIFO）顺序维护，即新释放的块被置于列表头部。

假设我们释放了一个大小为 $6$ 的块 $X$，它的物理邻居 $Y$（大小为 $1$）和 $Z$（大小为 $1$）也恰好是空闲的。紧接着，一个大小为 $6$ 的分配请求到达。

-   在**延迟合并**策略下：块 $X$ 被直接放到空闲列表的头部。当大小为 $6$ 的请求到达时，首次适应分配器检查的第一个块就是 $X$，大小正好匹配。分配成功，仅检查了 $1$ 个节点。

-   在**即时合并**策略下：系统检测到 $X$ 与 $Y$ 和 $Z$ 相邻，将它们合并成一个大小为 $8$ 的新块 $M$。根据特定的实现规则，这个新块 $M$ 可能会被插入到其合并前组件在列表中最先出现的位置。如果 $Y$ 和 $Z$ 在空闲列表中处于较深的位置，那么新块 $M$ 也会被放置在列表深处。当大小为 $6$ 的请求到达时，分配器可能需要遍历列表头部的多个较小的、不满足条件的块，才能最终找到块 $M$。在这个特定的例子中，可能需要检查 $6$ 个节点才能完成分配。

这个例子说明，看似“更优”的即时合并策略，由于其对空闲列表顺序的副作用，在某些情况下反而会增加后续分配的搜索开销。[操作系统](@entry_id:752937)设计充满了这类权衡。

### 应对碎片的终极手段：压缩与重定位

分配策略和合并只能减缓碎片，但无法根除它。当碎片化变得过于严重，以至于小请求都难以满足时，[操作系统](@entry_id:752937)可能需要采取更激进的措施：**内存压缩 (Memory Compaction)**，也称为碎片整理。

#### 内存压缩的机制

内存压缩通过移动内存中已分配的进程，将它们全部推向内存的一端，从而把所有零散的空闲块合并成一个完整的大空闲块[@problem_id:3628253]。这是一个开销极大的操作，因为它需要暂停系统，并复制大量数据。

然而，更大的挑战在于：如何移动一个正在运行的进程而不“破坏”它？进程的程序代码中包含了大量的地址引用，如[函数调用](@entry_id:753765)、全局变量访问和指针。如果物理地址被写死在代码中，那么移动进程后，这些地址就会全部失效。

解决方案是区分**[逻辑地址](@entry_id:751440) (Logical Address)** 和**物理地址 (Physical Address)**，并引入硬件支持进行**[动态重定位](@entry_id:748749) (Dynamic Relocation)**。在一种经典的实现中，每个进程都关联着两个特殊的硬件寄存器：**基址寄存器 (Base Register)** 和**界限寄存器 (Limit Register)**。

-   CPU在执行进程代码时，生成的是[逻辑地址](@entry_id:751440)，即相对于进程起始地址 $0$ 的偏移量。
-   当一个[逻辑地址](@entry_id:751440) $\ell$ 被用于访问内存时，[内存管理单元](@entry_id:751868)（MMU）硬件会首先检查它是否有效，即 $0 \le \ell  L_{\text{max}}$，其中 $L_{\text{max}}$ 是存储在界限寄存器中的进程大小。
-   如果地址有效，MMU会动态地将其转换为物理地址 $a_{\text{phys}}$：$a_{\text{phys}} = B + \ell$，其中 $B$ 是存储在基址寄存器中的该进程的物理起始地址。

这个机制的精妙之处在于，进程本身完全不知道自己位于物理内存的哪个位置[@problem_id:3628278]。它只在自己的[逻辑地址](@entry_id:751440)空间（从 $0$ 到 $L_{\text{max}}-1$）内运行。当[操作系统](@entry_id:752937)需要进行内存压缩时，它可以将一个进程的全部内存内容从一个物理位置复制到另一个新的物理位置，然后**只需更新该进程的基址寄存器**为新的起始地址。之后进程恢复执行，所有内存访问都会被硬件自动、透明地重定向到新的位置，而进程内部的任何代码和数据（包括所有指针）都无需修改。

#### 压缩的安全性和局限性

尽管[动态重定位](@entry_id:748749)非常强大，但它并非万能。它只对通过CPU和MMU的内存访问有效。某些系统组件，特别是执行**直接内存访问 (Direct Memory Access, DMA)** 的I/O设备，通常会绕过CPU和MMU，直接与物理内存交互。这些设备控制器通常被编程以使用绝对的物理地址。

这意味着，如果在进行内存压缩时，一个DMA操作正在向某进程的物理缓冲区传输数据，移动该缓冲区将导致数据被写入错误的位置，造成[数据损坏](@entry_id:269966)。因此，一个安全的内存压缩过程必须是一个精心设计的、原子性的操作[@problem_id:3628298]：

1.  **暂停活动**：暂停所有需要被移动的用户进程，并确保所有针对这些进程内存区域的DMA操作已经完成或被安全地暂停（静默）。
2.  **内存复制**：按计划将进程的内存块复制到新的位置。这里需要注意一个实现细节：如果源区域和目标区域有重叠，必须选择正确的复制方向（从高地址到低地址，或从低地址到高地址）以避免在复制过程中覆盖尚未复制的数据。
3.  **更新状态**：这是最关键的一步。[操作系统](@entry_id:752937)必须更新所有与被移动进程相关的地址引用。这包括：
    *   内核[数据结构](@entry_id:262134)（如进程控制块PCB）中记录的进程基址。
    *   硬件中的基址寄存器。
    *   所有被暂停的DMA操作描述符中的物理缓冲区地址。
4.  **恢复执行**：在所有状态都已更新完毕后，恢复被暂停的进程和DMA操作。

这个过程也揭示了连续[内存分配](@entry_id:634722)的物理本质。有人可能会设想，是否可以用软件“欺骗”硬件，将几个不相邻的空闲块“粘合”起来提供给一个需要连续内存的进程[@problem_id:3628311]。例如，用一些“填充字节”来桥接小间隙。这种想法从根本上是行不通的。无论是CPU的MMU还是DMA控制器，这些硬件在执行连续访问时，都只是简单地在一个物理地址上进行递增操作。它们无法理解“跳过一个间隙”或“读取填充字节”这样的软件概念。如果一个进程的地址范围在硬件层面覆盖了一个不属于它的间隙，硬件将毫无保留地访问该间隙中的内存，这可能属于另一个进程或操作系统内核，从而导致系统崩溃或[数据损坏](@entry_id:269966)。

### 定量分析与现实考量

到目前为止，我们的讨论大多是定性的。然而，在真实系统中，许多决策都基于定量的权衡。

#### 内存压缩的成本效益

内存压缩开销巨大，但它能换来未来分配效率的提升。那么，何时进行压缩才是值得的？我们可以建立一个简单的模型来分析这个问题[@problem_id:3628301]。

假设：
-   移动一个字节的CPU成本为 $c_m$。
-   当前已分配的总内存为 $A$ 字节。
-   在不压缩的情况下，每次分配平均需要检查 $1/p$ 个空闲块，其中 $p$ 是检查任意一个块就能满足请求的概率。
-   检查一个空闲块的CPU成本为 $c_s$。
-   压缩后，所有空闲空间合并为一个大块，每次分配只需检查 $1$ 个块。

压缩的一次性成本是移动所有已分配内存，即 $C_{\text{compaction}} = A \cdot c_m$。
在接下来的 $N$ 次分配中，与不压缩相比，每次分配节省的搜索成本是 $c_s (\frac{1}{p} - 1)$。$N$ 次分配的总节省是 $S_{\text{saved}}(N) = N \cdot c_s (\frac{1-p}{p})$。

当总节省等于一次性成本时，我们达到了盈亏[平衡点](@entry_id:272705)。令 $C_{\text{compaction}} = S_{\text{saved}}(N^{\star})$，我们可以解出需要多少次未来的分配才能“收回”压缩的成本：

$$N^{\star} = \frac{A c_m p}{c_s (1-p)}$$

这个公式告诉我们，当已分配内存 $A$ 很大、移动成本 $c_m$ 很高时，需要更多的未来分配才能证明压缩的合理性。反之，如果搜索成本 $c_s$ 很高，或者碎片化严重（$p$ 很小），那么进行压缩就更有吸[引力](@entry_id:175476)。

#### 对齐约束的影响

真实世界的硬件通常有**对齐 (Alignment)** 要求。例如，一个4字节的整数可能必须存放在能被4整除的地址上。[内存分配](@entry_id:634722)器必须遵守这些约束，这会进一步加剧碎片化。

考虑一个场景，分配器必须将每个块的起始地址安排在 $256$ 字节的倍数上[@problem_id:3628331]。当一个请求到达时，即使找到了一个足够大的空闲块，分配器也可能无法从该块的起始位置开始分配，而必须向后移动到第一个满足对齐要求的地址。这会在空闲块的内部产生一个小的、新的、可能无法使用的空闲碎片。经过一系列分配和释放后，这些因对齐而产生的“内部”碎片会显著增加总的[外部碎片](@entry_id:634663)量，有时甚至会使其成倍增加。

#### [内存泄漏](@entry_id:635048)的长期后果

最后，让我们考虑一个常见但致命的软件缺陷：**[内存泄漏](@entry_id:635048) (Memory Leak)**。当一个程序分配了内存但之后忘记释放它，这块内存就成了永久性的、不可回收的分配。在连续[内存模型](@entry_id:751871)中，即使是一个极小的、长期存在的[内存泄漏](@entry_id:635048)，其后果也可能非常严重[@problem_id:3628268]。

假设在广阔的内存中，有一个大小为 $s$ 的小块被永久泄漏，其位置在 $X$。随着时间的推移，所有其他合法的、临时的分配都会被创建和释放。最终，除了这个泄漏的块，内存中的所有其他空间都会变为空闲。

然而，由于这个泄漏的块像一个无法移除的“钉子”一样固定在内存中，它将总空闲内存（大小为 $M-s$）分割成了两部分：一块大小为 $X$，另一块大小为 $M-s-X$。由于这两块不相邻，它们无法被合并。因此，系统能够分配的最大连续内存块不再是总空闲内存 $M-s$，而是 $\max\{X, M-s-X\}$。

如果这个泄漏块的位置是随机的，我们可以计算出它对系统造成的平均损害。一个令人惊讶的数学结果是，如果泄漏位置 $X$ 在 $[0, M-s]$ 上[均匀分布](@entry_id:194597)，那么期望的最大可分配块大小是 $\frac{3}{4}(M-s)$。这意味着，平均而言，一个微不足道的泄漏会导致总可用空闲内存中的 $25\%$ 变得无法用于单个大块分配！这个结论深刻地揭示了在连续内存系统中，[外部碎片](@entry_id:634663)是多么根深蒂固且具有破坏性。这也正是促使[操作系统](@entry_id:752937)设计者最终转向更复杂的[非连续内存分配](@entry_id:752553)方案（如分页）的根本原因，我们将在下一章探讨这些方案。