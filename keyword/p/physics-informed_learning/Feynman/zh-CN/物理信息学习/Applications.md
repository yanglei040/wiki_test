## 应用与跨学科联系

我们花了一些时间来了解[物理信息学习](@article_id:297248)的机制——即如何构建一个能“说”[微分方程](@article_id:327891)语言的神经网络的具体细节。但是，一个新工具的好坏取决于它能让我们做哪些新事情，问哪些新问题。现在，我们的旅程将走出工作室，进入广阔的实践领域。我们将见证这些思想如何不仅仅是一种学术上的好奇心，而是我们探索、理解甚至创造周围世界的一副强大透镜。我们将看到它们如何解决棘手的方程，发现基本的对称性，构思出新颖的工程设计，并在迥然不同的科学领域之间搭建桥梁。这才是魔法真正开始的地方。

### 为求解器增压：解决旧方程的新方法

几个世纪以来，求解[偏微分方程](@article_id:301773)（PDEs）一直是科学和工程的核心任务。这些方程描述了从金属棒中的热流到池塘中水波涟漪的一切。传统上，我们使用复杂的、基于网格的数值方法来求解它们。物理信息神经网络（[PINNs](@article_id:305653)）提供了一种完全不同，且在许多方面更为优雅的理念。

想象一下，你想描述一根一维杆的一端突然被加热后热量如何传播。这由[热方程](@article_id:304863)这个经典的[偏微分方程](@article_id:301773)所控制。PINN 不仅仅着眼于方程本身；它一次性审视*整个问题*。它将物理定律（[热方程](@article_id:304863)本身）、初始状态（零时刻的温度分布）和边界条件（杆两端发生的情况）组合成一个单一的目标，或称“损失函数”。然后，网络调整其内部参数，直到其输出——一个关于空间和时间的[光滑函数](@article_id:299390) $\hat{u}(x,t)$——同时满足所有这些约束。这是一种整体性方法，其中方程与其背景密不可分，就像在自然界中一样。

这很强大，但当物理问题变得异常困难时会怎样？考虑[浅水方程](@article_id:323387)，它可以描述像溃坝或[潮涌](@article_id:323734)这样的现象。这些系统会产生[激波](@article_id:302844)——即尖锐的、移动的间断，在这些地方，像水高这样的量会突然改变。标准的数值方法在处理数据中的这些“悬崖”时非常吃力，而一个朴素的 PINN 也不例外。这时我们可以给我们的网络上一堂物理课。我们可以不要求它从头发现[激波](@article_id:302844)的概念，而是将[激波](@article_id:302844)的*结构*直接构建到其架构中。我们可以设计网络来产生一个平滑的过渡，也许使用像[双曲正切](@article_id:640741)这样的函数，然后让它学习关键的物理参数，例如[激波](@article_id:302844)的速度和位置。通过最小化从基本守恒定律（Rankine-Hugoniot 条件）推导出的损失函数，网络可以正确地推断出[激波](@article_id:302844)的速度。这是一场美妙的对话：我们提供解的一般形式，而网络在物理的指导下，填充细节。

### 洞察结构的力量：[归纳偏置](@article_id:297870)与对称性

物理学中最深刻的思想之一是对称性。如果我们移动或旋转我们的实验室，或者明天而不是今天进行实验，自然法则不会改变。一个好的世界模型，理应尊重这些相同的对称性。这个原则为我们设计更好的学习架构提供了一种强有力的方式。我们可以将物理世界的对称性直接构建到网络本身中。这被称为给网络一个*[归纳偏置](@article_id:297870)*。

让我们看看这会带来多大的不同。假设我们正在学习一个简单的一维物理系统的行为，该系统是平移不变的——也就是说，沿着这条线，物理规律处处相同。我们可以尝试用一个通用的、全连接的神经网络（MLP）来学习这个。如果我们用在某一点上“戳”一下系统会发生什么来训练它，它会学会预测对那个特定“戳”的响应。但如果我们随后在另一个不同的地方“戳”系统，MLP 将会完全不知所措。它没有学会*规则*；它只记住了一个*事件*。

现在，考虑一个[卷积神经网络](@article_id:357845)（CNN）。CNN 通过其将小滤波器在输入上滑动的架构，其DNA中就内建了平移对称性。它天生就明白同样的规则适用于任何地方。如果我们用同样一个单一的“戳”来训练这个CNN，它会学会底层的算子——“格林函数”或脉冲响应。现在，如果我们在其他任何地方“戳”它，它都能正确预测响应。它从单一信息中完美地泛化了，因为它的结构反映了物理的结构。这就像给网络戴上了一副眼镜，让它能看到世界固有的对称性。

这个想法远远超出了简单的平移。考虑像苯这样的分子。它的能量和其他化学性质不取决于它在空间中的朝向。一个预测其性质的模型应该对旋转和反射保持不变。我们可以构建具有这种内置属性的[图神经网络](@article_id:297304)（GNNs）。通过确保网络的计算只依赖于不变的量，比如原子间的距离，网络就自动尊重了分子的[几何对称性](@article_id:368160)。这样的[等变网络](@article_id:304312)不仅更准确，而且数据效率也高得多，因为它不需要看到分子在所有可能的朝向下才能学习到底层的物理规律。它已经知道了规则：朝向无关紧要。

### 从预测到设计：[逆问题](@article_id:303564)的艺术

到目前为止，我们一直在使用[物理信息学习](@article_id:297248)来回答“如果……会怎样？”的问题——即*正问题*。给定一个设置，会发生什么？但一个更令人兴奋的前沿是*逆问题*：给定一个[期望](@article_id:311378)的结果，我需要构建什么样的设置来实现它？这是工程和设计的核心。

想象一下，你想设计一种可在润滑时最小化摩擦的微观[表面纹理](@article_id:364490)。这是一个涉及[流体动力学](@article_id:319275)的极其复杂的问题。你可以在计算机模拟中尝试成千上万种不同的纹理，但这会非常缓慢。在这里，一个可微的[代理模型](@article_id:305860)就派上了用场。首先，我们运行大量高保真模拟来生成一个小数据集。然后，我们训练一个[神经网络](@article_id:305336)作为一个快速、近似的模拟器或代理模型，它可以为人设计的任何给定纹理预测摩擦力和承载能力。因为网络是可微的，我们可以问它：“如果我想减少摩擦，我应该朝哪个方向改变纹理的参数？”网络的梯度提供了答案。然后我们可以使用梯度下降来自动地在所有可能设计的空间中“行走”，走向一个最优的、低摩擦的表面。计算机不再只是一个计算器；它是设计过程中的一个创造性伙伴。

我们可以利用生成模型将此更进一步。我们能否训练一台机器成为一个“设计引擎”，能够按需生成无数新颖、高性能的设计，而不仅仅是找到一个最优设计？使用[条件生成对抗网络](@article_id:638458)（cGAN）可以实现这一点。我们可以训练一个*生成器*网络，在给定目标[摩擦系数](@article_id:361445)的条件下，提出一种纳米纹理。但我们如何确保它提出的方案是好的呢？我们用一个巧妙的技巧：给它两位老师。第一位是标准的 GAN *判别器*，一个“艺术评论家”，确保生成的纹理看起来像它在训练数据集中见过的那些逼真纹理。第二位，也是更重要的老师，是一个*基于物理的判别器*。这个第二位评论家不是一个需要训练的[神经网络](@article_id:305336)；它是接触力学方程的直接实现。它接收生成的设计，并根据已建立的物理定律计算该设计是否可行（例如，在压力下不会断裂）以及是否真正达到了目标摩擦系数。因此，生成器被迫学习一个受物理定律的严酷现实所约束的创造过程。它学会的不仅是梦想，更是梦想那些可以被实际建造出来的事物。

### 搭建桥梁：一门跨学科科学的语言

也许[物理信息学习](@article_id:297248)最深远的影响在于它能够为截然不同的科学领域创造一种通用语言。它提供了一个框架，在这个框架中，抽象的数学算子、杂乱的实验数据和基本的物理定律都可以被编织在一起。

例如，在固体力学中，我们常常想知道一个结构在给定载荷下将如何变形。[力场](@article_id:307740)与由此产生的[位移场](@article_id:301917)之间的关系是一个复杂的算子。深度算子网络（DeepONets）正是为了学习这[类函数](@article_id:307386)之间的映射而设计的。通过使用有限元模拟数据训练 DeepONet，我们可以创建一个模型，作为一整类材料或几何结构的通用“求解器”。但要在一个复杂的真实物体上有效地做到这一点，网络的架构必须被问题的几何形状所告知。例如，不仅向网络提供一个点的坐标，还提供它到边界的距离，这样可以让它学习到在边缘和角落发生的特殊物理现象。我们再次看到，最成功的模型是那些在设计时考虑了物理和几何背景的模型。

这种理念也改变了我们对使用数据的看法。一个多世纪以来，工程师们一直依赖经验关联式——从实验中推导出的简单公式，比如那些描述传热的公式。这些关联式非常宝贵，但它们的准确性或适用范围往往有限。我们可以不把它们扔掉，用“黑箱”[神经网络](@article_id:305336)从零开始，而是用机器学习来*增强*它们。我们可以将真实的努塞尔数 $\mathrm{Nu}$ 建模为[经典关联](@article_id:296821)式 $\mathrm{Nu}_{0}$ 乘以一个学习到的校正因子 $C_{\theta}$。[神经网络](@article_id:305336)的工作不是重新学习整个传热学，而是学习经典模型所遗漏的微小、微妙的校正。至关重要的是，我们可以设计这个学习过程，使其尊重我们已知的所有物理规律。我们可以强制校正因子为正，在物理学要求的范围内单调，并在我们知道经典理论是精确的渐近区域内消失。这种“灰箱”方法是一种强大而谦逊的方法；它将过去的智慧与现代数据科学的力量相结合。

这种协作精神在[系统生物学](@article_id:308968)和药理学等领域的[交叉](@article_id:315017)点上得到了终[极体](@article_id:337878)现。想象一下模拟一群癌细胞如何对药物做出反应。这个系统的一些部分是众所周知，比如药物在培养基中浓度如何演变的[药代动力学](@article_id:296934)。但其他部分，比如导致[细胞增殖](@article_id:332074)或死亡的细胞内复杂的信号通路网络，则是一个错综复杂的谜团。在这里，我们可以用神经普通[微分方程](@article_id:327891)（Neural ODE）构建一个优美的[混合模型](@article_id:330275)。我们可以硬编码已知的物理学（药物的衰减率），同时让[神经网络](@article_id:305336)学习未知的生物动力学。通过使用一个名为[状态增广](@article_id:301312)的巧妙数学技巧——将像药物剂量率这样的实验参数视为系统的一个状态——我们可以在来自许多不同实验的数据上训练一个单一、统一的模型。最终的模型是一个嵌合体，一部分是经典[药理学](@article_id:302851)，一部分是数据驱动的生物学，比两者单独存在时都更强大。

### 一场新的对话

正如我们所见，[物理信息学习](@article_id:297248)远不止是一套[算法](@article_id:331821)。它是21世纪科学探究的一种哲学。它鼓励在我们的原则与数据之间、在我们的理论模型与它们试图描述的纷繁现实之间进行持续的对话。它为我们提供了解决最古老方程的新方法，将我们最深刻的物理直觉[嵌入](@article_id:311541)人工智能，颠覆发现过程并设计未来，以及找到一个不同科学语言可以相遇并创造新事物的共同点。它向我们表明，自然法则不仅是要遵守的约束，更是能够教导我们最先进的计算工具如何以物理学家的清晰、直觉和好奇心来看待世界的向导。