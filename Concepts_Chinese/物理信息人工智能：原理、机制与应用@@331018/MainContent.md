## 引言
在人工智能领域，传统模型的一个显著局限是它们对海量数据的依赖，模型学习到的相关性在其训练条件之外可能变得脆弱且不可靠。这些“黑箱”系统通常无法掌握支配系统的基本因果定律，从而限制了它们在新场景中的预测能力。物理信息人工智能（PIAI）作为一种变革性[范式](@entry_id:161181)应运而生，旨在解决这一差距，试图为[神经网](@entry_id:276355)络注入自然法则。本文将全面介绍这一激动人心的领域。首先，在“原理与机制”部分，我们将深入探讨 PIAI 的核心工作方式，探索物理定律如何被编码到损失函数中，以及训练这些复杂模型所面临的挑战。随后，“应用与跨学科联系”部分将展示这种方法的深远影响，从发现新的科学定律到设计新型材料，再到创建[生物系统](@entry_id:272986)的[数字孪生](@entry_id:171650)。让我们从理解赋予这种人工智能物理良知的基本原理开始。

## 原理与机制

想象一下，你想教一个学生预测抛出小球的运动轨迹。一种方法是给他看成千上万个抛球的视频，让他记住每个视频中的轨迹。这是传统的机器学习方法。这个学生可能会在预测与视频中完全相同方式抛出的小球轨迹方面做得很好。但如果你问他关于在月球上抛球，或者在飓风中抛羽毛的问题，他会完全不知所措。他学到的是相关性，而不是因果关系。

还有另一种方法。你可以教给学生[牛顿运动定律](@entry_id:163846)和[空气动力学](@entry_id:193011)原理。现在，学生不需要看遍所有可能的情景。面对一个新问题——一个新的物体、一个新的环境——他可以应用这些基本定律来找出答案。他学到的是*物理学*。这就是物理信息人工智能背后的思维飞跃。我们不再满足于仅仅是出色记忆者的网络；我们想教给它们自然法则。

### 核心所在：有物理良知的损失函数

我们如何教[神经网](@entry_id:276355)络物理学呢？我们不能仅仅让它读教科书。[神经网](@entry_id:276355)络的语言是数学，特别是优化的数学。网络通过尝试最小化一个**损失函数**来学习——这个函数是一个数字，告诉它当前的预测有多“错误”。整个学习过程就是不懈地寻找一组内部参数（或权重），使这个损失尽可能小。

在传统机器学习中，损失函数衡量的是网络预测与一组已知数据点之间的差异。为了使我们的网络“[物理信息](@entry_id:152556)”，我们向这个函数中添加了一个特殊的新成分。我们赋予它一种良知。这个新项，通常称为**物理残差**，衡量网络输出在多大程度上遵循一个以[偏微分方程](@entry_id:141332)（PDE）形式表达的特定物理定律。

让我们来感受一下。[神经网](@entry_id:276355)络的核心就是一个非常灵活的函数，我们称之为 $u_\theta(x, t)$，其中 $\theta$ 代表所有可训练的参数。它接收空间和时间坐标 $(x, t)$ 作为输入，并给出一个值作为输出。现代软件框架的魔力在于一种叫做**[自动微分](@entry_id:144512)（AD）**的技术。这使我们能够精确而高效地计算网络输出相对于其输入的导数，例如 $\frac{\partial u_\theta}{\partial t}$ 或 $\frac{\partial^2 u_\theta}{\partial x^2}$。

假设我们关心的物理定律是[热传导方程](@entry_id:194763) $\frac{\partial u}{\partial t} - \alpha \frac{\partial^2 u}{\partial x^2} = 0$。我们可以通过简单地将网络输出代入方程来定义我们的物理残差：

$$
R_\theta(x, t) = \frac{\partial u_\theta}{\partial t} - \alpha \frac{\partial^2 u_\theta}{\partial x^2}
$$

如果我们的网络 $u_\theta$ 是[热传导方程](@entry_id:194763)的*完美*解，那么这个残差 $R_\theta$ 在任何地方都将为零。如果它是一个糟糕的近似， $R_\theta$ 将会很大。因此，我们将残差的平方在许多时空点上取平均，然后加到我们的[损失函数](@entry_id:634569)中。优化器在寻求最小化总损失的过程中，现在被迫去寻找一个函数 $u_\theta$ ，它不仅要拟合我们可能拥有的任何数据，而且还要尽可能地接近满足[热传导方程](@entry_id:194763)。网络学习热流模式，不是仅仅通过观察数据，而是通过控制方程本身的引导。

这个过程适用于任何[偏微分方程](@entry_id:141332)，无论多么复杂。例如，在[固体力学](@entry_id:164042)中，薄板在载荷下的行为由双谐波方程 $\nabla^4 u = f$ 描述。这涉及到四阶导数！然而，原理是相同的。我们可以指示我们的[自动微分](@entry_id:144512)工具计算网络输出的这些高阶导数，并构建相应的残差 [@problem_id:2126362]。当然，[自动微分](@entry_id:144512)的这种“魔力”不是没有代价的。计算梯度的过程，特别是用于训练深度网络的高效**反向模式AD**（也称为[反向传播](@entry_id:199535)），需要在内存中的一个“带子”上存储计算过程的中间值。对于非常深或复杂的网络，这种内存成本可能相当可观 [@problem_id:2154662]，这是我们必须时刻牢记的一个实际限制。

### 构建完整求解器：约束的艺术

一个[偏微分方程](@entry_id:141332)很少孤立存在。要指定一个唯一的物理现实，它需要上下文。它需要一个初始状态——系统在开始时是什么样子的？它还需要**边界条件**——在区域的边缘发生了什么？一个抛出小球的轨迹不仅取决于重力，还取决于它从哪里开始以及被抛出的速度。

一个[物理信息神经网络](@entry_id:145229)（PINN）也必须遵守这些条件。策略非常简单：我们只需在[损失函数](@entry_id:634569)中增加更多的项。我们为初始时刻的误差添加一个项，为边界上的误差添加另一个项。总[损失函数](@entry_id:634569)现在可能看起来像这样：

$$
\mathcal{L}(\theta) = \lambda_r \mathcal{L}_{\text{residual}} + \lambda_{ic} \mathcal{L}_{\text{initial}} + \lambda_{bc} \mathcal{L}_{\text{boundary}}
$$

每个 $\mathcal{L}$ 项衡量其问题部分的平方误差，而 $\lambda$ 系数是平衡它们相对重要性的权重。这就把我们带到了一个有趣的分岔路口：我们到底应该如何强制执行像边界条件这样的东西？

一种直接的方法称为**软强制**。我们让网络成为一个通用函数 $u_\theta(x)$ ，并在[损失函数](@entry_id:634569)中添加一个惩罚项，如 $\lambda_{bc} (u_\theta(x_{\text{boundary}}) - \text{value})^2$ 。这种方式很灵活，但这就像告诉优化器：“请尽量使边界误差变小。”对于一个有限的惩罚 $\lambda_{bc}$ ，边界条件很可能只能被近似满足。

一种更优雅、更强大的方法是**硬强制**。我们改变网络本身的架构，使其*通过构造*就满足边界条件。例如，如果我们需要在区间 $[0, 1]$ 上解决一个问题，条件是 $u(0)=0$ 和 $u(1)=0$ ，我们可以将我们的解定义为一个*拟设（ansatz）*：

$$
u_\theta^{\text{hard}}(x) = x(1-x) N_\theta(x)
$$

这里，$N_\theta(x)$ 是[神经网](@entry_id:276355)络。请注意，无论网络 $N_\theta(x)$ 学会了什么函数，$u_\theta^{\text{hard}}(x)$ 在 $x=0$ 和 $x=1$ 处将*永远*为零。边界条件被完美地、无代价地满足了！这消除了对边界损失项的需求，让优化器可以专注于满足内部区域的物理定律 [@problem_id:2411060]。这看起来像一个聪明的技巧，但它深刻地展示了将物理约束直接编码到模型结构中的思想。

然而，[损失函数](@entry_id:634569)的复合性质揭示了一个深层次的挑战。网络在服务多个主人：它必须同时最小化PDE残差、初始误差和边界误差。这是一个经典的**[多目标优化](@entry_id:637420)**问题 [@problem_id:3431056]。不同的损失项可能有迥然不同的尺度——残差可能涉及[二阶导数](@entry_id:144508)，使其比边界项敏感得多。如果我们天真地选择权重（$\lambda_r, \lambda_{ic}, \lambda_{bc}$），某个项很容易在训练过程中占据主导地位。优化器可能会找到一个[完美匹配](@entry_id:273916)边界条件但在内部完全违反PDE的解，反之亦然。训练过程可能会停滞或变得不稳定，不同目标的梯度会相互对抗。现代PINN研究的很大一部分致力于驯服这头野兽，开发自适应方法以在训练期间平衡这些相互竞争的目标。

### 通往收敛的崎岖之路

一旦我们精心构建了损失函数，就需要一个优化器来找到最小值。这通常被想象成一个球滚下山坡到达最低点。但对于PINN而言，“[损失景观](@entry_id:635571)”很少是一个简单的山坡。它通常是险恶、陌生的地形，充满了陡峭的峡谷、狭窄的深谷和广阔平坦的高原。

PDE本身的性质塑造了这个景观。一个“刚性”PDE，即描述尺度差异巨大的现象的方程（如气体动力学中的激波或化学中的急剧反应锋面），通常会产生一个“刚性”或**病态**的[损失景观](@entry_id:635571) [@problem_id:2411076]。这意味着景观在某些方向上极其陡峭，而在其他方向上几乎平坦，就像一个非常长而窄的深谷。

这种地形给优化器带来了巨大的挑战。简单的梯度下降法只会在峡谷的两壁之间来回反弹，沿着谷底的进展极其缓慢。更高级的[二阶优化](@entry_id:175310)器，如**[L-BFGS](@entry_id:167263)**，试图考虑景观的曲率，以便更直接地迈向最小值。它们在收敛于行为良好、碗状的最小值方面表现出色。但在刚性景观上，它们对曲率的建模通常极不准确，导致它们陷入困境。

正是在这里，像**Adam**这样的一阶自适应方法通常被证明更为鲁棒，尤其是在训练初期。Adam为每个参数维护一个[自适应学习率](@entry_id:634918)，有效地“重新缩放”景观，使其看起来条件更好。它可以在险峻的峡谷中航行而不会偏离航道。一个非常普遍且有效的策略是：使用鲁棒的[Adam优化器](@entry_id:171393)开始训练，以进入一个好的“吸引盆”，然后切换到高精度的[L-BFGS](@entry_id:167263)优化器，以快速找到该局部谷底 [@problem_id:2411076]。

### 回报：物理偏置的力量

考虑到所有这些复杂性，人们可能会想：何必这么麻烦？为什么不直接用一个巨大的[神经网](@entry_id:276355)络处理一个庞大的数据集，让它自己学习呢？答案在于内插和外推之间的深刻差异。

考虑一个用于预测基因编辑工具有效性的[机器学习模型](@entry_id:262335)。如果在一个特定温度下的实验数据上进行训练，它可能在该条件下变得非常准确。但如果我们在不同温度下部署它呢？一个纯粹由数据驱动的“黑箱”模型，只学习了特定于训练温度的相关性，很可能会彻底失败。而一个“机理”模型，即一个包含了过程的物理化学原理的模型——比如[反应速率](@entry_id:139813)如何通过热力学定律依赖于温度——则有更大的机会正确泛化。其结构偏向于系统的因果定律 [@problem_id:2727915]。

这就是物理信息人工智能的真正力量。通过将[偏微分方程](@entry_id:141332)“烘焙”到损失函数中，我们为网络注入了强大的**[归纳偏置](@entry_id:137419)**。它不只是学习任意模式，而是学习受物理上合理的约束的模式。这使得PINN非常数据高效。[黑箱模型](@entry_id:637279)可能需要数千个数据点来学习一个解，而PINN通常只需很少的数据点，甚至完全不需要数据，仅使用PDE和边界条件就能找到解。

当我们敢于进行外推时，这种物理基础也是我们最好的防御。想象一个用于[湍流](@entry_id:151300)的代理模型，它在[雷诺数](@entry_id:136372)高达 $10^5$ 的模拟数据上进行训练。如果我们要求它预测[雷诺数](@entry_id:136372)为 $10^6$ 时的流动，我们如何能相信它的答案？我们不能依赖标准的统计验证。相反，我们必须求助于物理学。我们必须检查它的预测是否符合在高[雷诺数](@entry_id:136372)下出现的已知[湍流](@entry_id:151300)普适定律，比如著名的“[壁面律](@entry_id:262057)”或者能量耗散必须恒为正的事实。如果模型违反了这些物理原则，无论它看起来多么“自信”，其预测都是毫无价值的 [@problem_id:3369174]。物理学不仅是训练过程中的向导，它还是真理的最终仲裁者。

### 前沿：新架构与更广阔的视野

旅程并未在此结束。研究人员正在不断设计新的架构来克服基础PINN的局限性。一个众所周知的问题是**谱偏置**：标准[神经网](@entry_id:276355)络天生倾向于学习低频、平滑的函数。这使得它们难以表示具有精细细节或高频[振荡](@entry_id:267781)的解，例如波或复杂的湍流涡。

解决这个问题的一个绝妙方法是使用**傅里叶特征映射**。我们不是向网络输入像 $x$ 这样的简单坐标，而是输入一个完整的正弦特征谱：$[\cos(x), \sin(x), \cos(2x), \sin(2x), \dots, \cos(Nx), \sin(Nx)]$。通过提供这些高频构建块作为输入，网络可以轻松地将它们组合起来，构建出高度复杂和精细的解，从而有效地克服其固有的谱偏置 [@problem_id:3408303]。

最后，将PINN置于更广阔的科学AI版图中非常重要。如我们所描述的，PINN是解决*单一*、特定PDE问题的强大工具。如果初始或边界条件改变，你必须重新训练网络。但如果你的目标不仅仅是解决一个问题，而是创建一个能够即时解决一整个*族系*问题的工具呢？

这就是**[神经算子](@entry_id:752448)**的领域。与学习一个函数的PINN不同，[神经算子](@entry_id:752448)学习的是一个*算子*——即一个从一个函数到另一个函数的映射。例如，它可以学习将任何有效的初始条件函数映射到稍后时间的相应解函数的算子。它在包含许多不同[PDE解](@entry_id:166250)的数据集上进行训练。一旦训练完成，它几乎可以瞬时预测一个新的、未见过的[初始条件](@entry_id:152863)的解，而无需任何进一步的优化 [@problem_id:3337943]。

这一区别揭示了该领域的宏大抱负。PINN向我们展示了如何通过遵守物理定律来教网络找到单个解。[神经算子](@entry_id:752448)则向我们展示了如何教网络本身成为一个通用求解器。这两条路径都代表着从脆弱、依赖数据的模型向新一代人工智能的转变，新一代人工智能能够利用支配我们宇宙的基本原理进行推理、预测和发现。

