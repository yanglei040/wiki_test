## 应用与跨学科联系

现在我们已经熟悉了稀疏恢复的原理，我们可以开始一段旅程，看看这个强大的思想将我们带向何方。这真是一段奇妙的旅程！我们能从稀疏采样的部分重构整体的观念，不仅仅是一个数学上的奇趣现象；它是一个似乎连大自然本身都偏爱的深刻原理。我们在医疗扫描仪的咔哒声中、射电望远镜的低语中、细胞内基因的复杂舞蹈中，甚至在人工智能的幽灵般结构中，都能找到它的回响。通过理解稀疏性，我们获得了一个观察世界的新视角，一个让我们在纷繁复杂中发现优雅简洁的视角。

### 从奈奎斯特指令到稀疏性的自由

几十年来，信号处理的世界一直由一条相当严格的法则所统治，这条法则由伟大的思想家 Harry Nyquist 和 [Claude Shannon](@entry_id:137187) 奠定。奈奎斯特-香农采样定理是一个优美的数学理论，它给出了一个明确的准则：如果你想完美地捕捉一个信号，比如一段音乐或一束无线电波，你必须以至少其最高频率两倍的速率进行采样。如果你采样得慢一些，信号的高频分量就会伪装成低频分量——一种称为[混叠](@entry_id:146322)的现象——原始信息将不可挽回地丢失。这就像在频闪灯下观察一个旋转的轮子；如果灯光闪烁得太慢，轮子可能看起来是静止的，甚至在倒转。

这个定理是数字革命的支柱。但它也带来了沉重的代价。如果一个信号哪怕只有一个非常高频的分量，你也必须以极高的速率对其进行采样，即使信号99%的能量都集中在低频区域。压缩感知大胆地提出了一个革命性的问题：如果我们知道信号以另一种方式是简单的呢？如果它不一定是“带限”的，而是*稀疏*的——意味着它仅由少数几个基本构建块构成，就像一首旋律仅由散布在巨大钢琴键盘上的少数几个音符组成？[@problem_id:2902634]

在这种情况下，奈奎斯特指令就显得小题大做了。我们不需要在钢琴的每一个琴键上都去听。稀疏恢复的核心洞见在于，如果我们仅在少数几个*随机选择*的时间点对信号进行采样，我们仍然可以通过寻找与我们的测量结果相符的“最稀疏”解释来完美地重建原始信号。随机性是关键！与产生结构化和致命混叠的均匀采样不同，[随机采样](@entry_id:175193)将混叠变成了一种微弱的、非相干的背景噪声。像 $\ell_1$ 最小化这样的[稀疏性](@entry_id:136793)寻求算法可以轻易地将少数强烈的、真实的音符与弥散的、类似噪声的伪影区分开来。我们需要的样本数量不再由最高频率决定，而是由活动分量的数量 $K$ 决定。我们通常可以用大约 $K \log(N/K)$ [数量级](@entry_id:264888)的测量就达成目标，其中 $N$ 是信号空间的总“大小”——与经典方法所需的 $N$ 个样本相比，这是一个巨大的节省 [@problem_id:3460544]。

这一原理彻底改变了那些曾受制于漫长[采集时间](@entry_id:266526)的领域。考虑核[磁共振](@entry_id:143712)（NMR）波谱学，这是化学和医学中用于确定分子结构的基石。一个[多维核磁共振](@entry_id:752272)实验可以产生一个作为分子独特指纹的“波谱”。但传统上，获取这个波谱需要在时域上对一个巨大的网格进行采样，这个实验可能需要数小时甚至数天。然而，最终的波谱几乎完全是空白空间，只包含少数几个尖锐的峰。这是[稀疏信号](@entry_id:755125)的教科书式例子。通过应用[非均匀采样](@entry_id:752610)（NUS）——这是[随机采样](@entry_id:175193)在该领域的实用名称——科学家现在可以用一小部分时间获得同样高质量的波谱 [@problem_id:3715731]。他们只对网格的一个小的、随机的[子集](@entry_id:261956)进行采样，然后让稀疏恢复的力量来填补其余部分。这不仅仅是一个微小的改进；它是一种[范式](@entry_id:161181)转变，为研究更复杂的分子和以前无法企及的动态过程打开了大门 [@problem_id:3715731] [@problem_id:3132852]。

同样的想法也适用于工程学。想象一下，你设计了一款新天线，并希望绘制其在远场的辐射模式。你无法在空间的每个角落都放置传感器。然而，电磁学物理告诉我们，这个远场模式可以用一组称为球谐函数的特殊函数来描述。如果天线相当简单，它的模式在这个[球谐函数](@entry_id:178380)基中将是稀疏的。因此，通过在*[近场](@entry_id:269780)*测量少数几个巧妙选择的随机点的场强，我们可以使用稀疏恢复高精度地重建整个[远场](@entry_id:269288)模式 [@problem_id:3333741]。我们看到了不可见之物。

### 分解数据：寻找机器中的幽灵

[稀疏性](@entry_id:136793)的力量不仅限于向量和信号。它完美地延伸到矩阵和数据，使我们能够在大而凌乱的数据集中发现隐藏的结构。[鲁棒主成分分析](@entry_id:754394)（RPCA）和[矩阵补全](@entry_id:172040)就是这方面的两个杰出例子。

想象你有一台安全摄像头对着一个静态场景，比如一条走廊。随着时间的推移，你收集了一段视频，它只是一系列帧的序列。我们可以将这些帧堆叠成一个大的数据矩阵 $M$。这个矩阵看起来很复杂，但它实际上由两个简单的部分组成：一个在每一帧中几乎都相同的静态背景，以及一些“前景”变化，比如一个人走过。背景部分是高度冗余的，可以用一个低秩矩阵 $L^{\star}$ 来描述。前景部分在任何给定时间只影响少数像素，可以用一个[稀疏矩阵](@entry_id:138197) $S^{\star}$ 来描述。我们的数据矩阵是它们的和：$M = L^{\star} + S^{\star}$。

问题是，我们只得到了 $M$。我们能把它分解回背景和前景两个分量吗？这似乎是不可能的，但稀疏恢复提供了答案。我们可以通过寻找“最简单”的分解来解决这个问题：找到秩最低的矩阵 $L$ 和最稀疏的矩阵 $S$，使得它们的和等于 $M$。使用[核范数](@entry_id:195543)作为秩的代理，$\ell_1$ 范数作为稀疏度的代理，这就变成了一个易于处理的凸[优化问题](@entry_id:266749)。然而，要使这种分离成为可能，一个关键的*非相干性*条件必须得到满足。低秩背景不能“看起来”像稀疏的，稀疏前景也不能串通起来“看起来”像低秩的。例如，如果背景本身只是一个明亮的像素点，它既是低秩的又是稀疏的，这会使分解变得模糊不清。非相干性确保了这两种类型的简单性在几何上是截然不同的，从而允许算法干净利落地将它们解开 [@problem_id:3474837]。

这就把我们带到了著名的[矩阵补全](@entry_id:172040)问题。你很可能每天都在使用这项技术。当 Netflix 或亚马逊等服务推荐电影或产品时，它正试图解决一个[矩阵补全](@entry_id:172040)问题。想象一个巨大的矩阵，行是用户，列是电影。每个条目是用户对电影的评分。这个矩阵大部分是空的；你只对所有可用的电影中的一小部分进行了评分。目标是填补缺失的条目，以预测你可能会喜欢什么。

关键假设是“品味”是低秩的。也就是说，你的偏好可以由少数几个潜在因素来描述（例如，你喜欢科幻小说，你偏爱1980年代的电影，你喜欢某位特定的导演）。如果这是真的，那么完整的[评分矩阵](@entry_id:172456)，如果我们能知道的话，将是低秩的。现在的问题是找到与我们*确实*拥有的少数评分一致的秩最低的矩阵 $M$。这同样可以通过最小化[核范数](@entry_id:195543)来解决。同样，为了让这个方法奏效，我们需要一个非[相干性](@entry_id:268953)条件。我们无法为一个从未评分的用户，或一部从未有人看过的电影预测评分。我们拥有的信息必须足够“分散”，以避免这些盲点。只要有足够数量的随机散布的评分，[矩阵补全](@entry_id:172040)就能奇迹般地填补其余的部分 [@problem_id:3459255]。

### [逆向工程](@entry_id:754334)复杂性

也许稀疏恢复最令人兴奋的应用不仅仅是在传感和数据处理方面，而是在科学发现本身——逆向工程复杂系统的隐藏蓝图。

考虑一个活细胞内基因和蛋白质的复杂网络。成千上万的组件在一个巨大、复杂的网络中相互作用，以维持生命。生物学家希望绘制出这个网络：哪个基因启动了哪个其他基因？人们普遍认为这些[调控网络](@entry_id:754215)是稀疏的；任何给定的基因都只由少数几个[主调控因子](@entry_id:265566)直接控制。我们无法直接看到布线。但我们可以进行实验。我们可以“扰动”系统——例如，通过敲除一个基因或引入一种药物——并测量所有其他基因活性的变化。每个实验都给了我们一个将[网络结构](@entry_id:265673)与我们的观察联系起来的[线性方程](@entry_id:151487)。如果我们进行几个不同的实验，我们就会得到一个[方程组](@entry_id:193238)。由于可能的连接数量巨大（对于 $n$ 个基因是 $n^2$），我们只能负担得起进行少量实验。但由于底层网络是稀疏的，我们又回到了压缩感知的领域。我们可以设计我们的实验扰动使其“非相干”，并使用稀疏恢复从我们有限的数据中推断出最可能的网络布线图 [@problem_id:3332733]。在某种意义上，我们正在用数学来对细胞的指挥和控制逻辑进行[X射线](@entry_id:187649)透视。

同样的“[逆向工程](@entry_id:754334)”思维方式现在正被应用于理解人工智能的奥秘。现代[神经网](@entry_id:276355)络是庞然大物，拥有数十亿个参数。然而，有一个被称为“彩票假说”的迷人想法，它表明在这些庞大的、训练好的网络中，存在一个更小的、稀疏的子网络（一张“中奖彩票”），它对网络的性能负主要责任。如果我们能找到这个子网络，我们就能创造出更高效的人工智能。寻找这个重要的稀疏权重集的问题可以被构建成一个稀疏恢复问题。训练数据提供了“测量”，而[网络架构](@entry_id:268981)提供了[线性系统](@entry_id:147850)的结构。通过[压缩感知](@entry_id:197903)的视角来分析这个问题，我们可以开始理解在何种条件下可以识别这些高效的子网络并从头开始训练它们 [@problem_id:3461748]。

### 可能性的边缘：硬度与随机性

面对所有这些看似神奇的应用，人们可能会倾向于认为稀疏恢复是解决所有科学问题的万能溶剂。作为诚实的科学家，理解其局限性是很重要的。在这里，我们发现了一个与计算基本性质的美妙而深刻的联系。

寻找一个[方程组](@entry_id:193238)的*绝对最稀疏*解的问题，在最坏情况下，是一个 NP-hard 问题。这意味着它属于一类被认为不存在高效（[多项式时间](@entry_id:263297)）算法的问题。解决它就像破解最困难的密码学代码一样困难 [@problem_id:3437351]。如果有人递给你一个精心构造的“对抗性”测量矩阵，找到产生这些测量的稀疏信号可能比宇宙的年龄还要长。

这如何与像 $\ell_1$-最小化这样的算法的惊人成功相协调呢？答案，再一次地，是*随机性*的奇迹。NP-hard 性存在于最坏情况下的、病态结构的问题中。压缩感知的理论告诉我们，如果我们*随机*选择测量矩阵（或者使用一种像部分傅里叶矩阵那样相对于信号*表现*为随机的设计），那么以极高的概率，所产生的问题将是一个“简单”的问题。那些有问题的、困难的实例被淹没在易于处理的实例的海洋中。

这是一个深刻的哲学观点。我们无法战胜最坏情况的[计算复杂性](@entry_id:204275)。相反，我们绕开了它。我们用概率来保证我们几乎永远不会遇到最坏的情况。这种稀疏性、硬度和随机性之间的相互作用，是现代[应用数学](@entry_id:170283)中最美丽、最重要的思想之一，它向我们展示了，即使面对理论上无法逾越的障碍，一点点聪明才智和一次掷骰子，也足以揭示隐藏在我们世界表面之下的简单而优雅的真理 [@problem_id:3437351]。