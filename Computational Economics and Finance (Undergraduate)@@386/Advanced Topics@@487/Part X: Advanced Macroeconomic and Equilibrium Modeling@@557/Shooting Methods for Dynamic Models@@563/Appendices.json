{"hands_on_practices": [{"introduction": "This first practice provides a clear and manageable introduction to the shooting method using a discrete-time linear-quadratic (LQ) model of reservoir management. You will apply the principles of dynamic optimization to derive the necessary conditions, forming a two-point boundary value problem. By implementing a forward simulation inside a simple loop, you will learn how to \"shoot\" for a terminal state target by adjusting an initial unknown, a foundational skill in solving dynamic models [@problem_id:2429166].", "id": "2429166", "problem": "Consider the discrete-time optimal management of a single water reservoir over a planning horizon of $T$ months, with $T = 12$. The reservoir storage is denoted by $s_t$ at the start of month $t$, and the controlled release is $r_t$ during month $t$, for $t \\in \\{0,1,\\ldots,T-1\\}$. The monthly inflow $I_t$ and the target demand $D_t$ are exogenous and known. A constant monthly evaporation-loss factor $e \\in [0,1)$ applies to the stock. The dynamics of storage are defined by the linear law of motion\n$$\ns_{t+1} = (1 - e) s_t + I_t - r_t,\n$$\nwith given initial storage $s_0$. The terminal storage must satisfy a hard boundary condition for flood prevention,\n$$\ns_T = S^\\star.\n$$\nThe planner minimizes the intertemporal quadratic loss\n$$\n\\sum_{t=0}^{T-1} \\left( \\tfrac{1}{2}\\alpha \\, (r_t - D_t)^2 + \\tfrac{1}{2}\\beta \\, s_t^2 \\right),\n$$\nwhere $\\alpha > 0$ and $\\beta \\ge 0$ are given weights. Releases $r_t$ are unconstrained real numbers, representing net controlled outflow (positive) or net pumping-in (negative), which keeps the problem linear-quadratic and ensures a unique interior optimizer for the stated loss and dynamics.\n\nYour task is to implement a single-shooting method for this finite-horizon, discrete-time dynamic optimization problem. Use first principles of dynamic optimization to construct the first-order necessary conditions and identify a suitable unknown boundary value to shoot on. Then, design a robust bracketed root-finding routine to adjust that unknown so that the terminal storage constraint $s_T = S^\\star$ is satisfied to high numerical accuracy. After satisfying the boundary condition, evaluate the minimized objective for the given parameters.\n\nRequirements for numerical implementation:\n- Use a single-shooting approach by selecting an unknown scalar boundary value that allows you to forward-simulate the state and control. Adjust this unknown using a bracketed root-finding method until the terminal constraint $s_T = S^\\star$ is met within a tight tolerance.\n- The forward simulation must be stable and should handle any $e \\in [0,1)$.\n- The final answer for each test case must be the minimized objective value as a real number.\n\nTest suite:\n- Case A (general case): $T = 12$, $\\alpha = 2.0$, $\\beta = 0.1$, $e = 0.02$, $s_0 = 30.0$, $S^\\star = 40.0$, $I = [22.0, 18.0, 15.0, 12.0, 10.0, 8.0, 7.0, 8.0, 10.0, 12.0, 16.0, 24.0]$, $D = [16.0, 16.5, 17.0, 17.5, 18.0, 19.0, 20.0, 19.5, 18.5, 17.5, 17.0, 16.5]$.\n- Case B (boundary case with no evaporation and exact balance): $T = 12$, $\\alpha = 10.0$, $\\beta = 0.0$, $e = 0.0$, $s_0 = 50.0$, $S^\\star = 50.0$, $I = [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]$, $D = [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]$.\n- Case C (edge trade-off with higher evaporation, stronger stock penalty, and small initial stock): $T = 12$, $\\alpha = 0.5$, $\\beta = 0.2$, $e = 0.05$, $s_0 = 5.0$, $S^\\star = 8.0$, $I = [5.0, 4.0, 3.5, 3.0, 2.5, 2.5, 2.8, 3.2, 3.5, 4.0, 4.5, 5.0]$, $D = [3.0, 3.2, 3.5, 3.8, 4.0, 4.2, 4.5, 4.2, 4.0, 3.8, 3.5, 3.2]$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the minimized objective values for Cases A, B, and C, in that order, as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, $[x_A,x_B,x_C]$).", "solution": "The user-provided problem is a well-posed, discrete-time, finite-horizon linear-quadratic dynamic optimization problem. It requires the implementation of a single-shooting method to find the optimal control policy for managing a water reservoir and to calculate the corresponding minimized objective function value.\n\nThe problem is valid as it is scientifically grounded in optimal control theory, mathematically consistent, complete, and objective. We can therefore proceed with the solution.\n\n### Step 1: Theoretical Formulation\n\nThe problem is to minimize the objective function $J$ over the sequence of controls $\\{r_t\\}_{t=0}^{T-1}$:\n$$\nJ = \\sum_{t=0}^{T-1} \\left( \\frac{1}{2}\\alpha (r_t - D_t)^2 + \\frac{1}{2}\\beta s_t^2 \\right)\n$$\nsubject to the state dynamics:\n$$\ns_{t+1} = (1 - e) s_t + I_t - r_t, \\quad \\text{for } t=0, \\ldots, T-1\n$$\nand the boundary conditions:\n$$\ns_0 = \\text{given}, \\quad s_T = S^\\star\n$$\n\nTo derive the necessary conditions for optimality, we use the discrete-time Hamiltonian method. The Hamiltonian at time $t$ is defined as:\n$$\nH_t = \\frac{1}{2}\\alpha (r_t - D_t)^2 + \\frac{1}{2}\\beta s_t^2 + \\lambda_{t+1} \\left( (1 - e)s_t + I_t - r_t \\right)\n$$\nwhere $\\lambda_{t+1}$ is the co-state variable (shadow price) associated with the state $s_{t+1}$.\n\nThe first-order necessary conditions are derived from Pontryagin's Minimum Principle for discrete systems:\n1.  **Control optimality condition**: The control $r_t$ must minimize the Hamiltonian at each time $t$.\n    $$\n    \\frac{\\partial H_t}{\\partial r_t} = \\alpha (r_t - D_t) - \\lambda_{t+1} = 0 \\implies r_t = D_t + \\frac{1}{\\alpha}\\lambda_{t+1}\n    $$\n2.  **Co-state dynamic equation**: The evolution of the co-state is given by:\n    $$\n    \\lambda_t = \\frac{\\partial H_t}{\\partial s_t} = \\beta s_t + (1 - e)\\lambda_{t+1}\n    $$\n3.  **State dynamic equation** (as given):\n    $$\n    s_{t+1} = (1 - e)s_t + I_t - r_t\n    $$\n4.  **Transversality condition**: Since the initial state $s_0$ is fixed and the terminal state $s_T$ is fixed, the corresponding co-state values $\\lambda_0$ and $\\lambda_T$ are free.\n\nThis set of equations forms a two-point boundary value problem (TPBVP) because we have a condition on the state at the beginning ($s_0$) and at the end ($s_T$), while the dynamic equations for the state and co-state are coupled and evolve in opposite directions in time (state forward, co-state backward).\n\n### Step 2: Single-Shooting Method Design\n\nThe single-shooting method transforms the TPBVP into an initial value problem (IVP). We select an unknown initial value, \"shoot\" forward in time by integrating the dynamic equations, and then adjust our initial guess until the terminal condition is met.\n\nA natural choice for the shooting variable is the initial value of the co-state, $\\lambda_0$. If $\\lambda_0$ were known, we could simulate the entire system forward.\n\nThe forward simulation procedure, given a guess for $\\lambda_0$, is as follows:\nInitialize with $s_t = s_0$ and $\\lambda_t = \\lambda_0$. For $t = 0, 1, \\ldots, T-1$:\n1.  From the co-state equation, solve for $\\lambda_{t+1}$:\n    $$\n    \\lambda_{t+1} = \\frac{\\lambda_t - \\beta s_t}{1 - e}\n    $$\n    This is well-defined since the problem specifies $e \\in [0, 1)$.\n2.  Use $\\lambda_{t+1}$ to find the optimal control $r_t$:\n    $$\n    r_t = D_t + \\frac{1}{\\alpha}\\lambda_{t+1}\n    $$\n3.  Use $r_t$ to find the next state $s_{t+1}$:\n    $$\n    s_{t+1} = (1 - e)s_t + I_t - r_t\n    $$\n4.  Update the state and co-state for the next iteration: set $s_t \\to s_{t+1}$ and $\\lambda_t \\to \\lambda_{t+1}$.\n\nThis simulation defines a function that maps the initial guess $\\lambda_0$ to the resulting terminal state, which we denote $s_T(\\lambda_0)$. Our goal is to find the specific value $\\lambda_0^*$ such that the terminal constraint is satisfied:\n$$\ns_T(\\lambda_0^*) = S^\\star\n$$\nThis is a root-finding problem for the residual function $f(\\lambda_0)$:\n$$\nf(\\lambda_0) = s_T(\\lambda_0) - S^\\star = 0\n$$\n\n### Step 3: Root-Finding using a Bracketed Method\n\nThe problem requires a bracketed root-finding method. We will use the bisection method, which is robust and guaranteed to converge if an initial bracket is found. A bracket is an interval $[\\lambda_a, \\lambda_b]$ where $f(\\lambda_a)$ and $f(\\lambda_b)$ have opposite signs.\n\nTo ensure the bisection method works, we must analyze the monotonicity of $f(\\lambda_0)$. Differentiating the simulation equations with respect to $\\lambda_0$, we can show that $\\frac{ds_T}{d\\lambda_0} < 0$ for all $t>0$ (as long as $\\alpha > 0$). This means $s_T(\\lambda_0)$, and thus $f(\\lambda_0)$, is a strictly monotonically decreasing function of $\\lambda_0$. This property guarantees that a unique root exists and simplifies finding a bracket.\n\nThe bisection algorithm proceeds as follows:\n1.  Establish an initial bracket $[\\lambda_{low}, \\lambda_{high}]$ such that $f(\\lambda_{low}) > 0$ and $f(\\lambda_{high}) < 0$. An initial guess for the interval can be expanded systematically until it brackets the root.\n2.  Iteratively narrow the interval:\n    a. Compute the midpoint $\\lambda_{mid} = (\\lambda_{low} + \\lambda_{high}) / 2$.\n    b. Evaluate $f(\\lambda_{mid})$.\n    c. If $f(\\lambda_{mid}) > 0$, the root must lie in $[\\lambda_{mid}, \\lambda_{high}]$. Set $\\lambda_{low} = \\lambda_{mid}$.\n    d. If $f(\\lambda_{mid}) < 0$, the root must lie in $[\\lambda_{low}, \\lambda_{mid}]$. Set $\\lambda_{high} = \\lambda_{mid}$.\n3.  Repeat until the interval width $(\\lambda_{high} - \\lambda_{low})$ is smaller than a specified tolerance. The root is then approximated by the midpoint of the final interval.\n\n### Step 4: Final Calculation\n\nOnce the optimal initial co-state, $\\lambda_0^*$, is found with sufficient accuracy, we perform one final forward simulation using this value. During this simulation, we compute the optimal state path $\\{s_t^*\\}_{t=0}^{T-1}$ and control path $\\{r_t^*\\}_{t=0}^{T-1}$. The minimized objective function value is then calculated by summing the period costs:\n$$\nJ^* = \\sum_{t=0}^{T-1} \\left( \\frac{1}{2}\\alpha (r_t^* - D_t)^2 + \\frac{1}{2}\\beta (s_t^*)^2 \\right)\n$$\nThis value is the final result for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the reservoir management problem for all test cases.\n    \"\"\"\n    # Test suite:\n    # (T, alpha, beta, e, s0, S_star, I, D)\n    test_cases = [\n        (\n            12, 2.0, 0.1, 0.02, 30.0, 40.0,\n            np.array([22.0, 18.0, 15.0, 12.0, 10.0, 8.0, 7.0, 8.0, 10.0, 12.0, 16.0, 24.0]),\n            np.array([16.0, 16.5, 17.0, 17.5, 18.0, 19.0, 20.0, 19.5, 18.5, 17.5, 17.0, 16.5]),\n        ),\n        (\n            12, 10.0, 0.0, 0.0, 50.0, 50.0,\n            np.array([10.0] * 12),\n            np.array([10.0] * 12),\n        ),\n        (\n            12, 0.5, 0.2, 0.05, 5.0, 8.0,\n            np.array([5.0, 4.0, 3.5, 3.0, 2.5, 2.5, 2.8, 3.2, 3.5, 4.0, 4.5, 5.0]),\n            np.array([3.0, 3.2, 3.5, 3.8, 4.0, 4.2, 4.5, 4.2, 4.0, 3.8, 3.5, 3.2]),\n        ),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        obj_val = solve_single_case(*case_params)\n        results.append(f\"{obj_val:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef solve_single_case(T, alpha, beta, e, s0, S_star, I, D):\n    \"\"\"\n    Solves a single instance of the dynamic optimization problem using a single-shooting method.\n    \n    Args:\n        T (int): Planning horizon.\n        alpha (float): Weight on control deviation.\n        beta (float): Weight on state deviation.\n        e (float): Evaporation factor.\n        s0 (float): Initial storage.\n        S_star (float): Target terminal storage.\n        I (np.array): Inflow vector.\n        D (np.array): Demand vector.\n\n    Returns:\n        float: The minimized objective function value.\n    \"\"\"\n\n    one_minus_e = 1.0 - e\n\n    def forward_simulation(lambda0):\n        \"\"\"\n        Simulates the system dynamics forward given an initial co-state lambda0.\n        Returns the terminal state s_T and the accumulated objective value.\n        \"\"\"\n        s = s0\n        lam = lambda0\n        total_objective = 0.0\n\n        for t in range(T):\n            # Add cost associated with state s_t\n            total_objective += 0.5 * beta * s**2\n\n            # Calculate co-state lambda_{t+1}\n            lam_next = (lam - beta * s) / one_minus_e\n\n            # Calculate control r_t\n            r = D[t] + lam_next / alpha\n            \n            # Add cost associated with control r_t\n            total_objective += 0.5 * alpha * (r - D[t])**2\n\n            # Calculate next state s_{t+1}\n            s_next = one_minus_e * s + I[t] - r\n            \n            # Update for next iteration\n            s = s_next\n            lam = lam_next\n\n        return s, total_objective\n\n    def residual_function(lambda0):\n        \"\"\"\n        Calculates the residual s_T(lambda0) - S_star. This is the function\n        for which we a find a root.\n        \"\"\"\n        s_T, _ = forward_simulation(lambda0)\n        return s_T - S_star\n\n    # --- Bracketed Root-Finding (Bisection Method) ---\n\n    # 1. Find a bracket [low, high] for lambda0\n    low, high = -100.0, 100.0\n    f_low = residual_function(low)\n    f_high = residual_function(high)\n\n    # Expand the bracket if the root is not contained\n    expansion_iter = 0\n    while f_low * f_high > 0 and expansion_iter < 20:\n        if abs(f_low) < abs(f_high):\n            low *= 2.0\n            f_low = residual_function(low)\n        else:\n            high *= 2.0\n            f_high = residual_function(high)\n        expansion_iter += 1\n\n    if f_low * f_high > 0:\n        raise RuntimeError(\"Failed to find a bracket for the root.\")\n\n    # The function s_T(lambda0) is monotonically decreasing.\n    # We ensure f(low) > 0 and f(high) < 0.\n    if f_low < f_high:\n        low, high = high, low\n        f_low, f_high = f_high, f_low\n\n    # 2. Bisection to find the root\n    tol = 1e-12\n    max_iter = 100\n    for _ in range(max_iter):\n        mid = (low + high) / 2.0\n        if high - low < tol:\n            break\n        \n        f_mid = residual_function(mid)\n\n        if abs(f_mid) < tol:\n            low = high = mid\n            break\n        \n        # As s_T(lambda0) is decreasing, so is the residual function.\n        # If f_mid > 0, the root is in the upper half of the interval.\n        if f_mid > 0:\n            low = mid\n        else:\n            high = mid\n            \n    lambda0_optimal = (low + high) / 2.0\n\n    # 3. Final calculation with the optimal lambda0\n    _, final_objective = forward_simulation(lambda0_optimal)\n\n    return final_objective\n\n# The script should be runnable \"as is\"\nif __name__ == \"__main__\":\n    solve()\n\n```"}, {"introduction": "Building upon the discrete-time case, this exercise transitions the shooting method to a continuous-time domain by modeling the optimal control of antibiotic resistance. You will use Pontryagin's Maximum Principle to derive a Hamiltonian system of ordinary differential equations (ODEs) and learn to solve it with a numerical integrator. Because this problem is linear-quadratic, it also reveals an elegant and efficient solution technique based on the principle of superposition, showcasing a powerful analytical shortcut for a special class of dynamic problems [@problem_id:2429165].", "id": "2429165", "problem": "Consider the following quadratic optimal control problem posed as a boundary value problem with a fixed terminal state. A scalar state $R(t)$ represents the share of drug-resistant pathogens in a hospital at time $t$, and a scalar control $u(t)$ represents the antibiotic usage intensity. The dynamics are linear and given by\n$$\\dot{R}(t) = \\theta u(t) - \\delta R(t), \\quad R(0) = R_0,$$\nwhere $\\theta &gt; 0$ is the marginal effect of antibiotic usage on resistance and $\\delta &gt; 0$ is the natural decay rate of resistance in the absence of antibiotic use. The objective is to minimize the quadratic discounted cost functional\n$$J[u] = \\int_0^T \\frac{1}{2}\\left(u(t)^2 + q R(t)^2\\right)\\,dt,$$\nsubject to reaching a target resistance level $R(T) = R_T$ at the fixed terminal time $T$. Assume all parameters are constants with $\\theta &gt; 0$, $\\delta &gt; 0$, $q &gt; 0$, $T &gt; 0$ and with given initial and terminal values $R_0$ and $R_T$. There are no control bounds.\n\nYour task is to write a complete, runnable program that:\n- Derives from first principles an implementable system of necessary conditions using Pontryagin's Maximum Principle and uses a shooting method to numerically enforce the terminal condition $R(T) = R_T$ by appropriately choosing the initial costate.\n- Solves the resulting two-point boundary value problem using forward integration and a root-finding update on the unknown initial costate. Your implementation must be fully self-contained and must not require any user input.\n- For each test case, returns the optimal initial control $u(0)$ and the minimized objective value $J$.\n\nFundamental base you may assume as known: the definition of Pontryagin's Maximum Principle, the definition of a Hamiltonian for an optimal control problem, the Euler–Lagrange logic linking necessary conditions to costates, and standard ordinary differential equations solution concepts. You must not assume specialized formulas tailored to this particular problem; begin from the definitions and derive the necessary conditions before implementing a numerical method.\n\nNumerical specification:\n- Use a forward shooting method. Treat the initial costate $\\lambda(0)$ as an unknown to be found so that, under the forward solution of the Hamiltonian system, the state hits the terminal target $R(T) = R_T$. You must enforce the terminal state condition to an absolute tolerance of $10^{-8}$.\n- Use a robust time integrator for the ordinary differential equations and a numerically stable update for $\\lambda(0)$. The numerical implementation must remain stable for the given test suite.\n\nTest suite:\n- Case A: $T = 10.0$, $R_0 = 0.1$, $R_T = 0.2$, $\\theta = 0.8$, $\\delta = 0.05$, $q = 0.4$.\n- Case B: $T = 0.5$, $R_0 = 0.2$, $R_T = 0.21$, $\\theta = 1.0$, $\\delta = 0.1$, $q = 0.7$.\n- Case C: $T = 5.0$, $R_0 = 0.3$, $R_T = 0.15$, $\\theta = 1.2$, $\\delta = 0.3$, $q = 0.2$.\n\nAnswer specification:\n- For each test case, compute the optimal initial control $u(0)$ and the minimized objective value $J$. Express both numbers as decimal floating-point values rounded to exactly $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list in the order $[u(0), J]$. For the three test cases above, the required format is\n\"[ [u0_A,J_A], [u0_B,J_B], [u0_C,J_C] ]\" but without spaces after commas inside each pair or between pairs. Concretely, your program must print a single line exactly of the form\n\"[[u0_A,J_A],[u0_B,J_B],[u0_C,J_C]]\"\nwhere each symbol is replaced by the corresponding decimal rounded to $6$ places.\n\nThere are no physical units in this problem. Angles do not appear in this problem. Percentages must not be used anywhere; all results must be decimal numbers.", "solution": "The problem presented is a continuous-time finite-horizon linear-quadratic optimal control problem with a fixed terminal state. The solution is obtained by applying Pontryagin's Maximum Principle to derive a system of necessary conditions, which results in a two-point boundary value problem (TPBVP). This TPBVP is then solved numerically using a forward shooting method.\n\nFirst, we establish the theoretical foundation from first principles. The state variable is $R(t)$ and the control variable is $u(t)$. The state dynamics are given by\n$$ \\dot{R}(t) = \\theta u(t) - \\delta R(t), \\quad R(0) = R_0 $$\nThe objective is to minimize the cost functional\n$$ J[u] = \\int_0^T \\frac{1}{2}\\left(u(t)^2 + q R(t)^2\\right)\\,dt $$\nsubject to the terminal constraint $R(T) = R_T$.\n\nTo apply Pontryagin's Maximum Principle, we introduce the costate (or adjoint) variable $\\lambda(t)$ and define the Hamiltonian $H$:\n$$ H(R, u, \\lambda) = \\frac{1}{2}(u(t)^2 + q R(t)^2) + \\lambda(t)(\\theta u(t) - \\delta R(t)) $$\nThe necessary conditions for an optimal trajectory $(R^*(t), u^*(t))$ are given by the state equation, the costate equation, and the optimality condition.\n\n$1$. State Equation:\n$$ \\dot{R}^*(t) = \\frac{\\partial H}{\\partial \\lambda} = \\theta u^*(t) - \\delta R^*(t) $$\nThis recovers the system dynamics with the initial condition $R^*(0) = R_0$ and the terminal condition $R^*(T) = R_T$.\n\n$2$. Costate Equation:\n$$ \\dot{\\lambda}^*(t) = -\\frac{\\partial H}{\\partial R} = -(q R^*(t) - \\delta \\lambda^*(t)) = -q R^*(t) + \\delta \\lambda^*(t) $$\nSince the terminal state $R(T)$ is fixed, there is no transversality condition on $\\lambda^*(T)$; its value is unrestricted.\n\n$3$. Optimality Condition:\nThe Hamiltonian must be minimized with respect to the control $u(t)$ at each time $t \\in [0, T]$. As the control is unconstrained, we find the minimum by setting the first derivative of $H$ with respect to $u$ to zero:\n$$ \\frac{\\partial H}{\\partial u} = u^*(t) + \\theta \\lambda^*(t) = 0 $$\nThis yields the optimal control law in terms of the costate:\n$$ u^*(t) = -\\theta \\lambda^*(t) $$\nThe second derivative, $\\frac{\\partial^2 H}{\\partial u^2} = 1 > 0$, confirms that this condition corresponds to a minimum.\n\nSubstituting the optimal control law back into the state and costate equations yields a system of two coupled linear ordinary differential equations (ODEs) for $R(t)$ and $\\lambda(t)$:\n$$ \\dot{R}(t) = -\\delta R(t) - \\theta^2 \\lambda(t) $$\n$$ \\dot{\\lambda}(t) = -q R(t) + \\delta \\lambda(t) $$\nThese equations are subject to the boundary conditions $R(0) = R_0$ and $R(T) = R_T$. This constitutes a two-point boundary value problem.\n\nTo solve this TPBVP, we employ a forward shooting method. The unknown initial condition is the initial costate, which we denote $p = \\lambda(0)$. The goal is to find the value of $p$ such that if we solve a forward integration of the ODE system starting with $(R(0), \\lambda(0)) = (R_0, p)$, the solution satisfies $R(T) = R_T$.\n\nDue to the linearity of the ODE system, we can apply the principle of superposition to solve for $p$ efficiently. The solution for $R(t)$ can be expressed as a linear combination of a homogeneous solution (driven by $R_0$) and a particular solution (driven by $p$). Let $(R_h(t), \\lambda_h(t))$ be the solution to the ODE system with initial conditions $(R_h(0), \\lambda_h(0)) = (R_0, 0)$, and let $(R_p(t), \\lambda_p(t))$ be the solution with initial conditions $(R_p(0), \\lambda_p(0)) = (0, 1)$. The general solution starting from $(R_0, p)$ is then given by:\n$$ R(t) = R_h(t) + p \\cdot R_p(t) $$\nTo satisfy the terminal condition $R(T) = R_T$, we must have:\n$$ R_T = R_h(T) + p \\cdot R_p(T) $$\nSolving for the unknown initial costate $p$ yields:\n$$ p = \\lambda(0) = \\frac{R_T - R_h(T)}{R_p(T)} $$\nThis provides a direct method to find the correct $\\lambda(0)$. The numerical procedure is as follows:\n$1$. Numerically integrate the ODE system from $t=0$ to $t=T$ with initial conditions $(R_0, 0)$ to find $R_h(T)$.\n$2$. Numerically integrate the same system from $t=0$ to $t=T$ with initial conditions $(0, 1)$ to find $R_p(T)$.\n$3$. Compute the required initial costate $\\lambda(0) = p$ using the formula above.\n\nWith the correct initial conditions $(R(0), \\lambda(0)) = (R_0, p)$ now fully determined, the minimized objective value $J$ can be calculated. The integrand of $J$ is $\\frac{1}{2}(u(t)^2 + qR(t)^2) = \\frac{1}{2}(\\theta^2 \\lambda(t)^2 + qR(t)^2)$. To compute the integral, we augment the ODE system with a third state variable, $V(t)$, representing the accumulated cost:\n$$ \\dot{V}(t) = \\frac{1}{2}(\\theta^2 \\lambda(t)^2 + q R(t)^2), \\quad V(0) = 0 $$\nBy integrating the augmented three-dimensional system from $t=0$ to $t=T$ with the initial condition $(R(0), \\lambda(0), V(0)) = (R_0, p, 0)$, the minimized objective value is obtained as $J = V(T)$. The optimal initial control is given by $u(0) = -\\theta \\lambda(0) = -\\theta p$. This procedure is implemented for each test case provided.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the quadratic optimal control problem for three test cases\n    using a forward shooting method based on the principle of superposition.\n    \"\"\"\n    # Test cases as provided in the problem statement.\n    # Format: (T, R0, RT, theta, delta, q)\n    test_cases = [\n        (10.0, 0.1, 0.2, 0.8, 0.05, 0.4),  # Case A\n        (0.5, 0.2, 0.21, 1.0, 0.1, 0.7),  # Case B\n        (5.0, 0.3, 0.15, 1.2, 0.3, 0.2),  # Case C\n    ]\n\n    results = []\n\n    for case in test_cases:\n        T, R0, RT, theta, delta, q = case\n\n        # Pre-compute squared parameters for efficiency in the ODE integrators.\n        theta_sq = theta**2\n\n        # Define the Hamiltonian ODE system for R(t) and lambda(t).\n        def hamiltonian_system(t, y):\n            \"\"\"\n            Represents the system d(y)/dt = A*y, where y = [R, lambda].\n            \"\"\"\n            R, lam = y\n            R_dot = -delta * R - theta_sq * lam\n            lam_dot = -q * R + delta * lam\n            return [R_dot, lam_dot]\n\n        # Use high-precision tolerances for the numerical integration to ensure\n        # the terminal condition is met accurately.\n        atol = 1e-12\n        rtol = 1e-12\n        \n        # --- Shooting Method using Superposition ---\n\n        # 1. Integrate for the homogeneous part of the solution, driven by R0.\n        #    Initial conditions: [R(0), lambda(0)] = [R0, 0].\n        y0_h = [R0, 0.0]\n        sol_h = solve_ivp(\n            hamiltonian_system, [0, T], y0_h, atol=atol, rtol=rtol\n        )\n        R_h_T = sol_h.y[0, -1]\n\n        # 2. Integrate for the particular part of the solution, driven by lambda(0).\n        #    Initial conditions: [R(0), lambda(0)] = [0, 1].\n        y0_p = [0.0, 1.0]\n        sol_p = solve_ivp(\n            hamiltonian_system, [0, T], y0_p, atol=atol, rtol=rtol\n        )\n        R_p_T = sol_p.y[0, -1]\n        \n        if abs(R_p_T) < 1e-10:\n            raise RuntimeError(\n                \"Shooting method failed: R_p(T) is near zero, \"\n                \"indicating lambda(0) has no influence on R(T).\"\n            )\n\n        # 3. Compute the correct initial costate lambda(0) such that R(T) = RT.\n        #    From linearity: RT = R_h(T) + lambda_0 * R_p(T)\n        lambda_0 = (RT - R_h_T) / R_p_T\n\n        # --- Final Calculation ---\n\n        # The optimal initial control is derived from the optimal lambda_0.\n        u0_opt = -theta * lambda_0\n\n        # Define the augmented system including the cost functional integral V(t).\n        def augmented_system(t, y):\n            \"\"\"\n            Represents the system for [R, lambda, V], where V is the accumulated cost.\n            \"\"\"\n            R, lam, V = y\n            R_dot = -delta * R - theta_sq * lam\n            lam_dot = -q * R + delta * lam\n            V_dot = 0.5 * (theta_sq * lam**2 + q * R**2)\n            return [R_dot, lam_dot, V_dot]\n\n        # 4. Integrate the full system forward with the correct initial conditions\n        #    [R(0), lambda(0), V(0)] to find the optimal trajectory and cost.\n        y0_final = [R0, lambda_0, 0.0]\n        sol_final = solve_ivp(\n            augmented_system, [0, T], y0_final, atol=atol, rtol=rtol\n        )\n\n        # The minimized total cost J is the value of V(T).\n        J_opt = sol_final.y[2, -1]\n        \n        # Verify terminal condition is met within the specified tolerance of 1e-8.\n        R_T_final = sol_final.y[0, -1]\n        if abs(R_T_final - RT) > 1e-8:\n             raise RuntimeError(\n                f\"Terminal condition not met. Error: {abs(R_T_final - RT)}\"\n            )\n\n        # Append the formatted results for the current test case.\n        # Results are rounded to 6 decimal places as specified.\n        results.append(f\"[{u0_opt:.6f},{J_opt:.6f}]\")\n\n    # Print the final output in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "This final practice demonstrates the full power of the shooting method by applying it to the non-linear neoclassical growth model, a cornerstone of modern macroeconomics. Unlike linear models, this system does not permit analytical shortcuts like superposition, requiring a more general and robust numerical approach. By finding the initial consumption that guides the economy to its long-run steady state, you will gain hands-on experience solving the kinds of non-linear boundary value problems that are common in cutting-edge economic research [@problem_id:2429167].", "id": "2429167", "problem": "Consider the deterministic finite-horizon optimal growth model with Constant Relative Risk Aversion (CRRA) preferences and Cobb–Douglas production. Let $t \\in \\{0,1,\\dots,T\\}$ denote time. The resource dynamics and first-order optimality condition imply the following objects.\n\n- Let $k_t$ denote capital at time $t$, $c_t$ consumption at time $t$, and parameters $\\alpha \\in (0,1)$, $\\beta \\in (0,1)$, $\\delta \\in (0,1)$, and $\\sigma > 0$.\n- Production is $f(k_t) = k_t^{\\alpha}$, marginal product is $f'(k_t) = \\alpha k_t^{\\alpha-1}$, and the gross resource available is $y_t = f(k_t) + (1-\\delta) k_t$.\n- The law of motion is $k_{t+1} = y_t - c_t$.\n- The Euler equation resulting from CRRA preferences with utility derivative $u'(c_t) = c_t^{-\\sigma}$ implies the forward recursion\n$$\nc_{t+1} = c_t \\left(\\beta \\left(f'(k_{t+1}) + 1 - \\delta\\right)\\right)^{1/\\sigma}\n$$\nwhen $k_{t+1} > 0$.\n\nFor any candidate initial consumption $c_0$ and given initial capital $k_0 > 0$, define the simulated $(T+1)$-step-ahead capital $k_{T+1}(c_0)$ by iterating the above two recursions forward from $(k_0,c_0)$ for $T$ steps and then one final application of the resource identity to obtain $k_{T+1} = f(k_T) + (1-\\delta)k_T - c_T$.\n\nDefine the target steady-state capital $k^{\\ast}$ as the unique positive solution to\n$$\nf'(k^{\\ast}) = \\frac{1}{\\beta} - (1 - \\delta),\n$$\ni.e.,\n$$\n\\alpha (k^{\\ast})^{\\alpha - 1} = \\frac{1}{\\beta} - (1 - \\delta).\n$$\n\nDefine the scalar error function\n$$\nE(c_0) = k_{T+1}(c_0) - k^{\\ast}.\n$$\n\nYour task is to write a complete program that, for each parameter set in the test suite below, computes a value $\\hat{c}_0$ satisfying $|E(\\hat{c}_0)| \\leq \\varepsilon$ for the specified tolerance $\\varepsilon$, subject to the feasibility restriction $0 < c_0 < y_0$ where $y_0 = f(k_0) + (1-\\delta) k_0$.\n\nThe test suite consists of four cases. In each case, the initial capital $k_0$ and the bracketing interval for $c_0$ are specified in terms of $k^{\\ast}$ and $y_0$ respectively. For each case, compute $k^{\\ast}$ from the parameters, then set $k_0$ and the bracket and initial candidate for $c_0$ as specified.\n\n- Case A (general feasibility and convergence):\n  - Parameters: $\\alpha = 0.33$, $\\beta = 0.96$, $\\delta = 0.08$, $\\sigma = 2.0$, $T = 75$.\n  - Initial capital: $k_0 = 0.8 \\, k^{\\ast}$.\n  - Define $y_0 = f(k_0) + (1-\\delta) k_0$.\n  - Bracket for $c_0$: $[c_{\\min}, c_{\\max}] = [0.05 \\, y_0, 0.95 \\, y_0]$.\n  - Initial candidate: $c_0^{(0)} = 0.5 \\, (c_{\\min} + c_{\\max})$.\n  - Tolerance: $\\varepsilon = 10^{-8}$.\n\n- Case B (short horizon with high curvature):\n  - Parameters: $\\alpha = 0.33$, $\\beta = 0.985$, $\\delta = 0.05$, $\\sigma = 10.0$, $T = 20$.\n  - Initial capital: $k_0 = 1.2 \\, k^{\\ast}$.\n  - Define $y_0 = f(k_0) + (1-\\delta) k_0$.\n  - Bracket for $c_0$: $[c_{\\min}, c_{\\max}] = [0.05 \\, y_0, 0.95 \\, y_0]$.\n  - Initial candidate: $c_0^{(0)} = 0.25 \\, (c_{\\min} + 3 c_{\\max})$.\n  - Tolerance: $\\varepsilon = 10^{-8}$.\n\n- Case C (very low initial capital and long horizon):\n  - Parameters: $\\alpha = 0.35$, $\\beta = 0.96$, $\\delta = 0.08$, $\\sigma = 1.5$, $T = 150$.\n  - Initial capital: $k_0 = 0.5 \\, k^{\\ast}$.\n  - Define $y_0 = f(k_0) + (1-\\delta) k_0$.\n  - Bracket for $c_0$: $[c_{\\min}, c_{\\max}] = [0.2 \\, y_0, 0.99 \\, y_0]$.\n  - Initial candidate: $c_0^{(0)} = 0.98 \\, y_0$.\n  - Tolerance: $\\varepsilon = 10^{-8}$.\n\n- Case D (initially at steady state):\n  - Parameters: $\\alpha = 0.3$, $\\beta = 0.95$, $\\delta = 0.08$, $\\sigma = 2.0$, $T = 60$.\n  - Initial capital: $k_0 = 1.0 \\, k^{\\ast}$.\n  - Define $y_0 = f(k_0) + (1-\\delta) k_0$.\n  - Bracket for $c_0$: $[c_{\\min}, c_{\\max}] = [0.2 \\, y_0, 0.95 \\, y_0]$.\n  - Initial candidate: $c_0^{(0)} = 0.5 \\, (c_{\\min} + c_{\\max})$.\n  - Tolerance: $\\varepsilon = 10^{-10}$.\n\nFor each case, return a floating-point approximation $\\hat{c}_0$ satisfying $|E(\\hat{c}_0)| \\leq \\varepsilon$ and $c_{\\min} \\leq \\hat{c}_0 \\leq c_{\\max}$. All quantities are unitless real numbers. The final output format must be a single line containing a comma-separated list of the four approximations, rounded to six decimal places, enclosed in square brackets, for example, $\\texttt{[0.123456,0.234567,0.345678,0.456789]}$.", "solution": "The problem statement is subjected to validation and is found to be valid. It represents a well-posed, scientifically grounded problem in computational economics, specifically an application of the shooting method to solve a deterministic finite-horizon optimal growth model. All parameters and functions are clearly defined, and the objectives are quantifiable and unambiguous.\n\nThe core of the problem is to find an initial consumption level, $\\hat{c}_0$, such that the capital stock at the end of a finite horizon, $k_{T+1}$, matches a predetermined target. This target is the model's steady-state capital, $k^{\\ast}$. This is a classic root-finding problem. We define an error function, $E(c_0)$, as the difference between the simulated terminal capital and the target steady-state capital:\n$$\nE(c_0) = k_{T+1}(c_0) - k^{\\ast}\n$$\nThe task is to find a $\\hat{c}_0$ such that $|E(\\hat{c}_0)|$ is smaller than a specified tolerance $\\varepsilon$.\n\nThe function $k_{T+1}(c_0)$ is constructed by simulating the economy's dynamics forward in time. Given an initial capital stock $k_0 > 0$ and a candidate initial consumption $c_0$, the trajectory of capital $\\{k_t\\}_{t=1}^{T+1}$ and consumption $\\{c_t\\}_{t=1}^{T}$ is determined by the following system of difference equations for $t = 0, 1, \\dots, T-1$:\n$$\nk_{t+1} = k_t^{\\alpha} + (1-\\delta)k_t - c_t\n$$\n$$\nc_{t+1} = c_t \\left(\\beta \\left(\\alpha k_{t+1}^{\\alpha-1} + 1 - \\delta\\right)\\right)^{1/\\sigma}\n$$\nThe simulation begins with $(k_0, c_0)$, computes $(k_1, c_1)$, then $(k_2, c_2)$, and so on, up to $(k_T, c_T)$. Finally, one more iteration of the capital accumulation equation yields $k_{T+1} = k_T^{\\alpha} + (1-\\delta)k_T - c_T$. This final value, $k_{T+1}$, is a function of the initial choice $c_0$.\n\nThe steady-state capital, $k^{\\ast}$, is derived from the condition that in a steady state, the marginal return on capital equals the rate of time preference, corrected for depreciation. It is the unique positive solution to $f'(k^{\\ast}) + 1 - \\delta = 1/\\beta$, which gives:\n$$\n\\alpha (k^{\\ast})^{\\alpha-1} = \\frac{1}{\\beta} - 1 + \\delta\n$$\nThis implies:\n$$\nk^{\\ast} = \\left(\\frac{\\alpha}{\\frac{1}{\\beta} - 1 + \\delta}\\right)^{\\frac{1}{1-\\alpha}}\n$$\n\nThe error function $E(c_0)$ is a strictly decreasing function of $c_0$. An increase in initial consumption, $c_0$, necessarily reduces initial investment, $k_1 = k_0^{\\alpha} + (1-\\delta)k_0 - c_0$. This lower capital stock, $k_1$, reduces the resources available in the next period, leading to a diminished path for both capital and consumption thereafter. Consequently, $k_{T+1}$ decreases as $c_0$ increases. This monotonicity guarantees that if a root to $E(c_0)=0$ exists, it is unique.\n\nGiven the monotonic nature of $E(c_0)$ and the provision of a bracketing interval $[c_{\\min}, c_{\\max}]$ for each test case, a robust root-finding algorithm is the appropriate tool. Brent's method is an excellent choice. It is available in the SciPy library as `scipy.optimize.brentq`. This method combines the security of the bisection method with the speed of the secant method and inverse quadratic interpolation. It is guaranteed to find a root within the specified bracket, provided the function values at the endpoints have opposite signs. As argued from economic principles, a low $c_0$ (like $c_{\\min}$) leads to over-accumulation of capital, so $k_{T+1}(c_{\\min}) > k^{\\ast}$ and $E(c_{\\min}) > 0$. Conversely, a high $c_0$ (like $c_{\\max}$) leads to capital depletion, so $k_{T+1}(c_{\\max}) < k^{\\ast}$ and $E(c_{\\max}) < 0$. The provided brackets are therefore valid for Brent's method. While the problem statement mentions an \"initial candidate\" $c_0^{(0)}$, this is not used by bracketing algorithms and is ignored in favor of the more robust bracket-based approach.\n\nThe computational procedure for each test case is as follows:\n$1$. Calculate the steady-state capital $k^{\\ast}$ using the given parameters $\\alpha$, $\\beta$, and $\\delta$.\n$2$. Define the initial capital $k_0$ and the available resources $y_0 = k_0^{\\alpha} + (1-\\delta)k_0$.\n$3$. Establish the search bracket $[c_{\\min}, c_{\\max}]$ using $y_0$ as specified.\n$4$. Implement the error function, $E(c_0)$, which performs the $T$-step forward simulation to compute $k_{T+1}(c_0)$ and returns the difference $k_{T+1}(c_0) - k^{\\ast}$. During the simulation, a check must be made to ensure that capital remains positive ($k_t > 0$). If a choice of $c_0$ leads to a non-positive capital stock at any step $t$, the path is economically infeasible. This occurs when $c_0$ is too high. In such cases, the function should return a large-magnitude negative value to correctly signal to the solver that this $c_0$ lies on the side of the root corresponding to low terminal capital.\n$5$. Use `scipy.optimize.brentq` to find the root $\\hat{c}_0$ of the error function $E(c_0)$ within the bracket $[c_{\\min}, c_{\\max}]$ to a high degree of precision, which will satisfy the problem's tolerance requirement on the function value.\n$6$. The computed value $\\hat{c}_0$ for each case is collected and formatted for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves for the optimal initial consumption c_0 for four different\n    parameterizations of a deterministic optimal growth model.\n    \"\"\"\n\n    test_cases = [\n        # Case A: General feasibility and convergence\n        {\n            'alpha': 0.33, 'beta': 0.96, 'delta': 0.08, 'sigma': 2.0, 'T': 75,\n            'k0_factor': 0.8, 'c_min_factor': 0.05, 'c_max_factor': 0.95,\n            'tolerance': 1e-8,\n        },\n        # Case B: Short horizon with high curvature\n        {\n            'alpha': 0.33, 'beta': 0.985, 'delta': 0.05, 'sigma': 10.0, 'T': 20,\n            'k0_factor': 1.2, 'c_min_factor': 0.05, 'c_max_factor': 0.95,\n            'tolerance': 1e-8,\n        },\n        # Case C: Very low initial capital and long horizon\n        {\n            'alpha': 0.35, 'beta': 0.96, 'delta': 0.08, 'sigma': 1.5, 'T': 150,\n            'k0_factor': 0.5, 'c_min_factor': 0.2, 'c_max_factor': 0.99,\n            'tolerance': 1e-8,\n        },\n        # Case D: Initially at steady state\n        {\n            'alpha': 0.3, 'beta': 0.95, 'delta': 0.08, 'sigma': 2.0, 'T': 60,\n            'k0_factor': 1.0, 'c_min_factor': 0.2, 'c_max_factor': 0.95,\n            'tolerance': 1e-10,\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        c0_hat = compute_c0_hat_for_case(params)\n        results.append(c0_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\ndef compute_c0_hat_for_case(params):\n    \"\"\"\n    Computes the optimal initial consumption for a single parameter set.\n    \"\"\"\n    alpha = params['alpha']\n    beta = params['beta']\n    delta = params['delta']\n    sigma = params['sigma']\n    T = params['T']\n\n    # 1. Calculate steady-state capital k_star\n    # From alpha * (k_star)^(alpha - 1) = 1/beta - 1 + delta\n    rhs = 1.0 / beta - 1.0 + delta\n    k_star = (alpha / rhs) ** (1.0 / (1.0 - alpha))\n\n    # 2. Set up initial conditions\n    k0 = params['k0_factor'] * k_star\n    y0 = k0**alpha + (1.0 - delta) * k0\n\n    # 3. Establish the search bracket for c0\n    c_min = params['c_min_factor'] * y0\n    c_max = params['c_max_factor'] * y0\n    \n    def error_function(c0):\n        \"\"\"\n        Computes the error E(c0) = k_{T+1}(c0) - k_star.\n        This function closes over the model parameters for the specific case.\n        \"\"\"\n        k = k0\n        c = c0\n\n        for _ in range(T):\n            # Law of motion for capital\n            k_next = k**alpha + (1.0 - delta) * k - c\n            \n            # Check for economic feasibility. If capital becomes non-positive,\n            # this c0 is too high, leading to a very low k_{T+1}.\n            # Return a large negative error to guide the solver.\n            if k_next <= 0:\n                return -1e100\n\n            # Euler equation for consumption\n            f_prime = alpha * k_next**(alpha - 1.0)\n            c_next = c * (beta * (f_prime + 1.0 - delta))**(1.0 / sigma)\n            \n            k = k_next\n            c = c_next\n\n        # After T steps, k is k_T and c is c_T. Compute k_{T+1}.\n        k_T_plus_1 = k**alpha + (1.0 - delta) * k - c\n        \n        return k_T_plus_1 - k_star\n\n    # 4. Use brentq to find the root of the error function\n    # The default tolerances of brentq are more than sufficient to meet the\n    # problem's requirement on the error value.\n    try:\n        c0_hat = brentq(error_function, c_min, c_max)\n    except ValueError:\n        # This might happen if error_function(c_min) and error_function(c_max)\n        # do not have opposite signs, indicating a problem with the setup.\n        # For this well-behaved problem, this is not expected.\n        return np.nan\n\n    return c0_hat\n\nsolve()\n```"}]}