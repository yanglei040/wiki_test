{"hands_on_practices": [{"introduction": "The heart of the Krusell-Smith algorithm is the \"perceived law of motion\" that agents use to forecast the future. A crucial, and sometimes subtle, choice is which aggregate variable to forecast. This exercise challenges you to think critically about this choice by exploring a hypothetical scenario: what if agents forecast the price of capital, $q_t$, instead of its quantity, $K_t$? By working through this question, you will solidify your understanding that the forecasted variable must be informative about the future aggregate state for the algorithm to succeed. [@problem_id:2441720]", "id": "2441720", "problem": "Consider a heterogeneous-agent, incomplete-markets economy used to implement the Krusell–Smith algorithm. Time is discrete. Aggregate output is produced by a representative firm with technology $Y_t = Z_t K_t^{\\alpha} L_t^{1-\\alpha}$, where $Z_t$ is an aggregate productivity shock following a finite-state Markov chain, $K_t$ is the aggregate installed capital stock at the beginning of period $t$, $L_t$ is aggregate labor supplied inelastically, and $\\alpha \\in (0,1)$. Capital accumulates according to $K_{t+1} = (1-\\delta) K_t + I_t$, with depreciation rate $\\delta \\in (0,1)$. A competitive capital-goods sector transforms consumption $I_t$ into installed capital $I_t$ one-for-one within the period. There are no investment adjustment costs. Let $q_t$ denote the period-$t$ price, in units of the consumption good, of one unit of installed capital delivered within period $t$. Households can buy and sell installed capital at price $q_t$ and rent it to firms; factor markets are competitive.\n\nIn a standard implementation of the Krusell–Smith algorithm, agents approximate expectations needed for optimal saving by maintaining and updating a perceived law of motion for an aggregate variable such as $\\log K_{t+1}$ as a function of current aggregates. Suppose instead that agents maintain and update a perceived law of motion for $\\log q_{t+1}$ (again as a function of current aggregates) and use this to form the expectations in their intertemporal optimality conditions.\n\nIn this benchmark environment, which statement is correct about replacing a perceived law of motion for $\\log K_{t+1}$ with one for $\\log q_{t+1}$?\n\nA. Because the capital good is produced one-for-one from consumption with no adjustment costs under perfect competition, $q_t$ equals $1$ in equilibrium at all dates. A forecast of $\\log q_{t+1}$ is therefore a constant and does not summarize the aggregate state needed to predict future factor prices; replacing the $\\log K_{t+1}$ forecast with $\\log q_{t+1}$ generally fails unless additional aggregates (for example, $K_t$ or factor prices) are also forecast.\n\nB. There is a one-to-one mapping between $q_t$ and $K_t$ in this environment, so forecasting $\\log q_{t+1}$ is equivalent to forecasting $\\log K_{t+1}$ and yields the same equilibrium.\n\nC. Including the aggregate productivity state $Z_t$ as an argument in the $\\log q_{t+1}$ forecasting rule suffices to recover all information needed for pricing, so the algorithm converges exactly as in the benchmark that forecasts $\\log K_{t+1}$.\n\nD. The relationship $q_{t+1} = \\alpha Z_{t+1} K_{t+1}^{\\alpha-1}$ pins down $K_{t+1}$ from $q_{t+1}$, so forecasting $\\log q_{t+1}$ is sufficient for implementing the algorithm.", "solution": "The problem statement describes a standard incomplete-markets model with aggregate risk, the type for which the Krusell-Smith algorithm is designed. The validity of the proposed alternative forecasting procedure hinges on the economic relationship between the price of capital, $q_t$, and the aggregate state of the economy.\n\nFirst, we must establish the equilibrium price of installed capital, $q_t$. The problem states that:\n1.  A competitive capital-goods sector exists.\n2.  This sector transforms consumption goods, $I_t$, into installed capital, $I_t$, on a one-for-one basis.\n3.  There are no investment adjustment costs.\n\nThese three conditions are decisive. In a competitive market, firms enter until profits are driven to zero. The cost to a capital-goods producer of creating one unit of new capital is, by the one-for-one transformation, one unit of the consumption good. The price of the consumption good is the numeraire, so its price is $1$. Therefore, the price at which new capital goods are sold must be $1$.\n\nNow, consider the market for installed capital. Households can acquire capital in two ways: buy a newly produced unit of capital for a price of $1$, or buy a unit of existing, installed capital from another household at price $q_t$. For the market to be in equilibrium, there can be no arbitrage opportunities.\n- If $q_t > 1$, an agent could buy a newly produced unit of capital for a price of $1$ and immediately sell it as installed capital for a price of $q_t$, realizing an instantaneous, risk-free profit of $q_t - 1 > 0$. This would lead to infinite demand for new capital and infinite supply of installed capital, which cannot be an equilibrium.\n- If $q_t < 1$, no agent would purchase newly produced capital at a price of $1$ when they could buy existing installed capital for a lower price, $q_t$. This would imply that investment $I_t$ must be zero. However, for a growing economy or even one in a steady state with positive depreciation $\\delta > 0$, positive investment is required to replace depreciated capital ($I_t = \\delta K_t$ in steady state). As long as investment $I_t > 0$, the price of installed capital cannot be sustained below the cost of producing new capital.\n\nTherefore, in any equilibrium with $I_t > 0$, it must be that $q_t = 1$. This holds for all time periods $t$. The price of installed capital is constant and equal to its replacement cost of $1$.\n\nThe household's problem requires forming expectations about future income to make an optimal consumption-savings decision. Individual income depends on the rental rate of capital, $r_{t+1}$, and the wage rate, $w_{t+1}$. In competitive factor markets, these are given by the marginal products of the factors:\n$$r_{t+1} = \\alpha Z_{t+1} K_{t+1}^{\\alpha-1} L^{1-\\alpha}$$\n$$w_{t+1} = (1-\\alpha) Z_{t+1} K_{t+1}^{\\alpha} L^{-\\alpha}$$\n(Assuming $L_t = L$ is constant).\nTo forecast these prices, the household must forecast the future aggregate state, which consists of the aggregate capital stock $K_{t+1}$ and the aggregate productivity shock $Z_{t+1}$. The standard Krusell-Smith algorithm posits a perceived law of motion for the aggregate capital stock, e.g., $\\log K_{t+1} = \\beta_0 + \\beta_1 \\log K_t + \\beta_2 Z_t$. This allows agents to form the necessary expectations.\n\nThe proposed alternative is to forecast $\\log q_{t+1}$. Since we have established that $q_{t+1} = 1$ in equilibrium, this means forecasting $\\log q_{t+1} = \\log(1) = 0$. A forecast of a constant ($0$) provides no information about the evolution of $K_t$ or $Z_t$. An agent who knows that $\\log q_{t+1}$ will be $0$ gains no knowledge about what $K_{t+1}$ will be, and thus cannot form expectations about $r_{t+1}$ and $w_{t+1}$. This makes it impossible to solve the household's optimization problem. The forecasting mechanism fails entirely.\n\nNow, we evaluate each option based on this derivation.\n\n**A. Because the capital good is produced one-for-one from consumption with no adjustment costs under perfect competition, $q_t$ equals $1$ in equilibrium at all dates. A forecast of $\\log q_{t+1}$ is therefore a constant and does not summarize the aggregate state needed to predict future factor prices; replacing the $\\log K_{t+1}$ forecast with $\\log q_{t+1}$ generally fails unless additional aggregates (for example, $K_t$ or factor prices) are also forecast.**\n\nThis statement is entirely consistent with our analysis. It correctly identifies that $q_t = 1$ due to the lack of adjustment costs. It correctly concludes that forecasting $\\log q_{t+1}$ is forecasting a constant, which is uninformative about the aggregate state ($K_{t+1}, Z_{t+1}$) needed to predict future factor prices. Consequently, the algorithm based on this forecast would fail. The final clause, that it fails unless other variables are also forecast, is also correct; forecasting a constant is useless by itself. Therefore, this statement is **Correct**.\n\n**B. There is a one-to-one mapping between $q_t$ and $K_t$ in this environment, so forecasting $\\log q_{t+1}$ is equivalent to forecasting $\\log K_{t+1}$ and yields the same equilibrium.**\n\nThis is incorrect. As established, $q_t$ is a constant, $q_t = 1$. The aggregate capital stock, $K_t$, is a time-varying state variable. There is no one-to-one mapping between a constant and a variable. This statement might be true in a model with investment adjustment costs, where a higher $K_t$ could imply a different level of investment and thus a different $q_t$, but the problem explicitly rules out such costs. Therefore, this statement is **Incorrect**.\n\n**C. Including the aggregate productivity state $Z_t$ as an argument in the $\\log q_{t+1}$ forecasting rule suffices to recover all information needed for pricing, so the algorithm converges exactly as in the benchmark that forecasts $\\log K_{t+1}$.**\n\nThis is incorrect. The rational expectations equilibrium requires that the perceived law of motion be consistent with the actual law of motion. The actual law of motion for $q_{t+1}$ is $q_{t+1} = 1$, which implies $\\log q_{t+1} = 0$. So, any perceived law of motion $\\log q_{t+1} = f(K_t, Z_t)$ must collapse to the constant function $f(K_t, Z_t) = 0$. Learning that the forecast is always $0$, regardless of the current state $(K_t, Z_t)$, provides no information to the agent about the future state $K_{t+1}$. The fundamental problem is that the forecasted variable, $q_{t+1}$, is not informative, not that the forecasting rule is missing an argument. Therefore, this statement is **Incorrect**.\n\n**D. The relationship $q_{t+1} = \\alpha Z_{t+1} K_{t+1}^{\\alpha-1}$ pins down $K_{t+1}$ from $q_{t+1}$, so forecasting $\\log q_{t+1}$ is sufficient for implementing the algorithm.**\n\nThis is incorrect. The expression on the right-hand side, $\\alpha Z_{t+1} K_{t+1}^{\\alpha-1}$ (assuming $L_t=1$), is the marginal product of capital, which equals the rental rate $r_{t+1}$ in equilibrium. The statement posits that the price of the capital asset, $q_{t+1}$, is equal to its rental rate, $r_{t+1}$. This confuses a stock price with a flow return. The asset's price, $q_{t+1}$, is the present value of all its future returns, not just the next period's return. In this model, $q_{t+1}=1$, while $r_{t+1}$ is a variable that depends on $K_{t+1}$ and $Z_{t+1}$. The posited relationship is fundamentally false. Therefore, this statement is **Incorrect**.\n\nIn summary, the absence of investment adjustment costs is the critical feature of the environment, which implies $q_t=1$ for all $t$. This renders $q_t$ an uninformative state variable for forecasting purposes.", "answer": "$$\\boxed{A}$$"}, {"introduction": "In a heterogeneous-agent model, the aggregate law of motion emerges from the combined actions of many different individuals. While the Krusell-Smith algorithm seeks a forecasting rule that is correct on average, there will always be period-by-period forecast errors. This practical coding exercise allows you to decompose the aggregate forecast error and attribute it to the specific saving behavior of different wealth quintiles, providing a clear link from micro-level heterogeneity to macro-level dynamics. [@problem_id:2441773]", "id": "2441773", "problem": "Consider a heterogeneous-agent setting with a finite partition of the wealth distribution into five disjoint groups indexed by quintiles. Let the aggregate capital at date $t$ be $K_t &gt; 0$. Each quintile $i \\in \\{1,2,3,4,5\\}$ holds a time-$t$ capital share $s_{i,t}$ satisfying $s_{i,t} \\ge 0$ and $\\sum_{i=1}^{5} s_{i,t} = 1$, and accumulates its capital to date $t+1$ by a realized gross accumulation factor $r_{i,t} &gt; 0$. Therefore the aggregate actual next-period capital is\n$$\nK_{t+1}^{actual} \\equiv \\sum_{i=1}^{5} K_{i,t+1} = \\sum_{i=1}^{5} s_{i,t}\\, r_{i,t}\\, K_t = K_t \\sum_{i=1}^{5} s_{i,t}\\, r_{i,t}.\n$$\nSuppose a log-linear forecast law of motion for aggregate capital of the form\n$$\n\\log K_{t+1}^{forecast} = a_0 + a_1 \\log K_t + a_2 z_t,\n$$\nwhere $a_0, a_1, a_2 \\in \\mathbb{R}$ are given constants and $z_t \\in \\mathbb{R}$ is a given scalar aggregate shock at date $t$. Define the implied forecast gross factor\n$$\nG_t^{forecast} \\equiv \\frac{K_{t+1}^{forecast}}{K_t} = \\exp\\!\\big(a_0 + (a_1 - 1)\\log K_t + a_2 z_t\\big).\n$$\nThe forecast error for the law of motion is\n$$\n\\varepsilon_t \\equiv K_{t+1}^{actual} - K_{t+1}^{forecast}.\n$$\nDefine the contribution of quintile $i$ to the forecast error as\n$$\nc_{i,t} \\equiv K_{i,t+1} - s_{i,t}\\, K_t\\, G_t^{forecast} = K_t\\, s_{i,t}\\, \\big(r_{i,t} - G_t^{forecast}\\big),\n$$\nso that $\\sum_{i=1}^{5} c_{i,t} = \\varepsilon_t$ holds by construction.\n\nGiven the above definitions, write a program that, for each parameter set in the test suite below, computes the list of quintile contributions $\\big[c_{1,t}, c_{2,t}, c_{3,t}, c_{4,t}, c_{5,t}\\big]$.\n\nAll quantities are pure numbers without physical units.\n\nTest suite (each line specifies $\\big(K_t, a_0, a_1, a_2, z_t, \\{s_{i,t}\\}_{i=1}^{5}, \\{r_{i,t}\\}_{i=1}^{5}\\big)$):\n\n- Case A (general case):\n  - $K_t = 100.0$, $a_0 = 0.0$, $a_1 = 1.0$, $a_2 = 0.01$, $z_t = 1.0$,\n  - $\\{s_{i,t}\\} = [0.10,\\, 0.15,\\, 0.20,\\, 0.25,\\, 0.30]$,\n  - $\\{r_{i,t}\\} = [1.05,\\, 1.03,\\, 1.02,\\, 1.01,\\, 1.00]$.\n\n- Case B (zero forecast error by construction):\n  - $K_t = 250.0$, $a_0 = 0.0$, $a_1 = 1.0$, $a_2 = 0.0$, $z_t = 0.0$,\n  - $\\{s_{i,t}\\} = [0.05,\\, 0.10,\\, 0.15,\\, 0.30,\\, 0.40]$,\n  - $\\{r_{i,t}\\} = [1.0,\\, 1.0,\\, 1.0,\\, 1.0,\\, 1.0]$.\n\n- Case C (boundary with a zero share and negative shock):\n  - $K_t = 80.0$, $a_0 = 0.02$, $a_1 = 0.98$, $a_2 = -0.005$, $z_t = -1.0$,\n  - $\\{s_{i,t}\\} = [0.0,\\, 0.20,\\, 0.30,\\, 0.25,\\, 0.25]$,\n  - $\\{r_{i,t}\\} = [1.04,\\, 0.99,\\, 1.01,\\, 1.00,\\, 1.02]$.\n\nOutput specification:\n- For each case, output the list $\\big[c_{1,t}, c_{2,t}, c_{3,t}, c_{4,t}, c_{5,t}\\big]$ rounded to six decimal places.\n- Aggregate the three case results into a single list in the order A, B, C.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each element is itself a bracketed list of five numbers rounded to six decimal places (for example, \"[[x11,x12,x13,x14,x15],[x21,x22,x23,x24,x25],[x31,x32,x33,x34,x35]]\").", "solution": "The problem as stated is subjected to rigorous validation before any attempt at a solution is made.\n\n**Step 1: Extract Givens**\n\nThe problem furnishes the following definitions, variables, and relationships:\n- Aggregate capital at time $t$: $K_t > 0$.\n- Number of quintiles: $5$.\n- Capital share of quintile $i$ at time $t$: $s_{i,t}$, with $s_{i,t} \\ge 0$ and $\\sum_{i=1}^{5} s_{i,t} = 1$.\n- Realized gross accumulation factor for quintile $i$ at time $t$: $r_{i,t} > 0$.\n- Aggregate actual next-period capital: $K_{t+1}^{\\text{actual}} = K_t \\sum_{i=1}^{5} s_{i,t}\\, r_{i,t}$.\n- Log-linear forecast law of motion for aggregate capital: $\\log K_{t+1}^{\\text{forecast}} = a_0 + a_1 \\log K_t + a_2 z_t$.\n- Coefficients of the law of motion: $a_0, a_1, a_2 \\in \\mathbb{R}$.\n- Aggregate shock at time $t$: $z_t \\in \\mathbb{R}$.\n- Implied forecast gross factor: $G_t^{\\text{forecast}} = \\frac{K_{t+1}^{\\text{forecast}}}{K_t} = \\exp\\!\\big(a_0 + (a_1 - 1)\\log K_t + a_2 z_t\\big)$.\n- Forecast error: $\\varepsilon_t = K_{t+1}^{\\text{actual}} - K_{t+1}^{\\text{forecast}}$.\n- Contribution of quintile $i$ to the forecast error: $c_{i,t} = K_t\\, s_{i,t}\\, \\big(r_{i,t} - G_t^{\\text{forecast}}\\big)$.\n\nThree test cases are provided with specific values for the set of parameters $\\big(K_t, a_0, a_1, a_2, z_t, \\{s_{i,t}\\}_{i=1}^{5}, \\{r_{i,t}\\}_{i=1}^{5}\\big)$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the required criteria:\n- **Scientifically Grounded**: The problem is set within the context of heterogeneous-agent macroeconomic models, specifically related to the Krusell-Smith algorithm, which is a standard tool in computational economics. The mathematical formalization is based on established economic modeling principles. It does not violate any scientific or mathematical laws.\n- **Well-Posed**: The problem is a direct computational task. For each test case, all necessary parameters are provided, and the formulas for the target quantities, $c_{i,t}$, are explicitly given. The calculation leads to a unique, stable, and meaningful numerical result for each quintile.\n- **Objective**: The problem is stated in precise, unambiguous mathematical language. There are no subjective, opinion-based, or non-formalizable components. The terms used are standard in the field.\n- **Complete and Consistent**: The setup is self-contained. The definitions are consistent with each other, for instance, it is correctly noted that $\\sum_{i=1}^{5} c_{i,t} = \\varepsilon_t$. The data provided for each test case is sufficient to perform the required computations. The condition $\\sum s_{i,t}=1$ is met for all cases.\n\n**Step 3: Verdict and Action**\n\nThe problem is well-defined, scientifically grounded, and computationally tractable. It is deemed **valid**. A solution will be provided.\n\nThe objective is to compute the list of quintile contributions to the forecast error, $\\big[c_{1,t}, c_{2,t}, c_{3,t}, c_{4,t}, c_{5,t}\\big]$, for each of the three supplied parameter sets. The procedure is a direct application of the definitions provided.\n\nFor each parameter set, the calculation proceeds in two stages:\n1.  Compute the implied forecast gross factor, $G_t^{\\text{forecast}}$, using the formula:\n    $$\n    G_t^{\\text{forecast}} = \\exp\\!\\big(a_0 + (a_1 - 1)\\log K_t + a_2 z_t\\big)\n    $$\n2.  For each quintile $i \\in \\{1,2,3,4,5\\}$, compute its contribution to the forecast error, $c_{i,t}$, using the formula:\n    $$\n    c_{i,t} = K_t\\, s_{i,t}\\, \\big(r_{i,t} - G_t^{\\text{forecast}}\\big)\n    $$\n\nThis procedure is now applied to each test case.\n\n**Case A:**\nGiven parameters: $K_t = 100.0$, $a_0 = 0.0$, $a_1 = 1.0$, $a_2 = 0.01$, $z_t = 1.0$, $\\{s_{i,t}\\} = [0.10, 0.15, 0.20, 0.25, 0.30]$, and $\\{r_{i,t}\\} = [1.05, 1.03, 1.02, 1.01, 1.00]$.\nFirst, compute $G_t^{\\text{forecast}}$:\n$$\nG_t^{\\text{forecast}} = \\exp\\big(0.0 + (1.0 - 1.0)\\log(100.0) + 0.01 \\cdot 1.0\\big) = \\exp(0.01) \\approx 1.01005017\n$$\nNext, compute $c_{i,t}$ for each quintile:\n- $c_{1,t} = 100.0 \\cdot 0.10 \\cdot (1.05 - 1.01005017) \\approx 0.399498$\n- $c_{2,t} = 100.0 \\cdot 0.15 \\cdot (1.03 - 1.01005017) \\approx 0.299248$\n- $c_{3,t} = 100.0 \\cdot 0.20 \\cdot (1.02 - 1.01005017) \\approx 0.198997$\n- $c_{4,t} = 100.0 \\cdot 0.25 \\cdot (1.01 - 1.01005017) \\approx -0.001254$\n- $c_{5,t} = 100.0 \\cdot 0.30 \\cdot (1.00 - 1.01005017) \\approx -0.301505$\nThe resulting list, rounded to six decimal places, is $[0.399498, 0.299248, 0.198997, -0.001254, -0.301505]$.\n\n**Case B:**\nGiven parameters: $K_t = 250.0$, $a_0 = 0.0$, $a_1 = 1.0$, $a_2 = 0.0$, $z_t = 0.0$, $\\{s_{i,t}\\} = [0.05, 0.10, 0.15, 0.30, 0.40]$, and $\\{r_{i,t}\\} = [1.0, 1.0, 1.0, 1.0, 1.0]$.\nFirst, compute $G_t^{\\text{forecast}}$:\n$$\nG_t^{\\text{forecast}} = \\exp\\big(0.0 + (1.0 - 1.0)\\log(250.0) + 0.0 \\cdot 0.0\\big) = \\exp(0.0) = 1.0\n$$\nSince $r_{i,t} = 1.0$ for all $i$ and $G_t^{\\text{forecast}} = 1.0$, the term $(r_{i,t} - G_t^{\\text{forecast}})$ is identically zero for all quintiles.\nTherefore, $c_{i,t} = 0.0$ for all $i \\in \\{1,2,3,4,5\\}$.\nThe resulting list is $[0.000000, 0.000000, 0.000000, 0.000000, 0.000000]$.\n\n**Case C:**\nGiven parameters: $K_t = 80.0$, $a_0 = 0.02$, $a_1 = 0.98$, $a_2 = -0.005$, $z_t = -1.0$, $\\{s_{i,t}\\} = [0.0, 0.20, 0.30, 0.25, 0.25]$, and $\\{r_{i,t}\\} = [1.04, 0.99, 1.01, 1.00, 1.02]$.\nFirst, compute $G_t^{\\text{forecast}}$:\n$$\nG_t^{\\text{forecast}} = \\exp\\big(0.02 + (0.98 - 1.0)\\log(80.0) + (-0.005) \\cdot (-1.0)\\big) = \\exp\\big(0.025 - 0.02\\log(80.0)\\big) \\approx 0.939281\n$$\nNext, compute $c_{i,t}$ for each quintile:\n- $c_{1,t} = 80.0 \\cdot 0.0 \\cdot (1.04 - 0.939281) = 0.000000$\n- $c_{2,t} = 80.0 \\cdot 0.20 \\cdot (0.99 - 0.939281) \\approx 0.811498$\n- $c_{3,t} = 80.0 \\cdot 0.30 \\cdot (1.01 - 0.939281) \\approx 1.697247$\n- $c_{4,t} = 80.0 \\cdot 0.25 \\cdot (1.00 - 0.939281) \\approx 1.214373$\n- $c_{5,t} = 80.0 \\cdot 0.25 \\cdot (1.02 - 0.939281) \\approx 1.614373$\nThe resulting list, rounded to six decimal places, is $[0.000000, 0.811498, 1.697247, 1.214373, 1.614373]$.\n\nThe final program will consolidate these results into the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes quintile contributions to the forecast error for a heterogeneous-agent model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (K_t, a_0, a_1, a_2, z_t, {s_i,t}, {r_i,t})\n        (100.0, 0.0, 1.0, 0.01, 1.0,\n         np.array([0.10, 0.15, 0.20, 0.25, 0.30]),\n         np.array([1.05, 1.03, 1.02, 1.01, 1.00])),\n\n        # Case B: (K_t, a_0, a_1, a_2, z_t, {s_i,t}, {r_i,t})\n        (250.0, 0.0, 1.0, 0.0, 0.0,\n         np.array([0.05, 0.10, 0.15, 0.30, 0.40]),\n         np.array([1.0, 1.0, 1.0, 1.0, 1.0])),\n\n        # Case C: (K_t, a_0, a_1, a_2, z_t, {s_i,t}, {r_i,t})\n        (80.0, 0.02, 0.98, -0.005, -1.0,\n         np.array([0.0, 0.20, 0.30, 0.25, 0.25]),\n         np.array([1.04, 0.99, 1.01, 1.00, 1.02])),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Unpack parameters for the current case\n        K_t, a0, a1, a2, z_t, s_it, r_it = case\n\n        # Step 1: Compute the implied forecast gross factor, G_t^forecast.\n        # G_t^forecast = exp(a0 + (a1 - 1)*log(K_t) + a2*z_t)\n        log_K_t = np.log(K_t)\n        G_t_forecast = np.exp(a0 + (a1 - 1) * log_K_t + a2 * z_t)\n\n        # Step 2: Compute the contribution of each quintile to the forecast error.\n        # c_i,t = K_t * s_i,t * (r_i,t - G_t^forecast)\n        c_it = K_t * s_it * (r_it - G_t_forecast)\n        \n        all_results.append(c_it)\n\n    # Format the output as specified: a list of lists, with numbers rounded\n    # to six decimal places, no spaces.\n    case_results_str = []\n    for result_list in all_results:\n        # Format each number to '.6f' and join into a string \"[n1,n2,...]\"\n        formatted_numbers = [f\"{x:.6f}\" for x in result_list]\n        case_str = f\"[{','.join(formatted_numbers)}]\"\n        case_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    # Example: [[x1,x2],[y1,y2]]\n    final_output_str = f\"[{','.join(case_results_str)}]\"\n    print(final_output_str)\n\nsolve()\n```"}, {"introduction": "Standard implementations of the Krusell-Smith algorithm often rely on a simple log-linear forecasting rule for its tractability. However, the true aggregate law of motion may be highly non-linear or exhibit distinct behavior across different economic regimes. This advanced hands-on problem guides you to explore the limitations of linear rules by comparing one to a powerful non-parametric alternative, a Gaussian Process, and to implement diagnostics that reveal whether the more flexible model can uncover these deeper structural features of the economy. [@problem_id:2441747]", "id": "2441747", "problem": "Consider a heterogeneous-agent economy with aggregate capital as the state variable. In the Krusell–Smith algorithm, agents approximate the unknown aggregate law of motion for next-period aggregate capital by a forecasting rule. Let the aggregate state at time $t$ be $s_t = (K_t, z_t)$, where $K_t \\in \\mathbb{R}_+$ is aggregate capital and $z_t \\in \\{0,1\\}$ is an aggregate productivity regime following a two-state Markov chain. The unknown aggregate law of motion is a measurable function $G:\\mathbb{R}_+ \\times \\{0,1\\} \\to \\mathbb{R}_+$ such that $K_{t+1} = G(K_t, z_t) + \\varepsilon_t$, where $\\varepsilon_t$ is an independently and identically distributed innovation with zero mean and finite variance.\n\nYou will study whether a non-parametric forecasting rule based on a Gaussian Process (GP) discovers non-linear or regime-switching aggregate dynamics in $G$ relative to a linear Ordinary Least Squares (OLS) benchmark used in the canonical Krusell–Smith algorithm. You must implement both forecasters from first principles and evaluate them on synthetic data generated from four well-specified data-generating processes (DGPs) for $G$. Your program must output a list of booleans, one per DGP, indicating whether the GP both improves predictive accuracy out of sample and reveals non-linear curvature or regime-dependent slopes in the implied mapping from $K_t$ to $K_{t+1}$.\n\nFoundational base:\n- In the Krusell–Smith algorithm, agents form expectations about $K_{t+1}$ via a forecasting rule $F(K_t, z_t)$ and update this rule until it is consistent with simulated aggregate outcomes. The OLS benchmark uses a linear specification. A GP prior combined with a squared-exponential kernel yields a non-parametric posterior mean predictor that minimizes mean-squared error in the function space induced by the kernel.\n- Ordinary Least Squares (OLS) chooses coefficients to minimize the sum of squared residuals. Gaussian Process (GP) regression with a zero-mean prior and a positive-definite kernel $k(\\cdot,\\cdot)$ yields, for training inputs $X \\in \\mathbb{R}^{n \\times d}$ and targets $y \\in \\mathbb{R}^n$, the posterior mean at a new input $x_\\star$ equal to $m(x_\\star) = k(x_\\star, X)\\left[K(X,X)+\\sigma_n^2 I\\right]^{-1}y$, where $K(X,X)$ is the Gram matrix with entries $k(x_i,x_j)$, $\\sigma_n^2$ is observation noise variance, and $I$ is the identity matrix.\n\nYour tasks:\n- Generate data by simulating $T$ periods of $(K_t, z_t)$ with $K_{t+1} = \\min\\{\\max\\{G(K_t, z_t) + \\varepsilon_t, K_{\\min}\\}, K_{\\max}\\}$, where $K_{\\min} = 2$ and $K_{\\max} = 18$ are clipping bounds to ensure numerical stability. The aggregate shock $z_t$ is a two-state Markov chain with transition probabilities $p_{00} = \\mathbb{P}(z_{t+1}=0 \\mid z_t=0) = 0.9$ and $p_{11} = \\mathbb{P}(z_{t+1}=1 \\mid z_t=1) = 0.9$, and $p_{01} = 1 - p_{00}$, $p_{10} = 1 - p_{11}$. Initialize at $K_0 = 8$ and $z_0 = 1$. Draw $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$.\n- Split the simulated sample into a training set of size $T_{\\text{train}}$ and a test set of size $T_{\\text{test}}$ (consecutive in time). Fit two forecasting rules on the training set:\n  - A linear OLS forecaster $F_{\\text{lin}}(K,z) = \\beta_0 + \\beta_1 K + \\beta_2 z$, with coefficients estimated by minimizing the training mean squared error.\n  - A GP forecaster with zero-mean prior and squared-exponential kernel\n    $$k(x,x') = \\sigma_f^2 \\exp\\left(-\\tfrac{1}{2}\\sum_{j=1}^2 \\left(\\frac{x_j - x'_j}{\\ell_j}\\right)^2\\right),$$\n    where $x = [K, z]^\\top$, with fixed hyperparameters $\\sigma_f = 1$, $\\ell_1 = 2.5$ for the $K$ dimension, $\\ell_2 = 0.5$ for the $z$ dimension, and observation noise standard deviation $\\sigma_n = 0.05$. Use the GP posterior mean predictor on both training and test inputs.\n- Compute the out-of-sample mean squared errors on the test set:\n  $$\\text{MSE}_{\\text{lin}} = \\frac{1}{T_{\\text{test}}}\\sum_{t=1}^{T_{\\text{test}}} \\left(K_{t+1}^{\\text{test}} - F_{\\text{lin}}(K_t^{\\text{test}}, z_t^{\\text{test}})\\right)^2,$$\n  $$\\text{MSE}_{\\text{gp}} = \\frac{1}{T_{\\text{test}}}\\sum_{t=1}^{T_{\\text{test}}} \\left(K_{t+1}^{\\text{test}} - F_{\\text{gp}}(K_t^{\\text{test}}, z_t^{\\text{test}})\\right)^2,$$\n  and the relative improvement\n  $$\\Delta = \\frac{\\text{MSE}_{\\text{lin}} - \\text{MSE}_{\\text{gp}}}{\\text{MSE}_{\\text{lin}}}.$$\n- Diagnose non-linearity and regime dependence from the GP predictor by evaluating it on a uniform grid $\\{K_i\\}_{i=1}^N$ over $[K_{\\min}, K_{\\max}]$ with $N = 50$ and step size $h = (K_{\\max}-K_{\\min})/(N-1)$. For each regime $z \\in \\{0,1\\}$:\n  - Compute the discrete second derivative\n    $$D^{(2)}_i(z) = \\frac{F_{\\text{gp}}(K_{i+1}, z) - 2 F_{\\text{gp}}(K_i, z) + F_{\\text{gp}}(K_{i-1}, z)}{h^2}, \\quad i = 2,\\dots, N-1,$$\n    and define the curvature measure $C = \\max_{z \\in \\{0,1\\}}\\max_{2 \\le i \\le N-1} \\left|D^{(2)}_i(z)\\right|$.\n  - Compute the discrete first derivative\n    $$D^{(1)}_i(z) = \\frac{F_{\\text{gp}}(K_{i+1}, z) - F_{\\text{gp}}(K_{i-1}, z)}{2 h}, \\quad i = 2,\\dots, N-1,$$\n    and define the regime-dependent slope gap $S = \\max_{2 \\le i \\le N-1} \\left|D^{(1)}_i(1) - D^{(1)}_i(0)\\right|.$\n- Decision rule: declare that the non-parametric forecasting rule “discovers a non-linear or regime-switching aggregate dynamic” if and only if $\\Delta \\ge \\tau_\\Delta$ and either $C \\ge \\tau_C$ or $S \\ge \\tau_S$, with thresholds $\\tau_\\Delta = 0.05$, $\\tau_C = 0.002$, and $\\tau_S = 0.05$.\n\nTest suite:\n- Use the following four DGPs for $G(K,z)$, simulation lengths, and noise levels. All constants and coefficients are real numbers.\n  1. Case L (linear “happy path”): $G(K,z) = a_0 + a_1 K + a_2 z$ with $a_0 = 0.5$, $a_1 = 0.92$, $a_2 = 0.1$, innovation standard deviation $\\sigma_\\varepsilon = 0.03$, $T_{\\text{train}} = 200$, $T_{\\text{test}} = 400$.\n  2. Case N (concave nonlinearity): $G(K,z) = a_0 + a_1 K + a_3 K^2 + a_2 z$ with $a_0 = 0.5$, $a_1 = 0.95$, $a_3 = -0.002$, $a_2 = 0.1$, $\\sigma_\\varepsilon = 0.03$, $T_{\\text{train}} = 200$, $T_{\\text{test}} = 400$.\n  3. Case R (regime-switching slope): $G(K,z) = a_0 + (a_1 + a_4 z) K + a_2 z$ with $a_0 = 0.5$, $a_1 = 0.90$, $a_4 = 0.10$, $a_2 = 0.1$, $\\sigma_\\varepsilon = 0.03$, $T_{\\text{train}} = 200$, $T_{\\text{test}} = 400$.\n  4. Case B (boundary/noisy linear): same as Case L but $\\sigma_\\varepsilon = 0.50$, $T_{\\text{train}} = 150$, $T_{\\text{test}} = 300$.\n\nImplementation details:\n- Use a fixed random seed to ensure reproducibility.\n- Fit OLS on regressors $[1, K, z]$ only. Do not include interaction terms in the OLS benchmark.\n- Use the GP posterior mean with the specified fixed hyperparameters $\\sigma_f = 1$, $\\ell_1 = 2.5$, $\\ell_2 = 0.5$, and $\\sigma_n = 0.05$.\n\nYour program should produce a single line of output containing the four boolean results corresponding to Cases L, N, R, and B, in that order, as a comma-separated list enclosed in square brackets (e.g., \"[False,True,True,False]\"). No other output is permitted.", "solution": "The objective is to evaluate and compare the performance of a linear Ordinary Least Squares (OLS) forecasting rule against a non-parametric Gaussian Process (GP) forecasting rule in the context of a Krusell-Smith-style macroeconomic model. The comparison is based on out-of-sample predictive accuracy and the ability of the forecasters to detect specific dynamic features, namely non-linear curvature and regime-switching slopes, in the aggregate law of motion for capital.\n\nThe methodology involves a series of computational experiments. For each of four distinct data-generating processes (DGPs), we first simulate a time series for aggregate capital $K_t$ and an aggregate productivity shock $z_t$. This simulated data is then partitioned into a training set and a subsequent test set. Both the OLS and GP models are fitted to the training data. Their predictive performance is then evaluated on the test set. Finally, the fitted GP model is analyzed to diagnose its learned functional form for non-linearity and regime dependence. A composite rule determines whether the GP successfully \"discovers\" the underlying dynamics based on thresholds for predictive improvement and structural feature detection.\n\nLet the aggregate state be $s_t = (K_t, z_t)$, where $K_t \\in \\mathbb{R}_+$ is aggregate capital and $z_t \\in \\{0,1\\}$ is a productivity shock. The shock $z_t$ follows a two-state symmetric Markov chain with transition matrix\n$$ P = \\begin{pmatrix} p_{00} & p_{01} \\\\ p_{10} & p_{11} \\end{pmatrix} = \\begin{pmatrix} 0.9 & 0.1 \\\\ 0.1 & 0.9 \\end{pmatrix} $$\nThe data for aggregate capital is generated sequentially according to the law of motion\n$$ K_{t+1} = \\min \\{ \\max \\{ G(K_t, z_t) + \\varepsilon_t, K_{\\min} \\}, K_{\\max} \\} $$\nwhere $G(K_t, z_t)$ is the specific DGP, $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$ is a Gaussian innovation, and $K_{\\min}=2$ and $K_{\\max}=18$ are reflective boundaries. We simulate a time series of total length $T = T_{\\text{train}} + T_{\\text{test}}$, starting from initial conditions $K_0 = 8$ and $z_0 = 1$. The first $T_{\\text{train}}$ observations of $(K_t, z_t, K_{t+1})$ constitute the training set, and the subsequent $T_{\\text{test}}$ observations form the test set.\n\nWe specify and fit two forecasting models:\n\n1.  **Linear OLS Forecaster**: The forecasting rule is assumed to be linear in capital and the shock indicator,\n    $$ F_{\\text{lin}}(K, z) = \\beta_0 + \\beta_1 K + \\beta_2 z $$\n    The coefficient vector $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\beta_2]^\\top$ is estimated using OLS on the training data $\\{ (K_t, z_t), K_{t+1} \\}_{t=0}^{T_{\\text{train}}-1}$. This is achieved by solving the standard normal equations $\\boldsymbol{\\beta} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$, where $X$ is the design matrix with rows $[1, K_t, z_t]$ and $\\mathbf{y}$ is the vector of target values $K_{t+1}$.\n\n2.  **Gaussian Process (GP) Forecaster**: This non-parametric approach models the unknown function $G$ as a draw from a GP prior. The posterior predictive mean provides the forecast $F_{\\text{gp}}(K,z)$. With a zero-mean prior and a squared-exponential kernel, the prediction at a new input $x_\\star = [K_\\star, z_\\star]^\\top$ is given by\n    $$ F_{\\text{gp}}(x_\\star) = \\mathbf{k}(x_\\star, X)^\\top [ \\mathbf{K}(X,X) + \\sigma_n^2 I ]^{-1} \\mathbf{y} $$\n    where $X$ and $\\mathbf{y}$ are the training inputs and targets, respectively. The kernel function is\n    $$ k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}\\sum_{j=1}^{2} \\left(\\frac{x_j - x'_j}{\\ell_j}\\right)^2\\right) $$\n    with fixed hyperparameters: amplitude $\\sigma_f=1$, length-scale for capital $\\ell_1=2.5$, length-scale for the shock $\\ell_2=0.5$, and observation noise standard deviation $\\sigma_n=0.05$. $\\mathbf{K}(X,X)$ is the Gram matrix of kernel evaluations between all pairs of training inputs, and $\\mathbf{k}(x_\\star, X)$ is the vector of kernel evaluations between the test point $x_\\star$ and each training input.\n\nThe evaluation proceeds in two stages. First, we measure predictive accuracy on the test set by computing the mean squared error (MSE) for both models, denoted $\\text{MSE}_{\\text{lin}}$ and $\\text{MSE}_{\\text{gp}}$. The relative improvement of the GP over OLS is quantified by $\\Delta = (\\text{MSE}_{\\text{lin}} - \\text{MSE}_{\\text{gp}}) / \\text{MSE}_{\\text{lin}}$.\n\nSecond, we diagnose the structure of the learned GP function $F_{\\text{gp}}$ by evaluating it on a fine grid of capital levels $\\{K_i\\}_{i=1}^{N}$ from $K_{\\min}$ to $K_{\\max}$ with $N=50$ points, for each regime $z \\in \\{0,1\\}$. We use central finite differences to approximate the derivatives.\n-   The curvature is measured by $C = \\max_{z, i} |D_i^{(2)}(z)|$, where $D_i^{(2)}(z)$ is the second derivative of $F_{\\text{gp}}$ with respect to $K$ at grid point $K_i$ in regime $z$. A large value of $C$ indicates significant non-linearity.\n-   The regime-dependent slope gap is measured by $S = \\max_i |D_i^{(1)}(1) - D_i^{(1)}(0)|$, where $D_i^{(1)}(z)$ is the first derivative (slope) of $F_{\\text{gp}}$ with respect to $K$. A large value of $S$ indicates that the MEC (marginal effect of capital) depends on the aggregate shock $z$, a feature present in regime-switching models.\n\nThe final decision rule is that the GP \"discovers a non-linear or regime-switching aggregate dynamic\" if its predictive improvement and its detected structural features both exceed specified thresholds: $\\Delta \\ge \\tau_\\Delta=0.05$ AND ($C \\ge \\tau_C=0.002$ OR $S \\ge \\tau_S=0.05$). This entire procedure is executed for four different DGPs corresponding to linear, non-linear, regime-switching, and high-noise linear cases.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of simulations.\n    np.random.seed(42)\n\n    # Define the four test cases as per the problem statement.\n    test_cases = [\n        {\n            'name': 'L',\n            'dgp': lambda K, z: 0.5 + 0.92 * K + 0.1 * z,\n            'sigma_eps': 0.03, 'T_train': 200, 'T_test': 400\n        },\n        {\n            'name': 'N',\n            'dgp': lambda K, z: 0.5 + 0.95 * K - 0.002 * K**2 + 0.1 * z,\n            'sigma_eps': 0.03, 'T_train': 200, 'T_test': 400\n        },\n        {\n            'name': 'R',\n            'dgp': lambda K, z: 0.5 + (0.90 + 0.10 * z) * K + 0.1 * z,\n            'sigma_eps': 0.03, 'T_train': 200, 'T_test': 400\n        },\n        {\n            'name': 'B',\n            'dgp': lambda K, z: 0.5 + 0.92 * K + 0.1 * z,\n            'sigma_eps': 0.50, 'T_train': 150, 'T_test': 300\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        # 1. Simulate data according to the DGP and parameters for the case.\n        K, z = simulate_data(case['dgp'], case['T_train'], case['T_test'], case['sigma_eps'])\n        \n        # 2. Split the simulated data into training and test sets.\n        T_total = case['T_train'] + case['T_test']\n        X_train = np.vstack((K[:case['T_train']], z[:case['T_train']])).T\n        y_train = K[1:case['T_train'] + 1]\n        \n        X_test = np.vstack((K[case['T_train']:T_total], z[case['T_train']:T_total])).T\n        y_test = K[case['T_train'] + 1:T_total + 1]\n\n        # 3. Fit the OLS forecaster and predict on the test set.\n        y_pred_ols = fit_predict_ols(X_train, y_train, X_test)\n\n        # 4. Fit the GP forecaster and predict on the test set.\n        gp_params = {'sigma_f': 1.0, 'l1': 2.5, 'l2': 0.5, 'sigma_n': 0.05}\n        alpha, X_train_fit = fit_gp(X_train, y_train, gp_params)\n        y_pred_gp = predict_gp(X_test, X_train_fit, alpha, gp_params)\n\n        # 5. Calculate the evaluation and diagnostic metrics.\n        delta, C, S = calculate_metrics(y_test, y_pred_ols, y_pred_gp, X_train_fit, alpha, gp_params)\n\n        # 6. Apply the decision rule to determine the outcome for the case.\n        tau_delta, tau_C, tau_S = 0.05, 0.002, 0.05\n        decision = (delta >= tau_delta) and ((C >= tau_C) or (S >= tau_S))\n        results.append(decision)\n\n    # Print the final list of boolean results in the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_data(dgp_func, T_train, T_test, sigma_eps):\n    \"\"\"\n    Simulates time series for aggregate capital (K) and productivity shock (z).\n    \"\"\"\n    K_min, K_max = 2.0, 18.0\n    p00, p11 = 0.9, 0.9\n    K0, z0 = 8.0, 1.0\n    T_total = T_train + T_test\n    \n    K = np.zeros(T_total + 1)\n    z = np.zeros(T_total)\n    \n    K[0] = K0\n    current_z = z0\n    \n    for t in range(T_total):\n        z[t] = current_z\n        \n        # Evolve the Markov state for the shock z.\n        rand_val = np.random.rand()\n        if current_z == 0:\n            next_z = 0 if rand_val < p00 else 1\n        else:\n            next_z = 1 if rand_val < p11 else 0\n        \n        # Evolve the aggregate capital K.\n        Gt = dgp_func(K[t], z[t])\n        epsilon_t = np.random.normal(0, sigma_eps)\n        K[t+1] = np.clip(Gt + epsilon_t, K_min, K_max)\n        \n        current_z = next_z\n        \n    return K, z\n\ndef fit_predict_ols(X_train, y_train, X_test):\n    \"\"\"\n    Fits a linear OLS model and makes predictions.\n    \"\"\"\n    # Add a constant term (intercept) to the design matrices.\n    X_train_ols = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n    X_test_ols = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n    \n    # Fit OLS coefficients using np.linalg.lstsq for numerical stability.\n    beta, _, _, _ = np.linalg.lstsq(X_train_ols, y_train, rcond=None)\n    \n    # Predict on the test set.\n    y_pred = X_test_ols @ beta\n    return y_pred\n\ndef squared_exponential_kernel(X1, X2, sigma_f, l1, l2):\n    \"\"\"\n    Computes the squared-exponential kernel matrix between two sets of points.\n    \"\"\"\n    length_scales = np.array([l1, l2])\n    X1_scaled = X1 / length_scales\n    X2_scaled = X2 / length_scales\n    \n    # Use scipy's cdist for efficient computation of pairwise squared Euclidean distances.\n    sq_dist = cdist(X1_scaled, X2_scaled, 'sqeuclidean')\n    \n    return sigma_f**2 * np.exp(-0.5 * sq_dist)\n\ndef fit_gp(X_train, y_train, gp_params):\n    \"\"\"\n    Fits a Gaussian Process model.\n    This involves computing the alpha vector for making future predictions.\n    \"\"\"\n    sigma_f = gp_params['sigma_f']\n    l1 = gp_params['l1']\n    l2 = gp_params['l2']\n    sigma_n = gp_params['sigma_n']\n    \n    K_train = squared_exponential_kernel(X_train, X_train, sigma_f, l1, l2)\n    K_noisy = K_train + sigma_n**2 * np.identity(X_train.shape[0])\n    \n    # Solve (K + sigma_n^2 * I) * alpha = y to find alpha.\n    # np.linalg.solve is preferred over inverting the matrix.\n    alpha = np.linalg.solve(K_noisy, y_train)\n    \n    return alpha, X_train\n\ndef predict_gp(X_test, X_train, alpha, gp_params):\n    \"\"\"\n    Makes predictions using a fitted GP model.\n    \"\"\"\n    sigma_f = gp_params['sigma_f']\n    l1 = gp_params['l1']\n    l2 = gp_params['l2']\n    \n    K_test_train = squared_exponential_kernel(X_test, X_train, sigma_f, l1, l2)\n    y_pred = K_test_train @ alpha\n    return y_pred\n\ndef calculate_metrics(y_test, y_pred_ols, y_pred_gp, X_train, alpha, gp_params):\n    \"\"\"\n    Calculates the relative MSE improvement (Delta), curvature (C), and slope gap (S).\n    \"\"\"\n    # Calculate MSEs and the relative improvement Delta.\n    mse_lin = np.mean((y_test - y_pred_ols)**2)\n    mse_gp = np.mean((y_test - y_pred_gp)**2)\n    delta = (mse_lin - mse_gp) / mse_lin if mse_lin > 0 else 0.0\n\n    # For diagnostics, create a grid of K values.\n    K_min, K_max = 2.0, 18.0\n    N = 50\n    h = (K_max - K_min) / (N - 1)\n    K_grid = np.linspace(K_min, K_max, N)\n\n    # Predict with the GP on the grid for both regimes z=0 and z=1.\n    X_grid_z0 = np.vstack((K_grid, np.zeros(N))).T\n    F_gp_z0 = predict_gp(X_grid_z0, X_train, alpha, gp_params)\n    \n    X_grid_z1 = np.vstack((K_grid, np.ones(N))).T\n    F_gp_z1 = predict_gp(X_grid_z1, X_train, alpha, gp_params)\n\n    # Compute curvature C using a second-order central finite difference.\n    D2_z0 = (F_gp_z0[2:] - 2*F_gp_z0[1:-1] + F_gp_z0[:-2]) / h**2\n    D2_z1 = (F_gp_z1[2:] - 2*F_gp_z1[1:-1] + F_gp_z1[:-2]) / h**2\n    C = max(np.max(np.abs(D2_z0)), np.max(np.abs(D2_z1)))\n    \n    # Compute slope gap S using a first-order central finite difference.\n    D1_z0 = (F_gp_z0[2:] - F_gp_z0[:-2]) / (2*h)\n    D1_z1 = (F_gp_z1[2:] - F_gp_z1[:-2]) / (2*h)\n    S = np.max(np.abs(D1_z1 - D1_z0))\n    \n    return delta, C, S\n\nif __name__ == '__main__':\n    solve()\n```"}]}