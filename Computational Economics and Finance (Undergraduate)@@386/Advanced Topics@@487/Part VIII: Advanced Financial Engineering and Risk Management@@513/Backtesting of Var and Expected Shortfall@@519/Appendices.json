{"hands_on_practices": [{"introduction": "A fundamental check for any Value at Risk ($VaR$) model is to see if the frequency of its violations matches the expected frequency, $\\alpha$. This exercise [@problem_id:2374206] serves as a powerful cautionary tale by guiding you to construct a scenario where a $VaR$ model perfectly passes this unconditional coverage test. However, you will also demonstrate that on the days a violation does occur, the losses are catastrophically larger than the $VaR$ prediction, a critical flaw that this simple test completely misses. This practice reveals the inherent limitations of backtests that only count violations and underscores the importance of measures like Expected Shortfall ($ES$), which consider the severity of tail losses.", "id": "2374206", "problem": "You are asked to demonstrate the limitations of backtesting using Value at Risk (VaR) by constructing synthetic profit-and-loss (P&amp;L) data and a deliberately misspecified VaR model that satisfies an unconditional coverage test while hiding extreme tail severity that would be evident to Expected Shortfall (ES). You must implement a complete program that produces deterministic outputs from first principles.\n\nBase definitions to use:\n- Losses are represented by a sequence $\\{L_t\\}_{t=1}^T$ where $L_t \\ge 0$ denotes the loss at time $t$.\n- Value at Risk (VaR) at tail probability level $\\alpha \\in (0,1)$ is the smallest threshold $v_\\alpha$ such that $\\mathbb{P}(L_t \\le v_\\alpha) \\ge 1 - \\alpha$. When a model supplies a sequence $\\{v_t\\}_{t=1}^T$ of one-step-ahead VaR forecasts, a violation at time $t$ is the event $L_t > v_t$.\n- The exceedance indicator is $I_t = \\mathbf{1}\\{L_t > v_t\\}$. Under correct unconditional coverage, the sequence $\\{I_t\\}$ is an independent and identically distributed Bernoulli process with success probability $\\alpha$.\n- Expected Shortfall (ES) at level $\\alpha$ is the conditional expectation of the loss given that the loss exceeds the VaR threshold: $\\mathrm{ES}_\\alpha = \\mathbb{E}[L_t \\mid L_t > v_\\alpha]$.\n- The Kupiec Unconditional Coverage Likelihood Ratio (LR) test compares the Bernoulli likelihood under the null violation probability $\\alpha$ versus the likelihood under the maximum-likelihood estimate $\\hat{p}$. The null hypothesis is that the unconditional violation rate equals $\\alpha$. The LR statistic is asymptotically $\\chi^2$ with $1$ degree of freedom, and from it you must compute a $p$-value and determine whether the test fails to reject the null at significance level $0.05$.\n\nYour tasks:\n1. For each provided test case, construct a synthetic sequence of losses $\\{L_t\\}_{t=1}^T$ and a constant VaR forecast sequence $\\{v_t\\}_{t=1}^T$ with $v_t \\equiv v$, such that:\n   - Exactly $x = \\alpha T$ time points are violations, with $L_t = 10 v$ on these violation times.\n   - All other times are non-violations, with $L_t = 0.5 v$.\n   - Note: All mathematical entities, including $10$, $0.5$, and $x$, must be respected as constraints in your construction.\n2. From the constructed $\\{L_t\\}$ and $\\{v_t\\}$, compute the exceedance indicators $\\{I_t\\}$ and the number of violations $x = \\sum_{t=1}^T I_t$ for each case.\n3. Implement the Kupiec Unconditional Coverage Likelihood Ratio test for each case based only on the Bernoulli likelihood principle and the asymptotic $\\chi^2$ distribution with $1$ degree of freedom. Use this test to compute the $p$-value and a boolean indicating whether the test fails to reject at significance level $0.05$.\n4. Compute the realized Expected Shortfall ratio for each case, defined as $M = \\left(\\frac{1}{x} \\sum_{t=1}^T I_t L_t\\right) / v$ when $x \\ge 1$. In your construction, this ratio should reveal systematic tail severity relative to the model VaR level.\n5. Your implementation must be deterministic and must not rely on any external randomness.\n\nTest suite:\nYou must run your program on the following parameter sets $(T,\\alpha,v)$, where $T$ is the length, $\\alpha$ is the violation probability as a decimal, and $v$ is the constant VaR level:\n- Case A: $(T,\\alpha,v) = (1000, 0.01, 1.0)$\n- Case B: $(T,\\alpha,v) = (250, 0.04, 2.0)$\n- Case C: $(T,\\alpha,v) = (200, 0.055, 0.5)$\n- Case D (small-sample coverage check): $(T,\\alpha,v) = (20, 0.1, 3.0)$\n\nFor each case, output a list of three items in this exact order:\n- A boolean indicating whether the Kupiec test fails to reject at significance level $0.05$.\n- The Kupiec test $p$-value as a floating-point number.\n- The realized Expected Shortfall ratio $M$ as a floating-point number.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of per-case results, where each per-case result is a three-element list in the order specified above. For example, the final single line should look like:\n- $[\\,[\\text{True}, 1.0, 10.0], [\\text{True}, 0.9999, 10.0]\\,]$\n- Your code must print exactly one line with this list, with no additional text.\n\nImportant implementation notes:\n- Treat violations as $L_t > v_t$ with losses and VaR both nonnegative.\n- Ensure that $x = \\alpha T$ is an integer in each test case as provided. Do not randomize the placement of violations; a simple deterministic placement is acceptable.\n- Use significance level $0.05$ for the Kupiec decision.\n- Angles and physical units are not applicable. Percentages must be expressed as decimals, which is already satisfied by the given $\\alpha$ values.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of financial risk management, specifically the backtesting of risk models. All definitions and constraints are well-posed, complete, and consistent, allowing for a unique and meaningful solution. The task is to construct a specific scenario to illustrate a known limitation of Value at Risk ($VaR$) when compared to Expected Shortfall ($ES$).\n\nThe objective is to create a synthetic profit-and-loss ($P\\&L$) history, represented by a sequence of losses $\\{L_t\\}_{t=1}^T$, and a corresponding sequence of one-step-ahead $VaR$ forecasts, $\\{v_t\\}_{t=1}^T$. This construction must be deliberately misspecified such that the $VaR$ model passes a standard backtest—the Kupiec unconditional coverage test—while simultaneously concealing a severe underestimation of tail loss magnitude, a flaw that an $ES$-based metric would expose.\n\nFirst, we address the construction of the synthetic data for each given test case, which is a triplet of parameters $(T, \\alpha, v)$. $T$ is the total number of time periods, $\\alpha$ is the target violation probability, and $v$ is the constant $VaR$ forecast level.\n\nThe problem dictates a constant $VaR$ forecast:\n$$ v_t = v \\quad \\forall t \\in \\{1, \\dots, T\\} $$\n\nA violation, or exceedance, at time $t$ is defined by the event $L_t > v_t$. The construction demands that the total number of violations, $x$, must be exactly $x = \\alpha T$. For the test cases provided, this product is an integer. To ensure determinism, we can place these $x$ violations at the beginning of the time series, from $t=1$ to $t=x$.\n\nThe loss values are specified as follows:\n- For violation periods ($t \\in \\{1, \\dots, x\\}$): $L_t = 10v$. This satisfies the violation condition $L_t > v$ as $v$ must be positive.\n- For non-violation periods ($t \\in \\{x+1, \\dots, T\\}$): $L_t = 0.5v$. This does not constitute a violation, as $0.5v \\ngtr v$.\n\nThe exceedance indicator sequence $\\{I_t\\}_{t=1}^T$ is thus $I_t = 1$ for $t \\in \\{1, \\dots, x\\}$ and $I_t = 0$ for $t \\in \\{x+1, \\dots, T\\}$.\n\nNext, we perform the Kupiec Unconditional Coverage Likelihood Ratio ($LR$) test. The null hypothesis, $H_0$, is that the true probability of a violation is $p = \\alpha$. The test statistic is based on comparing the likelihood of the observed data under $H_0$ with the likelihood under the maximum-likelihood estimate, $\\hat{p} = x/T$. The $LR$ statistic is given by:\n$$ LR_{uc} = 2 \\ln \\left( \\frac{L(\\hat{p})}{L(\\alpha)} \\right) = 2 \\ln \\left( \\frac{\\hat{p}^x (1 - \\hat{p})^{T-x}}{\\alpha^x (1-\\alpha)^{T-x}} \\right) $$\nThis expression can be rewritten as:\n$$ LR_{uc} = 2 \\left[ x \\ln\\left(\\frac{\\hat{p}}{\\alpha}\\right) + (T-x) \\ln\\left(\\frac{1-\\hat{p}}{1-\\alpha}\\right) \\right] $$\nBy our specific construction, the number of violations is $x = \\alpha T$. Therefore, the empirical violation rate is $\\hat{p} = x/T = (\\alpha T)/T = \\alpha$. Substituting $\\hat{p} = \\alpha$ into the $LR_{uc}$ formula yields:\n$$ LR_{uc} = 2 \\left[ (\\alpha T) \\ln\\left(\\frac{\\alpha}{\\alpha}\\right) + (T-\\alpha T) \\ln\\left(\\frac{1-\\alpha}{1-\\alpha}\\right) \\right] = 2 \\left[ (\\alpha T) \\ln(1) + T(1-\\alpha) \\ln(1) \\right] = 0 $$\nThe $LR_{uc}$ statistic is, therefore, identically zero. This statistic is asymptotically distributed as a chi-squared distribution with $1$ degree of freedom, $\\chi^2(1)$. The $p$-value of the test is the probability of observing a test statistic at least as extreme as the one computed, i.e., $P(\\chi^2(1) \\ge LR_{uc})$. With $LR_{uc}=0$, the $p$-value is:\n$$ p\\text{-value} = \\int_0^\\infty f(z; 1) dz = 1 $$\nwhere $f(z; 1)$ is the probability density function of the $\\chi^2(1)$ distribution. The null hypothesis is rejected if the $p$-value is below the significance level, which is given as $0.05$. Since $1.0 > 0.05$, we unconditionally fail to reject the null hypothesis for all test cases. The $VaR$ model passes the coverage test perfectly.\n\nFinally, we analyze the model's performance concerning the magnitude of tail losses using the realized Expected Shortfall ratio, $M$. This is defined as the ratio of the empirical average loss on violation days to the $VaR$ level:\n$$ M = \\frac{\\frac{1}{x} \\sum_{t=1}^T I_t L_t}{v} $$\nThe sum $\\sum_{t=1}^T I_t L_t$ represents the total loss across all $x$ violation days. According to our construction, the loss on each of these days is $L_t = 10v$. So, the sum is $x \\cdot 10v$. The average loss on violation days is:\n$$ \\frac{1}{x} \\sum_{t=1}^T I_t L_t = \\frac{x \\cdot 10v}{x} = 10v $$\nThis is the empirical, or realized, Expected Shortfall. The ratio $M$ is then:\n$$ M = \\frac{10v}{v} = 10 $$\nThis result, which is constant across all test cases, reveals the model's critical flaw. When a violation occurs, the resulting loss is, on average, $10$ times greater than the $VaR$ threshold. A simple coverage test is blind to this systematic and severe underestimation of risk in the tail of the loss distribution. This demonstrates the superiority of a coherent risk measure like $ES$, which considers not only the frequency but also the severity of tail losses.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the problem of demonstrating VaR backtesting limitations.\n    For each test case, it computes the Kupiec test result and the realized ES ratio.\n    \"\"\"\n    test_cases = [\n        # (T, alpha, v)\n        (1000, 0.01, 1.0),\n        (250, 0.04, 2.0),\n        (200, 0.055, 0.5),\n        (20, 0.1, 3.0),\n    ]\n\n    results = []\n    significance_level = 0.05\n\n    for T, alpha, v in test_cases:\n        # Per problem statement, x = alpha * T is an integer for all test cases.\n        # This setup is deterministic.\n        x = int(T * alpha)\n        \n        # 1. Kupiec Unconditional Coverage Test\n        # The null hypothesis is that the violation probability is alpha.\n        # The test statistic LR_uc = 2 * ln(L(p_hat) / L(alpha)).\n        # By construction, the observed violation rate p_hat = x / T = alpha.\n        # This forces the LR statistic to be 0 and the p-value to be 1.\n        # We implement the full formula for formal verification.\n\n        lr_stat = 0.0\n        if x == 0:\n            # log(p_hat) term is undefined, but limit of p_hat*log(p_hat) is 0.\n            # LR = 2* T * log(1/(1-alpha))\n            lr_stat = 2 * (T * np.log(1 / (1 - alpha)))\n        elif x == T:\n            # log(1-p_hat) term is undefined.\n            # LR = 2 * T * log(1/alpha)\n            lr_stat = 2 * (T * np.log(1 / alpha))\n        else:\n            p_hat = x / T\n            # Due to problem design, p_hat is exactly alpha.\n            # np.log(p_hat / alpha) will be np.log(1.0) = 0.\n            # The formula is robust to minor floating point inaccuracies if any existed.\n            term1 = x * np.log(p_hat / alpha)\n            term2 = (T - x) * np.log((1 - p_hat) / (1 - alpha))\n            lr_stat = 2 * (term1 + term2)\n\n        # p-value from chi-squared distribution with 1 degree of freedom\n        p_value = float(chi2.sf(lr_stat, df=1))\n\n        # Decision: Fail to reject H0 if p-value is not less than significance level\n        k_test_fails_to_reject = p_value >= significance_level\n\n        # 2. Realized Expected Shortfall Ratio (M)\n        # M = (average loss on violation days) / VaR\n        # Loss on violation days is L_t = 10 * v\n        # Average loss is therefore (x * 10 * v) / x = 10 * v\n        # M = (10 * v) / v = 10\n        es_ratio = 0.0\n        if x > 0:\n            # The calculation is analytically 10, as shown in the solution notes.\n            # Average loss conditional on violation = 10 * v\n            # Ratio M = (10 * v) / v\n            es_ratio = 10.0\n        # The problem statement defines M for x >= 1, so no else case is needed\n        # for the given test suite.\n\n        results.append([k_test_fails_to_reject, p_value, es_ratio])\n\n    # Final print statement must match the specified format exactly.\n    # The format from the template is f\"[{','.join(map(str, results))}]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Having established that the frequency of violations is not the whole story, we now turn to their timing. A reliable risk model should produce violations that are unpredictable and scattered randomly through time. This practice [@problem_id:2374172] asks you to explore what happens when this assumption fails by constructing a scenario where $VaR$ violations, while correct in their total number, are systematically clustered on a specific day of the week. By applying Christoffersen's full backtesting framework, you will learn to distinguish between unconditional coverage and the much stricter condition of conditional coverage, which requires violations to be independent over time.", "id": "2374172", "problem": "You are tasked with constructing and evaluating a Value at Risk (VaR) exception process that has correct unconditional coverage at a specified probability level but exhibits systematic timing. Consider a trading calendar consisting of a repeating five-day week with days labeled by integers where Monday is day $0$, Tuesday is day $1$, Wednesday is day $2$, Thursday is day $3$, and Friday is day $4$. Let $T$ denote the total number of trading days, indexed by $t \\in \\{0,1,\\ldots,T-1\\}$, and let $\\alpha \\in (0,1)$ denote the nominal VaR exception probability level. Define the VaR exception indicator sequence $\\{I_t\\}_{t=0}^{T-1}$ by $I_t \\in \\{0,1\\}$, where $I_t=1$ indicates that day $t$ is a VaR exception and $I_t=0$ otherwise. The model to be evaluated must satisfy the following two properties simultaneously:\n- The total number of exceptions over $T$ days equals $\\alpha T$ exactly.\n- All exceptions occur on Mondays only.\n\nAssume a known starting day-of-week for the first observation, denoted by $s \\in \\{0,1,2,3,4\\}$, where $s=0$ corresponds to Monday. The day-of-week of time $t$ is $(s+t) \\bmod 5$. The model should place exactly $\\alpha T$ exceptions at the earliest Mondays in the sample, with $I_t=1$ for those Monday indices and $I_t=0$ otherwise. You may assume that $\\alpha T$ is an integer and that the number of Mondays in the sample is at least $\\alpha T$.\n\nLet $\\{I_t\\}$ be subjected to Christoffersen’s framework for backtesting, which consists of:\n- The unconditional coverage hypothesis that the exception probability equals $\\alpha$.\n- The independence hypothesis that exceptions are independent over time.\n- The conditional coverage hypothesis that both unconditional coverage and independence jointly hold.\n\nUse likelihood ratio statistics defined in the Christoffersen framework, and report for each test:\n- The likelihood ratio statistic for unconditional coverage, denoted $LR_{\\mathrm{uc}}$.\n- The likelihood ratio statistic for independence, denoted $LR_{\\mathrm{ind}}$.\n- The conditional coverage statistic $LR_{\\mathrm{cc}} = LR_{\\mathrm{uc}} + LR_{\\mathrm{ind}}$.\n- The corresponding $p$-values computed under the chi-square distribution with degrees of freedom equal to $1$ for $LR_{\\mathrm{uc}}$, $1$ for $LR_{\\mathrm{ind}}$, and $2$ for $LR_{\\mathrm{cc}}$.\n- A binary rejection decision for each null hypothesis at significance level $\\delta=0.05$ expressed as a decimal.\n\nYour program must implement this construction and analysis for the following test suite of parameter triples $(T,\\alpha,s)$:\n- Test $1$: $(T,\\alpha,s) = (500, 0.05, 0)$.\n- Test $2$: $(T,\\alpha,s) = (495, 0.20, 3)$.\n- Test $3$: $(T,\\alpha,s) = (100, 0.10, 1)$.\n- Test $4$: $(T,\\alpha,s) = (500, 0.01, 4)$.\n- Test $5$: $(T,\\alpha,s) = (250, 0.04, 2)$.\n\nFor each test, output a list of nine values in the following order:\n$[LR_{\\mathrm{uc}}, p_{\\mathrm{uc}}, LR_{\\mathrm{ind}}, p_{\\mathrm{ind}}, LR_{\\mathrm{cc}}, p_{\\mathrm{cc}}, \\text{reject}_{\\mathrm{uc}}, \\text{reject}_{\\mathrm{ind}}, \\text{reject}_{\\mathrm{cc}}]$, where $p_{\\mathrm{uc}}$, $p_{\\mathrm{ind}}$, and $p_{\\mathrm{cc}}$ are $p$-values and $\\text{reject}_{\\mathrm{uc}}$, $\\text{reject}_{\\mathrm{ind}}$, $\\text{reject}_{\\mathrm{cc}}$ are Boolean values indicating rejection at level $\\delta=0.05$.\n\nFinal output format: Your program should produce a single line of output containing the results for all five tests as a comma-separated list of such nine-element lists, enclosed in square brackets, for example $[\\ldots]$. All probabilities must be expressed as decimals, not with a percentage sign.", "solution": "We formalize the VaR exception indicator sequence and the Christoffersen tests from first principles. Let $T \\in \\mathbb{N}$ be the number of trading days and $\\alpha \\in (0,1)$ the nominal exception probability. Define the indicator $I_t \\in \\{0,1\\}$ for $t \\in \\{0,1,\\ldots,T-1\\}$ with the following construction rules:\n- Compute the set of indices corresponding to Mondays as $\\mathcal{M} = \\{t \\in \\{0,\\ldots,T-1\\} : (s+t) \\bmod 5 = 0\\}$, where $s \\in \\{0,1,2,3,4\\}$ is the day-of-week of the first observation with $0$ representing Monday.\n- Let $K = \\alpha T$. By assumption, $K$ is an integer and $|\\mathcal{M}| \\ge K$.\n- Set $I_t = 1$ for the first $K$ elements of $\\mathcal{M}$ in ascending order of $t$, and set $I_t = 0$ for all other indices. This ensures that all exceptions occur on Mondays and that the total number of exceptions equals $\\alpha T$.\n\nChristoffersen’s unconditional coverage test considers the hypothesis\n$H_0^{\\mathrm{uc}}: \\mathbb{P}(I_t=1) = \\alpha$,\nwhich states that the exception probability equals $\\alpha$. Let $N_1 = \\sum_{t=0}^{T-1} I_t$ and $N_0 = T - N_1$. The likelihood under $H_0^{\\mathrm{uc}}$ for independent Bernoulli trials with success probability $\\alpha$ is\n$$\nL_0^{\\mathrm{uc}} = (1-\\alpha)^{N_0} \\alpha^{N_1}.\n$$\nThe unrestricted likelihood (maximized over $\\pi \\in [0,1]$) is achieved at $\\hat{\\pi} = N_1/T$, giving\n$$\nL_1^{\\mathrm{uc}} = (1-\\hat{\\pi})^{N_0} \\hat{\\pi}^{N_1}.\n$$\nThe likelihood ratio statistic is\n$$\nLR_{\\mathrm{uc}} = -2 \\left[ \\ln L_0^{\\mathrm{uc}} - \\ln L_1^{\\mathrm{uc}} \\right]\n= 2 \\left[ N_1 \\ln\\left(\\frac{N_1}{\\alpha T}\\right) + N_0 \\ln\\left(\\frac{N_0}{(1-\\alpha)T}\\right) \\right],\n$$\nwith the convention that terms of the form $0 \\cdot \\ln(0)$ are treated as $0$. Under $H_0^{\\mathrm{uc}}$, $LR_{\\mathrm{uc}}$ is asymptotically chi-square distributed with $1$ degree of freedom.\n\nThe independence test considers the first-order Markov dependence in the sequence $\\{I_t\\}$. Define the transition counts over $t=1,\\ldots,T-1$:\n- $N_{ij} = \\#\\{t \\in \\{1,\\ldots,T-1\\} : I_{t-1}=i, I_t=j\\}$ for $i,j \\in \\{0,1\\}$.\nLet $N_{0\\cdot} = N_{00} + N_{01}$ and $N_{1\\cdot} = N_{10} + N_{11}$. Under the unrestricted first-order Markov alternative, the maximum likelihood estimators are\n$$\n\\hat{p}_{01} = \\frac{N_{01}}{N_{0\\cdot}} \\ \\text{ if } N_{0\\cdot} > 0, \\quad\n\\hat{p}_{11} = \\frac{N_{11}}{N_{1\\cdot}} \\ \\text{ if } N_{1\\cdot} > 0,\n$$\nwith natural boundary handling if a denominator is zero. The unrestricted log-likelihood is\n$$\n\\ln L_1^{\\mathrm{ind}} = N_{00}\\ln(1-\\hat{p}_{01}) + N_{01}\\ln(\\hat{p}_{01}) + N_{10}\\ln(1-\\hat{p}_{11}) + N_{11}\\ln(\\hat{p}_{11}),\n$$\nwhere any term with zero coefficient is treated as zero. Under the null of independence with a common success probability $\\pi$, the maximum likelihood estimator based on transitions is\n$$\n\\hat{\\pi} = \\frac{N_{01} + N_{11}}{N_{00} + N_{01} + N_{10} + N_{11}} = \\frac{N_{01} + N_{11}}{T-1},\n$$\nand the restricted log-likelihood is\n$$\n\\ln L_0^{\\mathrm{ind}} = (N_{00}+N_{10})\\ln(1-\\hat{\\pi}) + (N_{01}+N_{11})\\ln(\\hat{\\pi}).\n$$\nThe independence likelihood ratio statistic is\n$$\nLR_{\\mathrm{ind}} = -2\\left[\\ln L_0^{\\mathrm{ind}} - \\ln L_1^{\\mathrm{ind}}\\right],\n$$\nwhich is asymptotically chi-square distributed with $1$ degree of freedom under the null of independence.\n\nThe conditional coverage statistic combines both components:\n$$\nLR_{\\mathrm{cc}} = LR_{\\mathrm{uc}} + LR_{\\mathrm{ind}},\n$$\nwhich is asymptotically chi-square distributed with $2$ degrees of freedom under the joint null.\n\nFor each test case $(T,\\alpha,s)$, we construct $\\{I_t\\}$ as specified, compute $N_1$, $N_0$, and the transition counts $N_{ij}$, then evaluate $LR_{\\mathrm{uc}}$, $LR_{\\mathrm{ind}}$, and $LR_{\\mathrm{cc}}$ along with their $p$-values using the survival function of the chi-square distribution with appropriate degrees of freedom. Rejection decisions are made by comparing each $p$-value to $\\delta = 0.05$. Because the construction enforces $N_1 = \\alpha T$ exactly, we have $\\hat{\\pi} = \\alpha$ and thus $LR_{\\mathrm{uc}} = 0$ identically for all test cases, implying no rejection of unconditional coverage at any conventional level. However, concentrating all exceptions on Mondays induces serial dependence relative to calendar time that is detectable by the independence test through differing transition probabilities, in particular $\\hat{p}_{11} = 0$ (no consecutive exceptions) and $\\hat{p}_{01} > 0$, which increases $LR_{\\mathrm{ind}}$ and typically leads to rejection of independence and conditional coverage, especially as $T$ and $\\alpha T$ increase.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef build_monday_exception_series(T: int, alpha: float, start_day: int) -> np.ndarray:\n    \"\"\"\n    Construct an exception indicator series I_t of length T such that:\n    - Exactly K = alpha*T exceptions occur (assumed integer),\n    - All exceptions occur on Mondays (weekday index 0),\n    - The first observation has weekday 'start_day' in {0,..,4}.\n    \"\"\"\n    K = int(round(alpha * T))\n    I = np.zeros(T, dtype=int)\n    monday_indices = [t for t in range(T) if ((start_day + t) % 5) == 0]\n    if K > len(monday_indices):\n        raise ValueError(\"Not enough Mondays to place all exceptions.\")\n    # Place exceptions on the earliest Mondays\n    for t in monday_indices[:K]:\n        I[t] = 1\n    return I\n\ndef safe_log(x: float) -> float:\n    \"\"\"Natural log with handling for x in {0,1} when multiplied by zero counts later.\"\"\"\n    # We will never call log(0) multiplied by positive coefficient if model is set correctly\n    # But to be safe, cap x in (0,1) open interval with tiny epsilon\n    eps = 1e-16\n    if x <= 0.0:\n        x = eps\n    elif x >= 1.0:\n        x = 1.0 - eps\n    return np.log(x)\n\ndef lr_uc(I: np.ndarray, alpha: float) -> float:\n    \"\"\"\n    Christoffersen's unconditional coverage LR statistic.\n    LR_uc = 2 * [ N1*ln(N1/(alpha*T)) + N0*ln(N0/((1-alpha)*T)) ] with 0*ln(0)=0.\n    \"\"\"\n    T = I.size\n    N1 = int(I.sum())\n    N0 = T - N1\n    # Handle 0*ln(0) -> 0 via conditional terms\n    term1 = 0.0 if N1 == 0 else N1 * np.log(N1 / (alpha * T))\n    term0 = 0.0 if N0 == 0 else N0 * np.log(N0 / ((1.0 - alpha) * T))\n    LR = 2.0 * (term1 + term0)\n    # Numerical cleanup: LR cannot be negative\n    return float(max(LR, 0.0))\n\ndef lr_ind(I: np.ndarray) -> float:\n    \"\"\"\n    Christoffersen's independence LR statistic based on 2x2 transition counts.\n    \"\"\"\n    # Compute transitions\n    I_prev = I[:-1]\n    I_curr = I[1:]\n    N00 = int(np.sum((I_prev == 0) & (I_curr == 0)))\n    N01 = int(np.sum((I_prev == 0) & (I_curr == 1)))\n    N10 = int(np.sum((I_prev == 1) & (I_curr == 0)))\n    N11 = int(np.sum((I_prev == 1) & (I_curr == 1)))\n\n    N0dot = N00 + N01\n    N1dot = N10 + N11\n    Ntrans = N0dot + N1dot  # equals len(I)-1\n\n    # Unrestricted MLEs with boundary handling\n    p01_hat = 0.0 if N0dot == 0 else N01 / N0dot\n    p11_hat = 0.0 if N1dot == 0 else N11 / N1dot\n\n    # Log-likelihood under alternative (Markov)\n    ll1 = 0.0\n    if N00 > 0:\n        ll1 += N00 * safe_log(1.0 - p01_hat)\n    if N01 > 0:\n        ll1 += N01 * safe_log(p01_hat)\n    if N10 > 0:\n        ll1 += N10 * safe_log(1.0 - p11_hat)\n    if N11 > 0:\n        ll1 += N11 * safe_log(p11_hat)\n\n    # Restricted MLE under independence\n    pi_hat = 0.0 if Ntrans == 0 else (N01 + N11) / Ntrans\n    ll0 = 0.0\n    if (N00 + N10) > 0:\n        ll0 += (N00 + N10) * safe_log(1.0 - pi_hat)\n    if (N01 + N11) > 0:\n        ll0 += (N01 + N11) * safe_log(pi_hat)\n\n    LR = -2.0 * (ll0 - ll1)\n    return float(max(LR, 0.0))\n\ndef analyze_case(T: int, alpha: float, start_day: int, sig: float = 0.05):\n    I = build_monday_exception_series(T, alpha, start_day)\n    LR_uc = lr_uc(I, alpha)\n    LR_ind = lr_ind(I)\n    LR_cc = LR_uc + LR_ind\n\n    # p-values from chi-square distributions\n    p_uc = chi2.sf(LR_uc, df=1)\n    p_ind = chi2.sf(LR_ind, df=1)\n    p_cc = chi2.sf(LR_cc, df=2)\n\n    reject_uc = p_uc < sig\n    reject_ind = p_ind < sig\n    reject_cc = p_cc < sig\n\n    return LR_uc, p_uc, LR_ind, p_ind, LR_cc, p_cc, reject_uc, reject_ind, reject_cc\n\ndef format_result(result_tuple):\n    # Format floats to 6 decimals, booleans as True/False\n    formatted = []\n    for i, v in enumerate(result_tuple):\n        if isinstance(v, float):\n            formatted.append(f\"{v:.6f}\")\n        elif isinstance(v, (np.floating,)):\n            formatted.append(f\"{float(v):.6f}\")\n        elif isinstance(v, (bool, np.bool_)):\n            formatted.append(\"True\" if bool(v) else \"False\")\n        else:\n            formatted.append(str(v))\n    return \"[\" + \",\".join(formatted) + \"]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is (T, alpha, start_day)\n    test_cases = [\n        (500, 0.05, 0),  # Test 1\n        (495, 0.20, 3),  # Test 2\n        (100, 0.10, 1),  # Test 3\n        (500, 0.01, 4),  # Test 4\n        (250, 0.04, 2),  # Test 5\n    ]\n\n    results = []\n    for case in test_cases:\n        T, alpha, start_day = case\n        result = analyze_case(T, alpha, start_day, sig=0.05)\n        results.append(format_result(result))\n\n    # Final print statement in the exact required format: a single line with a list of lists\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}, {"introduction": "The most insidious modeling errors are often the hardest to detect. One such error is \"look-ahead bias,\" where a model inadvertently uses future information to make its \"forecast.\" This exercise [@problem_id:2374186] provides a hands-on opportunity to investigate and quantify the impact of this critical flaw. You will implement two risk models: a properly specified baseline model and a \"ghost\" model that peeks into the future, and then compare their performance on multiple backtesting metrics. This practice will give you a concrete understanding of how look-ahead bias can spuriously inflate a model's performance and why rigorous chronological discipline is essential in building and validating any predictive model.", "id": "2374186", "problem": "You will implement and compare two rolling Gaussian risk models for Value-at-Risk (VaR) and expected shortfall (ES) and quantify how the inclusion of \"ghost features\" (information from the future) affects standard backtesting outcomes. All computations are to be performed on synthetic return series generated from a specified conditional heteroskedastic process. Your program must be self-contained, produce deterministic results, and output a single list of floats as specified below.\n\nDefinitions and assumptions:\n- Let $\\alpha \\in (0,1)$ denote the tail probability level for VaR and expected shortfall.\n- The one-step-ahead return at time $t$ is $r_t$. Negative values represent losses.\n- Under the Gaussian model with zero conditional mean and conditional standard deviation $\\sigma_t$, the $\\alpha$-quantile (left tail) is $q_\\alpha = \\Phi^{-1}(\\alpha)$, where $\\Phi^{-1}(\\cdot)$ is the inverse cumulative distribution function of the standard normal distribution. The conditional VaR forecast is $\\widehat{\\text{VaR}}_t = q_\\alpha \\sigma_t$.\n- The expected shortfall is the conditional expectation in the tail. For a standard normal random variable $Z$, the truncated mean satisfies $\\mathbb{E}[Z \\mid Z \\le q_\\alpha] = -\\varphi(q_\\alpha)/\\alpha$, where $\\varphi(\\cdot)$ is the standard normal probability density function. Hence the ES forecast is $\\widehat{\\text{ES}}_t = \\sigma_t \\, \\mathbb{E}[Z \\mid Z \\le q_\\alpha] = -\\sigma_t \\, \\varphi(q_\\alpha)/\\alpha$.\n- Backtesting the VaR uses exceedance indicators $I_t = \\mathbf{1}\\{r_t < \\widehat{\\text{VaR}}_t\\}$. Unconditional coverage under correct calibration implies that $I_t$ are independent and identically distributed Bernoulli with success probability $\\alpha$. Over a backtest sample of length $n$, let $x = \\sum_{t=1}^n I_t$ denote the number of exceedances. The likelihood ratio statistic for the unconditional coverage hypothesis equals\n$$\n\\text{LR}_{\\text{uc}} = 2\\left[ x \\ln\\left(\\frac{x/n}{\\alpha}\\right) + (n-x)\\ln\\left(\\frac{1 - x/n}{1-\\alpha}\\right)\\right],\n$$\nwith the conventions that $x \\ln(x/n)$ is $0$ when $x=0$ and $(n-x)\\ln(1 - x/n)$ is $0$ when $x=n$. Under the null hypothesis, $\\text{LR}_{\\text{uc}}$ is asymptotically $\\chi^2$ distributed with $1$ degree of freedom. The empirical hit rate is $\\widehat{p} = x/n$.\n- A strictly consistent scoring function for quantiles at level $\\alpha$ is the \"check loss\" $\\ell_t^{Q} = \\left(\\alpha - I_t\\right)\\left(r_t - \\widehat{\\text{VaR}}_t\\right)$, whose sample average $\\overline{\\ell}^{Q}$ should be small for well-calibrated quantile forecasts.\n- A simple ES backtesting diagnostic compares the realized tail conditional mean to the average ES forecast on exceedance days. Define the realized shortfall as $\\overline{r}_{\\text{tail}} = \\frac{1}{x}\\sum_{t: I_t=1} r_t$ (defined only if $x \\ge 1$) and the corresponding mean ES forecast on exceedance days as $\\overline{\\widehat{\\text{ES}}}_{\\text{tail}} = \\frac{1}{x}\\sum_{t: I_t=1} \\widehat{\\text{ES}}_t$. The absolute ES bias is $|\\overline{\\widehat{\\text{ES}}}_{\\text{tail}} - \\overline{r}_{\\text{tail}}|$.\n\nModels to compare:\n- Baseline model (proper chronology): At time $t$, estimate $\\sigma_t$ using the sample standard deviation over a rolling window of length $W$ from the $W$ most recent past returns $\\{r_{t-W},\\dots,r_{t-1}\\}$.\n- Ghost-feature model (look-ahead bias): At time $t$, erroneously estimate $\\sigma_t$ using a rolling window that includes the current return, namely $\\{r_{t-W+1},\\dots,r_t\\}$. This uses future information relative to the decision point, which is forbidden in genuine forecasting, but here is intentionally included to quantify its impact.\n\nData generating process:\n- Returns are generated by a Gaussian Generalized Autoregressive Conditional Heteroskedasticity (GARCH($1,1$)) process with zero conditional mean:\n$$\n\\sigma_t^2 = \\omega + \\alpha_G r_{t-1}^2 + \\beta_G \\sigma_{t-1}^2,\\quad r_t = \\sigma_t \\, z_t,\\quad z_t \\sim \\mathcal{N}(0,1),\n$$\nwith given parameters $\\omega > 0$, $\\alpha_G \\ge 0$, and $\\beta_G \\ge 0$ satisfying $\\alpha_G + \\beta_G < 1$ for covariance stationarity. When $\\alpha_G = 0$ and $\\beta_G = 0$, this reduces to independent and identically distributed Gaussian returns with variance $\\omega$. Initialize $\\sigma_0^2$ at the unconditional variance $\\omega/(1 - \\alpha_G - \\beta_G)$ when $\\alpha_G + \\beta_G < 1$, and otherwise at $\\omega$ for numerical stability.\n\nBacktesting metrics to compute for each model:\n- Hit-rate absolute error: $|\\widehat{p} - \\alpha|$.\n- Unconditional coverage likelihood ratio statistic: $\\text{LR}_{\\text{uc}}$.\n- Average quantile check loss: $\\overline{\\ell}^{Q}$.\n- Absolute ES bias: $|\\overline{\\widehat{\\text{ES}}}_{\\text{tail}} - \\overline{r}_{\\text{tail}}|$.\n\nRequired comparison outputs:\nFor each test case below, compute the following four differences (baseline minus ghost), in this order:\n$D_1 =$ hit-rate absolute error reduction $= |\\widehat{p}_{\\text{base}} - \\alpha| - |\\widehat{p}_{\\text{ghost}} - \\alpha|$,\n$D_2 =$ unconditional coverage likelihood ratio reduction $= \\text{LR}_{\\text{uc, base}} - \\text{LR}_{\\text{uc, ghost}}$,\n$D_3 =$ quantile check-loss reduction $= \\overline{\\ell}^{Q}_{\\text{base}} - \\overline{\\ell}^{Q}_{\\text{ghost}}$,\n$D_4 =$ ES absolute-bias reduction $= |\\overline{\\widehat{\\text{ES}}}_{\\text{tail, base}} - \\overline{r}_{\\text{tail, base}}| - |\\overline{\\widehat{\\text{ES}}}_{\\text{tail, ghost}} - \\overline{r}_{\\text{tail, ghost}}|$.\n\nA positive value of any $D_j$ indicates that the ghost-feature model appears \"better\" by that metric, which would be spurious because it illegitimately uses future information.\n\nTest suite:\nImplement the four test cases below. For each, simulate the returns with the provided pseudo-random number generator seed to ensure determinism, then compute the metrics with a rolling window of length $W$ and level $\\alpha$ as specified. All $\\alpha$ values must be treated as decimals, not percentages.\n\n- Case A (heteroskedastic, strong persistence): $T = 4000$, $\\omega = 10^{-6}$, $\\alpha_G = 0.05$, $\\beta_G = 0.94$, $W = 250$, $\\alpha = 0.01$, seed $= 1729$.\n- Case B (heteroskedastic, moderate persistence, shorter sample): $T = 2000$, $\\omega = 10^{-6}$, $\\alpha_G = 0.15$, $\\beta_G = 0.80$, $W = 100$, $\\alpha = 0.05$, seed $= 2718$.\n- Case C (independent and identically distributed Gaussian, edge case): $T = 4000$, $\\omega = 10^{-4}$, $\\alpha_G = 0.00$, $\\beta_G = 0.00$, $W = 250$, $\\alpha = 0.01$, seed $= 3141$.\n- Case D (boundary: small window): $T = 1500$, $\\omega = 10^{-6}$, $\\alpha_G = 0.10$, $\\beta_G = 0.85$, $W = 30$, $\\alpha = 0.02$, seed $= 1618$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of $16$ floats enclosed in square brackets, ordered as $[D_{1,A},D_{2,A},D_{3,A},D_{4,A},D_{1,B},D_{2,B},D_{3,B},D_{4,B},D_{1,C},D_{2,C},D_{3,C},D_{4,C},D_{1,D},D_{2,D},D_{3,D},D_{4,D}]$, where the subscript denotes the test case. Each float must be rounded to $6$ decimal places. No other text may be printed.\n\nNumerical notes:\n- The rolling standard deviation should use the unbiased sample variance with denominator $(W-1)$.\n- When computing the truncated-normal quantities, use $q_\\alpha = \\Phi^{-1}(\\alpha)$ and $\\mathbb{E}[Z \\mid Z \\le q_\\alpha] = -\\varphi(q_\\alpha)/\\alpha$.\n- The unconditional coverage likelihood ratio $\\text{LR}_{\\text{uc}}$ uses the conventions stated above for $x=0$ or $x=n$ to avoid undefined logarithms.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of financial econometrics, well-posed, and objective. It provides a complete and consistent set of requirements for a computational experiment comparing two risk models. The objective is to quantify the impact of look-ahead bias (\"ghost features\") on standard backtesting metrics for Value-at-Risk (VaR) and Expected Shortfall (ES).\n\nThe solution will be developed in four sequential stages:\n1.  Simulation of financial return data from a specified GARCH($1$,$1$) process.\n2.  Implementation of the two rolling-window volatility models: a properly specified baseline model and a flawed ghost-feature model.\n3.  Computation of VaR and ES forecasts, followed by the calculation of four distinct backtesting performance metrics for each model.\n4.  Calculation of the differences in these metrics between the two models to quantify the spurious performance improvement gained from look-ahead bias.\n\n**Stage 1: Data Generating Process**\n\nThe synthetic return series $\\{r_t\\}$ is generated from a Gaussian GARCH($1$,$1$) process. The conditional variance $\\sigma_t^2$ and return $r_t$ are given by:\n$$\n\\sigma_t^2 = \\omega + \\alpha_G r_{t-1}^2 + \\beta_G \\sigma_{t-1}^2\n$$\n$$\nr_t = \\sigma_t z_t, \\quad \\text{where} \\quad z_t \\sim \\mathcal{N}(0,1) \\quad \\text{i.i.d.}\n$$\nThe process is initialized at time $t=0$. For a stationary process, where $\\alpha_G + \\beta_G < 1$, the initial variance $\\sigma_0^2$ is set to the unconditional variance $\\sigma^2 = \\omega / (1 - \\alpha_G - \\beta_G)$. If $\\alpha_G + \\beta_G \\ge 1$, $\\sigma_0^2$ is set to $\\omega$.\nA series of $T$ returns, $\\{r_0, r_1, \\dots, r_{T-1}\\}$, is generated for each test case using a specific pseudo-random number generator seed to ensure deterministic and reproducible results.\n\n**Stage 2: Risk Model Specification and Forecasting**\n\nTwo models are used to forecast the one-step-ahead conditional standard deviation, $\\sigma_t$. The forecasts are performed over a backtesting period from $t=W$ to $t=T-1$, where $W$ is the rolling window size, for a total of $n = T-W$ observations. Both models estimate $\\sigma_t$ using the unbiased sample standard deviation (denominator $W-1$) of returns in a rolling window.\n\n- **Baseline Model (Proper Chronology):** The forecast for time $t$, denoted $\\sigma_{t, \\text{base}}$, is computed using the $W$ most recent *past* returns available at time $t-1$:\n$$ \\sigma_{t, \\text{base}} = \\text{StDev}(\\{r_{t-W}, r_{t-W+1}, \\dots, r_{t-1}\\}) $$\n- **Ghost-Feature Model (Look-Ahead Bias):** The forecast for time $t$, denoted $\\sigma_{t, \\text{ghost}}$, is computed using a window that erroneously includes the current return $r_t$:\n$$ \\sigma_{t, \\text{ghost}} = \\text{StDev}(\\{r_{t-W+1}, r_{t-W+2}, \\dots, r_t\\}) $$\n\nFor a given tail probability $\\alpha$, the VaR and ES forecasts for model $m \\in \\{\\text{base, ghost}\\}$ are:\n$$ \\widehat{\\text{VaR}}_{t,m} = q_\\alpha \\sigma_{t,m}, \\quad \\text{where} \\quad q_\\alpha = \\Phi^{-1}(\\alpha) $$\n$$ \\widehat{\\text{ES}}_{t,m} = -\\frac{\\varphi(q_\\alpha)}{\\alpha} \\sigma_{t,m} $$\nHere, $\\Phi^{-1}(\\cdot)$ is the inverse CDF (quantile function) and $\\varphi(\\cdot)$ is the PDF of the standard normal distribution. The constants $q_\\alpha$ and the multiplier for ES are pre-calculated for efficiency.\n\n**Stage 3: Backtesting Metrics**\n\nFor each model, we evaluate the quality of its forecasts using four metrics over the backtest sample of size $n=T-W$. A VaR exceedance is recorded when $r_t < \\widehat{\\text{VaR}}_t$, indicated by $I_t = \\mathbf{1}\\{r_t < \\widehat{\\text{VaR}}_t\\}$. The total number of exceedances is $x = \\sum_{i=1}^n I_t$.\n\n1.  **Hit-rate absolute error:** Measures the deviation of the empirical exceedance frequency $\\widehat{p} = x/n$ from the target level $\\alpha$:\n$$ |\\widehat{p} - \\alpha| $$\n2.  **Unconditional coverage likelihood ratio ($\\text{LR}_{\\text{uc}}$):** Tests the hypothesis that $\\mathbb{E}[I_t] = \\alpha$. The statistic is:\n$$ \\text{LR}_{\\text{uc}} = 2\\left[ x \\ln\\left(\\frac{x/n}{\\alpha}\\right) + (n-x)\\ln\\left(\\frac{1 - x/n}{1-\\alpha}\\right)\\right] $$\nThe specified conventions for $x=0$ or $x=n$ are strictly followed to handle logarithmic terms.\n3.  **Average quantile check loss ($\\overline{\\ell}^{Q}$):** A strictly consistent scoring function for quantiles. Lower values are better.\n$$ \\overline{\\ell}^{Q} = \\frac{1}{n}\\sum_{t=1}^{n} (\\alpha - I_t)(r_t - \\widehat{\\text{VaR}}_t) $$\n4.  **Absolute ES bias:** Measures the discrepancy between the average forecasted ES on exceedance days and the average realized return on those same days.\n$$ |\\overline{\\widehat{\\text{ES}}}_{\\text{tail}} - \\overline{r}_{\\text{tail}}| $$\nwhere $\\overline{r}_{\\text{tail}} = \\frac{1}{x}\\sum_{t: I_t=1} r_t$ and $\\overline{\\widehat{\\text{ES}}}_{\\text{tail}} = \\frac{1}{x}\\sum_{t: I_t=1} \\widehat{\\text{ES}}_t$. If $x=0$ (no exceedances), the bias is defined to be $0$, as there are no tail events to evaluate.\n\n**Stage 4: Comparative Analysis and Final Output**\n\nThe core of the analysis is the direct comparison of the two models. For each of the four test cases (A, B, C, D), we compute the four metrics for both the baseline and ghost-feature models. The final required outputs are the differences in these metrics, calculated as (baseline metric) - (ghost metric):\n\n- $D_1 = |\\widehat{p}_{\\text{base}} - \\alpha| - |\\widehat{p}_{\\text{ghost}} - \\alpha|$\n- $D_2 = \\text{LR}_{\\text{uc, base}} - \\text{LR}_{\\text{uc, ghost}}$\n- $D_3 = \\overline{\\ell}^{Q}_{\\text{base}} - \\overline{\\ell}^{Q}_{\\text{ghost}}$\n- $D_4 = |\\overline{\\widehat{\\text{ES}}}_{\\text{tail, base}} - \\overline{r}_{\\text{tail, base}}| - |\\overline{\\widehat{\\text{ES}}}_{\\text{tail, ghost}} - \\overline{r}_{\\text{tail, ghost}}|$\n\nA positive value for any $D_j$ indicates a spurious improvement in the performance of the ghost-feature model due to its use of future information. The procedure is executed for all four test cases, yielding $16$ scalar values that are formatted into the required single-line output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and compares two rolling Gaussian risk models (VaR and ES),\n    quantifying the effect of 'ghost features' (look-ahead bias) on backtesting outcomes.\n    \"\"\"\n\n    test_cases = [\n        # Case A (heteroskedastic, strong persistence)\n        {'T': 4000, 'omega': 1e-6, 'alpha_G': 0.05, 'beta_G': 0.94, 'W': 250, 'alpha': 0.01, 'seed': 1729},\n        # Case B (heteroskedastic, moderate persistence, shorter sample)\n        {'T': 2000, 'omega': 1e-6, 'alpha_G': 0.15, 'beta_G': 0.80, 'W': 100, 'alpha': 0.05, 'seed': 2718},\n        # Case C (independent and identically distributed Gaussian, edge case)\n        {'T': 4000, 'omega': 1e-4, 'alpha_G': 0.00, 'beta_G': 0.00, 'W': 250, 'alpha': 0.01, 'seed': 3141},\n        # Case D (boundary: small window)\n        {'T': 1500, 'omega': 1e-6, 'alpha_G': 0.10, 'beta_G': 0.85, 'W': 30, 'alpha': 0.02, 'seed': 1618},\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        T, omega, alpha_G, beta_G, W, alpha, seed = case.values()\n        \n        # 1. Generate GARCH(1,1) return series\n        rng = np.random.default_rng(seed)\n        z = rng.normal(size=T)\n        \n        returns = np.zeros(T)\n        sigma2 = np.zeros(T)\n\n        if alpha_G + beta_G < 1.0:\n            sigma2_uncond = omega / (1.0 - alpha_G - beta_G)\n        else:\n            sigma2_uncond = omega\n        \n        sigma2[0] = sigma2_uncond\n        returns[0] = np.sqrt(sigma2[0]) * z[0]\n\n        for t in range(1, T):\n            sigma2[t] = omega + alpha_G * returns[t-1]**2 + beta_G * sigma2[t-1]\n            returns[t] = np.sqrt(sigma2[t]) * z[t]\n        \n        # 2. Setup for backtesting\n        backtest_returns = returns[W:]\n        n_backtest = T - W\n        \n        q_alpha = norm.ppf(alpha)\n        es_multiplier = -norm.pdf(q_alpha) / alpha\n\n        # 3. Calculate metrics for both models\n        models = ['base', 'ghost']\n        metrics_all = {}\n\n        for model_type in models:\n            var_forecasts = np.zeros(n_backtest)\n            es_forecasts = np.zeros(n_backtest)\n            \n            for i in range(n_backtest):\n                t_current = W + i\n                if model_type == 'base':\n                    window = returns[t_current - W : t_current]\n                else: # ghost model\n                    window = returns[t_current - W + 1 : t_current + 1]\n                \n                sigma_t = np.std(window, ddof=1)\n                var_forecasts[i] = q_alpha * sigma_t\n                es_forecasts[i] = es_multiplier * sigma_t\n            \n            # --- Compute all 4 metrics ---\n            \n            # Exceedances\n            I = backtest_returns < var_forecasts\n            x = np.sum(I)\n            \n            # Metric 1: Hit-rate absolute error\n            p_hat = x / n_backtest\n            hit_rate_error = np.abs(p_hat - alpha)\n\n            # Metric 2: LR_uc\n            lr_uc = 0.0\n            if x > 0 and x < n_backtest :\n                term1 = x * np.log(p_hat / alpha)\n                term2 = (n_backtest - x) * np.log((1 - p_hat) / (1 - alpha))\n                lr_uc = 2 * (term1 + term2)\n            elif x == 0 and n_backtest > 0:\n                lr_uc = 2 * (n_backtest * np.log(1 / (1-alpha)))\n            elif x == n_backtest and n_backtest > 0:\n                 lr_uc = 2 * (n_backtest * np.log(1 / alpha))\n\n            # Metric 3: Average quantile check loss\n            avg_check_loss = np.mean((alpha - I) * (backtest_returns - var_forecasts))\n            \n            # Metric 4: Absolute ES bias\n            abs_es_bias = 0.0\n            if x > 0:\n                realized_shortfall = np.mean(backtest_returns[I])\n                mean_es_on_exceedance = np.mean(es_forecasts[I])\n                abs_es_bias = np.abs(mean_es_on_exceedance - realized_shortfall)\n                \n            metrics_all[model_type] = (hit_rate_error, lr_uc, avg_check_loss, abs_es_bias)\n\n        # 4. Compute differences (baseline - ghost)\n        base_metrics = metrics_all['base']\n        ghost_metrics = metrics_all['ghost']\n        diffs = [base_metrics[j] - ghost_metrics[j] for j in range(4)]\n        results.extend(diffs)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"}]}