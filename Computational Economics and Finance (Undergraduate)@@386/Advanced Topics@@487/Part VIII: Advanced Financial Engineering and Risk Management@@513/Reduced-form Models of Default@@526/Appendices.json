{"hands_on_practices": [{"introduction": "This first practice grounds your understanding of reduced-form models in the most fundamental concept: the Poisson process. You will simulate default times, which in this simple model are treated as the first arrival time of a Poisson process with a constant intensity, $\\lambda$. This hands-on simulation [@problem_id:2425480] is crucial for developing an intuition for how default events are distributed over time and for mastering the inverse transform sampling method, a cornerstone of computational finance.", "id": "2425480", "problem": "You are asked to implement a program that simulates default times in a reduced-form (intensity-based) model for independent firms under a constant default intensity. In this framework, the default time for a single firm is a random time with a constant intensity parameter $ \\lambda $, and firms default independently of each other. All time quantities are in years.\n\nYour program must, for each specified test case, generate $ N $ independent default times under a constant intensity $ \\lambda $, observe the process over a finite horizon $ [0, T] $ (defaults after $ T $ are not counted), partition the horizon into equal-width time bins of width $ w $ years, and count how many default times fall into each bin. If $ \\lambda = 0 $, interpret this as no defaults occurring in any finite horizon. If $ N = 0 $, interpret this as no firms to simulate. The bins must exactly partition $ [0, T] $ into $ B $ equal subintervals of width $ w $, where $ B = T / w $ is an integer. Use the provided random seeds to ensure reproducibility.\n\nFor each test case, the required output is the list of integer counts per bin, from the earliest time bin to the latest, where each bin corresponds to the half-open interval $ [t_k, t_{k+1}) $ for $ k = 0, 1, \\dots, B - 2 $, and the last bin includes the right endpoint $ T $, i.e., $ [t_{B-1}, T] $, where $ t_k = k w $.\n\nTest Suite:\n- Case 1 (general case): $ N = 1000 $, $ \\lambda = 0.05 $ per year, $ T = 80 $ years, $ w = 5 $ years, random seed $ s = 1729 $.\n- Case 2 (zero intensity edge case): $ N = 1000 $, $ \\lambda = 0 $, $ T = 20 $ years, $ w = 5 $ years, random seed $ s = 7 $.\n- Case 3 (high intensity, short horizon): $ N = 1000 $, $ \\lambda = 2.0 $ per year, $ T = 3.0 $ years, $ w = 0.5 $ years, random seed $ s = 2024 $.\n- Case 4 (zero firms edge case): $ N = 0 $, $ \\lambda = 0.1 $ per year, $ T = 10 $ years, $ w = 1 $ year, random seed $ s = 99 $.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, with no spaces anywhere. For example, if there were two test cases, an acceptable format would be: [[1,2,3],[0,0,1]]. In this problem, you must output four lists (one per test case) aggregated into a single top-level list on one line, with no additional text.\n\nAll times are in years, and all outputs are dimensionless integer counts per bin. Angles are not involved in this task. The final output must follow exactly the specified single-line format.", "solution": "The problem statement will first be subjected to a rigorous validation process before any attempt at a solution is made.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe explicit data, variables, and conditions provided are as follows:\n- **Model**: Reduced-form (intensity-based) model for independent firms.\n- **Default Intensity**: Constant parameter, denoted by $\\lambda$.\n- **Default Time**: A random variable $\\tau$ governed by the intensity $\\lambda$. If $\\lambda=0$, no default occurs.\n- **Number of Firms**: $N$. If $N=0$, no firms are simulated.\n- **Observation Horizon**: A finite interval $[0, T]$. Defaults after time $T$ are discarded.\n- **Time Bins**: The horizon $[0, T]$ is partitioned into $B$ equal-width bins of width $w$.\n- **Bin Partitioning**: $B = T/w$ is an integer. The bins are $[t_k, t_{k+1})$ for $k = 0, 1, \\dots, B-2$, and the last bin is $[t_{B-1}, T]$, where $t_k = kw$.\n- **Randomness**: A random seed $s$ is provided for each test case to ensure reproducibility.\n- **Output**: For each test case, a list of integer counts of defaults per bin.\n\n**Test Suite:**\n- **Case 1**: $N = 1000$, $\\lambda = 0.05$ yr$^{-1}$, $T = 80$ yr, $w = 5$ yr, $s = 1729$.\n- **Case 2**: $N = 1000$, $\\lambda = 0$ yr$^{-1}$, $T = 20$ yr, $w = 5$ yr, $s = 7$.\n- **Case 3**: $N = 1000$, $\\lambda = 2.0$ yr$^{-1}$, $T = 3.0$ yr, $w = 0.5$ yr, $s = 2024$.\n- **Case 4**: $N = 0$, $\\lambda = 0.1$ yr$^{-1}$, $T = 10$ yr, $w = 1$ yr, $s = 99$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n\n1.  **Scientifically Grounded**: The problem describes the simulation of default times using an intensity-based model. This is equivalent to modeling default as the first arrival time of a Poisson process with constant intensity $\\lambda$. The time to such an event is a well-known random variable following an exponential distribution. This framework is a cornerstone of quantitative finance and credit risk modeling. The premises are scientifically and mathematically sound.\n\n2.  **Well-Posed**: The problem is unambiguous. All parameters ($N$, $\\lambda$, $T$, $w$, $s$) are specified for each case. The task—simulating default times and counting them in discrete time bins—is clearly defined. The use of a random seed ensures that the simulation, while stochastic in principle, produces a unique, deterministic output. The problem is well-posed.\n\n3.  **Objective**: The language is formal and quantitative. There are no subjective, opinion-based, or ambiguous statements.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. It is a standard, well-defined simulation task based on fundamental principles of probability theory and their application in computational finance. A solution will be provided.\n\n### Solution Derivation\n\nThe problem requires the simulation of default times for $N$ independent firms. The default of each firm is governed by a constant intensity, $\\lambda$. This is a classic Poisson process model.\n\n**Theoretical Foundation**\n\nThe time to the first event (default) in a Poisson process with constant rate $\\lambda$ is a random variable $\\tau$ that follows an exponential distribution. The probability that a firm has not defaulted by time $t$, known as the survival probability, is given by:\n$$\nS(t) = P(\\tau > t) = e^{-\\lambda t}\n$$\nThe cumulative distribution function (CDF) of the default time $\\tau$, which gives the probability of default occurring at or before time $t$, is therefore:\n$$\nF(t) = P(\\tau \\le t) = 1 - S(t) = 1 - e^{-\\lambda t}\n$$\n\n**Simulation Methodology**\n\nTo generate random variates from a given distribution, the inverse transform sampling method is a standard and robust technique. Let $U$ be a random variable drawn from the standard uniform distribution on the interval $(0, 1)$, i.e., $U \\sim \\text{Uniform}(0, 1)$. We can generate a sample $\\tau$ from the exponential distribution by setting $F(\\tau) = U$ and solving for $\\tau$:\n$$\nU = 1 - e^{-\\lambda \\tau}\n$$\n$$\n1 - U = e^{-\\lambda \\tau}\n$$\n$$\n\\ln(1 - U) = -\\lambda \\tau\n$$\n$$\n\\tau = -\\frac{1}{\\lambda} \\ln(1 - U)\n$$\nSince $1 - U$ is also distributed as $\\text{Uniform}(0, 1)$, we can simplify the expression to:\n$$\n\\tau = -\\frac{1}{\\lambda} \\ln(U)\n$$\nThis formula is used to generate the default time for each of the $N$ firms.\n\n**Algorithmic Implementation**\n\nFor each test case, specified by the parameters $(N, \\lambda, T, w, s)$, the following algorithm is executed:\n\n1.  **Initialization**: A pseudo-random number generator is initialized with the given seed $s$ to ensure the result is reproducible.\n\n2.  **Edge Case Handling**:\n    *   If the number of firms $N = 0$, there are no defaults to simulate.\n    *   If the default intensity $\\lambda = 0$, the default time $\\tau = \\infty$ for any firm. In either case, no defaults will be observed within the finite horizon $[0, T]$.\n    *   The number of bins is $B = T/w$. The output for these cases is a list containing $B$ zeros.\n\n3.  **General Case Simulation** ($N > 0$ and $\\lambda > 0$):\n    a.  **Generate Random Variates**: A vector of $N$ independent random numbers $\\{U_i\\}_{i=1}^N$ is drawn from the distribution $\\text{Uniform}(0, 1)$.\n    b.  **Generate Default Times**: This vector is transformed into a vector of default times $\\{\\tau_i\\}_{i=1}^N$ using the inverse transform formula: $\\tau_i = -\\frac{1}{\\lambda} \\ln(U_i)$. This operation is performed in a vectorized manner for computational efficiency.\n    c.  **Apply Horizon Constraint**: The problem states that only defaults within the horizon $[0, T]$ are counted. Therefore, the simulated default times are filtered to keep only those for which $\\tau_i \\le T$.\n    d.  **Binning**: The observation interval $[0, T]$ is divided into $B = T/w$ bins. The bin edges are defined by the sequence $\\{0, w, 2w, \\dots, (B-1)w, T\\}$. A histogram is constructed by counting how many of the filtered default times fall into each bin. The problem specifies that the $k$-th bin is the half-open interval $[kw, (k+1)w)$ for $k \\in \\{0, \\dots, B-2\\}$, and the last bin is the closed interval $[(B-1)w, T]$. Standard numerical libraries provide histogram functions that correctly implement this logic. For instance, the `numpy.histogram` function in Python with its default settings precisely matches this requirement.\n    e.  **Output**: The resulting array of counts for each bin constitutes the final answer for the test case.\n\nThis procedure is systematically applied to each of the four test cases provided. The final output is an aggregation of the results from each case into a single formatted string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates default times in a reduced-form model and provides bin counts.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {'N': 1000, 'lambda': 0.05, 'T': 80.0, 'w': 5.0, 'seed': 1729},\n        # Case 2 (zero intensity edge case)\n        {'N': 1000, 'lambda': 0.0, 'T': 20.0, 'w': 5.0, 'seed': 7},\n        # Case 3 (high intensity, short horizon)\n        {'N': 1000, 'lambda': 2.0, 'T': 3.0, 'w': 0.5, 'seed': 2024},\n        # Case 4 (zero firms edge case)\n        {'N': 0, 'lambda': 0.1, 'T': 10.0, 'w': 1.0, 'seed': 99},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        lam = case['lambda']\n        T = case['T']\n        w = case['w']\n        seed = case['seed']\n\n        # The number of bins, B, is guaranteed to be an integer.\n        num_bins = int(T / w)\n\n        # Handle edge cases where N=0 or lambda=0.\n        # If N=0, there are no firms to default.\n        # If lambda=0, the intensity is zero, so the default time is infinite.\n        # In both scenarios, no defaults occur in any finite horizon.\n        if N == 0 or lam == 0:\n            counts = [0] * num_bins\n            results.append(counts)\n            continue\n\n        # Initialize the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate N uniform random variables from (0, 1).\n        # We use rng.random which is exclusive of 1.0, but log(0) is still an issue\n        # if the sample is exactly 0. In practice, this is exceedingly rare.\n        # np.random.uniform is on [low, high), so we can use it on (0,1)\n        # However, np.random.random() is sufficient and standard.\n        uniform_samples = rng.random(size=N)\n\n        # Apply the inverse transform sampling method to get exponentially distributed default times.\n        # tau = - (1/lambda) * ln(U)\n        default_times = -np.log(uniform_samples) / lam\n\n        # Filter out defaults that occur after the time horizon T.\n        defaults_in_horizon = default_times[default_times <= T]\n\n        # Define the bin edges for the histogram.\n        # np.linspace(0, T, num_bins + 1) creates edges [0, w, 2w, ..., T].\n        bin_edges = np.linspace(0.0, T, num_bins + 1)\n\n        # Use numpy.histogram to count defaults in each bin.\n        # By default, np.histogram uses bins as [edge, next_edge) for all but the\n        # last bin, which is [last_edge_-1, last_edge]. This matches the problem spec exactly.\n        # The problem states: \"[t_k, t_{k+1})\" for k=0..B-2, and \"[t_{B-1}, T]\" for the last bin.\n        counts_np, _ = np.histogram(defaults_in_horizon, bins=bin_edges)\n\n        # Convert the numpy array of counts to a list of standard Python integers.\n        counts = counts_np.tolist()\n        \n        results.append(counts)\n\n    # The required output format is a single line with no spaces.\n    # repr() creates a string representation, e.g., \"[[1, 2], [3, 4]]\".\n    # .replace(' ', '') removes all spaces to match the required format \"[[1,2],[3,4]]\".\n    final_output = repr(results).replace(' ', '')\n    print(final_output)\n\nsolve()\n```"}, {"introduction": "Building upon the basic simulation, this exercise takes you into the realm of statistical estimation, a key activity in quantitative risk modeling. Here, we move beyond a single default intensity to a more realistic model where the intensity $\\lambda$ depends on borrower-specific characteristics like FICO scores and income [@problem_id:2425483]. You will implement a Maximum Likelihood Estimation (MLE) to calibrate the model using a dataset that includes right-censored observations, a common feature in survival data, and then use your calibrated model to make predictions.", "id": "2425483", "problem": "Consider an intensity-based reduced-form model of default for unsecured peer-to-peer consumer loans. Let the default time be a nonnegative random variable with a conditionally constant default intensity over the observation window, denoted by $\\lambda(\\mathbf{x})$, where $\\mathbf{x}$ denotes borrower covariates. The default intensity is specified by a log-linear function of transformed covariates:\n$$\n\\lambda(\\mathbf{x}) \\equiv \\exp\\!\\left(\\theta_0 + \\theta_1\\,g_1(\\text{FICO}) + \\theta_2\\,g_2(\\text{INC})\\right),\n$$\nwhere $g_1(\\text{FICO}) = \\dfrac{\\text{FICO} - 700}{100}$ and $g_2(\\text{INC}) = \\ln\\!\\left(\\dfrac{\\text{INC}}{50000}\\right)$. The natural logarithm is used. The units for time are in years, and income is measured in United States dollars per year.\n\nAssume $6$ independent loans are observed, each with covariates, an observed time in years, and a right-censoring indicator. For loan $i$, the observed time is $t_i \\ge 0$, and the event indicator is $d_i \\in \\{0,1\\}$, where $d_i=1$ indicates that default occurred at time $t_i$ and $d_i=0$ indicates right-censoring at time $t_i$. The observed dataset is:\n- Loan A: FICO $650$, income $30000$, observed time $0.5$, default indicator $1$.\n- Loan B: FICO $720$, income $60000$, observed time $1.0$, default indicator $0$.\n- Loan C: FICO $680$, income $45000$, observed time $0.8$, default indicator $1$.\n- Loan D: FICO $760$, income $120000$, observed time $1.2$, default indicator $0$.\n- Loan E: FICO $700$, income $50000$, observed time $1.5$, default indicator $0$.\n- Loan F: FICO $600$, income $25000$, observed time $0.3$, default indicator $1$.\n\nUnder the model above, with the default time conditionally exponentially distributed given $\\mathbf{x}$ and the independence assumption across loans, estimate the parameter vector $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1,\\theta_2)$ by maximizing the likelihood implied by the model and the observed data.\n\nAfter estimating $\\boldsymbol{\\theta}$, compute the model-implied probability of default within a fixed horizon $H$ (in years) for each of the following test cases, using the same covariate transformations $g_1$ and $g_2$:\n\nTest suite of new loans and horizons:\n- Case $1$: FICO $710$, income $55000$, horizon $H=1.0$.\n- Case $2$: FICO $580$, income $20000$, horizon $H=1.0$.\n- Case $3$: FICO $800$, income $200000$, horizon $H=1.0$.\n- Case $4$: FICO $700$, income $50000$, horizon $H=0.5$.\n\nYour program must:\n- Ingest the fixed dataset above and estimate $\\boldsymbol{\\theta}$ using only the information provided and the model definition.\n- For each test case, compute the probability that default occurs within the specified horizon $H$, expressed as a decimal in $[0,1]$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as Cases $1$ through $4$, with each value rounded to $6$ decimal places. For example, an output should look like $[0.123456,0.234567,0.345678,0.456789]$.\n\nAll times must be treated in years. All probabilities must be expressed as decimals, not as percentages, and must be rounded to exactly $6$ decimal places in the final output.", "solution": "The problem statement is subjected to validation.\n\nGivens are extracted verbatim:\n- Model: Intensity-based reduced-form model.\n- Default intensity: $\\lambda(\\mathbf{x}) \\equiv \\exp\\!\\left(\\theta_0 + \\theta_1\\,g_1(\\text{FICO}) + \\theta_2\\,g_2(\\text{INC})\\right)$.\n- Covariate transformations: $g_1(\\text{FICO}) = \\dfrac{\\text{FICO} - 700}{100}$ and $g_2(\\text{INC}) = \\ln\\!\\left(\\dfrac{\\text{INC}}{50000}\\right)$.\n- Time units: years.\n- Data: $6$ independent loans with covariates ($\\text{FICO}_i, \\text{INC}_i$), observed time $t_i$, and default indicator $d_i \\in \\{0,1\\}$.\n    - Loan A: FICO $650$, income $30000$, $t_A=0.5$, $d_A=1$.\n    - Loan B: FICO $720$, income $60000$, $t_B=1.0$, $d_B=0$.\n    - Loan C: FICO $680$, income $45000$, $t_C=0.8$, $d_C=1$.\n    - Loan D: FICO $760$, income $120000$, $t_D=1.2$, $d_D=0$.\n    - Loan E: FICO $700$, income $50000$, $t_E=1.5$, $d_E=0$.\n    - Loan F: FICO $600$, income $25000$, $t_F=0.3$, $d_F=1$.\n- Task 1: Estimate $\\boldsymbol{\\theta} = (\\theta_0,\\theta_1,\\theta_2)$ via Maximum Likelihood Estimation (MLE).\n- Task 2: Compute model-implied probability of default within horizon $H$ for $4$ test cases.\n    - Case $1$: FICO $710$, income $55000$, $H=1.0$.\n    - Case $2$: FICO $580$, income $20000$, $H=1.0$.\n    - Case $3$: FICO $800$, income $200000$, $H=1.0$.\n    - Case $4$: FICO $700$, income $50000$, $H=0.5$.\n- Output format: Comma-separated list of probabilities in brackets, rounded to $6$ decimal places.\n\nValidation verdict:\nThe problem is scientifically grounded, well-posed, and objective. It describes a standard application of survival analysis, specifically an exponential regression model with right-censored data, a common technique in biostatistics and econometrics for modeling time-to-event phenomena. The model is specified completely, the data are provided, and the objective is clear. There are no contradictions, ambiguities, or violations of scientific principles. The problem is valid. We proceed with the solution.\n\nThe core of the problem is to estimate the parameter vector $\\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2)^T$ of a parametric survival model by maximizing the likelihood function of the observed data. The model assumes that the time to default, $T$, for a given loan with covariates $\\mathbf{x}$, is an exponential random variable with a constant rate parameter $\\lambda(\\mathbf{x})$, known as the default intensity. The intensity is given by the log-linear form $\\lambda(\\mathbf{x}) = \\exp(\\boldsymbol{\\theta}^T \\mathbf{x})$, where $\\mathbf{x} = [1, g_1(\\text{FICO}), g_2(\\text{INC})]^T$ is the vector of transformed covariates including a constant for the intercept term $\\theta_0$.\n\nFor an exponential distribution with rate $\\lambda$, the probability density function (PDF) is $f(t|\\lambda) = \\lambda e^{-\\lambda t}$ and the survival function is $S(t|\\lambda) = P(T > t) = e^{-\\lambda t}$. The dataset contains right-censored observations. For an observation $i$ with observed time $t_i$ and event indicator $d_i$:\n- If default occurred ($d_i=1$), the event was observed at time $t_i$, and its contribution to the likelihood is the PDF evaluated at $t_i$: $f(t_i|\\lambda_i) = \\lambda_i e^{-\\lambda_i t_i}$.\n- If the observation was right-censored ($d_i=0$), the event did not occur by time $t_i$, and its contribution to the likelihood is the probability of survival beyond $t_i$: $S(t_i|\\lambda_i) = e^{-\\lambda_i t_i}$.\n\nThe likelihood contribution for a single observation $i$ can be written compactly as:\n$$\nL_i(\\boldsymbol{\\theta}) = [f(t_i|\\lambda_i)]^{d_i} [S(t_i|\\lambda_i)]^{1-d_i} = (\\lambda_i e^{-\\lambda_i t_i})^{d_i} (e^{-\\lambda_i t_i})^{1-d_i} = \\lambda_i^{d_i} e^{-\\lambda_i t_i}.\n$$\nAssuming independence across the $N=6$ loans, the total likelihood is the product of individual contributions:\n$$\nL(\\boldsymbol{\\theta}) = \\prod_{i=1}^{N} L_i(\\boldsymbol{\\theta}) = \\prod_{i=1}^{N} \\lambda_i^{d_i} e^{-\\lambda_i t_i}.\n$$\nMaximizing the likelihood is equivalent to a more numerically stable task of an maximizing the log-likelihood function, $\\ell(\\boldsymbol{\\theta}) = \\ln L(\\boldsymbol{\\theta})$:\n$$\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\ln(\\lambda_i^{d_i} e^{-\\lambda_i t_i}) = \\sum_{i=1}^{N} (d_i \\ln \\lambda_i - \\lambda_i t_i).\n$$\nSubstituting $\\lambda_i = \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}_i)$, the log-likelihood becomes:\n$$\n\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left(d_i (\\boldsymbol{\\theta}^T \\mathbf{x}_i) - t_i \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}_i)\\right).\n$$\nThis function is to be maximized with respect to $\\boldsymbol{\\theta}$. This is a standard unconstrained optimization problem. We can find the optimal parameters $\\hat{\\boldsymbol{\\theta}}$ by finding the root of the gradient of the log-likelihood. The partial derivative with respect to each parameter $\\theta_j$ is:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{i=1}^{N} \\left( d_i x_{ij} - t_i \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}_i) x_{ij} \\right) = \\sum_{i=1}^{N} x_{ij} (d_i - t_i \\lambda_i).\n$$\nSetting the gradient vector $\\nabla_{\\boldsymbol{\\theta}} \\ell(\\boldsymbol{\\theta})$ to zero yields a system of nonlinear equations that must be solved numerically. This is achieved by minimizing the negative log-likelihood, $-\\ell(\\boldsymbol{\\theta})$, using a quasi-Newton method such as BFGS, which uses gradient information to guide the search for the minimum.\n\nFirst, we construct the $6 \\times 3$ design matrix $\\mathbf{X}$ from the training data, where each row corresponds to a loan and is of the form $[1, g_1(\\text{FICO}_i), g_2(\\text{INC}_i)]$. We also form the vectors of observed times $\\mathbf{t}$ and event indicators $\\mathbf{d}$. The negative log-likelihood function to be minimized is:\n$$\n-\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left( t_i \\exp(\\boldsymbol{\\theta}^T \\mathbf{x}_i) - d_i (\\boldsymbol{\\theta}^T \\mathbf{x}_i) \\right).\n$$\nThe gradient of this objective function is $\\mathbf{X}^T(\\mathbf{t} \\odot \\boldsymbol{\\lambda} - \\mathbf{d})$, where $\\odot$ denotes element-wise multiplication.\n\nOnce the Maximum Likelihood Estimate $\\hat{\\boldsymbol{\\theta}}$ is obtained, we can compute the default probability for a new loan with covariates $\\mathbf{x}_{\\text{new}}$ over a horizon $H$. The estimated default intensity is $\\hat{\\lambda}_{\\text{new}} = \\exp(\\hat{\\boldsymbol{\\theta}}^T \\mathbf{x}_{\\text{new}})$. The probability of default occurring by time $H$ is given by the cumulative distribution function of the exponential distribution:\n$$\nP(T \\le H | \\mathbf{x}_{\\text{new}}) = 1 - S(H | \\mathbf{x}_{\\text{new}}) = 1 - e^{-\\hat{\\lambda}_{\\text{new}} H}.\n$$\nThis calculation is performed for each of the four test cases provided in the problem statement, using their respective covariates and horizons. The final numerical results are rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating parameters for a reduced-form default model\n    and calculating default probabilities for new loans.\n    \"\"\"\n    \n    # Step 1: Define the training dataset from the problem statement.\n    # Data: (FICO, Income, Observed Time, Default Indicator)\n    training_data = [\n        (650, 30000, 0.5, 1),  # Loan A\n        (720, 60000, 1.0, 0),  # Loan B\n        (680, 45000, 0.8, 1),  # Loan C\n        (760, 120000, 1.2, 0),  # Loan D\n        (700, 50000, 1.5, 0),  # Loan E\n        (600, 25000, 0.3, 1),  # Loan F\n    ]\n\n    # Step 2: Define covariate transformation functions.\n    def g1(fico):\n        return (fico - 700) / 100\n\n    def g2(income):\n        return np.log(income / 50000)\n\n    # Step 3: Prepare the design matrix X and vectors t_obs, d_obs.\n    n_loans = len(training_data)\n    X = np.zeros((n_loans, 3))\n    t_obs = np.zeros(n_loans)\n    d_obs = np.zeros(n_loans)\n\n    for i, (fico, income, time, default) in enumerate(training_data):\n        X[i, 0] = 1 # Intercept\n        X[i, 1] = g1(fico)\n        X[i, 2] = g2(income)\n        t_obs[i] = time\n        d_obs[i] = default\n\n    # Step 4: Define the negative log-likelihood function and its gradient for optimization.\n    def neg_log_likelihood(theta):\n        \"\"\"Negative log-likelihood function for exponential regression model.\"\"\"\n        eta = X @ theta\n        lambda_vals = np.exp(eta)\n        log_lik = d_obs @ eta - t_obs @ lambda_vals\n        return -log_lik\n    \n    def grad_neg_log_likelihood(theta):\n        \"\"\"Gradient of the negative log-likelihood function.\"\"\"\n        eta = X @ theta\n        lambda_vals = np.exp(eta)\n        residual = t_obs * lambda_vals - d_obs\n        grad = X.T @ residual\n        return grad\n\n    # Step 5: Perform Maximum Likelihood Estimation using a numerical optimizer.\n    theta_initial = np.zeros(3) # Initial guess for theta = (theta_0, theta_1, theta_2)\n    \n    # Using BFGS algorithm which is a quasi-Newton method.\n    opt_result = minimize(\n        fun=neg_log_likelihood,\n        x0=theta_initial,\n        method='BFGS',\n        jac=grad_neg_log_likelihood,\n        options={'disp': False}\n    )\n    \n    theta_hat = opt_result.x\n\n    # Step 6: Define test cases and compute default probabilities.\n    test_cases = [\n        # (FICO, Income, Horizon H)\n        (710, 55000, 1.0),   # Case 1\n        (580, 20000, 1.0),   # Case 2\n        (800, 200000, 1.0),  # Case 3\n        (700, 50000, 0.5),   # Case 4\n    ]\n\n    results = []\n    for fico, income, horizon in test_cases:\n        # Create covariate vector for the test case\n        x_new = np.array([1, g1(fico), g2(income)])\n        \n        # Calculate estimated default intensity\n        lambda_hat = np.exp(x_new @ theta_hat)\n        \n        # Calculate probability of default within the horizon\n        prob_default = 1 - np.exp(-lambda_hat * horizon)\n        \n        results.append(prob_default)\n\n    # Step 7: Format the output as specified.\n    # The results must be rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "This final practice elevates our model from a static to a dynamic framework, reflecting the reality that perceived credit risk changes over time. You will work with a stochastic default intensity driven by a mean-reverting Ornstein-Uhlenbeck process, a classic example of an affine model [@problem_id:2425452]. Your task is to perform a model calibration, a sophisticated and practical skill that involves fitting the model's parameters to an observed term structure of Credit Default Swap (CDS) spreads, thereby ensuring your model's prices are consistent with the market.", "id": "2425452", "problem": "You are tasked with building a complete, runnable program that calibrates an affine reduced-form default intensity model to synthetic Credit Default Swap (CDS) data. The model is specified as follows. The default intensity is given by $\\lambda_t = Y_t + \\psi$, where $Y_t$ follows an Ornstein–Uhlenbeck (OU) process under the risk-neutral measure:\n$$\ndY_t = \\kappa \\left( \\theta - Y_t \\right) dt + \\sigma \\, dW_t,\n$$\nwith $\\kappa &gt; 0$, $\\sigma \\ge 0$, $\\theta \\in \\mathbb{R}$, and $Y_0 \\in \\mathbb{R}$. The affine offset $\\psi \\in \\mathbb{R}$ is a constant parameter. The CDS is priced under the reduced-form approach with a continuous premium approximation, constant risk-free interest rate $r \\ge 0$, fractional Loss Given Default (LGD) in $(0,1]$, and protection maturity $T &gt; 0$.\n\nBegin from the following foundational base without invoking any unproven shortcuts:\n- The Ornstein–Uhlenbeck stochastic differential equation and its well-known Gaussianity.\n- The survival probability definition $S(t) = \\mathbb{E}\\left[ \\exp\\left( - \\int_0^t \\lambda_s \\, ds \\right) \\right]$.\n- The continuous-premium fair CDS spread $s^{\\star}(T)$ solves the equation that equates the present value of the default leg and the present value of the premium leg,\n$$\ns^{\\star}(T) \\int_0^T e^{- r t} S(t) \\, dt = \\mathrm{LGD} \\int_0^T e^{- r t} \\left( - \\frac{dS(t)}{dt} \\right) dt.\n$$\n\nYour program must:\n- Derive and implement the survival probability $S(t)$ implied by the model given $Y_0$, $\\kappa$, $\\theta$, $\\sigma$, and $\\psi$.\n- Use $S(t)$ to compute the continuous-premium fair CDS par spread $s^{\\star}(T)$ for each quoted maturity $T$, given constant $r$ and fractional $\\mathrm{LGD}$, via the integral equation above. If needed, you may use numerical quadrature. No discrete accrual conventions should be used; assume the continuous-premium approximation exclusively.\n- Calibrate the unknown parameters $\\psi$ and $\\theta$ by minimizing the sum of squared errors between model-implied par spreads and the provided synthetic quotes across maturities for each dataset. Use a least-squares criterion and impose simple, reasonable bounds that guarantee well-posedness for the optimizer. You should treat $(\\kappa,\\sigma,r,\\mathrm{LGD},Y_0)$ as known inputs for each dataset.\n\nImportant units and conventions:\n- All rates (including $\\kappa$, $\\theta$, $\\sigma$, $Y_0$, $\\psi$, $r$, and CDS par spreads) must be treated as “per year” in decimal form (for example, $0.02$ means “two percent per year”).\n- There are no angles or physical units in this task.\n- Report all outputs as decimals.\n\nTest suite to implement and cover in your code (for each case below, generate synthetic CDS quotes by first computing model-implied par spreads $s^{\\star}(T)$ at the stated maturities using the “true” parameters, then calibrate $\\psi$ and $\\theta$ back from those quotes, treating $\\kappa$, $\\sigma$, $Y_0$, $r$, and $\\mathrm{LGD}$ as known):\n\n- Case A (happy path, moderate mean reversion and volatility):\n  - Known inputs: $\\kappa = 1.0$, $\\sigma = 0.1$, $Y_0 = 0.02$, $r = 0.02$, $\\mathrm{LGD} = 0.6$, maturities $T \\in \\{ 1.0, 3.0, 5.0 \\}$ years.\n  - True parameters for data generation: $\\theta_{\\text{true}} = 0.015$, $\\psi_{\\text{true}} = 0.01$.\n\n- Case B (boundary condition with deterministic intensity component, $\\sigma = 0$):\n  - Known inputs: $\\kappa = 0.7$, $\\sigma = 0.0$, $Y_0 = 0.03$, $r = 0.01$, $\\mathrm{LGD} = 0.6$, maturities $T \\in \\{ 1.0, 2.0, 4.0 \\}$ years.\n  - True parameters for data generation: $\\theta_{\\text{true}} = 0.03$, $\\psi_{\\text{true}} = 0.005$.\n\n- Case C (fast mean reversion, higher offset):\n  - Known inputs: $\\kappa = 3.0$, $\\sigma = 0.2$, $Y_0 = 0.01$, $r = 0.015$, $\\mathrm{LGD} = 0.6$, maturities $T \\in \\{ 2.0, 5.0, 7.0 \\}$ years.\n  - True parameters for data generation: $\\theta_{\\text{true}} = 0.012$, $\\psi_{\\text{true}} = 0.02$.\n\nAlgorithmic requirements:\n- For each case, first compute synthetic quotes using the true parameters. Then, using those quotes alone, calibrate $\\psi$ and $\\theta$ via least squares subject to simple bounds that keep $\\psi$ strictly positive and $\\theta$ within a wide, plausible interval. Use robust numerical integration for all needed integrals.\n- Output formatting: Your program should produce a single line of output containing a flat list of calibrated parameters ordered as\n$$\n[\\psi_A, \\theta_A, \\psi_B, \\theta_B, \\psi_C, \\theta_C],\n$$\nwhere the subscript denotes the case. Round each value to $6$ decimal places. For example, an acceptable format is $[0.010000,0.015000,0.005000,0.030000,0.020000,0.012000]$.\n\nYour implementation must be self-contained, require no user input, and not access any external resources. The final printed line must match the exact format described above. All quantities must be computed in per-year decimals.", "solution": "The problem statement is subjected to validation and is found to be valid. It is scientifically grounded in the principles of quantitative finance, specifically affine term structure models. The model, an Ornstein-Uhlenbeck process driving a default intensity, is a standard construct (a Vasicek-type model). The problem is well-posed, providing sufficient information for a unique solution via numerical calibration, and is free from ambiguity or contradiction. It represents a standard exercise in computational finance.\n\nThe solution proceeds in three principal stages: first, the derivation of the survival probability function; second, the formulation of the Credit Default Swap (CDS) spread calculation; and third, the specification of the numerical calibration procedure.\n\n### 1. Survival Probability Derivation\n\nThe default intensity is given by the affine process $\\lambda_t = Y_t + \\psi$, where $Y_t$ follows the Ornstein-Uhlenbeck (OU) process under the risk-neutral measure $\\mathbb{Q}$:\n$$\ndY_t = \\kappa (\\theta - Y_t) dt + \\sigma dW_t\n$$\nwith initial condition $Y_0$. The parameters $\\kappa$, $\\theta$, $\\sigma$, and $\\psi$ are constants.\n\nThe survival probability $S(t)$ until time $t$ is defined as the risk-neutral expectation of the discount factor associated with the cumulative default intensity:\n$$\nS(t) = \\mathbb{E}^{\\mathbb{Q}} \\left[ \\exp\\left( - \\int_0^t \\lambda_s ds \\right) \\bigg| \\mathcal{F}_0 \\right]\n$$\nSubstituting $\\lambda_s = Y_s + \\psi$ and using the linearity of the integral yields:\n$$\nS(t) = \\mathbb{E}^{\\mathbb{Q}} \\left[ \\exp\\left( - \\int_0^t (Y_s + \\psi) ds \\right) \\right] = \\exp(-\\psi t) \\mathbb{E}^{\\mathbb{Q}} \\left[ \\exp\\left( - \\int_0^t Y_s ds \\right) \\right]\n$$\nThe expectation term is a standard result for affine processes. For an OU process as defined for $Y_t$, the expectation is known to have an exponentially affine form in the initial state $Y_0$:\n$$\n\\mathbb{E}^{\\mathbb{Q}} \\left[ \\exp\\left( - \\int_0^t Y_s ds \\right) \\right] = \\exp\\left( A(t) - B(t)Y_0 \\right)\n$$\nwhere $A(t)$ and $B(t)$ are deterministic functions of time. These functions solve a system of ordinary differential equations derived from the Feynman-Kac theorem. The standard solutions are:\n$$\nB(t) = \\frac{1}{\\kappa} \\left( 1 - e^{-\\kappa t} \\right)\n$$\nand\n$$\nA(t) = \\left( \\theta - \\frac{\\sigma^2}{2\\kappa^2} \\right) [B(t) - t] - \\frac{\\sigma^2}{4\\kappa} B(t)^2\n$$\nThis formulation for $A(t)$ and $B(t)$ is robust and holds even for the case $\\sigma = 0$, where the model becomes deterministic. The case $\\kappa \\to 0$ represents a different process (Brownian motion) and is not covered here, as the problem specifies $\\kappa > 0$.\n\nCombining these results, the survival probability $S(t)$ is given by the following analytical expression:\n$$\nS(t) = \\exp \\left( -\\psi t + A(t) - B(t)Y_0 \\right)\n$$\n\n### 2. CDS Spread Calculation\n\nThe fair par spread for a continuous-premium CDS, denoted $s^{\\star}(T)$, is defined by the equation equating the present value (PV) of the premium leg and the PV of the default leg:\n$$\ns^{\\star}(T) \\int_0^T e^{-rt} S(t) dt = \\mathrm{LGD} \\int_0^T e^{-rt} \\left( -\\frac{dS(t)}{dt} \\right) dt\n$$\nwhere $r$ is the constant risk-free rate and $\\mathrm{LGD}$ is the fractional Loss Given Default.\n\nLet the PV of the premium leg be $PL(T) = \\int_0^T e^{-rt} S(t) dt$. The integral on the right-hand side, representing the PV of the LGD-scaled default probability density, can be simplified using integration by parts. Let $u = e^{-rt}$ and $dv = -S'(t)dt$. Then $du = -re^{-rt}dt$ and $v = S(t)$.\n$$\n\\int_0^T e^{-rt}(-S'(t))dt = \\left[ e^{-rt}S(t) \\right]_0^T - \\int_0^T S(t)(-re^{-rt})dt\n$$\n$$\n= \\left( e^{-rT}S(T) - S(0) \\right) + r \\int_0^T e^{-rt}S(t)dt\n$$\nSince $S(0) = 1$ (survival probability at time zero is unity), this becomes:\n$$\n\\int_0^T e^{-rt}(-S'(t))dt = e^{-rT}S(T) - 1 + r \\cdot PL(T)\n$$\nWait, this is an error in derivation. Re-evaluating the integration by parts:\nLet $u = e^{-rt}$ and $dv = -S'(t)dt$. The integral is $\\int u \\, dv$.\nBy parts: $\\int u \\, dv = uv - \\int v \\, du$.\n$uv = e^{-rt}(-S(t))$. Thus $[uv]_0^T = -e^{-rT}S(T) - (-e^0 S(0)) = 1-e^{-rT}S(T)$.\n$\\int v \\, du = \\int_0^T (-S(t))(-re^{-rt})dt = r \\int_0^T e^{-rt}S(t)dt = r \\cdot PL(T)$.\nTherefore, the correct expression for the default leg integral is:\n$$\n\\int_0^T e^{-rt}(-S'(t))dt = 1 - e^{-rT}S(T) - r \\cdot PL(T)\n$$\nThis expression is more stable numerically as it avoids differentiation of $S(t)$.\n\nThe fair spread $s^{\\star}(T)$ is then found by solving for it:\n$$\ns^{\\star}(T) = \\mathrm{LGD} \\frac{1 - e^{-rT}S(T) - r \\cdot PL(T)}{PL(T)}\n$$\nThe integral for $PL(T)$ does not have a closed-form solution and must be computed via numerical quadrature.\n\n### 3. Calibration Procedure\n\nThe goal is to calibrate the parameters $\\mathbf{p} = (\\psi, \\theta)$ by minimizing the sum of squared differences between the model-implied spreads, $s^{\\star}_{\\text{model}}(T_i; \\mathbf{p})$, and the synthetically generated market quotes, $s^{\\text{quote}}_i$, across a set of maturities $\\{T_i\\}_{i=1}^N$.\n\nThe objective function to minimize is:\n$$\nJ(\\psi, \\theta) = \\sum_{i=1}^N \\left( s^{\\star}_{\\text{model}}(T_i; \\psi, \\theta) - s^{\\text{quote}}_i \\right)^2\n$$\nThe known parameters for each calibration are $(\\kappa, \\sigma, Y_0, r, \\mathrm{LGD})$. This is a non-linear least-squares problem. It will be solved using a quasi-Newton optimization algorithm (specifically L-BFGS-B, which handles box constraints) subject to bounds on the parameters. As specified, we will impose $\\psi > 0$ and constrain $\\theta$ to a wide, reasonable interval to ensure the stability and financial sensibility of the solution.\n\nThe overall algorithm for each test case is as follows:\n1.  Using the \"true\" parameters $(\\psi_{\\text{true}}, \\theta_{\\text{true}})$ and the other knowns for the case, compute the synthetic CDS quotes $s^{\\text{quote}}_i$ for each maturity $T_i$ by implementing the spread formula derived above.\n2.  Define the objective function $J(\\psi, \\theta)$ which takes candidate parameters, computes the corresponding model spreads, and returns the sum of squared errors against the synthetic quotes.\n3.  Use an optimization routine (`scipy.optimize.minimize`) with an initial guess and predefined bounds to find the parameter vector $(\\hat{\\psi}, \\hat{\\theta})$ that minimizes $J$.\n4.  The resulting pair $(\\hat{\\psi}, \\hat{\\theta})$ constitutes the calibrated parameters for that case. This process is repeated for all specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import minimize\n\ndef B_func(t, kappa):\n    \"\"\"\n    Calculates the B(t) function for the affine model.\n    B(t) = (1/kappa) * (1 - exp(-kappa * t))\n    \"\"\"\n    if np.isclose(kappa, 0.0):\n        # Taylor expansion for small kappa: (1 - (1 - kt + (kt)^2/2)) / k = t - kt^2/2 -> t\n        return t\n    return (1.0 - np.exp(-kappa * t)) / kappa\n\ndef A_func(t, kappa, theta, sigma):\n    \"\"\"\n    Calculates the A(t) function for the affine model.\n    A(t) = (theta - sigma^2/(2*kappa^2)) * [B(t) - t] - (sigma^2 / (4*kappa)) * B(t)^2\n    \"\"\"\n    if np.isclose(kappa, 0.0):\n        # Limit for kappa -> 0 with dY_t = sigma * dW_t\n        # This case is not required by problem spec (kappa>0) but good for robustness.\n        # Direct derivation gives A(t) = sigma^2 * t^3 / 6\n        return sigma**2 * t**3 / 6.0\n        \n    b_val = B_func(t, kappa)\n    \n    # This calculation can be unstable if kappa is very small.\n    # Grouping terms to improve stability. theta * (B(t) - t) is one part.\n    # The sigma part: sigma^2 * [ - (B(t)-t)/(2k^2) - B(t)^2/(4k) ]\n    # = sigma^2 * [ (t-B(t))/(2k^2) - B(t)^2/(4k) ]\n    # = sigma^2/(4k^3) * [ 2k(t-B(t)) - k^2 B(t)^2 ]\n    # 2k(t-B(t)) = 2k(t - (1-e^-kt)/k) = 2kt - 2 + 2e^-kt\n    # k^2 B(t)^2 = (1-e^-kt)^2 = 1 - 2e^-kt + e^-2kt\n    # sum = 2kt - 3 + 4e^-kt - e^-2kt\n    # So A(t) = theta(B(t)-t) + sigma^2/(4k^3) * (2kt - 3 + 4e^-kt - e^-2kt)\n    # The standard form is generally stable enough for kappa values not extremely close to 0.\n    term1_factor = theta - (sigma**2) / (2.0 * kappa**2)\n    term1 = term1_factor * (b_val - t)\n    term2 = -(sigma**2) / (4.0 * kappa) * b_val**2\n    return term1 + term2\n\ndef survival_prob(t, Y0, kappa, theta, sigma, psi):\n    \"\"\"\n    Calculates the survival probability S(t) given model parameters.\n    S(t) = exp(-psi*t + A(t) - B(t)*Y0)\n    \"\"\"\n    if t < 1e-9:  # Survival probability at t=0 is 1.\n        return 1.0\n\n    a_val = A_func(t, kappa, theta, sigma)\n    b_val = B_func(t, kappa)\n    \n    exponent = -psi * t + a_val - b_val * Y0\n    return np.exp(exponent)\n\ndef calculate_spread(T_maturity, Y0, kappa, theta, sigma, psi, r, LGD):\n    \"\"\"\n    Calculates the fair CDS par spread s*(T) for a given maturity.\n    \"\"\"\n    # Integrand for the premium leg PV\n    integrand_pl = lambda t: np.exp(-r * t) * survival_prob(t, Y0, kappa, theta, sigma, psi)\n    \n    # Calculate premium leg PV using numerical quadrature\n    pl_pv, quad_err = quad(integrand_pl, 0, T_maturity, epsabs=1e-9, epsrel=1e-9)\n    \n    if pl_pv < 1e-12:\n        return np.inf  # Avoid division by zero\n        \n    s_T = survival_prob(T_maturity, Y0, kappa, theta, sigma, psi)\n    \n    # PV of default leg per unit of LGD, derived using integration by parts\n    dl_pv_per_lgd = 1.0 - np.exp(-r * T_maturity) * s_T - r * pl_pv\n    \n    spread = LGD * dl_pv_per_lgd / pl_pv\n    return spread\n\ndef solve():\n    \"\"\"\n    Main function to run test cases, calibrate parameters, and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"knowns\": {\"kappa\": 1.0, \"sigma\": 0.1, \"Y0\": 0.02, \"r\": 0.02, \"LGD\": 0.6},\n            \"maturities\": [1.0, 3.0, 5.0],\n            \"true_params\": {\"theta\": 0.015, \"psi\": 0.01},\n        },\n        {\n            \"name\": \"Case B\",\n            \"knowns\": {\"kappa\": 0.7, \"sigma\": 0.0, \"Y0\": 0.03, \"r\": 0.01, \"LGD\": 0.6},\n            \"maturities\": [1.0, 2.0, 4.0],\n            \"true_params\": {\"theta\": 0.03, \"psi\": 0.005},\n        },\n        {\n            \"name\": \"Case C\",\n            \"knowns\": {\"kappa\": 3.0, \"sigma\": 0.2, \"Y0\": 0.01, \"r\": 0.015, \"LGD\": 0.6},\n            \"maturities\": [2.0, 5.0, 7.0],\n            \"true_params\": {\"theta\": 0.012, \"psi\": 0.02},\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        knowns = case[\"knowns\"]\n        true_params = case[\"true_params\"]\n        maturities = case[\"maturities\"]\n\n        # Step 1: Generate synthetic CDS quotes using the true parameters\n        synthetic_quotes = [\n            calculate_spread(\n                T_maturity=T,\n                Y0=knowns[\"Y0\"],\n                kappa=knowns[\"kappa\"],\n                theta=true_params[\"theta\"],\n                sigma=knowns[\"sigma\"],\n                psi=true_params[\"psi\"],\n                r=knowns[\"r\"],\n                LGD=knowns[\"LGD\"],\n            ) for T in maturities\n        ]\n\n        # Step 2: Define the objective function for calibration\n        def objective_function(params):\n            psi_cal, theta_cal = params\n            \n            # Simple check for parameter validity\n            if psi_cal <= 0:\n                return 1e9 # Return a large number if parameters are out of bounds\n\n            model_spreads = [\n                calculate_spread(\n                    T_maturity=T,\n                    Y0=knowns[\"Y0\"],\n                    kappa=knowns[\"kappa\"],\n                    theta=theta_cal,\n                    sigma=knowns[\"sigma\"],\n                    psi=psi_cal,\n                    r=knowns[\"r\"],\n                    LGD=knowns[\"LGD\"],\n                ) for T in maturities\n            ]\n            \n            error = np.sum((np.array(model_spreads) - np.array(synthetic_quotes))**2)\n            return error\n\n        # Step 3: Calibrate parameters using least squares minimization\n        initial_guess = [0.01, 0.01]  # Generic initial guess\n        bounds = [(1e-7, 0.5), (-0.1, 0.5)] # Bounds: psi > 0, theta in a wide range\n\n        result = minimize(\n            objective_function,\n            initial_guess,\n            method='L-BFGS-B',\n            bounds=bounds,\n            options={'ftol': 1e-12, 'gtol': 1e-12}\n        )\n        \n        calibrated_psi, calibrated_theta = result.x\n        all_results.extend([calibrated_psi, calibrated_theta])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}