{"hands_on_practices": [{"introduction": "To truly appreciate the elegance of the Conjugate Gradient (CG) method, it is illuminating to compare it directly against a more intuitive but often less efficient alternative, the method of Steepest Descent. This exercise guides you through a step-by-step calculation for a simple two-asset portfolio optimization problem. By computing the progress made by each algorithm at every step, you will see concretely how the CG method's choice of a $\\Sigma$-conjugate search direction leads to a much faster convergence than the myopic, zigzagging path often taken by Steepest Descent [@problem_id:2382887].", "id": "2382887", "problem": "Consider an unconstrained meanâ€“variance portfolio selection problem with two risky assets. The objective is to minimize the quadratic function\n$$f(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\Sigma \\, w \\;-\\; \\mu^{\\top} w,$$\nwhere the covariance matrix is\n$$\\Sigma \\;=\\; \\begin{pmatrix} 2 & 1 \\\n$$4pt] 1 & 3 \\end{pmatrix},$$\nthe expected return vector is\n$$\\mu \\;=\\; \\begin{pmatrix} 1 \\\n$$4pt] 0 \\end{pmatrix},$$\nand the initial portfolio is\n$$w_{0} \\;=\\; \\begin{pmatrix} 0 \\\n$$4pt] 0 \\end{pmatrix}.$$\n\nAt step $k \\in \\{1,2\\}$, compare two search directions from the same current iterate: the steepest descent direction $d_{k}^{\\mathrm{SD}} \\,=\\, -\\nabla f(w_{k-1})$ and a Conjugate Gradient (CG) direction $d_{k}^{\\mathrm{CG}}$ constructed to be $\\Sigma$-conjugate to the previous CG direction (with $d_{1}^{\\mathrm{CG}} \\,=\\, -\\nabla f(w_{0})$ and $d_{2}^{\\mathrm{CG}}$ satisfying $\\big(d_{2}^{\\mathrm{CG}}\\big)^{\\top} \\Sigma \\, d_{1}^{\\mathrm{CG}} \\,=\\, 0$). For each direction at each step, take the exact line search step that minimizes $f$ along the corresponding line, and denote the resulting post-step points by $w_{k}^{\\mathrm{SD}}$ and $w_{k}^{\\mathrm{CG}}$. Define the suboptimality at step $k$ as\n$$s_{k} \\;=\\; f\\!\\big(w_{k}^{\\mathrm{SD}}\\big) \\;-\\; f\\!\\big(w_{k}^{\\mathrm{CG}}\\big).$$\n\nCompute $s_{1}$ and $s_{2}$ exactly. Report your final answer as a single row vector $\\big(s_{1} \\;\\; s_{2}\\big)$ using exact fractions. Do not include units and do not round.", "solution": "The problem statement is scientifically grounded, well-posed, and objective. It describes a standard quadratic optimization task for which the specified numerical algorithms, Steepest Descent (SD) and Conjugate Gradient (CG), are well-defined. We proceed with a formal derivation.\n\nThe objective function to minimize is given by\n$$f(w) = \\frac{1}{2} w^{\\top} \\Sigma w - \\mu^{\\top} w$$\nThe gradient of this function is\n$$\\nabla f(w) = \\Sigma w - \\mu$$\nThe initial point is $w_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The initial gradient is $\\nabla f(w_0) = \\Sigma w_0 - \\mu = -\\mu = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$.\nThe residual vector is defined as $r = -\\nabla f(w)$. The initial residual is $r_0 = -\\nabla f(w_0) = \\mu = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nThe optimal step size $\\alpha$ for an exact line search from an iterate $w_k$ along a direction $d_k$ is given by the formula that minimizes $f(w_k + \\alpha d_k)$:\n$$\\alpha_k = -\\frac{\\nabla f(w_k)^{\\top} d_k}{d_k^{\\top} \\Sigma d_k}$$\n\nStep $k=1$:\n\nFor both SD and CG, the initial search direction is the negative gradient:\n$$d_1^{\\mathrm{SD}} = d_1^{\\mathrm{CG}} = -\\nabla f(w_0) = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nThe step size $\\alpha_1$ is identical for both methods:\n$$\\alpha_1 = -\\frac{\\nabla f(w_0)^{\\top} d_1}{d_1^{\\top} \\Sigma d_1} = \\frac{r_0^{\\top} r_0}{r_0^{\\top} \\Sigma r_0}$$\nWe compute the terms:\n$$r_0^{\\top} r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$$\n$$r_0^{\\top} \\Sigma r_0 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\nThe step size is $\\alpha_1 = \\frac{1}{2}$.\nThe new iterates are identical:\n$$w_1^{\\mathrm{SD}} = w_1^{\\mathrm{CG}} = w_0 + \\alpha_1 d_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\nLet us denote this common iterate as $w_1$. The value of the objective function at this point is:\n$$f(w_1) = \\frac{1}{2} w_1^{\\top} \\Sigma w_1 - \\mu^{\\top} w_1 = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$$\n$$f(w_1) = \\frac{1}{2} \\begin{pmatrix} 1 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\left(\\frac{1}{2}\\right) - \\frac{1}{2} = \\frac{1}{4} - \\frac{1}{2} = -\\frac{1}{4}$$\nSince $f(w_1^{\\mathrm{SD}}) = f(w_1^{\\mathrm{CG}}) = -\\frac{1}{4}$, the suboptimality at step $1$ is:\n$$s_1 = f(w_1^{\\mathrm{SD}}) - f(w_1^{\\mathrm{CG}}) = 0$$\n\nStep $k=2$:\n\nWe start from the common iterate $w_1 = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix}$. The gradient at this point is:\n$$\\nabla f(w_1) = \\Sigma w_1 - \\mu = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\end{pmatrix}$$\nThe new residual is $r_1 = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$.\n\nFor the SD method, the search direction is $d_2^{\\mathrm{SD}} = -\\nabla f(w_1) = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix}$.\nThe step size is $\\alpha_2^{\\mathrm{SD}} = \\frac{r_1^{\\top} r_1}{r_1^{\\top} \\Sigma r_1}$.\nNumerator: $r_1^{\\top} r_1 = (0)^2 + (-\\frac{1}{2})^2 = \\frac{1}{4}$.\nDenominator: $r_1^{\\top} \\Sigma r_1 = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} -\\frac{1}{2} \\\\ -\\frac{3}{2} \\end{pmatrix} = \\frac{3}{4}$.\n$\\alpha_2^{\\mathrm{SD}} = \\frac{1/4}{3/4} = \\frac{1}{3}$.\nThe SD iterate is $w_2^{\\mathrm{SD}} = w_1 + \\alpha_2^{\\mathrm{SD}} d_2^{\\mathrm{SD}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$.\nThe function value is $f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{6} \\end{pmatrix}$.\n$f(w_2^{\\mathrm{SD}}) = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1-\\frac{1}{6} \\\\ \\frac{1}{2}-\\frac{3}{6} \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2} \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{6} \\\\ 0 \\end{pmatrix} - \\frac{1}{2} = \\frac{1}{2}\\left(\\frac{5}{12}\\right) - \\frac{1}{2} = -\\frac{7}{24}$.\n\nFor the CG method, the search direction is $d_2^{\\mathrm{CG}} = r_1 + \\beta_1 d_1^{\\mathrm{CG}}$, with $\\beta_1 = \\frac{r_1^{\\top}r_1}{r_0^{\\top}r_0}$.\nWith $r_1^{\\top} r_1 = \\frac{1}{4}$ and $r_0^{\\top} r_0 = 1$, we have $\\beta_1 = \\frac{1}{4}$.\n$$d_2^{\\mathrm{CG}} = r_1 + \\frac{1}{4}d_1^{\\mathrm{CG}} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{2} \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix}$$\nThe step size is $\\alpha_2^{\\mathrm{CG}} = -\\frac{\\nabla f(w_1)^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}} = \\frac{r_1^{\\top} d_2^{\\mathrm{CG}}}{(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}}}$.\nNumerator: $r_1^{\\top} d_2^{\\mathrm{CG}} = \\begin{pmatrix} 0 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\frac{1}{4}$.\nDenominator: $(d_2^{\\mathrm{CG}})^{\\top} \\Sigma d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -\\frac{5}{4} \\end{pmatrix} = \\frac{5}{8}$.\n$\\alpha_2^{\\mathrm{CG}} = \\frac{1/4}{5/8} = \\frac{2}{5}$.\nThe CG iterate is $w_2^{\\mathrm{CG}} = w_1 + \\alpha_2^{\\mathrm{CG}} d_2^{\\mathrm{CG}} = \\begin{pmatrix} \\frac{1}{2} \\\\ 0 \\end{pmatrix} + \\frac{2}{5} \\begin{pmatrix} \\frac{1}{4} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}+\\frac{1}{10} \\\\ -\\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{5} \\\\ -\\frac{1}{5} \\end{pmatrix}$.\nThis is the exact minimizer $w^* = \\Sigma^{-1}\\mu$, as expected for CG in $N=2$ dimensions.\nThe minimum function value is $f(w_2^{\\mathrm{CG}}) = f(w^*) = -\\frac{1}{2}\\mu^\\top w^* = -\\frac{1}{2}\\begin{pmatrix} 1 & 0 \\end{pmatrix}\\begin{pmatrix} 3/5 \\\\ -1/5 \\end{pmatrix} = -\\frac{3}{10}$.\n\nThe suboptimality at step $2$ is:\n$$s_2 = f(w_2^{\\mathrm{SD}}) - f(w_2^{\\mathrm{CG}}) = -\\frac{7}{24} - \\left(-\\frac{3}{10}\\right) = -\\frac{7}{24} + \\frac{3}{10} = \\frac{-35 + 36}{120} = \\frac{1}{120}$$\n\nThe final values are $s_1 = 0$ and $s_2 = \\frac{1}{120}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0 & \\frac{1}{120} \\end{pmatrix}\n}\n$$"}, {"introduction": "One of the most remarkable theoretical properties of the Conjugate Gradient method is its guaranteed convergence in at most $k$ iterations, where $k$ is the number of distinct eigenvalues of the system matrix. This practice moves beyond general principles to a concrete demonstration of this property using a covariance matrix $\\Sigma$ characteristic of a single-factor asset pricing model. You will prove that this specific matrix structure leads to exactly two distinct eigenvalues and, consequently, that the CG method finds the exact solution in just two steps, bridging abstract linear algebra with portfolio theory [@problem_id:2382876].", "id": "2382876", "problem": "Consider a four-asset meanâ€“variance setting where the return vector has covariance matrix $\\Sigma \\in \\mathbb{R}^{4 \\times 4}$ given by\n$$\n\\Sigma \\;=\\; I_{4} \\;+\\; v v^{\\top}, \\quad v \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nLet the excess-return vector be $b \\in \\mathbb{R}^{4}$ given by\n$$\nb \\;=\\; \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\nYou are to solve the linear system\n$$\n\\Sigma x \\;=\\; b\n$$\nusing the conjugate gradient (CG) method, starting from $x_{0} = 0_{4}$ and assuming exact arithmetic throughout. \n\nTasks:\n- Establish that $\\Sigma$ has exactly two distinct eigenvalues and is symmetric positive definite.\n- Prove that the conjugate gradient method terminates in exactly two iterations for this system under exact arithmetic, and compute the converged solution vector $x$ that satisfies $\\Sigma x = b$.\n- Briefly explain the economic structure that the specific form of $\\Sigma$ implies for the joint distribution of returns.\n\nReport only the converged solution vector $x$ as your final numeric answer, written as a single row matrix. No rounding is required.", "solution": "The problem statement will first be validated for scientific soundness and consistency.\n\nStep 1: Extract Givens.\n- The covariance matrix is $\\Sigma = I_{4} + v v^{\\top}$, where $I_4$ is the $4 \\times 4$ identity matrix.\n- The vector $v$ is given by $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n- The excess-return vector is $b = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix}$.\n- The linear system to solve is $\\Sigma x = b$.\n- The solution method is the conjugate gradient (CG) method.\n- The initial guess is the zero vector, $x_{0} = 0_{4}$.\n- The calculation assumes exact arithmetic.\n\nThe tasks are:\n1. Establish that $\\Sigma$ has exactly two distinct eigenvalues and is symmetric positive definite (SPD).\n2. Prove that the CG method terminates in exactly two iterations and compute the solution vector $x$.\n3. Explain the economic structure implied by the form of $\\Sigma$.\n\nStep 2: Validate Using Extracted Givens.\nThe problem is scientifically grounded, being a standard problem in computational finance and numerical linear algebra. The matrix $\\Sigma$ is a rank-one update of the identity, a well-understood structure. The conjugate gradient method is appropriate for systems where the matrix is SPD. To confirm this, we check the properties of $\\Sigma$.\n\nSymmetry: $\\Sigma^{\\top} = (I_{4} + vv^{\\top})^{\\top} = I_{4}^{\\top} + (vv^{\\top})^{\\top} = I_{4} + (v^{\\top})^{\\top}v^{\\top} = I_{4} + vv^{\\top} = \\Sigma$. The matrix is symmetric.\n\nEigenvalues: Let $u$ be an eigenvector of $\\Sigma$. The eigenvalue equation is $(I_{4} + vv^{\\top})u = \\lambda u$, which rearranges to $(vv^{\\top})u = (\\lambda - 1)u$. This shows that $u$ is also an eigenvector of the rank-one matrix $vv^{\\top}$ with eigenvalue $\\lambda - 1$.\nThe matrix $vv^{\\top}$ has at most one non-zero eigenvalue.\n- If $u$ is a multiple of $v$, say $u=cv$ for some non-zero scalar $c$, then $vv^{\\top}(cv) = v(v^{\\top}cv) = c(v^{\\top}v)v$. The corresponding eigenvalue of $vv^{\\top}$ is $\\lambda_{vv^{\\top}} = v^{\\top}v = 1^2 + 1^2 + 0^2 + 0^2 = 2$.\n- If $u$ is orthogonal to $v$ ($v^{\\top}u = 0$), then $vv^{\\top}u = v(v^{\\top}u) = v(0) = 0$. The eigenvalue is $0$. The space of vectors orthogonal to $v$ in $\\mathbb{R}^{4}$ has dimension $4-1=3$.\nSo, the eigenvalues of $vv^{\\top}$ are $2$ (with multiplicity $1$) and $0$ (with multiplicity $3$).\nThe eigenvalues of $\\Sigma = I_4 + vv^{\\top}$ are given by $\\lambda_{\\Sigma} = 1 + \\lambda_{vv^{\\top}}$. Thus, the eigenvalues of $\\Sigma$ are $1+2=3$ (multiplicity $1$) and $1+0=1$ (multiplicity $3$).\nSince $\\Sigma$ has exactly two distinct eigenvalues ($1$ and $3$), and both are positive, the symmetric matrix $\\Sigma$ is positive definite.\n\nThe problem is well-posed, complete, and consistent. It is a valid problem.\n\nStep 3: Verdict and Action.\nThe problem is valid. I will now provide the solution.\n\nThe problem is addressed in three parts as requested.\n\nPart 1: Eigenvalues and Symmetric Positive Definite (SPD) Property of $\\Sigma$.\nAs established in the validation step, the matrix $\\Sigma = I_4 + vv^{\\top}$ is symmetric. Its eigenvalues are $3$ and $1$. As all eigenvalues are positive, $\\Sigma$ is symmetric positive definite. This confirms that the conjugate gradient method is applicable.\n\nPart 2: Conjugate Gradient Termination and Solution.\nThe conjugate gradient method is guaranteed to find the exact solution to $\\Sigma x = b$ in at most $k$ iterations (assuming exact arithmetic), where $k$ is the number of distinct eigenvalues of $\\Sigma$. Since $\\Sigma$ has exactly two distinct eigenvalues, the method will terminate in exactly two iterations.\n\nTo compute the solution $x$, one could perform the two steps of the CG algorithm. However, a more insightful method utilizes the spectral properties of $\\Sigma$ that we have just derived. This approach is more robust and less prone to arithmetic error. The solution $x$ must lie in the Krylov subspace $\\mathcal{K}_{2}(\\Sigma, r_0)$ since the method converges in two steps and $x_0=0$. This subspace is spanned by $\\{r_0, \\Sigma r_0\\}$, where $r_0=b-\\Sigma x_0=b$.\n\nThe exact solution is given by $x = \\Sigma^{-1}b$. We can compute this by decomposing $b$ into the eigenspaces of $\\Sigma$.\nThe eigenspace corresponding to $\\lambda=3$ is spanned by the vector $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe eigenspace corresponding to $\\lambda=1$ is the orthogonal complement of the space spanned by $v$, i.e., the set of vectors $u$ such that $v^{\\top}u=0$.\n\nWe decompose $b$ into a component parallel to $v$, $b_{\\parallel}$, and a component orthogonal to $v$, $b_{\\perp}$.\n$b = b_{\\parallel} + b_{\\perp}$.\nThe parallel component is the projection of $b$ onto $v$:\n$$\nb_{\\parallel} = \\frac{b^{\\top}v}{v^{\\top}v} v = \\frac{\\begin{pmatrix} 2 & 1 & 5 & -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}}{\\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}} v = \\frac{2(1)+1(1)}{1^2+1^2} v = \\frac{3}{2} v = \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe orthogonal component is $b_{\\perp} = b - b_{\\parallel}$:\n$$\nb_{\\perp} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\nWe can verify that $v^{\\top}b_{\\perp} = 1(1/2) + 1(-1/2) + 0 + 0 = 0$, so $b_{\\perp}$ is indeed in the eigenspace for $\\lambda=1$.\n\nNow we apply $\\Sigma^{-1}$ to $b = b_{\\parallel} + b_{\\perp}$. Since $b_{\\parallel}$ is an eigenvector for $\\lambda=3$ and $b_{\\perp}$ is an eigenvector for $\\lambda=1$:\n$$\nx = \\Sigma^{-1}b = \\Sigma^{-1}(b_{\\parallel} + b_{\\perp}) = \\Sigma^{-1}b_{\\parallel} + \\Sigma^{-1}b_{\\perp}\n$$\n$$\nx = \\frac{1}{3} b_{\\parallel} + \\frac{1}{1} b_{\\perp}\n$$\nSubstituting the vectors:\n$$\nx = \\frac{1}{3} \\begin{pmatrix} 3/2 \\\\ 3/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 5 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1/2+1/2 \\\\ 1/2-1/2 \\\\ 0+5 \\\\ 0-4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix}.\n$$\nThis is the exact solution, which the CG algorithm finds in two steps.\n\nLet us verify the solution:\n$$\n\\Sigma x = (I_4 + vv^{\\top})x = x + v(v^{\\top}x)\n$$\n$$\nv^{\\top}x = \\begin{pmatrix} 1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} = 1(1) + 1(0) = 1.\n$$\n$$\n\\Sigma x = \\begin{pmatrix} 1 \\\\ 0 \\\\ 5 \\\\ -4 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} (1) = \\begin{pmatrix} 1+1 \\\\ 0+1 \\\\ 5+0 \\\\ -4+0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 5 \\\\ -4 \\end{pmatrix} = b.\n$$\nThe solution is correct.\n\nPart 3: Economic Structure.\nThe covariance matrix of asset returns $\\Sigma = I_4 + vv^{\\top}$ is characteristic of a single-factor asset pricing model. In such a model, the return $R_i$ of an asset $i$ is described by its exposure to a common market factor $F$ and an asset-specific idiosyncratic shock $\\epsilon_i$: $R_i = \\beta_i F + \\epsilon_i$. The covariance matrix for the vector of returns $R$ is $\\text{Cov}(R) = (\\beta\\beta^{\\top})\\sigma_F^2 + D$, where $\\beta$ is the vector of factor loadings, $\\sigma_F^2$ is the variance of the factor, and $D$ is the diagonal matrix of idiosyncratic variances.\n\nIn this problem, the structure $\\Sigma = vv^{\\top} + I_4$ implies:\n1.  There is a single systematic risk factor with unit variance ($\\sigma_F^2=1$).\n2.  The vector of factor exposures (loadings) is $v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. This means only the first two assets are exposed to this common risk factor, both with a loading of $1$. Assets 3 and 4 are not exposed to this factor.\n3.  The idiosyncratic risk matrix is $D = I_4$. This means each asset has an independent, specific risk component with a variance of $1$.\n\nConsequently, the non-zero covariance between assets 1 and 2, $(\\Sigma)_{12} = 1$, is entirely explained by their shared exposure to the common factor. All other pairs of assets are uncorrelated, as $(\\Sigma)_{ij} = 0$ for $i \\neq j$ unless $\\{i,j\\}=\\{1,2\\}$. The total variance of assets 1 and 2 is $(\\Sigma)_{11} = (\\Sigma)_{22} = 2$, comprising factor variance ($1^2 \\cdot 1 = 1$) and idiosyncratic variance ($1$). The total variance of assets 3 and 4 is $(\\Sigma)_{33} = (\\Sigma)_{44} = 1$, which is purely idiosyncratic as their factor loading is zero.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 0 & 5 & -4 \\end{pmatrix}}\n$$"}, {"introduction": "The efficiency of the Conjugate Gradient algorithm hinges on the careful construction of a sequence of search directions that are mutually conjugate with respect to the system matrix $A$, a property known as $A$-orthogonality. This computational exercise invites you to look under the hood and test this foundational principle through a controlled experiment. By implementing the CG algorithm and then deliberately perturbing one of the search directions, you will observe how this single disruption of $A$-orthogonality prevents the method from converging in the guaranteed $n$ steps, providing a powerful, hands-on demonstration of the theory [@problem_id:2382914].", "id": "2382914", "problem": "You are given a linear system arising from a penalized meanâ€“variance Markowitz portfolio model. Consider $n$ assets with return vector $\\mu \\in \\mathbb{R}^n$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite (SPD). Introduce a soft budget penalty with weight $\\eta &gt; 0$ to obtain the unconstrained quadratic objective\n$$\nf(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2,\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^n$ is the vector of ones. The first-order condition $\\nabla f(x) = 0$ reduces to the SPD linear system\n$$\nA x = b,\\quad\\text{with}\\quad A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top},\\quad b \\equiv \\mu + \\eta\\, \\mathbf{1}.\n$$\nYour task is to implement the Conjugate Gradient (CG) method to solve $A x = b$ and to demonstrate how perturbing a single search direction breaks $A$-orthogonality of the search directions and prevents exact convergence in at most $n$ steps.\n\nUse the following well-specified instance that is representative for computational finance:\n- Number of assets: $n = 6$.\n- Constant correlation matrix $R \\in \\mathbb{R}^{6 \\times 6}$ with correlation parameter $\\rho = 0.2$, defined by $R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$.\n- Asset standard deviations $s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]$; define $\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$.\n- Expected returns $\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$.\n- Penalty weight $\\eta = 10^{-2}$.\n\nAlgorithmic requirements:\n- Implement the Conjugate Gradient (CG) method starting from $x_0 = 0$. You must compute exactly $n$ iterations, without early termination, using mathematically justified updates that preserve $A$-conjugacy in exact arithmetic for SPD systems.\n- Implement a mechanism to perturb exactly one search direction. Specifically, if the perturbation iteration index is $k_{\\mathrm{perturb}} \\in \\{0,1,\\dots,n-1\\}$ and the scalar perturbation magnitude is $\\varepsilon \\geq 0$, then at iteration $j = k_{\\mathrm{perturb}}$ replace the current search direction $p_j$ by\n$$\n\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1,\n$$\nwhere $e_1 = [1,0,\\dots,0]^{\\top} \\in \\mathbb{R}^n$ is the first standard basis vector. If $k_{\\mathrm{perturb}} &lt; 0$ or $k_{\\mathrm{perturb}} \\ge n$ or $\\varepsilon = 0$, no perturbation should be applied.\n- Record all $n$ search directions as the columns of a matrix $P \\in \\mathbb{R}^{n \\times n}$ in the order they are used by CG.\n\nFor each run, after exactly $n$ iterations, compute both:\n1. The residual norm after $n$ iterations, defined by $\\lVert b - A x_n \\rVert_2$.\n2. The $A$-orthogonality defect of the search directions, defined as the maximum absolute value of off-diagonal entries of the Gram matrix $G \\equiv P^{\\top} A P$, that is,\n$$\n\\max_{i \\ne j} \\left| G_{ij} \\right|.\n$$\n\nTest suite:\nRun your implementation on the fixed $(A,b)$ above for the following five parameter pairs $(k_{\\mathrm{perturb}}, \\varepsilon)$:\n- Case $1$: $(-1, 0)$.\n- Case $2$: $(2, 1)$.\n- Case $3$: $(0, 1)$.\n- Case $4$: $(10, 1)$.\n- Case $5$: $(3, 10^{-3})$.\n\nFinal output format:\n- Your program must print a single line containing a flat list with $2$ numbers per test case, in order: for each case, first the residual norm after $n$ steps, then the $A$-orthogonality defect. All numbers must be rounded to exactly $8$ decimal places using standard rounding.\n- Concretely, the output must be a single line of the form\n$$\n[\\text{res}_1,\\text{def}_1,\\text{res}_2,\\text{def}_2,\\dots,\\text{res}_5,\\text{def}_5],\n$$\nwhere each $\\text{res}_i$ and $\\text{def}_i$ is a decimal numeral with exactly $8$ digits after the decimal point.", "solution": "The problem requires the implementation of the Conjugate Gradient (CG) method to solve a linear system $Ax=b$ derived from a mean-variance portfolio optimization problem. The central task is to demonstrate the importance of maintaining $A$-orthogonality among the search directions by analyzing the effect of a targeted perturbation.\n\nFirst, we conduct a formal validation of the problem statement.\n\nThe givens are:\n- Objective function: $f(x) \\equiv \\tfrac{1}{2} x^{\\top} \\Sigma x - \\mu^{\\top} x + \\tfrac{\\eta}{2}\\left(\\mathbf{1}^{\\top} x - 1\\right)^2$.\n- Linear system: $A x = b$.\n- System matrix: $A \\equiv \\Sigma + \\eta\\, \\mathbf{1}\\mathbf{1}^{\\top}$.\n- Right-hand side vector: $b \\equiv \\mu + \\eta\\, \\mathbf{1}$.\n- System dimension: $n = 6$.\n- Correlation parameter: $\\rho = 0.2$.\n- Correlation matrix: $R = \\rho \\mathbf{1}\\mathbf{1}^{\\top} + (1-\\rho) I$.\n- Asset standard deviations: $s = [0.15, 0.20, 0.25, 0.30, 0.22, 0.18]^{\\top}$.\n- Covariance matrix: $\\Sigma = \\operatorname{diag}(s)\\, R\\, \\operatorname{diag}(s)$.\n- Expected returns vector: $\\mu = [0.08, 0.10, 0.12, 0.15, 0.11, 0.09]^{\\top}$.\n- Penalty weight: $\\eta = 10^{-2}$.\n- CG starting vector: $x_0 = \\mathbf{0}$.\n- Iteration count: exactly $n=6$.\n- Perturbation rule: For $j = k_{\\mathrm{perturb}}$, replace search direction $p_j$ with $\\tilde{p}_j = p_j + \\varepsilon \\,\\lVert p_j \\rVert_2\\, e_1$. This is active for $k_{\\mathrm{perturb}} \\in \\{0, 1, \\dots, n-1\\}$ and $\\varepsilon > 0$.\n- Metrics: $\\lVert b - A x_n \\rVert_2$ and $\\max_{i \\ne j} \\left| (P^{\\top} A P)_{ij} \\right|$.\n\nThe problem is valid. It is scientifically grounded in established principles of computational finance and numerical linear algebra. The covariance matrix $\\Sigma$ is constructed to be symmetric positive definite (SPD). The system matrix $A = \\Sigma + \\eta \\mathbf{1}\\mathbf{1}^{\\top}$ is the sum of an SPD matrix $\\Sigma$ and a symmetric positive semi-definite matrix $\\eta \\mathbf{1}\\mathbf{1}^{\\top}$ (since $\\eta = 10^{-2} > 0$), which ensures that $A$ is also SPD. Consequently, the linear system $Ax=b$ is well-posed, and the CG method is a suitable and theoretically sound solution approach. All parameters and algorithmic requirements are specified with sufficient precision and are internally consistent.\n\nWe begin by constructing the system components. The dimension is $n=6$. The vectors $\\mu \\in \\mathbb{R}^6$ and $s \\in \\mathbb{R}^6$ are given. The vector $\\mathbf{1}$ is the vector of all ones in $\\mathbb{R}^6$, and $I$ is the $6 \\times 6$ identity matrix.\nThe correlation matrix $R$ is:\n$$ R = (1-0.2)I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} = 0.8 I + 0.2 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\nThe covariance matrix $\\Sigma$ is constructed as:\n$$ \\Sigma = \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) \\cdot R \\cdot \\operatorname{diag}([0.15, 0.20, 0.25, 0.30, 0.22, 0.18]) $$\nWith $\\eta = 0.01$, the system matrix $A$ and vector $b$ are:\n$$ A = \\Sigma + 0.01 \\cdot \\mathbf{1}\\mathbf{1}^{\\top} $$\n$$ b = \\mu + 0.01 \\cdot \\mathbf{1} $$\n\nThe Conjugate Gradient algorithm is an iterative method that solves $Ax=b$ by generating a sequence of search directions $\\{p_j\\}$ which are mutually $A$-orthogonal (or conjugate), i.e., $p_i^{\\top} A p_j = 0$ for $i \\ne j$. The standard algorithm proceeds as follows, starting with $x_0 = \\mathbf{0}$, $r_0 = b$, and $p_0 = r_0$:\nFor $j = 0, 1, \\dots, n-1$:\n1. Compute step size: $\\alpha_j = \\frac{r_j^{\\top} r_j}{p_j^{\\top} A p_j}$\n2. Update solution: $x_{j+1} = x_j + \\alpha_j p_j$\n3. Update residual: $r_{j+1} = r_j - \\alpha_j A p_j$\n4. Compute improvement factor: $\\beta_j = \\frac{r_{j+1}^{\\top} r_{j+1}}{r_j^{\\top} r_j}$\n5. Update search direction: $p_{j+1} = r_{j+1} + \\beta_j p_j$\n\nIn exact arithmetic, this process generates an $A$-orthogonal basis of search directions and finds the exact solution in at most $n$ iterations. The core of this problem is to disrupt this property. At a specified iteration $j=k_{\\text{perturb}}$, the search direction $p_j$ is perturbed:\n$$ \\tilde{p}_j = p_j + \\varepsilon \\lVert p_j \\rVert_2 e_1 $$\nwhere $e_1 = [1, 0, \\dots, 0]^{\\top}$. This perturbed direction $\\tilde{p}_j$ is then used in place of $p_j$ for the updates of $x_{j+1}$ and $r_{j+1}$. Crucially, the subsequent search direction $p_{j+1}$ is built using this perturbed direction:\n$$ p_{j+1} = r_{j+1} + \\beta_j \\tilde{p}_j $$\nThis perturbation breaks the chain of conjugacy. The new direction $\\tilde{p}_j$ is generally not $A$-orthogonal to the preceding directions $p_0, \\dots, p_{j-1}$. This error propagates, as all subsequent directions are built upon this contaminated step. As a result, the set of $n$ executed search directions $\\{p_0, \\dots, \\tilde{p}_j, \\dots, p_{n-1}\\}$ is no longer an $A$-orthogonal basis, and the theoretical guarantee of convergence in $n$ steps is lost.\n\nWe expect to observe:\n1. For unperturbed runs (Cases $1$ and $4$), the final residual norm $\\lVert b - A x_n \\rVert_2$ and the $A$-orthogonality defect $\\max_{i \\ne j} | (P^{\\top} A P)_{ij} |$ will be near machine precision (close to $0$). The matrix $P^{\\top} A P$ will be almost perfectly diagonal.\n2. For perturbed runs (Cases $2$, $3$, and $5$), both metrics will be significantly greater than $0$. This demonstrates that the algorithm failed to converge to the exact solution in $n$ steps and that the underlying conjugacy property of the search directions was destroyed. The magnitude of the error will depend on the perturbation strength $\\varepsilon$ and the iteration index $k_{\\text{perturb}}$.\n\nThe solution is implemented by first defining the system matrices from the problem data. A function is then created to execute the CG algorithm. This function incorporates the logic to perturb the specified search direction and stores all used directions. After exactly $n=6$ iterations, it computes the final residual norm and the $A$-orthogonality defect from the matrix of search directions. This process is repeated for each of the five test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Sets up the portfolio optimization problem, runs the Conjugate Gradient (CG)\n    method for several test cases with and without perturbation, and\n    calculates the specified performance metrics.\n    \"\"\"\n    # Problem Constants and Data\n    n = 6\n    rho = 0.2\n    s = np.array([0.15, 0.20, 0.25, 0.30, 0.22, 0.18])\n    mu = np.array([0.08, 0.10, 0.12, 0.15, 0.11, 0.09])\n    eta = 1e-2\n\n    # Construct the linear system Ax = b\n    ones = np.ones((n, 1))\n    R = rho * (ones @ ones.T) + (1 - rho) * np.identity(n)\n    D = np.diag(s)\n    Sigma = D @ R @ D\n    A = Sigma + eta * (ones @ ones.T)\n    b = mu + eta * ones.flatten()\n\n    def run_cg_perturbed(A_mat, b_vec, k_perturb, epsilon, n_iter):\n        \"\"\"\n        Runs the Conjugate Gradient algorithm for n_iter steps with an\n        optional perturbation on a specified search direction.\n        \n        Args:\n            A_mat (np.ndarray): The system matrix (n x n).\n            b_vec (np.ndarray): The right-hand side vector (n,).\n            k_perturb (int): The index of the iteration to perturb.\n            epsilon (float): The magnitude of the perturbation.\n            n_iter (int): The number of iterations to run.\n\n        Returns:\n            tuple: A tuple containing:\n                - residual_norm (float): The L2 norm of the final residual.\n                - defect (float): The A-orthogonality defect.\n        \"\"\"\n        dim = A_mat.shape[0]\n        x = np.zeros(dim)\n        r = b_vec - A_mat @ x\n        p = r\n        rs_old = r.T @ r\n        \n        p_storage = []\n        e1 = np.zeros(dim)\n        e1[0] = 1.0\n\n        for j in range(n_iter):\n            p_effective = p\n            \n            # Apply perturbation if conditions are met\n            if j == k_perturb and epsilon > 0:\n                p_norm = np.linalg.norm(p)\n                perturbation = epsilon * p_norm * e1\n                p_effective = p + perturbation\n\n            p_storage.append(p_effective)\n            \n            Ap = A_mat @ p_effective\n            alpha = rs_old / (p_effective.T @ Ap)\n            \n            x = x + alpha * p_effective\n            r = r - alpha * Ap\n            \n            rs_new = r.T @ r\n            \n            beta = rs_new / rs_old\n            p = r + beta * p_effective\n            rs_old = rs_new\n\n        # 1. Calculate final residual norm\n        final_residual_norm = np.linalg.norm(b_vec - A_mat @ x)\n        \n        # 2. Calculate A-orthogonality defect\n        P = np.array(p_storage).T\n        G = P.T @ A_mat @ P\n        np.fill_diagonal(G, 0.0) # We only care about off-diagonal elements\n        orthogonality_defect = np.max(np.abs(G))\n        \n        return final_residual_norm, orthogonality_defect\n\n    test_cases = [\n        (-1, 0.0),    # Case 1: No perturbation (invalid index)\n        (2, 1.0),     # Case 2: Perturb p_2 with eps=1\n        (0, 1.0),     # Case 3: Perturb p_0 with eps=1\n        (10, 1.0),    # Case 4: No perturbation (index out of bounds)\n        (3, 1e-3),    # Case 5: Perturb p_3 with small eps\n    ]\n\n    results = []\n    for k_perturb, epsilon in test_cases:\n        res_norm, defect = run_cg_perturbed(A, b, k_perturb, epsilon, n)\n        results.append(f\"{res_norm:.8f}\")\n        results.append(f\"{defect:.8f}\")\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}]}