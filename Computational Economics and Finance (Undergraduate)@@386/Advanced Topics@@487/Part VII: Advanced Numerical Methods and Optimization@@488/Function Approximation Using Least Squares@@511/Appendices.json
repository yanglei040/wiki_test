{"hands_on_practices": [{"introduction": "We begin our hands-on practice with a foundational exercise that demonstrates the complete workflow of empirical modeling. This practice involves approximating the famous Laffer curve, which postulates a relationship between tax rates and tax revenue. By working through this problem [@problem_id:2395010], you will gain practical experience in fitting polynomial models using Ordinary Least Squares (OLS), employing cross-validation to select the optimal model complexity, and ultimately using your fitted model to answer a core economic question: what tax rate maximizes revenue?", "id": "2395010", "problem": "Consider an economy where the government collects tax revenue as a function of the average tax rate. Let the tax rate be denoted by $t \\in [0,1]$ (expressed as a decimal, not a percentage), and suppose we observe noisy samples of tax revenue at different tax rates generated by a structural form. Your task is to approximate the unknown Laffer curve (tax revenue as a function of the tax rate) by fitting either a quadratic or cubic polynomial using Ordinary Least Squares (OLS), and then use the fitted approximation to compute the revenue-maximizing tax rate on the unit interval.\n\nStart from the following fundamental base:\n- Given sample pairs $\\{(t_i, y_i)\\}_{i=1}^n$, a polynomial of degree $d$ is $p_d(t) = \\sum_{j=0}^{d} \\beta_j t^j$. Ordinary Least Squares (OLS) estimates coefficients by minimizing the sum of squared residuals $\\sum_{i=1}^{n} (y_i - p_d(t_i))^2$.\n- Model selection can be performed by $K$-fold cross-validation that evaluates out-of-sample prediction error and chooses the degree $d$ with the lowest average validation error.\n\nYour program must:\n1. For each test case, simulate data $(t_i, y_i)$ as follows. Draw $n$ independent tax rates $t_i \\sim \\text{Uniform}[0,1]$. Generate noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$. Define the true revenue function by $R_{\\text{true}}(t) = A \\cdot t \\cdot (1 - t)^k$. Then set $y_i = R_{\\text{true}}(t_i) + \\varepsilon_i$. All randomness must be seeded deterministically as specified in the test suite.\n2. For each candidate degree $d \\in \\{2,3\\}$, fit $p_d(t)$ by OLS using the simulated data.\n3. Use $K$-fold cross-validation with $K=5$ folds (randomly shuffled using the given seed) to select the degree $d$ with the lowest average validation mean squared error. In case of a tie, prefer the lower degree.\n4. Refit the chosen degree on the full dataset to obtain final coefficients $\\hat{\\beta}_j$.\n5. Compute the estimated revenue-maximizing tax rate $\\hat{t}^\\star$ by maximizing the fitted polynomial $p_d(t)$ over the closed interval $[0,1]$. That is, find all real critical points by solving $p_d'(t)=0$ that lie in $[0,1]$, evaluate $p_d(t)$ at these points and at the endpoints $t=0$ and $t=1$, and set $\\hat{t}^\\star$ equal to the maximizer. Then compute the estimated maximum revenue $\\hat{R}^\\star = p_d(\\hat{t}^\\star)$.\n6. Angles do not appear in this problem. Express all tax rates as decimals in $[0,1]$ and all revenues as real numbers without percentage signs.\n\nUse the following test suite. For each case, use the provided seed for all random operations (both data generation and fold shuffling), draw $n$ points, use noise standard deviation $\\sigma$, and parameters $A$ and $k$ for the data-generating process $R_{\\text{true}}(t) = A \\cdot t \\cdot (1 - t)^k$:\n- Case $1$: seed $42$, $n=40$, $\\sigma=0.005$, $A=1.0$, $k=1$.\n- Case $2$: seed $123$, $n=50$, $\\sigma=0.01$, $A=1.0$, $k=2$.\n- Case $3$: seed $7$, $n=25$, $\\sigma=0.05$, $A=1.0$, $k=2$.\n\nFinal output format:\n- Your program should produce a single line of output containing a Python-style list of three sublists, one per test case, in the same order as above. Each sublist must be of the form $[d,\\ \\hat{t}^\\star,\\ \\hat{R}^\\star]$, where $d$ is the chosen degree (an integer in $\\{2,3\\}$), and $\\hat{t}^\\star$ and $\\hat{R}^\\star$ are rounded to exactly six decimal places. For example: $[[2,0.500000,0.250000],[3,0.333333,0.197531],[2,0.480000,0.210000]]$.", "solution": "The problem is subjected to validation against the specified criteria.\n\n**Step 1: Extract Givens**\n- **Domain**: Tax rate $t \\in [0,1]$.\n- **Data**: Sample pairs $\\{(t_i, y_i)\\}_{i=1}^n$.\n- **Model**: Polynomial of degree $d$, $p_d(t) = \\sum_{j=0}^{d} \\beta_j t^j$.\n- **Estimation Method**: Ordinary Least Squares (OLS) by minimizing the sum of squared residuals $\\sum_{i=1}^{n} (y_i - p_d(t_i))^2$.\n- **Model Selection**: $K$-fold cross-validation with $K=5$. The degree $d$ with the lowest average validation error is chosen. Candidate degrees are $d \\in \\{2,3\\}$. Ties are broken by choosing the lower degree.\n- **Data Generating Process**:\n    - $t_i \\sim \\text{Uniform}[0,1]$.\n    - Noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n    - True revenue function $R_{\\text{true}}(t) = A \\cdot t \\cdot (1 - t)^k$.\n    - Observed revenue sample $y_i = R_{\\text{true}}(t_i) + \\varepsilon_i$.\n- **Optimization**: Find the revenue-maximizing tax rate $\\hat{t}^\\star$ by maximizing the fitted polynomial $p_d(t)$ over the interval $[0,1]$. The estimated maximum revenue is $\\hat{R}^\\star = p_d(\\hat{t}^\\star)$.\n- **Test Cases**:\n    - Case $1$: seed=$42$, $n=40$, $\\sigma=0.005$, $A=1.0$, $k=1$.\n    - Case $2$: seed=$123$, $n=50$, $\\sigma=0.01$, $A=1.0$, $k=2$.\n    - Case $3$: seed=$7$, $n=25$, $\\sigma=0.05$, $A=1.0$, $k=2$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is scientifically grounded. It presents a standard task in econometrics and statistics: function approximation from noisy data. The use of polynomial regression, Ordinary Least Squares, and cross-validation for model selection are fundamental and well-established methods. The Laffer curve is a valid concept in economics, and the chosen functional form is a mathematically tractable model.\n- **Well-Posedness**: The problem is well-posed. All necessary parameters, data generation processes, and algorithmic steps are explicitly defined. The objectives are clear and lead to a unique computational result for each test case, given the deterministic seeds.\n- **Objectivity**: The problem statement is objective, using precise mathematical and statistical language, and is free of subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-defined computational exercise in applied statistics. A solution will be provided.\n\n---\n\nThe problem requires us to approximate an unknown function, representing the tax revenue as a function of the tax rate, from a set of noisy observations. We will use polynomial regression and select the optimal polynomial degree from a candidate set $\\{2, 3\\}$ using $K$-fold cross-validation. Subsequently, we will find the tax rate that maximizes the estimated revenue function over the valid domain $[0,1]$.\n\n**1. Data Generation**\nFor each test case, we are given a seed for the pseudo-random number generator, the number of samples $n$, the noise standard deviation $\\sigma$, and parameters $A$ and $k$ for the true revenue function. The data points $(t_i, y_i)$ for $i=1, \\dots, n$ are generated as follows:\n- The tax rates $t_i$ are drawn from a uniform distribution on the interval $[0, 1]$, i.e., $t_i \\sim \\text{U}[0, 1]$.\n- The corresponding true revenue is calculated using $R_{\\text{true}}(t_i) = A \\cdot t_i \\cdot (1 - t_i)^k$.\n- A noise term $\\varepsilon_i$ is drawn from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$.\n- The observed revenue is $y_i = R_{\\text{true}}(t_i) + \\varepsilon_i$.\n\n**2. Polynomial Regression via Ordinary Least Squares (OLS)**\nWe seek to approximate the true function with a polynomial of degree $d$, $p_d(t) = \\sum_{j=0}^{d} \\beta_j t^j$. The coefficients $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1, \\dots, \\beta_d]^T$ are estimated by minimizing the sum of squared residuals (SSR):\n$$\n\\text{SSR}(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} (y_i - p_d(t_i))^2\n$$\nThis is a linear least squares problem. Let $\\mathbf{y} = [y_1, \\dots, y_n]^T$ be the vector of observed revenues. Let $\\mathbf{X}$ be the $n \\times (d+1)$ design matrix, which is a Vandermonde matrix for polynomial fitting:\n$$\n\\mathbf{X} =\n\\begin{pmatrix}\n1 & t_1 & t_1^2 & \\dots & t_1^d \\\\\n1 & t_2 & t_2^2 & \\dots & t_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & t_n & t_n^2 & \\dots & t_n^d\n\\end{pmatrix}\n$$\nThe problem is to minimize $||\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}||_2^2$. The solution $\\hat{\\boldsymbol{\\beta}}$ is given by the normal equations:\n$$\n(\\mathbf{X}^T \\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T \\mathbf{y}\n$$\nAssuming $\\mathbf{X}^T \\mathbf{X}$ is invertible, the OLS estimate is $\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$. Computationally, this is solved efficiently using methods like QR decomposition, which is what numerical libraries typically implement.\n\n**3. Model Selection via $K$-Fold Cross-Validation**\nTo choose between a quadratic ($d=2$) and a cubic ($d=3$) model, we use $K$-fold cross-validation with $K=5$. This technique provides an estimate of the model's out-of-sample prediction error, helping to prevent overfitting.\nThe procedure is as follows for each candidate degree $d$:\n1. The dataset of $n$ samples is randomly shuffled and partitioned into $K=5$ disjoint subsets (folds) of nearly equal size.\n2. For each fold $k \\in \\{1, 2, 3, 4, 5\\}$:\n   a. The $k$-th fold is designated as the validation set. The remaining $K-1$ folds are combined to form the training set.\n   b. A polynomial of degree $d$ is fitted using OLS on the training set, yielding coefficients $\\hat{\\boldsymbol{\\beta}}^{(k)}$.\n   c. The mean squared error (MSE) is computed on the validation set: $MSE_k = \\frac{1}{|N_k|} \\sum_{i \\in \\text{fold } k} (y_i - p_d(t_i; \\hat{\\boldsymbol{\\beta}}^{(k)}))^2$, where $|N_k|$ is the number of samples in fold $k$.\n3. The cross-validation score for degree $d$ is the average of these MSEs: $CV(d) = \\frac{1}{K} \\sum_{k=1}^{K} MSE_k$.\nThe optimal degree, $d^\\star$, is chosen to be the one that minimizes this score: $d^\\star = \\arg\\min_{d \\in \\{2,3\\}} CV(d)$. If $CV(2) = CV(3)$, we select the simpler model, $d^\\star=2$.\n\n**4. Optimization of the Chosen Model**\nOnce the optimal degree $d^\\star$ is selected, the polynomial model is re-fitted using the entire dataset to obtain the final coefficient vector $\\hat{\\boldsymbol{\\beta}}$. We denote the final estimated revenue function as $p_{d^\\star}(t)$.\n\nThe next step is to find the tax rate $\\hat{t}^\\star$ that maximizes this function on the closed interval $[0, 1]$. By the Extreme Value Theorem, a continuous function on a closed interval must attain its maximum and minimum. The maximizer must be either at the endpoints of the interval ($t=0$ or $t=1$) or at an interior critical point where the derivative $p'_{d^\\star}(t)$ is zero.\n\nThe derivative of the polynomial is $p'_{d^\\star}(t) = \\sum_{j=1}^{d^\\star} j \\hat{\\beta}_j t^{j-1}$.\n- If $d^\\star=2$, the derivative is $p'_2(t) = \\hat{\\beta}_1 + 2\\hat{\\beta}_2 t$. Setting this to zero gives one critical point: $t_c = -\\frac{\\hat{\\beta}_1}{2\\hat{\\beta}_2}$.\n- If $d^\\star=3$, the derivative is $p'_3(t) = \\hat{\\beta}_1 + 2\\hat{\\beta}_2 t + 3\\hat{\\beta}_3 t^2$. This is a quadratic equation. Its roots can be found using the quadratic formula, yielding up to two real critical points.\n\nThe set of candidate values for the maximizer $\\hat{t}^\\star$ consists of the endpoints $\\{0, 1\\}$ and any real critical points that fall within the interval $[0, 1]$. We evaluate $p_{d^\\star}(t)$ at each of these candidate points. The value of $t$ that yields the highest revenue is the estimated optimal tax rate, $\\hat{t}^\\star$.\n$$\n\\hat{t}^\\star = \\arg\\max_{t \\in \\text{candidates}} p_{d^\\star}(t)\n$$\nThe estimated maximum revenue is then $\\hat{R}^\\star = p_{d^\\star}(\\hat{t}^\\star)$. This procedure is followed for each test case to produce the final results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Laffer curve approximation problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (seed, n, sigma, A, k)\n        (42, 40, 0.005, 1.0, 1),\n        (123, 50, 0.01, 1.0, 2),\n        (7, 25, 0.05, 1.0, 2),\n    ]\n\n    all_results = []\n    \n    for seed, n, sigma, A, k_param in test_cases:\n        # Set all sources of randomness for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        t = rng.uniform(0, 1, n)\n        true_revenue = A * t * (1 - t)**k_param\n        noise = rng.normal(0, sigma, n)\n        y = true_revenue + noise\n\n        # 2. & 3. Model selection using 5-fold cross-validation\n        K = 5\n        indices = np.arange(n)\n        rng.shuffle(indices)\n        folds = np.array_split(indices, K)\n        \n        candidate_degrees = [2, 3]\n        cv_errors = {}\n\n        for d in candidate_degrees:\n            fold_mses = []\n            for k_fold_idx in range(K):\n                val_indices = folds[k_fold_idx]\n                train_indices = np.concatenate([folds[i] for i in range(K) if i != k_fold_idx])\n\n                t_train, y_train = t[train_indices], y[train_indices]\n                t_val, y_val = t[val_indices], y[val_indices]\n\n                # Fit model on training data\n                X_train = np.vander(t_train, d + 1, increasing=True)\n                coeffs, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=None)\n\n                # Evaluate on validation data\n                X_val = np.vander(t_val, d + 1, increasing=True)\n                y_pred_val = X_val @ coeffs\n                mse = np.mean((y_val - y_pred_val)**2)\n                fold_mses.append(mse)\n            \n            cv_errors[d] = np.mean(fold_mses)\n\n        # Select the best degree, preferring the lower degree in case of a tie\n        if cv_errors[2] <= cv_errors[3]:\n            d_star = 2\n        else:\n            d_star = 3\n\n        # 4. Refit the chosen model on the full dataset\n        X_full = np.vander(t, d_star + 1, increasing=True)\n        # beta has shape (d_star + 1,) where beta[j] is the coefficient for t^j\n        beta, _, _, _ = np.linalg.lstsq(X_full, y, rcond=None)\n\n        # 5. Compute the estimated revenue-maximizing tax rate\n        \n        # Derivative coefficients: p'(t) = sum_{j=1 to d} j * beta[j] * t^(j-1)\n        # np.roots expects coefficients in descending power order\n        # For p'(t) = c_0*t^(d-1) + ... + c_{d-1}, coeffs are [c_0, ..., c_{d-1}]\n        # c_m = (m+1)*beta[m+1] in our notation.\n        # So for np.roots, we need [(d_star)*beta[d_star], (d_star-1)*beta[d_star-1], ..., 1*beta[1]]\n        deriv_coeffs = [j * beta[j] for j in range(d_star, 0, -1)]\n        critical_points = np.roots(deriv_coeffs)\n        \n        # Candidate points for the maximum on [0, 1]\n        candidate_t = [0.0, 1.0]\n        for root in critical_points:\n            # Filter for real roots within the interval [0, 1]\n            if np.isreal(root) and 0.0 <= root.real <= 1.0:\n                candidate_t.append(root.real)\n        \n        candidate_t = np.unique(candidate_t)\n\n        # Evaluate the polynomial at candidate points to find the maximum\n        # np.polyval expects coefficients in descending power order [beta_d, ..., beta_0]\n        poly_coeffs_desc = beta[::-1]\n        candidate_R = np.polyval(poly_coeffs_desc, candidate_t)\n        \n        max_idx = np.argmax(candidate_R)\n        t_star = candidate_t[max_idx]\n        R_star = candidate_R[max_idx]\n\n        all_results.append(\n            f\"[{d_star},{t_star:.6f},{R_star:.6f}]\"\n        )\n        \n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"}, {"introduction": "Having seen how to build a model with OLS, it is equally important to understand when the method might fail. This next practice [@problem_id:2395000] confronts a classic econometric pitfall: estimating a demand curve using observed market prices and quantities. You will discover firsthand why naively applying OLS in this setting can produce biased estimates, introducing you to the critical concept of endogeneity. Recognizing this issue is a vital step toward becoming a discerning analyst who understands the assumptions behind the tools they use.", "id": "2395000", "problem": "A competitive market for a homogeneous good is observed over $5$ periods. For each period $t \\in \\{1,2,3,4,5\\}$, you observe the equilibrium price $P_t$ (in currency units per unit) and quantity $Q_t$ (in thousands of units). You wish to approximate the demand curve $Q(P)$ by the linear function $Q(P) = \\alpha + \\beta P$ using Ordinary Least Squares (OLS), which minimizes the sum of squared residuals over the sample. The observed data are:\n$\\{(P_t,Q_t)\\}_{t=1}^5 = \\{(2,118),(4,110),(6,97),(8,89),(10,78)\\}$.\nWhich option correctly reports the OLS estimates of $(\\alpha,\\beta)$ for the linear demand curve based on these data and gives a valid economic-statistical reason why OLS is likely to be biased when estimated from equilibrium market data?\n\nA. $\\hat{\\alpha} = 128.7$, $\\hat{\\beta} = -5.05$; OLS is likely biased because price is endogenous in market equilibrium: unobserved demand shocks that shift $Q$ also affect the market-clearing $P$, inducing correlation between $P$ and the regression error. An exogenous cost shifter could serve as an instrument to address this.\n\nB. $\\hat{\\alpha} = 98.4$, $\\hat{\\beta} = -5.05$; OLS is unbiased because the Gauss–Markov conditions only require the error to have mean zero, regardless of its relationship with $P$.\n\nC. $\\hat{\\alpha} = 128.7$, $\\hat{\\beta} = -5.05$; OLS is likely unbiased because the price $P$ is chosen by the econometrician and therefore cannot be correlated with the regression error.\n\nD. $\\hat{\\alpha} = 128.7$, $\\hat{\\beta} = -4.00$; OLS is biased only if there is measurement error in $Q$, and the bias vanishes if $Q$ is measured perfectly.", "solution": "The problem statement is subjected to validation before any solution is attempted.\n\n### Step 1: Extract Givens\n- The context is a competitive market for a homogeneous good.\n- The observation period consists of $n=5$ periods, indexed by $t \\in \\{1,2,3,4,5\\}$.\n- For each period, the observed data are the equilibrium price $P_t$ and equilibrium quantity $Q_t$.\n- The data set is $\\{(P_t,Q_t)\\}_{t=1}^5 = \\{(2,118),(4,110),(6,97),(8,89),(10,78)\\}$.\n- The objective is to approximate the demand curve $Q(P)$ using a linear function $Q(P) = \\alpha + \\beta P$.\n- The estimation method is Ordinary Least Squares (OLS), minimizing the sum of squared residuals.\n- The secondary objective is to provide a valid economic-statistical reason for potential bias in the OLS estimates.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is grounded in standard microeconomic theory (supply and demand equilibrium) and econometric methodology (OLS estimation of a demand function). The concept of simultaneity bias in this context is a canonical topic in econometrics. The problem is scientifically and factually sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is a standard, formalizable quantitative exercise in econometrics. It is directly relevant to the topic of function approximation using least squares in an applied economic context.\n3.  **Incomplete or Contradictory Setup**: The problem provides all necessary data and methodological specifications to calculate the OLS estimates. There are $n=5$ observations and $k=2$ parameters to estimate. As the values of the regressor $P_t$ are not all identical, a unique OLS solution exists. The setup is complete and consistent.\n4.  **Unrealistic or Infeasible**: The data are numerical values and the market scenario is a standard textbook case. There are no unrealistic or infeasible conditions.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed and clearly structured. It asks for a specific calculation and a specific piece of theoretical reasoning. The terms are standard and unambiguous.\n6.  **Outside Scientific Verifiability**: The calculations are verifiable through arithmetic, and the economic reasoning is verifiable against established econometric principles.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A solution will be derived.\n\n### Derivation of Solution\nThe linear regression model is specified as $Q_t = \\alpha + \\beta P_t + \\epsilon_t$, where $\\epsilon_t$ is the error term. The OLS estimators for $\\beta$ and $\\alpha$ are given by:\n$$ \\hat{\\beta} = \\frac{\\sum_{t=1}^n (P_t - \\bar{P})(Q_t - \\bar{Q})}{\\sum_{t=1}^n (P_t - \\bar{P})^2} $$\n$$ \\hat{\\alpha} = \\bar{Q} - \\hat{\\beta} \\bar{P} $$\nwhere $n=5$ is the number of observations, $\\bar{P}$ is the sample mean of prices, and $\\bar{Q}$ is the sample mean of quantities.\n\nFirst, we compute the necessary summary statistics from the data $\\{(2,118),(4,110),(6,97),(8,89),(10,78)\\}$.\n\n1.  **Calculate sample means:**\n    $$ \\sum_{t=1}^5 P_t = 2 + 4 + 6 + 8 + 10 = 30 $$\n    $$ \\bar{P} = \\frac{30}{5} = 6 $$\n    $$ \\sum_{t=1}^5 Q_t = 118 + 110 + 97 + 89 + 78 = 492 $$\n    $$ \\bar{Q} = \\frac{492}{5} = 98.4 $$\n\n2.  **Calculate sum of squared deviations for $P_t$:**\n    $$ \\sum_{t=1}^5 (P_t - \\bar{P})^2 = (2-6)^2 + (4-6)^2 + (6-6)^2 + (8-6)^2 + (10-6)^2 $$\n    $$ = (-4)^2 + (-2)^2 + 0^2 + 2^2 + 4^2 = 16 + 4 + 0 + 4 + 16 = 40 $$\n\n3.  **Calculate sum of cross-products of deviations:**\n    $$ \\sum_{t=1}^5 (P_t - \\bar{P})(Q_t - \\bar{Q}) = (2-6)(118-98.4) + (4-6)(110-98.4) + (6-6)(97-98.4) + (8-6)(89-98.4) + (10-6)(78-98.4) $$\n    $$ = (-4)(19.6) + (-2)(11.6) + (0)(-1.4) + (2)(-9.4) + (4)(-20.4) $$\n    $$ = -78.4 - 23.2 - 0 - 18.8 - 81.6 = -202 $$\n\n4.  **Calculate the OLS estimates $\\hat{\\beta}$ and $\\hat{\\alpha}$:**\n    $$ \\hat{\\beta} = \\frac{-202}{40} = -5.05 $$\n    $$ \\hat{\\alpha} = \\bar{Q} - \\hat{\\beta} \\bar{P} = 98.4 - (-5.05)(6) = 98.4 + 30.3 = 128.7 $$\n\nThe OLS estimates are $\\hat{\\alpha} = 128.7$ and $\\hat{\\beta} = -5.05$.\n\nNext, we evaluate the reasoning for bias. The data represent market *equilibria*. In a competitive market, the equilibrium price $P_t$ and quantity $Q_t$ are determined simultaneously by the intersection of the supply and demand curves.\nLet the structural model be:\n- Demand: $Q_t^d = \\alpha + \\beta P_t + u_t$\n- Supply: $Q_t^s = \\gamma + \\delta P_t + v_t$\n- Equilibrium: $Q_t^d = Q_t^s = Q_t$\n\nThe term $u_t$ represents unobserved demand shocks (e.g., changes in consumer tastes, income), while $v_t$ represents unobserved supply shocks (e.g., changes in input costs, technology).\nWhen we estimate the demand equation $Q_t = \\alpha + \\beta P_t + \\epsilon_t$ using OLS, the regression error $\\epsilon_t$ is identical to the demand shock $u_t$.\nHowever, the equilibrium price $P_t$ is determined by setting demand equal to supply:\n$$ \\alpha + \\beta P_t + u_t = \\gamma + \\delta P_t + v_t $$\nSolving for $P_t$ yields:\n$$ P_t = \\frac{\\gamma - \\alpha}{\\beta - \\delta} + \\frac{v_t - u_t}{\\beta - \\delta} $$\nThis expression demonstrates that the regressor $P_t$ is a function of the error term $u_t$. This violates the OLS exogeneity assumption, which requires that the regressor be uncorrelated with the error term, i.e., $\\text{Cov}(P_t, u_t)=0$. Here, assuming $\\text{Cov}(u_t, v_t)=0$:\n$$ \\text{Cov}(P_t, u_t) = \\text{Cov}\\left(\\frac{v_t - u_t}{\\beta - \\delta}, u_t\\right) = \\frac{-\\text{Var}(u_t)}{\\beta - \\delta} $$\nSince $\\text{Var}(u_t) > 0$ and we expect $\\beta < 0$ (demand slope) and $\\delta > 0$ (supply slope), the denominator $\\beta - \\delta$ is negative. Thus, $\\text{Cov}(P_t, u_t) > 0$. This correlation between the regressor and the error term leads to biased and inconsistent OLS estimators. This problem is known as simultaneity bias or endogeneity bias. Price is an endogenous variable. To obtain a consistent estimate, one must use a method such as instrumental variables (IV), employing an instrument (e.g., an exogenous cost shifter) that is correlated with price but uncorrelated with the demand shock.\n\n### Option-by-Option Analysis\n\n**A. $\\hat{\\alpha} = 128.7$, $\\hat{\\beta} = -5.05$; OLS is likely biased because price is endogenous in market equilibrium: unobserved demand shocks that shift $Q$ also affect the market-clearing $P$, inducing correlation between $P$ and the regression error. An exogenous cost shifter could serve as an instrument to address this.**\nThe calculated estimates for $\\hat{\\alpha}$ and $\\hat{\\beta}$ are correct. The reasoning provided is a precise and correct explanation of simultaneity bias in this context. It correctly identifies price as endogenous, the role of unobserved shocks in a simultaneous equations system, and the resulting correlation between the regressor and the error term. The suggested remedy, using an instrumental variable such as a cost shifter, is the standard econometric approach.\n**Verdict: Correct.**\n\n**B. $\\hat{\\alpha} = 98.4$, $\\hat{\\beta} = -5.05$; OLS is unbiased because the Gauss–Markov conditions only require the error to have mean zero, regardless of its relationship with $P$.**\nThe value $\\hat{\\alpha} = 98.4$ is incorrect; it is the value of $\\bar{Q}$. The reasoning is fundamentally flawed. A critical Gauss-Markov assumption for unbiasedness is the exogeneity of regressors, $E[\\epsilon_t | P_t] = 0$, which implies $\\text{Cov}(P_t, \\epsilon_t) = 0$. The statement that the relationship between the error and the regressor is irrelevant is false and demonstrates a grave misunderstanding of econometric principles.\n**Verdict: Incorrect.**\n\n**C. $\\hat{\\alpha} = 128.7$, $\\hat{\\beta} = -5.05$; OLS is likely unbiased because the price $P$ is chosen by the econometrician and therefore cannot be correlated with the regression error.**\nThe numerical estimates are correct. However, the reasoning is factually inconsistent with the problem statement. The problem specifies that the data are *observed* from a *market equilibrium*, not generated in a controlled experiment where the econometrician sets the price. In an observational setting, price is determined by market forces and is endogenous, as explained above.\n**Verdict: Incorrect.**\n\n**D. $\\hat{\\alpha} = 128.7$, $\\hat{\\beta} = -4.00$; OLS is biased only if there is measurement error in $Q$, and the bias vanishes if $Q$ is measured perfectly.**\nThe value $\\hat{\\beta} = -4.00$ is incorrect. The reasoning is also incorrect. First, simultaneity causes bias even with perfectly measured data. Second, the claim that bias arises *only* from measurement error in $Q$ is false. Third, under the standard assumption of classical errors-in-variables, measurement error in the dependent variable ($Q$) increases the variance of the estimators but does not cause bias, whereas measurement error in the regressor ($P$) does cause bias (attenuation bias). The reasoning is confused and incorrect on multiple points.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$"}, {"introduction": "Our final practice advances to a sophisticated application in modern quantitative finance. We will approximate the complex Black-Scholes option pricing function, $C(S)$, using a much simpler polynomial, a common task when the true function is computationally expensive to evaluate. The true power of this technique [@problem_id:2394969] is revealed in the next step: we will differentiate our polynomial approximation to estimate the option's risk sensitivities, known as \"Greeks\" like Delta ($\\Delta = \\frac{\\partial C}{\\partial S}$) and Gamma ($\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}$). This method of \"differentiation via approximation\" is an essential tool for risk management and a prime example of how numerical methods empower financial analysis.", "id": "2394969", "problem": "You are given the task of approximating the price function of a European call option as a univariate function of the underlying asset level, and then using the approximation to compute sensitivities (\"Greeks\") by differentiation. For a European call with underlying level $S$, strike $K$, continuously compounded risk-free rate $r$ (expressed as a decimal), volatility $\\sigma$ (expressed as a decimal), and time to maturity $T$ (in years), the price under the Black–Scholes model is\n$$\nC(S) \\;=\\; S\\,\\Phi(d_1) \\;-\\; K\\,e^{-r T}\\,\\Phi(d_2),\n$$\nwhere\n$$\nd_1 \\;=\\; \\frac{\\ln(S/K) + \\left(r + \\tfrac{1}{2}\\sigma^2\\right) T}{\\sigma\\sqrt{T}}, \n\\qquad\nd_2 \\;=\\; d_1 \\;-\\; \\sigma\\sqrt{T},\n$$\n$\\Phi(\\cdot)$ is the cumulative distribution function of the standard normal distribution, and $\\ln(\\cdot)$ denotes the natural logarithm. The option Delta and Gamma are, respectively, the first and second partial derivatives of the price with respect to $S$:\n$$\n\\Delta(S) \\;=\\; \\frac{\\partial C}{\\partial S}(S), \n\\qquad\n\\Gamma(S) \\;=\\; \\frac{\\partial^2 C}{\\partial S^2}(S).\n$$\nUnder the Black–Scholes model, the analytical formulas are\n$$\n\\Delta_{\\mathrm{BS}}(S) \\;=\\; \\Phi(d_1), \n\\qquad\n\\Gamma_{\\mathrm{BS}}(S) \\;=\\; \\frac{\\phi(d_1)}{S\\,\\sigma\\,\\sqrt{T}},\n$$\nwhere $\\phi(\\cdot)$ is the standard normal probability density function.\n\nYour task is to, for each specified parameter set, construct a least-squares polynomial approximation of degree $m$ to the pricing function $C(S)$ using sample points on a prescribed interval for $S$, then compute the approximated Delta and Gamma at a specified evaluation point by differentiating the polynomial once and twice with respect to $S$, and finally report the absolute errors of these approximations relative to the Black–Scholes analytical Delta and Gamma.\n\nFor each test case:\n- Construct the equispaced grid $\\{S_i\\}_{i=1}^n$ on the closed interval $[S_{\\min}, S_{\\max}]$ with $n$ points.\n- Evaluate the exact price $y_i = C(S_i)$ at each grid point using the Black–Scholes formula above.\n- Let $\\mathcal{P}_m$ denote the set of polynomials in $S$ of degree at most $m$. Compute the least-squares polynomial approximant $\\hat{C}_m \\in \\mathcal{P}_m$ that minimizes $\\sum_{i=1}^n \\left(y_i - \\hat{C}_m(S_i)\\right)^2$ over the coefficients of $\\hat{C}_m$.\n- Differentiate $\\hat{C}_m$ with respect to $S$ to obtain $\\widehat{\\Delta}_m(S) = \\frac{\\mathrm{d}}{\\mathrm{d}S}\\hat{C}_m(S)$ and $\\widehat{\\Gamma}_m(S) = \\frac{\\mathrm{d}^2}{\\mathrm{d}S^2}\\hat{C}_m(S)$.\n- At the evaluation point $S_0$, compute the absolute errors $|\\widehat{\\Delta}_m(S_0) - \\Delta_{\\mathrm{BS}}(S_0)|$ and $|\\widehat{\\Gamma}_m(S_0) - \\Gamma_{\\mathrm{BS}}(S_0)|$.\n\nDesign details:\n- All rates and volatilities must be treated as decimals (for example, an annualized rate of $2\\%$ is entered and used as $0.02$).\n- No physical units are involved. There are no angles in this problem. Do not express any answers using a percentage sign.\n- The final output for the program must be a single line containing a comma-separated list of results enclosed in square brackets. Each test case contributes one inner list of two floats, in the order $[\\text{DeltaError}, \\text{GammaError}]$, each rounded to six decimal places. For example, an output with two test cases should look like $[[0.000123,0.045678],[0.000010,0.000200]]$.\n\nTest suite (each bullet specifies $(K, r, \\sigma, T, S_{\\min}, S_{\\max}, n, m, S_0)$):\n- Case A: $(100,\\, 0.02,\\, 0.2,\\, 0.5,\\, 50,\\, 150,\\, 101,\\, 5,\\, 100)$.\n- Case B: $(100,\\, 0.01,\\, 0.25,\\, 1.0,\\, 80,\\, 200,\\, 121,\\, 6,\\, 180)$.\n- Case C: $(100,\\, 0.03,\\, 0.05,\\, 1.0,\\, 80,\\, 120,\\, 81,\\, 5,\\, 100)$.\n- Case D: $(100,\\, 0.00,\\, 0.3,\\, 0.01,\\, 80,\\, 120,\\, 81,\\, 5,\\, 100)$.\n- Case E: $(120,\\, 0.02,\\, 0.2,\\, 1.0,\\, 60,\\, 120,\\, 121,\\, 5,\\, 60)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the inner list $[\\text{DeltaError}, \\text{GammaError}]$ for the corresponding test case in the order A through E, with each float rounded to six decimal places.", "solution": "The problem as stated has been subjected to rigorous validation. It requires the numerical approximation of a European call option's price function and its first two derivatives—Delta and Gamma—using a polynomial least-squares fit. The theoretical underpinnings are the Black-Scholes model, a foundational concept in mathematical finance, and the method of least squares, a standard tool in numerical analysis. All parameters, constraints, and objectives are specified with sufficient clarity and precision. The test cases provided are well within the domain of validity for the model and the numerical methods. No scientific or logical contradictions, ambiguities, or missing information have been identified. The problem is therefore deemed to be valid, well-posed, and scientifically grounded. We may proceed to construct the solution.\n\nThe objective is to approximate the Black-Scholes price function $C(S)$ for a European call option as a polynomial in the underlying asset price $S$. The price is given by\n$$\nC(S) = S\\,\\Phi(d_1) - K\\,e^{-r T}\\,\\Phi(d_2)\n$$\nwhere $K$ is the strike price, $r$ is the risk-free rate, $T$ is the time to maturity, $\\sigma$ is the volatility, and $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution. The terms $d_1$ and $d_2$ are defined as\n$$\nd_1 = \\frac{\\ln(S/K) + \\left(r + \\frac{1}{2}\\sigma^2\\right) T}{\\sigma\\sqrt{T}}, \\quad d_2 = d_1 - \\sigma\\sqrt{T}\n$$\nThe problem requires us to generate a set of $n$ sample points $(S_i, y_i)$ where the grid $\\{S_i\\}_{i=1}^n$ is equispaced on the interval $[S_{\\min}, S_{\\max}]$, and $y_i = C(S_i)$. We then seek a polynomial $\\hat{C}_m(S)$ of degree at most $m$,\n$$\n\\hat{C}_m(S) = \\sum_{j=0}^{m} p_j S^{m-j}\n$$\nthat minimizes the sum of squared residuals, $L = \\sum_{i=1}^n (y_i - \\hat{C}_m(S_i))^2$. This is a classical linear least-squares problem. The vector of coefficients $\\mathbf{p} = [p_0, p_1, \\dots, p_m]^T$ can be found by solving the normal equations $(\\mathbf{X}^T \\mathbf{X})\\mathbf{p} = \\mathbf{X}^T \\mathbf{y}$, where $\\mathbf{y}$ is the vector of prices $y_i$ and $\\mathbf{X}$ is the design matrix with entries $X_{ij} = S_i^{m-j}$. Numerically-stable algorithms, such as those based on QR decomposition, are typically employed to solve this system.\n\nOnce the coefficients of the approximating polynomial $\\hat{C}_m(S)$ are determined, we can readily compute its derivatives to approximate the option's sensitivities, known as \"Greeks.\" The Delta, $\\Delta = \\frac{\\partial C}{\\partial S}$, is the first derivative with respect to $S$, and the Gamma, $\\Gamma = \\frac{\\partial^2 C}{\\partial S^2}$, is the second derivative. The polynomial approximations for these are:\n$$\n\\widehat{\\Delta}_m(S) = \\frac{\\mathrm{d}}{\\mathrm{d}S}\\hat{C}_m(S) = \\sum_{j=0}^{m-1} (m-j) p_j S^{m-j-1}\n$$\n$$\n\\widehat{\\Gamma}_m(S) = \\frac{\\mathrm{d}^2}{\\mathrm{d}S^2}\\hat{C}_m(S) = \\sum_{j=0}^{m-2} (m-j)(m-j-1) p_j S^{m-j-2}\n$$\nThese approximate Greeks are then evaluated at a specified point $S_0$.\n\nThe accuracy of these approximations is assessed by comparing them against the analytical formulas for Delta and Gamma derived from the Black-Scholes model:\n$$\n\\Delta_{\\mathrm{BS}}(S) = \\Phi(d_1)\n$$\n$$\n\\Gamma_{\\mathrm{BS}}(S) = \\frac{\\phi(d_1)}{S\\,\\sigma\\,\\sqrt{T}}\n$$\nwhere $\\phi(\\cdot)$ is the probability density function (PDF) of the standard normal distribution. The absolute errors are calculated at the evaluation point $S_0$:\n$$\n\\text{DeltaError} = |\\widehat{\\Delta}_m(S_0) - \\Delta_{\\mathrm{BS}}(S_0)|\n$$\n$$\n\\text{GammaError} = |\\widehat{\\Gamma}_m(S_0) - \\Gamma_{\\mathrm{BS}}(S_0)|\n$$\nThe computational procedure for each test case involves these steps: generating the grid of asset prices, calculating the exact option prices at these points, fitting the polynomial, differentiating it, evaluating the approximate and analytical Greeks at $S_0$, and finally computing the absolute errors. Standard numerical libraries will be used for the normal distribution functions and for the polynomial fitting and differentiation, ensuring both accuracy and numerical stability.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# language: Python\n# version: 3.12\n# libraries:\n#     - name: numpy\n#       version: 1.23.5\n#     - name: scipy\n#       version: 1.11.4\n\ndef black_scholes_call(S, K, T, r, sigma):\n    \"\"\"\n    Calculates the Black-Scholes price for a European call option.\n    Note: S can be a numpy array.\n    \"\"\"\n    # Ensure S is a float array to avoid potential type issues\n    S = np.asarray(S, dtype=float)\n    \n    # Handle the case where S is very close to zero\n    S[S < 1e-9] = 1e-9\n\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n    return price\n\ndef black_scholes_delta(S, K, T, r, sigma):\n    \"\"\"\n    Calculates the analytical Black-Scholes Delta for a European call option.\n    \"\"\"\n    S = float(S)\n    if S < 1e-9:\n        S = 1e-9\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    return norm.cdf(d1)\n\ndef black_scholes_gamma(S, K, T, r, sigma):\n    \"\"\"\n    Calculates the analytical Black-Scholes Gamma for a European call option.\n    \"\"\"\n    S = float(S)\n    if S < 1e-9:\n        S = 1e-9\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    pdf_d1 = norm.pdf(d1)\n    return pdf_d1 / (S * sigma * np.sqrt(T))\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and generate the final output.\n    \"\"\"\n    # Test cases: (K, r, sigma, T, S_min, S_max, n, m, S0)\n    test_cases = [\n        (100, 0.02, 0.2, 0.5, 50, 150, 101, 5, 100),\n        (100, 0.01, 0.25, 1.0, 80, 200, 121, 6, 180),\n        (100, 0.03, 0.05, 1.0, 80, 120, 81, 5, 100),\n        (100, 0.00, 0.3, 0.01, 80, 120, 81, 5, 100),\n        (120, 0.02, 0.2, 1.0, 60, 120, 121, 5, 60),\n    ]\n\n    results = []\n    for case in test_cases:\n        K, r, sigma, T, S_min, S_max, n, m, S0 = case\n\n        # 1. Construct the grid and evaluate exact prices\n        S_grid = np.linspace(S_min, S_max, n)\n        y_grid = black_scholes_call(S_grid, K, T, r, sigma)\n\n        # 2. Compute the least-squares polynomial approximant\n        # np.polyfit returns coefficients in descending order of power\n        coeffs = np.polyfit(S_grid, y_grid, m)\n        \n        # 3. Differentiate the polynomial to get Greeks approximations\n        # np.poly1d creates a polynomial object from coefficients\n        C_poly = np.poly1d(coeffs)\n        Delta_poly = C_poly.deriv(1)\n        Gamma_poly = C_poly.deriv(2)\n\n        # 4. Evaluate approximated Greeks at the evaluation point S0\n        delta_approx = Delta_poly(S0)\n        gamma_approx = Gamma_poly(S0)\n\n        # 5. Evaluate analytical Greeks at S0\n        delta_exact = black_scholes_delta(S0, K, T, r, sigma)\n        gamma_exact = black_scholes_gamma(S0, K, T, r, sigma)\n\n        # 6. Compute absolute errors\n        delta_error = abs(delta_approx - delta_exact)\n        gamma_error = abs(gamma_approx - gamma_exact)\n        \n        results.append([delta_error, gamma_error])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f'[{d_err:.6f},{g_err:.6f}]' for d_err, g_err in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}]}