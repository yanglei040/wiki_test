{"hands_on_practices": [{"introduction": "The first step in any journey is determining which direction to go. For the steepest descent method, this means identifying the path of most rapid decrease from a given point, which is precisely the negative of the function's gradient. This practice challenges you to compute this initial search direction for the famous Rosenbrock function, a classic benchmark in optimization known for its challenging, curved valley [@problem_id:2221567].", "id": "2221567", "problem": "In the field of numerical optimization, the performance of new algorithms is often benchmarked using a set of standard test functions. One such function is the Rosenbrock function, which is challenging to minimize due to its narrow, parabolic valley.\n\nConsider a two-dimensional version of the Rosenbrock function given by\n$$f(x,y) = (a-x)^2 + b(y-x^2)^2$$\nwhere $a$ and $b$ are positive real constants.\n\nAn iterative minimization algorithm is initiated at the point $(x_0, y_0) = (0, 0)$. The first step of this algorithm involves determining the initial search direction. This direction is defined as the vector along which the function's value decreases most rapidly from the starting point.\n\nDetermine this initial search direction vector, $\\mathbf{d}_0$. Express your answer as a column vector in terms of the constants $a$ and $b$.\n\n", "solution": "The direction along which $f$ decreases most rapidly at a point is the negative gradient, so the initial search direction from $(x_{0},y_{0})=(0,0)$ is $\\mathbf{d}_{0}=-\\nabla f(0,0)$.\n\nCompute the gradient of $f(x,y)=(a-x)^{2}+b(y-x^{2})^{2}$:\n- The partial derivative with respect to $x$ is\n$$\n\\frac{\\partial f}{\\partial x}=2(x-a)-4bx(y-x^{2}).\n$$\n- The partial derivative with respect to $y$ is\n$$\n\\frac{\\partial f}{\\partial y}=2b(y-x^{2}).\n$$\n\nEvaluate these at $(0,0)$:\n$$\n\\frac{\\partial f}{\\partial x}(0,0)=-2a,\\qquad \\frac{\\partial f}{\\partial y}(0,0)=0.\n$$\nHence,\n$$\n\\nabla f(0,0)=\\begin{pmatrix}-2a\\\n$$4pt]0\\end{pmatrix},\n\\quad\n\\mathbf{d}_{0}=-\\nabla f(0,0)=\\begin{pmatrix}2a\\\n$$4pt]0\\end{pmatrix}.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}2a\\\\0\\end{pmatrix}}$$"}, {"introduction": "Once we know the direction, how far should we travel along it? This exercise moves beyond the first step to performing complete iterations of the steepest descent algorithm. By manually calculating the first two steps for a quadratic function using an exact line search, you will gain first-hand insight into the algorithm's mechanics and observe its characteristic zig-zagging behavior on ill-conditioned problems [@problem_id:2221576]. This phenomenon is a crucial consideration in practical applications.", "id": "2221576", "problem": "Consider the unconstrained optimization problem of minimizing the function $f(x, y) = 10x^2 + y^2$. The optimization process is initiated at the point $\\mathbf{x}_0 = (x_0, y_0) = (1, 1)$.\n\nThe steepest descent algorithm is to be used. For each iteration $k$, the step size, denoted by $\\alpha_k > 0$, is determined via an exact line search. This means that for a given point $\\mathbf{x}_k$ and descent direction $\\mathbf{p}_k$, the step size $\\alpha_k$ is chosen to find the global minimum of the single-variable function $g(\\alpha) = f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$.\n\nCalculate the coordinates of the point $\\mathbf{x}_2 = (x_2, y_2)$ after two complete iterations of this method. The coordinates in your final answer should be expressed as exact fractions in their simplest form.\n\n", "solution": "We minimize $f(x,y)=10x^{2}+y^{2}$ starting at $\\mathbf{x}_{0}=(1,1)$ using steepest descent with exact line search. The gradient and Hessian are\n$$\n\\nabla f(x,y)=\\begin{pmatrix}20x\\\\2y\\end{pmatrix},\\qquad H=\\begin{pmatrix}20&0\\\\0&2\\end{pmatrix}.\n$$\nAt iteration $k$, with $\\mathbf{g}_{k}=\\nabla f(\\mathbf{x}_{k})$ and direction $\\mathbf{p}_{k}=-\\mathbf{g}_{k}$, the exact line search minimizes $g(\\alpha)=f(\\mathbf{x}_{k}+\\alpha\\mathbf{p}_{k})$. Since $H$ is constant (quadratic function), the derivative is\n$$\ng'(\\alpha)=\\nabla f(\\mathbf{x}_{k}-\\alpha \\mathbf{g}_{k})^{\\mathsf{T}}(-\\mathbf{g}_{k})=\\left(\\mathbf{g}_{k}-\\alpha H\\mathbf{g}_{k}\\right)^{\\mathsf{T}}(-\\mathbf{g}_{k})=-(\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k})+\\alpha\\,\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}.\n$$\nSetting $g'(\\alpha)=0$ yields the exact step size\n$$\n\\alpha_{k}=\\frac{\\mathbf{g}_{k}^{\\mathsf{T}}\\mathbf{g}_{k}}{\\mathbf{g}_{k}^{\\mathsf{T}}H\\mathbf{g}_{k}}.\n$$\n\nIteration $0$: $\\mathbf{x}_{0}=(1,1)$ gives\n$$\n\\mathbf{g}_{0}=\\begin{pmatrix}20\\\\2\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}\\mathbf{g}_{0}=404,\\quad H\\mathbf{g}_{0}=\\begin{pmatrix}400\\\\4\\end{pmatrix},\\quad \\mathbf{g}_{0}^{\\mathsf{T}}H\\mathbf{g}_{0}=8008,\n$$\nso\n$$\n\\alpha_{0}=\\frac{404}{8008}=\\frac{101}{2002}.\n$$\nThen\n$$\n\\mathbf{x}_{1}=\\mathbf{x}_{0}-\\alpha_{0}\\mathbf{g}_{0}=\\begin{pmatrix}1-20\\alpha_{0}\\\\1-2\\alpha_{0}\\end{pmatrix}=\\begin{pmatrix}1-\\frac{1010}{1001}\\\\1-\\frac{101}{1001}\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}\\\\\\frac{900}{1001}\\end{pmatrix}.\n$$\n\nIteration $1$: $\\mathbf{x}_{1}=\\left(-\\frac{9}{1001},\\,\\frac{900}{1001}\\right)$ gives\n$$\n\\mathbf{g}_{1}=\\begin{pmatrix}20x_{1}\\\\2y_{1}\\end{pmatrix}=\\begin{pmatrix}-\\frac{180}{1001}\\\\\\frac{1800}{1001}\\end{pmatrix}.\n$$\nCompute\n$$\n\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}=\\frac{180^{2}+1800^{2}}{1001^{2}}=\\frac{180^{2}\\cdot 101}{1001^{2}},\\qquad\nH\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{3600}{1001}\\\\\\frac{3600}{1001}\\end{pmatrix},\\qquad\n\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}=\\frac{180\\cdot 3600}{1001^{2}}(1+10)=\\frac{7{,}128{,}000}{1001^{2}}.\n$$\nThus\n$$\n\\alpha_{1}=\\frac{\\mathbf{g}_{1}^{\\mathsf{T}}\\mathbf{g}_{1}}{\\mathbf{g}_{1}^{\\mathsf{T}}H\\mathbf{g}_{1}}=\\frac{3{,}272{,}400}{7{,}128{,}000}=\\frac{101}{220}.\n$$\nUpdate\n$$\n\\mathbf{x}_{2}=\\mathbf{x}_{1}-\\alpha_{1}\\mathbf{g}_{1}=\\begin{pmatrix}-\\frac{9}{1001}-\\frac{101}{220}\\left(-\\frac{180}{1001}\\right)\\\n$$4pt]\\frac{900}{1001}-\\frac{101}{220}\\left(\\frac{1800}{1001}\\right)\\end{pmatrix}=\\begin{pmatrix}-\\frac{9}{1001}+\\frac{909}{11011}\\\n$$4pt]\\frac{900}{1001}-\\frac{9090}{11011}\\end{pmatrix}=\\begin{pmatrix}\\frac{810}{11011}\\\n$$4pt]\\frac{810}{11011}\\end{pmatrix}.\n$$\nThe fractions are already in simplest terms because $\\gcd(810,11011)=1$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{810}{11011} & \\frac{810}{11011} \\end{pmatrix}}$$"}, {"introduction": "From manual calculation to automated solution, the true power of numerical methods lies in their implementation. This final practice brings everything together by asking you to write a program that implements the steepest descent algorithm, incorporating a practical backtracking line search to determine the step size [@problem_id:2434090]. You will apply your code to solve a utility maximization problem, a common task in economics, bridging the gap between theoretical understanding and practical computational skill.", "id": "2434090", "problem": "Consider the unconstrained utility maximization problem in two variables given by the quadratic utility function\n$$u(x) = b^{\\top} x - \\tfrac{1}{2} x^{\\top} Q x,$$\nwhere $x \\in \\mathbb{R}^{2}$, $b \\in \\mathbb{R}^{2}$, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is symmetric and positive definite. This problem can be equivalently expressed as the unconstrained minimization problem of the objective\n$$f(x) = -u(x) = \\tfrac{1}{2} x^{\\top} Q x - b^{\\top} x.$$\nLet the iteration be defined by $x^{(k+1)} = x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)$, where $\\nabla f(x) = Qx - b$. At each iteration, the step size $\\alpha_{k}$ must satisfy the sufficient decrease condition\n$$f\\left(x^{(k)} - \\alpha_{k} \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) + c \\, \\alpha_{k} \\, \\nabla f\\left(x^{(k)}\\right)^{\\top}\\left(-\\nabla f\\left(x^{(k)}\\right)\\right),$$\nwith $\\alpha_{k}$ chosen from the geometric sequence $\\{\\alpha_{0} \\rho^{m} : m \\in \\mathbb{N} \\cup \\{0\\}\\}$ for fixed constants $\\alpha_{0} \\in \\mathbb{R}_{++}$, $\\rho \\in (0,1)$, and $c \\in (0,1)$. Use the Euclidean norm $\\|\\cdot\\|_{2}$ for any norm computation. The algorithm must terminate when $\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$ or when the iteration count reaches a maximum $N_{\\max}$.\n\nFor each test case, compute the terminal iterate $x^{(T)}$ produced by the described procedure, and also compute the unique maximizer $x^{\\star}$ of $u(x)$, which is equivalently the unique minimizer of $f(x)$ and satisfies $Q x^{\\star} = b$. For each test case, report the scalar Euclidean error $\\|x^{(T)} - x^{\\star}\\|_{2}$.\n\nUse the following fixed parameter values for all test cases: initial step parameter $\\alpha_{0} = 1$, backtracking shrinkage factor $\\rho = 0.5$, sufficient decrease constant $c = 10^{-4}$, tolerance $\\varepsilon = 10^{-8}$, and maximum iterations $N_{\\max} = 10000$. All matrices and vectors are specified exactly below.\n\nTest suite:\n- Test case $1$: $Q = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Test case $2$: $Q = \\begin{bmatrix} 100 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 100 \\\\ 1 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix}$.\n- Test case $3$: $Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} \\tfrac{2}{5} \\\\ -\\tfrac{1}{5} \\end{bmatrix}$.\n- Test case $4$: $Q = \\begin{bmatrix} 4 & 1.5 \\\\ 1.5 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $x^{(0)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, namely $[\\|x^{(T)}_{1} - x^{\\star}_{1}\\|_{2}, \\|x^{(T)}_{2} - x^{\\star}_{2}\\|_{2}, \\|x^{(T)}_{3} - x^{\\star}_{3}\\|_{2}, \\|x^{(T)}_{4} - x^{\\star}_{4}\\|_{2}]$. The results must be real numbers (floats). No physical units are involved in this problem.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in the field of numerical optimization, specifically focusing on the steepest descent method applied to a quadratic objective function. All necessary parameters and data are provided, the terminology is precise, and there are no internal contradictions or logical flaws. The problem involves minimizing a strictly convex quadratic function, for which the steepest descent method with a backtracking line search is a standard and convergent algorithm. We will now proceed with the solution.\n\nThe problem is to minimize the quadratic objective function $f(x) = \\frac{1}{2} x^{\\top} Q x - b^{\\top} x$, where $x \\in \\mathbb{R}^{2}$, $b \\in \\mathbb{R}^{2}$, and $Q \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric and positive definite matrix.\n\nFirst, we establish the existence and uniqueness of the solution. The Hessian of the objective function is $\\nabla^2 f(x) = Q$. Since $Q$ is given as positive definite for all test cases, the function $f(x)$ is strictly convex. A strictly convex function on $\\mathbb{R}^{n}$ has at most one minimizer. Since $f(x)$ is coercive (i.e., $f(x) \\to \\infty$ as $\\|x\\|_2 \\to \\infty$), a unique global minimizer, which we denote by $x^{\\star}$, is guaranteed to exist.\n\nThe first-order necessary condition for optimality states that the gradient of the objective function must be zero at the minimizer. The gradient of $f(x)$ is $\\nabla f(x) = Qx - b$. Setting the gradient to zero gives the optimality condition:\n$$\n\\nabla f(x^{\\star}) = Qx^{\\star} - b = 0\n$$\nThis is a system of linear equations $Qx^{\\star} = b$. Since $Q$ is positive definite, it is invertible. Therefore, the unique analytical solution is given by:\n$$\nx^{\\star} = Q^{-1} b\n$$\n\nThe numerical procedure to find the solution is the method of steepest descent. This is an iterative algorithm that generates a sequence of points $\\{x^{(k)}\\}_{k=0}^{\\infty}$ that converges to $x^{\\star}$. The iteration is defined as:\n$$\nx^{(k+1)} = x^{(k)} + \\alpha_k p^{(k)}\n$$\nwhere $p^{(k)}$ is the search direction and $\\alpha_k > 0$ is the step size. For the method of steepest descent, the search direction is chosen to be the negative of the gradient at the current iterate, as this is the direction of the most rapid decrease of the function locally.\n$$\np^{(k)} = -\\nabla f(x^{(k)}) = -(Qx^{(k)} - b) = b - Qx^{(k)}\n$$\nThe update rule is thus:\n$$\nx^{(k+1)} = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\n$$\n\nThe step size $\\alpha_k$ is determined using a backtracking line search procedure to satisfy the sufficient decrease condition, also known as the Armijo condition. This ensures that each step makes meaningful progress towards the minimum. The condition is:\n$$\nf(x^{(k+1)}) \\le f(x^{(k)}) + c \\, \\alpha_k \\, \\nabla f(x^{(k)})^{\\top} p^{(k)}\n$$\nSubstituting $p^{(k)} = -\\nabla f(x^{(k)})$, the condition becomes:\n$$\nf\\left(x^{(k)} - \\alpha_k \\nabla f\\left(x^{(k)}\\right)\\right) \\le f\\left(x^{(k)}\\right) - c \\, \\alpha_k \\, \\|\\nabla f\\left(x^{(k)}\\right)\\|^{2}_{2}\n$$\nThe procedure for selecting $\\alpha_k$ at each iteration $k$ is as follows:\n$1$. Start with an initial step size $\\alpha = \\alpha_0 = 1$.\n$2$. While the Armijo condition is not satisfied, shrink the step size: $\\alpha \\leftarrow \\rho \\alpha$. Here, $\\rho = 0.5$.\n$3$. Once the condition is met, set $\\alpha_k = \\alpha$.\nThe parameters are given as $c = 10^{-4}$, $\\alpha_0 = 1$, and $\\rho = 0.5$.\n\nThe algorithm terminates when one of two conditions is met:\n$1$. The norm of the gradient is smaller than a specified tolerance $\\varepsilon = 10^{-8}$: $\\|\\nabla f(x^{(k)})\\|_{2} \\le \\varepsilon$. This indicates that the iterate is very close to the optimal solution $x^{\\star}$ where the gradient is zero.\n$2$. The number of iterations $k$ reaches the maximum allowed number, $N_{\\max} = 10000$.\n\nLet $x^{(T)}$ be the terminal iterate produced by the algorithm. The final required output for each test case is the Euclidean error $\\|x^{(T)} - x^{\\star}\\|_{2}$. For each given test case ($Q$, $b$, $x^{(0)}$), we will first compute the analytical solution $x^{\\star} = Q^{-1}b$ and then execute the described iterative algorithm to find $x^{(T)}$.\n\nFor test case $3$, we are given $Q = \\begin{bmatrix} 3 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, and $x^{(0)} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$. The analytical solution is $x^{\\star} = Q^{-1}b = \\frac{1}{5}\\begin{bmatrix} 2 & -1 \\\\ -1 & 3 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2/5 \\\\ -1/5 \\end{bmatrix}$. Thus, the initial point $x^{(0)}$ is the exact solution $x^{\\star}$. In this case, the gradient at the start is $\\nabla f(x^{(0)}) = Qx^{(0)} - b = 0$. The termination condition $\\|\\nabla f(x^{(0)})\\|_{2} = 0 \\le \\varepsilon$ is satisfied immediately at iteration $k=0$. The algorithm terminates, yielding $x^{(T)} = x^{(0)}$, and the error $\\|x^{(T)} - x^{\\star}\\|_{2}$ is $0$.\n\nThe implementation will now follow this logic for all test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the unconstrained utility maximization problem for four test cases\n    using the steepest descent method with backtracking line search.\n    \"\"\"\n    # Fixed parameters for the algorithm\n    alpha0 = 1.0\n    rho = 0.5\n    c = 1e-4\n    epsilon = 1e-8\n    N_max = 10000\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        {\n            \"Q\": np.array([[2.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([2.0, 1.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n        {\n            \"Q\": np.array([[100.0, 0.0], [0.0, 1.0]]),\n            \"b\": np.array([100.0, 1.0]),\n            \"x0\": np.array([10.0, -10.0]),\n        },\n        {\n            \"Q\": np.array([[3.0, 1.0], [1.0, 2.0]]),\n            \"b\": np.array([1.0, 0.0]),\n            \"x0\": np.array([2.0 / 5.0, -1.0 / 5.0]),\n        },\n        {\n            \"Q\": np.array([[4.0, 1.5], [1.5, 1.0]]),\n            \"b\": np.array([1.0, 2.0]),\n            \"x0\": np.array([0.0, 0.0]),\n        },\n    ]\n\n    results = []\n\n    def objective_function(x, Q, b):\n        \"\"\"Computes the value of the objective function f(x).\"\"\"\n        return 0.5 * x.T @ Q @ x - b.T @ x\n\n    for case in test_cases:\n        Q = case[\"Q\"]\n        b = case[\"b\"]\n        x_k = case[\"x0\"].copy()\n\n        # Compute the analytical solution x_star\n        x_star = np.linalg.solve(Q, b)\n\n        # Main loop for the steepest descent algorithm\n        for _ in range(N_max):\n            # Compute the gradient at the current iterate x_k\n            grad_f = Q @ x_k - b\n            grad_norm = np.linalg.norm(grad_f)\n\n            # Check for termination based on gradient norm\n            if grad_norm <= epsilon:\n                break\n            \n            # Backtracking line search to find the step size alpha_k\n            alpha = alpha0\n            f_k = objective_function(x_k, Q, b)\n            grad_norm_sq = grad_norm**2 # More efficient than dot product\n\n            while True:\n                # Armijo condition check\n                f_new = objective_function(x_k - alpha * grad_f, Q, b)\n                if f_new <= f_k - c * alpha * grad_norm_sq:\n                    break\n                \n                # Shrink alpha if condition is not met\n                alpha *= rho\n\n            # Update the iterate\n            x_k = x_k - alpha * grad_f\n        \n        # The loop terminates, x_k is the terminal iterate x_T\n        x_terminal = x_k\n\n        # Compute the final Euclidean error\n        error = np.linalg.norm(x_terminal - x_star)\n        results.append(error)\n\n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}