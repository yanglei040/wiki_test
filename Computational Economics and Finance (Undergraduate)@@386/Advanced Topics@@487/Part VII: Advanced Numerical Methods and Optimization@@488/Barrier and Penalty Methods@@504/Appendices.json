{"hands_on_practices": [{"introduction": "This practice guides you through implementing the sequential quadratic penalty method, a foundational technique for handling constraints. By comparing a \"cold-start\" against a \"warm-start\" strategy for a sequence of subproblems with increasing penalty parameter $\\rho$, you will gain practical insight into making these methods efficient. This exercise reveals how the solution for one penalty parameter provides an excellent starting point for the next, dramatically reducing the overall computational effort. [@problem_id:2423453]", "id": "2423453", "problem": "You are asked to implement a sequential quadratic penalty method for constrained optimization and to experimentally quantify the benefit of warm-starting by reusing the previous subproblem’s solution as the initial guess for the next penalty parameter. Implement a gradient-based solver with a backtracking Armijo line search to minimize a sequence of unconstrained penalized subproblems. For a fixed sequence of penalty parameters $\\{\\rho_k\\}_{k=1}^K$ with $\\rho_1 &lt; \\rho_2 &lt; \\dots &lt; \\rho_K$, compare two strategies for each test case: (i) cold-starting each subproblem from the same initial point, and (ii) warm-starting subproblem $k+1$ from the computed minimizer of subproblem $k$. Report the speedup factor defined as the ratio of the total number of gradient descent iterations taken by cold-starting to that taken by warm-starting across the full penalty sequence.\n\nFundamental base and definitions to use:\n- A constrained minimization problem has objective $f:\\mathbb{R}^n\\to\\mathbb{R}$, inequality constraints $g_i(x)\\le 0$ for $i\\in\\{1,\\dots,m\\}$, and equality constraints $h_j(x)=0$ for $j\\in\\{1,\\dots,p\\}$.\n- The classical quadratic penalty for inequalities is applied to the violation as $\\max\\{0, g_i(x)\\}^2$ and for equalities as $h_j(x)^2$.\n- The penalized subproblem for a given $\\rho&gt;0$ is to minimize \n$$\n\\Phi_\\rho(x)=f(x)+\\rho\\left(\\sum_{i=1}^m \\max\\{0,g_i(x)\\}^2+\\sum_{j=1}^p h_j(x)^2\\right).\n$$\n- Use gradient descent with a backtracking Armijo rule: given current $x$, gradient $\\nabla\\Phi_\\rho(x)$, initial step size $t_0$, shrinkage factor $\\beta\\in(0,1)$, and Armijo parameter $c\\in(0,1)$, choose the largest $t$ from the sequence $\\{t_0, \\beta t_0, \\beta^2 t_0,\\dots\\}$ satisfying\n$$\n\\Phi_\\rho(x - t \\nabla \\Phi_\\rho(x)) \\le \\Phi_\\rho(x) - c\\,t\\,\\|\\nabla \\Phi_\\rho(x)\\|_2^2.\n$$\n- Stop the inner solver when $\\|\\nabla \\Phi_\\rho(x)\\|_2\\le \\varepsilon$.\n\nImplementation requirements:\n- Implement the quadratic penalty method and gradient descent with backtracking Armijo line search exactly as defined above.\n- For inequality constraints, treat only the positive violation by using the $\\max\\{0,\\cdot\\}$ structure in both the penalty value and its gradient. For equality constraints, penalize the squared residual.\n- Use the penalty sequence $\\rho\\in\\{10,10^2,10^3\\}$, i.e., $\\rho \\in \\{10,100,1000\\}$.\n- Use gradient tolerance $\\varepsilon=10^{-6}$, Armijo parameter $c=10^{-4}$, shrinkage factor $\\beta=\\tfrac{1}{2}$, and initial step size $t_0=1$ for all subproblems. Cap the maximum number of gradient iterations per subproblem at $N_{\\max}=10^4$.\n- Count the number of outer gradient descent iterations (each accepted step after line search) taken to converge a subproblem; do not count line search backtracking steps separately.\n\nTest suite:\nImplement and solve the following three two-dimensional test cases. In each case, return the speedup factor\n$$\nS=\\frac{N_{\\mathrm{cold}}}{N_{\\mathrm{warm}}},\n$$\nwhere $N_{\\mathrm{cold}}$ is the total number of gradient descent iterations summed over all penalty parameters when cold-starting each subproblem from the specified initial point, and $N_{\\mathrm{warm}}$ is the total when warm-starting each subproblem from the previous subproblem’s solution.\n\n- Case $\\mathbf{A}$ (convex quadratic with a binding linear inequality):\n  - Objective: $f(x,y)=(x-1)^2+2\\,(y+2)^2$.\n  - Inequality: $g_1(x,y)=1-x-y\\le 0$.\n  - No equalities.\n  - Initial point: $x_0=(0,0)$.\n\n- Case $\\mathbf{B}$ (convex quadratic with an equality):\n  - Objective: $f(x,y)=(x-3)^2+(y-1)^2$.\n  - Equality: $h_1(x,y)=x-y=0$.\n  - No inequalities.\n  - Initial point: $x_0=(0,0)$.\n\n- Case $\\mathbf{C}$ (convex quadratic with a curved inequality):\n  - Objective: $f(x,y)=(x+2)^2+y^2$.\n  - Inequality: $g_1(x,y)=x^2+y^2-1\\le 0$.\n  - No equalities.\n  - Initial point: $x_0=(0,0)$.\n\nOutput specification:\n- For each case, compute the speedup factor $S$ as defined above.\n- Your program should produce a single line of output containing the three speedup factors as a comma-separated list enclosed in square brackets, in the order $\\left[S_A,S_B,S_C\\right]$, where $S_A$ corresponds to Case $\\mathbf{A}$, $S_B$ to Case $\\mathbf{B}$, and $S_C$ to Case $\\mathbf{C}$. For example, output of the form $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$ with numeric values.\n- Express each speedup factor as a floating-point number. You may round internally, but the printed values must be standard decimal floats.\n\nNo physical units are involved. Angles are not used. Percentages are not used.\n\nThe final program must be self-contained, require no input, and adhere to the specified runtime environment. The correctness will be assessed by verifying that the implementation follows the definitions and that warm-starting yields strictly fewer iterations or at least not more, producing meaningful speedup factors for the specified cases. The output must be exactly one line in the specified format.", "solution": "The problem requires the implementation of a sequential quadratic penalty method to solve constrained optimization problems. The core of the task is to compare the computational efficiency of two initialization strategies for the sequence of unconstrained subproblems: a cold-start strategy versus a warm-start strategy. The efficiency is to be quantified by a speedup factor, defined as the ratio of total gradient descent iterations.\n\nThe general form of the constrained optimization problem is to minimize an objective function $f(x)$ subject to a set of inequality constraints $g_i(x) \\le 0$ for $i \\in \\{1, \\dots, m\\}$ and equality constraints $h_j(x) = 0$ for $j \\in \\{1, \\dots, p\\}$, where $x \\in \\mathbb{R}^n$.\n\nThe quadratic penalty method approximates the solution to this problem by solving a sequence of unconstrained minimization problems. For a given penalty parameter $\\rho > 0$, the penalized objective function $\\Phi_\\rho(x)$ is constructed by adding terms to the original objective function that penalize violations of the constraints. The specific form of the penalized function is:\n$$\n\\Phi_\\rho(x) = f(x) + \\rho \\left( \\sum_{i=1}^m \\left(\\max\\{0, g_i(x)\\}\\right)^2 + \\sum_{j=1}^p \\left(h_j(x)\\right)^2 \\right)\n$$\nThis function, $\\Phi_\\rho(x)$, is then minimized with respect to $x$. By solving this unconstrained problem for a sequence of increasing penalty parameters, $\\rho_1 < \\rho_2 < \\dots < \\rho_K$, the sequence of minimizers $x^*(\\rho_k)$ converges to the solution of the original constrained problem.\n\nTo minimize each unconstrained subproblem $\\min_x \\Phi_\\rho(x)$, a gradient-based method is required. The gradient of the penalized objective function, $\\nabla \\Phi_\\rho(x)$, is derived using the chain rule. For an inequality constraint term $P_i(x) = \\rho (\\max\\{0, g_i(x)\\})^2$, the gradient is $\\nabla P_i(x) = 2 \\rho \\max\\{0, g_i(x)\\} \\nabla g_i(x)$. For an equality constraint term $Q_j(x) = \\rho (h_j(x))^2$, the gradient is $\\nabla Q_j(x) = 2 \\rho h_j(x) \\nabla h_j(x)$. Combining these with the gradient of the objective function, the full gradient is:\n$$\n\\nabla \\Phi_\\rho(x) = \\nabla f(x) + 2\\rho \\left( \\sum_{i=1}^m \\max\\{0, g_i(x)\\} \\nabla g_i(x) + \\sum_{j=1}^p h_j(x) \\nabla h_j(x) \\right)\n$$\nThe unconstrained minimization is performed using gradient descent. Starting from a point $x_k$, the next point $x_{k+1}$ is found by moving in the direction of the negative gradient:\n$$\nx_{k+1} = x_k - t \\nabla \\Phi_\\rho(x_k)\n$$\nThe step size $t > 0$ is determined by a backtracking line search employing the Armijo condition. For a given descent direction $d_k = -\\nabla \\Phi_\\rho(x_k)$, we seek the largest $t$ from the sequence $\\{t_0, \\beta t_0, \\beta^2 t_0, \\dots\\}$ that satisfies:\n$$\n\\Phi_\\rho(x_k + t d_k) \\le \\Phi_\\rho(x_k) + c \\, t \\, \\nabla \\Phi_\\rho(x_k)^T d_k\n$$\nwhich, using $d_k = -\\nabla \\Phi_\\rho(x_k)$, simplifies to the form given in the problem statement:\n$$\n\\Phi_\\rho(x_k - t \\nabla \\Phi_\\rho(x_k)) \\le \\Phi_\\rho(x_k) - c \\, t \\, \\|\\nabla \\Phi_\\rho(x_k)\\|_2^2\n$$\nThe algorithm iterates until the norm of the gradient is below a specified tolerance $\\varepsilon$, i.e., $\\|\\nabla \\Phi_\\rho(x)\\|_2 \\le \\varepsilon$. The parameters for this solver are fixed: initial step size $t_0=1$, Armijo parameter $c=10^{-4}$, shrinkage factor $\\beta=0.5$, and gradient norm tolerance $\\varepsilon=10^{-6}$. The maximum number of iterations per subproblem is capped at $N_{\\max}=10^4$.\n\nThe experiment compares two strategies over the penalty parameter sequence $\\rho \\in \\{10, 100, 1000\\}$:\n1.  **Cold-Start:** Each subproblem for $\\rho_k$ is initialized from the same starting point $x_0$. The total number of iterations, $N_{\\mathrm{cold}}$, is the sum of iterations required to solve each subproblem independently.\n2.  **Warm-Start:** The first subproblem (for $\\rho_1=10$) is initialized from $x_0$. The subproblem for each subsequent $\\rho_{k+1}$ is initialized using the solution obtained from the previous subproblem for $\\rho_k$. The total number of iterations, $N_{\\mathrm{warm}}$, is the sum of iterations across this sequence.\n\nThe rationale for warm-starting is that the solution $x^*(\\rho_k)$ is expected to be a good initial guess for the minimizer of $\\Phi_{\\rho_{k+1}}(x)$, especially when $\\rho_{k+1}$ is not drastically larger than $\\rho_k$. This should lead to faster convergence. The performance gain is measured by the speedup factor $S = N_{\\mathrm{cold}} / N_{\\mathrm{warm}}$.\n\nThe implementation will proceed by defining Python functions for each test case's objective and constraint functions and their respective gradients. A generalized solver function will execute the gradient descent with Armijo line search. A top-level function will manage the sequence of penalty parameters, apply both cold-start and warm-start strategies, count the total iterations for each, and compute the speedup. This process will be repeated for all three test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    \n    # --- Solver Parameters ---\n    SOLVER_PARAMS = {\n        'epsilon': 1e-6,\n        'c_armijo': 1e-4,\n        'beta': 0.5,\n        't0': 1.0,\n        'n_max': 10000\n    }\n    PENALTY_PARAMS = [10.0, 100.0, 1000.0]\n\n    # --- Test Case Definitions ---\n    \n    # Case A: (x-1)^2 + 2(y+2)^2, s.t. 1-x-y <= 0\n    case_A = {\n        'f': lambda x: (x[0] - 1.0)**2 + 2.0 * (x[1] + 2.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 1.0), 4.0 * (x[1] + 2.0)]),\n        'g': [lambda x: 1.0 - x[0] - x[1]],\n        'grad_g': [lambda x: np.array([-1.0, -1.0])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    # Case B: (x-3)^2 + (y-1)^2, s.t. x-y = 0\n    case_B = {\n        'f': lambda x: (x[0] - 3.0)**2 + (x[1] - 1.0)**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] - 3.0), 2.0 * (x[1] - 1.0)]),\n        'g': [],\n        'grad_g': [],\n        'h': [lambda x: x[0] - x[1]],\n        'grad_h': [lambda x: np.array([1.0, -1.0])],\n        'x0': np.array([0.0, 0.0])\n    }\n    \n    # Case C: (x+2)^2 + y^2, s.t. x^2+y^2-1 <= 0\n    case_C = {\n        'f': lambda x: (x[0] + 2.0)**2 + x[1]**2,\n        'grad_f': lambda x: np.array([2.0 * (x[0] + 2.0), 2.0 * x[1]]),\n        'g': [lambda x: x[0]**2 + x[1]**2 - 1.0],\n        'grad_g': [lambda x: np.array([2.0 * x[0], 2.0 * x[1]])],\n        'h': [],\n        'grad_h': [],\n        'x0': np.array([0.0, 0.0])\n    }\n\n    test_cases = [case_A, case_B, case_C]\n    \n    def get_penalized_funcs(case, rho):\n        \"\"\"Creates the penalized function and its gradient for a given case and rho.\"\"\"\n        \n        def phi(x):\n            f_val = case['f'](x)\n            g_sum = sum(max(0, g_func(x))**2 for g_func in case['g'])\n            h_sum = sum(h_func(x)**2 for h_func in case['h'])\n            return f_val + rho * (g_sum + h_sum)\n\n        def grad_phi(x):\n            grad_f_val = case['grad_f'](x)\n            \n            grad_g_sum = np.zeros_like(x)\n            for g_func, grad_g_func in zip(case['g'], case['grad_g']):\n                g_val = g_func(x)\n                if g_val > 0:\n                    grad_g_sum += 2.0 * g_val * grad_g_func(x)\n\n            grad_h_sum = np.zeros_like(x)\n            for h_func, grad_h_func in zip(case['h'], case['grad_h']):\n                h_val = h_func(x)\n                grad_h_sum += 2.0 * h_val * grad_h_func(x)\n                \n            return grad_f_val + rho * (grad_g_sum + grad_h_sum)\n        \n        return phi, grad_phi\n\n    def gradient_descent(phi, grad_phi, x_init, params):\n        \"\"\"\n        Performs gradient descent with backtracking Armijo line search.\n        \"\"\"\n        x = np.copy(x_init)\n        n_iters = 0\n        \n        for k in range(params['n_max']):\n            grad = grad_phi(x)\n            grad_norm_sq = np.dot(grad, grad)\n\n            if np.sqrt(grad_norm_sq) <= params['epsilon']:\n                break\n            \n            # Backtracking line search\n            t = params['t0']\n            phi_x = phi(x)\n            \n            while True:\n                x_new = x - t * grad\n                phi_new = phi(x_new)\n                armijo_check = phi_x - params['c_armijo'] * t * grad_norm_sq\n                \n                if phi_new <= armijo_check:\n                    break\n                t *= params['beta']\n            \n            x = x_new\n            n_iters += 1\n        \n        return x, n_iters\n\n    def run_penalty_method(case, solver_params, penalty_params):\n        \"\"\"\n        Runs the full sequential penalty method for a case,\n        calculating iterations for both cold and warm starts.\n        \"\"\"\n        # Cold start\n        total_iters_cold = 0\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            _, n_iters = gradient_descent(phi, grad_phi, case['x0'], solver_params)\n            total_iters_cold += n_iters\n            \n        # Warm start\n        total_iters_warm = 0\n        x_warm = np.copy(case['x0'])\n        for rho in penalty_params:\n            phi, grad_phi = get_penalized_funcs(case, rho)\n            x_sol, n_iters = gradient_descent(phi, grad_phi, x_warm, solver_params)\n            total_iters_warm += n_iters\n            x_warm = x_sol\n            \n        if total_iters_warm == 0:\n             # This case should not happen in this problem, but is a safeguard.\n             # If cold is also 0, speedup is 1. If cold > 0, speedup is \"infinite\".\n            return 1.0 if total_iters_cold == 0 else float('inf')\n            \n        return float(total_iters_cold) / float(total_iters_warm)\n\n    results = []\n    for case in test_cases:\n        speedup = run_penalty_method(case, SOLVER_PARAMS, PENALTY_PARAMS)\n        results.append(speedup)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Penalty methods are not just for optimizing an objective; they are also powerful tools for the preliminary step of finding a feasible point that satisfies a complex set of constraints. This exercise applies the penalty method to a common challenge in computational finance: constructing an initial portfolio that adheres to rules on budget, risk, and return. You will build and minimize a merit function composed of squared constraint violations to systematically drive your solution towards the feasible region. [@problem_id:2374527]", "id": "2374527", "problem": "Consider the task of constructing an initial portfolio vector that satisfies a set of economic and financial constraints. Let $n \\in \\mathbb{N}$ denote the number of assets, let $\\mu \\in \\mathbb{R}^n$ be the vector of expected returns, and let $\\Sigma \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite covariance matrix of returns. Let $u \\in \\mathbb{R}^n$ be a vector of componentwise upper bounds on weights. The portfolio weights are $w \\in \\mathbb{R}^n$. The constraints to be satisfied are:\n- Budget equality: $\\sum_{i=1}^n w_i = 1$.\n- Lower bounds (no short-selling): $w_i \\ge 0$ for all $i \\in \\{1,\\dots,n\\}$.\n- Upper bounds: $w_i \\le u_i$ for all $i \\in \\{1,\\dots,n\\}$.\n- Required expected return: $\\mu^\\top w \\ge R_{\\text{target}}$.\n- Risk cap: $w^\\top \\Sigma w \\le V_{\\max}$.\n\nDefine the inequality functions in the canonical form $g(w) \\le 0$ and the equality function $h(w) = 0$ as\n- $g_{\\text{ret}}(w) = R_{\\text{target}} - \\mu^\\top w$,\n- $g_{\\text{var}}(w) = w^\\top \\Sigma w - V_{\\max}$,\n- $g_{\\text{lo},i}(w) = -w_i$ for each $i \\in \\{1,\\dots,n\\}$,\n- $g_{\\text{up},i}(w) = w_i - u_i$ for each $i \\in \\{1,\\dots,n\\}$,\n- $h_{\\text{bud}}(w) = \\mathbf{1}^\\top w - 1$, where $\\mathbf{1}$ is the vector of ones in $\\mathbb{R}^n$.\n\nFor any penalty parameter $\\rho > 0$, define the merit function\n$$\nM_\\rho(w) \\;=\\; \\rho \\left( \\sum_{i=1}^n \\bigl(\\max\\{0, g_{\\text{lo},i}(w)\\}\\bigr)^2 \\;+\\; \\sum_{i=1}^n \\bigl(\\max\\{0, g_{\\text{up},i}(w)\\}\\bigr)^2 \\;+\\; \\bigl(\\max\\{0, g_{\\text{ret}}(w)\\}\\bigr)^2 \\;+\\; \\bigl(\\max\\{0, g_{\\text{var}}(w)\\}\\bigr)^2 \\;+\\; \\bigl(h_{\\text{bud}}(w)\\bigr)^2 \\right) \\;+\\; \\lambda \\,\\|w\\|_2^2,\n$$\nwhere $\\lambda > 0$ is a fixed regularization parameter and $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nFor a given tolerance $\\tau > 0$, define the maximum constraint violation at any $w$ as\n$$\n\\mathrm{vio}(w) \\;=\\; \\max\\!\\left( \\left| h_{\\text{bud}}(w) \\right|, \\;\\max\\{0, g_{\\text{ret}}(w)\\}, \\;\\max\\{0, g_{\\text{var}}(w)\\}, \\;\\max_{i=1,\\dots,n}\\max\\{0, g_{\\text{lo},i}(w)\\}, \\;\\max_{i=1,\\dots,n}\\max\\{0, g_{\\text{up},i}(w)\\} \\right).\n$$\n\nYour program must, for each test instance below, produce a vector $w$ that approximately minimizes $M_\\rho(w)$ for some $\\rho$ chosen from the sequence $\\{10^1, 10^2, 10^3, 10^4, 10^5, 10^6\\}$ and then report the value of $\\mathrm{vio}(w)$ at the obtained $w$. For each test instance, select the smallest $\\rho$ in the sequence that yields $\\mathrm{vio}(w) \\le \\tau$, if any; if no such $\\rho$ yields $\\mathrm{vio}(w) \\le \\tau$, report the $\\mathrm{vio}(w)$ achieved at the largest $\\rho = 10^6$. Use the fixed tolerance $\\tau = 10^{-6}$ and regularization $\\lambda = 10^{-8}$.\n\nTest suite:\n- Test case A:\n  - $n = 3$,\n  - $\\mu = [\\,0.06,\\; 0.10,\\; 0.14\\,]$,\n  - $\\Sigma = \\begin{bmatrix} 0.010 & 0.002 & 0.001 \\\\ 0.002 & 0.020 & 0.003 \\\\ 0.001 & 0.003 & 0.030 \\end{bmatrix}$,\n  - $u = [\\,0.8,\\; 0.8,\\; 0.8\\,]$,\n  - $R_{\\text{target}} = 0.09$,\n  - $V_{\\max} = 0.025$.\n- Test case B:\n  - $n = 3$,\n  - $\\mu = [\\,0.03,\\; 0.05,\\; 0.07\\,]$,\n  - $\\Sigma = \\begin{bmatrix} 0.008 & 0.001 & 0.0005 \\\\ 0.001 & 0.012 & 0.001 \\\\ 0.0005 & 0.001 & 0.015 \\end{bmatrix}$,\n  - $u = [\\,0.8,\\; 0.8,\\; 0.8\\,]$,\n  - $R_{\\text{target}} = 0.07$,\n  - $V_{\\max} = 0.05$.\n- Test case C:\n  - $n = 4$,\n  - $\\mu = [\\,0.05,\\; 0.08,\\; 0.12,\\; 0.04\\,]$,\n  - $\\Sigma = \\begin{bmatrix}\n  0.005 & 0.001 & 0.001 & 0.0005 \\\\\n  0.001 & 0.010 & 0.002 & 0.001 \\\\\n  0.001 & 0.002 & 0.020 & 0.0015 \\\\\n  0.0005 & 0.001 & 0.0015 & 0.004\n  \\end{bmatrix}$,\n  - $u = [\\,0.6,\\; 0.6,\\; 0.5,\\; 1.0\\,]$,\n  - $R_{\\text{target}} = 0.08$,\n  - $V_{\\max} = 0.012$.\n\nInitial conditions for all test instances: use any deterministic $w^{(0)} \\in \\mathbb{R}^n$; for example, the equal-weight vector $w^{(0)}$ with components $w^{(0)}_i = 1/n$.\n\nYour program must output, in a single line, the maximum constraint violations for the three test cases in the following exact format: a single list with three floating-point numbers rounded to exactly six digits after the decimal point and separated by commas, enclosed in square brackets, in the order of test cases A, B, C. For example, an output line must look like $[v_A,v_B,v_C]$, where each of $v_A$, $v_B$, $v_C$ is a float rounded to six decimal places. No additional text must be printed.", "solution": "The problem requires finding a portfolio weights vector $w \\in \\mathbb{R}^n$ that satisfies a set of linear and quadratic equality and inequality constraints. This is a feasibility problem in computational finance. The proposed method to find such a vector is the penalty method, which transforms the constrained problem into a sequence of unconstrained optimization problems.\n\nThe core of the method is the construction of a merit function, $M_\\rho(w)$, which must be minimized. This function, for a given penalty parameter $\\rho > 0$, is defined as:\n$$\nM_\\rho(w) \\;=\\; \\rho \\cdot P(w) \\;+\\; \\lambda \\,\\|w\\|_2^2\n$$\nwhere $P(w)$ is the penalty term and $\\lambda \\|w\\|_2^2$ is a regularization term. The penalty term aggregates the violations of all constraints:\n$$\nP(w) \\;=\\; \\sum_{j} \\bigl(\\max\\{0, g_j(w)\\}\\bigr)^2 \\;+\\; \\sum_{k} \\bigl(h_k(w)\\bigr)^2\n$$\nHere, $g_j(w) \\le 0$ are the inequality constraints and $h_k(w) = 0$ are the equality constraints. The problem statement explicitly defines these for the portfolio problem:\n- Inequalities: $g_{\\text{ret}}(w) = R_{\\text{target}} - \\mu^\\top w$, $g_{\\text{var}}(w) = w^\\top \\Sigma w - V_{\\max}$, $g_{\\text{lo},i}(w) = -w_i$, and $g_{\\text{up},i}(w) = w_i - u_i$.\n- Equality: $h_{\\text{bud}}(w) = \\mathbf{1}^\\top w - 1$.\n\nThe function $M_\\rho(w)$ is an unconstrained, continuously differentiable ($C^1$) function. Its properties are critical. The functions defining the constraints, $g_j(w)$ and $h_k(w)$, are either affine or, in the case of $g_{\\text{var}}(w)$, convex, because the covariance matrix $\\Sigma$ is positive definite. The function $\\max\\{0, \\cdot\\}$ is convex and non-decreasing. The composition of a convex function with a non-negative, non-decreasing, convex function (like $x \\mapsto x^2$ for $x \\ge 0$) preserves convexity. Therefore, each term $\\bigl(\\max\\{0, g_j(w)\\}\\bigr)^2$ is convex. Similarly, the square of an affine function, $\\bigl(h_k(w)\\bigr)^2$, is convex. As the regularization term $\\lambda \\|w\\|_2^2$ is strongly convex for $\\lambda > 0$, the merit function $M_\\rho(w)$, being a non-negative sum of convex functions including one strongly convex term, is itself strongly convex. This is a crucial property, as it guarantees that for any given $\\rho > 0$, $M_\\rho(w)$ has a unique global minimizer.\n\nTo find this minimizer numerically, we can employ a gradient-based optimization algorithm. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is a suitable choice for this unconstrained $C^1$ minimization problem. For the algorithm to be efficient and accurate, we must provide the analytical gradient of the merit function, $\\nabla M_\\rho(w)$. Using the chain rule, the gradient is:\n$$\n\\nabla M_\\rho(w) = 2\\rho \\left( \\sum_{j} \\max\\{0, g_j(w)\\} \\nabla g_j(w) \\;+\\; \\sum_{k} h_k(w) \\nabla h_k(w) \\right) \\;+\\; 2\\lambda w\n$$\nThe gradients of the individual constraint functions are straightforward to compute:\n- $\\nabla g_{\\text{lo},i}(w) = -e_i$ (where $e_i$ is the $i$-th standard basis vector)\n- $\\nabla g_{\\text{up},i}(w) = e_i$\n- $\\nabla g_{\\text{ret}}(w) = -\\mu$\n- $\\nabla g_{\\text{var}}(w) = 2 \\Sigma w$ (since $\\Sigma$ is symmetric)\n- $\\nabla h_{\\text{bud}}(w) = \\mathbf{1}$ (a vector of ones)\n\nSubstituting these into the expression for $\\nabla M_\\rho(w)$ yields a complete formula for the gradient vector that can be implemented numerically.\n\nThe overall procedure, as specified by the problem, is as follows:\n1. Initialize the portfolio with an equal-weight vector, $w^{(0)}_i = 1/n$.\n2. Iterate through the prescribed sequence of penalty parameters, $\\rho \\in \\{10^1, 10^2, \\dots, 10^6\\}$.\n3. In each iteration, numerically solve the unconstrained minimization problem $w^* = \\arg\\min_w M_\\rho(w)$ using the BFGS algorithm, starting from the solution of the previous iteration (a warm-start strategy).\n4. After finding the optimal $w^*$ for the current $\\rho$, calculate the maximum constraint violation, $\\mathrm{vio}(w^*)$, as defined in the problem statement.\n5. If $\\mathrm{vio}(w^*) \\le \\tau = 10^{-6}$, the process terminates for the current test case, and this violation value is the result.\n6. If the tolerance is not met, continue to the next, larger value of $\\rho$. If the loop completes without meeting the tolerance, the violation from the final step (with $\\rho = 10^6$) is reported.\n\nThis systematic approach ensures that we find a feasible solution with the required precision if one is found within the given sequence of $\\rho$, or we report the \"best-effort\" solution corresponding to the highest penalty. Since the feasible regions for all test cases are non-empty, we expect the algorithm to find a solution satisfying the tolerance $\\tau$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final results.\n    \"\"\"\n    \n    # Global parameters as specified in the problem\n    LAMBDA = 1e-8\n    TAU = 1e-6\n    RHO_SEQUENCE = np.array([1e1, 1e2, 1e3, 1e4, 1e5, 1e6])\n\n    # Test cases data\n    test_cases = [\n        # Case A\n        {\n            \"n\": 3,\n            \"mu\": np.array([0.06, 0.10, 0.14]),\n            \"Sigma\": np.array([[0.010, 0.002, 0.001],\n                               [0.002, 0.020, 0.003],\n                               [0.001, 0.003, 0.030]]),\n            \"u\": np.array([0.8, 0.8, 0.8]),\n            \"R_target\": 0.09,\n            \"V_max\": 0.025,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        },\n        # Case B\n        {\n            \"n\": 3,\n            \"mu\": np.array([0.03, 0.05, 0.07]),\n            \"Sigma\": np.array([[0.008, 0.001, 0.0005],\n                               [0.001, 0.012, 0.001],\n                               [0.0005, 0.001, 0.015]]),\n            \"u\": np.array([0.8, 0.8, 0.8]),\n            \"R_target\": 0.07,\n            \"V_max\": 0.05,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        },\n        # Case C\n        {\n            \"n\": 4,\n            \"mu\": np.array([0.05, 0.08, 0.12, 0.04]),\n            \"Sigma\": np.array([[0.005, 0.001, 0.001, 0.0005],\n                               [0.001, 0.010, 0.002, 0.001],\n                               [0.001, 0.002, 0.020, 0.0015],\n                               [0.0005, 0.001, 0.0015, 0.004]]),\n            \"u\": np.array([0.6, 0.6, 0.5, 1.0]),\n            \"R_target\": 0.08,\n            \"V_max\": 0.012,\n            \"lambda\": LAMBDA,\n            \"tau\": TAU\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        violation = solve_one_case(case_params, RHO_SEQUENCE)\n        # Format to exactly 6 digits after the decimal point\n        results.append(f\"{violation:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef get_constraint_values(w, params):\n    \"\"\"Calculates the values of all constraint functions.\"\"\"\n    mu, Sigma, u, R_target, V_max = params['mu'], params['Sigma'], params['u'], params['R_target'], params['V_max']\n    \n    g_lo = -w\n    g_up = w - u\n    g_ret = R_target - mu.dot(w)\n    g_var = w.dot(Sigma.dot(w)) - V_max\n    h_bud = np.sum(w) - 1.0\n    \n    return g_lo, g_up, g_ret, g_var, h_bud\n\ndef merit_function(w, rho, params):\n    \"\"\"Calculates the value of the merit function M_rho(w).\"\"\"\n    lam = params['lambda']\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    p_lo = np.maximum(0, g_lo)\n    p_up = np.maximum(0, g_up)\n    p_ret = np.maximum(0, g_ret)\n    p_var = np.maximum(0, g_var)\n    \n    penalty_term = np.sum(p_lo**2) + np.sum(p_up**2) + p_ret**2 + p_var**2 + h_bud**2\n    regularization_term = lam * np.sum(w**2)\n    \n    return rho * penalty_term + regularization_term\n\ndef merit_gradient(w, rho, params):\n    \"\"\"Calculates the gradient of the merit function M_rho(w).\"\"\"\n    n, mu, Sigma, lam = params['n'], params['mu'], params['Sigma'], params['lambda']\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    p_lo = np.maximum(0, g_lo)\n    p_up = np.maximum(0, g_up)\n    p_ret = np.maximum(0, g_ret)\n    p_var = np.maximum(0, g_var)\n    \n    # Gradient of penalty term for lo/up bounds\n    grad_bounds = p_up - p_lo # component-wise\n    \n    # Gradient of penalty term for return\n    grad_ret = p_ret * (-mu)\n    \n    # Gradient of penalty term for variance\n    grad_var = p_var * (2 * Sigma.dot(w))\n    \n    # Gradient of penalty term for budget\n    grad_bud = h_bud * np.ones(n)\n    \n    # Combine all gradient components\n    grad = 2 * rho * (grad_bounds + grad_ret + grad_var + grad_bud)\n    \n    # Add gradient of regularization term\n    grad += 2 * lam * w\n    \n    return grad\n\ndef calculate_violation(w, params):\n    \"\"\"Calculates the maximum constraint violation vio(w).\"\"\"\n    g_lo, g_up, g_ret, g_var, h_bud = get_constraint_values(w, params)\n    \n    vio_lo = np.max(np.maximum(0, g_lo))\n    vio_up = np.max(np.maximum(0, g_up))\n    vio_ret = np.maximum(0, g_ret)\n    vio_var = np.maximum(0, g_var)\n    vio_bud = np.abs(h_bud)\n    \n    return np.max([vio_lo, vio_up, vio_ret, vio_var, vio_bud])\n\ndef solve_one_case(params, rho_sequence):\n    \"\"\"Solves a single test case using the penalty method.\"\"\"\n    n = params['n']\n    tau = params['tau']\n    w0 = np.ones(n) / n\n    final_violation = -1.0\n    \n    for rho in rho_sequence:\n        res = minimize(\n            fun=merit_function,\n            x0=w0,\n            args=(rho, params),\n            method='BFGS',\n            jac=merit_gradient,\n            options={'gtol': 1e-9} \n        )\n        \n        w_opt = res.x\n        final_violation = calculate_violation(w_opt, params)\n        \n        if final_violation <= tau:\n            break\n        \n        w0 = w_opt # Warm start for the next iteration\n        \n    return final_violation\n\nif __name__ == '__main__':\n    solve()\n```"}, {"introduction": "Real-world optimization problems often involve a mix of hard boundaries and flexible targets, requiring a hybrid approach. In this exercise, you will model a student's study schedule by combining logarithmic barrier functions for strict time limits with penalty functions for soft constraints like avoiding \"cramming\" and meeting a total study goal. This practice challenges you to synthesize different techniques to solve a nuanced, multi-faceted optimization problem from the ground up. [@problem_id:2374575]", "id": "2374575", "problem": "Consider a time-allocation problem where a student chooses daily study hours to balance learning benefits with the cost of cramming. Let the decision variables be daily study hours $x_t$ for $t \\in \\{1,\\dots,T\\}$, with $x_t \\in (0,M)$ for each day. The student's daily learning benefit is modeled as a concave, increasing function $u_t(x_t) = b_t \\log(1 + x_t)$, where $b_t &gt; 0$ represents a productivity weight for day $t$. Cramming in a day is penalized by a convex \"squared hinge\" penalty of the form $\\rho \\max\\{0, x_t - h\\}^2$, where $h &gt; 0$ is a cramming threshold and $\\rho &gt; 0$ is the penalty weight. \n\nTo enforce the domain $x_t \\in (0,M)$ for each $t$, use a logarithmic barrier with parameter $\\mu &gt; 0$; to encourage a total study target of $\\sum_{t=1}^T x_t \\approx S$, use a quadratic penalty with weight $\\eta &gt; 0$. Define the unconstrained minimization objective\n$$\nf(x) \\;=\\; -\\sum_{t=1}^T b_t \\log(1 + x_t) \\;+\\; \\rho \\sum_{t=1}^T \\max\\{0, x_t - h\\}^2 \\;-\\; \\mu \\sum_{t=1}^T \\left[\\log x_t + \\log(M - x_t)\\right] \\;+\\; \\eta\\left(\\sum_{t=1}^T x_t - S\\right)^2,\n$$\nand solve\n$$\n\\min_{x \\in (0,M)^T} \\; f(x).\n$$\n\nYour task is to write a complete, runnable program that, for each test case below, approximately minimizes $f(x)$ with respect to $x = (x_1,\\dots,x_T)$ and returns the resulting vector of daily hours. The program must implement a gradient-based method with a line search that maintains feasibility $x_t \\in (0,M)$ by staying strictly inside the bounds and should terminate when the infinity norm of the gradient is below $10^{-6}$ or when a cap of $20000$ iterations is reached, whichever happens first. For numerical stability, you may assume an interior margin, i.e., enforce $x_t \\in [\\varepsilon, M - \\varepsilon]$ with a small $\\varepsilon \\in (0,10^{-6}]$.\n\nFor each test case, output the optimized vector $x^\\star$ rounded to $4$ decimal places. The final output must be a single line containing a list of lists, one inner list per test case, in the same order as provided. No extra text should be printed.\n\nTest suite (each case specifies $(T, b, S, M, h, \\mu, \\rho, \\eta)$):\n- Case $1$: $T = 7$, $b = [1,1,1,1,1,1,1]$, $S = 21$, $M = 10$, $h = 4$, $\\mu = 10^{-3}$, $\\rho = 1.0$, $\\eta = 5.0$.\n- Case $2$: $T = 5$, $b = [1.0,1.5,0.8,1.2,0.7]$, $S = 25$, $M = 10$, $h = 5$, $\\mu = 10^{-3}$, $\\rho = 0.5$, $\\eta = 10.0$.\n- Case $3$: $T = 4$, $b = [1,1,1,1]$, $S = 35$, $M = 10$, $h = 6$, $\\mu = 10^{-6}$, $\\rho = 0.1$, $\\eta = 2.0$.\n- Case $4$: $T = 6$, $b = [0.9,1.4,0.6,1.1,1.3,0.7]$, $S = 30$, $M = 12$, $h = 4$, $\\mu = 10^{-3}$, $\\rho = 5.0$, $\\eta = 15.0$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a list of lists with each inner list holding the $T$ optimized daily hours rounded to $4$ decimal places, for example, $[[x_{1,1},\\dots,x_{1,T_1}],[x_{2,1},\\dots],\\dots]$ where $x_{i,j}$ denotes day $j$ in test case $i$. No spaces are required, but they are allowed.", "solution": "The problem as stated is valid. It is a well-posed optimization problem grounded in established mathematical principles. The objective is to minimize a function $f(x)$ over a convex domain $x \\in (0,M)^T$. The function $f(x)$ is a sum of several terms. Let us analyze each.\nThe learning benefit term, $-\\sum_{t=1}^T b_t \\log(1 + x_t)$, is convex because $\\log(z)$ is a concave function, making $-\\log(z)$ convex, and the sum of convex functions remains convex ($b_t > 0$).\nThe cramming penalty, $\\rho \\sum_{t=1}^T \\max\\{0, x_t - h\\}^2$, is convex. The function $g(z) = z^2$ is convex and non-decreasing for $z \\ge 0$, and $h(z) = \\max\\{0, z\\}$ is convex. The composition $g(h(z))$ is therefore convex.\nThe logarithmic barrier term, $-\\mu \\sum_{t=1}^T [\\log x_t + \\log(M - x_t)]$, is a standard, strictly convex barrier function used to enforce the domain constraints.\nThe total study target penalty, $\\eta(\\sum_{t=1}^T x_t - S)^2$, is convex, as it is the composition of the convex quadratic function $z^2$ with an affine function.\nSince the objective function $f(x)$ is a sum of convex functions, and the logarithmic barrier term ensures strict convexity, $f(x)$ is a strictly convex function. The minimization of a strictly convex function over a convex set admits a unique solution. The problem is therefore scientifically sound and well-posed.\n\nTo solve this unconstrained minimization problem, we will implement a gradient descent algorithm with a backtracking line search. This method is appropriate because the objective function $f(x)$ is continuously differentiable over its domain $(0,M)^T$.\n\nThe core principle of gradient descent is to iteratively update the current solution $x^{(k)}$ by taking a step in the direction of the negative gradient, which is the direction of steepest descent of the objective function. The update rule is:\n$$\nx^{(k+1)} = x^{(k)} + s_k p^{(k)}\n$$\nwhere $p^{(k)} = -\\nabla f(x^{(k)})$ is the search direction and $s_k > 0$ is the step size.\n\nThe objective function is given by:\n$$\nf(x) = -\\sum_{t=1}^T b_t \\log(1 + x_t) + \\rho \\sum_{t=1}^T \\max\\{0, x_t - h\\}^2 - \\mu \\sum_{t=1}^T \\left[\\log x_t + \\log(M - x_t)\\right] + \\eta\\left(\\sum_{t=1}^T x_t - S\\right)^2\n$$\nTo compute the search direction, we must first derive the gradient of $f(x)$, $\\nabla f(x)$. The $t$-th component of the gradient, $\\frac{\\partial f}{\\partial x_t}$, is the sum of the partial derivatives of each term:\n$$\n(\\nabla f(x))_t = \\frac{\\partial}{\\partial x_t}f(x) = -\\frac{b_t}{1 + x_t} + 2\\rho \\max\\{0, x_t - h\\} - \\mu \\left( \\frac{1}{x_t} - \\frac{1}{M - x_t} \\right) + 2\\eta \\left(\\sum_{j=1}^T x_j - S\\right)\n$$\nThe derivative of the squared hinge term $\\max\\{0, x_t - h\\}^2$ with respect to $x_t$ is $2 \\max\\{0, x_t - h\\}$, which is continuous for all $x_t$.\n\nThe step size $s_k$ is determined by a backtracking line search procedure. This is critical for two reasons: ensuring a sufficient decrease in the objective function value at each step and maintaining feasibility, i.e., keeping $x^{(k+1)}$ within the domain $(0,M)^T$. The line search starts with an initial step size (e.g., $s=1$) and iteratively reduces it by a factor $\\beta \\in (0,1)$ until two conditions are met:\n1.  **Feasibility**: The new point $x_{new} = x^{(k)} + s p^{(k)}$ must be strictly within the domain. For numerical stability, we enforce $x_t \\in [\\varepsilon, M - \\varepsilon]$ for a small constant $\\varepsilon > 0$.\n2.  **Armijo Condition**: The step must provide a sufficient decrease in the objective function. This is verified using the condition:\n    $$\n    f(x^{(k)} + s p^{(k)}) \\le f(x^{(k)}) + \\alpha s (\\nabla f(x^{(k)}))^T p^{(k)}\n    $$\n    where $\\alpha \\in (0, 0.5)$ is a control parameter. Since $p^{(k)} = -\\nabla f(x^{(k)})$, the condition simplifies to $f(x_{new}) \\le f(x^{(k)}) - \\alpha s ||\\nabla f(x^{(k)})||_2^2$.\n\nThe algorithm proceeds as follows:\n1.  Initialize $x^{(0)}$ to a feasible point, for instance, $x_t^{(0)} = S/T$ for all $t=1,...,T$, clipped to the feasible range $[\\varepsilon, M-\\varepsilon]$. Set iteration counter $k=0$.\n2.  For $k=0, 1, 2, \\dots$ up to a maximum number of iterations:\n    a.  Compute the gradient $\\nabla f(x^{(k)})$.\n    b.  Check for convergence: if the infinity norm $||\\nabla f(x^{(k)})||_\\infty$ is below a tolerance $\\tau$ (e.g., $10^{-6}$), terminate and return $x^{(k)}$.\n    c.  Set the search direction $p^{(k)} = -\\nabla f(x^{(k)})$.\n    d.  Perform backtracking line search to find a suitable step size $s_k$.\n    e.  Update the solution: $x^{(k+1)} = x^{(k)} + s_k p^{(k)}$.\n3.  If the maximum number of iterations is reached, the algorithm terminates and returns the last computed solution.\nThis procedure guarantees convergence to the unique minimizer of the strictly convex objective function.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    \n    # --- Algorithm Parameters ---\n    MAX_ITER = 20000\n    TOL = 1e-6\n    # Epsilon for numerical stability at boundaries\n    EPS = 1e-7\n    # Backtracking line search parameters\n    ALPHA = 0.3\n    BETA = 0.8\n    # --------------------------\n\n    def objective_function(x, b, S, M, h, mu, rho, eta):\n        \"\"\"\n        Computes the value of the objective function f(x).\n        \"\"\"\n        # Barrier terms implicitly handle domain, but we can return inf for robustness\n        if np.any(x <= 0) or np.any(x >= M):\n            return np.inf\n            \n        term1_benefit = -np.sum(b * np.log(1 + x))\n        term2_cramming = rho * np.sum(np.maximum(0, x - h)**2)\n        term3_barrier = -mu * np.sum(np.log(x) + np.log(M - x))\n        term4_target = eta * (np.sum(x) - S)**2\n        \n        return term1_benefit + term2_cramming + term3_barrier + term4_target\n\n    def gradient(x, b, S, M, h, mu, rho, eta):\n        \"\"\"\n        Computes the gradient of the objective function f(x).\n        \"\"\"\n        grad = np.zeros_like(x)\n        \n        grad_term1 = -b / (1 + x)\n        grad_term2 = 2 * rho * np.maximum(0, x - h)\n        grad_term3 = -mu * (1/x - 1/(M - x))\n        grad_term4 = 2 * eta * (np.sum(x) - S)\n        \n        grad = grad_term1 + grad_term2 + grad_term3 + grad_term4\n        return grad\n\n    def optimize_case(T, b, S, M, h, mu, rho, eta):\n        \"\"\"\n        Performs gradient descent with backtracking line search for a single case.\n        \"\"\"\n        # Initialize x to a feasible starting point\n        x = np.full(T, S / T)\n        x = np.clip(x, EPS, M - EPS)\n        \n        for k in range(MAX_ITER):\n            grad_x = gradient(x, b, S, M, h, mu, rho, eta)\n            \n            # Check for convergence using the infinity norm of the gradient\n            if np.linalg.norm(grad_x, ord=np.inf) < TOL:\n                break\n                \n            # Set search direction (steepest descent)\n            p = -grad_x\n            \n            # --- Backtracking Line Search ---\n            s = 1.0\n            f_x = objective_function(x, b, S, M, h, mu, rho, eta)\n            grad_dot_p = np.dot(grad_x, p)\n\n            while True:\n                x_new = x + s * p\n                \n                # 1. Feasibility Check (stay within stable interior domain)\n                if np.any(x_new <= EPS) or np.any(x_new >= M - EPS):\n                    s *= BETA\n                    continue\n                \n                # 2. Armijo Condition (sufficient decrease)\n                f_x_new = objective_function(x_new, b, S, M, h, mu, rho, eta)\n                if f_x_new <= f_x + ALPHA * s * grad_dot_p:\n                    break  # Step size is acceptable\n                \n                s *= BETA\n                # Failsafe to prevent excessive shrinking of step size\n                if s < 1e-15:\n                    break\n            \n            if s < 1e-15:\n                # If step size becomes too small, terminate to avoid stalling\n                break\n\n            # Update solution\n            x = x + s * p\n            \n        return x\n\n    # --- Test Cases ---\n    test_cases = [\n        # (T, b, S, M, h, mu, rho, eta)\n        (7, np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]), 21.0, 10.0, 4.0, 1e-3, 1.0, 5.0),\n        (5, np.array([1.0, 1.5, 0.8, 1.2, 0.7]), 25.0, 10.0, 5.0, 1e-3, 0.5, 10.0),\n        (4, np.array([1.0, 1.0, 1.0, 1.0]), 35.0, 10.0, 6.0, 1e-6, 0.1, 2.0),\n        (6, np.array([0.9, 1.4, 0.6, 1.1, 1.3, 0.7]), 30.0, 12.0, 4.0, 1e-3, 5.0, 15.0),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T, b, S, M, h, mu, rho, eta = case\n        x_star = optimize_case(T, b, S, M, h, mu, rho, eta)\n        # Round the final vector to 4 decimal places and convert to a list\n        rounded_result = list(np.round(x_star, 4))\n        all_results.append(rounded_result)\n\n    # Format the final output string as a list of lists.\n    # str() on a list produces the desired bracketed, comma-separated format.\n    # Ex: str([1.23, 4.56]) -> '[1.23, 4.56]'\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```"}]}