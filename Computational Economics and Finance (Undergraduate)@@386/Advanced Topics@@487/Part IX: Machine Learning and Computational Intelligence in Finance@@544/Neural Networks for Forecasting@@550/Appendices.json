{"hands_on_practices": [{"introduction": "Before we can train and deploy complex neural networks, we must first understand their fundamental mechanics. This exercise guides you through the core computation of a simple Recurrent Neural Network (RNN) in a hypothetical financial forecasting scenario. By manually implementing the forward pass, you will gain a concrete understanding of how an RNN processes sequential data, updates its internal memory (its hidden state), and generates a prediction, demystifying the operations that occur within each time step [@problem_id:2387285].", "id": "2387285", "problem": "Consider a binary forecasting task in a financial context. For each equity, you observe a discrete-time sequence of engagement features over a fixed horizon. At each time step $t \\in \\{1,\\dots,T\\}$, the feature vector is $x_t \\in \\mathbb{R}^2$ with components $x_t = (v_t, s_t)$, where $v_t$ is a standardized comment-velocity measure and $s_t$ is a standardized sentiment score. The goal is to produce, for each sequence, a single probability in $(0,1)$ that the equity will transition into a high-attention regime in the next time step, interpreted here as a prospective \"meme stock\" event.\n\nYou are given a forecasting rule defined as follows. Let the hidden state dimension be $d = 3$, and let the parameters be\n$$\nW_{xh} = \\begin{bmatrix}\n0.8 & -0.4 \\\\\n0.3 & 0.5 \\\\\n-0.2 & 0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1 & 0.0 & -0.3 \\\\\n0.2 & 0.4 & 0.0 \\\\\n-0.5 & 0.1 & 0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix},\n$$\n$$\nW_{hy} = \\begin{bmatrix}\n0.6 & -0.2 & 0.4\n\\end{bmatrix},\\quad\nb_y = -0.05.\n$$\nDefine the hidden state recursion with initial condition $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$ and, for $t=1,\\dots,T$,\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right),\n$$\nwhere $\\tanh(\\cdot)$ acts elementwise. After processing the full sequence, form the logit\n$$\nz = W_{hy}\\,h_T + b_y,\n$$\nand the forecast probability\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}.\n$$\n\nTest suite. For each of the following sequences, you must compute the corresponding probability $p$ given by the rule above.\n\n- Case A (typical rising engagement and positive sentiment; $T=4$):\n  $$\n  \\left[(0.1, 0.4),\\ (0.2, 0.5),\\ (0.3, 0.6),\\ (0.4, 0.7)\\right].\n  $$\n\n- Case B (near-neutral dynamics around zero; $T=4$):\n  $$\n  \\left[(0.0, 0.0),\\ (0.0, 0.1),\\ (0.0, -0.1),\\ (0.0, 0.0)\\right].\n  $$\n\n- Case C (high velocity with negative sentiment; $T=4$):\n  $$\n  \\left[(0.5, -0.6),\\ (0.6, -0.5),\\ (0.7, -0.4),\\ (0.8, -0.3)\\right].\n  $$\n\n- Case D (oscillatory features, including negative velocity; $T=4$):\n  $$\n  \\left[(-0.2, 0.9),\\ (0.2, -0.9),\\ (-0.1, 0.8),\\ (0.1, -0.8)\\right].\n  $$\n\nYour program must output a single line containing the list of the four probabilities corresponding to Cases A, B, C, and D, in this order. Each probability must be rounded to exactly $6$ decimal places. The output format must be exactly a comma-separated list enclosed in square brackets, for example:\n$$\n[\\text{p\\_A},\\text{p\\_B},\\text{p\\_C},\\text{p\\_D}]\n$$\nwhere $\\text{p\\_A}$, $\\text{p\\_B}$, $\\text{p\\_C}$, and $\\text{p\\_D}$ are the rounded decimal representations of $p$ for Cases A through D, respectively.", "solution": "The problem statement is subjected to validation and found to be valid. It is scientifically grounded, well-posed, objective, and self-contained. It describes a standard discrete-time recurrent neural network (RNN), a fundamental model in computational science, and provides all necessary parameters, initial conditions, and input data for a deterministic calculation. There are no contradictions, ambiguities, or violations of scientific principles. We may, therefore, proceed with the solution.\n\nThe problem requires the computation of a forecast probability, $p$, based on a sequence of feature vectors. The model is a simple RNN with a hidden state dimension of $d=3$. For each discrete time step $t$ from $1$ to a fixed horizon $T$, an input feature vector $x_t \\in \\mathbb{R}^2$ is processed. The feature vector is composed of two components, $x_t = (v_t, s_t)$, representing comment velocity and sentiment score, respectively.\n\nThe core of the model is the hidden state recursion. The hidden state vector, $h_t \\in \\mathbb{R}^3$, evolves over time according to the equation:\n$$\nh_t = \\tanh\\!\\left(W_{xh}\\,x_t + W_{hh}\\,h_{t-1} + b_h\\right)\n$$\nThis calculation is performed for $t = 1, \\dots, T$. The initial hidden state is given as a zero vector, $h_0 = \\mathbf{0} \\in \\mathbb{R}^3$. The function $\\tanh(\\cdot)$ is the hyperbolic tangent, applied element-wise to the vector argument. The parameters are the weight matrices $W_{xh} \\in \\mathbb{R}^{3 \\times 2}$ and $W_{hh} \\in \\mathbb{R}^{3 \\times 3}$, and a bias vector $b_h \\in \\mathbb{R}^{3}$. Their values are provided as:\n$$\nW_{xh} = \\begin{bmatrix}\n0.8 & -0.4 \\\\\n0.3 & 0.5 \\\\\n-0.2 & 0.7\n\\end{bmatrix},\\quad\nW_{hh} = \\begin{bmatrix}\n0.1 & 0.0 & -0.3 \\\\\n0.2 & 0.4 & 0.0 \\\\\n-0.5 & 0.1 & 0.2\n\\end{bmatrix},\\quad\nb_h = \\begin{bmatrix}\n0.05 \\\\ -0.1 \\\\ 0.0\n\\end{bmatrix}\n$$\n\nAfter processing the entire input sequence of length $T$, the final hidden state, $h_T$, is used to compute a scalar logit value, $z$. This represents the input to the final activation function. The calculation for the logit is:\n$$\nz = W_{hy}\\,h_T + b_y\n$$\nwhere $W_{hy} \\in \\mathbb{R}^{1 \\times 3}$ is a weight matrix (or vector) and $b_y \\in \\mathbb{R}$ is a scalar bias. The provided values are:\n$$\nW_{hy} = \\begin{bmatrix}\n0.6 & -0.2 & 0.4\n\\end{bmatrix},\\quad\nb_y = -0.05\n$$\n\nFinally, the forecast probability, $p$, is obtained by applying the sigmoid function, $\\sigma(\\cdot)$, to the logit $z$. The sigmoid function squashes the logit into the interval $(0, 1)$, which is required for a probability.\n$$\np = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\n\nThe computational procedure for each test case is as follows:\n$1$. Initialize the hidden state as a zero vector: $h \\leftarrow \\begin{bmatrix} 0 & 0 & 0 \\end{bmatrix}^T$.\n$2$. For each time step $t$ from $1$ to $T=4$:\n    a. Take the input vector $x_t$ from the given sequence.\n    b. Compute the argument of the activation function: $u_t = W_{xh}\\,x_t + W_{hh}\\,h + b_h$.\n    c. Update the hidden state: $h \\leftarrow \\tanh(u_t)$.\n$3$. After the loop completes, $h$ now holds the value of $h_T$.\n$4$. Compute the logit: $z = W_{hy}\\,h + b_y$.\n$5$. Compute the final probability: $p = 1 / (1 + \\exp(-z))$.\n\nThis deterministic procedure is applied to the four specified input sequences (Case A, B, C, and D), each of length $T=4$. The resulting four probabilities are then collected and formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes forecast probabilities for four equity engagement sequences using a\n    specified Recurrent Neural Network (RNN) model.\n    \"\"\"\n    \n    # Define the parameters of the RNN model.\n    # All vectors are defined as column vectors for correct matrix algebra.\n    W_xh = np.array([\n        [0.8, -0.4],\n        [0.3, 0.5],\n        [-0.2, 0.7]\n    ])\n    W_hh = np.array([\n        [0.1, 0.0, -0.3],\n        [0.2, 0.4, 0.0],\n        [-0.5, 0.1, 0.2]\n    ])\n    b_h = np.array([[0.05], [-0.1], [0.0]])\n    W_hy = np.array([[0.6, -0.2, 0.4]])\n    b_y = -0.05\n\n    # Define the test cases from the problem statement.\n    # The order of cases is fixed to A, B, C, D.\n    test_cases = [\n        # Case A: Typical rising engagement and positive sentiment\n        [(0.1, 0.4), (0.2, 0.5), (0.3, 0.6), (0.4, 0.7)],\n        # Case B: Near-neutral dynamics around zero\n        [(0.0, 0.0), (0.0, 0.1), (0.0, -0.1), (0.0, 0.0)],\n        # Case C: High velocity with negative sentiment\n        [(0.5, -0.6), (0.6, -0.5), (0.7, -0.4), (0.8, -0.3)],\n        # Case D: Oscillatory features\n        [(-0.2, 0.9), (0.2, -0.9), (-0.1, 0.8), (0.1, -0.8)]\n    ]\n\n    results = []\n    \n    # Sigmoid function for final probability calculation.\n    def sigmoid(z):\n        return 1 / (1 + np.exp(-z))\n\n    for sequence in test_cases:\n        # Initialize hidden state h_0 = [0, 0, 0]^T.\n        # Dimension is (3, 1) to represent a column vector.\n        h = np.zeros((3, 1))\n\n        # Iterate through the time steps of the sequence.\n        for x_tuple in sequence:\n            # Reshape input tuple into a (2, 1) column vector.\n            x_t = np.array(x_tuple).reshape(2, 1)\n            \n            # Apply the RNN recurrence relation.\n            # h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)\n            h = np.tanh(W_xh @ x_t + W_hh @ h + b_h)\n        \n        # After the loop, h is the final hidden state h_T.\n        # Compute the logit z = W_hy * h_T + b_y.\n        # The result of matrix multiplication is a 1x1 array; .item() extracts the scalar.\n        z = (W_hy @ h + b_y).item()\n        \n        # Compute the final probability p = sigma(z).\n        p = sigmoid(z)\n        \n        results.append(p)\n\n    # Format the results to exactly 6 decimal places and create the output string.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "A powerful model is not just one that fits the data well, but one that learns the true underlying patterns without memorizing noise. This practice explores regularization, a critical technique for controlling model complexity and preventing overfitting, using a linear network to isolate the effects [@problem_id:2414325]. You will implement and compare $L_1$ and $L_2$ regularization, discovering firsthand how the $L_1$ penalty uniquely encourages sparsity, effectively performing automatic feature selection—a vital property for building parsimonious and interpretable financial models.", "id": "2414325", "problem": "You are asked to formalize and implement a controlled comparison of the effects of $L_1$ versus $L_2$ regularization on feature sparsity in a neural-network-inspired linear forecasting model for corporate earnings. The model is a single-layer linear network (a linear neuron), which is a special case of a neural network, used here to forecast a standardized target representing normalized corporate earnings. The fundamental base of the design is empirical risk minimization with mean squared error under convex regularization. Begin from the following foundations.\n\n1. Empirical risk minimization with mean squared error: given input matrix $X \\in \\mathbb{R}^{n \\times d}$ and target vector $y \\in \\mathbb{R}^n$, choose weights $w \\in \\mathbb{R}^d$ to minimize\n$$\n\\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\, \\mathcal{R}(w),\n$$\nwhere $n$ is the number of samples, $d$ is the number of features, $\\lambda \\ge 0$ is the regularization strength, and $\\mathcal{R}$ is a penalty functional.\n\n2. Definitions of $L_1$ and $L_2$ regularization: for $L_1$, define $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d \\lvert w_j \\rvert$; for $L_2$, define $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2 = \\frac{1}{2}\\sum_{j=1}^d w_j^2$.\n\n3. Feature sparsity is quantified by the support size, defined as the number of indices $j \\in \\{1,\\dots,d\\}$ such that $\\lvert w_j \\rvert$ exceeds a small numerical threshold $\\tau > 0$.\n\nConstruct a deterministic design that emulates commonly used financial predictors for earnings forecasting by using smoothly varying and correlated basis functions.\n\nA. Data construction (deterministic, no randomness):\n- Let $n = 64$ and $t = 0,1,2,\\dots,n-1$.\n- Define four base predictors using trigonometric functions:\n  - $b_1(t) = \\cos\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$,\n  - $b_2(t) = \\sin\\!\\left(2\\pi \\cdot 1 \\cdot t / n\\right)$,\n  - $b_3(t) = \\cos\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$,\n  - $b_4(t) = \\sin\\!\\left(2\\pi \\cdot 2 \\cdot t / n\\right)$.\n- Standardize each $b_j$ to zero mean and unit variance to obtain $s_j$, $j \\in \\{1,2,3,4\\}$.\n- Define the target (normalized corporate earnings) as a linear combination:\n  $$\n  y = 0.8\\, s_1 + 1.2\\, s_3 - 1.5\\, s_4.\n  $$\n- To induce realistic multicollinearity, define six additional predictors as mixtures of the standardized bases with a small orthogonal perturbation. Let $q(t) = \\cos\\!\\left(2\\pi \\cdot 5 \\cdot t / n\\right)$ and standardize to $s_q$. Then define:\n  - $f_1 = s_1$,\n  - $f_2 = s_2$,\n  - $f_3 = s_3$,\n  - $f_4 = s_4$,\n  - $f_5 = 0.8\\, s_1 + 0.2\\, s_2 + 0.01\\, s_q$,\n  - $f_6 = 0.5\\, s_2 - 0.3\\, s_3 + 0.01\\, s_q$,\n  - $f_7 = 0.1\\, s_1 + 0.9\\, s_4 + 0.01\\, s_q$,\n  - $f_8 = 0.3\\, s_3 + 0.4\\, s_4 + 0.3\\, s_2 + 0.01\\, s_q$,\n  - $f_9 = 0.6\\, s_1 - 0.1\\, s_3 + 0.01\\, s_q$,\n  - $f_{10} = 0.2\\, s_2 + 0.2\\, s_3 + 0.6\\, s_4 + 0.01\\, s_q$.\n- Standardize each $f_j$ to zero mean and unit variance to form the final design columns $X_{\\cdot j}$, $j \\in \\{1,\\dots,10\\}$. Standardize $y$ to zero mean (it already is, but enforce numerically). This yields $X \\in \\mathbb{R}^{64 \\times 10}$ and $y \\in \\mathbb{R}^{64}$.\n\nB. Solvers to implement:\n- For $L_2$ regularization (ridge), solve the first-order optimality condition in closed form by solving the linear system\n$$\n\\left(\\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y,\n$$\nwhere $I_d$ is the $d \\times d$ identity matrix.\n- For $L_1$ regularization (lasso), implement cyclic coordinate descent with soft-thresholding. For each coordinate $j \\in \\{1,\\dots,d\\}$, define\n$$\nH_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 \\quad \\text{and} \\quad \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right),\n$$\nand update\n$$\nw_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j,\\lambda)}{H_j}, \\quad \\text{where} \\quad \\operatorname{soft}(a,\\lambda) = \\operatorname{sign}(a)\\,\\max\\{ \\lvert a \\rvert - \\lambda, \\, 0 \\}.\n$$\nIterate coordinates cyclically until the $\\ell_\\infty$ change in $w$ across a full pass is below a tolerance $\\varepsilon > 0$.\n\nC. Feature sparsity metric:\n- Define the support threshold as $\\tau = 10^{-6}$.\n- The support size $s(w)$ is the number of indices $j$ such that $\\lvert w_j \\rvert > \\tau$.\n\nYou must:\n1. Construct $X$ and $y$ as prescribed.\n2. Implement the $L_2$ (ridge) solver and the $L_1$ (lasso) solver as above.\n3. For each test case below, compute the learned $w$ and report $s(w)$.\n\nTest suite (five cases):\n- Case $1$: $L_2$ with $\\lambda = 0.0$.\n- Case $2$: $L_2$ with $\\lambda = 10.0$.\n- Case $3$: $L_1$ with $\\lambda = 0.0$.\n- Case $4$: $L_1$ with $\\lambda = 0.5$.\n- Case $5$: $L_1$ with $\\lambda = 2.2$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases. Specifically, output\n$$\n[ s(w^{(1)}), \\, s(w^{(2)}), \\, s(w^{(3)}), \\, s(w^{(4)}), \\, s(w^{(5)}) ].\n$$\nOnly print this list, with no additional text. Angles are in radians; no physical units apply; all numerical answers must be integers. The program must be fully deterministic and require no user input.", "solution": "The problem statement presented is a well-posed and scientifically sound exercise in computational statistics and its application to financial modeling. It is self-contained, with all necessary parameters and procedures defined, and is free of contradictions or ambiguities. The problem is therefore deemed **valid**, and I will proceed with a full solution.\n\nThe task is to compare the sparsity-inducing effects of $L_1$ versus $L_2$ regularization in a linear forecasting model. This will be accomplished by constructing a synthetic dataset with controlled multicollinearity, solving the regularized regression problems, and quantifying the sparsity of the resulting weight vectors.\n\n**Part A: Data Construction**\n\nThe first step is the deterministic generation of the design matrix $X \\in \\mathbb{R}^{64 \\times 10}$ and the target vector $y \\in \\mathbb{R}^{64}$. The number of samples is $n=64$ and the number of features is $d=10$.\n\n1.  A time index vector is defined as $t = [0, 1, \\dots, n-1]$.\n2.  Four orthogonal base predictors are generated using trigonometric functions over the discrete time index $t$:\n    $$ b_1(t) = \\cos(2\\pi t / n), \\quad b_2(t) = \\sin(2\\pi t / n), \\quad b_3(t) = \\cos(4\\pi t / n), \\quad b_4(t) = \\sin(4\\pi t / n) $$\n3.  Each base predictor vector $b_j$ is standardized to have zero mean and unit variance. The standardization of a vector $v$ is given by $(v - \\mu_v) / \\sigma_v$, where $\\mu_v$ is the mean and $\\sigma_v$ is the population standard deviation. Let the standardized vectors be denoted $s_1, s_2, s_3, s_4$.\n4.  The target vector $y$, representing normalized corporate earnings, is constructed as a known linear combination of three of these standardized bases:\n    $$ y = 0.8 s_1 + 1.2 s_3 - 1.5 s_4 $$\n    By construction, $y$ has a mean of zero, so no further mean-centering is required.\n5.  To simulate multicollinearity common in economic data, ten features $f_1, \\dots, f_{10}$ are constructed. The first four are the base predictors themselves. The subsequent six are linear combinations of the bases, with a small orthogonal perturbation added to prevent perfect linear dependence. The perturbation is derived from $q(t) = \\cos(10\\pi t / n)$, which is standardized to a vector $s_q$. The features are defined as:\n    \\begin{align*}\n    f_1 &= s_1 \\\\\n    f_2 &= s_2 \\\\\n    f_3 &= s_3 \\\\\n    f_4 &= s_4 \\\\\n    f_5 &= 0.8 s_1 + 0.2 s_2 + 0.01 s_q \\\\\n    f_6 &= 0.5 s_2 - 0.3 s_3 + 0.01 s_q \\\\\n    f_7 &= 0.1 s_1 + 0.9 s_4 + 0.01 s_q \\\\\n    f_8 &= 0.3 s_3 + 0.4 s_4 + 0.3 s_2 + 0.01 s_q \\\\\n    f_9 &= 0.6 s_1 - 0.1 s_3 + 0.01 s_q \\\\\n    f_{10} &= 0.2 s_2 + 0.2 s_3 + 0.6 s_4 + 0.01 s_q\n    \\end{align*}\n6.  Finally, each feature vector $f_j$ is standardized to have zero mean and unit variance. These ten standardized vectors form the columns of the final design matrix $X$.\n\n**Part B: Solvers**\n\nThe problem requires implementing two solvers for the general empirical risk minimization problem:\n$$ \\min_{w \\in \\mathbb{R}^d} \\left\\{ \\frac{1}{2n} \\lVert y - X w \\rVert_2^2 + \\lambda \\mathcal{R}(w) \\right\\} $$\n\n**$L_2$ Regularization (Ridge Regression)**\n\nFor $L_2$ regularization, the penalty is $\\mathcal{R}(w) = \\frac{1}{2} \\lVert w \\rVert_2^2$. The objective function is strictly convex for any $\\lambda > 0$, and the gradient is:\n$$ \\nabla_w J(w) = \\frac{1}{n} X^\\top(Xw - y) + \\lambda w $$\nSetting the gradient to zero, $\\nabla_w J(w) = 0$, yields the first-order optimality condition:\n$$ \\frac{1}{n} X^\\top X w - \\frac{1}{n} X^\\top y + \\lambda w = 0 $$\n$$ \\left( \\frac{1}{n} X^\\top X + \\lambda I_d \\right) w = \\frac{1}{n} X^\\top y $$\nThis is the linear system specified in the problem statement, where $I_d$ is the $d \\times d$ identity matrix. This system can be solved directly using standard linear algebra routines. For the case $\\lambda = 0$, this reduces to the normal equations for Ordinary Least Squares (OLS).\n\n**$L_1$ Regularization (Lasso)**\n\nFor $L_1$ regularization, the penalty is $\\mathcal{R}(w) = \\lVert w \\rVert_1 = \\sum_{j=1}^d |w_j|$. The objective function is convex but not differentiable at points where any $w_j = 0$. A solution is found using an iterative method, specifically cyclic coordinate descent.\n\nThe algorithm updates one weight coordinate $w_j$ at a time, holding all other weights $w_k$ ($k \\neq j$) fixed. The a subgradient optimality condition for coordinate $j$ leads to the update rule:\n$$ w_j \\leftarrow \\frac{\\operatorname{soft}(\\rho_j, \\lambda)}{H_j} $$\nwhere the soft-thresholding operator is $\\operatorname{soft}(a, \\lambda) = \\operatorname{sign}(a) \\max(|a| - \\lambda, 0)$, and\n$$ \\rho_j = \\frac{1}{n} \\sum_{i=1}^n X_{ij} \\left( y_i - \\sum_{k \\ne j} X_{ik} w_k \\right), \\quad H_j = \\frac{1}{n}\\sum_{i=1}^n X_{ij}^2 $$\nSince each column of the design matrix $X$ is standardized to have unit variance, $H_j = 1$ for all $j \\in \\{1, \\dots, d\\}$. The update rule simplifies to:\n$$ w_j \\leftarrow \\operatorname{soft}(\\rho_j, \\lambda) $$\nThe algorithm cycles through all coordinates $j=1, \\dots, d$ repeatedly until the change in the weight vector $w$ between successive full passes falls below a small tolerance, $\\varepsilon$. For this implementation, a tolerance of $\\varepsilon = 10^{-9}$ on the $\\ell_\\infty$-norm of the change in $w$ is used. For the case $\\lambda = 0$, the soft-thresholding operator becomes the identity function, $\\operatorname{soft}(\\rho_j, 0) = \\rho_j$, and the algorithm becomes the Gauss-Seidel method for solving the OLS normal equations.\n\n**Part C: Feature Sparsity**\n\nSparsity is measured by the support size of the weight vector $w$. The support size $s(w)$ is the number of weights whose absolute value is greater than a numerical threshold $\\tau = 10^{-6}$:\n$$ s(w) = |\\{j \\in \\{1, \\dots, d\\} : |w_j| > \\tau \\}| $$\nThis metric will be computed for the solution vector $w$ obtained from each of the five test cases specified.\n\nThe implementation will now proceed by first generating the data, then applying the specified solvers for each test case, and finally computing the support size for each resulting weight vector.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Formalizes and implements a controlled comparison of L1 and L2 regularization\n    on feature sparsity in a linear forecasting model.\n    \"\"\"\n\n    # ------------------\n    # --- Parameters ---\n    # ------------------\n    N_SAMPLES = 64\n    N_FEATURES = 10\n    SPARSITY_THRESHOLD = 1e-6\n    LASSO_TOLERANCE = 1e-9\n    LASSO_MAX_ITER = 5000\n\n    # ----------------------------\n    # --- Part A: Data Construction ---\n    # ----------------------------\n\n    def standardize(v):\n        \"\"\"Standardizes a vector to have zero mean and unit variance.\"\"\"\n        # Using ddof=0 for population standard deviation, as is standard in ML.\n        return (v - v.mean()) / v.std(ddof=0)\n\n    t = np.arange(N_SAMPLES)\n\n    # Base predictors\n    b1 = np.cos(2 * np.pi * 1 * t / N_SAMPLES)\n    b2 = np.sin(2 * np.pi * 1 * t / N_SAMPLES)\n    b3 = np.cos(2 * np.pi * 2 * t / N_SAMPLES)\n    b4 = np.sin(2 * np.pi * 2 * t / N_SAMPLES)\n\n    # Standardized bases\n    s1 = standardize(b1)\n    s2 = standardize(b2)\n    s3 = standardize(b3)\n    s4 = standardize(b4)\n\n    # Target vector\n    y = 0.8 * s1 + 1.2 * s3 - 1.5 * s4\n    # Enforce zero mean numerically, as specified.\n    y = y - y.mean()\n\n    # Perturbation vector\n    q = np.cos(2 * np.pi * 5 * t / N_SAMPLES)\n    s_q = standardize(q)\n\n    # Feature construction\n    f_vectors = [\n        s1,\n        s2,\n        s3,\n        s4,\n        0.8 * s1 + 0.2 * s2 + 0.01 * s_q,\n        0.5 * s2 - 0.3 * s3 + 0.01 * s_q,\n        0.1 * s1 + 0.9 * s4 + 0.01 * s_q,\n        0.3 * s3 + 0.4 * s4 + 0.3 * s2 + 0.01 * s_q,\n        0.6 * s1 - 0.1 * s3 + 0.01 * s_q,\n        0.2 * s2 + 0.2 * s3 + 0.6 * s4 + 0.01 * s_q,\n    ]\n\n    # Final design matrix X\n    X = np.zeros((N_SAMPLES, N_FEATURES))\n    for j, f_vec in enumerate(f_vectors):\n        X[:, j] = standardize(f_vec)\n\n    # -----------------------\n    # --- Part B: Solvers ---\n    # -----------------------\n\n    def ridge_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Ridge regression using the closed-form solution.\"\"\"\n        n, d = X_mat.shape\n        A = (1 / n) * (X_mat.T @ X_mat) + lam * np.identity(d)\n        b = (1 / n) * (X_mat.T @ y_vec)\n        w = np.linalg.solve(A, b)\n        return w\n\n    def soft_threshold(a, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(a) * np.maximum(np.abs(a) - lam, 0)\n    \n    def lasso_solver(X_mat, y_vec, lam):\n        \"\"\"Solves Lasso regression using cyclic coordinate descent.\"\"\"\n        n, d = X_mat.shape\n        w = np.zeros(d)\n        \n        # H_j = (1/n) * sum(X_ij^2). Since columns are standardized to unit variance, H_j = 1.\n        # We can verify this for safety.\n        H_j = np.sum(X_mat**2, axis=0) / n\n\n        for _ in range(LASSO_MAX_ITER):\n            w_old = np.copy(w)\n            for j in range(d):\n                # Calculate rho_j according to the definition\n                # rho_j = (1/n) * X_j^T * (y - sum_{k!=j} X_k w_k)\n                y_pred = X_mat @ w\n                residual_without_j = y_vec - y_pred + X_mat[:, j] * w[j]\n                rho_j = (1 / n) * (X_mat[:, j].T @ residual_without_j)\n                \n                # Update w_j\n                w[j] = soft_threshold(rho_j, lam) / H_j[j]\n\n            if np.linalg.norm(w - w_old, ord=np.inf) < LASSO_TOLERANCE:\n                break\n        return w\n\n    # --------------------------------\n    # --- Part C: Sparsity Metric & Execution ---\n    # --------------------------------\n\n    def calculate_support_size(w_vec):\n        \"\"\"Calculates the support size of a weight vector.\"\"\"\n        return np.sum(np.abs(w_vec) > SPARSITY_THRESHOLD)\n\n    test_cases = [\n        {'type': 'l2', 'lambda': 0.0},\n        {'type': 'l2', 'lambda': 10.0},\n        {'type': 'l1', 'lambda': 0.0},\n        {'type': 'l1', 'lambda': 0.5},\n        {'type': 'l1', 'lambda': 2.2},\n    ]\n\n    results = []\n    for case in test_cases:\n        lam = case['lambda']\n        if case['type'] == 'l2':\n            w_solution = ridge_solver(X, y, lam)\n        else: # type == 'l1'\n            w_solution = lasso_solver(X, y, lam)\n        \n        sparsity = calculate_support_size(w_solution)\n        results.append(sparsity)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "In financial forecasting, simply minimizing the average error is often not enough; the *type* of error matters immensely. This practice challenges you to think like a quantitative modeler by designing a custom loss function that aligns with a specific financial objective: correctly predicting the direction of an asset's movement [@problem_id:2414391]. You will derive and implement a loss that combines the standard squared error with a penalty for directional mistakes, providing a powerful lesson in how to tailor a model's training objective to what is truly valuable in a real-world application.", "id": "2414391", "problem": "You are designing a loss for training a neural network that forecasts asset returns in computational economics and finance. The goal is to hard-penalize predictions that get the direction of the return wrong, not only the magnitude of the error. Work in discrete time indexed by $t \\in \\{1,\\dots,T\\}$. Let the realized simple return be $r_t \\in \\mathbb{R}$ and the model’s prediction be $\\hat{r}_t \\in \\mathbb{R}$. The standard baseline is the mean squared error, which is the empirical risk $(1/T)\\sum_{t=1}^T (r_t - \\hat{r}_t)^2$.\n\nStarting from fundamental principles used in statistical learning:\n- Empirical risk minimization over a sample is based on averaging a per-sample loss.\n- The squared loss encourages accurate magnitude matching.\n- To encode directional accuracy, a margin-based penalty uses the product $r_t \\hat{r}_t$ as a signed agreement measure, where $r_t \\hat{r}_t > 0$ indicates agreement in sign and $r_t \\hat{r}_t < 0$ indicates disagreement.\n- The minimal convex nonnegative penalty that is $0$ when a margin threshold is met and grows linearly when the margin is violated is the hinge form of an affine function.\n\nYour task:\n1. Derive, from the principles above, a convex per-sample loss $\\ell_t$ that augments the squared error $(r_t - \\hat{r}_t)^2$ with a direction-violation penalty that depends only on the margin $r_t \\hat{r}_t$ and two nonnegative hyperparameters: a penalty weight $\\lambda \\ge 0$ and a nonnegative margin threshold $m \\ge 0$. The penalty must be:\n   - Identically zero whenever the margin requirement is satisfied, that is, whenever $r_t \\hat{r}_t \\ge m$.\n   - A linear function with slope $\\lambda$ of the shortfall relative to the threshold whenever $r_t \\hat{r}_t < m$.\n   - Nonnegative for all $(r_t,\\hat{r}_t)$.\n   - Such that the total per-sample loss $\\ell_t$ is convex in $\\hat{r}_t$.\n2. Define the overall loss as the sample average $L = \\frac{1}{T}\\sum_{t=1}^T \\ell_t$.\n3. Implement a program that computes $L$ for the following test suite of $(r,\\hat{r},\\lambda,m)$ tuples. For each case, $r$ and $\\hat{r}$ are arrays of the same length $T$, and $\\lambda$ and $m$ are scalars:\n   - Case A (happy path, perfect direction and magnitude): $r = [0.01,-0.02]$, $\\hat{r} = [0.01,-0.02]$, $\\lambda = 100$, $m = 0$.\n   - Case B (direction always wrong, zero margin): $r = [0.01,-0.02]$, $\\hat{r} = [-0.01,0.02]$, $\\lambda = 100$, $m = 0$.\n   - Case C (boundary/margin test with small returns and positive margin): $r = [0.005,-0.003,0.004]$, $\\hat{r} = [0.0,-0.001,0.002]$, $\\lambda = 50$, $m = 0.001$.\n   - Case D (mixed correctness, zero margin): $r = [0.02,-0.01,-0.015]$, $\\hat{r} = [0.03,0.005,0.010]$, $\\lambda = 200$, $m = 0$.\n   - Case E (fallback to pure squared error): $r = [0.01,-0.02,0.03]$, $\\hat{r} = [0.009,-0.018,0.029]$, $\\lambda = 0$, $m = 0.005$.\n4. The program must compute the scalar loss $L$ for each case and produce a single line of output containing the five results as a comma-separated list enclosed in square brackets, with each number rounded to $6$ decimal places, for example, $[x_1,x_2,x_3,x_4,x_5]$.\n\nYour program must be a complete, runnable implementation that performs these computations with no user input. No physical units are involved. Angles are not used. Percentages must not be used anywhere; when quantities that might be informally described as percentages arise, they must instead be represented as decimals. Ensure scientific realism by treating returns as small real numbers, recognizing that directional accuracy is encoded via the sign of $r_t \\hat{r}_t$, and that the additional penalty strongly discourages sign mistakes.", "solution": "The problem presented is a task in the design of a specialized loss function for training neural networks in the context of financial asset return forecasting. The problem is scientifically grounded, well-posed, and all its components are clearly defined. It is a valid problem of mathematical engineering. I shall proceed with the derivation and solution.\n\nThe objective is to derive a per-sample loss function, denoted $\\ell_t$, for a single time step $t$. This loss must augment the standard squared error with a penalty for directional inaccuracy. The total loss for a sample of size $T$ is the empirical risk, or the sample average of these per-sample losses.\n\nLet $r_t$ be the realized return and $\\hat{r}_t$ be the predicted return at time $t$. The per-sample loss $\\ell_t$ is composed of two parts: a squared error term for magnitude accuracy and a penalty term for directional accuracy, which we shall denote $P_t$.\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + P_t(\\hat{r}_t)\n$$\n\nThe penalty term $P_t$ must adhere to several principles outlined in the problem statement.\n1.  It is a function of the margin, defined by the product $r_t \\hat{r}_t$, a penalty weight $\\lambda \\ge 0$, and a margin threshold $m \\ge 0$.\n2.  The penalty must be zero if the margin condition is satisfied, i.e., $r_t \\hat{r}_t \\ge m$.\n3.  If the margin condition is violated, i.e., $r_t \\hat{r}_t < m$, the penalty must be a linear function of the shortfall, $m - r_t \\hat{r}_t$, with a slope of $\\lambda$.\n4.  The penalty must be non-negative, $P_t \\ge 0$.\n\nThese conditions uniquely specify the form of $P_t$. The penalty is non-zero only when $m - r_t \\hat{r}_t > 0$, and in this case, it is equal to $\\lambda (m - r_t \\hat{r}_t)$. For all other cases, it is zero. This is precisely the definition of the hinge loss, which can be written compactly using the positive part function, $\\max(0, x)$.\nThus, the penalty term is:\n$$\nP_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\nThis form satisfies all requirements. Since $\\lambda \\ge 0$ and the $\\max$ function is always non-negative, $P_t \\ge 0$. The other conditions are met by construction.\n\nThe complete per-sample loss function is therefore:\n$$\n\\ell_t(\\hat{r}_t) = (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)\n$$\nA crucial requirement is that $\\ell_t$ must be a convex function with respect to the prediction $\\hat{r}_t$. A function is convex if the sum of convex functions is also convex. We analyze each term separately.\n\nThe first term, $L_{SE}(\\hat{r}_t) = (r_t - \\hat{r}_t)^2$, is a quadratic function of $\\hat{r}_t$. Its second derivative with respect to $\\hat{r}_t$ is $\\frac{\\partial^2}{\\partial \\hat{r}_t^2} (r_t - \\hat{r}_t)^2 = 2$, which is strictly positive. Therefore, the squared error term is strictly convex.\n\nThe second term, $P_t(\\hat{r}_t) = \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$, is also convex. This can be established by recognizing it is a composition of a convex function, $f(x) = \\max(0,x)$, with an affine function of $\\hat{r}_t$, namely $g(\\hat{r}_t) = m - r_t \\hat{r}_t$. The composition of a convex function with an affine mapping is convex. As the non-negative scalar $\\lambda$ preserves convexity, $P_t(\\hat{r}_t)$ is convex in $\\hat{r}_t$.\n\nSince both terms of $\\ell_t$ are convex in $\\hat{r}_t$, their sum, $\\ell_t(\\hat{r}_t)$, is also a convex function. This property is fundamental for ensuring that the optimization problem of minimizing the loss function is well-behaved and that gradient-based methods will converge to a global minimum.\n\nThe overall loss function $L$ for the entire sample is the empirical risk, defined as the average of the per-sample losses over $T$ observations:\n$$\nL = \\frac{1}{T} \\sum_{t=1}^T \\ell_t = \\frac{1}{T} \\sum_{t=1}^T \\left[ (r_t - \\hat{r}_t)^2 + \\lambda \\cdot \\max(0, m - r_t \\hat{r}_t) \\right]\n$$\nThis is the final form of the loss function to be implemented.\n\nFor the computational part of the problem, we apply this formula to the five test cases provided. For each tuple of $(r, \\hat{r}, \\lambda, m)$, where $r$ and $\\hat{r}$ are vectors of length $T$, the computation proceeds as follows:\n1.  Compute the element-wise squared error $(r_t - \\hat{r}_t)^2$ for each $t \\in \\{1,\\dots,T\\}$.\n2.  Compute the element-wise margin product $r_t \\hat{r}_t$.\n3.  Compute the element-wise penalty term $\\lambda \\cdot \\max(0, m - r_t \\hat{r}_t)$.\n4.  Sum the squared error and penalty for each sample point to get $\\ell_t$.\n5.  Compute the mean of these per-sample losses to find $L$.\n\nThe computed values for the five cases are as follows:\n- Case A: $L_A = 0.000000$\n- Case B: $L_B = 0.026000$\n- Case C: $L_C = 0.049828$\n- Case D: $L_D = 0.013650$\n- Case E: $L_E = 0.000002$\n\nThe implementation will follow this logic to produce the required output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the custom loss function for several test cases in financial forecasting.\n\n    The loss function L is defined as the average of per-sample losses l_t:\n    L = (1/T) * sum_{t=1 to T} l_t\n    where l_t = (r_t - r_hat_t)^2 + lambda * max(0, m - r_t * r_hat_t)\n    - r_t: realized return\n    - r_hat_t: predicted return\n    - lambda: penalty weight\n    - m: margin threshold\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: happy path, perfect direction and magnitude\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([0.01, -0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case B: direction always wrong, zero margin\n        {'r': np.array([0.01, -0.02]), 'r_hat': np.array([-0.01, 0.02]), 'lam': 100.0, 'm': 0.0},\n        # Case C: boundary/margin test with small returns and positive margin\n        {'r': np.array([0.005, -0.003, 0.004]), 'r_hat': np.array([0.0, -0.001, 0.002]), 'lam': 50.0, 'm': 0.001},\n        # Case D: mixed correctness, zero margin\n        {'r': np.array([0.02, -0.01, -0.015]), 'r_hat': np.array([0.03, 0.005, 0.010]), 'lam': 200.0, 'm': 0.0},\n        # Case E: fallback to pure squared error\n        {'r': np.array([0.01, -0.02, 0.03]), 'r_hat': np.array([0.009, -0.018, 0.029]), 'lam': 0.0, 'm': 0.005}\n    ]\n\n    results = []\n    for case in test_cases:\n        r = case['r']\n        r_hat = case['r_hat']\n        lam = case['lam']\n        m = case['m']\n\n        # Ensure inputs are NumPy arrays for vectorized operations\n        if not isinstance(r, np.ndarray):\n            r = np.array(r)\n        if not isinstance(r_hat, np.ndarray):\n            r_hat = np.array(r_hat)\n\n        # 1. Squared error term\n        squared_error = (r - r_hat)**2\n\n        # 2. Directional penalty term\n        # The margin is the product of the realized and predicted returns\n        margin = r * r_hat\n        # The penalty is applied when the margin is below the threshold m\n        penalty = lam * np.maximum(0, m - margin)\n\n        # 3. Per-sample loss is the sum of the two terms\n        per_sample_loss = squared_error + penalty\n\n        # 4. The overall loss is the mean of the per-sample losses\n        total_loss = np.mean(per_sample_loss)\n\n        # Append the formatted result\n        results.append(f\"{total_loss:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"}]}