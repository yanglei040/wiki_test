{"hands_on_practices": [{"introduction": "A core concept in regularization is the penalty parameter, $\\lambda$, which controls the trade-off between model fit and complexity. This exercise takes you back to first principles to understand the fundamental effect of this parameter. By exploring the limiting case as $\\lambda \\to \\infty$, you will prove how both Ridge and LASSO regression shrink model coefficients towards a simple, non-complex state, providing deep insight into their mechanism for combating overfitting [@problem_id:2426322].", "id": "2426322", "problem": "An econometrician models the excess return of an asset as a linear function of $p$ predictors observed over $n$ time periods. Let the response vector be $y \\in \\mathbb{R}^{n}$ and the predictor matrix be $X \\in \\mathbb{R}^{n \\times p}$, where the $i$-th row is $x_{i}^{\\top} \\in \\mathbb{R}^{p}$. Consider linear models with an intercept $ \\beta_{0} \\in \\mathbb{R} $ and slope vector $\\beta \\in \\mathbb{R}^{p}$, where the intercept is not penalized. Define the Ridge objective for a given penalty level $\\lambda \\geq 0$ by\n$$\nQ_{\\lambda}^{\\mathrm{R}}(\\beta_{0},\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-x_{i}^{\\top}\\beta\\right)^{2}+\\lambda \\|\\beta\\|_{2}^{2},\n$$\nand the Least Absolute Shrinkage and Selection Operator (LASSO) objective by\n$$\nQ_{\\lambda}^{\\mathrm{L}}(\\beta_{0},\\beta) \\equiv \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-x_{i}^{\\top}\\beta\\right)^{2}+\\lambda \\|\\beta\\|_{1}.\n$$\nFor each $\\lambda \\geq 0$, let $(\\beta_{0}^{\\mathrm{R}}(\\lambda),\\beta^{\\mathrm{R}}(\\lambda))$ and $(\\beta_{0}^{\\mathrm{L}}(\\lambda),\\beta^{\\mathrm{L}}(\\lambda))$ be minimizers of $Q_{\\lambda}^{\\mathrm{R}}$ and $Q_{\\lambda}^{\\mathrm{L}}$, respectively. Denote the sample means by\n$$\n\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} y_{i}\n\\quad\\text{and}\\quad\n\\bar{x} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} x_{i}.\n$$\nAssume only that $n \\geq 1$ and $p \\geq 1$, and that the intercept is not penalized in both objectives.\n\nProve, from first principles, that as $\\lambda \\to \\infty$, both $\\beta^{\\mathrm{R}}(\\lambda)$ and $\\beta^{\\mathrm{L}}(\\lambda)$ converge to the zero vector in $\\mathbb{R}^{p}$. Then determine the exact limit, in closed form, of the intercept parameter $\\beta_{0}^{\\mathrm{R}}(\\lambda)$ (equivalently $\\beta_{0}^{\\mathrm{L}}(\\lambda)$) as $\\lambda \\to \\infty$, expressed solely in terms of $\\bar{y}$ and $\\bar{x}$.\n\nYour final answer should be a single analytic expression giving the limit of the intercept as $\\lambda \\to \\infty$. No numerical approximation is required.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a rigorous mathematical derivation. It is therefore deemed valid. We shall proceed with the solution, which consists of two parts as requested by the problem statement. First, we will prove that the slope vectors for both Ridge and LASSO regression converge to the zero vector as the penalty parameter $\\lambda$ approaches infinity. Second, we will determine the limit of the intercept parameter under the same condition.\n\nLet the general objective function be denoted by $Q_{\\lambda}(\\beta_{0}, \\beta)$, which represents either the Ridge objective $Q_{\\lambda}^{\\mathrm{R}}$ or the LASSO objective $Q_{\\lambda}^{\\mathrm{L}}$. This can be written as:\n$$\nQ_{\\lambda}(\\beta_{0}, \\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - x_i^\\top \\beta)^2 + \\lambda P(\\beta)\n$$\nwhere $P(\\beta) = \\|\\beta\\|_{2}^{2}$ for Ridge regression and $P(\\beta) = \\|\\beta\\|_{1}$ for LASSO regression. Let $(\\beta_0(\\lambda), \\beta(\\lambda))$ denote the pair $(\\beta_0, \\beta)$ that minimizes this function for a given $\\lambda \\geq 0$.\n\nFirst, we prove that $\\lim_{\\lambda \\to \\infty} \\beta(\\lambda) = \\mathbf{0}$.\nBy the definition of a minimizer, for any other parameter pair $(\\beta_0', \\beta')$, the following inequality holds:\n$$\nQ_{\\lambda}(\\beta_0(\\lambda), \\beta(\\lambda)) \\leq Q_{\\lambda}(\\beta_0', \\beta')\n$$\nWe choose a specific, convenient point for comparison: $(\\beta_0', \\beta') = (\\bar{y}, \\mathbf{0})$, where $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^p$. The value of the objective function at this point is:\n$$\nQ_{\\lambda}(\\bar{y}, \\mathbf{0}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y} - x_i^\\top \\mathbf{0})^2 + \\lambda P(\\mathbf{0})\n$$\nFor both Ridge and LASSO penalties, $P(\\mathbf{0}) = 0$. Thus, the expression simplifies to:\n$$\nQ_{\\lambda}(\\bar{y}, \\mathbf{0}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n$$\nThis quantity is a finite constant that depends only on the data $y$ and does not depend on $\\lambda$. Let us denote this constant by $C$. The inequality from the definition of the minimizer becomes:\n$$\nQ_{\\lambda}(\\beta_0(\\lambda), \\beta(\\lambda)) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0(\\lambda) - x_i^\\top \\beta(\\lambda))^2 + \\lambda P(\\beta(\\lambda)) \\leq C\n$$\nThe first term, the mean squared error, is necessarily non-negative. Therefore, we can deduce an inequality for the penalty term:\n$$\n\\lambda P(\\beta(\\lambda)) \\leq C\n$$\nFor $\\lambda > 0$, we can write:\n$$\nP(\\beta(\\lambda)) \\leq \\frac{C}{\\lambda}\n$$\nAs $\\lambda \\to \\infty$, the right-hand side $\\frac{C}{\\lambda}$ approaches $0$. Since the penalty function $P(\\beta)$ is always non-negative, we must have $\\lim_{\\lambda \\to \\infty} P(\\beta(\\lambda)) = 0$ by the Squeeze Theorem.\n\nFor Ridge regression, $P(\\beta) = \\|\\beta\\|_2^2 = \\sum_{j=1}^{p} (\\beta_j)^2$. The condition $\\lim_{\\lambda \\to \\infty} \\|\\beta^{\\mathrm{R}}(\\lambda)\\|_2^2 = 0$ implies that each component of the vector must go to zero. Thus, $\\lim_{\\lambda \\to \\infty} \\beta^{\\mathrm{R}}(\\lambda) = \\mathbf{0}$.\nFor LASSO regression, $P(\\beta) = \\|\\beta\\|_1 = \\sum_{j=1}^{p} |\\beta_j|$. The condition $\\lim_{\\lambda \\to \\infty} \\|\\beta^{\\mathrm{L}}(\\lambda)\\|_1 = 0$ also implies that each component must go to zero. Thus, $\\lim_{\\lambda \\to \\infty} \\beta^{\\mathrm{L}}(\\lambda) = \\mathbf{0}$.\nThis completes the proof for the first part of the problem.\n\nSecond, we determine the limit of the intercept parameter, $\\beta_0(\\lambda)$, as $\\lambda \\to \\infty$.\nFor any fixed $\\lambda \\geq 0$, the optimal intercept $\\beta_0(\\lambda)$ is found by minimizing the objective function with respect to $\\beta_0$ for a given slope vector $\\beta$. Since the penalty term does not depend on $\\beta_0$, we only need to differentiate the mean squared error term.\n$$\n\\frac{\\partial Q_{\\lambda}}{\\partial \\beta_0} = \\frac{\\partial}{\\partial \\beta_0} \\left[ \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\beta_0 - x_i^\\top \\beta)^2 \\right] = \\frac{1}{n} \\sum_{i=1}^{n} 2(y_i - \\beta_0 - x_i^\\top \\beta)(-1)\n$$\nSetting this partial derivative to zero to find the optimal $\\beta_0(\\lambda)$ corresponding to the optimal $\\beta(\\lambda)$:\n$$\n-\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\beta_0(\\lambda) - x_i^\\top \\beta(\\lambda)) = 0\n$$\n$$\n\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0(\\lambda) - \\sum_{i=1}^{n} x_i^\\top \\beta(\\lambda) = 0\n$$\nUsing the definitions of the sample means, $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ and $\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i$:\n$$\nn\\bar{y} - n\\beta_0(\\lambda) - \\left( \\sum_{i=1}^{n} x_i \\right)^\\top \\beta(\\lambda) = 0\n$$\n$$\nn\\bar{y} - n\\beta_0(\\lambda) - (n\\bar{x})^\\top \\beta(\\lambda) = 0\n$$\nSince $n \\geq 1$, we can divide by $n$:\n$$\n\\bar{y} - \\beta_0(\\lambda) - \\bar{x}^\\top \\beta(\\lambda) = 0\n$$\nThis gives us the exact expression for the optimal intercept for any $\\lambda \\geq 0$:\n$$\n\\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\beta(\\lambda)\n$$\nThis relation is valid for both Ridge and LASSO estimators, as the derivation does not depend on the form of the penalty $P(\\beta)$. To find the limit as $\\lambda \\to \\infty$, we apply the limit operator to this expression:\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\lim_{\\lambda \\to \\infty} (\\bar{y} - \\bar{x}^\\top \\beta(\\lambda))\n$$\nSince $\\bar{y}$ and $\\bar{x}$ are constants with respect to $\\lambda$, and the inner product is a continuous function, we can pass the limit inside:\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\left( \\lim_{\\lambda \\to \\infty} \\beta(\\lambda) \\right)\n$$\nFrom the first part of our proof, we established that $\\lim_{\\lambda \\to \\infty} \\beta(\\lambda) = \\mathbf{0}$. Substituting this result yields the final answer:\n$$\n\\lim_{\\lambda \\to \\infty} \\beta_0(\\lambda) = \\bar{y} - \\bar{x}^\\top \\mathbf{0} = \\bar{y}\n$$\nTherefore, as the penalty parameter $\\lambda$ goes to infinity, the slope coefficients for both Ridge and LASSO are shrunk to zero, and the intercept term converges to the sample mean of the response variable, $\\bar{y}$.", "answer": "$$\\boxed{\\bar{y}}$$"}, {"introduction": "While both Ridge and LASSO impose penalties, their behavior differs dramatically, especially when predictors are correlated—a common situation in economic data. This practice problem presents a carefully constructed scenario to make these differences tangible. By calculating the coefficients for both models, you will directly observe LASSO's stark variable selection feature versus Ridge's tendency to shrink correlated coefficients together, illustrating a crucial practical distinction for model building and interpretation [@problem_id:2426291].", "id": "2426291", "problem": "An analyst is estimating a linear predictive model for a portfolio’s one-period-ahead excess return using two highly correlated predictors over two time periods, with no intercept term. Let the response vector be $y \\in \\mathbb{R}^{2}$ and the predictor matrix be $X \\in \\mathbb{R}^{2 \\times 2}$ with columns $x_{1}$ and $x_{2}$ given by\n$$\ny=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{1}=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}, \\quad x_{2}=\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}, \\quad X=\\begin{pmatrix}1 & 2 \\\\ 0 & 1\\end{pmatrix}.\n$$\nThe data generating process is $y=x_{1}$ (no noise). Consider two regularized estimators:\n- The Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\beta}^{\\text{LASSO}}(\\lambda)$ defined as any minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nwith penalty parameter $\\lambda=\\frac{6}{5}$.\n- The ridge regression estimator $\\hat{\\beta}^{\\text{Ridge}}(\\alpha)$ defined as the unique minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{2}} \\; \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2},\n$$\nwith penalty parameter $\\alpha=\\frac{1}{2}$.\n\nCompute both coefficient vectors exactly and report them as a single row vector $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$. Give exact values; do not round. Your final answer must be this single row vector.", "solution": "The problem requires the computation of two regularized linear regression coefficient vectors: the Ridge estimator and the LASSO estimator. The problem is well-defined, scientifically sound, and all necessary data are provided. We shall solve for each estimator separately.\n\nThe given data are:\nResponse vector $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nPredictor matrix $X = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}$.\nThe coefficient vector is $\\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}$.\n\nFirst, we compute some necessary matrix products.\nThe transpose of $X$ is $X^T = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix}$.\nThe Gram matrix is $X^T X = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}$.\nThe product $X^T y$ is $X^T y = \\begin{pmatrix} 1 & 0 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\n**1. Ridge Regression Estimator**\n\nThe ridge regression objective function is given by:\n$$L_{\\text{Ridge}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\alpha}{2} \\|\\beta\\|_{2}^{2}$$\nwith the penalty parameter $\\alpha = \\frac{1}{2}$.\nThis objective function is strictly convex and differentiable. The minimizer $\\hat{\\beta}^{\\text{Ridge}}$ is found by setting the gradient with respect to $\\beta$ to zero:\n$$\\nabla_{\\beta} L_{\\text{Ridge}} = -X^T(y - X\\beta) + \\alpha\\beta = 0$$\nRearranging the terms gives the normal equations for ridge regression:\n$$(X^T X + \\alpha I) \\beta = X^T y$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The solution is thus:\n$$\\hat{\\beta}^{\\text{Ridge}} = (X^T X + \\alpha I)^{-1} X^T y$$\nWe substitute the given values:\n$$X^T X + \\alpha I = \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 + \\frac{1}{2} & 2 \\\\ 2 & 5 + \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & 2 \\\\ 2 & \\frac{11}{2} \\end{pmatrix}$$\nTo find the inverse of this matrix, we first compute its determinant:\n$$\\det(X^T X + \\alpha I) = \\left(\\frac{3}{2}\\right)\\left(\\frac{11}{2}\\right) - (2)(2) = \\frac{33}{4} - 4 = \\frac{33 - 16}{4} = \\frac{17}{4}$$\nThe inverse is:\n$$(X^T X + \\alpha I)^{-1} = \\frac{1}{\\frac{17}{4}} \\begin{pmatrix} \\frac{11}{2} & -2 \\\\ -2 & \\frac{3}{2} \\end{pmatrix} = \\frac{4}{17} \\begin{pmatrix} \\frac{11}{2} & -2 \\\\ -2 & \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} & -\\frac{8}{17} \\\\ -\\frac{8}{17} & \\frac{6}{17} \\end{pmatrix}$$\nNow we can compute the ridge estimator:\n$$\\hat{\\beta}^{\\text{Ridge}} = \\begin{pmatrix} \\frac{22}{17} & -\\frac{8}{17} \\\\ -\\frac{8}{17} & \\frac{6}{17} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{22}{17} - \\frac{16}{17} \\\\ -\\frac{8}{17} + \\frac{12}{17} \\end{pmatrix} = \\begin{pmatrix} \\frac{6}{17} \\\\ \\frac{4}{17} \\end{pmatrix}$$\nThus, $\\hat{\\beta}_{1}^{\\text{Ridge}} = \\frac{6}{17}$ and $\\hat{\\beta}_{2}^{\\text{Ridge}} = \\frac{4}{17}$.\n\n**2. LASSO Estimator**\n\nThe LASSO objective function is:\n$$L_{\\text{LASSO}}(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$$\nwith the penalty parameter $\\lambda = \\frac{6}{5}$. Due to the non-differentiability of the $L_1$-norm, we use subgradient optimality conditions. The minimizer $\\hat{\\beta}^{\\text{LASSO}}$ must satisfy $0 \\in \\partial L_{\\text{LASSO}}(\\hat{\\beta})$.\nThe subgradient condition is $X^T(y - X\\hat{\\beta})_j \\in \\lambda \\cdot \\partial |\\beta_j| |_{\\hat{\\beta}_j}$ for each component $j \\in \\{1, 2\\}$. This can be written as:\n$$\n\\begin{cases}\n(X^T(y - X\\hat{\\beta}))_j = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j) & \\text{if } \\hat{\\beta}_j \\neq 0 \\\\\n|(X^T(y - X\\hat{\\beta}))_j| \\le \\lambda & \\text{if } \\hat{\\beta}_j = 0\n\\end{cases}\n$$\nLet's compute the vector of correlations with the residuals, $c(\\beta) = X^T(y - X\\beta)$:\n$$c(\\beta) = X^T y - X^T X \\beta = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 1 & 2 \\\\ 2 & 5 \\end{pmatrix}\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\beta_1 - 2\\beta_2 \\\\ 2 - 2\\beta_1 - 5\\beta_2 \\end{pmatrix}$$\nWe analyze the possible cases for the active set of coefficients.\n\nCase 1: $\\hat{\\beta}_1 = 0$, $\\hat{\\beta}_2 \\ne 0$.\nThe conditions are $|c_1(\\hat{\\beta})| \\le \\lambda$ and $c_2(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_2)$.\nWith $\\hat\\beta_1 = 0$, we have $c_2 = 2 - 5\\hat{\\beta}_2$.\nIf $\\hat{\\beta}_2 > 0$: $2 - 5\\hat{\\beta}_2 = \\lambda = \\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5} \\implies \\hat{\\beta}_2 = \\frac{4}{25}$. This is consistent with $\\hat{\\beta}_2 > 0$.\nWe must check the condition for $\\hat{\\beta}_1=0$: $|c_1| \\le \\lambda$.\n$|c_1| = |1 - 0 - 2\\hat{\\beta}_2| = |1 - 2(\\frac{4}{25})| = |1 - \\frac{8}{25}| = |\\frac{17}{25}| = \\frac{17}{25}$.\nThe condition is $\\frac{17}{25} \\le \\frac{6}{5}$. Since $\\frac{6}{5} = \\frac{30}{25}$, we have $\\frac{17}{25} \\le \\frac{30}{25}$, which is true.\nSo, $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$ is a valid solution.\n\nIf $\\hat{\\beta}_2 < 0$: $2 - 5\\hat{\\beta}_2 = -\\lambda = -\\frac{6}{5} \\implies 5\\hat{\\beta}_2 = 2 + \\frac{6}{5} = \\frac{16}{5} \\implies \\hat{\\beta}_2 = \\frac{16}{25}$. This contradicts the assumption $\\hat{\\beta}_2 < 0$.\n\nCase 2: $\\hat{\\beta}_1 \\ne 0$, $\\hat{\\beta}_2 = 0$.\nThe conditions are $c_1(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_1)$ and $|c_2(\\hat{\\beta})| \\le \\lambda$.\nWith $\\hat\\beta_2 = 0$, we have $c_1 = 1 - \\hat\\beta_1$.\nIf $\\hat{\\beta}_1 > 0$: $1 - \\hat{\\beta}_1 = \\lambda = \\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 - \\frac{6}{5} = -\\frac{1}{5}$. Contradiction.\nIf $\\hat{\\beta}_1 < 0$: $1 - \\hat{\\beta}_1 = -\\lambda = -\\frac{6}{5} \\implies \\hat{\\beta}_1 = 1 + \\frac{6}{5} = \\frac{11}{5}$. Contradiction.\nThis case yields no solution.\n\nCase 3: $\\hat{\\beta}_1 \\ne 0$, $\\hat{\\beta}_2 \\ne 0$.\nThis requires solving $c_j(\\hat{\\beta}) = \\lambda \\cdot \\text{sign}(\\hat{\\beta}_j)$ for $j=1,2$.\n1) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 > 0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$. The sum of two positive numbers cannot be negative. Contradiction.\n2) $\\hat{\\beta}_1 > 0, \\hat{\\beta}_2 < 0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - \\frac{6}{5} = -\\frac{1}{5}$ and $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - (-\\frac{6}{5}) = \\frac{16}{5}$. Solving this system yields $\\hat{\\beta}_2 = \\frac{18}{5}$, which contradicts $\\hat{\\beta}_2 < 0$.\n3) $\\hat{\\beta}_1 < 0, \\hat{\\beta}_2 > 0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$ and $2\\hat{\\beta}_1 + 5\\hat{\\beta}_2 = 2 - \\frac{6}{5} = \\frac{4}{5}$. Solving this system yields $\\hat{\\beta}_2 = -\\frac{18}{5}$, which contradicts $\\hat{\\beta}_2 > 0$.\n4) $\\hat{\\beta}_1 < 0, \\hat{\\beta}_2 < 0$: $\\hat{\\beta}_1 + 2\\hat{\\beta}_2 = 1 - (-\\frac{6}{5}) = \\frac{11}{5}$. The sum of two negative numbers cannot be positive. Contradiction.\nNo solution exists with both coefficients being non-zero.\n\nSince the LASSO objective function is strictly convex (as $X$ has full rank), a unique minimizer must exist. Our analysis of all possible active sets yielded only one valid solution.\nTherefore, the LASSO estimator is $\\hat{\\beta}^{\\text{LASSO}} = \\begin{pmatrix} 0 \\\\ \\frac{4}{25} \\end{pmatrix}$.\nSo, $\\hat{\\beta}_{1}^{\\text{LASSO}} = 0$ and $\\hat{\\beta}_{2}^{\\text{LASSO}} = \\frac{4}{25}$.\n\n**Final Answer Assembly**\nThe problem requests the final answer as a single row vector $\\big[\\hat{\\beta}_{1}^{\\text{LASSO}}, \\hat{\\beta}_{2}^{\\text{LASSO}}, \\hat{\\beta}_{1}^{\\text{Ridge}}, \\hat{\\beta}_{2}^{\\text{Ridge}}\\big]$.\nSubstituting the computed values:\n$$\\begin{pmatrix} 0 & \\frac{4}{25} & \\frac{6}{17} & \\frac{4}{17} \\end{pmatrix}$$\nThis is the final result.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & \\frac{4}{25} & \\frac{6}{17} & \\frac{4}{17} \\end{pmatrix}}\n$$"}, {"introduction": "Standard regularization techniques shrink coefficients towards zero, which is a sensible default but not always the most informed choice. Economic theory or prior evidence might suggest that coefficients should be close to other specific values. This hands-on problem demonstrates a powerful generalization of Ridge regression that allows you to shrink coefficients towards a non-zero target vector $\\beta_0$, effectively integrating domain knowledge into your model and showcasing the flexibility of the regularization framework [@problem_id:2426295].", "id": "2426295", "problem": "Consider the linear model in which an economic outcome vector $y \\in \\mathbb{R}^n$ is related to a regressor matrix $X \\in \\mathbb{R}^{n \\times p}$ through coefficients $\\beta \\in \\mathbb{R}^p$. You are asked to compute, for given inputs, the coefficient vector $\\beta^\\star \\in \\mathbb{R}^p$ that minimizes the objective\n$$\nJ(\\beta) \\;=\\; \\frac{1}{n}\\,\\lVert y - X\\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta - \\beta_0 \\rVert_2^2,\n$$\nwhere $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is a regularization parameter and $\\beta_0 \\in \\mathbb{R}^p$ is a fixed target coefficient vector motivated by economic theory.\n\nYour program must compute the unique minimizer $\\beta^\\star$ for each of the following test cases. For each case, return the coefficient vector as a list of real numbers rounded to six decimals. Aggregate the results for all test cases into a single line formatted as a comma-separated list enclosed in square brackets, with each inner list representing the coefficients for one case, in the order given below. No other text should be printed.\n\nTest suite:\n\n- Case $1$ (general case, shrinkage toward a nonzero target): \n  - $n = 5$, $p = 2$,\n  - $X = \\begin{bmatrix}\n  1 & 0.5 \\\\\n  1 & 1.0 \\\\\n  1 & 1.5 \\\\\n  1 & 2.0 \\\\\n  1 & 3.0\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 0.3 \\\\ 1.9 \\\\ 2.1 \\\\ 3.2 \\\\ 4.8 \\end{bmatrix}$,\n  - $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\lambda = 0.1$.\n\n- Case $2$ (boundary condition with no regularization): \n  - Same $X$, $y$, and $\\beta_0$ as Case $1$,\n  - $\\lambda = 0$.\n\n- Case $3$ (large regularization, coefficients close to the target): \n  - Same $X$, $y$, and $\\beta_0$ as Case $1$,\n  - $\\lambda = 100$.\n\n- Case $4$ (collinear regressors stabilized by regularization): \n  - $n = 4$, $p = 3$,\n  - $X = \\begin{bmatrix}\n  1 & 2 & 4 \\\\\n  1 & -1 & -2 \\\\\n  1 & 0.5 & 1.0 \\\\\n  1 & 3 & 6\n  \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix}$,\n  - $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $\\lambda = 0.5$.\n\nFinal output format:\n\n- Your program must produce a single line containing the results as a comma-separated list of lists with no spaces, e.g., for four cases it should look like\n  $[ [\\cdot,\\cdot], [\\cdot,\\cdot], [\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot] ]$\n  but without any spaces. Concretely, print\n  $[[\\beta^\\star_{1,1},\\ldots,\\beta^\\star_{1,p_1}],[\\beta^\\star_{2,1},\\ldots,\\beta^\\star_{2,p_2}],[\\beta^\\star_{3,1},\\ldots,\\beta^\\star_{3,p_3}],[\\beta^\\star_{4,1},\\ldots,\\beta^\\star_{4,p_4}]]$,\n  with each $\\beta^\\star_{k,j}$ rounded to six decimals.", "solution": "The provided problem is subjected to rigorous validation.\n\n**Step 1: Extract Givens**\nThe problem defines a linear model where an outcome vector $y \\in \\mathbb{R}^n$ is related to a regressor matrix $X \\in \\mathbb{R}^{n \\times p}$ via a coefficient vector $\\beta \\in \\mathbb{R}^p$. The task is to find the minimizer $\\beta^\\star$ of the objective function:\n$$\nJ(\\beta) \\;=\\; \\frac{1}{n}\\,\\lVert y - X\\beta \\rVert_2^2 \\;+\\; \\lambda \\,\\lVert \\beta - \\beta_0 \\rVert_2^2\n$$\nwhere $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is a regularization parameter and $\\beta_0 \\in \\mathbb{R}^p$ is a fixed target coefficient vector.\n\nThe givens for the four test cases are:\n- Case $1$: $n = 5$, $p = 2$, $X = \\begin{bmatrix} 1 & 0.5 \\\\ 1 & 1.0 \\\\ 1 & 1.5 \\\\ 1 & 2.0 \\\\ 1 & 3.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.3 \\\\ 1.9 \\\\ 2.1 \\\\ 3.2 \\\\ 4.8 \\end{bmatrix}$, $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$, $\\lambda = 0.1$.\n- Case $2$: Same $X, y, \\beta_0$ as Case $1$, with $\\lambda = 0$.\n- Case $3$: Same $X, y, \\beta_0$ as Case $1$, with $\\lambda = 100$.\n- Case $4$: $n = 4$, $p = 3$, $X = \\begin{bmatrix} 1 & 2 & 4 \\\\ 1 & -1 & -2 \\\\ 1 & 0.5 & 1.0 \\\\ 1 & 3 & 6 \\end{bmatrix}$, $y = \\begin{bmatrix} 2.0 \\\\ -1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix}$, $\\beta_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $\\lambda = 0.5$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the validation criteria.\n1.  **Scientifically Grounded**: The problem is a standard exercise in regularized linear regression, a fundamental topic in statistics, machine learning, and econometrics. The objective function is a generalization of Ridge regression, sometimes known as Tikhonov regularization with a non-zero prior or target. This is a well-established and scientifically sound technique.\n2.  **Well-Posed**: The objective function $J(\\beta)$ is a sum of two squared $\\ell_2$-norms, which are strictly convex functions. Therefore, $J(\\beta)$ is strictly convex for any $\\lambda > 0$, and convex for $\\lambda=0$. A strictly convex function has a unique minimizer. For $\\lambda = 0$, the problem reduces to Ordinary Least Squares (OLS), which has a unique solution if and only if the matrix $X^T X$ is invertible (i.e., $X$ has full column rank). The data provided for Case $2$ (where $\\lambda=0$) has a design matrix $X$ of full column rank, ensuring a unique solution. For Case $4$, the regressors are collinear, but the regularization term with $\\lambda > 0$ ensures the problem remains well-posed and has a unique solution.\n3.  **Objective**: The problem is stated using precise mathematical definitions and numerical data. It requests a specific, computable quantity, leaving no room for subjectivity.\n4.  **Complete and Consistent**: All necessary data ($X, y, \\beta_0, \\lambda, n, p$) are provided for each case. The dimensions of the matrices and vectors are consistent ($y \\in \\mathbb{R}^n, X \\in \\mathbb{R}^{n \\times p}, \\beta \\in \\mathbb{R}^p$).\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. It is scientifically sound, well-posed, objective, and complete. We may proceed with derivation of the solution.\n\n**Derivation of the Optimal Coefficient Vector**\n\nThe objective function to be minimized is:\n$$\nJ(\\beta) = \\frac{1}{n}\\,\\lVert y - X\\beta \\rVert_2^2 + \\lambda \\,\\lVert \\beta - \\beta_0 \\rVert_2^2\n$$\nTo find the minimizer $\\beta^\\star$, we must find the point where the gradient of $J(\\beta)$ with respect to $\\beta$ is zero. First, we expand the squared norm terms using vector transposes:\n$$\nJ(\\beta) = \\frac{1}{n}(y - X\\beta)^T(y - X\\beta) + \\lambda(\\beta - \\beta_0)^T(\\beta - \\beta_0)\n$$\nExpanding the products gives:\n$$\nJ(\\beta) = \\frac{1}{n}(y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta) + \\lambda(\\beta^T \\beta - \\beta^T \\beta_0 - \\beta_0^T \\beta + \\beta_0^T \\beta_0)\n$$\nSince $\\beta^T X^T y$ is a scalar, it is equal to its own transpose, $(y^T X \\beta)^T$. Thus, $y^T X \\beta = \\beta^T X^T y$. Similarly, $\\beta_0^T \\beta = \\beta^T \\beta_0$. The expression simplifies to:\n$$\nJ(\\beta) = \\frac{1}{n}(y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta) + \\lambda(\\beta^T \\beta - 2\\beta^T \\beta_0 + \\beta_0^T \\beta_0)\n$$\nNow, we compute the gradient of $J(\\beta)$ with respect to the vector $\\beta$, using the standard matrix calculus identities $\\nabla_{\\beta}(\\beta^T a) = a$ and $\\nabla_{\\beta}(\\beta^T A \\beta) = 2A\\beta$ for a symmetric matrix $A$:\n$$\n\\nabla_\\beta J(\\beta) = \\frac{1}{n}(-2X^T y + 2X^T X \\beta) + \\lambda(2\\beta - 2\\beta_0)\n$$\nSetting the gradient to zero to find the optimal $\\beta^\\star$:\n$$\n\\nabla_\\beta J(\\beta^\\star) = \\frac{2}{n}(X^T X \\beta^\\star - X^T y) + 2\\lambda(\\beta^\\star - \\beta_0) = 0\n$$\nDividing by $2$ and rearranging terms to solve for $\\beta^\\star$:\n$$\n\\frac{1}{n}X^T X \\beta^\\star - \\frac{1}{n}X^T y + \\lambda I \\beta^\\star - \\lambda \\beta_0 = 0\n$$\nwhere $I$ is the $p \\times p$ identity matrix. We group terms containing $\\beta^\\star$:\n$$\n\\left(\\frac{1}{n}X^T X + \\lambda I\\right) \\beta^\\star = \\frac{1}{n}X^T y + \\lambda \\beta_0\n$$\nTo simplify, we multiply the entire equation by $n$:\n$$\n(X^T X + n\\lambda I) \\beta^\\star = X^T y + n\\lambda \\beta_0\n$$\nThe matrix $(X^T X + n\\lambda I)$ is invertible for $\\lambda > 0$ because $X^T X$ is positive semi-definite and $n\\lambda I$ is positive definite, making their sum positive definite and thus invertible. If $\\lambda = 0$, the matrix is invertible if and only if $X$ has full column rank.\nSolving for $\\beta^\\star$ yields the closed-form solution:\n$$\n\\beta^\\star = (X^T X + n\\lambda I)^{-1} (X^T y + n\\lambda \\beta_0)\n$$\nThis general solution is applicable to all test cases provided. It represents a linear system of equations of the form $A \\beta^\\star = b$, where $A = X^T X + n\\lambda I$ and $b = X^T y + n\\lambda \\beta_0$. Numerically, it is preferable to solve this system directly rather than computing the matrix inverse explicitly. The provided Python solution will implement this logic.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal coefficient vector for several regularized regression problems.\n    \"\"\"\n\n    test_cases = [\n        {\n            # Case 1 (general case, shrinkage toward a nonzero target)\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1, 0.5], [1, 1.0], [1, 1.5], [1, 2.0], [1, 3.0]\n            ]),\n            \"y\": np.array([0.3, 1.9, 2.1, 3.2, 4.8]),\n            \"beta0\": np.array([0.0, 1.0]),\n            \"lambda\": 0.1\n        },\n        {\n            # Case 2 (boundary condition with no regularization)\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1, 0.5], [1, 1.0], [1, 1.5], [1, 2.0], [1, 3.0]\n            ]),\n            \"y\": np.array([0.3, 1.9, 2.1, 3.2, 4.8]),\n            \"beta0\": np.array([0.0, 1.0]),\n            \"lambda\": 0.0\n        },\n        {\n            # Case 3 (large regularization, coefficients close to the target)\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1, 0.5], [1, 1.0], [1, 1.5], [1, 2.0], [1, 3.0]\n            ]),\n            \"y\": np.array([0.3, 1.9, 2.1, 3.2, 4.8]),\n            \"beta0\": np.array([0.0, 1.0]),\n            \"lambda\": 100.0\n        },\n        {\n            # Case 4 (collinear regressors stabilized by regularization)\n            \"n\": 4, \"p\": 3,\n            \"X\": np.array([\n                [1, 2, 4], [1, -1, -2], [1, 0.5, 1.0], [1, 3, 6]\n            ]),\n            \"y\": np.array([2.0, -1.0, 0.5, 3.0]),\n            \"beta0\": np.array([0.0, 0.0, 0.0]),\n            \"lambda\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X = case[\"X\"]\n        y = case[\"y\"]\n        beta0 = case[\"beta0\"]\n        lam = case[\"lambda\"]\n        n = case[\"n\"]\n        p = case[\"p\"]\n\n        # The solution is beta_star = (X^T*X + n*lambda*I)^-1 * (X^T*y + n*lambda*beta_0)\n        # This is equivalent to solving the linear system A*beta_star = b where:\n        # A = X^T*X + n*lambda*I\n        # b = X^T*y + n*lambda*beta_0\n\n        XTX = X.T @ X\n        XTy = X.T @ y\n        I = np.identity(p)\n\n        A = XTX + n * lam * I\n        b = XTy + n * lam * beta0\n\n        # Solve the linear system A*beta = b for beta\n        beta_star = np.linalg.solve(A, b)\n\n        # Round the result to six decimal places and convert to a list\n        results.append(np.round(beta_star, 6).tolist())\n\n    # Format the output as a string representation of a list of lists, with no spaces.\n    final_output = str(results).replace(\" \", \"\")\n    print(final_output)\n\nsolve()\n```"}]}