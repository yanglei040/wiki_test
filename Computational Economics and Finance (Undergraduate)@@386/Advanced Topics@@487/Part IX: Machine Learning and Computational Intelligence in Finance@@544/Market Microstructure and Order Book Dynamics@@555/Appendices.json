{"hands_on_practices": [{"introduction": "A fundamental question in market microstructure is understanding how much the price moves in response to a trade. This relationship, known as market impact, is often modeled by a non-linear function. This exercise explores the widely-observed power-law relationship between trade size $Q$ and price impact $I(Q)$, which is a cornerstone of modern transaction cost analysis.\n\nBy working through this problem, you will learn a crucial data analysis technique: using logarithmic transformations to turn a non-linear model into a linear one, which can then be estimated using standard methods like Ordinary Least Squares (OLS) regression. This practice [@problem_id:2408364] builds a bridge between economic theory and practical econometric analysis using synthetic data where the true relationship is known, allowing you to test your estimation method rigorously.", "id": "2408364", "problem": "You are studying the immediate price impact of market orders in a Limit Order Book (LOB). Let the immediate market impact be defined as the change in the mid-price caused by executing a single market order of size $Q$, denoted $I(Q)$. In a static LOB with a continuous marginal supply density $s(u)$ per unit price shift $u$ from the prevailing mid-price, the executed quantity satisfies the consistency condition\n$$\nQ \\;=\\; \\int_{0}^{I(Q)} s(u)\\,du,\n$$\nand the impact $I(Q)$ is the price shift required to consume $Q$ units of liquidity. Assume that the marginal supply density is a monotone function of $u$ and that the LOB is locally stationary over the horizon of interest.\n\nYou will propose a parametric impact model and evaluate it empirically on synthetic data. The parametric model is the non-linear power law\n$$\nI(Q) \\;=\\; a\\,Q^{b},\n$$\nwith parameters $a&gt;0$ and $b&gt;0$. Observations are subject to multiplicative noise: the observed impact $I_{\\text{obs}}(Q)$ satisfies\n$$\nI_{\\text{obs}}(Q) \\;=\\; I(Q)\\,\\exp(\\varepsilon),\n$$\nwhere $\\varepsilon$ is independent and identically distributed Gaussian noise with mean $0$ and variance $\\tau^{2}$.\n\nTask:\n- For each dataset described in the test suite below, generate the synthetic data as follows: draw $N$ independent trade sizes $Q_{i}$ from a log-uniform distribution on $[Q_{\\min},Q_{\\max}]$ (that is, $\\log Q_{i}$ is uniform on $[\\log Q_{\\min},\\log Q_{\\max}]$), compute the noise $\\varepsilon_{i}\\sim \\mathcal{N}(0,\\tau^{2})$, and set $I_{i,\\text{obs}}=a\\,Q_{i}^{b}\\,\\exp(\\varepsilon_{i})$.\n- Design and implement an estimator for $b$ based on the joint sample $\\{(Q_{i},I_{i,\\text{obs}})\\}_{i=1}^{N}$ and well-justified statistical principles grounded in the noise model above. Your estimator must be well-defined for strictly positive $Q_{i}$ and $I_{i,\\text{obs}}$ and should not rely on external data.\n- For each test case, output only your estimate $\\hat{b}$ for that dataset.\n\nProvide a single program that executes all test cases and prints the list of $\\hat{b}$ estimates.\n\nFoundational base you may use includes:\n- The definition of immediate market impact $I(Q)$ as the price shift necessary to execute quantity $Q$ against a LOB.\n- The integral relation $Q=\\int_{0}^{I(Q)} s(u)\\,du$ that equates consumed depth to order size.\n- Properties of the Gaussian distribution and principles of Maximum Likelihood Estimation (MLE) under correctly specified models.\n- Properties of Ordinary Least Squares (OLS) regression when applied to transformed linear models.\n\nTest suite specifications:\nFor each case, you must use the specified random seed to ensure reproducibility. In each tuple, the parameters are $(\\text{seed}, a, b, \\tau, N, Q_{\\min}, Q_{\\max})$.\n\n- Case A (concave impact, low noise, wide size range): $(314159,\\, 0.5,\\, 0.6,\\, 0.05,\\, 2000,\\, 100,\\, 50000)$.\n- Case B (near-linear impact, moderate range): $(271828,\\, 1.2,\\, 1.0,\\, 0.05,\\, 1500,\\, 50,\\, 20000)$.\n- Case C (strongly concave, higher noise, very wide range): $(424242,\\, 0.8,\\, 0.3,\\, 0.10,\\, 3000,\\, 10,\\, 100000)$.\n- Case D (moderately concave, heavier noise, fewer samples): $(8675309,\\, 0.3,\\, 0.9,\\, 0.20,\\, 800,\\, 5,\\, 5000)$.\n\nAll symbols and numbers above are exact and must be followed precisely.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of the four estimates in the order A, B, C, D, enclosed in square brackets. For example, the output must look like\n$[\\hat{b}_{A},\\hat{b}_{B},\\hat{b}_{C},\\hat{b}_{D}]$,\nwith each $\\hat{b}$ represented as a floating-point number. Round each $\\hat{b}$ to exactly $6$ decimal places in the printed output.\n\nNo physical units or angle units are involved. Percentages are not used. The final answers must be floats. The program must not read any input and must be entirely self-contained and reproducible using the specified seeds.", "solution": "The problem statement has been subjected to rigorous validation and is deemed valid. It is scientifically grounded in the established theory of market microstructure, mathematically well-posed, and provides a complete and unambiguous specification for the task. We may therefore proceed with the derivation and implementation of a solution.\n\nThe problem requires the estimation of the parameter $b$ in the non-linear power-law model for market impact, given by\n$$\nI(Q) \\;=\\; a\\,Q^{b}\n$$\nThe observations are subject to multiplicative, log-normally distributed noise. For a set of $N$ trades with sizes $\\{Q_i\\}_{i=1}^{N}$, the observed impacts $\\{I_{i,\\text{obs}}\\}_{i=1}^{N}$ are given by the model\n$$\nI_{i,\\text{obs}} \\;=\\; a\\,Q_{i}^{b}\\,\\exp(\\varepsilon_{i})\n$$\nwhere the noise terms $\\varepsilon_{i}$ are independent and identically distributed draws from a Gaussian distribution with mean $0$ and variance $\\tau^2$, i.e., $\\varepsilon_{i} \\sim \\mathcal{N}(0, \\tau^2)$.\n\nTo estimate the parameters of this non-linear model, a standard and effective technique is to linearize the relationship through a logarithmic transformation. Applying the natural logarithm to both sides of the observation equation yields:\n$$\n\\log(I_{i,\\text{obs}}) \\;=\\; \\log\\left(a\\,Q_{i}^{b}\\,\\exp(\\varepsilon_{i})\\right)\n$$\nUsing the fundamental properties of the logarithm, specifically $\\log(xyz) = \\log(x) + \\log(y) + \\log(z)$ and $\\log(x^k) = k\\log(x)$, we can expand the right-hand side:\n$$\n\\log(I_{i,\\text{obs}}) \\;=\\; \\log(a) + \\log(Q_{i}^{b}) + \\log(\\exp(\\varepsilon_{i}))\n$$\nThis simplifies to a linear equation:\n$$\n\\log(I_{i,\\text{obs}}) \\;=\\; \\log(a) + b\\,\\log(Q_{i}) + \\varepsilon_{i}\n$$\nTo formalize this as a standard linear regression problem, we introduce the following variable transformations:\n- Let $y_i = \\log(I_{i,\\text{obs}})$ be the response variable.\n- Let $x_i = \\log(Q_i)$ be the predictor variable.\n- Let $\\beta_0 = \\log(a)$ be the intercept term.\n- Let $\\beta_1 = b$ be the slope coefficient, which is our parameter of interest.\n\nThe model can now be written in the canonical form of a simple linear regression:\n$$\ny_i \\;=\\; \\beta_0 + \\beta_1 x_i + \\varepsilon_i\n$$\nThe problem states that the trade sizes $Q_i$ and noise terms $\\varepsilon_i$ are generated independently. Consequently, the predictor variable $x_i = \\log(Q_i)$ is independent of the error term $\\varepsilon_i$. The errors $\\varepsilon_i$ are i.i.d. draws from a zero-mean, constant-variance ($\\tau^2$) normal distribution. These conditions fulfill all the assumptions of the classical linear regression model.\n\nUnder these conditions, the Ordinary Least Squares (OLS) estimator is the Best Linear Unbiased Estimator (BLUE) for the parameters $\\beta_0$ and $\\beta_1$. Moreover, because the error term is normally distributed, the OLS estimator is identical to the Maximum Likelihood Estimator (MLE), which possesses strong statistical properties including consistency and asymptotic efficiency.\n\nThe OLS method finds the parameter estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ that minimize the sum of squared residuals (SSR):\n$$\n\\text{SSR} \\;=\\; \\sum_{i=1}^{N} (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_i)^2\n$$\nThe well-established closed-form solution for the slope coefficient $\\hat{\\beta}_1$ is:\n$$\n\\hat{\\beta}_1 \\;=\\; \\frac{\\sum_{i=1}^{N}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{N}(x_i - \\bar{x})^2}\n$$\nwhere $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N} x_i$ and $\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N} y_i$ are the sample means of the predictor and response variables, respectively. Our estimate for the market impact exponent, $\\hat{b}$, is therefore this estimated slope coefficient, $\\hat{b} = \\hat{\\beta}_1$.\n\nThe algorithmic procedure for each test case is as follows:\n$1$. **Data Generation**:\n    - For a given test case specified by $(\\text{seed}, a, b, \\tau, N, Q_{\\min}, Q_{\\max})$, initialize a pseudorandom number generator with the specified seed for reproducibility.\n    - Generate $N$ trade sizes, $Q_i$, from a log-uniform distribution on the interval $[Q_{\\min}, Q_{\\max}]$. This is equivalent to drawing $\\log(Q_i)$ from a uniform distribution $\\mathcal{U}(\\log(Q_{\\min}), \\log(Q_{\\max}))$.\n    - Generate $N$ noise terms, $\\varepsilon_i$, from a normal distribution $\\mathcal{N}(0, \\tau^2)$.\n    - Compute the synthetic observed impacts $I_{i,\\text{obs}} = a\\,Q_i^b \\exp(\\varepsilon_i)$.\n$2$. **Model Estimation**:\n    - Transform the generated data by taking the natural logarithm: $y_i = \\log(I_{i,\\text{obs}})$ and $x_i = \\log(Q_i)$.\n    - Formulate the linear system $\\mathbf{y} = X\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y}$ is the vector of $y_i$ values, $\\boldsymbol{\\beta} = [\\beta_0, \\beta_1]^T$, and $X$ is the $N \\times 2$ design matrix whose first column is all ones and second column is the vector of $x_i$ values.\n    - Solve for the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ using a numerical least-squares solver. The second element of this vector, $\\hat{\\beta}_1$, provides the estimate $\\hat{b}$.\nThis method is statistically well-founded and computationally robust.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Generates synthetic market impact data and estimates the power-law exponent 'b'\n    for a series of test cases using Ordinary Least Squares on a log-transformed model.\n    \"\"\"\n\n    # Test cases: (seed, a, b, tau, N, Q_min, Q_max)\n    # The parameters correspond to: random seed, scale parameter 'a', exponent 'b',\n    # noise standard deviation 'tau', number of samples 'N', minimum trade size 'Q_min',\n    # and maximum trade size 'Q_max'.\n    test_cases = [\n        # Case A (concave impact, low noise, wide size range)\n        (314159, 0.5, 0.6, 0.05, 2000, 100, 50000),\n        # Case B (near-linear impact, moderate range)\n        (271828, 1.2, 1.0, 0.05, 1500, 50, 20000),\n        # Case C (strongly concave, higher noise, very wide range)\n        (424242, 0.8, 0.3, 0.10, 3000, 10, 100000),\n        # Case D (moderately concave, heavier noise, fewer samples)\n        (8675309, 0.3, 0.9, 0.20, 800, 5, 5000),\n    ]\n\n    results = []\n\n    for seed, a, b, tau, N, Q_min, Q_max in test_cases:\n        # Step 1: Generate synthetic data.\n        # Initialize a random number generator with the specified seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate N trade sizes Q from a log-uniform distribution on [Q_min, Q_max].\n        # This is done by drawing from a uniform distribution in log-space.\n        log_Q = rng.uniform(np.log(Q_min), np.log(Q_max), N)\n        Q = np.exp(log_Q)\n\n        # Generate N i.i.d. Gaussian noise terms.\n        epsilon = rng.normal(loc=0.0, scale=tau, size=N)\n\n        # Compute the observed impact I_obs = a * Q^b * exp(epsilon).\n        I_obs = a * (Q ** b) * np.exp(epsilon)\n\n        # Step 2: Transform the data to linearize the model.\n        # log(I_obs) = log(a) + b*log(Q) + epsilon\n        log_I_obs = np.log(I_obs)\n\n        # Step 3: Perform Ordinary Least Squares (OLS) regression.\n        # We model log_I_obs = beta_0 + beta_1 * log_Q, where beta_1 is our estimate for b.\n        y = log_I_obs\n        x = log_Q\n\n        # Construct the design matrix X for the linear system y = X * beta.\n        # The first column is for the intercept (beta_0), the second for the slope (beta_1).\n        X = np.vstack([np.ones(N), x]).T\n\n        # Solve for the coefficient vector beta = [beta_0, beta_1] using a\n        # standard and numerically stable least-squares solver.\n        # beta[0] will be the estimate for log(a), and beta[1] for b.\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n        \n        b_estimate = beta[1]\n        results.append(b_estimate)\n        \n    # Final print statement in the exact required format.\n    # The output must be a single line: a list of comma-separated values\n    # enclosed in square brackets, with each value rounded to 6 decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function.\nsolve()\n```"}, {"introduction": "Market makers are the cornerstone of liquidity, but their operations involve a delicate balance between earning the bid-ask spread and managing the risk of holding an undesirable inventory. This exercise guides you in building a simulation of a Designated Market Maker (DMM) who follows a set of rules for quoting while being exposed to a stochastically evolving risk environment. The model incorporates key concepts such as an inventory-based reservation price and a mean-reverting Vasicek process for the external risk factor.\n\nThis hands-on simulation [@problem_id:2408375] provides direct insight into the dynamic challenges faced by liquidity providers and serves as a foundational exercise in agent-based modeling in finance. You will implement a complete micro-level simulation from first principles, watching as the agent's profit and loss evolves in response to its decisions and random market events.", "id": "2408375", "problem": "You are tasked with building a self-contained discrete-time simulation of a designated market maker (DMM) operating in a limit order book under explicit microstructure constraints and a stochastic inventory risk factor. The DMM must continuously quote on both sides of the book, satisfy a maximum spread and a minimum quote size, and accrue revenue from executions while incurring an inventory risk penalty driven by a Vasicek process.\n\nModel specification:\n\n- Time is discretized into steps of size $\\Delta t > 0$, indexed by $t \\in \\{0,1,\\ldots,T-1\\}$.\n\n- There is a constant reference mid-price $m_0 \\in \\mathbb{R}$.\n\n- The DMM must always post quotes on both sides with a symmetric half-spread $h_t \\in \\mathbb{R}_{\\ge 0}$ such that the total spread is $2 h_t \\le S_{\\max}$, where $S_{\\max} > 0$ is the imposed maximum spread. The DMM’s minimum quote size is $q_{\\min} \\in \\mathbb{N}$, which must be posted on both the bid and the ask at every time step.\n\n- The DMM maintains an inventory $I_t \\in \\mathbb{Z}$, with initial inventory $I_0 \\in \\mathbb{Z}$. At each time step the inventory changes due to executions at the posted quotes.\n\n- The DMM computes a reservation price $r_t \\in \\mathbb{R}$ that skews quotes based on inventory and a Vasicek (Ornstein–Uhlenbeck) risk factor $X_t$:\n  $$ r_t = m_0 - \\varphi \\, I_t \\, X_t, $$\n  where $\\varphi \\in \\mathbb{R}_{\\ge 0}$ is a given sensitivity parameter.\n\n- The posted bid and ask at time $t$ are:\n  $$ p^{\\text{bid}}_t = r_t - h_t, \\qquad p^{\\text{ask}}_t = r_t + h_t, $$\n  with the constraint $2 h_t \\le S_{\\max}$. The intent is that the DMM adheres to the constraint by using\n  $$ h_t = \\min\\left(H, \\frac{S_{\\max}}{2}\\right), $$\n  where $H \\in \\mathbb{R}_{>0}$ is a given base half-spread.\n\n- The risk factor $X_t$ follows a Vasicek process in continuous time,\n  $$ dX_t = \\kappa \\, (\\theta - X_t) \\, dt + \\sigma \\, dW_t, $$\n  with $\\kappa \\in \\mathbb{R}_{>0}$, $\\theta \\in \\mathbb{R}$, $\\sigma \\in \\mathbb{R}_{\\ge 0}$, and standard Brownian motion $W_t$. In discrete time with step $\\Delta t$, simulate $X_t$ via\n  $$ X_{t+\\Delta t} = X_t + \\kappa \\, (\\theta - X_t) \\, \\Delta t + \\sigma \\, \\sqrt{\\Delta t} \\, Z_t, $$\n  where $\\{Z_t\\}$ are independent standard normal random variables, and $X_0$ is given.\n\n- Market order arrivals that can execute against the DMM’s posted quotes are independent Poisson processes with intensities $\\lambda_a \\in \\mathbb{R}_{\\ge 0}$ for buy market orders (hitting the ask) and $\\lambda_b \\in \\mathbb{R}_{\\ge 0}$ for sell market orders (hitting the bid). Over a time step $\\Delta t$, the probability of at least one arrival on a side is\n  $$ p_a = 1 - e^{-\\lambda_a \\Delta t}, \\qquad p_b = 1 - e^{-\\lambda_b \\Delta t}. $$\n  Assume that in each time step, on each side, at most one fill of size $q_{\\min}$ can occur. Let $F^{\\text{ask}}_t \\in \\{0,1\\}$ be an indicator for an ask-side fill, and $F^{\\text{bid}}_t \\in \\{0,1\\}$ be an indicator for a bid-side fill, with\n  $$ \\mathbb{P}(F^{\\text{ask}}_t = 1) = p_a, \\quad \\mathbb{P}(F^{\\text{bid}}_t = 1) = p_b, $$\n  and all random variables independent across time and sides.\n\n- The inventory updates as\n  $$ I_{t+1} = I_t + q_{\\min} \\, F^{\\text{bid}}_t - q_{\\min} \\, F^{\\text{ask}}_t. $$\n\n- The DMM’s realized spread revenue increment at time $t$ is\n  $$ \\Pi^{\\text{rev}}_t = q_{\\min} \\left( F^{\\text{ask}}_t \\, p^{\\text{ask}}_t - F^{\\text{bid}}_t \\, p^{\\text{bid}}_t \\right). $$\n\n- The DMM incurs a nonnegative instantaneous inventory risk penalty that depends on the current inventory and the current risk factor through\n  $$ \\rho_t = \\max(X_t, 0), \\qquad \\Pi^{\\text{risk}}_t = - \\frac{1}{2} \\, \\rho_t \\, I_t^2 \\, \\Delta t. $$\n\n- The total objective accumulated over the horizon is\n  $$ U = \\sum_{t=0}^{T-1} \\left( \\Pi^{\\text{rev}}_t + \\Pi^{\\text{risk}}_t \\right). $$\n\nSimulation protocol per time step $t \\in \\{0,1,\\ldots,T-1\\}$:\n\n1. Compute $r_t$ using $I_t$ and $X_t$; set $h_t = \\min(H, S_{\\max}/2)$; then set $p^{\\text{bid}}_t$ and $p^{\\text{ask}}_t$.\n2. Draw independent $F^{\\text{ask}}_t$ and $F^{\\text{bid}}_t$ with probabilities $p_a$ and $p_b$ respectively.\n3. Compute the revenue increment $\\Pi^{\\text{rev}}_t$.\n4. Compute the risk penalty increment $\\Pi^{\\text{risk}}_t$ using $\\rho_t = \\max(X_t, 0)$ and the current $I_t$.\n5. Update the inventory to $I_{t+1}$.\n6. Update $X_{t+\\Delta t}$ using the discrete Vasicek update with a fresh $Z_t \\sim \\mathcal{N}(0,1)$.\n\nRandomness and reproducibility:\n\n- Use a single pseudorandom number generator instance seeded with the integer $2025$, and reuse it across the entire simulation in the order the test cases are evaluated and within each case in the order of operations specified above.\n\nTest suite:\n\nFor each parameter set below, simulate the process and report the scalar $U$ rounded to six decimal places.\n\n- Test case A (balanced flow, unconstrained base spread):\n  - $T = 500$, $\\Delta t = 0.1$, $m_0 = 100.0$, $S_{\\max} = 0.20$, $H = 0.08$, $q_{\\min} = 1$,\n  - $\\lambda_a = 3.0$, $\\lambda_b = 3.0$,\n  - $\\kappa = 2.0$, $\\theta = 0.5$, $\\sigma = 0.5$, $X_0 = 0.5$,\n  - $\\varphi = 0.01$, $I_0 = 0$.\n\n- Test case B (no arrivals edge case):\n  - $T = 500$, $\\Delta t = 0.1$, $m_0 = 100.0$, $S_{\\max} = 0.20$, $H = 0.08$, $q_{\\min} = 1$,\n  - $\\lambda_a = 0.0$, $\\lambda_b = 0.0$,\n  - $\\kappa = 2.0$, $\\theta = 0.5$, $\\sigma = 0.5$, $X_0 = 0.5$,\n  - $\\varphi = 0.01$, $I_0 = 0$.\n\n- Test case C (tight spread cap, imbalanced flow, larger quote size, nonzero initial inventory):\n  - $T = 300$, $\\Delta t = 0.1$, $m_0 = 50.0$, $S_{\\max} = 0.20$, $H = 0.20$, $q_{\\min} = 2$,\n  - $\\lambda_a = 8.0$, $\\lambda_b = 1.0$,\n  - $\\kappa = 1.0$, $\\theta = 0.5$, $\\sigma = 0.3$, $X_0 = 0.2$,\n  - $\\varphi = 0.02$, $I_0 = 5$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the three results $[U_A, U_B, U_C]$ as a comma-separated list enclosed in square brackets, where each value is rounded to six decimal places (for example, $[12.345678,0.000000,-3.210987]$). No other text should be printed.", "solution": "The problem statement is submitted for validation.\n\n### Step 1: Extract Givens\n\n**Model Parameters:**\n-   Discrete time steps: $t \\in \\{0, 1, \\ldots, T-1\\}$ of duration $\\Delta t > 0$.\n-   Reference mid-price: $m_0 \\in \\mathbb{R}$.\n-   Maximum total spread: $S_{\\max} > 0$.\n-   Base half-spread: $H \\in \\mathbb{R}_{>0}$.\n-   Minimum quote size: $q_{\\min} \\in \\mathbb{N}$.\n-   Initial inventory: $I_0 \\in \\mathbb{Z}$.\n-   Reservation price sensitivity: $\\varphi \\in \\mathbb{R}_{\\ge 0}$.\n-   Vasicek process parameters: $\\kappa \\in \\mathbb{R}_{>0}$, $\\theta \\in \\mathbb{R}$, $\\sigma \\in \\mathbb{R}_{\\ge 0}$.\n-   Initial risk factor value: $X_0$.\n-   Market order arrival intensities: $\\lambda_a, \\lambda_b \\in \\mathbb{R}_{\\ge 0}$.\n\n**Model Equations and Logic:**\n-   Reservation price: $r_t = m_0 - \\varphi \\, I_t \\, X_t$.\n-   Half-spread: $h_t = \\min\\left(H, \\frac{S_{\\max}}{2}\\right)$.\n-   Posted quotes: $p^{\\text{bid}}_t = r_t - h_t, \\quad p^{\\text{ask}}_t = r_t + h_t$.\n-   Discrete Vasicek process for risk factor $X_t$: $X_{t+\\Delta t} = X_t + \\kappa \\, (\\theta - X_t) \\, \\Delta t + \\sigma \\, \\sqrt{\\Delta t} \\, Z_t$, with $Z_t \\sim \\mathcal{N}(0,1)$.\n-   Fill probabilities for time step $\\Delta t$: $p_a = 1 - e^{-\\lambda_a \\Delta t}, \\quad p_b = 1 - e^{-\\lambda_b \\Delta t}$.\n-   Fill indicators: $F^{\\text{ask}}_t, F^{\\text{bid}}_t \\in \\{0,1\\}$, with $\\mathbb{P}(F^{\\text{ask}}_t = 1) = p_a$ and $\\mathbb{P}(F^{\\text{bid}}_t = 1) = p_b$.\n-   Inventory update: $I_{t+1} = I_t + q_{\\min} \\, F^{\\text{bid}}_t - q_{\\min} \\, F^{\\text{ask}}_t$.\n-   Spread revenue increment: $\\Pi^{\\text{rev}}_t = q_{\\min} \\left( F^{\\text{ask}}_t \\, p^{\\text{ask}}_t - F^{\\text{bid}}_t \\, p^{\\text{bid}}_t \\right)$.\n-   Inventory risk penalty increment: $\\Pi^{\\text{risk}}_t = - \\frac{1}{2} \\, \\rho_t \\, I_t^2 \\, \\Delta t$, where $\\rho_t = \\max(X_t, 0)$.\n-   Total objective function: $U = \\sum_{t=0}^{T-1} \\left( \\Pi^{\\text{rev}}_t + \\Pi^{\\text{risk}}_t \\right)$.\n\n**Simulation Protocol:**\nThe protocol specifies a strict sequence of operations for each time step $t$:\n1.  Compute $r_t$, $h_t$, $p^{\\text{bid}}_t$, $p^{\\text{ask}}_t$ using current state $(I_t, X_t)$.\n2.  Draw random fill indicators $F^{\\text{ask}}_t, F^{\\text{bid}}_t$.\n3.  Compute revenue increment $\\Pi^{\\text{rev}}_t$.\n4.  Compute risk penalty $\\Pi^{\\text{risk}}_t$ using current state $(I_t, X_t)$.\n5.  Update inventory to $I_{t+1}$.\n6.  Update risk factor to $X_{t+\\Delta t}$ using a new random draw $Z_t$.\n\n**Reproducibility:**\n-   A single pseudorandom number generator (PRNG) must be used.\n-   The PRNG must be seeded with the integer $2025$.\n-   The PRNG must be used sequentially across test cases and within each simulation step according to the protocol.\n\n**Test Cases:**\n-   Case A: $T = 500, \\Delta t = 0.1, m_0 = 100.0, S_{\\max} = 0.20, H = 0.08, q_{\\min} = 1, \\lambda_a = 3.0, \\lambda_b = 3.0, \\kappa = 2.0, \\theta = 0.5, \\sigma = 0.5, X_0 = 0.5, \\varphi = 0.01, I_0 = 0$.\n-   Case B: $T = 500, \\Delta t = 0.1, m_0 = 100.0, S_{\\max} = 0.20, H = 0.08, q_{\\min} = 1, \\lambda_a = 0.0, \\lambda_b = 0.0, \\kappa = 2.0, \\theta = 0.5, \\sigma = 0.5, X_0 = 0.5, \\varphi = 0.01, I_0 = 0$.\n-   Case C: $T = 300, \\Delta t = 0.1, m_0 = 50.0, S_{\\max} = 0.20, H = 0.20, q_{\\min} = 2, \\lambda_a = 8.0, \\lambda_b = 1.0, \\kappa = 1.0, \\theta = 0.5, \\sigma = 0.3, X_0 = 0.2, \\varphi = 0.02, I_0 = 5$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria.\n1.  **Scientifically Grounded:** The model is a simplified but standard representation of a market maker's behavior, drawing upon established concepts in computational finance and market microstructure theory, such as inventory-based quoting (related to Ho & Stoll, Avellaneda & Stoikov) and the use of an Ornstein-Uhlenbeck process for a stochastic factor. The discretization of the stochastic process and the modeling of order arrivals are standard techniques in simulation. The problem is scientifically sound.\n2.  **Well-Posed:** The problem is a simulation task. It provides all necessary parameters, initial conditions, and a precise, sequential protocol for execution. The specification of the random number generation process, including the seed and order of operations, ensures that a unique, deterministic result can be computed. The problem is well-posed.\n3.  **Objective:** The problem is described using precise mathematical and algorithmic language. All terms are formally defined. There is no subjective or ambiguous content.\n4.  **Flaw Checklist:** A review of potential flaws shows none are present. The problem is not unsound, non-formalizable, incomplete, contradictory, unrealistic, or ill-posed. The sequence of random number generation within each time step is implicitly defined by the simulation protocol steps: random draws for market orders (step 2) occur before the random draw for the risk factor update (step 6). This is unambiguous.\n\n### Step 3: Verdict and Action\n\n**Verdict:** The problem is **valid**. It is a well-defined and scientifically grounded simulation exercise in computational finance.\n\n**Action:** Proceed with the solution.\n\nThe problem requires the implementation of a discrete-time Monte Carlo simulation of a designated market maker's (DMM) profit and loss accumulation. The DMM's behavior is governed by a set of rules for quoting and is subject to inventory risk, which is modulated by an external stochastic factor. The solution is constructed by algorithmically executing the specified simulation protocol.\n\nThe state of the system at any time step $t$ is defined by the DMM's inventory $I_t$ and the value of the risk factor $X_t$. The simulation proceeds by iterating from an initial state $(I_0, X_0)$ for $T$ steps of duration $\\Delta t$.\n\n**DMM Strategy and Quoting:**\nAt each time step $t$, the DMM computes a *reservation price* $r_t$. This price is a central concept in market making models, representing the agent's internal valuation of the asset, adjusted for its current risk exposure. The formula $r_t = m_0 - \\varphi \\, I_t \\, X_t$ formalizes this adjustment. A positive inventory ($I_t > 0$) coupled with a positive risk state ($X_t > 0$) lowers the reservation price, making the DMM more eager to sell (by lowering its ask) and less eager to buy (by lowering its bid), thereby reducing its inventory. The parameter $\\varphi$ dictates the aggressiveness of this inventory management.\nQuotes are then set symmetrically around this reservation price: $p^{\\text{bid}}_t = r_t - h_t$ and $p^{\\text{ask}}_t = r_t + h_t$. The half-spread $h_t$ is constant throughout the simulation, determined by the DMM's base spread $H$ and constrained by the regulatory maximum spread $S_{\\max}$, as $h_t = \\min(H, S_{\\max}/2)$.\n\n**Stochastic Dynamics:**\nThe simulation environment has two sources of randomness.\n1.  **Market Order Arrivals:** The arrival of market orders that execute against the DMM's quotes is modeled as two independent Poisson processes. For a discrete time step $\\Delta t$, this simplifies to independent Bernoulli trials. The probability of a buy market order hitting the DMM's ask is $p_a = 1 - e^{-\\lambda_a \\Delta t}$, and the probability of a sell market order hitting the bid is $p_b = 1 - e^{-\\lambda_b \\Delta t}$. At each step, we generate indicator variables $F^{\\text{ask}}_t$ and $F^{\\text{bid}}_t$ based on these probabilities.\n2.  **Risk Factor Evolution:** The risk factor $X_t$ follows a Vasicek (Ornstein-Uhlenbeck) process, a mean-reverting stochastic process. Its evolution is simulated using the Euler-Maruyama discretization: $X_{t+\\Delta t} = X_t + \\kappa (\\theta - X_t) \\Delta t + \\sigma \\sqrt{\\Delta t} Z_t$, where $Z_t$ is a standard normal random variable. This captures a fluctuating external factor that influences the DMM's perception of risk.\n\n**Objective Function and State Updates:**\nFor each time step $t$, we calculate the incremental change in the DMM's total utility.\n-   The spread revenue, $\\Pi^{\\text{rev}}_t = q_{\\min} ( F^{\\text{ask}}_t p^{\\text{ask}}_t - F^{\\text{bid}}_t p^{\\text{bid}}_t )$, is the classic profit from buying low and selling high.\n-   The inventory risk penalty, $\\Pi^{\\text{risk}}_t = - \\frac{1}{2} \\rho_t I_t^2 \\Delta t$ with $\\rho_t = \\max(X_t, 0)$, represents the cost of holding a non-zero inventory. This cost is quadratic in the inventory size, reflecting risk aversion, and is scaled by the time-varying, non-negative risk parameter $\\rho_t$.\nThe total utility $U$ is the sum of these increments over the simulation horizon.\n\nAfter calculating the increments for step $t$ using the state $(I_t, X_t)$, the state is updated to $(I_{t+1}, X_{t+1})$ to prepare for the next iteration. The inventory is updated based on the realized fills: $I_{t+1} = I_t + q_{\\min} F^{\\text{bid}}_t - q_{\\min} F^{\\text{ask}}_t$. The risk factor $X_t$ is updated according to its stochastic process.\n\n**Implementation and Reproducibility:**\nThe solution is implemented in Python, adhering to the specified simulation protocol. To ensure reproducibility of the stochastic simulation, a single instance of a pseudorandom number generator is initialized with the seed $2025$. This generator is then used for all random draws across all test cases, in the exact sequence dictated by the protocol: for each time step $t$, a draw for $F^{\\text{ask}}_t$, then for $F^{\\text{bid}}_t$, and finally for $Z_t$. The code calculates the total utility $U$ for each test case and reports the results in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(params, rng):\n    \"\"\"\n    Runs a single simulation for a given parameter set and RNG.\n\n    Args:\n        params (dict): A dictionary containing all model parameters.\n        rng (numpy.random.Generator): The random number generator instance.\n\n    Returns:\n        float: The total accumulated utility U.\n    \"\"\"\n    T = params['T']\n    dt = params['dt']\n    m0 = params['m0']\n    S_max = params['S_max']\n    H = params['H']\n    q_min = params['q_min']\n    lambda_a = params['lambda_a']\n    lambda_b = params['lambda_b']\n    kappa = params['kappa']\n    theta = params['theta']\n    sigma = params['sigma']\n    X0 = params['X0']\n    phi = params['phi']\n    I0 = params['I0']\n\n    # Initialize state variables\n    I_t = float(I0)\n    X_t = float(X0)\n    total_utility = 0.0\n\n    # Pre-calculate constant values\n    h_t = min(H, S_max / 2.0)\n    p_a = 1.0 - np.exp(-lambda_a * dt)\n    p_b = 1.0 - np.exp(-lambda_b * dt)\n    sqrt_dt = np.sqrt(dt)\n\n    for _ in range(T):\n        # Step 1: Compute reservation price and quotes\n        r_t = m0 - phi * I_t * X_t\n        p_ask_t = r_t + h_t\n        p_bid_t = r_t - h_t\n\n        # Step 2: Draw fill indicators\n        F_ask_t = 1 if rng.random() < p_a else 0\n        F_bid_t = 1 if rng.random() < p_b else 0\n\n        # Step 3: Compute revenue increment\n        pi_rev_t = q_min * (F_ask_t * p_ask_t - F_bid_t * p_bid_t)\n\n        # Step 4: Compute risk penalty increment\n        rho_t = max(X_t, 0.0)\n        pi_risk_t = -0.5 * rho_t * (I_t**2) * dt\n\n        # Accumulate utility\n        total_utility += pi_rev_t + pi_risk_t\n\n        # Step 5: Update inventory for the next step\n        I_t = I_t + q_min * F_bid_t - q_min * F_ask_t\n\n        # Step 6: Update risk factor for the next step\n        Z_t = rng.normal(0, 1)\n        X_t = X_t + kappa * (theta - X_t) * dt + sigma * sqrt_dt * Z_t\n        \n    return total_utility\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A\n        {\n            'T': 500, 'dt': 0.1, 'm0': 100.0, 'S_max': 0.20, 'H': 0.08, 'q_min': 1,\n            'lambda_a': 3.0, 'lambda_b': 3.0,\n            'kappa': 2.0, 'theta': 0.5, 'sigma': 0.5, 'X0': 0.5,\n            'phi': 0.01, 'I0': 0\n        },\n        # Test case B\n        {\n            'T': 500, 'dt': 0.1, 'm0': 100.0, 'S_max': 0.20, 'H': 0.08, 'q_min': 1,\n            'lambda_a': 0.0, 'lambda_b': 0.0,\n            'kappa': 2.0, 'theta': 0.5, 'sigma': 0.5, 'X0': 0.5,\n            'phi': 0.01, 'I0': 0\n        },\n        # Test case C\n        {\n            'T': 300, 'dt': 0.1, 'm0': 50.0, 'S_max': 0.20, 'H': 0.20, 'q_min': 2,\n            'lambda_a': 8.0, 'lambda_b': 1.0,\n            'kappa': 1.0, 'theta': 0.5, 'sigma': 0.3, 'X0': 0.2,\n            'phi': 0.02, 'I0': 5\n        }\n    ]\n\n    # Initialize a single PRNG instance as required for reproducibility\n    rng = np.random.default_rng(2025)\n    \n    results = []\n    for case_params in test_cases:\n        result = run_simulation(case_params, rng)\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "Instead of pre-defining an agent's trading strategy with fixed rules, how can we design an agent that learns to trade optimally on its own? This practice introduces the powerful framework of Reinforcement Learning (RL) applied to a classic microstructure problem: the trade-off between the execution cost of a market order and the execution uncertainty of a limit order. You will implement the Q-learning algorithm, allowing an agent to discover its own optimal strategy by interacting with a simplified, simulated limit order book.\n\nBy completing this exercise [@problem_id:2408335], you will gain practical experience with a cutting-edge technique used to solve complex sequential decision problems in finance. It provides a gateway to understanding the principles behind modern AI-driven trading systems, where agents adapt their behavior based on market conditions to maximize their objectives.", "id": "2408335", "problem": "Construct a fully specified, discrete-time market microstructure simulation with a single buyer agent interacting with a stylized limit order book. The environment is a finite Markov decision process in which the agent decides whether and where to place an order, and the order book evolves through exogenous events. The objective is to simulate learning dynamics that maximize expected discounted utility of execution while accounting for waiting costs. The required output is the agent’s learned greedy action in three specified states and the estimated average discounted return under the learned greedy policy for each of several parameter sets. All mathematical entities in this specification are in LaTeX.\n\nThe environment is defined as follows.\n\n1. Time, episodes, and discounting:\n- Time is discrete, indexed by $t \\in \\{0,1,2,\\dots\\}$, with a maximum per-episode horizon $T_{\\max} \\in \\mathbb{N}$.\n- Each episode starts at $t = 0$ and terminates upon the first execution of the agent’s buy order or at $t = T_{\\max}$, whichever occurs first.\n- The discount factor is $\\gamma \\in (0,1)$.\n- The episodic return is the discounted sum $G = \\sum_{t=0}^{\\tau} \\gamma^t r_t$, where $\\tau$ is the terminal time within the horizon.\n\n2. Order book state and constraints:\n- The state is a triple $s_t = (h_t,q_t,p_t)$ where:\n  - $h_t \\in \\{0,1\\}$ indicates whether the agent currently has an outstanding buy limit order at the best bid ($h_t=1$) or not ($h_t=0$).\n  - $q_t \\in \\{0,1,2,3\\}$ is the total number of outstanding orders at the best bid excluding the agent’s own order, capped at $3$.\n  - $p_t \\in \\{0,1,2,3\\}$ is the number of orders standing ahead of the agent’s order in the best bid queue (the agent’s queue position in terms of the number in front), capped at $3$. If $h_t=0$, then $p_t$ is set to $0$ by definition.\n- The initial state for each episode is $s_0 = (0,q_0,0)$, where $q_0 \\in \\{0,1,2,3\\}$ is specified by the test case.\n\n3. Actions:\n- At each decision time $t$, the agent chooses one action $a_t \\in \\mathcal{A} = \\{0,1,2,3\\}$ encoded as:\n  - $0$: wait (do nothing),\n  - $1$: place a limit buy order at the current best bid (valid only if $h_t=0$; otherwise it has no effect beyond waiting),\n  - $2$: place a market buy order that immediately executes at the best ask and terminates the episode,\n  - $3$: cancel the existing limit order (valid only if $h_t=1$; otherwise it has no effect beyond waiting).\n- Action validity:\n  - If $h_t=1$ and $a_t=1$, it is treated as action $0$ (wait).\n  - If $h_t=0$ and $a_t=3$, it is treated as action $0$ (wait).\n\n4. Prices, spread, and rewards:\n- Let the half-spread be $s/2$, where $s \\in \\{1,2,\\dots\\}$ is specified. Normalize the reference value to $0$.\n- If the agent executes a market buy (action $2$), the immediate reward is $-s/2$ and the episode terminates at that step.\n- If the agent’s limit buy order executes against an arriving sell market order while the agent is at the front of the bid queue ($p_t=0$), the immediate reward is $+s/2$ and the episode terminates at that step.\n- If an episode step does not terminate (no execution), the reward for that step is the waiting cost $-w$ where $w>0$ is specified.\n- If the episode reaches the horizon $T_{\\max}$ without execution, the accumulated rewards up to $T_{\\max}-1$ (inclusive) are counted; there is no terminal bonus or penalty at $T_{\\max}$ beyond the per-step waiting costs that were already applied.\n\n5. Exogenous order book events and transitions within a step:\n- At each non-terminal step when the agent did not choose action $2$, one exogenous event $e_t$ occurs, drawn independently with probabilities:\n  - sell market order arrival with probability $p_{\\mathrm{ms}}$,\n  - limit buy arrival at the best bid with probability $p_{\\mathrm{lb}}$,\n  - cancellation at the best bid with probability $p_{\\mathrm{cb}}$,\n  with $p_{\\mathrm{ms}} + p_{\\mathrm{lb}} + p_{\\mathrm{cb}} = 1$.\n- State transitions given the chosen action and the event:\n  - If $a_t = 1$ and $h_t=0$: the agent posts a limit buy at the best bid. The next state has $h_{t}^{+}=1$, $p_{t}^{+} = \\min\\{q_t,3\\}$, and $q_{t}^{+} = q_t$.\n  - If $a_t = 3$ and $h_t=1$: the agent cancels. The next state has $h_{t}^{+}=0$, $p_{t}^{+}=0$, and $q_{t}^{+}=q_t$.\n  - If $a_t \\in \\{0,1,3\\}$ after action effects (i.e., the episode has not terminated by action), an exogenous event occurs and updates $(h,q,p)$ as follows:\n    - If the event is a sell market order:\n      - If $h=1$ and $p=0$: the agent executes immediately; the episode terminates at this step with reward $+s/2$ and no waiting cost for this step.\n      - Else: if $q>0$, then $q \\leftarrow q-1$; if additionally $h=1$ and $p>0$, then $p \\leftarrow p-1$.\n    - If the event is a cancellation at the best bid:\n      - If $q>0$, then $q \\leftarrow q-1$.\n      - If $h=1$ and $p>0$, then $p \\leftarrow p-1$; if $p=0$, it remains $0$.\n    - If the event is a limit buy arrival at the best bid:\n      - $q \\leftarrow \\min\\{q+1,3\\}$; if $h=1$, then $p$ is unchanged.\n  - If the event does not lead to terminal execution, the per-step reward is $-w$.\n  - If $a_t=2$, the episode terminates immediately with reward $-s/2$, and no exogenous event occurs at that step.\n- All updates obey the caps $q \\in \\{0,1,2,3\\}$ and $p \\in \\{0,1,2,3\\}$, and $p \\le q$ whenever $h=1$.\n\n6. Learning dynamics:\n- The agent maintains a state-action value function $Q(s,a)$ initialized to $0$ for all $(s,a)$ at the start of training.\n- At each visited transition $(s_t,a_t,r_t,s_{t+1})$, the agent updates\n  $$Q(s_t,a_t) \\leftarrow (1-\\alpha) Q(s_t,a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s_{t+1},a')\\right],$$\n  where $\\alpha \\in (0,1]$ is the learning rate.\n- During training, the agent selects actions using an $\\varepsilon$-greedy rule with parameter $\\varepsilon \\in [0,1)$: with probability $\\varepsilon$, choose a uniformly random action from $\\mathcal{A}$; otherwise choose an action that maximizes $Q(s,a)$ at the current state. Ties in $\\arg\\max$ are broken deterministically by choosing the smallest action index.\n- After training, the learned greedy policy $\\pi(s) \\in \\mathcal{A}$ at any state $s$ is the smallest-index action maximizing $Q(s,a)$.\n\n7. Randomness control:\n- All exogenous events are sampled using a pseudorandom number generator seeded by an integer seed specified in each test case. For reproducibility, use this seed for training. For post-training evaluation, reseed the pseudorandom number generator with the seed incremented by $1$.\n\n8. Evaluation states and metrics:\n- Let the learned greedy action be evaluated at the following three states:\n  - $s^{(1)} = (0,q_0,0)$,\n  - $s^{(2)} = (1,1,0)$,\n  - $s^{(3)} = (1,3,3)$.\n- Additionally, estimate the average discounted return of the learned greedy policy by simulating $N_{\\mathrm{eval}}$ independent episodes from $s_0=(0,q_0,0)$ with $\\varepsilon=0$, discounting by $\\gamma$, and horizon $T_{\\max}$, using the post-training pseudorandom seed. The sample average of episode returns must be reported as a floating-point number rounded to four decimal places.\n\nTest suite. For each test case below, the parameters are given as ordered tuples\n$(s, w, \\gamma, \\alpha, \\varepsilon, T_{\\max}, E, N_{\\mathrm{eval}}, q_0, p_{\\mathrm{ms}}, p_{\\mathrm{lb}}, p_{\\mathrm{cb}}, \\text{seed})$ with the meanings defined above. All numbers are given explicitly:\n\n- Case A (general case):\n  - $(2,\\, 0.02,\\, 0.95,\\, 0.20,\\, 0.10,\\, 50,\\, 4000,\\, 2000,\\, 2,\\, 0.50,\\, 0.30,\\, 0.20,\\, 7)$.\n- Case B (fast execution environment):\n  - $(2,\\, 0.02,\\, 0.95,\\, 0.20,\\, 0.10,\\, 50,\\, 3000,\\, 2000,\\, 2,\\, 0.80,\\, 0.10,\\, 0.10,\\, 11)$.\n- Case C (costly waiting environment):\n  - $(2,\\, 0.20,\\, 0.95,\\, 0.20,\\, 0.10,\\, 50,\\, 4000,\\, 2000,\\, 3,\\, 0.20,\\, 0.50,\\, 0.30,\\, 13)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of four elements containing:\n- the learned greedy action at $s^{(1)}$ as an integer in $\\{0,1,2,3\\}$,\n- the learned greedy action at $s^{(2)}$ as an integer in $\\{0,1,2,3\\}$,\n- the learned greedy action at $s^{(3)}$ as an integer in $\\{0,1,2,3\\}$,\n- the estimated average discounted return under the learned greedy policy (rounded to four decimal places).\n\nThus, the final output must have the form\n$[[a^{(1)}_A,a^{(2)}_A,a^{(3)}_A,\\bar{G}_A],[a^{(1)}_B,a^{(2)}_B,a^{(3)}_B,\\bar{G}_B],[a^{(1)}_C,a^{(2)}_C,a^{(3)}_C,\\bar{G}_C]]$ where the subscripts refer to cases A, B, and C, respectively. No additional text should be printed.", "solution": "The user has provided a problem statement that requires the construction of a discrete-time market microstructure simulation. This simulation involves a single agent learning to trade in a stylized limit order book environment, modeled as a finite Markov Decision Process (MDP). The agent's goal is to maximize the expected discounted utility of execution by choosing optimal actions for placing, canceling, or executing orders. The problem is well-defined, scientifically sound, and computationally feasible. It falls within the domain of computational economics and finance, specifically agent-based modeling and reinforcement learning in financial markets. All parameters, dynamics, and evaluation metrics are specified with sufficient mathematical rigor to permit a unique, reproducible solution. Therefore, the problem is deemed valid.\n\nThe solution will be developed by implementing the Q-learning algorithm to find an optimal policy for the agent. The implementation will proceed in the following steps:\n\n1.  **State and Action Space Definition**: The state of the environment is a tuple $s_t = (h_t, q_t, p_t)$, where $h_t \\in \\{0, 1\\}$ indicates if the agent has an active limit order, $q_t \\in \\{0, 1, 2, 3\\}$ is the number of other orders at the best bid, and $p_t \\in \\{0, 1, 2, 3\\}$ is the agent's queue position. By definition, $p_t=0$ if $h_t=0$, and $p_t \\le q_t$ if $h_t=1$. This finite state space consists of $14$ valid, non-terminal states. A mapping will be created to assign a unique integer index to each state tuple for efficient implementation of the Q-table. The action space is $\\mathcal{A} = \\{0, 1, 2, 3\\}$, representing waiting, placing a limit order, placing a market order, and canceling an order, respectively.\n\n2.  **Environment Dynamics Simulation**: A function will be implemented to simulate one step of the MDP. This function, given a state $s_t$ and an action $a_t$, will calculate the immediate reward $r_t$ and the next state $s_{t+1}$. The logic strictly adheres to the problem specification:\n    *   **Action Handling**: Invalid actions (e.g., placing an order when one exists) are treated as waiting. A market order (action $2$) results in immediate episode termination with reward $r_t = -s/2$.\n    *   **State Transition**: For non-terminal actions, the state is first updated based on the agent's action (placing or canceling an order). Then, an exogenous event is sampled—sell market order ($p_{\\mathrm{ms}}$), limit buy order ($p_{\\mathrm{lb}}$), or cancellation ($p_{\\mathrm{cb}}$).\n    *   **Execution and Reward**: If a sell market order arrives when the agent is at the front of the queue ($h_t=1, p_t=0$), the episode terminates with reward $r_t = +s/2$. Any non-terminal step incurs a waiting cost, yielding a reward of $r_t = -w$.\n    *   **State Update**: If the episode does not terminate, the state variables $(h_t, q_t, p_t)$ are updated based on the exogenous event, respecting the specified caps and constraints.\n\n3.  **Q-Learning Implementation**: The agent's learning process is modeled using Q-learning. A Q-table, $Q(s, a)$, is initialized to zeros and stores the estimated value of taking action $a$ in state $s$.\n    *   **Training Loop**: The agent interacts with the environment for a specified number of episodes, $E$. Each episode runs for a maximum of $T_{\\max}$ steps.\n    *   **Action Selection**: During training, actions are selected using an $\\varepsilon$-greedy policy. With probability $\\varepsilon$, a random action is chosen to ensure exploration; otherwise, the action with the highest Q-value, $a = \\arg\\max_{a'} Q(s_t, a')$, is chosen (exploitation). Ties are broken by selecting the smallest action index.\n    *   **Q-Value Update**: After each transition $(s_t, a_t, r_t, s_{t+1})$, the Q-table is updated using the Bellman equation:\n        $$Q(s_t, a_t) \\leftarrow (1-\\alpha) Q(s_t, a_t) + \\alpha \\left[r_t + \\gamma \\max_{a' \\in \\mathcal{A}} Q(s_{t+1}, a')\\right]$$\n        Here, $\\alpha$ is the learning rate and $\\gamma$ is the discount factor. If $s_{t+1}$ is a terminal state, the term $\\gamma \\max_{a'} Q(s_{t+1}, a')$ is zero.\n\n4.  **Policy Evaluation**: After training is complete, the learned knowledge is evaluated.\n    *   **Greedy Policy**: A deterministic greedy policy, $\\pi(s)$, is extracted from the final Q-table: $\\pi(s) = \\arg\\max_{a' \\in \\mathcal{A}} Q(s, a')$, using the same tie-breaking rule. The agent's learned action is then determined for the three specified states: $s^{(1)} = (0, q_0, 0)$, $s^{(2)} = (1, 1, 0)$, and $s^{(3)} = (1, 3, 3)$.\n    *   **Performance Estimation**: To estimate the performance of the learned policy, $N_{\\mathrm{eval}}$ new episodes are simulated. For these evaluation episodes, the agent strictly follows the greedy policy ($\\varepsilon=0$), and the random number generator is re-seeded as specified. The total discounted return, $G = \\sum_{t=0}^{\\tau} \\gamma^t r_t$, is calculated for each episode. The final metric is the sample average of these returns, rounded to four decimal places.\n\nThis entire procedure is encapsulated in a Python script. All randomness is controlled via a seeded pseudorandom number generator to ensure reproducibility. The script will be executed for each of the three test cases, and the results will be formatted into the precise output string specified in the problem statement.", "answer": "```python\nimport numpy as np\n\n# State-space mapping global variables\nSTATE_TO_IDX = {}\nIDX_TO_STATE = []\n\ndef build_state_space():\n    \"\"\"Builds the mapping between state tuples and integer indices.\"\"\"\n    if STATE_TO_IDX:\n        return\n    idx = 0\n    # States where the agent has no order (h=0)\n    for q in range(4):\n        state = (0, q, 0)\n        STATE_TO_IDX[state] = idx\n        IDX_TO_STATE.append(state)\n        idx += 1\n    # States where the agent has an order (h=1)\n    for q in range(4):\n        for p in range(q + 1):\n            state = (1, q, p)\n            STATE_TO_IDX[state] = idx\n            IDX_TO_STATE.append(state)\n            idx += 1\n\ndef step(state, action, rng, params):\n    \"\"\"\n    Simulates one step of the environment dynamics.\n    Returns (next_state, reward, done).\n    \"\"\"\n    s_val, w, gamma, _, _, T_max, _, _, q0, p_ms, p_lb, p_cb, _ = params\n    half_spread = s_val / 2.0\n    h, q, p = state\n\n    # Action validity checks\n    if h == 1 and action == 1:\n        action = 0  # Wait\n    if h == 0 and action == 3:\n        action = 0  # Wait\n\n    # Action 2: Market order\n    if action == 2:\n        return None, -half_spread, True\n\n    # Pre-event state update from agent's action\n    # For actions 0 (wait), 1 (limit), 3 (cancel)\n    \n    # Action 1: Place limit order\n    if action == 1:\n        if h == 0:\n            h = 1\n            p = q # Agent joins at the back of the queue of size q\n    # Action 3: Cancel limit order\n    elif action == 3:\n        if h == 1:\n            h = 0\n            p = 0\n\n    # Exogenous event simulation\n    # Check for immediate execution of limit order\n    event = rng.choice(['ms', 'lb', 'cb'], p=[p_ms, p_lb, p_cb])\n    \n    if event == 'ms':\n        if h == 1 and p == 0:\n            return None, half_spread, True  # Limit order execution\n        else:\n            if q > 0:\n                q -= 1\n                if h == 1 and p > 0:\n                    p -= 1\n    elif event == 'cb':\n        if q > 0:\n            q -= 1\n            if h == 1 and p > 0:\n                p -= 1\n    elif event == 'lb':\n        q = min(q + 1, 3)\n\n    return (h, q, p), -w, False\n\ndef run_case(params):\n    \"\"\"Runs the full simulation for one set of parameters.\"\"\"\n    s_val, w, gamma, alpha, epsilon, T_max, E, N_eval, q_0, p_ms, p_lb, p_cb, seed = params\n    \n    # Initialize Q-table and RNG for training\n    q_table = np.zeros((len(IDX_TO_STATE), 4))\n    rng = np.random.default_rng(seed)\n\n    # Q-learning training\n    for _ in range(E):\n        state = (0, q_0, 0)\n        \n        for t in range(T_max):\n            state_idx = STATE_TO_IDX[state]\n\n            # Epsilon-greedy action selection\n            if rng.random() < epsilon:\n                action = rng.integers(0, 4)\n            else:\n                action = np.argmax(q_table[state_idx, :])\n\n            next_state, reward, done = step(state, action, rng, params)\n\n            # Q-table update\n            if done:\n                target = reward\n                q_table[state_idx, action] = (1 - alpha) * q_table[state_idx, action] + alpha * target\n                break\n            else:\n                next_state_idx = STATE_TO_IDX[next_state]\n                max_next_q = np.max(q_table[next_state_idx, :])\n                target = reward + gamma * max_next_q\n                q_table[state_idx, action] = (1 - alpha) * q_table[state_idx, action] + alpha * target\n                state = next_state\n\n    # Evaluation phase\n    # 1. Get greedy actions for specified states\n    s1 = (0, q_0, 0)\n    s2 = (1, 1, 0)\n    s3 = (1, 3, 3)\n    \n    # Check if a state is valid before trying to get its index.\n    # (1,1,0) can be invalid if q_0 is 0. But for all test cases, q_0 >= 2\n    # (1,3,3) is always valid in the defined state space.\n    action_s1 = np.argmax(q_table[STATE_TO_IDX[s1], :])\n    action_s2 = np.argmax(q_table[STATE_TO_IDX[s2], :])\n    action_s3 = np.argmax(q_table[STATE_TO_IDX[s3], :])\n\n    # 2. Estimate average discounted return\n    eval_rng = np.random.default_rng(seed + 1)\n    episode_returns = []\n    \n    for _ in range(N_eval):\n        state = (0, q_0, 0)\n        discounted_return = 0.0\n        \n        for t in range(T_max):\n            state_idx = STATE_TO_IDX[state]\n            action = np.argmax(q_table[state_idx, :]) # Greedy policy\n            \n            next_state, reward, done = step(state, action, eval_rng, params)\n            \n            discounted_return += (gamma ** t) * reward\n            \n            if done:\n                break\n            \n            state = next_state\n        \n        episode_returns.append(discounted_return)\n\n    avg_return = np.mean(episode_returns)\n\n    return [action_s1, action_s2, action_s3, avg_return]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Build the state space mappings\n    build_state_space()\n\n    # Test case parameters:\n    # (s, w, gamma, alpha, epsilon, T_max, E, N_eval, q_0, p_ms, p_lb, p_cb, seed)\n    test_cases = [\n        (2, 0.02, 0.95, 0.20, 0.10, 50, 4000, 2000, 2, 0.50, 0.30, 0.20, 7),\n        (2, 0.02, 0.95, 0.20, 0.10, 50, 3000, 2000, 2, 0.80, 0.10, 0.10, 11),\n        (2, 0.20, 0.95, 0.20, 0.10, 50, 4000, 2000, 3, 0.20, 0.50, 0.30, 13),\n    ]\n\n    all_results = []\n    for params in test_cases:\n        result = run_case(params)\n        all_results.append(result)\n\n    # Format the final output string\n    # e.g., [[1,0,3,-0.1234],[...],[...]]\n    outer_parts = []\n    for res in all_results:\n        a1, a2, a3, G_avg = res\n        # Format the average return to 4 decimal places\n        inner_str = f\"[{a1},{a2},{a3},{G_avg:.4f}]\"\n        outer_parts.append(inner_str)\n    \n    final_output = f\"[{','.join(outer_parts)}]\"\n    print(final_output)\n\nsolve()\n```"}]}