{"hands_on_practices": [{"introduction": "This first practice provides an intuitive entry point into instrumental variables estimation. We explore a scenario where a binary instrument—in this case, the outcome of a razor-thin election—is used to identify a causal effect. You will see that in this special case, the general Two-Stage Least Squares ($2SLS$) formula simplifies to the Wald estimator, an elegant ratio that clearly illustrates how IV works by comparing the instrument's effect on the outcome, $y_i$, to its effect on the endogenous treatment, $D_i$ [@problem_id:2445017]. This exercise is invaluable for building a foundational understanding of the logic behind instrumental variables.", "id": "2445017", "problem": "You are given three independent datasets, each representing a cross-section of jurisdictions that experienced very close elections. For jurisdiction $i$, let $y_i$ denote the change in an outcome of interest, $D_i$ denote the change in a fiscal policy variable, and $m_i$ denote the incumbent’s vote margin (defined as the incumbent’s vote share minus the challenger’s vote share). Define the binary indicator $Z_i$ by $Z_i = 1$ if $m_i &lt; 0$ and $Z_i = 0$ otherwise. Assume that, within a razor-thin bandwidth around zero, the sign of $m_i$ is quasi-random, so that $Z_i$ is valid as an instrument for $D_i$. Consider the linear structural relationship\n$$\ny_i = \\beta D_i + \\gamma \\cdot 1 + u_i,\n$$\nwhere $1$ is the intercept regressor, $\\beta$ and $\\gamma$ are unknown coefficients, and $u_i$ is an unobserved disturbance. Assume the orthogonality conditions\n$$\n\\mathbb{E}[u_i] = 0 \\quad \\text{and} \\quad \\mathbb{E}[Z_i u_i] = 0,\n$$\nso that the instrument set is the span of $\\{1, Z_i\\}$, and $D_i$ may be endogenous. For each dataset, only observations with $\\lvert m_i \\rvert \\le \\tau$ are included, where $\\tau$ is a given nonnegative threshold.\n\nFor each dataset below, compute the unique value of $\\beta$ that satisfies the sample orthogonality conditions implied by the model above with instruments $\\{1, Z_i\\}$. Your program must compute this value for each dataset using only the provided arrays. No external inputs are allowed. The required final outputs are real numbers.\n\nTest Suite (each dataset provides $(\\tau, (m_i)_i, (D_i)_i, (y_i)_i)$):\n\n- Dataset $1$:\n  - $\\tau = 0.020$,\n  - $m = (-0.010,\\,-0.015,\\,-0.008,\\,0.012,\\,0.005,\\,0.017)$,\n  - $D = (5.0,\\,5.0,\\,5.0,\\,1.0,\\,2.0,\\,3.0)$,\n  - $y = (10.0,\\,10.0,\\,10.0,\\,2.0,\\,4.0,\\,6.0)$.\n\n- Dataset $2$:\n  - $\\tau = 0.010$,\n  - $m = (-0.004,\\,-0.003,\\,-0.001,\\,0.006,\\,0.009,\\,0.002)$,\n  - $D = (2.6,\\,2.6,\\,2.6,\\,2.5,\\,2.5,\\,2.5)$,\n  - $y = (5.0,\\,5.0,\\,5.0,\\,4.8,\\,4.8,\\,4.8)$.\n\n- Dataset $3$:\n  - $\\tau = 0.015$,\n  - $m = (-0.010,\\,0.005,\\,0.007,\\,0.008,\\,0.012)$,\n  - $D = (4.0,\\,3.0,\\,3.0,\\,3.0,\\,3.0)$,\n  - $y = (8.5,\\,6.0,\\,6.0,\\,6.0,\\,6.0)$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number rounded to six decimal places. For example, if the three computed values are $a$, $b$, and $c$, print exactly the line\n$[a,b,c]$\nwith each of $a$, $b$, and $c$ rounded to six decimal places. No other text must be printed.", "solution": "The problem presented requires the computation of a structural parameter, $\\beta$, in a linear model with a potentially endogenous regressor. The estimation is to be conducted using the method of instrumental variables (IV), which is a specific application of Two-Stage Least Squares (2SLS) for an exactly identified system.\n\nFirst, let us formalize the problem. We are given the structural equation for an outcome variable $y_i$ as a function of a policy variable $D_i$:\n$$\ny_i = \\beta D_i + \\gamma \\cdot 1 + u_i\n$$\nHere, $\\beta$ and $\\gamma$ are the parameters of interest, $1$ represents the regressor for the intercept, and $u_i$ is an unobserved disturbance term. The variable $D_i$ may be endogenous, meaning it could be correlated with $u_i$.\n\nTo address this endogeneity, we are provided with an instrumental variable, $Z_i$. The instrument is a binary indicator derived from the incumbent's vote margin, $m_i$, and is defined as $Z_i = 1$ if $m_i < 0$ and $Z_i = 0$ otherwise. The problem provides the fundamental orthogonality conditions that allow for consistent estimation:\n$$\n\\mathbb{E}[u_i] = 0 \\quad \\text{and} \\quad \\mathbb{E}[Z_i u_i] = 0\n$$\nThese population moment conditions state that the disturbance $u_i$ is uncorrelated with the constant term and with the instrument $Z_i$. The set of instruments is therefore the span of $\\{1, Z_i\\}$.\n\nThe task is to find the value of $\\beta$ that satisfies the sample analogues of these orthogonality conditions. These sample moment conditions are:\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{\\beta} D_i - \\hat{\\gamma}) = 0 \\\\\n\\frac{1}{N} \\sum_{i=1}^{N} Z_i (y_i - \\hat{\\beta} D_i - \\hat{\\gamma}) = 0\n$$\nwhere $N$ is the number of observations, and $\\hat{\\beta}$ and $\\hat{\\gamma}$ are the estimators for $\\beta$ and $\\gamma$.\n\nThis is a system of two linear equations in two unknowns, $\\hat{\\beta}$ and $\\hat{\\gamma}$. We can solve this system to derive an explicit formula for $\\hat{\\beta}$.\nFrom the first equation, we can express $\\hat{\\gamma}$ in terms of $\\hat{\\beta}$:\n$$\n\\hat{\\gamma} = \\bar{y} - \\hat{\\beta} \\bar{D}\n$$\nwhere $\\bar{y} = \\frac{1}{N}\\sum y_i$ and $\\bar{D} = \\frac{1}{N}\\sum D_i$ are the sample means.\nSubstituting this expression for $\\hat{\\gamma}$ into the second equation gives:\n$$\n\\sum_{i=1}^{N} Z_i (y_i - \\hat{\\beta} D_i - (\\bar{y} - \\hat{\\beta} \\bar{D})) = 0 \\\\\n\\sum_{i=1}^{N} Z_i (y_i - \\bar{y}) - \\hat{\\beta} \\sum_{i=1}^{N} Z_i (D_i - \\bar{D}) = 0\n$$\nSolving for $\\hat{\\beta}$ yields the general IV estimator formula:\n$$\n\\hat{\\beta}_{IV} = \\frac{\\sum_{i=1}^{N} Z_i (y_i - \\bar{y})}{\\sum_{i=1}^{N} Z_i (D_i - \\bar{D})} = \\frac{\\widehat{\\text{Cov}}(Z, y)}{\\widehat{\\text{Cov}}(Z, D)}\n$$\nwhere $\\widehat{\\text{Cov}}(X, V)$ denotes the sample covariance between variables $X$ and $V$. A unique solution exists if and only if the denominator is non-zero, $\\widehat{\\text{Cov}}(Z, D) \\neq 0$, which is the instrument relevance condition.\n\nGiven that the instrument $Z_i$ is binary, this estimator simplifies to the Wald estimator. Let $N_1$ be the number of observations where $Z_i=1$ and $N_0$ be the number where $Z_i=0$. The Wald estimator is the ratio of the difference in the mean outcome to the difference in the mean treatment, between the two groups defined by the instrument:\n$$\n\\hat{\\beta}_{Wald} = \\frac{\\mathbb{E}[y | Z=1] - \\mathbb{E}[y | Z=0]}{\\mathbb{E}[D | Z=1] - \\mathbb{E}[D | Z=0]}\n$$\nThe sample analogue is:\n$$\n\\hat{\\beta} = \\frac{\\bar{y}_{Z=1} - \\bar{y}_{Z=0}}{\\bar{D}_{Z=1} - \\bar{D}_{Z=0}}\n$$\nwhere $\\bar{y}_{Z=j}$ and $\\bar{D}_{Z=j}$ are the sample means of $y_i$ and $D_i$ for the subgroup of observations where $Z_i=j$. This formula is computationally more direct and will be used for the calculations.\n\nThe problem specifies that for each dataset, only observations with $|m_i| \\le \\tau$ are to be used. For all three provided datasets, every data point satisfies this condition, so no observations are discarded.\n\n**Dataset 1:**\n- $\\tau = 0.020$\n- $m = (-0.010, -0.015, -0.008, 0.012, 0.005, 0.017)$\n- $D = (5.0, 5.0, 5.0, 1.0, 2.0, 3.0)$\n- $y = (10.0, 10.0, 10.0, 2.0, 4.0, 6.0)$\n\nThe instrument vector is $Z = (1, 1, 1, 0, 0, 0)$.\nFor the group with $Z_i=1$ (the first $3$ observations):\n- $\\bar{y}_{Z=1} = \\frac{10.0 + 10.0 + 10.0}{3} = 10.0$\n- $\\bar{D}_{Z=1} = \\frac{5.0 + 5.0 + 5.0}{3} = 5.0$\nFor the group with $Z_i=0$ (the last $3$ observations):\n- $\\bar{y}_{Z=0} = \\frac{2.0 + 4.0 + 6.0}{3} = 4.0$\n- $\\bar{D}_{Z=0} = \\frac{1.0 + 2.0 + 3.0}{3} = 2.0$\nThe estimate for $\\beta$ is:\n$$\n\\hat{\\beta}_1 = \\frac{10.0 - 4.0}{5.0 - 2.0} = \\frac{6.0}{3.0} = 2.0\n$$\n\n**Dataset 2:**\n- $\\tau = 0.010$\n- $m = (-0.004, -0.003, -0.001, 0.006, 0.009, 0.002)$\n- $D = (2.6, 2.6, 2.6, 2.5, 2.5, 2.5)$\n- $y = (5.0, 5.0, 5.0, 4.8, 4.8, 4.8)$\n\nThe instrument vector is $Z = (1, 1, 1, 0, 0, 0)$.\nFor the group with $Z_i=1$:\n- $\\bar{y}_{Z=1} = \\frac{5.0 + 5.0 + 5.0}{3} = 5.0$\n- $\\bar{D}_{Z=1} = \\frac{2.6 + 2.6 + 2.6}{3} = 2.6$\nFor the group with $Z_i=0$:\n- $\\bar{y}_{Z=0} = \\frac{4.8 + 4.8 + 4.8}{3} = 4.8$\n- $\\bar{D}_{Z=0} = \\frac{2.5 + 2.5 + 2.5}{3} = 2.5$\nThe estimate for $\\beta$ is:\n$$\n\\hat{\\beta}_2 = \\frac{5.0 - 4.8}{2.6 - 2.5} = \\frac{0.2}{0.1} = 2.0\n$$\n\n**Dataset 3:**\n- $\\tau = 0.015$\n- $m = (-0.010, 0.005, 0.007, 0.008, 0.012)$\n- $D = (4.0, 3.0, 3.0, 3.0, 3.0)$\n- $y = (8.5, 6.0, 6.0, 6.0, 6.0)$\n\nThe instrument vector is $Z = (1, 0, 0, 0, 0)$.\nFor the group with $Z_i=1$ (the first observation):\n- $\\bar{y}_{Z=1} = 8.5$\n- $\\bar{D}_{Z=1} = 4.0$\nFor the group with $Z_i=0$ (the last $4$ observations):\n- $\\bar{y}_{Z=0} = \\frac{6.0 + 6.0 + 6.0 + 6.0}{4} = 6.0$\n- $\\bar{D}_{Z=0} = \\frac{3.0 + 3.0 + 3.0 + 3.0}{4} = 3.0$\nThe estimate for $\\beta$ is:\n$$\n\\hat{\\beta}_3 = \\frac{8.5 - 6.0}{4.0 - 3.0} = \\frac{2.5}{1.0} = 2.5\n$$\nThe computed values for $\\beta$ for the three datasets are $2.0$, $2.0$, and $2.5$, respectively.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the instrumental variables (IV) estimate of beta for three datasets.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"tau\": 0.020,\n            \"m\": np.array([-0.010, -0.015, -0.008, 0.012, 0.005, 0.017]),\n            \"D\": np.array([5.0, 5.0, 5.0, 1.0, 2.0, 3.0]),\n            \"y\": np.array([10.0, 10.0, 10.0, 2.0, 4.0, 6.0]),\n        },\n        {\n            \"tau\": 0.010,\n            \"m\": np.array([-0.004, -0.003, -0.001, 0.006, 0.009, 0.002]),\n            \"D\": np.array([2.6, 2.6, 2.6, 2.5, 2.5, 2.5]),\n            \"y\": np.array([5.0, 5.0, 5.0, 4.8, 4.8, 4.8]),\n        },\n        {\n            \"tau\": 0.015,\n            \"m\": np.array([-0.010, 0.005, 0.007, 0.008, 0.012]),\n            \"D\": np.array([4.0, 3.0, 3.0, 3.0, 3.0]),\n            \"y\": np.array([8.5, 6.0, 6.0, 6.0, 6.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        tau, m, D, y = case[\"tau\"], case[\"m\"], case[\"D\"], case[\"y\"]\n\n        # Filter the data based on the threshold tau.\n        # For the given problem, all data points satisfy the condition,\n        # but this step ensures correctness for a general case.\n        inclusion_mask = np.abs(m) <= tau\n        m_filtered = m[inclusion_mask]\n        D_filtered = D[inclusion_mask]\n        y_filtered = y[inclusion_mask]\n\n        # Construct the binary instrument Z, where Z=1 if m < 0.\n        Z = (m_filtered < 0).astype(int)\n\n        # Identify the two groups based on the instrument value.\n        Z_is_1_mask = (Z == 1)\n        Z_is_0_mask = (Z == 0)\n\n        # Separate the y and D vectors into two groups.\n        y_group_1 = y_filtered[Z_is_1_mask]\n        y_group_0 = y_filtered[Z_is_0_mask]\n        D_group_1 = D_filtered[Z_is_1_mask]\n        D_group_0 = D_filtered[Z_is_0_mask]\n\n        # The problem statement guarantees a unique solution exists,\n        # which implies both groups are non-empty and the denominator of the\n        # Wald estimator is non-zero.\n        \n        # Calculate the mean of y and D for each group.\n        y_mean_1 = np.mean(y_group_1)\n        y_mean_0 = np.mean(y_group_0)\n        D_mean_1 = np.mean(D_group_1)\n        D_mean_0 = np.mean(D_group_0)\n\n        # Calculate beta using the Wald estimator formula.\n        beta = (y_mean_1 - y_mean_0) / (D_mean_1 - D_mean_0)\n\n        results.append(beta)\n\n    # Format the final output as a comma-separated list of values\n    # rounded to six decimal places, enclosed in square brackets.\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"}, {"introduction": "While the Wald estimator provides a clear intuition, most econometric problems require a more general approach to handle multiple instruments and control variables. This exercise guides you through the implementation of the full matrix-based $2SLS$ estimator [@problem_id:2445014]. You will work with a classic quasi-experimental setup, using ambulance dispatch preferences as an instrument for hospital choice, and apply the $2SLS$ formula to hypothetical data representing just-identified, overidentified, and weak-instrument scenarios.", "id": "2445014", "problem": "Consider an econometric model of patient outcomes where an endogenous measure of hospital quality is instrumented using ambulance dispatch protocols that favor the nearest hospital. Let the structural equation be\n$$\ny_i \\;=\\; \\alpha \\;+\\; \\beta\\,x_i \\;+\\; \\gamma\\,w_i \\;+\\; u_i,\n$$\nwhere $y_i$ is a scalar outcome for patient $i$ (mortality indicator), $x_i$ is a scalar endogenous regressor measuring hospital quality, $w_i$ is a scalar exogenous control (patient covariate), and $u_i$ is an unobserved error. A constant term is included in the model via the intercept parameter $\\alpha$. The orthogonality conditions are defined by instruments $z_i$ that satisfy\n$$\n\\mathbb{E}\\!\\left[z_i\\,u_i\\right] \\;=\\; 0,\n$$\nwith $z_i$ including all exogenous regressors and excluded instruments derived from ambulance dispatch rules.\n\nDefine the regressor matrix $X \\in \\mathbb{R}^{n \\times k}$ with columns $[\\,\\mathbf{1},\\,w,\\,x\\,]$, and the instrument matrix $Z \\in \\mathbb{R}^{n \\times L}$ that stacks $[\\,\\mathbf{1},\\,w,\\,\\text{excluded instruments}\\,]$. For each test case below, compute the Two-Stage Least Squares (TSLS) estimate of the coefficient $\\beta$ (the coefficient on $x$) implied by the orthogonality conditions, that is, the value obtained from the linear instrumental variables estimator defined via the projection of $X$ and $y$ onto the column space of $Z$. You must treat the intercept and the exogenous control $w$ as included in both $X$ and $Z$.\n\nYour program must implement the computation of the TSLS coefficient on $x$ for each test case and report only that coefficient for each case.\n\nTest suite:\n\n- Case A (just-identified, relevant instrument): $n = 8$. Use\n  - $w = [\\,0.20,\\,-0.10,\\,0.00,\\,0.50,\\,-0.30,\\,0.10,\\,-0.40,\\,0.30\\,]$,\n  - $x = [\\,2.00,\\,3.70,\\,2.20,\\,3.90,\\,1.80,\\,3.40,\\,2.10,\\,3.60\\,]$,\n  - excluded instrument $z^{(1)} = [\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,]$,\n  - $y = [\\,0.156,\\,0.020,\\,0.098,\\,0.076,\\,0.108,\\,0.054,\\,0.102,\\,0.038\\,]$.\n  Here, $Z = [\\,\\mathbf{1},\\,w,\\,z^{(1)}\\,]$ and $X = [\\,\\mathbf{1},\\,w,\\,x\\,]$.\n\n- Case B (weak but valid instrument): $n = 8$. Use\n  - $w = [\\,0.20,\\,-0.10,\\,0.00,\\,0.50,\\,-0.30,\\,0.10,\\,-0.40,\\,0.30\\,]$,\n  - $x = [\\,2.90,\\,3.10,\\,3.00,\\,3.20,\\,2.90,\\,3.10,\\,2.95,\\,3.05\\,]$,\n  - excluded instrument $z^{(1)} = [\\,0,\\,1,\\,0,\\,1,\\,1,\\,0,\\,0,\\,1\\,]$,\n  - $y = [\\,0.097,\\,0.051,\\,0.070,\\,0.113,\\,0.042,\\,0.072,\\,0.046,\\,0.081\\,]$.\n  Here, $Z = [\\,\\mathbf{1},\\,w,\\,z^{(1)}\\,]$ and $X = [\\,\\mathbf{1},\\,w,\\,x\\,]$.\n\n- Case C (overidentified, two excluded instruments): $n = 10$. Use\n  - $w = [\\,0.00,\\,0.20,\\,-0.20,\\,0.50,\\,-0.10,\\,0.30,\\,-0.40,\\,0.10,\\,-0.30,\\,0.40\\,]$,\n  - $x = [\\,3.80,\\,3.50,\\,2.20,\\,4.00,\\,2.40,\\,3.70,\\,1.90,\\,3.40,\\,2.10,\\,3.90\\,]$,\n  - excluded instruments:\n    - $z^{(1)} = [\\,1,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1,\\,0,\\,1\\,]$,\n    - $z^{(2)} = [\\,0.80,\\,0.60,\\,-0.70,\\,0.90,\\,-0.50,\\,0.70,\\,-0.90,\\,0.50,\\,-0.60,\\,0.80\\,]$,\n  - $y = [\\,0.022,\\,0.066,\\,0.082,\\,0.070,\\,0.098,\\,0.062,\\,0.094,\\,0.054,\\,0.100,\\,0.028\\,]$.\n  Here, $Z = [\\,\\mathbf{1},\\,w,\\,z^{(1)},\\,z^{(2)}\\,]$ and $X = [\\,\\mathbf{1},\\,w,\\,x\\,]$.\n\nComputation target for each case:\n\n- Compute the TSLS estimate of $\\beta$ using the definition based on projecting $X$ and $y$ onto the column space of $Z$.\n- Return only the scalar estimate of $\\beta$.\n\nFinal output specification:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one floating-point number per test case in the order Case A, Case B, Case C.\n- Each number must be rounded to six decimal places.\n- For example, an output with three results must look like $[b_A,b_B,b_C]$ where each $b_{\\cdot}$ is a float rounded to six decimal places.", "solution": "The provided problem statement has been subjected to validation and is determined to be valid. It is scientifically grounded in established econometric theory, well-posed with sufficient data for computation, and objectively formulated. No flaws that would prevent a rigorous solution were identified.\n\nThe problem requires the computation of the Two-Stage Least Squares ($TSLS$) estimate for the coefficient $\\beta$ in the linear structural model:\n$$\ny_i \\;=\\; \\alpha \\;+\\; \\gamma\\,w_i \\;+\\; \\beta\\,x_i \\;+\\; u_i\n$$\nHere, $y_i$ is the outcome, $x_i$ is an endogenous regressor, $w_i$ is an exogenous control variable, and $u_i$ is the error term for observation $i$. In matrix form, the model is written as $y = X\\mathbf{b} + u$, where $y \\in \\mathbb{R}^{n}$ is the vector of outcomes, $X \\in \\mathbb{R}^{n \\times k}$ is the matrix of regressors, $\\mathbf{b} \\in \\mathbb{R}^{k}$ is the vector of coefficients, and $u \\in \\mathbb{R}^{n}$ is the vector of unobserved errors.\n\nBased on the problem specification, the regressor matrix is $X = [\\begin{matrix} \\mathbf{1} & w & x \\end{matrix}]$, where $\\mathbf{1}$ is a vector of ones for the intercept $\\alpha$, $w$ is the vector for the exogenous covariate with coefficient $\\gamma$, and $x$ is the vector for the endogenous regressor with coefficient $\\beta$. Thus, the number of regressors is $k=3$, and the coefficient vector to be estimated is $\\mathbf{b} = [\\alpha, \\gamma, \\beta]^T$.\n\nThe endogeneity of $x$ means its values are correlated with the error term $u$, specifically $\\mathbb{E}[x_i u_i] \\neq 0$. This correlation violates a key assumption of Ordinary Least Squares ($OLS$), rendering the $OLS$ estimator of $\\mathbf{b}$ biased and inconsistent. The $TSLS$ method overcomes this by using a set of instrumental variables, collected in a matrix $Z \\in \\mathbb{R}^{n \\times L}$, which are correlated with the endogenous regressor $x$ but are uncorrelated with the error term $u$. This is captured by the orthogonality condition $\\mathbb{E}[Z^T u] = 0$. The matrix $Z$ is composed of all exogenous variables already in the model (the intercept and $w$, known as included instruments) and one or more additional variables known as excluded instruments.\n\nThe $TSLS$ procedure, as its name suggests, can be conceptualized in two stages.\n\nFirst Stage: The influence of the error term is purged from the endogenous regressor. This is accomplished by projecting the regressor matrix $X$ onto the linear space spanned by the columns of the instrument matrix $Z$. The orthogonal projection matrix onto the column space of $Z$ is given by:\n$$\nP_Z = Z(Z^T Z)^{-1}Z^T\n$$\nThe predicted values of $X$ from this projection, denoted $\\hat{X}$, are calculated as:\n$$\n\\hat{X} = P_Z X\n$$\nThese fitted values, $\\hat{X}$, are linear combinations of the instruments in $Z$ and are therefore asymptotically uncorrelated with the error term $u$.\n\nSecond Stage: An $OLS$ regression is performed using the outcome variable $y$ and the projected regressors $\\hat{X}$. The coefficient vector from this regression is the $TSLS$ estimator, $\\hat{\\mathbf{b}}_{\\text{TSLS}}$:\n$$\n\\hat{\\mathbf{b}}_{\\text{TSLS}} = (\\hat{X}^T \\hat{X})^{-1} \\hat{X}^T y\n$$\nFor a direct computational formula, we substitute $\\hat{X} = P_Z X$. Using the properties that a projection matrix is symmetric ($P_Z^T = P_Z$) and idempotent ($P_Z P_Z = P_Z$), we simplify the components of the estimator:\n$$\n\\hat{X}^T \\hat{X} = (P_Z X)^T (P_Z X) = X^T P_Z^T P_Z X = X^T P_Z X\n$$\nand\n$$\n\\hat{X}^T y = (P_Z X)^T y = X^T P_Z^T y = X^T P_Z y\n$$\nThis leads to the general expression for the $TSLS$ estimator:\n$$\n\\hat{\\mathbf{b}}_{\\text{TSLS}} = (X^T P_Z X)^{-1} (X^T P_Z y)\n$$\nBy substituting the definition of $P_Z$, we arrive at the full formula used for computation:\n$$\n\\hat{\\mathbf{b}}_{\\text{TSLS}} = \\left( X^T Z(Z^T Z)^{-1}Z^T X \\right)^{-1} \\left( X^T Z(Z^T Z)^{-1}Z^T y \\right)\n$$\nThis single formula is valid for just-identified cases (where the number of instruments $L$ equals the number of regressors $k$) and overidentified cases ($L > k$).\n\nFor each test case provided, we construct the vectors $y$ and the matrices $X$ and $Z$ from the data. We then apply the derived formula to compute the coefficient vector $\\hat{\\mathbf{b}}_{\\text{TSLS}} = [\\hat{\\alpha}, \\hat{\\gamma}, \\hat{\\beta}]^T$. The required result is the scalar estimate $\\hat{\\beta}$, which is the third element of this vector.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the TSLS estimate of the coefficient beta for each test case.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n\": 8,\n            \"w\": np.array([0.20, -0.10, 0.00, 0.50, -0.30, 0.10, -0.40, 0.30]),\n            \"x\": np.array([2.00, 3.70, 2.20, 3.90, 1.80, 3.40, 2.10, 3.60]),\n            \"excluded_instruments\": [\n                np.array([0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0])\n            ],\n            \"y\": np.array([0.156, 0.020, 0.098, 0.076, 0.108, 0.054, 0.102, 0.038]),\n        },\n        {\n            \"name\": \"Case B\",\n            \"n\": 8,\n            \"w\": np.array([0.20, -0.10, 0.00, 0.50, -0.30, 0.10, -0.40, 0.30]),\n            \"x\": np.array([2.90, 3.10, 3.00, 3.20, 2.90, 3.10, 2.95, 3.05]),\n            \"excluded_instruments\": [\n                np.array([0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0])\n            ],\n            \"y\": np.array([0.097, 0.051, 0.070, 0.113, 0.042, 0.072, 0.046, 0.081]),\n        },\n        {\n            \"name\": \"Case C\",\n            \"n\": 10,\n            \"w\": np.array([0.00, 0.20, -0.20, 0.50, -0.10, 0.30, -0.40, 0.10, -0.30, 0.40]),\n            \"x\": np.array([3.80, 3.50, 2.20, 4.00, 2.40, 3.70, 1.90, 3.40, 2.10, 3.90]),\n            \"excluded_instruments\": [\n                np.array([1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]),\n                np.array([0.80, 0.60, -0.70, 0.90, -0.50, 0.70, -0.90, 0.50, -0.60, 0.80]),\n            ],\n            \"y\": np.array([0.022, 0.066, 0.082, 0.070, 0.098, 0.062, 0.094, 0.054, 0.100, 0.028]),\n        },\n    ]\n\n    def compute_tsls_beta(y, w, x, excluded_instruments, n):\n        \"\"\"\n        Computes the TSLS estimate for the coefficient on x.\n\n        The formula is beta_hat_tsls = (X'Z(Z'Z)^-1 Z'X)^-1 X'Z(Z'Z)^-1 Z'y\n        \"\"\"\n        # Reshape vectors to be column vectors for matrix operations\n        y_col = y.reshape(-1, 1)\n        w_col = w.reshape(-1, 1)\n        x_col = x.reshape(-1, 1)\n        \n        # Construct the regressor matrix X = [1, w, x]\n        ones = np.ones((n, 1))\n        X = np.hstack((ones, w_col, x_col))\n\n        # Construct the instrument matrix Z = [1, w, excluded_instruments...]\n        z_parts = [ones, w_col]\n        for z_ex in excluded_instruments:\n            z_parts.append(z_ex.reshape(-1, 1))\n        Z = np.hstack(z_parts)\n\n        # Apply the TSLS formula\n        # Let's compute parts of the formula for clarity\n        # We use np.linalg.solve(A, B) for X = A^-1 B for better numerical stability\n        \n        # Compute projection matrix part: Pz = Z @ inv(Z'Z) @ Z'\n        # A more stable way than forming Pz explicitly is to use its components.\n        try:\n            ZTZ_inv = np.linalg.inv(Z.T @ Z)\n        except np.linalg.LinAlgError:\n            # Handle cases where Z'Z is singular.\n            return np.nan\n\n        # First part of the formula: X' * P_Z\n        # X_T_PZ = X.T @ Z @ ZTZ_inv @ Z.T\n        # This can be computed more efficiently.\n        \n        # Term inside the inverse: A = X'Z(Z'Z)^-1 Z'X\n        XZ_term = X.T @ Z\n        A = XZ_term @ ZTZ_inv @ XZ_term.T\n\n        # Term for the y part: B = X'Z(Z'Z)^-1 Z'y\n        Zy_term = Z.T @ y_col\n        B = XZ_term @ ZTZ_inv @ Zy_term\n        \n        # b_hat = inv(A) @ B\n        try:\n            b_hat = np.linalg.solve(A, B)\n        except np.linalg.LinAlgError:\n            # Handle cases where the matrix for the second stage is singular.\n            return np.nan\n\n        # The coefficient beta is the third element (index 2)\n        beta_hat = b_hat[2, 0]\n        return beta_hat\n\n    results = []\n    for case in test_cases:\n        beta_estimate = compute_tsls_beta(\n            case[\"y\"],\n            case[\"w\"],\n            case[\"x\"],\n            case[\"excluded_instruments\"],\n            case[\"n\"]\n        )\n        results.append(f\"{beta_estimate:.6f}\")\n    \n    # Format and print the final output as specified.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "Our final practice moves from applying $2SLS$ to investigating its properties through simulation, a fundamental skill in modern econometrics. You will act as the architect of a simulated world, generating data where you know the true causal effect, and then testing how well $2SLS$ performs under different conditions [@problem_id:2445083]. This exercise highlights the critical importance of instrument strength and diagnostics, like the first-stage $F$-statistic, in assessing the reliability of your estimates, providing deep insight into the estimator's behavior in finite samples.", "id": "2445083", "problem": "You are given a structural model motivated by a corporate finance setting in which the causal effect of a Chief Executive Officer (CEO) having a Master of Business Administration (MBA) versus a Doctor of Philosophy (PhD) on a firm strategy index is of interest. The outcome variable is denoted by $Y \\in \\mathbb{R}$ (a scalar strategy index). The endogenous regressor is a binary treatment $D \\in \\{0,1\\}$ indicating whether the CEO has an MBA (instead of a PhD). The instrument is $Z \\in \\mathbb{R}$, representing an exogenous index of tuition costs at top schools during the CEO's college years. There is one observed control $X \\in \\mathbb{R}$ (a standardized firm size proxy), and an unobserved disturbance $u \\in \\mathbb{R}$. The data generating process (DGP) is defined as follows for $i = 1, \\dots, n$:\n1. Structural outcome equation:\n$$\nY_i = \\beta_0 + \\beta_1 D_i + \\gamma X_i + u_i .\n$$\n2. Latent selection for education and realized treatment:\n$$\nD_i^{\\ast} = \\alpha_0 + \\pi Z_i + \\alpha_X X_i + v_i, \\quad D_i = \\mathbf{1}\\{D_i^{\\ast} > 0\\} .\n$$\n3. Endogeneity arises through correlation between $u_i$ and $v_i$:\n$$\nu_i = \\rho v_i + \\sqrt{1 - \\rho^2} \\, \\varepsilon_i ,\n$$\nwhere $v_i \\sim \\mathcal{N}(0,\\sigma_v^2)$, $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_u^2)$, and $(Z_i, X_i, v_i, \\varepsilon_i)$ are mutually independent across $i$ and jointly independent of each other. Instrument exogeneity holds because\n$$\n\\mathbb{E}[Z_i u_i] = 0, \\quad \\text{and} \\quad \\mathbb{E}[Z_i \\,|\\, X_i] = 0 .\n$$\nInstrument relevance is governed by the parameter $\\pi$, with larger $|\\pi|$ implying stronger correlation between $Z_i$ and $D_i$ conditional on $X_i$.\n\nThe structural parameters are fixed and identical across all test cases:\n- $\\beta_0 = 0$, $\\beta_1 = 0.8$, $\\gamma = 0.3$,\n- $\\alpha_0 = 0$, $\\alpha_X = 0.5$.\n\nYour task is to implement from first principles a two-stage least squares (2SLS) estimator of $\\beta_1$ using linear projection logic and ordinary least squares operations only. In particular, treat the instrument set as the constant, the instrument $Z$, and the control $X$, and the regressor set as the constant, the endogenous regressor $D$, and the control $X$. Additionally, compute the homoskedastic first-stage $F$-statistic for the null hypothesis that the excluded instrument coefficient(s) are zero in the first-stage regression of $D$ on the instrument set.\n\nAlgorithmic requirements:\n- Use only linear algebra implied by least squares projection to construct the 2SLS estimate. The derivation base available to you is that ordinary least squares computes linear projections by minimizing the sum of squared residuals, and that a valid instrument is uncorrelated with the structural error and correlated with the endogenous regressor.\n- Include a constant in both the instrument set and the regressor set.\n- Compute the first-stage $F$-statistic by comparing the restricted model (regressing $D$ on the constant and $X$) and the unrestricted model (regressing $D$ on the constant, $Z$, and $X$) using the standard homoskedastic analysis-of-variance identity.\n\nData generation protocol for each test case:\n- For each case, given $(n, \\pi, \\rho, \\sigma_v, \\sigma_u, \\text{seed})$, draw for $i = 1, \\dots, n$:\n$$\nZ_i \\sim \\mathcal{N}(0,1), \\quad X_i \\sim \\mathcal{N}(0,1), \\quad v_i \\sim \\mathcal{N}(0,\\sigma_v^2), \\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_u^2),\n$$\nmutually independent.\n- Form $D_i^{\\ast}$, then $D_i = \\mathbf{1}\\{D_i^{\\ast} > 0\\}$, and finally $Y_i$ using the equations above.\n\nTest suite:\nProvide results for the following four test cases, each specified by $(n, \\pi, \\rho, \\sigma_v, \\sigma_u, \\text{seed})$:\n- Case A (strong instrument, large sample): $(n, \\pi, \\rho, \\sigma_v, \\sigma_u, \\text{seed}) = (2000, 1.5, 0.7, 1.0, 1.0, 123)$.\n- Case B (weak instrument, large sample): $(n, \\pi, \\rho, \\sigma_v, \\sigma_u, \\text{seed}) = (2000, 0.1, 0.7, 1.0, 1.0, 456)$.\n- Case C (strong instrument, small sample): $(n, \\pi, \\rho, \\sigma_v, \\sigma_u, \\text{seed}) = (100, 1.5, 0.7, 1.0, 1.0, 789)$.\n- Case D (no endogeneity, strong instrument): $(n, \\pi, \\rho, \\sigma_v, \\sigma_u, \\text{seed}) = (1000, 1.5, 0.0, 1.0, 1.0, 321)$.\n\nWhat to compute per test case:\n- The 2SLS estimate $\\hat{\\beta}_1$ of $\\beta_1$ using the instrument set $\\{1, Z, X\\}$ for the regressor set $\\{1, D, X\\}$.\n- The first-stage homoskedastic $F$-statistic for testing the null that the coefficient on $Z$ is zero in the first-stage linear regression of $D$ on $\\{1, Z, X\\}$.\n\nOutput specification:\n- Your program should produce a single line of output containing a list of length $4$, where each entry corresponds to one test case in the order A, B, C, D.\n- Each entry must be a two-element list $[\\hat{\\beta}_1, F]$.\n- Each number must be rounded to $6$ decimal places.\n- The final output must therefore be a single line in the exact form:\n$$\n\\text{[}[\\hat{\\beta}_1^{(A)}, F^{(A)}],[\\hat{\\beta}_1^{(B)}, F^{(B)}],[\\hat{\\beta}_1^{(C)}, F^{(C)}],[\\hat{\\beta}_1^{(D)}, F^{(D)}]\\text{]}.\n$$\nNo additional text should be printed.", "solution": "The problem presented is a well-posed and scientifically grounded exercise in computational econometrics, requiring the implementation of the two-stage least squares (2SLS) estimator and the computation of a first-stage F-statistic for a simulated dataset. The data generating process is clearly defined, and all necessary parameters are provided. The problem is valid.\n\nThe core of the problem lies in the endogeneity of the regressor $D_i$. The structural equation is:\n$$\nY_i = \\beta_0 + \\beta_1 D_i + \\gamma X_i + u_i\n$$\nEndogeneity arises because the regressor $D_i$ is correlated with the error term $u_i$, i.e., $\\mathbb{E}[D_i u_i] \\neq 0$. This correlation stems from the fact that both $D_i$ and $u_i$ are functions of the same unobserved random shock, $v_i$. Consequently, applying ordinary least squares (OLS) to this structural equation will yield a biased and inconsistent estimator for the parameter of interest, $\\beta_1$.\n\nThe method of instrumental variables (IV), and specifically the 2SLS estimator, is designed to address this issue. It uses an instrument, $Z_i$, which is correlated with the endogenous regressor $D_i$ (relevance) but uncorrelated with the structural error term $u_i$ (exogeneity).\n\nLet us formulate the problem in matrix notation. Let $n$ be the sample size. The model can be written as:\n$$\n\\mathbf{y} = \\mathbf{W}\\boldsymbol{\\theta} + \\mathbf{u}\n$$\nwhere:\n- $\\mathbf{y}$ is the $n \\times 1$ vector of outcomes $Y_i$.\n- $\\mathbf{W}$ is the $n \\times 3$ matrix of regressors: $\\mathbf{W} = [\\mathbf{1}_n, \\mathbf{d}, \\mathbf{x}]$, where $\\mathbf{1}_n$ is a vector of ones, $\\mathbf{d}$ is the vector of treatments $D_i$, and $\\mathbf{x}$ is the vector of controls $X_i$.\n- $\\boldsymbol{\\theta}$ is the $3 \\times 1$ vector of parameters: $\\boldsymbol{\\theta} = [\\beta_0, \\beta_1, \\gamma]^T$.\n- $\\mathbf{u}$ is the $n \\times 1$ vector of structural errors $u_i$.\n\nThe endogeneity problem means $\\mathbb{E}[\\mathbf{W}^T\\mathbf{u}] \\neq \\mathbf{0}$, as the column $\\mathbf{d}$ of $\\mathbf{W}$ is correlated with $\\mathbf{u}$.\n\nThe 2SLS procedure resolves this by using a set of instruments. Let $\\mathbf{Z}$ be the $n \\times 3$ matrix of instruments, composed of the excluded instrument $\\mathbf{z}$ (vector of $Z_i$) and the exogenous regressors from $\\mathbf{W}$, namely the constant and the control $\\mathbf{x}$. Thus, $\\mathbf{Z} = [\\mathbf{1}_n, \\mathbf{z}, \\mathbf{x}]$. The validity of these instruments rests on the condition $\\mathbb{E}[\\mathbf{Z}^T\\mathbf{u}] = \\mathbf{0}$.\n\nThe 2SLS estimator is derived from the principle of linear projection, executed in two stages:\n\n**Stage 1: Purging the Endogenous Regressor**\nThe first stage purges the endogenous component from the regressor $\\mathbf{d}$. This is achieved by performing a linear projection of $\\mathbf{d}$ onto the column space of the instrument matrix $\\mathbf{Z}$. This projection is equivalent to an OLS regression of $\\mathbf{d}$ on $\\mathbf{Z}$. The predicted values from this regression, denoted $\\hat{\\mathbf{d}}$, represent the part of $\\mathbf{d}$ that is linearly explained by the instruments.\nThe OLS coefficient vector for this regression is $\\hat{\\boldsymbol{\\pi}} = (\\mathbf{Z}^T\\mathbf{Z})^{-1}\\mathbf{Z}^T\\mathbf{d}$.\nThe predicted values are $\\hat{\\mathbf{d}} = \\mathbf{Z}\\hat{\\boldsymbol{\\pi}}$. Note that since the exogenous regressors $\\mathbf{1}_n$ and $\\mathbf{x}$ are already in the instrument set $\\mathbf{Z}$, their projection onto the space spanned by $\\mathbf{Z}$ is simply themselves. Therefore, we construct a matrix of \"purified\" regressors, $\\hat{\\mathbf{W}} = [\\mathbf{1}_n, \\hat{\\mathbf{d}}, \\mathbf{x}]$. By construction, $\\hat{\\mathbf{W}}$ is a function of $\\mathbf{Z}$ only, and is therefore uncorrelated with $\\mathbf{u}$ in large samples.\n\n**Stage 2: Estimating the Structural Parameters**\nThe second stage involves an OLS regression of the outcome variable $\\mathbf{y}$ on the purified regressor matrix $\\hat{\\mathbf{W}}$. The resulting coefficient vector is the 2SLS estimator, $\\hat{\\boldsymbol{\\theta}}_{2SLS}$.\n$$\n\\hat{\\boldsymbol{\\theta}}_{2SLS} = (\\hat{\\mathbf{W}}^T \\hat{\\mathbf{W}})^{-1} \\hat{\\mathbf{W}}^T \\mathbf{y}\n$$\nThe parameter of interest, $\\hat{\\beta}_1$, is the second element of this vector $\\hat{\\boldsymbol{\\theta}}_{2SLS}$.\n\n**First-Stage F-Statistic for Instrument Relevance**\nA crucial condition for a valid IV estimation is instrument relevance, meaning the excluded instrument(s) must be significantly correlated with the endogenous regressor(s), conditional on other exogenous variables. A common diagnostic for weak instruments is the first-stage F-statistic. This statistic tests the null hypothesis that the coefficients on the excluded instruments are all zero in the first-stage regression.\n\nIn this problem, we have one endogenous regressor, $D_i$, and one excluded instrument, $Z_i$. The first stage is a linear regression of $D_i$ on $1$, $Z_i$, and $X_i$:\n$$\nD_i = \\pi_0 + \\pi_Z Z_i + \\pi_X X_i + \\text{residual}_i\n$$\nWe test the null hypothesis $H_0: \\pi_Z = 0$. This is done by comparing the sum of squared residuals (SSR) from this unrestricted model with the SSR from a restricted model where $\\pi_Z$ is forced to be zero.\n\n- **Unrestricted Model:** Regress $\\mathbf{d}$ on $\\mathbf{Z} = [\\mathbf{1}_n, \\mathbf{z}, \\mathbf{x}]$. Let the sum of squared residuals be $SSR_{unres}$.\n- **Restricted Model:** Regress $\\mathbf{d}$ on $\\mathbf{Z}_{res} = [\\mathbf{1}_n, \\mathbf{x}]$. Let the sum of squared residuals be $SSR_{res}$.\n\nThe homoskedastic F-statistic is then calculated as:\n$$\nF = \\frac{(SSR_{res} - SSR_{unres}) / q}{SSR_{unres} / (n - k_{unres})}\n$$\nwhere $q$ is the number of excluded instruments (here, $q=1$) and $k_{unres}$ is the number of parameters in the unrestricted first-stage model (here, $k_{unres}=3$, for the intercept, $Z_i$, and $X_i$).\n\nThe implementation will proceed as follows for each test case:\n1.  Generate the data $(Y_i, D_i, Z_i, X_i)$ for $i=1, \\dots, n$ according to the specified DGP.\n2.  Construct the matrices $\\mathbf{y}$, $\\mathbf{d}$, $\\mathbf{W}$, and $\\mathbf{Z}$.\n3.  Perform the first-stage regression of $\\mathbf{d}$ on $\\mathbf{Z}$ to obtain $\\hat{\\mathbf{d}}$. This also yields $SSR_{unres}$.\n4.  Perform the second-stage regression of $\\mathbf{y}$ on $\\hat{\\mathbf{W}} = [\\mathbf{1}_n, \\hat{\\mathbf{d}}, \\mathbf{x}]$ to obtain $\\hat{\\boldsymbol{\\theta}}_{2SLS}$, and extract $\\hat{\\beta}_1$.\n5.  Perform the restricted regression of $\\mathbf{d}$ on $[\\mathbf{1}_n, \\mathbf{x}]$ to obtain $SSR_{res}$.\n6.  Compute the F-statistic using the formula above.\n7.  Store and format the resulting $(\\hat{\\beta}_1, F)$ pair.", "answer": "```python\nimport numpy as np\n\ndef run_case(n, pi, rho, sigma_v, sigma_u, seed):\n    \"\"\"\n    Generates data and computes the 2SLS estimate and first-stage F-statistic\n    for a single test case.\n    \"\"\"\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # Fixed parameters\n    beta_0 = 0.0\n    beta_1 = 0.8\n    gamma = 0.3\n    alpha_0 = 0.0\n    alpha_X = 0.5\n\n    # 1. Data Generation\n    Z = rng.normal(loc=0, scale=1, size=n)\n    X = rng.normal(loc=0, scale=1, size=n)\n    v = rng.normal(loc=0, scale=sigma_v, size=n)\n    epsilon = rng.normal(loc=0, scale=sigma_u, size=n)\n\n    # Latent variable for D\n    D_star = alpha_0 + pi * Z + alpha_X * X + v\n    # Realized treatment\n    D = (D_star > 0).astype(float)\n\n    # Structural error\n    u = rho * v + np.sqrt(1 - rho**2) * epsilon\n\n    # Outcome variable\n    Y = beta_0 + beta_1 * D + gamma * X + u\n\n    # Reshape for matrix operations\n    y_vec = Y.reshape(-1, 1)\n    d_vec = D.reshape(-1, 1)\n\n    # Define matrices for 2SLS\n    # Regressor matrix W = [1, D, X]\n    W = np.c_[np.ones(n), D, X]\n    # Instrument matrix Z_mat = [1, Z, X]\n    Z_mat = np.c_[np.ones(n), Z, X]\n\n    # 2. 2SLS Estimation using explicit two stages\n    # Stage 1: Regress D on Z_mat to get D_hat\n    try:\n        # OLS formula: (Z'Z)^{-1}Z'd\n        ZTZ_inv = np.linalg.inv(Z_mat.T @ Z_mat)\n        pi_hat_vec = ZTZ_inv @ Z_mat.T @ d_vec\n        D_hat = Z_mat @ pi_hat_vec\n    except np.linalg.LinAlgError:\n        # In case of perfect multicollinearity (highly unlikely with random data)\n        return [np.nan, np.nan]\n\n    # Stage 2: Regress Y on [1, D_hat, X]\n    W_hat = np.c_[np.ones(n), D_hat, X]\n    try:\n        # OLS formula: (W_hat'W_hat)^{-1}W_hat'y\n        WHatTWHat_inv = np.linalg.inv(W_hat.T @ W_hat)\n        theta_hat_2sls = WHatTWHat_inv @ W_hat.T @ y_vec\n        beta1_hat = theta_hat_2sls[1, 0]\n    except np.linalg.LinAlgError:\n        beta1_hat = np.nan\n\n    # 3. First-Stage F-statistic Calculation\n    # Unrestricted model (D on 1, Z, X) residuals\n    # Predicted values are simply D_hat\n    residuals_unres = d_vec - D_hat\n    SSR_unres = residuals_unres.T @ residuals_unres\n\n    # Restricted model (D on 1, X)\n    Z_res = np.c_[np.ones(n), X]\n    try:\n        # OLS on restricted model\n        ZresTZres_inv = np.linalg.inv(Z_res.T @ Z_res)\n        delta_hat = ZresTZres_inv @ Z_res.T @ d_vec\n        D_hat_res = Z_res @ delta_hat\n        \n        residuals_res = d_vec - D_hat_res\n        SSR_res = residuals_res.T @ residuals_res\n    except np.linalg.LinAlgError:\n        SSR_res = np.nan\n\n    # Compute F-statistic\n    q = 1  # Number of excluded instruments\n    k_unres = Z_mat.shape[1]  # Number of regressors in unrestricted first stage\n    \n    if SSR_unres > 0 and not np.isnan(SSR_res):\n        numerator = (SSR_res - SSR_unres) / q\n        denominator = SSR_unres / (n - k_unres)\n        F_stat = (numerator / denominator)[0, 0]\n    else:\n        F_stat = np.nan\n\n    return [beta1_hat, F_stat]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A: Strong instrument, large sample\n        (2000, 1.5, 0.7, 1.0, 1.0, 123),\n        # Case B: Weak instrument, large sample\n        (2000, 0.1, 0.7, 1.0, 1.0, 456),\n        # Case C: Strong instrument, small sample\n        (100, 1.5, 0.7, 1.0, 1.0, 789),\n        # Case D: No endogeneity, strong instrument\n        (1000, 1.5, 0.0, 1.0, 1.0, 321),\n    ]\n\n    results_as_strings = []\n    for case_params in test_cases:\n        n, pi, rho, sigma_v, sigma_u, seed = case_params\n        beta1_hat, f_stat = run_case(n, pi, rho, sigma_v, sigma_u, seed)\n        \n        # Format results to 6 decimal places\n        formatted_beta1 = f\"{beta1_hat:.6f}\"\n        formatted_f_stat = f\"{f_stat:.6f}\"\n        \n        results_as_strings.append(f\"[{formatted_beta1},{formatted_f_stat}]\")\n\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"}]}