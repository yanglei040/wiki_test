{"hands_on_practices": [{"introduction": "A particle filter's performance can be highly sensitive to its initial setup, particularly the distribution from which the initial particles are drawn. This first exercise provides a foundational hands-on experience by having you implement a bootstrap particle filter for a common stochastic volatility model. By comparing a diffuse, uninformative prior with a sharply peaked but incorrect one, you will gain direct insight into how the filter's initial beliefs affect its estimation accuracy and convergence over time [@problem_id:2418266].", "id": "2418266", "problem": "Implement and analyze a Sequential Monte Carlo (SMC) particle filter for a nonlinear state-space model relevant to computational finance, and quantify the sensitivity of estimation performance to the initial particle distribution $p(x_0)$. You must compare a diffuse prior with a sharply peaked but incorrect prior. Your program must be a complete, runnable script that simulates data, runs the filters, and outputs quantitative metrics for a small test suite.\n\nThe latent state model is a stochastic volatility model often used for financial returns:\n- State transition (latent log-volatility): $x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t$, where $\\eta_t \\sim \\mathcal{N}(0,1)$ independently over $t$.\n- Observation (return): $y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$ independently over $t$ given $x_t$.\n\nAssume the following foundational base:\n- The latent Markov process and conditional independence of observations given states as specified above.\n- The goal is to approximate the sequence of filtering distributions $p(x_t \\mid y_{1:t})$ using a particle filter.\n- The particle filter to be implemented is the bootstrap particle filter, which uses the transition density as the proposal.\n\nTasks to perform:\n- Simulate a dataset $\\{(x_t, y_t)\\}_{t=1}^T$ from the model for each test case. Initialize $x_0$ from the stationary distribution implied by the transition dynamics, i.e., $x_0 \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$, and then generate $(x_t, y_t)$ forward for $t \\in \\{1,\\dots,T\\}$.\n- Implement the bootstrap particle filter with $N$ particles, systematic resampling, and resampling threshold at effective sample size $N/2$.\n- Use log-weights to avoid numerical underflow and compute the filtered mean $\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$ at each time $t$, where $w_t^{(i)}$ are the normalized weights at time $t$.\n- For each test case, run the particle filter twice on the same simulated data:\n  1. Diffuse prior: $x_0^{(i)} \\sim \\mathcal{N}(m_0, s_0^2)$ with $m_0 = \\mu$ and $s_0 = 3.0$.\n  2. Sharply peaked but incorrect prior: $x_0^{(i)} \\sim \\mathcal{N}(m_b, s_b^2)$ with $m_b = \\mu + 2\\,\\sigma_x$ and $s_b = 0.05$, where $\\sigma_x = \\sigma_v / \\sqrt{1-\\phi^2}$ is the stationary standard deviation of $x_t$.\n- For each run, compute the root mean squared error over the first $H$ steps:\n$$\\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}.$$\n- For each test case, compute the ratio\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}},$$\nso that $R > 1$ indicates that the diffuse prior yields lower early-stage error than the sharply peaked but incorrect prior.\n\nAlgorithmic requirements:\n- Use systematic resampling.\n- Use the effective sample size $ESS = \\left(\\sum_{i=1}^N (w_t^{(i)})^2 \\right)^{-1}$ and trigger resampling whenever $ESS < N/2$.\n- Use log-weights internally for numerical stability.\n- All simulations and filters must be driven by pseudorandom number generators with fixed seeds as specified below to ensure determinism.\n\nTest suite:\n- For each parameter set below, simulate data once with the given seed and then run the two filters (diffuse and peaked priors) using separate fixed seeds. For each case, report the ratio $R$.\n- Case $1$ (baseline persistence and many particles):\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$.\n  - Data simulation seed: $2023001$.\n  - Particle filter seeds: diffuse $= 9020001$, peaked $= 9020002$.\n- Case $2$ (few particles, more weight degeneracy risk):\n  - $\\mu = -0.7$, $\\phi = 0.95$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 150$.\n  - Data simulation seed: $2023002$.\n  - Particle filter seeds: diffuse $= 9020011$, peaked $= 9020012$.\n- Case $3$ (high persistence, stronger sensitivity to $p(x_0)$):\n  - $\\mu = -0.7$, $\\phi = 0.985$, $\\sigma_v = 0.15$, $T = 200$, $H = 25$, $N = 1000$.\n  - Data simulation seed: $2023003$.\n  - Particle filter seeds: diffuse $= 9020021$, peaked $= 9020022$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the test suite cases: $[R_1,R_2,R_3]$.\n- Each $R_j$ must be a floating-point number rounded to exactly six decimal places.\n- No other text should be printed.\n\nNo physical units are involved in this problem. All angles, if any appear implicitly in random number generation, are irrelevant; no angle units are required.\n\nYour program must be self-contained, require no input, and be reproducible under the specified seeds. It must implement all required steps without relying on any external data. The output must be fully determined by the specifications above and your deterministic pseudorandom number usage.", "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, objective, and complete. It describes a standard computational statistics task: the application of a Sequential Monte Carlo (SMC) method to a state-space model common in financial econometrics. All parameters, algorithms, and evaluation metrics are specified without ambiguity. The problem is therefore deemed valid and a solution will be constructed.\n\nThe problem requires the implementation of a bootstrap particle filter to estimate the latent log-volatility, $x_t$, in a stochastic volatility model. The model is defined by two equations:\n\n$1$. The state transition equation for the latent log-volatility process, $x_t$. This is an autoregressive process of order one, AR($1$):\n$$x_t = \\mu + \\phi \\left(x_{t-1} - \\mu\\right) + \\sigma_v \\,\\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0,1)$$\nHere, $\\mu$ is the long-run mean of the process, $\\phi$ is the persistence parameter, and $\\sigma_v$ is the volatility of the log-volatility. For the process to be stationary, it is required that $|\\phi| < 1$. The stationary distribution of this process is Gaussian:\n$$x_t \\sim \\mathcal{N}\\!\\left(\\mu, \\frac{\\sigma_v^2}{1-\\phi^2}\\right)$$\n\n$2$. The observation equation for the financial return, $y_t$. The return is conditionally independent given the current log-volatility and follows a normal distribution with mean $0$ and variance $\\exp(x_t)$:\n$$y_t \\mid x_t \\sim \\mathcal{N}\\left(0, \\exp(x_t)\\right)$$\nThis formulation captures the well-known volatility clustering effect, where large (small) returns are likely to be followed by large (small) returns.\n\nThe objective is to approximate the filtering distribution $p(x_t \\mid y_{1:t})$ for $t = 1, \\dots, T$. The bootstrap particle filter achieves this by sequentially updating a set of $N$ weighted random samples, or \"particles,\" $\\{x_t^{(i)}, w_t^{(i)}\\}_{i=1}^N$. These particles represent a discrete approximation of the filtering distribution. The algorithm proceeds iteratively for each time step $t=1, \\dots, T$.\n\nThe algorithm implemented herein follows these standard steps, beginning with a set of particles $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ and associated normalized weights $\\{w_{t-1}^{(i)}\\}_{i=1}^N$ that approximate $p(x_{t-1} \\mid y_{1:t-1})$:\n\n**Step 1: Resampling.** Weight degeneracy is a common problem in particle filters, where after a few iterations, one particle acquires a weight close to $1$ while all others have weights close to $0$. To mitigate this, we calculate the Effective Sample Size, $ESS = \\left(\\sum_{i=1}^N (w_{t-1}^{(i)})^2 \\right)^{-1}$. If $ESS$ falls below a threshold, specified as $N/2$, a resampling step is performed. Systematic resampling is used, which is an efficient variance-reduction technique. It involves selecting $N$ new particles from the existing set $\\{x_{t-1}^{(i)}\\}_{i=1}^N$ with replacement, where the probability of selecting particle $i$ is its weight $w_{t-1}^{(i)}$. After resampling, the new particle set is unweighted, so all weights are reset to $w_{t-1}^{(i)} = 1/N$.\n\n**Step 2: Propagation.** Each particle is evolved forward in time according to the state transition dynamics. Since the bootstrap filter is used, the proposal distribution is the state transition density itself. For each particle $i$, a new state is drawn:\n$$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}\\left(\\mu + \\phi(x_{t-1}^{(i)} - \\mu), \\sigma_v^2\\right)$$\nThis yields a new set of particles $\\{x_t^{(i)}\\}_{i=1}^N$ which represents the prior distribution at time $t$, $p(x_t \\mid y_{1:t-1})$.\n\n**Step 3: Weighting.** The importance weights are updated to incorporate the new observation $y_t$. The updated (unnormalized) weight for each particle is the product of its previous weight and the likelihood of the observation $y_t$ given the new particle state $x_t^{(i)}$:\n$$\\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\cdot p(y_t \\mid x_t^{(i)})$$\nThe likelihood $p(y_t \\mid x_t^{(i)})$ is given by the probability density function of $\\mathcal{N}(0, \\exp(x_t^{(i)}))$ evaluated at $y_t$. For numerical stability, computations are performed in the logarithmic domain:\n$$\\log \\tilde{w}_t^{(i)} = \\log w_{t-1}^{(i)} + \\log p(y_t \\mid x_t^{(i)})$$\nwhere $\\log p(y_t \\mid x_t^{(i)}) = -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}x_t^{(i)} - \\frac{y_t^2}{2\\exp(x_t^{(i)})}$.\n\n**Step 4: Normalization and Estimation.** The unnormalized log-weights are normalized to ensure they sum to one. First, to prevent numerical overflow, the maximum log-weight is subtracted before exponentiation. The normalized weights are:\n$$w_t^{(i)} = \\frac{\\exp(\\log \\tilde{w}_t^{(i)})}{\\sum_{j=1}^N \\exp(\\log \\tilde{w}_t^{(j)})}$$\nThe filtered estimate of the state mean at time $t$ is then computed as the weighted average of the particles:\n$$\\hat{m}_t = \\sum_{i=1}^N w_t^{(i)} x_t^{(i)}$$\n\nThe analysis requires comparing the filter's performance under two different initial particle distributions $p(x_0)$. A \"diffuse\" prior, $\\mathcal{N}(\\mu, 3.0^2)$, is centered on the true stationary mean but has high variance. This reflects weak prior knowledge. A \"sharply peaked but incorrect\" prior, $\\mathcal{N}(\\mu + 2\\sigma_x, 0.05^2)$ where $\\sigma_x$ is the stationary standard deviation, reflects strong but misplaced confidence. The comparison is quantified by the ratio $R$ of the Root Mean Squared Errors ($\\mathrm{RMSE}$) over an initial horizon $H$:\n$$R = \\frac{\\mathrm{RMSE}^{\\text{peaked}}_{1:H}}{\\mathrm{RMSE}^{\\text{diffuse}}_{1:H}}, \\quad \\mathrm{where} \\quad \\mathrm{RMSE}_{1:H} = \\sqrt{\\frac{1}{H} \\sum_{t=1}^H \\left(\\hat{m}_t - x_t\\right)^2}$$\nA ratio $R > 1$ indicates superior performance of the diffuse prior in the initial stages of filtering. This is expected, as a sharply incorrect prior can mislead the filter, especially in a highly persistent process where the state evolves slowly, requiring more time for the data to correct the initial error. The implementation will follow these principles, using the specified parameters and random seeds to ensure reproducibility.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef systematic_resample(weights, rng):\n    \"\"\"\n    Performs systematic resampling.\n\n    Args:\n        weights (np.ndarray): Array of normalized particle weights.\n        rng (np.random.Generator): A numpy random number generator.\n\n    Returns:\n        np.ndarray: Indices of resampled particles.\n    \"\"\"\n    N = len(weights)\n    # Generate N ordered pointers from a single random draw\n    u = rng.uniform(0.0, 1.0 / N)\n    positions = u + np.arange(N) / N\n    \n    # Calculate cumulative sum of weights\n    cumulative_weights = np.cumsum(weights)\n    \n    # Find indices of particles to keep\n    indices = np.searchsorted(cumulative_weights, positions)\n    return indices\n\ndef simulate_data(mu, phi, sigma_v, T, seed):\n    \"\"\"\n    Simulates data from the stochastic volatility model.\n\n    Args:\n        mu (float): Long-run mean of log-volatility.\n        phi (float): Persistence parameter.\n        sigma_v (float): Volatility of log-volatility.\n        T (int): Number of time steps.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: True states x (T+1) and observations y (T).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    \n    # Stationary distribution for x_0\n    if abs(phi) < 1:\n        sigma_x_sq = sigma_v**2 / (1 - phi**2)\n        sigma_x = np.sqrt(sigma_x_sq)\n        x[0] = mu + sigma_x * rng.standard_normal()\n    else: # Handle non-stationary case if necessary, here we assume it's stationary.\n        x[0] = mu\n        \n    for t in range(1, T + 1):\n        # State transition\n        x[t] = mu + phi * (x[t-1] - mu) + sigma_v * rng.standard_normal()\n        \n        # Observation\n        obs_std_dev = np.exp(x[t] / 2.0)\n        y[t-1] = obs_std_dev * rng.standard_normal()\n        \n    return x, y\n\ndef run_particle_filter(y_data, mu, phi, sigma_v, N, m_prior, s_prior, seed, H):\n    \"\"\"\n    Runs the bootstrap particle filter for the stochastic volatility model.\n\n    Args:\n        y_data (np.ndarray): Array of observations.\n        mu, phi, sigma_v (float): Model parameters.\n        N (int): Number of particles.\n        m_prior, s_prior (float): Mean and std dev for the initial particle distribution.\n        seed (int): Seed for the filter's random number generator.\n        H (int): Horizon for RMSE calculation (unused in function, for context).\n\n    Returns:\n        np.ndarray: Array of filtered state mean estimates.\n    \"\"\"\n    T = len(y_data)\n    rng = np.random.default_rng(seed)\n    \n    # Initialization (t=0)\n    # This represents the particle approximation of p(x_0)\n    particles_tm1 = rng.normal(loc=m_prior, scale=s_prior, size=N)\n    weights_tm1 = np.full(N, 1.0 / N)\n    \n    estimates = np.zeros(T)\n    \n    # Use precomputed constant for log-likelihood\n    LOG_2PI_HALF = 0.5 * np.log(2 * np.pi)\n\n    # Main loop for t=1,...,T (Python index 0 to T-1)\n    for t in range(T):\n        y_obs = y_data[t]\n        \n        # --- Step 1: Resampling (based on weights from t-1) ---\n        ess = 1.0 / np.sum(weights_tm1**2)\n        if ess < N / 2.0:\n            indices = systematic_resample(weights_tm1, rng)\n            particles_tm1 = particles_tm1[indices]\n            # After resampling, weights are implicitly reset\n            log_weights_tm1 = np.full(N, -np.log(N))\n        else:\n            log_weights_tm1 = np.log(weights_tm1)\n\n        # --- Step 2: Propagation (from t-1 to t) ---\n        noise = rng.standard_normal(N)\n        particles_t = mu + phi * (particles_tm1 - mu) + sigma_v * noise\n\n        # --- Step 3: Weighting (using observation y_t) ---\n        log_var = particles_t\n        log_likelihoods = -LOG_2PI_HALF - 0.5 * log_var - (y_obs**2) / (2.0 * np.exp(log_var))\n        \n        # Update log-weights\n        unnorm_log_weights_t = log_weights_tm1 + log_likelihoods\n        \n        # Normalize weights for stability\n        max_log_weight = np.max(unnorm_log_weights_t)\n        temp_weights = np.exp(unnorm_log_weights_t - max_log_weight)\n        weights_t = temp_weights / np.sum(temp_weights)\n\n        # --- Step 4: Estimation ---\n        estimates[t] = np.sum(weights_t * particles_t)\n\n        # --- Step 5: Prepare for next iteration ---\n        particles_tm1 = particles_t\n        weights_tm1 = weights_t\n        \n    return estimates\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023001, 'pf_seeds': {'diffuse': 9020001, 'peaked': 9020002}},\n        # Case 2\n        {'mu': -0.7, 'phi': 0.95, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 150,\n         'data_seed': 2023002, 'pf_seeds': {'diffuse': 9020011, 'peaked': 9020012}},\n        # Case 3\n        {'mu': -0.7, 'phi': 0.985, 'sigma_v': 0.15, 'T': 200, 'H': 25, 'N': 1000,\n         'data_seed': 2023003, 'pf_seeds': {'diffuse': 9020021, 'peaked': 9020022}},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        mu, phi, sigma_v, T, H, N = case['mu'], case['phi'], case['sigma_v'], case['T'], case['H'], case['N']\n        \n        # Simulate data once for the case\n        x_true, y_data = simulate_data(mu, phi, sigma_v, T, case['data_seed'])\n\n        # Define prior parameters\n        # Diffuse prior\n        m0_diffuse = mu\n        s0_diffuse = 3.0\n        \n        # Peaked, incorrect prior\n        sigma_x = sigma_v / np.sqrt(1 - phi**2)\n        m0_peaked = mu + 2.0 * sigma_x\n        s0_peaked = 0.05\n        \n        # Run filters\n        estimates_diffuse = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_diffuse, s0_diffuse, case['pf_seeds']['diffuse'], H\n        )\n        estimates_peaked = run_particle_filter(\n            y_data, mu, phi, sigma_v, N, m0_peaked, s0_peaked, case['pf_seeds']['peaked'], H\n        )\n        \n        # Compute RMSE over the first H steps\n        # x_true[0] is x_0, x_true[1:H+1] corresponds to y_data[0:H]\n        errors_diffuse = estimates_diffuse[:H] - x_true[1:H+1]\n        rmse_diffuse = np.sqrt(np.mean(errors_diffuse**2))\n        \n        errors_peaked = estimates_peaked[:H] - x_true[1:H+1]\n        rmse_peaked = np.sqrt(np.mean(errors_peaked**2))\n        \n        # Compute the ratio R\n        ratio_R = rmse_peaked / rmse_diffuse\n        results.append(f\"{ratio_R:.6f}\")\n\n    # Print final output in the required format\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solution\nsolve()\n```"}, {"introduction": "Beyond simply tracking a hidden state, particle filters are indispensable tools for estimating the parameters of complex models where the likelihood function is intractable. This practice moves into the realm of econometric inference, tasking you with estimating the parameters of a nonlinear Phillips curve with a time-varying NAIRU (Non-Accelerating Inflation Rate of Unemployment). You will use a particle filter to approximate the log-likelihood for different parameter values, a powerful technique that enables parameter estimation for a wide class of nonlinear state-space models [@problem_id:2418262].", "id": "2418262", "problem": "Consider a nonlinear Phillips curve with a time-varying Non-Accelerating Inflation Rate of Unemployment (NAIRU). Let the unobserved state be the NAIRU, denoted by $n_t$, and let observed unemployment be $u_t$, and observed inflation be $\\pi_t$. The model is a nonlinear state-space system defined by\n$$\n\\pi_t = \\alpha + \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t,\n$$\n$$\nn_t - \\mu = \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t,\n$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_\\varepsilon^2)$, $\\eta_t \\sim \\mathcal{N}(0,\\sigma_\\eta^2)$, and $\\alpha = 0$. The initial state $n_0$ is distributed according to the stationary distribution implied by the state equation, i.e.,\n$$\nn_0 \\sim \\mathcal{N}\\!\\left(\\mu,\\;\\frac{\\sigma_\\eta^2}{1-\\rho^2}\\right).\n$$\nYour task is to simulate data for three distinct parameter configurations (test cases), and then, for each test case, compute the maximum likelihood estimate over a specified finite grid for the parameter vector $\\theta = (\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$, keeping $\\alpha=0$, $\\mu$, and $\\rho$ fixed at the values specified in each test case.\n\nData generation must proceed as follows.\n\n- Common observed unemployment process $u_t$:\n  - Time length $T = 120$.\n  - Autoregressive process $u_t = \\mu_u + \\phi\\,(u_{t-1}-\\mu_u) + \\nu_t$ with $\\nu_t \\sim \\mathcal{N}(0,\\sigma_u^2)$.\n  - Parameters: $\\mu_u = 5.5$, $\\phi = 0.8$, $\\sigma_u = 0.3$, and initial value $u_0 = \\mu_u$.\n  - Use pseudorandom seed $100$ to generate $\\{u_t\\}_{t=1}^T$.\n- For each test case $i \\in \\{1,2,3\\}$, simulate the latent state $\\{n_t\\}_{t=1}^T$ and inflation $\\{\\pi_t\\}_{t=1}^T$ using the model above with the test-case-specific true parameters. For each test case $i$, use pseudorandom seed $200+i$ to generate the innovations in the state and measurement equations.\n\nThe test suite is defined by the following three cases, each providing fixed $(\\mu,\\rho)$ and the true $(\\beta^\\star,\\gamma^\\star,\\sigma_\\varepsilon^\\star,\\sigma_\\eta^\\star)$ used for data simulation, as well as the finite grid over which you must compute and maximize the likelihood.\n\n- Case $1$ (general case):\n  - Fixed: $\\mu = 5.5$, $\\rho = 0.95$.\n  - True simulation parameters: $\\beta^\\star = -0.5$, $\\gamma^\\star = 0.06$, $\\sigma_\\varepsilon^\\star = 0.2$, $\\sigma_\\eta^\\star = 0.1$.\n  - Estimation grid:\n    - $\\beta \\in \\{-0.6,\\,-0.5\\}$,\n    - $\\gamma \\in \\{0.04,\\,0.06\\}$,\n    - $\\sigma_\\varepsilon \\in \\{0.18,\\,0.22\\}$,\n    - $\\sigma_\\eta \\in \\{0.09,\\,0.11\\}$.\n- Case $2$ (nearly linear Phillips curve, small state noise):\n  - Fixed: $\\mu = 5.5$, $\\rho = 0.9$.\n  - True simulation parameters: $\\beta^\\star = -0.4$, $\\gamma^\\star = 0.0$, $\\sigma_\\varepsilon^\\star = 0.15$, $\\sigma_\\eta^\\star = 0.02$.\n  - Estimation grid:\n    - $\\beta \\in \\{-0.5,\\,-0.4\\}$,\n    - $\\gamma \\in \\{0.0,\\,0.02\\}$,\n    - $\\sigma_\\varepsilon \\in \\{0.12,\\,0.18\\}$,\n    - $\\sigma_\\eta \\in \\{0.01,\\,0.03\\}$.\n- Case $3$ (highly persistent NAIRU and stronger nonlinearity):\n  - Fixed: $\\mu = 5.5$, $\\rho = 0.98$.\n  - True simulation parameters: $\\beta^\\star = -0.6$, $\\gamma^\\star = 0.2$, $\\sigma_\\varepsilon^\\star = 0.25$, $\\sigma_\\eta^\\star = 0.15$.\n  - Estimation grid:\n    - $\\beta \\in \\{-0.7,\\,-0.6\\}$,\n    - $\\gamma \\in \\{0.18,\\,0.2\\}$,\n    - $\\sigma_\\varepsilon \\in \\{0.22,\\,0.28\\}$,\n    - $\\sigma_\\eta \\in \\{0.13,\\,0.17\\}$.\n\nFor each test case, given the observed pair $\\{(u_t,\\pi_t)\\}_{t=1}^T$ and fixed $(\\mu,\\rho)$, compute the log-likelihood value for every parameter tuple in the specified grid, and select the parameter tuple that maximizes this log-likelihood. Your program must use the same observed unemployment series $\\{u_t\\}_{t=1}^T$ for all test cases. For reproducibility, any additional pseudorandom numbers used within your likelihood computation must be generated using pseudorandom seed $300+i$ for test case $i$, and must be reused identically across all parameter tuples within the same test case.\n\nYour program must output the concatenated list of the maximizing parameter tuples, in the order of test cases $1$, $2$, and $3$, where each tuple is ordered as $(\\beta,\\gamma,\\sigma_\\varepsilon,\\sigma_\\eta)$. Each reported number must be rounded to exactly six decimal places. The final output must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $[\\beta_1,\\gamma_1,\\sigma_{\\varepsilon,1},\\sigma_{\\eta,1},\\beta_2,\\gamma_2,\\sigma_{\\varepsilon,2},\\sigma_{\\eta,2},\\beta_3,\\gamma_3,\\sigma_{\\varepsilon,3},\\sigma_{\\eta,3}]$.\n\nThe answers for all test cases must be floats. No physical units are involved; do not include any unit symbols. Angles are not involved. All fraction-like quantities must be provided as decimals. The output must strictly follow the single-line format described above.", "solution": "The problem statement constitutes a well-posed exercise in computational econometrics, specifically in the domain of parameter estimation for nonlinear state-space models. It is scientifically grounded, internally consistent, and provides all necessary information for a unique, verifiable solution. Therefore, the problem is deemed valid.\n\nThe central task is to find the maximum likelihood estimate of a parameter vector $\\theta = (\\beta, \\gamma, \\sigma_\\varepsilon, \\sigma_\\eta)$ for a nonlinear Phillips curve model, where the search space is restricted to a finite grid. The model is defined by a measurement equation for inflation $\\pi_t$ and a state equation for the unobserved NAIRU, $n_t$:\n$$\n\\pi_t = \\beta \\left(u_t - n_t\\right) + \\gamma \\left(u_t - n_t\\right)^3 + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)\n$$\n$$\nn_t = \\mu + \\rho \\left(n_{t-1} - \\mu\\right) + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n$$\n\nThe nonlinearity introduced by the cubic term $(u_t - n_t)^3$ in the measurement equation renders the likelihood function analytically intractable. For linear Gaussian state-space models, the Kalman filter provides an exact computation of the likelihood. However, for nonlinear or non-Gaussian systems, numerical approximation methods are required. The standard and principled approach for this class of problems is to use Sequential Monte Carlo (SMC) methods, commonly known as particle filters.\n\nThe methodology is to approximate the log-likelihood function, $\\log \\mathcal{L}(\\theta | Y_{1:T})$, for each parameter vector $\\theta$ on the specified grid. The log-likelihood can be decomposed into a sum of conditional log-likelihoods:\n$$\n\\log \\mathcal{L}(\\theta | Y_{1:T}) = \\sum_{t=1}^T \\log p(y_t | Y_{1:t-1}; \\theta)\n$$\nwhere $Y_{1:t} = \\{(\\pi_1, u_1), \\dots, (\\pi_t, u_t)\\}$ represents the history of observations up to time $t$. The particle filter provides a numerical estimate of each predictive density term, $p(\\pi_t | Y_{1:t-1}; \\theta)$.\n\nWe will implement a specific variant of the particle filter known as the Bootstrap Filter. The algorithm proceeds as follows for a given parameter vector $\\theta$ and a set of $N$ particles:\n\n1.  **Initialization (t=0)**: A set of $N$ particles $\\{n_0^{(j)}\\}_{j=1}^N$ is drawn from the initial state distribution, which is the stationary distribution of the state process: $n_0 \\sim \\mathcal{N}(\\mu, \\sigma_\\eta^2/(1-\\rho^2))$. The total log-likelihood is initialized to $0$.\n\n2.  **Sequential Update (for t = 1 to T)**:\n    a.  **Prediction (Propagation)**: Each particle is propagated forward in time according to the state transition equation. A random innovation is drawn for each particle:\n        $$\n        n_t^{(j)} = \\mu + \\rho(n_{t-1}^{(j)} - \\mu) + \\eta_t^{(j)}, \\quad \\text{where } \\eta_t^{(j)} \\sim \\mathcal{N}(0, \\sigma_\\eta^2)\n        $$\n    b.  **Weighting**: Each propagated particle $n_t^{(j)}$ is assigned a weight, $w_t^{(j)}$, based on how well it explains the current observation $\\pi_t$. The weight is the value of the probability density function (PDF) of the measurement error, evaluated at the observation:\n        $$\n        \\hat{\\pi}_t^{(j)} = \\beta(u_t - n_t^{(j)}) + \\gamma(u_t - n_t^{(j)})^3\n        $$\n        $$\n        w_t^{(j)} = p(\\pi_t | n_t^{(j)}, u_t; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma_\\varepsilon^2}} \\exp\\left(-\\frac{(\\pi_t - \\hat{\\pi}_t^{(j)})^2}{2\\sigma_\\varepsilon^2}\\right)\n        $$\n    c.  **Likelihood Approximation**: The conditional likelihood $p(\\pi_t | Y_{1:t-1}; \\theta)$ is approximated by the average of the unnormalized weights:\n        $$\n        \\hat{p}(\\pi_t | Y_{1:t-1}; \\theta) \\approx \\frac{1}{N} \\sum_{j=1}^N w_t^{(j)}\n        $$\n        The total log-likelihood is updated by adding the logarithm of this average weight. If the average weight is zero, the log-likelihood is negative infinity, indicating that the parameter vector is incompatible with the observations.\n    d.  **Resampling**: To combat the problem of particle degeneracy (where one particle acquires all the weight), a new set of particles is sampled with replacement from the current set $\\{n_t^{(j)}\\}$, with sampling probabilities given by the normalized weights $W_t^{(j)} = w_t^{(j)} / \\sum_k w_t^{(k)}$. We employ systematic resampling, which is an efficient and low-variance technique. The resampled particles are then used in the prediction step for time $t+1$.\n\nThe overall computational procedure is as follows:\nFirst, the common observed unemployment series $\\{u_t\\}_{t=1}^{T=120}$ is simulated. Then, for each of the three test cases:\n1.  The inflation series $\\{\\pi_t\\}_{t=1}^{T=120}$ is simulated using the test case's true parameters and specified random seed.\n2.  A set of random variates required for the particle filter (for initial draws, propagation, and resampling) is pre-generated using the specified seed for the estimation step. This ensures that the stochasticity of the filter itself is identical for all points on the parameter grid, guaranteeing a fair comparison of their likelihood values.\n3.  The approximated log-likelihood is computed for every parameter tuple in the test case's grid using the particle filter.\n4.  The parameter tuple that yields the highest log-likelihood value is identified as the maximum likelihood estimate for that case.\n\nFinally, the estimated parameters from all three cases are concatenated and formatted as per the problem specification.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main solver function to run simulations and estimations for all test cases.\n    \"\"\"\n    # Global parameters\n    T = 120\n    U_MU = 5.5\n    U_PHI = 0.8\n    U_SIGMA = 0.3\n    U_INITIAL = U_MU\n    U_SEED = 100\n    N_PARTICLES = 2000\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"case_id\": 1,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.95},\n            \"true_params\": {\"beta\": -0.5, \"gamma\": 0.06, \"sigma_e\": 0.2, \"sigma_n\": 0.1},\n            \"grid\": {\n                \"beta\": [-0.6, -0.5],\n                \"gamma\": [0.04, 0.06],\n                \"sigma_e\": [0.18, 0.22],\n                \"sigma_n\": [0.09, 0.11],\n            },\n            \"sim_seed\": 201,\n            \"est_seed\": 301,\n        },\n        {\n            \"case_id\": 2,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.9},\n            \"true_params\": {\"beta\": -0.4, \"gamma\": 0.0, \"sigma_e\": 0.15, \"sigma_n\": 0.02},\n            \"grid\": {\n                \"beta\": [-0.5, -0.4],\n                \"gamma\": [0.0, 0.02],\n                \"sigma_e\": [0.12, 0.18],\n                \"sigma_n\": [0.01, 0.03],\n            },\n            \"sim_seed\": 202,\n            \"est_seed\": 302,\n        },\n        {\n            \"case_id\": 3,\n            \"fixed_params\": {\"mu\": 5.5, \"rho\": 0.98},\n            \"true_params\": {\"beta\": -0.6, \"gamma\": 0.2, \"sigma_e\": 0.25, \"sigma_n\": 0.15},\n            \"grid\": {\n                \"beta\": [-0.7, -0.6],\n                \"gamma\": [0.18, 0.2],\n                \"sigma_e\": [0.22, 0.28],\n                \"sigma_n\": [0.13, 0.17],\n            },\n            \"sim_seed\": 203,\n            \"est_seed\": 303,\n        }\n    ]\n\n    def generate_unemployment_data(T, mu_u, phi, sigma_u, u0, seed):\n        rng = np.random.default_rng(seed)\n        u = np.zeros(T)\n        u_prev = u0\n        nu = rng.normal(0, sigma_u, T)\n        for t in range(T):\n            u_t = mu_u + phi * (u_prev - mu_u) + nu[t]\n            u[t] = u_t\n            u_prev = u_t\n        return u\n\n    def simulate_model_data(T, u_series, true_params, fixed_params, seed):\n        rng = np.random.default_rng(seed)\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        beta, gamma, sigma_e, sigma_n = true_params.values()\n        \n        n_series = np.zeros(T)\n        pi_series = np.zeros(T)\n        \n        # Initial state n_0 from stationary distribution\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        n_prev = rng.normal(mu, np.sqrt(sigma_n0_sq))\n\n        innov_eta = rng.normal(0, sigma_n, T)\n        innov_eps = rng.normal(0, sigma_e, T)\n\n        for t in range(T):\n            # State equation\n            n_t = mu + rho * (n_prev - mu) + innov_eta[t]\n            n_series[t] = n_t\n            \n            # Measurement equation\n            u_gap = u_series[t] - n_t\n            pi_t = beta * u_gap + gamma * (u_gap**3) + innov_eps[t]\n            pi_series[t] = pi_t\n            \n            n_prev = n_t\n            \n        return pi_series\n\n    def particle_filter_log_likelihood(pi_series, u_series, params, fixed_params, N, rand_draws):\n        beta, gamma, sigma_e, sigma_n = params\n        mu, rho = fixed_params[\"mu\"], fixed_params[\"rho\"]\n        T = len(pi_series)\n        z_init, z_prop, u_resample = rand_draws\n\n        # Initial state distribution\n        if rho**2 >= 1 or sigma_n <= 0 or sigma_e <= 0:\n            return -np.inf\n        sigma_n0_sq = sigma_n**2 / (1 - rho**2)\n        sigma_n0 = np.sqrt(sigma_n0_sq)\n\n        # Initialization (t=0)\n        particles = mu + sigma_n0 * z_init\n        log_likelihood = 0.0\n\n        for t in range(T):\n            # Prediction/Propagation\n            particles = mu + rho * (particles - mu) + sigma_n * z_prop[t]\n\n            # Weighting\n            u_gap = u_series[t] - particles\n            pi_expected = beta * u_gap + gamma * (u_gap**3)\n            \n            # Using scipy.stats.norm.logpdf for numerical stability with small weights\n            log_weights = norm.logpdf(pi_series[t], loc=pi_expected, scale=sigma_e)\n            \n            # Log-Likelihood update using LogSumExp trick\n            if np.all(np.isneginf(log_weights)):\n                return -np.inf\n                \n            max_log_weight = np.max(log_weights)\n            weights_stable = np.exp(log_weights - max_log_weight)\n            mean_weight = np.mean(weights_stable)\n            \n            log_likelihood += max_log_weight + np.log(mean_weight)\n            \n            # Normalization\n            normalized_weights = weights_stable / np.sum(weights_stable)\n            \n            # Resampling (Systematic)\n            if np.sum(normalized_weights) > 0:\n                positions = (np.arange(N) + u_resample[t]) / N\n                cum_weights = np.cumsum(normalized_weights)\n                indices = np.searchsorted(cum_weights, positions)\n                particles = particles[indices]\n            else: # All weights were zero.\n                return -np.inf\n\n        return log_likelihood\n\n\n    # --- Main Execution ---\n    # Step 1: Generate common unemployment data\n    u_data = generate_unemployment_data(T, U_MU, U_PHI, U_SIGMA, U_INITIAL, U_SEED)\n\n    final_results = []\n    \n    for case in test_cases:\n        # Step 2: Simulate case-specific inflation data\n        pi_data = simulate_model_data(T, u_data, case[\"true_params\"], case[\"fixed_params\"], case[\"sim_seed\"])\n\n        # Step 3: Pre-generate random numbers for the particle filter\n        rng_est = np.random.default_rng(case[\"est_seed\"])\n        rand_draws_pf = (\n            rng_est.standard_normal(N_PARTICLES),  # For initialization\n            rng_est.standard_normal((T, N_PARTICLES)),  # For propagation\n            rng_est.random(T),  # For resampling\n        )\n\n        # Step 4: Grid search for MLE\n        max_log_lik = -np.inf\n        best_params = None\n\n        param_grid = list(itertools.product(\n            case[\"grid\"][\"beta\"],\n            case[\"grid\"][\"gamma\"],\n            case[\"grid\"][\"sigma_e\"],\n            case[\"grid\"][\"sigma_n\"]\n        ))\n        \n        for params_tuple in param_grid:\n            log_lik = particle_filter_log_likelihood(\n                pi_data, u_data, params_tuple, case[\"fixed_params\"], N_PARTICLES, rand_draws_pf\n            )\n            \n            if log_lik > max_log_lik:\n                max_log_lik = log_lik\n                best_params = params_tuple\n        \n        final_results.extend(best_params)\n\n    # Format and print the final output\n    print(f\"[{','.join([f'{val:.6f}' for val in final_results])}]\")\n\nsolve()\n```"}, {"introduction": "An expert practitioner knows not only how to apply a technique, but also when. This final exercise presents a problem of detecting a structural break in a nonlinear model, a scenario where a particle filter seems like a natural tool. However, a deliberate simplification in the model, a deterministic state evolution ($\\sigma_{\\eta} = 0$), reveals that a much simpler computational approach is sufficient. This practice underscores the critical importance of carefully analyzing model assumptions before deploying computationally intensive algorithms, building your intuition for the boundary between complex and simple problems [@problem_id:2418273].", "id": "2418273", "problem": "Consider a univariate time series generated by a nonlinear state-space model with a single structural break in a time-invariant parameter governing the latent state dynamics. Let the latent state be denoted by $x_t$ and the observed series by $y_t$ for $t \\in \\{1,\\dots,T\\}$. The model is specified as follows:\n- State transition: $x_t = \\theta_t + \\rho \\, x_{t-1} + \\sigma_{\\eta} \\, \\eta_t$, where $\\eta_t \\sim \\mathcal{N}(0,1)$.\n- Measurement equation: $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$, where $\\varepsilon_t \\sim \\mathcal{N}(0,1)$.\n- Break structure for the parameter $\\theta_t$: there exists an unknown break time $\\tau \\in \\{1,\\dots,T\\}$ such that $\\theta_t = \\theta_1$ for $t \\le \\tau$ and $\\theta_t = \\theta_2$ for $t > \\tau$. The prior for $\\tau$ is uniform on $\\{1,\\dots,T\\}$.\n- The initial condition $x_0$ is known.\n\nYour task is to construct a program that, for each test case below, takes the model and the observed time series implied by the provided data generation process and returns the maximum a posteriori estimate $\\hat{\\tau} \\in \\{1,\\dots,T\\}$ of the structural break time, reported as an integer.\n\nFor data generation in all test cases, the following conditions apply:\n- The state innovation standard deviation is zero, $\\sigma_{\\eta} = 0$, so that $x_t$ evolves deterministically given parameters and $\\tau$.\n- The measurement innovation standard deviation is $\\sigma_{\\varepsilon} = 0.1$.\n- The autoregressive coefficient is $\\rho = 0.7$.\n- The initial state is $x_0 = 0$.\n- The time horizon is $T = 30$.\n- The measurement noise draws $\\{\\varepsilon_t\\}_{t=1}^{T}$ are fixed and given by the list\n$\\big[\\, 0.03,\\,-0.02,\\,0.01,\\,0.00,\\,-0.01,\\,0.02,\\,-0.03,\\,0.04,\\,-0.02,\\,0.01,\\,0.00,\\,-0.04,\\,0.05,\\,-0.01,\\,0.02,\\,-0.02,\\,0.03,\\,-0.03,\\,0.01,\\,0.00,\\,0.02,\\,-0.01,\\,0.04,\\,-0.02,\\,0.01,\\,-0.03,\\,0.02,\\,0.00,\\,-0.01,\\,0.03\\,\\big]$.\n- For each test case with specified $(\\theta_1,\\theta_2,\\tau_{\\text{true}})$, construct the latent path $\\{x_t\\}$ deterministically by the recursion $x_t = \\theta_t + \\rho \\, x_{t-1}$ with $\\theta_t$ defined by $\\tau_{\\text{true}}$, and then define the observed series by $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\, \\varepsilon_t$ using the fixed $\\{\\varepsilon_t\\}$ above. The resulting $\\{y_t\\}$ constitutes the data that your estimation procedure should use; you must not use $\\{\\varepsilon_t\\}$ or $\\tau_{\\text{true}}$ in estimation.\n\nTest Suite:\n- Case A (no break within sample): $\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 30$.\n- Case B (early break): $\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 8$.\n- Case C (late break): $\\theta_1 = 0.5$, $\\theta_2 = 1.5$, $\\tau_{\\text{true}} = 22$.\n\nFor each case, compute the maximum a posteriori estimate $\\hat{\\tau}$ of the break time based solely on the likelihood implied by the model and the uniform prior on $\\tau$. Your program should produce a single line of output containing the three estimated break times, in order for Cases A, B, and C, as a comma-separated list enclosed in square brackets, for example $[\\hat{\\tau}_A,\\hat{\\tau}_B,\\hat{\\tau}_C]$. The outputs must be integers. No physical units are involved in this problem, and angles are not applicable. The solution must not read any external input; all values needed are provided herein or must be hard-coded exactly as specified.", "solution": "The problem is valid. It is a well-posed exercise in parameter estimation for a nonlinear state-space model with a structural break. We proceed to the solution.\n\nThe objective is to find the maximum a posteriori (MAP) estimate of the unknown structural break time, $\\tau$. The parameter $\\tau$ can take any integer value in the set $\\{1, \\dots, T\\}$, where $T=30$ is the time horizon. The MAP estimate, denoted $\\hat{\\tau}$, is the value of $\\tau$ that maximizes the posterior probability given the observed data $y_{1:T} = \\{y_1, \\dots, y_T\\}$. Using Bayes' theorem, the posterior probability is:\n$$\nP(\\tau | y_{1:T}) = \\frac{P(y_{1:T} | \\tau) P(\\tau)}{P(y_{1:T})}\n$$\nWe wish to find $\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(\\tau | y_{1:T})$. The denominator $P(y_{1:T})$ is a normalizing constant independent of $\\tau$ and can be disregarded in the optimization. The problem specifies a uniform prior distribution for $\\tau$, such that $P(\\tau) = 1/T$ for all $\\tau \\in \\{1, \\dots, T\\}$. As this prior is also constant with respect to $\\tau$, it does not affect the location of the maximum. Therefore, the MAP estimation problem simplifies to a maximum likelihood (ML) estimation problem:\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} P(y_{1:T} | \\tau)\n$$\nwhere $P(y_{1:T} | \\tau)$ is the likelihood of the observed data, conditional on a hypothesized break time $\\tau$. It is often more convenient to work with the log-likelihood, $\\mathcal{L}(\\tau) = \\log P(y_{1:T} | \\tau)$. The value of $\\tau$ that maximizes the likelihood also maximizes the log-likelihood.\n$$\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1, \\dots, T\\}} \\mathcal{L}(\\tau)\n$$\nLet us now define the likelihood function. A critical simplification is provided: the state innovation standard deviation is zero, $\\sigma_{\\eta} = 0$. This renders the state transition equation deterministic for a given set of parameters. Specifically, for any hypothesized break time $k \\in \\{1, \\dots, T\\}$, the entire latent state path $\\{x_t(k)\\}_{t=1}^T$ is uniquely determined by the recursion:\n$$\nx_t(k) = \\theta_t(k) + \\rho x_{t-1}(k)\n$$\nwith initial condition $x_0(k) = x_0 = 0$. The parameter $\\theta_t(k)$ is defined as $\\theta_t(k) = \\theta_1$ for $t \\le k$ and $\\theta_t(k) = \\theta_2$ for $t > k$. Because the latent path is deterministic for a given $k$, there is no need for state estimation via filtering techniques, such as a particle filter, which would be necessary if $\\sigma_{\\eta} > 0$.\n\nThe measurement equation is $y_t = \\frac{1}{2} x_t^2 + \\sigma_{\\varepsilon} \\varepsilon_t$, with $\\varepsilon_t \\sim \\mathcal{N}(0,1)$. This implies that each observation $y_t$ is conditionally normal, given the state $x_t$:\n$$\ny_t | x_t \\sim \\mathcal{N}\\left(\\mu_t, \\sigma_{\\varepsilon}^2\\right) \\quad \\text{where} \\quad \\mu_t = \\frac{1}{2} x_t^2\n$$\nThe probability density function for a single observation $y_t$, given the state $x_t(k)$ determined by the hypothesis $\\tau=k$, is:\n$$\np(y_t | x_t(k)) = \\frac{1}{\\sqrt{2\\pi\\sigma_{\\varepsilon}^2}} \\exp\\left( -\\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right)\n$$\nThe measurement errors $\\varepsilon_t$ are independent across time. Thus, the total likelihood for the sequence $y_{1:T}$ is the product of the individual densities:\n$$\nP(y_{1:T} | \\tau=k) = \\prod_{t=1}^T p(y_t | x_t(k))\n$$\nThe corresponding log-likelihood is:\n$$\n\\mathcal{L}(k) = \\log P(y_{1:T} | \\tau=k) = \\sum_{t=1}^T \\log p(y_t | x_t(k)) = \\sum_{t=1}^T \\left[ -\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2) - \\frac{\\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2}{2\\sigma_{\\varepsilon}^2} \\right]\n$$\nTo maximize $\\mathcal{L}(k)$ with respect to $k$, we can ignore the constant terms $-\\frac{1}{2}\\log(2\\pi\\sigma_{\\varepsilon}^2)$ and the positive scaling factor $1/(2\\sigma_{\\varepsilon}^2)$. Maximizing the log-likelihood is therefore equivalent to minimizing the sum of squared errors (SSE) between the observed data and the model's prediction for the mean of $y_t$:\n$$\n\\hat{\\tau} = \\arg\\min_{k \\in \\{1, \\dots, T\\}} \\sum_{t=1}^T \\left(y_t - \\frac{1}{2} x_t(k)^2\\right)^2\n$$\nThe estimation procedure is as follows. For each test case:\n$1.$ First, generate the observed time series $\\{y_t\\}_{t=1}^T$ according to the problem specification, using the provided true parameters $(\\theta_1, \\theta_2, \\tau_{\\text{true}})$ and the fixed noise sequence $\\{\\varepsilon_t\\}_{t=1}^T$.\n$2.$ Then, to estimate $\\hat{\\tau}$, iterate through every possible candidate break time $k$ from $1$ to $T=30$.\n$3.$ For each candidate $k$, compute the hypothetical deterministic state path $\\{x_t(k)\\}_{t=1}^T$ using the known values of $\\theta_1$, $\\theta_2$, $\\rho$, and $x_0$.\n$4.$ Calculate the sum of squared errors, $SSE(k) = \\sum_{t=1}^T (y_t - \\frac{1}{2}x_t(k)^2)^2$.\n$5.$ The estimate $\\hat{\\tau}$ is the value of $k$ that yields the minimum SSE. If multiple values of $k$ produce the same minimum SSE, the smallest such $k$ is chosen by convention (e.g., by `argmin`).\n\nThis procedure is implemented for each test case to find the corresponding estimate $\\hat{\\tau}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the maximum a posteriori estimate of a structural break time\n    in a nonlinear state-space model for three test cases.\n    \"\"\"\n\n    # Global parameters and data generation settings from the problem statement\n    sigma_eps = 0.1\n    rho = 0.7\n    x0 = 0.0\n    T = 30\n    epsilon_t_draws = np.array([\n        0.03, -0.02, 0.01, 0.00, -0.01, 0.02, -0.03, 0.04, -0.02, 0.01,\n        0.00, -0.04, 0.05, -0.01, 0.02, -0.02, 0.03, -0.03, 0.01, 0.00,\n        0.02, -0.01, 0.04, -0.02, 0.01, -0.03, 0.02, 0.00, -0.01, 0.03\n    ])\n\n    test_cases = [\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 30},  # Case A\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 8},   # Case B\n        {'theta1': 0.5, 'theta2': 1.5, 'tau_true': 22}   # Case C\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        theta1 = case['theta1']\n        theta2 = case['theta2']\n        tau_true = case['tau_true']\n\n        #\n        # Step 1: Generate the observed time series y_t for the current case.\n        # This part simulates the data that the estimation procedure will use.\n        #\n        x_true = np.zeros(T + 1)\n        x_true[0] = x0\n        y_obs = np.zeros(T)\n        \n        for t in range(1, T + 1):\n            theta_t = theta1 if t <= tau_true else theta2\n            x_true[t] = theta_t + rho * x_true[t-1]\n            # y_obs is 0-indexed, so y_t corresponds to y_obs[t-1]\n            y_obs[t-1] = 0.5 * x_true[t]**2 + sigma_eps * epsilon_t_draws[t-1]\n            \n        #\n        # Step 2: Estimate the break time tau by maximizing the likelihood.\n        # This is equivalent to minimizing the sum of squared errors (SSE).\n        # The estimator only has access to y_obs and the model structure/parameters\n        # (theta1, theta2, rho, sigma_eps), not tau_true or epsilon_t_draws.\n        #\n        \n        sse_values = []\n        possible_taus = range(1, T + 1)\n        \n        for k in possible_taus:\n            # For each candidate break time k, compute the hypothetical state path\n            # and the corresponding SSE.\n            x_hypothetical = np.zeros(T + 1)\n            x_hypothetical[0] = x0\n            current_sse = 0.0\n            \n            for t in range(1, T + 1):\n                theta_t_hyp = theta1 if t <= k else theta2\n                x_hypothetical[t] = theta_t_hyp + rho * x_hypothetical[t-1]\n                \n                y_pred = 0.5 * x_hypothetical[t]**2\n                current_sse += (y_obs[t-1] - y_pred)**2\n                \n            sse_values.append(current_sse)\n            \n        # The MAP/ML estimate for tau is the one that minimizes the SSE.\n        # np.argmin returns the 0-based index of the minimum SSE.\n        # Since possible_taus starts from 1, the estimated tau is index + 1.\n        min_sse_index = np.argmin(sse_values)\n        tau_hat = possible_taus[min_sse_index]\n        \n        results.append(tau_hat)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}