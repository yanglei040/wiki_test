{"hands_on_practices": [{"introduction": "Bayesian estimation often relies on Markov Chain Monte Carlo (MCMC) methods, which can seem like a black box. This first exercise opens that box by having you implement the core machinery of the most famous MCMC algorithm—a Metropolis-Hastings sampler [@problem_id:2375844]. By building a \"block\" update for correlated parameters, you will handle parameter transformations and proposal distributions, gaining a deep, functional understanding of how posterior distributions are explored in practice.", "id": "2375844", "problem": "You are asked to implement a single blocked Metropolis–Hastings (MH) update for two highly negatively correlated structural parameters in a Dynamic Stochastic General Equilibrium (DSGE) model context, specifically the price and wage Calvo stickiness parameters. Let the two parameters be denoted by $\\xi_p \\in (0,1)$ (price stickiness) and $\\xi_w \\in (0,1)$ (wage stickiness). Work in a reduced-form setting where the likelihood is defined by a parsimonious linear Gaussian system approximating a New Keynesian block. The goal is to compute a block proposal in a transformed unconstrained space and decide whether to accept it using Bayes’ theorem and the Metropolis–Hastings acceptance criterion.\n\nFundamental base to use:\n- Bayes’ theorem: For data $Y$, prior $p(\\theta)$, and likelihood $p(Y \\mid \\theta)$, the posterior is $p(\\theta \\mid Y) \\propto p(Y \\mid \\theta) p(\\theta)$.\n- Metropolis–Hastings algorithm with a symmetric proposal: Given current state $\\theta$, propose $\\theta'$ from a proposal density $q(\\cdot \\mid \\theta)$ satisfying $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$, accept with probability $\\alpha = \\min\\{1, \\frac{p(\\theta' \\mid Y)}{p(\\theta \\mid Y)}\\}$.\n- Affine transformations and Jacobians: When transforming from constrained variables to unconstrained variables, the target density must either be expressed in the transformed space or the Jacobian must be included if the base measure is not transformed.\n\nModel and data:\n- Parameters: $\\theta = (\\xi_p, \\xi_w)$ with $\\xi_p \\in (0,1)$ and $\\xi_w \\in (0,1)$.\n- Transform to unconstrained parameters via the logit map: $\\psi = (\\psi_p, \\psi_w)$, where $\\psi_j = \\log\\left(\\frac{\\xi_j}{1-\\xi_j}\\right)$ for $j \\in \\{p, w\\}$. The inverse-logit (logistic) is $\\xi_j = \\frac{1}{1 + e^{-\\psi_j}}$.\n- Reduced-form mapping from $(\\xi_p,\\xi_w)$ to linear-Gaussian coefficients:\n  - Define $b(\\xi_p,\\xi_w) = (1 - \\xi_p)(1 - \\xi_w)$.\n  - Set $\\rho(\\theta) = 0.6 - 0.5 \\, b(\\xi_p,\\xi_w)$ and $\\alpha(\\theta) = 0.5 \\, b(\\xi_p,\\xi_w)$.\n- Observables: output gap $\\{y_t\\}_{t=1}^T$ and inflation $\\{\\pi_t\\}_{t=1}^T$, with initial $y_0$ given.\n- Conditional Gaussian likelihood:\n  - State equation: $y_t \\mid y_{t-1}, \\theta \\sim \\mathcal{N}(\\rho(\\theta)\\, y_{t-1}, \\sigma_y^2)$.\n  - Measurement equation: $\\pi_t \\mid y_t, \\theta \\sim \\mathcal{N}(\\alpha(\\theta)\\, y_t, \\sigma_\\pi^2)$.\n  - Assume conditional independence across $t$ and between the two Gaussian innovations.\n- Prior in transformed space: $\\psi \\sim \\mathcal{N}(\\mu_\\psi, \\Sigma_\\psi)$ with $\\mu_\\psi = (0,0)$ and $\\Sigma_\\psi = \\mathrm{diag}(1,1)$.\n- Work in $\\psi$-space so that the target density for MH is proportional to $p(Y \\mid \\theta(\\psi)) \\cdot \\phi(\\psi; \\mu_\\psi, \\Sigma_\\psi)$, where $\\phi$ is the Gaussian density for $\\psi$. No Jacobian is needed because the base measure is $d\\psi$.\n\nData (use exactly these values):\n- Horizon: $T = 12$.\n- Initial condition: $y_0 = 0$.\n- Output gap sequence $\\{y_t\\}_{t=1}^{12}$:\n  $[\\,0.050,\\;0.041,\\;0.028,\\;0.022,\\;0.018,\\;0.015,\\;0.013,\\;0.012,\\;0.010,\\;0.007,\\;0.005,\\;0.004\\,]$.\n- Inflation sequence $\\{\\pi_t\\}_{t=1}^{12}$:\n  $[\\,0.020,\\;0.018,\\;0.017,\\;0.016,\\;0.015,\\;0.013,\\;0.012,\\;0.011,\\;0.010,\\;0.0090,\\;0.0085,\\;0.0080\\,]$.\n- Likelihood standard deviations: $\\sigma_y = 0.05$ and $\\sigma_\\pi = 0.02$.\n\nBlocked proposal in transformed space:\n- Given current $\\psi$, propose $\\psi' = \\psi + L z$, where $L$ is the lower-triangular Cholesky factor of the specified proposal covariance matrix $S$ (so that $S = L L^\\top$) and $z \\sim \\mathcal{N}(0,I_2)$. Because the proposal is an additive Gaussian random walk in $\\psi$, it is symmetric. For reproducibility, you must not generate randomness; instead, for each test case you are given the standard normal vector $z$ to use directly.\n- Compute $\\theta' = \\theta(\\psi')$ by applying the inverse-logit componentwise.\n\nAcceptance rule:\n- Compute the log-posterior in $\\psi$-space:\n  $\\log \\pi(\\psi \\mid Y) = \\log p(Y \\mid \\theta(\\psi)) + \\log \\phi(\\psi; \\mu_\\psi, \\Sigma_\\psi) + C$, where $C$ is any constant independent of $\\psi$. It is valid to drop any additive constants shared by current and proposed states. Use the Gaussian log density forms without normalizing constants if you wish, since they cancel in the Metropolis–Hastings ratio.\n- Because the proposal is symmetric in $\\psi$-space, the acceptance probability is\n  $\\alpha = \\min\\{1, \\exp\\big( \\log \\pi(\\psi' \\mid Y) - \\log \\pi(\\psi \\mid Y) \\big)\\}$.\n- Accept if $u < \\alpha$, where $u \\in (0,1)$ is a provided deterministic value for each test case.\n\nTest suite:\nImplement the block-MH update once for each of the four independent test cases below. For each test case, you are given the current parameters $(\\xi_p,\\xi_w)$, the $2 \\times 2$ proposal covariance matrix $S$, the standard normal vector $z$, and the uniform comparator $u$.\n\n- Case 1 (happy path, strongly negative correlation):\n  - Current $(\\xi_p,\\xi_w) = (\\,0.85,\\;0.80\\,)$.\n  - Proposal covariance $S = \\begin{bmatrix} 0.0625 & -0.05625 \\\\ -0.05625 & 0.0625 \\end{bmatrix}$.\n  - Standard normal $z = (\\,0.5,\\;-1.0\\,)$.\n  - Comparator $u = 0.3$.\n\n- Case 2 (boundary proximity in one dimension):\n  - Current $(\\xi_p,\\xi_w) = (\\,0.98,\\;0.05\\,)$.\n  - Proposal covariance $S = \\begin{bmatrix} 0.0225 & -0.02025 \\\\ -0.02025 & 0.0225 \\end{bmatrix}$.\n  - Standard normal $z = (\\,1.0,\\;0.5\\,)$.\n  - Comparator $u = 0.6$.\n\n- Case 3 (uncorrelated small step):\n  - Current $(\\xi_p,\\xi_w) = (\\,0.60,\\;0.60\\,)$.\n  - Proposal covariance $S = \\begin{bmatrix} 0.0016 & 0 \\\\ 0 & 0.0016 \\end{bmatrix}$.\n  - Standard normal $z = (\\, -0.2,\\;0.3\\,)$.\n  - Comparator $u = 0.1$.\n\n- Case 4 (anisotropic, very strong negative correlation):\n  - Current $(\\xi_p,\\xi_w) = (\\,0.90,\\;0.70\\,)$.\n  - Proposal covariance $S = \\begin{bmatrix} 0.1225 & -0.0665 \\\\ -0.0665 & 0.04 \\end{bmatrix}$.\n  - Standard normal $z = (\\,0.3,\\;-0.4\\,)$.\n  - Comparator $u = 0.4$.\n\nImplementation details and output requirements:\n- Use the logit and inverse-logit transforms exactly as specified.\n- Use the Cholesky factorization of $S$ to construct the proposal increment $L z$ for each case.\n- Compute the log-likelihood using the model equations exactly as stated:\n  - For $t = 1,\\dots,T$, with $y_0 = 0$, add\n    $-\\frac{1}{2} \\frac{(y_t - \\rho(\\theta) y_{t-1})^2}{\\sigma_y^2}$ and\n    $-\\frac{1}{2} \\frac{(\\pi_t - \\alpha(\\theta) y_t)^2}{\\sigma_\\pi^2}$.\n- For the log-prior, use $\\psi \\sim \\mathcal{N}((0,0), I_2)$ and drop constants common to both current and proposed states, i.e., add $-\\frac{1}{2} \\lVert \\psi \\rVert^2$.\n- For each case, compute the acceptance indicator, defined as $1$ if accepted and $0$ if rejected.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases, for example $[1,0,1,1]$.\n\nThere are no physical units or angles in this problem. All returned values are integers ($0$ or $1$).", "solution": "The problem presented requires the implementation of a single, blocked Metropolis-Hastings (MH) update for two structural parameters of a stylized Dynamic Stochastic General Equilibrium (DSGE) model. The problem is scientifically grounded, mathematically well-posed, and provides all necessary components for a deterministic computation. It is therefore valid. We now proceed with the solution.\n\nThe objective is to decide whether to accept or reject a proposed new state for the parameter vector $\\theta = (\\xi_p, \\xi_w)$, which represents price and wage stickiness, respectively. The MH algorithm provides a procedure for sampling from a target probability distribution, which in a Bayesian context is the posterior distribution $p(\\theta \\mid Y)$.\n\nFirst, we address the parameter constraints. The parameters $\\xi_p$ and $\\xi_w$ are defined on the open interval $(0, 1)$. To perform an unconstrained optimization or sampling, it is standard practice to transform them into an unconstrained space $\\mathbb{R}^2$. The problem specifies the logit transformation:\n$$\n\\psi_j = \\log\\left(\\frac{\\xi_j}{1-\\xi_j}\\right) \\quad \\text{for } j \\in \\{p, w\\}\n$$\nThe vector $\\psi = (\\psi_p, \\psi_w)$ now resides in $\\mathbb{R}^2$. The inverse transformation, which maps from the unconstrained space back to the constrained space, is the logistic (or inverse-logit) function:\n$$\n\\xi_j = \\frac{1}{1 + e^{-\\psi_j}}\n$$\nWe will conduct the MH update in the $\\psi$-space.\n\nThe core of the MH algorithm lies in the acceptance probability, which depends on the ratio of the target density at the proposed state to that at the current state. In our Bayesian framework, the target density is the posterior density of the parameters, $p(\\psi \\mid Y)$. By Bayes' theorem, this is proportional to the product of the likelihood and the prior:\n$$\np(\\psi \\mid Y) \\propto p(Y \\mid \\theta(\\psi)) p(\\psi)\n$$\nHere, $p(Y \\mid \\theta(\\psi))$ is the likelihood of the observed data $Y$ given the parameters, and $p(\\psi)$ is the prior distribution over the transformed parameters. The problem states that the prior on $\\psi$ is a standard bivariate normal distribution, $\\psi \\sim \\mathcal{N}(0, I_2)$, where $I_2$ is the $2 \\times 2$ identity matrix. Because the prior is specified directly on $\\psi$, no Jacobian determinant from the change of variables is required in the posterior density expression.\n\nIt is computationally more stable and convenient to work with the logarithm of the posterior density:\n$$\n\\log p(\\psi \\mid Y) = \\log p(Y \\mid \\theta(\\psi)) + \\log p(\\psi) + C\n$$\nwhere $C$ is a normalizing constant that does not depend on $\\psi$ and can be ignored, as it cancels out in the MH acceptance ratio.\n\nThe log-prior density, ignoring constants, is:\n$$\n\\log p(\\psi) \\propto -\\frac{1}{2} \\psi^\\top (I_2)^{-1} \\psi = -\\frac{1}{2} (\\psi_p^2 + \\psi_w^2) = -\\frac{1}{2} \\lVert \\psi \\rVert^2\n$$\n\nThe log-likelihood function, $\\log p(Y \\mid \\theta(\\psi))$, is derived from the specified linear Gaussian state-space model. The data consist of time series for output gap $\\{y_t\\}_{t=1}^T$ and inflation $\\{\\pi_t\\}_{t=1}^T$. The model is:\n$$\ny_t \\mid y_{t-1}, \\theta \\sim \\mathcal{N}(\\rho(\\theta)\\, y_{t-1}, \\sigma_y^2)\n$$\n$$\n\\pi_t \\mid y_t, \\theta \\sim \\mathcal{N}(\\alpha(\\theta)\\, y_t, \\sigma_\\pi^2)\n$$\nThe coefficients $\\rho(\\theta)$ and $\\alpha(\\theta)$ depend on the structural parameters $\\theta = (\\xi_p, \\xi_w)$ via the intermediate variable $b(\\xi_p, \\xi_w) = (1 - \\xi_p)(1 - \\xi_w)$:\n$$\n\\rho(\\theta) = 0.6 - 0.5 \\cdot b(\\xi_p, \\xi_w)\n$$\n$$\n\\alpha(\\theta) = 0.5 \\cdot b(\\xi_p, \\xi_w)\n$$\nAssuming conditional independence of the innovations, the total log-likelihood is the sum of the individual log-densities over the time horizon $T=12$. Again, dropping terms that are constant with respect to $\\theta$:\n$$\n\\log p(Y \\mid \\theta) \\propto \\sum_{t=1}^{T} \\left[ -\\frac{(y_t - \\rho(\\theta) y_{t-1})^2}{2\\sigma_y^2} - \\frac{(\\pi_t - \\alpha(\\theta) y_t)^2}{2\\sigma_\\pi^2} \\right]\n$$\nThe initial condition is $y_0 = 0$. The data series for $\\{y_t\\}_{t=1}^{12}$ and $\\{\\pi_t\\}_{t=1}^{12}$, and the standard deviations $\\sigma_y=0.05$ and $\\sigma_\\pi=0.02$, are given.\n\nThe blocked MH update proceeds as follows for each test case:\n1.  **Initialization**: Start with the current parameters in the constrained space, $\\theta_{\\text{curr}} = (\\xi_{p, \\text{curr}}, \\xi_{w, \\text{curr}})$.\n2.  **Transformation**: Convert $\\theta_{\\text{curr}}$ to the unconstrained space: $\\psi_{\\text{curr}} = (\\text{logit}(\\xi_{p, \\text{curr}}), \\text{logit}(\\xi_{w, \\text{curr}}))$.\n3.  **Proposal Generation**: Generate a proposal $\\psi_{\\text{prop}}$ using a symmetric random walk:\n    $$\n    \\psi_{\\text{prop}} = \\psi_{\\text{curr}} + L z\n    $$\n    Here, $S$ is the proposal covariance matrix, $L$ is its lower-triangular Cholesky factor ($S = LL^\\top$), and $z \\sim \\mathcal{N}(0, I_2)$ is a vector of standard normal random variates. For reproducibility, a specific vector $z$ is provided for each case.\n4.  **Density Evaluation**: Compute the unnormalized log-posterior density for both the current state, $\\log \\pi(\\psi_{\\text{curr}} \\mid Y)$, and the proposed state, $\\log \\pi(\\psi_{\\text{prop}} \\mid Y)$, by summing their respective log-prior and log-likelihood values.\n5.  **Acceptance Decision**: The proposal is symmetric, so the Hastings correction term is $1$. The acceptance probability $\\alpha$ is:\n    $$\n    \\alpha = \\min\\left(1, \\frac{p(\\psi_{\\text{prop}} \\mid Y)}{p(\\psi_{\\text{curr}} \\mid Y)}\\right) = \\min\\left(1, \\exp\\left[\\log p(\\psi_{\\text{prop}} \\mid Y) - \\log p(\\psi_{\\text{curr}} \\mid Y)\\right]\\right)\n    $$\n    The proposal is accepted if a given uniform random variate $u \\in (0,1)$ satisfies $u < \\alpha$. The result for each case is an indicator variable: $1$ if accepted, $0$ if rejected.\n\nThis procedure is implemented for each of the four specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a single blocked Metropolis-Hastings update for four test cases\n    in a simplified DSGE model context.\n    \"\"\"\n    \n    # --- Data and Model Constants ---\n    T = 12\n    y0 = 0.0\n    y_data = np.array([0.050, 0.041, 0.028, 0.022, 0.018, 0.015, 0.013, 0.012, 0.010, 0.007, 0.005, 0.004])\n    pi_data = np.array([0.020, 0.018, 0.017, 0.016, 0.015, 0.013, 0.012, 0.011, 0.010, 0.0090, 0.0085, 0.0080])\n    sigma_y = 0.05\n    sigma_pi = 0.02\n    \n    # --- Test Cases ---\n    test_cases = [\n        {\n            \"xi_curr\": np.array([0.85, 0.80]),\n            \"S\": np.array([[0.0625, -0.05625], [-0.05625, 0.0625]]),\n            \"z\": np.array([0.5, -1.0]),\n            \"u\": 0.3\n        },\n        {\n            \"xi_curr\": np.array([0.98, 0.05]),\n            \"S\": np.array([[0.0225, -0.02025], [-0.02025, 0.0225]]),\n            \"z\": np.array([1.0, 0.5]),\n            \"u\": 0.6\n        },\n        {\n            \"xi_curr\": np.array([0.60, 0.60]),\n            \"S\": np.array([[0.0016, 0.0], [0.0, 0.0016]]),\n            \"z\": np.array([-0.2, 0.3]),\n            \"u\": 0.1\n        },\n        {\n            \"xi_curr\": np.array([0.90, 0.70]),\n            \"S\": np.array([[0.1225, -0.0665], [-0.0665, 0.04]]),\n            \"z\": np.array([0.3, -0.4]),\n            \"u\": 0.4\n        }\n    ]\n\n    # --- Helper Functions ---\n    def logit(xi):\n        return np.log(xi / (1.0 - xi))\n\n    def inv_logit(psi):\n        return 1.0 / (1.0 + np.exp(-psi))\n\n    def calculate_log_posterior(psi):\n        # 1. Transform back to constrained parameter space\n        xi = inv_logit(psi)\n        xi_p, xi_w = xi[0], xi[1]\n        \n        # Check for invalid xi values that might arise from extreme psi\n        # The problem statement ensures xi is in (0,1)\n        if not (0 < xi_p < 1 and 0 < xi_w < 1):\n            return -np.inf # Log posterior is -infinity if parameters are out of bounds\n\n        # 2. Calculate log-prior density (up to a constant)\n        # Prior psi ~ N(0, I)\n        log_prior = -0.5 * np.sum(psi**2)\n\n        # 3. Calculate log-likelihood (up to a constant)\n        b = (1.0 - xi_p) * (1.0 - xi_w)\n        rho = 0.6 - 0.5 * b\n        alpha_param = 0.5 * b\n        \n        log_likelihood = 0.0\n        y_prev = y0\n        \n        for t in range(T):\n            # State equation likelihood term\n            log_likelihood -= 0.5 * ((y_data[t] - rho * y_prev)**2) / (sigma_y**2)\n            # Measurement equation likelihood term\n            log_likelihood -= 0.5 * ((pi_data[t] - alpha_param * y_data[t])**2) / (sigma_pi**2)\n            y_prev = y_data[t]\n\n        return log_prior + log_likelihood\n\n    results = []\n    for case in test_cases:\n        xi_curr = case[\"xi_curr\"]\n        S = case[\"S\"]\n        z = case[\"z\"]\n        u = case[\"u\"]\n\n        # 1. Transform current parameters to unconstrained space\n        psi_curr = logit(xi_curr)\n        \n        # 2. Calculate log posterior of the current state\n        log_post_curr = calculate_log_posterior(psi_curr)\n\n        # 3. Generate proposal\n        try:\n            L = np.linalg.cholesky(S)\n        except np.linalg.LinAlgError:\n            # Handle cases where S is not positive-definite, although problem states they are\n            results.append(0) # Reject if proposal mechanism fails\n            continue\n            \n        psi_prop = psi_curr + L @ z\n        \n        # 4. Calculate log posterior of the proposed state\n        log_post_prop = calculate_log_posterior(psi_prop)\n\n        # 5. Acceptance decision\n        log_r = log_post_prop - log_post_curr\n        \n        # Avoid overflow/underflow issues, though unlikely here\n        if np.isneginf(log_r):\n            acceptance_prob = 0.0\n        else:\n            acceptance_prob = min(1.0, np.exp(log_r))\n\n        accepted = 1 if u < acceptance_prob else 0\n        results.append(accepted)\n\n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "After running thousands of MCMC iterations, your very first question should be: \"Did the sampler work correctly?\" This practice introduces a fundamental diagnostic check to help answer that question [@problem_id:2375892]. You will write code to monitor the sampler's acceptance rate—a key indicator of its efficiency—and automatically flag chains that are exploring the parameter space too slowly or too timidly, a non-negotiable step for ensuring your results are reliable.", "id": "2375892", "problem": "In Bayesian estimation of Dynamic Stochastic General Equilibrium (DSGE) models, Markov Chain Monte Carlo (MCMC) sampling is used to approximate the posterior distribution of model parameters. Each MCMC step either accepts a proposed move or rejects it. Let the acceptance indicator at iteration $t$ be $A_t \\in \\{0,1\\}$, where $A_t = 1$ denotes acceptance and $A_t = 0$ denotes rejection. The long-run mean acceptance probability is $\\alpha = \\mathbb{E}[A_t]$. Under stationarity after burn-in, the sample acceptance rate computed from usable draws is the sample mean of a sequence of Bernoulli random variables, which is an unbiased estimator of $\\alpha$.\n\nYou are to write a program that, for each chain in a fixed test suite, computes the post-burn-in acceptance rate and flags the chain if the rate is below $0.10$ or above $0.50$. Formally, for a chain with total iterations $T$, burn-in length $T_b$ (nonnegative integer), and acceptance indicators $\\{A_1,\\dots,A_T\\}$, define the number of usable draws as $n = T - T_b$. If $n \\le 0$, the chain must be flagged. Otherwise, compute the sample acceptance rate\n$$\n\\hat{\\alpha} \\;=\\; \\frac{1}{n}\\sum_{t=T_b+1}^{T} A_t.\n$$\nFlag the chain if $\\hat{\\alpha} < 0.10$ or $\\hat{\\alpha} > 0.50$. Equality at the boundaries $0.10$ and $0.50$ must not be flagged.\n\nYour program must hard-code and evaluate the following test suite of chains. Each chain is specified by its burn-in length and its full acceptance indicator sequence $\\{A_t\\}_{t=1}^T$:\n- Chain $1$: $T=20$, $T_b=5$, $\\{A_t\\} = [$ $0$, $1$, $0$, $0$, $1$, $1$, $0$, $0$, $1$, $0$, $0$, $0$, $1$, $0$, $0$, $1$, $0$, $0$, $1$, $0$ $]$.\n- Chain $2$: $T=20$, $T_b=0$, $\\{A_t\\} = [$ $1$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$ $]$.\n- Chain $3$: $T=10$, $T_b=2$, $\\{A_t\\} = [$ $0$, $1$, $1$, $1$, $0$, $1$, $1$, $0$, $1$, $1$ $]$.\n- Chain $4$: $T=10$, $T_b=0$, $\\{A_t\\} = [$ $1$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$, $0$ $]$.\n- Chain $5$: $T=8$, $T_b=0$, $\\{A_t\\} = [$ $1$, $0$, $1$, $0$, $1$, $0$, $1$, $0$ $]$.\n- Chain $6$: $T=5$, $T_b=5$, $\\{A_t\\} = [$ $0$, $1$, $0$, $1$, $0$ $]$.\n\nDesign for coverage:\n- Chain $1$ represents a typical case with acceptance rate between $0.10$ and $0.50$ after burn-in.\n- Chain $2$ is a low-acceptance case below $0.10$.\n- Chain $3$ is a high-acceptance case above $0.50$.\n- Chain $4$ is a boundary case exactly $0.10$.\n- Chain $5$ is a boundary case exactly $0.50$.\n- Chain $6$ has zero usable draws ($n = 0$).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of chains $1$ through $6$, with boolean values indicating whether each chain is flagged. For example, an output with six chains should look like [$\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5,\\text{result}_6$]. No physical units apply in this problem. All thresholds are decimals (not percentages), and angles are not involved.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a standard diagnostic task in the context of Markov Chain Monte Carlo (MCMC) methods used in computational economics. The definitions, formulae, and test data are complete, consistent, and allow for a unique, verifiable solution.\n\nThe objective is to analyze a set of MCMC acceptance indicator sequences, $\\{A_t\\}$, to determine if they should be flagged based on their post-burn-in acceptance rate. The procedure is deterministic and follows a clear set of rules.\n\nFor each chain with total iterations $T$ and a burn-in period of $T_b$, we first calculate the number of usable (post-burn-in) draws, $n$, using the formula:\n$$\nn = T - T_b\n$$\nThe first rule for flagging a chain is if the number of usable draws is non-positive, i.e., if $n \\le 0$.\n\nIf $n > 0$, we proceed to calculate the sample acceptance rate, $\\hat{\\alpha}$. This rate is the mean of the acceptance indicators over the usable draws, given by the formula:\n$$\n\\hat{\\alpha} = \\frac{1}{n}\\sum_{t=T_b+1}^{T} A_t\n$$\nThe second set of flagging rules applies to this rate. A chain is flagged if its sample acceptance rate is outside the recommended range of $[0.10, 0.50]$. Specifically, a chain is flagged if $\\hat{\\alpha} < 0.10$ or $\\hat{\\alpha} > 0.50$. The boundary values of $0.10$ and $0.50$ are considered acceptable and do not lead to a flag.\n\nWe now apply this procedure to each of the six specified chains.\n\nFor Chain $1$:\n- Parameters: $T = 20$, $T_b = 5$.\n- Number of usable draws: $n = 20 - 5 = 15$. Since $n > 0$, we proceed.\n- The post-burn-in sequence is $\\{A_t\\}_{t=6}^{20}$. The sum of acceptances is $\\sum_{t=6}^{20} A_t = 1+0+0+1+0+0+0+1+0+0+1+0+0+1+0 = 5$.\n- Sample acceptance rate: $\\hat{\\alpha} = \\frac{5}{15} = \\frac{1}{3} \\approx 0.333$.\n- Condition check: $0.10 < 0.333 < 0.50$. The rate is within the acceptable range.\n- Verdict: Not Flagged.\n\nFor Chain $2$:\n- Parameters: $T = 20$, $T_b = 0$.\n- Number of usable draws: $n = 20 - 0 = 20$. Since $n > 0$, we proceed.\n- The post-burn-in sequence is $\\{A_t\\}_{t=1}^{20}$. The sum of a acceptances is $\\sum_{t=1}^{20} A_t = 1$.\n- Sample acceptance rate: $\\hat{\\alpha} = \\frac{1}{20} = 0.05$.\n- Condition check: $\\hat{\\alpha} < 0.10$. The rate is too low.\n- Verdict: Flagged.\n\nFor Chain $3$:\n- Parameters: $T = 10$, $T_b = 2$.\n- Number of usable draws: $n = 10 - 2 = 8$. Since $n > 0$, we proceed.\n- The post-burn-in sequence is $\\{A_t\\}_{t=3}^{10}$. The sum of acceptances is $\\sum_{t=3}^{10} A_t = 1+1+0+1+1+0+1+1 = 6$.\n- Sample acceptance rate: $\\hat{\\alpha} = \\frac{6}{8} = 0.75$.\n- Condition check: $\\hat{\\alpha} > 0.50$. The rate is too high.\n- Verdict: Flagged.\n\nFor Chain $4$:\n- Parameters: $T = 10$, $T_b = 0$.\n- Number of usable draws: $n = 10 - 0 = 10$. Since $n > 0$, we proceed.\n- The post-burn-in sequence is $\\{A_t\\}_{t=1}^{10}$. The sum of acceptances is $\\sum_{t=1}^{10} A_t = 1$.\n- Sample acceptance rate: $\\hat{\\alpha} = \\frac{1}{10} = 0.10$.\n- Condition check: The rate is exactly on the lower boundary. As per the rules, equality does not trigger a flag.\n- Verdict: Not Flagged.\n\nFor Chain $5$:\n- Parameters: $T = 8$, $T_b = 0$.\n- Number of usable draws: $n = 8 - 0 = 8$. Since $n > 0$, we proceed.\n- The post-burn-in sequence is $\\{A_t\\}_{t=1}^{8}$. The sum of acceptances is $\\sum_{t=1}^{8} A_t = 1+0+1+0+1+0+1+0 = 4$.\n- Sample acceptance rate: $\\hat{\\alpha} = \\frac{4}{8} = 0.50$.\n- Condition check: The rate is exactly on the upper boundary. As per the rules, equality does not trigger a flag.\n- Verdict: Not Flagged.\n\nFor Chain $6$:\n- Parameters: $T = 5$, $T_b = 5$.\n- Number of usable draws: $n = 5 - 5 = 0$.\n- Condition check: Since $n \\le 0$, the chain is flagged immediately, without calculating a rate.\n- Verdict: Flagged.\n\nIn summary, the flagging results for the chains are as follows: Chain $1$ (Not Flagged), Chain $2$ (Flagged), Chain $3$ (Flagged), Chain $4$ (Not Flagged), Chain $5$ (Not Flagged), and Chain $6$ (Flagged).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes post-burn-in acceptance rates for a suite of MCMC chains\n    and flags them according to specified criteria.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (burn-in_length, acceptance_indicator_sequence)\n    test_cases = [\n        (5, [0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0]),\n        (0, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        (2, [0, 1, 1, 1, 0, 1, 1, 0, 1, 1]),\n        (0, [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n        (0, [1, 0, 1, 0, 1, 0, 1, 0]),\n        (5, [0, 1, 0, 1, 0])\n    ]\n\n    results = []\n    # Lower and upper bounds for the acceptance rate\n    LOWER_BOUND = 0.10\n    UPPER_BOUND = 0.50\n\n    for tb, at_sequence in test_cases:\n        # Total number of iterations\n        t_total = len(at_sequence)\n        \n        # Number of usable draws\n        n_usable = t_total - tb\n        \n        flagged = False\n        if n_usable <= 0:\n            # Flag if there are no usable draws\n            flagged = True\n        else:\n            # Extract the post-burn-in sequence\n            post_burn_in_a = at_sequence[tb:]\n            \n            # Count the number of acceptances\n            num_acceptances = sum(post_burn_in_a)\n            \n            # Compute the sample acceptance rate\n            # In Python 3, '/' performs float division by default.\n            acceptance_rate = num_acceptances / n_usable\n            \n            # Check if the rate is outside the acceptable range\n            # Equality with the boundaries is not flagged.\n            if acceptance_rate < LOWER_BOUND or acceptance_rate > UPPER_BOUND:\n                flagged = True\n        \n        results.append(flagged)\n\n    # Final print statement in the exact required format.\n    # The str() function for booleans returns 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "The output of a Bayesian estimation is not just a single point estimate, but an entire posterior distribution for each parameter. The real value of this rich output lies in using it to answer concrete economic questions [@problem_id:2375884]. This final practice shows you how to do just that, by taking the posterior draws of a shock's persistence parameter, $\\rho$, and transforming them to characterize an economically intuitive concept: the shock's half-life. This skill of constructing credible sets for derived quantities is central to turning model estimates into meaningful economic analysis.", "id": "2375884", "problem": "You are given a linearized Dynamic Stochastic General Equilibrium (DSGE) model in which the log productivity process follows a first-order autoregression, $a_t = \\rho a_{t-1} + \\varepsilon_t$, with $|\\rho| < 1$ and $\\varepsilon_t$ an independently and identically distributed innovation. Suppose a Markov Chain Monte Carlo (MCMC) procedure (Markov Chain Monte Carlo (MCMC)) has produced posterior draws for the persistence parameter $\\rho$. For a given value of $\\rho$ with $0 < \\rho < 1$, define the half-life $h$ of the productivity shock as the unique number $h > 0$ such that the ratio of the covariance at lag $h$ to the variance at lag $0$ equals $1/2$, that is, the unique $h$ satisfying\n$$\n\\frac{\\mathbb{E}[a_t a_{t+h}]}{\\mathbb{E}[a_t^2]} = \\frac{1}{2}.\n$$\nOnly draws with $0 < \\rho < 1$ are admissible for computing the half-life. Any draw with $\\rho \\le 0$ or $\\rho \\ge 1$ must be excluded from the calculation.\n\nFor each of the following three test cases, treat the given finite set as the entire MCMC output for $\\rho$ and compute the equal-tailed $0.90$ credible set for the half-life $h$ in model periods. The equal-tailed $0.90$ credible set is the closed interval whose lower endpoint is the $0.05$ posterior quantile of $h$ and whose upper endpoint is the $0.95$ posterior quantile of $h$. Report each endpoint as a real number rounded to six decimal places.\n\nTest suite (each set is a complete collection of posterior draws for $\\rho$):\n- Case $1$: $\\{\\,0.83,\\,0.85,\\,0.86,\\,0.87,\\,0.88,\\,0.89,\\,0.90,\\,0.91,\\,0.92,\\,0.93,\\,0.94,\\,0.95,\\,0.96,\\,0.97\\,\\}$.\n- Case $2$: $\\{\\,0.50,\\,0.55,\\,0.60,\\,0.65,\\,0.70,\\,0.75,\\,0.80\\,\\}$.\n- Case $3$: $\\{\\,{-0.20},\\,0.0001,\\,0.30,\\,0.60,\\,0.80,\\,0.95,\\,0.99,\\,0.999,\\,1.05\\,\\}$.\n\nYour program should produce a single line of output containing the three credible sets in order, each as a two-element list $[L,U]$ with $L$ the lower endpoint and $U$ the upper endpoint, all rounded to six decimals, aggregated as a comma-separated list enclosed in square brackets. For example, the required output format is\n[ [L1,U1], [L2,U2], [L3,U3] ]\nbut without spaces other than those implicitly produced by list delimiters; concretely, print exactly a single line of the form\n\"[[L1,U1],[L2,U2],[L3,U3]]\".\nAll half-life values are to be expressed in model periods as real numbers rounded to six decimal places.", "solution": "The problem statement has been rigorously validated and is determined to be scientifically grounded, well-posed, and objective. It presents a standard task in Bayesian econometrics that is free of any logical or factual inconsistencies. We may therefore proceed with the derivation of a solution.\n\nThe problem requires the computation of a $0.90$ equal-tailed credible set for the half-life $h$ of a productivity shock, based on a posterior sample of the persistence parameter $\\rho$ from a first-order autoregressive process, $a_t = \\rho a_{t-1} + \\varepsilon_t$.\n\nFirst, we must establish the functional relationship between the half-life $h$ and the persistence parameter $\\rho$. The process is assumed to be stationary, which is guaranteed by the condition $|\\rho| < 1$. The innovations $\\varepsilon_t$ are i.i.d. with $\\mathbb{E}[\\varepsilon_t] = 0$ and $\\mathbb{E}[\\varepsilon_t^2] = \\sigma_\\varepsilon^2$.\n\nThe variance of the process $a_t$, denoted $\\text{Var}(a_t) = \\mathbb{E}[a_t^2] = \\gamma_0$, is constant over time due to stationarity. We derive it as follows:\n$$\n\\text{Var}(a_t) = \\text{Var}(\\rho a_{t-1} + \\varepsilon_t)\n$$\nAs $a_{t-1}$ is determined by past innovations $\\{\\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots\\}$, it is uncorrelated with the current innovation $\\varepsilon_t$. Therefore:\n$$\n\\text{Var}(a_t) = \\rho^2 \\text{Var}(a_{t-1}) + \\text{Var}(\\varepsilon_t)\n$$\n$$\n\\gamma_0 = \\rho^2 \\gamma_0 + \\sigma_\\varepsilon^2\n$$\nSolving for $\\gamma_0$ yields the unconditional variance:\n$$\n\\gamma_0 = \\mathbb{E}[a_t^2] = \\frac{\\sigma_\\varepsilon^2}{1 - \\rho^2}\n$$\nNext, we find the autocovariance at lag $h$, defined as $\\gamma_h = \\mathbb{E}[a_t a_{t+h}]$. By iterating the AR($1$) process forward $h$ steps, we can express $a_{t+h}$ in terms of $a_t$ and subsequent shocks:\n$$\na_{t+h} = \\rho^h a_t + \\sum_{j=0}^{h-1} \\rho^j \\varepsilon_{t+h-j}\n$$\nThe autocovariance is then:\n$$\n\\mathbb{E}[a_t a_{t+h}] = \\mathbb{E}\\left[ a_t \\left( \\rho^h a_t + \\sum_{j=0}^{h-1} \\rho^j \\varepsilon_{t+h-j} \\right) \\right] = \\rho^h \\mathbb{E}[a_t^2] + \\mathbb{E}\\left[ a_t \\sum_{j=0}^{h-1} \\rho^j \\varepsilon_{t+h-j} \\right]\n$$\nThe second term is zero because $\\mathbb{E}[a_t \\varepsilon_{t+k}] = 0$ for any $k > 0$. Thus, the autocovariance function is:\n$$\n\\gamma_h = \\mathbb{E}[a_t a_{t+h}] = \\rho^h \\mathbb{E}[a_t^2] = \\rho^h \\gamma_0\n$$\nThe half-life $h$ is defined as the lag at which the autocorrelation function $\\frac{\\gamma_h}{\\gamma_0}$ equals $\\frac{1}{2}$:\n$$\n\\frac{\\mathbb{E}[a_t a_{t+h}]}{\\mathbb{E}[a_t^2]} = \\frac{\\rho^h \\gamma_0}{\\gamma_0} = \\rho^h = \\frac{1}{2}\n$$\nTo solve for $h$, we take the natural logarithm. The problem restricts the admissible draws to $0 < \\rho < 1$, which ensures that $\\ln(\\rho)$ is well-defined and negative.\n$$\n\\ln(\\rho^h) = \\ln\\left(\\frac{1}{2}\\right)\n$$\n$$\nh \\ln(\\rho) = -\\ln(2)\n$$\n$$\nh(\\rho) = -\\frac{\\ln(2)}{\\ln(\\rho)} = \\frac{\\ln(2)}{\\ln(1/\\rho)}\n$$\nThis equation provides the deterministic transformation from a given persistence parameter $\\rho$ to its corresponding half-life $h$.\n\nThe computational procedure to find the $0.90$ credible set for $h$ is as follows:\n1.  For each test case, collect the posterior draws for $\\rho$.\n2.  Filter the draws, retaining only those that satisfy the admissibility condition $0 < \\rho < 1$.\n3.  For each valid draw $\\rho_i$, compute the corresponding half-life $h_i = h(\\rho_i)$. This creates a posterior sample for $h$.\n4.  Sort the sample of half-life values: $h_{(1)} \\le h_{(2)} \\le \\dots \\le h_{(N)}$, where $N$ is the number of valid draws.\n5.  Compute the $0.05$ and $0.95$ quantiles of this sorted sample to find the lower endpoint $L$ and upper endpoint $U$ of the credible set. We use the standard linear interpolation method for quantiles. For a sample of size $N$, the index for a quantile $q$ is calculated as $i = q(N-1)$. The quantile value is then linearly interpolated between the data points at the indices $\\lfloor i \\rfloor$ and $\\lceil i \\rceil$.\n6.  Report the interval $[L, U]$, with both endpoints rounded to six decimal places.\n\nWe now apply this procedure to the three specified test cases.\n\n**Case 1:**\nThe set of posterior draws is $\\{\\,0.83,\\,0.85,\\,0.86,\\,0.87,\\,0.88,\\,0.89,\\,0.90,\\,0.91,\\,0.92,\\,0.93,\\,0.94,\\,0.95,\\,0.96,\\,0.97\\,\\}$.\nAll $N=14$ draws are within the valid range $(0, 1)$. We transform each $\\rho_i$ to $h_i$. The (already sorted) sample of $h$ values is:\n$\\{3.719917, 4.265104, 4.591418, 4.957243, 5.369516, 5.836894, 6.369335, 6.979596, 7.684634, 8.506669, 11.205601, 13.513406, 16.979511, 22.756834\\}$.\nThe sample size is $N=14$.\nFor the lower bound $L$ (quantile $q=0.05$): Index $i = 0.05 \\times (14-1) = 0.65$. The value is interpolated between the 1st and 2nd elements ($h_{(1)}$ and $h_{(2)}$).\n$L = (1-0.65)h_{(1)} + 0.65h_{(2)} = 0.35(3.719917) + 0.65(4.265104) = 4.074289$.\nFor the upper bound $U$ (quantile $q=0.95$): Index $i = 0.95 \\times (14-1) = 12.35$. The value is interpolated between the 13th and 14th elements ($h_{(13)}$ and $h_{(14)}$).\n$U = (1-0.35)h_{(13)} + 0.35h_{(14)} = 0.65(16.979511) + 0.35(22.756834) = 19.001574$.\nThe credible set is $[4.074289, 19.001574]$.\n\n**Case 2:**\nThe set of posterior draws is $\\{\\,0.50,\\,0.55,\\,0.60,\\,0.65,\\,0.70,\\,0.75,\\,0.80\\,\\}$.\nAll $N=7$ draws are valid. The transformed sample of $h$ values is:\n$\\{1.000000, 1.158504, 1.356919, 1.609438, 1.943360, 2.409421, 3.106278\\}$.\nThe sample size is $N=7$.\nFor $L$ (quantile $q=0.05$): Index $i = 0.05 \\times (7-1) = 0.3$.\n$L = (1-0.3)h_{(1)} + 0.3h_{(2)} = 0.7(1.000000) + 0.3(1.158504) = 1.047551$.\nFor $U$ (quantile $q=0.95$): Index $i = 0.95 \\times (7-1) = 5.7$.\n$U = (1-0.7)h_{(6)} + 0.7h_{(7)} = 0.3(2.409421) + 0.7(3.106278) = 2.897221$.\nThe credible set is $[1.047551, 2.897221]$.\n\n**Case 3:**\nThe set of posterior draws is $\\{\\,{-0.20},\\,0.0001,\\,0.30,\\,0.60,\\,0.80,\\,0.95,\\,0.99,\\,0.999,\\,1.05\\,\\}$.\nWe must filter these draws. Draws $\\rho = -0.20$ (since $\\rho \\le 0$) and $\\rho = 1.05$ (since $\\rho \\ge 1$) are inadmissible and must be excluded.\nThe valid draws are $\\{\\,0.0001,\\,0.30,\\,0.60,\\,0.80,\\,0.95,\\,0.99,\\,0.999\\,\\}$.\nThe number of valid draws is $N=7$. The transformed sample of $h$ values is:\n$\\{0.075257, 0.575716, 1.356919, 3.106278, 13.513406, 68.967564, 692.800490\\}$.\nThe sample size is $N=7$.\nFor $L$ (quantile $q=0.05$): Index $i = 0.05 \\times (7-1) = 0.3$.\n$L = (1-0.3)h_{(1)} + 0.3h_{(2)} = 0.7(0.075257) + 0.3(0.575716) = 0.225395$.\nFor $U$ (quantile $q=0.95$): Index $i = 0.95 \\times (7-1) = 5.7$.\n$U = (1-0.7)h_{(6)} + 0.7h_{(7)} = 0.3(68.967564) + 0.7(692.800490) = 505.650612$.\nThe credible set is $[0.225395, 505.650612]$.\n\nThe final results, rounded to six decimal places, are:\n- Case 1: $[4.074289, 19.001574]$\n- Case 2: $[1.047551, 2.897221]$\n- Case 3: $[0.225395, 505.650612]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the 0.90 equal-tailed credible set for the half-life h\n    of an AR(1) process based on MCMC draws for the persistence parameter rho.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        [0.83, 0.85, 0.86, 0.87, 0.88, 0.89, 0.90, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97],\n        # Case 2\n        [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80],\n        # Case 3\n        [-0.20, 0.0001, 0.30, 0.60, 0.80, 0.95, 0.99, 0.999, 1.05],\n    ]\n\n    results = []\n    \n    # Pre-calculated constant for efficiency\n    ln2 = np.log(2)\n\n    for case_rhos in test_cases:\n        # Convert to numpy array for vectorized operations\n        rhos = np.array(case_rhos)\n        \n        # 1. Filter: Retain only admissible draws where 0 < rho < 1\n        valid_rhos = rhos[(rhos > 0) & (rhos < 1)]\n        \n        # 2. Transform: Compute half-life for each valid rho.\n        # The formula is h = ln(2) / -ln(rho)\n        half_lives = ln2 / -np.log(valid_rhos)\n        \n        # The MCMC draws are not necessarily sorted, but the rho values provided\n        # in the problem are, which means the half_lives will also be sorted\n        # as the transformation is monotonic. np.quantile does not require\n        # pre-sorting, but it is a good practice to be aware of.\n        \n        # 3. Quantiles: Compute the 0.05 and 0.95 quantiles to get the\n        # equal-tailed 0.90 credible set.\n        # The 'linear' interpolation method is the standard and default.\n        quantiles = np.quantile(half_lives, [0.05, 0.95], interpolation='linear')\n        \n        # 4. Format: Round the endpoints to six decimal places.\n        lower_bound = round(quantiles[0], 6)\n        upper_bound = round(quantiles[1], 6)\n        \n        results.append([lower_bound, upper_bound])\n\n    # Final print statement must be in the exact required format: [[L1,U1],[L2,U2],[L3,U3]]\n    # We construct the string manually to avoid extra spaces from standard list-to-string conversion.\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    output_string = f\"[{','.join(formatted_results)}]\"\n    \n    print(output_string)\n\nsolve()\n```"}]}