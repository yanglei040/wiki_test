{"hands_on_practices": [{"introduction": "In quantitative finance, we often need to calculate expectations of random variables, such as asset prices, that are defined over infinite domains and follow non-uniform distributions. This exercise demonstrates how to adapt a standard, highly efficient Gauss-Legendre quadrature rule, defined on the simple interval $[-1, 1]$, to this more complex and realistic scenario. By mastering the change of variables technique shown here, you will be able to apply canonical quadrature methods to a vast range of problems involving log-normal, normal, and other distributions common in financial modeling [@problem_id:2396760].", "id": "2396760", "problem": "Consider a strictly positive random variable $X$ defined by $X=\\exp(\\mu+\\sigma Z)$ where $Z$ is a standard normal random variable, $\\mu\\in\\mathbb{R}$, and $\\sigma&gt;0$. For a measurable function $g:\\mathbb{R}_{+}\\to\\mathbb{R}$, the expectation $\\mathbb{E}[g(X)]$ is given by $\\int_{0}^{\\infty} g(x) f_{X}(x)\\,\\mathrm{d}x$, where $f_{X}$ is the probability density function of the lognormal distribution induced by $(\\mu,\\sigma)$. Your task is to construct a numerical quadrature rule for $\\mathbb{E}[g(X)]$ by transforming a standard Gauss–Legendre quadrature rule on the interval $[-1,1]$ into a rule that integrates with respect to the law of $X$, in a manner that incorporates the change of measure into the transformed nodes and weights.\n\nThe program you write must compute the numerical approximations of $\\mathbb{E}[g(X)]$ for each of the following test cases, using exactly $n=96$ nodes from a standard Gauss–Legendre quadrature rule on $[-1,1]$:\n\n- Test case $1$: $\\mu=0$, $\\sigma=0.25$, $g(x)=x$.\n- Test case $2$: $\\mu=0.1$, $\\sigma=0.5$, $g(x)=x^{2}$.\n- Test case $3$: $\\mu=-0.2$, $\\sigma=0.3$, $g(x)=\\ln(x)$.\n- Test case $4$: $\\mu=0.05$, $\\sigma=0.6$, $g(x)=\\max(x-K,0)$ with $K=\\exp(\\mu)$.\n- Test case $5$: $\\mu=0$, $\\sigma=10^{-6}$, $g(x)=x$.\n\nAll angles, if any, must be in radians. There are no physical units involved in this problem. For numerical reporting, each approximation must be rounded to $12$ decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The order must follow the test cases above, so the required format is\n$[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4},\\text{result}_{5}]$,\nwith each $\\text{result}_{i}$ a floating-point number rounded to $12$ decimal places. No additional text or formatting should be printed.", "solution": "The problem as stated is valid. It is scientifically grounded in the theory of mathematical finance and numerical analysis, specifically concerning the expectation of a function of a log-normally distributed random variable and its computation via Gaussian quadrature. The problem is well-posed, objective, and contains all necessary information to derive a unique solution. We will proceed.\n\nThe objective is to compute the expectation $\\mathbb{E}[g(X)]$ where the random variable $X$ follows a lognormal distribution, defined as $X = \\exp(\\mu + \\sigma Z)$ with $Z$ being a standard normal random variable, $Z \\sim N(0,1)$. The expectation is given by the integral:\n$$ \\mathbb{E}[g(X)] = \\int_{0}^{\\infty} g(x) f_X(x) \\,dx $$\nwhere $f_X(x)$ is the probability density function (PDF) of the lognormal distribution. A more direct path for numerical evaluation is to express the expectation in terms of the underlying standard normal variable $Z$. The expectation of a function of a random variable is defined as:\n$$ \\mathbb{E}[h(Z)] = \\int_{-\\infty}^{\\infty} h(z) f_Z(z) \\,dz $$\nwhere $f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$ is the standard normal PDF.\nSubstituting $X = \\exp(\\mu + \\sigma Z)$ into $\\mathbb{E}[g(X)]$, we get:\n$$ \\mathbb{E}[g(X)] = \\mathbb{E}[g(\\exp(\\mu + \\sigma Z))] = \\int_{-\\infty}^{\\infty} g(\\exp(\\mu + \\sigma z)) \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} \\,dz $$\nThis integral is over an infinite domain $(-\\infty, \\infty)$. The problem requires its approximation using a standard Gauss-Legendre quadrature rule, which is defined on the finite interval $[-1, 1]$. To use this rule, we must transform the variable of integration to this interval.\n\nA standard and robust technique is the inverse transform sampling method, also known as the quantile function method. Let $\\Phi(z)$ be the cumulative distribution function (CDF) of the standard normal distribution. We perform a change of variable $u = \\Phi(z)$. This transformation maps the domain $z \\in (-\\infty, \\infty)$ to $u \\in (0, 1)$. The inverse transformation is $z = \\Phi^{-1}(u)$, where $\\Phi^{-1}$ is the quantile function (or probit function) of the standard normal distribution. The differential is given by $du = \\Phi'(z) dz = \\phi(z) dz = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz$.\n\nSubstituting these into the expectation integral yields:\n$$ \\mathbb{E}[g(X)] = \\int_{0}^{1} g(\\exp(\\mu + \\sigma \\Phi^{-1}(u))) \\,du $$\nThe integral is now over the finite interval $(0, 1)$. To apply the Gauss-Legendre quadrature rule on $[-1, 1]$, we perform a final linear transformation of the integration variable. Let $u = \\frac{t+1}{2}$, which maps $t \\in [-1, 1]$ to $u \\in [0, 1]$. The differential is $du = \\frac{1}{2} dt$. The integral becomes:\n$$ \\mathbb{E}[g(X)] = \\int_{-1}^{1} g\\left(\\exp\\left(\\mu + \\sigma \\Phi^{-1}\\left(\\frac{t+1}{2}\\right)\\right)\\right) \\frac{1}{2} \\,dt $$\nThis integral is now in the canonical form $\\int_{-1}^{1} F(t) dt$ suitable for Gauss-Legendre quadrature, where the integrand is\n$$ F(t) = \\frac{1}{2} g\\left(\\exp\\left(\\mu + \\sigma \\Phi^{-1}\\left(\\frac{t+1}{2}\\right)\\right)\\right) $$\nThe $n$-point Gauss-Legendre quadrature approximation for an integral $\\int_{-1}^{1} F(t) dt$ is given by $\\sum_{i=1}^{n} w_i F(t_i)$, where $\\{t_i\\}_{i=1}^n$ are the nodes (roots of the $n$-th Legendre polynomial) and $\\{w_i\\}_{i=1}^n$ are the corresponding weights.\nTherefore, the numerical approximation for $\\mathbb{E}[g(X)]$ is:\n$$ \\mathbb{E}[g(X)] \\approx \\sum_{i=1}^{n} w_i \\left[ \\frac{1}{2} g\\left(\\exp\\left(\\mu + \\sigma \\Phi^{-1}\\left(\\frac{t_i+1}{2}\\right)\\right)\\right) \\right] $$\nwhere $n=96$ as specified. The nodes $t_i$ and weights $w_i$ are obtained from standard numerical libraries. The quantile function $\\Phi^{-1}$ is also a standard special function.\n\nThis method constructs a quadrature rule as required. We can define transformed nodes $\\tilde{x}_i$ and transformed weights $\\tilde{w}_i$ for approximating $\\int_0^\\infty g(x) f_X(x) dx$ directly. Let the transformed nodes be $\\tilde{x}_i = \\exp\\left(\\mu + \\sigma \\Phi^{-1}\\left(\\frac{t_i+1}{2}\\right)\\right)$ and the corresponding weights be $\\tilde{w}_i = w_i/2$. The approximation is then $\\sum_{i=1}^{n} \\tilde{w}_i g(\\tilde{x}_i)$. This demonstrates how the change of measure is incorporated into the construction.\n\nThe procedure for computation is as follows:\n1.  Obtain the $n=96$ standard Gauss-Legendre nodes $t_i$ and weights $w_i$ for the interval $[-1, 1]$.\n2.  For each test case, specified by parameters $\\mu$, $\\sigma$, and function $g(x)$:\n3.  For each node $t_i$:\n    a. Transform the node to the uniform domain $[0, 1]$: $u_i = (t_i+1)/2$.\n    b. Compute the standard normal quantile: $z_i = \\Phi^{-1}(u_i)$.\n    c. Compute the value of the lognormal variable: $x_i = \\exp(\\mu + \\sigma z_i)$.\n    d. Evaluate the function: $g(x_i)$.\n4.  Calculate the final sum: $\\text{Result} = \\frac{1}{2} \\sum_{i=1}^{n} w_i g(x_i)$.\n5.  Round the result to $12$ decimal places.\n\nThis algorithm will be implemented for all specified test cases.\n\n- **Test case 1**: $\\mu=0$, $\\sigma=0.25$, $g(x)=x$. Analytical solution: $\\mathbb{E}[X] = \\exp(\\mu+\\sigma^2/2) = \\exp(0.03125) \\approx 1.031735515784$.\n- **Test case 2**: $\\mu=0.1$, $\\sigma=0.5$, $g(x)=x^2$. Analytical solution: $\\mathbb{E}[X^2] = \\exp(2\\mu+2\\sigma^2) = \\exp(0.7) \\approx 2.013752707470$.\n- **Test case 3**: $\\mu=-0.2$, $\\sigma=0.3$, $g(x)=\\ln(x)$. Analytical solution: $\\mathbb{E}[\\ln(X)] = \\mathbb{E}[\\mu+\\sigma Z] = \\mu = -0.2$.\n- **Test case 4**: $\\mu=0.05$, $\\sigma=0.6$, $g(x)=\\max(x-K,0)$ with $K=\\exp(\\mu)$. This resembles a European call option payout. The analytical value is $e^{\\mu+\\sigma^2/2}\\Phi(\\sigma) - K\\Phi(0) \\approx 0.387819650393$.\n- **Test case 5**: $\\mu=0$, $\\sigma=10^{-6}$, $g(x)=x$. This tests numerical stability for small $\\sigma$. The analytical solution is $\\mathbb{E}[X] = \\exp(0.5 \\times 10^{-12}) \\approx 1.0 + 0.5 \\times 10^{-12}$. Rounded to 12 decimal places, this is $1.000000000001$.\n\nThe implementation will use `numpy` for vectorized calculations and `scipy.special` for `roots_legendre` and the inverse normal CDF `ndtri`.", "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_legendre, ndtri\n\ndef solve():\n    \"\"\"\n    Computes numerical approximations of E[g(X)] for a lognormal variable X\n    using a transformed Gauss-Legendre quadrature rule.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple contains: (mu, sigma, g_function, K)\n    # K is None if not applicable.\n    test_cases = [\n        (0.0, 0.25, lambda x, k: x, None),\n        (0.1, 0.5, lambda x, k: x**2, None),\n        (-0.2, 0.3, lambda x, k: np.log(x), None),\n        (0.05, 0.6, lambda x, k: np.maximum(x - k, 0), 'exp_mu'),\n        (0.0, 1e-6, lambda x, k: x, None)\n    ]\n    \n    n = 96  # Number of quadrature nodes as specified\n\n    # Get standard Gauss-Legendre nodes and weights for the interval [-1, 1]\n    t_nodes, l_weights = roots_legendre(n)\n\n    results = []\n    \n    for case in test_cases:\n        mu, sigma, g_func, k_spec = case\n        \n        # Determine the value of K if specified\n        K = None\n        if k_spec == 'exp_mu':\n            K = np.exp(mu)\n\n        # Vectorized calculation of the quadrature sum\n        # 1. Transform nodes from [-1, 1] to [0, 1] for CDF argument\n        u_nodes = (t_nodes + 1) / 2.0\n        \n        # 2. Compute standard normal quantiles (probit function)\n        # Add small epsilon to avoid u=0 or u=1 which result in inf\n        epsilon = 1e-16\n        u_nodes = np.clip(u_nodes, epsilon, 1 - epsilon)\n        z_nodes = ndtri(u_nodes)\n        \n        # 3. Compute the values of the lognormal variable at the transformed nodes\n        x_values = np.exp(mu + sigma * z_nodes)\n        \n        # 4. Evaluate the function g(x) for all x values\n        g_values = g_func(x_values, K)\n        \n        # 5. Compute the integral approximation using the quadrature rule\n        # The integral is transformed to 1/2 * integral on [-1, 1] dt\n        integral_approx = 0.5 * np.sum(l_weights * g_values)\n        \n        results.append(integral_approx)\n\n    # Format the final output as specified\n    formatted_results = [f\"{r:.12f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"}, {"introduction": "The remarkable speed and accuracy of Gaussian quadrature rely on the assumption that the function being integrated is smooth and well-approximated by a polynomial. This practice explores what happens when this assumption is violated, a frequent occurrence when pricing derivatives with complex payoff structures. By comparing the numerical performance of Gauss-Hermite quadrature for a vanilla call option (with a smooth payoff) versus a digital call option (with a discontinuous payoff), you will gain critical intuition about the method's convergence properties and limitations [@problem_id:2396797].", "id": "2396797", "problem": "Consider a frictionless market with a constant risk-free rate under the risk-neutral measure. Let $S_0 \\in \\mathbb{R}_{+}$, $K \\in \\mathbb{R}_{+}$, $r \\in \\mathbb{R}$, $\\sigma \\in \\mathbb{R}_{+}$, and $T \\in \\mathbb{R}_{+}$ be fixed parameters. The terminal asset price $S_T$ is modeled as lognormal with\n$$\nS_T \\;=\\; S_0 \\exp\\!\\big( (r - \\tfrac{1}{2}\\sigma^2)T + \\sigma \\sqrt{T}\\, Z \\big),\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a standard normal random variable. Define two European payoffs: a vanilla call with payoff $\\max\\{S_T - K, 0\\}$ and a digital call with payoff $\\mathbf{1}\\{S_T > K\\}$. The no-arbitrage prices at time $0$ are given by the risk-neutral expectations\n$$\nC \\;=\\; e^{-rT}\\,\\mathbb{E}\\!\\left[\\max\\{S_T - K,0\\}\\right],\\qquad D \\;=\\; e^{-rT}\\,\\mathbb{E}\\!\\left[\\mathbf{1}\\{S_T > K\\}\\right].\n$$\nFor numerical approximation, you must approximate each expectation as an integral with respect to the standard normal distribution by evaluating the function of $Z$ at a finite set of deterministic nodes for $n \\in \\{4,8,16,32,64\\}$, and compute absolute valuation errors relative to the exact analytical values. The exact analytical values for $C$ and $D$ are those obtained by the closed-form Black–Scholes formulas using the same parameters $S_0$, $K$, $r$, $\\sigma$, and $T$.\n\nYou are given the following test suite of parameter sets:\n- Test case $1$: $(S_0, K, r, \\sigma, T) = (100, 100, 0.03, 0.2, 1)$.\n- Test case $2$: $(S_0, K, r, \\sigma, T) = (100, 140, 0.03, 0.2, 1)$.\n- Test case $3$: $(S_0, K, r, \\sigma, T) = (100, 100, 0.03, 0.2, 0.01)$.\n\nFor each test case, for each $n \\in \\{4,8,16,32,64\\}$, and for each payoff (vanilla call and digital call), compute the absolute error between the numerical approximation and the exact analytical price. Your program must aggregate all results into a single line of output as follows. For each test case in the order $1,2,3$, first list the $5$ absolute errors for the vanilla call for $n = 4, 8, 16, 32, 64$ in ascending order of $n$, and then the $5$ absolute errors for the digital call for the same $n$ values. Concatenate these sequences for the three test cases into a single list.\n\nFinal output format: Your program should produce a single line containing a list of floating-point numbers in the exact order specified, enclosed in square brackets, with entries separated by commas, for example, $[a_1,a_2,\\dots,a_{30}]$.", "solution": "The problem presented is valid. It is a well-posed, scientifically grounded problem in computational finance that requires the application of standard numerical methods to a canonical model. I will provide a complete solution.\n\nThe task is to compute the prices of a European vanilla call option and a European digital call option using numerical quadrature and to evaluate the accuracy of this method against the exact analytical Black-Scholes formulas.\n\nThe price of a derivative security at time $t=0$ in a risk-neutral framework is the discounted expected value of its future payoff. For a payoff at time $T$ that is a function of the terminal asset price $S_T$, the price $V_0$ is given by:\n$$\nV_0 = e^{-rT} \\mathbb{E}^{\\mathbb{Q}} [ \\text{Payoff}(S_T) ]\n$$\nwhere $r$ is the constant risk-free rate, $T$ is the time to maturity, and the expectation $\\mathbb{E}^{\\mathbb{Q}}$ is taken under the risk-neutral measure $\\mathbb{Q}$. The terminal asset price $S_T$ is modeled as a lognormal random variable:\n$$\nS_T = S_0 \\exp\\left( \\left(r - \\frac{1}{2}\\sigma^2\\right)T + \\sigma\\sqrt{T} Z \\right)\n$$\nwhere $Z$ is a standard normal random variable, $Z \\sim \\mathcal{N}(0,1)$.\n\nThe expectation can be expressed as an integral with respect to the standard normal probability density function, $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$.\nFor a generic payoff function $P(S_T)$, the expectation is:\n$$\n\\mathbb{E}^{\\mathbb{Q}}[P(S_T)] = \\int_{-\\infty}^{\\infty} P(S_T(z)) \\phi(z) dz\n$$\nwhere $S_T(z)$ is the terminal price as a function of the realization $z$ of the random variable $Z$.\n\nFor the vanilla call option, the payoff is $\\max\\{S_T - K, 0\\}$, and its price $C$ is:\n$$\nC = e^{-rT} \\int_{-\\infty}^{\\infty} \\max\\{S_T(z) - K, 0\\} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz\n$$\nFor the digital call option, the payoff is $\\mathbf{1}\\{S_T > K\\}$, and its price $D$ is:\n$$\nD = e^{-rT} \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{S_T(z) > K\\} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz\n$$\n\nTo approximate these integrals numerically, we employ Gauss-Hermite quadrature. This method is designed for integrals of the form $\\int_{-\\infty}^{\\infty} e^{-x^2} g(x) dx$. The $n$-point quadrature rule is given by:\n$$\n\\int_{-\\infty}^{\\infty} e^{-x^2} g(x) dx \\approx \\sum_{i=1}^n w_i g(x_i)\n$$\nwhere $x_i$ are the roots (nodes) of the $n$-th degree physicist's Hermite polynomial $H_n(x)$, and $w_i$ are the associated weights.\n\nOur integrals involve the weighting function $\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2}$, not $e^{-x^2}$. A change of variables is necessary. Let $z = \\sqrt{2}x$, which implies $dz = \\sqrt{2}dx$. The term $\\phi(z)dz$ transforms as follows:\n$$\n\\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz = \\frac{1}{\\sqrt{2\\pi}} e^{-(\\sqrt{2}x)^2/2} (\\sqrt{2}dx) = \\frac{\\sqrt{2}}{\\sqrt{2\\pi}} e^{-x^2} dx = \\frac{1}{\\sqrt{\\pi}} e^{-x^2} dx\n$$\nTherefore, for a function $f(z)$, the integral becomes:\n$$\n\\int_{-\\infty}^{\\infty} f(z) \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} dz = \\int_{-\\infty}^{\\infty} f(\\sqrt{2}x) \\frac{1}{\\sqrt{\\pi}} e^{-x^2} dx = \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^{\\infty} f(\\sqrt{2}x) e^{-x^2} dx\n$$\nApplying the Gauss-Hermite quadrature rule, we get the approximation:\n$$\n\\mathbb{E}[f(Z)] \\approx \\frac{1}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i f(\\sqrt{2}x_i)\n$$\nwhere $(x_i, w_i)$ are the standard $n$-point Gauss-Hermite nodes and weights.\n\nThe numerical approximations for the option prices, $C_n$ and $D_n$, for a given number of nodes $n$, are:\n$$\nC_n = \\frac{e^{-rT}}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\max\\{S_T(\\sqrt{2}x_i) - K, 0\\}\n$$\n$$\nD_n = \\frac{e^{-rT}}{\\sqrt{\\pi}} \\sum_{i=1}^n w_i \\mathbf{1}\\{S_T(\\sqrt{2}x_i) > K\\}\n$$\n\nFor benchmarking, we use the exact analytical solutions provided by the Black-Scholes formulas. We define the terms $d_1$ and $d_2$:\n$$\nd_1 = \\frac{\\ln(S_0/K) + (r + \\frac{1}{2}\\sigma^2)T}{\\sigma\\sqrt{T}}\n$$\n$$\nd_2 = d_1 - \\sigma\\sqrt{T} = \\frac{\\ln(S_0/K) + (r - \\frac{1}{2}\\sigma^2)T}{\\sigma\\sqrt{T}}\n$$\nThe exact price of the vanilla call option, $C_{BS}$, is:\n$$\nC_{BS} = S_0 \\Phi(d_1) - K e^{-rT} \\Phi(d_2)\n$$\nwhere $\\Phi(\\cdot)$ is the cumulative distribution function (CDF) of the standard normal distribution.\n\nThe exact price of the digital call option, $D_{BS}$, corresponds to the discounted probability of the option finishing in-the-money, $\\mathbb{P}(S_T > K)$. This probability is $\\Phi(d_2)$. Thus:\n$$\nD_{BS} = e^{-rT} \\Phi(d_2)\n$$\n\nThe absolute valuation error for a given $n$ is calculated as the absolute difference between the numerical approximation and the analytical value:\n$$\n\\epsilon_C(n) = |C_n - C_{BS}|\n$$\n$$\n\\epsilon_D(n) = |D_n - D_{BS}|\n$$\n\nThe algorithm proceeds as follows for each test case:\n$1$. Calculate the analytical prices $C_{BS}$ and $D_{BS}$ using the provided parameters.\n$2$. For each specified number of nodes $n \\in \\{4, 8, 16, 32, 64\\}$:\n    a. Obtain the $n$-point Gauss-Hermite nodes $\\{x_i\\}$ and weights $\\{w_i\\}$.\n    b. Compute the corresponding nodes $\\{z_i = \\sqrt{2}x_i\\}$ for the standard normal integral.\n    c. Evaluate the terminal prices $\\{S_T(z_i)\\}$ at these nodes.\n    d. Compute the payoffs for both option types at each node.\n    e. Calculate the numerically approximated prices $C_n$ and $D_n$ by performing the weighted summation scaled by $e^{-rT}/\\sqrt{\\pi}$.\n    f. Compute the absolute errors $\\epsilon_C(n)$ and $\\epsilon_D(n)$.\n$3$. Collect and order the errors as specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes absolute errors in option pricing using Gauss-Hermite quadrature\n    against analytical Black-Scholes formulas for a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, K, r, sigma, T)\n        (100.0, 100.0, 0.03, 0.2, 1.0),\n        (100.0, 140.0, 0.03, 0.2, 1.0),\n        (100.0, 100.0, 0.03, 0.2, 0.01),\n    ]\n\n    # Number of quadrature points to test\n    n_values = [4, 8, 16, 32, 64]\n\n    # List to store all computed errors in the required order\n    all_results = []\n\n    def black_scholes_call(S0, K, r, sigma, T):\n        \"\"\"Analytical Black-Scholes price for a European vanilla call.\"\"\"\n        if T == 0:\n            return np.maximum(S0 - K, 0)\n        d1 = (np.log(S0 / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n        d2 = d1 - sigma * np.sqrt(T)\n        price = S0 * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n        return price\n\n    def black_scholes_digital_call(K, r, sigma, T, d2):\n        \"\"\"Analytical Black-Scholes price for a European digital call.\"\"\"\n        if T == 0:\n            return 1.0 if S0 > K else 0.0\n        price = np.exp(-r * T) * norm.cdf(d2)\n        return price\n\n    for S0, K, r, sigma, T in test_cases:\n        # Calculate d2 once, as it's used in both analytical formulas\n        if T > 0:\n            d2 = (np.log(S0 / K) + (r - 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n        else:\n            d2 = np.inf if S0 > K else -np.inf # Edge case an T=0\n\n        # 1. Calculate analytical benchmark prices\n        C_bs = black_scholes_call(S0, K, r, sigma, T)\n        D_bs = black_scholes_digital_call(K, r, sigma, T, d2)\n        \n        call_errors_case = []\n        digital_errors_case = []\n\n        # 2. Loop through number of quadrature nodes\n        for n in n_values:\n            # a. Get Gauss-Hermite nodes and weights\n            x_i_herm, w_i_herm = np.polynomial.hermite.hermgauss(n)\n            \n            # b. Change of variables for standard normal integral N(0,1)\n            # z ~ N(0,1), integral is against (1/sqrt(2pi)) * exp(-z^2/2)\n            # We use x for Hermite integral, against exp(-x^2)\n            # Change of variables: z = sqrt(2)*x => dz = sqrt(2)*dx\n            # (1/sqrt(2pi))exp(-z^2/2)dz = (1/sqrt(pi))exp(-x^2)dx\n            # So, our integral approx is (1/sqrt(pi)) * sum(w_i * f(sqrt(2)*x_i))\n            z_nodes = np.sqrt(2.0) * x_i_herm\n            \n            # c. Evaluate terminal price at each node\n            drift = (r - 0.5 * sigma**2) * T\n            diffusion = sigma * np.sqrt(T) * z_nodes\n            S_T_nodes = S0 * np.exp(drift + diffusion)\n            \n            # d. Compute payoffs at each node\n            call_payoff = np.maximum(S_T_nodes - K, 0)\n            digital_payoff = (S_T_nodes > K).astype(float)\n            \n            # e. Calculate numerical prices\n            # The scaling factor is exp(-rT)/sqrt(pi)\n            scale_factor = np.exp(-r * T) / np.sqrt(np.pi)\n            C_n = scale_factor * np.sum(w_i_herm * call_payoff)\n            D_n = scale_factor * np.sum(w_i_herm * digital_payoff)\n            \n            # f. Compute and store absolute errors\n            call_errors_case.append(abs(C_n - C_bs))\n            digital_errors_case.append(abs(D_n - D_bs))\n            \n        # 3. Collect and order results for this test case\n        all_results.extend(call_errors_case)\n        all_results.extend(digital_errors_case)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"}, {"introduction": "This capstone exercise moves beyond using pre-built quadrature rules to constructing your own, tailored to a specific problem. You will build a custom Gaussian quadrature rule designed for an empirical probability distribution, such as the daily returns of an asset, derived from historical data. This practice will guide you through the elegant connection between orthogonal polynomials, the three-term recurrence relation, and the eigenvalue problem of a Jacobi matrix, culminating in a powerful tool for bespoke financial analysis [@problem_id:2396803].", "id": "2396803", "problem": "You are given a discrete proxy for the empirical distribution of daily log-returns of the Standard and Poor's $500$ (S&amp;P $500$) index. The proxy is a histogram summarized as a set of support points (bin centers) and corresponding probabilities that sum to $1$, representing a probability measure. Let the support points be the array $x \\in \\mathbb{R}^m$ and the corresponding strictly positive probabilities be the array $w \\in \\mathbb{R}_+^m$ with $\\sum_{i=1}^m w_i = 1$. In this problem, the arrays are fixed as\n- $x = [-0.04,\\,-0.03,\\,-0.02,\\,-0.01,\\,0.0,\\,0.01,\\,0.02,\\,0.03,\\,0.04]$,\n- $w = [0.01,\\,0.04,\\,0.10,\\,0.20,\\,0.30,\\,0.20,\\,0.10,\\,0.04,\\,0.01]$.\nThese values are unitless decimal returns and form a valid discrete probability distribution.\n\nYour task is to implement, from first principles, the three-term recurrence for orthonormal polynomials with respect to the inner product defined by the discrete measure $(x,w)$ and then to build the corresponding Gaussian quadrature rule of a given order. The derivation must start from the following base:\n- The discrete inner product is defined by $\\langle f,g\\rangle = \\sum_{i=1}^m w_i\\, f(x_i)\\, g(x_i)$ for any functions $f$ and $g$ for which the sum is finite.\n- A sequence $\\{p_k\\}_{k\\ge 0}$ of orthonormal polynomials satisfies $\\langle p_k,p_\\ell\\rangle = \\delta_{k\\ell}$, where $\\delta_{k\\ell}$ is the Kronecker delta, and each $p_k$ has degree $k$.\n- For any measure with finite moments, there exists a three-term recurrence of the form $x\\,p_k(x) = a_k\\,p_{k+1}(x) + b_k\\,p_k(x) + a_{k-1}\\,p_{k-1}(x)$ with $a_k &gt; 0$, $b_k \\in \\mathbb{R}$, $p_{-1}\\equiv 0$, and $p_0$ normalized to have unit norm under the inner product.\n- By the spectral theorem for real symmetric matrices, the Gaussian quadrature nodes of order $n$ are the eigenvalues of the $n\\times n$ symmetric tridiagonal (Jacobi) matrix whose diagonal entries are $\\{b_0,\\dots,b_{n-1}\\}$ and whose sub- and super-diagonal entries are $\\{a_0,\\dots,a_{n-2}\\}$; the corresponding quadrature weights are proportional to the squares of the first components of the orthonormal eigenvectors, scaled by the zeroth moment $\\mu_0=\\langle 1,1\\rangle$.\n\nImplement the following algorithmic steps logically derived from these bases, without invoking any shortcut formulas:\n- Starting from $p_{-1}\\equiv 0$ and $p_0(x)\\equiv c$ chosen so that $\\langle p_0,p_0\\rangle = 1$, iteratively construct $p_{k+1}$ from $p_k$ and $p_{k-1}$ by orthogonalizing $x\\,p_k$ against $p_k$ and $p_{k-1}$, and normalizing using the given inner product to obtain the coefficients $a_k$ and $b_k$.\n- Assemble the Jacobi matrix of size $n$ for a requested quadrature order $n$.\n- Compute the $n$ Gaussian quadrature nodes and weights from the eigen-decomposition of the Jacobi matrix, noting that the weights must sum to $\\mu_0 = \\sum_{i=1}^m w_i$.\n\nDesign a program that, for each requested order $n$, validates the exactness property of the constructed Gaussian quadrature by testing monomials $f_k(x) = x^k$ for all integers $k \\in \\{0,1,\\dots,2n-1\\}$. For each $k$, compute the exact discrete expectation $\\sum_{i=1}^m w_i\\,x_i^k$ and the quadrature approximation $\\sum_{j=1}^n \\omega_j\\,\\xi_j^k$, where $\\{\\xi_j,\\omega_j\\}_{j=1}^n$ are the quadrature nodes and weights. Report, for each order $n$, the maximum absolute error over $k\\in\\{0,\\dots,2n-1\\}$.\n\nTest Suite:\n- Use the histogram arrays $x$ and $w$ specified above with $m=9$.\n- Evaluate the maximum absolute error for quadrature orders $n \\in \\{1,3,5,9\\}$.\n- Because these are unitless returns, no physical units are involved. Angles are not used. All final numerical answers must be plain real numbers.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the four results (one per test case, in the order $n=1$, then $n=3$, then $n=5$, then $n=9$) as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the maximum absolute error (a real number) for the corresponding quadrature order.", "solution": "The posed problem is valid. It is scientifically grounded in the well-established theory of orthogonal polynomials and Gaussian quadrature, specifically the Golub-Welsch algorithm. The problem statement provides a complete and consistent set of givens, including the discrete measure and the precise algorithmic steps to be followed. It is well-posed, objective, and computationally feasible. We may therefore proceed with the solution.\n\nThe core of the task is to construct a Gaussian quadrature rule for a given discrete probability measure. This measure is defined by a set of support points $x = \\{x_i\\}_{i=1}^m$ and corresponding positive weights (probabilities) $w = \\{w_i\\}_{i=1}^m$, where $m=9$ and $\\sum_{i=1}^m w_i = 1$. The inner product of two functions $f$ and $g$ with respect to this measure is given by:\n$$\n\\langle f, g \\rangle = \\sum_{i=1}^{m} w_i f(x_i) g(x_i)\n$$\nThe procedure involves two main stages: first, the generation of recurrence coefficients for the family of orthonormal polynomials associated with this inner product; second, the use of these coefficients to construct a Jacobi matrix whose spectral properties yield the desired quadrature nodes and weights.\n\nThe foundation of the method is the three-term recurrence relation for a sequence of orthonormal polynomials $\\{p_k(x)\\}_{k \\ge 0}$, where $p_k(x)$ has degree $k$ and $\\langle p_k, p_j \\rangle = \\delta_{kj}$. The recurrence is:\n$$\nx\\,p_k(x) = a_k\\,p_{k+1}(x) + b_k\\,p_k(x) + a_{k-1}\\,p_{k-1}(x)\n$$\nwith the convention $p_{-1}(x) \\equiv 0$ and $a_{-1} = 0$. The coefficients $a_k \\in \\mathbb{R}_+$ and $b_k \\in \\mathbb{R}$ are determined by the orthogonality condition. We will construct these coefficients iteratively using the Stieltjes procedure.\n\n**Step 1: Iterative Construction of Recurrence Coefficients**\n\nWe begin by establishing the base of the orthonormal sequence. The polynomial $p_0(x)$ is a constant, $p_0(x) = c$. Its norm must be unity:\n$$\n\\langle p_0, p_0 \\rangle = \\langle c, c \\rangle = \\sum_{i=1}^{m} w_i c^2 = c^2 \\sum_{i=1}^{m} w_i = 1\n$$\nThe zeroth moment is $\\mu_0 = \\sum_{i=1}^{m} w_i = 1$ based on the given probabilities. Thus, $c^2 = 1$, and we choose $c=1$, which gives $p_0(x) = 1$.\n\nFor $k \\ge 0$, we derive the expressions for $b_k$ and $a_k$. Taking the inner product of the recurrence relation with $p_k(x)$:\n$$\n\\langle x\\,p_k, p_k \\rangle = a_k \\langle p_{k+1}, p_k \\rangle + b_k \\langle p_k, p_k \\rangle + a_{k-1} \\langle p_{k-1}, p_k \\rangle\n$$\nDue to orthonormality, $\\langle p_{j}, p_k \\rangle = \\delta_{jk}$. The equation simplifies to:\n$$\n\\langle x\\,p_k, p_k \\rangle = b_k \\cdot 1\n$$\nThis provides the formula for $b_k$:\n$$\nb_k = \\langle x\\,p_k, p_k \\rangle = \\sum_{i=1}^{m} w_i x_i [p_k(x_i)]^2\n$$\nSince the given distribution $(x, w)$ is symmetric around $0$, all odd moments are zero. This implies that the polynomials $p_k(x)$ will have definite parity (even for even $k$, odd for odd $k$). The term $x_i [p_k(x_i)]^2$ forms an odd function with respect to the symmetric measure, and thus its expectation is zero. Consequently, all coefficients $b_k$ will be zero. This provides a useful check for our computation.\n\nTo find $a_k$, we rearrange the recurrence relation to define an unnormalized polynomial $\\tilde{p}_{k+1}(x)$:\n$$\n\\tilde{p}_{k+1}(x) = (x - b_k) p_k(x) - a_{k-1} p_{k-1}(x)\n$$\nBy definition, $p_{k+1}(x)$ is the normalization of $\\tilde{p}_{k+1}(x)$. The normalization constant is $a_k$, which is the norm of $\\tilde{p}_{k+1}(x)$:\n$$\na_k = \\| \\tilde{p}_{k+1} \\| = \\sqrt{\\langle \\tilde{p}_{k+1}, \\tilde{p}_{k+1} \\rangle} = \\left( \\sum_{i=1}^{m} w_i [\\tilde{p}_{k+1}(x_i)]^2 \\right)^{1/2}\n$$\nThe next orthonormal polynomial is then $p_{k+1}(x) = \\tilde{p}_{k+1}(x) / a_k$.\n\nThis iterative process starts with $p_{-1}(x) \\equiv 0$ and $p_0(x) \\equiv 1$. For each $k=0, 1, 2, \\dots$, we compute $b_k$, then $\\tilde{p}_{k+1}(x)$, then $a_k$, and finally $p_{k+1}(x)$. In implementation, the polynomials are represented by vectors of their values at the support points $\\{x_i\\}$.\n\n**Step 2: Construction of the Gaussian Quadrature Rule**\n\nFor a quadrature rule of order $n$, we require the coefficients $\\{b_0, \\dots, b_{n-1}\\}$ and $\\{a_0, \\dots, a_{n-2}\\}$. These form the $n \\times n$ symmetric tridiagonal Jacobi matrix, $J_n$:\n$$\nJ_n = \\begin{pmatrix}\nb_0 & a_0 & 0 & \\cdots & 0 \\\\\na_0 & b_1 & a_1 & \\cdots & 0 \\\\\n0 & a_1 & b_2 & \\ddots & \\vdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & a_{n-2} \\\\\n0 & 0 & \\cdots & a_{n-2} & b_{n-1}\n\\end{pmatrix}\n$$\nThe Golub-Welsch algorithm states that the $n$ Gaussian quadrature nodes, $\\{\\xi_j\\}_{j=1}^n$, are the eigenvalues of $J_n$. The corresponding quadrature weights, $\\{\\omega_j\\}_{j=1}^n$, are derived from the first components of the normalized eigenvectors of $J_n$. Let $v_j$ be the normalized eigenvector corresponding to the eigenvalue $\\xi_j$. The weight $\\omega_j$ is given by:\n$$\n\\omega_j = \\mu_0 \\cdot (v_{j,1})^2\n$$\nwhere $v_{j,1}$ is the first component of the aigenvector $v_j$. Since $\\mu_0=1$, the weights are simply the squares of the first components of the eigenvectors.\n\n**Step 3: Validation of Exactness**\n\nA Gaussian quadrature rule of order $n$ is exact for all polynomials of degree up to $2n-1$. We verify this property by comparing the exact value of the integral (expectation) of monomials $f_k(x) = x^k$ with the value computed by our quadrature rule. For each $k \\in \\{0, 1, \\dots, 2n-1\\}$, we calculate:\n- The exact discrete integral: $E_k = \\langle x^k, 1 \\rangle = \\sum_{i=1}^m w_i x_i^k$.\n- The quadrature approximation: $Q_k = \\sum_{j=1}^n \\omega_j \\xi_j^k$.\n\nThe maximum absolute error across all tested monomials, $\\max_{k \\in \\{0, \\dots, 2n-1\\}} |E_k - Q_k|$, is reported for each requested order $n$. For the case where $n=m=9$, the Gaussian quadrature rule must exactly recover the nodes and weights of the original discrete measure, i.e., $\\{\\xi_j, \\omega_j\\} \\equiv \\{x_i, w_i\\}$ (up to permutation). Consequently, the error for $n=9$ is expected to be zero to within machine precision for any function, including all tested monomials.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom numpy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Implements the Golub-Welsch algorithm to construct Gaussian quadrature rules\n    for a discrete measure and validates their exactness.\n    \"\"\"\n    # Define the discrete measure (proxy for S&P 500 log-returns)\n    x = np.array([-0.04, -0.03, -0.02, -0.01, 0.0, 0.01, 0.02, 0.03, 0.04], dtype=np.float64)\n    w = np.array([0.01, 0.04, 0.10, 0.20, 0.30, 0.20, 0.10, 0.04, 0.01], dtype=np.float64)\n    m = len(x)\n\n    # Test suite of quadrature orders\n    test_cases = [1, 3, 5, 9]\n    max_order = max(test_cases)\n    \n    results = []\n\n    # --- Step 1: Compute recurrence coefficients using the Stieltjes procedure ---\n    # We need coefficients a_k up to k=max_order-2 and b_k up to k=max_order-1.\n    # The loop will compute a_k and b_k for k=0...max_order-1.\n    \n    a_coeffs = []\n    b_coeffs = []\n\n    # Initialization: p_(-1) = 0, p_0 = 1/sqrt(mu_0)\n    mu_0 = np.sum(w) # Should be 1.0\n    p_k_minus_1 = np.zeros(m, dtype=np.float64)\n    p_k = np.ones(m, dtype=np.float64) / np.sqrt(mu_0)\n    \n    # Iteratively find coefficients a_k and b_k\n    for k in range(max_order):\n        # Calculate b_k = <x*p_k, p_k>\n        # Note: Since the distribution is symmetric, all b_k will be zero.\n        b_k = np.sum(w * x * p_k**2)\n        b_coeffs.append(b_k)\n\n        # Calculate unnormalized p_{k+1}\n        # p_tilde_{k+1} = (x - b_k)*p_k - a_{k-1}*p_{k-1}\n        a_k_minus_1 = a_coeffs[k - 1] if k > 0 else 0.0\n        p_k_plus_1_tilde = (x - b_k) * p_k - a_k_minus_1 * p_k_minus_1\n\n        # Calculate a_k = ||p_tilde_{k+1}||\n        norm_sq = np.sum(w * p_k_plus_1_tilde**2)\n        a_k = np.sqrt(norm_sq)\n        a_coeffs.append(a_k)\n        \n        # Normalize to get p_{k+1}\n        # Handle the case a_k -> 0, which happens if order n reaches m.\n        # This indicates the termination of the orthogonal polynomial sequence.\n        if a_k < np.finfo(float).eps:\n            p_k_plus_1 = np.zeros(m, dtype=np.float64)\n        else:\n            p_k_plus_1 = p_k_plus_1_tilde / a_k\n\n        # Update for the next iteration\n        p_k_minus_1 = p_k\n        p_k = p_k_plus_1\n\n    # --- Step 2 & 3: Build Jacobi matrix, compute quadrature, and validate for each order n ---\n    for n in test_cases:\n        # Assemble the n x n Jacobi matrix (J_n)\n        diag = np.array(b_coeffs[:n])\n        off_diag = np.array(a_coeffs[:n-1]) if n > 1 else np.array([])\n        J_n = np.diag(diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n        \n        # Eigen-decomposition gives quadrature nodes (eigenvalues) and weights (from eigenvectors)\n        # Using eigh as the matrix is symmetric.\n        nodes, evecs = eigh(J_n)\n        \n        # Weights are mu_0 * (first component of normalized eigenvectors)^2\n        # Since mu_0 = 1, weights are just the square of the first components.\n        weights = mu_0 * (evecs[0, :]**2)\n\n        # Validate exactness for polynomials of degree k = 0, 1, ..., 2n-1\n        max_abs_error = 0.0\n        for k in range(2 * n):\n            # Exact integral (discrete expectation)\n            exact_integral = np.sum(w * (x**k))\n            \n            # Quadrature approximation\n            # Note: 0^0 is 1, numpy handles this correctly for vectors.\n            quad_approx = np.sum(weights * (nodes**k))\n            \n            # Update maximum absolute error\n            abs_error = np.abs(exact_integral - quad_approx)\n            if abs_error > max_abs_error:\n                max_abs_error = abs_error\n        \n        results.append(max_abs_error)\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}]}