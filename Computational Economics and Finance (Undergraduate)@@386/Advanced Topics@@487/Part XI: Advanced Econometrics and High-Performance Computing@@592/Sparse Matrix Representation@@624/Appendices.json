{"hands_on_practices": [{"introduction": "Before applying complex algorithms, a crucial step in computational economics and finance is choosing how to represent your data. This practice provides a concrete scenario—modeling portfolio constraints—to analyze the fundamental trade-off in memory usage between different sparse matrix formats. By calculating the memory footprint for both the Dictionary of Keys (DOK) and Compressed Sparse Row (CSR) representations, you will gain a quantitative understanding of why format selection is critical for handling large-scale financial models [@problem_id:2432973].", "id": "2432973", "problem": "You are modeling linear constraints for a long-only portfolio selection problem with sector limits in computational economics and finance. There are $N$ assets and $S$ sectors. Each asset belongs to exactly one sector. Let $x \\in \\mathbb{R}^{N}$ denote asset decision variables and $y \\in \\mathbb{R}^{S}$ denote binary sector-usage indicators. The constraint that the portfolio holds assets from no more than $k$ sectors is represented by a sparse linear system with the following structure:\n1. For each sector $s \\in \\{1,\\dots,S\\}$, the constraint\n$$\\sum_{i \\in \\text{sector}(s)} 1 \\cdot x_i - M \\cdot y_s \\le 0,$$\nwhere $M$ is any positive constant, and each asset $i$ appears in exactly one such sum with coefficient $1$.\n2. A cap on the number of active sectors,\n$$\\sum_{s=1}^{S} 1 \\cdot y_s \\le k.$$\nArrange these into a single sparse constraint matrix $A \\in \\mathbb{R}^{(S+1)\\times (N+S)}$ acting on the concatenated vector $(x,y) \\in \\mathbb{R}^{N+S}$, with the right-hand-side vector $b \\in \\mathbb{R}^{S+1}$ that includes $k$ only in its final entry. Under these conditions, the number of nonzero entries in $A$ is\n$$\\mathrm{nnz} = N + 2S,$$\nsince there are $N$ unit coefficients across the $S$ sector rows for $x$, $S$ coefficients for the $y$-variables in those same rows, and $S$ unit coefficients in the final row for the $y$-variables.\n\nYou will compare the memory usage, in bytes, of two sparse representations of the matrix $A$ together with the right-hand-side vector $b$:\n- Dictionary of Keys (DOK): Store each nonzero entry as a triple $(\\text{row}, \\text{col}, \\text{value})$ using $64$-bit signed integers for indices and $64$-bit floating-point for values. Also store the right-hand-side vector $b$ as $64$-bit floating-point entries. Assume no additional overhead beyond these primitive fields.\n- Compressed Sparse Row (CSR): Store three arrays for $A$: data (length $\\mathrm{nnz}$, $64$-bit floating-point), column indices (length $\\mathrm{nnz}$, $64$-bit signed integers), and row pointer (length $(S+1)+1$, $64$-bit signed integers). Also store the right-hand-side vector $b$ as $64$-bit floating-point entries.\n\nAll index fields use $8$ bytes per entry and all floating-point fields use $8$ bytes per entry. The size of $M$ does not affect memory usage. The value of $k$ affects only the numeric value of one entry in $b$ and thus does not change the number of stored entries.\n\nTask: For each test case $(N,S,k)$, compute the exact total memory in bytes required by Dictionary of Keys (DOK) and Compressed Sparse Row (CSR) representations under the model above, including the storage of $b$. Then, for each test case, compute the ratio\n$$r = \\frac{\\text{bytes}_{\\mathrm{DOK}}}{\\text{bytes}_{\\mathrm{CSR}}},$$\nwhich is dimensionless. Report each $r$ rounded to three decimal places.\n\nTest suite (each tuple is $(N,S,k)$ with $N \\in \\mathbb{Z}_{\\ge 0}$, $S \\in \\mathbb{Z}_{\\ge 1}$, $k \\in \\{0,1,\\dots,S\\}$):\n- $(500, 10, 3)$\n- $(10000, 100, 5)$\n- $(0, 8, 0)$\n- $(1000, 1, 1)$\n- $(50, 200, 5)$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\"[r_1,r_2,r_3,r_4,r_5]\"$, where each $r_j$ is rounded to three decimal places as specified.", "solution": "The problem presented is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We will proceed with a systematic derivation of the memory requirements for the specified sparse matrix representations.\n\nThe problem asks for a comparison of memory usage between the Dictionary of Keys (DOK) and Compressed Sparse Row (CSR) formats for a linear constraint system $Ax \\le b$. The system is defined by $N$ assets and $S$ sectors. The constraint matrix $A$ has dimensions $(S+1) \\times (N+S)$, and the right-hand-side vector $b$ has dimension $S+1$. The number of nonzero entries in $A$ is given as $\\mathrm{nnz} = N + 2S$. All index fields are stored as $64$-bit ($8$-byte) integers, and all value fields are stored as $64$-bit ($8$-byte) floating-point numbers.\n\nFirst, we will calculate the total memory required for the Dictionary of Keys (DOK) representation, denoted as $\\text{bytes}_{\\mathrm{DOK}}$. In the DOK format, each of the $\\mathrm{nnz}$ nonzero entries of the matrix $A$ is stored as a tuple $(\\text{row}, \\text{col}, \\text{value})$. The memory for each component of the tuple is $8$ bytes for the row index, $8$ bytes for the column index, and $8$ bytes for the value. Therefore, the memory to store one nonzero entry is $8 + 8 + 8 = 24$ bytes. The total memory for the matrix $A$ is the product of the number of nonzero entries and the memory per entry:\n$$ \\text{Mem}_{\\mathrm{DOK}}(A) = \\mathrm{nnz} \\times 24 = (N + 2S) \\times 24 = 24N + 48S $$\nThe right-hand-side vector $b$ has $S+1$ entries, each being an $8$-byte float. Its memory footprint is:\n$$ \\text{Mem}(b) = (S+1) \\times 8 = 8S + 8 $$\nThe total memory for the DOK representation is the sum of the memory for the matrix $A$ and the vector $b$:\n$$ \\text{bytes}_{\\mathrm{DOK}} = \\text{Mem}_{\\mathrm{DOK}}(A) + \\text{Mem}(b) = (24N + 48S) + (8S + 8) = 24N + 56S + 8 $$\n\nNext, we calculate the total memory for the Compressed Sparse Row (CSR) representation, denoted as $\\text{bytes}_{\\mathrm{CSR}}$. The CSR format for matrix $A$ consists of three arrays: a `data` array for values, a `column indices` array, and a `row pointer` array. The `data` array stores the $\\mathrm{nnz}$ nonzero values, and the `column indices` array stores the column index for each of these values. Both arrays have length $\\mathrm{nnz}$ and use $8$-byte elements ($64$-bit float and $64$-bit integer, respectively). The `row pointer` array specifies the start of each row in the other two arrays; its length is the number of rows plus one, which is $(S+1)+1 = S+2$, and its elements are $8$-byte integers. The memory for matrix $A$ in CSR format is:\n$$ \\text{Mem}_{\\mathrm{CSR}}(A) = \\underbrace{(N+2S) \\times 8}_{\\text{data}} + \\underbrace{(N+2S) \\times 8}_{\\text{indices}} + \\underbrace{(S+2) \\times 8}_{\\text{row pointer}} $$\n$$ \\text{Mem}_{\\mathrm{CSR}}(A) = 16(N+2S) + 8(S+2) = 16N + 32S + 8S + 16 = 16N + 40S + 16 $$\nThe vector $b$ is stored identically to the previous case, requiring $\\text{Mem}(b) = 8S + 8$ bytes. The total memory for the CSR representation is the sum of memory for the matrix $A$ and the vector $b$:\n$$ \\text{bytes}_{\\mathrm{CSR}} = \\text{Mem}_{\\mathrm{CSR}}(A) + \\text{Mem}(b) = (16N + 40S + 16) + (8S + 8) = 16N + 48S + 24 $$\n\nFinally, we compute the ratio $r = \\frac{\\text{bytes}_{\\mathrm{DOK}}}{\\text{bytes}_{\\mathrm{CSR}}}$. Substituting the derived expressions:\n$$ r = \\frac{24N + 56S + 8}{16N + 48S + 24} $$\nThis expression can be simplified by dividing the numerator and the denominator by their greatest common divisor, which is $8$:\n$$ r = \\frac{3N + 7S + 1}{2N + 6S + 3} $$\nThis final formula is used to compute the required ratio for each test case. As specified in the problem, the parameters $k$ and $M$ correctly have no influence on the memory calculation. The ratio $r$ depends only on the number of assets $N$ and the number of sectors $S$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the ratio of memory usage between DOK and CSR sparse representations\n    for a series of test cases based on a portfolio optimization problem.\n    \"\"\"\n    test_cases = [\n        # Each tuple is (N, S, k)\n        (500, 10, 3),\n        (10000, 100, 5),\n        (0, 8, 0),\n        (1000, 1, 1),\n        (50, 200, 5)\n    ]\n\n    results = []\n    for case in test_cases:\n        N, S, k = case  # The parameter k is not used in the memory calculation.\n\n        # Derived formula for the ratio r = bytes_DOK / bytes_CSR\n        # r = (24*N + 56*S + 8) / (16*N + 48*S + 24)\n        # Simplified formula by dividing numerator and denominator by 8:\n        # r = (3*N + 7*S + 1) / (2*N + 6*S + 3)\n        numerator = 3 * N + 7 * S + 1\n        denominator = 2 * N + 6 * S + 3\n        \n        # Based on problem constraints (N>=0, S>=1), the denominator is always positive.\n        # min denominator: 2*0 + 6*1 + 3 = 9. So no division by zero check is needed.\n        ratio = numerator / denominator\n        \n        # Format the result to three decimal places.\n        results.append(f\"{ratio:.3f}\")\n\n    # Print the final output in the specified format: \"[r1,r2,r3,r4,r5]\"\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"}, {"introduction": "Beyond just storing data, the power of sparse representations lies in performing computations efficiently. This exercise explores the Hadamard (or element-wise) product, a common operation when combining different sources of financial data, such as a quantitative correlation model and a qualitative news-sentiment overlay. By implementing this operation, you will discover the core algorithmic principle for sparse matrix arithmetic: leveraging the intersection of non-zero indices to avoid wasteful computations and achieve significant performance gains [@problem_id:2433011].", "id": "2433011", "problem": "You are given pairs of real-valued sparse square matrices representing, respectively, an asset correlation matrix and a news-sentiment co-movement matrix. For each pair, compute the Hadamard product (element-wise product) defined by $A = C \\circ S$, where $A_{ij} = C_{ij} \\cdot S_{ij}$ for all indices $i,j$. Each sparse matrix is specified by its dimension $n$ and a finite list of nonzero entries. Indices are zero-based. Any entry not listed is exactly zero.\n\nFor each test case, compute and return three quantities:\n- The number of nonzero entries of $A$, denoted $\\mathrm{nnz}(A)$.\n- The sum of all entries of $A$, denoted $\\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} A_{ij}$.\n- The squared Frobenius norm of $A$, denoted $\\lVert A \\rVert_F^2 = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} A_{ij}^2$.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. Each test case’s result must itself be a list in the form $[\\mathrm{nnz}(A), \\text{sum}, \\text{frob\\_sq}]$, where $\\mathrm{nnz}(A)$ is an integer and the two floating-point values are printed rounded to one decimal place. For example: $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3]]$.\n\nTest suite:\n- Test case 1:\n  - Dimension: $n = 5$.\n  - Correlation matrix $C$ nonzeros:\n    - Diagonal: $(0,0,1)$, $(1,1,1)$, $(2,2,1)$, $(3,3,1)$, $(4,4,1)$.\n    - Off-diagonal symmetric pairs: $(0,1,0.3)$, $(1,0,0.3)$, $(0,3,-0.2)$, $(3,0,-0.2)$, $(2,4,0.5)$, $(4,2,0.5)$, $(1,2,0.1)$, $(2,1,0.1)$.\n  - Sentiment matrix $S$ nonzeros:\n    - Diagonal: $(0,0,0.5)$, $(2,2,1.5)$, $(4,4,-1.0)$.\n    - Off-diagonal symmetric pairs: $(0,1,10)$, $(1,0,10)$, $(0,3,3)$, $(3,0,3)$, $(1,3,7)$, $(2,4,-2)$, $(4,2,-2)$, $(1,2,8)$, $(2,1,8)$.\n- Test case 2:\n  - Dimension: $n = 4$.\n  - Correlation matrix $C$ nonzeros:\n    - Diagonal: $(0,0,1)$, $(1,1,1)$, $(2,2,1)$, $(3,3,1)$.\n    - Off-diagonal symmetric pairs: $(0,1,0.4)$, $(1,0,0.4)$, $(2,3,-0.7)$, $(3,2,-0.7)$.\n  - Sentiment matrix $S$ nonzeros:\n    - Off-diagonal symmetric pairs only: $(1,2,5)$, $(2,1,5)$, $(0,3,9)$, $(3,0,9)$, $(1,3,2)$, $(3,1,2)$.\n- Test case 3:\n  - Dimension: $n = 6$.\n  - Correlation matrix $C$ nonzeros:\n    - Diagonal: $(0,0,1)$, $(1,1,1)$, $(2,2,1)$, $(3,3,1)$, $(4,4,1)$, $(5,5,1)$.\n    - Off-diagonal symmetric pairs: $(0,2,0.2)$, $(2,0,0.2)$, $(1,2,-0.4)$, $(2,1,-0.4)$, $(3,4,0.9)$, $(4,3,0.9)$, $(4,5,0.6)$, $(5,4,0.6)$.\n  - Sentiment matrix $S$ nonzeros:\n    - Diagonal: $(1,1,2.0)$, $(3,3,-1.0)$, $(5,5,4.0)$.\n    - Off-diagonal symmetric pairs: $(0,2,-5.0)$, $(2,0,-5.0)$, $(1,2,1.0)$, $(2,1,1.0)$, $(3,4,10.0)$, $(4,3,10.0)$, $(4,5,-3.0)$, $(5,4,-3.0)$, $(0,1,7.0)$, $(2,3,8.0)$.\n\nAll matrix entries are real numbers. Indices $(i,j)$ satisfy $0 \\le i,j < n$. For each test case, entries listed are the complete set of nonzeros; any entry not listed must be treated as exactly zero. There are no duplicate index pairs in any list.\n\nFinal output format:\n- A single line containing $[r_1,r_2,r_3]$, where each $r_k$ is the list $[\\mathrm{nnz}(A), \\text{sum}, \\text{frob\\_sq}]$ for test case $k$, with the two floating-point values rounded to one decimal place as specified above.", "solution": "The problem has been analyzed and is deemed valid. It is a well-posed computational problem grounded in standard principles of linear algebra and computational science, with all necessary data provided. We will proceed with the solution.\n\nThe task is to compute three properties of a matrix $A$ resulting from the Hadamard product of two sparse matrices, $C$ and $S$. The Hadamard product is defined as $A = C \\circ S$, where each element $A_{ij}$ is the simple product of the corresponding elements of the source matrices: $A_{ij} = C_{ij} \\cdot S_{ij}$. The matrices are sparse, meaning most of their elements are zero. The properties to compute for each test case are: the number of nonzero entries, $\\mathrm{nnz}(A)$; the sum of all entries, $\\sum_{i,j} A_{ij}$; and the squared Frobenius norm, $\\lVert A \\rVert_F^2 = \\sum_{i,j} A_{ij}^2$.\n\nThe fundamental principle governing this problem is rooted in the definition of the Hadamard product for sparse matrices. An element $A_{ij}$ can be nonzero only if and only if both $C_{ij}$ and $S_{ij}$ are nonzero. If either $C_{ij}$ or $S_{ij}$ is zero, which is the case for most indices, then $A_{ij}$ is necessarily zero. This leads to a critical observation: the set of indices $(i,j)$ for which $A_{ij}$ is potentially nonzero is the intersection of the sets of nonzero indices of $C$ and $S$. Let $\\mathcal{N}(M)$ be the set of indices $(i,j)$ where the matrix $M$ has a nonzero entry. Then, the nonzero structure of $A$ is determined by:\n$$\n\\mathcal{N}(A) = \\mathcal{N}(C) \\cap \\mathcal{N}(S)\n$$\nAttempting to solve this problem by constructing the full $n \\times n$ dense matrices in memory would be a naive and computationally wasteful approach. For large dimensions $n$, this would lead to excessive memory consumption, proportional to $n^2$, and unnecessary computation involving vast numbers of zero-valued elements. The only correct and efficient method is to operate directly on the provided lists of nonzero entries, fully exploiting the sparsity of the data.\n\nOur algorithm is designed around this principle of intersecting sparse index sets. The most efficient data structure for testing membership and retrieving values associated with an index is a hash map, known as a dictionary in Python.\n\nThe algorithmic procedure is as follows:\n$1$. For a given pair of matrices $C$ and $S$, we first process one of them, say $C$, to build a hash map. We shall call this map `c_map`. The keys of this map will be the index tuples $(i, j)$, and the values will be the corresponding nonzero matrix entries $C_{ij}$. This step requires a single pass through the list of nonzeros of $C$, with a time complexity of $O(\\mathrm{nnz}(C))$.\n\n$2$. Next, we iterate through the list of nonzero entries for the second matrix, $S$. For each element $(i, j, S_{ij})$, we perform a lookup in `c_map` using the index tuple $(i, j)$ as the key. This hash map lookup has an average-case time complexity of $O(1)$.\n\n$3$. If the key $(i, j)$ is found in `c_map`, it signifies that we have located an index where both $C$ and $S$ have nonzero entries. We retrieve the value $C_{ij}$ from the map and compute the product $A_{ij} = C_{ij} \\cdot S_{ij}$.\n\n$4$. With each computed nonzero entry $A_{ij}$, we update our three aggregate quantities:\n- We increment the count of nonzero entries, $\\mathrm{nnz}(A)$.\n- We add $A_{ij}$ to the running total for the sum.\n- We add $A_{ij}^2$ to the running total for the squared Frobenius norm.\n\nThis process is repeated for all nonzero entries in $S$. After the iteration is complete, we have computed all required quantities for the matrix $A$. The total time complexity for this second phase is $O(\\mathrm{nnz}(S))$.\n\nThe overall time complexity of this algorithm is therefore $O(\\mathrm{nnz}(C) + \\mathrm{nnz}(S))$, which is linear with respect to the number of nonzero elements provided as input. The space complexity is $O(\\min(\\mathrm{nnz}(C), \\mathrm{nnz}(S)))$, as we only need to store one of the matrices in the hash map; for optimization, we would choose the smaller one. This approach is maximally efficient as it avoids any operations on zero-valued entries and leverages constant-time lookups.", "answer": "```python\nimport numpy as np\n# No other libraries are needed for this solution.\n\ndef solve():\n    \"\"\"\n    Solves the sparse matrix Hadamard product problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 5,\n            \"C_nonzeros\": [\n                (0, 0, 1.0), (1, 1, 1.0), (2, 2, 1.0), (3, 3, 1.0), (4, 4, 1.0),\n                (0, 1, 0.3), (1, 0, 0.3), (0, 3, -0.2), (3, 0, -0.2), (2, 4, 0.5),\n                (4, 2, 0.5), (1, 2, 0.1), (2, 1, 0.1)\n            ],\n            \"S_nonzeros\": [\n                (0, 0, 0.5), (2, 2, 1.5), (4, 4, -1.0), (0, 1, 10.0), (1, 0, 10.0),\n                (0, 3, 3.0), (3, 0, 3.0), (1, 3, 7.0), (2, 4, -2.0), (4, 2, -2.0),\n                (1, 2, 8.0), (2, 1, 8.0)\n            ]\n        },\n        {\n            \"n\": 4,\n            \"C_nonzeros\": [\n                (0, 0, 1.0), (1, 1, 1.0), (2, 2, 1.0), (3, 3, 1.0),\n                (0, 1, 0.4), (1, 0, 0.4), (2, 3, -0.7), (3, 2, -0.7)\n            ],\n            \"S_nonzeros\": [\n                (1, 2, 5.0), (2, 1, 5.0), (0, 3, 9.0), (3, 0, 9.0),\n                (1, 3, 2.0), (3, 1, 2.0)\n            ]\n        },\n        {\n            \"n\": 6,\n            \"C_nonzeros\": [\n                (0, 0, 1.0), (1, 1, 1.0), (2, 2, 1.0), (3, 3, 1.0), (4, 4, 1.0), (5, 5, 1.0),\n                (0, 2, 0.2), (2, 0, 0.2), (1, 2, -0.4), (2, 1, -0.4), (3, 4, 0.9),\n                (4, 3, 0.9), (4, 5, 0.6), (5, 4, 0.6)\n            ],\n            \"S_nonzeros\": [\n                (1, 1, 2.0), (3, 3, -1.0), (5, 5, 4.0), (0, 2, -5.0), (2, 0, -5.0),\n                (1, 2, 1.0), (2, 1, 1.0), (3, 4, 10.0), (4, 3, 10.0), (4, 5, -3.0),\n                (5, 4, -3.0), (0, 1, 7.0), (2, 3, 8.0)\n            ]\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        c_nonzeros = case[\"C_nonzeros\"]\n        s_nonzeros = case[\"S_nonzeros\"]\n\n        # To optimize, build the map from the smaller set of nonzeros.\n        if len(c_nonzeros) <= len(s_nonzeros):\n            map_nonzeros = c_nonzeros\n            iterate_nonzeros = s_nonzeros\n        else:\n            map_nonzeros = s_nonzeros\n            iterate_nonzeros = c_nonzeros\n\n        map_data = {(i, j): val for i, j, val in map_nonzeros}\n\n        nnz_A = 0\n        sum_A = 0.0\n        frob_sq_A = 0.0\n\n        for i, j, val_iter in iterate_nonzeros:\n            if (i, j) in map_data:\n                val_map = map_data[(i, j)]\n                a_ij = val_iter * val_map\n                \n                # An entry is counted if the product is non-zero.\n                # Given inputs are non-zero, this check is robust for potential floating point issues.\n                if a_ij != 0.0:\n                    nnz_A += 1\n                    sum_A += a_ij\n                    frob_sq_A += a_ij**2\n\n        all_results.append(f\"[{nnz_A},{sum_A:.1f},{frob_sq_A:.1f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"}, {"introduction": "This final practice demonstrates how sparse matrix techniques enable the analysis of complex economic systems that would otherwise be computationally prohibitive. You will calculate the Katz centrality, a sophisticated measure of a firm's influence within a supply chain network, by solving the linear system $(I - \\alpha A^{\\top}) x = \\mathbf{1}$. This exercise showcases the ultimate value of sparse representations: they are not just a storage optimization, but a key enabler for tackling large-scale, real-world problems in network economics [@problem_id:2432966].", "id": "2432966", "problem": "You are given directed, weighted supply chain networks represented by an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$, where $A_{ij}$ is the weight from firm $i$ to firm $j$. The Katz centrality $x \\in \\mathbb{R}^{n}$ of firms is defined as the unique solution to the linear system\n$$\n\\left(I - \\alpha A^{\\top}\\right) x = \\mathbf{1},\n$$\nwhere $I$ is the $n \\times n$ identity matrix, $\\alpha \\in \\mathbb{R}$ satisfies $\\alpha \\ge 0$, $A^{\\top}$ is the transpose of $A$, and $\\mathbf{1}$ is the $n$-dimensional vector of ones. Existence and uniqueness are guaranteed if $\\alpha < 1/\\rho(A)$, where $\\rho(A)$ is the spectral radius of $A$.\n\nYour task is to write a complete, runnable program that computes the Katz centrality vector $x$ for several specified sparse networks. Nodes are labeled by integers from $0$ to $n-1$. Each network is specified by the tuple $(n,\\ \\text{edge list},\\ \\alpha)$, where the edge list is a set of triples $(i,j,w)$ with $i,j \\in \\{0,\\dots,n-1\\}$ and $w \\in \\mathbb{R}$ denoting a directed edge $i \\to j$ with weight $w$. The adjacency matrix $A$ is defined by these edges, and any entry not listed is zero.\n\nCompute $x$ for each case below and report each vector with each component rounded to $6$ decimal places.\n\nTest suite:\n- Case $1$ (general acyclic supply chain, happy path):\n  - $n = 4$\n  - Edges: $(0,1,0.2)$, $(0,2,0.1)$, $(1,2,0.3)$, $(1,3,0.4)$, $(2,3,0.5)$\n  - $\\alpha = 0.85$\n- Case $2$ (boundary at $\\alpha = 0$):\n  - $n = 3$\n  - Edges: $(0,1,1)$, $(1,2,1)$, $(2,0,1)$\n  - $\\alpha = 0$\n- Case $3$ (near the invertibility boundary but feasible):\n  - $n = 3$\n  - Edges: $(0,1,1)$, $(1,2,1)$, $(2,0,1)$\n  - $\\alpha = 0.99$\n- Case $4$ (edge case: zero matrix):\n  - $n = 5$\n  - Edges: none\n  - $\\alpha = 0.7$\n\nNumerical accuracy requirement: Each component of each solution vector must be correct to within absolute error less than $10^{-6}$ with respect to the true solution.\n\nFinal output format: Your program should produce a single line of output containing the list of solution vectors (one per test case) as a single, comma-separated list enclosed in square brackets, where each vector is itself a comma-separated list enclosed in square brackets, and each number is a decimal with exactly $6$ digits after the decimal point. The order of the vectors in the outer list must match the order of the test cases listed above.", "solution": "The problem presented is a straightforward application of linear algebra to network analysis. We are tasked with computing the Katz centrality vector $x$ for a series of networks. The centrality vector $x$ is defined as the solution to the linear system:\n$$\n\\left(I - \\alpha A^{\\top}\\right) x = \\mathbf{1}\n$$\nHere, $I$ is the $n \\times n$ identity matrix, $A$ is the network's adjacency matrix, $\\alpha$ is a non-negative scalar attenuation factor, and $\\mathbf{1}$ is the $n$-dimensional vector of ones. The matrix $A^{\\top}$ is the transpose of $A$. The existence of a unique, non-negative solution $x$ is guaranteed under the condition $\\alpha < 1/\\rho(A)$, where $\\rho(A)$ is the spectral radius of $A$. The problem statement assures us this condition holds for all test cases.\n\nThe core of the task is to solve this system of linear equations, which can be denoted as $M x = b$, where the coefficient matrix is $M = I - \\alpha A^{\\top}$ and the right-hand side vector is $b = \\mathbf{1}$.\n\nThe networks are described as sparse, meaning the adjacency matrix $A$ contains a vast majority of zero entries. For computational efficiency, it is imperative to use data structures designed for sparse matrices, rather than dense $n \\times n$ arrays. The `scipy.sparse` library in Python provides the necessary tools.\n\nThe algorithm proceeds as follows for each test case $(n, \\text{edge list}, \\alpha)$:\n\n1.  **Construct the Matrix $M = I - \\alpha A^{\\top}$**:\n    *   First, we construct the transpose of the adjacency matrix, $A^{\\top}$, in a sparse format. The input edge list provides entries for $A$ in the form of triples $(i, j, w)$, which correspond to the matrix element $A_{ij} = w$. Consequently, for the transpose matrix $A^{\\top}$, this corresponds to the element $(A^{\\top})_{ji} = w$. We can efficiently assemble $A^{\\top}$ by providing lists of row indices, column indices, and data values to a sparse matrix constructor, for instance, `scipy.sparse.coo_matrix`.\n    *   The identity matrix $I$ of size $n \\times n$ is constructed using a function like `scipy.sparse.identity(n)`.\n    *   The matrix $M$ is then computed as the sparse matrix operation $I - \\alpha \\cdot A^{\\top}$. `SciPy` correctly handles scalar multiplication and subtraction for its sparse matrix formats.\n\n2.  **Construct the Vector $b$**:\n    *   The vector $b = \\mathbf{1}$ is an $n$-dimensional vector where every component is $1$. This is created using `numpy.ones(n)`.\n\n3.  **Solve the Linear System $M x = b$**:\n    *   With the sparse matrix $M$ and the dense vector $b$ prepared, we solve for $x$ using a specialized sparse linear solver. The function `scipy.sparse.linalg.spsolve(M, b)` is designed for this purpose. It employs efficient numerical methods (such as direct methods based on LU decomposition for sparse matrices) that avoid the prohibitive memory and computational costs of inverting a dense matrix.\n\n4.  **Handle Special Cases**:\n    *   It is instructive to consider the boundary conditions.\n    *   For Case $2$, where $\\alpha=0$, the equation simplifies to $(I - 0 \\cdot A^{\\top}) x = \\mathbf{1}$, which is $I x = \\mathbf{1}$. The solution is trivially $x = \\mathbf{1}$.\n    *   For Case $4$, where the edge list is empty, the adjacency matrix $A$ is the zero matrix. Thus, $A^{\\top}$ is also the zero matrix. The equation becomes $(I - \\alpha \\cdot 0) x = \\mathbf{1}$, which again is $I x = \\mathbf{1}$ and yields $x = \\mathbf{1}$, regardless of the value of $\\alpha$.\n    *   In both these instances, the computational machinery of sparse solvers is unnecessary, and the result can be stated directly. Our implementation will recognize these simple cases.\n\nFor the general cases, such as Case $1$ and Case $3$, the full numerical procedure is required. For Case $1$, the matrix $A$ is acyclic, resulting in a nilpotent $A^{\\top}$. The matrix $M = I - \\alpha A^{\\top}$ is lower triangular, and the system could be solved analytically via forward substitution, providing a good check for the numerical result. For Case $3$, $\\alpha=0.99$ is close to the stability boundary given $\\rho(A)=1$ for that specific matrix. The matrix $M$ becomes nearly singular, but the problem is still well-posed, and a numerical solver will produce a valid solution, which we analytically derived to be $x_i = 1/(1-\\alpha) = 100$ for all $i$.\n\nThe implementation will follow this logic, processing each test case, computing the vector $x$, and formatting the output to the specified precision of $6$ decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import coo_matrix, identity\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Solves the Katz centrality problem for a batch of test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: General acyclic supply chain\n        {'n': 4, 'edges': [(0, 1, 0.2), (0, 2, 0.1), (1, 2, 0.3), (1, 3, 0.4), (2, 3, 0.5)], 'alpha': 0.85},\n        # Case 2: Boundary at alpha = 0\n        {'n': 3, 'edges': [(0, 1, 1), (1, 2, 1), (2, 0, 1)], 'alpha': 0},\n        # Case 3: Near the invertibility boundary\n        {'n': 3, 'edges': [(0, 1, 1), (1, 2, 1), (2, 0, 1)], 'alpha': 0.99},\n        # Case 4: Zero matrix\n        {'n': 5, 'edges': [], 'alpha': 0.7},\n    ]\n\n    all_results_str = []\n\n    for case in test_cases:\n        n = case['n']\n        edges = case['edges']\n        alpha = case['alpha']\n\n        # The Katz centrality vector is the solution x to the linear system\n        # (I - alpha * A_transpose) * x = 1\n        # where 1 is a vector of ones.\n\n        # Handle trivial cases for efficiency and correctness.\n        # If alpha is 0 or there are no edges (A is zero matrix),\n        # the equation becomes I*x = 1, so x is a vector of ones.\n        if alpha == 0 or not edges:\n            x = np.ones(n)\n        else:\n            # Construct the transpose of the adjacency matrix A in sparse format.\n            # For an edge (i, j, w) in A, A[i,j] = w.\n            # In A_transpose, this becomes A_transpose[j,i] = w.\n            row_indices = [j for i, j, w in edges]\n            col_indices = [i for i, j, w in edges]\n            data = [w for i, j, w in edges]\n\n            # Use Coordinate (COO) format for easy construction, then convert to\n            # Compressed Sparse Row (CSR) for efficient arithmetic and solving.\n            A_transpose = coo_matrix((data, (row_indices, col_indices)), shape=(n, n)).asformat('csr')\n\n            # Create the identity matrix in sparse CSR format.\n            I = identity(n, format='csr')\n\n            # Form the coefficient matrix M = I - alpha * A_transpose.\n            M = I - alpha * A_transpose\n\n            # The right-hand side vector `b` is a vector of ones.\n            b = np.ones(n)\n\n            # Solve the sparse linear system Mx = b.\n            x = spsolve(M, b)\n\n        # Format the resulting vector according to the problem specification.\n        # Each component is rounded to 6 decimal places.\n        formatted_x = [f\"{val:.6f}\" for val in x]\n        all_results_str.append(f\"[{','.join(formatted_x)}]\")\n\n    # Print the final result in the exact single-line format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"}]}