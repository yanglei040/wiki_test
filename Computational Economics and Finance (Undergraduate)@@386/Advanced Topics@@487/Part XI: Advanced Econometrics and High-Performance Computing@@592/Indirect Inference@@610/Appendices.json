{"hands_on_practices": [{"introduction": "This first practice provides a complete, hands-on implementation of an indirect inference estimator from the ground up. We will use the intuitive physical model of a Galton \"plinko\" board to see how observable outcomes—the final distribution of dropped balls—can be used to infer the unobservable structural parameters of the system. This exercise solidifies the core workflow of indirect inference: simulating a structural model, computing auxiliary statistics, and minimizing the distance between observed and simulated statistics to find the best-fit parameters. [@problem_id:2401820]", "id": "2401820", "problem": "Consider a simplified model of a Galton-style \"plinko\" board used to illustrate stochastic processes. A large number of identical balls are dropped, one at a time, from the top. Each ball encounters $T$ rows of pegs. At each peg, the ball deflects either to the right or to the left. Let $S_{it}\\in\\{-d, +d\\}$ denote the horizontal increment of ball $i$ at row $t$, where $d&gt;0$ is the fixed horizontal step size. Assume $\\mathbb{P}(S_{it}=+d)=p$ and $\\mathbb{P}(S_{it}=-d)=1-p$, with $0\\le p\\le 1$, independently across balls and rows. The final horizontal position of ball $i$ after $T$ rows is $X_i=\\sum_{t=1}^{T} S_{it}$.\n\nLet $\\theta=(p,d)$ be the unknown structural parameter vector. You observe a dataset of $N_{\\text{obs}}$ independent final positions $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$ generated by the true $\\theta_0=(p_0,d_0)$. You are to estimate $\\theta$ using Indirect Inference (II), defined as follows. Choose an auxiliary model: a Normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$ whose auxiliary parameters are the sample mean and the sample variance computed from the final positions. Define the auxiliary statistic mapping $m(\\cdot)$ as $m(\\{x_i\\}) = \\big(\\bar{x}, s^2\\big)$, where $\\bar{x}$ is the sample mean and $s^2$ is the sample variance with divisor $N$ (population variance). For a candidate $\\theta$, let $m_{\\text{sim}}(\\theta)$ denote the auxiliary statistics computed from data generated under the structural model with parameter $\\theta$. The II estimator minimizes the quadratic distance\n$$\nQ(\\theta)=\\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big)^{\\top} W \\big(m_{\\text{obs}}-m_{\\text{sim}}(\\theta)\\big),\n$$\nwith weight matrix $W=I_2$, where $m_{\\text{obs}}=m(\\{X_i\\}_{i=1}^{N_{\\text{obs}}})$.\n\nYour task is to write a complete program that, for each test case specified below, does all of the following from first principles:\n- Generate the observed dataset $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$ using the structural model with the given true parameter $\\theta_0$ and the specified random seed.\n- Compute the observed auxiliary statistics $m_{\\text{obs}}$.\n- Compute the Indirect Inference estimate $\\hat{\\theta}=(\\hat{p},\\hat{d})$ as a minimizer of $Q(\\theta)$ over the admissible set $\\{(p,d): 0\\le p\\le 1, \\ d&gt;0\\}$.\n- Return $\\hat{\\theta}$ for each test case as specified in the final output format.\n\nAll probabilities must be represented as decimals in $[0,1]$. No physical units are involved. All angles, if any, are irrelevant to this problem. The estimation must be framed strictly in terms of the definitions given above.\n\nTest suite. For each item, you are given the number of rows $T$, the true probability $p_0$, the true step size $d_0$, the observed sample size $N_{\\text{obs}}$, and the random seed to generate the observed data. The random number generator must be initialized with the provided seed for observed data generation. The final sample variance must use the divisor $N_{\\text{obs}}$ (population variance).\n- Test case $1$: $T=20$, $p_0=0.6$, $d_0=1.2$, $N_{\\text{obs}}=50000$, seed $=13579$.\n- Test case $2$: $T=30$, $p_0=0.5$, $d_0=1.0$, $N_{\\text{obs}}=50000$, seed $=24680$.\n- Test case $3$: $T=25$, $p_0=0.9$, $d_0=0.8$, $N_{\\text{obs}}=50000$, seed $=11235$.\n\nFinal output format. Your program should produce a single line of output containing the results as a list of lists, one inner list per test case, in the same order as listed above. Each inner list must contain two decimal numbers $[\\hat{p},\\hat{d}]$ rounded to exactly six digits after the decimal point. For example, the output line must look like:\n[[p1,d1],[p2,d2],[p3,d3]]\nwith no spaces inserted, where each $p_j$ and $d_j$ is a decimal rounded to six places.", "solution": "The problem presented is a valid and well-posed exercise in parameter estimation using the method of Indirect Inference. It is scientifically grounded in probability theory and statistical estimation, and all necessary information for its resolution is provided. We shall proceed with a complete solution.\n\nThe core of the problem is to estimate the structural parameters $\\theta=(p,d)$ of a stochastic process, given a set of observations of the process's final state. The structural model describes a ball moving on a Galton board with $T$ rows. At each row $t \\in \\{1, \\dots, T\\}$, the ball's horizontal position changes by an increment $S_t$, which is $+d$ with probability $p$ and $-d$ with probability $1-p$. The final horizontal position after $T$ rows is $X = \\sum_{t=1}^{T} S_t$.\n\nFirst, we establish the analytical relationship between the structural parameters $\\theta=(p,d)$ and the moments of the final position $X$. Let $K$ be a random variable representing the number of deflections to the right, which follows a Binomial distribution, $K \\sim \\text{Binomial}(T,p)$. The total number of deflections is $T$, so there are $T-K$ deflections to the left. The final position $X$ can be expressed as:\n$$\nX = K \\cdot (+d) + (T-K) \\cdot (-d) = d(K - (T-K)) = d(2K - T)\n$$\nThe mean and variance of the Binomial variable $K$ are $\\mathbb{E}[K] = Tp$ and $\\text{Var}(K) = Tp(1-p)$. Using the properties of expectation and variance, we can find the mean $\\mu_X$ and variance $\\sigma_X^2$ of the final position $X$:\n$$\n\\mu(p, d) = \\mathbb{E}[X] = \\mathbb{E}[d(2K - T)] = d(2\\mathbb{E}[K] - T) = d(2Tp - T) = Td(2p-1)\n$$\n$$\n\\sigma^2(p, d) = \\text{Var}(X) = \\text{Var}(d(2K - T)) = d^2 \\text{Var}(2K) = 4d^2\\text{Var}(K) = 4d^2Tp(1-p)\n$$\nThese two equations form the binding function that maps the structural parameters $\\theta=(p,d)$ to the theoretical moments of the observable data.\n\nThe problem requires estimation by Indirect Inference (II). We are given an auxiliary model, which is a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, and the auxiliary statistics are the sample mean $\\bar{x}$ and sample variance $s^2$ (with divisor $N_{\\text{obs}}$). We have a set of $N_{\\text{obs}}$ observations $\\{X_i\\}_{i=1}^{N_{\\text{obs}}}$ generated from the true parameters $\\theta_0=(p_0,d_0)$. We calculate the observed auxiliary statistic vector $m_{\\text{obs}} = (\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$.\n\nThe II estimator $\\hat{\\theta}$ is found by minimizing the objective function:\n$$\nQ(\\theta) = (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))^{\\top} W (m_{\\text{obs}} - m_{\\text{sim}}(\\theta))\n$$\nwhere the weighting matrix is the identity matrix, $W=I_2$. The term $m_{\\text{sim}}(\\theta)$ represents the auxiliary statistics produced by the model with parameter $\\theta$. A crucial point is the evaluation of $m_{\\text{sim}}(\\theta)$. While one could perform a simulation for each value of $\\theta$ during optimization, this would introduce stochasticity into the objective function, complicating the minimization and requiring specification of an inner simulation size, which is not provided. The correct and standard approach in this context is to use the analytical moments derived above, which correspond to the expectation of the auxiliary statistics as the simulation size approaches infinity.\nThus, $m_{\\text{sim}}(\\theta) = (\\mu(p,d), \\sigma^2(p,d))$. The objective function becomes:\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - \\mu(p,d))^2 + (s^2_{\\text{obs}} - \\sigma^2(p,d))^2\n$$\n$$\nQ(p,d) = (\\bar{x}_{\\text{obs}} - Td(2p-1))^2 + (s^2_{\\text{obs}} - 4Td^2p(1-p))^2\n$$\nThe estimation task is to find the values $(\\hat{p}, \\hat{d})$ that minimize this function $Q(p,d)$ subject to the constraints $p \\in [0,1]$ and $d > 0$. This is a standard non-linear constrained optimization problem.\n\nThe algorithmic procedure to solve the problem for each test case is as follows:\n1.  **Generate Observed Data**: For a given test case with parameters $T$, $p_0$, $d_0$, $N_{\\text{obs}}$, and a specific random seed, generate $N_{\\text{obs}}$ observations. This is achieved efficiently by first sampling $N_{\\text{obs}}$ values $\\{K_i\\}_{i=1}^{N_{\\text{obs}}}$ from the distribution $\\text{Binomial}(T, p_0)$. Then, the observed final positions are calculated as $X_i = d_0(2K_i - T)$.\n2.  **Compute Observed Statistics**: From the generated dataset $\\{X_i\\}$, calculate the observed auxiliary statistics $m_{\\text{obs}}=(\\bar{x}_{\\text{obs}}, s^2_{\\text{obs}})$. Specifically, $\\bar{x}_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} X_i$ and $s^2_{\\text{obs}} = \\frac{1}{N_{\\text{obs}}} \\sum_{i=1}^{N_{\\text{obs}}} (X_i - \\bar{x}_{\\text{obs}})^2$.\n3.  **Numerical Optimization**: Minimize the objective function $Q(p,d)$ with respect to $p$ and $d$. We employ a numerical optimization algorithm suitable for constrained problems, such as a quasi-Newton method with box constraints (e.g., L-BFGS-B). The search space is defined by the bounds $p \\in [0,1]$ and $d \\in (0, \\infty)$. A small positive lower bound is used for $d$ for numerical stability.\n4.  **Report Estimate**: The pair of values $(\\hat{p}, \\hat{d})$ that minimizes $Q(p,d)$ constitutes the Indirect Inference estimate for the given test case. The results from all test cases are then collected and formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to solve the Indirect Inference estimation problem for all test cases.\n    \"\"\"\n\n    # Test cases as specified in the problem statement.\n    test_cases = [\n        # (T, p0, d0, N_obs, seed)\n        (20, 0.6, 1.2, 50000, 13579),\n        (30, 0.5, 1.0, 50000, 24680),\n        (25, 0.9, 0.8, 50000, 11235),\n    ]\n\n    all_results = []\n\n    for T, p0, d0, N_obs, seed in test_cases:\n        # Step 1: Generate the observed dataset {X_i}\n        # Use a more efficient method based on the Binomial distribution.\n        # k ~ Binomial(T, p) represents the number of right steps.\n        # X = k * (+d) + (T - k) * (-d) = d * (2k - T)\n        rng = np.random.default_rng(seed)\n        k_obs = rng.binomial(T, p0, size=N_obs)\n        x_obs = d0 * (2 * k_obs - T)\n\n        # Step 2: Compute the observed auxiliary statistics m_obs = (mean, variance)\n        mu_obs = np.mean(x_obs)\n        # Use population variance (divisor N), as specified (ddof=0 is default for np.var)\n        var_obs = np.var(x_obs)\n        m_obs = (mu_obs, var_obs)\n        \n        # Step 3: Define the objective function Q(theta) for minimization.\n        # theta is a tuple (p, d).\n        def objective_function(theta, T_val, m_obs_val):\n            p, d = theta\n            mu_obs_val, var_obs_val = m_obs_val\n\n            # Analytical moments from the structural model\n            mu_sim = T_val * d * (2 * p - 1)\n            var_sim = 4 * T_val * (d**2) * p * (1 - p)\n            \n            # Quadratic objective function Q(theta)\n            q_val = (mu_obs_val - mu_sim)**2 + (var_obs_val - var_sim)**2\n            return q_val\n\n        # Step 4: Perform numerical optimization to find the II estimate.\n        # Initial guess for the parameters (p, d)\n        initial_guess = [0.5, 1.0]\n\n        # Bounds for the parameters: 0 <= p <= 1 and d > 0.\n        # Use a small positive number for the lower bound of d for numerical stability.\n        bounds = [(0.0, 1.0), (1e-9, None)]\n\n        # Minimize the objective function. L-BFGS-B is suitable for box constraints.\n        result = minimize(\n            fun=objective_function,\n            x0=initial_guess,\n            args=(T, m_obs),\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n\n        # The optimized parameters are the II estimates\n        p_hat, d_hat = result.x\n        all_results.append([p_hat, d_hat])\n\n    # Final print statement in the exact required format.\n    # Create the list of lists with rounded values.\n    # e.g., [[0.600012, 1.199998], [0.500001, 1.000003], [0.900005, 0.799989]]\n    # Then convert to string and remove spaces to match the output format.\n    final_list_formatted = [[round(p, 6), round(d, 6)] for p, d in all_results]\n    output_str = str(final_list_formatted).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n\n```"}, {"introduction": "Having implemented an estimator, we now turn to a crucial theoretical aspect: its efficiency. This practice explores how the choice of the weighting matrix, $\\Omega$, in the objective function affects the precision of our parameter estimates. By deriving and calculating the asymptotic variance of an estimator under different weighting schemes, you will gain a concrete understanding of why the \"optimal\" weighting matrix leads to more precise estimates and narrower confidence intervals. [@problem_id:2401803]", "id": "2401803", "problem": "Consider a structural autoregressive moving-average model of order $(1,1)$ with structural parameter vector $\\theta = (\\phi,\\psi)'$, and suppose we estimate $\\theta$ by indirect inference using an auxiliary statistic $\\hat{s} \\in \\mathbb{R}^{3}$ that consists of three smooth functions of the data, for example, sample second-order moments and low-order autocovariances. Let $s(\\theta) \\in \\mathbb{R}^{3}$ denote the population counterpart of the auxiliary statistics implied by the structural model. Assume the following foundations:\n- By the Central Limit Theorem (CLT), as the sample size $T \\to \\infty$, the auxiliary estimator satisfies $\\sqrt{T}\\,(\\hat{s} - s(\\theta_{0})) \\xrightarrow{d} \\mathcal{N}(0, V)$ for a positive definite covariance matrix $V \\in \\mathbb{R}^{3 \\times 3}$.\n- The mapping $\\theta \\mapsto s(\\theta)$ is continuously differentiable at the true value $\\theta_{0}$ with full column rank Jacobian $G = \\left.\\frac{\\partial s(\\theta)}{\\partial \\theta'}\\right|_{\\theta=\\theta_{0}} \\in \\mathbb{R}^{3 \\times 2}$.\n- The simulation noise in constructing $s(\\theta)$ is negligible relative to sampling variability (for instance, due to an infinitely long simulated path per $\\theta$), so $s(\\theta)$ is treated as deterministic.\n\nDefine the indirect inference estimator as the minimizer of the quadratic criterion $Q_{T}(\\theta) = [\\hat{s} - s(\\theta)]' \\Omega [\\hat{s} - s(\\theta)]$, where $\\Omega \\in \\mathbb{R}^{3 \\times 3}$ is a symmetric positive definite weighting matrix. Starting from the CLT and differentiability assumptions above, derive the large-sample approximation for the covariance matrix of $\\sqrt{T}\\,(\\hat{\\theta} - \\theta_{0})$ as a function of $G$, $V$, and $\\Omega$.\n\nThen evaluate the effect of the weighting matrix on the asymptotic $95\\%$ confidence interval for the first structural parameter $\\phi$ using the following numerically specified, locally valid objects at $\\theta_{0}$:\n- The Jacobian matrix $G$ is\n$$\nG \\;=\\; \\begin{bmatrix}\n1.2 & 0.3 \\\\\n0.5 & -0.4 \\\\\n-0.2 & 0.8\n\\end{bmatrix}.\n$$\n- The asymptotic covariance matrix of the auxiliary statistics $V$ is\n$$\nV \\;=\\; \\begin{bmatrix}\n0.25 & 0.05 & 0.00 \\\\\n0.05 & 0.20 & 0.04 \\\\\n0.00 & 0.04 & 0.16\n\\end{bmatrix}.\n$$\n\nCompute the ratio $R$ of the asymptotic $95\\%$ confidence interval lengths for $\\phi$ under the two weighting schemes:\n- Identity weighting: $\\Omega = I_{3}$.\n- Optimal weighting: $\\Omega = V^{-1}$.\n\nExpress $R$ as $R = L_{I} / L_{\\mathrm{opt}}$, where $L_{I}$ and $L_{\\mathrm{opt}}$ are the asymptotic $95\\%$ confidence interval lengths under $\\Omega = I_{3}$ and $\\Omega = V^{-1}$, respectively. Round your final numerical answer for $R$ to four significant figures. State only the number; no units are required.", "solution": "The problem will be validated before a solution is attempted.\n\nFirst, I will extract the given information verbatim.\n- Model: Structural Autoregressive Moving-Average of order $(1,1)$, ARMA($1,1$).\n- Structural parameter vector: $\\theta = (\\phi,\\psi)' \\in \\mathbb{R}^{2}$.\n- Auxiliary statistic estimator: $\\hat{s} \\in \\mathbb{R}^{3}$.\n- Population counterpart of auxiliary statistics: $s(\\theta) \\in \\mathbb{R}^{3}$.\n- Asymptotic distribution of $\\hat{s}$: $\\sqrt{T}\\,(\\hat{s} - s(\\theta_{0})) \\xrightarrow{d} \\mathcal{N}(0, V)$ as sample size $T \\to \\infty$. $V \\in \\mathbb{R}^{3 \\times 3}$ is a positive definite covariance matrix. $\\theta_0$ is the true parameter value.\n- Differentiability of mapping $s(\\theta)$: The map $\\theta \\mapsto s(\\theta)$ is continuously differentiable at $\\theta_{0}$.\n- Jacobian matrix at $\\theta_0$: $G = \\left.\\frac{\\partial s(\\theta)}{\\partial \\theta'}\\right|_{\\theta=\\theta_{0}} \\in \\mathbb{R}^{3 \\times 2}$, with full column rank.\n- Simulation noise: Assumed to be negligible.\n- Indirect inference estimator definition: $\\hat{\\theta} = \\arg\\min_{\\theta} Q_{T}(\\theta)$, where $Q_{T}(\\theta) = [\\hat{s} - s(\\theta)]' \\Omega [\\hat{s} - s(\\theta)]$.\n- Weighting matrix: $\\Omega \\in \\mathbb{R}^{3 \\times 3}$ is symmetric and positive definite.\n- Specific Jacobian matrix: $G = \\begin{bmatrix} 1.2 & 0.3 \\\\ 0.5 & -0.4 \\\\ -0.2 & 0.8 \\end{bmatrix}$.\n- Specific covariance matrix: $V = \\begin{bmatrix} 0.25 & 0.05 & 0.00 \\\\ 0.05 & 0.20 & 0.04 \\\\ 0.00 & 0.04 & 0.16 \\end{bmatrix}$.\n- Task: Derive the large-sample covariance matrix of $\\sqrt{T}\\,(\\hat{\\theta} - \\theta_{0})$. Then compute the ratio $R = L_{I} / L_{\\mathrm{opt}}$ of asymptotic $95\\%$ confidence interval lengths for $\\phi$ under identity weighting ($\\Omega = I_{3}$) and optimal weighting ($\\Omega = V^{-1}$). Round the final numerical answer for $R$ to four significant figures.\n\nNext, I will validate the problem statement against the required criteria.\n1.  **Scientifically Grounded**: The problem is a standard exercise in asymptotic theory for estimators based on the Generalized Method of Moments (GMM), of which indirect inference is a specific case. The assumptions provided (CLT for auxiliary statistics, differentiability of the binding function) are the foundational elements for deriving the asymptotic distribution of such estimators. The problem is firmly rooted in established principles of econometrics and statistics. It is scientifically sound.\n2.  **Well-Posed**: The problem is clearly stated. All necessary data, including the functional forms of the matrices $G$ and $V$, are provided. The dimensions of the matrices and vectors are consistent: $\\theta$ is $2 \\times 1$, $s$ is $3 \\times 1$, $G$ is $3 \\times 2$, and $V$ and $\\Omega$ are $3 \\times 3$. The objective is to derive a known formula and apply it to numerical data to compute a specific ratio. The existence and uniqueness of a solution are guaranteed by the assumptions (e.g., full column rank of $G$, positive definiteness of $\\Omega$ and $V$). The problem is well-posed.\n3.  **Objective**: The problem is stated using precise, unambiguous mathematical and statistical terminology. It is free from any subjective or opinion-based content.\n\nThe verdict is that the problem is **valid**. A solution will now be derived.\n\nThe indirect inference estimator $\\hat{\\theta}$ minimizes the quadratic form $Q_{T}(\\theta) = [\\hat{s} - s(\\theta)]' \\Omega [\\hat{s} - s(\\theta)]$. The first-order condition (FOC) for minimization is obtained by setting the gradient of $Q_T(\\theta)$ with respect to $\\theta$ to zero.\n$$\n\\frac{\\partial Q_{T}(\\theta)}{\\partial \\theta} = -2 \\left(\\frac{\\partial s(\\theta)}{\\partial \\theta'}\\right)' \\Omega [\\hat{s} - s(\\theta)] = 0\n$$\nAt the estimator $\\hat{\\theta}$, this condition holds:\n$$\n\\left(\\frac{\\partial s(\\hat{\\theta})}{\\partial \\theta'}\\right)' \\Omega [\\hat{s} - s(\\hat{\\theta})] = 0\n$$\nBy consistency, $\\hat{\\theta} \\to \\theta_0$ as $T \\to \\infty$. So we can replace the Jacobian at $\\hat{\\theta}$ with the Jacobian at the true value $\\theta_0$, $G = \\frac{\\partial s(\\theta_0)}{\\partial \\theta'}$. The FOC is approximately $G' \\Omega [\\hat{s} - s(\\hat{\\theta})] \\approx 0$.\nNext, apply a first-order Taylor expansion of $s(\\hat{\\theta})$ around $\\theta_0$:\n$$\ns(\\hat{\\theta}) \\approx s(\\theta_0) + G (\\hat{\\theta} - \\theta_0)\n$$\nSubstituting this into the FOC:\n$$\nG' \\Omega [\\hat{s} - (s(\\theta_0) + G (\\hat{\\theta} - \\theta_0))] \\approx 0\n$$\nRearranging the terms gives:\n$$\nG' \\Omega [\\hat{s} - s(\\theta_0)] \\approx (G' \\Omega G) (\\hat{\\theta} - \\theta_0)\n$$\nSince $G$ has full column rank and $\\Omega$ is positive definite, the matrix $G' \\Omega G$ is invertible. We can solve for $(\\hat{\\theta} - \\theta_0)$:\n$$\n(\\hat{\\theta} - \\theta_0) \\approx (G' \\Omega G)^{-1} G' \\Omega [\\hat{s} - s(\\theta_0)]\n$$\nMultiplying by $\\sqrt{T}$:\n$$\n\\sqrt{T} (\\hat{\\theta} - \\theta_0) \\approx (G' \\Omega G)^{-1} G' \\Omega [\\sqrt{T} (\\hat{s} - s(\\theta_0))]\n$$\nWe are given that $\\sqrt{T}(\\hat{s} - s(\\theta_0)) \\xrightarrow{d} \\mathcal{N}(0, V)$. The expression for $\\sqrt{T} (\\hat{\\theta} - \\theta_0)$ is a linear transformation of this asymptotically normal vector. Therefore, $\\sqrt{T} (\\hat{\\theta} - \\theta_0)$ is also asymptotically normal with mean zero and a covariance matrix given by the sandwich formula:\n$$\n\\Sigma_\\theta = \\text{AsyCov}(\\sqrt{T} (\\hat{\\theta} - \\theta_0)) = (G' \\Omega G)^{-1} (G' \\Omega V \\Omega G) (G' \\Omega G)^{-1}\n$$\nThis is the general expression for the asymptotic covariance matrix.\n\nNow, we evaluate this for the two specified weighting matrices. The parameter of interest is $\\phi$, the first component of $\\theta$. We need the $(1,1)$ element of $\\Sigma_\\theta$, denoted $(\\Sigma_\\theta)_{11}$.\n\nCase 1: Identity weighting, $\\Omega = I_3$.\nThe covariance matrix simplifies to $\\Sigma_I = (G'G)^{-1} (G'VG) (G'G)^{-1}$.\nFirst, calculate the required matrix products:\n$$\nG'G = \\begin{pmatrix} 1.2 & 0.5 & -0.2 \\\\ 0.3 & -0.4 & 0.8 \\end{pmatrix} \\begin{pmatrix} 1.2 & 0.3 \\\\ 0.5 & -0.4 \\\\ -0.2 & 0.8 \\end{pmatrix} = \\begin{pmatrix} 1.73 & 0 \\\\ 0 & 0.89 \\end{pmatrix}\n$$\n$$\n(G'G)^{-1} = \\begin{pmatrix} 1/1.73 & 0 \\\\ 0 & 1/0.89 \\end{pmatrix}\n$$\nNext, we compute the middle term $G'VG$:\n$$\nG'VG = \\begin{pmatrix} 1.2 & 0.5 & -0.2 \\\\ 0.3 & -0.4 & 0.8 \\end{pmatrix} \\begin{pmatrix} 0.25 & 0.05 & 0.00 \\\\ 0.05 & 0.20 & 0.04 \\\\ 0.00 & 0.04 & 0.16 \\end{pmatrix} \\begin{pmatrix} 1.2 & 0.3 \\\\ 0.5 & -0.4 \\\\ -0.2 & 0.8 \\end{pmatrix} = \\begin{pmatrix} 0.4684 & 0.0271 \\\\ 0.0271 & 0.1193 \\end{pmatrix}\n$$\nThen, $\\Sigma_I$ is:\n$$\n\\Sigma_I = \\begin{pmatrix} \\frac{1}{1.73} & 0 \\\\ 0 & \\frac{1}{0.89} \\end{pmatrix} \\begin{pmatrix} 0.4684 & 0.0271 \\\\ 0.0271 & 0.1193 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{1.73} & 0 \\\\ 0 & \\frac{1}{0.89} \\end{pmatrix} = \\begin{pmatrix} \\frac{0.4684}{1.73^2} & \\dots \\\\ \\dots & \\dots \\end{pmatrix}\n$$\nThe asymptotic variance for $\\hat{\\phi}$ is the $(1,1)$ element:\n$$\n(\\Sigma_I)_{11} = \\frac{0.4684}{1.73^2} = \\frac{0.4684}{2.9929} \\approx 0.15650339\n$$\n\nCase 2: Optimal weighting, $\\Omega = V^{-1}$.\nThe general formula for $\\Sigma_\\theta$ simplifies significantly:\n$$\n\\Sigma_{\\mathrm{opt}} = (G'V^{-1}G)^{-1} (G'V^{-1}V V^{-1}G) (G'V^{-1}G)^{-1} = (G'V^{-1}G)^{-1}\n$$\nFirst, we must compute $V^{-1}$. The determinant of $V$ is:\n$$\n\\det(V) = 0.25(0.20 \\times 0.16 - 0.04^2) - 0.05(0.05 \\times 0.16 - 0) = 0.0072\n$$\nThe inverse is $V^{-1} = \\frac{1}{\\det(V)}\\text{adj}(V)$, where $\\text{adj}(V)$ is the adjugate matrix of $V$:\n$$\nV^{-1} = \\frac{1}{0.0072} \\begin{pmatrix} 0.0304 & -0.008 & 0.002 \\\\ -0.008 & 0.04 & -0.01 \\\\ 0.002 & -0.01 & 0.0475 \\end{pmatrix}\n$$\nNow compute $G'V^{-1}G$:\n$$\nG'V^{-1}G = \\frac{1}{0.0072} \\begin{pmatrix} 1.2 & 0.5 & -0.2 \\\\ 0.3 & -0.4 & 0.8 \\end{pmatrix} \\begin{pmatrix} 0.0304 & -0.008 & 0.002 \\\\ -0.008 & 0.04 & -0.01 \\\\ 0.002 & -0.01 & 0.0475 \\end{pmatrix} \\begin{pmatrix} 1.2 & 0.3 \\\\ 0.5 & -0.4 \\\\ -0.2 & 0.8 \\end{pmatrix}\n$$\nPerforming the multiplication yields:\n$$\nG'V^{-1}G = \\frac{1}{0.0072} \\begin{pmatrix} 0.047116 & -0.005016 \\\\ -0.005016 & 0.048816 \\end{pmatrix}\n$$\nTo find $\\Sigma_{\\mathrm{opt}}$, we invert this matrix:\n$$\n\\Sigma_{\\mathrm{opt}} = (G'V^{-1}G)^{-1} = 0.0072 \\left( \\begin{pmatrix} 0.047116 & -0.005016 \\\\ -0.005016 & 0.048816 \\end{pmatrix} \\right)^{-1}\n$$\nThe determinant of the inner matrix is $D = (0.047116)(0.048816) - (-0.005016)^2 \\approx 0.00227456$.\n$$\n\\Sigma_{\\mathrm{opt}} = \\frac{0.0072}{D} \\begin{pmatrix} 0.048816 & 0.005016 \\\\ 0.005016 & 0.047116 \\end{pmatrix}\n$$\nThe asymptotic variance for $\\hat{\\phi}$ is the $(1,1)$ element:\n$$\n(\\Sigma_{\\mathrm{opt}})_{11} = \\frac{0.0072 \\times 0.048816}{D} = \\frac{0.0003514752}{0.00227456} \\approx 0.15452441\n$$\n\nFinally, we compute the ratio of confidence interval lengths. The length of an asymptotic $95\\%$ confidence interval for $\\phi$ is $L = 2 z_{0.975} \\sqrt{(\\Sigma_\\theta)_{11}/T}$. The ratio $R$ is therefore:\n$$\nR = \\frac{L_I}{L_{\\mathrm{opt}}} = \\frac{2 z_{0.975} \\sqrt{(\\Sigma_I)_{11}/T}}{2 z_{0.975} \\sqrt{(\\Sigma_{\\mathrm{opt}})_{11}/T}} = \\sqrt{\\frac{(\\Sigma_I)_{11}}{(\\Sigma_{\\mathrm{opt}})_{11}}}\n$$\nSubstituting the calculated values:\n$$\nR = \\sqrt{\\frac{0.15650339}{0.15452441}} \\approx \\sqrt{1.0128069} \\approx 1.0063828\n$$\nRounding to four significant figures gives $1.006$. This result, being greater than $1$, confirms the theoretical property that $\\Omega = V^{-1}$ is the optimal weighting matrix, leading to a smaller asymptotic variance and thus a narrower confidence interval.", "answer": "$$\n\\boxed{1.006}\n$$"}, {"introduction": "This final exercise showcases the remarkable power of indirect inference when applied to highly complex and non-linear systems. We will tackle the challenge of estimating a parameter of a chaotic model, the logistic map, whose intricate dynamics make direct estimation methods difficult. This practice demonstrates a key advantage of the II framework: the ability to use a simple, even misspecified, auxiliary model to successfully probe the structure of a complex data-generating process. [@problem_id:2401774]", "id": "2401774", "problem": "Construct a self-contained program that implements an estimator based on Indirect Inference (II) to recover the structural parameter of a chaotic discrete-time model. The true data generating process (DGP) is defined by the logistic map with a single unknown parameter. The state equation is\n$$\nx_{t+1} = r \\, x_t \\, (1 - x_t), \\quad t = 0,1,2,\\dots,T-1,\n$$\nwith fixed known initial condition $x_0 \\in (0,1)$, and the observation equation is\n$$\ny_t = x_t + \\varepsilon_t, \\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2), \\quad t = 0,1,\\dots,T-1,\n$$\nwhere $\\varepsilon_t$ are independent and identically distributed Gaussian disturbances with known standard deviation $\\sigma > 0$ (possibly $\\sigma=0$). The only unknown structural parameter is $r \\in \\mathbb{R}$. The objective is to estimate $r$ using Indirect Inference (II) by matching auxiliary statistics computed on the observed data with those computed on simulated data from the structural model.\n\nDefine the auxiliary model as the autoregressive model of order one $AR(1)$ with intercept:\n$$\ny_t = a_0 + a_1 \\, y_{t-1} + u_t, \\quad t=1,2,\\dots,T-1,\n$$\nwhere $u_t$ are residuals with zero mean. For any series $\\{y_t\\}_{t=0}^{T-1}$, define the auxiliary statistic vector\n$$\n\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big),\n$$\nwhere $\\hat{a}_0(y)$ and $\\hat{a}_1(y)$ are the ordinary least squares (OLS) estimators of $a_0$ and $a_1$, and\n$$\n\\hat{s}_u(y) = \\sqrt{\\frac{1}{T-1}\\sum_{t=1}^{T-1} \\hat{u}_t(y)^2},\n$$\nwith $\\hat{u}_t(y)$ the OLS residuals. The Indirect Inference estimator of $r$ is defined as the minimizer of a quadratic distance between the auxiliary statistics computed from the observed data and those computed from simulated data generated by the structural model with candidate parameter values. Let $K \\in \\mathbb{N}$ be the number of independent simulated datasets for averaging. For a candidate parameter $r$, define $K$ simulated datasets $\\{y^{(k)}(r)\\}_{k=1}^K$ using the same sample size $T$, the same initial condition $x_0$, and Gaussian observation noise with the same known $\\sigma$. Define the average simulated auxiliary statistics as\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right).\n$$\nWith the identity weighting matrix $W = I_3$, the criterion function is\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right)^{\\top} W \\left(\\hat{b}(y^{obs}) - \\bar{b}(r) \\right),\n$$\nwhere $y^{obs}$ denotes the observed data series. The Indirect Inference estimator is\n$$\n\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r),\n$$\nover a prespecified finite grid $\\mathcal{R}$.\n\nYour program must implement this estimator exactly as defined above with the following fixed design elements to ensure determinacy and reproducibility:\n\n- Parameter grid: \n$$\n\\mathcal{R} = \\left\\{ 3.50 + 0.0025 \\, j \\,:\\, j = 0,1,2,\\dots,200 \\right\\}.\n$$\n- Number of simulation replications: $K = 15$.\n- Initial condition: $x_0 = 0.123456789$.\n- Weighting matrix: $W = I_3$.\n- For reproducibility across candidate values $r \\in \\mathcal{R}$, use common random numbers for the simulated datasets: for each test case, generate exactly $K$ independent Gaussian noise sequences of length $T$ from a pseudo-random number generator with a fixed seed $s_{sim}$, and reuse these $K$ sequences for all $r \\in \\mathcal{R}$. For the observed data, generate its Gaussian noise sequence from a pseudo-random number generator with a fixed seed $s_{obs}$. In all cases, draw $\\varepsilon_t$ as independent $\\mathcal{N}(0,\\sigma^2)$ variates. The initial condition $x_0$ must be the same across all simulations.\n\nTest suite. Implement and solve the estimator for each of the following test cases, where the true structural parameter, sample size, noise level, and seeds are specified:\n\n- Case A (general case): $r^{\\star} = 3.8000$, $T = 1000$, $\\sigma = 0.0200$, $s_{obs} = 1729$, $s_{sim} = 2468$.\n- Case B (near onset of chaos, lower noise): $r^{\\star} = 3.5700$, $T = 800$, $\\sigma = 0.0100$, $s_{obs} = 1730$, $s_{sim} = 2469$.\n- Case C (maximal chaos, no measurement noise): $r^{\\star} = 4.0000$, $T = 1200$, $\\sigma = 0.0000$, $s_{obs} = 1731$, $s_{sim} = 2470$.\n- Case D (shorter series, higher noise): $r^{\\star} = 3.9500$, $T = 300$, $\\sigma = 0.0500$, $s_{obs} = 1732$, $s_{sim} = 2471$.\n\nFor each case, generate the observed data $y^{obs}$ by simulating the logistic map with the specified $r^{\\star}$, $x_0$, and $T$, and adding Gaussian noise with the specified $\\sigma$ using the specified $s_{obs}$. Then compute the Indirect Inference estimator $\\hat{r}$ over the grid $\\mathcal{R}$ as defined above, using $K$ simulated datasets per grid point with common random numbers from the specified $s_{sim}$.\n\nFinal output format. Your program should produce a single line of output containing the four estimated values $\\hat{r}$ for Cases A–D, in that order, as a comma-separated list enclosed in square brackets. Each value must be printed as a decimal rounded to exactly six digits after the decimal point. For example, the output format is like \"[rA,rB,rC,rD]\" where $rA$, $rB$, $rC$, and $rD$ are the four rounded estimates. No units are involved and no additional text should be printed.", "solution": "The problem requires constructing an estimator grounded in the definition of Indirect Inference (II) and applying it to a chaotic structural model. The structural DGP is the logistic map with observation noise. The estimator is defined by the following elements:\n\n1. Structural model. The state equation is $x_{t+1} = r \\, x_t \\, (1 - x_t)$ for $t = 0,1,\\dots,T-1$ with $x_0 = 0.123456789$. The observed data are $y_t = x_t + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independent for each $t$.\n\n2. Auxiliary model. The auxiliary model is the $AR(1)$ with intercept: $y_t = a_0 + a_1 \\, y_{t-1} + u_t$ for $t = 1,\\dots,T-1$. For any series $y = (y_0,\\dots,y_{T-1})$, define the OLS estimates using the normal equations. Let $Y = (y_1,\\dots,y_{T-1})^{\\top}$ and $X$ be the $(T-1) \\times 2$ matrix with first column of ones and second column $(y_0,\\dots,y_{T-2})^{\\top}$. The OLS estimator is\n$$\n\\hat{\\beta}(y) = \\begin{pmatrix} \\hat{a}_0(y) \\\\ \\hat{a}_1(y) \\end{pmatrix} = (X^{\\top}X)^{-1} X^{\\top} Y,\n$$\nand the residuals are $\\hat{u}(y) = Y - X \\hat{\\beta}(y)$. The residual dispersion statistic is\n$$\n\\hat{s}_u(y) = \\sqrt{ \\frac{1}{T-1} \\sum_{t=1}^{T-1} \\hat{u}_t(y)^2 }.\n$$\nCollect the auxiliary statistics as $\\hat{b}(y) = \\big(\\hat{a}_0(y), \\hat{a}_1(y), \\hat{s}_u(y)\\big)$.\n\n3. Indirect Inference criterion. For a candidate parameter $r$, simulate $K = 15$ datasets $y^{(k)}(r)$ of length $T$ using the structural model with initial state $x_0$ and Gaussian noise with the known $\\sigma$. Compute the average of the auxiliary statistics across simulations,\n$$\n\\bar{b}(r) = \\frac{1}{K} \\sum_{k=1}^K \\hat{b}\\!\\left(y^{(k)}(r)\\right),\n$$\nand define the quadratic distance with identity weighting matrix $W = I_3$ as\n$$\nQ(r) = \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right)^{\\top} \\left(\\hat{b}(y^{obs}) - \\bar{b}(r)\\right).\n$$\nThe estimator is $\\hat{r} \\in \\arg\\min_{r \\in \\mathcal{R}} Q(r)$ over the finite grid $\\mathcal{R} = \\{3.50 + 0.0025 \\, j : j = 0,1,\\dots,200\\}$.\n\n4. Reproducibility and common random numbers. For each test case, fix the observed noise seed $s_{obs}$ to generate the single observed sequence $\\{\\varepsilon_t^{obs}\\}$, and fix the simulation seed $s_{sim}$ to generate exactly $K = 15$ independent Gaussian noise sequences $\\{\\varepsilon_t^{(k)}\\}_{k=1}^K$. Use these same $K$ sequences for all candidate $r \\in \\mathcal{R}$. This implements common random numbers so that $Q(r)$ varies due to the change in the structural dynamics rather than due to changes in simulation noise. The initial condition $x_0 = 0.123456789$ is the same across all simulations.\n\n5. Implementation plan consistent with first principles. For each test case: \n- Generate the observed series $y^{obs}$ by simulating $x_{t+1} = r^{\\star} x_t (1-x_t)$ with $x_0$ and then adding $\\varepsilon_t^{obs} \\sim \\mathcal{N}(0,\\sigma^2)$ for $t = 0,\\dots,T-1$. \n- Compute $\\hat{b}(y^{obs})$ via the OLS formulas above. \n- Generate $K$ independent Gaussian noise sequences with the specified $s_{sim}$, each of length $T$. For each candidate $r \\in \\mathcal{R}$, simulate the structural state path $x(r)$ from the logistic map using $x_0$, form the $K$ simulated datasets $y^{(k)}(r) = x(r) + \\varepsilon^{(k)}$, compute $\\hat{b}\\!\\left(y^{(k)}(r)\\right)$ for each $k$, obtain $\\bar{b}(r)$, evaluate $Q(r)$, and select the minimizer over $\\mathcal{R}$. If there are multiple minimizers due to numerical ties, select the smallest $r$ in $\\mathcal{R}$ that achieves the minimum, which is a well-defined rule.\n\n6. Numerical and statistical considerations. The logistic map for $r \\in [3.50,4.00]$ exhibits complex and often chaotic behavior. Indirect Inference leverages the auxiliary model to compare salient features (intercept, persistence, residual dispersion) between observed and simulated data. Matching these features across a set of simulations with common random numbers attains a criterion that guides the choice of $r$. The finite grid $\\mathcal{R}$ defines a sieve-type approximation to the continuous parameter space; the grid resolution $0.0025$ implies that the attainable accuracy for $\\hat{r}$ is within $0.0025$ in ideal conditions. The presence of measurement noise $\\sigma$ and sample size $T$ influence the variability of the auxiliary statistics and, consequently, the precision of $\\hat{r}$.\n\n7. Output. The final program computes $\\hat{r}$ for the four specified cases, in order A, B, C, D, and prints a single line with a bracketed, comma-separated list of the four estimates rounded to six decimal places, with no additional output.\n\nThis solution directly applies the definition of Indirect Inference by specifying the auxiliary statistics, the simulation framework, and the minimization over the prescribed grid, ensuring that each step is grounded in the mathematical formulation and that the results are reproducible due to fixed seeds and common random numbers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef simulate_logistic_path(r: float, T: int, x0: float) -> np.ndarray:\n    \"\"\"\n    Simulate the logistic map x_{t+1} = r x_t (1 - x_t) for t=0,...,T-2 with x_0 = x0.\n    Returns an array x of length T with x[0]=x0.\n    \"\"\"\n    x = np.empty(T, dtype=float)\n    x[0] = x0\n    for t in range(T - 1):\n        x[t + 1] = r * x[t] * (1.0 - x[t])\n    return x\n\ndef auxiliary_stats_ar1(y: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute auxiliary statistics from AR(1) with intercept:\n    y_t = a0 + a1 y_{t-1} + u_t, t=1..T-1.\n    Returns [a0_hat, a1_hat, s_u_hat] where s_u_hat = sqrt( (1/(T-1)) sum u_t^2 ).\n    \"\"\"\n    # Ensure 1D float array\n    y = np.asarray(y, dtype=float).ravel()\n    if y.size < 2:\n        # Degenerate case: not enough observations; return NaNs (should not occur in this problem)\n        return np.array([np.nan, np.nan, np.nan], dtype=float)\n    y_curr = y[1:]\n    y_lag = y[:-1]\n    # Design matrix with intercept\n    X = np.column_stack((np.ones_like(y_lag), y_lag))\n    # OLS via normal equations\n    XtX = X.T @ X\n    XtY = X.T @ y_curr\n    beta = np.linalg.solve(XtX, XtY)\n    resid = y_curr - X @ beta\n    s_u = np.sqrt(np.mean(resid ** 2))\n    return np.array([beta[0], beta[1], s_u], dtype=float)\n\ndef indirect_inference_estimate_r(\n    r_grid: np.ndarray,\n    K: int,\n    x0: float,\n    T: int,\n    sigma: float,\n    obs_seed: int,\n    sim_seed: int,\n    r_true: float\n) -> float:\n    \"\"\"\n    Compute the indirect inference estimate of r over the given grid r_grid.\n    - Generate observed data using r_true, x0, T, sigma with RNG seed obs_seed.\n    - Generate K simulation noise sequences using seed sim_seed (common random numbers).\n    - For each r in r_grid, simulate x path and form K simulated datasets by adding the fixed noise sequences.\n    - Compute auxiliary statistics for observed and average over simulated datasets.\n    - Minimize squared Euclidean distance between observed and average simulated auxiliary stats.\n    Returns the minimizing r (tie broken by smallest r).\n    \"\"\"\n    # Generate observed data\n    x_obs = simulate_logistic_path(r_true, T, x0)\n    rng_obs = np.random.default_rng(obs_seed)\n    eps_obs = rng_obs.normal(loc=0.0, scale=sigma, size=T)\n    y_obs = x_obs + eps_obs\n    b_obs = auxiliary_stats_ar1(y_obs)\n\n    # Pre-generate K noise sequences (common random numbers)\n    rng_sim = np.random.default_rng(sim_seed)\n    eps_bank = rng_sim.normal(loc=0.0, scale=sigma, size=(K, T))\n\n    # For each r, compute Q(r)\n    best_r = None\n    best_Q = np.inf\n\n    # Preallocate buffer for speed\n    # Loop over candidate r\n    for r in r_grid:\n        # Simulate state path once for this r\n        x_sim = simulate_logistic_path(r, T, x0)\n        # Add fixed noise sequences to produce K simulated datasets\n        # Each y_k has shape (T,)\n        Q_components = []\n        b_sum = np.zeros(3, dtype=float)\n        # Loop over K replications\n        for k in range(K):\n            y_k = x_sim + eps_bank[k]\n            b_k = auxiliary_stats_ar1(y_k)\n            b_sum += b_k\n        b_bar = b_sum / K\n        diff = b_obs - b_bar\n        Q_r = float(diff @ diff)  # Identity weighting\n        # Update best\n        if Q_r < best_Q - 1e-18 or (np.isclose(Q_r, best_Q) and (best_r is None or r < best_r)):\n            best_Q = Q_r\n            best_r = r\n\n    return float(best_r)\n\ndef solve():\n    # Fixed design elements\n    x0 = 0.123456789\n    K = 15\n    # Grid: {3.50 + 0.0025 * j, j=0..200}\n    r_grid = 3.50 + 0.0025 * np.arange(201, dtype=float)\n\n    # Define the test cases from the problem statement.\n    # Each tuple: (r_true, T, sigma, s_obs, s_sim)\n    test_cases = [\n        (3.8000, 1000, 0.0200, 1729, 2468),  # Case A\n        (3.5700,  800, 0.0100, 1730, 2469),  # Case B\n        (4.0000, 1200, 0.0000, 1731, 2470),  # Case C\n        (3.9500,  300, 0.0500, 1732, 2471),  # Case D\n    ]\n\n    results = []\n    for r_true, T, sigma, s_obs, s_sim in test_cases:\n        r_hat = indirect_inference_estimate_r(\n            r_grid=r_grid,\n            K=K,\n            x0=x0,\n            T=T,\n            sigma=sigma,\n            obs_seed=s_obs,\n            sim_seed=s_sim,\n            r_true=r_true\n        )\n        # Round to 6 decimals in final output formatting\n        results.append(r_hat)\n\n    # Format each result to exactly 6 decimal places\n    formatted = \",\".join(f\"{val:.6f}\" for val in results)\n    # Final print statement in the exact required format.\n    print(f\"[{formatted}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"}]}