{"hands_on_practices": [{"introduction": "The engine of Gibbs sampling runs on a sequence of simpler, one-dimensional distributions known as full conditionals. This exercise [@problem_id:1920315] focuses on the fundamental skill of deriving these building blocks from a more complex joint probability density function. By treating the variable $y$ as a constant, you will isolate the terms involving $x$ and, through algebraic manipulation like completing the square, recognize the functional form—or kernel—of a standard distribution. This process is a crucial prerequisite for implementing a Gibbs sampler for many models, especially those based on Gaussian-like distributions.", "id": "1920315", "problem": "In a statistical model, the joint probability density function (PDF) of two continuous random variables, $X$ and $Y$, is known to be proportional to a function of $x$ and $y$. The joint PDF, $f_{X,Y}(x,y)$, defined for all real numbers $x$ and $y$, is given by the relation:\n$$\nf_{X,Y}(x,y) \\propto \\exp(-(x^2 - 2xy + 4y^2))\n$$\nA common method to generate samples from such a joint distribution is the Gibbs sampler, which requires drawing from the full conditional distributions. Your task is to determine the full conditional distribution of the random variable $X$ given that the random variable $Y$ takes a specific value $y$.\n\nWhich of the following correctly describes the distribution of $X$ given $Y=y$?\n\nA. A Normal distribution with mean $y$ and variance $1/2$.\nB. A Normal distribution with mean $y$ and variance $1$.\nC. A Normal distribution with mean $2y$ and variance $1/4$.\nD. A Normal distribution with mean $-y$ and variance $1/2$.\nE. An Exponential distribution with rate parameter $1/y$.\n\n", "solution": "We are given a joint density proportional to\n$$\nf_{X,Y}(x,y)\\propto \\exp\\big(-(x^{2}-2xy+4y^{2})\\big).\n$$\nFor the full conditional distribution of $X$ given $Y=y$, treat $y$ as fixed and consider the kernel in $x$:\n$$\nf_{X|Y}(x|y)\\propto f_{X,Y}(x,y)\\propto \\exp\\big(-(x^{2}-2xy+4y^{2})\\big).\n$$\nDiscarding factors that do not depend on $x$, we obtain\n$$\nf_{X|Y}(x|y)\\propto \\exp\\big(-(x^{2}-2xy)\\big).\n$$\nComplete the square in $x$:\n$$\nx^{2}-2xy=(x-y)^{2}-y^{2},\n$$\nso\n$$\n\\exp\\big(-(x^{2}-2xy)\\big)=\\exp\\big(-((x-y)^{2}-y^{2})\\big)=\\exp(y^{2})\\,\\exp\\big(-(x-y)^{2}\\big).\n$$\nThe factor $\\exp(y^{2})$ is constant in $x$ and is absorbed into the normalizing constant. Therefore,\n$$\nf_{X|Y}(x|y)\\propto \\exp\\big(-(x-y)^{2}\\big).\n$$\nComparing with the Gaussian kernel $\\exp\\big(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\big)$, we identify\n$$\n\\mu=y,\\quad \\frac{1}{2\\sigma^{2}}=1\\;\\Rightarrow\\;\\sigma^{2}=\\frac{1}{2}.\n$$\nHence $X|Y=y$ is Normal with mean $y$ and variance $\\frac{1}{2}$, which corresponds to option A.", "answer": "$$\\boxed{A}$$"}, {"introduction": "Once the full conditional distributions are identified, the next step is to use them to generate samples. This practice [@problem_id:1920320] demystifies the iterative nature of the Gibbs sampling algorithm by guiding you through one complete \"Gibbs dance\" step. Starting from an initial point $(x^{(0)}, y^{(0)})$, you will sample a new value $x^{(1)}$ from its conditional distribution, and then immediately use this updated value to sample $y^{(1)}$. This hands-on numerical exercise makes the abstract update mechanism concrete and reinforces the essential inverse transform sampling method for generating random variates.", "id": "1920320", "problem": "Consider a two-dimensional random vector $(X, Y)$ whose joint probability distribution is defined by the following full conditional distributions:\n- The conditional distribution of $X$ given $Y=y$ is an Exponential distribution with a rate parameter of $y$. The probability density function is given by $p(x|y) = y \\exp(-yx)$ for $x > 0$.\n- The conditional distribution of $Y$ given $X=x$ is a Poisson distribution with a mean parameter of $x$. The probability mass function is given by $p(y=k|x) = \\frac{x^k \\exp(-x)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nYou are tasked with performing one full iteration of a Gibbs sampler. Starting from the initial state $(x^{(0)}, y^{(0)}) = (2, 3)$, you will generate a new state $(x^{(1)}, y^{(1)})$. The procedure for the iteration is as follows: first, draw a sample for $x^{(1)}$ from the distribution $p(x|y^{(0)})$, and then, using this new value $x^{(1)}$, draw a sample for $y^{(1)}$ from the distribution $p(y|x^{(1)})$.\n\nTo generate the necessary random variates, you must use the inverse transform sampling method. Use the following random numbers, which are drawn from a Uniform(0,1) distribution:\n- For generating $x^{(1)}$, use the uniform random number $u_x = 0.600$.\n- For generating $y^{(1)}$, use the uniform random number $u_y = 0.750$.\n\nWhat are the numerical values for the new state $(x^{(1)}, y^{(1)})$? The value for $x^{(1)}$ must be rounded to four significant figures.\n\n", "solution": "We perform one Gibbs update using inverse transform sampling.\n\n1) Sample $x^{(1)}$ from $p(x \\mid y^{(0)}=3)$.\nFor an Exponential rate $y$, the conditional CDF is\n$$\nF(x \\mid y)=1-\\exp(-yx), \\quad x>0.\n$$\nInverse transform uses $u_{x}=F(x \\mid y)$, hence\n$$\nx^{(1)}=F^{-1}(u_{x})=-\\frac{1}{y^{(0)}}\\ln\\!\\bigl(1-u_{x}\\bigr).\n$$\nWith $y^{(0)}=3$ and $u_{x}=0.600$,\n$$\nx^{(1)}=-\\frac{1}{3}\\ln(1-0.600)=-\\frac{1}{3}\\ln(0.4)=\\frac{1}{3}\\ln(2.5)\\approx 0.3054302439.\n$$\nRounded to four significant figures: $x^{(1)}=0.3054$.\n\n2) Sample $y^{(1)}$ from $p(y \\mid x^{(1)})$ using $u_{y}=0.750$.\nFor a Poisson mean $x$, the pmf is\n$$\np(y=k \\mid x)=\\frac{x^{k}\\exp(-x)}{k!}, \\quad k\\in\\{0,1,2,\\dots\\}.\n$$\nInverse transform for a discrete distribution selects the smallest $k$ such that $F(k \\mid x)=\\sum_{j=0}^{k}p(j \\mid x)\\ge u_{y}$.\n\nWith $x=x^{(1)}=\\frac{1}{3}\\ln(2.5)$, compute\n$$\np(0 \\mid x)=\\exp(-x)=\\exp\\!\\Bigl(-\\tfrac{1}{3}\\ln(2.5)\\Bigr)=2.5^{-1/3}\\approx 0.7368.\n$$\nSince $p(0 \\mid x)=0.7368<0.750$, continue to $k=1$:\n$$\np(1 \\mid x)=x\\exp(-x)=x\\,2.5^{-1/3}\\approx 0.30543\\times 0.7368\\approx 0.2250.\n$$\nThen\n$$\nF(1 \\mid x)=p(0 \\mid x)+p(1 \\mid x)\\approx 0.7368+0.2250=0.9618>0.750,\n$$\nso the smallest $k$ with $F(k \\mid x)\\ge 0.750$ is $k=1$. Therefore $y^{(1)}=1$.\n\nThus, the new state is $(x^{(1)},y^{(1)})=(0.3054,1)$ with $x^{(1)}$ rounded to four significant figures.", "answer": "$$\\boxed{\\begin{pmatrix}0.3054 & 1\\end{pmatrix}}$$"}, {"introduction": "This practice [@problem_id:2398229] demonstrates the power of Gibbs sampling by applying it to a sophisticated and practical challenge in computational economics: identifying business cycles. You will implement a sampler for a Markov-switching model, where hidden \"expansion\" and \"recession\" states govern observed GDP growth. This involves a multi-part Gibbs sampler that iteratively updates the model's parameters and infers the unobserved state of the economy at each point in time. This exercise brings together theory and practice, showing how the component skills of deriving and sampling from conditionals can be assembled to solve complex, real-world inference problems.", "id": "2398229", "problem": "You are given a two-state Markov-switching model for quarterly real Gross Domestic Product (GDP) growth designed to capture expansion and recession regimes. The hidden state at time $t$, denoted $s_t \\in \\{0,1\\}$, follows a time-homogeneous first-order Markov chain. Conditional on the state, the observed growth $y_t$ is Gaussian with a regime-dependent mean and a common known variance. The complete specification is:\n- State dynamics: $s_{t} \\mid s_{t-1} \\sim \\text{Categorical}\\left(P_{s_{t-1},\\cdot}\\right)$ with transition matrix $P = \\begin{pmatrix} p_{00} & 1-p_{00} \\\\ 1-p_{11} & p_{11} \\end{pmatrix}$, where $p_{00} = \\mathbb{P}(s_t = 0 \\mid s_{t-1} = 0)$, $p_{11} = \\mathbb{P}(s_t = 1 \\mid s_{t-1} = 1)$.\n- Observation model: $y_t \\mid s_t = k \\sim \\mathcal{N}(\\mu_k, \\sigma^2)$, for $k \\in \\{0,1\\}$ and known variance $\\sigma^2$.\n- Priors: $\\mu_0 \\sim \\mathcal{N}(m_0, V_0)$, $\\mu_1 \\sim \\mathcal{N}(m_1, V_1)$, $p_{00} \\sim \\text{Beta}(a_{00}, b_{00})$, $p_{11} \\sim \\text{Beta}(a_{11}, b_{11})$. The initial state $s_1$ has a fixed prior $\\mathbb{P}(s_1=0) = \\mathbb{P}(s_1=1) = 0.5$.\n\nYour task is to implement a Gibbs sampler that alternates between sampling the hidden state sequence $\\{s_t\\}_{t=1}^T$ using Forward-Filtering Backward-Sampling, sampling the regime means $\\mu_0$ and $\\mu_1$ using their conjugate Gaussian posteriors, and sampling the transition probabilities $p_{00}$ and $p_{11}$ using their conjugate Beta posteriors. Use the following well-tested facts and definitions as the fundamental base:\n- Bayes' rule for conditional probabilities and standard properties of the Gaussian and Beta distributions.\n- The definition of a first-order Markov chain and the Hidden Markov Model (HMM) structure with Gaussian emissions.\n- The Forward-Filtering Backward-Sampling identity for sampling hidden states in an HMM.\n\nFor each of the test cases below, run a Gibbs sampler with a fixed random seed, a fixed number of iterations, and a specified burn-in. After convergence burn-in, estimate the marginal posterior probability of recession at each time, $\\hat{\\pi}_t = \\mathbb{P}(s_t = 1 \\mid y_{1:T})$, by the Monte Carlo frequency across post-burn-in draws. Classify a time $t$ as recession if $\\hat{\\pi}_t \\geq 0.5$. For each test case, output the integer count of time indices classified as recession.\n\nAll GDP growth values $y_t$ are given as decimal fractions per quarter (for example, $0.008$ denotes $0.8$ in decimal, not a percentage). There are no physical units to report in the final answers since the required outputs are counts. Angles do not appear in this problem.\n\nTest suite parameter sets:\n\n- Case A (clear regime separation, moderate persistence):\n  - Observations $y_{1:20} = \\left(0.010, 0.008, 0.009, 0.007, 0.006, 0.007, -0.004, -0.006, -0.005, -0.007, -0.006, -0.004, 0.005, 0.006, 0.008, 0.009, 0.007, 0.006, 0.005, 0.007\\right)$.\n  - Known variance $\\sigma^2 = 0.000025$.\n  - Priors: $(m_0, V_0) = (0.007, 0.0001)$, $(m_1, V_1) = (-0.006, 0.0001)$, $(a_{00}, b_{00}) = (8, 2)$, $(a_{11}, b_{11}) = (8, 2)$.\n  - Gibbs sampler settings: number of iterations $N = 6000$, burn-in $B = 3000$, seed $= 12345$.\n\n- Case B (single observation boundary case, symmetric priors):\n  - Observations $y_{1:1} = \\left(0.000\\right)$.\n  - Known variance $\\sigma^2 = 0.000025$.\n  - Priors: $(m_0, V_0) = (0.004, 0.0001)$, $(m_1, V_1) = (-0.004, 0.0001)$, $(a_{00}, b_{00}) = (5, 5)$, $(a_{11}, b_{11}) = (5, 5)$.\n  - Gibbs sampler settings: number of iterations $N = 6000$, burn-in $B = 3000$, seed $= 12345$.\n\n- Case C (ambiguous regimes, lower persistence priors):\n  - Observations $y_{1:12} = \\left(0.003, 0.004, 0.002, -0.001, 0.000, -0.002, -0.003, 0.001, 0.002, 0.003, -0.002, -0.001\\right)$.\n  - Known variance $\\sigma^2 = 0.000025$.\n  - Priors: $(m_0, V_0) = (0.002, 0.0002)$, $(m_1, V_1) = (-0.002, 0.0002)$, $(a_{00}, b_{00}) = (2, 2)$, $(a_{11}, b_{11}) = (2, 2)$.\n  - Gibbs sampler settings: number of iterations $N = 6000$, burn-in $B = 3000$, seed $= 12345$.\n\nImplement your program to:\n- For each case, run the Gibbs sampler described above with the specified parameters.\n- After burn-in, compute $\\hat{\\pi}_t$ as the Monte Carlo frequency with which $s_t = 1$ across samples.\n- Count the number of indices $t$ with $\\hat{\\pi}_t \\geq 0.5$.\n- Aggregate the three integer counts corresponding to the three cases into a single list.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[x_A,x_B,x_C]$, where $x_A$, $x_B$, and $x_C$ are the integer counts for Case A, Case B, and Case C respectively.", "solution": "The problem requires the implementation of a Gibbs sampler for a two-state Markov-switching model of real GDP growth. The model parameters, including regime-specific means and state transition probabilities, are to be estimated from observed data within a Bayesian framework. The final objective is to classify each time point as being in a \"recession\" state ($s_t = 1$) based on the posterior probability and to count the total number of such periods for three distinct test cases.\n\nThe problem is scientifically well-posed, providing a complete specification of the model, priors, data, and the required algorithm. It represents a standard application of Markov Chain Monte Carlo (MCMC) methods, specifically Gibbs sampling, to a Hidden Markov Model (HMM), a common task in computational econometrics. All parameters are specified, and the task is unambiguous. The problem is therefore deemed valid and a solution will be constructed.\n\nThe core of the solution lies in iteratively sampling from the full conditional posterior distributions of the unknown variables: the sequence of hidden states $\\{s_t\\}_{t=1}^T$, the regime means $\\mu_0$ and $\\mu_1$, and the transition probabilities $p_{00}$ and $p_{11}$. This procedure constitutes a Gibbs sampler.\n\nLet the set of all parameters be $\\theta = \\{\\mu_0, \\mu_1, p_{00}, p_{11}\\}$ and the state sequence be $S = \\{s_t\\}_{t=1}^T$. The Gibbs sampler proceeds by initializing the parameters and then iterating the following steps:\n1. Sample $S^{(i+1)} \\sim p(S \\mid y_{1:T}, \\theta^{(i)})$.\n2. Sample $\\mu_0^{(i+1)}, \\mu_1^{(i+1)} \\sim p(\\mu_0, \\mu_1 \\mid y_{1:T}, S^{(i+1)}, \\sigma^2)$.\n3. Sample $p_{00}^{(i+1)}, p_{11}^{(i+1)} \\sim p(p_{00}, p_{11} \\mid S^{(i+1)})$.\n\nEach step is detailed below.\n\n**1. Sampling the State Sequence $S = \\{s_t\\}_{t=1}^T$**\n\nThe state sequence is sampled from its conditional posterior distribution $p(S \\mid y_{1:T}, \\theta)$ using the Forward-Filtering Backward-Sampling (FFBS) algorithm.\n\n_Forward Filtering_:\nWe first compute the filtered probabilities $\\alpha_t(k) = p(s_t = k, y_{1:t} \\mid \\theta)$ for $k \\in \\{0, 1\\}$ and $t=1, \\dots, T$.\n- **Initialization ($t=1$)**: The initial state prior is given as $\\mathbb{P}(s_1=k) = 0.5$. The filtering step begins with:\n  $$\n  \\alpha_1(k) = \\mathbb{P}(s_1=k) \\cdot p(y_1 \\mid s_1=k, \\theta) = 0.5 \\cdot \\mathcal{N}(y_1; \\mu_k, \\sigma^2)\n  $$\n  where $\\mathcal{N}(y; \\mu, \\sigma^2)$ is the probability density function of the normal distribution.\n- **Recursion ($t=2, \\dots, T$)**: For subsequent time steps, the filtered probability is updated using the Markov property:\n  $$\n  \\alpha_t(k) = p(y_t \\mid s_t=k, \\theta) \\sum_{j=0}^{1} p(s_t=k \\mid s_{t-1}=j, \\theta) \\cdot \\alpha_{t-1}(j)\n  $$\n  $$\n  \\alpha_t(k) = \\mathcal{N}(y_t; \\mu_k, \\sigma^2) \\sum_{j=0}^{1} P_{jk} \\cdot \\alpha_{t-1}(j)\n  $$\n  where $P_{jk}$ is the transition probability from state $j$ to state $k$. To prevent numerical underflow, the vector $\\alpha_t = (\\alpha_t(0), \\alpha_t(1))$ is typically normalized at each step. Let $\\hat{\\alpha}_t(k) = p(s_t=k \\mid y_{1:t}, \\theta) \\propto \\alpha_t(k)$. This normalization does not affect the backward sampling step.\n\n_Backward Sampling_:\nAfter computing the filtered probabilities up to $T$, we sample the states in reverse time order.\n- **Initialization ($t=T$)**: Sample $s_T$ from the final filtered distribution:\n  $$\n  p(s_T=k \\mid y_{1:T}, \\theta) \\propto \\alpha_T(k)\n  $$\n- **Recursion ($t=T-1, \\dots, 1$)**: For each preceding time step, sample $s_t$ conditioned on the already sampled future state $s_{t+1}$ and the filtered probabilities:\n  $$\n  p(s_t=j \\mid s_{t+1}=k, y_{1:T}, \\theta) \\propto p(s_{t+1}=k \\mid s_t=j) \\cdot p(s_t=j, y_{1:t}) \\propto P_{jk} \\cdot \\alpha_t(j)\n  $$\n  This gives a categorical distribution from which $s_t$ is drawn.\n\n**2. Sampling the Regime Means $\\mu_k$**\n\nThe means $\\mu_0$ and $\\mu_1$ are sampled independently conditional on the state sequence $S$. Given the conjugate prior setup (Normal prior, Normal likelihood), the posterior for each $\\mu_k$ is also Normal.\nLet $S$ be the sampled state sequence. Let $Y_k = \\{y_t \\mid s_t = k\\}$ be the subset of observations occurring in state $k$, and let $T_k = |Y_k|$ be the number of such observations. The prior for $\\mu_k$ is $\\mathcal{N}(m_k, V_k)$.\nThe posterior for $\\mu_k$ is $p(\\mu_k \\mid S, y_{1:T}) \\sim \\mathcal{N}(\\mu_{k, post}, V_{k, post})$, where the posterior variance $V_{k, post}$ and mean $\\mu_{k, post}$ are given by:\n$$\nV_{k, \\text{post}} = \\left( \\frac{1}{V_k} + \\frac{T_k}{\\sigma^2} \\right)^{-1}\n$$\n$$\n\\mu_{k, \\text{post}} = V_{k, \\text{post}} \\left( \\frac{m_k}{V_k} + \\frac{1}{\\sigma^2} \\sum_{y_t \\in Y_k} y_t \\right)\n$$\nIf a state $k$ is not visited in a given iteration (i.e., $T_k = 0$), the posterior for $\\mu_k$ is equal to its prior, $\\mathcal{N}(m_k, V_k)$. We draw a new sample for $\\mu_k$ from this posterior distribution.\n\n**3. Sampling the Transition Probabilities $p_{kk}$**\n\nThe transition probabilities $p_{00}$ and $p_{11}$ are sampled independently, conditional on the state sequence $S$. The priors are Beta distributions, which are conjugate to the Binomial (or Bernoulli) likelihood of state transitions.\nLet $N_{jk} = \\sum_{t=2}^T \\mathbb{I}(s_{t-1}=j, s_t=k)$ be the number of observed transitions from state $j$ to state $k$ in the sampled sequence $S$.\n- The prior for $p_{00}$ is $\\text{Beta}(a_{00}, b_{00})$. The data provides $N_{00}$ transitions from state $0$ to $0$ and $N_{01}$ transitions from state $0$ to $1$. The posterior distribution for $p_{00}$ is:\n  $$\n  p(p_{00} \\mid S) \\sim \\text{Beta}(a_{00} + N_{00}, b_{00} + N_{01})\n  $$\n- Similarly, the posterior distribution for $p_{11}$ is:\n  $$\n  p(p_{11} \\mid S) \\sim \\text{Beta}(a_{11} + N_{11}, b_{11} + N_{10})\n  $$\nWe draw new samples for $p_{00}$ and $p_{11}$ from these posterior Beta distributions. If $T=1$, there are no transitions, and the posteriors are identical to the priors.\n\n**4. Estimation and Classification**\n\nAfter running the Gibbs sampler for $N$ iterations and discarding the first $B$ as burn-in, we are left with $N-B$ samples from the joint posterior distribution. The marginal posterior probability of being in a recession ($s_t = 1$) at time $t$ is estimated using a Monte Carlo average over the post-burn-in samples of the state sequence, $\\{S^{(i)}\\}_{i=B+1}^N$:\n$$\n\\hat{\\pi}_t = \\mathbb{P}(s_t = 1 \\mid y_{1:T}) \\approx \\frac{1}{N-B} \\sum_{i=B+1}^{N} \\mathbb{I}(s_t^{(i)}=1)\n$$\nA time point $t$ is classified as a recession if this estimated probability is greater than or equal to $0.5$. The final result for each test case is the total count of time indices classified as recession.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\n# No other libraries outside the Python standard library are permitted.\n\ndef run_gibbs_sampler(y, sigma_sq, priors, settings):\n    \"\"\"\n    Runs a Gibbs sampler for the specified Markov-switching model.\n    \"\"\"\n    T = len(y)\n    m0, V0 = priors['mu0']\n    m1, V1 = priors['mu1']\n    a00, b00 = priors['p00']\n    a11, b11 = priors['p11']\n    \n    num_iter = settings['N']\n    burn_in = settings['B']\n    seed = settings['seed']\n    \n    rng = np.random.default_rng(seed)\n\n    # 1. Initialize parameters by drawing from priors\n    mu0 = rng.normal(m0, np.sqrt(V0))\n    mu1 = rng.normal(m1, np.sqrt(V1))\n    p00 = rng.beta(a00, b00)\n    p11 = rng.beta(a11, b11)\n    \n    # Storage for post-burn-in state samples\n    num_samples_to_store = num_iter - burn_in\n    if num_samples_to_store <= 0:\n        raise ValueError(\"Number of iterations must be greater than burn-in.\")\n    state_samples = np.zeros((num_samples_to_store, T), dtype=np.int8)\n    \n    # 2. Gibbs sampling iterations\n    for i in range(num_iter):\n        mus = np.array([mu0, mu1])\n        P = np.array([[p00, 1.0 - p00], [1.0 - p11, p11]])\n\n        # a. Sample states S = {s_t} using Forward-Filtering Backward-Sampling (FFBS)\n        \n        # Forward filtering\n        alpha_hat = np.zeros((T, 2))\n        \n        # t=1\n        likelihood_1 = norm.pdf(y[0], loc=mus, scale=np.sqrt(sigma_sq))\n        # Initial state prob = 0.5 for both states\n        alpha_hat[0, :] = 0.5 * likelihood_1\n        sum_alpha = np.sum(alpha_hat[0, :])\n        if sum_alpha > 0:\n            alpha_hat[0, :] /= sum_alpha\n\n        # t > 1\n        for t in range(1, T):\n            likelihood_t = norm.pdf(y[t], loc=mus, scale=np.sqrt(sigma_sq))\n            alpha_hat[t, :] = likelihood_t * (alpha_hat[t-1, :] @ P)\n            sum_alpha = np.sum(alpha_hat[t, :])\n            if sum_alpha > 0:\n                alpha_hat[t, :] /= sum_alpha\n        \n        # Backward sampling\n        states = np.zeros(T, dtype=np.int8)\n        \n        # t=T\n        p_sT = alpha_hat[T-1, :]\n        states[T-1] = rng.choice([0, 1], p=p_sT)\n\n        # t < T\n        for t in range(T-2, -1, -1):\n            s_next = states[t+1]\n            p_st = alpha_hat[t, :] * P[:, s_next]\n            sum_p = np.sum(p_st)\n            if sum_p > 0:\n                 p_st /= sum_p\n            else: # Fallback if probabilities are zero\n                p_st = np.array([0.5, 0.5])\n            states[t] = rng.choice([0, 1], p=p_st)\n\n        # b. Sample means mu_k\n        y_s0 = y[states == 0]\n        T0 = len(y_s0)\n        if T0 > 0:\n            V0_inv = 1.0 / V0\n            sigma_sq_inv = 1.0 / sigma_sq\n            V0_post_inv = V0_inv + T0 * sigma_sq_inv\n            V0_post = 1.0 / V0_post_inv\n            mu0_post = V0_post * (V0_inv * m0 + sigma_sq_inv * np.sum(y_s0))\n            mu0 = rng.normal(mu0_post, np.sqrt(V0_post))\n        else: # Sample from prior if state is not visited\n            mu0 = rng.normal(m0, np.sqrt(V0))\n\n        y_s1 = y[states == 1]\n        T1 = len(y_s1)\n        if T1 > 0:\n            V1_inv = 1.0 / V1\n            sigma_sq_inv = 1.0 / sigma_sq\n            V1_post_inv = V1_inv + T1 * sigma_sq_inv\n            V1_post = 1.0 / V1_post_inv\n            mu1_post = V1_post * (V1_inv * m1 + sigma_sq_inv * np.sum(y_s1))\n            mu1 = rng.normal(mu1_post, np.sqrt(V1_post))\n        else:\n            mu1 = rng.normal(m1, np.sqrt(V1))\n\n        # c. Sample transition probabilities p_kk\n        if T > 1:\n            N00 = np.sum((states[:-1] == 0) & (states[1:] == 0))\n            N01 = np.sum((states[:-1] == 0) & (states[1:] == 1))\n            N11 = np.sum((states[:-1] == 1) & (states[1:] == 1))\n            N10 = np.sum((states[:-1] == 1) & (states[1:] == 0))\n            \n            p00 = rng.beta(a00 + N00, b00 + N01)\n            p11 = rng.beta(a11 + N11, b11 + N10)\n        else: # T=1, no transitions, sample from priors\n            p00 = rng.beta(a00, b00)\n            p11 = rng.beta(a11, b11)\n\n        # Store sample if past burn-in\n        if i >= burn_in:\n            state_samples[i - burn_in, :] = states\n\n    # 3. Post-processing\n    # Estimate marginal posterior probability of recession (state 1)\n    # This is the mean of the indicator variable (0 or 1) across samples\n    pi_hat = np.mean(state_samples, axis=0)\n    \n    # Classify as recession if prob >= 0.5 and count\n    recession_count = np.sum(pi_hat >= 0.5)\n    \n    return int(recession_count)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        { # Case A\n            'y': np.array([0.010, 0.008, 0.009, 0.007, 0.006, 0.007, -0.004, -0.006, -0.005, -0.007, -0.006, -0.004, 0.005, 0.006, 0.008, 0.009, 0.007, 0.006, 0.005, 0.007]),\n            'sigma_sq': 0.000025,\n            'priors': {'mu0': (0.007, 0.0001), 'mu1': (-0.006, 0.0001), 'p00': (8, 2), 'p11': (8, 2)},\n            'settings': {'N': 6000, 'B': 3000, 'seed': 12345}\n        },\n        { # Case B\n            'y': np.array([0.000]),\n            'sigma_sq': 0.000025,\n            'priors': {'mu0': (0.004, 0.0001), 'mu1': (-0.004, 0.0001), 'p00': (5, 5), 'p11': (5, 5)},\n            'settings': {'N': 6000, 'B': 3000, 'seed': 12345}\n        },\n        { # Case C\n            'y': np.array([0.003, 0.004, 0.002, -0.001, 0.000, -0.002, -0.003, 0.001, 0.002, 0.003, -0.002, -0.001]),\n            'sigma_sq': 0.000025,\n            'priors': {'mu0': (0.002, 0.0002), 'mu1': (-0.002, 0.0002), 'p00': (2, 2), 'p11': (2, 2)},\n            'settings': {'N': 6000, 'B': 3000, 'seed': 12345}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_gibbs_sampler(case['y'], case['sigma_sq'], case['priors'], case['settings'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}]}