{"hands_on_practices": [{"introduction": "To master the Metropolis-Hastings algorithm, we must first understand its core decision-making step: the calculation of the acceptance probability. This exercise [@problem_id:1962612] provides a clear and direct scenario for you to practice this fundamental skill. By working with a discrete target distribution, we can focus on how the algorithm compares the probability of the proposed state to the current state, which is the heart of the acceptance mechanism.", "id": "1962612", "problem": "A statistician is implementing a Markov Chain Monte Carlo (MCMC) simulation to generate samples from a target probability distribution. The chosen target distribution is a Poisson distribution, which models the number of events, $k$, occurring in a fixed interval of time or space. The probability mass function for a Poisson distribution is given by:\n$$ P(k; \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!} $$\nwhere $\\lambda$ is the average rate of events. For this specific simulation, the parameter is set to $\\lambda = 5$.\n\nThe simulation uses the Metropolis-Hastings algorithm with a symmetric proposal distribution, meaning the probability of proposing a move from state $k_1$ to state $k_2$ is the same as proposing a move from $k_2$ to $k_1$.\n\nSuppose the current state of the chain is $k=5$. The algorithm then proposes a move to a new state, $k'=6$.\n\nCalculate the acceptance probability for this proposed move from $k=5$ to $k'=6$. Express your answer as a decimal rounded to three significant figures.\n\n", "solution": "In the Metropolis-Hastings algorithm with a symmetric proposal distribution, the acceptance probability for a proposed move from state $k$ to $k'$ is\n$$\n\\alpha = \\min\\left(1, \\frac{\\pi(k')}{\\pi(k)}\\right),\n$$\nwhere $\\pi(\\cdot)$ is the target probability mass function. For a Poisson distribution with parameter $\\lambda$, the PMF is\n$$\n\\pi(k) = \\frac{\\lambda^{k} \\exp(-\\lambda)}{k!}.\n$$\nTherefore, the ratio simplifies as\n$$\n\\frac{\\pi(k')}{\\pi(k)} = \\frac{\\lambda^{k'} \\exp(-\\lambda) / k'!}{\\lambda^{k} \\exp(-\\lambda) / k!} = \\lambda^{k'-k} \\frac{k!}{k'!}.\n$$\nWith $k=5$, $k'=6$, and $\\lambda=5$, we have $k'-k=1$ and $6! = 6 \\cdot 5!$, so\n$$\n\\frac{\\pi(6)}{\\pi(5)} = \\lambda \\cdot \\frac{5!}{6!} = \\lambda \\cdot \\frac{1}{6} = \\frac{5}{6}.\n$$\nHence, the acceptance probability is\n$$\n\\alpha = \\min\\left(1, \\frac{5}{6}\\right) = \\frac{5}{6}.\n$$\nExpressed as a decimal rounded to three significant figures, this is $0.833$.", "answer": "$$\\boxed{0.833}$$"}, {"introduction": "While the acceptance probability calculation is straightforward, the algorithm's overall success hinges on a thoughtful choice of the proposal distribution. A high acceptance rate can be misleading; it might indicate that the sampler is taking such small steps that it fails to explore the entire parameter space. This conceptual problem [@problem_id:1962668] explores a common pitfall where a seemingly \"safe\" proposal choice traps the sampler in a single high-probability region, preventing the discovery of a multimodal distribution's true shape.", "id": "1962668", "problem": "A data scientist is analyzing the posterior probability distribution for a parameter $\\theta$ of a complex climate model. The analysis reveals that the posterior distribution, denoted as $p(\\theta)$, is bimodal, with two distinct peaks of high probability located at $\\theta_A$ and $\\theta_B$, separated by a wide region of very low probability. To explore this distribution and estimate properties like the posterior mean, the scientist employs the Metropolis-Hastings (M-H) algorithm.\n\nThe M-H sampler is initialized with a starting value $\\theta_0$ located within the high-probability region around the first peak, $\\theta_A$. A symmetric proposal distribution $q(\\theta' | \\theta) = \\mathcal{N}(\\theta' | \\theta, \\sigma^2)$ is used, where $\\mathcal{N}$ is a normal distribution centered at the current state $\\theta$ with a standard deviation $\\sigma$, which represents the proposal step size. The scientist, aiming for a high acceptance rate, chooses a very small value for $\\sigma$ relative to the distance between the two modes, $|\\theta_A - \\theta_B|$.\n\nAfter running the M-H sampler for a very large number of iterations, which of the following descriptions most accurately characterizes the expected outcome of this simulation?\n\nA. The sampler will efficiently find the global maximum of the posterior distribution $p(\\theta)$ and remain there, thus providing an excellent point estimate for the parameter.\nB. The acceptance rate for proposed states will be very low, causing the chain to remain near the initial state $\\theta_0$ and explore very little of the parameter space.\nC. The generated chain of samples will be highly autocorrelated, and its histogram will largely represent the shape of the mode around $\\theta_A$, while failing to discover the mode around $\\theta_B$.\nD. The samples will alternate between the two modes in a systematic fashion, jumping from the region of $\\theta_A$ to the region of $\\theta_B$ and back again with regular frequency.\nE. The states of the chain will be nearly independent of one another, indicating that the sampler has successfully converged to the true bimodal posterior distribution.\n\n", "solution": "We analyze the Metropolis-Hastings (M-H) sampler with a symmetric proposal and very small proposal scale relative to the distance between two separated modes of the posterior $p(\\theta)$ at $\\theta_{A}$ and $\\theta_{B}$.\n\nFor a symmetric proposal $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta' \\mid \\theta, \\sigma^{2})$, the Metropolis-Hastings acceptance probability is\n$$\na(\\theta \\rightarrow \\theta') = \\min\\left\\{1, \\frac{p(\\theta')}{p(\\theta)}\\right\\}.\n$$\nThe chain is initialized at $\\theta_{0}$ in the high-probability region around $\\theta_{A}$. Because $\\sigma$ is chosen to be very small compared to the separation $|\\theta_{A} - \\theta_{B}|$, proposed moves satisfy $|\\theta' - \\theta| = O(\\sigma)$ and thus remain very close to the current state. In a high-probability region near a mode, $p(\\theta')$ is close to $p(\\theta)$ for small steps, so\n$$\n\\frac{p(\\theta')}{p(\\theta)} \\approx 1,\n$$\nimplying that $a(\\theta \\rightarrow \\theta') \\approx 1$ and the local acceptance rate is high. Hence, option B (very low acceptance rate) is contradicted by the small-step, within-mode behavior.\n\nNext, consider transitions between the modes. The probability to directly propose a state near $\\theta_{B}$ from a current state near $\\theta_{A}$ under the Gaussian proposal is\n$$\nq(\\theta_{B} \\mid \\theta) \\propto \\exp\\left(-\\frac{|\\theta_{B} - \\theta|^{2}}{2 \\sigma^{2}}\\right).\n$$\nSince $\\sigma \\ll |\\theta_{A} - \\theta_{B}|$, this proposal probability is exponentially small in $|\\theta_{A} - \\theta_{B}|^{2} / \\sigma^{2}$. Therefore, direct jumps across the low-probability valley are essentially never proposed on practical time scales. Alternatively, crossing the valley via many small steps requires repeatedly proposing moves into regions where $p(\\theta') \\ll p(\\theta)$, for which\n$$\na(\\theta \\rightarrow \\theta') = \\min\\left\\{1, \\frac{p(\\theta')}{p(\\theta)}\\right\\} \\ll 1,\n$$\nso such steps are overwhelmingly rejected. Consequently, the chain becomes effectively trapped near $\\theta_{A}$ for a very long time, failing to discover $\\theta_{B}$ in practice.\n\nBecause moves are very local and the chain stays within the same mode, successive samples are highly autocorrelated. The empirical histogram thus reflects the local shape around $\\theta_{A}$ but does not capture the separated mode near $\\theta_{B}$. This rules out option E (independence and successful convergence) and option D (regular alternation between modes). Option A is incorrect because M-H is a sampler targeting $p(\\theta)$, not an optimizer; moreover, with small steps and bimodality, it neither efficiently finds nor remains at a global maximum.\n\nTherefore, the most accurate description is that the chain exhibits high autocorrelation, predominantly samples the neighborhood of $\\theta_{A}$, and fails to discover the second mode $\\theta_{B}$.\n\nThe correct choice is C.", "answer": "$$\\boxed{C}$$"}, {"introduction": "Having seen how a poorly chosen proposal can hinder exploration, we now turn to the constructive task of designing more efficient ones. The key is to tailor the proposal distribution to the specific geometry of the target distribution. This problem [@problem_id:1962663] challenges you to analyze a target distribution with strong correlation between its variables and compare the effectiveness of two different proposal strategies. By calculating their relative acceptance probabilities, you will gain concrete insight into how aligning your proposal with the target's structure can dramatically improve sampler performance.", "id": "1962663", "problem": "A statistician is using the Metropolis-Hastings algorithm to sample from a bivariate target distribution on $\\mathbb{R}^2$ with a probability density function (PDF) $\\pi(x, y)$ that is proportional to\n$$\n\\pi(x, y) \\propto \\exp\\left(-\\frac{x^2 - 2\\rho xy + y^2}{2(1-\\rho^2)}\\right)\n$$\nwhere $\\rho$ is a constant correlation parameter such that $0 < \\rho < 1$.\n\nThe current state of the Markov chain is $(X_t, Y_t) = (c, c)$, where $c$ is a positive constant. Two different symmetric random-walk proposal schemes are considered to generate a candidate state $(X', Y')$.\n\n**Scheme 1:** A proposed state $(X'_1, Y'_1)$ is generated such that the step taken is $(X'_1 - X_t, Y'_1 - Y_t) = (\\epsilon, -\\epsilon)$ for some small positive constant $\\epsilon$. Thus, the proposed state is $(X'_1, Y'_1) = (c+\\epsilon, c-\\epsilon)$.\n\n**Scheme 2:** A proposed state $(X'_2, Y'_2)$ is generated such that the step taken is $(X'_2 - X_t, Y'_2 - Y_t) = (\\epsilon, \\epsilon)$, using the same constant $\\epsilon$. Thus, the proposed state is $(X'_2, Y'_2) = (c+\\epsilon, c+\\epsilon)$.\n\nLet $\\alpha_1$ be the Metropolis-Hastings acceptance probability for the proposal under Scheme 1, and let $\\alpha_2$ be the acceptance probability for the proposal under Scheme 2. Determine the ratio $R = \\frac{\\alpha_2}{\\alpha_1}$. Express your answer as a single closed-form analytic expression in terms of $c$, $\\epsilon$, and $\\rho$.\n\n", "solution": "For a symmetric random-walk Metropolis-Hastings proposal, the acceptance probability is $\\alpha=\\min\\{1, \\pi(x',y')/\\pi(x,y)\\}$. Since the proposal is symmetric in both schemes, the Hastings ratio cancels and\n$$\n\\frac{\\pi(x',y')}{\\pi(c,c)}=\\exp\\left(E(x',y')-E(c,c)\\right),\n$$\nwhere the log-density is\n$$\nE(x,y)=-\\frac{x^{2}-2\\rho xy+y^{2}}{2(1-\\rho^{2})},\n$$\nand we define\n$$\nQ(x,y)=x^{2}-2\\rho xy+y^{2},\\quad E(x,y)=-\\frac{Q(x,y)}{2(1-\\rho^{2})}.\n$$\nThus\n$$\n\\ln\\left(\\frac{\\pi(x',y')}{\\pi(c,c)}\\right)=-\\frac{Q(x',y')-Q(c,c)}{2(1-\\rho^{2})}.\n$$\n\nAt the current state $(c,c)$,\n$$\nQ(c,c)=c^{2}-2\\rho c^{2}+c^{2}=2c^{2}(1-\\rho).\n$$\n\nScheme 1: $(x',y')=(c+\\epsilon,c-\\epsilon)$. Compute\n$$\nx'^{2}=c^{2}+2c\\epsilon+\\epsilon^{2},\\quad y'^{2}=c^{2}-2c\\epsilon+\\epsilon^{2},\\quad x'y'=c^{2}-\\epsilon^{2}.\n$$\nHence\n$$\nQ_{1}=x'^{2}-2\\rho x'y'+y'^{2}=(2c^{2}+2\\epsilon^{2})-2\\rho(c^{2}-\\epsilon^{2})=2c^{2}(1-\\rho)+2\\epsilon^{2}(1+\\rho).\n$$\nTherefore\n$$\nQ_{1}-Q(c,c)=2\\epsilon^{2}(1+\\rho),\n$$\nand\n$$\n\\ln\\left(\\frac{\\pi(x'_{1},y'_{1})}{\\pi(c,c)}\\right)=-\\frac{2\\epsilon^{2}(1+\\rho)}{2(1-\\rho^{2})}=-\\frac{\\epsilon^{2}}{1-\\rho}.\n$$\nSince this is negative for $0<\\rho<1$, the acceptance probability is\n$$\n\\alpha_{1}=\\exp\\left(-\\frac{\\epsilon^{2}}{1-\\rho}\\right).\n$$\n\nScheme 2: $(x',y')=(c+\\epsilon,c+\\epsilon)$. Compute\n$$\nx'^{2}=c^{2}+2c\\epsilon+\\epsilon^{2},\\quad y'^{2}=c^{2}+2c\\epsilon+\\epsilon^{2},\\quad x'y'=c^{2}+2c\\epsilon+\\epsilon^{2}.\n$$\nHence\n$$\nQ_{2}=x'^{2}-2\\rho x'y'+y'^{2}=2(1-\\rho)(c^{2}+2c\\epsilon+\\epsilon^{2}),\n$$\nso\n$$\nQ_{2}-Q(c,c)=2(1-\\rho)(2c\\epsilon+\\epsilon^{2}),\n$$\nand\n$$\n\\ln\\left(\\frac{\\pi(x'_{2},y'_{2})}{\\pi(c,c)}\\right)=-\\frac{2(1-\\rho)(2c\\epsilon+\\epsilon^{2})}{2(1-\\rho^{2})}=-\\frac{2c\\epsilon+\\epsilon^{2}}{1+\\rho}.\n$$\nThis is also negative for $c>0$, $\\epsilon>0$, $0<\\rho<1$, so\n$$\n\\alpha_{2}=\\exp\\left(-\\frac{2c\\epsilon+\\epsilon^{2}}{1+\\rho}\\right).\n$$\n\nTherefore, the ratio $R=\\alpha_{2}/\\alpha_{1}$ is\n$$\nR=\\exp\\left(-\\frac{2c\\epsilon+\\epsilon^{2}}{1+\\rho}+\\frac{\\epsilon^{2}}{1-\\rho}\\right)\n=\\exp\\left(-\\frac{2c\\epsilon}{1+\\rho}+\\epsilon^{2}\\left[\\frac{1}{1-\\rho}-\\frac{1}{1+\\rho}\\right]\\right)\n=\\exp\\left(-\\frac{2c\\epsilon}{1+\\rho}+\\frac{2\\rho\\,\\epsilon^{2}}{1-\\rho^{2}}\\right).\n$$", "answer": "$$\\boxed{\\exp\\left(\\frac{2\\rho\\,\\epsilon^{2}}{1-\\rho^{2}}-\\frac{2c\\,\\epsilon}{1+\\rho}\\right)}$$"}]}