{"hands_on_practices": [{"introduction": "The Central Limit Theorem's (CLT) most fundamental application is on sums of independent and identically distributed random variables. This exercise [@problem_id:2405591] uses the intuitive scenario of a multiple-choice test to provide a direct, hands-on verification of the theorem. By modeling each question as a Bernoulli trial (correct or incorrect), you will simulate how the total score—a sum of these simple, discrete variables—converges in distribution to the continuous, bell-shaped normal curve as the number of questions grows.", "id": "2405591", "problem": "Consider an idealized multiple-choice testing environment. A single student answers a test with $Q$ independent questions. Each question offers $c$ answer choices, exactly one of which is correct. For each question, the student either knows the answer with probability $k$ (in which case the correct answer is chosen with probability $1$) or does not know the answer with probability $1-k$ (in which case the student guesses uniformly among the $c$ choices, and the correct answer is chosen with probability $1/c$). Let $X_i$ denote the indicator that question $i$ is answered correctly, for $i \\in \\{1,\\dots,Q\\}$. The total number of correct answers (the test score) is $S_Q = \\sum_{i=1}^{Q} X_i$. For each fixed triple $(Q,c,k)$, the per-question correctness probability is $p = k + (1-k)/c$, so that $X_i$ are independent and identically distributed Bernoulli random variables with parameter $p$. Define the standardized score $Z_Q = \\dfrac{S_Q - Q p}{\\sqrt{Q p (1-p)}}$.\n\nBy the Central Limit Theorem (CLT), as $Q \\to \\infty$, the distribution of $Z_Q$ is approximately standard normal. Your task is to perform a Monte Carlo computation to assess this approximation in finite samples by comparing the empirical distribution of $Z_Q$ to the standard normal distribution using the Kolmogorov–Smirnov (KS) distance.\n\nFor each test case listed below, you must:\n- Generate $N$ independent draws of $S_Q$ from the model specified above, with $N = 80000$ draws per test case.\n- Use a fixed random seed equal to $20240517$ for all random number generation to ensure reproducibility across all test cases.\n- Compute the empirical cumulative distribution function (empirical CDF) $F_N$ of the standardized scores $Z_Q$ computed from the simulated $S_Q$ draws.\n- Compute the Kolmogorov–Smirnov distance $D_N = \\sup_{x \\in \\mathbb{R}} \\left| F_N(x) - \\Phi(x) \\right|$, where $\\Phi$ is the standard normal cumulative distribution function.\n\nReport one real number for each test case, namely the value of $D_N$ defined above. Express all reported numerical outputs as decimals (not using any percentage symbol).\n\nTest suite of parameter values:\n- Case $1$ (general case): $(Q,c,k) = (100,4,0.6)$.\n- Case $2$ (small aggregation boundary): $(Q,c,k) = (5,4,0.6)$.\n- Case $3$ (symmetric success probability, large aggregation): $(Q,c,k) = (400,2,0.0)$.\n- Case $4$ (rare success probability, large aggregation): $(Q,c,k) = (400,20,0.0)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the four cases, in order, as a comma-separated list enclosed in square brackets, for example, $[d_1,d_2,d_3,d_4]$, where each $d_j$ is the computed Kolmogorov–Smirnov distance $D_N$ for case $j \\in \\{1,2,3,4\\}$.", "solution": "The problem statement has been rigorously validated and is found to be sound. It is a well-posed problem in computational statistics, grounded in fundamental principles of probability theory, specifically the Central Limit Theorem (CLT). The task is to numerically assess the rate of convergence of a standardized sum of Bernoulli random variables to the standard normal distribution using a Monte Carlo simulation.\n\nThe model describes a total score $S_Q$ from a test with $Q$ questions. Each question $i$ is answered correctly with a fixed probability $p$, so the outcome is an independent Bernoulli trial $X_i \\sim \\text{Bernoulli}(p)$. The probability $p$ is derived from the law of total probability:\n$$\np = P(\\text{correct}) = P(\\text{correct} | \\text{knows})P(\\text{knows}) + P(\\text{correct} | \\text{guesses})P(\\text{guesses})\n$$\nGiven the parameters, this is $p = (1) \\cdot k + (1/c) \\cdot (1-k)$, which simplifies to $p = k + (1-k)/c$.\n\nThe total score $S_Q = \\sum_{i=1}^{Q} X_i$ is the sum of $Q$ independent and identically distributed (i.i.d.) Bernoulli random variables. Therefore, $S_Q$ follows a Binomial distribution, $S_Q \\sim \\text{Binomial}(Q, p)$. The expected value (mean) of $S_Q$ is $E[S_Q] = Qp$, and its variance is $\\text{Var}(S_Q) = Qp(1-p)$.\n\nThe Central Limit Theorem asserts that the distribution of the standardized score,\n$$\nZ_Q = \\frac{S_Q - E[S_Q]}{\\sqrt{\\text{Var}(S_Q)}} = \\frac{S_Q - Qp}{\\sqrt{Qp(1-p)}}\n$$\napproaches the standard normal distribution $\\mathcal{N}(0,1)$ as $Q \\to \\infty$. The problem requires a quantitative evaluation of this approximation for finite $Q$ using the Kolmogorov-Smirnov (KS) distance.\n\nThe computational procedure is as follows:\n$1$. For each test case specified by the triple $(Q, c, k)$, the success probability $p = k + (1-k)/c$ is first computed.\n$2$. A Monte Carlo simulation is performed by generating $N = 80000$ independent samples of the total score, $\\{s_1, s_2, \\dots, s_N\\}$, from the distribution $S_Q \\sim \\text{Binomial}(Q, p)$. This is computationally more efficient than simulating $N \\times Q$ individual Bernoulli trials. A fixed random seed of $20240517$ is used to ensure the reproducibility of the results.\n$3$. Each simulated score $s_j$ is standardized to obtain a sample of the standardized score, $\\{z_1, z_2, \\dots, z_N\\}$, where each $z_j$ is calculated as:\n$$\nz_j = \\frac{s_j - Qp}{\\sqrt{Qp(1-p)}}\n$$\n$4$. The Kolmogorov-Smirnov distance, $D_N$, is computed between the empirical cumulative distribution function (ECDF) of the standardized sample $\\{z_j\\}$ and the theoretical CDF of the standard normal distribution, $\\Phi(x)$. The ECDF is given by $F_N(x) = \\frac{1}{N}\\sum_{j=1}^{N} \\mathbb{I}(z_j \\le x)$, where $\\mathbb{I}$ is the indicator function. The KS distance is defined as:\n$$\nD_N = \\sup_{x \\in \\mathbb{R}} |F_N(x) - \\Phi(x)|\n$$\nThis distance is calculated for each of the four test cases. The `scipy.stats.kstest` function provides a direct and numerically stable implementation for this computation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef solve():\n    \"\"\"\n    Computes the Kolmogorov-Smirnov distance between the empirical distribution\n    of a standardized test score and the standard normal distribution for several\n    test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (100, 4, 0.6),   # Case 1: general case\n        (5, 4, 0.6),     # Case 2: small aggregation boundary\n        (400, 2, 0.0),   # Case 3: symmetric success probability, large aggregation\n        (400, 20, 0.0),  # Case 4: rare success probability, large aggregation\n    ]\n\n    # Simulation parameters\n    N_draws = 80000\n    seed = 20240517\n\n    # Initialize a random number generator with the specified seed for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for case in test_cases:\n        Q, c, k = case\n\n        # Step 1: Calculate the per-question correctness probability 'p'.\n        # p = k + (1-k)/c\n        p = k + (1.0 - k) / c\n\n        # Step 2: Generate N draws of the total score S_Q from a Binomial distribution.\n        # S_Q ~ Binomial(Q, p)\n        # Using rng.binomial is more efficient than simulating Q Bernoulli trials N times.\n        s_q_samples = rng.binomial(n=Q, p=p, size=N_draws)\n\n        # Step 3: Compute the standardized scores Z_Q.\n        # Z_Q = (S_Q - E[S_Q]) / sqrt(Var(S_Q))\n        # E[S_Q] = Q * p\n        # Var(S_Q) = Q * p * (1-p)\n        mean_s_q = Q * p\n        std_dev_s_q = np.sqrt(Q * p * (1.0 - p))\n        \n        # Avoid division by zero if variance is zero (p=0 or p=1).\n        # In such a case, S_Q is constant and Z_Q is not well-defined. The KS distance\n        # would be maximal, but the problem cases avoid this scenario.\n        if std_dev_s_q == 0:\n            # This case is not expected based on problem description,\n            # but as a matter of robust implementation, it should be handled.\n            # a constant variable's CDF is a step function. The distance to a\n            # continuous CDF like the normal a distance of 0.5.\n            ks_distance = 0.5\n        else:\n            z_q_samples = (s_q_samples - mean_s_q) / std_dev_s_q\n\n            # Step 4: Compute the Kolmogorov-Smirnov distance D_N.\n            # kstest compares the empirical CDF of the sample with the theoretical\n            # standard normal CDF ('norm'). It returns the KS statistic and the p-value.\n            # We only need the statistic, which corresponds to D_N.\n            ks_statistic, _ = kstest(z_q_samples, 'norm')\n            ks_distance = ks_statistic\n        \n        results.append(ks_distance)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Moving from abstract theory to applied econometrics, this practice explores the statistical properties of the Ordinary Least Squares (OLS) estimator. This exercise [@problem_id:2405562] reveals that the OLS estimator, a cornerstone of financial and economic modeling, is itself a random variable constructed as a weighted sum of the model's underlying error terms. Understanding this structure allows you to see why the CLT governs its sampling distribution, justifying the use of normal-based statistics for hypothesis testing and constructing confidence intervals for regression coefficients.", "id": "2405562", "problem": "Consider a simple linear regression without intercept, where for each replication you observe $\\{(x_i,y_i)\\}_{i=1}^n$ generated by $y_i=\\beta x_i+\\epsilon_i$ with $\\{x_i\\}_{i=1}^n$ independent and identically distributed with $x_i\\sim \\mathcal{N}(0,1)$ and $\\{\\epsilon_i\\}_{i=1}^n$ independent of $\\{x_i\\}_{i=1}^n$. Let $\\hat{\\beta}$ denote the Ordinary Least Squares (OLS) estimator. Your program must, for each test case below, perform a large number of independent replications, and for each replication: (i) compute $\\hat{\\beta}$ and verify the exact decomposition of $\\hat{\\beta}-\\beta$ as a weighted sum of the error terms with weights that depend only on the realized regressors, and (ii) standardize $\\hat{\\beta}$ by the exact conditional variance given the regressors, and assess whether the standardized estimator has a standard normal distribution according to a formal goodness-of-fit test.\n\nDefinitions and requirements:\n- Ordinary Least Squares (OLS): the estimator $\\hat{\\beta}$ minimizes the sum of squared residuals $\\sum_{i=1}^n (y_i-b x_i)^2$ over $b\\in\\mathbb{R}$.\n- Show that $\\hat{\\beta}-\\beta$ can be written as a finite weighted sum $\\sum_{i=1}^n w_i\\epsilon_i$ where the weights $\\{w_i\\}_{i=1}^n$ depend only on $\\{x_i\\}_{i=1}^n$, and numerically verify this identity in each replication within an absolute tolerance $\\tau=10^{-10}$.\n- For each replication, standardize $\\hat{\\beta}$ by dividing by the exact conditional standard deviation $\\sqrt{\\mathrm{Var}(\\hat{\\beta}\\mid \\{x_i\\}_{i=1}^n)}$, where the conditional variance must be derived from first principles and expressed in terms of the weights $\\{w_i\\}_{i=1}^n$ and the conditional variances of $\\{\\epsilon_i\\}_{i=1}^n$ given $\\{x_i\\}_{i=1}^n$.\n- Assess normality of the standardized estimator using a Kolmogorov–Smirnov goodness-of-fit test against the standard normal distribution at significance level $\\alpha=0.05$. Report a boolean that is true if the null hypothesis of standard normality is not rejected and false otherwise.\n- Use a fixed pseudo-random number generator seed equal to $123456789$ so that results are reproducible across implementations.\n\nTest suite:\n- Case A (benchmark homoskedastic normal errors): $n=100$, $\\beta=1.7$, $\\epsilon_i\\sim \\mathcal{N}(0,1)$, number of independent replications $R=4000$.\n- Case B (heavy tails with finite variance, large sample): $n=1000$, $\\beta=-0.8$, $\\epsilon_i = s\\cdot t_{\\nu}$ with $\\nu=3$ and $s=\\sqrt{(\\nu-2)/\\nu}$ so that $\\mathrm{Var}(\\epsilon_i)=1$, number of independent replications $R=4000$.\n- Case C (heavy tails with finite variance, small sample): $n=30$, $\\beta=0.0$, $\\epsilon_i = s\\cdot t_{\\nu}$ with $\\nu=3$ and $s=\\sqrt{(\\nu-2)/\\nu}$ so that $\\mathrm{Var}(\\epsilon_i)=1$, number of independent replications $R=4000$.\n- Case D (heteroskedastic normal errors): $n=1000$, $\\beta=2.0$, $\\epsilon_i\\sim \\mathcal{N}(0,\\sigma_i^2)$ with $\\sigma_i^2 = 1+0.5\\cdot i/n$ for $i\\in\\{1,\\dots,n\\}$, number of independent replications $R=4000$.\n\nFor each case, you must generate fresh regressors $\\{x_i\\}_{i=1}^n$ and errors $\\{\\epsilon_i\\}_{i=1}^n$ independently across replications, enforce the specified distributional assumptions, and use the exact conditional variance for standardization. For the heteroskedastic case, the conditional variance must account for the non-constant error variances.\n\nFinal output requirements:\n- For each case, produce two booleans in order: first, whether the weighted-sum identity holds within tolerance $\\tau$ for all replications, and second, whether standard normality is not rejected at level $\\alpha$ for the collection of standardized estimators across the $R$ replications.\n- Aggregate the results for the four cases into a single line of output as a comma-separated list enclosed in square brackets, in the exact order\n$[\\text{A\\_identity},\\text{A\\_normality},\\text{B\\_identity},\\text{B\\_normality},\\text{C\\_identity},\\text{C\\_normality},\\text{D\\_identity},\\text{D\\_normality}]$.\n- The only acceptable outputs are booleans. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[ \\text{True},\\text{False},\\dots ]$).", "solution": "The problem statement is subjected to rigorous validation and is found to be valid. It is scientifically grounded in established principles of econometrics and statistics, well-posed with all necessary data and conditions provided, and articulated using objective, unambiguous language. The problem is a standard exercise in computational statistics, designed to verify theoretical properties of the Ordinary Least Squares (OLS) estimator and explore the applicability of the Central Limit Theorem (CLT) under different distributional assumptions for the error term.\n\nWe proceed with the solution, which requires analytical derivations followed by numerical simulation.\n\n**1. Derivation of the OLS Estimator $\\hat{\\beta}$**\n\nThe model is a simple linear regression without an intercept:\n$$ y_i = \\beta x_i + \\epsilon_i $$\nThe OLS estimator $\\hat{\\beta}$ is the value of $b$ that minimizes the Sum of Squared Residuals (SSR), $S(b)$:\n$$ S(b) = \\sum_{i=1}^{n} (y_i - b x_i)^2 $$\nTo find the minimum, we take the derivative of $S(b)$ with respect to $b$ and set it to zero. This gives the first-order condition (FOC):\n$$ \\frac{dS}{db} = \\sum_{i=1}^{n} 2(y_i - b x_i)(-x_i) = -2 \\sum_{i=1}^{n} (x_i y_i - b x_i^2) = 0 $$\nSolving for $b$ gives the OLS estimator $\\hat{\\beta}$:\n$$ \\sum_{i=1}^{n} x_i y_i = \\hat{\\beta} \\sum_{i=1}^{n} x_i^2 $$\n$$ \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sum_{i=1}^{n} x_i^2} $$\nThe second-order condition for a minimum is satisfied, as $\\frac{d^2S}{db^2} = 2 \\sum_{i=1}^{n} x_i^2 > 0$ almost surely, since the regressors $\\{x_i\\}$ are drawn from a continuous distribution and are not all zero with probability $1$.\n\n**2. Derivation of the Weighted-Sum Identity**\n\nWe must show that $\\hat{\\beta} - \\beta$ can be expressed as a weighted sum of the error terms, $\\sum_{i=1}^n w_i\\epsilon_i$. We substitute $y_i = \\beta x_i + \\epsilon_i$ into the formula for $\\hat{\\beta}$:\n$$ \\hat{\\beta} = \\frac{\\sum_{i=1}^{n} x_i (\\beta x_i + \\epsilon_i)}{\\sum_{j=1}^{n} x_j^2} = \\frac{\\beta \\sum_{i=1}^{n} x_i^2 + \\sum_{i=1}^{n} x_i \\epsilon_i}{\\sum_{j=1}^{n} x_j^2} $$\nSeparating the terms yields:\n$$ \\hat{\\beta} = \\frac{\\beta \\sum_{i=1}^{n} x_i^2}{\\sum_{j=1}^{n} x_j^2} + \\frac{\\sum_{i=1}^{n} x_i \\epsilon_i}{\\sum_{j=1}^{n} x_j^2} = \\beta + \\sum_{i=1}^{n} \\left( \\frac{x_i}{\\sum_{j=1}^{n} x_j^2} \\right) \\epsilon_i $$\nRearranging gives the desired expression for the estimation error:\n$$ \\hat{\\beta} - \\beta = \\sum_{i=1}^{n} w_i \\epsilon_i $$\nwhere the weights $w_i$ are given by:\n$$ w_i = \\frac{x_i}{\\sum_{j=1}^{n} x_j^2} $$\nThese weights depend only on the sample of regressors $\\{x_i\\}_{i=1}^n$, as required. This algebraic identity will be verified numerically to within a tolerance $\\tau = 10^{-10}$ in each replication.\n\n**3. Derivation of the Conditional Variance of $\\hat{\\beta}$**\n\nWe are asked to derive the variance of $\\hat{\\beta}$ conditional on the set of regressors $\\{x_i\\}_{i=1}^n$. When we condition on $\\{x_i\\}$, the regressors and thus the weights $\\{w_i\\}$ are treated as non-random constants.\n$$ \\mathrm{Var}(\\hat{\\beta} | \\{x_i\\}_{i=1}^n) = \\mathrm{Var}\\left(\\beta + \\sum_{i=1}^{n} w_i \\epsilon_i \\bigg| \\{x_i\\}_{i=1}^n\\right) $$\nSince $\\beta$ is a constant, its variance is zero. The error terms $\\{\\epsilon_i\\}$ are independent of each other and independent of the regressors $\\{x_i\\}$. Therefore:\n$$ \\mathrm{Var}(\\hat{\\beta} | \\{x_i\\}) = \\mathrm{Var}\\left(\\sum_{i=1}^{n} w_i \\epsilon_i \\bigg| \\{x_i\\}\\right) = \\sum_{i=1}^{n} w_i^2 \\mathrm{Var}(\\epsilon_i | \\{x_i\\}) = \\sum_{i=1}^{n} w_i^2 \\mathrm{Var}(\\epsilon_i) $$\nLet $\\sigma_{\\epsilon,i}^2 = \\mathrm{Var}(\\epsilon_i)$. The general formula for the conditional variance is:\n$$ \\mathrm{Var}(\\hat{\\beta} | \\{x_i\\}) = \\sum_{i=1}^{n} \\left(\\frac{x_i}{\\sum_{j=1}^{n} x_j^2}\\right)^2 \\sigma_{\\epsilon,i}^2 = \\frac{\\sum_{i=1}^{n} x_i^2 \\sigma_{\\epsilon,i}^2}{\\left(\\sum_{j=1}^{n} x_j^2\\right)^2} $$\nWe apply this general formula to each case:\n- **Cases A, B, C (Homoskedastic Errors):** In these cases, the errors have a constant variance, $\\sigma_{\\epsilon,i}^2 = 1$ for all $i$. The formula simplifies significantly:\n$$ \\mathrm{Var}(\\hat{\\beta} | \\{x_i\\}) = \\frac{\\sum_{i=1}^{n} x_i^2 (1)}{\\left(\\sum_{j=1}^{n} x_j^2\\right)^2} = \\frac{\\sum_{i=1}^{n} x_i^2}{\\left(\\sum_{j=1}^{n} x_j^2\\right)^2} = \\frac{1}{\\sum_{j=1}^{n} x_j^2} $$\n- **Case D (Heteroskedastic Errors):** The error variances are non-constant, $\\sigma_{\\epsilon,i}^2 = 1 + 0.5 \\cdot i/n$. We must use the general formula:\n$$ \\mathrm{Var}(\\hat{\\beta} | \\{x_i\\}) = \\frac{\\sum_{i=1}^{n} x_i^2 (1 + 0.5 \\cdot i/n)}{\\left(\\sum_{j=1}^{n} x_j^2\\right)^2} $$\n\n**4. Distribution of the Standardized Estimator**\n\nFor each replication, we compute the standardized estimator $Z$:\n$$ Z = \\frac{\\hat{\\beta} - \\beta}{\\sqrt{\\mathrm{Var}(\\hat{\\beta} | \\{x_i\\})}} $$\nWe assess its distribution.\n- **Cases A and D:** The errors $\\{\\epsilon_i\\}$ are drawn from Normal distributions. The estimation error, $\\hat{\\beta} - \\beta = \\sum w_i \\epsilon_i$, is a linear combination of Normal random variables (conditional on $\\{x_i\\}$). Thus, it is also conditionally Normal with mean $E[\\sum w_i \\epsilon_i]=0$ and variance $\\mathrm{Var}(\\hat{\\beta} | \\{x_i\\})$. The standardized estimator $Z$ is therefore Conditionally $\\mathcal{N}(0,1)$. Since its distribution is $\\mathcal{N}(0,1)$ for any realization of $\\{x_i\\}$, its unconditional distribution is also exactly $\\mathcal{N}(0,1)$. We expect the Kolmogorov-Smirnov (KS) test to not reject the null hypothesis of standard normality.\n\n- **Cases B and C:** The errors $\\{\\epsilon_i\\}$ are drawn from a scaled Student's t-distribution with $\\nu=3$ degrees of freedom. This distribution is symmetric and has finite variance, but it is not Normal and has heavy tails (its fourth moment is infinite). The estimation error is a sum of independent, non-identically distributed random variables, $w_i\\epsilon_i$. A Central Limit Theorem (CLT) for such sums suggests that the distribution of the standardized estimator will approach $\\mathcal{N}(0,1)$ as the sample size $n \\to \\infty$.\n    - **Case B ($n=1000$):** The sample size is large. The CLT should provide a very good approximation. We expect the KS test to not reject the null.\n    - **Case C ($n=30$):** The sample size is small. The convergence to normality guaranteed by the CLT can be slow, especially for heavy-tailed error distributions. We expect the distribution of the standardized estimator to still exhibit fatter tails than a standard Normal, leading the KS test to reject the null hypothesis.\n\n**5. Simulation Procedure**\n\nFor each of the four cases, we perform $R=4000$ replications. In each replication:\n1. Generate a sample of regressors $\\{x_i\\}_{i=1}^n$ from $\\mathcal{N}(0,1)$.\n2. Generate a sample of errors $\\{\\epsilon_i\\}_{i=1}^n$ from the specified distribution for the case.\n3. Compute $y_i = \\beta x_i + \\epsilon_i$.\n4. Calculate the OLS estimate $\\hat{\\beta}$.\n5. Verify the identity $|\\hat{\\beta} - \\beta - \\sum w_i \\epsilon_i| < 10^{-10}$. A flag tracks if this holds for all $R$ replications.\n6. Calculate the exact conditional variance $\\mathrm{Var}(\\hat{\\beta} | \\{x_i\\})$ using the appropriate formula derived above.\n7. Compute the standardized estimator $Z$ and store it.\n\nAfter all replications for a case, the collection of $R$ values of $Z$ is tested against a standard normal distribution using the KS test at a significance level $\\alpha=0.05$. The null hypothesis is not rejected if the p-value is greater than or equal to $\\alpha$. The results (identity verification and normality test outcome) are recorded as booleans for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest\n\ndef run_simulation(n, beta, err_type, R, tau, alpha):\n    \"\"\"\n    Performs a simulation for a single test case.\n\n    Args:\n        n (int): Sample size.\n        beta (float): True coefficient.\n        err_type (str): Type of error distribution ('norm', 't', 'hetero').\n        R (int): Number of replications.\n        tau (float): Tolerance for identity check.\n        alpha (float): Significance level for KS test.\n\n    Returns:\n        tuple[bool, bool]: A tuple containing:\n            - bool: True if the identity holds for all replications.\n            - bool: True if the normality hypothesis is not rejected.\n    \"\"\"\n    \n    # Store standardized estimators for the KS test\n    z_values = np.zeros(R)\n    \n    # Flag to track if the identity holds across all replications\n    identity_holds_all_reps = True\n\n    # Define error variance parameters for specific cases.\n    nu = 3.0  # Degrees of freedom for t-distribution\n    s = np.sqrt((nu - 2.0) / nu)  # Scale factor for t-distribution\n    \n    # Pre-calculate heteroskedastic variances if needed\n    if err_type == 'hetero':\n        # i is 1-based, so we use arange(1, n+1)\n        sigma_sq_hetero = 1.0 + 0.5 * np.arange(1, n + 1) / n\n\n    for i in range(R):\n        # 1. Generate regressors\n        x = np.random.normal(loc=0.0, scale=1.0, size=n)\n        \n        # 2. Generate errors based on type\n        if err_type == 'norm':\n            epsilon = np.random.normal(loc=0.0, scale=1.0, size=n)\n            sigma_sq_eps = 1.0\n        elif err_type == 't':\n            epsilon = np.random.standard_t(df=nu, size=n) * s\n            sigma_sq_eps = 1.0\n        elif err_type == 'hetero':\n            epsilon = np.random.normal(loc=0.0, scale=np.sqrt(sigma_sq_hetero))\n            sigma_sq_eps = sigma_sq_hetero\n        else:\n            raise ValueError(\"Invalid error type specified.\")\n\n        # 3. Generate dependent variable\n        y = beta * x + epsilon\n\n        # 4. Calculate OLS estimator\n        sum_x_sq = np.sum(x**2)\n        if sum_x_sq == 0: continue # Should not happen with continuous x\n        beta_hat = np.sum(x * y) / sum_x_sq\n\n        # 5. Verify the weighted-sum identity\n        if identity_holds_all_reps: # Avoid repeated calculations if already failed\n            lhs = beta_hat - beta\n            weights = x / sum_x_sq\n            rhs = np.sum(weights * epsilon)\n            if np.abs(lhs - rhs) > tau:\n                identity_holds_all_reps = False\n\n        # 6. Calculate the exact conditional variance\n        if err_type in ['norm', 't']: # Homoskedastic case\n            var_cond = 1.0 / sum_x_sq\n        else: # Heteroskedastic case\n            var_cond = np.sum(x**2 * sigma_sq_eps) / (sum_x_sq**2)\n        \n        std_cond = np.sqrt(var_cond)\n\n        # 7. Compute and store the standardized estimator\n        z_values[i] = (beta_hat - beta) / std_cond\n\n    # Perform Kolmogorov-Smirnov test for normality\n    ks_statistic, p_value = kstest(z_values, 'norm')\n    \n    # Null hypothesis is not rejected if p-value is >= alpha\n    normality_not_rejected = (p_value >= alpha)\n\n    return identity_holds_all_reps, normality_not_rejected\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Set the fixed seed for reproducibility\n    np.random.seed(123456789)\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 100, 'beta': 1.7, 'err_type': 'norm'},   # Case A\n        {'n': 1000, 'beta': -0.8, 'err_type': 't'},     # Case B\n        {'n': 30, 'beta': 0.0, 'err_type': 't'},      # Case C\n        {'n': 1000, 'beta': 2.0, 'err_type': 'hetero'} # Case D\n    ]\n    \n    # Global parameters\n    R = 4000\n    tau = 1e-10\n    alpha = 0.05\n    \n    results = []\n    for case_params in test_cases:\n        identity_ok, normality_ok = run_simulation(\n            n=case_params['n'],\n            beta=case_params['beta'],\n            err_type=case_params['err_type'],\n            R=R,\n            tau=tau,\n            alpha=alpha\n        )\n        results.extend([identity_ok, normality_ok])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "The Central Limit Theorem is powerful, but its conclusions rely on critical assumptions about the underlying data. This is especially important in finance, where asset returns can exhibit \"heavy tails.\" This exercise [@problem_id:2405635] explores the boundaries of the CLT by using the Pareto distribution, a classic model for such heavy-tailed phenomena, where the conditions of finite mean or finite variance may not hold. By simulating a process where the CLT's assumptions are violated, you will witness firsthand why a normal approximation can be dangerously misleading and appreciate the necessity of validating statistical assumptions in real-world modeling.", "id": "2405635", "problem": "Consider independent and identically distributed random variables $X_1, X_2, \\dots, X_n$ drawn from the Type I Pareto distribution with scale parameter $x_m = 1$ and tail index (shape) parameter $\\alpha > 0$. The cumulative distribution function is $F(x) = 1 - x^{-\\alpha}$ for $x \\ge 1$. It is a well-tested fact that the finite mean exists if and only if $\\alpha > 1$, in which case the population mean is $\\mu = \\dfrac{\\alpha}{\\alpha - 1}$; and the finite variance exists if and only if $\\alpha > 2$. The Law of Large Numbers (LLN) requires a finite mean for convergence of the sample mean to the population mean. The Central Limit Theorem (CLT) requires both a finite mean and a finite, nonzero variance for the standardized sample mean to converge in distribution to a Standard Normal distribution.\n\nYour task is to design and implement a computational experiment that tests, via simulation, the failure of the Central Limit Theorem for Pareto distributions with $\\alpha \\le 2$ by examining the distribution of the standardized sample mean, and to demonstrate explosive scaling when the mean is infinite ($\\alpha \\le 1$). The problem is framed for computational economics and finance, where heavy-tailed return and loss distributions invalidate Gaussian approximations used in risk aggregation.\n\nYou must implement the following, starting from the foundational definitions above and without using any shortcut formulas beyond these well-tested facts:\n\n1. For each $\\alpha \\in \\{1.5, 2.0\\}$ and for each sample size $n \\in \\{200, 1000, 3000\\}$, simulate $R = 4000$ independent replications of samples of size $n$ from the Pareto($\\alpha$) distribution with $x_m = 1$ using inverse transform sampling based on the fact that if $U \\sim \\text{Uniform}(0,1)$ then $X = U^{-1/\\alpha}$ has the Type I Pareto($\\alpha$) distribution with $x_m=1$. For each replication, compute the Studentized mean\n$$\nT_{n} \\;=\\; \\frac{\\sqrt{n}\\,\\big(\\overline{X}_n - \\mu\\big)}{S_n},\n$$\nwhere $\\mu = \\frac{\\alpha}{\\alpha-1}$ is the true mean (which exists for $\\alpha > 1$), $\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$ is the sample mean, and $S_n$ is the sample standard deviation with divisor $n-1$. Using the $R$ values of $T_n$, compute the Kolmogorov–Smirnov distance to the Standard Normal distribution,\n$$\nD_{n} \\;=\\; \\sup_{x \\in \\mathbb{R}} \\Big| \\widehat{F}_{T_n}(x) - \\Phi(x) \\Big|,\n$$\nwhere $\\widehat{F}_{T_n}$ is the empirical distribution function of the $R$ realizations of $T_n$ and $\\Phi$ is the Standard Normal cumulative distribution function. Report $D_n$ for each pair $(\\alpha, n)$.\n\n2. For the case $\\alpha = 0.8$ (infinite mean), for each $n \\in \\{200, 1000, 3000\\}$, simulate $R = 4000$ independent replications and compute the median of the absolute scaled sample mean,\n$$\nM_n \\;=\\; \\operatorname{median}\\Big( \\big| \\sqrt{n}\\,\\overline{X}_n \\big| \\Big),\n$$\nacross replications. Then compute the boolean indicator\n$$\n\\text{INC} \\;=\\; \\big( M_{200} < M_{1000} \\big) \\land \\big( M_{1000} < M_{3000} \\big),\n$$\nwhich tests strict growth of the typical magnitude of $\\sqrt{n}\\,\\overline{X}_n$ with $n$. This growth demonstrates that the sequence $\\{\\sqrt{n}\\,\\overline{X}_n\\}$ is not tight and therefore cannot converge to a Standard Normal distribution.\n\nTo ensure scientific realism, use the same random seed $s = 123456$ for all simulations to make the results reproducible. Use $R = 4000$ replications for each $(\\alpha,n)$ combination. You may generate samples in batches to control memory use, but the results must be exactly those implied by the given seed and specifications.\n\nTest suite and required outputs:\n- Use the parameter sets described above, namely:\n  - For $\\alpha = 1.5$: $n \\in \\{200, 1000, 3000\\}$, report the three values $D_{200}$, $D_{1000}$, $D_{3000}$.\n  - For $\\alpha = 2.0$: $n \\in \\{200, 1000, 3000\\}$, report the three values $D_{200}$, $D_{1000}$, $D_{3000}$.\n  - For $\\alpha = 0.8$: $n \\in \\{200, 1000, 3000\\}$, report the boolean $\\text{INC}$ and the three medians $M_{200}$, $M_{1000}$, $M_{3000}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be ordered as\n$$\n\\big[ D_{200}^{(\\alpha=1.5)}, D_{1000}^{(\\alpha=1.5)}, D_{3000}^{(\\alpha=1.5)}, D_{200}^{(\\alpha=2.0)}, D_{1000}^{(\\alpha=2.0)}, D_{3000}^{(\\alpha=2.0)}, \\text{INC}^{(\\alpha=0.8)}, M_{200}^{(\\alpha=0.8)}, M_{1000}^{(\\alpha=0.8)}, M_{3000}^{(\\alpha=0.8)} \\big].\n$$\nAll reported $D_n$ and $M_n$ must be real numbers (floats), and $\\text{INC}$ must be a boolean. No units are involved. Angles are not involved. The output must be exactly one line in the specified format.", "solution": "The problem statement is submitted for validation.\n\n**Step 1: Extract Givens**\n- Independent and identically distributed (i.i.d.) random variables: $X_1, X_2, \\dots, X_n$.\n- Distribution: Type I Pareto with scale parameter $x_m = 1$ and tail index (shape) parameter $\\alpha > 0$.\n- Cumulative Distribution Function (CDF): $F(x) = 1 - x^{-\\alpha}$ for $x \\ge 1$.\n- Existence of moments:\n    - Finite mean exists if and only if (iff) $\\alpha > 1$, with population mean $\\mu = \\dfrac{\\alpha}{\\alpha - 1}$.\n    - Finite variance exists iff $\\alpha > 2$.\n- Law of Large Numbers (LLN): Requires a finite mean.\n- Central Limit Theorem (CLT): Requires a finite mean and a finite, non-zero variance.\n- Simulation Method: Inverse transform sampling, $X = U^{-1/\\alpha}$ where $U \\sim \\text{Uniform}(0,1)$.\n- Simulation parameters:\n    - Replications: $R = 4000$.\n    - Random seed: $s = 123456$.\n- Task 1:\n    - Parameters: $\\alpha \\in \\{1.5, 2.0\\}$ and $n \\in \\{200, 1000, 3000\\}$.\n    - Statistic: Studentized mean $T_{n} = \\frac{\\sqrt{n}\\,\\big(\\overline{X}_n - \\mu\\big)}{S_n}$, where $\\overline{X}_n$ is the sample mean and $S_n$ is the sample standard deviation with divisor $n-1$.\n    - Metric: Kolmogorov–Smirnov distance $D_{n} = \\sup_{x \\in \\mathbb{R}} \\big| \\widehat{F}_{T_n}(x) - \\Phi(x) \\big|$, where $\\widehat{F}_{T_n}$ is the empirical CDF of $T_n$ and $\\Phi(x)$ is the standard normal CDF.\n- Task 2:\n    - Parameter: $\\alpha = 0.8$.\n    - Sample sizes: $n \\in \\{200, 1000, 3000\\}$.\n    - Statistic: $M_n = \\operatorname{median}\\Big( \\big| \\sqrt{n}\\,\\overline{X}_n \\big| \\Big)$ across $R$ replications.\n    - Metric: Boolean indicator $\\text{INC} = \\big( M_{200} < M_{1000} \\big) \\land \\big( M_{1000} < M_{3000} \\big)$.\n- Output Format: A single comma-separated list: $\\big[ D_{200}^{(\\alpha=1.5)}, D_{1000}^{(\\alpha=1.5)}, D_{3000}^{(\\alpha=1.5)}, D_{200}^{(\\alpha=2.0)}, D_{1000}^{(\\alpha=2.0)}, D_{3000}^{(\\alpha=2.0)}, \\text{INC}^{(\\alpha=0.8)}, M_{200}^{(\\alpha=0.8)}, M_{1000}^{(\\alpha=0.8)}, M_{3000}^{(\\alpha=0.8)} \\big]$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is evaluated against the validation criteria.\n- **Scientifically Grounded**: The problem is firmly based on the established statistical theory of the Central Limit Theorem, the Law of Large Numbers, and the properties of heavy-tailed distributions like the Pareto distribution. The use of Monte Carlo simulation to investigate the asymptotic behavior of estimators is a standard and rigorous method in computational statistics and econometrics.\n- **Well-Posed**: The problem is unambiguous. All parameters ($\\alpha, n, R, s$), statistics ($T_n, M_n$), and metrics ($D_n, \\text{INC}$) are precisely defined. The procedure is described algorithmically, which leads to a unique, verifiable numerical result.\n- **Objective**: The problem is stated in precise, objective mathematical and computational language, free of any subjectivity or opinion.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is a well-formulated computational experiment in statistics. A solution will be constructed.\n\n**Solution Derivation**\n\nThe core of this problem is the Central Limit Theorem (CLT), a cornerstone of statistics. The classical CLT states that for i.i.d. random variables $\\{X_i\\}$ with finite mean $\\mu$ and finite variance $\\sigma^2 > 0$, the standardized sample mean converges in distribution to a standard normal distribution:\n$$\n\\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1) \\quad \\text{as} \\quad n \\to \\infty\n$$\nThe problem asks to demonstrate via simulation what happens when these conditions are not met, specifically for the Pareto distribution, whose tail heaviness is controlled by the parameter $\\alpha$.\n\n**Theoretical Context of the Simulation**\n\n1.  **Case $\\alpha \\in \\{1.5, 2.0\\}$ (Finite Mean, Infinite Variance):**\n    For these values of $\\alpha$, the mean exists ($\\mu = 3$ for $\\alpha=1.5$ and $\\mu=2$ for $\\alpha=2.0$), but the variance is infinite. The standard CLT is not applicable. Instead, the Generalized CLT applies, which states that the sum of such variables, when appropriately centered and scaled, converges to a non-normal stable distribution. The aymptotic distribution of the Studentized mean, $T_n = \\sqrt{n}(\\overline{X}_n - \\mu)/S_n$, is also not normal because the sample standard deviation $S_n$ does not converge to a constant. The purpose of calculating the Kolmogorov-Smirnov distance $D_n$ to the standard normal CDF $\\Phi(x)$ is to quantify this failure of convergence to normality. We expect $D_n$ to remain significantly greater than zero, even for large $n$, demonstrating a persistent non-normal character.\n\n2.  **Case $\\alpha = 0.8$ (Infinite Mean):**\n    For $\\alpha < 1$, the mean of the Pareto distribution is infinite. This is a more severe violation of the CLT conditions; even the Law of Large Numbers fails. The sample mean $\\overline{X}_n$ does not converge to any constant. The theory of stable distributions indicates that for $\\alpha \\in (0, 2)$, the sum $\\sum X_i$ grows proportionally to $n^{1/\\alpha}$. Consequently, the sample mean $\\overline{X}_n = n^{-1}\\sum X_i$ grows proportionally to $n^{1/\\alpha - 1}$. The term investigated, $\\sqrt{n}\\,\\overline{X}_n$, should therefore scale with $n$ as $n^{1/2} \\cdot n^{1/\\alpha-1} = n^{1/\\alpha - 1/2}$. For $\\alpha=0.8$, this is $n^{1/0.8-0.5} = n^{1.25-0.5} = n^{0.75}$. Since the exponent is positive, this quantity is expected to grow with $n$. The simulation tests this by computing the median of its magnitude, $M_n$, and verifying that it is a strictly increasing function of $n$ for the chosen sample sizes. This provides computational evidence for the explosive nature of the sample mean when the population mean does not exist.\n\n**Implementation Plan**\n\nThe simulation will be implemented in Python using the `numpy` library for efficient vectorized computation and `scipy` for the Kolmogorov-Smirnov test.\n\n1.  **Initialization**: A random number generator will be seeded with $s=123456$ to ensure reproducibility.\n\n2.  **Part 1: Calculation of $D_n$ for $\\alpha \\in \\{1.5, 2.0\\}$**:\n    For each pair of $(\\alpha, n)$, we perform $R=4000$ replications.\n    a. Generate an $R \\times n$ matrix of uniform random numbers $U \\sim \\text{Uniform}(0,1)$.\n    b. Transform this matrix into Pareto-distributed variates using the inverse transform method: $X = U^{-1/\\alpha}$.\n    c. Compute the sample mean $\\overline{X}_n$ and sample standard deviation $S_n$ (with $n-1$ in the denominator, i.e., `ddof=1`) for each of the $R$ rows, yielding $R$ values for each statistic.\n    d. Calculate the true mean $\\mu = \\alpha/(\\alpha-1)$.\n    e. Compute the $R$ realizations of the Studentized mean $T_n = \\sqrt{n}(\\overline{X}_n - \\mu) / S_n$.\n    f. Calculate the Kolmogorov-Smirnov statistic $D_n$ by comparing the empirical distribution of the $R$ T-values to the standard normal distribution's CDF, $\\Phi(x)$. This is done using `scipy.stats.kstest`.\n\n3.  **Part 2: Calculation of $M_n$ and $\\text{INC}$ for $\\alpha=0.8$**:\n    For each $n \\in \\{200, 1000, 3000\\}$:\n    a. Generate an $R \\times n$ matrix of Pareto variates with $\\alpha=0.8$.\n    b. Compute the sample mean $\\overline{X}_n$ for each of the $R$ rows.\n    c. Calculate the statistic $|\\sqrt{n}\\,\\overline{X}_n|$ for each replication.\n    d. Find the median of these $R$ values to obtain $M_n$.\n    e. After calculating $M_{200}$, $M_{1000}$, and $M_{3000}$, the boolean indicator $\\text{INC}$ is evaluated based on the strict inequality condition.\n\n4.  **Output**: The computed values will be collected in the specified order and printed in the required format. This rigorous, vectorized approach ensures both correctness and computational efficiency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import kstest, norm\n\ndef solve():\n    \"\"\"\n    Performs a computational experiment to test the failure of the Central Limit Theorem\n    for Pareto distributions with heavy tails.\n    \"\"\"\n    # Define simulation parameters from the problem statement.\n    seed = 123456\n    R = 4000\n    rng = np.random.default_rng(seed)\n\n    all_results = []\n\n    # Part 1: Test convergence to Normal for alpha > 1\n    # Cases where mean is finite, but variance is infinite.\n    alphas_part1 = [1.5, 2.0]\n    n_values_part1 = [200, 1000, 3000]\n\n    for alpha in alphas_part1:\n        # The true mean exists for alpha > 1.\n        mu = alpha / (alpha - 1.0)\n        for n in n_values_part1:\n            # Generate R samples of size n in a vectorized manner.\n            # U ~ Uniform(0,1)\n            uniform_samples = rng.uniform(size=(R, n))\n            # X = U^(-1/alpha) gives Pareto(alpha) with x_m=1\n            pareto_samples = uniform_samples**(-1.0 / alpha)\n\n            # Compute sample means and standard deviations for each of the R replications.\n            sample_means = np.mean(pareto_samples, axis=1)\n            # Use ddof=1 for sample standard deviation (n-1 divisor).\n            sample_std_devs = np.std(pareto_samples, axis=1, ddof=1)\n\n            # Compute the Studentized mean T_n for each replication.\n            # Handle the unlikely case of S_n = 0.\n            T_n_values = np.full(R, np.nan)\n            valid_indices = sample_std_devs > 0\n            T_n_values[valid_indices] = np.sqrt(n) * (sample_means[valid_indices] - mu) / sample_std_devs[valid_indices]\n            \n            # Filter out any potential NaN values before the KS test.\n            T_n_values = T_n_values[~np.isnan(T_n_values)]\n\n            # Compute the Kolmogorov-Smirnov distance to the standard normal distribution.\n            # The 'statistic' attribute of the result is the D_n value.\n            ks_result = kstest(T_n_values, norm.cdf)\n            D_n = ks_result.statistic\n            all_results.append(D_n)\n\n    # Part 2: Test explosive scaling for alpha < 1\n    # Case where mean is infinite.\n    alpha_part2 = 0.8\n    n_values_part2 = [200, 1000, 3000]\n    medians = []\n\n    for n in n_values_part2:\n        # Generate R samples of size n.\n        uniform_samples = rng.uniform(size=(R, n))\n        pareto_samples = uniform_samples**(-1.0 / alpha_part2)\n\n        # Compute sample means for each replication.\n        sample_means = np.mean(pareto_samples, axis=1)\n\n        # Compute the absolute scaled sample mean for each replication.\n        abs_scaled_means = np.abs(np.sqrt(n) * sample_means)\n\n        # Compute the median of these values across replications.\n        M_n = np.median(abs_scaled_means)\n        medians.append(M_n)\n\n    # Compute the boolean indicator for strict growth of the median.\n    INC = (medians[0] < medians[1]) and (medians[1] < medians[2])\n\n    all_results.append(INC)\n    all_results.extend(medians)\n\n    # Final print statement in the exact required format.\n    # str() of a boolean is 'True' or 'False', which is a valid representation.\n    output_str_list = [str(r) for r in all_results]\n    print(f\"[{','.join(output_str_list)}]\")\n\nsolve()\n```"}]}