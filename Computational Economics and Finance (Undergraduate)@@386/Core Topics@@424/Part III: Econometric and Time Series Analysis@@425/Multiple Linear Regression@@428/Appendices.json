{"hands_on_practices": [{"introduction": "This first exercise puts you in the role of a data analyst tasked with building a predictive model from the ground up. By implementing an Ordinary Least Squares (OLS) regression to forecast mobile app downloads, you will put the core theory into practice, from solving the normal equations to evaluating model performance and diagnosing potential issues. This comprehensive task provides a practical, end-to-end application of the multiple linear regression workflow. [@problem_id:2413185]", "id": "2413185", "problem": "You are asked to implement and apply a multiple linear regression in a computational finance setting. The task is to build an Ordinary Least Squares (OLS) model of mobile application demand, where the target variable is the number of downloads. The predictors are price in United States Dollars (USD), user rating on a $1$ to $5$ scale, and advertising spend on social media in USD. The model must include an intercept. All predicted downloads must be expressed as counts (unitless), and all currency inputs are in USD. No angles are involved. All outputs must be numeric in the types boolean, integer, or float as specified.\n\nStarting from fundamental definitions, you must treat OLS as the solution to minimizing the sum of squared residuals across the training data. You must not rely on pre-canned black-box models beyond numeric linear algebra primitives. You must use the provided training data to estimate the model, then answer the test suite that follows.\n\nTraining data: Each row gives $(\\text{price}, \\text{user\\_rating}, \\text{advertising\\_spend\\_on\\_social\\_media}) \\to \\text{downloads}$, where downloads are counts.\n\n- Row $1$: $(0.99, 4.5, 12000) \\to 31320$\n- Row $2$: $(1.99, 4.2, 8000) \\to 24620$\n- Row $3$: $(0.00, 3.8, 5000) \\to 25700$\n- Row $4$: $(2.99, 4.7, 15000) \\to 29620$\n- Row $5$: $(4.99, 4.9, 20000) \\to 30970$\n- Row $6$: $(1.49, 3.5, 0) \\to 17370$\n- Row $7$: $(0.00, 4.0, 7000) \\to 27950$\n- Row $8$: $(3.99, 4.4, 11000) \\to 23820$\n- Row $9$: $(2.49, 3.9, 6000) \\to 21570$\n- Row $10$: $(0.99, 4.8, 18000) \\to 36720$\n- Row $11$: $(1.99, 4.1, 4000) \\to 21440$\n- Row $12$: $(0.49, 3.6, 2000) \\to 21540$\n\nValidation data for out-of-sample evaluation: Each row gives $(\\text{price}, \\text{user\\_rating}, \\text{advertising\\_spend\\_on\\_social\\_media}) \\to \\text{downloads}$.\n\n- V$1$: $(2.99, 4.0, 9000) \\to 23320$\n- V$2$: $(0.00, 4.9, 16000) \\to 37300$\n- V$3$: $(1.49, 3.7, 4000) \\to 21370$\n\nYou must implement the following, grounded in first principles of multiple linear regression:\n\n- Fit a linear model for downloads as a function of an intercept, price, user rating, and advertising spend by minimizing the sum of squared residuals over the training set.\n- Compute the estimated coefficient vector and residual variance using the training set only.\n- Use the fitted model to compute required quantities for the test suite below.\n\nTest suite and required outputs:\n\nLet $n$ denote the number of training observations and $k$ the number of model parameters, including the intercept. Here $n = 12$ and $k = 4$.\n\nYour program must compute the following five results, in order:\n\n$1.$ Prediction task (happy path): Using the fitted model, predict downloads for a new app with $(\\text{price} = 1.99,\\ \\text{user\\_rating} = 4.5,\\ \\text{advertising\\_spend\\_on\\_social\\_media} = 10000)$. Output the prediction rounded to the nearest integer (count).\n\n$2.$ Coefficient extraction: Output the estimated coefficient for price as a float rounded to $2$ decimal places.\n\n$3.$ Out-of-sample goodness-of-fit: Compute the coefficient of determination on the validation set, defined as $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$, where $\\bar{y}_{\\text{test}}$ is the mean of the validation targets. Output this as a float rounded to $4$ decimal places.\n\n$4.$ Statistical significance of price (edge case: small residual variance): Using a two-sided Studentâ€™s $t$-test at significance level $\\alpha = 0.05$ with degrees of freedom $n - k$, test the null hypothesis that the price coefficient equals $0$ against the alternative that it differs from $0$. Output a boolean: $True$ if significant, $False$ otherwise.\n\n$5.$ Numerical stability diagnostic (multicollinearity check): Compute the spectral condition number of the training design matrix with intercept using the $2$-norm, i.e., the ratio of its largest to smallest singular values. Output this as a float rounded to $2$ decimal places.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). The results must appear in the exact order specified: prediction (integer), price coefficient (float rounded to $2$ decimals), test $R^{2}$ (float rounded to $4$ decimals), price-significance boolean, condition number (float rounded to $2$ decimals).", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Task**: Implement and apply multiple linear regression using Ordinary Least Squares (OLS) to model mobile application demand.\n- **Target Variable ($y$)**: Number of downloads (counts).\n- **Predictors ($x_j$)**: Price (USD), user rating ($1-5$ scale), advertising spend (USD).\n- **Model Specification**: A linear model including an intercept term.\n- **Methodology**: Minimize the sum of squared residuals. No \"black-box\" models are to be used, only linear algebra primitives.\n- **Training Data** ($n=12$ observations):\n  - $(\\text{price}, \\text{user\\_rating}, \\text{ad\\_spend}) \\to \\text{downloads}$\n  - $(0.99, 4.5, 12000) \\to 31320$\n  - $(1.99, 4.2, 8000) \\to 24620$\n  - $(0.00, 3.8, 5000) \\to 25700$\n  - $(2.99, 4.7, 15000) \\to 29620$\n  - $(4.99, 4.9, 20000) \\to 30970$\n  - $(1.49, 3.5, 0) \\to 17370$\n  - $(0.00, 4.0, 7000) \\to 27950$\n  - $(3.99, 4.4, 11000) \\to 23820$\n  - $(2.49, 3.9, 6000) \\to 21570$\n  - $(0.99, 4.8, 18000) \\to 36720$\n  - $(1.99, 4.1, 4000) \\to 21440$\n  - $(0.49, 3.6, 2000) \\to 21540$\n- **Validation Data** ($3$ observations):\n  - $(2.99, 4.0, 9000) \\to 23320$\n  - $(0.00, 4.9, 16000) \\to 37300$\n  - $(1.49, 3.7, 4000) \\to 21370$\n- **Constants**: Number of training observations $n=12$, number of model parameters $k=4$.\n- **Test Suite**:\n  1.  Predict downloads for $(\\text{price}=1.99, \\text{user\\_rating}=4.5, \\text{ad\\_spend}=10000)$. Output: integer.\n  2.  Extract the estimated coefficient for price. Output: float, $2$ decimal places.\n  3.  Compute $R^{2}_{\\text{test}} = 1 - \\dfrac{\\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2}{\\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2}$ on the validation set. Output: float, $4$ decimal places.\n  4.  Test significance of the price coefficient using a two-sided t-test at $\\alpha = 0.05$ with $n-k$ degrees of freedom. Output: boolean.\n  5.  Compute the spectral condition number of the training design matrix. Output: float, $2$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard, well-defined application of multiple linear regression, a fundamental method in statistics and econometrics.\n- **Scientifically Grounded**: The problem is based on Ordinary Least Squares (OLS), a core and well-established statistical theory. The context is realistic for computational economics and finance.\n- **Well-Posed**: The problem provides sufficient data (12 observations for 4 parameters) to estimate the model coefficients. The tasks in the test suite are specific, unambiguous, and have unique solutions based on standard statistical formulas.\n- **Objective**: The problem statement is written in precise, objective language. The data are numerical, and the required calculations are based on established mathematical formulas, not subjective interpretation.\n- The problem does not violate any of the invalidity criteria. It is not based on false premises, it is formalizable, the data is complete, and the\ntasks are verifiable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\n### Solution\nThe problem requires fitting a multiple linear regression model using the Ordinary Least Squares (OLS) method. The model is of the form:\n$$ \\text{downloads} = \\beta_0 + \\beta_1 \\cdot \\text{price} + \\beta_2 \\cdot \\text{user\\_rating} + \\beta_3 \\cdot \\text{ad\\_spend} + \\epsilon $$\nIn matrix notation, this is expressed as $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, where $\\mathbf{y}$ is the vector of observed downloads, $\\mathbf{X}$ is the design matrix, $\\boldsymbol{\\beta}$ is the vector of coefficients, and $\\boldsymbol{\\epsilon}$ is the vector of errors.\n\nThe OLS method finds the coefficient vector $\\hat{\\boldsymbol{\\beta}}$ that minimizes the sum of squared residuals (SSR), $S = \\sum_{i=1}^{n} \\epsilon_i^2$.\n$$ S(\\boldsymbol{\\beta}) = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta})^T (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\beta}) $$\nTo minimize this quantity, we take the gradient with respect to $\\boldsymbol{\\beta}$ and set it to zero:\n$$ \\frac{\\partial S}{\\partial \\boldsymbol{\\beta}} = -2\\mathbf{X}^T\\mathbf{y} + 2\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\beta} = \\mathbf{0} $$\nThis yields the normal equations:\n$$ (\\mathbf{X}^T\\mathbf{X})\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^T\\mathbf{y} $$\nThe solution for the OLS estimator $\\hat{\\boldsymbol{\\beta}}$ is given by:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} $$\nThis equation will be solved using numerically stable linear algebra routines.\n\nFirst, we construct the design matrix $\\mathbf{X}_{\\text{train}}$ and the target vector $\\mathbf{y}_{\\text{train}}$ from the $n=12$ training observations. The matrix $\\mathbf{X}_{\\text{train}}$ has $k=4$ columns: one for the intercept ($\\beta_0$) and one for each of the three predictors.\n$$\n\\mathbf{y}_{\\text{train}} = \\begin{pmatrix} 31320 \\\\ 24620 \\\\ 25700 \\\\ 29620 \\\\ 30970 \\\\ 17370 \\\\ 27950 \\\\ 23820 \\\\ 21570 \\\\ 36720 \\\\ 21440 \\\\ 21540 \\end{pmatrix},\n\\quad\n\\mathbf{X}_{\\text{train}} = \\begin{pmatrix}\n1 & 0.99 & 4.5 & 12000 \\\\\n1 & 1.99 & 4.2 & 8000 \\\\\n1 & 0.00 & 3.8 & 5000 \\\\\n1 & 2.99 & 4.7 & 15000 \\\\\n1 & 4.99 & 4.9 & 20000 \\\\\n1 & 1.49 & 3.5 & 0 \\\\\n1 & 0.00 & 4.0 & 7000 \\\\\n1 & 3.99 & 4.4 & 11000 \\\\\n1 & 2.49 & 3.9 & 6000 \\\\\n1 & 0.99 & 4.8 & 18000 \\\\\n1 & 1.99 & 4.1 & 4000 \\\\\n1 & 0.49 & 3.6 & 2000 \\\\\n\\end{pmatrix}\n$$\nSolving the normal equations for $\\hat{\\boldsymbol{\\beta}} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3]^T$ gives the estimated coefficient vector. Let's proceed to solve the test suite questions.\n\n**1. Prediction Task**\nA new observation is given by the predictor vector $(\\text{price}=1.99, \\text{user\\_rating}=4.5, \\text{ad\\_spend}=10000)$. We form a design vector $\\mathbf{x}_{\\text{new}}$ with an intercept:\n$$ \\mathbf{x}_{\\text{new}} = \\begin{pmatrix} 1 & 1.99 & 4.5 & 10000 \\end{pmatrix}^T $$\nThe predicted number of downloads $\\hat{y}_{\\text{new}}$ is calculated as:\n$$ \\hat{y}_{\\text{new}} = \\mathbf{x}_{\\text{new}}^T \\hat{\\boldsymbol{\\beta}} $$\nThe result is rounded to the nearest integer.\n\n**2. Coefficient Extraction**\nThe estimated coefficient for price, $\\hat{\\beta}_1$, is the second element of the vector $\\hat{\\boldsymbol{\\beta}}$. This value is extracted and rounded to $2$ decimal places.\n\n**3. Out-of-sample Goodness-of-Fit**\nThe coefficient of determination for the validation set, $R^{2}_{\\text{test}}$, is computed as:\n$$ R^{2}_{\\text{test}} = 1 - \\frac{SSR_{\\text{test}}}{SST_{\\text{test}}} $$\nwhere $SSR_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\hat{y}_i)^2$ is the sum of squared residuals on the validation set, and $SST_{\\text{test}} = \\sum_{i \\in \\text{test}} (y_i - \\bar{y}_{\\text{test}})^2$ is the total sum of squares for the validation set. $\\bar{y}_{\\text{test}}$ is the mean of the observed downloads in the validation data. Predictions $\\hat{y}_i$ are generated using the fitted model: $\\hat{\\mathbf{y}}_{\\text{test}} = \\mathbf{X}_{\\text{test}}\\hat{\\boldsymbol{\\beta}}$. The result is rounded to $4$ decimal places.\n\n**4. Statistical Significance of Price**\nWe test the null hypothesis $H_0: \\beta_1 = 0$ against the alternative $H_1: \\beta_1 \\neq 0$ using a two-sided Student's t-test. The t-statistic is:\n$$ t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)} $$\nThe standard error of the coefficient, $SE(\\hat{\\beta}_1)$, is the square root of the corresponding diagonal element of the coefficient covariance matrix, $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\hat{\\sigma}^2 (\\mathbf{X}_{\\text{train}}^T \\mathbf{X}_{\\text{train}})^{-1}$. The residual variance, $\\hat{\\sigma}^2$, is an unbiased estimator of the error variance $\\sigma^2$ and is calculated from the training data:\n$$ \\hat{\\sigma}^2 = \\frac{SSR_{\\text{train}}}{n-k} = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-k} $$\nThe degrees of freedom for the t-distribution are $df = n-k = 12-4=8$. We compare the absolute value of the calculated t-statistic, $|t|$, to the critical value $t_{\\text{crit}}$ from the t-distribution for a significance level $\\alpha = 0.05$ and $df=8$. If $|t| > t_{\\text{crit}}$, we reject $H_0$, and the coefficient is statistically significant.\n\n**5. Numerical Stability Diagnostic**\nThe spectral condition number of the training design matrix $\\mathbf{X}_{\\text{train}}$ is the ratio of its largest singular value ($\\sigma_{\\max}$) to its smallest singular value ($\\sigma_{\\min}$):\n$$ \\kappa_2(\\mathbf{X}_{\\text{train}}) = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} $$\nThis value is a measure of multicollinearity in the predictors. A high value indicates potential numerical instability in estimating the coefficients. The result is rounded to $2$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Implements and applies a multiple linear regression model from first principles.\n    \"\"\"\n    # Define the problem data as specified.\n    # Training Data (n=12)\n    y_train_list = [31320, 24620, 25700, 29620, 30970, 17370, 27950, 23820, 21570, 36720, 21440, 21540]\n    X_train_predictors_list = [\n        [0.99, 4.5, 12000], [1.99, 4.2, 8000], [0.00, 3.8, 5000], [2.99, 4.7, 15000],\n        [4.99, 4.9, 20000], [1.49, 3.5, 0], [0.00, 4.0, 7000], [3.99, 4.4, 11000],\n        [2.49, 3.9, 6000], [0.99, 4.8, 18000], [1.99, 4.1, 4000], [0.49, 3.6, 2000]\n    ]\n\n    # Validation Data\n    y_val_list = [23320, 37300, 21370]\n    X_val_predictors_list = [\n        [2.99, 4.0, 9000], [0.00, 4.9, 16000], [1.49, 3.7, 4000]\n    ]\n\n    # New app data for prediction\n    x_new_predictors_list = [1.99, 4.5, 10000]\n    \n    # Convert lists to numpy arrays\n    y_train = np.array(y_train_list, dtype=float)\n    X_train_predictors = np.array(X_train_predictors_list, dtype=float)\n    y_val = np.array(y_val_list, dtype=float)\n    X_val_predictors = np.array(X_val_predictors_list, dtype=float)\n    x_new_predictors = np.array(x_new_predictors_list, dtype=float)\n\n    # Add intercept column to design matrices\n    n_train = X_train_predictors.shape[0]\n    n_val = X_val_predictors.shape[0]\n    X_train = np.hstack([np.ones((n_train, 1)), X_train_predictors])\n    X_val = np.hstack([np.ones((n_val, 1)), X_val_predictors])\n    x_new = np.hstack([1, x_new_predictors])\n    \n    # --- Model Fitting: Solve Normal Equations ---\n    # (X.T @ X) @ beta = X.T @ y\n    XTX = X_train.T @ X_train\n    XTy = X_train.T @ y_train\n    beta_hat = np.linalg.solve(XTX, XTy)\n\n    # --- Test Suite Computations ---\n    results = []\n\n    # 1. Prediction task\n    y_pred_new = x_new @ beta_hat\n    result1 = int(round(y_pred_new))\n    results.append(result1)\n\n    # 2. Coefficient extraction (price)\n    price_coeff = beta_hat[1]\n    result2 = round(price_coeff, 2)\n    results.append(result2)\n\n    # 3. Out-of-sample R-squared\n    y_pred_val = X_val @ beta_hat\n    residuals_val = y_val - y_pred_val\n    ssr_val = np.sum(residuals_val**2)\n    y_mean_val = np.mean(y_val)\n    sst_val = np.sum((y_val - y_mean_val)**2)\n    r2_val = 1 - (ssr_val / sst_val)\n    result3 = round(r2_val, 4)\n    results.append(result3)\n    \n    # 4. Statistical significance of price\n    n = n_train\n    k = X_train.shape[1] # number of parameters\n    df = n - k\n    y_pred_train = X_train @ beta_hat\n    residuals_train = y_train - y_pred_train\n    ssr_train = np.sum(residuals_train**2)\n    sigma_sq_hat = ssr_train / df\n    XTX_inv = np.linalg.inv(XTX)\n    var_beta_hat = sigma_sq_hat * XTX_inv\n    se_price_coeff = np.sqrt(var_beta_hat[1, 1])\n    t_statistic = price_coeff / se_price_coeff\n    alpha = 0.05\n    t_critical = t.ppf(1 - alpha/2, df)\n    result4 = bool(abs(t_statistic) > t_critical)\n    results.append(result4)\n    \n    # 5. Numerical stability diagnostic (condition number)\n    cond_num = np.linalg.cond(X_train, 2)\n    result5 = round(cond_num, 2)\n    results.append(result5)\n\n    # Final print statement in the exact required format.\n    # Example format: [28132,-1803.97,0.9744,True,1367.65]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "A robust regression model begins with a correctly specified design matrix. This exercise explores the critical concept of multicollinearity through the classic \"dummy variable trap,\" a common pitfall when working with categorical data. By investigating scenarios where the regressors are perfectly linearly dependent, you will gain a concrete understanding of why the matrix $X^{\\top} X$ becomes singular and how to design your model to ensure a unique and interpretable solution. [@problem_id:2413177]", "id": "2413177", "problem": "You are studying a linear model for hourly wages in a cross-sectional dataset with a binary gender indicator. Let $y \\in \\mathbb{R}^n$ be the vector of wages and let $X \\in \\mathbb{R}^{n \\times p}$ be the design matrix whose columns are an intercept (a column of ones if present), the dummy variable for male, and the dummy variable for female, depending on the case. Consider the ordinary least squares (OLS) problem of minimizing the sum of squared residuals $S(\\beta) = \\lVert y - X \\beta \\rVert_2^2$ over $\\beta \\in \\mathbb{R}^p$. If there is a unique minimizer, denote it by $\\hat{\\beta}$. If the set of minimizers is not a singleton, define the reported coefficient vector for that case to be the minimizer with the smallest Euclidean norm $\\lVert \\beta \\rVert_2$ among all minimizers of $S(\\beta)$. For each test case below, you must compute the column rank $r$ of $X$, determine whether the Gram matrix $X^\\top X$ is invertible (equivalently whether $r=p$), and compute the coefficient vector as specified. Report the invertibility indicator as the integer $1$ if $X^\\top X$ is invertible and $0$ otherwise. All coefficient entries must be rounded to $4$ decimal places.\n\nTest Suite. For each case, the design matrix $X$ and response vector $y$ are given explicitly. The ordering of the coefficients must match the column ordering of $X$ in that case.\n\n- Case A (attempt to regress on intercept, male dummy, and female dummy with both genders present). Here $n=6$ and $p=3$, with\n$$\nX_A=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\\\\\n1 & 1 & 0\\\\\n1 & 0 & 1\n\\end{bmatrix},\\quad\ny_A=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\n\n- Case B (proper encoding with intercept and a single dummy). Here $n=6$ and $p=2$, with\n$$\nX_B=\\begin{bmatrix}\n1 & 1\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 0\\\\\n1 & 1\\\\\n1 & 0\n\\end{bmatrix},\\quad\ny_B=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\nThe coefficient ordering is $[\\text{intercept},\\text{male}]$.\n\n- Case C (attempt to regress on intercept, male dummy, and female dummy when all observations are male). Here $n=4$ and $p=3$, with\n$$\nX_C=\\begin{bmatrix}\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\\\\\n1 & 1 & 0\n\\end{bmatrix},\\quad\ny_C=\\begin{bmatrix}\n21\\\\\n22\\\\\n20\\\\\n23\n\\end{bmatrix}.\n$$\n\n- Case D (no intercept, both dummies present). Here $n=6$ and $p=2$, with\n$$\nX_D=\\begin{bmatrix}\n1 & 0\\\\\n0 & 1\\\\\n1 & 0\\\\\n0 & 1\\\\\n1 & 0\\\\\n0 & 1\n\\end{bmatrix},\\quad\ny_D=\\begin{bmatrix}\n21\\\\\n19\\\\\n23\\\\\n18\\\\\n22\\\\\n20\n\\end{bmatrix}.\n$$\nThe coefficient ordering is $[\\text{male},\\text{female}]$.\n\nRequired final output format. Your program should produce a single line of output containing the results for the $4$ test cases as a comma-separated list enclosed in square brackets. Each result must itself be a list of the form $[r, \\text{invertibility\\_indicator}, [\\beta_1,\\dots,\\beta_p]]$, where $r$ is an integer, the invertibility indicator is the integer $1$ or $0$, and the coefficient sublist entries are floats rounded to $4$ decimal places. For example, the overall output must look like\n$$\n[[r_A, \\text{inv}_A, [\\dots]], [r_B, \\text{inv}_B, [\\dots]], [r_C, \\text{inv}_C, [\\dots]], [r_D, \\text{inv}_D, [\\dots]]].\n$$", "solution": "The problem requires the analysis of four different specifications of a linear regression model. The objective is to determine for each case the rank of the design matrix $X$, the invertibility of the Gram matrix $X^\\top X$, and the vector of estimated coefficients $\\beta$. The model is the standard ordinary least squares (OLS) setup, where we seek to find a vector $\\beta \\in \\mathbb{R}^p$ that minimizes the sum of squared residuals, $S(\\beta)$:\n$$ S(\\beta) = \\lVert y - X\\beta \\rVert_2^2 = (y - X\\beta)^\\top (y - X\\beta) $$\nwhere $y \\in \\mathbb{R}^n$ is the vector of observations and $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix.\n\nA minimizer $\\hat{\\beta}$ of $S(\\beta)$ must satisfy the normal equations:\n$$ (X^\\top X)\\hat{\\beta} = X^\\top y $$\nThe properties of the solution depend on the matrix $X^\\top X$.\n\nIf the columns of $X$ are linearly independent, the matrix $X$ is of full column rank, i.e., its rank $r$ is equal to the number of columns $p$. In this case, the Gram matrix $X^\\top X$ is a $p \\times p$ positive definite matrix and is therefore invertible. The normal equations have a unique solution given by:\n$$ \\hat{\\beta} = (X^\\top X)^{-1} X^\\top y $$\n\nIf the columns of $X$ are linearly dependent, the matrix $X$ is rank-deficient, i.e., $r < p$. This situation is known as perfect multicollinearity. The Gram matrix $X^\\top X$ is singular and not invertible. The normal equations are consistent but admit an infinite number of solutions. The problem specifies a unique criterion for selecting a solution in this case: we must choose the minimizer of $S(\\beta)$ that has the smallest Euclidean norm $\\lVert \\beta \\rVert_2$. This specific solution is given by:\n$$ \\hat{\\beta} = X^+ y $$\nwhere $X^+$ is the Moore-Penrose pseudoinverse of $X$. This is the standard solution provided by robust numerical linear algebra routines for least-squares problems.\n\nWe now proceed to analyze each case.\n\n**Case A**\nThe design matrix $X_A$ and response vector $y_A$ are given by:\n$$ X_A=\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix},\\quad y_A=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\nThe columns of $X_A$ correspond to an intercept, a dummy variable for male, and a dummy variable for female. The number of parameters is $p=3$. A linear dependency is immediately apparent: the first column (intercept) is the sum of the second and third columns (male and female dummies). This is the classic \"dummy variable trap\". Due to this multicollinearity, the columns are linearly dependent. The rank of $X_A$, denoted $r_A$, must be less than $p$. Since the second and third columns are linearly independent, the rank is $r_A = 2$.\nAs $r_A < p$, the Gram matrix $X_A^\\top X_A$ is singular, so its invertibility indicator is $0$. The coefficient vector $\\hat{\\beta}_A$ is the minimum-norm solution. The set of all least-squares solutions is defined by the equations $\\hat{\\beta}_0 + \\hat{\\beta}_1 = \\bar{y}_{\\text{male}} = 22$ and $\\hat{\\beta}_0 + \\hat{\\beta}_2 = \\bar{y}_{\\text{female}} = 19$. The solution set can be parameterized by $\\beta_0$: $[\\beta_0, 22-\\beta_0, 19-\\beta_0]^\\top$. Minimizing the norm of this vector, $\\beta_0^2 + (22-\\beta_0)^2 + (19-\\beta_0)^2$, yields $\\beta_0 = 41/3$. The corresponding minimum-norm coefficient vector is $\\hat{\\beta}_A = [41/3, 25/3, 16/3]^\\top$.\nNumerically, we find:\n- $r_A = 2$\n- Invertibility indicator: $0$\n- $\\hat{\\beta}_A \\approx [13.6667, 8.3333, 5.3333]$\n\n**Case B**\nThe design matrix $X_B$ and response vector $y_B$ are:\n$$ X_B=\\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\\\ 1 & 1 \\\\ 1 & 0 \\end{bmatrix},\\quad y_B=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\nHere, $p=2$. The columns correspond to an intercept and a male dummy. The female category is the reference group. The two columns are linearly independent. Thus, the matrix has full column rank, $r_B = 2 = p$.\nTherefore, $X_B^\\top X_B$ is invertible, and the invertibility indicator is $1$. The unique OLS solution is computed. $\\hat{\\beta}_0$ estimates the mean wage for the reference group (females), and $\\hat{\\beta}_1$ estimates the difference between the mean wage of males and females.\nMean female wage: $(\\frac{19+18+20}{3}) = 19$. Thus, $\\hat{\\beta}_0 = 19$.\nMean male wage: $(\\frac{21+23+22}{3}) = 22$. Thus, $\\hat{\\beta}_0 + \\hat{\\beta}_1 = 22$, which implies $\\hat{\\beta}_1 = 22 - 19 = 3$.\n- $r_B = 2$\n- Invertibility indicator: $1$\n- $\\hat{\\beta}_B = [19.0000, 3.0000]$\n\n**Case C**\nThe design matrix $X_C$ and response vector $y_C$ are:\n$$ X_C=\\begin{bmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 0 \\end{bmatrix},\\quad y_C=\\begin{bmatrix} 21 \\\\ 22 \\\\ 20 \\\\ 23 \\end{bmatrix} $$\nThe number of parameters is $p=3$. All observations are for males, leading to two forms of perfect multicollinearity. First, the intercept column is identical to the male dummy column. Second, the female dummy column is a zero vector. The column space is spanned by a single vector of ones. The rank is therefore $r_C = 1$.\nSince $r_C < p$, the matrix $X_C^\\top X_C$ is singular, and the invertibility indicator is $0$. We must find the minimum-norm solution. The model effectively tries to estimate the mean wage, $\\bar{y}_C = \\frac{21+22+20+23}{4} = 21.5$, using a linear combination of the predictors. The prediction for any observation is $\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1$. Thus, any solution must satisfy $\\hat{\\beta}_0 + \\hat{\\beta}_1 = 21.5$. The coefficient $\\hat{\\beta}_2$ does not affect the residuals, so for minimum norm it must be $0$. We then minimize $\\beta_0^2 + \\beta_1^2$ subject to $\\beta_0 + \\beta_1 = 21.5$, which yields $\\hat{\\beta}_0 = \\hat{\\beta}_1 = 21.5 / 2 = 10.75$.\n- $r_C = 1$\n- Invertibility indicator: $0$\n- $\\hat{\\beta}_C = [10.7500, 10.7500, 0.0000]$\n\n**Case D**\nThe design matrix $X_D$ and response vector $y_D$ are:\n$$ X_D=\\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix},\\quad y_D=\\begin{bmatrix} 21 \\\\ 19 \\\\ 23 \\\\ 18 \\\\ 22 \\\\ 20 \\end{bmatrix} $$\nThis model omits the intercept and includes a dummy for each category, male and female. The number of parameters is $p=2$. The columns are orthogonal, and thus linearly independent. The matrix has full column rank, $r_D = 2 = p$.\nConsequently, $X_D^\\top X_D$ is invertible (in fact, it is diagonal), and the invertibility indicator is $1$. The unique OLS solution exists. In this parameterization, the coefficients directly estimate the conditional mean for each group.\nThe coefficient for males, $\\hat{\\beta}_1$, is the average wage for males: $\\bar{y}_{\\text{male}} = 22$.\nThe coefficient for females, $\\hat{\\beta}_2$, is the average wage for females: $\\bar{y}_{\\text{female}} = 19$.\n- $r_D = 2$\n- Invertibility indicator: $1$\n- $\\hat{\\beta}_D = [22.0000, 19.0000]$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the OLS problem for four different regression model specifications.\n\n    For each case, it computes:\n    1. The column rank 'r' of the design matrix X.\n    2. An invertibility indicator for the Gram matrix X.T @ X (1 if invertible, 0 otherwise).\n    3. The OLS coefficient vector beta. If the solution is not unique, the minimum\n       Euclidean norm solution is computed.\n\n    The results are formatted into a list of lists as required.\n    \"\"\"\n\n    def analyze_case(X, y):\n        \"\"\"\n        Analyzes a single OLS regression case.\n        \n        Args:\n            X (np.ndarray): The design matrix of shape (n, p).\n            y (np.ndarray): The response vector of shape (n,).\n            \n        Returns:\n            list: A list containing [rank, invertibility_indicator, coefficients].\n        \"\"\"\n        # Get the number of parameters p (number of columns)\n        p = X.shape[1]\n\n        # 1. Compute the column rank of X\n        r = np.linalg.matrix_rank(X)\n\n        # 2. Determine if X.T @ X is invertible\n        # This is true if and only if X has full column rank (r == p).\n        invertibility_indicator = 1 if r == p else 0\n\n        # 3. Compute the coefficient vector beta\n        # np.linalg.lstsq correctly handles both full rank and rank-deficient cases.\n        # For rank-deficient cases, it returns the minimum L2-norm solution, which is\n        # the solution obtained via the Moore-Penrose pseudoinverse.\n        # rcond=None is specified to use the default machine precision-based cutoff.\n        beta, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n\n        # Round coefficients to 4 decimal places\n        beta_rounded = [round(b, 4) for b in beta]\n\n        return [r, invertibility_indicator, beta_rounded]\n\n    # Test Suite Definition\n    test_cases = [\n        # Case A: Intercept, male dummy, female dummy (dummy variable trap)\n        (\n            np.array([[1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1], [1, 1, 0], [1, 0, 1]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        ),\n        # Case B: Intercept and one dummy (correct specification)\n        (\n            np.array([[1, 1], [1, 0], [1, 1], [1, 0], [1, 1], [1, 0]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        ),\n        # Case C: Intercept, male dummy, female dummy (all obs are male)\n        (\n            np.array([[1, 1, 0], [1, 1, 0], [1, 1, 0], [1, 1, 0]]),\n            np.array([21, 22, 20, 23])\n        ),\n        # Case D: No intercept, two dummies (alternative correct specification)\n        (\n            np.array([[1, 0], [0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]),\n            np.array([21, 19, 23, 18, 22, 20])\n        )\n    ]\n\n    results = []\n    for X, y in test_cases:\n        result = analyze_case(X, y)\n        results.append(result)\n        \n    # The str() representation of a list in Python matches the required format.\n    # Ex: str([2, 0, [13.6667, 8.3333, 5.3333]]) gives '[2, 0, [13.6667, 8.3333, 5.3333]]'\n    # We join these string representations with commas.\n    results_str = ','.join(map(str, results))\n\n    # Print the final output in the exact required format.\n    print(f\"[{results_str}]\")\n\nsolve()\n```"}, {"introduction": "While OLS provides the best linear unbiased estimate regardless of error variance, a crucial assumption for valid inference is homoskedasticity. This Monte Carlo simulation allows you to directly observe the consequences when this assumption is violated, a common scenario in economic data. By generating data with heteroskedastic errors, you will demonstrate the failure of classical standard errors and confirm the reliability of heteroskedasticity-consistent (\"robust\") standard errors for constructing accurate confidence intervals. [@problem_id:2413193]", "id": "2413193", "problem": "Consider the linear regression model with heteroskedastic disturbances. For each observation $i \\in \\{1,\\dots,n\\}$, let the data-generating process be\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i,\n$$\nwith the exogeneity condition $E[\\varepsilon_i \\mid X] = 0$ and a heteroskedastic variance structure\n$$\n\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma^2 \\cdot \\text{income}_i,\n$$\nwhere $X$ denotes the full regressor matrix including the intercept. The goal is to use Monte Carlo simulation to demonstrate that the classical Ordinary Least Squares (OLS) standard errors fail under heteroskedasticity, while Heteroskedasticity-Consistent (HC) standard errors maintain correct coverage asymptotically.\n\nYou must write a complete, runnable program that:\n- Generates synthetic data according to the description above.\n- Estimates the regression by OLS, computes both classical homoskedasticity-only standard errors and Heteroskedasticity-Consistent (HC) robust standard errors.\n- Constructs two-sided confidence intervals for $\\beta_1$ with nominal coverage $0.95$ using the standard normal critical value.\n- Repeats the experiment over $R$ independent replications and reports the empirical coverage for each standard error method (the fraction of confidence intervals that contain the true $\\beta_1$).\n\nUse the following fundamental base for design and justification:\n- The OLS estimator is defined as the minimizer of the sum of squared residuals. The residuals are the deviations between observed $y_i$ and the fitted values implied by the linear combination of regressors.\n- The classical homoskedasticity-only standard errors require the assumption $\\operatorname{Var}(\\varepsilon \\mid X) = \\sigma^2 I$, and are inconsistent when the variance depends on regressors.\n- Heteroskedasticity-Consistent (HC) standard errors replace the incorrect homoskedastic residual covariance with an estimator that remains consistent when $\\operatorname{Var}(\\varepsilon \\mid X)$ is diagonal but not proportional to the identity matrix.\n\nData generation details for each replication:\n- Draw $\\text{income}_i$ independently from a Gamma distribution with shape parameter $k = 4$ and scale parameter $\\theta = 12$ (so $E[\\text{income}_i] = k \\theta = 48$). This choice ensures strictly positive $\\text{income}_i$ and realistic right-skewness.\n- Draw $\\text{educ}_i$ independently from a Normal distribution with mean $14$ and standard deviation $2$, then clip to the interval $[8, 22]$ to avoid implausible values.\n- Set the true parameter vector to $(\\beta_0,\\beta_1,\\beta_2) = (1, 0.8, 0.5)$.\n- For each $i$, draw $\\varepsilon_i$ from a Normal distribution with mean $0$ and variance $\\sigma^2 \\cdot \\text{income}_i$.\n- Construct $y_i$ accordingly.\n\nConfidence intervals:\n- For each replication, after estimating the model by OLS, compute two confidence intervals for $\\beta_1$: one using the classical homoskedasticity-only standard error and one using the Heteroskedasticity-Consistent (HC) robust standard error. In both cases, use the standard normal critical value corresponding to a nominal coverage of $0.95$ (i.e., the two-sided critical value $z_{0.975}$).\n- Record whether each interval contains the true $\\beta_1$.\n\nTest suite:\nRun the Monte Carlo simulation for the following parameter sets $(n, \\sigma^2, R)$:\n- Case A (happy path): $(n, \\sigma^2, R) = (500, 1.0, 1000)$.\n- Case B (small sample): $(n, \\sigma^2, R) = (60, 1.0, 1000)$.\n- Case C (more severe heteroskedasticity): $(n, \\sigma^2, R) = (500, 3.0, 1000)$.\n- Case D (milder heteroskedasticity): $(n, \\sigma^2, R) = (500, 0.2, 1000)$.\n\nRandomness and reproducibility:\n- Use a fixed seed for the pseudo-random number generator equal to $20240519$ so that results are exactly reproducible.\n\nProgram output requirements:\n- For each test case, report three items: the empirical coverage using homoskedasticity-only standard errors (a float), the empirical coverage using HC robust standard errors (a float), and a boolean indicating whether the robust coverage is closer to the nominal $0.95$ than the homoskedastic coverage (i.e., whether $|\\text{robust} - 0.95| < |\\text{classical} - 0.95|$).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one inner list per test case, for example:\n\"[[cA_classical,cA_robust,bA],[cB_classical,cB_robust,bB],[cC_classical,cC_robust,bC],[cD_classical,cD_robust,bD]]\"\n- All coverages must be reported as decimals (not percentages).\n\nAngle units are not applicable. No physical units are needed.\n\nYour program must be complete and runnable as is, without any user input or external files, and must strictly adhere to the specified output format.", "solution": "The problem as stated is valid. It presents a well-posed and standard econometric exercise for demonstrating the consequences of heteroskedasticity on the inference produced by the Ordinary Least Squares (OLS) estimator. The data-generating process is fully specified, the theoretical concepts are sound, and the objectives are clear and quantifiable. The task is to perform a Monte Carlo simulation, which is a fundamental technique in computational statistics and econometrics for evaluating the finite-sample properties of estimators and statistical tests.\n\nWe begin by formalizing the specified linear regression model. For each observation $i=1, \\dots, n$, the model is:\n$$\ny_i = \\beta_0 + \\beta_1 \\cdot \\text{income}_i + \\beta_2 \\cdot \\text{educ}_i + \\varepsilon_i\n$$\nIn matrix notation, this can be written as $y = X\\beta + \\varepsilon$, where $y$ is the $n \\times 1$ vector of observations on the dependent variable, $X$ is the $n \\times p$ matrix of regressors (with $p=3$), $\\beta$ is the $p \\times 1$ vector of true parameters, and $\\varepsilon$ is the $n \\times 1$ vector of disturbances. The regressor matrix for observation $i$ is $x_i^T = [1, \\text{income}_i, \\text{educ}_i]$.\n\nThe problem specifies two key assumptions about the error term $\\varepsilon$:\n$1$. The exogeneity condition: $E[\\varepsilon \\mid X] = 0$. This ensures that the OLS estimator is unbiased.\n$2$. A specific form of heteroskedasticity: $\\operatorname{Var}(\\varepsilon_i \\mid X) = \\sigma_i^2 = \\sigma^2 \\cdot \\text{income}_i$. This means the variance of the error term is not constant but depends on one of the regressors. The full covariance matrix of the error vector is $\\Omega = \\operatorname{Var}(\\varepsilon \\mid X) = \\operatorname{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$, which is not proportional to the identity matrix $I_n$.\n\nThe OLS estimator for $\\beta$ is derived by minimizing the sum of squared residuals, $S(\\beta) = (y - X\\beta)^T(y - X\\beta)$. The well-known solution is:\n$$\n\\hat{\\beta}_{OLS} = (X^T X)^{-1} X^T y\n$$\nUnder the exogeneity assumption, this estimator is unbiased, i.e., $E[\\hat{\\beta}_{OLS} \\mid X] = \\beta$. However, for valid statistical inference (constructing confidence intervals and performing hypothesis tests), we need a consistent estimator for the covariance matrix of $\\hat{\\beta}_{OLS}$. The true covariance matrix of the OLS estimator, conditional on $X$, is:\n$$\n\\operatorname{Var}(\\hat{\\beta}_{OLS} \\mid X) = (X^T X)^{-1} X^T \\Omega X (X^T X)^{-1}\n$$\n\nThe simulation compares two estimators for this covariance matrix.\n\nFirst, the classical, homoskedasticity-only variance estimator. This estimator incorrectly assumes that $\\Omega = \\sigma^2 I_n$. It is given by:\n$$\n\\widehat{\\operatorname{Var}}_{classical}(\\hat{\\beta}) = \\hat{s}^2 (X^T X)^{-1}\n$$\nwhere $\\hat{s}^2 = \\frac{1}{n-p} \\sum_{i=1}^n \\hat{\\varepsilon}_i^2$ is the standard unbiased estimator of the error variance, and $\\hat{\\varepsilon}_i = y_i - x_i^T \\hat{\\beta}$ are the OLS residuals. When heteroskedasticity is present, this formula is incorrect, and the resulting standard errors are biased and inconsistent. Confidence intervals constructed using these standard errors will not have the correct nominal coverage probability.\n\nSecond, the Heteroskedasticity-Consistent (HC) variance estimator, specifically the HC0 estimator proposed by White (1980). This estimator does not require the homoskedasticity assumption and provides a consistent estimate of the true covariance matrix. It replaces the unknown $\\Omega$ in the true formula with a diagonal matrix of squared OLS residuals, $\\hat{\\Omega} = \\operatorname{diag}(\\hat{\\varepsilon}_1^2, \\dots, \\hat{\\varepsilon}_n^2)$. The HC0 estimator is:\n$$\n\\widehat{\\operatorname{Var}}_{HC0}(\\hat{\\beta}) = (X^T X)^{-1} (X^T \\hat{\\Omega} X) (X^T X)^{-1}\n$$\nThe term $X^T \\hat{\\Omega} X$ is often called the \"sandwich meat\". The standard errors derived from this estimator are \"robust\" to heteroskedasticity of an unknown form. Asymptotically, confidence intervals based on HC standard errors will achieve the correct nominal coverage.\n\nThe simulation will proceed as follows for each test case $(n, \\sigma^2, R)$:\n$1$. The process is repeated for $R$ replications.\n$2$. In each replication, a synthetic dataset $(y, X)$ of size $n$ is generated according to the specified data-generating process with true parameters $(\\beta_0, \\beta_1, \\beta_2) = (1, 0.8, 0.5)$.\n$3$. The model is estimated using OLS to obtain $\\hat{\\beta}$ and the residuals $\\hat{\\varepsilon}$.\n$4$. The standard error for $\\hat{\\beta}_1$ is computed using both the classical formula and the HC0 robust formula. This involves taking the square root of the second diagonal element (index $1$) of the respective estimated covariance matrices.\n$5$. Two $95\\%$ confidence intervals for $\\beta_1$ are constructed: $[\\hat{\\beta}_1 \\pm z_{0.975} \\cdot SE(\\hat{\\beta}_1)]$, where $z_{0.975}$ is the critical value from the standard normal distribution ($z_{0.975} \\approx 1.95996$) and $SE(\\hat{\\beta}_1)$ is the corresponding standard error estimate.\n$6$. For each interval, we check if it contains the true value $\\beta_1 = 0.8$.\n$7$. After all replications, the empirical coverage for each method is calculated as the fraction of intervals that contained the true parameter. These coverages are compared to the nominal level of $0.95$.\nThe expectation is that the empirical coverage of the robust confidence intervals will be close to $0.95$, while the coverage of the classical intervals will deviate significantly, demonstrating their failure under heteroskedasticity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_simulation(n, sigma2, R, true_beta, rng):\n    \"\"\"\n    Runs a single Monte Carlo simulation for a given parameter set.\n    \"\"\"\n    beta1_true = true_beta[1]\n    num_params = len(true_beta)\n    \n    # Pre-calculate the critical value for 95% CI\n    # z_crit = 1.959964 is norm.ppf(0.975)\n    z_crit = norm.ppf(1 - 0.05 / 2.0)\n\n    # Counters for coverage\n    classical_hits = 0\n    robust_hits = 0\n\n    for _ in range(R):\n        # 1. Generate data\n        income = rng.gamma(shape=4, scale=12, size=n)\n        educ = rng.normal(loc=14, scale=2, size=n)\n        educ = np.clip(educ, 8, 22)\n        \n        X = np.column_stack((np.ones(n), income, educ))\n        \n        # Generate heteroskedastic errors\n        error_variances = sigma2 * income\n        epsilon = rng.normal(loc=0, scale=np.sqrt(error_variances))\n        \n        y = X @ true_beta + epsilon\n\n        # 2. OLS Estimation\n        try:\n            XTX = X.T @ X\n            XTX_inv = np.linalg.inv(XTX)\n            beta_hat = XTX_inv @ X.T @ y\n        except np.linalg.LinAlgError:\n            continue # Skip iteration if X is singular\n\n        beta1_hat = beta_hat[1]\n        residuals = y - X @ beta_hat\n\n        # 3. Calculate Classical (Homoskedastic) Standard Errors\n        s2_classical = np.sum(residuals**2) / (n - num_params)\n        var_beta_classical = s2_classical * XTX_inv\n        se_beta1_classical = np.sqrt(var_beta_classical[1, 1])\n\n        # 4. Calculate HC0 (Robust) Standard Errors\n        # Efficiently compute the sandwich meat: X^T * diag(res^2) * X\n        sandwich_meat = X.T @ (X * (residuals**2)[:, np.newaxis])\n        var_beta_hc0 = XTX_inv @ sandwich_meat @ XTX_inv\n        \n        # Ensure variance is non-negative due to potential floating point errors\n        var_beta1_hc0 = var_beta_hc0[1, 1]\n        if var_beta1_hc0 < 0:\n            var_beta1_hc0 = 0\n        se_beta1_hc0 = np.sqrt(var_beta1_hc0)\n\n        # 5. Construct Confidence Intervals and check coverage\n        # Classical CI\n        ci_classical_lower = beta1_hat - z_crit * se_beta1_classical\n        ci_classical_upper = beta1_hat + z_crit * se_beta1_classical\n        if ci_classical_lower <= beta1_true <= ci_classical_upper:\n            classical_hits += 1\n\n        # Robust CI\n        ci_robust_lower = beta1_hat - z_crit * se_beta1_hc0\n        ci_robust_upper = beta1_hat + z_crit * se_beta1_hc0\n        if ci_robust_lower <= beta1_true <= ci_robust_upper:\n            robust_hits += 1\n\n    # 6. Calculate empirical coverages\n    coverage_classical = classical_hits / R\n    coverage_robust = robust_hits / R\n\n    # 7. Compare performance\n    is_robust_better = abs(coverage_robust - 0.95) < abs(coverage_classical - 0.95)\n\n    return [coverage_classical, coverage_robust, is_robust_better]\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (n, sigma^2, R) = (500, 1.0, 1000)\n        (500, 1.0, 1000),\n        # Case B: (n, sigma^2, R) = (60, 1.0, 1000)\n        (60, 1.0, 1000),\n        # Case C: (n, sigma^2, R) = (500, 3.0, 1000)\n        (500, 3.0, 1000),\n        # Case D: (n, sigma^2, R) = (500, 0.2, 1000)\n        (500, 0.2, 1000),\n    ]\n\n    true_beta = np.array([1.0, 0.8, 0.5])\n    seed = 20240519\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for n, sigma2, R in test_cases:\n        result = run_simulation(n, sigma2, R, true_beta, rng)\n        results.append(result)\n\n    # Format the output string exactly as required\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        cov_c, cov_r, is_better = res\n        # Format boolean as lowercase 'true'/'false'\n        bool_str = 'true' if is_better else 'false'\n        output_str += f\"[{cov_c},{cov_r},{bool_str}]\"\n        if i < len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    # Final print statement in the exact required format.\n    print(output_str)\n\nsolve()\n```"}]}