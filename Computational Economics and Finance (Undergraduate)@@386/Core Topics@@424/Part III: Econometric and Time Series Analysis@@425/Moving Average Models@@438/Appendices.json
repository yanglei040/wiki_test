{"hands_on_practices": [{"introduction": "We begin our hands-on journey with the simplest yet fundamental moving average model, the MA(1) process. A key feature of an MA(1) model is that its structure is entirely captured by its first-order autocorrelation, making it a perfect laboratory for studying serial dependence. This exercise [@problem_id:2412526] challenges you to simulate data from an MA(1) process and test for a \"hot hand\" effect, providing direct practice in hypothesis testing for the very properties that define these models.", "id": "2412526", "problem": "You are given a univariate discrete-time series representing a basketball player's game-by-game deviation in points scored from their long-run average. Let the deviation at time index $t$ be denoted by $y_t$. Assume that $y_t$ follows a Moving Average (MA) model of order one, denoted Moving Average (MA)($1$), defined by\n$$\ny_t = \\varepsilon_t + \\theta \\varepsilon_{t-1},\n$$\nwhere $\\{\\varepsilon_t\\}$ is an independent and identically distributed sequence with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, and $\\theta \\in \\mathbb{R}$ is a constant.\n\nA \"hot hand\" effect is defined as the presence of positive first-order serial dependence in $y_t$, which corresponds to positive lag-$1$ autocorrelation of the centered series. For each test case below, you must determine whether the data provide evidence for a hot hand effect at a one-sided significance level $\\alpha$.\n\nFor each test case, you must:\n- Simulate a realization $\\{y_t\\}_{t=1}^N$ using the specified parameters $(N,\\theta,\\sigma^2)$ and random number seed.\n- Treat $y_t$ as deviations from the long-run average and use the centered series (subtracting the sample mean) to evaluate first-order serial dependence.\n- Decide whether there is evidence of a hot hand effect at significance level $\\alpha$.\n\nYou must use the following test suite. For each case, use the given integer random seed to initialize your random number generator before simulating:\n- Case $1$: $N=400$, $\\theta=0.8$, $\\sigma^2=9$, $\\alpha=0.05$, seed $=1729$.\n- Case $2$: $N=400$, $\\theta=0.0$, $\\sigma^2=9$, $\\alpha=0.05$, seed $=31415$.\n- Case $3$: $N=400$, $\\theta=-0.8$, $\\sigma^2=9$, $\\alpha=0.05$, seed $=271828$.\n- Case $4$: $N=30$, $\\theta=0.6$, $\\sigma^2=9$, $\\alpha=0.05$, seed $=123456$.\n- Case $5$: $N=2$, $\\theta=0.7$, $\\sigma^2=9$, $\\alpha=0.05$, seed $=7$.\n\nYour program should produce a single line of output containing a list of booleans corresponding to the test decision for each case in the order listed above, where each boolean is $True$ if the data provide evidence for a hot hand effect at level $\\alpha$, and $False$ otherwise. The output must be a single line in the exact format\n\"[result1,result2,result3,result4,result5]\".", "solution": "The problem statement is subjected to validation.\n\n**Step 1: Extract Givens**\n- A univariate discrete-time series $\\{y_t\\}$ follows a Moving Average model of order one, MA($1$): $y_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}$.\n- $\\{\\varepsilon_t\\}$ is an i.i.d. sequence with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$.\n- A \"hot hand\" effect is defined as positive first-order serial dependence in $y_t$, corresponding to positive lag-$1$ autocorrelation.\n- The task is to test for this effect at a one-sided significance level $\\alpha$.\n- The procedure involves: simulating a series $\\{y_t\\}_{t=1}^N$, centering it by subtracting the sample mean, and then performing the hypothesis test.\n- Test Cases:\n    - Case $1$: $N=400$, $\\theta=0.8$, $\\sigma^2=9$, $\\alpha=0.05$, seed$=1729$.\n    - Case $2$: $N=400$, $\\theta=0.0$, $\\sigma^2=9$, $\\alpha=0.05$, seed$=31415$.\n    - Case $3$: $N=400$, $\\theta=-0.8$, $\\sigma^2=9$, $\\alpha=0.05$, seed$=271828$.\n    - Case $4$: $N=30$, $\\theta=0.6$, $\\sigma^2=9$, $\\alpha=0.05$, seed$=123456$.\n    - Case $5$: $N=2$, $\\theta=0.7$, $\\sigma^2=9$, $\\alpha=0.05$, seed$=7$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is assessed against the criteria of scientific grounding, being well-posed, and objective.\n\n- **Scientific Grounding**: The problem is based on the standard MA($1$) model from time series analysis, a core topic in econometrics and statistics. The concepts of autocorrelation, hypothesis testing, and significance levels are fundamental statistical principles. The \"hot hand\" concept provides a thematic context but is rigorously defined as a testable statistical property. The problem is scientifically sound.\n- **Well-Posedness**: The problem provides all necessary parameters ($N, \\theta, \\sigma^2$) and random seeds for reproducible simulation of the time series. It clearly specifies the task: a one-sided hypothesis test for positive autocorrelation at a given significance level $\\alpha$. The procedure is unambiguous. A unique solution exists for each test case given the standard statistical methods for testing autocorrelation.\n- **Objectivity**: The problem is stated using precise, objective mathematical language. It avoids subjective or ambiguous terminology.\n\nThe problem does not violate any of the invalidity conditions. Case $5$ with $N=2$ is an extreme but valid statistical scenario. For a sample of size $N=2$, the sample autocorrelation is algebraically fixed, which is a demonstrable property of the statistic, not a flaw in the problem's formulation. The problem is therefore deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Theoretical Framework and Methodology**\n\nThe given time series $\\{y_t\\}$ is generated by a Moving Average process of order $1$, denoted MA($1$):\n$$\ny_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}\n$$\nwhere $\\varepsilon_t$ is a white noise process with $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$. The theoretical first-order autocorrelation coefficient, $\\rho_1$, of this process is given by:\n$$\n\\rho_1 = \\frac{\\text{Cov}(y_t, y_{t-1})}{\\text{Var}(y_t)} = \\frac{\\theta}{1+\\theta^2}\n$$\nA \"hot hand\" effect is defined as positive first-order serial dependence, which translates to a statistical test for $\\rho_1 > 0$. The hypothesis test is formulated as:\n- Null Hypothesis $H_0: \\rho_1 = 0$ (no first-order autocorrelation).\n- Alternative Hypothesis $H_A: \\rho_1 > 0$ (positive first-order autocorrelation).\n\nThis test is performed on the simulated data $\\{y_t\\}_{t=1}^N$. The test statistic is the sample lag-$1$ autocorrelation coefficient, $\\hat{\\rho}_1$, calculated from the centered series $\\tilde{y}_t = y_t - \\bar{y}$, where $\\bar{y}$ is the sample mean. The formula for $\\hat{\\rho}_1$ is:\n$$\n\\hat{\\rho}_1 = \\frac{\\sum_{t=2}^{N} (y_t - \\bar{y})(y_{t-1} - \\bar{y})}{\\sum_{t=1}^{N} (y_t - \\bar{y})^2}\n$$\nUnder the null hypothesis $H_0$, for a sufficiently large sample size $N$, the distribution of $\\hat{\\rho}_1$ can be approximated by a normal distribution:\n$$\n\\hat{\\rho}_1 \\approx \\mathcal{N}\\left(0, \\frac{1}{N}\\right)\n$$\nFrom this, we construct a standardized test statistic $Z = \\hat{\\rho}_1 \\sqrt{N}$, which follows a standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$, under $H_0$.\n\nFor a one-sided test with significance level $\\alpha$, we reject the null hypothesis $H_0$ if the observed test statistic exceeds the critical value $z_{1-\\alpha}$, which is the $(1-\\alpha)$-quantile of the standard normal distribution. The decision rule is:\n$$\n\\text{Reject } H_0 \\text{ if } \\hat{\\rho}_1 \\sqrt{N} > z_{1-\\alpha}\n$$\nThis large-sample approximation is appropriate for cases with large $N$ (e.g., $N=400$). Its accuracy diminishes for small $N$. For the case $N=30$, it is still considered acceptable. For the case $N=2$, the approximation is poor. However, for $N=2$, an analytical examination reveals that for any two distinct points $y_1, y_2$, the sample autocorrelation is fixed at $\\hat{\\rho}_1 = -1/2$. A negative sample autocorrelation can never provide evidence for a positive population autocorrelation. Thus, for $N=2$, we will always fail to reject $H_0: \\rho_1 = 0$ in favor of $H_A: \\rho_1 > 0$.\n\nThe procedure for each test case is as follows:\n$1$. Set the random seed for reproducibility.\n$2$. Generate a sequence of $N+1$ i.i.d. random variables, $\\{\\varepsilon_t\\}_{t=0}^N$, from $\\mathcal{N}(0, \\sigma^2)$.\n$3$. Construct the MA($1$) series $\\{y_t\\}_{t=1}^N$ of length $N$.\n$4$. Calculate the sample mean $\\bar{y}$ and center the series.\n$5$. Compute the sample autocorrelation $\\hat{\\rho}_1$. If the denominator is zero (which occurs with probability zero for continuous data), there is no variation and thus no evidence of correlation.\n$6$. Compute the test statistic $Z = \\hat{\\rho}_1 \\sqrt{N}$.\n$7$. Determine the critical value $z_{1-\\alpha}$ from the standard normal distribution.\n$8$. If $Z > z_{1-\\alpha}$, there is evidence of a hot hand effect (True); otherwise, there is not (False).", "answer": "```python\n# language: Python\n# version: 3.12\n# libraries:\n#   name: numpy\n#   version: 1.23.5\n#   name: scipy\n#   version: 1.11.4\n\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Moving Average (MA)(1) model.\n    \"\"\"\n\n    def analyze_hot_hand(N, theta, sigma_sq, alpha, seed):\n        \"\"\"\n        Simulates an MA(1) series and tests for positive first-order autocorrelation.\n\n        Args:\n            N (int): The length of the time series.\n            theta (float): The MA(1) parameter.\n            sigma_sq (float): The variance of the white noise term.\n            alpha (float): The significance level for the one-sided test.\n            seed (int): The random seed for reproducibility.\n\n        Returns:\n            bool: True if there is evidence of a hot hand, False otherwise.\n        \"\"\"\n        # 1. Set the random seed\n        np.random.seed(seed)\n\n        sigma = np.sqrt(sigma_sq)\n\n        # 2. Generate N+1 white noise terms to produce a series of length N\n        # We need eps_0, ..., eps_N to compute y_1, ..., y_N\n        eps = np.random.normal(loc=0.0, scale=sigma, size=N + 1)\n\n        # 3. Construct the MA(1) series y_t = eps_t + theta * eps_{t-1}\n        # The resulting series 'y' has length N, corresponding to t=1,...,N\n        y = eps[1:] + theta * eps[:-1]\n\n        # 4. Center the series by subtracting the sample mean\n        y_mean = np.mean(y)\n        y_centered = y - y_mean\n        \n        # 5. Compute the sample lag-1 autocorrelation coefficient, rho_hat_1\n        # Denominator of rho_hat_1: sum of squared deviations\n        denominator = np.sum(y_centered**2)\n        \n        # If variance is zero, all y_t are identical.\n        # Autocorrelation is undefined, and there is no evidence of dependence.\n        if denominator == 0.0:\n            return False\n\n        # Numerator of rho_hat_1: sum of cross-products of lagged centered values\n        # y_centered[1:] corresponds to (y_2-y_bar), ..., (y_N-y_bar)\n        # y_centered[:-1] corresponds to (y_1-y_bar), ..., (y_{N-1}-y_bar)\n        numerator = np.sum(y_centered[1:] * y_centered[:-1])\n        \n        rho_hat_1 = numerator / denominator\n\n        # For N <= 1, autocorrelation is not well-defined.\n        if N <= 1:\n            return False\n\n        # 6. Compute the test statistic Z = rho_hat_1 * sqrt(N)\n        # This is based on the large-sample approximation.\n        test_statistic = rho_hat_1 * np.sqrt(N)\n        \n        # 7. Determine the critical value for a one-sided test\n        # This is the (1-alpha) quantile of the standard normal distribution.\n        critical_value = norm.ppf(1 - alpha)\n        \n        # 8. Perform the test: reject H0 if test_statistic > critical_value\n        return test_statistic > critical_value\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, theta, sigma^2, alpha, seed)\n        (400, 0.8, 9.0, 0.05, 1729),\n        (400, 0.0, 9.0, 0.05, 31415),\n        (400, -0.8, 9.0, 0.05, 271828),\n        (30, 0.6, 9.0, 0.05, 123456),\n        (2, 0.7, 9.0, 0.05, 7),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, theta, sigma_sq, alpha, seed = case\n        result = analyze_hot_hand(N, theta, sigma_sq, alpha, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "Having explored the basic properties of MA models, we now turn to one of their primary applications: forecasting. An MA model provides a rule for making optimal one-step-ahead predictions, and the resulting forecast errors—the innovations—are a powerful source of information. In this practical exercise [@problem_id:2412539], you will implement a sequential forecasting algorithm for an MA(3) process to build a simple fraud detection system, learning how to use standardized forecast errors to flag anomalous events.", "id": "2412539", "problem": "You are given a formal model of a single user's normal credit card spending as a Moving Average of order three (MA(3)) time series. The MA(3) model is defined by the core probability structure of linear time series under weak stationarity and serially uncorrelated innovations: a scalar process $\\{y_t\\}$ satisfies $y_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\theta_3 \\varepsilon_{t-3}$, where $\\mu$ is a constant mean, $\\{\\varepsilon_t\\}$ is a zero-mean, serially uncorrelated innovation sequence with variance $\\sigma^2$, and $\\theta_1,\\theta_2,\\theta_3$ are real coefficients. Under this model and the law of iterated expectations, the one-step-ahead forecast $E[y_t \\mid \\mathcal{F}_{t-1}]$ minimizes the mean squared prediction error among $\\mathcal{F}_{t-1}$-measurable predictors, where $\\mathcal{F}_{t-1}$ is the sigma-field generated by $\\{y_s: s \\le t-1\\}$. A transaction at time $t$ is flagged as potentially fraudulent if it causes a large one-step-ahead forecast error relative to the innovation variance. You must design an algorithm that, for each time $t$, produces the one-step-ahead forecast from $\\mathcal{F}_{t-1}$ implied by the MA(3) structure, computes the resulting forecast error, standardizes it by the known standard deviation $\\sigma$, and flags the time $t$ if the absolute standardized error exceeds a pre-specified threshold $\\tau$ expressed in standard deviations. The conditional expectation of any unavailable past innovation in the forecast (for example, when $t \\le 3$) must be treated as zero because it is mean-zero and independent of $\\mathcal{F}_{t-1}$. Your algorithm should sequentially compute the one-step-ahead forecasts and the resulting forecast errors using only information available up to the prior time.\n\nProgram requirements:\n1) Input is hardcoded: You must implement your algorithm for a fixed test suite defined below. No user input is allowed. No external files or network access are allowed.\n2) For each test case, you are given $(\\mu, \\boldsymbol{\\theta}, \\sigma^2, \\tau, \\mathbf{y})$, where $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2,\\theta_3)$ and $\\mathbf{y} = [y_1,\\dots,y_T]$. Treat all quantities as real numbers. The innovation standard deviation is $\\sigma = \\sqrt{\\sigma^2}$. For each time index $t \\in \\{1,\\dots,T\\}$, compute the one-step-ahead forecast $\\widehat{y}_{t \\mid t-1}$ implied by the MA(3) structure using only $\\{y_s: s \\le t-1\\}$, define the forecast error $e_t = y_t - \\widehat{y}_{t \\mid t-1}$, compute the standardized error $z_t = e_t / \\sigma$, and flag time $t$ if $|z_t| > \\tau$. Output, for each test case, the list of flagged indices as integers in increasing order.\n3) You must use the following test suite:\n- Case A (general behavior with a single large spike): $\\mu = 100$, $\\boldsymbol{\\theta} = (0.5,-0.2,0.1)$, $\\sigma^2 = 25$, $\\tau = 3$, $\\mathbf{y} = [98,102,99,140,118,95,106,101]$.\n- Case B (short series edge case, early times): $\\mu = 50$, $\\boldsymbol{\\theta} = (0.7,0.1,-0.4)$, $\\sigma^2 = 4$, $\\tau = 2$, $\\mathbf{y} = [50,56,46]$.\n- Case C (alternating-sign parameters, borderline and one spike): $\\mu = 200$, $\\boldsymbol{\\theta} = (-0.4,0.3,-0.2)$, $\\sigma^2 = 16$, $\\tau = 2.5$, $\\mathbf{y} = [205,194,195,180,210,193]$.\n- Case D (white noise baseline, no flags expected): $\\mu = 0$, $\\boldsymbol{\\theta} = (0,0,0)$, $\\sigma^2 = 9$, $\\tau = 3$, $\\mathbf{y} = [1,-2,4,-5,6,-3]$.\n- Case E (back-to-back large deviations): $\\mu = 75$, $\\boldsymbol{\\theta} = (0.9,0.4,0.2)$, $\\sigma^2 = 100$, $\\tau = 2.5$, $\\mathbf{y} = [70,120,65,50]$.\n4) Final output format: Your program should produce a single line of output containing the results as a comma-separated list of lists of integers, enclosed in square brackets, with no spaces anywhere. For example, a valid output shape is $[[1,3],[2],[]]$. For the given test suite, the output must be a single line of the form $[L_A,L_B,L_C,L_D,L_E]$, where each $L_\\cdot$ is the list of flagged indices for the corresponding case (in increasing order).\n\nYour task: Implement a complete, runnable program that follows the algorithm implied by the MA(3) model definition, computes flags for the entire test suite, and prints the single-line aggregated output exactly in the required format. All numerical answers are dimensionless (no physical units). Angles are not involved. Percentages are not used. The only acceptable data types for each case’s result are lists of integers (possibly empty).", "solution": "The problem statement is subjected to validation and is found to be valid. It is a well-posed, scientifically grounded problem in computational time series analysis, based on the established theory of Moving Average (MA) models and optimal forecasting. All parameters and conditions are fully specified, allowing for a unique and verifiable solution.\n\nThe problem requires the implementation of an anomaly detection algorithm for a time series $\\{y_t\\}$ modeled as a Moving Average process of order $3$, denoted as MA($3$). The model is defined by the equation:\n$$y_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\theta_3 \\varepsilon_{t-3}$$\nHere, $\\mu$ is the constant mean of the process, and $\\{\\varepsilon_t\\}$ is a sequence of serially uncorrelated random variables (innovations or shocks) with mean $E[\\varepsilon_t] = 0$ and constant variance $Var(\\varepsilon_t) = \\sigma^2$. The coefficients $\\theta_1, \\theta_2, \\theta_3$ are real-valued parameters of the model.\n\nThe core of the task is to compute the one-step-ahead forecast, $\\widehat{y}_{t \\mid t-1}$, for each time $t \\in \\{1, 2, \\dots, T\\}$. This forecast is the conditional expectation of $y_t$ given the information available up to time $t-1$, represented by the sigma-field $\\mathcal{F}_{t-1} = \\sigma(\\{y_s: s \\le t-1\\})$. The conditional expectation is the predictor that minimizes the mean squared prediction error.\nThe forecast is calculated as:\n$$\\widehat{y}_{t \\mid t-1} = E[y_t \\mid \\mathcal{F}_{t-1}] = E[\\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\theta_3 \\varepsilon_{t-3} \\mid \\mathcal{F}_{t-1}]$$\nBy the linearity of expectation and properties of conditional expectation, this becomes:\n$$\\widehat{y}_{t \\mid t-1} = E[\\mu \\mid \\mathcal{F}_{t-1}] + E[\\varepsilon_t \\mid \\mathcal{F}_{t-1}] + \\theta_1 E[\\varepsilon_{t-1} \\mid \\mathcal{F}_{t-1}] + \\theta_2 E[\\varepsilon_{t-2} \\mid \\mathcal{F}_{t-1}] + \\theta_3 E[\\varepsilon_{t-3} \\mid \\mathcal{F}_{t-1}]$$\nLet us evaluate each term:\n1. Since $\\mu$ is a constant, $E[\\mu \\mid \\mathcal{F}_{t-1}] = \\mu$.\n2. The innovation $\\varepsilon_t$ is, by definition, unpredictable from past information. Thus, its conditional expectation given the past is its unconditional mean, $E[\\varepsilon_t \\mid \\mathcal{F}_{t-1}] = E[\\varepsilon_t] = 0$.\n3. For past innovations $\\varepsilon_{t-k}$ with $k > 0$, we must determine if they are known at time $t-1$. The model equation can be inverted to express the current innovation in terms of the current observation and past innovations:\n$$\\varepsilon_t = y_t - \\mu - \\theta_1 \\varepsilon_{t-1} - \\theta_2 \\varepsilon_{t-2} - \\theta_3 \\varepsilon_{t-3}$$\nThis relation allows for recursive calculation of the innovation sequence. Given observations $\\{y_s: s \\le t-1\\}$, the values of $\\{\\varepsilon_s: s \\le t-1\\}$ can be determined. Therefore, for $k \\in \\{1, 2, 3\\}$, the innovation $\\varepsilon_{t-k}$ is measurable with respect to $\\mathcal{F}_{t-1}$, and its conditional expectation is its own value: $E[\\varepsilon_{t-k} \\mid \\mathcal{F}_{t-1}] = \\varepsilon_{t-k}$.\n\nHowever, this holds only if the value of $\\varepsilon_{t-k}$ can be computed. For early time steps (e.g., $t=1$), the required past innovations $\\varepsilon_0, \\varepsilon_{-1}, \\varepsilon_{-2}$ are not determined by the observation sequence, which starts at $y_1$. The problem statement correctly specifies that the conditional expectation of any unavailable past innovation must be treated as its unconditional mean, which is $0$. This is the standard treatment for initializing the forecast function.\n\nCombining these facts, the forecast equation is:\n$$\\widehat{y}_{t \\mid t-1} = \\mu + \\theta_1 \\varepsilon_{t-1}^* + \\theta_2 \\varepsilon_{t-2}^* + \\theta_3 \\varepsilon_{t-3}^*$$\nwhere $\\varepsilon_{t-k}^* = \\varepsilon_{t-k}$ if its value has been computed from previous observations, and $\\varepsilon_{t-k}^* = 0$ otherwise.\n\nThe one-step-ahead forecast error is $e_t = y_t - \\widehat{y}_{t \\mid t-1}$. Substituting the expression for $\\widehat{y}_{t \\mid t-1}$ and using the model definition of $y_t$:\n$$e_t = (\\mu + \\varepsilon_t + \\sum_{k=1}^{3} \\theta_k \\varepsilon_{t-k}) - (\\mu + \\sum_{k=1}^{3} \\theta_k \\varepsilon_{t-k}) = \\varepsilon_t$$\nThis confirms that the forecast error is precisely the innovation at time $t$. This is a fundamental result. The algorithm, therefore, must iteratively compute the sequence of innovations.\n\nThe algorithm proceeds as follows:\n1. Initialize a history of innovations. For times $t \\le 0$, the innovations are unknown, so we use their expected value of $0$.\n2. For each time step $t$ from $1$ to $T$:\n    a. Determine the values of past innovations $\\varepsilon_{t-1}, \\varepsilon_{t-2}, \\varepsilon_{t-3}$ required for the forecast. If the history is too short, use $0$ for the missing values.\n    b. Calculate the one-step-ahead forecast: $\\widehat{y}_{t \\mid t-1} = \\mu + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\theta_3 \\varepsilon_{t-3}$.\n    c. Use the current observation $y_t$ to compute the current innovation: $\\varepsilon_t = y_t - \\widehat{y}_{t \\mid t-1}$.\n    d. Store the newly computed $\\varepsilon_t$ for use in subsequent forecasts.\n    e. Compute the standardized error $z_t = \\varepsilon_t / \\sigma$, where $\\sigma = \\sqrt{\\sigma^2}$.\n    f. If $|z_t| > \\tau$, where $\\tau$ is the given threshold, record the index $t$ (using 1-based indexing) as flagged.\n3. Collate the lists of flagged indices for all test cases into the specified output format.\nThis sequential procedure correctly implements the logic of forecasting and anomaly detection under the MA($3$) model.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MA(3) fraud detection problem for a fixed test suite.\n    \"\"\"\n    test_cases = [\n        # Case A: General behavior with a single large spike\n        {\n            'mu': 100.0, 'theta': (0.5, -0.2, 0.1), 'sigma2': 25.0, 'tau': 3.0,\n            'y': [98.0, 102.0, 99.0, 140.0, 118.0, 95.0, 106.0, 101.0]\n        },\n        # Case B: Short series edge case, early times\n        {\n            'mu': 50.0, 'theta': (0.7, 0.1, -0.4), 'sigma2': 4.0, 'tau': 2.0,\n            'y': [50.0, 56.0, 46.0]\n        },\n        # Case C: Alternating-sign parameters, borderline and one spike\n        {\n            'mu': 200.0, 'theta': (-0.4, 0.3, -0.2), 'sigma2': 16.0, 'tau': 2.5,\n            'y': [205.0, 194.0, 195.0, 180.0, 210.0, 193.0]\n        },\n        # Case D: White noise baseline, no flags expected\n        {\n            'mu': 0.0, 'theta': (0.0, 0.0, 0.0), 'sigma2': 9.0, 'tau': 3.0,\n            'y': [1.0, -2.0, 4.0, -5.0, 6.0, -3.0]\n        },\n        # Case E: Back-to-back large deviations\n        {\n            'mu': 75.0, 'theta': (0.9, 0.4, 0.2), 'sigma2': 100.0, 'tau': 2.5,\n            'y': [70.0, 120.0, 65.0, 50.0]\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        mu = case['mu']\n        theta1, theta2, theta3 = case['theta']\n        sigma = np.sqrt(case['sigma2'])\n        tau = case['tau']\n        y_series = case['y']\n\n        eps_history = []\n        flagged_indices = []\n\n        for t_idx, y_t in enumerate(y_series):\n            # Time t is 1-based\n            t = t_idx + 1\n\n            # Get past innovations, using 0 for unavailable history\n            eps_tm1 = eps_history[t_idx - 1] if t_idx - 1 >= 0 else 0.0\n            eps_tm2 = eps_history[t_idx - 2] if t_idx - 2 >= 0 else 0.0\n            eps_tm3 = eps_history[t_idx - 3] if t_idx - 3 >= 0 else 0.0\n\n            # Compute the one-step-ahead forecast\n            y_hat = mu + theta1 * eps_tm1 + theta2 * eps_tm2 + theta3 * eps_tm3\n\n            # Compute the current innovation (forecast error)\n            eps_t = y_t - y_hat\n            eps_history.append(eps_t)\n\n            # Standardize the error\n            z_t = eps_t / sigma\n\n            # Check if it exceeds the threshold\n            if abs(z_t) > tau:\n                flagged_indices.append(t)\n        \n        all_results.append(flagged_indices)\n\n    # Format the final output string exactly as required (no spaces)\n    result_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"}, {"introduction": "Our previous exercises operated under the assumption that the model parameters were known. In practice, these parameters must be estimated from data, and the model's validity must be checked. This capstone exercise [@problem_id:2412549] guides you through the full econometric workflow: fitting an MA(5) model to simulated financial data and performing diagnostic tests on the resulting residuals. By tackling cases of both correct specification and misspecification, you will develop critical skills for building and validating time series models in the real world.", "id": "2412549", "problem": "You are asked to implement an end-to-end estimation and diagnostic testing pipeline for Moving Average (MA) models in the context of computational economics and finance. The pipeline must be entirely self-contained and produce a single, verifiable line of output. Your program must implement the following, starting only from first principles and core definitions.\n\nConsider a univariate time series $\\{x_t\\}_{t=1}^T$ representing daily stock index log returns. The conventional mean model for daily returns in efficient markets uses a Moving Average process of order $q$ (MA($q$)) for the mean dynamics: for $q \\in \\mathbb{N}$, the MA($q$) model is\n$$\nx_t \\;=\\; \\mu \\;+\\; \\epsilon_t \\;+\\; \\theta_1 \\epsilon_{t-1} \\;+\\; \\cdots \\;+\\; \\theta_q \\epsilon_{t-q},\n$$\nwhere $\\mu \\in \\mathbb{R}$ is the unconditional mean, $\\{\\epsilon_t\\}$ is an independent and identically distributed white-noise sequence with $E[\\epsilon_t] = 0$ and $\\operatorname{Var}(\\epsilon_t) = \\sigma^2 \\in \\mathbb{R}_{+}$, and $\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_q) \\in \\mathbb{R}^q$. In efficient markets, the one-step-ahead forecast errors (the estimated innovations) should be unpredictable, implying no linear predictive power from $\\hat{\\epsilon}_t$ for $\\hat{\\epsilon}_{t+1}$.\n\nYour task is to:\n- Fit an MA($5$) model to each provided dataset using Conditional Sum of Squares (CSS), which is defined by recursively computing residuals via the model identity and minimizing the sum of squared residuals over the parameters. Specifically, with initial conditions $\\hat{\\epsilon}_t = 0$ for $t \\le 0$, define\n$$\n\\hat{\\epsilon}_t(\\mu, \\boldsymbol{\\theta}) \\;=\\; x_t \\;-\\; \\mu \\;-\\; \\sum_{j=1}^{5} \\theta_j \\hat{\\epsilon}_{t-j}(\\mu, \\boldsymbol{\\theta}),\n$$\nand minimize $\\sum_{t=1}^{T} \\hat{\\epsilon}_t(\\mu, \\boldsymbol{\\theta})^2$ over $(\\mu, \\boldsymbol{\\theta}) \\in \\mathbb{R}^{6}$.\n- After estimating parameters and constructing the estimated one-step-ahead innovations $\\{\\hat{\\epsilon}_t\\}$ from the fitted MA($5$), test the null hypothesis that $\\hat{\\epsilon}_t$ has no linear predictive power for $\\hat{\\epsilon}_{t+1}$ using an Ordinary Least Squares (OLS) regression of $\\hat{\\epsilon}_{t+1}$ on a constant and $\\hat{\\epsilon}_t$:\n$$\n\\hat{\\epsilon}_{t+1} \\;=\\; \\alpha \\;+\\; \\beta \\hat{\\epsilon}_t \\;+\\; u_{t+1}.\n$$\nUnder the null hypothesis $H_0: \\beta = 0$, compute a two-sided p-value based on the usual OLS $t$-statistic for $\\beta$ with $T-2$ degrees of freedom. Use significance level $\\alpha_{\\text{test}} = 0.05$.\n- Report for each dataset a boolean decision with the following convention: output $\\text{True}$ if you fail to reject $H_0$ at level $\\alpha_{\\text{test}}$ (interpreted as “the residuals are unpredictable”), and output $\\text{False}$ if you reject $H_0$ (interpreted as “the residuals have predictive power for $\\hat{\\epsilon}_{t+1}$”).\n\nFundamental bases and constraints you must adhere to:\n- Use only the definition of white noise, the definition of an MA($q$) model, and the construction of CSS residuals via the model identity as the estimation backbone. Do not use pre-built time series estimation black boxes.\n- The OLS regression and its $t$-test must follow standard linear regression principles, with the $t$-statistic computed from the estimated slope coefficient and its standard error, and the two-sided p-value computed accordingly.\n- There are no physical units in this problem. No angle units appear. No percentages should be printed.\n\nTest suite:\nSimulate four datasets that emulate daily S&amp;P $500$ returns with realistic daily volatility on the order of $1\\%$. For each case, simulate an innovation sequence $\\{w_t\\}$ with $w_t \\sim \\mathcal{N}(0,\\sigma^2)$, where $\\sigma = 0.01$, and construct the process as an Autoregressive Moving Average (ARMA) for generality:\n$$\nx_t - \\mu \\;=\\; \\phi (x_{t-1} - \\mu) \\;+\\; w_t \\;+\\; \\sum_{j=1}^{5} \\theta_j w_{t-j},\n$$\nwith $x_0 = \\mu$ and $w_t = 0$ for $t \\le 0$. This nests MA($5$) as the special case with $\\phi = 0$. For each dataset, use the following parameters:\n\n- Case $1$ (baseline MA($5$), “efficient” mean model):\n  - Seed $= 202311$, length $T = 900$, $\\mu = 0$, $\\phi = 0$, $\\boldsymbol{\\theta} = (0.4, -0.3, 0.2, -0.1, 0.05)$, $\\sigma = 0.01$.\n- Case $2$ (ARMA($1,5$) with mild autoregression, misspecified if one fits MA($5$) only):\n  - Seed $= 202312$, length $T = 900$, $\\mu = 0$, $\\phi = 0.3$, $\\boldsymbol{\\theta} = (0.4, -0.3, 0.2, -0.1, 0.05)$, $\\sigma = 0.01$.\n- Case $3$ (white noise mean dynamics, MA($0$)):\n  - Seed $= 202313$, length $T = 600$, $\\mu = 0$, $\\phi = 0$, $\\boldsymbol{\\theta} = (0, 0, 0, 0, 0)$, $\\sigma = 0.01$.\n- Case $4$ (pure autoregression AR($1$) with moderate persistence, misspecified if one fits MA($5$) only):\n  - Seed $= 202314$, length $T = 250$, $\\mu = 0$, $\\phi = 0.6$, $\\boldsymbol{\\theta} = (0, 0, 0, 0, 0)$, $\\sigma = 0.01$.\n\nYour program must:\n- Simulate each dataset exactly as specified using the given seeds and parameters.\n- Fit an MA($5$) model by CSS to each dataset to obtain $\\{\\hat{\\epsilon}_t\\}$.\n- Perform the OLS test of $H_0: \\beta = 0$ in the regression of $\\hat{\\epsilon}_{t+1}$ on a constant and $\\hat{\\epsilon}_t$ using a two-sided test at level $\\alpha_{\\text{test}} = 0.05$, and form the decision as described.\n- Produce a single line of output containing a Python-style list of booleans in the order of the cases $1,2,3,4$, for example, $[\\,\\text{True},\\text{False},\\text{True},\\text{False}\\,]$.\n\nYour program must be a complete, runnable script and must not require any user input or external files. The final output format must be exactly one line that is a comma-separated list of booleans enclosed in square brackets.", "solution": "The problem statement has been critically evaluated and is deemed valid. It is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of instructions for a standard exercise in time series econometrics: the estimation of a Moving Average (MA) model and the diagnostic testing of its residuals. The procedure is to be implemented from first principles, which is a rigorous test of fundamental understanding. The single minor inconsistency noted—the specified degrees of freedom for the $t$-test being $T-2$ instead of the canonical $T-3$ for a simple regression on a time series of length $T$—is interpreted as a direct and unambiguous instruction, not a flaw that invalidates the problem. We proceed with the solution as specified.\n\nThe solution is structured into three main stages:\n1.  Simulation of the time series data according to the specified Autoregressive Moving Average (ARMA) processes.\n2.  Estimation of a Moving Average model of order $5$, i.e., MA($5$), for each dataset using the Conditional Sum of Squares (CSS) method.\n3.  Diagnostic testing of the estimated model's residuals for first-order serial autocorrelation using an Ordinary Least Squares (OLS) regression-based test.\n\n**1. Time Series Simulation**\n\nThe problem requires the generation of four datasets from a general ARMA($1,5$) process. The model for the time series $\\{x_t\\}$ is given by:\n$$\nx_t - \\mu \\;=\\; \\phi (x_{t-1} - \\mu) \\;+\\; w_t \\;+\\; \\sum_{j=1}^{5} \\theta_j w_{t-j}\n$$\nwhere $\\mu$ is the mean, $\\phi$ is the autoregressive coefficient, $\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_5)$ are the moving average coefficients, and $\\{w_t\\}$ is a Gaussian white noise process with $w_t \\sim \\mathcal{N}(0, \\sigma^2)$. The simulation starts from initial conditions $x_0 = \\mu$ and $w_t = 0$ for $t \\le 0$. The data $\\{x_t\\}_{t=1}^T$ are generated recursively for each of the four specified parameter sets. This setup allows for examining the performance of the MA($5$) estimation procedure under both correct specification (Case 1 and 3) and misspecification (Case 2 and 4).\n\n**2. MA(5) Estimation via Conditional Sum of Squares (CSS)**\n\nFor each simulated time series $\\{x_t\\}_{t=1}^T$, we fit an MA($5$) model:\n$$\nx_t \\;=\\; \\mu \\;+\\; \\epsilon_t \\;+\\; \\sum_{j=1}^{5} \\theta_j \\epsilon_{t-j}\n$$\nThe parameters to be estimated are the mean $\\mu$ and the MA coefficients $\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_5)$. The estimation is performed by minimizing the Conditional Sum of Squares (CSS). This involves computing the model's residuals $\\{\\hat{\\epsilon}_t\\}$ recursively and minimizing their sum of squares.\n\nThe residuals are defined by rearranging the model equation:\n$$\n\\hat{\\epsilon}_t(\\mu, \\boldsymbol{\\theta}) \\;=\\; x_t \\;-\\; \\mu \\;-\\; \\sum_{j=1}^{5} \\theta_j \\hat{\\epsilon}_{t-j}(\\mu, \\boldsymbol{\\theta})\n$$\nTo start the recursion, we impose the initial condition $\\hat{\\epsilon}_t = 0$ for all $t \\le 0$. The objective function to be minimized is the sum of squared residuals:\n$$\nS(\\mu, \\boldsymbol{\\theta}) = \\sum_{t=1}^{T} \\hat{\\epsilon}_t(\\mu, \\boldsymbol{\\theta})^2\n$$\nThis is a non-linear optimization problem in the $6$ parameters $(\\mu, \\theta_1, \\ldots, \\theta_5)$. We employ a numerical quasi-Newton optimization algorithm (specifically, L-BFGS-B available in `scipy.optimize`) to find the parameter estimates $(\\hat{\\mu}, \\hat{\\boldsymbol{\\theta}})$ that minimize $S(\\mu, \\boldsymbol{\\theta})$. Once these estimates are found, the final series of estimated innovations, or residuals, $\\{\\hat{\\epsilon}_t\\}_{t=1}^T$, is computed.\n\n**3. Diagnostic Testing for Residual Autocorrelation**\n\nA correctly specified and estimated time series model should leave behind residuals that are unpredictable from their own past; that is, they should approximate a white noise process. To test this, we examine whether the estimated innovation at time $t$, $\\hat{\\epsilon}_t$, has any linear predictive power for the innovation at time $t+1$, $\\hat{\\epsilon}_{t+1}$. This is tested using an OLS regression:\n$$\n\\hat{\\epsilon}_{t+1} \\;=\\; \\alpha \\;+\\; \\beta \\hat{\\epsilon}_t \\;+\\; u_{t+1}, \\quad t = 1, \\ldots, T-1\n$$\nThe null hypothesis of no first-order serial autocorrelation is $H_0: \\beta = 0$. We compute the OLS estimate $\\hat{\\beta}$ and its associated $t$-statistic.\n\nThe OLS estimate for $\\beta$ is a component of the vector $\\hat{\\mathbf{b}} = (\\hat{\\alpha}, \\hat{\\beta})'$, given by:\n$$\n\\hat{\\mathbf{b}} = (X'X)^{-1}X'Y\n$$\nwhere $Y$ is the vector of dependent variables $(\\hat{\\epsilon}_2, \\ldots, \\hat{\\epsilon}_T)'$, and $X$ is the design matrix whose rows are $(1, \\hat{\\epsilon}_t)$ for $t=1, \\ldots, T-1$.\n\nThe $t$-statistic for $\\hat{\\beta}$ is calculated as:\n$$\nt_{\\hat{\\beta}} = \\frac{\\hat{\\beta}}{\\text{s.e.}(\\hat{\\beta})}\n$$\nThe standard error, $\\text{s.e.}(\\hat{\\beta})$, is the square root of the corresponding diagonal element of the estimated coefficient covariance matrix, $\\hat{\\sigma}_u^2 (X'X)^{-1}$. The variance of the regression error, $\\sigma_u^2$, is estimated by $s_u^2 = \\frac{1}{N_{\\text{reg}}-2} \\sum u_t^2$, where $N_{\\text{reg}}=T-1$ is the number of observations in the regression and $u_t$ are the OLS residuals.\n\nWe then compute a two-sided $p$-value for this $t$-statistic. According to the problem's explicit instruction, we use a Student's $t$-distribution with $T-2$ degrees of freedom.\n\nThe final decision is made by comparing the $p$-value to the significance level $\\alpha_{\\text{test}} = 0.05$:\n- If $p$-value $\\ge 0.05$, we fail to reject the null hypothesis $H_0$. This suggests the residuals are serially uncorrelated, and the model is adequate in this dimension. The output is `True`.\n- If $p$-value $< 0.05$, we reject $H_0$. This indicates significant serial correlation in the residuals, suggesting model misspecification. The output is `False`.\n\nThis complete pipeline is applied to each of the four simulated datasets, and the sequence of boolean decisions is reported as the final answer.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.stats import t\n\ndef solve():\n    \"\"\"\n    Main function to run the entire estimation and testing pipeline for all test cases.\n    \"\"\"\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1: MA(5)\",\n            \"seed\": 202311, \"T\": 900, \"mu\": 0.0, \"phi\": 0.0,\n            \"theta\": np.array([0.4, -0.3, 0.2, -0.1, 0.05]), \"sigma\": 0.01\n        },\n        {\n            \"name\": \"Case 2: ARMA(1,5)\",\n            \"seed\": 202312, \"T\": 900, \"mu\": 0.0, \"phi\": 0.3,\n            \"theta\": np.array([0.4, -0.3, 0.2, -0.1, 0.05]), \"sigma\": 0.01\n        },\n        {\n            \"name\": \"Case 3: MA(0) / White Noise\",\n            \"seed\": 202313, \"T\": 600, \"mu\": 0.0, \"phi\": 0.0,\n            \"theta\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]), \"sigma\": 0.01\n        },\n        {\n            \"name\": \"Case 4: AR(1)\",\n            \"seed\": 202314, \"T\": 250, \"mu\": 0.0, \"phi\": 0.6,\n            \"theta\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]), \"sigma\": 0.01\n        }\n    ]\n    \n    alpha_test = 0.05\n    results = []\n\n    for case in test_cases:\n        # Step 1: Simulate time series data\n        x_t = simulate_arma_process(\n            T=case[\"T\"], mu=case[\"mu\"], phi=case[\"phi\"], \n            theta=case[\"theta\"], sigma=case[\"sigma\"], seed=case[\"seed\"]\n        )\n        \n        # Step 2: Fit an MA(5) model and get residuals\n        residuals = estimate_ma_and_get_residuals(x_t, q=5)\n        \n        # Step 3: Perform diagnostic test on residuals\n        decision = perform_diagnostic_test(residuals, alpha_test)\n        results.append(decision)\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef simulate_arma_process(T, mu, phi, theta, sigma, seed):\n    \"\"\"\n    Simulates an ARMA(1, q) process given parameters.\n    \"\"\"\n    np.random.seed(seed)\n    q = len(theta)\n    \n    # Generate white noise innovations\n    w = np.random.normal(loc=0.0, scale=sigma, size=T)\n    \n    # Initialize the time series array\n    x = np.zeros(T)\n    \n    # Create padded arrays for past values\n    x_padded = np.concatenate((np.full(1, mu), x)) # x_padded[t] corresponds to x_{t-1}\n    w_padded = np.concatenate((np.zeros(q), w)) # w_padded[t+q] corresponds to w_t\n    \n    for t_idx in range(T): # t_idx from 0 to T-1\n        # In math notation, this corresponds to t = t_idx + 1\n        \n        # AR term: phi * (x_{t-1} - mu)\n        ar_term = phi * (x_padded[t_idx] - mu)\n        \n        # MA term: sum_{j=1 to q} theta_j * w_{t-j}\n        ma_term = 0\n        for j in range(q): # j from 0 to q-1\n            # lag j+1, theta_{j+1}, w_{t-(j+1)}\n            ma_term += theta[j] * w_padded[t_idx + q - (j + 1)]\n            \n        x[t_idx] = mu + ar_term + w[t_idx] + ma_term\n        x_padded[t_idx+1] = x[t_idx]\n        \n    return x\n\ndef estimate_ma_and_get_residuals(x, q):\n    \"\"\"\n    Estimates an MA(q) model using CSS and returns the residuals.\n    \"\"\"\n    # Objective function for CSS minimization\n    def css_objective(params, x_data, q_order):\n        T = len(x_data)\n        mu = params[0]\n        theta = params[1:]\n        eps_hat = np.zeros(T)\n        \n        for i in range(T): # i from 0 to T-1, corresponds to time t=i+1\n            ma_term = 0\n            for j in range(q_order): # j from 0 to q-1, corresponds to lag j+1\n                idx = i - (j + 1)\n                if idx >= 0:\n                    ma_term += theta[j] * eps_hat[idx]\n            eps_hat[i] = x_data[i] - mu - ma_term\n        \n        return np.sum(eps_hat**2)\n\n    # Initial guess for parameters (mu, theta_1, ..., theta_q)\n    initial_params = np.zeros(q + 1)\n    \n    # Minimize the CSS\n    opt_result = minimize(\n        css_objective, \n        initial_params, \n        args=(x, q),\n        method='L-BFGS-B'\n    )\n    \n    # Get estimated parameters\n    estimated_params = opt_result.x\n    \n    # Compute final residuals with estimated parameters\n    T = len(x)\n    mu_hat = estimated_params[0]\n    theta_hat = estimated_params[1:]\n    residuals = np.zeros(T)\n    for i in range(T):\n        ma_term = 0\n        for j in range(q):\n            idx = i - (j + 1)\n            if idx >= 0:\n                ma_term += theta_hat[j] * residuals[idx]\n        residuals[i] = x[i] - mu_hat - ma_term\n        \n    return residuals\n\ndef perform_diagnostic_test(eps_hat, alpha_level):\n    \"\"\"\n    Tests H0: beta=0 in eps_{t+1} = alpha + beta*eps_t + u_{t+1}.\n    Returns True if we fail to reject H0, False otherwise.\n    \"\"\"\n    T = len(eps_hat)\n    \n    # Prepare data for OLS regression\n    y = eps_hat[1:]       # eps_{t+1} for t=1,...,T-1\n    x_reg = eps_hat[:-1]  # eps_t for t=1,...,T-1\n    \n    N_reg = len(y) # Number of observations in regression, T-1\n    \n    # Add a constant (intercept) to the regressor\n    X_reg = np.vstack([np.ones(N_reg), x_reg]).T\n    \n    # OLS estimator: (X'X)^{-1}X'y\n    try:\n        XTX_inv = np.linalg.inv(X_reg.T @ X_reg)\n        b_hat = XTX_inv @ X_reg.T @ y\n    except np.linalg.LinAlgError:\n        # Handle cases of perfect multicollinearity (unlikely with this data)\n        return True # Cannot reject H0 if test cannot be performed\n\n    beta_hat = b_hat[1]\n    \n    # Calculate t-statistic\n    ols_residuals = y - X_reg @ b_hat\n    \n    # Estimate variance of regression error term u\n    # Degrees of freedom for residual variance is N_reg - number of parameters (2)\n    df_residual = N_reg - 2\n    if df_residual <= 0:\n        return True # Not enough data to perform test\n    \n    s2_u = np.sum(ols_residuals**2) / df_residual\n    \n    # Covariance matrix of beta estimates\n    var_b_hat = s2_u * XTX_inv\n    se_beta_hat = np.sqrt(var_b_hat[1, 1])\n    \n    if se_beta_hat == 0:\n        return True # Avoid division by zero\n        \n    t_stat = beta_hat / se_beta_hat\n    \n    # Calculate p-value using t-distribution\n    # As per problem, degrees of freedom is T-2\n    df_test = T - 2\n    p_value = 2 * t.sf(np.abs(t_stat), df=df_test)\n    \n    # Decision rule\n    return p_value >= alpha_level\n\nif __name__ == \"__main__\":\n    solve()\n\n```"}]}