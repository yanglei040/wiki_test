{"hands_on_practices": [{"introduction": "The Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) are the cornerstones of time series model identification. While the ACF measures the total correlation between observations at a given lag, the PACF cleverly isolates the direct relationship by removing the confounding effects of all intermediate lags. This exercise provides a foundational workout in computing the PACF directly from known ACF values, reinforcing the crucial mathematical relationship established by the Durbin-Levinson recursion. [@problem_id:1943287] Mastering this calculation is a key step toward interpreting diagnostic plots effectively.", "id": "1943287", "problem": "In the analysis of a stationary time series, the Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) are fundamental tools for identifying the underlying structure of the data. For a given stationary process, let $\\rho(k)$ denote the theoretical ACF at lag $k$, which measures the correlation between observations $k$ time steps apart. Let $\\phi_{kk}$ denote the theoretical PACF at lag $k$, which measures the correlation between observations $k$ time steps apart after removing the linear effects of the intermediate observations.\n\nSuppose a stationary time series has theoretical ACF values given by $\\rho(1) = 0.8$ and $\\rho(2) = 0.5$. Calculate the value of its theoretical PACF at lag 2, denoted by $\\phi_{22}$. Round your final answer to four significant figures.\n\n", "solution": "We use the Durbin–Levinson recursion relating the theoretical ACF $\\rho(k)$ and the PACF $\\phi_{kk}$. For $k=1$,\n$$\n\\phi_{11}=\\rho(1).\n$$\nFor $k=2$, the recursion gives\n$$\n\\phi_{22}=\\frac{\\rho(2)-\\phi_{11}\\rho(1)}{1-\\phi_{11}\\rho(1)}.\n$$\nSubstituting $\\phi_{11}=\\rho(1)$ yields\n$$\n\\phi_{22}=\\frac{\\rho(2)-\\rho(1)^{2}}{1-\\rho(1)^{2}}.\n$$\nWith the given values $\\rho(1)=0.8$ and $\\rho(2)=0.5$,\n$$\n\\phi_{22}=\\frac{0.5-0.8^{2}}{1-0.8^{2}}=\\frac{0.5-0.64}{1-0.64}=\\frac{-0.14}{0.36}=-\\frac{7}{18}\\approx -0.388888\\ldots\n$$\nRounding to four significant figures gives $-0.3889$.", "answer": "$$\\boxed{-0.3889}$$"}, {"introduction": "Autoregressive (AR) models describe processes where the current value is a linear function of its own past values, capturing the \"memory\" inherent in many economic and financial time series. A defining characteristic of an AR process of order $p$, denoted AR($p$), is its diagnostic signature: an ACF that decays gradually and a PACF that cuts off abruptly after lag $p$. This hands-on practice challenges you to simulate an AR(2) process from first principles and computationally verify this signature [@problem_id:2373131], bridging the gap between abstract theory and the practical analysis of sample data.", "id": "2373131", "problem": "Consider a univariate, covariance-stationary time series in computational economics and finance, where the underlying data generating process can be modeled as a linear autoregressive model. Your task is to construct and verify a process whose sample autocorrelation function (ACF) exhibits a clear sinusoidal pattern while the sample partial autocorrelation function (PACF) does not. The focus is on deriving and implementing the logic from first principles: definitions of covariance stationarity, autocovariance, autocorrelation, and the partial autocorrelation via the Yule–Walker system solved by the Levinson–Durbin recursion. Do not use any pre-built time series utilities; implement the required estimators directly from their definitions.\n\nFundamental definitions to use:\n\n- A zero-mean, weakly stationary process has autocovariance sequence $\\{\\gamma_{k}\\}_{k \\in \\mathbb{Z}}$ with $\\gamma_{k} = \\mathbb{E}[(X_{t} - \\mu)(X_{t-k} - \\mu)]$ that depends only on the lag $k$, where $\\mu = \\mathbb{E}[X_{t}]$ is constant and $\\operatorname{Var}(X_{t}) = \\gamma_{0} \\lt \\infty$ does not depend on $t$.\n\n- The autocorrelation at lag $k$ is $\\rho_{k} = \\gamma_{k}/\\gamma_{0}$.\n\n- The partial autocorrelation at lag $k$, denoted $\\alpha_{k}$, is the correlation between $X_{t}$ and $X_{t-k}$ after removing linear dependence on intermediate lags $1,\\dots,k-1$. It is the last coefficient in the solution of the Yule–Walker linear system for an autoregression of order $k$ and can be computed recursively via the Levinson–Durbin algorithm applied to the sample autocovariance sequence.\n\nYou will simulate a second-order autoregressive process $\\text{AR}(2)$ defined by\n$$\nX_{t} = \\phi_{1} X_{t-1} + \\phi_{2} X_{t-2} + \\varepsilon_{t},\n$$\nwhere $\\{\\varepsilon_{t}\\}$ is an independent and identically distributed white noise sequence with $\\varepsilon_{t} \\sim \\mathcal{N}(0,\\sigma^{2})$, with $\\sigma^{2} = 1$. Choose $(\\phi_{1},\\phi_{2})$ such that the roots of the characteristic polynomial $1 - \\phi_{1} z - \\phi_{2} z^{2}$ are complex and lie outside the unit circle to ensure covariance stationarity and oscillatory dynamics.\n\nImplement from first principles:\n\n- A simulator for $\\text{AR}(2)$ with a burn-in period to approximate stationarity.\n\n- The sample mean $\\hat{\\mu}$ and the sample autocovariance sequence $\\{\\hat{\\gamma}_{k}\\}_{k=0}^{K}$ using\n$$\n\\hat{\\mu} = \\frac{1}{n} \\sum_{t=1}^{n} X_{t}, \\qquad\n\\hat{\\gamma}_{k} = \\frac{1}{n} \\sum_{t=k+1}^{n} (X_{t} - \\hat{\\mu})(X_{t-k} - \\hat{\\mu}).\n$$\n\n- The sample autocorrelation sequence $\\hat{\\rho}_{k} = \\hat{\\gamma}_{k}/\\hat{\\gamma}_{0}$ for $k=0,1,\\dots,K$.\n\n- The sample PACF $\\{\\hat{\\alpha}_{k}\\}_{k=1}^{K}$ via the Levinson–Durbin recursion applied to $\\{\\hat{\\gamma}_{k}\\}_{k=0}^{K}$.\n\nVerification criteria for each simulated series must be computed as follows:\n\n- Define $K$ as the maximal lag for the diagnostic checks.\n\n- Declare that the ACF has a “clear sinusoidal pattern” if both conditions hold:\n  1. The number of sign changes in the sequence $\\{\\hat{\\rho}_{k}\\}_{k=1}^{K}$, after discarding terms with magnitude less than a tolerance $\\varepsilon$, is at least $S_{\\min}$.\n  2. The magnitude decays in the sense that $|\\hat{\\rho}_{K}| \\lt |\\hat{\\rho}_{1}|$.\n\n- Declare that the PACF “does not have a sinusoidal pattern” by verifying that it effectively cuts off after lag $2$ for an $\\text{AR}(2)$, operationalized as\n$$\n\\max_{k \\in \\{3,4,\\dots,K\\}} |\\hat{\\alpha}_{k}| \\le \\tau,\n$$\nfor a chosen tolerance $\\tau$.\n\nTest suite:\n\nYou must run exactly the following three cases, where each tuple specifies $(\\phi_{1}, \\phi_{2}, n, K, \\text{seed}, \\text{burn\\_in})$:\n\n- Case $1$: $(\\phi_{1}, \\phi_{2}, n, K, \\text{seed}, \\text{burn\\_in}) = (\\, 1.5,\\, -0.75,\\, 600,\\, 20,\\, 123,\\, 500 \\,)$.\n\n- Case $2$: $(\\phi_{1}, \\phi_{2}, n, K, \\text{seed}, \\text{burn\\_in}) = (\\, 1.2,\\, -0.64,\\, 400,\\, 20,\\, 456,\\, 500 \\,)$.\n\n- Case $3$: $(\\phi_{1}, \\phi_{2}, n, K, \\text{seed}, \\text{burn\\_in}) = (\\, 1.6,\\, -0.85,\\, 300,\\, 20,\\, 789,\\, 500 \\,)$.\n\nUse $\\varepsilon = 0.05$, $S_{\\min} = 3$, and $\\tau = 0.30$ in the above criteria.\n\nFor each case, output a boolean that is true if and only if both diagnostics are satisfied: the ACF is sinusoidal according to the above rule, and the PACF does not exhibit a sinusoidal pattern because it effectively cuts off beyond lag $2$ within tolerance $\\tau$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result1},\\text{result2},\\text{result3}]$, where each entry is a boolean corresponding to the three cases above, in order.\n\nNo physical units or angles are to be reported as outputs in this task. If any internal angle arises from theory, interpret angles in radians, but do not output any angles.", "solution": "The problem statement is subjected to validation. All givens, definitions, and constraints have been extracted and analyzed. The problem is found to be scientifically grounded, well-posed, and objective. It is a standard exercise in computational time series analysis, specifically concerning the properties of autoregressive models. The parameters for the test cases are chosen such that the specified conditions for stationarity and oscillatory behavior are met. For an $\\text{AR}(2)$ process $X_{t} = \\phi_{1} X_{t-1} + \\phi_{2} X_{t-2} + \\varepsilon_{t}$, stationarity requires that the roots of the characteristic polynomial $1 - \\phi_{1} z - \\phi_{2} z^{2} = 0$ lie outside the unit circle. This is equivalent to the conditions $\\phi_{1} + \\phi_{2} < 1$, $\\phi_{2} - \\phi_{1} < 1$, and $|\\phi_{2}| < 1$. Oscillatory behavior, which leads to a sinusoidal autocorrelation function, requires the roots to be complex, which occurs when $\\phi_{1}^{2} + 4\\phi_{2} < 0$. All three provided parameter sets $(\\phi_{1}, \\phi_{2})$ satisfy these conditions. The problem is therefore deemed valid and a solution will be constructed.\n\nThe task is to simulate a second-order autoregressive, $\\text{AR}(2)$, process and verify that its sample statistics conform to theoretical expectations. Specifically, for an $\\text{AR}(2)$ process with complex characteristic roots, the theoretical autocorrelation function (ACF) exhibits behavior analogous to a damped sinusoid, while the partial autocorrelation function (PACF) cuts off after lag $2$. We will implement the necessary algorithms from first principles to verify this behavior for given sample data.\n\nFirst, we simulate the $\\text{AR}(2)$ process defined by $X_{t} = \\phi_{1} X_{t-1} + \\phi_{2} X_{t-2} + \\varepsilon_{t}$, where $\\varepsilon_{t}$ is a white noise process with mean $0$ and variance $\\sigma^2=1$. A burn-in period is used to ensure the simulated series approximates a weakly stationary process. The initial values $X_0$ and $X_{-1}$ are set to $0$. After generating a total of $n + \\text{burn\\_in}$ points, the first $\\text{burn\\_in}$ samples are discarded, leaving a series of length $n$.\n\nNext, we compute the sample statistics. The sample mean is estimated as $\\hat{\\mu} = \\frac{1}{n} \\sum_{t=1}^{n} X_{t}$. Using this, we compute the sample autocovariance sequence $\\{\\hat{\\gamma}_{k}\\}_{k=0}^{K}$ according to the specified formula:\n$$\n\\hat{\\gamma}_{k} = \\frac{1}{n} \\sum_{t=k+1}^{n} (X_{t} - \\hat{\\mu})(X_{t-k} - \\hat{\\mu})\n$$\nThis is a biased estimator, as the sum contains $n-k$ terms while the divisor is $n$. The sample autocorrelation function (ACF) is then computed as $\\hat{\\rho}_{k} = \\hat{\\gamma}_{k}/\\hat{\\gamma}_{0}$ for lags $k=0, 1, \\dots, K$.\n\nThe core of the analysis is the computation of the sample partial autocorrelation function (PACF), denoted $\\{\\hat{\\alpha}_{k}\\}_{k=1}^{K}$. The PACF at lag $k$, $\\alpha_k$, is defined as the last coefficient, $\\phi_{k,k}$, in an autoregressive model of order $k$ fitted to the data. These coefficients are determined by the Yule–Walker equations, which relate the model coefficients to the autocorrelations:\n$$\n\\begin{pmatrix} 1 & \\rho_1 & \\dots & \\rho_{k-1} \\\\ \\rho_1 & 1 & \\dots & \\rho_{k-2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\rho_{k-1} & \\rho_{k-2} & \\dots & 1 \\end{pmatrix} \\begin{pmatrix} \\phi_{k,1} \\\\ \\phi_{k,2} \\\\ \\vdots \\\\ \\phi_{k,k} \\end{pmatrix} = \\begin{pmatrix} \\rho_1 \\\\ \\rho_2 \\\\ \\vdots \\\\ \\rho_k \\end{pmatrix}\n$$\nWe solve this system for $\\phi_{k,k}$ for each lag $k=1, \\dots, K$ using the computationally efficient Levinson–Durbin recursion. Given the sample autocorrelations $\\{\\hat{\\rho}_k\\}$, the sample PACF coefficients $\\{\\hat{\\alpha}_k = \\hat{\\phi}_{k,k}\\}$ are computed recursively.\nFor $k=1$:\n$$ \\hat{\\alpha}_1 = \\hat{\\phi}_{1,1} = \\hat{\\rho}_1 $$\nFor $k > 1$:\n$$ \\hat{\\alpha}_k = \\hat{\\phi}_{k,k} = \\frac{\\hat{\\rho}_k - \\sum_{j=1}^{k-1} \\hat{\\phi}_{k-1,j} \\hat{\\rho}_{k-j}}{1 - \\sum_{j=1}^{k-1} \\hat{\\phi}_{k-1,j} \\hat{\\rho}_{j}} $$\nThe other coefficients for the intermediate $\\text{AR}(k)$ model are updated as:\n$$ \\hat{\\phi}_{k,j} = \\hat{\\phi}_{k-1,j} - \\hat{\\alpha}_k \\hat{\\phi}_{k-1,k-j} \\quad \\text{for } j=1, \\dots, k-1 $$\nThis algorithm is applied iteratively for $k=1, \\dots, K$ to obtain the full sample PACF sequence.\n\nFinally, we apply the specified verification criteria. The ACF is deemed to have a \"clear sinusoidal pattern\" if two conditions are met:\n$1$. The number of sign changes in the sequence $\\{\\hat{\\rho}_{k}\\}_{k=1}^{K}$, considering only terms where $|\\hat{\\rho}_{k}| \\ge \\varepsilon = 0.05$, must be at least $S_{\\min} = 3$.\n$2$. The magnitude must exhibit overall decay, specified as $|\\hat{\\rho}_{K}| < |\\hat{\\rho}_{1}|$.\n\nThe PACF is declared to \"not have a sinusoidal pattern\" if it effectively cuts off after the true model order of $2$. This is tested by the condition:\n$$ \\max_{k \\in \\{3,4,\\dots,K\\}} |\\hat{\\alpha}_{k}| \\le \\tau $$\nwhere the tolerance is $\\tau = 0.30$.\n\nA test case passes verification if and only if both the ACF and PACF criteria are satisfied. The procedure is executed for each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_ar2_series(phi1, phi2, n, burn_in, seed):\n    \"\"\"\n    Simulates a zero-mean AR(2) process.\n\n    Args:\n        phi1 (float): The first AR coefficient.\n        phi2 (float): The second AR coefficient.\n        n (int): The length of the output series.\n        burn_in (int): The number of initial points to discard.\n        seed (int): The seed for the random number generator.\n\n    Returns:\n        numpy.ndarray: The simulated AR(2) time series of length n.\n    \"\"\"\n    total_len = n + burn_in\n    rng = np.random.default_rng(seed)\n    epsilon = rng.normal(loc=0.0, scale=1.0, size=total_len)\n    \n    x = np.zeros(total_len)\n    # Initial values are X_0=0, X_{-1}=0, so first two terms are just noise\n    if total_len > 0:\n        x[0] = epsilon[0]\n    if total_len > 1:\n        x[1] = phi1 * x[0] + epsilon[1]\n\n    for t in range(2, total_len):\n        x[t] = phi1 * x[t-1] + phi2 * x[t-2] + epsilon[t]\n        \n    return x[burn_in:]\n\ndef compute_acf_pacf(series, K):\n    \"\"\"\n    Computes sample ACF and PACF from first principles using Levinson-Durbin.\n\n    Args:\n        series (numpy.ndarray): The time series data.\n        K (int): The maximum lag to compute.\n\n    Returns:\n        tuple: A tuple containing:\n            - numpy.ndarray: The sample ACF up to lag K.\n            - numpy.ndarray: The sample PACF up to lag K.\n    \"\"\"\n    n = len(series)\n    mu_hat = np.mean(series)\n    demeaned_series = series - mu_hat\n\n    # Sample Autocovariance\n    gamma_hat = np.zeros(K + 1)\n    for k in range(K + 1):\n        # As per problem: (1/n) * sum_{t=k+1 to n} ...\n        # which involves n-k terms\n        gamma_hat[k] = (1/n) * np.sum(demeaned_series[k:] * demeaned_series[:n-k])\n\n    # Sample Autocorrelation\n    rho_hat = gamma_hat / gamma_hat[0]\n\n    # Sample Partial Autocorrelation (Levinson-Durbin recursion)\n    pacf = np.zeros(K + 1)\n    phi = np.zeros(K + 1)\n    \n    for k in range(1, K + 1):\n        # Calculate phi_k,k which is the PACF at lag k\n        num_sum = np.dot(phi[1:k], rho_hat[k-1:0:-1])\n        den_sum = np.dot(phi[1:k], rho_hat[1:k])\n        \n        phi_kk = (rho_hat[k] - num_sum) / (1 - den_sum)\n        pacf[k] = phi_kk\n        \n        # Update phi coefficients for the next iteration\n        phi_old = phi[1:k].copy()\n        phi[k] = phi_kk\n        phi[1:k] = phi_old - phi_kk * phi_old[::-1]\n\n    return rho_hat, pacf\n\ndef verify_criteria(rho_hat, pacf, K, epsilon, S_min, tau):\n    \"\"\"\n    Verifies the conditions for ACF and PACF patterns.\n\n    Args:\n        rho_hat (numpy.ndarray): The sample ACF.\n        pacf (numpy.ndarray): The sample PACF.\n        K (int): The maximum lag.\n        epsilon (float): Threshold for ACF magnitude.\n        S_min (int): Minimum number of sign changes for ACF.\n        tau (float): Threshold for PACF cutoff.\n\n    Returns:\n        bool: True if all criteria are met, False otherwise.\n    \"\"\"\n    # 1. ACF sinusoidal pattern verification\n    acf_is_sinusoidal = False\n    \n    # Condition 1.1: Sign changes\n    rho_filtered = [r for r in rho_hat[1:K+1] if np.abs(r) >= epsilon]\n    num_sign_changes = 0\n    if len(rho_filtered) > 1:\n        for i in range(len(rho_filtered) - 1):\n            if np.sign(rho_filtered[i]) != np.sign(rho_filtered[i+1]):\n                num_sign_changes += 1\n    \n    # Condition 1.2: Magnitude decay\n    magnitude_decay = np.abs(rho_hat[K]) < np.abs(rho_hat[1])\n    \n    if num_sign_changes >= S_min and magnitude_decay:\n        acf_is_sinusoidal = True\n\n    # 2. PACF cutoff verification\n    pacf_cuts_off = False\n    if K >= 3:\n        max_abs_pacf_tail = np.max(np.abs(pacf[3:K+1]))\n        if max_abs_pacf_tail <= tau:\n            pacf_cuts_off = True\n    else: # If K < 3, criteria cannot be checked, so it is considered not met.\n        pacf_cuts_off = False\n\n    return acf_is_sinusoidal and pacf_cuts_off\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (phi1, phi2, n, K, seed, burn_in)\n        (1.5, -0.75, 600, 20, 123, 500),\n        (1.2, -0.64, 400, 20, 456, 500),\n        (1.6, -0.85, 300, 20, 789, 500),\n    ]\n\n    epsilon = 0.05\n    S_min = 3\n    tau = 0.30\n\n    results = []\n    for case in test_cases:\n        phi1, phi2, n, K, seed, burn_in = case\n        \n        # 1. Simulate the AR(2) process\n        series = generate_ar2_series(phi1, phi2, n, burn_in, seed)\n        \n        # 2. Compute sample ACF and PACF from first principles\n        rho_hat, pacf_hat = compute_acf_pacf(series, K)\n        \n        # 3. Verify conditions\n        result = verify_criteria(rho_hat, pacf_hat, K, epsilon, S_min, tau)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"}, {"introduction": "In contrast to AR models that depend on past observations, Moving Average (MA) models express the current value as a function of past random shocks or errors. This alternative structure produces a distinct and opposite diagnostic signature: its ACF cuts off sharply after a certain lag $q$, while its PACF tails off exponentially. In this practice problem, you will generate an MA process by applying a moving average filter to white noise—a classic demonstration of the Slutsky-Yule effect—and observe this signature firsthand [@problem_id:2373117], gaining insight into how seemingly random data can harbor a clear, identifiable structure.", "id": "2373117", "problem": "You are given a white-noise-driven linear time series obtained by applying a simple moving average filter to independent and identically distributed (i.i.d.) innovations. Let $\\{x_t\\}_{t=0}^{T-1}$ be i.i.d. Gaussian innovations with mean $0$ and variance $\\sigma^2$. Define the filtered series $\\{y_t\\}$ by\n$$\ny_t \\;=\\; \\frac{1}{5}\\sum_{i=0}^{4} x_{t-i}\\quad \\text{for all } t \\in \\{4,5,\\dots,T-1\\},\n$$\nso that the available filtered sample is $\\{y_0,\\dots,y_{n-1}\\}$ with $n = T-4$, where $y_j$ corresponds to the original time index $t=j+4$. The Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) of $\\{y_t\\}$ are to be analyzed.\n\nDefinitions to be used:\n- The sample autocorrelation at lag $k \\in \\{0,1,\\dots,K\\}$ for a sample $\\{y_0,\\dots,y_{n-1}\\}$ with sample mean $\\bar y$ is\n$$\n\\hat r_k \\;=\\; \\frac{\\sum_{t=k}^{n-1} (y_t-\\bar y)(y_{t-k}-\\bar y)}{\\sum_{t=0}^{n-1} (y_t-\\bar y)^2}.\n$$\n- The sample partial autocorrelation at lag $k \\in \\{1,2,\\dots,K\\}$, denoted $\\widehat\\phi_k$, is the coefficient on $y_{t-k}$ in the linear projection of $y_t$ on $\\{y_{t-1},\\dots,y_{t-k}\\}$, defined as the unique solution to the normal equations for that least-squares projection.\n- The exact (population) autocorrelation function $\\rho_k$ of $\\{y_t\\}$ is to be derived from first principles using the definition of $\\{y_t\\}$ as a linear filter of white noise and the definition of the autocovariance function.\n\nFor each parameter set below, do the following steps:\n1. Generate $\\{x_t\\}_{t=0}^{T-1}$ using the specified pseudo-random seed, with $x_t \\sim \\mathcal N(0,\\sigma^2)$.\n2. Construct $\\{y_t\\}$ via the moving average filter of length $5$ as defined above, and discard the first $4$ indices so that the computational sample is $\\{y_0,\\dots,y_{n-1}\\}$ with $n=T-4$.\n3. Compute the sample autocorrelations $\\{\\hat r_k\\}_{k=0}^{K}$ with $K=20$ using the definition above.\n4. Compute the sample partial autocorrelations $\\{\\widehat\\phi_k\\}_{k=1}^{K}$ with $K=20$ using the definition above.\n5. From first principles, derive the exact (population) autocorrelation values $\\{\\rho_k\\}_{k=1}^{4}$ for the filtered process $\\{y_t\\}$. Use these exact values only for lags $k \\in \\{1,2,3,4\\}$.\n6. Compute the following three summary metrics:\n   - $e_1 \\;=\\; \\max_{k \\in \\{1,2,3,4\\}} \\left| \\hat r_k - \\rho_k \\right|$.\n   - $e_2 \\;=\\; \\max_{k \\in \\{5,6,\\dots,20\\}} \\left| \\hat r_k \\right|$.\n   - $e_3 \\;=\\; \\dfrac{\\frac{1}{15}\\sum_{k=6}^{20} |\\widehat\\phi_k|}{|\\widehat\\phi_1| + 10^{-12}}$.\n\nReport $(e_1,e_2,e_3)$ for each parameter set, rounded to exactly $6$ decimal places.\n\nTest suite (each item lists $(T,\\sigma,\\text{seed})$):\n- Case A: $(T,\\sigma,\\text{seed}) = (1000,\\,1.0,\\,11)$.\n- Case B: $(T,\\sigma,\\text{seed}) = (200,\\,2.0,\\,22)$.\n- Case C: $(T,\\sigma,\\text{seed}) = (80,\\,0.5,\\,33)$.\n- Case D: $(T,\\sigma,\\text{seed}) = (50,\\,1.0,\\,44)$.\n- Case E: $(T,\\sigma,\\text{seed}) = (25,\\,1.0,\\,55)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list with three floats in the order $[e_1,e_2,e_3]$. For example, the required structure is\n$$\n\\big[\\,[e_{1,A},e_{2,A},e_{3,A}],[e_{1,B},e_{2,B},e_{3,B}],\\dots\\big],\n$$\nand each float must be rounded to exactly $6$ decimal places.", "solution": "The problem presented is valid. It is scientifically grounded in the established theory of linear time series analysis, specifically concerning Moving Average (MA) processes. The problem is well-posed, providing all necessary definitions, parameters, and a clear set of computational tasks. There are no contradictions, ambiguities, or factual inaccuracies. We will therefore proceed with a complete solution.\n\nThe core of the problem is to analyze the properties of a time series $\\{y_t\\}$ generated by applying a simple moving average filter of order $q=4$ to a Gaussian white noise process $\\{x_t\\}$. This construction defines $\\{y_t\\}$ as a Moving Average process of order $4$, denoted MA($4$).\n\nFirst, we derive the exact, or population, autocorrelation function (ACF) $\\rho_k$ for the process $\\{y_t\\}$ from first principles. The process is defined as:\n$$\ny_t = \\frac{1}{5}\\sum_{i=0}^{4} x_{t-i}\n$$\nwhere $\\{x_t\\}$ are independent and identically distributed (i.i.d.) random variables with mean $E[x_t] = 0$ and variance $\\text{Var}(x_t) = E[x_t^2] = \\sigma^2$.\n\nThe mean of the process $\\{y_t\\}$ is:\n$$\nE[y_t] = E\\left[\\frac{1}{5}\\sum_{i=0}^{4} x_{t-i}\\right] = \\frac{1}{5}\\sum_{i=0}^{4} E[x_{t-i}] = 0\n$$\nThe process $\\{y_t\\}$ is thus zero-mean. The autocovariance function, $\\gamma_k = E[y_t y_{t-k}]$, determines its correlation structure. The variance of the process is the autocovariance at lag $k=0$:\n$$\n\\gamma_0 = E[y_t^2] = E\\left[ \\left(\\frac{1}{5}\\sum_{i=0}^{4} x_{t-i}\\right)^2 \\right] = \\frac{1}{25} E\\left[ \\sum_{i=0}^{4}\\sum_{j=0}^{4} x_{t-i}x_{t-j} \\right]\n$$\nDue to the i.i.d. nature of $\\{x_t\\}$, the expectation $E[x_{t-i}x_{t-j}]$ is non-zero only when the time indices match, i.e., $t-i=t-j$, which implies $i=j$. In this case, $E[x_{t-i}^2] = \\sigma^2$.\n$$\n\\gamma_0 = \\frac{1}{25} \\sum_{i=0}^{4} E[x_{t-i}^2] = \\frac{1}{25} \\sum_{i=0}^{4} \\sigma^2 = \\frac{5\\sigma^2}{25} = \\frac{\\sigma^2}{5}\n$$\nFor a lag $k > 0$, the autocovariance is:\n$$\n\\gamma_k = E[y_t y_{t-k}] = \\frac{1}{25} E\\left[ \\left(\\sum_{i=0}^{4} x_{t-i}\\right) \\left(\\sum_{j=0}^{4} x_{t-k-j}\\right) \\right]\n$$\nThe cross-term $E[x_{t-i}x_{t-k-j}]$ is non-zero only if $t-i = t-k-j$, which simplifies to $j=i-k$. We must count the number of pairs $(i,j)$ that satisfy this condition, subject to the constraints $i, j \\in \\{0, 1, 2, 3, 4\\}$. The number of such pairs is $5-k$ for $k \\in \\{1, 2, 3, 4\\}$.\nFor $k=1$, we have $4$ pairs: $(1,0), (2,1), (3,2), (4,3)$. Thus, $\\gamma_1 = \\frac{4\\sigma^2}{25}$.\nFor $k=2$, we have $3$ pairs: $(2,0), (3,1), (4,2)$. Thus, $\\gamma_2 = \\frac{3\\sigma^2}{25}$.\nFor $k=3$, we have $2$ pairs: $(3,0), (4,1)$. Thus, $\\gamma_3 = \\frac{2\\sigma^2}{25}$.\nFor $k=4$, we have $1$ pair: $(4,0)$. Thus, $\\gamma_4 = \\frac{\\sigma^2}{25}$.\nFor $k \\ge 5$, no pairs $(i, j)$ satisfy the condition $j=i-k$ within the specified ranges, so the filter windows do not overlap. Consequently, $\\gamma_k = 0$ for all $k \\ge 5$.\n\nThe population ACF is $\\rho_k = \\gamma_k / \\gamma_0$. Notice that $\\sigma^2$ cancels out.\n$$\n\\rho_k = \\frac{(5-k)\\sigma^2/25}{5\\sigma^2/25} = \\frac{5-k}{5} = 1 - \\frac{k}{5} \\quad \\text{for } k \\in \\{1, 2, 3, 4\\}\n$$\nSpecifically, the required values for metric $e_1$ are:\n- $\\rho_1 = 4/5 = 0.8$\n- $\\rho_2 = 3/5 = 0.6$\n- $\\rho_3 = 2/5 = 0.4$\n- $\\rho_4 = 1/5 = 0.2$\n\nThe computational procedure involves simulating the process and calculating sample statistics.\n1.  For each test case $(T, \\sigma, \\text{seed})$, a sequence $\\{x_t\\}_{t=0}^{T-1}$ of $T$ i.i.d. random variables is generated from a normal distribution $\\mathcal{N}(0, \\sigma^2)$ using the specified pseudo-random seed.\n2.  The filtered series $\\{y_t\\}$ is constructed by convolving $\\{x_t\\}$ with the filter kernel $[1/5, 1/5, 1/5, 1/5, 1/5]$. The 'valid' part of the convolution is taken, which corresponds to indices where the filter fully overlaps with the data, yielding a sample of length $n = T-4$. This sample is denoted $\\{y_0, \\dots, y_{n-1}\\}$ for computation, where $y_j$ corresponds to the process value at original time $t=j+4$.\n3.  The sample autocorrelation function (ACF) $\\{\\hat r_k\\}_{k=0}^{K}$ for $K=20$ is computed using the formula provided:\n    $$\n    \\hat r_k = \\frac{\\sum_{t=k}^{n-1} (y_t-\\bar y)(y_{t-k}-\\bar y)}{\\sum_{t=0}^{n-1} (y_t-\\bar y)^2}\n    $$\n    where $\\bar y$ is the sample mean of $\\{y_0, \\dots, y_{n-1}\\}$. The denominator is the total sum of squares of the mean-centered data, and the numerator is the sample autocovariance at lag $k$.\n4.  The sample partial autocorrelation function (PACF) $\\{\\widehat\\phi_k\\}_{k=1}^{K}$ for $K=20$ is computed. The value $\\widehat\\phi_k$ is defined as the last coefficient in the solution to the $k$-th order Yule-Walker equations. For each lag $k \\in \\{1, \\dots, K\\}$, we solve the system $\\mathbf{R}_k \\boldsymbol{\\phi}_k = \\mathbf{r}_k$, where $\\mathbf{R}_k$ is a $k \\times k$ Toeplitz matrix of sample autocorrelations $(\\mathbf{R}_k)_{ij} = \\hat r_{|i-j|}$, and $\\mathbf{r}_k = [\\hat r_1, \\dots, \\hat r_k]^T$. The PACF coefficient $\\widehat\\phi_k$ is the $k$-th element of the solution vector $\\boldsymbol{\\phi}_k$. This system is efficiently solved using a specialized Toeplitz solver. For $k=1$, $\\widehat\\phi_1$ is simply $\\hat r_1$.\n5.  With the theoretical values $\\{\\rho_k\\}$ and sample statistics $\\{\\hat r_k\\}, \\{\\widehat\\phi_k\\}$ computed, the final metrics are calculated as specified:\n    - $e_1 = \\max_{k \\in \\{1,2,3,4\\}} | \\hat r_k - \\rho_k |$: This metric measures the maximum deviation of the sample ACF from the true ACF for the lags where the correlation is non-zero.\n    - $e_2 = \\max_{k \\in \\{5,6,\\dots,20\\}} | \\hat r_k |$: This metric quantifies the magnitude of the sample ACF in the tail, where the true ACF is zero. For an MA process, the ACF should ideally cut off after lag $q=4$.\n    - $e_3 = (\\frac{1}{15}\\sum_{k=6}^{20} |\\widehat\\phi_k|) / (|\\widehat\\phi_1| + 10^{-12})$: This metric assesses the behavior of the PACF tail. For an MA process, the PACF is expected to decay exponentially, not cut off. This metric computes the average magnitude of the PACF tail (from lag $6$ onwards) relative to the magnitude of the first PACF coefficient.\n\nThe implementation will follow these steps precisely for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_toeplitz\n\ndef solve():\n    \"\"\"\n    Solves the time series analysis problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (T, sigma, seed)\n        (1000, 1.0, 11),\n        (200, 2.0, 22),\n        (80, 0.5, 33),\n        (50, 1.0, 44),\n        (25, 1.0, 55),\n    ]\n\n    all_results = []\n    for T, sigma, seed in test_cases:\n        # Set maximum lag for analysis\n        K = 20\n\n        # Step 1: Generate white noise process {x_t}\n        rng = np.random.default_rng(seed)\n        x = rng.normal(loc=0.0, scale=sigma, size=T)\n\n        # Step 2: Construct filtered series {y_t} via moving average\n        filter_coeffs = np.full(5, 1.0/5.0)\n        # 'valid' convolution generates the series y_t for t=4..T-1\n        # The resulting sample is of length n = T - 4\n        y = np.convolve(x, filter_coeffs, mode='valid')\n        n = len(y)\n\n        # Step 3: Compute sample autocorrelations {r_k}\n        y_mean = np.mean(y)\n        y_demeaned = y - y_mean\n        \n        # The denominator is constant for all lags\n        denom_acf = np.sum(y_demeaned**2)\n        \n        # Ensure denominator is not zero to avoid division errors, though unlikely\n        if denom_acf == 0:\n            r_hat = np.zeros(K + 1)\n        else:\n            r_hat = np.zeros(K + 1)\n            for k in range(K + 1):\n                # Numerator is the sample autocovariance at lag k\n                # sum_{t=k}^{n-1} (y_t-y_bar)(y_{t-k}-y_bar)\n                num_acf = np.dot(y_demeaned[k:], y_demeaned[:n-k])\n                r_hat[k] = num_acf / denom_acf\n\n        # Step 4: Compute sample partial autocorrelations {phi_k}\n        phi_hat = np.zeros(K + 1)  # Index 0 is unused, phi_hat[k] stores phi_k\n        if n > 1:\n            phi_hat[1] = r_hat[1]\n        \n            for k in range(2, K + 1):\n                if k >= n: # Cannot compute if lag k is too large for sample\n                    continue\n                # Solve Yule-Walker equations for PACF at lag k\n                # R_k * phi_k = r_k\n                # R_k is a Toeplitz matrix formed by r_hat[0]...r_hat[k-1]\n                # c is the first column of the Toeplitz matrix\n                c = r_hat[:k]\n                # b is the right-hand side vector\n                b = r_hat[1:k+1]\n                \n                try:\n                    # solve_toeplitz returns the solution to phi_k = [phi_k1, ..., phi_kk]\n                    solution = solve_toeplitz((c, c), b, check_finite=False)\n                    # The PACF at lag k is the last coefficient, phi_kk\n                    phi_hat[k] = solution[-1]\n                except np.linalg.LinAlgError:\n                    # Matrix is singular, PACF is ill-defined. Set to 0.\n                    phi_hat[k] = 0.0\n\n        # Step 5: Define exact (population) autocorrelation values for lags 1-4\n        rho_exact = np.array([0.8, 0.6, 0.4, 0.2])\n\n        # Step 6: Compute the three summary metrics\n        \n        # e1: max deviation of sample ACF from population ACF for lags 1-4\n        e1 = np.max(np.abs(r_hat[1:5] - rho_exact))\n\n        # e2: max magnitude of sample ACF for lags 5-20 (where population ACF is 0)\n        # Slicing up to K+1 to include r_hat[K]\n        e2 = np.max(np.abs(r_hat[5:K+1]))\n\n        # e3: ratio of average PACF tail magnitude to first PACF magnitude\n        phi_hat_abs = np.abs(phi_hat)\n        # Sum over k from 6 to 20 (15 terms)\n        num_e3 = (1.0/15.0) * np.sum(phi_hat_abs[6:K+1])\n        den_e3 = phi_hat_abs[1] + 1e-12\n        e3 = num_e3 / den_e3\n        \n        case_result = [round(e1, 6), round(e2, 6), round(e3, 6)]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a list of lists.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```"}]}