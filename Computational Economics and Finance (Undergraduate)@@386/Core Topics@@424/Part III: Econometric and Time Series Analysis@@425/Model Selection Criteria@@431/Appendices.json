{"hands_on_practices": [{"introduction": "The fundamental trade-off in model selection is between goodness-of-fit and complexity. Both the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) formalize this by adding a penalty term to the log-likelihood that increases with the number of parameters, $k$. This first exercise [@problem_id:1936657] isolates these penalty terms—$2k$ for AIC and $k \\ln(n)$ for BIC—allowing you to directly compare how they punish model complexity and observe how BIC's penalty uniquely scales with sample size, $n$. Understanding this structural difference is the first step toward interpreting the choices these criteria make in practice.", "id": "1936657", "problem": "An ecologist is analyzing a dataset of species abundance over time, which consists of $n=50$ distinct observations. They are considering a particular population dynamics model that requires estimating $k=5$ free parameters from the data. To evaluate the model's quality while penalizing complexity, they use two common information criteria.\n\nThe Akaike Information Criterion (AIC) is defined as $\\text{AIC} = 2k - 2\\ln(\\mathcal{L})$, and the Bayesian Information Criterion (BIC) is defined as $\\text{BIC} = k\\ln(n) - 2\\ln(\\mathcal{L})$. In these formulas, $\\mathcal{L}$ is the maximized value of the likelihood function for the model, $k$ is the number of estimated parameters, and $n$ is the number of observations.\n\nThe terms $2k$ (for AIC) and $k\\ln(n)$ (for BIC) are known as the \"penalty terms\" because they increase the value of the criterion as the model complexity (i.e., the number of parameters $k$) increases.\n\nCalculate the difference between the penalty term of the BIC and the penalty term of the AIC for this specific model and dataset. Express your answer as a numerical value rounded to three significant figures.\n\n", "solution": "The AIC penalty term is $2k$ and the BIC penalty term is $k\\ln(n)$. The difference (BIC penalty minus AIC penalty) is\n$$\nk\\ln(n) - 2k = k\\left(\\ln(n) - 2\\right).\n$$\nSubstituting $k=5$ and $n=50$ gives\n$$\n5\\left(\\ln(50) - 2\\right).\n$$\nUsing the numerical value $\\ln(50) \\approx 3.912023$, we obtain\n$$\n5\\left(3.912023 - 2\\right) = 5 \\times 1.912023 = 9.560115.\n$$\nRounded to three significant figures, the result is $9.56$.", "answer": "$$\\boxed{9.56}$$"}, {"introduction": "With the concept of a complexity penalty in hand, we now move to a practical model selection scenario. It is a common situation where a more complex model achieves a better fit (i.e., a higher log-likelihood), but is this improvement genuine or simply a case of overfitting? This practice [@problem_id:1447591] challenges you to use the Bayesian Information Criterion (BIC) to adjudicate between a simpler and a more complex model for biological growth. This exercise helps you develop the skill of making a principled choice that balances a model's explanatory power with its parsimony.", "id": "1447591", "problem": "A systems biologist is investigating the population dynamics of a specific strain of bacteria in response to a fluctuating nutrient environment. Two competing mathematical models, Model A and Model B, are proposed to describe the bacterial growth. The biologist performs an experiment and collects a time-series dataset consisting of $n=200$ measurements of the bacterial population density.\n\nModel A is a simpler model with $k_A = 3$ adjustable parameters. After fitting this model to the experimental data, the maximum value of the natural logarithm of the likelihood function is found to be $\\ln(\\mathcal{L}_A) = -180.5$.\n\nModel B is a more complex model that incorporates additional dynamics, involving a total of $k_B = 5$ adjustable parameters. Fitting this model to the same dataset yields a maximum natural log-likelihood of $\\ln(\\mathcal{L}_B) = -177.0$.\n\nTo decide which model is better supported by the data while penalizing for complexity, the researcher uses the Bayesian Information Criterion (BIC). The BIC is calculated using the formula:\n$$BIC = k \\ln(n) - 2 \\ln(\\mathcal{L})$$\nwhere $k$ is the number of parameters in the model, $n$ is the number of data points, and $\\ln(\\mathcal{L})$ is the maximum natural log-likelihood. A model with a lower BIC score is considered a better fit to the data.\n\nTo quantify the evidence of one model over the other, calculate the difference $\\Delta BIC = BIC_{B} - BIC_{A}$. Express your answer for $\\Delta BIC$ as a real number rounded to three significant figures.\n\n", "solution": "The Bayesian Information Criterion for a model with parameter count $k$, sample size $n$, and maximum log-likelihood $\\ln(\\mathcal{L})$ is\n$$\nBIC = k \\ln(n) - 2 \\ln(\\mathcal{L}).\n$$\nFor Model A with $k_{A} = 3$, $n = 200$, and $\\ln(\\mathcal{L}_{A}) = -180.5$,\n$$\nBIC_{A} = 3 \\ln(200) - 2(-180.5) = 3 \\ln(200) + 361.\n$$\nFor Model B with $k_{B} = 5$, $n = 200$, and $\\ln(\\mathcal{L}_{B}) = -177.0$,\n$$\nBIC_{B} = 5 \\ln(200) - 2(-177.0) = 5 \\ln(200) + 354.\n$$\nThe difference is\n$$\n\\Delta BIC = BIC_{B} - BIC_{A} = \\left(5 \\ln(200) + 354\\right) - \\left(3 \\ln(200) + 361\\right) = 2 \\ln(200) - 7.\n$$\nCompute $\\ln(200)$ numerically:\n$$\n\\ln(200) = \\ln(2) + 2 \\ln(10) \\approx 0.6931471806 + 2 \\times 2.302585093 \\approx 5.298317367.\n$$\nTherefore,\n$$\n\\Delta BIC \\approx 2 \\times 5.298317367 - 7 = 10.596634734 - 7 = 3.596634734.\n$$\nRounding to three significant figures gives $3.60$.", "answer": "$$\\boxed{3.60}$$"}, {"introduction": "Model selection criteria are not limited to choosing between a few pre-specified models; they can serve as a powerful engine for searching across a vast space of potential model structures. In this advanced computational exercise, you will tackle a cornerstone problem in econometrics: detecting a structural break in a time series. By treating each potential break point as a different model, you will use AIC and BIC to estimate the most likely location of the change, observing firsthand how their differing penalties can lead to different conclusions about the underlying data generating process [@problem_id:2410416].", "id": "2410416", "problem": "You are given a univariate time series generated by a linear model with a single potential structural break in the regression function at an unknown time index. Let the time index be $t \\in \\{1,2,\\dots,T\\}$. The data generating process is\n$$\ny_t =\n\\begin{cases}\n\\alpha_1 + \\beta_1 t + \\varepsilon_t, & \\text{if } t \\le \\tau_0, \\\\\n\\alpha_2 + \\beta_2 t + \\varepsilon_t, & \\text{if } t > \\tau_0,\n\\end{cases}\n$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ are independent and identically distributed Gaussian disturbances. The true break index is $\\tau_0 \\in \\{0,1,2,\\dots,T-1\\}$, where the value $\\tau_0=0$ denotes that there is no structural break and the data follow a single linear model $y_t=\\alpha+\\beta t+\\varepsilon_t$ for all $t$.\n\nFor any candidate split index $\\tau \\in \\{1,2,\\dots,T-1\\}$, define the piecewise linear model\n$$\ny_t =\n\\begin{cases}\n\\tilde{\\alpha}_1 + \\tilde{\\beta}_1 t + u_t, & \\text{if } t \\le \\tau, \\\\\n\\tilde{\\alpha}_2 + \\tilde{\\beta}_2 t + u_t, & \\text{if } t > \\tau,\n\\end{cases}\n$$\nwith $u_t \\sim \\mathcal{N}(0,\\tilde{\\sigma}^2)$ independent and identically distributed. The parameter vector of this model has $k_{\\text{br}}=5$ free parameters (two intercepts, two slopes, and one variance). The no-break model is $y_t=\\tilde{\\alpha}+\\tilde{\\beta} t + u_t$ with $k_{\\text{nb}}=3$ free parameters (one intercept, one slope, and one variance). For any model fitted by Ordinary Least Squares (OLS), let $\\widehat{\\text{SSR}}$ be its residual sum of squares over all $T$ observations. Under the Gaussian disturbance assumption, the maximized log-likelihood is\n$$\n\\ell = -\\frac{T}{2}\\left[\\log(2\\pi) + 1 + \\log\\left(\\frac{\\widehat{\\text{SSR}}}{T}\\right)\\right].\n$$\nDefine the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) as\n$$\n\\text{AIC} = 2k - 2\\ell, \\quad \\text{BIC} = k \\log(T) - 2\\ell,\n$$\nwhere $k$ is the number of free parameters of the model.\n\nTask: For each test case below, simulate one realization $\\{(t,y_t)\\}_{t=1}^T$ from the specified data generating process using the provided random seed for reproducibility. Consider the candidate set of split indices $\\tau$ restricted to the interval $[\\lceil \\lambda_{\\min} T \\rceil, \\lfloor \\lambda_{\\max} T \\rfloor]$ and enforce a minimum segment size of $m_{\\min}=5$ observations on each side of the split. In addition, include the no-break model as a candidate (denote this candidate by $\\tau=0$ in the output). For each candidate, fit the corresponding OLS model(s), compute $\\widehat{\\text{SSR}}$, evaluate $\\ell$, $\\text{AIC}$, and $\\text{BIC}$, and select the $\\tau$ that minimizes each criterion across all candidates. Report the selected break indices for AIC and BIC as integers, where $\\tau=0$ denotes selection of the no-break model.\n\nCorrectness evaluation: For a given test case with true break index $\\tau_0$ and tolerance $d$, define the correctness of a selected break index $\\widehat{\\tau}$ as follows. If $\\tau_0>0$, then correctness is $\\text{True}$ if $|\\widehat{\\tau}-\\tau_0|\\le d$ and $\\text{False}$ otherwise. If $\\tau_0=0$, then correctness is $\\text{True}$ if $\\widehat{\\tau}=0$ and $\\text{False}$ otherwise.\n\nAngle units are not applicable. No physical units are involved. All outputs must be integers or booleans as specified.\n\nTest suite: For each row, parameters are $(T,\\tau_0,\\alpha_1,\\beta_1,\\alpha_2,\\beta_2,\\sigma,\\text{seed},\\lambda_{\\min},\\lambda_{\\max},d)$.\n\n- Case $1$: $(200,100,0.0,0.2,0.0,0.8,0.5,123,0.15,0.85,2)$.\n- Case $2$: $(200,100,0.0,0.2,0.0,0.22,2.0,456,0.15,0.85,5)$.\n- Case $3$: $(60,9,1.0,0.1,3.0,0.1,0.3,789,0.15,0.85,1)$.\n- Case $4$: $(150,0,2.0,0.3,2.0,0.3,1.0,321,0.15,0.85,0)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list $[\\widehat{\\tau}_{\\text{AIC}},\\widehat{\\tau}_{\\text{BIC}},\\text{correct}_{\\text{AIC}},\\text{correct}_{\\text{BIC}}]$, where $\\widehat{\\tau}_{\\text{AIC}}$ and $\\widehat{\\tau}_{\\text{BIC}}$ are integers (with $0$ indicating selection of the no-break model), and $\\text{correct}_{\\text{AIC}}$ and $\\text{correct}_{\\text{BIC}}$ are booleans. The single printed line must therefore look like\n$$\n\\big[ [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot], [\\cdot,\\cdot,\\cdot,\\cdot] \\big],\n$$\nwith the dots replaced by the computed values in the order of the four test cases.", "solution": "The problem posed is to identify a potential structural break in a univariate time series using the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). The problem is well-defined, scientifically grounded in standard econometric and statistical theory, and contains all necessary information for a unique solution. It is therefore deemed valid. We proceed with the solution.\n\nThe core of the task is a model selection exercise. For a given time series, we must evaluate a set of candidate models and select the one that best describes the data according to two different criteria. The candidate models are a single linear trend model (the \"no-break\" model) and a series of piecewise linear trend models, each defined by a different break point $\\tau$.\n\nLet us first formalize the procedure.\n\n**1. Data Generation**\n\nFor each test case, we simulate a time series $\\{y_t\\}_{t=1}^T$ of length $T$ based on the specified data generating process (DGP):\n$$\ny_t =\n\\begin{cases}\n\\alpha_1 + \\beta_1 t + \\varepsilon_t, & \\text{if } t \\le \\tau_0, \\\\\n\\alpha_2 + \\beta_2 t + \\varepsilon_t, & \\text{if } t > \\tau_0,\n\\end{cases}\n$$\nwhere $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ are i.i.d. normal random variables. The time index $t$ is a sequence of integers from $1$ to $T$. A specific random seed is provided for each case to ensure the reproducibility of the generated noise term $\\varepsilon_t$. The case $\\tau_0=0$ signifies that no structural break exists, and the data are generated from a single linear model $y_t = \\alpha_1 + \\beta_1 t + \\varepsilon_t$ for all $t$.\n\n**2. Candidate Models and Estimation**\n\nWe must evaluate a set of candidate models. This set includes the no-break model and all valid single-break models.\n\n**a) The No-Break Model ($\\tau=0$)**\n\nThis model is a simple linear regression over the entire sample:\n$$\ny_t = \\tilde{\\alpha} + \\tilde{\\beta} t + u_t, \\quad t=1, \\dots, T\n$$\nThe parameters $(\\tilde{\\alpha}, \\tilde{\\beta})$ are estimated using Ordinary Least Squares (OLS) by minimizing the Residual Sum of Squares (SSR). Let the vector of observations be $\\mathbf{y} = [y_1, \\dots, y_T]^T$ and the design matrix be $\\mathbf{X}_{\\text{nb}}$ of size $T \\times 2$:\n$$\n\\mathbf{X}_{\\text{nb}} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\\\ \\vdots & \\vdots \\\\ 1 & T \\end{pmatrix}\n$$\nThe OLS estimator for the coefficient vector $\\mathbf{b} = [\\tilde{\\alpha}, \\tilde{\\beta}]^T$ is $\\hat{\\mathbf{b}} = (\\mathbf{X}_{\\text{nb}}^T \\mathbf{X}_{\\text{nb}})^{-1} \\mathbf{X}_{\\text{nb}}^T \\mathbf{y}$. The resulting residual sum of squares is $\\widehat{\\text{SSR}}_{\\text{nb}} = (\\mathbf{y} - \\mathbf{X}_{\\text{nb}}\\hat{\\mathbf{b}})^T (\\mathbf{y} - \\mathbf{X}_{\\text{nb}}\\hat{\\mathbf{b}})$. This model has $k_{\\text{nb}}=3$ free parameters: $\\tilde{\\alpha}$, $\\tilde{\\beta}$, and the variance of the disturbances, $\\tilde{\\sigma}^2$.\n\n**b) The Single-Break Models ($\\tau > 0$)**\n\nFor each candidate break point $\\tau$, the model is defined as:\n$$\ny_t =\n\\begin{cases}\n\\tilde{\\alpha}_1 + \\tilde{\\beta}_1 t + u_t, & \\text{if } t \\le \\tau, \\\\\n\\tilde{\\alpha}_2 + \\tilde{\\beta}_2 t + u_t, & \\text{if } t > \\tau,\n\\end{cases}\n$$\nThis is equivalent to fitting two independent linear regressions on two sub-samples: one for $t \\in \\{1, \\dots, \\tau\\}$ and the other for $t \\in \\{\\tau+1, \\dots, T\\}$.\nFor the first segment, we have $\\mathbf{y}_1 = [y_1, \\dots, y_\\tau]^T$ and design matrix $\\mathbf{X}_1(\\tau)$.\nFor the second segment, we have $\\mathbf{y}_2 = [y_{\\tau+1}, \\dots, y_T]^T$ and design matrix $\\mathbf{X}_2(\\tau)$.\nLet the SSR for each segment be $\\widehat{\\text{SSR}}_1(\\tau)$ and $\\widehat{\\text{SSR}}_2(\\tau)$, respectively. The total residual sum of squares for the model with a break at $\\tau$ is the sum of the two: $\\widehat{\\text{SSR}}_{\\text{br}}(\\tau) = \\widehat{\\text{SSR}}_1(\\tau) + \\widehat{\\text{SSR}}_2(\\tau)$. This model has $k_{\\text{br}}=5$ free parameters: two intercepts $(\\tilde{\\alpha}_1, \\tilde{\\alpha}_2)$, two slopes $(\\tilde{\\beta}_1, \\tilde{\\beta}_2)$, and a common variance $\\tilde{\\sigma}^2$.\n\nThe set of candidate break points $\\tau$ is restricted. The index $\\tau$ must be within the interval $[\\lceil \\lambda_{\\min} T \\rceil, \\lfloor \\lambda_{\\max} T \\rfloor]$. Additionally, each regression segment must contain at least $m_{\\min}$ observations. This imposes the constraints $\\tau \\ge m_{\\min}$ and $T-\\tau \\ge m_{\\min}$, which is equivalent to $\\tau \\le T - m_{\\min}$. The final search range for $\\tau$ is therefore:\n$$\n\\tau \\in \\left[ \\max(\\lceil \\lambda_{\\min} T \\rceil, m_{\\min}), \\min(\\lfloor \\lambda_{\\max} T \\rfloor, T - m_{\\min}) \\right]\n$$\n\n**3. Model Selection Criteria**\n\nFor each candidate model (identified by $\\tau \\in \\{0\\} \\cup \\{\\text{valid break indices}\\}$), we calculate its AIC and BIC values. First, we compute the maximized log-likelihood $\\ell$ using the model's $\\widehat{\\text{SSR}}$ over all $T$ observations:\n$$\n\\ell = -\\frac{T}{2}\\left[\\log(2\\pi) + 1 + \\log\\left(\\frac{\\widehat{\\text{SSR}}}{T}\\right)\\right]\n$$\nThen, AIC and BIC are computed as:\n$$\n\\text{AIC} = 2k - 2\\ell\n$$\n$$\n\\text{BIC} = k \\log(T) - 2\\ell\n$$\nwhere $k$ is the number of parameters for the model ($k_{\\text{nb}}=3$ or $k_{\\text{br}}=5$).\n\nThe optimal break point is selected by finding the model that minimizes the respective criterion.\n$$\n\\widehat{\\tau}_{\\text{AIC}} = \\underset{\\tau \\in \\text{candidates}}{\\operatorname{argmin}} \\text{ AIC}(\\tau)\n$$\n$$\n\\widehat{\\tau}_{\\text{BIC}} = \\underset{\\tau \\in \\text{candidates}}{\\operatorname{argmin}} \\text{ BIC}(\\tau)\n$$\n\n**4. Correctness Evaluation**\n\nFinally, the selected break indices $\\widehat{\\tau}_{\\text{AIC}}$ and $\\widehat{\\tau}_{\\text{BIC}}$ are evaluated for correctness against the true break index $\\tau_0$.\n- If $\\tau_0 > 0$ (a break exists), the estimate $\\widehat{\\tau}$ is correct if $|\\widehat{\\tau} - \\tau_0| \\le d$, where $d$ is a given tolerance.\n- If $\\tau_0 = 0$ (no break exists), the estimate is correct only if $\\widehat{\\tau} = 0$.\n\nThis entire procedure is to be executed for each test case provided. The final output must be structured as a list of lists, with each inner list containing the four specified results for a test case: $[\\widehat{\\tau}_{\\text{AIC}}, \\widehat{\\tau}_{\\text{BIC}}, \\text{correct}_{\\text{AIC}}, \\text{correct}_{\\text{BIC}}]$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the structural break detection problem for a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (T, tau_0, alpha_1, beta_1, alpha_2, beta_2, sigma, seed, lambda_min, lambda_max, d)\n        (200, 100, 0.0, 0.2, 0.0, 0.8, 0.5, 123, 0.15, 0.85, 2),\n        (200, 100, 0.0, 0.2, 0.0, 0.22, 2.0, 456, 0.15, 0.85, 5),\n        (60, 9, 1.0, 0.1, 3.0, 0.1, 0.3, 789, 0.15, 0.85, 1),\n        (150, 0, 2.0, 0.3, 2.0, 0.3, 1.0, 321, 0.15, 0.85, 0),\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        T, tau_0, alpha_1, beta_1, alpha_2, beta_2, sigma, seed, lambda_min, lambda_max, d = case\n        \n        # for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        t_all = np.arange(1, T + 1)\n        y = np.zeros(T)\n        errors = rng.normal(0, sigma, T)\n\n        if tau_0 == 0:\n            y = alpha_1 + beta_1 * t_all + errors\n        else:\n            mask_1 = t_all <= tau_0\n            mask_2 = t_all > tau_0\n            y[mask_1] = alpha_1 + beta_1 * t_all[mask_1] + errors[mask_1]\n            y[mask_2] = alpha_2 + beta_2 * t_all[mask_2] + errors[mask_2]\n\n        candidate_results = []\n        \n        # 2. Candidate Models Evaluation\n        \n        # Helper function for OLS to get SSR\n        def get_ssr(y_seg, t_seg):\n            if len(y_seg) < 2: return np.inf\n            X_seg = np.vstack([np.ones_like(t_seg), t_seg]).T\n            # np.linalg.lstsq returns sum of squared residuals as the second element\n            ssr = np.linalg.lstsq(X_seg, y_seg, rcond=None)[1]\n            if not ssr: # empty if perfect fit\n                return 0.0\n            return ssr[0]\n\n        # Helper function for Info Criteria calculation\n        def get_criteria(ssr, k, T_val):\n            if ssr <= 0: # Avoid log(0)\n                # This would imply a perfect fit, log-likelihood is effectively infinite\n                return np.inf, np.inf\n            logL = -T_val / 2 * (np.log(2 * np.pi) + 1 + np.log(ssr / T_val))\n            aic = 2 * k - 2 * logL\n            bic = k * np.log(T_val) - 2 * logL\n            return aic, bic\n\n        # a) No-Break Model (tau=0)\n        k_nb = 3\n        ssr_nb = get_ssr(y, t_all)\n        aic_nb, bic_nb = get_criteria(ssr_nb, k_nb, T)\n        candidate_results.append({'tau': 0, 'aic': aic_nb, 'bic': bic_nb})\n        \n        # b) Single-Break Models (tau > 0)\n        k_br = 5\n        m_min = 5\n        tau_min_search = max(math.ceil(lambda_min * T), m_min)\n        tau_max_search = min(math.floor(lambda_max * T), T - m_min)\n\n        for tau in range(tau_min_search, tau_max_search + 1):\n            y1, t1 = y[:tau], t_all[:tau]\n            y2, t2 = y[tau:], t_all[tau:]\n            \n            ssr1 = get_ssr(y1, t1)\n            ssr2 = get_ssr(y2, t2)\n            \n            ssr_br = ssr1 + ssr2\n            aic_br, bic_br = get_criteria(ssr_br, k_br, T)\n            candidate_results.append({'tau': tau, 'aic': aic_br, 'bic': bic_br})\n\n        # 3. Model Selection\n        best_aic_model = min(candidate_results, key=lambda x: x['aic'])\n        best_bic_model = min(candidate_results, key=lambda x: x['bic'])\n        \n        tau_hat_aic = best_aic_model['tau']\n        tau_hat_bic = best_bic_model['tau']\n\n        # 4. Correctness Evaluation\n        def check_correctness(tau_hat, tau_true, d_tol):\n            if tau_true == 0:\n                return tau_hat == 0\n            else:\n                return abs(tau_hat - tau_true) <= d_tol\n\n        correct_aic = check_correctness(tau_hat_aic, tau_0, d)\n        correct_bic = check_correctness(tau_hat_bic, tau_0, d)\n        \n        all_results.append([tau_hat_aic, tau_hat_bic, bool(correct_aic), bool(correct_bic)])\n\n    # Final print statement in the exact required format.\n    # Convert manually to avoid spaces introduced by default str(list)\n    result_str = '[' + ','.join(f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in all_results) + ']'\n    print(result_str.replace('True', 'true').replace('False', 'false'))\n\nsolve()\n```"}]}