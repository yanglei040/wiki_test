{"hands_on_practices": [{"introduction": "One of the most fundamental questions about a random process concerns the likelihood of it returning to its starting point. This exercise explores this question for a symmetric 1D random walk, a cornerstone concept in probability theory with wide applications in modeling market price ticks. By working through this problem, you will use combinatorial methods to derive the exact probability of the first return to the origin at a specific future time, building deep intuition about the recurrent nature of random walks [@problem_id:2425131].", "id": "2425131", "problem": "Consider a stylized market microstructure model in which the mid-price changes in discrete time by one tick up or down with equal probability at each step, independently over time. Let $\\{X_{k}\\}_{k \\geq 1}$ be independent and identically distributed with $\\mathbb{P}(X_{k}=1)=\\mathbb{P}(X_{k}=-1)=\\tfrac{1}{2}$, and define the symmetric one-dimensional random walk $S_{0}=0$ and $S_{n}=\\sum_{k=1}^{n} X_{k}$ for $n \\geq 1$. Define the first return time to the starting level by $\\tau=\\min\\{k \\geq 1 : S_{k}=0\\}$. For a fixed positive integer $n$, calculate the probability $\\mathbb{P}(\\tau=2n)$ as a closed-form function of $n$. Give your final answer as a single analytic expression in terms of $n$. No rounding is required.", "solution": "We begin from the definition of the symmetric simple random walk. The path of length $2n$ is determined by the sequence $(X_{1},\\dots,X_{2n})$ with each element in $\\{-1,1\\}$, and each such sequence has probability $2^{-2n}$. The event $\\{\\tau=2n\\}$ is the event that $S_{2n}=0$ and $S_{k} \\neq 0$ for all $1 \\leq k \\leq 2n-1$. Because $S_{k}$ has the same parity as $k$, a return to $0$ can occur only at even times, so focusing on $2n$ is natural.\n\nTo count the number of sequences that realize $\\{\\tau=2n\\}$, we classify first by the sign of the first step. By symmetry, the number of valid paths that start with $X_{1}=1$ equals the number that start with $X_{1}=-1$. It therefore suffices to count the number of valid paths that start with $X_{1}=1$ and end with $X_{2n}=-1$, remaining strictly positive in between, and then multiply by $2$.\n\nConsider paths with $X_{1}=1$, $X_{2n}=-1$, and $S_{k} \\geq 1$ for $1 \\leq k \\leq 2n-1$. Remove the first and last steps, and for the remaining $2n-2$ steps, define a transformed path by shifting down by $1$ unit: let $Y_{j}=S_{j+1}-1$ for $j=0,1,\\dots,2n-2$, with $Y_{0}=0$ and $Y_{2n-2}=0$. The condition $S_{k} \\geq 1$ for $1 \\leq k \\leq 2n-1$ becomes $Y_{j} \\geq 0$ for all $0 \\leq j \\leq 2n-2$. Thus the transformed path is a Dyck path of length $2n-2$, that is, a nearest-neighbor path on the integers starting and ending at $0$ that never goes below $0$. The number of such Dyck paths is the Catalan number $C_{n-1}$, where\n$$\nC_{n-1}=\\frac{1}{n}\\binom{2n-2}{n-1}.\n$$\nBy symmetry, the number of valid paths beginning with $X_{1}=-1$ and remaining strictly negative until time $2n-1$ (then stepping to $0$ at time $2n$) is also $C_{n-1}$. Therefore, the total number of sequences realizing $\\{\\tau=2n\\}$ is $2 C_{n-1}$.\n\nSince each sequence has probability $2^{-2n}$, we obtain\n$$\n\\mathbb{P}(\\tau=2n)=\\frac{2 C_{n-1}}{2^{2n}}=\\frac{2}{2^{2n}}\\cdot \\frac{1}{n}\\binom{2n-2}{n-1}.\n$$\nThis can be algebraically simplified to a central-binomial form. Using the identity\n$$\n\\frac{2}{n}\\binom{2n-2}{n-1}=\\frac{1}{2n-1}\\binom{2n}{n},\n$$\nwe conclude\n$$\n\\mathbb{P}(\\tau=2n)=\\frac{1}{2n-1}\\binom{2n}{n} 2^{-2n}.\n$$\nThis is the desired closed-form expression in terms of $n$.", "answer": "$$\\boxed{\\frac{1}{2n-1}\\binom{2n}{n}2^{-2n}}$$"}, {"introduction": "Idealized financial models often assume a world without frictions, but real-world trading always involves costs. This exercise provides a hands-on opportunity to bridge the gap between theory and practice by incorporating transaction costs into a dynamic portfolio rebalancing strategy [@problem_id:2425097]. You will derive the impact of these costs on wealth accumulation and implement a simulation to quantify the performance drag, a crucial skill in computational finance and algorithmic trading.", "id": "2425097", "problem": "Consider a single risky asset and a cash account with zero return. Time is discrete with periods indexed by $t \\in \\{1,2,\\dots,T\\}$. The risky asset has independent and identically distributed (i.i.d.) one-period gross returns $G_t \\in \\{1 + r_u, 1 + r_d\\}$, where $r_u > 0$, $r_d < 0$, and $\\mathbb{P}(G_t = 1 + r_u) = p \\in (0,1)$. The cash account has gross return equal to $1$ in every period. An investor starts with initial wealth $W_0 > 0$.\n\nThe investor follows a constant-fraction rebalancing strategy: at the end of each period $t$, immediately after observing $G_t$, the investor rebalances the portfolio to target a fixed risky weight $f \\in [0,1]$ for the next period. Transaction costs are proportional and constant through time at rate $\\tau \\in [0,1)$ per unit of dollar traded. Let the dollar trading volume at time $t$ be the absolute dollar amount exchanged between the risky asset and cash to achieve the target weight for the next period. The transaction cost paid at time $t$ is $\\tau$ times this dollar trading volume, deducted from total wealth. Assume the initial allocation at $t=0$ to the target weight $f$ is costless. Assume that $f \\tau < 1$.\n\nDefine the investor’s terminal wealth after $T$ periods under transaction cost rate $\\tau$ as $W_T^{(\\tau)}$. Define $W_T^{(0)}$ analogously with $\\tau = 0$.\n\nYour task is to compute, for each given parameter set, the expected effect of the transaction cost on terminal wealth, namely the quantity $\\mathbb{E}[W_T^{(\\tau)}] - \\mathbb{E}[W_T^{(0)}]$.\n\nExpress all answers as real numbers (floats). No physical units are involved.\n\nTest Suite. Your program must compute the value of $\\mathbb{E}[W_T^{(\\tau)}] - \\mathbb{E}[W_T^{(0)}]$ for each of the following parameter sets $(W_0, T, p, r_u, r_d, f, \\tau)$:\n\n- Case 1 (general case): $(1.0, 10, 0.55, 0.015, -0.010, 0.60, 0.002)$.\n- Case 2 (no cost boundary): $(1.0, 10, 0.55, 0.015, -0.010, 0.60, 0.000)$.\n- Case 3 (no risky exposure boundary): $(1.0, 15, 0.60, 0.020, -0.015, 0.00, 0.010)$.\n- Case 4 (zero horizon boundary): $(1.0, 0, 0.50, 0.020, -0.020, 0.50, 0.010)$.\n- Case 5 (higher volatility and exposure): $(1.0, 20, 0.60, 0.030, -0.020, 0.80, 0.005)$.\n- Case 6 (symmetric up/down, positive trading cost): $(1.0, 20, 0.50, 0.020, -0.020, 0.50, 0.010)$.\n\nFinal Output Format. Your program should produce a single line of output containing the results for Cases $1$ through $6$, in order, as a comma-separated list enclosed in square brackets. Each value must be rounded to $12$ decimal places. For example, a valid output format is $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_6]$.", "solution": "The problem as stated is subjected to rigorous validation and is found to be valid. It is scientifically grounded in the established principles of financial mathematics, specifically portfolio theory and asset price dynamics under transaction costs. The problem is well-posed, objective, and self-contained, with all necessary parameters and conditions provided for a unique solution to be determined. It is a standard exercise in computational finance concerning the impact of a given trading strategy on wealth dynamics.\n\nThe task is to compute the difference in expected terminal wealth between a portfolio rebalanced with transaction costs and one rebalanced without, i.e., $\\mathbb{E}[W_T^{(\\tau)}] - \\mathbb{E}[W_T^{(0)}]$. We shall first derive the expressions for the expected terminal wealth in both scenarios.\n\nLet us begin with the case of zero transaction costs, where $\\tau = 0$.\nThe investor starts with wealth $W_0$ and allocates a fraction $f$ to a risky asset and $1-f$ to a cash account. This initial allocation is costless. At the start of any period $t$ (i.e., after rebalancing at $t-1$), the wealth invested in the risky asset is $f W_{t-1}^{(0)}$ and in cash is $(1-f) W_{t-1}^{(0)}$. During period $t$, the risky asset's value is multiplied by a gross return $G_t$, while the cash account's value is multiplied by $1$. The wealth just before rebalancing at the end of period $t$ is:\n$$W_{t, \\text{pre-rebal}}^{(0)} = (f W_{t-1}^{(0)}) G_t + ((1-f) W_{t-1}^{(0)}) \\cdot 1 = W_{t-1}^{(0)} (1 - f + fG_t)$$\nSince $\\tau = 0$, there are no costs for rebalancing. Thus, the wealth after rebalancing, $W_t^{(0)}$, is equal to the pre-rebalancing wealth. The wealth evolution is given by the recurrence relation:\n$$W_t^{(0)} = W_{t-1}^{(0)} R_t^{(0)}, \\quad \\text{where } R_t^{(0)} = 1 - f + fG_t$$\nThe one-period portfolio return $R_t^{(0)}$ is an independent and identically distributed (i.i.d.) random variable because the asset returns $G_t$ are i.i.d. The terminal wealth after $T$ periods is $W_T^{(0)} = W_0 \\prod_{t=1}^T R_t^{(0)}$.\nBy the property of i.i.d. random variables, the expectation of the product is the product of expectations:\n$$\\mathbb{E}[W_T^{(0)}] = W_0 \\mathbb{E}\\left[\\prod_{t=1}^T R_t^{(0)}\\right] = W_0 \\left(\\mathbb{E}[R_1^{(0)}]\\right)^T$$\nThe expected one-period return is:\n$$\\mathbb{E}[R_1^{(0)}] = \\mathbb{E}[1 - f + fG_1] = 1 - f + f\\mathbb{E}[G_1]$$\nGiven $\\mathbb{P}(G_t = 1 + r_u) = p$ and $\\mathbb{P}(G_t = 1 + r_d) = 1-p$, we have $\\mathbb{E}[G_1] = p(1+r_u) + (1-p)(1+r_d)$. Let $\\mu_G = \\mathbb{E}[G_1]$. Then, the expected one-period portfolio return is $\\mu^{(0)} = 1 - f + f\\mu_G$. Finally:\n$$\\mathbb{E}[W_T^{(0)}] = W_0 \\left(p(1 - f + f(1+r_u)) + (1-p)(1 - f + f(1+r_d))\\right)^T = W_0 \\left(p(1+fr_u) + (1-p)(1+fr_d)\\right)^T$$\n\nNext, we consider the case with proportional transaction costs, $\\tau > 0$.\nThe wealth before rebalancing at time $t$, denoted $W_{t, \\text{pre}}^{(\\tau)}$, is still given by $W_{t, \\text{pre}}^{(\\tau)} = W_{t-1}^{(\\tau)} (1 - f + fG_t)$, where $W_{t-1}^{(\\tau)}$ is the wealth after all costs and rebalancing at time $t-1$. The key difference lies in the cost of rebalancing. The rebalancing trade depends on whether the risky asset's weight has drifted above or below the target $f$. The risky weight before rebalancing is $f'_t = \\frac{f W_{t-1}^{(\\tau)} G_t}{W_{t, \\text{pre}}^{(\\tau)}} = \\frac{fG_t}{1-f+fG_t}$. A trade is needed if $f'_t \\neq f$.\n- If $G_t > 1$, then $f'_t > f$. The investor must sell a portion of the risky asset.\n- If $G_t < 1$, then $f'_t < f$. The investor must buy more of the risky asset.\n\nLet us analyze these two scenarios to find the post-rebalancing wealth $W_t^{(\\tau)}$.\nLet $A_{t, \\text{pre}} = fW_{t-1}^{(\\tau)}G_t$ be the value of the risky asset before the trade, and $C_{t, \\text{pre}} = (1-f)W_{t-1}^{(\\tau)}$ be the cash value.\nLet $W_t^{(\\tau)}$ be the wealth after the trade. The target holdings are $A_{t, \\text{post}} = fW_t^{(\\tau)}$ and $C_{t, \\text{post}} = (1-f)W_t^{(\\tau)}$.\n\nCase 1: Sell risky asset ($G_t=1+r_u > 1$).\nThe amount of asset sold is $\\Delta A = A_{t, \\text{pre}} - A_{t, \\text{post}} = fW_{t-1}^{(\\tau)}G_t - fW_t^{(\\tau)}$. This is the trading volume. The cost is $\\tau \\Delta A$. The proceeds from the sale added to cash are $(1-\\tau)\\Delta A$.\nThe cash balance must be reconciled: $C_{t, \\text{post}} = C_{t, \\text{pre}} + (1-\\tau)\\Delta A$.\n$$(1-f)W_t^{(\\tau)} = (1-f)W_{t-1}^{(\\tau)} + (1-\\tau)(fW_{t-1}^{(\\tau)}G_t - fW_t^{(\\tau)})$$\nCollecting terms for $W_t^{(\\tau)}$ and $W_{t-1}^{(\\tau)}$:\n$$W_t^{(\\tau)}[(1-f) + f(1-\\tau)] = W_{t-1}^{(\\tau)}[(1-f) + f(1-\\tau)G_t]$$\n$$W_t^{(\\tau)}(1-f\\tau) = W_{t-1}^{(\\tau)}(1-f+f(1-\\tau)G_t)$$\nThus, the one-period gross return when $G_t = 1+r_u$ is:\n$$R_u^{(\\tau)} = \\frac{W_t^{(\\tau)}}{W_{t-1}^{(\\tau)}} = \\frac{1-f+f(1-\\tau)(1+r_u)}{1-f\\tau}$$\n\nCase 2: Buy risky asset ($G_t=1+r_d < 1$).\nThe amount of asset bought is $\\Delta A = A_{t, \\text{post}} - A_{t, \\text{pre}} = fW_t^{(\\tau)} - fW_{t-1}^{(\\tau)}G_t$. This is the trading volume. The cost is $\\tau \\Delta A$. To finance this purchase and cost, cash required is $(1+\\tau)\\Delta A$.\nThe cash balance must be reconciled: $C_{t, \\text{post}} = C_{t, \\text{pre}} - (1+\\tau)\\Delta A$.\n$$(1-f)W_t^{(\\tau)} = (1-f)W_{t-1}^{(\\tau)} - (1+\\tau)(fW_t^{(\\tau)} - fW_{t-1}^{(\\tau)}G_t)$$\nCollecting terms:\n$$W_t^{(\\tau)}[(1-f) + f(1+\\tau)] = W_{t-1}^{(\\tau)}[(1-f) + f(1+\\tau)G_t]$$\n$$W_t^{(\\tau)}(1+f\\tau) = W_{t-1}^{(\\tau)}(1-f+f(1+\\tau)G_t)$$\nThus, the one-period gross return when $G_t = 1+r_d$ is:\n$$R_d^{(\\tau)} = \\frac{W_t^{(\\tau)}}{W_{t-1}^{(\\tau)}} = \\frac{1-f+f(1+\\tau)(1+r_d)}{1+f\\tau}$$\n\nThe dynamics of wealth with transaction costs are $W_t^{(\\tau)} = W_{t-1}^{(\\tau)} R_t^{(\\tau)}$, where $R_t^{(\\tau)}$ is an i.i.d. random variable taking value $R_u^{(\\tau)}$ with probability $p$ and $R_d^{(\\tau)}$ with probability $1-p$.\nThe expected terminal wealth is:\n$$\\mathbb{E}[W_T^{(\\tau)}] = W_0 (\\mathbb{E}[R_1^{(\\tau)}])^T$$\nwhere the expected one-period return is $\\mu^{(\\tau)} = \\mathbb{E}[R_1^{(\\tau)}] = p R_u^{(\\tau)} + (1-p) R_d^{(\\tau)}$.\n\nThe problem requires computing $\\mathbb{E}[W_T^{(\\tau)}] - \\mathbb{E}[W_T^{(0)}]$. This can be calculated by applying the derived formulas.\nThe algorithm is as follows:\n1. For a given set of parameters $(W_0, T, p, r_u, r_d, f, \\tau)$, calculate the expected one-period return with costs, $\\mu^{(\\tau)}$.\n2. Calculate the expected one-period return without costs, $\\mu^{(0)}$ (which is equivalent to setting $\\tau=0$ in the formulas for $R_u^{(\\tau)}$ and $R_d^{(\\tau)}$).\n3. Compute $\\mathbb{E}[W_T^{(\\tau)}] = W_0 (\\mu^{(\\tau)})^T$ and $\\mathbb{E}[W_T^{(0)}] = W_0 (\\mu^{(0)})^T$.\n4. The final result is the difference between these two values. Special care must be taken for boundary cases $T=0$ or $f=0$, where no trading occurs and the difference is trivially $0$. Our formulas correctly degenerate to $1$ for the expected return in these cases, leading to a wealth of $W_0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected effect of transaction costs on terminal wealth for a series of test cases.\n    \"\"\"\n    # Test suite: (W0, T, p, ru, rd, f, tau)\n    test_cases = [\n        # Case 1 (general case)\n        (1.0, 10, 0.55, 0.015, -0.010, 0.60, 0.002),\n        # Case 2 (no cost boundary)\n        (1.0, 10, 0.55, 0.015, -0.010, 0.60, 0.000),\n        # Case 3 (no risky exposure boundary)\n        (1.0, 15, 0.60, 0.020, -0.015, 0.00, 0.010),\n        # Case 4 (zero horizon boundary)\n        (1.0, 0, 0.50, 0.020, -0.020, 0.50, 0.010),\n        # Case 5 (higher volatility and exposure)\n        (1.0, 20, 0.60, 0.030, -0.020, 0.80, 0.005),\n        # Case 6 (symmetric up/down, positive trading cost)\n        (1.0, 20, 0.50, 0.020, -0.020, 0.50, 0.010),\n    ]\n\n    def calculate_expected_wealth(W0, T, p, ru, rd, f, tau):\n        \"\"\"\n        Calculates the expected terminal wealth E[W_T] based on the derived formulas.\n        \"\"\"\n        # Handle boundary cases where no trading occurs.\n        if T == 0 or f == 0:\n            return W0\n\n        # Gross returns for the risky asset\n        Gu = 1.0 + ru\n        Gd = 1.0 + rd\n\n        # Calculate one-period portfolio gross returns including transaction costs.\n        # R_u corresponds to selling the appreciated asset.\n        numerator_u = 1.0 - f + f * (1.0 - tau) * Gu\n        denominator_u = 1.0 - f * tau\n        R_u = numerator_u / denominator_u\n\n        # R_d corresponds to buying the depreciated asset.\n        numerator_d = 1.0 - f + f * (1.0 + tau) * Gd\n        denominator_d = 1.0 + f * tau\n        R_d = numerator_d / denominator_d\n        \n        # Calculate expected one-period portfolio gross return.\n        mu = p * R_u + (1.0 - p) * R_d\n        \n        # Calculate expected terminal wealth.\n        expected_W_T = W0 * (mu ** T)\n        \n        return expected_W_T\n\n    results = []\n    for case in test_cases:\n        W0, T, p, ru, rd, f, tau = case\n        \n        # Calculate expected wealth with transaction costs\n        E_WT_tau = calculate_expected_wealth(W0, T, p, ru, rd, f, tau)\n        \n        # Calculate expected wealth without transaction costs (tau=0)\n        E_WT_0 = calculate_expected_wealth(W0, T, p, ru, rd, f, 0.0)\n        \n        # Compute the difference\n        difference = E_WT_tau - E_WT_0\n        results.append(round(difference, 12))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}, {"introduction": "A good model is one whose assumptions stand up to empirical scrutiny. This practice introduces a powerful statistical tool, the Ljung–Box test, to check a key assumption of the simple random walk model: the independence of returns and their volatility over time [@problem_id:2425100]. By testing for \"volatility clustering\"—a well-documented feature of financial markets—you will learn how to diagnose model deficiencies and understand why more advanced frameworks like GARCH are often necessary for realistic financial modeling.", "id": "2425100", "problem": "You are given a discrete-time stock price process modeled as a random walk. Let the price be $\\{S_t\\}_{t=0}^n$ with increments (returns) $\\{r_t\\}_{t=1}^n$ defined by $S_t = S_{t-1} + r_t$ for $t \\in \\{1,\\dots,n\\}$. A volatility proxy is defined as the squared increments $\\{v_t\\}_{t=1}^n$, where $v_t = r_t^2$. Volatility clustering is interpreted as linear dependence across time in the sequence $\\{v_t\\}$.\n\nYour task is to implement a statistical test for volatility clustering in $\\{v_t\\}$ based on the null hypothesis that $\\{v_t\\}$ is uncorrelated across time. Let $\\bar{v} = \\frac{1}{n}\\sum_{t=1}^n v_t$ denote the sample mean of $\\{v_t\\}$. For a lag $k \\in \\{1,\\dots,h\\}$, the sample autocorrelation of $\\{v_t\\}$ is\n$$\n\\rho_k \\;=\\; \\frac{\\sum_{t=k+1}^{n} \\left(v_t - \\bar{v}\\right)\\left(v_{t-k} - \\bar{v}\\right)}{\\sum_{t=1}^{n} \\left(v_t - \\bar{v}\\right)^2}.\n$$\nLet $h' = \\min\\{h,\\,n-1\\}$. Define the Ljung–Box statistic on $\\{v_t\\}$ at $h'$ lags as\n$$\nQ \\;=\\; n(n+2)\\sum_{k=1}^{h'} \\frac{\\rho_k^2}{n-k}.\n$$\nUnder the null hypothesis that $\\{v_t\\}$ is uncorrelated across time, the statistic $Q$ is asymptotically distributed as a chi-square random variable with $h'$ degrees of freedom. Using a significance level $\\alpha$, define the decision rule “clustering detected” if and only if\n$$\nQ \\;>\\; \\chi^2_{1-\\alpha}(h'),\n$$\nwhere $\\chi^2_{1-\\alpha}(h')$ is the upper $(1-\\alpha)$ quantile of the chi-square distribution with $h'$ degrees of freedom. If $h' = 0$ or $\\sum_{t=1}^{n} \\left(v_t - \\bar{v}\\right)^2 = 0$, the test must return the boolean value corresponding to “no clustering detected.” All booleans must be represented as $True$ or $False$.\n\nUse $h = 10$ and $\\alpha = 0.01$. All computations are unitless.\n\nImplement the test for the following parameterized data-generating processes. In all cases, $\\{\\varepsilon_t\\}$ denotes independent standard normal variates with $\\varepsilon_t \\sim \\mathcal{N}(0,1)$ and the specified random seed must be used to ensure reproducibility. The increments $\\{r_t\\}$ are generated as follows, and the price process satisfies $S_t = S_{t-1} + r_t$ with $S_0$ arbitrary.\n\nTest Suite (five cases):\n- Case A (independent and identically distributed Gaussian returns, long sample):\n  - $n = 2000$, $\\sigma = 1.0$, $r_t = \\sigma \\varepsilon_t$, seed $= 12345$.\n- Case B (regime-switching variance, long sample):\n  - $n = 2000$, $\\sigma_L = 1.0$, $\\sigma_H = 3.0$, block length $= 100$.\n  - Let $\\sigma_t = \\sigma_L$ for the first block of length $100$, then $\\sigma_t = \\sigma_H$ for the next block of length $100$, and alternate thereafter, until $t = n$.\n  - $r_t = \\sigma_t \\varepsilon_t$, seed $= 24680$.\n- Case C (generalized autoregressive conditional heteroskedasticity type recursion, long sample):\n  - $n = 2000$, $r_t = \\sigma_t \\varepsilon_t$, $\\sigma_t^2 = \\omega + \\alpha r_{t-1}^2 + \\beta \\sigma_{t-1}^2$.\n  - Parameters: $\\omega = 0.05$, $\\alpha = 0.35$, $\\beta = 0.60$, initial variance $\\sigma_0^2 = \\frac{\\omega}{1 - \\alpha - \\beta}$, seed $= 98765$.\n- Case D (degenerate zero returns):\n  - $n = 200$, $r_t = 0$ for all $t \\in \\{1,\\dots,n\\}$.\n- Case E (independent and identically distributed Gaussian returns, short sample):\n  - $n = 12$, $\\sigma = 1.0$, $r_t = \\sigma \\varepsilon_t$, seed $= 54321$.\n\nYour program must evaluate the test for each case and return a boolean indicating whether volatility clustering is detected at the specified significance level. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., “[resultA,resultB,resultC,resultD,resultE]”), where each entry is either $True$ or $False$ in that order for Cases A through E.", "solution": "The problem presented requires the implementation of a statistical test for volatility clustering in a time series of stock returns. The proposed method is the Ljung-Box test applied to the squared returns, which serve as a proxy for volatility. The problem statement is scientifically sound, well-posed, and provides all necessary information for a unique, reproducible solution. It is grounded in established principles of time series econometrics. Therefore, I will proceed with a full solution.\n\nThe core of the problem is to test the null hypothesis, $H_0$, that the series of squared returns, $\\{v_t\\}_{t=1}^n$ where $v_t = r_t^2$, is uncorrelated over time. The alternative hypothesis, $H_1$, is that at least one autocorrelation is non-zero, which is interpreted as the presence of volatility clustering.\n\nFirst, we define the sample mean of the volatility proxy series $\\{v_t\\}$ as:\n$$ \\bar{v} = \\frac{1}{n} \\sum_{t=1}^n v_t $$\nThe sample autocorrelation for a given lag $k$, where $k$ is a positive integer, measures the linear relationship between $v_t$ and its past value $v_{t-k}$. The formula provided is:\n$$ \\rho_k = \\frac{\\sum_{t=k+1}^{n} (v_t - \\bar{v})(v_{t-k} - \\bar{v})}{\\sum_{t=1}^{n} (v_t - \\bar{v})^2} $$\nThe denominator is a normalization factor, representing the total variance of the series $\\{v_t\\}$. It is important to handle the case where this denominator is zero, which occurs if and only if all $v_t$ are identical. In this degenerate case, autocorrelation is undefined, and the problem correctly specifies that we must conclude \"no clustering detected.\"\n\nThe Ljung-Box test aggregates the information from the first $h'$ sample autocorrelations into a single test statistic, $Q$. The number of lags is taken as $h' = \\min\\{h, n-1\\}$, where $h=10$ is the maximum lag considered and $n$ is the sample size. This adjustment is necessary for short time series where $n-1 < h$. The $Q$ statistic is defined as:\n$$ Q = n(n+2) \\sum_{k=1}^{h'} \\frac{\\rho_k^2}{n-k} $$\nThis statistic includes a correction factor, $n(n+2)$ and the term $(n-k)$ in the denominator, which improves the performance of the test in finite samples compared to the simpler Box-Pierce statistic.\n\nUnder the null hypothesis that the true autocorrelations of $\\{v_t\\}$ are all zero, the $Q$ statistic follows, asymptotically, a chi-square distribution with $h'$ degrees of freedom, denoted $\\chi^2(h')$.\n\nThe decision rule is based on a pre-defined significance level $\\alpha$, given as $0.01$. We reject the null hypothesis of no autocorrelation if the observed value of our statistic $Q$ is unusually large. \"Unusually large\" is defined as exceeding the $(1-\\alpha)$ quantile of the corresponding chi-square distribution. This is the critical value of the test, $\\chi^2_{1-\\alpha}(h')$. The decision rule is thus:\n- If $Q > \\chi^2_{1-\\alpha}(h')$, reject $H_0$. Conclude that volatility clustering is detected (return $True$).\n- If $Q \\le \\chi^2_{1-\\alpha}(h')$, do not reject $H_0$. Conclude that there is no evidence of volatility clustering (return $False$).\n- Per the problem, if $h' = 0$ or if the variance of $\\{v_t\\}$ is zero, we also return $False$.\n\nWe will now apply this procedure to each of the five specified cases.\n\n**Case A (IID Gaussian, $n=2000$):** The returns $r_t = \\sigma \\varepsilon_t$ are generated from an independent and identically distributed (IID) standard normal process. Consequently, the squared returns $v_t = r_t^2 = \\sigma^2 \\varepsilon_t^2$ are also an IID sequence (specifically, a scaled chi-square distribution with one degree of freedom). There is no theoretical autocorrelation in $\\{v_t\\}$. We expect the Ljung-Box test to fail to reject the null hypothesis, resulting in a verdict of $False$.\n\n**Case B (Regime Switching, $n=2000$):** The variance of the returns, $\\sigma_t^2$, switches between a low state ($\\sigma_L^2$) and a high state ($\\sigma_H^2$) in blocks. A period of high variance (large $r_t^2$) will be followed by more observations from the high-variance regime, and similarly for the low-variance regime. This block structure induces persistence, and therefore positive autocorrelation, in the squared returns $\\{v_t\\}$. The test should detect this structure, and we expect the result to be $True$.\n\n**Case C (GARCH(1,1), $n=2000$):** The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model is designed specifically to capture volatility clustering. The conditional variance $\\sigma_t^2$ is an explicit function of past squared returns ($r_{t-1}^2$) and past variances ($\\sigma_{t-1}^2$). The parameters $\\alpha = 0.35$ and $\\beta = 0.60$ lead to high persistence, as $\\alpha + \\beta = 0.95$, which is close to $1$. This ensures that shocks to volatility are long-lasting, creating strong autocorrelation in $\\{v_t\\}$. The test is expected to strongly reject the null hypothesis, yielding a result of $True$. The initial variance is correctly set to the unconditional variance of the process, $\\sigma^2 = \\frac{\\omega}{1-\\alpha-\\beta}$, which is a standard and proper initialization for a stationary GARCH simulation.\n\n**Case D (Degenerate, $n=200$):** The returns are always zero, $r_t=0$. This implies $v_t = r_t^2 = 0$ for all $t$. The series $\\{v_t\\}$ has zero variance. The denominator in the autocorrelation formula, $\\sum_{t=1}^{n} (v_t - \\bar{v})^2$, is zero. As explicitly instructed, the test must return $False$.\n\n**Case E (IID Gaussian, $n=12$):** This case is similar to Case A, with an underlying IID process, so theoretically there is no volatility clustering. However, the sample size $n=12$ is very small. For this case, the number of lags will be $h' = \\min\\{10, 12-1\\} = 10$. The Ljung-Box test is based on an asymptotic approximation, and its performance can be poor in such small samples. Nevertheless, we must follow the specified procedure. The most probable outcome, given the true data generating process, is still $False$, but a spurious rejection is more likely than in Case A.\n\nThe implementation will proceed by first generating the return series $\\{r_t\\}$ for each case using the specified random seeds for reproducibility. Then, the volatility proxy series $\\{v_t\\}$ is computed. Finally, the Ljung-Box test statistic $Q$ is calculated and compared against the critical value from the $\\chi^2(h')$ distribution to yield the final boolean result.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the volatility clustering test for all specified cases.\n    \"\"\"\n\n    # Global parameters for the Ljung-Box test\n    H_LAGS = 10\n    ALPHA = 0.01\n\n    def generate_returns(case_params):\n        \"\"\"\n        Generates the return series {r_t} based on the parameters for each case.\n        \"\"\"\n        case_id = case_params['id']\n        n = case_params['n']\n        seed = case_params['seed']\n        rng = np.random.default_rng(seed=seed)\n        \n        if case_id == 'A' or case_id == 'E':\n            sigma = case_params['sigma']\n            epsilon = rng.normal(size=n)\n            r = sigma * epsilon\n        \n        elif case_id == 'B':\n            sigma_l = case_params['sigma_L']\n            sigma_h = case_params['sigma_H']\n            block_length = case_params['block_length']\n            \n            epsilon = rng.normal(size=n)\n            sigma_t = np.zeros(n)\n            num_blocks = int(np.ceil(n / block_length))\n\n            for i in range(num_blocks):\n                start_idx = i * block_length\n                end_idx = min((i + 1) * block_length, n)\n                if i % 2 == 0:\n                    sigma_t[start_idx:end_idx] = sigma_l\n                else:\n                    sigma_t[start_idx:end_idx] = sigma_h\n            \n            r = sigma_t * epsilon\n\n        elif case_id == 'C':\n            omega = case_params['omega']\n            alpha_p = case_params['alpha']\n            beta_p = case_params['beta']\n\n            epsilon = rng.normal(size=n)\n            r = np.zeros(n)\n            sigma_sq = np.zeros(n)\n            \n            # Initial variance is the unconditional variance\n            sigma_sq_0 = omega / (1.0 - alpha_p - beta_p)\n            \n            # Initialize lagged terms\n            # r_{t-1}^2 is commonly initialized with unconditional variance\n            r_sq_lagged = sigma_sq_0\n            sigma_sq_lagged = sigma_sq_0\n\n            for t in range(n):\n                sigma_sq[t] = omega + alpha_p * r_sq_lagged + beta_p * sigma_sq_lagged\n                if sigma_sq[t] < 0: # Ensure non-negative variance\n                    sigma_sq[t] = 0\n                r[t] = np.sqrt(sigma_sq[t]) * epsilon[t]\n                \n                # Update lagged terms for the next iteration\n                r_sq_lagged = r[t]**2\n                sigma_sq_lagged = sigma_sq[t]\n\n        elif case_id == 'D':\n            r = np.zeros(n)\n\n        else:\n            raise ValueError(f\"Unknown case_id: {case_id}\")\n            \n        return r\n\n    def ljung_box_test(r_series, h, alpha):\n        \"\"\"\n        Performs the Ljung-Box test for volatility clustering.\n        \"\"\"\n        n = len(r_series)\n        v = r_series**2\n        \n        v_mean = np.mean(v)\n        v_centered = v - v_mean\n        v_variance_sum = np.sum(v_centered**2)\n\n        # Failsafe for zero variance as per problem statement\n        if np.isclose(v_variance_sum, 0.0):\n            return False\n\n        h_prime = min(h, n - 1)\n\n        # Failsafe for very short series\n        if h_prime == 0:\n            return False\n\n        rho_sq_terms = []\n        for k in range(1, h_prime + 1):\n            # Numerator of rho_k\n            autocov_k = np.sum(v_centered[k:] * v_centered[:-k])\n            rho_k = autocov_k / v_variance_sum\n            rho_sq_terms.append(rho_k**2 / (n - k))\n        \n        q_statistic = n * (n + 2) * np.sum(rho_sq_terms)\n        \n        # Degrees of freedom for the chi-square distribution is h'\n        degrees_of_freedom = h_prime\n        \n        # Critical value from chi-square distribution\n        critical_value = chi2.ppf(1 - alpha, df=degrees_of_freedom)\n        \n        return q_statistic > critical_value\n\n    test_cases = [\n        {'id': 'A', 'n': 2000, 'sigma': 1.0, 'seed': 12345},\n        {'id': 'B', 'n': 2000, 'sigma_L': 1.0, 'sigma_H': 3.0, 'block_length': 100, 'seed': 24680},\n        {'id': 'C', 'n': 2000, 'omega': 0.05, 'alpha': 0.35, 'beta': 0.60, 'seed': 98765},\n        {'id': 'D', 'n': 200, 'seed': None}, # Seed is irrelevant for zero returns\n        {'id': 'E', 'n': 12, 'sigma': 1.0, 'seed': 54321},\n    ]\n\n    results = []\n    for case in test_cases:\n        r_t = generate_returns(case)\n        is_clustered = ljung_box_test(r_t, H_LAGS, ALPHA)\n        results.append(is_clustered)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"}]}